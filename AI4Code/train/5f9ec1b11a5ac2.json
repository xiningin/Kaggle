{"cell_type":{"df089187":"code","57be5ec5":"code","4cc0dee4":"code","43de4347":"code","5cfb6f8f":"code","f4516acf":"code","e7937da4":"code","df8b8ac6":"code","8eea80be":"code","0440970a":"code","b022e0f4":"code","476c1960":"code","ae3337c7":"code","6a1c7df9":"code","5756e50a":"code","1dc9c8ad":"code","c106bf7a":"code","e7b28e01":"code","a622fd5c":"code","1c838771":"code","5832cebb":"code","eb6f7604":"code","dde85b6a":"code","925edc26":"code","e86988d7":"code","bf81f792":"code","c6e94bca":"code","e652901e":"code","f4cc9d85":"code","f991b244":"code","5f32781a":"code","a1b0d3f9":"code","6c6d503e":"code","a6a77eac":"code","8cf9f0a7":"code","af95158b":"code","58226900":"code","0a31c2e6":"code","001f6b63":"code","b9b04516":"code","c235d60b":"code","3564219a":"code","7232870a":"code","93451d60":"code","54795108":"markdown","0a08632f":"markdown","da60e508":"markdown","812a6ddd":"markdown","b6dfd841":"markdown","cd912c25":"markdown","c209a026":"markdown","0a988a3a":"markdown","f9874c26":"markdown","361d50f8":"markdown","ddc91b4c":"markdown","a6522eae":"markdown","6cee6cdf":"markdown","ea07632a":"markdown","a00fe977":"markdown","5f2fb461":"markdown","6e58b2d1":"markdown","a668b883":"markdown","bf8adaed":"markdown","dc459a51":"markdown","93d5012f":"markdown","ea77f234":"markdown","6758970c":"markdown"},"source":{"df089187":"from IPython.display import Image\nimport os\nImage(\"..\/input\/ensemble-learning-pic\/EL.png\")","57be5ec5":"import pandas as pd\nimport numpy as np\nimport time\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import mean_squared_error\nimport datetime\nimport warnings\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\n%matplotlib inline\nsns.set(style=\"darkgrid\")\nwarnings.filterwarnings(\"ignore\")\n","4cc0dee4":"test = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/test.csv')\nitem_categories = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/item_categories.csv')\nitems = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/items.csv')\nshops = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/shops.csv')\nsales = pd.read_csv('\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv',parse_dates=['date'],dtype={'date': 'str'})","43de4347":"# Concatenating item_categories, items, shops and sales dataframes as train\ndf = sales.join(items, on='item_id',rsuffix='_')\ndf = df.join(shops, on='shop_id', rsuffix='_')\ndf = df.join(item_categories, on='item_category_id', rsuffix='_')","5cfb6f8f":"def downcast_dtypes(df):\n    float_cols = [c for c in df if df[c].dtype == \"float64\"]\n    int_cols = [c for c in df if df[c].dtype in [\"int64\", \"int32\"]]\n    df[float_cols] = df[float_cols].astype(np.float32)\n    df[int_cols] = df[int_cols].astype(np.int16)\n    return df\n\ndf = downcast_dtypes(df)\nprint(df.info())","f4516acf":"df.head().T","e7937da4":"df.dtypes","df8b8ac6":"print('Dataframe shape :',df.shape)","8eea80be":"test_shop_ids = test['shop_id'].unique()\ntest_item_ids = test['item_id'].unique()\n# Only shops that exist in test set.\nleak_df = df[df['shop_id'].isin(test_shop_ids)]\n# Only items that exist in test set.\nleak_df = leak_df[leak_df['item_id'].isin(test_item_ids)]\nprint('Data set size before leaking:', df.shape[0])\nprint('Data set size after leaking:', leak_df.shape[0])\ndf = leak_df","0440970a":"print(df.isnull().sum())\nprint('\\nNo null records')","b022e0f4":"# We will drop all the strings (object type) and item_category_id as we will not use them.\ndf.drop(['item_name','shop_name','item_category_name','item_category_id'],axis=1,inplace=True)","476c1960":"print('Is column \\'shop_id\\' equal to \\'shop_id_\\' :',df['shop_id'].equals(df['shop_id_']),'\\n')\nprint('Is column \\'item_id\\' equal to \\'item_id_\\' :',df['item_id'].equals(df['item_id_']),'\\n')\nprint('\\nAll are same so we will drop the duplicates')\ndf.drop(['shop_id_','item_id_'],axis=1,inplace=True)","ae3337c7":"df = df[df['item_price']>0]\n# Dropped row where item_price is less than 0 ","6a1c7df9":"df = df.sort_values('date').groupby(['date_block_num', 'shop_id','item_id'], as_index=False)\ndf = df.agg({'item_price':['sum', 'mean'], 'item_cnt_day':['sum', 'mean','count']})\n# Rename features.\ndf.columns = ['date_block_num', 'shop_id', 'item_id', 'item_price', 'mitem_price', 'item_cnt', 'mitem_cnt', 'transactions']","5756e50a":"df.count()","1dc9c8ad":"df['year'] = df['date_block_num'].apply(lambda x: ((x\/\/12) + 2013))\ndf['month'] = df['date_block_num'].apply(lambda x: (x % 12))","c106bf7a":"plt.figure(figsize=(22,8))\nplt.subplot(2, 1, 1)\nsns.boxplot(x=df['item_cnt'])\nplt.subplot(2, 1, 2)\nsns.boxplot(x=df['item_price'])","e7b28e01":"df = df.query('item_cnt >= 0 and item_cnt <= 1500 and item_price < 400000')","a622fd5c":"df['cnt_m'] = df.sort_values('date_block_num').groupby(['shop_id','item_id'])['item_cnt'].shift(-1)","1c838771":"df.head()","5832cebb":"df.describe().T","eb6f7604":"corr = df.corr()\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nwith sns.axes_style(\"white\"):\n    f, ax = plt.subplots(figsize=(9, 7))\n    ax = sns.heatmap(corr,mask=mask,square=True,annot=True,fmt='0.2f',linewidths=.8,cmap=\"YlGnBu\")","dde85b6a":"fig = sns.jointplot(x='item_price',y='item_id',data=df,\n                   joint_kws={'alpha':0.2,'color':'orange'},\n                   marginal_kws={'color':'red'})","925edc26":"plt.figure(figsize=(20,6)) \nsns.countplot(df['shop_id'])","e86988d7":"plt.figure(figsize=(20,6)) \nsns.barplot(x=df['shop_id'],y=df['item_cnt'],palette='viridis')","bf81f792":"item_cat_price = df.groupby(['item_id']).sum()['item_price']\nplt.figure(figsize=(18,6))\nitem_cat_price.plot(color ='red')","c6e94bca":"ts = time.time()\nshop_ids = df['shop_id'].unique()\nitem_ids = df['item_id'].unique()\nempty_df = []\nfor i in range(34):\n    for shop in shop_ids:\n        for item in item_ids:\n            empty_df.append([i, shop, item])\n    \nempty_df = pd.DataFrame(empty_df, columns=['date_block_num','shop_id','item_id'])\nprint(time.time()-ts)","e652901e":"# Merge the train set with the complete set (missing records will be filled with 0).\ndf = pd.merge(empty_df, df, on=['date_block_num','shop_id','item_id'], how='left')\ndf.fillna(0, inplace=True)","f4cc9d85":"train_set = df.query('date_block_num >= 0 and date_block_num < 26').copy()\nvalidation_set = df.query('date_block_num >= 26 and date_block_num < 33').copy()\ntest_set = df.query('date_block_num == 33').copy()\n\nprint('Train set records:', train_set.shape[0])\nprint('Validation set records:', validation_set.shape[0])\nprint('Test set records:', test_set.shape[0])\n\nprint('Percent of train_set:',(train_set.shape[0]\/df.shape[0])*100,'%')\nprint('Percent of validation_set:',(validation_set.shape[0]\/df.shape[0])*100,'%')\nprint('Percent of test_set:',(test_set.shape[0]\/df.shape[0])*100,'%')","f991b244":"train_set.dropna(subset=['cnt_m'], inplace=True)\nvalidation_set.dropna(subset=['cnt_m'], inplace=True)","5f32781a":"# Creating training and validation sets\nx_train = train_set.drop(['cnt_m','date_block_num'],axis=1)\ny_train = train_set['cnt_m'].astype(int)\n\nx_val = validation_set.drop(['cnt_m','date_block_num'],axis=1)\ny_val = validation_set['cnt_m'].astype(int)","a1b0d3f9":"latest_records = pd.concat([train_set, validation_set]).drop_duplicates(subset=['shop_id', 'item_id'], keep='last')\nx_test = pd.merge(test, latest_records, on=['shop_id', 'item_id'], how='left', suffixes=['', '_'])\nx_test['year'] = 2015\nx_test['month'] = 9\nx_test.drop('cnt_m', axis=1, inplace=True)\nx_test = x_test[x_train.columns]","6c6d503e":"ts=time.time()\nsets = [x_train, x_val, x_test]\nfor dataset in sets:\n    for shop_id in dataset['shop_id'].unique():\n        for column in dataset.columns:\n            shop_median = dataset[(dataset['shop_id'] == shop_id)][column].median()\n            dataset.loc[(dataset[column].isnull()) & (dataset['shop_id'] == shop_id), column] = shop_median\n            \n# Fill remaining missing values on test set with mean.\nx_test.fillna(x_test.mean(), inplace=True)\nprint(time.time()-ts)","a6a77eac":"x_test.head()","8cf9f0a7":"from sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom scipy import stats","af95158b":"# These will be our base models\nm1 = LinearRegression()\nm2 = DecisionTreeRegressor()\nm3 = RandomForestRegressor(n_estimators=10)","58226900":"ts = time.time()\nfrom sklearn.ensemble import VotingRegressor\nmodel = VotingRegressor([('lr', m1), ('dt', m2),('rf', m3)])\nmodel.fit(x_train, y_train)\ntrain_pred = model.predict(x_train)\nval_pred = model.predict(x_val)\nprint('Total time taken :',time.time()-ts) ","0a31c2e6":"print('Train rmse:', np.sqrt(mean_squared_error(y_train, train_pred)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(y_val, val_pred)))","001f6b63":"perm = PermutationImportance(model, random_state=1).fit(x_val, y_val)\neli5.show_weights(perm, feature_names = x_val.columns.tolist())","b9b04516":"ts = time.time()\nm1.fit(x_train, y_train)\nm2.fit(x_train, y_train)\nm3.fit(x_train,y_train)\n\navg_train_pred1 = m1.predict(x_train)\navg_train_pred2 = m2.predict(x_train)\navg_train_pred3 = m3.predict(x_train)\n\navg_pred1 = m1.predict(x_val)\navg_pred2 = m2.predict(x_val)\navg_pred3 = m3.predict(x_val)\n\ntrain_pred_avg = (avg_train_pred1+avg_train_pred2+avg_train_pred3)\/3\nval_pred_avg = (avg_pred1+avg_pred2+avg_pred3)\/3\n\nprint('Total time taken: ',time.time()-ts)","c235d60b":"print('Train rmse:', np.sqrt(mean_squared_error(y_train, train_pred_avg)))\nprint('Validation rmse:', np.sqrt(mean_squared_error(y_val, val_pred_avg)))","3564219a":"ts = time.time()\nm1.fit(x_train, y_train)\nm2.fit(x_train, y_train)\nm3.fit(x_train,y_train)\n\nwavg_train_pred1 = m1.predict(x_train)\nwavg_train_pred2 = m2.predict(x_train)\nwavg_train_pred3 = m3.predict(x_train)\n\nprint('M1_train:',np.sqrt(mean_squared_error(y_train, wavg_train_pred1)))\nprint('M2_train:',np.sqrt(mean_squared_error(y_train, wavg_train_pred2)))\nprint('M3_train:',np.sqrt(mean_squared_error(y_train, wavg_train_pred3)))\n\nwavg_pred1 = m1.predict(x_val)\nwavg_pred2 = m2.predict(x_val)\nwavg_pred3 = m3.predict(x_val)\n\nprint('\\nM1_validation:',np.sqrt(mean_squared_error(y_val, wavg_pred1)))\nprint('M2_validation:',np.sqrt(mean_squared_error(y_val, wavg_pred2)))\nprint('M3_validation:',np.sqrt(mean_squared_error(y_val, wavg_pred3)))\n\nprint('\\nTotal time taken: ',time.time()-ts)","7232870a":"final_val_pred = 0.3 * wavg_pred1 + 0.2 * wavg_pred2 + 0.5 * wavg_pred3\nprint('Weighted Average:',np.sqrt(mean_squared_error(y_val, final_val_pred)))","93451d60":"train_set.to_csv('\/kaggle\/working\/train_set.csv',index=False)\nvalidation_set.to_csv('\/kaggle\/working\/validation_set.csv',index=False)\ntest_set.to_csv('\/kaggle\/working\/test_set.csv',index=False)","54795108":"Splitting the data into `train`, `validation` and `test` set.\n* Train set will be from `date_block_num` : 0-28 \n* Validation set will be from `date_block_num` : 29-32\n* Test set will be from `date_block_num` : 33","0a08632f":"Somewhere around `item_id`: 6000 we might have an outlier.","da60e508":"#### From the results we can notice that `Weighted Average` performs slightly better when compared to Max Voting and Averaging\n----------------------------------------------------------------------------------------------------------------------------------------------","812a6ddd":"<font color=\"chocolate\" size=+2.5><b>My Other Kernels<\/b><\/font>\n\nClick on the button to view kernel...\n\n\n<a href=\"https:\/\/www.kaggle.com\/nitindatta\/fifa-in-depth-analysis-with-linear-regression\" class=\"btn btn-success\" style=\"color:white;\">FIFA In-Depth Analysis<\/a><br><br>\n\n<a href=\"https:\/\/www.kaggle.com\/nitindatta\/storytelling-with-gwd-pre-print-data\" class=\"btn btn-success\" style=\"color:white;\">Storytelling with GWD pre_print data<\/a><br><br>\n\n<a href=\"https:\/\/www.kaggle.com\/nitindatta\/ensemble-learning-part-1\" class=\"btn btn-success\" style=\"color:white;\">Ensemble Learning Part 1<\/a><br><br>\n\n<a href=\"https:\/\/www.kaggle.com\/nitindatta\/ensemble-learning-part-2\" class=\"btn btn-success\" style=\"color:white;\">Ensemble Learning Part 2<\/a><br><br>\n\n<a href=\"https:\/\/www.kaggle.com\/nitindatta\/students-performance-in-exams-eda-in-depth\" class=\"btn btn-success\" style=\"color:white;\">Students performance in Exams- EDA in depth \ud83d\udcca\ud83d\udcc8<\/a><br><br>\n\n<a href=\"https:\/\/www.kaggle.com\/nitindatta\/pulmonary-embolism-dicom-preprocessing-eda\" class=\"btn btn-success\" style=\"color:white;\">\ud83e\ude7aPulmonary Embolism Dicom preprocessing & EDA\ud83e\ude7a<\/a><br><br>\n\n<a href=\"https:\/\/www.kaggle.com\/nitindatta\/first-kaggle-submission\" class=\"btn btn-success\" style=\"color:white;\">Titanic: Machine Learning from Disaster<\/a><br><br>\n\n<a href=\"https:\/\/www.kaggle.com\/nitindatta\/graduate-admission-chances\" class=\"btn btn-success\" style=\"color:white;\">\ud83d\udcd6 Graduate Admission Chances \ud83d\udcd5 \ud83d\udcd4<\/a><br><br>\n\n<a href=\"https:\/\/www.kaggle.com\/nitindatta\/flower-classification-augmentations-eda\" class=\"btn btn-success\" style=\"color:white;\">Flower_Classification+Augmentations+EDA<\/a><br><br>\n\n<a href=\"https:\/\/www.kaggle.com\/nitindatta\/storytelling-with-gwd-pre-print-data\" class=\"btn btn-success\" style=\"color:white;\">Storytelling with GWD pre_print data<\/a><br><br>\n\n\n### If these kernels impress you,give them an <font size=\"+2\" color=\"red\"><b>Upvote<\/b><\/font>.<br>\n\n<a href=\"#toc\" class=\"btn btn-primary\" role=\"button\" aria-pressed=\"true\" style=\"color:white\" data-toggle=\"popover\" title=\"go to Colors\">Go to TOP<\/a>","b6dfd841":"## To begin with, below is a short story on why ensemble learning is so widely used in competitions.\n \nYou need a plumber, and you find one that has 4.5 stars rating (out of 5) and charges 100 dollars to do the job. Now I offer you two plumbers, each with 4 stars rating, that charge 75 dollars apiece. My selling point to you is that they would visit one at a time, and the second plumber will fix whatever the first didn\u2019t do right. You laugh at me and take a rock-star plumber. Why would you spend 2x the time and 1.5x the money when the first plumber would probably do the job just fine?\n\nLet\u2019s start with the same premise, but now I offer you 4 plumbers. Each has a 3 star rating, so they charge 23 dollars apiece. They would come to your house together, work as a team, and fix your problem faster and for less money. You think about it for a second, because it would be nice to save 8 dollars. In the end, you decide to go with your rock-star plumber because: a) he must be good if he is charging 100 dollars; b) other 4 plumbers can\u2019t be as good or else they would be charging more. Even though you are probably right on both counts, that still doesn\u2019t guarantee you made the best choice.\n\nIn most societies there is an unwritten rule that a single expert is always better than 3 so-so experts combined. But let\u2019s see if that holds for predictions we have to make.\n\nBelow is a simple example of predicting 10 digits that are evenly split between 1 and 0.\n\n```\n1111100000    Ground truth \n1110100000    Strong learner (90%) Best at predicting 0s\n```\n\nIt seems like we have a very good model \u2013 a good expert, if you will. This model is perfect in predicting 0s, and pretty good at predicting 1s.\n\nNow we take three weak models, none of which are better than 70% in predicting digits.\n\n```\n1111100000     Ground truth\n1011110100     Weak learner (70%) Good at predicting 1s\n1101000010     Weak learner (70%) Good at predicting 0s\n0110101001     Weak learner (60%) Not good at predicting anything\n1111100000     Vote average of weak learners (100%)\n```\n\nWe take the average vote of their predictions since none of them are very good. Amazingly, we get a prediction at 100% accuracy. Is this a setup devised by yours truly in number selection, or does it actually hold in real life?\n\nIt is fairly intuitive that blending two good models will again yield a good model, and it also makes sense that the result could be better than either individual model. It is not so obvious that blending a good and a bad model could yield a better result. It is even less obvious that blending 3 bad models could yield a really good model, but that is the case.\n\nThis phenomenon is often referred to as the strength of weak learners. **This doesn\u2019t mean that combining any 3 weak learners will result in a great model**. A complementary expertise is needed. If you get 3 individuals with mediocre expertise that overlaps 95% between them, that would mean that each brings in only 5% unique knowledge compared to their union. On the other hand, 3 WEAK AND DIVERSE experts that overlap 70% in their knowledge and bring 30% of unique expertise each, are likely to blend into a good model. That is exactly the case with 3 weak learners I used in the example above: one of them is equally good\/bad at predicting everything, while the other two are good at predicting 1s and 0s, respectively.","cd912c25":"<a id=\"1\"><\/a> <br>\n# 1-Simple Ensemble Learning \n<a id=\"11\"><\/a> <br>\n### a.Max Voting\n ","c209a026":"Around `item_id`: 6000 there seems to be an outlier due to high `item_price`","0a988a3a":"<a id=\"13\"><\/a> <br>\n### c. Weighted Averaging\n\n\nIn this we will first calculate RMSE for each `base model` and then we will give higher weightage to model which has least RMSE.","f9874c26":"Data Leakages\n\nThe below code snippet is picked from [here](https:\/\/www.kaggle.com\/dimitreoliveira\/model-stacking-feature-engineering-and-eda).","361d50f8":"`item_cnt` is an important feature and plays a vital role in predicting the output.","ddc91b4c":"`item_cnt` is correlated with `transactions` and `year` is highly correlated with `date_block_num`","a6522eae":"<a id=\"12\"><\/a> <br>\n### b. Average Voting ","6cee6cdf":"### The above story\/information is picked from [here](https:\/\/www.kaggle.com\/c\/jigsaw-toxic-comment-classification-challenge\/discussion\/51058#290767).","ea07632a":"`Shop_id 31` has highest number of sales","a00fe977":"<font color=\"red\" size=5><center>Ensemble Learning -Part 1<\/center><\/font>","5f2fb461":"`Shop_id 9` has highest number of unique items","6e58b2d1":"From the above values it is clear that `Decision Tree` overfits the data.\n\nFor our weighted average the weights given will be as follows `Random Forest`:0.5, `Decision Tree`:0.2, `Linear Regression`:0.3","a668b883":" We will use the same dataframe without further processing for 'Advanced Ensemble Learning' so I will save it to csv and use it in ** 'Part-2'**","bf8adaed":"As the data consumes high memory we will downcast it.\n\n\nSource of the method: [LINK](https:\/\/www.kaggle.com\/kyakovlev\/1st-place-solution-part-1-hands-on-data)","dc459a51":"Highly skewed `item_cnt` and `item_price`.\nLet us remove any `item_cnt` above 1500 and `item_prce` above 400000","93d5012f":"There are some redundant values which we will remove later","ea77f234":"## Table of Contents\n\n1. [Simple Ensemble Learning](#1)\n\n   a. [Max Voting](#11)\n   \n   b. [Averaging](#12)\n   \n   c. [Weighted Averaging](#13) \n   \n   \n2. [Advanced Ensemble Learning Types](https:\/\/www.kaggle.com\/nitindatta\/ensemble-learning-part-2#3)\n\n    a. [Stacking](https:\/\/www.kaggle.com\/nitindatta\/ensemble-learning-part-2#31)\n    \n    b. [Blending](https:\/\/www.kaggle.com\/nitindatta\/ensemble-learning-part-2#32)\n    \n    c. [Bagging](https:\/\/www.kaggle.com\/nitindatta\/ensemble-learning-part-2#33)\n        \n    d. [Boosting](https:\/\/www.kaggle.com\/nitindatta\/ensemble-learning-part-2#34)\n      \n      * [XGBoost](https:\/\/www.kaggle.com\/nitindatta\/ensemble-learning-part-2#341)\n      \n      * [AdaBoost](https:\/\/www.kaggle.com\/nitindatta\/ensemble-learning-part-2#342)\n      \n      * [Light GBM](https:\/\/www.kaggle.com\/nitindatta\/ensemble-learning-part-2#343)\n      \n      * [Catboost](https:\/\/www.kaggle.com\/nitindatta\/ensemble-learning-part-2#344)","6758970c":"This notebook on Ensemble Learning will be divided into 2 parts.\n\n\nThis is **Part-1**\n### For Part-2 click [here](https:\/\/www.kaggle.com\/nitindatta\/ensemble-learning-part-1) \n"}}