{"cell_type":{"09f99deb":"code","6b4801e9":"code","c750355a":"code","ed169cf0":"code","538b2ecc":"code","5d918310":"code","1717aa4e":"code","56120fd4":"code","fcff3e44":"code","0c2a6725":"code","c7b72f39":"code","33e3cc59":"code","d26c6f7b":"code","687eba7e":"code","ef1a582b":"code","d51b49f9":"code","124a2bd8":"code","7680d73f":"code","49a1526c":"code","60b702ee":"code","e1c7b26f":"code","6fa9149d":"code","578bd82c":"code","c7ba6eca":"code","c20a4db2":"code","01941aa1":"code","ca08e068":"code","81ddc78b":"code","fdfd469e":"code","f4156fd5":"code","35426004":"code","da4794eb":"code","cd811273":"code","44908a03":"code","22515332":"code","1d02e43d":"code","b892b98d":"code","9495934e":"code","23db0e9f":"code","a4d09261":"code","373aaba5":"code","a26e8f11":"code","f44f4c37":"code","32a76df1":"code","1483d26e":"code","06dc9c33":"code","fa896a8f":"code","dd6a3226":"code","5976c7c7":"code","afd0285e":"code","d4f21e0c":"code","ebbadfc8":"code","ee052cdb":"code","349409d3":"code","094e23b2":"code","790f55db":"code","1ec60ce3":"code","96a6fe4a":"code","97fb66b3":"code","1a3cd873":"code","3cb95387":"code","e37e0843":"code","4ed7e2d4":"code","f2094d17":"code","41bb53d2":"code","509860f7":"code","0dc83365":"code","d4b9e5c3":"code","9d9c637f":"code","cd4f3232":"code","a156eb30":"code","b66694d5":"code","e7ac1f65":"code","bd1eaa0e":"code","3b7e1313":"code","3008c835":"code","a28ecf9b":"code","04b04113":"code","975a0196":"code","284ce3f2":"code","06fa5d49":"code","2d7bfb59":"code","8448db5f":"markdown","d9a64289":"markdown","7c35e420":"markdown","b5b47fbb":"markdown","8a85aeed":"markdown","ec9a4833":"markdown","9803da75":"markdown","51cef4fd":"markdown","e50d3b67":"markdown","605e0878":"markdown","c71744f3":"markdown","f33f3600":"markdown","bba9a17f":"markdown","d3fce40e":"markdown","d8827a11":"markdown","c30ff9b7":"markdown","53dbdf5b":"markdown","d79d9d46":"markdown","4f28dad7":"markdown","5641d9ba":"markdown","c3bfd042":"markdown","3a287039":"markdown","d937ef01":"markdown","5c486331":"markdown","60e8f860":"markdown","b0ce2d9f":"markdown","ca6bc1d4":"markdown","212273c1":"markdown"},"source":{"09f99deb":"\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set_style('whitegrid')\nplt.style.use('seaborn-deep')\nplt.style.use('fivethirtyeight')\nplt.rcParams['font.family'] = 'sans-serif'\nplt.rcParams['font.serif'] = 'Ubuntu'\nplt.rcParams['font.monospace'] = 'Ubuntu Mono'\nplt.rcParams['font.size'] = 10\nplt.rcParams['axes.labelsize'] = 12\nplt.rcParams['axes.titlesize'] = 12\nplt.rcParams['xtick.labelsize'] = 8\nplt.rcParams['ytick.labelsize'] = 8\nplt.rcParams['legend.fontsize'] = 12\nplt.rcParams['figure.titlesize'] = 14\nplt.rcParams['figure.figsize'] = (12, 8)\n\npd.options.mode.chained_assignment = None\npd.options.display.float_format = '{:.2f}'.format\npd.set_option('display.max_columns', 200)\npd.set_option('display.width', 400)\nimport warnings\nwarnings.filterwarnings('ignore')\nimport sklearn.base as skb\nimport sklearn.metrics as skm\nimport sklearn.model_selection as skms\nimport sklearn.preprocessing as skp\nimport sklearn.utils as sku\nimport sklearn.linear_model as sklm\nimport sklearn.neighbors as skn\nimport sklearn.ensemble as ske\nimport catboost as cb\nimport scipy.stats as sstats\nimport random\nseed = 12\nnp.random.seed(seed)\n\nfrom datetime import date","6b4801e9":"!pip install pandas-profiling\nimport pandas_profiling as pp","c750355a":"# important funtions\ndef datasetShape(df):\n    rows, cols = df.shape\n    print(\"The dataframe has\",rows,\"rows and\",cols,\"columns.\")\n    \n# select numerical and categorical features\ndef divideFeatures(df):\n    numerical_features = df.select_dtypes(include=[np.number])\n    categorical_features = df.select_dtypes(include=[np.object])\n    return numerical_features, categorical_features","ed169cf0":"base = '\/kaggle\/input\/jobathon-analytics-vidhya\/'\ndata_file = base + \"train.csv\"\ndf = pd.read_csv(data_file)\ndf.head()","538b2ecc":"data_file = base + \"test.csv\"\ndf_test = pd.read_csv(data_file)\ndf_test.head()","5d918310":"# set target feature\ntargetFeature='Response'","1717aa4e":"# check dataset shape\ndatasetShape(df)","56120fd4":"# remove ID from train data\ndf.drop(['ID'], inplace=True, axis=1)","fcff3e44":"# check for duplicates\nprint(df.shape)\ndf.drop_duplicates(inplace=True)\nprint(df.shape)","0c2a6725":"df.info()","c7b72f39":"df_test.info()","33e3cc59":"df.describe()","d26c6f7b":"cont_features, cat_features = divideFeatures(df)\ncat_features.head()","687eba7e":"# check target feature distribution\ndf[targetFeature].hist()\nplt.show()","ef1a582b":"# boxplots of numerical features for outlier detection\n\nfig = plt.figure(figsize=(16,16))\nfor i in range(len(cont_features.columns)):\n    fig.add_subplot(3, 3, i+1)\n    sns.boxplot(y=cont_features.iloc[:,i])\nplt.tight_layout()\nplt.show()","d51b49f9":"# distplots for categorical data\n\nfig = plt.figure(figsize=(16,20))\nfor i in range(len(cat_features.columns)):\n    fig.add_subplot(3, 3, i+1)\n    cat_features.iloc[:,i].hist()\n    plt.xlabel(cat_features.columns[i])\nplt.tight_layout()\nplt.show()","124a2bd8":"# plot missing values\n\ndef calc_missing(df):\n    missing = df.isna().sum().sort_values(ascending=False)\n    missing = missing[missing != 0]\n    missing_perc = missing\/df.shape[0]*100\n    return missing, missing_perc\n\nif df.isna().any().sum()>0:\n    missing, missing_perc = calc_missing(df)\n    missing.plot(kind='bar',figsize=(16,6))\n    plt.title('Missing Values')\n    plt.show()\nelse:\n    print(\"No missing values\")","7680d73f":"sns.pairplot(df)\nplt.show()","49a1526c":"# correlation heatmap for all features\ncorr = df.corr()\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr, mask = mask, annot=True)\nplt.show()","60b702ee":"profile = pp.ProfileReport(df, title='Pandas Profiling Report', explorative=True)\nprofile.to_file(\"profile.html\")","e1c7b26f":"profile.to_notebook_iframe()","6fa9149d":"skewed_features = cont_features.apply(lambda x: x.skew()).sort_values(ascending=False)\nskewed_features","578bd82c":"# plot missing values\n\ndef calc_missing(df):\n    missing = df.isna().sum().sort_values(ascending=False)\n    missing = missing[missing != 0]\n    missing_perc = missing\/df.shape[0]*100\n    return missing, missing_perc\n\nif df.isna().any().sum()>0:\n    missing, missing_perc = calc_missing(df)\n    missing.plot(kind='bar',figsize=(14,5))\n    plt.title('Missing Values')\n    plt.show()\nelse:\n    print(\"No Missing Values\")","c7ba6eca":"# remove all columns having no values\ndf.dropna(axis=1, how=\"all\", inplace=True)\ndf.dropna(axis=0, how=\"all\", inplace=True)\ndatasetShape(df)","c20a4db2":"# def fillNan(df, col, value):\n#     df[col].fillna(value, inplace=True)","01941aa1":"# # setting missing values to most occurring values\n# fillNan(df, 'Health Indicator', df['Health Indicator'].mode()[0])\n# fillNan(df_test, 'Health Indicator', df['Health Indicator'].mode()[0])\n# df['Health Indicator'].isna().any()","ca08e068":"# # setting missing values to most occurring values\n# # try changing with ML algo for missing\n# fillNan(df, 'Holding_Policy_Duration', df['Holding_Policy_Duration'].mode()[0])\n# fillNan(df_test, 'Holding_Policy_Duration', df['Holding_Policy_Duration'].mode()[0])\n# df['Holding_Policy_Duration'].isna().any()","81ddc78b":"# # setting missing values to most occurring values\n# # try changing with ML algo for missing\n# fillNan(df, 'Holding_Policy_Type', df['Holding_Policy_Type'].mode()[0])\n# fillNan(df_test, 'Holding_Policy_Type', df['Holding_Policy_Type'].mode()[0])\n# df['Holding_Policy_Type'].isna().any()","fdfd469e":"# # convert city code to int after removing C from it\n# df['City_Code'] = pd.to_numeric(df['City_Code'].map(lambda x:x[1:]))\n# df_test['City_Code'] = pd.to_numeric(df_test['City_Code'].map(lambda x:x[1:]))\n# df['City_Code'].head()","f4156fd5":"cont_features, cat_features = divideFeatures(df)\ncont_features.columns.tolist()","35426004":"# get all not null records for imputing\nX_impute = df[df['Health Indicator'].isna()==False]\ny_impute = X_impute.pop('Health Indicator')\n\n# remove categorical cols and targetFeature from X_impute\nX_impute = X_impute[cont_features.columns.tolist()]\nX_impute.drop(['Holding_Policy_Type', targetFeature], inplace=True, axis=1)\n\n# impute with CatBoostClassifier\nimputer_model = cb.CatBoostClassifier(random_state=seed, verbose=0)\nimputer_model.fit(X_impute, y_impute)","da4794eb":"# predict values for train section\nX_test_impute = df[df['Health Indicator'].isna()==True]\nX_test_impute = X_test_impute[X_impute.columns.tolist()]\ny_test_impute = imputer_model.predict(X_test_impute)\n\n# setting value after prediction in df\nfor i,x in enumerate(X_test_impute.index):\n    df.loc[x,'Health Indicator'] = y_test_impute[i]\n    \n# predict values for test section\nX_test_impute = df_test[df_test['Health Indicator'].isna()==True]\nX_test_impute = X_test_impute[X_impute.columns.tolist()]\ny_test_impute = imputer_model.predict(X_test_impute)\n\n# setting value after prediction in df\nfor i,x in enumerate(X_test_impute.index):\n    df_test.loc[x,'Health Indicator'] = y_test_impute[i]","cd811273":"# # convert Health Indicator to int after removing X from it\n# df['Health Indicator'] = pd.to_numeric(df['Health Indicator'].map(lambda x:x[1:]))\n# df_test['Health Indicator'] = pd.to_numeric(df_test['Health Indicator'].map(lambda x:x[1:]))\n# df['Health Indicator'].head()","44908a03":"cont_features, cat_features = divideFeatures(df)\ncont_features.columns.tolist()","22515332":"# get all not null records for imputing\nX_impute = df[df['Holding_Policy_Duration'].isna()==False]\ny_impute = X_impute.pop('Holding_Policy_Duration')\n\n# remove categorical cols and targetFeature from X_impute\nX_impute = X_impute[cont_features.columns.tolist()]\nX_impute.drop(['Holding_Policy_Type', targetFeature], inplace=True, axis=1)\n\n# impute with RandomForestClassifier\nimputer_model = cb.CatBoostClassifier(random_state=seed, verbose=0)\nimputer_model.fit(X_impute, y_impute)","1d02e43d":"# predict values for train section\nX_test_impute = df[df['Holding_Policy_Duration'].isna()==True]\nX_test_impute = X_test_impute[X_impute.columns.tolist()]\ny_test_impute = imputer_model.predict(X_test_impute)\n\n# setting value after prediction in df\nfor i,x in enumerate(X_test_impute.index):\n    df.loc[x,'Holding_Policy_Duration'] = y_test_impute[i]\n    \n# predict values for test section\nX_test_impute = df_test[df_test['Holding_Policy_Duration'].isna()==True]\nX_test_impute = X_test_impute[X_impute.columns.tolist()]\ny_test_impute = imputer_model.predict(X_test_impute)\n\n# setting value after prediction in df\nfor i,x in enumerate(X_test_impute.index):\n    df_test.loc[x,'Holding_Policy_Duration'] = y_test_impute[i]","b892b98d":"# get all not null records for imputing\nX_impute = df[df['Holding_Policy_Type'].isna()==False]\ny_impute = X_impute.pop('Holding_Policy_Type')\n\n# remove categorical cols and targetFeature from X_impute\ncols_impute = cont_features.columns.tolist()\ncols_impute.remove('Holding_Policy_Type')\nX_impute = X_impute[cols_impute]\nX_impute.drop([targetFeature], inplace=True, axis=1)\n\n# impute with RandomForestClassifier\nimputer_model = cb.CatBoostClassifier(random_state=seed, verbose=0)\nimputer_model.fit(X_impute, y_impute)","9495934e":"# predict values for train section\nX_test_impute = df[df['Holding_Policy_Type'].isna()==True]\nX_test_impute = X_test_impute[X_impute.columns.tolist()]\ny_test_impute = imputer_model.predict(X_test_impute)\n\n# setting value after prediction in df\nfor i,x in enumerate(X_test_impute.index):\n    df.loc[x,'Holding_Policy_Type'] = y_test_impute[i]\n    \n# predict values for test section\nX_test_impute = df_test[df_test['Holding_Policy_Type'].isna()==True]\nX_test_impute = X_test_impute[X_impute.columns.tolist()]\ny_test_impute = imputer_model.predict(X_test_impute)\n\n# setting value after prediction in df\nfor i,x in enumerate(X_test_impute.index):\n    df_test.loc[x,'Holding_Policy_Type'] = y_test_impute[i]","23db0e9f":"print(\"Train Missing:\",df.isna().any().sum())\nprint(\"Test Missing:\",df_test.isna().any().sum())","a4d09261":"# feature for age difference between Upper_Age and Lower_Age\ndf['age_diff'] = abs(df['Upper_Age'] - df['Lower_Age'])\ndf_test['age_diff'] = abs(df_test['Upper_Age'] - df_test['Lower_Age'])\ndf_test.head()","373aaba5":"# drop Lower_Age column as it is highly correlated with Upper_age and we also have its info in age_diff\ndf.drop('Lower_Age', axis=1, inplace=True)\ndf_test.drop('Lower_Age', axis=1, inplace=True)\ndf_test.head()","a26e8f11":"df['Holding_Policy_Duration'] = pd.to_numeric(df['Holding_Policy_Duration'].map(lambda x:'15' if x == '14+' else x))\ndf_test['Holding_Policy_Duration'] = pd.to_numeric(df_test['Holding_Policy_Duration'].map(lambda x:'15' if x == '14+' else x))\ndf_test['Holding_Policy_Duration'].head()","f44f4c37":"cont_features, cat_features = divideFeatures(df)\ncat_features","32a76df1":"# label encoding on categorical features\ndef mapFeature(data, f, data_test=None):\n    feat = data[f].unique()\n    feat_idx = [x for x in range(len(feat))]\n\n    data[f].replace(feat, feat_idx, inplace=True)\n    if data_test is not None:\n        data_test[f].replace(feat, feat_idx, inplace=True)","1483d26e":"for col in cat_features.columns:\n    mapFeature(df, col, df_test)\ndf_test.head()","06dc9c33":"# extract numerical and categorical for dummy and scaling later\ncustom_feat = ['City_Code', 'Health Indicator']\n# custom_feat = ['Health Indicator']\nfor feat in cat_features.columns:\n    if len(df[feat].unique()) > 2 and feat in custom_feat:\n        dummyVars = pd.get_dummies(df[feat], drop_first=True, prefix=feat+\"_\")\n        df = pd.concat([df, dummyVars], axis=1)\n        df.drop(feat, axis=1, inplace=True)\ndatasetShape(df)\n\ndf.head()","fa896a8f":"# extract numerical and categorical for dummy and scaling later\ncustom_feat = ['City_Code', 'Health Indicator']\n# custom_feat = ['Health Indicator']\nfor feat in cat_features.columns:\n    if len(df_test[feat].unique()) > 2 and feat in custom_feat:\n        dummyVars = pd.get_dummies(df_test[feat], drop_first=True, prefix=feat+\"_\")\n        df_test = pd.concat([df_test, dummyVars], axis=1)\n        df_test.drop(feat, axis=1, inplace=True)\ndatasetShape(df_test)\n\ndf_test.head()","dd6a3226":"# # dropping holding policy features\n# df.drop(['Holding_Policy_Duration', 'Holding_Policy_Type'], inplace=True, axis=1)\n# df_test.drop(['Holding_Policy_Duration', 'Holding_Policy_Type'], inplace=True, axis=1)","5976c7c7":"# helper functions\n\ndef log1p(vec):\n    return np.log1p(abs(vec))\n\ndef expm1(x):\n    return np.expm1(x)\n\ndef clipExp(vec):\n    return np.clip(expm1(vec), 0, None)\n\ndef printScore(y_train, y_train_pred):\n    print(skm.roc_auc_score(y_train, y_train_pred))","afd0285e":"# shuffle samples\ndf_shuffle = df.sample(frac=1, random_state=seed).reset_index(drop=True)\n\ndf_y = df_shuffle.pop(targetFeature)\ndf_X = df_shuffle\n\n# split into train dev and test\nX_train, X_test, y_train, y_test = skms.train_test_split(df_X, df_y, train_size=0.8, random_state=seed)\nprint(f\"Train set has {X_train.shape[0]} records out of {len(df_shuffle)} which is {round(X_train.shape[0]\/len(df_shuffle)*100)}%\")\nprint(f\"Test set has {X_test.shape[0]} records out of {len(df_shuffle)} which is {round(X_test.shape[0]\/len(df_shuffle)*100)}%\")","d4f21e0c":"# scaler = skp.RobustScaler()\n# scaler = skp.MinMaxScaler()\nscaler = skp.StandardScaler()\n\n# apply scaling to all numerical variables except dummy variables as they are already between 0 and 1\nX_train = pd.DataFrame(scaler.fit_transform(X_train), columns=X_train.columns)\n\n# scale test data with transform()\nX_test = pd.DataFrame(scaler.transform(X_test), columns=X_train.columns)\n\n# view sample data\nX_train.describe()","ebbadfc8":"# X_train_small = X_train.sample(frac=0.3)\n# y_train_small = y_train.iloc[X_train_small.index.tolist()]\n# X_train_small.shape","ee052cdb":"class_weights = sku.class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\nclass_weights = dict(enumerate(class_weights))\nclass_weights","349409d3":"sample_weights = sku.class_weight.compute_sample_weight('balanced', y_train)\nsample_weights","094e23b2":"knn = skn.KNeighborsClassifier(n_neighbors = 5, n_jobs=-1)\nknn.fit(X_train, y_train)\n\n# predict\ny_train_pred = knn.predict(X_train)\ny_test_pred = knn.predict(X_test)\nprint(skm.accuracy_score(y_train, y_train_pred))\nprint(skm.accuracy_score(y_test, y_test_pred))\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","790f55db":"log_model = sklm.LogisticRegression()\nlog_model.fit(X_train, y_train, sample_weight=sample_weights)\n\n# predict\ny_train_pred = log_model.predict(X_train)\ny_test_pred = log_model.predict(X_test)\nprint(skm.accuracy_score(y_train, y_train_pred))\nprint(skm.accuracy_score(y_test, y_test_pred))\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","1ec60ce3":"enet_model = sklm.ElasticNetCV(l1_ratio = [0.1, 0.5, 0.7, 0.9, 0.95, 0.99, 1],\n                    alphas = [1, 0.1, 0.01, 0.001, 0.0005], cv=10)\nenet_model.fit(X_train, y_train)\n\n# predict\ny_train_pred = enet_model.predict(X_train)\ny_test_pred = enet_model.predict(X_test)\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","96a6fe4a":"ridge_model = sklm.RidgeCV(scoring = \"neg_mean_squared_error\", \n                    alphas = [0.0001, 0.0005, 0.001, 0.005, 0.01, 0.1, 1.0, 10], cv=5\n                   )\nridge_model.fit(X_train, y_train)\n\n# predict\ny_train_pred = ridge_model.predict(X_train)\ny_test_pred = ridge_model.predict(X_test)\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","97fb66b3":"import catboost as cb\n\ncat_model = cb.CatBoostClassifier(loss_function='Logloss', verbose=0, eval_metric='AUC', class_weights=class_weights,\n                           use_best_model=True, iterations=500)\ncat_model.fit(X_train, y_train, eval_set=(X_test, y_test))\nprint(cat_model.best_score_)\n\ny_train_pred = cat_model.predict(X_train)\ny_test_pred = cat_model.predict(X_test)\nprint(skm.accuracy_score(y_train, y_train_pred))\nprint(skm.accuracy_score(y_test, y_test_pred))\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","1a3cd873":"# # Grid used\n# param_test1 = {\n#     'n_estimators': [10, 50, 100, 500],\n#     'max_depth': np.arange(2, 12, 2)\n# }\n# gb_cv1 = skms.GridSearchCV(estimator = ske.GradientBoostingClassifier(loss='deviance', random_state=seed), \n#                              param_grid = param_test1, n_jobs=-1, \n#                              cv=5, verbose=1)\n# # gb_cv1.fit(X_train_small, y_train_small)\n# gb_cv1.fit(X_train, y_train, sample_weight=sample_weights)\n# print(gb_cv1.best_params_, gb_cv1.best_score_)\n# # n_estimators = 1000\n# # max_depth = 10","3cb95387":"# # Grid used\n# param_test2 = {\n#     'min_samples_split': np.arange(2, 12, 3),\n#     'min_samples_leaf': np.arange(1, 10, 3)\n# }\n# gb_cv2 = skms.GridSearchCV(estimator = ske.GradientBoostingClassifier(loss='deviance', random_state=seed,\n#                                                                  n_estimators=50,\n#                                                                  max_depth=7), \n#                              param_grid = param_test2, n_jobs=-1, \n#                              cv=5, verbose=1)\n# gb_cv2.fit(X_train, y_train)\n# print(gb_cv2.best_params_, gb_cv2.best_score_)\n# print(gb_cv2.best_estimator_)\n# # min_samples_split = 8\n# # min_samples_leaf = 1","e37e0843":"gb_model = ske.GradientBoostingClassifier(loss='deviance', random_state=seed, verbose=0,\n                                    n_estimators=50, max_depth=7,\n                                    min_samples_leaf=1, min_samples_split=8)\ngb_model.fit(X_train, y_train, sample_weight=sample_weights)\n\n# predict\ny_train_pred = gb_model.predict(X_train)\ny_test_pred = gb_model.predict(X_test)\nprint(skm.accuracy_score(y_train, y_train_pred))\nprint(skm.accuracy_score(y_test, y_test_pred))\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","4ed7e2d4":"# # Grid used\n# param_test1 = {\n#     'n_estimators': [10, 50, 100, 500, 1000],\n#     'max_depth': np.arange(2, 12, 2)\n# }\n# extra_cv1 = skms.GridSearchCV(estimator = ske.ExtraTreesClassifier(criterion='gini', random_state=seed), \n#                              param_grid = param_test1, scoring='neg_mean_squared_error', n_jobs=-1, \n#                              cv=5, verbose=1)\n# # extra_cv1.fit(X_train_small, y_train_small)\n# extra_cv1.fit(X_train, y_train)\n# print(extra_cv1.best_params_, extra_cv1.best_score_)\n# # n_estimators = 200\n# # max_depth = 10","f2094d17":"# # Grid used\n# param_test2 = {\n#     'min_samples_split': np.arange(5, 18, 3),\n#     'min_samples_leaf': np.arange(1, 10, 2)\n# }\n# extra_cv2 = skms.GridSearchCV(estimator = ske.ExtraTreesClassifier(criterion='gini', random_state=seed,\n#                                                                  n_estimators=200,\n#                                                                  max_depth=10), \n#                               param_grid = param_test2, scoring='neg_mean_squared_error', n_jobs=-1, \n#                               cv=5, verbose=1)\n# extra_cv2.fit(X_train, y_train)\n# print(extra_cv2.best_params_, extra_cv2.best_score_)\n# print(extra_cv2.best_estimator_)\n# # min_samples_split = 5\n# # min_samples_leaf = 1","41bb53d2":"extra_model = ske.ExtraTreesClassifier(criterion='gini', random_state=1, verbose=0, n_jobs=-1,\n                              n_estimators=200,max_depth=10,\n                              min_samples_split = 5, min_samples_leaf = 1)\nextra_model.fit(X_train, y_train, sample_weight=sample_weights)\n\n# predict\ny_train_pred = extra_model.predict(X_train)\ny_test_pred = extra_model.predict(X_test)\nprint(skm.accuracy_score(y_train, y_train_pred))\nprint(skm.accuracy_score(y_test, y_test_pred))\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","509860f7":"ada_model = ske.AdaBoostClassifier(random_state=1)\nada_model.fit(X_train, y_train, sample_weight=sample_weights)\n\n# predict\ny_train_pred = ada_model.predict(X_train)\ny_test_pred = ada_model.predict(X_test)\nprint(skm.accuracy_score(y_train, y_train_pred))\nprint(skm.accuracy_score(y_test, y_test_pred))\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","0dc83365":"rf_model = ske.RandomForestClassifier(verbose=0, random_state=1, n_jobs=-1, class_weight='balanced_subsample',\n                                 n_estimators=200,max_depth=10, \n                                 min_samples_split = 7, min_samples_leaf = 1\n                                )\nrf_model.fit(X_train, y_train)\n\n# predict\ny_train_pred = rf_model.predict(X_train)\ny_test_pred = rf_model.predict(X_test)\nprint(skm.accuracy_score(y_train, y_train_pred))\nprint(skm.accuracy_score(y_test, y_test_pred))\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","d4b9e5c3":"import xgboost as xg","9d9c637f":"# # Grid used\n# param_test1 = {\n#     'max_depth': np.arange(5, 12, 2),\n#     'learning_rate': np.arange(0.04, 0.07, 0.01)\n# }\n# xgb_cv1 = skms.GridSearchCV(estimator = xg.XGBClassifier(n_estimators=100, objective='reg:squarederror', nthread=4, seed=seed), \n#                              param_grid = param_test1, scoring='neg_mean_squared_error', n_jobs=4, \n#                              iid=False, cv=5, verbose=1)\n# xgb_cv1.fit(X_train_small, y_train_small)\n# print(xgb_cv1.best_params_, xgb_cv1.best_score_)\n# # max_depth = 10\n# # learning_rate = 0.04","cd4f3232":"# param_test2 = {\n#  'subsample': np.arange(0.5, 1, 0.1),\n#  'min_child_weight': range(1, 6, 1)\n# }\n# xgb_cv2 = skms.GridSearchCV(estimator = xg.XGBClassifier(n_estimators=500, max_depth = 10, \n#                                                      objective= 'reg:squarederror', nthread=4, seed=seed), \n#                             param_grid = param_test2, scoring='neg_mean_squared_error', n_jobs=4,\n#                             cv=5, verbose=1)\n# xgb_cv2.fit(X_train_small, y_train_small)\n# print(xgb_cv2.best_params_, xgb_cv2.best_score_)\n# print(xgb_cv2.best_estimator_)\n# # subsample = 0.5\n# # min_child_weight = 2","a156eb30":"# working without scaling\nxgb_model = xg.XGBClassifier(objective ='binary:logistic', random_state=seed, verbose=0,\n                      n_estimators=500, max_depth = 10)\nxgb_model.fit(X_train, y_train)\n\n# predict\ny_train_pred = xgb_model.predict(X_train)\ny_test_pred = xgb_model.predict(X_test)\nprint(skm.accuracy_score(y_train, y_train_pred))\nprint(skm.accuracy_score(y_test, y_test_pred))\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","b66694d5":"import lightgbm as lgb\nlgb_model = lgb.LGBMClassifier(objective='binary', class_weight=class_weights, random_state=1, n_jobs=-1,\n                         n_estimators=50)\nlgb_model.fit(X_train, y_train)\n\n# predict\ny_train_pred = lgb_model.predict(X_train)\ny_test_pred = lgb_model.predict(X_test)\nprint(skm.accuracy_score(y_train, y_train_pred))\nprint(skm.accuracy_score(y_test, y_test_pred))\nprintScore(y_train, y_train_pred)\nprintScore(y_test, y_test_pred)","e7ac1f65":"import tensorflow as tf\nimport tensorflow_addons as tfa\nprint(\"TF version:-\", tf.__version__)\nimport keras as k\ntf.random.set_seed(seed)","bd1eaa0e":"THRESHOLD = .999\nbestModelPath = '.\/best_model.hdf5'\n\nclass myCallback(k.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('val_accuracy') > THRESHOLD):\n            print(\"\\n\\nStopping training as we have reached our goal.\")   \n            self.model.stop_training = True\n\nmycb = myCallback()\ncheckpoint = k.callbacks.ModelCheckpoint(filepath=bestModelPath, monitor='val_loss', verbose=1, save_best_only=True)\n\ncallbacks_list = [mycb,\n                  checkpoint\n                 ]\n            \ndef plotHistory(history):\n    print(\"Min. Validation ACC Score\",min(history.history[\"val_accuracy\"]))\n    pd.DataFrame(history.history).plot(figsize=(12,6))\n    plt.show()","3b7e1313":"epochs = 40\n\nmodel_1 = k.models.Sequential([\n    k.layers.Dense(2048, activation='relu', input_shape=(X_train.shape[1],)),\n#     k.layers.Dropout(0.3),\n    \n    k.layers.Dense(1024, activation='relu'),\n    k.layers.Dropout(0.2),\n\n    k.layers.Dense(512, activation='relu'),\n    k.layers.Dropout(0.2),\n\n    k.layers.Dense(128, activation='relu'),\n    k.layers.Dropout(0.2),\n\n    k.layers.Dense(1, activation='sigmoid'),\n])\nprint(model_1.summary())\n\nmodel_1.compile(optimizer='adam',\n              loss='binary_crossentropy',\n              metrics=[\n#                   tfa.metrics.F1Score(num_classes=1),\n                  'accuracy'\n              ]\n)\nhistory = model_1.fit(X_train, y_train, validation_data=(X_test, y_test), epochs=epochs, \n                      batch_size=2048, \n#                       class_weight=class_weights,\n                      callbacks=[callbacks_list]\n                     )\n","3008c835":"plotHistory(history)","a28ecf9b":"# y_train_pred = model_1.predict(X_train)\n# y_test_pred = model_1.predict(X_test)\n# print(skm.accuracy_score(y_train, y_train_pred))\n# print(skm.accuracy_score(y_test, y_test_pred))\n# printScore(y_train, y_train_pred)\n# printScore(y_test, y_test_pred)","04b04113":"# Generate Ensembles\n\n# def rmse_cv(model):\n#     '''\n#     Use this function to get quickly the rmse score over a cv\n#     '''\n#     rmse = np.sqrt(-skms.cross_val_score(model, X_train, y_train, \n#                                          scoring=\"neg_mean_squared_error\", cv = 5, n_jobs=-1))\n#     return rmse\n\n# class MixModel(skb.BaseEstimator, skb.RegressorMixin, skb.TransformerMixin):\n#     '''\n#     Here we will get a set of models as parameter already trained and \n#     will calculate the mean of the predictions for using each model predictions\n#     '''\n#     def __init__(self, algs):\n#         self.algs = algs\n\n#     # Define clones of parameters models\n#     def fit(self, X, y):\n#         self.algs_ = [skb.clone(x) for x in self.algs]\n        \n#         # Train cloned base models\n#         for alg in self.algs_:\n#             alg.fit(X, y)\n\n#         return self\n    \n#     # Average predictions of all cloned models\n#     def predict(self, X):\n#         predictions = np.column_stack([\n#             stacked_model.predict(X) for stacked_model in self.algs_\n#         ])\n#         return np.apply_along_axis(lambda x: np.bincount(x).argmax(), axis=1, arr=predictions)","975a0196":"# mixed_model = MixModel(algs = [\n# #     ridge_model, \n# #     enet_model, \n# #     extra_model, \n# #     cat_model,\n# #     rf_model,\n# #     xgb_model,\n# #     gb_model,\n# #     lgb_model,\n#     ada_model\n# ])\n# # score = rmse_cv(mixed_model)\n# # print(\"\\nAveraged base algs score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\n\n# mixed_model.fit(X_train, y_train)\n\n# # predict\n# y_train_pred = mixed_model.predict(X_train)\n# y_test_pred = mixed_model.predict(X_test)\n# printScore(y_train, y_train_pred)\n# printScore(y_test, y_test_pred)","284ce3f2":"def getTestResults(m=None):\n    df_final = df.sample(frac=1, random_state=1).reset_index(drop=True)\n    test_cols = [x for x in df_final.columns if targetFeature not in x]\n    df_final_test = df_test[test_cols]\n    df_y = df_final.pop(targetFeature)\n    df_X = df_final\n\n#     scaler = skp.RobustScaler()\n#     scaler = skp.MinMaxScaler()\n    scaler = skp.StandardScaler()\n\n    df_X = pd.DataFrame(scaler.fit_transform(df_X), columns=df_X.columns)\n    df_final_test = pd.DataFrame(scaler.transform(df_final_test), columns=df_X.columns)\n\n    sample_weights = sku.class_weight.compute_sample_weight('balanced', df_y)\n    \n    if m is None:\n\n#         lmr = sklm.LogisticRegression()\n#         lmr.fit(df_X, df_y)\n\n        lmr = cb.CatBoostClassifier(loss_function='Logloss', verbose=0, eval_metric='AUC', class_weights=class_weights)\n        lmr.fit(df_X, df_y)\n\n#         lmr = ske.ExtraTreesClassifier(criterion='gini', random_state=1, verbose=0, n_jobs=-1,\n#                               n_estimators=200,max_depth=10, min_samples_split = 5, min_samples_leaf = 1)\n#         lmr.fit(df_X, df_y, sample_weight=sample_weights)\n\n#         lmr = ske.AdaBoostClassifier(random_state=seed)\n#         lmr.fit(df_X, df_y, sample_weight=sample_weights)\n\n#         lmr = ske.GradientBoostingClassifier(loss='deviance', random_state=seed, verbose=0,\n#                                     n_estimators=50, max_depth=7,min_samples_leaf=1, min_samples_split=8)\n#         lmr.fit(df_X, df_y, sample_weight=sample_weights)\n\n#         lmr = ske.RandomForestClassifier(verbose=0, random_state=1, n_jobs=-1, class_weight='balanced_subsample',\n#                                  n_estimators=200,max_depth=10, min_samples_split = 7, min_samples_leaf = 1)\n#         lmr.fit(df_X, df_y)\n\n#         lmr = xg.XGBClassifier(objective ='binary:logistic', random_state=seed, verbose=0,\n#                       n_estimators=500, max_depth = 10)\n#         lmr.fit(df_X, df_y)\n\n#         lmr = lgb.LGBMClassifier(objective='binary', class_weight=class_weights, random_state=1, n_jobs=-1, n_estimators=50)\n#         lmr.fit(df_X, df_y)\n\n    else:\n        lmr = m\n\n    # predict\n    y_train_pred = lmr.predict(df_X)\n    y_test_pred = lmr.predict(df_final_test)\n    if m is not None:\n        y_train_pred = [round(y[0]) for y in y_train_pred]\n        y_test_pred = [round(y[0]) for y in y_test_pred]\n    print(skm.accuracy_score(df_y, y_train_pred))\n    printScore(df_y, y_train_pred)\n    return y_test_pred\n\n# ML models\nresults = getTestResults()\n\n# Neural Network model\n# results = getTestResults(k.models.load_model(bestModelPath))","06fa5d49":"submission = pd.DataFrame({\n    'ID': df_test['ID'],\n    targetFeature: results,\n})\nprint(submission.Response.value_counts())\nsubmission.head()","2d7bfb59":"submission.to_csv('.\/submission_Cat-robust.csv', index=False)","8448db5f":"### AdaBoost","d9a64289":"### Logistic Regression","7c35e420":"### RandomForest","b5b47fbb":"# Step 4: Data Modelling\n\n### Split Train-Test Data","8a85aeed":"## Derive Features","ec9a4833":"### CatBoost","9803da75":"### Feature Scaling","51cef4fd":"### Gradient Boosting","e50d3b67":"### Handle Missing","605e0878":"### Extra Trees","c71744f3":"### Holding_Policy_Type Missing Prediction","f33f3600":"## Create Dummy Features","bba9a17f":"# Health Insurance Lead Prediction - JOB-A-THON","d3fce40e":"### LightGBM","d8827a11":"# Step 2: EDA","c30ff9b7":"# Step 1: Reading and Understanding the Data","53dbdf5b":"### Profiling for Whole Data","d79d9d46":"### Univariate Analysis","4f28dad7":"### One-Hot Encoding","5641d9ba":"### Skewness","c3bfd042":"### KNN","3a287039":"## Model Building","d937ef01":"### Health Indicator Missing Prediction","5c486331":"### XGBoost","60e8f860":"## Deep Learning Model","b0ce2d9f":"# Test Evaluation & Submission","ca6bc1d4":"# Step 3: Data Preparation","212273c1":"### Holding_Policy_Duration Missing Prediction"}}