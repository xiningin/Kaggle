{"cell_type":{"95027df5":"code","bb0e4b38":"code","17163efe":"code","12a5572b":"code","bc0f9f76":"code","b1972d9c":"code","094ffc92":"code","9982a415":"code","0181f3cb":"code","6b1f7f49":"code","89bf8ff1":"code","52cf35bf":"code","99b41130":"code","61adbe15":"markdown","1f03938d":"markdown","5e616316":"markdown","7feb93ab":"markdown","d02191fe":"markdown","10ff1425":"markdown","0ebb3fe2":"markdown","b25901a5":"markdown","84b2e34b":"markdown","bc31b0be":"markdown","a2a69b93":"markdown","b87ac941":"markdown","f0fed295":"markdown","3ce4d1f3":"markdown","358d9284":"markdown","e0965450":"markdown"},"source":{"95027df5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bb0e4b38":"!pip install pymap3d==2.1.0\n!pip install l5kit\n","17163efe":"import matplotlib.pyplot as plt\n\nimport numpy as np\n\nfrom l5kit.data import ChunkedDataset, LocalDataManager\nfrom l5kit.dataset import EgoDataset, AgentDataset\n\nfrom l5kit.rasterization import build_rasterizer\nfrom l5kit.configs import load_config_data\nfrom l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\nfrom l5kit.geometry import transform_points\nfrom tqdm import tqdm\nfrom collections import Counter\nfrom l5kit.data import PERCEPTION_LABELS\nfrom prettytable import PrettyTable\n\nimport os\n","12a5572b":"# set env variable for data\nos.environ[\"L5KIT_DATA_FOLDER\"] = \"\/kaggle\/input\/lyft-motion-prediction-autonomous-vehicles\"\n# get config\ncfg = load_config_data(\"\/kaggle\/input\/lyftconfigfiles\/visualisation_config.yaml\")\nprint(cfg)","bc0f9f76":"print(f'current raster_param:\\n')\nfor k,v in cfg[\"raster_params\"].items():\n    print(f\"{k}:{v}\")","b1972d9c":"dm = LocalDataManager()\ndataset_path = dm.require(cfg[\"val_data_loader\"][\"key\"])\nzarr_dataset = ChunkedDataset(dataset_path)\nzarr_dataset.open()\nprint(zarr_dataset)","094ffc92":"frames = zarr_dataset.frames\ncoords = np.zeros((len(frames), 2))\nfor idx_coord, idx_data in enumerate(tqdm(range(len(frames)), desc=\"getting centroid to plot trajectory\")):\n    frame = zarr_dataset.frames[idx_data]\n    coords[idx_coord] = frame[\"ego_translation\"][:2]\n\n\nplt.scatter(coords[:, 0], coords[:, 1], marker='.')\naxes = plt.gca()\naxes.set_xlim([-2500, 1600])\naxes.set_ylim([-2500, 1600])","9982a415":"agents = zarr_dataset.agents\nprobabilities = agents[\"label_probabilities\"]\nlabels_indexes = np.argmax(probabilities, axis=1)\ncounts = []\nfor idx_label, label in enumerate(PERCEPTION_LABELS):\n    counts.append(np.sum(labels_indexes == idx_label))\n    \ntable = PrettyTable(field_names=[\"label\", \"counts\"])\nfor count, label in zip(counts, PERCEPTION_LABELS):\n    table.add_row([label, count])\nprint(table)","0181f3cb":"rast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)","6b1f7f49":"data = dataset[50]\n\nim = data[\"image\"].transpose(1, 2, 0)\nim = dataset.rasterizer.to_rgb(im)\ntarget_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\ndraw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n\nplt.imshow(im[::-1])\nplt.show()","89bf8ff1":"cfg[\"raster_params\"][\"map_type\"] = \"py_satellite\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)\ndata = dataset[50]\n\nim = data[\"image\"].transpose(1, 2, 0)\nim = dataset.rasterizer.to_rgb(im)\ntarget_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\ndraw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n\nplt.imshow(im[::-1])\nplt.show()","52cf35bf":"dataset = AgentDataset(cfg, zarr_dataset, rast)\ndata = dataset[0]\n\nim = data[\"image\"].transpose(1, 2, 0)\nim = dataset.rasterizer.to_rgb(im)\ntarget_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\ndraw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n\nplt.imshow(im[::-1])\nplt.show()","99b41130":"from IPython.display import display, clear_output\nimport PIL\n \ncfg[\"raster_params\"][\"map_type\"] = \"py_semantic\"\nrast = build_rasterizer(cfg, dm)\ndataset = EgoDataset(cfg, zarr_dataset, rast)\nscene_idx = 2\nindexes = dataset.get_scene_indices(scene_idx)\nimages = []\n\nfor idx in indexes:\n    \n    data = dataset[idx]\n    im = data[\"image\"].transpose(1, 2, 0)\n    im = dataset.rasterizer.to_rgb(im)\n    target_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n    center_in_pixels = np.asarray(cfg[\"raster_params\"][\"ego_center\"]) * cfg[\"raster_params\"][\"raster_size\"]\n    draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n    clear_output(wait=True)\n    display(PIL.Image.fromarray(im[::-1]))","61adbe15":"5. Analyse de la configuration actuelle pour des champs int\u00e9ressants :\n\n lorsqu'il est charg\u00e9 en python, le fichier yaml est converti en un dict python.\n\nraster_params contient toutes les informations relatives \u00e0 la transformation du monde 3D en plan image:\n\n    . raster_size: la taille du plan image\n    . pixel_size: combien de m\u00e8tres correspondent \u00e0 un pixel\n    . ego_center: notre raster est centr\u00e9 autour d'un agent, nous pouvons d\u00e9placer l'agent dans le plan image avec ce param\u00e8tre\n    . map_type: le rast\u00e9riseur \u00e0 utiliser. Nous prenons actuellement en charge un satellite et un s\u00e9mantique. Nous examinerons les diff\u00e9rences plus loin dans ce script\n\n","1f03938d":"9. Travailler avec l'abstraction de donn\u00e9es:\n\nM\u00eame s'il est absolument parfait de travailler avec les donn\u00e9es brutes, nous fournissons \u00e9galement des classes qui abstraits l'acc\u00e8s aux donn\u00e9es pour offrir un moyen plus simple de g\u00e9n\u00e9rer des entr\u00e9es et des cibles.\nObjets de base\n\nEn plus du rast\u00e9riseur, notre bo\u00eete \u00e0 outils contient d'autres classes que vous voudrez peut-\u00eatre utiliser lors de la cr\u00e9ation de votre solution. Le package d'ensemble de donn\u00e9es, par exemple, impl\u00e9mente d\u00e9j\u00e0 des ensembles de donn\u00e9es pr\u00eats pour PyTorch, de sorte que vous pouvez d\u00e9marrer et commencer \u00e0 coder imm\u00e9diatement.\nPackage de jeu de donn\u00e9es\n\nNous utiliserons deux classes du package de donn\u00e9es pour cet exemple. Les deux peuvent \u00eatre it\u00e9r\u00e9s et renvoyer des images multicanaux du rast\u00e9riseur avec les d\u00e9calages de trajectoires futures et d'autres informations.\n\n    EgoDataset: cet ensemble de donn\u00e9es it\u00e8re sur les annotations AV\n    AgentDataset: cet ensemble de donn\u00e9es it\u00e8re sur les annotations d'autres agents\n\nLes deux prennent en charge le multi-threading (via PyTorch DataLoader) OOB.","5e616316":"6. Chargement des donn\u00e9es:\n\nLe m\u00eame fichier de configuration est \u00e9galement utilis\u00e9 pour charger les donn\u00e9es. Chaque division des donn\u00e9es a sa propre section et plusieurs ensembles de donn\u00e9es peuvent \u00eatre utilis\u00e9s (dans leur ensemble ou en tranches). Dans ce court exemple, nous n'utiliserons que le premier ensemble de donn\u00e9es de l'ensemble d'\u00e9chantillons. Vous pouvez changer cela en configurant la variable 'train_data_loader' dans le fichier config.\n\nVous avez peut-\u00eatre \u00e9galement remarqu\u00e9 que nous construisons un objet LocalDataManager. Cela r\u00e9soudra les chemins relatifs de la configuration en utilisant la variable d'environnement L5KIT_DATA_FOLDER que nous venons de d\u00e9finir.\n","7feb93ab":"3. Chargement des packages:","d02191fe":"1. Introduction:\nLes v\u00e9hicules autonomes devraient \u00eatre capables de rouler en toute autonomie, dans des conditions de trafic r\u00e9el et sur une infrastructure non sp\u00e9cifique, sans l'intervention d'un conducteur. \nLeur bon fonctionnement d\u00e9pendra fortement du niveau de s\u00e9curit\u00e9 des \u00e9quipements, de la qualit\u00e9 des logiciels et la fiabilit\u00e9 des informations utilis\u00e9es par l\u2019intelligence embarqu\u00e9e.\n","10ff1425":"4. Configuration o\u00f9 r\u00e9sident nos donn\u00e9es:\n\nOn s'attend \u00e0 ce que les donn\u00e9es vivent dans un dossier qui peut \u00eatre configur\u00e9 \u00e0 l'aide de la variable d'environnement `L5KIT_DATA_FOLDER`. Mon dossier de donn\u00e9es devrait contenir des sous-dossiers pour les cartes a\u00e9riennes et s\u00e9mantiques ainsi que les sc\u00e8nes (fichiers `.zarr`).\nDans cet exemple, la variable env est d\u00e9finie sur le dossier de donn\u00e9es local. Je dois m\u2019assurer que le chemin pointe vers le bon emplacement .\n\nLe code est construit  pour qu'il fonctionne avec une configuration `yaml` lisible par l'homme. Ce fichier de configuration contient beaucoup d'informations utiles, cependant, nous nous concentrerons uniquement sur quelques fonctionnalit\u00e9s concernant le chargement et la visualisation ici.\n\n\n","0ebb3fe2":"3. Visualisation des trajectoires :\n\nLes packages de base pour la visualisation sont:\nRasterization :\ncontient des classes pour obtenir des donn\u00e9es visuelles sous forme de tenseurs multicanaux et les transformer en images RGB interpr\u00e9tables. Chaque classe a au moins une m\u00e9thode rasterize pour obtenir le tenseur et une m\u00e9thode to_rgb pour le convertir en image. Quelques exemples sont:\n\n BoxRasterizer: \ncet objet rend les agents (par exemple les v\u00e9hicules ou les pi\u00e9tons) sous forme de bo\u00eetes 2D orient\u00e9es\n    SatelliteRasterizer: cet objet rend un recadrage orient\u00e9 \u00e0 partir d'une carte satellite.\n\nVisualisation :\n\ncontient des utilitaires pour dessiner des informations suppl\u00e9mentaires (par exemple des trajectoires) sur des images RGB. Ces utilitaires sont couramment utilis\u00e9s apr\u00e8s un appel \u00e0 to_rgb pour ajouter d'autres informations \u00e0 la visualisation finale. Un exemple est:\n\n   . draw_trajectory: cette fonction trace des trajectoires 2D \u00e0 partir des coordonn\u00e9es et du d\u00e9calage de lacet sur une image\n","b25901a5":"8. Autre distribution plus facile des types d'agents.\n\nNous pouvons obtenir tous les agents label_probabilities et obtenir l'argmax pour chaque raw. parce que les fichiers .zarr correspondent \u00e0 un tableau numpy, nous pouvons utiliser toutes les op\u00e9rations et fonctions numpy traditionnelles.\n","84b2e34b":"7. Travailler avec les donn\u00e9es brutes\n\nLes fichiers .zarr prennent en charge la plupart des op\u00e9rations de tableau numpy traditionnelles. Dans la cellule suivante, nous parcourons les images pour obtenir un nuage de points des emplacements AV\n","bc31b0be":"11. Et si nous voullons changer le rasterizer?\n\nNous pouvons le faire facilement en cr\u00e9ant un nouveau rasterizer\n et un nouveau jeu de donn\u00e9es pour celui-ci. Dans cet exemple, nous changeons la valeur en py_satellite qui rend les bo\u00eetes sur une image a\u00e9rienne.\n\n","a2a69b93":"2. Installation de:\nL5kit et \npymap3d pour Python 3-D coordinate conversions ","b87ac941":"12. Et si nous voullons visualiser un agent?\n\nHeureux que vous ayez demand\u00e9! Nous pouvons simplement remplacer l'EgoDataset par un AgentDataset. Maintenant, nous it\u00e9rons sur les agents et non plus sur l'AV, et le premier se trouve \u00eatre la voiture de vitesse (vous verrez celle-ci beaucoup dans le jeu de donn\u00e9es).\n\n","f0fed295":"10 .Que faire sinous voullons visualiser le v\u00e9hicule autonome (AV)?\n\nPrenons un \u00e9chantillon du jeu de donn\u00e9es et utilisons notre rast\u00e9riseur pour obtenir une image RGB que nous pouvons tracer.\n\nSi nous voulons tracer la trajectoire de v\u00e9rit\u00e9 terrain, nous pouvons convertir la target_position du jeu de donn\u00e9es (d\u00e9placements en m\u00e8tres en coordonn\u00e9es mondiales) en coordonn\u00e9es de pixels dans l'espace image, et appeler notre fonction utilitaire draw_trajectory (notez que vous pouvez utiliser cette fonction pour les trajectoires pr\u00e9vues , ainsi que).\n\n\n\n","3ce4d1f3":"13. Origine et orientation du syst\u00e8me:\n\n\u00c0 ce stade, vous avez peut-\u00eatre remarqu\u00e9 que nous retournons l'image sur l'axe Y avant de la tracer.\n\nLors du passage de la 3D \u00e0 la 2D, nous nous en tenons \u00e0 un syst\u00e8me de droite, o\u00f9 l'origine est dans le coin inf\u00e9rieur gauche avec des valeurs x positives allant \u00e0 droite et des valeurs y positives remontant le plan de l'image. La cam\u00e9ra est orient\u00e9e vers le bas de l'axe z n\u00e9gatif.\n\nCependant, opencv et pyplot placent l'origine dans le coin sup\u00e9rieur gauche avec x positif allant \u00e0 droite et positif y descendant dans le plan image. La cam\u00e9ra est tourn\u00e9e vers le bas de l'axe z positif.\n\nLe retournement effectu\u00e9 sur l'image r\u00e9sultante est \u00e0 des fins de visualisation pour tenir compte de la diff\u00e9rence entre les deux cadres de coordonn\u00e9es.\n\nDe plus, toutes nos rotations sont dans le sens anti-horaire pour une valeur positive de l'angle.\n\u00c0 quoi ressemble une sc\u00e8ne enti\u00e8re?\n\nIl est facile de visualiser une sc\u00e8ne individuelle \u00e0 l'aide de notre bo\u00eete \u00e0 outils. EgoDataset et AgentDataset fournissent deux m\u00e9thodes pour obtenir des indices int\u00e9ressants:\n\n    get_frame_indices renvoie les indices pour une image donn\u00e9e. Pour l'EgoDataset, cela correspond \u00e0 une seule observation, alors que plus d'un index pourrait \u00eatre disponible pour le AgentDataset, car cette trame donn\u00e9e peut contenir plus d'un agent valide\n    get_scene_indices renvoie les indices pour une sc\u00e8ne donn\u00e9e. Pour les deux ensembles de donn\u00e9es, ceux-ci peuvent renvoyer plusieurs index\n\nDans cet exemple, nous visualisons la deuxi\u00e8me sc\u00e8ne du point de vue de l'ego:","358d9284":"![image.png](attachment:image.png)","e0965450":"Cette image est une :\"Semantic Maps for Autonomous Vehicles\""}}