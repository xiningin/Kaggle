{"cell_type":{"635ef9cc":"code","eb4ba60b":"code","9012fc02":"code","048fc8fd":"code","8579eaa8":"code","af792517":"code","42f235db":"code","867039bd":"code","ef864e00":"code","3cf4f6ce":"code","b42aacda":"code","335fff5f":"code","3a020110":"code","b63634ee":"code","aa9eb14e":"code","114df668":"code","b11f2ebf":"markdown","2f9305e6":"markdown","9cb5a91d":"markdown","375643f1":"markdown","39bca885":"markdown"},"source":{"635ef9cc":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","eb4ba60b":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom math import sqrt, log, inf\nfrom sklearn.metrics import f1_score, accuracy_score\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","9012fc02":"# computes the boundary between two groups using their means and stds using quadratic equation.\ndef bounder(m1, s1, m2, s2):\n    if s1 == s2:\n        return (m1 + m2) \/ 2\n\n    a = s2 ** 2 - s1 ** 2\n    b = m1 * s2 ** 2 - m2 * s1 ** 2\n    c = (m1 * s2) ** 2 - (m2 * s1) ** 2 - s1 ** 2 * s1 ** 2 * log(s1 \/ s2)\n\n    d = b ** 2 - a * c\n\n    if d < 0:\n        print(\"d < 0!\")\n        return (m1 + m2) \/ 2\n\n    x = b + sqrt(d)\n    x \/= a\n\n    if not m1 < x < m2:\n        x = b - sqrt(d)\n        x \/= a\n\n    return x\n\nclass Sampler:\n    train = None\n    test = None\n    _x = [0, 50, 60, 100, 150, 200, 250, 300, 350, 400, 450, 500]\n    _y = [500, 510, 520, 530, 540, 550, 560, 570, 580, 590, 600, 650, 700]\n    train_borders = None\n    test_borders = None\n    case_mapping = {0: 0, 1: 4, 2: 6, 3: 0, 4: 3, 5: 5, 6: 6, 7: 5, 8: 0, 9: 4, 10: 0, 11: 0}\n    train_cases = None\n    open_channels_sample = None\n    stats = None\n    sample_cases = None\n    \n    def __init__(self, train, test=None, case_mapping=None):\n        self.train = train\n        if test is None:\n            self.test = train\n            self._y = self._x\n            self.case_mapping = {0: 0, 1: 1, 2: 2, 3: 3, 4: 4, 5: 5, 6: 6, 7: 3, 8: 4, 9: 6, 10: 5}\n        else:\n            self.test = test\n\n        if case_mapping is not None:\n            self.case_mapping = case_mapping\n\n        self.refresh_borders()\n        self.train['open_channels'] = self.train['open_channels'].astype('int64')\n\n    def refresh_borders(self):\n        self.train_borders = [(self._x[i], self._x[i + 1]) for i in range(len(self._x) - 1)]\n        self.test_borders = [(self._y[i], self._y[i + 1]) for i in range(len(self._y) - 1)]\n    \n    def collect_stats(self):\n        self.stats = {}\n        # stats structure: { case index -> {target value -> {'mean': x, 'std': y, 'count': z} } }\n        for num, (left, right) in enumerate(self.train_borders):\n            self.stats[num] = {}\n            temp_df = self.train[(self.train.time > left) & (self.train.time <= right)]\n            grp_df = temp_df.groupby(['open_channels'])['signal'].agg(['mean', 'std'])\n            for indx in grp_df.index:\n                self.stats[num][indx] = {'mean': grp_df.loc[indx, 'mean'],\n                                         'std': grp_df.loc[indx, 'std']}\n    \n    # computes \u0435\u0440\u0443 array of borders for each case of data.\n    def compute_bounds(self):\n        self.collect_stats()\n\n        self.train_cases = {}\n        for case_num in self.stats:\n\n            raw_case = self.stats[case_num]\n\n            target_values = [target_value for target_value in raw_case]\n            target_values.sort()\n            case = [(raw_case[target_value]['mean'],\n                     raw_case[target_value]['std']) for target_value in target_values]\n\n            bounds = []\n            for params, next_params in zip(case[:-1], case[1:]):\n                bound = bounder(*params, *next_params)\n                bounds.append(bound)\n\n            bounds = [-inf] + bounds + [inf]\n\n            self.train_cases[case_num] = {'bounds': bounds, 'values': target_values}\n\n    # Computes prediction for test data using existing borders of train data and case_mapping.\n    def compute_samples(self):\n        result = []\n        self.sample_cases = {}\n        for num, (left, right) in enumerate(self.test_borders):\n            temp_df = self.test[(self.test.time > left) & (self.test.time <= right)]\n            signal = list(temp_df.signal)\n            predicted_part = -np.ones(len(temp_df))\n\n            case = self.train_cases[self.case_mapping[num]]\n            bounds = case['bounds']\n            target = case['values']\n\n            for i, signal_value in enumerate(signal):\n                for target_value, bound, next_bound in zip(target, bounds[:-1], bounds[1:]):\n                    if bound <= signal_value <= next_bound:\n                        predicted_part[i] = target_value\n\n            assert -1 not in predicted_part\n\n            result.extend(predicted_part)\n            self.sample_cases[num] = np.array(predicted_part)\n\n        self.open_channels_sample = np.array(result)\n\n        return self.open_channels_sample","048fc8fd":"scatter_figsize = (11, 4)\nhist_figsize = (10, 3)\nmarkersize = 1\ncmap = plt.get_cmap('tab10')\nadd_color = np.array([[0., 0.56, 0.45, 1.]])\ncolors = [[color] for color in cmap(range(10))]\ncolors.append(add_color)\nmy_colors = np.array(colors)\n\n\ndef scatter_colored_by_target(df, col_name='signal', show=True,\n                              colors=my_colors, title=None):\n    plt.rcParams['figure.figsize'] = scatter_figsize\n\n    target_array = np.sort(df.open_channels.unique())\n    for target_value in target_array:\n        color = colors[target_value]\n        _df = df[df.open_channels == target_value]\n        plt.scatter(_df.time, _df[col_name], c=color, label=target_value, s=markersize)\n    if title is not None:\n        plt.title(title)\n    plt.xlabel('time')\n    plt.ylabel('signal')\n    plt.legend(title='open channels', loc='upper right',\n               bbox_to_anchor=(1.15, 1))\n    if show:\n        plt.show()\n\n\ndef hist_colored_by_target(df, col_name='signal', show=True,\n                           colors=my_colors, title=None):\n    plt.rcParams['figure.figsize'] = hist_figsize\n\n    target_array = np.sort(df.open_channels.unique())\n    for target_value in target_array:\n        color = colors[target_value]\n        plt.hist(df[df.open_channels == target_value][col_name],\n                 color=color, label=target_value)\n\n    plt.legend(title='open channels', loc='upper right',\n               bbox_to_anchor=(1.2, 1))\n    if title is not None:\n        plt.title(title)\n    if show:\n        plt.show()\n","8579eaa8":"base = os.path.abspath('\/kaggle\/input\/clean-kalman\/clean_kalman\/')\ntrain = pd.read_csv(os.path.join(base + '\/train_clean_kalman.csv'))\ntest = pd.read_csv(os.path.join(base + '\/test_clean_kalman.csv'))","af792517":"train_model = Sampler(train)\ntrain_model.compute_bounds()\ntrain_y = train_model.compute_samples()","42f235db":"accuracy_score(train.open_channels, train_y)","867039bd":"f1_score(train.open_channels, train_y, average = 'macro')","ef864e00":"train['y'] = train_y\n\nfor num, (left, right) in enumerate(train_model.train_borders):\n    temp_df = train[(train.time > left) & (train.time <= right)]\n    temp_y = temp_df.y\n    \n    local_accuracy = accuracy_score(temp_df.open_channels, temp_y)\n    local_f1 = f1_score(temp_df.open_channels, temp_y, average = 'macro')\n    \n    print(f''' On case {num}: accuracy={local_accuracy}, f1_score={local_f1}''')","3cf4f6ce":"case_num = 4\nleft, right = train_model.train_borders[case_num]\n\ntemp_df = train[(train.time > left) & (train.time <= right)]\n\ntitle = f'Case {case_num}. For time in [{left}, {right}).' \nscatter_colored_by_target(temp_df, title=title, show=False)\nfor x in train_model.train_cases[case_num]['bounds'][1:-1]:\n    plt.axhline(x, c='k', linewidth=3)\nplt.show()\n\nhist_colored_by_target(temp_df, title=title, show=False)\nfor x in train_model.train_cases[case_num]['bounds'][1:-1]:\n    plt.axvline(x, c='k', linewidth=3)\nplt.show()","b42aacda":"model = Sampler(train, test)\nmodel.compute_bounds()\ny = model.compute_samples()\n\nsample_sub = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/sample_submission.csv')\nsample_sub['open_channels'] = np.array(y).astype('int64')\nsample_sub.to_csv('submission_0.csv', index=False, float_format='%.4f')","335fff5f":"from sklearn.ensemble import RandomForestClassifier\ndef fe(_df, sampler, y, name='y'):\n    df = _df.copy()\n    \n    for num, (left, right) in enumerate(sampler.test_borders):\n        temp_df = df[(df.time > left) & (df.time <= right)]\n        temp_df['batch'] = num\n        df.update(temp_df['batch'] )\n    \n    df[name] = y\n    df[f'prev_{name}'] = [0] + list(df[name][:-1])\n    df[f'next_{name}'] = list(df[name][1:]) + [0]\n    df[f'{name}_neighborhood_equal']  = (df[f'prev_{name}'] == df[f'next_{name}']).astype('int64')\n    \n    return df\n\nnew_train = fe(train, train_model, train_y)\ncols = ['signal', 'y', 'prev_y', 'next_y', 'y_neighborhood_equal']\nX = new_train[cols].values\nopen_channels = new_train.open_channels\n\nnew_test = fe(test, model, y)\ntest_X = new_test[cols].values\n\n\nnew_train.head()","3a020110":"# RF hyperparameters taken from this notebook https:\/\/www.kaggle.com\/sggpls\/shifted-rfc-pipeline\nclf = RandomForestClassifier(n_estimators=150, max_depth=19,  \n                             random_state=42, n_jobs=10, verbose=2)\n\nclf.fit(X, open_channels)","b63634ee":"pred_train_y = clf.predict(X)\naccuracy = accuracy_score(train.open_channels, pred_train_y)\nf1 = f1_score(train.open_channels, pred_train_y, average = 'macro')\nprint(f'accuracy={accuracy}, f1_score={f1}')","aa9eb14e":"pred_y = clf.predict(test_X)\nsample_sub = pd.read_csv('\/kaggle\/input\/liverpool-ion-switching\/sample_submission.csv')\nsample_sub['open_channels'] = np.array(pred_y).astype('int64')\nsample_sub.to_csv('submission.csv', index=False, float_format='%.4f')","114df668":"f_imp = pd.DataFrame({'feature': cols, \n                      'importance': clf.feature_importances_}\n                    ).sort_values('importance', ascending = False).reset_index()\nf_imp['importance_normalized'] = f_imp['importance'] \/ f_imp['importance'].sum()\nplt.figure()\nax = plt.subplot()\nax.barh(list(reversed(list(f_imp.index[:15]))), \n        f_imp['importance_normalized'].head(15), \n        align = 'center', edgecolor = 'k')\nax.set_yticks(list(reversed(list(f_imp.index[:15]))))\nax.set_yticklabels(f_imp['feature'].head(15))\nplt.show()","b11f2ebf":"Vizualization tools:","2f9305e6":"### Let's see how it works on train data.","9cb5a91d":"## Reference\n* [A signal processing approach - Kalman Filtering](https:\/\/www.kaggle.com\/teejmahal20\/a-signal-processing-approach-kalman-filtering?scriptVersionId=30703319)\n* [clean_kalman](https:\/\/www.kaggle.com\/ragnar123\/clean-kalman)\n* [SHIFTED-RFC Pipeline](https:\/\/www.kaggle.com\/sggpls\/shifted-rfc-pipeline)","375643f1":"### Let's improve the result, using random forest classifier.","39bca885":"### Predict for test data and submit"}}