{"cell_type":{"97540f28":"code","171ff874":"code","667ba5af":"code","ad512619":"code","199ce449":"code","f3d7ce99":"code","3d2fdf89":"code","88a8cd5f":"code","7e1c6425":"code","14f0a599":"code","4e1c1719":"code","2b2e8740":"code","e8817c61":"code","d9814c9f":"code","37c61da0":"code","6e386ccf":"code","c1777077":"code","e0c77e63":"code","5425d320":"code","49716a34":"code","33c14bfa":"code","386297f7":"markdown","c115cc1f":"markdown","8a43e0eb":"markdown","9233c142":"markdown","da8e24d3":"markdown","e8c1f018":"markdown","dd58b560":"markdown","902aaaef":"markdown","1f515f37":"markdown","a29ccb34":"markdown","b6df1ad8":"markdown"},"source":{"97540f28":"import os\nimport math\nfrom functools import partial\n\nimport numpy as np\nimport pandas as pd\nimport torch\nimport re\nimport nltk\nimport torch.nn as nn\nimport pytorch_lightning as pl\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import TweetTokenizer\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import BertTokenizer, BertForSequenceClassification, BertConfig\nfrom transformers import DataCollatorWithPadding, AdamW, get_scheduler, set_seed, get_linear_schedule_with_warmup\nfrom datasets import load_metric, Dataset\nfrom sklearn.model_selection import StratifiedKFold\nfrom tqdm.auto import tqdm","171ff874":"os.environ[\"TOKENIZERS_PARALLELISM\"] = \"false\"","667ba5af":"class MODEL_EVAL_METRIC:\n    accuracy = \"accuracy\"\n    f1_score = \"f1_score\"\n\nclass Config:\n    MODEL_SAVE_DIR=\".\/hf_results\/\"\n    MAX_LENGTH=512\n    GRADIENT_ACCUMULATION_STEPS = 1\n    TWEET_COL = \"processed_text\"\n    RANDOM_STATE = 42\n    BATCH_SIZE = 32\n    OUT_SIZE = 2\n    NUM_FOLDS = 5\n    NUM_EPOCHS = 3\n    NUM_WORKERS = 2\n    TRANSFORMER_CHECKPOINT = \"bert-base-uncased\"\n    # The hidden_size of the output of the last layer of the transformer model used\n    TRANSFORMER_OUT_SIZE = 768\n    PAD_TOKEN_ID = 0\n    DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    MODEL_EVAL_METRIC = MODEL_EVAL_METRIC.accuracy\n    FAST_DEV_RUN = False    \n    PATIENCE = 5        \n    # model hyperparameters\n    MODEL_HPARAMS = {\n        \"learning_rate\": 2e-5,\n        \"adam_epsilon\": 1e-8,\n        \"weight_decay\": 0.0,\n        \"warmup_steps\": 0\n    }\n\nDATA_PATH = \"\/kaggle\/input\/nlp-getting-started\/\"\n\n# For results reproducibility \n# sets seeds for numpy, torch, python.random and PYTHONHASHSEED.\nset_seed(Config.RANDOM_STATE)","ad512619":"df_train = pd.read_csv(DATA_PATH + 'train.csv')\ndf_test = pd.read_csv(DATA_PATH + 'test.csv')\nprint(f\"Rows in train.csv = {len(df_train)}\")\nprint(f\"Rows in test.csv = {len(df_test)}\")\npd.set_option('display.max_colwidth', None)\ndf_train.head()","199ce449":"def strat_kfold_dataframe(df, num_folds=5):\n    # we create a new column called kfold and fill it with -1\n    df[\"kfold\"] = -1\n    # randomize of shuffle the rows of dataframe before splitting is done\n    df.sample(frac=1, random_state=Config.RANDOM_STATE).reset_index(drop=True)\n    y = df[\"target\"].values\n    skf = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=Config.RANDOM_STATE)\n    # stratification is done on the basis of y labels, a placeholder for X is sufficient\n    for fold, (train_idx, val_idx) in enumerate(skf.split(X=df, y=y)):\n        df.loc[val_idx, \"kfold\"] = fold\n    return df\n\ndf_train = strat_kfold_dataframe(df_train, num_folds=Config.NUM_FOLDS)            ","f3d7ce99":"nltk.download('stopwords')\nnltk.download('wordnet')","3d2fdf89":"punct = \"\/-'?!.,#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~`\" + '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'\ndef clean_special_chars(text, punct):\n    for p in punct:\n        text = text.replace(p, ' ')\n    return text\n\ndef process_tweet(df, text, keyword):\n    lemmatizer = WordNetLemmatizer()    \n    tokenizer = TweetTokenizer(preserve_case=False, strip_handles=True, reduce_len=True)    \n    processed_text = []\n    stop = stopwords.words(\"english\")\n    for tweet, keyword in zip(df[text], df[keyword]):\n        tweets_clean = []        \n        # remove stock market tickers like $GE        \n        tweet = re.sub(r'\\$\\w*', '', tweet)\n        # remove old style retweet text \"RT\"\n        tweet = re.sub(r'^RT[\\s]+', '', tweet)\n        # remove hyperlinks\n        tweet = re.sub(r'http\\S+', '', tweet)\n        # remove hashtags\n        # only removing the hash #, @, ... sign from the word\n        tweet = re.sub(r'\\.{3}|@|#', '', tweet)    \n        tweet = clean_special_chars(tweet, punct)\n        # remove junk characters which don't have an ascii code\n        tweet = tweet.encode(\"ascii\", \"ignore\").decode(\"utf-8\", \"ignore\")\n        # tokenize tweets        \n        tweet_tokens = tokenizer.tokenize(tweet)\n        for word in tweet_tokens:\n            # remove stopwords and punctuation\n            #if (word.isalpha() and len(word) > 2 and word not in stop and word not in string.punctuation):\n                #stem_word = stemmer.stem(word)  # stemming word            \n                #lem_word = lemmatizer.lemmatize(word)\n                #tweets_clean.append(lem_word) \n                tweets_clean.append(word)\n        processed_text.append(\" \".join(tweets_clean))        \n    df['processed_text'] = np.array(processed_text)","88a8cd5f":"# Fill in missing values\ndf_train[\"keyword\"] = df_train[\"keyword\"].fillna(\"no_keyword\")\ndf_test[\"keyword\"] = df_test[\"keyword\"].fillna(\"no_keyword\")\nprocess_tweet(df_train, 'text', \"keyword\")\nprocess_tweet(df_test, 'text', \"keyword\")\n# length of the processed tweet\ndf_train[\"prcsd_tweet_len\"] = df_train[\"processed_text\"].apply(lambda row: len(row.split()))\ndf_test[\"prcsd_tweet_len\"] = df_test[\"processed_text\"].apply(lambda row: len(row.split()))\ndf_train.iloc[50:52, :]","7e1c6425":"tokenizer = BertTokenizer.from_pretrained(Config.TRANSFORMER_CHECKPOINT)\n# DataCollatorWithPadding pads each batch to the longest sequence length\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","14f0a599":"# Process each row of the huggingface Dataset \ndef tokenize_tweets(tokenizer, with_labels, row):\n    result = tokenizer(row[Config.TWEET_COL], padding=False, truncation=True)\n    if with_labels:\n        result[\"labels\"] = row[\"target\"]\n    return result\n\npreprocess_train_data = partial(tokenize_tweets, tokenizer, True)  \npreprocess_test_data = partial(tokenize_tweets, tokenizer, False)  ","4e1c1719":"def get_fold_dls(fold, df):\n    train_df = df[df.kfold != fold].reset_index(drop=True)\n    valid_df = df[df.kfold == fold].reset_index(drop=True)\n    ds_train_raw = Dataset.from_pandas(train_df)\n    ds_valid_raw = Dataset.from_pandas(valid_df)\n    raw_ds_col_names = ds_train_raw.column_names    \n    ds_train = ds_train_raw.map(preprocess_train_data, batched=True, remove_columns=raw_ds_col_names)\n    ds_valid = ds_valid_raw.map(preprocess_train_data, batched=True, remove_columns=raw_ds_col_names)\n    dl_train = DataLoader(ds_train, batch_size=Config.BATCH_SIZE, shuffle=True, collate_fn=data_collator, num_workers=Config.NUM_WORKERS)\n    dl_valid = DataLoader(ds_valid, batch_size=Config.BATCH_SIZE, collate_fn=data_collator, num_workers=Config.NUM_WORKERS)\n    return dl_train, dl_valid, ds_train, ds_valid","2b2e8740":"# Optimizer\n# Split weights in two groups, one with weight decay and the other not.\ndef get_optimizer(model):\n    no_decay = [\"bias\", \"LayerNorm.weight\"]\n    optimizer_grouped_parameters = [\n        {\n            \"params\": [p for n, p in model.named_parameters() if not any(nd in n for nd in no_decay)],\n            \"weight_decay\": Config.MODEL_HPARAMS[\"weight_decay\"],\n        },\n        {\n            \"params\": [p for n, p in model.named_parameters() if any(nd in n for nd in no_decay)],\n            \"weight_decay\": 0.0,\n        },\n    ]\n    return AdamW(optimizer_grouped_parameters, lr=Config.MODEL_HPARAMS[\"learning_rate\"])","e8817c61":"# lr_scheduler = get_scheduler(\n#         name=\"linear\",\n#         optimizer=optimizer,\n#         num_warmup_steps=Config.MODEL_HPARAMS[\"warmup_steps\"],\n#         num_training_steps=num_train_steps\n#     )\n\ndef get_lr_scheduler(optimizer, dl_train):\n    # Scheduler and math around the number of training steps.\n    num_update_steps_per_epoch = math.ceil(len(dl_train) \/ Config.GRADIENT_ACCUMULATION_STEPS)\n    num_train_steps = Config.NUM_EPOCHS * num_update_steps_per_epoch\n    print(f\"num_update_steps_per_epoch = {num_update_steps_per_epoch}\")\n    print(f\"num_train_steps = {num_train_steps}\")\n    lr_scheduler = get_linear_schedule_with_warmup(\n            optimizer=optimizer,\n            num_warmup_steps=Config.MODEL_HPARAMS[\"warmup_steps\"],\n            num_training_steps=num_train_steps,\n        )\n    return lr_scheduler        ","d9814c9f":"def train_fn(epoch, model, optimizer, lr_scheduler, train_dataloader):\n    progress_bar = tqdm(range(len(train_dataloader)))\n    train_loss_epoch = []\n    model.train()\n    for step, batch in enumerate(train_dataloader):\n        batch = {k: v.to(Config.DEVICE) for k, v in batch.items()}\n        outputs = model(**batch)\n        loss = outputs.loss      \n        train_loss_epoch.append(loss.item())  \n        loss.backward()        \n        optimizer.step()\n        lr_scheduler.step()\n        optimizer.zero_grad()\n        progress_bar.update(1)        \n    train_loss_mean = np.mean(train_loss_epoch)\n    return train_loss_mean    ","37c61da0":"def eval_fn(epoch, model, val_dataloader, val_metric):\n    progress_bar = tqdm(range(len(val_dataloader)))\n    val_loss_epoch = []\n    model.eval()    \n    with torch.no_grad():\n        for step, batch in enumerate(val_dataloader):    \n            batch = {k: v.to(Config.DEVICE) for k, v in batch.items()}           \n            outputs = model(**batch)\n            val_loss_epoch.append(outputs.loss.item())\n            predictions = outputs.logits.argmax(dim=-1) \n            val_metric.add_batch(predictions=predictions, references=batch[\"labels\"])\n            progress_bar.update(1)\n\n    val_metric_epoch = val_metric.compute()\n    val_metric_epoch = round(val_metric_epoch['accuracy'], 4)\n    val_loss_mean = np.mean(val_loss_epoch)    \n    return val_loss_mean, val_metric_epoch","6e386ccf":"def fold_train_evaluate(fold):    \n    fold_best_model_path = \"\"\n    fold_str = f\"fold_{fold}\"\n    print(f\"Running fold {fold}\")\n    # we use validation loss as the criteria to save best model for a CV fold\n    fold_val_loss_min = np.inf\n    dl_train, dl_valid, ds_train, ds_valid = get_fold_dls(fold, df_train)\n    print(f\"Created data loaders for {fold_str}\")\n    config = BertConfig.from_pretrained(Config.TRANSFORMER_CHECKPOINT, num_labels=Config.OUT_SIZE)\n    model = BertForSequenceClassification.from_pretrained(Config.TRANSFORMER_CHECKPOINT,config=config)\n    model.to(Config.DEVICE)  \n    optimizer = get_optimizer(model)\n    lr_scheduler = get_lr_scheduler(optimizer, dl_train)\n    val_metric = load_metric(Config.MODEL_EVAL_METRIC)    \n    for epoch in range(Config.NUM_EPOCHS):\n        print(f\"Running training for epoch {epoch+1}\")\n        epoch_train_loss = train_fn(epoch+1, model, optimizer, lr_scheduler, dl_train)        \n        print(f\"Running validation for epoch {epoch+1}\")\n        epoch_val_loss, epoch_val_metric = eval_fn(epoch+1, model, dl_valid, val_metric)\n        print(f\"EPOCH {epoch+1}: \")\n        print(f\"train_loss: {round(epoch_train_loss, 4)}\")            \n        print(f\"val_loss: {round(epoch_val_loss, 4)}\")\n        print(f\"{Config.MODEL_EVAL_METRIC}: {epoch_val_metric}\")\n        if epoch_val_loss < fold_val_loss_min:\n            print(f\"Validation loss decreased from \" +\n                  f\"{round(fold_val_loss_min, 6)} --> {round(epoch_val_loss, 6)}. Saving model...\")\n            fold_best_model_path = Config.MODEL_SAVE_DIR + fold_str                  \n            model.save_pretrained(fold_best_model_path)\n            fold_val_loss_min = epoch_val_loss            \n\n    del optimizer, lr_scheduler, model   \n    return fold_val_loss_min, fold_best_model_path      ","c1777077":"fold_results = []\nfor fold in range(Config.NUM_FOLDS):\n    fold_val_loss_min, fold_best_model_path = fold_train_evaluate(fold)\n    fold_results.append((fold_val_loss_min, fold_best_model_path))    ","e0c77e63":"# Load the best performing model \nfold_results_sorted = sorted(fold_results, key=lambda x:x[0])\nbest_model_across_folds = fold_results_sorted[0][1]\nbest_model = BertForSequenceClassification.from_pretrained(best_model_across_folds)","5425d320":"# Create data loader for test data\nds_test_raw = Dataset.from_pandas(df_test)\nds_test_raw_col_names = ds_test_raw.column_names\nds_test = ds_test_raw.map(preprocess_test_data, batched=True, remove_columns=ds_test_raw_col_names)\ndl_test = DataLoader(ds_test, batch_size=Config.BATCH_SIZE, collate_fn=data_collator, num_workers=Config.NUM_WORKERS)","49716a34":"# perform predictions on test data\ntest_preds = []\nbest_model.to(Config.DEVICE)\nwith torch.no_grad():\n    for step, batch in tqdm(enumerate(dl_test)):\n        batch = {k: v.to(Config.DEVICE) for k, v in batch.items()}           \n        outputs = best_model(**batch)\n        predictions = outputs.logits.argmax(dim=-1)         \n        test_preds.extend(predictions.cpu().detach().numpy().tolist())\n\nprint(f\"Completed prediction for {len(test_preds)} test rows\")","33c14bfa":"# Create the submission file\ndf_submission = pd.read_csv(DATA_PATH + 'sample_submission.csv')\ndf_submission['target']= test_preds\ndf_submission.to_csv('my_submission.csv',index=False)","386297f7":"### Tweet preprocessing","c115cc1f":"### Finetune BERT for text classification using pytorch and huggingface","8a43e0eb":"### Inference on the test set using best model","9233c142":"### Get train and validation dataset and dataloaders for a fold","da8e24d3":"### Configuration for training","e8c1f018":"## Dataset for transformer model\nWe use hugging face Dataset library to create a custom dataset from pandas dataframe","dd58b560":"### Load the data","902aaaef":"### The training function","1f515f37":"### K Fold CV\nSplit the training dataframe into kfolds for cross validation. We do this before any processing is done\non the data. We use stratified kfold if the target distribution is unbalanced","a29ccb34":"### The validation function ","b6df1ad8":"### The training loop"}}