{"cell_type":{"41df0a0d":"code","6fe64285":"code","d28fd4c3":"markdown","6d24e9c4":"markdown","5a57051c":"markdown","912366b7":"markdown","507b96a5":"markdown"},"source":{"41df0a0d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n# To create temporary directory, type in following in the Console\n# os.chdir(\"\/kaggle\/\")\n# !mkdir temp\n# os.listdir()\n\n# Any results we write to the current directory are saved as output in '\/kaggle\/working\/' directory\nprint()\nprint(os.listdir('..'))\nprint(os.listdir('\/kaggle\/input'))\nprint(os.listdir('\/kaggle\/working\/'))\n# print(os.listdir('\/kaggle\/temp\/'))\n","6fe64285":"\n\"\"\"\nCourse:  Convolutional Neural Networks for Image Classification\n\nSection-1\nQuick Win #5: Convolution in Real Time by camera\n\nDescription:\nTrack object via convolution in Real Time\nCalculate object centre and visualize tracker line\n\nFile: track_object_via_convolution_in_real_time.py\n\"\"\"\n\n\n# Algorithm:\n# --> Defining 1-channeled common 3x3 filter (kernel) for edge detection\n# --> Preparing OpenCV windows to be shown\n# --> Reading frames from camera in the loop\n#     --> Initializing Conv2D layer for GRAY input\n#     --> Implementing 2D convolution to the captured frame\n#     --> Finding the biggest contour\n#         --> Drawing bounding box, label, and prepare tracker line\n#         --> Cutting detected fragment\n#         --> Showing OpenCV windows, visualizing tracker line\n#     --> Calculating FPS rate\n#\n# Result: OpenCV windows with results\n\n\n# Importing needed libraries\nimport cv2\nimport numpy as np\n\nimport tensorflow as tf\n\nfrom timeit import default_timer as timer\n\nfrom collections import deque\n\n\n\"\"\"\nStart of:\nAdditional libraries to overcome possible CUDA <--> Tensorflow issues\n\"\"\"\n# Discussion #1\n# https:\/\/stackoverflow.com\/questions\/43147983\/could-\n# not-create-cudnn-handle-cudnn-status-internal-error\n\n# Discussion #2\n# https:\/\/stackoverflow.com\/questions\/47068709\/your-\n# cpu-supports-instructions-that-this-tensorflow-binary-was-not-compiled-to-u\n\n# import os\n# os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'\n#\n# from tensorflow.compat.v1 import ConfigProto\n# from tensorflow.compat.v1 import InteractiveSession\n# config = ConfigProto()\n# config.gpu_options.allow_growth = True\n# session = InteractiveSession(config=config)\n\n\"\"\"\nEnd of:\nAdditional libraries to overcome possible CUDA <--> Tensorflow issues\n\"\"\"\n\n\n\"\"\"\nStart of:\nDefining 1-channeled common 3x3 filter (kernel) for edge detection\n\"\"\"\n\n# Sobel filter to detect vertical changes on image\nf1 = np.array([[1, 0, -1],\n               [2, 0, -2],\n               [1, 0, -1]])\n\n\"\"\"\nEnd of:\nDefining 1-channeled common 3x3 filter (kernel) for edge detection\n\"\"\"\n\n\n\"\"\"\nStart of:\nPreparing OpenCV windows to be shown\n\"\"\"\n\n# Giving names to the windows\n# Specifying that windows are resizable\n\n# Window to show current view from camera in Real Time\n# cv2.namedWindow('Current view', cv2.WINDOW_NORMAL)\n\n# Window to show found contour\n# cv2.namedWindow('Contour', cv2.WINDOW_NORMAL)\n\n# Window to show cut fragment\n# cv2.namedWindow('Cut fragment', cv2.WINDOW_NORMAL)\n\n# Window to show tracker line\n# cv2.namedWindow('Tracker line', cv2.WINDOW_NORMAL)\n\n\"\"\"\nEnd of:\nPreparing OpenCV windows to be shown\n\"\"\"\n\n\n\"\"\"\nStart of:\nReading frames from camera in the loop\n\"\"\"\n\n# Defining 'VideoCapture' object\n# to read stream video from camera\n# Index of the built-in camera is usually 0\n# Try to select other cameras by passing 1, 2, 3, etc.\n# camera = cv2.VideoCapture(0)\ncamera = cv2.VideoCapture('\/kaggle\/input\/images-for-testing\/bird.mp4')\n\n\n# Preparing variables for spatial dimensions of the captured frames\nh, w = None, None\n\n\n# Getting version of OpenCV that is currently used\n# Converting string into the list by dot as separator and getting first number\nv = cv2.__version__.split('.')[0]\n\n\n# Creating image with black background\ntemp = np.zeros((720, 1280, 3), np.uint8)\n\n\n# Initializing deque object for center points of the detected object\npoints = deque(maxlen=50)\n\n\n# Defining counter for FPS (Frames Per Second)\ncounter = 0\n\n# Starting timer for FPS\n# Getting current time point in seconds\nfps_start = timer()\n\n\n# Defining loop to catch frames\nwhile True:\n    # Capturing frames one-by-one from camera\n    _, frame_bgr = camera.read()\n\n    # If the frame was not retrieved\n    # e.g.: at the end of the video,\n    # then we break the loop\n    if not _:\n        break\n\n    \"\"\"\n    Start of:\n    Initializing Conv2D layer for GRAY input\n    \"\"\"\n\n    # Getting spatial dimension of the frame\n    # We do it only once from the very beginning\n    # All other frames have the same dimensions\n    if w is None or h is None:\n        # Slicing from tuple only first two elements\n        (h, w) = frame_bgr.shape[:2]\n\n        # If you're using environment for GPU, there might be an issue like:\n        '''Failed to get convolution algorithm.\n        This is probably because cuDNN failed to initialize'''\n        # In this case, try to switch to the environment for CPU usage only\n        # instead of GPU\n\n        # Initializing Conv2D layer for GRAY input\n        # We do it only once from the very beginning\n        layer = tf.keras.layers.Conv2D(filters=1,\n                                       kernel_size=(3, 3),\n                                       strides=1,\n                                       padding='same',\n                                       activation='relu',\n                                       input_shape=(h, w, 1),\n                                       use_bias=False,\n                                       kernel_initializer=\n                                       tf.keras.initializers.constant(f1))\n\n    \"\"\"\n    End of:\n    Initializing Conv2D layer for GRAY input\n    \"\"\"\n\n    \"\"\"\n    Start of:\n    Implementing 2D convolution to the captured frame\n    \"\"\"\n\n    # Converting captured frame to GRAY by OpenCV function\n    frame_GRAY = cv2.cvtColor(frame_bgr, cv2.COLOR_BGR2GRAY)\n\n    # Reshaping frame to get following: (batch size, rows, columns, channels)\n    x_input_GRAY = frame_GRAY.reshape(1, h, w, 1).astype(np.float32)\n\n    # Passing GRAY input to the initialized Conv2D layer\n    # Calculating time spent for 2D convolution\n    start = timer()\n    output = layer(x_input_GRAY)\n    end = timer()\n\n    # Slicing from the output just filter\n    # Converting output Tensor into Numpy array\n    output = np.array(output[0, :, :, 0])\n\n    # To exclude values that are less than 0 and more than 255,\n    # Numpy function 'clip' is applied\n    # It keeps values of Numpy array in the given range\n    # And it replaces non-needed values with boundary numbers\n    output = np.clip(output, 0, 255).astype(np.uint8)\n\n    \"\"\"\n    End of:\n    Implementing 2D convolution to the captured frame\n    \"\"\"\n\n    \"\"\"\n    Start of:\n    Finding the biggest contour\n    \"\"\"\n\n    # Applying dilation\n    # Morphological filter that increases areas of foreground pixels\n    # dilated = cv2.dilate(output, None, iterations=3)\n    dilated = output\n\n    # Finding contours\n    # (!) Different versions of OpenCV returns different number of parameters\n    # when using function cv2.findContours()\n\n    # In OpenCV version 3 function cv2.findContours() returns three parameters:\n    # modified image, found contours and hierarchy\n    # All found contours from current frame are stored in the list\n    # Each individual contour is a Numpy array of(x, y) coordinates\n    # of the boundary points of the object\n    # We are interested only in contours\n\n    # Checking if OpenCV version 3 is used\n    if v == '3':\n        _, contours, _ = cv2.findContours(dilated,\n                                          cv2.RETR_TREE,\n                                          cv2.CHAIN_APPROX_NONE)\n\n    # In OpenCV version 4 function cv2.findContours() returns two parameters:\n    # found contours and hierarchy\n    # All found contours from current frame are stored in the list\n    # Each individual contour is a Numpy array of(x, y) coordinates\n    # of the boundary points of the object\n    # We are interested only in contours\n\n    # Checking if OpenCV version 4 is used\n    else:\n        contours, _ = cv2.findContours(dilated,\n                                       cv2.RETR_TREE,\n                                       cv2.CHAIN_APPROX_NONE)\n\n    # Finding the biggest Contour by sorting from biggest to smallest\n    contours = sorted(contours, key=cv2.contourArea, reverse=True)\n\n    \"\"\"\n    Start of:\n    Drawing bounding box, label, and prepare tracker line\n    \"\"\"\n\n    # Extracting rectangle coordinates around biggest contour if any was found\n    if contours:\n        # Function cv2.boundingRect() is used to get an approximate rectangle\n        # around the region of interest in binary image after contour was found\n        (x_min, y_min, box_width, box_height) = cv2.boundingRect(contours[0])\n\n        # Drawing bounding box on the current BGR frame\n        cv2.rectangle(frame_bgr, (x_min, y_min),\n                      (x_min + box_width, y_min + box_height),\n                      (0, 255, 0), 3)\n\n        # Preparing text for the label\n        label = 'Person'\n\n        # Putting text with label on the current BGR frame\n        cv2.putText(frame_bgr, label, (x_min - 5, y_min - 25),\n                    cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0, 255, 0), 2)\n\n        # Getting current center coordinates of the bounding box\n        center = (int(x_min + box_width \/ 2), int(y_min + box_height \/ 2))\n\n        # Adding current pont to the queue\n        points.appendleft(center)\n\n        \"\"\"\n        End of:\n        Drawing bounding box, label, and prepare tracker line\n        \"\"\"\n\n        \"\"\"\n        Start of:\n        Cutting detected fragment\n        \"\"\"\n\n        # Cutting detected fragment from BGR frame\n        cut_fragment_bgr = frame_bgr[y_min + int(box_height * 0.1):\n                                     y_min + box_height - int(box_height * 0.1),\n                                     x_min + int(box_width * 0.1):\n                                     x_min + box_width - int(box_width * 0.1)]\n\n        \"\"\"\n        End of:\n        Cutting detected fragment\n        \"\"\"\n\n        \"\"\"\n        Start of:\n        Showing OpenCV windows, visualizing tracker line\n        \"\"\"\n        \n        # Showing current view from camera in Real Time\n        # Pay attention! 'cv2.imshow' takes images in BGR format\n        # cv2.imshow('Current view', frame_bgr)\n\n        # Showing found contour\n        # cv2.imshow('Contour', output)\n\n        # Showing cut fragment\n        # cv2.imshow('Cut fragment', cut_fragment_bgr)\n\n        # Changing background to BGR(230, 161, 0)\n        # B = 230, G = 161, R = 0\n        temp[:, :, 0] = 230\n        temp[:, :, 1] = 161\n        temp[:, :, 2] = 0\n\n        # Visualizing tracker line\n        for i in range(1, len(points)):\n            # If no points collected yet\n            if points[i - 1] is None or points[i] is None:\n                continue\n\n            # Draw the line between points\n            cv2.line(temp, points[i - 1], points[i], (0, 255, 0), 3)\n\n        # Adding text with center coordinates of the bounding box\n        cv2.putText(temp, 'X: {0}'.format(center[0]),\n                    (50, 100), cv2.FONT_HERSHEY_DUPLEX, 3, (255, 255, 255),\n                    4, cv2.LINE_AA)\n        cv2.putText(temp, 'Y: {0}'.format(center[1]),\n                    (50, 200), cv2.FONT_HERSHEY_DUPLEX, 3, (255, 255, 255),\n                    4, cv2.LINE_AA)\n\n        # Showing tracker line\n        # cv2.imshow('Tracker line', temp)\n\n        \"\"\"\n        End of:\n        Showing OpenCV windows, visualizing tracker line\n        \"\"\"\n\n    # If no contour is found, showing OpenCV windows with information\n    else:\n        # Showing current view from camera in Real Time\n        # Pay attention! 'cv2.imshow' takes images in BGR format\n        # cv2.imshow('Current view', frame_bgr)\n\n        # Changing background to BGR(230, 161, 0)\n        # B = 230, G = 161, R = 0\n        temp[:, :, 0] = 230\n        temp[:, :, 1] = 161\n        temp[:, :, 2] = 0\n\n        # Adding text with information\n        cv2.putText(temp, 'No contour', (100, 450),\n                    cv2.FONT_HERSHEY_DUPLEX, 4, (255, 255, 255), 4, cv2.LINE_AA)\n\n        # Showing information in prepared OpenCV windows\n        # cv2.imshow('Contour', temp)\n        # cv2.imshow('Cut fragment', temp)\n        # cv2.imshow('Tracker line', temp)\n\n    \"\"\"\n    End of:\n    Finding the biggest contour\n    \"\"\"\n\n    \"\"\"\n    Start of:\n    Calculating FPS\n    \"\"\"\n\n    # Increasing counter for FPS\n    counter += 1\n\n    # Stopping timer for FPS\n    # Getting current time point in seconds\n    fps_stop = timer()\n\n    # Checking if timer reached 1 second\n    # Comparing\n    if fps_stop - fps_start >= 1.0:\n        # Showing FPS rate\n        print('FPS rate is: ', counter)\n\n        # Reset FPS counter\n        counter = 0\n\n        # Restart timer for FPS\n        # Getting current time point in seconds\n        fps_start = timer()\n\n    \"\"\"\n    End of:\n    Calculating FPS\n    \"\"\"\n\n    # Breaking the loop if 'q' is pressed\n    # if cv2.waitKey(1) & 0xFF == ord('q'):\n        # break\n\n\"\"\"\nEnd of:\nReading frames from camera in the loop\n\"\"\"\n\n\n# Releasing camera\ncamera.release()\n\n# Destroying all opened OpenCV windows\n# cv2.destroyAllWindows()\n\n\n\"\"\"\nSome comments\nOpenCV function 'cv2.findContours'\nMore details and examples are here:\nhttps:\/\/docs.opencv.org\/4.0.0\/d4\/d73\/tutorial_py_contours_begin.html\n\n\nFunction 'cv2.putText' adds text to images.\nMore details and examples are here:\nprint(help(cv2.putText))\nhttps:\/\/docs.opencv.org\/master\/dc\/da5\/tutorial_py_drawing_functions.html\n\n\nFunction 'cv2.dilate' increases the boundaries of regions of foreground pixels.\nMore details and examples are here:\nprint(help(cv2.dilate))\nhttps:\/\/docs.opencv.org\/master\/db\/df6\/tutorial_erosion_dilatation.html\nhttps:\/\/docs.opencv.org\/master\/d4\/d86\/group__imgproc__filter.html\n#ga4ff0f3318642c4f469d0e11f242f3b6c\n\n\nPython deque object is a generalization of stacks and queues.\nMore details and examples are here:\nfrom collections import deque\nprint(help(deque))\nhttps:\/\/docs.python.org\/3\/library\/collections.html#collections.deque\n\n\"\"\"\n","d28fd4c3":"**Design**, **Train** & **Test** deep CNN for Image Classification.\n\n**Join** the course & enjoy new opportunities to get deep learning skills:\n\n\n[https:\/\/www.udemy.com\/course\/convolutional-neural-networks-for-image-classification\/](https:\/\/www.udemy.com\/course\/convolutional-neural-networks-for-image-classification\/?referralCode=12EE0D74A405BF4DDE9B)\n\n\n![](https:\/\/github.com\/sichkar-valentyn\/1-million-images-for-Traffic-Signs-Classification-tasks\/blob\/main\/images\/slideshow_classification.gif?raw=true)\n","6d24e9c4":"# \ud83c\udf93 Course:  Convolutional Neural Networks for Image Classification\n\n## &nbsp; \u26e9\ufe0f Section-1\n### &nbsp; &nbsp; \ud83c\udf9b\ufe0f Quick Win #5: Convolution in Real Time by camera\n\n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;**Description:**  \n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;*Track object via convolution in Real Time.*  \n&nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;*Calculate object centre and visualize tracker line.*  \n","5a57051c":"# \ud83c\udf93 Related course for classification tasks","912366b7":"### \ud83d\udca1 Algorithm:\n\n\u2714\ufe0f **Define** 1-channeled common 3x3 filter (kernel) for edge detection  \n\u2714\ufe0f **Prepare** OpenCV windows to be shown  \n\u2714\ufe0f **Read** frames from camera in the loop  \n\u2714\ufe0f **Initialize** Conv2D layer for GRAY input  \n\u2714\ufe0f **Implement** 2D convolution to the captured frame  \n\u2714\ufe0f **Find** the biggest contour  \n\u2714\ufe0f **Draw** bounding box, label, and prepare tracker line  \n\u2714\ufe0f **Cut** detected fragment  \n\u2714\ufe0f **Show** OpenCV windows, visualizing tracker line  \n\u2714\ufe0f **Calculat** FPS rate  \n   \n  \n### \ud83c\udfaf **Result:**  \n\u2705 **Result:** OpenCV windows with results  \n\n  ","507b96a5":"# \ud83d\udce5 Import libraries"}}