{"cell_type":{"05479839":"code","57dac655":"code","7fb4c353":"code","5b7a86a2":"code","57f05ff6":"code","d539d5ce":"code","993c4651":"code","59cb3110":"code","af2b082f":"code","71ddd936":"code","c79b09f8":"code","7a3e8477":"code","27fd537d":"code","b2827fec":"code","66ba132e":"code","91e6aa48":"code","a7934664":"code","857cfca9":"code","708c12fe":"code","7c6d01f7":"code","8533a485":"code","2fcf5a31":"code","4c2f9fa0":"code","257cd79b":"code","145306da":"code","f332f51b":"code","a81187ca":"code","54e09147":"code","25084186":"code","7cc42d7e":"code","c555cc12":"code","7100d8a0":"code","3d61a735":"code","b291be0d":"code","3daa770d":"code","d6464909":"code","619e2324":"code","6cef1add":"code","c2c8ad34":"markdown","822e2689":"markdown","9c330a50":"markdown","19e6ff01":"markdown","7748aa41":"markdown","68135d9a":"markdown","ebbe5d55":"markdown","a5d57548":"markdown","1c4a45f4":"markdown","58d42e1a":"markdown","a77c2a29":"markdown","930d3f23":"markdown","1a7fd793":"markdown","ad24b290":"markdown","c539b40a":"markdown","f2ffa93c":"markdown","c9032073":"markdown","6a81d15a":"markdown","2c6421e6":"markdown","ffbd27b5":"markdown"},"source":{"05479839":"%matplotlib inline\nimport os\nimport pandas as pd\nfrom tqdm import tqdm\nimport datetime as dt\nimport numpy as np\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_palette(sns.color_palette('tab20', 20))\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression, LinearRegression\nfrom sklearn.ensemble import RandomForestClassifier\npd.set_option('display.max_columns', 999)\nnp.random.seed(1987)\nplt.rcParams['figure.figsize'] = [16, 10]\nplt.rcParams['font.size'] = 14","57dac655":"start = dt.datetime.now()\n!cp ..\/input\/surveykernelexternals\/* .\n\ndata_dir = '..\/input\/kaggle-survey-2018\/'\nMETA_DATA_PATH = '..\/input\/meta-kaggle\/'\nquestions = pd.read_csv(os.path.join(data_dir, 'SurveySchema.csv'))\nresponses = pd.read_csv(os.path.join(data_dir, 'multipleChoiceResponses.csv'),\n                        low_memory=False)\nresponses['cnt'] = 1\ntarget_order = pd.DataFrame({\n    'Q26': ['Definitely not', 'Definitely yes', 'Maybe', 'Probably not', 'Probably yes'],\n    'target': [0, 4, 2, 1, 3],\n})\ntarget_order = target_order.sort_values(by='target')\ntarget_order['color'] = ['rgb(217,30,30)', 'rgb(242,143,56)', 'rgb(242,211,56)',\n                         'rgb(10,136,186)', 'rgb(12,51,131)']\nresponses = responses.merge(target_order, how='left', on='Q26')\nquestions.shape\nquestions.head()\nresponses.shape\nresponses.head()\n\nq26 = responses[1:].groupby(['Q26', 'target'])[['cnt']].count()\nq26 = q26.sort_values(by='target').reset_index()\nq26","7fb4c353":"data = [\n    go.Bar(\n        x=q26.Q26.values,\n        y=q26.cnt.values,\n        marker=dict(color=-q26['target'].values,\n                    colorscale='Portland', showscale=False)\n    ),\n]\nlayout = go.Layout(\n    title='Do you consider yourself to be a data scientist?',\n    yaxis=dict(title='# of Respondents:', ticklen=5, gridwidth=2),\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='q26')","5b7a86a2":"responses[['Q24', 'Q25', 'Q26']][:2]","57f05ff6":"q24_clean = pd.DataFrame({\n    'Q24': ['I have never written code and I do not want to learn',\n            '40+ years', '30-40 years', '20-30 years',\n            'I have never written code but I want to learn', '10-20 years',\n            '5-10 years', '3-5 years', '< 1 year', '1-2 years'],\n    'q24_clean': ['None', '10+', '10+', '10+', 'None', '10+', '5-10', '3-5', '<1', '1-2'],\n    'f24': [-1, 10, 10, 10, -1, 10, 7.5, 4, 0.5, 1.5]\n})\nenhanced_responses = responses.merge(q24_clean, on='Q24')\nq24_cross = enhanced_responses[1:].groupby([\n    'Q26', 'target', 'color', 'q24_clean', 'f24'])[['cnt']].count()\\\n    .reset_index().sort_values(by='f24')\nq24_cross","d539d5ce":"data = []\nfor ds in target_order.Q26.values:\n    df = q24_cross[q24_cross.Q26 == ds]\n    data.append(\n        go.Bar(x=df.q24_clean.values, y=df.cnt.values,\n               marker=dict(color=df.color.values),\n               name=ds)\n    )\nlayout = go.Layout(\n    title='Data Analysis Experience',\n    xaxis=dict(title='How long have you been writing code to analyze data? (years)',\n               ticklen=5, zeroline=False, gridwidth=2),\n    yaxis=dict(title='Data Scientist Confidence', ticklen=5, gridwidth=2),\n    showlegend=True\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='data-code')","993c4651":"q25_clean = pd.DataFrame({\n    'Q25': ['I have never studied machine learning and I do not plan to',\n            '20+ years', '10-15 years', '4-5 years', '5-10 years', '3-4 years',\n            'I have never studied machine learning but plan to learn in the future',\n            '2-3 years', '1-2 years', '< 1 year'],\n    'q25_clean': ['None', '4+', '4+', '4+', '4+', '3-4', 'None', '2-3', '1-2', '<1'],\n    'f25': [-1, 5, 5, 5, 5, 3.5, -1, 2.5, 1.5, 0.5]\n})\nenhanced_responses = responses.merge(q25_clean, on='Q25')\nq26_cross = enhanced_responses[1:].groupby([\n    'Q26', 'target', 'color', 'q25_clean', 'f25'])[['cnt']]\\\n    .count().reset_index().sort_values(by='f25')\nq26_cross\n","59cb3110":"data = []\nfor ds in target_order.Q26.values:\n    df = q26_cross[q26_cross.Q26 == ds]\n    data.append(\n        go.Bar(x=df.q25_clean.values, y=df.cnt.values,\n               marker=dict(color=df.color.values),\n               name=ds)\n    )\nlayout = go.Layout(\n    title='Machine Learning Experience',\n    xaxis=dict(title='Machine Learning Experience (years)',\n               ticklen=5, zeroline=False, gridwidth=2),\n    yaxis=dict(title='Data Scientist Confidence', ticklen=5, gridwidth=2),\n    showlegend=True\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='mlexp')","af2b082f":"q6_cnt = responses[1:].groupby(['Q6'])[['cnt']].count().reset_index()\nq6_mean = responses[1:].groupby(['Q6'])[['target']].mean().reset_index()\n\nq6_clean = pd.merge(q6_cnt, q6_mean, on='Q6')\nq6_clean.columns = ['Q6', 'Q6_cnt', 'Q6_dsc']\nq6_clean = q6_clean.sort_values(by='Q6_dsc', ascending=False)\nq6_clean\nenhanced_responses = responses.merge(q6_clean, on='Q6')\n\nq6_cross = enhanced_responses[1:].groupby([\n    'Q6', 'Q6_cnt', 'Q6_dsc', 'Q26', 'target', 'color'])[['cnt']] \\\n    .count().reset_index().sort_values(by='Q6_dsc')\nq6_cross = q6_cross[q6_cross.Q6_cnt > 500]\nq6_cross","71ddd936":"data = []\nfor ds in target_order.Q26.values:\n    df = q6_cross[q6_cross.Q26 == ds]\n    data.append(\n        go.Bar(x=df.Q6.values, y=df.cnt.values,\n               marker=dict(color=df.color.values),\n               name=ds)\n    )\nlayout = go.Layout(\n    title='An individual with data scientist job title is probably a data scientist :)',\n    xaxis=dict(title='Title most similar to current role',\n               ticklen=5, zeroline=False, gridwidth=2),\n    yaxis=dict(title='Data Scientist Confidence', ticklen=5, gridwidth=2),\n    showlegend=True\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='job-title')","c79b09f8":"('<1' > '2',\n '10-15' < '3-4',\n 'Doctoral degree' < 'No formal education past high school')","7a3e8477":"def first_number(s):\n    s = s.replace('+', '-')\n    s = s.replace(' ', '')\n    s = s.replace(',000', '')\n    s = s.replace('%', '-')\n    try:\n        return int(s.split('-')[0])\n    except Exception:\n        return np.nan\n\n# Q1 5 23860\n# What is your gender? - Selected Choice\nQ1 = pd.DataFrame(\n    {'Q1': ['Female', 'Male', 'Prefer not to say', 'Prefer to self-describe']})\nQ1['f1'] = [0, 1, 2, 2]\nprint(Q1)\n\n# Q2 13 23860\n# What is your age (# years)?\nQ2 = pd.DataFrame(\n    {'Q2': ['45-49', '30-34', '35-39', '22-24', '25-29',\n            '18-21', '40-44', '55-59', '60-69',\n            '50-54', '80+', '70-79']})\nQ2['f2'] = Q2.Q2.apply(first_number)\nprint(Q2)\n\n# Q4 8 23439\n# What is the highest level of formal education that you have attained or plan to attain within the next 2 years?\nQ4 = pd.DataFrame(\n    {'Q4': ['Doctoral degree', 'Bachelor\u2019s degree', 'Master\u2019s degree', 'Professional degree',\n            'Some college\/university study without earning a bachelor\u2019s degree',\n            'I prefer not to answer',\n            'No formal education past high school']})\nQ4['f4'] = [5, 2, 3, 2, 1, np.nan, 0]\nprint(Q4)\n\n# Q8 12 21102\n# How many years of experience do you have in your current role?\nQ8 = pd.DataFrame({'Q8': ['5-10', '0-1', '10-15', '3-4', '1-2',\n                          '2-3', '15-20', '4-5', '20-25',\n                          '25-30', '30 +']})\nQ8['f8'] = Q8.Q8.apply(first_number)\nprint(Q8)\n\n# Q9 20 20186\n# What is your current yearly compensation (approximate $USD)?\nQ9 = pd.DataFrame({'Q9': ['I do not wish to disclose my approximate yearly compensation',\n                          '10-20,000', '0-10,000', '20-30,000', '125-150,000', '30-40,000',\n                          '50-60,000', '100-125,000', '90-100,000', '70-80,000', '80-90,000',\n                          '60-70,000', '400-500,000', '40-50,000', '150-200,000', '500,000+',\n                          '300-400,000', '200-250,000', '250-300,000']})\nQ9['f9'] = Q9.Q9.apply(first_number)\nprint(Q9)\n\n# Q10 7 20670\n# Does your current employer incorporate machine learning methods into their business?\nQ10 = pd.DataFrame({'Q10': [\n    'I do not know',\n    'No (we do not use ML methods)',\n    'We are exploring ML methods (and may one day put a model into production)',\n    'We recently started using ML methods (i.e., models in production for less than 2 years)',\n    'We have well established ML methods (i.e., models in production for more than 2 years)',\n    'We use ML methods for generating insights (but do not put working models into production)'\n]})\nQ10['f10'] = [-1, 0, 1, 2, 3, 1]\nprint(Q10)\n\n# Q12_MULTIPLE_CHOICE 7 19199\n# What is the primary tool that you use at work or school to analyze data? (include text response) - Selected Choice\nQ12_MULTIPLE_CHOICE = pd.DataFrame({'Q12_MULTIPLE_CHOICE': [\n    'Cloud-based data software & APIs (AWS, GCP, Azure, etc.)',\n    'Basic statistical software (Microsoft Excel, Google Sheets, etc.)',\n    'Local or hosted development environments (RStudio, JupyterLab, etc.)',\n    'Advanced statistical software (SPSS, SAS, etc.)',\n    'Other',\n    'Business intelligence software (Salesforce, Tableau, Spotfire, etc.)'\n]})\nQ12_MULTIPLE_CHOICE['f12_MULTIPLE_CHOICE'] = [4, 1, 3, 2, -1, 1]\nprint(Q12_MULTIPLE_CHOICE)\n\n# Q23 7 18548\n# Approximately what percent of your time at work or school is spent actively coding?\nQ23 = pd.DataFrame({'Q23': [\n    '0% of my time',\n    '1% to 25% of my time',\n    '75% to 99% of my time',\n    '50% to 74% of my time',\n    '25% to 49% of my time',\n    '100% of my time']})\nQ23['f23'] = Q23.Q23.apply(first_number)\nprint(Q23)\n\n# Q24 11 18534\n# How long have you been writing code to analyze data?\nQ24 = pd.DataFrame({'Q24': [\n    'I have never written code but I want to learn', '5-10 years', '3-5 years', '< 1 year',\n    '1-2 years', '10-20 years', '20-30 years', '30-40 years',\n    'I have never written code and I do not want to learn', '40+ years'\n]})\nQ24['f24'] = [0, 6, 4, 0.5, 1.5, 12, 24, 30, -1, 40]\nprint(Q24)\n\n# Q25 11 18492\n# For how many years have you used machine learning methods (at work or in school)?\nQ25 = pd.DataFrame({'Q25': [\n    'I have never studied machine learning but plan to learn in the future', '< 1 year',\n    '4-5 years', '2-3 years', '1-2 years', '5-10 years', '3-4 years',\n    'I have never studied machine learning and I do not plan to', '20+ years', '10-15 years'\n]})\nQ25['f25'] = [0, 0.5, 4, 2.5, 1.5, 6, 3.5, -1, 20, 12]\nprint(Q25)\n\n# Q26 6 18481\n# Do you consider yourself to be a data scientist?\nQ26 = pd.DataFrame({\n    'Q26': ['Definitely not', 'Definitely yes', 'Maybe', 'Probably not', 'Probably yes'],\n    'target': [0, 4, 2, 1, 3],\n})\nQ26 = Q26.sort_values(by='target')\nQ26['color'] = ['rgb(217,30,30)', 'rgb(242,143,56)', 'rgb(242,211,56)', 'rgb(10,136,186)',\n                'rgb(12,51,131)']\nprint(Q26)\n\n# Q40 7 15880\n# Which better demonstrates expertise in data science: academic achievements or independent projects? - Your views:\nQ40 = pd.DataFrame({'Q40': [\n    'Independent projects are much less important than academic achievements',\n    'Independent projects are slightly less important than academic achievements',\n    'Independent projects are equally important as academic achievements',\n    'Independent projects are slightly more important than academic achievements',\n    'Independent projects are much more important than academic achievements',\n    'No opinion; I do not know'\n]})\nQ40['f40'] = [0, 1, 2, 3, 4, np.nan]\nprint(Q40)\n\n# Q41_Part_1 5 14937\n# How do you perceive the importance of the following topics? - Fairness and bias in ML algorithms:\nQ41_Part_1 = pd.DataFrame(\n    {'Q41_Part_1': ['Very important', 'Slightly important', 'Not at all important',\n                    'No opinion; I do not know']})\nQ41_Part_1['f41_Part_1'] = [3, 2, 1, np.nan]\nprint(Q41_Part_1)\n\n# Q41_Part_2 5 14937\n# How do you perceive the importance of the following topics? - Being able to explain ML model outputs and\/or predictions\nQ41_Part_2 = pd.DataFrame({\n    'Q41_Part_2': ['Very important', 'Slightly important', 'Not at all important',\n                   'No opinion; I do not know']})\nQ41_Part_2['f41_Part_2'] = [3, 2, 1, np.nan]\nprint(Q41_Part_2)\n\n# Q41_Part_3 5 14937\n# How do you perceive the importance of the following topics? - Reproducibility in data science\nQ41_Part_3 = pd.DataFrame(\n    {'Q41_Part_3': ['Very important', 'Slightly important', 'Not at all important',\n                    'No opinion; I do not know']})\nQ41_Part_3['f41_Part_3'] = [3, 2, 1, np.nan]\nprint(Q41_Part_3)\n\n# Q43 12 13120\n# Approximately what percent of your data projects involved exploring unfair bias in the dataset and\/or algorithm?\nQ43 = pd.DataFrame({'Q43': [\n    '0-10', '20-30', '0', '10-20', '30-40', '60-70', '40-50', '90-100', '70-80', '50-60',\n    '80-90']})\nQ43['f43'] = Q43.Q43.apply(first_number)\n\n# Q46 12 13290\n# Approximately what percent of your data projects involve exploring model insights?\nQ46 = pd.DataFrame({'Q46': [\n    '10-20', '20-30', '0', '50-60', '0-10', '40-50', '90-100', '80-90', '30-40', '70-80',\n    '60-70']})\nQ46['f46'] = [15, 25, 0, 55, 5, 45, 95, 85, 35, 75, 65]\nprint(Q46)\n\n# Q48 6 13369\n# Do you consider ML models to be \"black boxes\" with outputs that are difficult or impossible to explain?\nQ48 = pd.DataFrame({'Q48': [\n    'I view ML models as \"black boxes\" but I am confident that experts are able to explain model outputs',\n    'Yes, most ML models are \"black boxes\"',\n    'I am confident that I can understand and explain the outputs of many but not all ML models',\n    'I am confident that I can explain the outputs of most if not all ML models',\n    'I do not know; I have no opinion on the matter'\n]})\nQ48['f48'] = [0, 1, 2, 3, np.nan]\nprint(Q48)\n\n# Merge ordinal features\nresponses = pd.read_csv(\n    os.path.join(data_dir, 'multipleChoiceResponses.csv'), low_memory=False)\nenhanced_responses = responses[1:].copy()\nenhanced_responses = enhanced_responses[\n    [c for c in enhanced_responses.columns if '_OTHER_TEXT' not in c]].copy()\n\nenhanced_responses = enhanced_responses.merge(Q1, on='Q1', how='left')\nenhanced_responses = enhanced_responses.merge(Q2, on='Q2', how='left')\nenhanced_responses = enhanced_responses.merge(Q4, on='Q4', how='left')\nenhanced_responses = enhanced_responses.merge(Q8, on='Q8', how='left')\nenhanced_responses = enhanced_responses.merge(Q9, on='Q9', how='left')\nenhanced_responses = enhanced_responses.merge(Q10, on='Q10', how='left')\nenhanced_responses = enhanced_responses.merge(Q12_MULTIPLE_CHOICE, on='Q12_MULTIPLE_CHOICE',\n                                              how='left')\nenhanced_responses = enhanced_responses.merge(Q23, on='Q23', how='left')\nenhanced_responses = enhanced_responses.merge(Q24, on='Q24', how='left')\nenhanced_responses = enhanced_responses.merge(Q25, on='Q25', how='left')\nenhanced_responses = enhanced_responses.merge(Q26, on='Q26', how='left')\nenhanced_responses = enhanced_responses.merge(Q40, on='Q40', how='left')\nenhanced_responses = enhanced_responses.merge(Q41_Part_1, on='Q41_Part_1', how='left')\nenhanced_responses = enhanced_responses.merge(Q41_Part_2, on='Q41_Part_2', how='left')\nenhanced_responses = enhanced_responses.merge(Q41_Part_3, on='Q41_Part_3', how='left')\nenhanced_responses = enhanced_responses.merge(Q43, on='Q43', how='left')\nenhanced_responses = enhanced_responses.merge(Q46, on='Q46', how='left')\nenhanced_responses = enhanced_responses.merge(Q48, on='Q48', how='left')\n\nenhanced_responses = enhanced_responses[~enhanced_responses.target.isna()].copy()\nprint(enhanced_responses.shape)\n\nordinal_cols = ['Q1', 'Q2', 'Q4', 'Q8', 'Q9', 'Q10', 'Q12_MULTIPLE_CHOICE', 'Q23', 'Q24',\n                'Q25', 'Q26', 'Q40', 'Q41_Part_1', 'Q41_Part_2', 'Q41_Part_3', 'Q43', 'Q46',\n                'Q48']\n\npercentage_cols = [c for c in enhanced_responses.columns if\n                   c.startswith('Q34') or c.startswith('Q35')]\nfor col in percentage_cols:\n    enhanced_responses['f' + col[1:]] = enhanced_responses[col].astype(np.float64)\n\nenhanced_responses['cnt'] = 1\nenhanced_responses['probably'] = 1 * (enhanced_responses['target'] > 2)\nenhanced_responses['definitely'] = 1 * (enhanced_responses['Q26'] == 'Definitely yes')\nprint(len(ordinal_cols))\nprint(ordinal_cols)\n\nprint([c for c in enhanced_responses.columns if c.startswith('f')])\nprint(len([c for c in enhanced_responses.columns if c.startswith('f')]))\nprint(enhanced_responses.shape)","27fd537d":"numeric_features = [c for c in enhanced_responses.columns if c.startswith('f')]\nprint(numeric_features)\n\ncorr = enhanced_responses[numeric_features + ['target']].corr()\ncorr_cols = np.abs(corr[['target']]).sort_values(by='target').index[-13:]\ncorr = enhanced_responses[corr_cols].corr()\n\nresponses[['Q' + c[1:] for c in corr_cols[:-1]]][:1].columns\nresponses[['Q' + c[1:] for c in corr_cols[:-1]]][:1].values","b2827fec":"corr_labels = pd.DataFrame({\n    'Q': ['Q9', 'Q43', 'Q35_Part_2', 'Q24', 'Q48', 'Q4', 'Q12_MULTIPLE_CHOICE', 'Q35_Part_3',\n          'Q46', 'Q23', 'Q10', 'Q25'],\n    'L': ['Salary', 'Unfair Bias%', 'Online courses%', 'Data Analytics Years', 'Black Box',\n          'Education', 'Primary tool', 'Work%', 'Insights%', 'Coding%', 'ML Employer',\n          'Machine Learning Years']\n})\n\ncorr = enhanced_responses[corr_cols].corr(method='pearson').round(2)\n\nxcols = list(corr_labels.L.values) + ['Data Science Confidence']\nycols = list(corr_labels.L.values) + ['Data Science Confidence']\n\nlayout = dict(\n    title='Ordinal feature correlations',\n    width=900,\n    height=900,\n    #     margin=go.Margin(l=200, r=50, b=50, t=250, pad=4),\n    margin=go.layout.Margin(l=200, r=50, b=50, t=250, pad=4),\n)\nfig = ff.create_annotated_heatmap(\n    z=corr.values,\n    x=list(xcols),\n    y=list(ycols),\n    colorscale='Portland',\n    #     reversescale=True,\n    showscale=True,\n    font_colors=['#efecee', '#3c3636'])\nfig['layout'].update(layout)\npy.iplot(fig, filename='OrdinalCorrelations')","66ba132e":"aggr = enhanced_responses.groupby(['f25', 'target'])[['cnt']].sum().reset_index()\naggr.shape\naggr.head()\nregr = LinearRegression()\nregr.fit(enhanced_responses[['f25']], enhanced_responses.target)\nxs = np.array([[x] for x in np.arange(-1, 21)])\nps = regr.predict(xs)","91e6aa48":"data = [\n    go.Scatter(\n        y=aggr.target.values,\n        x=aggr.f25.values,\n        mode='markers',\n        marker=dict(\n            sizemode='diameter',\n            sizeref=1,\n            size=np.sqrt(aggr.cnt.values),\n            color=aggr.target.values,\n            colorscale='Portland',\n            reversescale=True,\n            showscale=False),\n        text=aggr['cnt'].values\n    ),\n    go.Scatter(\n        x=xs[:, 0],\n        y=ps,\n        mode='lines',\n        line=dict(color='black', width=3, dash='dash')\n    )\n]\nlayout = go.Layout(\n    autosize=True,\n    title='Even the strongest correlation is not really strong',\n    hovermode='closest',\n    xaxis=dict(title='Machine Learning Experience (years)', ticklen=5,\n               showgrid=False, zeroline=False, showline=False, range=[-1.5, 6.5]),\n    yaxis=dict(title='Data Science Confidence', showgrid=False,\n               zeroline=False, ticklen=5, gridwidth=2,\n               tickvals=np.arange(5),\n               ticktext=['No Way', 'Unlikely', 'Maybe?', 'Probably', 'Hell Yeah!']),\n    showlegend=False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='scatter_countries')","a7934664":"country_mean = enhanced_responses.groupby('Q3')[['target', 'probably', 'definitely']].mean()\ncountry_cnt = enhanced_responses.groupby('Q3')[['cnt']].sum()\ncountry_stats = pd.merge(country_mean, country_cnt, left_index=True,\n                         right_index=True).reset_index()\ncountry_stats.sort_values(by='cnt')\ncountry_stats.mean()","857cfca9":"y = 'probably'\ndata = [dict(\n    type='choropleth',\n    locations=country_stats['Q3'],\n    locationmode='country names',\n    z=country_stats[y],\n    text=country_stats['Q3'],\n    colorscale='Portland',\n    reversescale=True,\n    marker=dict(line=dict(color='rgb(180,180,180)', width=0.5)),\n    colorbar=dict(autotick=False, tickprefix='', title=y),\n)]\nlayout = dict(\n    title='A pretty (useless) map about Data Scientist Confidence',\n    geo=dict(showframe=False, showcoastlines=True, projection=dict(type='Mercator'))\n)\nfig = dict(data=data, layout=layout)\npy.iplot(fig, validate=False, filename='world-map')","708c12fe":"col_stats = pd.DataFrame({'name': enhanced_responses.columns,\n                          'nunique': enhanced_responses.nunique().values,\n                          'count': enhanced_responses.count().values,\n                          })\ncol_stats['Q'] = col_stats.name.apply(lambda s: s.split('_')[0])\ncol_stats = col_stats[col_stats.Q.str.startswith('Q')]\nmulti_question_columns = col_stats.groupby('Q')[['count']].count().reset_index()\nmulti_question_columns = multi_question_columns[multi_question_columns['count'] > 1]\n\ncol_stats.head(20)\ncol_stats.shape\nmulti_question_columns.shape\nmulti_question_columns","7c6d01f7":"# Count of multiple choices\nfor q, c in multi_question_columns.values:\n    if q not in ordinal_cols:\n        qs = col_stats[col_stats.Q == q].name.values\n        enhanced_responses['f{}_answers'.format(q[1:])] = enhanced_responses[qs].count(axis=1)\n\n# Binary columns\nbinary_cols = col_stats[col_stats['nunique'] == 1]\nfor col in binary_cols.name.values:\n    enhanced_responses['f{}_binary'.format(col[1:])] = 1 - (1 * enhanced_responses[col].isna())\n\n# One Hot Encoded categorical columns\ncategorical_cols = col_stats[(col_stats['nunique'] > 1) & (col_stats['nunique'] < 100)]\ncategorical_cols = categorical_cols[~categorical_cols.Q.isin(ordinal_cols)]\nfor col in categorical_cols.name.values:\n    df = pd.get_dummies(enhanced_responses[col].values)\n    for i in range(df.shape[1]):\n        enhanced_responses['f{}_v_{}'.format(col[1:], i)] = df.values[:, i]","8533a485":"features = [c for c in enhanced_responses.columns if c.startswith('f')]\n'We have {} respondents and {} features.'.format(enhanced_responses.shape[0], len(features))","2fcf5a31":"# XGB\nfix = {'nthread': 3, 'booster': 'gbtree', 'silent': 1, 'eval_metric': 'auc',\n       'objective': 'binary:logistic'}\nconfig = dict(min_child_weight=10, eta=0.05, colsample_bytree=0.5, max_depth=6, subsample=0.8)\nconfig.update(fix)\n\nX = enhanced_responses[features].values\ny = enhanced_responses['probably'].values\nXtr, Xv, ytr, yv = train_test_split(X, y, test_size=0.25, random_state=42)\nprint(Xtr.shape, Xv.shape, y.mean())\n\nfs = ['f%i' % i for i in range(len(features))]\ndtrain = xgb.DMatrix(Xtr, label=ytr, feature_names=fs)\ndvalid = xgb.DMatrix(Xv, label=yv, feature_names=fs)\n\nxgb_probably = xgb.train(config, dtrain, 500, [(dtrain, 'train'), (dvalid, 'valid')],\n                         early_stopping_rounds=20, maximize=True, verbose_eval=50)\nfpr_xgb_probably, tpr_xgb_probably, thresholds = metrics.roc_curve(yv, xgb_probably.predict(\n    dvalid))\nauc_xgb_probably = metrics.auc(fpr_xgb_probably, tpr_xgb_probably)\nauc_xgb_probably","4c2f9fa0":"# Logistic Regression\nX = enhanced_responses[features].values\ny = enhanced_responses['probably'].values\nX[np.isnan(X)] = -1\nscaler = StandardScaler()\nX = scaler.fit_transform(X)\nXtr, Xv, ytr, yv = train_test_split(X, y, test_size=0.25, random_state=42)\n\nlr = LogisticRegression(solver='lbfgs')\nlr.fit(Xtr, ytr)\nfpr_lr_probably, tpr_lr_probably, _ = metrics.roc_curve(yv, lr.predict_proba(Xv)[:, 1])\nauc_lr_probably = metrics.auc(fpr_lr_probably, tpr_lr_probably)\nauc_lr_probably","257cd79b":"# Random Forest\nX = enhanced_responses[features].values\ny = enhanced_responses['probably'].values\nX[np.isnan(X)] = -999\nXtr, Xv, ytr, yv = train_test_split(X, y, test_size=0.25, random_state=42)\nrf = RandomForestClassifier(\n    n_estimators=500, max_depth=10, max_features=0.3,\n    min_samples_split=5, n_jobs=3, verbose=1)\nrf.fit(Xtr, ytr)\nfpr_rf_probably, tpr_rf_probably, _ = metrics.roc_curve(yv, rf.predict_proba(Xv)[:, 1])\nauc_rf_probably = metrics.auc(fpr_rf_probably, tpr_rf_probably)\nauc_rf_probably","145306da":"# XGB\nX = enhanced_responses[features].values\ny = enhanced_responses['definitely'].values\nXtr, Xv, ytr, yv = train_test_split(X, y, test_size=0.25, random_state=42)\nprint(Xtr.shape, Xv.shape, y.mean())\nfs = ['f%i' % i for i in range(len(features))]\ndtrain = xgb.DMatrix(Xtr, label=ytr, feature_names=fs)\ndvalid = xgb.DMatrix(Xv, label=yv, feature_names=fs)\nxgb_definitely = xgb.train(config, dtrain, 500, [(dtrain, 'train'), (dvalid, 'valid')],\n                           early_stopping_rounds=20, maximize=True, verbose_eval=50)\nfpr_xgb_definitely, tpr_xgb_definitely, _ = metrics.roc_curve(yv, xgb_definitely.predict(dvalid))\nauc_xgb_definitely = metrics.auc(fpr_xgb_definitely, tpr_xgb_definitely)\nauc_xgb_definitely","f332f51b":"data = [\n    go.Scatter(x=fpr_xgb_probably, y=tpr_xgb_probably, mode='lines',\n               name='XGB probably', line=dict(width=4), opacity=0.8),\n    go.Scatter(x=fpr_xgb_definitely, y=tpr_xgb_definitely, mode='lines',\n               name='XGB definitely', line=dict(width=4), opacity=0.8),\n    go.Scatter(x=fpr_lr_probably, y=tpr_lr_probably, mode='lines',\n               name='LR probably', line=dict(width=4), opacity=0.8),\n    go.Scatter(x=fpr_rf_probably, y=tpr_rf_probably, mode='lines',\n               name='RF probably', line=dict(width=4), opacity=0.8),\n]\nlayout = go.Layout(\n    title='Similar classifier performance (AUC ~ 0.78)',\n    xaxis=dict(title='FPR', ticklen=5, zeroline=False, gridwidth=2),\n    yaxis=dict(title='TPR', ticklen=5, gridwidth=2),\n    showlegend=True,\n    height=700,\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='users')","a81187ca":"competitions = pd.read_csv(os.path.join(META_DATA_PATH, 'Competitions.csv'))\ncompetitions = competitions.rename(columns={'Id': 'CompetitionId'})\ncompetitions = competitions[competitions.HostSegmentTitle != 'InClass']\ncompetitions = competitions[competitions.EvaluationAlgorithmAbbreviation == 'AUC']\ncompetitions = competitions[competitions.RewardType == 'USD']\ncompetitions = competitions[competitions.FinalLeaderboardHasBeenVerified]\ncompetitions = competitions[competitions.CanQualifyTiers]\ncompetitions = competitions[competitions.RewardQuantity > 1000]\ncompetitions = competitions[competitions.TotalTeams > 200]\ncompetitions = competitions[['CompetitionId', 'Slug', 'Title', 'DeadlineDate',\n                             'RewardQuantity', 'TotalTeams']]\ncompetitions.shape\ncompetitions.sort_values(by='TotalTeams', ascending=False)","54e09147":"submissions = pd.read_csv(os.path.join(META_DATA_PATH, 'Submissions.csv'),\n                          usecols=['TeamId', 'IsAfterDeadline', 'PrivateScoreFullPrecision'],\n                          low_memory=False)\nsubmissions = submissions[~submissions.IsAfterDeadline]\nsubmissions['PrivateScoreFullPrecision'] = submissions.PrivateScoreFullPrecision.astype(np.float64)\nsubmissions = submissions[submissions.PrivateScoreFullPrecision < 0.999]\nsubmissions = submissions[submissions.PrivateScoreFullPrecision > 0.5]\nbest_submissions = submissions.groupby('TeamId')[['PrivateScoreFullPrecision']].max().reset_index()\n\nteams = pd.read_csv(os.path.join(META_DATA_PATH, 'Teams.csv'),\n                   usecols=['Id', 'CompetitionId', 'PrivateLeaderboardRank'])\nteams = teams.rename(columns={'Id': 'TeamId'})\nteam_results = teams.merge(best_submissions, on='TeamId')\nbest_competition_results = team_results.groupby('CompetitionId')[['PrivateScoreFullPrecision']].max().reset_index()\ncompetitions = competitions.merge(best_competition_results, on='CompetitionId')\ncompetitions = competitions.sort_values(by='PrivateScoreFullPrecision')\ncompetitions","25084186":"results = pd.DataFrame({\n    'Title': list(competitions.Title.values) + ['Data Science Confidence'],\n    'AUC': list(competitions.PrivateScoreFullPrecision.values) + [auc_xgb_probably]\n})\nresults = results.sort_values(by='AUC')\nresults.index = np.arange(len(results))\n\ndata = [\n    go.Bar(\n        y=results['AUC'].values,\n        x=results.index,\n        marker=dict(\n            color=['black' if title == 'Data Science Confidence' else '#00BBFF' for title in results.Title.values],\n        ),\n        text=results.Title.values,\n    )\n]\nlayout = go.Layout(\n    autosize=True,\n    title='Most kaggle competitions are easier than predicting Data Science Confidence :)',\n    hovermode='closest',\n    xaxis=dict(title='Kaggle competitions', ticklen=0, zeroline=False, gridwidth=0),\n    yaxis=dict(title='AUC', ticklen=5, gridwidth=2),\n    showlegend=False\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='KaggleCompetitionBenchmark')","7cc42d7e":"question_desc = pd.DataFrame([\n    ['Q1', 'Gender'],\n    ['Q10', 'ML employer'],\n    ['Q11', 'Work activity'],\n    ['Q12', 'Primary tool'],\n    ['Q13', 'IDE'],\n    ['Q14', 'Hosted notebook'],\n    ['Q15', 'Cloud computing service'],\n    ['Q16', 'Programming languages'],\n    ['Q17', 'Top language'],\n    ['Q18', 'Recommended language'],\n    ['Q19', 'ML framework'],\n    ['Q2', 'Age'],\n    ['Q20', 'Top ML library'],\n    ['Q21', 'Visualization'],\n    ['Q22', 'Top visualization'],\n    ['Q23', 'Code%'],\n    ['Q24', 'Analysis experience'],\n    ['Q25', 'ML experience'],\n    ['Q26', 'DS confidence'],\n    ['Q27', 'Cloud computing product'],\n    ['Q28', 'ML product'],\n    ['Q29', 'RDB'],\n    ['Q3', 'Country'],\n    ['Q30', 'Big Data'],\n    ['Q31', 'Data type'],\n    ['Q32', 'Top data type'],\n    ['Q33', 'Public datasets'],\n    ['Q34', 'Cleaning%'],\n    ['Q35', 'Self-taught%'],\n    ['Q36', 'Online Platform'],\n    ['Q37', 'Top online platform'],\n    ['Q38', 'Media source'],\n    ['Q39', 'MOOC'],\n    ['Q4', 'Education'],\n    ['Q40', 'Independent project'],\n    ['Q41', 'Fairness'],\n    ['Q42', 'Metrics'],\n    ['Q43', 'Unfair bias%'],\n    ['Q44', 'Unfair bias difficulty'],\n    ['Q45', 'Model insights circumstances'],\n    ['Q46', 'Model insights%'],\n    ['Q47', 'Model insights methods'],\n    ['Q48', 'Black box'],\n    ['Q49', 'Reproducibility tools'],\n    ['Q5', 'Major'],\n    ['Q50', 'Reproducibility barriers'],\n    ['Q6', 'Role'],\n    ['Q7', 'Industry'],\n    ['Q8', 'Experience'],\n    ['Q9', 'Compensation'],\n], columns=['Q', 'short_description'])\nquestion_desc['QDesc'] = question_desc['Q'] + ' - ' + question_desc['short_description']\nquestion_desc","c555cc12":"feature_importance = xgb_probably.get_fscore()\nf1 = pd.DataFrame({'f': list(feature_importance.keys()), 'xgb_imp': list(feature_importance.values())})\nf2 = pd.DataFrame({'f': fs, 'feature': features})\nfeature_importance = pd.merge(f1, f2, how='outer', on='f')\nrf_imp = f1 = pd.DataFrame({'feature': features, 'rf_imp': rf.feature_importances_})\nfeature_importance = pd.merge(feature_importance, rf_imp, how='outer', on='feature')\nfeature_importance = feature_importance.fillna(0)\nfeature_importance['xgb_imp'] = feature_importance.xgb_imp.values \/ feature_importance.xgb_imp.sum()\nfeature_importance['Q'] = feature_importance.feature.apply(lambda s: 'Q' + s.split('_')[0][1:])\nfeature_importance = feature_importance.merge(question_desc, on='Q')\nfeature_importance = feature_importance.sort_values(by='xgb_imp', ascending=False)\n\nfeature_importance.shape\nfeature_importance.head()\nfeature_importance.sum()","7100d8a0":"question_importance = feature_importance.groupby('QDesc').sum().reset_index()\nquestion_importance['imp'] = (question_importance.xgb_imp + question_importance.rf_imp) \/ 2\nquestion_importance = question_importance.sort_values(by='imp', ascending=True)\nquestion_importance.index = np.arange(len(question_importance))\nquestion_importance.head()\nquestion_importance.shape","3d61a735":"top25 = question_importance[-25:].copy()\ndata = [\n    go.Bar(\n        y=question_importance['QDesc'].values,\n        x=question_importance['xgb_imp'].values,\n        orientation='h',\n        text=top25.QDesc.values,\n        name='XGB'\n    ),\n    go.Bar(\n        y=question_importance['QDesc'].values,\n        x=question_importance['rf_imp'].values,\n        orientation='h',\n        text=question_importance.QDesc.values,\n        name='RF'\n    ),\n]\nlayout = go.Layout(\n    height=1000,\n    autosize=True,\n    title='Question Importance',\n    barmode='stack',\n    hovermode='closest',\n    xaxis=dict(title='Relative Question Importance', ticklen=5, zeroline=False, gridwidth=2, domain=[0.2, 1]),\n    yaxis=dict(title='', ticklen=5, gridwidth=2),\n    showlegend=True\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='QuestionImportance')","b291be0d":"def f2q(f):\n    return 'Q' + f.split('_')[0][1:]\n\n\nbfs_filename = '..\/input\/surveykernelexternals\/bfs_result_df.csv'\nif not os.path.exists(bfs_filename):\n    bfs_result = []\n\n    for q_exclude in tqdm(feature_importance.Q.unique()):\n        remaining_features = [f for f in features if f2q(f) != q_exclude]\n        X = enhanced_responses[remaining_features].values\n        y = enhanced_responses['probably'].values\n        X[np.isnan(X)] = -999\n        Xtr, Xv, ytr, yv = train_test_split(X, y, test_size=0.25, random_state=42)\n        rf_bfs = RandomForestClassifier(n_estimators=500, max_depth=10, max_features=0.3,\n                                        min_samples_split=5, n_jobs=3, verbose=0)\n        _ = rf_bfs.fit(Xtr, ytr)\n        fpr_rf_bfs, tpr_rf_bfs, _ = metrics.roc_curve(yv, rf_bfs.predict_proba(Xv)[:, 1])\n        auc_rf_bfs = metrics.auc(fpr_rf_bfs, tpr_rf_bfs)\n        bfs_result.append([q_exclude, len(remaining_features), auc_rf_bfs])\n\n    bfs_result_df = pd.DataFrame(bfs_result, columns=['Q', 'n_features_bfs', 'auc_bfs'])\n    bfs_result_df.sort_values(by='auc_bfs')\n    bfs_result_df.to_csv('bfs_result_df.csv', index=False)\nelse:\n    bfs_result_df = pd.read_csv(bfs_filename)\n\nbfs_result_df_with_desc = bfs_result_df.merge(question_desc, on='Q')\nbfs_result_df_with_desc = bfs_result_df_with_desc.sort_values(by='auc_bfs', ascending=False)","3daa770d":"data = [\n    go.Scatter(\n        y = bfs_result_df_with_desc.auc_bfs.values,\n        x = np.arange(len(bfs_result_df_with_desc)),\n        mode='markers',\n        marker=dict(\n            sizemode = 'diameter',\n            sizeref = 1,\n            size= 10,\n            color = bfs_result_df_with_desc.auc_bfs.values,\n            colorscale='Portland',\n            reversescale=True,\n            showscale=False),\n        text=bfs_result_df_with_desc['QDesc'].values,\n        name='RF BFS'\n    ),\n    go.Scatter(\n        x=np.arange(len(bfs_result_df_with_desc)),\n        y=auc_rf_probably * np.ones(len(bfs_result_df_with_desc)),\n        mode='lines',\n        line=dict(color='black', width=3, dash='dash'),\n        name='RF all'\n    )\n]\nlayout = go.Layout(\n    autosize=True,\n    title='BFS - removing each questions individually',\n    hovermode='closest',\n    xaxis= dict(title='Questions', ticklen=5,\n                showgrid=False, zeroline=False, showline=False),\n    yaxis=dict(title='Model Performance (AUC)', showgrid=False,\n               zeroline=False, ticklen=5, gridwidth=2),\n    showlegend=True,\n\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='bfs')\nbfs_result_df_with_desc.head()","d6464909":"ffs_filename = '..\/input\/surveykernelexternals\/ffs_result_df.csv'\nif not os.path.exists(ffs_filename):\n    ffs_result = []\n    best_auc = 0.\n    best_candidate =''\n    selected_questions = []\n    for n in range(10):\n        possible_questions = np.setdiff1d(feature_importance.Q.unique(), selected_questions)\n        for q_candidate in tqdm(possible_questions):\n            current_features = [f for f in features if f2q(f) in selected_questions + [q_candidate]]\n            X = enhanced_responses[current_features].values\n            y = enhanced_responses['probably'].values\n            X[np.isnan(X)] = -999\n            Xtr, Xv, ytr, yv = train_test_split(X, y, test_size=0.25, random_state=42)\n            rf_ffs = RandomForestClassifier(n_estimators=500, max_depth=10, max_features=0.3,\n                                            min_samples_split=5, n_jobs=3, verbose=0)\n            _ = rf_ffs.fit(Xtr, ytr)\n            fpr_rf_ffs, tpr_rf_ffs, _ = metrics.roc_curve(yv, rf_ffs.predict_proba(Xv)[:, 1])\n            auc_rf_ffs = metrics.auc(fpr_rf_ffs, tpr_rf_ffs)\n            ffs_result.append([n, q_candidate, len(selected_questions),\n                               len(current_features), auc_rf_ffs])\n            if auc_rf_ffs > best_auc:\n                best_auc = auc_rf_ffs\n                best_candidate = q_candidate\n        selected_questions.append(best_candidate)\n    \n    ffs_result_df = pd.DataFrame(ffs_result, columns=['n', 'Q', 'nsq', 'nsf', 'auc_ffs'])\n    ffs_result_df.sort_values(by='auc_ffs')\n    ffs_result_df.to_csv(ffs_filename, index=False)\nelse:\n    ffs_result_df = pd.read_csv(ffs_filename)\n\nffs_result_df_with_desc = ffs_result_df.merge(question_desc, on='Q')\nffs_result_df.shape\nffs_result_df.head()","619e2324":"data = [\n    go.Scatter(\n        x = ffs_result_df_with_desc.n.values,\n        y = ffs_result_df_with_desc.auc_ffs.values,\n        mode='markers',\n        marker=dict(\n            sizemode = 'diameter',\n            sizeref = 1,\n            size= 10,\n            color = ffs_result_df_with_desc.auc_ffs.values,\n            colorscale='Portland',\n            reversescale=True,\n            showscale=False),\n        text=ffs_result_df_with_desc['QDesc'].values,\n        name='RF FFS'\n    ),\n    go.Scatter(\n        x=np.arange(10),\n        y=auc_rf_probably * np.ones(10),\n        mode='lines',\n        line=dict(color='black', width=3, dash='dash'),\n        name='RF All'\n    )\n]\nlayout = go.Layout(\n    autosize=True,\n    title='You do not really need all the features',\n    hovermode='closest',\n    xaxis= dict(title='Number of questions', ticklen= 5,\n                showgrid=True, zeroline=False, showline=False),\n    yaxis=dict(title='Model Performance (AUC)', showgrid=True,\n               zeroline=False, ticklen=5, gridwidth=2),\n    showlegend=True,\n)\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename='ffs')","6cef1add":"end = dt.datetime.now()\nprint('Latest run {}.\\nTotal time {}s'.format(end, (end - start).seconds))","c2c8ad34":"Your job title is not everything though.\n\n**Did you know that [Mikel Bober-Irizar](https:\/\/www.kaggle.com\/anokas) became kaggle grandmaster at 17 years old?**","822e2689":"## Probably\nWe use binary classification (1 for Probably or Definitely DS and 0 for the rest).","9c330a50":"We could try One Hot Encoding for each answer option but that would result a large sparse matrix. At this point I would prefer fewer compact features.\nWith some manual effort we reorder the answers. Another trick would be to use mean target encoding.","19e6ff01":"We have roughly 4K Maybe- Probably-Definitely DS.  That is strange since based on the job title we have only 4K respondents who has currently data scientist role.\n\nLet's look at the first row!","7748aa41":"# Summary\n\nWhen I took the survey one of the most surprising question was:\n\n**Q26 : Do you consider yourself to be a data scientist? (Definitely not,  Probably not, Maybe, Probably yes, Definitely yes) **\n\nAlthough I have seven years work experience in data analytics\/data science and have dozens of successful data science competitions under my belt, I had to stop for moment and hesitated before clicking on definitely yes. I never really liked the term *data science*, it was not exactly well defined in the first place and imho it got quite meaningless during the hype of the last few years. If I have to explain what I do I usually go with data analytics and machine learning.\n\nThere are lots of articles out there about fake vs. true data scientists or data analysts vs. data scientists or even data scientists vs statisticians. \nYou could probably draw a 3 to 5 dimensional Venn diagram about data science or at least you should know that it should be the sexiest job...\n\n### Definition (Data Scientist)\n*\"A data scientist is an individual that practices data science.\" * [techopedia](https:\/\/www.techopedia.com\/definition\/30202\/data-science)\n\nThanks that's very useful :) Let's rather dig into the survey data and find out what do you think about data scientists!\nAnyway you should not care about data science anymore AI is next hype :D\n\n\nIn the following sections we will explore the data in order to try to predict this specific question. We start with exploratory analysis and feature extraction then we continue with modeling. In the end we finish with some model interpretation methods. Most of the code blocks are hidden by default. Enjoy!\n\n# Exploratory Data Analysis ","68135d9a":"## Categorical Features\n\nLet's add features for the categorical answers (e.g. Country or current Role).\n\nFor multiple possible choice we count the number of different answers. One with many different used machine learning techniques might be more confident.\n\nLot's of the columns has only one single value we use binary features for them.\nThe columns with too high cardinality are dropped the rest is One Hot Encoded.\n","ebbe5d55":"The questions are sorted by the total RF and XGB importance. Note that these results could be quite different because XGBoost is an iterative boosted learner while Random Forest builds each tree separately. \n\nThe most important questions are **Work**  (Role, Cleaning%, Self-Thaught%, Activity) and **Machine Learning** (Experience, Frameworks, Model Interpretability) related.\n\nAge and Gender are among the least important features. At least in Data Science Confidence there is not any gender bias.\n\nMajor and Industry is not that important either. Actually, this is one my favorite aspect of the field of data science. I had lots of colleagues with different backgrounds and I had a chance to work in several different industries.","a5d57548":"We find that removing Role, Code% and ML XP would hurt the model.\n\nOn the other hand, we find that removing Q34 - Cleaning% or Q35 - Self-taught% would slightly increase the performance. Wait! We have seen previously that these features were \"important\". Well if you debug the code you might find that we accidentally handled the free form % answers as multiple choice questions and OHE-ed them. Which resulted hundreds of not too useful features :)\n\n\n## Forward Feature Selection\n\nLet's assume your product manager asks you to build production ready model to predict data science confidence based on these survey questions. However the UX team done some research and found that people do not like to answer more than 10 questions :)\n\nInstead of trying each possible subsets we could use greedy FFS to try to add features one by one. It still needs some time to run but at the end we find out that we could build the model with only 10 questions without performance loss.","1c4a45f4":"Ok I did not expect that answer :)\n\n## Experience and Job title\n\nMy first intuition was that experience and job title are probably the most important factors.\nIndeed, two years machine learning or three years data analytics experience increases your confidence to admit that you are a data scientist.\n\n\n","58d42e1a":"# A really pretty map\n\nWhenever the dataset has country codes it is too tempting to draw a map. Anyway, every kernel (marketing dashboard) deserves a pretty map [3].","a77c2a29":"# Feature Extraction\n\n## Ordinal Features\nThe survey has mainly multiple-choice questions with occasional additional free form text fields.\nThe answers were collected as strings however many of the questions had a clear ordinal scale (e.g. What is your age).\nThe default lexicographic order is not really useful:","930d3f23":"# Modeling\n\nCan we predict Data Science Confidence based on the other survey questions?\nWe will try Random Forest, Logistic Regression and XGBoost.","1a7fd793":"## How bad is the model?\n\nOk we got 0.78 AUC. It is far from random guesses (AUC ~ 0.5) but also far from a perfect model (AUC ~ 1.0).\n\nKaggle has already hosted dozens of competitions with AUC evaluation metric. Let's select a few and collect the best Leaderboard scores for them.\n\nDid you know there is a [meta kaggle dataset](https:\/\/www.kaggle.com\/kaggle\/meta-kaggle\/home) with all sorts of public data about kaggle users, competitions, kernels and discussions?","ad24b290":"# Feature Importance\n\nRandomForest and XGB provide convenient feature importance results. With the individual feature importance we could aggregate relative question importance.\n\nNote that the feature importance result is just a heuristic. It is calculated based on the number of times a feature is used to split the data across all trees.","c539b40a":"## Definitely\n\nWe use binary classification (1 for Definitely DS and 0 for the rest).\n\nThe models reach similar performance on this dataset. XGBoost might give you higher leaderboard score as usual.","f2ffa93c":"Russia is quite confident (65%) maybe ods.ai skewed the results :) Japan is one example for the less confident countries (43%). I know a lot of great kaggle competitors from both countries.\nI would not draw too much conclusions from the map especially for the smaller countries where we have only a few dozen respondents.","c9032073":"Note that there are quite a few competitions with almost perfect AUC. If you would like to inject leakage to your Data Science Confidence prediction model, just add OHE-Q26 features. It's easy to reach 1.0 with them :)","6a81d15a":"# Feature Selection\nFeature importance results might be tricky. They are based on how many times the feature is used but that does not necessary mean that the feature is necessary.\n\n## Backward Feature Selection\nOne simple experiment is to remove each question individually and check how it affects the model performance.","2c6421e6":"One advantage of having numerical features instead of categorical that we can quickly draw a correlation matrix. \n\nAmong the reordered variables machine learning experience has the strongest correlation (still pretty weak!) with our target. The time spent with actual coding and insights are also important. \nSalary is the least important factor even Unfair Bias [2] has stronger correlation with data science confidence.\n\nThe percentage of online courses has negative correlation with the target. If you had just finished an online course, you are probably not a data scientist (yet).\n\nNote that we used Pearson correlation you could try Spearman's rank correlation. It might work better for ordinal features.\n\nBtw the [Datasaurus example](https:\/\/www.autodeskresearch.com\/publications\/samestats) shows that you should not draw too much conclusions based on a raw correlation matrix [4].","ffbd27b5":"# References\n[1] I noticed that @vfdev had the same idea and started to investigate this question too. You should check his [kernel](https:\/\/www.kaggle.com\/vfdev5\/who-are-they-data-scientists).\n\n[2] [Amazon ditched AI recruiting tool that favored men for technical jobs](https:\/\/www.theguardian.com\/technology\/2018\/oct\/10\/amazon-hiring-ai-gender-bias-recruiting-engine)\n\n[3] https:\/\/marketoonist.com\/\n\n[4] https:\/\/www.autodeskresearch.com\/publications\/samestats"}}