{"cell_type":{"1192a3da":"code","2ece2ce6":"code","d2471d21":"code","401e32b2":"code","ae98994b":"code","c4a301c0":"code","59be391a":"code","01c92a59":"code","1523d2d6":"code","eaa83293":"code","4eed85b8":"code","e6fa0910":"code","5e93b9d6":"code","bae7569a":"code","00bc33d9":"code","f72084ed":"code","34036cdf":"code","84267854":"code","5e6ff62b":"code","997d446f":"code","84b98197":"code","6e792a18":"code","bb5c82a6":"code","5acced93":"code","fcbcf951":"code","56c55407":"code","5d095ac0":"code","cabcc222":"code","a8509aba":"code","468e9316":"code","981ef2f5":"code","49b9f725":"code","08843ba0":"code","3a988f88":"code","abf7293f":"code","feb75bcb":"code","44c3c3fe":"code","3d62e538":"code","5b30b8ff":"code","1c2d4a88":"code","ed5a7c97":"markdown","e82aded6":"markdown","c8057165":"markdown","1ddf33ff":"markdown","05008cce":"markdown","903db59d":"markdown","3cdeed2d":"markdown","b0b07db1":"markdown","aaa7df81":"markdown","b253211d":"markdown","11863b41":"markdown","aea4f275":"markdown","9dfda8bd":"markdown","f6178c9a":"markdown","4971bece":"markdown","7faad572":"markdown","4e38b4e6":"markdown","a309e32d":"markdown","de8c25f1":"markdown","cedaa16c":"markdown","e747f154":"markdown","613e6554":"markdown","368b543e":"markdown","1cd5c674":"markdown","a496ffba":"markdown","c8016c90":"markdown","f5867054":"markdown","49313f99":"markdown","a29f388d":"markdown","d7d4ad5a":"markdown","ac22cfc3":"markdown"},"source":{"1192a3da":"import pandas as pd\nimport numpy as np","2ece2ce6":"#df = pd.read_csv(\"..\/input\/videogamesales\/vgsales.csv\")\ndf = pd.read_csv('..\/input\/clean-datav3\/clean_dates.csv') #To not repeat the fixing dates functions\nprint(\"Dataset shape : \"+str(df.shape))\ndf.head()","d2471d21":"df.isna().sum()","401e32b2":"!pip install selenium\n!apt-get install -y libgtk-3-0 libdbus-glib-1-2 xvfb\n!pip install webdriverdownloader","ae98994b":"!cat \/etc\/os-release\n!ls -l .\n!echo \"ls -l \/kaggle\"\n!ls -l \/kaggle\n\n!echo \"\\nls -l \/kaggle\/working\"\n!ls -l \/kaggle\/working\n!ls -l \"..\/input\"\n!mkdir \"..\/working\/firefox\"\n","c4a301c0":"!ls -l \"..\/working\"\n!cp -a \"..\/input\/firefox-63.0.3.tar.bz2\/firefox\/.\" \"..\/working\/firefox\"\n!ls -l \"..\/working\/firefox\"\n!chmod -R 777 \"..\/working\/firefox\"\n!ls -l \"..\/working\/firefox\"\n","59be391a":"# INSTALL LATEST VERSION OF THE WEB DRIVER\nfrom webdriverdownloader import GeckoDriverDownloader\ngdd = GeckoDriverDownloader()\ngdd.download_and_install(\"v0.23.0\")","01c92a59":"!apt-get install -y libgtk-3-0 libdbus-glib-1-2 xvfb\n!export DISPLAY=:99","1523d2d6":"from bs4 import BeautifulSoup\nimport re\nimport requests\nfrom datetime import datetime\nimport time\nfrom selenium import webdriver as selenium_webdriver\nfrom selenium.webdriver.firefox.options import Options as selenium_options\nfrom selenium.webdriver.common.desired_capabilities import DesiredCapabilities as selenium_DesiredCapabilities\nbrowser_options = selenium_options()\nbrowser_options.add_argument(\"--headless\")\nbrowser_options.add_argument(\"--window-size=1920,1080\")\n\ncapabilities_argument = selenium_DesiredCapabilities().FIREFOX\ncapabilities_argument[\"marionette\"] = True\n\nbrowser = selenium_webdriver.Firefox(\n    options=browser_options,\n    firefox_binary=\"..\/working\/firefox\/firefox\",\n    capabilities=capabilities_argument\n)\nbrowser.set_window_size(1366, 768)","eaa83293":"#Request-html we're not using it to delete later\n!pip install requests-html\nfrom requests_html import HTMLSession\nfrom requests_html import AsyncHTMLSession\nsession = HTMLSession()\nr = session.get(\"https:\/\/www.google.com\/search?q=Release date of Super Mario Bros.\")\ns = r.html.find('#main', first=True)","4eed85b8":"url = \"https:\/\/www.google.com\/search?q=Release date of \"\n#url =  \"https:\/\/en.wikipedia.org\/wiki\/\"\nfixed = 0\nnot_fixed = 0\niteration = 0\nstart_time = time.time()","e6fa0910":"def find_date(game_name,url_ext=\"\"):\n    global fixed,not_fixed,iteration\n    if ':' in game_name:\n        game_name = game_name.split(':')[0]\n    answer = False\n    print(\"Looking Released year of : \"+game_name + \" From the platform :\"+url_ext)\n    iteration += 1\n    current_time = time.time()\n    if (iteration % 50 == 0):\n        print('*'*80)\n        print('Iteration : '+str(iteration)+' Seconds passed : '+ str(current_time-start_time))\n        print('We have : '+str(fixed) +' date fixed and '+str(not_fixed)+str(' not fixed'))\n        print('*'*80)\n    #browser.get(url+str(game_name))  #For known video games\n    browser.get(url+str(game_name)+\" \"+str(url_ext))\n    title = browser.title\n    for i in range(5): #Here i loop with because some of them are not having same number of divs , even tho still having problems \n        search = browser.find_elements_by_xpath('\/\/*[@id=\"rso\"]\/div[1]\/div[1]\/div[1]\/div[1]\/div\/div['+str(i)+']\/div\/div[1]\/div\/div\/div[1]')\n        if search :\n            answer = search[0].text       \n            if ',' in answer :\n                answer = answer.split(',')[1]   \n                #Here we need to split by \\ and take the first part , because some of them will be like 2005\\nUsa\n            if '\\n' in answer :    \n                answer = answer.split('\\n')[0]\n            try:\n                answer = str(re.search(r'\\d+', answer).group())\n            except:\n                pass\n            if len(answer) > 0:\n                fixed += 1\n                answer = float(answer)\n                print(\"Found year : \"+str(answer))\n            break\n        else :\n            continue\n    if not answer :\n        not_fixed += 1\n        print(\"Not found :(\")\n        answer = None\n    return answer\n\n","5e93b9d6":"df[df['Year'].isna()]","bae7569a":"find_date(\"Maze Craze: A Game of Cops 'n Robbers\",url_ext=\"2600\")","00bc33d9":"def find_date2(game_name):\n    global fixed,not_fixed,iteration\n    #print(\"Looking Released year of : \"+game_name)\n    iteration += 1\n    current_time = time.time()\n    if (iteration % 50 == 0):\n        print('*'*80)\n        print('Iteration : '+str(iteration)+' Seconds passed : '+ str(current_time-start_time))\n        print('We have : '+str(fixed) +' date fixed and '+str(not_fixed)+str(' not fixed'))\n        print('*'*80)\n    pagex = requests.get(\"https:\/\/en.wikipedia.org\/wiki\/\"+game_name)\n    soup = BeautifulSoup(pagex.content, 'html.parser')\n    result = soup.find(id='content')\n    job_elems = result.find_all('div', class_='plainlist')\n    #print(str(job_elems))\n    x = re.search(r'\\d{4}', str(job_elems))\n    if x:\n        date = float(datetime.strptime(x.group(), '%Y').date().year)\n        #print(date.year)\n        fixed += 1\n    else :\n        date = None\n        not_fixed += 1\n    return date","f72084ed":"find_date(\"Wii Sports\")","34036cdf":"find_date2(\"The Chronicles of Narnia: The Lion, The Witch and The Wardrobe\")","84267854":"# for i in range(200):\n#     game_name = df.Name[i]\n#     print(game_name)\n#     pagex = requests.get(\"https:\/\/en.wikipedia.org\/wiki\/\"+game_name)\n#     soup = BeautifulSoup(pagex.content, 'html.parser')\n#     result = soup.find(id='content')\n#     job_elems = result.find_all('div', class_='plainlist')\n#     #print(str(job_elems))\n#     x = re.search(r'\\d{4}', str(job_elems))\n#     print(x)\n#     if x:\n#         date = float(datetime.strptime(x.group(), '%Y').date().year)\n#         print(date)\n#         fixed += 1\n#     else :\n#         date = None\n#         not_fixed += 1","5e6ff62b":"# df['Year'] = df.apply(lambda row : find_date(row['Name'],row['Platform']) if np.isnan(row['Year']) else row['Year'],axis=1) #Selenium\n# print('By First function :')\n# print(\"Dates fixed : \"+str(fixed))\n# print(\"Dates not fixed : \"+str(not_fixed))\n# iteration =  0\n# fixed = 0\n# not_fixed = 0\n# df['Year'] = df.apply(lambda row : find_date2(row['Name']) if np.isnan(row['Year']) else row['Year'],axis=1) #BSoup\n# print('By Second function :')\n# print(\"Dates fixed : \"+str(fixed))\n# print(\"Dates not fixed : \"+str(not_fixed))\n# browser.close() #To close the browser we used to scrap , you can comment this no need","997d446f":"df.isna().sum()","84b98197":"df.to_csv('clean_dates.csv',index=False) #U need to download it then upload it into inputs since we have right read-only in cmds","6e792a18":"df['Year'] = df['Year'].fillna(0.0) #To delete later when u fix the last missing dates , just so we can visualize","bb5c82a6":"df['Year'] = df['Year'].astype(int)\ndf['Global_Sales'] = df['Global_Sales'].astype(int)\ndf['NA_Sales'] = df['NA_Sales'].astype(int)\ndf['Year'] = df['Year'].apply(pd.to_numeric, errors='ignore')\ndf.info()\n","5acced93":"import matplotlib.pyplot as plt\nimport seaborn as sns","fcbcf951":"NA_SALES = df['NA_Sales'].sum()\nEU_SALES = df['EU_Sales'].sum()\nJP_SALES = df['JP_Sales'].sum()\nOTHER_SALES = df['Other_Sales'].sum()\nX = ['North America','Europe','Japan','Other']\nY = [NA_SALES,EU_SALES,JP_SALES,OTHER_SALES ]\nplt.bar(X,Y,color=('orange','purple','darkolivegreen','pink'))\nplt.xlabel(\"Countries\")\nplt.ylabel(\"Price in (in millions)\")","56c55407":"df.head()","5d095ac0":"sales_genre_platform = df[['Global_Sales','Genre','Platform']]\ngrouped = sales_genre_platform.groupby('Genre').agg({'Global_Sales':'sum','Platform':lambda x:x.value_counts().index[0]})\ngrouped.reset_index(inplace=True)\ngrouped.head()","cabcc222":"plt.figure(figsize=(1520,800))\nplot = sns.relplot(x=\"Genre\", y=\"Global_Sales\", hue=\"Platform\", data=grouped,height=8.27, aspect=11.7\/8.27,s=90);\nplt.title(\"Games Sold by Genre and Platform\")\nplot.set(xlabel=\"Genre of games\",ylabel='Sales in Millions')","a8509aba":"sales_platform = df[['Global_Sales','Platform']]\ngrouped2 = sales_platform.groupby('Platform').sum().sort_values(by='Global_Sales',ascending=False)\ngrouped2.reset_index(inplace=True)\nplt.figure(figsize=(20,7))\ngrouped2 = grouped2.head()\nplt.bar(grouped2.Platform,grouped2.Global_Sales,color=('orange','purple','darkolivegreen','pink','darkblue'))\nplt.xlabel('Platforms')\nplt.ylabel('Sales in millions $')\ngrouped2.head()","468e9316":"year_plat_sales = df[['Year','Platform','Global_Sales']]\nyear_plat_sales = year_plat_sales[year_plat_sales['Year'] != 0] #Drop uknow year rows\nyear_plat_sales.loc[(year_plat_sales.Platform == '2600'),'Platform']='Atari 2600'#2600 means the Atari 2600 so better make it clear\n\n#grouped3 = year_plat_sales.groupby(['Year','Platform'])[['Global_Sales']].sum() #To remember used to grouped by and exlude some columns but not usefull here\ngrouped3 = year_plat_sales.groupby('Year').agg({'Global_Sales':'sum','Platform':lambda x:x.value_counts().index[0]})\n#As we did before for the platform we're using only more frequent not the best solution but we want to analyse the different sales in each platform we'll need to treat each year separatly so we can read the graph\ngrouped3.reset_index(inplace=True)\nplt.figure(figsize=(20,7))\n\nplot = sns.lineplot(x=\"Year\", y=\"Global_Sales\",\n             hue=\"Platform\", \n             data=grouped3)\ngrouped3['Platform'].unique()","981ef2f5":"year_country_sales = df[['Year','NA_Sales','EU_Sales','JP_Sales','Other_Sales']]\nyear_country_sales = year_country_sales[year_country_sales['Year'] != 0] #Drop uknow year rows\ngrouped4 = year_country_sales.groupby('Year').sum() #Group By year\ngrouped4.reset_index(inplace=True)\nplt.figure(figsize=(20,7))\ngrouped4 = grouped4.rename(columns={'NA_Sales': 'Sales1', 'EU_Sales': 'Sales2','JP_Sales':'Sales3','Other_Sales':'Sales4'})\n#Here i rename the columns so i change change the format of the data\ngrouped4 = pd.wide_to_long(grouped4,stubnames='Sales',i='Year',j='Country') #Long format is usefull here to plot our graph \ngrouped4.reset_index(inplace=True)\ngrouped4['Country'] = grouped4['Country'].replace([1,2,3,4], ['NA','EU','JP','Other']) #Because i had to make them numbers to use the function wide_to_long\nplot = sns.lineplot(x=\"Year\", y=\"Sales\",\n             hue=\"Country\", \n             data=grouped4)","49b9f725":"import pickle\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.linear_model import LinearRegression\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import r2_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import model_selection\nfrom sklearn.model_selection import KFold","08843ba0":"model_df = df.copy()\nmodel_df = model_df[model_df['Year'] <= 2016] #Since we dont have enough data from 2017+\nmodel_df.drop('Name',axis=1,inplace=True)","3a988f88":"feature = [\"Platform\", \"Genre\"]\nle = LabelEncoder()\n\nfor col in feature:\n    model_df[col] = le.fit_transform(model_df[col])\n\n#cat_data = pd.DataFrame(cat_data).transpose()\n#cat_data = cat_data.apply(lambda x:x.fillna(x.value_counts().index[0])) #Fill these 58 missing Publisher by most frequent\n#Here we can do it in a better way is that for each missing publisher we gonna take the platform and filter all others data that have same platform \n#then take the most frequent publisher in these data , its gonna be more accurate\nmodel_df.head()","abf7293f":"X = model_df[['Year','Genre','Platform', 'Global_Sales']].values\n#X = model_df[['Year','Genre', 'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales']].values\nY = model_df['NA_Sales'].values #We can change the prediction from here if we want\n#Maybe we can try to predict the rank also\nX","feb75bcb":"x_train,x_valid,y_train,y_valid = train_test_split(X,Y,test_size=0.2)","44c3c3fe":"xgb = XGBRegressor(random_state = 5, n_estimators = 1500, learning_rate=0.03)\nxgb.fit(x_train,y_train)\nr2_score(y_valid, xgb.predict(x_valid))","3d62e538":"lr = LinearRegression()\nlr.fit(x_train,y_train)\npredictions = lr.predict(x_valid)\n\nprint(lr.score(x_valid, y_valid))\nr2_score(y_valid,predictions)","5b30b8ff":"lr_kfold = LinearRegression()\nkfold = model_selection.KFold(n_splits=10, random_state=100)\nresults_kfold = model_selection.cross_val_score(lr_kfold, x_train, y_train, cv=kfold)\nprint(\"Accuracy: %.2f%%\" % (results_kfold.mean()*100.0)) ","1c2d4a88":"test=  ['Action','PS2']\ntest = le.fit_transform(test)\ntest = np.insert(test,0,2006)\ntest = np.append(test,65)\ntest = np.array([test])\n\nlr.predict(test)[0]","ed5a7c97":"##### As we can see the most sold games are action video games on PS3 (This is gotta be an old data )","e82aded6":"### Now we gonna use our function to fill all the na  values in our column \"Year\" by scrapping the correct value from google.","c8057165":"### Preparing Our data for applying a machine learning algorithm","1ddf33ff":"The two first functions are going to fix 173 missing dates and we're going to save them in clean_data.csv","05008cce":"# **Data Visualization**","903db59d":"![image.png](attachment:image.png)","3cdeed2d":"### We'll need to vectorize the caregorical data","b0b07db1":"### Now let's see the different sales in countries by years","aaa7df81":"### Our functions to scrap releasing dates","b253211d":"### That doesn't mean that games are sold more in NA , but NA made most millions by selling video games than other countries","11863b41":"### The dataset was using Float form in date , i decided to make them int better ( U can leave them float not a big deal)","aea4f275":"### Using BeautifulSoup (By Omar el Yousfi)","9dfda8bd":"### Let's Encode our categorical Data","f6178c9a":"*We gonna consider that the sales are made the same year as realizing date*","4971bece":"### Let's Try to visualize the number of sales in each country , then adding the platform , publisher , genre in our visualization","7faad572":"### Using Cross-Validation to improve the accuracy","4e38b4e6":"## Preparing Selenium and webbrowser to use in kaggle (you can skip the following commands just for installation )","a309e32d":"# **Model Creation**","de8c25f1":"## Dealing with missing data","cedaa16c":"What we can conclude from that is that most of people who plays video games on pc are not buying the game ( Using cracks ) , However on ps3 even that there is cracks but the facts that you can play online anymore after cracking a game was pushing people to buy the games .\nAnd also Buying a playstation3 was cheaper than buying a pc that can turns ps3 games","e747f154":"### Testing Our fucntion :","613e6554":"### Here , we're aggregating the column Global_sales by sum so we can have the sum of sales for each genre , and the most frequent platform sold for this same game","368b543e":"Saving the data so we wont execute the function everytime.","1cd5c674":"# **Data Pre-processing**","a496ffba":"#### We'll be using 2 models","c8016c90":"### As you can see still lot of missing dates , its very hard to scrap the values from google because the xpath or class are constatly changing , however we filled 106 missing dates from 271 better than nothing","f5867054":"### The next analysis we'll try to use the release date to analyse some features","49313f99":"## The sales are falling down in 2018+ because we don't have these news data , to fix that let's predict the prices in 2020 based on old data untill 2016 (Not done yet )","a29f388d":"### Importing libraries and preparing browser to use with selenium","d7d4ad5a":"### To fix these missing data i'm going to scrap the information from google research using **BeautifulSoup** and **requests**\n","ac22cfc3":"#### Using Kfold Cross_Validation (no big changes )"}}