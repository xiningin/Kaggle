{"cell_type":{"654d4a8f":"code","f2ce5f77":"code","6efda7ca":"code","a4ae29d5":"code","7f9f9f31":"code","5e9aa161":"code","d427ab65":"code","d6286955":"code","ef985b30":"code","b6f29276":"code","3f97b173":"code","7d537fcb":"code","7588968a":"code","59f8b73f":"code","1fba851b":"code","8c81489c":"code","217f6296":"code","023bb6e6":"code","f91cb732":"code","42130457":"code","04fafb36":"code","87a24a82":"code","ac9fce45":"markdown","938acf51":"markdown","f8d8c64b":"markdown","895e0524":"markdown","55b0b5b1":"markdown","c6a55720":"markdown","7c9fc564":"markdown","df3a3648":"markdown","4c066222":"markdown","40588458":"markdown","c831f20f":"markdown","16920539":"markdown","4ff50235":"markdown","f3c66926":"markdown","dca2e174":"markdown","c85f7430":"markdown","25ae2b70":"markdown","ab10ba0c":"markdown","53646c08":"markdown","136961c2":"markdown","8e98904d":"markdown","0295ee0b":"markdown","89e40bc1":"markdown","318717de":"markdown","a310c385":"markdown","f888eb2f":"markdown","38fcba0e":"markdown","4d14b919":"markdown","b6381beb":"markdown","d23a0400":"markdown","45efe080":"markdown","cfb88fa1":"markdown","fdd5413d":"markdown"},"source":{"654d4a8f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport spacy # NLP\nimport re # regular expressions\nimport html # HTML content, like &amp;\nfrom spacy.lang.en.stop_words import STOP_WORDS # stopwords\n\nnlp = spacy.load('en_core_web_lg') #Load spacy, up here so I do not have to load it constantly\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f2ce5f77":"!pip install -U sparkmagic #Install or update sparkmagic, for spark jupyter displays\n!pip install -U pyspark #Install or update pyspark","6efda7ca":"from pyspark.sql.types import * #Import spark types\nfrom pyspark.sql.functions import * #Import spark functions\n\nimport pyspark\nfrom pyspark.sql import SparkSession #Import the spark session\nfrom pyspark import SparkContext #Create a spark context\nfrom pyspark.sql import SQLContext #Create an SQL context\n\nfrom pyspark.ml.feature import Tokenizer #Used to tokenize the tweet data\nfrom pyspark.ml.feature import CountVectorizer #Used to make the data into vectors\nfrom pyspark.ml import Pipeline #Build a pipeline\nfrom pyspark.ml.classification import RandomForestClassifier #The chosen classifier\nfrom pyspark.ml.evaluation import MulticlassClassificationEvaluator #Metrics\n\nconf = pyspark.SparkConf().setAll([('spark.executor.memory', '16g'), ('spark.executor.cores', '1'), ('spark.cores.max', '1'), ('spark.driver.memory','16g')])\nsc = SparkContext.getOrCreate(conf = conf) #Initialize the spark context\nsqlContext = SQLContext.getOrCreate(sc) #Create an SQL Context\nspark = SparkSession.builder.master(\"local[*]\").getOrCreate() #Make a spark session","a4ae29d5":"tweets = spark.read.csv(\"..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv\", inferSchema = True, header = False) #Read in the data\ntweets.show(10) #Show the first 10 columns","7f9f9f31":"labels = [\"target\", \"id\", \"date\", \"flag\", \"user\", \"tweet\", \"cleanTweet\"] #Column names\n\n#Fix column names, as this dataset did not have a header\ntweets = tweets.select(col(\"_c0\").alias(labels[0]), col(\"_c1\").alias(labels[1]), col(\"_c2\").alias(labels[2]),\n                      col(\"_c3\").alias(labels[3]), col(\"_c4\").alias(labels[4]), col(\"_c5\").alias(labels[5]))\ntweets.show(10) #Show the dataset","5e9aa161":"#For each column, count cases where the column is NaN or Null\ntweets.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in tweets.columns]).show() #Check for null values","d427ab65":"tweets.groupBy(\"target\").count().orderBy(\"count\").show() #Check how many of each target value there is","d6286955":"#Change tweet sentiment to 0 and 1 instead of 0 and 4\ntweets = tweets.withColumn(\"target\", \\\n              when(tweets[\"target\"] == 4, 1).otherwise(tweets[\"target\"]))","ef985b30":"tweets.groupBy(\"target\").count().orderBy(\"count\").show() #Check how many of each target value there is","b6f29276":"punctuations = \"\"\"!()-![]{};:+'\"\\,<>.\/?@#$%^&*_~\u00c2\"\"\" #List of punctuations to remove, including a weird A that will not process out any other way\n\n#CleanTweets: parces the tweets and removes punctuation, stop words, digits, and links.\n#Input: the list of tweets that need parsing\n#Output: the parsed tweets\ndef cleanTweets(tweetParse):\n    length = len(tweetParse)\n    for i in range(0,length):\n        tweet = tweetParse[i] #Putting the tweet into a variable so that it is not calling tweetParse[i] over and over\n        tweet = html.unescape(tweet) #Removes leftover HTML elements, such as &amp;\n        tweet = re.sub(r\"@\\w+\", ' ', tweet) #Completely removes @'s, as other peoples' usernames mean nothing\n        tweet = re.sub(r'http\\S+', ' ', tweet) #Removes links, as links provide no data in tweet analysis in themselves\n        tweet = re.sub(r\"\\d+\\S+\", ' ', tweet) #Removes numbers, as well as cases like the \"th\" in \"14th\"\n        tweet = ''.join([punc for punc in tweet if not punc in punctuations]) #Removes the punctuation defined above\n        tweet = tweet.lower() #Turning the tweets lowercase real quick for later use\n    \n        tweetWord = tweet.split() #Splits the tweet into individual words\n        tweetParse[i] = ''.join([word + \" \" for word in tweetWord if nlp.vocab[word].is_stop == False]) #Checks if the words are stop words\n        \n    return tweetParse #Returns the parsed tweets","3f97b173":"tweetText = tweets.select(\"tweet\").collect() #Collect the tweet data\ntweetText = [str(tweet.tweet) for tweet in tweetText] #Make the tweets strings\ncleanTweet = cleanTweets(tweetText) #Clean the tweets","7d537fcb":"sentiment = tweets.select(\"target\").collect() #Collect the sentiment data\nsentiment = [int(tweet.target) for tweet in sentiment] #Make the sentiments into integers\nprint(sentiment[:10]) #Look at the list of sentiments","7588968a":"tweetSentiment = [] #A list to hold the combined tweet-sentiment pair\nlengthClean = len(cleanTweet) #Get the length for looping\n\n#For loop to combine clean tweet with its sentiment\nfor index in range(0, lengthClean):\n    tweetSentiment.append((cleanTweet[index], sentiment[index])) #Add the combined pair to the tweetSentiment list\n\nprint(tweetSentiment[:10]) #Print a couple of the pairings","59f8b73f":"tweetColumns = [\"cleanTweet\",\"trueTarget\"] #Put the column names we want to use\nsentDF = spark.createDataFrame(data = tweetSentiment, schema = tweetColumns) #Put the data into a new dataframe\nsentDF.cache() #Cashe the dataset\nsentDF.show(10) #Take a peek at the new dataset","1fba851b":"sentDF.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in sentDF.columns]).show() #Check for null values","8c81489c":"#Take a peek at the non-null counts\nprint(\"Tweet: \", sentDF.filter(isnan(sentDF.cleanTweet) == False).count(), \"\\nTarget: \", sentDF.filter(isnan(sentDF.trueTarget) == False).count())","217f6296":"sentDF = sentDF.where(isnan(sentDF.cleanTweet) == False) #Remove the null value\nsentDF.select([count(when(isnan(c) | col(c).isNull(), c)).alias(c) for c in sentDF.columns]).show() #Check for null values","023bb6e6":"Train_Test = sentDF.randomSplit([0.75, 0.25]) #Split the data 75-25\nsentTrain = Train_Test[0] #Put the train data into its own variable\nsentTest = Train_Test[1] #Put the test data into its own variable\nprint(\"Train: \", sentTrain.count(), \"\\nTest: \", sentTest.count()) #Print split numbers","f91cb732":"tokenizer = Tokenizer().setInputCol(\"cleanTweet\").setOutputCol(\"tokenTweet\") #Build the tokenizer\nvectorizer = CountVectorizer().setInputCol(\"tokenTweet\").setOutputCol(\"features\") #Build the vectors\nforest = RandomForestClassifier(labelCol = \"trueTarget\", featuresCol=\"features\", numTrees = 3, maxDepth = 16) #Build the forest classifier\nmlPipe = Pipeline(stages = [tokenizer, vectorizer, forest]) #Build the pipeline to do all above steps","42130457":"#model = mlPipe.fit(sentTrain) #Fit the data","04fafb36":"#predictions = model.transform(sentTest) #Predict the data","87a24a82":"#predictions.show(5) #Show the prediction table","ac9fce45":"---","938acf51":"There are no null values.","f8d8c64b":"# Load Data","895e0524":"# Train-Test Split the Data","55b0b5b1":"---","c6a55720":"# Fix the Sentiments","7c9fc564":"# NOTE: This notebook was meant to work with Spark. I also had a ML aspect with NLP, but Kaggle could not handle it. It seems the best course of action in this case would be to use a different platform, perhaps also with SystemML.","df3a3648":"# Connect Clean Tweets with Sentiments","4c066222":"---","40588458":"Source: https:\/\/stackoverflow.com\/questions\/44627386\/how-to-find-count-of-null-and-nan-values-for-each-column-in-a-pyspark-dataframe","c831f20f":"# Sentiment Counts","16920539":"Reusing my clean tweets code from my Coronavirus tweet analysis: https:\/\/www.kaggle.com\/lunamcbride24\/coronavirus-tweet-processing\nSource on adding the new column: https:\/\/stackoverflow.com\/questions\/48164206\/pyspark-adding-a-column-from-a-list-of-values-using-a-udf","4ff50235":"Coded by Luna McBride\n\nThe point of this project is to work with Apache Spark's features.","f3c66926":"There are an equal number of positive (4) and negative (0) tweets in the dataset. There are no neutral values","dca2e174":"Source for spark handling in Kaggle: https:\/\/www.kaggle.com\/tylerx\/machine-learning-with-spark","c85f7430":"## Collect Sentiments","25ae2b70":"## Connect the Tweets and Sentiments","ab10ba0c":"# Fix cleanTweet Nulls","53646c08":"---","136961c2":"---","8e98904d":"---","0295ee0b":"Source: https:\/\/classes.ischool.syr.edu\/ist718\/content\/unit09\/lab-sentiment_analysis\/","89e40bc1":"# Spark Project: Tweet Sentiment Analysis","318717de":"# Build Pipeline to Tokenize\/Vectorize the Tweets","a310c385":"# Clean the Tweets","f888eb2f":"# Check for Null Values","38fcba0e":"## Put into a PySpark Dataframe","4d14b919":"---","b6381beb":"# Build the Model","d23a0400":"---","45efe080":"# Spark Handling","cfb88fa1":"---","fdd5413d":"---"}}