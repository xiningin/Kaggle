{"cell_type":{"db935259":"code","3f164577":"code","79695859":"code","524c4534":"code","ff5c33ae":"code","c2876253":"code","80ae8430":"code","e1c19411":"code","362790da":"code","d77fa909":"code","2cae0792":"code","393db3a8":"code","3c8f6be6":"code","649cd842":"code","3d6c6428":"code","5fb89705":"code","97cc6d13":"code","bbc77d91":"code","89c8305f":"code","7ed1c98b":"code","989a4c51":"code","91baf58f":"code","63be3540":"code","7140c89f":"code","0d1f155b":"code","05bba68a":"code","df145a14":"code","a9b34237":"code","bd7f4e19":"code","9ed42e81":"code","835df1a4":"code","8c2a69ba":"code","a49df079":"code","91753371":"code","990dcf1a":"code","6f242cb7":"code","0bac8b3a":"code","09f8288b":"code","5404014f":"code","aee7c240":"code","3ec3e26a":"code","f9dcf69f":"code","7c9291ae":"markdown","c7b7adf5":"markdown","99c7ef63":"markdown","912d8dfb":"markdown","25c0feaa":"markdown","c3c70589":"markdown","731aa813":"markdown","c1e54274":"markdown","1a77595f":"markdown","6b00d563":"markdown","b2f02d8a":"markdown","7566f276":"markdown","b53a90fb":"markdown","364c94b6":"markdown","ea9e3840":"markdown","a22a3a47":"markdown","0290bb5f":"markdown","7cf4051d":"markdown","d5352167":"markdown","748ee319":"markdown","e3565bde":"markdown","2fdb2da4":"markdown","dc2f6e13":"markdown","fe62aec3":"markdown","d8daaf5d":"markdown","2d51d1ca":"markdown","514359fc":"markdown","6bb72f67":"markdown","41c29130":"markdown","1955160b":"markdown","3346a386":"markdown","1cc63899":"markdown","b68383dc":"markdown","e97b78b3":"markdown","ee76d254":"markdown","27ae2b44":"markdown","302c3a67":"markdown","2b003eed":"markdown","c5fbb7be":"markdown","de2d9c79":"markdown","eba5b905":"markdown","f0c0235e":"markdown","a6d55149":"markdown","29f64b19":"markdown","d1220ec0":"markdown"},"source":{"db935259":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3f164577":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn-dark')","79695859":"data=pd.read_csv('\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv')","524c4534":"data.head()","ff5c33ae":"data.shape","c2876253":"data.describe()","80ae8430":"data.info()","e1c19411":"data.isnull().values.any()","362790da":"data.isnull().sum()","d77fa909":"import missingno as msno\nmsno.matrix(data)\nplt.show()\n","2cae0792":"missing_percantage=data['salary'].isnull().sum()\/len(data)*100\n\nprint(round(missing_percantage,2),'%')","393db3a8":"column=data.select_dtypes(include=['object'])\nfor col in column:\n    display(data[col].value_counts())","3c8f6be6":"plt.figure(figsize=(10,7))\nsns.countplot(x='gender',data=data)\nlabels = (data['gender'])","649cd842":"plt.figure(figsize=(10,7))\nsns.countplot(x='gender',hue='status',data=data)\nplt.show()","3d6c6428":"plt.figure(figsize=(10,7))\nsns.boxplot(y='gender',x='salary',data=data)","5fb89705":"plt.figure(figsize=(10,7))\nsns.countplot(x='ssc_b',data=data)\nplt.show()","97cc6d13":"plt.figure(figsize=(10,7))\nsns.countplot(x='hsc_b',hue='hsc_s',data=data)\nplt.show()","bbc77d91":"plt.figure(figsize=(10,7))\nsns.countplot(x='ssc_b',hue='status',data=data)\nplt.show()","89c8305f":"plt.figure(figsize=(15,8))\nsns.catplot(x='hsc_b',hue='hsc_s',col='status',data=data,kind='count')\nplt.show()","7ed1c98b":"plt.figure(figsize=(10,7))\nsns.countplot(x=\"degree_t\", hue='status',data=data)","989a4c51":"plt.figure(figsize=(10,7))\nsns.countplot(x=\"specialisation\", hue='status',data=data)","91baf58f":"plt.figure(figsize = (15,7))\nax=plt.subplot(121)\nsns.violinplot(x='degree_t',y='salary',hue='gender',data=data,split=True,scale=\"count\")\nax.set_title('UG Degree')\nax=plt.subplot(122)\nsns.violinplot(x='specialisation',y='salary',hue='gender',data=data,split=True,scale=\"count\")\nax.set_title('MBA ')","63be3540":"plt.figure(figsize = (15, 15))\nax=plt.subplot(221)\nsns.boxplot(x='status',y='ssc_p',data=data)\nax.set_title('Secondary school percentage')\nax=plt.subplot(222)\nsns.boxplot(x='status',y='hsc_p',data=data)\nax.set_title('Higher Secondary school percentage')\nax=plt.subplot(223)\nsns.boxplot(x='status',y='degree_p',data=data)\nax.set_title('UG Degree percentage')\nax=plt.subplot(224)\nsns.boxplot(x='status',y='mba_p',data=data)\nax.set_title('MBA percentage')","7140c89f":"plt.figure(figsize=(10,7))\nsns.violinplot(x=data[\"gender\"], y=data[\"salary\"], hue=data[\"workex\"])\nplt.title(\"Gender vs Salary based on work experience\")","0d1f155b":"plt.figure(figsize=(10,5))\nsns.distplot(data['salary'], bins=50, hist=False)\nplt.title(\"Salary Distribution\")\nplt.show()","05bba68a":"plt.figure(figsize=(10,7))\nsns.boxplot(x='gender',y='salary',data=data)\nplt.show()","df145a14":"data[\"gender\"] = data.gender.map({\"M\":0,\"F\":1})\ndata[\"ssc_b\"] = data.ssc_b.map({\"Others\":0,\"Central\":1})\ndata[\"hsc_b\"] = data.hsc_b.map({\"Others\":0,\"Central\":1})\ndata[\"hsc_s\"] = data.hsc_s.map({\"Commerce\":0,\"Science\":1,\"Arts\":2})\ndata[\"degree_t\"] = data.degree_t.map({\"Comm&Mgmt\":0,\"Sci&Tech\":1, \"Others\":2})\ndata[\"workex\"] = data.workex.map({\"No\":0, \"Yes\":1})\ndata[\"status\"] = data.status.map({\"Not Placed\":0, \"Placed\":1})\ndata[\"specialisation\"] = data.specialisation.map({\"Mkt&HR\":0, \"Mkt&Fin\":1})","a9b34237":"plt.figure(figsize=(12,8))\nsns.heatmap(data.corr(),annot=True)\nplt.show()","bd7f4e19":"# Seperating Features and Target\nX = data[[ 'ssc_p', 'hsc_p', 'hsc_s', 'degree_p',  'workex','etest_p', 'specialisation', 'mba_p',]]\ny = data['status']","9ed42e81":"from sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import make_scorer, accuracy_score,precision_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score ,precision_score,recall_score,f1_score\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import  LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB","835df1a4":"# Let us now split the data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42,stratify =y)\n\n\nprint(\"X-Train:\",X_train.shape)\nprint(\"X-Test:\",X_test.shape)\nprint(\"Y-Train:\",y_train.shape)\nprint(\"Y-Test:\",y_test.shape)","8c2a69ba":"logreg = LogisticRegression(solver= 'lbfgs',max_iter=400)\nlogreg.fit(X_train, y_train)\n\nlog_pred = logreg.predict(X_test)\n\nacc_log = round(logreg.score(X_train, y_train) * 100, 2)","a49df079":"random_forest = RandomForestClassifier(n_estimators=200,criterion='gini',\n max_depth= 4 ,\n max_features= 'auto',random_state=42)\nrandom_forest.fit(X_train, y_train)\n\nran_pred = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, y_train)\nacc_random_forest = round(random_forest.score(X_train, y_train) * 100, 2)","91753371":"plt.subplots(figsize=(20,6))\na_index=list(range(1,50))\na=pd.Series()\nx=range(1,50)\n#x=[1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,50)):\n    model=KNeighborsClassifier(n_neighbors=i) \n    model.fit(X_train, y_train) \n    prediction=model.predict(X_test)\n    a=a.append(pd.Series(accuracy_score(y_test,prediction)))\nplt.plot(a_index, a,marker=\"*\",color='r')\nplt.xticks(x)\nplt.show()","990dcf1a":"knn = KNeighborsClassifier(n_neighbors = 10)\nknn.fit(X_train, y_train)\nknn_pred = knn.predict(X_test) \nacc_knn = round(knn.score(X_train, y_train) * 100, 2)\n","6f242cb7":"gaussian = GaussianNB()\ngaussian.fit(X_train, y_train)\ngau_pred = gaussian.predict(X_test) \nacc_gaussian = round(gaussian.score(X_train, y_train) * 100, 2)","0bac8b3a":"linear_svc = LinearSVC()\nlinear_svc.fit(X_train, y_train)\n\nsvc_pred = linear_svc.predict(X_test)\n\nacc_linear_svc = round(linear_svc.score(X_train, y_train) * 100, 2)","09f8288b":"results = pd.DataFrame({\n    'Model': [ 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes',  \n              ' Support Vector Machine'\n            ],\n    'Train Score': [ acc_knn, acc_log, \n              acc_random_forest, acc_gaussian,  \n              acc_linear_svc],\n    'Accuracy_score':[round(accuracy_score(y_test,knn_pred) * 100, 2),\n                round(accuracy_score(y_test,log_pred) * 100, 2),\n                round(accuracy_score(y_test,ran_pred) * 100, 2),\n                round(accuracy_score(y_test,gau_pred)* 100, 2),\n                round(accuracy_score(y_test,svc_pred)* 100, 2)\n                \n        \n    ]\n\n})\nresult_df = results.sort_values(by='Accuracy_score', ascending=False)\nresult_df = result_df.set_index('Accuracy_score')\nresult_df","5404014f":"plt.subplots(figsize=(10,6))\nax=sns.pointplot(x='Model',y=\"Accuracy_score\",data=results)\nlabels = (results[\"Accuracy_score\"])\n# add result numbers on barchart\nfor i, v in enumerate(labels):\n    ax.text(i, v+0.2, str(v), horizontalalignment = 'center', size = 15, color = 'red')","aee7c240":"randomForestFinalModel = RandomForestClassifier(n_estimators=200,criterion='gini',\n max_depth= 4 ,\n max_features= 'auto',random_state=42)\nrandomForestFinalModel.fit(X_train, y_train)\npredictions_rf = randomForestFinalModel.predict(X_test)\n\ncm_logit = confusion_matrix(y_test, predictions_rf)\nprint('Confusion matrix for Random Forest\\n',cm_logit)\n\naccuracy_logit = accuracy_score(y_test,predictions_rf)\nprecision_logit =precision_score(y_test, predictions_rf)\nrecall_logit =  recall_score(y_test, predictions_rf)\nf1_logit = f1_score(y_test,predictions_rf)\nprint('accuracy_random_Forest : %.3f' %accuracy_logit)\nprint('precision_random_Forest : %.3f' %precision_logit)\nprint('recall_random_Forest : %.3f' %recall_logit)\nprint('f1-score_random_Forest : %.3f' %f1_logit)\nauc_logit = roc_auc_score(y_test,predictions_rf)\nprint('AUC_random_Forest: %.2f' % auc_logit)\n","3ec3e26a":"\ncf_matrix = confusion_matrix(y_test, predictions_rf)\nfig = plt.figure(figsize=(10,7))\ngroup_names = ['True Neg','False Pos','False Neg','True Pos']\ngroup_counts = [\"{0:0.0f}\".format(value) for value in\n                cf_matrix.flatten()]\ngroup_percentages = [\"{0:.2%}\".format(value) for value in\n                     cf_matrix.flatten()\/np.sum(cf_matrix)]\nlabels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n          zip(group_names,group_counts,group_percentages)]\nlabels = np.asarray(labels).reshape(2,2)\nsns.heatmap(cf_matrix, annot=labels, annot_kws={\"size\": 16}, fmt='')\nplt.title(\"Confusion Matrix\")\nplt.xlabel(\"Predicted Label\")\nplt.ylabel(\"True Label\")\nplt.show()","f9dcf69f":"a=[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\nb=[0.0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\nfig =plt.figure(figsize=(20,12),dpi=50)\nfpr, tpr, thresholds = roc_curve(y_test,predictions_rf )\nplt.plot(fpr, tpr,color ='blue',label ='random Forest',linewidth=2 )\nplt.plot(a,b,color='black',linestyle ='dashed',linewidth=2)\nplt.legend(fontsize=15)\nplt.xlabel('False Positive Rate',fontsize=15)\nplt.ylabel('True Positive Rate',fontsize=15)","7c9291ae":"### OBSERVATIONS:\n* We have samples of 139 Male studets and 76 Female students.\n* The number of male students are almost double as compared to female.\n* More outliers on Male,Male students are getting high CTC jobs.\n* Male students are offered slightly greater salary than female on an average.\n","c7b7adf5":"Now we will train several Machine Learning models and compare their results. Note that because the dataset does not provide labels for their testing-set, we need to use the predictions on the training set to compare the algorithms with each other.","99c7ef63":"### OBSERVATION:\nLooks like except for hsc_s and degree_t with 3 classes, all other have 2 classes each and also we can see that this data is slightly imbalanced as we have 148 placed students and 67 not placed students.","912d8dfb":"# Getting the object columns","25c0feaa":"### OBSERVATION :\n* Looking at the distribution we can say that the most of the students get a package between 200k-400k and most salaries above 400,000 are outliers.\n* Male candidates are making more money as compared to female candidates.","c3c70589":"## Roc_curve\nAnother way to evaluate and compare your binary classifier is provided by the ROC AUC Curve. This curve plots the true positive rate (also called recall) against the false positive rate (ratio of incorrectly classified negative instances), instead of plotting the precision versus the recall.","731aa813":"#### We have 7 columns with real values and 8 with object datatype\n#### It is clear that only salary has null columns. Let's see how much?","c1e54274":"# Handle missing data\nChecking Missing value is present or not in our dataset","1a77595f":"\nLet us quickly view the first few rows and extract some prelimindary information about the dataset.","6b00d563":"# Observation :\n*  Looks like Commerce and Science degree students are preffered by companies which is obvious. Students who opted for Others have very low placement chance.\n* Specialisation is a clear indicator in placements. Compared to MktandFin, Mkt&HR students have low placements. This might be because there is low requirements for HR in a company.\n* More Highly Paid Jobs for Mkt&Fin students.\n* Commerce&Mgmt students occasionally get dream placements with high salary\n\n","b2f02d8a":"As we can see, the Random Forest classifier goes on the first place. ","7566f276":"### K Nearest Neighbor:","b53a90fb":"### OBSERVATION :\n* Work Experience is a clear indicator as more work experience results in higher CTC jobs.","364c94b6":"## 2.2 Feature selection\nWe will now select the features (X) for our model. These features will help our model identify patterns. The features will be columns.\n\n\"When feature engineering is done, we usually tend to decrease the dimensionality by selecting the \"right\" number of features that capture the essential.\"","ea9e3840":"## Conclusion\nHere are a few things to keep in mind:\n\n* Specialisations Matter. Choose the right one.\n* Go for Internship. Work Experience helps.\n* Don't worry about grades for salary (although you need them to get placed).","a22a3a47":"If you have reached till here, So i hope you liked my Analysis.\n\nDon't forget to upvote if you like it!.\n\nI'm a beginner and any suggestion in the comment box is highly appreciated.\n\nIf you have any doubt reagrding any part of the notebook, feel free to comment your doubt in the comment box.\n\nThank you!!","0290bb5f":"#### Importing Libraries","7cf4051d":"### Random Forest:","d5352167":"## Observetion:\n* There is count of central board students is very high as compared to all other boards in ssc_b but its reverse in hsc_b.\n* Look like not much difference between in the fraction of placed candidates in respective boards. \n* Board doesn't matter in placements.","748ee319":"## 1.1 Feature: Gender\n* ### Does gender affect placements?","e3565bde":"# About the Dataset\nIt has the following columns:\n\n1. sl_no : Serial Number\n\n2. gender : Gender- Male='M',Female='F'\n\n3. ssc_p : Secondary Education percentage- 10th Grade\n\n4. ssc_b : Board of Education- Central\/ Others\n\n5. hsc_p : Higher Secondary Education percentage- 12th Grade\n\n6. hsc_b : Board of Education- Central\/ Others\n\n7. hsc_s : Specialization in Higher Secondary Education\n\n8. degree_p : Degree Percentage\n\n9. degree_t : Under Graduation(Degree type)- Field of degree education\n\n10. workex : Work Experience\n\n11. etest_p : Employability test percentage ( conducted by college)\n\n12. specialisation : Post Graduation(MBA)- Specialization\n\n13. mba_p : MBA percentage\n\n14. status : Status of placement- Placed\/Not placed\n\n15. salary : Salary offered by corporate to candidates\n","2fdb2da4":"## Random_Forest Model","dc2f6e13":"![](https:\/\/teresas.ac.in\/wp-content\/uploads\/2018\/04\/placement-services.png)","fe62aec3":"# 1.2 Feature : ssc_b,hsc_b,hsc_s\n* ### Does the board of education affect placements?","d8daaf5d":"# 2. Feature Engineering\n## 2.1 Creating Dummies Variables\nDummy variable is a categorical variable that has been transformed into numeric. For example the column Gender, we have \"male\" and \"female\" we will transform these variables into numeric. Creating a new column just for Men. and Women, where 1 will be set to positive and 0 to negative\n\n","2d51d1ca":"# 1.4 Feature : ssc_p,hsc_p,degree_p,mba_p\n* ### Does your academic score influence your chance of placement?","514359fc":"# 1. Exploring & Visualizations Data by each Features","6bb72f67":"### Gaussian Naive Bayes:","41c29130":"# Building Machine Learning Models","1955160b":"### Linear Support Vector Machine:","3346a386":"### K Nearest Neighbor:","1cc63899":"# 1.5 Feature : workex\n* ### Does Work Experience increases results in higher CTC jobs?","b68383dc":"### OBSERVATION :\n* Most of the candidates educational performances are between 60-80%\n* We can see that getting good percentages in MBA does not guarantee placement of the candidate.\n* Comparitively there's a slight difference between the percentage scores between both the groups, But still placed candidates still has an upper hand. So as per the plot,percentage do not influence the placement status\n* #### These percentages don't have any influence over their salary.\n\n","e97b78b3":"\n <center><h1 style=\"color:red\">Don't forget to upvote if you like it! :)<\/h1><\/center>","ee76d254":"### Logistic Regression:","27ae2b44":"# Campus Recruitment","302c3a67":"## Correlation\nCorrelation is a statistical technique that can show whether and how strongly pairs of variables are related\u00b6","2b003eed":"# Contents\n* Include Libraries\n* Import DataSet\n* Handle Missing Value\n* EDA(Exploratory Data Analysis)\n* Feature Engineering\n* Machine learning Model\n* Random Forest Classifier\n* ROC Curve\n","c5fbb7be":"## Confusion_matrix","de2d9c79":"# 1.6 Feature : salary","eba5b905":"This means that around 31% candidates were not placed which is sad but let's see what were the reasons :)","f0c0235e":"As we see at K between [10-17] KNN is giving maximam accuracy","a6d55149":"What value of K KNN will give high accuracy?","29f64b19":"## Which is the best Model ?","d1220ec0":"# 1.3 Feature : degree_t,specialisation\n* ### Which degree and MBA specialization has the highest Salary?"}}