{"cell_type":{"25cfca23":"code","28830688":"code","905865b7":"code","b497501b":"code","83fab3b4":"code","b78bb452":"code","1d0e5663":"code","d7d24fd7":"code","7c93d278":"code","69dbae36":"code","d4fa0f2c":"code","0166f84d":"code","8c720a6c":"code","8a47828b":"code","9ada889e":"code","47355e22":"code","c1efdb1e":"code","58713f0c":"code","352b573e":"code","2df12c65":"code","2d13a0db":"code","d2b7ff84":"code","ac22a4da":"code","ef2b5f4d":"code","624ee3c9":"code","153f2327":"code","f980e33f":"code","4b419a3b":"code","792caec9":"code","bb3c67fc":"code","e572c79b":"code","9e2ad888":"code","83d963c7":"markdown","941a9e9a":"markdown","811f6b41":"markdown","dbbf65b2":"markdown","309efc14":"markdown","c05afc9d":"markdown","0bf79f06":"markdown","b363fbef":"markdown","2b0be410":"markdown","e8fedc3c":"markdown","4a9e4354":"markdown","dd3f11aa":"markdown","efb1e08f":"markdown","926313ff":"markdown"},"source":{"25cfca23":"# Ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# data manipulation\nimport pandas as pd\nimport numpy as np\nimport random as rnd\nimport re\n\nfrom collections import Counter\n\n# visualization\nimport seaborn as sns\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\n%matplotlib inline\nmpl.style.use( 'ggplot' )\nsns.set_style( 'white' )\npylab.rcParams[ 'figure.figsize' ] = 8 , 6\n\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\n\n# Modelling Algorithms\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nimport xgboost as xgb\n\n# Modelling Helpers\nfrom sklearn.metrics import make_scorer, accuracy_score, log_loss, confusion_matrix\nfrom sklearn.preprocessing import Imputer, Normalizer, scale, OneHotEncoder, LabelEncoder\nfrom sklearn.model_selection import StratifiedShuffleSplit, train_test_split, KFold, StratifiedKFold, GridSearchCV, learning_curve\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.feature_selection import RFECV\n\n# Load Data\ntrain_df = pd.read_csv('..\/input\/train.csv')\ntest_df = pd.read_csv('..\/input\/test.csv')\n# combine these datasets to run certain operations on both datasets together\nfull_data = [train_df, test_df]\n\n# preview the data\ntrain_df.head()","28830688":"# Data info\ntrain_df.info()\nprint('_'*40)\ntest_df.info()\n\n# Distribution of numerical feature values\ntrain_df.describe()\n# Review survived rate using `percentiles=[.61, .62]` knowing our problem description mentions 38% survival rate.\n# Review Parch distribution using `percentiles=[.75, .8]`\n# SibSp distribution `[.68, .69]`\n# Age and Fare `[.1, .2, .3, .4, .5, .6, .7, .8, .9, .99]`\n\n# Distribution of categorical features\ntrain_df.describe(include=['O'])","905865b7":"def check_missing_data(df):\n    flag=df.isna().sum().any()\n    if flag==True:\n        total = df.isnull().sum()\n        percent = (df.isnull().sum())\/(df.isnull().count()*100)\n        output = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n        data_type = []\n        # written by MJ Bahmani\n        for col in df.columns:\n            dtype = str(df[col].dtype)\n            data_type.append(dtype)\n        output['Types'] = data_type\n        return(np.transpose(output))\n    else:\n        return(False)\n\n    \n# Fill empty and NaNs values with NaN\ntrain_df = train_df.fillna(np.nan)\ntest_df = test_df.fillna(np.nan)\n\ncheck_missing_data(train_df)\n#check_missing_data(test_df)\n\n# remove rows that have NA's\n#train_df = train_df.dropna()","b497501b":"# Outlier detection \ndef detect_outliers(df, n, features):\n    \"\"\"\n    Takes a dataframe df of features and returns a list of the indices\n    corresponding to the observations containing more than n outliers according\n    to the Tukey method.\n    \"\"\"\n    outlier_indices = []\n    \n    # iterate over features(columns)\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        \n        # outlier step\n        outlier_step = 1.5 * IQR\n        \n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        \n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n        \n    # select observations containing more than n outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    \n    return multiple_outliers   \n\n# detect outliers from Age, SibSp, Parch and Fare\nOutliers_to_drop = detect_outliers(train_df, 2, [\"Age\",\"SibSp\",\"Parch\",\"Fare\"])\n# Show the outliers rows\ntrain_df.loc[Outliers_to_drop]\n# Drop outliers\n# train_df = train_df.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)\n# full_data = [train_df, test_df]","83fab3b4":"# correlation with target\ntrain_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n# train_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n# train_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n# train_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","b78bb452":"# plot helper functions\ndef plot_histograms( df , variables , n_rows , n_cols ):\n    fig = plt.figure( figsize = ( 16 , 12 ) )\n    for i, var_name in enumerate( variables ):\n        ax=fig.add_subplot( n_rows , n_cols , i+1 )\n        df[ var_name ].hist( bins=10 , ax=ax )\n        ax.set_title( 'Skew: ' + str( round( float( df[ var_name ].skew() ) , ) ) ) # + ' ' + var_name ) #var_name+\" Distribution\")\n        ax.set_xticklabels( [] , visible=False )\n        ax.set_yticklabels( [] , visible=False )\n    fig.tight_layout()  # Improves appearance a bit.\n    plt.show()\n\ndef plot_distribution( df , var , target , **kwargs ):\n    row = kwargs.get( 'row' , None )\n    col = kwargs.get( 'col' , None )\n    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n    facet.map( sns.kdeplot , var , shade= True )\n    facet.set( xlim=( 0 , df[ var ].max() ) )\n    facet.add_legend()\n\ndef plot_categories( df , cat , target , **kwargs ):\n    row = kwargs.get( 'row' , None )\n    col = kwargs.get( 'col' , None )\n    facet = sns.FacetGrid( df , row = row , col = col )\n    facet.map( sns.barplot , cat , target )\n    facet.add_legend()\n\ndef plot_correlation_map( df ):\n    corr = df.corr()\n    _ , ax = plt.subplots( figsize =( 12 , 10 ) )\n    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n    _ = sns.heatmap(\n        corr, \n        cmap = cmap,\n        square=True, \n        cbar_kws={ 'shrink' : .9 }, \n        ax=ax, \n        annot = True, \n        annot_kws = { 'fontsize' : 12 }\n    )\n\ndef describe_more( df ):\n    var = [] ; l = [] ; t = []\n    for x in df:\n        var.append( x )\n        l.append( len( pd.value_counts( df[ x ] ) ) )\n        t.append( df[ x ].dtypes )\n    levels = pd.DataFrame( { 'Variable' : var , 'Levels' : l , 'Datatype' : t } )\n    levels.sort_values( by = 'Levels' , inplace = True )\n    return levels\n\ndef plot_variable_importance( X , y ):\n    tree = DecisionTreeClassifier( random_state = 99 )\n    tree.fit( X , y )\n    plot_model_var_imp( tree , X , y )\n    \ndef plot_model_var_imp( model , X , y ):\n    imp = pd.DataFrame( \n        model.feature_importances_  , \n        columns = [ 'Importance' ] , \n        index = X.columns \n    )\n    imp = imp.sort_values( [ 'Importance' ] , ascending = True )\n    imp[ : 10 ].plot( kind = 'barh' )\n    print (model.score( X , y ))\n","1d0e5663":"# visualization\n# plot_distribution(train_df, var = 'Age', target = 'Survived', row = 'Sex')\n# plot_categories(train_df, cat = 'Embarked', target = 'Survived')\nplot_correlation_map(train_df)\n\n# scatterplot: to identify the type of relationship (if any) between two quantitative variables\n# g = sns.FacetGrid(train_df, hue=\"Survived\", col=\"Pclass\", margin_titles=True, palette={1:\"seagreen\", 0:\"gray\"})\n# g=g.map(plt.scatter, \"Fare\", \"Age\",edgecolor=\"w\").add_legend();\n\n# boxplot: depicting groups of numerical data through their quartiles\n# ax= sns.boxplot(x=\"Pclass\", y=\"Age\", data=train_df)\n# ax= sns.stripplot(x=\"Pclass\", y=\"Age\", data=train_df, jitter=True, edgecolor=\"gray\")\n# plt.show()\n\n# pairplot\n# g = sns.pairplot(train[[u'Survived', u'Pclass', u'Sex', u'Age', u'Parch', u'Fare', u'Embarked',\n#        u'Title']], hue='Survived', palette = 'seismic',size=1.2,diag_kind = 'kde',diag_kws=dict(shade=True),plot_kws=dict(s=10) )\n# g.set(xticklabels=[])\n\n# histogram\n# g = sns.FacetGrid(train_df, col='Survived')\n# g.map(plt.hist, 'Age', bins=20)\n\n# grid = sns.FacetGrid(train_df, col='Survived', row='Pclass', size=2.2, aspect=1.6)\n# grid.map(plt.hist, 'Age', alpha=.5, bins=20)\n# grid.add_legend();\n\n# pointplot\n# grid = sns.FacetGrid(train_df, row='Embarked', size=2.2, aspect=1.6)\n# grid.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\n# grid.add_legend()\n\n# barplot\n# grid = sns.FacetGrid(train_df, row='Embarked', col='Survived', size=2.2, aspect=1.6)\n# grid.map(sns.barplot, 'Sex', 'Fare', alpha=.5, ci=None)\n# grid.add_legend()\n\n# other plots\n# train_df['Age'].hist(bins=70)\n# sns.factorplot('Embarked','Survived', data=train_df,size=4,aspect=3)\n\n# peaks for survived\/not survived passengers by their age\n# facet = sns.FacetGrid(train_df, hue=\"Survived\",aspect=4)\n# facet.map(sns.kdeplot,'Age',shade= True)\n# facet.set(xlim=(0, train_df['Age'].max()))\n# facet.add_legend()\n\n# fig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(15,5))\n# sns.factorplot('Embarked',data=train_df,kind='count',order=['S','C','Q'],ax=axis1)\n# sns.factorplot('Survived',hue=\"Embarked\",data=train_df,kind='count',order=[1,0],ax=axis2)\n# sns.countplot(x='Embarked', data=train_df, ax=axis1)\n# sns.countplot(x='Survived', hue=\"Embarked\", data=train_df, order=[1,0], ax=axis2)\n\n# group by embarked, and get the mean for survived passengers for each value in Embarked\n# embark_perc = train_df[[\"Embarked\", \"Survived\"]].groupby(['Embarked'],as_index=False).mean()\n# sns.barplot(x='Embarked', y='Survived', data=embark_perc,order=['S','C','Q'],ax=axis3)\n\n# Multivariate plots\n# scatter plot matrix\n# pd.plotting.scatter_matrix(train_df,figsize=(10,10))\n# plt.figure();\n\n# violinplots on petal-length for each species\n# f,ax=plt.subplots(1,2,figsize=(18,8))\n# sns.violinplot(\"Pclass\",\"Age\", hue=\"Survived\", data=train_df,split=True,ax=ax[0])\n# ax[0].set_title('Pclass and Age vs Survived')\n# ax[0].set_yticks(range(0,110,10))\n# sns.violinplot(\"Sex\",\"Age\", hue=\"Survived\", data=train_df,split=True,ax=ax[1])\n# ax[1].set_title('Sex and Age vs Survived')\n# ax[1].set_yticks(range(0,110,10))\n# plt.show()\n\n# kdeplot\n# sns.FacetGrid(train_df, hue=\"Survived\", size=5).map(sns.kdeplot, \"Fare\").add_legend()\n# plt.show();\n\n# jointplot\n# sns.jointplot(x='Fare',y='Age' ,data=train_df, kind='reg');\n\n# Swarm plot\n# sns.swarmplot(x='Pclass',y='Age',data=train_df);\n\n# Heatmap\n# plt.figure(figsize=(7,4)) \n# sns.heatmap(train_df.corr(),annot=True,cmap='cubehelix_r') #draws  heatmap with input as the correlation matrix calculted by(iris.corr())\n# plt.show();\n\n# distplot\n# f,ax=plt.subplots(1,3,figsize=(20,8))\n# sns.distplot(train_df[train_df['Pclass']==1].Fare,ax=ax[0])\n# ax[0].set_title('Fares in Pclass 1')\n# sns.distplot(train_df[train_df['Pclass']==2].Fare,ax=ax[1])\n# ax[1].set_title('Fares in Pclass 2')\n# sns.distplot(train_df[train_df['Pclass']==3].Fare,ax=ax[2])\n# ax[2].set_title('Fares in Pclass 3')\n# plt.show()","d7d24fd7":"# drop the ticket and cabin features\nprint(\"Before\", train_df.shape, test_df.shape, full_data[0].shape, full_data[1].shape)\ntrain_df = train_df.drop(['Ticket', 'Cabin', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\nfull_data = [train_df, test_df]\nprint(\"After\", train_df.shape, test_df.shape, full_data[0].shape, full_data[1].shape)\n\n# extract Title as a new feature\nfor dataset in full_data:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n#title[ 'Title' ] = full_data[ 'Name' ].map( lambda name: name.split( ',' )[1].split( '.' )[0].strip() )\n\n# replace many titles with a more common name or classify them as Rare\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n    'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\n    dataset['Title'] = dataset['Title'].replace('Mlle', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Ms', 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n    \ntrain_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()\n\n# convert the categorical titles to ordinal.\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\nfor dataset in full_data:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n    \n# drop name feature\ntrain_df = train_df.drop(['Name'], axis=1)\ntest_df = test_df.drop(['Name'], axis=1)\nfull_data = [train_df, test_df]\ntrain_df.shape, test_df.shape\n\n# sex feature to numerical values\nfor dataset in full_data:\n    dataset['Sex'] = dataset['Sex'].map( {'female': 1, 'male': 0} ).astype(int)\n# sex = pd.Series( np.where( full_data.Sex == 'male' , 1 , 0 ) , name = 'Sex' )","7c93d278":"# As we see, children(age < ~16) on aboard seem to have a high chances for Survival.\n# So, we can classify passengers as males, females, and child\ndef get_person(passenger):\n    age,sex = passenger\n    return 'child' if age < 16 else sex\n    \ntrain_df['Person'] = train_df[['Age','Sex']].apply(get_person,axis=1)\ntest_df['Person'] = test_df[['Age','Sex']].apply(get_person,axis=1)\n\n# No need to use Sex column since we created Person column\n# train_df.drop(['Sex'],axis=1,inplace=True)\n# test_df.drop(['Sex'],axis=1,inplace=True)\n\n# create dummy variables for Person column, & drop Male as it has the lowest average of survived passengers\nperson_dummies_titanic  = pd.get_dummies(train_df['Person'])\nperson_dummies_titanic.columns = ['Child','Female','Male']\nperson_dummies_titanic.drop(['Male'], axis=1, inplace=True)\n\nperson_dummies_test  = pd.get_dummies(test_df['Person'])\nperson_dummies_test.columns = ['Child','Female','Male']\nperson_dummies_test.drop(['Male'], axis=1, inplace=True)\n\ntrain_df = train_df.join(person_dummies_titanic)\ntest_df = test_df.join(person_dummies_test)\n\nfig, (axis1,axis2) = plt.subplots(1,2,figsize=(10,5))\n\n# sns.factorplot('Person',data=train_df,kind='count',ax=axis1)\nsns.countplot(x='Person', data=train_df, ax=axis1)\n\n# average of survived for each Person(male, female, or child)\nperson_perc = train_df[[\"Person\", \"Survived\"]].groupby(['Person'],as_index=False).mean()\nsns.barplot(x='Person', y='Survived', data=person_perc, ax=axis2, order=['male','female','child'])\n\ntrain_df.drop(['Person'],axis=1,inplace=True)\ntest_df.drop(['Person'],axis=1,inplace=True)\nfull_data = [train_df, test_df]","69dbae36":"# Age feature\nguess_ages = np.zeros((2,3))\nguess_ages\n\n# Fill missing values of Age with the average of Age (mean)\n#imputed = pd.DataFrame()\n#imputed[ 'Age' ] = full_data.Age.fillna( full.Age.mean() )\n\nfor dataset in full_data:\n# fill with random value\n#     age_avg = dataset['Age'].mean()\n#     age_std = dataset['Age'].std()\n#     age_null_count = dataset['Age'].isnull().sum()\n#     age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\n#     dataset['Age'][np.isnan(dataset['Age'])] = age_null_random_list\n    for i in range(0, 2):\n        for j in range(0, 3):\n            guess_df = dataset[(dataset['Sex'] == i) & \\\n                                  (dataset['Pclass'] == j+1)]['Age'].dropna()\n            age_guess = guess_df.median()\n\n            # Convert random age float to nearest .5 age\n            guess_ages[i,j] = int( age_guess\/0.5 + 0.5 ) * 0.5\n            \n    for i in range(0, 2):\n        for j in range(0, 3):\n            dataset.loc[ (dataset.Age.isnull()) & (dataset.Sex == i) & (dataset.Pclass == j+1),\\\n                    'Age'] = guess_ages[i,j]\n\n    dataset['Age'] = dataset['Age'].astype(int)\n\ntrain_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)\n\nfor dataset in full_data:    \n    dataset.loc[ dataset['Age'] <= 16, 'Age'] = 0\n    dataset.loc[(dataset['Age'] > 16) & (dataset['Age'] <= 32), 'Age'] = 1\n    dataset.loc[(dataset['Age'] > 32) & (dataset['Age'] <= 48), 'Age'] = 2\n    dataset.loc[(dataset['Age'] > 48) & (dataset['Age'] <= 64), 'Age'] = 3\n    dataset.loc[ dataset['Age'] > 64, 'Age']\n\ntrain_df = train_df.drop(['AgeBand', 'Sex'], axis=1)\ntest_df.drop(['Sex'],axis=1,inplace=True)\n\nfull_data = [train_df, test_df]\n\nfor dataset in full_data:\n    dataset['Age*Class'] = dataset.Age * dataset.Pclass\n\ntrain_df.loc[:, ['Age*Class', 'Age', 'Pclass']].head(10)\ntrain_df.head()","d4fa0f2c":"# New feature based on Parch and SibSp \nfor dataset in full_data:\n    dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\n\ntrain_df[['FamilySize', 'Survived']].groupby(['FamilySize'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n\n# introducing other features based on the family size\nfor dataset in full_data:\n    dataset[ 'Family_Single' ] = dataset[ 'FamilySize' ].map( lambda s : 1 if s == 1 else 0 )\n    dataset[ 'Family_Small' ]  = dataset[ 'FamilySize' ].map( lambda s : 1 if 2 <= s <= 4 else 0 )\n    dataset[ 'Family_Large' ]  = dataset[ 'FamilySize' ].map( lambda s : 1 if 5 <= s else 0 )\n\ntrain_df = train_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp', 'FamilySize'], axis=1)\n\n# for dataset in full_data:\n#     dataset['IsAlone'] = 0\n#     dataset.loc[dataset['FamilySize'] == 1, 'IsAlone'] = 1\n# train_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()\n# train_df = train_df.drop(['FamilySize'], axis=1)\n# test_df = test_df.drop(['FamilySize'], axis=1)\n\nfull_data = [train_df, test_df]\n\ntrain_df.head()","0166f84d":"# Embarked feature\nfreq_port = train_df.Embarked.dropna().mode()[0]\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].fillna(freq_port)\n    \ntrain_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)\n\nfor dataset in full_data:\n    dataset['Embarked'] = dataset['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\n\ntrain_df.head()","8c720a6c":"# Fare feature\ntest_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)\n\ntrain_df['FareBand'] = pd.qcut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by='FareBand', ascending=True)\n\nfor dataset in full_data:\n    dataset.loc[ dataset['Fare'] <= 7.91, 'Fare'] = 0\n    dataset.loc[(dataset['Fare'] > 7.91) & (dataset['Fare'] <= 14.454), 'Fare'] = 1\n    dataset.loc[(dataset['Fare'] > 14.454) & (dataset['Fare'] <= 31), 'Fare']   = 2\n    dataset.loc[ dataset['Fare'] > 31, 'Fare'] = 3\n    dataset['Fare'] = dataset['Fare'].astype(int)\n\ntrain_df = train_df.drop(['FareBand'], axis=1)\nfull_data = [train_df, test_df]\n    \ntrain_df.head()","8a47828b":"# The preprocessing phase is to normalize labels. The LabelEncoder in Scikit-learn will convert \n# each unique string value into a number, making out data more flexible for various algorithms.\n\n# def encode_features(df_train, df_test):\n#     features = ['Fare', 'Cabin', 'Age', 'Sex', 'Lname', 'NamePrefix']\n#     df_combined = pd.concat([df_train[features], df_test[features]])\n    \n#     for feature in features:\n#         le = LabelEncoder()\n#         le = le.fit(df_combined[feature])\n#         df_train[feature] = le.transform(df_train[feature])\n#         df_test[feature] = le.transform(df_test[feature])\n#     return df_train, df_test\n    \n# data_train, data_test = encode_features(train_df, test_df)\n# data_train.head()","9ada889e":"train, test = train_test_split(train_df, test_size = 0.3, random_state = 0)\nsplit_train_X = train.drop(\"Survived\", axis=1)\nsplit_train_Y = train[\"Survived\"]\nsplit_test_X = test.drop(\"Survived\", axis=1)\nsplit_test_Y = test[\"Survived\"]","47355e22":"logreg = LogisticRegression()\nlogreg.fit(split_train_X, split_train_Y)\nlogreg.score(split_test_X, split_test_Y)","c1efdb1e":"coeff_df = pd.DataFrame(train_df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\ncoeff_df.sort_values(by='Correlation', ascending=False)","58713f0c":"svc = SVC(kernel='linear')\n# svc.fit(split_train_X, split_train_Y)\n# svc.score(split_test_X, split_test_Y)\n\n# Confusion Matrix: gives the number of correct and incorrect classifications made by the classifier.\nX = train_df[train_df.columns[1:]]\nY = train_df['Survived']\n_ , ax = plt.subplots(figsize =(12 , 10))\ny_pred = cross_val_predict(svc, X, Y, cv = 10)\nsns.heatmap(confusion_matrix(Y, y_pred), ax = ax, annot = True, fmt = '2.0f')","352b573e":"a_index = list(range(1,11))\na = pd.Series()\nx = [0,1,2,3,4,5,6,7,8,9,10]\nfor i in list(range(1,11)):\n    model = KNeighborsClassifier(n_neighbors = i) \n    model.fit(split_train_X, split_train_Y)\n    a = a.append(pd.Series(model.score(split_test_X, split_test_Y)))\nplt.plot(a_index, a)\nplt.xticks(x)\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()\nprint('Accuracies for different values of n are:',a.values,'with the max value as ',a.values.max())","2df12c65":"gaussian = GaussianNB()\ngaussian.fit(split_train_X, split_train_Y)\ngaussian.score(split_test_X, split_test_Y)","2d13a0db":"perceptron = Perceptron()\nperceptron.fit(split_train_X, split_train_Y)\nperceptron.score(split_test_X, split_test_Y)","d2b7ff84":"decision_tree = DecisionTreeClassifier()\ndecision_tree.fit(split_train_X, split_train_Y)\ndecision_tree.score(split_test_X, split_test_Y)","ac22a4da":"# Hyper-Parameters Tuning\nrandom_forest = RandomForestClassifier(n_estimators=100)\n# Choose some parameter combinations to try\nparameters = {'n_estimators': [4, 6, 9], \n              'max_features': ['log2', 'sqrt','auto'], \n              'criterion': ['entropy', 'gini'],\n              'max_depth': [2, 3, 5, 10], \n              'min_samples_split': [2, 3, 5],\n              'min_samples_leaf': [1,5,8]\n             }\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(accuracy_score)\n\n# Run the grid search\nkfold = StratifiedKFold(n_splits=10)\ngrid_obj = GridSearchCV(random_forest, parameters, cv=kfold, scoring=acc_scorer)\ngrid_obj.fit(X, Y)\n\n# Set the clf to the best combination of parameters\nrandom_forest = grid_obj.best_estimator_\ngrid_obj.best_score_","ef2b5f4d":"from sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\ndef plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(random_forest,\"random_forest learning curves\", X, Y)","624ee3c9":"# Feature importance\nfig, axes = plt.subplots(1, 1, sharex=\"all\", figsize=(15,15))\n\nname = \"RandomForest\"\nindices = np.argsort(random_forest.feature_importances_)[::-1][:40]\ng = sns.barplot(y = train_df.columns[indices][:40],x = random_forest.feature_importances_[indices][:40] , orient='h')\ng.set_xlabel(\"Relative importance\",fontsize=12)\ng.set_ylabel(\"Features\",fontsize=12)\ng.tick_params(labelsize=9)\ng.set_title(\"RandomForest feature importance\")","153f2327":"train_df_X = train_df.drop(\"Survived\", axis=1)\ntrain_df_Y = train_df[\"Survived\"]\ntest_df_X  = test_df.drop(\"PassengerId\", axis=1).copy()\ntrain_df_X.shape, train_df_Y.shape, test_df_X.shape\n# plot_variable_importance(train_X, train_Y)\ntrain_X = train_df_X.values\ntrain_Y = train_df_Y.values\ntest_X = test_df_X.values\n\nclassifiers = [\n    KNeighborsClassifier(3),\n    SVC(probability=True),\n    DecisionTreeClassifier(),\n    RandomForestClassifier(),\n    AdaBoostClassifier(),\n    GradientBoostingClassifier(),\n    GaussianNB(),\n    LinearDiscriminantAnalysis(),\n    QuadraticDiscriminantAnalysis(),\n    LogisticRegression()]\n\nkfold = StratifiedKFold(n_splits=10, random_state=0)\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, train_X, y = train_Y, scoring = \"accuracy\", cv = kfold, n_jobs=4))\n\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")\n\n# acc_dict = {}\n# for train_index, test_index in kfold.split(train_X, train_Y):\n#     split_train_X, split_test_X = train_X[train_index], train_X[test_index]\n#     split_train_Y, split_test_Y = train_Y[train_index], train_Y[test_index]\n\n#     for model in classifiers:\n#         name = model.__class__.__name__\n#         model.fit(split_train_X, split_train_Y)\n#         #plot_model_var_imp(model, split_train_X, split_train_Y)\n#         train_predictions = model.predict(split_test_X)\n#         acc = accuracy_score(split_test_Y, train_predictions)\n#         if name in acc_dict:\n#             acc_dict[name] += acc\n#         else:\n#             acc_dict[name] = acc\n\n# log_cols = [\"Classifier\", \"Accuracy\"]\n# log = pd.DataFrame(columns=log_cols)\n# for model in acc_dict:\n#     acc_dict[model] = acc_dict[model] \/ 11.0\n#     log_entry = pd.DataFrame([[model, acc_dict[model]]], columns=log_cols)\n#     log = log.append(log_entry)\n\n# plt.xlabel('Accuracy')\n# plt.title('Classifier Accuracy')\n# sns.set_color_codes(\"muted\")\n# sns.barplot(x='Accuracy', y='Classifier', data=log, color=\"b\")","f980e33f":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n# Voting Classifier\nensemble_lin_rbf = VotingClassifier(estimators=[('KNN', KNeighborsClassifier(n_neighbors=10)),\n                                              ('RBF', SVC(probability=True,kernel='rbf',C=0.5,gamma=0.1)),\n                                              ('RFor', RandomForestClassifier(n_estimators=500,random_state=0)),\n                                              ('LR', LogisticRegression(C=0.05)),\n                                              ('DT', DecisionTreeClassifier(random_state=0)),\n                                              ('NB', GaussianNB()),\n                                              ('svm', SVC(kernel='linear',probability=True))\n                                             ], \n                       voting='soft').fit(split_train_X, split_train_Y)\nprint('The accuracy for ensembled model is:', ensemble_lin_rbf.score(split_test_X, split_test_Y))\ncross = cross_val_score(ensemble_lin_rbf, X, Y, cv = 10, scoring = \"accuracy\")\nprint('The cross validated score is', cross.mean())\n\n# Bagged DecisionTree\nmodel = BaggingClassifier(base_estimator = DecisionTreeClassifier(), random_state = 0, n_estimators = 100)\nmodel.fit(split_train_X, split_train_Y)\nprediction = model.predict(split_test_X)\nprint('The accuracy for bagged Decision Tree is:', accuracy_score(prediction, split_test_Y))\nresult = cross_val_score(model, X, Y, cv=10, scoring='accuracy')\nprint('The cross validated score for bagged Decision Tree is:', result.mean())\n\n# AdaBoost (Adaptive Boosting)\nada = AdaBoostClassifier(n_estimators = 200, random_state = 0, learning_rate = 0.1)\nresult = cross_val_score(ada,X, Y, cv = 10, scoring='accuracy')\nprint('The cross validated score for AdaBoost is:', result.mean())\n\n# Hyper-Parameter Tuning for AdaBoost\n# n_estimators = list(range(100,1100,100))\n# learn_rate = [0.05,0.1,0.2,0.3,0.25,0.4,0.5,0.6,0.7,0.8,0.9,1]\n# hyper = {'n_estimators':n_estimators, 'learning_rate':learn_rate}\n# gd = GridSearchCV(estimator = AdaBoostClassifier(), param_grid = hyper,verbose = True)\n# gd.fit(X,Y)\n# print(gd.best_score_)\n# print(gd.best_estimator_)\n\n# Confusion Matrix for the Best Model\nada = AdaBoostClassifier(n_estimators = 200, random_state = 0, learning_rate = 0.05)\nresult = cross_val_predict(ada, X, Y, cv = 10)\nsns.heatmap(confusion_matrix(Y,result), cmap='winter', annot=True, fmt='2.0f')\nplt.show()","4b419a3b":"# Helper function for Stacking\nntrain = train_X.shape[0]\nntest = test_X.shape[0]\nSEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\nkf = StratifiedKFold(n_splits=NFOLDS, shuffle=True, random_state = SEED)\n\n# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)\n        \n# Out-of-Fold prediction function        \ndef get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    fold = 0\n    for (train_index, test_index) in kf.split(x_train, y_train):\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n\n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[fold, :] = clf.predict(x_test)\n        fold += 1\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)\n","792caec9":"# Parameters of the listed classifiers\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 6,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_estimators': 500,\n     #'max_features': 0.2,\n    'max_depth': 5,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# Support Vector Classifier parameters \nsvc_params = {\n    'kernel' : 'linear',\n    'C' : 0.025\n    }\n\n# Create 5 objects that represent our 4 models\nrf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\net = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\nada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\ngb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\nsvc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)\n\n# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, train_X, train_Y, test_X) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf, train_X, train_Y, test_X) # Random Forest\nada_oof_train, ada_oof_test = get_oof(ada, train_X, train_Y, test_X) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb, train_X, train_Y, test_X) # Gradient Boost\nsvc_oof_train, svc_oof_test = get_oof(svc, train_X, train_Y, test_X) # Support Vector Classifier\n\nprint(\"Training is complete\")\nrf_feature = rf.feature_importances(train_X, train_Y)\net_feature = et.feature_importances(train_X, train_Y)\nada_feature = ada.feature_importances(train_X, train_Y)\ngb_feature = gb.feature_importances(train_X, train_Y)","bb3c67fc":"# Second-Level Predictions from the First-level Output\nbase_predictions_train = pd.DataFrame( {'RandomForest': rf_oof_train.ravel(),\n     'ExtraTrees': et_oof_train.ravel(),\n     'AdaBoost': ada_oof_train.ravel(),\n      'GradientBoost': gb_oof_train.ravel()\n    })\nbase_predictions_train.head()\n\n# Correlation Heatmap of the Second Level Training set\ndata = [\n    go.Heatmap(\n        z= base_predictions_train.astype(float).corr().values ,\n        x=base_predictions_train.columns.values,\n        y= base_predictions_train.columns.values,\n          colorscale='Viridis',\n            showscale=True,\n            reversescale = True\n    )\n]\npy.iplot(data, filename='labelled-heatmap')","e572c79b":"x_train = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train), axis=1)\nx_test = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test), axis=1)","9e2ad888":"# Having now concatenated and joined both the first-level train and test predictions as x_train and x_test, \n# we can now fit a second-level learning model.\ngbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1).fit(train_X, train_Y)\n\npredictions = gbm.predict(test_X)\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_df[\"PassengerId\"],\n        \"Survived\": predictions\n    })\nsubmission.to_csv('submission.csv', index=False)","83d963c7":"## Model Evaluation and Comparison\nNow it's time to figure out which algorithm is going to deliver the best model. We could rank our evaluation of all the models to choose the best one for our problem.\n### Cross Validation\nMany a times, the data is imbalanced, to overcome this and get a generalized model, we use **Cross Validation**. An algorithm may underfit over a dataset for some training data and sometimes also overfit the data for other training set. Thus with cross-validation, we can achieve a generalised model.","941a9e9a":"Next we model using Support Vector Machines which are supervised learning models with associated learning algorithms that analyze data used for classification and regression analysis. Given a set of training samples, each marked as belonging to one or the other of **two categories**, an SVM training algorithm builds a model that assigns new test samples to one category or the other, making it a non-probabilistic binary linear classifier. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Support_vector_machine).","811f6b41":"# Model, Predict and Solve the Problem\n\nAll right so now we have cleaned the features and extracted relevant information and dropped the categorical columns our features should now all be numeric, a format suitable to feed into our Machine Learning models. There are 60+ predictive modelling algorithms to choose from. We must understand the type of problem and solution requirement to narrow down to a select few models which we can evaluate. Our problem is a classification and regression problem. We want to identify relationship between output (Survived or not) with other variables or features (Gender, Age, Port...). We are also perfoming a category of machine learning which is called supervised learning as we are training our model with a given dataset. With these two criteria - Supervised Learning plus Classification and Regression, we can narrow down our choice of models to a few. These include:\n* Logistic Regression\n* KNN (k-Nearest Neighbors)\n* Support Vector Machines\n* Naive Bayes Classifier\n* Decision Tree\n* Random Forrest\n* Multiple layer perceprton (neural network)\n\n## Accuracy and precision\nWe know that the titanic problem is a binary classification and to evaluate, we just need to calculate the accuracy.\n\n1. **accuracy**\n\n    * Your score is the percentage of passengers you correctly predict. This is known simply as \"accuracy\u201d.\n\n2. **precision** : \n\n    * In pattern recognition, information retrieval and binary classification, precision (also called positive predictive value) is the fraction of relevant instances among the retrieved instances, \n3. **recall** : \n\n    * recall is the fraction of relevant instances that have been retrieved over the total amount of relevant instances. \n4. **F-score** :\n\n    * the F1 score is a measure of a test's accuracy. It considers both the precision p and the recall r of the test to compute the score: p is the number of correct positive results divided by the number of all positive results returned by the classifier, and r is the number of correct positive results divided by the number of all relevant samples (all samples that should have been identified as positive). The F1 score is the harmonic average of the precision and recall, where an F1 score reaches its best value at 1 (perfect precision and recall) and worst at 0.\n\n5. **What is the difference between accuracy and precision?**\n    * \"Accuracy\" and \"precision\" are general terms throughout science. A good way to internalize the difference are the common \"bullseye diagrams\". In machine learning\/statistics as a whole, accuracy vs. precision is analogous to bias vs. variance.\n\n### Best Practice\n* Use cross-validation to make sure your model generalizes to new data (i.e., it doesn\u2019t \u201coverfit\u201d)\n* Implementing a good cross-validation strategy in training the models to find optimal parameter values\n* Use parameter tuning and grid search to select the best performing model out of several different classification algorithms\n\n### Developer Documentation\n* [Sklearn Estimator Overview](http:\/\/scikit-learn.org\/stable\/user_guide.html)\n* [Sklearn Estimator Detail](http:\/\/scikit-learn.org\/stable\/modules\/classes.html)\n* [Choosing Estimator Mind Map](http:\/\/scikit-learn.org\/stable\/tutorial\/machine_learning_map\/index.html)\n* [Choosing Estimator Cheat Sheet](https:\/\/s3.amazonaws.com\/assets.datacamp.com\/blog_assets\/Scikit_Learn_Cheat_Sheet_Python.pdf)\n* [Ensemble Methods](http:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.ensemble)\n* [Generalized Linear Models (GLM)](http:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.linear_model)\n* [Naive Bayes](http:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.naive_bayes)\n* [Nearest Neighbors](http:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.neighbors)\n* [Support Vector Machines (SVM)](http:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.svm)\n* [Decision Trees](http:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.tree)\n* [Discriminant Analysis](http:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.discriminant_analysis)","dbbf65b2":"In pattern recognition, the k-Nearest Neighbors algorithm (or k-NN for short) is a non-parametric method used for classification and regression. A sample is classified by a majority vote of its neighbors, with the sample being assigned to the class most common among its k nearest neighbors (k is a positive integer, typically small). If k = 1, then the object is simply assigned to the class of that single nearest neighbor. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/K-nearest_neighbors_algorithm).","309efc14":"The perceptron is an algorithm for supervised learning of binary classifiers (functions that can decide whether an input, represented by a vector of numbers, belongs to some specific class or not). It is a type of linear classifier, i.e. a classification algorithm that makes its predictions based on a linear predictor function combining a set of weights with the feature vector. The algorithm allows for online learning, in that it processes elements in the training set one at a time. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Perceptron).","c05afc9d":"# **Tutorial Summary for Beginners**\n\nThis notebook summaries my learnings from several most voted tutorail notebooks of the [Titanic competetion](https:\/\/www.kaggle.com\/c\/titanic\/overview). It will walk through the typical workflow for solving data science competitions step-by-step, summary best practice for each step and do comparasion of different models, comprehensive walkthrough of different tech stacks.\n<img src=\"http:\/\/s8.picofile.com\/file\/8344100018\/workflow3.png\" \/>\n\nIf you Like the notebook and think that it helped you, <font color=\"red\"><b> please upvote<\/b><\/font>. It will keep me motivated.\n\n---\n\n## Table of Content\n1. Data Analysis and Wrangling\n    * Exploratory Data Analysis (EDA)\n    * Feature Engineering\n2. Modeling\n    * Model Evaluation and Comparison\n    * Ensembling and Stacking Models\n3. Submit the Result","0bf79f06":"## Ensembling and Stacking Models\n### Ensembling\nEnsembling is the combination of various simple models to create a single powerful model, which improves the stability of the model. \n#### Voting Classifier\nIt is the simplest way of combining predictions from many different simple machine learning models. It gives an average prediction result based on the prediction of all the submodels. The submodels or the basemodels are all of diiferent types.\n#### Bagging\nBagging is a general ensemble method. It works by applying similar classifiers on small partitions of the dataset and then taking the average of all the predictions. Due to the averaging, there is reduction in variance. Unlike Voting Classifier, Bagging makes use of similar classifiers. Bagging works best with models with high variance. An example for this can be Decision Tree or Random Forests.\n#### Boosting\nBoosting is an ensembling technique which uses sequential learning of classifiers. It is a step by step enhancement of a weak model. Boosting works as follows:\n\nA model is first trained on the complete dataset. Now the model will get some instances right while some wrong. Now in the next iteration, the learner will focus more on the wrongly predicted instances or give more weight to it. Thus it will try to predict the wrong instance correctly. Now this iterative process continous, and new classifers are added to the model until the limit is reached on the accuracy.\n### Stacking\nStacking uses predictions of base classifiers as input for training to a second-level model.\n\n### Best Practices\n* There have been quite a few articles and Kaggle competition winner stories about the merits of having trained models that are more uncorrelated with one another producing better scores. The more uncorrelated the results, the better the final score.\n","b363fbef":"We can use Logistic Regression to validate our assumptions and decisions for feature creating and completing goals. This can be done by calculating the coefficient of the features in the decision function.\n\nPositive coefficients increase the log-odds of the response (and thus increase the probability), and negative coefficients decrease the log-odds of the response (and thus decrease the probability).\n\nSex is highest positivie coefficient, implying as the Sex value increases (male: 0 to female: 1), the probability of Survived=1 increases the most.\nInversely as Pclass increases, probability of Survived=1 decreases the most.\nThis way Age*Class is a good artificial feature to model as it has second highest negative correlation with Survived.\nSo is Title as second highest positive correlation.","2b0be410":"## Feature Engineering\n### Observations\n* Ticket feature may be dropped from our analysis as it contains high ratio of duplicates (22%) and there may not be a correlation between Ticket and survival.\n* Cabin feature may be dropped as it is highly incomplete or contains many null values both in training and test dataset.\n* PassengerId may be dropped from training dataset as it does not contribute to survival.\n* Pclass We observe significant correlation (>0.5) among Pclass=1 and Survived.\n* Sex We confirm the observation during problem definition that Sex=female had very high survival rate at 74%.\n* Name feature is relatively non-standard, may not contribute directly to survival, so maybe dropped. We may want to engineer the Name feature to extract Title as a new feature.\n* We may want to complete Age and the Embarked feature as they are correlated to survival or another important feature\n* We may want to create new feature for Age bands. This turns a continous numerical feature into an ordinal categorical feature.\n* We may want to create a new feature called Family based on Parch and SibSp to get total count of family members on board.\n* We may also want to create a Fare range feature if it helps our analysis.\n\n### Decisions\n* Drop Ticket, Cabin and PassengerId features\n* Extract Title as a new feature and drop name feature\n* Numerical features imputation by guessing missing values using other correlated features\n* Age bands\n* Person feature as males, females, and child\n* New feature based on Parch and SibSp \n\n### Best Practice\n* The 4 C's of Data Cleaning: 1) **correcting** aberrant values and outliers, 2) **completing** missing information, 3) **creating** new features for analysis, and 4) **converting** fields to the correct format for calculations and presentation.\n* Convert features which contain strings to numerical values. This is required by most model algorithms.\n* Continuous value can be placed into quartile bins\/groups follow the distribution of the feature, i.e. convert it into ordinal values by either Binning or Normalisation\n* Categorical variables need to be transformed to numeric variables\n* Extract, combine features to create new features, in the mean time drop useless or redundent features\n* Feature imputation: 1) replacing missing values with a sensible values given the distribution of the data, e.g., the mean, median or mode. 2) go with prediction, i.e. create a model predicting feature values based on other variables\n\n### Developer Documentation\n* [pandas.DataFrame](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.html)\n* [pandas.DataFrame.info](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.info.html)\n* [pandas.DataFrame.describe](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.describe.html)\n* [Indexing and Selecting Data](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/indexing.html)\n* [pandas.isnull](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.isnull.html)\n* [pandas.DataFrame.sum](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.sum.html)\n* [pandas.DataFrame.mode](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.mode.html)\n* [pandas.DataFrame.copy](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.copy.html)\n* [pandas.DataFrame.fillna](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.fillna.html)\n* [pandas.DataFrame.drop](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.drop.html)\n* [pandas.Series.value_counts](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.Series.value_counts.html)\n* [pandas.DataFrame.loc](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.DataFrame.loc.html)\n* [Categorical Encoding](http:\/\/pbpython.com\/categorical-encoding.html)\n* [Sklearn LabelEncoder](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.LabelEncoder.html)\n* [Sklearn OneHotEncoder](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.preprocessing.OneHotEncoder.html)\n* [Pandas Categorical dtype](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/categorical.html)\n* [pandas.get_dummies](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/generated\/pandas.get_dummies.html)","e8fedc3c":"# Submit the Result\nSubmission to the competition using SVC results in scoring 3,016 of 11,481 competition entries. Any suggestions to improve our score are most welcome.\n\n## References\n\nThis notebook is based on great work done by others solving the Titanic competition.\n- [Titanic Data Science Solutions](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions?scriptVersionId=10431564)\n- [An Interactive Data Science Tutorial](https:\/\/www.kaggle.com\/helgejo\/an-interactive-data-science-tutorial)\n- [A Comprehensive ML Workflow with Python](https:\/\/www.kaggle.com\/mjbahmani\/a-comprehensive-ml-workflow-with-python)\n- [Introduction to Ensembling\/Stacking in Python](https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python)","4a9e4354":"In machine learning, naive Bayes classifiers are a family of simple probabilistic classifiers based on applying Bayes' theorem with strong (naive) independence assumptions between the features. Naive Bayes classifiers are highly scalable, requiring a number of parameters linear in the number of variables (features) in a learning problem. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Naive_Bayes_classifier).","dd3f11aa":"Decision tree as a predictive model which maps features (tree branches) to conclusions about the target value (tree leaves). Tree models where the target variable can take a finite set of values are called classification trees; in these tree structures, leaves represent class labels and branches represent conjunctions of features that lead to those class labels. Decision trees where the target variable can take continuous values (typically real numbers) are called regression trees. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Decision_tree_learning).","efb1e08f":"The next model Random Forests is one of the most popular. Random forests or random decision forests are an ensemble learning method for classification, regression and other tasks, that operate by constructing a multitude of decision trees (n_estimators=100) at training time and outputting the class that is the mode of the classes (classification) or mean prediction (regression) of the individual trees. Reference [Wikipedia](https:\/\/en.wikipedia.org\/wiki\/Random_forest).\n\nWhile both Decision Tree and Random Forest score the same, we would choose to use Random Forest as of decision trees' habit of overfitting to the training set. ","926313ff":"# Data Analysis and Wrangling\nThis section will talk about cleanup, imputation, outliers detection and transform\/encode of the data. Statistical summaries and visulization plots will be used to help recognizing underlying pattens to exploit in the model.\n## Exploratory Data Analysis (EDA)\nAnalyze, identify patterns, and explore the data with the following questions:\n1. Which features are categorical? Within categorical features are the values nominal, ordinal, ratio, or interval based?\n> Categorical: Survived, Sex, and Embarked. Ordinal: Pclass.\n2. Which features are numerical? Within numerical features are the values discrete, continuous, or timeseries based? \n> Continous: Age, Fare. Discrete: SibSp, Parch.\n3. Which features are mixed data types?\n> Ticket is a mix of numeric and alphanumeric data types. Cabin is alphanumeric.\n4. Which features may contain errors or typos?\n> Name feature may contain errors or typos as there are several ways used to describe a name including titles, round brackets, and quotes used for alternative or short names.\n5. Which features contain blank, null or empty values?\n> Cabin > Age > Embarked features contain a number of null values in that order for the training dataset.\n> Cabin > Age are incomplete in case of test dataset\n6. What is the distribution of numerical feature values across the samples? This helps us determine, among other early insights, how representative is the training dataset of the actual problem domain.\n> * Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).\n> * Survived is a categorical feature with 0 or 1 values.\n> * Around 38% samples survived representative of the actual survival rate at 32%.\n> * Most passengers (> 75%) did not travel with parents or children.\n> * Nearly 30% of the passengers had siblings and\/or spouse aboard.\n> * Fares varied significantly with few passengers (<1%) paying as high as $512.\n> * Few elderly passengers (<1%) within age range 65-80\n7. What is the distribution of categorical features?\n> * Names are unique across the dataset (count=unique=891)\n> * Sex variable as two possible values with 65% male (top=male, freq=577\/count=891).\n> * Cabin values have several dupicates across samples. Alternatively several passengers shared a cabin.\n> * Embarked takes three possible values. S port used by most passengers (top=S)\n> * Ticket feature has high ratio (22%) of duplicate values (unique=681).\n\n### Best Practice\n* Feature correlation analysis may help in creating, completing, or correcting features.\n* A histogram chart is useful for analyzing continous numerical variables like Age, where banding or ranges will help identify useful patterns. "}}