{"cell_type":{"abd9faf4":"code","d2b31098":"code","9d50f9c3":"code","da061e46":"code","59af949c":"code","94a17b00":"code","80cb2281":"code","dcf5e71e":"code","3706f3d3":"code","12ea79ae":"code","e17598d2":"code","761e274a":"code","e5529c9e":"code","146d56d1":"code","1f68e52b":"code","f9153a81":"code","922fae1b":"code","52aec43d":"code","9965167b":"code","2b797bb7":"code","7bf598ea":"code","eac072a4":"code","c84548c3":"code","f46c5ccf":"code","99b4df1d":"code","2b3420ae":"code","6df70c57":"code","e4325c04":"code","0aac93f8":"code","001060c9":"code","acb0736b":"code","1a07b496":"code","fa1336d2":"code","4bc81147":"code","87244964":"code","7a6d1ce7":"code","4a36d21c":"code","36e894a4":"code","5d1ee20e":"code","2b410af0":"code","86ad0a49":"code","49a5b9fe":"code","b4c4e543":"code","6db2bdf6":"code","d6f64260":"code","8e2bdc92":"code","6fb648f7":"code","5b994dbf":"code","f20b4b0e":"markdown","03981ae9":"markdown","691f7594":"markdown","4476f75f":"markdown","1e30c28f":"markdown","6b0d9492":"markdown","2595c6be":"markdown","9c762580":"markdown","818cca18":"markdown","4b211abc":"markdown","cc2f7696":"markdown","3bb73c29":"markdown","b979e562":"markdown"},"source":{"abd9faf4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport math\nimport datetime\nfrom wordcloud import WordCloud, STOPWORDS\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d2b31098":"train_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntrain_df.head()","9d50f9c3":"train_df.shape","da061e46":"test_df.head()","59af949c":"test_df.shape","94a17b00":"train_df.isna().sum()","80cb2281":"test_df.isna().sum()","dcf5e71e":"train_df['keyword'] = train_df['keyword'].fillna('DUMMY_VALUE')\ntest_df['keyword'] = test_df['keyword'].fillna('DUMMY_VALUE')\ntrain_df.isna().sum()","3706f3d3":"train_df['keyword'].value_counts()","12ea79ae":"train_df['final_text'] = train_df['keyword'] + train_df['text']\ntest_df['final_text'] = test_df['keyword'] + test_df['text']","e17598d2":"cols_del =['id','keyword','location']\ntrain_df = train_df.drop(cols_del,axis=1)\ntest_df = test_df.drop(cols_del,axis=1)","761e274a":"train_df.head()","e5529c9e":"test_df.head()","146d56d1":"sns.countplot(x = 'target', data = train_df)\nplt.xlabel('Class Names')\nplt.ylabel('Count')\nplt.title('Distribution of classes in the training dataset')\nplt.show()","1f68e52b":"train_df['word_count'] = train_df.final_text.apply(len)\ntrain_df.head()","f9153a81":"def cleaned_text(text):\n    clean=re.sub(\"http\\S+\",\"\",text)\n    clean=re.sub(\"pic.twitter\\S+\",\"\",clean)\n    clean=re.sub(\"@\\S+\",\"\",clean)\n    clean = re.sub('#', '', clean)\n    clean = re.sub('goooooooaaaaaal', 'goal', clean)\n    clean = re.sub('SOOOO', 'SO', clean)\n    clean = re.sub('LOOOOOOL', 'LOL', clean)\n    clean = re.sub('Cooool', 'cool', clean)\n    clean = re.sub('|', '', clean)\n    clean = re.sub(r'\\?{2,}', '? ', clean)\n    clean = re.sub(r'\\.{2,}', '. ', clean)\n    clean = re.sub(r'\\!{2,}', '! ', clean)\n    clean = re.sub('&amp;', '&', clean)\n    clean = re.sub('Comin', 'Coming', clean)\n    clean = re.sub('&gt;', '> ', clean)\n    clean = re.sub('&lt;', '< ', clean)\n    clean = re.sub(r'.:', '', clean)\n    clean = re.sub('baaaack', 'back', clean)\n    clean = re.sub('RT', '', clean)\n    clean = re.sub('\\s{2,}', ' ', clean)\n    clean = clean.lower()\n    return clean\ntrain_df['cleaned_text'] = train_df['final_text'].apply(cleaned_text)\ntest_df['cleaned_text'] = test_df['final_text'].apply(cleaned_text)","922fae1b":"train_df.head()","52aec43d":"train_disaster = train_df[train_df['target']==1]\ntrain_normal = train_df[train_df['target']==0]","9965167b":"\nfig, ax = plt.subplots(1, 2)\n\ndisaster = train_disaster.word_count.to_list()\nnormal = train_normal.word_count.to_list()\n\nax[0].hist(disaster, bins=50, alpha = 0.5, color = 'r')\nax[1].hist(normal, bins=50, alpha = 0.5, color = 'g')\n\nplt.show()","2b797bb7":"train_disaster.word_count.describe()","7bf598ea":"train_disaster[train_disaster['word_count'] == 163]['cleaned_text'].iloc[0]","eac072a4":"spec_chars = [\"!\",'\"',\"#\",\"%\",\"&\",\"'\",\"(\",\")\",\n              \"*\",\"+\",\",\",\"-\",\".\",\"\/\",\":\",\";\",\"<\",\n              \"=\",\">\",\"?\",\"@\",\"[\",\"\\\\\",\"]\",\"^\",\"_\",\n              \"`\",\"{\",\"|\",\"}\",\"~\",\"\u2013\"]\nfor char in spec_chars:\n    train_df['cleaned_text'] = train_df['cleaned_text'].str.replace(char, ' ')\n    train_disaster['cleaned_text'] = train_disaster['cleaned_text'].str.replace(char, ' ')\n    train_normal['cleaned_text'] = train_normal['cleaned_text'].str.replace(char, ' ')\n    test_df['cleaned_text'] = test_df['cleaned_text'].str.replace(char, ' ')\n    ","c84548c3":"train_df[train_df['word_count'] == 163]['cleaned_text'].iloc[0]","f46c5ccf":"stopwords = set(STOPWORDS) \nwordcloud = WordCloud(background_color ='white', \n                    stopwords = stopwords, \n                    min_font_size = 10).generate(' '.join(train_disaster['cleaned_text']))","99b4df1d":"print(wordcloud)\nfig = plt.figure(1)\nplt.figure(figsize=(14,8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","2b3420ae":"wordcloud = WordCloud(background_color ='white', \n                    stopwords = stopwords, \n                    min_font_size = 10).generate(' '.join(train_normal['cleaned_text']))\nprint(wordcloud)\nfig = plt.figure(1)\nplt.figure(figsize=(14,8))\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis('off')\nplt.show()","6df70c57":"from sklearn.model_selection import train_test_split\n\ntrain, test = train_test_split(train_df, test_size=0.2)","e4325c04":"# reqire to download tensorflow 2 for bert without that it shows error\n!pip install bert-for-tf2","0aac93f8":"import tensorflow as tf\nfrom tensorflow import keras\nfrom pylab import rcParams\nimport bert\nfrom bert import BertModelLayer\nfrom bert.loader import StockBertConfig, map_stock_config_to_params, load_stock_weights\nfrom bert.tokenization.bert_tokenization import FullTokenizer\nfrom sklearn.metrics import confusion_matrix, classification_report\n\n%matplotlib inline\n%config InlineBackend.figure_format='retina'\n\nsns.set(style='whitegrid', palette='muted', font_scale=1.2)\n\nHAPPY_COLORS_PALETTE = [\"#01BEFE\", \"#FFDD00\", \"#FF7D00\", \"#FF006D\", \"#ADFF02\", \"#8F00FF\"]\n\nsns.set_palette(sns.color_palette(HAPPY_COLORS_PALETTE))\n\nrcParams['figure.figsize'] = 12, 8\n\nRANDOM_SEED = 42\n\nnp.random.seed(RANDOM_SEED)\ntf.random.set_seed(RANDOM_SEED)","001060c9":"!wget https:\/\/storage.googleapis.com\/bert_models\/2018_10_18\/uncased_L-12_H-768_A-12.zip","acb0736b":"!unzip uncased_L-12_H-768_A-12.zip","1a07b496":"os.makedirs(\"model\", exist_ok=True)\n!mv uncased_L-12_H-768_A-12\/ model\nbert_model_name=\"uncased_L-12_H-768_A-12\"\n\nbert_ckpt_dir = os.path.join(\"model\/\", bert_model_name)\nbert_ckpt_file = os.path.join(bert_ckpt_dir, \"bert_model.ckpt\")\nbert_config_file = os.path.join(bert_ckpt_dir, \"bert_config.json\")","fa1336d2":"class DisasterDetectionData:\n  DATA_COLUMN = \"cleaned_text\"\n  LABEL_COLUMN = \"target\"\n\n  def __init__(self, train, test, tokenizer: FullTokenizer, classes, max_seq_len=192):\n    self.tokenizer = tokenizer\n    self.max_seq_len = 0\n    self.classes = classes\n    \n    ((self.train_x, self.train_y), (self.test_x, self.test_y)) = map(self._prepare, [train, test])\n\n    print(\"max seq_len\", self.max_seq_len)\n    self.max_seq_len = min(self.max_seq_len, max_seq_len)\n    self.train_x, self.test_x = map(self._pad, [self.train_x, self.test_x])\n\n  def _prepare(self, df):\n    x, y = [], []\n    \n    for _, row in tqdm(df.iterrows()):\n      text, label = row[DisasterDetectionData.DATA_COLUMN], row[DisasterDetectionData.LABEL_COLUMN]\n      tokens = self.tokenizer.tokenize(text)\n      tokens = [\"[CLS]\"] + tokens + [\"[SEP]\"]\n      token_ids = self.tokenizer.convert_tokens_to_ids(tokens)\n      self.max_seq_len = max(self.max_seq_len, len(token_ids))\n      x.append(token_ids)\n      y.append(self.classes.index(label))\n\n    return np.array(x), np.array(y)\n\n  def _pad(self, ids):\n    x = []\n    for input_ids in ids:\n      input_ids = input_ids[:min(len(input_ids), self.max_seq_len - 2)]\n      input_ids = input_ids + [0] * (self.max_seq_len - len(input_ids))\n      x.append(np.array(input_ids))\n    return np.array(x)","4bc81147":"tokenizer = FullTokenizer(vocab_file=os.path.join(bert_ckpt_dir, \"vocab.txt\"))\ntokenizer.tokenize(\"I can't wait to visit Bulgaria again!\")","87244964":"tokens = tokenizer.tokenize(\"I can't wait to visit Bulgaria again!\")\ntokenizer.convert_tokens_to_ids(tokens)","7a6d1ce7":"from tqdm import tqdm\n\ndef create_model(max_seq_len, bert_ckpt_file):\n\n  with tf.io.gfile.GFile(bert_config_file, \"r\") as reader:\n      bc = StockBertConfig.from_json_string(reader.read())\n      bert_params = map_stock_config_to_params(bc)\n      bert_params.adapter_size = None\n      bert = BertModelLayer.from_params(bert_params, name=\"bert\")\n        \n  input_ids = keras.layers.Input(shape=(max_seq_len, ), dtype='int32', name=\"input_ids\")\n  bert_output = bert(input_ids)\n\n  print(\"bert shape\", bert_output.shape)\n\n  cls_out = keras.layers.Lambda(lambda seq: seq[:, 0, :])(bert_output)\n  cls_out = keras.layers.Dropout(0.5)(cls_out)\n  logits = keras.layers.Dense(units=512, activation=\"tanh\")(cls_out)\n  logits = keras.layers.Dropout(0.5)(logits)\n  logits = keras.layers.Dense(units=128, activation=\"tanh\")(cls_out)\n  logits = keras.layers.Dropout(0.3)(logits)\n  logits = keras.layers.Dense(units=len(classes), activation=\"softmax\")(logits)\n\n  model = keras.Model(inputs=input_ids, outputs=logits)\n  model.build(input_shape=(None, max_seq_len))\n\n  load_stock_weights(bert, bert_ckpt_file)\n        \n  return model","4a36d21c":"classes = train.target.unique().tolist()\n\ndata = DisasterDetectionData(train, test, tokenizer, classes, max_seq_len=128)","36e894a4":"data.train_x.shape","5d1ee20e":"data.train_x[0]","2b410af0":"model = create_model(data.max_seq_len, bert_ckpt_file)\nmodel.summary()","86ad0a49":"model.compile(\n  optimizer=keras.optimizers.Adam(1e-5),\n  loss=keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n  metrics=[keras.metrics.SparseCategoricalAccuracy(name=\"acc\")]\n)","49a5b9fe":"from tensorflow.keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n\n\n    \ncallbacks = [ReduceLROnPlateau(monitor='val_loss', patience=1, verbose=1, factor=0.5), EarlyStopping(monitor='val_loss', patience=2)]\n\nhistory = model.fit(\n  x=data.train_x, \n  y=data.train_y,\n  validation_split=0.1,\n  batch_size=16,\n  shuffle=True,\n  epochs=5,\n  callbacks=callbacks\n)","b4c4e543":"from matplotlib.ticker import MaxNLocator\nfrom matplotlib import rc\n\nax = plt.figure().gca()\nax.xaxis.set_major_locator(MaxNLocator(integer=True))\n\nax.plot(history.history['loss'])\nax.plot(history.history['val_loss'])\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'])\nplt.title('Loss over training epochs')\nplt.show();","6db2bdf6":"ax = plt.figure().gca()\nax.xaxis.set_major_locator(MaxNLocator(integer=True))\n\nax.plot(history.history['acc'])\nax.plot(history.history['val_acc'])\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(['train', 'test'])\nplt.title('Accuracy over training epochs')\nplt.show();","d6f64260":"_, train_acc = model.evaluate(data.train_x, data.train_y)\n_, test_acc = model.evaluate(data.test_x, data.test_y)\n\nprint(\"train acc\", train_acc)\nprint(\"test acc\", test_acc)","8e2bdc92":"y_pred = model.predict(data.test_x).argmax(axis=-1)\nprint(classification_report(data.test_y, y_pred))","6fb648f7":"cm = confusion_matrix(data.test_y, y_pred)\ndf_cm = pd.DataFrame(cm, index=classes, columns=classes)\nhmap = sns.heatmap(df_cm, annot=True, fmt=\"d\")\nhmap.yaxis.set_ticklabels(hmap.yaxis.get_ticklabels(), rotation=0, ha='right')\nhmap.xaxis.set_ticklabels(hmap.xaxis.get_ticklabels(), rotation=30, ha='right')\nplt.ylabel('True label')\nplt.xlabel('Predicted label');","5b994dbf":"sentences = [\n  \"Just happened a terrible car crash\",\n    \"Heard about #earthquake is different cities, stay safe everyone.\",\n    \"No I don't like cold!\",\n    \"@RosieGray Now in all sincerety do you think the UN would move to Israel if there was a fraction of a chance of being annihilated?\"\n  ]\n\npred_tokens = map(tokenizer.tokenize, sentences)\npred_tokens = map(lambda tok: [\"[CLS]\"] + tok + [\"[SEP]\"], pred_tokens)\npred_token_ids = list(map(tokenizer.convert_tokens_to_ids, pred_tokens))\n\npred_token_ids = map(lambda tids: tids +[0]*(data.max_seq_len-len(tids)),pred_token_ids)\npred_token_ids = np.array(list(pred_token_ids))\n\npredictions = model.predict(pred_token_ids).argmax(axis=-1)\n\nfor text, label in zip(sentences, predictions):\n    if classes[label]==1:\n        target=\"Disaster Tweet\"\n        print(\"text:\", text, \"\\nClass:\", target)\n        print()\n    else:\n        target=\"Normal Tweet\"\n        print(\"text:\", text, \"\\nClass:\", target)\n        print()\n        \n  ","f20b4b0e":"### Word cloud Disaster Tweets","03981ae9":"Next we will delete id, keyword and the location feature as it has large number of mising values and it also doesnt serve the purpose in detecting disaster tweets.","691f7594":"### Observation:\n\n- Disaster tweets are comparatively shorter in length as compared to normal tweets in general","4476f75f":"As we can see the model correctly predicted disaster tweets in first two sentences and Normal in last two sentences.","1e30c28f":"#### About BERT\n\nBERT is an acronym of **Bidirectional Encoder Representations from Transformers**. The term bidirectional means that the context of a word is given by both the words that follow it and by the words preceding it. This technique makes this algorithm hard to train but very effective. Exploring the surrounding text around words is computationally expensive but allows a deeper understanding of words and sentences.\n\n![bert](https:\/\/pytorch.org\/tutorials\/_images\/bert.png)\n\nUnidirectional context-oriented algorithm already exist. A neural network can be trained to predict which word will follow a sequence of given words, once trained on a huge dataset of sentences. However, predicting that word from both the previous and following words is not an easy task. \n\nThe only way to do so effectively is to mask some words in a sentence and predict them too, e.g., the sentence **the quick brown fox jumps over the lazy dog** might be masked as **the X brown fox jumps over the Y dog** with label (**X = quick, Y = lazy**) to become a labelled record in a training set of sentences. One can easily derive a training set from a bundle of unsupervised texts by simply masking 15% of words (as BERT does), and training the neural network to deduce the missing words from the remaining ones.\n\nNotice that BERT is truly a deep learning algorithm, while context-free algorithms such as word2vec, based on shallow recurrent networks, may not be. \n\nHowever, as such, BERT\u2019s training is very expensive, due to its transformer aspect. Training on a huge body of text \u2013 for example, all English-language Wikipedia pages \u2013 is an Herculean effort that requires decidedly nontrivial computational power.\n\nWhatever the task, it is not necessary to pre-train the BERT model, but only to fine-tune a pre-trained model on the specific dataset that relates to the problem we want to use BERT to study. We will try to use such a pre-trained model to perform our simple classification task: more exciting use cases may be found on the GitHub page of the project mentioned above, as well as elsewhere on the Web.\n\nFirst, we choose the pre-trained model: in the BERT GitHub repository there are several choices available, we will use `uncased_L-12_H-768_A-12`.\n\nThe pre-trained model can be downloaded from the repository and extracted into a local folder. This folder will contain the following files:\n\n- **bert_config.json**\n- **bert_model.ckpt.data-00000-of-00001**\n- **bert_model.ckpt.index**\n- **vocab.txt**\n\nThe first file contains all the configuration necessary to build a network layer to use this BERT model, while the latter files are needed to properly tokenize our texts. The largest file contains the model, which may be loaded from the BERT library using the methods demonstrated below.\n\nTo remain focused on the model, the assumption will be that our code is run inside a directory. This is necessary before running the following programs:\n\nBefore setting up the model, our dataset is tokenized according to the format expected by the BERT layers; this can be done via the **FullTokenizer** class from the BERT package. \n\nNext, the tokenizer is fed with each sentence in our datsaset. The tokenizer result, which is a list of strings, between **[CLS]** and **[SEP]** is enclosed, as required by the BERT algorithm implementation.\n\nThe output of our model will be simply a number between 0 and 1.","6b0d9492":"### Testing model on Random sentences","2595c6be":"segregating disaster and normal tweets for plotting","9c762580":"### Distribution of Word Count of Disaster Tweets","818cca18":"### Class Distribution","4b211abc":"### Observations\n\n- From both the Word clouds we can easily see the difference between disaster and normal tweets","cc2f7696":"### Word Cloud Normal Tweets","3bb73c29":"Next we will merge Keyword and text feature and use this combined feature for detecting disaster tweets.","b979e562":"### Bert Modelling"}}