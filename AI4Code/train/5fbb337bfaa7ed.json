{"cell_type":{"a65c1ed7":"code","58cf4161":"code","d82bb09b":"code","581886d5":"code","d68e7aba":"code","c189df83":"code","389b9d8d":"code","7a8bc862":"code","56ac20df":"code","ee80dc44":"code","10119791":"code","9b9e8bb5":"code","f99dbc45":"code","70a97e4f":"code","90ce00d8":"code","46b2cc7c":"code","ab840ef4":"code","7ec61850":"code","6d88aeaa":"markdown","46279a1e":"markdown","03766fbe":"markdown","79c5ec75":"markdown","b5b32acc":"markdown","c659fd18":"markdown"},"source":{"a65c1ed7":"# Importing packages\n\nimport os\nimport pandas as pd\nfrom pandas import DataFrame,Series\nfrom sklearn import tree\nimport matplotlib\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\nfrom sklearn.preprocessing import StandardScaler\nimport statsmodels.formula.api as smf\nimport statsmodels.api as sm\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\nfrom sklearn import neighbors\nfrom sklearn import linear_model\n%matplotlib inline","58cf4161":"f = pd.read_csv(\"..\/input\/movie-metadatacsv\/movie_metadata.csv\")","d82bb09b":"data=DataFrame(f)\ndata.head()[:2]","581886d5":"X_data=data.dtypes[data.dtypes!='object'].index\nX_train=data[X_data]\nX_train.head()[:2]","d68e7aba":"X_train.describe()","c189df83":"# Finding all the columns with NULL values\n\nnp.sum(X_train.isnull())","389b9d8d":"# Filling all Null values\nX_train=X_train.fillna(0)\ncolumns=X_train.columns.tolist()\ny=X_train['imdb_score']\nX_train.drop(['imdb_score'],axis=1,inplace=True)\nX_train.head()[:2]","7a8bc862":"# GETTING Correllation matrix\ncorr_mat=X_train.corr(method='pearson')\nplt.figure(figsize=(20,10))\nsns.heatmap(corr_mat,vmax=1,square=True,annot=True,cmap='cubehelix')","56ac20df":"X_Train=X_train.values\nX_Train=np.asarray(X_Train)\n\n# Finding normalised array of X_Train\nX_std=StandardScaler().fit_transform(X_Train)","ee80dc44":"number_of_samples = len(y)\nnp.random.seed(0)\nrandom_indices = np.random.permutation(number_of_samples)\nnum_training_samples = int(number_of_samples*0.75)\nx_train = X_Train[random_indices[:num_training_samples]]\ny_train=y[random_indices[:num_training_samples]]\nx_test=X_Train[random_indices[num_training_samples:]]\ny_test=y[random_indices[num_training_samples:]]\ny_Train=list(y_train)\n","10119791":"model=linear_model.Ridge()\nmodel.fit(x_train,y_train)\ny_predict=model.predict(x_train)\n\nerror=0\nfor i in range(len(y_Train)):\n    error+=(abs(y_Train[i]-y_predict[i])\/y_Train[i])\ntrain_error_ridge=error\/len(y_Train)*100\nprint(\"Train error = \"'{}'.format(train_error_ridge)+\" percent in Ridge Regression\")\n\nY_test=model.predict(x_test)\ny_Predict=list(y_test)\n\nerror=0\nfor i in range(len(y_test)):\n    error+=(abs(y_Predict[i]-Y_test[i])\/y_Predict[i])\ntest_error_ridge=error\/len(Y_test)*100\nprint(\"Test error = \"'{}'.format(test_error_ridge)+\" percent in Ridge Regression\")","9b9e8bb5":"matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\n\npreds = pd.DataFrame({\"preds\":model.predict(x_train), \"true\":y_train})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\npreds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")\nplt.title(\"Residual plot in Ridge Regression\")","f99dbc45":"n_neighbors=5\nknn=neighbors.KNeighborsRegressor(n_neighbors,weights='uniform')\nknn.fit(x_train,y_train)\ny1_knn=knn.predict(x_train)\ny1_knn=list(y1_knn)\n\nerror=0\nfor i in range(len(y_train)):\n    error+=(abs(y1_knn[i]-y_Train[i])\/y_Train[i])\ntrain_error_knn=error\/len(y_Train)*100\nprint(\"Train error = \"+'{}'.format(train_error_knn)+\" percent\"+\" in Knn algorithm\")\n\ny2_knn=knn.predict(x_test)\ny2_knn=list(y2_knn)\nerror=0\nfor i in range(len(y_test)):\n    error+=(abs(y2_knn[i]-Y_test[i])\/Y_test[i])\ntest_error_knn=error\/len(Y_test)*100\nprint(\"Test error = \"'{}'.format(test_error_knn)+\" percent\"+\" in knn algorithm\")","70a97e4f":"matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\npreds = pd.DataFrame({\"preds\":knn.predict(x_train), \"true\":y_train})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\npreds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")\nplt.title(\"Residual plot in Knn\")","90ce00d8":"dec = tree.DecisionTreeRegressor(max_depth=1)\ndec.fit(x_train,y_train)\ny1_dec=dec.predict(x_train)\ny1_dec=list(y1_dec)\ny2_dec=dec.predict(x_test)\ny2_dec=list(y2_dec)\n\nerror=0\nfor i in range(len(y_train)):\n    error+=(abs(y1_dec[i]-y_Train[i])\/y_Train[i])\ntrain_error_tree=error\/len(y_Train)*100\nprint(\"Train error = \"+'{}'.format(train_error_tree)+\" percent\"+\" in Decision Tree Regressor\")\n\nerror=0\nfor i in range(len(y_test)):\n    error+=(abs(y1_dec[i]-Y_test[i])\/Y_test[i])\ntest_error_tree=error\/len(Y_test)*100\nprint(\"Test error = \"'{}'.format(test_error_tree)+\" percent in Decision Tree Regressor\")","46b2cc7c":"matplotlib.rcParams['figure.figsize'] = (6.0, 6.0)\npreds = pd.DataFrame({\"preds\":dec.predict(x_train), \"true\":y_train})\npreds[\"residuals\"] = preds[\"true\"] - preds[\"preds\"]\npreds.plot(x = \"preds\", y = \"residuals\",kind = \"scatter\")\nplt.title(\"Residual plot in Decision Tree\")","ab840ef4":"train_error=[train_error_ridge,train_error_knn,train_error_tree]\ntest_error=[test_error_ridge,test_error_knn,test_error_tree]\n\ncol={'Train Error':train_error,'Test Error':test_error}\nmodels=['Ridge Regression','Knn','Decision Tree']\ndf=DataFrame(data=col,index=models)\ndf","7ec61850":"df.plot(kind='bar')","6d88aeaa":"**Since 5 components can explain more than 70% of the variance, we choose the number of the components to be 5**","46279a1e":"**Decision Tree Regressor**","03766fbe":"**Knn Algorithm**","79c5ec75":"*Getting non-object elements*\n","b5b32acc":"**Hello everyone.This is a notebook comparing various regression models such as Ridge,Knn,Bayesian Regression,Decision Tree and SVM.**\n*It is extremely beneficial for beginners to take a close look at the notebook so as to get an insight as to how different algorithms work and also which algorithms can perform better in some cases depending upon cases*","c659fd18":"**Ridge Regression**"}}