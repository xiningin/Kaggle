{"cell_type":{"7eaaa61f":"code","7ab9ccde":"code","c04c7ac4":"code","eaf7e696":"code","ea331c86":"code","5d2360fa":"code","a670af2d":"code","f03f9f8b":"code","57cd8c86":"code","976157f5":"code","b6be5aab":"code","b99dfef2":"code","fd3c594c":"code","884acbeb":"code","41994d62":"code","fd0903b2":"code","021b7e78":"code","f3d85c27":"code","d0964c38":"code","3e5c907d":"code","e672093c":"code","e48dd4b9":"code","ea672c71":"code","638abfa2":"code","fbe580f9":"code","86d5c3d2":"code","bdf51bda":"code","ab099896":"code","25386ba5":"code","9c1aeced":"code","da077022":"code","f0a31417":"code","f7d14a1d":"code","a8ceb90f":"code","495e7b87":"code","40ee422d":"code","5a6c88bb":"code","82e0e4c6":"code","a1d7b851":"code","cc4be581":"code","6ec62b8c":"code","421beee3":"code","3ef9cb32":"code","d7ce6457":"code","fb190351":"code","bca0b6fe":"code","16173254":"code","bf99b061":"code","d7756312":"code","b6b25818":"code","91d58de1":"code","19819c05":"code","0a4d4e13":"code","11035911":"code","a788f001":"code","675b8e19":"code","8e1a2c88":"code","fa5c5b2c":"code","0104746f":"code","85a72f31":"code","a192ac7d":"code","2f8a4b80":"code","1665e773":"code","e19bd749":"code","84251a24":"code","810a03db":"code","54ffb1b6":"code","96dadf5a":"code","4f490d9b":"code","6cb20448":"code","a8c66204":"code","ba9866b3":"code","1e310bdc":"code","b56b641d":"markdown","31bc4944":"markdown","e84ff083":"markdown","1facff81":"markdown","5c990652":"markdown","a34a90ad":"markdown","1d8b8a68":"markdown","e59bca80":"markdown","7ede812f":"markdown","3084b6c6":"markdown","168c3584":"markdown","c53940ca":"markdown","42c0332d":"markdown"},"source":{"7eaaa61f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\nimport tensorflow as tf\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.utils import shuffle","7ab9ccde":"train_read = pd.read_csv(\"..\/input\/facial-keypoints-detection\/training.zip\", \n                       compression='zip')\nprint ('shape of dataframe: ', train_read.shape)","c04c7ac4":"# see few columns of the training data \ntrain_read.head(3).T","eaf7e696":"print ('nan in every cols: ', train_read.isna().sum())","ea331c86":"fig, axes = plt.subplots(5, 6, figsize=(15, 9))\nax = axes.ravel() \nfor i in range(30):\n  ax[i].hist(train_read[train_read.columns[i]], bins=50, density=True, alpha=0.7, color='magenta')\n  ax[i].set_title(train_read.columns[i],fontsize=7)\n  # ax[i].axes.get_xaxis().set_visible(False)\nplt.tight_layout()  ","5d2360fa":"train_clean = train_read.dropna(axis=0, how='any', inplace=False)\ntrain_clean = train_clean.reset_index(drop=True)","a670af2d":"clean_imgs = []\n# print (train_clean[['Image']].shape)\nfor i in range(0, len(train_clean)):\n  x_c = train_clean['Image'][i].split(' ') # split the pixel values based on the space \n  x_c = [y for y in x_c] # create the listed pixels\n  clean_imgs.append(x_c)\nclean_imgs_arr = np.array(clean_imgs, dtype='float') # arrays are always better than lists","f03f9f8b":"clean_imgs_arr = np.reshape(clean_imgs_arr, (train_clean.shape[0], 96, 96, 1))\ntrain_ims_clean = clean_imgs_arr\/255.","57cd8c86":"clean_keypoints_df = train_clean.drop('Image', axis=1)\nprint ('check shape after dropping Image col in clean df: ', clean_keypoints_df.shape)\n\nclean_keypoints_arr = clean_keypoints_df.to_numpy()\nprint ('check shape of clean keypoints arr: ', clean_keypoints_arr.shape)","976157f5":"def standardize_keypoint(keypoints):\n  y_points = (keypoints - 48.)\/48. \n  print ('check keypoints max and min: ', np.max(y_points), np.min(y_points))\n  return y_points\ndef revert_standardize(keypoints):\n    ys_points = 48*(keypoints + 1.)\n    print ('check keypoints max and min: ', np.max(ys_points), np.min(ys_points))\n    return ys_points","b6be5aab":"def vis_im_keypoint(img, points, axs):\n  axs.imshow(img.reshape(96, 96))\n  # points should be in the standardized format \n  xcoords = 48* (points[0::2] + 1.)\n  ycoords = 48* (points[1::2] + 1.) \n  axs.scatter(xcoords, ycoords, color='red', marker='o')\n\ndef vis_im_keypoint_notstandard(img, points, axs):\n  # fig = plt.figure(figsize=(6, 4))\n  axs.imshow(img.reshape(96, 96))\n  # points should be in the standardized \n  xcoords = (points[0::2] + 0.)\n  ycoords = (points[1::2] + 0.) \n  axs.scatter(xcoords, ycoords, color='red', marker='o')","b99dfef2":"# imgs_train_clean, imgs_val_clean, points_train_clean, points_val_clean = train_test_split(train_ims_clean, clean_keypoints_arr, \n#                                                                   test_size=0.05, random_state=21)\n\n# print ('train clean image data size: ', imgs_train_clean.shape)\n# print ('train clean keypoints data size: ', points_train_clean.shape)\n# print ('validation clean image data size: ', imgs_val_clean.shape)","fd3c594c":"# points_train_standardize_clean = standardize_keypoint(points_train_clean)\n# points_val_standardize_clean = standardize_keypoint(points_val_clean)\n\n# print ('check example standardize keypoint: ', points_train_standardize_clean[10])","884acbeb":"def flip_im_points1(img, points):\n  flip_im = np.fliplr(img)\n  xcoords = points[0::2]\n  ycoords = points[1::2]\n  new_points = []\n  for i in range(len(xcoords)):\n    xp = xcoords[i]\n    yp = ycoords[i]\n    new_points.append(xp*(-1))\n    new_points.append(yp)\n  return flip_im, np.asarray(new_points)  \n\ndef flip_im_points0(img, points): # use keypoints that are not standardized\n  flip_im = np.fliplr(img)\n  xcoords = points[0::2]\n  ycoords = points[1::2]\n  new_points = []\n  for i in range(len(xcoords)):\n    xp = xcoords[i]\n    yp = ycoords[i]\n    new_points.append(96.-xp)\n    new_points.append(yp)\n  return flip_im, np.asarray(new_points)","41994d62":"import imgaug as ia\nimport imgaug.augmenters as iaa\n\ndef gnoise_lincontrast(im_tr, pt_tr):\n  seq = iaa.Sequential([iaa.LinearContrast((0.6, 1.5)), \n                        iaa.Sometimes(\n        0.80, iaa.GaussianBlur(sigma=(0., 2.0)))])\n  aug_ims = []\n  aug_pts = []\n  for im, pt in zip(im_tr, pt_tr):\n    #f_im, f_pts = flip_im_points1(im, pt)\n    f_im = seq(image=im)\n    aug_ims.append(im)\n    aug_ims.append(f_im)\n    aug_pts.append(pt)\n    aug_pts.append(pt)\n  return np.asarray(aug_ims), np.asarray(aug_pts)\n    ","fd0903b2":"aug_ims_train_clean_g, aug_points_train_clean_g = gnoise_lincontrast(train_ims_clean, clean_keypoints_arr)\nprint (type(aug_ims_train_clean_g), aug_ims_train_clean_g.shape, aug_points_train_clean_g.shape)","021b7e78":"fig = plt.figure(figsize=(8, 9))\nnpics= 16\ncount = 1\nfor i in range(npics):\n  ipic = i # use this to see original and augmented image side by side\n#   ipic = np.random.choice(aug_ims_train_clean.shape[0])\n  ax = fig.add_subplot(npics\/4 , 4, count, xticks=[],yticks=[])\n  vis_im_keypoint_notstandard(aug_ims_train_clean_g[ipic], aug_points_train_clean_g[ipic], ax)\n  count = count + 1\n\n# plt.title('Gaussian Blur and Linear Contrast')\nplt.tight_layout()\nplt.show()    ","f3d85c27":"# include rotation augmentation \n\nfrom imgaug.augmentables import Keypoint, KeypointsOnImage\n\ndef rotate_aug(im_tr, pt_tr):\n  seq = iaa.Sequential([iaa.Affine(rotate=15, scale=(0.8, 1.2))])\n  #image_aug, kps_aug = seq(image=image, keypoints=kps)\n  aug_ims = []\n  aug_pts = []\n  coordlist = []\n  for im, pt in zip(im_tr, pt_tr):\n    #f_im, f_pts = flip_im_points1(im, pt)\n    xcoord = pt[0::2]\n    ycoord = pt[1::2]\n    for i in range(len(xcoord)): \n      coordlist.append(Keypoint(xcoord[i], ycoord[i]))\n    kps = KeypointsOnImage(coordlist, shape=im.shape)  \n    f_im, f_kp = seq(image=im, keypoints=kps)\n    #new_xcoords = []\n    #new_ycoords = []\n    all_coords = []\n    for k in range(len(kps.keypoints)):\n      before = kps.keypoints[k]\n      after = f_kp.keypoints[k]\n      # print(\"Keypoint %d: (%.8f, %.8f) -> (%.8f, %.8f)\" % (\n      #     i, before.x, before.y, after.x, after.y)\n      # )\n      all_coords.append(after.x)\n      all_coords.append(after.y)\n      all_coords_arr = np.asarray(all_coords)\n    aug_ims.append(im)\n    aug_ims.append(f_im)\n    aug_pts.append(pt)\n    aug_pts.append(all_coords)\n    coordlist.clear()\n  return np.asarray(aug_ims), np.asarray(aug_pts)","d0964c38":"aug_ims_train_clean_g2, aug_points_train_clean_g2 = rotate_aug(aug_ims_train_clean_g, aug_points_train_clean_g)\n\nprint (type(aug_ims_train_clean_g2), aug_ims_train_clean_g2.shape, aug_points_train_clean_g2.shape)","3e5c907d":"fig = plt.figure(figsize=(8, 9))\nnpics= 20\ncount = 1\nfor i in range(npics):\n  ipic = i # use this to see original and augmented image side by side\n#   ipic = np.random.choice(aug_ims_train_clean.shape[0])\n  ax = fig.add_subplot(npics\/4 , 5, count, xticks=[],yticks=[])\n  vis_im_keypoint_notstandard(aug_ims_train_clean_g2[ipic], aug_points_train_clean_g2[ipic], ax)\n  count = count + 1\n\n\nplt.tight_layout()\nplt.show()","e672093c":"### add the flipped images in the training data-set\ndef aug_flip(im_tr, pt_tr):\n  aug_ims = []\n  aug_pts = []\n  for im, pt in zip(im_tr, pt_tr):\n    f_im, f_pts = flip_im_points1(im, pt)\n    aug_ims.append(im)\n    aug_ims.append(f_im)\n    aug_pts.append(pt)\n    aug_pts.append(f_pts)\n  return np.asarray(aug_ims), np.asarray(aug_pts)\n\ndef aug_flip0(im_tr, pt_tr):\n  aug_ims = []\n  aug_pts = []\n  for im, pt in zip(im_tr, pt_tr):\n    f_im, f_pts = flip_im_points0(im, pt)\n    aug_ims.append(im)\n    aug_ims.append(f_im)\n    aug_pts.append(pt)\n    aug_pts.append(f_pts)\n  return np.asarray(aug_ims), np.asarray(aug_pts)","e48dd4b9":"# aug_points_train_clean_g2_norm = standardize_keypoint(aug_points_train_clean_g2)\n# points_val_standardize_clean = standardize_keypoint(points_val_clean)\n\n# print ('check example standardize keypoint: ', aug_points_train_clean_g2_norm[10])","ea672c71":"aug_ims_train_clean_g3, aug_points_train_clean_g3 = aug_flip0(aug_ims_train_clean_g2, \n                                                         aug_points_train_clean_g2)\n\nprint ('size of training data now: ', aug_ims_train_clean_g3.shape, aug_points_train_clean_g3.shape)\n\n# aug_ims_train_clean_g3, aug_points_train_clean_g3 = aug_flip(aug_ims_train_clean_g2, \n#                                                          aug_points_train_clean_g2_norm)\n\n# print ('size of training data now: ', aug_ims_train_clean_g3.shape, aug_points_train_clean_g3.shape)","638abfa2":"fig = plt.figure(figsize=(10, 9))\nnpics= 24\ncount = 1\nfor i in range(npics):\n  ipic = i # use this to see original and augmented image side by side\n#   ipic = np.random.choice(aug_ims_train_clean.shape[0])\n  ax = fig.add_subplot(npics\/4 , 6, count, xticks=[],yticks=[])\n  vis_im_keypoint_notstandard(aug_ims_train_clean_g3[ipic], aug_points_train_clean_g3[ipic], ax)\n  count = count + 1\n\n\nplt.tight_layout()\nplt.show()","fbe580f9":"fig = plt.figure(figsize=(10, 9))\nnpics= 24\ncount = 1\nfor i in range(npics):\n  #ipic = i # use this to see original and augmented image side by side\n  ipic = np.random.choice(aug_ims_train_clean_g3.shape[0])\n  ax = fig.add_subplot(npics\/4 , 6, count, xticks=[],yticks=[])\n  vis_im_keypoint_notstandard(aug_ims_train_clean_g3[ipic], aug_points_train_clean_g3[ipic], ax)\n  count = count + 1\n\n\nplt.tight_layout()\nplt.show()","86d5c3d2":"from sklearn.utils import shuffle\naug_ims_train_final, aug_points_train_final = shuffle(aug_ims_train_clean_g3, aug_points_train_clean_g3)\nprint ('check number of training files: ', len(aug_points_train_final))","bdf51bda":"from tensorflow.keras.layers import Input, Conv2D, MaxPooling2D, Dropout, \\\n     Flatten, BatchNormalization, Dense, Concatenate, GlobalAveragePooling2D\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.activations import elu, relu\nfrom tensorflow.keras.optimizers import Adam, RMSprop, SGD\n# from tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.regularizers import l2","ab099896":"# input_im = Input(shape=(96, 96, 1))\n# def model():\n#   #layer 1: \n#   conv1 = Conv2D(32, (3, 3), activation='relu', )(input_im) #96 x 96 x 32\n#   conv2 = Conv2D(32, (3, 3), activation='relu', )(conv1) #96 x 96 x 32\n#   pool1 = MaxPooling2D((2, 2))(conv2) \n#   conv3 = Conv2D(64, (3, 3), activation='relu', )(pool1) #48 x 16 x 64\n#   conv4 = Conv2D(64, (3, 3), activation='relu', )(conv3)\n#   pool2 = MaxPooling2D(pool_size=(2, 2))(conv4)\n#   conv5 = Conv2D(128, (3, 3), padding='same', activation='relu',)(pool2)\n#   conv6 = Conv2D(128, (3, 3), padding='same', activation='relu',)(conv5)\n#   conv7 = Conv2D(128, (3, 3), padding='same', activation='relu',)(conv6)\n#   pool3 = MaxPooling2D(pool_size=(2, 2))(conv7)\n#   conv8 = Conv2D(256, (3, 3), padding='same', activation='relu',)(pool3)\n#   conv9 = Conv2D(256, (3, 3), padding='same', activation='relu',)(conv8)\n#   conv10 = Conv2D(256, (3, 3), padding='same', activation='relu',)(conv9)\n#   pool4 = MaxPooling2D(pool_size=(2, 2))(conv10)\n#   flat = Flatten()(pool4)\n#   den1 = Dense(128, activation='relu')(flat)\n#   den1 = Dropout(0.20)(den1)\n#   den2 = Dense(64, activation='relu')(den1)\n#   den2 = Dropout(0.20)(den2)\n#   pred = Dense(clean_keypoints_arr.shape[1])(den2)\n#   model = Model(inputs=input_im, outputs=pred, name='VGG_Like')\n#   return model ","25386ba5":"class customCallbacks(tf.keras.callbacks.Callback):\n  def on_epoch_end(self, epoch, logs=None):\n    self.epoch = epoch + 1\n    if self.epoch % 50 == 0:\n      print ('epoch num {}, train mae: {}, validation mae: {}'.format(epoch, logs['mae'], logs['val_mae']))\n\n\nlearning_rate = 1e-3\n\ndef lrdecay(epoch):\n  lr = 1e-2\n  if epoch > 1600:\n    lr *= 1e-1\n  elif epoch > 800:\n    lr *= 3e-1\n  elif epoch > 400:\n    lr *= 5e-1\n  elif epoch > 200:\n    lr *= 7e-1\n  elif epoch > 100:\n    lr *= 9e-1\n  if epoch % 50 == 0:    \n    print('Learning rate: ', lr)    \n  return lr\n\n\ndef lrexpdecay(epoch):\n  decay = 0.1\n  lr = learning_rate*(np.exp(-decay*epoch))\n  return lr\n\n\ndef earlystop(mode):\n  if mode=='acc':\n    estop = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=20, mode='max')\n  elif mode=='loss':\n    estop = tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=20, mode='min')\n  return estop\n\nreduce_lr = tf.keras.callbacks.ReduceLROnPlateau(monitor='val_mae', factor=0.8,\n                              patience=25, min_lr=1e-5, verbose=1)\n\nlrdecay = tf.keras.callbacks.LearningRateScheduler(lrdecay) # learning rate decay\n\nsgd = SGD(lr=1e-2, momentum = 0.9,nesterov=True)\nadam = Adam(learning_rate=3e-3)","9c1aeced":"# face_key_model_aug = model()\n# face_key_model_aug.summary()","da077022":"# face_key_model_aug.compile(loss='mse', \n#                        optimizer=adam, \n#                        metrics=['acc', 'mae'])","f0a31417":"# face_key_model_aug_train = face_key_model_aug.fit(aug_ims_train_clean, aug_points_train_clean, \n#                                                   validation_data=(imgs_val_clean, points_val_standardize_clean), \n#                                                   batch_size=128, epochs=500, \n#                                                   callbacks=[customCallbacks(), lrdecay], \n#                                           verbose=0)","f7d14a1d":"# mae = face_key_model_aug_train.history['mae']\n# val_mae = face_key_model_aug_train.history['val_mae']\n\n# loss = face_key_model_aug_train.history['loss']\n# val_loss = face_key_model_aug_train.history['val_loss']\n\n# acc = face_key_model_aug_train.history['acc']\n# val_acc = face_key_model_aug_train.history['val_acc']\n\n# fig = plt.figure(figsize=(7, 3))\n\n# fig.add_subplot(121)\n# plt.plot(range(len(loss)), loss, linestyle='-', color='red', alpha=0.7, label='Train Loss')\n# plt.plot(range(len(loss)), val_loss, linestyle='-.', color='navy', alpha=0.7, label='Val Loss')\n# plt.xlabel('Epochs', fontsize=12)\n# plt.ylabel('Loss', fontsize=13)\n# plt.legend(fontsize=12)\n\n# fig.add_subplot(122)\n# plt.plot(range(len(mae)), mae, linestyle='-', color='red', alpha=0.7, label='Train MAE')\n# plt.plot(range(len(val_mae)), val_mae, linestyle='-.', color='navy', alpha=0.7, label='Val MAE')\n# plt.xlabel('Epochs', fontsize=12)\n# plt.ylabel('MAE', fontsize=13)\n# plt.legend(fontsize=12)\n\n# plt.tight_layout()\n# plt.show()","a8ceb90f":"### let's try to predict some keypoints on the test data-set\ntest_read = pd.read_csv(\"..\/input\/facial-keypoints-detection\/test.zip\", \n                       compression='zip')\nprint ('test dataframe shape; ', test_read.shape)\ntest_read.head(3)","495e7b87":"test_ims = []\n\nfor i in range(0, 1783):\n  x_t = test_read['Image'][i].split(' ') # split the pixel values based on the space \n  x_t = [y for y in x_t] # create the listed pixels\n  test_ims.append(x_t)\ntest_imgs_arr = np.array(test_ims, dtype='float') # arrays are always better than lists\n\ntest_imgs_arr = np.reshape(test_imgs_arr, (1783, 96, 96, 1))\ntest_ims = test_imgs_arr\/255.","40ee422d":"# predict_points_aug = face_key_model_aug.predict(test_ims)\n\n# print ('check shape of predicted points: ', predict_points_aug.shape)","5a6c88bb":"# fig = plt.figure(figsize=(8, 8))\n# npics= 12\n# count = 1\n# for i in range(npics):\n#   # ipic = i\n#   ipic = np.random.choice(test_ims.shape[0])\n#   ax = fig.add_subplot(npics\/3 , 4, count, xticks=[],yticks=[])\n#   vis_im_keypoint(test_ims[ipic], predict_points_aug[ipic], ax)\n#   count = count + 1\n\n\n# plt.tight_layout()\n# plt.show()","82e0e4c6":"# revert the points from standardized coordinate to image shape coordinate\n# predict_points_aug_s = revert_standardize(predict_points_aug)\n# test_predicts = pd.DataFrame(predict_points_aug_s, columns = list(clean_keypoints_df.columns))\n\n# print ('check the new predict data frame: ', '\\n')\n# test_predicts.head(3)","a1d7b851":"lookup_table = pd.read_csv('..\/input\/facial-keypoints-detection\/IdLookupTable.csv')\nprint ('lookup table shape: ', lookup_table.shape)\nlookup_table.head()","cc4be581":"# the required format of output\nsub_form_table = pd.read_csv('..\/input\/facial-keypoints-detection\/SampleSubmission.csv')\nsub_form_table.head()","6ec62b8c":"# for i in range(lookup_table.shape[0]):\n#     lookup_table.Location[i] = test_predicts.loc[lookup_table.ImageId[i]-1][lookup_table.FeatureName[i]]","421beee3":"# sub_form_table.Location = lookup_table.Location\n# new_submission = sub_form_table","3ef9cb32":"# CHECK THE FORMAT\n# new_submission.head(3)","d7ce6457":"\n# new_submission.to_csv('Submission.csv', index=False)","fb190351":"# all_imgs = []\n# print (train_read[['Image']].shape)\n# for i in range(0, 7049):\n#   x = train_read['Image'][i].split(' ') # split the pixel values based on the space \n#   x = [y for y in x] # create the listed pixels\n#   all_imgs.append(x)\n# all_imgs_arr = np.array(all_imgs, dtype='float') # arrays are always better than lists :)","bca0b6fe":"# all_imgs_arr = np.reshape(all_imgs_arr, (7049, 96, 96, 1))\n# train_ims = all_imgs_arr\/255.","16173254":"# keypoints_df = train_read.drop('Image', axis=1)\n# # print ('check shape after dropping Image col: ', keypoints_df.shape)\n\n# keypoints_arr = keypoints_df.to_numpy()\n# print ('check shape: ', keypoints_arr.shape)","bf99b061":"# imgs_train, imgs_val, points_train, points_val = train_test_split(train_ims, keypoints_arr, \n#                                                                   test_size=0.15, random_state=21)\n# print ('train image data size: ', imgs_train.shape)\n# print ('train keypoints data size: ', points_train.shape)\n# print ('validation image data size: ', imgs_val.shape)","d7756312":"# # now we perform the imputation on the train and validation keypoints separately \n# print ('check if nan exixts in the train and valid set; ', np.isnan(np.min(points_train)), np.isnan(np.min(points_val))) \n\n# import numpy.ma as ma # masked array\n# points_train_imputed = np.where(np.isnan(points_train), ma.array(points_train, mask=np.isnan(points_train)).mean(axis=0), \n#                                 points_train)\n# points_val_imputed = np.where(np.isnan(points_val), ma.array(points_val, mask=np.isnan(points_val)).mean(axis=0), \n#                                 points_val)\n# print ('check if nan exixts in the train and valid set after imputation; ', \n#        np.isnan(np.min(points_train_imputed)), np.isnan(np.min(points_val_imputed)))","b6b25818":"# points_train_standardize = standardize_keypoint(points_train_imputed)\n# points_val_standardize = standardize_keypoint(points_val_imputed)\n\n# print ('check example standardize keypoint: ', points_train_standardize[10])","91d58de1":"# aug_ims_train, aug_points_train = aug_sample(imgs_train, points_train_standardize)","19819c05":"# print ('check shape of an augmented image:  ', aug_ims_train[1].shape)","0a4d4e13":"# fig = plt.figure(figsize=(8, 9))\n# npics= 16\n# count = 1\n# for i in range(npics):\n#   # ipic = i\n#   ipic = np.random.randint(1, len(aug_ims_train), 1)\n#   ax = fig.add_subplot(npics\/4 , 4, count, xticks=[],yticks=[])\n#   vis_im_keypoint(aug_ims_train[ipic[0]], aug_points_train[ipic[0]], ax)\n#   count = count + 1\n\n\n# plt.tight_layout()\n# plt.show()    \n\n# print ('total training images now: ', aug_ims_train.shape, aug_points_train.shape)","11035911":"# fig = plt.figure(figsize=(8, 9))\n# npics= 16\n# count = 1\n# for i in range(npics):\n#   ipic = i \n# #ipic = np.random.randint(1, len(aug_ims_train), 1)\n#   ax = fig.add_subplot(npics\/4 , 4, count, xticks=[],yticks=[])\n#   vis_im_keypoint(aug_ims_train[ipic], aug_points_train[ipic], ax)\n#   count = count + 1\n\n\n# plt.tight_layout()\n# plt.show()","a788f001":"def inception_like(input_layer, filter1, filter2, filter3):\n  # 1x1 conv\n  conv1 = Conv2D(filter1, (1,1), padding='same', activation='relu')(input_layer)\n  bn1 = BatchNormalization()(conv1)\n  # 3x3 conv\n  conv3 = Conv2D(filter2, (3,3), padding='same', activation='relu')(input_layer)\n  bn3 = BatchNormalization()(conv3)\n  # 5x5 conv\n  conv5 = Conv2D(filter3, (5,5), padding='same', activation='relu')(input_layer)\n  bn5 = BatchNormalization()(conv5)\n  # 3x3 max pooling\n#   pool = MaxPooling2D((3,3), strides=(1,1), padding='same')(input_layer)\n  pool = MaxPooling2D((2,2), strides=(1,1), padding='same')(input_layer)\n  # concatenate filters, assumes filters\/channels last\n  layer_out = Concatenate(axis=-1)([bn3, bn5, pool])\n  return layer_out","675b8e19":"input_im = Input(shape=(96, 96, 1))\ndef model2():\n#   x = Conv2D(64, (3, 3), padding='same', strides=(2, 2), activation='relu', )(input_im)\n#   x = MaxPooling2D((3, 3), padding='same', strides=(2, 2), )(x)\n#   x = Conv2D(64, (1, 1), padding='same', strides=(1, 1), activation='relu', )(x)\n# #   x =  Conv2D(64, (3, 3), padding='same', strides=(1, 1), activation='relu', )(x) \n#   x = Conv2D(96, (3, 3), padding='same', strides=(1, 1), activation='relu', )(x)\n#   x = MaxPooling2D((3, 3), padding='same', strides=(2, 2) )(x)\n#   x = Conv2D(16, (3, 3), padding='same', activation='relu', )(input_im)\n#   x = Conv2D(32, (3, 3), padding='same', activation='relu', )(input_im)  \n#   x = Conv2D(64, (3, 3), padding='same', activation='relu', )(x)  \n  x1 = inception_like(input_im, 64, 64, 32)\n  x1 = MaxPooling2D((3, 3), padding='same', strides=(2, 2) )(x1)\n\n  x2 = inception_like(x1, 64, 64, 32)\n  x2 = MaxPooling2D((3, 3), padding='same', strides=(2, 2) )(x2)\n  \n  x2_1 = inception_like(x2, 96, 96, 64)\n  x2_1 = MaxPooling2D((3, 3), padding='same', strides=(2, 2) )(x2_1)  \n\n  x3 = inception_like(x2_1, 96, 128, 64)\n  #x3 = MaxPooling2D((3, 3), padding='same', strides=(2, 2) )(x3)\n  x3 = MaxPooling2D()(x3)\n\n  x3_1 = inception_like(x3, 128, 256, 128)\n  #x3_1 = MaxPooling2D((3, 3), padding='same', strides=(2, 2) )(x3_1) \n  x3_1 = GlobalAveragePooling2D()(x3_1)\n\n  x4 = Flatten()(x3_1)\n  x4 = Dense(1024, kernel_regularizer=l2(l2=0.03))(x4)\n  x4 = Dropout(0.2)(x4)\n\n  #x5 = Dense(128, kernel_regularizer=l2(l2=0.02))(x4)\n  #x5 = Dropout(0.1)(x5)\n\n  pred = Dense(30)(x4)\n  model = Model(inputs=input_im, outputs=pred, name='Inception_Like')\n\n  return model","8e1a2c88":"face_key_model2_aug = model2()\nface_key_model2_aug.summary()","fa5c5b2c":"tf.keras.utils.plot_model(face_key_model2_aug, show_shapes=True)","0104746f":"face_key_model2_aug.compile(loss='mse', \n                       optimizer=Adam(learning_rate=3e-3), \n                       metrics=['mae'])","85a72f31":"# face_key_model2_aug_train = face_key_model2_aug.fit(aug_ims_train, aug_points_train, \n#                                                   validation_data=(imgs_val, points_val_standardize), \n#                                                   batch_size=256, epochs=500, \n#                                                   callbacks=[customCallbacks(), reduce_lr], verbose=0)","a192ac7d":"# face_key_model2_aug_train_clean = face_key_model2_aug.fit(aug_ims_train_final, aug_points_train_final, \n#                                                   validation_data=(imgs_val_clean, points_val_clean), \n#                                                   batch_size=64, epochs=300, \n#                                                   callbacks=[customCallbacks(), reduce_lr], \n#                                           verbose=0)\n\nface_key_model2_aug_train_clean = face_key_model2_aug.fit(aug_ims_train_final, aug_points_train_final, \n                                                  validation_split= 0.05, \n                                                  batch_size=64, epochs=300, \n                                                  callbacks=[customCallbacks(), reduce_lr], \n                                          verbose=0)","2f8a4b80":"mae = face_key_model2_aug_train_clean.history['mae']\n# mae = [i for i in mae if i<60]\nprint (type(mae))\nval_mae = face_key_model2_aug_train_clean.history['val_mae']\n# val_mae = [i for i in val_mae if i<60]\n\nloss = face_key_model2_aug_train_clean.history['loss']\n# loss = [i for i in loss if i<1200]\nval_loss = face_key_model2_aug_train_clean.history['val_loss']\n# val_loss = [i for i in val_loss if i<1200]\n\nfig = plt.figure(figsize=(8, 4))\n\nfig.add_subplot(121)\nplt.plot(range(len(loss)), loss, linestyle='-', color='red', alpha=0.7, label='Train Loss')\nplt.plot(range(len(val_loss)), val_loss, linestyle='-.', color='navy', alpha=0.7, label='Val Loss')\nplt.xlabel('Epochs', fontsize=12)\nplt.ylabel('Loss', fontsize=13)\nplt.yscale('log')\nplt.legend(fontsize=12)\n\nfig.add_subplot(122)\nplt.plot(range(len(mae)), mae, linestyle='-', color='red', alpha=0.7, label='Train MAE')\nplt.plot(range(len(val_mae)), val_mae, linestyle='-.', color='navy', alpha=0.7, label='Val MAE')\nplt.xlabel('Epochs', fontsize=12)\nplt.ylabel('MAE', fontsize=13)\nplt.yscale('log')\nplt.legend(fontsize=12)\n\nplt.tight_layout()\nplt.show()","1665e773":"# predict_points_aug2 = face_key_model2_aug.predict(test_ims)\n\n# print ('check shape of predicted points: ', predict_points_aug2.shape)","e19bd749":"predict_points_aug2_clean = face_key_model2_aug.predict(test_ims)\n\nprint ('check shape of predicted points: ', predict_points_aug2_clean.shape)","84251a24":"fig = plt.figure(figsize=(8, 8))\nnpics= 12\ncount = 1\nfor i in range(npics):\n  # ipic = i\n  ipic = np.random.choice(test_ims.shape[0])\n  ax = fig.add_subplot(npics\/3 , 4, count, xticks=[],yticks=[])\n  vis_im_keypoint_notstandard(test_ims[ipic], predict_points_aug2_clean[ipic], ax)\n  count = count + 1\n\n\nplt.tight_layout()\nplt.show()","810a03db":"# predict_points_aug2_s = revert_standardize(predict_points_aug2_clean)\npredict_points_aug2_s = predict_points_aug2_clean\nprint ('check max and min: ', predict_points_aug2_s.max(), predict_points_aug2_s.min())\n# test_predicts2 = pd.DataFrame(predict_points_aug2_s, columns = list(clean_keypoints_df.columns))\n\n# print ('check the new predict data frame: ', '\\n')\n# test_predicts2.head(3)","54ffb1b6":"print ('check the new predict data frame: ', predict_points_aug2_s.shape)\npredict_points_aug2_s[predict_points_aug2_s > 95.99] = 96.0\npredict_points_aug2_s[predict_points_aug2_s < 0.0] = 0.0\n\nprint ('check max and min now: ', predict_points_aug2_s.max(), predict_points_aug2_s.min())","96dadf5a":"test_predicts3 = pd.DataFrame(predict_points_aug2_s, columns = list(clean_keypoints_df.columns))\n\nprint ('check the new predict data frame: ', '\\n')\ntest_predicts3.head(3)","4f490d9b":"for i in range(lookup_table.shape[0]):\n    lookup_table.Location[i] = test_predicts3.loc[lookup_table.ImageId[i]-1][lookup_table.FeatureName[i]]","6cb20448":"sub_form_table.Location = lookup_table.Location\nnew_submission = sub_form_table","a8c66204":"# CHECK THE FORMAT\nnew_submission.head(3)","ba9866b3":"new_submission.to_csv('Submission11.csv', index=False)","1e310bdc":"print (new_submission['Location'].max(), new_submission['Location'].min())","b56b641d":"#### Data Description \nA nice overview and the data description can be found in the [Kaggle pages](https:\/\/www.kaggle.com\/c\/facial-keypoints-detection\/data). So I skip this part.  Let's get started with the available training data. ","31bc4944":"## Proceed Using the Clean Data. ","e84ff083":"#### Functions to Standardize Keypoints and Revert Back to Original ","1facff81":"## Proceed Using the Complete Data \n\nBefore, I have discussed 2 major points regading using the complete data-set. Just to revise them again here-- \n* Since there are lots of NaN values -- As expected the keypoints do follow a normal distribution, so we can actually _fill the NaN entries with the distribution mean_ as one of the simplest strategies.   \n* **For data imputation, from ML perspective it is necessary to first split the data into train-test and then apply the transformation otherwise we are prone to induce data-leakage.**   \n","5c990652":"### Load Necessary Libraries ","a34a90ad":"#### Necessary Imports for CNN  ","1d8b8a68":"* As expected the keypoints do follow a normal distribution. \n* So we can actually _fill the NaN entries with the distribution mean_ as one of the simplest strategies.   \n\nHere we can test two different workflows. First one is we only use the clean data i.e. drop all the rows with NaN and train a model. \n\nSecond workflow will use data imputation.  \n\n**For data imputation, from ML perspective it is necessary to first split the data into train-test and then apply the transformation otherwise we are prone to induce data-leakage.**   \n\nFirst I will use only Clean data here (submission score with a simple VGG-16 like network is 10.3, including horizontal flip in the augmentation).  ","e59bca80":"#### Train Test Split \n\nWe will use a relatively higher `test_size` because we will perform augmentation on the training data","7ede812f":"### Preparing the Submission File ","3084b6c6":"### Visualizing the Distribution of Keypoints ","168c3584":"#### Load Training Data","c53940ca":"#### Include Augmentation \n\nWe will include horizontal flip to the training data. \n\nThe keypoints are already standardized before. So the range is within -1 to 1. So flipping at this stage means, we keep the y coordinates same, but multiply the x coordinates by -1.   \n\nAlso using [Imgaug](https:\/\/imgaug.readthedocs.io\/en\/latest\/) library we will include gaussian blur, linear contrast, and rotation. ","42c0332d":"----------------------------------------------------------------------------------------------------------------------------"}}