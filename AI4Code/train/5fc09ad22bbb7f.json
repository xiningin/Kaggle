{"cell_type":{"019746aa":"code","f6d9dddf":"code","7d0e0599":"code","994a34ae":"code","203bbbe4":"code","2c76a286":"code","14768de2":"code","a1fed5a4":"code","6f308dfc":"code","31e3db01":"code","e5a38725":"code","30c4ab0f":"code","2362e6f7":"code","a0a95c4e":"code","894177ed":"code","1bc34ef3":"code","86535ec3":"code","e21aaeb5":"code","93efed07":"code","8bd269c8":"code","7b333b94":"code","73859a31":"code","5ce7a689":"code","1dc7ae28":"code","a7ca62e0":"code","4762fa25":"code","559d71a6":"markdown","a8b52320":"markdown","1544bb99":"markdown","4b30b7e5":"markdown","e3faf3ed":"markdown","ecbc3bbc":"markdown"},"source":{"019746aa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport matplotlib.pyplot as plt        \nimport seaborn as sns  \n\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler, LabelEncoder\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\nfrom sklearn.impute import IterativeImputer\n\nfrom sklearn.model_selection import StratifiedKFold, KFold,GroupKFold, RepeatedStratifiedKFold, RepeatedKFold\nfrom sklearn.metrics import f1_score, confusion_matrix, accuracy_score, roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\nfrom imblearn.over_sampling import RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.combine import SMOTEENN \n\nimport xgboost as xgb\n\nimport optuna \nfrom optuna.visualization.matplotlib import plot_optimization_history\nfrom optuna.visualization.matplotlib import plot_param_importances\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfrom pandas_profiling import ProfileReport as profile\n\nimport pkg_resources as pkg\nprint( f\"pandas_profiling version: {pkg.get_distribution('pandas_profiling').version}\")","f6d9dddf":"train = pd.read_csv('\/kaggle\/input\/song-popularity-prediction\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/song-popularity-prediction\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/song-popularity-prediction\/sample_submission.csv')","7d0e0599":"train.head(5)","994a34ae":"total = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(10)","203bbbe4":"f, ax = plt.subplots(figsize=(15, 6))\nplt.xticks(rotation='90')\nsns.barplot(x=missing_data.index, y=missing_data['Percent'])\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent', fontsize=15)\nplt.title('Percent of missing values by feature', fontsize=15)\nplt.show()","2c76a286":"f, ax = plt.subplots(figsize=(15, 6))\nplt.xticks(rotation='90')\nsns.histplot(x=train.song_duration_ms)\nplt.title('song_duration_ms', fontsize=15)\nplt.show()","14768de2":"FEATURES = ['song_duration_ms', 'acousticness', 'danceability', 'energy', 'instrumentalness', 'key', 'liveness', 'loudness', 'audio_mode',\n       'speechiness', 'tempo', 'time_signature', 'audio_valence']\n\nimputer = IterativeImputer()\ntrain[FEATURES] = imputer.fit_transform(train[FEATURES])\ntest[FEATURES] = imputer.transform(test[FEATURES])","a1fed5a4":"total = train.isnull().sum().sort_values(ascending=False)\npercent = (train.isnull().sum()\/train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(10)","6f308dfc":"f, ax = plt.subplots(figsize=(15, 6))\nplt.xticks(rotation='90')\nsns.countplot(x=train.song_popularity)\nplt.title('song_popularity', fontsize=15)\nplt.show()","31e3db01":"FEATURE_COLUMNS = [ 'key', 'audio_mode', 'time_signature']\nCAT_FEATURES = []\n\ndef add_cross_features(df):\n    for feature1 in FEATURE_COLUMNS:    \n        for feature2 in FEATURE_COLUMNS:\n            if feature1 != feature2:\n                x2_feature_name = f'{feature1}-{feature2}'\n                x2_feature_name_2 = f'{feature2}-{feature1}'\n                if x2_feature_name not in df.columns and x2_feature_name_2 not in df.columns:\n                    df[x2_feature_name] = df[feature1].astype(str) + '_' + df[feature2].astype(str)\n                    CAT_FEATURES.append(x2_feature_name)\n    return df\n                \ntrain = add_cross_features(train)\ntest = add_cross_features(test)\n\nALL_FEATURES = FEATURES\nfor f in CAT_FEATURES:\n    if f not in ALL_FEATURES:\n        ALL_FEATURES.append(f)\n    \nprint(len(ALL_FEATURES))","e5a38725":"def encode_cat_feature( df, test_df, feature_name):\n    print(feature_name)\n    all_values = np.concatenate( (df[feature_name].values, test_df[feature_name].values))    \n    encoder = LabelEncoder()\n    encoder.fit(all_values)\n    df[feature_name] = encoder.transform(df[feature_name])\n    test_df[feature_name] = encoder.transform(test_df[feature_name])\n    return df, test_df\n\nCAT_FEATURES.append('audio_mode')\nCAT_FEATURES.append('key')\nCAT_FEATURES.append('time_signature')\n\nfor cat in CAT_FEATURES:\n    train, test = encode_cat_feature(train, test, cat)\n    \n#train.drop(['audio_mode', 'key', 'time_signature'], axis=1, inplace=True) \n#test.drop(['audio_mode', 'key', 'time_signature'], axis=1, inplace=True) \nALL_FEATURES","30c4ab0f":"RANDOM_SEED = 42\n\nsampler = RandomOverSampler()\nX_over, y_over = sampler.fit_resample(train, train.song_popularity)\n\nsampler = RandomUnderSampler()\nX_under, y_under = sampler.fit_resample(train, train.song_popularity)\n\nsampler = SMOTEENN(random_state=RANDOM_SEED)\nX_sme, y_sme = sampler.fit_resample(train, train.song_popularity)","2362e6f7":"train.drop(['id'], axis=1, inplace=True)\ntest.drop(['id'], axis=1, inplace=True)\n\n\nUSE_UNDER_SAMPLER = False\nUSE_OVER_SAMPLER = False\nUSE_SMOTEENN_SAMPLER = False\n\nif USE_UNDER_SAMPLER:\n    target = y_under.astype(int)\n    train = X_under\nelif USE_OVER_SAMPLER:\n    target = y_over.astype(int)\n    train = X_over\nelif USE_SMOTEENN_SAMPLER:\n    target = y_sme.astype(int)\n    train = X_sme\nelse:\n    target = train.song_popularity.astype(int)\n\n\ntrain.drop(['song_popularity'], axis=1, inplace=True)\n\n## HIGH CORR\n#train.drop(['acousticness', 'loudness'], axis=1, inplace=True) \n#test.drop(['acousticness', 'loudness'], axis=1, inplace=True) ","a0a95c4e":"#train_profile = profile(train, title=\"Train Data\", minimal=False)\n#display(train_profile)","894177ed":"#test_profile = profile(test, title=\"Test Data\", minimal=False)\n#display(test_profile)","1bc34ef3":"test","86535ec3":"def scale_feature( df, test_df, feature_name):\n    print(feature_name)\n    all_values = np.concatenate( (df[feature_name].values, test_df[feature_name].values)).reshape(-1, 1)\n    scaler = RobustScaler()\n    scaler.fit(all_values)\n    df[feature_name] = scaler.transform(df[feature_name].values.reshape(-1, 1))\n    test_df[feature_name] = scaler.transform(test_df[feature_name].values.reshape(-1, 1))\n    return df, test_df\n\nSCALE_FEATURES = ['song_duration_ms', 'key-audio_mode', 'key-time_signature', 'audio_mode-time_signature']\n\ntrain_scaled = train\ntest_scaled = test\nfor cat in SCALE_FEATURES:\n    train_scaled, test_scaled = scale_feature(train_scaled, test_scaled, cat)","e21aaeb5":"#scaler = RobustScaler()\n\n#train_scaled = pd.DataFrame(scaler.fit_transform(train), columns=train.columns)\n#test_scaled = pd.DataFrame(scaler.transform(test), columns=test.columns)\n\n#train_scaled = train\n#test_scaled = test","93efed07":"print( train_scaled.shape) \nprint( test_scaled.shape) ","8bd269c8":"NUM_BOOST_ROUND = 600\nEARLY_STOPPING_ROUNDS = 200\nVERBOSE_EVAL = 100\nRANDOM_SEED = 42\nTUNING = False\nN_TRIALS = 25\n\n    \ndef objective(trial, X, y):\n    \n    scale_pos_weight = sum(y==0)\/sum(y==1)\n    \n    param_grid = {\n        'verbosity': 1,\n        'objective': 'binary:logistic', \n        'eval_metric' : 'auc',\n        'n_estimators' : NUM_BOOST_ROUND,\n        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.01),\n        'eta': trial.suggest_float('eta', 0.001, 0.1),\n        'max_depth': trial.suggest_int('max_depth', 50, 500),     \n        'min_child_weight': trial.suggest_float('min_child_weight', 50, 250),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.1, 0.9),\n        'gamma': trial.suggest_float('gamma', 0, 100),\n        'subsample': trial.suggest_float('subsample', 0.1, 0.9),\n        'lambda': trial.suggest_float('lambda', 1, 10),\n        'alpha': trial.suggest_float('alpha', 0, 9),\n        'scale_pos_weight': scale_pos_weight,\n    }    \n        \n    X_train, X_valid, y_train, y_valid = train_test_split( X, y, test_size=0.25, \n                                                          random_state=RANDOM_SEED, shuffle=True)\n    \n    dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n    dvalid = xgb.DMatrix(X_valid, label=y_valid, enable_categorical=True)\n        \n    model = xgb.XGBClassifier( use_label_encoder=True, **param_grid)  \n    model.fit( X_train.values, y_train.values,\n            eval_set = [ (X_train.values, y_train.values), (X_valid.values, y_valid.values) ],\n            callbacks = [xgb.callback.EarlyStopping(rounds=EARLY_STOPPING_ROUNDS, save_best=True)],\n            verbose=False)    \n    \n    oof_pred = model.predict(X_valid)\n    score = f1_score(y_valid, oof_pred)\n    oof_auc_score = roc_auc_score(y_valid, oof_pred)\n    print(f'OOF F1 score: {score}, AUC score: {oof_auc_score}')\n    \n    return oof_auc_score\n\nif TUNING:\n    study = optuna.create_study(direction='maximize')\n    objective_func = lambda trial: objective(trial, train_scaled, target)\n    study.optimize(objective_func, n_trials=N_TRIALS)  \n\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n    print(\"Best trial:\")\n    trial = study.best_trial\n    print(\"  Value: {}\".format(trial.value))\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","7b333b94":"TOTAL_SPLITS = 2\nN_REPEATS = 2\nNUM_BOOST_ROUND = 1000\nEARLY_STOPPING_ROUNDS = 100\nVERBOSE_EVAL = 200\n\ndef run_train(X, y, run_params, splits, num_boost_round, verbose_eval, early_stopping_rounds ):\n    models = []\n    scores = []\n    eval_results = {}  # to record eval results for plotting\n    #folds = RepeatedStratifiedKFold(n_splits=splits, n_repeats=N_REPEATS, random_state=RANDOM_SEED)\n    folds = StratifiedKFold(n_splits=splits, random_state=RANDOM_SEED)\n        \n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        print(f'Fold {fold_n+1} started')\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n        dtrain = xgb.DMatrix(X_train, label=y_train, enable_categorical=True)\n        dvalid = xgb.DMatrix(X_valid, label=y_valid, enable_categorical=True)\n        model = xgb.train( run_params, dtrain,\n            num_boost_round = num_boost_round,\n            evals=[(dvalid, 'evals')], \n            verbose_eval = verbose_eval,\n            early_stopping_rounds=early_stopping_rounds\n        )   \n\n        scores.append(model.best_score)\n        models.append(model)\n    return models, eval_results, scores\n\nscale_pos_weight = sum(target==0)\/sum(target==1)\n\nunder_params = {\n    'verbosity': 1,\n    'objective': 'binary:logistic', \n    'eval_metric': ['logloss','auc'],\n    'learning_rate': 0.176722404455782796,\n    'eta': 0.9764414987043088,\n    'max_depth': 200,\n    'min_child_weight': 80,\n    'colsample_bytree': 0.8,\n    'gamma': 2.2,\n    'subsample': 0.95,\n    'lambda': 2.54,\n    'alpha': 0.8,\n}\n\nrun_params = {\n    'verbosity': 1,\n    'objective': 'binary:logistic', \n    'eval_metric': ['logloss','auc'],\n    'learning_rate': 0.1974105129432222,\n    'eta': 0.29151787034760856,\n    'scale_pos_weight': scale_pos_weight,\n}\n\nmodels, eval_results, scores = run_train(train_scaled, target, under_params, \n                                         TOTAL_SPLITS, NUM_BOOST_ROUND, VERBOSE_EVAL, EARLY_STOPPING_ROUNDS)","73859a31":"from sklearn.metrics import ConfusionMatrixDisplay\n\ny_pred = np.zeros( (len(models), len(train_scaled)) )\nfor i in range(len(models)):\n    y_pred[i] = models[i].predict(xgb.DMatrix(train_scaled, enable_categorical=True))\n\ny_pred = np.mean(y_pred, axis=0)\ny_predicted = np.where(y_pred > 0.5, 1, 0)\n\ncm = confusion_matrix(target, y_predicted)\ncm_display = ConfusionMatrixDisplay(cm).plot()","5ce7a689":"from sklearn.metrics import roc_curve, precision_recall_curve\nfrom sklearn.metrics import RocCurveDisplay, PrecisionRecallDisplay\n\nfig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 6))\n\nfpr, tpr, _ = roc_curve(target, y_pred)\nprec, recall, _ = precision_recall_curve(target, y_pred)\n\nroc_display = RocCurveDisplay(fpr=fpr, tpr=tpr).plot(ax=ax1)\npr_display = PrecisionRecallDisplay(precision=prec, recall=recall).plot(ax=ax2)","1dc7ae28":"y_pred = np.zeros( (len(models), len(test)) )\nfor i in range(len(models)):\n    y_pred[i] = models[i].predict(xgb.DMatrix(test, enable_categorical=True))\n\ny_pred = np.mean(y_pred, axis=0)\ny_predicted = np.where(y_pred > 0.5, 1, 0)","a7ca62e0":"# more like a sanity check\n\nf, ax = plt.subplots(figsize=(15, 6))\nplt.xticks(rotation='90')\nsns.countplot(x=y_predicted)\nplt.show()","4762fa25":"submission['song_popularity'] = y_predicted\nsubmission.to_csv('submission.csv', index=False, float_format='%.6f')\nsubmission.head(20)","559d71a6":"### Submission","a8b52320":"## Tune","1544bb99":"### Scale data","4b30b7e5":"### Balance target","e3faf3ed":"### Impute missing data","ecbc3bbc":"### Validation"}}