{"cell_type":{"f82144f8":"code","f00b833f":"code","b6c3f042":"code","9abca837":"code","8170b2c3":"code","d45c6b6c":"code","353511fc":"code","59c6cf40":"code","55c7a3e5":"code","6cb8d116":"code","cf5fa268":"code","2a3030fa":"code","757a5893":"code","2af2a944":"code","e607061b":"code","53aa8963":"code","e3cd1fe2":"code","629a49dd":"code","cc77c521":"code","df149a88":"code","e96d0150":"code","c029a8b9":"code","edd55f55":"code","2b47871a":"markdown","90979849":"markdown","ce086805":"markdown","7c76eeb9":"markdown","738981fb":"markdown","4d44e012":"markdown","caa34932":"markdown","5159520a":"markdown","463ae8a2":"markdown","ec0917e4":"markdown","a4b1cc3d":"markdown","4d78a474":"markdown","55df389b":"markdown","5f3aa21b":"markdown","1ad03ea1":"markdown","f33d6d63":"markdown","2ea3250f":"markdown","d01d47b9":"markdown","e9dfa06a":"markdown","5505446e":"markdown","d047515c":"markdown","4a9da6e4":"markdown"},"source":{"f82144f8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns \n\n%matplotlib inline\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f00b833f":"#Setting the path\npath =\"..\/input\"\nos.chdir(path)","b6c3f042":"# reading data\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")\nsample = pd.read_csv(\"..\/input\/sample.csv\")","9abca837":"train.shape,  test.shape","8170b2c3":"# Get the Columns Name from train dataset\n\ntrain.columns, test.columns","d45c6b6c":"# Understand the each variable metadata or information\ntrain.info()","353511fc":"numeric_features = train.select_dtypes(include = ['int64', 'float64']).columns\ncategorical_features = train.iloc[:, 0:12].select_dtypes(include = ['object']).columns\n\nprint(\"Numeric features:\", numeric_features)\nprint(\"Categorical features:\", categorical_features)","59c6cf40":"train.isnull().sum()","55c7a3e5":"train.head(5)","6cb8d116":"train.Loan_Status.value_counts(normalize = True)","cf5fa268":"sns.countplot(train['Loan_Status'],label=\"Count\")\nplt.show()","2a3030fa":"train.describe()","757a5893":"ApplicantIncome_cv = train['ApplicantIncome'].std()\/train['ApplicantIncome'].mean()\nCoapplicantIncome_cv = train['CoapplicantIncome'].std()\/train['CoapplicantIncome'].mean()\nLoanAmount_cv = train['LoanAmount'].std()\/train['LoanAmount'].mean()\nLoan_Amount_Term_cv = train['Loan_Amount_Term'].std()\/train['Loan_Amount_Term'].mean()\nCredit_History_cv = train['Credit_History'].std()\/train['Credit_History'].mean()\n\nprint(\"Coefficient of variance for ApplicantIncome is:\", ApplicantIncome_cv)\nprint(\"Coefficient of variance for CoapplicantIncome is:\", CoapplicantIncome_cv)\nprint(\"Coefficient of variance for LoanAmount is:\", LoanAmount_cv)\nprint(\"Coefficient of variance for Loan_Amount_Term is:\", Loan_Amount_Term_cv)\nprint(\"Coefficient of variance for Credit_History is:\", Credit_History_cv)","2af2a944":"#Numerical Variable\nplt.figure(1)\nplt.subplot(121)\nsns.distplot(train['ApplicantIncome']);\n\nplt.subplot(122)\ntrain['ApplicantIncome'].plot.box(figsize=(16,5))\n\nplt.show();","e607061b":"#Numerical Variable\nplt.figure(1)\nplt.subplot(121)\nsns.distplot(train['CoapplicantIncome']);\n\nplt.subplot(122)\ntrain['CoapplicantIncome'].plot.box(figsize=(16,5))\n\nplt.show();","53aa8963":"#Numerical Variable\nplt.figure(1)\nplt.subplot(121)\nsns.distplot(train['LoanAmount'].dropna());\n\nplt.subplot(122)\ntrain['LoanAmount'].dropna().plot.box(figsize=(16,5))\n\nplt.show();","e3cd1fe2":"#Numerical Variable\nprint(train['Loan_Amount_Term'].value_counts())\n\nplt.figure(1)\nplt.subplot(121)\nsns.distplot(train['Loan_Amount_Term'].dropna());\n\nplt.subplot(122)\ntrain['Loan_Amount_Term'].dropna().plot.box(figsize=(16,5))\n\nplt.show();","629a49dd":"#Numerical Variable\nprint(train['Credit_History'].value_counts())\nplt.figure(1)\nplt.subplot(121)\nsns.distplot(train['Credit_History'].dropna());\n\nplt.subplot(122)\ntrain['Credit_History'].dropna().plot.box(figsize=(16,5))\n\nplt.show();","cc77c521":"#Categorical Features\n\nplt.figure(1)\nplt.subplot(221)\ntrain['Gender'].value_counts(normalize=True).plot.bar(figsize=(20,10), title= 'Gender')\n\nplt.subplot(222)\ntrain['Married'].value_counts(normalize=True).plot.bar(title= 'Married')\n\nplt.subplot(223)\ntrain['Self_Employed'].value_counts(normalize=True).plot.bar(title= 'Self_Employed')\n\nplt.subplot(224)\ntrain['Credit_History'].value_counts(normalize=True).plot.bar(title= 'Credit_History')\n\nplt.show()","df149a88":"# Ordinal Variables\nplt.figure(1)\nplt.subplot(131)\ntrain['Dependents'].value_counts(normalize=True).plot.bar(figsize=(22,4),title= 'Dependents')\n\nplt.subplot(132)\ntrain['Education'].value_counts(normalize=True).plot.bar(title= 'Education')\n\nplt.subplot(133)\ntrain['Property_Area'].value_counts(normalize=True).plot.bar(title= 'Property_Area')\n\nplt.show()","e96d0150":"\nsns.countplot(train['Gender'], hue=train['Loan_Status'])\nplt.show()\n\nsns.countplot(train['Dependents'], hue=train['Loan_Status'])\nplt.show()\n\nsns.countplot(train['Education'], hue=train['Loan_Status'])\nplt.show()\n\nsns.countplot(train['Self_Employed'], hue=train['Loan_Status'])\nplt.show()\n\nsns.countplot(train['Property_Area'], hue=train['Loan_Status'])\nplt.show()","c029a8b9":"#Looking at a correlation among all numeric variables\n\ncorr_matrix = train[numeric_features].corr()\nf, ax = plt.subplots(figsize=(9, 6))\nsns.heatmap(corr_matrix, vmax=.8, annot=True, square=True, cmap=\"BuPu\");","edd55f55":"from sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.compose import ColumnTransformer\n\nnumeric_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='median')),\n    ('scaler', StandardScaler())])\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='missing')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numeric_transformer, numeric_features),\n        ('cat', categorical_transformer, categorical_features)])","2b47871a":"From above we can conclude following :\n\n+ Applicantincome, CoapplicantIncome, LoanAmount are left skweed as mean is larger than the median.\n\n+ Loan_Amount_Term is mildly normally distributed.\n\n+ We can't deduce nature of Credit_History as of now.\n","90979849":"## 3.2 Independant Features\n\n### 3.2.1 Univariate analysis\n\nBelow are the types of features available in this datasets:\n\n 1. Categorical features(Nominal): These variables are categorical without order or ranking. Below are the list of such variables in this data: \n      + Gender, \n      + Married, \n      + Self_Employed, \n      + Credit_History\n         \n 2. Categorical features(Ordinal): These variables in categorical in nature having some order involved. \n      + Dependents, \n      + Education, \n      + Property_Area\n     \n 3. Numerical features(Ratio): These features have numerical values. \n      + ApplicantIncome, \n      + CoapplicantIncome, \n      + LoanAmount, \n      + Loan_Amount_Term\n\n#### 3.2.1.1 Descriptive Statistics","ce086805":"> From above table we can see that there are missing values in **Gender**, **Dependants**, **Self_Employed**, **Loan Amount**, **Loan_Amount_term** & **Credit_History**.\n\nLet's look at top 5 rows;","7c76eeb9":"From above we can conclude following :\n \n + Out of 13 variables in train data, ther are 5 numeric varables and 8 are categorical variables.\n + We can see presence of missing values in some variables.\n \n Let's store numeric and categorical variable seperatly and try to examine missing value in details.","738981fb":"> From the above plots, we can conclude that distribution is heavily skewed with most of the value is in the left side indicating some outlier values towards higher income side.","4d44e012":" + There are 614 rows and 13 variables in training dataset. \n + There are 367 rows and 12 variables in training dataset. \n \nLet's look at variable name;","caa34932":"# 2. Problem Statement\n\n**About Company**\nDream Housing Finance company deals in all home loans. They have presence across all urban, semi urban and rural areas. Customer first apply for home loan after that company validates the customer eligibility for loan.\n\n**Problem**\nCompany wants to automate the loan eligibility process (real time) based on customer detail provided while filling online application form. These details are Gender, Marital Status, Education, Number of Dependents, Income, Loan Amount, Credit History and others. To automate this process, they have given a problem to identify the customers segments, those are eligible for loan amount so that they can specifically target these customers. Here they have provided a partial data set.\n\n|Variable|Description|\n|---------|-----------|\n|Loan_ID|Unique Loan ID|\n|Gender|Male\/ Female|\n|Married|Applicant married (Y\/N)|\n|Dependents|Number of dependents|\n|Education|Applicant Education (Graduate\/ Under Graduate)|\n|Self_Employed|Self employed (Y\/N)|\n|ApplicantIncome|Applicant income|\n|CoapplicantIncome|Coapplicant income|\n|LoanAmount|Loan amount in thousands|\n|Loan_Amount_Term|Term of loan in months|\n|Credit_History|credit history meets guidelines|\n|Property_Area|Urban\/ Semi Urban\/ Rural|\n|Loan_Status|Loan approved (Y\/N)|\n<br>\n> It is a classification problem where we have to predict whether a loan would be approved or not. In a classification problem, we have to predict discrete values based on a given set of independent variable(s).\n\n>Evaluation Metric is accuracy i.e. percentage of loan approval you correctly predict.","5159520a":"> From the above plots, we can conclude that distribution is bimodal with values centered around 380 and ~180. This indicates most of the loan term are of higher period.","463ae8a2":"### 3.2.2 Bivariate analysis\n\n#### 3.2.2.1 Distribution plots\n\nLet's plot our independant features with target variables. It will unearth some relationship between them.","ec0917e4":"## 4.2 Model Building","a4b1cc3d":"> From the above plots, we can conclude that distribution is mildly normal one, however presence of some outlier at higher side is affecting the distribution.","4d78a474":"#### 3.2.1.2 Distribution plots","55df389b":"# 3. Exploratory data analysis\n\n## 3.1 Target Variable","5f3aa21b":"#### 3.2.2.2 Correlation Analysis\n\nLet's look at correlation between each numerical features.","1ad03ea1":"Below inferences can be made from above bar plots:\n\n> Most of the applicants don't have any dependents.\n\n> More than 70% of the applicants are Graduate.\n\n> Most of the applicants are from Semiurban.","f33d6d63":"> From above table, we can see that classes are unbiased in nature.","2ea3250f":"> Though this variable is integer while loading, nature of this variable is binary with most of loan applicant having credit history equal to 1.","d01d47b9":"# 1. Introduction\n\nIn this kernel, we are exploring classification problem using loan prediction data from start to finish. After exploring data by Exploratory data analysis, we are going to test various Machine learning algorithm.\n\nLet's begin then!!","e9dfa06a":"# 4. Machine Learning\n\n## 4.1 Data Preprocessing\n\nBased on above EDA, we need to do below preprocessing steps before building our models:\n\n 1. Imputation of missing values\n     + For numerical variables: imputation using mean or median\n     + For categorical variables: imputation using mode\n 2. Label encoding\n     + Target variable\n 3. One hot encoding\n     + Categorical independent features with more than 2 levels\n 4. Standardization","5505446e":"# 2. Glimpse of data\n\nLet's understand our training data in details.","d047515c":"> From the above plots, we can conclude that distribution is heavily skewed with most of the value is in the left side indicating some outlier values towards higher income side.","4a9da6e4":"# 3. Getting started\n\n> At first, we are importing all important libraries and datasets."}}