{"cell_type":{"ab019ee3":"code","93d9e4ee":"code","a1c375c0":"code","a9974fd6":"code","883017ed":"code","067cdd29":"code","059383b9":"code","c21d805b":"code","55ab9ab5":"code","21e12bca":"code","94b67607":"code","e779da20":"code","cd2f8ad6":"code","73a2e58e":"code","3b915d5c":"code","281221cf":"code","662891ea":"code","a6bf6c8c":"markdown","2bf4a8ec":"markdown","5c58653d":"markdown","ea68be5d":"markdown","ed6bd956":"markdown","9ad033b7":"markdown","3cbedccd":"markdown","b0a9c3b4":"markdown","73ca6252":"markdown","4c569cc4":"markdown","c34aead9":"markdown","cae9bff9":"markdown"},"source":{"ab019ee3":"import numpy as np\nimport pandas as pd\nimport string\nimport re\nimport nltk # Natural Language Toolkits\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords  \nfrom sklearn.feature_extraction.text import CountVectorizer \nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix","93d9e4ee":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","a1c375c0":"train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","a9974fd6":"train.head()","883017ed":"test.head()","067cdd29":"train.shape, test.shape","059383b9":"print(\"Total Row = {}\".format(train.shape[0]+test.shape[0]))","c21d805b":"ps = PorterStemmer()\nnltk.download(\"stopwords\")\n","55ab9ab5":"tweets = pd.concat([train,test],axis=0)\ntweets = tweets.fillna(\"\")\ncol = [\"id\",\"keyword\",\"location\",\"text\",\"target\"]\ntweets = np.array(tweets)\ntweets = pd.DataFrame(data = tweets, columns = col)\ntarget = tweets.iloc[:,-1:]\n\ntweets_k = tweets # keyword\ntweets_l = tweets # location\n\ntweets.drop_duplicates(inplace=True)","21e12bca":"collect = []\nfor i in range(10876):\n    text1 = re.sub(\"[^a-zA-z]\", \" \", tweets[\"text\"][i]) \n    text2 = text1.lower() \n    text3 = text2.split()                               \n    text5 = \" \".join(text3)\n    collect.append(text5)\n\ncollect_k = []\nfor i in range(10876):\n    text1 = re.sub(\"[^a-zA-z]\", \" \", tweets_k[\"keyword\"][i]) \n    text2 = text1.lower() \n    text3 = text2.split()                               \n    text5 = \" \".join(text3)\n    collect_k.append(text5)\n\ncollect_l = []\nfor i in range(10876):\n    text1 = re.sub(\"[^a-zA-z]\", \" \", tweets_l[\"location\"][i]) \n    text2 = text1.lower() \n    text3 = text2.split()                               \n    text5 = \" \".join(text3)\n    collect_l.append(text5)\n","94b67607":"cv = CountVectorizer(max_features=2000) \ncollectx = cv.fit_transform(collect).toarray() \ncollectx_k = cv.fit_transform(collect_k).toarray() \ncollectx_l = cv.fit_transform(collect_l).toarray()\n\ncollectx = pd.DataFrame(collectx)\ncollectx_k = pd.DataFrame(collectx_k)\ncollectx_l = pd.DataFrame(collectx_l)\n\ncollectx_concat = pd.concat([collectx_k,collectx_l,collectx], axis = 1)\ncollectx_concat = collectx_concat.values\ncollectx_concat = collectx_concat.tolist()\n\nX = collectx_concat","e779da20":"X = (np.array(X))\nX = X.astype(np.int64)\ny = target.iloc[:7613,:].values\ny = y.astype(np.int64)","cd2f8ad6":"X1 = X[0:7613]\nX2 = X[7613:10876]\n","73a2e58e":"from sklearn.naive_bayes import MultinomialNB\nmodel = MultinomialNB()\nmodel.fit(X1,y)","3b915d5c":"pred_train = model.predict(X1)\n\ncm = confusion_matrix(y,pred_train)\nprint(\"\\nConfussion Matrix\\n\")\nprint(cm,\"\\n\")\naccuracy = (cm[0,0] + cm[1,1])\/(cm[0,0] + cm[0,1]+ cm[1,0] + cm[1,1])\nprint(\"Accuracy = \" + \"%\" + \"%d\" %((accuracy*100)))","281221cf":"X2 = X[7613:10876]\npred_test = model.predict(X2)","662891ea":"submission =pd.DataFrame({\n     'id': test['id'],\n     'target': pred_test\n     })\nsubmission.to_csv(\"submission.csv\",index=False)","a6bf6c8c":"### Create Model by Using Multinomial Naive Bayes from Sci-Kit Learn","2bf4a8ec":"# Natural Language Processing with Disaster Tweets","5c58653d":"### Concatinate Train & Test Data\n\nThere were \"not a number: NaN\" state on both keyword and location columns, thus fillna that is used.","ea68be5d":"### Porter Stemmer & Stopwords\n\nPorterStemmer is a process for removing the commoner morphological and inflexional endings from words in English.","ed6bd956":"### Changing data type into array and int64","9ad033b7":"### Creat Submission File","3cbedccd":"### Prediction on Train Data","b0a9c3b4":"### Split train and test data as X1 & X2","73ca6252":"### Re Library\nRemove everything except the letters (a-zA-z) from keyword, location and text columns","4c569cc4":"### Prediction on Test Data","c34aead9":"### Import Test & Train Data","cae9bff9":"### Counter Vectorizer"}}