{"cell_type":{"e60d8e34":"code","c6adb55a":"code","2d259650":"code","64558ee4":"code","17ebdc93":"code","00f2e4d7":"code","fca1ca53":"code","06ad7814":"code","d2652b92":"code","0c7207dd":"code","60c8a24f":"code","92492ddc":"code","afbfa131":"code","a921a862":"code","c97295c7":"code","d4b0f69d":"code","f6dc73da":"code","35b83842":"code","b82d6f7d":"code","a0380616":"code","2dfecfb1":"code","003fe754":"code","2709d962":"code","e157082d":"code","517a61b9":"code","aca9196f":"code","d3d11e87":"code","5b4aaa60":"code","66cf8e52":"code","011c8297":"code","270cbfb5":"code","09c79218":"code","8cab7cf4":"code","0d253b9c":"code","289742bd":"code","c887c362":"code","d6567899":"code","f1ffe1e4":"code","c1c183fc":"code","e7587031":"code","5b608730":"markdown","f63ac9ec":"markdown","d5243ef2":"markdown","2f14eca4":"markdown","81290e16":"markdown","13805e7d":"markdown","1e97e3d1":"markdown","3873548d":"markdown"},"source":{"e60d8e34":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c6adb55a":"%%time\ndf_train=pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/train.csv\")\ndf_test=pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/test.csv\")\ndf_sybmission=pd.read_csv(\"..\/input\/tabular-playground-series-oct-2021\/sample_submission.csv\")\n","2d259650":"df_train.shape","64558ee4":"df_test.shape","17ebdc93":"df_train.dtypes","00f2e4d7":"df_train.dtypes.value_counts()","fca1ca53":"df_train.isna().sum()","06ad7814":"df_test.isna().sum()","d2652b92":"df_train.columns","0c7207dd":"df_train.describe","60c8a24f":"import numpy as np\nimport pandas as pd\nimport scipy.stats as st\nimport statsmodels as sm\nimport warnings\n# Matplotlib e Seaborn\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style=\"white\", palette=\"muted\", color_codes=True)\n# Plotly\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport plotly.graph_objs as go\ninit_notebook_mode()","92492ddc":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","afbfa131":"\n\n%%time\nmissing_data(df_train)\n\n","a921a862":"%%time\nmissing_data(df_test)","c97295c7":"def plot_feature_scatter(df1, df2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(4,4,figsize=(14,14))\n\n    for feature in features:\n        i += 1\n        plt.subplot(4,4,i)\n        plt.scatter(df1[feature], df2[feature], marker='+')\n        plt.xlabel(feature, fontsize=9)\n    plt.show()","d4b0f69d":"df_train.shape","f6dc73da":"# select the float columns\ndf_float = df_train.select_dtypes(include=[np.float64])\n# select the int columns\ndf_int = df_train.select_dtypes(include=[np.int64])","35b83842":"df_float","b82d6f7d":"df_int[:-1]","a0380616":"df_float.columns","2dfecfb1":"features = ['f0', 'f1','f2','f3', 'f4', 'f5', 'f6', 'f7', \n           'f8', 'f9', 'f10','f11','f12', 'f13', 'f14', 'f15', \n           ]\nplot_feature_scatter(df_float[::20],df_float[::20], features)\n\n","003fe754":"sns.countplot(df_train['target'], palette='Set3')","2709d962":"df_train['target'].value_counts()","e157082d":"print(\"There are {}% target values with 1\".format(100 * df_train[\"target\"].value_counts()[1]\/df_train.shape[0]))\n","517a61b9":"print(\"There are {}% target values with 0\".format(100 * df_train[\"target\"].value_counts()[0]\/df_train.shape[0]))","aca9196f":"features=df_float.columns","d3d11e87":"plt.figure(figsize=(16,6))\nfeatures = df_train.columns.values[2:202]\nplt.title(\"Distribution of mean values per row in the train and test set\")\nsns.distplot(df_train[features].mean(axis=1),color=\"green\", kde=True,bins=120, label='train')\nsns.distplot(df_test[features].mean(axis=1),color=\"blue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","5b4aaa60":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train and test set\")\nsns.distplot(df_train[features].mean(axis=0),color=\"magenta\",kde=True,bins=120, label='train')\nsns.distplot(df_test[features].mean(axis=0),color=\"darkblue\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","66cf8e52":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per row in the train and test set\")\nsns.distplot(df_train[features].std(axis=1),color=\"black\", kde=True,bins=120, label='train')\nsns.distplot(df_test[features].std(axis=1),color=\"red\", kde=True,bins=120, label='test')\nplt.legend();plt.show()","011c8297":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of std values per column in the train and test set\")\nsns.distplot(df_train[features].std(axis=0),color=\"blue\",kde=True,bins=120, label='train')\nsns.distplot(df_test[features].std(axis=0),color=\"green\", kde=True,bins=120, label='test')\nplt.legend(); plt.show()","270cbfb5":"t0 = df_train.loc[df_train['target'] == 0]\nt1 = df_train.loc[df_train['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per row in the train set\")\nsns.distplot(t0[features].mean(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","09c79218":"plt.figure(figsize=(16,6))\nplt.title(\"Distribution of mean values per column in the train set\")\nsns.distplot(t0[features].mean(axis=0),color=\"green\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=0),color=\"darkblue\", kde=True,bins=120, label='target = 1')\nplt.legend(); plt.show()","8cab7cf4":"plt.figure(figsize=(16,6))\n\nplt.title(\"Distribution of min values per row in the train and test set\")\nsns.distplot(df_train[features].min(axis=1),color=\"red\", kde=True,bins=120, label='train')\nsns.distplot(df_test[features].min(axis=1),color=\"orange\", kde=True,bins=120, label='test')\nplt.legend()\nplt.show()","0d253b9c":"%%time\ncorrelations = df_train.corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']]\ncorrelations.head(10)","289742bd":"%%time\ncorrelations = df_train[features].corr().abs().unstack().sort_values(kind=\"quicksort\").reset_index()\ncorrelations = correlations[correlations['level_0'] != correlations['level_1']]\ncorrelations.head(10)\n\n","c887c362":"correlations.tail(10)","d6567899":"correlations.head(10)","f1ffe1e4":"%%time\nunique_max_train = []\nunique_max_test = []\nfor feature in features:\n    values = df_train[feature].value_counts()\n    unique_max_train.append([feature, values.max(), values.idxmax()])\n    values = df_test[feature].value_counts()\n    unique_max_test.append([feature, values.max(), values.idxmax()])\n\n","c1c183fc":"np.transpose((pd.DataFrame(unique_max_train, columns=['Feature', 'Max duplicates', 'Value'])).\\\n            sort_values(by = 'Max duplicates', ascending=False).head(15))","e7587031":"np.transpose((pd.DataFrame(unique_max_test, columns=['Feature', 'Max duplicates', 'Value'])).\\\n            sort_values(by = 'Max duplicates', ascending=False).head(15))","5b608730":"Let's see also the least correlated features.","f63ac9ec":"**Correlations with all the features( Float and Int both)**","d5243ef2":"\n\nLet's look to the top most correlated features, besides the same feature pairs.\n","2f14eca4":"We calculate now the correlations between the features in train set.\nThe following table shows the first 10 the least correlated features.","81290e16":"**Correlations with only the Float features**","13805e7d":"Let's see also the top 15 number of duplicates values per test set.","1e97e3d1":"Duplicate values\u00b6\n\nLet's now check how many duplicate values exists per columns.","3873548d":"\nDistribution of min and max\n\nLet's check the distribution of min per row in the train and test set.\n"}}