{"cell_type":{"a4d92a83":"code","0c7168e5":"code","5d79d36c":"code","8e4f2a55":"code","8438b9fd":"code","ec6ac04f":"code","39b0e089":"code","1da5cd66":"code","36c7d86c":"code","443c361b":"code","b2a7fde4":"code","4d28eaf8":"code","f77b97d7":"code","3371f69c":"code","1198bbd3":"code","c1b273e5":"code","84d6d919":"markdown","50a25541":"markdown","5152de70":"markdown","c6e2409f":"markdown","a8ded62b":"markdown","dc6c4900":"markdown","048bca66":"markdown","2e0f2a34":"markdown","a32d476a":"markdown","28863dcf":"markdown","501f65b2":"markdown","fe6736b7":"markdown","492a4f9e":"markdown","cd5b4f7c":"markdown","f55c0dcc":"markdown"},"source":{"a4d92a83":"import math\nimport numpy as np\nimport pandas as pd\npd.set_option(\"max_rows\", 12)\npd.set_option(\"max_columns\", None)\npd.set_option(\"max_colwidth\", 1024)","0c7168e5":"import plotly.tools as pt\nimport plotly.offline as po\nimport plotly.graph_objs as pg\npo.init_notebook_mode(connected=True)","5d79d36c":"import gensim\nfrom sklearn import (\n    manifold,\n    preprocessing\n)","8e4f2a55":"multiple_choice = (pd.read_csv(\"..\/input\/multipleChoiceResponses.csv\", low_memory=False)\n     .pipe(lambda frame: frame.set_axis(\n         pd.MultiIndex.from_tuples(\n             [tuple(entry.split(\" - \", 1)) for entry in frame.loc[0, :]]),\n         axis=1, inplace=False\n     ))\n     .drop(0, axis=0)\n     .reset_index(drop=True)\n)\nfree_form = (pd.read_csv(\"..\/input\/freeFormResponses.csv\", low_memory=False)\n     .pipe(lambda frame: frame.set_axis(\n         pd.MultiIndex.from_tuples(\n             [tuple(entry.split(\" - \", 1)) for entry in frame.loc[0, :]]),\n         axis=1, inplace=False\n     ))\n     .drop(0, axis=0)\n     .reset_index(drop=True)\n)","8438b9fd":"pd.concat([\n    series\n        .rename(series.name[0])\n        .reset_index(drop=True)\n    for series in [\n        free_form\n            .iloc[:, 0]\n            .loc[[10217, 10, 18255]],\n        free_form\n            .iloc[:, 12]\n            .loc[[4686, 12144, 5970]],\n        free_form\n            .iloc[:, 14]\n            .loc[[20085, 1022, 9231]],\n        free_form\n            .iloc[:, 33]\n            .loc[[6144, 12144, 23038]]\n    ]\n], axis=1)","ec6ac04f":"# Define a toolbox\ndata = (multiple_choice\n     # Drop students, not employed and other, out of scope of the discussion.  Sorry :)\n     .pipe(lambda frame: frame.loc[\n         frame\n             .loc[:, pd.IndexSlice[\n                 frame.columns.get_level_values(0).str.contains(\"title most similar to your current role\"), :\n             ]]\n             .iloc[:, 0]\n             .apply(lambda title: title != \"Student\"\n                                  and title != \"Not employed\"\n                                  and title != \"Other\"),\n         pd.IndexSlice[:, :]\n     ])\n     # Assign a toolbox\n     .assign(\n         toolbox=lambda frame: frame\n             # Focus on tools-related questions\n             .loc[\n                 :,\n                 pd.IndexSlice[\n                     frame.columns.levels[0][\n                         frame.columns.levels[0].str.contains(\"IDE's\")\n                         | frame.columns.levels[0].str.contains(\"hosted notebooks\")\n                         #| frame.columns.levels[0].str.contains(\"cloud computing services\")\n                         | frame.columns.levels[0].str.contains(\"programming languages\")\n                         | frame.columns.levels[0].str.contains(\"machine learning frameworks\")\n                         | frame.columns.levels[0].str.contains(\"data visualization libraries or tools\")\n                         | frame.columns.levels[0].str.contains(\"cloud computing products\")\n                         | frame.columns.levels[0].str.contains(\"machine learning products\")\n                         | frame.columns.levels[0].str.contains(\"relational database products\")\n                         | frame.columns.levels[0].str.contains(\"big data and analytics products\")\n                     ],\n                     frame.columns.levels[1][\n                         ~(frame.columns.levels[1].str.contains(\"Other - Text\")\n                           | frame.columns.levels[1].str.contains(\"Choice - None\")\n                           | frame.columns.levels[1].str.contains(\"Choice - I have not used any cloud providers\"))\n                     ]\n                 ]\n             ]\n             # And produce a list of tools from every row\n             .apply(lambda row: row.dropna().tolist(), axis=1)\n     )\n     # And drop the samples with empty toolbox\n     .pipe(lambda frame: frame.loc[frame.toolbox.apply(len) > 0])\n)","39b0e089":"data.loc[:, [\"toolbox\"]].sample(3, random_state=1)","1da5cd66":"# Define a simple word2vec model (or actually tool2vec in this particular case :D )\nmodel = (data\n     .pipe(lambda frame: gensim.models.Word2Vec(\n         sentences=frame.toolbox.tolist(),\n         size=48, # Pretty arbitrary, but seems to make sense\n         window=frame.toolbox.apply(len).max(), # Why to truncate any context?  Extend boundaries to fit the biggest\n         min_count=1, # Even the rarely used tools should be represented\n         seed=0, # To make the algo deterministic\n         workers=1, # Not that it is a very computationally-heavy task :D\n         sg=1, # Skip-gram makes more sense to me, at least in this particular case\n         iter=10, # Pretty arbitrary too, but again, seems to make sense\n     ))\n)","36c7d86c":"tools = pd.DataFrame.from_dict(\n    {word: model.wv[word] for word in model.wv.vocab},\n    orient=\"index\"\n)","443c361b":"tools.sample(3, random_state=0)","b2a7fde4":"(tools\n     # Reduce to 2D\n     .pipe(lambda frame: pd.DataFrame(manifold.TSNE(\n         n_components=2, perplexity=50.0, random_state=0\n     ).fit_transform(frame.values), index=frame.index))\n     # And plot :)\n     .pipe(lambda frame: po.iplot(pg.Figure(\n         data=[pg.Scatter(\n             x=frame[1], y=frame[0],\n             mode=\"markers+text\",\n             text=frame.index,\n             marker=pg.scatter.Marker(\n                 size=pd.Series(data.toolbox.sum())\n                     .value_counts(normalize=True)\n                     .pow(0.25).mul(100),\n                 color=tools.apply(lambda row: 1 if \"Amazon\" in row.name\n                                                 else 1 if \"AWS\" in row.name\n                                                 else 2 if \"Microsoft\" in row.name\n                                                 else 2 if \"Azure\" in row.name\n                                                 else 3 if \"Google\" in row.name\n                                                 else 4 if \"IBM\" in row.name\n                                                 else 5, axis=1),\n                 colorscale=\"Rainbow\"\n             )\n         )],\n         layout=pg.Layout(title=\"Our tools reduced to 2D\")\n     ), link_text=''))\n)","4d28eaf8":"(tools\n     # Reduce to 3D\n     .pipe(lambda frame: pd.DataFrame(manifold.TSNE(\n         n_components=3, perplexity=50.0, random_state=0\n     ).fit_transform(frame.values), index=frame.index))\n     # And plot this beautiful 3D scatterplot :)\n     .pipe(lambda frame: po.iplot(pg.Figure(\n         data=[pg.Scatter3d(\n             x=frame[0], y=frame[1], z=frame[2],\n             mode=\"markers+text\",\n             text=frame.index,\n             marker=pg.scatter3d.Marker(\n                 size=pd.Series(data.toolbox.sum())\n                     .value_counts(normalize=True)\n                     .pow(0.25).mul(50),\n             )\n         )],\n         layout=pg.Layout(title=\"Our tools reduced to 3D\")\n     ), link_text=''))\n)","f77b97d7":"data = (data\n     # Make a DataFrame of vectors and join to our data.  Just to keep it together\n     .pipe(lambda frame: frame.join(\n         frame.toolbox\n             # Compute a mean of every toolbox -> Series of ND arrays\n             .apply(lambda toolbox: np.mean(\n                 np.vstack([model.wv[tool] for tool in toolbox]),\n                 axis=0\n             ))\n             # Convert Series of ND arrays to the DataFrame of shape (len(Series), ND)\n             .pipe(lambda series: pd.DataFrame(\n                 np.vstack(series.values),\n                 index=frame.index\n             ))\n             # And enhance the second axis (to ease the manipulation)\n             .pipe(lambda frame: frame.set_axis(\n                 pd.MultiIndex.from_product([[\"toolbox_vector\"], frame.columns]), axis=1, inplace=False\n             ))\n     ))\n)","3371f69c":"titles = (data\n     .set_index((\"Select the title most similar to your current role (or most recent title if retired):\", \"Selected Choice\"))\n     .rename_axis(\"title\")\n     .pipe(lambda frame: pd.concat((\n         frame\n             .loc[:, \"toolbox_vector\"]\n         ,\n     ), axis=1))\n)","1198bbd3":"titles.sample(3, random_state=0)","c1b273e5":"(titles\n     .sample(frac=0.1, random_state=0)\n     # Reduce to 2D\n     .pipe(lambda frame: pd.DataFrame(manifold.TSNE(\n         n_components=2, perplexity=50.0, random_state=0\n     ).fit_transform(frame.values), index=frame.index))\n     # And draw a plot once again\n     .pipe(lambda frame: po.iplot(pg.Figure(\n         data=[pg.Scatter(\n             x=frame.loc[[title], 0],\n             y=frame.loc[[title], 1],\n             mode=\"markers+text\",\n             name=title\n         ) for title in frame.index.unique()],\n         layout=pg.Layout(title=\"(Subset of) our community.  You are probably somewhere here too! :)\")\n     ), link_text=''))\n)","84d6d919":"Well, I've been expecting \"a space of overlapping clusters\", but I've been hoping for it to be less overlapping...\n\nBut still:\n* software engineers and statisticians look almost linearly separable\n* data analysts and business analysts look almost the same\n* data scientists cover both software engineers and data analysts\n\nIt does make some sense to me :)","50a25541":"3D scatterplot is here pretty much only because I like 3D scatterplots: it is not very informative, but it seems amusing to play around with :)","5152de70":"As a community, we basically do one simple thing: we process data.  It is only that this \"thing\" has it's \"flavors\" - web scraping, database administration, business analytics, etc. - and, of course, different job titles for all the fine people involved in these worthy activities :)\n\nBut these titles are seen as categories, completely unrelated and sharply separated.  It seems counterintuitive: we all know that data scientist in one company may quickly become data analyst in the other, having changed nothing but Linkedin headline, and data analyst is much less different from business analyst than software engineer from university statistician.\n\nI would like to depict this intuition somehow: **to paint our community** not as a bag of separate groups, but **as a space of overlapping and interrelated clusters**.\n\nHow?  My proposal is by using the simple ideas that:\n* tools exist in a context: shout \"NumPy?\" and hear \"Pandas, Matplolib, Scikit-Learn!\" in responce\n* \"what we do\" is vaguely defined by the tools we use: one assembles the furniture with a screwdriver, not with the jackhammer\n* \"who we are\" is defined by \"what we do\",\n\nand a couple of distributional semantics (**word2vec**) and dimensionality reduction (**t-SNE**) techniques\n\nScroll down to learn what our data thinks about it.  And for a couple of **interactive visualizations** :)","c6e2409f":"Toolboxes -> Word2Vec -> tools","a8ded62b":"# 3. Action","dc6c4900":"# Again, back to job titles.\n\nAaaand here comes the ugly oversimplification: person is defined just as a mean of his\/her tools.\n\nI am really sorry about it, but the thesis is that \"what we do is who we are\", even if it is not the most accurate representation.\n\nAnyway, comments are very much welcome! :)","048bca66":"There are also 12 people who classified GNU Nano as an IDE, at least 5 who stated that their gender is \"Attack helicopter\", and of course some Russian curses...  We are a REALLY diverse community with a great sense of humour :)\n\nAnyway, back to job titles...","2e0f2a34":"Toolboxes as defined by answers:","a32d476a":"# 2. \"Know your data\"","28863dcf":"Of course I could not resist to draw at least this.\n\nAnd the result seems amusing:\n* separate clusters for the software products of tech giants (I've highlighted them by color)\n* and a sign of common sense in a fact that common tools like Scikit-Learn and Plotly are somewhere (kinda) near each other :)\n\nThere are also bad news for me: this thing thinks that Google Cloud Functions, for example, has much more in common with Google BigQuery, than with Azure Functions, and so it is not going to do any better about people using these tools.  It was expected, but still, hope dies last :)","501f65b2":"But before I start, I would like to know our data.  If one asked me what is the one most important step in a process of working with data, my answer would be \"know your data\", really, really well.\n\nBut regarding the dataset in question, it appears to me that it is even more important to understand it, and to remind ourselves that it is not just numbers and categories: it is about real people.  It is about us.\n\nSo I am zooming as close to this dataset as I can: I am going to read some of the free form responces :)","fe6736b7":"# Digression: space of our tools","492a4f9e":"# 1. Thesis","cd5b4f7c":"Vectors of us:","f55c0dcc":"Vectors of tools:"}}