{"cell_type":{"693a50cc":"code","b9cd7a98":"code","15d2134d":"code","d2ec1d2c":"code","3ce8b49e":"code","671bdff5":"code","dbdea9d4":"code","706f529d":"code","8e4b2e23":"code","4ca86f17":"code","c2eeee40":"code","754be65d":"code","6b8d1faa":"code","870f7d94":"code","497f6741":"code","9304f0ca":"code","dd5c02cb":"code","84744bb5":"code","fcaecc1d":"code","33c85ea5":"code","b0ad62f4":"code","fcc9d106":"code","de6e4a6d":"code","9315b7d1":"code","5017e970":"code","b023f87f":"code","1debbeab":"code","8765ab01":"code","61e3e0cc":"code","913a5131":"code","6458d999":"code","0ce445bd":"code","3a0a13e8":"code","755a8078":"code","8d6a12fb":"code","b7c346b5":"code","df5bacac":"code","67e8ef32":"code","5b5ec5e4":"code","b4d0fcdd":"code","293ac4a4":"code","837cfbcd":"code","ef5436ae":"code","62e22997":"code","8dd8e99d":"code","d7d94383":"code","3510c8a9":"code","a25472b1":"code","aece0774":"code","5f7efd47":"code","3906655a":"code","ed305bd2":"code","bd2f000e":"code","bf9fea9a":"code","04c63905":"code","894f498c":"code","054423f7":"code","3ef6f819":"code","59e03ae6":"code","b2aa8e0a":"code","5e6cea8c":"code","a99228fb":"code","aae4aaa8":"code","4f2936a7":"code","23424dd1":"code","55e78613":"code","20e35c47":"code","a5ff6fe8":"code","96b7c551":"code","63b47fcb":"code","5c6542ea":"code","bf1a6236":"code","e5e51aa6":"markdown","dd7cb2f5":"markdown","5f585e10":"markdown","24057632":"markdown","b770e3e9":"markdown","e8533ebd":"markdown","289eb1c0":"markdown","4fa92936":"markdown","8617691c":"markdown","9350aaf8":"markdown","8f936e21":"markdown","29530764":"markdown","640b847f":"markdown","e6c9a873":"markdown","7a20317f":"markdown","d98dc97d":"markdown","2ca6c1e9":"markdown","61febbc6":"markdown","459c516c":"markdown","31d9d161":"markdown","2c84f549":"markdown","facbaa2f":"markdown","8b22e28b":"markdown","f04ec804":"markdown","1f22152f":"markdown","b6af662f":"markdown","6947072a":"markdown","1b4f55f1":"markdown","9d5ab31e":"markdown","5785c521":"markdown","d5480cf7":"markdown","a534bf2a":"markdown","c094dbc7":"markdown","eecba897":"markdown","d61ac0e3":"markdown","99f82a05":"markdown","9fcb357d":"markdown","3484eb61":"markdown","22052ade":"markdown","b72a6433":"markdown","98a184a8":"markdown","805f768f":"markdown","c9471918":"markdown","555f1838":"markdown","c9885fc3":"markdown"},"source":{"693a50cc":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport os\n\nfrom functools import reduce\n\nimport squarify as sq\n\nimport folium\nfrom folium.plugins import HeatMap\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom PIL import Image\n\nfrom sklearn.cluster import KMeans\n\nfrom pmdarima.arima import auto_arima\nfrom pmdarima.arima import ADFTest\n\nimport urllib\nfrom io import BytesIO \nimport requests\n\nimport warnings\nwarnings.filterwarnings('ignore')","b9cd7a98":"filesList = []\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n       \n        file = os.path.join(dirname, filename)\n        fileExt = file[40:-12] \n \n        #read each file in their variable name\n        exec('data_{:s} = pd.read_csv(\"{:s}\")'.format(fileExt, file)) \n        \n        #append list of files\n        filesList.append('data_{:s}'.format(fileExt))\n\n        # -- Display the files that will be loaded and the varible names\n        # print('data_{}'.format(fileExt))\n\nfilesList\n","15d2134d":"#list with datasets with PK and FK 'order_id'\nordersList = [data_orders, data_order_items]\n\n#merge, join multiple datasets with the same FK:\ndata = reduce(lambda data1, data2: pd.merge(data1, data2, on='order_id', how='left'), ordersList)\n\ndata = pd.merge(data, data_products, on='product_id', how='outer')\n\ndata = pd.merge(data, data_sellers, on='seller_id', how='outer')\n\n#merge final data with the same FK (customer_id):\ndata = pd.merge(data, data_customers, on='customer_id', how='outer')","d2ec1d2c":"# De-normalized data - All datasets together, OLAP.\nprint('columns:', data.columns)\nprint('shape:', data.shape)","3ce8b49e":"type_and_missing = pd.concat([data.isna().sum().sort_values(ascending = False), \n                              ((data.isna().sum()\/data.isna().count())*100).sort_values(ascending=False), \n                              data.dtypes], axis=1, keys=['Total', 'Missing_%','Type'])\ntype_and_missing[type_and_missing['Total'] > 0]","671bdff5":"data.loc[data.seller_id.isna()]['order_status'].value_counts()\n# data.loc[(data.seller_id.isna()) & (data.order_status == 'shipped')]","dbdea9d4":"data[data.customer_city.str.contains('sao b')]['customer_city'].value_counts()","706f529d":"#Verify if there is cities with wrong names\ndata[data.customer_city.str.contains('sao p')]['customer_city'].value_counts()","8e4b2e23":"data[data.customer_city.str.contains('hor')]['customer_city'].value_counts()","4ca86f17":"totalOrders = data_orders.order_id.nunique()\n\nprint('Unique customer cities:',data.customer_city.nunique())\nprint('Unique customer states:',data.customer_state.nunique())\nprint('Unique seller states:',data.seller_state.nunique())\nprint('Average price:',data.price.sum() \/ totalOrders)\nprint('Average qnt of products by order:',data.order_item_id.sum() \/ totalOrders)\nprint('Average freight price:',data.freight_value.sum() \/ totalOrders)\nprint('Total revenue for the period was:',data.price.sum())\nprint('Number of unique customers:',data.customer_unique_id.nunique())\nprint('Total order quantity:', totalOrders)\nprint('Average number of product by order:',data.freight_value.sum() \/ totalOrders)\n","c2eeee40":"priceGrouped = data.groupby('order_id')['price'].sum()\npriceGrouped.describe()","754be65d":"#Data Order Prices Analysis\nplt.figure(figsize=(20,5))\nsns.boxplot(x=priceGrouped, color='red')","6b8d1faa":"totalByState = data.groupby('customer_state')['price'].sum().sort_values(ascending=False)\n\nplt.figure(figsize=(15,5))\nplt.title('Total Price (Order Value in R$) by State of the Customer')\nsns.barplot(x=totalByState.index, y=totalByState)","870f7d94":"totalOrdersByState = data.groupby('customer_state')['order_id'].nunique().sort_values(ascending=False)\ntotalOrdersByState\n\nplt.figure(figsize=(15,5))\nplt.title('Total Orders (Count) by State of the Customer')\nsns.barplot(x=totalOrdersByState.index, y=totalOrdersByState)","497f6741":"totalSellerByState = data.groupby('seller_state')['order_id'].nunique().sort_values(ascending=False)\ntotalSellerByState\n\n# Define the plot sizes\nfig, ax = plt.subplots(1, figsize = (15,5))\nplt.title('States with more sells')\n\nsq.plot(sizes= totalSellerByState,\n        color = ['brown','turquoise','lime','royalblue','wheat','gainsboro','aqua','indigo','sandybrown','pink','dodgerblue'],\n        label= totalSellerByState.index,\n        alpha = .8)\n\nplt.show()","9304f0ca":"freightAvgState = (data.groupby('customer_state')['freight_value'].sum() \/data.groupby('customer_state')['order_id'].nunique()).sort_values(ascending=False)\n\nplt.figure(figsize=(15,5))\nplt.title('Average freight value in R$ by State of the Customer')\nsns.barplot(x=freightAvgState.index, y=freightAvgState)","dd5c02cb":"data_geolocation = data_geolocation.drop_duplicates(subset='geolocation_zip_code_prefix', keep='first')\n\ndataNonDuplicates = data.drop_duplicates(subset='customer_unique_id', keep='first')\n\ndataGeo = pd.merge(dataNonDuplicates[['order_status','seller_city','seller_state','customer_city','customer_state', 'customer_zip_code_prefix']], \n                                     data_geolocation, \n                                     left_on='customer_zip_code_prefix', \n                                     right_on='geolocation_zip_code_prefix')","84744bb5":"mapCustomers = folium.Map(location=[-15.7941, -47.8825], \n                          tiles='cartodbpositron', \n#                           width=700, height=400,\n                          zoom_start=5)\n\nHeatMap(data=dataGeo[['geolocation_lat', 'geolocation_lng']], \n                     radius=13).add_to(mapCustomers)\n\nprint('Where are the active clients from: ')\nmapCustomers","fcaecc1d":"productCategoryOrders = data['product_category_name'].value_counts().head(10)\n\nplt.figure(figsize=(15,5))\nplt.title('Top 10 categories of products with more orders')\nsns.barplot(y=productCategoryOrders.index, x=productCategoryOrders, orient='h')","33c85ea5":"dataReviews = pd.merge(dataNonDuplicates[['order_id','seller_city','seller_state','customer_city','customer_state', 'order_status', \n                                          'order_purchase_timestamp', 'order_estimated_delivery_date', 'price', 'freight_value', 'product_photos_qty']], \n                                          data_order_reviews, \n                                          on='order_id', how='left')","b0ad62f4":"reviewsScore = dataReviews.review_score.value_counts()\n\nplt.figure(figsize=(15,5))\nplt.title('Reviews')\nsns.barplot(y=reviewsScore.index, x=reviewsScore, orient='h')","fcc9d106":"sellerStateReviews = dataReviews.groupby('seller_state')['review_score'].mean()\ncustomerStateReviews = dataReviews.groupby('customer_state')['review_score'].mean()\n\nconcatReviews = pd.concat([sellerStateReviews.rename('seller'), customerStateReviews.rename('customer')], axis=1)","de6e4a6d":"fig, axes = plt.subplots(2, 1, figsize=(15,8))\nfig.suptitle('Average reviews by state of seller (1) and customer (2)')\n\nsns.barplot(x=concatReviews.index, y=concatReviews.seller, ax=axes[0])\n\nsns.barplot(x=concatReviews.index, y=concatReviews.customer, ax=axes[1])","9315b7d1":"# Convert objects date columns to dates desconsidering the errors\ndateColumns = ['order_purchase_timestamp', 'order_approved_at', 'order_delivered_carrier_date', 'order_delivered_customer_date', 'order_estimated_delivery_date']\n\nfor column in dateColumns:\n    data[column] = pd.to_datetime(data[column], errors='coerce')\n\n#Calculate the difference between the estimated date and the delivery (negative values means delay)\ndata['DiffDeliveryDays'] = ((data['order_estimated_delivery_date'] - data['order_delivered_customer_date']).dt.days) + 1\n\n#Calculate the time between end of purchase and delivery\ndata['DeliveryCountDays'] = ((data['order_delivered_customer_date'] - data['order_purchase_timestamp']).dt.days)","5017e970":"data[['order_estimated_delivery_date', 'order_delivered_customer_date', 'DiffDeliveryDays', 'order_purchase_timestamp','DeliveryCountDays']]","b023f87f":"reviewsByDates = pd.merge(data[['order_id', 'seller_id','DeliveryCountDays', 'DiffDeliveryDays', 'order_purchase_timestamp']], \n                                 data_order_reviews, \n                                 on='order_id', how='left')\n\nreviewsByDates.corr()['review_score'].drop('review_score')","1debbeab":"fig, axes = plt.subplots(1, 2, figsize=(15,5))\nfig.suptitle('Correlation between days of delivery and reviews')\n\nsns.regplot(x='DeliveryCountDays', y='review_score', data=reviewsByDates, ax=axes[0], line_kws={\"color\": \"red\"})\n\nsns.regplot(x='DiffDeliveryDays', y='review_score', data=reviewsByDates, ax=axes[1], line_kws={\"color\": \"red\"})","8765ab01":"print('Average review score for dalayed items:',(reviewsByDates[reviewsByDates.DiffDeliveryDays<0]['review_score'].mean()))\nprint('Average review score for anticipated items:',(reviewsByDates[reviewsByDates.DiffDeliveryDays>0]['review_score'].mean()))","61e3e0cc":"negReviewsText = reviewsByDates[reviewsByDates.review_score <3]['review_comment_message'].dropna()\n\n# Words to desconsider on the visualization\nstopwords = set(STOPWORDS)\nstopwords.update([\"\/n\" ,\"da\", \"meu\", \"em\", \"voc\u00ea\", \"de\", \"ao\", \"os\", \"e\", 'ou', '\u00e9'])\n \n\ndef open_img(url):\n    response = requests.get(url)\n    img = Image.open(BytesIO(response.content))\n    img_np = np.array(img)\n    return img_np\n\nmask = open_img('https:\/\/cdn-images-1.medium.com\/max\/537\/1*CK7ulJRe6nqeorManOJUzg.png')\n\n\n# WordCLoud generate\nwordcloudInst = WordCloud(stopwords=stopwords,\n                      background_color=\"white\",\n                      random_state=1,\n                    #   colormap='Reds', \n                      mask=mask,\n                      contour_width=2, \n                      contour_color='Red',\n                      collocations=False, \n                      repeat = False,                   \n                      width=700, height=700).generate(negReviewsText.to_string(index=False))\n \n# Configure visualization of the image\nfig, ax = plt.subplots(figsize=(20,10))\nax.imshow(wordcloudInst, interpolation='bilinear')\nax.set_axis_off()\n \n#Show Img \nplt.title('Negative reviews Wordcloud')\nplt.imshow(wordcloudInst)","913a5131":"posReviewsText = reviewsByDates[reviewsByDates.review_score >=3]['review_comment_message'].dropna()\n\nmask = open_img(\"https:\/\/cdn-images-1.medium.com\/max\/537\/1*q4vrNnlsX_d4glglS35AGw.png\")\n\n\n# WordCLoud generate\nwordcloudInst = WordCloud(stopwords=stopwords,\n                      background_color=\"white\",\n                      random_state=1,\n                    #   colormap='Greens', \n                      mask=mask,\n                      contour_width=2, \n                      contour_color='Green',  \n                      collocations=False, \n                      repeat = False,    \n                      width=700, height=700).generate(posReviewsText.to_string(index=False))\n \n# Configure visualization of the image\nfig, ax = plt.subplots(figsize=(20,10))\nax.imshow(wordcloudInst, interpolation='bilinear')\nax.set_axis_off()\n \n#Show Img \nplt.title('Positive reviews Wordcloud')\nplt.imshow(wordcloudInst)","6458d999":"worstSellers = pd.DataFrame(reviewsByDates[reviewsByDates['DiffDeliveryDays'] < 0].groupby('seller_id')['DiffDeliveryDays'].count().sort_values(ascending=False).head(50)).reset_index()\nworstSellers.head(5)\n\n#If we want to see the sellers on details:\n# data_sellers.merge(worstSellers, on='seller_id', how='right')","0ce445bd":"#Create a month-grouped review score\ndateScores = reviewsByDates.set_index('order_purchase_timestamp', inplace=False)\n\n#Calculate the mean review score over time\ndateScoresCount = pd.DataFrame(dateScores.resample('m')['review_score'].count())\ndateScoresMean = pd.DataFrame(dateScores.resample('m')['review_score'].mean())\n\nfig, ax_left = plt.subplots(figsize=(15,5))\nax_right = ax_left.twinx()\n\nax_right.set_ylabel('Count of reviews')\nax_left.set_ylabel('Avg Review Score')\n\n\nplt.title('Review score over time')\nax_right = sns.lineplot(x='order_purchase_timestamp', y='review_score', ax = ax_right, data=dateScoresCount, color='blue', marker='o', label='Number of Reviews')\nax_left = sns.lineplot(x='order_purchase_timestamp', y='review_score', ax = ax_left, data=dateScoresMean, color='red', marker='*', markersize=12, label='Average Score')\n\nax_right.legend(bbox_to_anchor=(.86, 1), loc='upper left', borderaxespad=0.)\nax_left.legend(bbox_to_anchor=(.83, 0.92), loc='upper left', borderaxespad=0.)\n\nplt.show()","3a0a13e8":"#Voucher vs reviews\n\n#join usind order id as key\nreviewsByPayments = pd.merge(reviewsByDates[['order_id', 'review_score']], data_order_payments[['order_id', 'payment_type']], on='order_id', how='left')\nreviewsByPayments = reviewsByPayments.groupby('payment_type')['review_score'].mean()\n\nplt.figure(figsize=(15,5))\nplt.title('Average review score by payment type')\nsns.barplot(x=reviewsByPayments.index, y=reviewsByPayments)","755a8078":"#Find the Order id of orders with good review score and no description of the review.\nidReviewsNonDescriptive = reviewsByDates[(reviewsByDates['review_score']>=5) & (reviewsByDates['review_comment_message'].isna())]\nidReviewsNonDescriptive = idReviewsNonDescriptive.merge(data[['order_id', 'DiffDeliveryDays', 'DeliveryCountDays', 'price', 'product_category_name']])\n\nidReviewsNonMeans = pd.DataFrame({'no Review Avg': idReviewsNonDescriptive.DiffDeliveryDays.mean(), \n                                  'overallAvg': data.DiffDeliveryDays.mean()}, index=[1])\n\nidReviewsNonMeans.plot(kind='bar', figsize=(10,5), title='Average difference days from positive review score orders \\n with and without review comments')","8d6a12fb":"#Find the Order id of orders with good review score and no description of the review.\nidReviewsNonMeans = pd.DataFrame({'no Review Avg': idReviewsNonDescriptive.DeliveryCountDays.mean(), \n                                  'overall Avg': data.DeliveryCountDays.mean()}, index=[1])\n\nidReviewsNonMeans.plot(kind='bar', figsize=(10,5), title='Average DeliveryCountDays days from positive review score orders\\n with and without review comments')","b7c346b5":"prodCatReviews = pd.merge(data[['product_id','order_id','DiffDeliveryDays']], data_products[['product_category_name','product_id']], on='product_id', how='left')\n\nprodCatReviews = prodCatReviews.merge(dataReviews[['order_id', 'price','review_score', 'review_comment_message']], on='order_id', how='left')","df5bacac":"prodCatReviewsByPrice = pd.DataFrame(prodCatReviews.groupby('price')['review_score'].mean()).reset_index()\n\nplt.figure(figsize=(15,5))\nplt.title('Correlation between price and reviews')\nsns.regplot(y='price', x='review_score', data=prodCatReviewsByPrice, line_kws={\"color\": \"red\"})","67e8ef32":"c1 = prodCatReviewsByPrice[prodCatReviewsByPrice['price']<30]['review_score'].mean()\nc2 = prodCatReviewsByPrice[(prodCatReviewsByPrice['price']>=30) & (prodCatReviewsByPrice['price']<100)]['review_score'].mean()\nc3 = prodCatReviewsByPrice[(prodCatReviewsByPrice['price']>=100) & (prodCatReviewsByPrice['price']<1000)]['review_score'].mean()\nc4 = prodCatReviewsByPrice[(prodCatReviewsByPrice['price']>=1000)]['review_score'].mean()\n\nlevels = ['up to R$30', 'R$30 - R$100', 'R$100 - R$1000', 'above R$1000']\n\nfor i in range(len(levels)):\n    value = 'c{}'.format(i+1)\n    print('Avg Review Score for price {}: {}'.format(levels[i], globals()[value]))\n","5b5ec5e4":"plotCats = pd.DataFrame(prodCatReviews.groupby('product_category_name')['review_score'].mean().sort_values(ascending=False)).reset_index()\n\nfig, axes = plt.subplots(2, 1, figsize=(15,8), sharex=True)\nfig.suptitle('Five best and worst reviewed categories')\n\nsns.barplot(y='product_category_name', x='review_score', ax=axes[0], data=plotCats.head(5))\nsns.barplot(y='product_category_name', x='review_score', ax=axes[1], data=plotCats.tail(5))\n","b4d0fcdd":"#Data Order Prices Analysis\nplt.figure(figsize=(20,5))\nplt.title('Outliers from best and worst reviewed categories')\nsns.boxplot(x=plotCats.review_score, color='red')","293ac4a4":"#select the categories with less orders\/reviews\nirrelevant_Cat = prodCatReviews.groupby('product_category_name')['order_id'].count().sort_values().head(5).index\n\n#desconsider this list of categories on our set for the plot\nplotCats = plotCats[~plotCats['product_category_name'].isin(irrelevant_Cat)]\n\n#plot\nfig, axes = plt.subplots(2, 1, figsize=(15,8), sharex=True)\nfig.suptitle('Five best and worst reviewed categories')\n\nsns.barplot(y='product_category_name', x='review_score', ax=axes[0], data=plotCats.head(5))\nsns.barplot(y='product_category_name', x='review_score', ax=axes[1], data=plotCats.tail(5))","837cfbcd":"#select the cities with more delay averages\nmoreDelays = data.groupby('customer_city')['DiffDeliveryDays'].mean().sort_values().dropna().head(100).index\n\n#desconsider this list of categories on our set for the plot\ndelayGeo = data[data['customer_city'].isin(moreDelays)]\n\n\ndelayGeo = pd.merge(delayGeo[['customer_zip_code_prefix']], \n                              data_geolocation, \n                              left_on='customer_zip_code_prefix', \n                              right_on='geolocation_zip_code_prefix')\n\n\nmapDeliveryDelay = folium.Map(location=[-15.7941, -47.8825], \n                              tiles='cartodbpositron', \n                          #   width=700, height=400,\n                              zoom_start=5)\n\nHeatMap(data=delayGeo[['geolocation_lat', 'geolocation_lng']], \n                        radius=20).add_to(mapDeliveryDelay)\n\nprint('Where are the worst delays average: ')\nmapDeliveryDelay","ef5436ae":"#Day of the week feature creation\ndata['weekDay'] = data.order_approved_at.dt.weekday\n\nlastOrder = data.order_purchase_timestamp.max()\n\n#Recency feature creation\nprint('Last order on the dataset is at:', lastOrder)\n\ndata['recency'] = ((lastOrder - data['order_purchase_timestamp']).dt.days)\n\n#Frequency of purchases feature creation - How many unique orders does each customer have\ndata['totalOrders'] = data.groupby('customer_unique_id')['order_id'].transform('nunique')\n\n#Total Spent feature creation\ndata['totalSpent'] = data.groupby('customer_unique_id')['price'].transform('sum')\n","62e22997":"clusteringData = data[['customer_unique_id', 'totalOrders', 'totalSpent', 'recency']]\n\n#group by the id and show the total orders and total spent, using the minimum recency when there are more than 2 orders.\nclusteringData = clusteringData.loc[clusteringData.groupby('customer_unique_id').recency.idxmin()] #.set_index('customer_unique_id')\n\nclusteringData","8dd8e99d":"#Total Spent Outliers\nplt.figure(figsize=(15,3))\nplt.title('Outliers from total Spent feature')\nsns.boxplot(x=clusteringData.totalSpent, color='red')","d7d94383":"#Detect the outliers using IQR technique\n\nq1 = clusteringData['totalSpent'].quantile(0.25)\nq3 = clusteringData['totalSpent'].quantile(0.75)\niqr = q3 - q1\n \nprint(\"Old Shape: \", clusteringData.shape)\n \n#Upper and Lower Limits\nupper = q3 + 1.5 * iqr\nlower = q1 - 1.5 * iqr\n\nprint(\"Lower bound:\", lower)\nprint(\"Upper bound:\", upper)\n \n# new_df = data[data['Income'] > upper]\n# new_df = data[data['Income']  < lower]\n\n# Capping (above or below certain limit, all Incomes will be the same)\n\nclusteringData['totalSpent'] = np.where(clusteringData['totalSpent'] > upper, upper, \n                               np.where(clusteringData['totalSpent'] < lower, lower,\n                               clusteringData['totalSpent']))\n\nprint(\"New Shape: \", clusteringData.shape)\n\n#distribution of Income without the greater outliers\nplt.figure(figsize=(15,3))\nplt.title('Total Spent features without outliers')\nsns.boxplot(x=clusteringData['totalSpent'], color='red')","3510c8a9":"#Function to process individual variables, calculate the elbow and Silhouette Scores and cluster by the feature.\n\ndef cluster_individual(data, column, invert=False):\n    data = data\n    columnName = str(column)\n    className = \"class_\" + columnName\n    X = data[column]\n    X = pd.DataFrame(X)\n    \n\n    print('Results for clustering the feature:', columnName)\n    \n\n    # n_clusters = 3\n\n    #Define the model using Elbow method to show best K value \n    kmeans_kwargs = {\n        \"n_clusters\": 5,\n        \"init\": \"random\",\n        \"n_init\": 10,\n        \"max_iter\": 300,\n        \"random_state\": 1,\n    }\n\n\n#--------> Sort Clusters by yasirroni\n#--------> https:\/\/github.com\/yasirroni\/sorted_cluster\n#--------> Copyright (c) 2020 Muhammad Yasirroni\n\n    def sorted_cluster(x, model=None):\n        if model == None:\n            model = KMeans()\n        model = sorted_cluster_centers_(model, x)\n        model = sorted_labels_(model, x)\n        return model\n\n    def sorted_cluster_centers_(model, x):\n        model.fit(x)\n        new_centroids = []\n        magnitude = []\n        for center in model.cluster_centers_:\n            magnitude.append(np.sqrt(center.dot(center)))\n        idx_argsort = np.argsort(magnitude)\n        model.cluster_centers_ = model.cluster_centers_[idx_argsort]\n        return model\n\n    def sorted_labels_(sorted_model, x):\n        sorted_model.labels_ = sorted_model.predict(x)\n        return sorted_model\n\n#--------> Sort Clusters by yasirroni\n\n    # Final model \n    km = KMeans(**kmeans_kwargs).fit(X)\n\n    # assign a cluster to each\n    # X[className] = km.fit_predict(X)\n    cluster = sorted_cluster(X, km)\n    X[className] = cluster.predict(X)\n\n\n\n    #------Plot results of the clustering using the above K\n    plt.figure(figsize=(15,3))\n    plt.title('Results of clustering')\n\n    sns.scatterplot(x=X[columnName], y=X.index, hue=X[className], s=30)\n    \n    #Append the clustering to the original DataSet\n    data[className] = X[className] + 1\n\n    if invert:\n        data[className] = data[className].map({5:1,\n                                            4:2,\n                                            3:3,\n                                            2:4,\n                                            1:5})\n\n    # return data","a25472b1":"cluster_individual(clusteringData, 'recency', invert=True)","aece0774":"cluster_individual(clusteringData, 'totalSpent')","5f7efd47":"cluster_individual(clusteringData, 'totalOrders')","3906655a":"#RFM table\n\ndef segmentCustomers(data):\n    fmMean = (data['class_totalOrders'] + data['class_totalSpent']) \/ 2\n    \n    if (data['class_recency'] >= 4) and (fmMean >= 4):\n        return 'Champions'\n    if (data['class_recency'] >= 2 and data['class_recency'] <= 5) and (fmMean >= 3 and fmMean <= 5):\n        return 'Loyal Customers'\n    if (data['class_recency'] >= 3 and data['class_recency'] <= 5) and (fmMean >= 1 and fmMean <= 3):\n        return 'Potential Loyslist'\n    if (data['class_recency'] >= 4 and data['class_recency'] <= 5) and (fmMean >= 0 and fmMean <= 1):\n        return 'New Customers'\n    if (data['class_recency'] >= 3 and data['class_recency'] <= 4) and (fmMean >= 0 and fmMean <= 1):\n        return 'Promising'\n    if (data['class_recency'] >= 2 and data['class_recency'] <= 3) and (fmMean >= 2 and fmMean <= 3):\n        return 'Customer Needing Attention'\n    if (data['class_recency'] >= 2 and data['class_recency'] <= 3) and (fmMean >= 0 and fmMean <= 2):\n        return 'About to Sleep'\n    if (data['class_recency'] >= 0 and data['class_recency'] <= 2) and (fmMean >= 2 and fmMean <= 5):\n        return 'At Risk'\n    if (data['class_recency'] >= 0 and data['class_recency'] <= 1) and (fmMean >= 4 and fmMean <= 5):\n        return \"Can't Lose Then\"\n    if (data['class_recency'] >= 1 and data['class_recency'] <= 2) and (fmMean >= 1 and fmMean <= 2):\n        return 'Hibernating'\n    return 'Lost'\n\n\nclusteringData['customerSegment'] = clusteringData.apply(segmentCustomers, axis=1)\n\nclusteringData['customerSegment']","ed305bd2":"plotSeg = clusteringData['customerSegment'].value_counts(normalize=True)*100\nplotSegIndex = plotSeg.reset_index()\n\nlbl = (plotSeg.index[i] + \"\\n\" + str(round(plotSeg[i], 2)) + '%' for i in range(len(plotSeg)))\n\n# Define the plot sizes\nfig, ax = plt.subplots(1, figsize = (15,10))\nplt.title('Segments of customers')\n\nsq.plot(sizes= plotSeg,\n        color = ['brown','turquoise','lime','royalblue','wheat','gainsboro','aqua','indigo','sandybrown','pink','dodgerblue'],\n        pad = False,\n        label= lbl,\n        alpha = .8)\n\nplt.show()","bd2f000e":"from statsmodels.tsa.seasonal import seasonal_decompose\n\ndataTime = data[['order_id', 'order_purchase_timestamp', 'price']].set_index('order_purchase_timestamp', inplace=False)\n\ndataTimeMean = pd.DataFrame(dataTime.resample('W')['price'].sum()) #resample using weeks as parameter\n# dataTimeMean = dataTimeMean.fillna(0)\ndataTimeMean.drop(dataTimeMean.tail(8).index, inplace=True) # Drop the last observations with 0 \n\n#Decompose Time Series\ndecompose = seasonal_decompose(dataTimeMean, extrapolate_trend=12)\n\n#Trend\nobs = decompose.observed\n#Trend\ntrend = decompose.trend\n#Seazonal\nseazon = decompose.seasonal\n#Error\nrandom = decompose.resid","bf9fea9a":"#plot\nfig, axes = plt.subplots(4, 1, figsize=(15,8), sharex=True)\nfig.suptitle('Time Series of Purchase Values')\n\nsns.lineplot(x=obs.index, y=obs, ax=axes[0], data=obs)\nsns.lineplot(x=trend.index, y=trend, ax=axes[1], data=trend)\nsns.lineplot(x=seazon.index, y=seazon, ax=axes[2], data=seazon)\nsns.lineplot(x=random.index, y=random, ax=axes[3], data=random)","04c63905":"#Is the data stationary?\nadfTest = ADFTest(alpha=0.05)\nadfTest.should_diff(dataTimeMean)","894f498c":"dataTimeMean['x1'] = dataTimeMean.price.shift(1)\n\ndataTimeMean.dropna(axis=0, inplace=True)\n\ndataTimeMean.head()","054423f7":"#Making the series stationary\ndataTimeMean.price.diff(1).plot(figsize=(20, 6))","3ef6f819":"dataTimeMean.price.diff(1).groupby(dataTimeMean.index.month).mean().plot(kind='bar', figsize=(10, 6))","59e03ae6":"from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n\nplot_acf(dataTimeMean.price)\nplt.show()","b2aa8e0a":"plot_pacf(dataTimeMean.price, lags=10)\nplt.show()","5e6cea8c":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error\n\nX = dataTimeMean[['x1']]\ny = dataTimeMean.price.values\n\nlinearRegression = LinearRegression().fit(X, y)\npredictionsLinear = linearRegression.predict(X)\n\nprint('RMSE for Linear Regression was: \\n', mean_squared_error(y, predictionsLinear, squared=False))","a99228fb":"from xgboost import XGBRegressor\n\nxgb = XGBRegressor(n_estimators=150, learning_rate=0.05)\nxgb.fit(X, y, \n        early_stopping_rounds=5, \n        eval_set=[(X, y)], \n        verbose=False\n)\n\npredictions_XGBoost = xgb.predict(X)\n\nprint('RMSE for xgb was: \\n', mean_squared_error(y, predictions_XGBoost, squared=False))\n","aae4aaa8":"from sklearn.ensemble import RandomForestRegressor\nrandomF = RandomForestRegressor(n_estimators = 500, random_state=0).fit(X, y)\n\npredictions_RandomForest = randomF.predict(X)\nprint('RMSE for Random Forest was: \\n', mean_squared_error(y, predictions_RandomForest, squared=False))","4f2936a7":"dataTimeMean['pred_1'] = xgb.predict(X)\ndataTimeMean[['price', 'pred_1']].plot(figsize=(15, 6))","23424dd1":"train_size = int(len(dataTimeMean.price) * 2 \/ 3)\n\ntrain = X[:train_size]\ntest = X[train_size-1:]\n\nplt.plot(train)\nplt.plot(test)\nplt.legend(['train', 'test'])","55e78613":"plt.figure(figsize=(20, 6))\nplt.plot(dataTimeMean.price)\nforecast = xgb.predict(test)\nplt.plot(test.index, forecast)\nplt.legend(['Price', 'Predicted'])","20e35c47":"lastPrice = dataTimeMean['price'][-1]\n\ndatesToPredict = pd.Series(dataTimeMean.index[-1] + pd.Timedelta('1 w'))\n\nforecastData =  pd.DataFrame({'x1': str(lastPrice)}, index=datesToPredict)  \nforecastData.x1 = pd.to_numeric(forecastData.x1)\n\n\nprint(\"Next week's predicted price is: R$\", xgb.predict(forecastData))","a5ff6fe8":"plt.figure(figsize=(20, 6))\nplt.plot(dataTimeMean.price)\nforecast = xgb.predict(test)\nplt.scatter(forecastData.index, xgb.predict(forecastData), color='red')\nplt.title(\"Nex week's price forecast\")\nplt.legend(['Actual price', 'Forecast'])","96b7c551":"#Delete data generated from XGB\ndataTimeMean = dataTimeMean.drop(columns=['pred_1', 'x1'], axis=0)\n\ntrain = dataTimeMean[:train_size]\ntest = dataTimeMean[train_size-1:]\n\nplt.plot(train)\nplt.plot(test)","63b47fcb":"autoArimaModel = auto_arima(train, m=12, seazonal=False, \n                            trace=False, random_state=1, n_fits=50)\n\nautoArimaModel.summary()","5c6542ea":"dateRange = pd.date_range(dataTimeMean.index[-1], freq='W', periods=13)[1:]\n\npredict = pd.DataFrame(autoArimaModel.predict(n_periods=len(dateRange)), index=dateRange)\npredict.columns = ['price']\npredict.price = predict.price.round(2)\n\nprint('Next weeks pridiction from auto-ARIMA')\nprint(predict.price)","bf1a6236":"plt.figure(figsize=(15,5))\nplt.ticklabel_format(useOffset=False, style='plain')\nplt.plot(train, label = 'Train')\nplt.plot(test, label = 'Test')\nplt.plot(predict, label = 'Prediction')\nplt.title('Next weeks pridiction from auto-ARIMA')\nplt.legend(loc='best')\nplt.show()","e5e51aa6":"#### Product & Reviews Categories\n\nHere we will check which are the products with the most purchases. ","dd7cb2f5":"#### The price and states features\n\nWith a standard deviation of 210.17 and an average of 136.68, we noticed a large variation in this variable, especially in the difference between 75% - 134.90 and max - 6735.00 ","5f585e10":"Now we can join (or merge) all the data into one. Let's disregard the 'data_order_payments' and 'data_order_reviews' data sets for now. Each 'order_id' can have several types of payments and reviews, which in our merge process will cause duplicates in the data and interfere with continuous measures, such as price. ","24057632":"Decomposing the data, we noticed a strong downward trend in values according to seasonality, at the end of each year, in December, probably after an up caused by the Black Friday. \n\nLet's investigate the autocorrelation factor to understand better what parameters we should use.We then noticed a strong downward trend in values according to seasonality, at the end of each year. ","b770e3e9":"## References\n\nhttps:\/\/www.kaggle.com\/ceruttivini\/rfm-segmentation-and-customer-analysis\n\nhttps:\/\/medium.com\/maxmilhas-tech\/o-que-%C3%A9-rfm-e-como-aplic%C3%A1-lo-ao-seu-time-de-customer-service-b9c35817ed01\n\nhttps:\/\/towardsdatascience.com\/time-series-forecasting-using-auto-arima-in-python-bb83e49210cd","e8533ebd":"Voucher payments could have a higher average of positive reviews, as not much money was spent and the customer would be more willing to use the product. However, the analysis shows that there is no direct relationship between these two variables. Only payments set to 'not_defined' have a significantly lower average, probably due to canceled or uncompleted purchases. ","289eb1c0":"Below, we'll also look at which states get the sellers with the highest number of sales. We are also going to analyze which states have the highest freight values, in view of the country's large territorial extension. ","4fa92936":"#### Find the best regression model","8617691c":"## Part 2: Data Science\n\nWith the insights obtained above, we go to the data science phase, looking for patterns that can favor predictions, groupings, etc. ","9350aaf8":"We have to consider that the data were normalized, with relationships between them. So a single purchase has a unique 'order_id' but may appear multiple times in the dataset as it may have more than one product, more than one payment method, more than one seller, etc. It is not a Primary Key. So we must calculate the averages always considering the value of unique sales, not the frequency. ","8f936e21":"In the positive evaluations, we noticed a tendency to praise delivery before the deadline, product quality and packaging. ","29530764":"## Loading files and denormalize the from OLTP system\n\nDatasets are divided by primary and foreign keys, as in OLTP systems. To analyze with more performance, let's denormalize and create a single dataset for OLAP, containing all the fields.  ","640b847f":"## Part 1: Business Analytics","e6c9a873":"#### Time series forecast (sum of sales figures)\n\nTo predict the sum of the sales value for the coming weeks, we are going to use an ARIMA model ","7a20317f":"The average of 'DiffDeliveryDays' (which deals with delays\/anticipation of delivery) and 'DeliveryCountDays' (which deals with the number of days to delivery) are very similar in the full dataset and in the set that has only positive ratings and no comments in the review. So we cannot consider this the main satisfaction factor for these purchases.\n\n\nSo let's evaluate the product categories","d98dc97d":"#### Region features\n\nLet's create a new set with latitude and longitude data for this analysis. We disregard duplicates keeping only the first occurrence, so we have a unique set of latitude and longitude for each zip code, which is not a problem since the zip code prefix we have usually refers to a neighborhood, not the full address. ","2ca6c1e9":"#### Find null data around dataset","61febbc6":"#### Dates & Delivery Features\n\nHere we analyze the relationships between positive review scores and the characteristics of purchase and delivery dates ","459c516c":"#### Applying auto-ARIMA model as an option for goal defining for next weeks","31d9d161":"## Conclusion\n\nAnalyzing each of the variables we were able to obtain important insights about the Olist website's customers:\n- We separated the worst sellers in terms of delivery, so we can create a strategy to work around the situation.\n- We discovered the places with the greatest delays and thus we can set logistical objectives to solve the problem.\n- We understood which terms are used in positive and negative reviews.\n- We could forecast revenue for the coming weeks, supporting the team that will set the goals.\n- We understood seasonal trends that affect revenue.\n- We segmented customers using RFM techniques, grouping each one into a subgroup that can be used by the marketing team to target specific campaigns.\n- We observed where our active customers are, geographically.\n- We analyzed which product categories obtain the best performances in evaluations and sales quantity.\n- We found out which states have the most expensive freight, which have the most sales, which have the largest purchases and the revenue for each.","2c84f549":"##### Categories with best reviews","facbaa2f":"Many positive reviews do not have a description of the review. Let's analyze the factors that may have influenced the good evaluation, even without having the text written by the consumer.\n\nIs delivery time most responsible for good scores without description? ","8b22e28b":"There doesn't seem to be a direct relationship between price and reviews either. ","f04ec804":"#### Outliers removing","1f22152f":"Para determinar o significado dos nulls, vamos entender o status da ordem para cada coluna que cont\u00e9m nulls. Assim descobrimos, por exemplo, que aquando temos nulls nos campos ligados \u00e0 'seller', \u00e9 porque a compra n\u00e3o foi concretizada, j\u00e1 que os status correspondentes s\u00e3o ['unavailable', 'canceled', 'created', 'shipped', 'invoiced'], sem 'delivered'. \n\nJ\u00e1 nas vari\u00e1veis ligadas \u00e0s caracter\u00edsticas dos produtos que cont\u00e9m valores nulos, provavelmente foi falta de preenchimento. Mesmo caso para reviews.\n\nFuturamente podemos dividir o conjunto de dados em compras que foram conclu\u00eddas e que n\u00e3o foram conclu\u00eddas, de modo a facilitar as an\u00e1lises futuras.","b6af662f":"Let's check if over time, we have an improvement or worsening in the average rating score.\n\nThere is stability in Reviews_Score over time, with a high in January 2017 to be disregarded, as we had few reviews in that period. The stability of Reviews_Score is maintained even with the improvement in the number of reviews until Oct-2018, when there is a significant drop in both indexes, indicating an influencing event. ","6947072a":"We see that in SP, RJ and MG we have the highest total values. This is an expected behavior since in the Southeast region we have a much higher number of purchases. ","1b4f55f1":"First, we will load all csv's on the given folder","9d5ab31e":"## EDA - Exploratory Data Analysis\n\nNow that we have the data all in one set, let's move on to the exploratory analysis step to better understand what's in our hands. \n","5785c521":"As observed on Partial Autocorrelation plot, a shift of 1 will be sufficient for a regression model. So, we will create this new feature to apply a regression model to forecast next week's price.","d5480cf7":"So, we will choose XGB Regressor.","a534bf2a":"#### Summary of valuable features","c094dbc7":"#### Geographic data consistency analysis\nApparently, we do not have inconsistent data, common in Brazil, such as fields indicating 'sao paulo', 'sao paulo' and 's. paulo' indicating the same city. We think it's ok then. ","eecba897":"We can then identify sellers with very late deliveries and define the worst accordingly, in order to improve the service. ","d61ac0e3":"Correlation between days elapsed for delivery and days late\/anticipated and reviews. \n\nClearly, we notice that earlier and faster deliveries tend to receive better reviews and, on the contrary, delayed or late deliveries tend to receive worse ratings. ","99f82a05":"Since we know that delays and antecipations in deliveries play an important role in satisfaction, let's analyze which locations have the worst delivery rates, so that we can define logistics strategies to reverse the situation. ","9fcb357d":"##### Reviews\n\nWe noticed that the vast majority of reviews are positive, with 5 stars. We also found that there is not a very important relationship between the state of the buyer or seller and valuations, which indicates that the distance and cultural differences present in the large country that is Brazil do not have a high influence. ","3484eb61":"Let's see how the model XGB Regressor fits on unseen data (test set)","22052ade":"#### Clustering of customers\n\nWe can now carry out the grouping of customers, assigning a score to each of the features created and applying the RFM technique. ","b72a6433":"We can see that the complaints are mostly related to delivery, non-receipt and the quality of the product itself. ","98a184a8":"#### Recency, Total Spent and Number of Purchases - Feature Creation\n\nFirst, we are going to create 3 new features by calculating how long it has been since the customer made their last purchase, how much the customer spent in total with the site and how many purchases each customer made, all taking into account the last purchase in the dataset as a reference. So we can perform groupings later. ","805f768f":"#### Customer segmentation using RFM Technique\n","c9471918":"We will now forecast the next week's \"price\" feature, using the trainned XGB model","555f1838":"It is important to note, however, that the category with the best ratings obtain very few purchase records, as well as the main ones with the lowest rating, configuring an outliers situation. ","c9885fc3":"So let's redo the chart, ignoring the categories with few purchases\/assessments to make more assertive decisions. "}}