{"cell_type":{"9f460411":"code","27af5639":"code","ef45d978":"code","8c3e08b6":"code","6a190047":"code","92b53e6b":"markdown"},"source":{"9f460411":"import torch\nimport math\n\nclass LegendrePolynomial3(torch.autograd.Function):\n    \"\"\"\n    We can implement our own custom autograd Functions by subclassing torch.autograd.Function \n    and implementing the forward and backward passes which operate on Tensors.\n    \"\"\"\n\n    @staticmethod\n    def forward(ctx, input):\n        \"\"\"\n        In forward pass we receive a Tensor containing input and return a Tensor containing output. \n        ctx is a context object that can be used to stash information for backward computation. \n        You can cache arbitrary objects for use in the backward pass using the \n        ctx.save_for_backward method.\n        \"\"\"\n        ctx.save_for_backward(input)\n        return 0.5 * (5 * input ** 3 - 3 * input)\n\n    @staticmethod\n    def backward(ctx, grad_output):\n        \"\"\"\n        In backward pass we receive a Tensor containing gradient of the loss with respect to the output, \n        and we need to compute the gradient of the loss with respect to the input.\n        \"\"\"\n        input, = ctx.saved_tensors\n        return grad_output * 1.5 * (5 * input ** 2 - 1)","27af5639":"dtype = torch.float\ndevice = torch.device(\"cpu\")\n# device = torch.device(\"cuda:0\")  # Uncomment this to run on GPU","ef45d978":"# Create Tensors to hold input and outputs.\n# By default, 'requires_grad=False', which indicates that we do not need to\n# compute gradients with respect to these Tensors during the backward pass.\n\nx = torch.linspace(-math.pi, math.pi, 2000, device=device, dtype=dtype)\ny = torch.sin(x)","8c3e08b6":"# Create random Tensors for weights. \n# For this example, we need 4 weights: y = a + b * P3(c + d * x), \n# these weights need to be initialized not too far from the correct result to ensure convergence.\n# Setting 'requires_grad=True' indicates that we want to compute gradients with\n# respect to these Tensors during the backward pass.\n\na = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\nb = torch.full((), -1.0, device=device, dtype=dtype, requires_grad=True)\nc = torch.full((), 0.0, device=device, dtype=dtype, requires_grad=True)\nd = torch.full((), 0.3, device=device, dtype=dtype, requires_grad=True)","6a190047":"learning_rate = 5e-6\n\nfor t in range(2000):\n    # To apply our Function, we use Function.apply method. We alias this as 'P3'.\n    P3 = LegendrePolynomial3.apply\n\n    # Forward pass: compute predicted y using operations; \n    # we compute P3 using our custom autograd operation.\n    y_pred = a + b * P3(c + d * x)\n\n    # Compute and print loss\n    loss = (y_pred - y).pow(2).sum()\n    if t % 100 == 99:\n        print(t, loss.item()) # loss.item() gets the scalar value held in the loss.\n\n    # Use autograd to compute the backward pass.\n    loss.backward()\n\n    # Update weights using gradient descent\n    with torch.no_grad():\n        a -= learning_rate * a.grad\n        b -= learning_rate * b.grad\n        c -= learning_rate * c.grad\n        d -= learning_rate * d.grad\n\n        # Manually zero the gradients after updating weights\n        a.grad = None\n        b.grad = None\n        c.grad = None\n        d.grad = None\n\nprint(f'Result: y = {a.item()} + {b.item()} * P3({c.item()} + {d.item()} x)')","92b53e6b":"# Learning PyTorch: Defining new autograd functions\n\nSource: https:\/\/pytorch.org\/tutorials\/beginner\/pytorch_with_examples.html#pytorch-defining-new-autograd-functions\n\n\nUnder the hood, each primitive autograd operator is really two functions that operate on Tensors. The `forward` function computes output Tensors from input Tensors. The `backward` function receives the gradient of the output Tensors with respect to some scalar value, and computes the gradient of the input Tensors with respect to that same scalar value.\n\nIf `x` is a Tensor that has `x.requires_grad=True` then `x.grad` is another Tensor holding the gradient of `x` with respect to some scalar value.\n\nIn PyTorch we can easily define our own autograd operator by defining a subclass of `torch.autograd.Function` and implementing the `forward` and `backward` functions. We can then use our new autograd operator by constructing an instance and calling it like a function, passing Tensors containing input data.\n\nIn this example we define our model as ![image.png](attachment:87454617-306b-4d9c-a424-0f4d992c70a1.png) instead of ![image.png](attachment:9fa1ae53-0199-4166-805d-64a7a92b5917.png), where ![image.png](attachment:dd0cfd1e-cceb-41d4-b4f8-ffcbe7d850b9.png) is the Legendre polynomial of degree three.\n\nWe write our own custom autograd function for computing forward and backward of ![image.png](attachment:ab9b1699-7b46-4cc5-997f-f1940c308dbd.png), and use it to implement our model."}}