{"cell_type":{"5977b0c4":"code","141f80e0":"code","52233982":"code","203c85ca":"code","2707003b":"code","a1e980a2":"code","ef3a598d":"code","8163fd1f":"code","c3ca54fb":"code","2d6d4d9a":"code","504f046d":"code","73592bb4":"code","fb528a62":"code","d17ec75d":"code","6ad70e28":"code","eb0d9281":"code","a721f307":"code","d8dae9d6":"code","fd78ba14":"code","854e4798":"code","a4aa6357":"code","64a6440b":"code","f4dbd5a8":"code","617a5814":"code","e60ebeec":"code","b488e9f5":"code","d85662f9":"code","eafdbc44":"code","ae1ae803":"code","5506da7a":"code","9824dd30":"code","4aa42614":"code","8615b193":"code","3e072611":"code","64a33a16":"code","e99d13bf":"code","e3a5b136":"code","614f1925":"code","f051e3f7":"code","753ee219":"code","9ae44d90":"code","52ba5bfa":"code","1e23aa2d":"code","9d36e566":"code","0dc78ad4":"code","7f35494c":"code","137c082a":"code","47cb9474":"code","eb60b210":"code","bdfe6a54":"code","35587d7c":"code","14811e0e":"code","86214013":"code","657d460c":"code","db0fb8fc":"code","ce0fc2d9":"code","64494648":"code","214a87e5":"code","3c59e127":"code","2d149c38":"code","add90889":"code","8da53585":"code","8c6e421d":"code","b280299a":"code","31227a3f":"code","e83cd166":"code","e3ec638e":"markdown","a59c520c":"markdown","e6b93e7b":"markdown","1628c5b2":"markdown","8fac6fd9":"markdown","494ac44b":"markdown","d7ce169a":"markdown","05036845":"markdown","4688102b":"markdown","2a870887":"markdown","01a285f3":"markdown","15d4bb00":"markdown","c799cdac":"markdown","ef8a5a3a":"markdown","c197ff87":"markdown","85b64c9d":"markdown","6ca8a391":"markdown","b318ee07":"markdown","af426c18":"markdown","74fc6743":"markdown","a94df8ef":"markdown","e469de94":"markdown","1f020c88":"markdown"},"source":{"5977b0c4":"import os\nimport gc\nimport random\nimport json\nimport glob\n\nimport numpy as np\nimport pandas as pd\n\nfrom xgboost import XGBRegressor\nimport tensorflow as tf\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","141f80e0":"DIRECTORY = '..\/input\/cyclistic-bike-share'","52233982":"def set_display():\n    \"\"\"Function sets display options for charts and pd.DataFrames.\n    \"\"\"\n    # Plots display settings\n    plt.style.use('fivethirtyeight')\n    plt.rcParams['figure.figsize'] = 12, 8\n    plt.rcParams.update({'font.size': 14})\n    # DataFrame display settings\n    pd.set_option('display.max_columns', None)\n    pd.set_option('display.max_rows', None)\n    pd.options.display.float_format = '{:.4f}'.format\n    \n    \nset_display()","203c85ca":"# Read the data from all csv files.\npaths = glob.glob(f'{DIRECTORY}\/*.csv')\ndata = pd.concat((pd.read_csv(path) for path in paths), ignore_index=True)\n\nprint(f'Data shape: {data.shape}')\ndata.head()","2707003b":"# ID and date columns have object data type.\ndata.dtypes","a1e980a2":"# There are missing values in station name and ID columns\n# and some latitude and longitude columns.\ndata.isna().sum()","ef3a598d":"# Cleaning data types for date columns.\ndata['started_at'] = pd.to_datetime(data['started_at'])\ndata['ended_at'] = pd.to_datetime(data['ended_at'])\ndata.head()","8163fd1f":"# All the bike stations are located in the city of Chicago\n# with coordinates varying in a narrow range.\nstats = data.describe()\nstats","c3ca54fb":"# Data covers 13 months starting from April 2021 till April 2021.\nprint(f'Time span: {data[\"started_at\"].min()} - {data[\"started_at\"].max()}')","2d6d4d9a":"# Feature analysis\nfor feature in ('start_station_name', 'end_station_name'):\n    print(f'Feature \"{feature}\": {data[feature].nunique()} unique values')","504f046d":"# Categorical features\nfig, ax = plt.subplots(1, 2, figsize=(16, 8))\ndata['rideable_type'].value_counts().plot.pie(\n    autopct='%1.2f%%', ax=ax[0], fontsize=12, startangle=135)\ndata['member_casual'].value_counts().plot.pie(\n    autopct='%1.2f%%', ax=ax[1], fontsize=12, startangle=135)\nplt.suptitle('Categorical Features', fontsize=20)\nplt.show()","73592bb4":"# Frequency of start and end points in the data set.\nfig, ax = plt.subplots(1, 2, figsize=(16, 8))\nax[0].hist(data['start_station_name'].value_counts(), bins=20)\nax[1].hist(data['end_station_name'].value_counts(), bins=20)\nax[0].set(xlabel='Start station frequency')\nax[1].set(xlabel='End station frequency')\nplt.suptitle('Bike Station Popularity', fontsize=20)\nplt.show()","fb528a62":"# Range of coordinates and seasonality\ndata.hist(bins=20, figsize=(16, 16))\nplt.suptitle('Seasonality and Station Coordinates')\nplt.show()","d17ec75d":"# Rent duration\ndata['duration'] = (data['ended_at'] - data['started_at']) \/ np.timedelta64(60, 's')  # In minutes\n\n# Location features\ndata['same_station'] = (data['start_station_name'] == data['end_station_name']).astype(int)\ndata['park'] = data['start_station_name'].str.lower().str.contains('park')\ndata['park'] = data['park'].fillna(0).astype(int)\n\n# Temporal features\ndata['year'] = data['started_at'].dt.year\ndata['month'] = data['started_at'].dt.month\ndata['weekofyear'] = data['started_at'].dt.isocalendar().week\ndata['dayofyear'] = data['started_at'].dt.dayofyear\ndata['day'] = data['started_at'].dt.day\ndata['dayofweek'] = data['started_at'].dt.dayofweek\ndata['hour'] = data['started_at'].dt.hour","6ad70e28":"# Demand analysis with respect to rent duration and start \/ end points.\nfig, ax = plt.subplots(1, 2, figsize=(16, 8))\ndata['same_station'].value_counts().plot.pie(\n    explode=[0, 0.25], autopct='%1.2f%%', ax=ax[0], fontsize=12, startangle=135)\ndata['duration'].hist(bins=20, ax=ax[1], log=True)\nplt.suptitle('Bike Demand', fontsize=20)\nax[1].set(ylabel='Frequency')\nax[1].set(xlabel='Duration, minutes')\nax[1].legend()\nplt.show()","eb0d9281":"data['park'].value_counts().plot.pie(\n    autopct='%1.2f%%', fontsize=12, startangle=135)\nplt.suptitle('Proximity to Park', fontsize=20)\nplt.show()","a721f307":"# Since the data covers 13 months, we drop the latast month\n# to be able to show how demand is distributed throughout the year.\nfor feature in ('month', 'weekofyear', 'dayofyear', 'day',\n                'dayofweek', 'hour'):\n    grouped_data = data[data['started_at'] < '2021-04'].groupby(by=feature)['ride_id'].count()\n    grouped_data = grouped_data \/ grouped_data.sum()\n    plt.bar(grouped_data.index, grouped_data.values)\n    plt.title(f'Bike Demand by {feature}')\n    plt.ylabel('Percentage of total demand')\n    plt.show()","d8dae9d6":"# Sum up the number of rents per day for each bike station.\ndaily_data = data.groupby(by=['start_station_name', 'dayofyear']).agg(\n    {'ride_id': 'count', 'start_lat': 'min', 'start_lng': 'min',\n     'park': 'min', 'year': 'min', 'month': 'min',\n     'weekofyear': 'min', 'day': 'min', 'dayofweek': 'min'}).reset_index()\n\ndaily_data = daily_data.rename(columns={'ride_id': 'n_bikes'})\n\nprint('Data shape:', daily_data.shape)\ndaily_data.head()","fd78ba14":"daily_data.describe()","854e4798":"daily_data['n_bikes'].hist(bins=100)\nplt.ylabel('Frequency')\nplt.xlabel('N bikes rented')\nplt.title('Daily Bike Rents')\nplt.show()","a4aa6357":"# Days when no bikes were rented at the station are missing in the data set.\n# Create a new DataFrame for all possible combinations of station name\n# and days of the period from April 2020 till April 2021.\nstations = daily_data['start_station_name'].unique().tolist()\nprint('Unique stations:', len(stations))\nall_days_stations = pd.DataFrame(columns=stations)\nall_days_stations['date'] = pd.date_range(start='2020-04-01', end='2021-04-30', freq='D')\nall_days_stations = all_days_stations.fillna(0)\nprint('Data shape before melt:', all_days_stations.shape)\nprint('Dates range:', all_days_stations['date'].min(), all_days_stations['date'].max())\n\n# We need station name, year and day of year columns\n# to join this data with the original DataFrame.\nall_days_stations['year'] = all_days_stations['date'].dt.year\nall_days_stations['dayofyear'] = all_days_stations['date'].dt.dayofyear\n\nall_days_stations = pd.melt(\n    all_days_stations, id_vars=['year', 'dayofyear'],\n    value_vars=stations, var_name='start_station_name', value_name='n_bikes')\n\nprint('Data shape after melt:', all_days_stations.shape)\nall_days_stations.head()","64a6440b":"# Combine with the original data.\ndaily_data = pd.merge(all_days_stations.drop('n_bikes', axis=1),\n                      daily_data, on=['start_station_name', 'year', 'dayofyear'],\n                      how='left')\nprint('Data shape:', daily_data.shape)\ndaily_data.head()","f4dbd5a8":"# Fill in missing values.\ndaily_data['n_bikes'] = daily_data['n_bikes'].fillna(0)\n\n# Fill in missing values for location and time features\n# by copying previous values in a sorted DataFrame.\ndaily_data.sort_values(\n    by=['start_station_name', 'n_bikes'],\n    ascending=False, inplace=True)\ndaily_data[['start_lat', 'start_lng', 'park']] = daily_data[\n    ['start_lat', 'start_lng', 'park']].fillna(method='ffill')\n\ndaily_data.sort_values(\n    by=['year', 'dayofyear', 'n_bikes'],\n    ascending=False, inplace=True)\ndaily_data[['year', 'month', 'weekofyear', 'day', 'dayofweek']] = daily_data[\n    ['year', 'month', 'weekofyear', 'day', 'dayofweek']].fillna(method='ffill')","617a5814":"# Fix data types\nfor feature in ('weekofyear', 'month', 'year'):\n    daily_data[feature] = daily_data[feature].astype(int)","e60ebeec":"# Demand Heat-Map\ndemand_by_period = daily_data.pivot_table(\n    index='year', columns='month', values='n_bikes', aggfunc='sum')\nax = sns.heatmap(demand_by_period, center=0, annot=False, cmap='RdBu_r')\nl, r = ax.get_ylim()\nax.set_ylim(l + 0.5, r - 0.5)\nplt.yticks(rotation=0)\nplt.title('Demand by Period')\nplt.show()","b488e9f5":"demand_by_station = daily_data.pivot_table(\n    index='start_station_name', columns='month', values='n_bikes', aggfunc='sum')\nax = sns.heatmap(demand_by_station, center=0, annot=False, cmap='RdBu_r')\nl, r = ax.get_ylim()\nax.set_ylim(l + 0.5, r - 0.5)\nplt.yticks(rotation=0)\nplt.title('Demand by Station')\nplt.show()","d85662f9":"# Group the bike stations into clusters according to their popularity.\n# To avoid data leakage we do not use the entire data set to group the stations.\n# We take the data up till March 2021 and bin total number of rent events\n# for each station during that period.\nrents_by_station = daily_data[\n    ~((daily_data['year'] == 2021) & (daily_data['month'] == 4))\n].groupby(by='start_station_name')['n_bikes'].sum()\n\nbins = pd.cut(rents_by_station, bins=10, labels=[i for i in range(10)])\ncluster_dict = dict(zip(rents_by_station.index, bins))\n\nwith open('station_clusters.json', 'w') as f:\n    json.dump(cluster_dict, f, indent=2)\n\ndaily_data['cluster'] = daily_data['start_station_name'].apply(lambda x: cluster_dict[x])\ndaily_data.head()","eafdbc44":"# Distribution of stations between clusters.\nclusters = daily_data['cluster'].value_counts() \/ 395  # 395 days for each bike station.\nplt.bar(clusters.index, clusters.values)\nplt.xlabel('Cluster IDs')\nplt.ylabel('Number of stations')\nplt.title('Station Clusters')\nplt.show()","ae1ae803":"# Lagged target values: the number of bikes rented at the same station\n# the previous day and 7 days before the predicted day.\n# To get the lagged target values for each station separately\n# we sort the data by station and year and day of year\n# and shift target values in a grouped DataFrame.\ndaily_data.sort_values(\n    by=['start_station_name', 'year', 'dayofyear'],\n    inplace=True)\n\nfor num in (1, 7):\n    daily_data[f'lag_{num}'] = daily_data.groupby(\n        by='start_station_name')['n_bikes'].shift(num)\n    \n# Rolling mean of the target for the last 7 days before the predicted day.\ndaily_data['rol_week'] = daily_data.groupby(\n        by='start_station_name')['lag_1'].transform(lambda x: x.rolling(7).mean())","5506da7a":"# Save the data for future use.\ndaily_data.to_csv('daily_data.csv', index=False)","9824dd30":"# Correlation matrix\ncorrelation = daily_data.corr()\nax = sns.heatmap(correlation, center=0, annot=True, cmap='RdBu_r', fmt='0.2f')\nl, r = ax.get_ylim()\nax.set_ylim(l + 0.5, r - 0.5)\nplt.yticks(rotation=0)\nplt.title('Correlation Matrix')\nplt.show()","4aa42614":"# Select row indexes for every fourth week of the data set for validation.\nvalid_idx = daily_data[daily_data['weekofyear'] % 4 == 0].index\ndata_valid = daily_data.loc[valid_idx, :]\ndata_train = daily_data.drop(data_valid.index)\nprint(f'Train data shape: {data_train.shape}\\n'\n      f'Validation data shape: {data_valid.shape}')","8615b193":"y_train = data_train.pop('n_bikes')\ny_valid = data_valid.pop('n_bikes')","3e072611":"data_train.drop('start_station_name', axis=1, inplace=True)\ndata_valid.drop('start_station_name', axis=1, inplace=True)\ndata_train.head()","64a33a16":"EARLY_ROUNDS = 50\n\nmodel = XGBRegressor(\n    n_estimators=500,\n    objective='reg:squarederror', \n    booster='gbtree'\n)\n\nmodel.fit(data_train, y_train, eval_set=[(data_valid, y_valid)],\n          eval_metric='rmse', early_stopping_rounds=EARLY_ROUNDS)","e99d13bf":"# Save the regressor.\nmodel.save_model('xgb_demand_model.bin')","e3a5b136":"print(f'Validation RMSE = {model.best_score}')","614f1925":"# Check the feature importance.\nimportance = pd.DataFrame({\n    'features': data_train.columns,\n    'importance': model.feature_importances_\n})\nimportance.sort_values(by='importance', inplace=True)\n\nplt.barh(importance['features'], importance['importance'])\nplt.title('XGBoost Feature Importance')\nplt.show()","f051e3f7":"# Check the actual magnitude of errors and their distribution.\ndata_valid['prediction'] = model.predict(data_valid)\ndata_valid['n_bikes'] = y_valid\ndata_valid['mae'] = (data_valid['prediction'] - data_valid['n_bikes']).abs()\n\nprint('XGBoost Validation MAE:', data_valid['mae'].mean())\n\nplt.hist(data_valid['mae'], bins=100)\nplt.xlabel('Error, bikes a day')\nplt.ylabel('Frequency')\nplt.title('XGBoost Mean Absolute Error')\nplt.show()","753ee219":"# The largest errors.\ndata_valid[data_valid['mae'] > 100]","9ae44d90":"def set_seed(seed=42):\n    \"\"\"Utility function to use for reproducibility.\n    :param seed: Random seed\n    :return: None\n    \"\"\"\n    np.random.seed(seed)\n    random.seed(seed)\n    tf.random.set_seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    os.environ['TF_DETERMINISTIC_OPS'] = '1'\n    \n    \nset_seed()","52ba5bfa":"# Drop samples with NaN values from the daily data.\ndaily_data = daily_data.dropna()\n\n# Update training and validation subsets.\nvalid_idx = daily_data[daily_data['weekofyear'] % 4 == 0].index\ndata_valid = daily_data.loc[valid_idx, :]\ndata_train = daily_data.drop(data_valid.index)\n\nprint(f'Train data shape: {data_train.shape}\\n'\n      f'Validation data shape: {data_valid.shape}')\n\ndata_train.head()","1e23aa2d":"# Move categorical columns into a separate input array.\ndata_train_cat = data_train.pop('start_station_name')\ndata_valid_cat = data_valid.pop('start_station_name')\n\n# Move target values into separate variables.\ny_train = data_train.pop('n_bikes')\ny_valid = data_valid.pop('n_bikes')","9d36e566":"def get_category_encoding_layer(cat_feature: np.array, dtype: str,\n                                max_tokens=None):\n    \"\"\"Function creates category encoding layer\n    with string or integer lookup index.\n    :param cat_feature: Array containing categorical input data for one feature\n    :param dtype: String describing data type of the categorical feature (one of 'string' or 'int64')\n    :param max_tokens: Maximum number of tokens in the lookup index\n    :return: Lambda function with categorical encoding layers and lookup index\n    \"\"\"\n    # Lookup layer which turns strings or integers into indices\n    if dtype == 'string':\n        index = tf.keras.layers.experimental.preprocessing.StringLookup(\n            max_tokens=max_tokens)\n    else:  # 'int64'\n        index = tf.keras.layers.experimental.preprocessing.IntegerLookup(\n            max_tokens=max_tokens)\n    # Learn the data scale\n    index.adapt(cat_feature)\n    # Category encoding layer\n    encoder = tf.keras.layers.experimental.preprocessing.CategoryEncoding(\n        max_tokens=index.vocab_size())\n    return lambda feature: encoder(index(feature))","0dc78ad4":"# Create normalization layer and adapt it to the numerical data.\nnormalizer = tf.keras.layers.experimental.preprocessing.Normalization(axis=None)\nnormalizer.adapt(data_train.sample(n=30_000))\n\n# Create  encoding layer for categorical feature.\nencoding_layer = get_category_encoding_layer(\n    data_train_cat.drop_duplicates().values, dtype='string')","7f35494c":"# Model input layers\nnum_input = tf.keras.Input(shape=(data_train.shape[1],), dtype=tf.float32)\ncat_input = tf.keras.Input(shape=(1,), dtype='string')\n\n# Process inputs separately and combine.\nx_1 = normalizer(num_input)\nx_2 = encoding_layer(cat_input)\nx = tf.keras.layers.concatenate([x_1, x_2])\n\nx = tf.keras.layers.Dense(\n    128, activation='relu',\n    kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\nx = tf.keras.layers.Dense(\n    64, activation='relu',\n    kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\nx = tf.keras.layers.Dense(\n    32, activation='relu',\n    kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n\noutput = tf.keras.layers.Dense(1)(x)\n\nmodel = tf.keras.Model([num_input, cat_input], output)\n\nmodel.compile(optimizer='adam',\n              loss='mse',\n              metrics=[tf.keras.metrics.MeanAbsoluteError(),\n                       tf.keras.metrics.RootMeanSquaredError()]\n              )","137c082a":"# Visualize the model graph\ntf.keras.utils.plot_model(model, show_shapes=True, show_dtype=True)","47cb9474":"# Train the model\nhistory = model.fit(x=[data_train, data_train_cat], y=y_train,\n                    epochs=20, batch_size=64)","eb60b210":"# Save the model\nmodel.save('tf_demand_model')","bdfe6a54":"# Check the magnitude of validation errors and their distribution.\ndata_valid['prediction'] = model.predict(x=[data_valid, data_valid_cat], batch_size=512)\ndata_valid['n_bikes'] = y_valid\n\ndata_valid['mae'] = (data_valid['prediction'] - data_valid['n_bikes']).abs()\nprint('Validation MAE =', data_valid['mae'].mean())","35587d7c":"plt.hist(data_valid['mae'], bins=100)\nplt.xlabel('Error, bikes a day')\nplt.ylabel('Frequency')\nplt.title('Neural Net Mean Absolute Error')\nplt.show()","14811e0e":"# Drop samples with missing start station name or end coordinates\n# from the original data for individual rides.\ndata = data.dropna(subset=['start_station_name', 'end_lat', 'end_lng'])\n\n# Array of all station names.\nunique_stations = data['start_station_name'].drop_duplicates().values\n\n# Remove samples with negative ride duration.\ndata = data[data['duration'] > 0]\n\n# Drop samples with too large ride duration (outliers).\nthreshold = round(data['duration'].quantile(0.97), 2)\nprint(f'97% quantile for ride duration: {threshold} minutes')\ndata = data[data['duration'] <= threshold]\n\n# Drop unnecessary columns\ndata.drop(labels=['same_station', 'ride_id', 'started_at', 'ended_at',\n                  'start_station_id', 'end_station_id', 'end_station_name'],\n          axis=1, inplace=True)\n\n# Add categorical column with station clusters for start points.\ndata['cluster'] = data['start_station_name'].apply(lambda x: cluster_dict[x])","86214013":"# Split the data into train and validation sets.\ndata_valid = data.sample(frac=0.15, random_state=0)\ndata_train = data.drop(data_valid.index)\n\nprint(f'Train data shape: {data_train.shape}\\n'\n      f'Validation data shape: {data_valid.shape}')","657d460c":"# Define the target values: one for ride duration\n# and two for end point coordinates.\ntarget_columns = ['duration', 'end_lat', 'end_lng']\n\ny_train = data_train[target_columns]\ny_valid = data_valid[target_columns]\n\ndata_train.drop(labels=target_columns, axis=1, inplace=True)\ndata_valid.drop(labels=target_columns, axis=1, inplace=True)","db0fb8fc":"tf.keras.backend.clear_session()\ndel daily_data, data_train_cat, data_valid_cat\ngc.collect()","ce0fc2d9":"# Process numerical and categorical features separately.\ncat_features = ['rideable_type', 'start_station_name', 'member_casual']\nnum_features = [col for col in data_train.columns if col not in cat_features]\n\nprint('Categorical features:', cat_features)\nprint('Numerical features:', num_features)","64494648":"data_train_bike = data_train.pop('rideable_type')\ndata_train_station = data_train.pop('start_station_name')\ndata_train_user = data_train.pop('member_casual')\n\ndata_valid_bike = data_valid.pop('rideable_type')\ndata_valid_station = data_valid.pop('start_station_name')\ndata_valid_user = data_valid.pop('member_casual')","214a87e5":"data_train = data_train.astype(np.float32)\ndata_valid = data_valid.astype(np.float32)","3c59e127":"# Normalization layer for numerical features.\nnormalizer = tf.keras.layers.experimental.preprocessing.Normalization(axis=None)\nnormalizer.adapt(data_train.sample(n=30_000))  # Adapt to a fraction of train data.\n\n# Three encoding layers for categorical features.\nencoding_layer_bike = get_category_encoding_layer(\n    data_train_bike.drop_duplicates().values, dtype='string')  # Learn all unique values.\nencoding_layer_station = get_category_encoding_layer(unique_stations, dtype='string')\nencoding_layer_user = get_category_encoding_layer(\n    data_train_user.drop_duplicates().values, dtype='string')","2d149c38":"# Model input layers\nnum_input = tf.keras.Input(shape=(data_train.shape[1],),\n                           dtype=tf.float32, name='numeric')\n\ncat_input_bike = tf.keras.Input(shape=(1,), dtype='string', name='bike')\ncat_input_station = tf.keras.Input(shape=(1,), dtype='string', name='station')\ncat_input_user = tf.keras.Input(shape=(1,), dtype='string', name='user')","add90889":"# Process inputs separately and combine.\nx_1 = normalizer(num_input)\nx_2 = encoding_layer_bike(cat_input_bike)\nx_3 = encoding_layer_station(cat_input_station)\nx_4 = encoding_layer_user(cat_input_user)\n\nx = tf.keras.layers.concatenate([x_1, x_2, x_3, x_4])\n\nx = tf.keras.layers.Dense(\n    128, activation='relu',\n    kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\nx = tf.keras.layers.Dense(\n    64, activation='relu',\n    kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\nx = tf.keras.layers.Dense(\n    32, activation='relu',\n    kernel_regularizer=tf.keras.regularizers.l2(0.001))(x)\n\noutput = tf.keras.layers.Dense(3)(x)\n\nmodel = tf.keras.Model([num_input, cat_input_bike, cat_input_station, cat_input_user], output)\n\nmodel.compile(optimizer='adam',\n              loss='mse',\n              metrics=[tf.keras.metrics.MeanAbsoluteError(),\n                       tf.keras.metrics.RootMeanSquaredError()]\n              )","8da53585":"# Visualize the model graph\ntf.keras.utils.plot_model(model, show_shapes=True, show_dtype=True)","8c6e421d":"del data\ngc.collect()","b280299a":"history = model.fit(\n    x=[data_train, data_train_bike, data_train_station, data_train_user],\n    y=y_train, epochs=5, batch_size=1024, shuffle=False)","31227a3f":"# Save the model\nmodel.save('tf_user_model')","e83cd166":"# Check the magnitude of validation errors and their distribution.\nprediction = model.predict(\n    x=[data_valid, data_valid_bike, data_valid_station, data_valid_user],\n    batch_size=1024)\n\nerrors = np.abs(prediction - y_valid.values)\n\nfor i, target in enumerate(target_columns):\n    print(f'{target} MAE = {np.mean(errors[:, i])}')\n\n    plt.hist(errors[:, i], bins=100)\n    plt.xlabel('Error')\n    plt.ylabel('Frequency')\n    plt.title(f'Mean Absolute Error: {target}')\n    plt.show()","e3ec638e":"Bike rent stations differ in popularity: while most stations attract relatively small number of users, some locations see tens of thousands of users a year.","a59c520c":"The two upper charts show overall number of bikes rented across all stations. There is a strong seasonality with demand rising in the summer and falling in the winter months.\n\nBike stations are located in a city of Chicago. Longitude and latitude coordinates are relatively close to each other with most frequent values being in the middle (city center).","e6b93e7b":"#### Feature engineering","1628c5b2":"### Neural network with TensorFlow\nFor this model we can use stations names as a categorical feature without any concern about the dimensionality of the input data. Other original and engineered features also can be used. The model takes 2 inputs: an array of numerical features and an array with categorical values for station name and outputs a single value - expected number of bikes rented at this station on a given day.","8fac6fd9":"## Predict rent duration and end coordinates for individual rides\nThe model takes **4 inputs**: array of numarical features and 3 arrays of categorical values (for bike type, station name and user category) and outputs an array that contains **3 values**: expected **ride duration** in minutes, predicted **end point latitude and longitude**.","494ac44b":"## Convert Data into Daily Format","d7ce169a":"#### Treating missing values\n\nData set contains missing values in the station name and ID columns, while all start coordinates and most of the end coordinates are present. Attempt to fill in missing values based on the coordinates fails. For any station both latitude and longutude vary slightly, and coordinates in samples with missing station name do not match with any known locations, even when coordinates are rounded to 3 decimal points.\n\nSeveral approaches could be applied to this problem:\n- Drop and ignore the samples with missing station names.\n- Introduce a new category \"station_unknown\" and use it to replace all missing values. Testing showed that it made the model accuracy worse compared to dropping the samples with incomplete information.\n- Apply nearest neighbours algorithm to assign station names based on the most similar coordinates. This approach is better than introducing \"station_unknown\" category but still makes the model worse compared to dropping the samples with NaNs.\n\nWe cannot be sure if missing values in station name columns result from data incompleteness or if the bikes actually could be taken from some locations outside the bike rent stations and left outside the stations when the ride is over. In this version of the notebook we drop the incomplete samples, because this approach leads to a better accuracy.","05036845":"## Predict Daily Demand for Bikes per Station","4688102b":"## Load the Data","2a870887":"# How Many Bikes City Needs?\n### Predict Demand and User Behaviour at a Bike Sharing Service\n\nData set contains description of **bike rides in Chicago** for a period from April 2020 till April 2021 - a total of **3,826,978 samples** and **713 bike stations**. Each sample contains start and end station name, ID, latitude, longitude, time, bike type and user category. User ID is not available.\n\nWith the available data we can solve several tasks:\n- **Predict total number of bikes rented at each bike station daily.** Input features include location, temporal features, categorical features defining station type and demand in previous periods.\n- **Predict ride duration and end point coordinates for each individual ride.** Input features include location, temporal features, categorical features defining bike type, user type and station type.\n\n**Models:**\n- XGBoost regression model\n- TensorFlow neural networks","01a285f3":"The charts above demonstrate several strong patterns in the data:\n- Seasonality: demand is the highest in the summer and lowest in winter months. Visuble growth in demand starts in June, and decline starts in September. Peak demand is seen in August. The lowest demand is seen in February. Relative spike in demand for bikes is seen in March. The same general outlook is seen on the charts with monthly, weekly and dayly rent events.\n- During the month the demand shifts in a narrow range. It's not obvious from the chart, what factors drive these changes. First day of each month usually demonstrates lower demand compared to average. Low demand in the last day obviously can be explained by the fact that not every month has 31 days.\n- During the week the highest demand is seen on Saturday and the lowest - at the beginning of the week. The number of users grows gradually from Monday till Friday with a sharp spike on Saturday. On Sunday demand is still higher than on weekdays but much lower compared to Saturday.\n- During the day the demand is steadily growing reaching the maximum at about 5 p.m. and then decreases till the night and early morning hours.","15d4bb00":"Demand Heat-Map shows that 2021 differs from the previous year by significantly lower demand for bikes in April. Later data is not available. However, it's likely that because of the pandemic demand patterns for the entire year 2021 are not exactly the same as in 2020. In this situation we cannot simply use the data from a comparable period of the previous year to predict bike rents for the latest month. We need to train a model that takes into account multiple factors and constantly retrain it adding most recent data to catch any sudden changes in the users' behaviour.","c799cdac":"Based on the analysis on station names only about 4% of bike stations are located near park areas.","ef8a5a3a":"## Data Processing and Analysis","c197ff87":"#### Feature engineering for daily data","85b64c9d":"XGBoost model RMSE is about 9.5 bikes a day while the target values vary between 1 and 758 with a standard deviation of 28.\n\nAll the features that are used in the input data are meaningful to the model with seasonal and temporal features being the most important (week of the year, month and year). Location features including proximity to parks are in the middle of the importance ranking. Day features are the less important.","6ca8a391":"There are three types of bikes. The most popular category is docked bike (67.5% of data samples). Electric bikes and classic bikes are less popular. Bike users come from two major categories. Most of the users are \"members\" (59%). Casual users account for about 41% of data samples.","b318ee07":"We can conclude that basic XGBoost model is relatively good at predicting daily demand for bikes at various rent stations. Occasional large errors, most likely, come from rare special events at unique locations where a large number of bikes was rented at once and overestimation of demand for stations lokated in the city center.","af426c18":"### XGBoost model\n\nFor this model we drop station names assuming that geographical coordinates, station clusters, temporal features and lagging target values will be enough to predict daily bike rents.\n\nTesting showed that encoding 713 bike station names and using this as an input feature does not improve XGBoost model accuracy but decreases the impact of other features.","74fc6743":"For most bike stations daily demand level does not exceed 10-20 bikes a day. However, at some locations the demand is much higher and increases significantly in high summer season.\n\nTransformed daily data set has one problem we have to deal with: it does not contain samples for stations where no bikes were rented during the day. Total number of stations is 713. Period from April 2020 till April 2021 covers 395 days. The data set contains 185,982 rows. Dividing 185,982 samples by 713 unique stations we get 260.8, which is less than the expected number of days. We will fill in missing rows with 0 bikes rented before creating and training the model.","a94df8ef":"Available data covers 13 months. Two approaches for training and validation are possible:\n- Use the data from April 2020 till March 2021 for training and the data for April 2021 for validation.\n- Use some portions of the data for each month (for example the last week of each month for validation).","e469de94":"Most users take the bike in one location and return it to another station. Only 10% of users return the bike to the same station where they rented it.\n\nRent duration is exponentially distributed: most users rent bikes for short rides, but there are users who rent bikes for longer periods of time (up to 1-1.5 months). Also, the chart shows errors in the data set, where rent duration is negative, which means that either the start or the end time is incorrect.","1f020c88":"Number of bikes rented at every individual bike station varies in a wide range depending on the season and location. This difference would be reflected in a \"cluster\" feature, which we will introduce groupping the stations into 10 clusters depending on their popularity."}}