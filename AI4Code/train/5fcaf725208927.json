{"cell_type":{"8c13fe7e":"code","47bc2865":"code","40158c5e":"code","3b38c920":"code","08532d24":"code","7ee5fb6e":"code","63f5e50d":"code","5fea81ee":"code","1fd36de2":"code","721e9bbb":"code","4b6914c5":"code","f1d9f9e5":"code","8dbeac54":"code","9232b07b":"code","45f987db":"code","0ba00e17":"code","a439e2f1":"code","3be7f438":"code","64fbee79":"code","cce8bfa0":"code","d9a20447":"code","45272776":"code","f3cc6a49":"code","06651666":"code","f733757f":"code","bfd235d2":"code","e60213dd":"code","aca862ed":"code","a7590f31":"code","00fb364a":"code","8db3b81f":"code","34e1a759":"code","50f2c2cc":"code","c7146bf9":"code","fdb32b10":"code","8bf7d750":"code","43114bd0":"code","5a3aa790":"code","3532325a":"code","24ffa348":"code","efd8c1cc":"code","fc4481ca":"code","0bce52db":"code","981a25b1":"markdown","a7494356":"markdown","9df29064":"markdown","25526f49":"markdown","cea16c09":"markdown","0ca9fd00":"markdown","679b21e8":"markdown","83faead5":"markdown","8f8328b2":"markdown","17e5f92c":"markdown","1b7116dd":"markdown","7a09979e":"markdown","419958e1":"markdown","2b68947a":"markdown","6533c486":"markdown","e436aa3f":"markdown","449f11c4":"markdown","efcb5ad8":"markdown","659e4db7":"markdown","09e1f871":"markdown","addcf12e":"markdown","5f7e58de":"markdown","4a2a73cc":"markdown","96f034f3":"markdown","f5e8452c":"markdown","0375b80e":"markdown","2fd99a50":"markdown","fb12b2ed":"markdown","17ec6228":"markdown","39a03908":"markdown","3d69d718":"markdown","629000e3":"markdown","814b3af3":"markdown","8e03b6b6":"markdown","5a07e43e":"markdown","ce4d7161":"markdown","bc97390b":"markdown","bfd40575":"markdown","aa19753b":"markdown","f5d7b946":"markdown","d23ae710":"markdown","0d9e7035":"markdown","0c2924cb":"markdown"},"source":{"8c13fe7e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport warnings\nimport itertools\nimport sklearn\nimport scipy\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport squarify\nimport matplotlib.ticker as ticker\nimport statsmodels.api as sm\nfrom statsmodels.graphics.tsaplots import plot_acf, plot_pacf\nfrom statsmodels.tsa.arima_model import ARMA\nfrom statsmodels.tsa.statespace.sarimax import SARIMAX\nfrom statsmodels.tsa.stattools import adfuller\nimport statsmodels.api as sm\nfrom scipy.spatial.distance import euclidean\nimport sys\nfrom sklearn.preprocessing import MinMaxScaler\nimport math\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n","47bc2865":"Rawdata = pd.read_csv('..\/input\/crimes-in-boston\/crime.csv',encoding='latin-1')\n# Drop \"INCIDENT_NUMBER\" colume, we are not going to use it in our analysis.\nRawdata.drop(\"INCIDENT_NUMBER\",axis=1, inplace=True) \n# Split 'OCCURRED_ON_DATE' colume into 'DATE' and 'Time'. 'Date' will give us the exact date of the crime\nRawdata[[\"DATE\",\"TIME\"]]=Rawdata['OCCURRED_ON_DATE'].str.split(\" \",expand=True) ","40158c5e":"Rawdata.info()","3b38c920":"# plot line chart\ndef lineplt(x,y,xlabel,ylabel,title,size,tick_spacing):\n    fig,ax=plt.subplots(figsize = size)\n    plt.plot(x,y)\n    ax.xaxis.set_major_locator(ticker.MultipleLocator(tick_spacing))\n    plt.xlabel(xlabel,fontsize = 15)\n    plt.ylabel(ylabel,fontsize = 15)\n    plt.title(title,fontsize = 20)\n    plt.show()\n\n# Create 2 columes DateFrame\ndef createdf(c1,d1,c2,d2):\n    dic = {c1:d1,c2:d2}\n    df = pd.DataFrame(dic)\n    return df\n\n# Plot histogram\ndef plthis(d,bin, title):\n    plt.figure(figsize=(10,8))\n    plt.hist(d, bins=bin)\n    plt.title(title, fontsize = 20)\n    plt.show()","08532d24":"# Put Date and Count into a new Dataframe\nc = createdf(\"Date\",Rawdata[\"DATE\"].value_counts().index,\"Count\",Rawdata[\"DATE\"].value_counts())\n\n# c is the total number of crimes per day\nc.head(5)","7ee5fb6e":"plthis(c[\"Count\"],50, \"Crimes Count Distribution\")","63f5e50d":"print('skewness is ' + str(c['Count'].skew()))\nprint('kurtosis is ' + str(c['Count'].kurt()))","5fea81ee":"bin=pd.cut(c[\"Count\"],50)\nfre= createdf(\"Bin\",bin.value_counts().index,\"Count\",bin.value_counts())\nfre_sort = fre.sort_values(by = \"Bin\", ascending = True)","1fd36de2":"(_,p) = scipy.stats.shapiro(fre_sort[\"Count\"])\nprint('p-value is ' + str(p))","721e9bbb":"(_,p) = scipy.stats.kstest(fre_sort[\"Count\"],'norm')\nprint('p-value is ' + str(p))","4b6914c5":"c=c.sort_values(by=\"Date\",ascending = True)\nlineplt(c[\"Date\"],c[\"Count\"],\"Date\",\"Count\",\"Crimes by Time\",(20,15),80)","f1d9f9e5":"fig = plt.figure(figsize=(16,16))\nax1 = fig.add_subplot(411)\nfig = plot_acf(c[\"Count\"],lags=200,ax=ax1)\nplt.title('Autocorrelation Lag=200')\nax2 = fig.add_subplot(412)\nfig = plot_pacf(c[\"Count\"],lags=200,ax=ax2)\nplt.title('Partial Autocorrelation Lag=200')\nax3 = fig.add_subplot(413)\nfig = plot_acf(c[\"Count\"],lags=15,ax=ax3)\nplt.title('Autocorrelation Lag=15')\nax4 = fig.add_subplot(414)\nfig = plot_pacf(c[\"Count\"],lags=15,ax=ax4)\nplt.title('Partial Autocorrelation Lag=15')\nplt.subplots_adjust(left=None, bottom=None, right=None, top=None,\n                wspace=None, hspace=0.5)\nplt.show()\n","8dbeac54":"res = sm.tsa.seasonal_decompose(c['Count'],freq=12,model=\"additive\")\n# # original = res\ntrend = res.trend\nseasonal = res.seasonal\nresidual = res.resid\n\nfig,ax=plt.subplots(figsize = (20,15))\nax1 = fig.add_subplot(411)\nax1.xaxis.set_major_locator(ticker.MultipleLocator(80))\nax1.plot(c['Count'], label='Original')\nax1.legend(loc='best')\nax2 = fig.add_subplot(412)\nax2.xaxis.set_major_locator(ticker.MultipleLocator(80))\nax2.plot(trend, label='Trend')\nax2.legend(loc='best')\nax3 = fig.add_subplot(413)\nax3.xaxis.set_major_locator(ticker.MultipleLocator(10))\nax3.plot(seasonal[:100],label='Seasonality')\nax3.legend(loc='best')\nax4 = fig.add_subplot(414)\nax4.xaxis.set_major_locator(ticker.MultipleLocator(80))\nax4.plot(residual, label='Residuals')\nax4.legend(loc='best')\nplt.tight_layout()\n","9232b07b":"def test_stationarity(series,mlag = 365, lag = None,):\n    print('ADF Test Result')\n    res = adfuller(series, maxlag = mlag, autolag = lag)\n    output = pd.Series(res[0:4],index = ['Test Statistic', 'p value', 'used lag', 'Number of observations used'])\n    for key, value in res[4].items():\n        output['Critical Value ' + key] = value\n    print(output)","45f987db":"test_stationarity(c['Count'],lag = 'AIC')","0ba00e17":"d1 = c.copy()\nd1['Count'] = d1['Count'].diff(1)\nd1 = d1.dropna()\nlineplt(d1[\"Date\"],d1[\"Count\"],\"Date\",\"Count\",\"Crimes by Time\",(20,15),80)\nprint('Average= '+str(d1['Count'].mean()))\nprint('Std= ' + str(d1['Count'].std()))\nprint('SE= ' + str(d1['Count'].std()\/math.sqrt(len(d1))))\nprint(test_stationarity(d1['Count'],lag = 'AIC'))","a439e2f1":"fig_2 = plt.figure(figsize=(16,8))\nax1_2 = fig_2.add_subplot(211)\nfig_2 = plot_acf(d1[\"Count\"],lags=15,ax=ax1_2)\nax2_2 = fig_2.add_subplot(212)\nfig_2 = plot_pacf(d1[\"Count\"],lags=15,ax=ax2_2)","3be7f438":"timeseries = c['Count']\np,d,q = (4,1,2)\narma_mod = ARMA(timeseries,(p,d,q)).fit()\nsummary = (arma_mod.summary2(alpha=.05, float_format=\"%.8f\"))\nprint(summary)","64fbee79":"predict_data = arma_mod.predict(start='2016-07-01', end='2017-07-01', dynamic = False)\ntimeseries.index = pd.DatetimeIndex(timeseries.index)\nfig, ax = plt.subplots(figsize=(20, 15))\nax = timeseries.plot(ax=ax)\npredict_data.plot(ax=ax)\nplt.show()","cce8bfa0":"p = d = q = range(0, 2)\n \n# Generate all different combinations of p, q and q triplets\npdq = list(itertools.product(p, d, q))\n \n# Generate all different combinations of seasonal p, q and q triplets\nseasonal_pdq = [(x[0], x[1], x[2], 7) for x in list(itertools.product(p, d, q))]\n \nprint('Examples of parameter combinations for Seasonal ARIMA...')\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[1]))\nprint('SARIMAX: {} x {}'.format(pdq[1], seasonal_pdq[2]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[3]))\nprint('SARIMAX: {} x {}'.format(pdq[2], seasonal_pdq[4]))","d9a20447":"res = pd.DataFrame(columns = ['order', 'seasonal_order', 'AIC'])","45272776":"warnings.filterwarnings(\"ignore\") # specify to ignore warning messages\nfor param in pdq:\n    for param_seasonal in seasonal_pdq:\n        try:\n            mod = SARIMAX(c['Count'],order=param,seasonal_order=param_seasonal) \n            results = mod.fit()\n            data = {'order': param, 'seasonal_order': param_seasonal, 'AIC':results.aic}\n#             print('ARIMA{}x{}7 - AIC:{}'.format(param, param_seasonal, results.aic))\n            res = res.append(data,ignore_index=True)\n        except:\n            continue\nres = res.sort_values(by = 'AIC', ascending = True)\nprint(res.head(5))","f3cc6a49":"model=SARIMAX(c['Count'], order=(1,1,1), seasonal_order=(1,1,1, 7)).fit()\nsummary = model.summary()\nprint(summary)\n# print(c['Count'].index.inferred_freq)\n","06651666":"model.plot_diagnostics(figsize=(15, 12))\nplt.show()","f733757f":"predict_data = model.predict(start='2016-07-01', end='2017-07-01', dynamic = False)\ntimeseries.index = pd.DatetimeIndex(timeseries.index)\nfig, ax = plt.subplots(figsize=(20, 15))\nax = timeseries.plot(ax=ax)\npredict_data.plot(ax=ax)\nplt.show()","bfd235d2":"# Get forecast 30 steps ahead in future\npred_uc = model.get_forecast(steps=30)\n\n# Get confidence intervals of forecasts\npred_ci = pred_uc.conf_int()","e60213dd":"ax = c['Count'][-60:].plot(label='observed', figsize=(15, 10))\npred_uc.predicted_mean.plot(ax=ax, label='Forecast')\nax.fill_between(pred_ci.index,\n                pred_ci.iloc[:, 0],\n                pred_ci.iloc[:, 1], color='k', alpha=.25)\nax.set_xlabel('Date')\nax.set_ylabel('Counts')\n \nplt.legend()\nplt.show()","aca862ed":"week = createdf(\"Week\",Rawdata[\"DAY_OF_WEEK\"].value_counts().index,\"Count\",Rawdata[\"DAY_OF_WEEK\"].value_counts())\nweek=week.reindex(['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'])\nplt.bar(week[\"Week\"] , week[\"Count\"], width=0.3)\nplt.ylim(36000, 50000)\nplt.title('Crimes by WeekDay')\nplt.show()","a7590f31":"target = Rawdata[(Rawdata['DATE'] > \"2016-07-01\") & (Rawdata['DATE'] < \"2017-08-01\")]\ntarget = target.sort_values(by=\"DATE\",ascending = True)","00fb364a":"t1 = createdf(\"Date\",target[\"DATE\"].value_counts().index,\"Count\",target[\"DATE\"].value_counts())\nt1 = t1.sort_values(by=\"Date\",ascending = True)\nlineplt(t1[\"Date\"],t1[\"Count\"],\"Date\",\"Count\",\"Crimes by Time(2016-07-01~2017-08-01)\",(15,8),80)","8db3b81f":"test_stationarity(t1['Count'],mlag = 180,lag='AIC')","34e1a759":"target.info()","50f2c2cc":"print(target[\"DISTRICT\"].unique())","c7146bf9":"# target = target.dropna()\nfig,ax = plt.subplots(figsize =(15,40))\n# ax.xaxis.set_major_locator(ticker.MultipleLocator(5))\ni = 0\nfor dis in target[\"DISTRICT\"].unique():\n    if dis is not np.nan :\n        i += 1\n        da = target[target[\"DISTRICT\"] == dis]\n        d = createdf(\"Date\",da[\"DATE\"].value_counts().index,\"Count\",da[\"DATE\"].value_counts())\n        d = d.sort_values(by=\"Date\",ascending = True)\n        fig.add_subplot(12,1,i)\n        plt.plot(d[\"Date\"],d[\"Count\"])     \n        plt.subplots_adjust(left=None, bottom=None, right=None, top=None,\n                wspace=None, hspace=0.4)\n        ax=plt.gca()\n        ax.xaxis.set_major_locator(ticker.MultipleLocator(50))\n        plt.title(dis,fontsize = 20)\nplt.show()","fdb32b10":"def featureScaling(arr):\n    scaler = MinMaxScaler(feature_range=(0, 1))\n    result = scaler.fit_transform(arr)\n    return result","8bf7d750":"t1[\"Count\"]=featureScaling(t1[\"Count\"].values.reshape(-1,1))\nlineplt(t1[\"Date\"],t1[\"Count\"],\"Date\",\"Count\",\"Crimes by Time(2016-07-01~2017-08-01)\",(15,8),80)","43114bd0":"def dtw(seq1,seq2): #\u52a8\u6001\u65f6\u95f4\u89c4\u6574\uff1a\u5f62\u53c2\u4e3a\u65f6\u95f4\u5e8f\u5217seq1,seq2 \n    m1=len(seq1)\n    m2=len(seq2)\n    #\u521d\u59cb\u5316\u8ddd\u79bb\u77e9\u9635\n    distance=np.zeros(shape=(m1,m2)) #m1\u884c,m2\u5217\u7684\u8ddd\u79bb\u77e9\u9635\n    for i in range(m1):\n        for j in range(m2):\n            distance[i,j]=(seq1[i]-seq2[j])**2 #\u4e00\u7ef4\u6570\u7ec4\u5143\u7d20\u4e4b\u95f4\u7684\u6b27\u5f0f\u8ddd\u79bb\u7684\u5e73\u65b9    \n    #\u6784\u5efa\u4e00\u4e2a\u4e0e\u77e9\u9635d\u76f8\u540c\u5927\u5c0f\u7d2f\u79ef\u8ddd\u79bb\u77e9\u9635\u7684D\n    D=np.zeros(shape=(m1,m2))\n    D[0,0]=distance[0,0] #\u7b2c\u4e00\u4e2a\u5143\u7d20\u548c\u8ddd\u79bb\u77e9\u9635\u4fdd\u6301\u4e00\u81f4\n    for i in range(1,m1): #\u7d2f\u79ef\u8ddd\u79bb\u77e9\u9635\u7684\u5de6\u8fb9\u754c\n        D[i,0]=distance[i,0]+D[i-1,0]\n    for j in range(1,m2):#\u7d2f\u79ef\u8ddd\u79bb\u77e9\u9635\u7684\u4e0a\u8fb9\u754c\n        D[0,j]=distance[0,j]+D[0,j-1]\n    for i in range(1,m1):\n        for j in range(1,m2):\n            D[i,j]=distance[i,j]+np.min([D[i-1,j-1],D[i-1,j],D[i,j-1]])\n    return D[m1-1,m2-1] #\u51fd\u6570\u8fd4\u56de\u503c\u4e3a\u6700\u5c0f\u52a8\u6001\u89c4\u5212\u8def\u5f84","5a3aa790":"for dis in target[\"DISTRICT\"].unique():\n    if dis is not np.nan :\n        da = target[target[\"DISTRICT\"] == dis]\n        d = createdf(\"Date\",da[\"DATE\"].value_counts().index,\"Count\",da[\"DATE\"].value_counts())\n        d = d.sort_values(by=\"Date\",ascending = True)\n        d[\"Count\"]=featureScaling(d[\"Count\"].values.reshape(-1,1))\n        print(dis + ' distance: ' + str(dtw(t1[\"Count\"],d[\"Count\"])))","3532325a":"t2 = createdf(\"District\",target['DISTRICT'].value_counts(dropna=False).index,\"Count\",target['DISTRICT'].value_counts(dropna=False))","24ffa348":"t2[\"Count\"].sum()\nt2['Percent'] = t2[\"Count\"]\/t2[\"Count\"].sum()","efd8c1cc":"fig = plt.figure(figsize=(16,8))\nplot = squarify.plot(sizes = t2[\"Percent\"], # \u6307\u5b9a\u7ed8\u56fe\u6570\u636e\n                     label = t2[\"District\"], # \u6307\u5b9a\u6807\u7b7e\n                     alpha = 0.6, # \u6307\u5b9a\u900f\u660e\u5ea6\n                     value = t2[\"Percent\"].apply(lambda x: format(x, '.2%')) , # \u6dfb\u52a0\u6570\u503c\u6807\u7b7e\n                     edgecolor = 'white', # \u8bbe\u7f6e\u8fb9\u754c\u6846\u4e3a\u767d\u8272\n                     linewidth =3 # \u8bbe\u7f6e\u8fb9\u6846\u5bbd\u5ea6\u4e3a3\n                    )\nplot.set_title('Crimes by Districts',fontdict = {'fontsize':25})\nplt.show()","fc4481ca":"Rawdata.Lat.replace(-1, None, inplace=True)\nRawdata.Long.replace(-1, None, inplace=True)","0bce52db":"fig = plt.figure(figsize=(16,8))\nsns.scatterplot(x='Lat',\n               y='Long',\n                hue='DISTRICT',\n                alpha=0.01,\n               data=Rawdata)\nplt.legend(loc=2)","981a25b1":"## 2.4 SARIMA Model","a7494356":"We know the crimes have a kind of pattern and I want to know why is that. To simplify, I will only use 1 year data to do analysis.","9df29064":"I'm going to use Dynamic Time Warping (DTW) Algorithm to calculate the similarity. The algorithm will return a distance. That represent the distance between 2 time series. A higher distance means the 2 time series are not similar to each other.","25526f49":"Let's try to forecast the data for next 30 days!","cea16c09":"After seasonal decomposing, we can have a clear view of the pattern of the distribution of crimes. But there is one problem. When we try to plot the data, the chart will have different shape if we use different scales. For example, if the range of y axis is between 0~300, then the variation will be very clear, and we can see if there is a trend. But if the range is between 0~3000, then the variation will be unclear and the shape of the date could be a straight line. So we need to value the stationarity of the data becasue I'm planning to use ARIMA model.","0ca9fd00":"Because the data is not stationary, we need to do first difference to the date in order to make it stationary.","679b21e8":"Becasue we have too many districts, a pie chart might not be a good choice. So I choose to use squarify treemap.","83faead5":"### 2.1.1 Shapiro-Wilk test\n(For N > 5000 the W test statistic is accurate but the p-value may not be.)","8f8328b2":"## 4.3 Q3\n\nThe distribution of the crimes is a \"Sin\" shape. By using Dynamic Time Warping (DTW) Algorithm, we find the shape of D4, A1, B3, C11, B2 is relatively similar to the shape of whole data and these districts each contributes more than 10% of the whole crimes. So we can say it could be the variation of these districts that cause the variation of the whole data.","17e5f92c":"Because the ranges of the distribution are different, for exmaple the E18 is between 0 to 30 while D4 is between 0 to 60. In order to minimize the influence of scale, we need to standardize the data.","1b7116dd":"# 3. Why the distribution of crimes shows a pattern?","7a09979e":"I seperate the data into 50 bins and plot the data using histogram. We can see that the distribution seems like a normal distribution. Many days have about 250 ~ 300 crimes happened. Some days can have more than 350 crimes and some days have less than 150 crimes happened.\n\nThe Skewness is negative means that the distribution is left skewed, but not that much.  \nThe kurtosis is greater than 0 means that the peak of the distribution is sharper than a standard normal distribution.\n\nLet's test if the crimes count distribution follows a normal distribution, even it seems like a normal distribution.","419958e1":"By looking at the Autocorrelation and Partical Autocorrelation (lag = 200 and lag = 15), we can conclude that:\n\n(1) From lag 1 to lag 100, the correlation is positive and from lag 100 to lag 200, the correlation is negative. So around every 100 days, the correlation will be reversed. This can describe the \"Sin\" shape.\n\n(2) When we make the lag shorter, we can see more details about the correlation. The partical correlations are significant when lag 1, lag 6 and lag 7. So we can conclude that the crimes are correlated with yesterday and the same day in last week.","2b68947a":"The everyday crimes count is contributed by these district. So I want to see if some of these districts have a significant influence to the whole distribution.","6533c486":"During our analysis, we find the weekly correlation influence a lot. So let's plot the distribution of weekly crimes.","e436aa3f":"By looking at the plot of each district, it's really hard to identify the influence of each district.\n\nI plan to use the similarity of shapes to represent the influence. If the shape of the distributino of one District is very similar to the whole distribution, I will say that district has more influence to the whole distribution.","449f11c4":"# 2.2.1 ADF Test","efcb5ad8":"# 2 Does the frequency of crimes have any pattern?\n","659e4db7":"Now the new prediction is much better. But it's not doing well when there are extreme numbers.","09e1f871":"After the fist differencing, the chat looks much more stationary and the ADF test shows a pretty low p value. So we can rejct H0. The Time series is stational after first differencing.","addcf12e":"# In this EDA, I'm going to solve 3 questions","5f7e58de":"The report shows that, the \"used lag\" is 34 and p value is 0.19. So we can conclude, in the range of 34 days, we can **NOT Reject** the null hypotheses which is the time series is non-stationary.","4a2a73cc":"The result shows:\n\nMost crimes happened on Friday and least crimes happened on Sunday. There are many parameters can cause this results, we need more outside data to identify the reason.","96f034f3":"So totally, we have 319071 pieces of data and most columes have no NaN cell, which is a good thing. In this EDA, I will use 'DISTRICT', 'DATE' columes very frequently.","f5e8452c":"### 2.1.2 Kolmogorov-Smirnov test","0375b80e":"In this model, we will try all combinations of (p,d,q) and (P,D,Q,7) and use AIC standard to find the best combination that can minimize AIC. I use \"7\" becasue the period is 7 days.","2fd99a50":"1. Does the frequency of crimes have any pattern?\n2. Is it possible to forecast the daily frequency of crimes? How?\n3. If the frequency of crime has some patterns, why is that?","fb12b2ed":"By looking at the prediction of 1 year data, the yellow line is the prediction of daily crimes. It looks the predictions are always **under estimated**. \n\nIt could because the seasonal correlation. So we need to use another model called SARIMA model.","17ec6228":"1. Does the frequency of crimes have any pattern?\n2. Is it possible to forecast the daily frequency of crimes? How?\n3. If the frequency of crime has some patterns, why is that?","39a03908":"From the chart above, we can see there are many peaks and troughs and they shows a kind of pattern like \"sin\" function. I'm going to use some numbers and charts to describe the pattern in detail.","3d69d718":"The result shows that (1,1,1) (1,1,1,7) is the best combination to minimize AIC. We will use it to build our SARIMA model.","629000e3":"## 2.2 Distribution of Crimes by Time","814b3af3":"# 4 Conclusion","8e03b6b6":"## 2.1 The distribution of the Counts of Crimes by date","5a07e43e":"The Autocorrelation and Partial Autocorrelation charts are not very perfet. We can see there is a seasonal pattern every 7 days. \n\nWe will deal with it later. But let's build the ARIMA model first","ce4d7161":"The chart below shows the location of these districts on map","bc97390b":"## 2.3 ARIMA Model","bfd40575":"From the result of this two tests, we can see that the p value is very small, much smaller than 5%. So we can conclude that the distribution is **Significantly different** from normal distribution under 95% confidence.","aa19753b":"The results shows that D4, A1, B3, C11 have a relative low distance and B2 has a median distance. We can say the distribution of these districts could be relatively similar to the whole distribution.\n\nBut we need to see the proportion of each districts.","f5d7b946":"# 1 Data Overview","d23ae710":"It's very interesting to see that D4, C11, B2, B3 and A1 all have more than 10% contribute to the whole crimes. Also the shape of the these districts are similar to the whole distribution. \n\nWe could say the variation of the wholee distribution is cause by the variation of these districts.","0d9e7035":"## 4.2 Q2\nIt's possible to forecast the daily frequency of crimes using ARIMA model, but due to the seasonality, the forecasting model is not perfect.\n\nAfter using SARIMA model, the forecasting looks better.\n\nAlso, we find most crimes happened on Friday and least crimes happened on Sunday.","0c2924cb":"## 4.1 Q1\nThe frequency of crimes looks like a normal shape distribution. But it doesn\u2019t pass the Shapiro-Wilk test and Kolmogorov-Smirnov test, so it's Significantly Different from a normal distribution.\n"}}