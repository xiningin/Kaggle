{"cell_type":{"caf35128":"code","bce517d7":"code","9eb598b1":"code","d224832e":"code","941ceecd":"code","67d216ef":"code","b3be7c25":"code","2ff3f76e":"code","21ef2d68":"code","87d74401":"markdown","7a0b94d1":"markdown","be4ad34a":"markdown","0ac579df":"markdown"},"source":{"caf35128":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bce517d7":"import numpy as np\nimport pandas as pd\n\nimport sklearn\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\n\nfrom xgboost import XGBRegressor\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMRegressor\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostRegressor\nfrom catboost import CatBoostClassifier","9eb598b1":"train = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\")\nsample_solution = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/sample_solution.csv\")","d224832e":"X = train.drop(columns=['id','claim']).copy()\ny = train['claim'].copy()\ntest_data = test.drop(columns=['id']).copy()","941ceecd":"# Data preprocessing functions\ndef get_stats_per_row(data):\n    features = [x for x in data.columns.values if x[0]==\"f\"] \n    # n_missing is the number of null value in a row\n    data['n_missing'] = data[features].isna().sum(axis=1)\n    \n    data['max_row'] = data[features].max(axis=1)\n    data['min_row'] = data[features].min(axis=1)\n    data['std'] = data[features].std(axis=1)\n    \n    # If the difference between the mean and the median was large, the median function was used.\n    # If the difference is small the quantile function was used.\n    # Quantile function has better than mean function.\n    median_set = ['f9', 'f12', 'f26', 'f27', 'f28', 'f32', 'f33', 'f35', 'f62', 'f74', 'f82', 'f86', 'f98', 'f108', 'f116']\n    for col_name in features:\n        if col_name in median_set:\n            data[col_name].fillna(data[col_name].median(), inplace=True)\n        else:\n            data[col_name].fillna(data[col_name].quantile(0.75), inplace=True)\n            \n    # Multiply feature is the feature that multiply all values in a row\n    data['multiply'] = 1\n    for feature in features:\n        data['multiply'] = data[feature] * data['multiply']\n    \n    return data\n\nX = get_stats_per_row(X)\ntest_data = get_stats_per_row(test_data)","67d216ef":"# Create preprocessing pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\n\npipeline = Pipeline([\n    ('scale', StandardScaler())\n])\n\nX = pd.DataFrame(columns=X.columns, data=pipeline.fit_transform(X))\ntest_data = pd.DataFrame(columns=test_data.columns, data=pipeline.transform(test_data))","b3be7c25":"# Model parameter\nbest_params = {\n    'iterations': 15585, \n    'objective': 'CrossEntropy', \n    'bootstrap_type': 'Bernoulli', \n    'od_wait': 1144, \n    'learning_rate': 0.023575206684596582, \n    'reg_lambda': 36.30433203563295, \n    'random_strength': 43.75597655616195, \n    'depth': 7, \n    'min_data_in_leaf': 11, \n    'leaf_estimation_iterations': 1, \n    'subsample': 0.8227911142845009,\n    'task_type' : 'GPU',\n    'devices' : '0',\n    'verbose' : 0\n}","2ff3f76e":"%%time\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_curve, auc\nfrom catboost import CatBoostClassifier\n\nkf = KFold(n_splits=5, shuffle=True, random_state=1)\n\npred_tmp = []\nscores = []\n\nfor fold, (idx_train, idx_valid) in enumerate(kf.split(X)):\n    X_train, y_train = X.iloc[idx_train], y.iloc[idx_train]\n    X_valid, y_valid = X.iloc[idx_valid], y.iloc[idx_valid]\n\n    model = CatBoostClassifier(**best_params)\n    model.fit(X_train, y_train)\n\n    # validation prediction\n    pred_valid = model.predict_proba(X_valid)[:,1]\n    fpr, tpr, _ = roc_curve(y_valid, pred_valid)\n    score = auc(fpr, tpr)\n    scores.append(score)\n    \n    print(f\"Fold: {fold + 1} Score: {score}\")\n    print('::'*20)\n    \n    # test prediction\n    y_hat = model.predict_proba(test_data)[:,1]\n    pred_tmp.append(y_hat)\n    \nprint(f\"Overall Validation Score: {np.mean(scores)}\")","21ef2d68":"# Average predictions over all folds\npredictions = np.mean(np.column_stack(pred_tmp),axis=1)\n\n# Create submission file\nsample_solution['claim'] = predictions\nsample_solution.to_csv('.\/catb_baseline.csv', index=False)","87d74401":"# 3. Modeling","7a0b94d1":"# 2. Data preprocessing","be4ad34a":"https:\/\/www.kaggle.com\/tolight20\/tps-gmo-tolight20\n\n## Comparison analysis\n\n- From there CatBoost was best model\n- Best imputation was quantile 0.75\n- And extra feature improved AUC score\n- So let's Start by that","0ac579df":"# 1. Calling the Data sets"}}