{"cell_type":{"8f156782":"code","840c1ff4":"code","736e8d3f":"code","2407b384":"code","2f2ff934":"code","28ce182c":"code","8ae6726a":"code","2161750b":"code","3e240aa1":"code","bec3dd35":"code","bc84662d":"code","31b8402f":"code","4c17aca9":"code","2597dc9b":"code","1158115e":"code","f94bbc2f":"code","3d592451":"code","647dde90":"code","7cce5232":"code","5127afb9":"code","895479fa":"code","aad7212f":"code","92aca4b9":"code","34af25be":"code","21d73c74":"code","a693a053":"code","3b9a9012":"code","82258404":"code","3c3625ce":"code","49f4b2ea":"code","a495f103":"code","9a03f9c4":"code","ed804b4f":"code","23684a21":"code","8311e787":"code","212a9187":"code","16ec98d4":"code","45a3f747":"code","745bf7a8":"code","14baad2a":"code","4e453388":"code","08e0edbb":"code","54498dbc":"code","2482c268":"code","26806162":"code","00734c37":"code","343e54e3":"code","c05df652":"code","e9c167e9":"code","21e1d887":"code","8dda6e21":"code","2bb70d2c":"code","92d7b31d":"code","417667be":"markdown","247bae3d":"markdown","cff90285":"markdown","de958501":"markdown","032cf9d6":"markdown","ca9805b1":"markdown","a91f36da":"markdown","7036eb3a":"markdown","c185809a":"markdown","aec64d6d":"markdown","4fcd03ae":"markdown","453fc63c":"markdown","9908f9f3":"markdown","1ff8e2be":"markdown","94425c02":"markdown"},"source":{"8f156782":"!pip install seaborn==0.11\n\nimport numpy as np\nimport pandas as pd\nimport warnings\nwarnings.filterwarnings(action='ignore')\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy as sp\nimport missingno as msno\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler, StandardScaler\nfrom sklearn.metrics import f1_score\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom catboost import CatBoostClassifier\n\nfrom bayes_opt import BayesianOptimization\nfrom skopt  import BayesSearchCV \n\n\n\nimport os","840c1ff4":"# load_data\ntrain = pd.read_csv('\/kaggle\/input\/kakr-4th-competition\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/kakr-4th-competition\/test.csv')\nsample_submission = pd.read_csv('..\/input\/kakr-4th-competition\/sample_submission.csv')","736e8d3f":"train.info()","2407b384":"test.info()","2f2ff934":"train.describe()","28ce182c":"test.describe()","8ae6726a":"import missingno as msno\nmsno.matrix(train), msno.matrix(test)","2161750b":"train.income.describe()","3e240aa1":"pd.concat([train['income'].value_counts(),\n          train['income'].value_counts(normalize = True).mul(100)], axis = 1, keys = ('counts', 'percentage'))","bec3dd35":"cont_var = ['age', 'education_num', 'capital_gain', 'capital_loss', 'hours_per_week']\ncat_var = ['workclass', 'education', 'marital_status', 'occupation', 'relationship', 'race', 'sex', 'native_country']","bc84662d":"for i in cont_var:\n    fig, ax = plt.subplots(1, 1, figsize = (20, 6))\n    sns.kdeplot(train[i], shade = True, hue = train['income'])","31b8402f":"for i in cat_var:\n    fig, ax = plt.subplots(1, 1, figsize = (20, 6))\n    sns.countplot(x = i, hue = train['income'], data = train)","4c17aca9":"# Workclass\npd.crosstab(train['workclass'], train['income'])","2597dc9b":"# Education\npd.crosstab(train['education'], train['income'])","1158115e":"# Marital Status\npd.crosstab(train['marital_status'], train['income'])","f94bbc2f":"# Occupation\npd.crosstab(train['occupation'], train['income'])","3d592451":"# Relationship\npd.crosstab(train['relationship'], train['income'])","647dde90":"# Race\npd.crosstab(train['race'], train['income'])","7cce5232":"# Sex\npd.crosstab(train['sex'], train['income'])","5127afb9":"# Native Country\npd.crosstab(train['native_country'], train['income'])","895479fa":"sns.heatmap(train.corr(), square = True, linecolor = 'white', linewidth = '0.2', \n            cmap = 'coolwarm', vmax = 1.0, vmin = -1.0)","aad7212f":"# Tables -side by side\n\nfrom IPython.display import display_html\ndef display_side_by_side(*args):\n    html_str=''\n    for df in args:\n        html_str+=df.to_html()\n    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)","92aca4b9":"test2 = test\ntrain2 = train","34af25be":"train2 = train.apply(LabelEncoder().fit_transform)\ntest2 = test.apply(LabelEncoder().fit_transform)","21d73c74":"columns = list(test)\nfor i in columns:\n    print(sp.stats.ks_2samp(train2[i], test2[i]), i)","a693a053":"for i in list(train.columns):\n    if i != 'income':\n        fig, ax = plt.subplots(1, 1, figsize = (20, 6))\n        ax = sns.distplot(train2[i], kde = True)\n        ax = sns.distplot(test2[i], kde = True)\n        plt.legend()\n        plt.show()","3b9a9012":"for i in list(cat_var):\n    display_side_by_side(pd.concat([train[i].value_counts(),\n        train[i].value_counts(normalize = True).mul(100)], axis = 1, keys = ('counts', 'percentage')), \n            pd.concat([test[i].value_counts(),\n        test[i].value_counts(normalize = True).mul(100)], axis = 1, keys = ('counts', 'percentage')))","82258404":"for i in list(train.columns):\n    if i != 'income':\n        print(sp.stats.mannwhitneyu(train2[i], test2[i]), i)","3c3625ce":"# Split and encode label\nlabel = train['income']\ndel train['income']\nlabel = label.map(lambda x: 1 if x == '>50K' else 0)","49f4b2ea":"# Split train\/test\nx_train, x_valid, y_train, y_valid = train_test_split(train, label, \n                                                          test_size=0.2,\n                                                          random_state=2020,\n                                                          shuffle=True)","a495f103":"# Preprocess data\n\ndef preprocess(x_train, x_valid, x_test):\n    \n    global tmp_x_train\n    global tmp_x_valid\n    global tmp_x_test\n    \n    tmp_x_train = x_train.copy()\n    tmp_x_valid = x_valid.copy()\n    tmp_x_test  = x_test.copy()\n    \n    tmp_x_train = tmp_x_train.reset_index(drop=True)\n    tmp_x_valid = tmp_x_valid.reset_index(drop=True)\n    tmp_x_test  = tmp_x_test.reset_index(drop=True)\n    \n    # column \uc81c\uac70\n    tmp_x_train.drop(['id','fnlwgt','education','relationship','native_country','workclass'], axis=1, inplace=True)\n    tmp_x_valid.drop(['id','fnlwgt','education','relationship','native_country','workclass'], axis=1, inplace=True)\n    tmp_x_test.drop(['id','fnlwgt','education','relationship','native_country','workclass'], axis=1, inplace=True)\n    \n    # marital status\n    tmp_x_train['marital_status'] = (tmp_x_train['marital_status'] == 'Married-civ-spouse').astype(int)\n    tmp_x_valid['marital_status'] = (tmp_x_valid['marital_status'] == 'Married-civ-spouse').astype(int)\n    tmp_x_test['marital_status'] = (tmp_x_test['marital_status'] == 'Married-civ-spouse').astype(int)\n    \n    # race\n    tmp_x_train['race'] = ((tmp_x_train['race'] == 'White') | (tmp_x_train['race'] == 'Asian-Pac-Islander')).astype(int)\n    tmp_x_valid['race'] = ((tmp_x_valid['race'] == 'White') | (tmp_x_valid['race'] == 'Asian-Pac-Islander')).astype(int)\n    tmp_x_test['race'] = ((tmp_x_test['race'] == 'White') | (tmp_x_test['race'] == 'Asian-Pac-Islander')).astype(int)\n    \n    # capital_gain, loss\n    tmp_x_train['cap_gain_high'] = (tmp_x_train['capital_gain'] != 0).astype(int)\n    tmp_x_train['cap_loss_high'] = (tmp_x_train['capital_loss'] >= 1700).astype(int)\n    tmp_x_train['capital_gain'] = tmp_x_train['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n    \n    tmp_x_valid['cap_gain_high'] = (tmp_x_valid['capital_gain'] != 0).astype(int)\n    tmp_x_valid['cap_loss_high'] = (tmp_x_valid['capital_loss'] >= 1700).astype(int)\n    tmp_x_valid['capital_gain'] = tmp_x_valid['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n    \n    tmp_x_test['cap_gain_high'] = (tmp_x_test['capital_gain'] != 0).astype(int)\n    tmp_x_test['cap_loss_high'] = (tmp_x_test['capital_loss'] >= 1700).astype(int)\n    tmp_x_test['capital_gain'] = tmp_x_test['capital_gain'].map(lambda x : np.log(x) if x != 0 else 0)\n    \n    # age\n    tmp_x_train.loc[tmp_x_train['age'] < 20, 'age_range'] = '~20'\n    tmp_x_train.loc[tmp_x_train['age'] >= 65, 'age_range'] = '~65'\n    down = 20\n    for i in range(45\/\/5):\n        tmp_x_train.loc[(tmp_x_train['age'] >= down) & (tmp_x_train['age'] < down+5), 'age_range'] = str(down)+'~'+str(down+5)\n        down += 5\n    tmp_x_train['age'] = tmp_x_train['age_range']\n    tmp_x_train.drop(['age_range'], axis=1, inplace=True)\n    \n    tmp_x_valid.loc[tmp_x_valid['age'] < 20, 'age_range'] = '~20'\n    tmp_x_valid.loc[tmp_x_valid['age'] >= 65, 'age_range'] = '~65'\n    down = 20\n    for i in range(45\/\/5):\n        tmp_x_valid.loc[(tmp_x_valid['age'] >= down) & (tmp_x_valid['age'] < down+5), 'age_range'] = str(down)+'~'+str(down+5)\n        down += 5\n    tmp_x_valid['age'] = tmp_x_valid['age_range']\n    tmp_x_valid.drop(['age_range'], axis=1, inplace=True)\n    \n    tmp_x_test.loc[tmp_x_test['age'] < 20, 'age_range'] = '~20'\n    tmp_x_test.loc[tmp_x_test['age'] >= 65, 'age_range'] = '~65'\n    down = 20\n    for i in range(45\/\/5):\n        tmp_x_test.loc[(tmp_x_test['age'] >= down) & (tmp_x_test['age'] < down+5), 'age_range'] = str(down)+'~'+str(down+5)\n        down += 5\n    tmp_x_test['age'] = tmp_x_test['age_range']\n    tmp_x_test.drop(['age_range'], axis=1, inplace=True)\n\n#     # age2\n#     tmp_x_train['age2'] = tmp_x_train['age'] ** 2\n#     tmp_x_valid['age2'] = tmp_x_train['age'] ** 2\n#     tmp_x_test['age2'] = tmp_x_train['age'] ** 2\n        \n    # edu_num\n    tmp_x_train['edu_num_high'] = (tmp_x_train['education_num'] >= 13).astype(int)\n    tmp_x_valid['edu_num_high'] = (tmp_x_valid['education_num'] >= 13).astype(int)\n    tmp_x_test['edu_num_high'] = (tmp_x_test['education_num'] >= 13).astype(int)\n    \n    # hours-per-week\n    tmp_x_train['hpw_high'] = (tmp_x_train['hours_per_week'] >= 50).astype(int)\n    tmp_x_valid['hpw_high'] = (tmp_x_valid['hours_per_week'] >= 50).astype(int)\n    tmp_x_test['hpw_high'] = (tmp_x_test['hours_per_week'] >= 50).astype(int)\n    \n    # min-max scaler\n    mmscaler = MinMaxScaler()\n    tmp_x_train['education_num'] = mmscaler.fit_transform(tmp_x_train['education_num'].values.reshape(-1,1))\n    tmp_x_valid['education_num'] = mmscaler.transform(tmp_x_valid['education_num'].values.reshape(-1,1))\n    tmp_x_test['education_num'] = mmscaler.transform(tmp_x_test['education_num'].values.reshape(-1,1))\n    \n    tmp_x_train['hours_per_week'] = mmscaler.transform(tmp_x_train['hours_per_week'].values.reshape(-1,1))\n    tmp_x_valid['hours_per_week'] = mmscaler.transform(tmp_x_valid['hours_per_week'].values.reshape(-1,1))\n    tmp_x_test['hours_per_week'] = mmscaler.transform(tmp_x_test['hours_per_week'].values.reshape(-1,1))\n\n    # Label Encoding\n    label_enc = LabelEncoder()\n    \n    tmp_x_train['race'] = label_enc.fit_transform(tmp_x_train['race'])\n    tmp_x_valid['race'] = label_enc.fit_transform(tmp_x_valid['race'])\n    tmp_x_test['race'] = label_enc.fit_transform(tmp_x_test['race'])\n    \n    tmp_x_train['sex'] = label_enc.fit_transform(tmp_x_train['sex'])\n    tmp_x_valid['sex'] = label_enc.fit_transform(tmp_x_valid['sex'])\n    tmp_x_test['sex'] = label_enc.fit_transform(tmp_x_test['sex'])\n    \n    tmp_x_train['marital_status'] = label_enc.fit_transform(tmp_x_train['marital_status'])\n    tmp_x_valid['marital_status'] = label_enc.fit_transform(tmp_x_valid['marital_status'])\n    tmp_x_test['marital_status'] = label_enc.fit_transform(tmp_x_test['marital_status'])\n    \n    # ohe\n    tmp_all = pd.concat([tmp_x_train, tmp_x_valid, tmp_x_test])\n    \n    ohe = OneHotEncoder(sparse=False)\n    cat_columns = ['age', 'marital_status', 'occupation', 'race', 'sex']\n    ohe.fit(tmp_all[cat_columns])\n    \n    \n    ohe_columns = list()\n    for lst in ohe.categories_:\n        ohe_columns += lst.tolist()\n    \n    tmp_train_cat = pd.DataFrame(ohe.transform(tmp_x_train[cat_columns]), columns=ohe_columns)\n    tmp_valid_cat = pd.DataFrame(ohe.transform(tmp_x_valid[cat_columns]), columns=ohe_columns)\n    tmp_test_cat  = pd.DataFrame(ohe.transform(tmp_x_test[cat_columns]), columns=ohe_columns)\n    \n    tmp_train_cat.columns = ohe.get_feature_names(cat_columns)\n    tmp_valid_cat.columns = ohe.get_feature_names(cat_columns)\n    tmp_test_cat.columns = ohe.get_feature_names(cat_columns)\n    \n    tmp_x_train = pd.concat([tmp_x_train, tmp_train_cat], axis=1)\n    tmp_x_valid = pd.concat([tmp_x_valid, tmp_valid_cat], axis=1)\n    tmp_x_test = pd.concat([tmp_x_test, tmp_test_cat], axis=1)\n\n    tmp_x_train = tmp_x_train.drop(columns=cat_columns)\n    tmp_x_valid = tmp_x_valid.drop(columns=cat_columns)\n    tmp_x_test = tmp_x_test.drop(columns=cat_columns)\n        \n    # drop capital_gain \/ capital_loss\n    \n    tmp_x_train.drop(columns = ['capital_gain', 'capital_loss'], inplace = True)\n    tmp_x_valid.drop(columns = ['capital_gain', 'capital_loss'], inplace = True)\n    tmp_x_test.drop(columns = ['capital_gain', 'capital_loss'], inplace = True)\n    \n    \n    return tmp_x_train.values, tmp_x_valid.values, tmp_x_test.values","9a03f9c4":"preprocess(x_train, x_valid, test)","ed804b4f":"# param_grid = {'penalty':['l1', 'l2', 'elasticnet'], 'C':[0.001, 0.01, 0.1, 1, 10, 100],\n#              'solver':['lbfgs', 'liblinear'], 'l1_ratio':[0.001, 0.01, 0.1]}\n\n# logit_grid = GridSearchCV(LogisticRegression(), param_grid=param_grid, verbose=3)\n\n# logit_grid.fit(tmp_x_train, y_train)\n# print('Best score reached: {} with params: {} '.format(logit_grid.best_score_, logit_grid.best_params_))","23684a21":"log_reg = LogisticRegression(C=10, l1_ratio=0.001, solver='lbfgs', penalty='l2')","8311e787":"# param_grid = {'criterion':['gini', 'entropy'], 'max_depth':[2, 4, 5, 7, 9, 10], 'n_estimators':[100, 200, 300, 400, 500]}\n\n# randfor_grid = GridSearchCV(RandomForestClassifier(), param_grid=param_grid, verbose=3)\n\n# randfor_grid.fit(tmp_x_train, y_train)\n# print('Best score reached: {} with params: {} '.format(randfor_grid.best_score_, randfor_grid.best_params_))","212a9187":"rfc = RandomForestClassifier(max_depth=10, n_estimators=100, criterion='gini')","16ec98d4":"# param_grid = {'depth':[2, 4, 5, 7, 9, 10], 'learning_rate':[0.001, 0.01, 0.1, 0.2, 0.3], 'iterations':[30, 50, 100]}\n\n# catboost_grid = GridSearchCV(CatBoostClassifier(), param_grid, verbose=3)\n\n# catboost_grid.fit(tmp_x_train, y_train)\n# print('Best score reached: {} with params: {} '.format(catboost_grid.best_score_, catboost_grid.best_params_))","45a3f747":"cb = CatBoostClassifier(iterations=100, depth=4, learning_rate=0.2, verbose=False)","745bf7a8":"# from sklearn.metrics import f1_score\n\n# def lgb_f1_score(y_hat, data):\n#     y_true = data.get_label()\n#     y_hat = np.round(y_hat) # scikits f1 doesn't like probabilities\n#     return 'f1', f1_score(y_true, y_hat), True","14baad2a":"# def bayes_parameter_opt_lgb(X, y, init_round=20, opt_round=30, n_folds=5, random_seed=2020, n_estimators=10000,\n#                             learning_rate=0.05, output_process=False):\n#     # prepare data\n\n#     train_data = lgb.Dataset(data=X, label=y)\n#     # parameters\n\n#     def lgb_eval(num_leaves, feature_fraction, bagging_fraction, max_depth, \n#                  lambda_l1, lambda_l2, min_split_gain, min_child_weight):\n        \n#         global cv_result\n\n#         params = {'objective':'binary','num_iterations':1000, 'learning_rate':0.05,\n#                   'early_stopping_round':100, 'metric':'binary_logloss'} #rmse\n#         params[\"num_leaves\"] = int(round(num_leaves))\n#         params['feature_fraction'] = max(min(feature_fraction, 1), 0)\n#         params['bagging_fraction'] = max(min(bagging_fraction, 1), 0)\n#         params['max_depth'] = int(round(max_depth))\n#         params['lambda_l1'] = max(lambda_l1, 0)\n#         params['lambda_l2'] = max(lambda_l2, 0)\n#         params['min_split_gain'] = min_split_gain\n#         params['min_child_weight'] = min_child_weight\n        \n#         cv_result = lgb.cv(params, train_data, nfold=3, seed=random_seed,\n#                            stratified=False, verbose_eval =200, metrics=['binary_logloss']) #rmse\n\n#         return min(cv_result['binary_logloss-mean']) \n\n#     # setting range of the parameters\n#     lgbBO = BayesianOptimization(lgb_eval, {'num_leaves': (24, 45),\n#                                             'feature_fraction': (0.1, 0.9),\n#                                             'bagging_fraction': (0.5, 1),\n#                                             'max_depth': (5, 8.99),\n#                                             'lambda_l1': (0, 5),\n#                                             'lambda_l2': (0, 3),\n#                                             'min_split_gain': (0.001, 0.1),\n#                                             'min_child_weight': (5, 60)}, random_state=2020)\n#     # optimize\n#     lgbBO.maximize(init_points=init_round, n_iter=opt_round)\n    \n#     # output optimization process\n#     if output_process==True: lgbBO.points_to_csv(\"bayes_opt_result.csv\")\n    \n#     # return\n#     return lgbBO\n\n# opt_params = bayes_parameter_opt_lgb(tmp_x_train, y_train, init_round=5, opt_round=10, n_folds=3,\n#                                      random_seed=2020, n_estimators=10000, learning_rate=0.05)","4e453388":"# print(opt_params.max['params'])","08e0edbb":"lgbm = LGBMClassifier(bagging_fraction = 0.5,\n                   feature_fraction = 0.9,\n                   lambda_l1 = 5.0,\n                   lambda_l2 = 3.0,\n                   max_depth = 9,\n                   min_child_weight = 60.0,\n                   min_split_gain = 0.1,\n                   num_leaves = 28)","54498dbc":"# param_grid = {'max_depth':[2, 4, 5, 7, 9, 10], 'learning_rate':[0.001, 0.01, 0.1, 0.2, 0.3], 'min_child_weight':[2, 4, 5, 6, 7]}\n\n# xgb_grid = GridSearchCV(XGBClassifier(tree_method='gpu_hist'), param_grid=param_grid, verbose=3)\n\n# xgb_grid.fit(tmp_x_train, y_train)\n# print('Best score reached: {} with params: {} '.format(xgb_grid.best_score_, xgb_grid.best_params_))","2482c268":"xgb = XGBClassifier(learning_rate=0.1, max_depth=5, min_child_weight=6)","26806162":"n_splits = 5\nskf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=2020)","00734c37":"val_scores = list()\n\nnew_x_train_list = [np.zeros((train.shape[0], 1)) for _ in range(5)]\nnew_x_test_list  = [np.zeros((test.shape[0], 1)) for _ in range(5)]\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(train, label)):\n    print(f\"Fold {i} Start\")\n    x_train, y_train = train.iloc[trn_idx, :], label[trn_idx]\n    x_valid, y_valid = train.iloc[val_idx, :], label[val_idx]\n    \n    # \uc804\ucc98\ub9ac\n    x_train, x_valid, x_test = preprocess(x_train, x_valid, test)\n    \n    # \ubaa8\ub378 \uc815\uc758\n    clfs = [log_reg,\n           rfc,\n           cb,\n           lgbm,\n           xgb] # \ucd94\uac00 \ubaa8\ub378 \uc785\ub825\n    \n    for model_idx, clf in enumerate(clfs):\n        clf.fit(x_train, y_train)\n        \n        new_x_train_list[model_idx][val_idx, :] = clf.predict_proba(x_valid)[:, 1].reshape(-1, 1)\n        new_x_test_list[model_idx][:] += clf.predict_proba(x_test)[:, 1].reshape(-1, 1) \/ n_splits","343e54e3":"new_train = pd.DataFrame(np.concatenate(new_x_train_list, axis=1), columns=None)\nnew_label = label\nnew_test = pd.DataFrame(np.concatenate(new_x_test_list, axis=1), columns=None)\n\nnew_train.shape, new_label.shape, new_test.shape","c05df652":"# new_opt_params = bayes_parameter_opt_lgb(new_train, new_label, init_round=5, opt_round=10, n_folds=3,\n#                                      random_seed=2020, n_estimators=10000, learning_rate=0.05)","e9c167e9":"# print(new_opt_params.max['params'])","21e1d887":"new_lgb = LGBMClassifier(bagging_fraction = 0.5,\n                     feature_fraction = 0.9,\n                     lambda_l1 = 0.03,\n                     lambda_l2 = 0.10,\n                     max_depth = 9,\n                     min_child_weight = 11.20,\n                     min_split_gain = 0.001,\n                     num_leaves = 45)","8dda6e21":"val_scores = list()\noof_pred = np.zeros((test.shape[0], ))\n\nfor i, (trn_idx, val_idx) in enumerate(skf.split(new_train, new_label)):\n    x_train, y_train = new_train.iloc[trn_idx, :], new_label[trn_idx]\n    x_valid, y_valid = new_train.iloc[val_idx, :], new_label[val_idx]\n    \n    # preprocess (again)\n    scaler = StandardScaler()\n    x_train = scaler.fit_transform(x_train)\n    x_valid = scaler.transform(x_valid)\n    x_test  = scaler.transform(new_test)\n    \n    # define model (again)\n    clf = new_lgb\n    \n    \n    # model\n    new_lgb.fit(x_train, y_train,\n            eval_set = [[x_valid, y_valid]], \n            eval_metric = 'log_loss',        # \ubaa8\ub378\uc5d0 \ub530\ub77c \ud310\ub2e8 (xgbosst : xgb_f1)\n            early_stopping_rounds = 100,\n            verbose = 100,  )\n\n    # F1 Score\n    trn_f1_score = f1_score(y_train, clf.predict(x_train), average='micro')\n    val_f1_score = f1_score(y_valid, clf.predict(x_valid), average='micro')\n    print('{} Fold, train f1_score : {:.4f}4, validation f1_score : {:.4f}\\n'.format(i, trn_f1_score, val_f1_score))\n    \n    val_scores.append(val_f1_score)\n    \n    oof_pred += clf.predict_proba(x_test)[:, 1] \/ n_splits\n    \n\n# Cross Validation F1 Score\nprint('Cross Validation Score : {:.4f}'.format(np.mean(val_scores)))","2bb70d2c":"sample_submission.loc[:, 'prediction'] = (oof_pred > 0.5).astype(int)\nsample_submission.head()","92d7b31d":"sample_submission.to_csv('submission.csv', index=False)","417667be":"### Logit - Hyperparameter Tuning","247bae3d":"### XGB - Hyperparameter Tuning","cff90285":"### CatBoostClassifier - Hyperparameter Tuning","de958501":"## Stacking Ensemble","032cf9d6":"# Submission","ca9805b1":"# Model Building","a91f36da":"### Random Forest - Hyperparameter Tuning","7036eb3a":"# EDA","c185809a":"### Train-Test Comparison","aec64d6d":"# Data OverView","4fcd03ae":"#### KS test\n(H0: The two samples are drawn from the same distribution)","453fc63c":"### Target Variable Check","9908f9f3":"### LGBM Model - Bayesian Hyperparameter Tuning","1ff8e2be":"### Manny-U test\n\nH0: The distributions of populations are equal","94425c02":"### cont \/ cat variable check"}}