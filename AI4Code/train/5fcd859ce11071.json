{"cell_type":{"397b96ec":"code","ea3a3f54":"code","1c5cbcbf":"code","a12df502":"code","f6988fa1":"code","fe576719":"code","0ff78e82":"code","d759aac7":"code","d6ba0790":"code","8f8db135":"code","c62cffb8":"code","f4766246":"code","034af7ec":"code","e2c37e1a":"code","9b8f512b":"code","115e219f":"code","0c46d449":"code","81c603f6":"code","fad93dce":"code","7dd13757":"code","505a3282":"code","1e4ceb1f":"code","06a235f4":"code","36073282":"code","d9206d56":"code","46417d87":"code","45262bd2":"code","3cf61ec3":"code","f98e4626":"code","78013aa7":"code","b3b506ac":"code","5ec28ad4":"markdown","8578e6be":"markdown","687e0eff":"markdown","c1c74c38":"markdown","fd1a7445":"markdown","9b972519":"markdown","eb2def79":"markdown","cd396589":"markdown","9add3d63":"markdown","6fc99c30":"markdown","d0f3f764":"markdown","239ad94d":"markdown","53a2af13":"markdown","363b38a8":"markdown","69d40b83":"markdown","d89fa985":"markdown","25fee6b1":"markdown","1407d40c":"markdown","9228f8d2":"markdown","d204f957":"markdown","4ff773b7":"markdown","0c912515":"markdown","8c777088":"markdown"},"source":{"397b96ec":"# https:\/\/pypi.org\/project\/rouge\/\n!pip install rouge > \/dev\/null","ea3a3f54":"from rouge import Rouge \n\nhypothesis = \"Some London Underground stations should be closed, as the city is trying to reduce the impact of a coronavirus outbreak.\".lower()\n\nreference = \"Up to 40 stations on the London Underground network are to be shut as the city attempts to reduce the effect of the coronavirus outbreak.\".lower()\n\nrouge = Rouge()\nscores = rouge.get_scores(hypothesis, reference)\nscores","1c5cbcbf":"import pandas as pd\nfrom tqdm import tqdm\ntqdm.pandas()\nfrom pathlib import Path\n\nfrom matplotlib import pyplot as plt\n%config InlineBackend.figure_format = 'retina'","a12df502":"PATH_TO_CRYPTO_NEWS = Path('..\/input\/news-about-major-cryptocurrencies-20132018-40k\/')","f6988fa1":"train_df = pd.read_csv(PATH_TO_CRYPTO_NEWS \/ 'crypto_news_parsed_2013-2017_train.csv')\nvalid_df = pd.read_csv(PATH_TO_CRYPTO_NEWS \/ 'crypto_news_parsed_2018_validation.csv')","fe576719":"train_df.info()","0ff78e82":"# readling empty strings is a bit different locally and here, but not a big deal \ntrain_df['text'].fillna(' ', inplace=True)","d759aac7":"valid_df.info()","d6ba0790":"train_df.head(2)","8f8db135":"train_df['url'].nunique() == len(train_df)","c62cffb8":"train_df.loc[:5, 'url']","f4766246":"train_df['title'].apply(lambda s: len(s.split())).describe()","034af7ec":"from wordcloud import WordCloud, STOPWORDS\n\nwordcloud = WordCloud(background_color='black', stopwords = STOPWORDS,\n                max_words = 200, max_font_size = 100, \n                random_state = 17, width=800, height=400)\n\nplt.figure(figsize=(16, 12))\nwordcloud.generate(str(train_df['title']))\nplt.imshow(wordcloud);","e2c37e1a":"train_df['text'].apply(lambda s: len(s.split())).describe()","9b8f512b":"first_sentences_dumb = train_df['text'].apply(lambda s: s.split('.')[0])\nfirst_sentences_dumb.apply(lambda s: len(s.split())).describe()","115e219f":"first_ten_words_dumb = first_sentences_dumb.apply(lambda s: \" \".join(s.split()[:10]))\nfirst_ten_words_dumb.value_counts().head(20)","0c46d449":"from nltk.tokenize import sent_tokenize","81c603f6":"def extract_first_sent(text):\n    \n    sent_tok = sent_tokenize(text)\n    \n    return sent_tok[0].strip() if sent_tok else ''","fad93dce":"first_sentences = train_df['text'].progress_apply(extract_first_sent)\nfirst_sentences.apply(lambda s: len(s.split())).describe()","7dd13757":"first_ten_words = first_sentences.apply(lambda s: \" \".join(s.split()[:10]))\nfirst_ten_words.value_counts().head(20)","505a3282":"train_df['year'].value_counts()","1e4ceb1f":"valid_df['year'].value_counts()","06a235f4":"train_df['author'].nunique()","36073282":"train_df['author'].value_counts().head()","d9206d56":"train_df['source'].nunique()","46417d87":"train_df['source'].value_counts().head()","45262bd2":"true_val_titles = valid_df['title'].str.lower()","3cf61ec3":"first_sentences_val = valid_df['text'].progress_apply(extract_first_sent)\nfirst_thirty_words_val = first_sentences_val.loc[valid_df.index].apply(lambda s: \" \".join(s.split()[:30]).lower())","f98e4626":"%%time\nrouge = Rouge()\nscores = rouge.get_scores(hyps=first_thirty_words_val, refs=true_val_titles, avg=True, ignore_empty=True)","78013aa7":"scores","b3b506ac":"final_metric = (scores['rouge-1']['f'] + scores['rouge-2']['f'] + scores['rouge-l']['f']) \/ 3\nfinal_metric","5ec28ad4":"**Year**","8578e6be":"**Author**","687e0eff":"Let's perform a sanity check \u2013 whether texts actually start as normal articles and don't have any placeholders in the beggining (like timestamp), this we check simply by taking first 10 words (10 is an arbitrary choice) of each sentence and checking the number of unique values. ","c1c74c38":"**Source**\n\nThese is a feature of the actual scraping, some articles come from websites having tags in metadata (no more information on that).","fd1a7445":"**Title**\n\nThese are on avearge pretty short, the median is just 9 words","9b972519":"#### Idea\nWe are interested in exploring text summarization and, in particular, headline generation, here is a first baseline, we just take first 30 words of each text as a hypothesis for a title. \n\n#### Data\n[Kaggle Dataset](https:\/\/www.kaggle.com\/kashnitsky\/news-about-major-cryptocurrencies-20132018-40k) with ~40k articles sharing news on major cryptocurrencies. \n\n#### Task\nAll articles have `title` and `text`, the task is to generate a title given the text. The chosen metric is an avarage of ROUGE-1, ROUGE-2, and ROUGE-L, see [this report](http:\/\/www.dialog-21.ru\/media\/4661\/camerareadysubmission-157.pdf) describing the metric, page 3.\n\n#### Results\n\nROUGE scores (F1 variant):\n- ROUGE-1 \u2013 18.4%\n- ROUGE-2 \u2013 5.3%\n- ROUGE-L \u2013 16.9%\n- Average - 13.5%\n\nPretty mediocre. Hope ML models will do a better job","eb2def79":"**URL**\n\nIt is an id of a news article","cd396589":"#### Reading and briefly exploring data","9add3d63":"**Text**\n\nText are pretty long, of normal length for a news online, the median is around 400 words","6fc99c30":"#### Installing the Rouge package and playing around with the metric ","d0f3f764":"We can take a look at some of the actual articles on the Web","239ad94d":"Example of Rouge calculation","53a2af13":"Average between ROUGE-1, ROUGE-2, and ROUGE-L (the metric of interest)","363b38a8":"Let's extract first sentences of each text in a dumb way simply splitting by the dot (it's far from perfect!).\n\nFirst sentences are longer than titles, the median is 21 words, max. 232. So also makes sense to try just a part of a first sentence as a hypothesis for a title.","69d40b83":"The train-validation split is done based on year.","d89fa985":"Dunno if wordclouds have ever been useful but let's build one","25fee6b1":"That's it for the analysis, let's now create a first headline generation baseline. We saw that titles are short, up to 30 words, so we'll just use first 30 words as a hypothesis for a title.","1407d40c":"Indeed, we see some problems with taking everything before the first dot as a first sentence. \n\nPicularities of the 1st sentence:\n\n - Splitting on a dot is imperfect, thus it splits some phrases like \"Dr. Brown\"\n - Looks like there're duplicates of some news (plagiarism?), published in different sources (\"Noelle Acheson is a 10-year veteran of company analysis\" over 40 times)\n - Already fixed some commom welcome messages (\"The views and opinions expressed here are solely those of \")","9228f8d2":"https:\/\/www.ccn.com\/paris-hiltons-hotel-mogul-father-to-sell-38-million-mansion-for-cryptocurrency\/\n\n<img src=\"https:\/\/habrastorage.org\/webt\/4c\/3n\/eg\/4c3neg5owcdohooydlz4dbdwzdo.png\" width=70% \/>","d204f957":"#### Now calculating ROUGE scores for the validation part with first 30 words as hypotheses.","4ff773b7":"#### So let's try to use `sent_tokenize` to better extract first sentence","0c912515":"Now we see that first sentences are on average twice longer that those dumb ones","8c777088":"Now it's a bit better, though still not perfect\n"}}