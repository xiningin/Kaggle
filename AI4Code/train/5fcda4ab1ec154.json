{"cell_type":{"2e30636d":"code","6bf66a86":"code","6de4d5b7":"code","626da369":"code","8d979a7c":"code","b23ef612":"code","a2efd33c":"code","bc6c6c1e":"code","05a00c7a":"code","9a27a007":"code","3d520bb8":"code","ab7fa5ce":"code","b63ef9e6":"code","6688ea29":"code","16da4850":"code","b2045b8a":"code","1b0e9c82":"code","218defa4":"code","5de97a19":"code","4190e220":"code","2b407679":"code","47b093d1":"code","8bd4ae61":"markdown","7027e407":"markdown","e1d3763e":"markdown","466120f3":"markdown","8254bf0d":"markdown","5382ee64":"markdown","5757d338":"markdown","9885729f":"markdown","39e1c625":"markdown","6206cea2":"markdown","62e33ddd":"markdown","064aa57d":"markdown","eaf72e73":"markdown","5e3281e9":"markdown","abdb7b19":"markdown","58df99e4":"markdown","f9b490e2":"markdown","833a1f40":"markdown","06deae10":"markdown","9ee48775":"markdown","a18298b3":"markdown","8795abed":"markdown","2f3f24c1":"markdown","8afb9b11":"markdown","cc03ae13":"markdown","c50e0f60":"markdown","313459eb":"markdown","90599973":"markdown","4e76fa14":"markdown","46e067b6":"markdown","63ab1297":"markdown"},"source":{"2e30636d":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score","6bf66a86":"df_train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\ndisplay(df_train)","6de4d5b7":"df_train.info()","626da369":"# counting NaN values\n\nNaN = df_train.isna().sum()\nprint(NaN)","8d979a7c":"# how many total missing values do we have?\ntotal_cells = np.product(df_train.shape)\ntotal_missing = NaN.sum()\n\n# percent of data that is NaN\nprint((total_missing\/total_cells)*100,\"%\")","b23ef612":"df_train = df_train.drop(['PassengerId','Name','Ticket','Age','Cabin'],axis=1)\ndf_train.dropna(subset=['Embarked'], inplace=True)\n\n# counting NaN values again\ntotal_NaN = df_train.isna().sum().sum()\nprint(total_NaN)","a2efd33c":"# visualising the data after getting rid of redundant features\n\nfig, axs = plt.subplots(ncols=4, nrows=2, figsize=(20,10))\nindex = 0\naxs = axs.flatten()\nfor k,v in df_train.items():\n    sns.histplot(x=k, data=df_train, ax=axs[index], facecolor='pink', edgecolor='black')\n    index += 1\nplt.tight_layout(pad=0.4, w_pad=0.5, h_pad=5.0)","bc6c6c1e":"plt.figure(figsize=(12, 6))\nsns.heatmap(df_train.corr().abs(), annot=True);","05a00c7a":"# rewriting data as int values since some algorithms can't convert string to float\n\ndf_train = df_train.replace(['male'],0)\ndf_train = df_train.replace(['female'],1)\n\ndf_train = df_train.replace(['C'],0)\ndf_train = df_train.replace(['Q'],1)\ndf_train = df_train.replace(['S'],2)","9a27a007":"# splitting the data\n\nfrom sklearn.model_selection import train_test_split\nlabels = df_train['Survived']\nfeatures = df_train.drop('Survived',axis=1)\nfeatures_train,features_test,labels_train,labels_test = train_test_split(features,labels,test_size=0.1)","3d520bb8":"from sklearn.naive_bayes import GaussianNB\nclf = GaussianNB()\nclf.fit(features_train,labels_train)\npred = clf.predict(features_test)\nscore1 = accuracy_score(pred,labels_test)*100\nprint(score1)","ab7fa5ce":"from sklearn import svm\nclf = svm.SVC()\nclf.fit(features_train,labels_train)\npred = clf.predict(features_test)\nscore2 = accuracy_score(pred,labels_test)*100\nprint(score2)","b63ef9e6":"from sklearn import tree\nclf = tree.DecisionTreeClassifier()\nclf.fit(features_train,labels_train)\npred = clf.predict(features_test)\nscore3 = accuracy_score(pred,labels_test)*100\nprint(score3)","6688ea29":"from sklearn.ensemble import RandomForestClassifier\n\nclf = RandomForestClassifier()\nclf.fit(features_train,labels_train)\npred = clf.predict(features_test)\nscore4 = accuracy_score(pred,labels_test)*100\nprint(score4)","16da4850":"print(\"Accuracy of Naive Bayes =\", score1)\nprint(\"Accuracy of SVM =\", score2)\nprint(\"Accuracy of Decision Tree =\", score3)\nprint(\"Accuracy of Random Forest =\", score4)","b2045b8a":"display(df_test)","1b0e9c82":"df_test.info()","218defa4":"NaN = df_test.isna().sum()\nprint(NaN)","5de97a19":"df_test['Fare'].fillna(7.0,inplace=True)\nNaN = df_test.isna().sum()\nprint(NaN)","4190e220":"# rewriting data as int values since some algorithms can't convert string to float\n\ndf_test = df_test.replace(['male'],0)\ndf_test = df_test.replace(['female'],1)\n\ndf_test = df_test.replace(['C'],0)\ndf_test = df_test.replace(['Q'],1)\ndf_test = df_test.replace(['S'],2)","2b407679":"clf = RandomForestClassifier()\nclf.fit(features,labels)\npred = clf.predict(df_test.drop(['PassengerId','Name','Ticket','Age','Cabin'],axis=1))\nprint(pred)","47b093d1":"output = pd.DataFrame(df_test['PassengerId'],columns = ['PassengerId'])\noutput['Survived'] = pd.DataFrame(pred,columns = ['Survived'])\n\noutput.to_csv('TitanicPredicted.csv', index=False)\noutput","8bd4ae61":"<h4>2.2 Visualising each feature (histplot)<\/h4>","7027e407":"<h4>2.3 Looking for correlations (heatmap)<\/h4>","e1d3763e":"Only around **8%** of the total data is missing, so it's probably safe to drop it.","466120f3":"# Titanic - Machine Learning from Disaster","8254bf0d":"\ud83d\udccc *This is my first independent project so there may be mistakes since I'm just a beginner :')*","5382ee64":"No more NaN values!","5757d338":"Let's load and display the training dataset.","9885729f":"I will divide the entire process into four parts :\n1. Datasets\n2. Features\n3. Algorithm\n4. Output","39e1c625":"<h4>2.1 Removing redundant columns and missing values<\/h4>","6206cea2":"The dataset contains 12 columns, out of which :\n1. First is `PassengerId`. \n2. Second is `Survived` which has two values - 1 for survived, 0 for deceased.\n3. The rest of the columns contain information about the passenger.\n\nHere, `Survived` is the target variable.\n\nThere are 891 rows. It can also be seen that the dataset contains some NaN values.","62e33ddd":"`Age`, `Cabin` and `Embarked` have NaN values. Since `Embarked` has only two, we will remove them later.\n\nLet's find out the percentage of the dataset that is missing.","064aa57d":"The data is labelled, and is unordered\/discrete. Hence we will try <font color=darkviolet><b>Naive Bayes<\/b><\/font>, <font color=darkviolet><b>Decision Tree<\/b><\/font>, <font color=darkviolet><b>SVM<\/b><\/font> and <font color=darkviolet><b>Random Forest<\/b><\/font>.\n\nBefore creating the models, let's divide our training set into training (90%) and testing (10%) sets.","eaf72e73":"Some libraries will be imported later on in this notebook, as and when required.","5e3281e9":"<h2>2. The Features<\/h2>","abdb7b19":"<h4>4.2 Predicting with Random Forest classifier<\/h4>","58df99e4":"**Notes :**\n<blockquote><code>Pclass<\/code> - Ticket class (1 = Upper, 2 = Middle, 3 = Lower) <br>\n    <code>Age<\/code> - Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5 <br>\n    <code>SibSp<\/code> - No. of siblings\/spouses aboard <br>\n    <code>Parch<\/code> - No. of parents\/children aboard (0 = Children travelling with a nanny) <br>\n    <code>Embarked<\/code> - Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton) <\/blockquote>","f9b490e2":"<h2>4. The Output<\/h2>","833a1f40":"<h4>3.3 Decision Tree<\/h4>","06deae10":"<h4>4.3 Creating the output csv file<\/h4>","9ee48775":"Looking at the data, it is obvious that `PassengerId`, `Ticket` and `Name` will not be useful in training the model. Furthermore, the columns with NaN values will create problems while training the model. Hence these columns will be dropped.\n\nAs stated earlier, we will also remove the rows which have missing values under `Embarked`.","a18298b3":"Right now, the testing set has features and NaN values we'll have to remove, since they won't let the model make predictions. Let's find out which of these columns have missing values.","8795abed":"<h4>4.1 Preparing the testing dataset<\/h4>","2f3f24c1":"<h4>3.4 Random Forest<\/h4>","8afb9b11":"It can be seen that none of the columns are really related as such. `Pclass` (ticket class) and `Fare` do have some correlation which is obvious. The target variable `Survived` is most closely related to `Pclass`, but the correlation is not very strong (only 0.34).","cc03ae13":"<h4>3.2 SVM<\/h4>","c50e0f60":"<h2>3. The Algorithm<\/h2>","313459eb":"<h3>Importing Libraries<\/h3>","90599973":"In most cases, <font color=darkviolet>Random Forest<\/font> gives the best accuracy. At times some of the algorithms might give similar accuracies<b>*<\/b>, while <font color=darkviolet>SVM<\/font> gives the worst everytime. Here, we will use <b>Random Forest<\/b>. We will train it again with the full training dataset and then make predictions.\n\n<i>* this is because stochastic algorithms use randomness during learning, which ensures that a different model is trained each run<\/i>","4e76fa14":"<h2>1. The Datasets<\/h2>","46e067b6":"Now, `Age`, `Fare` and `Cabin` has NaN values. We will not focus on `Age` and `Cabin`, since they will be dropped while training the model. `Fare` has only one NaN value, so we will fill it in.","63ab1297":"<h4>3.1 Naive Bayes<\/h4>"}}