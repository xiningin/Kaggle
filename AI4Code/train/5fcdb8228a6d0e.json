{"cell_type":{"8d13b0a2":"code","36bafe8c":"code","a75d0792":"code","3b0eac44":"code","c6a3986b":"code","43cfe904":"code","a1c0058c":"code","f0505ef7":"code","71ca276a":"code","40b6dce1":"code","6823fd92":"code","b41c4c98":"code","36bbac4a":"markdown","f883cad2":"markdown"},"source":{"8d13b0a2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","36bafe8c":"train = pd.read_csv(\"..\/input\/learn-together\/train.csv\")\ntest = pd.read_csv(\"..\/input\/learn-together\/test.csv\")","a75d0792":"#splitting the training set into training and validation subsets\n\nfrom sklearn.model_selection import train_test_split\n\n# Split into validation and training data, set to random_state 1\ntrain_set, validation_set = train_test_split(train, test_size = 0.20, random_state = 1)\n\ny_train = train_set.Cover_Type\ny_validation = validation_set.Cover_Type\n\ntrain_set.drop(['Cover_Type'], axis = 1, inplace = True)\nvalidation_set.drop(['Cover_Type'], axis = 1, inplace = True)","3b0eac44":"#. choosing classifier\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(random_state = 0)\nclassifier.fit(train_set, y_train)\ny_train_predicted_basic = classifier.predict(train_set)\ny_validation_predicted_basic = classifier.predict(validation_set)","c6a3986b":"#checking accuracy\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import accuracy_score\n\n\nscores_rf_training_basic = cross_val_score(classifier, train_set, y_train, cv=10, scoring='accuracy')\naccuracy_training_basic = accuracy_score(y_train_predicted_basic,y_train)\n\nscores_rf_validation_basic = cross_val_score(classifier, validation_set, y_validation, cv=10, scoring='accuracy')\naccuracy_validation_basic = accuracy_score(y_validation_predicted_basic,y_validation)\n# Get the mean accuracy score\n\nprint(\"cross_val_score on basic RF (train): \", scores_rf_training_basic.mean())\nprint(\"cross_val_score on basic RF (validation): \", scores_rf_validation_basic.mean())\nprint(\"accuracy score train: \", accuracy_training_basic)\nprint(\"accuracy score validation: \", accuracy_validation_basic)\n\n","43cfe904":"classifier.get_params().keys()","a1c0058c":"# optimizing the parameters - Grid Search \n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\n\"\"\"\nfrom sklearn.model_selection import GridSearchCV\nparameters = [{'n_estimators': [1,10,100,200,300,400]},\n             {'n_estimators': [1,10,100,200,300,400],'max_features': ['sqrt']},\n             {'n_estimators': [1,10,100,200,300,400], 'bootstrap': [False],'max_features': ['sqrt']},\n             {'n_estimators': [1,10,100,200,300,400],'max_features': ['log2']},\n             {'n_estimators': [1,10,100,200,300,400], 'bootstrap': [False],'max_features': ['log2']}\n             ]\ngrid_search = GridSearchCV(estimator = classifier, param_grid = parameters, cv = 10, n_jobs = -1)\ngrid_search = grid_search.fit(train_set, y_train)\nbest_accuracy = grid_search.best_score_\nbest_parameters = grid_search.best_params_\nprint(\"best_accuracy: \", best_accuracy, \"\\nbest_parameters: \", best_parameters)\"\"\"\n","f0505ef7":"# make predictions using the model\n\nfinal_classifier = RandomForestClassifier(bootstrap = False, max_features = 'sqrt', n_estimators = 300, random_state = 0)\n\nfinal_classifier = final_classifier.fit(train_set, y_train)\npredictions_test = final_classifier.predict(test)\n\n","71ca276a":"#Gradient Boosting - XGBoost classifier\n\"\"\"X_train = train_set\ny_train = y_train\nX_test = validation_set\ny_test = y_validation\n\nfrom xgboost import XGBClassifier\nimport time\n\nxgb = XGBClassifier(n_estimators=100)\ntraining_start = time.perf_counter()\nxgb.fit(X_train, y_train)\ntraining_end = time.perf_counter()\nprediction_start = time.perf_counter()\npreds = xgb.predict(X_test)\nprediction_end = time.perf_counter()\nacc_xgb = (preds == y_test).sum().astype(float) \/ len(preds)*100\nxgb_train_time = training_end-training_start\nxgb_prediction_time = prediction_end-prediction_start\nprint(\"XGBoost's prediction accuracy is: %3.2f\" % (acc_xgb)) #76.49\nprint(\"Time consumed for training: %4.3f\" % (xgb_train_time)) #14.595\nprint(\"Time consumed for prediction: %6.5f seconds\" % (xgb_prediction_time)) #0.07857 seconds\"\"\"\n\n","40b6dce1":"#Permutation importance\n# https:\/\/eli5.readthedocs.io\/en\/latest\/blackbox\/permutation_importance.html\n\nimport eli5\nfrom eli5.sklearn import PermutationImportance\n\nperm = PermutationImportance(final_classifier, random_state = 1).fit(train_set,y_train)\neli5.show_weights(perm, feature_names = train_set.columns.tolist())\n\n\"\"\"\nWeight \tFeature\n0.2315 \u00b1 0.0043 \tElevation\n0.0435 \u00b1 0.0025 \tHorizontal_Distance_To_Roadways\n0.0229 \u00b1 0.0010 \tId\n0.0162 \u00b1 0.0011 \tHorizontal_Distance_To_Hydrology\n0.0138 \u00b1 0.0019 \tHorizontal_Distance_To_Fire_Points\n0.0108 \u00b1 0.0002 \tSoil_Type3\n0.0101 \u00b1 0.0008 \tSoil_Type10\n0.0065 \u00b1 0.0006 \tSoil_Type39\n0.0051 \u00b1 0.0009 \tSoil_Type38\n0.0030 \u00b1 0.0002 \tSoil_Type4\n0.0018 \u00b1 0.0004 \tWilderness_Area4\n0.0014 \u00b1 0.0004 \tVertical_Distance_To_Hydrology\n0.0006 \u00b1 0.0003 \tHillshade_9am\n0.0005 \u00b1 0.0004 \tAspect\n0.0004 \u00b1 0.0002 \tHillshade_Noon\n0.0003 \u00b1 0.0001 \tSoil_Type35\n0.0003 \u00b1 0.0001 \tSoil_Type2\n0.0002 \u00b1 0.0000 \tSoil_Type12\n0.0002 \u00b1 0.0002 \tSoil_Type40\n0.0002 \u00b1 0.0001 \tSoil_Type30\n\u2026 35 more \u2026\n\"\"\"","6823fd92":"from sklearn.feature_selection import SelectFromModel\n\n# ... load data\n\n\n# perm.feature_importances_ attribute is now available, it can be used\n# for feature selection - let's e.g. select features which increase\n# accuracy by at least 0.01:\nsel = SelectFromModel(perm, threshold=0.01, prefit=True)\nX_trans = sel.transform(train_set)\n","b41c4c98":"#creating output file\noutput = pd.DataFrame({'Id': test.Id,\n                       'Cover_Type': predictions_test})\noutput.to_csv('rf_basic.csv', index=False)\n","36bbac4a":"* Permutation Importance","f883cad2":"* Feature Selection"}}