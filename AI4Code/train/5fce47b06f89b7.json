{"cell_type":{"58c9f6aa":"code","6a36e847":"code","423ff176":"code","1c0b37ba":"code","f68e9b65":"code","6aeea4f4":"code","8ec5d0ef":"code","bba186f6":"code","a86ebaff":"code","8f08d7fc":"code","6de83bd8":"code","8115f825":"code","87e51286":"code","d2ca2ec0":"code","f8524ae6":"code","d38f1bf4":"code","bcff0da3":"code","21941de9":"code","42ed5694":"code","2520d432":"code","8dd43321":"code","782e8b64":"code","d139cca7":"code","d8c808db":"code","b2a32ff8":"code","d9e0e397":"code","64c4b363":"code","be5d5f55":"code","715ef8c5":"code","de524381":"code","44cc6a60":"markdown","bf64f580":"markdown"},"source":{"58c9f6aa":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\n\nimport missingno as mno\nfrom sklearn import linear_model\n\nimport os\nprint(os.listdir(\"..\/input\"))","6a36e847":"partials = list()\nstations = list()\n\nwith pd.HDFStore('..\/input\/madrid.h5') as data:\n    stations = [k[1:] for k in data.keys() if k != '\/master']\n    for station in stations:\n        df = data[station]\n        df['station'] = station\n        partials.append(df)","423ff176":"measures = pd.concat(partials, sort=False).sort_index()\nmeasures.head()","1c0b37ba":"measures.describe()","f68e9b65":"measures.isnull().sum().apply(lambda x: (x \/ len(measures) * 100))","6aeea4f4":"mno.matrix(measures, figsize = (24, 8))","8ec5d0ef":"plt.figure(figsize=(12,12))\nsns.heatmap(measures.drop('station', axis=1).corr(), square=True, annot=True, cmap='rainbow')","bba186f6":"# get list of unique timestamps\nRANDOM_STATION = '28079017'\none_station = measures[measures['station'] == RANDOM_STATION]\ntimestamps = one_station.index.to_series()","a86ebaff":"# only for EDA purposes to understand what we're dealing with\n# for station in stations:\n#     print(\"Station: {}\".format(station))\n#     total_observations = len(measures[measures['station'] == station])\n#     print(\"Total observations per station: {}\".format(total_observations))\n#     print(measures[measures['station'] == station].notnull().sum().apply(lambda x: (x \/ total_observations * 100)))","8f08d7fc":"column_to_check = 'NMHC'\nlist_of_dfs = list()\nfor station in stations:\n    station_data = measures[measures['station'] == station]\n    total_observations = len(station_data)\n#     print(\"Total observations per station: {}\".format(total_observations))\n#     print(station_data[column_to_check].notnull().sum())\n    if station_data[column_to_check].notnull().sum() > 0:\n        list_of_dfs.append(station_data[['station',column_to_check]])\n\ndf_nmhc = pd.concat(list_of_dfs)","6de83bd8":"f = plt.figure(figsize=(30,8))\nsns.scatterplot(x='date', y='NMHC', data=df_nmhc.reset_index())","8115f825":"f = plt.figure(figsize=(30,8))\nsns.boxplot(x='station', y='NMHC', data=df_nmhc.dropna(axis=0).reset_index())","87e51286":"f = plt.figure(figsize=(30,8))\nsns.violinplot(x='station', y='NMHC', data=df_nmhc.dropna(axis=0).reset_index())","d2ca2ec0":"df_nmhc_8h = df_nmhc.dropna(axis=0).groupby('station').resample('8H').mean().reset_index()\ndf_nmhc_8h.head()","f8524ae6":"f = plt.figure(figsize=(30,16))\nax = f.add_subplot(3,1,1)\nsns.scatterplot(x='date', y='NMHC', data=df_nmhc_8h, ax=ax)\nax = f.add_subplot(3,1,2)\npalette = sns.color_palette(\"rainbow\", 16)\nsns.boxplot(x='station', y='NMHC', data=df_nmhc_8h, palette=palette, ax=ax)\nax = f.add_subplot(3,1,3)\npalette = sns.color_palette(\"rainbow\", 16)\nsns.violinplot(x='station', y='NMHC', data=df_nmhc_8h, palette=palette)\n","d38f1bf4":"df_nmhc_year = df_nmhc.dropna(axis=0).resample('8H').mean()\ndf_nmhc_year['year'] = df_nmhc_year.index.year\ndf_nmhc_year.reset_index(inplace=True)","bcff0da3":"f = plt.figure(figsize=(30,16))\nax = f.add_subplot(2,1,1)\npalette = sns.color_palette(\"rainbow\", 16)\nsns.boxplot(x='year', y='NMHC', data=df_nmhc_year, palette=palette, ax=ax)\nax = f.add_subplot(2,1,2)\npalette = sns.color_palette(\"rainbow\", 16)\nsns.violinplot(x='year', y='NMHC', data=df_nmhc_year, palette=palette)","21941de9":"nan_columns = ['CO']","42ed5694":"def random_imputation(df, feature):\n\n    number_missing = df[feature].isnull().sum()\n    observed_values = df.loc[df[feature].notnull(), feature]\n    df.loc[df[feature].isnull(), feature + '_imp'] = np.random.choice(observed_values, number_missing, replace = True)\n    \n    return df","2520d432":"co_mean = measures['CO'].groupby('date').mean()","8dd43321":"type(co_mean)","782e8b64":"df_mean = pd.DataFrame(data=co_mean, columns=['CO'])","d139cca7":"# group data by different time periods\ndf_gr_D = df_mean.groupby(pd.Grouper(freq='D')).transform(np.mean).resample('D').mean()\ndf_gr_M = df_gr_D.groupby(pd.Grouper(freq='M')).transform(np.mean).resample('M').mean()","d8c808db":"# prepare final dataset\ndf_detailed = df_gr_D.copy()\ndf_detailed['year'] = df_detailed.index.year\ndf_detailed['month'] = df_detailed.index.month\ndf_detailed['day'] = df_detailed.index.day\ndf_detailed.head()","b2a32ff8":"# finding max and min values per each month\nmax_vals_df = pd.DataFrame(columns=['year', 'month', 'volume'])\nMIN_YEAR = min(df_detailed['year'])\nMAX_YEAR = max(df_detailed['year'])\nfor i in range(MIN_YEAR, MAX_YEAR+1):\n    max_val = max(df_detailed['CO'][df_detailed['year'] == i])\n    if max_val > 0.0:\n        month = df_detailed[(df_detailed['CO'] == max_val) & (df_detailed['year'] == i)]['month'].values[0]\n        to_add = pd.DataFrame([[i, month, max_val]], columns=['year', 'month', 'volume'])\n        max_vals_df = max_vals_df.append(to_add)\n\nmin_vals_df = pd.DataFrame(columns=['year', 'month', 'volume'])\nfor i in range(MIN_YEAR, MAX_YEAR+1):\n    min_val = min(df_detailed['CO'][df_detailed['year'] == i])\n    if min_val > 0.0:\n        month = df_detailed[(df_detailed['CO'] == min_val) & (df_detailed['year'] == i)]['month'].values[0]\n        to_add = pd.DataFrame([[i, month, min_val]], columns=['year', 'month', 'volume'])\n        min_vals_df = min_vals_df.append(to_add)","d9e0e397":"max_vals_df['date'] = pd.to_datetime(max_vals_df.year.astype('str')+\"-\"+max_vals_df.month.astype('str'), format=\"%Y-%m\")\nmax_vals_df = max_vals_df.set_index('date', drop=True)\nmax_vals_df.drop(['year', 'month'], axis=1, inplace=True)\nmax_vals_df","64c4b363":"min_vals_df['date'] = pd.to_datetime(min_vals_df.year.astype('str')+\"-\"+min_vals_df.month.astype('str'), format=\"%Y-%m\")\nmin_vals_df = min_vals_df.set_index('date', drop=True)\nmin_vals_df.drop(['year', 'month'], axis=1, inplace=True)\nmin_vals_df","be5d5f55":"# really useful function\ndef zero_to_nan(values):\n    \"\"\"Replace every 0 with 'nan' and return a copy.\"\"\"\n    return [float('nan') if x == 0 else x for x in values]\n\n\ndf_detailed['CO'] = zero_to_nan(df_detailed['CO'])","715ef8c5":"# plot 1 - difference between max and min values through time\nf = plt.figure(figsize=(18,6))\nax = f.add_subplot(2,1,1)\nsns.lineplot(data = min_vals_df.reset_index(), x='date', y='volume')\nplt.xlabel('')\nplt.ylabel('MIN CO pollution, mg\/m3')\nax = f.add_subplot(2,1,2)\nsns.lineplot(data = max_vals_df.reset_index(), x='date', y='volume')\nplt.xlabel('Year')\nplt.ylabel('MAX CO pollution, mg\/m3')","de524381":"# plot 2 - changes in air pollution through years\nyears = []\nfor i in range(MIN_YEAR, MAX_YEAR+1, 2):\n    years.append(i)\n\nf = plt.figure(figsize=(18,6))\nplt.plot(df_detailed.index, df_detailed['CO'], c='r')\nplt.xlabel('Year')\nplt.ylabel('mg\/m3')\nplt.title('CO pollution in Madrid 2001-2018')","44cc6a60":"And yes, we see a lot of outliers that can indicate particular events in the city that led to these measures that particular day\/hour. Let's group our data into 8-hours sets and see how will it look.","bf64f580":"We already see lots of suspicious records that might be outliers and distort the visualization. Let's check it with boxplot and violin plot to understand how really outliers influence the data."}}