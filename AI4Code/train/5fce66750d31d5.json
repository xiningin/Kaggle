{"cell_type":{"31745dbc":"code","aaf5a1c3":"code","f2c13106":"code","12f82442":"code","cf78710b":"code","9962794a":"code","637eeeec":"code","743a3b3b":"code","e4fef7fa":"code","4a3f26a1":"code","083f4cb9":"code","d6eb85d9":"code","c9d9da2a":"code","dbb6963a":"code","8c2740b9":"code","5fd84a37":"code","eff10ead":"code","23937c71":"code","5701e417":"code","fca9b10e":"code","b7208cf4":"markdown","374ec1d9":"markdown","f8c065c3":"markdown","e3935941":"markdown","244569c1":"markdown","a3241e6b":"markdown","8fcb19e2":"markdown","2cbf746c":"markdown","1b7696fa":"markdown","e40c3f6f":"markdown","1a85ae14":"markdown","f5cc5163":"markdown","885ea132":"markdown"},"source":{"31745dbc":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport statsmodels.api as sm\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.feature_selection import f_regression, SelectKBest\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA","aaf5a1c3":"df = pd.read_csv('\/kaggle\/input\/graduate-admissions\/Admission_Predict.csv')\ndf = df.iloc[:, 1:]\ndf.head()","f2c13106":"X = df.iloc[:, :-1]\ny = df.iloc[:, -1]","12f82442":"# shape of the dataset\ndf.shape","cf78710b":"# categorical and numeric features\nnumerical = ['GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR ', 'CGPA']\ncategorical = ['Research']","9962794a":"with plt.style.context('seaborn-whitegrid'):\n    plt.figure(figsize=(10, 6))\n    sns.histplot(y, binwidth=0.05, binrange=(0.3, 1), kde=True)\n    plt.title('Distribution of Target')","637eeeec":"# using power transformation to target distribution normal\nplt.hist(df['Chance of Admit ']**2)\nplt.show()","743a3b3b":"with plt.style.context('seaborn-whitegrid'):\n    fig, axes = plt.subplots(2, 3, figsize=(9, 6))\n    plt.subplots_adjust(wspace=0.4, hspace=0.4)\n    axes = axes.ravel()\n    for ax, f in zip(axes, numerical):\n        sns.histplot(data=df, x=f, ax=ax)\n    fig.suptitle('Distribution Numerical Features', fontsize=20)","e4fef7fa":"plt.figure(figsize=(10, 6))\nsns.heatmap(df.iloc[:, :-1].corr(), vmin=-1, vmax=1, annot=True, cmap='PiYG')\nplt.title('Linear Correlation', fontsize=15)\nplt.show()","4a3f26a1":"df.corr()['Chance of Admit ']","083f4cb9":"with plt.style.context('ggplot'):\n    plt.figure(figsize=(5, 5))\n    sns.countplot(data=df, x='Research', edgecolor='black')\n    plt.yticks(np.arange(0, 226, 25))","d6eb85d9":"sns.boxplot(data=df, x='Research', y=y)","c9d9da2a":"features = pd.get_dummies(df.iloc[:, :-1], columns=['Research'])\ntarget = df.iloc[:, -1]\n\n# regression analysis\nmod = sm.OLS(target, sm.add_constant(features))\nres = mod.fit()\nprint(res.summary())","dbb6963a":"from sklearn.model_selection import train_test_split\n\nX = df.iloc[:, :-1]\ny = df.iloc[:, -1]**2\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=2021)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","8c2740b9":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import MinMaxScaler, OneHotEncoder\n\nnumeric_transformer = Pipeline([\n    ('scale', MinMaxScaler())\n])\n\ncategorical_transformer = Pipeline([\n    ('onehot', OneHotEncoder())\n])\n\npreprocessor = ColumnTransformer([\n    ('num', numeric_transformer, numerical),\n    ('cat', categorical_transformer, categorical)\n])","5fd84a37":"lr_pipe = Pipeline([\n    ('pre', preprocessor), \n    ('PCA', PCA(n_components=2, random_state=2021)),\n    ('lr', LinearRegression(n_jobs=-1))\n])\n\nlr_pipe.fit(X_train, y_train)\npred = lr_pipe.predict(X_test)\nprint(mean_absolute_error(y_test, pred))","eff10ead":"plt.scatter(np.array(y_test), pred)","23937c71":"plt.hist(y_test - pred)","5701e417":"pca_pipe = Pipeline([\n    ('pre', preprocessor),\n    #('tsne', TSNE(n_components=2, perplexity=30.0, random_state=2021)),\n    ('PCA', PCA(n_components=2, random_state=2021)),\n])\n\nX_pca = pca_pipe.fit_transform(X_train)\n# % total variance explained after dim reduction using Pca\nprint(np.sum(pca_pipe['PCA'].explained_variance_)*100)","fca9b10e":"px.scatter(x=X_pca[:, 0], y=X_pca[:, 1], color=y_train)","b7208cf4":"- model indicates high multicollinearity with features\n- model explains 80 % variation in the target variable","374ec1d9":"- Outliers can be observed","f8c065c3":"- I have taken Muti-collinearity threshold as 0.8\n- Multi-collinearity can be observed with few features like TOEFL, GRE and  CGPA. ","e3935941":"# Data Exploration and EDA","244569c1":"- the residual histogram is not normally distributed indicating anomalies in the dataset","a3241e6b":"# Import Libraries","8fcb19e2":"# Anomaly detection","2cbf746c":"# Regression Analysis","1b7696fa":"# Data Preprocessing Pipeline","e40c3f6f":"# Regression","1a85ae14":"- The target is slighly negative or left skewed.","f5cc5163":"# Data Preparation","885ea132":"# Load Data"}}