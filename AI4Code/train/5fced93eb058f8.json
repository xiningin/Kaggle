{"cell_type":{"4e158919":"code","5bcd78f5":"markdown","f72b7626":"markdown"},"source":{"4e158919":"# numpy.linalg.eig module takes in a square matrix as the input and returns eigen values and eigen vectors.\n# It also raises an LinAlgError if the eigenvalue computation does not converge.\n\nimport numpy as np\nfrom numpy import linalg as LA\n\ninput = np.array([[2,-1],[4,3]])\n\nw, v = LA.eig(input)","5bcd78f5":"### <u> Calculate Eigenvalues and Eigenvectors In Python <\/u>","f72b7626":"# <u> Eigenvalues and Eigenvectors <\/u>\n\n### <u> Eigenvector <\/u>\nEvery vector (list of numbers) has a direction when it is plotted on XY chart. Eigenvectors are those vectors when a linear transformation (such as multiplying it to a scalar) is performed on them, their direction does not change.\n\n### <u>Eigenvalue<\/u>\u200a\nThe scalar that is used to transform (stretch\/compress) an Eigenvector.\n    \nEigenvectors and eigenvalues are used to reduce noise in data. They can help us improve efficiency in computationally intensive tasks. They also eliminate features that have a strong correlation between them and also help in reducing over-fitting.\n\n### <u>Use case - Eigenvalues and Eigenvectors<\/u>\n\nWhen we are building forecasting models that are trained on images, sound and\/or textual contents then the input feature sets can end up having a large set of features. It is also difficult to understand and visualize data with more than 3 dimensions. As a result, we often use one-hot encoding to transform values in textual features to separate numerical columns which can end up taking a large amount of space on a disk. Component analysis is one of the key strategies that is utilised to reduce dimension space without losing valuable information. <b> The core of component analysis (PCA) is built on the concept of eigenvalues and eigenvectors. <\/b>\n\nEigenvalues and Eigenvectors have their importance in linear differential equations where we want to find a rate of change or when we want to maintain relationships between two variables.\n\nAdditionally, eigenvectors and eigenvalues are used in facial recognition techniques such as EigenFaces.\n\nOccasionally we gather data that contains a large amount of noise. Finding important or meaningful patterns within the data can be extremely difficult. Eigenvectors and eigenvalues can be used to construct spectral clustering.\n\nWe can also use eigenvector to rank items in a dataset.\n\nLastly, in non-linear motion dynamics, eigenvalues and eigenvectors can be used to help us understand the data better as they can be used to transform and represent data into manageable sets.\n\n<i>Eigenvalues and Eigenvectors provides Summary of a large matrix<\/i>\n\n### <u>What are Eigenvalues and Eigenvectors?<\/u>\n\nEigenvectors are used to make linear transformation understandable. Think of eigenvectors as stretching\/compressing an X-Y line chart without changing its direction.\n\n<i>Eigenvectors and eigenvalues revolve around the concept of matrices.<\/i>\n\nMatrices are used in machine learning problems to represent a large set of information. Eigenvalues and eigenvectors is about constructing one vector with one value to represent a large matrix.\n\nSuppose we have a square matrix $R^n$ = $\\begin{bmatrix}\n       A & B & C \\\\[0.3em]\n       D & E & F \\\\[0.3em]\n       G & H & I\n     \\end{bmatrix}$\n     \n- The eigenvector is an array with n entries where n is the number of rows\/ columns of a square matrix. The eigenvector is represented as $x$ <b>The direction of an eigenvector does not change when a linear transformation is applied to it. <\/b>\n- Therefore, Eigenvector should be a non-null vector.\n- <b>Eigenvalues <\/b>: We are required to find a number of values, known as eigenvalues($\\lambda$) such that $A  x - \\lambda  x = 0$\n\nThe above equation states that we need to multiply a scalar $\\lambda$ (eigenvalue) to the vector $x$ such that it is equal to the linear transformation of matrix A once it is scaled by vector $x$ (eigenvector).\n\n- $A  x - \\lambda  x = 0$ $\\implies$ $(A - \\lambda  I)  x = 0$\n\n- <b>The equation $A  x - \\lambda  x = 0$ should not be invertible. i.e. $Determinant(A - \\lambda  I) = 0$<\/b>\n\n### <u>Example<\/u>\n\nFor a matrix A of size n, find Eigenvalues of size n.\n\nAim: Find Eigenvector and Eigenvalues of A such that: $A  x - \\lambda  x = 0$ where (x = Eigenvector and $\\lambda$ = Eigenvalues)\n\n<b>Find $\\lambda$ Such that Determinant(A \u2014 $\\lambda$ I) = 0<\/b>\n     \nLet $A = \\begin{bmatrix}\n       A & B & C \\\\[0.3em]\n       D & E & F \\\\[0.3em]\n       G & H & I\n     \\end{bmatrix}$\n\n$$A \u2014 \\lambda  I = \n\\begin{bmatrix}\nA & B & C \\\\[0.3em]\nD & E & F \\\\[0.3em]\nG & H & I\n\\end{bmatrix}\n-\n\\begin{bmatrix}\n\\lambda & 0 & 0 \\\\[0.3em]\n0 & \\lambda & 0 \\\\[0.3em]\n0 & 0 & \\lambda\n\\end{bmatrix}\n=\n\\begin{bmatrix}\nA - \\lambda & B & C \\\\[0.3em]\nD & E - \\lambda & F \\\\[0.3em]\nG & H & I - \\lambda\n\\end{bmatrix}\n$$\n \n$$Determinant(A \u2014 \\lambda I) = \n(A - \\lambda) \\begin{bmatrix}\nE - \\lambda & F\\\\[0.3em]\nH & I - \\lambda\n\\end{bmatrix}\n- B\\begin{bmatrix}\nD & F\\\\[0.3em]\nG & I - \\lambda\n\\end{bmatrix}\n+ C\\begin{bmatrix}\nD & E - \\lambda\\\\[0.3em]\nG & H\n\\end{bmatrix}\n=0\n$$\n\n<b>Once we solve the equation above, we will get the values of Lambda. These values are the Eigenvalues.<\/b>\n\n\nOnce we have calculated eigenvalues, we can calculate the Eigenvector for each of the Eigenvalues of the matrix A by using Gaussian Elimination. Gaussian elimination is about converting the matrix to row echelon form. Finally it is about solving the linear system by back substitution.\n\nIf a square matrix has a size $R^n$ we will get $n$ eigenvalues and $n$ eigenvectors.\n\n### <u>Example<\/u>\nLet $A = \n\\begin{bmatrix}\n2 & -1 \\\\[0.3em]\n4 & 3\n\\end{bmatrix}$\n\n$$ A \u2014 \\lambda  I = \n\\begin{bmatrix}\n2 & -1 \\\\[0.3em]\n4 & 3\n\\end{bmatrix}\n-\\lambda\n\\begin{bmatrix}\n1 & 0 \\\\[0.3em]\n0 & 1\n\\end{bmatrix}\n=\n\\begin{bmatrix}\n2 - \\lambda & -1 \\\\[0.3em]\n4 & 3 - \\lambda\n\\end{bmatrix}\n$$\n\n$$ \nDeterminant(A \u2014 \\lambda I) =\nDeterminant\\Bigg(\n\\begin{matrix}\n2 - \\lambda & -1 \\\\[0.3em]\n4 & 3 - \\lambda\n\\end{matrix}\\Bigg)\n= \\lambda^2 - 5\\lambda + 10\n$$\n\nComputing above quadratic equation to 0, we get two <b> Eigenvalues($\\lambda$) <\/b>:\n\n$$\\frac{5}{2} + \\iota \\frac{\\sqrt{15}}{2} , \\frac{5}{2} - \\iota \\frac{\\sqrt{15}}{2}$$\n\n\nSubstitute Eigenvalue(s) ($\\lambda$) into the following equation:\n\n$$(A - \\lambda I)  x = 0$$\n\n$$\n\\begin{bmatrix}\n2 & -1 \\\\[0.3em]\n4 & 3\n\\end{bmatrix}\n-\n\\Bigg(\\frac{5}{2} + \\iota \\frac{\\sqrt{15}}{2}\\Bigg) \\Bigg(\\begin{matrix}\n1 & 0 \\\\[0.3em]\n0 & 1\n\\end{matrix}\\Bigg)\n=\n\\begin{bmatrix}\n-\\frac{1}{2} - \\iota \\frac{\\sqrt{15}}{2} & -1 \\\\[0.3em]\n4 & \\frac{1}{2} - \\iota \\frac{\\sqrt{15}}{2}\n\\end{bmatrix}\n$$\n\n$$\n\\begin{bmatrix}\n2 & -1 \\\\[0.3em]\n4 & 3\n\\end{bmatrix}\n-\n\\Bigg(\\frac{5}{2} - \\iota \\frac{\\sqrt{15}}{2}\\Bigg) \\Bigg(\\begin{matrix}\n1 & 0 \\\\[0.3em]\n0 & 1\n\\end{matrix}\\Bigg)\n=\n\\begin{bmatrix}\n-\\frac{1}{2} + \\iota \\frac{\\sqrt{15}}{2} & -1 \\\\[0.3em]\n4 & \\frac{1}{2} + \\iota \\frac{\\sqrt{15}}{2}\n\\end{bmatrix}\n$$\n"}}