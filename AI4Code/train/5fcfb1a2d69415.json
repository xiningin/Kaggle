{"cell_type":{"b7e16e3c":"code","ff849159":"code","7ef56bea":"code","f6433539":"code","d5999a29":"code","2c3d0274":"code","c548e210":"code","2c4c6563":"code","7425d002":"code","922e05dd":"code","2d7620db":"code","28409df8":"code","f9c1688d":"code","8efd3360":"code","b61fb770":"code","06e46ac1":"code","f94f78e0":"code","40fe445f":"code","f2d14870":"code","e55d2cce":"code","41fcf4b6":"code","e2227440":"code","1e4bb4be":"code","008c8371":"code","44c170dd":"code","b0345ac9":"code","1760e5b7":"code","2c1e7f42":"code","380aade4":"code","70a35cd0":"code","5413241d":"code","47962b28":"code","429566f8":"code","544743e1":"code","737056dd":"code","5e2406d2":"code","b154d074":"code","5ebe5908":"code","181e14b5":"code","7d70bf1f":"code","b9733bcd":"code","9ba2b87b":"code","64742255":"code","2b055120":"code","6a666f3a":"code","0075bec8":"code","0150d8f2":"code","3fd317c9":"code","c1e2ff92":"code","19a95512":"code","facb5b88":"code","9e769c84":"code","a8f52411":"code","c443de53":"code","a606c613":"code","423bc738":"code","ebc48d0e":"code","976b6b03":"code","0503967f":"code","098831a3":"code","f38e6ddf":"code","376fd1ce":"markdown","076bdbd8":"markdown","69fb075f":"markdown","5448be83":"markdown","6584cdd4":"markdown","2ebb335f":"markdown","34a74cf6":"markdown","e582d97d":"markdown","f5e1318d":"markdown","898c75b6":"markdown","c7f420ae":"markdown","c5cfb01f":"markdown","5e063630":"markdown","76b54e5f":"markdown","69007063":"markdown","91e3f73f":"markdown","f31dc5e1":"markdown","e95b2cac":"markdown","42d29765":"markdown","7c410586":"markdown","62e12de2":"markdown","535c15c2":"markdown","45ce978d":"markdown","772248eb":"markdown","af565bfc":"markdown","90804d7f":"markdown","ecd0879e":"markdown","22beaa94":"markdown","97b28b61":"markdown","03777a89":"markdown","0458861b":"markdown","1206aa51":"markdown","fe843f3c":"markdown","1d1b46d2":"markdown","dbd89474":"markdown","1525159a":"markdown","ab19f8a1":"markdown","c2ebffd5":"markdown","5c001b85":"markdown","9a75bfe0":"markdown","f64f99ca":"markdown","c2d2b0b5":"markdown","c1d95eac":"markdown","8311114b":"markdown"},"source":{"b7e16e3c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","ff849159":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport plotly.express as px\nimport matplotlib\nimport time\nimport warnings\nimport matplotlib.pyplot as plt\nwarnings.filterwarnings(\"ignore\")\nplt.style.use('seaborn-darkgrid')\nimport statsmodels.api as sm\nmatplotlib.rcParams['axes.labelsize'] = 20\nmatplotlib.rcParams['xtick.labelsize'] = 12\nmatplotlib.rcParams['ytick.labelsize'] = 12\nmatplotlib.rcParams['text.color'] = 'k'","7ef56bea":"from scipy import stats\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_validate\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_absolute_error, r2_score\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor \nfrom sklearn.model_selection import GridSearchCV","f6433539":"filepath = '..\/input\/walmart-recruiting-store-sales-forecasting\/'","d5999a29":"dados = pd.read_csv(f'{filepath}train.csv.zip', parse_dates=['Date'], compression='zip')\ndados.info()","2c3d0274":"lojas = pd.read_csv(f'{filepath}stores.csv')\nlojas.info()","c548e210":"features = pd.read_csv(f'{filepath}features.csv.zip', parse_dates=['Date'], compression='zip')\nfeatures.info()","2c4c6563":"predizer = pd.read_csv(f'{filepath}test.csv.zip', parse_dates=['Date'], compression='zip')\npredizer.info()","7425d002":"plt.figure(figsize=(10,8))\nsns.distplot(dados.Weekly_Sales)","922e05dd":"plt.figure(figsize=(10,8))\nsns.boxplot(x=\"Type\", y=\"Size\", data=lojas)","2d7620db":"valores_por_loja_tamanho = dados.merge(lojas, on='Store', how='left')\nlojas_completas = valores_por_loja_tamanho.merge(features, on=['Store','Date'], how='left')\nlojas_completas.info()","28409df8":"plt.figure(figsize=(20,6))\nsns.boxplot(x=\"Store\", y=\"Weekly_Sales\",hue='IsHoliday_x', data=lojas_completas)","f9c1688d":"vendas_no_tempo = lojas_completas.groupby(['Date','Type']).mean()['Weekly_Sales'].reset_index()","8efd3360":"plt.figure(figsize=(30,8))\nsns.lineplot(x=\"Date\", y=\"Weekly_Sales\",hue='Type', data=vendas_no_tempo)","b61fb770":"vendas_agrupadas = lojas_completas.groupby(['Store','Date','Type','IsHoliday_x']).sum()['Weekly_Sales'].reset_index()","06e46ac1":"plt.figure(figsize=(20,6))\nsns.boxplot(x=\"Store\", y=\"Weekly_Sales\",hue='IsHoliday_x', data=vendas_agrupadas)","f94f78e0":"feriados = {\n    'Date' : (pd.to_datetime(['2010-02-12', '2011-02-11', '2012-02-10', '2013-02-08',\n                             '2010-09-10', '2011-09-09', '2012-09-07', '2013-09-06',\n                            '2010-11-26', '2011-11-25', '2012-11-23', '2013-11-29',\n                            '2010-12-31', '2011-12-30', '2012-12-28', '2013-12-27'])),\n    'TypeHoliday' :(['SB', 'SB', 'SB', 'SB',\n                    'Labor', 'Labor', 'Labor', 'Labor',\n                    'Thanksgiving', 'Thanksgiving', 'Thanksgiving', 'Thanksgiving',\n                    'Christmas', 'Christmas', 'Christmas', 'Christmas']) \n\n}","40fe445f":"lojas_feriados = lojas_completas.merge(pd.DataFrame(feriados), on='Date', how='left')\nlojas_feriados['TypeHoliday'].fillna(0,inplace=True)\nlojas_feriados","f2d14870":"lojas_feriados['Month'] = pd.to_datetime(lojas_feriados.Date).dt.month\nlojas_feriados['Year'] = pd.to_datetime(lojas_feriados.Date).dt.year","e55d2cce":"plot_feriados = lojas_feriados.groupby(['Store','Year','TypeHoliday','Type']).sum()['Weekly_Sales'].reset_index()\nplt.figure(figsize=(15,8))\nsns.boxplot(x=\"Year\", y=\"Weekly_Sales\",hue='TypeHoliday', data=plot_feriados.loc[plot_feriados['TypeHoliday']!=0])","41fcf4b6":"plt.figure(figsize=(15,8))\nsns.boxplot(x=\"Type\", y=\"Weekly_Sales\",hue='TypeHoliday', data=plot_feriados.loc[plot_feriados['TypeHoliday']!=0])","e2227440":"plt.figure(figsize=(30,8))\nsns.boxplot(x=\"Store\", y=\"MarkDown1\",hue='IsHoliday_x', data=lojas_feriados)","1e4bb4be":"plt.figure(figsize=(30,8))\nsns.boxplot(x=\"Store\", y=\"MarkDown2\",hue='IsHoliday_x', data=lojas_feriados)","008c8371":"plt.figure(figsize=(30,8))\nsns.boxplot(x=\"Store\", y=\"MarkDown3\",hue='IsHoliday_x', data=lojas_feriados)","44c170dd":"plt.figure(figsize=(30,8))\nsns.boxplot(x=\"Store\", y=\"MarkDown4\",hue='IsHoliday_x', data=lojas_feriados)","b0345ac9":"plt.figure(figsize=(30,8))\nsns.boxplot(x=\"Store\", y=\"MarkDown5\",hue='IsHoliday_x', data=lojas_feriados)","1760e5b7":"filling = lojas_feriados.groupby(['Store','IsHoliday_x']).median()[['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5']].reset_index()\nfilling.rename(columns={'MarkDown1':'FMD1','MarkDown2':'FMD2','MarkDown3':'FMD3','MarkDown4':'FMD4','MarkDown5':'FMD5'}, inplace=True)","2c1e7f42":"lojas_preenchidas = lojas_feriados.merge(filling, on=['Store','IsHoliday_x'], how='inner')","380aade4":"lojas_preenchidas.MarkDown1.fillna(lojas_preenchidas['FMD1'],inplace=True)\nlojas_preenchidas.MarkDown2.fillna(lojas_preenchidas['FMD2'],inplace=True)\nlojas_preenchidas.MarkDown3.fillna(lojas_preenchidas['FMD3'],inplace=True)\nlojas_preenchidas.MarkDown4.fillna(lojas_preenchidas['FMD4'],inplace=True)\nlojas_preenchidas.MarkDown5.fillna(lojas_preenchidas['FMD5'],inplace=True)\nlojas_preenchidas.drop(['FMD1','FMD2','FMD3','FMD4','FMD5'], axis=1, inplace=True)\nlojas_preenchidas.info()","70a35cd0":"lojas_preenchidas['IsHoliday'] = pd.get_dummies(lojas_preenchidas.IsHoliday_x)[1]\nlojas_preenchidas['SB'] =pd.get_dummies(lojas_preenchidas.TypeHoliday)['SB']\nlojas_preenchidas['Labor'] =pd.get_dummies(lojas_preenchidas.TypeHoliday)['Labor']\nlojas_preenchidas['Thanksgiving'] =pd.get_dummies(lojas_preenchidas.TypeHoliday)['Thanksgiving']\nlojas_preenchidas['Christmas'] =pd.get_dummies(lojas_preenchidas.TypeHoliday)['Christmas']\nlojas_preenchidas['TypeA'] = pd.get_dummies(lojas_preenchidas.Type)['A']\nlojas_preenchidas['TypeB'] = pd.get_dummies(lojas_preenchidas.Type)['B']\nlojas_preenchidas['TypeC'] = pd.get_dummies(lojas_preenchidas.Type)['C']\nlojas_preenchidas.head()","5413241d":"plt.figure(figsize=(25,20))\nsns.heatmap(lojas_preenchidas.fillna(0).corr(), annot=True)","47962b28":"plt.figure(figsize=(20,10))\nlojas_preenchidas['Weekly_Sales_Size'] = lojas_preenchidas['Weekly_Sales']\/lojas_preenchidas['Size']\nsns.lineplot(x=\"Date\", y=\"Weekly_Sales_Size\",hue='Type', data=lojas_preenchidas)","429566f8":"lojas_limpas = lojas_preenchidas.drop(['MarkDown4','MarkDown5','IsHoliday_y','IsHoliday_x','Type','TypeHoliday','TypeC','Weekly_Sales_Size'], axis=1)","544743e1":"OfT = lojas_limpas.loc[lojas_limpas['Date'] >= '2012-10-05']\nlojas_predict = lojas_limpas.loc[lojas_limpas['Date'] < '2012-10-05']","737056dd":"# Conjunto OfT\nX_OfT = OfT.drop(['Weekly_Sales','Date'], axis=1)\ny_OfT = OfT['Weekly_Sales']\n# Conjunto OfS\nX = lojas_predict.drop(['Weekly_Sales','Date'], axis=1)\ny = lojas_predict['Weekly_Sales']","5e2406d2":"X_OfT.head()","b154d074":"def WMAE(y_obs, y_pred, flag):\n    peso = flag*5+1*(1-flag)\n    \n    indice = (1\/sum(peso))*sum(peso*abs(y_obs-y_pred))\n\n    return indice","5ebe5908":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25)","181e14b5":"print(f'Formato do conjunto de treino OfS: {X_train.shape}')\nprint(f'Formato do conjunto de teste: {X_test.shape}')","7d70bf1f":"parameters = {'normalize': [False,True], 'fit_intercept': [False,True]} \nmodel = LinearRegression()\n\nSEED = 1988\nnp.random.seed(SEED)\ncv = KFold(10, shuffle=True)\n\n\n\nclf = GridSearchCV(model, parameters, cv=cv, verbose=5, n_jobs=8)\nclf.fit(X_train, y_train)\nclf.best_params_\nclf.predict(X_test)\nwmae = WMAE(y_test, clf.predict(X_test), X_test.IsHoliday)\nr2 = r2_score(y_test, clf.predict(X_test))\nresults = cross_validate(model, X, y, cv=cv, return_train_score = False)\nmedia = results['test_score'].mean()\ndesvio = results['test_score'].std()\n\nprint(\"LinearRegression\")\nprint(\"------------------------------\")\nprint(f'Parametros \u00f3timos = {clf.best_params_}')\nprint(f'Mean: {media*100}')\nprint(f'Accuracy: [{(media-2*desvio)*100} , {(media+2*desvio)*100}]')\nprint(f'WMAE = {wmae} and R-square = {r2}')\nprint(\"------------------------------\")","b9733bcd":"SEED = 1988\nnp.random.seed(SEED)\nln = LinearRegression(fit_intercept = True, normalize = False)\nrf = RandomForestRegressor(max_depth = 30, max_features =  18, min_samples_leaf = 2 ,n_estimators = 200)\ngbm = GradientBoostingRegressor(max_depth = 5, max_features = 12, min_samples_leaf = 5, n_estimators = 200)\nnp.random.seed(SEED)\n\nln.fit(X_train, y_train)\nln_y_pred = ln.predict(X_test)\nwmae_ln_pred = WMAE(y_test, ln_y_pred, X_test.IsHoliday)\nr2_ln_pred = r2_score(y_test, ln_y_pred)\n\nrf.fit(X_train, y_train)\nrf_y_pred = rf.predict(X_test)\nwmae_rf_pred = WMAE(y_test, rf_y_pred, X_test.IsHoliday)\nr2_rf_pred = r2_score(y_test, rf_y_pred)\n\ngbm.fit(X_train, y_train)\ngbm_y_pred = gbm.predict(X_test)\nwmae_gbm_pred = WMAE(y_test, gbm_y_pred, X_test.IsHoliday)\nr2_gbm_pred = r2_score(y_test, gbm_y_pred)\n\nprint(\"LinearRegression\")\nprint(\"------------------------------\")\nprint(f'WMAE = {wmae_ln_pred} and R-square = {r2_ln_pred}')\nprint(\"------------------------------\")\nprint(\"RandomForest\")\nprint(\"------------------------------\")\nprint(f'WMAE = {wmae_rf_pred} and R-square = {r2_rf_pred}')\nprint(\"------------------------------\")\nprint(\"GradientBoosting\")\nprint(\"------------------------------\")\nprint(f'WMAE = {wmae_gbm_pred} and R-square = {r2_gbm_pred}')\nprint(\"------------------------------\")","9ba2b87b":"plt.figure(figsize = (20,12))\nplt.scatter(y_test,ln_y_pred,label='LR',marker = 'o',color='r')\nplt.scatter(y_test,rf_y_pred,label='RF',marker = 'o',color='b')\nplt.scatter(y_test,gbm_y_pred,label='GBR',marker = 'o',color='y')\nplt.title('Modelos',fontsize = 25)\nplt.legend(fontsize = 20)\nplt.show()","64742255":"ln_y_OfT = ln.predict(X_OfT)\nwmae_ln_OfT = WMAE(y_OfT, ln_y_OfT, X_OfT.IsHoliday)\nr2_ln_OfT = r2_score(y_OfT, ln_y_OfT)\n\n\nrf_y_OfT = rf.predict(X_OfT)\nwmae_rf_OfT = WMAE(y_OfT, rf_y_OfT, X_OfT.IsHoliday)\nr2_rf_OfT = r2_score(y_OfT, rf_y_OfT)\n\n\ngbm_y_OfT = gbm.predict(X_OfT)\nwmae_gbm_OfT = WMAE(y_OfT, gbm_y_OfT, X_OfT.IsHoliday)\nr2_gbm_OfT = r2_score(y_OfT, gbm_y_OfT)\n\nprint(\"LinearRegression\")\nprint(\"------------------------------\")\nprint(f'WMAE = {wmae_ln_OfT} and R-square = {r2_ln_OfT}')\nprint(\"------------------------------\")\nprint(\"RandomForest\")\nprint(\"------------------------------\")\nprint(f'WMAE = {wmae_rf_OfT} and R-square = {r2_rf_OfT}')\nprint(\"------------------------------\")\nprint(\"GradientBoosting\")\nprint(\"------------------------------\")\nprint(f'WMAE = {wmae_gbm_OfT} and R-square = {r2_gbm_OfT}')\nprint(\"------------------------------\")","2b055120":"plt.figure(figsize = (20,12))\nplt.scatter(y_OfT,ln_y_OfT,label='LR',marker = 'o',color='r')\nplt.scatter(y_OfT,rf_y_OfT,label='RF',marker = 'o',color='b')\nplt.scatter(y_OfT,gbm_y_OfT,label='GBR',marker = 'o',color='y')\nplt.title('Modelos',fontsize = 25)\nplt.legend(fontsize = 20)\nplt.show()","6a666f3a":"predicoes_loja = predizer.merge(features, on=['Store','Date'], how='inner')","0075bec8":"lojas_p_features = predicoes_loja.merge(lojas, on='Store', how='inner')","0150d8f2":"filling_p = lojas_p_features.groupby(['Store','IsHoliday_x','Type']).median()[['MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5','CPI','Unemployment']].reset_index()\nfilling_p.rename(columns={'MarkDown1':'FMD1','MarkDown2':'FMD2','MarkDown3':'FMD3','MarkDown4':'FMD4','MarkDown5':'FMD5','CPI': 'C','Unemployment':'UN'}, inplace=True)","3fd317c9":"lojas_preenchidas_p = lojas_p_features.merge(filling_p, on=['Store','IsHoliday_x','Type'], how='inner')","c1e2ff92":"lojas_preenchidas_p.MarkDown1.fillna(lojas_preenchidas_p['FMD1'],inplace=True)\nlojas_preenchidas_p.MarkDown2.fillna(lojas_preenchidas_p['FMD2'],inplace=True)\nlojas_preenchidas_p.MarkDown3.fillna(lojas_preenchidas_p['FMD3'],inplace=True)\nlojas_preenchidas_p.MarkDown4.fillna(lojas_preenchidas_p['FMD4'],inplace=True)\nlojas_preenchidas_p.MarkDown5.fillna(lojas_preenchidas_p['FMD5'],inplace=True)\nlojas_preenchidas_p.CPI.fillna(lojas_preenchidas_p['C'],inplace=True)\nlojas_preenchidas_p.Unemployment.fillna(lojas_preenchidas_p['UN'],inplace=True)\nlojas_preenchidas_p.drop(['FMD1','FMD2','FMD3','FMD4','FMD5','C','UN'], axis=1, inplace=True)\nlojas_preenchidas_p.info()","19a95512":"lojas_feriados_p = lojas_preenchidas_p.merge(pd.DataFrame(feriados), on='Date', how='left')\nlojas_feriados_p['TypeHoliday'].fillna(0,inplace=True)\nlojas_feriados_p","facb5b88":"lojas_feriados_p['IsHoliday'] = pd.get_dummies(lojas_feriados_p.IsHoliday_x)[1]\nlojas_feriados_p['SB'] = pd.get_dummies(lojas_feriados_p.TypeHoliday)['SB']\nlojas_feriados_p['Labor'] = 0\nlojas_feriados_p['Thanksgiving'] = pd.get_dummies(lojas_feriados_p.TypeHoliday)['Thanksgiving']\nlojas_feriados_p['Christmas'] = pd.get_dummies(lojas_feriados_p.TypeHoliday)['Christmas']\n\nlojas_feriados_p","9e769c84":"lojas_feriados_p['TypeA'] = pd.get_dummies(lojas_feriados_p.Type)['A']\nlojas_feriados_p['TypeB'] = pd.get_dummies(lojas_feriados_p.Type)['B']\nlojas_feriados_p['TypeC'] = pd.get_dummies(lojas_feriados_p.Type)['C']\nlojas_feriados_p['Month'] = pd.to_datetime(lojas_feriados_p.Date).dt.month\nlojas_feriados_p['Year'] =  pd.to_datetime(lojas_feriados_p.Date).dt.year","a8f52411":"colunas = X_train.columns","c443de53":"lojas_predizer = lojas_feriados_p[colunas]\nlojas_predizer.fillna(0, inplace=True)","a606c613":"lojas_preenchidas_p['Weekly_Sales'] = rf.predict(lojas_predizer).round(2)","423bc738":"predicoes_finais = lojas_preenchidas_p[['Store','Dept','Date','Weekly_Sales']]","ebc48d0e":"predicoes_finais['Id'] = (predicoes_finais[['Store','Dept','Date']].astype('str').apply('_'.join, axis=1))","976b6b03":"sample_submission = (f'{filepath}sampleSubmission.csv.zip')","0503967f":"submission = predicoes_finais[['Id','Weekly_Sales']]","098831a3":"submission.info()","f38e6ddf":"submission.to_csv('submission.csv', index=False)","376fd1ce":"Agora vamos quebrar nossas vari\u00e1veis nos X e Y que utilizaremos para treinar os modelos.","076bdbd8":"O Modelo <b> Random Forest Regressor <\/b>, parece mais consistente nesse cen\u00e1rio, vamos analisar a amostra <b>OfT<\/b>.","69fb075f":"Mesma quantidade de vari\u00e1veis, ok. ","5448be83":"### Sele\u00e7\u00e3o de Vari\u00e1veis.","6584cdd4":"Veremos que o nosso baseline \u00e9 em 14.910, vamos verificar como os outros modelos se comportam. ","2ebb335f":"* ### Random Forest Regressor e Gradient Boosting Regressor","34a74cf6":"Com todas as vari\u00e1veis preenchidas no DataSet, vamos analisar as poss\u00edveis correla\u00e7\u00f5es entre elas.","e582d97d":"As classifica\u00e7\u00f5es de tipos de lojas parece ser feito de acordo com o tamanho das lojas.\nVamos agrupar as informa\u00e7\u00f5es de features e das lojas para continuar as an\u00e1lises de vari\u00e1veis.","f5e1318d":"Mesmo n\u00e3o utilizando a abordagem de s\u00e9ries temporais, vamos fazer uma quebra de treino e teste considerando que nossas vendas se comportam com uma fun\u00e7\u00e3o do tempo, para isso vamos quebrar a s\u00e9rie de Teste em <b> Out of Sample <\/b> e <b> Out of Time <\/b>","898c75b6":"Podemos identificar que existe alguma flutua\u00e7\u00e3o nas vendas em feriados, vamos colocar os feriados fornecidos para identificar poss\u00edveis intera\u00e7\u00f5es.","c7f420ae":"Quebrando os conjuntos de treino e teste","c5cfb01f":"## Treino e Teste","5e063630":"## Os Modelos","76b54e5f":"Notamos que os MarkDowns tem bastantes outliers por loja, por isso iremos tratar os missing values utilizando as medianas dos valores por loja e para caso tenha feriado ou n\u00e3o na data.","69007063":"# Conclus\u00f5es","91e3f73f":"### Tratando MissingValues","f31dc5e1":"A primeira vista, considerando as vendas durante um determinado per\u00edodo de tempo para prever vendas futuras, foi considerado a utiliza\u00e7\u00e3o da abordagem por <b> S\u00e9ries Temporais <\/b>. Para tal dever\u00edamos analisar as vendas no tempo e tentar ajusar um modelo de classe ARIMAX para projetar vendas futuras. \nAnalisando as features, notamos que existem fetures de datas futuras, ou datas as quais iremos estimar, dados estas informa\u00e7\u00f5es podemos utiliza algum modelo de <b> Regress\u00e3o\/Classifica\u00e7\u00e3o <\/b>.\nCaso fossemos utilizar um modelo ARIMAX, tirariamos as vari\u00e1veis de departamento, buscando analisar apenas as vendas semanais. \nFizemos uma pequena an\u00e1lise das vendas e percebemos que as diferen\u00e7as entre semanas tem um comportamento estacion\u00e1rio, o que permitiria uma abordagem de s\u00e9ries temporais, o que n\u00e3o foi o caso selecionado.","e95b2cac":"# An\u00e1lise de Vari\u00e1veis","42d29765":"Podemos verificar algumas correla\u00e7\u00f5es l\u00f3gicas entre feriados, mas tamb\u00e9m podemos notar algumas correla\u00e7\u00f5es entre MarkDowns. Vamos retirar algumas vari\u00e1veis afim de simplificar o modelo e mitigar problemas de multicolinearidade dado que ainda n\u00e3o decidimos qual modelo vamos utilizar em nossas predi\u00e7\u00f5es.","7c410586":"### Teste Out of Time","62e12de2":"Foi feita uma rodada de GridSearch, para determinar os melhores par\u00e2mentros dos modelos em quest\u00e3o. Com esses par\u00e2metros calibraremos e faremos as predi\u00e7\u00f5es e an\u00e1lise dos dados. ","535c15c2":"### Calibrando e Calculando a Efetividade dos Modelos","45ce978d":"O modelo <b> Random Forest <\/b> se mostrou mais consistente nos testes com estes par\u00e2metros. O <b> Gradient Boosting Regressor <\/b> teve uma efetividade melhor em testes com outros par\u00e2metros, que demandam muito mais do nosso poder computacional, portanto optamos por n\u00e3o utiliza-lo.","772248eb":"Podemos verificar que as lojas de tipo C tem uma variabilidade menor durante o tempo, talvez essas lojas n\u00e3o tenham tanta influ\u00eancia dos feriados, vamos analisar mais a fundo.","af565bfc":"Como nossos dados est\u00e3o granulares no n\u00edvel de departamento, vamos agrupar por loja para tentar identificar melhor as tend\u00eancias","90804d7f":"Antes de come\u00e7ar os testes de modelos, vamos tratar os valores faltantes. \nNota-se que os MarkDowns s\u00e3o os unicos valores faltantes neste dataset, e considerando a documenta\u00e7\u00e3o e a natureza dessas features vamos fazer uma r\u00e1pida an\u00e1lise para verificar o comportamento dessas vari\u00e1veis.","ecd0879e":"Como estamos falando em vendas semanais, vamos separar um m\u00eas de vendas para nossa an\u00e1lise out of time.","22beaa94":"Como verificamos que os feriados tem diferentes pesos nas vendas das lojas, vamos separalos em vari\u00e1veis para auxiliar as nossas an\u00e1lises.","97b28b61":"# Overview","03777a89":"Vamos retirar as vari\u00e1veis com alguma colinearidade com os MarkDonw, al\u00e9m das vari\u00e1veis categ\u00f3ricas e da Weekly_Sale_Size que serviu apenas para uma an\u00e1lise pontual do comportamento.","0458861b":"Podemos ver que os feriados de A\u00e7\u00e3o de Gra\u00e7as tem um valor de vendas superior aos demais feriados. Vamos considerar isso nas predi\u00e7\u00f5es.\nTamb\u00e9m certificamos que as lojas do tipo C n\u00e3o tem forte influ\u00eancia dos feriados.","1206aa51":"As lojas C tem um comportamento bem diferente, o que pode causar alguns problemas em nosso modelo. Vamos manter as vari\u00e1veis de tipo afim de tentar reduzir esse problema.","fe843f3c":"### Regress\u00e3o Linear","1d1b46d2":"Analisamos as vari\u00e1veis e as poss\u00edveis informa\u00e7\u00f5es para o modelo e levantamos algumas hip\u00f3teses e conclus\u00e7oes:\n<n>1) Para obter um baixo <b>WMAE<\/b> consideramos os departamentos como uma vari\u00e1vel de alto poder de predi\u00e7\u00e3o apra o modelo. Para isso assumimos que os n\u00fameros dos departamentos tem consist\u00eancia entre as lojas, ou seja, o Dept 1 da loja 1 representa o mesmo departamento que o Dept 1 da loja 45. Uma outra forma de modelar seria utilizar a soma total das vendas por lojas e quebrar as vendas por departamento de acordo com a representatividade de cada departamento por loja. Fizemos a simula\u00e7\u00e3o e encontramos resultados interessantes, por\u00e9m um pouco distantes das lojas do Tipo C.\n<n>2) Caso fossesmo tratar com uma predi\u00e7\u00e3o para lojas futuras, seria interessante n\u00e3o considerar a vari\u00e1vel Loja e Departamento nas predi\u00e7\u00f5es, afim de n\u00e3o viesar os resultados por lojas j\u00e1 conhecidas.\n<n>3) Em caso de predi\u00e7\u00f5es de vendas futuras, onde n\u00e3o conhecemos as features agregadas, poderiamos buscar a abordagem de s\u00e9ries temporais, e tentar o fit de um modelo <b>ARIMA<\/b> para os dados,para isso dever\u00edamos usar a diferen\u00e7a de vendas semanais agregadas por loja, dado que essa vari\u00e1vei <b>Weekly_Sales.diff()<\/b> possui um comportamento estacion\u00e1rio no tmepo.","dbd89474":"Para coparar os modelos que iremos ajustar as predi\u00e7\u00f5es usando a fun\u00e7\u00e3o determinada pelo desafio de WMAE","1525159a":"Aplicando as Predi\u00e7\u00f5es","ab19f8a1":"Foram escolhidos tr\u00eas modelos para comparar performance e verificar qual seria o modelo adotado para esse caso: LinearRegression, RandomForestRegressor e GradientBoostRegressor. Cada um dos tres modelos tem uma especificidade para como fitam os dados, portanto vamos olhar todos.\nA <b> Regress\u00e3o Linear <\/b> nos servir\u00e1 como baseline para a performance dos modelos.","c2ebffd5":"Vamos extrair as colunas de m\u00eas e ano para identificar se os feriados tem comportamento diferente no tempo","5c001b85":"Vamos fazer uma an\u00e1lise das vendas por loja e departamentos do Walmart. ","9a75bfe0":"# Predi\u00e7\u00f5es","f64f99ca":"Vamos separar os data sets e analisar as informa\u00e7\u00f5es que est\u00e3o contidas em cada um","c2d2b0b5":"# Modelo","c1d95eac":"Vamos usar o m\u00e9todo de CrossValidation para veirificar a coer\u00eancia dos nossos modelos, assim como sua estabilidade nos conjuntos de dados. \nUsaremos tamb\u00e9m o GridSearch para vefiricar quais os par\u00e2mentros \u00f3timos de cada modelo que estamos utilizando.","8311114b":"Uma ultima an\u00e1lise, vimos que as vendas das lojas C n\u00e3o se comportam da mesma maneira que as demais lojas, portanto vamos verificar como elas se comportam, dividindo a loja pelo seu tamanho. "}}