{"cell_type":{"0e85cd1b":"code","146ab7a0":"code","fcf69045":"code","d6424ccc":"code","11b11f9a":"code","caa5f9e1":"code","b78b7e8d":"code","b91eeb4f":"code","d3a93ec0":"code","b220a126":"code","0e02fe03":"code","de2012ad":"code","252bdcfe":"code","c50f4e26":"code","55ccb764":"code","adc6b6b3":"code","8db4f7ee":"code","40b37f00":"code","7b4eee9e":"code","bc495f19":"code","5ee82ff6":"code","1f74ba30":"code","c547e355":"code","1e0fce24":"code","c6bb7942":"code","afbca292":"code","43b18f60":"code","7ab72de1":"code","7be6273b":"markdown","68d6b6f3":"markdown","bbc9de2b":"markdown","cd81aa99":"markdown"},"source":{"0e85cd1b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","146ab7a0":"import numpy as np #linear algebra\nimport pandas as pd #data processing\n\nimport matplotlib.pyplot as plt #data visualization\nimport seaborn as sns #data visualization\n\nimport warnings\nwarnings.filterwarnings(\"ignore\") #to ignore the warnings\n\n#for model building\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nimport xgboost as xgb","fcf69045":"df= pd.read_csv(\"\/kaggle\/input\/breast-cancer-wisconsin-data\/data.csv\")","d6424ccc":"df.shape","11b11f9a":"df.info()","caa5f9e1":"df.head(30)","b78b7e8d":"df.columns","b91eeb4f":"df.drop(['Unnamed: 32','id'], axis = 1 , inplace=True)\ndf.columns","d3a93ec0":"df.diagnosis.replace({\"M\":1,\"B\":0},inplace=True)\ndf.diagnosis.unique()","b220a126":"corr = df.corr()\nplt.figure(figsize=(20,20))\nsns.heatmap(df.corr(), cmap='YlGnBu', annot = True)\nplt.title(\"Correlation Map\", fontweight = \"bold\", fontsize=16)","0e02fe03":"corr[abs(corr['diagnosis']) > 0.59].index    #: The following columns are the one's that show the greatest correlation with our diagnosis column.","de2012ad":"df.drop('diagnosis', axis=1).corrwith(df.diagnosis).plot(kind='bar', grid=True, figsize=(12, 10), title=\"Correlation with target\",color=\"green\");","252bdcfe":"#dividing data\nX = df.drop(\"diagnosis\",axis = 1)\ny = df.diagnosis","c50f4e26":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\n","55ccb764":"df.head(10)","adc6b6b3":"#importing train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.33,random_state=42,shuffle=True, stratify=y)","8db4f7ee":"print('X_train shape : ', X_train.shape )\nprint('y_train shape : ', y_train.shape )\nprint('X_test shape : ', X_test.shape )\nprint('y_test shape : ', y_test.shape )","40b37f00":"from sklearn.neighbors import KNeighborsClassifier\n\n\ntest_scores = []\ntrain_scores = []\nK=[]\nfor i in range(1,15):\n\n    knn = KNeighborsClassifier(i)\n    knn.fit(X_train,y_train)\n    \n    train_scores.append(knn.score(X_train,y_train))\n    test_scores.append(knn.score(X_test,y_test))\n    K.append(i)","7b4eee9e":"\nresults= { 'train_scores':train_scores,\n         'test_scores':test_scores,\n         'K':K}\nresultsdf =  pd.DataFrame(results,\n        columns=['train_scores','test_scores', 'K'])\nresultsdf","bc495f19":"#appling KNN model for k=1\nKNNModel = KNeighborsClassifier(1)\nKNNModel.fit(X_train,y_train)\n\ntrain_score = KNNModel.score(X_train,y_train)\ntest_score = KNNModel.score(X_test,y_test)\nprint( 'train_score ', train_score)\nprint( 'test_score ',test_score )","5ee82ff6":"#prediction step\ny_pred = knn.predict(X_test)\n\n\nconfusion_matrix(y_test,y_pred)\npd.crosstab(y_test, y_pred, rownames=['True'], colnames=['Predicted'], margins=True)","1f74ba30":"#Import Random Forest Model\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Create a Gaussian Classifier\nRFclfModel=RandomForestClassifier(n_estimators=100)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nRFclfModel.fit(X_train,y_train)\n\ny_pred_RFclfModel=RFclfModel.predict(X_test)","c547e355":"from sklearn import metrics\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_test, y_pred_RFclfModel))","1e0fce24":"#confusion_matrix\nconfusion_matrix(y_test,y_pred)\npd.crosstab(y_test, y_pred_RFclfModel, rownames=['True'], colnames=['Predicted'], margins=True)","c6bb7942":"from sklearn.svm import SVC\nSVCModel = SVC(kernel = 'linear', random_state=0)\n\nSVCModel.fit(X_train,y_train)\ny_pred_SVCModel=SVCModel .predict(X_test)","afbca292":"from sklearn import metrics\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy=:\",metrics.accuracy_score(y_test, y_pred_SVCModel))","43b18f60":"# confusion matrix\ncm_svm= confusion_matrix(y_test,y_pred_SVCModel)\npd.crosstab(y_test, y_pred_SVCModel, rownames=['True'], colnames=['Predicted'], margins=True)","7ab72de1":"sns.heatmap(cm_svm,annot=True)","7be6273b":"##### Observation: After dropping the two columns, we are now left with 31 columns. Let us see how well do they correlate with the diagnosis column.","68d6b6f3":"### Observation:\n\nOnly the 'diagnosis' column, which we have to predict is of object datatype.\n\nThere's only ID column of int type. We will probably drop it anyway.","bbc9de2b":"#### replace \nM:1\nB:0","cd81aa99":"Observation: In order to conduct our analysis easily, we have converted the target column as:\n\nMalignant - 1\nBenignant - 0"}}