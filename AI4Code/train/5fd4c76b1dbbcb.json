{"cell_type":{"ebfbb74a":"code","5f58c483":"code","8f792fb3":"code","f60d9029":"code","4e7fcab1":"code","9d9c7add":"code","724f4480":"code","624307c8":"code","25cfb647":"code","d9765f4e":"code","1ed986af":"code","8bd4b23f":"code","9b7d83e4":"code","178fb4a2":"code","97a5c5ab":"code","12ce3fd5":"code","5cd07679":"code","f7c85ef1":"code","4e96f40c":"code","f16d843f":"code","5d3d072f":"code","b7aba6a1":"code","d30b416f":"code","2bf8108c":"code","c65ec641":"code","4f07b71c":"code","557bf243":"code","1e175784":"code","0c2af371":"code","55e23943":"code","3a0030aa":"code","dd13be07":"code","6b8c335e":"code","a9ef3a7d":"code","1b3e9c30":"code","4181c9eb":"code","895d07c1":"code","e20535f6":"code","db0c9a40":"code","1f229719":"code","927a2f7f":"code","d322447b":"code","5e17eb08":"code","2c704f85":"code","00eefe01":"code","264ae343":"code","96061df6":"code","318b9610":"code","cc9b1ee3":"code","3b18d8b7":"code","37cb6f4c":"code","c99629b8":"code","03d98b7b":"code","93c46576":"code","7ca5e265":"code","b9c723ef":"code","182ff0c0":"code","0f88d22e":"code","efbe8d79":"code","9fd7b75a":"code","80720301":"code","86daaae1":"code","b156ebf6":"code","face817e":"code","7ee8c0eb":"code","28d95bd2":"code","8751877f":"code","7173f14e":"code","78ada727":"code","2c0f1cfd":"code","1a5bc33e":"code","efeef956":"code","e489e351":"code","bfc5f48e":"code","1e024ec7":"code","fec59106":"code","d3065bf1":"code","e23ca3bd":"code","9a62f296":"code","8c202cfe":"code","a4ff5266":"code","2471644c":"code","902c3789":"code","04d9af79":"markdown","c9172e3b":"markdown","5ad02027":"markdown","4759ee14":"markdown","85e45763":"markdown","727c3e44":"markdown","abcc46b4":"markdown","89a06cb9":"markdown","9098973e":"markdown","48b7718b":"markdown","015fc8cc":"markdown","002a01c3":"markdown","378dbe25":"markdown","53a562a7":"markdown","4db11e11":"markdown","55a80ade":"markdown","1b617fee":"markdown","ed53f268":"markdown","42bdf1ba":"markdown","0665c5be":"markdown","5535c5a2":"markdown","de428705":"markdown","716f1bc6":"markdown","66765291":"markdown","3b6caf9c":"markdown","dc8c05a7":"markdown","4dee5077":"markdown","f50ff7d0":"markdown","98278c21":"markdown","c16a6086":"markdown","154c62bc":"markdown","81c2b278":"markdown","5c321e6b":"markdown","b152867a":"markdown","be7961e3":"markdown","34b491fa":"markdown","a71167b9":"markdown","5a382562":"markdown","e4eb2088":"markdown","f53f8438":"markdown","15f7b13e":"markdown","34f661ba":"markdown","7b2a1c0a":"markdown","4339c2ef":"markdown","4b2b5788":"markdown","ac7a02ed":"markdown","3809736c":"markdown","ee1863d6":"markdown","fc456a39":"markdown","f7e39652":"markdown","c116340e":"markdown","fb4fdd0c":"markdown","69df0201":"markdown","14eb08c4":"markdown","3d96f91e":"markdown","52fec85c":"markdown","93ec294d":"markdown","440806f3":"markdown","ecfef970":"markdown"},"source":{"ebfbb74a":"# data analysis\nimport pandas as pd\nimport numpy as np\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# machine learning\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier","5f58c483":"train_df = pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('..\/input\/titanic\/test.csv')\ncomb_train_test_df = [train_df, test_df]","8f792fb3":"train_df.head()","f60d9029":"train_df.columns.values","4e7fcab1":"train_df.dtypes.unique()","9d9c7add":"train_df.select_dtypes(include=[\"object\"]).head()","724f4480":"train_df.select_dtypes(include=[\"float64\", \"int64\"]).head()","624307c8":"print(f'Number of all columns: {len(train_df.columns)}')\nprint(f'Number of numerical columns: {len(train_df.select_dtypes(include=[\"float64\", \"int64\"]).columns)}')\nprint(f'Number of categorical \/ mixed types columns: {len(train_df.select_dtypes(include=[\"object\"]).columns)}')","25cfb647":"for column in train_df.select_dtypes(include=[\"float64\", \"int64\"]).columns:\n    plt.figure()\n    train_df.boxplot([column])","d9765f4e":"total = train_df.PassengerId.count()\nnull_sum = train_df.isna().sum()[train_df.isna().sum() != 0]\n\npd.DataFrame(data={'Amount of null values': null_sum,\n                   'Percentage of null values': (null_sum \/ total*100).round(2).astype(str) + '%'})","1ed986af":"train_df.describe()","8bd4b23f":"train_df.describe(include=['object'])","9b7d83e4":"total = train_df.PassengerId.count()\nnull_sum = train_df.isna().sum()[train_df.isna().sum() != 0]\ndupl_df = train_df.apply(lambda x: x.duplicated()).sum()\nunique_df = train_df.apply(lambda x: x.value_counts()).count()\ndupes_and_uniques = dupl_df + unique_df","178fb4a2":"pd.DataFrame(data={\n    'Amount of possible duplicates': dupl_df,\n    'Amount of unique values': unique_df, \n    'Total sum of possible dupes and unique values': dupes_and_uniques\n})","97a5c5ab":"not_dupes_columns = dupl_df[dupl_df != 0].axes[0].tolist()\nunique_columns = dupl_df[dupl_df == 0].axes[0].tolist()\npoten_dupes = ['Ticket', 'Fare', 'Cabin']\nfor i in poten_dupes:\n    not_dupes_columns.remove(i)\nnot_dupes_columns = unique_columns + not_dupes_columns","12ce3fd5":"cat = train_df.apply(lambda x: train_df.dtypes == 'object').PassengerId\n\nreal_dupl_df = {i: True for i in train_df.columns.tolist() if i not in not_dupes_columns}\nreal_dupl_df_2 = {i: False for i in train_df.columns.tolist() if i in not_dupes_columns}\nreal_dupl_df.update(real_dupl_df_2)","5cd07679":"pd.DataFrame(data={\n    'Amount of possible duplicates': dupl_df,\n    'Amount of unique values': unique_df,\n    'Duplicate ratio': (dupl_df \/ total*100).round(2).astype(str) + '%',\n    'Duplicated?': real_dupl_df,\n})","f7c85ef1":"train_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","4e96f40c":"train_df[['Sex', 'Survived']].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","f16d843f":"train_df[['Age', 'Survived']].groupby(['Age'], as_index=False).mean().sort_values(by='Age', ascending=True)","5d3d072f":"g = sns.FacetGrid(train_df, col='Survived')\ng.map(plt.hist, \"Age\", bins=20)","b7aba6a1":"g = sns.FacetGrid(train_df, row=\"Embarked\", height=3, aspect=1.6)\ng.map(sns.pointplot, 'Pclass', 'Survived', 'Sex', palette='deep')\ng.add_legend()","d30b416f":"g = sns.FacetGrid(train_df, row='Embarked', col='Survived', height=3, aspect=1.6)\ng.map(sns.barplot, 'Sex', 'Fare', alpha=0.5, ci=None)\ng.add_legend()","2bf8108c":"print(f'BEFORE SHAPE --> train: {train_df.shape}, test: {test_df.shape}, combine_train: {comb_train_test_df[0].shape}, combine_test: {comb_train_test_df[1].shape}')","c65ec641":"train_df = train_df.drop(['Ticket', 'Cabin'], axis=1)\ntest_df = test_df.drop(['Ticket', 'Cabin'], axis=1)\ncomb_train_test_df = [train_df, test_df]","4f07b71c":"print(f'AFTER SHAPE --> train: {train_df.shape}, test: {test_df.shape}, combine_train: {comb_train_test_df[0].shape}, combine_test: {comb_train_test_df[1].shape}')","557bf243":"for i in comb_train_test_df:\n    i['Title'] = i.Name.str.extract(' ([A-Za-z]+)\\.', expand=True)\n\ncrossed_sex_title = pd.crosstab(train_df.Title, train_df.Sex)\ncrossed_survived_title = pd.crosstab(train_df.Title, train_df.Survived)\nsex_title_surv_df = pd.concat([crossed_sex_title, crossed_survived_title], axis=1, sort=False)\n\nsex_title_surv_df['Survival ratio in %'] = (sex_title_surv_df[1] \/ total*100).round(2)\nsex_title_surv_df.sort_values(by=['Survival ratio in %'])","1e175784":"unique_titles = comb_train_test_df[0].Title.unique().tolist()\nright_titles = ['Mr','Mrs','Miss','Master', 'Ms', 'Mme', 'Mlle']\ntitles_to_replace = [unique_titles.remove(i) for i in right_titles]\ntitles_to_replace = unique_titles\ntitles_to_replace","0c2af371":"for i in comb_train_test_df:\n    i.Title = i.Title.replace(titles_to_replace, 'Rare')\n    i.Title = i.Title.replace('Ms', 'Miss')\n    i.Title = i.Title.replace('Mlle', 'Miss')\n    i.Title = i.Title.replace('Mme', 'Mrs')","55e23943":"train_df[['Title', 'Survived']].groupby(['Title'], as_index=False).mean().sort_values(by=['Survived'])","3a0030aa":"title_map = {\"Mr\": 1, \n             \"Miss\": 2, \n             \"Mrs\": 3, \n             \"Master\": 4, \n             \"Rare\": 5,}","dd13be07":"for i in comb_train_test_df:\n    i.Title = i.Title.map(title_map)","6b8c335e":"train_df.isna().sum()","a9ef3a7d":"train_df = train_df.drop(['Name', 'PassengerId'], axis=1)\ntest_df = test_df.drop(['Name', 'PassengerId'], axis=1) \ncomb_train_test_df = [train_df, test_df]\nprint(comb_train_test_df[0].shape, comb_train_test_df[1].shape) # test_df is smaller because it hasn't got a Survived column","1b3e9c30":"for i in comb_train_test_df:\n    i.Sex = i.Sex.map({'female': 1, \n                       'male': 0,}).astype(int)","4181c9eb":"sns.heatmap(train_df.corr())","895d07c1":"train_df.corr().sort_values(by=['Age'])","e20535f6":"train_df[['Age', 'Pclass']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Age', ascending=False)","db0c9a40":"g = sns.FacetGrid(train_df, row='Pclass', col='Sex', height=2, aspect=1.6)\ng.map(plt.hist, 'Age', alpha=.5, bins=20)\ng.add_legend()","1f229719":"train_df[['Age', 'Pclass', 'Sex']].groupby(['Pclass', 'Sex'], as_index=False).mean().sort_values(by='Age', ascending=False)","927a2f7f":"train_df[['Age', 'Pclass', 'Sex']]","d322447b":"new_ages = np.zeros((2,3)) # because we want to append 6 numbers\n\nfor i in comb_train_test_df:\n    for m in range(0,2): # because Sex has 2 unique values\n        for n in range(0,3): # because Pclass has 3 unique values\n            guess_df = i[(i.Sex == m) & (i.Pclass == n+1)]['Age'].dropna() # n+1 because Pclass starts from 1 not 0\n            new_age = int(round(guess_df.mean(),0))\n            new_ages[m,n] = new_age\n            \n    for m in range(0,2):\n        for n in range(0,3):\n            i.loc[(i.Age.isnull()) & (i.Sex == m) & (i.Pclass == n+1), 'Age'] = new_ages[m,n]\n            ","5e17eb08":"train_df","2c704f85":"for i in comb_train_test_df:\n    i['Family']= i['SibSp'] + i['Parch'] + 1 # 1 has been added to include passenger in family size","00eefe01":"train_df[['Family', 'Survived']].groupby(['Family'], as_index=False).mean().sort_values(by=['Survived'], ascending=False)","264ae343":"for i in comb_train_test_df:\n    i['IsAlone'] = 0\n    i.loc[i['Family'] == 1, 'IsAlone'] = 1","96061df6":"train_df[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","318b9610":"train_df = train_df.drop(['Parch', 'SibSp', 'Family'], axis=1)\ntest_df = test_df.drop(['Parch', 'SibSp', 'Family'], axis=1)\ncomb_train_test_df = [train_df, test_df]","cc9b1ee3":"train_df","3b18d8b7":"train_df['AgeBand'] = pd.cut(train_df['Age'], 5)\ntrain_df[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by=['AgeBand'], ascending=True)","37cb6f4c":"for i in comb_train_test_df:\n    i.loc[i.Age <= 16, 'Age'] = 0\n    i.loc[(i.Age > 16) & (i.Age <= 32), 'Age'] = 1\n    i.loc[(i.Age > 32) & (i.Age <= 48), 'Age'] = 2\n    i.loc[(i.Age > 48) & (i.Age <= 64), 'Age'] = 3\n    i.loc[i.Age > 64, 'Age'] = 4\n\ntrain_df = train_df.drop(['AgeBand'], axis=1)\ncomb_train_test_df = [train_df, test_df]\ntrain_df.head()","c99629b8":"train_df['FareBand'] = pd.cut(train_df['Fare'], 4)\ntrain_df[['FareBand', 'Survived']].groupby(['FareBand'], as_index=False).mean().sort_values(by=['FareBand'], ascending=True)","03d98b7b":"for i in comb_train_test_df:\n    i.loc[i.Fare <= 128, 'Fare'] = 0\n    i.loc[(i.Fare > 128) & (i.Fare <= 256), 'Fare'] = 1\n    i.loc[(i.Fare > 256) & (i.Fare <= 384), 'Fare'] = 2\n    i.loc[i.Fare > 384, 'Fare'] = 3\n\ntrain_df = train_df.drop(['FareBand'], axis=1)\ncomb_train_test_df = [train_df, test_df]\ntrain_df.head()","93c46576":"train_df.isnull().sum()","7ca5e265":"for i in comb_train_test_df:\n    i['Embarked'] = i['Embarked'].fillna(train_df.Embarked.dropna().mode()[0])","b9c723ef":"train_df.isnull().sum()","182ff0c0":"train_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by=['Survived'], ascending=False)","0f88d22e":"train_df","efbe8d79":"for i in comb_train_test_df:\n    i.Embarked = i.Embarked.map({'S': 0, 'C': 1, 'Q': 2}).astype(int)","9fd7b75a":"train_df","80720301":"test_df.isnull().sum()","86daaae1":"test_df['Fare'].fillna(test_df['Fare'].dropna().median(), inplace=True)","b156ebf6":"test_df.isnull().sum()","face817e":"test_df.corr()","7ee8c0eb":"test_df[test_df.Title.isnull()]","28d95bd2":"test_df[['Pclass', 'Sex', 'Age', 'Fare', 'Embarked', 'Title', 'IsAlone']].groupby(['Pclass', 'Title', 'Sex'], as_index=False).mean().sort_values(by=['Title', 'Sex'], ascending=False)","8751877f":"s = pd.Series(['Option 1', # index no 2\n               'Option 2', # index no 1\n               'Values from train_df', \n               'Diff between Option1 and values from train_df',  \n               'Diff between Option2 and values from train_df',\n              ])\n\noption1 = [1, 1, 2.228571, 0.342857, 0.600000, 3.0, 0.171429]\noption2 = [1, 1, 1.500000, 0.857143, 0.571429, 2.0, 0.785714]\ntrain_row = [1, 1, 2, 0, 1, np.nan, 1]\ndiff_option1 = np.subtract(option1, train_row)\ndiff_option2 = np.subtract(option2, train_row)\n\ndiff_df = pd.DataFrame(np.array([\n            option1, \n            option2,\n            train_row,\n            diff_option1,\n            diff_option2,\n            ]), \n            columns=train_df.drop('Survived', axis=1).columns)\ndiff_df.set_index([s])","7173f14e":"print(f'Total sum of diff for option1: {np.nansum(abs(diff_option1))-3} and for option2: {np.nansum(abs(diff_option2))-2}')\n","78ada727":"test_df['Title'].fillna(2, inplace=True)","2c0f1cfd":"test_df.Title.isnull().sum()","1a5bc33e":"X_train = train_df.drop(\"Survived\", axis=1)\ny_train = train_df['Survived']\nX_test = test_df\nX_train.shape, y_train.shape, X_test.shape","efeef956":"logreg = LogisticRegression()\nlogreg.fit(X_train, y_train)\ny_pred = logreg.predict(X_test)\nacc_logreg = logreg.score(X_train, y_train)","e489e351":"print(f\"Accuracy for Logistic Regression: {round(acc_logreg, 4)*100}%\")","bfc5f48e":"coeff_df = pd.DataFrame(train_df.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df['Correlation'] = pd.Series(logreg.coef_[0])\ncoeff_df.sort_values(by=['Correlation'], ascending=False)","1e024ec7":"svc = SVC()\nsvc.fit(X_train, y_train)\ny_pred = svc.predict(X_test)\nacc_svc = svc.score(X_train, y_train)","fec59106":"print(f\"Accuracy for Support Vector Machines: {round(acc_svc, 4)*100}%\")","d3065bf1":"knn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\ny_pred = knn.predict(X_test)\nacc_knn = knn.score(X_train, y_train)","e23ca3bd":"print(f\"Accuracy for K-Nearest Neighbors: {round(acc_knn, 4)*100}%\")","9a62f296":"dec_tree = DecisionTreeClassifier()\ndec_tree.fit(X_train, y_train)\ny_pred = dec_tree.predict(X_test)\nacc_dec_tree = dec_tree.score(X_train, y_train)","8c202cfe":"print(f\"Accuracy for Decision Tree: {round(acc_dec_tree, 4)*100}%\")","a4ff5266":"rand_for = RandomForestClassifier(n_estimators=100)\nrand_for.fit(X_train, y_train)\ny_pred = rand_for.predict(X_test)\naac_rand_for = rand_for.score(X_train, y_train)","2471644c":"print(f\"Accuracy for Random Forest: {round(aac_rand_for, 4)*100}%\")","902c3789":"models = pd.DataFrame({\n            'Model': ['Logistic Regression', 'Support Vector Machines', 'K-Nearest Neighbors', 'Decision Tree', 'Random Forest'],\n            'Score': [acc_logreg, acc_svc, acc_knn, acc_dec_tree, aac_rand_for]})\nmodels.sort_values(by='Score', ascending=False)","04d9af79":"### 5.6 Completing a categorical feature","c9172e3b":"Firstly, we need to split train and test sets into X and y. ","5ad02027":"The best survival ratio have passengers that had 2,3 or 4 family members aboard. Because survival ratio varies according to family size we try to create another feature called IsAlone.","4759ee14":"Accuracy has been improved by changing continuous features Age and Fare into ordinal ones. Previous accuracy outcome was 68.24%. ","85e45763":"### 6.2 Support Vector Machines","727c3e44":"As Embarked is missing only 2 data points (0.22%), it will be replaced by mode.","abcc46b4":"## 3. Analyse by pivoting features","89a06cb9":"### 6.5 Random Forest","9098973e":"As we don't need Name column anymore, we can drop it. We drop PassengerId column (reason in 2.7.3) as well.","48b7718b":"## 6. Model, predict and solve\n","015fc8cc":"Restoring default values for Age and Fare caused accuracy equals 98.32%.","002a01c3":"Ports Southampton and Queenstown have better survival ratio for women, especially in 1st and 2nd classes. Port Cherbourg was more gracious for men, particularly 2nd class. \n\nAssumption for hypothesis that 1st class was better when it comes to survival ratio is confirmed for most cases.\n\n**Conslusion**: Pclass and Embarked should be included in training model.\n","378dbe25":"Port Queenstown has very similar survival rate when it comes to sex. Port Cherbourg was finally the best for people who paid the most (not much better for women).\n\nGenerally, the higher price of ticket, the better survival ratio. \n\n**Conlusion**: Fare should be included in training model.","53a562a7":"### 2.6. Look at the distribution of categorical features","4db11e11":"### 6.1 Logistic Regression","55a80ade":"### 2.3. Check for features that may contain errors or typos","1b617fee":"### 5.4 Completing numerical continuous features\nAge with 19.87% of missing values will be replaced by using correlated features. The alternatives are using a mean, standard deviation etc. or combine different methods.","ed53f268":"The best survival ratio had women with Miss and Mrs titles (together 25.36%). \nIn order to increase number of many titles, some rare title will be replaced by 'Rare'.","42bdf1ba":"**Conclusion**: \n* The higher Sex (i.e. female), the higher probabiblity of being survived.\n* The higher Pclass, the lower probabiblity of being survived.\n* The higher Age, the lower probabiblity of being survived.\n* Title turned out to be a good artificial features as it has 4th correlation rate.","0665c5be":"As we can see that Title is strongly correlated with Sex and IsAlone.","5535c5a2":"The problem concerns classification domain and tries to answer the question \"*Will a passenger survive or not?*\". Hence, we can use plenty of various ML algorithms:\n* Logistic Regression\n* K-Nearest Neighbors\n* Support Vector Machines\n* Naive Bayes classifier\n* Decision Tree\n* Random Forrest\n* ...","de428705":"### 5.3 Converting a categorical feature\nIt's required by most models. Sex's male and female will be replaced by 0 and 1 respectively.","716f1bc6":"## 1. Acquire data\n### 1. Convert datasets into Pandas dataframes and combine them to run certain operations","66765291":"Not duplicated: PassengerId and Name.\n\nSome of features have few unique (not duplicated) values and may be considered as potential duplicates. \n\n* For few unique values: **If amount of not duplicated values and duplicates bring the total number to length of dataset, the feature is not duplicated.** \nHence, not duplicated features are: \n    * Survived\n    * Pclass\n    * Sex\n    * Age (since there are only 88 unique values, the duplicates hypothesis can be rejected)\n    * SibSp\n    * Parch\n    * Embarked (total sum of uniques (3) and dupes (887) is 890, so it means that this feature is not duplicated but some datapoint is missing)\n* For many unique values: Feature may be duplicated.","3b6caf9c":"### 2.4. Check for features that may contain blank, null or empty values**","dc8c05a7":"### 2.7. Assumptionss based on data analysis\n\n#### 2.7.1 Correlating\n\n#### 2.7.2 Completing features that correlate with \"Survived\"\n* \"Age\" feature (19.87% datapoints are missing) --> It's a significant number of missing values but \"Age\" has probably relevant impact on Survived label.\n* \"Embarked\" feature (2 datapoints are missing (0.22%))\n\n#### 2.7.3 Correcting\n* Drop \"Ticket\" feature as it has a high ratio of duplicates (23.57%) and ticket probably is not correlated with Survived.\n* Drop \"Cabin\" feature as it's highly incompleted (77.1% of null values).\n* Drop features that not contributes to \"Survived\": PassengerId, Name.\n\n#### 2.7.4 Creating\n* Create new feature called \"Family\" based on SibSp and Parch to get to know how family members affect survival ratio. \n* Engineer Name feature to extract \"Title\" as a new feature.\n* Create new features for Age and Fare where instead of continuous values there will be ranges.\n\n#### 2.7.5 Classifying\n1. Children like enough are more likely to survive.\n2. Probably women are more likely to survive.\n3. Passengers of first class are more likely to survive.\n","4dee5077":"* PassengerId --> There are 891 passengers.\n* Survived (1 yes, 2 no) --> There are 38% survival ratio.\n* Pclass (1 = 1st, 2 = 2nd, 3 = 3rd) --> Average of a ticket class is 2.\n* Age --> Average age is 29. The youngest passenger is five-month-old child (0.42) and the oldest one is 80 years old.\n* SibSp --> Average number of siblings\/spouses aboard is 0.5 what means that on average, for 2 people there are 1 sibling or spouse aboard as well.\n* Parch --> Average number of parents\/children aboard is 0.38. 75% of passengers don't have any parent or children aboard.\n* Fare --> Average passenger fare costs 32 dollars (the most expensive - 512.33 dollars and the cheapest one - 0 dollars).","f50ff7d0":"To check for the assumptions for classifying (take a look at point 2.7.5) we get to know what is survival ratio for these features.","98278c21":"### 5.2 Creating new features\nWe would like to extract title from Name feature to get more information when it comes to Survived. \nWe are performing it for both training and testing sets.","c16a6086":"In order to prevent data leakage, newly created features will be excluded.","154c62bc":"Accuracy has been improved by changing continuous features Age and Fare into ordinal ones. Previous accuracy outcome was 80.81%. \nNow we can check coefficients.","81c2b278":"Test set has fewer columns than train test because of excluded \"Survived\" column.","5c321e6b":"It's seem that there are not many outliers among numerical variables since PassengerId, Survived and Pclass don't have any outlier dots. Outliers in Age are in correct age range (0-80), so they cannot be classified as outliers. SibSp and Parch look good as well - it's possible to have many family members aboard. ","b152867a":"## 2. Analyse by descibing data\n### 2.1. Preview the data and get columns names","be7961e3":"Since we have only 5 unique titles, feature Title can be converted to ordinal values in ascending order.","34b491fa":"### 2.2. Features datatypes","a71167b9":"We do the same with Fare feature.","5a382562":"### 2.5. Look at the distribution of numerical features","e4eb2088":"Option2 has lesser difference and Title = 2, so our missing data point will be replace by 2.","f53f8438":"Embarked feature is last categorical column. It will be converted to numerical.","15f7b13e":"### 2.2.1. Categorical and mixed datatype features","34f661ba":"### 6.3 K-Nearest Neighbors","7b2a1c0a":"### 5.5 Creating new features based on existing features\nWe can create new feature Family using SibSp (number of siblings\/spouses aboard) and Parch (number of parents\/children aboard). This will allow to see how having a family aboard helps with survival ratio.","4339c2ef":"Assumption for impact of Age have been confirmed. There's seemingly that on average, children until 2 years have been survived. Adults between 18-38-year-old and 80 year olds have high ratio of survival.\n\n**Conclusion**: Age should be considered for model training.","4b2b5788":"Assumptions for impact of Sex and Pclass have been confirmed. \n\n**Conclusion**: Sex and Pclass should be considered for model training.","ac7a02ed":"The best models turned out: **Decision Tree** and **Random Forest**. ","3809736c":"Generally, the better class, the higher age. In all classes, on average, women are younger than men.","ee1863d6":"### 6.4 Decision Tree","fc456a39":"We can draw a conclusion that having a family aboard is increasing chance for survive.","f7e39652":"We checked if Title column doesn't have null values. ","c116340e":"## 5. Wrangle data\n### 5.1 Correcting by dropping features\nAs we thought in 2.7.3 columns: Ticket and Cabin should be dropped because of high ratio of duplicates and plenty of null values. The columns will be removed from both training and testing sets to stay them consistent.\n\nColumns Name and PassengerId will remain becuase they will be useful for creating new features.","fb4fdd0c":"* Name --> All names are unique and are not duplicated.\n* Sex --> There are 2 types of sex and the most pouplar gender is male (65%).\n* Ticket --> There are 681 unique type of tickets and the most popular one (7 passengers) is \"1601\".\n* Cabin --> There are many missing values but based on data points for 23% passengers we can draw a conlussion that the most popular cabin number was G6 (4 passengers).\n* Embarked (C = Cherbourg, Q = Queenstown, S = Southampton) --> Based on data for above 99% of passengers we can conclude that the most popular port was Southampton.","69df0201":"Restoring default values for Age and Fare caused accuracy equals 98.32%.","14eb08c4":"Age has a significant correlation with Pclass (-0.369226).","3d96f91e":"### 2.2.2. Numerical features","52fec85c":"### 5.7 Converting categorical feature into numerical","93ec294d":"### 5.8 Checking test set","440806f3":"## 4. Analyse by visualizing data","ecfef970":"Hence \"Age\" contains continuously data points, the better result will give checking for correlation on histogram."}}