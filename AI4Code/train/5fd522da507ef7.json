{"cell_type":{"c08c86b4":"code","41f97ad6":"code","0fa1b8bc":"code","e6f4ee97":"code","42a6f74d":"code","c3d1eabe":"code","f0f816e5":"code","74e2c1f8":"code","23a37321":"code","55558143":"code","d32c3a8c":"code","2ec1b316":"code","41adda6a":"code","dd2d5270":"code","f04e3be2":"code","1bf758ee":"code","b5ad7df3":"code","a3c80817":"code","f77c4e67":"code","7a05db19":"code","d62e7998":"code","10d00ebb":"code","a48e9981":"code","dd8b00f2":"code","ef684b3b":"code","304a7f89":"code","5f571da4":"code","409ad175":"code","a8341f7e":"code","5b15d462":"code","dcf49df0":"code","cee87d0a":"code","504554f5":"code","d85d1daa":"code","9e9881a9":"code","48518a31":"code","594e456d":"code","03a9c585":"code","48db27eb":"markdown","e3f0b3b1":"markdown","78a2f09e":"markdown","6e2cd108":"markdown","cafb1cbb":"markdown","20f9880d":"markdown","5633f08e":"markdown","50e90a14":"markdown","bb725038":"markdown","ca527ced":"markdown","58f22db7":"markdown","724a5a7c":"markdown","fe30a8b0":"markdown","1e67421b":"markdown","258f0ba7":"markdown","64c7241b":"markdown","684f5718":"markdown","acb56648":"markdown","c71b1765":"markdown","66c39e7e":"markdown","8fbcee7a":"markdown","54c9336f":"markdown","18f09c90":"markdown","198d9cbf":"markdown","db6894b0":"markdown","23031a9b":"markdown","95978b1a":"markdown","15421494":"markdown","f9e4c4c1":"markdown","31350872":"markdown","e64dcf44":"markdown","31137066":"markdown","60cd0143":"markdown","9f62e201":"markdown","ff424a99":"markdown","da8f4896":"markdown","539b90c4":"markdown","318bfe2c":"markdown","760ba2b6":"markdown","8974f4e9":"markdown","c5a00017":"markdown","7991fd05":"markdown","bebd21db":"markdown","6088c0b3":"markdown","250af84b":"markdown","1934986a":"markdown","6f8b5939":"markdown","ee52aec7":"markdown","1bd2e07f":"markdown","54b1a4c5":"markdown","db8e20ec":"markdown"},"source":{"c08c86b4":"# Basic Libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sb\nimport matplotlib.pyplot as plt # we only need pyplot\nsb.set() # set the default Seaborn style for graphics\n\n%matplotlib inline\nimport itertools\n\nfrom PIL import Image\nimport PIL\n\nimport warnings\nimport plotly.offline as py\npy.init_notebook_mode() # show output even if offline\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff","41f97ad6":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n\ncancerdata = pd.read_csv(\"\/kaggle\/input\/data.csv\")\ncancerdata.head()","0fa1b8bc":"cancerdata.info()","e6f4ee97":"cancerdata.drop('description', axis = 1, inplace = True)\n\n","42a6f74d":"cancerdata.head()","c3d1eabe":"cancerdata.describe()","f0f816e5":"cancerdata.info()","74e2c1f8":"\ndiagnosis = pd.DataFrame(cancerdata['diagnosis'])\n#to have 2 diagnosis format B&M and 0&1, diagnosis_bi stands for diagnosis_binary\ndiagnosis_bi = diagnosis.replace(to_replace = dict(M = 1, B = 0), inplace = False)\ncancerdata['diagnosis_bi'] = diagnosis_bi\n\nsb.countplot(cancerdata[\"diagnosis\"])\n\n\n","23a37321":"trace = go.Pie(labels = ['benign','malignant'], values = cancerdata['diagnosis'].value_counts(), \n               textfont=dict(size=12), opacity = 1,\n               marker=dict(colors=['green', 'red'], \n                           line=dict(color='#000000', width=1.5)))\n\n\nlayout = dict(title =  'Distribution of diagnosis variable')\n           \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)","55558143":"# Import all mean, se, worst data and all\nmeans = pd.DataFrame(cancerdata.loc[:,\"radius_mean\":\"fractal_dimension_mean\"])\n\nse = pd.DataFrame(cancerdata.loc[:,\"radius_se\":\"fractal_dimension_se\"])\n\n    \nworst = pd.DataFrame(cancerdata.loc[:,\"radius_worst\":\"fractal_dimension_worst\"])\n\nall_variables = pd.DataFrame(cancerdata.loc[:,\"radius_mean\":\"fractal_dimension_worst\"])\n    \n    # Draw the distributions of all \nf, axes = plt.subplots(30, 3, figsize=(18, 120))\n\ncount = 0\nfor var in all_variables:\n    sb.boxplot(all_variables[var], orient = \"h\", ax = axes[count,0],color=\"r\")\n    sb.distplot(all_variables[var], ax = axes[count,1],color=\"g\")\n    sb.violinplot(all_variables[var], ax = axes[count,2])\n    count += 1","d32c3a8c":"meansDF = pd.concat([diagnosis, means], axis = 1 )\n\nf, axes = plt.subplots(10, 1, figsize=(18, 40))\n\ncount = 0\nfor var in means:\n    sb.swarmplot(x = var, y = \"diagnosis\", data = meansDF, orient = \"h\", ax = axes[count])\n    count += 1","2ec1b316":"seDF = pd.concat([diagnosis, se], axis = 1 )#,join_axes= [y_train.index]\n\nf, axes = plt.subplots(10, 1, figsize=(18 , 40))\n\ncount = 0\nfor var in se:\n    sb.swarmplot(x = var, y = \"diagnosis\", data = seDF, orient = \"h\", ax = axes[count])\n    count += 1","41adda6a":"worstDF = pd.concat([diagnosis, worst], axis = 1 )#,join_axes= [y_train.index]\n\nf, axes = plt.subplots(10, 1, figsize=(18 , 40))\n\ncount = 0\nfor var in worst:\n    sb.swarmplot(x = var, y = \"diagnosis\", data = worstDF, orient = \"h\", ax = axes[count])\n    count += 1","dd2d5270":"\n\nmatrix = all_variables.corr()\n\nmatrix_col = matrix.columns.tolist()\n\nmatrix_array = np.array(matrix)\n\n#print(variables_data.corr())\n\ntrace = go.Heatmap(z = matrix_array,\n                   x = matrix_col,\n                   y = matrix_col,\n                   xgap = 2,\n                   ygap = 2,\n                   colorscale='agsunset',\n                   colorbar   = dict() ,\n                  )\nlayout = go.Layout(dict(title = 'Correlation Matrix for variables',\n                        autosize = False,\n                        height  = 870,\n                        width   = 950,\n                        margin  = dict(r = 0 ,l = 35,\n                                       t = 70,b = 210,\n                                     ),\n                        yaxis   = dict(tickfont = dict(size = 14)),\n                        xaxis   = dict(tickfont = dict(size = 14)),\n                       )\n                  )\nfig = go.Figure(data = [trace],layout = layout)\npy.iplot(fig)","f04e3be2":"# Heatmap of the Correlation Matrix\nsb.set(font_scale=5)  #font size of coloumns and rows\nf, axes = plt.subplots(1, 1, figsize=(80, 80))\nall_heatmap = sb.heatmap(all_variables.corr(), vmin = -1, vmax = 1, linewidths = 1,\n           annot = True, fmt = \".2f\", annot_kws = {\"size\": 50}, cmap = \"RdYlBu_r\")\n\nfigure = all_heatmap.get_figure()\n#figure.savefig('all_heatmap.png',dpi=25) #save heatmap as png","1bf758ee":"import matplotlib.patches as mpatches\n\n#Set legend discription\nB1=mpatches.Patch(color='green',label='Benign')\nM1=mpatches.Patch(color='firebrick',label='Malignant')\n\n\ncorrelated_1 = pd.DataFrame(cancerdata[[\"diagnosis_bi\", \"radius_mean\", \"perimeter_worst\"]])\ncorrelated_2 = pd.DataFrame(cancerdata[[\"diagnosis_bi\", \"perimeter_mean\", \"area_worst\"]])\ncorrelated_3 = pd.DataFrame(cancerdata[[\"diagnosis_bi\", \"radius_worst\", \"area_mean\"]])\ncorrelated_4 = pd.DataFrame(cancerdata[[\"diagnosis_bi\", \"area_mean\", \"area_worst\"]])\n","b5ad7df3":"sb.set(font_scale=1.5)  #font size of coloumns and rows\nf, axes = plt.subplots(2, 2, figsize=(24 ,24))\n\naxes[0, 0].scatter(x = \"radius_mean\", y = \"perimeter_worst\", cmap = 'RdYlGn_r', c = 'diagnosis_bi', data = correlated_1)\naxes[0, 0].set_xlabel('radius_mean')\naxes[0, 0].set_ylabel('perimeter_worst')\naxes[0, 1].scatter(x = \"perimeter_mean\", y = \"area_worst\", cmap = 'RdYlGn_r', c = 'diagnosis_bi' , data = correlated_2)\naxes[0, 1].set_xlabel('perimeter_mean')\naxes[0, 1].set_ylabel('area_worst')\naxes[1, 0].scatter(x = \"radius_worst\", y = \"area_mean\", cmap = 'RdYlGn_r', c = 'diagnosis_bi' , data = correlated_3)\naxes[1, 0].set_xlabel('radius_worst')\naxes[1, 0].set_ylabel('area_mean')\naxes[1, 1].scatter(x = \"area_mean\", y = \"area_worst\", cmap = 'RdYlGn_r', c = 'diagnosis_bi' , data = correlated_4)\naxes[1, 1].set_xlabel('area_mean')\naxes[1, 1].set_ylabel('area_worst')\n\naxes[0, 0].legend(handles=[B1,M1])\naxes[0, 1].legend(handles=[B1,M1])\naxes[1, 0].legend(handles=[B1,M1])\naxes[1, 1].legend(handles=[B1,M1])","a3c80817":"uncorrelated_1 = pd.DataFrame(cancerdata[[\"diagnosis_bi\", \"fractal_dimension_worst\", \"area_mean\"]])\nuncorrelated_2 = pd.DataFrame(cancerdata[[\"diagnosis_bi\", \"fractal_dimension_worst\", \"radius_se\"]])\nuncorrelated_3 = pd.DataFrame(cancerdata[[\"diagnosis_bi\", \"texture_mean\", \"smoothness_worst\"]])\nuncorrelated_4 = pd.DataFrame(cancerdata[[\"diagnosis_bi\", \"texture_mean\", \"symmetry_se\"]])\n\nf, axes = plt.subplots(2, 2, figsize=(24 ,24))\nsb.set(font_scale=1.5)  #font size of coloumns and rows\naxes[0, 0].scatter(x = \"fractal_dimension_worst\", y = \"area_mean\", cmap = 'RdYlGn_r', c = 'diagnosis_bi', data = uncorrelated_1)\naxes[0, 0].set_xlabel('fractal_dimension_worst')\naxes[0, 0].set_ylabel('area_mean')\naxes[0, 1].scatter(x = \"fractal_dimension_worst\", y = \"radius_se\", cmap = 'RdYlGn_r', c = 'diagnosis_bi' , data = uncorrelated_2)\naxes[0, 1].set_xlabel('fractal_dimension_worst')\naxes[0, 1].set_ylabel('radius_se')\naxes[1, 0].scatter(x = \"texture_mean\", y = \"smoothness_worst\", cmap = 'RdYlGn_r', c = 'diagnosis_bi' , data = uncorrelated_3)\naxes[1, 0].set_xlabel('texture_mean')\naxes[1, 0].set_ylabel('smoothness_worst')\naxes[1, 1].scatter(x = \"texture_mean\", y = \"symmetry_se\", cmap = 'RdYlGn_r', c = 'diagnosis_bi' , data = uncorrelated_4)\naxes[1, 1].set_xlabel('texture_mean')\naxes[1, 1].set_ylabel('symmetry_se')\n\naxes[0, 0].legend(handles=[B1,M1])\naxes[0, 1].legend(handles=[B1,M1])\naxes[1, 0].legend(handles=[B1,M1])\naxes[1, 1].legend(handles=[B1,M1])","f77c4e67":"#ne_correlated = negative correlation\nne_correlated_1 = pd.DataFrame(cancerdata[[\"diagnosis_bi\", \"fractal_dimension_mean\", \"perimeter_worst\"]])\nne_correlated_2 = pd.DataFrame(cancerdata[[\"diagnosis_bi\", \"radius_worst\", \"smoothness_se\"]])\nne_correlated_3 = pd.DataFrame(cancerdata[[\"diagnosis_bi\", \"fractal_dimension_mean\", \"radius_mean\"]])\nne_correlated_4 = pd.DataFrame(cancerdata[[\"diagnosis_bi\", \"fractal_dimension_mean\", \"area_worst\"]])\n\nf, axes = plt.subplots(2, 2, figsize=(24 ,24))\nsb.set(font_scale=1.5)  #font size of coloumns and rows\naxes[0, 0].scatter(x = \"fractal_dimension_mean\", y = \"perimeter_worst\", cmap = 'RdYlGn_r', c = 'diagnosis_bi', data = ne_correlated_1)\naxes[0, 0].set_xlabel('fractal_dimension_mean')\naxes[0, 0].set_ylabel('perimeter_worst')\naxes[0, 1].scatter(x = \"radius_worst\", y = \"smoothness_se\", cmap = 'RdYlGn_r', c = 'diagnosis_bi' , data = ne_correlated_2)\naxes[0, 1].set_xlabel('radius_worst')\naxes[0, 1].set_ylabel('smoothness_se')\naxes[1, 0].scatter(x = \"fractal_dimension_mean\", y = \"radius_mean\", cmap = 'RdYlGn_r', c = 'diagnosis_bi' , data = ne_correlated_3)\naxes[1, 0].set_xlabel('fractal_dimension_mean')\naxes[1, 0].set_ylabel('radius_mean')\naxes[1, 1].scatter(x = \"fractal_dimension_mean\", y = \"area_worst\", cmap = 'RdYlGn_r', c = 'diagnosis_bi' , data = ne_correlated_4)\naxes[1, 1].set_xlabel('fractal_dimension_mean')\naxes[1, 1].set_ylabel('area_worst')\n\naxes[0, 0].legend(handles=[B1,M1])\naxes[0, 1].legend(handles=[B1,M1])\naxes[1, 0].legend(handles=[B1,M1])\naxes[1, 1].legend(handles=[B1,M1])","7a05db19":"# Import KMeans from sklearn.cluster\nfrom sklearn.cluster import KMeans\nfrom sklearn.metrics import accuracy_score\n\n\ncorrelated = pd.DataFrame(cancerdata[[\"radius_mean\", \"perimeter_worst\", \"perimeter_mean\", \"area_worst\", \"radius_worst\", \n                                      \"area_se\", \"perimeter_se\", \"radius_se\", \"concavity_mean\", \"concave_points_mean\", \"area_mean\"]])\n\n\n# Vary the Number of Clusters\nmin_clust = 1\nmax_clust = 40\ninit_algo = 'k-means++'\n\n\n# Compute Within Cluster Sum of Squares\nwithin_ss = []\nfor num_clust in range(min_clust, max_clust+1):\n    kmeans = KMeans(n_clusters = num_clust, init = init_algo, n_init = 5)\n    kmeans.fit(correlated)\n    within_ss.append(kmeans.inertia_)\n    \nsb.set(font_scale=1)  #font size of coloumns and rows\n\n# Angle Plot : Within SS vs Number of Clusters\nf, axes = plt.subplots(1, 1, figsize=(16,4))\nplt.plot(range(min_clust, max_clust+1), within_ss)\nplt.xlabel('Number of Clusters')\nplt.ylabel('Within Cluster Sum of Squares')\nplt.xticks(np.arange(min_clust, max_clust+1, 1.0))\nplt.grid(which='major', axis='y')\nplt.show()","d62e7998":"\n\n# Set \"optimal\" Clustering Parameters\nnum_clust = 2\ninit_algo = 'k-means++'\n\n# Create Clustering Model using KMeans\nkmeans = KMeans(n_clusters = num_clust,         \n               init = init_algo,\n               n_init = 20)                 \n\n# Fit the Clustering Model on the Data\nkmeans.fit(correlated)","10d00ebb":"# Print the Cluster Centers\nprint(\"Features\", \"\\tradius_mean\", \"\\tperimeter_worst\", \"\\tperimeter_mean\", \"\\tarea_worst\", \"\\tradius_worst\", \n      \"\\tarea_se\", \"\\tperimeter_se\", \"\\tradius_se\", \"\\tconcavity_mean\", \"\\tconcave_points_mean\", \"\\tarea_mean\")\n\nfor i, center in enumerate(kmeans.cluster_centers_):\n    print(\"Cluster\", i, end=\":\\t\")\n    for coord in center:\n        print(round(coord, 2), end=\"\\t\\t\")\n    print()\nprint()\n\n# Print the Within Cluster Sum of Squares\nprint(\"Within Cluster Sum of Squares :\", kmeans.inertia_)\nprint()\n\n# Predict the Cluster Labels\nlabels = kmeans.predict(correlated)\n\n# Append Labels to the Data\ncorrelated_labeled = correlated.copy()\ncorrelated_labeled[\"Cluster\"] = pd.Categorical(labels)\n\nsb.set(font_scale=1) \nf, axes = plt.subplots(1, 2, figsize=(12, 4))\n# Summary of the Cluster Labels\nsb.countplot(correlated_labeled[\"Cluster\"],ax = axes[0])\n#heatmap for Kmean\nfrom sklearn.metrics import confusion_matrix\n\nsb.heatmap(confusion_matrix(cancerdata['diagnosis_bi'].values.tolist(), labels.tolist()),\n           annot = True, fmt = 'd', annot_kws = {'size': 18},ax = axes[1])\n\n\n#accuracy score\nkmean_accuracy = accuracy_score(cancerdata['diagnosis_bi'].values.tolist(), labels.tolist())\nprint(\"Accuracy of cluster =\",kmean_accuracy)\n","a48e9981":"# Plot the Clusters on 2D grids\nsb.pairplot(correlated_labeled, vars = correlated.columns.values, hue=\"Cluster\")\n\n#plt.savefig('correlated_pairplot.png',dpi=100) #save image","dd8b00f2":"from sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import plot_precision_recall_curve\nfrom sklearn.datasets import load_wine\nfrom sklearn.model_selection import train_test_split\n\n# split data train 70 % and test 30 %\nx_train, x_test, y_train, y_test = train_test_split(correlated, diagnosis, test_size=0.3, random_state = 100)\n\n","ef684b3b":"def show_metrics():\n    tp = cm_test[1,1]\n    fn = cm_test[1,0]\n    fp = cm_test[0,1]\n    tn = cm_test[0,0]\n    print('For test prediction')\n    print('Accuracy  =     {:.3f}'.format((tp+tn)\/(tp+tn+fp+fn)))\n    print('Precision =     {:.3f}'.format(tp\/(tp+fp)))\n    print('Recall    =     {:.3f}'.format(tp\/(tp+fn)))\n    print('F1_score  =     {:.3f}'.format(2*(((tp\/(tp+fp))*(tp\/(tp+fn)))\/\n                                                 ((tp\/(tp+fp))+(tp\/(tp+fn))))))","304a7f89":"# Import essential models and functions from sklearn\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.tree import export_graphviz\nimport graphviz\nfrom graphviz import Graph\n# Decision Tree using Train Data\ndectree = DecisionTreeClassifier(max_depth = 7, random_state = 100)  # create the decision tree object\ndectree.fit(x_train, y_train)                    # train the decision tree model\n\n# Predict Response corresponding to Predictors\ny_train_pred = dectree.predict(x_train)\ny_test_pred = dectree.predict(x_test)\n\n# Check the Goodness of Fit (on Train Data)\nprint(\"Goodness of Fit of Model \\tTrain Dataset\")\nprint(\"Classification Accuracy \\t:\", dectree.score(x_train, y_train))\nprint()\ndectree_acc=dectree.score(x_test, y_test)\n# Check the Goodness of Fit (on Test Data)\nprint(\"Goodness of Fit of Model \\tTest Dataset\")\nprint(\"Classification Accuracy \\t:\", dectree.score(x_test, y_test))\nprint()\n\n# Plot the Confusion Matrix for Train and Test\n\ncm_train = confusion_matrix(y_train, y_train_pred)\ncm_test = confusion_matrix(y_test, y_test_pred)\n\n\n# Plot the Confusion Matrix for Train and Test\nsb.set(font_scale=1) \nf, axes = plt.subplots(1, 2, figsize=(12, 4))\nsb.heatmap(cm_train,\n           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\nsb.heatmap(cm_test, \n           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n\n\n# Plot the Decision Tree\ntreedot = export_graphviz(dectree,                                      # the model\n                          feature_names = x_train.columns,              # the features \n                          out_file = None,                              # output file\n                          filled = True,                                # node colors\n                          rounded = True,                               # make pretty\n                          special_characters = True)                    # postscript\n\ngraphviz.Source(treedot)\n#graph.format = 'png'\n#graph.render('dtree_render',view=True)#save image","5f571da4":"\nshow_metrics()\n\nplot_roc_curve(dectree,x_test, y_test)\n","409ad175":"from sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix\n\nfrom sklearn.model_selection import KFold\n\n#random forest classifier with n_estimators=100 (default)\nclf_rf = RandomForestClassifier(n_estimators= 40, max_depth = 7, oob_score = True, random_state = 100)      \n\nclr_rf = clf_rf.fit(x_train,y_train.values.ravel())\n\n# Check the Goodness of Fit (on Train Data)\nprint(\"Goodness of Fit of Model \\tTrain Dataset\")\nprint(\"Classification Accuracy \\t:\", clf_rf.score(x_train, y_train))\nprint()\nrf_acc = clf_rf.score(x_test, y_test)\n# Check the Goodness of Fit (on Test Data)\nprint(\"Goodness of Fit of Model \\tTest Dataset\")\nprint(\"Classification Accuracy \\t:\", clf_rf.score(x_test, y_test))\nprint()\n\n\nprint('Out of bag Score is: ', clf_rf.oob_score_) # mean prediction error\n\ny_train_pred = clf_rf.predict(x_train)\ny_test_pred = clf_rf.predict(x_test)\n\ncm_train = confusion_matrix(y_train, y_train_pred)\ncm_test = confusion_matrix(y_test, y_test_pred)\n\n\n# Plot the Confusion Matrix for Train and Test\nsb.set(font_scale=1) \nf, axes = plt.subplots(1, 2, figsize=(12, 4))\nsb.heatmap(cm_train,\n           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\nsb.heatmap(cm_test, \n           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n\n\n","a8341f7e":"show_metrics()\n\nrf_roc = plot_roc_curve(clr_rf,x_test, y_test)\n","5b15d462":"from sklearn.feature_selection import RFECV\nimport plotly.express as px\n\nx2_train, x2_test, y2_train, y2_test = train_test_split(all_variables, diagnosis, test_size=0.3, random_state = 100)\n\nn_features = RandomForestClassifier(n_estimators= 40, max_depth = 7, random_state = 100) \nrfecv = RFECV(estimator=n_features, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation\nrfecv = rfecv.fit(x2_train, y2_train.values.ravel())\nrfecv.ranking_[8]\nrfecv_para =x2_train.columns[rfecv.support_]\n\nprint('Ideal number of features :', rfecv.n_features_)\nprint('Best features :',rfecv_para )\n\n#sb.set(font_scale=1) \n#plt.figure(figsize=(16, 9))\n#plt.title('Recursive Feature Elimination with Cross-Validation', fontsize=18, fontweight='bold', pad=20)\n#plt.xlabel('Number of features selected', fontsize=14, labelpad=20)\n#plt.ylabel('% Correct Classification', fontsize=14, labelpad=20)\n#plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_, color='red', linewidth=3)\n\n#show the graph using plotly\nfig = px.line( x= range(1, len(rfecv.grid_scores_) + 1),y= rfecv.grid_scores_,labels={'x':'n_features', 'y':'scoring'})\n\nfig.update_layout(title='Recursive Feature Elimination with Cross-Validation',\n                   xaxis_title='Number of features selected',\n                   yaxis_title='% Correct Classification',\n                 )\n\n\nfig.show()","dcf49df0":"RFECV_features = pd.DataFrame(cancerdata[rfecv_para])\n\n# split data train 70 % and test 30 %\nx2_train, x2_test, y2_train, y2_test = train_test_split(RFECV_features, diagnosis, test_size=0.3, random_state = 100)\n\n#random forest classifier with n_estimators=10 (default)\nclf_rf_2 = RandomForestClassifier(n_estimators= 40, max_depth = 7, oob_score = True, random_state = 100)      \n\nclr_rf_2 = clf_rf_2.fit(x2_train,y2_train.values.ravel())\n\n\n# Check the Goodness of Fit (on Train Data)\nprint(\"Goodness of Fit of Model \\tTrain Dataset\")\nprint(\"Classification Accuracy \\t:\", clr_rf_2.score(x2_train, y2_train))\nprint()\nrfecv_acc = clr_rf_2.score(x2_test, y2_test)\n# Check the Goodness of Fit (on Test Data)\nprint(\"Goodness of Fit of Model \\tTest Dataset\")\nprint(\"Classification Accuracy \\t:\", clr_rf_2.score(x2_test, y2_test))\nprint()\n\n\nprint('Out of bag Score is: ', clr_rf_2.oob_score_) # mean predicition error\n\ny2_train_pred = clr_rf_2.predict(x2_train)\ny2_test_pred = clr_rf_2.predict(x2_test)\n\ncm_train = confusion_matrix(y2_train, y2_train_pred)\ncm_test = confusion_matrix(y2_test, y2_test_pred)\n\n\n# Plot the Confusion Matrix for Train and Test\nsb.set(font_scale=1) \nf, axes = plt.subplots(1, 2, figsize=(12, 4))\nsb.heatmap(cm_train,\n           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\nsb.heatmap(cm_test, \n           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])","cee87d0a":"show_metrics()\n\nrfecv_roc = plot_roc_curve(clr_rf_2,x2_test, y2_test,name='RFECV')\n","504554f5":"ax=plt.gca()\nplot_roc_curve(clr_rf,x_test, y_test,ax=ax)\nrfecv_roc.plot(ax=ax,name = 'RFECV')","d85d1daa":"cancer_pred = cancerdata.sample(n=5) #select 5random data\ncancer_pred\n\n","9e9881a9":"\ncorrelated_pred = pd.DataFrame(cancer_pred[rfecv_para])\ndiagnosis_pred = clr_rf_2.predict(correlated_pred)\n# Summarize the Actuals and Predictions\ndiagnosis_pred = pd.DataFrame(diagnosis_pred, columns = [\"PredDiagnosis\"], index = cancer_pred.index)\ncancer_correlated_acc = pd.concat([cancer_pred[[\"id\", \"diagnosis\"]], diagnosis_pred], axis = 1)\n\n\n\n# Predict Probabilities corresponding to Predictors\ndiagnosis_prob = clf_rf_2.predict_proba(correlated_pred)\n\n\n\ndiagnosis_prob = pd.DataFrame(list(diagnosis_prob[:,1]), columns = [\"ProDiagnosis\"], index = cancer_pred.index)\ncancer_correlated_conf = pd.concat([cancer_correlated_acc, diagnosis_prob], axis = 1)\n\ncancer_correlated_conf","48518a31":"from sklearn.metrics import confusion_matrix\nfrom sklearn import svm\nfrom sklearn.metrics import f1_score,accuracy_score,recall_score,precision_score\n\n\n\n\nsvc = svm.SVC()  \nsvc.fit(x_train, y_train.values.ravel())                    \n\n                   \n\n# Predict Response corresponding to Predictors\ny_train_pred = svc.predict(x_train)\ny_test_pred = svc.predict(x_test)\n\n# Check the Goodness of Fit (on Train Data)\nprint(\"Goodness of Fit of Model \\tTrain Dataset\")\nprint(\"Classification Accuracy \\t:\", svc.score(x_train, y_train))\nprint()\nsvc_acc = svc.score(x_test, y_test)\n# Check the Goodness of Fit (on Test Data)\nprint(\"Goodness of Fit of Model \\tTest Dataset\")\nprint(\"Classification Accuracy \\t:\", svc.score(x_test, y_test))\nprint()\n\ncm_train = confusion_matrix(y_train, y_train_pred)\ncm_test = confusion_matrix(y_test, y_test_pred)\n\n\n# Plot the Confusion Matrix for Train and Test\nsb.set(font_scale=1) \nf, axes = plt.subplots(1, 2, figsize=(12, 4))\nsb.heatmap(cm_train,\n           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[0])\nsb.heatmap(cm_test, \n           annot = True, fmt=\".0f\", annot_kws={\"size\": 18}, ax = axes[1])\n\n","594e456d":"show_metrics()\n\nplot_roc_curve(svc,x_test, y_test)","03a9c585":"x = [\"Kmean accuracy\",\"Dectree accuracy\",\"RandomForest accuracy\",\"SVC accuracy\",\"RFECV accuracy\"]\ny = [kmean_accuracy,dectree_acc,rf_acc,svc_acc,rfecv_acc]\n\nfig = go.Figure()\n#fig = px.bar(x=x,y=y)\nfig.add_trace(go.Histogram(histfunc=\"sum\", y=y, x=x, name=\"sum\"))\n#fig.update_traces(texttemplate='%{text:.2s}', textposition='outside')\nfig.show()","48db27eb":"### True Positives (TP) - correctly identified as Malignant\n### True Negatives (TN) - correctly identified as Benign\n### False Positives (FP) - Benign incorrectly identified as Malignant\n### False Negatives (FN) - Malignant incorrectly identified as Benign\n\n#### Accuracy (ratio of correctly predicted oberservations to total observations) = TP+TN\/TP+FP+FN+TN \n#### Precision (Ratio of correctly predicited positive observation to total predicited observations aka low false positive rate) = TP\/TP+FP\n#### Recall (ratio of correctly predicted positive observations to the all observations in actual class) = TP\/TP+FN\n#### F1 Score (weighted average of Precision and Recall) = 2*(Recall * Precision) \/ (Recall + Precision)\n","e3f0b3b1":"### 2.5.2 Uncorrelated Features:","78a2f09e":"### The ROC curve shows that there is indeed an improvement to our prediction model from using RFECV. It is important to detect Maglinant tumours early so that it is easier to get treated; if a Maglinant tumour is diagnosed as a Benign tumour instead it would be harder to treat it once it starts to spread.","6e2cd108":"## 2.6 Clustering\n","cafb1cbb":"## 2.5 Correlation\n\n### With the Correlation Matrix, we can see that there are quite a few variables with positive correlation. Lets take a look at some of the positively correlated features: \n","20f9880d":"# 1. Import Data & Cleaning up dataset","5633f08e":"#### From this swarmplot visualization, it can be seen that the mean values of texture, smoothness, symmetry and fractal dimension does not have any correlation with diagnosis results (M\/B) whether it be larger or smaller value.\n\n#### On the other hand, the mean values of radius, perimeter, area, compactness, concavity and concave points does have correlation with diagnosis results, with larger values showing a correlation with Malignant results. These variables are also likely to be correlated with one another.","50e90a14":"### 2.5.1 Postive correlated Feature:","bb725038":"### 27 feature have highest score as shown in graph, We will all 27 variables and see the accuracy as well as the number of false negative and false positives. Although there are some other numbers (11,13,19,20) that perform almost similarly, but our objective is to improve our accuracy for our model as much as possible. so we will use the 27 features as recommended by RFECV.","ca527ced":"## 3.4 Recursive Feature Elimination with Cross Validation (RFECV)\n### To further improve the accuracy of our Random Forest Classifier, we will be using Recursive Feature Elimination with Cross Validation. Recursive Feature Elimination in simple terms is to recursively remove the weakest feature in a model until the specified number of features is reached. Using it with Cross Validation will determine what is the ideal number of features. We will use RFECV to see if it is possible to further improve the accuracy of our prediction model by using only features selected through RFECV.","58f22db7":"## 3.3 Random Forest\n### Before doing up the random forest the following package need to be downloaded in the anaconda command prompt:\nconda install -c r r-randomforest\n\n### We will use the same test and train dataset to fit our random forest predicition model. Random forest has another score called the out of bag score: its a method to measure the prediciton error of the random forest classifier. It is used as replacement for validation when your dataset is too small to split into train, test and validation. ","724a5a7c":"### After doing an exploration on various Machine Learning methods, we have learnt many new things outside of lectures. The most prominent method we have learnt is definitely the random forest classifier, there is so many things to learn about this classifier and what we have done so far is barely scratching the surface for this classifier.\n\n### The SVM that we learnt also could have outperformed the random forest classifier we did. Nevertheless, we are still happy with the prediction model we have completed thus far, having a good accuracy of 0.97 as well as only 1 false positive and 3 false negatives. ","fe30a8b0":"## This heatmap to be used in presentation***","1e67421b":"### A simple way to interpret the ROC curve is if the graph leans more to the upper left corner, the better or the more accurate the model is. We can see that it is very close to the upper left, but lets see if random forest will give us a much better accuracy.","258f0ba7":"# Reference\n\n[1]\"Breast Cancer Wisconsin (Diagnostic) Data Set\", Kaggle.com, 2020. [Online]. Available: https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data. [Accessed: 29- Apr- 2020].\n[2]\"[Classification] Breast Cancer or Not (with 15 ML)\", Kaggle.com, 2020. [Online]. Available: https:\/\/www.kaggle.com\/mirichoi0218\/classification-breast-cancer-or-not-with-15-ml. [Accessed: 29- Apr- 2020].\n[3]\"Feature Selection and Data Visualization\", Kaggle.com, 2020. [Online]. Available: https:\/\/www.kaggle.com\/kanncaa1\/feature-selection-and-data-visualization#Feature-Selection-and-Random-Forest-Classification. [Accessed: 29- Apr- 2020].\n","64c7241b":"### 2.3.3 Worst (Largest mean value) of variables","684f5718":"## 2.3 Visualizing correlation of each variable with Diagnosis (M\/B)\n\n### 2.3.1 Mean of variables","acb56648":"# EE0005 Mini Project\n\n### Group Members: Zhu Yanbiao(U1921778L), Men bunnaroth(U1922018C), Koh Kuang Yuan(U1923821K)\n\n### Data Analysis on Breast Cancer\n### Problem: From 2011 to 2015, breast cancer is the most common cancer among women in Singapore. Over 1,000 women are diagnosed with breast cancer each year and 400 die from the disease. Using the breast cancer wisconsin diagnostic dataset on Kaggle, we want build a prediction model to accurately determine a Malignant tumour. Detecting cancer early is important because it is easier to treat it early on before it starts to spread.\n\n### The dataset we will be using is the Wisconsin Breast Cancer diagnostic Dataset from Kaggle. \n\n","c71b1765":"### 2.3.2 SE (Standard Error) of variables","66c39e7e":"### In this swarm point visualization, the worst, or the largest mean value of the variable is used to visualize the correlation with diagnosis result. With all 3 swarmplots, some conclusion can be gathered from the visualization:\n\n### The mean, standard error and largest mean value of variables radius, perimeter and area have a strong correlation with diagnosis, with larger parameters showing a stronger correlation with Malignant results. With the same behaviour, they are likely to be correlated with one another too.\n\n### The variables of compactness, concavity and concave points might have a correlation with Maglinant results. The mean values of both variables show larger parameters have a correlation with Maglinant results, standard error of both variables show no correlation with Maglinant results and the largest mean value with larger parameters shows a correlation with Malignant results with the exception of outliers. \n\n\n### Texture, Smoothness, Symmetery and fractal dimension does not have any correlation with Malignant results.\n","8fbcee7a":"# 4. Other Prediction Models\n### There is another predicition model we learnt but did not use because it uses alot of algorithm and it takes too much time to do all of them, so we will put it down here to show what we managed to learn.","54c9336f":"### Overview of Data types","18f09c90":"# 3. Prediction Model\n### In this section, we will be using decision tree and random forest as our prediction model. To explain it simply, a random forest is a multitude of decision tree. It is more accurate than a single tree and can prevent overfitting of data. We will use a ROC curve to determine which model have a better performance as well as use precision, recall and F1 score in addition. ","198d9cbf":"### The accuracy of this model, although accurate, still loses to both the Decision Tree and Random Forest Classifier. The number of False negatives predicted is also very high. Maybe other algorithmn will yield us better results. Let's look at the ROC curve of this model;","db6894b0":"###### May need to installed the latest libary to avoid error ","23031a9b":"### The ROC curve has shown that the random forest classifier defintely has a better diagnose ability. Using the random forest classifier as our prediciton model, can we still further improve the accuracy of our prediction model? More importantly; can we still further bring down the number of false positives and false negatives predicted?","95978b1a":"### Out of the 30 variables, we will shorten it to 11 variables to use it to predict whether the patient is diagnosed with a Benign or Malignant cancer. But before we start training our prediction model, let's look at the clustering of these 11 variables.","15421494":"### We ran the prediction many times, and the prediction model got most of the diagnosis correct. However, we did encountered 1 false diagnosis where a Malignant tumour was identified as a Benign tumour. Although this is a very bad diagnosis, we are still satisfied with the performance of our model, as we are able to achieve an accuracy of above 95% out of 569 different patient's sample","f9e4c4c1":"### K-means clustering is an unsupervised machine learning method. It does not require training of a model and is something we learnt from lectures. We can see that K-means clustering is a bad machine learning for our scenario, but is still good to use as part of data exploration. ","31350872":"### We can see that the accuracy for test dataset is at 0.89 - this is actually quite accurate. From the confusion matrix we have 7 false positive and 11 false negative. A ROC curve is a plot of true positive rate against the false postive rate. Lets look at the ROC curve and the 4 scores;","e64dcf44":"## 2.2 Visualizing the spread of Data","31137066":"# 2. Exploratory Analysis\n## 2.1 Diagnosis Distribution (Malignant or Benign)","60cd0143":"### Import and read Data from breast cancer dataset","9f62e201":"### Using only features given by RFECV has made the Random Forest Classifier much more accurate; it has been improved to a score of 0.97, and a slight increase in the out of bag score. The number of false positives and false negatives has also been brought down even further to 1 false positive and 3 false negatives. Let's look at the ROC curve;","ff424a99":"#### In this swarmplot visualization, the standard error of each variables's mean value is used to compare with Malignant results. Only radius_se, perimeter_se and area_se has correlation with Maglinant results. ","da8f4896":"# 5. Conclusion","539b90c4":"## 4.1 Support Vector machines \nSVM are a set of supervised learning methods used for classification, regression and outliers detection.\n\nThe advantages of support vector machines are:\n\n    1. Effective in high dimensional spaces.\n\n    2. Still effective in cases where number of dimensions is greater than the number of samples.\n\n    3. Uses a subset of training points in the decision function (called support vectors), so it is also memory efficient.\n\n    4. Versatile: different Kernel functions can be specified for the decision function. Common kernels are provided, but it is also possible to specify custom kernels.\n\nThe disadvantages of support vector machines include:\n\n    1. If the number of features is much greater than the number of samples, avoid over-fitting in choosing Kernel functions and regularization term is crucial.\n\n    2. SVMs do not directly provide probability estimates, these are calculated using an expensive five-fold cross-validation \n\n### Because there are many algorithm for SVM, we will only use default algorithm Radial basis function(RBF) from SVC(Support Vector Classification) to as our prediction model","318bfe2c":"### With the swarm plot visualization and correlation maxtrix, we now know the correlation variables have with Malignant results as well as with one another. We now have 11 variables to look more in depth to; these 11 variables all have correlation to Malignant results as well as positive correlation with another. These variables are: radius_mean, perimeter_worst, perimeter_mean, area_worst, radius_worst, area_se, perimeter_se, radius_se, concavity_mean, concave_points_mean, area_mean","760ba2b6":"### 2.5.3 Negatively correlated feature:","8974f4e9":"## 3.5 Predciton\n### We will select 5 random data from the csv file to show that our prediction model is able to predict the tumour accurately using given statistics.","c5a00017":"## 3.2 Decision Tree\n### We will first use the decision tree as our prediction model. Lets look at the accuracy with the 11 chosen variables","7991fd05":"### Cleaning up the Dataset\n#### In this step the column 'description' should also be removed. The 'id' column stands for the patient's id, but we wanted our predicition model to be able to not only identify the correct tumour but also to identify which patient's tumour it is, which is why we kept the id.","bebd21db":"## 2.4 Correlation Matrix\n\n### After visualization the correlarration of all variables with Maglinant results, next comes the visualization of the correlation each variable has with one another.","6088c0b3":"### Now lets compare the two ROC curve in the same plot.","250af84b":"## 3.1 Training the model\n### We will first train the model with a 70% train and 30% test. we fix the training as well as testing to make it easier to debug as well as to visualise the accuracy of our prediction model using random_state = 100","1934986a":"### We decided to draw out the 11 parameters that we chose onto a pairplot to have a better understanding on the diagnosis through the plot and how are the diagnosis being divided by the machine learning.","6f8b5939":"### The ROC curve looks quite similar to the Random Forest Classifier. This prediction model might have been a better model if the number of false negatives could be brought down to even lower.","ee52aec7":"### We can see that the random forest classifier performs slightly better than the Decision Tree. However we have a out-of-bag score to cross validate with our test results. The accuracy has improved to 0.92 and is similar to the out of bag score, and the number of false positives and false negatives has been brought down.This is a much more accurate prediction model than the Decision Tree. Let's look at the ROC curve;","1bd2e07f":"### The negative correlated features look similar to the uncorrelated features. These negative correlated features might not be a good feature to use later on.","54b1a4c5":"### Check for any missing values\/redundant data","db8e20ec":"### These will be called out later on to calculate the accuracy, precision, recall and F1 scores."}}