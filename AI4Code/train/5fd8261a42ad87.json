{"cell_type":{"7e3e264d":"code","4f6733f2":"code","8e7d9a95":"code","0c0869b2":"code","53baf71e":"code","57bb09a6":"code","c7e90552":"code","621c821b":"code","6c609175":"code","e9366ed5":"code","51f33109":"code","f520fa1f":"code","a5a9045c":"code","94c6f873":"code","bbd69874":"code","fdcee78b":"code","390822d5":"code","8b0ae6c3":"code","2386d8aa":"code","0678a167":"code","a5dc966a":"code","42cb44bd":"code","db4c0187":"code","0a8baf08":"code","9fe81f60":"code","1478e2f6":"code","5752c5c4":"code","7d5d8f44":"code","08d6428c":"code","d25bf8e9":"code","3001d092":"code","f7f04674":"code","82cd88fe":"code","964685ce":"code","1bf21848":"code","b935d804":"code","55bae294":"code","b18a3445":"code","670c729a":"code","4e328fae":"code","b7b904d1":"code","6d364d50":"code","bcd63889":"code","b053478f":"code","d6355396":"code","4a75d514":"code","d75ecdd7":"code","4c3cc76e":"markdown","985a2fd7":"markdown","4acae24e":"markdown","90a9d2c0":"markdown","2b215e30":"markdown","6ffddc20":"markdown","c566a3dd":"markdown","d39f842c":"markdown","ddec795e":"markdown","ff2fc89b":"markdown","e97da14b":"markdown","bac37915":"markdown","a250694b":"markdown","612ff23d":"markdown","15aa21ea":"markdown","9ac38f93":"markdown","24b42c02":"markdown","f5e5aacc":"markdown","e04c6181":"markdown","8decf485":"markdown","9ffd9d07":"markdown","8fe95503":"markdown","0368005b":"markdown","21c637d5":"markdown","3387f4d3":"markdown","1936be46":"markdown","a1da127a":"markdown","ceaf38ff":"markdown","c0125f4c":"markdown","511fd48c":"markdown","20391a26":"markdown","79061060":"markdown","d4f8d97e":"markdown","d39ca9c4":"markdown","e8a51af2":"markdown","4c5136d5":"markdown","1b2bf235":"markdown","0c91c159":"markdown","11be866b":"markdown","42bca8db":"markdown","8a6f2720":"markdown","034dba0e":"markdown","def886c0":"markdown","81d2e7f0":"markdown","756dcc0c":"markdown","405b9fc0":"markdown"},"source":{"7e3e264d":"import time\nstart_time = time.time()\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Notice there is a new datafile I have uploaded that we will use in this notebook","4f6733f2":"class Timing:\n    \"\"\"\n    Utility class to time the notebook while running. \n    \"\"\"\n    def __init__(self, start_time):\n        self.start_time = start_time\n        self.counter = 0\n\n    def timer(self, message=None):\n        \"\"\"\n        Timing function that returns the time taken for this step since the starting time. Message is optional otherwise we use a counter. \n        \"\"\"\n        if message:\n            print(f\"{message} at {time.time()-self.start_time}\")\n        else:\n            print(f\"{self.counter} at {time.time()-self.start_time}\")\n            self.counter += 1\n        return\n    \ntiming = Timing(start_time)","8e7d9a95":"def load_data(number_of_rows:int =None, purpose=None)->pd.DataFrame:\n    \"\"\"\n    Returns a pandas DataFrame with the loan data inside\n    number_of_rows: Controls the number of rows read in, default and maximum is 22,60,668 rows\n    restriction: Restricts the columns read in to correct for information you should not have depending on the task at hand\n        \"time_of_issue\": Returns only the data that the lender has access to during the issuing of the loan\n    \"\"\"\n    root = \"..\/input\/lending-club-loan-data\"\n    use_cols= None\n    if purpose not in [None, 'time_of_issue']:\n        raise ValueError(f\"Invalid Purpose {purpose}\")\n    if purpose:\n        col_root = \"..\/input\/columns-available-at-time-of-loan\"\n        columnframe = pd.read_csv(os.path.join(col_root, purpose+\".csv\"))\n        illegals = ['sec_app_fico_range_low ', 'sec_app_inq_last_6mths ', 'sec_app_earliest_cr_line ', 'revol_bal_joint ', 'sec_app_mths_since_last_major_derog ', 'sec_app_revol_util ', 'sec_app_collections_12_mths_ex_med ', 'sec_app_open_acc ', 'fico_range_low', 'sec_app_fico_range_high ', 'verified_status_joint', 'last_fico_range_low', 'sec_app_chargeoff_within_12_mths ', 'fico_range_high', 'total_rev_hi_lim \\xa0', 'sec_app_mort_acc ', 'sec_app_num_rev_accts ', 'last_fico_range_high']\n        use_cols = [x for x in list(columnframe['name']) if x not in illegals]\n\n\n\n    path = os.path.join(root, \"loan.csv\")\n\n    maximum_rows = 2260668\n    if not number_of_rows:\n        return pd.read_csv(path, low_memory=False, usecols=use_cols)\n    else:\n        if number_of_rows > maximum_rows or number_of_rows < 1:\n            raise ValueError(f\"Number of Rows Must be a Number between 1 and {data.shape[0]}\")\n        else:\n            return pd.read_csv(path, low_memory=False, nrows=number_of_rows, usecols=use_cols)","0c0869b2":"data = load_data(number_of_rows=None, purpose=\"time_of_issue\")","53baf71e":"def investigate(data)->None:\n    print(data.shape)\n    print(data.info())\n    print(data.describe())","57bb09a6":"investigate(data)","c7e90552":"def type_list_generator(data, separated=False):\n    \"\"\"\n    Prints out 3 list to store which columns are of which type.\n    Interest rate can be in the list or not depending on the seperated variable\n    \"\"\"\n    numericals = ['loan_amnt','funded_amnt','funded_amnt_inv', 'annual_inc','mort_acc','emp_length', 'int_rate']\n    if separated:\n        numericals.pop()\n    strings = ['issue_d', 'zip_code']\n    categoricals = [x for x in data.columns if x not in numericals and x not in strings] # ['term', 'grade', 'sub_grade', 'emp_title', 'home_ownership', 'verification_status', 'purpose', 'title', 'addr_state', 'initial_list_status', 'application_type', 'disbursement_method']\n    return numericals, strings, categoricals","621c821b":"numericals, strings, categoricals = type_list_generator(data)","6c609175":"def drop_nan_columns(data, ratio=1.0)->pd.DataFrame:\n    \"\"\"\n    The ratio parameter (0.0<=ratio<1.0) lets you drop columns which has 'ratio'% of nans. (i.e if ratio is 0.8 then all columns with 80% or more entries being nan get dropped)\n    Returns a new dataframe\n    \"\"\"\n    col_list = []\n    na_df = data.isna()\n    total_size = na_df.shape[0]\n    for col in na_df:\n        a = na_df[col].value_counts()\n        if False not in a.keys():\n            col_list.append(col)\n        elif True not in a.keys():\n            pass\n        else:\n            if a[True]\/total_size >= ratio:\n                col_list.append(col)\n    print(f\"{len(col_list)} columns dropped- {col_list}\")\n    return data.drop(col_list, axis=1)","e9366ed5":"data = drop_nan_columns(data, ratio=0.5)","51f33109":"def investigate_nan_columns(data)->None:\n    \"\"\"\n    Prints an analysis of the nans in the dataframe\n    \"\"\"\n    col_dict = {}\n    na_df = data.isna()\n    total_size = na_df.shape[0]\n    for col in na_df:\n        a = na_df[col].value_counts()\n        if False not in a.keys():\n            col_dict[col] = 1.0\n        elif True not in a.keys():\n            pass\n        else:\n            col_dict[col] =  a[True]\/total_size\n    print(f\"{col_dict}\")\n    return","f520fa1f":"investigate_nan_columns(data)","a5a9045c":"data['emp_title'].value_counts()","94c6f873":"unemployed = ['unemployed', 'none', 'Unemployed', 'other', 'Other']\nfor item in unemployed:\n    if item in data['emp_title']:\n        print(\"Found It at \", item)","bbd69874":"def handle_nans(data)->None:\n    \"\"\"\n    Handle the nans induvidually per column\n    emp_title: make Nan -> Unemployed\n    emp_length: make Nan - > 10+ years this is both mode filling and value filing\n    title: make Nan -> Other\n    \"\"\"\n    data['emp_title'] = data['emp_title'].fillna(\"Unemployed\")\n    data['title'] = data['title'].fillna('Other')\n    mode_cols = ['emp_length', 'annual_inc', 'mort_acc', 'zip_code']\n    for col in mode_cols:\n        data[col] = data[col].fillna(data[col].mode()[0])\n    return\nhandle_nans(data)","fdcee78b":"any(data.isna().any()) # True iff there some NaN values anywhere in the dataset","390822d5":"investigate(data)","8b0ae6c3":"def handle_types(data, numericals, strings, categoricals):\n    def helper_emp_length(x):\n        if x == \"10+ years\": return 10\n        elif x == \"2 years\": return 2\n        elif x == \"< 1 year\": return 0\n        elif x == \"3 years\": return 3\n        elif x == \"1 year\": return 1\n        elif x == \"4 years\": return 4\n        elif x == \"5 years\": return 5\n        elif x == \"6 years\": return 6\n        elif x == \"7 years\": return 7\n        elif x == \"8 years\": return 8\n        elif x == \"9 years\": return 9\n        else:\n            return 10\n    data['emp_length'] = data['emp_length'].apply(helper_emp_length)\n\n    for category in categoricals:\n        try:\n            data[category] = data[category].astype('category')\n        except:\n            pass\n    data['issue_d'] = data['issue_d'].astype('datetime64')\n    return\n","2386d8aa":"handle_types(data, numericals, strings, categoricals)","0678a167":"def correlation_heatmap(data):\n    corrmat = data.corr()\n    sns.heatmap(corrmat, vmax=0.9, square=True)\n    plt.title(\"Correlation Heatmap\")\n    plt.xlabel(\"Features\")\n    plt.ylabel(\"Features\")\n    plt.show()\n    timing.timer(\"Heatmap\")\n    return\n\ncorrelation_heatmap(data)","a5dc966a":"def distplot(data):\n    \"\"\"\n    Reveals a positive skew\n    \"\"\"\n    from scipy.stats import norm\n    sns.distplot(data['int_rate'], fit=norm)\n    plt.title(\"Distribution and Skew of Interest Rate\")\n    plt.xlabel(\"Interest Rate in %\")\n    plt.ylabel(\"Occurance in %\")\n    plt.show()\n    timing.timer(\"Skew with distplot\")\n    return\n\ndistplot(data)","42cb44bd":"def boxplot(data):\n    \"\"\"\nCreates 4 boxplots\n            \n    \"\"\"\n    fig, axes = plt.subplots(2,2) # create figure and axes\n    col_list = ['annual_inc', 'loan_amnt', 'int_rate', 'emp_length']\n    by_dict = {0: 'home_ownership', 1:\"disbursement_method\", 2:\"verification_status\", 3:\"grade\"}\n\n    for i,el in enumerate(col_list):\n        a = data.boxplot(el, by=by_dict[i], ax=axes.flatten()[i])\n\n    #fig.delaxes(axes[1,1]) # remove empty subplot\n    plt.tight_layout()\n    plt.title(\"Various Boxplots\")\n    plt.show()\n    timing.timer(\"Boxplot\")\n    return\n\nboxplot(data)","db4c0187":"def lines(data):\n    \"\"\"\n    Employment length vs interest rate\n    \"\"\"\n    sns.lineplot(x=data['emp_length'], y=data['int_rate'])\n    plt.title(\"Employment Length vs Interest Rate\")\n    plt.xlabel(\"Employment Length in yrs\")\n    plt.ylabel(\"Interest Rate in %\")\n    plt.show()\n    timing.timer(\"Lines\")\n    return\n\nlines(data)","0a8baf08":"def scatter(data):\n    \"\"\"\n    Scatter Sub_Grade vs Risk\n    \"\"\"\n    info = data.copy()\n    a = info.groupby('sub_grade').mean()\n    \n    sns.scatterplot(x=a.index, y=a['int_rate'])\n    plt.title(\"Subgrade vs Interest Rate ScatterPlot\")\n    plt.xlabel(\"Subgrade\")\n    plt.ylabel(\"Interest Rate in %\")\n    plt.show()\n    timing.timer(\"Scatter\")\n    return\n\nscatter(data)","9fe81f60":"# 3D Scatterplot\ndef three_D_scatter(data):\n    \"\"\"\n    Loan Amount vs Employment Length vs Interest Rate\n    \"\"\"\n    from mpl_toolkits import mplot3d\n    import numpy as np\n    info = data[:1000]\n\n    fig = plt.figure()\n    ax = plt.axes(projection='3d')\n\n    xs = info['loan_amnt']\n    zs = info['emp_length']\n    ys = info['int_rate']\n    ax.scatter(xs, ys, zs, s=1, alpha=1)\n\n\n    ax.set_xlabel('Loan Amount')\n    ax.set_ylabel('Interest Rate')\n    ax.set_zlabel('Employment Length')\n    plt.title(\"3D Scatterplot\")\n    plt.show()\n    timing.timer(\"3D Scatter\")\n    return\n\nthree_D_scatter(data)","1478e2f6":"# Violin Plot\ndef violin_plot(data):\n    sns.violinplot(x=\"home_ownership\", y=\"int_rate\", data=data, hue=\"term\")\n    plt.title(\"Violin Plot\")\n    plt.xlabel(\"Home Ownership\")\n    plt.ylabel(\"Interest Rate in %\")\n    plt.show()\n    timing.timer(\"Violin\")\n    return\n\nviolin_plot(data)","5752c5c4":"# Bubble Chart\ndef bubble_chart(data):\n    info = data[:1000]\n    sns.lmplot(x=\"loan_amnt\", y=\"int_rate\",data=info,  fit_reg=False,scatter_kws={\"s\": info['annual_inc']*0.005})\n    plt.title(\"Bubble Chart\")\n    plt.xlabel(\"Loan Amount\")\n    plt.ylabel(\"Interest Rate in %\")\n    plt.show()\n    timing.timer(\"bubble\")\n    return\n\nbubble_chart(data)","7d5d8f44":"def load_split_data(number_of_rows=None, purpose=None, column='int_rate', test_size=0.2):\n    from sklearn.model_selection import train_test_split\n    data = load_data(number_of_rows=number_of_rows, purpose=purpose)\n    target = data[column]\n    data.drop(column, axis=1, inplace=True)\n    return train_test_split(data, target, test_size=test_size)","08d6428c":"X_train, X_test, y_train, y_test = load_split_data(50000, purpose=\"time_of_issue\")\nnumericals, strings, categoricals = type_list_generator(X_train, separated=True)","d25bf8e9":"X_train = drop_nan_columns(X_train, ratio=0.5)\nX_test = drop_nan_columns(X_test, ratio=0.5)\nhandle_nans(X_train)\nhandle_nans(X_test)\nhandle_types(X_train, numericals, strings, categoricals)\nhandle_types(X_test, numericals, strings, categoricals)\n# For this notebook we will ignore the string variables, however there are ways to use them using other prepreocessing techniques if desired\nX_train = X_train.drop(strings, axis=1)\nX_test = X_test.drop(strings, axis=1)\ntiming.timer(\"Cleaned Data\")","3001d092":"def manage_skews(train_target, test_target):\n    \"\"\"\n    Applying Square Root in order\n    \"\"\"\n    timing.timer(\"Unskewed Data\")\n    return np.sqrt(train_target), np.sqrt(test_target)\n\ny_train, y_test = manage_skews(y_train, y_test)","f7f04674":"def scale_numerical_data(X_train, X_test, numericals):\n    from sklearn.preprocessing import StandardScaler\n    sc = StandardScaler()\n    X_train[numericals] = sc.fit_transform(X_train[numericals])\n    X_test[numericals] = sc.transform(X_test[numericals])\n    timing.timer(\"Scaled Data\")\n    return\n\nscale_numerical_data(X_train, X_test, numericals)","82cd88fe":"X_train['emp_title'].value_counts()","964685ce":"def shrink_categoricals(X_train, X_test, categoricals, top=25):\n    \"\"\"\n    Mutatues categoricals to only keep the entries which are the top 25 of the daframe otherwise they become other\n    \"\"\"\n    for category in categoricals:\n        if category not in X_train.columns:\n            continue\n        tops = X_train[category].value_counts().index[:top]\n        def helper(x):\n            if x in tops:\n                return x\n            else:\n                return \"Other\"\n        X_train[category] = X_train[category].apply(helper)\n        X_test[category] = X_test[category].apply(helper)\n    timing.timer(\"Shrunk Categories\")\n    return\n","1bf21848":"shrink_categoricals(X_train, X_test, categoricals)\n\nX_train['emp_title'].value_counts()","b935d804":"def encode_categorical_data(X_train, X_test, categoricals):\n    from sklearn.preprocessing import LabelEncoder\n    for category in categoricals:\n        if category not in X_train.columns:\n            continue\n        le = LabelEncoder()\n        X_train[category] = le.fit_transform(X_train[category])\n        X_test[category] = le.transform(X_test[category])\n    timing.timer(\"Encoded Categoricals\")\n    return\n\nencode_categorical_data(X_train, X_test, categoricals)","55bae294":"def dimensionality_reduction(X_train, X_test):\n    from sklearn.decomposition import PCA\n    pca = PCA(n_components=0.95)\n    X_train = pca.fit_transform(X_train)\n    X_test = pca.transform(X_test)\n    timing.timer(\"Dimensionality Reduced\")\n    return X_train, X_test\n\nX_train, X_test = dimensionality_reduction(X_train, X_test)\nX_train.shape","b18a3445":"def random_forest(X_train, y_train, optimal=False):\n    \"\"\"\n    Optimal = True returns an untrained model\n    \"\"\"\n    from sklearn.ensemble import RandomForestRegressor\n    if optimal:\n        return RandomForestRegressor(n_estimators=120, max_depth=25, bootstrap=True, max_features=3)\n    from sklearn.model_selection import GridSearchCV\n \n    param_grid = [{'n_estimators':[60, 70, 80, 100, 120], 'max_depth':[15, 20, 25, None], 'bootstrap':[True, False], 'max_features':[None, 2, 3]}]\n    forest = RandomForestRegressor()\n    grid_search = GridSearchCV(forest, param_grid, cv=3, scoring=\"r2\")\n    grid_search.fit(X_train, y_train)\n    timing.timer(\"Forest Grid Search Complete\")\n    final = grid_search.best_params_\n    print(final)\n    return grid_search.best_estimator_\n    \n\ndef regression(X_train, y_train, optimal=False):\n    \"\"\"\n    Optimal = True returns an untrained model\n    \"\"\"\n    from sklearn.linear_model import ElasticNetCV\n    if optimal:\n        return ElasticNetCV(alphas=[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10], l1_ratio=0.0)\n    from sklearn.model_selection import GridSearchCV\n\n    elastic_net = ElasticNetCV()\n    param_grid = {'alphas':[[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10]], 'l1_ratio':[0.0, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]}\n    grid_search = GridSearchCV(elastic_net, param_grid, scoring=\"r2\", cv=3)\n    grid_search.fit(X_train, y_train)\n    timing.timer(\"Regression Grid Search Complete\")\n    print(grid_search.best_params_)\n    return grid_search.best_estimator_ \n\n\ndef knn(X_train, y_train, optimal=False):\n    \"\"\"\n    Optimal = True returns an untrained model\n    \"\"\"\n    from sklearn.neighbors import KNeighborsRegressor\n    if optimal:\n        return KNeighborsRegressor(n_neighbors=10, weights='distance')\n    \n    from sklearn.model_selection import GridSearchCV\n    \n    model = KNeighborsRegressor()\n    param_grid = {'n_neighbors':[2,4,6,8,10,12,14], 'weights':['uniform', 'distance']}\n    grid_search = GridSearchCV(model, param_grid, scoring=\"r2\", cv=3)\n    grid_search.fit(X_train, y_train)\n    timing.timer(\"Neighbors Grid Search Complete\")\n    print(grid_search.best_params_)\n    return grid_search.best_estimator_ \n\ndef svm(X_train, y_train, optimal=False):\n    \"\"\"\n    Optimal = True returns an untrained model\n    \"\"\"\n    from sklearn.svm import SVR\n    if optimal:\n        return SVR()\n    from sklearn.model_selection import RandomizedSearchCV\n\n    svr = SVR()\n\n    param_grid = {'kernel':['rbf', 'sigmoid', 'poly', 'linear'], 'C':[0.8, 1.0, 1.2]}\n    n_iter = 2\n    rsv = RandomizedSearchCV(svr, param_grid, n_iter=n_iter, scoring=\"r2\")\n    rsv.fit(X_train, y_train)\n    timing.timer(\"SVR Random Search Complete\")\n    final = rsv.best_params_\n    print(final)\n    return rsv.best_estimator_","670c729a":"\"\"\"\nmodel_creators = [random_forest,regression, knn, svm]\nmodels = []\nfor creator in (model_creators):\n    models.append(creator(X_train, y_train))\n    \nfor model in models:\n    model.score(X_test, y_test)\n\"\"\"","4e328fae":"def ensemble(model_list):\n    from sklearn.ensemble import VotingRegressor\n    vtr = VotingRegressor(model_list)\n    return vtr","b7b904d1":"class GradientBoost:\n    def __init__(self, model_class, example, n_estimators=2):\n        self.model_class = model_class\n        self.parameters = example.get_params()\n        self.n_estimators = n_estimators\n        self.estimators = []\n        for n in  range(n_estimators):\n            model = model_class()\n            model.set_params(**self.parameters)\n            self.estimators.append(model)\n\n    def fit_helper(self, X, y, i=0):\n        if i >= len(self.estimators):\n            return\n        else:\n            self.estimators[i].fit(X, y)\n            preds = self.estimators[i].predict(X)\n            error = y - preds\n            self.fit_helper(X, error, i=i+1)\n\n           \n    def fit(self, X, y):\n        self.fit_helper(X, y)\n\n    def predict(self, X):\n        prediction = self.estimators[0].predict(X)\n        for estimator in self.estimators[1:]:\n            prediction += estimator.predict(X)\n        return prediction\n\n    def score(self, X, y):\n        from sklearn.metrics import r2_score\n        preds = self.predict(X)\n        return r2_score(y, preds)","6d364d50":"order = {0: \"rfr\", 1:\"lin_reg\", 2:\"knn\", 3:\"svr\", 4:\"vtr\", 5:\"gb\"}\nmodel_creators = [random_forest,regression, knn, svm]\nmodel_list = []\nmodels = []\nfor i, creator in enumerate(model_creators):\n    model_list.append( (order[i] , creator(X_train, y_train, optimal=True) ) )\n    models.append(creator(X_train, y_train, optimal=True))\n    timing.timer(f\"Appended Model {i}\")\nmodels.append(ensemble(model_list))\nfrom sklearn.svm import SVR\nmodels.append(GradientBoost(SVR, models[3], 2))","bcd63889":"def train_and_test(models, order):\n    for i, model in enumerate(models):\n        model.fit(X_train, y_train)\n        timing.timer(f\"Finished Fitting model {i}\")\n    scores = []\n    for model in models:\n        scores.append(model.score(X_test, y_test))\n    final = {}\n    for score_no in range(len(scores)):\n        final[order[score_no]] = scores[score_no]\n    return final","b053478f":"train_and_test(models, order)","d6355396":"best_model = models[2]\n# We take the first 20 inputs and compare the predictions with the outputs\ntruths = y_test[0:20]**2 # Squaring to undo the skew solution in order to truly reflect the data\npreds = best_model.predict(X_test[0:20])**2\nresidual_error = truths - preds\nprint(residual_error)","4a75d514":"plt.scatter(truths, preds)\nplt.plot([7.0, 22], [7.0, 22], c = \"red\")\nplt.title(\"Model Analysis\")\nplt.xlabel(\"Truth\")\nplt.ylabel(\"Prediction\")\nplt.show()","d75ecdd7":"truths, preds","4c3cc76e":"Looking at the datatypes it's easy to tell a few things need to be done regarding typing:-\n* There are a lot of categorical columns (e.g. grade) but right now these are only being read as object or strings, we must convert them to categorical variables.\n* We can also safely convert employment length to numbers so that the model can use it as a numerical column.\n* We also need to convert date to a datetime datatype to best use it","985a2fd7":"It seems a bit suspicious that there is no employment title for people without a job, very possibly these are what the NaN values represent. To confirm this let us create a list of titles that could feasibly represent the unemployed and see if any of them appear in the column","4acae24e":"The below code let's you create the model and tune its hyperparameters, it will not run if you use the full dataset. Either add a size parameter and set it below 20,000 when loading split data above or skip this block of code to proceed with the notebook. Now let us actually create the models and see what the best hyperparameters are","90a9d2c0":"As we can see, both the ensemble and Gradient Boosting could reduce the error beyond the SVM threshold. The best model we have is clearly the KNN at around 99.8% R^2 score. However the SVM itself is quite a good model, ","2b215e30":"Now we've taken out the really useless columns, let's check the other ones so that we get a sense of how many NaN entries the rest of our data has","6ffddc20":"We use the data cleaning methods from above in order to prepare the data for processing","c566a3dd":"The insights we can take from each plot:- \n* (0,0) - This graph tells us nothing about the relation to interest rates, but gives us interesting insights on the economy from which the data was extracted, namely it is likely not a savings based economy. You can tell this by looking at how people who own their houses are not that much wealthier than those who have it on a mortgage. This implies that even when induviudals have enough income to perhaps save an eventually buy a house or a buy a lower grade house they could afford they are opting to take a loan and commit to this expenditure. It is also an extremely unequal economy, with the outliers being so high it makes the averages look like they are at the zero mark.\n* (0,1) - This graph tells us the intutive idea that cash loans on average are of a smaller sum than DirectPay loans, presumably for security reasons. The suprising observation is the lack of any significant outliers, implying that this lending institution is a relatively safe one which caps the loans it gives, meaning there isn't a single loan so high that it would count as a significant outlier.\n* (1,0) - This graph suggests that verification status does seem to have a relationship with interest rate. The average steadily increases the more verfified the borrower is.\n* (1,1) - This graph suggest there is no relationship between the length of employment and the Grade of the loan","d39f842c":"Now we can create a training function","ddec795e":"Now we create a loading data function. This specifically has the ability to load only the columns that are mentioned in the datafile time_of_issue.csv. The data in this file is nothing more than the names of the variables\/ information which the bank has access to at the time of actually issuing the loan","ff2fc89b":"## Data Visualization\n<a id=\"visualization**\"><\/a>\n***\nWe can now start Exploratory Data Analysis to find deeper patterns in the data.","e97da14b":"From this plot we can see a few things\n* Clearly there is a huge correlation (nearly one to one) with the loan_amnt (Which is the amount requested by the borrower), and the funded amounts (amount funded by investors). This suggests that we probably want to merge these columns as they add dimensionality but do not provide that much extra information\n* From looking at the variables related to interest rates the first observation is that some variables like mortgage account balance and (surprisingly) annual income seem to have nearly no correlation\n* In general the most correlated variable seems to be employment length, we could plot these two variables against each other to get a clearer sense of their relationship\n\nUnfortunately overall it seems the numerical variables are not the most correlated, either there is a non-linear relationship in the data or our categorical features are where the bulk of our useful features will be\n","bac37915":"Clearly there is a strong linear relationship between subgrade and interest rate. This makes it the best feature we have seen so far, and understandably so because the interest rate is in most cases a function of the risk of a loan.","a250694b":"**Correlation Heatmap**\n\n* Correlation Heatmaps are a great way to spot linear relations between numerical columns in your dataset.\n* The basic theory is that you use an inbuilt pandas function *corr* to calculate each variables correlation to every other variable\n* Plotting this resulting correlation matrix in a heatmap gives you a sense of which features are correlated to the target variables, hinting at the features you should select or are more important in the model you will make.","612ff23d":"## Data Modelling\n<a id=\"modelling**\"><\/a>\n***\nWe are finally onto the modelling stage. We will create 4 different models - Random Forest, Support Vector Machine, Linear Regression, KNearestNeighbors and fine-tune them using scikit-learn","15aa21ea":"The optimal model to return for each estimator was entered after running this section with a size of 500,000. It is computationally infeasible to run the Hyperparameter Tuning Algorithms on the full dataset","9ac38f93":"First, let us fix the skew that we saw in the distribution plot using the square root transformation","24b42c02":"## Conclusion\n***\n\nThank you for viewing this Kernel. The notebook is a work in progress and will be updated as per feedback and future ideas.","f5e5aacc":"From over 20 columns to only 5! While it may seem like we ought to have lost a lot of data the 95% of entropy being saved ensures we have preserved most core patterns.\nYou could do the notebook without the above step to see how much slower the training of the models is without this step","e04c6181":"## Model Assesment\n<a id=\"assesment**\"><\/a>\n***\nLet us look at our models accuracy now to get a sense of how close we are to the true interest rate.\n","8decf485":"We are nearly done with our data processing. The final step is to run a dimensionality reduction algorithm.This is because having many dimensions to data can make training models significantly slower and also sometimes less accurate as well. \n\nWe will use PCA, an algorithm which tries to project data into lower dimensions while preserving as much entropy as possible. Instead of stating how many dimensions the PCA algorithm should project down to, we will simply state what percentage of entropy we would like preserved, 95% is a good standard bar.\nThe reason we are doing the step last is because after the PCA it is virtually impossible to figure out what each column represents in terms of the original data.","9ffd9d07":"## Data Processing\n<a id=\"processing**\"><\/a>\n***\nFirst let us define a function to split data and return it to us. This is useful because we want to be very sure of what manipulations we are doing to test data, in order to ensure we aren't cheating","8fe95503":"From an initial look at the data it seems like some columns are entirely nan columns (e.g. desc, there at least 2 such columns). We should probably drop these columns which have too many NaNs entirely as opposed to dropping rows","0368005b":"**LinePlots**\n\nGood for seeing trends in data between induvidual variables","21c637d5":"Finally we will try to use a Boosting technique -Gradient Boosting, \nGradient Boosting has a simple theory :-\n* A prelimnary model is trained on a dataset, and its residual errors are recorded. \n* Another model is then trained to model these residual errors, \n* The sum of the induvidual predictions is considered the prediction of the overall system.\n* This can chain to multiple models and not just two.\n\nScikit-Learn has an inbuilt GradientBoostingRegressor, but this uses Decision trees as its fundamental unit. We would rather use the SVM that is performing so poorly induvidually (comapared to the others), so we will have to manually define the class","3387f4d3":"**Scatter Plot**\n\nThe most basic type of plot, but we will scatter averages because otherwise the graph will be too dense for us to actually learn anything","1936be46":"Model training becomes too long with the entire dataset and so we will use only a subset of the data","a1da127a":"**Distribution Plot**\n\n* Distribution Plots are very similar to histograms and essentially show how data is distributed across its different values. \n* They are extremely useful in finding skews in the data. Most models perform best when the data they deal with is normally distributed (especially linear models). So if we find a skew we may want to apply a skew solution function the variable in order to make it resemble a normal distribution\n","ceaf38ff":"Exploratory Data Analysis done. Now we will prepare our data for the model","c0125f4c":"Now we can look at the data again and actually understand it","511fd48c":"## Introduction\n<a id=\"introduction**\"><\/a>\n***\n\nHello and welcome to a simple exploration and model of the Lending Club Loan Dataset. \n\nToday we will first be performing exploratory data analysis and visualizations including but not limited to\n* Violin plots\n* Box Plots\n* Correlation Heatmaps\n* Etc.\n\nWe will then build different models using Scikit-Learn, once we have made the basic models from RandomForests to Support Vector Machines, we will attempt to make our models stronger using\n* Ensemble Learning Techniques\n* Boosting Techniques\n\nThe notebook itself is divided into 3 parts\n1. [Introduction and basic EDA]\n2. [Data Visualization]\n3. [Data Processing]\n4. [Data Modelling]\n5. [Model Assesment]","20391a26":"It seems for employment title NaNs are likely all unemployed (it is highly unlikely that no one who recieved loans was unemployed.). \n* For length, mortgage account, annual income and zip code we will use mode filling.\n* For title we will cast it to other.","79061060":"The extended tail forward gives a clear sign of a positive skew in our interest rate. This means that there are much more lower values than there are high values.\nPossible solutions we could apply include the square root and log functions","d4f8d97e":"This function us that employment title has over 7% NaN values, while length has less than 7%. The others are nearly negligible Let us look at the employment title to get a sense of the best strategy for dealing with these values.","d39ca9c4":"The rest of the plots are various different plots which show relationships in the data\n* 3D scatter\n* Violin Plot\n* Bubble plot","e8a51af2":"Apart from these models we will add two other models. The first is an ensemble learner, let us use an averaging ensemble technique to combine the models and see if it improves the R^2 Score","4c5136d5":"Looking at the columns and their details then we can segment them into three types","1b2bf235":" Next, we should normalize all of our data, this once again simply makes most models more effective and speeds up convergence","0c91c159":"## Data Exploration\n<a id=\"exploration**\"><\/a>\n***\n\nLet us start by creating a function to look at the data","11be866b":"**Boxplots**\n\n* Boxplots are an extremely useful way to plot outliers, while also seeing how numerical data varies across different categories. \n\n","42bca8db":"And that's it! We have finished an extremely basic cleaning of the dataset. \n","8a6f2720":"There are other columns, like purpose, that have this same issue. When there are so many different categories the model is likely to get extremely confused or is just unlikely to generalize well.\nAdditionally there is also a harm that once we fit an encoder onto our categorical columns, then there will be a completely new profession in the test set that the encoder hasn't seen before, this would throw an exception\nTo solve this problem we keep only the instances that make up the top 15 categories of that variable, and cast the rest to a standard value like \"Other\"","034dba0e":"We can now confirm that there are no NaN values in our dataset","def886c0":"It seems interest rate vs employment length shows some non-linear relation with a clear drop in average interest rate from working 7 years to working 10 years, probably because stability in occupation is a sign of lower risk.\nThe interest rate for people who have worked less than a year seems low though, possibly this is because these are small buisness or enterprise loans that are valued at a lower interest rate so that the buisness itself has a greater chance of success and thus repaying the loan.","81d2e7f0":"Before we start let us make a utility class to help time the various steps of the program","756dcc0c":"As you can see we have a pretty good model that can predict within around a 1% accuracy what the interest rate will be. We can also visualize this accuracy in a graph","405b9fc0":"Finally we will encode all of our categorical variables so that models can process them, but before we do that, we need to realize something about the size of our dataset\ni.e if you look at the columns such as employment title you can already see a whole bunch of different occupations - 432653 to be precise"}}