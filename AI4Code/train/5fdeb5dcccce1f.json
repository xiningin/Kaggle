{"cell_type":{"0e9d2b04":"code","ac33b9c3":"code","9277e971":"code","cd9c3309":"code","f4102c91":"code","04d17d80":"code","20118663":"code","30763e0c":"code","643337a2":"code","2f8999f8":"code","dad81a5f":"code","2a56c603":"code","9d724797":"code","e99cf546":"code","729b572f":"code","6d19f5d0":"code","deef2140":"code","a01f41de":"code","616e4b3d":"code","389a5230":"code","fd1d3b6b":"code","b5ee9fe2":"code","4dff1ee7":"code","61022f3a":"code","62e46380":"code","0bb6e571":"code","57669703":"code","952798e7":"code","9deb1491":"code","915a6202":"code","d2883074":"code","d89964ea":"code","b0b5bfc8":"code","cad18596":"code","4012aa5e":"code","83061360":"code","2f084da4":"code","4e9736f5":"code","809b6611":"code","ec3f23ca":"code","aff6dbed":"code","da19bb07":"code","c8f01f24":"code","933506f7":"code","b9777f64":"code","5910edb7":"code","0a019c3a":"code","7f9c5249":"code","d73ab335":"code","8c277ab0":"code","30e16eee":"code","a15b20a2":"code","fba9f156":"code","aa3dc94e":"code","19279b50":"code","5af768d1":"code","334a0c8c":"code","a09ec219":"code","e75b63c5":"code","ff713381":"code","15ca84da":"code","7a01f01d":"code","a7662854":"code","29ab47a5":"code","30249119":"code","20c481d8":"code","a4665150":"code","6c40c221":"code","f2d7637d":"code","628e6f92":"code","dc9270fd":"code","680a0ada":"code","f8000815":"code","586e5d99":"code","2d445c65":"code","e281d478":"code","ade9dc5c":"code","9dffb93a":"code","d0cc6932":"code","33110509":"code","4fdae566":"code","a8e8b4fc":"code","45631308":"code","e25e8302":"code","0d8600d0":"code","32a3c40d":"code","03643ddd":"code","aabafd81":"code","07c2180f":"code","1217dfca":"code","c67ca713":"code","625ada64":"code","2e324fb4":"code","e61ddd84":"code","a4c3d404":"code","b99c20b9":"code","2adcbfb9":"code","505bec4f":"markdown","01cc0241":"markdown","6882ed34":"markdown","79e3f211":"markdown","53072569":"markdown","0f29016e":"markdown","aebc3c5d":"markdown","23e66110":"markdown","9e81edc6":"markdown","ca152a62":"markdown","b194d52a":"markdown","3c080779":"markdown","a9624630":"markdown","d398dd20":"markdown","dca149b6":"markdown","bbfc6815":"markdown","c4817417":"markdown","0437c278":"markdown","cc9bb153":"markdown","92e880d7":"markdown","cfaee6fb":"markdown","294d7e7d":"markdown","304fba59":"markdown","6cb625c1":"markdown","09ef08e7":"markdown","c65d0470":"markdown","8938aa62":"markdown","f1ccbdb7":"markdown","2abb0c6d":"markdown","9d4ed658":"markdown","fa4e58a8":"markdown","4e8c4f0e":"markdown","8d2ca49c":"markdown","e0e6e8fe":"markdown","432f18e9":"markdown","5992ecfb":"markdown","f69867c0":"markdown","b29aca72":"markdown","51782d19":"markdown","20938849":"markdown","d6b0cd0a":"markdown","731ad19a":"markdown","ede99091":"markdown","aea6ff83":"markdown","17888d3e":"markdown","dfea009f":"markdown","1b7a38b6":"markdown","80b6b07b":"markdown","4fa14cf6":"markdown","a3e2f2ed":"markdown","b34e2138":"markdown","30c5f4b0":"markdown","9e53265a":"markdown","8a38fc11":"markdown","c599293a":"markdown","8dfbae88":"markdown","5a23bb9c":"markdown","104f387b":"markdown","f838e04c":"markdown","e9c71aad":"markdown","2510fb86":"markdown","6c57ccf8":"markdown","5eae6615":"markdown","1dc4c86a":"markdown","f5396e5c":"markdown","8c227569":"markdown","6b41dd42":"markdown","07e302a3":"markdown","5d5a4722":"markdown","4d4e2a95":"markdown","7495bb86":"markdown","ff52b7c4":"markdown","f75b8dcb":"markdown","153c14a7":"markdown","737fd9c1":"markdown"},"source":{"0e9d2b04":"# Data manipulation\nimport numpy as np\nimport pandas as pd","ac33b9c3":"# Data visualization \nimport matplotlib.pyplot as plt\nimport seaborn as sns","9277e971":"# Scikit-Learn imports\n\n## Preprocessing and feature extraction\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler, OneHotEncoder\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n## Models\nfrom sklearn.linear_model import LinearRegression, Lasso\nfrom sklearn.neighbors import KNeighborsRegressor\n\n## Model selection and fine tuning\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error","cd9c3309":"# XGBoost import\nfrom xgboost import XGBRegressor","f4102c91":"# Tensorflow Keras imports\nfrom tensorflow.keras import Sequential, layers\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.losses import MeanSquaredError\nfrom tensorflow.keras.metrics import RootMeanSquaredError\nfrom tensorflow.keras.callbacks import EarlyStopping","04d17d80":"# Utils \nfrom scipy.stats import uniform, randint","20118663":"# Each row represents a single track, each column represents a field of the track (audio features and identifiers)\ndata = pd.read_csv(\"..\/input\/spotify-dataset-19212020-160k-tracks\/data.csv\") \n\n# This file is an extension to the \"data_by_artist.csv\" file with genres implementation for each artist. \n# Each row represents a single artist, each column represents an audio feature.\ndata_w_genres = pd.read_csv('..\/input\/spotify-dataset-19212020-160k-tracks\/data_w_genres.csv') ","30763e0c":"data.head()","643337a2":"data.info()","2f8999f8":"data['artists+name'] = data.apply(lambda row: row['artists'] + row['name'], axis=1)","dad81a5f":"df = data[data['artists+name'].duplicated()]","2a56c603":"df.shape","9d724797":"data[data['name']=='champagne problems']","e99cf546":"# We gather the list of indices corresponding to the maximum popularity for each duplicated artist\/song pair.\nindices = []\n\nfor name in df['artists+name'].unique():\n    subset = data[data['artists+name'] == name].copy()\n    m = subset['popularity'].max()\n    index = subset[subset['popularity'] == m].index[0]\n    indices.append(index)","729b572f":"data_bis = data.loc[indices].copy()\ndata_bis['artists+name'].duplicated().sum()","6d19f5d0":"data_ter = data.copy()\nfor i, row in data.iterrows():\n    if row['artists+name'] in df['artists+name'].unique():\n        data_ter.drop(index=i, inplace=True)","deef2140":"frames = [data_bis, data_ter]\ndata_four = pd.concat(frames)","a01f41de":"data_four['artists+name'].duplicated().sum()","616e4b3d":"data_four.shape","389a5230":"data_four.drop(columns=['release_date','id','artists+name'], inplace=True)","fd1d3b6b":"data_four = data_four[data_four['tempo'] != 0].copy()","b5ee9fe2":"data_four.reset_index(inplace=True, drop=True)","4dff1ee7":"data_four.to_csv('data\/songs.csv', index=False)","61022f3a":"# del df\n# del data_bis\n# del data_ter\n# del data_four","62e46380":"songs=pd.read_csv('data\/songs.csv')","0bb6e571":"songs.describe()","57669703":"fig, ax = plt.subplots(figsize=(15, 4))\nax = songs.groupby('year')['popularity'].mean().plot()\nax.set_title('Mean Popularity over the years')\nax.set_ylabel('Mean Popularity', weight='bold')\nax.set_xlabel('Year', weight='bold')\nax.set_xticks(range(1920, 2021, 5))\nplt.show()","952798e7":"fig, ax = plt.subplots(figsize=(16, 4))\nsns.histplot(songs['acousticness'],  bins=30)\nplt.show()","9deb1491":"fig, ax = plt.subplots(figsize=(15, 6))\nax1_data =  songs.groupby('acousticness')['popularity'].mean().to_frame().reset_index()\nax = sns.scatterplot(x=ax1_data['acousticness'], y=ax1_data['popularity'], color='blue', ax=ax)\nax.set_title('Acousticness vs. Mean Popularity')\nax.set_ylabel('Mean Popularity', fontsize=12)\nplt.tight_layout()\nplt.show()","915a6202":"sns.histplot(songs['danceability'], color='green', bins=30)\nplt.show()","d2883074":"fig, ax = plt.subplots(1, figsize=(15, 6), sharey=True, sharex = True)\nax_data =  songs.groupby('danceability')['popularity'].mean().to_frame().reset_index()\nax = sns.scatterplot(x='danceability', y='popularity', data=ax_data, color='green', ax=ax)\nax.set_title('danceability')\nax.set_ylabel('Mean Popularity', fontsize=12)\nplt.tight_layout()\nplt.show()","d89964ea":"fig, ax = plt.subplots(figsize = (15, 4))\nax = sns.histplot(songs['duration_ms']\/60000, color='orange')\nax.set_title('Length of Tracks (in minutes!)')\nax.set_xticks(range(0,25,1))\nax.set_xlim(0,25)\nplt.show()","b0b5bfc8":"a = songs['energy'].corr(songs['danceability']).round(3)\nprint(f'The Pearson correlation coefficient is: {a:^10}')","cad18596":"a = songs['energy'].corr(songs['loudness']).round(3)\nprint(f'The Pearson correlation coefficient is: {a:^10}')","4012aa5e":"songs.drop(columns=['loudness'], inplace=True)","83061360":"fig, ax = plt.subplots(1, figsize=(15, 6), sharey=True, sharex = True)\nax_data =  songs.groupby('energy')['popularity'].mean().to_frame().reset_index()\nax = sns.scatterplot(x='energy', y='popularity', data=ax_data, color='pink', ax=ax)\nax.set_title('energy')\nax.set_ylabel('Mean Popularity', fontsize=12)\nplt.tight_layout()\nplt.show()","2f084da4":"fig, ax = plt.subplots(1, figsize=(15, 6), sharey=True, sharex = True)\nax_data =  songs.groupby('instrumentalness')['popularity'].mean().to_frame().reset_index()\nax = sns.scatterplot(x='instrumentalness', y='popularity', data=ax_data, color='grey', ax=ax)\nax.set_title('instrumentalness')\nax.set_ylabel('Mean Popularity', fontsize=12)\nplt.tight_layout()\nplt.show()","4e9736f5":"fig, ax = plt.subplots(figsize=(15, 4))\nsns.histplot(songs['liveness'], color='purple', bins=30)\nplt.show()","809b6611":"fig, ax = plt.subplots(1, figsize=(15, 6), sharey=True, sharex = True)\nax_data =  songs.groupby('speechiness')['popularity'].mean().to_frame().reset_index()\nax = sns.scatterplot(x='speechiness', y='popularity', data=ax_data, color='brown', ax=ax)\nax.axvline(x=0.57, ymin=0, ymax=1, color='red', linestyle='dashed')\nax.set_title('speechiness')\nax.set_ylabel('Mean Popularity', fontsize=12)\nplt.tight_layout()\nplt.show()","ec3f23ca":"songs['speech_over.57']=1*(songs['speechiness']>=0.57)","aff6dbed":"songs.drop(columns='speechiness',inplace=True)","da19bb07":"genres = data_w_genres[['artists', 'genres']].copy()\ngenres['artists'] = genres['artists'].map(lambda x: '['+x+']')\ngenres.head()","c8f01f24":"X = songs.sort_values(by='year').drop(columns=['popularity']).copy()\ny = songs.sort_values(by='year')['popularity'].copy()\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, shuffle=False)\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, shuffle=False)","933506f7":"X_train.tail(5)","b9777f64":"X_val.tail(5)","5910edb7":"ct = ColumnTransformer([('minmax', MinMaxScaler(), ['year', 'tempo', 'duration_ms']),\n                        ('categorical', OneHotEncoder(), ['key']),\n                        ('drop_cols', 'drop', ['artists', 'name'])],\n                       remainder='passthrough')\n\nct.fit(X_train)\n\nX_train_preprocessed = ct.transform(X_train)\nX_val_preprocessed = ct.transform(X_val)\nX_test_preprocessed = ct.transform(X_test)","0a019c3a":"X_train_preprocessed.shape","7f9c5249":"# Usual linear regression\n\n## We instantiate the model\nlin_reg = LinearRegression()\n\n# We fit the model on preprocessed train data\nlin_reg.fit(X_train_preprocessed, y_train)\n\n# We make predictions on the validation set, also preprocessed\ny_pred = lin_reg.predict(X_val_preprocessed)\n\n# We output the root mean squared error on the validation set\nmean_squared_error(y_val, y_pred, squared=False)","d73ab335":"# Lasso\nlasso=Lasso()\n\nlasso.fit(X_train_preprocessed,y_train)\n\ny_pred = lasso.predict(X_val_preprocessed)\n\nmean_squared_error(y_val, y_pred, squared=False)","8c277ab0":"# XGBoost regressor\nxgb_regressor = XGBRegressor(n_estimators=100, max_depth=20, learning_rate=0.01)\n\nxgb_regressor.fit(X_train_preprocessed, y_train)\n\ny_pred = xgb_regressor.predict(X_val_preprocessed)\n\nmean_squared_error(y_val, y_pred, squared=False)","30e16eee":"# K-Nearest-Neighbors (KNN) regressor\nneigh = KNeighborsRegressor(n_neighbors=7)\n\nneigh.fit(X_train_preprocessed, y_train)\n\ny_pred=neigh.predict(X_val_preprocessed)\n\nmean_squared_error(y_val, y_pred, squared=False)","a15b20a2":"import enchant","fba9f156":"def check_if_english(text):\n    d = enchant.Dict(\"en_US\")\n    c = [d.check(word) for word in text.split(' ')]\n    if np.mean(c) < 0.5:\n        return 0\n    else: \n        return 1","aa3dc94e":"X_train['name_en'] = X_train['name'].map(check_if_english)\nX_val['name_en'] = X_val['name'].map(check_if_english)\nX_test['name_en'] = X_test['name'].map(check_if_english)","19279b50":"ct = ColumnTransformer([('minmax', MinMaxScaler(), ['year', 'tempo', 'duration_ms']),\n                        ('categorial', OneHotEncoder(), ['key']),\n                        ('drop_cols', 'drop', ['artists','name'])],\n                       remainder='passthrough')\n\nct.fit(X_train)\n\nX_train_preprocessed = ct.transform(X_train)\nX_val_preprocessed = ct.transform(X_val)\nX_test_preprocessed = ct.transform(X_test)","5af768d1":"lin_reg = LinearRegression()\n\nlin_reg.fit(X_train_preprocessed, y_train)\n\ny_pred = lin_reg.predict(X_val_preprocessed)\n\nmean_squared_error(y_val, y_pred, squared=False)","334a0c8c":"lasso = Lasso()\n\nlasso.fit(X_train_preprocessed,y_train)\n\ny_pred = lasso.predict(X_val_preprocessed)\n\nmean_squared_error(y_val, y_pred, squared=False)","a09ec219":"xgb_regressor = XGBRegressor(n_estimators=100, max_depth=20, learning_rate=0.01)\n\nxgb_regressor.fit(X_train_preprocessed, y_train)\n\ny_pred = xgb_regressor.predict(X_val_preprocessed)\n\nmean_squared_error(y_val, y_pred, squared=False)","e75b63c5":"neigh = KNeighborsRegressor(n_neighbors=7)\n\nneigh.fit(X_train_preprocessed, y_train)\n\ny_pred = neigh.predict(X_val_preprocessed)\n\nmean_squared_error(y_val, y_pred, squared=False)","ff713381":"def check_language(text, abbreviation):\n    '''\n    abbreviation is a string, that corresponds to the abbreviation of the language you want to check. \n    \n    For example: \n    - French is \"fr_FR\", \n    - Spanish is \"es\", \n    - Arabic is \"ar\"\n    - etc.\n    '''\n    d = enchant.Dict(abbreviation)\n    c = [d.check(word) for word in text.split(' ')]\n    if np.mean(c) < 0.5:\n        return 0\n    else: \n        return 1","15ca84da":"X_train['name_fr'] = X_train['name'].map(lambda text: check_language(text, abbreviation='fr_FR'))\nX_val['name_fr'] = X_val['name'].map(lambda text: check_language(text, abbreviation='fr_FR'))\nX_test['name_fr'] = X_test['name'].map(lambda text: check_language(text, abbreviation='fr_FR'))","7a01f01d":"X_train['name_sp'] = X_train['name'].map(lambda text: check_language(text, abbreviation='es'))\nX_val['name_sp'] = X_val['name'].map(lambda text: check_language(text, abbreviation='es'))\nX_test['name_sp'] = X_test['name'].map(lambda text: check_language(text, abbreviation='es'))","a7662854":"X_train['name_ar'] = X_train['name'].map(lambda text: check_language(text, abbreviation='ar'))\nX_val['name_ar'] = X_val['name'].map(lambda text: check_language(text, abbreviation='ar'))\nX_test['name_ar'] = X_test['name'].map(lambda text: check_language(text, abbreviation='ar'))","29ab47a5":"X_train['name_ru'] = X_train['name'].map(lambda text: check_language(text, abbreviation='ru'))\nX_val['name_ru'] = X_val['name'].map(lambda text: check_language(text, abbreviation='ru'))\nX_test['name_ru'] = X_test['name'].map(lambda text: check_language(text, abbreviation='ru'))","30249119":"X_train.to_csv('data\/X_train.csv', index=False)\nX_test.to_csv('data\/X_test.csv', index=False)\nX_val.to_csv('data\/X_val.csv', index=False)","20c481d8":"# X_train = pd.read_csv('data\/X_train.csv')\n# X_test = pd.read_csv('data\/X_test.csv')\n# X_val = pd.read_csv('data\/X_val.csv')","a4665150":"# Defining a new dataframe that corresponds to X_train, but where popularity has not been dropped. \nsongs_train=songs.sort_values(by='year').loc[:54568].copy() # the 54,568 corresponds to the last index of X_train","6c40c221":"# This cell aims at creating a dictionary that gives, for each artist in the train set, its mean popularity\n# or the mean popularity in the whole train set if the artist is only present once.\nartists_and_pop = {}\n\ntrain_mean_pop = songs_train['popularity'].mean()\n\nfor artist in X_train['artists'].unique():\n    temp = songs_train[songs_train['artists'] == artist]['popularity'].copy()\n    if len(temp) > 1:\n        artists_and_pop[artist] = temp.mean()\n    elif len(temp) == 1:\n        artists_and_pop[artist] = train_mean_pop\n    else:\n        print('Stopping iteration due to unexpected result.')\n        break","f2d7637d":"# We map this dictionary upon the \"artists\" column.\nX_train['artists'] = X_train['artists'].map(artists_and_pop)","628e6f92":"# For the validation set, we also map the dictionary upon the \"artists\" column.\n# If an artist, active after 1996 only, is not in the train set, we attribute the mean popularity to him\/her.\nX_val['artists'] = X_val['artists'].map(lambda artist: artists_and_pop.get(artist, train_mean_pop))","dc9270fd":"ct = ColumnTransformer([('minmax', MinMaxScaler(), ['year', 'tempo', 'duration_ms', 'artists']),\n                        ('categorial', OneHotEncoder(), ['key']),\n                        ('drop_cols', 'drop', ['name'])],\n                       remainder='passthrough')\n\nct.fit(X_train)\n\nX_train_preprocessed = ct.transform(X_train)\nX_val_preprocessed=ct.transform(X_val)","680a0ada":"lin_reg = LinearRegression()\n\nlin_reg.fit(X_train_preprocessed, y_train)\n\ny_pred = lin_reg.predict(X_val_preprocessed)\n\nmean_squared_error(y_val, y_pred, squared=False)","f8000815":"lasso=Lasso()\n\nlasso.fit(X_train_preprocessed,y_train)\n\ny_pred = lasso.predict(X_val_preprocessed)\n\nmean_squared_error(y_val, y_pred, squared=False)","586e5d99":"xgb_regressor = XGBRegressor(n_estimators=100, max_depth=20, learning_rate=0.01)\n\nxgb_regressor.fit(X_train_preprocessed, y_train)\n\ny_pred = xgb_regressor.predict(X_val_preprocessed)\n\nmean_squared_error(y_val, y_pred, squared=False)","2d445c65":"neigh = KNeighborsRegressor(n_neighbors=7)\n\nneigh.fit(X_train_preprocessed, y_train)\n\ny_pred=neigh.predict(X_val_preprocessed)\n\nmean_squared_error(y_val, y_pred, squared=False)","e281d478":"neigh = KNeighborsRegressor()\n\nparams = {\n    'weights': ['uniform', 'distance'], \n    'n_neighbors': randint(2, 15),\n    'algorithm': ['ball_tree', 'kd_tree', 'brute']\n}\n\nrnd_search = RandomizedSearchCV(estimator=neigh, \n                                param_distributions=params,\n                                n_iter=10, \n                                cv=5,\n                                verbose=1,\n                                n_jobs=-1)\n\nrnd_search.fit(X_train_preprocessed, y_train)\n\nrnd_search.best_score_","ade9dc5c":"neigh = KNeighborsRegressor(n_neighbors=18)\n\nneigh.fit(X_train_preprocessed, y_train)\n\ny_pred=neigh.predict(X_val_preprocessed)\n\nmean_squared_error(y_val, y_pred, squared=False)","9dffb93a":"def build_nn_model():\n    # We instantiate the sequential model.\n    model = Sequential()\n\n    # We add several Dense layers with ReLU activation, and 1 Dropout layer to prevent overfitting.\n    model.add(layers.Dense(100, activation = 'relu',input_dim=30))\n    model.add(layers.Dense(50, activation = 'relu'))\n    model.add(layers.Dropout(0.2))\n    model.add(layers.Dense(30, activation = 'relu'))\n\n    # Finally, the last layer will count 1 neuron with linear activation since we are dealing with a regression model. \n    model.add(layers.Dense(1, activation = 'linear'))\n    return model\n    \nmodel = build_nn_model()\n\nmodel.summary()","d0cc6932":"adam = Adam(learning_rate=0.00001)\nmse = MeanSquaredError()\n\n# We compile the model with mean squared error as loss and root mean squared error as metric. \nmodel.compile(loss=mse, optimizer=adam, metrics=[RootMeanSquaredError()])","33110509":"es = EarlyStopping(monitor='val_loss', patience=10, verbose=1)\n\nhistory = model.fit(X_train_preprocessed, y_train, \n                    validation_data=(X_val_preprocessed, y_val),\n                    epochs = 1000, \n                    batch_size = 32, \n                    callbacks = [es], \n                    verbose = 2)","4fdae566":"def plot_history(history):\n    fig, axes = plt.subplots(nrows=1, ncols=2, figsize=(17, 5))\n    \n    axes[0].plot(history.history['loss'], color='darkred', label='Train - Loss')\n    axes[0].plot(history.history['val_loss'], color='darkblue', label='Validation - Loss')\n    axes[0].legend()\n    axes[0].set_title('Loss (MSE) on train and validation sets')    \n    \n    axes[1].plot(history.history['root_mean_squared_error'], color='darkred', label='Train - RMSE')\n    axes[1].plot(history.history['val_root_mean_squared_error'], color='darkblue', label='Validation - RMSE')\n    axes[1].legend()\n    axes[1].set_title('RMSE on train and validation sets')\n\nplot_history(history)","a8e8b4fc":"y_pred = model.predict(X_val_preprocessed)\n\nmean_squared_error(y_pred, y_val, squared=False)","45631308":"# Recovering our train and test dataset with the languages of the song titles\n# Will work only if you have saved the output of the language check function (for the song name) \nX_train_old = pd.read_csv('data\/X_train.csv')\nX_val = pd.read_csv('data\/X_val.csv')\nX_test = pd.read_csv('data\/X_test.csv')\n\nX_train = pd.concat([X_train_old, X_val])","e25e8302":"y_train_old = y_train.copy()\ny_train = pd.concat([y_train_old, y_val])","0d8600d0":"# Defining a new dataframe that corresponds to X_train, but where popularity has not been dropped. \nsongs_train = songs.sort_values(by='year').loc[:130014].copy() ","32a3c40d":"X_test_no_2021=X_test[X_test['year']!=2021].copy()\ny_test_no_2021=y_test[:X_test_no_2021.shape[0]].copy() ","03643ddd":"# Preprocessing the 'artists' column as seen before\nartists_and_pop = {}\n\ntrain_mean_pop = songs_train['popularity'].mean()\n\nfor artist in X_train['artists'].unique():\n    temp = songs_train[songs_train['artists'] == artist]['popularity'].copy()\n    if len(temp) > 1:\n        artists_and_pop[artist] = temp.mean()\n    elif len(temp) == 1:\n        artists_and_pop[artist] = train_mean_pop\n\nX_train['artists'] = X_train['artists'].map(artists_and_pop)\nX_test_no_2021['artists'] = X_test_no_2021['artists'].map(lambda artist: artists_and_pop.get(artist, train_mean_pop))","aabafd81":"# Adding usual preprocessing steps\nct = ColumnTransformer([('minmax', MinMaxScaler(), ['year', 'tempo', 'duration_ms', 'artists']),\n                        ('categorial', OneHotEncoder(), ['key']),\n                        ('drop_cols', 'drop', ['name'])],\n                       remainder='passthrough')\n\nct.fit(X_train)\n\nX_train_preprocessed = ct.transform(X_train)\nX_test_preprocessed=ct.transform(X_test_no_2021)","07c2180f":"neigh = KNeighborsRegressor(n_neighbors=18)\n\nneigh.fit(X_train_preprocessed, y_train)\n\ny_pred=neigh.predict(X_test_preprocessed)\n\nmean_squared_error(y_test_no_2021, y_pred, squared=False)","1217dfca":"# We re-instantiate the model thanks to the function defined previously.\nmodel = build_nn_model()\n\nadam = Adam(learning_rate=0.00001)\nmse = MeanSquaredError()\nmodel.compile(loss=mse, optimizer=adam, metrics=[RootMeanSquaredError()])\n\nes = EarlyStopping(monitor = 'val_loss', patience = 10, verbose = 1)\n\nhistory = model.fit(X_train_preprocessed, y_train, \n                    validation_split=0.3,\n                    epochs = 1000, \n                    batch_size = 32, \n                    callbacks = [es], \n                    verbose = 0)\n\nplot_history(history)","c67ca713":"y_pred_nn = model.predict(X_test_preprocessed)\nmean_squared_error(y_test_no_2021, y_pred_nn, squared=False)","625ada64":"songs_sorted = songs.sort_values(by='year').copy()\nsongs_sorted_no_2021 = songs_sorted[songs_sorted['year'] < 2021]","2e324fb4":"songs_sorted_no_2021['y_pred_knn'] = songs_sorted_no_2021['popularity'].copy()\nsongs_sorted_no_2021['y_pred_knn'].loc[130012:] = np.reshape(y_pred, (-1, )).copy() # I found the value 130,012 by displaying the first song of the test set... Not the best way to do, I know! \nsongs_sorted_no_2021['y_pred_nn'] = songs_sorted_no_2021['popularity'].copy()\nsongs_sorted_no_2021['y_pred_nn'].loc[130012:] = np.reshape(y_pred_nn, (-1, )).copy()","e61ddd84":"fig, ax = plt.subplots(figsize=(15, 4))\nax.plot(songs_sorted_no_2021.groupby('year')['y_pred_knn'].mean(),label='Predicted Popularity - K-Nearest Neigh.')\nax.plot(songs_sorted_no_2021.groupby('year')['y_pred_nn'].mean(),color=\"green\",label='Predicted Popularity - Neural Net.')\nax.plot(songs_sorted_no_2021.groupby('year')['popularity'].mean(),color=\"orange\",label='True Popularity')\nax.legend()\nax.set_title('Songs Popularity - Historic and Predictions')\nax.set_ylabel('Popularity', weight='bold')\nax.set_xlabel('Year', weight='bold')\nax.set_xticks(range(1920, 2020, 5))\n\n\nplt.show()","a4c3d404":"# !pip install youtube-search-python\n# !pip install IPython","b99c20b9":"import random\nfrom youtubesearchpython import VideosSearch\nfrom IPython.display import YouTubeVideo","2adcbfb9":"songs = pd.read_csv('data\/data.csv')\n\nrandom_song = songs['name'].loc[random.randint(0, len(songs))]\n\nvideosSearch = VideosSearch(random_song, limit=1)\n\nsong_yt_id = videosSearch.result()['result'][0]['id']\nYouTubeVideo(song_yt_id, width=1000, height=500)","505bec4f":"First, we need to split the data. \n\nIf we consider the usecase of Spotify, the company certainly wants to predict the popularity of future songs. Thus, we should split the dataset between songs issued before a certain year and songs issued after it. \n\nWe will make a train set of all the songs previous to 1996, a validation set composed of the songs issued between 1996 and 2014 to fine-tune and select our models (18% of the whole dataset), and a test set composed of the songs issued on and after 2014 (10% of the total dataset).","01cc0241":"For illustration purposes, here are the last songs of the train and val dataset. These songs were indeed issued in 1996 and 2014, respectively. ","6882ed34":"## Splitting the data","79e3f211":"`energy` and `loudness` are strongly correlated, we will probably have to abandon one of the two. Given how the two plots \"Popularity vs. energy\" and \"Popularity vs. loudness\" (not shown here) look, we will keep energy and discard loudness.","53072569":"Another area of concern is that **some songs have a null tempo**. It does not make any sense to have `tempo=0`. If we print the corresponding data subset, we can see that tempo is not the only null metric for these songs: `danceability`, `speechiness`, `valence` are also equal to 0. \n\nThis is the case for approximately 100 songs, we will thus drop such songs. ","0f29016e":"## Scoring the neural network","aebc3c5d":"The K-Nearest Neighbors regressor seems to perform better than the neural network on our test set.","23e66110":"This is our **root mean squared error on the test set**, ie. songs issued roughly between 2014 and 2020 (10% of the whole dataset). It is not very impressive, we will try to explain why later. ","9e81edc6":"- What could also be improved in this notebook is the whole pre-processing process: it should be implemented into a single pipeline. ","ca152a62":"A data scientist should always know his\/her dataset well! \n\nRun the following cells to play a random song in the dataset \ud83d\ude09","b194d52a":"Here, we can see that our metric of interest (`popularity`) has values that range from 0 to 100. 25% of the songs are 1\/100 popular or less, whereas only 25% of the songs achieve more than 43\/100 popularity.  ","3c080779":"### **Duration_ms**\n\nTracks last from 5 seconds to 90 minutes.","a9624630":"- We can see that our prediction is systematically higher than the real value. The origin of this \"problem\" can lie in the fact that the metric seems to have changed (as raised in [this thread](https:\/\/www.kaggle.com\/yamaerenay\/spotify-dataset-19212020-160k-tracks\/discussion\/214183)). Our models fail to adapt to the drop that occurs after the beginning of the 2000s, and it is even worse when including 2021! ","d398dd20":"And we can score the model on the test set. ","dca149b6":"### Introducing song names","bbfc6815":"## Visualising our result","c4817417":"### **Acousticness**\n\nThe `acousticness` ranges from 0 to 1. Acousticness of the majority of tracks is either close to 0 or 1.","0437c278":"### **Energy**\n\n`energy` measures the intensity and activity: energetic tracks feel faster and louder.","cc9bb153":"The computations are quite lengthy, so I would advise you save your train, validation and test set once and for all as I did in the cell below.","92e880d7":"We thus drop this approach with genres and will develop another method to take artists into account.","cfaee6fb":"Intuitively, we could suspect `energy` and `danceability` or `energy` and `loudness` would be similar. \n\nLet us check this by using Pearson's correlation coefficient. This correlation coefficient ranges from \u22121 to 1: a value of 1 implies that a linear equation describes the relationship between X and Y perfectly; a value of \u22121 implies that all data points lie on a line for which Y decreases as X increases. A value of 0 implies that there is no linear correlation between the variables.","294d7e7d":"In this Notebook, I will try to predict the popularity of songs in the Spotify Dataset, available [here](https:\/\/www.kaggle.com\/yamaerenay\/spotify-dataset-19212020-160k-tracks). \n\nMy approach is based on splitting the dataset according to the years, to be as similar to a real usecase as possible, without any data leakage.\n\nThis work is a part of the recruitment process for an internship at [Illuin Technology](https:\/\/www.illuin.tech\/en\/).\n\nAll intermediary data should be placed in a sub-folder called 'data' if you want to run the notebook as it is.","304fba59":"### Basic preprocessing - First iterations","6cb625c1":"You can start by loading the non-duplicates, cleaned dataset, by uncommenting the line below.\n\nThis analysis of the dataset was inspired by [this notebook](https:\/\/www.kaggle.com\/anatpeled\/spotify-popularity-prediction) on Kaggle. We figured that plotting several metrics compared to popularity could give us a first insight of relevant features to predict popularity.\n\nThanks to Guy Kahana & Anat Peled for enabling me to have a quick, clear vision of the dataset! ","09ef08e7":"We sense that `release_date` will not have much more added value than the year songs were released. We will thus drop this column. We have to find out whether there may be duplicates in the dataset in order to understand if we can only keep the name as a primary key. \n\n**We thus define a duplicate as several songs appearing in the dataset, for which the song title and the artist are strictly identical.** \n\nIn order to identify these possible duplicates, we create the `artist+name` column.","c65d0470":"We clearly see the better performance of the K-Nearest Neighbors regressor on this graph.","8938aa62":"## Recomposing the train set","f1ccbdb7":"We thus tried to use the songs' genre, thanks to the `data_w_genres` dataframe that was provided.","2abb0c6d":"Let's have more visual look at the training of the model.","9d4ed658":"Since we know that, for an equal number of plays, Spotify gives a higher popularity score to tracks that have been played recently than to those that have been played earlier, we might want to plot popularity according to time. ","fa4e58a8":"After some investigations, it seems like the artists presents in the `data_w_genres` dataframe are not the same ones as the ones present in the main file. ","4e8c4f0e":"# 3. Model construction","8d2ca49c":"## Model explorations","e0e6e8fe":"### **Liveness**\n\n`liveness` detects the presence of an audience. High liveness suggests the track was live.","432f18e9":"# 2. EDA and data visualisation","5992ecfb":"# 1. Imports and data cleaning","f69867c0":"The Random Search process was running indefinitely, probably because of a problem with the multiprocessing package (mentioned in [this issue](https:\/\/github.com\/jupyter\/notebook\/issues\/5261)). \n\nHaving tried several values for `n_neighbors`, performance - as measured by the root mean squared error on the validation set - seems to increase with the number of neighbors used. For instance, with `n_neighbors=18`:","b29aca72":"# 5. Evaluation on the test set","51782d19":"As we saw with the plot `popularity v. year`, there is a sudden drop of popularity around the end of 2020 that might be due to a change in the metric (cf. [this thread](https:\/\/www.kaggle.com\/yamaerenay\/spotify-dataset-19212020-160k-tracks\/discussion\/214183)). Thus, and since we know popularity scores tend to lag on the Spotify API, we drop the songs from 2021. ","20938849":"Wondering how to pre-process the artist, we realized that we had a dataframe linking artists to their music genre. This could be a good popularity indicator. ","d6b0cd0a":"We can see that after a few epochs, none of the two losses\/metrics is decreasing anymore. If we kept training the network, the `val_loss` would probably start increasing again as we would be overfitting the train set. ","731ad19a":"# 6. Conclusion","ede99091":"Uncomment and run this cell to avoid the lengthy computations with the `enchant` library.","aea6ff83":"At that stage, I wanted to find out whether the language was relevant to predict popularity. To do so, I re-ran models with the additional binary variable, indicating if the song title is in English or not.","17888d3e":"## Simple neural network","dfea009f":"This is problematic. These four lines obviously represent the same song (\"Champagne problems\" by Taylor Swift, issued around the end of 2020), but **they have very different popularity ratings**: the latter vary from 54 to 85!","1b7a38b6":"### **Danceability**\n\nThe `danceability` ranges from 0 to 1. Danceability seems normally distributed.","80b6b07b":"Now we will train our model on a larger train set, composed of the former `X_train` and `X_val`, and test its performances on `X_test`. ","4fa14cf6":"Does the `name` of the song influence its popularity?\n\nI tried to lemmatize the title and use a TF-IDF Vectorization afterwards, but song titles in the dataset are in multiple languages. They could not be used as such.\n\nThus, I imagined that a song could have more audience if its title were in English. I decided to use the [enchant](https:\/\/pypi.org\/project\/pyenchant\/) library to **check whether at least half the words in the song's title are in English**. ","a3e2f2ed":"### **Instrumentalness**\n\nThe `instrumentalness` being close to 1 means there are no vocals.","b34e2138":"## Fine-tuning KNN hyperparameters","30c5f4b0":"# 4. Further explorations","9e53265a":"# 7. A small bonus...","8a38fc11":"The drop around the year 2021 is not surprising as we have read that popularity \"lags\" by a few days\/weeks. The rest of the trend is interesting, as popularity rises until the year 2000 and then falls. ","c599293a":"After our unsuccessful first approach with the genres, we will now try another one, based on what we would do \"in real life\". \n\n- If the artist has done **more than one song in the train set**, we will compute the artist's mean popularity in the train set and replace the artist's name by his\/her popularity.\n- Otherwise, we will replace the artist's name by the mean popularity of the train dataset. We need this distinction to avoid training the model to look for a popularity score that is alreay included in the artist's popularity.","8dfbae88":"The outline of the document is: \n1. Imports and data cleaning\n2. EDA and data visualisation \n3. Model construction\n4. Model fine-tuning\n5. Evaluation on the test set\n6. Conclusion\n7. Bonus!","5a23bb9c":"## Scoring the KNN regressor","104f387b":"There are thus 14,948 duplicates. Let us look at a concrete example.","f838e04c":"I then add two columns that will be a copy of popularity, except on the test set, where these two columns will represent the predicted values for our best predictors.","e9c71aad":"**Info on popularity score**\n\n(according to Spotify for Developers - Documentation \/ https:\/\/developer.spotify.com\/documentation\/web-api\/reference\/tracks\/get-several-tracks\/)\n\nThe `popularity` of a track is a value between 0 and 100, with 100 being the most popular. The popularity is calculated by algorithm and is based, in the most part, on the total number of plays the track has had and how recent those plays are.\n\nGenerally speaking, songs that are being played a lot now will have a higher popularity than songs that were played a lot in the past. Duplicate tracks (e.g. the same track from a single and an album) are rated independently. Artist and album popularity is derived mathematically from track popularity. Note that the popularity value may lag actual popularity by a few days: the value is not updated in real time.","2510fb86":"### **Speechiness**\nWhen studying `speechiness`, we can see that songs that are too \"speechy\" are less popular. We will thus create a binary variable: \"Speech over 0.57\".","6c57ccf8":"**Author**: In\u00e8s Multrier  \n**Contact**: ines.multrier@polytechnique.edu","5eae6615":"**We are now ready to explore various models!**","1dc4c86a":"We can easily check that: \n\n$14948 + 159441 = 174389$\n\nWhere the latter corresponds to the number of entries in the initial dataset. We have thus succeeded in removing duplicates. Now that each song is unique, we will remove the `id` which is unnecessary. We also do not need `artist+name` anymore, as it was only meant to look for duplicates.","f5396e5c":"### Introducing the artists' popularity","8c227569":"Let's do the same with our neural network.","6b41dd42":"\nWe use `ColumnTransformer` to do various preprocessing tasks simultaneously: scaling the year, tempo and duration (contrarily to the others, these features are not on a 0-1 scale), one-hot encoding the key as it is a categorical variable ranging from 1 to 11 (more on the music key [here](https:\/\/en.wikipedia.org\/wiki\/Key_(music)) ): this will create 10 columns taking 0\/1 values.\n\nWe will use the artists and the songs title later on, for now we drop the columns. ","07e302a3":"We can now make predictions on the validation set with our model. In fact, we already know what the root mean squared error on the validation set will be, as we used `X_val`as a validation set during the training of our model, so the `val_RMSE`was computed at each step. We can verify this by running the cell below.","5d5a4722":"- If I had more than a week's time, I would have liked to find a way to use the lyrics of the songs. Several APIs propose this service but they require specific credentials and are generally not free. I tried to scrap a lyrics website but the result was not fully satisfying and I was banned after too many queries \ud83d\ude22 With clean lyrics, we could translate the dataset and introduce it into our analysis\/modelling.","4d4e2a95":"Here, I will try to plot the predicted popularity for the test set, compare it to the true popularity for the train and test set. Thus, I start by sorting the songs.","7495bb86":"Now that we have reconstituted our new train set, we can score our best model (KNN with 18 neighbors) on the test set which it has \"never seen before\".","ff52b7c4":"It did reduce the mean squared error! We will therefore dig deeper into this approach and add binary features that will tell us whether the song's name is in French, Spanish, Russian or Arabic (almost all of the world's most spoken languages, except Chinese and Hindi).\n\n**Warning**: the column creation is quite long to run (my guess is that `enchant` can be capricious), so I would advise you to do it only once and the next times, directly load `X_train`, `X_test` and `X_val` from the `csv` files with the cell at the end of this section.","f75b8dcb":"According to this info, a solution would be to replace the popularity for duplicates by the maximum of the popularity ratings. Indeed, popularity is a \"positive\" metric: the only thing that can make a song's popularity decrease is if it is not played at all for a while. Thus, if a song has three perfectly identical duplicates but one has a higher popularity score, it means it has been played more and, in particular, it has been played more recently. Thus, this score is more relevant than the two other to monitor this song's popularity. ","153c14a7":"### **Artists and genres?**\n\nUsing the artists' popularity without any data leakage is going to be tough but makes sense. Intuitively, **a song's popularity is largely based on its artist's popularity** but according to the Spotify API documentation, artists' popularity is itself computed based on the popularity of their tracks! ","737fd9c1":"The `len(songs_train[songs_train['artists'] == artist]['popularity'])>1` condition is supposed to ensure that if an artist has only done one song in the train set, then the song's popularity will not be counted as the artist's popularity (the model would be impossible to train!).\n\nStill, what might happen with this approach is that the artists's popularity will be a crucial information for the model: during training, it will give it more importance than it should. For the validation and the test set, we will use popularity from artists that are already present in the train set. "}}