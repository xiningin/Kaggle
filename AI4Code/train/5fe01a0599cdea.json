{"cell_type":{"0063edaa":"code","eb705b8f":"code","2dfb0919":"code","41697096":"code","4a946b7e":"code","681263ba":"code","75bb2bfa":"code","c826899e":"code","9c8431b1":"code","da0901b0":"code","c3d752e5":"code","01d44df5":"code","e76399a7":"code","595f11fe":"code","91f2d506":"code","9c91fc70":"code","e24d5a30":"code","1211d37e":"code","2566f738":"code","c1b8d8e9":"code","36f43848":"code","9028352f":"code","592f456c":"code","fa049360":"code","35e1db1a":"code","5e969973":"code","3f9c8885":"code","9d562fc6":"code","030c1d9d":"code","63ca4422":"code","00288594":"code","413412bc":"code","5e75b9a1":"code","08db7026":"code","bc74304c":"markdown","c61b04b2":"markdown","58f32081":"markdown","73ce6086":"markdown","3f2cea67":"markdown","ce168a69":"markdown","b9c44648":"markdown","bd411123":"markdown","44a44d4a":"markdown","08bc4551":"markdown","f7ac5596":"markdown","dc31d142":"markdown","a1124d97":"markdown","70bee370":"markdown","19e01ac2":"markdown"},"source":{"0063edaa":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Any results you write to the current directory are saved as output.","eb705b8f":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.cluster import KMeans","2dfb0919":"train = pd.read_csv('..\/input\/train.csv', index_col='ID')\ntest = pd.read_csv('..\/input\/test.csv', index_col='ID')","41697096":"train.head()","4a946b7e":"train.info()","681263ba":"sns.distplot(train.target)","75bb2bfa":"sns.distplot(np.log1p(train.target))\ny = np.log1p(train.target)","c826899e":"def rmsle(y, pred):\n    assert len(y) == len(pred)\n    return np.sqrt(np.mean(np.power(y-pred, 2)))","9c8431b1":"cols_with_onlyone_val = train.columns[train.nunique() == 1]\ntrain.drop(cols_with_onlyone_val.values, axis=1, inplace=True)\ntest.drop(cols_with_onlyone_val.values, axis=1, inplace=True)\ntrain = train.round(32)\ntest = test.round(32)","da0901b0":"colsToRemove = []\ncolumns = train.columns\nfor i in range(len(columns)-1):\n    v = train[columns[i]].values\n    dupCols = []\n    for j in range(i + 1,len(columns)):\n        if np.array_equal(v, train[columns[j]].values):\n            colsToRemove.append(columns[j])\ntrain.drop(colsToRemove, axis=1, inplace=True)\ntest.drop(colsToRemove, axis=1, inplace=True)","c3d752e5":"X = train.drop('target', axis = 1)","01d44df5":"X['mean'] = X.mean(axis = 1)\nX['sum'] = X.sum(axis = 1)\nX['max'] = X.max(axis = 1)\nX['min'] = X.min(axis = 1)\nX['std'] = X.std(axis = 1)","e76399a7":"test['mean'] = test.mean(axis = 1)\ntest['sum'] = test.sum(axis = 1)\ntest['max'] = test.max(axis = 1)\ntest['min'] = test.min(axis = 1)\ntest['std'] = test.std(axis = 1)","595f11fe":"cols = X[X > 0].count()[X[X > 0].count() > 200].sort_values().index\nX_train = X[cols].copy()\nX_test = test[cols].copy()","91f2d506":"clust = KMeans(n_clusters=15).fit(X_train)\nX_with_clust = X_train.join(pd.DataFrame(clust.transform(X_train), columns= [ 'clust_' + str(i) for i in range(15)], index=X_train.index))\nX_with_clust['cluster_name'] = clust.predict(X_train)","9c91fc70":"test_with_clust = X_test.join(pd.DataFrame(clust.transform(X_test), columns= [ 'clust_' + str(i) for i in range(15)], index=X_test.index))\ntest_with_clust['cluster_name'] = clust.predict(X_test)","e24d5a30":"rf = RandomForestRegressor(n_estimators=100)\nrf.fit(X_with_clust, y)\nfeatures = pd.DataFrame({'importance': rf.feature_importances_, 'name': X_with_clust.columns})\nX_short = X_with_clust[features.sort_values('importance', ascending=False).name[:500]]\ntest_short = test_with_clust[features.sort_values('importance', ascending=False).name[:500]]","1211d37e":"import lightgbm\nimport hyperopt\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials\n\nN_HYPEROPT_PROBES = 500\n\nHYPEROPT_ALGO = tpe.suggest  #  tpe.suggest OR hyperopt.rand.suggest","2566f738":"def get_lgb_params(space):\n    lgb_params = dict()\n    lgb_params['boosting_type'] = space['boosting_type'] if 'boosting_type' in space else 'gbdt'\n    lgb_params['application'] = 'regression'\n    lgb_params['metric'] = 'l2_root'\n    lgb_params['learning_rate'] = space['learning_rate']\n    lgb_params['num_leaves'] = int(space['num_leaves'])\n    lgb_params['min_data_in_leaf'] = int(space['min_data_in_leaf'])\n    lgb_params['min_sum_hessian_in_leaf'] = space['min_sum_hessian_in_leaf']\n    lgb_params['max_depth'] = -1\n    lgb_params['lambda_l1'] = space['lambda_l1'] if 'lambda_l1' in space else 0.0\n    lgb_params['lambda_l2'] = space['lambda_l2'] if 'lambda_l2' in space else 0.0\n    lgb_params['max_bin'] = int(space['max_bin']) if 'max_bin' in space else 256\n    lgb_params['feature_fraction'] = space['feature_fraction']\n    lgb_params['bagging_fraction'] = space['bagging_fraction']\n    lgb_params['bagging_freq'] = int(space['bagging_freq']) if 'bagging_freq' in space else 1\n\n    return lgb_params","c1b8d8e9":"obj_call_count = 0\ncur_best_loss = np.inf","36f43848":"X_1, X_test, y_1, y_test = train_test_split(X_short, y, test_size=0.2)\nX_train, X_val, y_train, y_val = train_test_split(X_1, y_1, test_size=0.3)\nD_train = lightgbm.Dataset(X_train, y_train)\nD_val = lightgbm.Dataset(X_val, y_val)","9028352f":"def objective(space):\n    global obj_call_count, cur_best_loss\n\n    obj_call_count += 1\n\n    #print('\\nXGB objective call #{} cur_best_loss={:7.5f}'.format(obj_call_count,cur_best_loss) )\n\n    lgb_params = get_lgb_params(space)\n\n    sorted_params = sorted(space.items(), key=lambda z: z[0])\n    params_str = str.join(' ', ['{}={}'.format(k, v) for k, v in sorted_params])\n    #print('Params: {}'.format(params_str) )\n\n    model = lightgbm.train(lgb_params,\n                           D_train,\n                           num_boost_round=10000,\n                           # metrics='mlogloss',\n                           valid_sets=D_val,\n                           # valid_names='val',\n                           # fobj=None,\n                           # feval=None,\n                           # init_model=None,\n                           # feature_name='auto',\n                           # categorical_feature='auto',\n                           early_stopping_rounds=100,\n                           # evals_result=None,\n                           verbose_eval=False,\n                           # learning_rates=None,\n                           # keep_training_booster=False,\n                           # callbacks=None\n                           )\n\n    nb_trees = model.best_iteration\n    val_loss = model.best_score\n\n    #print('nb_trees={} val_loss={}'.format(nb_trees, val_loss))\n\n    y_pred = model.predict(X_test, num_iteration=nb_trees)\n    test_loss = rmsle(y_test, y_pred)\n\n    #print('test_loss={}'.format(test_loss))\n\n    if test_loss<cur_best_loss:\n        cur_best_loss = test_loss\n        print('NEW BEST LOSS={}'.format(cur_best_loss))\n\n\n    return{'loss':test_loss, 'status': STATUS_OK }","592f456c":"space ={\n        'num_leaves': hp.quniform ('num_leaves', 10, 200, 1),\n        'min_data_in_leaf':  hp.quniform ('min_data_in_leaf', 10, 200, 1),\n        'feature_fraction': hp.uniform('feature_fraction', 0.75, 1.0),\n        'bagging_fraction': hp.uniform('bagging_fraction', 0.75, 1.0),\n        'learning_rate': hp.loguniform('learning_rate', -5.0, -2.3),\n        'min_sum_hessian_in_leaf': hp.loguniform('min_sum_hessian_in_leaf', 0, 2.3),\n        'max_bin': hp.quniform ('max_bin', 64, 512, 1),\n        'bagging_freq': hp.quniform ('bagging_freq', 1, 5, 1),\n        'lambda_l1': hp.uniform('lambda_l1', 0, 10 ),\n        'lambda_l2': hp.uniform('lambda_l2', 0, 10 ),\n       }","fa049360":"trials = Trials()","35e1db1a":"best = hyperopt.fmin(fn=objective,\n                     space=space,\n                     algo=HYPEROPT_ALGO,\n                     max_evals=N_HYPEROPT_PROBES,\n                     trials=trials,\n                     verbose=0)","5e969973":"print(get_lgb_params(best))","3f9c8885":"def run_lgb(train_X, train_y, val_X, val_y, test_X, params):\n    \n    lgtrain = lightgbm.Dataset(train_X, label=train_y)\n    lgval = lightgbm.Dataset(val_X, label=val_y)\n    evals_result = {}\n    model = lightgbm.train(params, lgtrain, 1000, valid_sets=[lgval], early_stopping_rounds=100, verbose_eval=False, evals_result=evals_result)\n    \n    pred_test_y = model.predict(test_X, num_iteration=model.best_iteration)\n    return pred_test_y, model, evals_result","9d562fc6":"X_train = X_short.reset_index(drop=True)\ny_train = y.reset_index(drop=True)","030c1d9d":"params = get_lgb_params(best)\nkf = KFold(n_splits=5, shuffle=True, random_state=2017)\npred_test_lgb = 0\nfor dev_index, val_index in kf.split(X_train):\n    dev_X, val_X = X_train.loc[dev_index,:], X_train.loc[val_index,:]\n    dev_y, val_y = y_train[dev_index], y_train[val_index]\n    pred_test, model, evals_result = run_lgb(dev_X, dev_y, val_X, val_y, test_short,params)\n    pred_test_lgb += pred_test\npred_test_lgb \/= 5.\npred_test_lgb = np.expm1(pred_test_lgb)","63ca4422":"from catboost import CatBoostRegressor","00288594":"cb_model = CatBoostRegressor(iterations=500,\n                             learning_rate=0.05,\n                             depth=10,\n                             eval_metric='RMSE',\n                             random_seed = 42,\n                             bagging_temperature = 0.2,\n                             od_type='Iter',\n                             metric_period = 50,\n                             od_wait=20)","413412bc":"X_train, X_val, y_train, y_val = train_test_split(X_short, y, test_size = 0.2, random_state = 42)\ncb_model.fit(X_train, y_train,\n             eval_set=(X_val, y_val),\n             use_best_model=True,\n             verbose=False)","5e75b9a1":"pred_test_cat = np.expm1(cb_model.predict(test_short))","08db7026":"sub = pd.DataFrame()\nsub['ID'] = test.index\nsub['target'] = (pred_test_lgb + pred_test_cat)\/2\nsub.to_csv('submission.csv',index=False)","bc74304c":"Some columns contain too few values. Delete them","c61b04b2":"This does not look like a normal distribution, so let's use the logarithm","58f32081":"Function to count metrics","73ce6086":"As we can see: our data is table of anonimus digits, we can not guess of this, so we can use only mathematics","3f2cea67":"Drop columns with only 1 value","ce168a69":"Add clustering, as addition features","b9c44648":"## Combine models","bd411123":"## CatBoost","44a44d4a":"## Feature engineering","08bc4551":"Drop equal columns ","f7ac5596":"Select features with Rendom Forest regressor","dc31d142":"## Target variable","a1124d97":"We've chosen the parameters and now we can train the model","70bee370":"Thanks to:<br>\nhttps:\/\/www.kaggle.com\/the1owl\/love-is-the-answer<br>\nhttps:\/\/www.kaggle.com\/alexpengxiao\/preprocessing-model-averaging-by-xgb-lgb-1-39<br>\nhttps:\/\/www.kaggle.com\/samratp\/lightgbm-xgboost-catboost","19e01ac2":"## Search for lightGBM params"}}