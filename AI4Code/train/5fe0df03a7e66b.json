{"cell_type":{"9491dd9f":"code","87da6730":"code","820c02b2":"code","069669da":"code","600609dc":"code","f214cda4":"code","659bba80":"code","22e4c496":"code","5c5351f7":"code","7fd74862":"code","22ecb920":"code","03148922":"code","79875b7d":"code","8ea09620":"code","123eb3f3":"code","8c438184":"markdown","6b96357f":"markdown","c4b7b9e9":"markdown","9097543a":"markdown","638410ee":"markdown","054a53a9":"markdown","b4758c9a":"markdown","d4c46009":"markdown","f74561e8":"markdown","52503fb4":"markdown","6ba802d8":"markdown","9d9ae659":"markdown","013a407b":"markdown","8475a885":"markdown","cada6351":"markdown","53537807":"markdown"},"source":{"9491dd9f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\n\n\nfrom sklearn.metrics import mean_absolute_error,accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom xgboost import XGBClassifier,XGBRegressor","87da6730":"X_full = pd.read_csv(\"..\/input\/train.csv\")\nX_test_full = pd.read_csv(\"..\/input\/test.csv\")\nprint(\"Loaded.\")","820c02b2":"#X_full.shape  \n#(262144, 258)\n#X_test.shape\n#(131073, 257)\n\nX_full.head(10)","069669da":"#X_full.isnull().sum() #it's not showing some columns... how do  know for sure?\ncols_missingvals = [col for col in X_full.columns if X_full[col].isnull().any()]\nprint(cols_missingvals)","600609dc":"\ny = X_full.target\nX = X_full.drop(['target','id'], axis=1)\nX_test = X_test_full.drop(['id'], axis=1)\n\nX_train, X_valid, y_train, y_valid = train_test_split(X,y,train_size=0.8,test_size=0.2,random_state=0)\n\nmy_model = XGBRegressor(n_estimators=100, learning_rate=0.05)\nmy_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)\n","f214cda4":"predictions = my_model.predict(X_valid)\nprint(\"MAE: \" + str(mean_absolute_error(predictions, y_valid)))\n","659bba80":"# preds = my_model.predict(X_valid)\n# preds_test = my_model.predict(X_test)\n# preds_test_rounded = np.around(preds_test,decimals=1)\n\n# output = pd.DataFrame({'id': X_test_full.id,\n#                        'target': preds_test_rounded})\n# output.to_csv('submission.csv', index=False)\n\n# a = pd.read_csv('submission.csv')\n# a.head(10)\n","22e4c496":"# b = pd.read_csv('..\/input\/sample_submission.csv')\n# b.head(10)","5c5351f7":"import matplotlib.pyplot as plt\n\n#plot bar chart with matplotlib\nplt.figure(figsize=(17,10))\n\ny_pos = np.arange(len(X.columns))\n\nplt.bar(y_pos, my_model.feature_importances_, align='center', alpha=0.5)\nplt.xticks(y_pos, X.columns)\nplt.xticks(rotation=90)\n\nplt.xlabel('Features')\nplt.ylabel('Feature Importance')\n\nplt.title('Feature importances')\n\nplt.show()","7fd74862":"from plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\ndata = [go.Bar(\n            x=y_pos,\n            y=my_model.feature_importances_\n    )]\n\n#iplot(data)\n\nlayout = go.Layout(\n    xaxis = go.layout.XAxis(\n        tickmode = 'array',\n        tickvals = y_pos,\n        ticktext = X.columns,\n        tickangle = -90\n    )\n)\n\nfig = go.Figure(\n    data = data,\n    layout = layout\n)\n\niplot(fig)","22ecb920":"features = ['bluesy-rose-wallaby-discard','cranky-cardinal-dogfish-ordinal','homey-sepia-bombay-sorted','hasty-blue-sheep-contributor',\n            'blurry-wisteria-oyster-master','baggy-mustard-collie-hint','beady-champagne-bullfrog-grandmaster','blurry-flax-sloth-fepid',\n           'grumpy-zucchini-kudu-kernel','bluesy-amber-walrus-fepid','hazy-tan-schnauzer-hint','gloppy-turquoise-quoll-goose',\n            'snoopy-red-zonkey-unsorted','snappy-brass-malamute-entropy','squeaky-khaki-lionfish-distraction',\n            'crappy-pumpkin-saola-grandmaster','wheezy-harlequin-earwig-gaussian','tasty-buff-monkey-learn','dorky-turquoise-maltese-important',\n           'hasty-puce-fowl-fepid','stuffy-periwinkle-zebu-discard','breezy-myrtle-loon-discard','woolly-gold-millipede-fimbus',\n           'bluesy-amethyst-octopus-gaussian','dorky-cream-flamingo-novice','gimpy-asparagus-eagle-novice','stealthy-yellow-lobster-goose',\n           'freaky-olive-insect-ordinal','greasy-scarlet-paradise-goose','pretty-copper-insect-discard','gloppy-buff-frigatebird-dataset',\n           'wheezy-lavender-catfish-master','cheeky-pear-horse-fimbus','stinky-olive-kiwi-golden','stealthy-azure-gopher-hint',\n            'sleazy-russet-iguana-unsorted','surly-corn-tzu-kernel','woozy-apricot-moose-hint','greasy-magnolia-spider-grandmaster',\n           'chewy-bistre-buzzard-expert','wheezy-myrtle-mandrill-entropy','muggy-turquoise-donkey-important','blurry-buff-hyena-entropy']","03148922":"y = X_full.target\nX = X_full[features]\nX_test = X_test_full[features]\nX_train, X_valid, y_train, y_valid = train_test_split(X,y,train_size=0.8,test_size=0.2,random_state=0)","79875b7d":"# my_model = XGBClassifier(n_estimators=100, learning_rate=0.05)\n# my_model.fit(X_train, y_train, \n#              early_stopping_rounds=5, \n#              eval_set=[(X_valid, y_valid)], \n#              verbose=False)\n# predictions = my_model.predict(X_valid)\n# print(\"Accuracy Score: \" + str(accuracy_score(predictions, y_valid)))\n\n#Accuracy Score: 0.5095271700776288","8ea09620":"my_model = XGBRegressor(n_estimators=100, learning_rate=0.05)\nmy_model.fit(X_train, y_train, \n             early_stopping_rounds=5, \n             eval_set=[(X_valid, y_valid)], \n             verbose=False)\npredictions = my_model.predict(X_valid)\nprint(\"MAE: \" + str(mean_absolute_error(predictions, y_valid)))","123eb3f3":"preds = my_model.predict(X_valid)\npreds_test = my_model.predict(X_test)\npreds_test_rounded = np.around(preds_test,decimals=1)\n\noutput = pd.DataFrame({'id': X_test_full.id,\n                       'target': preds_test_rounded})\noutput.to_csv('submission.csv', index=False)\n","8c438184":"**WTF???**\n\nWHYYYYYY???","6b96357f":"# Update - Feature Importance\n\nSo today I cam across this [this excellent kernel](https:\/\/www.kaggle.com\/aleksandradeis\/how-to-get-upvotes-for-a-kernel-on-kaggle) by [Aleksandra Deis](https:\/\/www.kaggle.com\/aleksandradeis).\n\nShe used a bar graph to plot the feature importance and I realized it would be really useful for my project. Let's give it a shot.","c4b7b9e9":"So that is a LOT of data! I don't see an index, and 'target' is our target (don't say duh).\n\nWell good thing is there's no categorical data. Now let's check for missing values.","9097543a":"# Notes\n\n* First I had used XGBClassifier. But that gave me targets in 0s and 1s. Since the output required was in decimals (0.5), I assumed this problem to be a regression one and switched to XGBRegressor. Takes way longer!\n* The Numpy.around() method came in handy. Had to Google a lot! \n* Private Score: 0.50036, Public Score: 0.50093\n* I'm not gonna continue practicing on this, takes way too much time. I might return to this later. I could switch back to XGBClassifier and quickly try out different feature sets. Whichever gives the highest accuracy score can be used as the feature set for XGBRegressor. Maybe if I drop a few columns it'll get quicker.","638410ee":"# Predictions\n\nAlright time for predictions, fingers crossed.","054a53a9":"# Submission\n\n**NOTE**: I commented the old code. To uncomment, select all (Ctrl+A) and hit Ctrl+\/","b4758c9a":"That was mucher tougher than I thought. I'm sure there must be an easier way... \n\nLet's give it a shot then!","d4c46009":"So no missing values too! This dataset makes no sense. ","f74561e8":"# Problem\n\nI don't know! I have absolutely no idea. The sample submission has two columns, the second column has 0.5 in each row. What does that mean? \n\nAnyhoo, let's at least have a look at the data first.","52503fb4":"# Model\n\nLet's split the data into train and valid and then use XGBClassifier to create the model. (Switched to XGBRegressor later!!)\n\nAnd what about the features? How do I know which ones to add and which ones to remove? I guess that's the challenge. Well, doesn't seem like my cup of tea. I'll come back later. For now, I'll just add all the columns (removing target and id).","6ba802d8":"Woah! That is SO interesting! This is the first time I'm seeing how important data visualization is.\n\nAs we can see there are many columns that don't have any importance at all. I'll drop them and check my model's accuracy. But first I need to divide the graph. I need to make separate graphs using fewer columns at a time so that I can see the names of the unnecessary columns.\n\nActually, on another look, it seems: no. useful columns < no. of useless columns. So I'll find out the names of the useful columns and add them as features.","9d9ae659":"Awesome! :D So let's note down the features (excited!).\n","013a407b":"0.5??? MAE is usually in tens of thousands...\n\nSo how do I improve the model? Obviously I can't do any data cleaning. So I'm guessing my model's accuracy is dependent entirely on my features. So how do I select the features?\n\nI could go Brute-force (I see no other option). Remove one column at a time, remodel and check the accuracy. That would take way too long! I'll try few...","8475a885":"# Data preparation","cada6351":"Earlier I had done the Titanic competition, enjoyed it. Now want to try this one too as it is also a Classification problem. ","53537807":"On second thought, what if I just used plotly instead? I could hover over each column, it'd show me the column name and I could note it down. That would be quicker (Now I gotta Google bar charts using plotly...). You could also use the zoom feature.\n"}}