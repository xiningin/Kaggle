{"cell_type":{"639cc824":"code","c6abd762":"code","1ba0852a":"code","c2777feb":"code","4d92d2ec":"code","ffc50d75":"code","8e8a3266":"code","ad766542":"code","602c3229":"code","2fe6815c":"code","c951fb95":"code","01829189":"code","12c776bf":"code","9d0bfc38":"code","0aad10e7":"code","0f09d895":"code","e1cd8b60":"code","3dac9873":"code","9237904f":"code","20bf1604":"code","8d0d64e0":"code","c8028837":"code","9d2cee8c":"code","9ed1f700":"code","85d1e9dd":"markdown","b8f3991c":"markdown","1388d719":"markdown","41b7abba":"markdown","f68b048f":"markdown","5178825c":"markdown","a989ad3a":"markdown","e0be9e13":"markdown","4933a209":"markdown","12d78473":"markdown","469f7ccb":"markdown","d2a8d49e":"markdown","e1b26998":"markdown","226af90d":"markdown","e617a207":"markdown","b199c46b":"markdown","6f1c7065":"markdown","1f3b7a79":"markdown","b36951a1":"markdown","9276d8e5":"markdown","359db49d":"markdown","4095ae21":"markdown","1b4c99ec":"markdown"},"source":{"639cc824":"import numpy as np\nimport pandas as pd\n\n# Preprocessing\nimport re\nimport nltk\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.corpus import stopwords\n\n# Feature extraction, model evaluation and hyperparemter optimization\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\n# Classifiers\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier","c6abd762":"df = pd.read_csv(\"..\/input\/spam.csv\", encoding = \"latin-1\")\ndf.head()","1ba0852a":"print(sum(df.iloc[:, 2].notna()))\ndf.iloc[:, 2].unique()","c2777feb":"print(sum(df.iloc[:, 3].notna()))\ndf.iloc[:, 3].unique()","4d92d2ec":"print(sum(df.iloc[:, 4].notna()))\ndf.iloc[:, 4].unique()","ffc50d75":"df = df[[\"v1\", \"v2\"]]\ndf.head()","8e8a3266":"df.columns = [\"class\", \"message\"]\ndf.head()","ad766542":"# Download the last available version of the stopwords\nnltk.download(\"stopwords\")","602c3229":"def clean_message(message):\n    \"\"\"\n    Receives a raw message and clean it using the following steps:\n    1. Remove all non-words in the message\n    2. Transform the message in lower case\n    3. Remove all stop words\n    4. Perform stemming\n\n    Args:\n        message: the raw message\n    Returns:\n        a clean message using the mentioned steps above.\n    \"\"\"\n    \n    message = re.sub(\"[^A-Za-z]\", \" \", message)\n    message = message.lower()\n    message = message.split()\n    stemmer = PorterStemmer()\n    message = [stemmer.stem(word) for word in message if word not in set(stopwords.words(\"english\"))]\n    message = \" \".join(message)\n    return message","2fe6815c":"# Testing how our function works\nmessage = df.message[0]\nprint(message)\n\nmessage = clean_message(message)\nprint(message)","c951fb95":"corpus = []\nfor i in range(0, len(df)):\n    message = clean_message(df.message[i])\n    corpus.append(message)","01829189":"corpus[:5]","12c776bf":"print(round(sum(df[\"class\"] == \"ham\") \/ len(df) * 100, 2))\nprint(round(sum(df[\"class\"] == \"spam\") \/ len(df) * 100, 2))","9d0bfc38":"count_vectorizer = CountVectorizer()\nfeatures = count_vectorizer.fit_transform(corpus).toarray()\nfeatures.shape","0aad10e7":"labels = df[\"class\"].values\nlabels[:5]","0f09d895":"features_train, features_test, labels_train, labels_test = train_test_split(features, labels, \n    test_size = 0.20, stratify = labels, random_state = 42)","e1cd8b60":"print(count_vectorizer.get_feature_names()[:10])\nprint(count_vectorizer.get_feature_names()[-10:])","3dac9873":"nb_classifier = MultinomialNB()","9237904f":"k_fold = StratifiedKFold(n_splits = 10)\nscores = cross_val_score(nb_classifier, features_train, labels_train, cv = k_fold)\nprint(\"mean:\" , scores.mean(), \"std:\", scores.std())","20bf1604":"nb_classifier.fit(features_train, labels_train)\nlabels_predicted = nb_classifier.predict(features_test)\naccuracy_score(labels_test, labels_predicted)","8d0d64e0":"confusion_matrix(labels_test, labels_predicted, labels = [\"ham\", \"spam\"])","c8028837":"kfold = StratifiedKFold(n_splits = 10)\nparameters = {\"alpha\": np.arange(0, 1, 0.1)}\nsearcher = GridSearchCV(MultinomialNB(), param_grid = parameters, cv = kfold)\nsearcher.fit(features_train, labels_train)\nbest_multinomial_nb = searcher.best_estimator_\n\nprint(\"Best CV params\", searcher.best_params_)\nprint(\"Best CV accuracy\", searcher.best_score_)\nprint(\"Test accuracy of best grid search hypers:\", searcher.score(features_test, labels_test))","9d2cee8c":"labels_predicted = best_multinomial_nb.predict(features_test)\nprint(\"Accuracy Score:\", accuracy_score(labels_test, labels_predicted))\nprint(classification_report(labels_test, labels_predicted))","9ed1f700":"%%time\n\nmodels = [LogisticRegression(), DecisionTreeClassifier(), RandomForestClassifier()]\nfor model in models:\n    model.fit(features_train, labels_train)\n\n    scores = cross_val_score(model, features_train, labels_train, cv = kfold)\n    print(type(model))\n    print(\"Mean score:\" , scores.mean(), \"Std:\", scores.std())\n    print()\n\n    predictions = model.predict(features_test)\n    accuracy_score(labels_test, predictions)\n\n    labels_predicted = model.predict(features_test)\n    print(\"Test Accuracy Score:\", accuracy_score(labels_test, labels_predicted))\n    print(classification_report(labels_test, labels_predicted))","85d1e9dd":"## Business Problem\nCan we use this dataset to build a protection model that will accurately classify which messages are spam? This application is widely used from the email service providers like Gmail, Yahoo, and so on.\n\n<img src=\"https:\/\/i.gifer.com\/Ou1t.gif\" style=\"height:400px\"\/>\nImage Source: https:\/\/i.gifer.com\/Ou1t.gif","b8f3991c":"<img src=\"https:\/\/media.giphy.com\/media\/2j5RA3SioKdck\/giphy.gif\" style=\"height:400px\"\/>\nImage Source: https:\/\/media.giphy.com\/media\/2j5RA3SioKdck\/giphy.gif","1388d719":"### Scores definitions\nTo choose our model we'll use the accuracy, recall, precision, and f1 score. Here are some definitions for this metrics:\n* Accuracy: Overall, how often is the classifier correct?\n* Recall: When it's actually yes, how often does it predict yes?\n* Precision: When it predicts yes, how often is it correct?\n* F1 score: can be interpreted as a weighted average of the precision and recall.","41b7abba":"## Loading the Dataset","f68b048f":"### Other Classifiers: Logistic Regression, Decision Tree, Random Forest","5178825c":"### Cleaning the messages\nWe want to keep only the important and useful words. To achieve these we will follow the steps:\n1. **Keep only the words** in the message\n2. Transform all words in **lower case**. We want **\"Love\"** and **\"love\"** to mean the same thing.\n3. Remove all **stop words**. Stop words usually refers to the most common words in a language, for example: **\"the\", \"a\", \"is\", etc.** We don't need these words. They don't give us any insight.\n4. Perform **stemming**. Stemming is a process in which we get the **root of the words**. We want all the different versions of the same word to be presented in one word. They all mean the same thing. Example: **\"love\", \"loving\", \"lovely\".**","a989ad3a":"### Fine-Tuning Multinomial Naive Bayes","e0be9e13":"## Loading the needed libraries","4933a209":"## Conclusions\nWell, it is controversial which is the best model. It depends on what's important for our spam detection. Personally, I think that the precision metric for the spam class is very important, but the recall is also important. In such a case when we don't know which classifier to choose. We can use the best f1 score. If some classifiers have exactly the same f1 score, we can choose the simpler one. So, if we follow this rule, we can see that the logistic regression give us the best score.","12d78473":"### Fitting a Multinomial Naive Bayes Classifier.\nThe multinomial Naive Bayes classifier is suitable for classification with discrete features (e.g., word counts for text classification). The multinomial distribution normally requires integer feature counts. However, in practice, fractional counts such as tf-idf may also work. That's why we're going to try this model first.","469f7ccb":"## Warning: check me later\nThere is something strange here after the fine-tuning we have a little bit more bad accuracy.\n**Is this the accuracy paradox?**","d2a8d49e":"## Cleaning the Dataset\nWe can see that we have 5 columns with very confusing names. However, it's easy to see that the first column contains the target. The second one contains the message text. The other columns may be some additional notes. Let's explore them a little bit.","e1b26998":"## Modelling the Data","226af90d":"### Splitting the Data into Test and Training Sets","e617a207":"The columns are still with confusing names, let's rename.","b199c46b":"## Dataset Information\nThe SMS Spam Collection is a set of SMS tagged messages that have been collected for SMS Spam research. It contains one set of SMS messages in English of 5,574 messages, tagged according being ham (legitimate) or spam.\n\nSource: https:\/\/www.kaggle.com\/uciml\/sms-spam-collection-dataset\/home","6f1c7065":"## Exploring the Data\nLet's see what part of the messages are **spam** and what are legitimate (**ham**).","1f3b7a79":"### Creating a Bag of Words Model\nBag of Words model is a very popular **NLP model** used to **preprocess the texts** to classify before fitting the classification algorithms.","b36951a1":"# TODOs\n* Try different values for test_size in the \"train_test_split\" function\n* Try CountVectorizer with some values for the \"max_features\" parameter\n* Use pickle to save the trained classifiers\n\n# Future Ideas\n* Compare more classifiers\n* Try to use TFIDF and compare the results\n* Try to use Dimensionality Reduction\n\n# Notes\n* Reduce the number of the features, because they are too many now and this may lead to overfitting\n* Add more data to avoid high bias","9276d8e5":"## Model Selection and Improvement","359db49d":"# Resources:\n* https:\/\/www.dataschool.io\/simple-guide-to-confusion-matrix-terminology\/\n* https:\/\/en.wikipedia.org\/wiki\/Natural_language_processing\n* https:\/\/en.wikipedia.org\/wiki\/Stop_words\n* https:\/\/en.wikipedia.org\/wiki\/Stemming\n* http:\/\/scikit-learn.org\/stable\/index.html\n* https:\/\/stats.stackexchange.com\/questions\/250273\/benefits-of-stratified-vs-random-sampling-for-generating-training-data-in-classi\/250742#250742\n* https:\/\/stats.stackexchange.com\/questions\/117643\/why-use-stratified-cross-validation-why-does-this-not-damage-variance-related-b\/117649#117649?newreg=2a9d984517504dcbbf55fda2f11489b7\n* https:\/\/stats.stackexchange.com\/questions\/49540\/understanding-stratified-cross-validation","4095ae21":"Well, it seems that these column contains some additional comments about the messages. However, they contain only a few values and there is no documentation about them in the source. I think it's safe to remove them and try to build our machine learning model only on the message text.","1b4c99ec":"# Spam Detection\n## Author: Ventsislav Yordanov"}}