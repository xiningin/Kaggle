{"cell_type":{"01f95a5a":"code","c8814f3d":"code","f629f6fb":"code","7afe7756":"code","49c8b46c":"code","a1eec561":"code","f8a86623":"code","1988c56e":"code","7657aa11":"code","d9bfff1d":"code","5d068bbd":"code","3594c1d9":"code","a08584b1":"markdown","6328263d":"markdown","25d979b4":"markdown","447e0f14":"markdown"},"source":{"01f95a5a":"! pip install sklearn","c8814f3d":"pip install glove_python","f629f6fb":"import os\nimport pandas as pd\nimport numpy as np","7afe7756":"DATASET_DIR = \"..\/input\/automated-essay-scoring-dataset\/\"\nGLOVE_DIR = '.\/glove.6B\/'\nSAVE_DIR = '.\/'\n","49c8b46c":"X = pd.read_csv(os.path.join(DATASET_DIR, 'training_set_rel3.tsv'), sep='\\t', encoding='ISO-8859-1')\ny = X['domain1_score']\nX = X.dropna(axis=1)\nX = X.drop(columns=['rater1_domain1', 'rater2_domain1'])\n\nX.head()","a1eec561":"minimum_scores = np.array([-1, 2, 1, 0, 0, 0, 0, 0, 0])\nmaximum_scores = np.array([-1, 12, 6, 3, 3, 4, 4, 30, 60])","f8a86623":"old_min = minimum_scores[X['essay_set']]\nold_max = maximum_scores[X['essay_set']]\nold_range = old_max - old_min \nnew_range = (4 - 1)  \nX['score'] = (((X['domain1_score'] - old_min) * new_range) \/ old_range) + 1\n\n# round score to nearest integer for cohen kappa calculation\ny = np.around(X['score'])\n\nX.head()","1988c56e":"\nimport numpy as np\nimport nltk\nimport re\nfrom nltk.corpus import stopwords\nfrom gensim.models import Word2Vec\n\ndef essay_to_wordlist(essay_v, remove_stopwords):\n    \"\"\"Remove the tagged labels and word tokenize the sentence.\"\"\"\n    essay_v = re.sub(\"[^a-zA-Z]\", \" \", essay_v)\n    words = essay_v.lower().split()\n    if remove_stopwords:\n        stops = set(stopwords.words(\"english\"))\n        words = [w for w in words if not w in stops]\n    return (words)\n\ndef essay_to_sentences(essay_v, remove_stopwords):\n    \"\"\"Sentence tokenize the essay and call essay_to_wordlist() for word tokenization.\"\"\"\n    tokenizer = nltk.data.load('tokenizers\/punkt\/english.pickle')\n    raw_sentences = tokenizer.tokenize(essay_v.strip())\n    sentences = []\n    for raw_sentence in raw_sentences:\n        if len(raw_sentence) > 0:\n            sentences.append(essay_to_wordlist(raw_sentence, remove_stopwords))\n    return sentences\n\ndef makeFeatureVec(words, model, num_features):\n    \"\"\"Make Feature Vector from the words list of an Essay.\"\"\"\n    featureVec = np.zeros((num_features,),dtype=\"float32\")\n    num_words = 0.\n      \n    featureVec = np.divide(featureVec,num_words)\n    return featureVec\n\n\ndef getAvgFeatureVecs(essays, model, num_features):\n    \"\"\"Main function to generate the word vectors for word2vec model.\"\"\"\n    counter = 0\n    essayFeatureVecs = np.zeros((len(essays),num_features),dtype=\"float32\")\n    for essay in essays:\n        essayFeatureVecs[counter] = makeFeatureVec(essay, model, num_features)\n        counter = counter + 1\n    return essayFeatureVecs","7657aa11":"\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Lambda, Flatten\nfrom tensorflow.keras.models import Sequential, load_model, model_from_config\nimport tensorflow.keras.backend as K\n\ndef get_model():\n    \"\"\"Define the model.\"\"\"\n    model = Sequential()\n    model.add(LSTM(300, dropout=0.4, recurrent_dropout=0.4, input_shape=[1, 300], return_sequences=True))\n    model.add(LSTM(64, recurrent_dropout=0.4))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation='relu'))\n\n    model.compile(loss='mean_squared_error', optimizer='rmsprop', metrics=['mae'])\n    model.summary()\n\n    return model","d9bfff1d":"from sklearn.model_selection import KFold\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import cohen_kappa_score\nfrom glove import Glove, Corpus\n\n\ncv = KFold(n_splits=5, shuffle=True)\nresults = []\ny_pred_list = []\n\ncount = 1\nfor traincv, testcv in cv.split(X):\n    \n    print(\"\\n--------Fold {}--------\\n\".format(count))\n    X_test, X_train, y_test, y_train = X.iloc[testcv], X.iloc[traincv], y.iloc[testcv], y.iloc[traincv]\n    \n    train_essays = X_train['essay']\n    test_essays = X_test['essay']\n    \n    sentences = []\n    \n    for essay in train_essays:\n        # Obtaining all sentences from the training essays.\n        sentences += essay_to_sentences(essay, remove_stopwords = True)\n    \n    # Initializing variables for word2vec model.\n    num_features = 300 \n    min_word_count = 40\n    num_workers = 4\n    context = 10\n    downsampling = 1e-3\n\n    corpus = Corpus()\n\n    #Training the corpus to generate the co occurence matrix which is used in GloVe\n    corpus.fit(sentences, window=10)\n\n    model = Glove(no_components=5, learning_rate=0.05) \n    model.fit(corpus.matrix, epochs=50, no_threads=4, verbose=True)\n    model.add_dictionary(corpus.dictionary)\n    print(corpus.dictionary)\n\n    model.save('glove.model')\n    \n    \n    # Generate training and testing data word vectors.\n    clean_train_essays = []\n    for essay_v in train_essays:\n        clean_train_essays.append(essay_to_wordlist(essay_v, remove_stopwords=True))\n    trainDataVecs = getAvgFeatureVecs(clean_train_essays, model, num_features)\n    \n    clean_test_essays = []\n    for essay_v in test_essays:\n        clean_test_essays.append(essay_to_wordlist( essay_v, remove_stopwords=True ))\n    testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n    \n    trainDataVecs = np.array(trainDataVecs)\n    testDataVecs = np.array(testDataVecs)\n    \n    # Reshaping train and test vectors to 3 dimensions. (1 represnts one timestep)\n    trainDataVecs = np.reshape(trainDataVecs, (trainDataVecs.shape[0], 1, trainDataVecs.shape[1]))\n    testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n    \n    lstm_model = get_model()\n    lstm_model.fit(trainDataVecs, y_train, batch_size=64, epochs=50)\n    y_pred = lstm_model.predict(testDataVecs)\n    \n    # Save any one of the 8 models.\n    if count == 5:\n         lstm_model.save('.\/final_lstm.h5')\n            \n    # Round y_pred to the nearest integer.\n    y_pred = np.around(y_pred)\n    \n    # Evaluate the model on the evaluation metric. \"Quadratic mean averaged Kappa\"\n    result = cohen_kappa_score(y_test.values,y_pred,weights='quadratic')\n    print(\"Kappa Score: {}\".format(result))\n    results.append(result)\n\n    count += 1","5d068bbd":"print(\"Average Kappa score after a 5-fold cross validation: \", np.around(np.array(results).mean(),decimals=4))","3594c1d9":"    import math\n    from gensim.test.utils import datapath\n    \n    contentBad = \"\"\"\n        In \u201cLet there be dark,\u201d Paul Bogard talks about the importance of darkness.\n\nDarkness is essential to humans. Bogard states, \u201cOur bodies need darkness to produce the hormone melatonin, which keeps certain cancers from developing, and our bodies need darkness for sleep, sleep. Sleep disorders have been linked to diabetes, obesity, cardiovascular disease and depression and recent research suggests are main cause of \u201cshort sleep\u201d is \u201clong light.\u201d Whether we work at night or simply take our tablets, notebooks and smartphones to bed, there isn\u2019t a place for this much artificial light in our lives.\u201d (Bogard 2). Here, Bogard talks about the importance of darkness to humans. Humans need darkness to sleep in order to be healthy.\n\nAnimals also need darkness. Bogard states, \u201cThe rest of the world depends on darkness as well, including nocturnal and crepuscular species of birds, insects, mammals, fish and reptiles. Some examples are well known\u2014the 400 species of birds that migrate at night in North America, the sea turtles that come ashore to lay their eggs\u2014and some are not, such as the bats that save American farmers billions in pest control and the moths that pollinate 80% of the world\u2019s flora. Ecological light pollution is like the bulldozer of the night, wrecking habitat and disrupting ecosystems several billion years in the making. Simply put, without darkness, Earth\u2019s ecology would collapse...\u201d (Bogard 2). Here Bogard explains that animals, too, need darkness to survive.\n    \"\"\" \n    \n    contentGood = \"\"\"\n        In response to our world\u2019s growing reliance on artificial light, writer Paul Bogard argues that natural darkness should be preserved in his article \u201cLet There be dark\u201d. He effectively builds his argument by using a personal anecdote, allusions to art and history, and rhetorical questions.\n\nBogard starts his article off by recounting a personal story \u2013 a summer spent on a Minnesota lake where there was \u201cwoods so dark that [his] hands disappeared before [his] eyes.\u201d In telling this brief anecdote, Bogard challenges the audience to remember a time where they could fully amass themselves in natural darkness void of artificial light. By drawing in his readers with a personal encounter about night darkness, the author means to establish the potential for beauty, glamour, and awe-inspiring mystery that genuine darkness can possess. He builds his argument for the preservation of natural darkness by reminiscing for his readers a first-hand encounter that proves the \u201cirreplaceable value of darkness.\u201d This anecdote provides a baseline of sorts for readers to find credence with the author\u2019s claims.\n\nBogard\u2019s argument is also furthered by his use of allusion to art \u2013 Van Gogh\u2019s \u201cStarry Night\u201d \u2013 and modern history \u2013 Paris\u2019 reputation as \u201cThe City of Light\u201d. By first referencing \u201cStarry Night\u201d, a painting generally considered to be undoubtedly beautiful, Bogard establishes that the natural magnificence of stars in a dark sky is definite. A world absent of excess artificial light could potentially hold the key to a grand, glorious night sky like Van Gogh\u2019s according to the writer. This urges the readers to weigh the disadvantages of our world consumed by unnatural, vapid lighting. Furthermore, Bogard\u2019s alludes to Paris as \u201cthe famed \u2018city of light\u2019\u201d. He then goes on to state how Paris has taken steps to exercise more sustainable lighting practices. By doing this, Bogard creates a dichotomy between Paris\u2019 traditionally alluded-to name and the reality of what Paris is becoming \u2013 no longer \u201cthe city of light\u201d, but moreso \u201cthe city of light\u2026before 2 AM\u201d. This furthers his line of argumentation because it shows how steps can be and are being taken to preserve natural darkness. It shows that even a city that is literally famous for being constantly lit can practically address light pollution in a manner that preserves the beauty of both the city itself and the universe as a whole.\n\nFinally, Bogard makes subtle yet efficient use of rhetorical questioning to persuade his audience that natural darkness preservation is essential. He asks the readers to consider \u201cwhat the vision of the night sky might inspire in each of us, in our children or grandchildren?\u201d in a way that brutally plays to each of our emotions. By asking this question, Bogard draws out heartfelt ponderance from his readers about the affecting power of an untainted night sky. This rhetorical question tugs at the readers\u2019 heartstrings; while the reader may have seen an unobscured night skyline before, the possibility that their child or grandchild will never get the chance sways them to see as Bogard sees. This strategy is definitively an appeal to pathos, forcing the audience to directly face an emotionally-charged inquiry that will surely spur some kind of response. By doing this, Bogard develops his argument, adding gutthral power to the idea that the issue of maintaining natural darkness is relevant and multifaceted.\n\nWriting as a reaction to his disappointment that artificial light has largely permeated the prescence of natural darkness, Paul Bogard argues that we must preserve true, unaffected darkness. He builds this claim by making use of a personal anecdote, allusions, and rhetorical questioning.\n    \"\"\"\n    \n    content = contentGood\n    \n    if len(content) > 20:\n        num_features = 300\n        clean_test_essays = []\n        clean_test_essays.append(essay_to_wordlist( content, remove_stopwords=True ))\n        testDataVecs = getAvgFeatureVecs( clean_test_essays, model, num_features )\n        testDataVecs = np.array(testDataVecs)\n        testDataVecs = np.reshape(testDataVecs, (testDataVecs.shape[0], 1, testDataVecs.shape[1]))\n\n        preds = lstm_model.predict(testDataVecs)\n\n        if math.isnan(preds):\n            preds = 0\n        else:\n            preds = np.around(preds)\n\n        if preds < 0:\n            preds = 0\n    else:\n        preds = 0\n        \n    print(preds)","a08584b1":"# Preprocessing the data","6328263d":"# Defining the Model","25d979b4":"# Importing the data","447e0f14":"# Predict the Essay"}}