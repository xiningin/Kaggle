{"cell_type":{"9dfc213f":"code","25e19441":"code","3b26db90":"code","1d914725":"code","8f2d8fd8":"code","f9446eb5":"code","257fcc05":"code","970f97c2":"code","9120e5f9":"code","3424d125":"code","173c1107":"code","810ec29d":"markdown","62334d72":"markdown","61e34789":"markdown","552d9627":"markdown","90783b7d":"markdown","35ed9f4a":"markdown","7ef5da37":"markdown","67097267":"markdown","34d00686":"markdown","1a98b1a5":"markdown","cb83cdb3":"markdown","bf5b6c87":"markdown","593f722e":"markdown","c55a40e9":"markdown","d495de82":"markdown"},"source":{"9dfc213f":"from time import sleep\nfrom random import random\nimport pandas as pd\nimport requests\nfrom tqdm.notebook import tqdm\nfrom bs4 import BeautifulSoup\nfrom urllib.parse import urljoin\nimport urllib.parse as urlparse\nfrom urllib.parse import parse_qs","25e19441":"# Constants\nBASE_URL = 'https:\/\/www.flipkart.com\/'\nSEARCH_QUERY = \"headphones\"\nTOP_N_PRODUCTS = 10\nREVIEW_PAGES_TO_SCRAPE_FROM_PER_PRODUCT = 100 #10 Reviews exist per page","3b26db90":"SAMPLE_URL = \"https:\/\/www.flipkart.com\/boat-rockerz-400-bluetooth-headset\/product-reviews\/itm14d0416b87d55?pid=ACCEJZXYKSG2T9GS&lid=LSTACCEJZXYKSG2T9GSVY4ZIC&marketplace=FLIPKART&page=1\"\nr = requests.get(SAMPLE_URL)    \nsoup = BeautifulSoup(r.content, 'html.parser') \nprint(soup.prettify()[:500])","1d914725":"# Extracting all review blocks\n## Note col._2wzgFH.K0kLPL means 3 entities namely 'col', ' _2wzgFH' and 'K0kLPL' \n## This is written in HTML as 'col _2wzgFH K0kLPL'\n## This can also be seen in Bullet 3\n\nrows = soup.find_all('div',attrs={'class':'col _2wzgFH K0kLPL'})\nprint(f\"Count of rows(reviews):{len(rows)}\\n\\n\\n\")\n# iteration over all blocks\nfor row in rows:\n    # Print a sample row(review html block)\n    # print(f\"row:\\n{row} \\n\\n\")\n    \n    # finding all rows within the block\n    sub_row = row.find_all('div',attrs={'class':'row'})\n        \n    # extracting text from 1st and 2nd row\n    rating = sub_row[0].find('div').text\n    print(f\"rating:{rating} \\n\\n\")\n    \n    summary = sub_row[0].find('p').text\n    print(f\"summary:{summary} \\n\\n\")\n    \n    review = sub_row[1].find_all('div')[2].text\n    print(f\"review:{review} \\n\\n\")\n    \n    location = sub_row[3].find('p',attrs={'class':'_2mcZGG'}).find_all('span')[1].text\n    location = \"\".join(location.split(\",\")[1:]).strip()\n    print(f\"location:{location} \\n\\n\")\n    \n    date = sub_row[3].find_all('p',attrs={'class':'_2sc7ZR'})[1].text\n    print(f\"date:{date} \\n\\n\")\n    \n    \n    sub_row_2 = row.find_all('div',attrs={'class':'_1e9_Zu'})[0].find_all('span',attrs={'class':'_3c3Px5'})\n    \n    upvotes = sub_row_2[0].text\n    print(f\"upvotes:{upvotes} \\n\\n\")\n    \n    downvotes = sub_row_2[1].text\n    print(f\"downvotes:{downvotes} \\n\\n\")\n    \n    break","8f2d8fd8":"def get_popular_product_s_titles_and_urls(search_query : str, popular_products_count_limit : int = None):\n    \n    search_url = f\"{BASE_URL}search?q={search_query}&sort=popularity\"\n    search_response = requests.get(search_url)\n    \n    # Pause the loop for 1-3 seconds to simulate natural setting not overwhelm the server with back to back requests without any pause\n    # sleep(randint(1,3))\n    \n    search_html_soup = BeautifulSoup(search_response.content, 'html.parser')\n    search_results_products = search_html_soup.find_all('div',attrs={'class':'_4ddWXP'})\n    \n    product_titles, product_urls = [],[]\n    \n    product_count = 0\n    \n    for product in tqdm(search_results_products, desc=\"Search Results Iteration\", position=0, leave=True):\n        \n        ad_mention_subrow = product.find(\"div\", attrs={\"class\":\"_4HTuuX\"})\n        \n        is_ad = not not ad_mention_subrow\n        \n        if not is_ad:\n            \n            title_mention_subrow = product.find(\"a\", attrs={\"class\":\"s1Q9rs\"})\n            \n            product_title = title_mention_subrow[\"title\"]\n            product_relative_url = title_mention_subrow[\"href\"]\n            product_url = urljoin(BASE_URL,product_relative_url)\n            \n            parsed_url = urlparse.urlparse(product_url)\n            parsed_url_path = parsed_url.path\n            parsed_url_path_split = parsed_url_path.split(\"\/\")\n            parsed_url_path_split[2] = \"product-reviews\"\n            parsed_url_path_modified = \"\/\".join(parsed_url_path_split)\n            parsed_url_modified = parsed_url._replace(path=parsed_url_path_modified)\n            product_url = parsed_url_modified.geturl()\n            \n            product_titles.append(product_title)\n            product_urls.append(product_url)\n            \n            product_count += 1\n            \n            if popular_products_count_limit and (product_count >= popular_products_count_limit):\n                break\n                \n    return product_titles, product_urls","f9446eb5":"product_titles, product_urls = get_popular_product_s_titles_and_urls(SEARCH_QUERY, TOP_N_PRODUCTS);","257fcc05":"from prettytable import PrettyTable\nx = PrettyTable()\nx.field_names = [\"# Products\", \"# Reviews Per Page\", \"# Pages\", \"# Total Reviews Count\"]\nx.add_row([len(product_urls), 10, REVIEW_PAGES_TO_SCRAPE_FROM_PER_PRODUCT, len(product_urls)*10*REVIEW_PAGES_TO_SCRAPE_FROM_PER_PRODUCT])\nprint(x)","970f97c2":"dataset = []\n\nfor idx, url in enumerate(tqdm(product_urls, desc='products')):\n    # iterating over review pages\n    for i in tqdm(range(1,REVIEW_PAGES_TO_SCRAPE_FROM_PER_PRODUCT+1), desc=\"review pages\", position=0, leave=False):\n        parsed = urlparse.urlparse(url)\n        pid = parse_qs(parsed.query)['pid'][0]\n        URL = f\"{url}&page={i}\"\n        \n        r = requests.get(URL)\n        \n        # Pause the loop for 0-1 seconds to simulate natural setting not overwhelm the server with back to back requests without any pause\n        sleep(random())\n        soup = BeautifulSoup(r.content, 'html.parser') \n\n        rows = soup.find_all('div',attrs={'class':'col _2wzgFH K0kLPL'})\n\n        for row in rows:\n\n            # finding all rows within the block\n            sub_row = row.find_all('div',attrs={'class':'row'})\n\n            # extracting text from 1st 2nd and 4th row\n            rating = sub_row[0].find('div').text\n            summary = sub_row[0].find('p').text\n            summary = summary.strip()\n            review = sub_row[1].find_all('div')[2].text\n            review = review.strip()\n            location=\"\"\n            location_row = sub_row[3].find('p',attrs={'class':'_2mcZGG'})\n            if location_row:\n                location_row = location_row.find_all('span')\n                if len(location_row)>=2:\n                    location = location_row[1].text\n                    location = \"\".join(location.split(\",\")[1:]).strip()\n            date = sub_row[3].find_all('p',attrs={'class':'_2sc7ZR'})[1].text\n\n            sub_row_2 = row.find_all('div',attrs={'class':'_1e9_Zu'})[0].find_all('span',attrs={'class':'_3c3Px5'})\n\n            upvotes = sub_row_2[0].text\n            downvotes = sub_row_2[1].text\n\n            # appending to data\n            dataset.append({'product_id':pid, 'product_title':product_titles[idx], 'rating': rating, 'summary': summary, 'review': review, 'location' : location, 'date' : date, 'upvotes' : upvotes, 'downvotes' : downvotes})","9120e5f9":"df = pd.DataFrame(dataset)\n\nwith pd.option_context('display.max_colwidth', -1):\n    display(df.head(5))\n    display(df.tail(5))","3424d125":"count_reviews = df.shape[0]\nprint(f\"Count of reviews:{count_reviews}\")","173c1107":"df.to_csv(\".\/flipkart_reviews_dataset.csv\", index=False)","810ec29d":"# Extracting data\n\nA website can be divided into many components and sub components. At times it is a complex grid structure which needs to decoded.  \n1. You can easily view the structure by `Ctrl + Shift + C`\n2. Now if you hover on any review, you'll notice that each block has name `col._2wzgFH.K0kLPL`\n![](https:\/\/github.com\/kabirnagpal\/Web-Scraping\/blob\/main\/Images\/div-name.png?raw=true)\n\n3. Further this is divided into mutiple rows. The first row contains the rating, while the second contains the actual review. \n![](https:\/\/github.com\/kabirnagpal\/Web-Scraping\/blob\/main\/Images\/rating.png?raw=true)\n![](https:\/\/github.com\/kabirnagpal\/Web-Scraping\/blob\/main\/Images\/review.png?raw=true)\nWe'll follow exact same approach to extract data.","62334d72":"### Specify Search Query and Popular Product Count Limit(optional)","61e34789":"# Iterating over multiple products and multiple pages","552d9627":"## Importing modules\n**[Request](https:\/\/requests.readthedocs.io\/en\/master\/)** Module is used to get the HTML code for the URL given.\n\n**Note**: *Not all webpages can be requested. For example most social media does not allow to scrape data due to privacy issues. These pages require special access of Developer APIs to scrape data.*","90783b7d":"<center> <h1 style=\"background-color:blue; color:white\">Flipkart Reviews Scraping using BeautifulSoup<\/h1>","35ed9f4a":"# Web Scraping\n\nData scraping is one of the most used ways to collect data. In simple terms it means, to get HTML code for a webpage and scan it for data.","7ef5da37":"# Requesting Desired Webpage","67097267":"# View Sample set of reviews that we collected","34d00686":"# Table of Contents\n\n<center> <div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n  <h3 class=\"list-group-item list-group-item-action active\" style=\"background-color:yellow; color:black\" data-toggle=\"list\"  role=\"tab\" aria-controls=\"home\">Notebook Content<\/h3><\/div><\/center>  \n<left>  \n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Web-Scraping\" role=\"tab\" aria-controls=\"profile\" style=\"color:purple\"><b>1. Web Scraping<\/b><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Requesting-Desired-Webpage\" role=\"tab\" aria-controls=\"profile\" style=\"color:purple\"><b>2. Requesting Desired Webpage<\/b><\/a>\n  <a id=\"section2\" class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Extracting-data\" role=\"tab\" aria-controls=\"messages\" style=\"color:purple\"><b>3. Extracting data<\/b><\/a>\n  <a class=\"list-group-item list-group-item-action\"  data-toggle=\"list\" href=\"#Test-for-a-single-product-review-page\" role=\"tab\" aria-controls=\"settings\" style=\"color:purple\"><b>4. Test for a single product review page<\/b><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Search-based-Product-URL-Discovery\" role=\"tab\" aria-controls=\"settings\" style=\"color:purple\"><b>5. Search based Product URL Discovery<\/b><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Collect-Product-Page-URLs-for-Top-10-Popular-Products-for-'Headphones'-search-query\" role=\"tab\" aria-controls=\"settings\" style=\"color:purple\">&nbsp;&nbsp;&nbsp;5.1 Collect Product Page URLs for Top 10 Popular Products for 'Headphones' search query<\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Iterating-over-multiple-products-and-multiple-pages\" role=\"tab\" aria-controls=\"settings\" style=\"color:purple\"><b>6. Iterating over multiple products and multiple pages<\/b><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#View-Sample-set-of-reviews-that-we-collected\" role=\"tab\" aria-controls=\"settings\" style=\"color:purple\"><b>7. View Sample set of reviews that we collected<\/b><\/a>\n  <a class=\"list-group-item list-group-item-action\" data-toggle=\"list\" href=\"#Serialize-the-dataframe-to-a-csv-file\" role=\"tab\" aria-controls=\"settings\" style=\"color:purple\"><b>8. Serialize the dataframe to a csv file<\/b><\/a><\/left>   ","1a98b1a5":"# Test for a single product review page","cb83cdb3":"# Serialize the dataframe to a csv file","bf5b6c87":"**[Beautiful Soup](https:\/\/www.crummy.com\/software\/BeautifulSoup\/)** is the most used package for scanning\/scraping data.  \nIn this notebook we'll see how to use Beautiful Soup and get a set of reviews and its associated metadata posted by the customers on its website for 2 of the headphones and create a dataset out of it.  \n**Let's Get started**","593f722e":"## Collect Product Page URLs for Top 10 Popular Products for 'Headphones' search query","c55a40e9":"# Search based Product URL Discovery","d495de82":"If you're know HTML, this might look familiar.  \nNext we'll see how to get our data."}}