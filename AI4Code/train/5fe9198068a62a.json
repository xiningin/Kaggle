{"cell_type":{"101f6e4b":"code","fdd99bd7":"code","021fd7ef":"code","0322981a":"code","64d6a808":"code","caaffa94":"code","6d262701":"code","b9eec36b":"code","01595585":"code","362aced7":"code","2d78830e":"code","8e45357d":"code","583fcdd4":"code","ee4c2823":"code","b6cf8756":"code","69c3270e":"markdown","bc963a2f":"markdown","b6c7fa96":"markdown","b59fb876":"markdown","2ab68495":"markdown","4683162b":"markdown","08f38150":"markdown","3070c546":"markdown","1c7abd3c":"markdown","8f3e85d0":"markdown","a5ced684":"markdown","2e2a65d6":"markdown","5cf1cbc8":"markdown","c941c004":"markdown","1f04f168":"markdown","d63cc5a3":"markdown","b6b2b36a":"markdown","9c6ed98a":"markdown","60577166":"markdown","50465468":"markdown","247e8d7a":"markdown","cc9a60f4":"markdown"},"source":{"101f6e4b":"!pip install --upgrade wandb","fdd99bd7":"import os \nimport random\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\nsns.set(style='white', context='notebook', palette='deep')\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\nimport gc\n\nimport tensorflow as tf\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.callbacks import *\n\nimport tensorflow_addons as tfa\n\nimport wandb\nfrom wandb.keras import WandbCallback","021fd7ef":"# Load the data\ndef load_data(path):\n\n    train = pd.read_csv(path+\"train.csv\")\n    test = pd.read_csv(path+\"test.csv\")\n    \n    x_tr = train.drop(labels=[\"label\"], axis=1)\n    y_tr = train[\"label\"]\n    \n    print(f'Train: we have {x_tr.shape[0]} images with {x_tr.shape[1]} features and {y_tr.nunique()} classes')\n    print(f'Test: we have {test.shape[0]} images with {test.shape[1]} features')\n    \n    return x_tr, y_tr, test\n\n\ndef seed_all(seed):\n    np.random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    tf.random.set_seed(seed)","0322981a":"# Build CNN model \n# CNN architechture: In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\n\ndef build_model(config):\n    \n    fs = config.filters         # 32\n    k1 = config.kernel_1        #[(5,5), (3,3)]\n    k2 = config.kernel_2        # [(5,5), (3,3)]\n    pad = config.padding\n    activ = config.activation   # 'relu'\n    pool = config.pooling       # (2,2)\n    dp = config.dropout         # 0.25\n    dp_out = config.dropout_f   # 0.5\n    dense_units = config.dense_units  # 256\n    batch_norm = False\n    \n    inp = Input(shape=(28,28,1))    # IMG_H, IMG_W, NO_CHANNELS\n    \n    # layer-1:: CNN-CNN-(BN)-Pool-dp\n    x = Conv2D(filters=fs, kernel_size=k1, padding=pad, activation=activ)(inp)\n    x = Conv2D(filters=fs, kernel_size=k1, padding=pad, activation=activ)(x)\n    if batch_norm:\n        x = BatchNormalization()(x)\n    x = MaxPool2D(pool_size=(2,2))(x)\n    x = Dropout(dp)(x)    \n    \n    # layer-2:: CNN-CNN-(BN)-Pool-dp\n    x = Conv2D(filters=fs*2, kernel_size=k2, padding=pad, activation=activ)(inp)\n    x = Conv2D(filters=fs*2, kernel_size=k2, padding=pad, activation=activ)(x)\n    if batch_norm:\n        x = BatchNormalization()(x)\n    x = MaxPool2D(pool_size=(2,2), strides=(2,2))(x)\n    x = Dropout(dp)(x)  \n    \n    x = Flatten()(x)\n    #     x = GlobalAveragePooling2D()(x)\n    \n    # FC head\n    x = Dense(dense_units, activation=activ)(x)\n    x = Dropout(dp_out)(x)\n    \n    out = Dense(10, activation=\"softmax\")(x)\n    \n    model = tf.keras.models.Model(inp, out)\n    \n    print(model.summary())\n    return model","64d6a808":"def build_lenet(config):\n    \n    fs = config.filters       # 32     \n    k1 = config.kernel_1      # 3  \n    k2 = config.kernel_2          \n    pad = config.padding\n    activ = config.activation     \n    dp = config.dropout           \n    \n    inp = Input(shape=(28,28,1))  # (IMG_H, IMG_W, NO_CHANNELS)\n    \n    x = Conv2D(fs, kernel_size = k1, activation=activ)(inp)\n    x = BatchNormalization()(x)\n    x = Conv2D(fs, kernel_size = k1, activation=activ)(x)\n    x = BatchNormalization()(x)\n    x = Conv2D(fs, kernel_size = 5, strides=2, padding='same', activation=activ)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(dp)(x)\n    \n    x = Conv2D(fs*2, kernel_size = k1, activation=activ)(x)\n    x = BatchNormalization()(x)\n    x = Conv2D(fs*2, kernel_size = k1, activation=activ)(x)\n    x = BatchNormalization()(x)\n    x = Conv2D(fs*2, kernel_size = 5, strides=2, padding='same', activation=activ)(x)\n    x = BatchNormalization()(x)\n    x = Dropout(dp)(x)\n    \n    x = Conv2D(fs*4, kernel_size = 4, activation=activ)(x)\n    x = BatchNormalization()(x)\n    x = Flatten()(x)\n    x = Dropout(dp)(x)\n    \n    out = Dense(10, activation='softmax')(x)\n    \n    model = tf.keras.models.Model(inp, out)\n\n    print(model.summary())\n    return model","caaffa94":"DEBUG = True          # set to True in case of setup\/testing\/debugging -- False when you ready to run experiments\n# DATA_AUGM = False   # set to True if you wish to add data augmentation \n\nBATCH_SIZE = 64\n\nif DEBUG:\n    EPOCHS = 3          \nelse: \n    EPOCHS = 40","6d262701":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\napi_key = user_secrets.get_secret(\"API_key\")","b9eec36b":"!wandb login $api_key","01595585":"# hyperparams = dict(\n#      filters = 32,\n#      kernel_1 = (5,5),\n#      kernel_2 = (3,3),\n#      padding = 'same',\n#      pooling = (2,2),\n#      lr = 0.001,\n#      wd = 0.0,\n#      lr_schedule = 'RLR',    # cos, cyclic, step decay\n#      optimizer = 'Adam',     # RMS\n#      dense_units=256,\n#      activation='relu',      # elu, LeakyRelu\n#      dropout = 0.25,\n#      dropout_f = 0.5,\n#      batch_size = BATCH_SIZE,\n#      epochs = EPOCHS,\n#  )\n\n# wandb.init(project=\"kaggle-titanic\", config=hyperparams)\n# config = wandb.config","362aced7":"def train():\n    \n    \n    hyperparams = dict(\n        filters=32,\n        kernel_1=(5,5),\n        kernel_2=(3,3),\n        padding='same',\n        pooling=(2,2),\n        lr=0.001,\n        wd=0.0,\n        lr_schedule='RLR',    # cos, cyclic, step decay\n        optimizer='Adam',     # 'RMS'\n        dense_units=256,\n        activation='relu',      # elu, LeakyRelu\n        dropout=0.25,\n        dropout_f=0.5,\n        batch_size=BATCH_SIZE,\n        epochs=EPOCHS)\n    \n    wandb.init(project=\"kaggle-mnist\", config=hyperparams)\n    config = wandb.config\n    \n    \n    SEED = 26\n    seed_all(SEED)\n    \n    #     # Define image sizes and reshape to a 3-dim tensor\n    #     global\n    IMG_H, IMG_W = 28, 28\n    NO_CHANNELS = 1           # for greyscale images\n    \n    # load data\n    x_train, y_train, x_test = load_data(path=\"..\/input\/digit-recognizer\/\")\n    \n    \n    # Normalize the data\n    x_train = x_train \/ 255.0\n    x_test = x_test \/ 255.0\n\n    # Reshape to a 4-dim tensor\n    x_train = x_train.values.reshape(-1, IMG_H, IMG_W, NO_CHANNELS)\n    x_test = x_test.values.reshape(-1, IMG_H, IMG_W, NO_CHANNELS)\n    \n    # Encode labels\n    y_train_ohe = tf.keras.utils.to_categorical(y_train, num_classes=10)\n\n    print('Tensor shape (train): ', x_train.shape)\n    print('Tensor shape (test): ', x_test.shape)\n    print('Tensor shape (target ohe): ', y_train_ohe.shape)\n    \n    # Split the train and the validation set for the fitting\n    x_tr, x_val, y_tr, y_val = train_test_split(x_train, y_train_ohe, test_size=0.15, random_state=SEED)\n    \n    print('Tensors shape (train):', x_tr.shape, y_tr.shape)\n    print('Tensors shape (valid):', x_val.shape, y_val.shape)\n    \n    print('Build architecture 1')\n    model = build_model(config=config)\n    \n#     print('Build architecture 2 - LeNet5')\n#     model = build_lenet(config=config)\n    \n    \n    # Define the optimizer\n    if config.optimizer=='Adam':\n        opt = Adam(config.lr)\n    elif config.optimizer=='RMS':\n        opt = RMSprop(lr=config.lr, rho=0.9, epsilon=1e-08, decay=0.0)\n    elif config.optimizer=='Adam+SWA':\n        opt = Adam(LR)\n        opt = tfa.optimizers.SWA(opt)\n    else: \n        opt = 'adam'    # native adam optimizer \n    \n    \n    # Compile the model\n    model.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n    \n    # Set callbacks\n\n    callbacks = [\n        EarlyStopping(monitor='val_loss', patience=15, verbose=1), \n        ReduceLROnPlateau(monitor='val_accuracy', patience=10, verbose=1, factor=0.5, min_lr=1e-4),\n        WandbCallback(monitor='val_loss', validation_data=(x_val, y_val))]  \n    \n    model.fit(x_train, y_train_ohe, \n                     batch_size=config.batch_size,    # BATCH_SIZE, \n                     epochs=config.epochs,            # EPOCHS, \n                     validation_data=(x_val, y_val), \n                     callbacks=callbacks,\n                     verbose=1) ","2d78830e":"sweep_config = {\n#     'program': 'train.py',     # 'tutorial-cnn-keras-hyperparameter-opt-w-b-p2.ipynb',\n    'method': 'random',         # 'grid', 'hyperopt', 'bayesian'\n    'metric': {\n        'name': 'val_loss',     # or 'val_accuracy'\n        'goal': 'minimize'      # 'maximize'\n    },\n    'parameters': {\n        'filters': {\n            'values': [16, 32, 64]\n        },\n        'lr': {\n            'distribution': 'uniform',\n            'min': 0.0005,\n            'max': 0.002\n        },\n        'dp': {\n            'values': [0.4, 0.5]\n        }\n    }\n}","8e45357d":"sweep_id = wandb.sweep(sweep_config, entity=\"ime\", project='kaggle-mnist')","583fcdd4":"wandb.agent(sweep_id, function=train, count=3, project='kaggle-mnist')   #","ee4c2823":"# !wandb agent ime\/kaggle-mnist\/jp5i153v --count 3 -p 'kaggle-mnist'","b6cf8756":"# !wandb agent ime\/kaggle-mnist\/nkf2bmi7 --count 5 -p kaggle-mnist -e ime","69c3270e":"Start the agent running with `wandb.agent()`\n\n(here, I limit the number of trial runs using the `count` arg)","bc963a2f":"# CNN Keras - Hyperparameter Optimization Tutorial [W&B Part 2]\n\n\n### Weights & Biases (W&B)","b6c7fa96":"4. This will open a new page where we can setup our Sweep configuration. Change this `.yaml` file accordingly - as before we need to define a `method`, a `metric and goal` and `a list of parameters`\n\n![image.png](attachment:image.png)","b59fb876":"### 2. Make a New Project (or go to an existing one if you already have)\n\n![image.png](attachment:image.png)\n\nNote: I've chosen a public access project for demonstration reasons - you can keep yours private if you prefer! ","2ab68495":"Initialize the sweep by running `wandb.sweep()`. This returns the `sweep_id` that we use to run the sweep agent.","4683162b":"### 4. Login to W&B\n\nFrom the command line or inside a nb cell run `!wandb login <your_API_key>` to login and initiate W&B\n\nor, use the `wandb.login()` function and copy and paste the API key when asked","08f38150":"## Master Params\n\nI use the master variable DEBUG to control the no. of epochs per trial and hence the runtime of the commit.\n\n- set `DEBUG = True` in case of setup\/testing\/debugging\n\n- set `DEBUG = False` when you ready to run experiments","3070c546":"### Please if you find usefull this notebook upvote it so as to keep the motive to share more! :)\n\n### Fork the notebook \n   - ### setup your config \n   - ### change `DEBUG=False`  \n   - ### set the no of trials, and \n\n### start experimenting with hyperparameter search!\n\n### Please, if you fork - don't forget to upvote too!!","1c7abd3c":"5. Now we are ready to start the Sweep\n\n    - press Initialize Sweep\n\n    - run `wandb agent project\/sweep_id` to start the agent running in our machine (Kaggle here..)","8f3e85d0":"Some optional arguments\n\n`--count`: limits the number of total runs, I put a limit of 5 here for demonstration reasons\n\n`-p`: project name\n\n`-e`: entity\/user name","a5ced684":"# Introduction (purpose of kernel)\n\nAfter quite some time in Kaggle I decided to start sharing some of the knowledge I gained through my journey here.\n\nThis a series of tutorial notebooks devoted to CNN modelling, experiment tracking and hyperparameter optimization using Weights and Biases (W&B). To my knowledge there are many awsome pipelines for hyperparameter optimisation using Bayesian opt., random\/grid search etc, but not with W&B. So I decided to make a step-by-step guide on how to use this tool in kaggle (sidenote: although I personally prefer scripts for this kind of tasks, here I've tried to use a notebook for demonstration reasons).\n\nAs an example I use the famous digits recognition problem (MNIST dataset) and a simple CNN model [Keras TF]. After the standard data loading and preprocessing routines I will focus on the CNN model and how to build the configuration for the hyperparameter tuning.\n\n### Please, if you find usefull this notebook upvote it so as to keep the motive to share more! :)","2e2a65d6":"### Method-2\n\nAnother easy way to start the configuration for the hyperparameter optimization (sweep) is to use directly the UI:\n\n1. go to the Table format\n2. select some runs (at least one)\n3. press Create Sweep (by clicking the vertical dots, as seen below)\n\n\n![image.png](attachment:image.png)","5cf1cbc8":"### 3. Select a framework to start working on..\n\n\n![image.png](attachment:image.png)\n\n\nNote: Keep aside your username and login token (API key)","c941c004":"# CNN Model\n\n#### Architecture 1 - a simple CNN architecture build with the Keras Functional API (TF backend). The main building blocks and their usage are:\n\n- Convolutional (Conv2D) layer: It is like a set of learnable filters. I choosed to set 32 filters for the first two layers and 64 filters for the two last ones. Each filter transforms a part of the image (defined by the kernel size) using the kernel filter. The kernel filter matrix is applied on the whole image. Filters can be seen as a transformation of the image. The CNN can isolate features that are useful everywhere from these transformed images (feature maps).\n\n- Pooling (MaxPool2D) layer: This layer simply acts as a downsampling filter. It looks at the 2 neighboring pixels and picks the maximal value. These are used to reduce computational cost, and to some extent also reduce overfitting. We have to choose the pooling size (i.e the area size pooled each time) more the pooling dimension is high, more the downsampling is important.\n\n- Dropout: is a regularization method, where a proportion of nodes in the layer are randomly ignored (setting their wieghts to zero) for each training sample. This drops randomly a propotion of the network and forces the network to learn features in a distributed way. This technique also improves generalization and reduces the overfitting.\n\n- Activation function: 'relu' is the rectifier activation function, used to add non linearity to the model.\n\n- Flatten layer: is used to convert the final feature maps into a one single 1D vector. This flattening step is needed so that you can make use of fully connected layers after some convolutional\/maxpool layers. It combines all the found local features of the previous convolutional layers.\n\n- Fully-Connected (Dense) layers: to act as a (NN) classifier. Note that in the last layer (Dense(10,activation=\"softmax\")) the net outputs distribution of probability of each class.\n\n\n#### Architecture 2 - LeNet5 with some tweaks by C.Deotte (see credits)","1f04f168":"### 5. Create the W&B configuration\n\nLet's make a dictionary with all the hyperparameters that we'd like to track! For CNN type models I usually keep track of the following params:\n\n- filters\n- kernel size\n- learning rate (init)\n- optimizer\n- dense units\n- dropout rate etc\n\nBut you can add as many others you like.","d63cc5a3":"# Configure WandB\n\n\n### 1. First go to the Weights & Biases webpage and register (if you don't have an account yet) or sign-in","b6b2b36a":"When the agent finishes all the trial runs you should be able to visualize the results in the wandb dashboard, e.g.\n\n![image.png](attachment:image.png)","9c6ed98a":"Next, we need to put in a single function all the modelling pipeline, i.e.\n\n- the data loading,\n- preprocessing,\n- model building and\n- training parts","60577166":"# Resources\n\n- W&B tutorial: https:\/\/www.kaggle.com\/lavanyashukla01\/better-models-faster-with-weights-biases\/notebook\n- W&B docs sweep: https:\/\/docs.wandb.com\/sweeps\/python-api\n\n## Inspiring kernels\n\n- Chriss Deotte - [25 Million Images! [0.99757] MNIST](https:\/\/www.kaggle.com\/cdeotte\/25-million-images-0-99757-mnist#Train-15-CNNs)\n\n- Yassine Ghouzam - [introduction-to-cnn-keras-0-997-top-6](https:\/\/www.kaggle.com\/yassineghouzam\/introduction-to-cnn-keras-0-997-top-6)","50465468":"### 01\/08\/2020\n\n\n### [See Part 1 of the notebook series on how to keep tracking of your experiments with W&B](https:\/\/www.kaggle.com\/imeintanis\/tutorial-cnn-keras-experiment-tracking-w-b-p1?scriptVersionId=40015533)","247e8d7a":"# Hyperparameter Optimization\n\n\n### Method 1\n\nBuild the sweep (hyperparameter search) configuration with python dict.\n\nThe sweeps config can be defined as a dictionary or a YAML file. Let's walk through some of them together:\n\n- `Metric` \u2013 This is the metric the sweeps are attempting to optimize. Metrics can take a name (this metric should be logged by your training script) and a goal (maximize or minimize).\n\n- `Search Strategy` \u2013 Specified using the 'method' variable. We support several different search strategies with sweeps.\n\n    - `Grid Search` \u2013 Iterates over every combination of hyperparameter values.\n    - `Random Search` \u2013 Iterates over randomly chosen combinations of hyperparameter values.\n    - `Bayesian Search` \u2013 Creates a probabilistic model that maps hyperparameters to probability of a metric score, and chooses parameters with high probability of improving the metric. The objective of Bayesian optimization is to spend more time in picking the hyperparameter values, but in doing so trying out fewer hyperparameter values.\n    - `Stopping Criteria` \u2013 The strategy for determining when to kill off poorly peforming runs, and try more combinations faster. We offer several custom scheduling algorithms like HyperBand and Envelope.\n\n- `Parameters` \u2013 A dictionary containing the hyperparameter names, and discreet values, max and min values or distributions from which to pull their values to sweep over.","cc9a60f4":"## Helpers"}}