{"cell_type":{"d78555ec":"code","41128317":"code","7d0a32f0":"code","2a697543":"code","5dfc9073":"code","fbeae311":"code","da3bdf9c":"code","61392cee":"code","a4506581":"code","4c9b7844":"code","4002ae48":"code","56460932":"code","f457d362":"code","05e00c83":"code","4a561701":"code","f5db64d2":"code","814f1a86":"code","c6e5805f":"code","9dd94083":"code","c103d417":"code","437ef3fc":"code","feb108cd":"code","334d30c7":"code","a3be789a":"code","ccee16a2":"code","5866d8d0":"code","ff1d2cbd":"code","b196654e":"code","1af06e4d":"code","ec792db3":"code","0cc91173":"code","0ae9bc63":"code","a6d9ad0b":"code","35dc0cc5":"code","0a0cb53d":"code","719465a9":"code","d4fbd6bd":"code","5b1c0e82":"code","d9493db8":"code","354fe512":"code","552ae91a":"code","1f444786":"markdown","4bd04715":"markdown","e5809441":"markdown","ad4c55e2":"markdown","cbf2005c":"markdown","f4a46bb7":"markdown","5629852b":"markdown","d4c78423":"markdown","8459dd01":"markdown","ea4624d0":"markdown","c3cebc93":"markdown","d147ea2e":"markdown","8ad7d1d8":"markdown","af652002":"markdown","92be78bb":"markdown","3f549806":"markdown","c13f30a8":"markdown","c49d2602":"markdown","0abf940e":"markdown","f7c2180f":"markdown","423073e1":"markdown","2170bb24":"markdown","6f07d302":"markdown","5b0df1eb":"markdown","80a4c092":"markdown","76543117":"markdown","a3360658":"markdown","d41ea00b":"markdown","4cdedb1c":"markdown","893f9717":"markdown","41b02221":"markdown","caf980bc":"markdown","70876042":"markdown","693644f6":"markdown","2d0cf3ba":"markdown","c9a03c3d":"markdown","56e0add2":"markdown","5b62b4f2":"markdown","595263b2":"markdown","1e227a5f":"markdown","b922aad3":"markdown","e0856eb1":"markdown","02b2c34f":"markdown","80659570":"markdown","d06560c5":"markdown","0b32ffc6":"markdown","7e81c356":"markdown"},"source":{"d78555ec":"import math\nimport numpy as np\n\nimport pandas as pd\npd.set_option('display.max_columns', 60)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.wrappers.scikit_learn import KerasClassifier\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn import metrics\nfrom sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix, classification_report","41128317":"INT8_MIN = np.iinfo(np.int8).min\nINT8_MAX = np.iinfo(np.int8).max\nINT16_MIN = np.iinfo(np.int16).min\nINT16_MAX = np.iinfo(np.int16).max\nINT32_MIN = np.iinfo(np.int32).min\nINT32_MAX = np.iinfo(np.int32).max\n\nFLOAT16_MIN = np.finfo(np.float16).min\nFLOAT16_MAX = np.finfo(np.float16).max\nFLOAT32_MIN = np.finfo(np.float32).min\nFLOAT32_MAX = np.finfo(np.float32).max\n\n\ndef memory_usage(data, detail = 1):\n    if detail:\n        display(data.memory_usage())\n    memory = data.memory_usage().sum() \/ (1024 * 1024)\n    print(\"Memory usage : {0:.2f}MB\".format(memory))\n    return memory\n\n\ndef compress_dataset(data):\n    memory_before_compress = memory_usage(data, 0)\n    print()\n    print('=' * 50)\n    for col in data.columns:\n        col_dtype = data[col][:100].dtype\n\n        if col_dtype != 'object':\n            print(\"Name: {0:24s} Type: {1}\".format(col, col_dtype))\n            col_series = data[col]\n            col_min = col_series.min()\n            col_max = col_series.max()\n\n            if col_dtype == 'float64':\n                print(\" variable min: {0:15s} max: {1:15s}\".format(str(np.round(col_min, 4)), str(np.round(col_max, 4))))\n                if (col_min > FLOAT16_MIN) and (col_max < FLOAT16_MAX):\n                    data[col] = data[col].astype(np.float16)\n                    print(\"  float16 min: {0:15s} max: {1:15s}\".format(str(FLOAT16_MIN), str(FLOAT16_MAX)))\n                    print(\"compress float64 --> float16\")\n                elif (col_min > FLOAT32_MIN) and (col_max < FLOAT32_MAX):\n                    data[col] = data[col].astype(np.float32)\n                    print(\"  float32 min: {0:15s} max: {1:15s}\".format(str(FLOAT32_MIN), str(FLOAT32_MAX)))\n                    print(\"compress float64 --> float32\")\n                else:\n                    pass\n                memory_after_compress = memory_usage(data, 0)\n                print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) \/ memory_before_compress))\n                print('=' * 50)\n\n            if col_dtype == 'int64':\n                print(\" variable min: {0:15s} max: {1:15s}\".format(str(col_min), str(col_max)))\n                type_flag = 64\n                if (col_min > INT8_MIN \/ 2) and (col_max < INT8_MAX \/ 2):\n                    type_flag = 8\n                    data[col] = data[col].astype(np.int8)\n                    print(\"     int8 min: {0:15s} max: {1:15s}\".format(str(INT8_MIN), str(INT8_MAX)))\n                elif (col_min > INT16_MIN) and (col_max < INT16_MAX):\n                    type_flag = 16\n                    data[col] = data[col].astype(np.int16)\n                    print(\"    int16 min: {0:15s} max: {1:15s}\".format(str(INT16_MIN), str(INT16_MAX)))\n                elif (col_min > INT32_MIN) and (col_max < INT32_MAX):\n                    type_flag = 32\n                    data[col] = data[col].astype(np.int32)\n                    print(\"    int32 min: {0:15s} max: {1:15s}\".format(str(INT32_MIN), str(INT32_MAX)))\n                    type_flag = 1\n                else:\n                    pass\n                memory_after_compress = memory_usage(data, 0)\n                print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) \/ memory_before_compress))\n                if type_flag == 32:\n                    print(\"compress (int64) ==> (int32)\")\n                elif type_flag == 16:\n                    print(\"compress (int64) ==> (int16)\")\n                else:\n                    print(\"compress (int64) ==> (int8)\")\n                print('=' * 50)\n\n    print()\n    memory_after_compress = memory_usage(data, 0)\n    print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) \/ memory_before_compress))\n    \n    return data","7d0a32f0":"df_train = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/train.csv')\ndf_test = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/test.csv')","2a697543":"df_train.head()","5dfc9073":"df_train.drop('Id', axis = 1, inplace = True)","fbeae311":"print(f'Train set shape:   {df_train.shape}')","da3bdf9c":"df_train.info()","61392cee":"df_train.describe()","a4506581":"df_train.drop(['Soil_Type7', 'Soil_Type15'], axis = 1, inplace = True)","4c9b7844":"df_train.isnull().sum().max() != 0","4002ae48":"df_test.head()","56460932":"df_test.drop('Id', axis = 1, inplace = True)","f457d362":"print(f'Test set shape:   {df_test.shape}')","05e00c83":"df_test.info()","4a561701":"df_test.describe()","f5db64d2":"df_test.drop(['Soil_Type7', 'Soil_Type15'], axis = 1, inplace = True)","814f1a86":"df_test.isnull().sum().max() != 0","c6e5805f":"df_train['Cover_Type'].unique()","9dd94083":"sns.countplot(x = df_train['Cover_Type'])","c103d417":"df_train['Cover_Type'].value_counts()","437ef3fc":"df_train.drop(df_train[df_train['Cover_Type'] == 5].index, axis = 0, inplace = True)","feb108cd":"non_binary_columns = list(df_train.columns[:10])\nsns.heatmap(df_train[non_binary_columns].corr())","334d30c7":"columns = list(df_test.columns)\n\nscaler = StandardScaler()\n\ndf_train[columns] = scaler.fit_transform(df_train[columns])\ndf_test = pd.DataFrame(scaler.transform(df_test), columns = df_test.columns)","a3be789a":"df_train = compress_dataset(df_train)\ndf_test = compress_dataset(df_test)","ccee16a2":"X = df_train[columns]\ny = df_train['Cover_Type']","5866d8d0":"y = pd.get_dummies(y)\ny.head()","ff1d2cbd":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify = y, test_size = 0.2, random_state = 12)\nprint(f'X train shape:  {X_train.shape}')\nprint(f'X test shape:   {X_test.shape}')\nprint(f'y train shape:  {y_train.shape}')\nprint(f'y test shape:   {y_test.shape}')","b196654e":"def model_cnn(optimizer, initializer):\n    model = Sequential()\n    model.add(Dense(128, activation = 'relu', kernel_initializer = initializer, input_shape = [X.shape[1]]))\n    model.add(Dropout(0.3))\n    model.add(BatchNormalization())\n    model.add(Dense(64, activation = 'relu', kernel_initializer = initializer))\n    model.add(Dropout(0.3))\n    model.add(BatchNormalization())\n    model.add(Dense(32, activation = 'relu', kernel_initializer = initializer))\n    model.add(Dropout(0.3))\n    model.add(BatchNormalization())\n    model.add(Dense(16, activation = 'relu', kernel_initializer = initializer))\n    model.add(Dropout(0.3))\n    model.add(BatchNormalization())\n    model.add(Dense(8, activation = 'relu', kernel_initializer = initializer))\n    model.add(Dropout(0.3))\n    model.add(BatchNormalization())\n    model.add(Dense(6, activation = 'softmax', kernel_initializer = initializer))\n\n    model.compile(optimizer = optimizer, loss = 'categorical_crossentropy', metrics = ['accuracy'])\n\n    return model\n\nearly_stop = EarlyStopping(monitor = 'val_accuracy', patience = 10, \n                           verbose = 1, mode = 'max', restore_best_weights = True)\nred_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.5, patience = 5, verbose = 1)","1af06e4d":"model = KerasClassifier(build_fn = model_cnn, epochs = 100, batch_size = 1024, \n                        verbose = 1, callbacks = [early_stop, red_lr], validation_data = (X_test, y_test))","ec792db3":"params_grid = dict(optimizer = ['adam', 'rmsprop'],\n                   initializer = ['uniform', 'glorot_uniform', 'normal'])\n\ngrid_model = GridSearchCV(estimator = model, param_grid = params_grid, \n                          cv = 10, verbose = 0)\n# grid_model.fit(X_train, y_train)","0cc91173":"# pd.DataFrame(grid_model.cv_results_)[['param_initializer', 'param_optimizer', 'mean_test_score', 'std_test_score', 'rank_test_score']].sort_values('rank_test_score')","0ae9bc63":"# print(f'Best: {grid_model.best_score_} using {grid_model.best_params_}')","a6d9ad0b":"# best_model = grid_model.best_estimator_\n\n# preds = best_model.predict(X_test)\n# preds += 1\n# preds[preds > 4] += 1\n# preds","35dc0cc5":"# y_true = np.argmax(np.array(y_test), axis = 1)\n# y_true += 1\n# y_true[y_true > 4] += 1\n# y_true","0a0cb53d":"# print(classification_report(y_true, preds))","719465a9":"# cm = confusion_matrix(y_true, preds)\n# fig, ax = plt.subplots(figsize = (10,10))\n# cmd = ConfusionMatrixDisplay(cm, display_labels = pd.DataFrame(y_true)[0].sort_values().unique())\n# cmd.plot(cmap = plt.cm.Blues, ax = ax)\n# plt.show()","d4fbd6bd":"# sns.countplot(x = preds)\n# plt.grid()","5b1c0e82":"# model = best_model.fit(X_train, y_train, validation_data = (X_test, y_test), \n#                   verbose = 1, callbacks = [early_stop, red_lr])","d9493db8":"# plt.figure(figsize = (15, 5))\n\n# plt.plot(model.history['accuracy'])\n# plt.plot(model.history['val_accuracy'])\n\n# plt.title('Model Accuracy', size = 16)\n# plt.xlabel('Epoch')\n# plt.legend(['Train accuracy', 'Test accuracy'], loc = 4)\n# plt.grid()\n# plt.show()\n           \n# plt.figure(figsize = (15, 5))\n\n# plt.plot(model.history['loss'])\n# plt.plot(model.history['val_loss'])\n\n# plt.title('Model loss', size = 16)\n# plt.xlabel('Epoch')\n# plt.legend(['Train loss', 'Test loss'], loc = 7)\n# plt.grid()\n# plt.show()","354fe512":"# sub = pd.read_csv('..\/input\/tabular-playground-series-dec-2021\/sample_submission.csv')\n# preds = best_model.predict(df_test)\n# preds += 1\n# preds[preds > 4] += 1\n# sub['Cover_Type'] = preds\n# sub.head()","552ae91a":"# sub.to_csv('gs_best.csv', index = False)","1f444786":"Time to predict classes for test data. We need to modify predicted values in following way:\n* 0 -> 1\n* 1 -> 2\n* 2 -> 3\n* 3 -> 4\n* 4 -> 6\n* 5 -> 7\n\nbecause indices of columns don't correspond to values of class.","4bd04715":"## Introduction","e5809441":"Previous versions of this notebook present models of neural network, which are diferrent from optimizers and initializers. I had an idea to verify quality of these models by GridSearchCV, but it took very long time, so I decided to verify each model separately. And here are the results:\n\n* **V1** - optimizer: **rmsprop**, initializer: **glorot_uniform** -------> _**0.94749**_\n* **V2** - optimizer: **rmsprop**, initializer: **normal** ----------------> _**0.94635**_\n* **V3** - optimizer: **rmsprop**, initializer: **uniform** ---------------> _**0.94638**_\n* **V4** - optimizer: **adam**,    initializer: **glorot_uniform** ----------> _**0.94871**_\n* **V5** - optimizer: **adam**,    initializer: **normal** -------------------> _**0.94897**_\n* **V6** - optimizer: **adam**,    initializer: **uniform** ------------------> _**0.94903**_ <-best\n \nThere is a code below, which contains data preprocessing. Besides, it enable to search the best neural network model by GridSearchCV tool and other things related to evaluation of model.","ad4c55e2":"All columns consist of **integers** and the set is huge - it using a lot of memory.","cbf2005c":"Let's scale and match our datasets. ","f4a46bb7":"## Correlation","5629852b":"## Target summary","d4c78423":"At the end let's check dataset has some missing values.","8459dd01":"Let's build neural network model. We use GarchSearchCV tool to select the best optimizer and initializer.","ea4624d0":"## Memory releasing","c3cebc93":"## CNN","d147ea2e":"Let's find out something more about data. ","8ad7d1d8":"Let's define our features and target.","af652002":"We have **7** classes. We need to check that classes are balanced or not.","92be78bb":"`Id` column **is redundant**. Let's remove it.","3f549806":"All columns consist of **integers** and the set is huge - it using a lot of memory. Just like in train set.","c13f30a8":"Datasets are very large and use huge quantity of memory, so we need to convert type of columns to ones using less memory.","c49d2602":"Unfortunatelly, **classes are imbalanced**. Class no. 5 appears only once. We'll remove it.","0abf940e":"Our test set has **1 000 000 rows** and **54 columns**.","f7c2180f":"## Test set summary","423073e1":"We need to extract informations about the values of class from target matrix. We can do it in two steps. First, we take indices of these columns, where no. 1 appears in each rows and then we modify them exactly the same way as before.","2170bb24":"Columns `Soil_Type7` and `Soil_Type15` contain only **one value - 0**. They don't introduce a variability, so we can remove them. All remaining columns`Soil_Type` has **two values - 0 and 1**.","6f07d302":"Let's see what a train set looks like","5b0df1eb":"Let's check how big is our data.","80a4c092":"## Standard Scaler","76543117":"## Submission","a3360658":"## Evaluation","d41ea00b":"We need to modify target column by formatting vector (number of rows, 1) to matrix **(number of rows, number of classes)**. Each column of new target matrix corresponds to one class, so each row consists of 0s (which means no class) and 1 (specific class) ","4cdedb1c":"## Import necessary libraries and datasets","893f9717":"Let's find out something more about data. ","41b02221":"Results of models.","caf980bc":"## Train set summary","70876042":"At the end let's check dataset has some missing values.","693644f6":"`Id` column **is redundant**. Let's remove it.","2d0cf3ba":"Split data into train set and test set.","c9a03c3d":"Let's see a distribution of each column.","56e0add2":"There are **no missing values** in train dataset.","5b62b4f2":"Columns `Soil_Type7` and `Soil_Type15` contain only **one value - 0**. They don't introduce a variability, so we can remove them. All remaining columns`Soil_Type` has **two values - 0 and 1**. Just like in train set.","595263b2":"Our train set has **4 000 000 rows** and **55 columns**.","1e227a5f":"Let's see what a test set looks like","b922aad3":"Let's check a number of classes in target column.","e0856eb1":"Let's see a distribution of each column.","02b2c34f":"There is **no correlation between non-binary feature**.","80659570":"Let's check how big is our data.","d06560c5":"There are **no missing values** in test dataset.","0b32ffc6":"Now we can evaluate our model.","7e81c356":"We'll carry out exactly the same steps as above."}}