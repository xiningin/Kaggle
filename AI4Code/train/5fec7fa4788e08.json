{"cell_type":{"6d414242":"code","b6b59e3c":"code","f013b572":"code","a0c2f7c1":"code","8dc0c9d7":"code","9921918c":"code","0cb041f0":"code","975c4926":"code","e2ef46d2":"code","41fe8b4d":"code","576be08f":"code","59923b53":"code","87ea3c51":"code","e31bc130":"code","b3d307da":"code","8f8a31fb":"code","27eba61a":"code","2b0597b2":"code","92b03582":"code","a402f657":"code","87789aaf":"markdown","9e81067d":"markdown","3c0ddebd":"markdown","7613a6fa":"markdown","71639859":"markdown","6ea40583":"markdown","dc73571b":"markdown","ea0f29a4":"markdown","7544f1b5":"markdown","096bea18":"markdown","c364d6c6":"markdown","218d1348":"markdown","80ae8847":"markdown","3e5cd128":"markdown","2b32efb6":"markdown","e4b75112":"markdown","946f709b":"markdown","52617fb8":"markdown","3a38b8fb":"markdown","39b47bda":"markdown","851a4b2d":"markdown","85ca4dbc":"markdown","6fcabc85":"markdown","fb081ec3":"markdown","7a902440":"markdown","77185075":"markdown","fa256927":"markdown","eb29e5f7":"markdown","846673be":"markdown","e82150f0":"markdown","e1007daa":"markdown"},"source":{"6d414242":"#Loading of Dataset and required Libraries\n\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt \nimport seaborn as sns\n\n\ndt= pd.read_csv(\n    \"\/kaggle\/input\/gufhtugu-publications-dataset-challenge\/GP Orders - 5.csv\",\n    encoding=\"utf_8\")\n\nimport warnings  \nwarnings.filterwarnings('ignore')","b6b59e3c":"#to check few rows\ndt.head()","f013b572":"#to check the number of column & rows\nprint(\"dimensions are : \", dt.shape)","a0c2f7c1":"#to check the columns names, data type and null values (if any)\n\nprint(dt.info())","8dc0c9d7":"dt.isna().sum().plot(kind=\"bar\")","9921918c":"print(\"Before drop, total rows are: \", dt.shape[0])\n\n#drop the null values\ndt.dropna(inplace=True)\n\nprint(\"After drop, total rows are: \", dt.shape[0])\n\nprint(dt.isna().sum())","0cb041f0":"#rename the columns\ndt = dt.rename(columns={'Order Number': 'Order_Number',\"Order Status\":\"Order_Status\", \"Book Name\":\"Book_Name\",\"Order Date & Time\":\"Date_Time\",\"Payment Method\":\"Payment_Method\",\"Total items\":\"Total_Items\",\"Total weight (grams)\":\"Weight\"})\nprint(\"After rename, columns names are: \", \"\\n\", \"\\n\" , dt.columns)","975c4926":"#change the type of \"Date_Time\" columns\ndt['Date_Time'] = pd.to_datetime(dt['Date_Time'])\nprint(dt.info())","e2ef46d2":"\n\n#to separate, from multiple to single book title per line\n\n#print('No of rows BEFORE splitting : ',dt.shape[0])\n\nscol = dt['Book_Name'].str.split('\/', expand=True).stack()\nscol.index = scol.index.droplevel(-1) \nscol.name = 'Book_Name' \ndt = dt.drop(columns='Book_Name').join(scol)\n\n#print('No of rows AFTER splitting : ',dt.shape[0])\n\n#manually rename some books names\ndt['Book_Name'] = dt['Book_Name'].replace('\u0627\u0646\u0679\u0631\u0646\u06cc\u0679 \u0633\u06d2 \u067e\u06cc\u0633\u06c1 \u06a9\u0645\u0627\u0626\u06cc\u06ba','Internet Sy Pysy Kamaen')\ndt['Book_Name'] = dt['Book_Name'].replace('\u0627\u0646\u0679\u0631\u0646\u06cc\u0679 \u0633\u06d2 \u067e\u06cc\u0633\u06c1 \u06a9\u0645\u0627\u0626\u06cc\u06ba\u061f- \u0645\u0633\u062a\u062d\u0642\u06cc\u0646 \u0632\u06a9\u0648\u0627\u0629','Internet Sy Pysy Kamaen')\ndt['Book_Name'] = dt['Book_Name'].replace('\u0688\u06cc\u0679\u0627 \u0633\u0627\u0626\u0646\u0633','Data Science')\ndt['Book_Name'] = dt['Book_Name'].replace('\u0688\u06cc\u0679\u0627 \u0633\u0627\u0626\u0646\u0633 \u06d4 \u0627\u06cc\u06a9 \u062a\u0639\u0627\u0631\u0641','Data Science')\ndt['Book_Name'] = dt['Book_Name'].replace('\u0645\u0634\u06cc\u0646 \u0644\u0631\u0646\u0646\u06af','Machine Learning')\ndt['Book_Name'] = dt['Book_Name'].replace('(C++) ++\u0633\u06cc','(C++)')\n\n\n#extracting top 20 books for fuzzywuzzy\ntop_bks=dt[\"Book_Name\"].value_counts().head(20).reset_index()\ntop_bks.columns=['Book_Name','Sold_Qty']\nall_bks = dt[\"Book_Name\"].unique()\n\n#renaming the books name to close matching using fuzzywuzzy\nfrom fuzzywuzzy import process\n\nfor bks in top_bks['Book_Name']:\n    matches = process.extract(bks, all_bks , limit = len(all_bks))\n    for potential_match in matches:\n        if potential_match[1] > 90:\n                dt.loc[dt['Book_Name'] == potential_match[0],\"Book_Name\"] = bks\n    \ndt.reset_index(drop=True, inplace=True)\nprint(\"Top 10 unique Books are: \\n\",dt[\"Book_Name\"].value_counts().head(10))","41fe8b4d":"dt['City'] = dt['City'].replace(['karachi','KARACHI'],'Karachi')\ndt['City'] = dt['City'].replace('FSD','Faisalabad')\ndt['City'] = dt['City'].replace(['lahore','LAHORE'],'Lahore')\n\n#extracting top 20 cities for fuzzywuzzy\n\nfuzz_top_City=dt[\"City\"].value_counts().head(20).reset_index()\nfuzz_top_City.columns=['City','Sold_Qty']\nfuzz_all_City = dt[\"City\"].unique()\n\n#removing the typo mistake in books name\n\nfrom fuzzywuzzy import process\nfor city in fuzz_top_City['City']:\n    matches = process.extract(city, fuzz_all_City , limit = len(fuzz_all_City))\n    for potential_match in matches:\n        if potential_match[1] > 90:\n                dt.loc[dt['City'] == potential_match[0],\"City\"] = city\n                \nprint(\"Top 10 Cities are: \\n\",dt[\"City\"].value_counts().head(10))","576be08f":"cty = dt['City'].value_counts().iloc[:10]\nbks = dt['Book_Name'].value_counts().iloc[:10]\n\n#tcb=top 10 cities and top 10 books\ntcb=dt.groupby([\"City\",\"Book_Name\"])[\"Order_Number\"].count().reset_index().sort_values(\"Order_Number\", ascending=False)\n\ntcb=tcb[tcb[\"Book_Name\"].isin(bks.index)]\ntcb=tcb[tcb[\"City\"].isin(cty.index)]\ntcb.head()","59923b53":"#to add another column for Province against each city\nprov = {'Karachi': \"Sindh\", 'Lahore': \"Punjab\", 'Islamabad': \"Islamabad\", 'Rawalpindi': \"Punjab\", 'Faisalabad': \"Punjab\",\n       'Peshawar': \"KPK\", 'Multan': \"Punjab\", 'Gujranwala': \"Punjab\", 'Sialkot': \"Punjab\", 'Hyderabad': \"Sindh\"}\n\ntcb[\"Province\"] = tcb[\"City\"].map(prov)\ntcb.columns=['City','Book_Name','Total_Order','Province']\n\ntopbks=tcb.groupby(\"Book_Name\")[\"Total_Order\"].sum().reset_index().sort_values(\"Total_Order\", ascending=False)\ntopcty=tcb.groupby(\"City\")[\"Total_Order\"].sum().reset_index().sort_values(\"Total_Order\", ascending=False)\ntoppro=tcb.groupby(\"Province\")[\"Total_Order\"].sum().reset_index().sort_values(\"Total_Order\", ascending=False)\n\ntcb","87ea3c51":"fig, ax = plt.subplots()\nax=sns.barplot(x=\"Book_Name\",y=\"Total_Order\",data=topbks,ci=None)\nax.set_xticklabels(topbks[\"Book_Name\"], rotation=90,fontsize=18)\nfig.set_size_inches([18,12])\n#ax.yaxis.set_major_formatter(mtick.PercentFormatter(10000))\nax.set_title('Top 10 Ordered Books ',fontsize=20)\nax.set_xlabel(\"Books Titles\",fontsize=18)\nax.set_ylabel(\"Number of Order(s)\",fontsize=18) \nplt.show()","e31bc130":"\nfig, ax = plt.subplots()\nax=sns.barplot(x=\"City\",y=\"Total_Order\",data=topcty,ci=None)\nax.set_xticklabels(topcty[\"City\"], rotation=90)\nfig.set_size_inches([18, 12])\n#ax.yaxis.set_major_formatter(mtick.PercentFormatter(10000))\nax.set_title('Top 10 Cities - Order Wise',fontsize=20)\nax.set_xlabel(\"City Name(s)\",fontsize=18)\nax.set_ylabel(\"Number of Order(s)\",fontsize=18) \nplt.show()","b3d307da":"fig, ax = plt.subplots()\nax=sns.barplot(x=\"Province\",y=\"Total_Order\",data=toppro,ci=None)\nax.set_xticklabels(toppro[\"Province\"], rotation=90)\nfig.set_size_inches([12, 8])\nax.set_title('Top Province - Order Wise',fontsize=20)\nax.set_xlabel(\"Province Name(s)\",fontsize=18)\nax.set_ylabel(\"Number of Order(s)\",fontsize=18) \nplt.show()","8f8a31fb":"fig, ax = plt.subplots()\nax=sns.pointplot(x=\"Book_Name\",y=\"Total_Order\",data=tcb,hue=\"City\")\n#ax.set_xticklabels(tcb[\"Book_Name\"], rotation=90)\n#ax.set_xticklabels(rotation=90)\nfig.set_size_inches(18,9)\n#ax.yaxis.set_major_formatter(mtick.PercentFormatter(10000))\nax.set_title('Top 10 Books - City wise',fontsize=20)\nax.set_xlabel(\"Books Name\",fontsize=18)\nax.set_ylabel(\"Number of Order(s)\",fontsize=18) \nplt.xticks(rotation=90)\nplt.show()","27eba61a":"fig, ax = plt.subplots()\nax=sns.barplot(x=\"Book_Name\",y=\"Total_Order\",data=tcb,hue=\"City\")\n#ax.set_xticklabels(tcb[\"Book_Name\"], rotation=90)\n#ax.set_xticklabels(rotation=90)\nfig.set_size_inches(18,9)\n#ax.yaxis.set_major_formatter(mtick.PercentFormatter(10000))\nax.set_title('Top 10 Books - City wise',fontsize=20)\nax.set_xlabel(\"Books Name\",fontsize=18)\nax.set_ylabel(\"Number of Order(s)\",fontsize=18) \nplt.xticks(rotation=90)\nplt.show()","2b0597b2":"\nfig, ax = plt.subplots()\nsns.set_style(\"whitegrid\")\nax=sns.pointplot(x=\"Book_Name\",y=\"Total_Order\",data=tcb,hue=\"Province\", ci=None)\n#ax.set_xticklabels(tcb[\"Book_Name\"], rotation=90)\n#ax.set_xticklabels(rotation=90)\nfig.set_size_inches(18,9)\n#ax.yaxis.set_major_formatter(mtick.PercentFormatter(10000))\nax.set_title('Top 10 Books - Province wise',fontsize=20)\nax.set_xlabel(\"Books Name\",fontsize=18)\nax.set_ylabel(\"Number of Order(s)\",fontsize=18) \nplt.xticks(rotation=90)\nplt.show()","92b03582":"fig, ax = plt.subplots()\nax=sns.barplot(x=\"Book_Name\",y=\"Total_Order\",data=tcb,hue=\"Province\",ci=None)\n#ax.set_xticklabels(tcb[\"Book_Name\"], rotation=90)\n#ax.set_xticklabels(rotation=90)\nfig.set_size_inches(18,9)\n#ax.yaxis.set_major_formatter(mtick.PercentFormatter(10000))\nax.set_title('Top 10 Books - Province wise',fontsize=20)\nax.set_xlabel(\"Books Name\",fontsize=18)\nax.set_ylabel(\"Number of Order(s)\",fontsize=18) \nplt.xticks(rotation=90)\nplt.show()","a402f657":"fig, ax = plt.subplots()\nax=sns.swarmplot(x=\"Book_Name\",y=\"Total_Order\",data=tcb,hue=\"Province\")\nplt.legend(bbox_to_anchor=(1, 1), loc=2)\n#ax.set_xticklabels(tcb[\"Book_Name\"], rotation=90)\n#ax.set_xticklabels(rotation=90)\nfig.set_size_inches(18,9)\n#ax.yaxis.set_major_formatter(mtick.PercentFormatter(10000))\nax.set_title('Top 10 Books - Province wise',fontsize=20)\nax.set_xlabel(\"Books Name\",fontsize=18)\nax.set_ylabel(\"Number of Order(s)\",fontsize=18) \nplt.xticks(rotation=90)\nplt.show()","87789aaf":"# Top 10 Books - Province wise","9e81067d":"for the ease of display, the urd names have been replaced with roman English","3c0ddebd":"# Loading data","7613a6fa":"\"Book_Name\" column contains more than one book. lets split it.","71639859":"graph shows that \"Internet Sy Pysy Kamaen\"  is top selling book. which was ordered more than 1600 times. \n\"Sukkur To Florida\" is at 10th position","6ea40583":"# **Handling Inconsistent Data in City Names**","dc73571b":"Graph shows that most orders were received from Karachi which are above 2500.","ea0f29a4":"graph shows that \"Internet Sy Pysy Kamaen\" is top selling book for almost all the top cities excep for Lahore, where the sale of  \"Sukkur to Florida is slightly higher then \"Internet Sy Pysy Kamaen\". C++ is the least selling book in top 10 books","7544f1b5":"graph shows that highest no of orders were received from Punjab. followed by Sindh, Islamabad and KPK.","096bea18":"so data contains 19239 rows and 8 columns","c364d6c6":"# Data cleaning","218d1348":"**Rename the columns**","80ae8847":"**Now our data is clean and we are ready to visualize**","3e5cd128":"The \"City\" column contains many typos. lets fix it.","2b32efb6":"**Task Details:**\nFind out Top 10 selling books.\n\n**Expected Submission:**\nThe names of Top Selling Books\n\n**Evaluation:**\nShow some visualization and see if you can break the top selling books by province and city","e4b75112":"# Top 10 Ordered Books","946f709b":"# Basic Data Exploration","52617fb8":"Lets rename the columns to more appropriate","3a38b8fb":"**Handling of missing values.**","39b47bda":"# Top 10 Cities - Order Wise","851a4b2d":"Thanks for viewing my notebook, you are welcome to give any comments\/suggetions to further improve my work.","85ca4dbc":"These 3 columns contains the missing values","6fcabc85":"graph shows that for all the books,maximum order were receved from Sindh province.","fb081ec3":"**To add another column for Province against each city**","7a902440":"\"Non-Null Count\" of few columns shows the presence of null values","77185075":"**Creating new dataset**","fa256927":"# Top 10 Books - City wise","eb29e5f7":"# **Handling Inconsistent Data in Books Name**","846673be":"# Top Province - Order Wise","e82150f0":"**\"Date_Time\" columns has \"object\" type, we will change it to datetime64 type.**","e1007daa":"now data doesnt contains any missing values"}}