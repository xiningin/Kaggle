{"cell_type":{"48f3a4a5":"code","94db19c0":"code","77528ea1":"code","9b9cefe1":"code","18d70cb4":"code","a731daaa":"code","4d323acd":"code","7932eaa7":"code","2bb94cc2":"code","abd011c7":"code","f1cea62b":"code","3ccde900":"code","6f2e2d36":"code","f51bc8ce":"code","1203b166":"code","1834d0b7":"code","2733c132":"code","526ae1f5":"code","5bcee5f7":"code","0b89bbfa":"code","8ec37c13":"code","b19ae9b7":"code","ff002cef":"code","6b517518":"code","43f4427c":"code","30146074":"code","aa6cc198":"code","e81f05c7":"code","aa9acf07":"code","460ac7d1":"code","79e453d4":"code","3dc4a55f":"code","1ed58130":"code","37e52695":"code","cbdc1c01":"code","b452ead5":"code","ac497c74":"code","6a5a1b3d":"code","c13107e8":"code","a92ca85e":"code","b6c235d3":"code","01942a76":"markdown","26ec7137":"markdown","21fe1706":"markdown","3f033f57":"markdown","1b041d32":"markdown","abcb90ff":"markdown","def070c0":"markdown","cdebd51a":"markdown","b97e7190":"markdown","c5a89e02":"markdown"},"source":{"48f3a4a5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","94db19c0":"df = pd.read_csv('\/kaggle\/input\/word2vec-nlp-tutorial\/labeledTrainData.tsv.zip', delimiter=\"\\t\")","77528ea1":"df = df.drop(['id'], axis=1)\ndf.head()","9b9cefe1":"df.info()","18d70cb4":"df.sentiment.value_counts()","a731daaa":"df1=pd.read_csv(\"\/kaggle\/input\/word2vec-nlp-tutorial\/testData.tsv.zip\",delimiter= \"\\t\")\ndf1.head()","4d323acd":"train_len=df['review'].apply(len)\ntrain_len.describe()\n","7932eaa7":"test_len=df['review'].apply(len)\ntest_len.describe()","2bb94cc2":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfig=plt.figure(figsize=(14,8))\nfig.add_subplot(1,2,1)\nsns.distplot((train_len),color='red')\n\nfig.add_subplot(1,2,2)\nsns.distplot((test_len),color='blue')","abd011c7":"df['word_n'] = df['review'].apply(lambda x : len(x.split(' ')))\ndf1[\"word_n\"]=df1[\"review\"].apply(lambda x : len(x.split(\" \")))","f1cea62b":"fig=plt.figure(figsize=(14,6))\nfig.add_subplot(1,2,1)\nsns.distplot(df['word_n'],color='red')\n\nfig.add_subplot(1,2,2)\nsns.distplot(df1['word_n'],color='blue')","3ccde900":"sns.countplot(df['sentiment'])","6f2e2d36":"from wordcloud import WordCloud\ncloud=WordCloud(width=800, height=600).generate(\" \".join(df['review'])) \n# join function can help merge all words into one string. \" \" means space can be a seperator between words.\nplt.figure(figsize=(16,10))\nplt.imshow(cloud)\nplt.axis('off')","f51bc8ce":"import re\nimport json\n","1203b166":"TAG_RE = re.compile(r'<[^>]+>')","1834d0b7":"df['review']=df['review'].apply(lambda x:TAG_RE.sub('', x))\ndf1['review']=df1['review'].apply(lambda x: TAG_RE.sub('', x))","2733c132":"from wordcloud import WordCloud\ncloud=WordCloud(width=800, height=600).generate(\" \".join(df['review'])) \n# join function can help merge all words into one string. \" \" means space can be a seperator between words.\nplt.figure(figsize=(16,10))\nplt.imshow(cloud)\nplt.axis('off')","526ae1f5":"df['review']=df['review'].apply(lambda x: re.sub(\"[^a-zA-Z]\",\" \",x))\ndf1['review']=df1['review'].apply(lambda x: re.sub(\"[^a-zA-Z]\",\" \",x))","5bcee5f7":"df1.sample(4)","0b89bbfa":"df[\"review\"].str.find(\"?\").value_counts()","8ec37c13":"df['word_n_2'] = df['review'].apply(lambda x : len(x.split(' ')))\ndf1['word_n_2'] = df1['review'].apply(lambda x : len(x.split(' ')))\n\nfig, axe = plt.subplots(1,1, figsize=(7,5))\nsns.boxenplot(x=df['sentiment'], y=df['word_n_2'], data=df)","b19ae9b7":"# from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\n\nstop_words = set(stopwords.words(\"english\")) \n# lemmatizer = WordNetLemmatizer()","ff002cef":"df[\"review\"]=df['review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))","6b517518":"df1[\"review\"]=df1['review'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))","43f4427c":"test=df1.drop([\"word_n\",\"word_n_2\",\"id\"],axis=1)","30146074":"X=df.drop([\"word_n\",\"word_n_2\",\"sentiment\"],axis=1)","aa6cc198":"X.head(3)","e81f05c7":"Y=df.drop([\"word_n\",\"word_n_2\",\"review\"],axis=1)","aa9acf07":"Y.head(3)","460ac7d1":"from sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.tokenize import RegexpTokenizer\n#tokenizer to remove unwanted elements from out data like symbols and numbers\ntoken = RegexpTokenizer(r'[a-zA-Z0-9]+')\ncv = CountVectorizer(lowercase=True,stop_words='english',ngram_range = (1,1),tokenizer = token.tokenize)\ntext_counts= cv.fit_transform(df[\"review\"])","79e453d4":"text_counts","3dc4a55f":"from sklearn.feature_extraction.text import TfidfVectorizer\ntf=TfidfVectorizer()\ntext_tf= tf.fit_transform(df['review'])","1ed58130":"text_tf","37e52695":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(text_tf,Y, test_size=0.2, random_state=42)","cbdc1c01":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","b452ead5":"from sklearn.naive_bayes import MultinomialNB\nfrom sklearn import metrics\n# Model Generation Using Multinomial Naive Bayes\nclf = MultinomialNB().fit(X_train, y_train.values.ravel())\npredicted= clf.predict(X_test)\nprint(\"MultinomialNB Accuracy:\",metrics.accuracy_score(y_test, predicted)*100)\n","ac497c74":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,predicted)\ncm","6a5a1b3d":"from sklearn.preprocessing import StandardScaler  # doctest: +SKIP\n>>> scaler = StandardScaler(with_mean=False)  # doctest: +SKIP\n>>> # Don't cheat - fit only on training data\n>>> scaler.fit(X_train)  # doctest: +SKIP\n>>> X_train = scaler.transform(X_train)  # doctest: +SKIP\n>>> # apply same transformation to test data\n>>> X_test = scaler.transform(X_test)","c13107e8":"from sklearn.tree import DecisionTreeClassifier\ndt=DecisionTreeClassifier().fit(X_train,y_train)\npredict=dt.predict(X_test)\nprint(\"Decision Tree Accuracy:\",metrics.accuracy_score(y_test, predict)*100)","a92ca85e":"from sklearn.ensemble import GradientBoostingClassifier\nbg=GradientBoostingClassifier(random_state=0,n_estimators=200)\nbg.fit(X_train,y_train.values.ravel())\ny_pred=bg.predict(X_test)\nprint(\"Boosting Accuracy:\",metrics.accuracy_score(y_test, y_pred)*100)","b6c235d3":"cm=confusion_matrix(y_test,y_pred)\ncm","01942a76":"Creating a word cloud to see, the words which appear mostly","26ec7137":"Remoiving unwanted HTML tags such as **br**  which appears the maximum","21fe1706":"Using regrex library, we can remove the **html** tags easily from the sentiments","3f033f57":"Loading the test data set ","1b041d32":"Keeping only alphabets in the review segment ","abcb90ff":"* We can see train and test data have statistical features \n* The mean words count is 1327 and std is 1005 words\n* The character count seems to show similar distribution with word count","def070c0":"Loading the training data set ","cdebd51a":"From the Word CLoud , we can see that all the **html** tags are removed from the sentiments ","b97e7190":"Visualizing the train and test data set ","c5a89e02":"Spitting the review in the words "}}