{"cell_type":{"ce7ecba6":"code","b2dd3d75":"code","8db8f313":"code","277c473f":"code","445373d7":"code","dab709a8":"code","a4c9cac8":"code","eb043235":"code","0cfc5b92":"code","78a94cd7":"code","c24a88d9":"code","f27a934b":"code","1113ee07":"code","a27394af":"code","11da518d":"code","d2154318":"code","78ace870":"code","e42518d8":"code","3fc4ab38":"code","5b42ae84":"code","44de0e4d":"code","eff33252":"code","b8d930e6":"code","44dec332":"code","1873c71f":"code","6827a671":"code","087f26d4":"code","29a2024a":"markdown","79843c66":"markdown","877937d3":"markdown","7f119e30":"markdown","b556ed0f":"markdown","1ebb93ce":"markdown","6b80bf67":"markdown","f110fead":"markdown","a736df13":"markdown","b2552916":"markdown","94d00418":"markdown","d9949e14":"markdown","75a90928":"markdown","e9228771":"markdown","50a71961":"markdown","e326c516":"markdown","60e51585":"markdown","1a7ea77d":"markdown","03be0517":"markdown","b54b2194":"markdown"},"source":{"ce7ecba6":"import os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style('darkgrid')\nimport re\nfrom collections import Counter\nfrom tqdm import tqdm\nfrom time import time\n\nimport tensorflow as tf\nfrom tensorflow.keras import models, layers, optimizers, losses, metrics, utils, applications, callbacks\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\nfrom nltk.corpus import stopwords\nstopwords_eng = set(stopwords.words('english'))\n\nfrom sklearn.model_selection import train_test_split as tts\ndef tvt_split(X, y, split_sizes=[8,1,1], stratify=True):\n    split_sizes = np.array(split_sizes)\n    if stratify:\n        train_X, test_X, train_y, test_y = tts(X, y, test_size=split_sizes[2]\/split_sizes.sum(), stratify=y)\n        train_X, val_X, train_y, val_y = tts(train_X, train_y, test_size=split_sizes[1]\/(split_sizes[0]+split_sizes[1]), stratify=train_y)\n    else:\n        train_X, test_X, train_y, test_y = tts(X, y, test_size=split_sizes[2]\/split_sizes.sum())\n        train_X, val_X, train_y, val_y = tts(train_X, train_y, test_size=split_sizes[1]\/(split_sizes[0]+split_sizes[1]))\n    return train_X, val_X, test_X, train_y, val_y, test_y\n\n\ninput_dir = '..\/input\/enron-email-dataset'\n","b2dd3d75":"data = pd.read_csv(input_dir+'\/emails.csv')\ndata.info()","8db8f313":"print(data.file[0])","277c473f":"print(data.message[0])","445373d7":"metadata_pattern = re.compile(r'^.+(?=:)', re.M) # this way we will get greedy behavior\nmetadata_names = metadata_pattern.findall(data.message[0])\nmetadata_names","dab709a8":"metadata_pattern = re.compile(r'^.+?(?=:)', re.M) # we place ? after the quantifier (+) to prevent greedy behavior\nmetadata_names = metadata_pattern.findall(data.message[0])\nmetadata_names","a4c9cac8":"for metadatum_name in metadata_names:\n    pattern = re.compile(r'(?<=%s:\\s).+$'%metadatum_name, re.M)\n    print(metadatum_name, pattern.search(data.message[0]))\n    ","eb043235":"for metadatum_name in tqdm(metadata_names):\n    pattern = re.compile(r'(?<=%s:\\s).+$'%metadatum_name, re.M)\n    data[metadatum_name] = data.message.apply(lambda x: pattern.search(x).group() if pattern.search(x) != None else None)","0cfc5b92":"weekday_pattern = re.compile(r'\\A\\w+(?=,?)', re.M) # from the beginning of the input to the first comma, composed of one or more alphanumeric characters\nmonthday_pattern = re.compile(r'(?<=,\\s)\\d+\\b', re.M) # from the first comma+whitespace, composed of one or more digits, ending with a word boundary\nmonth_pattern = re.compile(r'(?<=\\d\\s)\\w+(?=\\s\\d)', re.M)\nyear_pattern = re.compile(r'(?<=[A-Z][a-z]{2}\\s)\\d+(?=\\s\\d\\d:)', re.M)\nhour_pattern = re.compile(r'(?<=\\d{4}\\s)\\d\\d(?=:\\d\\d)', re.M)\nminute_pattern = re.compile(r'(?<=\\d\\d:)\\d\\d(?=:\\d\\d)', re.M)\nsecond_pattern = re.compile(r'(?<=\\d\\d:\\d\\d:)\\d\\d(?=\\s)', re.M)","78a94cd7":"data['Weekday'] = data['Date'].apply(lambda x: weekday_pattern.search(x).group())\ndata['Monthday'] = data['Date'].apply(lambda x: monthday_pattern.search(x).group())\ndata['Month'] = data['Date'].apply(lambda x: month_pattern.search(x).group())\ndata['Year'] = data['Date'].apply(lambda x: year_pattern.search(x).group())\ndata['Hour'] = data['Date'].apply(lambda x: hour_pattern.search(x).group())\ndata['Minute'] = data['Date'].apply(lambda x: minute_pattern.search(x).group())\ndata['Second'] = data['Date'].apply(lambda x: second_pattern.search(x).group())","c24a88d9":"print(set(data['Weekday']))\nprint(set(data['Monthday']))\nprint(set(data['Month']))\nprint(set(data['Year']))\nprint(set(data['Hour']))\nprint(set(data['Minute']))\nprint(set(data['Second']))","f27a934b":"data['Date'][0]","1113ee07":"time_multi_pattern = re.compile(\n    pattern=r'(?P<Weekday>\\A[A-Z][a-z]{2})\\W+?(?P<Monthday>\\d+)\\W+?(?P<Month>[A-Z][a-z]{2})\\W+?(?P<Year>\\d{4})\\W+?(?P<Hour>\\d\\d)\\W+?(?P<Minute>\\d\\d)\\W+?(?P<Second>\\d\\d)',\n    flags=re.M\n)\n\ntime_multi_pattern.groupindex","a27394af":"for new_col_name in time_multi_pattern.groupindex:\n    data[new_col_name] = data['Date'].apply(lambda x: time_multi_pattern.search(x).groupdict()[new_col_name])","11da518d":"print(set(data['Weekday']))\nprint(set(data['Monthday']))\nprint(set(data['Month']))\nprint(set(data['Year']))\nprint(set(data['Hour']))\nprint(set(data['Minute']))\nprint(set(data['Second']))","d2154318":"# Define the patterns:\nxfn_pattern = re.compile(r'%s.*$'%metadata_names[-1], re.M) # X-FileName: is the last one in the metadata_names list (thus [-1] index) \ncontent_pattern = re.compile('[^\\n].*', flags=re.S) #anything that's not a newline (caret negates \\n) followed by any (including none) number of any characters\n\n# Test the patterns:\nxfn_end = xfn_pattern.search(data.message[10]).end() # Find the X-FileName information and take the index of its last character \nmatch = content_pattern.search(data.message[10], pos=xfn_end) # Start searching from this index\nprint(match.group())","78ace870":"data['Message Content'] = data.message.apply(\n    lambda x: content_pattern.search(\n        x,\n        pos = xfn_pattern.search(x).end()\n    ).group()\n)","e42518d8":"data['Message Length'] = data['Message Content'].apply(lambda x: len(x))\n\nsns.distplot(data['Message Length'])","3fc4ab38":"for order_of_magnitude in reversed(range(2,6)):\n    max_ = 10**order_of_magnitude\n    print(\"Messages not longer than %i characters:\"%max_)\n    plt.hist(data.query('`Message Length`<@max_')['Message Length'], bins=100)\n    #histplot(data.query('`Message Length`<@max_'), x='Message Length')\n    plt.show()","5b42ae84":"# Extract the labels of the two most productive e-mail senders\n\nlabel_0, label_1 = list(data['From'].value_counts().index)[:2]\nlabel_0, label_1","44de0e4d":"# A function to remove stopwords (not very informative words, like \"the\", \"a\", \"and\", and so on)\ndef remove_stopwords(txt):\n    for stopword in stopwords_eng:\n        while stopword in txt:\n            txt = txt.replace(stopword,'')\n    return txt\n\n# Number of messages per one class - I chose 1280, because after splitting it 8:1:1 into train:val:test sets it gives us 1024:128:128, which are all powers of 2\nn_per_class = 1280\n\n# Take that many messages sent by this person after shuffling them (.sample() method)\nmessages_0 = data.query('From==@label_0')['Message Content'].sample(frac=1)[:n_per_class].values\nmessages_1 = data.query('From==@label_1')['Message Content'].sample(frac=1)[:n_per_class].values\n\n# Remove stopwords by applying the function defined above\nmessages_0 = [remove_stopwords(s) for s in messages_0]\nmessages_1 = [remove_stopwords(s) for s in messages_1]\n","eff33252":"# Maximum number of words, we are going to embed\nmax_features = 2048\n\n# Number of embedding dimensions in the word embedding space constructed by the embedding layer\nembed_dim = 256\n\n# The length of the sequence (message) - The longer messages will be cropped to 256, while the shorter ones will be padded with zeros to be exactly 256 tokens long\nmaxlen = 256\n\n# Take messages from both classes and shuffle them before feeding to tokenizer\nmessages_all = messages_0+messages_1\nnp.random.shuffle(messages_all)\n\n# The tokenizer ascribes a number to each token (word) in the sequence\ntokenizer = Tokenizer(num_words=max_features)\ntokenizer.fit_on_texts(messages_all)\nword_index = tokenizer.word_index # This dictionary translates each word to its index (corresponding number)\n\n# Transform messages into sequences of numbers corresponding to its particular words\nseqs_0 = tokenizer.texts_to_sequences(messages_0)\nseqs_1 = tokenizer.texts_to_sequences(messages_1)\n\n# Pad sequences, i.e. make them exactly 256 tokens long (as described above)\nseqs_0 = pad_sequences(seqs_0, maxlen=maxlen)\nseqs_1 = pad_sequences(seqs_1, maxlen=maxlen)\n\n# Concatenate the sequences\nseqs_all = np.concatenate([seqs_0, seqs_1], axis=0)\n\n# Create and concatenate the labels\nlabels_0 = np.zeros(shape=(seqs_0.shape[0]))\nlabels_1 = np.ones(shape=(seqs_1.shape[0]))\nlabels_all = np.concatenate([labels_0, labels_1], axis=0)\n\n# Split the dataset into training, validaiton and test sets in default proportions 8:1:1\ntrain_X, val_X, test_X, train_y, val_y, test_y = tvt_split(seqs_all, labels_all, stratify=True)","b8d930e6":"for X, y in [train_X, train_y], [val_X, val_y], [test_X, test_y]:\n    print(X.shape, y.shape)\n    print(np.bincount(y.astype(np.int32)))","44dec332":"model = models.Sequential(layers=[\n    layers.Embedding(input_dim=max_features, output_dim=embed_dim, input_length=maxlen),\n    layers.Bidirectional(layers.GRU(32, activation='relu', return_sequences=True, dropout=.1, recurrent_dropout=.1)),\n    layers.Bidirectional(layers.GRU(32, activation='relu', return_sequences=False, dropout=.1, recurrent_dropout=.1)),\n    layers.Dense(64, activation='relu', kernel_regularizer='l2'),\n    layers.BatchNormalization(),\n    layers.Dropout(.2),\n    layers.Dense(32, activation='relu', kernel_regularizer='l2'),\n    layers.BatchNormalization(),\n    layers.Dropout(.1),\n    layers.Dense(1, activation='sigmoid')\n])\nmodel.summary()","1873c71f":"callbacks_list = [\n    callbacks.ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True, save_freq='epoch'), # Save the best model, i.e. the one with the lowest validation loss (measured after each epoch)\n    callbacks.ReduceLROnPlateau(monitor='val_loss', factor=.2, patience=5), # Reduce the learning rate by a factor of 5 if validation loss has not been decreasing for 5 epochs\n    callbacks.EarlyStopping(patience=10) # Stop training after 10 epochs of no validation loss reduction\n]\n\nmodel.compile(\n    optimizer='rmsprop',\n    loss='binary_crossentropy',\n    metrics=['acc']\n)\n\nEPOCHS = 24\n\nhistory = model.fit(\n    train_X, train_y,\n    validation_data = (val_X, val_y),\n    epochs = EPOCHS, batch_size=64,\n    shuffle = True,\n    verbose = 1,\n    callbacks = callbacks_list\n)\n\nmodel.save('last_model.h5') # Save the final model, so that we can compare it with the best model (the one with the lowest validation loss)","6827a671":"train_loss = history.history['loss']\nval_loss = history.history['val_loss']\n\ntrain_acc = history.history['acc']\nval_acc = history.history['val_acc']\n\nlr = history.history['lr']\n\n\nplt.plot(np.arange(1,EPOCHS+1), train_loss, 'r--', label='Training loss')\nplt.plot(np.arange(1,EPOCHS+1), val_loss, 'g-', label='Validation loss')\nplt.title('Training and validation loss')\nplt.legend()\nplt.show()\n\nplt.plot(np.arange(1,EPOCHS+1), train_acc, 'r--', label='Training accuracy')\nplt.plot(np.arange(1,EPOCHS+1), val_acc, 'g-', label='Validation accuracy')\nplt.title('Training and validation accuracy')\nplt.legend()\nplt.show()\n\nplt.plot(np.arange(1,EPOCHS+1), lr, 'b-', label='Learning Rate')\nplt.yscale('log')\nplt.title('Learning rate')\nplt.legend()\nplt.show()","087f26d4":"model = models.load_model('last_model.h5')\nprint(\"\\t\\tLAST MODEL:\")\nprint('Training set evaluation:\\tLoss: %.4f\\tAccuracy: %.4f' % tuple(model.evaluate(train_X, train_y, verbose=0)))\nprint('Validation set evaluation:\\tLoss: %.4f\\tAccuracy: %.4f' % tuple(model.evaluate(val_X, val_y, verbose=0)))\nprint('Testing set evaluation:\\t\\tLoss: %.4f\\tAccuracy: %.4f' % tuple(model.evaluate(test_X, test_y, verbose=0)))\n\nmodel = models.load_model('best_model.h5')\nprint(\"\\n\\t\\tBEST MODEL:\")\nprint('Training set evaluation:\\tLoss: %.4f\\tAccuracy: %.4f' % tuple(model.evaluate(train_X, train_y, verbose=0)))\nprint('Validation set evaluation:\\tLoss: %.4f\\tAccuracy: %.4f' % tuple(model.evaluate(val_X, val_y, verbose=0)))\nprint('Testing set evaluation:\\t\\tLoss: %.4f\\tAccuracy: %.4f' % tuple(model.evaluate(test_X, test_y, verbose=0)))\n","29a2024a":"No, it's not a bug. It's just that most messages are quite short (shorter than 0.25\\*10^6), but there are few even as long as 2\\*10^6.\n\nTherefore, if we want to have an informative look, we need to put an upper limit on the value investigated (here: message length). Actually, let's try a few such limits, i.e. decreasing orders of magnitude, from 100000 down to 100 characters:","79843c66":"Since 'Date' feature contains many separate pieces of information, such as year, month, day of the week, and day of the month, and also precise hour, we can extract also these features and pack them into individual columns.","877937d3":"We see that 'file' column contains filepaths, while the entire message content (including its \"metadata\") is packed into the 'message' column. We can engineer a lot of features out from that using Regular Expressions.\n\nThe pattern shown below searches for substrings, which:\n\n* Begin with a beginning of a line - signified by caret ^ (also, for this we need to specifiy re.M flag)\n* Contain any number (but at least one) of any signs after this beginning of a line - signified by a dot (any sign) and a plus (at least one, but no upper limit)\n* Finish with a colon, which we do not take as a part of this substrings (so called look-ahead) - this is the meaning of '(?=:)' part\n\nNormally, RegEx would \"try\" to take the largest such pattern it finds, which in the case of the line containing information about date, would include characters up to the semicolon separating minutes from seconds. To prevent this (and \"switch\" it from so called \"greedy behavior\" to \"non-greedy behavior\") we place a question mark after the quantifier (in this case plus sign).\n\nIf you would like to see the difference expand the below cell.","7f119e30":"Let's take a look at the structure of files and messages","b556ed0f":"The only \"raw\" feature left to extract is the actual content of the message.\n\nIt is contained right below the 'X-FileName:' line. So we will find the last index of this line and extract the substring starting with a newline (\\n) character followed by any (including none) number of any characters until the end of string.\n\nTo make dots match **any sign, including newlines (\\n)**, we need to pass the re.S flag.","1ebb93ce":"We could also do it in a more clever way, creating a single pattern, which will search for all these groups (again, I refer you to the tutorial linked above).","6b80bf67":"As a reasonable baseline for a text classification task, I decided to use a two-layer stack of bidirectional GRUs with, followed by two Dense layers, regularized with batch normalization, L2 and a modest amount of dropout:","f110fead":"Sanity check for the shapes of sets and an equal distribution of classes:","a736df13":"Come to think of that. One of the most obvious features we could engineer out of that, is the length of the proper message. We can plot the distribution of message lengths with tools provided by Seaborn:","b2552916":"3) Make sure everything's alright","94d00418":"1) Define the patterns","d9949e14":"If you would like to learn more about RegEx, look-ahead, greedy behavior, and other things, I highly recommend this tutorial:\n\nhttps:\/\/www.youtube.com\/playlist?list=PLyb_C2HpOQSDxe5Y9viJ0JDqGUCetboxB","75a90928":"# Sender classifier (two classes)\n\nLet's now take the two most common e-mail senders in the dataset and build a model, which, based on the message content, tries to predict the person who sent it.\n\n","e9228771":"For each metadatum_name we find only one match, which we can extract with the .group() method and add as a new column to the dataframe.","50a71961":"The same results with much less code","e326c516":"It's interesting that the model with the best validation accuracy (the \"best model\") performs significantly worse (2.3%) on the test set than the model with a 1.5% lower validation accuracy.\n\nThat's it for now. I'm going to continue to work on the sender prediction in a separate notebook.","60e51585":"2) Create the columns","1a7ea77d":"We can access the content of each respective group with the groupdict().","03be0517":"Seems to work. Let's create the 'Message Content' column:","b54b2194":"The following loop extracts strings which begin right after each of the metadata categories we extracted (stored in the metadata_names list) plus a colon and a whitespace, and continue until the end of the line (marked by a dollar sign). Again, we need to pass it the re.M flag for the multiline special character (caret previously, dollar sign now) to work as we intend it to."}}