{"cell_type":{"1c81d6cc":"code","0ebe044c":"code","5f087f4c":"markdown"},"source":{"1c81d6cc":"agent_as_a_string = \"\"\"\nimport time\nimport numpy as np\nfrom random import choice\nfrom math import sqrt\nfrom numba import njit\nfrom numba import prange\n\n# The following several functions are run with the Numba JIT compiler\n# resulting in a dramatic speed increase.\n\n@njit() # Numba function\ndef win_for_tup_jit(b, tup, I, color):\n    # b[] holds the board state.\n    # tup[] holds 4 board positions that are in a row.\n    # I is one of those positions.\n    # Would placing a color piece on I, result in 4 in a row?\n    for x in tup:\n        if (not ((b[x] == color) or (b[x] == 0))):\n            return False\n        if (not ((b[x] == color) or (x == I))):\n            return False\n    return True  \n\n@njit()\ndef winning_move_jit(b, tupCounts, spaceTups, x_in, color_in):\n    # Would placing a color_in piece on I, result in 4 in a row?\n    # b[] holds the board state.\n    # tupCounts[] and spaceTups[,,] are lookup tables to help examine\n    # different groups of 4 positions in a row.\n    tup = np.zeros(4, np.int32)\n    x = int(x_in)\n    color = int(color_in)\n    for n in range(tupCounts[x]):# for each tup\n        for z in range(4):\n            tup[z] = spaceTups [x, n, z]\n        if (win_for_tup_jit(b, tup, x, color)):\n            return(True)\n    return False   \n\n@njit()\ndef lowest_empty_row_jit(b, j):\n    # Return the row for a stone placed in column j.\n    # (My coordinates are upside down wrt the contest. )\n    r = 6\n    c = 7\n    for i in range(r): # rows\n        x = (i * c) + j\n        if (b[x] == 0):\n            return i\n        # Paronoid check to avoid race conditions if Numba makes this parallel.\n        #if ((b[x] == 0) and (i == 0)):\n        #    return i\n        #if ((b[x] == 0) and (i > 0) and (b[x - c] != 0)):\n        #    return i\n    return(r)\n\n@njit()\ndef get_sensible_moves_jit(b, tupCounts, spaceTups, color, otherColor):\n    # Return a list of moves worth considering, plus the status of the board.\n    # Is there a winning or forced move? Return a list of length one.\n    # Also return a status flag.\n    # status 0 : tie game, no legal moves\n    # status 1 : ongoing game\n    # status 2 : Color will win by playing the (single) move in obviousMoves[]\n    r = 6 # number of rows\n    c = 7 # number of columns\n    N = 42 # number of board spaces\n    legalMoves = [np.int32(x) for x in range(0)] # weird hack so numba can guess what type is in the list\n    for j in range(c):\n        i = lowest_empty_row_jit(b, j)\n        if (i < r): # legal move\n            x = (i * c) + j\n            if (winning_move_jit(b, tupCounts, spaceTups, x, color)):\n                #print(\"win\")\n                obviousMoves = [x]\n                return (obviousMoves, 2)\n            legalMoves.append(x)\n    \n    if (len(legalMoves) == 0):\n        return (legalMoves, 0)  # tie game\n    \n    if (len(legalMoves) == 1):\n        return (legalMoves, 1)  # ongoing game\n    \n    for x in legalMoves:\n        if (winning_move_jit(b, tupCounts, spaceTups, x, otherColor)):\n            #print(\"forced\")\n            obviousMoves = [x]\n            return (obviousMoves, 1)\n    \n    lm = legalMoves.copy()\n    for x in lm:\n        if (x + c < N):\n            b[x] = color # temporarily place stone here\n            if ((len(legalMoves) > 1) and (winning_move_jit(b, tupCounts, spaceTups, x + c, otherColor))):\n                legalMoves.remove(x)\n            b[x] = 0 # remove temporarily stone\n    return (legalMoves, 1)  # no obvious move\n\n\n@njit()\ndef friendly_tupple_jit(b, tup, x, color):\n    # tup[] holds 4 board positions that are in a row.\n    # x is one of those positions.\n    # If there are no othercolor pieces in tup, how many color pieces are there?\n    count = np.int32(0)\n    nope = False\n    if (b[x] != 0):\n        return (-1)\n    for i in range(4):\n        z = tup[i]\n        if ((b[z] != color) and (b[z] != 0)):\n            nope = True\n        if (b[z] == color):\n            count += 1\n    if (nope): return(-1)\n    return count\n\n@njit()\ndef calcMoveFeatures_jit(b, tupCounts, spaceTups, x, color):\n    # calculate features for a move at position x and return in feat[] \n    # tupCounts[] and spaceTups[,,] are lookup tables to help examine\n    # different groups of 4 positions in a row.\n    c = 7\n    r = 6\n    feat = np.zeros(22, np.int32)\n    tup = np.zeros(4, np.int32)\n    #self.feat.fill(0)\n    otherColor = 2\n    if (color == 2):\n        otherColor = 1\n    i = x \/\/ c\n    j =  x % c\n    #feat = feat * 0  # clear feat[]\n    feat[0] = min(j, (c - 1) - j)  # distance from edge\n    tups = tupCounts[x]\n    for n in range(tups):\n        #tup = self.spaceTups [x, n, :]\n        for z in range(4): \n            tup[z] = spaceTups [x, n, z]\n        count = friendly_tupple_jit(b, tup, x, color)\n        if (count > -1):\n            feat[count + 1] += 1\n        count = friendly_tupple_jit(b, tup, x, otherColor)\n        if (count > 0):\n            feat[count + 4] += 1\n            \n    if (i >= r - 1):\n        return feat # we're on the top row, so leave all other features at zero\n    I = i + 1 # looking at space above x\n    b[x] = color  # temporarily put friendly stone on space x\n    xp = (I * c) + j\n    tups = tupCounts[xp]\n    for n in range(tups):\n        #tup = self.spaceTups [xp, n, :]\n        for z in range(4):\n            tup[z] = spaceTups [xp, n, z]\n        count = friendly_tupple_jit(b, tup, xp, color)\n        if (count > -1):\n            feat[count + 8] += 1\n        count = friendly_tupple_jit(b, tup, xp, otherColor)\n        if (count > 0):\n            feat[count + 11] += 1\n        \n    b[x] = 0  # remove friendly stone from space x\n    \n    if (i >= r - 2):\n        return feat # we're on the next-to top row, so leave all other features at zero \n    I = i + 2 # looking at space above x\n    b[x] = color  # temporarily put friendly stone on space x\n    b[xp] = otherColor  # temporarily put enemy stone on space xp\n    xpp = (I * c) + j\n    tups = tupCounts[xpp]\n    for n in range(tups):\n        #tup = self.spaceTups [xpp, n, :]\n        for z in range(4):\n            tup[z] = spaceTups [xpp, n, z]\n        count = friendly_tupple_jit(b, tup, xpp, color)\n        if (count > -1):\n            feat[count + 15] += 1\n        count = friendly_tupple_jit(b, tup, xpp, otherColor)\n        if (count > 0):\n            feat[count + 18] += 1\n    \n    b[x] = 0  # remove friendly stone from space x\n    b[xp] = 0  # remove enemy stone from space xp\n    return feat\n    \n@njit()          \ndef calc_meta_features_jit(feat, x):\n    #calculate meta-features for a move at position x and return in metaFeat[]\n    \n    # all binary features\n    #metaFeat = metaFeat * 0  # clear metaFeat[]\n    metaFeat = np.zeros((4 + 6 + (21 * 3)), np.int32)\n    #self.metaFeat .fill(0)\n    c = 7\n    i = x \/\/ c\n    n = 0\n    y = feat [0] # distance from edge -> 4 possibilities, 4 'binary' variables\n    metaFeat [y] = 1 # only 1 can be non-zero\n    n += 4\n    # row -> 6 (essentially boolean) parameters\n    metaFeat [n + i] = 1 # only 1 can be non-zero\n    n += 6\n    for f in range(1, len(feat)):\n        if (feat[f] == 0):\n            metaFeat[n] = 1\n        elif (feat[f] == 1):\n            metaFeat[n + 1] = 1\n        elif (feat[f] > 1):\n            metaFeat[n + 2] = 1\n        n += 3\n    return metaFeat\n\n@njit()\ndef linear_move_scores_jit(b, tupCounts, spaceTups, wts, moves, color):\n    # For every move in the list, calculate a score and return them in scores[]\n    # moves[] holds the list of moves.\n    # b[] holds the board state.\n    # wts[] holds the linear weights for the features.\n    # tupCounts[] and spaceTups[,,] are lookup tables to help examine\n    # different groups of 4 positions in a row.\n    #min_score = 0.05\n    scores = [np.float64(x) for x in range(0)] # weird hack so numba can guess what type belongs in the list\n    total = 0.0\n    for i in prange(len(moves)):\n        x = moves[i]\n        feat = calcMoveFeatures_jit(b, tupCounts, spaceTups, x, color) # fills feat\n        metaFeat = calc_meta_features_jit(feat, x)         # fills metafeat\n        score = np.sum (metaFeat * wts) \n        scores.append(score)\n        total += score\n    #scores = scores \/ np.sum(scores)\n    for i in range(len(scores)):\n        scores[i] \/= total\n    #for sc in scores:\n    #    if (sc < min_score): sc = min_score\n    return scores\n\n\n@njit()  \ndef choose_linear_move_jit(b, tupCounts, spaceTups, wts, color):\n    # Get the sensible_moves[], score them, return the move with the highest score.\n    # score is a weighted sum of features.\n    #print(\"choose_linear_move_jit start\")\n    otherColor = 2\n    if (color == 2):\n        otherColor = 1\n    sensible_moves, status = get_sensible_moves_jit(b, tupCounts, spaceTups, color, otherColor) # also sets self.status\n    #print(\"choose_linear_move_jit calculated\")\n    if (len(sensible_moves) == 1):   # If it's a win, there will only be one move, so it returns w\/ correct status.\n        return(sensible_moves[0], status)\n    if (len(sensible_moves) == 0):\n        return(-1, 0)   # tie  \n    scores = linear_move_scores_jit(b, tupCounts, spaceTups, wts, np.array(sensible_moves), color)\n    \n    #print(\"choose_linear_move_jit calculated\")\n    x = sensible_moves[int(np.argmax(np.array(scores)))]\n    #print(sensible_moves)\n    #print(scores)\n    #print(x)\n    return (x, 1)\n\n\nclass GAME_MANAGER():\n    \n    # This class holds the board, the weights for move features and some lookup tables.\n    # It has a method that calculates the tables.\n    # Note: \"tupples\" doesn't mean Python tupples.\n    \n    def __init__(self):\n        self.c = 7\n        self.r = 6\n        self.N = self.c * self.r\n        self.K = 4\n        self.b = np.zeros((self.N), np.int32)\n        self.tupCounts = np.zeros((self.N), np.int32) # how many tupples contain this space\n        self.spaceTups = np.zeros((self.N, 16, self.K), np.int32)# all the tupples \n        self.feat = np.zeros(22, np.int32)\n        self.metaFeat = np.zeros((4 + 6 + (21 * 3)), np.int32)\n        self.precalcTups()  # calculates tupCounts[] and spaceTups[,,]\n        self.wts = np.array([\\\n0, 3.30366111407672, 6.48699261816764, 16.0570530870234, \n0, 6.89154554317436, 12.2291091073301, 14.9610249625214, \n8.2764687731129, 5.36422024027117, 14.3516002179586, 2.1021183007845, \n0, 0.719857647774551, 2.34487198327812, 11.2057343305379, \n0, 27.8825518166704, 43.8228479471015, 0, \n1.99103479234425, 0.726840470816211, 0.560352057248385, 4.22057034862125, \n6.83487190573809, 0, 17.491336765736, 35.714660516821, \n0, 5.87818176004432, 39.6996173910764, 0, \n3.6234074451734, 4.49879127350228, 0.806205543215103, 1.71224017973768, \n3.8749091066669, 2.53500463742859, 4.460882129649, 1.37971933373037, \n44.3252027404174, 29.7283585631117, 0, 0, \n2.19363385696125, 1.47116517810174, 11.3140778960712, 2.27101838697622, \n0, 1.74611338807736, 6.51927915172379, 0, \n2.94225634035759, 0, 3.50453812682987, 0, \n0.267382395928564, 1.98693848260618, 0, 1.58478060332119, \n4.71565983853622, 0, 17.8551290536231, 7.25762646495066, \n5.7077735522044, 1.10042450222865, 0, 8.22587152564711, \n4.70559492215376, 0.324579480451124, 16.3108479710915, 8.11547320076288, \n0, ]) \n        \n    \n    def precalcTups(self):\n        # tupCounts[] and spaceTups[,,] are lookup tables to help examine\n        # different groups of 4 positions in a row.\n        #tupCounts *= 0\n        # horizontal\n        tup = np.zeros((self.K), np.int32)\n        for j in range (0, (self.c - self.K) + 1):\n            for i in range(self.r):\n                # fill tup[]\n                for h in range(self.K):\n                    tup[h] = (i * self.c) + j + h\n                for h in range(self.K):\n                    x = tup [h]\n                    n = self.tupCounts [x]\n                    for z in range(self.K):\n                        self.spaceTups [x, n, z] = tup [z]\n                    self.tupCounts [x] = n + 1\n        # vertical\n        for j in range (self.c):\n            for i in range(0, (self.r - self.K) + 1):\n                # fill tup[]\n                for h in range(self.K):\n                    tup[h] = ((i + h) * self.c) + j\n                for h in range(self.K):\n                    x = tup [h]\n                    n = self.tupCounts [x]\n                    for z in range(self.K):\n                        self.spaceTups [x, n, z] = tup [z]\n                    self.tupCounts [x] = n + 1\n        # diagonal up, up\n        for j in range (0, (self.c - self.K) + 1):\n            for i in range(0, (self.r - self.K) + 1):\n                # fill tup[]\n                for h in range(self.K):\n                    tup[h] = ((i + h) * self.c) + j + h\n                for h in range(self.K):\n                    x = tup [h]\n                    n = self.tupCounts [x]\n                    for z in range(self.K):\n                        self.spaceTups [x, n, z] = tup [z]\n                    self.tupCounts [x] = n + 1\n        # diagonal something, something...\n        for j in range (0, (self.c - self.K) + 1):\n            for i in range(self.r-1, self.r - self.K, -1):\n                # fill tup[]\n                for h in range(self.K):\n                    tup[h] = ((i - h) * self.c) + j + h\n                for h in range(self.K):\n                    x = tup [h]\n                    n = self.tupCounts [x]\n                    for z in range(self.K):\n                        self.spaceTups [x, n, z] = tup [z]\n                    self.tupCounts [x] = n + 1\n        #print(\"precalcTups\",self.tupCounts)\n        return\n     \n    #@njit()       \n    def reset_b(self, B):\n        for i in range(len(B)):\n            self.b[i] = B[i]\n\n\n# This class uses all moves as first (AMAF) which is similar to RAVE in MCTS\n# There are 42 positions on the board. \n# raveS[] holds the score for each position.\n# raveV[] holds the number of visits for each position.\n# This class holds methods for UCT-like move selection using RAVE plus policy scores.\nclass AMAF():\n    def __init__(self, N):\n        self.N = N\n        self.raveS = np.zeros (N, int)  # myColor AMAF\n        self.raveV = np.zeros (N, int)\n        \n    def clearit(self):\n        self.raveS.fill(0)\n        self.raveV.fill(0)\n   \n    def amaf_scores(self, moves):\n        #return amaf scores for moves \n        scores = []\n        for x in moves:\n            scores.append( self.raveS[x] \/ max(1, self.raveV[x]) )\n        return(scores)\n    \n    def PUCT_scores(self, Cpuct, moves, scores):\n        if (len(moves) == 0) : print(\"PUCT_scores BADNESS! length 0\")\n        if (len(scores) == 0) : print(\"scores PUCT_scores BADNESS! length 0\")\n        encounters = 0\n        pscores = scores.copy()\n        for x in moves:\n            encounters += self.raveV[x]\n        if (encounters == 0):\n            return (pscores)\n        for k in range(len(moves)):\n            x = moves[k]\n            u = self.raveS[x] \/ max(1, self.raveV[x])\n            u += scores[k] * Cpuct * sqrt(encounters) \/ (self.raveV[x] + 1)\n            pscores[k] = u\n        return(pscores)\n    \n    def PUCT_no_policy(self, Cpuct, moves):\n        # Returns a move using UCT-like algorithm.\n        # Used when no policy scores are avaulable.\n        # moves[] holds the moves to be considered.\n        if (len(moves) == 0) : print(\"PUCT_no_policy BADNESS! length 0\")\n        encounters = 0\n        best_score = 0\n        best = moves[0]\n        for x in moves:\n            encounters += self.raveV[x]\n        if (encounters == 0):\n            return (choice(moves))\n        for x in moves:\n            u = self.raveS[x] \/ max(1, self.raveV[x])\n            if (u == 2):\n                return(x)  # perfect score, so why explore\n            u += (1.0 \/len(moves) ) * Cpuct * sqrt(encounters) \/ (self.raveV[x] + 1)\n            if (u >= best_score):\n                best_score = u\n                best = x\n        #print(\"amaf_puct\", x)\n        return(best)\n    \n    def PUCT(self, Cpuct, moves, scores):\n        # Returns a move using an UCT-like algorithm.\n        # scores[] holds the policy scores.\n        # moves[] holds the moves to be considered.\n        if (len(moves) == 0) : print(\"PUCT BADNESS! length 0\")\n        if (len(scores) == 0) : print(\"scores PUCT BADNESS! length 0\")\n        encounters = 0\n        best_score = 0\n        best = moves[0]\n        for x in moves:\n            encounters += self.raveV[x]\n        if (encounters == 0):\n            return (choice(moves))\n        for k in range(len(moves)):\n            x = moves[k]\n            u = self.raveS[x] \/ max(1, self.raveV[x])\n            if (u == 2):\n                return(x)  # perfect score, so why explore\n            u += scores[k] * Cpuct * sqrt(encounters) \/ (self.raveV[x] + 1)\n            if (u >= best_score):\n                best_score = u\n                best = x\n        #print(\"amaf_puct\", x)\n        return(best)\n    \n    \n    def reinforce(self, moves, reward):\n        # moves holds a list of all the moves (for this color) in one playout.\n        for k in moves:\n            self.raveV[k] += 1  \n            self.raveS[k] += reward\n\n\nclass BRAIN():\n    # This class chooses a move using MonteCarlo with adaptive playouts.\n\n    def __init__(self):\n        self.gm = GAME_MANAGER() # holds the board and some tables\n        self.colorAMAF = AMAF(self.gm.N) # AMAF object for color moves in playouts\n        self.otherAMAF = AMAF(self.gm.N) # AMAF object for otherColor moves in playouts\n        \n     \n    def brain_linear_move(self, b, color, V = False):\n        # Pick a move just using heuristics. (Not called)\n        ts = time.time()\n        self.gm.reset_b(b)\n        otherColor = 1 + (2 - color)\n        moves, status = get_sensible_moves_jit(self.gm.b, self.gm.tupCounts, self.gm.spaceTups, color, otherColor)\n        linear_move_scores_jit(self.gm.b, self.gm.tupCounts, self.gm.spaceTups, self.gm.wts, np.array(moves), otherColor) \n       \n        x, status = choose_linear_move_jit(self.gm.b, self.gm.tupCounts, self.gm.spaceTups, self.gm.wts, color)\n        if (V): print(\" JIT tm\",time.time() - ts)   \n   \n        return(x, status)\n    \n    \n    def clearAMAF(self):\n        self.colorAMAF.clearit()\n        self.otherAMAF.clearit()\n        \n            \n    def MC_adaptive_move(self, root, color, start, time_limit, POLICY):\n        # Choose a move using Monte Carlo playouts.\n        # First check that there's not only one real choice.\n        # Use all time available.\n        # Use adaptive playouts.\n        # Play move with most visits at the root\n        # Only reinforce discretionary moves.\n        # if (POLICY): calculate and use heuristic move scores ('priors') in playouts\n        \n        self.clearAMAF()\n        showit = True\n        \n        CpuctRoot = 7.0 # exploration term for root\n        Cpuct = 4.0     # exploration term for all other nodes\n        otherColor = 2\n        if (color == 2):\n            otherColor = 1\n        \n        self.gm.b = root.copy()   # Set b in game_manager (This is the root board state)\n        # Get sensible moves \n        legalMoves, status = get_sensible_moves_jit(self.gm.b, self.gm.tupCounts, self.gm.spaceTups, color, otherColor)\n        # If the move is obvious, we return it along with the status\n        if (status == 2):\n            print(\"win\")\n        #    return(legalMoves[0], status)\n        if (len(legalMoves) == 1):\n            print(\"forced move\")\n            return(legalMoves[0], status)\n        if (len(legalMoves) == 0):\n            print(\"tie\")\n            return(-1, 0)   # tie\n        \n        # There must be 2 or more sensible moves to choose from\n        use_policy = POLICY\n        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        # run MonteCarlo playouts for all 'sensible' moves\n        # \n        \n        rootMoves = legalMoves.copy()\n      \n        training_visits = np.zeros(7)\n        root_visits = np.zeros(len(rootMoves))\n        root_rewards = 0.0\n        if (use_policy):\n            root_scores = linear_move_scores_jit(self.gm.b, self.gm.tupCounts, self.gm.spaceTups, self.gm.wts, np.array(rootMoves), color) \n        else:\n            root_scores = np.ones(len(rootMoves))\n            root_scores = root_scores \/ np.sum(root_scores)\n        # run the playouts\n        end = time.time()\n        reps = 0 \n        \n        # Run as many playouts as time permits (within reason).\n        while ((reps == 0) or ((end-start < time_limit) and (reps < 30000))):\n            # Run a single Monte carlo playout - with adaptive playouts using AMAF and priors\n            use_policy = POLICY\n            reps += 1\n            self.gm.b = root.copy() # reset board to root state\n            movesColor = [] # moves made by color this playout\n            movesOther = [] # moves made by otherColor this playout\n            # make a move at the root, a color move\n            if (use_policy):\n                j = self.colorAMAF.PUCT(CpuctRoot, rootMoves, root_scores)\n            else:\n                j = self.colorAMAF.PUCT_no_policy(CpuctRoot, rootMoves)\n            training_visits[j % 7] += 1 # keep track of starting column\n            #print(\"root move\", j)\n            self.gm.b[j] = color\n            movesColor.append(j)\n            cnt = 1\n            status = 1\n            while (status == 1):\n                # ~~~~~~~~~~~~~~~~~~~ otherColor moves ~~~~~~~~~~~~~~~\n                moves, status = get_sensible_moves_jit(self.gm.b, self.gm.tupCounts, self.gm.spaceTups, otherColor, color)\n                if (status == 2):\n                    result = otherColor\n                    x = moves[0]\n                    self.gm.b[x] = otherColor\n                    movesOther.append(x)\n                if (status == 0):\n                    result = 0\n                if (status == 1):\n                    if (len(moves) == 1):\n                        x = moves[0]\n                        #movesOther.append(x) #sort of a forced move so better not to use it\n                    else:\n                        if (use_policy):\n                            scores = linear_move_scores_jit(self.gm.b, self.gm.tupCounts, self.gm.spaceTups, self.gm.wts, np.array(moves), otherColor) \n                            x = self.otherAMAF.PUCT(Cpuct, moves, scores)\n                        else:\n                            x = self.otherAMAF.PUCT_no_policy(Cpuct, moves)\n                        movesOther.append(x)    # use this move to reinforce RAVE\n                    self.gm.b[x] = otherColor\n                    cnt += 1\n                    # ~~~~~~~~~~~~~~~~~~~ color moves ~~~~~~~~~~~~~~~\n                    moves, status = get_sensible_moves_jit(self.gm.b, self.gm.tupCounts, self.gm.spaceTups, color, otherColor)\n                    if (status == 2):\n                        result = color\n                        x = moves[0]\n                        self.gm.b[x] = color\n                        movesColor.append(x)\n                    if (status == 0):\n                        result = 0\n                    if (status == 1):\n                        if (len(moves) == 1):\n                            x = moves[0]\n                            #movesColor.append(x) #sort of a forced move so better not to use it\n                        else:\n                            if (use_policy):\n                                scores = linear_move_scores_jit(self.gm.b, self.gm.tupCounts, self.gm.spaceTups, self.gm.wts, np.array(moves), color) \n                                x = self.colorAMAF.PUCT(Cpuct, moves, scores)\n                            else:\n                                x = self.colorAMAF.PUCT_no_policy(Cpuct, moves)\n                            movesColor.append(x)\n                        self.gm.b[x] = color\n                        cnt += 1\n           \n            reward = 0\n            if (result == color):\n                reward = 2\n            elif (result == 0):\n                reward = 1\n            root_rewards += reward\n            # reinforce rave (AMAF)\n            self.colorAMAF.reinforce(movesColor, reward)\n            self.otherAMAF.reinforce(movesOther, 2 - reward)\n            end = time.time()\n            if (end-start > time_limit):\n                break\n            \n        #print(scores)\n        end = time.time()\n        #print(\"root_scores\", root_scores)\n        for k in range(len(rootMoves)):   # for display\/diagnostics\n            x = rootMoves[k]\n            root_scores[k] = 100*self.colorAMAF.raveS[x] \/ max(1, self.colorAMAF.raveV[x])\n        #print(\"root_scores\", root_scores)\n        j = int(np.argmax(training_visits)) # the column\n        i = lowest_empty_row_jit(root, j) # the row\n        x = j + i * self.gm.c\n        root_rewards \/= reps  # average reward for color\n        #print (root_visits, 'reps', reps, end-start, x % 7)\n        #print (training_visits, 'reps', reps, end-start, x % 7)\n        # end of Monte Carlo block\n        #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n        #b = root.copy() # reset board to root state\n        if (showit): \n            print (root_visits, \"root_rewards\", root_rewards, 'stone_count', np.sum(root != 0), 'playouts', reps, \"***\")\n            #net_move_scores   (b, rootMoves, color, otherColor, showit = True)\n        \n        status = 1 # ongoing game\n        # return column and status\n        return (x, status)  \n   \nbrain = BRAIN() # The object that holds everything\n\n\ndef mirror_top_to_bottom(B):\n   # Change board to my (upside down) internal coordinates \n    b = np.zeros((brain.gm.N), int)\n    for i in range(6): # rows of b\n        for j in range(7):\n            x = (i * 7) + j\n            mx = ((5 - i) * 7) + j # mirrored top to bottom\n            b[mx] = B[x]\n    return(b)    \n\n\n    \ndef my_agent(observation, configuration):\n    # calls brain to make a move \n    bail_early = False\n    start = time.time()\n    #The amount of thinking time per move. Add small margin.\n    time_limit = configuration.actTimeout - 0.25\n    \n    # My coordinates are upside down, so I must flip the input\n    b = mirror_top_to_bottom(np.array(observation.board))\n    \n    \n    stone_count = np.sum(b != 0)\n    if (stone_count <= 1): time_limit = configuration.agentTimeout \/ 2\n    color = observation.mark \n    \n    \n    if (stone_count <= 1): \n       print('time_limit', time_limit)\n        \n        \n    rootMove, status = brain.MC_adaptive_move(b, color, start, time_limit, POLICY = True)\n    \n    \n    #x, status = brain.brain_linear_move(b, color) just uses heuristics, no search\n    if (bail_early and ((status == 2) or (stone_count > 40))): \n        print('About to win (or tie); cant have that, bailing')\n        return(-1)   # Cause an error so as to fail validation and not waste a submission slot.\n    \n    x = rootMove # x is the board position (one of 42)\n    x = x % brain.gm.c  # We must return the column\n    \n    \n    print('time_limit', time_limit, \"my duration\",time.time() - start, 'x', x)\n    return(x)\n\"\"\"\n","0ebe044c":"agent_file = open(\"submission.py\", \"w\")\nagent_file.write(agent_as_a_string)\nagent_file.close()","5f087f4c":"My best agent uses adaptive playouts, Monte Carlo tree search (MCTS), and an opening table. (As of this writing, it scores 1465 and is in third place.) Rather than publish that and risk ruining people's fun, I'm sharing an agent with just the most interesting part, the adaptive playouts. (It scores 1240 which would be about 20th place.) Hopefully, that's more useful anyway.\n\nMonte Carlo with adaptive playouts, is sort of like MCTS, except it doesn't build a tree or table, and it treats every move in the playout as if it were an internal node. What follows is a rough description, some discussion, and finally the code for a working agent.\n\nLet's start by considering a plain vanilla Monte Carlo agent. On the agent's turn, it plays as many random games (called 'playouts') from the root as time permits. After every playout, it calculates a reward for the agent (based on win, lose, or tie), and assigns that to the first position played during that playout. Finally, it finds the position with the highest average reward and returns that position as the agent's move.\n\nWe can make a stronger agent by having it make stronger moves in the playouts. We'll start by limiting the random moves to \u201csensible\u201d moves. If the player can win in one move, that's the only sensible move for that board state. If a move would permit the other player to immediately win, that isn't a \u201csensible\u201d move, assuming there are any other moves available. Testing for sensible moves requires considerably more computation than simply testing for legality. That means the agent will complete fewer playouts in the allotted time. But the agent still plays better because the playouts are better matched to the domain. The agent is stronger per playout, but more importantly, it is also stronger per second of thinking time.\n\nNow we introduce the all moves as first (AMAF) heuristic. At the end of a playout, the first position played receives the reward, but every other position played by that color receives the reward as well. And we calculate the average reward for the positions played by the other color too. For those familiar with MCTS, AMAF is essentially the same as RAVE. Since we aren't building a tree, it is RAVE from the root: a separate RAVE variable for each color.\n\nThe playouts become adaptive when we use the AMAF values to pick moves during the playouts. Much as with MCTS, we'll incorporate an UCT-like term to encourage exploration in early playouts, but steer later playouts to pick moves with higher AMAF scores.\n\nThe final element is to calculate a prior score for each position in the playouts. (This agent uses heuristics, but a policy net would work too.) So, for every move in a playout, the agent calculates prior scores for every sensible move for that board state. With this addition, the moves in early playouts are biased toward exploration but also toward positions with higher prior scores. As with MCTS, the move returned is the position that was most visited at the root.\n\nLike most Monte Carlo agents, this one plays a relatively weak early game. (Another way to say that is that it needs more time to find good moves in the early game. But this contest enforces a uniform time limit for every move.) The most efficient way to make this agent stronger is to add an opening table and bypass the early game entirely. It's also stronger when combined with an actual MCTS algorithm, but that's a smaller gain and requires more effort."}}