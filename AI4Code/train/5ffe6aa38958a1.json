{"cell_type":{"438fa5fe":"code","7f61bd79":"code","43345376":"code","c94a92b1":"code","de444b36":"code","fd0da621":"code","5750791e":"code","e606f357":"code","ef3a0327":"code","6dff4f5c":"code","31038cc8":"code","8820f6d4":"code","ad50cf77":"code","e98b68bf":"code","b08b33ab":"code","18e09201":"code","7d34b66b":"code","78ba260a":"code","93985151":"code","96c0cdba":"code","3c3b53f2":"code","68279cf0":"code","ce28316c":"code","15adc097":"code","ae24b262":"code","1b03c43a":"code","a50bf3b6":"code","9243f6f1":"code","aaa2cf0b":"code","cdd9a435":"code","2c451f55":"code","154864b6":"code","5c532ad9":"code","777220b4":"code","98aefe0c":"code","8647537e":"code","f2526573":"code","21a1364d":"code","9f9fae2f":"code","f5d17205":"code","1819b792":"markdown","58cce6ea":"markdown","27691470":"markdown","45c0a5f2":"markdown","1bd39750":"markdown","ba9badec":"markdown","c43c4a7d":"markdown","9cce7cdb":"markdown","299e96e8":"markdown","805c7a64":"markdown","1314b0f8":"markdown","80e4dbd0":"markdown","5ca01a27":"markdown","7ed763f6":"markdown","993089cf":"markdown","ca7a8620":"markdown","d17b8239":"markdown","7484e458":"markdown","255d3658":"markdown","fc701a6d":"markdown","3203a835":"markdown","ef0ded79":"markdown","43868cd9":"markdown","c3f7b2bc":"markdown","c34bc529":"markdown","06677815":"markdown","fc6f3d3c":"markdown","20396df6":"markdown","947af3c3":"markdown","3f117a46":"markdown","436cc27a":"markdown","823aed43":"markdown","38402096":"markdown","42894d20":"markdown","f9b37114":"markdown","987e7f13":"markdown","f4c8386a":"markdown","6b8dbb0a":"markdown","779b8a73":"markdown","7380df45":"markdown"},"source":{"438fa5fe":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n\n# Modelling Algorithms\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\n\n# Modelling Helpers\nfrom sklearn.preprocessing import Imputer , Normalizer , scale , normalize\nfrom sklearn.model_selection import train_test_split , StratifiedKFold\n#from sklearn.feature_selection import RFECV\n","7f61bd79":"# Helper functions \n# Reference: https:\/\/www.kaggle.com\/helgejo\/an-interactive-data-science-tutorial\n\ndef plot_distribution( df , var , target , **kwargs ):\n    row = kwargs.get( 'row' , None )\n    col = kwargs.get( 'col' , None )\n    facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col )\n    facet.map( sns.kdeplot , var , shade= True )\n    facet.set( xlim=( 0 , df[ var ].max() ) )\n    facet.add_legend()\n\ndef plot_categories( df , cat , target , **kwargs ):\n    row = kwargs.get( 'row' , None )\n    col = kwargs.get( 'col' , None )\n    facet = sns.FacetGrid( df , row = row , col = col )\n    facet.map( sns.barplot , cat , target )\n    facet.add_legend()\n\ndef plot_correlation_map( df ):\n    corr = df.corr()\n    _ , ax = plt.subplots( figsize =( 8 , 7 ) )\n    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n    _ = sns.heatmap(\n        corr, \n        cmap = cmap,\n        square=True, \n        cbar_kws={ 'shrink' : 0.5}, \n        ax=ax, \n        annot = True, \n        annot_kws = { 'fontsize' : 11 }\n    )\n\n","43345376":"\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c94a92b1":"titanic_train=pd.read_csv('..\/input\/train.csv')\ntitanic_test=pd.read_csv('..\/input\/test.csv')","de444b36":"print('Size of training data:  rows:{}, cols:{}'.format(titanic_train.shape[0], titanic_train.shape[1]))\nprint('Size of test data:      rows:{}, cols:{}'.format(titanic_test.shape[0], titanic_test.shape[1]))\nprint('')\n","fd0da621":"titanic_train_header = list(titanic_train)\ntitanic_test_header = list(titanic_test)\n\nprint('Train Header:', titanic_train_header)\nprint('Difference between train and test headers: ', [item for item in titanic_train_header if item not in titanic_test_header ])","5750791e":"titanic_train.head()","e606f357":"titanic_train.sample(10)","ef3a0327":"titanic_train.describe()","6dff4f5c":"# Check how many unique values exist\ntitanic_train['Sex'].unique()","31038cc8":"titanic_train[\"SexBin\"] = (titanic_train.Sex == 'female').astype(int)\ntitanic_train.describe()","8820f6d4":"corr = titanic_train.corr()\ncmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n_ = sns.heatmap(corr, \n            cmap=cmap, \n            annot=True)","ad50cf77":"plot_correlation_map(titanic_train)","e98b68bf":"#plot_distribution( titanic_train , var = 'Age' , target = 'Survived' , row = 'Sex' )\nfacet = sns.FacetGrid(titanic_train, col=\"Sex\", row='Survived')\n#_ = facet.map(sns.kdeplot , 'Age' , shade= True )\nfacet.map(plt.hist, \"Age\", density=True)","b08b33ab":"df_female = titanic_train.query('SexBin == 1')\nplot_correlation_map(df_female)","18e09201":"df_male = titanic_train.query('SexBin == 0')\nplot_correlation_map(df_male)","7d34b66b":"# Plot survival rate by Embarked\nplot_categories( titanic_train , cat = 'Embarked' , target = 'Survived' )\nplot_categories( df_male , cat = 'Embarked' , target = 'Survived' )\nplot_categories( df_female , cat = 'Embarked' , target = 'Survived' )","78ba260a":"plot_categories( titanic_train , cat = 'Pclass' , target = 'Survived' )\nplot_categories( df_male , cat = 'Pclass' , target = 'Survived' )\nplot_categories( df_female , cat = 'Pclass' , target = 'Survived' )","93985151":"full_data = pd.concat([titanic_train.drop('Survived', axis=1).drop('SexBin', axis=1), titanic_test], axis = 0, sort=False) \nfull_data.shape\nfull_data.sample(5)","96c0cdba":"sex = (full_data.Sex == 'female').astype(int) # Male = 0, Female = 1","3c3b53f2":"# Create a new variable for every unique value of Embarked\n#embarked = pd.get_dummies( full_data.Embarked , prefix='Embarked' ).idxmax(1)\nembarked_list = full_data.Embarked.unique()\nembarked_dict = pd.Series(range(len(embarked_list)), embarked_list)\nprint(embarked_dict)\nembarked = pd.get_dummies( full_data.Embarked).idxmax(1)\nembarked_id = embarked.map(embarked_dict).rename('Embarked').to_frame()\nembarked.head()","68279cf0":"cabin_num = full_data.Cabin.fillna('U')\ncabin_num.head()","ce28316c":"cabin  = cabin_num.str[0]\ncabin_list = cabin.unique()\ncabin_dict = pd.Series(range(len(cabin_list)), cabin_list)\nprint(cabin_dict)\ncabin_id = pd.get_dummies( cabin ).idxmax(1)\ncabin_id = cabin_id.map(cabin_dict).rename('Cabin').to_frame()\ncabin_id.sample(5)","15adc097":"full_data.Age = full_data.Age.fillna(full_data.Age.mean())\nfull_data.Fare = full_data.Fare.fillna(full_data.Fare.mean())\nfull_data.isna().sum()","ae24b262":"# Reference: https:\/\/medium.com\/dunder-data\/selecting-subsets-of-data-in-pandas-6fcd0170be9c\nage = full_data.Age\nage_sq = (age*age).rename('Age_sq')  # rename the series\nage_sex = ((sex*2-1)*age).rename('Age_sex')\n#embark_sex = ((sex*2-1)*embarked_id).rename('embark_sex')\nclass_sex = ((sex*2-1)*full_data.Pclass).rename('Class_sex')\n\nfull_x = pd.concat([full_data[['Pclass', 'Age', 'SibSp', 'Parch']], age_sq, sex, age_sex, class_sex, cabin_id, embarked_id], axis=1)\n\n\n# Min-max normalization\nfull_x = (full_x - full_x.min())\/(full_x.max()-full_x.min())\n#full_x.Sex = full_x.Sex*2 - 1\n\ntrain_valid_x = full_x[0:titanic_train.shape[0]]\ntrain_valid_y = titanic_train['Survived']\n\ntest_x = full_x[titanic_train.shape[0]:]\n\ntrain_valid_x.head()\n\n","1b03c43a":"train_x, valid_x, train_y, valid_y = train_test_split(train_valid_x, train_valid_y, train_size=0.8)","a50bf3b6":"model_lr = LogisticRegression()\nmodel_rf = RandomForestClassifier()\nmodel_dt = DecisionTreeClassifier()\nmodel_svc = LinearSVC()\nmodel_knn = KNeighborsClassifier()\n","9243f6f1":"model_lr.fit(train_x, train_y)\nmodel_rf.fit(train_x, train_y)  \nmodel_dt.fit(train_x, train_y)\nmodel_svc.fit(train_x, train_y)\nmodel_knn.fit(train_x, train_y)\n\n","aaa2cf0b":"print('LR Scores:  Training:{} \\t Valid:{}'.format(model_lr.score(train_x, train_y), model_lr.score(valid_x, valid_y)))\nprint('RF Scores:  Training:{} \\t Valid:{}'.format(model_rf.score(train_x, train_y), model_rf.score(valid_x, valid_y)))\nprint('DT Scores:  Training:{} \\t Valid:{}'.format(model_dt.score(train_x, train_y), model_dt.score(valid_x, valid_y)))\nprint('SVC Scores:  Training:{} \\t Valid:{}'.format(model_svc.score(train_x, train_y), model_svc.score(valid_x, valid_y)))\nprint('KNN Scores:  Training:{} \\t Valid:{}'.format(model_knn.score(train_x, train_y), model_knn.score(valid_x, valid_y)))","cdd9a435":"from sklearn.model_selection import cross_val_score\nscores_rf = cross_val_score(model_rf, train_valid_x, train_valid_y, cv=10)\nscores_knn = cross_val_score(model_knn, train_valid_x, train_valid_y, cv=10)\n\nprint('RF \\t Mean: {} Std:{}'.format(np.mean(scores_rf), np.std(scores_rf)))\nprint('KNN \\t Mean: {} Std:{}'.format(np.mean(scores_knn), np.std(scores_svc)))\n\n\n","2c451f55":"print(train_valid_x.columns.shape)\nprint(model_rf.feature_importances_)\n\nimportances = pd.DataFrame({'feature':train_valid_x.columns,'importance':np.round(model_rf.feature_importances_,3)})\nprint(importances.sort_values(by=['importance'], ascending=False))\n","154864b6":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(model_rf, train_valid_x, train_valid_y, cv=10)\nprint(' Confusion Matrix \\n', confusion_matrix(train_valid_y, predictions))\n\nprint('\\n Weight of training set:', np.sum(train_valid_y==1)\/len(train_valid_y))","5c532ad9":"#model_rf_improved = RandomForestClassifier()\nmodel_rf_improved = RandomForestClassifier(n_estimators=100, oob_score=True, random_state=42)\nprint('Weights:{}'.format(model_rf.class_weight))\n\nmodel_rf_improved.fit(train_valid_x, train_valid_y)\nprint('oob_score:{}'.format(model_rf_improved.oob_score_))\n\nscores_rf_improved = cross_val_score(model_rf_improved, train_valid_x, train_valid_y, cv=10)\n\nmin_estimators = 10\nmax_estimators = 200\n\nerror_rate = {}\n\nfor i in range(min_estimators, max_estimators + 1, 5):\n    model_rf_improved.set_params(n_estimators=i)\n    model_rf_improved.fit(train_valid_x, train_valid_y)\n\n    oob_error = 1 - model_rf_improved.oob_score_\n    error_rate[i] = oob_error\n    \n    #y_scores = model_rf_improved.predict_proba(train_valid_x)\n    #y_scores = y_scores[:,1]\n\n    #precision, recall, threshold = precision_recall_curve(train_valid_y, y_scores)\n","777220b4":"# Convert dictionary to a pandas series for easy plotting \noob_series = pd.Series(error_rate)\nfig, ax = plt.subplots(figsize=(10, 10))\n\noob_series.plot(kind='line',\n                color = 'red')\nplt.xlabel('n_estimators')\nplt.ylabel('OOB Error Rate')\nplt.title('OOB Error Rate Across various Forest sizes \\n(From 15 to 1000 trees)')","98aefe0c":"model_rf_improved = RandomForestClassifier(n_estimators=125, oob_score=True, random_state=42, warm_start=True)\nmodel_rf_improved.fit(train_valid_x, train_valid_y)","8647537e":"from sklearn.metrics import precision_recall_curve\n\n# getting the probabilities of our predictions\ny_scores = model_rf_improved.predict_proba(train_valid_x)\ny_scores = y_scores[:,1]\n\nprecision, recall, threshold = precision_recall_curve(train_valid_y, y_scores)","f2526573":"def plot_precision_and_recall(precision, recall, threshold):\n    plt.plot(threshold, precision[:-1], \"r-\", label=\"precision\", linewidth=5)\n    plt.plot(threshold, recall[:-1], \"b\", label=\"recall\", linewidth=5)\n    plt.xlabel(\"threshold\", fontsize=19)\n    plt.legend(loc=\"upper right\", fontsize=19)\n    plt.ylim([0, 1])\n\nplt.figure(figsize=(14, 7))\nplot_precision_and_recall(precision, recall, threshold)\nplt.show()","21a1364d":"def plot_precision_vs_recall(precision, recall):\n    plt.plot(recall, precision, \"g--\", linewidth=2.5)\n    plt.ylabel(\"recall\", fontsize=19)\n    plt.xlabel(\"precision\", fontsize=19)\n    plt.axis([0, 1.5, 0, 1.5])\n\nplt.figure(figsize=(14, 7))\nplot_precision_vs_recall(precision, recall)\nplt.show()","9f9fae2f":"test_data = full_data[titanic_train.shape[0]:]\n\nthreshold = 0.4\n\npredicted = model_rf_improved.predict_proba(test_x)\n#predicted [:,0] = (predicted [:,0] < threshold).astype('int')\n#predicted [:,1] = (predicted [:,1] >= threshold).astype('int')\n\nY_prediction_default = model_rf_improved.predict(test_x)\n\nY_prediction = (predicted[:, 0] <= threshold).astype('int')\n\n#print('Prediction probabilities which were changed by setting a different threshold')\n#print(predicted[Y_prediction!=Y_prediction_default])\n\n","f5d17205":"\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_data[\"PassengerId\"],\n        \"Survived\": Y_prediction\n    })\nsubmission.to_csv('submission.csv', index=False)","1819b792":"# 2.3 Data Cleanup & Feature Definition\n\nLets find if there is any missing data and if it is an error. \nAlso, convert non-numeric data into numeric. \n\nNote that this will be done for both training and test data because we need to perform similar operations on both data sets. We did not use test data for analysis. \n","58cce6ea":"\nWe can use Randomforest classifier and perform more advanced optimizations to further improve the accuracy\n\n## 5.1 Feature Importance\n\nFeature importance can be obtained in random forest which gives you an idea of how much weight each feature carries. This can be useful to eliminate less important or noisy features ","27691470":"# 1.  Objectives\n\n## 1.1 Learning to code ML project \nEntering some Kaggle competitions may be a good idea to get started. Things I am looking to learn: \n\n1. More systematic use of iPython notebooks\n2. Using pandas for python \n3. Data visualization\n4. Creating and running a model using existing frameworks \n5. Creating and running a model from scratch (extended objective)\n\n## 1.2 Learning how Kaggle works\nWell, I am completely new to everything here .. except with some background in private Jupyter notebooks and python and numpy. \n\n1. Learn my way around\n2. How to write a good Kernel\n3. How to import datasets, train and submit results\n","45c0a5f2":"# 5.3 Improving the model\n\nThere are a few tricks used here: \n\nn_estimators: complexity of random forest model \noob_score: this uses an OOB train and cross validation method to improve the training\nclass weight: since ourput dataset has more examples of class: 0 (not survived), we can adjust for it in the training of random forest\n\n  \nOOB score: This is a metric similar to cross validation score computed at training time","1bd39750":"It appears that embarking location has more correlation with Male passengers than with female passengers\n\n**Pclass**","ba9badec":"### 2.3.3  Cabin - Use as category","c43c4a7d":"## 2.2 Import Data\n\n","9cce7cdb":"Randomly sample some data\n","299e96e8":"### 2.3.4 Ticket Number - Use as category\n---- Not for now ---- ","805c7a64":"### 2.3.2 Embarked - numeric value dataframe\nNew dataframe with classess assigned to embarked .. Can this instead be a single feature with 3 possible values?","1314b0f8":"# 4.2 Training the model","80e4dbd0":"# 5. Random Forest: Advanced Optimizations","5ca01a27":"**Do we need to know what headers mean?**\nTo some extent - yes. The headers can be used to learn which features to use. For instance, it is likely that Name did not have significant influence on the survival rate? However, Sex is more likely to have had an influence. The same is true for Fare and Cabin information.  \n\nFirst analysis, non statistical - this can later be re-analyzed based on correlation between survival chance. \n\n*ID:*  \n  * PassengerId: Not feature\n\n*Ground Truth:*   \n  * Survived: \n\n*Feature: *\n  1. Pclass:   (Likely that class has an impact on survival rate)\n  2. Sex\n  3. Age\n  4. SibSp : Siblings or spouces on board\n  5. Parch: Parents\/children on board\n  6. Fare\n  7. Cabin: Having a cabin or not is important \n  8. Embarked: Port of embarkation\n\n*Further Engineering Maybe required*\n  1. Name: could be a feature as it may represent wealth or background in some cases. We should try this if result independent of this is not satisfactory\n  2. Ticket: Ticket number may have some clues .. ?\n","7ed763f6":"We will take the first letter from Cabin number and convert that into a feature. It turns out there are 8 different classess for cabins. ","993089cf":"### 2.3.5 Fill missing values\n\nAge: Mean(age)\n","ca7a8620":"# 5.5 Submitting our prediction \n\nFinally, we use the model to submit our prediction. One final thing we have used here is to choose a threshold value that is different from default. Based on precision\/recall curves, we choose a prediction probability of 0.3","d17b8239":"## 2.3 Analyzing the data  \n\nStatistical Characteristic: \nThis is important, because you can use this information to normalize the data, and also get insight into the data","7484e458":"## 4.4 K-Fold Cross Validation\n\nK-Fold cross validation splits the training set into K sub-sets (folds). K-iterations of training and validation are run while K-1 sets are chosen for training and 1 for validation each time. ","255d3658":"Survival rate of females seems to be higher as expected from the correlation plot","fc701a6d":"Obviously, the missing field was the test result. It exists in train data because that acts as the ground truth. \n\nOkay, so what does the data look like?  ","3203a835":"**Correlation between different features**\n\nThe following code draws a nice correlation plot. Ideally, this should go into a helper function. ","ef0ded79":"Plotting Distribution ( https:\/\/www.kaggle.com\/helgejo\/an-interactive-data-science-tutorial) ","43868cd9":"## 5.2 Confusion Matrix\n\nConfusion matrix gives an indication of which class is being misclassified more. \nThe matrix below suggests that there is some imbalance in the data","c3f7b2bc":"**Beginning with Kaggle**\n\n[ Reference: https:\/\/www.kaggle.com\/niklasdonges\/end-to-end-project-with-python#Random-Forest]\n\n\n\n","c34bc529":"## 3.1 Concatenate all features and normalize\nWe select followign features as important: \n1. Class \n2. Sex\n3. Age\n4. SibSp\n5. Parch\n6. Cabin\n7. Embarked\n\nLet's drop the unnecessary columns and append the cleaned-up features (Cabin, Embarked)\n\nnormalization can be done for both training and test sets here. Alternatively, it is possible to use the same normalizaton settings for training and test sets separately","06677815":"### 2.3.1 Sex - Numeric value\n\nNew series with numerical value of \"sex\"","fc6f3d3c":"What did we just import? ","20396df6":"# 3. Prepare data for training! ","947af3c3":"# 5.4 Precision and Recall\n","3f117a46":"Plotting again using the helper function. ","436cc27a":"This suggests that the threshold for determining survival in our model should be around 0.45","823aed43":"Sex is not listed here, because it is a string field. Let us try to make this a binary field","38402096":"Why do we have one less column in test?\n\n","42894d20":"**Embark Location**","f9b37114":"## 3.2 Split train, validation \n\n**scikit-learn**   \nscikit is an API that implements methods for various machine learning utilities. Primarily including: \n1. data pre-processing\n2. statistical methods (random forest, logistic regression, svm etc)\n\nWe will first use this for pre-processing steps such as dividing the data for training and validation, and normalization. ","987e7f13":"# 4. Model \n\n## 4.1 Model Selection\n### 4.1.1 Logistic Regression","f4c8386a":"## 4.3 Check prediction on validation set","6b8dbb0a":"**Pandas Dataframe Query** \nThere will be better ways to do this, but for now, I split the database into male and female to analyze the correlation matrix. ","779b8a73":"# 2. Getting Started \n\n## 2.1 Import Libraries","7380df45":"Many values of Cabin are undefined. This may be passengers who had no cabin. We can fill all unassigned values with 'U'"}}