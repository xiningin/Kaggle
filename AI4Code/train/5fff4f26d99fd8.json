{"cell_type":{"d170eb6e":"code","76477577":"code","803ffa04":"code","19ce1700":"code","d5989bae":"code","62ddb321":"code","c91dcb25":"code","de927011":"code","4ca6492b":"code","1ebf36d1":"code","b40a1f62":"code","7420e898":"code","7a033460":"code","12c2c2bf":"code","031e4e7b":"code","0d8ceac9":"code","f92ca2ce":"code","1f4bd78e":"code","f3aed9df":"code","96083618":"code","b768ad20":"code","b7c9b084":"code","665e4e77":"code","dd588187":"code","9606478c":"code","0a8a2621":"markdown","113c96a9":"markdown","ece62067":"markdown","84b99bb4":"markdown","e2436950":"markdown","502a39e0":"markdown","b144b2e5":"markdown","4912e09e":"markdown","9e9904fb":"markdown","4819541f":"markdown","52d4d3d8":"markdown","79b5078a":"markdown","165f37e6":"markdown","2958a692":"markdown","0a628956":"markdown","4e2dbd93":"markdown","4990cd55":"markdown","797ac437":"markdown","a6f0f925":"markdown","f26e0e2d":"markdown","91175bff":"markdown","52616787":"markdown"},"source":{"d170eb6e":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","76477577":"tf.__version__\ntf.keras.__version__","803ffa04":"mnist = tf.keras.datasets.mnist","19ce1700":"(X_train , y_train) , (X_test , y_test) = mnist.load_data()","d5989bae":"print(X_train.shape)\nprint(y_train.shape)\n\nprint(X_test.shape)\nprint(y_test.shape)","62ddb321":"img = X_train[0]\nplt.imshow(img , cmap=\"binary\")\nplt.title(f\"Actual number :{y_train[0]}\")\n# plt.axis(\"off\")\nplt.show()","c91dcb25":"plt.figure(figsize=(20,20))\nsns.heatmap(img\/255 , annot=True , cmap=\"binary\")\n# plt.grid(True)\nplt.axis(\"off\")\nplt.show()","de927011":"img.min() , img.max()","4ca6492b":"X_train_valid , X_train = X_train[:5000]\/255. , X_train[5000:]\/255.\ny_train_valid , y_train = y_train[:5000] , y_train[5000:]\n\nX_test = X_test\/255.","1ebf36d1":"print(X_train.shape)\nprint(y_train.shape)\n\nprint(X_train_valid.shape)\nprint(y_train_valid.shape)","b40a1f62":"LAYERS = [\n    \n    tf.keras.layers.Flatten(input_shape=[28,28] , name=\"InputLayer\"),\n    tf.keras.layers.Dense(300 , activation=\"relu\" , name=\"HiddenLayer1\"),\n    tf.keras.layers.Dense(100 , activation=\"relu\" , name=\"HiddenLayer2\"),\n    tf.keras.layers.Dense(10 , activation=\"softmax\" , name=\"OutputLayer\")\n\n    \n]","7420e898":"model_clf = tf.keras.models.Sequential(LAYERS)","7a033460":"model_clf.summary()","12c2c2bf":"weights , Biases = model_clf.layers[1].get_weights()","031e4e7b":"print(weights.shape)\nprint(Biases.shape)","0d8ceac9":"LOSS_FUNCTION = \"sparse_categorical_crossentropy\"\nOPTIMIZER = \"SGD\"\nMETRICS = [\"accuracy\"]\n\nmodel_clf.compile(loss=LOSS_FUNCTION , optimizer=OPTIMIZER , metrics=METRICS)","f92ca2ce":"EPOCHS = 30\nVALIDATION = (X_train_valid , y_train_valid)\n\nhistory = model_clf.fit(X_train , y_train , epochs=EPOCHS , validation_data=VALIDATION)","1f4bd78e":"history.params","f3aed9df":"history.history.keys()","96083618":"pd.DataFrame(history.history)","b768ad20":"pd.DataFrame(history.history).plot(figsize=(10,7))\nplt.grid(True)\nplt.show()","b7c9b084":"model_clf.evaluate(X_test,y_test)","665e4e77":"X_new = X_test[:3]\ny_prob = model_clf.predict(X_new)\ny_prob","dd588187":"y_pred = np.argmax(y_prob ,axis=-1)\ny_pred","9606478c":"for img , predicted , actual in zip(X_new , y_pred , y_test[:3]):\n    plt.imshow(img , cmap=\"binary\")\n    plt.title(f\"Actual :{actual} , Predicted :{predicted}\")\n    plt.show()\n    print(\"---\"*20)","0a8a2621":"<img src=\"https:\/\/miro.medium.com\/proxy\/1*pO5X2c28F1ysJhwnmPsy3Q.gif\">\n","113c96a9":"**ploting some above DataFarme to get clear picture of our parameters**","ece62067":"**InDepth visualization of the Image so we can understand how image pixels are divided**\n\n> ***img is divided with \/255 because to scale the image pixels between 0 to 1***","84b99bb4":"**Sample image of our Training sets**","e2436950":"**Parameters we get after training our model**","502a39e0":"**Loading the mnist dataset through the keras which is a high level API**","b144b2e5":"**Spliting the loaded data into**\n\n**Train(on which our going to be train) and**\n\n**Test(on which our model going to test) sets**","4912e09e":"**Assigning the Layers**","9e9904fb":"**Compilation of the model with specific loss function , optimizer , metrics**","4819541f":"<h2>I hope you liked the notebook and detailed explanation for above calculations do UPVOTE and share with your friends<\/h2>","52d4d3d8":"**Listing all the libraries require to train the mnist model**","79b5078a":"**We are using summary to track down how many Layers are used to compile the model**\n\n> ***values inside the params are explained below***\n\n> ***HiddenLayer1 is showing 235500 params that params are (784 x 300 +300) you must be wondering how +300 value is added to it. It is nthing but the Biases what we are adding\nsame calculation goes with rest of the layers***\n\n> ***You must be wondering what are Trainable params and Non-Trainable params why non-trainable params is 0. This is because we using all the params to train so its showing some numbers but what about non trainable params is it always 0? No! you must have heard term Transfer learning in that we see the actual numbers of non-trainable params***","165f37e6":"**what are the endpoints of the above given visualization**","2958a692":"**Checking the shape or Dimensions of Train and Test sets**","0a628956":"**As we are using our layers in Sequential manner we are using the keras sequential model**","4e2dbd93":"**Checking the trained weights if they are working fine with our Test data points**","4990cd55":"<h2>Artificial Neural Network (ANN)<\/h2>\nAn artificial neural network has three or more layers that are interconnected. The first layer consists of input neurons. Those neurons send data on to the deeper layers, which in turn will send the final output data to the last output layer.\n\n\nAll the inner layers are hidden and are formed by units which adaptively change the information received from layer to layer through a series of transformations. Each layer acts both as an input and output layer that allows the ANN to understand more complex objects. Collectively, these inner layers are called the neural layer.\n\nThe units in the neural layer try to learn about the information gathered by weighing it according to the ANN\u2019s internal system. These guidelines allow units to generate a transformed result, which is then provided as an output to the next layer.\n\nAn additional set of learning rules makes use of backpropagation, a process through which the ANN can adjust its output results by taking errors into account. Through backpropagation, each time the output is labeled as an error during the supervised training phase, the information is sent backward. Each weight is updated proportionally to how much they were responsible for the error.\n\nHence, the error is used to recalibrate the weight of the ANN\u2019s unit connections to take into account the difference between the desired outcome and the actual one. In due time, the ANN will \u201clearn\u201d how to minimize the chance for errors and unwanted results.\n\nTraining an artificial neural network involves choosing from allowed models for which there are several associated algorithms.\n\nAn ANN has several advantages but one of the most recognized of these is the fact that it can actually learn from observing data sets. In this way, ANN is used as a random function approximation tool. These types of tools help estimate the most cost-effective and ideal methods for arriving at solutions while defining computing functions or distributions.\n\nANN takes data samples rather than entire data sets to arrive at solutions, which saves both time and money. ANNs are considered fairly simple mathematical models to enhance existing data analysis technologies.\n\nThey can be used for many practical applications, such as predictive analysis in business intelligence, spam email detection, natural language processing in chatbots, and many more.\n","797ac437":"**Testing the our model if it is working fine and predicting proper numbers**","a6f0f925":"**Tracking down the Weights and Biases of the layers and showing their shape**","f26e0e2d":"<img src=\"https:\/\/techvidvan.com\/tutorials\/wp-content\/uploads\/sites\/2\/2020\/05\/Architecture.jpg\">","91175bff":"**Training of our model with epochs and validation-data(subset of 5000 data points)**\n\n> ***Why the batch size is 1719 for every epoch whether is it fixed or user-input data?\nwhen we don't specify the batch size it takes the default valus of 32 and divides the number of data points in the train data set which is 55000\/32 ~ 1719***","52616787":"**We are dividing the training dataset into 2 sub sets to train and test within training samples**\n\n**we are substituting 60000 data points into 2 subsets**\n\n**5000 data points into X_train_valid on which we are going to test**\n\n**rest 55000 data points to train the model to give maximum accuracy**"}}