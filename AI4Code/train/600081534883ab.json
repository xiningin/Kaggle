{"cell_type":{"b5221911":"code","0bf1a303":"code","1ada4c09":"code","ba113c3d":"code","c2e36d00":"code","92eb81f7":"code","2e38b920":"code","b2078ec4":"code","2d2b2407":"code","706047b1":"code","c912f549":"code","56ab6c0c":"code","23963077":"code","518ae65d":"code","583a4a4e":"code","30a82bf6":"code","968293c4":"code","da9536ba":"code","cb43a59a":"code","fc2aaf8a":"code","e7f340fd":"code","dd59b5f3":"code","5142c475":"code","78c0bdd0":"code","9d61fe47":"code","c0d25bec":"code","2d7e9a03":"code","891d3280":"code","9daef515":"code","b864351e":"code","4cde11e0":"code","7352b8fd":"code","e505d73c":"code","d092b285":"code","65d0c30e":"code","b8660369":"code","7be4eb48":"code","33e285ea":"code","0c383462":"code","7cd60a9e":"code","9049ce21":"code","a0e05f43":"code","fec462ff":"code","a8c50bb1":"code","263a5441":"code","e7e00766":"code","045d4605":"code","9d765c92":"code","ed17c33e":"code","29919f2a":"code","f395bf51":"code","645c9ee7":"code","6337edd3":"code","d89ca36d":"code","acbb5868":"code","3e9ff164":"code","26cfa700":"code","efa5a71c":"code","00e8acc9":"code","7ed35447":"code","e96d48e7":"code","4b134b17":"code","0bdb1cb5":"code","aae84998":"code","5575d0c2":"code","082a367e":"code","601f77fd":"code","f103e671":"code","b3849deb":"code","8b600305":"code","d44d2fee":"code","a689c857":"code","4c181edc":"code","a339090d":"code","05143093":"code","52f811aa":"code","a5e7ea1c":"code","2bd8d4e0":"code","32d3c33c":"markdown","0fdce6fa":"markdown","2d6e0fa9":"markdown","13ee6a05":"markdown","c3eae77b":"markdown","c2c86f7b":"markdown","f7eb67ec":"markdown","842396c2":"markdown","ce02ae96":"markdown","926c3d44":"markdown","70b0876f":"markdown","ddd7a2e4":"markdown","20db6fc1":"markdown","61e2bedc":"markdown","61b0c5a6":"markdown","8a156175":"markdown","d95c1a3d":"markdown","fddb6cc7":"markdown"},"source":{"b5221911":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns   # visualization tool\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","0bf1a303":"data = pd.read_csv(\"..\/input\/2016.csv\")  #first we are input the data ","1ada4c09":"data.shape   #we can check how many columns and rows are there ","ba113c3d":"data.columns     #name of the columns in dataset","c2e36d00":"#There is a simple problem in dataset. when there is a space between columns of name, we muss put '_' or write together.","92eb81f7":"data.columns = [each.lower() for each in data.columns] \ndata.columns = [each.split()[0]+\"_\"+each.split()[1] if(len(each.split())>1) else each for each in data.columns]","2e38b920":"data.columns    #our new database","b2078ec4":"data.rename(columns={'economy_(gdp':'economy_gdp','health_(life':'health_life','trust_(government':'trust_government'},inplace=True) ","2d2b2407":"data.columns","706047b1":"data.info()     # we learn information about database","c912f549":"data.isnull().sum()       # we are checking for missing data, we haven't got missing data","56ab6c0c":"data.describe()    #we are describe the data","23963077":"data.head(10)   # first 10 data in database","518ae65d":"print(len(data.region.unique())) \ndata.region.unique()   #check about how many different regions","583a4a4e":"data_vc = data['region'].value_counts()      #We check which region is the most in dataset.\ndata_vc.head(10)     #first 10 data show ","30a82bf6":"data.corr()     # we are looking at the correlation between numerical values","968293c4":"f,ax = plt.subplots(figsize=(18,18))    #correlation between numerical values' maps\nsns.heatmap(data.corr() , annot = True, linewidths = .5, fmt= '.1f', ax=ax)\nplt.legend()\nplt.show() ","da9536ba":"data.plot(kind = 'scatter', x='happiness_score', y='upper_confidence',alpha=0.5,color='r')\nplt.xlabel(\"happiness_score\")\nplt.ylabel(\"upper_confidence\")\nplt.title(\"happiness - confidence Scatter Plot\")\nplt.show() ","cb43a59a":"west_eu=data[data.region==\"Western Europe\"]  \nwest_eu.head(10)  ","fc2aaf8a":"cen_east_eu=data[data.region==\"Central and Eastern Europe\"] #i wanna work with Central and Eastern Europe, so i create another columns for Central and Eastern Europe\ncen_east_eu.head(10) ","e7f340fd":"ort = sum(data['happiness_score']) \/ len(data['happiness_score'])\nprint(\"average_life_standard:\",ort)\ndata['happiness_score_level'] = [\"easy life\" if i > ort else \"hard life\" for i in data['happiness_score']]\ndata.loc[:100] ","dd59b5f3":"data['yasam'] = [\"high life\" if i<15 else \"middle life\" if (30>i) else \"low life\" for i in data ['happiness_rank']]\ndata.head(40) ","5142c475":"data[(data['happiness_rank']<15) & (data['economy_gdp']>1.35)] \n#data[np,logical_and(data['happiness_score_level']= 'yasanabilir' , data['yasam']='orta yasam')] ","78c0bdd0":"data['health_life'].corr(data['economy_gdp'])  #Correlation between health life and economy ","9d61fe47":"data.generosity.plot(kind = 'hist',bins = 30,figsize = (10,10))  #we learn generosity frequency\nplt.show ","c0d25bec":"data.region = [each.lower() for each in data.region] \ndata.region = [each.split()[0]+\"_\"+each.split()[1] if(len(each.split())>1) else each for each in data.region]\ndata.region.unique() ","2d7e9a03":"data.plot(kind = 'scatter', x='economy_gdp',y='generosity',alpha=0.8,color='green')\nplt.xlabel(\"Economy\")\nplt.ylabel(\"Generosity\")\nplt.title('Economy-Generosity')\nplt.show()","891d3280":"data.plot(kind = 'scatter', x='economy_gdp',y='happiness_score',alpha=0.8,color='green')\nplt.xlabel(\"Economy\")\nplt.ylabel(\"Generosity\")\nplt.title('Economy-Happiness_score')\nplt.show() ","9daef515":"#Dataframe from Dictionary\ncountry = [\"Italy\",\"Germany\"]\npopulation = [\"11\",\"12\"]\nlist_label = [\"country\",\"population\"]\nlist_col = [country,population]\nzipped = list(zip(list_label,list_col))\ndata_dict = dict(zipped)\ndf = pd.DataFrame(data_dict)\ndf ","b864351e":"df[\"capital\"]=[\"Roma\",\"Berlin\"]\ndf ","4cde11e0":"df[\"income\"]=0     #Broadcasting entire column\ndf ","7352b8fd":"data1=data.loc[:,[\"health_life\",\"economy_gdp\",\"freedom\"]]    #Plot\ndata1.plot() ","e505d73c":"#Subplots\ndata1.plot(subplots = True)\nplt.show() ","d092b285":"#scatter plot   we are loking for the corralation between two columns\ndata1.plot(kind = \"scatter\", x=\"health_life\",y=\"economy_gdp\")\nplt.show() ","65d0c30e":"#hist \ndata.plot(kind = \"hist\", y=\"happiness_score\",bins=30,range = (0,10),normed = True)\nplt.show()   ","b8660369":"#histogram subplot with non cumulative and cumulative\nfig, axes = plt.subplots(nrows=2,ncols=1)\ndata.plot(kind = \"hist\",y=\"happiness_score\",bins = 30,range=(1,10),normed = True,ax = axes[0])\ndata.plot(kind = \"hist\",y=\"happiness_score\",bins = 30,range=(1,10),normed = True,ax = axes[1],cumulative = True)\nplt.savefig('graph.png')\nplt.show() ","7be4eb48":"#we are creating the date which is the string,however we want change datetime object\ntime_list = [\"2018-03-08\",\"2018-04-12\"]\nprint(type(time_list[1]))  #now date is string\ndatatime_object = pd.to_datetime(time_list)\nprint(type(datatime_object))  #now date is datetime object  ","33e285ea":"data2=data.head() \ndate_list = [\"2017-01-10\",\"2017-02-10\",\"2017-03-10\",\"2018-03-15\",\"2018-03-16\"]\ndatetime_object = pd.to_datetime(date_list)\ndata2[\"date\"] = datetime_object\n#lets make date as index\ndata2 = data2.set_index(\"date\")\ndata2   ","0c383462":"#Now we can select according to our date index\nprint(data2.loc[\"2018-03-16\"])\nprint(data2.loc[\"2017-03-10\":\"2018-03-16\"])  ","7cd60a9e":"#Now we can select according to our data's columns\nprint(data2.loc[:,\"happiness_score\"]) ","9049ce21":"print(data2.loc[:, \"happiness_score\":\"freedom\"]) ","a0e05f43":"print(data2.loc[:,:\"happiness_score\"]) ","fec462ff":"print(data2.loc[::-1,:]) ","a8c50bb1":"data2.resample(\"Y\").mean()   #we are learning how many years are there in data","263a5441":"#lets resample with month\ndata2.resample(\"M\").mean() \n# there are alot of nan because data2 does not include all months. We can solve this problem next step","e7e00766":"#we can solve this problem with interpolate in happiness_rank\ndata2.resample(\"M\").first().interpolate(\"linear\") ","045d4605":"#second way is interpolatewith mean\ndata2.resample(\"M\").mean().interpolate(\"linear\") ","9d765c92":"data2[\"happiness_rank\"][2] ","ed17c33e":"data2.happiness_rank[2] ","29919f2a":"data2[[\"happiness_rank\",\"happiness_score\"]] ","f395bf51":"data.loc[1:10,\"happiness_score\"]  #10 and happiness_score are inclusive ","645c9ee7":"#slicing data frame\ndata.loc[10:1:-1,\"happiness_rank\":\"economy_gdp\"] ","6337edd3":"#From something to end\ndata.loc[1:10,\"economy_gdp\":] ","d89ca36d":"boolean=data.happiness_score > 7.0  #Creating boolean series\ndata[boolean] ","acbb5868":"data.head(50)  ","3e9ff164":"# Combining filters \nfirst_filter = data.happiness_score > 7\nsecond_filter = data.economy_gdp >1.47\ndata[first_filter & second_filter] ","26cfa700":"# Filtering column based others\ndata.happiness_score[data.economy_gdp>1.47] ","efa5a71c":"# Plain Pyton function \ndef div(n):\n    return n\/2\ndata.happiness_score.apply(div) ","00e8acc9":"# if you want we can use second way as lambda function\ndata.happiness_score.apply(lambda n:n\/2) ","7ed35447":"#Defining column using other columns\ndata[\"confidence_interval\"] = data.upper_confidence - data.lower_confidence \ndata.head() ","e96d48e7":"# or\ndata[\"confidence_interval_avrg\"] = (data.upper_confidence + data.lower_confidence) \/ 2\ndata.head() ","4b134b17":"#we are looking our index name\nprint(data.index.name) ","0bdb1cb5":"#we are changing index name\ndata.index.name = \"index_name\"\ndata.head() ","aae84998":"#If we want to modify index we need to change all of them\ndata.head() \ndata3=data.copy()  # i wanna copy because i dont want to change my orginal data\ndata3.index = range(100,257,1) \ndata3.head()  ","5575d0c2":"data1 = data.set_index([\"region\",\"country\"])\ndata1.head (50)    ","082a367e":"dic={\"treatment\" : [\"A\",\"A\",\"B\",\"B\"], \"gender\":[\"F\",\"M\",\"F\",\"M\"],\"response\":[10,45,5,9],\"age\":[15,4,72,65]}\ndf=pd.DataFrame(dic)\ndf ","601f77fd":"#Pivoting \ndf.pivot(index=\"treatment\",columns = \"gender\",values=\"response\") ","f103e671":"#unstack\ndf1 = df.set_index([\"treatment\",\"gender\"])\ndf1 ","b3849deb":"#level determines indexes\ndf1.unstack(level=0) ","8b600305":"df1.unstack(level=1) ","d44d2fee":"#Change inner and outer level index position\ndf2 = df1.swaplevel(0,1)\ndf2 ","a689c857":"# Reverse of pivoting \ndf","4c181edc":"#df.pivot(index=\"treatment\", columns=\"gender\",values=\"response\")\npd.melt(df,id_vars=\"treatment\",value_vars=[\"age\",\"response\"]) ","a339090d":"df","05143093":"df.groupby(\"treatment\").mean() ","52f811aa":"df.groupby(\"gender\").mean() ","a5e7ea1c":"df.groupby(\"age\").max() ","2bd8d4e0":"df.groupby(\"treatment\") [[\"age\",\"response\"]].min() ","32d3c33c":"**Transforming Data**","0fdce6fa":"I create the average life standard. higher than the average of life standart,write 'yasanabilir',otherwise write 'yasanmasi zor'","2d6e0fa9":"i wanna work with Central and Eastern Europe, so i create another columns for Central and Eastern Europe. now i can analysis easly","13ee6a05":"**Index Objects and Labeled Data**","c3eae77b":"i wanna see between (happiness rank lower than 15 but economy_gdp higher than 1.35","c2c86f7b":"**Categoricals and Groupby**","f7eb67ec":"**Resampling Pandas Time Series**","842396c2":"**Indexing Pandas Time Series**","ce02ae96":"**Buildung Data Frames From Scratch**","926c3d44":"    **Filtering DataFrames**\ncreating boolean series Combining filters Filtering column based others","70b0876f":"when the happiness_rank lower than 15, means yuksek yasam. when the happiness_rank between 15-30, means orta yasam. otherweis dusuk yasam.\nwirte first 40 data","ddd7a2e4":"**Visual Exploratory Data Analysis**\n* Plot\n* Subplot\n* Histogram","20db6fc1":"**Pivoting Data Frames**","61e2bedc":"**Manipulating Dataframes With Pandas**","61b0c5a6":"i wanna work with Western Europe, so i create another columns for Western Europe. now i can analysis easly","8a156175":"**Hierarchical Indexing**","d95c1a3d":"**Stacking and Unstacking Dataframe**","fddb6cc7":"**Melting Data Frames**"}}