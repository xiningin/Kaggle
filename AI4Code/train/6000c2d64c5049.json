{"cell_type":{"03e119ab":"code","fe3856ed":"code","2d9b33b6":"code","7bd00590":"code","71e170f9":"code","fe2e2b98":"code","036c7e0d":"code","2df4a14a":"code","324438a6":"code","570229ba":"code","00822e5e":"code","f3343835":"code","d1a90b95":"code","fb21cf2b":"code","a75624bf":"code","30b03729":"code","5fe9d2df":"code","8d0afef5":"code","8c0cf083":"code","0cc2bfc5":"code","50f78765":"code","26e7af66":"code","1d16179d":"code","0d20d862":"code","c075edf5":"code","faf5116e":"code","dc563787":"code","51989d20":"code","ddcc82c1":"code","03e57b68":"code","380560b1":"markdown","e1c1a145":"markdown","413c3b16":"markdown","5cc4aca8":"markdown","5d930a77":"markdown","03e8b80e":"markdown"},"source":{"03e119ab":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nsns.set_style('dark')\nimport sklearn\nimport tensorflow as tf\nfrom tensorflow import keras","fe3856ed":"data_path = '..\/input\/dont-call-me-turkey\/train.json'\ntest_path = '..\/input\/dont-call-me-turkey\/test.json'\n\ndf = pd.read_json(data_path)\ntest_df = pd.read_json(test_path)","2d9b33b6":"df.head()","7bd00590":"df.shape","71e170f9":"test_df.head()","fe2e2b98":"test_df.shape","036c7e0d":"df.info()","2df4a14a":"df.describe()","324438a6":"sns.countplot(df['is_turkey'])\nplt.tight_layout()","570229ba":"length = df['audio_embedding'].apply(len)\n\ny = length\nx = np.arange(1, len(length)+1)\n\nsns.countplot(length)\nplt.tight_layout()","00822e5e":"plt.yscale('log')\nsns.countplot(length)\nplt.tight_layout()","f3343835":"df.head()","d1a90b95":"df['duration'] = df['end_time_seconds_youtube_clip'] = df['start_time_seconds_youtube_clip']\ndf.head()","fb21cf2b":"corr = df.corr()\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, cmap=cmap, vmax=.3, center=0,\n            square=True, linewidths=.5)\nplt.tight_layout()","a75624bf":"from keras.preprocessing.sequence import pad_sequences\nmaxlen = 10\n\ndata = pad_sequences(df['audio_embedding'], maxlen=maxlen, padding='post')\nlabels = df['is_turkey'].values","30b03729":"from keras.utils import to_categorical\n\ntrain_size = int((80\/100) * df.shape[0])\n\ntrain_data = data[: train_size]\ntrain_labels = to_categorical(labels[: train_size])\n\nvalid_data = data[train_size: ]\nvalid_labels = to_categorical(labels[train_size: ])\n\nassert(len(train_data) == len(train_labels))\nassert(len(valid_data) == len(valid_labels))","5fe9d2df":"train_data[0]","8d0afef5":"num_features = len(train_data[0][0])\nnum_features","8c0cf083":"train_labels[0]","0cc2bfc5":"test_data = pad_sequences(test_df['audio_embedding'], maxlen=maxlen, padding='post')\ntest_data[0]","50f78765":"def build_model():\n    inp = keras.layers.Input(shape=(maxlen, num_features))\n    x = keras.layers.BatchNormalization()(inp)\n    \n    x = keras.layers.Bidirectional( keras.layers.LSTM(128, return_sequences=True) )(x)\n    x = keras.layers.Bidirectional( keras.layers.LSTM(64, return_sequences=True) )(x)\n    \n    avg_pool = keras.layers.GlobalAveragePooling1D()(x)\n    max_pool = keras.layers.GlobalMaxPooling1D()(x)\n    \n    concat = keras.layers.concatenate([avg_pool, max_pool])\n    \n    hidden_1 = keras.layers.Dense(64)(concat)\n    hidden_2 = keras.layers.Dropout(0.5)(hidden_1)\n    hidden_activ = keras.layers.LeakyReLU()(hidden_2)\n    \n    output = keras.layers.Dense(2, activation=\"softmax\")(hidden_activ)\n    \n    model = keras.Model(inputs=inp, outputs=output)\n    \n    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc'])\n    \n    return model","26e7af66":"model = build_model()\nkeras.utils.plot_model(model, dpi=62)","1d16179d":"reduce_lr = keras.callbacks.ReduceLROnPlateau(monitor='val_acc', factor=0.2, patience=2, verbose=1, min_lr=1e-8)\n\nmy_cb = tf.keras.callbacks.EarlyStopping(monitor='val_acc', patience=10, restore_best_weights=True)\n\nhistory = model.fit(train_data, train_labels, batch_size=64, epochs=100, \n                    validation_data=(valid_data, valid_labels), callbacks=[reduce_lr, my_cb], verbose=2)","0d20d862":"epochs = len(history.history['loss'])\nepochs","c075edf5":"y1 = history.history['loss']\ny2 = history.history['val_loss']\nx = np.arange(1, epochs+1)\n\nplt.plot(x, y1, y2)\nplt.legend(['loss', 'val_loss'])\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.tight_layout()","faf5116e":"y1 = history.history['acc']\ny2 = history.history['val_acc']\nx = np.arange(1, epochs+1)\n\nplt.plot(x, y1, y2)\nplt.legend(['acc', 'val_acc'])\nplt.xlabel('Epochs')\nplt.ylabel('Acc')\nplt.tight_layout()","dc563787":"model.evaluate(valid_data, valid_labels)","51989d20":"predictions = np.argmax(model.predict(test_data), axis=1)\npredictions","ddcc82c1":"submission = pd.DataFrame({'vid_id': test_df['vid_id'], 'is_turkey': predictions})\nsubmission.head()","03e57b68":"submission.to_csv('submission.csv', index=False)","380560b1":"# Data preprocessing","e1c1a145":"# Data preparation","413c3b16":"# Building and compiling the model","5cc4aca8":"# Let's bring in the imports and the data","5d930a77":"# Make predictions","03e8b80e":"# Training the model"}}