{"cell_type":{"22e9dd54":"code","c167f327":"code","96d3605e":"code","104f3aa3":"code","8a529720":"code","98432b20":"code","7e9a8091":"code","087ee67e":"code","07af87f0":"code","b98b8a38":"code","49bda861":"code","d589f2f1":"code","3956fd75":"code","afed9706":"code","5ef72362":"code","5a1f8f73":"code","1da1a0f2":"code","c0eff6d2":"code","1fe178f7":"code","b130f658":"code","781a6aaa":"code","a3702ff6":"code","2a0c5d12":"code","5933f879":"code","0c8ba06a":"code","a3cb7942":"code","aa82664b":"code","5c7e1ffd":"code","3b2eb92d":"code","06b7f66c":"code","0668325d":"code","3bc296e3":"code","3ec8272e":"code","a9ce86b8":"code","b4bca199":"code","24df34e4":"code","b614184e":"code","d1cef0c1":"code","9b9f80be":"code","9789a336":"code","f0e5dac0":"code","a80b88ca":"code","75dfa023":"code","d1ddce58":"code","d6434188":"code","a3043054":"code","1b1a825a":"code","0579c2f3":"code","7b313663":"code","dc29cb91":"code","72fd408d":"markdown","dd453edf":"markdown","7e193a79":"markdown","feb83a24":"markdown","57f9bb19":"markdown","32c1d9a9":"markdown","fe5c9afd":"markdown","c1487084":"markdown","97cbbd44":"markdown","166c4a22":"markdown","5472b1e8":"markdown","8dd7c5c2":"markdown","0f9efe72":"markdown","691da439":"markdown","3d6ac321":"markdown","02e26a23":"markdown","1dac78b4":"markdown","5cb3a384":"markdown","162fd6ba":"markdown","c1e1536c":"markdown","246c8b9f":"markdown","eee3334b":"markdown","32587d7a":"markdown","a17a617b":"markdown","be227621":"markdown","803b282f":"markdown","2b502009":"markdown","b340e90f":"markdown","7fde8289":"markdown","9cfece9d":"markdown"},"source":{"22e9dd54":"# import libraries\nimport pandas as pd\nimport numpy as np","c167f327":"train = pd.read_csv(r'\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv(r'\/kaggle\/input\/titanic\/test.csv')","96d3605e":"train.head(5)","104f3aa3":"train.isna().sum()","8a529720":"dfs = [train ,test]\n\nfor df in dfs:\n    df['Age'].fillna(df['Age'].median(), inplace = True)","98432b20":"train.isna().sum()","7e9a8091":"train['Cabin'].value_counts()","087ee67e":"train['Cabin'].unique()","07af87f0":"for df in dfs:\n    df['Cabin'].fillna(0)","b98b8a38":"cabins = []\nfor i in train['Cabin']:\n    cabins.append(str(i))","49bda861":"letters = []\nfor i in cabins:\n    letter= i[0]\n    letters.append(letter)","d589f2f1":"train['Cabin'] = letters","3956fd75":"cabins = []\nfor i in test['Cabin']:\n    cabins.append(str(i))","afed9706":"letters = []\nfor i in cabins:\n    letter = i[0]\n    letters.append(letter)","5ef72362":"test['Cabin'] = letters","5a1f8f73":"train['Cabin'].head()","1da1a0f2":"train['Embarked'].value_counts()","c0eff6d2":"for df in dfs:\n    df['Embarked'].fillna('S')","1fe178f7":"import seaborn as sns\nimport matplotlib.pyplot as plt\n#seaborn & matplotlib are excellent python libraries to perform clean visualizations.\n#I highly suggest you get familiar with them!\n\n#correlation matrix \ncorr_matrix = train.corr()\nfig, ax = plt.subplots(figsize = (10,8))\nsns.heatmap(corr_matrix, annot = True, fmt='.2g', vmin = -1,\n            vmax = 1, center = 0, cmap = 'coolwarm')","b130f658":"train.dtypes","781a6aaa":"#boxplot\nnumeric_cols = ['Survived', 'Pclass', 'Age', 'SibSp', 'Parch', 'Fare']\nfig, ax = plt.subplots(figsize = (10,5))\nsns.boxplot(data = train[numeric_cols], orient = 'h', palette = 'Set2')","a3702ff6":"from pandas.plotting import scatter_matrix\nscatter_matrix(train[numeric_cols], figsize= (12,8))","2a0c5d12":"train.hist(bins = 20, figsize = (12,8))","5933f879":"sns.countplot(train[train['Survived'] == 1]['Pclass']).set_title('Count Survived for each Class')","0c8ba06a":"len(train[train['Pclass'] == 1]), len(train[train['Pclass'] == 2]), len(train[train['Pclass'] == 3])","a3cb7942":"train[train['Pclass'] == 1]['Survived'].sum(), train[train['Pclass'] == 2]['Survived'].sum(), train[train['Pclass'] == 3]['Survived'].sum()   ","aa82664b":"percentages = []\nfirst = 136 \/ 216\nsecond = 87\/ 184\nthird = 119\/491\npercentages.append(first)\npercentages.append(second)\npercentages.append(third)","5c7e1ffd":"percents = pd.DataFrame(percentages)\npercents.index+=1","3b2eb92d":"percents['PClass'] = ['1', '2', '3']\ncols= ['Percent', 'PClass']\npercents.columns = [i for i in cols]\nsns.barplot(y = 'Percent', x = 'PClass', data = percents).set_title('Percent Survived for Passenger Class')","06b7f66c":"train['Family'] = train.apply(lambda x: x['SibSp'] + x['Parch'], axis = 1)\ntest['Family'] = test.apply(lambda x: x['SibSp'] + x['Parch'], axis = 1)","0668325d":"#dropping columns from the dataframe \ntrain.drop(['SibSp', 'Parch', 'Name', 'Ticket'], axis = 1, inplace = True)\ntest.drop(['SibSp', 'Parch', 'Name', 'Ticket'], axis = 1, inplace = True)","3bc296e3":"train.head(5)","3ec8272e":"test.isna().sum()","a9ce86b8":"test['Fare'].fillna(test['Fare'].median(), inplace = True)","b4bca199":"train_df = pd.get_dummies(train)\ntest_df = pd.get_dummies(test)","24df34e4":"#axis 1 refers to columns!\ntrain_df.drop('PassengerId', axis = 1, inplace = True)","b614184e":"y = train_df['Survived']\ntrain_df.drop('Survived', axis = 1, inplace = True)\ntrain_df.drop('Cabin_T', axis = 1, inplace = True)\ntest_df.drop('PassengerId', axis = 1, inplace = True)","d1cef0c1":"X_test = test_df\nX_train = train_df","9b9f80be":"from sklearn.ensemble import RandomForestClassifier\n","9789a336":"rfc = RandomForestClassifier","f0e5dac0":"param_grid = {\n    'n_estimators': [200, 500, 1000],\n    'max_features': ['auto'],\n    'max_depth': [6, 7, 8],\n    'criterion': ['entropy']\n}","a80b88ca":"CV = GridSearchCV(estimator = rfc, param_grid = param_grid, cv = 5)\nCV.fit(X_train, y)\nCV.best_estimator_","75dfa023":"rfc.fit(X_train, y)","d1ddce58":"y_pred = rfc.predict(X_test)","d6434188":"#we can reshape this array using .reshape(-1, 1)\ny_pred","a3043054":"#reshape array so that it can be used in a dataframe for easy submission!\nsubmission1 = y_pred.reshape(-1, 1)","1b1a825a":"sub_df = pd.DataFrame(submission1)","0579c2f3":"sub_df['PassengerId'] = test['PassengerId']\nsub_df['Survived'] = submission1\ncols = ['PassengerId',\n       'Survived']\nsub_df.drop(0, axis = 1, inplace = True)\nsub_df.columns = [i for i in cols]\nsub_df = sub_df.set_index('PassengerId')","7b313663":"sub_df.head(10)","dc29cb91":"#put file path in string!\nsub_df.to_csv(r'submission13.csv')","72fd408d":"### Age ","dd453edf":"GridSearchCV and RandomizedSearchCV are excellent tools for determing the best hyperparameters for your models! This can increase your model accuracy significantly. The only downside is it takes quite some time to run so if using very large datasets you will want to convert to numpy arrays for much faster training time. The dataset in this competition is fairly small so we won't both with this.","7e193a79":"### Cabin","feb83a24":"### Tuning Hyperparameters ","57f9bb19":"### Submission To CSV","32c1d9a9":"To ensure our data is trainable in our algorithm, we must take a look at any missing values. There are a combination of techniques to fix these missing values including:\n* Fill with either the median or mean. Using the median may be preferable as it is more robust to outliers. If you high extreme values on each end, then the mean may be affected severally. (i.e. mean income of the district that Bill Gates lives in.)\n* Drop the column if the majority of data is missing.\n* Fill with 0 if appropriate. Many times a missing value may signify \"no item\". This is why it is important to examine the columns with missing data closely.","fe5c9afd":"The most important part of each value is what cabin letter they are in. We will aim to pull only the first character (letter) from each row.","c1487084":"It worked! We have grabbed the first letter from each row.","97cbbd44":"Take a look at your submission object now by calling (submission). It should print out the array reshaped. I won't include it here because it will make the reader scroll quite a bit to pass the section.","166c4a22":"Our param grid is set up as a dictionary so that GridSearch can take in and read the parameters. This search will perform 3 X 1 X 3 X 1 = 9 different combinations and then fit them 5 times (cv = 5), resulting in 45 models trained. Calling best_estimator_ or best_params_ will give us the model that peformed the best.","5472b1e8":"## Cleaning the Dataframe","8dd7c5c2":"### Exploratory Analysis","0f9efe72":"Now all of our missing data is filled in so we can go ahead with our model!","691da439":"### One Hot Encoding","3d6ac321":"We will fill with the mode of the data column. This being 'S' as it will alter out data the least. ","02e26a23":"In the above code we iterate through each dataframe and fill the missing Age values with the median of each dataframe! Kinda cool right. If we check, we should see that Age now has 0 missing values.","1dac78b4":"We can see above how one hot encoding alters the dataframe.","5cb3a384":"## Prepare Data","162fd6ba":"I choose to fill the missing cabin columns with 0 instead of drop it becuase cabin may be associated with passenger class! We will have a look at a correlation matrix that includes categorical columns once we have used One Hot Encoding!","c1e1536c":"### Check Test DataFrame For Any Missing Values Too!","246c8b9f":"# Titanic: A Beginner's Tutorial","eee3334b":"It's a good thing that we filled fare with the median value as there is an outlier.","32587d7a":"## Table of Contents:\n* [Cleaning the Dataframe](#Cleaning-the-Dataframe)  \n    * [Age](#Age)  \n    * [Cabin](#Cabin)  \n    * [Embarked](#Embarked)  \n* [Exploratory Analysis](#Exploratory-Analysis)\n* [Feature Engineering](#Feature-Engineering)  \n* [One Hot Encoding](#One-Hot-Encoding)\n* [Preparing the Data](#Preparing-the-Data)\n* [Tuning Hyperparameters](#Tuning-Hyperparameters)\n* [Submission to CSV](#Submission-To-CSV)","a17a617b":"One Hot Encoding, one of the most useful techniques that a data scientist can know. This techniques label encodes categorical columns resulting in a 1 if the value is true, with all associated values in that row taking value 0. Take for example our 'Embarked' column. Using one hot encoding will create embarked_S, embarked_C, and embarked_Q columns for each row. The True value will take a 1. This is crucial to preparing data for our model as it won't take kindly to non-numerics.","be227621":"Please feel free to ask any questions to clarify topics in the comment section. This dataset is a great guide to get your feet wet in predictive modeling and machine learning. ","803b282f":"We have just achieved top 10% Thanks for following along. Make sure to give this notebook an upvote if it was helpful!","2b502009":"The majority of first class passengers survived with about slighly lower than 50% of 2nd class passengers surviving. The majority of third class passengers did not survive. Therefore, we can see that Passenger Class impacted your survival chance aboard the Titanic. ","b340e90f":"Keep in mind that the model accuracy could be improved by finding titles for each of the passengers. For simplicity of this tutorial, we won't be covering that but if enough people request it I will make the change in following versions. ","7fde8289":"### Feature Engineering","9cfece9d":"### Embarked"}}