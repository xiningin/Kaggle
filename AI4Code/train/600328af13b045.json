{"cell_type":{"2517cbae":"code","a6550271":"code","72f38dad":"code","e7ca8741":"code","1b411906":"code","48750c1c":"code","b51ca0b3":"code","05d40735":"code","b05d4149":"code","c5ab9f82":"code","8615ab76":"code","63dc1813":"code","2ab4acca":"code","8482c2a0":"code","31ab0035":"code","a2c6b187":"code","2ab671cb":"code","68762de8":"code","9475f102":"code","53d5ddae":"code","4de9a717":"code","3d397bc7":"code","3dd576b3":"code","c2084d96":"code","d904e07b":"code","e7c229d9":"code","96355235":"code","c587e817":"code","42f2b368":"code","f4c09597":"code","e46f6cbc":"code","2315941c":"code","eea0fb38":"code","fe43257a":"code","faf7cd37":"code","99e08401":"code","8cc7a9f1":"code","430aa169":"code","a360c308":"code","1b7597ed":"code","cc621e61":"code","344c71dd":"code","5cc30ddc":"code","5723a3cc":"code","4afd50c7":"code","4e8b98b3":"code","1a3ffdde":"code","19579968":"code","03f3d3f9":"code","c9291fbe":"code","8cd15d2a":"code","99a582e2":"code","1f515137":"code","421c78d0":"code","d990430f":"code","6c0cd153":"code","be76ddcf":"code","308b5975":"code","e34d3b1a":"code","72be385d":"code","4d15462c":"code","848fe6c6":"code","ff79caef":"markdown","83af5024":"markdown","f9630a22":"markdown","255e62fd":"markdown","dae36316":"markdown","9c64c985":"markdown","ad75c66b":"markdown","9235de93":"markdown","f9ea3c31":"markdown","b572b692":"markdown","6ef447bb":"markdown","6c3f15b3":"markdown","ed0f8df7":"markdown","617ef738":"markdown","e7e70d6d":"markdown","68d2368c":"markdown","ce7973f9":"markdown","9b1aa7b5":"markdown","04e8e4a1":"markdown","c4feb609":"markdown","c9fd47f5":"markdown","5c5f640b":"markdown","199bf141":"markdown","ed5b40e4":"markdown","22d6d0f4":"markdown","f32f7571":"markdown","78830a6f":"markdown","24d6e314":"markdown","03e20099":"markdown","82282bfa":"markdown","b99f7219":"markdown","8f1992ac":"markdown","e8a32263":"markdown","cc55ce31":"markdown","b73fc74b":"markdown","67713f6a":"markdown","93399cd9":"markdown","813efaf8":"markdown","5d4f3d29":"markdown","fed83f0d":"markdown","57a5da11":"markdown","51aafd63":"markdown","ec1392d7":"markdown","c9fbe5c5":"markdown","9fa9dc12":"markdown","4afe04bb":"markdown","9dc581b9":"markdown","508b052e":"markdown","30177aee":"markdown","30aa3d74":"markdown","1582de1d":"markdown","aaadb6eb":"markdown","9b9540e2":"markdown","3d057b50":"markdown","f3e2953c":"markdown"},"source":{"2517cbae":"#!pip install kaggle                   #Install the Kaggle API package (Not necessary since we are in Kaggle)\n!pip install cord-19-tools             #Install the COVID-19 Data Tools package\n#!pip install plotly                   #Exists in Kaggle already\n\n#!pip install spacy                    #Exists in Kaggle already\n!pip install spacy-langdetect\n\n#!pip install pycountry                #Exists in Kaggle already\n!pip install geonamescache\n!pip install geopy\n!pip install reverse_geocoder\n\n#!pip install nltk                     #Exists in Kaggle already\n\n!pip install ktrain\n#!pip install --upgrade scikit-learn   #Exists in Kaggle already\n\n#!pip install pyLDAvis                 #Exists in Kaggle already\n#!pip install wordcloud                #Exists in Kaggle already\n\n#!pip install torch                    #Exists in Kaggle already\n#!pip install transformers             #Exists in Kaggle already\n!pip install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz","a6550271":"# Import the required libraries\nimport os\n#import shutil\n\nimport pandas as pd\nimport numpy as np\nimport re\n\nimport cotools as co                   #COVID-19 Data Tools\n#from pprint import pprint\nimport pycountry\nimport geonamescache\nimport reverse_geocoder as rg\n#import sys\n\nimport plotly as py\nimport plotly.graph_objs as go\nimport plotly.express as px\nfrom IPython.display import display, HTML\nfrom ipywidgets import interact, interactive, fixed, interact_manual, widgets\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\nfrom mpl_toolkits.mplot3d import Axes3D\nimport matplotlib.pyplot as plt\n#import seaborn as sns\nimport pyLDAvis\nimport pyLDAvis.sklearn\n\n%matplotlib inline\n\n#please make sure that sklearn version is 0.22.2.post1\nfrom sklearn.feature_extraction.text import HashingVectorizer, TfidfVectorizer \nfrom sklearn.manifold import TSNE\n\nimport spacy\nfrom spacy_langdetect import LanguageDetector\nimport en_core_sci_lg\n\nfrom transformers import pipeline\n\nimport gc\nimport nltk\n#from nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import wordnet\n#nltk.download('wordnet')\n#nltk.download('averaged_perceptron_tagger')\n#nltk.download('punkt')","72f38dad":"pd.options.mode.chained_assignment = None","e7ca8741":"def log_progress(sequence, every=None, size=None, name='Items'):\n    \n    '''\n    Use the ipwidgets package to display a progress bar to give the user an indication\n    of the progress of code execution for certain portions of the notebook.\n    This function is intended to be used in the definition of a for loop to indicate\n    how far execution has gotten through the object being iterated over in the for loop.\n\n    Inputs: sequence - contains the for loop iteration (e.g., list or iterator)\n            every (integer) - number of steps to display\n\n    Outputs:  displays the progress bar in the notebook\n              yield record  - returns the current iteration object back to the calling for loop\n    \n    '''\n    \n    from ipywidgets import IntProgress, HTML, VBox\n    from IPython.display import display\n\n    # Determine the parameters for the progress bar based on the function inputs\n    is_iterator = False\n    if size is None:\n        try:\n            size = len(sequence)\n        except TypeError:\n            is_iterator = True\n    if size is not None:\n        if every is None:\n            if size <= 200:\n                every = 1\n            else:\n                every = int(size \/ 200)     # every 0.5%\n    else:\n        assert every is not None, 'sequence is iterator, set every'\n    \n    # Instantiate and display the progress bar        \n    if is_iterator:\n        progress = IntProgress(min=0, max=1, value=1)\n        progress.bar_style = 'info'\n    else:\n        progress = IntProgress(min=0, max=size, value=0)\n    label = HTML()\n    box = VBox(children=[label, progress])\n    display(box)\n    \n    # Update the progress bar state at each iteration of the for loop using this function\n    index = 0\n    try:\n        for index, record in enumerate(sequence, 1):\n            if index == 1 or index % every == 0:\n                if is_iterator:\n                    label.value = '{name}: {index} \/ ?'.format(\n                        name=name,\n                        index=index\n                    )\n                else:\n                    progress.value = index\n                    label.value = u'{name}: {index} \/ {size}'.format(\n                        name=name,\n                        index=index,\n                        size=size\n                    )\n                    \n            # return the current iteration object, preserving the state of the function\n            yield record\n    except:\n        progress.bar_style = 'danger'\n        raise\n    else:\n        progress.bar_style = 'success'\n        progress.value = index\n        label.value = \"{name}: {index}\".format(\n            name=name,\n            index=str(index or '?')\n        )\n        \n        \n\ndef process_text(text):\n    \n    '''\n    Use the spaCy library natural language processing capabilities to clean an input text, \n    in string format, for punctuation, stop words, and lemmatization.\n\n    Input:  text - a string to clean and lemmatize\n\n    Output  a modified version of the input string that has been cleaned by removing \n            punctuation, stop words, and pronouns, and has had the remaining words \n            converted into corresponding lemmas\n            \n    '''\n    \n    # Create a spaCy \"Doc\" object from the input text string.\n    doc = nlp(text.lower())\n    \n    result = [] # list that will contain the lemmas for each word in the input string\n    \n    for token in doc:\n        \n        if token.text in nlp.Defaults.stop_words:   #screen out stop words\n            continue\n        if token.is_punct:                          #screen out punctuations\n            continue\n        if token.lemma_ == '-PRON-':                #screen out pronouns\n            continue\n        \n        result.append(token.lemma_)\n    \n    # Return the lemmatized version of the cleaned input text string\n    return \" \".join(result)","1b411906":"# Set the path where the raw data is\ndata_dir = '\/kaggle\/input\/CORD-19-research-challenge'\n\n# Set the current working directory path to where the raw data is\nos.chdir(data_dir)\n\n# Set the path where the formatted data will be stored\noutput_dir = '\/kaggle\/working\/'\n\n# Read in the metadata.csv file as a pandas DataFrame\nmetadata_information = pd.read_csv('metadata.csv')","48750c1c":"metadata_information.shape","b51ca0b3":"def is_date(string, fuzzy=False):\n    \n    '''\n    Checks if string input can be interpreted as a date\n    \n    Input:  string - string to check whether it is a valid date\n \n    Output:  boolean - True if string is a valid date; False otherwise \n \n    '''\n    \n    from dateutil.parser import parse\n    \n    try: \n        parse(string, fuzzy=fuzzy)\n        return True\n\n    except ValueError:\n        return False","05d40735":"#print('Please input in the earliest date to filter the research paper (yyyy-mm-dd)!')\n#filter_date = str(input())\n#print('Would you like to only consider research papers published in journals (Y\/N)?')\n#from_journal_sources_only = str(input())\n\n# Modify this date per user requirements, or enter a non-valid date string to disable publication date filtering\nfilter_date = '2019-12-01'\nfrom_journal_sources_only = 'No'\n\n# paper_id_list is a list of the IDs for all papers published after the specified date\n# (or all papers if the date filtering is disabled).\n\nif is_date(filter_date) == True and from_journal_sources_only == 'Yes':\n    paper_id_list = metadata_information[(metadata_information['publish_time'] >= filter_date) & (metadata_information['journal'].notnull())].dropna(subset=['sha'])['sha'].tolist()\n\nelif is_date(filter_date) == True and from_journal_sources_only == 'No':\n    paper_id_list = metadata_information[metadata_information['publish_time'] >= filter_date].dropna(subset=['sha'])['sha'].tolist()\n\nelif is_date(filter_date) == False and from_journal_sources_only == 'Yes':\n    paper_id_list = metadata_information[metadata_information['journal'].notnull()].dropna(subset=['sha'])['sha'].tolist()\n    \nelse:\n    paper_id_list = metadata_information['sha'].tolist()","b05d4149":"def create_library(list_of_folders, list_of_papers = paper_id_list):\n\n    '''\n    Read JSON files for each paper from a list of subdirectories, and convert the JSON data for each paper\n    into a Python dictionary.\n\n    Inputs:  list_of_folders (list) - a list of subfolder names (strings) where the raw paper data is found\n             list_of_papers (list) - list of IDs for the target paper from which we want to extract the\n                                     relevant data for further analysis\n    Output:  internal_library (list) - list of dictionaries, where each dictionary contains the detailed\n                                       data for one paper\n    \n    '''\n    \n    import json\n    \n    internal_library = []     # list of dictionaries for the papers; each dictionary describes one paper \n\n    # Iterate through each subfolder with the raw paper data; use the log_progress()\n    # function to give the user an indication of execution progress. \n    for i in log_progress(list_of_folders, every = 1):\n\n        # Check two different sub-subfolders under each subfolder in the list of folders to find the\n        # papers - \"...\/pdf_json\" and \"...\/pmc_json\".\n        # For each paper, convert the JSON file into a Python dictionary (data{}).\n        # Add this dictionary to the list of dictionaries for all papers (internal_library[])\n        try:\n\n            pdf_file_path = data_dir + '\/' + i + '\/' + i + '\/pdf_json'\n            pdf_file_list = [i for i in os.listdir(pdf_file_path) if i.split('.')[0] in list_of_papers]\n            print('There are {a} papers in the {c} group after {b}.'.format(a = len(pdf_file_list), b = filter_date, c = str(i + str('_pdf'))))\n\n            for each_file in pdf_file_list:\n                file_path = data_dir + '\/' + i + '\/' + i + '\/pdf_json\/' + each_file\n\n                with open(file_path) as f:\n                    data = json.load(f)\n\n                internal_library.append(data)\n\n        except:\n            continue\n\n        try:\n\n            pmc_file_path = data_dir + '\/' + i + '\/' + i + '\/pmc_json'\n            pmc_file_list = [i for i in os.listdir(pmc_file_path) if i.split('.')[0] in list_of_papers]\n            print('There are {a} papers in the {c} group after {b}.'.format(a = len(pmc_file_list), b = filter_date, c = str(i + str('_pmc'))))\n\n            for each_file in pmc_file_list:\n                file_path = data_dir + '\/' + i + '\/' + i + '\/pmc_json\/' + each_file\n\n                with open(file_path) as f:\n                    data = json.load(f)\n\n                internal_library.append(data)\n\n        except:\n            continue\n            \n    return internal_library\n\n\n\ndef data_creation(list_of_folders, metadata, date = filter_date, list_of_papers = paper_id_list):\n    \n    '''\n    Extract the key data for each paper and create a combined DataFrame with this information\n   \n    Inputs: list_of_folders (list) - a list of subfolder names (strings) where the raw paper data is found\n            metadata (DataFrame) - metadata information on all papers\n            date (string) - publication date; only include papers published after this date\n            list_of_papers (list) - list of IDs for the target papers from which we want to extract the\n                                relevant data for further analysis\n    Outputs: test_df (DataFrame) - DataFrame containing the key data for each paper  \n    \n    '''\n    \n    # Use the create_library() function above to read the raw JSON data and create a list \n    # of dictionaries (internal_library). Each dictionary describes each paper in the raw data.\n    # Note that we are using the date-filtered list of paper IDs (paper_id_list) to target\n    # only those papers we want based on publication date (if the filtering is enabled).\n    internal_library = create_library(list_of_folders = selected_folders, list_of_papers = paper_id_list)\n\n    title_list = []          # list of paper titles (list of strings)\n    abstract_list = []       # list of paper abstracts (list of strings)\n    text_list = []           # list of paper full texts (list of strings)\n\n    # Extract title, abstract text, and body text for each paper\n    for i in list(range(0, len(internal_library))):\n\n        # Get the title from the \"metadata\" dictionary for each paper. The \"metatdata\" dictionary\n        # for each paper is a dictionary that contains title and author information.        \n        title_list.append(internal_library[i].get('metadata').get('title'))\n\n        # Use cord-19-tools package functions co.abstract() and co.text() to extract\n        # abstract text and body text from the paper dictionary        \n        try:\n            abstract_list.append(co.abstract(internal_library[i]))\n        except:\n            abstract_list.append('No Abstract')\n\n        text_list.append(co.text(internal_library[i]))\n\n        \n    # Extract paper ID information\n    paper_id = [i.get('paper_id') for i in internal_library]   # list of the ID for each paper\n\n    # Extract the location and country that published the research paper\n    primary_location_list = []      # list of the primary locations for the authors of each paper\n    primary_country_list = []       # list of the primary countries for the authors of each paper\n\n    \n    # Extract list of \"metadata\" dictionaries for each paper\n    internal_metadata = [i['metadata'] for i in internal_library]\n\n    # Extract the primary location and country for the authors of each paper\n    # individual_paper_metadata is the 'metadata' dictionary for one paper\n    for individual_paper_metadata in internal_metadata:\n\n        # Extract the list of author dictionaries for the current paper (one dictionary per author)\n        authors_information = individual_paper_metadata.get('authors')\n\n        if len(authors_information) == 0:\n            primary_location_list.append('None')\n            primary_country_list.append('None')\n\n        else:\n            location = None\n            country = None\n            i = 1\n\n            # Find the first author of the paper with valid data for location and country,\n            # extract this information, and add to the respective lists for all the papers\n            while location == None and i <= len(authors_information):\n\n                if bool(authors_information[i-1].get('affiliation')) == True:\n\n                    location = authors_information[i-1].get('affiliation').get('location').get('settlement')\n                    country = authors_information[i-1].get('affiliation').get('location').get('country')\n\n                i += 1\n\n            primary_location_list.append(location)\n            primary_country_list.append(country)\n                \n    \n    # Take all the information extracted for each paper and merge it into one combined DataFrame    \n    \n    # Create a DataFrame with one column - the paper ID for each paper\n    index_df = pd.DataFrame(paper_id, columns =  ['paper_id'])\n\n    # Create a DataFrame with two columns - the primary location and country of the authors for each paper\n    geographical_df = pd.DataFrame(primary_location_list, columns = ['Location'])\n    geographical_df['Country'] = primary_country_list\n\n    # Create a DataFrame with three columns - title, abstract text, and body text for each paper\n    paper_info_df = pd.DataFrame(title_list, columns = ['Title'])\n    paper_info_df['Abstract'] = abstract_list\n    paper_info_df['Text'] = text_list\n    \n    # Concatenate the above three DataFrames into a single combined DataFrame\n    combined_df = pd.concat([index_df, geographical_df, paper_info_df], axis = 1)\n    \n    # Extract sha (paper ID), abstract text, url, and publication date for each paper from the metadata\n    # DataFrame passed as input to this function\n    part_1 = metadata[['sha', 'abstract', 'url', 'publish_time']]\n\n    # Merge the information from the metadata DataFrame with the DataFrame with the raw paper data\n    test_df = combined_df.merge(part_1, left_on = ['paper_id'], right_on = ['sha'], how = 'left')\n    test_df.drop(['sha'], axis = 1,inplace = True)\n    test_df = test_df[['paper_id', 'url', 'publish_time', 'Location', 'Country', 'Title', 'Abstract', 'abstract', 'Text']]\n    \n    # In the event where the JSON's abstract is null but there is an abstract in the \n    # metadata, make the substitution\n    test_df['Abstract'] = np.where(test_df['Abstract'] == '', test_df['abstract'], test_df['Abstract'])\n    test_df.drop(['abstract'], axis = 1, inplace = True)\n    \n    gc.collect()\n    \n    return test_df","c5ab9f82":"# Define as a list the names of all the subdirectories under the \"\/kaggle\/input\/CORD-19-research-challenge\"\n# directory where the dataset files are stored\nselected_folders = ['comm_use_subset', 'noncomm_use_subset', 'custom_license', 'biorxiv_medrxiv']\n\n# Call the data_creation() function to extract the desired data from each paper and create a single,\n# combined DataFrame, test_df\ntest_df = data_creation(list_of_folders = selected_folders, metadata = metadata_information)","8615ab76":"test_df.to_csv(output_dir + 'Checkpoint_1.csv', index = False)","63dc1813":"# Cleaning up after each section to save space\ndel paper_id_list\ndel metadata_information\ndel selected_folders\n\nimport gc\ngc.collect()","2ab4acca":"def cleaning_dataset(dataset, columns_to_clean):\n    \n    '''\n    Clean text of specified columns in DataFrame\n\n    Inputs:  dataset (DataFrame) - Dataframe to clean\n             columns to clean (list) - list of columns in the DataFrame for which we want to clean the text\n\n    Output:  cleaned DataFrame \n    \n    '''    \n    \n    # each_column is one of the defined columns from the DataFrame\n    # Use the log_progress() helper function defined above to indicate the progress of the execution\n    for each_column in log_progress(columns_to_clean, every = 1):\n\n        # Fill in any null text items with \"No Information\"\n        dataset[each_column] = dataset[each_column].fillna('No Information')\n\n        # Remove square-bracketed references (i.e., [1])\n        dataset[each_column] = dataset[each_column].apply(lambda x: re.sub(r'\\[.*?]', r'', x))\n\n        # Remove parenthesis references (i.e., (1))\n        dataset[each_column] = dataset[each_column].apply(lambda x: re.sub(r'\\((.*?)\\)', r'', x))\n\n        # Remove garbage characters\n        dataset[each_column] = dataset[each_column].apply(lambda x: re.sub(r'[^a-zA-z0-9.%\\s-]', r'', x))\n\n        # Remove unnecessary white space\n        dataset[each_column] = dataset[each_column].apply(lambda x: re.sub(r' +', r' ', x))\n\n        # Remove unnecessary white space at the end of the text section\n        dataset[each_column] = dataset[each_column].apply(lambda x: x.rstrip())\n\n        # Remove white space before punctuation marks\n        dataset[each_column] = dataset[each_column].apply(lambda x: re.sub(r'\\s([?.!\"](?:\\s|$))', r'\\1', x))\n\n        \n    cleaned_abstract = []     # list of cleaned abstracts for all the papers\n    abstract_count = []       # list of the word counts for each paper abstract\n\n    # Clean up abstracts as abstracts may contain unnecessary starting words like 'background' or 'abstract'\n    # Count the words in each cleaned abstract and add the list of abstract word counts for each paper to\n    # the test_df Data Frame\n    #\n    # i is the abstract text (string) for one paper\n    for i in dataset['Abstract']:\n\n        if i.split(' ')[0].lower() == 'background' or i.split(' ')[0].lower() == 'abstract':\n            cleaned_abstract.append(' '.join(i.split(' ')[1:]))\n            abstract_count.append(len(i.split(' ')[1:]))\n\n        else:\n            cleaned_abstract.append(i)\n            abstract_count.append(len(i.split()))\n\n    dataset['Abstract'] = cleaned_abstract\n    dataset['Abstract Word Count'] = abstract_count\n\n    # Removing the words figure X.X from the passages because it contributes no meaning\n    fig_exp = re.compile(r\"Fig(?:ure|.|-)\\s+(?:\\d*[a-zA-Z]*|[a-zA-Z]*\\d*|\\d*)\", flags=re.IGNORECASE) \n    dataset['Text'] = [(re.sub(fig_exp, '', i)) for i in test_df['Text']]\n\n    # Remove other instances of poor references and annotations\n    poor_annotation_exp_1 = re.compile(r'(\\d)\\s+(\\d]*)', flags = re.IGNORECASE)\n    dataset['Text'] = [(re.sub(poor_annotation_exp_1, '', i)) for i in test_df['Text']]\n\n    poor_annotation_exp_2 = re.compile(r'(\\d])*', flags = re.IGNORECASE)\n    dataset['Text'] = [(re.sub(poor_annotation_exp_2, '', i)) for i in test_df['Text']]\n    \n    gc.collect()\n    \n    return dataset","8482c2a0":"text_columns = ['Title', 'Abstract', 'Text']\ntest_df = cleaning_dataset(dataset = test_df, columns_to_clean = text_columns)","31ab0035":"test_df['Abstract'].describe(include='all')","a2c6b187":"test_df.drop_duplicates(['Abstract', 'Text'], inplace = True)","2ab671cb":"test_df['Text'].describe(include = 'all')","68762de8":"# Make sure the country name entries in the DataFrame are strings\ntest_df['Country'] = test_df['Country'].astype(str)\n\n# Extract the country name list from the DataFrame so we can work with it\ncountry_list = test_df['Country'].tolist()\n\n# Clean up the country names\n\n# Substitute \"None\" for any null entries\n# new_items is now a temporary list of the country names\nnew_items = ['None' if x == 'nan' else x for x in country_list]\n\n# Remove garbage characters, brackets and parentheses, extra white space, etc.\nnew_items = [re.sub(r'\\[.*?]', r'', x) for x in new_items]\nnew_items = [re.sub(r'\\((.*?)\\)', r'', x) for x in new_items]\nnew_items = [x.split(',')[0] for x in new_items]\nnew_items = [re.sub(r'[^a-zA-z\\s-]', r'', x) for x in new_items]\nnew_items = [re.sub(r' +', r' ', x) for x in new_items]\nnew_items = [re.sub(r'\\[\\[|\\]\\]', '', x) for x in new_items]\nnew_items = [x.strip('[').strip(']') for x in new_items]\n\n# Ensure that the country names are the uniform, proper English names for each country\nnew_items = ['mexico' if x.lower() == 'mxico' else x for x in new_items]\nnew_items = ['brazil' if x.lower().rstrip() == 'brasil' else x for x in new_items]\nnew_items = ['china' if x.lower() == 'china-japan' or x.lower() == 'prchina' or x.lower() == 'prc' or x.lower() == 'china-australia' else x for x in new_items]\nnew_items = ['united kingdom' if x.lower() == 'united-kingdom' else x for x in new_items]\nnew_items = ['tunisia' if x.lower() == 'tunisie' else x for x in new_items]\nnew_items = ['russia' if x.lower() == 'runion' or x.lower() == 'ussr' else x for x in new_items]\nnew_items = ['senegal' if x.lower() == 'sngal' else x for x in new_items]\nnew_items = ['spain' if x.lower() == 'espaa' else x for x in new_items]\nnew_items = ['slovakia' if x.lower() == 'czechoslovakia' else x for x in new_items]\nnew_items = ['usa' if x.lower() == 'ljsa' else x for x in new_items]\nnew_items = ['germany' if x.lower() == 'w-germany' or x.lower() == 'deutschland' else x for x in new_items]\nnew_items = ['belgium' if x.lower() == 'belgique' else x for x in new_items]\nnew_items = ['slovenia' if x.lower() == 'yugoslavia' else x for x in new_items]\nnew_items = ['italy' if x.lower() == 'italien' else x for x in new_items]\nnew_items = ['emirates' if x.lower() == 'uae' else x for x in new_items]\nnew_items = ['india' if x.lower() == 'india-' else x for x in new_items]\n\n# Change to \"None\" any entries that are not actually names of a country\nnew_items = ['None' if x.lower() == 'umrs' or x.lower() == 'frg' or x.lower() == 'university' or x.lower() == 'maroc' or x.lower() == 'universidade' or x.lower() == 'ucbl' or x.lower() == 'telephone' or x.lower() == 'mcgovern' or x.lower() == 'school' or x.lower() == 'professor' else x for x in new_items]","9475f102":"country_list = []    # list of cleaned and ISO standard country names for all the papers\n\n# Use the pycountry package to find the official ISO standard country names for each\n# entry in the temporary list of cleaned country names (new_items). Once the list\n# is updated with the standard names, replace this column in the text_df Data Frame\n#\n# new_items is a temporary list of the cleaned country names\n# Use the log_progress() helper function defined above to indicate the progress of the execution\nfor i in log_progress(new_items, every = 1):\n    \n    try:\n        if len(i.split()) > 1:\n            list_to_try = i.split()\n\n            for x in list_to_try:\n                try:\n                    country = pycountry.countries.search_fuzzy(x)[0].name\n\n                except:\n                    continue\n\n            country_list.append(country)\n\n        else:\n            country = pycountry.countries.search_fuzzy(i)[0].name\n            country_list.append(country)\n            \n    except:\n        country_list.append('None')\n        \ntest_df['Country'] = country_list","53d5ddae":"test_df.to_csv(output_dir + 'Checkpoint_2.csv', index = False)","4de9a717":"# Cleaning up after each section to save space\ndel text_columns\ndel country_list\ndel new_items\n\ngc.collect()","3d397bc7":"graphing_data = pd.DataFrame(test_df.groupby(['Country']).count()['paper_id']).reset_index()\n\ndata = dict (type = 'choropleth',\n             locations = graphing_data['Country'],\n             locationmode = 'country names',\n             colorscale = 'viridis', reversescale = True,\n             z = graphing_data['paper_id'])\n\nmap = go.Figure(data=[data])\n\nmap.update_layout(\n    title_text = 'Break down of Research Papers by Countries',\n)\n\nmap.show()","3dd576b3":"# Use geonamescache Python library to get city names, their longitude and latitude coordinates, and map\n# the cities to their respective states.\n#\ngeoname = geonamescache.GeonamesCache()\nglobal_city_dictionary = geoname.get_cities()\n\n# List of US city names\nus_cities_list = [global_city_dictionary[code]['name'] for code in list(global_city_dictionary.keys()) if global_city_dictionary[code]['countrycode'] == 'US']\n\n# List of longitudes for each city\nus_cities_lng_list = [global_city_dictionary[code]['longitude'] for code in list(global_city_dictionary.keys()) if global_city_dictionary[code]['countrycode'] == 'US']\n\n# List of latitudes for each city\nus_cities_lat_list = [global_city_dictionary[code]['latitude'] for code in list(global_city_dictionary.keys()) if global_city_dictionary[code]['countrycode'] == 'US']\n\n\n\ndef get_states(longitude, latitude):\n    \n    '''\n    Return a list of US states corresponding to input lists of longitude and latitude coordinates\n\n    Inputs:  longitude (list) - list of longitude coordinates\n             latitude (list) - list of corresponding latitude coordinates\n\n    Outputs: us_states (list) - list of US states corresponding to longitude and latitude coordinates\n\n    '''\n    \n    coord_list = list(zip(latitude, longitude))     # list of longitude-latitude coordinate pairs\n    \n    # Use reverse_geocoder Python package to get location information (closest city\/town, \n    # country, US state, etc.) from longitude and latitude coordinates.\n    # The search() method returns a dictionary with the location information for each set of \n    # coordinates in the list. If the location is in a US state, the \"admin1\" dictionary\n    # item should have the state name.\n    results = rg.search(coord_list)\n    \n    us_states = []\n            \n    for i in results:\n        try:\n            state = i['admin1']     # retrieving state information\n        except:\n            state = ''              # return empty string if there is no information\n        \n        us_states.append(state)\n    \n    return us_states\n\n\n# Call the above get_states() function to get the list of states corresponding to the list\n# of US city longitude and latitude coordinates\nus_states_list = get_states(us_cities_lng_list, us_cities_lat_list)\n\n# Create dictionary mapping for US city names to states\ncity_to_state = {}\nfor city, state in zip(us_cities_list, us_states_list):\n    if state:\n        city_to_state[city] = state","c2084d96":"del geoname; del global_city_dictionary\ndel us_cities_list; del us_cities_lng_list; del us_cities_lat_list; del us_states_list","d904e07b":"# Extract rows from test_df for which the paper \"Country\" is the US\ngraphing_data_2 = test_df[test_df['Country'] == 'United States']\n\n# For a paper where the primary author country is in the US, the \"Location\" column should have \n# the city name and state name or abbreviation\nlocation_list = [city_to_state.get(i) for i in graphing_data_2['Location']]\ngraphing_data_2.loc[:,'Location'] = location_list\n\n# Count the papers by location (US state)\ngraphing_data_2 = graphing_data_2.groupby(['Location']).count()['paper_id'].reset_index()\n\n# Create a dictionary mapping of US state names to their postal abbreviations\nstate_dict = {'Alabama': 'AL', 'Alaska': 'AK', 'American Samoa': 'AS', 'Arizona': 'AZ', 'Arkansas': 'AR',\n              'California': 'CA', 'Colorado': 'CO', 'Connecticut': 'CT',\n              'Delaware': 'DE', 'District of Columbia': 'DC',\n              'Florida': 'FL',\n              'Georgia': 'GA', 'Guam': 'GU',\n              'Hawaii': 'HI',\n              'Idaho': 'ID', 'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA',\n              'Kansas': 'KS', 'Kentucky': 'KY',\n              'Louisiana': 'LA',\n              'Maine': 'ME', 'Maryland': 'MD', 'Massachusetts': 'MA', 'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS', 'Missouri': 'MO', 'Montana': 'MT',\n              'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH', 'New Jersey': 'NJ', 'New Mexico': 'NM', 'New York': 'NY', 'North Carolina': 'NC', 'North Dakota': 'ND', 'Northern Mariana Islands':'MP',\n              'Ohio': 'OH', 'Oklahoma': 'OK', 'Oregon': 'OR',\n              'Pennsylvania': 'PA', 'Puerto Rico': 'PR',\n              'Rhode Island': 'RI',\n              'South Carolina': 'SC', 'South Dakota': 'SD',\n              'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT',\n              'Vermont': 'VT', 'Virgin Islands': 'VI', 'Virginia': 'VA',\n              'Washington': 'WA', 'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY'\n}\n\n# Create the graph that shows paper counts by US state\ngraphing_data_2['Code'] = graphing_data_2['Location'].map(state_dict)\n\nfig = go.Figure(data=go.Choropleth(\n    locations=graphing_data_2['Code'],     # spatial coordinates\n    z = graphing_data_2['paper_id'],       # data to be color-coded\n    locationmode = 'USA-states',           # set of locations match entries in `locations`\n    colorscale = 'viridis', reversescale = True,\n))\n\nfig.update_layout(\n    title_text = 'Break down of Research Papers by US States',\n    geo_scope='usa',                       # limited map scope to USA\n)\n\nfig.show()","e7c229d9":"#Cleaning up after each section to save space\ndel state_dict\ndel city_to_state\ndel location_list\ndel map; del graphing_data\ndel fig; del graphing_data_2\n\ngc.collect()","96355235":"test_df.dropna(subset = ['Text'], inplace = True)","c587e817":"def dimension_reduction(dataset, n = 3, n_components = 3, use_hashing_vectorizer = False):\n\n    '''\n    Obtain TF-IDF scores based on n-grams (features) for text in the input dataset. Then use\n    t-SNE dimensionality reduction to project the TF-IDF scores on a lower-dimensional space\n    that can be visualized\n\n    Inputs:  dataset (DataFrame) - a dataset with text for multiple documents\n             n (integer) - defines the n-gram dimension to use in the TF-IDF analysis\n             n_components (integer) - defines the dimensionality of the space to project to using t-SNE\n             use_hashing_vectorizor (bool) - gives the option to use a different text vectorizing method\n\n    Outputs:  tsne_df (DataFrame) - contains t-SNE output data that can be visualized\n \n    '''\n    \n    dataset = dataset.reset_index().drop(['index'], axis = 1)\n    \n    # Extract Trigram vectors for all papers in our set and obtain TDF-IDF scores \n    # using the scikit-learn TfidfVectorizer class\n    if use_hashing_vectorizer == False:\n    \n        vectorizer=TfidfVectorizer(ngram_range=(n,n))\n        vectorized_vectors=vectorizer.fit_transform(dataset['Text'].tolist())\n        \n    else:\n        \n        vectorizer=HashingVectorizer(ngram_range=(n,n))\n        vectorized_vectors=vectorizer.fit_transform(dataset['Text'].tolist())\n\n    # Use t-SNE dimensionality reduction (reduce to three dimensions) to identify outliers\n    tsne_reduction = TSNE(n_components = 3, perplexity = 10, learning_rate = 100, random_state = 777)\n    tsne_data = tsne_reduction.fit_transform(vectorized_vectors)\n\n    # Convert first 3 components of T-SNE into DataFrame for 3-D visualization\n    tsne_df = pd.DataFrame(tsne_data, columns = [i for i in range(0, tsne_data.shape[1])])\n    \n    gc.collect()\n    \n    return tsne_df\n\n\n\ndef visualizing_dimensions(dataset):\n\n    '''\n    Create a 3-D plot using the data in the input dataset DataFrame\n\n    Input:  dataset (DataFrame) - contains the data to visualize\n\n    Output:  the function does not return anything per se, but generates a 3-D plot based on the input data\n   \n    '''\n\n    fig = plt.figure(1, figsize=(7, 5))\n    ax = Axes3D(fig, rect=[0, 0, .95, 1], elev=48, azim=134)\n\n    ax.scatter(dataset[0], dataset[1], dataset[2], c=dataset[2], cmap='viridis', linewidth=0.5)\n\n    ax.set_xlabel('Component A')\n    ax.set_ylabel('Component B')\n    ax.set_zlabel('Component C')\n\n    plt.show()\n    gc.collect()\n    \n    \n    \ndef outlier_removals(dim_reduced_dataset, dataset, n_components = 3, number_std_dev = 2.5, verbose = 1):\n    \n    '''\n    Remove outlier items from the larger input dataset based on the the metrics in the \n    input dim_reduced_dataset (implicitly this is assumed to be scores for the text in \n    dataset). The criteria for identifying outliers is an item whose score in \n    dim_reduced_dataset is more than number_std_dev away from the mean score in\n    any of the axes of dim_reduced_dataset\n\n    Inputs:  dim_reduced_dataset (DataFrame) - contains metrics or scores in n_components \n                                               dimensions (reduced) for items in dataset\n             dataset (DataFrame) - larger dataset for which we want to remove outliers \n                                   based on the scores in dim_reduced_dataset\n             n_components (integer) - number of dimensions in dim_reduced_dataset\n             number_std_dev (float) - criteria for identifying outlier scores\n             verbose (int) - if = 1, function prints out additional information\n\n    Output:  cleaned_df (DataFrame) - based on input dataset, but with the identified outlier items removed\n  \n    '''\n    \n    outlier_papers = []\n    print('{a} standard deviation is being used to clean the dataset.'.format(a = number_std_dev))\n    print()\n    \n    # Identify outlier text based on dim_reduced_dataset scores\n    for i in range(0, n_components):\n        \n        # Create upper and lower bounds for outliers as the mean +\/- number_std_dev for each dimension\n        upper = dim_reduced_dataset[i].mean() + number_std_dev*dim_reduced_dataset[i].std()\n        lower = dim_reduced_dataset[i].mean() - number_std_dev*dim_reduced_dataset[i].std()\n\n        # Extract the outlier items based on the upper and lower bounds and get the index values\n        outlier_df = dim_reduced_dataset[(dim_reduced_dataset[i] >= upper) | (dim_reduced_dataset[i] <= lower)]\n        outlier_list = outlier_df.reset_index()['index'].tolist()\n        \n        outlier_papers += outlier_list\n    \n    # List of outlier item index values\n    outlier_papers = list(set(outlier_papers))\n    \n    # Report how many outliers are identified\n    if verbose == 1:\n        print('There are {a} outlier papers identified.'.format(a = len(outlier_papers)))\n        print()\n        \n    # Extract the outlier items from the input dataset (all columns)     \n    outlier_papers_df = dataset.iloc[outlier_papers,:]\n    \n    # Display the text of the outlier items\n    if verbose == 1:\n        print('These are the texts that are determined as abnormal.')\n        print()\n        for i in outlier_papers_df['Text']:\n            print(i)\n            print()\n    \n    # Remove the outliers from the input dataset\n    cleaned_df = dataset.drop(outlier_papers, axis = 0)\n    cleaned_df.reset_index().drop(columns = ['index'], axis = 1)\n    \n    gc.collect()\n    \n    return cleaned_df\n\n\n\ndef full_cleaning_process(dataset, n = 3, n_components = 3, use_hashing_vectorizer = False, std_dev = 3, verbose = 1):\n    \n    '''\n    Execute a cleaning process on the input dataset, where outliers are \n    identified based on TF-IDF analysis and t-SNE dimensionality reduction, \n    and then removed from the dataset\n\n    Inputs:  dataset (DataFrame) - set of text items for which we want to \n                                   remove outliers\n             n (integer) - defines the n-gram dimension to use in the TF-IDF \n                           analysis\n             n_components (integer) - defines the dimensionality of the space \n                                      to project to using t-SNE\n             use_hashing_vectorizor (bool) - gives the option to use a different \n                                             text vectorizing method\n             number_std_dev (float) - criteria for identifying outlier scores\n             verbose (int) - if = 1, function prints out additional information\n\n    Output:  output_df (DataFrame) - based on input dataset, but with the identified \n             outlier items removed the function also displays 3-D plots before \n            and after outlier removal\n         \n    '''\n    \n    starting_datashape = dataset.shape[0]     # number of items before cleaning\n    \n    # Complete TF-IDF analysis and t-SNE dimensionality reduction on the input dataset\n    dim_reduced_dataset = dimension_reduction(dataset, n = n, n_components = n_components, use_hashing_vectorizer = use_hashing_vectorizer)\n    print('Before Cleaning Up -')\n    \n    # Show the 3-D plot of scores before removing outliers\n    visualizing_dimensions(dim_reduced_dataset)\n    \n    # Identify and remove outliers \n    output_df = outlier_removals(dim_reduced_dataset, dataset, n_components = n_components, number_std_dev = std_dev, verbose = verbose)\n    \n    ending_datashape = output_df.shape[0]     # number of items after cleaning\n    \n    print('{a} rows were dropped in this cleaning process.'.format(a = starting_datashape - ending_datashape))\n    print()\n    \n    # Show the 3-D plot of scores after removing outliers\n    print('After Cleaning Up -')\n    visualizing_dimensions(dimension_reduction(output_df, n = 3, n_components = 3, use_hashing_vectorizer = False))\n    \n    gc.collect()\n    \n    return output_df     # cleaned DataFrame with outlier items removed","42f2b368":"test_df = full_cleaning_process(test_df, std_dev = 2.5)","f4c09597":"# Set the minimum word count for text of papers we want to keep\nminimum_word_count = 150\n\n# Reset the index for the DataFrame and drop the \"index\" column that gets created as part of the reset\ntest_df = test_df.reset_index().drop(['index'], axis = 1)\n\n# Create a new column that contains the word count for each paper\ntest_df['Text Word Count'] = [len(i.split()) for i in test_df['Text']]\n\ndirty_list = []     # list of paper indexes that have a word count <= minimum_word_count\n\nfor index, value in test_df.iterrows():\n    \n    if (value['Text Word Count'] <= minimum_word_count):\n        dirty_list.append(index)\n\n# Extract the papers that contain less than the minimum word count        \nweird_papers_df = test_df.iloc[dirty_list,:]\n\n# Print the text of those papers\nfor index, value in weird_papers_df.iterrows():\n    print(value['Text Word Count'], value['Text'])\n    print()","e46f6cbc":"test_df = test_df.drop(dirty_list, axis = 0)\ntest_df = test_df.reset_index().drop(['index'], axis = 1)","2315941c":"visualizing_dimensions(dimension_reduction(test_df, n = 3, n_components = 3, use_hashing_vectorizer = False))","eea0fb38":"test_df.to_csv(output_dir + 'Checkpoint_3.csv', index = False)","fe43257a":"#Cleaning up after each section to save space\ngc.collect()","faf7cd37":"# Load the spaCy en_core_sci_lg English biomedical language model\nnlp = en_core_sci_lg.load()\n\n# Add language detection to the spaCy Natural Language Processing pipeline\nnlp.add_pipe(LanguageDetector(), name=\"language_detector\", last=True)","99e08401":"language_list = []     # list of primary languages and match scores for each paper\n\n# For each paper in the cleaned dataset, run the text through the spaCy NLP pipeline, and\n# add the detected language and match score to language_list[] (use the log_progress() helper\n# function to provide the user with a progress bar).\nfor i in log_progress(test_df['Text'], every = 1):\n    \n    # To avoid exceeding memory allocation limits, we limit the text set to the pipeline\n    # to 1,000,000 characters. This should be more than sufficient to get a good\n    # language match.    \n    if len(i) <= 1000000:\n    \n        doc = nlp(i)\n        language_list.append(doc._.language)\n        \n    else:\n        \n        cut_off_index = i[:1000000].rfind('.')\n        focus_i = i[:cut_off_index + 1]\n        \n        doc = nlp(focus_i)\n        language_list.append(doc._.language)","8cc7a9f1":"# Convert the language name\/abbreviation into upper case\nfiltered_language_list = [i['language'].upper() for i in language_list]\n\n# Add the language for each paper to the test_df DataFrame\ntest_df['Language'] = filtered_language_list\n\n# Filter out only research papers in English to perform topic modelling.\nenglish_df = test_df[test_df['Language'] == 'EN']\nprint('There are {a} research papers in English out of {b} research papers.'.format(a = english_df.shape[0], b = test_df.shape[0]))","430aa169":"cleaned_lemma_list = []     # list of lemmas for the text of each paper; each list item is a string with the\n                            # lemmas for the text of the paper, separated by a blank space\n\n# Use the spaCy NLP library to lemmatize the paper text.\n# Use the log_progress() helper function defined above to indicate the progress of the execution.\nfor i in log_progress(english_df['Text'], every = 1):\n    \n    # Variable length for efficiency\n    nlp.max_length = len(i)\n\n    # Create spaCy \"Doc\" for the text of each paper\n    doc = nlp(i)\n    \n    # Extract the lemma for each token and join in one text string\n    cleaned_lemma_list.append(\" \".join([token.lemma_ for token in doc]))\n    \nenglish_df['Lem Text'] = cleaned_lemma_list","a360c308":"import ktrain\nktrain.text.preprocessor.detect_lang = ktrain.text.textutils.detect_lang\n\nmax_vocab = int(english_df['Text Word Count'].describe().max())     # set the maximum vocabulary size\nnum_of_topics = int((len(english_df['Text Word Count']) \/ 2)**0.5)  # set the number of topics\n#max_vocab = 10000\n\n# Run topic modeling using Keras wrappers on LDA\ntm = ktrain.text.get_topic_model(english_df['Lem Text'], n_topics = num_of_topics, n_features = max_vocab, lda_max_iter = 25, lda_mode = 'batch')","1b7597ed":"tm.print_topics()","cc621e61":"%%time\nthreshold_value = 0.25\ntm.build(english_df['Lem Text'], threshold = threshold_value)","344c71dd":"topic_list = []              # list of topic numbers for the primary topic for each paper\ntopic_words_list = []        # list of the words for the primary topic of each paper\n\n# Iterate through the lemmatized paper texts\n# Use the log_progress() helper function to give the user a progress indication on the execution\nfor i in log_progress(english_df['Lem Text'], every = 1):\n    \n    # Identify the most likely topic or primary topic for the paper text\n    topic_list.append(np.argmax(tm.predict([i])))\n    \n    # Identify the string of words for the most likely topic for the paper text\n    topic_words_list.append(tm.topics[np.argmax(tm.predict([i]))])\n\n# Add the list of most likely topics for each paper, and the corresponding topic word string to english_df     \nenglish_df['Topic Number'] = topic_list\nenglish_df['List of Topics'] = topic_words_list","5cc30ddc":"tm.save(output_dir + 'Topic Model')","5723a3cc":"def get_random_colors(n, name='hsv', hex_format=True):\n\n    '''\n    Returns an array that maps each index in 0, 1, ..., n-1 to a distinct\n    RGB color; the keyword argument name must be a standard mpl colormap name.\n    \n    '''\n\n    from matplotlib.colors import rgb2hex\n\n    cmap = plt.cm.get_cmap(name, n)\n    result = []\n    for i in range(n):\n        color = cmap(i)\n        if hex_format: color = rgb2hex(color)\n        result.append(color)\n    return np.array(result)\n\n\n\ndef visualize_documents(texts=None, doc_topics=None, \n                        width=700, height=700, point_size=5, title='Document Visualization',\n                        extra_info={},\n                        colors=None,\n                        filepath=None,):\n    '''\n    Generates a visualization of a set of documents based on a topic model.\n    If <texts> is supplied, raw documents will be first transformed into \n    document-topic matrix.  If <doc_topics> is supplied, then this will be \n    used for visualization instead.\n        \n    Inputs:  texts(list of str) - list of document texts.  \n                                  Mutually-exclusive with <doc_topics>\n             doc_topics(ndarray) - pre-computed topic distribution for each \n                                   document in texts.\n                                   Mutually-exclusive with <texts>.\n             width(int) - width of image\n             height(int) - height of image\n             point_size(int) - size of circles in plot\n             title(str) - title of visualization\n             extra_info(dict of lists) - A user-supplied information for each  \n                                             datapoint (attributes of the datapoint).\n                                             The keys are field names.  The values are \n                                             lists - each of which must be the same \n                                             number of elements as <texts> or <doc_topics>. \n                                             These fields are displayed when hovering over \n                                             datapoints in the visualization.\n            colors(list of str) - list of Hex color codes for each datapoint.\n                                  Length of list must match either len(texts) or doc_topics.shape[0].\n            filepath(str) - Optional filepath to save the interactive visualization\n            \n    Output:  visualization displayed to stdout\n        \n    '''\n    \n    # error-checking\n    if texts is not None: \n        length = len(texts)\n    else: \n        length = doc_topics.shape[0]\n\n    if colors is not None and len(colors) != length:\n        raise ValueError('length of colors is not consistent with length of texts or doctopics')\n    if texts is not None and doc_topics is not None:\n        raise ValueError('texts is mutually-exclusive with doc_topics')\n    if texts is None and doc_topics is None:\n        raise ValueError('One of texts or doc_topics is required.')\n    if extra_info:\n        invalid_keys = ['x', 'y', 'topic', 'fill_color']\n        for k in extra_info.keys():\n            if k in invalid_keys:\n                raise ValueError('cannot use \"%s\" as key in extra_info' %(k))\n            lst = extra_info[k]\n            if len(lst) != length:\n                raise ValueError('texts and extra_info lists must be same size')\n\n    # check fo bokeh\n    try:\n        import bokeh.plotting as bp\n        from bokeh.plotting import save\n        from bokeh.models import HoverTool\n        from bokeh.io import output_notebook\n    except:\n        warnings.warn('visualize_documents method requires bokeh package: pip3 install bokeh')\n        return\n\n    # prepare data\n    if doc_topics is not None:\n        X_topics = doc_topics\n    else:\n        if tm.verbose:  print('transforming texts...', end='')\n        X_topics = tm.predict(texts, harden=False)\n        if tm.verbose: print('done.')\n\n    # reduce to 2-D\n    if tm.verbose:  print('reducing to 2 dimensions...', end='')\n    tsne_model = TSNE(n_components=2, verbose=tm.verbose, random_state=777, angle=.99, init='pca')\n    tsne_lda = tsne_model.fit_transform(X_topics)\n    print('done.')\n\n    # get random colormap\n    colormap = get_random_colors(tm.n_topics)\n\n    # generate inline visualization in Jupyter notebook\n    lda_keys = tm._harden_topics(X_topics)\n    if colors is None: colors = colormap[lda_keys]\n    topic_summaries = tm.get_topics(n_words=5)\n    os.environ[\"BOKEH_RESOURCES\"]=\"inline\"\n    output_notebook()\n    dct = { \n            'x':tsne_lda[:,0],\n            'y':tsne_lda[:, 1],\n            'topic':[topic_summaries[tid] for tid in lda_keys],\n            'fill_color':colors,}\n    tool_tups = [('index', '$index'),\n                 ('(x,y)','($x,$y)'),\n                 ('topic', '@topic')]\n    for k in extra_info.keys():\n        dct[k] = extra_info[k]\n        tool_tups.append((k, '@'+k))\n\n    source = bp.ColumnDataSource(data=dct)\n    hover = HoverTool( tooltips=tool_tups)\n    p = bp.figure(plot_width=width, plot_height=height, \n                  tools=[hover, 'pan', 'wheel_zoom', 'box_zoom', 'reset'],\n                  title=title)\n    p.circle('x', 'y', size=point_size, source=source, fill_color= 'fill_color')\n    bp.show(p)\n    if filepath is not None:\n        bp.output_file(filepath)\n        bp.save(p)\n    return","4afd50c7":"text_information = {'Paper': english_df['paper_id'].astype(str).tolist(), 'Topic': english_df['Topic Number'].astype(str).tolist(),'Title': english_df['Title'].tolist()}\nvisualize_documents(texts = english_df['Lem Text'].tolist(), extra_info = text_information)","4e8b98b3":"dtm = tm.vectorizer.fit_transform(english_df['Lem Text'].tolist()) \nLDAvis_prepared = pyLDAvis.sklearn.prepare(tm.model, dtm, tm.vectorizer)\n#pyLDAvis.enable_notebook()\n#pyLDAvis.display(LDAvis_prepared, template_type='notebook')\n#display(pyLDAvis.display(LDAvis_prepared))\npyLDAvis.save_html(LDAvis_prepared, output_dir + 'lda_visualization.html')","1a3ffdde":"os.chdir('\/kaggle\/working')","19579968":"graphing_data_3 = english_df[['Topic Number', 'List of Topics']] \ngraphing_data_3 = graphing_data_3.assign(Keywords=graphing_data_3['List of Topics'].str.split(' ')).explode('List of Topics')\ngraphing_data_3 = graphing_data_3.explode('Keywords')\ngraphing_data_3 = graphing_data_3[['Topic Number', 'Keywords']]\n\ngraphing_data_3 = graphing_data_3.groupby(['Topic Number', 'Keywords']).size().reset_index(name='Count')\ngraphing_data_3 = graphing_data_3.sort_values(by='Count', ascending=False)\n\nfig = px.bar(graphing_data_3, y=\"Topic Number\", x=\"Count\", color='Keywords', orientation = 'h', height = 1000)\nfig.show()","03f3d3f9":"test_df = test_df.reset_index()\nenglish_df = english_df[['Lem Text', 'Topic Number', 'List of Topics']].reset_index() #This works because english_df is a subset of the main df (Lem Text)\n\ntest_df = test_df.merge(english_df, on = 'index', how = 'left')","c9291fbe":"test_df.to_csv(output_dir + 'Checkpoint_4.csv', index = False)","8cd15d2a":"final_value_list = []     # list of the occurrence count for the most frequent topic word for each paper\nfinal_topic_list = []     # list of the topic word with the maximum occurrence count for each paper\n\n# Iterate over the rows of test_df (i.e., each paper)\nfor index, value in test_df.iterrows():\n    \n    count_value_list = []     # list where each item is the number of times each topic word for a paper\n                              # appears in the paper lemmatized text\n    \n    count_topic_list = []     # list of the topic words for a paper\n    \n    # The \"List of Topics\" column contains the most likely topic word string for each paper\n    if pd.isnull(value['List of Topics']) == True:\n        \n        final_value_list.append(None)\n        final_topic_list.append(None)\n        \n    else:\n    \n        # Split the topic word string for each paper into a list of the individual words\n        list_of_topics = value['List of Topics'].split()\n\n        # each_topic is one of the topic words of the topic for the paper\n        for each_topic in list_of_topics:\n\n            # Count the number of times the each topic word appears in the paper lemmatized text string\n            count_value = value['Lem Text'].count(each_topic)\n            \n            # Add the count for the current topic word to the list\n            count_value_list.append(count_value)\n            \n            # Add the current topic word to the list\n            count_topic_list.append(each_topic)\n\n        # Get the highest occurrence count for all the topic words for the current paper   \n        max_value = max(count_value_list)\n        \n        # Get the index of the max_value in the count value list (maps to the topic word with the highest\n        # occurrence in the paper text)\n        max_index = count_value_list.index(max_value)\n        \n        # Get the topic word that has the highest occurrence count in the paper text\n        max_topic = count_topic_list[max_index]\n\n        # Add the topic word that occurs most frequently in the paper text, and the corresponding\n        # occurrence count, to the appropriate list\n        final_value_list.append(max_value)\n        final_topic_list.append(max_topic)\n        \n# Add a column to test_df that has the single topic word that occurs most frequently in the paper\n# text; this is classified as the sub-topic for the paper\ntest_df['Sub-Topic'] = final_topic_list","99a582e2":"top_N = 50\na = test_df['List of Topics'].str.cat(sep=' ')\nwords = nltk.tokenize.word_tokenize(a)\nword_dist = nltk.FreqDist(words)\nrslt = pd.DataFrame(word_dist.most_common(top_N), columns=['Keywords', 'Frequency'])","1f515137":"del a; del words; del word_dist","421c78d0":"d = {}\nfor a, x in rslt.values:\n    d[a] = x\n\ndef f(word_count):\n    \n    '''\n    Generate a WordCloud visualization with a specified \n    number of words\n    \n    '''\n    \n    wordcloud = WordCloud(width=1600,height=800,background_color=\"white\",max_words=word_count)\n    wordcloud.generate_from_frequencies(frequencies=d)\n    plt.figure(figsize=(20,8), facecolor='k')\n    plt.imshow(wordcloud, interpolation=\"bilinear\")\n    plt.axis(\"off\")\n    plt.show()","d990430f":"widgets.interact_manual(f, word_count=widgets.IntSlider(min=10, max=50, step=5, value=20))#This is purely for publishing purpose to showcase interactivity\nwidgets.interact(f, word_count=widgets.IntSlider(min=10, max=50, step=5, value=20))","6c0cd153":"fig = px.bar(rslt, y='Keywords', x='Frequency', orientation = 'h',\n             color='Frequency', color_continuous_scale='viridis',\n             labels={'pop':'Count of Keywords'}, height=600)\nfig.show()","be76ddcf":"import plotly.offline as py_offline\n\npy_offline.init_notebook_mode(connected = True)\n\ndef update_plot(Frequency):\n    \n    '''\n    Generate a plot of term frequency for the papers in the dataset\n    \n    '''\n    \n    filtered_df = rslt.query('Frequency> ' + str(Frequency))\n    data = [go.Bar(x = filtered_df.Keywords,\n                   y = filtered_df.Frequency)\n                   ]\n    layout = go.Layout(\n        xaxis = dict(title = 'Keywords'),\n        yaxis = dict(title = 'Frequency'),\n        title = f'Words with Frequency greater than {Frequency}')\n    chart = go.Figure(data = data, layout = layout, )\n    py_offline.iplot(chart, filename = 'Keyword Frequency')\n\nwidgets.interact_manual(update_plot, Frequency=widgets.IntSlider(min=100, max=1000, step=100, value=200)) #This is purely for publishing purpose to showcase interactivity\nwidgets.interact(update_plot, Frequency=widgets.IntSlider(min=100, max=1000, step=100, value=200))","308b5975":"# Cleaning up after each section to save space\ndel language_list; del filtered_language_list\ndel cleaned_lemma_list\ndel max_vocab; del num_of_topics\ndel topic_list; del topic_words_list\ndel text_information\ndel dtm\ndel LDAvis_prepared\ndel graphing_data_3\ndel english_df\ndel count_value_list; del count_topic_list\ndel final_topic_list; del final_value_list\ndel list_of_topics\n\ngc.collect()","e34d3b1a":"def creating_search_field(question):\n    \n    '''\n    Take in a question and create a search field (list of search words and synonyms) \n    to identify relevant papers\n    \n    Input:  question (string) - input question from the search engine query\n    \n    Output: search field synonyms (list) - list of search words with associated synonyms\n    \n    '''\n    \n    # Call the process text() helper function to clean and lemmatize the question string.\n    # task_question is a spaCy \"Doc\" object created from the string lemmatized question text\n    task_question = nlp(process_text(question))\n    \n    search_field = []     # list of search words extracted from the input question\n    \n    # Examine each token in the question, and if the token is a noun or a verb, add this\n    # as a search word to the search_field[] list\n    for token in task_question:\n        if token.pos_ == 'NOUN' or token.pos == 'VERB':\n            search_field.append(token.lemma_)\n            \n    search_field_synonyms = []     # list of synonyms for the search words\n    \n    # i is one of the search words in the search_field[] list\n    for i in search_field:\n        \n        # Use the Python nltk wordnet library to generate synonyms for each search word\n        syns = wordnet.synsets(i)\n        for x in syns:\n            search_field_synonyms.append(x.name().split('.')[0])\n    \n    # Combine the synonyms and the original search word list into a single list      \n    search_field_synonyms = list(set(search_field_synonyms + search_field))\n    \n    # Due to the limited number of papers that have the top keywords of 'coronavirus' and 'covid-19', \n    # we create an artifical list to widen search. This will not be necessary as number of papers increases.\n    if 'coronavirus' or 'covid-19' or 'covid19' or 'covid' or 'sars' in search_field_synonyms:\n        \n        search_field_synonyms.append('cov')\n        search_field_synonyms.append('sars-cov-2')\n        search_field_synonyms.append('sars-cov')\n        search_field_synonyms.append('pdcov')\n        search_field_synonyms.append('mers')\n        search_field_synonyms.append('sars')\n        search_field_synonyms.append('mers-cov')\n        search_field_synonyms.append('ncov')\n    \n    return search_field_synonyms\n\ndef search_database(dataset, search_field_synonyms, area_of_search = 'Sub-Topic'):\n    \n    '''\n    Create a DataFrame consisting of only the relevant papers based on a list of search words\n    \n    Inputs:  dataset (DataFrame) - set of papers to extract the relevant ones based on search words\n             search_field_synonyms (list) - list of search words (strings)\n             area_of_search (string) - column in dataset to compare against the search words to\n                                       identify relevant papers\n\n    Output:  search_df (DataFrame) - set of relevant papers based on the search words\n    \n    A paper is considered relevant if its associated sub-topic matches one of the\n    search words.\n    \n    '''\n    \n    search_df = pd.DataFrame()     # set of relevant papers based on the search words\n    \n    # i is one of the search words\n    for i in search_field_synonyms:\n        \n        # Iterate over the non-null values in the target column of dataset\n        for x in filter(None,dataset[area_of_search].unique().tolist()):\n            \n            if i in x:\n                \n                # search word matches paper - extract corresponding row of dataset\n                subset_df = dataset[dataset[area_of_search] == x]\n                \n                # add this row to the set of relevant papers\n                search_df = pd.concat([search_df, subset_df], axis = 0)\n    \n    # Create a list of the relevant paper indexes\n    index_list = search_df['index'].unique().tolist()\n    \n    # Reset the index for the set of relevant papers\n    search_df.reset_index().drop(['index'], axis = 1, inplace = True)\n   \n    # Fill in any holes in the \"url\" column of the set of relevant papers\n    search_df['url'].fillna(value = 'None', inplace = True) \n    \n    # Create a list of the topic numbers from the topic model for the set of relevant papers\n    topic_list = search_df['Topic Number'].unique().tolist()\n    \n    # If the set of relevant papers is less than 50, add any papers not already in the set\n    # with a topic number matching any of the topic numbers of the papers in the relevant set.\n    if search_df.shape[0] <= 50:\n        \n        subset_df = dataset[dataset['Topic Number'].isin(topic_list)]\n        subset_df = subset_df[~subset_df['index'].isin(index_list)]\n        \n        search_df = pd.concat([search_df, subset_df], axis = 0)\n    \n    gc.collect()\n    \n    return search_df\n\n\n\ndef search_engine(dataset, question):\n    \n    '''\n    Compare the input question with the set of relevant papers to find the best matching papers\n    \n    Inputs:  dataset (DataFrame) - contains only the set of relevant papers corresponding to the\n                                   input question\n             question (string) - question we want to answer from the set of relevant papers\n            \n    Outputs: dataset (DataFrame) - modified set of relevant papers that has been scored and sorted\n    \n    For each paper in the set, we evaluate each sentence of the paper text for similarity to the\n    input question using the spaCy Doc object similarity score. We then select the sentence\n    with the best score, along with the sentences before and after for context. We add\n    the information on the selected sentence and score to the input dataset and sort the papers\n    in the set based on the scores.\n    \n    '''\n        \n    prior_sentence_list = []        # list of sentences before the selected sentence for each paper\n    selected_sentence_list = []     # list of selected sentences for each paper (best similarity score)\n    posteriori_sentence_list = []   # list of sentences after the selected sentence for each paper\n    \n    sentence_score_list = []        # list of best similarity score for each paper\n    sentence_position_list = []     # list of selected sentence position for each paper\n    \n    document_score_list = []        # list of overall scores for each paper\n    document_length_list = []       # list of the number of sentences for each paper\n    \n    # Call the process_text() helper function to clean and lemmatize the question string.\n    # task_question is a spaCy \"Doc\" object created from the string lemmatized question text\n    task_question = nlp(process_text(question))\n    \n    # Reset the index on the input dataset\n    dataset = dataset.reset_index().drop(['index'], axis = 1)\n\n    # i is the text for one paper (string)\n    # Use the log_progress() helper function to give the user a progress indication on the execution\n    for i in log_progress(dataset['Text'], every = 1):\n        \n        sentence_list = []                # list of sentences (strings) from the current paper text\n        sentence_similarity_list = []     # list of similarity scores for each sentence\n        position_list = []                # list of positions for each sentence (1,  2, 3, etc.)\n        \n        # Performing Sentence Boundary Detection\n        nlp.max_length = len(i)*1.1\n        \n        # document is a spaCy \"Doc\" object from the paper sentence text\n        document = nlp(i)\n        \n        count = 1     # keep track of sentence position (first sentence, second sentence, etc.)\n        \n        # Count the number of sentences in the paper text\n        number_of_sentences_in_document = len(list(document.sents))\n        \n        # Perform sSemantic Search - for each sentence, get similarity score relative to the input question\n        # each_sentence is one sentence in the paper text\n        for each_sentence in document.sents:\n              \n            # Call the process_text() helper function to clean and lemmatize the current sentence.\n            # sentence_search is a spaCy \"Doc\" object created from the string lemmatized sentence text\n            # for the current sentence            \n            sentence_search = nlp(process_text(str(each_sentence)))\n            \n            if (sentence_search.vector_norm):\n                \n                # Get similarity score for each sentence relative to the input question\n                similarity_score = task_question.similarity(sentence_search)\n                \n                # Add the sentence text, similarity score, and position to the appropriate lists\n                sentence_list.append(each_sentence)\n                sentence_similarity_list.append(similarity_score)\n                position_list.append(count)\n                \n            count += 1     # increment sentence position counter\n        \n        # Identify the maximum similarity score for all the sentences in the current document\n        max_similarity_score = max(sentence_similarity_list)\n        \n        # Identify the position of the sentence in the current document with the maximum score\n        pos_max_similarity_score = sentence_similarity_list.index(max_similarity_score)\n\n        # Get the sentence with the best score and its position in the document\n        max_similarity_sentence = sentence_list[pos_max_similarity_score]\n        max_position = position_list[pos_max_similarity_score]\n\n        # Compute an average score for the current paper\n        overall_score = round(sum(sentence_similarity_list) \/ len(sentence_similarity_list),4)\n        \n        # Identify sentences before and after the selected sentence for\n        # each paper to provide contextual understanding.\n        # Handle various edge cases in addition to the normal case\n        if len(sentence_list) == 1:                                         # edge case 1\n\n            start_sentence = ''\n            end_sentence = ''\n\n        elif len(sentence_list) == 2 and max_position == position_list[0]:  # edge case 2\n\n            start_sentence = ''\n            end_sentence = sentence_list[pos_max_similarity_score + 1]\n\n        elif len(sentence_list) == 2 and max_position == position_list[-1]: # edge case 3\n\n            start_sentence = sentence_list[pos_max_similarity_score - 1]\n            end_sentence = ''\n\n        elif len(sentence_list) > 2 and max_position == position_list[0]:   # edge case 4\n\n            start_sentence = ''\n            end_sentence = sentence_list[pos_max_similarity_score + 1]\n\n        elif len(sentence_list) > 2 and max_position == position_list[-1]:  # edge case 5\n\n            start_sentence = sentence_list[pos_max_similarity_score - 1]\n            end_sentence = ''\n\n        else:                                                               # normal case\n\n            start_sentence = sentence_list[pos_max_similarity_score - 1]\n            end_sentence = sentence_list[pos_max_similarity_score + 1]\n        \n        \n        # Add all the information to the appropriate lists defined above\n        \n        prior_sentence_list.append(start_sentence)\n        selected_sentence_list.append(max_similarity_sentence)\n        posteriori_sentence_list.append(end_sentence)\n\n        sentence_score_list.append(max_similarity_score)\n        sentence_position_list.append(max_position)\n\n        document_score_list.append(overall_score)\n        document_length_list.append(number_of_sentences_in_document)\n     \n    # Add columns to the set of relevant papers based on the search and scoring results\n    dataset['Top Sentence'] = selected_sentence_list\n    dataset['Sentence Score'] = sentence_score_list\n    dataset['Document Score'] = document_score_list\n    dataset['Top Sentence Location'] = sentence_position_list\n    dataset['Start Sentence'] = prior_sentence_list\n    dataset['End Sentence'] = posteriori_sentence_list\n    dataset['Document Length'] = document_length_list\n\n    # Sort the set of relevant papers by document score and sentence score values (best scores first)\n    dataset = dataset.sort_values(by=['Document Score', 'Sentence Score'], ascending=False).reset_index().drop(['index'], axis = 1)\n    \n    gc.collect()\n    \n    return dataset\n\n\n\ndef generating_best_sentence(dataset):\n    \n    '''\n    From the scored and sorted set of papers relative to the input question, identify the\n    paper with the best matching sentence, and extract certain key information from that paper\n    \n    Inputs:  dataset (DataFrame) - the set of papers that have been scored\n                                   and sorted for best matching sentences \n                                   \n    Outputs: top_paper_url   - url of the selected paper (string)\n             top_paper_title - title of the selected paper (string)\n             top_score       - best sentence score of the selected paper (float)\n             top_sentence    - best sentence of the selected paper (string)\n             top_position    - best sentence position in the selected paper (integer)\n             start_sentence  - sentence before the best sentence in the selected paper (string)\n             end_sentece     - sentence after the best sentence in the selected paper (string)\n    \n    '''\n    \n    # Identify the paper from the set that has the best matching sentence for the input question\n    row = dataset[dataset['Sentence Score'] == dataset['Sentence Score'].max()].reset_index()\n    \n    # Extract the information we want about the best matching paper\n    \n    top_score = row['Sentence Score'][0]\n    top_sentence = row['Top Sentence'][0]\n    top_position = row['Top Sentence Location'][0]\n\n    start_sentence = row['Start Sentence'][0]\n    end_sentence = row['End Sentence'][0]\n    \n    top_paper_url = row['url'][0]\n    top_paper_title = row['Title'][0]\n    \n    gc.collect()\n    \n    return top_paper_url, top_paper_title, top_score, top_sentence, top_position, start_sentence, end_sentence\n\n\n\ndef generating_summariaries(dataset, top_n = 10):\n    \n    '''\n    Take the top matching papers from the larger scored and sorted set of papers, and\n    add a text summary for each paper using the spaCy NLP pipeline\n    \n    Inputs:  dataset (DataFrame) - the set of papers that have been scored\n                                   and sorted for best matching sentences \n             top_n (integer) - number of matching papers to identify from the input set\n    \n    Output: focus_df(DataFrame) - set of the top top_n matching papers with summaries\n                                  for each paper added to the paper data\n    '''\n\n    # Take the first top_n papers from the scored and sorted set of papers \n    focus_df = dataset.head(top_n)\n    \n    # Instantiate a summarizer in the spaCy pipeline\n    summarizer = pipeline(task=\"summarization\", model=\"bart-large-cnn\")\n    \n    summary_list = []     # list of summaries (text strings) for the top top_n papers\n\n    # For each paper in the top top_n set, generate summary text for the paper\n    # Use the log_progress() helper function to give the user a progress indication on the execution\n    for i in log_progress(focus_df['Text'], every = 1):\n        \n        summary = summarizer(i)\n        summary_list.append(summary[0]['summary_text'])\n    \n    \n    # Add the summary text for each paper in the top top_n set as a new column\n    focus_df['Summary'] = summary_list\n    \n    gc.collect()\n    \n    return focus_df\n\n\n\ndef generating_answers(dataset, question):\n    \n    '''\n    Take the top matching papers from the larger scored and sorted set of papers, and\n    generate an answer (text statement) to the input question, based on the abstract\n    and body texts for the selected papers, using the spaCy NLP pipeline\n    \n    Inputs:  dataset (DataFrame) - the set of papers that have been scored\n                                   and sorted for best matching sentences\n             question (string) - input question from the search engine query\n                         \n    Output:  printout_answer (string) - a text statement that gives the best answer\n                                        to the input question that we can extract \n                                        from the set of papers.\n    \n    '''\n    \n    # Re-sort the input dataset by the sentence score for each paper\n    focus_df = dataset.sort_values(by=['Sentence Score'], ascending=False).head(150)\n    \n    # Instantiate a question-answering task in the spaCy pipeline\n    question_answer = pipeline(task=\"question-answering\")\n    \n    ret_list = []    # list of abstract text, the top sentence, and the sentences before and after\n                     # for the papers in focus_df\n    \n    for i in list(zip(focus_df['Abstract'].astype(str), focus_df['Start Sentence'].astype(str), focus_df['Top Sentence'].astype(str), focus_df['End Sentence'].astype(str))):\n        ret_list.append(' '.join(i))\n        \n    # Join all the paper abstract texts and selected sentences together in one string to provide\n    # as input to the spaCy question-answering task    \n    context_generation = ' '.join(ret_list)\n\n    # Get the text answer and score (how good does spaCy think the answer is) \n    # generated for the input question\n    generated_answer = question_answer({'question': question, 'context': context_generation})['answer'].capitalize()\n    generated_score = question_answer({'question': question, 'context': context_generation})['score']\n    \n    # Generate the answer text to return\n    if float(generated_score) >= 0.5:\n        printout_answer = str('I found a good answer to your question - ') + generated_answer\n        \n    elif float(generated_score) < 0.5 and float(generated_score) >= 0.3:\n        printout_answer = str('Please try to refine your question. In the meantime, here you go - ') + generated_answer\n    \n    else:\n        printout_answer = str('I am thoroughly confused. But here is my best answer - ') + generated_answer\n    \n    gc.collect()\n    \n    return printout_answer\n\n\n\ndef in_depth_display(row_dataset):\n    \n    '''\n    Take one paper from the set of top top_results matching papers and use \n    the spaCy NLP library to identify entities present in the paper\n    \n    Input:  row_dataset (DataFrame) - one paper from the set of top top_results\n                                      matching papers\n    \n    Output:  print to stdout the list of entities identified in the paper\n    \n    '''\n    \n    ent_list = []     # list of entities identified in the paper (list of strings)\n\n    # Create a spaCy \"Doc\" object for the body text of the paper\n    nlp.max_length = len(row_dataset['Text'])\n    doc = nlp(row_dataset['Text']) \n    #displacy.render(doc, style='ent')\n    \n    # Extract each entity from the \"Doc\" object and add it to the paper entity list\n    for ent in doc.ents:\n        ent_list.append(ent.text)\n        \n    cleaned_ent_list = [i.lower() for i in ent_list]\n    \n    gc.collect()\n    \n    # Sort the list of entities and print it to stdout as a single string\n    print(', '.join(sorted(list(set(ent_list)))))\n\n    \n    \ndef generate_report(dataset, search_question = 'What do we know about COVID-19 risk factors?', top_results = 5):\n    \n    '''\n    Generate a report of the search engine results for an input question. This is the main function\n    that is called to exercise the search engine.\n\n    Inputs:  dataset (DataFrame) - set of papers to analyze to answer the question\n             search_question (string) - question which we want to answer from the dataset\n             top_results (integer) - number of relevant papers we want to identify from the dataset\n\n    Outputs:  report (printed to stdout) that includes the best answer to the question and a\n              short list of relevant papers\n              \n    The output report includes the following:\n    \n       - the input question\n       - the best answer we can get for the question (text statement)\n       - the number of papers found to be relevant to the question\n       - the single best matching paper for the question:\n           * paper url\n           * paper title\n           * top highlights from the paper (best matching sentence in context)\n       - top top_results papers that best match the question; for each paper:\n           * url\n           * overall score\n           * title\n           * key sentence in context\n           * summary\n           * list of entities\n    \n    '''\n    \n    question = search_question\n    nlp = en_core_sci_lg.load()\n    \n    # Generate the list of search words for the input question (creating_search_field() function above)\n    search_list = creating_search_field(question)\n    \n    # Generate the set of relevant papers based on the search words (search_database() function above)\n    search_df = search_database(dataset, search_list)\n    \n    # Get the scored and sorted set of the relevant papers (search_engine() function above)\n    result_df = search_engine(search_df, question)\n    \n    # Get the key information from the paper with the best matching sentence to the input\n    # question (generating_best_sentence() function above)\n    top_paper_url, top_paper_title, top_score, top_sentence, top_position, start_sentence, end_sentence = generating_best_sentence(result_df)    \n    \n    # Select the top top_results matching papers, and add a text summary for each\n    # paper to the paper data (generating_summaries() function above)\n    output_df = generating_summariaries(result_df, top_n = top_results)\n    \n    # Generate the best answer (text statement) to the input question\n    # (generating_answers() function above)\n    printout_answer = generating_answers(result_df, question) \n    \n    # Print the output report\n    \n    print('Question : {q1}'.format(q1 = question))\n    print('Answer : {q2}'.format(q2 = printout_answer))\n    print()\n    print(\"``````````````````````````````````````````````````````````````````````````````````````````````````\")\n    print('{a} out of {b} Research Papers are related to question'.format(a = search_df.shape[0], b = dataset.shape[0]))\n    print()\n    print('Paper URL : {x}'.format(x = top_paper_url))\n    print('Title of Paper : {f}'.format(f = top_paper_title))\n    print('Top Highlights : [{c}] | Sentence {d} | {e1} \\033[1;34;47m {e2} \\033[0m {e3}'.format(c = round(top_score,4), d = top_position, e1 = start_sentence, e2 = top_sentence, e3 = end_sentence))\n    print()\n    print(\"``````````````````````````````````````````````````````````````````````````````````````````````````\")\n    print()\n\n    counter = 1\n    \n    while counter <= top_results:\n\n        print(str(counter) + '.')\n        print('Paper URL : {x}'.format(x = output_df['url'][counter - 1]))\n        print('Paper Overall Score : {y}'.format(y = output_df['Document Score'][counter - 1]))\n        print('Title of Paper : {f}'.format(f = output_df['Title'][counter - 1]))\n        print('Key Sentence : [{g}] | Sentence {h1} out of {h2} | {i1} \\033[1;34;47m {i2} \\033[0m {i3}'.format(g = round(output_df['Sentence Score'][counter - 1],4), h1 = output_df['Top Sentence Location'][counter - 1], h2 = output_df['Document Length'][counter - 1], i1 = output_df['Start Sentence'][counter - 1], i2 = output_df['Top Sentence'][counter - 1], i3 = output_df['End Sentence'][counter - 1]))\n        print()\n        print('Summary of Paper: {z}'.format(z = output_df['Summary'][counter - 1]))\n        print()\n        print('List of Entities in Paper: ')\n        in_depth_display(output_df.iloc[counter - 1])\n        print()\n        counter += 1\n        \n    gc.collect()","72be385d":"test_df = test_df.drop(columns=['Lem Text'])","4d15462c":"nlp = en_core_sci_lg.load()   # Scientific NLP Library","848fe6c6":"def widget_interaction(task_question):\n    print('Loading your Search!')\n    generate_report(test_df, search_question = task_question, top_results = 5)\n    print()\n    print('Completed!')\n    \n#Just to show interactive ability when published on Kaggle <- IGNORE this if you are running on your local systems\nwidgets.interact_manual(widget_interaction,\n                         task_question= ['What is known about coronavirus transmission, incubation, and environmental stability?',\n                                        'What is the incubation period for coronavirus in humans?',\n                                        'How does the incubation periods for coronavirus varies with age and health?',\n                                        'How long does a human remain contagious with coronavirus?',\n                                        'How long does a human remain contagious with coronavirus after recovery?',\n                                        'What is the prevalence of coronavirus asymptomatic shedding?',\n                                        'What is the prevalence of coronavirus asymptomatic shedding in children?',\n                                        'What is the prevalence of coronavirus transmission in human?',\n                                        'What is the prevalence of coronavirus transmission in children?',\n                                        'How does seasonality affect coronavirus transmission?',\n                                        'What is the physical science behind coronavirus?',\n                                        'What is the charge distribution of coronavirus?',\n                                        'What is coronavirus adhesive ability to hydrophilic or phobic surfaces?',\n                                        'What is coronavirus environmental survivability?',\n                                        'What is coronavirus viral shedding ability?',\n                                        'What is the persistence of coronavirus on surfaces of different material?',\n                                        'What is the persistence of coronavirus on copper, stainless steel and plastic?',\n                                        'What is the persistence and stability of coronavirus on various substrates and sources?',\n                                        'What is the persistence and stability of coronavirus in nasal discharge?',\n                                        'What is the persistence and stability of coronavirus in sputum?',\n                                        'What is the persistence and stability of coronavirus in urine and fecal matter?',\n                                        'What is the persistence and stability of coronavirus in blood?',\n                                        'What is the history of coronavirus?',\n                                        'What is the process of coronavirus from shedding to infecting?',\n                                        'What is the process of coronavirus from an infected person?',\n                                        'What is the implementation of diagnostics for coronavirus?',\n                                        'What products can be used to improve clinical processes for coronavirus?',\n                                        'What is the disease models from humans and animal is similar to coronavirus infection, disease and transmission?',\n                                        'What tools and studies exist to monitor phenotypic change and potential adaptation of coronavirus?',\n                                        'What immune response and immunity are there to coronavirus?',\n                                        'What is the effectiveness of movement control strategies to prevent secondary coronavirus transmission in health care and community settings?',\n                                        'What is the effectiveness of personal protective equipment against coronavirus to reduce risk of transmission in health care and community settings?',\n                                        'What is the role of the environment in the transmission of coronavirus?'])\n    \nwidgets.interact(widget_interaction,\n                 task_question= ['What is known about coronavirus transmission, incubation, and environmental stability?',\n                                'What is the incubation period for coronavirus in humans?',\n                                'How does the incubation periods for coronavirus varies with age and health?',\n                                'How long does a human remain contagious with coronavirus?',\n                                'How long does a human remain contagious with coronavirus after recovery?',\n                                'What is the prevalence of coronavirus asymptomatic shedding?',\n                                'What is the prevalence of coronavirus asymptomatic shedding in children?',\n                                'What is the prevalence of coronavirus transmission in human?',\n                                'What is the prevalence of coronavirus transmission in children?',\n                                'How does seasonality affect coronavirus transmission?',\n                                'What is the physical science behind coronavirus?',\n                                'What is the charge distribution of coronavirus?',\n                                'What is coronavirus adhesive ability to hydrophilic or phobic surfaces?',\n                                'What is coronavirus environmental survivability?',\n                                'What is coronavirus viral shedding ability?',\n                                'What is the persistence of coronavirus on surfaces of different material?',\n                                'What is the persistence of coronavirus on copper, stainless steel and plastic?',\n                                'What is the persistence and stability of coronavirus on various substrates and sources?',\n                                'What is the persistence and stability of coronavirus in nasal discharge?',\n                                'What is the persistence and stability of coronavirus in sputum?',\n                                'What is the persistence and stability of coronavirus in urine and fecal matter?',\n                                'What is the persistence and stability of coronavirus in blood?',\n                                'What is the history of coronavirus?',\n                                'What is the process of coronavirus from shedding to infecting?',\n                                'What is the process of coronavirus from an infected person?',\n                                'What is the implementation of diagnostics for coronavirus?',\n                                'What products can be used to improve clinical processes for coronavirus?',\n                                'What is the disease models from humans and animal is similar to coronavirus infection, disease and transmission?',\n                                'What tools and studies exist to monitor phenotypic change and potential adaptation of coronavirus?',\n                                'What immune response and immunity are there to coronavirus?',\n                                'What is the effectiveness of movement control strategies to prevent secondary coronavirus transmission in health care and community settings?',\n                                'What is the effectiveness of personal protective equipment against coronavirus to reduce risk of transmission in health care and community settings?',\n                                'What is the role of the environment in the transmission of coronavirus?'])","ff79caef":"#### 3.3.3.2 Geographical Distribution of Papers\n\nNow that we have a reasonably cleaned dataset, we want to examine the geographical distribution of where the papers originate. This will give a view of where in the world the most research on COVID-19 and related coronaviruses is being done currently, based on the latest dataset.\n\nThe following code examines the data and counts the number of papers by country. The graphic below displays the results.","83af5024":"![46292966.PNG](attachment:46292966.PNG)","f9630a22":"#### 3.3.1.1 Load Checkpoint 1\n\nIf for some reason you want to reload the Checkpoint 1 version of the \"test_df\" DataFrame from the file, use the following code:\n\n```test_df = pd.read_csv(output_dir + 'Checkpoint1.csv')```","255e62fd":"### 3.3.5 Identifying and Removing Papers whose Primary Language is not English\n\nIt is possible that the dataset at this point contains papers that are not in English. As a final cleaning step, we want to remove from the dataset papers whose primary language is not English. This ensures that the subsequent machine learning techniques we employ to help address the questions for Task 1 are performed on a dataset that is as clean as possible.\n\nThe following code uses the *spaCy* Python package to detect the primary language of each paper. Since we are dealing with biomedical data, we use the *spaCy* model *en_core_sci_lg*, which is an English model specifically designed for biomedical data, with a larger vocabulary and 600k word vectors. For each paper in the cleaned dataset, the code uses the *spaCy* language detection feature to detect the primary language. It also provides a score (0 to 1; 1 being the best match) for the language it selected. These two pieces of information are then added to our cleaned DataFrame. Finally, the English language papers are extracted and placed in a new DataFrame, \"english_df\". This is the DataFrame that we will use in the subsequent processing to provide some insight on the Task 1 questions.","dae36316":"## 1.3 Results Preview\n\nThis section presents sample output of the search engine implemented in [Section 4](#4.-Search-Engine \"Search Engine\").\n\nUse the following list to help interpret key fields in the sample search results shown in [Figure 1](#Figure_1).\n\n**Note**: All scores are based on a range of 0 to 1 where 0 indicates the paper has no probability of answering the query and 1 indicates the paper has the best probability of answering the query.\n- Question: This field displays the question that the search engine is acting on. When the notebook is downloaded, [Section 4](#4.-Search-Engine \"Search Engine\") provides a dropdown menu that allows the user to select from a list of questions related to this challenge. This interaction is not possible once the notebook has been uploaded to Kaggle.\n- Answer: This field displays the best response resulting from the search. The line of text below this field shows the number of research papers that were searched to find the answer. In this example, 1053 research papers were searched.\n- Paper URL: This field provides a link to the research paper, if available.\n- Top Highlights: This field contains a score that is an indicator of how closely related the best matched research paper is to answering the question. The key sentence, the sentence in the paper the best provides insights on the answer, is high-lighted, and the sentences that appear before and after are provided for context. \n- Paper overall score: A score that indicates how well suited the paper is for answering the query.\n- Key sentence score: A score that indicates how well the high-lighted sentence answers the query.\n- Summary of Paper: This field contains a code generated summary of the paper.\n- List of Entities in Paper: This field provides all of the keywords found in the paper.","9c64c985":"#### 3.2.2.2 Creating the Preprocessed Dataset \n\nFrom the output of the metadata_information.shape method shown below, we see that the metadata file has information on 51078 papers, but the raw data in the subdirectories includes JSON files for 59,311 papers. Thus, not all the papers in raw data are present in the metadata file. For our analysis, we will use the papers that have JSON files, as this should provide more contextual understanding and answers for researchers.\n**Note**: 51,078 indicates the number of rows in the resulting output file while 18 represents the number of columns. Each row represents a paper and each column represents the metadata associated with that paper.","ad75c66b":"### 3.3.4 Identifying and Removing Papers that are Outliers\n\nTo further clean the dataset, we want to remove any papers that would be classified as outliers. \nAn outlier would be a paper that:\n\n- has no text\n- has text that does not appear to be relevant to the corpus as a whole, based on TF-IDF scoring\n- has less than 150 words of text\n\nThe following code identifies and removes outlier papers based on these criteria.","9235de93":"![Task1_banner.jpg](attachment:Task1_banner.jpg)\n","f9ea3c31":"### 3.4.1 Summary Visualization - LDA of papers\n\nThe following are visualizations of all research papers written in English and their similarities among different topics.","b572b692":"## 3.4 Topic Modelling\n\nNow that we have a cleaned and lemmatized (and possibly filtered by publication date) dataset of papers, we are ready to build a topic model. We use the *ktrain* Python library to do this. The topic modeling proceeds as follows:\n\n#### Train the Model to Discover the Topics in the Dataset\n\nThe first step is to train the model to discover the topics in our dataset, using the Latent Dirichlet Allocation (LDA) algorithm in the *ktrain* library. Note that the number of topics we specify is defined by an empirical formula that loosely follows Zhao, et al., \"A heuristic approach to determine an appropriate number of topics in topic modeling\", in *Proceedings 12th Annual MCBIOS Conference*, March 2015.\n\nOnce the topics are discovered, we can examine them using the ```print_topics()``` method. This gives us some idea of the latent meaning behind each topic.","6ef447bb":"![](http:\/\/)","6c3f15b3":"Now we can execute the outlier removal cleaning process on our current version of the dataset, \"test_df\". ","ed0f8df7":"# <center>[>> PYLDA Visualization <<](lda_visualization.html)<\/center>","617ef738":"#### Lemmatize the Text of the English Papers\n\nAs a final step, we want to lemmatize the text for the English papers in our dataset. To accomplish this, we use the *spaCy\" natural language processing library to create a \"Doc\" object from the text of each paper. We then extract the lemmas of the text for each paper, and create a string for each paper, which consists of the lemmas for the each word of the paper text, separated by a single space. The result of the code below is a list of lemmas (strings) for the text of each paper, which is added as a column to the \"english_df\" DataFrame.","e7e70d6d":"#### 3.3.4.1 Identifying and Removing Papers with No Body Text\n\nThe first step is straightforward - we simply use the *pandas* ```.dropna()``` method to drop any papers that have a null value in the \"Text\" column (paper full text) of our DataFrame.","68d2368c":"### 3.4.4 Summary Visualization - Common Terms in the Research Papers","ce7973f9":"At this point, we want to save our topic model - we use the ```save()``` method for this. This gives us the possibility of reloading the model for future analysis, rather than re-training the model again (assuming that the dataset has not changed). To reload the model, use ```tm = ktrain.text.load_topic_model('<path to the saved model>')```.","9b1aa7b5":"#### 3.3.4.4 Checkpoint 3\n\nBefore continuing, we again save our current version of the \"test_df\" DataFrame as \"\/kaggle\/working\/Checkpoint 3.csv\", and perform a memory cleanup.","04e8e4a1":"> ### 3.3.3 Cleaning the Country Names\n\nThe country names, as extracted from the raw data, are not necessarily the uniform English names - they sometimes are abbreviations, misspellings, the name in a different language, include garbage characters, etc. We want to clean up these country name strings, and also update the country names so that they match the ISO 3166 standard names. The following code cleans up the country fields and updates the names to the ISO standard using the *pycountry* Python package.","c4feb609":"#### 3.3.4.3 Removing Papers with Less Than 150 Words \n\nThe last step in elimination outliers from the dataset is to identify papers containing less than 150 words. The minimum of 150 words has been derived using numerous rounds of experimentation. It also supports the minimum input requirements to generate a smart summary for a paper as part of our results.\n\nThe code below creates a word count for each paper and adds the list of word counts as a new column in the DataFrame. It then identifies those papers that contain less than 150 words of text.","c9fd47f5":"### 3.4.3 Checkpoint 4\n\nAfter the merge, we will save the current \"test_df\" DataFrame as \"\/kaggle\/working\/Checkpoint 4.csv\".","5c5f640b":"# 1. Introduction\n\nWith the current COVID-2019 Pandemic, medical professionals, scientists, and technologists are racing to contribute to the efforts to simultaneously create and trial a vaccine, mitigate the spread of infection, and improve the prognosis for those impacted by the virus. The full set of COVID-19 research, combined with related research from previous viral epidemics, is an ever-growing collection of journal articles that serves as a global compilation of the latest relevant knowledge, trial results, studies, and reports.  Having the ability to quickly locate and incorporate critical published data and conclusions in support of innovative research and new diagnostic and surveillance tools is essential for those working to expedite solutions to address the many challenging aspects of the global COVID-19 crisis.\n\n## 1.1 Objectives\n\nThis objective of this submission is to address the CORD-19 Challenge by applying automation techniques to synthesize a sizable database of current research into a much more manageable set of relevant knowledge related to multiple aspects of understanding and addressing the issues related to the COVID-19 Pandemic. Access to specific aspects of this data set will enable researchers to pinpoint specific elements of the emerging research that comprises the 59,000+ set of papers in the Kaggle database that focus on COVID-19 and related viruses.  This specific effort intends to offer a means of dramatically  reducing the time to reach answers to key questions being asked today.\n\nThis notebook presents code that parses, summarizes, and provides critical insights and visualizations from what would otherwise be an overwhelming corpus of published research. This process begins by identifying journal articles that specifically address the novel Coronavirus Disease, COVID-19, and the underlying virus SARS-Cov-2 (previously known as 2019 n-COV) as these pieces of research present the highest level of relevance and often summarize previous findings from research conducted on related Coronaviruses, other respiratory viruses, and viruses in general. Our specific objective is to extract and present key findings related to the transmission, incubation, and environmental stability of the SARS-Cov-2 \/ COVID-19.\n\n## 1.2 How it Works\n\nThe code in this notebook isolates and presents what are determined to be the most relevant articles for medical professionals, scientists, and technologists who are at the forefront of this battle against COVID-19. [Section 1.3](#1.3-Results-Preview \"Results Preview\") shows a portion of a search results and explains key fields displayed.\n\n### 1.2.1 General Pros and Cons of our Approach\n\n**Pros**\n1.\tFast and targeted search to produce and display a small set of highest relevance papers. \n2.\tGenerates accurate and relevant answers for specific questions posed. \n3.\tThis approach functions particularly well in situations where no or limited research papers exist specifically for COVID-19 as it provides the researcher with data reported and conclusions reached for related diseases (such as influenza in this case) that is relevant to the topic selected.\n\n**Cons**\n1.\tCurrently, this approach takes a subset of research papers deemed relevant to the question posed for analysis  As a result, there may be limitations in providing optimal answers; for example, the scope may be confined to comparing properties of two viruses within a single topic scope such as the economic impact of COVID-19 vs. the economic Impact of SARS, which leads to the possibility of a providing a sub-optimal conclusion.\n2.\tThe solution provides an answer for specific questions only (indicated terms).","199bf141":"### 3.2.3 Checkpoint 1\n\nAt this point, we want to save our combined DataFrame as a .csv file, \"\/kaggle\/working\/Checkpoint 1.csv\". We also want to delete the variables we no longer need to reclaim some memory space before continuing code execution.","ed5b40e4":"With the above two functions defined, the following two lines of code define the list of of subfolder names under \"\/kaggle\/input\/CORD-19-research-challenge\" where the raw paper data is found, and then call the data_creation() function to extract the desired data from each paper and and create a single, combined DataFrame, \"test_df\".","22d6d0f4":"Taking a look at the text of the papers with less than 150 words, we can see two things. Firstly, these papers contain little to no information that is useful to provide context or insights to medical researchers. Secondly, some of these papers have non-English content. We remove these papers from the dataset.","f32f7571":"### 3.2.2 Extracting and Formatting the Data for all Papers in the Dataset\n\nWe can now load, extract, and reformat the data for the papers into a format suitable for further analysis. The raw data for the CORD-19 Research Challenge is accessible on the Kaggle site\nat the following path:  \"\/kaggle\/input\/CORD-19-research-challenge\". \n\nIn addition, for storing the formatted data, we will use \"\/kaggle\/working\/\".\n\n#### 3.2.2.1 Loading the Metadata\n\nThe *metada.csv* file, located in the **Input** directory, contains useful summary information about each article in the dataset. The information provided for each paper includes (but is not limited to) the following:\n\n* title\n* license under which the paper is published\n* abstract text\n* publication date\n* authors\n* publishing journal\n* whether the data includes the full text of the paper\n\nWe use metadata to extract some additional information not found in the JSON files for each paper. The follow code reads in the *metadata.csv* file and converts it into a *pandas* DataFrame for further processing.","78830a6f":"#### 3.3.3.1 Checkpoint 2\n\nBefore continuing, we save our current version of the \"test_df\" DataFrame as \"\/kaggle\/working\/Checkpoint 2.csv\", and reclaim some memory space by deleting variables we no longer need.","24d6e314":"The following two lines of code define the text columns in the \"test_df\" DataFrame that we want to clean (\"Title\", \"Abstract\", and \"Text\"), and then call the cleaning_data set function with \"test_df\" as the input data frame and the defined columns as the columns to clean.","03e20099":"<h1>Table of Contents<span class=\"tocSkip\"><\/span><\/h1>\n<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#1.-Introduction\" data-toc-modified-id=\"1.-Introduction-1\"><span class=\"toc-item-num\">1&nbsp;&nbsp;<\/span>Introduction<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#1.1-Objectives\" data-toc-modified-id=\"1.1-Objectives-1.1\"><span class=\"toc-item-num\">1.1&nbsp;&nbsp;<\/span>Objectives<\/a><\/span><\/li><li><span><a href=\"#1.2-How-it-Works\" data-toc-modified-id=\"1.2-How-it-Works-1.2\"><span class=\"toc-item-num\">1.2&nbsp;&nbsp;<\/span>How it Works<\/a><\/span><\/li><li><span><a href=\"#1.3-Results-Preview\" data-toc-modified-id=\"1.3-Results-Preview-1.3\"><span class=\"toc-item-num\">1.3&nbsp;&nbsp;<\/span>Results Preview<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#2.-Methodology\" data-toc-modified-id=\"2.-Methodology-2\"><span class=\"toc-item-num\">2&nbsp;&nbsp;<\/span>Methodology<\/a><\/span><\/li><li><span><a href=\"#3.-The-Code\" data-toc-modified-id=\"3.-The-Code-3\"><span class=\"toc-item-num\">3&nbsp;&nbsp;<\/span>The Code<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#3.1-Preparation\" data-toc-modified-id=\"3.1-Preparation-3.1\"><span class=\"toc-item-num\">3.1&nbsp;&nbsp;<\/span>Preparation<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#3.1.1-Installing-and-Importing-the-Necessary-Python-Packages\" data-toc-modified-id=\"3.1.1-Installing-and-Importing-the-Necessary-Python-Packages-3.1.1\"><span class=\"toc-item-num\">3.1.1&nbsp;&nbsp;<\/span>Installing and Importing the Necessary Python Packages<\/a><\/span><\/li><li><span><a href=\"#3.1.2-Defining-Useful-Helper-Functions\" data-toc-modified-id=\"3.1.2-Defining-Useful-Helper-Functions-3.1.2\"><span class=\"toc-item-num\">3.1.2&nbsp;&nbsp;<\/span>Defining Useful Helper Functions<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#3.2-Acquiring-and-Preprocessing-the-Dataset\" data-toc-modified-id=\"3.2-Acquiring-and-Preprocessing-the-Dataset-3.2\"><span class=\"toc-item-num\">3.2&nbsp;&nbsp;<\/span>Acquiring and Preprocessing the Dataset<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#3.2.1-Downloading-the-CORD-19-Dataset-from-Kaggle\" data-toc-modified-id=\"3.2.1-Downloading-the-CORD-19-Dataset-from-Kaggle-3.2.1\"><span class=\"toc-item-num\">3.2.1&nbsp;&nbsp;<\/span>Downloading the CORD-19 Dataset from Kaggle<\/a><\/span><\/li><li><span><a href=\"#3.2.2-Extracting-and-Formatting-the-Data-for-all-Papers-in-the-Dataset\" data-toc-modified-id=\"3.2.2-Extracting-and-Formatting-the-Data-for-all-Papers-in-the-Dataset-3.2.2\"><span class=\"toc-item-num\">3.2.2&nbsp;&nbsp;<\/span>Extracting and Formatting the Data for all Papers in the Dataset<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#3.2.2.1-Loading-the-Metadata\" data-toc-modified-id=\"3.2.2.1-Loading-the-Metadata-3.2.2.1\"><span class=\"toc-item-num\">3.2.2.1&nbsp;&nbsp;<\/span>Loading the Metadata<\/a><\/span><\/li><li><span><a href=\"#3.2.2.3-Extracting-Key-Data-from-each-Paper\" data-toc-modified-id=\"3.2.2.3-Extracting-Key-Data-from-each-Paper-3.2.2.2\"><span class=\"toc-item-num\">3.2.2.2&nbsp;&nbsp;<\/span>Extracting Key Data from each Paper<\/a><\/span><\/li><li><span><a href=\"#Load-the-Data-for-all-the-Papers\" data-toc-modified-id=\"Load-the-Data-for-all-the-Papers-3.2.2.3\"><span class=\"toc-item-num\">3.2.2.3&nbsp;&nbsp;<\/span>Load the Data for all the Papers<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#3.2.3-Checkpoint-1\" data-toc-modified-id=\"3.2.3-Checkpoint-1-3.2.3\"><span class=\"toc-item-num\">3.2.3&nbsp;&nbsp;<\/span>Checkpoint 1<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#3.3-Cleaning-the-Dataset\" data-toc-modified-id=\"3.3-Cleaning-the-Dataset-3.3\"><span class=\"toc-item-num\">3.3&nbsp;&nbsp;<\/span>Cleaning the Dataset<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#3.3.1-Cleaning-the-Paper-Text-Sections\" data-toc-modified-id=\"3.3.1-Cleaning-the-Paper-Text-Sections-3.3.1\"><span class=\"toc-item-num\">3.3.1&nbsp;&nbsp;<\/span>Cleaning the Paper Text Sections<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#3.3.1.1-Load-Checkpoint-1\" data-toc-modified-id=\"3.3.1.1-Load-Checkpoint-1-3.3.1.1\"><span class=\"toc-item-num\">3.3.1.1&nbsp;&nbsp;<\/span>Load Checkpoint 1<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#3.3.2-Removing-Duplicate-Papers\" data-toc-modified-id=\"3.3.2-Removing-Duplicate-Papers-3.3.2\"><span class=\"toc-item-num\">3.3.2&nbsp;&nbsp;<\/span>Removing Duplicate Papers<\/a><\/span><\/li><li><span><a href=\"#3.3.3-Cleaning-the-Country-Names\" data-toc-modified-id=\"3.3.3-Cleaning-the-Country-Names-3.3.3\"><span class=\"toc-item-num\">3.3.3&nbsp;&nbsp;<\/span>Cleaning the Country Names<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#3.3.3.1-Checkpoint-2\" data-toc-modified-id=\"3.3.3.1-Checkpoint-2-3.3.3.1\"><span class=\"toc-item-num\">3.3.3.1&nbsp;&nbsp;<\/span>Checkpoint 2<\/a><\/span><\/li><li><span><a href=\"#3.3.3.2-Geographical-Distribution-of-Papers\" data-toc-modified-id=\"3.3.3.2-Geographical-Distribution-of-Papers-3.3.3.2\"><span class=\"toc-item-num\">3.3.3.2&nbsp;&nbsp;<\/span>Geographical Distribution of Papers<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#3.3.4-Identifying-and-Removing-Papers-that-are-Outliers\" data-toc-modified-id=\"3.3.4-Identifying-and-Removing-Papers-that-are-Outliers-3.3.4\"><span class=\"toc-item-num\">3.3.4&nbsp;&nbsp;<\/span>Identifying and Removing Papers that are Outliers<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#3.3.4.1-Identifying-and-Removing-Papers-with-No-Body-Text\" data-toc-modified-id=\"3.3.4.1-Identifying-and-Removing-Papers-with-No-Body-Text-3.3.4.1\"><span class=\"toc-item-num\">3.3.4.1&nbsp;&nbsp;<\/span>Identifying and Removing Papers with No Body Text<\/a><\/span><\/li><li><span><a href=\"#3.3.4.2-Removing-Papers-that-Do-Not-Appear-to-be-Relevant-to-the-Corpus-as-a-Whole\" data-toc-modified-id=\"3.3.4.2-Removing-Papers-that-Do-Not-Appear-to-be-Relevant-to-the-Corpus-as-a-Whole-3.3.4.2\"><span class=\"toc-item-num\">3.3.4.2&nbsp;&nbsp;<\/span>Removing Papers that Do Not Appear to be Relevant to the Corpus as a Whole<\/a><\/span><\/li><li><span><a href=\"#3.3.4.3-Removing-Papers-with-Less-Than-150-Words\" data-toc-modified-id=\"3.3.4.3-Removing-Papers-with-Less-Than-150-Words-3.3.4.3\"><span class=\"toc-item-num\">3.3.4.3&nbsp;&nbsp;<\/span>Removing Papers with Less Than 150 Words<\/a><\/span><\/li><li><span><a href=\"#3.3.4.4-Checkpoint-3\" data-toc-modified-id=\"3.3.4.4-Checkpoint-3-3.3.4.4\"><span class=\"toc-item-num\">3.3.4.4&nbsp;&nbsp;<\/span>Checkpoint 3<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#3.3.5-Identifying-and-Removing-Papers-whose-Primary-Language-is-not-English\" data-toc-modified-id=\"3.3.5-Identifying-and-Removing-Papers-whose-Primary-Language-is-not-English-3.3.5\"><span class=\"toc-item-num\">3.3.5&nbsp;&nbsp;<\/span>Identifying and Removing Papers whose Primary Language is not English<\/a><\/span><\/li><\/ul><\/li><li><span><a href=\"#3.4-Topic-Modelling\" data-toc-modified-id=\"3.4-Topic-Modelling-3.4\"><span class=\"toc-item-num\">3.4&nbsp;&nbsp;<\/span>Topic Modelling<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#3.4.1-Summary-Visualization---LDA-of-papers\" data-toc-modified-id=\"3.4.1-Summary-Visualization---LDA-of-papers-3.4.1\"><span class=\"toc-item-num\">3.4.1&nbsp;&nbsp;<\/span>Summary Visualization - LDA of papers<\/a><\/span><\/li><li><span><a href=\"#3.4.2->>-PYLDA-Visualization-<<\" data-toc-modified-id=\"3.4.2->>-PYLDA-Visualization-<<-3.4.2\"><span class=\"toc-item-num\">3.4.2&nbsp;&nbsp;<\/span><a href=\"lda_visualization.html\" target=\"_blank\">&gt;&gt; PYLDA Visualization &lt;&lt;<\/a><\/a><\/span><\/li><li><span><a href=\"#3.4.3-Checkpoint-4\" data-toc-modified-id=\"3.4.3-Checkpoint-4-3.4.3\"><span class=\"toc-item-num\">3.4.3&nbsp;&nbsp;<\/span>Checkpoint 4<\/a><\/span><\/li><li><span><a href=\"#3.4.4-Summary-Visualization---Common-Terms-in-the-Research-Papers\" data-toc-modified-id=\"3.4.4-Summary-Visualization---Common-Terms-in-the-Research-Papers-3.4.4\"><span class=\"toc-item-num\">3.4.4&nbsp;&nbsp;<\/span>Summary Visualization - Common Terms in the Research Papers<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#4.-Search-Engine\" data-toc-modified-id=\"4.-Search-Engine-4\"><span class=\"toc-item-num\">4&nbsp;&nbsp;<\/span>Search Engine<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#4.1-Task-Report\" data-toc-modified-id=\"4.1-Task-Report-4.1\"><span class=\"toc-item-num\">4.1&nbsp;&nbsp;<\/span>Task Report<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#4.1.1-Task-1---Questions\" data-toc-modified-id=\"4.1.1-Task-1---Questions-4.1.1\"><span class=\"toc-item-num\">4.1.1&nbsp;&nbsp;<\/span>Task 1 - Questions<\/a><\/span><\/li><\/ul><\/li><\/ul><\/li><li><span><a href=\"#5.-License-Agreements\" data-toc-modified-id=\"5.-License-Agreements-5\"><span class=\"toc-item-num\">5&nbsp;&nbsp;<\/span>License Agreements<\/a><\/span><ul class=\"toc-item\"><li><span><a href=\"#5.1-PSF-LICENSE-AGREEMENT-FOR-PYTHON-3.8.2\" data-toc-modified-id=\"5.1-PSF-LICENSE-AGREEMENT-FOR-PYTHON-3.8.2-5.1\"><span class=\"toc-item-num\">5.1&nbsp;&nbsp;<\/span>PSF LICENSE AGREEMENT FOR PYTHON 3.8.2<\/a><\/span><\/li><li><span><a href=\"#5.2-The-3-Clause-BSD-License\" data-toc-modified-id=\"5.2-The-3-Clause-BSD-License-5.2\"><span class=\"toc-item-num\">5.2&nbsp;&nbsp;<\/span>The 3-Clause BSD License<\/a><\/span><\/li><li><span><a href=\"#5.3-NumPy-license\" data-toc-modified-id=\"5.3-NumPy-license-5.3\"><span class=\"toc-item-num\">5.3&nbsp;&nbsp;<\/span>NumPy license<\/a><\/span><\/li><li><span><a href=\"#5.4-GNU-LESSER-GENERAL-PUBLIC-LICENSE\" data-toc-modified-id=\"5.4-GNU-LESSER-GENERAL-PUBLIC-LICENSE-5.4\"><span class=\"toc-item-num\">5.4&nbsp;&nbsp;<\/span>GNU LESSER GENERAL PUBLIC LICENSE<\/a><\/span><\/li><li><span><a href=\"#5.5-The-MIT-License\" data-toc-modified-id=\"5.5-The-MIT-License-5.5\"><span class=\"toc-item-num\">5.5&nbsp;&nbsp;<\/span>The MIT License<\/a><\/span><\/li><li><span><a href=\"#5.6-License-agreement-for-matplotlib-versions-1.3.0-and-later\" data-toc-modified-id=\"5.6-License-agreement-for-matplotlib-versions-1.3.0-and-later-5.6\"><span class=\"toc-item-num\">5.6&nbsp;&nbsp;<\/span>License agreement for matplotlib versions 1.3.0 and later<\/a><\/span><\/li><li><span><a href='#5.7-3-clause-license-(\"BSD-License-2.0\",-\"Revised-BSD-License\",-\"New-BSD-License\",-or-\"Modified-BSD-License\")' data-toc-modified-id='5.7-3-clause-license-(\"BSD-License-2.0\",-\"Revised-BSD-License\",-\"New-BSD-License\",-or-\"Modified-BSD-License\")-5.7'><span class=\"toc-item-num\">5.7&nbsp;&nbsp;<\/span>3-clause license (\"BSD License 2.0\", \"Revised BSD License\", \"New BSD License\", or \"Modified BSD License\")<\/a><\/span><\/li><li><span><a href=\"#5.8-Apache-License\" data-toc-modified-id=\"5.8-Apache-License-5.8\"><span class=\"toc-item-num\">5.8&nbsp;&nbsp;<\/span>Apache License<\/a><\/span><\/li><li><span><a href=\"#5.9-Geopy-License\" data-toc-modified-id=\"5.9-Geopy-License-5.9\"><span class=\"toc-item-num\">5.9&nbsp;&nbsp;<\/span>Geopy License<\/a><\/span><\/li><\/ul><\/li><\/ul><\/div>","82282bfa":"The visualization above shows the number of articles published by state (United States). To view the number of articles published by a particular state, move the cursor over the state's location on the map. States with no papers published are gray.\n\nThe visualization shows that the top states in the US in terms of the most papers, based on the current dataset, are Texas, followed closely by Pennsylvania, Maryland, and New York. The next lower group includes California, Indiana, and North Carolina.\n\nBefore continuing on with identifying and removing papers that are outliers, we pause again to delete variables we no longer need and reclaim some memory space.","b99f7219":"#### Extracting Relevant Papers based on Date\n\nIn order to save memory, we include the option here to filter for papers published after a specified date. The function below, ```is_date()```, takes a string as input and uses the *dateutil* package to see if the string can be interpreted as a date. The function returns \"True\" if it can; \"False\" otherwise.\n\nThe code block following the ```is_date()``` function then takes a date as a string input (our default value is \"2019-12-01\"), and then identifies the IDs of from the metadata of those papers that were published after this date. To set a specific date, simply modify the \"filter_date\" variable appropriately, or enter a non-valid date string to disable this filtering altogether. The result of this code block, \"paper_id_list\", is the list of IDs for papers published after the specified date (or all papers if the filtering is disabled). In addition, there is the ability to specify that we want papers that have already been published in journals. If set to 'Yes', the code below will also filter for those papers.\n\n**Note:** In the case of COVID-19, we would recommend that the search includes papers that have yet to be published in journals. Since this is a relatively new research area, it is possible that there are many papers of interest that have yet to be formally published in an established journal; for example, there may be many relevant papers from conference proceedings in the current dataset. Therefore, we recommend ```from_journal_sources_only``` to be set as 'No'.","8f1992ac":"## 3.2 Acquiring and Preprocessing the Dataset\n\nIn this step, downloading the CORD-19 dataset and extracting and formatting the data are explained.\n\n### 3.2.1 Downloading the CORD-19 Dataset from Kaggle\n\n**ATTENTION**\nThe need to download the CORD-19 dataset from Kaggle is applicable only if this notebook is downloaded and run outside of Kaggle. If this is the case, follow the instructions on the Kaggle site. \n\nIf wanting to write code for downloading the dataset, the following steps can be used as guidance:\n1. Import the Kaggle API\n2. Set the local directory where you want to download the dataset\n3. Set the Kaggle account credentials, and establish an authenticated Kaggle API instance.\n4. Download the CORD-19 Research Challenge dataset using the Kaggle API method","e8a32263":"Now that we have removed papers identified as outliers via the previous steps, we can recheck the dataset for relevancy using the functions defined above. After undergoing all the above cleaning steps, we can observe in the plot below that the remaining papers are clustered fairly closely together, showing that they retain certain similarities in overall content.","cc55ce31":"#### Merge Back the Non-English Papers\n\nFor easier referencing, we now merge back non-English and English papers into a combined dataset. Topic modeling was only done on the set of English papers. This means that non-English paper will have \"NA\" as their topic model number.","b73fc74b":"#### 3.2.2.3 Extracting Key Data from each Paper\n\nEach paper in the dataset is represented by a JSON file, and is located in one of four subdirectories under the \"\/kaggle\/input\/CORD-19-research-challenge\" directory, depending on how the article is licensed:\n\n- \"comm_use_subset\"\n- \"noncomm_use_subset\"\n- \"custom_license\"\n- \"biorxiv_medrxiv\"\n\nFor each paper, we want to extract the following data:\n\n- paper ID\n- url link\n- publication date\n- title\n- abstract text\n- body text\n- primary location and country for the author(s)\n\nNote that the paper url link and the publication date are contained in the dataset metadata, while the rest of the items are in the individual paper JSON files. We will get url line and publication date from the metadata, and then merge these items with the rest of the data in a single DataFrame.\n\nThe two functions below extract this data, and then format it into a single combined DataFrame which contains the key information that we need for the papers.\n\nThe ```create_library()``` function reads in the JSON data for each paper from each subdirectory (only those published after the specified date, if date filtering is enabled). The detailed information for each paper is formatted into a Python dictionary, which in turn is appended to a list - the result being a list of dictionaries, where each dictionary describes one paper.\n\nThe ```data_creation()``` function uses the list of dictionaries from the ```create_library()``` function, extracts the data items listed above, and combines this data for each paper into a single combined DataFrame. In addition, it retrieves the url line and publication date for each paper from the metadata DataFrame we created previously, and then merges this into to the combined DataFrame.\n\n##### Extract the Primary Location, and Country for the Authors of each Paper\n\nOne of the items in the dictionary for each paper contains information about the paper title and author. It is referenced by the key, \"metadata\". The value of this item is itself a dictionary, and one of the items in the \"metadata\" dictionary is a list of dictionaries which provides complete information on the author(s) of the paper (one dictionary for each author). This item is referenced by the key, \"authors\". Each author dictionary contains entries for:\n\n- first name\n- middle name\n- last name\n- name suffix\n- author affiliation ((institution, location, country, etc.)\n- email address\n\nThe primary location and country (i.e., this information for the first author) for the author(s) of each paper is contained in the \"affiliation\" item of the dictionary for each author. Part of the ```data_creation()``` function extracts the author information from the \"metadata\" dictionary, examines the \"affiliation\" data structure for each author, and extracts the location and country information for the first author (or the first author that has valid information for these items). These items for each paper are then added to the combined DataFrame. ","67713f6a":"The visualization above shows the number of articles published by country. To view the number of articles published by a particular country, move the cursor over the country's location on the map. If a country is not readily visible in the viewing pane, you can scroll to other parts of the map by right-clicking and holding on the map while moving the cursor to scroll to other parts of the map. Countries with no papers are gray.\n\nThe visualization shows that the top countries in terms of the most papers, based on the current dataset, are Italy, followed by China, and then the US.\n\nThe following code looks at the United States specifically, and counts the number of papers by state. The vizualization below displays those results.","93399cd9":"### 3.1.2 Defining Useful Helper Functions\n\nThe following two helper functions are used as part of the code.\n\n```log_progress()``` uses the *ipwidgets* Python package to create and display a progress bar which gives the user running the notebook an indication of the progress of code execution for certain portions of the notebook where this function is called.\n\n\n```process_text()``` uses the *spaCy* package natural language processing capabilities to clean and lemmatize an input text string.  The function creates a spaCy \"Doc\" object from the input string, which is a list of tokens that break down the string into its constituent parts; e.g., individual words, spaces, punctuation marks, and lemmas corresponding to each word. It then runs through the tokens, screens out punctuation, stop words, and pronouns, and returns a list of the lemmas corresponding to each remaining word.","813efaf8":"# 4. Search Engine\n\nNow that we have a cleaned dataset for the papers, including a trained topic model for the papers, we use that to implement a search engine. This search engine uses the dataset to provide answers to questions; specifically, we want to provide some insights to the researchers for all of the questions posed as part of Task 1 of this challenge.\n\nWhen exercised with a question (e.g., \"What is known about coronavirus transmission?\"), the search engine analyzes the dataset and generates a report that consists of the following:\n\n- the best answer (a statement) to the question\n- the total number of papers in the dataset that are related to the question\n- the single paper that seems to best match the question:\n\t* paper url\n\t* title\n\t* best matching sentence in context\n- a list of additional papers (number specified in the search engine query) that are most relevant to the question; for each of these papers:\n\t* paper url\n\t* title\n\t* best matching sentence in context\n\t* a summary of the paper\n\t* a list of entities (e.g., relevant key words) in the paper\n\nFor more details about how to interpret the search engine output, along with highlights of the results we obtained for the questions posed as part of Task 1, refer to the Section 1.3 above.\n\nThe search engine is implemented using the eight functions defined in the code block below. For details on each function, see the comments in the code block. To generate the report output described above, the search engine proceeds through the following tasks:\n\n- generate a list of search words from the input question\n- identify a subset of relevant papers for the question; a paper is identified as relevant if its associated sub-topic matches one of the search words\n- for each paper in the relevant subset:\n\t* identify the best matching sentence in each paper and corresponding match score\n\t* create an overall score for the paper as a whole\n- sort the relevant subset of papers by overall score and best sentence score\n- identify the paper in the relevant subset with the best matching sentence (highest sentence score)\n- identify the top few papers based on overall score, and generate a summary for each paper\n- analyze the set of relevant papers and create the best text answer to the input question\n- identify the entities present in the top few papers","5d4f3d29":"## 3.1 Preparation\n\nFor the preparation task, required software packages are imported and installed, and helper functions are defined to make the code more readable and efficient.\n\n### 3.1.1 Installing and Importing the Necessary Python Packages\n\nThe following code blocks install the necessary Python packages on the system, and import them into the Python environment.","fed83f0d":"#### Add a Sub-topic for each Paper\n\nAs an additional refinement, we want to associate each paper with a sub-topic. The topic for each paper has a list of topic words that indicate the meaning of the topic. We will define the paper sub-topic as the topic word that occurs most frequently in the lemmatized paper text. Defining the paper sub-topic in this way gives us a more refined search list to use later as part of the search engine that identifies the subset of papers that provide the best answers to a particular question we want to research in the dataset.\n\nThe code below identifies the sub-topic for each paper per the above definition, and adds the list of sub-topics for each paper as a new column, \"Sub-Topic\", to the \"test_df\" DataFrame.","57a5da11":"The following bar graph shows a visualization of the list of topics and its frequency based on the latent topics discovered using LDA topic modelling. When published, if you wish to view this information, please go to the Output Section and download 'lda_visualization.html' and open it in your browser. \n\nThis has been seperated because when displaying the HTML file inline, it will cause the width of the cell output to change, which affects other visualizations in this notebook.","51aafd63":"# 2. Methodology\n\nTo address the objectives of this notebook, our approach is to model one of the ways we can use a bricks-and-mortar library to perform research. This intuitive approach shapes the results of our search engine, the manner in which we visualize the 51,000+ research paper dataset from Kaggle (our library of documents), and even how we measure whether or not the results are appropriate.\n\nStaying with the bricks-and-mortar library model, there were two options to explore at the beginning. The first option involved requesting the librarian identify papers with keywords before grouping them into topics. The second option involved having the librarian identify topics before searching within these topics for keywords. Each option contains its own pros and cons. The following example scenario illustrates the differences between the two options. \n\n**Scenario**: A researcher wants to identify the economic impact of COVID-19. Therefore, the keyword is \u201cCOVID-19,\u201d and the topic is \"economic.\"\n\nWith the first option, the librarian identifies research papers dealing with COVID-19, and returns a collection of papers that cover a wide range of topics, including transmission, environmental stability, and economics. Using the search results, the librarian then does a secondary search on topics using \"economics.\" This gives the researcher the power to not only focus on economics, but to also explore other topics under COVID-19 (e.g., the librarian may choose a keyword other than \"economics\" for subsequent secondary searches).\n\nWith the second option, the librarian first identifies topics (such as economics) and only research papers with the specified topic(s) are identified. This set of papers would contain a variety of keywords. The secondary search using COVID-19 related keywords then returns COVID-19 papers. This allows researchers to explore economic impacts of other diseases or viruses like influenza in subsequent secondary searches. \n\nThe difference between these two approaches only becomes apparent when researching a new and emerging subject. Through the first approach, there exists a possibility that no research papers, or a limited number of research papers exist on COVID-19. Whereas, in the second approach, even though no research papers or a limited number of research papers exist, the alternative of searching for papers with related information (such as research on other diseases or viruses) is available. Due to the potentially limited search results of the first option, we decided to go with the second approach, topic generation before the keyword search.  \n\nLSA, pLSA, LDA, and LDA2Vec are the four most popular topic models. They all work on the same assumptions of each having a mixture of topics and consisting of a collection of words. The construction of a topic model is based on the idea that there are semantic structures in document text that are hidden (or unobserved). Therefore, the purpose of topic modeling is to uncover these hidden or latent variables (called topics) that shape the meaning of a document or dictionary (corpus).\nWhile LDA has several shortcomings, ultimately, LDA was chosen because of the Dirichlet prior on top of the data generating process. This Dirichlet prior gives LDA its probabilistic nature. This allows a research paper\u2019s latent topic to change with respect to the overall library, which is more representative of an ever-changing and ever-growing library.  \n\n[Figure 2](#Figure_2) depicts our overall methodology.","ec1392d7":"### 3.3.2 Removing Duplicate Papers\n\nIf we examine some characteristics of the \"Abstract\" column of the \"test_df\" DataFrame, we notice that there appear to be a significant number (~700+) of duplicate abstracts in the data set. One possible reason for this is that a given paper may have been published in more than one journal. We obviously do not want such duplicates. The following code looks for duplicated papers, indicated either by duplicate abstract text or duplicate full text, and removes the corresponding rows from the DataFrame. In the output below we see that dropping the apparent duplicates has effectively eliminated the number of non-unique texts in our cleaned dataset.","c9fbe5c5":"#### Obtain the Most Likely or Primary Topic for each Paper\n\nFinally, we run the ```predict()``` method on each of the paper texts (the lemmatized text) to identify the most likely topic or the primary topic for each of the papers. For each paper, we take the resulting topic number and the topic words (string), and add this to the \"english_df\" DataFrame.","9fa9dc12":"# 5. License Agreements\n\n## 5.1 PSF LICENSE AGREEMENT FOR PYTHON 3.8.2\n1. This LICENSE AGREEMENT is between the Python Software Foundation (\"PSF\"), and\n   the Individual or Organization (\"Licensee\") accessing and otherwise using Python\n   3.8.2 software in source or binary form and its associated documentation.\n\n2. Subject to the terms and conditions of this License Agreement, PSF hereby\n   grants Licensee a nonexclusive, royalty-free, world-wide license to reproduce,\n   analyze, test, perform and\/or display publicly, prepare derivative works,\n   distribute, and otherwise use Python 3.8.2 alone or in any derivative\n   version, provided, however, that PSF's License Agreement and PSF's notice of\n   copyright, i.e., \"Copyright \u00a9 2001-2020 Python Software Foundation; All Rights\n   Reserved\" are retained in Python 3.8.2 alone or in any derivative version\n   prepared by Licensee.\n\n3. In the event Licensee prepares a derivative work that is based on or\n   incorporates Python 3.8.2 or any part thereof, and wants to make the\n   derivative work available to others as provided herein, then Licensee hereby\n   agrees to include in any such work a brief summary of the changes made to Python\n   3.8.2.\n\n4. PSF is making Python 3.8.2 available to Licensee on an \"AS IS\" basis.\n   PSF MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR IMPLIED.  BY WAY OF\n   EXAMPLE, BUT NOT LIMITATION, PSF MAKES NO AND DISCLAIMS ANY REPRESENTATION OR\n   WARRANTY OF MERCHANTABILITY OR FITNESS FOR ANY PARTICULAR PURPOSE OR THAT THE\n   USE OF PYTHON 3.8.2 WILL NOT INFRINGE ANY THIRD PARTY RIGHTS.\n\n5. PSF SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF PYTHON 3.8.2\n   FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR LOSS AS A RESULT OF\n   MODIFYING, DISTRIBUTING, OR OTHERWISE USING PYTHON 3.8.2, OR ANY DERIVATIVE\n   THEREOF, EVEN IF ADVISED OF THE POSSIBILITY THEREOF.\n\n6. This License Agreement will automatically terminate upon a material breach of\n   its terms and conditions.\n\n7. Nothing in this License Agreement shall be deemed to create any relationship\n   of agency, partnership, or joint venture between PSF and Licensee.  This License\n   Agreement does not grant permission to use PSF trademarks or trade name in a\n   trademark sense to endorse or promote products or services of Licensee, or any\n   third party.\n\n8. By copying, installing or otherwise using Python 3.8.2, Licensee agrees\n   to be bound by the terms and conditions of this License Agreement.\n \n \n## 5.2 The 3-Clause BSD License\n \nNote: This license has also been called the \"New BSD License\" or \"Modified BSD License\". See also the 2-clause BSD License.\n\nCopyright <YEAR> <COPYRIGHT HOLDER>\n\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\n1. Redistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\n2. Redistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and\/or other materials provided with the distribution.\n\n3. Neither the name of the copyright holder nor the names of its contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT HOLDER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n## 5.3 NumPy license\n    \nCopyright \u00a9 2005-2020, NumPy Developers.\nAll rights reserved.\nRedistribution and use in source and binary forms, with or without modification, are permitted provided that the following conditions are met:\n\nRedistributions of source code must retain the above copyright notice, this list of conditions and the following disclaimer.\n\nRedistributions in binary form must reproduce the above copyright notice, this list of conditions and the following disclaimer in the documentation and\/or other materials provided with the distribution.\n\nNeither the name of the NumPy Developers nor the names of any contributors may be used to endorse or promote products derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \u201cAS IS\u201d AND ANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE DISCLAIMED. IN NO EVENT SHALL THE COPYRIGHT OWNER OR CONTRIBUTORS BE LIABLE FOR ANY DIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES (INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES; LOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND ON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT (INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS SOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n\n## 5.4 GNU LESSER GENERAL PUBLIC LICENSE\n    \nVersion 2.1, February 1999\n\nCopyright (C) 1991, 1999 Free Software Foundation, Inc.\n51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA\nEveryone is permitted to copy and distribute verbatim copies\nof this license document, but changing it is not allowed.\n\n[This is the first released version of the Lesser GPL.  It also counts\n as the successor of the GNU Library Public License, version 2, hence\n the version number 2.1.]\nPreamble\nThe licenses for most software are designed to take away your freedom to share and change it. By contrast, the GNU General Public Licenses are intended to guarantee your freedom to share and change free software--to make sure the software is free for all its users.\n\nThis license, the Lesser General Public License, applies to some specially designated software packages--typically libraries--of the Free Software Foundation and other authors who decide to use it. You can use it too, but we suggest you first think carefully about whether this license or the ordinary General Public License is the better strategy to use in any particular case, based on the explanations below.\n\nWhen we speak of free software, we are referring to freedom of use, not price. Our General Public Licenses are designed to make sure that you have the freedom to distribute copies of free software (and charge for this service if you wish); that you receive source code or can get it if you want it; that you can change the software and use pieces of it in new free programs; and that you are informed that you can do these things.\n\nTo protect your rights, we need to make restrictions that forbid distributors to deny you these rights or to ask you to surrender these rights. These restrictions translate to certain responsibilities for you if you distribute copies of the library or if you modify it.\n\nFor example, if you distribute copies of the library, whether gratis or for a fee, you must give the recipients all the rights that we gave you. You must make sure that they, too, receive or can get the source code. If you link other code with the library, you must provide complete object files to the recipients, so that they can relink them with the library after making changes to the library and recompiling it. And you must show them these terms so they know their rights.\n\nWe protect your rights with a two-step method: (1) we copyright the library, and (2) we offer you this license, which gives you legal permission to copy, distribute and\/or modify the library.\n\nTo protect each distributor, we want to make it very clear that there is no warranty for the free library. Also, if the library is modified by someone else and passed on, the recipients should know that what they have is not the original version, so that the original author's reputation will not be affected by problems that might be introduced by others.\n\nFinally, software patents pose a constant threat to the existence of any free program. We wish to make sure that a company cannot effectively restrict the users of a free program by obtaining a restrictive license from a patent holder. Therefore, we insist that any patent license obtained for a version of the library must be consistent with the full freedom of use specified in this license.\n\nMost GNU software, including some libraries, is covered by the ordinary GNU General Public License. This license, the GNU Lesser General Public License, applies to certain designated libraries, and is quite different from the ordinary General Public License. We use this license for certain libraries in order to permit linking those libraries into non-free programs.\n\nWhen a program is linked with a library, whether statically or using a shared library, the combination of the two is legally speaking a combined work, a derivative of the original library. The ordinary General Public License therefore permits such linking only if the entire combination fits its criteria of freedom. The Lesser General Public License permits more lax criteria for linking other code with the library.\n\nWe call this license the \"Lesser\" General Public License because it does Less to protect the user's freedom than the ordinary General Public License. It also provides other free software developers Less of an advantage over competing non-free programs. These disadvantages are the reason we use the ordinary General Public License for many libraries. However, the Lesser license provides advantages in certain special circumstances.\n\nFor example, on rare occasions, there may be a special need to encourage the widest possible use of a certain library, so that it becomes a de-facto standard. To achieve this, non-free programs must be allowed to use the library. A more frequent case is that a free library does the same job as widely used non-free libraries. In this case, there is little to gain by limiting the free library to free software only, so we use the Lesser General Public License.\n\nIn other cases, permission to use a particular library in non-free programs enables a greater number of people to use a large body of free software. For example, permission to use the GNU C Library in non-free programs enables many more people to use the whole GNU operating system, as well as its variant, the GNU\/Linux operating system.\n\nAlthough the Lesser General Public License is Less protective of the users' freedom, it does ensure that the user of a program that is linked with the Library has the freedom and the wherewithal to run that program using a modified version of the Library.\n\nThe precise terms and conditions for copying, distribution and modification follow. Pay close attention to the difference between a \"work based on the library\" and a \"work that uses the library\". The former contains code derived from the library, whereas the latter must be combined with the library in order to run.\n\nTERMS AND CONDITIONS FOR COPYING, DISTRIBUTION AND MODIFICATION\n0. This License Agreement applies to any software library or other program which contains a notice placed by the copyright holder or other authorized party saying it may be distributed under the terms of this Lesser General Public License (also called \"this License\"). Each licensee is addressed as \"you\".\n\nA \"library\" means a collection of software functions and\/or data prepared so as to be conveniently linked with application programs (which use some of those functions and data) to form executables.\n\nThe \"Library\", below, refers to any such software library or work which has been distributed under these terms. A \"work based on the Library\" means either the Library or any derivative work under copyright law: that is to say, a work containing the Library or a portion of it, either verbatim or with modifications and\/or translated straightforwardly into another language. (Hereinafter, translation is included without limitation in the term \"modification\".)\n\n\"Source code\" for a work means the preferred form of the work for making modifications to it. For a library, complete source code means all the source code for all modules it contains, plus any associated interface definition files, plus the scripts used to control compilation and installation of the library.\n\nActivities other than copying, distribution and modification are not covered by this License; they are outside its scope. The act of running a program using the Library is not restricted, and output from such a program is covered only if its contents constitute a work based on the Library (independent of the use of the Library in a tool for writing it). Whether that is true depends on what the Library does and what the program that uses the Library does.\n\n1. You may copy and distribute verbatim copies of the Library's complete source code as you receive it, in any medium, provided that you conspicuously and appropriately publish on each copy an appropriate copyright notice and disclaimer of warranty; keep intact all the notices that refer to this License and to the absence of any warranty; and distribute a copy of this License along with the Library.\n\nYou may charge a fee for the physical act of transferring a copy, and you may at your option offer warranty protection in exchange for a fee.\n\n2. You may modify your copy or copies of the Library or any portion of it, thus forming a work based on the Library, and copy and distribute such modifications or work under the terms of Section 1 above, provided that you also meet all of these conditions:\n\na) The modified work must itself be a software library.\nb) You must cause the files modified to carry prominent notices stating that you changed the files and the date of any change.\nc) You must cause the whole of the work to be licensed at no charge to all third parties under the terms of this License.\nd) If a facility in the modified Library refers to a function or a table of data to be supplied by an application program that uses the facility, other than as an argument passed when the facility is invoked, then you must make a good faith effort to ensure that, in the event an application does not supply such function or table, the facility still operates, and performs whatever part of its purpose remains meaningful.\n(For example, a function in a library to compute square roots has a purpose that is entirely well-defined independent of the application. Therefore, Subsection 2d requires that any application-supplied function or table used by this function must be optional: if the application does not supply it, the square root function must still compute square roots.)\n\nThese requirements apply to the modified work as a whole. If identifiable sections of that work are not derived from the Library, and can be reasonably considered independent and separate works in themselves, then this License, and its terms, do not apply to those sections when you distribute them as separate works. But when you distribute the same sections as part of a whole which is a work based on the Library, the distribution of the whole must be on the terms of this License, whose permissions for other licensees extend to the entire whole, and thus to each and every part regardless of who wrote it.\n\nThus, it is not the intent of this section to claim rights or contest your rights to work written entirely by you; rather, the intent is to exercise the right to control the distribution of derivative or collective works based on the Library.\n\nIn addition, mere aggregation of another work not based on the Library with the Library (or with a work based on the Library) on a volume of a storage or distribution medium does not bring the other work under the scope of this License.\n\n3. You may opt to apply the terms of the ordinary GNU General Public License instead of this License to a given copy of the Library. To do this, you must alter all the notices that refer to this License, so that they refer to the ordinary GNU General Public License, version 2, instead of to this License. (If a newer version than version 2 of the ordinary GNU General Public License has appeared, then you can specify that version instead if you wish.) Do not make any other change in these notices.\n\nOnce this change is made in a given copy, it is irreversible for that copy, so the ordinary GNU General Public License applies to all subsequent copies and derivative works made from that copy.\n\nThis option is useful when you wish to copy part of the code of the Library into a program that is not a library.\n\n4. You may copy and distribute the Library (or a portion or derivative of it, under Section 2) in object code or executable form under the terms of Sections 1 and 2 above provided that you accompany it with the complete corresponding machine-readable source code, which must be distributed under the terms of Sections 1 and 2 above on a medium customarily used for software interchange.\n\nIf distribution of object code is made by offering access to copy from a designated place, then offering equivalent access to copy the source code from the same place satisfies the requirement to distribute the source code, even though third parties are not compelled to copy the source along with the object code.\n\n5. A program that contains no derivative of any portion of the Library, but is designed to work with the Library by being compiled or linked with it, is called a \"work that uses the Library\". Such a work, in isolation, is not a derivative work of the Library, and therefore falls outside the scope of this License.\n\nHowever, linking a \"work that uses the Library\" with the Library creates an executable that is a derivative of the Library (because it contains portions of the Library), rather than a \"work that uses the library\". The executable is therefore covered by this License. Section 6 states terms for distribution of such executables.\n\nWhen a \"work that uses the Library\" uses material from a header file that is part of the Library, the object code for the work may be a derivative work of the Library even though the source code is not. Whether this is true is especially significant if the work can be linked without the Library, or if the work is itself a library. The threshold for this to be true is not precisely defined by law.\n\nIf such an object file uses only numerical parameters, data structure layouts and accessors, and small macros and small inline functions (ten lines or less in length), then the use of the object file is unrestricted, regardless of whether it is legally a derivative work. (Executables containing this object code plus portions of the Library will still fall under Section 6.)\n\nOtherwise, if the work is a derivative of the Library, you may distribute the object code for the work under the terms of Section 6. Any executables containing that work also fall under Section 6, whether or not they are linked directly with the Library itself.\n\n6. As an exception to the Sections above, you may also combine or link a \"work that uses the Library\" with the Library to produce a work containing portions of the Library, and distribute that work under terms of your choice, provided that the terms permit modification of the work for the customer's own use and reverse engineering for debugging such modifications.\n\nYou must give prominent notice with each copy of the work that the Library is used in it and that the Library and its use are covered by this License. You must supply a copy of this License. If the work during execution displays copyright notices, you must include the copyright notice for the Library among them, as well as a reference directing the user to the copy of this License. Also, you must do one of these things:\n\na) Accompany the work with the complete corresponding machine-readable source code for the Library including whatever changes were used in the work (which must be distributed under Sections 1 and 2 above); and, if the work is an executable linked with the Library, with the complete machine-readable \"work that uses the Library\", as object code and\/or source code, so that the user can modify the Library and then relink to produce a modified executable containing the modified Library. (It is understood that the user who changes the contents of definitions files in the Library will not necessarily be able to recompile the application to use the modified definitions.)\nb) Use a suitable shared library mechanism for linking with the Library. A suitable mechanism is one that (1) uses at run time a copy of the library already present on the user's computer system, rather than copying library functions into the executable, and (2) will operate properly with a modified version of the library, if the user installs one, as long as the modified version is interface-compatible with the version that the work was made with.\nc) Accompany the work with a written offer, valid for at least three years, to give the same user the materials specified in Subsection 6a, above, for a charge no more than the cost of performing this distribution.\nd) If distribution of the work is made by offering access to copy from a designated place, offer equivalent access to copy the above specified materials from the same place.\ne) Verify that the user has already received a copy of these materials or that you have already sent this user a copy.\nFor an executable, the required form of the \"work that uses the Library\" must include any data and utility programs needed for reproducing the executable from it. However, as a special exception, the materials to be distributed need not include anything that is normally distributed (in either source or binary form) with the major components (compiler, kernel, and so on) of the operating system on which the executable runs, unless that component itself accompanies the executable.\n\nIt may happen that this requirement contradicts the license restrictions of other proprietary libraries that do not normally accompany the operating system. Such a contradiction means you cannot use both them and the Library together in an executable that you distribute.\n\n7. You may place library facilities that are a work based on the Library side-by-side in a single library together with other library facilities not covered by this License, and distribute such a combined library, provided that the separate distribution of the work based on the Library and of the other library facilities is otherwise permitted, and provided that you do these two things:\n\na) Accompany the combined library with a copy of the same work based on the Library, uncombined with any other library facilities. This must be distributed under the terms of the Sections above.\nb) Give prominent notice with the combined library of the fact that part of it is a work based on the Library, and explaining where to find the accompanying uncombined form of the same work.\n8. You may not copy, modify, sublicense, link with, or distribute the Library except as expressly provided under this License. Any attempt otherwise to copy, modify, sublicense, link with, or distribute the Library is void, and will automatically terminate your rights under this License. However, parties who have received copies, or rights, from you under this License will not have their licenses terminated so long as such parties remain in full compliance.\n\n9. You are not required to accept this License, since you have not signed it. However, nothing else grants you permission to modify or distribute the Library or its derivative works. These actions are prohibited by law if you do not accept this License. Therefore, by modifying or distributing the Library (or any work based on the Library), you indicate your acceptance of this License to do so, and all its terms and conditions for copying, distributing or modifying the Library or works based on it.\n\n10. Each time you redistribute the Library (or any work based on the Library), the recipient automatically receives a license from the original licensor to copy, distribute, link with or modify the Library subject to these terms and conditions. You may not impose any further restrictions on the recipients' exercise of the rights granted herein. You are not responsible for enforcing compliance by third parties with this License.\n\n11. If, as a consequence of a court judgment or allegation of patent infringement or for any other reason (not limited to patent issues), conditions are imposed on you (whether by court order, agreement or otherwise) that contradict the conditions of this License, they do not excuse you from the conditions of this License. If you cannot distribute so as to satisfy simultaneously your obligations under this License and any other pertinent obligations, then as a consequence you may not distribute the Library at all. For example, if a patent license would not permit royalty-free redistribution of the Library by all those who receive copies directly or indirectly through you, then the only way you could satisfy both it and this License would be to refrain entirely from distribution of the Library.\n\nIf any portion of this section is held invalid or unenforceable under any particular circumstance, the balance of the section is intended to apply, and the section as a whole is intended to apply in other circumstances.\n\nIt is not the purpose of this section to induce you to infringe any patents or other property right claims or to contest validity of any such claims; this section has the sole purpose of protecting the integrity of the free software distribution system which is implemented by public license practices. Many people have made generous contributions to the wide range of software distributed through that system in reliance on consistent application of that system; it is up to the author\/donor to decide if he or she is willing to distribute software through any other system and a licensee cannot impose that choice.\n\nThis section is intended to make thoroughly clear what is believed to be a consequence of the rest of this License.\n\n12. If the distribution and\/or use of the Library is restricted in certain countries either by patents or by copyrighted interfaces, the original copyright holder who places the Library under this License may add an explicit geographical distribution limitation excluding those countries, so that distribution is permitted only in or among countries not thus excluded. In such case, this License incorporates the limitation as if written in the body of this License.\n\n13. The Free Software Foundation may publish revised and\/or new versions of the Lesser General Public License from time to time. Such new versions will be similar in spirit to the present version, but may differ in detail to address new problems or concerns.\n\nEach version is given a distinguishing version number. If the Library specifies a version number of this License which applies to it and \"any later version\", you have the option of following the terms and conditions either of that version or of any later version published by the Free Software Foundation. If the Library does not specify a license version number, you may choose any version ever published by the Free Software Foundation.\n\n14. If you wish to incorporate parts of the Library into other free programs whose distribution conditions are incompatible with these, write to the author to ask for permission. For software which is copyrighted by the Free Software Foundation, write to the Free Software Foundation; we sometimes make exceptions for this. Our decision will be guided by the two goals of preserving the free status of all derivatives of our free software and of promoting the sharing and reuse of software generally.\n\nNO WARRANTY\n\n15. BECAUSE THE LIBRARY IS LICENSED FREE OF CHARGE, THERE IS NO WARRANTY FOR THE LIBRARY, TO THE EXTENT PERMITTED BY APPLICABLE LAW. EXCEPT WHEN OTHERWISE STATED IN WRITING THE COPYRIGHT HOLDERS AND\/OR OTHER PARTIES PROVIDE THE LIBRARY \"AS IS\" WITHOUT WARRANTY OF ANY KIND, EITHER EXPRESSED OR IMPLIED, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED WARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE. THE ENTIRE RISK AS TO THE QUALITY AND PERFORMANCE OF THE LIBRARY IS WITH YOU. SHOULD THE LIBRARY PROVE DEFECTIVE, YOU ASSUME THE COST OF ALL NECESSARY SERVICING, REPAIR OR CORRECTION.\n\n16. IN NO EVENT UNLESS REQUIRED BY APPLICABLE LAW OR AGREED TO IN WRITING WILL ANY COPYRIGHT HOLDER, OR ANY OTHER PARTY WHO MAY MODIFY AND\/OR REDISTRIBUTE THE LIBRARY AS PERMITTED ABOVE, BE LIABLE TO YOU FOR DAMAGES, INCLUDING ANY GENERAL, SPECIAL, INCIDENTAL OR CONSEQUENTIAL DAMAGES ARISING OUT OF THE USE OR INABILITY TO USE THE LIBRARY (INCLUDING BUT NOT LIMITED TO LOSS OF DATA OR DATA BEING RENDERED INACCURATE OR LOSSES SUSTAINED BY YOU OR THIRD PARTIES OR A FAILURE OF THE LIBRARY TO OPERATE WITH ANY OTHER SOFTWARE), EVEN IF SUCH HOLDER OR OTHER PARTY HAS BEEN ADVISED OF THE POSSIBILITY OF SUCH DAMAGES.\n\nEND OF TERMS AND CONDITIONS\nHow to Apply These Terms to Your New Libraries\nIf you develop a new library, and you want it to be of the greatest possible use to the public, we recommend making it free software that everyone can redistribute and change. You can do so by permitting redistribution under these terms (or, alternatively, under the terms of the ordinary General Public License).\n\nTo apply these terms, attach the following notices to the library. It is safest to attach them to the start of each source file to most effectively convey the exclusion of warranty; and each file should have at least the \"copyright\" line and a pointer to where the full notice is found.\n\none line to give the library's name and an idea of what it does.\nCopyright (C) year  name of author\n\nThis library is free software; you can redistribute it and\/or\nmodify it under the terms of the GNU Lesser General Public\nLicense as published by the Free Software Foundation; either\nversion 2.1 of the License, or (at your option) any later version.\n\nThis library is distributed in the hope that it will be useful,\nbut WITHOUT ANY WARRANTY; without even the implied warranty of\nMERCHANTABILITY or FITNESS FOR A PARTICULAR PURPOSE.  See the GNU\nLesser General Public License for more details.\n\nYou should have received a copy of the GNU Lesser General Public\nLicense along with this library; if not, write to the Free Software\nFoundation, Inc., 51 Franklin Street, Fifth Floor, Boston, MA  02110-1301  USA\n \n## 5.5 The MIT License\n \nCopyright <YEAR> <COPYRIGHT HOLDER>\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of this software and associated documentation files (the \"Software\"), to deal in the Software without restriction, including without limitation the rights to use, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies of the Software, and to permit persons to whom the Software is furnished to do so, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all copies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY, FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM, OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE SOFTWARE.\n\n  \n## 5.6 License agreement for matplotlib versions 1.3.0 and later\n=========================================================\n\n1. This LICENSE AGREEMENT is between the Matplotlib Development Team\n(\"MDT\"), and the Individual or Organization (\"Licensee\") accessing and\notherwise using matplotlib software in source or binary form and its\nassociated documentation.\n\n2. Subject to the terms and conditions of this License Agreement, MDT\nhereby grants Licensee a nonexclusive, royalty-free, world-wide license\nto reproduce, analyze, test, perform and\/or display publicly, prepare\nderivative works, distribute, and otherwise use matplotlib\nalone or in any derivative version, provided, however, that MDT's\nLicense Agreement and MDT's notice of copyright, i.e., \"Copyright (c)\n2012- Matplotlib Development Team; All Rights Reserved\" are retained in\nmatplotlib  alone or in any derivative version prepared by\nLicensee.\n\n3. In the event Licensee prepares a derivative work that is based on or\nincorporates matplotlib or any part thereof, and wants to\nmake the derivative work available to others as provided herein, then\nLicensee hereby agrees to include in any such work a brief summary of\nthe changes made to matplotlib .\n\n4. MDT is making matplotlib available to Licensee on an \"AS\nIS\" basis.  MDT MAKES NO REPRESENTATIONS OR WARRANTIES, EXPRESS OR\nIMPLIED.  BY WAY OF EXAMPLE, BUT NOT LIMITATION, MDT MAKES NO AND\nDISCLAIMS ANY REPRESENTATION OR WARRANTY OF MERCHANTABILITY OR FITNESS\nFOR ANY PARTICULAR PURPOSE OR THAT THE USE OF MATPLOTLIB\nWILL NOT INFRINGE ANY THIRD PARTY RIGHTS.\n\n5. MDT SHALL NOT BE LIABLE TO LICENSEE OR ANY OTHER USERS OF MATPLOTLIB\n FOR ANY INCIDENTAL, SPECIAL, OR CONSEQUENTIAL DAMAGES OR\nLOSS AS A RESULT OF MODIFYING, DISTRIBUTING, OR OTHERWISE USING\nMATPLOTLIB , OR ANY DERIVATIVE THEREOF, EVEN IF ADVISED OF\nTHE POSSIBILITY THEREOF.\n\n6. This License Agreement will automatically terminate upon a material\nbreach of its terms and conditions.\n\n7. Nothing in this License Agreement shall be deemed to create any\nrelationship of agency, partnership, or joint venture between MDT and\nLicensee.  This License Agreement does not grant permission to use MDT\ntrademarks or trade name in a trademark sense to endorse or promote\nproducts or services of Licensee, or any third party.\n\n8. By copying, installing or otherwise using matplotlib ,\nLicensee agrees to be bound by the terms and conditions of this License\nAgreement.\n \n## 5.7 3-clause license (\"BSD License 2.0\", \"Revised BSD License\", \"New BSD License\", or \"Modified BSD License\")\nCopyright (c) <year>, <copyright holder>\nAll rights reserved.\n\nRedistribution and use in source and binary forms, with or without\nmodification, are permitted provided that the following conditions are met:\n    * Redistributions of source code must retain the above copyright\n      notice, this list of conditions and the following disclaimer.\n    * Redistributions in binary form must reproduce the above copyright\n      notice, this list of conditions and the following disclaimer in the\n      documentation and\/or other materials provided with the distribution.\n    * Neither the name of the <organization> nor the\n      names of its contributors may be used to endorse or promote products\n      derived from this software without specific prior written permission.\n\nTHIS SOFTWARE IS PROVIDED BY THE COPYRIGHT HOLDERS AND CONTRIBUTORS \"AS IS\" AND\nANY EXPRESS OR IMPLIED WARRANTIES, INCLUDING, BUT NOT LIMITED TO, THE IMPLIED\nWARRANTIES OF MERCHANTABILITY AND FITNESS FOR A PARTICULAR PURPOSE ARE\nDISCLAIMED. IN NO EVENT SHALL <COPYRIGHT HOLDER> BE LIABLE FOR ANY\nDIRECT, INDIRECT, INCIDENTAL, SPECIAL, EXEMPLARY, OR CONSEQUENTIAL DAMAGES\n(INCLUDING, BUT NOT LIMITED TO, PROCUREMENT OF SUBSTITUTE GOODS OR SERVICES;\nLOSS OF USE, DATA, OR PROFITS; OR BUSINESS INTERRUPTION) HOWEVER CAUSED AND\nON ANY THEORY OF LIABILITY, WHETHER IN CONTRACT, STRICT LIABILITY, OR TORT\n(INCLUDING NEGLIGENCE OR OTHERWISE) ARISING IN ANY WAY OUT OF THE USE OF THIS\nSOFTWARE, EVEN IF ADVISED OF THE POSSIBILITY OF SUCH DAMAGE.\n \n## 5.8 Apache License\n\nVersion 2.0, January 2004\n\nhttp:\/\/www.apache.org\/licenses\/\n\nTERMS AND CONDITIONS FOR USE, REPRODUCTION, AND DISTRIBUTION\n\n1. Definitions.\n\n\"License\" shall mean the terms and conditions for use, reproduction, and distribution as defined by Sections 1 through 9 of this document.\n\n\"Licensor\" shall mean the copyright owner or entity authorized by the copyright owner that is granting the License.\n\n\"Legal Entity\" shall mean the union of the acting entity and all other entities that control, are controlled by, or are under common control with that entity. For the purposes of this definition, \"control\" means (i) the power, direct or indirect, to cause the direction or management of such entity, whether by contract or otherwise, or (ii) ownership of fifty percent (50%) or more of the outstanding shares, or (iii) beneficial ownership of such entity.\n\n\"You\" (or \"Your\") shall mean an individual or Legal Entity exercising permissions granted by this License.\n\n\"Source\" form shall mean the preferred form for making modifications, including but not limited to software source code, documentation source, and configuration files.\n\n\"Object\" form shall mean any form resulting from mechanical transformation or translation of a Source form, including but not limited to compiled object code, generated documentation, and conversions to other media types.\n\n\"Work\" shall mean the work of authorship, whether in Source or Object form, made available under the License, as indicated by a copyright notice that is included in or attached to the work (an example is provided in the Appendix below).\n\n\"Derivative Works\" shall mean any work, whether in Source or Object form, that is based on (or derived from) the Work and for which the editorial revisions, annotations, elaborations, or other modifications represent, as a whole, an original work of authorship. For the purposes of this License, Derivative Works shall not include works that remain separable from, or merely link (or bind by name) to the interfaces of, the Work and Derivative Works thereof.\n\n\"Contribution\" shall mean any work of authorship, including the original version of the Work and any modifications or additions to that Work or Derivative Works thereof, that is intentionally submitted to Licensor for inclusion in the Work by the copyright owner or by an individual or Legal Entity authorized to submit on behalf of the copyright owner. For the purposes of this definition, \"submitted\" means any form of electronic, verbal, or written communication sent to the Licensor or its representatives, including but not limited to communication on electronic mailing lists, source code control systems, and issue tracking systems that are managed by, or on behalf of, the Licensor for the purpose of discussing and improving the Work, but excluding communication that is conspicuously marked or otherwise designated in writing by the copyright owner as \"Not a Contribution.\"\n\n\"Contributor\" shall mean Licensor and any individual or Legal Entity on behalf of whom a Contribution has been received by Licensor and subsequently incorporated within the Work.\n\n2. Grant of Copyright License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable copyright license to reproduce, prepare Derivative Works of, publicly display, publicly perform, sublicense, and distribute the Work and such Derivative Works in Source or Object form.\n\n3. Grant of Patent License. Subject to the terms and conditions of this License, each Contributor hereby grants to You a perpetual, worldwide, non-exclusive, no-charge, royalty-free, irrevocable (except as stated in this section) patent license to make, have made, use, offer to sell, sell, import, and otherwise transfer the Work, where such license applies only to those patent claims licensable by such Contributor that are necessarily infringed by their Contribution(s) alone or by combination of their Contribution(s) with the Work to which such Contribution(s) was submitted. If You institute patent litigation against any entity (including a cross-claim or counterclaim in a lawsuit) alleging that the Work or a Contribution incorporated within the Work constitutes direct or contributory patent infringement, then any patent licenses granted to You under this License for that Work shall terminate as of the date such litigation is filed.\n\n4. Redistribution. You may reproduce and distribute copies of the Work or Derivative Works thereof in any medium, with or without modifications, and in Source or Object form, provided that You meet the following conditions:\n\nYou must give any other recipients of the Work or Derivative Works a copy of this License; and\nYou must cause any modified files to carry prominent notices stating that You changed the files; and\nYou must retain, in the Source form of any Derivative Works that You distribute, all copyright, patent, trademark, and attribution notices from the Source form of the Work, excluding those notices that do not pertain to any part of the Derivative Works; and\nIf the Work includes a \"NOTICE\" text file as part of its distribution, then any Derivative Works that You distribute must include a readable copy of the attribution notices contained within such NOTICE file, excluding those notices that do not pertain to any part of the Derivative Works, in at least one of the following places: within a NOTICE text file distributed as part of the Derivative Works; within the Source form or documentation, if provided along with the Derivative Works; or, within a display generated by the Derivative Works, if and wherever such third-party notices normally appear. The contents of the NOTICE file are for informational purposes only and do not modify the License. You may add Your own attribution notices within Derivative Works that You distribute, alongside or as an addendum to the NOTICE text from the Work, provided that such additional attribution notices cannot be construed as modifying the License.\n\nYou may add Your own copyright statement to Your modifications and may provide additional or different license terms and conditions for use, reproduction, or distribution of Your modifications, or for any such Derivative Works as a whole, provided Your use, reproduction, and distribution of the Work otherwise complies with the conditions stated in this License.\n5. Submission of Contributions. Unless You explicitly state otherwise, any Contribution intentionally submitted for inclusion in the Work by You to the Licensor shall be under the terms and conditions of this License, without any additional terms or conditions. Notwithstanding the above, nothing herein shall supersede or modify the terms of any separate license agreement you may have executed with Licensor regarding such Contributions.\n\n6. Trademarks. This License does not grant permission to use the trade names, trademarks, service marks, or product names of the Licensor, except as required for reasonable and customary use in describing the origin of the Work and reproducing the content of the NOTICE file.\n\n7. Disclaimer of Warranty. Unless required by applicable law or agreed to in writing, Licensor provides the Work (and each Contributor provides its Contributions) on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied, including, without limitation, any warranties or conditions of TITLE, NON-INFRINGEMENT, MERCHANTABILITY, or FITNESS FOR A PARTICULAR PURPOSE. You are solely responsible for determining the appropriateness of using or redistributing the Work and assume any risks associated with Your exercise of permissions under this License.\n\n8. Limitation of Liability. In no event and under no legal theory, whether in tort (including negligence), contract, or otherwise, unless required by applicable law (such as deliberate and grossly negligent acts) or agreed to in writing, shall any Contributor be liable to You for damages, including any direct, indirect, special, incidental, or consequential damages of any character arising as a result of this License or out of the use or inability to use the Work (including but not limited to damages for loss of goodwill, work stoppage, computer failure or malfunction, or any and all other commercial damages or losses), even if such Contributor has been advised of the possibility of such damages.\n\n9. Accepting Warranty or Additional Liability. While redistributing the Work or Derivative Works thereof, You may choose to offer, and charge a fee for, acceptance of support, warranty, indemnity, or other liability obligations and\/or rights consistent with this License. However, in accepting such obligations, You may act only on Your own behalf and on Your sole responsibility, not on behalf of any other Contributor, and only if You agree to indemnify, defend, and hold each Contributor harmless for any liability incurred by, or claims asserted against, such Contributor by reason of your accepting any such warranty or additional liability.\n\nEND OF TERMS AND CONDITIONS\n\n## 5.9 Geopy License\nCopyright (c) 2006-2018 geopy authors (see AUTHORS)\n\nPermission is hereby granted, free of charge, to any person obtaining a copy of\nthis software and associated documentation files (the \"Software\"), to deal in\nthe Software without restriction, including without limitation the rights to\nuse, copy, modify, merge, publish, distribute, sublicense, and\/or sell copies\nof the Software, and to permit persons to whom the Software is furnished to do\nso, subject to the following conditions:\n\nThe above copyright notice and this permission notice shall be included in all\ncopies or substantial portions of the Software.\n\nTHE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\nIMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\nFITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\nAUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\nLIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\nOUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE\nSOFTWARE.","4afe04bb":"#### 3.3.4.2 Removing Papers that Do Not Appear to be Relevant to the Corpus as a Whole\n\nTo identify papers that do not appear to be relevant to the corpus as a whole - i.e., research on COVID-19 and related coronaviruses - we perform TF-IDF analysis, using trigrams, across all the paper text. Once we have the corpus vocabulary (features) and TF-IDF scores, we use t-SNE dimensionality reduction to compare the TF-IDF paper scores and highlight papers with scores that significantly differ from those of the main body of papers. A significant difference in TF-IDF score indicates that the content of the paper is not that closely related to that of the main body of papers.\n\nThe ```dimension_reduction()``` function below uses the *scikit-learn* ```TfidfVectorizer``` class to extract trigrams (features) and and compute TF-IDF scores for the text of all the papers in the dataset. Since the number of features across the papers may become quite large, the code then uses t-SNE dimensionality reduction to project the paper TF-IDF scores into a three-dimensional space that can be visualized and in which outliers can be identified per standard criteria.\n\nThe ```visualizing_dimensions()``` function is used to visualize the results of the t-SNE dimensionality reduction in a 3-D plot.\n\nThe ```outlier_removals()``` function is used to identify and remove outliers in the t-SNE dimensionality reduction results. To quantitatively identify the outlier papers, we use a standard 95% confidence interval criterion. Specifically, we select those papers with scores more than 2.5 standard deviations (this value is selected based on experimentation) away from the mean of the main cluster in any of the three axes. Once identified, the outlier papers are removed from the input dataset.\n\nThe ```full_cleaning_process()``` function uses the other three functions to execute the process of TF-IDF analysis, t-SNE dimensionality reduction, and outlier identification and removal, for the input dataset. The function also reports how many outliers were identified, prints the text of the outliers (to illustrate why they are outliers), and shows the 3-D plots of the paper scores before and after the cleaning process.","9dc581b9":"**Note:** This approach was tested against the CORD-19 dataset available on 04\/16\/2020. As the corpus of papers grows, it is possible that this model will eventually fail to run due to Kaggle's memory limitations. Should this occur, we recommend downloading this notebook and running it locally in an environment with more than 16 GB of RAM.","508b052e":"In the output above, the first plot shows the results of the relevancy analysis before outlier removal. We see that most of the papers are clustered together, with a handful of papers exhibiting scores significantly outside the main cluster (the number may vary depending on the number of papers included in the dataset at this point based on publication data filtering). Those papers are outliers that we want to remove from the dataset - the number of outliers identified is given in the output. From the text of the outlier papers, we can get some idea of why they are not relevant - typically it is because they do not have meaningful text content. The second plot shows the relevancy results after outlier removal. We see now that the remaining papers are clustered fairly closely together in terms of TD-IDF score. This indicates that our cleaned corpus is more homogeneous now in terms of the content of the papers, and we have successfully removed those papers that are not relevant or helpful with respect to the subject of interest.","30177aee":"#  3. The Code\n\nThe major tasks we performed to develope code for analyzing CORD-19 data to provide insight on what is known about COVID-19 transmission, incubation, and environmental stability consists of the following:\n\n1. Preparation\n2. Acquiring and preprocessing the dataset\n3. Cleaning the dataset \n4. Topic modeling on the processed dataset\n5. Data search engine - Insights on Task questions","30aa3d74":"# 4.1 Task Report\n\nTo generate results for each of the questions posed as part of Task 1, we provide here an interactive widget to exercise the search engine. All the individual questions are preloaded and a can be selected via a drop-down box. Once a particular question is selected the search engine is called to generate the report.\n\nThe interactivity of the search engine as we have provided it here is lost when this notebook is committed to Kaggle and viewed on the Kaggle site as a static notebook. To fully interact with the search engine, we recommend that you download the notebook and run it on your own PC or server. This also allows you to go into the widget code below and refine any of the questions there (e.g., add additional keywords or synonyms) or create your own questions.","1582de1d":"## 3.3 Cleaning the Dataset\n\nNow that we have extracted the relevant raw data for all the papers and reformatted it into a single *pandas* DataFrame, we need to do some additional cleaning of the dataset. This includes:\n\n- removing unnecessary or unhelpful characters and words from the paper text\n- removing duplicate papers\n- making the country names uniform English names\n- removing papers that are outliers\n- removing papers who primary language is not English\n\n### 3.3.1 Cleaning the Paper Text Sections\n\nThe text extracted for the paper JSON files is somewhat dirty. The function ```cleaning_dataset()``` below takes as input a DataFrame, and a list of columns in that DataFrame to clean, and cleans text of those columns. Specifically, it:\n\n- fills holes (i.e., null values) in the DataFrame with the string, \"No Information\"\n- removes unnecessary garbage characters and white space\n- removes references and annotations (i.e., [1], (1), etc.)\n- removes \"figure X.X\" references \n\nAdditionally for the abstract text of each paper, it removes unnecessary starting words, such as \"background\" or \"abstract\", and also counts the number of words in each abstract and adds a column to the DataFrame which contains the number of words in the abstract for each paper.\n","aaadb6eb":"![Task1_printout.JPG](attachment:Task1_printout.JPG)","9b9540e2":"#### Compute the Document-Topic Matrix\n\nNext we compute the document-topic matrix, which gives the probabilities of selecting each topic when sampling from each of the documents. We use the *ktrain* ```build()``` method for this.","3d057b50":"<a id='Figure_1'><\/a>\n**Figure 1**  Example Search Engine Results","f3e2953c":"### 4.1.1 Task 1 - Questions"}}