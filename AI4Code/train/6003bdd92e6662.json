{"cell_type":{"f3edba8a":"code","0dc21674":"code","ac748aef":"code","b6ad6334":"code","8d5daee8":"code","e7be5901":"code","7c3c12ab":"code","360daf66":"code","bc52a70f":"code","9c8e500c":"code","d750624d":"code","7125e3fb":"code","9c950497":"code","3b5191ef":"code","7eea0674":"code","031c8818":"code","aad0ad7c":"code","873c4fe0":"code","9fada9d4":"code","5e1c9471":"code","aeae9a32":"code","47ccc3f7":"code","2cbda092":"code","fe6227b7":"code","03421e9f":"code","b1fff65b":"code","1d3e3061":"code","f4d1ae95":"code","e729e3fb":"code","eeb12261":"code","b3217a59":"code","45ae2068":"code","9155dc80":"code","825f7515":"code","6be8b131":"code","9112b4ea":"code","43404ba8":"code","bc55e2d5":"code","b9a1311b":"code","4cd10790":"code","5dd5d5ad":"code","67b4a02e":"code","7318799e":"code","2ccce244":"code","2dd37755":"code","5ceff159":"code","7b318b9e":"code","8481ec17":"code","e5e32a6f":"code","f4d0844d":"code","26de78b9":"code","cd539fd7":"code","9d40489a":"code","7496ae4c":"code","3a7cc9a3":"code","7e199a77":"code","899cdb42":"code","ba941e64":"code","57fc665f":"code","ca57ca18":"code","e1e4503c":"code","064240c7":"code","01e6516a":"code","5335e536":"code","a6e7ce6b":"code","042095d7":"code","4981f5d3":"code","99c18dc0":"code","adaae29a":"code","649b1e0c":"code","067b0619":"code","49ba28a9":"code","b1babd96":"code","6eb9d92b":"code","26ae846f":"code","546b84f2":"code","6f4a0d3d":"markdown","bb6ee889":"markdown","ed8516ef":"markdown","37f7bf0e":"markdown","8da776e0":"markdown","5246171d":"markdown","b93f898a":"markdown","78785fd2":"markdown","67b8167f":"markdown","ef767e66":"markdown","46fc7f31":"markdown","df74659c":"markdown","ad3a0ed4":"markdown","7c65abfa":"markdown","ee46ce08":"markdown","d98adac4":"markdown","47038407":"markdown","e0593a30":"markdown","523aa85e":"markdown","2c340a57":"markdown","88723e6a":"markdown","0b51b46c":"markdown","b50da7d6":"markdown","384f1641":"markdown","af094b71":"markdown","3f65ca07":"markdown","628c40c6":"markdown","7c241879":"markdown","5b7549f0":"markdown","96ff63da":"markdown","d1c5a111":"markdown","f1550d4d":"markdown","e1c2954b":"markdown","e2344d33":"markdown"},"source":{"f3edba8a":"# To print multiple output in a cell\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = 'all'","0dc21674":"## Import all the required libraries\n\nimport pandas as pd\nimport numpy as np\nimport os\n\nimport matplotlib.pyplot as plt\n# % matplotlib inline","ac748aef":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","b6ad6334":"train=pd.read_csv('..\/input\/janatahack-demand-forecasting-analytics-vidhya\/train.csv')\n\ntest=pd.read_csv('..\/input\/janatahack-demand-forecasting-analytics-vidhya\/test.csv')\n\nsample=pd.read_csv('..\/input\/janatahack-demand-forecasting-analytics-vidhya\/sample_submission.csv')","8d5daee8":"train.head(10)\nprint('Shape of training data is {}'.format(train.shape))\n\nprint('-------------'*5)\n\ntest.head()\nprint('Shape of test data is {}'.format(test.shape))\n\nprint('--------------'*5)\n\nsample.head()","e7be5901":"train.describe()","7c3c12ab":"train.isnull().sum()","360daf66":"train['week'].unique()","bc52a70f":"# Number of units sold in accordance with the week\n\ntrain.groupby('week').sum()['units_sold'].plot(figsize=(12,8))\nfont = {'family': 'serif',\n        'color':  'darkred',\n        'weight': 'normal',\n        'size': 26,\n        }\nplt.xlabel('Week',fontdict=font)\nplt.ylabel('units_sold',fontdict=font)\n","9c8e500c":"# amount earned through sales in each week\n\ntrain.groupby('week').sum()['total_price'].plot(figsize=(12,8))\nfont = {'family': 'serif',\n        'color':  'darkred',\n        'weight': 'normal',\n        'size': 26,\n        }\nplt.xlabel('Week',fontdict=font)\nplt.ylabel('total_price',fontdict=font)","d750624d":"train['store_id'].unique()","7125e3fb":"## product sold by each of the store\n\n\ntrain.groupby('store_id').sum()['units_sold'].plot(figsize=(15,8),kind='bar')\nfont = {'family': 'serif',\n        'color':  'darkred',\n        'weight': 'normal',\n        'size': 26,\n        }\nplt.xlabel('store_id',fontdict=font)\nplt.ylabel('units_sold',fontdict=font)","9c950497":"## Product was on display at a prominent place at the store\n\n# Impact on sales on the basis of display\ntrain.groupby(['is_display_sku','store_id']).sum()['units_sold']","3b5191ef":"# join test and train data\n\ntrain['train_or_test']='train'\ntest['train_or_test']='test'\ndf=pd.concat([train,test])","7eea0674":"df.head()\ndf.shape","031c8818":"# function to utilize date time column i.e '''week'''\n\ndef create_week_date_featues(dataframe):\n\n    df['Month'] = pd.to_datetime(df['week']).dt.month\n\n    df['Day'] = pd.to_datetime(df['week']).dt.day\n\n    df['Dayofweek'] = pd.to_datetime(df['week']).dt.dayofweek\n\n    df['DayOfyear'] = pd.to_datetime(df['week']).dt.dayofyear\n\n    df['Week'] = pd.to_datetime(df['week']).dt.week\n\n    df['Quarter'] = pd.to_datetime(df['week']).dt.quarter \n\n    df['Is_month_start'] = pd.to_datetime(df['week']).dt.is_month_start\n\n    df['Is_month_end'] = pd.to_datetime(df['week']).dt.is_month_end\n\n    df['Is_quarter_start'] = pd.to_datetime(df['week']).dt.is_quarter_start\n\n    df['Is_quarter_end'] = pd.to_datetime(df['week']).dt.is_quarter_end\n\n    df['Is_year_start'] = pd.to_datetime(df['week']).dt.is_year_start\n\n    df['Is_year_end'] = pd.to_datetime(df['week']).dt.is_year_end\n\n    df['Semester'] = np.where(df['week'].isin([1,2]),1,2)\n\n    df['Is_weekend'] = np.where(df['week'].isin([5,6]),1,0)\n\n    df['Is_weekday'] = np.where(df['week'].isin([0,1,2,3,4]),1,0)\n\n    df['Days_in_month'] = pd.to_datetime(df['week']).dt.days_in_month\n    \n    return df","aad0ad7c":"df=create_week_date_featues(df)","873c4fe0":"df.head(5)\ndf.shape","9fada9d4":"from sklearn.preprocessing import LabelEncoder","5e1c9471":"col=['store_id','sku_id','Is_month_start','Is_month_end','Is_quarter_start','Is_quarter_end','Is_year_start','Is_year_end']","aeae9a32":"for i in col:\n    df = pd.get_dummies(df, columns=[i])","47ccc3f7":"df.head()","2cbda092":"# col2=['Is_month_start','Is_month_end','Is_quarter_start','Is_quarter_end','Is_year_start','Is_year_end']\n\n# for i in col2:\n#     df = pd.get_dummies(df, columns=[i])","fe6227b7":"# drop the columns\ndf.drop(['record_ID','week'],inplace=True,axis=1)","03421e9f":"df.head()\ndf.shape","b1fff65b":"# Total price columns\n\ndf['total_price'].plot(kind='hist')","1d3e3061":"df['total_price']=np.log1p(df['total_price'])\ndf['total_price'].plot(kind='hist')","f4d1ae95":"df['base_price'].plot(kind='hist')","e729e3fb":"df['base_price']=np.log1p(df['base_price'])\ndf['base_price'].plot(kind='hist')","eeb12261":"df.head()","b3217a59":"train_1=df.loc[df.train_or_test.isin(['train'])]\ntest_1=df.loc[df.train_or_test.isin(['test'])]\ntrain_1.drop(columns={'train_or_test'},axis=1,inplace=True)\ntest_1.drop(columns={'train_or_test'},axis=1,inplace=True)","45ae2068":"train_1.head()\ntrain_1.shape\ntest_1.shape\ntest_1.head()","9155dc80":"test_1.drop(['units_sold'],axis=1,inplace=True)","825f7515":"train_1.shape\ntest_1.shape","6be8b131":"x=train_1.drop(['units_sold'],axis=1)\ny=train_1['units_sold']","9112b4ea":"x=x.values\ntest_data=test_1.values\n\ny=y.values","43404ba8":"x.shape\ntest_data.shape\n","bc55e2d5":"from sklearn.model_selection import train_test_split","b9a1311b":"# x_train,x_valid,y_train,y_valid=train_test_split(x,y,test_size=0.35)","4cd10790":"# x_train.shape","5dd5d5ad":"from xgboost import XGBRegressor\nfrom xgboost import plot_importance\nimport xgboost as xgb\n\n# function to plot all features based out of its importance.\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\n\nimport time","67b4a02e":"def rmsle(y_true, y_pred):\n    return np.sqrt(np.mean(np.power(np.log1p(y_true)-np.log1p(y_pred), 2)))","7318799e":"# Perform cross-validation\nfrom sklearn.model_selection import train_test_split, KFold, StratifiedKFold","2ccce244":"model = XGBRegressor(\n    max_depth=12,\n    booster = \"gbtree\",\n    n_estimators=200,\n    eval_metric = 'rmse',\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,\n    seed=42,\n    objective='reg:linear')","2dd37755":"kfold, scores = KFold(n_splits=5, shuffle=True, random_state=0), list()\n\nfor train, test in kfold.split(x):\n    x_train, x_test =x[train], x[test]\n    y_train, y_test = y[train], y[test]\n    model.fit(x_train, y_train,verbose=True,\n              eval_set=[(x_train, y_train), (x_test, y_test)],\n              early_stopping_rounds = 50)\n#     preds = model.predict(x_test)\n#     score = rmsle(y_test, preds)\n#     scores.append(score)\n#     print(score)\n    \n    \n# print(\"Average: \", sum(scores)\/len(scores))","5ceff159":"# # defint the model parameters\n\n# ts = time.time()\n\n# model = XGBRegressor(\n#     max_depth=12,\n#     booster = \"gbtree\",\n#     n_estimators=500,\n#     min_child_weight=350, \n#     colsample_bytree=0.8, \n#     subsample=0.8, \n#     eta=0.3,\n#     seed=42,\n#     objective='reg:linear')\n\n# model.fit(\n#     x_train, \n#     y_train, \n#     eval_metric=\"rmse\", \n#     eval_set=[(x_train, y_train), (x_valid, y_valid)], \n#     verbose=True, \n#     early_stopping_rounds = 100)\n\n# time.time() - ts","7b318b9e":"test_data.shape","8481ec17":"# create prediction on test data\n\npred=model.predict(test_data)","e5e32a6f":"len(pred)","f4d0844d":"# sample['units_sold']=pred.round()\n\nsample['units_sold']=pred\n","26de78b9":"sample.head()","cd539fd7":"sample['units_sold'].unique()","9d40489a":"sample['units_sold']=abs(sample['units_sold']).astype('int')","7496ae4c":"sample.to_csv('submission_xgb.csv',index=False,encoding='utf-8')","3a7cc9a3":"sub_path = \"..\/input\/testing-minmaxbestbasestacking\"\nall_files = os.listdir(sub_path)\nprint(all_files)","7e199a77":"for f in all_files:\n    print(f)","899cdb42":"d=pd.read_csv('..\/input\/testing-minmaxbestbasestacking\/866_126527_us_submission_lgbm_22.csv')\nd.head()\nd.shape","ba941e64":"# # Read and concatenate submissions\n\nouts = [pd.read_csv(os.path.join(sub_path, f), index_col=0) for f in all_files ]\nconcat_sub = pd.concat(outs, axis=1)\nconcat_sub.head()","57fc665f":"cols = list(map(lambda x: \"target\" + str(x), range(len(concat_sub.columns))))\n\ncols = list(map(lambda x: \"target\" + str(x), range(len(concat_sub.columns))))\nconcat_sub.columns = cols","ca57ca18":"concat_sub.head()","e1e4503c":"concat_sub.reset_index(inplace=True)","064240c7":"concat_sub.head()","01e6516a":"ncol = concat_sub.shape[1]\nncol","5335e536":"# get the data fields ready for stacking\nconcat_sub['target_mean'] = concat_sub.iloc[:, 1:ncol].mean(axis=1)\nconcat_sub['target_median'] = concat_sub.iloc[:, 1:ncol].median(axis=1)\nconcat_sub.head()","a6e7ce6b":"concat_sub.describe()","042095d7":"# Create 1st submission for the mean with round\n# concat_sub['target_mean'].round()\n","4981f5d3":"col2=['record_ID','units_sold']","99c18dc0":"# concat_sub['target'] = concat_sub['target_mean']\n\nconcat_sub['target'] = concat_sub['target_mean'].round()\ndata=concat_sub[['record_ID', 'target']]\ndata.columns=col2","adaae29a":"data.head()","649b1e0c":"# data.to_csv('submission_mean.csv', index=False, float_format='%.6f')\n\ndata.to_csv('submission_mean_round.csv', index=False, float_format='%.6f')","067b0619":"# Create 1st submission for the median","49ba28a9":"data_med=data","b1babd96":"data_med['units_sold']=concat_sub['target_median']\ndata_med","6eb9d92b":"data_med.to_csv('submission_median.csv', index=False, float_format='%.6f')","26ae846f":"data['units_sold']= 1.85\/3 * data['units_sold'] + 1.15\/3 * data_med['units_sold']","546b84f2":"data.to_csv('submission_mean_median_blend_2.csv', index=False, float_format='%.6f')","6f4a0d3d":"## Data Preprocessing","bb6ee889":"## Data Description","ed8516ef":"## Got this \n\n> Your private score for this submission is : 453.80995302601514, \n> Had it been a live contest, your rank would be : 33\n\nRemember my final standing in the competion is 44. So in both cases my ranking is improve.\n\n- So Ensemble techniqe is working.\n\n- Be ready to use it in upcoming hackathons.","37f7bf0e":"## Treating skewed features","8da776e0":"> Edit-23\/07\/2020","5246171d":"## Time to train the model","b93f898a":"## MinMaxBestBaseStacking","78785fd2":"## Hint1:  \n\nTreat the skewness of the target variable too. \n- In my final notebook i am using it and getting a better score.","67b8167f":"### Create the final submission","ef767e66":"To begin this exploratory analysis, first import libraries and define functions for plotting the data using `matplotlib`. Depending on the data, not all plots will be made. (Hey, I'm just a simple kerneling bot, not a Kaggle Competitions Grandmaster!)","46fc7f31":"### Creating RMSLE function ","df74659c":"No improvement ... ","ad3a0ed4":"- Max number of product are sold by ```8023``` store id.","7c65abfa":"There are 3 csv files in the current version of the dataset:\n","ee46ce08":"I am not using this ```RMSLE``` function in my model because of error. If anyone able to figure out let me know in the comment section. ","d98adac4":"### Creating Time Based Features, So further we can convert this time series problem in to a regression one.","47038407":"<center><img src='https:\/\/www.googleapis.com\/download\/storage\/v1\/b\/kaggle-user-content\/o\/inbox%2F1279142%2F4f09fa27a17b01fa40700e7b80d87add%2Fdataset_description.jpg?generation=1594430740572308&alt=media'\/><\/center>","e0593a30":"## About","523aa85e":"Demand Forecasting is the pivotal business process around which strategic and operational plans of a company are devised. Based on the Demand Forecast, strategic and long-range plans of a business like budgeting, financial planning, sales and marketing plans, capacity planning, risk assessment and mitigation plans are formulated.","2c340a57":"### Data Visualization","88723e6a":"## Note1:\n\nMy current ranking in the table is ```29```.\n- If any one want some hint let me know in the comment section. ","0b51b46c":"### Encode the categorical variable","b50da7d6":"## Note: \n- With this kernel I have no intention to ruin the competition spirit, it is created to just help you to get started.\n\nIf you like my work then do \n\n    - Do follow\n    - Do upvote\n    - Have doubts regarding this kernel use comment section.\n    \n\nAs this competition is still going on .So I will upload my final kernel once competition got over.\n\n- Stay tuned for new and improved version.","384f1641":"## Evaluation Metric\n- The evaluation metric for this competition is 100*RMSLE (Root Mean Squared Log Error).","af094b71":"Treat the -ve predicted values.","3f65ca07":"# Let's Begin\n\n> In this notebook i provide you some hints if you implement them in your notebook,Surely gonna get better results.(because i already implemented and got much better results).\n\n> - So keep your eye on given hints.","628c40c6":"## Trying some thing new\n\nAs this competition is over but resently i amazed with new ensemble technique ```MinMaxBestBaseStacking```. So to just check is it really effective or not. \n\n[Original Source Repo](https:\/\/github.com\/QuantScientist\/Deep-Learning-Boot-Camp\/blob\/master\/Kaggle-PyTorch\/PyTorch-Ensembler\/utils.py)\n\n- For the sake of learning.","7c241879":"## Hint2:\n\nIn this notebook i use ```XGboost( a gradient boosting algorithm )```. Use other available gradient boosting algorithms,gonna get better results. \n\n- I am also using other one.\n- Don't waste you time with ```xgboost``` as this algo is not generalzing well for this data(based on my experience).\n- I though that much hint is very much sufficient and probably you got me what i am trying to say.\n\n- See you on Leaderboard.","5b7549f0":"# <center> Demand Forecasting<\/center>\n________","96ff63da":"## Training the model with xgboost","d1c5a111":"### Check the prediction with the Blend of mean and median ","f1550d4d":"<center><img src='https:\/\/datahack-prod.s3.ap-south-1.amazonaws.com\/__sized__\/contest_cover\/cover_1_3vEBqwk-thumbnail-1200x1200.png'\/><\/center>","e1c2954b":"## Problem Statement\n\nOne of the largest retail chains in the world wants to use their vast data source to __build an efficient forecasting model__ to predict the sales for each SKU in its portfolio at its __76 different stores__ using historical sales data for the __past 3 years__ on a week-on-week basis. Sales and promotional information is also available for each week - product and store wise. \n\nHowever, no other information regarding stores and products are available. Can you still forecast accurately the sales values for every such product\/SKU-store combination for the __next 12 weeks accurately__? \n\n- If yes, then dive right in! Let's Play","e2344d33":"### Got this \n\n> Your private score for this submission is : 449.7631659776047, \n\n> Had it been a live contest, your rank would be : 24\n\n\nMy actual ranking in this competion is 44.. Got a improvemnt of 20. Really impressed with the results."}}