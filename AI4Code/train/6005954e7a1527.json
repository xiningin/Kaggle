{"cell_type":{"ccb5ee5e":"code","893d78a7":"code","262c86b8":"code","effd284a":"code","7d4e609d":"code","c754bba4":"code","95f53eeb":"code","bd9cd91a":"code","6385ba7c":"code","1230d29b":"code","4ea1ea5b":"code","c2851658":"code","1b990c04":"code","d6484b47":"code","7df04366":"code","c48a2c10":"code","6947b161":"code","d321dc89":"code","f6d34f18":"code","c746f28f":"code","c0abbcc3":"code","b17f585b":"code","29a2dc95":"code","c3882272":"code","ce1b8ac3":"code","ef4ab221":"code","d89f8dd1":"code","f4db9e6e":"code","19c5fe04":"code","5e448402":"code","ce617aff":"code","1151a357":"code","3fbe61e7":"code","a32db0a0":"code","1f9b390f":"code","c612672d":"code","b9c25f25":"code","e6fb1752":"code","520999ba":"code","170fd7aa":"code","e08c4ee5":"code","67bcbe92":"code","cd6b526d":"code","e258cb70":"code","9c4df359":"code","ac802891":"code","251af5be":"markdown","beb8c0a7":"markdown","96382d82":"markdown","5e97f9b2":"markdown","88c08ce7":"markdown","2b8527a4":"markdown","989af3c1":"markdown","b160d2ae":"markdown","dd5e62ce":"markdown"},"source":{"ccb5ee5e":"from collections import defaultdict\nfrom operator import itemgetter\nimport string\nimport random\nfrom typing import Tuple, List","893d78a7":"import matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.model_selection import StratifiedShuffleSplit\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.preprocessing import OrdinalEncoder","262c86b8":"import catboost\nfrom catboost import CatBoostClassifier, Pool\nimport h2o\nfrom h2o.estimators.random_forest import H2ORandomForestEstimator\nimport lightgbm as lgb\nimport sklearn\nfrom sklearn.tree import DecisionTreeClassifier\nimport xgboost as xgb","effd284a":"print('Versions:')\nprint('CatBoost: {}'.format(catboost.__version__))\nprint('H2O: {}'.format(h2o.__version__))\nprint('LightGBM: {}'.format(lgb.__version__))\nprint('scikit-learn: {}'.format(sklearn.__version__))\nprint('XGBoost: {}'.format(xgb.__version__))","7d4e609d":"sns.set()\nsns.set_context('talk')","c754bba4":"h2o.init()\nh2o.no_progress()","95f53eeb":"def return_positive_semi_definite_matrix(n_dim: int) -> np.ndarray:\n    \"\"\"Return positive semi-definite matrix.\n    \n    Args:\n        n_dim (int): size of square matrix to return\n    Returns:\n        p (np.array): positive semi-definite array of shape (n_dim, n_dim)\n    \"\"\"\n    m = np.random.randn(n_dim, n_dim)\n    p = np.dot(m, m.T)\n    return p","bd9cd91a":"def sigmoid(x: np.array) -> np.array:                                                                  \n    \"\"\"Return sigmoid(x) for some activations x.\n    \n    Args:\n        x (np.array): input activations\n    Returns:\n        s (np.array): sigmoid(x)\n    \"\"\"\n    s = 1 \/ (1 + np.exp(-x))\n    return s","6385ba7c":"def return_weak_features_and_targets(\n    num_features: int,\n    num_samples: int,\n    mixing_factor: float,\n) -> Tuple[np.array, np.array]:\n    \"\"\"Return weakly predictive features and a target variable.\n    \n    Create a multivariate Gaussian-distributed set of features and a \n    response variable that is conditioned on a weighted sum of the features.\n    \n    Args:\n        num_features (int): number of variables in Gaussian distribution\n        num_samples (int): number of samples to take\n        mixing_factor (float): squashes the weighted sum into the linear \n            regime of a sigmoid.  Smaller numbers squash closer to 0.5.\n    Returns:\n        X (np.array): weakly predictive continuous features \n            (num_samples, num_features)\n        Y (np.array): targets (num_samples,)\n    \"\"\"\n    cov = return_positive_semi_definite_matrix(num_features)\n    X = np.random.multivariate_normal(\n        mean=np.zeros(num_features), cov=cov, size=num_samples)\n    weights = np.random.randn(num_features)\n    y_probs = sigmoid(mixing_factor * np.dot(X, weights))\n    y = np.random.binomial(1, p=y_probs)\n    return X, y","1230d29b":"def return_c_values(cardinality: int) -> Tuple[list, list]:\n    \"\"\"Return categorical values for C+ and C-.\n    \n    Create string values to be used for the categorical variable c.\n    We build two sets of values C+ and C-.  All values from C+ end with\n    \"A\" and all values from C- end with \"B\". The cardinality input \n    determines len(c_pos) + len(c_neg). \n\n    Args:\n        cardinality (int): cardinality of c\n    Returns:\n        c_pos (list): categorical values from C+ sample\n        c_neg (list): categorical values from C- sample\n    \"\"\"\n    suffixes = [\n        \"{}{}\".format(i, j) \n        for i in string.ascii_lowercase \n        for j in string.ascii_lowercase]\n    c_pos = [\"{}A\".format(s) for s in suffixes][:int(cardinality\/2)]\n    c_neg = [\"{}B\".format(s) for s in suffixes][:int(cardinality\/2)] \n    return c_pos, c_neg    ","4ea1ea5b":"def return_strong_features(\n    y_vals: np.array, \n    cardinality: int, \n    z_pivot: int=10\n) -> Tuple[np.array, np.array]:                       \n    \"\"\"Return strongly predictive features.\n    \n    Given a target variable values `y_vals`, create a categorical variable \n    c and continuous variable z such that y is perfectly predictable from \n    c and z, with y = 1 iff c takes a value from C+ OR z > z_pivot.\n\n    Args:\n        y_vals (np.array): targets\n        cardinality (int): cardinality of the categorical variable, c\n        z_pivot (float): mean of z\n    Returns:\n        c (np.array): strongly predictive categorical variable \n        z (np.array): strongly predictive continuous variable\n    \"\"\"\n    z = np.random.normal(loc=z_pivot, scale=5, size=2 * len(y_vals))\n    z_pos, z_neg = z[z > z_pivot], z[z <= z_pivot]\n    c_pos, c_neg = return_c_values(cardinality)\n    c, z = list(), list()\n    for y in y_vals:\n        coin = np.random.binomial(1, 0.5)\n        if y and coin:\n            c.append(random.choice(c_pos + c_neg))\n            z.append(random.choice(z_pos))\n        elif y and not coin:\n            c.append(random.choice(c_pos))\n            z.append(random.choice(z_neg))\n        else:\n            c.append(random.choice(c_neg))\n            z.append(random.choice(z_neg))\n    return np.array(c), np.array(z)","c2851658":"def return_main_dataset(\n    num_weak: int,\n    num_samp: int,\n    cardinality: int=100,\n    mixing_factor: float=0.025,\n) -> pd.DataFrame:\n    \"\"\"Generate training samples.\n    \n    Generate a dataset with features c and z that are perfectly predictive \n    of y and additional features x_i that are weakly predictive of y and\n    correlated with eachother. \n    \n    Args:\n        num_weak (int): number of weakly predictive features x_i to create\n        num_samp (int): number of sample to create\n        cardinality (int): cardinality of the predictive categorical variable.\n          half of these values will be correlated with y=1 and the other \n          with y=0.\n        mixing_factor (float): see `return_weak_features_and_targets`\n    Returns:\n        df (pd.DataFrame): dataframe with y, z, c, and x_i columns\n    \"\"\"\n    X, y = return_weak_features_and_targets(num_weak, num_samp, mixing_factor)\n    c, z = return_strong_features(y, cardinality)\n    xcol_names = ['x{}'.format(i) for i in range(num_weak)]\n    df = pd.DataFrame(X, columns=xcol_names)\n    df['y'] = y\n    df['z'] = z\n    df['c'] = c\n    df['c'] = df['c'].astype('category')\n    df = df[['y', 'c', 'z'] + xcol_names]\n    return df","1b990c04":"def encode_as_onehot(df_main: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Replace string values for c with one-hot encoding.\"\"\"\n    df_onehot = pd.get_dummies(df_main, 'c')\n    df_onehot['y'] = df_main['y'].copy()\n    return df_onehot\n\ndef encode_as_int(df_main: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Replace string values for c with integer encoding.\"\"\"\n    ord_enc = OrdinalEncoder(dtype=np.int)\n    c_encoded = ord_enc.fit_transform(df_main[['c']])\n    df_catnum = df_main.copy()\n    df_catnum['c'] = c_encoded\n    df_catnum['c'] = df_catnum['c'].astype('category')\n    return df_catnum, ord_enc\n    \ndef encode_as_magic_int(df_main: pd.DataFrame) -> pd.DataFrame:\n    \"\"\"Replace string values for c with \"magic\" integer encoding.\n    \n    A magic encoding is one in which the sorted integer values keep all \n    C+ values (values of c that end with \"A\") next to each other and all \n    C- values (values of c that end with \"B\") next to eachother.   \n    \"\"\"\n    values = sorted(df_main['c'].unique(), key=lambda x: x[-1])\n    ord_enc = OrdinalEncoder(categories=[values], dtype=np.int)\n    c_encoded = ord_enc.fit_transform(df_main[['c']])\n    df_catnum = df_main.copy()\n    df_catnum['c'] = c_encoded\n    df_catnum['c'] = df_catnum['c'].astype('category')\n    return df_catnum, ord_enc","d6484b47":"datasets_sml = {}\nencoders_sml = {}\ndatasets_sml['main'] = return_main_dataset(\n    num_weak=2, num_samp=20, cardinality=4, mixing_factor=0.025)\ndatasets_sml['int'], encoders_sml['int'] = encode_as_int(datasets_sml['main'])\ndatasets_sml['mint'], encoders_sml['mint'] = encode_as_magic_int(datasets_sml['main'])\ndatasets_sml['ohe'] = encode_as_onehot(datasets_sml['main'])","7df04366":"datasets_sml['main'].head()","c48a2c10":"datasets_sml['main'].dtypes","6947b161":"datasets_sml['int'].head()","d321dc89":"datasets_sml['int'].dtypes","f6d34f18":"datasets_sml['mint'].head()","c746f28f":"datasets_sml['mint'].dtypes","c0abbcc3":"datasets_sml['ohe'].head()","b17f585b":"datasets_sml['ohe'].dtypes","29a2dc95":"datasets = {}\nencoders = {}\ndatasets['main'] = return_main_dataset(\nnum_weak=100, num_samp=10_000, cardinality=200, mixing_factor=0.025)\ndatasets['int'], encoders['int'] = encode_as_int(datasets['main'])\ndatasets['mint'], encoders['mint'] = encode_as_magic_int(datasets['main'])\ndatasets['ohe'] = encode_as_onehot(datasets['main'])","c3882272":"fig, axes = plt.subplots(1, 2, figsize=(15,9))\nbool_mask = datasets['main']['y']==1\nyticks = np.array([10, 30, 50, 70, 90, 110, 130, 150, 170, 190])\n\ntitles = ['normal integer encoding', 'magic integer encoding']\nds_names = ['int', 'mint']\nfor iax, (title, ds_name) in enumerate(zip(titles, ds_names)):\n\n    ax = axes[iax]\n    df = datasets[ds_name]\n    \n    xpts = df.loc[~bool_mask, 'z']\n    ypts = df.loc[~bool_mask, 'c']\n    ax.scatter(xpts, ypts, s=6, label='y=0')\n\n    xpts = df.loc[bool_mask, 'z']\n    ypts = df.loc[bool_mask, 'c']\n    ax.scatter(xpts, ypts, s=6, label='y=1')\n\n    ax.set_xlabel('z')\n    ax.set_ylabel('c (string, integer encoding)')\n    ax.set_ylim(-20, 220)\n    ax.legend()\n    ax.set_title(title)\n\n    ax.set_yticks(yticks)\n    str_ytls = encoders[ds_name].inverse_transform(yticks.reshape(-1,1)).flatten()\n    str_ytls = ['({},{})'.format(s,i) for i,s in zip(yticks, str_ytls)]\n    ax.set_yticklabels(str_ytls);\n\nplt.suptitle('Strong Features')\nplt.tight_layout()\nplt.subplots_adjust(top=0.85)","ce1b8ac3":"fig, axes = plt.subplots(\n    2, 3, sharex=True, sharey=True, figsize=(15,12))\ndataset = datasets['main']\nbool_mask = dataset['y']==1\n\naxes = axes.flatten()\nfor iax, (xa, xb) in enumerate([\n    ('x0', 'x1'), ('x10', 'x11'), ('x20', 'x21'), \n    ('x30', 'x31'), ('x40', 'x41'), ('x50', 'x51'),\n]):\n    ax = axes[iax]\n    \n    sns.kdeplot(\n        dataset.loc[bool_mask, xa], \n        dataset.loc[bool_mask, xb],\n        cmap='Reds', \n        shade=False, \n        shade_lowest=False, \n        ax=ax, \n        label='y=1',\n        levels=5)\n\n    sns.kdeplot(\n        dataset.loc[~bool_mask, xa], \n        dataset.loc[~bool_mask, xb],\n        cmap='Blues',\n        shade=False,\n        shade_lowest=False, \n        ax=ax, \n        label='y=0',\n        levels=5,\n        linestyles='--'\n    )\n\n    ax.set_xlabel(xa)\n    ax.set_ylabel(xb)\n    ax.set_xlim(-20, 20)\n    ax.set_ylim(-20, 20)\n    ax.legend()\n    \nplt.suptitle('Weak Features')\nplt.tight_layout()\nplt.subplots_adjust(top=0.9)","ef4ab221":"metric = roc_auc_score\ntarget_col = 'y'\nn_splits = 10\ntest_size = 0.3","d89f8dd1":"def get_feature_names(df, include_c):\n    names = [f for f in df.columns if not f.startswith('y')]\n    if not include_c:\n        names = [f for f in names if not f.startswith('c')]\n    return names\n\ndef print_auc_mean_std(results):\n    print(\"    AUC: mean={:4.4f}, sd={:4.4f}\".format(\n        np.mean(results['metric']), np.std(results['metric'])))\n    \ndef print_sorted_mean_importances(results, n=5):\n    data = defaultdict(list)\n    imps = results['importances']\n    for d in imps:\n        for fname, imp in d.items():\n            data[fname].append(imp)\n    mu = {fname: np.mean(vals) for fname, vals in data.items()}\n    mu = sorted(mu.items(), key=itemgetter(1), reverse=True)[:n]\n    print(\"    Importances:\")\n    for fname, val in mu:\n        print(\"{:>20}: {:0.03f}\".format(fname, val))","f4db9e6e":"def evaluate_sklearn_model(df_data, feature_names, model):\n    metrics, feature_importances = list(), list()\n    X = df_data[feature_names]\n    y_true = df_data[target_col]\n    folds = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size)\n    for train_idx, test_idx in folds.split(X, y_true):\n        model.fit(X.loc[train_idx], y_true.loc[train_idx])\n        y_pred = model.predict_proba(X.loc[test_idx])\n        metrics.append(metric(y_true[test_idx], y_pred[:, 1]))\n        try:\n            feature_importances.append(\n                dict(zip(feature_names, model.feature_importances_)))\n        except AttributeError:  # not a random forest\n            feature_importances.append(\n                dict(zip(feature_names, model.coef_.ravel()))\n            )\n    return {'metric': metrics, 'importances': feature_importances}","19c5fe04":"skl_trials = [\n    {\n        'name': 'no c',\n        'params': {\n            'df_data': datasets['ohe'],\n            'feature_names': get_feature_names(datasets['ohe'], include_c=False),\n            'model': DecisionTreeClassifier(),\n        }\n    },\n    {\n        'name': 'onehot encoding',\n        'params': {\n            'df_data': datasets['ohe'],\n            'feature_names': get_feature_names(datasets['ohe'], include_c=True),\n            'model': DecisionTreeClassifier(),\n        }\n    },\n    {\n        'name': 'int encoding (normal)',\n        'params': {\n            'df_data': datasets['int'],\n            'feature_names': get_feature_names(datasets['int'], include_c=True),\n            'model': DecisionTreeClassifier(),\n        }\n    },\n    {\n        'name': 'int encoding (magic)',\n        'params': {\n            'df_data': datasets['mint'],\n            'feature_names': get_feature_names(datasets['mint'], include_c=True),\n            'model': DecisionTreeClassifier(),\n        }\n    }\n]","5e448402":"for trial in skl_trials:\n    results = evaluate_sklearn_model(**trial['params'])\n    print('trial: {}'.format(trial['name']))\n    print_auc_mean_std(results)\n    print_sorted_mean_importances(results)\n    print()","ce617aff":"class H2ODecisionTree:                                                              \n    \"\"\"                                                                             \n    Simple class that overloads an H2ORandomForestEstimator to mimic a              \n    decision tree classifier. Only train, predict and varimp are implemented.       \n    \"\"\" \n    def __init__(self):\n        self.model = None\n\n    def train(self, x, y, training_frame):\n        self.model = H2ORandomForestEstimator(ntrees=1, mtries=len(x))\n        self.model.train(x=x, y=y, training_frame=training_frame)\n\n    def predict(self, frame):\n        return self.model.predict(frame)\n    \n    def varimp(self):\n        return self.model.varimp()","1151a357":"def evaluate_h2o_model(df_data, feature_names, model):\n    h2ofr = h2o.H2OFrame(df_data)\n    h2ofr.col_names = list(df_data.columns)\n    if 'c' in df_data:\n        h2ofr['c'] = h2ofr['c'].asfactor() # make categorical\n    metrics, feature_importances = list(), list()\n    \n    X = df_data[feature_names]\n    y = df_data[target_col]\n    folds = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size)\n    for train_idx, test_idx in folds.split(X, y):\n        train_idx, test_idx = sorted(train_idx), sorted(test_idx)\n        model.train(x=feature_names, y=target_col, training_frame=h2ofr[train_idx, :])\n        predictions = model.predict(h2ofr[test_idx, feature_names]).as_data_frame()\n        try:\n            prediction_scores = predictions['True']\n        except KeyError:\n            prediction_scores = predictions['predict']\n        metrics.append(metric(df_data[target_col].values[test_idx], prediction_scores))\n        feature_importances.append(dict([(v[0], v[3]) for v in model.varimp()]))\n    return {'metric': metrics, 'importances': feature_importances}","3fbe61e7":"h2o_trials = [\n    {\n        'name': 'no c',\n        'params': {\n            'df_data': datasets['ohe'],\n            'feature_names': get_feature_names(datasets['ohe'], include_c=False),\n            'model': H2ODecisionTree(),\n        }\n    },\n    {\n        'name': 'onehot encoding',\n        'params': {\n            'df_data': datasets['ohe'],\n            'feature_names': get_feature_names(datasets['ohe'], include_c=True),\n            'model': H2ODecisionTree(),\n        }\n    },\n    {\n        'name': 'int encoding (normal)',\n        'params': {\n            'df_data': datasets['int'],\n            'feature_names': get_feature_names(datasets['int'], include_c=True),\n            'model': H2ODecisionTree(),\n        }\n    },\n    {\n        'name': 'int encoding (magic)',\n        'params': {\n            'df_data': datasets['mint'],\n            'feature_names': get_feature_names(datasets['mint'], include_c=True),\n            'model': H2ODecisionTree(),\n        }\n    },\n    {\n        'name': 'main',\n        'params': {\n            'df_data': datasets['main'],\n            'feature_names': get_feature_names(datasets['main'], include_c=True),\n            'model': H2ODecisionTree(),\n        }\n    },\n]","a32db0a0":"for trial in h2o_trials:\n    results = evaluate_h2o_model(**trial['params'])\n    print('trial: {}'.format(trial['name']))\n    print_auc_mean_std(results)\n    print_sorted_mean_importances(results)\n    print()","1f9b390f":"class LightGBMDecisionTree:\n    \n    def __init__(self):\n        self.params = {\n            'objective': 'binary',\n            'bagging_freq': 0,\n        }\n        \n    def train(self, lgb_train):\n        self.gbm = lgb.train(\n            self.params, \n            lgb_train, \n            num_boost_round=1)\n    \n    def predict(self, X):\n        return self.gbm.predict(X)","c612672d":"def evaluate_lgb_model(df_data, feature_names, model):        \n    metrics, feature_importances = list(), list()\n    XX = df_data[feature_names]\n    yy = df_data[target_col]\n    folds = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size) \n    \n    for train_idx, test_idx in folds.split(XX, yy):\n        \n        lgb_train = lgb.Dataset(\n            XX.loc[train_idx], \n            label=yy.loc[train_idx],\n#            categorical_feature=['c'],  # we use categorical pandas column\n        )\n        lgb_test = lgb.Dataset(\n            XX.loc[test_idx], \n            label=yy.loc[test_idx],\n#            categorical_feature=['c'],   # we use categorical pandas column\n        )\n        \n        model.train(lgb_train)\n        yy_pred = model.predict(XX.loc[test_idx])\n        \n        metrics.append(roc_auc_score(yy.loc[test_idx], yy_pred))\n        fi = model.gbm.feature_importance(importance_type='gain')\n        fi = fi \/ fi.sum()\n        feature_importances.append(\n            {name: val for name, val in zip(model.gbm.feature_name(), fi)}\n        )\n    return {'metric': metrics, 'importances': feature_importances}","b9c25f25":"lgb_trials = [\n    {\n        'name': 'no c',\n        'params': {\n            'df_data': datasets['ohe'],\n            'feature_names': get_feature_names(datasets['ohe'], include_c=False),\n            'model': LightGBMDecisionTree(),\n        }\n    },\n    {\n        'name': 'onehot encoding',\n        'params': {\n            'df_data': datasets['ohe'],\n            'feature_names': get_feature_names(datasets['ohe'], include_c=True),\n            'model': LightGBMDecisionTree(),\n        }\n    },\n    {\n        'name': 'int encoding (normal)',\n        'params': {\n            'df_data': datasets['int'],\n            'feature_names': get_feature_names(datasets['int'], include_c=True),\n            'model': LightGBMDecisionTree(),\n        }\n    },\n    {\n        'name': 'int encoding (magic)',\n        'params': {\n            'df_data': datasets['mint'],\n            'feature_names': get_feature_names(datasets['mint'], include_c=True),\n            'model': LightGBMDecisionTree(),\n        }\n    },\n]","e6fb1752":"for trial in lgb_trials:\n    results = evaluate_lgb_model(**trial['params'])\n    print('trial: {}'.format(trial['name']))\n    print_auc_mean_std(results)\n    print_sorted_mean_importances(results)\n    print()","520999ba":"class XGBoostDecisionTree:\n    \n    def __init__(self):\n        self.params = {\n            'booster': 'gbtree',\n            'subsample': 1.0,\n            'num_parallel_tree': 1,\n            'objective': 'binary:logistic',\n        }\n        \n    def train(self, xgb_train):\n        self.bst = xgb.train(\n            self.params, \n            xgb_train, \n            num_boost_round=1)\n    \n    def predict(self, X):\n        return self.bst.predict(X)","170fd7aa":"def evaluate_xgb_model(df_data, feature_names, model):        \n    metrics, feature_importances = list(), list()\n    \n    # XGBoost does not play well with dtype='category'\n    if 'c' in df_data.columns:\n        df_data['c'] = df_data['c'].astype('int')\n        \n    XX = df_data[feature_names]\n    yy = df_data[target_col]\n    folds = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size) \n    \n    for train_idx, test_idx in folds.split(XX, yy):\n        \n        xgb_train = xgb.DMatrix(\n            XX.loc[train_idx], \n            label=yy.loc[train_idx],\n        )\n        xgb_test = xgb.DMatrix(\n            XX.loc[test_idx], \n            label=yy.loc[test_idx],\n        )\n        \n        model.train(xgb_train)\n        yy_pred = model.predict(xgb_test)\n        \n        metrics.append(roc_auc_score(yy.loc[test_idx], yy_pred))\n        \n        fi = model.bst.get_score(importance_type='gain')\n        total = sum(fi.values())\n        fi = {k: v\/total for k,v in fi.items()}\n        feature_importances.append(fi)\n\n    # undo any dtype changes\n    if 'c' in df_data.columns:\n        df_data['c'] = df_data['c'].astype('category')\n        \n    return {'metric': metrics, 'importances': feature_importances}","e08c4ee5":"xgb_trials = [\n    {\n        'name': 'no c',\n        'params': {\n            'df_data': datasets['ohe'],\n            'feature_names': get_feature_names(datasets['ohe'], include_c=False),\n            'model': XGBoostDecisionTree(),\n        }\n    },\n    {\n        'name': 'onehot encoding',\n        'params': {\n            'df_data': datasets['ohe'],\n            'feature_names': get_feature_names(datasets['ohe'], include_c=True),\n            'model': XGBoostDecisionTree(),\n        }\n    },\n    {\n        'name': 'int encoding (normal)',\n        'params': {\n            'df_data': datasets['int'],\n            'feature_names': get_feature_names(datasets['int'], include_c=True),\n            'model': XGBoostDecisionTree(),\n        }\n    },\n    {\n        'name': 'int encoding (magic)',\n        'params': {\n            'df_data': datasets['mint'],\n            'feature_names': get_feature_names(datasets['mint'], include_c=True),\n            'model': XGBoostDecisionTree(),\n        }\n    },\n]","67bcbe92":"for trial in xgb_trials:\n    results = evaluate_xgb_model(**trial['params'])\n    print('trial: {}'.format(trial['name']))\n    print_auc_mean_std(results)\n    print_sorted_mean_importances(results)\n    print()","cd6b526d":"class CatBoostDecisionTree:\n    \n    def __init__(self):\n        self.cbc = CatBoostClassifier(\n            n_estimators=1,\n            subsample=1.0,\n            verbose=0)\n        \n    def train(self, train_pool):\n        self.cbc.fit(train_pool)\n    \n    def predict(self, X):\n        return self.cbc.predict_proba(X)","e258cb70":"def evaluate_cb_model(df_data, feature_names, model):        \n    metrics, feature_importances = list(), list()\n            \n    XX = df_data[feature_names]\n    yy = df_data[target_col]\n    folds = StratifiedShuffleSplit(n_splits=n_splits, test_size=test_size) \n    \n    if 'c' in df_data.columns:\n        cat_features = ['c']\n    else:\n        cat_features = []\n    \n    for train_idx, test_idx in folds.split(XX, yy):\n        \n        train_pool = Pool(\n            XX.loc[train_idx], \n            label=yy.loc[train_idx],\n            cat_features=cat_features,\n        )\n        test_pool = Pool(\n            XX.loc[test_idx], \n            label=yy.loc[test_idx],\n            cat_features=cat_features\n        )\n                \n        model.train(train_pool)\n        yy_pred = model.predict(test_pool)\n        \n        metrics.append(roc_auc_score(yy.loc[test_idx], yy_pred[:,1]))\n        \n        fi = dict(zip(model.cbc.feature_names_, model.cbc.feature_importances_))\n        total = sum(fi.values())\n        fi = {k: v\/total for k,v in fi.items()}\n        feature_importances.append(fi)\n\n        \n    return {'metric': metrics, 'importances': feature_importances}","9c4df359":"cb_trials = [\n    {\n        'name': 'no c',\n        'params': {\n            'df_data': datasets['ohe'],\n            'feature_names': get_feature_names(datasets['ohe'], include_c=False),\n            'model': CatBoostDecisionTree(),\n        }\n    },\n    {\n        'name': 'onehot encoding',\n        'params': {\n            'df_data': datasets['ohe'],\n            'feature_names': get_feature_names(datasets['ohe'], include_c=True),\n            'model': CatBoostDecisionTree(),\n        }\n    },\n    {\n        'name': 'int encoding (normal)',\n        'params': {\n            'df_data': datasets['int'],\n            'feature_names': get_feature_names(datasets['int'], include_c=True),\n            'model': CatBoostDecisionTree(),\n        }\n    },\n    {\n        'name': 'int encoding (magic)',\n        'params': {\n            'df_data': datasets['mint'],\n            'feature_names': get_feature_names(datasets['mint'], include_c=True),\n            'model': CatBoostDecisionTree(),\n        }\n    },\n    {\n        'name': 'main',\n        'params': {\n            'df_data': datasets['main'],\n            'feature_names': get_feature_names(datasets['main'], include_c=True),\n            'model': CatBoostDecisionTree(),\n        }\n    }\n]","ac802891":"for trial in cb_trials:\n    results = evaluate_cb_model(**trial['params'])\n    print('trial: {}'.format(trial['name']))\n    print_auc_mean_std(results)\n    print_sorted_mean_importances(results)\n    print()","251af5be":"# H2O","beb8c0a7":"# scikit-learn","96382d82":"# Dataset Creation - Real Example","5e97f9b2":"# CatBoost","88c08ce7":"# Categorical Variables in Decision Trees\n\nIn 2016, [Roam Analytics](https:\/\/roamanalytics.com\/) wrote a nice blog post titled [Are categorical variables getting lost in your random forests?](https:\/\/roamanalytics.com\/2016\/10\/28\/are-categorical-variables-getting-lost-in-your-random-forests\/).  The TL:DR from their post is: \n\n> Decision tree models can handle categorical variables without one-hot encoding them. However, popular implementations of decision trees (and random forests) differ as to whether they honor this fact. We show that one-hot encoding can seriously degrade tree-model performance. Our primary comparison is between H2O (which honors categorical variables) and scikit-learn (which requires them to be one-hot encoded).\n\nIn this kernel, we see how their approach holds up in 2020. We make a minor adjustment to their dataset to avoid \"lucky integer encoding\" and we add CatBoost, LightGBM, and XGBoost to the analysis.  We find that single decision trees in CatBoost, H2O, and LightGBM can handle categorical variables without one-hot encoding but scikit-learn and XGBoost cant.  ","2b8527a4":"# XGBoost","989af3c1":"# Examine Decision Tree Implementations\n\n * sklearn\n * CatBoost\n * H2O\n * LightGBM\n * XGBoost","b160d2ae":"# Dataset Creation - Small Example","dd5e62ce":"# LightGBM"}}