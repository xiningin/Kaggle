{"cell_type":{"43b304de":"code","d5ca20a7":"code","139f9785":"code","5d77570e":"code","91a8f969":"code","f68b948e":"code","a68c2b18":"code","b356eabc":"code","daba8b09":"code","302b8d45":"code","9cf604c7":"code","9c9dee11":"code","cb775f67":"code","ca120ced":"code","9288c8a8":"code","6fa8d816":"code","54d87932":"code","8d2d9460":"code","eb568c5f":"code","efefa1de":"code","4dd81118":"code","fc7afefc":"code","25d3ae4b":"code","a1504b11":"code","8ca3e43c":"code","ce2bd312":"code","f2beeeaa":"code","c3bb7a69":"code","3c85cc3f":"code","a8d87ce5":"code","05a08037":"code","cf3ed84e":"code","c7ab002a":"code","ef02ca71":"code","3a9a9d49":"code","bf8515fb":"code","bb1dc8a1":"code","e8e9352a":"code","ba8b5fc4":"code","776da682":"code","97fa75cd":"code","de785b0c":"code","d5698f21":"code","8db8284b":"code","7b0f80ad":"code","d305a498":"code","d5d0fdf6":"code","dd255ffd":"code","04d11cc9":"code","85ff388d":"code","be69513f":"code","7d6e1314":"code","b40dbc31":"code","1f74251b":"code","fe69e005":"code","c67da183":"code","cb508954":"code","c316cc8b":"code","b4dac7ec":"code","af4b5856":"markdown","1182a17a":"markdown","2235232e":"markdown","53051993":"markdown","81d0c026":"markdown","4b51c39d":"markdown","b3a11e66":"markdown","136e9efd":"markdown","c1e08656":"markdown","6939ee2f":"markdown","8d1ba33a":"markdown","a3cd9897":"markdown","75a01e69":"markdown","6f871ec2":"markdown","c48e2b71":"markdown","e353ba13":"markdown","91016d28":"markdown","ec5185e6":"markdown","69888f81":"markdown","c132ee5c":"markdown","024a98c4":"markdown","269b6be7":"markdown","837560cb":"markdown","18212de9":"markdown"},"source":{"43b304de":"import os\nimport gc\nimport numpy as np\nimport pandas as pd\nimport time\nimport math\nfrom numba import jit\nfrom math import log, floor\nimport scipy\nfrom scipy import signal\nfrom scipy.signal import butter, deconvolve\nfrom sklearn.neighbors import KDTree\nfrom pathlib import Path\nfrom sklearn.utils import shuffle\nimport seaborn as sns\nfrom matplotlib import colors\nimport matplotlib.pyplot as plt\nfrom matplotlib.colors import Normalize\n\nimport lightgbm as lgb\nfrom tqdm import tqdm\nfrom sklearn.model_selection import GroupKFold, KFold\nfrom sklearn.metrics import cohen_kappa_score, accuracy_score, mean_squared_error, f1_score","d5ca20a7":"DATA_PATH = \"..\/input\/liverpool-ion-switching\"\n\ntrain_df = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'))\ntest_df = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'))\nsubmission_df = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'))","139f9785":"print(f\"Train data: {train_df.shape}\")\nprint(f\"Test data: {test_df.shape}\")","5d77570e":"train_df.head()","91a8f969":"test_df.head()","f68b948e":"train_df.describe()","a68c2b18":"test_df.describe()","b356eabc":"def reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage(deep=True).sum() \/ 1024 ** 2 # just added \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage(deep=True).sum() \/ 1024 ** 2\n    percent = 100 * (start_mem - end_mem) \/ start_mem\n    print('Mem. usage decreased from {:5.2f} Mb to {:5.2f} Mb ({:.1f}% reduction)'.format(start_mem, end_mem, percent))\n    return df","daba8b09":"def get_stats(df):\n    stats = pd.DataFrame(index=df.columns, columns=['na_count', 'n_unique', 'type', 'memory_usage'])\n    for col in df.columns:\n        stats.loc[col] = [df[col].isna().sum(), df[col].nunique(dropna=False), df[col].dtypes, df[col].memory_usage(deep=True, index=False) \/ 1024**2]\n    stats.loc['Overall'] = [stats['na_count'].sum(), stats['n_unique'].sum(), None, df.memory_usage(deep=True).sum() \/ 1024**2]\n    return stats\n\ndef print_header():\n    print('col         conversion        dtype    na    uniq  size')\n    print()\n    \ndef print_values(name, conversion, col):\n    template = '{:10}  {:16}  {:>7}  {:2}  {:6}  {:1.2f}MB'\n    print(template.format(name, conversion, str(col.dtypes), col.isna().sum(), col.nunique(dropna=False), col.memory_usage(deep=True, index=False) \/ 1024 ** 2))","302b8d45":"get_stats(train_df)","9cf604c7":"get_stats(test_df)","9c9dee11":"get_stats(submission_df)","cb775f67":"def plot_time_data(data_df, title=\"Time variation data\", color='b'):\n    plt.figure(figsize=(18,8))\n    plt.plot(data_df[\"time\"], data_df[\"signal\"], color=color)\n    plt.title(title, fontsize=24)\n    plt.xlabel(\"Time [sec]\", fontsize=20)\n    plt.ylabel(\"Signal\", fontsize=20)\n    plt.show()","ca120ced":"plot_time_data(train_df,\"Train data\",'g')","9288c8a8":"plot_time_data(test_df,\"Test data\",'m')","6fa8d816":"plot_time_data(train_df[0:500],\"Train data\",'b')","54d87932":"plot_time_data(train_df[7000:7500],\"Train data\",'g')","8d2d9460":"plot_time_data(train_df[8000:8500],\"Train data\",'b')","eb568c5f":"plot_time_data(train_df[9000:9500],\"Train data\",'g')","efefa1de":"plot_time_data(train_df[12000:12500],\"Train data\",'b')","4dd81118":"plot_time_data(train_df[15000:15500],\"Train data\",'r')","fc7afefc":"plot_time_data(train_df[16000:16500],\"Train data\",'b')","25d3ae4b":"def plot_time_channel_data(data_df, title=\"Time variation data\"):\n    plt.figure(figsize=(18,8))\n    plt.plot(data_df[\"time\"], data_df[\"signal\"], color='b', label='Signal')\n    plt.plot(data_df[\"time\"], data_df[\"open_channels\"], color='r', label='Open channel')\n    plt.title(title, fontsize=24)\n    plt.xlabel(\"Time [sec]\", fontsize=20)\n    plt.ylabel(\"Signal & Open channel data\", fontsize=20)\n    plt.legend(loc='upper right')\n    plt.grid(True)\n    plt.show()","a1504b11":"plot_time_channel_data(train_df[0:500],'Train data: signal & open channel data')","8ca3e43c":"plot_time_channel_data(train_df[7000:7500],'Train data: signal & open channel data (0.7-0.75 sec.)')","ce2bd312":"plot_time_channel_data(train_df[8000:8500],'Train data: signal & open channel data (0.8-0.85 sec.)')","f2beeeaa":"plot_time_channel_data(train_df[9000:9500],'Train data: signal & open channel data (0.9-0.95 sec.)')","c3bb7a69":"plot_time_channel_data(train_df[15000:15500],'Train data: signal & open channel data (1.5-1.55 sec.)')","3c85cc3f":"plot_time_channel_data(train_df[16000:16500],'Train data: signal & open channel data (1.6-1.65 sec.)')","a8d87ce5":"plot_time_channel_data(train_df[1200000:1200500],'Train data: signal & open channel data (120-120.05 sec.)')","05a08037":"plot_time_channel_data(train_df[1300000:1300500],'Train data: signal & open channel data (130-130.05 sec.)')","cf3ed84e":"plot_time_channel_data(train_df[1400000:1400500],'Train data: signal & open channel data (140-140.05 sec.)')","c7ab002a":"plot_time_channel_data(train_df[1500000:1500500],'Train data: signal & open channel data (150-150.05 sec.)')","ef02ca71":"plot_time_channel_data(train_df[3500000:3500500],'Train data: signal & open channel data (350-350.05 sec.)')","3a9a9d49":"plot_time_channel_data(train_df[4100000:4100500],'Train data: signal & open channel data (410-410.05 sec.)')","bf8515fb":"plot_time_channel_data(train_df[4500000:4500500],'Train data: signal & open channel data (450-450.05 sec.)')","bb1dc8a1":"def plot_open_channel_count(data_df, title):\n    plt.figure(figsize=(8,6))\n    sns.countplot(data_df['open_channels'])\n    plt.title(title)\n    plt.show()","e8e9352a":"plot_open_channel_count(train_df,'Open channels distribution')","ba8b5fc4":"plot_open_channel_count(train_df[0:1000000],'Open channels distribution (0-100 sec.)')","776da682":"plot_open_channel_count(train_df[1000000:2000000],'Open channels distribution (100-200 sec.)')","97fa75cd":"plot_open_channel_count(train_df[2000000:3000000],'Open channels distribution (200-300 sec.)')","de785b0c":"plot_open_channel_count(train_df[3000000:4000000],'Open channels distribution (300-400 sec.)')","d5698f21":"plot_open_channel_count(train_df[4000000:5000000],'Open channels distribution (400-500 sec.)')","8db8284b":"def average_signal_smoothing(signal, kernel_size=3, stride=1):\n    sample = []\n    start = 0\n    end = kernel_size\n    while end <= len(signal):\n        start = start + stride\n        end = end + stride\n        sample.extend(np.ones(end - start)*np.mean(signal[start:end]))\n    return np.array(sample[::kernel_size])","7b0f80ad":"def plot_signal_signal_smoothed_open_channel(data_df, title):\n    sm_df = average_signal_smoothing(data_df[\"signal\"])\n    plt.figure(figsize=(18,8))\n    plt.plot(data_df[\"time\"], data_df[\"signal\"], color='b', label='Signal')\n    plt.plot(data_df[\"time\"][2:], sm_df, color='g', label='Smoothed signal')\n    plt.plot(data_df[\"time\"], data_df[\"open_channels\"], color='r', label='Open channel')\n    plt.title(title, fontsize=24)\n    plt.xlabel(\"Time [sec]\", fontsize=20)\n    plt.ylabel(\"Signal & Open channel data\", fontsize=20)\n    plt.legend(loc='upper right')\n    plt.grid(True)\n    plt.show()    ","d305a498":"plot_signal_signal_smoothed_open_channel(train_df[0:200], \"Train data: signal, smoothed signal & open channel data (0-0.02 sec.)\")","d5d0fdf6":"plot_signal_signal_smoothed_open_channel(train_df[0:200], \"Train data: signal, smoothed signal & open channel data (0-0.02 sec.)\")","dd255ffd":"plot_signal_signal_smoothed_open_channel(train_df[7200:7400], \"Train data: signal, smoothed signal & open channel data (0.72-0.74 sec.)\")","04d11cc9":"plot_signal_signal_smoothed_open_channel(train_df[15300:15500], \"Train data: signal, smoothed signal & open channel data (1.53-1.55 sec.)\")","85ff388d":"plot_signal_signal_smoothed_open_channel(train_df[3500000:3500200],'Train data: signal, smoothed signal & open channel data (350-350.02 sec.)')","be69513f":"train_df['train'] = True\ntest_df['train'] = False\nall_data = pd.concat([train_df, test_df], sort=False).reset_index(drop=True)\nall_data['train'] = all_data['train'].astype('bool')\n\nall_data = all_data.sort_values(by=['time']).reset_index(drop=True)\nall_data.index = ((all_data.time * 10_000) - 1).values\nall_data['batch'] = all_data.index \/\/ 50_000\nall_data['batch_index'] = all_data.index  - (all_data.batch * 50_000)\nall_data['batch_slices'] = all_data['batch_index']  \/\/ 5_000\nall_data['batch_slices2'] = all_data.apply(lambda r: '_'.join([str(r['batch']).zfill(3), str(r['batch_slices']).zfill(3)]), axis=1)\n\n# 50_000 Batch Features\nall_data['signal_batch_min'] = all_data.groupby('batch')['signal'].transform('min')\nall_data['signal_batch_max'] = all_data.groupby('batch')['signal'].transform('max')\nall_data['signal_batch_std'] = all_data.groupby('batch')['signal'].transform('std')\nall_data['signal_batch_mean'] = all_data.groupby('batch')['signal'].transform('mean')\nall_data['signal_batch_median'] = all_data.groupby('batch')['signal'].transform('median')\n#all_data['signal_batch_mad'] = all_data.groupby('batch')['signal'].transform('mad')\nall_data['signal_batch_skew'] = all_data.groupby('batch')['signal'].transform('skew')\nall_data['mean_abs_chg_batch'] = all_data.groupby(['batch'])['signal'].transform(lambda x: np.mean(np.abs(np.diff(x))))\nall_data['median_abs_chg_batch'] = all_data.groupby(['batch'])['signal'].transform(lambda x: np.median(np.abs(np.diff(x))))\nall_data['abs_max_batch'] = all_data.groupby(['batch'])['signal'].transform(lambda x: np.max(np.abs(x)))\nall_data['abs_min_batch'] = all_data.groupby(['batch'])['signal'].transform(lambda x: np.min(np.abs(x)))\nall_data['abs_mean_batch'] = all_data.groupby(['batch'])['signal'].transform(lambda x: np.mean(np.abs(x)))\nall_data['abs_median_batch'] = all_data.groupby(['batch'])['signal'].transform(lambda x: np.median(np.abs(x)))\nall_data['moving_average_batch_1000_mean'] = all_data.groupby(['batch'])['signal'].rolling(window=1000).mean().mean(skipna=True)\n\n\nall_data['range_batch'] = all_data['signal_batch_max'] - all_data['signal_batch_min']\nall_data['maxtomin_batch'] = all_data['signal_batch_max'] \/ all_data['signal_batch_min']\nall_data['abs_avg_batch'] = (all_data['abs_min_batch'] + all_data['abs_max_batch']) \/ 2\n\n# 5_000 Batch Features\nall_data['signal_batch_5k_min'] = all_data.groupby('batch_slices2')['signal'].transform('min')\nall_data['signal_batch_5k_max'] = all_data.groupby('batch_slices2')['signal'].transform('max')\nall_data['signal_batch_5k_std'] = all_data.groupby('batch_slices2')['signal'].transform('std')\nall_data['signal_batch_5k_mean'] = all_data.groupby('batch_slices2')['signal'].transform('mean')\nall_data['signal_batch_5k_median'] = all_data.groupby('batch_slices2')['signal'].transform('median')\nall_data['signal_batch_5k_mad'] = all_data.groupby('batch_slices2')['signal'].transform('mad')\nall_data['mean_abs_chg_batch_5k'] = all_data.groupby(['batch_slices2'])['signal'].transform(lambda x: np.mean(np.abs(np.diff(x))))\nall_data['median_abs_chg_batch_5k'] = all_data.groupby(['batch'])['signal'].transform(lambda x: np.median(np.abs(np.diff(x))))\nall_data['abs_max_batch_5k'] = all_data.groupby(['batch_slices2'])['signal'].transform(lambda x: np.max(np.abs(x)))\nall_data['abs_min_batch_5k'] = all_data.groupby(['batch_slices2'])['signal'].transform(lambda x: np.min(np.abs(x)))\nall_data['abs_mean_batch_5k'] = all_data.groupby(['batch_slices2'])['signal'].transform(lambda x: np.mean(np.abs(x)))\nall_data['abs_median_batch_5k'] = all_data.groupby(['batch_slices2'])['signal'].transform(lambda x: np.median(np.abs(x)))\n\nall_data['moving_average_batch_5k_1000_mean'] = all_data.groupby(['batch_slices2'])['signal'].rolling(window=1000).mean().mean(skipna=True)\n\nall_data['range_batch_5k'] = all_data['signal_batch_5k_max'] - all_data['signal_batch_5k_min']\nall_data['maxtomin_batch_5k'] = all_data['signal_batch_5k_max'] \/ all_data['signal_batch_5k_min']\nall_data['abs_avg_batch_5k'] = (all_data['abs_min_batch_5k'] + all_data['abs_max_batch_5k']) \/ 2\n\n#add shifts\nall_data['signal_shift+1'] = all_data.groupby(['batch']).shift(1)['signal']\nall_data['signal_shift-1'] = all_data.groupby(['batch']).shift(-1)['signal']\nall_data['signal_shift+2'] = all_data.groupby(['batch']).shift(2)['signal']\nall_data['signal_shift-2'] = all_data.groupby(['batch']).shift(-2)['signal']\nall_data['signal_shift+1_5k'] = all_data.groupby(['batch_slices2']).shift(1)['signal']\nall_data['signal_shift-1_5k'] = all_data.groupby(['batch_slices2']).shift(-1)['signal']\nall_data['signal_shift+2_5k'] = all_data.groupby(['batch_slices2']).shift(2)['signal']\nall_data['signal_shift-2_5k'] = all_data.groupby(['batch_slices2']).shift(-2)['signal']\n\nall_data['abs_max_signal_shift+1_5k'] = all_data['signal_shift+1_5k'].transform(lambda x: np.max(np.abs(x)))\nall_data['abs_max_signal_shift-1_5k'] = all_data['signal_shift-1_5k'].transform(lambda x: np.max(np.abs(x)))\n","7d6e1314":"for c in ['signal_batch_mean','signal_batch_median',\n          'mean_abs_chg_batch','abs_max_batch','abs_min_batch', 'abs_median_batch',\n          'moving_average_batch_1000_mean', \n          'range_batch','abs_avg_batch',\n          'signal_batch_5k_mean', 'signal_batch_5k_median', 'signal_batch_5k_mad', \n          'moving_average_batch_5k_1000_mean', \n          'mean_abs_chg_batch_5k','abs_max_batch_5k', 'abs_min_batch_5k', 'abs_median_batch_5k', \n          'range_batch_5k','abs_avg_batch_5k',\n          'signal_shift+1', 'signal_shift+2', 'signal_shift-1', 'signal_shift-2', #'signal_shift+3','signal_shift-3',\n          'signal_shift+1_5k', 'signal_shift+2_5k', 'signal_shift-1_5k', 'signal_shift-2_5k', #'signal_shift+3_5k','signal_shift-3_5k',\n          'abs_max_signal_shift+1_5k', 'abs_max_signal_shift-1_5k'\n         ]:\n    all_data[f'{c}_msignal'] = all_data[c] - all_data['signal']","b40dbc31":"FEATURES = [f for f in all_data.columns if f not in ['open_channels','index','time','train','batch',\n                                                     'signal_batch_max', 'signal_batch_mad', 'maxtomin_batch'\n                                                    'batch_index','batch_slices','batch_slices2', 'median_abs_chg_batch_5k',\n                                                    'abs_mean_batch', 'abs_median_batch', 'abs_avg_batch', 'signal_batch_median',\n                                                     'abs_max_batch', 'abs_max_batch_5k', 'abs_median_batch_5k', \n                                                    'moving_average_batch_1000_mean', 'moving_average_batch_5k_1000_mean']]\nprint('....: FEATURE LIST :....')\nprint([f for f in FEATURES])\nprint(f\"Features: {len(FEATURES)}\")","1f74251b":"get_stats(all_data)","fe69e005":"TARGET = 'open_channels'\nall_data['train'] = all_data['train'].astype('bool')\ntrain_df = all_data.query('train').copy()\ntest_df = all_data.query('not train').copy()\ntrain_df['open_channels'] = train_df['open_channels'].astype(int)\nX = train_df[FEATURES]\nX_test = test_df[FEATURES]\ny = train_df[TARGET].values\nsub = test_df[['time']].copy()\ngroups = train_df['batch']\noof_df = train_df[['signal','open_channels']].copy()","c67da183":"del all_data\ndel train_df\ndel test_df\ngc.collect()","cb508954":"\nTOTAL_FOLDS = 7\n\nMODEL_TYPE = 'LGBM'\nSHUFFLE = True\nNUM_BOOST_ROUND = 3_500\nEARLY_STOPPING_ROUNDS = 50\nVERBOSE_EVAL = 500\nRANDOM_SEED = 99943\n\n\nparams = {'learning_rate': 0.05,\n          'max_depth': -1,\n          'num_leaves': 2**8+1,\n          'feature_fraction': 0.8,\n          'bagging_fraction': 0.8,\n          'bagging_freq': 0,\n          'n_jobs': 8,\n          'seed': RANDOM_SEED,\n          'metric': 'rmse',\n          'objective' : 'regression',\n          'num_class': 1\n        }\n\nkfold = KFold(n_splits=TOTAL_FOLDS, shuffle=SHUFFLE, random_state=RANDOM_SEED)\n\nfeature_importance_df = pd.DataFrame()\n\nfold_ = 1 \nfor tr_idx, val_idx in kfold.split(X, y, groups=groups):\n    print(f'====== Fold {fold_:0.0f} of {TOTAL_FOLDS} ======')\n    X_tr, X_val = X.iloc[tr_idx], X.iloc[val_idx]\n    y_tr, y_val = y[tr_idx], y[val_idx]\n    train_set = lgb.Dataset(X_tr, y_tr)\n    val_set = lgb.Dataset(X_val, y_val)\n\n    model = lgb.train(params,\n                      train_set,\n                      num_boost_round = NUM_BOOST_ROUND,\n                      early_stopping_rounds = EARLY_STOPPING_ROUNDS,\n                      valid_sets = [train_set, val_set],\n                      verbose_eval = VERBOSE_EVAL)\n\n    preds = model.predict(X_val, num_iteration=model.best_iteration)\n    preds = np.round(np.clip(preds, 0, 10)).astype(int)\n    test_preds = model.predict(X_test, num_iteration=model.best_iteration)\n    test_preds = np.round(np.clip(test_preds, 0, 10)).astype(int)\n\n    oof_df.loc[oof_df.iloc[val_idx].index, 'oof'] = preds\n    sub[f'open_channels_fold{fold_}'] = test_preds\n\n    f1 = f1_score(oof_df.loc[oof_df.iloc[val_idx].index]['open_channels'],\n                  oof_df.loc[oof_df.iloc[val_idx].index]['oof'],\n                            average = 'macro')\n    rmse = np.sqrt(mean_squared_error(oof_df.loc[oof_df.index.isin(val_idx)]['open_channels'],\n                                      oof_df.loc[oof_df.index.isin(val_idx)]['oof']))\n\n    \n    #feature importance\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = FEATURES\n    fold_importance_df[\"importance\"] = model.feature_importance()\n    fold_importance_df[\"fold\"] = fold_ + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    print(f'Fold {fold_} - validation f1: {f1:0.5f}')\n    print(f'Fold {fold_} - validation rmse: {rmse:0.5f}')\n\n    fold_ += 1\n\noof_f1 = f1_score(oof_df['open_channels'],\n                    oof_df['oof'],\n                    average = 'macro')\noof_rmse = np.sqrt(mean_squared_error(oof_df['open_channels'],\n                                      oof_df['oof']))","c316cc8b":"cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n        .groupby(\"Feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:100].index)\nbest_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n\nplt.figure(figsize=(14,14))\nsns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('LightGBM Features (averaged over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","b4dac7ec":"s_cols = [s for s in sub.columns if 'open_channels' in s]\n\nsub['open_channels'] = sub[s_cols].median(axis=1).astype(int)\nsub[['time','open_channels']].to_csv('.\/submission.csv',\n        index=False,\n        float_format='%0.4f')","af4b5856":"Inspect few rows of train and test data.","1182a17a":"## Average denoising","2235232e":"Use `describe` to report statistical info about train and test data.  \n\nDescribe train.","53051993":"Describe test.","81d0c026":"# <a id='2'>Prepare the data analysis<\/a>  \n\n## Include packages","4b51c39d":"Let's plot the feature importance.","b3a11e66":"The memory used is acceptable. We decide to not perform memory reduction for now.","136e9efd":"### Check memory","c1e08656":"### Release memory","6939ee2f":"# <a id='5'>Model<\/a>  ","8d1ba33a":"## Open channel distribution\n\nLet's look to the open channels distribution only.","a3cd9897":"## Read the data\n\nWe read train, test and submission files.","75a01e69":"## Memory reduction\n\nWe will check memory allocation. If we see that the memory exceeds our RAM resources, we might decide to perform memory reduction.","6f871ec2":"## Signal distribution\n\nWe will look now in detail to the signals in both train and test data.  \nFirst we create a function to plot the signal data.","c48e2b71":"<h1>Ion Switching Advanced EDA and Prediction<\/h1>\n\n<img src=\"https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/Liverpool\/ion%20image.jpg\"><\/img>\n\n<br>\n\n# <a id='0'>Content<\/a>\n\n- <a href='#1'>Introduction<\/a>  \n- <a href='#2'>Prepare the data analysis<\/a>  \n- <a href='#3'>Data exploration<\/a>   \n- <a href='#4'>Feature engineering<\/a>\n- <a href='#5'>Model<\/a>\n- <a href='#6'>Submission<\/a>  \n- <a href='#7'>References<\/a>","e353ba13":"# <a id='4'>Feature engineering<\/a>  ","91016d28":"Create additional features by substracting from a part of the engineered features the engineered signal.","ec5185e6":"# <a id='6'>Submission<\/a>  ","69888f81":"## Signal and open channel\n\nWe will show now both signal and channel signal on the same graph.  \n\nFor this we create a function to show the signal and open channel data.  \nWe will call this function with various segments of signal samples.","c132ee5c":"The signals are sampled at 0.1 ms time intervals.   \nIn **train** there are 5M sample data (from 0 to 500 s) and in **test** there are 2M sample data (from 500 to 700 s).  \nThere are 100 batches of 5 s in train and 40 batches of 5 sec. in test.  \nNumber of open channels is from 0 to 10, average (including 0) being 2.72.  ","024a98c4":"# <a id='1'>Introduction<\/a>  \n\nIon channels are pore-forming membrane proteins that allow ions to pass through the channel pore. Their functions include establishing a resting membrane potential, shaping action potentials and other electrical signals by gating the flow of ions across the cell membrane, controlling the flow of ions across secretory and epithelial cells, and regulating cell volume. Ion channels are present in the membranes of all excitable cells.\n\nWhen ion channels open, they pass electric currents. Existing methods of detecting these state changes are slow and laborious. Humans must supervise the analysis, which imparts considerable bias, in addition to being tedious. These difficulties limit the volume of ion channel current analysis that can be used in research.\n\nThe University of Liverpool\u2019s Institute of Ageing and Chronic Disease is working to advance ion channel research. Their team of scientists have asked for Kagglers help. In this competition, we use ion channel data to better model automatic identification methods. \n\nThe data in train and test are actually batches of 50,000 values, sampled at 0.1 ms, i.e. a total of 50 sec.\/batch, totally 100 batches in train and 40 batches in test.\n","269b6be7":"# <a id='7'>References<\/a>  \n\n[1] University of Liverpool - Ion Switching, Identify the number of channels open at each time point, Competition introduction, https:\/\/www.kaggle.com\/c\/liverpool-ion-switching  \n[2] Ion channel, Wikipedia article, https:\/\/en.wikipedia.org\/wiki\/Ion_channel","837560cb":"For more visibility, we can show samples of data. We will use time intervals of 50 ms.","18212de9":"# <a id='3'>Data exploration<\/a>  \n\n\n## Glimpse the data"}}