{"cell_type":{"cbab4ba8":"code","ee4c958d":"code","fb804b2d":"code","fb1ff65c":"code","a7218558":"code","f80e613a":"code","54998e4b":"code","9bfc616d":"code","5c5e3086":"code","99ca895c":"code","7671db93":"code","69a900ba":"code","13f49282":"code","95681e31":"code","d8470eaa":"code","093f1177":"code","d5daae3f":"code","e0c33bb5":"code","a18eb72c":"code","19f54724":"code","20c60496":"code","f77d89cc":"code","43a6f56f":"code","585887b4":"code","5f234521":"code","a0e3763f":"code","6b84fd56":"code","1a1ef817":"code","0238f509":"code","039aaff4":"code","4a2e6d9b":"code","4031d0e6":"code","a83e8866":"code","4b3abbad":"code","52bb7b4e":"code","29bb8fa6":"code","37c7821e":"code","74cfaae9":"code","fbaf4761":"code","3f7b7592":"code","0beb5e42":"code","3e7fec24":"code","f39dc6fd":"code","2a28d4bf":"code","f0099a54":"code","d70f6e76":"markdown","8cf4a85e":"markdown","97da556e":"markdown","ad7662cc":"markdown","22313d8d":"markdown","95d4d964":"markdown","bf6e1f9a":"markdown","1895ea45":"markdown","dc4688d1":"markdown","e1552d59":"markdown","6de28134":"markdown","7651e096":"markdown"},"source":{"cbab4ba8":"import torch\nimport torch.nn.functional as F\nimport torchvision\nfrom torchvision import datasets,transforms\nfrom torch import nn\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport torchvision.models as models\nimport numpy as np \nimport pandas as pd","ee4c958d":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","fb804b2d":"IMGSIZE = 90\ntransform1 = transforms.Compose([transforms.RandomHorizontalFlip(),\n                                transforms.RandomRotation(0.2),\n                                transforms.ToTensor(),\n                                transforms.Resize((IMGSIZE,IMGSIZE))\n                               ])","fb1ff65c":"full_data = torchvision.datasets.ImageFolder(root = '..\/input\/flowers-recognition\/flowers\/flowers', transform = transform1)","a7218558":"classes = full_data.classes\nprint(\"Classes:\",classes)\nnum_classes = len(full_data.classes)\nprint(\"Number of Classes:\",num_classes)","f80e613a":"train_data, test_data = torch.utils.data.random_split(full_data, [3458, 865])  # In 80% & 20% ratio","54998e4b":"train_loader = torch.utils.data.DataLoader(dataset = train_data, batch_size = 46, shuffle = True)\n\ntest_loader = torch.utils.data.DataLoader(dataset = test_data, batch_size = 46, shuffle = True)","9bfc616d":"def normalize_image(image):\n    image_min = image.min()\n    image_max = image.max()\n    image.clamp_(min = image_min, max = image_max)\n    image.add_(-image_min).div_(image_max - image_min + 1e-5)\n    return image\n\ndef plot_images(images, labels, classes, normalize = True):\n\n    n_images = len(images)\n\n    rows = int(np.sqrt(n_images))\n    cols = int(np.sqrt(n_images))\n\n    fig = plt.figure(figsize = (10, 10))\n\n    for i in range(rows*cols):\n\n        ax = fig.add_subplot(rows, cols, i+1)\n        \n        image = images[i]\n\n        if normalize:\n            image = normalize_image(image)\n\n        ax.imshow(image.permute(1, 2, 0).cpu().numpy())\n        ax.set_title(classes[labels[i]])\n        ax.axis('off')","5c5e3086":"N_IMAGES = 9\n\nimages, labels = zip(*[(image, label) for image, label in [train_data[i] for i in range(N_IMAGES)]])\n\nclasses = full_data.classes\n\nplot_images(images, labels, classes)","99ca895c":"googlenet = models.googlenet(pretrained=True)","7671db93":"print(googlenet)","69a900ba":"q = googlenet.fc.in_features\ngooglenet.fc = nn.Linear(q, num_classes)\n\ngooglenet = googlenet.cuda()","13f49282":"criter = nn.CrossEntropyLoss()\noptimz = torch.optim.Adam(googlenet.parameters(),lr=1e-5,weight_decay=1e-5)","95681e31":"TrainLoss = []\nTrainAcc = []\nTestLoss = []\nTestAcc = []\nnum_epochs = 30","d8470eaa":"# Train the model\ntotal_step = len(train_loader)\n\nfor epoch in range(num_epochs):\n    trainAcc = 0\n    testAcc = 0\n    for i, (images, labels) in enumerate(train_loader):\n        googlenet.train()\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = googlenet(images)\n        trainloss = criter(outputs, labels)\n        \n        # Backward and optimize\n        optimz.zero_grad()\n        trainloss.backward()\n        optimz.step()\n        \n        # Checking accuracy\n        preds = outputs.data.max(dim=1,keepdim=True)[1]\n        trainAcc += preds.eq(labels.data.view_as(preds)).cpu().sum()\n    \n    trainAcc = trainAcc\/len(train_loader.dataset) * 100\n    \n    for i, (images, labels) in enumerate(test_loader):\n        googlenet.eval()\n        images = images.to(device)\n        labels = labels.to(device)\n    \n        # Forward pass\n        outputs = googlenet(images)\n        testloss = criter(outputs, labels)\n    \n        # Checking accuracy\n        preds = outputs.data.max(dim=1,keepdim=True)[1]\n        testAcc += preds.eq(labels.data.view_as(preds)).cpu().sum()\n    \n    testAcc = testAcc\/len(test_loader.dataset) * 100\n    \n    print(\"Epoch {} =>  Train Loss : {trainloss:.2f};   Train Accuracy : {trainAcc:.2f}%;   Test Loss : {testloss:.2f};   Test Accuracy : {testAcc:.2f}%\".format(epoch+1, trainloss=trainloss.item(), trainAcc=trainAcc, testloss=testloss.item(), testAcc=testAcc))\n  \n    TrainLoss.append(trainloss)\n    TrainAcc.append(trainAcc)\n\n    TestLoss.append(testloss)\n    TestAcc.append(testAcc)","093f1177":"# Save the model checkpoint\ntorch.save(googlenet.state_dict(), 'GoogleNetModel.ckpt')","d5daae3f":"plt.plot(range(30),TrainAcc)\nplt.plot(range(30),TestAcc)\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.title(\"Accuracy of GoogleNet\")\nplt.legend([\"Training Accuracy\", \"Testing Accuracy\"])\nplt.show()","e0c33bb5":"plt.plot(range(30),TrainLoss)\nplt.plot(range(30),TestLoss)\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.title(\"Loss of GoogleNet\")\nplt.legend([\"Training Loss\", \"Testing Loss\"])\nplt.show()","a18eb72c":"densenet161 = models.densenet161(pretrained=True)","19f54724":"print(densenet161)","20c60496":"q = densenet161.classifier.in_features\ndensenet161.classifier = nn.Linear(q, num_classes)\n\ndensenet161 = densenet161.cuda()","f77d89cc":"criter = nn.CrossEntropyLoss()\noptimz = torch.optim.Adam(densenet161.parameters(),lr=1e-5,weight_decay=1e-5)","43a6f56f":"TrainLoss = []\nTrainAcc = []\nTestLoss = []\nTestAcc = []\nnum_epochs = 9","585887b4":"# Train the model\ntotal_step = len(train_loader)\n\nfor epoch in range(num_epochs):\n    trainAcc = 0\n    testAcc = 0\n    for i, (images, labels) in enumerate(train_loader):\n        densenet161.train()\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = densenet161(images)\n        trainloss = criter(outputs, labels)\n        \n        # Backward and optimize\n        optimz.zero_grad()\n        trainloss.backward()\n        optimz.step()\n        \n        # Checking accuracy\n        preds = outputs.data.max(dim=1,keepdim=True)[1]\n        trainAcc += preds.eq(labels.data.view_as(preds)).cpu().sum()\n    \n    trainAcc = trainAcc\/len(train_loader.dataset) * 100\n    \n    for i, (images, labels) in enumerate(test_loader):\n        densenet161.eval()\n        images = images.to(device)\n        labels = labels.to(device)\n    \n        # Forward pass\n        outputs = densenet161(images)\n        testloss = criter(outputs, labels)\n    \n        # Checking accuracy\n        preds = outputs.data.max(dim=1,keepdim=True)[1]\n        testAcc += preds.eq(labels.data.view_as(preds)).cpu().sum()\n    \n    testAcc = testAcc\/len(test_loader.dataset) * 100\n    \n    print(\"Epoch {} =>  Train Loss : {trainloss:.2f};   Train Accuracy : {trainAcc:.2f}%;   Test Loss : {testloss:.2f};   Test Accuracy : {testAcc:.2f}%\".format(epoch+1, trainloss=trainloss.item(), trainAcc=trainAcc, testloss=testloss.item(), testAcc=testAcc))\n  \n    TrainLoss.append(trainloss)\n    TrainAcc.append(trainAcc)\n\n    TestLoss.append(testloss)\n    TestAcc.append(testAcc)","5f234521":"# Save the model checkpoint\ntorch.save(densenet161.state_dict(), 'densenet161Model.ckpt')","a0e3763f":"plt.plot(range(9),TrainAcc)\nplt.plot(range(9),TestAcc)\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.title(\"Accuracy of DenseNet-161\")\nplt.legend([\"Training Accuracy\", \"Testing Accuracy\"])\nplt.show()","6b84fd56":"plt.plot(range(9),TrainLoss)\nplt.plot(range(9),TestLoss)\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.title(\"Loss of DenseNet-161\")\nplt.legend([\"Training Loss\", \"Testing Loss\"])\nplt.show()","1a1ef817":"resnet18 = models.resnet18(pretrained=True)","0238f509":"print(resnet18)","039aaff4":"q = resnet18.fc.in_features\nresnet18.fc = nn.Sequential(\n    nn.Linear(q, num_classes),\n    nn.Dropout(p=0.5))\n\nresnet18 = resnet18.cuda()","4a2e6d9b":"criter = nn.CrossEntropyLoss()\noptimz = torch.optim.Adam(resnet18.parameters(),lr=1e-5,weight_decay=1e-5)","4031d0e6":"TrainLoss = []\nTrainAcc = []\nTestLoss = []\nTestAcc = []\nnum_epochs = 15","a83e8866":"# Train the model\ntotal_step = len(train_loader)\n\nfor epoch in range(num_epochs):\n    trainAcc = 0\n    testAcc = 0\n    for i, (images, labels) in enumerate(train_loader):\n        resnet18.train()\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = resnet18(images)\n        trainloss = criter(outputs, labels)\n        \n        # Backward and optimize\n        optimz.zero_grad()\n        trainloss.backward()\n        optimz.step()\n        \n        # Checking accuracy\n        preds = outputs.data.max(dim=1,keepdim=True)[1]\n        trainAcc += preds.eq(labels.data.view_as(preds)).cpu().sum()\n    \n    trainAcc = trainAcc\/len(train_loader.dataset) * 100\n    \n    for i, (images, labels) in enumerate(test_loader):\n        resnet18.eval()\n        images = images.to(device)\n        labels = labels.to(device)\n    \n        # Forward pass\n        outputs = resnet18(images)\n        testloss = criter(outputs, labels)\n    \n        # Checking accuracy\n        preds = outputs.data.max(dim=1,keepdim=True)[1]\n        testAcc += preds.eq(labels.data.view_as(preds)).cpu().sum()\n    \n    testAcc = testAcc\/len(test_loader.dataset) * 100\n    \n    print(\"Epoch {} =>  Train Loss : {trainloss:.2f};   Train Accuracy : {trainAcc:.2f}%;   Test Loss : {testloss:.2f};   Test Accuracy : {testAcc:.2f}%\".format(epoch+1, trainloss=trainloss.item(), trainAcc=trainAcc, testloss=testloss.item(), testAcc=testAcc))\n  \n    TrainLoss.append(trainloss)\n    TrainAcc.append(trainAcc)\n\n    TestLoss.append(testloss)\n    TestAcc.append(testAcc)","4b3abbad":"# Save the model checkpoint\ntorch.save(resnet18.state_dict(), 'resnet18Model.ckpt')","52bb7b4e":"plt.plot(range(15),TrainAcc)\nplt.plot(range(15),TestAcc)\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.title(\"Accuracy of ResNet101\")\nplt.legend([\"Training Accuracy\", \"Testing Accuracy\"])\nplt.show()","29bb8fa6":"plt.plot(range(15),TrainLoss)\nplt.plot(range(15),TestLoss)\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.title(\"Loss of ResNet101\")\nplt.legend([\"Training Loss\", \"Testing Loss\"])\nplt.show()","37c7821e":"resnet101 = models.resnet101(pretrained=True)","74cfaae9":"print(resnet101)","fbaf4761":"q = resnet101.fc.in_features\nresnet101.fc = nn.Linear(q, num_classes)\n\nresnet101 = resnet101.cuda()","3f7b7592":"criter = nn.CrossEntropyLoss()\noptimz = torch.optim.Adam(resnet101.parameters(),lr=1e-5,weight_decay=1e-5)","0beb5e42":"TrainLoss = []\nTrainAcc = []\nTestLoss = []\nTestAcc = []\nnum_epochs = 11","3e7fec24":"# Train the model\ntotal_step = len(train_loader)\n\nfor epoch in range(num_epochs):\n    trainAcc = 0\n    testAcc = 0\n    for i, (images, labels) in enumerate(train_loader):\n        resnet101.train()\n        images = images.to(device)\n        labels = labels.to(device)\n        \n        # Forward pass\n        outputs = resnet101(images)\n        trainloss = criter(outputs, labels)\n        \n        # Backward and optimize\n        optimz.zero_grad()\n        trainloss.backward()\n        optimz.step()\n        \n        # Checking accuracy\n        preds = outputs.data.max(dim=1,keepdim=True)[1]\n        trainAcc += preds.eq(labels.data.view_as(preds)).cpu().sum()\n    \n    trainAcc = trainAcc\/len(train_loader.dataset) * 100\n    \n    for i, (images, labels) in enumerate(test_loader):\n        resnet101.eval()\n        images = images.to(device)\n        labels = labels.to(device)\n    \n        # Forward pass\n        outputs = resnet101(images)\n        testloss = criter(outputs, labels)\n    \n        # Checking accuracy\n        preds = outputs.data.max(dim=1,keepdim=True)[1]\n        testAcc += preds.eq(labels.data.view_as(preds)).cpu().sum()\n    \n    testAcc = testAcc\/len(test_loader.dataset) * 100\n    \n    print(\"Epoch {} =>  Train Loss : {trainloss:.2f};   Train Accuracy : {trainAcc:.2f}%;   Test Loss : {testloss:.2f};   Test Accuracy : {testAcc:.2f}%\".format(epoch+1, trainloss=trainloss.item(), trainAcc=trainAcc, testloss=testloss.item(), testAcc=testAcc))\n  \n    TrainLoss.append(trainloss)\n    TrainAcc.append(trainAcc)\n\n    TestLoss.append(testloss)\n    TestAcc.append(testAcc)","f39dc6fd":"# Save the model checkpoint\ntorch.save(resnet101.state_dict(), 'resnet101Model.ckpt')","2a28d4bf":"plt.plot(range(11),TrainAcc)\nplt.plot(range(11),TestAcc)\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.title(\"Accuracy of ResNet101\")\nplt.legend([\"Training Accuracy\", \"Testing Accuracy\"])\nplt.show()","f0099a54":"plt.plot(range(11),TrainLoss)\nplt.plot(range(11),TestLoss)\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.title(\"Loss of ResNet101\")\nplt.legend([\"Training Loss\", \"Testing Loss\"])\nplt.show()","d70f6e76":"# **Importing Data**","8cf4a85e":"**Following Models Used :**\n*    **Transfer Learning**\n    *   **GoogleNet**\n    *   **DenseNet-161**\n    *   **ResNet-18**\n    *   **ResNet-101**\n    \n*   **Comparing Performance of Above Models**","97da556e":"# **Train-Test Data Split**","ad7662cc":"# **ResNet101**","22313d8d":"# **Comparing Performance of Above Models  :**","95d4d964":"# **Importing Some Basic Libraries**","bf6e1f9a":"# **GoogleNet**","1895ea45":"| <h3><b>Model<\/b><\/h3> | <h3><b>Train Loss<\/b><\/h3> | <h3><b>Train Accuracy<\/b><\/h3> | <h3><b>Test Loss<\/b><\/h3> | <h3><b>Test Accuracy<\/b><\/h3> |\n| --- | --- | --- |--- | --- |\n| <h5><b>GoogleNet<\/b><\/h5> | <h5>0.14<\/h5> | <h5>95.58%<\/h5> | <h5>0.39<\/h5> | <h5>83.12%<\/h5> |\n| <h5><b>DenseNet-161<\/b><\/h5> | <h5>0.45<\/h5> | <h5>95.78%<\/h5> | <h5>0.27<\/h5> | <h5>87.86%<\/h5> |\n| <h5><b>ResNet-18<\/b><\/h5> | <h5><\/h5> | <h5>%<\/h5> | <h5><\/h5> | <h5>%<\/h5> |\n| <h5><b>ResNet-101<\/b><\/h5> | <h5>0.25<\/h5> | <h5>99.33%<\/h5> | <h5>0.60<\/h5> | <h5>88.32%<\/h5> |\n","dc4688d1":"# DenseNet161","e1552d59":"# **Initializing GPU Usage**","6de28134":"# **Transfer Learning:**","7651e096":"# **ResNet18**"}}