{"cell_type":{"9a8cbd5f":"code","d0270651":"code","a93e813e":"code","80d59346":"code","31d91703":"code","032315bd":"code","32099c43":"code","1b7041bd":"code","f92ddd9d":"code","3ae0e83a":"code","71b6f83d":"code","3b41db77":"code","1cfb643c":"code","f06c40b3":"code","fd88f026":"code","4f0666f0":"code","520ab0a8":"code","406f4002":"code","b1414a12":"code","995da7a0":"code","82dfc54e":"code","eca5964c":"code","cb33fca5":"code","4c486190":"code","99326729":"code","9712b25b":"code","3584dbdb":"code","ac3c9f5e":"code","13bdc3bc":"code","114f4b29":"code","87742e2a":"code","84bfd530":"code","589eab41":"code","521027a1":"code","c3d1e6f6":"code","0b4f6713":"code","0ae85732":"code","8592e121":"code","44279210":"code","fd02543c":"code","da397f19":"code","462d89dd":"code","49e12c28":"code","431598d6":"code","22ae5e50":"code","8c39a5f2":"code","f469d6f1":"code","ad9d245c":"code","51783992":"code","bd223952":"code","9c65b091":"code","8add2bd0":"code","cc26bf50":"code","1b1c3d8a":"code","5bec9466":"code","6df1f57c":"code","b8f8e148":"code","24189216":"code","764004b0":"code","a5190043":"code","367e62be":"code","a62cdb41":"code","0eb4329f":"code","c7702794":"code","1167c8c6":"code","e8ce3ec9":"code","a68c15b0":"code","8a5d18f6":"code","161abc60":"code","ca950a65":"code","39505a09":"code","d8e630fd":"code","c2e6699b":"code","7ebc5e21":"code","6edc4304":"code","6d7020c4":"code","885a0e12":"code","198f9951":"code","5d6b3c54":"code","492578bc":"code","ed38f0e7":"code","63ebb84c":"code","2191170c":"code","1ca0b83b":"code","966fba20":"code","4e615c7a":"code","39fcfdb8":"code","55be154a":"code","00c34543":"code","d81e3d83":"code","e7555e0d":"code","f39a2ae0":"code","15987086":"code","2e075b20":"code","b106cfa1":"code","a06b4b3e":"code","b42877e7":"code","3a171fe3":"code","0c4a2ee0":"code","0bbfdceb":"code","181c07de":"code","65306637":"code","d4a692c4":"code","c7817195":"code","13b8ed0b":"code","3d4795f1":"code","df2618de":"code","38b69e50":"code","71af1fd2":"code","47bdfa4b":"code","537a10d6":"code","09d745df":"code","6d4c6551":"markdown","6499636e":"markdown","e0e374dc":"markdown","e25a7531":"markdown","89df4af6":"markdown","6a32a483":"markdown","1e87b093":"markdown","ff874d74":"markdown","bfae09ce":"markdown","6355d57a":"markdown","c9a50cba":"markdown","83c7e863":"markdown","fd172e1a":"markdown","5482a0a6":"markdown","e2e575cd":"markdown","1f588c98":"markdown","86a535d3":"markdown","2e509865":"markdown","045d3bc4":"markdown","bf00c9be":"markdown","7e1810ec":"markdown","ccdda9da":"markdown","2f0229fd":"markdown","89f290b6":"markdown","e2461a39":"markdown","9f301b44":"markdown","67f8b954":"markdown","f9ca00be":"markdown","4de71e98":"markdown","4a00e576":"markdown","02a03184":"markdown","295c2564":"markdown","b45731a7":"markdown","7a2c1813":"markdown","7add6129":"markdown","f0c408b3":"markdown","7aa00c68":"markdown","95ef75f9":"markdown","fa8ff1bd":"markdown","8530b72e":"markdown","b2267c74":"markdown","a3c796b5":"markdown","5a6dba0a":"markdown","350a61db":"markdown","b7ee354c":"markdown","034165c1":"markdown","bbc00800":"markdown","0d732f75":"markdown","269109e7":"markdown","3b2fca89":"markdown","ea18f599":"markdown","4c71f457":"markdown","44033e1d":"markdown","2b6fcafe":"markdown","8942e697":"markdown","eaa3f32b":"markdown","0c4d930a":"markdown","6e671b88":"markdown","c0439337":"markdown","6d926e09":"markdown","ab579376":"markdown","a6424e8e":"markdown","cf7e1e2b":"markdown","0754b695":"markdown","28182e68":"markdown","c236e83c":"markdown","5d609a70":"markdown","4b66a90a":"markdown","d7d2f67e":"markdown","0728a92c":"markdown","5df7ccb9":"markdown","80c8c31b":"markdown","3e2096ff":"markdown","cc61190a":"markdown","2cd0d0f9":"markdown"},"source":{"9a8cbd5f":"# Imports\nimport pandas as pd\nimport numpy as np\nimport scipy\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV, Ridge, Lasso, ElasticNet\nfrom sklearn.metrics import mean_squared_error, make_scorer\n\nfrom sklearn.linear_model import BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold\nimport xgboost as xgb\nimport lightgbm as lgb\n\nfrom scipy.stats import skew\nfrom scipy.stats import norm\nfrom IPython.display import display\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom pandas_profiling import ProfileReport\n\n#ignore annoying warning (from sklearn and seaborn)\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn\n\n# Definitions\npd.set_option('display.float_format', lambda x: '%.3f' % x)\n%matplotlib inline","d0270651":"# Reading the train data\ndf_train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf_train.head()","a93e813e":"# Shape of train data\ndf_train.shape","80d59346":"# Reading the train data\ndf_test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\ndf_test.head()","31d91703":"# Shape of test data\ndf_test.shape","032315bd":"# prof = ProfileReport(df_train)\n# prof.to_file(output_file = \"output.html\")","32099c43":"#descriptive statistics summary\ndf_train['SalePrice'].describe()","1b7041bd":"#histogram\nsns.distplot(df_train['SalePrice']);","f92ddd9d":"#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())","3ae0e83a":"#scatter plot grlivarea\/saleprice\nvar = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","71b6f83d":"#scatter plot totalbsmtsf\/saleprice\nvar = 'TotalBsmtSF'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","3b41db77":"#box plot overallqual\/saleprice\nvar = 'OverallQual'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","1cfb643c":"var = 'YearBuilt'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","f06c40b3":"#correlation matrix\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","fd88f026":"#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","4f0666f0":"#scatterplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(df_train[cols], size = 2.5)\nplt.show();","520ab0a8":"# Removing SalePrice column from the dataset\ndf_train2 = df_train.drop(columns=['SalePrice'])\n\n# Adding a flag to Train and Test dataset to separate them later\ndf_train2['Flag'] = \"Train\"\ndf_test2 = df_test\ndf_test2['Flag'] = \"Test\"\n\n# Combining the two datasets\ndf_combo = pd.concat([df_train2,df_test2], ignore_index=True)\nprint(\"The shape of the combined dataset:\",df_combo.shape)","406f4002":"df_combo.reset_index\ndf_combo.head()","b1414a12":"# missing data in the combined dataset\ntotal = df_combo.isnull().sum().sort_values(ascending=False)\npercent = (df_combo.isnull().sum()\/df_combo.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data = missing_data[missing_data['Total'] >0]\nmissing_data","995da7a0":"# less than 6 records missing data in the train dataset \ntotal2 = df_train.isnull().sum().sort_values(ascending=False)\npercent2 = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data2 = pd.concat([total2, percent2], axis=1, keys=['Total', 'Percent'])\nmissing_data2 = missing_data2[(missing_data2['Total'] >0) & (missing_data2['Total'] <=5)]\nmissing_data2","82dfc54e":"# Handle missing values for features where median\/mean or most common value doesn't make sense\n\n# Dropping the record with NaN in 'Electrical' column in Train dataset\ndf_combo = df_combo.drop(df_combo[(df_combo['Electrical'].isnull()) & (df_combo['Flag'] == 'Train')].index)\ndf_train = df_train.dropna(subset = ['Electrical'])\ndf2 = df_train\n\n# Dropping columns with missing values >= 15% of the # of records\ndrop_col = missing_data.index[missing_data['Percent']>=0.15]\ndf_combo = df_combo.drop(columns = drop_col)\n                                  \n# GarageType etc : data description says NA for garage features is \"no garage\"\ndf_combo.loc[:, \"GarageCond\"] = df_combo.loc[:, \"GarageCond\"].fillna(\"No\")\ndf_combo.loc[:, \"GarageYrBlt\"] = df_combo.loc[:, \"GarageYrBlt\"].fillna(0)\ndf_combo.loc[:, \"GarageFinish\"] = df_combo.loc[:, \"GarageFinish\"].fillna(\"No\")\ndf_combo.loc[:, \"GarageQual\"] = df_combo.loc[:, \"GarageQual\"].fillna(\"No\")\ndf_combo.loc[:, \"GarageType\"] = df_combo.loc[:, \"GarageType\"].fillna(\"No\")\ndf_combo.loc[:, \"GarageArea\"] = df_combo.loc[:, \"GarageArea\"].fillna(0)\ndf_combo.loc[:, \"GarageCars\"] = df_combo.loc[:, \"GarageCars\"].fillna(0)\n\n# BsmtQual etc : data description says NA for basement features is \"no basement\"\ndf_combo.loc[:, \"BsmtQual\"] = df_combo.loc[:, \"BsmtQual\"].fillna(\"No\")\ndf_combo.loc[:, \"BsmtCond\"] = df_combo.loc[:, \"BsmtCond\"].fillna(\"No\")\ndf_combo.loc[:, \"BsmtExposure\"] = df_combo.loc[:, \"BsmtExposure\"].fillna(\"No\")\ndf_combo.loc[:, \"BsmtFinType1\"] = df_combo.loc[:, \"BsmtFinType1\"].fillna(\"No\")\ndf_combo.loc[:, \"BsmtFinType2\"] = df_combo.loc[:, \"BsmtFinType2\"].fillna(\"No\")\ndf_combo.loc[:, \"BsmtHalfBath\"] = df_combo.loc[:, \"BsmtHalfBath\"].fillna(0)\ndf_combo.loc[:, \"BsmtFullBath\"] = df_combo.loc[:, \"BsmtFullBath\"].fillna(0)\ndf_combo.loc[:, \"BsmtUnfSF\"] = df_combo.loc[:, \"BsmtUnfSF\"].fillna(0)\ndf_combo.loc[:, \"BsmtFinSF2\"] = df_combo.loc[:, \"BsmtFinSF2\"].fillna(0)\ndf_combo.loc[:, \"TotalBsmtSF\"] = df_combo.loc[:, \"TotalBsmtSF\"].fillna(0)\ndf_combo.loc[:, \"BsmtFinSF1\"] = df_combo.loc[:, \"BsmtFinSF1\"].fillna(0)\n\n# MasVnrType : NA most likely means no veneer\ndf_combo.loc[:, \"MasVnrType\"] = df_combo.loc[:, \"MasVnrType\"].fillna(\"None\")\ndf_combo.loc[:, \"MasVnrArea\"] = df_combo.loc[:, \"MasVnrArea\"].fillna(0)\n\n# Utilities : This column mostly has AllPub as value and won't be helpful\ndf_combo = df_combo.drop(columns = \"Utilities\")\n\n# MSZoning (The general zoning classification): 'RL' is by far the most common value. So we can fill in missing values with 'RL'\ndf_combo.loc[:, \"MSZoning\"] = df_combo.loc[:, \"MSZoning\"].fillna(df_combo['MSZoning'].mode()[0])\n\n# Functional\ndf_combo.loc[:, \"Functional\"] = df_combo.loc[:, \"Functional\"].fillna(\"Typ\")\n\n# Exterior1st and Exterior2nd: Again Both Exterior 1 & 2 have only one missing value. We will just substitute in the most common string\ndf_combo.loc[:, \"Exterior1st\"] = df_combo.loc[:, \"Exterior1st\"].fillna(df_combo['Exterior1st'].mode()[0])\ndf_combo.loc[:, \"Exterior2nd\"] = df_combo.loc[:, \"Exterior2nd\"].fillna(df_combo['Exterior2nd'].mode()[0])\n\n# KitchenQual: Only one NA value, and same as Electrical, we set 'TA' (which is the most frequent) for the missing value in KitchenQual.\ndf_combo.loc[:, \"KitchenQual\"] = df_combo.loc[:, \"KitchenQual\"].fillna(df_combo['KitchenQual'].mode()[0])\n\n# SaleType : Fill in again with most frequent which is \"WD\"\ndf_combo['SaleType'] = df_combo['SaleType'].fillna(df_combo['SaleType'].mode()[0])","eca5964c":"# Checking if there are anymore NaNs in the dataset\nprint(\"Count of NaNs in the combined dataset: \", sum(df_combo.isnull().sum()))","cb33fca5":"# Keeping a copy of the combined dataset\ndf_combo_basic = df_combo","4c486190":"# standardizing data\nsaleprice_scaled = StandardScaler().fit_transform(df_train['SalePrice'][:,np.newaxis]);\nlow_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\nhigh_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\nprint('outer range (low) of the distribution:')\nprint(low_range)\nprint('\\nouter range (high) of the distribution:')\nprint(high_range)","99326729":"# boxplot for the scaled SalePrice data\nplt.figure(figsize=(3, 1))\nsp_box = sns.boxplot(x = saleprice_scaled, width = 0.2)\nsp_box","9712b25b":"# bivariate analysis saleprice\/grlivarea\nvar = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","3584dbdb":"# deleting points in the train part of the combined dataset\ndf_train.sort_values(by = 'GrLivArea', ascending = False)[:2]","ac3c9f5e":"df_combo = df_combo.drop(df_combo[df_combo['Id'] == 1299].index)\ndf_combo = df_combo.drop(df_combo[df_combo['Id'] == 524].index)\n\ndf_train = df_train.drop(df_train[df_train['Id'] == 1299].index)\ndf_train = df_train.drop(df_train[df_train['Id'] == 524].index)","13bdc3bc":"#bivariate analysis saleprice\/grlivarea\nvar = 'TotalBsmtSF'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","114f4b29":"# histogram and normal probability plot\nsns.distplot(df_train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = scipy.stats.probplot(df_train['SalePrice'], plot=plt)","87742e2a":"#applying log transformation\ndf_train['SalePrice'] = np.log(df_train['SalePrice'])","84bfd530":"#transformed histogram and normal probability plot\nsns.distplot(df_train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = scipy.stats.probplot(df_train['SalePrice'], plot=plt)","589eab41":"#histogram and normal probability plot\nsns.distplot(df_train['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = scipy.stats.probplot(df_train['GrLivArea'], plot=plt)","521027a1":"#data transformation\ndf_combo['GrLivArea'] = np.log(df_combo['GrLivArea'])\ndf_train['GrLivArea'] = np.log(df_train['GrLivArea'])","c3d1e6f6":"#transformed histogram and normal probability plot\nsns.distplot(df_train['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = scipy.stats.probplot(df_train['GrLivArea'], plot=plt)","0b4f6713":"#histogram and normal probability plot\nsns.distplot(df_train['TotalBsmtSF'], fit=norm);\nfig = plt.figure()\nres = scipy.stats.probplot(df_train['TotalBsmtSF'], plot=plt)","0ae85732":"#create column for new variable (one is enough because it's a binary categorical feature)\n#if area>0 it gets 1, for area==0 it gets 0\ndf_train['HasBsmt'] = pd.Series(len(df_train['TotalBsmtSF']), index=df_train.index)\ndf_train['HasBsmt'] = 0 \ndf_train.loc[df_train['TotalBsmtSF']>0,'HasBsmt'] = 1\n#transform data\ndf_train.loc[df_train['HasBsmt']==1,'TotalBsmtSF'] = np.log(df_train['TotalBsmtSF'])","8592e121":"#create column for new variable (one is enough because it's a binary categorical feature)\n#if area>0 it gets 1, for area==0 it gets 0\ndf_combo['HasBsmt'] = pd.Series(len(df_combo['TotalBsmtSF']), index=df_combo.index)\ndf_combo['HasBsmt'] = 0 \ndf_combo.loc[df_combo['TotalBsmtSF']>0,'HasBsmt'] = 1\n#transform data\ndf_combo.loc[df_combo['HasBsmt']==1,'TotalBsmtSF'] = np.log(df_combo['TotalBsmtSF'])","44279210":"#histogram and normal probability plot\nsns.distplot(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], fit=norm);\nfig = plt.figure()\nres = scipy.stats.probplot(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], plot=plt)","fd02543c":"#scatter plot\nplt.scatter(df_train['GrLivArea'], df_train['SalePrice']);","da397f19":"#scatter plot\nplt.scatter(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], df_train[df_train['TotalBsmtSF']>0]['SalePrice']);","462d89dd":"# Some numerical features are actually really categories\ndf_combo = df_combo.replace({\"MSSubClass\" : {20 : \"SC20\", 30 : \"SC30\", 40 : \"SC40\", 45 : \"SC45\", \n                                       50 : \"SC50\", 60 : \"SC60\", 70 : \"SC70\", 75 : \"SC75\", \n                                       80 : \"SC80\", 85 : \"SC85\", 90 : \"SC90\", 120 : \"SC120\", \n                                       150 : \"SC150\", 160 : \"SC160\", 180 : \"SC180\", 190 : \"SC190\"},\n                       \"MoSold\" : {1 : \"Jan\", 2 : \"Feb\", 3 : \"Mar\", 4 : \"Apr\", 5 : \"May\", 6 : \"Jun\",\n                                   7 : \"Jul\", 8 : \"Aug\", 9 : \"Sep\", 10 : \"Oct\", 11 : \"Nov\", 12 : \"Dec\"}\n                      })","49e12c28":"df_combo = df_combo.replace({\n                       \"BsmtCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"BsmtExposure\" : {\"No\" : 0, \"Mn\" : 1, \"Av\": 2, \"Gd\" : 3},\n                       \"BsmtFinType1\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtFinType2\" : {\"No\" : 0, \"Unf\" : 1, \"LwQ\": 2, \"Rec\" : 3, \"BLQ\" : 4, \n                                         \"ALQ\" : 5, \"GLQ\" : 6},\n                       \"BsmtQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"ExterCond\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \"ExterQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\": 3, \"Gd\": 4, \"Ex\" : 5},\n                       \n                       \"Functional\" : {\"Sal\" : 1, \"Sev\" : 2, \"Maj2\" : 3, \"Maj1\" : 4, \"Mod\": 5, \n                                       \"Min2\" : 6, \"Min1\" : 7, \"Typ\" : 8},\n                       \"GarageCond\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"GarageQual\" : {\"No\" : 0, \"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"HeatingQC\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"KitchenQual\" : {\"Po\" : 1, \"Fa\" : 2, \"TA\" : 3, \"Gd\" : 4, \"Ex\" : 5},\n                       \"LandSlope\" : {\"Sev\" : 1, \"Mod\" : 2, \"Gtl\" : 3},\n                       \"LotShape\" : {\"IR3\" : 1, \"IR2\" : 2, \"IR1\" : 3, \"Reg\" : 4},\n                       \"PavedDrive\" : {\"N\" : 0, \"P\" : 1, \"Y\" : 2},\n                       \n                       \"Street\" : {\"Grvl\" : 1, \"Pave\" : 2},\n                       \"Utilities\" : {\"ELO\" : 1, \"NoSeWa\" : 2, \"NoSewr\" : 3, \"AllPub\" : 4}}\n                     )","431598d6":"df_combo[\"SimplOverallQual\"] = df_combo.OverallQual.replace({1 : 1, 2 : 1, 3 : 1, # bad\n                                                       4 : 2, 5 : 2, 6 : 2, # average\n                                                       7 : 3, 8 : 3, 9 : 3, 10 : 3 # good\n                                                      })\n\ndf_combo[\"SimplOverallCond\"] = df_combo.OverallCond.replace({1 : 1, 2 : 1, 3 : 1, # bad\n                                                       4 : 2, 5 : 2, 6 : 2, # average\n                                                       7 : 3, 8 : 3, 9 : 3, 10 : 3 # good\n                                                      })\n\ndf_combo[\"SimplGarageCond\"] = df_combo.GarageCond.replace({1 : 1, # bad\n                                                     2 : 1, 3 : 1, # average\n                                                     4 : 2, 5 : 2 # good\n                                                    })\n\ndf_combo[\"SimplGarageQual\"] = df_combo.GarageQual.replace({1 : 1, # bad\n                                                     2 : 1, 3 : 1, # average\n                                                     4 : 2, 5 : 2 # good\n                                                    })\n\ndf_combo[\"SimplFunctional\"] = df_combo.Functional.replace({1 : 1, 2 : 1, # bad\n                                                     3 : 2, 4 : 2, # major\n                                                     5 : 3, 6 : 3, 7 : 3, # minor\n                                                     8 : 4 # typical\n                                                    })\n\ndf_combo[\"SimplKitchenQual\"] = df_combo.KitchenQual.replace({1 : 1, # bad\n                                                       2 : 1, 3 : 1, # average\n                                                       4 : 2, 5 : 2 # good\n                                                      })\n\ndf_combo[\"SimplHeatingQC\"] = df_combo.HeatingQC.replace({1 : 1, # bad\n                                                   2 : 1, 3 : 1, # average\n                                                   4 : 2, 5 : 2 # good\n                                                  })\n\ndf_combo[\"SimplBsmtFinType1\"] = df_combo.BsmtFinType1.replace({1 : 1, # unfinished\n                                                         2 : 1, 3 : 1, # rec room\n                                                         4 : 2, 5 : 2, 6 : 2 # living quarters\n                                                        })\n\ndf_combo[\"SimplBsmtFinType2\"] = df_combo.BsmtFinType2.replace({1 : 1, # unfinished\n                                                         2 : 1, 3 : 1, # rec room\n                                                         4 : 2, 5 : 2, 6 : 2 # living quarters\n                                                        })\n\ndf_combo[\"SimplBsmtCond\"] = df_combo.BsmtCond.replace({1 : 1, # bad\n                                                 2 : 1, 3 : 1, # average\n                                                 4 : 2, 5 : 2 # good\n                                                })\n\ndf_combo[\"SimplBsmtQual\"] = df_combo.BsmtQual.replace({1 : 1, # bad\n                                                 2 : 1, 3 : 1, # average\n                                                 4 : 2, 5 : 2 # good\n                                                })\n\ndf_combo[\"SimplExterCond\"] = df_combo.ExterCond.replace({1 : 1, # bad\n                                                   2 : 1, 3 : 1, # average\n                                                   4 : 2, 5 : 2 # good\n                                                  })\n\ndf_combo[\"SimplExterQual\"] = df_combo.ExterQual.replace({1 : 1, # bad\n                                                   2 : 1, 3 : 1, # average\n                                                   4 : 2, 5 : 2 # good\n                                                  })","22ae5e50":"# Overall quality of the house\ndf_combo[\"OverallGrade\"] = df_combo[\"OverallQual\"] * df_combo[\"OverallCond\"]\n\n# Overall quality of the garage\ndf_combo[\"GarageGrade\"] = df_combo[\"GarageQual\"] * df_combo[\"GarageCond\"]\n\n# Overall quality of the exterior\ndf_combo[\"ExterGrade\"] = df_combo[\"ExterQual\"] * df_combo[\"ExterCond\"]\n\n# Overall kitchen score\ndf_combo[\"KitchenScore\"] = df_combo[\"KitchenAbvGr\"] * df_combo[\"KitchenQual\"]\n\n# Overall garage score\ndf_combo[\"GarageScore\"] = df_combo[\"GarageArea\"] * df_combo[\"GarageQual\"]\n\n# Simplified overall quality of the house\ndf_combo[\"SimplOverallGrade\"] = df_combo[\"SimplOverallQual\"] * df_combo[\"SimplOverallCond\"]\n\n# Simplified overall quality of the exterior\ndf_combo[\"SimplExterGrade\"] = df_combo[\"SimplExterQual\"] * df_combo[\"SimplExterCond\"]\n\n# Simplified overall garage score\ndf_combo[\"SimplGarageScore\"] = df_combo[\"GarageArea\"] * df_combo[\"SimplGarageQual\"]\n\n# Simplified overall kitchen score\ndf_combo[\"SimplKitchenScore\"] = df_combo[\"KitchenAbvGr\"] * df_combo[\"SimplKitchenQual\"]\n\n# Total number of bathrooms\ndf_combo[\"TotalBath\"] = df_combo[\"BsmtFullBath\"] + (0.5 * df_combo[\"BsmtHalfBath\"]) + \\\ndf_combo[\"FullBath\"] + (0.5 * df_combo[\"HalfBath\"])\n\n# Total SF for house (incl. basement)\ndf_combo[\"AllSF\"] = df_combo[\"GrLivArea\"] + df_combo[\"TotalBsmtSF\"]\n\n# Total SF for 1st + 2nd floors\ndf_combo[\"AllFlrsSF\"] = df_combo[\"1stFlrSF\"] + df_combo[\"2ndFlrSF\"]\n\n# Total SF for porch\ndf_combo[\"AllPorchSF\"] = df_combo[\"OpenPorchSF\"] + df_combo[\"EnclosedPorch\"] + \\\ndf_combo[\"3SsnPorch\"] + df_combo[\"ScreenPorch\"]\n\n# Has masonry veneer or not\ndf_combo[\"HasMasVnr\"] = df_combo.MasVnrType.replace({\"BrkCmn\" : 1, \"BrkFace\" : 1, \"CBlock\" : 1, \n                                               \"Stone\" : 1, \"None\" : 0})\n\n# House completed before sale or not\ndf_combo[\"BoughtOffPlan\"] = df_combo.SaleCondition.replace({\"Abnorml\" : 0, \"Alloca\" : 0, \"AdjLand\" : 0, \n                                                      \"Family\" : 0, \"Normal\" : 0, \"Partial\" : 1})","8c39a5f2":"# print(\"Find most important features relative to target\")\n# corr = df_combo.corr()\n# corr.sort_values([\"SalePrice\"], ascending = False, inplace = True)\n# print(corr.SalePrice)","f469d6f1":"# train[\"OverallQual-s2\"] = train[\"OverallQual\"] ** 2\n# train[\"OverallQual-s3\"] = train[\"OverallQual\"] ** 3\n# train[\"OverallQual-Sq\"] = np.sqrt(train[\"OverallQual\"])\n# train[\"AllSF-2\"] = train[\"AllSF\"] ** 2\n# train[\"AllSF-3\"] = train[\"AllSF\"] ** 3\n# train[\"AllSF-Sq\"] = np.sqrt(train[\"AllSF\"])\n# train[\"AllFlrsSF-2\"] = train[\"AllFlrsSF\"] ** 2\n# train[\"AllFlrsSF-3\"] = train[\"AllFlrsSF\"] ** 3\n# train[\"AllFlrsSF-Sq\"] = np.sqrt(train[\"AllFlrsSF\"])\n# train[\"GrLivArea-2\"] = train[\"GrLivArea\"] ** 2\n# train[\"GrLivArea-3\"] = train[\"GrLivArea\"] ** 3\n# train[\"GrLivArea-Sq\"] = np.sqrt(train[\"GrLivArea\"])\n# train[\"SimplOverallQual-s2\"] = train[\"SimplOverallQual\"] ** 2\n# train[\"SimplOverallQual-s3\"] = train[\"SimplOverallQual\"] ** 3\n# train[\"SimplOverallQual-Sq\"] = np.sqrt(train[\"SimplOverallQual\"])\n# train[\"ExterQual-2\"] = train[\"ExterQual\"] ** 2\n# train[\"ExterQual-3\"] = train[\"ExterQual\"] ** 3\n# train[\"ExterQual-Sq\"] = np.sqrt(train[\"ExterQual\"])\n# train[\"GarageCars-2\"] = train[\"GarageCars\"] ** 2\n# train[\"GarageCars-3\"] = train[\"GarageCars\"] ** 3\n# train[\"GarageCars-Sq\"] = np.sqrt(train[\"GarageCars\"])\n# train[\"TotalBath-2\"] = train[\"TotalBath\"] ** 2\n# train[\"TotalBath-3\"] = train[\"TotalBath\"] ** 3\n# train[\"TotalBath-Sq\"] = np.sqrt(train[\"TotalBath\"])\n# train[\"KitchenQual-2\"] = train[\"KitchenQual\"] ** 2\n# train[\"KitchenQual-3\"] = train[\"KitchenQual\"] ** 3\n# train[\"KitchenQual-Sq\"] = np.sqrt(train[\"KitchenQual\"])\n# train[\"GarageScore-2\"] = train[\"GarageScore\"] ** 2\n# train[\"GarageScore-3\"] = train[\"GarageScore\"] ** 3\n# train[\"GarageScore-Sq\"] = np.sqrt(train[\"GarageScore\"])","ad9d245c":"categorical_features = df_combo.select_dtypes(include = [\"object\"]).columns\nnumerical_features = df_combo.select_dtypes(exclude = [\"object\"]).columns\n\nprint(\"Numerical features : \" + str(len(numerical_features)))\nprint(\"Categorical features : \" + str(len(categorical_features)))\ndf_combo_num = df_combo[numerical_features]\ndf_combo_cat = df_combo[categorical_features]","51783992":"# Repeating the same steps for the basic combined dataset\ncategorical_features_basic = df_combo_basic.select_dtypes(include = [\"object\"]).columns\nnumerical_features_basic = df_combo_basic.select_dtypes(exclude = [\"object\"]).columns\n\ndf_combo_basic_num = df_combo_basic[numerical_features_basic]\ndf_combo_basic_cat = df_combo_basic[categorical_features_basic]","bd223952":"# Handle remaining missing values for numerical features by using median as replacement\nprint(\"NAs for numerical features in the combined dataset : \" + str(df_combo_num.isnull().values.sum()))\n# df_combo_num = df_combo_num.fillna(df_combo_num.median())\nprint(\"Remaining NAs for numerical features in the combined dataset : \" + str(df_combo_num.isnull().values.sum()))","9c65b091":"# skewness = train_num.apply(lambda x: skew(x))\n# skewness = skewness[abs(skewness) > 0.5]\n# print(str(skewness.shape[0]) + \" skewed numerical features to log transform\")\n# skewed_features = skewness.index\n# train_num[skewed_features] = np.log1p(train_num[skewed_features])","8add2bd0":"print(\"NAs for categorical features in the combined dataset : \" + str(df_combo_cat.isnull().values.sum()))\ndf_combo_cat2 = df_combo_cat\ndf_combo_cat2 = df_combo_cat2.drop(columns = \"Flag\")\ndf_combo_cat2 = pd.get_dummies(df_combo_cat2)\n\ndf_combo_cat = pd.concat([df_combo_cat2, df_combo_cat['Flag']], axis=1)\n\nprint(\"Remaining NAs for categorical features in the combined dataset : \" + str(df_combo_cat.isnull().values.sum()))","cc26bf50":"# Creating dummy variables for the basic combined dataset\ndf_combo_basic_cat2 = df_combo_basic_cat\ndf_combo_basic_cat2 = df_combo_basic_cat2.drop(columns = \"Flag\")\ndf_combo_basic_cat2 = pd.get_dummies(df_combo_basic_cat2)\n\ndf_combo_basic_cat = pd.concat([df_combo_basic_cat2, df_combo_basic_cat['Flag']], axis=1)\n\nprint(\"Remaining NAs for categorical features in the basic combined dataset : \" + str(df_combo_basic_cat.isnull().values.sum()))","1b1c3d8a":"df_combo2 = pd.concat([df_combo_num, df_combo_cat], axis=1)\ndf_combo2.shape","5bec9466":"# Repeating the same step for basic dataset\ndf_combo_basic2 = pd.concat([df_combo_basic_num, df_combo_basic_cat], axis=1)\ndf_combo_basic2.shape","6df1f57c":"df_train2 = df_combo2[df_combo2['Flag'] == \"Train\"]\nSp = df_train.loc[:,'SalePrice'].to_frame()\n\nprint(\"Shape 1: \",df_train2.shape)\nprint(\"Shape 2: \",Sp.shape)","b8f8e148":"df_train2.tail(5)","24189216":"Sp.tail(5)","764004b0":"df_train2 = pd.concat([df_train2, Sp], axis=1)\ndf_train2.shape","a5190043":"df_train2 = df_train2.set_index('Id')\ndf_train2.index.names = [None]\n\ndf_test2 = df_combo2[df_combo2['Flag'] == \"Test\"]\ndf_test2 = df_test2.set_index('Id')\ndf_test2.index.names = [None]\n\nprint(\"Shape of train dataset: \", df_train2.shape)\nprint(\"Shape of test dataset: \", df_test2.shape)","367e62be":"# Repeating the same step for basic dataset\ndf_train_basic2 = df_combo_basic2[df_combo_basic2['Flag'] == \"Train\"]\n\n# To avoid using log transformed SalePrice column\ndf_train_basic2 = pd.concat([df_train_basic2, df2['SalePrice']], axis=1)\ndf_train_basic2 = df_train_basic2.set_index('Id')\ndf_train_basic2.index.names = [None]\n\ndf_test_basic2 = df_combo_basic2[df_combo_basic2['Flag'] == \"Test\"]\ndf_test_basic2 = df_test_basic2.set_index('Id')\ndf_test_basic2.index.names = [None]\n\nprint(\"Shape of train dataset: \", df_train_basic2.shape)\nprint(\"Shape of test dataset: \", df_test_basic2.shape)","a62cdb41":"cln = list(df_train2.columns)\ncln.remove('SalePrice')\ncln.remove('Flag')\nnumerical_features = list(numerical_features)\nnumerical_features.remove('Id')\n\n\ncln2 = list(df_train_basic2.columns)\ncln2.remove('SalePrice')\ncln2.remove('Flag')\nnumerical_features_basic = list(numerical_features_basic)\nnumerical_features_basic.remove('Id')","0eb4329f":"X_train, X_valid, y_train, y_valid = train_test_split(df_train2[cln], df_train2.loc[:,'SalePrice'].to_frame(), test_size=0.25, random_state=13)\nX_train2, X_valid2, y_train2, y_valid2 = train_test_split(df_train_basic2[cln2], df_train_basic2.loc[:,'SalePrice'].to_frame(), test_size=0.25, random_state=13)","c7702794":"stdSc = StandardScaler()\n\nX_train.loc[:, numerical_features] = stdSc.fit_transform(X_train.loc[:, numerical_features])\nX_valid.loc[:, numerical_features] = stdSc.transform(X_valid.loc[:, numerical_features])\ndf_test2.loc[:, numerical_features] = stdSc.transform(df_test2.loc[:, numerical_features])\n\n# y_train = stdSc.fit_transform(y_train)\n# y_valid = stdSc.transform(y_valid)","1167c8c6":"stdSc_basic = StandardScaler()\n\nX_train2.loc[:, numerical_features_basic] = stdSc_basic.fit_transform(X_train2.loc[:, numerical_features_basic])\nX_valid2.loc[:, numerical_features_basic] = stdSc_basic.transform(X_valid2.loc[:, numerical_features_basic])\ndf_test_basic2.loc[:, numerical_features_basic] = stdSc_basic.transform(df_test_basic2.loc[:, numerical_features_basic])\n\ny_train2 = pd.Series((np.ravel(stdSc_basic.fit_transform(y_train2)) + 12), index= X_train2.index)\ny_valid2 = pd.Series((np.ravel(stdSc_basic.transform(y_valid2)) + 12), index= X_valid2.index)","e8ce3ec9":"# Checking if all X_train looks good\n\ncf = X_train.select_dtypes(include = [\"object\"]).columns\nnf = X_train.select_dtypes(exclude = [\"object\"]).columns\n\nprint(\"Numerical features : \" + str(len(nf)))\nprint(\"Categorical features : \" + str(len(cf)))","a68c15b0":"y_train2.describe()","8a5d18f6":"# boxplot for the train SalePrice data\nplt.figure(figsize=(3, 1))\nyt_box = sns.boxplot(x = y_train2, width = 0.2)\nyt_box","161abc60":"y_valid2.describe()","ca950a65":"# boxplot for the validation SalePrice data\nplt.figure(figsize=(3, 1))\nyt_box = sns.boxplot(x = y_valid2, width = 0.2)\nyt_box","39505a09":"# Define error measure for official scoring : RMSE\nscorer = make_scorer(mean_squared_error, greater_is_better = False)\n\ndef rmse_cv_train(model, X_train, y_train, cv=5):\n    rmse = np.sqrt(-cross_val_score(model, X_train, y_train, scoring = scorer, cv = 5))\n    return(rmse)\n\n# def rmse_cv_valid(model, X_valid, y_valid, cv=5):\n#     rmse= np.sqrt(-cross_val_score(model, X_valid, y_valid, scoring = scorer, cv = 5))\n#     return(rmse)","d8e630fd":"# We use a validation dataset to validate which model\/ model parameter is the best one.\n\ndef Linear_regression_no_reg(X_train, y_train, X_valid, y_valid):\n    # Linear Regression\n    lr = LinearRegression()\n    lr2 = LinearRegression()\n    lr.fit(X_train, y_train)\n    \n    rmse_train = rmse_cv_train(lr2, X_train, y_train).mean()\n    rmse_valid = np.sqrt(mean_squared_error(y_valid, lr.predict(X_valid)))\n    \n    r2_train = cross_val_score(lr2, X_train, y_train, cv=5).mean()\n    r2_valid = lr.score(X_valid, y_valid)\n\n    # Results from the fit model\n    print(\"RMSE on Training set without error cross validation :\", np.sqrt(mean_squared_error(y_train, lr.predict(X_train))))\n    print(\"RMSE on Validation set without error cross validation :\", rmse_valid)\n    print(\"\")\n    print(\"RMSE on Training set with error cross validation :\", rmse_train)\n#     print(\"RMSE on Validation set with error cross validation :\", rmse_valid)\n    print(\"\")\n    print(\"R2 on Training set without error cross validation :\", lr.score(X_train, y_train))\n    print(\"R2 on Validation set without error cross validation :\", r2_valid)\n    print(\"\")\n    print(\"R2 on Training set with error cross validation :\", r2_train)\n#     print(\"R2 on Validation set with error cross validation :\", r2_valid)\n    \n    y_train_pred = np.ravel(lr.predict(X_train))\n    y_valid_pred = np.ravel(lr.predict(X_valid))\n\n\n#     # Plot residuals\n#     plt.scatter(y_train_pred, y_train_pred - np.ravel(y_train.to_numpy()), c = \"blue\", marker = \"s\", label = \"Training data\")\n#     plt.scatter(y_valid_pred, y_valid_pred - np.ravel(y_valid.to_numpy()), c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\n#     plt.title(\"Linear regression\")\n#     plt.xlabel(\"Predicted values\")\n#     plt.ylabel(\"Residuals\")\n#     plt.legend(loc = \"upper left\")\n#     plt.hlines(y = 0, xmin = 9, xmax = 15, color = \"red\")\n#     plt.show()\n\n#     # Plot predictions\n#     plt.scatter(y_train_pred, np.ravel(y_train.to_numpy()), c = \"blue\", marker = \"s\", label = \"Training data\")\n#     plt.scatter(y_valid_pred, np.ravel(y_valid.to_numpy()), c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\n#     plt.title(\"Linear regression\")\n#     plt.xlabel(\"Predicted values\")\n#     plt.ylabel(\"Real values\")\n#     plt.legend(loc = \"upper left\")\n#     plt.plot([9, 15], [9, 15], c = \"red\")\n#     plt.show()\n    return(rmse_train, rmse_valid, r2_train, r2_valid, y_valid_pred)","c2e6699b":"LR_no_reg = Linear_regression_no_reg(X_train, y_train, X_valid, y_valid)","7ebc5e21":"def Linear_regression_Ridge(X_train, y_train, X_valid, y_valid):    \n    \n    # Ridge Linear Regression\n    ridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])\n    ridge.fit(X_train, y_train)\n    alpha = ridge.alpha_\n    print(\"Best alpha :\", alpha)\n\n    print(\"Try again for more precision with alphas centered around \" + str(alpha))\n    ridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4], \n                    cv = 5)\n    ridge.fit(X_train, y_train)\n    alpha = ridge.alpha_\n    coef = np.ravel(ridge.coef_)\n    print(\"Best alpha :\", alpha)\n    print(\"\")\n    \n    ridge2 = Ridge(alpha = alpha)\n    \n    rmse_train = rmse_cv_train(ridge2, X_train, y_train).mean()\n    rmse_valid = np.sqrt(mean_squared_error(y_valid, ridge.predict(X_valid)))\n    \n    r2_train = cross_val_score(ridge2, X_train, y_train, cv=5).mean()\n    r2_valid = ridge.score(X_valid, y_valid)\n    \n    coefs = pd.Series(coef, index = X_train.columns)\n  \n    # Results from the fit model\n    print(\"RMSE on Training set without error cross validation :\", np.sqrt(mean_squared_error(y_train, ridge.predict(X_train))))\n    print(\"RMSE on Validation set without error cross validation :\", rmse_valid)\n    print(\"\")\n    print(\"RMSE on Training set with error cross validation :\", rmse_train)\n#     print(\"RMSE on Validation set with error cross validation :\", rmse_valid)\n    print(\"\")\n    print(\"R2 on Training set without error cross validation :\", ridge.score(X_train, y_train))\n    print(\"R2 on Validation set without error cross validation :\", r2_valid)\n    print(\"\")\n    print(\"R2 on Training set with error cross validation :\", r2_train)\n#     print(\"R2 on Validation set with error cross validation :\", r2_valid)\n \n    y_train_rdg = np.ravel(ridge.predict(X_train))\n    y_valid_rdg = np.ravel(ridge.predict(X_valid))\n\n    # Plot residuals\n    plt.scatter(y_train_rdg, y_train_rdg - np.ravel(y_train.to_numpy()), c = \"blue\", marker = \"s\", label = \"Training data\")\n    plt.scatter(y_valid_rdg, y_valid_rdg - np.ravel(y_valid.to_numpy()), c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\n    plt.title(\"Linear regression with Ridge regularization\")\n    plt.xlabel(\"Predicted values\")\n    plt.ylabel(\"Residuals\")\n    plt.legend(loc = \"upper left\")\n    plt.hlines(y = 0, xmin = 9, xmax = 15, color = \"red\")\n    plt.show()\n\n    # Plot predictions\n    plt.scatter(y_train_rdg, np.ravel(y_train.to_numpy()), c = \"blue\", marker = \"s\", label = \"Training data\")\n    plt.scatter(y_valid_rdg, np.ravel(y_valid.to_numpy()), c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\n    plt.title(\"Linear regression with Ridge regularization\")\n    plt.xlabel(\"Predicted values\")\n    plt.ylabel(\"Real values\")\n    plt.legend(loc = \"upper left\")\n    plt.plot([9, 15], [9, 15], c = \"red\")\n    plt.show()\n    \n    # Plot important coefficients\n    print(\"\")\n    print(\"Ridge picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n          str(sum(coefs == 0)) + \" features\")\n    imp_coefs = pd.concat([coefs.sort_values().head(10),\n                         coefs.sort_values().tail(10)])\n    imp_coefs.plot(kind = \"barh\")\n    plt.title(\"Coefficients in the Ridge Model\")\n    plt.show()\n    return(rmse_train, rmse_valid, r2_train, r2_valid, y_valid_rdg)","6edc4304":"LR_Ridge = Linear_regression_Ridge(X_train, y_train, X_valid, y_valid)","6d7020c4":"def Linear_regression_Lasso(X_train, y_train, X_valid, y_valid):    \n    \n    # Lasso Linear Regression\n    lasso = LassoCV(alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, \n                          0.3, 0.6, 1], \n                max_iter = 50000, cv = 5)\n    lasso.fit(X_train, y_train)\n    alpha = lasso.alpha_\n    print(\"Best alpha :\", alpha)\n\n    print(\"Try again for more precision with alphas centered around \" + str(alpha))\n    lasso = LassoCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, \n                          alpha * .85, alpha * .9, alpha * .95, alpha, alpha * 1.05, \n                          alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, alpha * 1.35, \n                          alpha * 1.4], \n                max_iter = 50000, cv = 5)\n    lasso.fit(X_train, y_train)\n    alpha = lasso.alpha_\n    coef = np.ravel(lasso.coef_)\n    print(\"Best alpha :\", alpha)\n    print(\"\")\n    \n    lasso2 = Lasso(alpha = alpha)\n    \n    rmse_train = rmse_cv_train(lasso2, X_train, y_train).mean()\n    rmse_valid = np.sqrt(mean_squared_error(y_valid, lasso.predict(X_valid)))\n    \n    r2_train = cross_val_score(lasso2, X_train, y_train, cv=5).mean()\n    r2_valid = lasso.score(X_valid, y_valid)\n    \n    coefs = pd.Series(coef, index = X_train.columns)\n  \n    # Results from the fit model\n    print(\"RMSE on Training set without error cross validation :\", np.sqrt(mean_squared_error(y_train, lasso.predict(X_train))))\n    print(\"RMSE on Validation set without error cross validation :\", rmse_valid)\n    print(\"\")\n    print(\"RMSE on Training set with error cross validation :\", rmse_train)\n#     print(\"RMSE on Validation set with error cross validation :\", rmse_valid)\n    print(\"\")\n    print(\"R2 on Training set without error cross validation :\", lasso.score(X_train, y_train))\n    print(\"R2 on Validation set without error cross validation :\", r2_valid)\n    print(\"\")\n    print(\"R2 on Training set with error cross validation :\", r2_train)\n#     print(\"R2 on Validation set with error cross validation :\", r2_valid)\n\n    y_train_las = np.ravel(lasso.predict(X_train))\n    y_valid_las = np.ravel(lasso.predict(X_valid))\n\n    # Plot residuals\n    plt.scatter(y_train_las, y_train_las - np.ravel(y_train.to_numpy()), c = \"blue\", marker = \"s\", label = \"Training data\")\n    plt.scatter(y_valid_las, y_valid_las - np.ravel(y_valid.to_numpy()), c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\n    plt.title(\"Linear regression with Lasso regularization\")\n    plt.xlabel(\"Predicted values\")\n    plt.ylabel(\"Residuals\")\n    plt.legend(loc = \"upper left\")\n    plt.hlines(y = 0, xmin = 9, xmax = 15, color = \"red\")\n    plt.show()\n\n    # Plot predictions\n    plt.scatter(y_train_las, np.ravel(y_train.to_numpy()), c = \"blue\", marker = \"s\", label = \"Training data\")\n    plt.scatter(y_valid_las, np.ravel(y_valid.to_numpy()), c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\n    plt.title(\"Linear regression with Lasso regularization\")\n    plt.xlabel(\"Predicted values\")\n    plt.ylabel(\"Real values\")\n    plt.legend(loc = \"upper left\")\n    plt.plot([9, 15], [9, 15], c = \"red\")\n    plt.show()\n    \n    # Plot important coefficients\n    print(\"\")\n    print(\"Lasso picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  \\\n          str(sum(coefs == 0)) + \" features\")\n    imp_coefs = pd.concat([coefs.sort_values().head(10),\n                         coefs.sort_values().tail(10)])\n    imp_coefs.plot(kind = \"barh\")\n    plt.title(\"Coefficients in the Lasso Model\")\n    plt.show()\n    return(rmse_train, rmse_valid, r2_train, r2_valid, y_valid_las)","885a0e12":"LR_Lasso = Linear_regression_Lasso(X_train, y_train, X_valid, y_valid)","198f9951":"def Linear_regression_ElasticNet(X_train, y_train, X_valid, y_valid):\n    # ElasticNet\n    elasticNet = ElasticNetCV(l1_ratio = [0.001, 0.005, 0.01, 0.05, 0.1, 0.3, 0.5, 0.7, 0.9],\n                              alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, \n                                        0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n                              max_iter = 50000, cv = 5)\n    elasticNet.fit(X_train, y_train)\n    alpha = elasticNet.alpha_\n    ratio = elasticNet.l1_ratio_\n    print(\"Best l1_ratio :\", ratio)\n    print(\"Best alpha :\", alpha )\n\n    print(\"Try again for more precision with l1_ratio centered around \" + str(ratio))\n    elasticNet = ElasticNetCV(l1_ratio = [ratio * .85, ratio * .9, ratio * .95, ratio, ratio * 1.05, ratio * 1.1, ratio * 1.15],\n                              alphas = [0.0001, 0.0003, 0.0006, 0.001, 0.003, 0.006, 0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6], \n                              max_iter = 50000, cv = 5)\n    elasticNet.fit(X_train, y_train)\n    if (elasticNet.l1_ratio_ > 1):\n        elasticNet.l1_ratio_ = 1    \n    alpha = elasticNet.alpha_\n    ratio = elasticNet.l1_ratio_\n    print(\"Best l1_ratio :\", ratio)\n    print(\"Best alpha :\", alpha )\n\n    print(\"Now try again for more precision on alpha, with l1_ratio fixed at \" + str(ratio) + \n          \" and alpha centered around \" + str(alpha))\n    elasticNet = ElasticNetCV(l1_ratio = ratio,\n                              alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, alpha * .9, \n                                        alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15, alpha * 1.25, alpha * 1.3, \n                                        alpha * 1.35, alpha * 1.4], \n                              max_iter = 50000, cv = 5)\n    elasticNet.fit(X_train, y_train)\n    if (elasticNet.l1_ratio_ > 1):\n        elasticNet.l1_ratio_ = 1    \n    alpha = elasticNet.alpha_\n    ratio = elasticNet.l1_ratio_\n    print(\"Best l1_ratio :\", ratio)\n    print(\"Best alpha :\", alpha )\n    print(\"\")\n    \n    elasticNet2 = ElasticNet(l1_ratio = ratio, alpha = alpha)\n    \n    rmse_train = rmse_cv_train(elasticNet2, X_train, y_train).mean()\n    rmse_valid = np.sqrt(mean_squared_error(y_valid, elasticNet.predict(X_valid)))\n    \n    r2_train = cross_val_score(elasticNet2, X_train, y_train, cv=5).mean()\n    r2_valid = elasticNet.score(X_valid, y_valid)\n  \n    # Results from the fit model\n    print(\"RMSE on Training set without error cross validation :\", np.sqrt(mean_squared_error(y_train, elasticNet.predict(X_train))))\n    print(\"RMSE on Validation set without error cross validation :\", rmse_valid)\n    print(\"\")\n    print(\"RMSE on Training set with error cross validation :\", rmse_train)\n#     print(\"RMSE on Validation set with error cross validation :\", rmse_valid)\n    print(\"\")\n    print(\"R2 on Training set without error cross validation :\", elasticNet.score(X_train, y_train))\n    print(\"R2 on Validation set without error cross validation :\", r2_valid)\n    print(\"\")\n    print(\"R2 on Training set with error cross validation :\", r2_train)\n#     print(\"R2 on Validation set with error cross validation :\", r2_valid)\n    \n    y_train_ela = np.ravel(elasticNet.predict(X_train))\n    y_valid_ela = np.ravel(elasticNet.predict(X_valid))\n\n    # Plot residuals\n    plt.scatter(y_train_ela, y_train_ela - np.ravel(y_train.to_numpy()), c = \"blue\", marker = \"s\", label = \"Training data\")\n    plt.scatter(y_valid_ela, y_valid_ela - np.ravel(y_valid.to_numpy()), c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\n    plt.title(\"Linear regression with ElasticNet regularization\")\n    plt.xlabel(\"Predicted values\")\n    plt.ylabel(\"Residuals\")\n    plt.legend(loc = \"upper left\")\n    plt.hlines(y = 0, xmin = 9, xmax = 15, color = \"red\")\n    plt.show()\n\n    # Plot predictions\n    plt.scatter(np.ravel(y_train.to_numpy()), y_train_ela, c = \"blue\", marker = \"s\", label = \"Training data\")\n    plt.scatter(np.ravel(y_valid.to_numpy()), y_valid_ela, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\n    plt.title(\"Linear regression with ElasticNet regularization\")\n    plt.xlabel(\"Predicted values\")\n    plt.ylabel(\"Real values\")\n    plt.legend(loc = \"upper left\")\n    plt.plot([9, 15], [9, 15], c = \"red\")\n    plt.show()\n\n    # Plot important coefficients\n    coefs = pd.Series(np.ravel(elasticNet.coef_), index = X_train.columns)\n    print(\"ElasticNet picked \" + str(sum(coefs != 0)) + \" features and eliminated the other \" +  str(sum(coefs == 0)) + \" features\")\n    imp_coefs = pd.concat([coefs.sort_values().head(10),\n                         coefs.sort_values().tail(10)])\n    imp_coefs.plot(kind = \"barh\")\n    plt.title(\"Coefficients in the ElasticNet Model\")\n    plt.show()\n    \n    return(rmse_train, rmse_valid, r2_train, r2_valid, y_valid_ela)","5d6b3c54":"LR_ElasticNet = Linear_regression_ElasticNet(X_train, y_train, X_valid, y_valid)","492578bc":"avg_valid_pred = np.mean([LR_Lasso[4],LR_ElasticNet[4]], axis = 0)\navg_valid_pred.shape","ed38f0e7":"def rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","63ebb84c":"rmsle(y_valid, avg_valid_pred)","2191170c":"# # Ridge\n# Basic_LR_Ridge = Linear_regression_Ridge(X_train2, y_train2, X_valid2, y_valid2)","1ca0b83b":"# # Lasso\n# Basic_LR_Lasso = Linear_regression_Lasso(X_train2, y_train2, X_valid2, y_valid2)","966fba20":"# # ElasticNet\n# Basic_LR_ElasticNet = Linear_regression_ElasticNet(X_train2, y_train2, X_valid2, y_valid2)","4e615c7a":"res = [LR_no_reg[0:4], LR_Ridge[0:4], LR_Lasso[0:4], LR_ElasticNet[0:4]] #, Basic_LR_Ridge, Basic_LR_Lasso, Basic_LR_ElasticNet]\nres2 = pd.DataFrame(res, columns =['rmse_train', 'rmse_valid', 'r2_train', 'r2_valid'])\nres2['Data_Model'] = list(['LR_no_reg', 'LR_Ridge', 'LR_Lasso', 'LR_ElasticNet']) #, 'Basic_LR_Ridge', 'Basic_LR_Lasso', 'Basic_LR_ElasticNet'])\ncols = ['Data_Model', 'rmse_train', 'r2_train', 'rmse_valid','r2_valid']\nres2 = res2[cols]\nres2","39fcfdb8":"res2 = res2.drop(index = 0)\nres2 = res2.sort_values(by=['rmse_valid'], ascending=True)\nres2","55be154a":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=13).get_n_splits(X_train.values)\n    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","00c34543":"lasso = Lasso(alpha =0.0008)","d81e3d83":"ENet = ElasticNet(alpha= 0.01, l1_ratio= 0.0475)","e7555e0d":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)","f39a2ae0":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)","15987086":"model_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)","2e075b20":"model_lgb = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)","b106cfa1":"score = rmsle_cv(lasso)\nprint(\"\\nLasso training score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","a06b4b3e":"score = rmsle_cv(ENet)\nprint(\"ElasticNet training score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","b42877e7":"score = rmsle_cv(KRR)\nprint(\"Kernel Ridge training score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","3a171fe3":"score = rmsle_cv(GBoost)\nprint(\"Gradient Boosting training score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","0c4a2ee0":"score = rmsle_cv(model_xgb)\nprint(\"Xgboost training score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","0bbfdceb":"score = rmsle_cv(model_lgb)\nprint(\"LGBM training score: {:.4f} ({:.4f})\\n\" .format(score.mean(), score.std()))","181c07de":"class AveragingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, models):\n        self.models = models\n        \n    # we define clones of the original models to fit the data in\n    def fit(self, X, y):\n        self.models_ = [clone(x) for x in self.models]\n        \n        # Train cloned base models\n        for model in self.models_:\n            model.fit(X, y)\n\n        return self\n    \n    #Now we do the predictions for cloned models and average them\n    def predict(self, X):\n        predictions = np.column_stack([\n            model.predict(X) for model in self.models_\n        ])\n        return np.mean(predictions, axis=1)","65306637":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\nscore = rmsle_cv(averaged_models)\nprint(\" Averaged base models training score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","d4a692c4":"averaged_models = AveragingModels(models = (ENet, GBoost, KRR, lasso))\naveraged_models.fit(X_train, y_train)\naveraged_valid_pred = averaged_models.predict(X_valid)","c7817195":"print(\" Averaged base models validation score: {:.4f}\\n\".format(np.sqrt(mean_squared_error(y_valid, averaged_valid_pred))))","13b8ed0b":"class StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)","3d4795f1":"# stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n#                                                  meta_model = lasso)\n\n# score = rmsle_cv(stacked_averaged_models)\n# print(\"Stacking Averaged models score: {:.4f} ({:.4f})\".format(score.mean(), score.std()))","df2618de":"# def rmsle(y, y_pred):\n#     return np.sqrt(mean_squared_error(y, y_pred))","38b69e50":"# stacked_averaged_models.fit(X_train, y_train)\n# stacked_train_pred = stacked_averaged_models.predict(X_train.values)\n# stacked_pred = np.expm1(stacked_averaged_models.predict(test.values))\n# print(rmsle(y_train, stacked_train_pred))","71af1fd2":"# model_xgb.fit(X_train, y_train)\n# xgb_train_pred = model_xgb.predict(X_train)\n# xgb_pred = np.expm1(model_xgb.predict(df_test2.values))\n# print(rmsle(y_train, xgb_train_pred))","47bdfa4b":"# model_lgb.fit(X_train, y_train)\n# lgb_train_pred = model_lgb.predict(train)\n# lgb_pred = np.expm1(model_lgb.predict(df_test2.values))\n# print(rmsle(y_train, lgb_train_pred))","537a10d6":"# '''RMSE on the entire Train data when averaging'''\n\n# print('RMSLE score on train data:')\n# print(rmsle(y_train,stacked_train_pred*0.70 +\n#                xgb_train_pred*0.15 + lgb_train_pred*0.15 ))","09d745df":"# ensemble = stacked_pred*0.70 + xgb_pred*0.15 + lgb_pred*0.15","6d4c6551":"#### 'SalePrice' correlation matrix (heatmap style)","6499636e":"#### Correlation matrix (heatmap style)","e0e374dc":"Ok, now we are dealing with the big boss. What do we have here?\n\n* Something that, in general, presents skewness.\n* A significant number of observations with value zero (houses without basement).\n* A big problem because the value zero doesn't allow us to do log transformations.\n\nTo apply a log transformation here, we'll create a variable that can get the effect of having or not having basement (binary variable). Then, we'll do a log transformation to all the non-zero observations, ignoring those with value zero. This way we can transform data, without losing the effect of having or not basement.\n\nI'm not sure if this approach is correct. It just seemed right to me. That's what I call 'high risk engineering'.","e25a7531":"We see some very interesting results above for both R2 and RMSE. When we look at results without <b>training error<\/b> cross validation, we see that they are pretty reasonable numbers. For example, R2 is 0.88 without training error validation.\n\nBut, the moment we look at results which have error validation (*taken a mean of all errors after cv*) we notice the results look very bad. This means the model is overfitted. It fits very well to the training data, but doesn't do well with validation dataset. We need regularization to avoid overfitting","89df4af6":"#### Linear Regression without Regularization\n\nSuch a model would fit very well with train data, but will be very bad on validation dataset. The model is overfitted","6a32a483":"#### Loading Packages","1e87b093":"* *Deviate from the normal distribution*\n* *Have appreciable positive skewness*\n* *Show peakedness*","ff874d74":"#### Linear Regression with ElasticNet regularization (L1 and L2 penalty)\n\nElasticNet is a compromise between Ridge and Lasso regression. It has a L1 penalty to generate sparsity and a L2 penalty to overcome some of the limitations of Lasso, such as the number of variables (Lasso can't select more features than it has observations, but it's not the case here anyway).","bfae09ce":"#### Stacking Averaged models Score\n\nTo make the two approaches comparable (by using the same number of models) , we just average Enet KRR and Gboost, then we add lasso as meta-model.","6355d57a":"#### Relationship of SalesPrice with numerical variables","c9a50cba":"## Regression Models","83c7e863":"#### Linear Regression with Lasso regularization (L1 penalty)","fd172e1a":"### Comparing results across models and datasets","5482a0a6":"So far we have done some basic data cleaning by handling missing values. We can keep a copy of this database and fit regression models to this dataset. The idea is to compare the extra benefit from using additional data transformations done below","e2e575cd":"Final Training and Prediction\nStackedRegressor:","1f588c98":"Inspired by Alexandru Papiu's script : https:\/\/www.kaggle.com\/apapiu\/house-prices-advanced-regression-techniques\/regularized-linear-models\n\nAs a general rule of thumb, a skewness with an absolute value > 0.5 is considered at least moderately skewed","86a535d3":"The point here is to test 'SalePrice' in a very lean way. We'll do this paying attention to:\n\n* <b>Histogram<\/b> - Kurtosis and skewness.\n* <b>Normal probability plot<\/b> - Data distribution should closely follow the diagonal that represents the normal distribution.","2e509865":"#### Relationship of SalesPrice with categorical variables","045d3bc4":"#### Log transform of the skewed numerical features (other than the ones that have already been transformed) to lessen impact of outliers","bf00c9be":"#### Bivariate analysis","7e1810ec":"Then we will create new features, in 3 ways : \n\n 1. Simplifications of existing features\n 2. Combinations of existing features\n 3. Polynomials on the top 10 existing features (Not doing this in the current model, because we need meaningful variables)","ccdda9da":"We can feel tempted to eliminate some observations (e.g. TotalBsmtSF > 3000) but I suppose it's not worth it. We can live with that, so we'll not do anything.","2f0229fd":"*Some numerical features are actually really categories. So converting them to categorical variables*","89f290b6":"#### Univariate analysis\n\nThe primary concern here is to establish a threshold that defines an observation as an outlier. To do so, we'll standardize the data. In this context, data standardization means converting data values to have mean of 0 and a standard deviation of 1.","e2461a39":"### Stacking models\nSimplest Stacking approach : <b>Averaging base models<\/b>\n\nWe begin with this simple approach of averaging base models. We build a new class to extend scikit-learn with our model and also to laverage encapsulation and code reuse (inheritance)\n\n<b>Averaged base models class<\/b>","9f301b44":"### Handling Missing Data\n\nImportant questions when thinking about missing data:\n\n* How prevalent is the missing data?\n* Is missing data random or does it have a pattern?\n\nThe answer to these questions is important for practical reasons because missing data can imply a reduction of the sample size. This can prevent us from proceeding with the analysis. Moreover, from a substantive perspective, we need to ensure that the missing data process is not biased and hidding an inconvenient truth.","67f8b954":"#### Analyzing 'SalePrice' (*the dependent variable*) in the training dataset","f9ca00be":"#### 3. Polynomials on the top 10 existing features","4de71e98":"*'SalePrice' has a positive realtionship with 'OverallQaul'*","4a00e576":"*To handle the missing values, we will drop all columns where the missing data is more than 15% of the records. For the other columns, we can fill it in with mean\/median\/mode value. We will also drop rows where data is missing in <= 5 records in the <b>Train data ONLY<\/b>. We should not drop records in the test dataset. We can fill the NaNs with 0 or a relvant value*","02a03184":"#### Taking a mean of ElasticNet and Lasso","295c2564":"We can say that, in general, 'SalePrice' exhibit equal levels of variance across the range of 'TotalBsmtSF'. Cool!","b45731a7":"Done! Let's check what's going on with 'GrLivArea'.","7a2c1813":"Based on the analysis above, these are the variables most correlated with 'SalePrice'. My thoughts on this:\n\n* 'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'. Check!\n* 'GarageCars' and 'GarageArea' are also some of the most strongly correlated variables. However, as we discussed in the last sub-point, the number of cars that fit into the garage is a consequence of the garage area. 'GarageCars' and 'GarageArea' are like twin brothers. You'll never be able to distinguish them. Therefore, we just need one of these variables in our analysis (we can keep 'GarageCars' since its correlation with 'SalePrice' is higher).\n* 'TotalBsmtSF' and '1stFloor' also seem to be twin brothers. We can keep 'TotalBsmtSF' just to say that our first guess was right (re-read 'So... What can we expect?').\n* 'FullBath'?? Really? \n* 'TotRmsAbvGrd' and 'GrLivArea', twin brothers again. Is this dataset from Chernobyl?\n* Ah... 'YearBuilt'... It seems that 'YearBuilt' is slightly correlated with 'SalePrice'. Honestly, it scares me to think about 'YearBuilt' because I start feeling that we should do a little bit of time-series analysis to get this right\n\nLet's proceed to the scatter plots.","7add6129":"### Transforming important columns to fit normal distribution","f0c408b3":"### Less simple Stacking : Adding a Meta-model\n\nIn this approach, we add a meta-model on averaged base models and use the out-of-folds predictions of these base models to train our meta-model.\n\nThe procedure, for the training part, may be described as follows:\n\nSplit the total training set into two disjoint sets (here train and .holdout )\n\nTrain several base models on the first part (train)\n\nTest these base models on the second part (holdout)\n\nUse the predictions from 3) (called out-of-folds predictions) as the inputs, and the correct responses (target variable) as the outputs to train a higher level learner called meta-model.\n\nThe first three steps are done iteratively . If we take for example a 5-fold stacking , we first split the training data into 5 folds. Then we will do 5 iterations. In each iteration, we train every base model on 4 folds and predict on the remaining fold (holdout fold).\n\nSo, we will be sure, after 5 iterations , that the entire data is used to get out-of-folds predictions that we will then use as new feature to train our meta-model in the step 4.\n\nFor the prediction part , We average the predictions of all base models on the test data and used them as meta-features on which, the final prediction is done with the meta-model.","7aa00c68":"*Encode some categorical features as ordered numbers when there is information in the order*","95ef75f9":"#### In the search for normality","fa8ff1bd":"#### Splitting train dataset into 'model train dataset' and 'model validation dataset'","8530b72e":"### Dealing with Outliars in the train dataset\n\nOutliers is also something that we should be aware of. Why? Because outliers can markedly affect our models and can be a valuable source of information, providing us insights about specific behaviours.\n\nOutliers is a complex subject and it deserves more attention. Here, we'll just do a quick analysis through the standard deviation of 'SalePrice' and a set of scatter plots.","b2267c74":"#### 1. Simplifications of existing features","a3c796b5":"#### Using the basic datasets (train and validation)\n\nTo understand the role \"feature engineering\" plays in improving results, we are going to use the three regularized models with the basic datasets and compare results","5a6dba0a":"#### Scatter plots between 'SalePrice' and correlated variables","350a61db":"### Understanding the data and handling missing data and outliers\n#### Quick EDA using Pandas Profiling","b7ee354c":"* Low range values are similar and not too far from 0.\n* High range values are far from 0 and the 7.something values are really out of range.\n\nFor now, we'll not consider any of these values as an outlier but we should be careful with those two 7.something values.","034165c1":"Although we already know some of the main figures, this mega scatter plot gives us a reasonable idea about variables relationships.\n\nOne of the figures we may find interesting is the one between 'TotalBsmtSF' and 'GrLiveArea'. In this figure we can see the dots drawing a linear line, which almost acts like a border. It totally makes sense that the majority of the dots stay below that line. Basement areas can be equal to the above ground living area, but it is not expected a basement area bigger than the above ground living area (unless you're trying to buy a bunker).\n\nThe plot concerning 'SalePrice' and 'YearBuilt' can also make us think. In the bottom of the 'dots cloud', we see what almost appears to be a shy exponential function (be creative). We can also see this same tendency in the upper limit of the 'dots cloud' (be even more creative). Also, notice how the set of dots regarding the last years tend to stay above this limit (I just wanted to say that prices are increasing faster now).\n\nOk, enough of Rorschach test for now. Let's move forward to what's missing: missing data!","bbc00800":"Older versions of this scatter plot (previous to log transformations), had a conic shape (go back and check 'Scatter plots between 'SalePrice' and correlated variables (move like Jagger style)'). As you can see, the current scatter plot doesn't have a conic shape anymore. That's the power of normality! Just by ensuring normality in some variables, we solved the homoscedasticity problem.\n\nNow let's check 'SalePrice' with 'TotalBsmtSF'.","0d732f75":"In my opinion, this heatmap is the best way to get a quick overview of our 'plasma soup' and its relationships. (Thank you @seaborn!)\n\nAt first sight, there are two red colored squares that get my attention. The first one refers to the 'TotalBsmtSF' and '1stFlrSF' variables, and the second one refers to the 'Garage*X*' variables. Both cases show how significant the correlation is between these variables. Actually, this correlation is so strong that it can indicate a situation of multicollinearity. If we think about these variables, we can conclude that they give almost the same information so multicollinearity really occurs. Heatmaps are great to detect this kind of situations and in problems dominated by feature selection, like ours, they are an essential tool.\n\nAnother thing that got my attention was the 'SalePrice' correlations. We can see our well-known 'GrLivArea', 'TotalBsmtSF', and 'OverallQual' saying a big 'Hi!', but we can also see many other variables that should be taken into account. That's what we will do next.","269109e7":"#### 2. Combinations of existing features","3b2fca89":"<b>Ensembling StackedRegressor, XGBoost and LightGBM<\/b>\n\nWe add XGBoost and LightGBM to the StackedRegressor defined previously.\n\nWe first define a rmsle evaluation function","ea18f599":"#### Create dummy features for categorical values via one-hot encoding","4c71f457":"#### Reading in data","44033e1d":"#### Joining the numerical and categorical features back into 1 dataframe","2b6fcafe":"#### Standardizing Numerical Features","8942e697":"The best approach to test homoscedasticity for two metric variables is graphically. Departures from an equal dispersion are shown by such shapes as cones (small dispersion at one side of the graph, large dispersion at the opposite side) or diamonds (a large number of points at the center of the distribution).\n\nStarting by 'SalePrice' and 'GrLivArea'...","eaa3f32b":"#### In summary\n\nWe can conclude that:\n\n* 'GrLivArea' and 'TotalBsmtSF' seem to be linearly related with 'SalePrice'. Both relationships are positive, which means that as one variable increases, the other also increases. In the case of 'TotalBsmtSF', we can see that the slope of the linear relationship is particularly high.\n* 'OverallQual' and 'YearBuilt' also seem to be related with 'SalePrice'. The relationship seems to be stronger in the case of 'OverallQual', where the box plot shows how sales prices increase with the overall quality.\n\nWe just analysed four variables, but there are many other that we should analyse. The trick here seems to be the choice of the right features (feature selection) and not the definition of complex relationships between them (feature engineering).\n\nThat said, let's separate the wheat from the chaff.","0c4d930a":"#### Combining Train and Test data to handle missing values","6e671b88":"#### Linear Regression with Ridge regularization (L2 penalty)","c0439337":"Ok, 'SalePrice' is not normal. It shows 'peakedness', positive skewness and does not follow the diagonal line.\n\nBut everything's not lost. A simple data transformation can solve the problem. This is one of the awesome things you can learn in statistical books: in case of positive skewness, log transformations usually works well.","6d926e09":"## Feature Engineering and Selection\n\nxxxxxxxx","ab579376":"#### Gradient Boosting Regression :\nWith huber loss that makes it robust to outliers","a6424e8e":"### Additional feature transformations","cf7e1e2b":"We do standardizing of numerical features after train-test split to avoid leakage. If we do standardization with both train and test dataset in one dataframe, then we are influencing the train data using test data; hence leakage","0754b695":"#### Averaged base models score\n\nWe just average four models here ENet, GBoost, KRR and lasso. Of course we could easily add more models in the mix.","28182e68":"Not doing log transformation of all numerical features with skewness >0.5 because, we have some flags in this as well. We don't want to do log transformation of them. Ideally, should apply log transformations on a case by case basis","c236e83c":"#### Stacking averaged Models Class","5d609a70":"#### Find most important features relative to target","4b66a90a":"### Create new features","d7d2f67e":"#### Separating out Test and Train datasets","0728a92c":"<b>Citation<\/b>:\n\nKaggle is a great place to learn application of ML concepts. To enhance my understanding of regression concepts, I have gone through some higly rated notebooks on Kaggle. This notebook has used codes from the following notebooks-\n1. https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python\n2. https:\/\/www.kaggle.com\/juliencs\/a-study-on-regression-applied-to-the-ames-dataset\n3. https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard\n\nThank you for sharing your work on Kaggle!","5df7ccb9":"*It seems that 'SalePrice' and 'GrLivArea' show a <b>linear relationship.<\/b>*","80c8c31b":"Who is 'SalePrice'?\n\nThe answer to this question lies in testing for the assumptions underlying the statistical bases for multivariate analysis. We already did some data cleaning and discovered a lot about 'SalePrice'. Now it's time to go deep and understand how 'SalePrice' complies with the statistical assumptions that enables us to apply multivariate techniques.\n\nAccording to [Hair et al. (2013)](https:\/\/amzn.to\/2uC3j9p), four assumptions should be tested:\n\n* <b>Normality<\/b> - When we talk about normality what we mean is that the data should look like a normal distribution. This is important because several statistic tests rely  on this (e.g. t-statistics). In this exercise we'll just check univariate normality for 'SalePrice' (which is a limited approach). Remember that univariate normality doesn't ensure multivariate normality (which is what we would like to have), but it helps. Another detail to take into account is that in big samples (>200 observations) normality is not such an issue. However, if we solve normality, we avoid a lot of other problems (e.g. heteroscedacity) so that's the main reason why we are doing this analysis.\n\n* <b>Homoscedasticity<\/b> - I just hope I wrote it right. Homoscedasticity refers to the 'assumption that dependent variable(s) exhibit equal levels of variance across the range of predictor variable(s)' [(Hair et al., 2013)](https:\/\/amzn.to\/2uC3j9p). Homoscedasticity is desirable because we want the error term to be the same across all values of the independent variables.\n\n* <b>Linearity<\/b>- The most common way to assess linearity is to examine scatter plots and search for linear patterns. If patterns are not linear, it would be worthwhile to explore data transformations. However, we'll not get into this because most of the scatter plots we've seen appear to have linear relationships.\n\n* <b>Absence of correlated errors<\/b> - Correlated errors, like the definition suggests, happen when one error is correlated to another. For instance, if one positive error makes a negative error systematically, it means that there's a relationship between these variables. This occurs often in time series, where some patterns are time related. We'll also not get into this. However, if you detect something, try to add a variable that can explain the effect you're getting. That's the most common solution for correlated errors.\n","3e2096ff":"#### In the search for writing 'homoscedasticity' right at the first attempt","cc61190a":"#### Separating numerical features (excluding 'SalePrice') and categorical features","2cd0d0f9":"What has been revealed:\n\n* The two values with bigger 'GrLivArea' seem strange and they are not following the crowd. We can speculate why this is happening. Maybe they refer to agricultural area and that could explain the low price. I'm not sure about this but I'm quite confident that these two points are not representative of the typical case. Therefore, we'll define them as outliers and delete them.\n* The two observations in the top of the plot are those 7.something observations that we said we should be careful about. They look like two special cases, however they seem to be following the trend. For that reason, we will keep them."}}