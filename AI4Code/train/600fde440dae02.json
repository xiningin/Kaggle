{"cell_type":{"5fbd14af":"code","12ba86e2":"code","da416c66":"code","d0e6a875":"code","04d325aa":"code","32e37dcf":"code","586550ed":"code","77e0164c":"code","b3f6b9f7":"code","d72db6bc":"code","17b7372f":"code","64712c6d":"code","60563f2a":"code","b7d8958b":"code","c61f0891":"code","052c14fb":"code","e2a1f6bd":"code","32f3e5dd":"code","61f62093":"code","b6be0a28":"markdown","b329b71e":"markdown","ff9545a4":"markdown","0ce9e91c":"markdown","345d85b7":"markdown","90eda40e":"markdown","722b880c":"markdown","87316924":"markdown","120fd6cc":"markdown"},"source":{"5fbd14af":"import numpy as np\nimport pandas as pd\n\nimport os","12ba86e2":"fake = pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/Fake.csv\")\nfake.dropna(inplace=True)\nfake.head()","da416c66":"true = pd.read_csv(\"\/kaggle\/input\/fake-and-real-news-dataset\/True.csv\")\ntrue.dropna(inplace=True)\ntrue.head()","d0e6a875":"# Combine DataFrames\n\nfake['class'] = np.zeros(fake.shape[0])\ntrue['class'] = np.ones(true.shape[0])\n\nnews = pd.concat([fake, true])\nnews","04d325aa":"news['title_text'] = news['title'] + ' ' + news['text']\nnews.drop(['title', 'text'], axis=1, inplace=True)","32e37dcf":"from sklearn.feature_extraction.text import CountVectorizer\nfrom nltk.tokenize import RegexpTokenizer\n\ntoken = RegexpTokenizer(r'[a-zA-Z]+')\n\ncv = CountVectorizer(\n    lowercase=True, \n    stop_words='english', \n    ngram_range = (1,2), \n    tokenizer = token.tokenize,\n    max_features=5000\n)\n\ntext_counts = cv.fit_transform(news['title_text'])","586550ed":"text_counts","77e0164c":"import plotly.graph_objects as go\n\nfig = go.Figure(\n    data=[\n        go.Pie(\n            labels=['Fake', 'True'], \n            values=list(news['class'].value_counts()), \n            hole=.3\n        )\n    ]\n)\n\nfig.update_layout(title=\"Fake and Real News Ratio\")\n\nfig.show()","b3f6b9f7":"X = text_counts\ny = news['class']","d72db6bc":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","17b7372f":"from sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(class_weight='balanced', penalty='l2', solver='liblinear', max_iter=1000)\n\nmodel.fit(X_train, y_train)","64712c6d":"from sklearn.metrics import accuracy_score, f1_score, log_loss","60563f2a":"y_pred = model.predict(X_train)\n\nacc = accuracy_score(y_train, y_pred)\nf1 = f1_score(y_train, y_pred)\nll = log_loss(y_train, y_pred)\n\nprint(f\"(train) accuracy_score: {acc}\")\nprint(f\"(train) f1_score: {f1}\")\nprint(f\"(train) log_loss: {ll}\")","b7d8958b":"y_pred = model.predict(X_test)\n\nacc = accuracy_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\nll = log_loss(y_test, y_pred)\n\nprint(f\"(test) accuracy_score: {acc}\")\nprint(f\"(test) f1_score: {f1}\")\nprint(f\"(test) log_loss: {ll}\")","c61f0891":"import seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import plot_confusion_matrix\n\ndisp = plot_confusion_matrix(model, X, y, display_labels=['Fake', 'True'], normalize=None)\n\nplt.grid(False)\n\nplt.show()","052c14fb":"from sklearn.metrics import classification_report\n\ny_pred = model.predict(X)\n\nprint(classification_report(y, y_pred, target_names=['Fake', 'True'], zero_division=0))","e2a1f6bd":"text_counts","32f3e5dd":"top_features = {}\n\nfor col, coef in zip(cv.get_feature_names(), model.coef_[0]):\n    if np.abs(coef) > 0.3:\n        top_features[col] = int(np.abs(coef)*100)","61f62093":"import numpy as np\nimport matplotlib.pyplot as plt\nfrom wordcloud import WordCloud\n\nx, y = np.ogrid[:300, :300]\n\nmask = (x - 150) ** 2 + (y - 150) ** 2 > 130 ** 2\nmask = 255 * mask.astype(int)\n\n\nwc = WordCloud(background_color=\"white\", repeat=True, mask=mask, scale=2)\nwc.generate_from_frequencies(top_features)\n\nplt.figure(figsize=(15, 15))\nplt.axis(\"off\")\nplt.imshow(wc, interpolation=\"bilinear\")\nplt.show()","b6be0a28":"# Classification Report","b329b71e":"# Feature Selection","ff9545a4":"# Confusion Matrix","0ce9e91c":"## Bag of Words","345d85b7":"## Relevant Features","90eda40e":"# Feature Engineering","722b880c":"## Visualizations","87316924":"# Load Data","120fd6cc":"# Predictions"}}