{"cell_type":{"43a00558":"code","b2c2d8c9":"code","855d5337":"code","fa7c2f12":"code","0c21c4fe":"code","08c34ae7":"code","72a798e6":"code","b95c2aa0":"code","8debc93f":"code","7c4fc495":"code","3f0e6689":"code","ef861ebc":"code","a2c4766f":"code","f85bf3d2":"code","341932d3":"code","aa2ec96b":"code","5c707364":"code","855561f7":"code","0143e2a5":"code","004b6656":"code","d0825928":"code","c04232aa":"code","1ab5a9b3":"code","e10992c8":"code","2e2b5bef":"code","dd0af10c":"code","76d57df5":"code","de3b81c9":"markdown","9edc7e9d":"markdown"},"source":{"43a00558":"# Need to install autokeras\n!pip install autokeras\n!pip install git+https:\/\/github.com\/keras-team\/keras-tuner.git@1.0.2rc1","b2c2d8c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","855d5337":"train_data = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')","fa7c2f12":"print(\"Shape of training data\",train_data.shape)\nprint(\"Shape of testing data\",test_data.shape)","0c21c4fe":"feature_cols = ['MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n       'SaleCondition']\n\ntarget_cols = ['SalePrice']\n\nX_train = train_data[feature_cols]\ny_train = train_data[target_cols]\nX_test = test_data[feature_cols]\n# X_all = pd.concat([X_train,X_test],axis=1)","08c34ae7":"# Get list of categorical variables\ns = (X_train.dtypes == 'object')\nobject_cols = list(s[s].index)\n\nprint(\"Categorical variables:\")\nprint(object_cols)\n\ns = (X_train.dtypes != 'object')\nnumerical_cols = list(s[s].index)\n\nprint(\"Non-Categorical variables:\")\nprint(numerical_cols)","72a798e6":"# Get names of columns with missing values\ncols_with_missing = [col for col in train_data.columns\n                     if train_data[col].isnull().any()]\n# cols_with_missing\n\ntrain_data_categorical_cols_imputed = train_data[object_cols].apply(lambda x: x.fillna(x.value_counts().index[0]))\ntest_data_categorical_cols_imputed = test_data[object_cols].apply(lambda x: x.fillna(x.value_counts().index[0]))\n\ntrain_data_non_categorical_cols_imputed =train_data[numerical_cols].fillna(train_data[numerical_cols].mean())\ntest_data_non_categorical_cols_imputed =test_data[numerical_cols].fillna(test_data[numerical_cols].mean())\nX_train = pd.concat([train_data_categorical_cols_imputed, train_data_non_categorical_cols_imputed], axis=1)\nX_test = pd.concat([test_data_categorical_cols_imputed, test_data_non_categorical_cols_imputed], axis=1)","b95c2aa0":"# This step for using prediction in xgboost\nprint(X_train.shape)\nprint(X_test.shape)\nframes = [X_train, X_test]\n\nX_all = pd.concat(frames)\nprint(X_all.shape)\nprint(y_train.shape)","8debc93f":"# train_df_categorical_cols_imputed = train_df[categorical_columns].apply(lambda x: x.fillna(x.value_counts().index[0]))","7c4fc495":"def display_scores(scores):\n    print(\"Scores: \",scores)\n    print(\"Mean:\",scores.mean())\n    print(\"Standard deviation:\",scores.std())","3f0e6689":"from sklearn.preprocessing import OneHotEncoder\n\n# Apply one-hot encoder to each column with categorical data\nOH_encoder = OneHotEncoder(handle_unknown='ignore', sparse=False)\nOH_cols_train = pd.DataFrame(OH_encoder.fit_transform(X_train[object_cols]))\nOH_cols_test = pd.DataFrame(OH_encoder.fit_transform(X_test[object_cols]))\n# OH_cols_valid = pd.DataFrame(OH_encoder.transform(X_valid[object_cols]))\nOH_cols_X_all = pd.DataFrame(OH_encoder.fit_transform(X_all[object_cols]))\n# X_all\n\n# One-hot encoding removed index; put it back\nOH_cols_train.index = X_train.index\nOH_cols_test.index = X_test.index\nOH_cols_X_all.index = X_all.index\n\n\n# OH_cols_valid.index = X_valid.index\n\n# Remove categorical columns (will replace with one-hot encoding)\nnum_X_train = X_train.drop(object_cols, axis=1)\nnum_X_test = X_test.drop(object_cols, axis=1)\nnum_X_all = X_all.drop(object_cols, axis=1)\n# num_X_valid = X_valid.drop(object_cols, axis=1)\n\n# Add one-hot encoded columns to numerical features\nOH_X_train = pd.concat([num_X_train, OH_cols_train], axis=1)\nOH_X_test = pd.concat([num_X_test, OH_cols_test], axis=1)\nOH_X_all = pd.concat([num_X_all,OH_cols_X_all],axis=1)\n# OH_X_valid = pd.concat([num_X_valid, OH_cols_valid], axis=1)\n\n# print(\"MAE from Approach 3 (One-Hot Encoding):\") \n# print(score_dataset(OH_X_train, OH_X_valid, y_train, y_valid))","ef861ebc":"training_features = OH_X_train.copy()\ntraining_labels = y_train.copy()\ntest_features = OH_X_test.copy()\ntest_xgb = OH_X_all[1460:]\ntest_keras = OH_X_all[1460:]\ntest_bagging = OH_X_all[1460:]\nprint(test_keras.shape)\nprint(training_features.shape)\nprint(test_xgb.shape)\nprint(training_labels.shape)\n\n# (1460, 79)\n# (1459, 79)\n# (2919, 79)","a2c4766f":"from sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import balanced_accuracy_score,make_scorer,mean_squared_error\nrmse_accuracy = make_scorer(mean_squared_error,squared=False)","f85bf3d2":"from sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\n# from sklearn import neighbors\nimport xgboost as xgb\n\nlinr_reg = LinearRegression()\ntree_reg = DecisionTreeRegressor()\nrandom_forest_reg = RandomForestRegressor(max_features=2, max_leaf_nodes=5, n_estimators=100)\n\n# knn_reg = neighbors.KNeighborsRegressor()\n\n\nlinr_reg.fit(training_features,training_labels)\ntree_reg.fit(training_features,training_labels)\nrandom_forest_reg.fit(training_features,training_labels.values.ravel())\n# knn_reg.fit(training_features,training_labels)","341932d3":"from sklearn.model_selection import cross_val_score\nscores_lin_reg = cross_val_score(linr_reg,training_features,training_labels,scoring='neg_mean_squared_error',cv=10)\nlinr_rmse_scores = np.sqrt(-scores_lin_reg)\ndisplay_scores(linr_rmse_scores)","aa2ec96b":"scores_tree_reg = cross_val_score(tree_reg,training_features,training_labels,scoring='neg_mean_squared_error',cv=10)\ntree_rmse_scores = np.sqrt(-scores_tree_reg)\ndisplay_scores(tree_rmse_scores)","5c707364":"scores_random_forest_reg = cross_val_score(random_forest_reg,training_features,training_labels.values.ravel(),scoring='neg_mean_squared_error',cv=10)\nrandom_forest_rmse_scores = np.sqrt(-scores_random_forest_reg)\ndisplay_scores(random_forest_rmse_scores)","855561f7":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = [{'n_estimators':[30,100, 200],'max_features':[2,4,6,8],'max_leaf_nodes':[5,10]},\n             {'bootstrap':[False],'n_estimators':[3,10],'max_features':[2,3,4]}]\n\n# forest_model = random_forest_reg()\n\ngrid_search = GridSearchCV(random_forest_reg,param_grid,cv=5,\n                          scoring=rmse_accuracy,\n                          return_train_score=True)\n\ngrid_search.fit(training_features,training_labels.values.ravel())","0143e2a5":"grid_search.best_estimator_","004b6656":"grid_search.best_params_","d0825928":"from sklearn.ensemble import BaggingRegressor\nbag_reg = BaggingRegressor(DecisionTreeRegressor(),n_estimators=500,max_samples=100,bootstrap=True,\n                          n_jobs=1)\nbag_reg.fit(training_features,training_labels)\nprediction_bagging = bag_reg.predict(test_bagging)\npredictions = pd.DataFrame(prediction_bagging,columns=['SalePrice'])\n# predictions.head()\nfinal_submission = pd.concat([test_data['Id'],predictions['SalePrice']],axis=1)\nfinal_submission.to_csv('Bagging_decision_tree.csv',index=False)","c04232aa":"test_features.shape","1ab5a9b3":"training_features = OH_X_all[:1460]\ntest_features = OH_X_all[1460:]\ntraining_labels = y_train.copy()\nprint(training_features.shape)\nprint(training_labels.shape)\nprint(test_features.shape)","e10992c8":"import autokeras as ak\n# Initialize the structured data regressor.\nkeras_reg = ak.StructuredDataRegressor(\n    overwrite=True,\n    max_trials=3,\n\n) # It tries 10 different models.\n# Feed the structured data regressor with training data.\nkeras_reg.fit(\n    # The path to the train.csv file.\n    training_features,\n    # The name of the label column.\n   training_labels,\n    epochs=10)\n\n# # Evaluate the best model with testing data.\n# print(reg.evaluate(test_file_path, 'Price'))","2e2b5bef":"# Predict with the best model.\npredicted_keras = keras_reg.predict(test_features)\npredictions = pd.DataFrame(predicted_keras,columns=['SalePrice'])\n# predictions.head()\nfinal_submission = pd.concat([test_data['Id'],predictions['SalePrice']],axis=1)\nfinal_submission.to_csv('salesprice_keras.csv',index=False)","dd0af10c":"from sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nxgb_reg = xgb.XGBRegressor()\n# dtrain = xgb.DMatrix(training_features, label=training_labels)\n# train_test_split(training_features.as_matrix(), training_labels.as_matrix(), test_size=0.25)\nX_train , X_val, y_train, y_val = train_test_split(training_features.to_numpy(), training_labels.to_numpy(), test_size=0.25)\n# train_test_split(training_features,training_labels)\nxgb_reg.fit(X_train,y_train,eval_set=[(X_val,y_val)],early_stopping_rounds=5)\n\ntest_features = test_xgb.to_numpy()\nprediction_xgb=xgb_reg.predict(test_features)\n#now we pass the testing data to the trained algorithm\npredictions = pd.DataFrame(prediction_xgb,columns=['SalePrice'])\n# predictions.head()\nfinal_submission = pd.concat([test_data['Id'],predictions['SalePrice']],axis=1)\nfinal_submission.to_csv('Prediction_xgb.csv',index=False)","76d57df5":"from sklearn.ensemble import VotingRegressor\n\nvoting_reg = VotingRegressor(estimators=[('xgb_reg',xgb_reg),('rf',random_forest_reg),\n                                        ('bagging',bag_reg)]\n                             )\n\n#  ('keras',keras_reg),\n# print(type(voting_reg))\n\nvoting_reg.fit(training_features.to_numpy(),training_labels.to_numpy())\n\n# Predict with the best model.\npredicted_voting = voting_reg.predict(test_features)\npredictions = pd.DataFrame(predicted_voting,columns=['SalePrice'])\n# predictions.head()\nfinal_submission = pd.concat([test_data['Id'],predictions['SalePrice']],axis=1)\nfinal_submission.to_csv('salesprice_voting_reg.csv',index=False)\n\n","de3b81c9":"# Keras and Autokeras for the model","9edc7e9d":"# Bagging and Pasting with Decison Tree"}}