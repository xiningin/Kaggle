{"cell_type":{"b23f46f7":"code","58264268":"code","71054f2b":"code","978d6c4a":"code","e2a57202":"code","8420b60a":"code","b10b14f8":"code","a80087c5":"code","9143f6b2":"code","960510d8":"code","7b94e1f9":"code","5f2cc44d":"code","fbe4ffa1":"code","edf2140b":"code","30a00476":"code","3dc6c59d":"code","029288ca":"markdown","d54d332f":"markdown","ca324b53":"markdown","fd105d04":"markdown","347908b6":"markdown","e6295db2":"markdown","020697a9":"markdown","ad2cffa8":"markdown","74d42633":"markdown","0565292e":"markdown"},"source":{"b23f46f7":"!conda install -y gdown \nimport gdown ","58264268":"url = 'https:\/\/drive.google.com\/uc?id=1rdbF95HfP4K_lznMirByqvkYzEzaavg-'\noutput = 'input.zip'\ngdown.download(url, output)","71054f2b":"!unzip input.zip","978d6c4a":"!pip install imutils","e2a57202":"rm input.zip","8420b60a":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.model_selection import train_test_split\nfrom tensorflow.keras.layers import AveragePooling2D, Dense, Dropout, Flatten, Input\nfrom sklearn.preprocessing import LabelBinarizer\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import load_model\nfrom sklearn.metrics import classification_report\nfrom tensorflow.keras.applications import EfficientNetB0\nfrom imutils import paths\nfrom collections import deque\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport cv2\nimport os\nimport pickle","b10b14f8":"LABELS = set([\"Crown and Root Rot\", \"Healthy Wheat\", \"Leaf Rust\", \"Wheat Loose Smut\"])\nimagePaths = list(paths.list_images('.\/Large Wheat Disease Classification Dataset'))\ndata = []\nlabels = []\n# loop over the image paths\nfor imagePath in imagePaths:\n # extract the class label from the filename\n label = imagePath.split(os.path.sep)[-2]\n# if the label of the current image is not part of the labels\n # are interested in, then ignore the image\n if label not in LABELS:\n  continue\n# load the image, convert it to RGB channel ordering, and resize\n # it to be a fixed 224x224 pixels, ignoring aspect ratio\n image = cv2.imread(imagePath)\n image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n image = cv2.resize(image, (224, 224))\n# update the data and labels lists, respectively\n data.append(image)\n labels.append(label)","a80087c5":"# convert the data and labels to NumPy arrays\ndata = np.array(data)\nlabels = np.array(labels)\n# perform one-hot encoding on the labels\nlb = LabelBinarizer()\nlabels = lb.fit_transform(labels)\n# partition the data into training and testing splits using 75% of\n# the data for training and the remaining 25% for testing\n(trainX, testX, trainY, testY) = train_test_split(data, labels,\n test_size=0.25, stratify=labels, random_state=42)","9143f6b2":"# initialize the training data augmentation object\ntrainAug = ImageDataGenerator(\n rotation_range=30,\n zoom_range=0.15,\n width_shift_range=0.2,\n height_shift_range=0.2,\n shear_range=0.15,\n horizontal_flip=True,\n fill_mode=\"nearest\")\n# initialize the validation\/testing data augmentation object (which\n# we'll be adding mean subtraction to)\nvalAug = ImageDataGenerator()\n# define the ImageNet mean subtraction (in RGB order) and set the\n# the mean subtraction value for each of the data augmentation\n# objects\nmean = np.array([123.68, 116.779, 103.939], dtype=\"float32\")\ntrainAug.mean = mean\nvalAug.mean = mean\n","960510d8":"# load the EfficientNet-B0, ensuring the head FC layer sets are left ff\n\nheadmodel = EfficientNetB0(\n    include_top=False,\n    weights=\"imagenet\",\n    input_tensor=Input(shape=(224,224,3)),\n    classes=1000,\n    classifier_activation=\"relu\",\n)\n# construct the head of the model that will be placed on top of the\n# the base model\nmodel = headmodel.output\nmodel = AveragePooling2D(pool_size=(5, 5))(model)\nmodel = Flatten(name=\"flatten\")(model)\nmodel = Dense(512, activation=\"relu\")(model)\nmodel = Dropout(0.4)(model)\nmodel = Dense(len(lb.classes_), activation=\"softmax\")(model)\n# place the head FC model on top of the base model (this will become\n# the actual model we will train)\nmoodel = Model(inputs=headmodel.input, outputs=model)\n# loop over all layers in the base model and freeze them so they will\n# *not* be updated during the training process\nfor layer in headmodel.layers:\n    layer.trainable = False","7b94e1f9":"from keras.utils.vis_utils import plot_model\nplot_model(moodel, to_file='model_EffNetB0.png', show_shapes=True, show_layer_names=True)","5f2cc44d":"plot_model(moodel, to_file='model_Extended.png')","fbe4ffa1":"# compile our model (this needs to be done after our setting our\n# layers to being non-trainable)\nopt = Adam(learning_rate=1e-3)\nmoodel.compile(loss=\"categorical_crossentropy\", optimizer=opt,\n               metrics=[\"accuracy\"])\n# train the head of the network for a few epochs (all other layers\n# are frozen) -- this will allow the new FC layers to start to become\n# initialized with actual \"learned\" values versus pure random\nH = moodel.fit(\n    trainAug.flow(trainX, trainY, batch_size=64),\n    steps_per_epoch=len(trainX) \/\/ 64,\n    validation_data=valAug.flow(testX, testY),\n    validation_steps=len(testX) \/\/ 64,\n    epochs=20)","edf2140b":"H1 = moodel.fit(\n    trainAug.flow(trainX, trainY, batch_size=64),\n    steps_per_epoch=len(trainX) \/\/ 64,\n    validation_data=valAug.flow(testX, testY),\n    validation_steps=len(testX) \/\/ 64,\n    epochs=10)","30a00476":"# evaluate the network\npredictions = moodel.predict(testX, batch_size=64)\nprint(classification_report(testY.argmax(axis=1),\n                            predictions.argmax(axis=1), target_names=lb.classes_))\n# plot the training loss and accuracy\nN = 20\nplt.plot(np.arange(0, N), H.history['accuracy'], label=\"Training Accuracy\")\nplt.plot(np.arange(0, N), H.history['val_accuracy'], label=\"Test Accuracy\")\nplt.title('EfficientNetB0 Model Train vs Test Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(loc='lower right')\nplt.show()\nplt.savefig(\".\\Accuracy_Plot_EffNetB0.png\")\nplt.plot(H.history['loss'], label=\"Training Loss\")\nplt.plot(H.history['val_loss'], label=\"Test Loss\")\nplt.title('EfficientNetB0 Model Train vs Test Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(loc='upper right')\nplt.show()\nplt.savefig(\".\\Loss_Plot_EffNetB0.png\")","3dc6c59d":"N = 10\nplt.plot(np.arange(0, N), H1.history['accuracy'], label=\"Training Accuracy\")\nplt.plot(np.arange(0, N), H1.history['val_accuracy'], label=\"Test Accuracy\")\nplt.title('EfficientNetB0 Model Train vs Test Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Epoch')\nplt.legend(loc='lower right')\nplt.show()\nplt.savefig(\".\\Accuracy_Plot_EffNetB0.png\")\nplt.plot(H1.history['loss'], label=\"Training Loss\")\nplt.plot(H1.history['val_loss'], label=\"Test Loss\")\nplt.title('EfficientNetB0 Model Train vs Test Loss')\nplt.ylabel('Loss')\nplt.xlabel('Epoch')\nplt.legend(loc='upper right')\nplt.show()\nplt.savefig(\".\\Loss_Plot_EffNetB0.png\")","029288ca":"## Data Augmentation","d54d332f":"### From both the above images, it seems 26 epoch is the best suitable choice.. after 26, it starts overfitting","ca324b53":"## Data Preprocessing","fd105d04":"## Training Model via Transfer Learning","347908b6":"## Extending the Epoches","e6295db2":"## Libraries and Imports","020697a9":"## Model Definition","ad2cffa8":"# Wheat Disease Detection on Leaf Images using EfficientNetB0","74d42633":"## Data Labelling and Splitting the Data","0565292e":"## Import Data from Google Drive Folder"}}