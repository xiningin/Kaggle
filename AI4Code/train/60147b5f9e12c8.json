{"cell_type":{"57533589":"code","eeff9001":"code","4c2d3cfe":"code","a7a818ea":"markdown","9432904f":"markdown"},"source":{"57533589":"import os\nimport random\nimport warnings\n\nimport numpy as np\nimport pandas as pd\n\nfrom lightgbm import LGBMClassifier\n\nfrom mlxtend.classifier import StackingCVClassifier\n\nfrom sklearn.ensemble import AdaBoostClassifier, RandomForestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.tree import DecisionTreeClassifier\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nrandom_state = 1\nrandom.seed(random_state)\nnp.random.seed(random_state)\nos.environ['PYTHONHASHSEED'] = str(random_state)\n\n\nprint('> Loading data')\nX_train = pd.read_csv('\/kaggle\/input\/learn-together\/train.csv', index_col='Id')\nX_test = pd.read_csv('\/kaggle\/input\/learn-together\/test.csv', index_col='Id')\n\ny_train = X_train['Cover_Type'].copy()\nX_train = X_train.drop(['Cover_Type'], axis='columns')\n\n\nprint('> Adding and dropping features')\n\ndef add_features(X_):\n    X = X_.copy()\n\n    X['Hydro_Elevation_diff'] = X[['Elevation',\n                                   'Vertical_Distance_To_Hydrology']\n                                  ].diff(axis='columns').iloc[:, [1]]\n\n    X['Hydro_Euclidean'] = np.sqrt(X['Horizontal_Distance_To_Hydrology']**2 +\n                                   X['Vertical_Distance_To_Hydrology']**2)\n\n    X['Hydro_Fire_sum'] = X[['Horizontal_Distance_To_Hydrology',\n                             'Horizontal_Distance_To_Fire_Points']\n                            ].sum(axis='columns')\n\n    X['Hydro_Fire_diff'] = X[['Horizontal_Distance_To_Hydrology',\n                              'Horizontal_Distance_To_Fire_Points']\n                             ].diff(axis='columns').iloc[:, [1]].abs()\n\n    X['Hydro_Road_sum'] = X[['Horizontal_Distance_To_Hydrology',\n                             'Horizontal_Distance_To_Roadways']\n                            ].sum(axis='columns')\n\n    X['Hydro_Road_diff'] = X[['Horizontal_Distance_To_Hydrology',\n                              'Horizontal_Distance_To_Roadways']\n                             ].diff(axis='columns').iloc[:, [1]].abs()\n\n    X['Road_Fire_sum'] = X[['Horizontal_Distance_To_Roadways',\n                            'Horizontal_Distance_To_Fire_Points']\n                           ].sum(axis='columns')\n\n    X['Road_Fire_diff'] = X[['Horizontal_Distance_To_Roadways',\n                             'Horizontal_Distance_To_Fire_Points']\n                            ].diff(axis='columns').iloc[:, [1]].abs()\n    \n    # Compute Soil_Type number from Soil_Type binary columns\n    X['Stoneyness'] = sum(i * X['Soil_Type{}'.format(i)] for i in range(1, 41))\n    \n    # For all 40 Soil_Types, 1=rubbly, 2=stony, 3=very stony, 4=extremely stony, 0=?\n    stoneyness = [4, 3, 1, 1, 1, 2, 0, 0, 3, 1, \n                  1, 2, 1, 0, 0, 0, 0, 3, 0, 0, \n                  0, 4, 0, 4, 4, 3, 4, 4, 4, 4, \n                  4, 4, 4, 4, 1, 4, 4, 4, 4, 4]\n    \n    # Replace Soil_Type number with \"stoneyness\" value\n    X['Stoneyness'] = X['Stoneyness'].replace(range(1, 41), stoneyness)\n    \n    return X\n\n\ndef drop_features(X_):\n    X = X_.copy()\n    drop_cols = ['Soil_Type7', 'Soil_Type8', 'Soil_Type9', 'Soil_Type14', 'Soil_Type15', \n                 'Soil_Type16', 'Soil_Type18', 'Soil_Type19', 'Soil_Type21', 'Soil_Type25', \n                 'Soil_Type26', 'Soil_Type27', 'Soil_Type28', 'Soil_Type34', 'Soil_Type36', \n                 'Soil_Type37']\n    \n    X = X.drop(drop_cols, axis='columns')\n\n    return X\n\nprint('  -- Processing train data')\nX_train = add_features(X_train)\nX_train = drop_features(X_train)\n\nprint('  -- Processing test data')\nX_test = add_features(X_test)\nX_test = drop_features(X_test)","eeff9001":"print('> Adding cluster based feature')\nfrom sklearn.mixture import GaussianMixture\n\ngmix = GaussianMixture(n_components=10)\ngmix.fit(X_test)\n\nX_train['Test_Cluster'] = gmix.predict(X_train)\nX_test['Test_Cluster'] = gmix.predict(X_test)","4c2d3cfe":"print('> Setting up classifiers')\nn_jobs = -1\n\nab_clf = AdaBoostClassifier(n_estimators=200,\n                            base_estimator=DecisionTreeClassifier(\n                                min_samples_leaf=2,\n                                random_state=random_state),\n                            random_state=random_state)\n\nlg_clf = LGBMClassifier(n_estimators=400,\n                        num_leaves=100,\n                        verbosity=0,\n                        random_state=random_state,\n                        n_jobs=n_jobs)\n\nrf_clf = RandomForestClassifier(n_estimators=400,\n                                min_samples_leaf=1,\n                                verbose=0,\n                                random_state=random_state,\n                                n_jobs=n_jobs)\n\nensemble = [('ab', ab_clf),\n            ('lg', lg_clf),\n            ('rf', rf_clf)]\n\nstack = StackingCVClassifier(classifiers=[clf for label, clf in ensemble],\n                             meta_classifier=rf_clf,\n                             cv=5,\n                             use_probas=True,\n                             use_features_in_secondary=True,\n                             verbose=1,\n                             random_state=random_state,\n                             n_jobs=n_jobs)\n\n\nprint('> Fitting & predicting')\nstack = stack.fit(X_train, y_train)\nprediction = stack.predict(X_test)\n\n\nprint('> Creating submission')\nsubmission = pd.DataFrame({'Id': X_test.index, 'Cover_Type': prediction})\nsubmission.to_csv('submission.csv', index=False)\n\n\nprint('> Done !')","a7a818ea":"Here is my addition, since the test dataset is so large I wanted to extract features from it that could be fed back into the training data. For this I used a GaussianMixture clustering model built against the Test data and then added back to both Train\/Test as a new feature.","9432904f":"Thanks to [kwabenantim](https:\/\/www.kaggle.com\/kwabenantim) for this excellent ensemble model.\n\nSee: https:\/\/www.kaggle.com\/kwabenantim\/forest-cover-stacking-multiple-classifiers"}}