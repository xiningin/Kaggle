{"cell_type":{"c6abcd6a":"code","dca39c3f":"code","73b3d564":"code","1a203a91":"code","424943c5":"code","b0f968e4":"code","7f769ce3":"code","5df89378":"code","67c7c4e8":"code","08a4ee7d":"code","67c29c21":"code","d6af7a14":"code","8c98cbef":"code","72728245":"code","27867264":"code","14e44000":"code","49d5d83f":"code","802e6184":"code","58996e5a":"code","a096957c":"code","10791c74":"code","887f6d07":"code","ec77b0cc":"code","21d84366":"code","abdb50b2":"code","d96b6bf0":"code","db4f5a11":"code","57a05d9f":"code","7b4e9bac":"code","82df5936":"code","6e3b670e":"code","7ff437d2":"code","acdb51f6":"code","5d843b80":"code","2a9245b0":"code","4c8e2298":"code","9b4a02a6":"code","c3bc99d0":"code","47c5c167":"code","877a7c1f":"code","01a2d234":"code","4252176a":"markdown","f424df51":"markdown","d3e9dc33":"markdown","8cb34a4b":"markdown","515cc725":"markdown","760a8b2c":"markdown"},"source":{"c6abcd6a":"!pip install rouge gdown transformers deepspeed mpi4py bert-score","dca39c3f":"# Importing basic libtest.jsonles\nimport numpy as np\nimport pandas as pd\nimport json\nimport argparse\n\n# Importing some tools for flattening 2d arrays to 1d\nfrom functools import reduce\nfrom operator import add\n\n# Importing hugging face library for getting the transformers and tokenizers\nfrom transformers import AutoTokenizer, BartForConditionalGeneration,PegasusForConditionalGeneration,AdamW,get_linear_schedule_with_warmup \n\n# Importing some pytorch classes and functions\nimport torch\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, TensorDataset, random_split, RandomSampler, Dataset\nfrom torch import cuda \n\nimport pytorch_lightning as pl\nfrom pytorch_lightning.loggers import WandbLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint,EarlyStopping\nfrom pytorch_lightning.plugins import DeepSpeedPlugin\nfrom deepspeed.ops.adam import FusedAdam\n\n\n\nimport wandb\n\n\n\n# Importing libraries for evaluation\nfrom rouge import Rouge\nfrom bert_score import score\n\n# Importing library for progress bar GUI\nfrom fastai.text.core import progress_bar\n\n# Importing library for parallization\nfrom joblib import Parallel,delayed,Memory\n\n#Importing library for warnings\nimport warnings\n\n\nwarnings.filterwarnings('ignore') # Ignore warnings in the notebook\n\ndevice = 'cuda' if cuda.is_available() else 'cpu' # Enable gpu if exsited else use cpu\n# wandb.login()","73b3d564":"!nvidia-smi","1a203a91":"# Downloading trainning , validation , and testing sets.\n!gdown https:\/\/raw.githubusercontent.com\/allenai\/scitldr\/master\/SciTLDR-Data\/SciTLDR-AIC\/train.jsonl\n!gdown https:\/\/raw.githubusercontent.com\/allenai\/scitldr\/master\/SciTLDR-Data\/SciTLDR-AIC\/dev.jsonl\n!gdown https:\/\/raw.githubusercontent.com\/allenai\/scitldr\/master\/SciTLDR-Data\/SciTLDR-AIC\/test.jsonl","424943c5":"# Reading the train json file and put it on array\ntrain_data = []\nwith open('.\/train.jsonl','r') as f:\n    for i in f.readlines():\n        data = json.loads(i)\n        train_data.append(data)","b0f968e4":"# Reading the validaiton json file and put it on array\nval_data = []\nwith open('.\/dev.jsonl','r') as f:\n    for i in f.readlines():\n        data = json.loads(i)\n        val_data.append(data)","7f769ce3":"test_data = []\nwith open('.\/test.jsonl','r') as f:\n    for i in f.readlines():\n        data = json.loads(i)\n        test_data.append(data)","5df89378":"def splitting_text(data,typ='train'):\n    '''\n    splitting the given data to orginial text and their summaries \n    using shared memory parallization technique for faster processing  \n    '''\n    docs = []\n    gold_summary = []\n\n    def text_parsing(text):\n        if typ != 'test':\n            for _ in range(len(text['target'])):\n                docs.append(''.join(text['source']))\n        elif typ== 'test':\n            docs.append(''.join(text['source']))\n            \n        gold_summary.append(text['target'])\n        \n\n    def parallel_parsing(data):\n        Parallel(n_jobs=-1, require='sharedmem'\n                 )(delayed(text_parsing)(text) for text in data)\n\n    parallel_parsing(data)\n\n    return (docs, gold_summary)","67c7c4e8":"# Getting the train, and test data splitted using splitting_text function declared above\ntrain_text,train_summ = splitting_text(train_data)\nval_text,val_summ = splitting_text(val_data)\ntest_text,test_summ = splitting_text(test_data)","08a4ee7d":"class TransformersBaseTokenizer:\n\n    \"\"\"Class for encoding and decoding given texts for transformers\"\"\"\n\n    def __init__(\n        self,\n        pretrained_tokenizer,\n        model_type='bart',\n        **kwargs\n        ):\n        self._pretrained_tokenizer = pretrained_tokenizer\n        self.max_seq_len = pretrained_tokenizer.model_max_length\n        self.model_type = model_type\n        self.pad_token_id = pretrained_tokenizer.pad_token_id\n\n    def __call__(self, *args, **kwargs):\n        return self\n\n    def tokenizer(self, t):\n        \"\"\"Limits the maximum sequence length and add the special tokens\"\"\"\n\n        if self.model_type == 'bart':\n            \n            CLS = self._pretrained_tokenizer.cls_token\n            SEP = self._pretrained_tokenizer.sep_token\n            \n            tokens = \\\n                self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len\n                - 2]\n            tokens = [CLS] + tokens + [SEP]\n\n        elif self.model_type == 'pegasus':\n            eos = self._pretrained_tokenizer.eos_token\n            tokens = \\\n                self._pretrained_tokenizer.tokenize(t)[:self.max_seq_len\n                - 2]\n            tokens = tokens + [eos]\n                    \n\n        return tokens\n\n    def _numercalise(self, t):\n        \"\"\"Convert text to there coressponding ids\"\"\"\n        \n        tokenized_text = self._pretrained_tokenizer(\n                t,\n                max_length=self.max_seq_len,\n                return_tensors='pt',\n                padding='max_length',\n                truncation=True,\n                add_special_tokens=True,\n                is_split_into_words=False,\n                )\n        return tokenized_text\n\n    def _textify(self, input_ids):\n        \"\"\"Convert ids to thier coressponding text\"\"\"\n\n        text = self._pretrained_tokenizer.batch_decode(input_ids,\n                skip_special_tokens=True,\n                clean_up_tokenization_spaces=False)\n        return text\n\n    def _chunks(self, lst, n):\n        \"\"\"splitting the text into batches\"\"\"\n\n        for i in range(0, len(lst), n):\n            yield lst[i:i + n]\n\n    def numercalise(self, t, batch_size=4):\n        \"\"\"Convert text to thier coressponding ids and get the attention mask to differentiate between pad and input texts\"\"\"\n\n        if isinstance(t, str):\n            t = [t]  # convert str to list of str\n\n        results = Parallel(n_jobs=-1)(delayed(self._numercalise)(batch)\n                for batch in progress_bar(list(self._chunks(t,\n                batch_size))))\n        input_ids = []\n        attention_masks = []\n        for i in results:\n            input_ids.append(i['input_ids'])\n            attention_masks.append(i['attention_mask'])\n\n        return {'input_ids': torch.cat(input_ids),\n                'attention_mask': torch.cat(attention_masks)}\n\n    def textify(self, tensors, batch_size):\n        \"\"\"Convert ids to thier coressponding text\"\"\"\n\n        if len(tensors.shape) == 1:\n            tensors = [tensors]  # convert 1d tensor to 2d\n\n        results = Parallel(n_jobs=-1, backend='threading'\n                           )(delayed(self._textify)(summary_ids)\n                             for summary_ids in\n                             progress_bar(list(self._chunks(tensors,\n                             batch_size))))\n\n        return reduce(add, results)","67c29c21":"# Download the bart tokenizer from hugging face api\nbart_tokenizer = AutoTokenizer.from_pretrained('facebook\/bart-large-xsum')","d6af7a14":"# Passing the bart tokenizer to our TransformersBaseTokenizer wrapper\ntokenizer = TransformersBaseTokenizer(bart_tokenizer)","8c98cbef":"# Converting the text into there coressponding input_ids and attention_mask to be interperted by the model\n\ntrain_inputs = tokenizer.numercalise(train_text,16)\n\nval_inputs = tokenizer.numercalise(val_text,16)\n\ntest_inputs = tokenizer.numercalise(test_text,16)\n\ntrain_outputs = tokenizer.numercalise(reduce(add,train_summ),16)\n\nval_outputs = tokenizer.numercalise(reduce(add,val_summ),16)","72728245":"# Getting the labels from train and val\nlabels = train_outputs['input_ids']\nval_labels = val_outputs['input_ids']","27867264":"train_inputs['labels'] = labels","14e44000":"val_inputs['labels'] = val_labels","49d5d83f":"hparams = argparse.Namespace()\n\nhparams.freeze_encoder = True\nhparams.freeze_embeds = True\nhparams.eval_beams = 2","802e6184":"def shift_tokens_right(input_ids, pad_token_id):\n    \"\"\" Shift input ids one token to the right, and wrap the last non pad token (usually <eos>).\n      This is taken directly from modeling_bart.py\n  \"\"\"\n\n    prev_output_tokens = input_ids.clone()\n    index_of_eos = (input_ids.ne(pad_token_id).sum(dim=1)\n                    - 1).unsqueeze(-1)\n    prev_output_tokens[:, 0] = input_ids.gather(1,\n            index_of_eos).squeeze()\n    prev_output_tokens[:, 1:] = input_ids[:, :-1]\n    \n    return prev_output_tokens","58996e5a":"def freeze_params(model):\n    ''' Making the input part of the model as non trainable parameters'''\n\n    for layer in model.parameters():\n        layer.requires_grad = False","a096957c":"class Model(pl.LightningModule):\n\n    def __init__(\n        self,\n        lr,\n        tokenizer,\n        model,\n        params,\n        n_warmup_steps=None,\n        n_training_steps =None\n        ):\n\n        super(Model, self).__init__()\n\n        self.tokenizer = tokenizer\n        self.model = model\n        self.lr = lr\n        self.params = params\n        self.n_warmup_steps = n_warmup_steps\n        self.n_training_steps = n_training_steps\n        if self.params.freeze_encoder:\n            freeze_params(self.model.get_encoder())\n\n        if self.params.freeze_embeds:\n            self.freeze_embeds()\n\n    def freeze_embeds(self):\n        ''' freeze the positional embedding parameters of the model '''\n\n        freeze_params(self.model.model.shared)\n        for d in [self.model.model.encoder, self.model.model.decoder]:\n            freeze_params(d.embed_positions)\n            freeze_params(d.embed_tokens)\n\n    def forward(self, input_ids, **kwargs):\n        return self.model(input_ids, **kwargs)\n\n    def configure_optimizers(self):\n        optimizer = FusedAdam(self.parameters(), lr=self.lr)\n        scheduler = get_linear_schedule_with_warmup(\n                                                      optimizer,\n                                                      num_warmup_steps=self.n_warmup_steps,\n                                                      num_training_steps=self.n_training_steps\n                                                    )\n        \n        return dict(optimizer=optimizer,lr_scheduler=dict(scheduler=scheduler,interval='step'))\n\n    def training_step(self, batch, batch_idx):\n\n    # Load the data into variables\n\n        src_ids, src_mask = batch[0], batch[1]\n        tgt_ids = batch[2]\n\n    # Shift the decoder tokens right (but NOT the tgt_ids)\n\n        decoder_input_ids = shift_tokens_right(tgt_ids,\n                self.tokenizer.pad_token_id)\n\n    # Run the model and get the logits\n\n        outputs = self(src_ids, attention_mask=src_mask,\n                       decoder_input_ids=decoder_input_ids,\n                       use_cache=False)\n        labels_logits = outputs[0]\n\n    # Create the loss function\n\n        ce_loss_fct = \\\n            torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n\n    # Calculate the loss on the un-shifted tokens\n\n        loss = ce_loss_fct(labels_logits.view(-1,\n                           labels_logits.shape[-1]), tgt_ids.view(-1))\n\n        self.log(\n            'train_loss\/epoch',\n            loss,\n            prog_bar=True,\n            logger=True,\n            on_epoch=True,\n            on_step = False\n            )\n        \n        self.log(\n            'train_loss\/step',\n            loss,\n            prog_bar=True,\n            logger=True,\n            on_epoch=False,\n            on_step = True\n            )\n\n        return {\"loss\": loss}\n\n    def validation_step(self, batch, batch_idx):\n\n    # Load the data into variables\n\n        src_ids, src_mask = batch[0], batch[1]\n        tgt_ids = batch[2]\n        \n        decoder_input_ids = shift_tokens_right(tgt_ids,\n                self.tokenizer.pad_token_id)\n\n    # Run the model and get the logits\n\n        outputs = self(src_ids, attention_mask=src_mask,\n                       decoder_input_ids=decoder_input_ids,\n                       use_cache=False)\n        labels_logits = outputs[0]\n\n    # Create the loss function\n\n        ce_loss_fct = \\\n            torch.nn.CrossEntropyLoss(ignore_index=self.tokenizer.pad_token_id)\n\n    # Calculate the loss on the un-shifted tokens\n\n        loss = ce_loss_fct(labels_logits.view(-1,\n                           labels_logits.shape[-1]), tgt_ids.view(-1))\n        self.log(\n            'val_loss\/epoch',\n            loss,\n            prog_bar=True,\n            logger=True,\n            on_epoch=True,\n            on_step = False,\n            )\n        \n        self.log('val_loss\/step',loss,prog_bar=True,logger=True,on_epoch=False,on_step = True)\n\n        return {'loss': loss}\n    \n    def _chunks(self, lst, n):\n        \"\"\"splitting the text into batches\"\"\"\n\n        for i in range(0, len(lst['input_ids']), n):\n            yield lst['input_ids'][i:i + n],lst['attention_mask'][i:i + n]\n\n    def _generate_text(\n        self,\n        text,\n        mask,\n        eval_beams,\n        early_stopping=True,\n        max_len=150,\n        penalty_length = 0.2\n    ):\n        \n        generated_ids = self.model.generate(\n            text.to(device),\n            attention_mask=mask.to(device),\n            use_cache=True,\n            decoder_start_token_id=self.tokenizer.pad_token_id,\n            num_beams=eval_beams,\n            max_length=max_len,\n            early_stopping=early_stopping,\n            length_penalty = penalty_length,\n            no_repeat_ngram_size=3\n            \n            )\n        \n        return [self.tokenizer.decode(w, skip_special_tokens=True,\n                clean_up_tokenization_spaces=True) for w in\n                generated_ids]\n\n    def generate_text(\n        self,\n        text,\n        eval_beams,\n        early_stopping=True,\n        max_len=50,\n        batch_size=2,\n        length_penalty = 0.2\n        ):\n        ''' Function to generate text '''\n        summaries = []\n        \n        for ids,mask in progress_bar(list(self._chunks(text,batch_size))):\n            summaries.extend(self._generate_text(ids,mask=mask,eval_beams = eval_beams,early_stopping = early_stopping,max_len = max_len,penalty_length = length_penalty))\n    \n        return summaries\n            \n\n\n    def save(self, model_name):\n        model_extension =  model_name + '.h5'\n        torch.save(self.model,model_extension)\n        print ('Model is saved')\n        return '.\/'+  model_extension","10791c74":"# Create a dataloading module as per the PyTorch Lightning Docs\n\nclass SummaryDataModule(pl.LightningDataModule):\n\n    def __init__(\n        self,\n        train,\n        val=None,\n        test=None,\n        batch_size=2,\n        ):\n        super().__init__()\n        self.train = train\n        self.val = val\n        self.test = test\n        self.batch_size = batch_size\n\n  # Load the training, validation and test sets in Pytorch Dataset objects\n\n    def train_dataloader(self):\n        dataset = TensorDataset(self.train['input_ids'],\n                                self.train['attention_mask'],\n                                self.train['labels'])\n        train_data = DataLoader(dataset,\n                                sampler=RandomSampler(dataset),\n                                batch_size=self.batch_size)\n        return train_data\n\n    def val_dataloader(self):\n        dataset = TensorDataset(self.val['input_ids'],\n                                self.val['attention_mask'],\n                                self.val['labels'])\n        val_data = DataLoader(dataset, batch_size=self.batch_size)\n        return val_data\n\n    def test_dataloader(self):\n        dataset = TensorDataset(self.test['input_ids'],\n                                self.test['attention_mask'],\n                                self.test['labels'])\n        test_data = DataLoader(dataset, batch_size=self.batch_size)\n        return test_data","887f6d07":"# Load the data into the model for training\nsummary_data = SummaryDataModule(train = train_inputs,val = val_inputs ,\n                                 batch_size = 1)","ec77b0cc":"bart_model = BartForConditionalGeneration.from_pretrained(\"facebook\/bart-large-xsum\")","21d84366":"steps_per_epoch=len(train_text) \/\/ 1\ntotal_training_steps = steps_per_epoch * 5\nwarmup_steps = total_training_steps \/\/ 5","abdb50b2":"model = Model(lr = 3e-5, tokenizer = bart_tokenizer, model = bart_model, params = hparams,  n_warmup_steps=warmup_steps,\n  n_training_steps=total_training_steps)","d96b6bf0":"checkpoint = ModelCheckpoint(verbose=True,save_top_k = 1,monitor = 'val_loss\/epoch',mode = 'min',filename='scitldr-{epoch:02d}-{val_loss:.2f}')\nwandb_logger = WandbLogger(project='Abstractive Text Summarization')","db4f5a11":"early_stopping_callback = EarlyStopping(monitor='val_loss\/epoch', patience=3)","57a05d9f":"trainer = pl.Trainer(gpus=1,\n                     precision=16,\n                     max_epochs = 5,\n                     auto_lr_find = False,\n                     callbacks=[checkpoint,early_stopping_callback],\n                     logger = wandb_logger,\n                     plugins = DeepSpeedPlugin(stage=1,cpu_offload=True)\n                     progress_bar_refresh_rate = 50)","7b4e9bac":"trainer.fit(model,summary_data)","82df5936":"path = model.save('bart_model')","6e3b670e":"!tar -zcvf bart_model.tar.gz .\/bart_model.h5","7ff437d2":"from IPython.display import FileLink\nFileLink(r'.\/bart_model.tar.gz')","acdb51f6":"def generate_summary(seed_line, model_,num_beam = 4,penalty_length =0.2):\n\n  # Put the model on eval mode\n\n    model_.to(torch.device('cuda'))\n    model_.eval()\n\n    line = model_.generate_text(seed_line, eval_beams=num_beam,length_penalty = penalty_length)\n\n    return line","5d843b80":"!gdown --id 19TvvqlPk7UxLGO4M1V5IEQWCef0_veP3\n!tar -xvzf .\/bart_model.tar.gz","2a9245b0":"model = torch.load('.\/bart_model.h5')","4c8e2298":"b_model = Model(lr = 2e-5, tokenizer = bart_tokenizer, model = model, params = hparams)","9b4a02a6":"class Scorer:\n\n    def __init__(\n        self,\n        generate_text,\n        gold_text,\n        article,\n        ):\n        self.generated = generate_text\n        self.gold_text = gold_text\n        self.article = article\n\n    def rouge_score(self):\n        rouge_scores = []\n        rouge = Rouge()\n        for i,summ in enumerate(self.gold_text):\n            maxx  = {'rouge-1':{'f': 0}}\n            for summary in summ:\n                try:\n                    scores = rouge.get_scores(self.generated[i], summary)\n                except:\n                    print(summary,i)\n                    return 'no'\n                rouge_f = scores[0]['rouge-1']['f']\n                if maxx['rouge-1']['f'] <= rouge_f:\n                        maxx = scores[0]\n            rouge_scores.append(maxx)\n        rouge_1_f = []\n        rouge_2_f = []\n        rouge_l_f = []\n        for rouge in rouge_scores:\n            rouge_1_f.append(rouge['rouge-1']['f'])\n            rouge_2_f.append(rouge['rouge-2']['f'])\n            rouge_l_f.append(rouge['rouge-l']['f'])\n            \n        \n        return [np.mean(rouge_1_f),np.mean(rouge_2_f),np.mean(rouge_l_f)]\n    \n    def bert_score(self):\n        bert_scores = []\n        for i,summ in enumerate(self.gold_text):\n            maxx  = -1000\n            for summary in summ:\n                P,R,F1 = score([self.generated[i]], [summary])\n                    \n                if maxx <= F1:\n                        maxx = F1\n            bert_scores.append(maxx)\n        return np.mean(bert_scores)\n            ","c3bc99d0":"num_beams = [2,6]\np_l = [0.2,0.6]","47c5c167":"report = {'rouge-1':[],'rouge-2':[],'rouge-l':[],'num_beam':[],'length_penalty':[],'bert_score':[]}","877a7c1f":"for beam in num_beams:\n    for length in p_l:\n        bart_summ = generate_summary(test_inputs,model,num_beam=beam,penalty_length=length)\n        bart_scorer =Scorer(bart_summ,test_summ,test_text)\n        bert_score = bart_scorer.bert_score()\n        rog = bart_scorer.rouge_score()\n        report['rouge-1'].append(rog[0])\n        report['rouge-2'].append(rog[1])\n        report['rouge-l'].append(rog[2])\n        report['bert_score'].append(bert_score)\n        report['num_beam'].append(beam)\n        report['length_penalty'].append(length)","01a2d234":"pd.DataFrame(report)","4252176a":"### BART Finetunning","f424df51":"# Evaluation","d3e9dc33":"### Tokenizing data for BART Model","8cb34a4b":"## Tokenization","515cc725":"### Tokenization Class","760a8b2c":"# Model Finetunning"}}