{"cell_type":{"60d6e6af":"code","2288c766":"code","c75f1933":"code","20b688e9":"code","4711cc70":"code","e80a5082":"code","0fdabe4e":"code","9f1b2ae7":"code","18d3ca4e":"code","3f35a0f8":"code","8e43afbd":"code","a72264ef":"code","ce5017fa":"code","87cdd7b9":"code","d0df737a":"code","ced6b58a":"code","cfcdd50c":"code","cb7f8520":"code","f2974f05":"code","0d1cc926":"code","548a3455":"code","f5d815ed":"code","66213838":"code","6e7f2e89":"code","16792f08":"code","aff9fa10":"code","4e47d077":"code","8a8c86b0":"code","6f4d357e":"code","cdebe38e":"code","8e99a155":"code","ed3842f3":"code","926c6607":"code","9937e46c":"code","1411c5e6":"code","813059a8":"code","eff7ca08":"code","3f59a05a":"code","7b737ff7":"code","e8ba214d":"code","f8891183":"code","2d7254a0":"code","f9d70236":"code","61dd48fe":"code","e2e3e5ad":"code","f7453634":"code","585f5415":"code","8fb19829":"code","42b36c03":"code","7448c042":"code","f1c11f8c":"code","e9b69911":"code","b4b11f73":"code","db313e59":"code","4b93eb18":"code","87dcad1d":"code","23cbc4a1":"code","9c02cecf":"code","41732c65":"code","c12c902e":"code","5aad43ef":"code","4af43636":"code","efa6bc88":"code","131e90ea":"code","eccff745":"code","9fdf4ebe":"code","1f053b85":"code","6a7fefb3":"code","ffa94830":"code","766d21a8":"code","cd36322e":"code","001a061e":"code","52cc211e":"code","afd9f40e":"code","d29e2972":"code","60b64419":"code","14f029e4":"code","9c8fa1ba":"code","4f7c1bcd":"code","5f39b26a":"code","197d6765":"code","6d0ee588":"code","16cf8a2a":"code","88eabc73":"code","50a76b8a":"code","b0d59822":"code","ea32d555":"code","f13d1061":"code","66d26195":"code","beac9fe2":"code","f060650c":"code","01a629f3":"code","8990d7cf":"code","f83bbd76":"code","554d6968":"code","3c3fd1f7":"code","27d50a04":"code","9c6f0064":"code","c59253f4":"markdown","176ac219":"markdown","0840f538":"markdown","25bebd6e":"markdown","5262d8da":"markdown","f8a7bdbe":"markdown","d6bbd400":"markdown","a9917346":"markdown","9b8b53a0":"markdown","b20e81de":"markdown","7d48a0b9":"markdown","51aab967":"markdown","ef427d5b":"markdown","c59e50e4":"markdown","61433010":"markdown","132a8b46":"markdown","7df44f96":"markdown","ce22d9a9":"markdown","8465f715":"markdown","81205a62":"markdown","e3704fa7":"markdown","ca3a751b":"markdown","f9d07e57":"markdown","e8684469":"markdown","0e7ae0c0":"markdown","2717c86e":"markdown","e1a374d7":"markdown","8fa2be80":"markdown","96b0bd98":"markdown","f5c508e0":"markdown","d7702abc":"markdown","3b78fe1d":"markdown","720948a5":"markdown","f7e7a286":"markdown","773d8944":"markdown"},"source":{"60d6e6af":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2288c766":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nimport matplotlib\nfrom matplotlib.colors import ListedColormap\n\nfrom sklearn import datasets, metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression, LogisticRegressionCV\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedKFold\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder, StandardScaler\nfrom scipy.special import boxcox, inv_boxcox\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.svm import SVC\n\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n \nplt.style.use('ggplot')\nsns.set(font_scale=1.5)\n%config InlineBackend.figure_format = 'retina'\n%matplotlib inline\n\n\n# Pallets used for visualizations\ncolor= \"Spectral\"\ncolor_sns= sns.color_palette(color)\ncolor_plt = ListedColormap(sns.color_palette(color).as_hex())\ncolor_hist = 'teal'","c75f1933":"# files in Jupyter\n#df_full = pd.read_csv('..\/datasets\/train.csv')\n#test_df = pd.read_csv('..\/datasets\/test.csv')\n\n# files in kaggle\ndf_full = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","20b688e9":"df = df_full\n\n# saving the IDs for the first and last data point in the test set \n# because we will be merging both train and test\ntest_first_id = test_df['PassengerId'].iloc[0]\ntest_last_id = test_df['PassengerId'].iloc[-1]\nprint('Test DataFrame:')\nprint('First ID = ', test_first_id, '  Last ID= ',test_last_id )","4711cc70":"df.head()","e80a5082":"df.tail()","0fdabe4e":"test_df.head()","9f1b2ae7":"test_df.tail()","18d3ca4e":"df.shape","3f35a0f8":"test_df.shape","8e43afbd":"# filling the target column in the test_df with a unique number just to distinguish it from the 0, 1 \ntest_df['Survived'] = 44","a72264ef":"test_df.shape","ce5017fa":"df = df.append(test_df, ignore_index = True, sort=True) [df_full.columns.tolist()] # df_full.columns.tolist() is for getting the order of the columns correctl\n","87cdd7b9":"df.tail()","d0df737a":"# Getting family name from name: (to check families withs same last name):\ndf['family_name']= df['Name'].str.split(',', expand=True)[0]","ced6b58a":"# it is actually wrong to calculate the family size from the family name, as there are multiple families with the same last name\n# so delete this\n#df['family_size_from_name']= df.groupby('family_name')['family_name'].transform('count')","cfcdd50c":"# sibsp = # of siblings \/ spouses aboard the Titanic\n# parch = # of parents \/ children aboard the Titanic\n# therefore, family_size = sibsp + parch + the passenger him\/her self\ndf['family_size'] = df ['SibSp'] + df['Parch'] + 1","cb7f8520":"fig, ax = plt.subplots( figsize=(15, 6))\nno_test= df.iloc[0:(df[df['PassengerId'] == test_first_id].index[0]), :]\nsns.countplot(data = no_test , x = 'family_size' , hue = 'Survived', palette=color_sns)\n\nax.set_xlabel('Family Size')\nax.set_ylabel('Frequency')\nfig.suptitle('The Distribution of the Number of Family Members on the Ship and Survival', fontsize = 20)\n\nplt.show()","f2974f05":"df['is_alone'] = np.where((df['family_size']== 1) , 1, 0)","0d1cc926":"family_names_in_train_test = [x for x in df.loc[df['Survived']!= 44]['family_name'].unique() if x in df.loc[df['Survived']== 44]['family_name'].unique()]","548a3455":"non_test_family_df= df.drop( df[ df['Survived'] == 44 ].index )\nfamily_survival_map = non_test_family_df[['family_name', 'Survived']].groupby(['family_name'], as_index=False).mean().sort_values(by='Survived', ascending=False).set_index('family_name').to_dict()['Survived']\ndf['family_survival_rate']= df.loc[df['family_name'].isin(family_survival_map)]['family_name'].replace(family_survival_map)\ndf['family_survival_rate']= df['family_survival_rate'].fillna(-1)","f5d815ed":"fig, ax = plt.subplots( figsize=(15, 6))\nno_test= df.iloc[0:(df[df['PassengerId'] == test_first_id].index[0]), :]\nax.hist(no_test['family_survival_rate'], color = color_hist)\n\nax.set_xlabel('Family Survival Rate')\nax.set_ylabel('Frequency')\nfig.suptitle('The Distribution Family Survival Rate', fontsize = 20)\n\nplt.show()","66213838":"# Getting title from name:\ndf['title']= df['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]","6e7f2e89":"title_map={\n    \"Mr\": \"Mr\",\n    \"Miss\": \"Miss\",\n    \"Mrs\": \"Mrs\",\n    \"Master\": \"Master\",\n    \"Rev\": \"Other\",\n    \"Dr\": \"Other\",\n    \"Col\": \"Other\",\n    \"Ms\": \"Mrs\",\n    \"Mlle\": \"Miss\",\n    \"Major\": \"Other\",\n    \"Dona\": \"Other\",\n    \"Mme\": \"Mrs\",\n    \"Sir\" : \"Royal\",\n    \"Capt\": \"Other\", \n    \"Lady\" : \"Royal\", \n    \"Don\": \"Other\",\n    \"the Countess\": \"Royal\", \n    \"Jonkheer\" : \"Other\",   \n    }\ndf['title'] = df['title'] .replace(title_map)","16792f08":"fig, ax = plt.subplots( figsize=(15, 6))\nno_test= df.iloc[0:(df[df['PassengerId'] == test_first_id].index[0]), :]\nsns.countplot(data = no_test , x = 'title' , hue = 'Survived', palette=color_sns)\n\nax.set_xlabel('Passengar Title')\nax.set_ylabel('Frequency')\nfig.suptitle('The Distribution of Passenger Title and Survival', fontsize = 20)\n\nplt.show()","aff9fa10":"df['marrid_female'] = np.where((df['title']== 'Mrs') , 1, 0)","4e47d077":"title_map={\n    \"Mr\": \"Mr\",\n    \"Miss\": \"Miss\/Mrs\",\n    \"Mrs\": \"Miss\/Mrs\",\n    \"Master\": \"Master\",\n    \"Other\": \"Other\",\n    \"Royal\": \"Other\",\n    }\ndf['title'] = df['title'] .replace(title_map)","8a8c86b0":"# why there are some people with same ticket number? group them and see.\ndf['Ticket'].value_counts()","6f4d357e":"# finding the number of people traveling in groups from the ticket number, as all of them will have the same ticket number\ndf['group_size_by_ticket']= df.groupby('Ticket')['Ticket'].transform('count')","cdebe38e":"fig, ax = plt.subplots( figsize=(15, 6))\nno_test= df.iloc[0:(df[df['PassengerId'] == test_first_id].index[0]), :]\nsns.countplot(data = no_test , x = 'group_size_by_ticket' , hue = 'Survived', palette=color_sns)\n\nax.set_xlabel('group size by ticket')\nax.set_ylabel('Frequency')\nfig.suptitle('The Distribution of the Number of Passengers Grouped by the Same Ticket on the Ship and Survival', fontsize = 20)\n\nplt.show()","8e99a155":"non_test_ticket_df= df.drop( df[ df['Survived'] == 44 ].index )\ngroup_survival_map = non_test_ticket_df[['Ticket', 'Survived']].groupby(['Ticket'], as_index=False).mean().sort_values(by='Survived', ascending=False).set_index('Ticket').to_dict()['Survived']\ndf['group_survival_rate']= df.loc[df['Ticket'].isin(group_survival_map)]['Ticket'].replace(group_survival_map)\ndf['group_survival_rate']= df['group_survival_rate'].fillna(-1)","ed3842f3":"fig, ax = plt.subplots( figsize=(15, 6))\nno_test= df.iloc[0:(df[df['PassengerId'] == test_first_id].index[0]), :]\nax.hist(no_test['group_survival_rate'], color = color_hist)\n\nax.set_xlabel('Group Survival Rate')\nax.set_ylabel('Frequency')\nfig.suptitle('The Distribution Group Survival Rate', fontsize = 20)\n\nplt.show()","926c6607":"df['survival_rate'] = (df['group_survival_rate'] + df['family_survival_rate'])\/2\ndf['survival_rate'] = df['survival_rate'].fillna(-1)\ndf= df.drop (['group_survival_rate' ,'family_survival_rate'], axis=1)","9937e46c":"fig, ax = plt.subplots( figsize=(15, 6))\nno_test= df.iloc[0:(df[df['PassengerId'] == test_first_id].index[0]), :]\nax.hist(no_test['survival_rate'], color = color_hist)\n\nax.set_xlabel('overall Survival Rate')\nax.set_ylabel('Frequency')\nfig.suptitle('The Distribution overall Survival Rate', fontsize = 20)\n\nplt.show()","1411c5e6":"df['individual_fare'] = df['Fare']\/ df['group_size_by_ticket']","813059a8":" # if a female's title is not miss, and have parch then she's a mother \ndf['is_mother']= np.where((df['Sex']==\"female\")& (df['title'] != 'Miss') & (df['Parch'] > 0) , 1, 0)","eff7ca08":"# checking the missing value in the train & test dataset\nfig, ax = plt.subplots( figsize = (15, 8))\n\n# train data \nsns.heatmap(df.isnull(), yticklabels=False, cbar=False, cmap='viridis')\nax.set_title('Main Data Frame')","3f59a05a":"#Finding missing data and the percentage of it in each column\ntotal = df.isnull().sum().sort_values(ascending = False)\npercent = (df.isnull().sum() \/ df.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis = 1, keys = ['Total_NaN', 'Percent_Nan'])\nmissing_data","7b737ff7":"df.columns","e8ba214d":"df.dtypes","f8891183":"fig, ax = plt.subplots( figsize=(15, 6))\nax.hist(df['Age'], bins = 80, color = color_hist)\n\nax.set_xlabel('Age')\nax.set_ylabel('Frequency')\nfig.suptitle('The Distribution of Age', fontsize = 20)\n\nax.get_xaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n\nplt.show();","2d7254a0":"fig, ax = plt.subplots( figsize=(15, 6))\nax.hist(df['Fare'], bins = 80, color = color_hist)\n\nax.set_xlabel('Fare')\nax.set_ylabel('Frequency')\nfig.suptitle('The Distribution of Fare', fontsize = 20)\n\nax.get_xaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n\nplt.show();","f9d70236":"fig, ax = plt.subplots( figsize=(15, 6))\nax.hist(df['individual_fare'], bins = 80, color = color_hist)\n\nax.set_xlabel('Fare')\nax.set_ylabel('Frequency')\nfig.suptitle('The Distribution of Individual Fare', fontsize = 20)\n\nax.get_xaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n\nplt.show();","61dd48fe":"# Defining the function 'impute_age' to fill the mean age with respect to each Pclass\ndef impute_age(age_pclass): # passing age_pclass as ['Age', 'Pclass']\n    \n    # Passing age_pclass[0] which is 'Age' to variable 'Age'\n    Age = age_pclass[0]\n    \n    # Passing age_pclass[2] which is 'Pclass' to variable 'Pclass'\n    Pclass = age_pclass[1]\n    \n    # Applying condition based on the Age and filling the missing data respectively \n    if pd.isnull(Age):\n\n        if Pclass == 1:\n            return 38\n\n        elif Pclass == 2:\n            return 30\n\n        else:\n            return 25\n\n    else:\n        return Age","e2e3e5ad":"# Grabing age and apply the impute_age function with the train data frame\n#df['Age'] = df.apply(lambda x:impute_age(x[['Age', 'Pclass']]), axis = 1)\n\n# Tried filling by considering different columns (Ages of mothers)\n#df.loc[df[\"is_mother\"]==1, \"Age\"].fillna(df.loc[df[\"is_mother\"]==1 , \"Age\"].median(), inplace = True)\n#df.loc[df[\"Parch\"]==0, \"Age\"].fillna(df.loc[df[\"Parch\"]==0, \"Age\"].median(), inplace = True)\n#remaining null values substituted by median age of Sex+Title\n#df[\"Age\"].fillna(df.groupby(['Sex','title'])[\"Age\"].transform(\"median\"), inplace=True) # Fill age based on median \n\n\n# filling missing ages with \ndf['Age'] = df.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(int(x.median())))","f7453634":"# Extracting Deck from cabin\ndf['Cabin']= df['Cabin'].fillna('M')\ndf['Cabin']= df['Cabin'].str.replace('\\d+', '')\ndf['Cabin']= df['Cabin'].str.split(' ', expand=True)[0]","585f5415":"# mapping cabin names to deck names \ncabin_map= {'T':'A', 'A':'A', 'B':'B', 'C':'C', 'D':'D', 'E':'E', 'F':'F', 'G':'G' , 'M':'M'}\ndf['Deck'] = df['Cabin'].replace(cabin_map)\ndf['Deck'].value_counts()","8fb19829":"fig, ax = plt.subplots( figsize=(15, 6))\nno_test= df.iloc[0:(df[df['PassengerId'] == test_first_id].index[0]), :]\nsns.countplot(data = no_test , x = 'Deck' , hue = 'Survived', palette=color_sns, order = no_test['Deck'].value_counts().index)\n\nax.set_xlabel('Deck')\nax.set_ylabel('Frequency')\nfig.suptitle('The Distribution of Passengers in Deck (M= Missing)', fontsize = 20)\n\nplt.show()","42b36c03":"cabin_map= {'T':'ABC', 'A':'ABC', 'B':'ABC', 'C':'ABC', 'D':'DE', 'E':'DE', 'F':'FG', 'G':'FG' , 'M':'M'}\ndf['Deck'] = df['Cabin'] .replace(cabin_map)\ndf= df.drop ('Cabin', axis=1)\ndf['Deck'].value_counts()","7448c042":"# Fill the missing rows in Embarked with the port of highest embarkation 'S' in the train data frame\ndf['Embarked'].fillna(value = 'S', inplace = True)","f1c11f8c":"# Filling the missing value of Fare with the mean\ndf[\"Fare\"] = df[\"Fare\"].fillna( df.loc[(df['Pclass']==3) & ( df['Parch']==0) &( df['SibSp'] ==0)]['Fare'].median())\ndf[\"individual_fare\"] = df[\"individual_fare\"].replace(np.NaN, df[\"individual_fare\"].mean())","e9b69911":"df.isnull().sum().sum()","b4b11f73":"df.describe()","db313e59":"df[df.select_dtypes('object').columns.tolist()].describe()","4b93eb18":"visual_df = df.iloc[0:(df[df['PassengerId'] == test_first_id].index[0]), :] # train set","87dcad1d":"visual_df[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","23cbc4a1":"visual_df[['Embarked', 'Survived']].groupby(['Embarked'], as_index=False).mean().sort_values(by='Survived', ascending=False)","9c02cecf":"visual_df[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","41732c65":"visual_df[[\"title\", \"Survived\"]].groupby(['title'], as_index=False).mean().sort_values(by='Survived', ascending=False)","c12c902e":"visual_df[[\"Parch\", \"Survived\"]].groupby(['Parch'], as_index=False).mean().sort_values(by='Survived', ascending=False)","5aad43ef":"visual_df[[\"SibSp\", \"Survived\"]].groupby(['SibSp'], as_index=False).mean().sort_values(by='Survived', ascending=False)","4af43636":"visual_df[['family_size', 'Survived']].groupby(['family_size'], as_index=False).mean().sort_values(by='Survived', ascending=False)","efa6bc88":"visual_df[['group_size_by_ticket', 'Survived']].groupby(['group_size_by_ticket'], as_index=False).mean().sort_values(by='Survived', ascending=False)","131e90ea":"\nvisual_df[['family_name', 'Survived']].groupby(['family_name'], as_index=False).mean().sort_values(by='Survived', ascending=False).head()\n                                                                                                                        ","eccff745":"fig, axs = plt.subplots(figsize = (13, 10)) \nno_id_vis_df= visual_df.drop('PassengerId',axis=1 )\nmask = np.triu(np.ones_like(no_id_vis_df.corr(), dtype = np.bool))\nsns.heatmap(no_id_vis_df.corr(), ax = axs, annot = True, mask = mask, cmap = sns.diverging_palette(180, 10, as_cmap = True))\nplt.title('Correlation Of The Titanic Train DataFrame')\n\n# fix for mpl bug that cuts off top\/bottom of seaborn viz\nb, t = plt.ylim() # discover the values for bottom and top\nb += 0.5 # Add 0.5 to the bottom\nt -= 0.5 # Subtract 0.5 from the top\nplt.ylim(b, t) # update the ylim(bottom, top) values\nplt.show()","9fdf4ebe":"# a function that takes a dataframe and transforms it into a standard form after dropping nun_numirical columns\ndef to_standard (df):\n    \n    num_df = df[df.select_dtypes(include = np.number).columns.tolist()]\n    \n    ss = StandardScaler()\n    std = ss.fit_transform(num_df)\n    \n    std_df = pd.DataFrame(std, index = num_df.index, columns = num_df.columns)\n    return std_df","1f053b85":"ax, fig = plt.subplots(1, 1, figsize = (15, 8))\nplt.title('The distribution of All Numeric Variable in the Dataframe', fontsize = 20) #Change please\n\nsns.boxplot(y = \"variable\", x = \"value\", data = pd.melt(to_standard(visual_df)), palette = 'Spectral')\nplt.xlabel('Range after Standarization', size = 16)\nplt.ylabel('Attribue', size = 16)\n\n\n# fix for mpl bug that cuts off top\/bottom of seaborn viz\nb, t = plt.ylim() # discover the values for bottom and top\nb += 0.5 # Add 0.5 to the bottom\nt -= 0.5 # Subtract 0.5 from the top\nplt.ylim(b, t) # update the ylim(bottom, top) values\n\nplt.show()","6a7fefb3":"plt.figure(figsize=(10,5))\nno_test= df.iloc[0:(df[df['PassengerId'] == test_first_id].index[0]), :]\nax = sns.countplot(x=\"Pclass\", data=no_test, hue= 'Survived', palette = color_sns)\nplt.title('Pclass Survivors')\nplt.xlabel('Pclass')\nplt.ylabel('count')","ffa94830":"sns.set_style('darkgrid')\nfig, ax = plt.subplots(figsize=(15,5))\nax1 = sns.boxplot(x=\"Embarked\", y=\"individual_fare\", hue=\"Pclass\", data=visual_df, palette = color_sns);\nax1.set_title(\"The distribution of Individual Fare with Embarked and Pclass \", fontsize = 18)","766d21a8":"sns.set_style('darkgrid')\nfig, ax = plt.subplots(figsize=(15,5))\nax1 = sns.boxplot(x=\"Embarked\", y=\"Fare\", hue=\"Pclass\", data=visual_df, palette = color_sns);\nax1.set_title(\"The distribution of Fare with Embarked and Pclass\", fontsize = 18)","cd36322e":"ax=sns.countplot(data = visual_df , x = 'Survived' , hue = 'Sex', orient= 'v', palette= color_sns);\nax.set_title(\"The distribution of Sex\", fontsize = 18)","001a061e":"sns.countplot(data = visual_df , x = 'Survived' , hue = 'Embarked', orient= 'v', palette=color_sns);","52cc211e":"plt.figure(figsize=(10,5))\nno_test= df.iloc[0:(df[df['PassengerId'] == test_first_id].index[0]), :]\nplt.title('Siplings & Spouses Survivors')\n\nsns.barplot(x = 'SibSp', y= 'Age', data = no_test, hue='Survived',\n            palette = color_sns,\n            capsize = 0.05,                          \n             errwidth = 2,  \n            ci = 'sd'   \n            )\n\nplt.show()","afd9f40e":"plt.figure(figsize=(10,5))\nno_test= df.iloc[0:(df[df['PassengerId'] == test_first_id].index[0]), :]\nsns.countplot(x = 'Parch', data = no_test, hue='Survived', palette = color_sns)\nplt.title('Parent & childern Survivors')\nplt.show()","d29e2972":"# Mapping titles to numbers that are ranked by the survivel rates\ntitle_map={'Mr': 0, 'Other':1, 'Master':2, 'Miss':3, 'Mrs':4, 'Royal':5}\n#df['title'] = df['title'] .replace(title_map)\ndf['title'].value_counts()","60b64419":"#map each Embarked value to a numerical value\nembarked_map = {'S': 1, 'Q': 2, 'C': 3}\n#df['Embarked'] = df['Embarked'].map(embarked_map)\n\npclass_map ={ 1:'1', 2:'2' , 3:'3'}\ndf['Pclass'] = df['Pclass'].map(pclass_map)\n\ndf['Sex']= df['Sex'].replace({'male':0 , 'female':1})\n\n\ndf['Age']=pd.cut(df['Age'],bins=[0,2,17,35, 65, 99],labels=['Baby','Child','YoungerAdult','Adult','Elderly'])\ndf['Age']= df['Age'].replace({'Baby':0 , 'Child':1, 'YoungerAdult':2, 'Adult':3, 'Elderly':4})\n\n\n#df['Deck'] = df['Deck'].replace ({'M':0 , 'FG':1, 'DE':2, 'ABC':3})\n\ndf['Fare'] = pd.qcut(df['Fare'], 13)\ndf['Fare'] = LabelEncoder().fit_transform(df['Fare'])\n\ndf['individual_fare'] = pd.qcut(df['individual_fare'], 13)\ndf['individual_fare'] = LabelEncoder().fit_transform(df['individual_fare'])\n\nfamily_map = {1: 'Alone', 2: 'Small', 3: 'Small', 4: 'Small', 5: 'Medium', 6: 'Medium', 7: 'Large', 8: 'Large', 11: 'Large'}\ndf['family_size'] = df['family_size'].map(family_map)","14f029e4":"df = df.drop(['Name', 'Ticket', 'family_name', 'SibSp', 'Parch', 'individual_fare', 'Pclass', 'is_mother', 'survival_rate','Embarked', 'is_alone'] , axis = 1)","9c8fa1ba":"#df['Fare'] = boxcox(df['Fare'], 0.15)","4f7c1bcd":"df.shape","5f39b26a":"# changing all Categorical columns to dummies (0,1)\ndf = pd.get_dummies(df, columns = df.select_dtypes('object').columns, drop_first = True)\ndf.shape","197d6765":"# split data from from test_first_id to the end to be in the test\ntest_df = df.iloc[df[df['PassengerId'] == test_first_id].index[0]:, :]\ntest_df.head()","6d0ee588":"# deleting the test dataset from the main df\ndf = df.iloc[0:(df[df['PassengerId'] == test_first_id].index[0]), :]\ndf.tail()","16cf8a2a":"df = df.drop('PassengerId', axis = 1)\ntest_df = test_df.drop(['PassengerId','Survived'] , axis = 1)\ntest_df.head()","88eabc73":"test_df.shape","50a76b8a":"# a function that gets the predictions and saves them into a csv file with the correct format\ndef submission_file (test_pred):\n    for_id = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv') >>> in kaggle\n\n    #for_id = pd.read_csv('..\/datasets\/test.csv') # >>> in Jupyter\n    my_submission = pd.DataFrame({'PassengerId':for_id.PassengerId, 'Survived':test_pred.reshape(418)})\n    #my_submission.to_csv('submission.csv', index = False) # dropping the index column before saving it","b0d59822":"BOLD = '\\033[1m'\nEND = '\\033[0m'\nkfold=5","ea32d555":"y = df['Survived']\nX = df.drop('Survived', axis = 1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.1, stratify = y, shuffle = True, random_state=42)","f13d1061":"print('baseline accuracy' )\ny.value_counts()\/len(y)","66d26195":"def model_metrics(model, kfold, X_train, X_test, y_train, y_test, test_df):\n    \n    model.fit(X_train, y_train)\n\n    #metrics\n    results = cross_val_score(model, X_train, y_train, cv = kfold)\n    print(\"CV scores: \", results); print(\"CV Standard Deviation: \", results.std()); print();\n    print('CV Mean score: ', results.mean()); \n    print('Train score:   ', model.score(X_train, y_train))\n    print('Test score:    ', model.score(X_test, y_test))\n    \n    pred = model.predict(X_test)\n    # CODE HERE PLEASE\n    print()\n    print('Confusion Matrix: ')\n    print(confusion_matrix(y_test, pred))\n    print('Classification Report:  ')\n    print(classification_report(y_test, pred))\n    \n    test_pred = model.predict(test_df)\n    return test_pred","beac9fe2":"def basic_classifiers (X_train, X_test, y_train, y_test, test_df):\n    kfold = 5\n    # Scaling \n    scaler = StandardScaler()\n    X_train = scaler.fit_transform(X_train)\n    X_test = scaler.transform(X_test)\n    \n    ######################################################################################################  K Neighbors Classifier model\n    \n    print(); print(BOLD + 'K Neighbors Classifier Model:' + END)\n    knn = KNeighborsClassifier()\n    knn_pred = model_metrics(knn, kfold, X_train, X_test, y_train, y_test, test_df)\n    #submission_file (test_pred)\n    \n    ###################################################################################################### Logistic Regression\n    \n    print(); print(BOLD + 'Logistic Regression Model:' + END)\n    logistic_regression = LogisticRegression()\n    lg_pred = model_metrics(logistic_regression, kfold, X_train, X_test, y_train, y_test, test_df)\n    #submission_file (lg_pred)\n    \n    ###################################################################################################### Decision Tree\n    \n    print(); print(BOLD + 'Decision Tree Classifier Model:' + END)\n    decision_tree = DecisionTreeClassifier()\n    dt_pred = model_metrics(decision_tree, kfold, X_train, X_test, y_train, y_test, test_df)\n    #submission_file (dt_pred)\n    \n    ###################################################################################################### Random Forest Classifier\n    print(); print(BOLD + 'Random Forest Classifier Model:' + END)\n    random_forest = RandomForestClassifier()\n\n    rf_pred = model_metrics(random_forest, kfold, X_train, X_test, y_train, y_test, test_df)\n    #submission_file (rf_pred)\n    \n    ###################################################################################################### Extra Trees Classifier\n    print(); print(BOLD + 'Extra Trees Classifier Model:' + END)\n    extra_trees = ExtraTreesClassifier()\n\n    et_pred = model_metrics(extra_trees, kfold, X_train, X_test, y_train, y_test, test_df)\n    #submission_file (et_pred)\n    \n    ###################################################################################################### AdaBoost Classifier\n    print(); print(BOLD + 'AdaBoost Classifier Model:' + END)\n    ada_boost = AdaBoostClassifier()\n\n    ab_pred = model_metrics(ada_boost, kfold, X_train, X_test, y_train, y_test, test_df)\n    #submission_file (ab_pred)\n    \n    ###################################################################################################### SVC Classifier\n    print(); print(BOLD + 'SVC Classifier Model:' + END)\n    svc = SVC()\n\n    svc_pred = model_metrics(svc, kfold, X_train, X_test, y_train, y_test, test_df)\n    #submission_file (svc_pred)\n    \n    ","f060650c":"basic_classifiers( X_train, X_test, y_train, y_test, test_df)","01a629f3":"def RandomForest_GridSearch(X_train, X_test, y_train, y_test, test_df):\n    print(); print(BOLD + 'Grid Search with Random Forest Classifier Model:' + END)\n    kfold=5\n    rf_params = {\n        #'n_estimators': [10, 50, 100, 150, 200, 250],\n        'max_features':[2, 3, 5, 7, 8],\n        #'max_depth': [1, 2, 3, 4, 5, 8],\n        #'criterion':['gini', 'entropy'],\n    }\n\n    random_forest = RandomForestClassifier(n_estimators=100)\n    gs = GridSearchCV(random_forest, param_grid=rf_params, cv=5, verbose = 1)\n    gs_pred = model_metrics(gs, kfold, X_train, X_test, y_train, y_test, test_df)\n    #submission_file (gs_pred)\n    return gs.best_estimator_","8990d7cf":"rf_gs_best_estimator = RandomForest_GridSearch(X_train, X_test, y_train, y_test, test_df)","f83bbd76":"def ExtraTrees_GridSearch(X_train, X_test, y_train, y_test, test_df):\n    print(); print(BOLD + 'Grid Search with Extra Trees Model:' + END)\n    # Scaling \n    \n    \n    kfold=5\n    rf_params = {\n        #'n_estimators': [10, 100, 400, 800, 1100, 1850],\n        #'max_features':['auto'],\n        'max_depth': [1, 2, 3, 4, 5, 8],\n        #'criterion':['gini'],\n    }\n\n    extra_trees = ExtraTreesClassifier(n_estimators=100)    \n    gs = GridSearchCV(extra_trees, param_grid=rf_params, cv=5, verbose = 1)\n    gs_pred = model_metrics(gs, kfold, X_train, X_test, y_train, y_test, test_df)\n    \n    #submission_file (gs_pred)\n    return gs.best_estimator_","554d6968":"et_gs_best_estimator = ExtraTrees_GridSearch(X_train, X_test, y_train, y_test, test_df)","3c3fd1f7":"# second best model \net_gs_best_estimator","27d50a04":"def Best_Score_model (X_train, X_test, y_train, y_test, test_df):\n    print(); print(BOLD + 'Best Score Model (RandomForestClassifier):' + END)\n    opt_model = RandomForestClassifier(criterion='gini',n_estimators=1850,\n                                        max_depth=7,min_samples_split=6,\n                                        min_samples_leaf=6, max_features='auto',\n                                        oob_score=True, random_state=42,\n                                        n_jobs=-1, verbose=1)\n                                 \n\n    opt_pred = model_metrics(opt_model, kfold, X_train, X_test, y_train, y_test, test_df)\n    #submission_file (opt_pred)","9c6f0064":"Best_Score_model (X_train, X_test, y_train, y_test, test_df)","c59253f4":"Sometimes there are a difference between the size of the family and the size of the travelling group!! why? \nMaybe it's because they have friends travelling with them or mabe some maids and helpers.","176ac219":"#### 11. Summary Statistics\nTranspose the output of pandas `describe` method to create a quick overview of each numeric feature.","0840f538":"## Feature Engineering \nThere are alot of columns in this dataframe that contain some information about the passengers:\n- we can derive titles from names\n- we can derive family names from names\n- we can get the family size from the Sibsp and parch columns\n- we can get family survival rate for each family\n- we can get the Deck area from the Cabin\n- We can get the group size from the ticket number\n- we can get the group survival rate for each group\n- we can get if the female is a mother or not\n- we can get if the passenger is traveling alone or not","25bebd6e":"## Changing some of the data types","5262d8da":"#### 1. Read CSV file","f8a7bdbe":"## Combining thre two dataframes ","d6bbd400":"#### 5. What are your data types? ","a9917346":"## Survival Rate:","9b8b53a0":"## Conclusion and Recommendations\n\nAs a second project in our Data Science Immersive Course with General Assembly and MiSK Academy, we were asked to finish this \"Titanic\" Competition on Kaggle, We used multiple data cleaning methods, employed EDA methods including a good number of visualizations, to get to know the data well. Finally, we applied multiple machine learning methods in order to predict if the passenger survived or not of the houses in the test data set.The classification models that we used are KNN, LogisticRegression,  DecisionTree, AdaBoost, SVC , Extra Trees Model, and RandomForest with and without Grid Search. We acchived our best score with a Random Forest Classifier. \n\nAccuracy  = 0.79904","b20e81de":"#### 4a. How complete is the data? and any Issues","7d48a0b9":"## Visualize Correlation of the data","51aab967":"## Problem Statment","ef427d5b":"## Preprocessing and Modeling","c59e50e4":"## The Distribution of Numerical Columns in the Dataframe","61433010":"when multible people have the same ticket number, they will have the same fare as well, so it it safe to assume that they divide the fare between themselves.\nTherefore the actual individual fare is Fare \/ group_size_by_ticket","132a8b46":"#### 3. Briefly describe the data\n\nTake your time looking through the data and briefly describe the data in the markdown cell below. Note things about what the columns might mean, and the general information that is conveyed in the dataframe.","7df44f96":"## Kaggle Submission File","ce22d9a9":"## Evaluation and Conceptual Understanding\n\nAfter evaluating all of the applied models, and the feature engineering methods,  we concluded that it is very hard to get a score higher than 0.79 in Kaggle. The best model that achieved the accuracy of 0.79904 was  RandomForest classifier with a grid search. The Extra tree with grid search achieved a similar result with an accuracy of 0.794. Other models like KNN and logistic regression did not perform well. Decision tree and random forest alone had overfitting as the train scores were too hight compared to the test. The below scores were for the best performing model (Random Forest Classifier)\n\nCV scores:  [0.83229814 0.86956522 0.7875     0.8375     0.82389937]\nCV Standard Deviation:  0.026344977444281922\n\nCV Mean score:  0.83015254502129\nTrain score:    0.8439450686641697\nTest score:     0.7888888888888889\n\nConfusion Matrix: \n\n[[47  8]\n\n [11 24]]\n \nClassification Report:  \n              precision    recall  f1-score   support\n\n           0       0.81      0.85      0.83        55\n           1       0.75      0.69      0.72        35\n\n    accuracy                           0.79        90\n   macro avg       0.78      0.77      0.77        90\nweighted avg       0.79      0.79      0.79        90\n","8465f715":"## Data Cleaning and Exploratory Data Analysis","81205a62":"## Executive Summary\n\nAs a second project in our Data Science Immersive Course with General Assembly and MiSK Academy, we were asked to finish this \"Titanic\" Competition on Kaggle, We used multiple data cleaning methods, employed EDA methods including a good number of visualizations, to get to know the data well. Finally, we applied multiple machine learning methods in order to predict if the passenger survived or not of the houses in the test data set. We acchived our best score with a Random Forest Classifier.\n\nAccuracy  = 0.79904","e3704fa7":"### Contents:\n- [Datasets Description](#Datasets-Description)\n- [Data Import & Cleaning](#Data-Import-and-Cleaning)\n- [Exploratory Data Analysis](#Exploratory-Data-Analysis)\n- [Data Visualization](#Visualize-the-data)\n- [Descriptive and Inferential Statistics](#Descriptive-and-Inferential-Statistics)\n- [Outside Research](#Outside-Research)\n- [Conclusions and Recommendations](#Conclusions-and-Recommendations)","ca3a751b":"#### 7. Create a data dictionary","f9d07e57":"|Feature|Type|Dataset|Description|\n|---|---|---|---|\n|PassengerId|int|df|Passenger id|\n|Survived|int|df|Survival as 0 = No, 1 = Yes|\n|Pclass|int|df|Ticket class as 1 = 1st, 2 = 2nd, 3 = 3rd|\n|Name|object|df|Name of the passenger|\n|Sex|object|df|Sex \/ Gender|\n|Age|float|df|Age in years|\n|SibSp|int|df|Number of siblings \/ spouses aboard the Titanic|\n|Parch|int|df|Number of parents \/ children aboard the Titanic|\n|Ticket|object|df|Ticket number|\n|Fare|float|df|Passenger fare|\n|Cabin|object|df|Cabin number|\n|Embarked|object|df|Port of Embarkation as C = Cherbourg, Q = Queenstown, S = Southampton|","e8684469":"# Data Cleaning","0e7ae0c0":"### Thank you very much,\n\n#### Raghad, Fatmah, Hessah","2717c86e":"## Changing Some Column types to get better results","e1a374d7":"The sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew. While there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others. We will use machine learning to create a predictive model that predicts which passengers survived the Titanic shipwreck using passenger data (ie name, age, gender, socio-economic class, etc).","8fa2be80":"## Applying Machine learning models for predictions","96b0bd98":"#### Use Seaborn's heatmap with pandas `.corr()` to visualize correlations between all numeric features","f5c508e0":"### Filling Age missing data\nwe tried filling the Age missing data by different ways, but the filling with considering the sex and pclass gave use the highest score in Kaggle","d7702abc":"<img src=\"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/5\/5d\/Titanic_side_plan_annotated_English.png\" style=\"height: 300px; width: 900px\">","3b78fe1d":"#### 2. Display data","720948a5":"## Datasets Description\n\n- The training set should be used to build your machine learning models. For the training set, we were provided with the target for each passenger.\n\n- The test set should be used to see how well your model performs on unseen data. For the test set, we do not provide the ground truth for each passenger. It is your job to predict these outcomes. ","f7e7a286":"<img src=\"http:\/\/imgur.com\/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n\n# Project 2: Kaggle Challenges with Titanic Survival (Classification)\n\n\n\n### Group 5:\n\n- Raghad Alharbi\n- Fatimah Aljohani\n- Hessah Hamed Alkhattabi\n\n<img src=\"https:\/\/cdn.britannica.com\/s:700x500\/79\/4679-050-BC127236\/Titanic.jpg\" style=\"height: 300px; width: 900px\">\n","773d8944":"## Kaggle Competition Score:"}}