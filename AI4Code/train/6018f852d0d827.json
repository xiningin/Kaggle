{"cell_type":{"b396759b":"code","251ec23c":"code","f29a2b4b":"code","a112c490":"code","f66a0bea":"code","de166638":"code","5ecb7013":"code","a9cee776":"code","fd31a5aa":"code","f189bce9":"code","55d06c0f":"code","45fa7f17":"code","a12d09f1":"code","146e0e29":"code","367a161c":"code","7c05775d":"code","8cf71473":"code","2e24e92a":"code","16123e28":"code","81de57a2":"code","218c1e1f":"code","6a881386":"code","46f6f592":"markdown","b312c0e4":"markdown","3144bd5c":"markdown","17853886":"markdown"},"source":{"b396759b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\n\n## basic imports\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\n\n# sklearn imports\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder, OrdinalEncoder\nfrom category_encoders import TargetEncoder, LeaveOneOutEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix\nfrom sklearn.model_selection import KFold\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer, make_column_transformer\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier,GradientBoostingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom xgboost import XGBClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.model_selection import StratifiedShuffleSplit,train_test_split\n\n# Keras imports \nfrom keras.models import Sequential\nfrom keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\nfrom keras.utils import np_utils\n\n","251ec23c":"# loading data and getting shape\ndf_train = pd.read_csv(\"..\/input\/cat-in-the-dat\/train.csv\") \ndf_test = pd.read_csv(\"..\/input\/cat-in-the-dat\/test.csv\")\ndf_sample_submission = pd.read_csv(\"..\/input\/cat-in-the-dat\/sample_submission.csv\")\nprint(df_train.shape, df_test.shape, df_sample_submission.shape)\n","f29a2b4b":"## analysing the binary features\nbinary = [ 'bin_0', 'bin_1', 'bin_2', 'bin_3', 'bin_4']\nfor i in binary:\n    print(df_train[i].value_counts())\n","a112c490":"nominal = ['nom_0', 'nom_1','nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\nfor i in nominal:\n    print(f'{i}: {df_train[i].value_counts()}')","f66a0bea":"## Do not worth work with 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9' because the processing will be very hard\nnominal = ['nom_0', 'nom_1','nom_2', 'nom_3', 'nom_4', 'nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9']\nfor i in nominal:\n    print(f'The diferent values in {i} feature is: {df_train[i].value_counts().count()}')","de166638":"ordinal = ['ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5', 'day', 'month']\nfor i in ordinal:\n    print(f'{i}: {df_train[i].value_counts()}')","5ecb7013":"## Do not worth work with 'ord_3', 'ord_4', 'ord_5' because the processing will be very hard\nordinal = ['ord_0', 'ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5', 'day', 'month']\nfor i in ordinal:\n    print(f'The diferent values in {i} feature is: {df_train[i].value_counts().count()}')","a9cee776":"## Slicing for validate the best model \n## using only 5000 lines to validate ... to spend my time\nX = df_train.drop(['id', 'target'], axis = 1).head(10000)\nX_teste= df_test.drop('id', axis = 1).head(10000)\ny = df_train['target'].values\n","fd31a5aa":"X.shape, X_teste.shape","f189bce9":"## let's transform to categorical data\ncolumns = ['bin_3', 'bin_4', 'nom_0', 'nom_1', 'nom_2', 'nom_3', 'nom_4','nom_5', 'nom_6', 'nom_7', 'nom_8', 'nom_9', 'ord_0','ord_1', 'ord_2', 'ord_3', 'ord_4', 'ord_5']\nfor i in columns:\n    X[i] = pd.Categorical(X[i]).codes\n    X_teste[i] = pd.Categorical(X_teste[i]).codes\n    \nX = X.values\nX_teste = X_teste.values    \n\nohe = OneHotEncoder(categorical_features=[5, 6,7,8,9])\nX = ohe.fit_transform(X).toarray()\nX_teste = ohe.fit_transform(X_teste).toarray()","55d06c0f":"X.shape, X_teste.shape","45fa7f17":"%%time\n## Trying to validade the model here... testing with some algoritms and verifying the best perform\nkfold = StratifiedKFold(n_splits=5, shuffle = True, random_state=0)\nmodelos = {'Logistic': LogisticRegression(solver = 'lbfgs', C =  0.1, max_iter = 5000), 'naive': GaussianNB(), 'random': RandomForestClassifier(n_estimators=100), 'tree': DecisionTreeClassifier(), 'knn': KNeighborsClassifier(), 'svc': SVC(gamma='scale'),\n          'neural': MLPClassifier(learning_rate='adaptive', max_iter=2000)}\nresultados = {}\ndesvio_padrao = {}\nmatrizes = []\nfor i, j in modelos.items():\n    for ind_treinamento, ind_teste in kfold.split(X, np.zeros(shape = (X.shape[0], 1))):\n        modelo = j\n        modelo.fit(X[ind_treinamento], y[ind_treinamento])\n        previsoes = modelo.predict(X[ind_teste])\n        precisao = accuracy_score(y[ind_teste], previsoes)\n        matrizes.append(confusion_matrix(y[ind_teste], previsoes))        \n        resultados[i] = np.mean(precisao)\n        desvio_padrao[i] =  np.std(precisao)\n    matriz_final = np.mean(matrizes, axis = 0)\n    print(f'Resultados para o modelo {i}: {resultados[i]}')\n    print(f'Desvio padr\u00e3o para o modelo {i}: {desvio_padrao[i]}')\n    print(matriz_final)","a12d09f1":"## analysing better approach\n## not good results until here and the processing is very hard\nsns.barplot(list(modelos.keys()), list(resultados.values()))\n\nplt.grid()","146e0e29":"modelo_svc = SVC(gamma='scale', verbo)\nmodelo_svc.fit(X, y)\npredicao = modelo_svc.predict(X_teste)\ndf_submission = pd.DataFrame({'id': df_test['id'], 'target': predicao})\ndf_submission.to_csv('predict_svc.csv', index = False)","367a161c":"modelo_random = RandomForestClassifier(n_estimators=100,  verbose = True)\nmodelo_random.fit(X, y)\npredicao = modelo_random.predict(X_teste)\ndf_submission = pd.DataFrame({'id': df_test['id'], 'target': predicao})\ndf_submission.to_csv('predict_random.csv', index = False)","7c05775d":"# Organizing data\ntarget = df_train['target']\ntrain_id = df_train['id']\ntest_id = df_test['id']\ndf_train.drop(['target', 'id'], axis=1, inplace=True)\ndf_test.drop('id', axis=1, inplace=True)\n\nprint(df_train.shape)\nprint(df_test.shape)","8cf71473":"%%time\n\n# using Dummies variables in the model and expand a lot the dimensions  - sparse matrix\ntraintest = pd.concat([df_train, df_test])\ndummies = pd.get_dummies(traintest, columns=traintest.columns, drop_first=True, sparse=True)\ndf_train_ohe = dummies.iloc[:df_train.shape[0], :]\ndf_test_ohe = dummies.iloc[df_train.shape[0]:, :]\n\nprint(df_train_ohe.shape)\nprint(df_test_ohe.shape)","2e24e92a":"dummies.shape","16123e28":"%%time\n## using tocoo() to transform in coordinate format and tocsr() to compress matrix\nX = df_train_ohe.sparse.to_coo().tocsr()\nX_teste = df_test_ohe.sparse.to_coo().tocsr()\ny = target.values\n","81de57a2":"X, X_teste","218c1e1f":"%%time\n## functions to create the models - LogisticRegression and XgbootClassifier\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score as auc\nfrom sklearn.linear_model import LogisticRegression\n\n# Model\ndef run_cv_model(train, test, target, model_fn, params={}, eval_fn=None, label='model'):\n    kf = KFold(n_splits=5)\n    fold_splits = kf.split(train, target)\n    cv_scores = []\n    pred_full_test = 0\n    pred_train = np.zeros((train.shape[0]))\n    i = 1\n    for dev_index, val_index in fold_splits:\n        print('Started ' + label + ' fold ' + str(i) + '\/5')\n        dev_X, val_X = train[dev_index], train[val_index]\n        dev_y, val_y = target[dev_index], target[val_index]\n        params2 = params.copy()\n        pred_val_y, pred_test_y = model_fn(dev_X, dev_y, val_X, val_y, test, params2)\n        pred_full_test = pred_full_test + pred_test_y\n        pred_train[val_index] = pred_val_y\n        if eval_fn is not None:\n            cv_score = eval_fn(val_y, pred_val_y)\n            cv_scores.append(cv_score)\n            print(label + ' cv score {}: {}'.format(i, cv_score))\n        i += 1\n    print('{} cv scores : {}'.format(label, cv_scores))\n    print('{} cv mean score : {}'.format(label, np.mean(cv_scores)))\n    print('{} cv std score : {}'.format(label, np.std(cv_scores)))\n    pred_full_test = pred_full_test \/ 5.0\n    results = {'label': label,\n              'train': pred_train, 'test': pred_full_test,\n              'cv': cv_scores}\n    return results\n\n\ndef runLR(train_X, train_y, test_X, test_y, test_X2, params):\n    print('Train LR')\n    model = LogisticRegression(**params)\n    model.fit(train_X, train_y)\n    print('Predict 1\/2')\n    pred_test_y = model.predict_proba(test_X)[:, 1]\n    print('Predict 2\/2')\n    pred_test_y2 = model.predict_proba(test_X2)[:, 1]\n    return pred_test_y, pred_test_y2\n\n\ndef runXGB(train_X, train_y, test_X, test_y, test_X2, params):\n    print('Train XGB')\n    model = XGBoostClassifier()\n    model.fit(train_X, train_y)\n    print('Predict 1\/2')\n    pred_test_y = model.predict_proba(test_X)[:, 1]\n    print('Predict 2\/2')\n    pred_test_y2 = model.predict_proba(test_X2)[:, 1]\n    return pred_test_y, pred_test_y2\n\n### You can change the function when you call the function. In this case I call using runLR\n\nlr_params = {'solver': 'lbfgs', 'C': 0.1}\nresults = run_cv_model(X, X_teste, target, runXGB, lr_params, auc, 'lr')","6a881386":"modelo_logistic = LogisticRegression(solver = 'lbfgs', C=0.1)\nmodelo_logistic.fit(X, y)\npredicao = modelo_logistic.predict(X_teste)\ndf_submission = pd.DataFrame({'id': test_id, 'target': predicao})\ndf_submission.to_csv('predict_logistic.csv', index = False)","46f6f592":"## Result 0.80739 with LogisticRegression","b312c0e4":"## Result: 0.60 (not very good)","3144bd5c":"## Result 0.62 (not very good too)\n\n","17853886":"# **Let's try another approach**"}}