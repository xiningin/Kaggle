{"cell_type":{"71baf6dd":"code","9ad24f08":"code","29b12b69":"code","17c90c57":"code","7753305b":"code","2345696f":"code","cad8cbb8":"code","56721f31":"code","4b736b0b":"code","095e2edb":"code","b466860d":"code","3ab9deca":"code","2ef65e5c":"code","073e7cae":"code","9b3dc8c9":"code","6655b348":"code","19ad148d":"code","a58805d6":"code","d71a7880":"code","f1554ec6":"code","df35c904":"markdown","a9e4b29c":"markdown","4273a44e":"markdown","f3868ce8":"markdown","992be871":"markdown","974d4bea":"markdown","ddd797bc":"markdown","f945a629":"markdown","d275f286":"markdown","3b8a0701":"markdown","9b4989d3":"markdown","accda84d":"markdown","20448bcc":"markdown","59572476":"markdown","9d3ebc4e":"markdown","54a6e0f4":"markdown","898973d0":"markdown","fb3a7aa2":"markdown","23107434":"markdown"},"source":{"71baf6dd":"import math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# Original data set retrieved from here:\n# https:\/\/datamarket.com\/data\/set\/22u3\/international-airline-passengers-monthly-totals-in-thousands-jan-49-dec-60#!ds=22u3&display=line\n\ndata = pd.read_csv(\"..\/input\/international-airline-passengers.csv\", \n                      usecols = [1], \n                      engine = \"python\", \n                      skipfooter = 3)\n","9ad24f08":"# Print some data rows.\ndata.head()","29b12b69":"# Create a time series plot.\nplt.figure(figsize = (15, 5))\nplt.plot(data, label = \"Airline Passengers\")\nplt.xlabel(\"Months\")\nplt.ylabel(\"1000 International Airline Passengers\")\nplt.title(\"Monthly Total Airline Passengers 1949 - 1960\")\nplt.legend()\nplt.show()","17c90c57":"# Let's load the required libs.\n# We'll be using the Tensorflow backend (default).\nfrom keras.models import Sequential\nfrom keras.layers.recurrent import LSTM\nfrom keras.layers.core import Dense, Activation, Dropout\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.utils import shuffle","7753305b":"# Get the raw data values from the pandas data frame.\ndata_raw = data.values.astype(\"float32\")\n\n# We apply the MinMax scaler from sklearn\n# to normalize data in the (0, 1) interval.\nscaler = MinMaxScaler(feature_range = (0, 1))\ndataset = scaler.fit_transform(data_raw)\n\n# Print a few values.\ndataset[0:5]\ndataset.shape","2345696f":"# Using 60% of data for training, 40% for validation.\nTRAIN_SIZE = 0.60\n\ntrain_size = int(len(dataset) * TRAIN_SIZE)\ntest_size = len(dataset) - train_size\ntrain, test = dataset[0:train_size, :], dataset[train_size:len(dataset), :]\nprint(\"Number of entries (training set, test set): \" + str((len(train), len(test))))\n\n","cad8cbb8":"# FIXME: This helper function should be rewritten using numpy's shift function. See below.\ndef create_dataset(dataset, window_size = 1):\n    data_X, data_Y = [], []\n    for i in range(len(dataset) - window_size - 1):\n        a = dataset[i:(i + window_size), 0]\n        data_X.append(a)\n        data_Y.append(dataset[i + window_size, 0])\n    return(np.array(data_X), np.array(data_Y))","56721f31":"# Create test and training sets for one-step-ahead regression.\nwindow_size = 1\ntrain_X, train_Y = create_dataset(train, window_size)\ntest_X, test_Y = create_dataset(test, window_size)\nprint(\"Original training data shape:\")\nprint(train_X.shape)\n\n\n# Reshape the input data into appropriate form for Keras.\ntrain_X = np.reshape(train_X, (train_X.shape[0], 1, train_X.shape[1]))\ntest_X = np.reshape(test_X, (test_X.shape[0], 1, test_X.shape[1]))\nprint(\"New training data shape:\")\nprint(train_X.shape)","4b736b0b":"def fit_model_original(train_X, train_Y, window_size = 1):\n    model = Sequential()\n    model.add(LSTM(4, \n                   input_shape = (1, window_size)))\n    model.add(Dense(1))\n    model.compile(loss = \"mean_squared_error\", \n                  optimizer = \"adam\")\n    model.fit(train_X, \n              train_Y, \n              epochs = 100, \n              batch_size = 1, \n              verbose = 2)\n    return(model)\n\n# Define the model.\ndef fit_model_new(train_X, train_Y, window_size = 1):\n    model2 = Sequential()\n    model2.add(LSTM(input_shape = (window_size, 1), \n               units = window_size, \n               return_sequences = True))\n    model2.add(Dropout(0.5))\n    model2.add(LSTM(256))\n    model2.add(Dropout(0.5))\n    model2.add(Dense(1))\n    model2.add(Activation(\"linear\"))\n    model2.compile(loss = \"mse\", \n              optimizer = \"adam\")\n    model2.summary()\n\n    # Fit the first model.\n    model2.fit(train_X, train_Y, epochs = 100, \n              batch_size = 1, \n              verbose = 2)\n    return(model2)\n#model1=fit_model_original(train_X, train_Y)\n\nmodel2=fit_model_new(train_X, train_Y)","095e2edb":"def predict_and_score(model, X, Y):\n    # Make predictions on the original scale of the data.\n    pred_scaled =model.predict(X)\n    pred = scaler.inverse_transform(pred_scaled)\n    # Prepare Y data to also be on the original scale for interpretability.\n    orig_data = scaler.inverse_transform([Y])\n    # Calculate RMSE.\n    score = math.sqrt(mean_squared_error(orig_data[0], pred[:, 0]))\n    return(score, pred, pred_scaled)\n\nrmse_train, train_predict, train_predict_scaled = predict_and_score(model2, train_X, train_Y)\nrmse_test, test_predict, test_predict_scaled = predict_and_score(model2, test_X, test_Y)\n\nprint(\"Training data score: %.2f RMSE\" % rmse_train)\nprint(\"Test data score: %.2f RMSE\" % rmse_test)\n\ntest_predict.size","b466860d":"#print(test_X.shape)\n#print(test_X[0:1,:,:].shape)\n#print(test_X[0:1,:,:])\nX_single = test_X[0:1,:,:]\n#print(test_Y.shape)\n#print(test_Y[0:1].shape)\n#print(test_Y[0:1])\n\n# create empty array\nfrom numpy import empty\ntest_predict_at_a_time = empty([test_X.size,1])\n \nprint(\"initial X: \", X_single)\nfor i in range((test_X.size)):\n    Y_single = test_Y[i:i+1]\n    rmse_test, predict, predict_scaled = predict_and_score(model2, X_single, Y_single)\n    test_predict_at_a_time[i]= predict\n    print(\"Test data score: %.2f RMSE\" % rmse_test)\n    print(\"predicted: \", predict[0])\n    X_single = predict_scaled.copy() \n    X_single=np.reshape(X_single[0], (1, 1, 1))\n    print(\"given X: \", X_single)\ntest_predict_at_a_time[-3:]","3ab9deca":"# Start with training predictions.\ntrain_predict_plot = np.empty_like(dataset)\ntrain_predict_plot[:, :] = np.nan\ntrain_predict_plot[window_size:len(train_predict) + window_size, :] = train_predict\n\n# Add test predictions.\ntest_predict_plot = np.empty_like(dataset)\ntest_predict_plot[:, :] = np.nan\ntest_predict_plot[len(train_predict) + (window_size * 2) + 1:len(dataset) - 1, :] = test_predict\n\n\n# Add test predictions.\ntest_predict_at_a_time_plot = np.empty_like(dataset)\ntest_predict_at_a_time_plot[:, :] = np.nan\ntest_predict_at_a_time_plot[len(train_predict) + (window_size * 2) + 1:len(dataset) - 1, :] = test_predict_at_a_time\n\n# Create the plot.\nplt.figure(figsize = (15, 5))\nplt.plot(scaler.inverse_transform(dataset), label = \"True value\")\nplt.plot(train_predict_plot, label = \"Training set prediction\")\nplt.plot(test_predict_plot, label = \"Test set prediction\")\nplt.plot(test_predict_at_a_time_plot, label = \"Test set prediction at a time\")\nplt.xlabel(\"Months\")\nplt.ylabel(\"1000 International Airline Passengers\")\nplt.title(\"Comparison true vs. predicted training \/ test\")\nplt.legend()\nplt.show()\n","2ef65e5c":"SAMPLES = 5000\nPERIOD = 50\nx = np.linspace(-PERIOD * np.pi, PERIOD * np.pi, SAMPLES)\nseries = pd.DataFrame(np.sin(x))\n\nplt.figure(figsize = (15, 5))\nplt.plot(series.values[:PERIOD])\nplt.xlabel(\"x\")\nplt.ylabel(\"y\")\nplt.title(\"y = sin(x), first %d samples\" % PERIOD)\nplt.show()","073e7cae":"# Normalize data on the (-1, 1) interval.\nscaler = MinMaxScaler(feature_range = (-1, 1))\nscaled = scaler.fit_transform(series.values)\n\n# Convert to data frame.\nseries = pd.DataFrame(scaled)\n\n# Helper function to create a windowed data set.\n# FIXME: Copying & overwriting is flawed!\ndef create_window(data, window_size = 1):    \n    data_s = data.copy()\n    for i in range(window_size):\n        data = pd.concat([data, data_s.shift(-(i + 1))], \n                            axis = 1)\n        \n    data.dropna(axis=0, inplace=True)\n    return(data)\n\n# FIXME: We'll use this only for demonstration purposes.\nseries_backup = series.copy()\nt = create_window(series_backup, 1)\nt.head()","9b3dc8c9":"window_size = 50\nseries = create_window(series, window_size)\nprint(\"Shape of input data:\")\nprint(series.shape)","6655b348":"# Using 80% of data for training, 20% for validation.\n# FIXME: Need to align with example 1.\nTRAIN_SIZE = 0.80\n\nnrow = round(TRAIN_SIZE * series.shape[0])\n\ntrain = series.iloc[:nrow, :]\ntest = series.iloc[nrow:, :]\n\n# Shuffle training data.\ntrain = shuffle(train)\n\ntrain_X = train.iloc[:, :-1]\ntest_X = test.iloc[:, :-1]\n\ntrain_Y = train.iloc[:, -1]\ntest_Y = test.iloc[:, -1]\n\nprint(\"Training set shape for X (inputs):\")\nprint(train_X.shape)\nprint(\"Training set shape for Y (output):\")\nprint(train_Y.shape)","19ad148d":"train_X = np.reshape(train_X.values, (train_X.shape[0], train_X.shape[1], 1))\ntest_X = np.reshape(test_X.values, (test_X.shape[0], test_X.shape[1], 1))\n\nprint(train_X.shape)\nprint(test_X.shape)","a58805d6":"# Define the model.\nmodel2 = Sequential()\nmodel2.add(LSTM(input_shape = (window_size, 1), \n               units = window_size, \n               return_sequences = True))\nmodel2.add(Dropout(0.5))\nmodel2.add(LSTM(256))\nmodel2.add(Dropout(0.5))\nmodel2.add(Dense(1))\nmodel2.add(Activation(\"linear\"))\nmodel2.compile(loss = \"mse\", \n              optimizer = \"adam\")\nmodel2.summary()","d71a7880":"# Fit the model.\nmodel2.fit(train_X, \n          train_Y, \n          batch_size = 512,\n          epochs = 3,\n          validation_split = 0.1)","f1554ec6":"# Predict on test data.\npred_test = model2.predict(test_X)\n\n# Apply inverse transformation to get back true values.\ntest_y_actual = scaler.inverse_transform(test_Y.values.reshape(test_Y.shape[0], 1))\n\nprint(\"MSE for predicted test set: %2f\" % mean_squared_error(test_y_actual, pred_test))\n\nplt.figure(figsize = (15, 5))\nplt.plot(test_y_actual, label=\"True value\")\nplt.plot(pred_test, label=\"Predicted value\")\nplt.xlabel(\"x\")\nplt.ylabel(\"sin(x)\")\nplt.title(\"Comparison true vs. predicted test set\")\nplt.legend()\nplt.show()","df35c904":"### Results\n#### Predictions and model evaluation\n\nAs can be seen below, already the simple model performs not too poorly. The advantage of using the RMSE is that it's in the same unit as the original data, i.e. 1.000 passengers \/ month.","a9e4b29c":"#### Plotting original data, predictions, and forecast\n\nWith a plot we can compare the predicted vs. actual passenger figures.","4273a44e":"#### Data preparation","f3868ce8":"## References and links\n\n* Example 1 source code adapted from here: https:\/\/machinelearningmastery.com\/time-series-prediction-lstm-recurrent-neural-networks-python-keras\/\n* Another great post: https:\/\/towardsdatascience.com\/using-lstms-to-forecast-time-series-4ab688386b1f\n* Example 2 code adapted from the above: https:\/\/github.com\/kmsravindra\/ML-AI-experiments\/tree\/master\/AI\/LSTM-time_series\n* Good paper comparing different time series modeling techniques, including LSTM: https:\/\/arxiv.org\/pdf\/1705.09137.pdf\n* Another excellent paper: https:\/\/arxiv.org\/pdf\/1705.05690.pdf\n* Brilliant LSTM course by Nando de Freitas: https:\/\/www.youtube.com\/watch?v=56TYLaQN4N8\n","992be871":"#### Build simple LSTM model on training data\n\nThe LSTM architecture here consists of:\n\n* One input layer.\n* One LSTM layer of 4 blocks.\n* One `Dense` layer to produce a single output.\n* Use MSE as loss function.\n\nMany different architectures could be considered. But this is just a quick test, so we'll keep things nice and simple.","974d4bea":"### Next steps and things to explore\n\n* Work with de-trended, stationary time series. Does it improve performance?\n* Different window size (multiple regression). See Example 2.\n* LSTM architecture, i.e. more layers, neurons etc.\n* Impact of various hyperparameters in LSTM network on prediction accuracy.\n* Model selection steps to find the \"best\" model.","ddd797bc":"#### Get data into shape to use in Keras","f945a629":"### Next steps and things to explore\n\n* Should clean up first before doing anything else! ;)\n","d275f286":"### Results\n#### Predictions and model evaluation","3b8a0701":"# LSTM Time Series Explorations with Keras\n\nThis is a very short exploration into applying LSTM techniques using the Keras library. Code and content is based on several cool blog posts and papers; see references section for more info.\n\nThere are further notes on LSTM theory and Keras in an accompanying Slideshare: https:\/\/www.slideshare.net\/RalphSchlosser\/lstm-tutorial\n\n**FIXMEs**: \n* Compare and contrast different parametrizations or architectures.\n* Consolidate helper functions, simplify.\n\n## Example 1: Airline Passenger Data\n \nIn this example we wish to make forcasts on a  time series of international airline passengers.\n \nThe time series data forcast can be modeled as a univariate regression-type problem, concretely let ${X_t}$ denote the number of airline passengers in month $t$. Then: \n \n$$\nX_t = f(X_{t-1}, \\Theta)\n$$\n \nwhich we aim to solve using the a simple LSTM neural network. \n\nHere $X_t$ is the number of passengers at time step $t$, $X_{t-1}$ denotes  number of passengers at the previous time step, and $\\Theta$ refers to all the other model parameters, including LSTM hyperparameters.\n\n*Note*: For better readability, in the code for this as well as the next example, the predicted new value at time step $t$ is written as `Y`. \n\n### Loading and plotting the data","9b4989d3":"## Example 2: Sinus wave data\n\nFor the second example, we'll generate synthetic data from a sinus curve, i.e.: $y = sin(x)$.\n\nUnlike in the example above, now we'll build a *multiple regression* model where we treat a range of input values at previous time steps as inputs for predicting the output value at the next time step. \n\nThe number of previous time steps is called the *window size*. In the below we'll be using a window size of $50$, i.e.: \n\n$$\nX_t = f(X_{t-1}, X_{t-2}, ..., X_{t-50}, \\Theta)\n$$\n\n### Generating and plotting the data","accda84d":"In general we can observe a strong upwards trend in terms of numbers of passgengers with some seasonality component. The seasonality may be understood to conincide with holiday periods, but we'd need to have a closer look at the actual time periods to confirm this.\n\nWe could also consider de-trending the time series and applying further \"cleaning\" techniques, which would be a prerequisite e.g. in an *ARIMA* setting.\n\nHowever, for simplicity reasons we will just proceed with the data as is.\n\nThe only transformations we'll be doing are:\n\n* Scale data to the $(0, 1)$ interval for increased numerical stability.\n* Re-shape the data so we have one column as **response** (called $Y$ in the code) and another one as **predictor** variable (called $X in the code).","20448bcc":"#### Get data into shape to use in Keras","59572476":"#### Split into training \/ test set","9d3ebc4e":"Here, we have a univariate data set which records the number of airline passengers for each month.\n\nLet's now plot the time series of the data in order to get some ideas about underlying trends, seasonality etc.","54a6e0f4":"#### Split into test \/ training data\n\nAs usual, the data gets split into training and test data so we can later assess how well the final model performs. \n\nAgain, this could be much improved, e.g. using CV and more sophisticated steps to select the \"best\" model.","898973d0":"### Building the LSTM model\n\n#### Data preparation\n\nFirst, we'll demonstrate the sliding window principle using a window size of 1; subsequently we'll move on to window size 50.","fb3a7aa2":"#### Build LSTM model on training data\n\nThe model architecture used here is slightly more complex. Its elements are:\n\n* LSTM input layer with 50 units.\n* `Dropout` layer to prevent overfitting (see: http:\/\/www.cs.toronto.edu\/~rsalakhu\/papers\/srivastava14a.pdf).\n* A second LSTM layer with 256 units.\n* A further `Dropout` layer.\n* A `Dense` layer to produce a single output.\n* Use MSE as loss function.","23107434":"### Building the LSTM model\n"}}