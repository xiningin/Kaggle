{"cell_type":{"ee3c0469":"code","43640b16":"code","34714bdd":"code","417eb7a5":"code","da6861d6":"code","e00f1d38":"code","bcb7dcef":"code","0262999b":"code","b203815b":"code","da514f44":"code","3e8d5033":"code","91855a7e":"code","af9398bb":"code","8444bf1c":"code","844e2637":"code","0cf176f7":"code","c6abd23e":"code","4218b160":"code","22b5459d":"code","7b0449be":"markdown"},"source":{"ee3c0469":"!pip install autogluon ","43640b16":"!pip install scikit-learn -U","34714bdd":"# Importing core libraries\nimport numpy as np\nimport pandas as pd\nimport gc\nfrom scipy.stats import skew\n\n# Importing AutoGluon\nfrom autogluon.tabular import TabularDataset, TabularPredictor\n\n# Scikit Learn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import MiniBatchKMeans\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer","417eb7a5":"# Derived from the original script https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage \n# by Guillaume Martin\n\ndef reduce_mem_usage(df, verbose=True):\n    numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    start_mem = df.memory_usage().sum() \/ 1024**2    \n    for col in df.columns:\n        col_type = df[col].dtypes\n        if col_type in numerics:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)    \n    end_mem = df.memory_usage().sum() \/ 1024**2\n    if verbose: print('Mem. usage decreased to {:5.2f} Mb ({:.1f}% reduction)'.format(end_mem, 100 * (start_mem - end_mem) \/ start_mem))\n    return df","da6861d6":"# Loading data \nX_train = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/train.csv\").set_index('id')\nX_test = pd.read_csv(\"..\/input\/tabular-playground-series-sep-2021\/test.csv\").set_index('id')\n\ny_train = X_train['claim']\nX_train = X_train.drop('claim', axis='columns')","e00f1d38":"def get_stats_per_row(data):\n    data['mv_row'] = data.isna().sum(axis=1)\n    data['min_row'] = data.min(axis=1)\n    data['std_row'] = data.std(axis=1)\n    return data\n\ndef impute_skewed_features(data):\n    skewed_feat = data.skew()\n    skewed_feat = [*skewed_feat[abs(skewed_feat.values) > 1].index]\n\n    for feat in skewed_feat:\n        median = data[feat].median()\n        data[feat] = data[feat].fillna(median)\n        \n    return data\n\npipeline = Pipeline([\n    ('impute', SimpleImputer(strategy='mean')),\n    ('scale', StandardScaler())\n])\n\nX_train = pd.DataFrame(pipeline.fit_transform(impute_skewed_features(get_stats_per_row(X_train))),\n                       columns=X_train.columns,\n                       index=X_train.index)\nX_test = pd.DataFrame(pipeline.transform(impute_skewed_features(get_stats_per_row(X_test))),\n                      columns=X_test.columns,\n                      index=X_test.index)","bcb7dcef":"# Adding t-SNE and UMAP projections\nprj_train = pd.read_csv(\"..\/input\/really-not-missing-at-random\/train.csv\")\nprj_test = pd.read_csv(\"..\/input\/really-not-missing-at-random\/test.csv\")\n\nprojections = ['t_sne_0', 't_sne_1', 't_umap_0', 't_umap_1']\nX_train[projections] = prj_train[projections]\nX_test[projections] = prj_test[projections]","0262999b":"X_train['claim'] = y_train","b203815b":"### REDUCE MEMORY USAGE\nX_train = reduce_mem_usage(X_train)\nX_test = reduce_mem_usage(X_test)\ngc.collect()","da514f44":"VALIDATION = False\nif VALIDATION is True:\n    X_train, X_val = train_test_split(X_train, test_size=int(len(X_train) * 0.2), random_state=42)\n    train_data = TabularDataset(X_train)\n    val_data = TabularDataset(X_val)\nelse:\n    train_data = TabularDataset(X_train)\n    val_data = TabularDataset(X_train.iloc[:100_000, :])\n\nSUBSAMPLE = False\nif SUBSAMPLE is True:\n    subsample_size = 10_000  # subsample subset of data for faster demo, try setting this to much larger values\n    train_data = train_data.sample(n=subsample_size, random_state=0)\n    \ntrain_data.head()","3e8d5033":"label = 'claim'\nprint(\"Summary of target variable: \\n\", train_data[label].describe())","91855a7e":"!mkdir agModels","af9398bb":"lgbm1_params = {\n    'metric' : 'auc',\n    'max_depth' : 3,\n    'num_leaves' : 7,\n    'n_estimators' : 5000,\n    'colsample_bytree' : 0.3,\n    'subsample' : 0.5,\n    'reg_alpha' : 18,\n    'reg_lambda' : 17,\n    'learning_rate' : 0.095,\n    'device' : 'gpu',\n    'objective' : 'binary'\n}\n\nlgbm2_params = {\n    'metric' : 'auc',\n    'objective': 'binary',\n    'n_estimators': 10000,\n    'learning_rate': 0.095,\n    'subsample': 0.6,\n    'subsample_freq': 1,\n    'colsample_bytree': 0.4,\n    'reg_alpha': 10.0,\n    'reg_lambda': 1e-1,\n    'min_child_weight': 256,\n    'min_child_samples': 20,\n    'device' : 'gpu',\n    'max_depth' : 3,\n    'num_leaves' : 7\n}\n\nlgbm3_params = {\n    'metric' : 'auc',\n    'objective' : 'binary',\n    'device_type': 'gpu', \n    'n_estimators': 10000, \n    'learning_rate': 0.12230165751633416, \n    'num_leaves': 1400, \n    'max_depth': 8, \n    'min_child_samples': 3100, \n    'reg_alpha': 10, \n    'reg_lambda': 65, \n    'min_split_gain': 5.157818977461183, \n    'subsample': 0.5, \n    'subsample_freq': 1, \n    'colsample_bytree': 0.2\n}\n\ncatb1_params = {\n    'eval_metric' : 'AUC',\n    'iterations': 15585, \n    'objective': 'CrossEntropy',\n    'bootstrap_type': 'Bernoulli', \n    'od_wait': 1144, \n    'learning_rate': 0.023575206684596582, \n    'reg_lambda': 36.30433203563295, \n    'random_strength': 43.75597655616195, \n    'depth': 7, \n    'min_data_in_leaf': 11, \n    'leaf_estimation_iterations': 1, \n    'subsample': 0.8227911142845009,\n    'task_type' : 'GPU',\n    'devices' : '0',\n    'verbose' : 0\n}\n\ncatb2_params = {\n    'eval_metric' : 'AUC',\n    'depth' : 5,\n    'grow_policy' : 'SymmetricTree',\n    'l2_leaf_reg' : 3.0,\n    'random_strength' : 1.0,\n    'learning_rate' : 0.1,\n    'iterations' : 10000,\n    'loss_function' : 'CrossEntropy',\n    'task_type' : 'GPU',\n    'devices' : '0',\n    'verbose' : 0\n}\n\nxgb1_params = {\n    'eval_metric' : 'auc',\n    'lambda': 0.004562711234493688, \n    'alpha': 7.268146704546314, \n    'colsample_bytree': 0.6468987558386358, \n    'colsample_bynode': 0.29113878257290376, \n    'colsample_bylevel': 0.8915913499148167, \n    'subsample': 0.37130229826185135, \n    'learning_rate': 0.021671163563123198, \n    'grow_policy': 'lossguide', \n    'max_depth': 18, \n    'min_child_weight': 215, \n    'max_bin': 272,\n    'n_estimators': 10000,\n    'random_state': 0,\n    'use_label_encoder': False,\n    'objective': 'binary:logistic',\n    'tree_method': 'gpu_hist',\n    'gpu_id': 0,\n    'predictor': 'gpu_predictor'\n}\n\nxgb2_params = dict(\n    eval_metric='auc',\n    max_depth=3,\n    subsample=0.5,\n    colsample_bytree=0.5,\n    learning_rate=0.01187431306013263,\n    n_estimators=10000,\n    n_jobs=-1,\n    use_label_encoder=False,\n    objective='binary:logistic',\n    tree_method='gpu_hist',\n    gpu_id=0,\n    predictor='gpu_predictor'\n)","8444bf1c":"save_path = 'agModels'  # specifies folder to store trained models\npresets='best_quality'\nmetric = 'roc_auc'\nhours = 5.0\npredictor = (TabularPredictor(label=label, eval_metric=metric, path=save_path)\n             .fit(train_data,\n                  excluded_model_types = ['KNN', 'XT' ,'RF', 'NN', 'FASTAI'],\n                  hyperparameters = {'GBM': lgbm1_params, \n                                     'CAT': catb1_params,\n                                     'XGB': xgb1_params\n                                    },\n                  presets=presets,\n                  time_limit= int(60 * 60 * hours))\n            )","844e2637":"results = predictor.fit_summary(show_plot=True)","0cf176f7":"leaderboard = predictor.leaderboard(val_data)","c6abd23e":"test_data = TabularDataset(X_test)\ntest_preds = predictor.predict_proba(test_data)","4218b160":"# Predicting and submission\nsubmission = pd.DataFrame({'id':X_test.index, \n                           'claim': test_preds.iloc[:,1].ravel()})\n\nsubmission.to_csv(\"submission.csv\", index=False)","22b5459d":"submission","7b0449be":"AutoGluon is an auto-ml package, developed by J Mueller, X Shi, A Smola:\n\nMueller, Jonas, Xingjian Shi, and Alexander Smola. \"Faster, Simpler, More Accurate: Practical Automated Machine Learning with Tabular, Text, and Image Data.\" Proceedings of the 26th ACM SIGKDD International Conference on Knowledge Discovery & Data Mining. 2020.\n\nFor tabular data, AutoGluon can produce models to predict the values in one column based on the values in the other columns. With just a single call to fit(), you can achieve high accuracy in standard supervised learning tasks (both classification and regression).\n\nIn the economy of a competition it can help you to create benchmarks, get insights on models' workings and accelerate your experimentations."}}