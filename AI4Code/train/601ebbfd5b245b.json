{"cell_type":{"dc7bf145":"code","35fd4e52":"code","61d51d5b":"code","9868b6b5":"code","790b010c":"code","3bc828b7":"code","94bfa935":"code","f9ddd081":"code","bef4aecf":"code","451ec88c":"code","c9426301":"code","81a1c50f":"code","2b11c8f8":"code","18272892":"code","df1dd63c":"code","b407b4d1":"code","04b2c967":"code","8ab29455":"code","84e7c6d5":"code","c1e88c33":"code","c69b96c8":"code","cafc60d2":"code","2a45b2a1":"code","a0ca9cd0":"code","18d88195":"code","f1fd31f6":"code","55a4ab33":"code","4314aa0f":"code","566d747f":"code","d70b1642":"code","1fa98fa5":"code","7e494a30":"code","7ea94067":"markdown","48619eb5":"markdown"},"source":{"dc7bf145":"!pip install transformers\n!pip install torchtext==0.9.0\n!pip install numpy requests nlpaug\n!pip install torch>=1.8.0 fairseq>=0.9.0 sacremoses>=0.0.43 fastBPE>=0.1.0","35fd4e52":"data_path = '..\/input\/math80600aw21\/'\noutput_path = '.\/'","61d51d5b":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\n\n#data augmentation\nimport nlpaug.augmenter.word as naw\nimport nlpaug.augmenter.sentence as nas\nimport time\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n\nimport torch\nfrom torch.utils.data import random_split\nfrom torchtext.legacy.data import Field, TabularDataset, BucketIterator, Iterator\nfrom transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport logging\nlogging.getLogger(\"transformers.tokenization_utils_base\").setLevel(logging.ERROR)","9868b6b5":"#create dataframe\ntrainurl = f\"{data_path}train.csv\"\ntesturl = f\"{data_path}test.csv\"\nnodeid2paperidurl = f\"{data_path}nodeid2paperid.csv\"\ntexturl = f\"{data_path}text.csv\"\nsampleurl = f\"{data_path}sample.csv\"\ntrain_df = pd.read_csv(trainurl,names=['label', 'node idx'])\ntest_df = pd.read_csv(testurl,names=['node idx'])\nnodeid2paperid_df = pd.read_csv(nodeid2paperidurl)\ntext_df = pd.read_csv(texturl, names=['paper id', 'title', 'abstract'])\nsample_df = pd.read_csv(sampleurl)","790b010c":"#merge dataframe\n# test\ntest = pd.merge(nodeid2paperid_df ,text_df,  how = 'left', on = 'paper id')\ntest_merged = pd.merge(test_df,test,  how = 'left', on = 'node idx')\n#test_merged.drop(['paper id',\"abstract\"], axis=1,inplace=True)\ntest_merged.drop(['paper id'], axis=1,inplace=True)\n\n# train\ntrain = pd.merge(nodeid2paperid_df ,text_df,  how = 'left', on = 'paper id')\ntrain_merged = pd.merge(train_df,train,  how = 'left', on = 'node idx')\n#train_merged.drop(['node idx',\"abstract\",'paper id'], axis=1,inplace=True)\ntrain_merged.drop(['node idx','paper id'], axis=1,inplace=True)","3bc828b7":"#Data cleaning \n# tried but the accuracy was unchanged\nimport re\n\ndef RegexpTokenizer(text):\n    s=text\n    \n    s = re.sub(' +', ' ',s, re.MULTILINE)\n    s=re.sub(r\"(\\$+)(?:(?!\\1)[\\s\\S])*\\1\", \"\", s, re.MULTILINE)\n    s=re.sub(r'\\\\url{([^}]*)}', \"\", s, re.MULTILINE)\n    s=re.sub(r'\\\\emph{([^}]*)}', \"\\\\1\", s, re.MULTILINE)\n    s=re.sub(r'\\\\textit{([^}]*)}',\"\\\\1\", s, re.MULTILINE)\n    s=re.sub(r'\\\\quotes{([^}]*)}',\"\\\\1\", s, re.MULTILINE)\n    s=re.sub(r'\\\\texttt{([^}]*)}',\"\\\\1\", s, re.MULTILINE)\n    s = re.sub(\"[^0-9a-zA-Z]+\",\" \",s, re.MULTILINE)\n    #s=re.sub(r\"([\\(\\[]).*?([\\)\\]])\", \"\",s, re.MULTILINE)\n    #s=re.sub(r\"[\\\\[#@$$&{}\\]\\\"]\", \"\",s, re.MULTILINE)\n    s = re.sub(' +', ' ',s, re.MULTILINE)\n\n    return s","94bfa935":"#add fake label in the test only in the aim to use the evalution function\ntest_merged.insert(1,'label', 0, False)","f9ddd081":"#export dataset cleaned\ntrain_merged.to_csv(f'{output_path}train_cleaned.csv', index=False)\ntest_merged.to_csv(f'{output_path}test_cleaned.csv', index=False)","bef4aecf":"# function for augment training dataset\ndef TextAugmTranslate(df, label=12, iter=1):\n\n  Substituteword = naw.context_word_embs.ContextualWordEmbsAug(model_path='roberta-base', #   bert-base-uncased\n                                                              model_type='', action='substitute', \n                                                              temperature=1.0, top_k=100, top_p=None, \n                                                              name='ContextualWordEmbs_Aug', aug_min=1, \n                                                              aug_max=10, aug_p=0.3, stopwords=None, device='cuda', \n                                                              force_reload=False, optimize=None, stopwords_regex=None, \n                                                              verbose=0, silence=True)\n\n  for i in range(iter): #label example multi by 2**i\n    start = time.time()\n    print(f\"iter {i} | init shape : {df[df['label']==label].shape} of label {label} | start time {time.asctime( time.localtime(start))}\")  \n    train_aug = df[df['label']==label].copy()\n    temp = df.copy()\n\n    train_aug['title'] = train_aug['title'].apply(lambda x: Substituteword.augment(x)) # word substitute to create new title\n    train_aug['abstract'] = train_aug['abstract'].apply(lambda x: Substituteword.augment(x)) # word substitute to create new abstract\n \n    temp=pd.concat([temp,train_aug], ignore_index=True)\n    df=temp\n    end = time.time()\n\n    print(f\"new shape : {df[df['label']==label].shape} of label {label} | end time {time.asctime( time.localtime(end))}\")\n\n  return df","451ec88c":"# Load CSV file with dataset.\ntrain = pd.read_csv(f\"{output_path}train_cleaned.csv\")\n\ntest = pd.read_csv(f\"{output_path}test_cleaned.csv\")","c9426301":"# Let's have a look at the class balance.\nsns.countplot(train.label)\nplt.xlabel('labels');","81a1c50f":"# split train dataset into train, validation sets\ntrain, val= train_test_split(train,random_state=80600, test_size=0.2,shuffle =True, stratify=train['label'])","2b11c8f8":"#many data augmentation have been tried. The issue with the augmentation was the nature of the texts. The vacabulary used is too specific so the translation or the word substitution loose this specificty.\n#a better solution would have been to use more data from Arxiv articles.\n\ntrain = TextAugmTranslate(train,12,5) # label 12 example multi by 2**5\ntrain = TextAugmTranslate(train,0,2) # label 0 example multi by 2**2\ntrain = TextAugmTranslate(train,1,1) # label 1 example multi by 2**2  # best *1\n#train = TextAugmTranslate(train,7,2) # label 7 example multi by 2**2\n#train = TextAugmTranslate(train,11,2) # label 11 example multi by 2**2\n\n#train = TextAugmTranslate(train,12,3) # label 12 example multi by 2**2\n#train = TextAugmTranslate(train,14,3) # label 14 example multi by 2**2\n#train = TextAugmTranslate(train,15,3) # label 15 example multi by 2**2\n#train = TextAugmTranslate(train,18,3) # label 18 example multi by 2**2\n\n#train = TextAugmTranslate(train,17,2) # label 17 example multi by 2**2","18272892":"#tried but decrease the accuracy\n\n#train_16 = train.loc[train.loc[:,\"label\"]==16].sample(n=10000)\n#train= pd.concat([train[train['label']!=16],train_16], axis=0).sample(frac=1).reset_index(drop=True)","df1dd63c":"# Let's have a look at the class balance.\nsns.countplot(train.label)\nplt.xlabel('labels');","b407b4d1":"#concat title and abstract\ntrain['LongSentence'] = train['title'] + \" \" + train['abstract']\nval['LongSentence'] = val['title'] + \" \" + val['abstract']\ntest['LongSentence'] = test['title'] + \" \" + test['abstract']","04b2c967":"# Save preprocessed data,\ntrain.to_csv(f\"{output_path}prep_train_aug.csv\")\nval.to_csv(f\"{output_path}prep_val.csv\")\ntest.to_csv(f\"{output_path}prep_test.csv\")","8ab29455":"# Plot histogram with the length. Truncate max length to 5000 tokens.\nplt.style.use(\"ggplot\")\n\nplt.figure(figsize=(10, 8))\ntrain['length'] = train['LongSentence'].apply(lambda x: len(x.split()))\nsns.distplot(train['length'])\nplt.title('Frequence of documents of a given length', fontsize=14)\nplt.xlabel('length', fontsize=14)\nNone","84e7c6d5":"# Set random seed and set device to GPU.\ntorch.manual_seed(17)\n\nif torch.cuda.is_available():\n    device = torch.device('cuda')\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\nelse:\n    device = torch.device('cpu')\n\nprint(device)","c1e88c33":"#use SciBERT. a BERT trained on scientific papers\ndo_lower_case = True\n\nMODEL_NAME = 'allenai\/scibert_scivocab_uncased'\ntokenizer = BertTokenizer.from_pretrained(MODEL_NAME, do_lower_case=do_lower_case) #","c69b96c8":"# Set tokenizer hyperparameters.\nMAX_SEQ_LEN = 512\nBATCH_SIZE = 16\nPAD_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\nUNK_INDEX = tokenizer.convert_tokens_to_ids(tokenizer.unk_token)\n\n\n# Define columns to read.\nlabel_field = Field(sequential=False, use_vocab=False, batch_first=True)\ntext_field = Field(use_vocab=False, \n                   tokenize=tokenizer.encode, \n                   include_lengths=False, \n                   batch_first=True,\n                   fix_length=MAX_SEQ_LEN, \n                   pad_token=PAD_INDEX, \n                   unk_token=UNK_INDEX)\n\nfields = {'LongSentence' : ('LongSentence', text_field), 'label' : ('label', label_field)}\n\n\n# Read preprocessed CSV into TabularDataset and split it into train, test and valid.\ntrain_data = TabularDataset(path=f\"{output_path}\/prep_train_aug.csv\", \n                                                   format='CSV', \n                                                   fields=fields, \n                                                   skip_header=False)\n\nvalid_data = TabularDataset(path=f\"{output_path}\/prep_val.csv\", \n                                                   format='CSV', \n                                                   fields=fields, \n                                                   skip_header=False)\n\ntest_data = TabularDataset(path=f\"{output_path}\/prep_test.csv\", \n                                                   format='CSV', \n                                                   fields=fields, \n                                                   skip_header=False)\n\n# Create train and validation iterators.\ntrain_iter, valid_iter = BucketIterator.splits((train_data, valid_data),\n                                               batch_size=BATCH_SIZE,\n                                               device=device,\n                                               shuffle=True,\n                                               sort_key=lambda x: len(x.LongSentence), \n                                               sort=True, \n                                               sort_within_batch=False)\n\n# Test iterator, no shuffling or sorting required.\ntest_iter = Iterator(test_data, batch_size=BATCH_SIZE, device=device, train=False, shuffle=False, sort=False)","cafc60d2":"# Functions for saving and loading model parameters and metrics.\ndef save_checkpoint(path, model, valid_loss):\n    torch.save({'model_state_dict': model.state_dict(),\n                  'valid_loss': valid_loss}, path)\n\n    \ndef load_checkpoint(path, model):    \n    state_dict = torch.load(path, map_location=device)\n    model.load_state_dict(state_dict['model_state_dict'])\n    \n    return state_dict['valid_loss']\n\n\ndef save_metrics(path, train_loss_list, valid_loss_list, global_steps_list):   \n    state_dict = {'train_loss_list': train_loss_list,\n                  'valid_loss_list': valid_loss_list,\n                  'global_steps_list': global_steps_list}\n    \n    torch.save(state_dict, path)\n\n\ndef load_metrics(path):    \n    state_dict = torch.load(path, map_location=device)\n    return state_dict['train_loss_list'], state_dict['valid_loss_list'], state_dict['global_steps_list']","2a45b2a1":"# Model with extra layers on top of SciBERT\nclass SciBERTClassifier(torch.nn.Module):\n    def __init__(self, dropout_rate=0.3):\n        super(SciBERTClassifier, self).__init__()\n        \n        self.SciBERT = BertModel.from_pretrained('allenai\/scibert_scivocab_uncased')\n        self.d1 = torch.nn.Dropout(dropout_rate)\n        self.l1 = torch.nn.Linear(768, 64)\n        self.bn1 = torch.nn.LayerNorm(64)\n        self.d2 = torch.nn.Dropout(dropout_rate)\n        self.l2 = torch.nn.Linear(64, 20)\n        \n    def forward(self, input_ids, attention_mask):\n        x = self.SciBERT(input_ids=input_ids, attention_mask=attention_mask)\n        x = x[1]\n        x = self.d1(x)\n        x = self.l1(x)\n        x = self.bn1(x)\n        x = torch.nn.Tanh()(x)\n        x = self.d2(x)\n        x = self.l2(x)\n        #print(x.shape)\n        return x","a0ca9cd0":"def pretrain(model, \n             optimizer, \n             train_iter, \n             valid_iter, \n             scheduler = None,\n             valid_period = len(train_iter),\n             num_epochs = 5):\n    \n    # Pretrain linear layers, do not train bert\n    for param in model.SciBERT.parameters():\n        param.requires_grad = False\n    \n    model.train()\n    \n    # Initialize losses and loss histories\n    train_loss = 0.0\n    valid_loss = 0.0 \n\n    global_step = 0  \n    \n    # Train loop\n    for epoch in range(num_epochs):\n        for (source, target), _ in train_iter:\n            mask = (source != PAD_INDEX).type(torch.uint8)\n            \n            logits = model(input_ids=source,  \n                           attention_mask=mask)\n            y_pred = torch.max(logits, dim=-1)[1]\n            loss = torch.nn.CrossEntropyLoss()(logits, target)\n            loss.backward()\n            \n            # Optimizer and scheduler step\n            optimizer.step()    \n            scheduler.step()\n                \n            optimizer.zero_grad()\n            \n            # Update train loss and global step\n            train_loss += loss.item()\n            global_step += 1\n            \n            # Validation loop. Save progress and evaluate model performance.\n            if global_step % valid_period == 0:\n                model.eval()\n                \n                with torch.no_grad():                    \n                    for (source, target), _ in valid_iter:\n                        mask = (source != PAD_INDEX).type(torch.uint8)\n                        \n                        logits = model(input_ids=source, \n                                       attention_mask=mask)\n   \n                        y_pred = torch.max(logits, dim=-1)[1]\n\n                        loss = torch.nn.CrossEntropyLoss()(logits, target)\n\n                        valid_loss += loss.item()\n\n                # Store train and validation loss history\n                train_loss = train_loss \/ valid_period\n                valid_loss = valid_loss \/ len(valid_iter)\n                model.train()\n\n                # print summary\n                print('Epoch [{}\/{}], global step [{}\/{}], PreTrain Loss: {:.4f}, Val Loss: {:.4f}'\n                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_iter),\n                              train_loss, valid_loss))\n                \n                train_loss = 0.0                \n                valid_loss = 0.0\n    \n    # Set Scibert parameters back to trainable\n    for param in model.SciBERT.parameters():\n        param.requires_grad = True\n       \n    print('Pre-training done!')","18d88195":"# Training Function\n\ndef train(model,\n          optimizer,\n          train_iter,\n          valid_iter,\n          scheduler = None,\n          num_epochs = 5,\n          valid_period = len(train_iter),\n          output_path = output_path):\n    \n    # Initialize losses and loss histories\n    train_loss = 0.0\n    valid_loss = 0.0\n\n    train_loss_list = []\n    valid_loss_list = []\n\n    best_valid_loss = float('Inf')\n    \n    global_step = 0\n    global_steps_list = []\n    \n    model.train()\n    \n    # Train loop\n    for epoch in range(num_epochs):\n        for (source, target), _ in train_iter:\n            mask = (source != PAD_INDEX).type(torch.uint8)\n\n            logits = model(input_ids=source,  \n                           attention_mask=mask)\n\n            y_pred = torch.max(logits, dim=-1)[1]                   \n            \n            loss = torch.nn.CrossEntropyLoss()(logits, target)\n            \n            loss.backward()\n            \n            # Optimizer and scheduler step\n            optimizer.step()    \n            scheduler.step()\n                \n            optimizer.zero_grad()\n\n            # Update train loss and global step\n            train_loss += loss.item()\n            global_step += 1\n\n            # Validation loop. Save progress and evaluate model performance.\n            if global_step % valid_period == 0:\n                model.eval()\n                \n                with torch.no_grad():                    \n                    for (source, target), _ in valid_iter:\n                        mask = (source != PAD_INDEX).type(torch.uint8)\n\n                        logits = model(input_ids=source, \n                                       attention_mask=mask)\n                           \n                        y_pred = torch.max(logits, dim=-1)[1]\n\n                        loss = torch.nn.CrossEntropyLoss()(logits, target)         \n                        valid_loss += loss.item()\n\n                # Store train and validation loss history\n                train_loss = train_loss \/ valid_period\n                valid_loss = valid_loss \/ len(valid_iter)\n                train_loss_list.append(train_loss)\n                valid_loss_list.append(valid_loss)\n                global_steps_list.append(global_step)\n\n                # print summary\n                print('Epoch [{}\/{}], global step [{}\/{}], Train Loss: {:.4f}, Valid Loss: {:.4f}'\n                      .format(epoch+1, num_epochs, global_step, num_epochs*len(train_iter),\n                              train_loss, valid_loss))\n                \n                # checkpoint\n                if best_valid_loss > valid_loss:\n                    best_valid_loss = valid_loss\n                    save_checkpoint(output_path + 'model.pkl', model, best_valid_loss)\n                    save_metrics(output_path + 'metric.pkl', train_loss_list, valid_loss_list, global_steps_list)\n                        \n                train_loss = 0.0                \n                valid_loss = 0.0\n\n                model.train()\n\n    save_metrics(output_path + 'metric.pkl', train_loss_list, valid_loss_list, global_steps_list)\n    print('Training done!')","f1fd31f6":"# Main training loop\nNUM_EPOCHS = 6\n\nsteps_per_epoch = len(train_iter)\n\nmodel = SciBERTClassifier(0.4)\nmodel = model.to(device)\n\n\noptimizer = AdamW(model.parameters(), lr=1e-4)\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps=steps_per_epoch*1, \n                                            num_training_steps=steps_per_epoch*NUM_EPOCHS)\n\nprint(\"======================= Start pretraining ==============================\")\n\npretrain(model=model,\n         train_iter=train_iter,\n         valid_iter=valid_iter,\n         optimizer=optimizer,\n         scheduler=scheduler,\n         num_epochs=NUM_EPOCHS)\n\nNUM_EPOCHS = 6 #15\nprint(\"======================= Start training =================================\")\noptimizer = AdamW(model.parameters(), lr=2e-6) #2e-6\nscheduler = get_linear_schedule_with_warmup(optimizer, \n                                            num_warmup_steps=steps_per_epoch*2, \n                                            num_training_steps=steps_per_epoch*NUM_EPOCHS)\n\ntrain(model=model, \n      train_iter=train_iter, \n      valid_iter=valid_iter, \n      optimizer=optimizer, \n      scheduler=scheduler, \n      num_epochs=NUM_EPOCHS)","55a4ab33":"plt.figure(figsize=(10, 8))\ntrain_loss_list, valid_loss_list, global_steps_list = load_metrics(output_path + 'metric.pkl')\nplt.plot(global_steps_list, train_loss_list, label='Train')\nplt.plot(global_steps_list, valid_loss_list, label='Valid')\nplt.xlabel('Global Steps', fontsize=14)\nplt.ylabel('Loss', fontsize=14)\nplt.legend(fontsize=14)\nplt.show()","4314aa0f":"# Evaluation Function\n\ndef evaluate(model, val_loader):\n    y_pred = []\n    y_true = []\n\n    model.eval()\n    with torch.no_grad():\n        for (source, target), _ in val_loader:\n                mask = (source != PAD_INDEX).type(torch.uint8)\n                \n                output = model(source, attention_mask=mask)\n\n                preds= torch.max(output, axis=-1)[1]\n                y_pred += preds.tolist()\n                y_true += target.tolist()\n    \n    print('Classification Report:')\n    print(classification_report(y_true, y_pred, labels=range(20), digits=4))\n\n    plt.figure(figsize=(15, 15))    \n    cm = confusion_matrix(y_true, y_pred, labels=range(20))\n    ax = plt.subplot()\n    sns.heatmap(cm, annot=True, ax = ax, cmap=\"YlGnBu\", fmt=\"d\")\n    ax.set_title('Confusion Matrix')\n    ax.set_xlabel('Predicted Labels')\n    ax.set_ylabel('True Labels')\n    ax.xaxis.set_ticklabels(range(20))\n    ax.yaxis.set_ticklabels(range(20))","566d747f":"model = SciBERTClassifier()\nmodel = model.to(device)\n\nload_checkpoint(output_path + 'model.pkl', model)\n\nevaluate(model, valid_iter)","d70b1642":"test = pd.read_csv(f\"{output_path}prep_test.csv\")","1fa98fa5":"#Submission function\ndef get_submission(model, test_loader, test_ids):\n    all_preds=[]\n    model.eval()\n    with torch.no_grad():\n      for (source, target), _ in test_loader:\n                mask = (source != PAD_INDEX).type(torch.uint8)\n                \n                output = model(source, attention_mask=mask)\n                preds= torch.max(output, axis=-1)[1]\n                all_preds += preds.tolist()\n                #print(len(all_preds))\n    #test dataframe with prediction()\n    df= pd.DataFrame({\n        \"id\" : test_ids.values,\n        \"label\" : np.array(all_preds)\n    })\n    \n    df.to_csv(f\"submission.csv\",index=False)","7e494a30":"get_submission(model,test_iter, test['node idx'])","7ea94067":"<a id='0'><\/a>\n# <p style=\"background-color:lightgray; font-family:newtimeroman; font-weight: 700; font-size:300%; text-align:center; border-radius: 50px 50px;\">Text classification with SciBERT <\/p>","48619eb5":"Reference :\n\nSciBERT : https:\/\/arxiv.org\/pdf\/1903.10676v3.pdf"}}