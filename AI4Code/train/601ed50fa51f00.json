{"cell_type":{"8f7f3455":"code","2b0346e3":"code","1a19d506":"code","076b0f61":"code","3127440c":"code","7dd0b86a":"code","90b75502":"code","d21df550":"code","03497980":"code","4751f8d8":"code","b2a223dc":"code","f64eb800":"code","894070a0":"code","64e5162b":"code","12c63f62":"code","4bdde0d6":"code","7e8f7b0b":"code","043b4211":"code","0a405eae":"code","37952f16":"code","1ffcd45b":"code","115bfe12":"code","c8b2254f":"code","177a4f6f":"code","a5215246":"code","bef15b90":"code","d8c780b7":"code","fa8dd3af":"markdown","e92d5e55":"markdown","d44141ba":"markdown","edcff8b0":"markdown","18ef9335":"markdown","252304e2":"markdown","19541896":"markdown","908cc69a":"markdown","ba7b05c6":"markdown","6db0a4b3":"markdown","38a22d60":"markdown","b7a4206e":"markdown","f2a3086f":"markdown","262a5f0d":"markdown","32bf8f77":"markdown","f80b3ebb":"markdown","ed41e50a":"markdown","03e5c4f0":"markdown"},"source":{"8f7f3455":"# **Step 1: Import Python Packages** \n\n# Fastai, Librosa, Spacy, Scispacy, PySound, Seaborn, etc","2b0346e3":"!pip install scispacy\n!pip install pysoundfile\n!apt-get install libav-tools -y\n!apt-get install zip\n!pip freeze > '..\/working\/dockerimage_snapshot.txt'","1a19d506":"from fastai.text import *\nfrom fastai.vision import *\nimport spacy\nfrom spacy import displacy\nimport scispacy\nimport librosa\nimport librosa.display\nimport soundfile as sf\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import Counter\nimport IPython\nimport os\nfrom glob import glob\nfrom tqdm import tqdm\nimport numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport pylab\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","076b0f61":"# **Step 3: Define Helper Functions**\n\n# Create spectrograms and word frequency plots","3127440c":"def get_wav_info(wav_file):\n    data, rate = sf.read(wav_file)\n    return data, rate\n\ndef create_spectrogram(wav_file):\n    # adapted from Andrew Ng Deep Learning Specialization Course 5\n    data, rate = get_wav_info(wav_file)\n    nfft = 200 # Length of each window segment\n    fs = 8000 # Sampling frequencies\n    noverlap = 120 # Overlap between windows\n    nchannels = data.ndim\n    if nchannels == 1:\n        pxx, freqs, bins, im = plt.specgram(data, nfft, fs, noverlap = noverlap)\n    elif nchannels == 2:\n        pxx, freqs, bins, im = plt.specgram(data[:,0], nfft, fs, noverlap = noverlap)\n    return pxx\n\ndef create_melspectrogram(filename,name):\n    # adapted from https:\/\/www.kaggle.com\/devilsknight\/sound-classification-using-spectrogram-images\n    plt.interactive(False)\n    clip, sample_rate = librosa.load(filename, sr=None)\n    fig = plt.figure(figsize=[0.72,0.72])\n    ax = fig.add_subplot(111)\n    ax.axes.get_xaxis().set_visible(False)\n    ax.axes.get_yaxis().set_visible(False)\n    ax.set_frame_on(False)\n    S = librosa.feature.melspectrogram(y=clip, sr=sample_rate)\n    librosa.display.specshow(librosa.power_to_db(S, ref=np.max))\n    filename  = Path('\/kaggle\/working\/spectrograms\/' + name + '.jpg')\n    plt.savefig(filename, dpi=400, bbox_inches='tight',pad_inches=0)\n    plt.close()    \n    fig.clf()\n    plt.close(fig)\n    plt.close('all')\n    del filename,name,clip,sample_rate,fig,ax,S\n\ndef wordBarGraphFunction(df,column,title):\n    # adapted from https:\/\/www.kaggle.com\/benhamner\/most-common-forum-topic-words\n    topic_words = [ z.lower() for y in\n                       [ x.split() for x in df[column] if isinstance(x, str)]\n                       for z in y]\n    word_count_dict = dict(Counter(topic_words))\n    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\n    popular_words_nonstop = [w for w in popular_words if w not in stopwords.words(\"english\")]\n    plt.barh(range(50), [word_count_dict[w] for w in reversed(popular_words_nonstop[0:50])])\n    plt.yticks([x + 0.5 for x in range(50)], reversed(popular_words_nonstop[0:50]))\n    plt.title(title)\n    plt.show()\n\ndef wordCloudFunction(df,column,numWords):\n    topic_words = [ z.lower() for y in\n                       [ x.split() for x in df[column] if isinstance(x, str)]\n                       for z in y]\n    word_count_dict = dict(Counter(topic_words))\n    popular_words = sorted(word_count_dict, key = word_count_dict.get, reverse = True)\n    popular_words_nonstop = [w for w in popular_words if w not in stopwords.words(\"english\")]\n    word_string=str(popular_words_nonstop)\n    wordcloud = WordCloud(stopwords=STOPWORDS,\n                          background_color='white',\n                          max_words=numWords,\n                          width=1000,height=1000,\n                         ).generate(word_string)\n    plt.clf()\n    plt.imshow(wordcloud)\n    plt.axis('off')\n    plt.show()","7dd0b86a":"overview = pd.read_csv('..\/input\/medical-speech-transcription-and-intent\/medical speech transcription and intent\/Medical Speech, Transcription, and Intent\/overview-of-recordings.csv')\noverview = overview[['file_name','phrase','prompt','overall_quality_of_the_audio','speaker_id']]\noverview=overview.dropna()\noverviewAudio = overview[['file_name','prompt']]\noverviewAudio['spec_name'] = overviewAudio['file_name'].str.rstrip('.wav')\noverviewAudio = overviewAudio[['spec_name','prompt']]\noverviewText = overview[['phrase','prompt']]\nnoNaNcsv = '..\/input\/medical-speech-transcription-and-intent\/medical speech transcription and intent\/Medical Speech, Transcription, and Intent\/overview-of-recordings.csv'\nnoNaNcsv = pd.read_csv(noNaNcsv)\nnoNaNcsv = noNaNcsv.dropna()\nnoNaNcsv = noNaNcsv.to_csv('overview-of-recordings.csv',index=False)\nnoNaNcsv","90b75502":"overview[110:120]","d21df550":"sns.set_style(\"whitegrid\")\npromptsPlot = sns.countplot(y='prompt',data=overview)\npromptsPlot\n\nqualityPlot = sns.FacetGrid(overview,aspect=2.5)\nqualityPlot.map(sns.kdeplot,'overall_quality_of_the_audio',shade= True)\nqualityPlot.set(xlim=(2.5, overview['overall_quality_of_the_audio'].max()))\nqualityPlot.set_axis_labels('overall_quality_of_the_audio', 'Proportion')\nqualityPlot","03497980":"overview[62:63]","4751f8d8":"en_core_sci_sm = '..\/input\/scispacy-pretrained-models\/scispacy pretrained models\/Scispacy Pretrained Models\/en_core_sci_sm-0.1.0\/en_core_sci_sm\/en_core_sci_sm-0.1.0'\nnlp = spacy.load(en_core_sci_sm)\ntext = overview['phrase'][62]\ndoc = nlp(text)\nprint(list(doc.sents))\nprint(doc.ents)\ndisplacy.render(next(doc.sents), style='dep', jupyter=True,options = {'compact': True, 'word_spacing': 45, 'distance': 90})","b2a223dc":"IPython.display.Audio('..\/input\/medical-speech-transcription-and-intent\/medical speech transcription and intent\/Medical Speech, Transcription, and Intent\/recordings\/test\/1249120_20518958_23074828.wav')","f64eb800":"overview[118:119]","894070a0":"en_core_sci_sm = '..\/input\/scispacy-pretrained-models\/scispacy pretrained models\/Scispacy Pretrained Models\/en_core_sci_sm-0.1.0\/en_core_sci_sm\/en_core_sci_sm-0.1.0'\nnlp = spacy.load(en_core_sci_sm)\ntext = overview['phrase'][118]\ndoc = nlp(text)\nprint(list(doc.sents))\nprint(doc.ents)\ndisplacy.render(next(doc.sents), style='dep', jupyter=True,options = {'compact': True, 'word_spacing': 45, 'distance': 90})","64e5162b":"IPython.display.Audio('..\/input\/medical-speech-transcription-and-intent\/medical speech transcription and intent\/Medical Speech, Transcription, and Intent\/recordings\/test\/1249120_43788827_53247832.wav')","12c63f62":"plt.figure(figsize=(15,15))\nwordCloudFunction(overview,'phrase',10000000)","4bdde0d6":"plt.figure(figsize=(10,10))\nwordBarGraphFunction(overview,'phrase',\"Most Common Words in Medical Text Transcripts\")","7e8f7b0b":"np.random.seed(7)\npath = Path('..\/input\/medical-speech-transcription-and-intent\/medical speech transcription and intent\/Medical Speech, Transcription, and Intent\/')\ndata_clas = (TextList.from_csv(path, 'overview-of-recordings.csv', \n                               cols='phrase')\n                   .random_split_by_pct(.2)\n                   .label_from_df(cols='prompt')\n                   .databunch(bs=42))\nMODEL_PATH = \"\/tmp\/model\/\"\nlearn = text_classifier_learner(data_clas,model_dir=MODEL_PATH,arch=AWD_LSTM)\nlearn.fit_one_cycle(5)","043b4211":"learn.unfreeze()\nlearn.fit_one_cycle(5)","0a405eae":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(10,10), dpi=60)","37952f16":"testAudio = \"..\/input\/medical-speech-transcription-and-intent\/medical speech transcription and intent\/Medical Speech, Transcription, and Intent\/recordings\/train\/1249120_44176037_58635902.wav\"\nx = create_spectrogram(testAudio)","1ffcd45b":"filename = \"..\/input\/medical-speech-transcription-and-intent\/medical speech transcription and intent\/Medical Speech, Transcription, and Intent\/recordings\/train\/1249120_44176037_58635902.wav\"\nclip, sample_rate = librosa.load(filename, sr=None)\nfig = plt.figure(figsize=[5,5])\nS = librosa.feature.melspectrogram(y=clip, sr=sample_rate)\nlibrosa.display.specshow(librosa.power_to_db(S, ref=np.max))","115bfe12":"!mkdir \/kaggle\/working\/spectrograms\n\nData_dir_train=np.array(glob(\"..\/input\/medical-speech-transcription-and-intent\/medical speech transcription and intent\/Medical Speech, Transcription, and Intent\/recordings\/train\/*\"))\nData_dir_test=np.array(glob(\"..\/input\/medical-speech-transcription-and-intent\/medical speech transcription and intent\/Medical Speech, Transcription, and Intent\/recordings\/test\/*\"))\nData_dir_val=np.array(glob(\"..\/input\/medical-speech-transcription-and-intent\/medical speech transcription and intent\/Medical Speech, Transcription, and Intent\/recordings\/validate\/*\"))\n\nfor file in tqdm(Data_dir_train):\n    filename,name = file,file.split('\/')[-1].split('.')[0]\n    create_melspectrogram(filename,name)\nfor file in tqdm(Data_dir_test):\n    filename,name = file,file.split('\/')[-1].split('.')[0]\n    create_melspectrogram(filename,name)\nfor file in tqdm(Data_dir_val):\n    filename,name = file,file.split('\/')[-1].split('.')[0]\n    create_melspectrogram(filename,name)","c8b2254f":"path = Path('\/kaggle\/working\/')\nnp.random.seed(7)\ndata = ImageDataBunch.from_df(path,df=overviewAudio, folder=\"spectrograms\", valid_pct=0.2, suffix='.jpg',\n        ds_tfms=get_transforms(), size=299, num_workers=0).normalize(imagenet_stats)\nlearn = create_cnn(data, models.resnet50, metrics=accuracy)\nlearn.fit_one_cycle(10)","177a4f6f":"learn.unfreeze()\nlearn.lr_find()\nlearn.recorder.plot()","a5215246":"learn.fit_one_cycle(50)","bef15b90":"interp = ClassificationInterpretation.from_learner(learn)\ninterp.plot_confusion_matrix(figsize=(10,10), dpi=60)","d8c780b7":"!zip -r spectrograms.zip \/kaggle\/working\/spectrograms\/\n!rm -rf spectrograms\/*","fa8dd3af":"**Part 3 of 3: Classify Ailment from Audio Description**\n\nNext I will convert the .wav files into .jpg spectrograms and then again I will attempt to classify the audio descriptions according to the category of the ailment that is being described.","e92d5e55":"Prior work has shown that it can be advantageous to transform the spectrogram into a melspectrogram before proceeding with computer vision applications.  For more information, see: https:\/\/en.wikipedia.org\/wiki\/Mel-frequency_cepstrum.\n\nHere is a representative melspectrogram:","d44141ba":"**Summary** \n\nHere we use the [fastai.text_classifier_learner()](https:\/\/docs.fast.ai\/text.learner.html) functions to classify text descriptions of medical symptoms according to the category of the ailment  being described.  Likewise, we use the [fastai.create_cnn()](https:\/\/docs.fast.ai\/vision.learner.html) functions to classify melspectrogram audio descriptions of medical symptoms according to the category of the ailment being described in the audio file.\n\n\nPlease note that some of the labels are incorrect and some of the audio files have poor quality.  To improve the models that are produced by this kernel I would recommend cleaning the dataset in much more detail.","edcff8b0":"I stumbled across an interesting dataset containing verbal descriptions of medical symptoms (.wav, audio data) paired with text transcriptions (.csv, text data) and labeled according to the category of the ailment.  I have never worked with audio data before and so I decided to explore this dataset.\n\nHere I use the fastai library to classify medical text and audio according to the category of the ailment being described.\n","18ef9335":"A spectrogram is a visual representation of a sound. The x-axis represents time, the y-axis represents frequency, and the third dimension (intensity or color) represents the amplitutde of a specific frequency at a specific point in time.\n","252304e2":"**Credit:**","19541896":"**Medical Text and Audio Classification with Fastai**","908cc69a":"Then I use the [fastai.create_cnn()](https:\/\/docs.fast.ai\/vision.learner.html) to classify the melspectrogram images according to the category of the ailment that is being described in the audio description.","ba7b05c6":"These are the most common words that are described in the text descriptions:\n","6db0a4b3":"Here is another example:\n","38a22d60":"In the end we were able to classify the category of the ailment being described from the audio description of the symptoms and we were able to do so with an accuracy that was much better than random chance (albeit much less accurate than earlier when we performed this same classification task using the text transcriptions instead of the audio files).\n","b7a4206e":"Next I convert all of the .wav audio files into .jpg melspectrogram files.","f2a3086f":"**Part 2 of 3: Classify Ailment from Text Description**\n\nNext I will use the [fastai.text_classifier_learner()](https:\/\/docs.fast.ai\/text.learner.html) functions to categorize the text descriptions according to the ailment category being described.","262a5f0d":"The categories of ailments and the quality of the audio descriptions are described below:","32bf8f77":"And here we zoom in on one specific example:\n","f80b3ebb":"**Part 1 of 3: Exploratory Data Analysis and Data Visualization**\n\n\nThe dataset consists of verbal descriptions of medical symptoms (.wav, audio data) paired with text transcriptions (.csv, text data) and labeled according to the category of the ailment.\n\nHere is a sample of the .csv file that accompanies the .wav audio files.","ed41e50a":"Inspired by:\n* [Jeremey Howard's Deep Learning Course](https:\/\/course.fast.ai\/) (Lesson 1: Fastai and Convolutional Neural Networks; Lesson 4: NLP; Tabular data; Collaborative filtering; Embeddings)\n* [Andrew Ng's Deep Learning Course](https:\/\/www.coursera.org\/specializations\/deep-learning) (Lesson 5: Spectrograms and Audio Data)\n\nWith select functions adapted from:\n\n* [most-common-forum-topic-words](https:\/\/www.kaggle.com\/benhamner\/most-common-forum-topic-words) (plot word frequencies)\n* [play-audio-read-the-files-create-a-spectrogram](https:\/\/www.kaggle.com\/vbookshelf\/play-audio-read-the-files-create-a-spectrogram) (preview audio files)\n* [sound-classification-using-spectrogram-images](https:\/\/www.kaggle.com\/devilsknight\/sound-classification-using-spectrogram-images) (create spectrograms)\n\n\n\n","03e5c4f0":"It worked! We were able to classify the category of the ailment being described from the text description of the symptoms and we were able to do so with a high accuracy.  \n\nNow let's try to do the same thing but with the audio descriptions."}}