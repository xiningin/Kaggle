{"cell_type":{"18ce9cd5":"code","b1b2692b":"code","c60b0e1f":"code","1dfff032":"code","202ae7c0":"code","46bc739a":"code","6a84cd99":"code","38cd6fd3":"code","c46eecb5":"code","fa7297b3":"code","4bc7f442":"code","3c989e91":"code","65b891d7":"code","3ee04cb0":"code","57bca1e0":"code","4c572bcf":"code","61b44cd0":"code","e7c87391":"code","23821a1e":"code","52710e35":"code","93ed7be5":"code","14c19fd9":"code","504d47b3":"code","d1840bde":"code","22c160bd":"code","1e2bfa88":"code","0cfb63e4":"code","088e3ac4":"code","c3123753":"code","292142a1":"code","396707c7":"code","ce11ec17":"code","8cda714f":"code","5a894fb8":"code","1ccc0023":"code","63201b6b":"code","c1a1a5f5":"code","79ca9f8f":"code","5dbf552e":"code","0e5a12df":"code","b9c7f9fd":"code","719e8722":"code","d052732c":"code","64a5eed1":"code","972cd65e":"code","723ff7ec":"code","64e8f072":"code","d733818e":"markdown","07ce48ae":"markdown","41852354":"markdown","932bc14c":"markdown","c105b66e":"markdown","eea974c1":"markdown","c4101e2a":"markdown","69260036":"markdown","07d3871f":"markdown","7b7918e8":"markdown","2d30372d":"markdown","475ac668":"markdown","35873897":"markdown","c87dff98":"markdown","e9e367a9":"markdown","f5cffd4e":"markdown","c7d2801a":"markdown","d4476670":"markdown","f9e0ad58":"markdown","f27d545b":"markdown","927be178":"markdown","bb0bc67d":"markdown","e48b65b2":"markdown","57939681":"markdown","41aa87a1":"markdown","24b24afe":"markdown","bb04b69a":"markdown","7710c0cd":"markdown","5454b0db":"markdown","cc754db7":"markdown"},"source":{"18ce9cd5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","b1b2692b":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","c60b0e1f":"train_data=train_data.dropna()","1dfff032":"train_data.head()","202ae7c0":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","46bc739a":"test_data=test_data.dropna()","6a84cd99":"test_data.head()","38cd6fd3":"train_data.info()","c46eecb5":"import numpy as np \n\n# data processing\nimport pandas as pd \n\n# data visualization\nimport seaborn as sns\n%matplotlib inline\nfrom matplotlib import pyplot as plt\nfrom matplotlib import style\n\n# Algorithms\nfrom sklearn import linear_model\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB","fa7297b3":"total = train_data.isnull().sum().sort_values(ascending=False)\npercent_1 = train_data.isnull().sum()\/train_data.isnull().count()*100\npercent_2 = (round(percent_1, 1)).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent_2], axis=1, keys=['Total', '%'])\nmissing_data.head()","4bc7f442":"#survival:    Survival \n#PassengerId: Unique Id of a passenger. \n#pclass:    Ticket class     \n#sex:    Sex     \n#Age:    Age in years     \n#sibsp:    # of siblings \/ spouses aboard the Titanic     \n#parch:    # of parents \/ children aboard the Titanic     \n#ticket:    Ticket number     \n#fare:    Passenger fare     \n#cabin:    Cabin number     \n#embarked:    Port of Embarkation\ntrain_data.describe()","3c989e91":"train_data['Survived'].mean()","65b891d7":"train_data.head(10)","3ee04cb0":"train_data.columns.values","57bca1e0":"women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nprint(\"% of women who survived:\", rate_women)","4c572bcf":"men = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of men who survived:\", rate_men)","61b44cd0":"survived = 'survived'\nnot_survived = 'not survived'\nfig, axes = plt.subplots(nrows=1, ncols=2,figsize=(10, 4))\nwomen = train_data[train_data['Sex']=='female']\nmen = train_data[train_data['Sex']=='male']\nax = sns.distplot(women[women['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[0], kde =False)\nax = sns.distplot(women[women['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[0], kde =False)\nax.legend()\nax.set_title('Female')\nax = sns.distplot(men[men['Survived']==1].Age.dropna(), bins=18, label = survived, ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived']==0].Age.dropna(), bins=40, label = not_survived, ax = axes[1], kde = False)\nax.legend()\n_ = ax.set_title('Male')","e7c87391":"train_data.groupby('Pclass').mean()","23821a1e":"class_sex_grouping = train_data.groupby(['Pclass','Sex']).mean()\nclass_sex_grouping","52710e35":"class_sex_grouping['Survived'].plot.bar()","93ed7be5":"#PassengerId\u2019 from the train set, because it does not contribute to a persons survival probability\ntrain_data = train_data.drop(['PassengerId'], axis=1)","14c19fd9":"#Converting Features \ntrain_data.info()","504d47b3":"train_data = train_data.drop(['Cabin'], axis=1)\n","d1840bde":"#test_data = test_data.drop(['Cabin'], axis=1)","22c160bd":"#Converting \u201cFare\u201d from float to int64, using the \u201castype()\u201d function pandas\n\ndata = [train_data, test_data]\n\nfor dataset in data:\n    dataset['Fare'] = dataset['Fare'].fillna(0)\n    dataset['Fare'] = dataset['Fare'].astype(int)","1e2bfa88":"#Converting \u201cAge\u201d from float to int64, using the \u201castype()\u201d function pandas\n\ndata = [train_data, test_data]\n\nfor dataset in data:\n    dataset['Age'] = dataset['Age'].fillna(0)\n    dataset['Age'] = dataset['Age'].astype(int)","0cfb63e4":"train_data.info()\n","088e3ac4":"#Drop Name and Ticket \ntrain_data = train_data.drop(['Ticket'], axis=1)\ntest_data = test_data.drop(['Ticket'], axis=1)","c3123753":"#Convert \u2018Embarked\u2019 feature into numeric.\nports = {\"S\": 0, \"C\": 1, \"Q\": 2}\ndata = [train_data, test_data]\n\nfor dataset in data:\n    dataset['Embarked'] = dataset['Embarked'].map(ports)","292142a1":"#Convert \u2018Sex\u2019 feature into numeric.\ngenders = {\"male\": 0, \"female\": 1}\ndata = [train_data, test_data]\n\nfor dataset in data:\n    dataset['Sex'] = dataset['Sex'].map(genders)","396707c7":"#Drop Name  \ntrain_data = train_data.drop(['Name'], axis=1)\ntest_data = test_data.drop(['Name'], axis=1)","ce11ec17":"train_data.head()","8cda714f":"from sklearn.ensemble import RandomForestClassifier\nX_train = train_data.drop(\"Survived\", axis=1)\nY_train = train_data[\"Survived\"]\nX_test  = test_data.drop(\"PassengerId\", axis=1).copy()\nrandom_forest = RandomForestClassifier(n_estimators=100)\nrandom_forest.fit(X_train, Y_train)\n\nY_prediction = random_forest.predict(X_test)\n\nrandom_forest.score(X_train, Y_train)\nacc_random_forest = round(random_forest.score(X_train, Y_train) * 100, 2)","5a894fb8":"sgd = linear_model.SGDClassifier(max_iter=5, tol=None)\nsgd.fit(X_train, Y_train)\nY_pred = sgd.predict(X_test)\n\nsgd.score(X_train, Y_train)\n\nacc_sgd = round(sgd.score(X_train, Y_train) * 100, 2)","1ccc0023":" knn = KNeighborsClassifier(n_neighbors = 3) \n knn.fit(X_train, Y_train) \n Y_pred = knn.predict(X_test)  \n acc_knn = round(knn.score(X_train, Y_train) * 100, 2)","63201b6b":"gaussian = GaussianNB() \ngaussian.fit(X_train, Y_train)  \nY_pred = gaussian.predict(X_test) \nacc_gaussian = round(gaussian.score(X_train, Y_train) * 100, 2)","c1a1a5f5":"decision_tree = DecisionTreeClassifier() \ndecision_tree.fit(X_train, Y_train)  \nY_pred = decision_tree.predict(X_test)  \nacc_decision_tree = round(decision_tree.score(X_train, Y_train) * 100, 2)","79ca9f8f":"perceptron = Perceptron(max_iter=15)\nperceptron.fit(X_train, Y_train)\n\nY_pred = perceptron.predict(X_test)\n\nacc_perceptron = round(perceptron.score(X_train, Y_train) * 100, 2)","5dbf552e":"results = pd.DataFrame({\n    'Model': ['KNN','Random Forest', 'Naive Bayes', 'Perceptron', \n              'Stochastic Gradient Decent','Decision Tree'],\n    'Score': [acc_knn,acc_random_forest, acc_gaussian, acc_perceptron, \n              acc_sgd, acc_decision_tree]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head()","0e5a12df":"from sklearn.model_selection import cross_val_score\nrf = RandomForestClassifier(n_estimators=100)\nscores = cross_val_score(rf, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","b9c7f9fd":"dt= DecisionTreeClassifier() \nscores = cross_val_score(dt, X_train, Y_train, cv=10, scoring = \"accuracy\")\nprint(\"Scores:\", scores)\nprint(\"Mean:\", scores.mean())\nprint(\"Standard Deviation:\", scores.std())","719e8722":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(random_forest, X_train, Y_train, cv=3)\nconfusion_matrix(Y_train, predictions)","d052732c":"from sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\npredictions = cross_val_predict(decision_tree, X_train, Y_train, cv=3)\nconfusion_matrix(Y_train, predictions)","64a5eed1":"from sklearn.metrics import precision_score, recall_score\n\nprint(\"Precision:\", precision_score(Y_train, predictions))\nprint(\"Recall:\",recall_score(Y_train, predictions))","972cd65e":"from sklearn.metrics import f1_score\nf1_score(Y_train, predictions)","723ff7ec":"from sklearn.metrics import precision_recall_curve\n\n# getting the probabilities of our predictions\ny_scores = random_forest.predict_proba(X_train)\ny_scores = y_scores[:,1]\n\nprecision, recall, threshold = precision_recall_curve(Y_train, y_scores)\ndef plot_precision_and_recall(precision, recall, threshold):\n    plt.plot(threshold, precision[:-1], \"r-\", label=\"precision\", linewidth=5)\n    plt.plot(threshold, recall[:-1], \"b\", label=\"recall\", linewidth=5)\n    plt.xlabel(\"threshold\", fontsize=19)\n    plt.legend(loc=\"upper right\", fontsize=19)\n    plt.ylim([0, 1])\n\nplt.figure(figsize=(14, 7))\nplot_precision_and_recall(precision, recall, threshold)\nplt.show()","64e8f072":"from sklearn.ensemble import RandomForestClassifier\n\ny = train_data[\"Survived\"]\n\nfeatures = [\"Pclass\", \"Sex\", \"SibSp\", \"Parch\"]\nX = pd.get_dummies(train_data[features])\nX_test = pd.get_dummies(test_data[features])\n\nmodel = RandomForestClassifier(n_estimators=100, max_depth=5, random_state=1)\nmodel.fit(X, y)\npredictions = model.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('my_submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","d733818e":"# Model 1 RANDOM FOREST","07ce48ae":"67% out of the training-set survived the Titanic. We can also see that the passenger ages range from 0.9 to 80. ","41852354":"# F-Score","932bc14c":"# Building Models","c105b66e":"This notebook is to predict survival classes from the famous titanic dataset. It presents exploratory data analysis and visualization as well as predictive modeling with different classification algorithms .","eea974c1":"Random Forest classifier  and Decision Tree appear to be be the best model . But ,let us check, how random-forest Decision Tree performs, when we use cross validation.","c4101e2a":"Decision Tree Classifier model has a average accuracy of 79% with a standard deviation of 0.08 %. The standard deviation shows us, how precise the estimates are . This means in our case that the accuracy of this model can differ + \u2014 0.08%. This result shows that Decision tree model will be better.","69260036":"# Data Wrangling","07d3871f":"# Data Preprocessing \n","7b7918e8":"# The best Model","2d30372d":"# Importing The Libraries","475ac668":"The confusion matrix for the decision tree shows 42 True Positive , 18 False positive  with 22 false negative and 101 True negative .","35873897":"# Survival Prediction for Titanic ","c87dff98":"Random Forest Classifier model has a average accuracy of 73% with a standard deviation of 0.12 %. The standard deviation shows us, how precise the estimates are .\nThis means in our case that the accuracy of our model can differ + \u2014 0.12%.","e9e367a9":"For women the survival chances are higher between 14 and 40.While for men is lower ","f5cffd4e":"# Model 2 Stochastic Gradient Descent (SGD):","c7d2801a":"The model predicts 85% of the time, a passengers survival correctly (precision). The recall tells us that it predicted the survival of 82 % of the people who actually survived . The f-score is 84% ,wjich  the harmonic mean of precision and recall","d4476670":"#  Age and Sex:","f9e0ad58":"#  Further Evaluation\n## Confusion Matrix:","f27d545b":"# K-Fold Cross Validation:","927be178":"# Precision Recall Curve","bb0bc67d":"11 features with the target variable (survived).What features could contribute to a high survival rate ?","e48b65b2":"# Model 4 Gaussian Naive Bayes:","57939681":"K-Fold Cross Validation randomly splits the training data into K subsets called folds. Let\u2019s image we would split our data into 4 folds (K = 4). Our random forest model would be trained and evaluated 4 times","41aa87a1":"# Model 6 Perceptron:","24b24afe":"# Model 3 K Nearest Neighbor:","bb04b69a":"first class had a 67% chance of survival, compared to a 50% chance for those in 3rd class.","7710c0cd":"# Model 5 Decision Tree","5454b0db":"# Precision and Recall:","cc754db7":" \u2018Fare\u2019 is a float and we have to deal with 4 categorical features: Name, Sex, Ticket and Embarked. Lets investigate and transfrom one after another."}}