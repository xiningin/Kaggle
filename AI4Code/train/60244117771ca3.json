{"cell_type":{"6d4ff9d0":"code","2770bcf5":"code","dfa1765b":"code","2f4d14ad":"code","e3da0c94":"code","3af12478":"code","58a99a0f":"code","5ce03867":"code","a46d77d4":"code","7d4c6110":"code","35dcf73e":"code","086d0f22":"code","da360dd9":"code","200c1147":"code","e62b8e7b":"code","8be35682":"code","c2e363bd":"code","e180d83a":"code","418eb19a":"code","f98307fb":"code","3ff0f1a0":"code","e15de9b2":"code","76879c89":"code","2c5454f8":"code","2965750b":"code","a7ff3a02":"code","1af43a9d":"code","62217a4e":"code","695804ef":"code","16574ce5":"code","38af13c9":"code","dacc9605":"code","7940d758":"code","bb8162cc":"code","a3f6c627":"code","d5366a6a":"markdown","d09b842c":"markdown","5690ffce":"markdown","16fe34c8":"markdown","983b225b":"markdown","9fd8e0ae":"markdown","9390ddb1":"markdown","77f47db6":"markdown","4b699b28":"markdown","59967666":"markdown","20c3616c":"markdown","a6355894":"markdown","348d43a9":"markdown"},"source":{"6d4ff9d0":"!mkdir models","2770bcf5":"import pandas as pd\nfrom pandas import DataFrame\nfrom pandas import concat\n \nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns  #advanced visualization library\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport time, gc \nimport torch \nfrom torch import nn,optim\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom sklearn import preprocessing  \nimport transformers \nfrom tqdm.notebook import tqdm\n\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom transformers import AdamW\nfrom transformers import get_linear_schedule_with_warmup\ntqdm.pandas()","dfa1765b":"seq_len = 24 \nTRAIN_BATCH_SIZE =128\nVALID_BATCH_SIZE =64\ndevice = torch.device('cuda')","2f4d14ad":"# usefull class \nclass AverageMeter:\n    \"\"\"\n    Computes and stores the average and current value\n    \"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n","e3da0c94":"data = pd.read_csv('..\/input\/chicago-traffic\/chicago.csv')\ndata[\"TIME\"]=pd.to_datetime(data[\"TIME\"], format=\"%m\/%d\/%Y %H:%M:%S %p\")\ndata.drop(data[data['SPEED']==0].index,inplace =True)\ndata = data[data['SPEED']<100]\ndata['day'] = data['TIME'].dt.day\ndata['MONTH'] = data['TIME'].dt.month\ndata['YEAR'] = data['TIME'].dt.year\ndata = data.groupby(['REGION_ID','HOUR','MONTH','day', 'WEST','EAST', 'SOUTH','NORTH','DAY_OF_WEEK','YEAR'])[['SPEED','BUS_COUNT','NUM_READS']].agg('mean').reset_index()\ndata['CENTER_LAT']=data['NORTH']*0.5+0.5*data['SOUTH']\ndata['CENTER_LON']=data['EAST']*0.5+0.5*data['WEST']","3af12478":"def haversine_array(lat1, lng1, lat2, lng2):\n    lat1, lng1, lat2, lng2 = map(np.radians, (lat1, lng1, lat2, lng2))\n    AVG_EARTH_RADIUS = 6371  # in km\n    lat = lat2 - lat1\n    lng = lng2 - lng1\n    d = np.sin(lat * 0.5) ** 2 + np.cos(lat1) * np.cos(lat2) * np.sin(lng * 0.5) ** 2\n    h = 2 * AVG_EARTH_RADIUS * np.arcsin(np.sqrt(d))\n    return h","58a99a0f":"def width(x) : \n    return haversine_array(x['NORTH'],x['WEST'],x['NORTH'],x['EAST'])\ndef length(x) : \n    return haversine_array(x['NORTH'],x['EAST'],x['SOUTH'],x['EAST'])","5ce03867":"data['length'] =  data[['WEST','EAST', 'SOUTH','NORTH']].progress_apply(length,axis=1)\ndata['width']  =  data[['WEST','EAST', 'SOUTH','NORTH']].progress_apply(width,axis=1)","a46d77d4":"data['area'] = data['length']*data['width']\ndata['reders_per_area'] = data['BUS_COUNT']\/data['area'] \ndata['READS_per_area'] = data['NUM_READS']\/data['area'] \ndata['BUS_ratio'] = data['BUS_COUNT']\/data['NUM_READS'] ","7d4c6110":"categorical_features = ['REGION_ID','MONTH','HOUR','day','DAY_OF_WEEK','YEAR']\nLSTM = ['HOUR','DAY_OF_WEEK']\nNumerical_features = ['NUM_READS','BUS_COUNT','area','length','width','CENTER_LAT','CENTER_LON','reders_per_area','READS_per_area','BUS_ratio']","35dcf73e":"from sklearn.preprocessing import StandardScaler\nfor i in Numerical_features : \n    scalar=StandardScaler()\n    scalar.fit(data[i].values.reshape(-1, 1))\n    data[i]=scalar.transform(data[i].values.reshape(-1, 1)) ","086d0f22":"data['MINUTE'] = '00'\ndata['Time'] = pd.to_datetime(data[['YEAR','MONTH','day','HOUR','MINUTE']].astype(str).agg('-'.join,axis=1),format='%Y-%m-%d-%H-%M')","da360dd9":"def get_emb_dim(df,categorical,LSTM):\n    output=[]\n    for categorical_var in categorical:\n\n      cat_emb_name= categorical_var.replace(\" \", \"\")+'_Embedding'\n\n      no_of_unique_cat  = df[categorical_var].nunique()\n      embedding_size = int(min(np.ceil((no_of_unique_cat)\/2), 12))\n      output.append((no_of_unique_cat,embedding_size))    \n      print('Categorica Variable:', categorical_var,\n          'Unique Categories:', no_of_unique_cat,\n          'Embedding Size:', embedding_size)\n    output_lstm = []\n    for categorical_var in LSTM :\n\n        cat_emb_name= categorical_var.replace(\" \", \"\")+'_Embedding'\n\n        no_of_unique_cat  = df[categorical_var].nunique()\n        embedding_size = int(min(np.ceil((no_of_unique_cat)\/2), 12))\n        output_lstm.append((no_of_unique_cat,embedding_size))    \n      \n    return output,output_lstm","200c1147":"emb_size , lstm_emb = get_emb_dim(data,categorical_features,LSTM)","e62b8e7b":"for f in categorical_features : \n  label_encoder = preprocessing.LabelEncoder()\n  label_encoder.fit(data[f].astype('str'))\n  data[f] = label_encoder.transform(data[f].astype('str').fillna('-1'))","8be35682":"def loss_fn(outputs, targets):\n    return nn.MSELoss()(outputs, targets)","c2e363bd":"def train_fn(data_loader, model, optimizer, scheduler):\n    model.train()\n    tr_loss = 0 \n    counter = 0 \n    \n    losses = AverageMeter()\n    rmse=  AverageMeter()\n    tk0 = tqdm(enumerate(data_loader), total=len(data_loader))\n    for bi, d in tk0 :\n        \n        targets = d[\"SPEED\"]\n        targets = targets.to(device, dtype=torch.float)\n        optimizer.zero_grad()\n        outputs = model(d)\n  \n        loss = loss_fn(outputs, targets.view(-1,1))\n        tr_loss += loss.item()\n        counter +=1 \n        loss.backward()\n        optimizer.step()\n        \n        losses.update(loss.item(), targets.size(0))\n        rmse.update(np.sqrt(loss.item()), targets.size(0))\n        tk0.set_postfix(loss=losses.avg, RMSE=rmse.avg)\n    return tr_loss\/counter","e180d83a":"def eval_fn(data_loader, model):\n    model.eval()\n    fin_loss = 0\n    counter = 0\n    \n    losses = AverageMeter()\n    rmse=  AverageMeter()\n    tk0 = tqdm(enumerate(data_loader), total=len(data_loader))\n    \n    with torch.no_grad():\n        \n        for bi, d in tk0 :\n            \n            targets = d[\"SPEED\"]\n            targets = targets.to(device, dtype=torch.float)\n            outputs = model(d)\n\n            loss = loss_fn(outputs, targets.view(-1,1))\n            fin_loss +=loss.item()\n            counter += 1\n            \n            losses.update(loss.item(), targets.size(0))\n            rmse.update(np.sqrt(loss.item()), targets.size(0))\n            tk0.set_postfix(loss=losses.avg, RMSE=rmse.avg)\n        \n        return fin_loss\/counter ","418eb19a":"def predict(model,data_loader) : \n    model.eval()\n    \n    \n    losses = AverageMeter()\n    rmse=  AverageMeter()\n    tk0 = tqdm(enumerate(data_loader), total=len(data_loader))\n\n    with torch.no_grad():\n        \n        for bi, d in tk0 :\n            \n            targets = d[\"SPEED\"]\n            targets = targets.to(device, dtype=torch.float)\n            outputs = model(d)\n\n            loss = loss_fn(outputs, targets.view(-1,1))\n            \n            if bi==0 : \n                out = outputs \n            else : \n                out = torch.cat([out,outputs],dim = 0 )\n            \n            losses.update(loss.item(), targets.size(0))\n            rmse.update(np.sqrt(loss.item()), targets.size(0))\n            tk0.set_postfix(loss=losses.avg, RMSE=rmse.avg)\n        \n        return out.cpu().detach().numpy() \n    ","f98307fb":"def run(model,EPOCHS):\n    \n    train_data_loader = torch.utils.data.DataLoader(\n        train_dataset,\n        batch_size=TRAIN_BATCH_SIZE,\n        num_workers=8\n    )\n    \n    \n    valid_data_loader = torch.utils.data.DataLoader(\n        valid_dataset,\n        batch_size=VALID_BATCH_SIZE,\n        num_workers=4\n    )\n\n    \n\n\n    num_train_steps = int(len(train_data_loader)) * EPOCHS\n    optimizer = AdamW(model.parameters(), lr=1e-2)\n    scheduler = get_linear_schedule_with_warmup(\n        optimizer,\n        num_warmup_steps=0,\n        num_training_steps=num_train_steps\n    )\n    train_loss =  []\n    val_loss = []\n    for epoch in range(EPOCHS):\n        print(f'--------- Epoch {epoch} ---------')\n        tr_loss=train_fn(train_data_loader, model, optimizer, scheduler)\n        train_loss.append(tr_loss)\n        print(f\" train_loss  = {np.sqrt(tr_loss)}\")\n\n        \n        val = eval_fn(valid_data_loader, model)\n        val_loss.append(val)\n        print(f\" val_loss  = {np.sqrt(val)}\")\n        torch.save(model.state_dict(), 'models\/model_rnn.bin')\n\n        scheduler.step()\n    return val_loss,train_loss","3ff0f1a0":"def series_to_supervised(data, n_in=1, n_out=1, col=[], dropnan=True):\n    \n    df = DataFrame(data)\n    cols, names = list(), list()\n    \n    # input sequence (t-n, ... t-1)\n    for i in range(n_in, 0, -1):\n        cols.append(df.shift(i))\n        names += [('%s(t-%d)' % (j, i)) for j in col]\n        \n    # forecast sequence (t, t+1, ... t+n)\n    for i in range(0, n_out):\n        cols.append(df.shift(-i))\n        if i == 0:\n            names += [('%s(t)' % (j)) for j in col]\n        else:\n            names += [('%s(t+%d)' % (j, i)) for j in col]\n            \n    # put it all together\n    agg = concat(cols, axis=1)\n    agg.columns = names\n    # drop rows with NaN values\n    \n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg","e15de9b2":"df = pd.DataFrame()\n\nfor i in tqdm( data.REGION_ID.unique() , total = len(data.REGION_ID.unique())):\n     \n    sub_set = data[categorical_features+Numerical_features+['SPEED','Time']]\n    sub_set = sub_set.sort_values('Time',ascending=False)\n    sub_set = sub_set[sub_set['REGION_ID']==i]\n    sub_set = series_to_supervised(sub_set,seq_len,1,sub_set.columns.tolist())\n    df = df.append(sub_set,ignore_index = True )\n    \n    \ndf.shape","76879c89":"class dataset() : \n    \n    def __init__( self , data, categorical_features , Numericla_features ,window_size) : \n        \n        self.window_size = window_size   \n        self.df = data\n        self.categorical = categorical_features\n        self.Numeric = Numericla_features\n        out = dict()\n        \n        for i in self.categorical : \n            col = [ '%s(t)' % i ] \n            out[i] = torch.tensor( self.df[col].values , dtype=torch.long )\n        \n        for i in self.Numeric : \n            col = [ '%s(t)' % i ] \n            out[i] = torch.tensor( self.df[col].values , dtype=torch.float )\n        old = []\n        for i in LSTM+['SPEED'] : \n            col = [('%s(t-%d)' % (i, j)) for j in range(self.window_size, 0, -1)] \n            if i == 'SPEED' : \n                out[i+'_(t-k)'] =  torch.tensor( self.df[col].values , dtype=torch.float )\n            else : \n                out[i+'_(t-k)'] =  torch.tensor( self.df[col].values , dtype=torch.long )\n            old.append(i+'_(t-k)')\n        self.old = old\n            \n        out['SPEED'] = torch.tensor(self.df['SPEED(t)'].values ,dtype=torch.float ) \n        self.df = out\n        \n    def __len__(self) : \n        \n        return len(self.df['SPEED'])\n    \n    def __getitem__(self,item) : \n        \n        out = dict()\n        \n        for i in self.categorical : \n            out[i] = torch.tensor( self.df[i][item] , dtype=torch.long )\n        \n        for i in self.Numeric : \n            out[i] = torch.tensor( self.df[i][item] , dtype=torch.float )\n        \n        for i in self.old : \n            if i == 'SPEED_(t-k)':\n                out[i] = torch.tensor( self.df[i][item] , dtype=torch.float )\n            else : \n                out[i] = torch.tensor( self.df[i][item] , dtype=torch.long )\n        \n        out['SPEED'] = torch.tensor(self.df['SPEED'][item],dtype=torch.float ) \n        \n        return out\n","2c5454f8":"import math\ndef init_normal(m):\n    if type(m) == nn.Linear:\n        nn.init.kaiming_uniform_(m.weight, a=math.sqrt(3))","2965750b":"class Seq2SeqRnn(nn.Module):\n    \n    def __init__(self,cat,num,emb_size ,lstm_emb, hidden_size ):\n        \n        super().__init__()\n\n        self.hidden_size = hidden_size\n        self.output_size=1\n        self.numerical =num \n        self.cat = cat \n        self.emb_size = emb_size \n        \n        \n        outputs_cat = nn.ModuleList()\n        \n        for inp , emb  in emb_size :\n            embedding_layer = nn.Embedding(inp+2,emb)\n            outputs_cat.append(embedding_layer)\n        self.outputs_cat = outputs_cat \n        \n        outputs_cat = nn.ModuleList()\n        for inp , emb  in lstm_emb :\n            embedding_layer = nn.Embedding(inp+2,emb)\n            outputs_cat.append(embedding_layer)\n        self.lstm_emb = outputs_cat \n        \n        \n        n_emb = sum([e[1] for e in self.emb_size])\n        n_emb_lstm = sum([e[1] for e in lstm_emb])\n        self.input_size = n_emb_lstm + 1\n        self.rnn = nn.GRU(input_size= self.input_size , hidden_size=hidden_size, num_layers=1, \n                           bidirectional=True, batch_first=True)\n        \n        \n        self.fc = nn.Sequential(  \n                              nn.Linear( n_emb + len(num) + hidden_size*2 ,1024),\n                              nn.BatchNorm1d(1024),\n                              nn.Dropout(0.5),\n                              nn.ReLU(),\n                              nn.Linear(1024,512),\n                              nn.BatchNorm1d(512),\n                              nn.Dropout(0.3),\n                              nn.ReLU(),\n                              nn.Linear(512,256),\n                              nn.BatchNorm1d(256),\n                              nn.Dropout(0.2),\n                              nn.ReLU(),\n                              nn.Linear(256,1)\n    )\n        self.fc.apply(init_normal)\n           \n    \n        \n        \n    def forward(self, data ):\n        \n        cat_lstm = []\n        for i in range(len(self.lstm_emb)) : \n            inputs = data[LSTM[i]+'_(t-k)'].to(device,dtype=torch.long) \n            out = self.lstm_emb[i](inputs)\n            cat_lstm.append(out) \n\n        x_lstm = torch.cat(cat_lstm,dim= 2 )\n        x_speed = data['SPEED_(t-k)'].to(device,dtype=torch.float) \n        x_speed = x_speed.unsqueeze(2)\n        x_lstm = torch.cat([x_lstm,x_speed],dim= 2 )\n        out , hid = self.rnn(x_lstm)\n        x_lstm = torch.cat([hid[0],hid[1]],dim=1)\n        \n        \n        outputs_emb = [] \n        for i in range(len(self.cat)) : \n            inputs = data[self.cat[i]].to(device,dtype=torch.long) \n            out = self.outputs_cat[i](inputs)\n            outputs_emb.append(out.squeeze(1)) \n\n        x_cat = torch.cat(outputs_emb,dim= 1)\n\n        output_num = []\n\n        for i in self.numerical : \n            inputs = (data[i].view(-1,1)).to(device,dtype=torch.float)\n            output_num.append(inputs)\n        \n        x_num = torch.cat(output_num,dim=1)\n\n\n        x_all = torch.cat([x_num,x_cat,x_lstm],dim=1) \n        x_final = self.fc(x_all)\n        return x_final","a7ff3a02":"df_train ,df_test   = model_selection.train_test_split( df ,test_size = 0.1 ,random_state = 44  )\ndf_train , df_valid = model_selection.train_test_split( df_train ,test_size = 0.2 ,random_state = 44  )","1af43a9d":"gc.collect()","62217a4e":"start_time = time.time()\ntrain_dataset = dataset(\n        df_train,categorical_features,Numerical_features,seq_len\n    )\n\nvalid_dataset = dataset(\n        df_valid,\n        categorical_features,\n        Numerical_features,seq_len\n\n)\nmodel=Seq2SeqRnn(cat = categorical_features,num = Numerical_features, emb_size = emb_size , hidden_size=64,lstm_emb =lstm_emb )\nmodel.load_state_dict(torch.load('..\/input\/model33\/model_rnn.bin', map_location=\"cuda:0\"))\nmodel = model.to(device)\n    \ngetattr(tqdm, '_instances', {}).clear()\nval_loss,tr_loss = run(model,40)","695804ef":"print(\"total training time is %s Minute \" %(236+((time.time() - start_time)\/60)))\nprint(\"hardware : NVidia K80 GPUs \")","16574ce5":"test_dataset = dataset(\n        df_test , categorical_features , Numerical_features , seq_len\n    )\ntest_data_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=VALID_BATCH_SIZE,\n        num_workers=4\n    )\nval = eval_fn(test_data_loader, model)\nprint(f\" TEST RMSE = {np.sqrt(val)}\")","38af13c9":"def plot(region , year , month , day ) : \n    df['year']  = df['Time(t)'].dt.year\n    df['month'] = df['Time(t)'].dt.month\n    df['days']  = df['Time(t)'].dt.day\n    sub_plot = df[(df['REGION_ID(t)']==region)&(df['year']==year)&(df['month']==month)& (df['days']<day)]\n    sub_plot = sub_plot.sort_values('Time(t)')\n    y=  sub_plot['SPEED(t)'].values\n    x_data = dataset(\n        sub_plot , categorical_features , Numerical_features , seq_len\n    ) \n    x_data_loader = torch.utils.data.DataLoader(\n        x_data,\n        batch_size=VALID_BATCH_SIZE,\n        num_workers=4\n    )\n    predictions  =predict(model,x_data_loader).reshape(1,-1)[0]\n    plt.figure(figsize=(24, 8))\n    plt.plot(sub_plot['Time(t)'].values , sub_plot['SPEED(t)'].values, '--',label = 'real values')\n    plt.plot(sub_plot['Time(t)'].values , predictions,label = 'predicted values')\n    plt.ylabel(f'SPEED in region id {region}')\n    plt.title(f' predicted vs real values ')\n    plt.xlabel('Time')\n    plt.grid(True)\n    plt.legend()\n\n    plt.show()","dacc9605":"plot(1,2019,1,10)","7940d758":"plot(5,2018,4,14)","bb8162cc":"plot(7,2018,4,14)\n","a3f6c627":"plot(7,2019,9,17)\n","d5366a6a":"![image.png](attachment:image.png)","d09b842c":"# Data Loading","5690ffce":"## The Network Architecture\n","16fe34c8":"# Entity embeddings hyperparametres tuning ","983b225b":"## Dataset class","9fd8e0ae":"# Engine ","9390ddb1":"# Importations ","77f47db6":"# Config ","4b699b28":"## Model evalutation","59967666":"## Data preprocessing","20c3616c":"## Train Test Validation split","a6355894":"## Feature engineering","348d43a9":"## Training "}}