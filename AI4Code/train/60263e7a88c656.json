{"cell_type":{"a10cd2c4":"code","1bc174a6":"code","69100403":"code","cb9855ad":"code","a353da23":"code","cf73c5ec":"code","6e7a7aa1":"code","67aa9586":"code","a1df7c44":"code","275e15ba":"code","2f3ae084":"code","517a6f9c":"code","d4302173":"code","f26aac76":"code","ada46da0":"code","82b7bded":"code","8d49b4f0":"code","41225410":"code","00762afb":"code","b4ba4b46":"code","1b365736":"code","07752b0e":"code","7f8e3100":"code","ed99b98b":"code","e6bed70f":"code","adc7f047":"code","cac37fe0":"code","d943b31a":"code","2eaadffb":"code","44427673":"code","ad20ace0":"code","c6fa465a":"code","17c888e7":"code","8741b0e6":"code","8a50a2dc":"code","bbd04360":"code","f7aca1b0":"code","3acf0ae9":"code","6f8adab7":"code","885c896e":"code","7f159d0a":"code","0bd7dc08":"code","8c163a90":"code","a48f5356":"code","495f0fc8":"code","2d181f53":"code","49b2a423":"code","16694fb6":"code","bf982ae9":"code","860d9dd7":"code","7d168234":"code","d35c8580":"code","7f381e83":"code","2d56584d":"code","b3db015b":"code","5d089084":"code","4a7380de":"markdown","ddbd4414":"markdown","123aefa4":"markdown","9f5b24ea":"markdown","593551fc":"markdown","09d0eebe":"markdown","2c791d8a":"markdown","648dc450":"markdown","3af19f2b":"markdown","69aef60b":"markdown","acb4baa4":"markdown","98efa184":"markdown","45ff2753":"markdown","93a1e0b8":"markdown","f9f001d4":"markdown","87b917a3":"markdown","145eabf8":"markdown","837976b0":"markdown","86e14a58":"markdown","e3a89319":"markdown","6fdd5692":"markdown","cef8da27":"markdown","fced02be":"markdown","bd3f9dab":"markdown","5d9864db":"markdown","0f497ae5":"markdown","31f931a9":"markdown","117012ae":"markdown","b74ae5ca":"markdown","caf99e67":"markdown","9687b2a9":"markdown","47777a06":"markdown","bdca31c9":"markdown","dd6abe6b":"markdown","0ef80418":"markdown","1ea8bf2d":"markdown","79bf4053":"markdown","11f8164e":"markdown","80a1b9f1":"markdown","a70abb3f":"markdown","d93ca01d":"markdown","a1e77362":"markdown","57cd143b":"markdown","832b9bdb":"markdown","c4063cae":"markdown","c652e23b":"markdown","7aec91a2":"markdown","e34aa3b5":"markdown","0e8537b4":"markdown"},"source":{"a10cd2c4":"import math\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport sklearn\nfrom sklearn import metrics\nfrom sklearn import datasets\nfrom sklearn import pipeline\nfrom sklearn import linear_model\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn import neural_network","1bc174a6":"train_csv = pd.read_csv('..\/input\/titanic\/train.csv')\ntitanic_df = pd.DataFrame(train_csv)\ntest_csv = pd.read_csv('..\/input\/titanic\/test.csv')\ntest = pd.DataFrame(test_csv)","69100403":"display(titanic_df.keys())","cb9855ad":"survived = titanic_df[\"Survived\"]\ntitanic_df = titanic_df.drop(\"Survived\", axis=1)\ntitanic_df.insert(0, \"Survived\", survived)\ndisplay(titanic_df)\ntitanic_df.info()","a353da23":"titanic_df.isna().sum()","cf73c5ec":"titanic_df[\"Title\"] = titanic_df[\"Name\"].str.extract(' ([A-Za-z]+)\\.', expand=False)\npd.crosstab(titanic_df[\"Title\"], titanic_df[\"Sex\"])","6e7a7aa1":"pd.crosstab(titanic_df.Title ,titanic_df.Sex).T.style.background_gradient() #Checking the Initials with the Sex","67aa9586":"titanic_df[\"Title\"] = titanic_df[\"Title\"].replace([\"Lady\", \"Countess\",\"Capt\", \"Col\",\\\n \t\"Don\", \"Dr\", \"Major\", \"Rev\", \"Sir\", \"Jonkheer\", \"Dona\"], \"Other\")\n\ntitanic_df[\"Title\"] = titanic_df[\"Title\"].replace(\"Mlle\", \"Miss\")\ntitanic_df[\"Title\"] = titanic_df[\"Title\"].replace(\"Ms\", \"Miss\")\ntitanic_df[\"Title\"] = titanic_df[\"Title\"].replace(\"Mme\", \"Mrs\")\n\npd.crosstab(titanic_df.Title ,titanic_df.Survived, values=titanic_df.Age, aggfunc='mean').T.style.background_gradient() #Checking the Initials with the Sex","a1df7c44":"sns.barplot(x=\"Title\", y=\"Survived\", data=titanic_df)","275e15ba":"titanic_df[\"Title_cat\"] = titanic_df[\"Title\"].map({'Mr' : 0, 'Mrs' : 1, 'Miss' : 2, 'Master' : 3, 'Other' : 4})\ndisplay(titanic_df[[\"Title\", \"Title_cat\"]])","2f3ae084":"titanic_df[[\"Title\", \"Title_cat\"]].isna().any()","517a6f9c":"test[\"Title\"] = test[\"Name\"].str.extract(' ([A-Za-z]+)\\.', expand=False)\n\ntest[\"Title\"] = test[\"Title\"].replace([\"Lady\", \"Countess\",\"Capt\", \"Col\",\\\n \t\"Don\", \"Dr\", \"Major\", \"Rev\", \"Sir\", \"Jonkheer\", \"Dona\"], \"Other\")\n\ntest[\"Title\"] = test[\"Title\"].replace(\"Mlle\", \"Miss\")\ntest[\"Title\"] = test[\"Title\"].replace(\"Ms\", \"Miss\")\ntest[\"Title\"] = test[\"Title\"].replace(\"Mme\", \"Mrs\")\n\ntest[\"Title_cat\"] = test[\"Title\"].map({'Mr' : 0, 'Mrs' : 1, 'Miss' : 2, 'Master' : 3, 'Other' : 4})\n\ntest[\"Title_cat\"].isna().any()","d4302173":"sns.barplot(x=\"Embarked\", y=\"Survived\", data=titanic_df)","f26aac76":"titanic_df[\"Embarked\"] = titanic_df[\"Embarked\"].fillna(titanic_df[\"Embarked\"].mode()[0])\ntitanic_df[\"Embarked_s\"] = titanic_df[\"Embarked\"].map({ 'S' : 1, 'C' : 0, 'Q' : 0, np.nan : 0}).astype(int)\ntitanic_df[\"Embarked_c\"] = titanic_df[\"Embarked\"].map({ 'S' : 0, 'C' : 1, 'Q' : 0, np.nan : 0}).astype(int)\ntitanic_df[\"Embarked_q\"] = titanic_df[\"Embarked\"].map({ 'S' : 0, 'C' : 0, 'Q' : 1, np.nan : 0}).astype(int)\n\ndisplay(titanic_df[[\"Embarked\", \"Embarked_s\", \"Embarked_c\", \"Embarked_q\"]])\ntitanic_df[\"Embarked\"].isna().any()","ada46da0":"test[\"Embarked_s\"] = test[\"Embarked\"].map({ 'S' : 1, 'C' : 0, 'Q' : 0, np.nan : 0}).astype(int)\ntest[\"Embarked_c\"] = test[\"Embarked\"].map({ 'S' : 0, 'C' : 1, 'Q' : 0, np.nan : 0}).astype(int)\ntest[\"Embarked_q\"] = test[\"Embarked\"].map({ 'S' : 0, 'C' : 0, 'Q' : 1, np.nan : 0}).astype(int)\n\ntest[[\"Embarked_s\", \"Embarked_c\", \"Embarked_q\"]].isna().any()","82b7bded":"sns.barplot(x=\"Sex\", y=\"Survived\", data=titanic_df)","8d49b4f0":"titanic_df[\"Male\"] = 0\ntitanic_df[\"Female\"] = 0\ntitanic_df[\"Male\"] = titanic_df[\"Sex\"].map( {'female': 0, 'male': 1} ).astype(int)\ntitanic_df[\"Female\"] = titanic_df[\"Sex\"].map( {'male': 0, 'female': 1} ).astype(int)\ndisplay(titanic_df[[\"Sex\", \"Male\", \"Female\"]])","41225410":"test[\"Male\"] = 0\ntest[\"Female\"] = 0\ntest[\"Male\"] = test[\"Sex\"].map( {'female': 0, 'male': 1} ).astype(int)\ntest[\"Female\"] = test[\"Sex\"].map( {'male': 0, 'female': 1} ).astype(int)\ntest[[\"Male\", \"Female\"]].isna().any()","00762afb":"temp = pd.crosstab(titanic_df.Pclass ,titanic_df.Survived, values=titanic_df.Survived, aggfunc='count').T.style.background_gradient() #Checking the Initials with the Sex\ndisplay(temp)\nsns.countplot('Pclass',data=titanic_df, hue='Survived')","b4ba4b46":"corr = abs(titanic_df.corr())    # get the data correlation\nplt.figure(figsize=(12,8))\nmask = np.triu(np.ones_like(corr, dtype=bool))\nsns.heatmap(corr, mask=mask, annot=True, cmap=plt.cm.Reds, vmin=0, vmax=1, linewidth=.5)\nplt.show()","1b365736":"sns.catplot(y=\"Age\", x=\"Pclass\", hue=\"Survived\", kind=\"violin\", data=titanic_df)","07752b0e":"def convert_title(df):\n    means = df.groupby([\"Title_cat\", \"Pclass\"])[\"Age\"].mean()\n    for i in range(df[\"Title_cat\"].nunique()):\n     for j in range(means[i].shape[0]):\n            df.loc[ (df.Age.isnull()) & (df.Title_cat == i) & (df.Pclass == j +1 ), 'Age'] = means[i][j+1]\n            \nconvert_title(titanic_df)\nprint(titanic_df[\"Age\"].isna().any())            \nprint(titanic_df[\"Age\"])","7f8e3100":"convert_title(test)\ntest.Age.isna().any()","ed99b98b":"sns.catplot(x=\"Title\", kind=\"count\", palette=sns.color_palette(\"deep\"), data=titanic_df)","e6bed70f":"def convert_titles_binary(data):\n    data[\"Mr\"] = 0\n    data[\"Mrs\"] = 0\n    data[\"Miss\"] = 0\n    data[\"Master\"] = 0\n    data[\"Mr\"] = data[\"Title\"].map({'Mr' : 1, 'Mrs' : 0, 'Miss' : 0, 'Master' : 0, 'Other' : 0})\n    data[\"Mrs\"] = data[\"Title\"].map({'Mr' : 0, 'Mrs' : 1, 'Miss' : 0, 'Master' : 0, 'Other' : 0})\n    data[\"Miss\"] = data[\"Title\"].map({'Mr' : 0, 'Mrs' : 0, 'Miss' : 1, 'Master' : 0, 'Other' : 0})\n    data[\"Master\"] = data[\"Title\"].map({'Mr' : 0, 'Mrs' : 0, 'Miss' : 0, 'Master' : 1, 'Other' : 0})\n\nconvert_titles_binary(titanic_df)\nconvert_titles_binary(test)\n\ntitanic_df[[\"Mr\", \"Mrs\", \"Miss\", \"Master\"]].isna().any()\ntest[[\"Mr\", \"Mrs\", \"Miss\", \"Master\"]].isna().any()","adc7f047":"titanic_df.drop(\"Cabin\", axis=1, inplace=True)\nprint(titanic_df.isna().any())\n\ntest.drop(\"Cabin\", axis=1, inplace=True)\nprint(test.isna().any())","cac37fe0":"titanic_df[\"FamilySize\"] = titanic_df[\"Parch\"] + titanic_df[\"SibSp\"] + 1\ntitanic_df[[\"FamilySize\", \"Survived\"]].groupby([\"FamilySize\"], as_index=False).mean().sort_values(by=\"Survived\", ascending=False)","d943b31a":"test[\"FamilySize\"] = test[\"Parch\"] + test[\"SibSp\"] + 1","2eaadffb":"titanic_df['IsAlone'] = 0\ntitanic_df.loc[titanic_df['FamilySize'] == 1, 'IsAlone'] = 1\n\ntest['IsAlone'] = 0\ntest.loc[test['FamilySize'] == 1, 'IsAlone'] = 1","44427673":"sns.regplot(x=\"Age\", y=\"Pclass\", data=titanic_df, logx=True)","ad20ace0":"titanic_df[\"AgePclass\"] = titanic_df[\"Age\"] * titanic_df[\"Pclass\"]\ntest[\"AgePclass\"] = test[\"Age\"] * test[\"Pclass\"]","c6fa465a":"def age_band(num):\n    for i in range(1, 100):\n        if num < 10*i :  return f'{(i-1) * 10} ~ {i*10}'\n\n        \ntitanic_df['age_band'] = titanic_df['Age'].apply(age_band)\ntitanic_age = titanic_df[['age_band', 'Survived']].groupby('age_band')['Survived'].value_counts().sort_index().unstack().fillna(0)\ntitanic_age['Survival rate'] = titanic_age[1] \/ (titanic_age[0] + titanic_age[1]) * 100\n\nfig, ax = plt.subplots(1, 1, figsize=(10, 7))\n\n# ax2\ncolor_map = ['#d4dddd' for _ in range(9)]\ncolor_map[0] = color_map[8] = '#244747'\n\nax.bar(titanic_age['Survival rate'].index, titanic_age['Survival rate'], \n       color=color_map, width=0.55, \n       edgecolor='black', \n       linewidth=0.7)\n\n\n\nfor s in [\"top\",\"right\",\"left\"]:\n    ax.spines[s].set_visible(False)\n\n\n\nfor i in titanic_age['Survival rate'].index:\n    ax.annotate(f\"{titanic_age['Survival rate'][i]:.02f}%\", \n                   xy=(i, titanic_age['Survival rate'][i] + 2.3),\n                   va = 'center', ha='center',fontweight='light', \n                   color='#4a4a4a')\n\n\n# mean line + annotation\nmean = titanic_df['Survived'].mean() *100\nax.axhline(mean ,color='black', linewidth=0.4, linestyle='dashdot')\n    \n\n# Title & Subtitle    \nfig.text(0.06, 1, 'Age Band and Survival Rate', fontsize=15)\n\ngrid_y_ticks = np.arange(0, 101, 20)\nax.set_yticks(grid_y_ticks)\nax.grid(axis='y', linestyle='-', alpha=0.4)\n\nplt.tight_layout()\nplt.show()","17c888e7":"def createAgeBand(dataset):\n    dataset.loc[ dataset['Age'] <= 10, 'AgeBand'] = 0\n    dataset.loc[(dataset['Age'] > 10) & (dataset['Age'] <= 20), 'AgeBand'] = 1\n    dataset.loc[(dataset['Age'] > 20) & (dataset['Age'] <= 30), 'AgeBand'] = 2\n    dataset.loc[(dataset['Age'] > 30) & (dataset['Age'] <= 40), 'AgeBand'] = 3\n    dataset.loc[(dataset['Age'] > 40) & (dataset['Age'] <= 50), 'AgeBand'] = 4\n    dataset.loc[(dataset['Age'] > 50) & (dataset['Age'] <= 60), 'AgeBand'] = 5\n    dataset.loc[(dataset['Age'] > 60) & (dataset['Age'] <= 70), 'AgeBand'] = 6\n    dataset.loc[(dataset['Age'] > 70) & (dataset['Age'] <= 80), 'AgeBand'] = 7\n    dataset.loc[ dataset['Age'] > 80, 'AgeBand'] = 8\n    \n\ncreateAgeBand(titanic_df)\ntitanic_df.AgeBand.isna().any()","8741b0e6":"sns.barplot(x=\"AgeBand\", y=\"Survived\", palette=sns.color_palette(\"coolwarm\"), data=titanic_df)","8a50a2dc":"createAgeBand(test)\ntest.AgeBand.isna().any()","bbd04360":"print(titanic_df.isna().sum().any())\nprint(test.isna().sum().any())","f7aca1b0":"data_fs = titanic_df[[\"AgePclass\", \"Fare\", \"Mr\", \"Mrs\", \"Miss\", \"Master\", \"Embarked_s\", \"Embarked_c\", \"Embarked_q\", \"Male\", \"Female\", \"FamilySize\", \"Survived\"]]\nt_fs = data_fs[\"Survived\"]\nx_fs = data_fs.drop('Survived', axis=1)\n\nx_train_fs, x_test_fs, t_train_fs, t_test_fs = sklearn.model_selection.train_test_split(x_fs, t_fs, test_size=0.2, random_state=0)\nSGD_cls_fs = pipeline.make_pipeline(preprocessing.StandardScaler(), linear_model.SGDClassifier(loss='log', alpha=0, learning_rate='constant', eta0=0.01)).fit(x_train_fs, t_train_fs)\ny_train_prob_fs = SGD_cls_fs.predict_proba(x_train_fs)\ny_test_prob_fs = SGD_cls_fs.predict_proba(x_test_fs)\n","3acf0ae9":"acurSGD_fs_train = SGD_cls_fs.score(x_train_fs, t_train_fs)\nacurSGD_fs_valid = SGD_cls_fs.score(x_test_fs, t_test_fs)\nceSGD_fs_train = metrics.log_loss(t_train_fs, y_train_prob_fs)\nceSGD_fs_valid = metrics.log_loss(t_test_fs, y_test_prob_fs)\n\nprint('Accuracy score on train', acurSGD_fs_train)\nprint('Accuracy score on validation', acurSGD_fs_valid)\nprint()\nprint('CE on train', ceSGD_fs_train)\nprint('CE on test',ceSGD_fs_valid)","6f8adab7":"data_ia = titanic_df[[\"AgePclass\", \"Fare\", \"Mr\", \"Mrs\", \"Miss\", \"Master\", \"Embarked_s\", \"Embarked_c\", \"Embarked_q\", \"Male\", \"Female\", \"IsAlone\", \"Survived\"]]\nt_ia = data_ia[\"Survived\"]\nx_ia = data_ia.drop('Survived', axis=1)\n\nx_train_ia, x_test_ia, t_train_ia, t_test_ia = sklearn.model_selection.train_test_split(x_ia, t_ia, test_size=0.2, random_state=0)\nSGD_cls_ia = pipeline.make_pipeline(preprocessing.StandardScaler(), linear_model.SGDClassifier(loss='log', alpha=0, learning_rate='constant', eta0=0.01)).fit(x_train_ia, t_train_ia)\ny_train_prob_ia = SGD_cls_ia.predict_proba(x_train_ia)\ny_test_prob_ia = SGD_cls_ia.predict_proba(x_test_ia)","885c896e":"acurSGD_ia_train = SGD_cls_ia.score(x_train_ia, t_train_ia)\nacurSGD_ia_valid = SGD_cls_ia.score(x_test_ia, t_test_ia)\nceSGD_ia_train = metrics.log_loss(t_train_ia, y_train_prob_ia)\nceSGD_ia_valid = metrics.log_loss(t_test_ia, y_test_prob_ia)\n\nprint('Accuracy score on train', acurSGD_ia_train)\nprint('Accuracy score on validation', acurSGD_ia_valid)\nprint()\nprint('CE on train', ceSGD_ia_train)\nprint('CE on test',ceSGD_ia_valid)","7f159d0a":"data_ab = titanic_df[[\"AgePclass\", \"AgeBand\", \"Fare\", \"Mr\", \"Mrs\", \"Miss\", \"Master\", \"Embarked_s\", \"Embarked_c\", \"Embarked_q\", \"Male\", \"Female\", \"FamilySize\", \"Survived\"]]\nt_ab = data_ab[\"Survived\"]\nx_ab = data_ab.drop('Survived', axis=1)\n\nx_train_ab, x_test_ab, t_train_ab, t_test_ab = sklearn.model_selection.train_test_split(x_ab, t_ab, test_size=0.2, random_state=0)\nSGD_cls_ab = pipeline.make_pipeline(preprocessing.StandardScaler(), linear_model.SGDClassifier(loss='log', alpha=0, learning_rate='constant', eta0=0.01)).fit(x_train_ab, t_train_ab)\ny_train_prob_ab = SGD_cls_ab.predict_proba(x_train_ab)\ny_test_prob_ab = SGD_cls_ab.predict_proba(x_test_ab)","0bd7dc08":"acurSGD_ab_train = SGD_cls_ab.score(x_train_ab, t_train_ab)\nacurSGD_ab_valid = SGD_cls_ab.score(x_test_ab, t_test_ab)\nceSGD_ab_train = metrics.log_loss(t_train_ab, y_train_prob_ab)\nceSGD_ab_valid = metrics.log_loss(t_test_ab, y_test_prob_ab)\n\nprint('Accuracy score on train', acurSGD_ab_train)\nprint('Accuracy score on validation', acurSGD_ab_valid)\nprint()\nprint('CE on train', ceSGD_ab_train)\nprint('CE on test',ceSGD_ab_valid)","8c163a90":"MLP_cls_fs = neural_network.MLPClassifier(activation='logistic', solver='sgd', alpha=0, max_iter=10000).fit(x_train_fs, t_train_fs)\ny_train_prob_fs = MLP_cls_fs.predict_proba(x_train_fs)\ny_test_prob_fs = MLP_cls_fs.predict_proba(x_test_fs)","a48f5356":"acurMLP_fs_train =  MLP_cls_fs.score(x_train_fs, t_train_fs)\nacurMLP_fs_valid = MLP_cls_fs.score(x_test_fs, t_test_fs)\nceMLP_fs_train = metrics.log_loss(t_train_fs, y_train_prob_fs)\nceMLP_fs_valid = metrics.log_loss(t_test_fs, y_test_prob_fs)\n\nprint('Accuracy score on train', acurMLP_fs_train)\nprint('Accuracy score on validation', acurMLP_fs_valid)\nprint()\nprint('CE on train', ceMLP_fs_train)\nprint('CE on test',ceMLP_fs_valid)","495f0fc8":"MLP_cls_ia = neural_network.MLPClassifier(activation='logistic', solver='sgd', alpha=0, max_iter=10000).fit(x_train_ia, t_train_ia)\ny_train_prob_ia = MLP_cls_ia.predict_proba(x_train_ia)\ny_test_prob_ia = MLP_cls_ia.predict_proba(x_test_ia)","2d181f53":"acurMLP_ia_train =  MLP_cls_ia.score(x_train_ia, t_train_ia)\nacurMLP_ia_valid = MLP_cls_ia.score(x_test_ia, t_test_ia)\nceMLP_ia_train = metrics.log_loss(t_train_ia, y_train_prob_ia)\nceMLP_ia_valid = metrics.log_loss(t_test_ia, y_test_prob_ia)\n\nprint('Accuracy score on train', acurMLP_ia_train)\nprint('Accuracy score on validation', acurMLP_ia_valid)\nprint()\nprint('CE on train', ceMLP_ia_train)\nprint('CE on test',ceMLP_ia_valid)","49b2a423":"MLP_cls_ab = neural_network.MLPClassifier(activation='logistic', solver='sgd', alpha=0, max_iter=10000).fit(x_train_ab, t_train_ab)\ny_train_prob_ab = MLP_cls_ab.predict_proba(x_train_ab)\ny_test_prob_ab = MLP_cls_ab.predict_proba(x_test_ab)","16694fb6":"acurMLP_ab_train =  MLP_cls_ab.score(x_train_ab, t_train_ab)\nacurMLP_ab_valid = MLP_cls_ab.score(x_test_ab, t_test_ab)\nceMLP_ab_train = metrics.log_loss(t_train_ab, y_train_prob_ab)\nceMLP_ab_valid = metrics.log_loss(t_test_ab, y_test_prob_ab)\n\nprint('Accuracy score on train', acurMLP_ab_train)\nprint('Accuracy score on validation', acurMLP_ab_valid)\nprint()\nprint('CE on train', ceMLP_ab_train)\nprint('CE on test',ceMLP_ab_valid)","bf982ae9":"print([acurSGD_fs_train, acurSGD_ia_train, acurSGD_ab_train, acurMLP_fs_train, acurMLP_ia_train, acurMLP_ab_train])\nprint([acurSGD_fs_valid, acurSGD_ia_valid, acurSGD_ab_valid, acurMLP_fs_valid, acurMLP_ia_valid, acurMLP_ab_valid])\nprint([ceSGD_fs_train, ceSGD_ia_train, ceSGD_ab_train, ceMLP_fs_train, ceMLP_ia_train, ceMLP_ab_train])\nprint([ceSGD_fs_valid, ceSGD_ia_valid, ceSGD_ab_valid, ceMLP_fs_valid, ceMLP_ia_valid, ceMLP_ab_valid])","860d9dd7":"names = [\"SGD FamilySize\", \"SGD IsAlone\", \"SGD FamilySize-AgeBand\", \"MLP FamilySize\", \"MLP IsAlone\", \"MLP FamilySize-AgeBand\"]\ntrain_acur = [acurSGD_fs_train, acurSGD_ia_train, acurSGD_ab_train, acurMLP_fs_train, acurMLP_ia_train, acurMLP_ab_train]\nvalid_acur = [acurSGD_fs_valid, acurSGD_ia_valid, acurSGD_ab_valid, acurMLP_fs_valid, acurMLP_ia_valid, acurMLP_ab_valid]\ntrain_loss = [ceSGD_fs_train, ceSGD_ia_train, ceSGD_ab_train, ceMLP_fs_train, ceMLP_ia_train, ceMLP_ab_train]\nvalid_loss = [ceSGD_fs_valid, ceSGD_ia_valid, ceSGD_ab_valid, ceMLP_fs_valid, ceMLP_ia_valid, ceMLP_ab_valid]\n\ndf = pd.DataFrame(list(zip(names, train_acur, valid_acur, train_loss, valid_loss)), columns = ['Name', 'Train_accuracy', 'Validation_accuracy', 'Train_loss', 'Validation_loss'])","7d168234":"sns.catplot(x=\"Train_accuracy\", y=\"Name\", hue=\"Name\", kind=\"bar\", palette=sns.color_palette(\"deep\"), data=df, height=5, aspect=4)","d35c8580":"sns.catplot(x=\"Train_loss\", y=\"Name\", hue=\"Name\", kind=\"bar\", palette=sns.color_palette(\"deep\"), data=df, height=5, aspect=4)","7f381e83":"sns.catplot(x=\"Validation_accuracy\", y=\"Name\", hue=\"Name\", kind=\"bar\", palette=sns.color_palette(\"deep\"), data=df, height=5, aspect=4)","2d56584d":"sns.catplot(x=\"Validation_loss\", y=\"Name\", hue=\"Name\", kind=\"bar\", palette=sns.color_palette(\"deep\"), data=df, height=5, aspect=4)","b3db015b":"test[\"Fare\"] = test[\"Fare\"].fillna(test[\"Fare\"].mean())\ntest.Fare.isna().any()","5d089084":"test_ac = test[[\"AgePclass\", \"AgeBand\", \"Fare\", \"Mr\", \"Mrs\", \"Miss\", \"Master\", \"Embarked_s\", \"Embarked_c\", \"Embarked_q\", \"Male\", \"Female\", \"FamilySize\"]]\n\ny_pred = SGD_cls_ab.predict(test_ac)\n\nsubmission = pd.DataFrame({\n        \"PassengerId\": test[\"PassengerId\"],\n        \"Survived\": y_pred\n    })\nsubmission.to_csv('submission.csv', index=False)","4a7380de":"As observed before, it look like `Pclass`, and `Age` have some correlation between them, we may assume that the age of a passenger's may indicates which class he was travled at.  \nOlder people sometimes tend to have more money and it could indicate they traveled in higher class.  \nWe will try to create a new feature `AgePclass` as a multiplation between those two features.","ddbd4414":"We need to fill the missing `Age` values in the test dataset.","123aefa4":"`Parch` and `SibSp` could be merged into one `FamilySize` variable.","9f5b24ea":"It look like there is a dipendency between `Age`, `Pclass`, and `Title_cat`.\nLet's try to fill the emtpy `Age` values with the means of those 2 variables.","593551fc":"Now let's observe the data and it's structure.\nIt contains 891 rows with 11 features and the target which means if a passenger survived or not.","09d0eebe":"# Import The Data","2c791d8a":"# Test","648dc450":"We can assume that as the age growth, the class of a passenger could growth too, lets see a graph to demonstrate.","3af19f2b":"Understanding the keys meaning:\n* `PassengareId` - Id of the passenger .\n* `Survived` - Whether the passenger survives or not.\n* `Pclass` - The ticket class (SES)\n* `Name` - Name of the passenger.\n* `Sex` - Gender of the passenger.\n* `Age` - Age of the passenger.\n* `SibSp` - Number of siblings and spouse.\n* `Parch` - Number of parents and children.\n* `Ticket` - Ticket number.\n* `Fare` - How much the passenger paid.\n* `Cabin` - Cabin of the passenger.\n* `Embarked` - Which port the passenger borded.","69aef60b":"The test dataset missing one value in `Fare` variable, we will fill it with the mean of passengers with the same `Pclass`, `Title_cat`.","acb4baa4":"### Cabin Variable\n`Cabin` which indicates where a passenger was staying at the titanic, is missing 687 values out of 891, that's about 77% of missing values.\nFor now we will try to drop that variable from this model and will see if adding it later on will improve the results.\n","98efa184":"# Intro\nIn this competition I will investigate the data of passengers who boarded the Titanic.  \nI will try to study the connection between the different features in this data in order to create a model which will be able to predict whether a passenger survived or not.  \nI will use two types of models with different features to find the best model for this data.\n* SGD - a GD-like linear regression algorithm.\n* MLP - an artificial nueral network.","45ff2753":"* Guy Kabiri\n* https:\/\/www.kaggle.com\/guykabiri","93a1e0b8":"We can see that the best train results was with SGD with `AgeBand` feature.  \nThe best validation results was with SGD with `AgeBand` feature as well.  \nAlso, the smallest loss (train, and validation) was with SGD with `AgeBand` feature.  \nSo we will use this model with the test.","f9f001d4":"## Train","87b917a3":"We can see that the accuracy is a little bit higher and the loss is a little bit smaller in this model.","145eabf8":"# Imports","837976b0":"# Links\n\n\n* Fill `Embarked` missing values with the most common values. [Suggested here](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions#Completing-a-categorical-feature)\n\n* Create `Title` feature instead of `Name`. [Suggested here](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions#Creating-new-feature-extracting-from-existing)\n\n* Drop `PassengerId`, `Cabin` and `Ticket`. [Suggested here](https:\/\/www.kaggle.com\/ldfreeman3\/a-data-science-framework-to-achieve-99-accuracy#3.22-Clean-Data)\n\n* Dropping `SibSp` and `Parch` in favor for `FamilySize` feature. [Suggested here](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions#Create-new-feature-combining-existing-features)\n\n* Consider `isAlone` feature instead of `SibSp`, `Parch` and `FamilySize`. [Suggested here](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions#Create-new-feature-combining-existing-features)\n\n* Consider change `Age` into an ordinal variable based on age ranges. [Suggested here](https:\/\/www.kaggle.com\/startupsci\/titanic-data-science-solutions#Create-new-feature-combining-existing-features)","86e14a58":"# Score\n\n![](https:\/\/user-images.githubusercontent.com\/52006798\/99272182-2b9a3600-2830-11eb-9432-a078aad8da1c.jpg)\n![](https:\/\/user-images.githubusercontent.com\/52006798\/99245606-85890480-280c-11eb-8ac3-c0783006dacf.jpg)\n![](https:\/\/user-images.githubusercontent.com\/52006798\/99245667-a2bdd300-280c-11eb-8149-dcd446f4e8d5.jpg)","e3a89319":"## Feature Engeneering","6fdd5692":"### Back to `Title`\nNow after done handling the `Age` feature, we can convert the `Title_cat` into seperate binary features.","cef8da27":"Creating the `FamilySize` variable for the test dataset as well.","fced02be":"As predicted, female had much higher chance to survive.  \n`Sex` is a non-numerical column, we will convert into two binary variables `Male`, and `Female`.","bd3f9dab":"We can see that when a passenger has some relatives traveled with him, he had some better chances to survived.  \nWe can create another feature that might increase the model accuracy.  \n`IsAlone` feature will indicate whether a passenger was traveling alone or with relatives.","5d9864db":"### Age Variable:\n`Age` missing 177 value, not too much in terms of this data size, it can be filled with the median value of passengers with similar values, to decide which values, let's see the correlation between `Age` and the other features.","0f497ae5":"### Embarked Variable:\n`Embarked` indicates in which port a passenger boarded the Titanic.  \nIt could be crutial to determine whether a passenger survived or not becuase it might indicate on a passenger's room location on the titanic.","31f931a9":"### MLP Train with `FamilySize` Featrure:","117012ae":"There are a lot of titles, some of them can be combined.\n* Combine all the rates titles into `Rare`.\n* Combine the variante of `Miss`.\n* Combine the variante of `Msr`.","b74ae5ca":"# Explore The Data\n\nFirst of all, let's see the keys.","caf99e67":"### MLP Train with `AgeBand` Featrure:","9687b2a9":"### Name Variable:\nThe name seem to be irrelevent to the chance of a passenger to survive, but we can use it to extract a passenger's title into a `Title` categorical variable.\nFirst let's see all the different titles the passengers have.","47777a06":"From this graph it looks like there are different surviving chances across the different age bands.  \nWe will create a new `AgeBand` feature.","bdca31c9":"The test dataset does not miss any `Embarked` values, let's just convert it to 3 binary variables as well.","dd6abe6b":"It is clear that a passenger from 1st class and 2nd class has much more chances to survive compare to 3rd class passengers.","0ef80418":"Now the `Title` variable can be converted into a categorical variable.","1ea8bf2d":"## Handle Non-Numerical Variables and Missing Values\n","79bf4053":"We know the 1st class is better than 2nd class, and 2nd class is better than 3rd class.  \nIn this graph we can actually see that the more a passenger is older, his class is higher.","11f8164e":"## Variable Analysis\n* Categorical Variables: Survived, Sex, Cabin and Embarked.\n* Ordinal Variables: Pclass, SubSp and Parch\n* Numerical Variables: Age and Fare.","80a1b9f1":"# Compare the Models","a70abb3f":"Now let's perform this manipulation on the test as well.","d93ca01d":"### SGD Train with `AgeBand` Featrure:","a1e77362":"### SGD Train with `IsAlone` Featrure:","57cd143b":"### Pclass Variable:\n`Pclass` indicates a passenger class, it indicates his room location and could affect his surviving chances.","832b9bdb":"Convert the test dataset to binary as well.","c4063cae":"### SGD Train with `FamilySize` Featrure:","c652e23b":"It looks like there are some diffrences in surviving chances between the different `Embarked` locations.  \nThere are only 3 unique values, it will be converted into 3 binary variables.\nBut before we change the variavble into a numerical one, we need to fill the missing values, it can be filled with the most common value.","7aec91a2":"We get better results with `FamilySize` feature and not with `IsAlone` so we will continue with it.","e34aa3b5":"### Sex Variable:\nThe gender of a passenger might have a strong connection to surviving chances because rescures might help first to women.\n","0e8537b4":"### MLP Train with `IsAlone` Featrure:"}}