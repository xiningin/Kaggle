{"cell_type":{"5d178a61":"code","940ddb68":"code","2f6a35ab":"code","8145cf1b":"code","95ab30eb":"code","cc89c6f5":"code","18d8b63c":"code","f31b6497":"code","3ef9ad79":"code","c44837d0":"code","54035fd2":"code","61662b0e":"code","f4641a7a":"code","aef6c645":"code","7a6f017b":"code","5fc78108":"code","f7dbffe9":"code","e87e4668":"code","353dbd7a":"code","5a5f22de":"code","0788aa99":"code","d93161a3":"code","06adfa64":"code","61d533e1":"code","33aaee2d":"code","fde2c5b2":"code","c013a0f2":"code","52d79ae0":"code","209e567d":"code","6c49a4e1":"code","997584d5":"code","39f88cb0":"code","742c3013":"code","98e35400":"code","798d610d":"code","37e4a859":"code","9f0ea586":"code","a22595c0":"code","55991183":"code","eece01d1":"code","c07b8e9e":"code","16bf4c8b":"code","e9455271":"code","03cad06d":"code","0693da1c":"code","65063a4b":"code","941eb1da":"code","2537165b":"code","03f91902":"code","dec61e0f":"code","e953320b":"markdown","a72ae36a":"markdown","359cdc29":"markdown","00be73d8":"markdown","6e953b2d":"markdown","f93dd8bb":"markdown","95f68e51":"markdown","804e2f8b":"markdown","beff2457":"markdown","48c9274f":"markdown","61a689f6":"markdown","2707e5f7":"markdown","efbbc21f":"markdown","e4d3eca0":"markdown","378c178a":"markdown","49010e97":"markdown","284105c1":"markdown","08025216":"markdown","930ead7a":"markdown","0a8380cc":"markdown","332d62e8":"markdown","e9b16f74":"markdown","ca003e8d":"markdown","47e9cc3f":"markdown","1345c6ef":"markdown","64ce3dfc":"markdown","44255a52":"markdown","ce63971f":"markdown","97f4d589":"markdown","e454dc4d":"markdown","aa198d00":"markdown","dcb60238":"markdown","05e60b64":"markdown","a040bf8c":"markdown","a0bd896c":"markdown","cdbdee96":"markdown","036eb4da":"markdown","aecdebc7":"markdown","ead62327":"markdown","107f9281":"markdown","c54f935e":"markdown","3251b624":"markdown","f0dd80ae":"markdown","390f1279":"markdown","ae837438":"markdown"},"source":{"5d178a61":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport matplotlib.style as style\nstyle.use('seaborn-whitegrid')\nimport seaborn as sns\n\n\npd.set_option('display.max_rows', 100)\npd.set_option('display.max_columns', 500)\n","940ddb68":"train_feature_url = '..\/input\/train_features.csv'\ntest_feature_url = '..\/input\/test_features.csv'\ntrain_labels_url = '..\/input\/train_labels.csv'\n\n","2f6a35ab":"train_F = pd.read_csv(train_feature_url, index_col='id')\ntest_F = pd.read_csv(test_feature_url, index_col='id')\ntrain_L = pd.read_csv(train_labels_url, index_col='id')\n\ntrain_F.shape, test_F.shape, train_L.shape  #,  train_F_C.shape, test_F_C.shape, # ","8145cf1b":"train_F.dtypes","95ab30eb":"null_list = []\n\nfor col in train_F.columns:\n  if train_F[col].isnull().sum() > 0:\n    null_list.append(col)\n    \nnull_list.append('recorded_by')\nnull_list.append('date_recorded')\n    \nnull_list","cc89c6f5":"def convert_datetime(df, col):\n  df[col] = pd.to_datetime(df[col])\n  df['day_of_week'] = df[col].dt.weekday_name \n  df['year'] = df[col].dt.year\n  df['month'] = df[col].dt.month \n  df['day'] = df[col].dt.day \n  \n  return None\n\n","18d8b63c":"convert_datetime(train_F, 'date_recorded')\nconvert_datetime(test_F, 'date_recorded')\n\ntrain_F.dtypes","f31b6497":"train_F['region_code'] = train_F['region_code'].astype('category')\ntest_F['region_code'] = test_F['region_code'].astype('category')\ntrain_F['district_code'] = train_F['district_code'].astype('category')\ntest_F['district_code'] = test_F['district_code'].astype('category')\ntrain_F['wpt_name'] = train_F['wpt_name'].astype('category')\ntest_F['wpt_name'] = test_F['wpt_name'].astype('category')\ntrain_F['ward'] = train_F['ward'].astype('category')\ntest_F['ward'] = test_F['ward'].astype('category')\n\n\ntrain_F.dtypes","3ef9ad79":"def df_split(df):\n  numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n  df_num = df.select_dtypes(include=numerics)\n  df_cat = df.drop(df_num, axis = 'columns')\n  print (df.shape, df_num.shape, df_cat.shape)\n  return df_num, df_cat\n  ","c44837d0":"train_F_num, train_F_cat = df_split(train_F)\n\ntest_F_num, test_F_cat = df_split(test_F)","54035fd2":"train_F_num.describe().T","61662b0e":"train_F_num['construction_year'].loc[train_F_num['construction_year'] == 0] = train_F_num['year']\ntest_F_num['construction_year'].loc[test_F_num['construction_year'] == 0] = test_F_num['year']\n\ntrain_F_num.describe().T","f4641a7a":"mean_lat_train = train_F_num['latitude'].mean()\nmean_long_train = train_F_num['longitude'].mean()\nmean_lat_test = test_F_num['latitude'].mean()\nmean_long_test = test_F_num['longitude'].mean()\n\n\ntrain_F_num['distance'] = np.sqrt((train_F_num['longitude'] - mean_long_train)**2 + (train_F_num['latitude'] - mean_lat_train)**2)\ntest_F_num['distance'] = np.sqrt((test_F_num['longitude'] - mean_long_test)**2 + (test_F_num['latitude'] - mean_lat_test)**2)\n\ntrain_F_num['distance3d'] = np.sqrt((train_F_num['gps_height']**2 + train_F_num['longitude'] - mean_long_train)**2 + (train_F_num['latitude'] - mean_lat_train)**2)\ntest_F_num['distance3d'] = np.sqrt((test_F_num['gps_height']**2 + test_F_num['longitude'] - mean_long_test)**2 + (test_F_num['latitude'] - mean_lat_test)**2)\n\n\ntrain_F_num.describe().T","aef6c645":"for col in train_F_cat.columns:\n  print (col, train_F_cat[col].nunique())","7a6f017b":"null_list","5fc78108":"cols_kept = []\n\nfor col in train_F_cat.columns:\n  if col not in null_list:\n    if train_F_cat[col].nunique() <= 125:\n      cols_kept.append(col)\n    \nprint (len(cols_kept))\n    \ncols_kept\n\n","f7dbffe9":"small_cat_train = train_F_cat[cols_kept]\nsmall_cat_test = test_F_cat[cols_kept]\n\nsmall_cat_train.shape, small_cat_test.shape","e87e4668":"def dummy_df(category_df):\n  df_dummy = pd.DataFrame()\n  for col in category_df.columns:\n    df_dummy = pd.concat([df_dummy, pd.get_dummies(category_df[col], drop_first=True, prefix = 'Is')], axis='columns')\n  return df_dummy","353dbd7a":"df_dumb_train = dummy_df(small_cat_train)\ndf_dumb_test = dummy_df(small_cat_test)","5a5f22de":"df_dumb_train.shape, df_dumb_test.shape","0788aa99":"a = list(df_dumb_train.columns.values)\n\nprint(a)\nprint(len(a))","d93161a3":"b = list(df_dumb_test.columns.values)\n\nprint(b)\nprint(len(b))","06adfa64":"a == b","61d533e1":"def ex_cols(a,b):\n  ex_a = []\n  ex_b = []\n  for i in range(0,len(a)):\n    if a[i] not in b:\n      ex_a.append(a[i])\n  for j in range(0,len(b)):\n    if b[j] not in a:\n      ex_b.append(b[j])\n  return ex_a, ex_b\n\nex_a, ex_b = ex_cols(a,b)\n\nex_a,ex_b","33aaee2d":"for col in df_dumb_train.columns:\n  if col in ex_a:\n    del df_dumb_train[col]\n\nfor col in df_dumb_test.columns:\n  if col in ex_b:\n    del df_dumb_test[col]\n\ndf_dumb_train.shape, df_dumb_test.shape","fde2c5b2":"c = list(df_dumb_train.columns.values)\nd = list(df_dumb_test.columns.values)\n\nc == d","c013a0f2":"for i in range(0,len(c)):\n  if c[i] != d[i]:\n    print(\"No match\")\n    ","52d79ae0":"X_train = pd.concat([train_F_num,df_dumb_train],axis='columns')\nX_test = pd.concat([test_F_num,df_dumb_test],axis='columns')\n\nX_train.shape, X_test.shape","209e567d":"X_train.head()","6c49a4e1":"np.any(np.isnan(X_train)), np.any(np.isnan(X_test))\n","997584d5":"train_L.head()","39f88cb0":"train_L['status_group'] = train_L['status_group'].astype('category')\n\ntrain_L.dtypes, train_L.shape","742c3013":"y_train = train_L['status_group']\n\ny_train.value_counts()","98e35400":"majority_class = y_train.mode()[0]\n\nprint(majority_class)\n\ny_pred = pd.DataFrame(np.full(shape=len(X_test), fill_value = majority_class))\n","798d610d":"temp = X_test.reset_index()\n\ny_pred['id'] = temp['id'].values\ny_pred.rename(columns={0:'status_group'}, inplace=True)\ny_pred.set_index('id', inplace=True)\n\ny_pred.head()\n\nprint(y_pred.shape)","37e4a859":"# from google.colab import files\n\n# y_pred.to_csv('majority_class.csv')\n# files.download('majority_class.csv')","9f0ea586":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\n\n\nclf_rf = RandomForestClassifier(n_estimators=100, \n                                max_depth=34,\n                                min_samples_split = 17,\n                                min_samples_leaf = 1,\n                                criterion = 'gini', \n                                max_features = 6, \n                                oob_score = True, \n                                random_state=237)\n\nclf_lr = LogisticRegression(random_state=237, solver='lbfgs', multi_class='multinomial', max_iter=1000)\n\n","a22595c0":"def quick_eval(X,y, clf):\n  from sklearn.model_selection import train_test_split\n  from sklearn.preprocessing import StandardScaler\n  from sklearn.preprocessing import RobustScaler\n  from sklearn.metrics import accuracy_score\n\n  \n  X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, shuffle = True, random_state=237)\n  print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)\n  \n  scaler = StandardScaler()\n#   scaler = RobustScaler()\n  X_train_clf = scaler.fit_transform(X_train)\n  X_test_clf = scaler.transform(X_test)\n  \n  clf.fit(X_train_clf, y_train)\n  \n  y_pred_train = clf.predict(X_train_clf)\n  \n  y_pred = clf.predict(X_test_clf)\n  \n  \n  return accuracy_score(y_train,y_pred_train), accuracy_score(y_test, y_pred)","55991183":"# %%time\n\nquick_eval(X_train, y_train, clf_rf)","eece01d1":"# %%time\n\n# from sklearn.model_selection import RandomizedSearchCV\n# from scipy.stats import randint as sp_randint\n# # parameters for GridSearchCV\n# # specify parameters and distributions to sample from\n# param_dist = {\"n_estimators\": [100, 200,300],\n#               \"max_features\": sp_randint(5, 9),\n#               \"max_depth\": [18,22,26,30,34,38],\n#               \"min_samples_split\": sp_randint(8, 32),\n#               \"min_samples_leaf\": sp_randint(1, 20)              \n#              }\n# # run randomized search\n# n_iter_search = 200\n# random_search = RandomizedSearchCV(clf_rf, param_distributions=param_dist,\n#                                    n_iter=n_iter_search)\n\n# random_search.fit(X_train, y_train)","c07b8e9e":"# def report(results, n_top=5):\n#     for i in range(1, n_top + 1):\n#         candidates = np.flatnonzero(results['rank_test_score'] == i)\n#         for candidate in candidates:\n#             print(\"Model with rank: {0}\".format(i))\n#             print(\"Mean validation score: {0:.3f} (std: {1:.3f})\".format(\n#                   results['mean_test_score'][candidate],\n#                   results['std_test_score'][candidate]))\n#             print(\"Parameters: {0}\".format(results['params'][candidate]))\n#             print(\"\")\n            \n# report(random_search.cv_results_)            ","16bf4c8b":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import MaxAbsScaler\n\nscaler = StandardScaler()\n# scaler = MinMaxScaler()\n# scaler = RobustScaler()\n# scaler = MaxAbsScaler()\n\nX_train_clf = scaler.fit_transform(X_train)\nX_test_clf = scaler.transform(X_test)\n\n\nX_train_clf.shape, y_train.shape, X_test_clf.shape","e9455271":"%%time\n\nfrom sklearn.model_selection import cross_validate\n\nfrom sklearn.metrics import accuracy_score\n\n\nscores = cross_validate(clf_rf,\n                        X_train_clf,y_train, \n                        scoring = 'accuracy', cv=5) \n\n","03cad06d":"pd.DataFrame(scores)","0693da1c":"def predictor(X_train, X_test, y_train, clf):\n  from sklearn.preprocessing import StandardScaler\n  from sklearn.preprocessing import RobustScaler\n \n  from sklearn.metrics import accuracy_score\n  \n\n  y_pred = pd.DataFrame()\n  \n  temp_test = X_test.reset_index()\n  y_id = temp_test['id']\n#   scaler = StandardScaler()\n  scaler = RobustScaler()\n  \n  X_train_clf = scaler.fit_transform(X_train)\n\n  X_test_clf = scaler.transform(X_test)\n  clf.fit(X_train_clf, y_train)\n  \n  y_pred_train = clf.predict(X_train_clf)\n  \n  print (f'\\nThe accuracy score of the training set is {round(accuracy_score(y_train, y_pred_train), 5)}\\n')\n  \n  prediction = pd.DataFrame(clf.predict(X_test_clf))\n  \n  y_pred = pd.concat([y_id, prediction], axis='columns')\n\n  y_pred.rename(columns={0:'status_group'}, inplace=True)\n  \n  y_pred.set_index('id', inplace=True)\n  \n  return y_pred","65063a4b":"%%time\n\ndf = predictor(X_train, X_test, y_train, clf_rf)\n\n","941eb1da":"df.head()","2537165b":"df['status_group'].value_counts()","03f91902":"df.shape","dec61e0f":"# from google.colab import files\n\n# df.to_csv('submission.csv')\n# files.download('submission.csv')","e953320b":"# Kaggle Competition - Tanzania Waterpoint Prediction","a72ae36a":"### Data Overview","359cdc29":"We have 39 features and 1 target variable. I will do some further investigation to see if I can get more features.","00be73d8":"This is the final check to see if the prediction vector is formatted correctly and has the correct shape. ","6e953b2d":"The first guess in a prediction is the majority class baseline. It's the simplest model and the result of this prediction serves as a reference point against future predictions. For the majority class baseline, I am predicting against the actual test target using the mode of the training target. I achieved **53.75%** accuracy using this baseline.","f93dd8bb":"### Creating the Prediction Vector","95f68e51":"### Cross Validation","804e2f8b":"### Feature Cleanup and Engineering","beff2457":"Now that I have a baseline accuracy, I want to improve on that. Looking at the training target variable, one can see it has three values. Therefore, I will need a *multinomial* classifier.\n\nMy choice of classifiers are *Multinomial Logistic Regression* and *Random Forest*.\n\n**Documentation**:\n\n[Logistic Regression](http:\/\/https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LogisticRegression.html)\n\n[Random Forest](http:\/\/https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html)\n\n**Background:**\n\n[Logistic Regression - Wikipedia](http:\/\/https:\/\/en.wikipedia.org\/wiki\/Logistic_regression)\n\n[Random Forest - Wikipedia](http:\/\/https:\/\/en.wikipedia.org\/wiki\/Random_forest)","48c9274f":"**Features**\n\n        Your goal is to predict the operating condition of a waterpoint for each record in the dataset. You are provided the following set of information about the waterpoints:\n\n      amount_tsh : Total static head (amount water available to waterpoint) - numeric\n      date_recorded : The date the row was entered - datetime\n      funder : Who funded the well\n      gps_height : Altitude of the well\n      installer : Organization that installed the well\n      longitude : GPS coordinate\n      latitude : GPS coordinate\n      wpt_name : Name of the waterpoint if there is one\n      num_private :\n      basin : Geographic water basin\n      subvillage : Geographic location\n      region : Geographic location\n      region_code : Geographic location (coded)\n      district_code : Geographic location (coded)\n      lga : Geographic location\n      ward : Geographic location\n      population : Population around the well\n      public_meeting : True\/False\n      recorded_by : Group entering this row of data\n      scheme_management : Who operates the waterpoint\n      scheme_name : Who operates the waterpoint\n      permit : If the waterpoint is permitted\n      construction_year : Year the waterpoint was constructed\n      extraction_type : The kind of extraction the waterpoint uses\n      extraction_type_group : The kind of extraction the waterpoint uses\n      extraction_type_class : The kind of extraction the waterpoint uses\n      management : How the waterpoint is managed\n      management_group : How the waterpoint is managed\n      payment : What the water costs\n      payment_type : What the water costs\n      water_quality : The quality of the water\n      quality_group : The quality of the water\n      quantity : The quantity of water\n      quantity_group : The quantity of water\n      source : The source of the water\n      source_type : The source of the water\n      source_class : The source of the water\n      waterpoint_type : The kind of waterpoint\n      waterpoint_type_group : The kind of waterpoint\n\n\n**Labels**\n\nThere are three possible values:\n\n      functional : the waterpoint is operational and there are no repairs needed\n      functional needs repair : the waterpoint is operational, but needs repairs\n      non functional : the waterpoint is not operational","61a689f6":"In this kernel, I will walkthorough the procedure of how I achieved over 80% accuracy in the prediction of Tanzania's waterpoint. It's important to recognize before proceding that the goal of this prediction is to maximize accuracy so I will not discuss the interpreation of each of the features or the statistical significance of each of them.","2707e5f7":"**Converting numeric columns to category type**","efbbc21f":"In this process, there is an occasional mismatch between the number of training and testing features which will cause problems with the model down the road. I will fix those issues here.","e4d3eca0":"### Prediction using better classifiers","378c178a":"### Discussion","49010e97":"**Converting the date variable into a datetime object and splitting it into pieces**","284105c1":"### Validation with just the training set split","08025216":"#### Check to see the Train and Test Features have the same columns in the same order.","930ead7a":"#### Converting Categorical variables to Dummy Variables","0a8380cc":"**Tuning the parameters - Randomized Grid Search - Random Forest (est. runtime 5 hours)**","332d62e8":"### Submission Download","e9b16f74":"I will use a function to divide the training and testing data into numerical and catagorical data.","ca003e8d":"In order to improve on the accuracy, I will use a Randomized Grid Search to find the tuning parameters that maximize accuracy.","47e9cc3f":"The results of the cross-validation show consistent results as well as minimal overfit to the training model. With a Random Forest classifier, one of the problems with it usually is overfit to the training set which causes massive differences between training and testing accuracy. As this accuracy gap is the smallest I've experienced, I'm keeping this model. ","1345c6ef":"#### Splitting into Numeric and Nonnumeric variables","64ce3dfc":"I will do some datatype conversion to exploit characteristics of variables that are not in the original feature.","44255a52":"The first step is to import the data and make sure the import is the desired one. ","ce63971f":"#### Feature Engineering in the numeric sets","97f4d589":"In this cross-validation stage, I confirm that the results are consistent.","e454dc4d":"**Classifiers**","aa198d00":"### Target Variable","dcb60238":"### Sanity Check","05e60b64":"**Creating new variables \"distance\" and \"distance3D\"**","a040bf8c":"#### Combining to form the Feature Matrices","a0bd896c":"I will now convert the categorical variables into dummy variable dataframes using the pandas get_dummies() method. In the last step, I will concat these into one large dummy variable dataframe.","cdbdee96":"### Importing packages and files","036eb4da":"### Majority Class Baseline - First,Guess","aecdebc7":"#### Creating Dummy variables out of the less unique (<=125 categories) categorical variables.","ead62327":"In the last pre-modeling step for the features, I recombine the feature engineered numeric and categorical matrices.","107f9281":"Now that the feature matrices are complete, I start work on the target variable vector to make confirm it is working properly.","c54f935e":"### The Problem\n\nCan you predict which water pumps are faulty?\nUsing data from Taarifa and the Tanzanian Ministry of Water, can you predict which pumps are functional, which need some repairs, and which don't work at all? Predict one of these three classes based on a number of variables about what kind of pump is operating, when it was installed, and how it is managed. A smart understanding of which waterpoints will fail can improve maintenance operations and ensure that clean, potable water is available to communities across Tanzania.","3251b624":"**Finding Columns with Null Values for later**","f0dd80ae":"I will now look at the numeric data using the pandas describe() method to find any flaws and fix them as best as possible.","390f1279":"This was a walkthrough through the process of taking a dataset and using it to achieve a high accuracy score. As stated earlier, this is not a discussion of the significance of the features but rather the accuracy of the model with the classifier as a whole. I started with a quick investigation and data cleanup followed by feature engineering. From there, I formatted the training target variable and used that to get a majority class baseline prediction vector. I then introduced the Logistic Regression and Random Forest Classifers, choosing to use the Random Forest for its improved accuracy. Using a Randomized Grid Search, I fine tuned the parameters of the Random Forest Classifer to achieve low overfit and greater accuracy. I cross validated the results to demonstrate the consistency of the results with each other and with the previous results. Finally with this tuned model, I used it to predict the test target vector using the test features matrix. \n\nFrom this process, I saw that the random forest classifier beat the multinomial logistic regression classifier by about 10-15% but among the drawbacks to using a random forest classifier are 1) overfit to the training set and 2) lack of interpretability of the features. However, since the stated goal was to achieve maximum accuracy, the choice of the random forest classifier is acceptable to me. ","ae837438":"This is a check to see that there are no strange values that cause errors in the prediction. "}}