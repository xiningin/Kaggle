{"cell_type":{"9d6893ba":"code","d741f9e1":"code","e9b678bc":"code","0fc186a4":"code","0bb19fa7":"code","5edbe6af":"code","e59cb365":"code","5345ef5b":"code","6527c1d3":"code","cd0bba98":"code","273e63cf":"code","20ca830e":"code","16b61e74":"code","df104fd6":"code","8e687717":"code","2e1ae121":"code","98d7e235":"code","ab243f34":"code","a2e892bb":"code","066a728a":"code","2ee96f14":"code","bb86a531":"code","5ff246ee":"code","75107696":"code","50e0465a":"markdown","ae6ce2e9":"markdown","e42eed0c":"markdown","4235985d":"markdown","d06631b5":"markdown","8b0297a2":"markdown","0d3d85fa":"markdown","8331ae78":"markdown","15dd069c":"markdown","e03c7773":"markdown","c1a57f8d":"markdown","62c40f4c":"markdown","cd030985":"markdown","182f9b3b":"markdown","24694953":"markdown","f2f3742e":"markdown","24758429":"markdown","79988992":"markdown","00feeac6":"markdown","1e3b5b93":"markdown","7d629b13":"markdown","f2aeaa57":"markdown","29c1570b":"markdown","3f6cc48e":"markdown","b2318d1f":"markdown","26e7157f":"markdown","577e8b65":"markdown"},"source":{"9d6893ba":"import numpy as np\nimport matplotlib.pyplot as plt\nimport os\nimport unicodedata\nimport re","d741f9e1":"filepath = '\/kaggle\/input\/dialog.txt'\ndata = []\nwith open(filepath, 'rb') as f:\n    for line in f:\n        data.append(line.decode(encoding='utf-8', errors='replace')[:-2])\nprint(f\" Loaded {len(data)} lines of data ...\")","e9b678bc":"# Function for plotting histogram\ndef plothist(sizes):\n    print(f\"Minimum line size = {min(sizes)}\\nMaximum line size = {max(sizes)}\")\n    _ = plt.hist(sizes, range(0, 250))\n    plt.xlabel('Sentence Length', fontsize=15)\n    plt.ylabel('Number of Occurences', fontsize=15)\n    plt.title('Length Distribution', fontsize=25)\n    plt.show()","0fc186a4":"line_sizes = [len(line) for line in data]\nplothist(line_sizes)","0bb19fa7":"ind = line_sizes.index(max(line_sizes))\ndata[ind]","5edbe6af":"def make_pair_conversation(data):\n    # return list of pair conversation\n    pair = []\n    for i in range(len(data)-1):\n        inp = data[i]\n        rep = data[i+1]\n        pair.append([inp, rep])\n\n    return pair","e59cb365":"sample = data[:501]\nsamp_pairs = make_pair_conversation(sample)\nprint(f\"The sample pairs has {len(samp_pairs)} pairs and each pair has {len(samp_pairs[0])} conversations\")","5345ef5b":"# A single Multipurpose class for managing all the Vocabulary stuff.\n\npad_token = 0\nsos_token = 1\neos_token = 2\n\nclass Voc:\n    def __init__(self):\n        self.trimmed = False\n        self.word2index = {'PAD':pad_token, 'SOS':sos_token, 'EOS':eos_token}\n        self.word2count = {}\n        self.index2word = {pad_token:'PAD', sos_token:'SOS',\n                           eos_token:'EOS'}\n        self.numword = 3\n\n    def add_sentence(self, sentence):\n        for word in sentence.split(' '):\n            self.addword(word)\n\n    def addword(self, word):\n        if word not in self.word2index:\n            self.word2index[word] = self.numword\n            self.word2count[word] = 1\n            self.index2word[self.numword] = word\n            self.numword += 1\n        else:\n            self.word2count[word] += 1\n\n    def trim(self, min_count):\n        \"\"\"\n        based on the wordcount dictionary, keep the words with\n        frequency greater than min_count\n        \"\"\"\n        if self.trimmed:\n            return\n        self.trimmed = True\n        keep_word = []\n        for word, num_freq in self.word2count.items():\n            if num_freq >= min_count:\n                keep_word.append(word)\n\n        # reinitialize the dictionaries\n        self.word2index = {'PAD':pad_token, 'SOS':sos_token, 'EOS':eos_token}\n        self.word2count = {}\n        self.index2word = {pad_token:'PAD', sos_token:'SOS',\n                           eos_token:'EOS'}\n        self.numword = 3\n        for word in keep_word:\n            self.addword(word)","6527c1d3":"def unicodeToAscii(line):\n    return ''.join([ch for ch in unicodedata.normalize('NFD', line) if unicodedata.category(ch) != 'Mn'])\n\n# Lowercase, trim, and remove non-letter characters\ndef normalizeString(s):\n    s = unicodeToAscii(s.lower().strip())\n    s = re.sub(r\"([.!?])\", r\" \\1\", s)\n    s = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", s)\n    s = re.sub(r\"\\s+\", r\" \", s).strip()\n    return s","cd0bba98":"# normalizing the function and initializing the Vocab dictionary\ndef normalize_and_voc(pair_convs):\n    pairs = [[normalizeString(s) for s in one_pair] for one_pair in pair_convs]\n    voc = Voc()\n    return voc, pairs","273e63cf":"def filterpair(pairs, max_length):\n    \"\"\"\n    Input: pair with format: [input, response],\n    we check the length of both input, reponse to identify\n    whether or not they are smaller than max length\n    return pair with length < max_length\n    \"\"\"\n    valid_pair = []\n    for pair in pairs:\n        input_words, reponse_words = pair[0].split(' '), pair[1].split(' ')\n        if len(input_words) < max_length and len(reponse_words) < max_length:\n            valid_pair.append(pair)\n    return valid_pair","20ca830e":"def loadPrepareData(pair_convs, max_length):\n    voc, pairs = normalize_and_voc(pair_convs)\n    valid_pair = filterpair(pairs, max_length)\n    print(f\"load total {len(pairs)} pairs\")\n    print(f\"load total {len(valid_pair)} pairs with length <= {max_length}\")\n    for pair in valid_pair:\n        voc.add_sentence(pair[0])\n        voc.add_sentence(pair[1])\n    print(f\"Total number of words in vocab is : {voc.numword}\")\n    return voc, valid_pair","16b61e74":"voc, valid_pair = loadPrepareData(samp_pairs, 25) # max length 25","df104fd6":"for pair in valid_pair[:3]:\n    print(pair)","8e687717":"def trim_rareword(voc, pairs, min_count):\n    voc.trim(min_count)\n    # trim the voc class with min_count so that every word in \n    # word2index will satisfy min_count freq req\n    trimmed_pair = []\n    for pair in pairs:\n        inp_sent = pair[0]\n        resp_sent = pair[1]\n        keep_inp = True\n        keep_resp = True\n        # loop over every word in both inp and resp sentence\n        for word in inp_sent.split(' '):\n            if word not in voc.word2index: #condition\n                keep_inp = False\n                break\n\n        for word in resp_sent.split(' '):\n            if word not in voc.word2index: #condition\n                keep_resp = False\n                break\n\n        if keep_inp and keep_resp:\n            trimmed_pair.append(pair)\n\n    print(f\"The trimming process make the total {len(pairs)} ==> {len(trimmed_pair)} trimmed pair\")\n    return voc, trimmed_pair","2e1ae121":"voc, trimmed_pair = trim_rareword(voc, valid_pair, min_count = 3)","98d7e235":"def index_from_sentence(voc, sentence):\n    \"\"\"\n    Input: a single sentence\n    output: return index resp matching with words in sentence\n    based on voc.word2index\n    \"\"\"\n    return [voc.word2index[word] for word in sentence.split(' ')] + [eos_token]\n    # eos token to indicate \"sentence ends here\"","ab243f34":"# Testing the tokenizer\nindex_from_sentence(voc, trimmed_pair[5][0])","a2e892bb":"from tensorflow.keras.preprocessing.sequence import pad_sequences\n# pads a list of sentences ,l\ndef zeroPadding(l, fillvalue=pad_token):\n    return pad_sequences(l, value=fillvalue, padding='post')\n\n# a binary mask for where there is padding and where there is not\n# l is a list of sentences\ndef binaryMatrix(l, value=pad_token):\n    m = []\n    for i, seq in enumerate(l):\n        m.append([])\n        for token in seq:\n            if token == pad_token:\n                m[i].append(0)\n            else:\n                m[i].append(1)\n    return m\n","066a728a":"def prepared_input(l, voc):\n    # tokenize\n    indexes_batch = [index_from_sentence(voc, sentence) for sentence in l]\n    # pad\n    padded_list_index = zeroPadding(indexes_batch)\n    # find lengths\n    lengths = [len(indexes) for indexes in indexes_batch]\n    return padded_list_index, lengths\n\ndef prepared_output(l, voc):\n    # tokenize\n    indexes_batch = [index_from_sentence(voc, sent) for sent in l]\n    # pad\n    padded_list_index = zeroPadding(indexes_batch)\n    # max output length\n    max_output_length = max([len(indexes) for indexes in indexes_batch])\n    # make the mask\n    mask = binaryMatrix(padded_list_index)\n\n    return padded_list_index, mask, max_output_length","2ee96f14":"# combine all and return all items needed given a batch of pairs\ndef get_batch_pair(voc, batch_pair):\n\n\n    # divide the batch pair to batch input and batch response\n    input_batch, resp_batch = [], []\n    for pair in batch_pair:\n        input_batch.append(pair[0])\n        resp_batch.append(pair[1])\n\n    inp_seq, len_input = prepared_input(input_batch, voc)\n    op_seq, mask, max_len = prepared_output(resp_batch, voc)\n\n    return inp_seq, len_input, op_seq, mask, max_len","bb86a531":"inp_seq, len_inp, op_seq, mask, max_len = get_batch_pair(voc, trimmed_pair)","5ff246ee":"print(f\"Length of input_sequence = {len(inp_seq)}\\n \\\nLength of output_sequence = {len(op_seq)}\\n \\\nWith Maximum Length = {max_len}\")","75107696":"vocab_dict = voc.word2index\nprint(f\"Length of vocab dict is {len(vocab_dict)}\")","50e0465a":"**This method of Vocab Dictionary is really popular and you can find multiple sources for this class on Kaggle. If you think I have taken this from you, Do let me know and I will add the source.**","ae6ce2e9":"# MAKING CONVERSATION PAIRS","e42eed0c":"**We will also generate a binary mask to indicate the place of padding in the sequence**","4235985d":"# TOKENIZING THE SEQUENCES","d06631b5":"**Let's check this with a sample**","8b0297a2":"# MAKING A VOCAB DICTIONARY","0d3d85fa":"**Keep in mind that after deleting the rare words from vocab we also need to delete the sentences containing the rare words.** \\\nYou can also consider keeping an \"unknown token\" in the word","8331ae78":"**Applying the functions to inputs and outputs**","15dd069c":"**You can see that 64 pairs are deleted from the data because their length is greater than 25**","e03c7773":"# DESCRIPTION\n**In this notebook, I aim to accomplish the following**\n1. Load The dataset\n2. Make the conversation pairs.\n3. Create a Vocabulary Dictionary for all the words in the corpus\n4. Trim the rare words in the corpus\n5. Remove the accents and numbers from the conversations\n6. Trim the sentences to a MAX_LENGTH\n7. Tokenize and Pad the sequences\n\nYou can use the tokenized and padded sequences for Many NLP Tasks such as Chatbot creation and Language Modelling.\n","c1a57f8d":"# END","62c40f4c":"## Appying the processing","cd030985":"**Function applying above functions**","182f9b3b":"**The pairs are made like 0-1, 1-2, 2-3,....etc**","24694953":"# LOADING THE DATA","f2f3742e":"**We ensure sentences must have length smaller than max length \\\nMax_length value is based on our choice, the greater value, the more training data we have.**","24758429":"# TRIMMING THE RARE WORDS","79988992":"**You can get the vocab dictionary too**","00feeac6":"**Look's like it is a story\nYou might have to trim it before feeding it to any model**","1e3b5b93":"**HOPE YOU LIKED IT.\nDO LET ME KNOW IF I MADE A MISTAKE OR IF YOU HAVE ANY SUGGESTIONS**\n### HAPPY LEARNING!","7d629b13":"# PADDING SEQUENCES","f2aeaa57":"**Maximum sentence Length is 3047!! \\\nLet's check it out**","29c1570b":"## Distribution of Lengths","3f6cc48e":"**Lets test the functions using 'samp_pairs'**","b2318d1f":"Also let's check out the pairs","26e7157f":"**Since we are just taking a sample, the words don't have high frequency, hence a lot of them don't make it**","577e8b65":"# REMOVING ACCENTS "}}