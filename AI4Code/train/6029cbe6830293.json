{"cell_type":{"063675dc":"code","0edb952e":"code","e428bcbb":"code","eeafbef5":"code","224ecc89":"code","bf042f7d":"code","292aef48":"code","56f49e6f":"code","4a3a8373":"code","360f2cf0":"code","f0c7ea96":"code","149a77c7":"code","2ae549b9":"code","63117d81":"code","f38f3750":"code","85b3f127":"code","61b8d15f":"code","75b289ac":"code","70164938":"code","e177ccb7":"code","55badaef":"code","b8309498":"code","9721f33a":"code","6c0f041d":"code","f46cc71e":"code","98ea7b3f":"code","b3bb7e33":"code","dbe8752d":"code","6f093647":"code","4916b531":"code","11a21fc4":"code","db7bf6ff":"code","e02a36c9":"code","381a0d71":"code","efbe2e24":"code","655cfe03":"code","819dc0ba":"code","86184708":"code","9c53c5c6":"code","5c229d86":"code","71b63103":"code","f6ef417e":"code","9387e25f":"code","7b66a3e2":"code","0c7c227c":"code","dcf4c4d4":"code","e4724055":"code","4ea0598a":"code","30cba0ae":"code","7e127142":"code","30a8f329":"code","95355fc9":"code","6883bb1d":"code","e8419ba3":"code","88b87819":"markdown","ac37ec5c":"markdown","c32de355":"markdown","e0a5f907":"markdown","2421ad2c":"markdown","f7d954ee":"markdown","5f3f9483":"markdown","eb079673":"markdown","51beab9e":"markdown","1b222e48":"markdown","cf9ed795":"markdown","9737fb94":"markdown","b9c3f171":"markdown","77cf5a12":"markdown","20439143":"markdown","468a1bfd":"markdown","fb3024de":"markdown","e606797b":"markdown","57ee36a5":"markdown","a9accf5e":"markdown","2c4f369a":"markdown","c11f5686":"markdown","536632ab":"markdown","34879b9b":"markdown","411d183d":"markdown","46955037":"markdown","e1464e01":"markdown","d3b84fba":"markdown","3fe92be1":"markdown","a92b6749":"markdown","1a65633c":"markdown","646b2de1":"markdown","740a37fd":"markdown","959bf9f3":"markdown","41b9e93e":"markdown","f7a12ad5":"markdown","71b25955":"markdown","d099ef99":"markdown","70af6422":"markdown","bb94b763":"markdown","322d134e":"markdown","02c9d5e6":"markdown","2e32287d":"markdown","3180b76b":"markdown","1e2339a8":"markdown","a7fc2c2b":"markdown","4619f243":"markdown","07609430":"markdown","6cf6ddc9":"markdown","448afdf5":"markdown","5668c9cb":"markdown","245699ac":"markdown","aee3a93f":"markdown","0917a7c8":"markdown","8f2fec96":"markdown","1e7a1161":"markdown","84b76ab8":"markdown","bb652297":"markdown","632e1552":"markdown","52d89dc7":"markdown","f66464fd":"markdown","21fb3dc9":"markdown","a86de2fb":"markdown","002d63cd":"markdown","9d775400":"markdown","60c8a70e":"markdown","92811476":"markdown","0eabbb39":"markdown","f7d57264":"markdown","07639887":"markdown","a8ea3e84":"markdown","d117d829":"markdown","5ccf0c03":"markdown","8fee81e3":"markdown","3643b3d9":"markdown","06dd2091":"markdown","a3391bc5":"markdown","725088be":"markdown","55907417":"markdown"},"source":{"063675dc":"import numpy as np\nfrom sklearn.datasets import fetch_20newsgroups\nfrom sklearn import decomposition\nfrom scipy import linalg\nimport matplotlib.pyplot as plt","0edb952e":"%matplotlib inline\nnp.set_printoptions(suppress=True)","e428bcbb":"categories = ['alt.atheism', 'talk.religion.misc', 'comp.graphics', 'sci.space']\nremove = ('headers', 'footers', 'quotes')\nnewsgroups_train = fetch_20newsgroups(subset='train', categories=categories, remove=remove)\nnewsgroups_test = fetch_20newsgroups(subset='test', categories=categories, remove=remove)","eeafbef5":"newsgroups_train.filenames.shape, newsgroups_train.target.shape","224ecc89":"print(\"\\n\".join(newsgroups_train.data[:3]))","bf042f7d":"np.array(newsgroups_train.target_names)[newsgroups_train.target[:3]]","292aef48":"newsgroups_train.target[:10]","56f49e6f":"num_topics, num_top_words = 6, 8","4a3a8373":"from sklearn.feature_extraction import stop_words\n\nsorted(list(stop_words.ENGLISH_STOP_WORDS))[:20]","360f2cf0":"import nltk\nnltk.download('wordnet')","f0c7ea96":"from nltk import stem","149a77c7":"wnl = stem.WordNetLemmatizer()\nporter = stem.porter.PorterStemmer()","2ae549b9":"word_list = ['feet', 'foot', 'foots', 'footing']","63117d81":"[wnl.lemmatize(word) for word in word_list]","f38f3750":"[porter.stem(word) for word in word_list]","85b3f127":"import spacy","61b8d15f":"from spacy.lemmatizer import Lemmatizer\nlemmatizer = Lemmatizer()","75b289ac":"[lemmatizer.lookup(word) for word in word_list]","70164938":"nlp = spacy.load(\"en_core_web_sm\")","e177ccb7":"sorted(list(nlp.Defaults.stop_words))[:20]","55badaef":"#Exercise:\n","b8309498":"#Exercise:\n","9721f33a":"from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer","6c0f041d":"import nltk\n# nltk.download('punkt')","f46cc71e":"# from nltk import word_tokenize\n\n# class LemmaTokenizer(object):\n#     def __init__(self):\n#         self.wnl = stem.WordNetLemmatizer()\n#     def __call__(self, doc):\n#         return [self.wnl.lemmatize(t) for t in word_tokenize(doc)]","98ea7b3f":"vectorizer = CountVectorizer(stop_words='english') #, tokenizer=LemmaTokenizer())","b3bb7e33":"vectors = vectorizer.fit_transform(newsgroups_train.data).todense() # (documents, vocab)\nvectors.shape #, vectors.nnz \/ vectors.shape[0], row_means.shape","dbe8752d":"print(len(newsgroups_train.data), vectors.shape)","6f093647":"vocab = np.array(vectorizer.get_feature_names())","4916b531":"vocab.shape","11a21fc4":"vocab[7000:7020]","db7bf6ff":"%time U, s, Vh = linalg.svd(vectors, full_matrices=False)","e02a36c9":"print(U.shape, s.shape, Vh.shape)","381a0d71":"s[:4]","efbe2e24":"np.diag(np.diag(s[:4]))","655cfe03":"#Exercise: confrim that U, s, Vh is a decomposition of `vectors`\n","819dc0ba":"#Exercise: Confirm that U, Vh are orthonormal\n","86184708":"plt.plot(s);","9c53c5c6":"plt.plot(s[:10])","5c229d86":"num_top_words=8\n\ndef show_topics(a):\n    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n    topic_words = ([top_words(t) for t in a])\n    return [' '.join(t) for t in topic_words]","71b63103":"show_topics(Vh[:10])","f6ef417e":"m,n=vectors.shape\nd=5  # num topics","9387e25f":"clf = decomposition.NMF(n_components=d, random_state=1)\n\nW1 = clf.fit_transform(vectors)\nH1 = clf.components_","7b66a3e2":"show_topics(H1)","0c7c227c":"vectorizer_tfidf = TfidfVectorizer(stop_words='english')\nvectors_tfidf = vectorizer_tfidf.fit_transform(newsgroups_train.data) # (documents, vocab)","dcf4c4d4":"newsgroups_train.data[10:20]","e4724055":"W1 = clf.fit_transform(vectors_tfidf)\nH1 = clf.components_","4ea0598a":"show_topics(H1)","30cba0ae":"plt.plot(clf.components_[0])","7e127142":"clf.reconstruction_err_","30a8f329":"%time u, s, v = np.linalg.svd(vectors, full_matrices=False)","95355fc9":"from sklearn import decomposition\nimport fbpca","6883bb1d":"%time u, s, v = decomposition.randomized_svd(vectors, 10)","e8419ba3":"%time u, s, v = fbpca.pca(vectors, 10)","88b87819":"## Singular Value Decomposition (SVD)","ac37ec5c":"[Topic Frequency-Inverse Document Frequency](http:\/\/www.tfidf.com\/) (TF-IDF) is a way to normalize term counts by taking into account how often they appear in a document, how long the document is, and how commmon\/rare the term is.\n\nTF = (# occurrences of term t in document) \/ (# of words in documents)\n\nIDF = log(# of documents \/ # documents with term t in it)","c32de355":"#### NLTK","e0a5f907":"### When to use these?","2421ad2c":"### Another approach: sub-word units","f7d954ee":"Let's look at some of the data.  Can you guess which category these messages are in?","5f3f9483":"## Non-negative Matrix Factorization (NMF)","eb079673":"## Truncated SVD","51beab9e":"<img src=\"images\/skomoroch.png\" alt=\"\" style=\"width: 65%\"\/>","1b222e48":"Latent Semantic Analysis (LSA) uses Singular Value Decomposition (SVD).","cf9ed795":"- Matrices are \"stupendously big\"\n- Data are often **missing or inaccurate**.  Why spend extra computational resources when imprecision of input limits precision of the output?\n- **Data transfer** now plays a major role in time of algorithms.  Techniques the require fewer passes over the data may be substantially faster, even if they require more flops (flops = floating point operations).\n- Important to take advantage of **GPUs**.\n\n(source: [Halko](https:\/\/arxiv.org\/abs\/0909.4061))","9737fb94":"#### Exercise: And what stop words are in sklearn but not spacy?","b9c3f171":"![](https:\/\/raw.githubusercontent.com\/fastai\/course-nlp\/85e505295efeed88ce61dc0ff5e424bde9741a15\/images\/face_pca.png)\n\n(source: [NMF Tutorial](http:\/\/perso.telecom-paristech.fr\/~essid\/teach\/NMF_tutorial_ICME-2014.pdf))\n\nA more interpretable approach:\n\n![](https:\/\/raw.githubusercontent.com\/fastai\/course-nlp\/85e505295efeed88ce61dc0ff5e424bde9741a15\/images\/face_outputs.png)\n\n(source: [NMF Tutorial](http:\/\/perso.telecom-paristech.fr\/~essid\/teach\/NMF_tutorial_ICME-2014.pdf))","77cf5a12":"![](https:\/\/raw.githubusercontent.com\/fastai\/course-nlp\/85e505295efeed88ce61dc0ff5e424bde9741a15\/images\/svd_fb.png)\n\n(source: [Facebook Research: Fast Randomized SVD](https:\/\/research.fb.com\/fast-randomized-svd\/))","20439143":"These were long considered standard techniques, but they can often **hurt** your performance **if using deep learning**. Stemming, lemmatization, and removing stop words all involve throwing away information.\n\nHowever, they can still be useful when working with simpler models.","468a1bfd":"### Timing comparison","fb3024de":"## Getting started","e606797b":"### Stemming and Lemmatization","57ee36a5":"Notes:\n- For NMF, matrix needs to be at least as tall as it is wide, or we get an error with fit_transform\n- Can use df_min in CountVectorizer to only look at words that were in at least k of the split texts","a9accf5e":"#### Motivation","2c4f369a":"### Stop words","c11f5686":"#### Idea","536632ab":"### NMF in summary","34879b9b":"Stemming and Lemmatization both generate the root form of the words. \n\nLemmatization uses the rules about a language.  The resulting tokens are all actual words\n\n\"Stemming is the poor-man\u2019s lemmatization.\" (Noah Smith, 2011) Stemming is a crude heuristic that chops the ends off of words.  The resulting tokens may not be actual words. Stemming is faster.","411d183d":"#### Answer","46955037":"Confirm that U, V are orthonormal","e1464e01":"### Spacy","d3b84fba":"#### Applications of NMF","3fe92be1":"## End","a92b6749":"We will use [scikit-learn's implementation of NMF](http:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.NMF.html):","1a65633c":"Topic modeling is a fun way to start our study of NLP. We will use two popular **matrix decomposition techniques**. \n\nWe start with a **term-document matrix**:\n![term-doc](https:\/\/github.com\/fastai\/course-nlp\/raw\/85e505295efeed88ce61dc0ff5e424bde9741a15\/images\/document_term.png)\n\nsource: [Introduction to Information Retrieval](http:\/\/player.slideplayer.com\/15\/4528582\/#)\n\nWe can decompose this into one tall thin matrix times one wide short matrix (possibly with a diagonal matrix in between).\n\nNotice that this representation does not take into account word order or sentence structure.  It's an example of a **bag of words** approach.","646b2de1":"### NMF from sklearn","740a37fd":"#### Exercise: What stop words appear in spacy but not in sklearn?","959bf9f3":"Next, scikit learn has a method that will extract all the word counts for us.  In the next lesson, we'll learn how to write our own version of CountVectorizer, to see what's happening underneath the hood.","41b9e93e":"The target attribute is the integer index of the category.","f7a12ad5":"- inherently stable\n- performance guarantees do not depend on subtle spectral properties\n- needed matrix-vector products can be done in parallel\n\n(source: [Halko](https:\/\/arxiv.org\/abs\/0909.4061))","71b25955":"\"SVD is not nearly as famous as it should be.\" - Gilbert Strang","d099ef99":"We saved a lot of time when we calculated NMF by only calculating the subset of columns we were interested in. Is there a way to get this benefit with SVD? Yes there is! It's called truncated SVD.  We are just interested in the vectors corresponding to the **largest** singular values.","70af6422":"Consider the most extreme case - reconstructing the matrix using an outer product of two vectors. Clearly, in most cases we won't be able to reconstruct the matrix exactly. But if we had one vector with the relative frequency of each vocabulary word out of the total word count, and one with the average number of words per document, then that outer product would be as close as we can get.\n\nNow consider increasing that matrices to two columns and two rows. The optimal decomposition would now be to cluster the documents into two groups, each of which has as different a distribution of words as possible to each other, but as similar as possible amongst the documents in the cluster. We will call those two groups \"topics\". And we would cluster the words into two groups, based on those which most frequently appear in each of the topics. ","bb94b763":"Your turn!  Now, try lemmatizing and stemming the following collections of words:\n\n- fly, flies, flying\n- organize, organizes, organizing\n- universe, university","322d134e":"- [Data source](http:\/\/scikit-learn.org\/stable\/datasets\/twenty_newsgroups.html): Newsgroups are discussion groups on Usenet, which was popular in the 80s and 90s before the web really took off.  This dataset includes 18,000 newsgroups posts with 20 topics.\n- [Chris Manning's book chapter](https:\/\/nlp.stanford.edu\/IR-book\/pdf\/18lsi.pdf) on matrix factorization and LSI \n- Scikit learn [truncated SVD LSI details](http:\/\/scikit-learn.org\/stable\/modules\/decomposition.html#lsa)\n\n### Other Tutorials\n- [Scikit-Learn: Out-of-core classification of text documents](http:\/\/scikit-learn.org\/stable\/auto_examples\/applications\/plot_out_of_core_classification.html): uses [Reuters-21578](https:\/\/archive.ics.uci.edu\/ml\/datasets\/reuters-21578+text+categorization+collection) dataset (Reuters articles labeled with ~100 categories), HashingVectorizer\n- [Text Analysis with Topic Models for the Humanities and Social Sciences](https:\/\/de.dariah.eu\/tatom\/index.html): uses [British and French Literature dataset](https:\/\/de.dariah.eu\/tatom\/datasets.html) of Jane Austen, Charlotte Bronte, Victor Hugo, and more","02c9d5e6":"### Additional Resources","2e32287d":"Spacy doesn't offer a stemmer (since lemmatization is considered better-- this is an example of being opinionated!)","3180b76b":"Stemming and lemmatization are implementation dependent.","1e2339a8":"We'll take a dataset of documents in several different categories, and find topics (consisting of groups of words) for them.  Knowing the actual categories helps us evaluate if the topics we find make sense.\n\nWe will try this with two different matrix factorizations: **Singular Value Decomposition (SVD)** and **Non-negative Matrix Factorization (NMF)**","a7fc2c2b":"#### Acknowledgements\n* Original Code: https:\/\/github.com\/fastai\/course-nlp, by [Rachel Thomas](https:\/\/www.kaggle.com\/mathrachel), and [Jeremy Howard](https:\/\/www.kaggle.com\/jhoward).\n* Original Course: https:\/\/www.fast.ai\/2019\/07\/08\/fastai-nlp\/, by [Rachel Thomas](https:\/\/www.kaggle.com\/mathrachel), and [Jeremy Howard](https:\/\/www.kaggle.com\/jhoward).\n* Brazilian Group of Study: https:\/\/contas.tcu.gov.br\/ords\/f?p=portal:detalhe:::::V:161124, organized by [Erick Muzart](https:\/\/www.kaggle.com\/erickmuzart), and [Fernando Melo](https:\/\/www.kaggle.com\/nandobr).\n* Course adapted to Kaggle at https:\/\/www.kaggle.com\/c\/nlpbsb by [Debora Reis](https:\/\/www.kaggle.com\/deborareis)\n\n---","4619f243":"We would clearly expect that the words that appear most frequently in one topic would appear less frequently in the other - otherwise that word wouldn't make a good choice to separate out the two topics. Therefore, we expect the topics to be **orthogonal**.\n\nThe SVD algorithm factorizes a matrix into one matrix with **orthogonal columns** and one with **orthogonal rows** (along with a diagonal matrix, which contains the **relative importance** of each factor).\n\n![SVD](https:\/\/raw.githubusercontent.com\/fastai\/course-nlp\/85e505295efeed88ce61dc0ff5e424bde9741a15\/images\/svd_fb.png)\n\n(source: [Facebook Research: Fast Randomized SVD](https:\/\/research.fb.com\/fast-randomized-svd\/))\n\nSVD is an **exact decomposition**, since the matrices it creates are big enough to fully cover the original matrix. SVD is extremely widely used in linear algebra, and specifically in data science, including:\n\n- semantic analysis\n- collaborative filtering\/recommendations ([winning entry for Netflix Prize](https:\/\/datajobs.com\/data-science-repo\/Recommender-Systems-%5BNetflix%5D.pdf))\n- calculate Moore-Penrose pseudoinverse\n- data compression\n- principal component analysis","07609430":"Benefits: Fast and easy to use!\n\nDownsides: took years of research and expertise to create","6cf6ddc9":"Confirm this is a decomposition of the input.","448afdf5":"## Data Processing","5668c9cb":"#### Advantages of randomized algorithms:","245699ac":"We get topics that match the kinds of clusters we would expect! This is despite the fact that this is an **unsupervised algorithm** - which is to say, we never actually told the algorithm how our documents are grouped.","aee3a93f":"### TF-IDF","0917a7c8":"Stemming and lemmatization are language dependent.  Languages with more complex morphologies may show bigger benefits.  For example, Sanskrit has a very [large number of verb forms](https:\/\/en.wikipedia.org\/wiki\/Sanskrit_verbs). ","8f2fec96":"### Motivation","1e7a1161":"from [Information Retrieval](https:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/stemming-and-lemmatization-1.html) textbook:\n\nAre the below words the same?\n\n*organize, organizes, and organizing*\n\n*democracy, democratic, and democratization*","84b76ab8":"What can we say about the singular values s?","bb652297":"#### Topics","632e1552":"Latent Semantic Analysis (LSA) uses SVD.  You will sometimes hear topic modelling referred to as LSA.","52d89dc7":"[SentencePiece](https:\/\/github.com\/google\/sentencepiece) library from Google","f66464fd":"Scikit Learn comes with a number of built-in datasets, as well as loading utilities to load several standard external datasets. This is a [great resource](http:\/\/scikit-learn.org\/stable\/datasets\/), and the datasets include Boston housing prices, face images, patches of forest, diabetes, breast cancer, and more.  We will be using the newsgroups dataset.\n\nNewsgroups are discussion groups on Usenet, which was popular in the 80s and 90s before the web really took off.  This dataset includes 18,000 newsgroups posts with 20 topics.  ","21fb3dc9":"For more on randomized SVD, check out my [PyBay 2017 talk](https:\/\/www.youtube.com\/watch?v=7i6kBz1kZ-A&list=PLtmWHNX-gukLQlMvtRJ19s7-8MrnRV6h6&index=7).\n\nFor significantly more on randomized SVD, check out the [Computational Linear Algebra course](https:\/\/github.com\/fastai\/numerical-linear-algebra).","a86de2fb":"**More Reading**:\n\n- [The Why and How of Nonnegative Matrix Factorization](https:\/\/arxiv.org\/pdf\/1401.5226.pdf)","002d63cd":"From [Intro to Information Retrieval](https:\/\/nlp.stanford.edu\/IR-book\/html\/htmledition\/dropping-common-terms-stop-words-1.html):\n\n*Some extremely common words which would appear to be of little value in helping select documents matching a user need are excluded from the vocabulary entirely. These words are called stop words.*\n\n*The general trend in IR systems over time has been from standard use of quite large stop lists (200-300 terms) to very small stop lists (7-12 terms) to no stop list whatsoever. Web search engines generally do not use stop lists.*","9d775400":"#### Shortcomings of classical algorithms for decomposition:","60c8a70e":"Randomized SVD from Facebook's library fbpca:","92811476":"Stop words vary from library to library","0eabbb39":"hint: definition of *perijove* is the point in the orbit of a satellite of Jupiter nearest the planet's center ","f7d57264":"We will return to SVD in **much more detail** later.  For now, the important takeaway is that we have a tool that allows us to exactly factor a matrix into orthogonal columns and orthogonal rows.","07639887":"fastai\/course-nlp","a8ea3e84":"# Topic Modeling with NMF and SVD","d117d829":"Spacy is a very modern & fast nlp library. Spacy is opinionated, in that it typically offers one highly optimized way to do something (whereas nltk offers a huge variety of ways, although they are usually not as optimized).\n\nYou will need to install it.\n\nif you use conda:\n```\nconda install -c conda-forge spacy\n```\nif you use pip:\n```\npip install -U spacy\n```\n\nYou will then need to download the English model:\n```\nspacy -m download en_core_web_sm\n```","5ccf0c03":"## Stop words, stemming, lemmatization","8fee81e3":"#### Answer","3643b3d9":"- [Face Decompositions](http:\/\/scikit-learn.org\/stable\/auto_examples\/decomposition\/plot_faces_decomposition.html#sphx-glr-auto-examples-decomposition-plot-faces-decomposition-py)\n- [Collaborative Filtering, eg movie recommendations](http:\/\/www.quuxlabs.com\/blog\/2010\/09\/matrix-factorization-a-simple-tutorial-and-implementation-in-python\/)\n- [Audio source separation](https:\/\/pdfs.semanticscholar.org\/cc88\/0b24791349df39c5d9b8c352911a0417df34.pdf)\n- [Chemistry](http:\/\/ieeexplore.ieee.org\/document\/1532909\/)\n- [Bioinformatics](https:\/\/bmcbioinformatics.biomedcentral.com\/articles\/10.1186\/s12859-015-0485-4) and [Gene Expression](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC2623306\/)\n- Topic Modeling (our problem!)\n\n![](https:\/\/raw.githubusercontent.com\/fastai\/course-nlp\/85e505295efeed88ce61dc0ff5e424bde9741a15\/images\/nmf_doc.png)\n\n(source: [NMF Tutorial](http:\/\/perso.telecom-paristech.fr\/~essid\/teach\/NMF_tutorial_ICME-2014.pdf))","06dd2091":"## Look at our data","a3391bc5":"Rather than constraining our factors to be *orthogonal*, another idea would to constrain them to be *non-negative*. NMF is a factorization of a non-negative data set $V$: $$ V = W H$$ into non-negative matrices $W,\\; H$. Often positive factors will be **more easily interpretable** (and this is the reason behind NMF's popularity). \n\n![](https:\/\/raw.githubusercontent.com\/fastai\/course-nlp\/85e505295efeed88ce61dc0ff5e424bde9741a15\/images\/face_nmf.png)\n\n(source: [NMF Tutorial](http:\/\/perso.telecom-paristech.fr\/~essid\/teach\/NMF_tutorial_ICME-2014.pdf))\n\nNonnegative matrix factorization (NMF) is a non-exact factorization that factors into one skinny positive matrix and one short positive matrix.  NMF is NP-hard and non-unique.  There are a number of variations on it, created by adding different constraints. ","725088be":"## The problem","55907417":"There is no single universal list of stop words."}}