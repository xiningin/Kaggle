{"cell_type":{"71e71337":"code","b308f150":"code","43eae7b7":"code","ca7e62e0":"code","b8f5788c":"code","c46ae7ec":"code","5fe9816e":"code","1960b87a":"code","874bd2cf":"code","ee95f81d":"code","2b7e5f6d":"code","7c78d81a":"code","a2eabf73":"code","88075ecb":"code","3429995d":"code","d6ac2b6b":"code","c6e4d94f":"code","1188778d":"code","ef9c3e38":"code","8a82802d":"code","615e6ab3":"code","87f80a29":"code","d316a526":"code","8f38e1a9":"code","6228e91d":"code","92bf81c6":"code","ae838bc9":"code","bf03be07":"code","5d2bf9ed":"code","d95e4444":"code","b4a7e796":"code","1c4edad4":"code","c2917843":"code","d29685df":"code","44ce60ab":"code","7d5d8b5a":"code","0501f235":"code","a5f933e4":"code","0e945e77":"code","86d32108":"code","af347ffb":"code","5f8ac50f":"code","ae2024bc":"code","165e44a6":"code","4a8bbce2":"code","22c15ee3":"code","478ca6c4":"code","f9191017":"code","39b8e523":"code","2baae6a4":"code","53ecd8e3":"code","3cee50d0":"code","c0e06fba":"code","19f05a4d":"code","e16d5748":"code","57d19022":"code","e659fd68":"code","752c3ac3":"code","0a258a2b":"markdown","dd96880c":"markdown","7a2c5d19":"markdown","1f93352b":"markdown","f32b2035":"markdown","f90223df":"markdown","886992be":"markdown","99602b20":"markdown","e692f9cf":"markdown","9f8f492d":"markdown","42c5df61":"markdown","2c855c6c":"markdown","ef5e4718":"markdown","a2f74786":"markdown","861843bb":"markdown","9a896562":"markdown","5e069a4e":"markdown","3a1b7c57":"markdown","e6fdf189":"markdown","449b72be":"markdown","3251ceb6":"markdown","5c5c66b5":"markdown","c986cdc7":"markdown","754f5a58":"markdown","8c18eaa8":"markdown","71d4710d":"markdown"},"source":{"71e71337":"import os\nimport numpy as np \nimport pandas as pd\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom bs4 import BeautifulSoup\nfrom fuzzywuzzy import fuzz\nfrom wordcloud import WordCloud, STOPWORDS\nfrom os import path\nfrom sklearn.manifold import TSNE\nfrom sklearn.preprocessing import MinMaxScaler\nfrom tqdm import tqdm\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics.classification import accuracy_score, log_loss\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.calibration import CalibratedClassifierCV\nimport xgboost as xgb\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport spacy","b308f150":"train_df = pd.read_csv(\"\/kaggle\/input\/quora-question-pairs\/train.csv.zip\")\ntrain_df.head()","43eae7b7":"plt.figure(figsize = (10,7))\nsns.countplot(train_df[\"is_duplicate\"])\nplt.title(\"Barplot of is_duplicate\")","ca7e62e0":"print(\"Total number of qustion pairs : \", train_df.shape[0])\nprint(\"% of question pairs which are similar : \", ((train_df[train_df[\"is_duplicate\"]==1].shape[0]) \/ train_df.shape[0])*100)\nprint(\"% of question pairs which are not similar : \", ((train_df[train_df[\"is_duplicate\"]==0].shape[0]) \/ train_df.shape[0])*100)","b8f5788c":"distinct_qus = len(set(train_df['qid1'].tolist() + train_df['qid2'].tolist()))\nprint(\"Total number of distinct questions : \", distinct_qus)\n\nappended_series = train_df['qid1'].append(train_df['qid2'])\nqus_freq_more_than_one = sum(appended_series.value_counts()>1)\nprint(\"Repeated questions(Number of questions having frequency more than one time) : \", qus_freq_more_than_one)\nprint(\"Highest repeat frequency : \", max(appended_series.value_counts()))","c46ae7ec":"plt.figure(figsize = (10,7))\nsns.barplot([\"Distinct\", \"Repeated\"], [distinct_qus, qus_freq_more_than_one])\nplt.title(\"Barplot indicating distinct and repeated questions\")","5fe9816e":"print(\"Number of duplicate question pairs : \", train_df[['qid1','qid2']].duplicated().sum())","1960b87a":"plt.figure(figsize = (20,12))\nsns.distplot(appended_series.value_counts(),bins = 200, kde = False, color = \"blue\")\nplt.yscale('log', nonposy='clip')","874bd2cf":"train_df.isna().sum()","ee95f81d":"train_df = train_df.fillna('')","2b7e5f6d":"train_df['freq_qid1'] = train_df.groupby('qid1')['qid1'].transform('count') \ntrain_df['freq_qid2'] = train_df.groupby('qid2')['qid2'].transform('count') ","7c78d81a":"train_df['q1len'] =train_df['question1'].str.len() \ntrain_df['q2len'] = train_df['question2'].str.len()","a2eabf73":"train_df['q1_n_words'] = train_df.apply(lambda row: len(row.question1.split(\" \")),axis=1)\ntrain_df['q2_n_words'] = train_df.apply(lambda row: len(row.question2.split(\" \")),axis=1)","88075ecb":"def stripped_common_words(row):\n        set1 = set(map(lambda i: i.lower().strip(), row.question1.split(\" \")))\n        set2 = set(map(lambda i: i.lower().strip(), row.question2.split(\" \")))    \n        return 1.0 * len(set1 & set2)\ntrain_df['word_Common'] = train_df.apply(stripped_common_words, axis=1)","3429995d":"def stripped_word_total(row):\n        set1 = set(map(lambda i: i.lower().strip(), row.question1.split(\" \")))\n        set2 = set(map(lambda i: i.lower().strip(), row.question2.split(\" \")))    \n        return 1.0 * (len(set1) + len(set2))\ntrain_df['word_Total'] = train_df.apply(stripped_word_total, axis=1)","d6ac2b6b":"def stripped_word_share(row):\n        set1 = set(map(lambda i: i.lower().strip(), row.question1.split(\" \")))\n        set2 = set(map(lambda i: i.lower().strip(), row.question2.split(\" \")))    \n        return 1.0 * len(set1 & set2)\/(len(set1) + len(set2))\ntrain_df['word_share'] = train_df.apply(stripped_word_share, axis=1)","c6e4d94f":"train_df['freq_q1+q2'] = train_df['freq_qid1']+train_df['freq_qid2']\ntrain_df['freq_q1-q2'] = abs(train_df['freq_qid1']-train_df['freq_qid2'])","1188778d":"train_df.head()","ef9c3e38":"print (\"Minimum number of words in question1 : \" , train_df['q1_n_words'].min())\nprint (\"Minimum number of words in question2 : \" , train_df['q2_n_words'].min())\nprint (\"Number of Questions with minimum words [question1] :\", train_df[train_df['q1_n_words']== 1].shape[0])\nprint (\"Number of Questions with minimum words [question2] :\", train_df[train_df['q2_n_words']== 1].shape[0])","8a82802d":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14, 8))\nax1.set_title(\"Violin plot of word_share across both the duplicacy level\")\nsns.violinplot(x = 'is_duplicate', y = 'word_share', data = train_df,ax=ax1)\nax2.set_title(\"Distribution of word_share across both the duplicacy level\")\nsns.distplot(train_df[train_df['is_duplicate'] == 1.0]['word_share'] , label = \"1\", ax=ax2)\nsns.distplot(train_df[train_df['is_duplicate'] == 0.0]['word_share'] , label = \"0\" , ax=ax2)\nplt.show()","615e6ab3":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14, 8))\nax1.set_title(\"Violin plot of word_Common across both the duplicacy level\")\nsns.violinplot(x = 'is_duplicate', y = 'word_Common', data = train_df,ax=ax1)\nax2.set_title(\"Distribution of word_Common across both the duplicacy level\")\nsns.distplot(train_df[train_df['is_duplicate'] == 1.0]['word_Common'] , label = \"1\", color = 'red',ax=ax2)\nsns.distplot(train_df[train_df['is_duplicate'] == 0.0]['word_Common'] , label = \"0\" , color = 'blue' ,ax=ax2)\n","87f80a29":"!pip install distance","d316a526":"import distance","8f38e1a9":"stop_words = stopwords.words(\"english\")\ndef text_preprocess(txt):\n    txt = str(txt).lower()\n    txt = txt.replace(\",000,000\", \"m\").replace(\",000\", \"k\").replace(\"\u2032\", \"'\").replace(\"\u2019\", \"'\")\\\n          .replace(\"won't\", \"will not\").replace(\"cannot\", \"can not\").replace(\"can't\", \"can not\")\\\n          .replace(\"n't\", \" not\").replace(\"what's\", \"what is\").replace(\"it's\", \"it is\")\\\n          .replace(\"'ve\", \" have\").replace(\"i'm\", \"i am\").replace(\"'re\", \" are\")\\\n          .replace(\"he's\", \"he is\").replace(\"she's\", \"she is\").replace(\"'s\", \" own\")\\\n        .replace(\"%\", \" percent \").replace(\"\u20b9\", \" rupee \").replace(\"$\", \" dollar \")\\\n        .replace(\"\u20ac\", \" euro \").replace(\"'ll\", \" will\")\n    txt = re.sub(r\"([0-9]+)000000\", r\"\\1m\", txt)\n    txt = re.sub(r\"([0-9]+)000\", r\"\\1k\", txt)\n    porter = PorterStemmer()\n    pattern = re.compile('\\W')\n    if type(txt) == type(''):\n        txt = re.sub(pattern, ' ', txt)\n    if type(txt) == type(''):\n        txt = porter.stem(txt)\n        example1 = BeautifulSoup(txt)\n        txt = example1.get_text()\n               \n    return txt","6228e91d":"safe_div = 0.0001\ndef fetch_token_features(q1, q2):\n    token_features = [0.0]*10\n    q1_tokens = q1.split()\n    q2_tokens = q2.split()\n    if len(q1_tokens) == 0 or len(q2_tokens) == 0:\n        return token_features\n    q1_words = set([word for word in q1_tokens if word not in stop_words])\n    q2_words = set([word for word in q2_tokens if word not in stop_words])\n    q1_stops = set([word for word in q1_tokens if word in stop_words])\n    q2_stops = set([word for word in q2_tokens if word in stop_words])\n    common_word_count = len(q1_words & q2_words)\n    common_stop_count = len(q1_stops & q2_stops)\n    common_token_count = len(set(q1_tokens) & set(q2_tokens))\n    token_features[0] = common_word_count \/ (min(len(q1_words), len(q2_words)) + safe_div)\n    token_features[1] = common_word_count \/ (max(len(q1_words), len(q2_words)) + safe_div)\n    token_features[2] = common_stop_count \/ (min(len(q1_stops), len(q2_stops)) + safe_div)\n    token_features[3] = common_stop_count \/ (max(len(q1_stops), len(q2_stops)) + safe_div)\n    token_features[4] = common_token_count \/ (min(len(q1_tokens), len(q2_tokens)) + safe_div)\n    token_features[5] = common_token_count \/ (max(len(q1_tokens), len(q2_tokens)) + safe_div)\n    token_features[6]= int(q1_tokens[-1] == q2_tokens[-1])\n    token_features[7] = int(q1_tokens[0] == q2_tokens[0])\n    token_features[8] = abs(len(q1_tokens) - len(q2_tokens))\n    token_features[9] = (len(q1_tokens) + len(q2_tokens))\/2\n    return token_features","92bf81c6":"# fetch the Longest Common sub string\ndef fetch_longest_substr_ratio(a, b):\n    strs = list(distance.lcsubstrings(a, b))\n    if len(strs) == 0:\n        return 0\n    else:\n        return len(strs[0]) \/ (min(len(a), len(b)) + 1)","ae838bc9":"def extract_features(df):\n    # preprocessing each question\n    df[\"question1\"] = df[\"question1\"].apply(text_preprocess)\n    df[\"question2\"] = df[\"question2\"].apply(text_preprocess)\n    token_features = df.apply(lambda row: fetch_token_features(row.question1, row.question2), axis=1)\n    df[\"cwc_min\"] = list(map(lambda i: i[0], token_features))\n    df[\"cwc_max\"] = list(map(lambda i: i[1], token_features))\n    df[\"csc_min\"] = list(map(lambda i: i[2], token_features))\n    df[\"csc_max\"] = list(map(lambda i: i[3], token_features))\n    df[\"ctc_min\"] = list(map(lambda i: i[4], token_features))\n    df[\"ctc_max\"] = list(map(lambda i: i[5], token_features))\n    df[\"last_word_eq\"]  = list(map(lambda i: i[6], token_features))\n    df[\"first_word_eq\"] = list(map(lambda i: i[7], token_features))\n    df[\"abs_len_diff\"]  = list(map(lambda i: i[8], token_features))\n    df[\"mean_len\"] = list(map(lambda i: i[9], token_features))\n    df[\"token_set_ratio\"] = df.apply(lambda row: fuzz.token_set_ratio(row.question1, row.question2), axis=1)\n    df[\"token_sort_ratio\"] = df.apply(lambda row: fuzz.token_sort_ratio(row.question1, row.question2), axis=1)\n    df[\"fuzz_ratio\"] = df.apply(lambda row: fuzz.QRatio(row.question1, row.question2), axis=1)\n    df[\"fuzz_partial_ratio\"] = df.apply(lambda row: fuzz.partial_ratio(row.question1, row.question2), axis=1)\n    df[\"longest_substr_ratio\"]  = df.apply(lambda row: fetch_longest_substr_ratio(row.question1, row.question2), axis=1)\n    return df","bf03be07":"train_df = extract_features(train_df)","5d2bf9ed":"train_df.head(2)","d95e4444":"train_df_duplicate = train_df[train_df['is_duplicate'] == 1]\ntrain_df_nonduplicate = train_df[train_df['is_duplicate'] == 0]\n\nduplicate_flatten = np.dstack([train_df_duplicate[\"question1\"], train_df_duplicate[\"question2\"]]).flatten()\nnonduplicate_flatten = np.dstack([train_df_nonduplicate[\"question1\"], train_df_nonduplicate[\"question2\"]]).flatten()\nprint (\"Number of questions in duplicate pairs set(class 1) : \",duplicate_flatten.shape[0])\nprint (\"Number of questions in non-duplicate pairs set(class 0) : \",nonduplicate_flatten.shape[0])","b4a7e796":"os.chdir(\"\/kaggle\/working\/\")\nnp.savetxt('train_duplicate.txt', duplicate_flatten, delimiter=' ', fmt='%s')\nnp.savetxt('train_nonduplicate.txt', nonduplicate_flatten, delimiter=' ', fmt='%s')\n#Reading the text files\ntrain_duplicate_w = open(path.join(\"\/kaggle\/working\/\", 'train_duplicate.txt')).read()\ntrain_nonduplicate_w = open(path.join(\"\/kaggle\/working\/\", 'train_nonduplicate.txt')).read()\nprint (\"Total number of words in duplicate pair set :\",len(train_duplicate_w))\nprint (\"Total number of words in non duplicate pair set :\",len(train_nonduplicate_w))","1c4edad4":"stop_words = set(STOPWORDS)\nstop_words.add(\"said\")\nstop_words.add(\"br\")\nstop_words.add(\" \")\nstop_words.remove(\"not\")\nstop_words.remove(\"no\")\nstop_words.remove(\"like\")","c2917843":"wc = WordCloud(background_color=\"white\", max_words=len(train_duplicate_w), stopwords=stop_words)\nwc.generate(train_duplicate_w)\nplt.title(\"Word cloud for duplicate Question pairs\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")","d29685df":"wc = WordCloud(background_color=\"white\", max_words=len(train_nonduplicate_w), stopwords=stop_words)\nwc.generate(train_nonduplicate_w)\nplt.title(\"Word cloud for non-duplicate Question pairs\")\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")","44ce60ab":"sns.pairplot(train_df, hue='is_duplicate', vars=['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio'])","7d5d8b5a":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14, 8))\nax1.set_title(\"Violin plot of token_sort_ration across both the duplicacy level\")\nsns.violinplot(x = 'is_duplicate', y = 'token_sort_ratio', data = train_df , ax=ax1)\nax2.set_title(\"Distribution of token_sort_ration across both the duplicacy level\")\nsns.distplot(train_df[train_df['is_duplicate'] == 1.0]['token_sort_ratio'] , label = \"1\",ax=ax2)\nsns.distplot(train_df[train_df['is_duplicate'] == 0.0]['token_sort_ratio'] , label = \"0\" , ax=ax2)","0501f235":"fig,(ax1,ax2) = plt.subplots(1,2,figsize=(14, 8))\nax1.set_title(\"Violin plot of fuzz_ratio across both the duplicacy level\")\nsns.violinplot(x = 'is_duplicate', y = 'fuzz_ratio', data = train_df , ax=ax1)\nax2.set_title(\"Distribution of fuzz_ratio across both the duplicacy level\")\nsns.distplot(train_df[train_df['is_duplicate'] == 1.0]['fuzz_ratio'] , label = \"1\",ax=ax2)\nsns.distplot(train_df[train_df['is_duplicate'] == 0.0]['fuzz_ratio'] , label = \"0\" , ax=ax2)","a5f933e4":"train_df_subset = train_df[0:4000]\nX = MinMaxScaler().fit_transform(train_df_subset[['cwc_min', 'cwc_max', 'csc_min', 'csc_max' , 'ctc_min' , 'ctc_max' , 'last_word_eq', 'first_word_eq' , 'abs_len_diff' , 'mean_len' , 'token_set_ratio' , 'token_sort_ratio' ,  'fuzz_ratio' , 'fuzz_partial_ratio' , 'longest_substr_ratio']])\ny = train_df_subset['is_duplicate'].values","0e945e77":"tsne_in_2d = TSNE(n_components=2, random_state=42,n_iter=1000,verbose=1).fit_transform(X)\nplt.figure(figsize=(12,8))\nsns.scatterplot(x =tsne_in_2d[:, 0], y = tsne_in_2d[:, 1], hue = y,palette=\"bright\")","86d32108":"train = pd.read_csv(\"\/kaggle\/input\/quora-question-pairs\/train.csv.zip\")","af347ffb":"train['question1'] = train.apply(lambda row: str(row.question1), axis=1)\ntrain['question2'] = train.apply(lambda row: str(row.question2), axis=1)","5f8ac50f":"merge_questions = list(train['question1']) + list(train['question2'])\ntfidf = TfidfVectorizer(lowercase=False)\ntfidf.fit_transform(merge_questions)","ae2024bc":"word_to_idf = dict(zip(tfidf.get_feature_names(), tfidf.idf_))","165e44a6":"w2v = spacy.load('en_core_web_sm')","4a8bbce2":"w2v_vec_q1 = []\nfor qus1 in tqdm(list(train['question1'])):\n    doc_q1 = w2v(qus1)\n    mean_vec_q1 = np.zeros([len(doc_q1), len(doc_q1[0].vector)])\n    for word in doc_q1:\n        vec = word.vector\n        try:\n            idf = word_to_idf[str(word)]\n        except:\n            idf = 0\n        mean_vec_q1 += vec * idf\n    mean_vec_q1 = mean_vec_q1.mean(axis=0)\n    w2v_vec_q1.append(mean_vec_q1)\ntrain['q1_feats_m'] = list(w2v_vec_q1)","22c15ee3":"w2v_vec_q2 = []\nfor qus2 in tqdm(list(train['question2'])):\n    doc_q2 = w2v(qus2)\n    mean_vec_q2 = np.zeros([len(doc_q2), len(doc_q2[0].vector)])\n    for word in doc_q2:\n        vec = word.vector\n        try:\n            idf = word_to_idf[str(word)]\n        except:\n            idf = 0\n        mean_vec_q2 += vec * idf\n    mean_vec_q2 = mean_vec_q2.mean(axis=0)\n    w2v_vec_q2.append(mean_vec_q2)\ntrain['q2_feats_m'] = list(w2v_vec_q2)","478ca6c4":"df_q1 = pd.DataFrame(train['q1_feats_m'].values.tolist(), index= train.index)\ndf_q2 = pd.DataFrame(train['q2_feats_m'].values.tolist(), index= train.index)\ndf_q1[\"id\"] = train_df[\"id\"]\ndf_q2[\"id\"] = train_df[\"id\"]\ndf_q = df_q1.merge(df_q1,on =\"id\",how = \"left\")","f9191017":"train_df.drop([\"qid1\", \"qid2\", \"question1\",\"question2\"], axis=1, inplace = True)\ndf_final = train_df.merge(df_q,on =\"id\",how = \"left\")","39b8e523":"print(\"Size of final dataframe  :\", df_final.shape)","2baae6a4":"df_final.head()","53ecd8e3":"y = df_final[\"is_duplicate\"]\ndf_final.drop(['id', 'is_duplicate'],axis=1,inplace=True)\nX_train,X_test, y_train, y_test = train_test_split(df_final, y, stratify=y, test_size=0.3,random_state = 42)","3cee50d0":"print(\"Training data size :\",X_train.shape)\nprint(\"Test data size :\",X_test.shape)","c0e06fba":"print(\"Distribution of target variable in training data\")\nprint(\"class 0 : \", y_train.value_counts()[0]\/len(y_train), \"class 1: \",y_train.value_counts()[1]\/len(y_train))\nprint(\"Distribution of target variable in test data\")\nprint(\"class 0 : \", y_test.value_counts()[0]\/len(y_test), \"class 1: \",y_test.value_counts()[1]\/len(y_test))","19f05a4d":"def plot_confusion_precison_recall_matrix(cm):\n    A =(((cm.T)\/(cm.sum(axis=1))).T)\n    B =(cm\/cm.sum(axis=0))\n    labels = [\"No\",\"Yes\"]\n    cmap=sns.light_palette(\"blue\")\n    fig,(ax1,ax2,ax3) = plt.subplots(1, 3,figsize=(20,4))\n    sns.heatmap(cm, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels,ax=ax1)\n    ax1.set_xlabel('Predicted Class')\n    ax1.set_ylabel('Original Class')\n    ax1.set_title(\"Confusion matrix\")\n    sns.heatmap(B, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels,ax=ax2)\n    ax2.set_xlabel('Predicted Class')\n    ax2.set_ylabel('Original Class')\n    ax2.set_title(\"Precision matrix\")\n    sns.heatmap(A, annot=True, cmap=cmap, fmt=\".3f\", xticklabels=labels, yticklabels=labels,ax=ax3)\n    ax3.set_xlabel('Predicted Class')\n    ax3.set_ylabel('Original Class')\n    ax3.set_title(\"Recall matrix\")","e16d5748":"y_pred = np.zeros((len(y_test),2))\nfor i in range(len(y_test)):\n    rand_probs = np.random.rand(1,2)\n    y_pred[i] = (rand_probs\/rand_probs.sum(axis=1)[0])\nprint(\"Log loss on Test Data using Random Model\",log_loss(y_test, y_pred, eps=1e-15))\n\ny_pred = np.argmax(y_pred, axis=1)\ncm = confusion_matrix(y_test, y_pred)\nplot_confusion_precison_recall_matrix(cm)","57d19022":"alpha = [10 ** x for x in range(-5, 2)]\nval_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l2', loss='log', random_state=42)\n    clf.fit(X_train, y_train)\n    calib_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    calib_clf.fit(X_train, y_train)\n    y_pred = calib_clf.predict_proba(X_test)\n    val_error_array.append(log_loss(y_test, y_pred, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, y_pred, labels=clf.classes_, eps=1e-15))\n\nplt.plot(alpha, val_error_array,c='b')\nfor i, logloss in enumerate(np.round(val_error_array,3)):\n    plt.annotate((alpha[i],np.round(logloss,3)), (alpha[i],val_error_array[i]))\nplt.grid()\nplt.title(\"Validation Error for different values of alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Log loss\")\n\n\n\nbest_alpha = np.argmin(val_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l2', loss='log', random_state=42)\nclf.fit(X_train, y_train)\ncalib_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\ncalib_clf.fit(X_train, y_train)\n\ny_pred = calib_clf.predict_proba(X_train)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, y_pred, labels=clf.classes_, eps=1e-15))\ny_pred = calib_clf.predict_proba(X_test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, y_pred, labels=clf.classes_, eps=1e-15))\ny_pred =np.argmax(y_pred,axis=1)\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Misclassification error :\", ((cm[0,1] + cm[1,0]) \/ len(y_pred))*100)\nplot_confusion_precison_recall_matrix(cm)","e659fd68":"alpha = [10 ** x for x in range(-5, 2)] \nval_error_array=[]\nfor i in alpha:\n    clf = SGDClassifier(alpha=i, penalty='l1', loss='hinge', random_state=42)\n    clf.fit(X_train, y_train)\n    calib_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\n    calib_clf.fit(X_train, y_train)\n    y_pred = calib_clf.predict_proba(X_test)\n    val_error_array.append(log_loss(y_test, y_pred, labels=clf.classes_, eps=1e-15))\n    print('For values of alpha = ', i, \"The log loss is:\",log_loss(y_test, y_pred, labels=clf.classes_, eps=1e-15))\n\nplt.plot(alpha, val_error_array,c='b')\nfor i, logloss in enumerate(np.round(val_error_array,3)):\n    plt.annotate((alpha[i],np.round(logloss,3)), (alpha[i],val_error_array[i]))\nplt.grid()\nplt.title(\"Validation Error for different values of alpha\")\nplt.xlabel(\"Alpha i's\")\nplt.ylabel(\"Log loss\")\n\n\n\nbest_alpha = np.argmin(val_error_array)\nclf = SGDClassifier(alpha=alpha[best_alpha], penalty='l1', loss='hinge', random_state=42)\nclf.fit(X_train, y_train)\ncalib_clf = CalibratedClassifierCV(clf, method=\"sigmoid\")\ncalib_clf.fit(X_train, y_train)\n\ny_pred = calib_clf.predict_proba(X_train)\nprint('For values of best alpha = ', alpha[best_alpha], \"The train log loss is:\",log_loss(y_train, y_pred, labels=clf.classes_, eps=1e-15))\ny_pred = calib_clf.predict_proba(X_test)\nprint('For values of best alpha = ', alpha[best_alpha], \"The test log loss is:\",log_loss(y_test, y_pred, labels=clf.classes_, eps=1e-15))\ny_pred =np.argmax(y_pred,axis=1)\ncm = confusion_matrix(y_test, y_pred)\nprint(\"Misclassification error :\", ((cm[0,1] + cm[1,0]) \/ len(y_pred))*100)\nplot_confusion_precison_recall_matrix(cm)","752c3ac3":"params = {}\nparams['objective'] = 'binary:logistic'\nparams['eval_metric'] = 'logloss'\nparams['eta'] = 0.02\nparams['max_depth'] = 4\n\nd_train = xgb.DMatrix(X_train, label=y_train)\nd_test = xgb.DMatrix(X_test, label=y_test)\n\nwatchlist = [(d_train, 'train'), (d_test, 'valid')]\n\nbst = xgb.train(params, d_train, 400, watchlist, early_stopping_rounds=20, verbose_eval=10)\n\nxgdmat = xgb.DMatrix(X_train,y_train)\ny_pred = bst.predict(d_test)\nprint(\"The test log loss is:\",log_loss(y_test, y_pred, labels=clf.classes_, eps=1e-15))","0a258a2b":"### Notebook - Table of Content\n\n1. [**Importing necessary libraries**](#1.-Importing-necessary-libraries)   \n2. [**Loading data**](#2.-Loading-data)   \n3. [**Basic Data Analysis**](#3.-Basic-Data-Analysis)  \n    3.1 [**Checking for class imbalance**](#3.1-Checking-for-class-imbalance)  \n    3.2 [**Number of distinct questions**](#3.2-Number-of-distinct-questions)  \n4. [**Data preprocessing**](#4.-Data-preprocessing)  \n    4.1 [**Checking for duplicates**](#4.1-Checking-for-duplicates)  \n    4.2 [**Checking for missing values**](#4.2-Checking-for-missing-values)  \n5. [**Basic Feature Extraction**](#5.-Basic-Feature-Extraction)  \n    5.1 [**Analysis on few extracted features**](#5.1-Analysis-on-few-extracted-features)  \n6. [**Text preprocessing**](#6.-Text-preprocessing)   \n    6.1 [**Analysing extracted features **](#6.1-Analysing-extracted-features )  \n7. [**Visualization using t-SNE**](#7.-Visualization-using-t-SNE)   \n8. [**Featurization through weighted tf-idf based word vectors**](#8.-Featurization-through-weighted-tf-idf-based-word-vectors) \n9. [**Fitting a random model**](#9.-Fitting-a-random-model)\n10. [**Machine Learning models**](#10.-Machine-Learning-models)  \n    10.1 [**Fitting Logistic Regression model with Hyperparameter tuning**](#10.1-Fitting-Logistic-Regression-model-with-Hyperparameter-tuning)  \n    10.2 [**Fititng Linear SVM with hyperparameter tuning**](#10.2-Fititng-Linear-SVM-with-hyperparameter-tuning)  \n    10.3 [**Fitting Xgboost model**](#10.3-Fitting-Xgboost-model)  ","dd96880c":"### 3. Basic Data Analysis\n\n#### 3.1 Checking for class imbalance\n\n#### Barplot of is_duplicate feature","7a2c5d19":"#### 5.1.a Univariate analysis of feature word_share","1f93352b":"#### 5.1.b Univariate analysis of feature word_common","f32b2035":"### 10.3 Fitting Xgboost model","f90223df":"### 2. Loading data ","886992be":"#### 6.1.d Distribution of the token_sort_ratio ","99602b20":"### 1. Importing necessary libraries ","e692f9cf":"### 7. Visualization using t-SNE","9f8f492d":"## 10. Machine Learning models\n\n### 10.1 Fitting Logistic Regression model with Hyperparameter tuning","42c5df61":"**Additional NOTE**\n\nIf you are interested in learning or exploring more about importance of feature selection in machine learning, then refer to my below blog offering.\n\nhttps:\/\/www.analyticsvidhya.com\/blog\/2020\/10\/a-comprehensive-guide-to-feature-selection-using-wrapper-methods-in-python\/","2c855c6c":"#### Frequency of each question","ef5e4718":"## 9. Fitting a random model","a2f74786":"#### 6.1.c Distribution of the token_sort_ratio ","861843bb":"### 6. Text preprocessing\n\nIt involves - \n- Removing html tags\n- Removing Punctuations\n- Removing Stopwords\n- Performing stemming\n- Expanding contractions etc.","9a896562":"### 8. Featurization through weighted tf-idf based word vectors","5e069a4e":"### 4. Data preprocessing","3a1b7c57":"### 5. Basic Feature Extraction\n\n- freq_qid1 = Frequency of qid1's\n- freq_qid2 = Frequency of qid2's\n- q1len = Length of q1\n- q2len = Length of q2\n- q1_n_words = Number of words in Question 1\n- q2_n_words = Number of words in Question 2\n- word_Common = (Number of common unique words in Question 1 and Question 2)\n- word_Total =(Total num of words in Question 1 + Total num of words in Question 2)\n- word_share = (word_common)\/(word_Total)\n- freq_q1+freq_q2 = sum total of frequency of qid1 and qid2\n- freq_q1-freq_q2 = absolute difference of frequency of qid1 and qid2","e6fdf189":"### 6.1 Analysing extracted features \n\n#### 6.1.a Word cloud formation","449b72be":"#### 5.1 Analysis on few extracted features","3251ceb6":"#### 4.2 Checking for missing values","5c5c66b5":"### Splitting into train and test set with 80:20 ratio","c986cdc7":"### 10.2 Fititng Linear SVM with hyperparameter tuning","754f5a58":"#### 6.1.b Analysis of ['ctc_min', 'cwc_min', 'csc_min', 'token_sort_ratio'] using pair plot","8c18eaa8":"#### 3.2 Number of distinct questions","71d4710d":"#### 4.1 Checking for duplicates"}}