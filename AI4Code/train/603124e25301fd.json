{"cell_type":{"95e83741":"code","e18ab48b":"code","e5843ac3":"code","b40e96c6":"code","5d1c5328":"code","71cb3a21":"code","4ae26a63":"code","4118c46f":"code","b969d87b":"code","f2d9da89":"code","7a29ac90":"code","c1fd3e23":"code","995c2e5f":"code","4091ee7f":"code","971279ac":"markdown","6c3d2497":"markdown","cb0153af":"markdown","fda2881f":"markdown","c8ea2e5b":"markdown","b8a467f2":"markdown","7e08b374":"markdown","e153611c":"markdown","81323c97":"markdown","9396401e":"markdown","bc14ed86":"markdown","3e411134":"markdown","e7ee89f7":"markdown","56a41cd5":"markdown","0ae0af09":"markdown","d7effff5":"markdown","01e401f8":"markdown","3d6a5d98":"markdown","fd8a7153":"markdown","b210a3a2":"markdown"},"source":{"95e83741":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e18ab48b":"train = pd.read_csv('..\/input\/emnist\/emnist-letters-train.csv')\ntest = pd.read_csv('..\/input\/emnist\/emnist-letters-test.csv')","e5843ac3":"print('training dataset dimensions: ', train.shape)\nprint('test dataset dimensions: ', test.shape, '\\n')","b40e96c6":"# view the head of the training dataset\ntest.head()","5d1c5328":"# update column names for both datasets\ncolumns = ['labels']\nfor i in range(train.shape[1]-1):\n    columns.append(i)\n    \ntrain.columns = columns\ntest.columns = columns\n\nclasses = train['labels'].unique()\nprint('number of classes: ', len(classes))\n\ntrain.head()","71cb3a21":"from sklearn.model_selection import train_test_split\n\n# split training and validation data using sklearn\nx_train, x_val, y_train, y_val = train_test_split(train.drop(['labels'], axis=1),\n                                                  train.labels - 1,\n                                                  train_size=0.8,\n                                                  test_size=0.2,\n                                                  random_state=42)\n\n# reshape and normalize test data\nx_train = x_train \/ 255.0\nx_val = x_val \/ 255.0\n\ntestX = test.values[:, 1:].reshape(test.shape[0],28, 28, 1).astype('float32')\nx_test = testX \/ 255.0\ny_test = test['labels'].values - 1 # this is just to make the neurons in the output layer start at 0\n\nprint('trianing set: ', x_train.shape, y_train.shape)\nprint('validation set: ', x_val.shape, y_val.shape)\nprint('test set: ', x_test.shape, y_test.shape)","4ae26a63":"import matplotlib.pyplot as plt\nimport random\n# select random samples from training dataset\ntrain_samples = random.sample(range(0, len(x_train)), 9)\ntest_samples = random.sample(range(0, len(x_val)), 9)\n\nx_train\nplt.figure(figsize=(6, 6))\nplt.suptitle('Training set')\nfor i in train_samples:\n    plt.subplot(3, 3, train_samples.index(i)+1)\n    plt.imshow(x_train.iloc[i,:].values.reshape(28,28), cmap='binary')\n    plt.title(f'label: {y_train.iloc[i]}')\n    plt.axis('off')\n    \nplt.figure(figsize=(6, 6))\nplt.suptitle('Test set')\nfor i in test_samples:\n    plt.subplot(3, 3, test_samples.index(i)+1)\n    plt.imshow(x_val.iloc[i,:].values.reshape(28,28), cmap='binary')\n    plt.title(f'label: {y_val.iloc[i]}')\n    plt.axis('off')","4118c46f":"import tensorflow as tf\n\n# set accuracy\ndesired_accuracy = 0.92\n    \n# create callback to stop training when we reached desired accuracy\nclass myCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs={}):\n        if(logs.get('accuracy') is not None and logs.get('accuracy') >= desired_accuracy):\n            print('\\nReached 92% training accuracy: cancelling training...')\n            self.model.stop_training = True\n\n# instantiate callback\ncallbacks = myCallback()\n    \n\ndef train_model():\n    # define model\n    model = tf.keras.models.Sequential([\n        # initial normalization\n        tf.keras.layers.Reshape((28, 28, 1), input_shape=(784,)),\n#         tf.keras.layers.BatchNormalization(),\n        \n        # first convolution\n        tf.keras.layers.Conv2D(8, (3, 3), activation='relu'), # applies kernels to our data\n        tf.keras.layers.MaxPooling2D(2, 2), # reduce dimension\n#         tf.keras.layers.BatchNormalization(),\n#         tf.keras.layers.Dropout(0.4),\n        \n        # second convolution\n        tf.keras.layers.Conv2D(16, (3, 3), activation='relu'),\n        tf.keras.layers.MaxPooling2D(2, 2),\n#         tf.keras.layers.BatchNormalization(),\n#         tf.keras.layers.Dropout(0.4),\n        \n        # third convolution\n        tf.keras.layers.Conv2D(24, (3, 3), activation='relu'),\n#         tf.keras.layers.MaxPooling2D(2, 2),\n#         tf.keras.layers.BatchNormalization(),\n#         tf.keras.layers.Dropout(0.4),\n\n    \n        # feed to DNN\n        tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(128, activation='relu'),\n#         tf.keras.layers.Dropout(0.2),\n        tf.keras.layers.Dense(len(classes), activation=tf.nn.softmax) # generalized logistic regression\n    ])\n    \n    # use sparse categorical crossentropy since values are labeled from 0-25\n    model.compile(optimizer='adam',loss='sparse_categorical_crossentropy',\n                  metrics=['accuracy'])\n    \n    return model","b969d87b":"# view model summary before running neural network\nmodel = train_model()\nmodel.summary()\n","f2d9da89":"# train neural network and have it automatically stop on 95% accuracy\nhistory = model.fit(x_train, y_train, epochs=1000,\n                    validation_data=(x_val, y_val),\n                    batch_size=4096, verbose=1,\n                    callbacks=[callbacks])","7a29ac90":"from tensorflow.keras.preprocessing.image import img_to_array, load_img\n\nsuccessive_outputs = [layer.output for layer in model.layers[1:]]\nvisualization_model = tf.keras.models.Model(inputs = model.input, outputs = successive_outputs)\n\nnumber = random.sample(range(0, len(x_val)), 1)[0]\n\n# Run the image through the network and return the intermediate representations for the data.\nsuccessive_feature_maps = visualization_model.predict(x_val.iloc[number, :].to_numpy().reshape(1, 784))\n\n# These are the names of the layers, so can have them as part of our plot\nlayer_names = [layer.name for layer in model.layers[1:]]\n\n# Now let's display our representations\nfor layer_name, feature_map in zip(layer_names, successive_feature_maps):\n  if len(feature_map.shape) == 4:\n    # Just do this for the conv \/ maxpool layers, not the fully-connected layers\n    n_features = feature_map.shape[-1]  # number of features in feature map\n    # The feature map has shape (1, size, size, n_features)\n    size = feature_map.shape[1]\n    # We will tile our images in this matrix\n    display_grid = np.zeros((size, size * n_features))\n    for i in range(n_features):\n      # Postprocess the feature to make it visually palatable\n      x = feature_map[0, :, :, i]\n      x -= x.mean()\n      x \/= x.std()\n      x *= 64\n      x += 128\n      x = np.clip(x, 0, 255).astype('uint8')\n      # We'll tile each filter into this big horizontal grid\n      display_grid[:, i * size : (i + 1) * size] = x\n    # Display the grid\n    scale = 10. \/ n_features\n    plt.figure(figsize=(scale * n_features, scale))\n    plt.title(layer_name)\n    plt.grid(False)\n    plt.imshow(display_grid, aspect='auto', cmap='viridis')","c1fd3e23":"# Plot training vs validation accruacy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('CNN Model Accuracy')\nplt.ylabel('Accuracy')\nplt.xlabel('Number of Epochs')\nplt.legend(['training', 'validation'], loc='lower right')\nplt.show()\n\n# Plot training vs validation losses\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('CNN Model Loss')\nplt.ylabel('Loss')\nplt.xlabel('Number of Epochs')\nplt.legend(['training', 'validation'], loc='upper right')\nplt.show()","995c2e5f":"evaluate = model.evaluate(x_test, y_test)\n\n# predict values for all data\npredictions = model.predict(x_test)\nprint(\"predictions shape:\", predictions.shape)\n","4091ee7f":"# select random data from test set and predict it's classification\ntest_samples = random.sample(range(0, len(x_test)), 30) # select 30 samples\n\n# display results\nplt.figure(figsize=(15, 6))\nfor i in test_samples:\n    plt.subplot(3, 10, test_samples.index(i) + 1)  \n    plt.imshow(x_test[i].reshape(28,28), cmap='magma')\n    plt.title(f'predicted: {np.argmax(predictions[i])} \\nactual: {y_test[i]}')\n    plt.axis('off')","971279ac":"## **Viewing the sample images of the training and test set**\nUsing matplotlib, let's plot some of the images from the training dataset. This will print out random samples everytime we run the code","6c3d2497":"## **Creating a training and validation split**\nNow we'll create a random train and validation dataset split. This means that **a sample from the training data will be used to evaluate the performance the our neural network while adjusting its hyperparameters**. After splitting the traning and validation set randomly, we will then reshape and normalize the inputs to make it compatible with the input requirements of the neural network","cb0153af":"**After a few epochs our neural network has a reached ~92% accuracy in the training set and a ~90% accuracy in the validation set**. Adding the dropouts will make the accuracy of the validation test greater than the training set. Dropouts will ignore a percentage of the neurons in the specified layer which, in return, can help avoid overfitting and help the neural network generalize better. In our case, the results are already acceptable without droputs.","fda2881f":"## **Plotting the losses and the accuracy of the training and validation set**\nLet's see how our model performs as we increase the number of epochs","c8ea2e5b":"### **What does our evaluation accuracy mean?**\n* If our training set is significantly higher that our validation set, it means that we are overfitiing our training data. This will result to our neural network performing poorly in our validation and test data. \n\n* In our case, however, we can see that our test accuracy is farily close with our training and  validation accuracy. This means that our CNN generalized well with our test data.","b8a467f2":"## **Training the neural network:**\nSo far we've defined some the parameters of our neural network . We also created a training and validation split. Now we'll use these data to train our model","7e08b374":"#### **We can now see the what our model predicted vs what the actual label is. Depending on the sample displayed, we expect that there will be some misclassifications since our accuracy isn't 100%**\n\n#### **Our labels are defined from 0 (a or A) to 25 (z or Z)**\n\n","e153611c":"### ***Thank you for checking this notebook out!***","81323c97":"## **Neural network evaluation**\nHere we will evaluate the performace of our neural network by using the test set. Based on the output, there will be instances that the predicted will be different from the actual label.","9396401e":"Plotting the accuracy and loss over time gives us an idea well our neural network is performing for each epoch. **We can also use these graphs to identify at what epoch can we stop training** to avoid overtraining our network. For example I've set the number of epochs in this network to be 1000. Viewing the plot can help us identify which epoch to stop\n\nIn relation to this, **we can also set our network to automatically stop training at a given training accuracy**. This is done by using **callbacks**.","bc14ed86":"## **Visualizing the convolution and pooling filters**\nThis visualization will let us see the features that our neural network learned as it passed through the convolutions. **Observe that specific parts of the letter lights up** in each layer. We can interpret this as the network identifying those features of the letter","3e411134":"From here we can see that we are dealing with a training with 88799 examples and a test dataset having  14799 examples, both having 784 pixels per data (the first column being the label). Our plan is to **create a validation split from the training data and then use the test set for evaluating how well can the neural network predict new data**","e7ee89f7":"## **Loading the training and test datasets:**\nThe training and test datasets are specified as 'emnist-letters-train.csv' and 'emnist-letters-test.csv' respectively. We will read these load these read these files using pandas","56a41cd5":"The 26 classes here correspond to the 26 unique characters from the english alphabet","0ae0af09":"## **Exploring the data:**\nAlthough we already know that we are dealing with a letters dataset, it's better to see what our data looks like:","d7effff5":"### **Hi! We will go through how a basic convolutional neural network works in tensorflow. If you find this helpful\/insightful please do upvote this notebook :)**\n\n### **thank you!**","01e401f8":"### **What have we implemented\/experimented in this neural network so far?**\nWe have the following layers in our CNN:\n* **Convolutional layers** - what convolutional layers do is that it applies various kernels on the image to extract or identify important features that we will feed into the neural network\n\n* **Pooling filters** - pooling helps reduce the spatial size of an image to reduce the amount of parameters when computing for the output. Progressively reducing the dimensions (down sampling) of the images\/feature representations in each convolution will later help reducing the computational cost of the neural network\n\nWe also have omitted the following:\n* **Droputs** -  dropouts ignore a percentage of the 'neurons' in each layers. Doing this helps in preventing overfitting the training data\n\n* **Batch normalization**  - this helps in  re-centering and re-scaling our outputs in every layer\n\n**NOTE:**\n\n At this point we've already repetitively tuned the hyperparameters to get the desired result from the neural network. That is why we've omitted normalizing and dropouts in the model\n \n \n \n **What are these so-called hyperparameters?**\n \nthese are the what we change in our neural network to optimize its performance. Some include the number of neurons in a layer, what activation function we will use, the optimizer, learning rate, batch size, and epochs.","3d6a5d98":"**By visualizing each convolution and pooling layer, we will have an idea of what 'features' are being learned by our neural network**. For example, it may be trying to identify specific edges and curves, or even 'holes' for specific letters. Pooling helps down sample these features so our neural network will be able to train faster","fd8a7153":"From the initial view of the training dataset, the first column contains the labels and the rest of the columns are the pixles. let's just fix the column names to properly label each cell:","b210a3a2":"### **Creating the CNN Model**\nSince we have an idea of what our data is, we can finally start creating our neural network"}}