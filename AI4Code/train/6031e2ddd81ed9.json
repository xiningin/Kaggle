{"cell_type":{"8cbc7b5b":"code","d613e515":"code","a0fbea56":"code","2c40f7a2":"code","8e612251":"code","25ff46ca":"code","b14d4b7f":"code","c42eca43":"code","66955c1d":"code","57e1e447":"code","f9d9e82e":"code","cd4fdbf0":"code","1f28efa1":"code","3d1d4a2b":"code","16831746":"code","90e6f3cb":"code","5128f135":"code","4bf8cb74":"code","dcf54132":"code","ca497f87":"code","c767e356":"code","502bae40":"code","22c552ca":"code","8b49626d":"code","1f8da503":"code","1931b641":"markdown","c977d958":"markdown","db65b2a5":"markdown","c1b2f2d8":"markdown","667baf71":"markdown","4d6b1b82":"markdown","2ab335b8":"markdown","fbb0b1c2":"markdown","271ca5f3":"markdown"},"source":{"8cbc7b5b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d613e515":"import pandas as pd\nimport numpy as np\nimport re\nfrom nltk import word_tokenize\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM, Embedding\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom nltk.corpus import stopwords\n\nfrom keras.layers import SimpleRNN,LSTM,CuDNNGRU,CuDNNLSTM,Conv1D,MaxPooling1D,Dropout\nfrom keras import regularizers\nfrom keras.layers import BatchNormalization\nfrom keras import optimizers\nfrom keras import initializers\n\nfrom keras.callbacks import *\nfrom keras import backend as K\nimport keras \n\n\n\nfrom keras.callbacks import *\nfrom keras.optimizers import Adam\n\n\n\n\n","a0fbea56":"cols = ['sentiment','id','date','query_string','user','text']\ndf = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv', encoding='latin1', names=cols)","2c40f7a2":"df.head()\ndf.info()\ndf['sentiment'].value_counts()","8e612251":"df.drop(['id','date','query_string','user'],axis = 1)","25ff46ca":"#Define a pattern\n\npat1= '#[^ ]+'\npat2 = 'www.[^ ]+'\npat3 = '@[^ ]+'\npat4 = '[0-9]+'\npat5 = 'http[^ ]+'\nnegations_dic = {\"isn't\":\"is not\", \"aren't\":\"are not\", \"wasn't\":\"was not\", \"weren't\":\"were not\",   \n                \"haven't\":\"have not\",\"hasn't\":\"has not\",\"hadn't\":\"had not\",\"won't\":\"will not\",\n                \"wouldn't\":\"would not\", \"don't\":\"do not\", \"doesn't\":\"does not\",\"didn't\":\"did not\",\n                \"can't\":\"can not\",\"couldn't\":\"could not\",\"shouldn't\":\"should not\",\"mightn't\":\"might not\",\n                \"mustn't\":\"must not\"}\n\n\npattern = '|'.join((pat1,pat2,pat3,pat4,pat5))\nneg_pattern = re.compile(r'\\b(' + '|'.join(negations_dic.keys()) + r')\\b')","b14d4b7f":"#Cleaning Data and removing Stop Words\nstop_words = stopwords.words('english')\nclean_tweets = []\n\n\nfor t in df['text']:\n    t.lower()\n    t = re.sub(pattern,'',t)\n    t = neg_pattern.sub(lambda x: negations_dic[x.group()], t)\n    t = word_tokenize(t)\n    t = [x for x in t if len(x) >1]\n    t = [x for x in t if x not in stop_words]\n    t = [x for x in t if x.isalpha()]\n    t = \" \".join(t)\n    t = re.sub(\"n't\",\"not\",t)\n    t = re.sub(\"'s\",\"is\",t)\n    clean_tweets.append(t)","c42eca43":"df_clean = pd.DataFrame(clean_tweets,columns = ['text'])\ndf_clean['sentiment']=df['sentiment'].replace(4,1)","66955c1d":"print(df_clean['text'].head(20))\ndf_clean['sentiment'].value_counts()    ","57e1e447":"length = []\nfor t in df_clean['text'] :\n    l = len(re.findall(r'\\w+', t))\n    length.append(l)\n","f9d9e82e":"np.percentile(length, [50,75,90,95,98])","cd4fdbf0":"x = df_clean['text']\ny = df_clean['sentiment']\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.1, random_state=0)","1f28efa1":"#Tokenization\ntk = Tokenizer()\ntk.fit_on_texts(x_train)\n\nx_train_tok = tk.texts_to_sequences(x_train)\nx_test_tok = tk.texts_to_sequences(x_test)\n\nmax_len = 20\nx_train_pad = pad_sequences(x_train_tok, maxlen=max_len)\nx_test_pad = pad_sequences(x_test_tok, maxlen=max_len)","3d1d4a2b":"unique_vocab = len(tk.word_index)\nprint(unique_vocab)","16831746":"#Word Embedding\nfrom gensim.models import KeyedVectors","90e6f3cb":"word2vec = KeyedVectors.load_word2vec_format(\"..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin\", binary=True)\nprint('Found %s word vectors of word2vec' % len(word2vec.vocab))","5128f135":"# Testing Some Simliarities \nw2 = 'love'\nword2vec.most_similar_cosmul(positive=w2)\n","4bf8cb74":"w1 ='hate'\nword2vec.most_similar_cosmul(positive=w1)\n\n","dcf54132":"word2vec.similarity('woman', 'girl')","ca497f87":"\n\n##https:\/\/github.com\/bckenstler\/CLR\n\n\nclass CyclicLR(keras.callbacks.Callback):\n    \"\"\"This callback implements a cyclical learning rate policy (CLR).\n    The method cycles the learning rate between two boundaries with\n    some constant frequency, as detailed in this paper (https:\/\/arxiv.org\/abs\/1506.01186).\n    The amplitude of the cycle can be scaled on a per-iteration or \n    per-cycle basis.\n    This class has three built-in policies, as put forth in the paper.\n    \"triangular\":\n        A basic triangular cycle w\/ no amplitude scaling.\n    \"triangular2\":\n        A basic triangular cycle that scales initial amplitude by half each cycle.\n    \"exp_range\":\n        A cycle that scales initial amplitude by gamma**(cycle iterations) at each \n        cycle iteration.\n    For more detail, please see paper.\n    \n    # Example\n        ```python\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., mode='triangular')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```\n    \n    Class also supports custom scaling functions:\n        ```python\n            clr_fn = lambda x: 0.5*(1+np.sin(x*np.pi\/2.))\n            clr = CyclicLR(base_lr=0.001, max_lr=0.006,\n                                step_size=2000., scale_fn=clr_fn,\n                                scale_mode='cycle')\n            model.fit(X_train, Y_train, callbacks=[clr])\n        ```    \n    # Arguments\n        base_lr: initial learning rate which is the\n            lower boundary in the cycle.\n        max_lr: upper boundary in the cycle. Functionally,\n            it defines the cycle amplitude (max_lr - base_lr).\n            The lr at any cycle is the sum of base_lr\n            and some scaling of the amplitude; therefore \n            max_lr may not actually be reached depending on\n            scaling function.\n        step_size: number of training iterations per\n            half cycle. Authors suggest setting step_size\n            2-8 x training iterations in epoch.\n        mode: one of {triangular, triangular2, exp_range}.\n            Default 'triangular'.\n            Values correspond to policies detailed above.\n            If scale_fn is not None, this argument is ignored.\n        gamma: constant in 'exp_range' scaling function:\n            gamma**(cycle iterations)\n        scale_fn: Custom scaling policy defined by a single\n            argument lambda function, where \n            0 <= scale_fn(x) <= 1 for all x >= 0.\n            mode paramater is ignored \n        scale_mode: {'cycle', 'iterations'}.\n            Defines whether scale_fn is evaluated on \n            cycle number or cycle iterations (training\n            iterations since start of cycle). Default is 'cycle'.\n    \"\"\"\n\n    def __init__(self, base_lr=0.001, max_lr=0.006, step_size=2000., mode='triangular',\n                 gamma=1., scale_fn=None, scale_mode='cycle'):\n        super(CyclicLR, self).__init__()\n\n        self.base_lr = base_lr\n        self.max_lr = max_lr\n        self.step_size = step_size\n        self.mode = mode\n        self.gamma = gamma\n        if scale_fn == None:\n            if self.mode == 'triangular':\n                self.scale_fn = lambda x: 1.\n                self.scale_mode = 'cycle'\n            elif self.mode == 'triangular2':\n                self.scale_fn = lambda x: 1\/(2.**(x-1))\n                self.scale_mode = 'cycle'\n            elif self.mode == 'exp_range':\n                self.scale_fn = lambda x: gamma**(x)\n                self.scale_mode = 'iterations'\n        else:\n            self.scale_fn = scale_fn\n            self.scale_mode = scale_mode\n        self.clr_iterations = 0.\n        self.trn_iterations = 0.\n        self.history = {}\n\n        self._reset()\n\n    def _reset(self, new_base_lr=None, new_max_lr=None,\n               new_step_size=None):\n        \"\"\"Resets cycle iterations.\n        Optional boundary\/step size adjustment.\n        \"\"\"\n        if new_base_lr != None:\n            self.base_lr = new_base_lr\n        if new_max_lr != None:\n            self.max_lr = new_max_lr\n        if new_step_size != None:\n            self.step_size = new_step_size\n        self.clr_iterations = 0.\n        \n    def clr(self):\n        cycle = np.floor(1+self.clr_iterations\/(2*self.step_size))\n        x = np.abs(self.clr_iterations\/self.step_size - 2*cycle + 1)\n        if self.scale_mode == 'cycle':\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(cycle)\n        else:\n            return self.base_lr + (self.max_lr-self.base_lr)*np.maximum(0, (1-x))*self.scale_fn(self.clr_iterations)\n        \n    def on_train_begin(self, logs={}):\n        logs = logs or {}\n\n        if self.clr_iterations == 0:\n            K.set_value(self.model.optimizer.lr, self.base_lr)\n        else:\n            K.set_value(self.model.optimizer.lr, self.clr())        \n            \n    def on_batch_end(self, epoch, logs=None):\n        \n        logs = logs or {}\n        self.trn_iterations += 1\n        self.clr_iterations += 1\n\n        self.history.setdefault('lr', []).append(K.get_value(self.model.optimizer.lr))\n        self.history.setdefault('iterations', []).append(self.trn_iterations)\n\n        for k, v in logs.items():\n            self.history.setdefault(k, []).append(v)\n        \n        K.set_value(self.model.optimizer.lr, self.clr())\n","c767e356":"embedding_dim = 300\n\nembedding_matrix = np.zeros((unique_vocab+1, embedding_dim)) # intial embedding matrix with zeros \\\n                                                             #with dim (# of token word , # of features)\n# Now get the feature for token words\n\nfor word, i in tk.word_index.items():      \n    if word in word2vec.vocab:\n        embedding_matrix[i] = word2vec[word]      #emb mat from 0 to 11549   if unique_vocab not +1\n\n        \nprint('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))     #axis=1 --> row","502bae40":"from keras.layers import SpatialDropout1D\nfrom keras.layers import advanced_activations\nfrom keras.layers import TimeDistributed\nfrom keras.layers import Conv2D\nfrom keras.layers import Bidirectional\nopt = optimizers.adam(lr=0.001)\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=unique_vocab+1, output_dim=embedding_dim, input_length=max_len,\n                    weights=[embedding_matrix],trainable=True))\n\n\n\nmodel.add(SpatialDropout1D((0.25)))\n\nmodel.add(Conv1D(filters=300, kernel_size=5, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2,stride=1))\nmodel.add(BatchNormalization()) \nmodel.add(SpatialDropout1D((0.5)))\n\nmodel.add(Conv1D(filters=300, kernel_size=5, padding='same', activation='relu'))\nmodel.add(MaxPooling1D(pool_size=2,stride=1))\nmodel.add(BatchNormalization()) \nmodel.add(SpatialDropout1D((0.5)))\n\n\n\nmodel.add(Conv1D(300,4,padding='same',kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)))\nmodel.add(advanced_activations.LeakyReLU(alpha=0.3))\nmodel.add(MaxPooling1D(pool_size=2,stride=1))\nmodel.add(BatchNormalization()) \nmodel.add(SpatialDropout1D((0.5)))\n\nmodel.add(Conv1D(300,4,padding='same',kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01)))\nmodel.add(advanced_activations.LeakyReLU(alpha=0.3))\nmodel.add(MaxPooling1D(pool_size=2,stride=1))\nmodel.add(BatchNormalization()) \nmodel.add(SpatialDropout1D((0.5)))\n\n\n\nmodel.add(Bidirectional(CuDNNLSTM(256,kernel_regularizer=regularizers.l1_l2(l1=0.01, l2=0.01),return_sequences=False)))\n\n\nmodel.add(Dense(1,activation='sigmoid'))\n\n\n\n\nmodel.compile(optimizer = opt,loss = 'binary_crossentropy',metrics =['accuracy'])\n","22c552ca":"print(model.summary())","8b49626d":"clr_triangular = CyclicLR(mode='triangular2',step_size = 8400)\nhistory= model.fit(x=x_train_pad,y=y_train,validation_data=(x_test_pad,y_test),epochs =200,batch_size = 512,callbacks=[clr_triangular])","1f8da503":"# summarize history for accuracy\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n","1931b641":"### Loading Google News Word Embedding","c977d958":"## Buiding  Bi-LSTM + ConvNN Model","db65b2a5":"### Using Cyclical Learning Rate (CLR)","c1b2f2d8":"## Knowing The avg Length Of The Tweets ","667baf71":"## Import Required libraries","4d6b1b82":"## Preprocessing the text","2ab335b8":"### Importing & Exploring The DataSet","fbb0b1c2":"#### Splitting The Train\/Test Splits & Tokenize","271ca5f3":"## Loading The Embedding "}}