{"cell_type":{"11397ff1":"code","77dca5b6":"code","505c28b7":"code","c9601408":"code","dd96ece3":"code","41ccb445":"code","5aa87f34":"code","f078f27f":"code","3a37f7df":"code","e118088a":"code","2338b6c6":"code","6d09b130":"code","281e5322":"code","aba15cbb":"code","533e07f3":"code","694912a8":"code","0c5c5465":"code","fb1ce208":"code","146ebefd":"markdown","bfc3ad8d":"markdown","12c2cf65":"markdown","99a8f62a":"markdown","7adcc040":"markdown","28d97f53":"markdown","94a1ced4":"markdown","1889f116":"markdown","a2804775":"markdown","940610e0":"markdown"},"source":{"11397ff1":"\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport math\nimport xgboost\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_validate\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nsub = pd.read_csv('..\/input\/gender_submission.csv')\n\ntrain.head()","77dca5b6":"#Does the passanger have a cabin?\ntrain['cabin_binary'] = train[\"Cabin\"].apply(lambda i: 0 if str(i) == \"nan\" else 1)\n\n#Family Size\ntrain['family_size'] = 1 + train['SibSp'] + train['Parch']\ntrain['solo'] = train[\"family_size\"].apply(lambda i: 1 if i == 1 else 0)\n\n#Fix Nulls\ntrain['Embarked'] = train['Embarked'].fillna('S')\ntrain['Age'] = train['Age'].fillna(int(np.mean(train['Age'])))\n\n#A few age specific Binaries\ntrain['Child'] = train[\"Age\"].apply(lambda i: 1 if i <= 17 and i > 6 else 0)\ntrain['toddler'] = train[\"Age\"].apply(lambda i: 1 if i <= 6 else 0)\ntrain['Elderly'] = train[\"Age\"].apply(lambda i: 1 if i >= 60 else 0)\n\n# Fancy fancy\ntrain['fancy'] = train['Fare'].apply(lambda i: 1 if i >= 100 else 0)\n\n# standard\ntrain['standard_fare'] = train['Fare'].apply(lambda i: 1 if i <= 10.0 else 0)\n\n#No requirement to standardize in DT models, but might as well\nfare_scaler = StandardScaler()\nfare_scaler.fit(train['Fare'].values.reshape(-1, 1))\ntrain['fare_std'] = fare_scaler.transform(train['Fare'].values.reshape(-1, 1))\n\n#get status of passanger\ntrain['title'] = 'default'\n\nfor i in train.values:\n    name = i[3] #First checks for rare titles (Thanks Anisotropic's wonderful Kernel for inspiration\/\/help here!)\n    for e in ['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']:\n        if e in name:\n            train.loc[train['Name'] == name, 'title'] = 'rare'\n    if 'Miss' in name or  'Mlle' in name or 'Ms' in name or 'Mme' in name or 'Mrs' in name:\n        train.loc[train['Name'] == name, 'title'] = 'Ms'\n    if 'Mr.' in name or 'Master' in name:\n        train.loc[train['Name'] == name, 'title'] = 'Mr'\n\n\ntrain.head(10)","505c28b7":"#Does the passanger have a cabin?\ntest['cabin_binary'] = test[\"Cabin\"].apply(lambda i: 0 if str(i) == \"nan\" else 1)\n\n#Family Size\ntest['family_size'] = 1 + test['SibSp'] + test['Parch']\ntest['solo'] = test[\"family_size\"].apply(lambda i: 1 if i == 1 else 0)\n\n#Fix Nulls\ntest['Embarked'] = test['Embarked'].fillna('S')\ntest['Age'] = test['Age'].fillna(int(np.mean(test['Age'])))\n\n#A few age specific Binaries\ntest['Child'] = test[\"Age\"].apply(lambda i: 1 if i <= 17 and i > 6 else 0)\ntest['toddler'] = test[\"Age\"].apply(lambda i: 1 if i <= 6 else 0)\ntest['Elderly'] = test[\"Age\"].apply(lambda i: 1 if i >= 60 else 0)\n\n# Fancy fancy\ntest['fancy'] = test['Fare'].apply(lambda i: 1 if i >= 100 else 0)\ntest['standard_fare'] = test['Fare'].apply(lambda i: 1 if i <= 10.0 else 0)\n\n#standardize\ntest['fare_std'] = fare_scaler.transform(test['Fare'].values.reshape(-1, 1))\n\n#get status of passanger\ntest['title'] = 'default'\n\nfor i in test.values:\n    name = i[2] #First checks for rare titles (Thanks Anisotropic's wonderful Kernel for inspiration\/\/help here!)\n    for e in ['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona']:\n        if e in name:\n            test.loc[test['Name'] == name, 'title'] = 'rare'\n    if 'Miss' in name or  'Mlle' in name or 'Ms' in name or 'Mme' in name or 'Mrs' in name:\n        test.loc[test['Name'] == name, 'title'] = 'Ms'\n    if 'Mr.' in name or 'Master' in name:\n        test.loc[test['Name'] == name, 'title'] = 'Mr'\n\n\ntest.head(10)","c9601408":"train = pd.get_dummies(train, columns=[\"Sex\", \"Embarked\", \"title\"])\ntest = pd.get_dummies(test, columns=[\"Sex\", \"Embarked\", \"title\"])\n\ntrain = train.drop(['Name','PassengerId', 'Ticket', 'Cabin', 'Fare', 'SibSp'], axis = 1)\ntest = test.drop(['Name','PassengerId', 'Ticket', 'Cabin', 'Fare', 'SibSp'], axis = 1)\n\ntrain.head()","dd96ece3":"#COR MATRIX OF numerical vars\nplt.figure(figsize=(14,12))\nplt.title('Corelation Matrix', size=12)\nsns.heatmap(train.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=plt.cm.RdBu, linecolor='white', annot=True)","41ccb445":"#Family size histo\nsns.distplot(train['family_size'])","5aa87f34":"#boxplot of family size and survival\nsns.boxplot(\"Survived\", y=\"family_size\", data = train)","f078f27f":"#Fare to Age relationship?\nsns.lmplot('fare_std', 'Age', data = train, \n           fit_reg=False,scatter_kws={\"marker\": \"D\", \"s\": 20}) \n","3a37f7df":"#boxplot of age and survival\nsns.boxplot(\"Survived\", \"Age\", data = train)","e118088a":"#bar chart of age and survival\nsns.lmplot(\"Survived\", \"fare_std\", data = train, fit_reg=False)","2338b6c6":"X = train.drop(['Survived'], axis = 1)\ny = train['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","6d09b130":"#Random Forest Setup\nranfor = RandomForestClassifier()\nparameters = {'n_estimators':[10,50,100], 'random_state': [42, 138], \\\n              'max_features': ['auto', 'log2', 'sqrt']}\nranfor_clf = GridSearchCV(ranfor, parameters)\nranfor_clf.fit(X_train, y_train)\n\n\n'''CROSS VALIDATE'''\ncv_results = cross_validate(ranfor_clf, X_train, y_train)\ncv_results['test_score']  \n\ny_pred = ranfor_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred))","281e5322":"##Decision Tree Go\ndt = DecisionTreeClassifier()\nparameters = {'random_state': [42, 138],'max_features': ['auto', 'log2', 'sqrt']}\ndt_clf = GridSearchCV(dt, parameters)\ndt_clf.fit(X_train, y_train)\n\ny_pred = dt_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred))","aba15cbb":"ada = AdaBoostClassifier(base_estimator = DecisionTreeClassifier())\nparameters = {'n_estimators':[10,50,100], 'random_state': [42, 138], 'learning_rate': [0.1, 0.5, 0.8, 1.0]}\nada_clf = GridSearchCV(ada, parameters)\nada_clf.fit(X_train, y_train)\n\ncv_results = cross_validate(ada_clf, X_train, y_train)\ncv_results['test_score']  \n\ny_pred = ada_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred))","533e07f3":"gradBoost = GradientBoostingClassifier()\nparameters = {'n_estimators':[10,50,100], 'random_state': [42, 138], 'learning_rate': [0.1, 0.5, 0.8, 1.0], \\\n             'loss' : ['deviance', 'exponential']}\ngb_clf = GridSearchCV(gradBoost, parameters)\ngb_clf.fit(X_train, y_train)\n\ncv_results = cross_validate(gb_clf, X_train, y_train)\ncv_results['test_score']  \n\ny_pred = gb_clf.predict(X_test)\nprint(accuracy_score(y_test, y_pred))","694912a8":"xg = xgboost.XGBClassifier(max_depth = 3, n_estimators = 600, learning_rate = 0.05)\nxg.fit(X_train, y_train)\n\ncv_results = cross_validate(xg, X_train, y_train)\ncv_results['test_score']  \n\ny_pred = xg.predict(X_test)\nprint(accuracy_score(y_test, y_pred))","0c5c5465":"'''Confusion Matrix'''\ny_pred = xg.predict(X_test)\n# TN, FP, FN, TP\nconfusion_matrix(y_test, y_pred)","fb1ce208":"predictions = xg.predict(test)\n\nsub['Survived'] = predictions\nsub.to_csv(\"first_submission_xgb.csv\", index=False)","146ebefd":" Remove Unneccesary Features and Encode Categorical","bfc3ad8d":"## Feature Generation and Removal\n\nTime to generate a few features which may be of use in classification.","12c2cf65":"# Titanic Revisited (w\/ XGBoost)\n\n### Goal\nI originally worked with this dataset about 2.5 years ago when working through Udacity's Nanodegree in Data Analytics. I has, more or less, no idea what I was doing then. I thought it would be nice to revisit this dataset and see if I could get a better accuracy than my first time through (which was around 74% if I recall). Particularly, I've been wanting to work with XGBoost for awhile and this seems an appropriate classification problem to give it a go on!","99a8f62a":"Alright, first letsfirst try some more traditional models.. AKA: Random Forest and Standard Tree\n","7adcc040":"Lets send the test data through the same pipeline!","28d97f53":"## Quick EDA Visuals\nLets do just a little bit of EDA with Seaborn:","94a1ced4":"## Classifing with XGBoost\n\nI'll be comparing XG to AdaBoost, GradientBoost, RandomForest, andddd maybe a SVC or something else.\n\nFirst off to split the train data into a train\/test for cross validation testing","1889f116":"So.. A liiiittttleee better than the Gradient boost. I'm happy with it for a quick project like this.  Lets write it out and submit.","a2804775":"At 81% with a random forest.. I'm stoked as this is already better than my last attempt.  Lets keep pushing on with it and give some boosting models  a go.","940610e0":"![](http:\/\/)So GradientBoosting has given me the best score at 82% so far..  lets finally make our way to XG:"}}