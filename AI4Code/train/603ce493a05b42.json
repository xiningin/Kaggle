{"cell_type":{"64be18ad":"code","035ae16c":"code","1a719c8b":"code","ce0c19c4":"code","33034bde":"code","0c774911":"code","e7c5e5e3":"code","0aa12bf5":"code","994300f7":"code","1c9a1da9":"code","ac448c4e":"code","767747e3":"code","75b9b23a":"code","6edaaa25":"code","52e4c3fd":"code","c0205d76":"code","1165a58d":"code","07873f7e":"code","b294ec9a":"code","b2d33ee4":"code","425c642e":"code","e87598a9":"code","c4011bfc":"code","cc145bc1":"code","64097a9a":"code","86a1ce63":"code","96a2ce28":"code","9888863a":"code","abfda54e":"code","bf414d95":"code","483971b5":"code","4ec435b4":"code","818eaad5":"code","40a1f58e":"code","b43fe709":"code","96f51e60":"code","21653b7d":"code","be66b9ca":"code","479ba0b3":"code","bbba0c6c":"code","cb994a10":"code","4a8afa59":"code","8d44ba33":"code","6c3ad4ee":"code","562db53f":"code","a47ff942":"code","94408b03":"code","00c3f28b":"code","9c8cd876":"code","33583b6a":"code","041cc3e8":"code","4e38ead9":"code","d9e76ada":"code","da76d508":"markdown","373cf256":"markdown","0a88fa4c":"markdown","49e785c0":"markdown","4263f5d9":"markdown","51dd05f4":"markdown","53e146dc":"markdown","89a160d6":"markdown"},"source":{"64be18ad":"! conda install -c conda-forge gdcm -y","035ae16c":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport os\nIMG_FORMAT = \".dcm\"\nIMG_PATHS = []\nIMAGE_IDS = []\nIMAGE_NAMES = []\nSETS = []\nSERIES = []\nSTUDIES = []\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if filename.endswith(IMG_FORMAT):\n            img_path = os.path.join(dirname, filename)\n            Splitted = img_path.split('\/')\n            # print(Splitted)\n            img_name = os.path.basename(img_path)\n            img_id = img_name.rstrip(IMG_FORMAT)\n            series_name = Splitted[-2]\n            study_name = Splitted[-3]\n            set_name = Splitted[-4]\n            IMG_PATHS.append(img_path)\n            IMAGE_NAMES.append(img_name)\n            IMAGE_IDS.append(img_id)\n            SETS.append(set_name)\n            SERIES.append(series_name)\n            STUDIES.append(study_name)","1a719c8b":"df_ext = pd.DataFrame.from_dict({\"Image_Path\":IMG_PATHS,\n                             \"Image_Name\":IMAGE_NAMES,\n                             \"Image_ID\":IMAGE_IDS,\n                             \"Set_Name\": SETS,\n                             \"Series_Name\":SERIES,\n                             \"Study_Name\":STUDIES})\ndf_ext.head()","ce0c19c4":"img_lvl_pth = \"..\/input\/siim-covid19-detection\/train_image_level.csv\"\ndf_img = pd.read_csv(img_lvl_pth)\ndf_img.sort_values(by=['id'],inplace=True)\ndf_img.head()","33034bde":"df_img.isna().sum()","0c774911":"std_lvl_pth = \"..\/input\/siim-covid19-detection\/train_study_level.csv\"\ndf_std = pd.read_csv(std_lvl_pth)\ndf_std['id'] = df_std['id'].str.replace('_study',\"\")\ndf_std.rename({'id': 'StudyInstanceUID'},axis=1, inplace=True)\ndf_std.head(3)\n# df_std.sort_values(by=['StudyInstanceUID'],inplace=True)\ndf_std.head()","e7c5e5e3":"sub_pth = \"..\/input\/siim-covid19-detection\/sample_submission.csv\"\ndf_sub = pd.read_csv(sub_pth)\ndf_sub.head()","0aa12bf5":"df = df_img.merge(df_std, on='StudyInstanceUID')\ndf.head(3)","994300f7":"from copy import deepcopy\ndf_train = deepcopy(df)","1c9a1da9":"df_train.loc[df_train['Negative for Pneumonia']==1, 'study_label'] = 'negative'\ndf_train.loc[df_train['Typical Appearance']==1, 'study_label'] = 'typical'\ndf_train.loc[df_train['Indeterminate Appearance']==1, 'study_label'] = 'indeterminate'\ndf_train.loc[df_train['Atypical Appearance']==1, 'study_label'] = 'atypical'\ndf_train.drop(['Negative for Pneumonia','Typical Appearance', 'Indeterminate Appearance', 'Atypical Appearance'], axis=1, inplace=True)\ndf_train['id'] = df_train['id'].str.replace('_image', IMG_FORMAT)\n\ndef get_label(string):\n    return string.split()[0]\n\ndf_train['image_label'] = df_train['label'].map(get_label)\ndf_train.head(3)","ac448c4e":"df_train.columns","767747e3":"df_train[\"image_label\"].value_counts()","75b9b23a":"df_ext.head(3)","6edaaa25":"string = \"a29c5a68b07b\"\nstring.zfill(15)","52e4c3fd":"df_train.shape","c0205d76":"df_ext[\"id\"] = df_ext[\"Image_Name\"].str.zfill(19)\ndf_ext.shape","1165a58d":"df_ext.head(3)","07873f7e":"df_train.head(3)","b294ec9a":"df_ext[\"Set_Name\"].value_counts()","b2d33ee4":"df_test = df_ext[df_ext[\"Set_Name\"]==\"test\"]\ndf_test.head(3)","425c642e":"df_train.head(3)","e87598a9":"df_train_test = deepcopy(df_train)\nCOLS = list(df_train_test.columns)\ndef subtract_lists(x,y):\n    \"\"\"Subtract Two Lists (List Difference)\"\"\"\n    return [item for item in x if item not in y]\ndef merge_list_to_dict(test_keys,test_values):\n    \"\"\"Using dictionary comprehension to merge two lists to dictionary\"\"\"\n    merged_dict = {test_keys[i]: test_values[i] for i in range(len(test_keys))}\n    return merged_dict\n# NAN_COLS = subtract_lists(COLS,[\"id\"])\nTO_ATTACH = merge_list_to_dict(COLS,[np.nan]*len(COLS))\nfor index, row in df_test.iterrows():\n    TO_ATTACH[\"id\"] = row[\"id\"]\n    df_train_test = df_train_test.append(TO_ATTACH, ignore_index = True)\ndf_train_test.head(3)","c4011bfc":"df_train_test.tail(3)","cc145bc1":"df_train_test.shape","64097a9a":"df_train.shape","86a1ce63":"df_test.shape","96a2ce28":"df_train.shape[0] + df_test.shape[0]","9888863a":"df_train_test[\"id\"] = df_train_test[\"id\"].str.zfill(19)\ndf_train_test.head(3)","abfda54e":"df_ext[\"id\"] = df_ext[\"id\"].str.zfill(19)\ndf_ext.head(3)","bf414d95":"df_train_test.shape","483971b5":"df_ext.shape","4ec435b4":"df = df_ext.merge(df_train_test, on='id')\ndf.head(3)","818eaad5":"df[\"Set_Name\"].value_counts()","40a1f58e":"df.shape","b43fe709":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom glob import glob\nfrom tqdm.notebook import tqdm\nimport matplotlib.pyplot as plt\nfrom skimage import exposure\nimport cv2\nimport warnings\nwarnings.filterwarnings('ignore')\nimport shutil \nimport tensorflow as tf\n%matplotlib inline\n\n\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nimport pprint\nimport pydicom as dicom\nfrom pydicom.pixel_data_handlers.util import apply_voi_lut\nimport wandb\n\nimport PIL\nfrom PIL import Image\nfrom colorama import Fore, Back, Style\nviz_counter=0\n\ndef create_dir(dir, v=1):\n    \"\"\"\n    Creates a directory without throwing an error if directory already exists.\n    dir : The directory to be created.\n    v : Verbosity\n    \"\"\"\n    if not os.path.exists(dir):\n        os.makedirs(dir)\n        if v:\n            print(\"Created Directory : \", dir)\n        return 1\n    else:\n        if v:\n            print(\"Directory already existed : \", dir)\n        return 0\n\nvoi_lut=True\nfix_monochrome=True\n\ndef dicom_dataset_to_dict(filename):\n    \"\"\"Credit: https:\/\/github.com\/pydicom\/pydicom\/issues\/319\n               https:\/\/www.kaggle.com\/raddar\/convert-dicom-to-np-array-the-correct-way\n    \"\"\"\n    \n    dicom_header = dicom.dcmread(filename) \n    \n    #====== DICOM FILE DATA ======\n    dicom_dict = {}\n    repr(dicom_header)\n    for dicom_value in dicom_header.values():\n        if dicom_value.tag == (0x7fe0, 0x0010):\n            #discard pixel data\n            continue\n        if type(dicom_value.value) == dicom.dataset.Dataset:\n            dicom_dict[dicom_value.name] = dicom_dataset_to_dict(dicom_value.value)\n        else:\n            v = _convert_value(dicom_value.value)\n            dicom_dict[dicom_value.name] = v\n      \n    del dicom_dict['Pixel Representation']\n    \n    #====== DICOM IMAGE DATA ======\n    # VOI LUT (if available by DICOM device) is used to transform raw DICOM data to \"human-friendly\" view\n    if voi_lut:\n        data = apply_voi_lut(dicom_header.pixel_array, dicom_header)\n    else:\n        data = dicom_header.pixel_array\n    # depending on this value, X-ray may look inverted - fix that:\n    if fix_monochrome and dicom_header.PhotometricInterpretation == \"MONOCHROME1\":\n        data = np.amax(data) - data\n    data = data - np.min(data)\n    data = data \/ np.max(data)\n    modified_image_data = (data * 255).astype(np.uint8)\n    \n    return dicom_dict, modified_image_data\n\ndef _sanitise_unicode(s):\n    return s.replace(u\"\\u0000\", \"\").strip()\n\ndef _convert_value(v):\n    t = type(v)\n    if t in (list, int, float):\n        cv = v\n    elif t == str:\n        cv = _sanitise_unicode(v)\n    elif t == bytes:\n        s = v.decode('ascii', 'replace')\n        cv = _sanitise_unicode(s)\n    elif t == dicom.valuerep.DSfloat:\n        cv = float(v)\n    elif t == dicom.valuerep.IS:\n        cv = int(v)\n    else:\n        cv = repr(v)\n    return cv\n\n\nimport os, fnmatch\ndef find(pattern, path):\n    \"\"\"Utility to find files wrt a regex search\"\"\"\n    result = []\n    for root, dirs, files in os.walk(path):\n        for name in files:\n            if fnmatch.fnmatch(name, pattern):\n                result.append(os.path.join(root, name))\n    return result\ndef props(arr):\n    print(\"Shape :\",arr.shape,\"Maximum :\",arr.max(),\"Minimum :\",arr.min(),\"Data Type :\",arr.dtype)","96f51e60":"path = \"\/kaggle\/input\/siim-covid19-detection\/test\/00188a671292\/3eb5a506ccf3\/3dcdfc352a06.dcm\"\ndicom_dict, modified_image_data = dicom_dataset_to_dict(path)\nprops(modified_image_data)","21653b7d":"from tqdm import tqdm","be66b9ca":"\"\"\"\nShapes that you wish to resize to\n\"\"\"\n\nShape_X = 512\nShape_Y = 512\nimage_id = []\ndim0 = []\ndim1 = []\nsplits = []\nimg_paths = []\n\nfor split in ['test', 'train']:\n    # save_dir = f'\/kaggle\/tmp\/{split}\/'\n    save_dir = f'\/kaggle\/working\/resized_data\/{split}\/'\n    print(split)\n    os.makedirs(save_dir, exist_ok=True)\n    \n    for dirname, _, filenames in tqdm(os.walk(f'\/kaggle\/input\/siim-covid19-detection\/{split}')):\n        for file in filenames:\n            # set keep_ratio=True to have original aspect ratio\n            fpath = os.path.join(dirname, file)\n            dicom_dict, modified_image_data = dicom_dataset_to_dict(fpath)\n            res = cv2.resize(modified_image_data,(Shape_Y,Shape_X)) # cv2 has this opposite\n            save_path = os.path.join(save_dir, file.replace('dcm', 'png'))\n            cv2.imwrite(save_path,res)\n            img_id = file.replace('.dcm', '')\n            image_id.append(img_id)\n            dim0.append(modified_image_data.shape[0])\n            dim1.append(modified_image_data.shape[1])\n            img_paths.append(fpath)\n            splits.append(split)\n\"\"\"\n2475\/?\n12386\/?\n07:34 | 5.38it\/s\n36:51 | 8.13it\/s\n\"\"\"\nprint(\"Generation Complete!\")\n","479ba0b3":"import os\nimport zipfile\nimport shutil\n\n#taken from : https:\/\/www.kaggle.com\/xhlulu\/recursion-2019-load-resize-and-save-images\n\ndef zip_and_remove(path):\n    ziph = zipfile.ZipFile(f'{path}.zip', 'w', zipfile.ZIP_DEFLATED)\n    \n    for root, dirs, files in os.walk(path):\n        for file in files:\n            file_path = os.path.join(root, file)\n            ziph.write(file_path)\n            os.remove(file_path)\n    \n    ziph.close()\n    shutil.rmtree(path)\nsave_dir = 'resized_data'\nzip_and_remove(save_dir)","bbba0c6c":"new_df = pd.DataFrame.from_dict({'Image_Path': img_paths, 'dim0': dim0, 'dim1': dim1})\nnew_df.head(3)","cb994a10":"df.head(3)","4a8afa59":"df.shape","8d44ba33":"new_df.shape","6c3ad4ee":"final_df = df.merge(new_df,on=\"Image_Path\")\nfinal_df.head()","562db53f":"final_df.shape","a47ff942":"final_df.tail()","94408b03":"# import cv2\n# how to load a string to json\n# import ast\n# jsonobj = ast.literal_eval(str(hdf))\n\nfrom ast import literal_eval","00c3f28b":"n = len(final_df)\n# Already defined above : Shape_Y,Shape_X = 512,512\nNEW_BOXES = []\nfor i in range(n):\n    if type(final_df['boxes'][i])==str:\n        boxes = literal_eval(final_df['boxes'][i])\n        BIG_BOX = []\n        for box in boxes:\n            xbase,ybase = (box['x']*(Shape_Y\/final_df['dim1'][i]), box['y']*(Shape_X\/final_df['dim0'][i]))\n            new_width,new_height = box['width']*(Shape_Y\/final_df['dim1'][i]), box['height']*(Shape_X\/final_df['dim0'][i])\n            CURR_BOX = {\"x\": xbase,\n                        \"y\" : ybase,\n                        \"width\" : new_width,\n                        \"height\" : new_height}\n            BIG_BOX.append(CURR_BOX)\n    else:\n        BIG_BOX = \"\"\n    NEW_BOXES.append(str(BIG_BOX))","9c8cd876":"final_df['corrected_boxes'] = NEW_BOXES","33583b6a":"# Correction Factors\nfinal_df['cfy'] = Shape_Y\/final_df['dim1']\nfinal_df['cfx'] = Shape_X\/final_df['dim0']","041cc3e8":"!pip install openpyxl","4e38ead9":"final_df.to_csv('Extracted_Study_Series_Img.csv',index=False)\nfinal_df.to_excel('Extracted_Study_Series_Img.xlsx',index=False)","d9e76ada":"\"\"\"\nsubset_df = final_df[final_df[\"Set_Name\"]==\"train\"].sample(n=20,random_state=2021)\nsubset_df.head()\nfor path in subset_dcm_files:\n    dicom_dict, modified_image_data = dicom_dataset_to_dict(path)\n    res = cv2.bitwise_and(resized_image_data,resized_image_data,mask = pred_img_preprocessed)\n    fig, ax = plt.subplots(1, 3, figsize=(20, 12))\n    ax[0].imshow(resized_image_data, cmap=\"viridis\")\n    ax[0].axis('off')\n    ax[1].imshow(pred_img_preprocessed, cmap=\"viridis\")    \n    ax[1].axis('off')\n    ax[2].imshow(res, cmap=\"viridis\")    \n    ax[2].axis('off')\n    plt.savefig(str(viz_counter)+\".png\",dpi=300)\n    viz_counter+=1\n    cv2.imwrite(str(viz_counter)+\".png\",res)\n    viz_counter+=1\n    plt.show()\n\"\"\"","da76d508":"### Utility Functions","373cf256":"### Create a dummy df with NaN values for test ","0a88fa4c":"Incoming in the next Notebook!","49e785c0":"### Define your Required Reshape Size Here!\n\n### Get Original Image Shapes from Dicom Images","4263f5d9":"### Visualize","51dd05f4":"###  Rescaling Imgs : Bounding Box Rescaling Needed\n\nRemember that if you're trying to rescale images, the bounding boxes need to be reshaped as well!\n\n##### Note : The BBox have shifted due to resizing\n\nA rectangle defined via an anchor point xy and its width and height.\n\nThe rectangle extends from xy[0] to xy[0] + width in x-direction and from xy[1] to xy[1] + height in y-direction.\n\n```\n:                +------------------+\n:                |                  |\n:              height               |\n:                |                  |\n:               (xy)---- width -----+\n```","53e146dc":"### Fill df_train_test with nan data","89a160d6":"### Note that all images are stored in paths with the form `set`\/`study`\/`series`\/`image`\n\nWe will try to extract this information into a dataframe. This will be needed when inferencing on models and preparing the submission files!"}}