{"cell_type":{"1b393668":"code","32d6ed7a":"code","b9c4e97c":"code","d0d5cb8a":"code","0a4ecde7":"code","b7c736bf":"code","c7119289":"code","6bb9dd84":"code","1f34dfd0":"code","a1aee60e":"code","3e1a5823":"code","b1f6180d":"code","5f75f6c0":"code","f97bce06":"code","7dd91dfb":"code","2d66bd5f":"code","218998c7":"code","773718b8":"code","7c9c836e":"code","46f87307":"code","f9ded994":"code","be7472a6":"code","5f4b44f7":"code","4bc2dbbd":"code","600ebf42":"code","0810dd7c":"code","13da3ec4":"code","d01b6aae":"code","c4074f6e":"code","8d7ce657":"code","b4265fcb":"code","1db154b2":"code","04d5bd26":"code","c5821a26":"code","0765f0af":"code","d4995372":"code","ee1b24b6":"code","8a273fa6":"code","ba764e25":"code","81c89e8c":"code","ad16c677":"code","07e85658":"code","63797043":"code","bbcd6d46":"code","520d68f2":"code","e78fc8cf":"markdown","7f253e65":"markdown","5dd3058a":"markdown","812a9b4a":"markdown","876790f3":"markdown","f33ed752":"markdown","a29378eb":"markdown","be24f04f":"markdown","0ce18173":"markdown","5e34215a":"markdown","263c53c2":"markdown","35b2bdb8":"markdown","dafdaaa1":"markdown","c807b451":"markdown","ba77c66d":"markdown","6a13213f":"markdown","68cf432a":"markdown","68eae6db":"markdown","bdc2604e":"markdown","eecd4379":"markdown","cb11bb6c":"markdown","8091e19c":"markdown","1472afce":"markdown","1bef31f4":"markdown","315021a1":"markdown","81496351":"markdown","3a9a0eb4":"markdown","f1130d3e":"markdown"},"source":{"1b393668":"import pandas as pd\nimport numpy as np\nfrom pandas import ExcelWriter\nfrom pandas import ExcelFile\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn import preprocessing\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier # Import Decision Tree Classifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import metrics\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import ensemble\nfrom scipy.stats import spearmanr\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport copy\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\nimport scipy.stats as stats","32d6ed7a":"df_train = pd.read_excel('\/kaggle\/input\/sales-transactions-dataset\/Train.xlsx')\ndf_test = pd.read_excel('\/kaggle\/input\/sales-transactions-dataset\/Train.xlsx')\nprint(\"==\"*45)\nprint(\"The Training dataset has {0} rows and {1} columns\".format(df_train.shape[0], df_train.shape[1]))\nprint(\" The Testing dataset has {0} rows and {1} columns\".format(df_test.shape[0], df_test.shape[1]))\nprint(\"==\"*45)","b9c4e97c":"# Check the Head of the Data Set\ndf_train.head()","d0d5cb8a":"# Check the Head of the Data Set\ndf_test.head()","0a4ecde7":"# Check the Tail of the Data Set\ndf_train.tail()","b7c736bf":"df_train.dtypes,df_test.dtypes","c7119289":"# Check the Unique Values in the Data Set\nprint('Unique Records in Train Data: {}'.format(df_train.nunique()))\nprint('Unique Records in Test Data : {}'.format(df_test.nunique()))","6bb9dd84":"df_test.dtypes","1f34dfd0":"# Check the Data Types in the Data Set\nprint('Stas:{}'.format(df_train.describe()))\n\nprint('Stas:{}'.format(df_test.describe()))\n","a1aee60e":"# Check the Null Values in the Data sets\nprint('Null Values in Train Data: {}'.format(df_train.isnull().sum()))\nprint('Null Values in Test Data : {}'.format(df_test.isnull().sum()))\n","3e1a5823":"# Sum of Quantity and Total Sales Values by Grouping Suspicious\n\ndf_train.groupby('Suspicious').agg({'Quantity':'sum','TotalSalesValue':'sum'})\n","b1f6180d":"# 'Quantity' seems left skewed hence imputing it with median\ndf_train['Quantity'].fillna(df_train.TotalSalesValue.median(),inplace=True)","5f75f6c0":"# 'Total  Sales Value' seems left skewed hence imputing it with median\ndf_train['TotalSalesValue'].fillna(df_train.TotalSalesValue.median(),inplace=True)","f97bce06":"# Making NAN Values Where \n\n#df_train['TotalSalesValue']= df_train['TotalSalesValue'].where(df_train['TotalSalesValue']>250000)","7dd91dfb":"#df_train['Quantity']= df_train['Quantity'].where(df_train['Quantity']>50000)","2d66bd5f":"# Check the Null Values in the Data sets\nprint('Null Values in Train Data: {}'.format(df_train.isnull().sum()))\nprint('Null Values in Test Data : {}'.format(df_test.isnull().sum()))\n","218998c7":"# Since data in 'LoanAmount' column seems right skewed hence imputing it with median seems good option.\ndf_train['Quantity'].fillna(df_train.Quantity.median(),inplace=True)\ndf_train['TotalSalesValue'].fillna(df_train.TotalSalesValue.median(),inplace=True)","773718b8":"# Check the Null Values in the Data sets\nprint('Null Values in Train Data: {}'.format(df_train.isnull().sum()))\nprint('Null Values in Test Data : {}'.format(df_test.isnull().sum()))\n","7c9c836e":"df_train.insert(4,\"Price\",df_train['TotalSalesValue']\/df_train['Quantity'])\ndf_train.head()","46f87307":"df_test.insert(4,\"Price\",df_test['TotalSalesValue']\/df_test['Quantity'])\ndf_test.head()","f9ded994":"# Converted Target Variable \"Suspicious\" to Numeric\n\ndf_train = df_train.replace(to_replace =\"Yes\",value =1) \ndf_train = df_train.replace(to_replace =\"No\",value =2) \ndf_train = df_train.replace(to_replace =\"indeterminate\",value =3)\n\ndf_train.head()","be7472a6":"# To Check the Suspicious in Interger Type\ndf_train.dtypes","5f4b44f7":"# in the Above we can see that the Suspicious Class in Object Type so we need convert that in to integer\n#df_train['Suspicious'] = df_train.Suspicious.astype(int)","4bc2dbbd":"# '''To Remove white Space Using strip leading and trailing space'''\ndf_train['ProductID'] = df_train['ProductID'].str.strip()\nprint (df_train.head(10))","600ebf42":"## label_encoder object knows how to understand word labels.\n\nfrom sklearn.preprocessing import LabelEncoder\n\nlabel_encoder = preprocessing.LabelEncoder() \n  \ndf_train['ProductID']= label_encoder.fit_transform(df_train['ProductID'])\ndf_train['SalesPersonID']= label_encoder.fit_transform(df_train['SalesPersonID'])\ndf_test['ProductID']= label_encoder.fit_transform(df_test['ProductID'])\ndf_test['SalesPersonID']= label_encoder.fit_transform(df_test['SalesPersonID'])\n#df_train['ProductID'].unique()\n ","0810dd7c":"suspicious = df_train['Suspicious']\n\nmean=suspicious.mean()\nmedian=suspicious.median()\nmode=suspicious.mode()\n\nprint('Mean: ',mean,'\\nMedian: ',median,'\\nMode: ',mode[0])\nplt.figure(figsize=(10,5))\nplt.hist(suspicious,bins=100,color='grey')\nplt.axvline(mean,color='red',label='Mean')\nplt.axvline(median,color='yellow',label='Median')\nplt.axvline(mode[0],color='green',label='Mode')\nplt.xlabel('suspicious')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()","13da3ec4":"quantity = df_train['Quantity']\n\nmean=quantity.mean()\nmedian=quantity.median()\nmode=quantity.mode()\n\nprint('Mean: ',mean,'\\nMedian: ',median,'\\nMode: ',mode[0])\nplt.figure(figsize=(10,5))\nplt.hist(quantity,bins=100,color='grey')\nplt.axvline(mean,color='red',label='Mean')\nplt.axvline(median,color='yellow',label='Median')\nplt.axvline(mode[0],color='green',label='Mode')\nplt.xlabel('quantity')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()","d01b6aae":"sales = df_train['TotalSalesValue']\n\nmean=sales.mean()\nmedian=sales.median()\nmode=sales.mode()\n\nprint('Mean: ',mean,'\\nMedian: ',median,'\\nMode: ',mode[0])\nplt.figure(figsize=(10,5))\nplt.hist(sales,bins=100,color='grey')\nplt.axvline(mean,color='red',label='Mean')\nplt.axvline(median,color='yellow',label='Median')\nplt.axvline(mode[0],color='green',label='Mode')\nplt.xlabel('sales')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()","c4074f6e":"df_train['Quantity'].hist(bins=1000)","8d7ce657":"df_train['TotalSalesValue'].hist(bins=1000)","b4265fcb":"df_train.head(100)","1db154b2":"# Log Transformation of Skewed Data\n\ndf_train['Quantity']= np.log10(df_train['Quantity'])\ndf_train['TotalSalesValue']= np.log10(df_train['TotalSalesValue'])\n\n#print(df_train(10))\n","04d5bd26":"sales = df_train['TotalSalesValue']\n\nmean=sales.mean()\nmedian=sales.median()\nmode=sales.mode()\n\nprint('Mean: ',mean,'\\nMedian: ',median,'\\nMode: ',mode[0])\nplt.figure(figsize=(10,5))\nplt.hist(sales,bins=100,color='grey')\nplt.axvline(mean,color='red',label='Mean')\nplt.axvline(median,color='yellow',label='Median')\nplt.axvline(mode[0],color='green',label='Mode')\nplt.xlabel('sales')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()","c5821a26":"quantity = df_train['Quantity']\n\nmean=quantity.mean()\nmedian=quantity.median()\nmode=quantity.mode()\n\nprint('Mean: ',mean,'\\nMedian: ',median,'\\nMode: ',mode[0])\nplt.figure(figsize=(10,5))\nplt.hist(quantity,bins=100,color='grey')\nplt.axvline(mean,color='red',label='Mean')\nplt.axvline(median,color='yellow',label='Median')\nplt.axvline(mode[0],color='green',label='Mode')\nplt.xlabel('quantity')\nplt.ylabel('Frequency')\nplt.legend()\nplt.show()","0765f0af":"# Delete the Unique Records which matches to rows \n\ndf_train = df_train.drop(['ReportID'], axis=1)\n\ndf_train.dtypes","d4995372":"# skewness along the index axis \ndf_train.skew(axis = 0, skipna = True) ","ee1b24b6":"# skewness along the index axis \ndf_train.skew(axis = 0, skipna = True) ","8a273fa6":"# Separate Target \"Suspicious\" as y and others as x to split the data set\n\nX = df_train.loc[:, df_train.columns != 'Suspicious']\ny = df_train.loc[:, df_train.columns == 'Suspicious']","ba764e25":"seed = 2\ntest_size = 0.2\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size, random_state=seed)","81c89e8c":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import classification_report\n\ndtc = DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=100,\n            max_features=None, max_leaf_nodes=50, min_samples_leaf=10,\n            min_samples_split=4, min_weight_fraction_leaf=0.0,\n            presort=False, random_state=42, splitter='random')","ad16c677":"dtc.fit(X_train,y_train)\n\ny_dtc = dtc.predict(X_test)","07e85658":"print(classification_report(y_test,y_dtc))","63797043":"#Create a Random Forest Classifier\nRFC=RandomForestClassifier(n_estimators=1000)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nRFC=RFC.fit(X_train, y_train)\n\nRFC_Pred=RFC.predict(X_test)","bbcd6d46":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\n\nAccuracy_Score = accuracy_score(y_test, RFC_Pred)\nPrecision_Score = precision_score(y_test, RFC_Pred,  average=\"macro\")\nRecall_Score = recall_score(y_test, RFC_Pred,  average=\"macro\")\nF1_Score = f1_score(y_test, RFC_Pred,  average=\"macro\")\n\nprint('Average Accuracy: %0.2f +\/- (%0.1f) %%' % (Accuracy_Score.mean()*100, Accuracy_Score.std()*100))\nprint('Average Precision: %0.2f +\/- (%0.1f) %%' % (Precision_Score.mean()*100, Precision_Score.std()*100))\nprint('Average Recall: %0.2f +\/- (%0.1f) %%' % (Recall_Score.mean()*100, Recall_Score.std()*100))\nprint('Average F1-Score: %0.2f +\/- (%0.1f) %%' % (F1_Score.mean()*100, F1_Score.std()*100))\n\nCM = confusion_matrix(y_test, RFC_Pred)","520d68f2":"print(classification_report(y_test, RFC_Pred))","e78fc8cf":"This shows some descriptive statistics on the data set. Notice, it only shows the statistics on the numerical columns. From here you can see the following statistics:\n\nRow count, which aligns to what the shape attribute showed us.\n\n* The mean, or average.\n* The standard deviation, or how spread out the data is.\n* The minimum and maximum value of each column\n* The number of items that fall within the first, second, and third percentiles.","7f253e65":"From the above heatmap we can conlude that we are not missing any information.","5dd3058a":"##### Check Types Data in the Train Data","812a9b4a":"##### Check the Head of the Train Data Set","876790f3":"#### Split the Train_Data in to Train and Test","f33ed752":"##### Check the Tail of the Train Data Set","a29378eb":"##### Check the Head of the Test Data Set","be24f04f":"##### Checking the Proporation of Suspicious Variable Using Pie Chart with Percentage","0ce18173":"##### Check Types Data in the Test Data","5e34215a":"#### Log Transformation of Skewed Data","263c53c2":"#### Read and Exploratory Analysis","35b2bdb8":"##### Check the Unique Values in the Train and Test Data Set","dafdaaa1":"### The Problem Statement :\n#### To Classify Each Report ID as Yes (Suspicious) \/ No (Not Suspicious) \/ Indeterminate (Doubtful).","c807b451":"<h1 align='center' style='color:purple'>Suspicious Prediction on the Transaction Data<\/h1>","ba77c66d":"#### Now Try applying Data Transformation, Binning and Sampling Techniques like Oversampling, Undersampling like SMote, Adasyn and RandomUndersampling Techniques ","6a13213f":"### Adding New Column \"Price\" to Check the Unit Price","68cf432a":"#### SMOTE (Synthetic Minority Over-sampling Technique)","68eae6db":"#### 2. Random Forest Model","bdc2604e":"Based on the above Statement, we have been given a Transaction Data of Retailer Store of their daily sale of Products.\nThe Management has given the authorization to sell the products in a certain range of price to all the Sales Persons and\neach  product has been identified as a Unique Product ID, which would help them to identify, what are all the products\nare selling High\/Medium\/Low to check Profits each Particular Product ID, \n\nThey have given Sales Person ID to each Sales Person, which helps them to identify who are all doing their job in best or satisfactory or bad and it avoids bias towards their promotions and bonuses and other employee benefits.\n\n In this nature of transactions, they can also identify errors and fraudulent activities done by the sales persons so that\n they can classify the Negative sales Transaction of the sold goods to take further actions by the Management.\n \n Errors : Software\/Technical Errors\/ Data Entry error \/ Product ID Lable Coding Error et...                                 \n Fraud  : Intentionally Sales Persons are involved in this kind of activities.\n","eecd4379":"### Visualisation \n1. Pie Plot  \n2. Bar Plot\n3. Histogram\n4. Scattor Plot\n5. Line Plot","cb11bb6c":"##### Describe Stat Summary of the Train and Test Data ","8091e19c":"#### Import Necessary Libraries ","1472afce":"##### Visualisation Null Values Using Heatmap","1bef31f4":"##### Separate Target \"Suspicious\" as y and others as x to split the data set","315021a1":"#### 1. Decision Tree ","81496351":"### Building Decision Tree and Random Forest Models","3a9a0eb4":"##### Sum of Quantity and Total Sales Values by Grouping Suspicious","f1130d3e":"##### Check the Null Values in the Train and Test Data sets"}}