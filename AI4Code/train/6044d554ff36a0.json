{"cell_type":{"28cdf3b7":"code","52af58bd":"code","4a6faaf4":"code","e353ab15":"code","7b0ea0c9":"code","ab384109":"code","26a8031d":"code","d219c7ec":"code","a1faad3e":"code","c8c46e99":"code","4fbec0f9":"code","172f3a2a":"code","3d591574":"code","9a849650":"code","23c956e0":"code","1a185bfd":"code","35420223":"code","139ee398":"code","5317c1ca":"code","ab34a51e":"markdown","3114b47f":"markdown","35a5bd01":"markdown"},"source":{"28cdf3b7":"import os\nimport math\nfrom tqdm import tqdm\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.model_selection import train_test_split","52af58bd":"train_df = pd.read_csv(\"..\/input\/ndsc-beginner\/train.csv\")\ntrain_df = train_df.sample(frac=1.)\nval_df = train_df[:1000]\ntrain_df = train_df[1000:]\nval_df.head()","4a6faaf4":"# Embdedding setup, save it in a dictionary for easier queries\nembeddings_index = {}\nf = open('..\/input\/tutorial-how-to-train-your-custom-word-embedding\/custom_glove_100d.txt')\nfor line in tqdm(f):\n    values = line.split(\" \")\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float32')\n    embeddings_index[word] = coefs\nf.close()\n\nprint('Found %s word vectors.' % len(embeddings_index))","e353ab15":"# Convert values to embeddings\ndef text_to_array(text):\n    empyt_emb = np.zeros(100)\n    text = text[:-1].split()[:100]\n    embeds = [embeddings_index.get(x, empyt_emb) for x in text]\n    embeds+= [empyt_emb] * (100 - len(embeds))\n    return np.array(embeds)","7b0ea0c9":"val_vects = np.array([text_to_array(X_text) for X_text in (val_df[\"title\"][:])])\nval_y_labels = np.array(val_df[\"Category\"])\n# val_y = np.zeros((len(val_y_labels), 58))\n# val_y[np.arange(len(val_y_labels)), val_y_labels] = 1","ab384109":"# Understand what a batch is made of\nbatch_size = 128\ni = 0\ntexts = train_df.iloc[i*batch_size:(i+1)*batch_size, 1]\ntext_arr = np.array([text_to_array(text) for text in texts])\nbatch_labels = np.array(train_df[\"Category\"][i*batch_size:(i+1)*batch_size])\nbatch_targets = np.zeros((batch_size, 58))\nbatch_targets[np.arange(batch_size), batch_labels] = 1\nprint(np.shape(text_arr))\nprint(np.shape(batch_targets))\nprint(text_arr)\nprint(batch_targets)","26a8031d":"# Write generator, which \nbatch_size = 128\n\ndef batch_gen(train_df):\n    n_batches = math.floor(len(train_df) \/ batch_size)\n    while True: \n        train_df = train_df.sample(frac=1.)  # Shuffle the data.\n        for i in range(n_batches):\n            texts = train_df.iloc[i*batch_size:(i+1)*batch_size, 1]\n            text_arr = np.array([text_to_array(text) for text in texts])\n            batch_labels = np.array(train_df[\"Category\"][i*batch_size:(i+1)*batch_size])\n            yield text_arr, batch_labels","d219c7ec":"from keras.models import Sequential\nfrom keras.layers import CuDNNLSTM, Dense, Bidirectional, Activation, Dropout","a1faad3e":"import matplotlib.pyplot as plt\n\ndef plot_history(history):\n    plt.figure(figsize=(12,6))\n    plt.plot(history.history[\"loss\"], color=\"purple\")\n    plt.plot(history.history[\"acc\"], color=\"blue\")\n    plt.plot(history.history[\"val_loss\"], color=\"red\")\n    plt.plot(history.history[\"val_acc\"], color=\"green\")\n    plt.xlim(0,)\n    plt.ylim(0.5,1.5)\n    plt.legend(['loss', 'acc', \"val_loss\", \"val_acc\"], loc='upper right')\n    plt.show()","c8c46e99":"model = Sequential()\nmodel.add(Bidirectional(CuDNNLSTM(128, return_sequences=True),\n                        input_shape=(100, 100)))\nmodel.add(Dropout(0.05))\nmodel.add(Bidirectional(CuDNNLSTM(128)))\nmodel.add(Dense(58))\nmodel.add(Activation('softmax'))\n\nmodel.compile(loss='sparse_categorical_crossentropy',\n              optimizer='adam',\n              metrics=['accuracy'])","4fbec0f9":"mg = batch_gen(train_df)\nhistory = model.fit_generator(mg, epochs=90,\n                    steps_per_epoch=1000,\n                    validation_data=(val_vects, val_y_labels),\n                    verbose=True)","172f3a2a":"plot_history(history)","3d591574":"history = model.fit_generator(mg, epochs=1,\n                    steps_per_epoch=1000,\n                    validation_data=(val_vects, val_y_labels),\n                    verbose=True)","9a849650":"# Make the prediction from the model\nbatch_size = 256\ndef batch_gen(test_df):\n    n_batches = math.ceil(len(test_df) \/ batch_size)\n    for i in range(n_batches):\n        texts = test_df.iloc[i*batch_size:(i+1)*batch_size, 1]\n        text_arr = np.array([text_to_array(text) for text in texts])\n        yield text_arr\n\ntest_df = pd.read_csv(\"..\/input\/ndsc-beginner\/test.csv\")\ntest_df[\"Supercategory\"] = test_df[\"image_path\"].str[0]\nsupercats = np.array(test_df[\"Supercategory\"])\nsupercat_dict = {\n    \"b\" : np.array([1]*17 + [0]*14 + [0]*27),\n    \"f\" : np.array([0]*17 + [1]*14 + [0]*27),\n    \"m\" : np.array([0]*17 + [0]*14 + [1]*27)\n}\n\nall_preds = []\nfor x in tqdm(batch_gen(test_df)):\n    all_preds.extend(model.predict(x))","23c956e0":"import matplotlib.pyplot as plt\nplt.plot(all_preds[0])\nplt.plot(all_preds[int(len(all_preds)\/2)])\nplt.plot(all_preds[-1])\nplt.yscale(\"log\")\nplt.show()","1a185bfd":"print(np.shape(all_preds))\ny_te = [np.argmax(pred) for pred,supercat in zip(all_preds,supercats)]","35420223":"submit_df = pd.DataFrame({\"itemid\": test_df[\"itemid\"], \"Category\": y_te})\nsubmit_df.to_csv(\"submission.csv\", index=False)","139ee398":"submit_df.head()","5317c1ca":"submit_df.tail()","ab34a51e":"# Training","3114b47f":"# Inference","35a5bd01":"# Setup"}}