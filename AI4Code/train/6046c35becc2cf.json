{"cell_type":{"e4e608a7":"code","d52e37c7":"code","f5b83e4d":"code","ca604573":"code","296ee702":"code","fd9b4e39":"code","20b70af0":"code","b75186ea":"code","20ffc02c":"code","42c8d1cc":"code","c385067b":"code","1088fb01":"code","003bc147":"code","4b9cf3c9":"code","9b3302b1":"code","9b61b843":"code","c51d9026":"code","030c09a6":"code","5015c614":"code","c5dc632b":"code","467c3e94":"code","d8bf9d53":"code","d9918f52":"code","d9f966ba":"code","e68577cf":"code","e1b52af3":"code","1923b5a2":"code","bc361bf8":"code","b477a7a5":"code","e9fd75e4":"code","365ed5b5":"code","131f765d":"code","174d05c9":"code","9d7fa7cc":"code","0d8e75f1":"code","def22def":"code","c2510b08":"code","6f3113dc":"code","1de5168d":"code","33b1f359":"code","637d933d":"code","71ebc086":"code","2fce38a7":"code","b1862b8d":"code","c861c461":"code","68237cf8":"code","55b4325c":"code","689d1d5c":"code","e45f765c":"code","be553e6b":"code","1bca44a8":"code","d5da458f":"code","786e5cc9":"code","33f16bae":"code","4844cff2":"code","8acf4fc9":"code","44659572":"code","d642b2ae":"code","8f4b5fe7":"code","5b901903":"code","ed6833a9":"code","046bf5c8":"code","da51d0f1":"code","8e9e006f":"code","73cbd587":"code","f8132362":"code","93f7566b":"code","41b24b4c":"code","fb7f82ab":"code","2c98980b":"code","c85f5977":"code","c98700c3":"code","4a50b7cc":"code","c9f98f08":"code","afc8ec6a":"code","ecc4caa9":"code","18c2b48e":"code","f8636103":"code","42b697ab":"code","61ddc2b3":"code","848fcb2d":"code","83b07237":"code","e8cc38c6":"code","8b0a362a":"code","be2f9341":"code","058cee26":"code","005cb851":"code","d6fb8df9":"code","79be71ca":"code","4def1222":"code","b6e78d23":"code","b01bcc88":"code","8804b32b":"code","facb3a3b":"code","d18a8876":"code","19b737d3":"code","01a99260":"code","27e532dd":"code","4731adeb":"code","69d96fdc":"code","ef1295bb":"code","f83376cc":"code","c6fa1ecb":"code","7872c18f":"code","6791abe5":"code","975ee1ff":"code","c540bb91":"code","01e62caa":"code","881fa92d":"code","a5b1dce0":"code","02ab2d87":"code","9030f468":"code","4eb14a8e":"code","b98ff57f":"code","3b7317d9":"code","2b3c83c1":"code","d730068b":"code","4c27c88a":"code","99dfe799":"code","c50a3d4f":"code","ef95d8d7":"code","70744f39":"code","cb0a1dee":"code","2d9d69bd":"code","5c0ecbd0":"code","3446036f":"code","92e8343f":"code","db855b4a":"code","5af71ad6":"code","515b1acf":"code","ad94fe4c":"code","39b47f23":"code","c4b5cb42":"code","1e617986":"code","baaeaca9":"code","3d5d27b8":"code","b6dc7319":"code","2b5c94bc":"code","fb7a8583":"code","d44cf491":"code","11a4ff19":"markdown","3031b5d5":"markdown","9a83e3ff":"markdown","1e15a515":"markdown","bbf9868c":"markdown","58f6ef68":"markdown","a71ad20e":"markdown","e5b23cac":"markdown","065afd27":"markdown","dae043bf":"markdown","63097c7f":"markdown","ecc0e561":"markdown","cae4f3a4":"markdown","fc94093d":"markdown","ba09a4e1":"markdown","ced65778":"markdown","959e27fc":"markdown","cad74a4a":"markdown","d507c6c6":"markdown","d5e601a0":"markdown","a43d3228":"markdown","45c95aa5":"markdown","3fb39705":"markdown","30788af3":"markdown","c9a28609":"markdown","b3484bd0":"markdown","5c8aca14":"markdown"},"source":{"e4e608a7":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n#flags for debugging and experimentations\nKEEP_OUTLIERS=True\nKEEP_ZERO_VALUES=True\nEXIT_FOR_DEBUG=False\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","d52e37c7":"train = pd.read_csv(\"..\/input\/learn-together\/train.csv\")\ntest = pd.read_csv(\"..\/input\/learn-together\/test.csv\")","f5b83e4d":"train.head()","ca604573":"train.shape,test.shape","296ee702":"#Keep backup of orignal data\ntrain_orig,test_orig=train,test\ntrain_orig.shape,test_orig.shape,train.shape,test.shape","fd9b4e39":"train.dtypes","20b70af0":"test.dtypes","b75186ea":"test.head()","20ffc02c":"train.describe()","42c8d1cc":"test.describe()","c385067b":"#print Count of Unique Values in each column\nfor column in list(train.columns):\n    print (\"{0:25} {1}\".format(column, train[column].nunique()))","1088fb01":"# Columns Identified which can be dropped for model\ncolumns_to_drop=['Soil_Type7','Soil_Type15']\ncolumns_to_drop","003bc147":"#print Count of Unique Values in each column\nfor column in list(test.columns):\n    print (\"{0:25} {1}\".format(column, test[column].nunique()))","4b9cf3c9":"#Change data type of Continous variable columns to float\ncontinuos_columns={'Elevation':float,'Aspect':float,'Slope':float,'Horizontal_Distance_To_Hydrology':float,'Vertical_Distance_To_Hydrology':float,\n                   'Horizontal_Distance_To_Roadways':float,'Hillshade_9am':float,'Hillshade_Noon':float,'Hillshade_3pm':float,'Horizontal_Distance_To_Fire_Points':float}\ntrain=train.astype(continuos_columns)\ncontinuos_column_list=list(continuos_columns.keys())\ntrain[continuos_column_list].dtypes","9b3302b1":"test=test.astype(continuos_columns)\ntest[continuos_column_list].dtypes","9b61b843":"train_test_continuous_values=pd.DataFrame(train[continuos_column_list].nunique() ,columns=['train']).join(pd.DataFrame(test[continuos_column_list].nunique() ,columns=['test']) )\n    \ntrain_test_continuous_values","c51d9026":"\ntrain_test_continuous_values.plot(kind='barh',figsize=(15,6))","030c09a6":"train = train.drop([\"Id\"], axis = 1)\n\ntest_ids = test[\"Id\"]\n\ntest = test.drop([\"Id\"], axis = 1)","5015c614":"#Check if there is any missing\/null values in dataset\ntrain.isnull().values.all()","c5dc632b":"test.isnull().values.all()","467c3e94":"Cover_Type_Name=['Spruce_Fir','Lodgepole_Pine','Ponderosa_Pine','Cottonwood_Willow','Aspen','Douglas_fir','Krummholz']\nCover_Type_Name","d8bf9d53":"#Uniqe Cover_Type values , will be used for chart , Soil Type by Cover Type\nCover_Type=train['Cover_Type'].unique()\nCover_Type=sorted(Cover_Type)\nCover_Type_Dict=dict(zip(Cover_Type,Cover_Type_Name))\nCover_Type_Dict","d9918f52":"#Create a new Column having Name of Cover Type\ntrain['Cover_Type_Name'] = train['Cover_Type'].map(Cover_Type_Dict)\ntrain[['Cover_Type','Cover_Type_Name']].head()","d9f966ba":"\nWilderness_Area_Columns=[\"Wilderness_Area\" + str(i) for i in range(1,5)]\nWilderness_Area_Columns","e68577cf":"Wilderness_Area_Names=['Rawah_Wilderness','Neota_Wilderness','Comanche_Peak_Wilderness','Cache_la_Poudre_Wilderness']\nWilderness_Area_Names","e1b52af3":"# Wilderness_Area_Columns\ntrain['Wilderness_Area']=train[Wilderness_Area_Columns].idxmax(axis=1)\ntrain.head()","1923b5a2":"#Uniqe Wilderness_Area values , will be used later\nWilderness_Area=train['Wilderness_Area'].unique()\nWilderness_Area=sorted(Wilderness_Area)\nWilderness_Area_Dict=dict(zip(Wilderness_Area,Wilderness_Area_Names))\nWilderness_Area_Dict","bc361bf8":"#Create a new Column having Name of Wilderness_Area\ntrain['Wilderness_Area_Name'] = train['Wilderness_Area'].map(Wilderness_Area_Dict)\ntrain[['Wilderness_Area','Wilderness_Area_Name']].head()","b477a7a5":"Soil_Type_Columns=[\"Soil_Type\" +str(i) for i in range(1,41)]\nprint(Soil_Type_Columns)","e9fd75e4":"#Dichotomous\/Boolean Variables\/columns\nBoolean_Columns=Soil_Type_Columns + Wilderness_Area_Columns\nprint(np.shape(Boolean_Columns))\nprint(Boolean_Columns)","365ed5b5":"#Create a new Column Soil Type, will convert all 40 columns into one column \"Reverse One Hot Encoding\"\ntrain['Soil_Type']=train[Soil_Type_Columns].idxmax(axis=1)","131f765d":"#Just check is Reverse One Hot coding done properly , check values of first 9 Soil type columns\n#train[['Soil_Type'] + Soil_Type_Columns].loc[ train['Soil_Type']==\"Soil_Type1\",:]\n#search_values=[\"Soil_Type\" +str(i) for i in range(1,9)]\n#train[['Soil_Type'] + Soil_Type_Columns].loc[ train['Soil_Type'].isin(search_values),:]","174d05c9":"test['Soil_Type']=test[Soil_Type_Columns].idxmax(axis=1)","9d7fa7cc":"test.head()","0d8e75f1":"Soil_Type_Names=[\"Cathedral_family_-_Rock_outcrop_complex_extremely_stony\",\n\"Vanet_-_Ratake_families_complex_very_stony\",\n\"Haploborolis_-_Rock_outcrop_complex_rubbly\",\n\"Ratake_family_-_Rock_outcrop_complex_rubbly\",\n\"Vanet_family_-_Rock_outcrop_complex_complex_rubbly\",\n\"Vanet_-_Wetmore_families_-_Rock_outcrop_complex_stony\",\n\"Gothic_family\",\n\"Supervisor_-_Limber_families_complex\",\n\"Troutville_family_very_stony\",\n\"Bullwark_-_Catamount_families_-_Rock_outcrop_complex_rubbly\",\n\"Bullwark_-_Catamount_families_-_Rock_land_complex_rubbly\",\n\"Legault_family_-_Rock_land_complex_stony\",\n\"Catamount_family_-_Rock_land_-_Bullwark_family_complex_rubbly\",\n\"Pachic_Argiborolis_-_Aquolis_complex\",\n\"unspecified_in_the_USFS_Soil_and_ELU_Survey\",\n\"Cryaquolis_-_Cryoborolis_complex\",\n\"Gateview_family_-_Cryaquolis_complex\",\n\"Rogert_family_very_stony\",\n\"Typic_Cryaquolis_-_Borohemists_complex\",\n\"Typic_Cryaquepts_-_Typic_Cryaquolls_complex\",\n\"Typic_Cryaquolls_-_Leighcan_family_till_substratum_complex\",\n\"Leighcan_family_till_substratum_extremely_bouldery\",\n\"Leighcan_family_till_substratum_-_Typic_Cryaquolls_complex\",\n\"Leighcan_family_extremely_stony\",\n\"Leighcan_family_warm_extremely_stony\",\n\"Granile_-_Catamount_families_complex_very_stony\",\n\"Leighcan_family_warm_-_Rock_outcrop_complex_extremely_stony\",\n\"Leighcan_family_-_Rock_outcrop_complex_extremely_stony\",\n\"Como_-_Legault_families_complex_extremely_stony\",\n\"Como_family_-_Rock_land_-_Legault_family_complex_extremely_stony\",\n\"Leighcan_-_Catamount_families_complex_extremely_stony\",\n\"Catamount_family_-_Rock_outcrop_-_Leighcan_family_complex_extremely_stony\",\n\"Leighcan_-_Catamount_families_-_Rock_outcrop_complex_extremely_stony\",\n\"Cryorthents_-_Rock_land_complex_extremely_stony\",\n\"Cryumbrepts_-_Rock_outcrop_-_Cryaquepts_complex\",\n\"Bross_family_-_Rock_land_-_Cryumbrepts_complex_extremely_stony\",\n\"Rock_outcrop_-_Cryumbrepts_-_Cryorthents_complex_extremely_stony\",\n\"Leighcan_-_Moran_families_-_Cryaquolls_complex_extremely_stony\",\n\"Moran_family_-_Cryorthents_-_Leighcan_family_complex_extremely_stony\",\n\"Moran_family_-_Cryorthents_-_Rock_land_complex_extremely_stony\"\n]\nprint(Soil_Type_Names)","def22def":"\n# Soil Type Name have hidden features like Stone Level, Is Complex, Is Rubbly.. lets extract these features\nsoil_feature_list=dict()\n\ni=0\nfor Soil_Type in Soil_Type_Names:\n    stony_level=0\n    is_complex=0\n    is_rubbly=0\n    if Soil_Type.find(\"stony\")!=-1:\n        stony_level=1\n        if Soil_Type.find(\"very\")!=-1:\n            stony_level=2\n        if Soil_Type.find(\"extremely\")!=-1:\n            stony_level=3\n    if Soil_Type.find(\"rubbly\")!=-1:\n         is_rubbly=1\n    if Soil_Type.find(\"complex\")!=-1:\n         is_complex=1\n    soil_quality=(stony_level,is_complex,is_rubbly)\n    #print(Soil_Type_Columns[i],soil_quality)\n    soil_feature_list[Soil_Type_Columns[i]]=soil_quality\n    i=i+1\n            \nsoil_feature_list","c2510b08":"def get_soil_features(row):\n    soil_type=row[\"Soil_Type\"]\n    soil_features=soil_feature_list[soil_type]\n    # Soil Quality columns will have composition of all three soild related column\n    soil_quality=1000 + (soil_features[0]*100) + (soil_features[1]*10) + soil_features[2] \n\n    return pd.Series({'0':soil_features[0],'1':soil_features[1],'2':soil_features[2],'3':soil_quality})","6f3113dc":"\ntrain[['Soil_Stony_Level',\"Soil_Complex\",\"Soil_Rubbly\",\"Soil_Quality\"]]=train.apply(lambda row:get_soil_features(row),axis=1)\n\n#Convert Soil_Stony_Level to OHE columns , Lets keep both as stone level categorial can also acts as numeric column\nsoil_stony_OHE_cols=pd.get_dummies(train['Soil_Stony_Level'])\nsoil_stony_OHE_cols_names=[\"Soil_Stony_Level_\" + str(colname) for colname in  soil_stony_OHE_cols.columns ]\nsoil_stony_OHE_cols.columns=soil_stony_OHE_cols_names\ntrain=train.join(soil_stony_OHE_cols)","1de5168d":"train.head()","33b1f359":"test[['Soil_Stony_Level',\"Soil_Complex\",\"Soil_Rubbly\",\"Soil_Quality\"]]=test.apply(lambda row:get_soil_features(row),axis=1)\nsoil_stony_OHE_cols=pd.get_dummies(test['Soil_Stony_Level'])\nsoil_stony_OHE_cols_names=[\"Soil_Stony_Level_\" + str(colname) for colname in  soil_stony_OHE_cols.columns ]\nsoil_stony_OHE_cols.columns=soil_stony_OHE_cols_names\ntest=test.join(soil_stony_OHE_cols)\ntest.head()","637d933d":"test.head()","71ebc086":"\n\n#continuos_column_list.append('Soil_Stony_Level')\ncategorial_column_list = ['Cover_Type_Name', 'Wilderness_Area', 'Soil_Type','Soil_Stony_Level','Soil_Complex','Soil_Rubbly','Soil_Quality']","2fce38a7":"#exit for debug","b1862b8d":"#print Count of Unique Values in each continuos column\ntrain_test_continuous_values","c861c461":"# Distribution of Target (Cover Type)\ntrain.Cover_Type_Name.value_counts().plot(kind=\"barh\",figsize=(10,5) ,color='green')\nplt.xlabel(\"Count\")\nplt.ylabel(\"Cover Type\")","68237cf8":"#Function to plot charts of All features\ndef features_plots(data,continuous_vars,discrete_vars=None):\n    plt.figure(figsize=(20,24.5))\n    for i, cv in enumerate(continuous_vars):\n        plt.subplot(9, 2, i+1)\n        plt.hist(data[cv], bins=len(data[cv].unique()),color='green')\n        plt.title(cv)\n        plt.ylabel('Frequency')\n    if discrete_vars is not None:\n        for i, dv in enumerate(discrete_vars):\n            plt.subplot(9, 2, i+ 1 + len(continuous_vars))\n            data[dv].value_counts().plot(kind='bar', title=dv,color='blue')\n            plt.ylabel('Frequency')","55b4325c":"continuous_vars=continuos_column_list\ndiscrete_vars = ['Cover_Type_Name', 'Wilderness_Area', 'Soil_Type','Soil_Stony_Level','Soil_Quality']\nfeatures_plots(train,continuous_vars,discrete_vars)","689d1d5c":"features_plots(test,continuous_vars)","e45f765c":"number_of_columns=5\nnumber_of_rows=2\ncolumn_names = list(continuos_column_list)\nplt.figure(figsize=(4*number_of_columns,6*number_of_rows))\nfor i in range(0,len(column_names)):\n    plt.subplot(number_of_rows +1,number_of_columns,i+1)\n    dist=sns.distplot(train[column_names[i]],kde=True,color='green',vertical =False,\n                      kde_kws={'color':'red','lw':3,'label':'KDE'},\n                      hist_kws={'color':'green','lw':4,'label':'HIST','alpha':0.8}) \n    #dist.set_title(\"Distribution Plot\")","be553e6b":"number_of_columns=5\nnumber_of_rows=2\ncolumn_names = list(continuos_column_list)\nplt.figure(figsize=(4*number_of_columns,6*number_of_rows))\nfor i in range(0,len(column_names)):\n    plt.subplot(number_of_rows +1,number_of_columns,i+1)\n    dist=sns.distplot(test[column_names[i]],kde=True,color='green',vertical =False,\n                      kde_kws={'color':'red','lw':3,'label':'KDE'},\n                      hist_kws={'color':'green','lw':4,'label':'HIST','alpha':0.8}) \n    #dist.set_title(\"Distribution Plot\")","1bca44a8":"train['Horizontal_Distance_To_Hydrology'].value_counts(normalize=True).head()","d5da458f":"test['Horizontal_Distance_To_Hydrology'].value_counts(normalize=True).head()","786e5cc9":"train['Vertical_Distance_To_Hydrology'].value_counts(normalize=True).head()","33f16bae":"test['Vertical_Distance_To_Hydrology'].value_counts(normalize=True).head()","4844cff2":"train['Hillshade_3pm'].value_counts(normalize=True)","8acf4fc9":"test['Hillshade_3pm'].value_counts(normalize=True)","44659572":"train['Hillshade_3pm'].value_counts(normalize=True)[0],test['Hillshade_3pm'].value_counts(normalize=True)[0]","d642b2ae":"def replace_zeros_with_median(data,column):\n    #Need to check , which option provide better results\n    if KEEP_ZERO_VALUES==False:\n        median=data.loc[data[column]!=0,column].median()\n        data[column] = np.where(data[column] ==0, median,data[column])\n    else:\n        pass","8f4b5fe7":"#Replace 0 with median value in Horizontal_Distance_To_Hydrology , Vertical_Distance_To_Hydrology for both test and train\nreplace_zeros_with_median(train,'Horizontal_Distance_To_Hydrology')\ntrain['Horizontal_Distance_To_Hydrology'].value_counts(normalize=True).head()","5b901903":"replace_zeros_with_median(test,'Horizontal_Distance_To_Hydrology')\ntest['Horizontal_Distance_To_Hydrology'].value_counts(normalize=True).head()","ed6833a9":"replace_zeros_with_median(train,'Vertical_Distance_To_Hydrology')\ntrain['Vertical_Distance_To_Hydrology'].value_counts(normalize=True)","046bf5c8":"replace_zeros_with_median(test,'Vertical_Distance_To_Hydrology')\ntest['Vertical_Distance_To_Hydrology'].value_counts(normalize=True).head()","da51d0f1":"features_plots(train,['Vertical_Distance_To_Hydrology','Horizontal_Distance_To_Hydrology'])","8e9e006f":"features_plots(test,['Vertical_Distance_To_Hydrology','Horizontal_Distance_To_Hydrology'])","73cbd587":"#Univariate Analysis on Traning Data for Soil Type\ntrain['Soil_Type'].value_counts().plot(kind='bar', title=\"Distribution of Soil Types\",color='blue',figsize = (18, 6))","f8132362":"# Distribution of Soil Type in test data\ntest['Soil_Type'].value_counts().plot(kind='bar', title=\"Distribution of Soil Types\",color='blue',figsize = (18, 6))","93f7566b":"groupby_soiltype_covertype=train.groupby([\"Soil_Type\",\"Cover_Type_Name\"])[\"Soil_Type\"].count().unstack(\"Cover_Type_Name\").fillna(0)\ngroupby_soiltype_covertype[Cover_Type_Name].plot.bar(title=\"Distribution of Soil Types by Cover Types\",stacked=True,figsize=(18,7))","41b24b4c":"groupby_soilquality_covertype=train.groupby([\"Soil_Quality\",\"Cover_Type_Name\"])[\"Soil_Quality\"].count().unstack(\"Cover_Type_Name\").fillna(0)\ngroupby_soilquality_covertype[Cover_Type_Name].plot.bar(title=\"Distribution of Soil Quality by Cover Types\",stacked=True,figsize=(18,7))\n#groupby_soilquality_covertype","fb7f82ab":"soil_quality_list=[val for val in (set(train['Soil_Quality'].values))]\ngroupby_covertype_soiltquality=train.groupby([\"Cover_Type_Name\",\"Soil_Quality\"])[\"Cover_Type_Name\"].count().unstack(\"Soil_Quality\").fillna(0)\ngroupby_covertype_soiltquality[soil_quality_list].plot.bar(title=\"Distribution of Soil Quality by Cover Types\",stacked=True,figsize=(18,7))\n#groupby_covertype_soiltquality","2c98980b":"pd.options.display.float_format = '{:,.4f}'.format\ntrain['Soil_Type'].value_counts(normalize=True)","c85f5977":"# Filter out where values are less than threshold_to_drop% , these columns can be dropped\nthreshold_to_drop=0.02\ntrain_soil_type_columns_to_drop=train['Soil_Type'].value_counts(normalize=True).loc[train['Soil_Type'].value_counts(normalize=True).values<threshold_to_drop]\nlist(train_soil_type_columns_to_drop.index)","c98700c3":"test['Soil_Type'].value_counts(normalize=True)","4a50b7cc":"# Filter out where values are less than 1%, can be changed to drop more columns\n\ntest_soil_type_columns_to_drop=test['Soil_Type'].value_counts(normalize=True).loc[test['Soil_Type'].value_counts(normalize=True).values<threshold_to_drop]\nlist(test_soil_type_columns_to_drop.index)","c9f98f08":"columns_to_drop = columns_to_drop + list( set(list(train_soil_type_columns_to_drop.index) + list( test_soil_type_columns_to_drop.index)))\n                        \ncolumns_to_drop","afc8ec6a":"train=train.drop(columns_to_drop,axis=1)\ntest=test.drop(columns_to_drop,axis=1)","ecc4caa9":"def box_plots(data,continuous_vars):\n    plt.figure(figsize=(15,24.5))\n    for i, cv in enumerate(continuous_vars):\n        plt.subplot(int(len(continuous_vars)\/2) +1 , 2, i+1)\n        plt.boxplot(data[cv])\n        plt.title(cv)\n        plt.ylabel('Value')","18c2b48e":"\nbox_plots(train,continuos_column_list)","f8636103":"def get_outlier_percentage(data,column_name):\n    column_values=list(data[column_name])\n    q75, q25 = np.percentile(column_values, [75 ,25])\n    iqr = q75 - q25\n    outlier_percentage=(len(data) - len([x for x in column_values if q75+(1.5*iqr)>=x>= q25-(1.5*iqr)]))*100\/float(len(data))\n    return outlier_percentage","42b697ab":"# % of Outliers for Continuous Columns\noutlier_dict={}\nfor key in continuos_column_list:\n    outlier_percentage=get_outlier_percentage(train,key)\n    print(key,\":\",outlier_percentage)\n    outlier_dict[key]=outlier_percentage","61ddc2b3":"#Chart of Outliers\nfrom matplotlib.ticker import StrMethodFormatter\nplt.figure(figsize=(15,5))\nplt.barh(*zip(*outlier_dict.items()))\n\nax=plt.gca()\n\nax.set_xticklabels(outlier_dict.values(),rotation=0)\nax.set_xlabel(\"Percentage of Outliers\")\nax.xaxis.set_major_formatter(StrMethodFormatter('{x:,.1f} %'))\nplt.show()","848fcb2d":"number_of_columns=5\nnumber_of_rows=2\ncolumn_names = list(continuos_column_list)\nplt.figure(figsize=(4*number_of_columns,6*number_of_rows))\nfor i in range(0,len(column_names)):\n    plt.subplot(number_of_rows +1,number_of_columns,i+1)\n    dist=sns.distplot(train[column_names[i]],kde=True,color='green',vertical =False,\n                      kde_kws={'color':'red','lw':3,'label':'KDE'},\n                      hist_kws={'color':'green','lw':4,'label':'HIST','alpha':0.8})","83b07237":"def remove_outliers(data,column):\n    Q1=data[column].quantile(0.25)\n    Q3=data[column].quantile(0.75)\n    IQR=Q3-Q1\n    data = data[~((data[column]< (Q1 - 1.5 * IQR)) |(data[column] > (Q3 + 1.5 * IQR)))]\n    return data","e8cc38c6":"def replace_outliers_with_capping(data,column):\n    #Done lower degree (2%)  capping to avoid data loss\n    upper_lim = data[column].quantile(.98)\n    lower_lim = data[column].quantile(.02)\n    data.loc[(data[column] > upper_lim),column] = upper_lim\n    data.loc[(data[column] < lower_lim),column] = lower_lim\n    return data","8b0a362a":"def replace_outliers_with_median(data,column):\n    Q1=data[column].quantile(0.25)\n    Q3=data[column].quantile(0.75)\n    IQR=Q3-Q1\n    upper=Q3 + (1.5*IQR)\n    lower=Q1 - (1.5*IQR)\n    outlier_mask=[True if x <lower or x >upper else False for x in data[column]]\n    column_median=data[column].median()\n    data.loc[outlier_mask,column]=column_median\n    return data","be2f9341":"#Before outlier modified\ntrain.shape","058cee26":"for column in continuos_column_list:\n    #Need to check which option provide better results\n    \n    if KEEP_OUTLIERS == False:\n        #train=remove_outliers(train,column)\n        #train=replace_outliers_with_median(train,column)\n        train=replace_outliers_with_capping(train,column)\n    else:\n        pass","005cb851":"#After outlier removals\nprint(train.shape)\ntrain.head()","d6fb8df9":"\nfeatures_plots(test,continuous_vars)","79be71ca":"number_of_columns=5\nnumber_of_rows=2\ncolumn_names = list(continuos_column_list)\nplt.figure(figsize=(4*number_of_columns,6*number_of_rows))\nfor i in range(0,len(column_names)):\n    plt.subplot(number_of_rows +1,number_of_columns,i+1)\n    dist=sns.distplot(train[column_names[i]],kde=True,color='green',vertical =False,\n                      kde_kws={'color':'red','lw':3,'label':'KDE'},\n                      hist_kws={'color':'green','lw':4,'label':'HIST','alpha':0.8})","4def1222":"from sklearn import preprocessing\nx_norm=  train[continuos_column_list] #Normalize numeric values \npd.set_option('display.width',1700)\npd.set_option('precision',2)\n#corr_mat=x_norm.corr(method=\"pearson\")\n\ncorr_mat=train[ continuos_column_list +['Cover_Type']].corr(method=\"pearson\")\ncorr_mat","b6e78d23":"plt.figure(figsize=(10,10))\nsns.set(font_scale=0.9)\nsns.heatmap(corr_mat,annot=True,square=True,vmax=0.8,cmap=\"inferno\",fmt='.2f')","b01bcc88":"def pair_boxplot(data,continuous_vars,by_column):\n    \n    fig=plt.figure(figsize=(20,60))\n    i=0\n    for i, cv in enumerate(continuous_vars):\n        ax=fig.add_subplot(int(len(continuous_vars))+1, 2, i+1)\n        #data.boxplot(column=cv,return_type='axes',by=by_column,ax=ax)\n        #print(len(continuous_vars))\n        data.boxplot(column=cv,return_type='axes',by=by_column,ax=ax)\n        ax.set_title(\"Box Plot for \" + cv + \" and \" + by_column)\n        ax.set_xlabel(\"Cover Type\")\n        ax.set_ylabel(cv)\n        #plt.title(cv)\n        #plt.ylabel('Value')","8804b32b":"pair_boxplot(train,continuous_vars,'Cover_Type')","facb3a3b":"train.boxplot(column=['Elevation'],return_type='axes',by=\"Cover_Type\",figsize=(15,7))\nplt.show()","d18a8876":"#Lets create a new feature to widen the gap in Elevation\ntrain['ELV_SQR']=train['Elevation']**2\ntest['ELV_SQR']=test['Elevation']**2\ncontinuos_column_list.append(\"ELV_SQR\")\ntrain.boxplot(column=['ELV_SQR'],return_type='axes',by=\"Cover_Type\",figsize=(15,7))\nplt.show()","19b737d3":"\n#joint_plot=sns.jointplot(\"Cover_Type\",\"Elevation\",data=train,kind='kde',color='green')","01a99260":"#kde_plot=sns.kdeplot(train[\"Cover_Type\"],train[\"Elevation\"],cmap=\"PRGn\",cbar=True)","27e532dd":"sns.pairplot(train,height=4,vars=['Elevation','Slope','Aspect','Horizontal_Distance_To_Hydrology'],hue='Cover_Type')","4731adeb":"sns.pairplot(train,height=4,vars=['Elevation','Vertical_Distance_To_Hydrology','Horizontal_Distance_To_Roadways','Horizontal_Distance_To_Fire_Points'],hue='Cover_Type')","69d96fdc":"sns.pairplot(train,height=4,vars=['Elevation','Hillshade_9am','Hillshade_Noon','Hillshade_3pm'],hue='Cover_Type')","ef1295bb":"'''\ncmap = sns.cubehelix_palette(as_cmap=True)\n\nf, ax = plt.subplots(figsize=(20,10))\npoints = ax.scatter(train['Hillshade_Noon'],train['Elevation'] , c=train['Cover_Type'],label=Cover_Type_Name ,s=20, cmap='rainbow')\n#plt.xticks(np.arange(0, 400,20))\n#plt.axis('scaled')\nf.colorbar(points)\n#ax.legend(loc='upper left',fontsize='large')\n#plt.legend()\nplt.show()\n'''","f83376cc":"#plt.subplots(figsize=(15,10))\n#sns.swarmplot(\"Cover_Type\",\"Elevation\",data=train)","c6fa1ecb":"sns.set()","7872c18f":"'''\nfacet=sns.FacetGrid(train,col='Wilderness_Area_Name',hue='Cover_Type_Name',col_wrap=2,height=7,aspect=1.5)\nfacet.map(plt.scatter,'Horizontal_Distance_To_Hydrology','Elevation',alpha=0.5)\nfacet.add_legend()\n'''","6791abe5":"def pair_scatterplot(data,continuous_vars,y_column,target_column):\n    #plt.figure(figsize=(20,10))\n    cmap = sns.cubehelix_palette(as_cmap=True)\n    for i, cv in enumerate(continuous_vars):\n        #plt.subplot(7, 2, i+1)\n        #data.boxplot(column=cv,return_type='axes',by=by_column)\n        f, ax = plt.subplots(figsize=(20,10))\n        ax.set_xlabel(cv)\n        ax.set_ylabel(y_column)\n        points = ax.scatter(train[cv],train[y_column] , c=train[target_column],label=Cover_Type_Name ,s=20, cmap='rainbow')\n        f.colorbar(points)\n    plt.show()","975ee1ff":"#pair_scatterplot(train,continuous_vars,'Elevation','Cover_Type')","c540bb91":"#Drop Extra columns\nextra_columns_in_train=['Cover_Type_Name' ,'Wilderness_Area','Wilderness_Area_Name' ,'Soil_Type']\ntrain=train.drop(extra_columns_in_train,axis=1)\nextra_columns_in_test=['Soil_Type']\ntest=test.drop(extra_columns_in_test,axis=1)","01e62caa":"#Lets Remove all Soil Type columns , to check if it helps\nsoil_type_cols=[colname for colname in train.columns if colname.find(\"Soil_Type\")!=-1 ]\n#print(soil_type_cols)\n#train=train.drop(soil_type_cols,axis=1)\n#test=test.drop(soil_type_cols,axis=1)","881fa92d":"train.head()","a5b1dce0":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nscaler=MinMaxScaler()\ntrain[continuos_column_list] = scaler.fit_transform(train[continuos_column_list])\ntest[continuos_column_list] = scaler.fit_transform(test[continuos_column_list])","02ab2d87":"test.head()","9030f468":"train.head()","4eb14a8e":"X,Y=train.drop(['Cover_Type'], axis=1), train['Cover_Type']","b98ff57f":"from sklearn.decomposition import PCA\npca =PCA(n_components=15,whiten=True)\nx_reduced=pca.fit_transform(X)","3b7317d9":"pca.explained_variance_","2b3c83c1":"pca.explained_variance_ratio_","d730068b":"plt.plot(pca.explained_variance_ratio_)\nplt.xlabel(\"Dimension\")\nplt.ylabel(\"Explained Variance Ratio\")","4c27c88a":"#!pip install lightgbm\nfrom lightgbm import LGBMClassifier, plot_importance\ndef get_LGBC():\n    return LGBMClassifier(n_estimators=500,  \n                     learning_rate= 0.1,\n                     objective= 'multiclass', \n                     num_class=7,\n                     random_state= 2019,\n#                      class_weight=class_weight_lgbm,\n                     n_jobs=-1)\nlgbc= get_LGBC()\nlgbc.fit(X,Y)\n\nplot_importance(lgbc, ignore_zero=False, max_num_features=20)","99dfe799":"x_train, x_test, y_train, y_test = train_test_split(train.drop(['Cover_Type'], axis=1), train['Cover_Type'], test_size=0.2)","c50a3d4f":"'''\nfrom sklearn.svm import LinearSVC\nsvc_clf=LinearSVC(max_iter=10000)\nsvc_clf.fit(x_train,y_train)\naccuracy=svc_clf.score(x_test,y_test)\nprint(accuracy)\n'''","ef95d8d7":"'''\nimport sklearn.decomposition \n# Create a Randomized PCA model that takes two components\nrandomized_pca = sklearn.decomposition.RandomizedPCA(n_components=2)\n\n# Fit and transform the data to the model\nreduced_data_rpca = randomized_pca.fit_transform(X)\n\n# Create a regular PCA model \npca = sklearn.decomposition.PCA(n_components=2)\n\n# Fit and transform the data to the model\nreduced_data_pca = pca.fit_transform(X)\n\n# Inspect the shape\nreduced_data_pca.shape\n\n# Print out the data\nprint(reduced_data_rpca)\nprint(reduced_data_pca)\n'''","70744f39":"'''\nfrom sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=100)\nclf.fit(x_train, y_train)\ntrain_accuracy=clf.score(x_train, y_train)\ntest_accuracy=clf.score(x_test,y_test)\nprint(train_accuracy,test_accuracy)\n'''","cb0a1dee":"'''\nfrom sklearn.ensemble import GradientBoostingClassifier\nclf = GradientBoostingClassifier(random_state=10, learning_rate=0.05,\nn_estimators=200, max_depth=5, max_features=20)\nclf.fit(x_train, y_train)\ny_pred = clf.predict(x_test)\ntrain_accuracy=clf.score(x_train, y_train)\ntest_accuracy=clf.score(x_test,y_test)\nprint(train_accuracy,test_accuracy)\n\n'''","2d9d69bd":"'''\nfrom sklearn.metrics import make_scorer, accuracy_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import GradientBoostingClassifier\n\n# Choose the type of classifier. \nclf = GradientBoostingClassifier()\n\n# Choose some parameter combinations to try\nparameters = {'n_estimators': [40, 60, 90,120], \n              'max_features': [10, 20,30], \n              'learning_rate':[0.01,0.05,0.1,0.2],\n              'max_depth': [2, 3, 5, 10], \n              'random_state': [5, 10, 15]\n             }\n\n# Type of scoring used to compare parameter combinations\nacc_scorer = make_scorer(accuracy_score)\n\n# Run the grid search\ngrid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer,cv=5)\ngrid_obj = grid_obj.fit(x_train, y_train)\n\n# Set the clf to the best combination of parameters\nclf = grid_obj.best_estimator_\n\n# Fit the best algorithm to the data. \nclf.fit(x_train, y_train)\npredictions = clf.predict(x_test)\nprint(accuracy_score(y_test, predictions),clf)\n'''","5c0ecbd0":"def get_rf():\n    from sklearn.ensemble import RandomForestClassifier\n    model=RandomForestClassifier(n_estimators=700,\n                                              criterion='gini', \n                                              max_depth=16,\n                                              min_samples_split=2,\n                                              min_samples_leaf=3, \n                                              min_weight_fraction_leaf=0.0, \n                                              max_features=15, \n                                              max_leaf_nodes=None, \n                                              bootstrap=True, \n                                              oob_score=False,\n                                              n_jobs=1, \n                                              random_state=10,\n                                              verbose=0, \n                                              warm_start=False, \n                                              class_weight=None)\n    return model","3446036f":"from xgboost import XGBClassifier, plot_importance\ndef get_xgb():\n    \n#model = RandomForestClassifier(n_estimators=100)\n#model.fit(X_train, y_train)\n\n\n#!pip install xgboost\n#https:\/\/towardsdatascience.com\/fine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e\n#https:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/\n    \n\n\n    model = XGBClassifier(silent=False, \n                          learning_rate=0.185,  \n                          colsample_bytree = 0.6,\n                          subsample = 0.8,\n                          n_estimators=180,\n                          objective='multi:softmax',\n                          num_class=7,\n                          reg_alpha = 0.2,\n                          max_depth=6,\n                          min_child_weight=1,\n                          scale_pos_weight=0,\n                          reg_lambda=1,\n                          random_state=10,\n                          gamma=1)\n \n    return model\n\n","92e8343f":"from sklearn import metrics\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import label_binarize\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom scipy import interp\nfrom itertools import cycle\n\ndef model_performance(model, X_train, y_train, y_test, y_score,n_classes=0):\n    print ('Model : ',model)\n    print( 'Test accuracy (Accuracy Score): %f'%metrics.accuracy_score(y_test, y_score))\n    print ('Test accuracy (ROC AUC Score): %f'%metrics.roc_auc_score(y_test, y_score))\n    #print ( 'Train accuracy: %f'%model.score(X_train, y_train))\n    \n    lw = 2\n    \n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_score[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    # Compute micro-average ROC curve and ROC area\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_score.ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n    \n    plt.figure(figsize=(20,10))\n    \n    \n    plt.plot(fpr[2], tpr[2], color='darkorange',\n         lw=lw, label='ROC curve (area = %0.2f)' % roc_auc[2])\n    plt.plot([0, 1], [0, 1], color='navy', lw=lw, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    \n    \n    # Compute macro-average ROC curve and ROC area\n\n    # First aggregate all false positive rates\n    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n    # Then interpolate all ROC curves at this points\n    mean_tpr = np.zeros_like(all_fpr)\n    for i in range(n_classes):\n        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n    # Finally average it and compute AUC\n    mean_tpr \/= n_classes\n\n    fpr[\"macro\"] = all_fpr\n    tpr[\"macro\"] = mean_tpr\n    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n    # Plot all ROC curves\n    \n    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n             label='micro-average ROC curve (area = {0:0.2f})'\n                   ''.format(roc_auc[\"micro\"]),\n             color='deeppink', linestyle=':', linewidth=4)\n\n    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n             label='macro-average ROC curve (area = {0:0.2f})'\n                   ''.format(roc_auc[\"macro\"]),\n             color='navy', linestyle=':', linewidth=4)\n\n    colors = cycle(['aqua', 'darkorange', 'cornflowerblue','green','yellow','blue','red'])\n    for i, color in zip(range(n_classes), colors):\n        plt.plot(fpr[i], tpr[i], color=color, lw=lw,\n                 label='ROC curve of class {0} (area = {1:0.2f})'\n                 ''.format(i, roc_auc[i]))\n\n    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Some extension of Receiver operating characteristic to multi-class')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\n","db855b4a":"train.shape,test.shape","5af71ad6":"from sklearn.model_selection import train_test_split\n","515b1acf":"X_train, X_val, y_train, y_val = train_test_split(train.drop(['Cover_Type'], axis=1), train['Cover_Type'], test_size=0.1)","ad94fe4c":"X_train.shape, X_val.shape, y_train.shape, y_val.shape","39b47f23":"\n\nmodel =get_xgb()\n\n#eval_set = [(X_val, y_val)]\n\n\neval_set = [(X_train, y_train), (X_val, y_val)]\n\n#model=get_rf()\nmodel.fit(X_train, y_train,eval_metric=[\"merror\", \"mlogloss\"], eval_set=eval_set, verbose=True,early_stopping_rounds=10)\n\nfrom sklearn.metrics import classification_report, accuracy_score\nprint(\"Train Accuracy:\",model.score(X_train, y_train))\npredictions = model.predict(X_val)\nprint(\"Test Accuracy:\",accuracy_score(y_val, predictions))\n","c4b5cb42":"model_performance(model, X_train,label_binarize(y_train,classes=[1, 2,3,4,5,6,7]),label_binarize(y_val,classes=[1, 2,3,4,5,6,7]) ,label_binarize(predictions,classes=[1, 2,3,4,5,6,7]) ,n_classes=7)","1e617986":"#from sklearn.metrics import classification_report, accuracy_score\nresults = model.evals_result()\n#print(results)\n\nepochs = len(results['validation_0']['merror'])\nx_axis = range(0, epochs)\n\nfig=plt.figure(figsize=(20,10))\n# plot log loss\nax = fig.add_subplot(2,2,1)\nplt.plot(x_axis, results['validation_0']['mlogloss'], label='Train')\nplt.plot(x_axis, results['validation_1']['mlogloss'], label='Test')\nplt.legend()\nplt.ylabel('Log Loss')\nplt.title('XGBoost Log Loss')\n#plt.show()\n\n# plot classification error\n\n\nax = fig.add_subplot(2,2,2)\nplt.plot(x_axis, results['validation_0']['merror'], label='Train')\nplt.plot(x_axis, results['validation_1']['merror'], label='Test')\nplt.legend()\nplt.ylabel('Classification Error')\nplt.title('XGBoost Classification Error')\nplt.show()\n","baaeaca9":"#from sklearn.metrics import classification_report, accuracy_score","3d5d27b8":"#model.score(X_train, y_train)","b6dc7319":"#predictions = model.predict(X_val)\n#accuracy_score(y_val, predictions)","2b5c94bc":"test.head()","fb7a8583":"test_pred = model.predict(test)","d44cf491":"#Save test predictions to file\noutput = pd.DataFrame({'id': test_ids,\n                       'Cover_Type': test_pred})\noutput.to_csv('submission_xgb_v3.csv', index=False)","11a4ff19":"** Features not much co related , so no conclusion from Correlation Matrix **","3031b5d5":" **<font color =\"red\"> Cover Type is evenly distributed<\/font>**","9a83e3ff":"# References\n\n* Explorative Data Analysis to extract the most relevant features\n-> https:\/\/towardsdatascience.com\/exploratory-data-analysis-8fc1cb20fd15\n   https:\/\/realpython.com\/python-data-cleaning-numpy-pandas\/\n* Feature engineering\n -> https:\/\/towardsdatascience.com\/feature-engineering-for-machine-learning-3a5e293a5114\n* Cross-validation so we can use the entire training data\n -> https:\/\/www.analyticsvidhya.com\/blog\/2018\/05\/improve-model-performance-cross-validation-in-python-r\/\n* Grid-Search to find the optimal parameters for our classifier so we can fight overfitting\n -> https:\/\/towardsdatascience.com\/grid-search-for-model-tuning-3319b259367e\n* XGBoost Classifier -> https:\/\/towardsdatascience.com\/fine-tuning-xgboost-in-python-like-a-boss-b4543ed8b1e\n -> https:\/\/www.analyticsvidhya.com\/blog\/2018\/09\/an-end-to-end-guide-to-understand-the-math-behind-xgboost\/\n","1e15a515":"#### To check distribution-Skewness","bbf9868c":"1. <font color='red'>Vertical Distance to Hydrology have Negative Values, need to investigate on this. <\/font>\n# 2. <font color='red'>Vertical Distance to Hydrology,Horizontal Distance to Hydrology ,Hillshade_3pm peaked near zero , need to investigate on this.<\/font>\n# 3. <font color='red'>Soil Type have skewed toward few types.<\/font>","58f6ef68":"1. **<font color='red'>Multiple Columns have outliers<\/font>**\n\nhttps:\/\/towardsdatascience.com\/ways-to-detect-and-remove-the-outliers-404d16608dba","a71ad20e":"### PCA\nhttps:\/\/medium.com\/sfu-big-data\/principal-component-analysis-deciphered-79968b47d46c","e5b23cac":"**No clear clue from Heat Map **","065afd27":"# Import Data","dae043bf":"# Feature Engineering \nhttps:\/\/towardsdatascience.com\/feature-engineering-for-machine-learning-3a5e293a5114","63097c7f":"# Model Training","ecc0e561":" **<font color =\"red\"> Aspect seems to be a categorial column, having 361 unique values in both train and test data. Rest data seems have continuous spread. <\/font>**","cae4f3a4":"### Elevation in important but other features are not providing futher clue.","fc94093d":"# Multivariate Analysis on Traning Data","ba09a4e1":"\n**<font color =\"red\"> Let's delete the Id column in the training set but store it for the test set before deleting <\/font>**","ced65778":"# Explorative Data Analysis\n## Univariate Analysis on Traning Data","959e27fc":"** <font color='red'>Soil Type seems to be distributed non evenly  <\/font> **","cad74a4a":"** Skewness of Columns below shows high % of outliers **\n* Horizontal_Distance_To_Hydrology \n* Vertical_Distance_To_Hydrology \n* Horizontal_Distance_To_Roadways \n* Hillshade_9am \n* Hillshade_Noon \n* Horizontal_Distance_To_Fire_Points \n","d507c6c6":"### Elevation is most important feature for classification of Cover Type","d5e601a0":"# Predictions","a43d3228":" **<font color =\"red\"> No Missing Values in Train and Test data<\/font>**","45c95aa5":"1. **<font color =\"red\"> We can see Soil Type 7 and 15 have single value , these columns can be dropped<\/font>**\n2. **<font color =\"red\"> Soil Type ,Wilderness Area , Cover Type are categorial columns , rest columns are continuous columns.<\/font>**\n3. **<font color =\"red\"> Soil Type ,Wilderness Area are One Hot Encoded <\/font>**","3fb39705":"** Need to work on PCA implementation **","30788af3":"1.  <font color='red'>Horizontal_Distance_To_Hydrology have 11% values as 0 in train however in test it 4%, seems  0 represent some missing value here <\/font>\n2.  <font color='red'>Vertical_Distance_To_Hydrology have 12% values as 0 in train however in test it 6%, seems  0 represent some missing value here <\/font>\n3.  <font color='red'>Hillshade_3pm have values as 0 less than 1% in train and test, so not considering it as missing value.","c9a28609":" **<font color =\"blue\">Lets convert One Hot Encoded Columns  Wilderness Area and , Cover Type and Soil Type into Categorial Columns  might be used later for EDA <\/font>**","b3484bd0":"Let's use 80% of the Data for training, and 20% for validation","5c8aca14":" ** <font color=red>All columnss are numeric in both train and test data. <\/font>**"}}