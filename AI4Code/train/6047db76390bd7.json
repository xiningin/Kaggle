{"cell_type":{"471b8405":"code","726d2d96":"code","3c6dd1b2":"code","db9da1c7":"code","336082cb":"code","a1769ed9":"code","4ef58bf0":"code","1fe376d0":"code","d675e6e7":"code","7869a92a":"code","e141f6ac":"code","c53776c5":"code","d2171890":"code","712df959":"code","99a76f56":"code","fb01229b":"code","505a170e":"code","e1329475":"code","764a1196":"code","6d286580":"code","fae3a4c0":"code","9b411d8c":"code","b00bc0e1":"code","83ae3630":"code","ffd43ce5":"code","cfd241fc":"code","48aa9eb3":"code","634b1005":"code","f977727e":"code","3b695658":"code","157cfef0":"code","b10967f9":"code","2ecd88c8":"code","96d88f9f":"code","57c36aa1":"markdown","6851d982":"markdown","2b0113bb":"markdown","e350fce1":"markdown","ea70fdf3":"markdown","da78b377":"markdown","09b55bb6":"markdown","d21dda49":"markdown","161e4d93":"markdown","4edcaac3":"markdown","ff26dac0":"markdown","3cc32fae":"markdown","47fbee36":"markdown","3dac78a9":"markdown","099ce2e8":"markdown","dd2ad937":"markdown","ea8d3400":"markdown","2f8f8c45":"markdown","8bf42bd4":"markdown","f8a6e40c":"markdown","79270a28":"markdown","044abb59":"markdown","7e862743":"markdown","b6d89506":"markdown","a34175ac":"markdown","90e9ccd5":"markdown","926e4613":"markdown","e385c5ec":"markdown","44db3a90":"markdown","2aaec706":"markdown","62149f22":"markdown","d9dc569b":"markdown"},"source":{"471b8405":"# Math, Numpy and Pandas\nimport math\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# operating system\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n#warnings supression\nimport warnings\nwarnings.filterwarnings('ignore')","726d2d96":"data = pd.read_csv (os.path.join(dirname, filename))\nprint(\"The data shape is : {} \".format(data.shape))\n","3c6dd1b2":"# Take a look at the first few instances\npd.set_option('display.max_columns', None)\ndata.head()","db9da1c7":"data.info()","336082cb":"data.describe()","a1769ed9":"# Take a better look at categorical data\ncat_columns = data.select_dtypes(include = ['object'])\nunique_values = cat_columns.nunique(dropna=False)\nprint (unique_values)","4ef58bf0":"data = pd.get_dummies(data, drop_first = True)\nprint(\"The data shape is : {} \".format(data.shape))\ndata.head()\n","1fe376d0":"data.info()","d675e6e7":"data.describe()","7869a92a":"sns.pairplot(data.sample(10000), hue='satisfaction_satisfied')","e141f6ac":"# Focus on interesting features\nfocus_features = ['age', 'flight_distance', 'inflight_wifi_service', 'online_boarding', 'seat_comfort', 'inflight_entertainment']\n\n\nsns.set(font_scale= 1.2)\nsns.set_style('ticks')\n\nfor i, feature in enumerate(focus_features):\n    sns.displot(data=data, x=feature, kind='kde', hue='satisfaction_satisfied')  \n    \nsns.despine()\n","c53776c5":"print (data.isnull().sum())","d2171890":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\ncorrmat = data.corr()\nplt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax=0.9, square=True)\nprint ('Correlation between departure and arrival delays: ', corrmat['arrival_delay_in_minutes']['departure_delay_in_minutes'])","712df959":"# Fix data by imputation\ndata['arrival_delay_in_minutes'].fillna(data['departure_delay_in_minutes'], inplace = True)\nprint (data.isnull().sum())","99a76f56":"plt.scatter (data['departure_delay_in_minutes'], np.random.rand(data.shape[0]))\nplt.scatter (data['arrival_delay_in_minutes'], np.random.rand(data.shape[0]))","fb01229b":"print ((data['arrival_delay_in_minutes']> 500).sum())\nprint ((data['departure_delay_in_minutes']> 500).sum())\nprint (((data['departure_delay_in_minutes']> 500) + (data['arrival_delay_in_minutes']> 500)).sum())\n","505a170e":"data = data[(data['arrival_delay_in_minutes'] <= 500)]\ndata = data[(data['departure_delay_in_minutes'] <= 500)]\nplt.scatter (data['departure_delay_in_minutes'], np.random.rand(data.shape[0]))\nplt.scatter (data['arrival_delay_in_minutes'], np.random.rand(data.shape[0]))","e1329475":"plt.scatter (data['flight_distance'], np.random.rand(data.shape[0]))","764a1196":"# Prepare for modeling - migrate to Numpy and split to training and test sets\nfrom sklearn.model_selection import train_test_split\n\nused_data = data.drop (['Unnamed: 0', 'satisfaction_satisfied'], axis=1)\nx = used_data.values \ny = data['satisfaction_satisfied'].values\n\n# Normalize features\nfor feature in range (x.shape[1]):\n    min = x[:,feature].min()\n    max = x[:,feature].max()\n    x[:,feature] = (x[:,feature]-min) \/ (max-min)\n    \nx_train, x_test, y_train, y_test = train_test_split (x, y, test_size=0.2, random_state=42)\nprint(\"The training data size is : {} \".format(x_train.shape))\nprint(\"The test data size is : {} \".format(x_test.shape))","6d286580":"#Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\n\ndct = DecisionTreeClassifier(max_depth=None)\ndct.fit(x_train,y_train)\ndct_training_score = 100*dct.score(x_train, y_train)\nprint ('Tree Depth:', dct.get_depth())\nprint ('Tree Leaves:', dct.get_n_leaves())\ndct_test_score = 100*dct.score(x_test, y_test)\nprint(\"Decision Tree accuracy. Train : {:.2f}%, Test: {:.2f}%. \".format(dct_training_score, dct_test_score))","fae3a4c0":"# Tree depth dependency\nmax_d = dct.get_depth()\ndct_training_score, dct_test_score = np.zeros(max_d), np.zeros(max_d)\nfor i in range (max_d):\n  dct = DecisionTreeClassifier(max_depth=i+1)\n  dct.fit(x_train,y_train)\n  dct_training_score[i] = 100*dct.score(x_train, y_train)\n  dct_test_score[i] = 100*dct.score(x_test, y_test)\n\nprint (np.around (dct_training_score, decimals=2))  \nprint (np.around (dct_test_score, decimals=2))\nplt.plot (dct_training_score)\nplt.plot(dct_test_score)\n","9b411d8c":"# Get most important tree features\nfeatures = used_data.columns\nimportances = dct.feature_importances_\nleading_indices = (-importances).argsort()[:23]\nprint (\"Leading features sorted by importance:\")\nfor i in range (23):\n    print (i+1, features[leading_indices[i]], round(100*importances[leading_indices[i]],2), '%')","b00bc0e1":"# Tree based on 3 best features\nleading_x = data[['online_boarding', 'inflight_wifi_service', 'type_of_travel_Personal Travel']]\nlx_train, lx_test, ly_train, ly_test = train_test_split (leading_x, y, test_size=0.2, random_state=42)\n\ndct = DecisionTreeClassifier(max_depth=None)\ndct.fit(lx_train,ly_train)\ndct_training_score = 100*dct.score(lx_train, ly_train)\nprint ('Tree Depth on leading 3 parameters:', dct.get_depth())\nprint ('Tree Leaves using leading 3 parameters:', dct.get_n_leaves())\ndct_test_score = 100*dct.score(lx_test, ly_test)\nprint(\"Decision Tree accuracy for leading 3 parameters. Train : {:.2f}%, Test: {:.2f}%. \".format(dct_training_score, dct_test_score))\n","83ae3630":"# Adaboost\nfrom sklearn.ensemble import AdaBoostClassifier\n\nadb = AdaBoostClassifier(n_estimators=500)\nadb.fit(x_train,y_train)\nadaboost_training_score = 100*adb.score(x_train,y_train)\nadaboost_test_score = 100*adb.score(x_test,y_test)\nprint(\"Adaboost accuracy. Train : {:.2f}%, Test: {:.2f}%. \".format(adaboost_training_score, adaboost_test_score))","ffd43ce5":"# Gradient boosting\nfrom sklearn.ensemble import GradientBoostingClassifier\n# Try 100 estimators\ngdb = GradientBoostingClassifier(n_estimators=100)\ngdb.fit(x_train,y_train)\ngradient_boost_training_score = 100*gdb.score(x_train,y_train)\ngradient_boost_test_score = 100*gdb.score(x_test,y_test)\nprint(\"Gradient boost accuracy for 100 estimators. Train : {:.2f}%, Test: {:.2f}%. \".format(gradient_boost_training_score, gradient_boost_test_score))\n# Try 200 estimators\ngdb = GradientBoostingClassifier(n_estimators=200)\ngdb.fit(x_train,y_train)\ngradient_boost_training_score = 100*gdb.score(x_train,y_train)\ngradient_boost_test_score = 100*gdb.score(x_test,y_test)\nprint(\"Gradient boost accuracy for 200 estimators. Train : {:.2f}%, Test: {:.2f}%. \".format(gradient_boost_training_score, gradient_boost_test_score))","cfd241fc":"from sklearn.ensemble import RandomForestClassifier\n# Try 100 estimators\nrfc = RandomForestClassifier(n_estimators=100)\nrfc.fit(x_train,y_train)\nrandom_forest_training_score = 100*rfc.score(x_train,y_train)\nrandom_forest_test_score = 100*rfc.score(x_test,y_test)\nprint(\"Random forest accuracy, 100 estimators. Train : {:.2f}%, Test: {:.2f}%. \".format(random_forest_training_score, random_forest_test_score))\n# Try 200 estimators\nrfc = RandomForestClassifier(n_estimators=200)\nrfc.fit(x_train,y_train)\nrandom_forest_training_score = 100*rfc.score(x_train,y_train)\nrandom_forest_test_score = 100*rfc.score(x_test,y_test)\nprint(\"Random forest accuracy, 200 estimators. Train : {:.2f}%, Test: {:.2f}%. \".format(random_forest_training_score, random_forest_test_score))","48aa9eb3":"# Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\n\nnbc = GaussianNB()\nnbc.fit (x_train, y_train)\nnbc_training_score = 100*nbc.score(x_train,y_train)\nnbc_test_score = 100*nbc.score(x_test,y_test)\nprint(\"Naive Bayes accuracy. Train : {:.2f}%, Test: {:.2f}%. \".format(nbc_training_score, nbc_test_score))","634b1005":"# K Nearest neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit (x_train, y_train)\nknn_training_score = 100*knn.score(x_train,y_train)\nknn_test_score = 100*knn.score(x_test,y_test)\nprint(\"KNN accuracy. Train : {:.2f}%, Test: {:.2f}%. \".format(knn_training_score, knn_test_score))","f977727e":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogr = LogisticRegression(max_iter=1000)\nlogr.fit (x_train, y_train)\nlogr_training_score = 100*logr.score(x_train,y_train)\nlogr_test_score = 100*logr.score(x_test,y_test)\nprint(\"Logistic Regression accuracy. Train : {:.2f}%, Test: {:.2f}%. \".format(logr_training_score, logr_test_score))","3b695658":"# Look at coefficients\ncoefficients = logr.coef_[0]\nleading_indices = (-coefficients).argsort()[:23]\nprint (\"Sorted coefficients for features:\")\nfor i in range (23):\n    print (i+1, features[leading_indices[i]], round(coefficients[leading_indices[i]],2))","157cfef0":"# SVM - default kernel\nimport time\nfrom sklearn.svm import SVC\nsvc = SVC()\nsizes = np.array([10, 1000, 2000, 3000, 4000, 5000, 6000])\ntimes = np.zeros(len(sizes))\nsvc_training_score = np.zeros(len(sizes))\nsvc_test_score = np.zeros(len(sizes))\nfor ind, size in enumerate(sizes):\n    select = np.random.choice (np.arange(len(y_train)), size=size)\n    x_sample = x_train[select,:]\n    y_sample = y_train[select]\n    start_time = time.monotonic() \n    svc.fit (x_sample, y_sample)\n    end_time = time.monotonic() \n    times[ind] = (end_time - start_time)\n    svc_training_score[ind] = 100*svc.score(x_train,y_train)\n    svc_test_score[ind] = 100*svc.score(x_test,y_test)\nprint (\"Sizes: \", sizes)\nprint (\"Times: \", times)\ntimes \/= times[1]\nsizes = sizes \/ sizes[1]\nplt.scatter (sizes, times)\nplt.scatter (sizes, np.sqrt(times))\nprint (\"Training Scores:\", svc_training_score)\nprint (\"Test scores:\", svc_test_score)\nprint(\"Estimated time for training on 100,000 instances: \", times[-1]*(100\/6)**2 \/ 60, \"minutes.\")\n","b10967f9":"# SVM - optimize Kernel\nkernels = (['linear', 'poly', 'rbf', 'sigmoid'])\nselect = np.random.choice (np.arange(len(y_train)), size=1000)\nx_sample = x_train[select,:]\ny_sample = y_train[select]\nfor ker in (kernels):\n    svc = SVC(kernel=ker)\n    svc.fit (x_sample, y_sample)\n    svc_training_score = 100*svc.score(x_train,y_train)\n    svc_test_score = 100*svc.score(x_test,y_test)\n    print(\"SVC kernel: \", ker, \". Train : {:.2f}%, Test: {:.2f}%. \".format(svc_training_score, svc_test_score))","2ecd88c8":"# SVM - optimize C\nfrom sklearn.svm import SVC\nC_values = np.linspace(0.1, 10.0, num=20)\nsvc_training_score = np.zeros (len(C_values))\nsvc_test_score = np.zeros (len(C_values))\nselect = np.random.choice (np.arange(len(y_train)), size=1000)\nx_sample = x_train[select,:]\ny_sample = y_train[select]\nfor i, C in enumerate(C_values):\n    svc = SVC(kernel='linear', C=C)\n    svc.fit (x_sample, y_sample)\n    svc_training_score[i] = 100*svc.score(x_train,y_train)\n    svc_test_score[i] = 100*svc.score(x_test,y_test)\nplt.scatter (C_values, svc_training_score)\nplt.scatter (C_values, svc_test_score)\n","96d88f9f":"# Stacking\nfrom sklearn.ensemble import StackingClassifier\nestimators = [\\\n    ('dct', DecisionTreeClassifier(max_depth=8)), \\\n    ('gdb', GradientBoostingClassifier(n_estimators=200)), \\\n    ('rfc', RandomForestClassifier(n_estimators=100)), \\\n    ('nbc', GaussianNB()), \\\n    ('logr', LogisticRegression(max_iter=1000)) \\\n#   , ('svc', SVC(kernel='linear'))\n             ]\nclf = StackingClassifier(estimators=estimators, final_estimator=LogisticRegression())\nclf.fit(x_train, y_train)\nclf_training_score = 100*clf.score(x_train,y_train)\nclf_test_score = 100*clf.score(x_test,y_test)\nprint(\"Stacked classifier accuracy. Train : {:.2f}%, Test: {:.2f}%. \".format(clf_training_score, clf_test_score))\n","57c36aa1":"It seems the tree has a similar training and test accuracy up to around depth 7 or 8, where a gap between training and test sets begin to develop (=overlearning). This means we can limit tree depth at 8 for future estimators. Next, let's look at the important features in the final tree:","6851d982":"We have 5 categorical columns, most of them with only 2 possible values (this includes the target). The last one (customer class) has 3 categorical values. \n\n# Hotkey Encoding\nWe will use hotkey encoding to deal with the categorical data:","2b0113bb":"It seems that only very few instances have a delay (in either arrival or departure) of > 500 minutes. Let's test this hypothesis:","e350fce1":"It seems the linear kernel did a lot better than the others (including the default which was the Radial Basis Function or RBF). In fact, with a linear kernel the SVC algorithm (87%) does better than all the non-tree algorithms, though only marginally better than logistic regression (86.5%) and naive Bayes (86%). As mentioned, this may improve slightly if we train on more data, but this will be very time consuming and is not expected to result in huge gains. The final parameter we want to optimize for SVM is the regularization parameter C as it sometimes has a large effect on the results","ea70fdf3":"Random forest seems to be doing better than all tree boosting methods, with over 96% accuracy on the test set. In addition, 100 estimators seem to reach a plateau, and moving to 200 estimators shows almost no improvement.","da78b377":"There are 3 features which stand out: online_boarding (36.2%), inflight_wifi_service (17.7%), and type_of_travel (15.4%). Together they include almost 70% of the total importance. The next feature down (inflight_entertainment) has much lower importance (around 5%). Let's try to redo the analysis using only the leading three parameters:","09b55bb6":"It seems that even with 500 estimators (the default is 50) Adaboost cannot achieve any better results than a decision tree (92.97% vs. 94.41%). Next, we shall try gradient boosting:","d21dda49":"This result is just about as good as random forest on its own. Our final conclusion is therefore that random forest is the best model, all tree algorithms in general do better on this dataset than all non-tree algorithms, and that stacking is not helpful in improving the result beyond the best single model which has around 96% accuracy.","161e4d93":"Here we don't have a lot to gain by removing the long distances since up to around 4000 miles the data is pretty dense, so let's leave them as-is.","4edcaac3":"# Non-tree-based models\n\nEven though we have achieved excellent results with trees (especially random forest) we would like to try some other models, not based on trees.\n\n**Naive Bayes**","ff26dac0":"We have 24 columns of data, some are categorical (e.g. gender and satisfaction), some are integer (e.g. age, flight distance), one is float (arrival delay in minutes). Many contain discrete values between 0-5 which were probably collected using customer satisfaction surveys.","3cc32fae":"Next - the flight distances.","47fbee36":"The original times graph seems to be quadratic, and indeed the graph of its square roots seems linear. This shows the quadratic dependence of the training time on the number of instances. Training on the entire dataset is estimated to take over 3 hours. On the other hand, the testing accuracy doesn't seem to improve much as the number of training instances increases (the first value is very low but is based on only 10 instances, the others are all around 65% and are based on thousands of instances). \nWe should now look for a better kernel function to improve this accuracy:","3dac78a9":"We begin by some standard includes and accessing the data file","099ce2e8":"It is very obvious that the two delays are highly correlated (see the two whitest cells off the diagonal), and indeed the correlation value is 0.965. We can therefore use the departure delay data to impute the missing arrival delay data","dd2ad937":"We see that the features which stood out for the decision tree (online boarding, inflight wifi service and personal travel) are the first two and the last features in this list. However, additonal features stand out such as customer loyalty, customer class, onboard service and checkin service. All the other features have a coefficent below 0.3 in absolute value, so are probsbly less important.\n\n**Support Vector Machines**\n\nFinally, we turn to SVM. Fitting an SVM model to the data is time-consuming as its time should be a quadratic function of the number of instances used. We can see this below:\n","ea8d3400":"Only one column (arrival delay) has any missing values, and the number is not large. We could just remove the relevant instances, but something in the correlation diagrams above indicates that perhaps we could impute them! For this we first want to check how much the arrival delay and departure delay are correlated:","2f8f8c45":"All the data is now numeric and we have added one column (from the \"customer_class\" column which was split into two by our dummy variables). In addition, we can see for the first time statistics for the categorical data. For example: 49.2% of passengers are male (so 50.8% are female), 82% are loyal and 18% disloyal, 31% travel for personal reasons and 69% for business, the split between classes is 44.8% Economy, 7.2% Economy plus and the rest (48%) business, and most importantly, 43% are satisfied and so 57% are unsatisfied. This means our target are almost balanced and in any case, not too skewed.\n\n# Pairplot\n\nLet's have a quick look at the feature correlation and relationship with the target. To save time we only run the initial analysis on about 10% of the data:","8bf42bd4":"Looking at the pairplot, we can see that there are quite a few features on the diagonal (you might want to double-click to zoom in) where the red and blue distributions (for different labels) are not overlapping. This is good news as these features may have some power to predict our target. Let us focus on some of these features:","f8a6e40c":"# Outliers\nMost columns in our data have a relatively narrow range of possibilities (such as the values from 0 to 5). In this context, the flight distance and departure and arrival delays stand out. Let's take a closer look:","79270a28":"**So our leading 3 parameters can predict both the training and test sets to about 89% accuracy, with a much simpler tree (depth 9, and only 45 leaves), which is pretty cool!**\n\n# Other Tree-based algorithms\nLet's go back to the full dataset and see if other tree-based algorithms can improve over the results of a single tree. First - we'll try Adaboost.\n","044abb59":"So by dropping only 46 instances we get rid of all delays larger than 500 minutes. Let's do that.","7e862743":"**Logistic Regression**","b6d89506":"# **Missing Data**\nNext, we look at the data to find if anything is missing","a34175ac":"So, no dramatic effect of C on performance after all. \n\nUsing all the information above, let's see if we can stack several models and get something better than the best of them. We'll use a decision tree of depth 8 (where we know overfitting begins), gradient boosting, random forest, naive Bayes and Logistic regression. Adding SVC with a linear Kernel would have been nice, but training it on the entire dataset will be very time consuming, so we'll pass on that:","90e9ccd5":"Unsurprisingly, the decision tree can perfectly model the training set. More informatively, it gets more than 94% of the test set correct which means the data is amenable to machine learning. We also learn that the perfect tree has depth 36 and more than 4000 leaves, which is quite large. Let us try to see how the depth affects accuracy:","926e4613":"**K-Nearest Neighbors**","e385c5ec":"Here we finally surpass the single decision tree with a test score > 95% but 200 estimators were necessary for that and 100 were not enough. Next, we try a random forest:","44db3a90":"# Get Data","2aaec706":"So far this has the best score of all the non-tree models, but it is way behind the tree-based models. For logistic regression it is also possible to look at the model coefficients. Features which have the biggest impact on the predictions should have high absolute-value coefficients:","62149f22":"# Prepare data for modeling\nNow that the data is clean and numerical, we want to separate features from targets, and split it between training and test sets. We also remove the redundant index column (unnamed) in the process, leaving 23 features to model on. Finally, we normalize the data so that each column has values between 0 and 1.","d9dc569b":"We are now ready to begin modeling.\n\n# Decision Tree model\nArguably, the most straightforward way to predict the targets from the features is using a decision tree. We use the sklearn model, with unrestricted depth:"}}