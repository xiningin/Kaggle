{"cell_type":{"ef3b20bc":"code","f5f9accc":"code","21025837":"code","38858518":"code","0f88cdb3":"code","27155b52":"code","eddb574c":"markdown","02a77e50":"markdown","b7aee59b":"markdown","4b7c2e8f":"markdown","06edcba0":"markdown","0674f6b3":"markdown","54aec970":"markdown","f9311601":"markdown","946c7469":"markdown","687855d7":"markdown"},"source":{"ef3b20bc":"import numpy as np \nimport pandas as pd\nfrom sklearn import datasets\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import f1_score, accuracy_score\n\n%matplotlib inline\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt","f5f9accc":"iris = datasets.load_iris()\n# We will use the petal length, petal width as features\nX = iris[\"data\"][:, (2, 3)]\n# We will also add a bias term in each row\nX = np.c_[np.ones([len(X), 1]), X]\n# Turn the dataset labels to a binary classification problem, 1 if Iris virginica, else 0\ny = (iris[\"target\"] == 2).astype(np.int).reshape(-1, 1)\n# Split to train and test datasets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","21025837":"def sigmoid(t):\n    return 1 \/ (1 + np.exp(-t))\n\ndef gradient_descent(X, y, theta, eta):\n    m = len(X)\n    a = sigmoid(X.dot(theta))\n    error = a - y\n    gradients = (1\/m) * X.T.dot(error)\n    theta = theta - eta * gradients\n    return theta, gradients\n\ndef log_loss(y, p):\n    epsilon = 1e-6\n    # Add a small value in case p is 0 which would cause an error\n    p = p + epsilon\n    return -np.mean(y * np.log(p) + ((1 - y) * np.log(1 - p)))\n                                        \ndef l2_penalty(loss, a, theta):\n    # See usage for info\n    return loss + a * (1\/2 * np.sum(theta[1:]))","38858518":"class LogisticRegression():\n    \n    def __init__(self, eta=0.1, alpha=0.1):        \n        # First we initialize the hyperparameters for our model\n        self.eta = eta\n        self.alpha = alpha\n        self.use_regularizer = alpha > 0\n        \n        # We also add some parameters for internal use\n        self.min_cost = float('inf')\n        self.best_theta = None\n        self.best_epoch = 0\n        self.cost_history = []\n        \n        \n    def fit(self, X, y, n_epochs):\n        # Add a seed for predictable results\n        np.random.seed(42)\n        # Random parameter initialization\n        self.theta = np.random.randn(X.shape[1], 1) \n        \n        # Start training for number of epochs\n        for epoch in range(n_epochs):\n            # Make predictions using sigmoid and calculate logistic loss\n            logits = X.dot(self.theta)\n            preds = sigmoid(logits)\n            cost = log_loss(y, preds)\n            \n            # Add l2 regularization penalty to the loss\n            if self.use_regularizer:\n                cost = l2_penalty(log_loss(y, preds), self.alpha, self.theta)\n        \n            self.cost_history.append(cost)\n    \n            if epoch % 500 == 0:\n                print((epoch, cost))\n        \n            # Add early stopping. We will not actually stop but rather save the\n            # theta that had the lowest cost. A thing to note is that this way\n            # will only work with batch gradient descent. Mini-Batch and Stohastic\n            # will need a more sophisticated stopping mechanism\n            if cost < self.min_cost:\n                self.best_epoch = epoch\n                self.min_cost = cost\n                self.best_theta = self.theta\n\n            # Perform a gradient descent step\n            self.theta, gradients = gradient_descent(X_train, y_train, self.theta, self.eta)\n    \n            # Apply regularization to gradient descent. This regularization assumes that the \n            # first parameter of theta, theta0 is the bias term so it does not need to be regularized\n            if self.use_regularizer:\n                gradients + np.r_[np.zeros((1, self.theta.shape[1])), self.alpha * self.theta[1:]]\n            \n        print('Best parameters at epoch: ', self.best_epoch)\n                \n                \n    def predict(self, X):\n        # We will use the theta that had the lowest cost to make predictions\n        y_prob = sigmoid(X_test.dot(self.best_theta))\n        # We will use 0.5, which is the typical decision boundary \n        # for logistic regression. All propabilities with a score\n        # greater than 0.5 will be considered positive\n        y_predict = y_prob > 0.5\n        print(['Accuracy', accuracy_score(y_predict, y_test), 'f1 score', f1_score(y_predict, y_test)])\n\n        return y_predict","0f88cdb3":"n_epochs = 10000\nlog_reg = LogisticRegression(alpha=0.0)\nlog_reg.fit(X_train, y_train, n_epochs)\npredictions = log_reg.predict(X_test)\n\nplt.figure(figsize=(9, 3))\nplt.plot(range(n_epochs), log_reg.cost_history, \"b\")\nplt.show()","27155b52":"n_epochs = 10000\nlog_reg = LogisticRegression(alpha=0.12)\nlog_reg.fit(X_train, y_train, n_epochs)\npredictions = log_reg.predict(X_test)\n\nplt.figure(figsize=(9, 3))\nplt.plot(range(n_epochs), log_reg.cost_history, \"g\")\nplt.show()","eddb574c":"Thanks for reading, upvote if you found anything useful and see you around \u270c\ufe0f","02a77e50":"# Logistic Regression","b7aee59b":"**Train Model**\n\nWe will create a class for Logistic regression, kinda like Sklearn does","4b7c2e8f":"# Make Predictions","06edcba0":"> We will instantiate a new Logistic regression model, notice the alpha hyperparameter that controls regularization. It is set at 0 which will result in not using regularization. The cost function keeps decreasing until the end. Also notice the best epoch and f1 score","0674f6b3":"> This time we will create a new model but we will add some value to alpha in order to apply regularization. To see its effects we have to notice some things:\n\n> * First is that the best epoch is much smaller this time which means the model came by the best theta values earlier in training than our first model. \n\n> * Second and most important is the fact that though the cost did not get as low as the first model the f1 score is much higher and that means that this model generalized better to data that it has not trained on and so it is the superior model","54aec970":"# Thank you","f9311601":"![](https:\/\/miro.medium.com\/max\/1000\/1*Hh53mOF4Xy4eORjLilKOwA.png)\n\n*We will use the iris dataset from sklearn to use with our logistic regression classifier*","946c7469":"# Get the Data","687855d7":"**Helper functions**\n\nLets add some helper function that will come in handy for performing **Logistic Regression**"}}