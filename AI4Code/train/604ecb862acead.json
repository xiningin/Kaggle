{"cell_type":{"cca1617e":"code","5dcdb0f9":"code","42679ea8":"code","a83e8b54":"code","78076a8c":"code","614e7b8a":"code","e0d64edb":"code","6fb96ab7":"markdown","a9624a2c":"markdown","14e0fa91":"markdown","73a6fc41":"markdown"},"source":{"cca1617e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nimport warnings\nfrom textwrap import wrap\nsns.set(style=\"whitegrid\")\n\n#turn off warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn\n\n# Load data\nsurvey_data = pd.read_csv('..\/input\/multipleChoiceResponses.csv')\nsurvey_schema = pd.read_csv('..\/input\/SurveySchema.csv')\n\n# Split out titles from the first row of multiple choice response survey data\nsurvey_data_titles = survey_data.iloc[0]\nsurvey_data = survey_data.iloc[1:]","5dcdb0f9":"# Describe data\nsurvey_data.describe(include = 'all')","42679ea8":"# Get columns that will do well in correlation\ncorr_columns = [cname for cname in survey_data.columns \n                if survey_data[cname].count()\/len(survey_data[cname]) > 0.5\n                and survey_data[cname].nunique() > 2\n                and survey_data[cname].mode().iloc[0] != -1\n               ]\n\n# Factorize data so it does well in correlation\ncorr_data = survey_data[corr_columns].apply(lambda x: pd.factorize(x)[0])\n\n# Run correlation\ncorrmat = corr_data.corr()\n\n# Show correlations on a heatmap\nplt.subplots(figsize=(12,9))\nplt.title(\"Correlation between most data and all participants\")\nmask = np.zeros_like(corrmat, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corrmat, mask=mask, vmax=0.9, cmap=\"YlGnBu\",\n            square=True, cbar_kws={\"shrink\": .5})","a83e8b54":"# Get columns that will do well in correlation\ncorr_columns = [\"Q1\",\"Q2\",\"Q3\",\"Q4\",\"Q5\",\"Q6\",\"Q7\",\"Q8\",\"Q9\",\"Q10\"]\ncorr_data = survey_data[corr_columns]\n\n# Whittle data down to just non-students\ncorr_data = corr_data[corr_data[\"Q6\"].isin(['Not employed','Student']) == False]\n\n# Factorize data so it does well in correlation\ncorr_data = corr_data.apply(lambda x: pd.factorize(x)[0])\n\n# Layer better titles in for each column\nfor i in corr_columns:\n    new_title = '\\n'.join(wrap(survey_data_titles[i],30))\n    corr_data.rename(columns={i: new_title}, inplace=True)\n\n# Run correlation\ncorrmat = corr_data.corr()\n\n# Show correlations on a heatmap\nplt.subplots(figsize=(12,9))\nplt.title(\"Correlation of a few key columns for the workforce\")\nmask = np.zeros_like(corrmat, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corrmat, mask=mask, vmax=0.9, cmap=\"YlGnBu\",\n            square=True, cbar_kws={\"shrink\": .5})","78076a8c":"# Get columns that will do well in correlation\ncorr_columns = [\"Q1\",\"Q2\",\"Q3\",\"Q4\",\"Q5\",\"Q6\"]\ncorr_data = survey_data[corr_columns]\n\n# Whittle data down to just students\ncorr_data = corr_data[corr_data[\"Q6\"].isin(['Not employed','Student'])]\n\n# Factorize data so it does well in correlation\ncorr_data = corr_data.apply(lambda x: pd.factorize(x)[0])\n\n# Layer better titles in for each column\nfor i in corr_columns:\n    new_title = '\\n'.join(wrap(survey_data_titles[i],30))\n    corr_data.rename(columns={i: new_title}, inplace=True)\n\n# Run correlation\ncorrmat = corr_data.corr()\n\n# Show correlations on a heatmap\nplt.subplots(figsize=(12,9))\nplt.title(\"Correlation of a few key columns for students\")\nmask = np.zeros_like(corrmat, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corrmat, mask=mask, vmax=0.9, cmap=\"YlGnBu\",\n            square=True, cbar_kws={\"shrink\": .5})","614e7b8a":"# Create a countplot of the Q1 through Q10 columns\nfor i in range(1,11):\n    col_name = 'Q'+str(i)\n    col_title = '\\n'.join(wrap(survey_data_titles[col_name],30))\n    col_name_list = [col_name]\n    survey_data[col_name_list].head(5)\n    fig = plt.figure(figsize=(12, 5))\n    ax = sns.countplot(data = survey_data[col_name_list], \n                       x=col_name,\n                       order = survey_data[col_name].value_counts().index\n                      )\n    ax.set_title(col_title)\n    for item in ax.get_xticklabels():\n        item.set_rotation(90)","e0d64edb":"# set up plot data and columns to create histograms for all of the columns\nplot_columns = survey_data.columns\nplot_column_titles = survey_data_titles[plot_columns]\nplot_data = survey_data[plot_columns]\nnum_data_columns = len(plot_columns)\n\n# metadata for plot\nnum_plot_columns = 5\nheight_subplots = 3\nvertical_above_subplots = 2\nmax_bins = 10\nnum_plot_rows = math.ceil(num_data_columns\/num_plot_columns)\n\n# initialize plot figure\nfig = plt.figure(figsize=(12, num_plot_rows*(height_subplots+vertical_above_subplots)))\n\n# loop through columns to generate subplots\nfor i in plot_columns:\n    col_num = plot_data.columns.get_loc(i)\n    \n    ax = plt.subplot(num_plot_rows, num_plot_columns, col_num+1)\n    title = '\\n'.join(wrap(plot_column_titles[i],20))\n    ax.set_title(title)\n    ax.get_yaxis().set_visible(False)\n    # f.axes.set_ylim([0, train.shape[0]])\n    \n    # factorize object data\n    if plot_data[i].dtype == \"object\":\n        plot_data[i] = pd.factorize(plot_data[i])[0]\n    \n    unique_vals = np.size(plot_data[i].unique())\n    \n    # add plots of data\n    if plot_data[i].dtype == \"object\" and unique_vals < max_bins:\n        sns.countplot(data = plot_data[i], x=i, ax=ax)\n    else:\n        if unique_vals < max_bins:\n            bins = unique_vals\n        else:\n            bins = max_bins\n        sns.distplot(plot_data[i], bins=bins, ax=ax)\n\nplt.subplots_adjust(top=vertical_above_subplots)\nplt.tight_layout()","6fb96ab7":"----\n**2)** Create correlation map of key features...","a9624a2c":"----\n**1)** Describe the data...","14e0fa91":"----\n**3)** Plot counts and histograms of the data...","73a6fc41":"**Exporation of the Kaggle ML and DS Survey**\n\nThis notebook covers some initial exploration of the survey data from the Kaggle 2018 Machine Learning and Data Science survey.\n\nI focused heavily on correlation and histograms for now."}}