{"cell_type":{"98225c02":"code","fb0716e2":"code","6b0dfcfc":"code","ba168303":"code","68294200":"code","37c6d9e3":"code","98d2b8a1":"code","7050430e":"code","3a165b74":"code","089de71c":"code","dbe42e14":"code","790338a7":"markdown","5be2bb90":"markdown","3e8d7668":"markdown","89a6cb5b":"markdown","168318f4":"markdown"},"source":{"98225c02":"import numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fb0716e2":"train_transaction = pd.read_csv('..\/input\/train_transaction.csv', index_col='TransactionID')\ntrain_identity = pd.read_csv('..\/input\/train_identity.csv', index_col='TransactionID')\ntrain = train_transaction.merge(train_identity, how='left', left_index=True, right_index=True)\ndel train_transaction, train_identity","6b0dfcfc":"# From https:\/\/www.kaggle.com\/pavelvpster\/ieee-fraud-eda-lightgbm-baseline\n\nfrom sklearn.preprocessing import LabelEncoder\n\ndef encode_categorial_features_fit(df, columns_to_encode):\n    encoders = {}\n    for c in columns_to_encode:\n        if c in df.columns:\n            encoder = LabelEncoder()\n            encoder.fit(df[c].astype(str).values)\n            encoders[c] = encoder\n    return encoders\n\ndef encode_categorial_features_transform(df, encoders):\n    out = pd.DataFrame(index=df.index)\n    for c in encoders.keys():\n        if c in df.columns:\n            out[c] = encoders[c].transform(df[c].astype(str).values)\n    return out\n\ncategorial_features_columns = [\n    'id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21',\n    'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29', 'id_30', 'id_31',\n    'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38',\n    'DeviceType', 'DeviceInfo', 'ProductCD', 'P_emaildomain', 'R_emaildomain',\n    'card1', 'card2', 'card3', 'card4', 'card5', 'card6',\n    'addr1', 'addr2',\n    'M1', 'M2', 'M3', 'M4', 'M5', 'M6', 'M7', 'M8', 'M9',\n    'P_emaildomain_vendor', 'P_emaildomain_suffix', 'P_emaildomain_us',\n    'R_emaildomain_vendor', 'R_emaildomain_suffix', 'R_emaildomain_us'\n]\n\n\ncategorial_features_encoders = encode_categorial_features_fit(train, categorial_features_columns)\ntemp = encode_categorial_features_transform(train, categorial_features_encoders)\ncolumns_to_drop = list(set(categorial_features_columns) & set(train.columns))\ntrain = train.drop(columns_to_drop, axis=1).merge(temp, how='left', left_index=True, right_index=True)\ndel temp","ba168303":"# From https:\/\/www.kaggle.com\/gemartin\/load-data-reduce-memory-usage\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n#        else:\n#            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df\n\n\ntrain = reduce_mem_usage(train)","68294200":"y_train = train['isFraud'].copy()\nx_train = train.drop('isFraud', axis=1)\ndel train","37c6d9e3":"from sklearn.model_selection import KFold\nimport lightgbm as lgb\nfrom sklearn.metrics import roc_auc_score\n\n\ndef test(x_train, y_train):\n    \n    params = {\n        'objective': 'binary',\n        'metric': 'auc',\n        'is_unbalance': False,\n        'boost_from_average': True,\n        'num_threads': 4,\n        \n        'num_leaves': 500,\n        'min_data_in_leaf': 25,\n        'max_depth': 50\n    }\n    \n    scores = []\n    \n    cv = KFold(n_splits=5)\n    for train_idx, valid_idx in cv.split(x_train, y_train):\n        \n        x_train_train = x_train.iloc[train_idx]\n        y_train_train = y_train.iloc[train_idx]\n        x_train_valid = x_train.iloc[valid_idx]\n        y_train_valid = y_train.iloc[valid_idx]\n        \n        lgb_train = lgb.Dataset(data=x_train_train.astype('float32'), label=y_train_train.astype('float32'))\n        lgb_valid = lgb.Dataset(data=x_train_valid.astype('float32'), label=y_train_valid.astype('float32'))\n        \n        lgb_model = lgb.train(params, lgb_train, valid_sets=lgb_valid, verbose_eval=100)\n        y = lgb_model.predict(x_train_valid.astype('float32'), num_iteration=lgb_model.best_iteration)\n        \n        score = roc_auc_score(y_train_valid.astype('float32'), y)\n        print('Fold score:', score)\n        scores.append(score)\n    \n    average_score = sum(scores) \/ len(scores)\n    print('Average score:', average_score)\n    return average_score","98d2b8a1":"import itertools\nfrom sklearn.preprocessing import LabelEncoder\n\n\nbaseline_score = test(x_train, y_train)\nprint('Baseline score =', baseline_score)\n\n# Modify this list\nfeatures_to_test = ['id_02', 'id_20', 'D8', 'D11', 'DeviceInfo']\n\nnew_features = {}\n\nfor features in itertools.combinations(features_to_test, 2):\n    new_feature = features[0] + '__' + features[1]\n    print('Test feature:', new_feature, '\/ interaction of', features[0], 'and', features[1])\n    \n    temp = x_train.copy()\n    temp[new_feature] = temp[features[0]].astype(str) + '_' + temp[features[1]].astype(str)\n    temp[new_feature] = LabelEncoder().fit_transform(temp[new_feature].values)\n    \n    score = test(temp, y_train)\n    print('Score =', score)\n    \n    new_features[new_feature] = score","7050430e":"new_features_df = pd.DataFrame.from_dict(new_features, orient='index', columns=['score']).sub(baseline_score)\nnew_features_df","3a165b74":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\nplt.figure(figsize=(12, 12))\np = sns.barplot(x=new_features_df.index, y='score', data=new_features_df, color='gray')\n\n# Rotate labels\nfor x in p.get_xticklabels():\n    x.set_rotation(90)\n\nplt.show()","089de71c":"selected_features = list(new_features_df[new_features_df['score']>0].index)\nselected_features","dbe42e14":"new_x_train = x_train.copy()\n\nfor feature in selected_features:\n    feature_1, feature_2 = feature.split('__')\n    \n    new_x_train[feature] = new_x_train[feature_1].astype(str) + '_' + new_x_train[feature_2].astype(str)\n    new_x_train[feature] = LabelEncoder().fit_transform(new_x_train[feature].values)\n\nprint('Score with selected features =', test(new_x_train, y_train))","790338a7":"## Test all new features together","5be2bb90":"## Select features","3e8d7668":"## Load and prepare data","89a6cb5b":"## LightGBM","168318f4":"## Test features interaction"}}