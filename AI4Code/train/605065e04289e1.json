{"cell_type":{"51f41f58":"code","67d4f276":"code","2b7289b0":"code","d24bc8e1":"code","15f848a4":"code","85b9b195":"code","a78ac281":"code","8e9f0f8a":"code","c885aeae":"code","bdd6fae1":"code","7ca1734d":"code","0e3d3170":"code","aa12278a":"code","6cb48f82":"code","37a7e1ce":"code","370b9ed7":"code","779176f9":"code","1c1263e7":"code","4c475a2f":"code","27dfc282":"code","67edd9bf":"code","23718551":"code","b77e1fb5":"code","41cffb09":"code","a2314569":"code","925169d0":"code","e4b0df65":"code","81433c80":"code","9e389302":"code","496631ff":"code","a266dd12":"code","30611b9c":"code","e4ffdb0c":"code","d909d285":"code","55eaf426":"code","88433ea1":"code","35386a3e":"code","aa6d9647":"code","43a3fed4":"code","47b8756d":"code","c7a21eda":"code","ce49a65d":"code","c1370be2":"code","4deaed8e":"code","05b371a8":"code","64275968":"code","e2f8b559":"code","f48f3247":"code","035994a0":"code","5479d0ba":"code","a466a085":"code","2c733c46":"code","95d51d0d":"code","e56979a7":"code","adbb7701":"code","01641de9":"code","78d3511a":"code","2825e9cb":"code","81fc5a02":"markdown","d8f27700":"markdown","25db334b":"markdown","a5e0ef4b":"markdown","8d0e1662":"markdown","da653a34":"markdown","47210cd7":"markdown","e9fec5ae":"markdown","f199a2be":"markdown","a2f38571":"markdown","0c766403":"markdown","0f46441b":"markdown","6bdc2056":"markdown","f06c168e":"markdown","2f1b69f6":"markdown","683de863":"markdown","08a7477a":"markdown","ca23d4de":"markdown","3f27b697":"markdown","1a6b3686":"markdown","a0305419":"markdown","5bc19a8f":"markdown","759f3927":"markdown","f8451575":"markdown","1b3843eb":"markdown","f1d4058b":"markdown","547dc50f":"markdown","64b6a683":"markdown","2c23c3bc":"markdown","e29af451":"markdown","7acd877b":"markdown"},"source":{"51f41f58":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns \nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","67d4f276":"train_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ntrain_data.head()","2b7289b0":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()","d24bc8e1":"train_data['train_test'] = 1\ntest_data['train_test'] = 0\ntest_data['Survived'] = np.NaN\nall_data = pd.concat([train_data,test_data])\n\n%matplotlib inline\nall_data.columns","15f848a4":"women = train_data.loc[train_data.Sex == 'female'][\"Survived\"]\nrate_women = sum(women)\/len(women)\n\nmen = train_data.loc[train_data.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\n\nprint(\"% of women who survived:\", rate_women)\nprint(\"% of men who survived:\", rate_men)","85b9b195":"train_data.info()\ntrain_data.describe()","a78ac281":"train_data.describe().columns","8e9f0f8a":"#Group variables into numerical and categorical groups\ndf_num = train_data[['Age','SibSp','Parch','Fare']]\ndf_cat = train_data[['Survived','Pclass','Sex','Ticket','Cabin','Embarked']]","c885aeae":"#distributions for all numeric variables \nfor i in df_num.columns:\n    plt.hist(df_num[i])\n    plt.title(i)\n    plt.show()","bdd6fae1":"print(df_num.corr())\nsns.heatmap(df_num.corr())","7ca1734d":"# compare survival rate across Age, SibSp, Parch, and Fare \npd.pivot_table(train_data, index = 'Survived', values = ['Age','SibSp','Parch','Fare'])","0e3d3170":"for i in df_cat.columns:\n    sns.barplot(df_cat[i].value_counts().index,df_cat[i].value_counts()).set_title(i)\n    plt.show()\n ","aa12278a":"#Comparing survival and each of these categorical variables\nprint(pd.pivot_table(train_data, index = 'Survived', columns = 'Pclass', values = 'Ticket' ,aggfunc ='count'))\nprint()\nprint(pd.pivot_table(train_data, index = 'Survived', columns = 'Sex', values = 'Ticket' ,aggfunc ='count'))\nprint()\nprint(pd.pivot_table(train_data, index = 'Survived', columns = 'Embarked', values = 'Ticket' ,aggfunc ='count'))","6cb48f82":"df_cat.Cabin\ntrain_data['cabin_multiple'] = train_data.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\n# after looking at this, we may want to look at cabin by letter or by number. Let's create some categories for this \n# letters \n# multiple letters \ntrain_data['cabin_multiple'].value_counts()","37a7e1ce":"pd.pivot_table(train_data, index = 'Survived', columns = 'cabin_multiple', values = 'Ticket' ,aggfunc ='count')","370b9ed7":"#creates categories based on the cabin letter (n stands for null)\n#in this case we will treat null values like it's own category\n\ntrain_data['cabin_adv'] = train_data.Cabin.apply(lambda x: str(x)[0])","779176f9":"#comparing surivial rate by cabin\nprint(train_data.cabin_adv.value_counts())\npd.pivot_table(train_data,index='Survived',columns='cabin_adv', values = 'Name', aggfunc='count')","1c1263e7":"#understand ticket values better \n#numeric vs non numeric \ntrain_data['numeric_ticket'] = train_data.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\ntrain_data['ticket_letters'] = train_data.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) >0 else 0)","4c475a2f":"train_data['numeric_ticket'].value_counts()","27dfc282":"#lets us view all rows in dataframe through scrolling. This is for convenience \npd.set_option(\"max_rows\", None)\ntrain_data['ticket_letters'].value_counts()","67edd9bf":"#difference in numeric vs non-numeric tickets in survival rate \npd.pivot_table(train_data,index='Survived',columns='numeric_ticket', values = 'Ticket', aggfunc='count')","23718551":"#feature engineering on person's title \ntrain_data.Name.head(50)\ntrain_data['name_title'] = train_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n#mr., ms., master. etc","b77e1fb5":"train_data['name_title'].value_counts()","41cffb09":"#create all categorical variables that we did above for both training and test sets \nall_data['cabin_multiple'] = all_data.Cabin.apply(lambda x: 0 if pd.isna(x) else len(x.split(' ')))\nall_data['cabin_adv'] = all_data.Cabin.apply(lambda x: str(x)[0])\nall_data['numeric_ticket'] = all_data.Ticket.apply(lambda x: 1 if x.isnumeric() else 0)\nall_data['ticket_letters'] = all_data.Ticket.apply(lambda x: ''.join(x.split(' ')[:-1]).replace('.','').replace('\/','').lower() if len(x.split(' ')[:-1]) >0 else 0)\nall_data['name_title'] = all_data.Name.apply(lambda x: x.split(',')[1].split('.')[0].strip())\n\n#impute nulls for continuous data \n#all_data.Age = all_data.Age.fillna(train_data.Age.mean())\nall_data.Age = all_data.Age.fillna(train_data.Age.median())\n#all_data.Fare = all_data.Fare.fillna(train_data.Fare.mean())\nall_data.Fare = all_data.Fare.fillna(train_data.Fare.median())\n\n#drop null 'embarked' rows. Only 2 instances of this in training and 0 in test \nall_data.dropna(subset=['Embarked'],inplace = True)\n\n#tried log norm of sibsp (not used)\nall_data['norm_sibsp'] = np.log(all_data.SibSp+1)\nall_data['norm_sibsp'].hist()\n\n# log norm of fare (used)\nall_data['norm_fare'] = np.log(all_data.Fare+1)\nall_data['norm_fare'].hist()\n\n# converted fare to category for pd.get_dummies()\nall_data.Pclass = all_data.Pclass.astype(str)\n\n#created dummy variables from categories (also can use OneHotEncoder)\nall_dummies = pd.get_dummies(all_data[['Pclass','Sex','Age','SibSp','Parch','norm_fare','Embarked','cabin_adv',\n                                       'cabin_multiple','numeric_ticket','name_title','train_test']])\n\n#Split to train test again\nX_train = all_dummies[all_dummies.train_test == 1].drop(['train_test'], axis =1)\nX_test = all_dummies[all_dummies.train_test == 0].drop(['train_test'], axis =1)\n\n\ny_train = all_data[all_data.train_test==1].Survived\ny_train.shape","a2314569":"# Scale data \nfrom sklearn.preprocessing import StandardScaler\nscale = StandardScaler()\nall_dummies_scaled = all_dummies.copy()\nall_dummies_scaled[['Age','SibSp','Parch','norm_fare']]= scale.fit_transform(all_dummies_scaled[['Age','SibSp','Parch','norm_fare']])\nall_dummies_scaled\n\nX_train_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 1].drop(['train_test'], axis =1)\nX_test_scaled = all_dummies_scaled[all_dummies_scaled.train_test == 0].drop(['train_test'], axis =1)\n\ny_train = all_data[all_data.train_test==1].Survived","925169d0":"from sklearn.model_selection import cross_val_score\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import tree\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC","e4b0df65":"#I usually use Naive Bayes as a baseline for my classification tasks \ngnb = GaussianNB()\ncv = cross_val_score(gnb,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","81433c80":"lr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","9e389302":"lr = LogisticRegression(max_iter = 2000)\ncv = cross_val_score(lr,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","496631ff":"dt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","a266dd12":"dt = tree.DecisionTreeClassifier(random_state = 1)\ncv = cross_val_score(dt,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","30611b9c":"knn = KNeighborsClassifier()\ncv = cross_val_score(knn,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","e4ffdb0c":"knn = KNeighborsClassifier()\ncv = cross_val_score(knn,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","d909d285":"rf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,X_train,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","55eaf426":"rf = RandomForestClassifier(random_state = 1)\ncv = cross_val_score(rf,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","88433ea1":"svc = SVC(probability = True)\ncv = cross_val_score(svc,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","35386a3e":"from xgboost import XGBClassifier\nxgb = XGBClassifier(random_state =1)\ncv = cross_val_score(xgb,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","aa6d9647":"#Voting classifier takes all of the inputs and averages the results. For a \"hard\" voting classifier each classifier gets 1 vote \"yes\" or \"no\" and the result is just a popular vote. For this, you generally want odd numbers\n#A \"soft\" classifier averages the confidence of each of the models. If a the average confidence is > 50% that it is a 1 it will be counted as such\nfrom sklearn.ensemble import VotingClassifier\nvoting_clf = VotingClassifier(estimators = [('lr',lr),('knn',knn),('rf',rf),('gnb',gnb),\n                                            ('svc',svc),('xgb',xgb)], voting = 'soft')","43a3fed4":"cv = cross_val_score(voting_clf,X_train_scaled,y_train,cv=5)\nprint(cv)\nprint(cv.mean())","47b8756d":"voting_clf.fit(X_train_scaled,y_train)\ny_hat_base_vc = voting_clf.predict(X_test_scaled).astype(int)\nbasic_submission = {'PassengerId': test_data.PassengerId, 'Survived': y_hat_base_vc}\nbase_submission = pd.DataFrame(data=basic_submission)\nbase_submission.to_csv('base_submission.csv', index=False)","c7a21eda":"from sklearn.model_selection import GridSearchCV \nfrom sklearn.model_selection import RandomizedSearchCV","ce49a65d":"#simple performance reporting function\ndef clf_performance(classifier, model_name):\n    print(model_name)\n    print('Best Score: ' + str(classifier.best_score_))\n    print('Best Parameters: ' + str(classifier.best_params_))","c1370be2":"lr = LogisticRegression()\nparam_grid = {'max_iter' : [2000],\n              'penalty' : ['l1', 'l2'],\n              'C' : np.logspace(-4, 4, 20),\n              'solver' : ['liblinear']}\n\nclf_lr = GridSearchCV(lr, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_lr = clf_lr.fit(X_train_scaled,y_train)\nclf_performance(best_clf_lr,'Logistic Regression')","4deaed8e":"knn = KNeighborsClassifier()\nparam_grid = {'n_neighbors' : [3,5,7,9],\n              'weights' : ['uniform', 'distance'],\n              'algorithm' : ['auto', 'ball_tree','kd_tree'],\n              'p' : [1,2]}\nclf_knn = GridSearchCV(knn, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_knn = clf_knn.fit(X_train_scaled,y_train)\nclf_performance(best_clf_knn,'KNN')","05b371a8":"svc = SVC(probability = True)\nparam_grid = tuned_parameters = [{'kernel': ['rbf'], 'gamma': [.1,.5,1,2,5,10],\n                                  'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['linear'], 'C': [.1, 1, 10, 100, 1000]},\n                                 {'kernel': ['poly'], 'degree' : [2,3,4,5], 'C': [.1, 1, 10, 100, 1000]}]\nclf_svc = GridSearchCV(svc, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_svc = clf_svc.fit(X_train_scaled,y_train)\nclf_performance(best_clf_svc,'SVC')","64275968":"#Because the total feature space is so large, I used a randomized search to narrow down the paramters for the model. I took the best model from this and did a more granular search \n\"\"\"\nrf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [100,500,1000], \n                                  'bootstrap': [True,False],\n                                  'max_depth': [3,5,10,20,50,75,100,None],\n                                  'max_features': ['auto','sqrt'],\n                                  'min_samples_leaf': [1,2,4,10],\n                                  'min_samples_split': [2,5,10]}\n                                  \nclf_rf_rnd = RandomizedSearchCV(rf, param_distributions = param_grid, n_iter = 100, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_rf_rnd = clf_rf_rnd.fit(X_train_scaled,y_train)\nclf_performance(best_clf_rf_rnd,'Random Forest')\"\"\"","e2f8b559":"rf = RandomForestClassifier(random_state = 1)\nparam_grid =  {'n_estimators': [400,450,500,550],\n               'criterion':['gini','entropy'],\n                                  'bootstrap': [True],\n                                  'max_depth': [15, 20, 25],\n                                  'max_features': ['auto','sqrt', 10],\n                                  'min_samples_leaf': [2,3],\n                                  'min_samples_split': [2,3]}\n                                  \nclf_rf = GridSearchCV(rf, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_rf = clf_rf.fit(X_train_scaled,y_train)\nclf_performance(best_clf_rf,'Random Forest')","f48f3247":"best_rf = best_clf_rf.best_estimator_.fit(X_train_scaled,y_train)\nfeat_importances = pd.Series(best_rf.feature_importances_, index=X_train_scaled.columns)\nfeat_importances.nlargest(20).plot(kind='barh')","035994a0":"\"\"\"xgb = XGBClassifier(random_state = 1)\n\nparam_grid = {\n    'n_estimators': [20, 50, 100, 250, 500,1000],\n    'colsample_bytree': [0.2, 0.5, 0.7, 0.8, 1],\n    'max_depth': [2, 5, 10, 15, 20, 25, None],\n    'reg_alpha': [0, 0.5, 1],\n    'reg_lambda': [1, 1.5, 2],\n    'subsample': [0.5,0.6,0.7, 0.8, 0.9],\n    'learning_rate':[.01,0.1,0.2,0.3,0.5, 0.7, 0.9],\n    'gamma':[0,.01,.1,1,10,100],\n    'min_child_weight':[0,.01,0.1,1,10,100],\n    'sampling_method': ['uniform', 'gradient_based']\n}\n\n#clf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\n#best_clf_xgb = clf_xgb.fit(X_train_scaled,y_train)\n#clf_performance(best_clf_xgb,'XGB')\nclf_xgb_rnd = RandomizedSearchCV(xgb, param_distributions = param_grid, n_iter = 1000, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_xgb_rnd = clf_xgb_rnd.fit(X_train_scaled,y_train)\nclf_performance(best_clf_xgb_rnd,'XGB')\"\"\"","5479d0ba":"xgb = XGBClassifier(random_state = 1)\n\nparam_grid = {\n    'n_estimators': [450,500,550],\n    'colsample_bytree': [0.75,0.8,0.85],\n    'max_depth': [None],\n    'reg_alpha': [1],\n    'reg_lambda': [2, 5, 10],\n    'subsample': [0.55, 0.6, .65],\n    'learning_rate':[0.5],\n    'gamma':[.5,1,2],\n    'min_child_weight':[0.01],\n    'sampling_method': ['uniform']\n}\n\nclf_xgb = GridSearchCV(xgb, param_grid = param_grid, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_xgb = clf_xgb.fit(X_train_scaled,y_train)\nclf_performance(best_clf_xgb,'XGB')","a466a085":"y_hat_xgb = best_clf_xgb.best_estimator_.predict(X_test_scaled).astype(int)\nxgb_submission = {'PassengerId': test_data.PassengerId, 'Survived': y_hat_xgb}\nsubmission_xgb = pd.DataFrame(data=xgb_submission)\nsubmission_xgb.to_csv('xgb_submission3.csv', index=False)","2c733c46":"best_lr = best_clf_lr.best_estimator_\nbest_knn = best_clf_knn.best_estimator_\nbest_svc = best_clf_svc.best_estimator_\nbest_rf = best_clf_rf.best_estimator_\nbest_xgb = best_clf_xgb.best_estimator_\n\nvoting_clf_hard = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc)], voting = 'hard') \nvoting_clf_soft = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc)], voting = 'soft') \nvoting_clf_all = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc), ('lr', best_lr)], voting = 'soft') \nvoting_clf_xgb = VotingClassifier(estimators = [('knn',best_knn),('rf',best_rf),('svc',best_svc), ('xgb', best_xgb),('lr', best_lr)], voting = 'soft')\n\nprint('voting_clf_hard :',cross_val_score(voting_clf_hard,X_train,y_train,cv=5))\nprint('voting_clf_hard mean :',cross_val_score(voting_clf_hard,X_train,y_train,cv=5).mean())\n\nprint('voting_clf_soft :',cross_val_score(voting_clf_soft,X_train,y_train,cv=5))\nprint('voting_clf_soft mean :',cross_val_score(voting_clf_soft,X_train,y_train,cv=5).mean())\n\nprint('voting_clf_all :',cross_val_score(voting_clf_all,X_train,y_train,cv=5))\nprint('voting_clf_all mean :',cross_val_score(voting_clf_all,X_train,y_train,cv=5).mean())\n\nprint('voting_clf_xgb :',cross_val_score(voting_clf_xgb,X_train,y_train,cv=5))\nprint('voting_clf_xgb mean :',cross_val_score(voting_clf_xgb,X_train,y_train,cv=5).mean())","95d51d0d":"#in a soft voting classifier you can weight some models more than others. I used a grid search to explore different weightings\n#no new results here\nparams = {'weights' : [[1,1,1],[1,2,1],[1,1,2],[2,1,1],[2,2,1],[1,2,2],[2,1,2]]}\n\nvote_weight = GridSearchCV(voting_clf_soft, param_grid = params, cv = 5, verbose = True, n_jobs = -1)\nbest_clf_weight = vote_weight.fit(X_train_scaled,y_train)\nclf_performance(best_clf_weight,'VC Weights')\nvoting_clf_sub = best_clf_weight.best_estimator_.predict(X_test_scaled)","e56979a7":"#Make Predictions \nvoting_clf_hard.fit(X_train_scaled, y_train)\nvoting_clf_soft.fit(X_train_scaled, y_train)\nvoting_clf_all.fit(X_train_scaled, y_train)\nvoting_clf_xgb.fit(X_train_scaled, y_train)\n\nbest_rf.fit(X_train_scaled, y_train)\ny_hat_vc_hard = voting_clf_hard.predict(X_test_scaled).astype(int)\ny_hat_rf = best_rf.predict(X_test_scaled).astype(int)\ny_hat_vc_soft =  voting_clf_soft.predict(X_test_scaled).astype(int)\ny_hat_vc_all = voting_clf_all.predict(X_test_scaled).astype(int)\ny_hat_vc_xgb = voting_clf_xgb.predict(X_test_scaled).astype(int)","adbb7701":"#convert output to dataframe \nfinal_data = {'PassengerId': test_data.PassengerId, 'Survived': y_hat_rf}\nsubmission = pd.DataFrame(data=final_data)\n\nfinal_data_2 = {'PassengerId': test_data.PassengerId, 'Survived': y_hat_vc_hard}\nsubmission_2 = pd.DataFrame(data=final_data_2)\n\nfinal_data_3 = {'PassengerId': test_data.PassengerId, 'Survived': y_hat_vc_soft}\nsubmission_3 = pd.DataFrame(data=final_data_3)\n\nfinal_data_4 = {'PassengerId': test_data.PassengerId, 'Survived': y_hat_vc_all}\nsubmission_4 = pd.DataFrame(data=final_data_4)\n\nfinal_data_5 = {'PassengerId': test_data.PassengerId, 'Survived': y_hat_vc_xgb}\nsubmission_5 = pd.DataFrame(data=final_data_5)\n\nfinal_data_comp = {'PassengerId': test_data.PassengerId, 'Survived_vc_hard': y_hat_vc_hard, 'Survived_rf': y_hat_rf, 'Survived_vc_soft' : y_hat_vc_soft, 'Survived_vc_all' : y_hat_vc_all,  'Survived_vc_xgb' : y_hat_vc_xgb}\ncomparison = pd.DataFrame(data=final_data_comp)","01641de9":"#track differences between outputs \ncomparison['difference_rf_vc_hard'] = comparison.apply(lambda x: 1 if x.Survived_vc_hard != x.Survived_rf else 0, axis =1)\ncomparison['difference_soft_hard'] = comparison.apply(lambda x: 1 if x.Survived_vc_hard != x.Survived_vc_soft else 0, axis =1)\ncomparison['difference_hard_all'] = comparison.apply(lambda x: 1 if x.Survived_vc_all != x.Survived_vc_hard else 0, axis =1)","78d3511a":"comparison.difference_hard_all.value_counts()","2825e9cb":"#prepare submission files \nsubmission.to_csv('submission_rf.csv', index =False)\nsubmission_2.to_csv('submission_vc_hard.csv',index=False)\nsubmission_3.to_csv('submission_vc_soft.csv', index=False)\nsubmission_4.to_csv('submission_vc_all.csv', index=False)\nsubmission_5.to_csv('submission_vc_xgb2.csv', index=False)","81fc5a02":"We can view the respective null values and the statistical information for each column in train dataset.","d8f27700":"1) Drop null values from Embarked (only 2)\n\n2) Include only relevant variables (Since we have limited data, I wanted to exclude things like name and passanger ID so that we could have a reasonable number of features for our models to deal with)\nVariables: 'Pclass', 'Sex','Age', 'SibSp', 'Parch', 'Fare', 'Embarked', 'cabin_adv', 'cabin_multiple', 'numeric_ticket', 'name_title'\n\n3) Do categorical transforms on all data. Usually we would use a transformer, but with this approach we can ensure that our traning and test data have the same colums. We also may be able to infer something about the shape of the test data through this method. I will stress, this is generally not recommend outside of a competition (use onehot encoder).\n\n4) Impute data with mean for fare and age (Should also experiment with median)\n\n5) Normalized fare using logarithm to give more semblance of a normal distribution\n\n6) Scaled data 0-1 with standard scaler","25db334b":"# Titanic: Machine Learning from Disaster","a5e0ef4b":"# 3) Feature Engineering","8d0e1662":"# 7)  Model Additional Ensemble Approaches\n1) Experimented with a hard voting classifier of three estimators (KNN, SVM, RF) (81.6%)\n\n2) Experimented with a soft voting classifier of three estimators (KNN, SVM, RF) (82.3%) (Best Performance)\n\n3) Experimented with soft voting on all estimators performing better than 80% except xgb (KNN, RF, LR, SVC) (82.9%)\n\n4) Experimented with soft voting on all estimators including XGB (KNN, SVM, RF, LR, XGB) (83.5%)","da653a34":"# 6) Model Tuned Performance\n\nAfter getting the baselines, let's see if we can improve on the indivdual model results!I mainly used grid search to tune the models. I also used Randomized Search for the Random Forest and XG boosted model to simplify testing time.\n\nModel\t               Baseline\tTuned Performance\n\nNaive Bayes\t          72.6%\t         NA\n\nLogistic Regression\t  82.1%\t        82.6%\n\nDecision Tree\t      77.6%\t         NA\nK Nearest Neighbor\t  80.5%\t        83.0%\nRandom Forest\t      80.6%\t        83.6\nSupport Vector Classifier\t83.2%\t83.2%\nXtreme Gradient Boosting\t81.8%\t85.3%","47210cd7":"There are two main categories for data:\n\n1) Numerical data\n\n   -the data is grouped by numbers\n   \n   -discrete and continuous data \n   \n   -eg: number of apples,the room temperature,the height and so on\n\n2) Categorical data\n\n   -the data is grouped by categories\n   \n   -eg: the type of ice-cream,the blood groups and the difficulty level\n   \nIt's crucial to analyse variables by its categories.As there is unique effective way deal with each of them!","e9fec5ae":"I'm absolutely new to data science and machine learning so this is just my first exercise on learning data science projects on Kaggle.However,I will keep up all the efforts to learn the data science.Feel free to comment on some useful beginner projects that we can use to learn and improve the basic skills.Moreover,we can keep connecting and learn data science together.Upvote it if you think it is useful for the reference I provide in this notebook!Thank you and have a nice day =D !","f199a2be":"In this stage,our task is to view and investigate the pattern of each variable in the train set.There are a total of 891 rows of train.csv and 418 rows of test.csv.","a2f38571":"Load the data from train dataset in Kaggle.The command .head() is to show you the header of each columns in datasets and its respective contents(first 5 rows including the labels row).This shows that the dataset is successfully loaded in this notebook.","0c766403":"# 5) Model Building (Baseline Validation Performance)\n\nBefore going further, I like to see how various different models perform with default parameters. I tried the following models using 5 fold cross validation to get a baseline. With a validation set basline, we can see how much tuning improves each of the models. Just because a model has a high basline on this validation set doesn't mean that it will actually do better on the eventual test set.\n\n* Naive Bayes (72.6%)\n* Logistic Regression (82.1%)\n* Decision Tree (77.6%)\n* K Nearest Neighbor (80.5%)\n* Random Forest (80.6%)\n* Support Vector Classifier (83.2%)\n* Xtreme Gradient Boosting (81.8%)\n* Soft Voting Classifier - All Models (82.8%)","0f46441b":"# 1) Load the data","6bdc2056":"The result shown that majority of the women on board survived during the disaster if compared to men.Hence, gender can be considered as a strong indicator of survival in Titanic shipwreck.","f06c168e":"The respective relationship of cabin and ticket with survival rate are still unclear.Feature engineering can assist to improve on it.","2f1b69f6":"There are three files provided in the data tab for this competition: \n(1) train.csv\n(2) test.csv\n(3) gender_submission.csv\n\nGiven that the sample submission file (gender_submission.csv) assumes that all female passengers survived and all male passengers died in the disaster.Is this reasonable in train.csv?","683de863":"# 9) Reference","08a7477a":"(i) Cabin - Simplify cabins by cabin letter (cabin_adv) or the purchase of tickets across multiple cabins (cabin_multiple)","ca23d4de":"(ii) Tickets - categorized them into different ticket types like numeric and non-numeric","3f27b697":"Same goes to our test dataset.Above displayed table indicated that the test dataset is successfully loaded in this notebook.","1a6b3686":"The display tables show the correspond correlation between each variables.","a0305419":"(i) Some basic information on Kaggle platform and its competitions\n\n>   Alexis Cook\u2019s Titanic Tutorial-https:\/\/www.kaggle.com\/alexisbcook\/titanic-tutorial\n    \n(ii) Example of data science project pipelines on Titanic shipwreck dataset\n\n>   Ken Jee's Titanic Project Example-https:\/\/www.kaggle.com\/kenjee\/titanic-project-example\n     \n>   Its respective Youtube video can found out through this link : https:\/\/www.youtube.com\/watch?v=I3FBJdiExcg\n     \n   If you interested in data science and\/or sport analytics,please do check out his youtube channel!\n     \n>   Ken Jee - https:\/\/www.youtube.com\/c\/kenjee1?sub_confirmation=1\n     \n   There are many materials and tools in his channel.","5bc19a8f":"This notebook shows all the relevant steps I took to make a simple data science project for my first Kaggle submission.The goal is to correctly predict if someone survived the Titanic shipwreck.There is brunch of challenges and difficulties I faced during the learning process.But I think these are not unique to you or someone who is just starting with Kaggle.At the end of this notebook,I provide some good example of notebook you can consider it for learning and referencing.Hope all of you benetifical from it. So let's get started!","759f3927":"It's quite hard to see the normal distribution for those variables.Perhaps we should take the non-normal distributions and consider normalizing them?","f8451575":"## 2) Data Exploration","1b3843eb":"# 4. Data Preprocessing for Model","f1d4058b":"# 8) Results","547dc50f":"iii) Does a person's title relate to survival rates?","64b6a683":"# Overview\n\n1) Load the data \n\n2) Data Exploration\n\n3) Feature Engineering\n\n4) Data Preprocessing for Model\n\n5) Basic Model Building\n\n6) Model Tuning\n\n7) Ensemble Modle Building\n\n8) Results\n\n9) Reference","2c23c3bc":"Let's start with numerical data first.","e29af451":"First,we need to extract the title from the name.There are quite a few titles involved in this dataset but we can reduce them to Mrs, Miss, Mr and Master.The library \u2018strings\u2019 can help in searching for substrings.","7acd877b":"It shows that the avearage of variables towards the survival rate.\n\nNow, we continue on categorical variables."}}