{"cell_type":{"38e0f73f":"code","5da55432":"code","c03b5b15":"code","7423fb10":"code","836ba6a5":"code","0dd10f78":"code","29861a90":"code","2bb5f773":"code","223d2310":"code","7b073983":"code","44bb5330":"code","1b6f0b8d":"code","ff300972":"code","7fa1e0d2":"code","7763a8a2":"code","2d2e6102":"code","e3d34217":"code","d1e75291":"code","14acfbec":"code","d327fa87":"code","8abe9e13":"code","99eaf5bf":"code","1ceffeac":"code","25a7af01":"code","ae82d521":"code","d02a14bb":"code","dbdc76a4":"code","5f1df421":"code","82a92b43":"code","a8bf0636":"code","14e2ce98":"code","e6981f09":"code","72be7c5b":"code","ab949be8":"code","6f7732b2":"code","88cef3a7":"markdown","98818161":"markdown","09f3cb7b":"markdown","c72f9ecd":"markdown","69860504":"markdown","2a588d94":"markdown","22996cd6":"markdown","550d28f6":"markdown","95526058":"markdown","4b7951e3":"markdown","985cf167":"markdown","e23b37be":"markdown","16c4b9bf":"markdown","7dbf09a3":"markdown","25638fcc":"markdown","dd67dc23":"markdown","89b32da6":"markdown","e654dde2":"markdown","3710721d":"markdown","1600a2f8":"markdown","03255c9a":"markdown","3dd75054":"markdown","6119831c":"markdown","82fee078":"markdown"},"source":{"38e0f73f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Configurations\nN_SPLITS = 5\nSEED = 42\n\ntrain_df = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/train.csv\")\ndisplay(train_df.head())\n#print('Train data dimension: ', train_df.shape)\n\ntest_df = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/test.csv\")\n#print('Test data dimension: ', test_df.shape)\n#display(test_df.head())\n\nsample_submission = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/sample_submission.csv\")\n#print('Sample submission dimension: ', sample_submission.shape)\n#display(sample_submission.head())\n\n\nprint(f\"Missing data in the train data: {train_df.isna().sum(axis=0).any()}\")\nprint(f\"Missing data in the test data: {test_df.isna().sum(axis=0).any()}\")","5da55432":"f, ax = plt.subplots(nrows=1, ncols=3, figsize=(18, 4))\n\n# Unimodal\nsns.distplot(np.random.normal(10, 5, 10000), ax=ax[0], hist=False, color='blue')\nax[0].set_title('Unimodal', fontsize=14)\nax[0].set_yticklabels([])\nax[0].set_xticklabels([])\n\n# Bimodal\nsample_bimodal = pd.DataFrame({'feature1' : np.random.normal(10, 5, 10000),\n                   'feature2' : np.random.normal(40, 10, 10000),\n                   'feature3' : np.random.randint(0, 2, 10000),\n                  })\n\nsample_bimodal['combined'] = sample_bimodal.apply(lambda x: x.feature1 if (x.feature3 == 0 ) else x.feature2, axis=1)\n\nsns.distplot(sample_bimodal['combined'].values, ax=ax[1], color='blue', hist=False)\n\nax2 = ax[1].twinx()  # instantiate a second axes that shares the same x-axis\n\nsns.distplot(sample_bimodal.feature1, ax=ax2, color='blue', kde_kws={'linestyle':'--'}, hist=False)\nsns.distplot((sample_bimodal.feature2), ax=ax2, color='blue', kde_kws={'linestyle':'--'}, hist=False)\n\nf.tight_layout()  # otherwise the right y-label is slightly clipped\n\nax[1].set_title('Bimodal', fontsize=14)\nax[1].set_yticklabels([])\nax[1].set_xticklabels([])\nax2.set_yticklabels([])\n\n\n# Multimodal\nsample_multi = pd.DataFrame({'feature1' : np.random.normal(10, 5, 10000),\n                   'feature2' : np.random.normal(40, 10, 10000),\n                   'feature3' : np.random.randint(0, 3, 10000),\n                               'feature4' : np.random.normal(80, 4, 10000),\n                  })\n\nsample_multi['combined'] = sample_multi.apply(lambda x: x.feature1 if (x.feature3 == 0 ) else (x.feature2 if x.feature3 == 1 else x.feature4), axis=1 )\n\nsns.distplot(sample_multi['combined'].values, ax=ax[2], color='blue', hist=False)\n\nax3 = ax[2].twinx()  # instantiate a second axes that shares the same x-axis\n\nsns.distplot(sample_multi.feature1, ax=ax3, color='blue', kde_kws={'linestyle':'--'}, hist=False)\nsns.distplot((sample_multi.feature2), ax=ax3, color='blue', kde_kws={'linestyle':'--'}, hist=False)\nsns.distplot((sample_multi.feature4), ax=ax3, color='blue', kde_kws={'linestyle':'--'}, hist=False)\n\nf.tight_layout()  # otherwise the right y-label is slightly clipped\n\nax[2].set_title('Multimodal', fontsize=14)\nax[2].set_yticklabels([])\nax[2].set_xticklabels([])\nax3.set_yticklabels([])\n\nplt.show()","c03b5b15":"#display(train_df.target.describe())\nf, ax = plt.subplots(nrows=1, ncols=3, figsize=(18, 4))\nsns.distplot(train_df.target, ax=ax[0])\nsns.boxplot(train_df.target, ax=ax[1])\nstats.probplot(train_df['target'], plot=ax[2])\nplt.show()","7423fb10":"# Drop one outlier\ntrain_df = train_df[train_df.target != 0].reset_index(drop=True)","836ba6a5":"df = pd.DataFrame({'feature1' : np.random.normal(10, 5, 1000),\n                   'feature2' : np.random.normal(40, 10, 1000),\n                   'feature3' : np.random.randint(0, 2, 1000),\n                  })\n\ndf['target'] = df.apply(lambda x: x.feature1 if (x.feature3 == 0 ) else x.feature2, axis=1)\n\ndisplay(df.head())\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(14, 6))\ncolor = 'blue'\nsns.distplot(df.target, ax=ax, color=color)\nax.tick_params(axis='y', labelcolor=color)\n\nax2 = ax.twinx()  # instantiate a second axes that shares the same x-axis\n\ncolor = 'red'\nsns.distplot(df.feature1, ax=ax2, color=color, hist=False)\nsns.distplot((df.feature2), ax=ax2, color=color, hist=False)\n\nax2.tick_params(axis='y', labelcolor=color)\nax.set_title('Example of a Bimodal Distribution', fontsize=16)\n\nfig.tight_layout()  # otherwise the right y-label is slightly clipped\nplt.show()","0dd10f78":"f, ax = plt.subplots(nrows=4, ncols=4, figsize=(12, 12))\n#f.suptitle('Distribution of Features', fontsize=16)\nsns.distplot(train_df['cont1'], ax=ax[0, 0])\nsns.distplot(train_df['cont2'], ax=ax[0, 1])\nsns.distplot(train_df['cont3'], ax=ax[0, 2])\nsns.distplot(train_df['cont4'], ax=ax[0, 3])\n\nsns.distplot(train_df['cont5'], ax=ax[1, 0])\nsns.distplot(train_df['cont6'], ax=ax[1, 1])\nsns.distplot(train_df['cont7'], ax=ax[1, 2])\nsns.distplot(train_df['cont8'], ax=ax[1, 3])\n\nsns.distplot(train_df['cont9'], ax=ax[2, 0])\nsns.distplot(train_df['cont10'], ax=ax[2, 1])\nsns.distplot(train_df['cont11'], ax=ax[2, 2])\nsns.distplot(train_df['cont12'], ax=ax[2, 3])\n\nsns.distplot(train_df['cont13'], ax=ax[3, 0])\nsns.distplot(train_df['cont14'], ax=ax[3, 1])\nf.delaxes(ax[3, 2])\nf.delaxes(ax[3, 3])\nplt.tight_layout()\nplt.show()\n\n#f, ax = plt.subplots(nrows=14, ncols=3, figsize=(12, 28))\n#for i, var in enumerate(train_df.columns[train_df.columns.str.startswith('cont')]):\n#    sns.distplot(train_df[var], ax=ax[i, 0])\n#    sns.boxplot(train_df[var], ax=ax[i, 1])\n#    stats.probplot(train_df[var], plot=ax[i, 2])\n#plt.tight_layout()\n#plt.show()","29861a90":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 10))\nax.set_title(\"Correlation Matrix\", fontsize=16)\nsns.heatmap(train_df[train_df.columns[train_df.columns != 'id']].corr(), vmin=-1, vmax=1, cmap='coolwarm', annot=True)\n\nfor tick in ax.xaxis.get_major_ticks():\n    tick.label.set_fontsize(14) \n    tick.label.set_rotation(90) \nfor tick in ax.yaxis.get_major_ticks():\n    tick.label.set_fontsize(14)\n    tick.label.set_rotation(0) \nplt.show()","2bb5f773":"# Baseline model parameters copied from\n# https:\/\/www.kaggle.com\/ttahara\/tps-jan-2021-gbdts-baseline\n\nmodel_params = {\n    \"objective\": \"root_mean_squared_error\",\n    \"learning_rate\": 0.1, \n    \"seed\": 42,\n    'max_depth': 7,\n    'colsample_bytree': .85,\n    \"subsample\": .85,\n}\n    \ntrain_params = {\n    \"early_stopping_rounds\": 100,\n    \"verbose_eval\": 50,\n}\n\ndef visualize_results(y_pred, y_train, features, feature_importances):\n    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 4))\n\n    color = 'blue'\n    ax[0].set_ylabel('Ground Truth', color=color, fontsize=14)\n    sns.distplot(y_train, ax=ax[0], color=color)\n    ax[0].tick_params(axis='y', labelcolor=color)\n\n    ax2 = ax[0].twinx()  # instantiate a second axes that shares the same x-axis\n\n    color = 'red'\n    ax2.set_ylabel('Predicted', color=color, fontsize=14)  # we already handled the x-label with ax1\n    sns.distplot(pd.Series(y_pred), ax=ax2, color=color)\n    ax2.tick_params(axis='y', labelcolor=color)\n    ax[0].set_title('Distribution of Predicted Values and Ground Truth', fontsize=16)\n\n    pd.DataFrame({'features' : features, \n                  'feature_importance': feature_importances}\n                ).set_index('features').sort_values(by='feature_importance', ascending=False).head(10).plot(kind='bar', ax=ax[1])\n    ax[1].set_title('Top 10 Most Important Features', fontsize=16)\n\n    for tick in ax[1].yaxis.get_major_ticks():\n        tick.label.set_fontsize(14)\n        tick.label.set_rotation(0) \n\n    fig.tight_layout()  # otherwise the right y-label is slightly clipped\n    plt.show()\n    \n","223d2310":"def run_model(X, y, X_test):\n    \"\"\"\n    Baseline is based on\n    https:\/\/www.kaggle.com\/ttahara\/tps-jan-2021-gbdts-baseline\n    \n    Arg:\n    * X: training data containing features\n    * y: training data containing target variables\n    * X_test: test data to predict\n    \n    Returns:\n    * predictions for X_test\n    \"\"\"\n    # Initialize variables\n    y_oof_pred = np.zeros(len(X))\n    y_test_pred = np.zeros(len(X_test))\n\n    kf = KFold(n_splits=N_SPLITS, shuffle=True, random_state=SEED)\n\n\n    for fold, (train_idx, val_idx) in enumerate(kf.split(X, y)):\n        print(f\"Fold {fold + 1}:\")\n\n        # Prepare training and validation data\n        X_train = X.iloc[train_idx].reset_index(drop=True)\n        X_val = X.iloc[val_idx].reset_index(drop=True)\n\n        y_train = y.iloc[train_idx].reset_index(drop=True)\n        y_val = y.iloc[val_idx].reset_index(drop=True)  \n\n        train_data = lgb.Dataset(X_train, label=y_train)\n        val_data = lgb.Dataset(X_val, label=y_val)\n\n        # Define model\n        model = lgb.train(params=model_params,\n                          train_set=train_data,\n                          valid_sets=[train_data, val_data],\n                          **train_params)\n\n        # Calculate evaluation metric: Root Mean Squared Error (RMSE)\n        y_val_pred = model.predict(X_val)\n        score = np.sqrt(mean_squared_error(y_val, y_val_pred))\n        print(f\"RMSE: {score:.5f}\\n\")\n\n        y_oof_pred[val_idx] = y_val_pred\n\n        # Make predictions\n        y_test_pred += model.predict(X_test)\n\n    # Calculate evaluation metric for out of fold validation set\n    oof_score = np.sqrt(mean_squared_error(y, y_oof_pred))\n    print(f\"OOF RMSE: {oof_score: 5f}\")\n\n    # Average predictions over all folds\n    y_test_pred = y_test_pred \/ N_SPLITS\n    visualize_results(y_oof_pred, y, X.columns, model.feature_importance(importance_type=\"gain\"))\n\n    return y_test_pred\n\n\nfeatures_baseline = train_df.columns[train_df.columns.str.startswith('cont')]\ntarget = ['target']\n\ndisplay(train_df[features_baseline].head().style.set_caption('Training data'))\n\ny_pred = run_model(train_df[features_baseline], \n                   train_df[target], \n                   test_df[features_baseline])","7b073983":"df = pd.DataFrame({'feature1' : np.random.normal(10, 5, 1000),\n                   'feature2' : np.random.normal(40, 10, 1000),\n                   'feature3' : np.random.randint(0, 2, 1000),\n                  })\n\ndf['target'] = df.apply(lambda x: x.feature1 if (x.feature3 == 0 ) else x.feature2, axis=1)\n\ndisplay(df.head())\nfig, ax = plt.subplots(nrows=1, ncols=1, figsize=(14, 6))\ncolor = 'blue'\nsns.distplot(df.target, ax=ax, color=color)\nax.tick_params(axis='y', labelcolor=color)\n\nax2 = ax.twinx()  # instantiate a second axes that shares the same x-axis\n\ncolor = 'red'\nsns.distplot(df.feature1, ax=ax2, color=color, hist=False)\nsns.distplot((df.feature2), ax=ax2, color=color, hist=False)\n\nax2.tick_params(axis='y', labelcolor=color)\nax.set_title('Example of a Bimodal Distribution', fontsize=16)\n\nfig.tight_layout()  # otherwise the right y-label is slightly clipped\nplt.show()","44bb5330":"features = df.columns[df.columns.str.startswith('feature')]\ntarget = ['target']\n\npred = run_model(df[features].head(800), \n          df[target].head(800), \n          df[features].tail(200))\n","1b6f0b8d":"from sklearn.mixture import GaussianMixture\n\ngmm = GaussianMixture(n_components=2, random_state=42)\n\ngmm.fit(train_df.target.values.reshape(-1, 1))\n\ntrain_df['target_class'] = gmm.predict(train_df.target.values.reshape(-1, 1))","ff300972":"f, ax = plt.subplots(nrows=1, ncols=2, figsize=(18, 6))\nsns.kdeplot(data=train_df.target, ax=ax[0])\nax[0].set_title('Before GMM', fontsize=16)\nsns.kdeplot(data=train_df[train_df.target_class==0].target, label='Component 1', ax=ax[1])\nsns.kdeplot(data=train_df[train_df.target_class==1].target, label='Component 2', ax=ax[1])\nax[1].set_title('After GMM', fontsize=16)\nplt.show()","7fa1e0d2":"\ndef get_gmm_class_feature(feat, n):\n    gmm = GaussianMixture(n_components=n, random_state=42)\n\n    gmm.fit(train_df[feat].values.reshape(-1, 1))\n\n    train_df[f'{feat}_class'] = gmm.predict(train_df[feat].values.reshape(-1, 1))\n    test_df[f'{feat}_class'] = gmm.predict(test_df[feat].values.reshape(-1, 1))\n\nget_gmm_class_feature('cont1', 4)\nget_gmm_class_feature('cont2', 10)\nget_gmm_class_feature('cont3', 6)\nget_gmm_class_feature('cont4', 4)\nget_gmm_class_feature('cont5', 3)\nget_gmm_class_feature('cont6', 2)\nget_gmm_class_feature('cont7', 3)\nget_gmm_class_feature('cont8', 4)\nget_gmm_class_feature('cont9', 4)\nget_gmm_class_feature('cont10', 8)\nget_gmm_class_feature('cont11', 5)\nget_gmm_class_feature('cont12', 4)\nget_gmm_class_feature('cont13', 6)\nget_gmm_class_feature('cont14', 6)\n\ntrain_df.head()","7763a8a2":"features = list(features_baseline) + list(train_df.columns[train_df.columns.str.contains('class') & ~train_df.columns.str.contains('target')])\ntarget = ['target']\n\ndisplay(train_df[features].head().style.set_caption('Training data'))\n\ny_test_pred = run_model(train_df[features], \n          train_df[target], \n          test_df[features])","2d2e6102":"f, ax = plt.subplots(nrows=1, ncols=1, figsize=(12, 10))\nax.set_title(\"Correlation Matrix\", fontsize=16)\ncols = ['target', 'cont1_class', 'cont2_class', 'cont3_class',\n       'cont4_class', 'cont5_class', 'cont6_class', 'cont7_class',\n       'cont8_class', 'cont9_class', 'cont10_class', 'cont11_class',\n       'cont12_class', 'cont13_class', 'cont14_class']\nsns.heatmap(train_df[cols].corr(), vmin=-1, vmax=1, cmap='coolwarm', annot=True)\n\nfor tick in ax.xaxis.get_major_ticks():\n    tick.label.set_fontsize(14) \n    tick.label.set_rotation(90) \nfor tick in ax.yaxis.get_major_ticks():\n    tick.label.set_fontsize(14)\n    tick.label.set_rotation(0) \nplt.show()","e3d34217":"for j in range(1,15):\n    for i in range(train_df.cont1_class.nunique()):\n        train_df[f'cont{j}_class_{i+1}'] = np.where(train_df[f'cont{j}_class'] == (i+1), train_df[f'cont{j}'], np.nan)\n        test_df[f'cont{j}_class_{i+1}'] = np.where(test_df[f'cont{j}_class'] == (i+1), test_df[f'cont{j}'], np.nan)\n        \n","d1e75291":"train_df[['cont1', 'cont1_class', 'cont1_class_1', 'cont1_class_2', 'cont1_class_3']].head().style.set_caption('Example of newly created features for cont1 from cont1_class')","14acfbec":"features = list(features_baseline) + list(train_df.columns[train_df.columns.str.contains('class_')])\ntarget = ['target']\n\ndisplay(train_df[features].head().style.set_caption('Training data'))\n\ny_test_pred = run_model(train_df[features], \n          train_df[target], \n          test_df[features])","d327fa87":"for i in range(1,15):\n    train_df[f'cont{i}_bin_10'] = pd.cut(train_df[f'cont{i}'], bins=10, labels=False)\n    test_df[f'cont{i}_bin_10'] = pd.cut(test_df[f'cont{i}'], bins=10, labels=False)","8abe9e13":"f, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 6))\nsns.distplot(train_df['cont1'], ax=ax[0])\nax[0].set_title('Distribution of cont1', fontsize=14)\nsns.countplot(train_df['cont1_bin_10'], ax=ax[1], color='steelblue')\nax[1].set_title('cont1 after Binning', fontsize=14)\n\nplt.show()","99eaf5bf":"features = list(features_baseline) + list(train_df.columns[train_df.columns.str.contains('bin')])\ntarget = ['target']\n\ndisplay(train_df[features].head().style.set_caption('Training data'))\n\ny_test_pred = run_model(train_df[features], \n          train_df[target], \n          test_df[features])","1ceffeac":"features = train_df.columns[train_df.columns.str.startswith('cont') & ~train_df.columns.str.contains('class')& ~train_df.columns.str.contains('bin')]\n\ntrain_df['sum'] = train_df[features].sum(axis=1)\ntrain_df['mean'] = train_df[features].mean(axis=1)\ntrain_df['min'] = train_df[features].min(axis=1)\ntrain_df['max'] = train_df[features].max(axis=1)\ntrain_df['std'] = train_df[features].std(axis=1)\ntrain_df['var'] = train_df[features].var(axis=1)\n\ntest_df['sum'] = test_df[features].sum(axis=1)\ntest_df['mean'] = test_df[features].mean(axis=1)\ntest_df['min'] = test_df[features].min(axis=1)\ntest_df['max'] = test_df[features].max(axis=1)\ntest_df['std'] = test_df[features].std(axis=1)\ntest_df['var'] = test_df[features].var(axis=1)","25a7af01":"features = list(features_baseline) + list(['sum', 'mean', 'min', 'max', 'std'])\ntarget = ['target']\n\ndisplay(train_df[features].head().style.set_caption('Training data'))\n\ny_test_pred = run_model(train_df[features], \n          train_df[target], \n          test_df[features])","ae82d521":"import featuretools as ft\nes = ft.EntitySet(id = 'data')\n\noriginal_cols = ['id', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14']\n\n\nes = es.entity_from_dataframe(entity_id = 'data', \n                              dataframe = pd.concat([train_df, test_df], axis=0)[original_cols], \n                              index = 'id', \n                              time_index = None)\nes['data']","d02a14bb":"ft.list_primitives().head()","dbdc76a4":"new_features, new_feature_names = ft.dfs(entityset = es, target_entity = 'data', \n                                 trans_primitives = ['divide_numeric'])\n\nnew_features.head()","5f1df421":"new_features = new_features.reset_index(drop=False)\ny_test_pred = run_model(new_features[new_features.id.isin(train_df.id)],\n                        train_df[target], \n                        new_features[new_features.id.isin(test_df.id)])","82a92b43":"new_features, new_feature_names = ft.dfs(entityset = es, target_entity = 'data', \n                                 trans_primitives = ['multiply_numeric'])\n\nnew_features.head()","a8bf0636":"new_features = new_features.reset_index(drop=False)\ny_test_pred = run_model(new_features[new_features.id.isin(train_df.id)],\n                        train_df[target], \n                        new_features[new_features.id.isin(test_df.id)])","14e2ce98":"merged_df = pd.concat([train_df, test_df], axis=0).sort_values(by='id').reset_index(drop=True)","e6981f09":"original_cols = ['id', 'cont1', 'cont2', 'cont3', 'cont4', 'cont5', 'cont6', 'cont7',\n       'cont8', 'cont9', 'cont10', 'cont11', 'cont12', 'cont13', 'cont14', 'target']\n\ndisplay(merged_df[original_cols].head().style.set_caption('Merged Dataframe for EDA'))\n\nmerged_df[original_cols].describe()","72be7c5b":"f, ax = plt.subplots(nrows=4, ncols=4, figsize=(12, 12))\n\n#f.suptitle('Differenced Features')\nj= -1\nfor i in range(1,15):\n    k = (i-1)%4 # ncols\n    \n    if k == 0:\n        j += 1\n        \n    sns.distplot(np.diff(merged_df[f'cont{i}']), ax=ax[j, k])\n    ax[j, k].set_title(f'cont{i}', fontsize=14)\nplt.tight_layout()\n\nplt.show()","ab949be8":"f, ax = plt.subplots(nrows=4, ncols=4, figsize=(14, 14))\n\n#f.suptitle('Log Features')\nj= -1\nfor i in range(1,15):\n    k = (i-1)%4 # ncols\n    \n    if k == 0:\n        j += 1\n        \n    sns.distplot(np.log(merged_df[f'cont{i}']), ax=ax[j, k])\n    ax[j, k].set_title(f'cont{i}', fontsize=14)\n    \nplt.tight_layout()\nplt.show()","6f7732b2":"submission = test_df[['id']].copy()\nsubmission['target'] = y_test_pred\n\nsubmission.to_csv(\"submission.csv\", index=False)","88cef3a7":"Let's see if we can get any interesting insights if we difference the features with `np.diff`. Differencing means that we take the difference between two consecutive datapoints of a column.","98818161":"# Tabular Playground #1: Bimodal Regression\n\n* [Problem Definition](#Problem-Definition)<br>\n* [Modality](#Modality)<br>\n* [Data Overview](#Data-Overview)<br>\n    * [Target](#Target)<br>\n    * [cont# Features](#cont-Features)<br>\n* [Model Baseline](#Model-Baseline)<br>\n* [Feature Engineering Techniques](#Feature-Engineering-Techniques)<br>\n    * [Gaussian Mixture Modelling](#Gaussian-Mixture-Modelling)<br>\n    * [Binning](#Binning)<br>\n    * [Statistical Features](#Statistical-Features)<br>\n    * [Deep Feature Synthesis](#Deep-Feature-Synthesis)<br>\n    * [Summary](#Summary)<br>\n* [EDA](#EDA)<br>\n\n# Problem Definition\n\nIn this challenge, we are asked to build a **regression** model. Without further context, we are given some features with **continuous values** to predict a continuous target. This way, we can practice on focussing on the data without requiring any specific domain knowledge because the column names `cont#` do not indicate any further information.\n\nThere is no missing data but instead we have a different obstacle that we have to overcome - **bimodal distribution of the target variable** and multimodal distributions of the features.","09f3cb7b":"# Submission","c72f9ecd":"## Summary\n\nSo far, the feature engineering techniques only gave us minor improvements: \n\n|           | OOF RMSE | Delta to Benchmark |\n|:---------:|----------|--------------------|\n| **Benchmark** | **0.703148** | n\/a                |\n| GMM (class)       | 0.703054 | 0.000094           |\n| GMM (separated)      | 0.703033 | **0.000115**           |\n| Binning   | 0.703237 | -0.000089          |\n| Statistical Features   | 0.703229 | -0.00081|\n| DFS (divide_numeric)  | 0.703145 | 0.00003|\n| DFS (multiply_numeric)  | 0.0.703144 | 0.00004|\n\n\n\nIn contrast to the other feature engineering techniques, with the binning features we get a worse score than with the other techniques. This is probably due to the fact that although we have more features now, a. they are highly correlated to each other (e.g. collinearity between `cont1` and `cont1_bin10`) and b. we lose some information when binning features.","69860504":"# Data Overview\n\n## Target\n* It looks like an overlay of two different distributions - when the data distribution has two peaks, it is called **bimodal distribution**.\n* There is exactly one data point with a target value of 0 - this looks very much like an **outlier**. We should probably drop it since it is only one single data point.\n* The data does not seem to be skewed and therefore does not necessarily need to be transformed (if non-tree-based models are used - for tree-based models this would not matter anyways).","2a588d94":"You can list out all possibilities with the function `list_primitives()`.","22996cd6":"# EDA\n\nFrom above section, we understand the urgency of gaining some insights from the data. For this purpose, we will concatenate the training and the testing data and sort them by the column `id`.","550d28f6":"## Statistical Features\nAnother common approach is to create new features from the data's **statistics (mean, sum, std, etc.)**.","95526058":"Let's use our baseline model as is and use it for modelling the data.\nFrom the below results, we can see that the LightGBM model is able to handle the bimodal distribution.","4b7951e3":"Let's try a couple of primitives. How about dividing each feature by another feature using the primitive `divide_numeric`. You can see that we **end up with 196 features**.","985cf167":"From a top level point of view, we can see that **none of the features seem to be correlated to the `target`** and `cont14`. \n\nAdditionally, there seems to be a highly correlated cluster consisting of `cont1` and `cont6` through `cont13`.","e23b37be":"**Can tree-based models handle bimodal distributions?**\n\nTL;DR: Yes, tree-based models in general should be able to handle bimodal distributions.\n\nDecision trees are insensitive to the targets distribution. Therefore, we do not necessarily need to transform the target to fit a normal distribution. Therefore, we would expect tree-based models to be able to handle bimodal distributions without any transformations as well.\nLet's see if this is true.\n\nIn the following minimal example, we have three features and a target that has a bimodal distribution as shown in the plot. \nFor simplicity reasons, the target equals `feature1` if `feature3` == 0 and else the targets equals `feature2`.","16c4b9bf":"# Feature Engineering Techniques\n\nWe have seen that our baseline model in theory is able to model the bimodal distribution of our target. However, we can also see that this highly depends on the quality of our features. In the [Data Overview](#Data-Overview), we saw that the features have a low absolute correlation to the target. In this section, we will be exploring different feature engineering techniques.\n\n## Gaussian Mixture Modelling (GMM)\n\nWe can use Gaussian Mixture Modelling to separate the two distributions. It is an unsupervised learning algorithm.\n\n[Discussion: When you have bimodal distribution](https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/discussion\/86951)","7dbf09a3":"The out of fold (OOF) RMSE score is **0.703148 - this is our benchmark** for the next steps. This score is quite bad since RMSE of 0 would be ideal. If we look at the distributions of our predictions versus the ground truth, we can see that our model is doing quite poorly.","25638fcc":"# Modality\n\nIn this challenge, we will learn about the modality of a distribution. You can find the modality of a distribution by counting the number of its peaks. There are unimodal, bimodal and multimodal distributions.\nThe most commonly known distribution is unimodal with only one peak. This is probably also the most comfortable distribution to work with. If you have two peaks, it is called bimodal, and if you have three or more peaks, then it is called multimodal.\n\nAlso, **don't get it confused with \"multimodal learning\"**, which describes problems with mixed feature modalities, such as pictures and text.","dd67dc23":"## Binning\n\nWork in progress","89b32da6":"## cont# Features\nLet's look at the `cont#` features in bulk.\nFor below plots we can see some **odd distributions: all `cont#` features show multiple 'peaks' with no sign of a normal distribution.**\n\nUm, ok... what is going on?! We definitely need to dig deeper here before we can start building our model.\n\nWe already saw that our `target` variable has a bimodal distribution. Now we have some feature distributions with multiple peaks. This is called **multimodal distribution**.\n","e654dde2":"Since the features have multimodal distributions, it would be worthwhile checking what happens if we add a GMM feature for each `cont#` feature. ","3710721d":"Let's also try it for multiplication:","1600a2f8":"# Model Baseline\n\nAlothough, we did not do any feature engineering so far, let's create a simple LightGBM baseline to get a **benchmark** first. The baselibe in [Tawara](https:\/\/www.kaggle.com\/ttahara)'s notebook [TPS Jan 2021: GBDTs Baseline](https:\/\/www.kaggle.com\/ttahara\/tps-jan-2021-gbdts-baseline) will be the base for our experiments.","03255c9a":"Log transform","3dd75054":"From the above experiments, we can see that `cont2`, `cont3`, `cont4` and `cont13` seem to be the most important features for our model. Let's explore them a little more.\n\nHopefully you now have a few ideas on how to investigate the features. If you need more inspiration, I recommend the discussion section of the [BNP Paribas Cardif Claims Management Competition](https:\/\/www.kaggle.com\/c\/bnp-paribas-cardif-claims-management\/discussion).\n\nHappy Kaggling!","6119831c":"With the newly added features, we get a RMSE of **0.703054**. This is only a 0.000094 improvement to our baseline and our distribution is still not close to the distribution of the ground truth.\n\nLet's try another idea: What happens if we use the classes we got from the GMM to separate each feature:","82fee078":"## Deep Feature Synthesis\nAnother common approach is creating new features by combining features with **basic mathematical operations (addition, subtraction, multiplication, division)**.\nIf we were given the names of each feature, we could start by creating new features based on our intuition. For example if you have two features `hours_spent_writing_kernels` and\n`number_of_kernels`, you could combine them by dividion to get a new feature `average_time_to_write_a_kernel`= `hours_spent_writing_kernels`\/`number_of_kernels`.\n\nHowever, in this challenge we do not have any information about the features. Therefore, we cannot create new features by intuition. An idea would be to randomly combine features and hoping to see a correlation to our target. However, for 14 features that will take a lot of time. We can somewhat automate this with Deep Feature Synthesis.\n\nIf you wanto read more about this topic in depth, I highly recommend this kernel: [Automated Feature Engineering Tutorial](https:\/\/www.kaggle.com\/willkoehrsen\/automated-feature-engineering-tutorial)."}}