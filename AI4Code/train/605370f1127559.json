{"cell_type":{"e4f82c91":"code","fb22a016":"code","3492b271":"code","c6dfa8f2":"code","47933d98":"code","a7950696":"code","294ae7dd":"code","fd4f8850":"code","80219c00":"code","5889f76f":"code","83587cf0":"code","d29761ed":"code","d2dba4f1":"code","902f0073":"code","f8b0d394":"code","0dc333f7":"code","312199f4":"code","ccda47d4":"code","d90a8993":"code","9e619fb1":"code","bc311d61":"code","8d397185":"code","f4682b40":"code","e77df065":"code","9427fb36":"code","0e6bb159":"code","abe59db2":"code","cee944a3":"code","f4d6a936":"code","902f6a09":"code","313032b5":"code","ec41edbc":"code","41ae2b92":"code","2180ea82":"code","815dc4e2":"code","bc2ed4dd":"code","40a614e3":"code","37d26f24":"code","eb34f690":"code","ebc89f6e":"code","73277375":"code","ca8edc45":"markdown","1b71a7ac":"markdown","1ef06979":"markdown","5f9c1d3c":"markdown","4f58478d":"markdown","1f7c8fd9":"markdown","552f0181":"markdown","7147b3c0":"markdown","bfb0cfda":"markdown","857c61e9":"markdown","03dab4b9":"markdown","e3363801":"markdown","cb0845cf":"markdown","d0cbfd72":"markdown","4157470c":"markdown","6d7ae556":"markdown","1118f23e":"markdown","c151af5a":"markdown","ca0ecb83":"markdown"},"source":{"e4f82c91":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fb22a016":"# Install spotipy and seaborn\n!pip install spotipy --upgrade\n!pip3 install seaborn==0.11.0","3492b271":"# Import Spotipy for Spotify API\nimport spotipy \nfrom spotipy.oauth2 import SpotifyClientCredentials, SpotifyOAuth\nimport spotipy.util as util\n\n# For visualizations\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# For preparing training data\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, RobustScaler\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split as TTS\n\n\n# Various classifiers\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\n\n# For finding optimized parameters of classifiers\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV\n\n# For testing models \nfrom sklearn.metrics import plot_confusion_matrix, accuracy_score, precision_recall_fscore_support, plot_roc_curve\n\n# For using secret credentials \nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nCID = user_secrets.get_secret(\"CID\")\nSECRET = user_secrets.get_secret(\"Secret\")\nUSER_ID = user_secrets.get_secret(\"UserID\")","c6dfa8f2":"def get_playlist_trackIDS(username,playlist_id):\n    \n    \"\"\"Takes Spotify username and the ID number of a Spotify playlist, \n    returns a dictionary of 'ids' and 'years' for each song.\"\"\"\n    \n    # Grab tracks from playlist\n    results = sp.user_playlist_tracks(username, playlist_id)\n    \n    # Store track data\n    tracks = results['items']\n    \n    # Grab more data in chunks\n    while results['next']:\n        results = sp.next(results)\n        tracks.extend(results['items'])\n    \n    #Grab track ids \n    ids = [] \n    years = []\n    for i in range(len(tracks)): \n        if tracks[i][\"track\"][\"id\"]:\n            ids.append(tracks[i][\"track\"][\"id\"])\n            years.append(int(tracks[i]['track']['album'][\"release_date\"][:4]))\n        else:\n            continue\n            \n    return {'ids': ids, 'years': years}","47933d98":"def make_playlist_df(ids, years):\n    \n    \"\"\"Takes the list of track ids and their release year \n    (dictionary generated by 'get_playlist_trackIDS' function),\n    returns a dataframe of song info and audio features. \"\"\"\n    \n    \n    # Grab dictionary of audio features for songs in chuncks of 100\n    allEntries = []\n    for i in range(int(len(ids)\/100) + 1):\n        k = i*100\n        j = k+100\n        \n        if j < len(ids):\n            songFeatures = sp.audio_features(ids[k:j])\n            allEntries.extend(songFeatures)\n        else: \n            songFeatures = sp.audio_features(ids[k:])\n            allEntries.extend(songFeatures)\n            \n            \n    # Comb through entries and skip over NoneType entries\n    entryList = []\n    yearList = []\n    for entry, year in zip(allEntries, years):\n        if entry:\n            entryList.append(entry)\n            yearList.append(year)\n            \n               \n    # Create DataFrame and adds a column for years\n    df = pd.DataFrame(entryList)\n    df['year'] = yearList\n\n    \n    return df","a7950696":"def create_user_spotipyObject(cid, secret):\n    \n    \"\"\"Asks user for their cid, secret, and Spotify User ID, \n    redirects to authorization url, \n    asks user to input the redirect url, and authorizes user.\n    Returns a Spotipy Object for navigating Spotify profile.\"\"\"\n    \n    \n    scope = 'user-library-read playlist-read-private playlist-modify-private playlist-modify-public playlist-read-collaborative'\n    \n    # Get authentification url\n    auth = SpotifyOAuth(cid, secret, redirect_uri='http:\/\/localhost\/callback\/', scope=scope)\n    auth_url = auth.get_authorize_url()\n    \n    # Click authentification url to get localhost url with code\n    print('Click this link: ', auth_url)\n    print('A new window will open but no page will load, just copy entire localhost url.')\n    print('Enter localhost url')\n    response_url = input('Copied url: ')\n    \n    #extract code\n    code = auth.parse_response_code(response_url)\n    auth_token = auth.get_access_token(code, as_dict=False)\n    \n    # Create authorized personal Spotipy object\n    sp = spotipy.Spotify(auth=auth_token) \n    sp.trace=False \n    \n    return sp","294ae7dd":"def gridsearch_tests(X_data, y_data, model_param_dict, randomized = False, cv = 5 ):\n    \"\"\"Takes training data, and a dictionary of classification models and parameters,\n    performs a GridSearchCV on each model, then returns an accuracy scores for each.\"\"\"\n    \n    # Choose Randomized GridSearch CV or not\n    if randomized == True:\n        gs = RandomizedSearchCV\n    elif randomized == False: \n        gs = GridSearchCV\n        \n    #this code from --> https:\/\/www.youtube.com\/watch?v=HdlDYng8g9s     \n    scores_rs= []\n    for model_name, mp in model_param_dict.items():\n        # Perform GridSearch\n        rs = gs(mp['model'], mp['params'], cv=cv, return_train_score=False)\n        rs.fit(X_data, y_data)\n        # Store the results\n        scores_rs.append({\n            'model': model_name,\n            'best_score': rs.best_score_,\n            'best_params': rs.best_params_\n        })\n    # Results\n    return pd.DataFrame(scores_rs)","fd4f8850":"def create_good_song_playlist(playlist_id, song_ids, userID):\n    \n    \"\"\"Takes id of Spotify playlist, a list of song IDs, and the User ID, \n    then imports those songs into the Spotify playlist.\"\"\"\n    \n    # Create a batch of 50 songs \n    for i in range(int(len(song_ids)\/50)):\n        j = i*50\n        if (j+50) > len(song_ids):\n            songs = song_ids[j:]\n        else:\n            songs = song_ids[j:j+50]\n        \n        # Import batch to spotify playlist\n        sp.user_playlist_add_tracks(userID, playlist_id, songs, position=0)","80219c00":"# Spotipy Object for navigating personal Spotify account\n# Enter credentials as integers, or use imported secret credentials \nsp = create_user_spotipyObject(CID, SECRET)","5889f76f":"# Spotify Playlist IDs\nGoodSongs = \"2MLQdFwrvVT5KNEQZexPSm\"\nBadSongs = '5DvUWRUffyDpKVYEItMcLN'\nTestSongs = '4Pu7b9djRd3qAO9OTUrUiR'\n\n# get the IDs and release year for the songs in each playlist\nGoodSongsPL = get_playlist_trackIDS(USER_ID, GoodSongs)\nBadSongsPL = get_playlist_trackIDS(USER_ID, BadSongs)\nTestSongsPL = get_playlist_trackIDS(USER_ID, TestSongs)\n","83587cf0":"# Create DataFrames for each playlist\ngoodDF = make_playlist_df(GoodSongsPL['ids'], GoodSongsPL['years'])\nbadDF = make_playlist_df(BadSongsPL['ids'], BadSongsPL['years'])\ntest_DF = make_playlist_df(TestSongsPL['ids'], TestSongsPL['years'])\n\n# Add label column\ngoodDF['quality'] = 1\nbadDF['quality'] = 0\ntest_DF['quality'] = 'test'\n\n# List of training features\nfeatures = ['danceability', 'energy', 'key', 'loudness', 'mode', 'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'duration_ms',\n       'time_signature', 'year', 'quality']\n\n\n\n# Filter by training features\ngoodDF = goodDF[features]\nbadDF = badDF[features]\ntestDF = test_DF[features]\n\n# Combine DataFrames and reset the index\nfullDF_0 = pd.concat([goodDF, badDF])\nmainDF = fullDF_0.reset_index(drop=True)","d29761ed":"#Final DataFrame\nmainDF","d2dba4f1":"testDF","902f0073":"# No null values\nmainDF.info()","f8b0d394":"# General statistics\nmainDF.describe()","0dc333f7":"# Create color palette for visualizations\ncolors = ['#4A4C48', \"#76BE50\"]\nspotifyPalette = sns.color_palette(colors)","312199f4":"# List of features to compare\npairplot_cols = ['danceability', 'energy', 'loudness',  'speechiness',\n       'acousticness', 'instrumentalness', 'liveness', 'valence', 'tempo', 'year', \n       'duration_ms', 'key', 'mode', 'time_signature']\n\n\nsns.set_context('paper', font_scale = 1.9)\n    \n# Create Pairplot\ng = sns.pairplot(mainDF, hue = 'quality', vars = pairplot_cols, corner = False, palette = spotifyPalette)\ng._legend.remove()\nplt.legend(title='Quality', bbox_to_anchor=(2, 6), loc='upper center', labels=['Good', 'Bad'], fontsize= 'large')\n\n# Display\nplt.show(g)","ccda47d4":"# Create subplots comparing distribution of Good and Bad songs for each feature\nsns.set_theme(style=\"whitegrid\") \nsns.set_context('paper', font_scale = 5)\n\n# Figure and Axes, set title\nf, axes = plt.subplots(2, 7, figsize=(100,60), constrained_layout=True, sharex=True)\nf.suptitle('Distribution of classification groups', fontsize=100)\n\n# Iterate through columns, create violinplot for good and bad songs\nfor col, ax in zip(mainDF.columns, axes.flatten()):\n    sns.violinplot(data=mainDF, x='quality', y=mainDF[col], fliersize=15, linewidth=5, ax=ax, palette = spotifyPalette)\n    ax.set_xlabel('', fontsize = 50)\n    ax.set_ylabel(str(col), fontsize = 70)\n    ax.set_xticklabels(['Bad', 'Good'], fontsize = 70)\n\n\nf.show()","d90a8993":"# List of features to drop\n#drop_list = ['danceability', 'liveness', 'instrumentalness', 'loudness', 'quality']\n\n# Dataframe for all training data\nX_all = mainDF.drop('quality', axis=1)\n\n# Labels for categories\ny = mainDF['quality'].values","9e619fb1":"X_all","bc311d61":"# One Hot Encode categorical data and scale the others\ncol_trans = make_column_transformer(\n    (RobustScaler(), ['energy', 'speechiness', 'acousticness', 'valence', 'tempo', 'duration_ms', 'year'] ), \n    (OneHotEncoder(), ['key', 'mode', 'time_signature' ]), remainder = 'passthrough')\n\n# All training features\nX = col_trans.fit_transform(X_all)","8d397185":"pd.DataFrame(X)","f4682b40":"# Get training and test sets\nxtrain, xtest, ytrain, ytest = TTS(X, y, test_size=0.2, random_state=9, stratify=y)","e77df065":"# Dictionary of various models and parameters for a Randomized GridSearch CV \nmodel_params_randomGSCV = {\n    \n    'KNeighbors' : {\n        'model': KNeighborsClassifier(),\n        'params': {\n            'n_neighbors':np.arange(1,100, 1),\n            'weights': ['uniform', 'distance']\n        }\n        \n    },\n    \n    'logistic_regression' : {\n        'model': LogisticRegression(solver='liblinear', multi_class='auto'),\n        'params': {\n            'C':np.arange(1,100, 1),\n            'intercept_scaling': np.arange(.01,1, .01)\n        }\n        \n    },\n    \n    'decision_tree': {\n        'model': DecisionTreeClassifier(),\n        'params': {\n            'max_depth':np.arange(1,100, 1), \n            'splitter': ['best', 'random'],\n            'min_samples_split': np.arange(2,10, 1)\n        }\n    },\n    \n    'random_forest': {\n        'model': RandomForestClassifier(),\n        'params': {\n            'criterion': ['gini', 'entropy'], \n            'n_estimators':np.arange(1,100, 1), \n            'min_samples_split': np.arange(2,10, 1)\n        }\n    },\n    'svm':{\n        'model': SVC(gamma='auto'),\n        'params': {\n            'C': np.arange(1,100, 1),\n            'kernel':['rbf', 'linear']}\n    },\n    \n    'naive_bayes':{\n        'model': GaussianNB(),\n        'params': {\n            'var_smoothing': np.arange(0.001, 1, 0.001)\n        }\n            \n    },\n}\n#this code from --> https:\/\/www.youtube.com\/watch?v=HdlDYng8g9s ","9427fb36":"# Perform randomized gridsearchcv of various models\nrandomGSCV_results = gridsearch_tests(X, y, model_params_randomGSCV, randomized = True, cv = 5)","0e6bb159":"# Plot results\nsns.set_context('paper', font_scale = 1)\ng = sns.barplot(data = randomGSCV_results, x = 'model', y = 'best_score', palette = spotifyPalette)\nplt.xticks(rotation='vertical')\ng.set_title('Randomized GridSearch CV Results')\nplt.show(g)","abe59db2":"# Show score and parameters of Random Forest\nscore = randomGSCV_results.best_score.max()\nparams = randomGSCV_results[randomGSCV_results['model'] == 'random_forest']['best_params'].values[0]\n\nprint('Score: ', score, '\\nParameters: ', params)","cee944a3":"# Dictionary of various models and parameters for regular GridSearch CV \nmodel_paramsGS = {\n    \n    'KNeighbors' : {\n        'model': KNeighborsClassifier(weights = 'distance'),\n        'params': {\n            'n_neighbors':[27,28,29,30,31]\n        }\n        \n    },\n    \n    'logistic_regression' : {\n        'model': LogisticRegression(solver='liblinear', multi_class='auto'),\n        'params': {\n            'C':[10, 11,12],\n            'intercept_scaling': [0.001, .01, .02, 0.3]\n        }\n        \n    },\n    \n    \n    'random_forest': {\n        'model': RandomForestClassifier(criterion = 'entropy'),\n        'params': { \n            'n_estimators':[26,27,28], \n            'min_samples_split': [2, 3, 4]\n        }\n    },\n    'svm':{\n        'model': SVC(gamma='auto', kernel = 'rbf'),\n        'params': {\n            'C': [1,2,3]\n    },\n    \n}\n}\n","f4d6a936":"# Perform regular gridsearchcv of shorter list of models\nGSCV_results = gridsearch_tests(X, y, model_paramsGS, randomized = False, cv = 5)","902f6a09":"# Plot results\nh = sns.barplot(data = GSCV_results, x = 'model', y = 'best_score', palette = spotifyPalette)\nplt.xticks(rotation='vertical')\nh.set_title('Normal GridSearch CV Results')\nplt.show(h)","313032b5":"# Show score and parameters of Random Forest\nscore = GSCV_results.best_score.max()\nparams = GSCV_results[GSCV_results['model'] == 'random_forest']['best_params'].values[0]\n\nprint('Score: ', score, '\\nParameters: ', params)","ec41edbc":"# Train Model\nclf = RandomForestClassifier(min_samples_split = 4, n_estimators = 27, criterion = 'entropy')\nclf.fit(xtrain, ytrain)\n\n# Plot confusion matrix\nsns.set_context('paper', font_scale = 1)\nfig=plot_confusion_matrix(clf, xtest, ytest, display_labels=[\"Bad Songs\",\"Good Songs\"])\nfig.figure_.suptitle(\"Confusion Matrix for  \" + str(clf))\nplt.show()","41ae2b92":"# ROC curve\nrocplot = plot_roc_curve(clf, xtest, ytest)","2180ea82":"# Test and score \nypred = clf.predict(xtest)\nscore =  precision_recall_fscore_support(y_true=ytest, y_pred = ypred)\nacc =  accuracy_score(ytest, ypred)\n\n# Labels for dataframe\nscores = []\ntest_names = ['Prescision', 'Recall', 'F-1']\nlabel_names = [\"Bad Songs\",\"Good Songs\"]\n\n# Get all values from 'score'\nfor test, name in zip(score[:-1], test_names):\n    for i, quality in enumerate(label_names):\n        score_info = {'Test': name, 'Label': quality, 'Score':test[i]}\n        scores.append(score_info)\n        \n# Add score from 'acc' and create dataframe    \nscores.append({'Test': 'Accuracy', 'Label': 'Good Songs', 'Score':acc})          \ndfs = pd.DataFrame(scores)\n\n# Create barplot for scores\nb = sns.barplot(data= dfs, x='Test', y='Score', hue = 'Label', order = ['Accuracy', 'Prescision', 'Recall', 'F-1'], palette=spotifyPalette)\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\n\n# Add score values to the barplot\nfor p in b.patches:\n        b.annotate(\"%.3f\" % p.get_height(), (p.get_x() + p.get_width() \/ 2., p.get_height()),\n             ha='center', va='center', rotation=0, xytext=(0, 4), textcoords='offset points')\n\nplt.show(b)\n","815dc4e2":"# Combine the test dataframe with the training dataframe\ntest_combined = pd.concat([mainDF, testDF])\n\n# Scaling and One Hot Encoding\ncombined_trans = col_trans.fit_transform(test_combined)","bc2ed4dd":"# Convert from array to DataFrame\ncombined_df = pd.DataFrame(combined_trans)\n\n# Filter out the test DataFrame\nX_test = combined_df[combined_df.iloc[:,-1] == 'test']\n\n# Remove the 'quality' column\nX_test = X_test.iloc[:,:-1]","40a614e3":"X_test","37d26f24":"# Send test set through classifier and get predictions\ntest_pred = clf.predict(X_test)\ntest_pred","eb34f690":"# Add predictions to original DataFrame\ntest_DF['pred'] = test_pred\ntest_DF","ebc89f6e":"# Select the songs predicted as 'Good'\nTestGoodSongs = test_DF[test_DF['pred'] == 1]\nTestGoodSongs","73277375":"# Import 'Good' songs from test playlist into a new playlist\n\n##create_good_song_playlist('18z1uZdJQcxRwrePutvjG0', TestGoodSongs['id'], USER_ID)","ca8edc45":"#### The results are promising. This model is performing at about 90% accuracy. ","1b71a7ac":"## Exploratory Analysis","1ef06979":"## Implementation","5f9c1d3c":"#### This playlist is a collection of different songs from different genres I think I might like. After preparing the data, the new playlist data is sent through the classifier and the predictions are added to the original DataFrame. \"Good\" songs are filtered out, song IDs are extracted, then the IDs are used to import the songs to an already existing playlist. These should be songs that I will probably like. ","4f58478d":"### Making predictions","1f7c8fd9":"## Summary\n#### The goal of this project is to train a classifier to predict whether or not I will like a certain song from a given Spotify playlist, then import those songs into a playlist on my Spotify account. \n#### Using the package Spoitpy, song attributes are extracted from a playlist of 'Good Songs' and a playlist of 'Bad Songs'. After comparing the distribution of attributes for each song quaility, the most defining features are selected. Various classification algorithms are tested using Randomized Gridsearch CV, then the top 4 performing classifiers are tested with a more specific Gridsearc CV. The best model was a Random Forrest, with an F-1 score of 0.90 and a AUC score of 0.96. \n#### This model was then used on a completely new playlist consisting of about 2,000 songs I might like. After the predictions were made the 'Bad' songs are filtered out, the song ID's of the \"Good\" songs are extracted and then used to import those songs into an already-existing empty playlist on my Spotify account.\n#### Note: when running this kernal, there is a point in the 'Data Collection' section that require input from the user to connect to their account. \n\n### Suggestions and constructive criticism will be greatly appreciated! ","552f0181":"### Pairplot of continuous data comparing distribution of 'Good' and 'Bad' songs","7147b3c0":"#### The pairplot and violin plots above show the distribution of each song type ('Good' or 'Bad'). Plots with clearly separated data show the features that will be most helpful for training model, while plots with uniform mixing or correlation show features that won't be very helpful at all. \n\n#### The distribution of good\/bad songs for 'danceability', 'liveness', and 'instrumentalness' look almost identical, and could possibly be left out of the training. There is also a clear correlation between 'energy' and 'loudness', so one of those could be left out as well. \n\n#### Note: I attempted to remove these features but the model performed slightly worse, so I put them back.","bfb0cfda":"### Testing Final Model\n#### The model with the best accuracy score was the Random Forest Classifier. Now this specific model is trained and analyzed with a confusion matrix, the ROC curve, and tested for accuracy, precision, recall, and F-1 scores. ","857c61e9":"### Preparing Data\n#### To prepare the data for training, the songs and labels need to be extracted to their own dataframes. The categorical data, like 'time_signature', 'key', and 'mode', need to be transformed into dummy variables, while the rest of the data will be normalized. Then the data will be split into training data and test data. ","03dab4b9":"### Preparing Data\n#### Preparing the test playlist data will follow a similar flow as above. The new data is combined with the training data before transforming the columns in order to get the same amount of dummy variables as the training set. After the transformation, the test playlist is filtered out, unnecessary columns are removed. ","e3363801":"#### The data for this project was taken from two Spotify playlists. One playlist is a collection of songs I like, the other consists of genres I don't like (mostly dubstep, workout music,pop country, and Nickleback). \n#### Using the Spotify API and the Spotipy package, various song attributes are collected into a DataFrame for training. For the final test implementation, a Playlist was created consiting of about 2,000 songs I thought I might like, which is also collected at the same time as the others. ","cb0845cf":"### Import songs to Spotify Playlist","d0cbfd72":"### Testing Various Models\n\n#### To determine the best model, various classification models are fed though a Randomized GridSearch CV to get a vague idea of which models and which parameters had the best scores. Then the top performing models were fed through a normal GridSearch with finer tuned parameter options to decide on the best model, which ended up being the Random Forest algorithm.  ","4157470c":"## Imports","6d7ae556":"## Model Testing","1118f23e":"### Violin plots comparing distribution of 'Good' and 'Bad' songs","c151af5a":"## Data Collection","ca0ecb83":"## Functions"}}