{"cell_type":{"c568392c":"code","b0af1407":"code","c119aca3":"code","fde66e86":"code","b07dcc1f":"code","195a38c1":"code","c3cf9b89":"code","96f78352":"code","3d4ffa1a":"code","f4799dca":"code","a16658e6":"code","1112203b":"code","886f798f":"code","59f4e981":"code","feb17e99":"code","7fe15fc8":"code","0370caa3":"code","319001d3":"code","d29381ee":"code","b9a793b8":"code","e652c12c":"code","abe1712f":"code","05a000dc":"code","d24272fd":"code","3625e9bb":"code","a4a50a19":"code","54f08e3f":"code","3e7792ec":"code","fc238435":"code","f40491b0":"code","6bf003cd":"code","a4954daa":"code","832e382f":"code","ecfa59b3":"code","41a19835":"code","ee089b16":"code","cfc18359":"code","1015d3b6":"code","a6024dab":"code","4881e6e3":"code","c90da837":"code","b4776ec1":"code","72bf3a88":"code","51f50172":"code","c6553803":"code","0abfa584":"code","5d7b3dfa":"markdown","cd97e5b6":"markdown","31c86bc0":"markdown","356edf08":"markdown","5844c078":"markdown","00bfb5cb":"markdown","434195c9":"markdown","b18733c7":"markdown","0d441773":"markdown","83aa37ff":"markdown","3fb4868b":"markdown"},"source":{"c568392c":"# https:\/\/github.com\/aravindpai\/Speech-Recognition\/blob\/master\/Speech%20Recognition.ipynb","b0af1407":"#path\nimport os\nfrom os.path import isdir, join\nfrom pathlib import Path\n\n# Scientific Math \nimport numpy as np\nfrom scipy.fftpack import fft\nfrom scipy import signal\nfrom scipy.io import wavfile\nfrom sklearn.model_selection import train_test_split\n\n# Visualization\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport plotly.offline as py\nimport plotly.graph_objs as go\n\n#Deep learning\nimport tensorflow.keras as keras\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras import Input, layers\nfrom tensorflow.keras import backend as K\n\nimport random\nimport copy\nimport librosa\n\n%matplotlib inline","c119aca3":"print(os.listdir(\"..\/input\"))\n","fde66e86":"train_audio_path = '..\/input\/tensorflow-speech-recognition-challenge\/train\/audio\/'\nprint(os.listdir(train_audio_path))","b07dcc1f":"samples, sample_rate = librosa.load(train_audio_path+'yes\/0a7c2a8d_nohash_0.wav', sr = 16000)\n\nfig = plt.figure(figsize=(14, 8))\nax1 = fig.add_subplot(211)\nax1.set_title('Raw wave of ' + '..\/input\/train\/audio\/yes\/0a7c2a8d_nohash_0.wav')\nax1.set_xlabel('time')\nax1.set_ylabel('Amplitude')\nax1.plot(np.linspace(0, sample_rate\/len(samples), sample_rate), samples)","195a38c1":"import IPython.display as ipd\nipd.Audio(samples, rate=sample_rate)","c3cf9b89":"dirs = [f for f in os.listdir(train_audio_path) if isdir(join(train_audio_path, f))]\ndirs.sort()\nprint('Number of labels: ' + str(len(dirs[1:])))\nprint(dirs)","96f78352":"all_wav = []\nunknown_wav = []\nlabel_all = []\nlabel_value = {}\ntarget_list = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\nunknown_list = [d for d in dirs if d not in target_list and d != '_background_noise_' ]\nprint('target_list : ',end='')\nprint(target_list)\nprint('unknowns_list : ', end='')\nprint(unknown_list)\nprint('silence : _background_noise_')\ni=0;\nbackground = [f for f in os.listdir(join(train_audio_path, '_background_noise_')) if f.endswith('.wav')]\nbackground_noise = []\nfor wav in background : \n    samples, sample_rate = librosa.load(join(join(train_audio_path,'_background_noise_'),wav))\n    samples = librosa.resample(samples, sample_rate, 8000)\n    background_noise.append(samples)\n\nfor direct in dirs[1:]:\n    waves = [f for f in os.listdir(join(train_audio_path, direct)) if f.endswith('.wav')]\n    label_value[direct] = i\n    i = i + 1\n    print(str(i)+\":\" +str(direct) + \" \", end=\"\")\n    for wav in waves:\n        samples, sample_rate = librosa.load(join(join(train_audio_path,direct),wav), sr = 16000)\n        samples = librosa.resample(samples, sample_rate, 8000)\n        if len(samples) != 8000 : \n            continue\n            \n        if direct in unknown_list:\n            unknown_wav.append(samples)\n        else:\n            label_all.append(direct)\n            all_wav.append([samples, direct])","3d4ffa1a":"wav_all = np.reshape(np.delete(all_wav,1,1),(len(all_wav)))\nlabel_all = [i for i in np.delete(all_wav,0,1).tolist()]","f4799dca":"#Random pick start point\ndef get_one_noise(noise_num = 0):\n    selected_noise = background_noise[noise_num]\n    start_idx = random.randint(0, len(selected_noise)- 1 - 8000)\n    return selected_noise[start_idx:(start_idx + 8000)]","a16658e6":"max_ratio = 0.1\nnoised_wav = []\naugment = 1\ndelete_index = []\nfor i in range(augment):\n    new_wav = []\n    noise = get_one_noise(i)\n    for i, s in enumerate(wav_all):\n        if len(s) != 8000:\n            delete_index.append(i)\n            continue\n        s = s + (max_ratio * noise)\n        noised_wav.append(s)\nnp.delete(wav_all, delete_index)\nnp.delete(label_all, delete_index)","1112203b":"wav_vals = np.array([x for x in wav_all])\nlabel_vals = [x for x in label_all]\nwav_vals.shape","886f798f":"labels = copy.deepcopy(label_vals)\nfor _ in range(augment):\n    label_vals = np.concatenate((label_vals, labels), axis = 0)\nlabel_vals = label_vals.reshape(-1,1)","59f4e981":"#knowns audio random sampling\nunknown = unknown_wav\nnp.random.shuffle(unknown_wav)\nunknown = np.array(unknown)\nunknown = unknown[:2000*(augment+1)]\nunknown_label = np.array(['unknown' for _ in range(2000*(augment+1))])\nunknown_label = unknown_label.reshape(2000*(augment+1),1)","feb17e99":"delete_index = []\nfor i,w in enumerate(unknown):\n    if len(w) != 8000:\n        delete_index.append(i)\nunknown = np.delete(unknown, delete_index, axis=0)","7fe15fc8":"#silence audio\nsilence_wav = []\nnum_wav = (2000*(augment+1))\/\/len(background_noise)\nfor i, _ in enumerate(background_noise):\n    for _ in range((2000*(augment+1))\/\/len(background_noise)):\n        silence_wav.append(get_one_noise(i))\nsilence_wav = np.array(silence_wav)\nsilence_label = np.array(['silence' for _ in range(num_wav*len(background_noise))])\nsilence_label = silence_label.reshape(-1,1)\nsilence_wav.shape","0370caa3":"wav_vals    = np.reshape(wav_vals,    (-1, 8000))\nnoised_wav  = np.reshape(noised_wav,  (-1, 8000))\nunknown       = np.reshape(unknown,   (-1, 8000))\nsilence_wav = np.reshape(silence_wav, (-1, 8000))","319001d3":"print(wav_vals.shape)\nprint(noised_wav.shape)\nprint(unknown.shape)\nprint(silence_wav.shape)","d29381ee":"print(label_vals.shape)\nprint(unknown_label.shape)\nprint(silence_label.shape)","b9a793b8":"wav_vals = np.concatenate((wav_vals, noised_wav), axis = 0)\nwav_vals = np.concatenate((wav_vals, unknown), axis = 0)\nwav_vals = np.concatenate((wav_vals, silence_wav), axis = 0)","e652c12c":"label_vals = np.concatenate((label_vals, unknown_label), axis = 0)\nlabel_vals = np.concatenate((label_vals, silence_label), axis = 0)","abe1712f":"print(len(wav_vals))\nprint(len(label_vals))","05a000dc":"train_wav, test_wav, train_label, test_label = train_test_split(wav_vals, label_vals, \n                                                                    test_size=0.2,\n                                                                    random_state = 1993,\n                                                                   shuffle=True)","d24272fd":"# Parameters\nlr = 0.001\ngenerations = 20000\nnum_gens_to_wait = 250\nbatch_size = 512\ndrop_out_rate = 0.5\ninput_shape = (8000,1)","3625e9bb":"#For Conv1D add Channel\ntrain_wav = train_wav.reshape(-1,8000,1)\ntest_wav = test_wav.reshape(-1,8000,1)","a4a50a19":"label_value = target_list\nlabel_value.append('unknown')\nlabel_value.append('silence')","54f08e3f":"new_label_value = dict()\nfor i, l in enumerate(label_value):\n    new_label_value[l] = i\nlabel_value = new_label_value","3e7792ec":"#Make Label data 'string' -> 'class num'\ntemp = []\nfor v in train_label:\n    temp.append(label_value[v[0]])\ntrain_label = np.array(temp)\n\ntemp = []\nfor v in test_label:\n    temp.append(label_value[v[0]])\ntest_label = np.array(temp)\n\n#Make Label data 'class num' -> 'One hot vector'\ntrain_label = keras.utils.to_categorical(train_label, len(label_value))\ntest_label = keras.utils.to_categorical(test_label, len(label_value))","fc238435":"print('Train_Wav Demension : ' + str(np.shape(train_wav)))","f40491b0":"print('Train_Label Demension : ' + str(np.shape(train_label)))","6bf003cd":"print('Test_Wav Demension : ' + str(np.shape(test_wav)))","a4954daa":"print('Test_Label Demension : ' + str(np.shape(test_label)))","832e382f":"print('Number Of Labels : ' + str(len(label_value)))","ecfa59b3":"#Conv1D Model\ninput_tensor = Input(shape=(input_shape))\n\nx = layers.Conv1D(8, 11, padding='valid', activation='relu', strides=1)(input_tensor)\nx = layers.MaxPooling1D(2)(x)\nx = layers.Dropout(drop_out_rate)(x)\nx = layers.Conv1D(16, 7, padding='valid', activation='relu', strides=1)(x)\nx = layers.MaxPooling1D(2)(x)\nx = layers.Dropout(drop_out_rate)(x)\nx = layers.Conv1D(32, 5, padding='valid', activation='relu', strides=1)(x)\nx = layers.MaxPooling1D(2)(x)\nx = layers.Dropout(drop_out_rate)(x)\nx = layers.Conv1D(64, 5, padding='valid', activation='relu', strides=1)(x)\nx = layers.MaxPooling1D(2)(x)\nx = layers.Dropout(drop_out_rate)(x)\nx = layers.Conv1D(128, 3, padding='valid', activation='relu', strides=1)(x)\nx = layers.MaxPooling1D(2)(x)\nx = layers.Flatten()(x)\nx = layers.Dense(256, activation='relu')(x)\nx = layers.Dropout(drop_out_rate)(x)\nx = layers.Dense(128, activation='relu')(x)\nx = layers.Dropout(drop_out_rate)(x)\noutput_tensor = layers.Dense(len(label_value), activation='softmax')(x)\n\nmodel = tf.keras.Model(input_tensor, output_tensor)\n\nmodel.compile(loss=keras.losses.categorical_crossentropy,\n             optimizer=keras.optimizers.Adam(lr = lr),\n             metrics=['accuracy'])\n","41a19835":"model.summary()","ee089b16":"history = model.fit(train_wav, train_label, validation_data=[test_wav, test_label],\n          batch_size=batch_size, \n          epochs=100,\n          verbose=1)","cfc18359":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","1015d3b6":"!pip install -q pyyaml h5py","a6024dab":"# save model\nfrom keras.models import load_model\nmodel.save('voice_model.h5')","4881e6e3":"from IPython.display import FileLink\nFileLink(r'voice_model.h5')","c90da837":"test_path = '..\/input\/yes-test'\nprint(os.listdir(test_path))","b4776ec1":"\n#reading the voice commands\nsamples, sample_rate = librosa.load(test_path + '\/' + 'yes_test001.wav', sr = 16000)\nnew = librosa.resample(samples, sample_rate, 8000)[1000:9000]\nipd.Audio(new,rate=8000)","72bf3a88":"len(new)","51f50172":"target_list","c6553803":"def predict(audio):\n    prob=model.predict(audio.reshape(1,8000,1))\n    index=np.argmax(prob[0])\n    return target_list[index]","0abfa584":"#converting voice commands to text\npredict(new)","5d7b3dfa":"Random sampling from unknown wav data\n","cd97e5b6":"Concatenate wavs, labels ","31c86bc0":"Random sampling from '_background_noise_' \n\nRandom pick background noise \n","356edf08":"### Load Data\n\ntarget list is ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']\nunknown list is other\nsilence will be made from '_background_noise_'\n\nTrain data's sampling rate is 16000Hz, but for making lower computation cost, Resample to 8000hz\n\nAfter training, test set also will resample to 8000Hz","5844c078":"May Some wav data has different length. So, Delete it","00bfb5cb":"# predict","434195c9":"Check Dimensions","b18733c7":"### Data Augmentation\n\nFor Data Augmentation. I will mix train wav, and same length(1 sec) noise(10%) from '_background_noise_'\n","0d441773":"### Train!","83aa37ff":"### Prepare Train","3fb4868b":"split wav, label"}}