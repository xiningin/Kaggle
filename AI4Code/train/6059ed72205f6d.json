{"cell_type":{"a9a0cd12":"code","6df22c26":"code","24b1afe3":"code","0a0b9aa2":"code","bde0b8ca":"code","12c6e1bf":"code","6e3d076c":"code","60beaa0f":"code","186d0015":"code","d93bcea5":"code","9cb1435a":"code","643bf197":"code","3bc3600e":"code","f86777ad":"code","e716c516":"code","9867e39e":"code","6e74d9ba":"code","83c01820":"code","3893f7de":"code","5deef64e":"code","87d19b55":"code","2c6f99f0":"code","aadba938":"code","14f3a7f0":"code","a5e4b820":"code","86bef415":"code","a501b6fe":"code","fa623904":"code","5c7bf2c1":"code","be10d032":"code","448102b8":"code","82ad19ff":"code","cfb7313f":"code","064255fe":"code","2c7573ef":"code","e49eaf8b":"code","74abcee2":"code","6d9c3ff9":"code","37b485f2":"code","fe74aa1e":"code","720e3ae4":"code","7d9d054f":"code","edb521a1":"code","6ac6da6b":"code","3e50b287":"code","de03bd31":"code","28018976":"code","88dffd64":"code","adfac95d":"code","6133321b":"code","cb1add74":"code","04dd2c84":"code","09fd717e":"code","73fdb690":"code","91ef1712":"code","d1a74b61":"code","edf7dea5":"code","e5db3339":"code","25156504":"code","089adb02":"code","5fe3dd4e":"code","3b8d2c7c":"code","af3a8673":"code","042413a9":"code","bbb988e3":"code","c864b600":"code","622ec3ef":"code","f693b291":"code","4e13b70d":"code","1cac6b2e":"code","f7900c42":"code","13242f9d":"code","8cbb0a1a":"code","e5c762c7":"code","f8f4230a":"code","7cee5892":"code","d21674c7":"code","49049a71":"code","1bb7aa62":"code","6b3a07be":"code","58e5a57a":"code","a3f20fe8":"code","76b8ed30":"code","8429e0c7":"code","687d9de3":"code","29048361":"code","994649fa":"code","1b14e014":"markdown","7e402f8f":"markdown","86ff5eee":"markdown","6482871c":"markdown","64e2bf8a":"markdown","4191f40b":"markdown","4055ff86":"markdown","2c424ea8":"markdown","19b3ff9a":"markdown","c9b8a3aa":"markdown","26222ff6":"markdown","47217692":"markdown"},"source":{"a9a0cd12":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6df22c26":"import cv2\nimport numpy as np\nfrom scipy import misc\ni = misc.ascent()","24b1afe3":"import matplotlib.pyplot as plt\nimport seaborn as sns","0a0b9aa2":"\nplt.grid(False)\nplt.gray()\nplt.axis('off')\nplt.imshow(i)\nplt.show()","bde0b8ca":"i_transformed = np.copy(i)\nsize_x = i_transformed.shape[0]\nsize_y = i_transformed.shape[1]\nprint(size_x)\nprint(size_y)","12c6e1bf":"i_transformed.shape #Yhe image is 512*512 array","6e3d076c":"i_transformed[1]","60beaa0f":"# This filter detects edges nicely\n# It creates a convolution that only passes through sharp edges and straight\n# lines.\n\n#Experiment with different values for fun effects.\n#filter = [ [0, 1, 0], [1, -4, 1], [0, 1, 0]]\n\n# A couple more filters to try for fun!\nfilter = [ [-1, -2, -1], [0, 0, 0], [1, 2, 1]]\n#filter = [ [-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]\n\n# If all the digits in the filter don't add up to 0 or 1, you \n# should probably do a weight to get it to do so\n# so, for example, if your weights are 1,1,1 1,2,1 1,1,1\n# They add up to 10, so you would set a weight of .1 if you want to normalize them\nweight  = 1","186d0015":"for x in range(1,size_x-1):\n  for y in range(1,size_y-1):\n      convolution = 0.0\n      convolution = convolution + (i[x - 1, y-1] * filter[0][0])\n      convolution = convolution + (i[x, y-1] * filter[0][1])\n      convolution = convolution + (i[x + 1, y-1] * filter[0][2])\n      convolution = convolution + (i[x-1, y] * filter[1][0])\n      convolution = convolution + (i[x, y] * filter[1][1])\n      convolution = convolution + (i[x+1, y] * filter[1][2])\n      convolution = convolution + (i[x-1, y+1] * filter[2][0])\n      convolution = convolution + (i[x, y+1] * filter[2][1])\n      convolution = convolution + (i[x+1, y+1] * filter[2][2])\n      convolution = convolution * weight\n      if(convolution<0):\n        convolution=0\n      if(convolution>255):\n        convolution=255\n      i_transformed[x, y] = convolution","d93bcea5":"# Plot the image. Note the size of the axes -- they are 512 by 512\nplt.gray()\nplt.grid(False)\nplt.imshow(i_transformed)\n#plt.axis('off')\nplt.show()   ","9cb1435a":"filter = [ [0, 1, 0], [1, -4, 1], [0, 1, 0]]\nfor x in range(1,size_x-1):\n  for y in range(1,size_y-1):\n      convolution = 0.0\n      convolution = convolution + (i[x - 1, y-1] * filter[0][0])\n      convolution = convolution + (i[x, y-1] * filter[0][1])\n      convolution = convolution + (i[x + 1, y-1] * filter[0][2])\n      convolution = convolution + (i[x-1, y] * filter[1][0])\n      convolution = convolution + (i[x, y] * filter[1][1])\n      convolution = convolution + (i[x+1, y] * filter[1][2])\n      convolution = convolution + (i[x-1, y+1] * filter[2][0])\n      convolution = convolution + (i[x, y+1] * filter[2][1])\n      convolution = convolution + (i[x+1, y+1] * filter[2][2])\n      convolution = convolution * weight\n      if(convolution<0):\n        convolution=0\n      if(convolution>255):\n        convolution=255\n      i_transformed[x, y] = convolution","643bf197":"plt.gray()\nplt.grid(False)\nplt.imshow(i_transformed)\n#plt.axis('off')\nplt.show() ","3bc3600e":"filter = [ [-1, 0, 1], [-2, 0, 2], [-1, 0, 1]]\nfor x in range(1,size_x-1):\n  for y in range(1,size_y-1):\n      convolution = 0.0\n      convolution = convolution + (i[x - 1, y-1] * filter[0][0])\n      convolution = convolution + (i[x, y-1] * filter[0][1])\n      convolution = convolution + (i[x + 1, y-1] * filter[0][2])\n      convolution = convolution + (i[x-1, y] * filter[1][0])\n      convolution = convolution + (i[x, y] * filter[1][1])\n      convolution = convolution + (i[x+1, y] * filter[1][2])\n      convolution = convolution + (i[x-1, y+1] * filter[2][0])\n      convolution = convolution + (i[x, y+1] * filter[2][1])\n      convolution = convolution + (i[x+1, y+1] * filter[2][2])\n      convolution = convolution * weight\n      if(convolution<0):\n        convolution=0\n      if(convolution>255):\n        convolution=255\n      i_transformed[x, y] = convolution","f86777ad":"plt.gray()\nplt.grid(False)\nplt.imshow(i_transformed)\n#plt.axis('off')\nplt.show() ","e716c516":"new_x = int(size_x\/2)\nnew_y = int(size_y\/2)\nnewImage = np.zeros((new_x, new_y))\nfor x in range(0, size_x, 2):\n  for y in range(0, size_y, 2):\n    pixels = []\n    pixels.append(i_transformed[x, y])\n    pixels.append(i_transformed[x+1, y])\n    pixels.append(i_transformed[x, y+1])\n    pixels.append(i_transformed[x+1, y+1])\n    newImage[int(x\/2),int(y\/2)] = max(pixels)\n\n# Plot the image. Note the size of the axes -- now 256 pixels instead of 512\nplt.gray()\nplt.grid(False)\nplt.imshow(newImage)\n#plt.axis('off')\nplt.show()    ","9867e39e":"import keras\nimport tensorflow \nimport matplotlib.pyplot as plt","6e74d9ba":"fashion_mnist=keras.datasets.fashion_mnist","83c01820":"(train_images,train_labels),(test_images, test_labels)=fashion_mnist.load_data()\n","3893f7de":"train_images.shape","5deef64e":"train_labels.shape","87d19b55":"test_images.shape","2c6f99f0":"test_labels.shape","aadba938":"train_labels","14f3a7f0":"train_images[0]","a5e4b820":"plt.imshow(train_images[0])","86bef415":"train_images[1]","a501b6fe":"plt.imshow(train_images[1])","fa623904":"df=pd.DataFrame(train_labels)\ndf.nunique() # we have 10 unique labels ","5c7bf2c1":"#Normalization of images before using algorithm\ntrain_images= train_images.reshape(60000, 28, 28, 1)\ntrain_images=train_images\/255\ntrain_images.shape","be10d032":"test_images= test_images.reshape(10000, 28, 28, 1)\ntest_images.shape","448102b8":"from tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Flatten\nfrom tensorflow.keras.layers import Conv2D, MaxPool2D","82ad19ff":"model= Sequential()","cfb7313f":"model.add(Conv2D(filters=62,kernel_size=3,activation=\"relu\",input_shape=(28,28,1)))","064255fe":"model.add(MaxPool2D(pool_size=2, strides=2))","2c7573ef":"model.add(Conv2D(filters=62,kernel_size=3,activation=\"relu\"))","e49eaf8b":"model.add(MaxPool2D(pool_size=2, strides=2))","74abcee2":"model.add(Flatten(input_shape=(28,28)))","6d9c3ff9":"model.add(Dense(units=128, activation=\"relu\"))\n","37b485f2":"model.add(Dense(units=10, activation=\"softmax\"))","fe74aa1e":"model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])","720e3ae4":"model.summary() #This is the summary of our convolutional neural networks","7d9d054f":"model.fit(train_images,train_labels,epochs=5)","edb521a1":"model.evaluate(test_images,test_labels)","6ac6da6b":"f, axarr = plt.subplots(3,4)\nFIRST_IMAGE=0\nSECOND_IMAGE=7\nTHIRD_IMAGE=26\nCONVOLUTION_NUMBER = 1\nfrom tensorflow.keras import models\nlayer_outputs = [layer.output for layer in model.layers]\nactivation_model = tensorflow.keras.models.Model(inputs = model.input, outputs = layer_outputs)\nfor x in range(0,4):\n  f1 = activation_model.predict(test_images[FIRST_IMAGE].reshape(1, 28, 28, 1))[x]\n  axarr[0,x].imshow(f1[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n  axarr[0,x].grid(False)\n  f2 = activation_model.predict(test_images[SECOND_IMAGE].reshape(1, 28, 28, 1))[x]\n  axarr[1,x].imshow(f2[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n  axarr[1,x].grid(False)\n  f3 = activation_model.predict(test_images[THIRD_IMAGE].reshape(1, 28, 28, 1))[x]\n  axarr[2,x].imshow(f3[0, : , :, CONVOLUTION_NUMBER], cmap='inferno')\n  axarr[2,x].grid(False)","3e50b287":"classifications = model.predict(test_images)\nclassifications[0]","de03bd31":"model2= Sequential()\nmodel2.add(Conv2D(filters=64,kernel_size=3,activation=\"relu\",input_shape=(28,28,1)))\nmodel2.add(MaxPool2D(2,2))\nmodel2.add(Flatten())\nmodel2.add(Dense(units=128, activation=\"relu\"))\nmodel2.add(Dense(units=10, activation=\"softmax\"))\n\n                ","28018976":"model2.summary()","88dffd64":"model2.compile(optimizer=\"adam\",loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"]) ","adfac95d":"model2.fit(train_images,train_labels,epochs=5)","6133321b":"val=0\ncount=0\nfor i in classifications[0]:\n    if i > val:\n        val = i\n        count += 1\n     \n        print(count)\n    ","cb1add74":"print(test_labels[0])","04dd2c84":"plt.imshow(test_images[0])","09fd717e":"test=pd.read_csv(\"..\/input\/mnist-in-csv\/mnist_test.csv\")\ntest","73fdb690":"train= pd.read_csv(\"..\/input\/mnist-in-csv\/mnist_train.csv\")\ntrain","91ef1712":"mnist = tf.keras.datasets.mnist\n(x_train, y_train),(x_test, y_test) = mnist.load_data()","d1a74b61":"x_train.shape","edf7dea5":"y_train.shape","e5db3339":"x_train= x_train\/255\nx_train","25156504":"x_test=x_test\/255","089adb02":"np.unique(y_train)","5fe3dd4e":"x_test[0]","3b8d2c7c":"plt.imshow(x_test[0])","af3a8673":"from tensorflow.keras import Sequential","042413a9":"model=Sequential()","bbb988e3":"from tensorflow.keras.layers import Dense, Flatten","c864b600":"model.add(Flatten())","622ec3ef":"model.add(Dense(units=128, activation=\"relu\"))","f693b291":"model.add(Dense(units=10,activation=\"softmax\"))","4e13b70d":"from tensorflow.keras.callbacks import EarlyStopping","1cac6b2e":"callback=EarlyStopping(monitor=\"val_loss\", patience=5)","f7900c42":"model.compile(optimizer=\"adam\", loss=\"sparse_categorical_crossentropy\", metrics=[\"accuracy\"])","13242f9d":"history=model.fit(x_train,y_train,validation_data=(x_test,y_test),callbacks=[callback],epochs=20)","8cbb0a1a":"model.summary()","e5c762c7":"model.evaluate(x_test,y_test)","f8f4230a":"df=pd.DataFrame(model.history.history)\ndf","7cee5892":"df.plot(figsize=(15,10))","d21674c7":"predictions=model.predict(x_test)\npredictions","49049a71":"y_test","1bb7aa62":"predictions_df=pd.DataFrame(predictions)\npredictions_df.head()","6b3a07be":"from scipy.special import softmax\n\nprobabilities = softmax(predictions, axis=1)\nprobabilities","58e5a57a":"predictions_df=pd.DataFrame(probabilities)\npredictions_df.head()","a3f20fe8":"predictions_df[0]","76b8ed30":"x_test[0].shape","8429e0c7":"y_test[0]","687d9de3":"model.predict(x_test)","29048361":"plt.imshow(x_test[0])","994649fa":"predictions[0]","1b14e014":"## 1. What is Convolutional Neural Network?","7e402f8f":"Convolution matrix, or mask is a small matrix. It is used for blurring, sharpening, embossing, edge detection, and more.\n\nConvolutiona layer \ntakes an input volume\nApplies a filter at every position of the input\nOutputs another volume (usually of different size)","86ff5eee":"Now let's create a convolution. We will iterate over the image, leaving a 1 pixel margin, and multiply out each of the neighbors of the current pixel by the value defined in the filter.\n\ni.e. the current pixel's neighbor above it and to the left will be multiplied by the top left item in the filter etc. etc. We'll then multiply the result by the weight, and then ensure the result is in the range 0-255\n\nFinally we'll load the new value into the transformed image.","6482871c":"This code will show a (2, 2) pooling. The idea here is to iterate over the image, and look at the pixel and it's immediate neighbors to the right, beneath, and right-beneath. Take the largest of them and load it into the new image. Thus the new image will be 1\/4 the size of the old -- with the dimensions on X and Y being halved by this process. You'll see that the features get maintained despite this compression!","64e2bf8a":"There's a bit of a change here in that the training data needed to be reshaped. That's because the first convolution expects a single tensor containing everything, so instead of 60,000 28x28x1 items in a list, we have a single 4D list that is 60,000x28x28x1, and the same for the test images. If you don't do this, you'll get an error when training as the Convolutions do not recognize the shape.","4191f40b":"# we train a new model with fewer convolutional layer","4055ff86":"Now we can plot the image to see the effect of the convolution!","2c424ea8":"The image is stored as a numpy array, so we can create the transformed image by just copying that array. Let's also get the dimensions of the image so we can loop over it later.","19b3ff9a":"Lets use another filter","c9b8a3aa":"Lets use another filter","26222ff6":"Now we can create a filter as a 3x3 array.","47217692":"<font color=\"blue\">\nLets look at the effets of max pooling on the Image"}}