{"cell_type":{"224ba1f7":"code","9bde78b8":"code","061d88e4":"code","bc76fa76":"code","10abd41f":"code","bd60a93a":"code","b71360a0":"code","ccd14091":"code","ee97039a":"code","8fd83c3f":"code","0126e883":"code","703900da":"code","a4fa867a":"code","f724c660":"code","438e7636":"code","de7725f7":"code","843ee292":"code","d5939b56":"code","ac6c069f":"code","6a9d1ae3":"code","bd59f742":"code","a5de009f":"code","82a69fba":"code","74c061a5":"code","53a308e9":"code","b9612240":"code","900f1799":"code","bd3b19a7":"code","34f46beb":"code","8a4652cb":"code","d2037182":"code","8eafb009":"code","539261c5":"code","2633f973":"code","94ec42c6":"code","3c366554":"code","73784a8c":"code","2acccd66":"code","058cf357":"code","fd3359e4":"code","a335aae4":"code","ff75b9c3":"code","201e4327":"code","8d9170dd":"code","02f9fda8":"code","65e9464a":"code","85609cc6":"code","641040dd":"code","ecaa092c":"code","8b48a98a":"code","4e2764b9":"code","30dd30eb":"code","92aed15f":"code","a88cd83d":"code","7d853b19":"code","58160402":"code","3e6f7e8b":"code","c51d15c9":"code","34092dc9":"code","5574063d":"code","0c6bd8a0":"code","c8bfa9d9":"code","13f87c40":"code","b9510cb7":"code","fa30f533":"code","80c65548":"code","72eee264":"code","ee229112":"code","d103e9d1":"code","2e52ca38":"code","662f7031":"code","7b3e7d4f":"code","fb70023b":"code","719e7f8e":"code","de2def35":"code","ae56b7f5":"code","94d8994e":"code","834e165f":"code","463cb2cc":"code","4376888f":"code","b69065e9":"code","f33529de":"code","739e3801":"code","bd0dfdb9":"code","7f73eee6":"code","1cbd6638":"code","c20fa7c4":"code","50cdb16c":"code","d6749846":"code","4ebcc650":"code","8be72e1a":"markdown","8124b084":"markdown","a15f7f0a":"markdown","2f0995b4":"markdown","25c3893a":"markdown","d4a1cdd3":"markdown","9982121f":"markdown","6a1fe82f":"markdown","5509630f":"markdown","4848b04f":"markdown","830cef23":"markdown","96e170b3":"markdown","58a53dd4":"markdown","b3c20b05":"markdown","e0ed69a3":"markdown","d6dce683":"markdown","4ce93e0a":"markdown","5cb22f85":"markdown","ba7a1d00":"markdown","a3bc168b":"markdown","62361397":"markdown","4ebc85a0":"markdown","9c0e830d":"markdown","3ae067e8":"markdown","32b86984":"markdown","74804177":"markdown","0322c59e":"markdown","42df8705":"markdown","c6cee56e":"markdown","77680ca0":"markdown","b1451057":"markdown","cb683f62":"markdown","dd9729f8":"markdown","615e8b18":"markdown","bf4bbcca":"markdown","11f3ff27":"markdown","259ce788":"markdown"},"source":{"224ba1f7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9bde78b8":"df = pd.read_csv('..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv')","061d88e4":"df.head()","bc76fa76":"df.info()","10abd41f":"df.shape","bd60a93a":"df.isna().sum()","b71360a0":"df.sex.value_counts()","ccd14091":"df.sex[df.DEATH_EVENT==1].value_counts()","ee97039a":"fig_dims = (15, 7)\nfig, ax = plt.subplots(figsize=fig_dims)\ndf.sex[df.DEATH_EVENT==1].value_counts().plot(kind='bar',figsize=(10,6),color=['orange','blue'])\nplt.title(\"Count of the number of males and females with heart disease\", color=\"green\")\nplt.xticks(rotation=0);\nax.tick_params(axis='x', colors='red')\nax.tick_params(axis='y', colors='red')","8fd83c3f":"table=pd.crosstab(df.DEATH_EVENT,df.sex)","0126e883":"fig_dims = (15, 7)\nfig, ax = plt.subplots(figsize=fig_dims)\ntable.plot(kind='bar',figsize=fig_dims ,color=['orange','blue'],ax=ax)\nplt.title(\"Frequency of Heart Disease vs Sex\",color=\"green\")\nplt.xlabel(\"0= Heart Disease, 1= No disease\",color=\"green\")\nplt.ylabel(\"Number of people with heart disease\",color=\"green\")\nplt.legend([\"Female\",\"Male\"])\nplt.xticks(rotation=0);\nax.tick_params(axis='x', colors='red')\nax.tick_params(axis='y', colors='red')","703900da":"df.describe()","a4fa867a":"df.hist(figsize=(15,15))","f724c660":"df_agelog=np.log(df[\"age\"])","438e7636":"df_agelog.skew()","de7725f7":"df_agelog.hist(figsize=(5,5))","843ee292":"df.skew()","d5939b56":"df_cret_phos_log=np.log(df['creatinine_phosphokinase'])\ndf_cret_phos_log.skew()","ac6c069f":"df_agelog.hist(figsize=(5,5))","6a9d1ae3":"df_plate_log=np.sqrt(df['platelets'])\ndf_ejec_frac_log=np.log(df['ejection_fraction'])\ndf_ser_sod_log=np.power(df['serum_sodium'],3)\nprint('skew of plate_log={}, skew of ejec_frac={} and skew of ser_sod={}'.format(df_plate_log.skew(),df_ejec_frac_log.skew(),df_ser_sod_log.skew()\n                           ))                         ","bd59f742":"df_plate_log.hist()","a5de009f":"df_ejec_frac_log.hist()","82a69fba":"df_ser_sod_log.hist()","74c061a5":"df[\"age\"]=df_agelog\ndf['creatinine_phosphokinase']=df_cret_phos_log\ndf['platelets']=df_plate_log\ndf['ejection_fraction']=df_ejec_frac_log\ndf['serum_sodium']=df_ser_sod_log","53a308e9":"df.head()","b9612240":"from sklearn.preprocessing import MinMaxScaler\nscal=MinMaxScaler()\nfeatures= ['age', 'creatinine_phosphokinase', 'ejection_fraction', 'platelets', 'serum_sodium','time','serum_creatinine']\ndf[features] = scal.fit_transform(df[features])\ndf.head()","900f1799":"fig_dims = (20, 10)\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.scatterplot(data=df, x=\"creatinine_phosphokinase\", \n                y=\"age\", ax=ax)\nax.tick_params(axis='x', colors='red')\nax.tick_params(axis='y', colors='red')","bd3b19a7":"fig_dims = (20, 10)\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.boxplot(data=df, orient=\"h\", palette=\"Set2\", ax=ax)\nax.tick_params(axis='x', colors='red')\nax.tick_params(axis='y', colors='red')","34f46beb":"from scipy import stats\nz=np.abs(stats.zscore(df.serum_creatinine))# Outlier removal with zscore method.","8a4652cb":"threshold=3\nprint(np.where(z>3))","d2037182":"df.iloc[9]","8eafb009":"z1=np.abs(stats.zscore(df.platelets))","539261c5":"threshold=3\nprint(np.where(z1>3))","2633f973":"df.iloc[15]","94ec42c6":"cor_mat=df.corr()\nfig,ax=plt.subplots(figsize=(15,10))\nsns.heatmap(cor_mat,annot=True,linewidths=0.5,fmt=\".3f\")","3c366554":"df.DEATH_EVENT.values","73784a8c":"x=df.drop(\"DEATH_EVENT\",axis=1).values\ny=df.DEATH_EVENT","2acccd66":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(x,y,random_state=0,test_size=0.2)","058cf357":"from sklearn.metrics import accuracy_score,recall_score,f1_score,precision_score,roc_auc_score,confusion_matrix\n\ndef metrics(Y_test,Y_pred):\n    acc=accuracy_score(Y_test,Y_pred)\n    rec=recall_score(Y_test,Y_pred)\n    f1=f1_score(Y_test,Y_pred)\n    print(\"Accuracy= {}\".format(acc),\n          \"\\n Recall= {}\".format(rec),\n         \"\\n f1 score= {}\".format(f1))\n    l1=[ acc,rec,f1]\n    return l1","fd3359e4":"from sklearn.naive_bayes import ComplementNB\nclf = ComplementNB()\nclf.fit(X_train,Y_train)","a335aae4":"Naive_bayes_preds = clf.predict(X_test)\nNBayes=metrics(Y_test, Naive_bayes_preds)","ff75b9c3":"# confusion matrix \ncm_NBayes = confusion_matrix(Y_test, Naive_bayes_preds)  \nprint (\"Confusion Matrix : \\n\", cm_NBayes)  \nTN=cm_NBayes[0,0]# True is of prediction and Negative is of test\nFP=cm_NBayes[0,1]# False is of prediction and Positive is of test\nFN=cm_NBayes[1,0]# True is of prediction and Negative is of test\nTP=cm_NBayes[1,1]# False is of prediction and Positive is of test\nprint(\"True Positive cases= {} True Negative cases={} False Positive cases={} False Negative cases= {}\".format(TP,TN,FP,FN))\n# accuracy score of the model \n#print('Test accuracy = ', accuracy_score(Y_test, prediction))\nmetrics(Y_test, Naive_bayes_preds)#Recall It answers the question how many are at the risk of dying and how many is correctly predicted.\n#F1-score is best when there is uneven class distribution or unsymmetric dataset.\n\nprecision_NBayes=TP\/(TP+FP)\nprint(\"precision=\", precision_NBayes)#How many of those who we labeled as dead are actually died due to heart disease?\nSpecificity_NBayes = TN\/(TN+FP)\nprint(\"Specificity=\", Specificity_NBayes)#Of all the people who are healthy, how many of those did we correctly predict?","201e4327":"accuracy_NBayes=NBayes[0]\naccuracy_NBayes\nrecall_NBayes=NBayes[1]\nrecall_NBayes\nf1score_NBayes=NBayes[2]\nf1score_NBayes\nprint(\"acc= {}, rec= {}, f1score ={}\".format(accuracy_NBayes,recall_NBayes,f1score_NBayes))","8d9170dd":"plt.figure(figsize=(5,5))\n\nsns.heatmap(data=cm_NBayes,linewidths=.5, annot=True,square = True,  cmap = 'OrRd')\n\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nsc_new_N=round(NBayes[0],3)\nall_sample_title = 'Accuracy Score: {0}'.format(sc_new_N)\nplt.title(all_sample_title, size = 15)","02f9fda8":"import statsmodels.api as sm ","65e9464a":"log_clas = sm.Logit(Y_train,X_train).fit() ","85609cc6":"log_clas.summary()","641040dd":"logit_Y_pred = log_clas.predict(X_test) \nprediction = list(map(round, logit_Y_pred)) \n  \n# comparing original and predicted values of y \nprint('Acutal values', list(Y_test.values)) \nprint('Predictions :', prediction) \nlogclas=metrics(Y_test, prediction)","ecaa092c":"from sklearn.metrics import (confusion_matrix,  \n                           accuracy_score) \n  \n# confusion matrix \ncm_logit = confusion_matrix(Y_test, prediction)  \nprint (\"Confusion Matrix : \\n\", cm_logit)  \nTN=cm_logit[0,0]# True is of prediction and Negative is of test\nFP=cm_logit[0,1]# False is of prediction and Positive is of test\nFN=cm_logit[1,0]# True is of prediction and Negative is of test\nTP=cm_logit[1,1]# False is of prediction and Positive is of test\nprint(\"True Positive cases= {} True Negative cases={} False Positive cases={} False Negative cases= {}\".format(TP,TN,FP,FN))\n# accuracy score of the model \n#print('Test accuracy = ', accuracy_score(Y_test, prediction))\nmetrics(Y_test, prediction)#Recall It answers the question how many are at the risk of dying and how many is correctly predicted.\n#F1-score is best when there is uneven class distribution or unsymmetric dataset.\nprecision_log_clas=TP\/(TP+FP)\nprint(\"precision=\", precision_log_clas)#How many of those who we labeled as dead are actually died due to heart disease?\nSpecificity_log_clas = TN\/(TN+FP)\nprint(\"Specificity=\", Specificity_log_clas)#Of all the people who are healthy, how many of those did we correctly predict?","8b48a98a":"accuracy_logclas=logclas[0]\naccuracy_logclas\nrecall_logclas=logclas[1]\nrecall_logclas\nf1score_logclas=logclas[2]\nf1score_logclas\nprint(\"acc= {}, rec= {}, f1score ={}\".format(accuracy_logclas,recall_logclas,f1score_logclas))","4e2764b9":"plt.figure(figsize=(5,5))\n\nsns.heatmap(data=cm_logit,linewidths=.5, annot=True,square = True,  cmap = 'OrRd')\n\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nsc_new_l=round(logclas[0],3)\nall_sample_title = 'Accuracy Score: {0}'.format(sc_new_l)\nplt.title(all_sample_title, size = 15)","30dd30eb":"# Defining the decision tree algorithm\nfrom sklearn.tree import DecisionTreeClassifier#for checking testing results\nfrom sklearn.metrics import classification_report, confusion_matrix#for visualizing tree \nfrom sklearn.tree import plot_tree\ndtree=DecisionTreeClassifier()\ndtree.fit(X_train,Y_train)\n\nprint('Decision Tree Classifer Created')","92aed15f":"# Predicting the values of test data\ny_pred = dtree.predict(X_test)\nprint(\"Classification report - \\n\", classification_report(Y_test,y_pred))","a88cd83d":"Dectree=metrics(Y_test, y_pred)","7d853b19":"# confusion matrix \ncm_dectree = confusion_matrix(Y_test, y_pred)  \nprint (\"Confusion Matrix : \\n\", cm_dectree)  \nTN=cm_dectree[0,0]# True is of prediction and Negative is of test\nFP=cm_dectree[0,1]# False is of prediction and Positive is of test\nFN=cm_dectree[1,0]# True is of prediction and Negative is of test\nTP=cm_dectree[1,1]# False is of prediction and Positive is of test\nprint(\"True Positive cases= {} True Negative cases={} False Positive cases={} False Negative cases= {}\".format(TP,TN,FP,FN))\n# accuracy score of the model \n#print('Test accuracy = ', accuracy_score(Y_test, prediction))\nmetrics(Y_test, y_pred)#Recall It answers the question how many are at the risk of dying and how many is correctly predicted.\n#F1-score is best when there is uneven class distribution or unsymmetric dataset.\nprecision_dectree=TP\/(TP+FP)\nprint(\"precision=\", precision_dectree)#How many of those who we labeled as dead are actually died due to heart disease?\nSpecificity_dectree = TN\/(TN+FP)\nprint(\"Specificity=\", Specificity_dectree)#Of all the people who are healthy, how many of those did we correctly predict?","58160402":"accuracy_Dectree=Dectree[0]\naccuracy_Dectree\nrecall_Dectree=Dectree[1]\nrecall_Dectree\nf1score_Dectree=Dectree[2]\nf1score_Dectree\nprint(\"acc= {}, rec= {}, f1score ={}\".format(accuracy_Dectree,recall_Dectree,f1score_Dectree))","3e6f7e8b":"cm = confusion_matrix(Y_test,y_pred)\nplt.figure(figsize=(5,5))\n\nsns.heatmap(data=cm,linewidths=.5, annot=True,square = True,  cmap = 'OrRd')\n\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nsc_new=round(dtree.score(X_test, Y_test),3)\nall_sample_title = 'Accuracy Score: {0}'.format(sc_new)\nplt.title(all_sample_title, size = 15)\n","c51d15c9":"# Visualising the graph without the use of graphviz\n\nplt.figure(figsize = (100,100))\ndec_tree = plot_tree(decision_tree=dtree, feature_names = df.columns, \n                     class_names =df.columns.values , filled = True , precision = 4, rounded = True)\n","34092dc9":"np.random.seed(42)\nfrom sklearn.svm import SVC\nSVC_clf=SVC()\nSVC_clf.fit(X_train,Y_train)\nSVC_score=SVC_clf.score(X_test,Y_test)\nSVC_Y_pred=SVC_clf.predict(X_test)\n#print(SVC_score)\nSVC=metrics(Y_test,SVC_Y_pred)","5574063d":"# confusion matrix \ncm_SVC = confusion_matrix(Y_test,SVC_Y_pred)  \nprint (\"Confusion Matrix : \\n\", cm_SVC)  \nTN=cm_SVC[0,0]# True is of prediction and Negative is of test\nFP=cm_SVC[0,1]# False is of prediction and Positive is of test\nFN=cm_SVC[1,0]# True is of prediction and Negative is of test\nTP=cm_SVC[1,1]# False is of prediction and Positive is of test\nprint(\"True Positive cases= {} True Negative cases={} False Positive cases={} False Negative cases= {}\".format(TP,TN,FP,FN))\n# accuracy score of the model \n#print('Test accuracy = ', accuracy_score(Y_test, prediction))\nmetrics(Y_test,SVC_Y_pred)#Recall It answers the question how many are at the risk of dying and how many is correctly predicted.\n#F1-score is best when there is uneven class distribution or unsymmetric dataset.\nprecision_SVC=TP\/(TP+FP)\nprint(\"precision=\", precision_SVC)#How many of those who we labeled as dead are actually died due to heart disease?\nSpecificity_SVC = TN\/(TN+FP)\nprint(\"Specificity=\", Specificity_SVC)#Of all the people who are healthy, how many of those did we correctly predict?","0c6bd8a0":"accuracy_SVC=SVC[0]\naccuracy_SVC\nrecall_SVC=SVC[1]\nrecall_SVC\nf1score_SVC=SVC[2]\nf1score_SVC\nprint(\"acc= {}, rec= {}, f1score ={}\".format(accuracy_SVC,recall_SVC,f1score_SVC))","c8bfa9d9":"plt.figure(figsize=(5,5))\n\nsns.heatmap(data=cm_SVC,linewidths=.5, annot=True,square = True,  cmap = 'OrRd')\n\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nsc_new_S=round(SVC[0],3)\nall_sample_title = 'Accuracy Score: {0}'.format(sc_new_S)\nplt.title(all_sample_title, size = 15)","13f87c40":"np.random.seed(42)\nfrom sklearn.ensemble import RandomForestClassifier\nRF_clf=RandomForestClassifier(n_estimators=450)\nRF_clf.fit(X_train,Y_train)\nRF_score=RF_clf.score(X_test,Y_test)\nRF_Y_pred=RF_clf.predict(X_test)\n#print(RF_score)\nRandFor=metrics(Y_test,RF_Y_pred)","b9510cb7":"# confusion matrix \ncm_RandFor = confusion_matrix(Y_test,RF_Y_pred)  \nprint (\"Confusion Matrix : \\n\", cm_RandFor)  \nTN=cm_RandFor[0,0]# True is of prediction and Negative is of test\nFP=cm_RandFor[0,1]# False is of prediction and Positive is of test\nFN=cm_RandFor[1,0]# True is of prediction and Negative is of test\nTP=cm_RandFor[1,1]# False is of prediction and Positive is of test\nprint(\"True Positive cases= {} True Negative cases={} False Positive cases={} False Negative cases= {}\".format(TP,TN,FP,FN))\n# accuracy score of the model \n#print('Test accuracy = ', accuracy_score(Y_test, prediction))\nmetrics(Y_test,RF_Y_pred)#Recall It answers the question how many are at the risk of dying and how many is correctly predicted.\n#F1-score is best when there is uneven class distribution or unsymmetric dataset.\nprecision_Randfor=TP\/(TP+FP)\nprint(\"precision=\", precision_Randfor)#How many of those who we labeled as dead are actually died due to heart disease?\nSpecificity_Randfor = TN\/(TN+FP)\nprint(\"Specificity=\", Specificity_Randfor)#Of all the people who are healthy, how many of those did we correctly predict?","fa30f533":"accuracy_RandFor=RandFor[0]\naccuracy_RandFor\nrecall_RandFor=RandFor[1]\nrecall_RandFor\nf1score_RandFor=RandFor[2]\nf1score_RandFor\nprint(\"acc= {}, rec= {}, f1score ={}\".format(accuracy_RandFor,recall_RandFor,f1score_RandFor))","80c65548":"plt.figure(figsize=(5,5))\n\nsns.heatmap(data=cm_RandFor,linewidths=.5, annot=True,square = True,  cmap = 'OrRd')\n\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nsc_new_R=round(RandFor[0],3)\nall_sample_title = 'Accuracy Score: {0}'.format(sc_new_R)\nplt.title(all_sample_title, size = 15)","72eee264":"from xgboost import XGBClassifier\nXGB_clf=XGBClassifier()\nXGB_clf.fit(X_train,Y_train)\nXGB_score=XGB_clf.score(X_test,Y_test)\nXGB_Y_pred=XGB_clf.predict(X_test)\neGradBoost=metrics(Y_test,XGB_Y_pred)","ee229112":"# confusion matrix \ncm_eGradBoost = confusion_matrix(Y_test,XGB_Y_pred)  \nprint (\"Confusion Matrix : \\n\", cm_eGradBoost)  \nTN=cm_eGradBoost[0,0]# True is of prediction and Negative is of test\nFP=cm_eGradBoost[0,1]# False is of prediction and Positive is of test\nFN=cm_eGradBoost[1,0]# True is of prediction and Negative is of test\nTP=cm_eGradBoost[1,1]# False is of prediction and Positive is of test\nprint(\"True Positive cases= {} True Negative cases={} False Positive cases={} False Negative cases= {}\".format(TP,TN,FP,FN))\n# accuracy score of the model \n#print('Test accuracy = ', accuracy_score(Y_test, prediction))\nmetrics(Y_test,XGB_Y_pred)#Recall It answers the question how many are at the risk of dying and how many is correctly predicted.\n#F1-score is best when there is uneven class distribution or unsymmetric dataset.\nprecision_eGradBoost=TP\/(TP+FP)\nprint(\"precision=\", precision_eGradBoost)#How many of those who we labeled as dead are actually died due to heart disease?\nSpecificity_eGradBoost = TN\/(TN+FP)\nprint(\"Specificity=\", Specificity_eGradBoost)#Of all the people who are healthy, how many of those did we correctly predict?","d103e9d1":"accuracy_eGradBoost=eGradBoost[0]\naccuracy_eGradBoost\nrecall_eGradBoost=eGradBoost[1]\nrecall_eGradBoost\nf1score_eGradBoost=eGradBoost[2]\nf1score_eGradBoost\nprint(\"acc= {}, rec= {}, f1score ={}\".format(accuracy_eGradBoost,recall_eGradBoost,f1score_eGradBoost))","2e52ca38":"plt.figure(figsize=(5,5))\n\nsns.heatmap(data=cm_eGradBoost,linewidths=.5, annot=True,square = True,  cmap = 'OrRd')\n\nplt.ylabel('Actual label')\nplt.xlabel('Predicted label')\nsc_new_R=round(eGradBoost[0],3)\nall_sample_title = 'Accuracy Score: {0}'.format(sc_new_R)\nplt.title(all_sample_title, size = 15)","662f7031":"model_comp = pd.DataFrame({'Model': ['Logistic Regression','Random Forest',\n                    'Naive Bayes','Support Vector Machine',\"Decison Tree\",\"Extreme Gradient Classifier\"], 'Accuracy': [accuracy_logclas*100,\n                    accuracy_RandFor*100,accuracy_NBayes*100,accuracy_SVC*100,accuracy_Dectree*100,accuracy_eGradBoost*100], 'Precision': [precision_log_clas*100,\n                    precision_Randfor*100,precision_NBayes*100,precision_SVC*100,precision_dectree*100,precision_eGradBoost*100], 'Recall': [recall_logclas*100,\n                    recall_RandFor*100,recall_NBayes*100,recall_SVC*100,recall_Dectree*100,recall_eGradBoost*100], 'Specficity': [Specificity_log_clas*100,\n                    Specificity_Randfor*100,Specificity_NBayes*100,Specificity_SVC*100,Specificity_dectree*100,Specificity_eGradBoost*100],'F1-score': [f1score_logclas*100,\n                    f1score_RandFor*100,f1score_NBayes*100,f1score_SVC*100,f1score_Dectree*100,f1score_eGradBoost*100]})\nmodel_comp","7b3e7d4f":"print(\" Best evaluation parameters achieved with Random Forest:\") \nmetrics(Y_test,RF_Y_pred)","fb70023b":"final_metrics={'Accuracy': RF_clf.score(X_test,Y_test),\n                   'Precision': precision_score(Y_test,RF_Y_pred),\n                   'Recall': recall_score(Y_test,RF_Y_pred),\n                   'F1': f1_score(Y_test,RF_Y_pred),\n                   'AUC': roc_auc_score(Y_test,RF_Y_pred)}\n\nmetrics=pd.DataFrame(final_metrics,index=[0])\n\nmetrics.T.plot.bar(title='Final metric evaluation',legend=False)","719e7f8e":"from sklearn.metrics import roc_curve\n\n# roc curve for models\nfpr1, tpr1, thresh1 = roc_curve(Y_test, RF_Y_pred, pos_label=1)\nfpr2, tpr2, thresh2 = roc_curve(Y_test, prediction,pos_label=1)\nfpr3, tpr3, thresh3 = roc_curve(Y_test, Naive_bayes_preds, pos_label=1)\nfpr4, tpr4, thresh4 = roc_curve(Y_test, SVC_Y_pred, pos_label=1)\nfpr5, tpr5, thresh5 = roc_curve(Y_test, y_pred, pos_label=1)\nfpr6, tpr6, thresh6 = roc_curve(Y_test, XGB_Y_pred, pos_label=1)\nrandom_probs = [0 for i in range(len(Y_test))]\np_fpr, p_tpr, _ = roc_curve(Y_test, random_probs, pos_label=1)","de2def35":"import matplotlib.pyplot as plt\nplt.style.use('seaborn')\n\n# plot roc curves\nplt.plot(fpr1, tpr1, linestyle='--',color='orange', label='Random Forest')\nplt.plot(fpr2, tpr2, linestyle='--',color='grey', label='Logistic Classification')\nplt.plot(fpr3, tpr3, linestyle='--',color='grey', label='Naive Bayes Classification')\nplt.plot(fpr4, tpr4, linestyle='--',color='grey', label='Support Vector Classification')\nplt.plot(fpr5, tpr5, linestyle='--',color='grey', label='Desicion Tree')\nplt.plot(fpr6, tpr6, linestyle='--',color='brown', label='Extreme Gradient Classification')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC curve')\n# x label\nplt.xlabel('False Positive Rate')\n# y label\nplt.ylabel('True Positive rate')\n\nplt.legend(loc='best')\nplt.savefig('ROC',dpi=300)\nplt.show();","ae56b7f5":"user_input=input(\"Enter the values one by one\")\nuser_input=user_input.split(\",\")\n\n\nfor i in range(len(user_input)):\n    # convert each item to int type\n    user_input[i] = float(user_input[i])\n\nuser_input=np.array(user_input)\nuser_input=user_input.reshape(1,-1)\nuser_input=scal.transform(user_input)\nscv_Y_pred=scv.predict(user_input)\nif(scv_Y_pred[0]==0):\n  print(\"Warning! You have chances of getting a heart disease!\")\nelse:\n  print(\"You are healthy and are less likely to get a heart disease!\")\n","94d8994e":"import pickle as pkl\npkl.dump(Knn_clf,open(\"final_model.p\",\"wb\"))","834e165f":"import sklearn\nsklearn_version = sklearn.__version__\nprint(sklearn_version)","463cb2cc":"!pip install streamlit\n!pip install pyngrok===4.1.1\nfrom pyngrok import ngrok","4376888f":"%%writefile healthy-heart-app.py\nimport streamlit as st\nimport base64\nimport sklearn\nimport numpy as np\nimport pickle as pkl\nfrom sklearn.preprocessing import MinMaxScaler\nscal=MinMaxScaler()\n#Load the saved model\nmodel=pkl.load(open(\"final_model.p\",\"rb\"))\n\n\n\n\n\nst.set_page_config(page_title=\"Healthy Heart App\",page_icon=\"\u2695\ufe0f\",layout=\"centered\",initial_sidebar_state=\"expanded\")\n\n\n\ndef preprocess(age,sex,cp,trestbps,restecg,chol,fbs,thalach,exang,oldpeak,slope,ca,thal ):   \n \n    \n    # Pre-processing user input   \n    if sex==\"male\":\n        sex=1 \n    else: sex=0\n    \n    \n    if cp==\"Typical angina\":\n        cp=0\n    elif cp==\"Atypical angina\":\n        cp=1\n    elif cp==\"Non-anginal pain\":\n        cp=2\n    elif cp==\"Asymptomatic\":\n        cp=2\n    \n    if exang==\"Yes\":\n        exang=1\n    elif exang==\"No\":\n        exang=0\n \n    if fbs==\"Yes\":\n        fbs=1\n    elif fbs==\"No\":\n        fbs=0\n \n    if slope==\"Upsloping: better heart rate with excercise(uncommon)\":\n        slope=0\n    elif slope==\"Flatsloping: minimal change(typical healthy heart)\":\n          slope=1\n    elif slope==\"Downsloping: signs of unhealthy heart\":\n        slope=2  \n \n    if thal==\"fixed defect: used to be defect but ok now\":\n        thal=6\n    elif thal==\"reversable defect: no proper blood movement when excercising\":\n        thal=7\n    elif thal==\"normal\":\n        thal=2.31\n\n    if restecg==\"Nothing to note\":\n        restecg=0\n    elif restecg==\"ST-T Wave abnormality\":\n        restecg=1\n    elif restecg==\"Possible or definite left ventricular hypertrophy\":\n        restecg=2\n\n\n    user_input=[age,sex,cp,trestbps,restecg,chol,fbs,thalach,exang,oldpeak,slope,ca,thal]\n    user_input=np.array(user_input)\n    user_input=user_input.reshape(1,-1)\n    user_input=scal.fit_transform(user_input)\n    prediction = model.predict(user_input)\n\n    return prediction\n\n    \n\n       \n    # front end elements of the web page \nhtml_temp = \"\"\" \n    <div style =\"background-color:pink;padding:13px\"> \n    <h1 style =\"color:black;text-align:center;\">Healthy Heart App<\/h1> \n    <\/div> \n    \"\"\"\n      \n# display the front end aspect\nst.markdown(html_temp, unsafe_allow_html = True) \nst.subheader('by Amlan Mohanty ')\n      \n# following lines create boxes in which user can enter data required to make prediction\nage=st.selectbox (\"Age\",range(1,121,1))\nsex = st.radio(\"Select Gender: \", ('male', 'female'))\ncp = st.selectbox('Chest Pain Type',(\"Typical angina\",\"Atypical angina\",\"Non-anginal pain\",\"Asymptomatic\")) \ntrestbps=st.selectbox('Resting Blood Sugar',range(1,500,1))\nrestecg=st.selectbox('Resting Electrocardiographic Results',(\"Nothing to note\",\"ST-T Wave abnormality\",\"Possible or definite left ventricular hypertrophy\"))\nchol=st.selectbox('Serum Cholestoral in mg\/dl',range(1,1000,1))\nfbs=st.radio(\"Fasting Blood Sugar higher than 120 mg\/dl\", ['Yes','No'])\nthalach=st.selectbox('Maximum Heart Rate Achieved',range(1,300,1))\nexang=st.selectbox('Exercise Induced Angina',[\"Yes\",\"No\"])\noldpeak=st.number_input('Oldpeak')\nslope = st.selectbox('Heart Rate Slope',(\"Upsloping: better heart rate with excercise(uncommon)\",\"Flatsloping: minimal change(typical healthy heart)\",\"Downsloping: signs of unhealthy heart\"))\nca=st.selectbox('Number of Major Vessels Colored by Flourosopy',range(0,5,1))\nthal=st.selectbox('Thalium Stress Result',range(1,8,1))\n\n\n\n#user_input=preprocess(sex,cp,exang, fbs, slope, thal )\npred=preprocess(age,sex,cp,trestbps,restecg,chol,fbs,thalach,exang,oldpeak,slope,ca,thal)\n\n\n\n\nif st.button(\"Predict\"):    \n  if pred[0] == 0:\n    st.error('Warning! You have high risk of getting a heart attack!')\n    \n  else:\n    st.success('You have lower risk of getting a heart disease!')\n    \n   \n\n\n\nst.sidebar.subheader(\"About App\")\n\nst.sidebar.info(\"This web app is helps you to find out whether you are at a risk of developing a heart disease.\")\nst.sidebar.info(\"Enter the required fields and click on the 'Predict' button to check whether you have a healthy heart\")\nst.sidebar.info(\"Don't forget to rate this app\")\n\n\n\nfeedback = st.sidebar.slider('How much would you rate this app?',min_value=0,max_value=5,step=1)\n\nif feedback:\n  st.header(\"Thank you for rating the app!\")\n  st.info(\"Caution: This is just a prediction and not doctoral advice. Kindly see a doctor if you feel the symptoms persist.\") \n\n\n     \n\n\n\n\n\n\n\n\n\n\n\n","b69065e9":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Activation,Dropout ,Flatten\nfrom tensorflow.keras.layers.experimental.preprocessing import Normalization","f33529de":"normalize = Normalization()","739e3801":"X_train.shape","bd0dfdb9":"model = Sequential([\n    Dense(12, activation=tf.nn.relu,input_shape=(239,12)),\n    Dropout(0.5),\n    Dense(1, activation=tf.nn.sigmoid),\n])\n\nmodel.compile(loss='binary_crossentropy', optimizer='rmsprop',metrics=['accuracy'])","7f73eee6":"model.summary()","1cbd6638":"from tensorflow.keras.callbacks import EarlyStopping\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', patience=10,restore_best_weights=True)","c20fa7c4":"model.fit(x=X_train, \n          y=Y_train, \n          epochs=5,\n          batch_size=10,\n          validation_data=(X_test, Y_test),\n          callbacks=[early_stop]\n          )","50cdb16c":"history_dict=model.history.history\nhistory_dict","d6749846":"model.evaluate(X_test, Y_test)","4ebcc650":"pred = y_pred=model.predict_classes(np.expand_dims(X_test[4], axis=0))\nprint(\"prediction= {} and verses real={}\".format(pred,X_test[4]))","8be72e1a":"XGBoost is a popular and efficient open-source implementation of the gradient boosted trees algorithm. Gradient boosting is a supervised learning algorithm, which attempts to accurately predict a target variable by combining the estimates of a set of simpler, weaker models.","8124b084":"# Creating Features and Target variable","a15f7f0a":"We can see from the coefficiants that age and serum_creatinine are significant and highly impacts death rate . Conversely time taken or the follow up period is highly inversely related and highly significant. Y variable is 42.47 correctly explained by the predictor variables given by Pseudo R-square.","2f0995b4":"### Variables in the dataset:\n\n1.Age: Age of the patient\n\n2.Anaemia: If the patient had the haemoglobin below the normal range\n\n3.Creatinine_phosphokinase: The level of the creatine phosphokinase in the blood in mcg\/L\n\n4.Diabetes: If the patient was diabetic\n\n5.Ejection_fraction: Ejection fraction is a measurement of how much blood the left ventricle pumps out with each contraction\n\n6.High_blood_pressure: If the patient had hypertension\n\n7.Platelets: Platelet count of blood in kiloplatelets\/mL\n\n<a href=\"https:\/\/www.healthline.com\/health\/high-creatinine-symptoms\">8.Serum_creatinine: The level of serum creatinine in the blood in mg\/dL<\/a>\n\n9.Serum_sodium: The level of serum sodium in the blood in mEq\/L\n\n10.Sex: The sex of the patient\n\n11.Smoking: If the patient smokes actively or ever did in past\n\n12.Time: It is the time of the patient's follow-up visit for the disease in months\n\n13.Death_event: If the patient deceased during the follow-up period","25c3893a":"It shows the correlations between various features of the dataset like sex of a patient determines whether he smokes or not or whether the lower time of the patient's follow-up visit for the disease in months leads to death. This is important as it shows no Multicolinearity between variables.","d4a1cdd3":"Works by constructing a hyperplane that seperates points between two classes.The observed training observations are seperate into parts by a maximal distance from the hyperplane. The maximal distance is the margin.","9982121f":"**Naive bayes**\nIt is a set of supervised algorithmn that applies the bayes theorem with the assumption that independence between every pair of feature.","6a1fe82f":"# Looking at the evaluation metrics for our best model\nAs we can see, the Random Forest Classifier gives us an accuracy of 85%.\n\nLet us evaluate the model now.","5509630f":"**Detecting Outliers and Boxplots**","4848b04f":"**Serum_creatinine**","830cef23":"# Fitting and Comparing different Models","96e170b3":"* We can say that from AUC that there is a almost 80% chance of the model to correct predict the death and the healtiness of apatient.\n* Precision says 90%+ of those who we labeled as dead are actually died due to heart disease.\n* Recall answers the question 60%+ are at the risk of dying and they are correctly predicted.\n* Accuarcy gives the how accurate our prediction where to the actual values.\n* F1-score is best when there is uneven class distribution or unsymmetric dataset.","58a53dd4":"# Using NN to build classification model","b3c20b05":"Data Visualization","e0ed69a3":"# Bagging vs Boosting\nIn this case  we can see as false prediction of death reamins low Random Forrest which is a Bagging of decision trees does better. While if there is a chance of falsely predicting death of a patient due to Cardiovascular problems Boosting algorithmn does better. It can be due various reasons but as we don't falsely predict a survival of a patient we should go with the Random Forest Classification model.","d6dce683":"### Logistic Regresion","4ce93e0a":"#### Data Transformation\nWe will apply Log Transformation to convert the all the contionous data which are all left skewed to normal distribution.","5cb22f85":"<a>The structure of this notebook<\/a>\n<ol>\n<li>Introduction<\/li>\n<li>Data and preparation-visualisation\/cleaning <\/li>\n<li>Build predictive model: Naive bayes, Logistic Regression, Desicion Tree, Support Vector Classification <\/li>\n<li>Build Ensemble Model like Bagging and Boosting<\/li>\n<li>Choose the Best Model<\/li>\n<li>Comapre between the best found model and a sequential Neural network model<\/li>\n<li>Deploy that Model<\/li>\n<\/ol>","ba7a1d00":"# Death-->1\n# Alive-->0","a3bc168b":"**Platelets**","62361397":"# Creating Features and Target variable","4ebc85a0":"When the platelet count drops below 20,000, the patient may have spontaneous bleeding that may result in death. Thrombocytopenia occurs due to platelet destruction or impaired platelet production. In this case as it is normalized we are seeing it as 0.076 but it si definately near if not below 20,000 platelets. Hence leading to death hence we are not removing any outliers.","9c0e830d":"### Random Forest Classification\nIt uses bagging technique with random feature selection to add additional diversity to the decision tree model.It ensemble a group of decision trees one by one by voting among them.","3ae067e8":"#### We can say this is a late stage of CKD and an opportunity to get the best treatment outcome has unfortunately been lost. When serum creatinine level is 10.0 mg\/dl, it means that 90% of kidney function has already been lost and this points to end stage kidney disease (ESKD).Hence Death event is very much related due to this outlier. As we we are are looking for death events we won't remove such outliers in the data.","32b86984":"# Building a Correlation Matrix","74804177":"Logistic regression is used for classification where the response variable is categorical not numerical.","0322c59e":"# Ensemble Techniques","42df8705":"### **XGBoost**","c6cee56e":"# User Input","77680ca0":"Cardiovascular diseases (CVDs) are the number 1 cause of death globally, taking an estimated 17.9 million lives each year, which accounts for 31% of all deaths worlwide.\nHeart failure is a common event caused by CVDs and this dataset contains 12 features that can be used to predict mortality by heart failure.\n\nMost cardiovascular diseases can be prevented by addressing behavioural risk factors such as tobacco use, unhealthy diet and obesity, physical inactivity and harmful use of alcohol using population-wide strategies.\n\nPeople with cardiovascular disease or who are at high cardiovascular risk (due to the presence of one or more risk factors such as hypertension, diabetes, hyperlipidaemia or already established disease) need early detection and management wherein a machine learning model can be of great help.\n","b1451057":"# Decison Tree","cb683f62":"### Scaling:\nReduces the weight attached high valued continous data into a predefined scale.Here Min-Max will scale each variable in 0 to 1 with one being the highest value instead of any arbitary value which would have shifted the scale unfairly in one predictor varable's favor.","dd9729f8":"# **Support Vector Classification**","615e8b18":"Binary branching structure to classify an arbitary input X. Each node in a tree contains asimple feature comparison against some field.Result is either true or false which determines which direction to proceed. Also known as CART.Here it is for classification.","bf4bbcca":"# Splitting the data into train and test sets","11f3ff27":"# Women-->0\n# Men-->1","259ce788":"# Comparisons\u00b6"}}