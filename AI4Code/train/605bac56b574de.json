{"cell_type":{"bf7058fb":"code","c808814f":"code","9f3fa100":"code","af149318":"code","1c28bc3c":"code","0d5ead08":"code","2982f7af":"code","cb71763c":"code","59166bf7":"code","00c6fb85":"code","108a386c":"code","2c687520":"code","0162c25d":"code","71b47780":"code","a36348f1":"code","523eb05b":"code","88a8c114":"code","779f0ff0":"code","4309b147":"code","19c6d8da":"code","31e5fefa":"code","0678d010":"code","f839ffd3":"code","59543042":"code","06d185da":"code","a85ccb95":"code","0931173d":"code","3e3a25c2":"code","a58b7cfa":"code","0c8376de":"code","8cbb7105":"code","6226cfa4":"code","61d95b5a":"code","828ec352":"code","e199702d":"markdown","9fcb354e":"markdown","40a3b807":"markdown","2f6c72ac":"markdown","fc66bfc1":"markdown","202d9158":"markdown","52abcefc":"markdown","a3d4b892":"markdown","66dd3425":"markdown"},"source":{"bf7058fb":"import math\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gc\n\nimport lightgbm as lgb\nimport optuna\nfrom optuna.samplers import TPESampler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold\nfrom sklearn import metrics\nfrom sklearn.metrics import roc_auc_score, plot_roc_curve","c808814f":"INT8_MIN = np.iinfo(np.int8).min\nINT8_MAX = np.iinfo(np.int8).max\nINT16_MIN = np.iinfo(np.int16).min\nINT16_MAX = np.iinfo(np.int16).max\nINT32_MIN = np.iinfo(np.int32).min\nINT32_MAX = np.iinfo(np.int32).max\n\nFLOAT16_MIN = np.finfo(np.float16).min\nFLOAT16_MAX = np.finfo(np.float16).max\nFLOAT32_MIN = np.finfo(np.float32).min\nFLOAT32_MAX = np.finfo(np.float32).max\n\n\ndef memory_usage(data, detail = 1):\n    if detail:\n        display(data.memory_usage())\n    memory = data.memory_usage().sum() \/ (1024 * 1024)\n    print(\"Memory usage : {0:.2f}MB\".format(memory))\n    return memory\n\n\ndef compress_dataset(data):\n    memory_before_compress = memory_usage(data, 0)\n    print()\n    print('=' * 50)\n    for col in data.columns:\n        col_dtype = data[col][:100].dtype\n\n        if col_dtype != 'object':\n            print(\"Name: {0:24s} Type: {1}\".format(col, col_dtype))\n            col_series = data[col]\n            col_min = col_series.min()\n            col_max = col_series.max()\n\n            if col_dtype == 'float64':\n                print(\" variable min: {0:15s} max: {1:15s}\".format(str(np.round(col_min, 4)), str(np.round(col_max, 4))))\n                if (col_min > FLOAT16_MIN) and (col_max < FLOAT16_MAX):\n                    data[col] = data[col].astype(np.float16)\n                    print(\"  float16 min: {0:15s} max: {1:15s}\".format(str(FLOAT16_MIN), str(FLOAT16_MAX)))\n                    print(\"compress float64 --> float16\")\n                elif (col_min > FLOAT32_MIN) and (col_max < FLOAT32_MAX):\n                    data[col] = data[col].astype(np.float32)\n                    print(\"  float32 min: {0:15s} max: {1:15s}\".format(str(FLOAT32_MIN), str(FLOAT32_MAX)))\n                    print(\"compress float64 --> float32\")\n                else:\n                    pass\n                memory_after_compress = memory_usage(data, 0)\n                print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) \/ memory_before_compress))\n                print('=' * 50)\n\n            if col_dtype == 'int64':\n                print(\" variable min: {0:15s} max: {1:15s}\".format(str(col_min), str(col_max)))\n                type_flag = 64\n                if (col_min > INT8_MIN \/ 2) and (col_max < INT8_MAX \/ 2):\n                    type_flag = 8\n                    data[col] = data[col].astype(np.int8)\n                    print(\"     int8 min: {0:15s} max: {1:15s}\".format(str(INT8_MIN), str(INT8_MAX)))\n                elif (col_min > INT16_MIN) and (col_max < INT16_MAX):\n                    type_flag = 16\n                    data[col] = data[col].astype(np.int16)\n                    print(\"    int16 min: {0:15s} max: {1:15s}\".format(str(INT16_MIN), str(INT16_MAX)))\n                elif (col_min > INT32_MIN) and (col_max < INT32_MAX):\n                    type_flag = 32\n                    data[col] = data[col].astype(np.int32)\n                    print(\"    int32 min: {0:15s} max: {1:15s}\".format(str(INT32_MIN), str(INT32_MAX)))\n                    type_flag = 1\n                else:\n                    pass\n                memory_after_compress = memory_usage(data, 0)\n                print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) \/ memory_before_compress))\n                if type_flag == 32:\n                    print(\"compress (int64) ==> (int32)\")\n                elif type_flag == 16:\n                    print(\"compress (int64) ==> (int16)\")\n                else:\n                    print(\"compress (int64) ==> (int8)\")\n                print('=' * 50)\n\n    print()\n    memory_after_compress = memory_usage(data, 0)\n    print(\"Compress Rate: [{0:.2%}]\".format((memory_before_compress-memory_after_compress) \/ memory_before_compress))\n    \n    return data","9f3fa100":"df_train = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/train.csv')\ndf_train.head()","af149318":"df_train = df_train.drop('id', axis = 1)","1c28bc3c":"print(f'Train set shape:   {df_train.shape}')","0d5ead08":"df_train.info()","2982f7af":"df_train.describe()","cb71763c":"df_train.isnull().sum().max() == 0","59166bf7":"df_test = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/test.csv')\ndf_test.head()","00c6fb85":"df_test = df_test.drop('id', axis = 1)","108a386c":"print(f'Test set shape:   {df_test.shape}')","2c687520":"df_test.info()","0162c25d":"df_test.describe()","71b47780":"df_test.isnull().sum().max() == 0","a36348f1":"plt.figure(figsize = (5,5))\nplt.pie(x = df_train['target'].value_counts(), labels = ['1', '0'], autopct = '%1.2f%%', \n        explode = [0.05, 0], startangle = 90)","523eb05b":"df_train_sample = df_train.sample(n = 20000)\ndf_test_sample = df_test.sample(n = 20000)","88a8c114":"fig, axes = plt.subplots(10,10, figsize = (30, 30))\naxes = axes.flatten()\n\nfor idx, ax in enumerate(axes):\n    \n    sns.kdeplot(data = df_train_sample, ax = ax, fill = True, x = f'f{idx}', \n                palette = ['#4DB6AC', 'red'])\n    sns.kdeplot(data = df_test_sample, ax = ax, fill = True, x = f'f{idx}', \n                palette = ['#4DB6AC', 'blue'])\n \n    ax.set_xticks([]); ax.set_yticks([]); ax.set_xlabel('')\n    ax.set_ylabel(''); ax.spines['left'].set_visible(False)\n    ax.set_title(f'f{idx}', loc = 'right', weight = 'bold', fontsize = 10)\n\nfig.supxlabel('Probability Density Function Estimation', ha = 'center', fontweight = 'bold')\nfig.tight_layout()\nplt.show()","779f0ff0":"peaks = ['f0','f2','f4','f9','f12','f16','f19','f20','f23','f24','f27',\n    'f28','f30','f31','f32','f33','f35','f39','f42','f44','f46','f48',\n    'f49','f51','f52','f53','f56','f58','f59','f60','f61','f62','f63',\n    'f64','f68','f69','f72','f73','f75','f76','f78','f79','f81','f83',\n    'f84','f87','f88','f89','f90','f92','f93','f94','f95','f98','f99']\n\nno_peaks = [feats for feats in df_test.columns if feats not in peaks]\n\ndf_train['median_peaks'] = df_train[peaks].median(axis = 1)\ndf_train['median_no_peaks'] = df_train[no_peaks].median(axis = 1)\ndf_test['median_peaks'] = df_test[peaks].median(axis = 1)\ndf_test['median_no_peaks'] = df_test[no_peaks].median(axis = 1)\n\ndf_train['mean_peaks'] = df_train[peaks].mean(axis = 1)\ndf_train['mean_no_peaks'] = df_train[no_peaks].mean(axis = 1)\ndf_test['mean_peaks'] = df_test[peaks].mean(axis = 1)\ndf_test['mean_no_peaks'] = df_test[no_peaks].mean(axis = 1)\n\ndf_train['std_peaks'] = df_train[peaks].std(axis = 1)\ndf_train['std_no_peaks'] = df_train[no_peaks].std(axis = 1)\ndf_test['std_peaks'] = df_test[peaks].std(axis = 1)\ndf_test['std_no_peaks'] = df_test[no_peaks].std(axis = 1)\n\ndf_train['sum_peaks'] = df_train[peaks].sum(axis = 1)\ndf_train['sum_no_peaks'] = df_train[no_peaks].sum(axis = 1)\ndf_test['sum_peaks'] = df_test[peaks].sum(axis = 1)\ndf_test['sum_no_peaks'] = df_test[no_peaks].sum(axis = 1)\n\ndf_train['min_peaks'] = df_train[peaks].min(axis = 1)\ndf_train['min_no_peaks'] = df_train[no_peaks].min(axis = 1)\ndf_test['min_peaks'] = df_test[peaks].min(axis = 1)\ndf_test['min_no_peaks'] = df_test[no_peaks].min(axis = 1)\n\ndf_train['max_peaks'] = df_train[peaks].max(axis = 1)\ndf_train['max_no_peaks'] = df_train[no_peaks].max(axis = 1)\ndf_test['max_peaks'] = df_test[peaks].max(axis = 1)\ndf_test['max_no_peaks'] = df_test[no_peaks].max(axis = 1)\n\ndf_train['skew_peaks'] = df_train[peaks].skew(axis = 1)\ndf_train['skew_no_peaks'] = df_train[no_peaks].skew(axis = 1)\ndf_test['skew_peaks'] = df_test[peaks].skew(axis = 1)\ndf_test['skew_no_peaks'] = df_test[no_peaks].skew(axis = 1)","4309b147":"df_train.head()","19c6d8da":"df_test.head()","31e5fefa":"scaler = StandardScaler()\n\nfloat_columns = [feats for feats in df_train.select_dtypes('float')]\n\ndf_train[float_columns] = scaler.fit_transform(df_train[float_columns])\ndf_test = pd.DataFrame(scaler.transform(df_test), columns = df_test.columns)","0678d010":"df_train.head()","f839ffd3":"df_test.head()","59543042":"df_train = compress_dataset(df_train)","06d185da":"df_test = compress_dataset(df_test)","a85ccb95":"X = df_train.select_dtypes('float16')\ny = df_train['target']","0931173d":"def objective(trial):\n    params = {'subsample': trial.suggest_discrete_uniform('subsample', 0.6, 0.9, 0.1),\n              'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree', 0.6, 0.9, 0.1),\n              'max_depth': trial.suggest_int(\"max_depth\", 5, 9, 2),\n              'reg_alpha': trial.suggest_int(\"reg_alpha\", 0, 60, 20),\n              'reg_lambda': trial.suggest_int('reg_lambda', 0, 60, 20),\n              'learning_rate': trial.suggest_loguniform('learning_rate', 0.01, 0.1)}\n        \n    model = lgb.LGBMClassifier(**params, random_state = 51, n_estimators = 1000, device = 'gpu', n_jobs = -1)\n    \n    scores = []\n    cv = KFold(n_splits = 5, shuffle = True)\n    \n    for train_idx, test_idx in cv.split(X):\n        X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n        X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n        \n        model.fit(X_train, y_train, early_stopping_rounds = 200, eval_metric = 'auc', \n                  eval_set = [(X_train, y_train), (X_test, y_test)], verbose = 200)\n        preds = model.predict_proba(X_test)[:, 1]\n        \n        score = roc_auc_score(y_test, preds)\n        scores.append(score \/ cv.n_splits)\n    \n    return sum(scores)","3e3a25c2":"study = optuna.create_study(direction = 'maximize', sampler = TPESampler())\nstudy.optimize(objective, n_trials = 30)","a58b7cfa":"params = study.best_params\nparams","0c8376de":"model = lgb.LGBMClassifier(**params, n_estimators = 2000, device = 'gpu', n_jobs = -1)\n\nscores = []\npreds_tests = []\ncv = KFold(n_splits = 10, shuffle = True)\n\nfor train_idx, test_idx in cv.split(X):\n    X_train, y_train = X.iloc[train_idx], y.iloc[train_idx]\n    X_test, y_test = X.iloc[test_idx], y.iloc[test_idx]\n\n    model.fit(X_train, y_train, early_stopping_rounds = 200, eval_metric = 'auc', \n              eval_set = [(X_train, y_train), (X_test, y_test)], verbose = 200)\n    \n    preds = model.predict_proba(X_test)[:, 1]\n    preds_test = model.predict_proba(df_test)[:, 1]\n    \n    score = roc_auc_score(y_test, preds)\n    scores.append(score)\n    preds_tests.append(preds_test)\n\nprint('************************************')    \nprint(f\"Mean AUROC score:       {np.mean(scores)}\")\nprint(f\"Std AUROC:              {np.std(scores)}\")","8cbb7105":"plot_roc_curve(model, X, y)\nplt.grid()","6226cfa4":"lgb.plot_importance(model, figsize = (10, 30))","61d95b5a":"sub = pd.read_csv('..\/input\/tabular-playground-series-nov-2021\/sample_submission.csv')\nsub['target'] = np.mean(preds_tests, axis = 0)\nsub.head()","828ec352":"sub.to_csv('lgbm_0.74631.csv', index = False)","e199702d":"## Standard Scaler","9fcb354e":"## Release memory","40a3b807":"## Test set summary","2f6c72ac":"**Summary:**\n1. Train set contains **600 000** rows and **102** columns (including `id`)\n2. Test set contains **540 000** rows and **101** columns (including `id`)\n3. All columns are 'float' type (except `target`)\n4. There are **no missing values** in train set and test set.\n5. Classes in target column are **balansed**.","fc66bfc1":"## Feature engineering","202d9158":"## Train set summary","52abcefc":"## Target summary","a3d4b892":"## Submission","66dd3425":"## Optuna"}}