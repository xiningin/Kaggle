{"cell_type":{"c7e9c083":"code","842b3fcf":"code","c1747aaa":"code","70f88baf":"code","cfc155ea":"code","ad2b8509":"code","c5301884":"code","8b86310b":"code","2f12405d":"code","7310da4d":"code","319566b6":"code","872fa8f6":"code","43d7585e":"code","a01d8560":"code","4f42c937":"code","9d1dbca8":"code","26eaa89f":"code","cce48f65":"code","0512b54f":"code","8e2fe084":"code","4c73a413":"code","4b30cf8a":"code","93006ccc":"code","63ba897e":"code","6c16a842":"code","9bf2f126":"code","b415b9d7":"code","bb1f48df":"code","da8d6c6f":"code","65ced907":"code","a0cff66f":"code","01d29622":"code","e7235bca":"code","9e3556ae":"code","6a805c78":"code","3edef942":"code","2eab3d9e":"code","2982a1b8":"code","a792d7b9":"code","a71d8f05":"code","9bdabf12":"code","cb7268da":"code","af2ccaa2":"code","f11be966":"code","02cb8a54":"code","11399222":"code","4d01af81":"code","ea093633":"code","b15d2a1f":"code","47b8d182":"code","ffb97de1":"code","5e01d34e":"code","d3eac5ad":"code","fc7a85ce":"code","11eacf8b":"code","fb6ecdc3":"code","63123fd0":"code","2185ebf7":"code","19770d87":"code","7714be3c":"code","9516ee26":"code","482aad63":"code","2a9608e7":"code","e61378e8":"code","31e5485a":"code","7404cb28":"code","87849342":"code","6ae541fc":"code","3609792b":"code","beffdb29":"code","bb89b251":"code","083dc528":"code","6e094fb1":"code","c93a1aff":"code","e841e76e":"code","961cf684":"code","f7fb44b8":"code","d2f3523c":"code","7be2db7a":"code","4f7010c0":"code","6f886122":"code","300ad048":"code","3f15c837":"code","04e21675":"code","9dc0e0be":"code","a8b5cae8":"code","094162af":"code","340c4511":"code","1e77c8bf":"code","11401b36":"code","bdffcbdd":"code","8f4fab4c":"code","dd4c8e7a":"code","0d7e7985":"code","cd09063f":"code","157f6919":"code","112c9ddf":"code","0e977a99":"code","a02d52c0":"code","3fac979e":"code","5bbce9d2":"code","11b5b941":"code","04207416":"code","593d0a19":"code","904692b6":"code","f00ef630":"code","815258e5":"code","813dad7a":"code","8dbc8bfb":"code","53251325":"code","95350cf0":"code","52ed3052":"code","a6197488":"code","aa4f5b7f":"code","1b0731ac":"code","f5842ae9":"code","31dbda87":"code","45def52b":"code","b7a2a7dd":"code","4019efaf":"markdown","d91cc2ba":"markdown","55afe9e4":"markdown","b1603aa6":"markdown","91537c26":"markdown","ebc9dcaf":"markdown","7c11559d":"markdown","28377d9c":"markdown","1914e403":"markdown","91d8099a":"markdown","2ae216da":"markdown","57a9bec5":"markdown","1a925076":"markdown","746f7a4f":"markdown","3d821f9e":"markdown","9513c3a1":"markdown","9fb964d2":"markdown","387dca34":"markdown","ffd7399c":"markdown","81ecd341":"markdown","a1000dca":"markdown","45765e1a":"markdown","9496824b":"markdown","5e700d46":"markdown","c2efb5cf":"markdown","12b85748":"markdown","57b8588b":"markdown","a24725b9":"markdown","54fce56b":"markdown","cb1976db":"markdown","10c030df":"markdown","221372b3":"markdown","076de594":"markdown","744ef77c":"markdown","36cbcd4b":"markdown","6bdfb24f":"markdown","e5818ec2":"markdown","840bbfc2":"markdown","f8085b75":"markdown","eb8a6899":"markdown","0728bb2e":"markdown","e470bc68":"markdown","0b34a27a":"markdown","6b21041b":"markdown","1b336729":"markdown","21492677":"markdown","26521fe8":"markdown","4b8ff812":"markdown","c4ca1bcf":"markdown","eb54bd81":"markdown","460ccc53":"markdown","c4990ff4":"markdown","76305619":"markdown","4f3280a1":"markdown","3a8544dd":"markdown","e472c556":"markdown","3184d388":"markdown","33859b53":"markdown","d84b409d":"markdown","a079c103":"markdown","a9d7cb1e":"markdown","a3db491a":"markdown","83688ceb":"markdown","50d3ca46":"markdown","b233d163":"markdown","1f081a3d":"markdown","8ae5dad1":"markdown","ab9cf223":"markdown","55d3342d":"markdown","ff34e890":"markdown","527182d5":"markdown","3402c450":"markdown","6ad79f3a":"markdown","8055998e":"markdown","aa865c68":"markdown","229ec5ae":"markdown","16f41b4f":"markdown","a7370841":"markdown","acb55b15":"markdown","db0a6924":"markdown","71a98c41":"markdown","38cb5378":"markdown","c5b30e37":"markdown","7a2c166a":"markdown","633fb0fb":"markdown"},"source":{"c7e9c083":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","842b3fcf":"import numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nfrom scipy.stats import variation\n\n# pd.set_option(\"display.max_columns\", None)\n# pd.set_option(\"display.max_rows\", None)","c1747aaa":"# I used dataworld platform. It's up to you. \ndf_m = pd.read_csv('https:\/\/query.data.world\/s\/h3pbhckz5ck4rc7qmt2wlknlnn7esr',encoding='latin-1')\ndf_f= pd.read_csv('https:\/\/query.data.world\/s\/sq27zz4hawg32yfxksqwijxmpwmynq')","70f88baf":"# Ladies first\ndf_f.head().T","cfc155ea":"df_m.head().T","ad2b8509":"df_f.shape,df_m.shape","c5301884":"df_f.info(),df_m.info()","8b86310b":"df = pd.concat([df_m,df_f])","2f12405d":"df.shape","7310da4d":"df_f = df_f.rename(columns = {\"SubjectId\":\"subjectid\"})","319566b6":"df = pd.concat([df_m,df_f])\ndf.shape","872fa8f6":"df.head().T","43d7585e":"df.info()","a01d8560":"df.isnull().sum().any()","4f42c937":"# Another term of \"df.isnull().sum().any()\"\ndf.isnull().any().any()","9d1dbca8":"df.duplicated().sum()","26eaa89f":"df.isnull().sum()","cce48f65":"NaN_list =[]\nfor columns in df.columns:\n    if df[columns].isnull().sum()>0:\n        print(\"{name} = {qty}\".format(name = columns, qty = df[columns].isnull().sum()))\n        NaN_list.append(columns)","0512b54f":"NaN_list","8e2fe084":"df = df.drop(NaN_list, axis=1)","4c73a413":"df.isnull().sum().any()","4b30cf8a":"df[[\"DODRace\",\"SubjectNumericRace\"]]","93006ccc":"df.drop(\"SubjectNumericRace\", axis = 1, inplace = True)","63ba897e":"df.DODRace.value_counts(dropna = False)","6c16a842":"# Alternative code that does the same:\n# df = df[(df[\"DODRace\"] == \"White\") | (df[\"DODRace\"] == \"Black\") | (df[\"DODRace\"] == \"Hispanic\")]\ndf = df[df[\"DODRace\"].isin([1,2,3])]\ndf.DODRace.value_counts(dropna = False)","9bf2f126":"df.shape","b415b9d7":"print(df[\"DODRace\"].value_counts());\ndf[\"DODRace\"].value_counts().plot(kind=\"pie\", autopct='%1.1f%%',figsize=(10,10));","bb1f48df":"df[['Weightlbs','weightkg']]","da8d6c6f":"# df.drop([\"weightkg\"], axis = 1, inplace=True)","65ced907":"for columns in df.select_dtypes(include=[np.number]).columns:\n    if df[columns].min() == 0:\n        print(columns)","a0cff66f":"df[\"Weightlbs\"].argmin()","01d29622":"df.iloc[824][[\"Weightlbs\",\"Heightin\"]]","e7235bca":"df.drop(index = df[\"Weightlbs\"].argmin(), inplace=True)","9e3556ae":"# Don't forget to drop indexes. Unnecessary column\ndf.reset_index(drop=True, inplace=True)","6a805c78":"df.select_dtypes(exclude=[np.number]).head().T","3edef942":"# to find how many unique values object features have\nfor columns in df.select_dtypes(exclude=[np.number]).columns:\n    print(f\"{columns} has {df[columns].nunique()} unique value\")","2eab3d9e":"df.groupby([\"Component\"])[\"DODRace\"].value_counts()","2982a1b8":"df.groupby([\"Component\",\"Branch\"])[\"DODRace\"].value_counts()","a792d7b9":"drop_list_nonnumeric = [\"Date\", \"Installation\", \"Component\",\"PrimaryMOS\"]\ndf.drop(drop_list_nonnumeric, axis=1, inplace=True)","a71d8f05":"df.shape","9bdabf12":"df.head().T","cb7268da":"df.drop(\"subjectid\", axis = 1, inplace = True)","af2ccaa2":"df_temp = df.corr()\n\ncount = \"done\"\nfeature =[]\ncollinear=[]\nfor col in df_temp.columns:\n    for i in df_temp.index:\n        if (df_temp[col][i]> .9 and df_temp[col][i] < 1) or (df_temp[col][i]< -.9 and df_temp[col][i] > -1) :\n                feature.append(col)\n                collinear.append(i)\n                # print(f\"multicolinearity alert in between {col} - {i}\")\nprint(\"Number of strong corelated features:\", count)","f11be966":"df_col = pd.DataFrame([feature, collinear], index=[\"feature\",\"collinear\"]).T\ndf_col","02cb8a54":"df_col.value_counts(\"feature\")","11399222":"df[\"DODRace\"] = df.DODRace.map({1 : \"White\", 2 : \"Black\", 3 : \"Hispanic\"})\ndf.DODRace.value_counts()","4d01af81":"from sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler","ea093633":"X= df.drop(\"DODRace\",axis=1)\nX = pd.get_dummies(data=X,drop_first=True)\ny= df.DODRace","b15d2a1f":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=6, stratify =y)","47b8d182":"scaler =MinMaxScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","ffb97de1":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import classification_report,confusion_matrix,plot_confusion_matrix","5e01d34e":"log_model = LogisticRegression(class_weight='balanced',max_iter=10000,random_state=6)\n\nlog_model.fit(X_train,y_train)\n\ny_pred = log_model.predict(X_test)\n\nprint(\"test scores\",\"\\n--------------\")\n\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test, y_pred))\nplot_confusion_matrix(log_model,X_test,y_test,values_format='.0f')","d3eac5ad":"y_pred = log_model.predict(X_train)\nprint(\"train scores\",\"\\n--------------\")\nprint(confusion_matrix(y_train,y_pred))\nprint(classification_report(y_train, y_pred))\nplot_confusion_matrix(log_model,X_train,y_train);","fc7a85ce":"from sklearn.model_selection import cross_val_score, cross_validate","11eacf8b":"f1score = make_scorer(f1_score, average=\"weighted\")\nmodel = LogisticRegression(class_weight='balanced',max_iter=10000,random_state=6)\nscores = cross_val_score(model, X_train, y_train, cv = 5, scoring = f1score, n_jobs = -1)\nprint([round(i, 4) for i in scores], \"\\n\")\nprint(f\" f1score : %{scores.mean()*100:.2f}, std : %{scores.std()*100:.3f} \\n\")","fb6ecdc3":"from sklearn.model_selection import GridSearchCV","63123fd0":"f1_Hispanic =  make_scorer(f1_score, average=None, labels=[\"Hispanic\"] )","2185ebf7":"param_grid = { \"class_weight\" : [\"balanced\", None],\n               'penalty': [\"l1\",\"l2\"],\n               'solver' : ['saga','lbfgs'],\n             }","19770d87":"model = LogisticRegression(class_weight='balanced',max_iter=10000,random_state=6)\nlog_model_grid = GridSearchCV(model, param_grid, verbose=3, scoring=f1_Hispanic, refit=True,n_jobs=-1)","7714be3c":"log_model_grid.fit(X_train,y_train)","9516ee26":"log_model_grid.best_params_","482aad63":"y_pred = log_model_grid.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\nplot_confusion_matrix(log_model_grid,X_test,y_test);","2a9608e7":"from sklearn.metrics import roc_curve, auc","e61378e8":"def plot_multiclass_roc(clf, X_test, y_test, n_classes, figsize=(5,5)):\n    y_score = clf.decision_function(X_test)\n\n    # structures\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n\n    # calculate dummies once\n    y_test_dummies = pd.get_dummies(y_test, drop_first=False).values\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_test_dummies[:, i], y_score[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    # roc for each class\n    fig, ax = plt.subplots(figsize=figsize)\n    ax.plot([0, 1], [0, 1], 'k--')\n    ax.set_xlim([0.0, 1.0])\n    ax.set_ylim([0.0, 1.05])\n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    ax.set_title('Receiver operating characteristic example')\n    for i in range(n_classes):\n        ax.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f) for label %i' % (roc_auc[i], i))\n    ax.legend(loc=\"best\")\n    ax.grid(alpha=.4)\n    sns.despine()\n    plt.show()","31e5485a":"plot_multiclass_roc(log_model_grid, X_test, y_test, n_classes=3, figsize=(16, 10));","7404cb28":"from sklearn.svm import SVC","87849342":"svm_model = SVC(class_weight=\"balanced\")\n\nsvm_model.fit(X_train, y_train)\n\ny_pred = svm_model.predict(X_test)\n\nprint(\"test scores\",\"\\n--------------\")\n\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\nplot_confusion_matrix(svm_model,X_train,y_train);","6ae541fc":"y_pred = svm_model.predict(X_train)\n\nprint(\"train scores\",\"\\n--------------\")\n\nprint(confusion_matrix(y_train,y_pred))\nprint(classification_report(y_train, y_pred))\nplot_confusion_matrix(svm_model,X_train,y_train);","3609792b":"param_grid = {'C': np.linspace(200,400,5),\n              'decision_function_shape' : ['ovr','ovo'],\n              'gamma': [\"scale\", \"auto\", 1,0.1,0.01],\n              'kernel': ['rbf'],\n              'class_weight':[\"balanced\",None]\n             }","beffdb29":"model = SVC(class_weight=\"balanced\")\nsvm_model_grid = GridSearchCV(model, param_grid, verbose=3, scoring=f1_Hispanic, refit=True,n_jobs=-1)","bb89b251":"svm_model_grid.fit(X_train, y_train)","083dc528":"svm_model_grid.best_params_","6e094fb1":"y_pred = svm_model_grid.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\nplot_confusion_matrix(svm_model_grid,X_test,y_test);","c93a1aff":"plot_multiclass_roc(svm_model_grid, X_test, y_test, n_classes=3, figsize=(16, 10));","e841e76e":"from sklearn.ensemble import RandomForestClassifier","961cf684":"rf_mod = RandomForestClassifier()","f7fb44b8":"rf_mod.fit(X_train,y_train)","d2f3523c":"y_pred = rf_mod.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\nplot_confusion_matrix(rf_mod,X_test,y_test);","7be2db7a":"y_pred = rf_mod.predict(X_train)\nprint(confusion_matrix(y_train, y_pred))\nprint(classification_report(y_train, y_pred))\nplot_confusion_matrix(rf_mod,X_train,y_train);","4f7010c0":"param_grid = {'n_estimators':[400,500],\n             'criterion': [\"gini\",\"entropy\"],\n             'max_depth':[10,12,14,16],\n             'min_samples_split':[18,20,22],\n             'class_weight': ['balanced',None]}","6f886122":"rf_model = RandomForestClassifier()\nrf_grid_model = GridSearchCV(rf_model, param_grid, verbose=3, scoring=f1_Hispanic, refit=True,n_jobs=-1)","300ad048":"rf_grid_model.fit(X_train,y_train)","3f15c837":"rf_grid_model.best_params_","04e21675":"y_pred = rf_grid_model.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\nplot_confusion_matrix(rf_grid_model,X_test,y_test);","9dc0e0be":"def plot_multiclass_roc_for_tree(clf, X_test, y_test, n_classes, figsize=(5,5)):\n    y_score = clf.predict_proba(X_test)\n\n    # structures\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n\n    # calculate dummies once\n    y_test_dummies = pd.get_dummies(y_test, drop_first=False).values\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_test_dummies[:, i], y_score[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    # roc for each class\n    fig, ax = plt.subplots(figsize=figsize)\n    ax.plot([0, 1], [0, 1], 'k--')\n    ax.set_xlim([0.0, 1.0])\n    ax.set_ylim([0.0, 1.05])\n    ax.set_xlabel('False Positive Rate')\n    ax.set_ylabel('True Positive Rate')\n    ax.set_title('Receiver operating characteristic example')\n    for i in range(n_classes):\n        ax.plot(fpr[i], tpr[i], label='ROC curve (area = %0.2f) for label %i' % (roc_auc[i], i))\n    ax.legend(loc=\"best\")\n    ax.grid(alpha=.4)\n    sns.despine()\n    plt.show()","a8b5cae8":"plot_multiclass_roc_for_tree(rf_grid_model, X_test, y_test, n_classes=3, figsize=(16, 10));","094162af":"from xgboost import XGBClassifier","340c4511":"xgb_model = XGBClassifier()","1e77c8bf":"xgb_model.fit(X_train,y_train)","11401b36":"y_pred = xgb_model.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\nplot_confusion_matrix(xgb_model,X_test,y_test);","bdffcbdd":"y_pred = xgb_model.predict(X_train)\nprint(confusion_matrix(y_train, y_pred))\nprint(classification_report(y_train, y_pred))\nplot_confusion_matrix(xgb_model,X_train,y_train);","8f4fab4c":"from sklearn.utils import class_weight\nclasses_weights = class_weight.compute_sample_weight(class_weight='balanced', y=y_train)\nclasses_weights","dd4c8e7a":"comp = pd.DataFrame(classes_weights)\n\ncomp[\"label\"]= y_train.reset_index(drop=True)\ncomp.groupby(\"label\")[0].value_counts()","0d7e7985":"param_grid = {\"n_estimators\":[100, 300],\n              'max_depth':[3,5,6],\n              \"learning_rate\": [0.1, 0.3],\n              \"subsample\":[0.5, 1],\n              \"colsample_bytree\":[0.5, 1]}","cd09063f":"xgb_model = XGBClassifier()\nxgb_grid_model = GridSearchCV(xgb_model, param_grid, scoring=f1_Hispanic, n_jobs = -1,refit=True, verbose = 2).fit(X_train, y_train)","157f6919":"xgb_grid_model.best_params_","112c9ddf":"y_pred = xgb_grid_model.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\nplot_confusion_matrix(xgb_grid_model,X_test,y_test);","0e977a99":"plot_multiclass_roc_for_tree(xgb_grid_model, X_test, y_test, n_classes=3, figsize=(16, 10));","a02d52c0":"X.describe().loc[\"mean\"]","3fac979e":"a = pd.DataFrame(X.iloc[205]).T\na","5bbce9d2":"a = pd.get_dummies(a)","11b5b941":"a = scaler.transform(a)\nX_scaled = scaler.fit_transform(X)","04207416":"final_model = SVC(C=400,class_weight=None,decision_function_shape=\"ovr\", gamma=\"auto\",kernel=\"rbf\" ).fit(X_scaled,y)","593d0a19":"y[205]","904692b6":"from imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.pipeline import Pipeline","f00ef630":"over = SMOTE(sampling_strategy={\"Hispanic\": 1250})\nunder = RandomUnderSampler(sampling_strategy={\"White\":2800})\nsteps = [('o', over), ('u', under)]\npipeline = Pipeline(steps=steps)\nX_resampled, y_resampled = pipeline.fit_resample(X_train, y_train)","815258e5":"#from imblearn.combine import SMOTEENN,SMOTETomek\n#ros1 = SMOTETomek()\n#ros2 = SMOTEENN()\n#X_resampled, y_resampled = ros1.fit_resample(X_train, y_train)","813dad7a":"# X_resampled, y_resampled = ros2.fit_resample(X_resampled, y_resampled)","8dbc8bfb":"y_resampled.value_counts()","53251325":"y_train.value_counts()","95350cf0":"param_grid = {'C': np.linspace(50,200,4),\n              'decision_function_shape' : ['ovr'], #'ovo'\n              'gamma': [\"scale\", \"auto\", 1,2],\n              'kernel': ['rbf'],\n              'class_weight':[\"balanced\",None]}\n\nf1_Hispanic =  make_scorer(f1_score, average=None, labels = [\"Hispanic\"])\nf1_score_weighed = make_scorer(f1_score, average=\"weighted\")\n\nmodel = SVC()\nsvm_model_grid = GridSearchCV(model, param_grid, verbose=1, scoring=f1_score_weighed, refit=True,n_jobs=-1)\n\nsvm_model_grid.fit(X_resampled, y_resampled)","52ed3052":"svm_model_grid.best_params_","a6197488":"y_pred = svm_model_grid.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\nplot_confusion_matrix(svm_model_grid,X_test,y_test);","aa4f5b7f":"plot_multiclass_roc(svm_model_grid, X_test, y_test, n_classes=3, figsize=(16, 10));","1b0731ac":"param_grid = {\"n_estimators\":[100, 300],\n              'max_depth':[6,8],\n              \"learning_rate\": [0.1, 0.3],\n              \"subsample\":[0.5],\n              \"colsample_bytree\":[0.5, 1]}\n\nxgb_model = XGBClassifier()\nxgb_grid_model = GridSearchCV(xgb_model, param_grid, scoring=f1_Hispanic, n_jobs = -1,refit=True, verbose = 2).fit(X_resampled, y_resampled)","f5842ae9":"xgb_grid_model.best_params_","31dbda87":"y_pred = xgb_grid_model.predict(X_test)\nprint(confusion_matrix(y_test, y_pred))\nprint(classification_report(y_test, y_pred))\nplot_confusion_matrix(xgb_grid_model,X_test,y_test);","45def52b":"from sklearn.metrics import matthews_corrcoef\nmatthews_corrcoef(y_test, y_pred)","b7a2a7dd":"from sklearn.metrics import cohen_kappa_score\ncohen_kappa_score(y_test, y_pred)","4019efaf":"## cohen_kappa_score","d91cc2ba":"<span style=\"color:DeepPink\">There are lots of columns. Lets look at the shape and info of them in one row.<\/span>","55afe9e4":"### Let's Start to Beginner Friendly EDA","b1603aa6":"<span style=\"color:DeepPink\">*If you wish, you can also perform the above operations with your own personal codes.*<\/span><br>\nLet's prepare our data and start getting results from our models without drowning in correlational relations.","91537c26":"<span style=\"color:DeepPink\">You can make these codes generic by taking them to your own personal codebooks. There is no copyright :D <\/span>","ebc9dcaf":"<span style=\"color:DeepPink\">There was an instruction above:<\/span>\n- Drop DODRace class if value count below 500 (we assume that our data model can't learn if it is below 500)","7c11559d":"- For now, we will only import a few libraries to do the EDA process. \n- If you want to understand what these libraries are, I strongly suggest you consult **Uncle GOOGLE**.","28377d9c":"<center><img src=\"https:\/\/i.scdn.co\/image\/ab67616d00001e029b6cd365d38496b580026989\" alt=\"drawing\" width=\"200\"\/><\/center>","1914e403":"# 2. SVM --> SVC Model","91d8099a":"___","2ae216da":"I've decided to drop subjectnumericrace column.","57a9bec5":"# Lets Make a Prediction with the Final Model","1a925076":"**Before EDA, our community gave us some tips, please respect them for better results.**\n- Drop unnecessary colums\n- Drop DODRace class if value count below 500 (we assume that our data model can't learn if it is below 500)\n- Find unusual value in Weightlbs\n\n<span style=\"color:DeepPink\">Of course, we will do the additional manipulations we want without being limited to just these instructions.<\/span>","746f7a4f":"___","3d821f9e":"From the output of the above code, you already saw that there are Nan values in the \"Ethnicity\" column.<br>\n<span style=\"color:DeepPink\">But now let's handle NaN values by writing a generic code that drops NaN values.<\/span>","9513c3a1":"# Modelling","9fb964d2":"___","387dca34":"#### Weightlbs and Heightin columns:","ffd7399c":"# Using <span style=\"color:DeepPink\">SMOTE<\/span>","81ecd341":"## SVC Over\/ Under Sampling","a1000dca":"Our def function to be used in drawing the roc curve for the multiclass target label is as follows:","45765e1a":"# 1. Logistic model","9496824b":"# XGBoost Model","5e700d46":"___","c2efb5cf":"**first**<span style=\"color:DeepPink\"> **.fit(train)**<\/span> --> **second**<span style=\"color:DeepPink\"> **.predict(test)**<\/span> --> **third**<span style=\"color:DeepPink\"> **.predict(train)**<\/span> --> **fourth**<span style=\"color:DeepPink\"> **cross_val**<\/span> --> **fifth**<span style=\"color:DeepPink\"> **GridSearchCV**<\/span> --> **sixth**<span style=\"color:DeepPink\"> **ROC - AUC**<\/span>","12b85748":"___","57b8588b":"**What is SMOTE?**<br>\n\n&nbsp;&nbsp;&nbsp;SMOTE stands for Synthetic Minority Oversampling Technique. This is a statistical technique for increasing the number of cases in your dataset in a balanced way. The module works by generating new instances from existing minority cases that you supply as input. This implementation of SMOTE does not change the number of majority cases.<br>\n\n&nbsp;&nbsp;&nbsp;The new instances are not just copies of existing minority cases; instead, the algorithm takes samples of the feature space for each target class and its nearest neighbors, and generates new examples that combine features of the target case with features of its neighbors. This approach increases the features available to each class and makes the samples more general.<br>\n\n&nbsp;&nbsp;&nbsp;SMOTE takes the entire dataset as an input, but it increases the percentage of only the minority cases. For example, suppose you have an imbalanced dataset where just 1% of the cases have the target value A (the minority class), and 99% of the cases have the value B. To increase the percentage of minority cases to twice the previous percentage, you would enter 200 for SMOTE percentage in the module's properties.<br>\n\n*Source:* <a href=\"https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/smote\" style=\"color:DeepPink;\">SMOTE<\/a>","a24725b9":"___","54fce56b":"# Quick Review Of Dataset","cb1976db":"___","10c030df":"**What is matthews_corrcoef?**<br>\n\n<span style=\"color:DeepPink\">The Matthews correlation coefficient is used in machine learning as a measure of the quality of binary and multiclass classifications. It takes into account true and false positives and negatives and is generally regarded as a balanced measure which can be used even if the classes are of very different sizes.<\/span>","221372b3":"___","076de594":"___","744ef77c":"#### Last Adjustment of EDA's","36cbcd4b":"<span style=\"color:DeepPink\">**Important warning!**<\/span><br>\nYou have to be very careful while dropping data. Because each value is like a golden for our ML algorithm.<br>\nThat's why I just did the following and proceeded without further tinkering with my Weightlbs and Heightin columns. Please indicate your different experiences in the comments.","6bdfb24f":"If you want, you can set a limit on the Weightlbs and Heightin columns and then drop the values outside the limit. When I do the necessary research (The minimum and maximum values of the height and weight of a healthy individual who can become a soldier), I guess the codes like these might be good:<br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style=\"color:DeepPink\">df = df[(df['Weightlbs'] >= 100) & (df['Weightlbs'] <= 270)]<\/span><br>\n&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;<span style=\"color:DeepPink\">df = df[(df['Heightin'] >= 59) & (df['Heightin'] <= 80)]<\/span>","e5818ec2":"Our def function to be used in drawing the roc curve for the multiclass target label is as follows (***tree based model***:","840bbfc2":"<h1><center><span style=\"color:LightCoral; opacity: 1;\">THANKS FOR YOUR TIME!<\/span><\/center><\/h1>","f8085b75":"<span style=\"color:DeepPink\">Now let's run our **GridsearchCV** code, which gives f1 scores in a single target label \"Hispanic\".<\/span><br>\nBut first, I have to explain what <span style=\"color:DeepPink\">**GridSearchCV**<\/span> does; \n- It gives results both with hyperparams in grid_param and by cross validation.","eb8a6899":"# <center>ADDITIONAL<\/center>","0728bb2e":"___","e470bc68":"___","0b34a27a":"___","6b21041b":"**What is cohen_kappa_score?**<br>\n\n<span style=\"color:DeepPink\">This function computes Cohen's kappa [1], a score that expresses the level of agreement between two annotators on a classification problem. It is defined as. \u03ba = ( p o \u2212 p e ) \/ ( 1 \u2212 p e ).<\/span>","1b336729":"___","21492677":"# RF (Random Forest)","26521fe8":"Let's create a DataFrame that consists of corr results","4b8ff812":"The last thing to do: We must change the target column of our model , as categorically. Because in ML models, of the target column consists of numbers, the models may make mistakes in the weightings.","c4ca1bcf":"<span style=\"color:DeepPink\">Now let's look at the correlational relations and then find our features that can be multicollinearity and do the necessary operations.<\/span><br>\nLet's assign our dataset to a temporary dataset (in order not to corrupt the origin dataset) and create a generic code that shows our correlation ranges. <span style=\"color:DeepPink\">Do not forget;<\/span> Correlation values can also be negative. So let's set up our code like this:","eb54bd81":"- <span style=\"color:DeepPink\">**SubjectNumericRace**<\/span>: a single or multi-digit code indicating a subject\u2019s self-reported race or races (verified through interview). Where 1 = White, 2 = Black, 3 = Hispanic, 4 = Asian, 5 = Native American, 6 = Pacific Islander, 8 = Other\n- <span style=\"color:DeepPink\">**DODRace**<\/span>: Department of Defense Race; a single digit indicating a subject\u2019s self-reported preferred single race where selecting multiple races is not an option. This variable is intended to be comparable to the Defense Manpower Data Center demographic data. Where 1 = White, 2 = Black, 3 = Hispanic, 4 = Asian, 5 = Native American, 6 = Pacific Islander, 8 = Other","460ccc53":"## matthews_corrcoef","c4990ff4":"___","76305619":"<span style=\"color:DeepPink\">Now let's look at the columns that need to be considered, such as Wightlbs, and do the necessary operations.<\/span>","4f3280a1":"<span style=\"color:DeepPink\">Let's create our non-numeric list, which will cause our data to swell when we apply the get dummies operation and will not yield any meaningful results, and perform our drop operation.<\/span>","3a8544dd":"My function below dropped all my features that could be correlating outside the range of <span style=\"color:DeepPink\">0.9 - 0.1<\/span> and <span style=\"color:DeepPink\">-0.9 and 0.1.<\/span>","e472c556":"<span style=\"color:DeepPink\">*If you want better results from ML models: You need to look at and understand the **HYPERPARAM** parameters of each model.*<\/span>","3184d388":"<span style=\"color:DeepPink\">Let's apply all the EDA codes so far for our newly created dataset.<\/span>","33859b53":"# EDA","d84b409d":"Now let's take a look at what nan values are.","a079c103":"- Ingest Data from links above\n- <span style=\"color:DeepPink\">If you get a different insight from Kaggle data, please write it in the **comments**.<\/span>","a9d7cb1e":"## Xgboost Over\/ Under Sampling","a3db491a":"First of all, it is useful to look at how our \"Component\" and \"Branch\" features explain our data with the following groupings:","83688ceb":"### Basic Libraries For EDA","50d3ca46":"Ok! We solved the problem!!!","b233d163":"<h1><center><span style=\"color:LightCoral; opacity: 1;\">HELLO EVERYONE!<\/span><\/center><\/h1>","1f081a3d":"___","8ae5dad1":"Below is our function that performs **cross validation** according to the \"f1 score\".","ab9cf223":"As you can see, the number of columns has increased, so not all of our columns are the same. **Calm down!** I took a look at for you and carefully examined the two datasets. Only one column name is different. Let's fix it and reconcat it and start our EDA process.<br>\n<span style=\"color:DeepPink\">Let's make necessary treatment :)<\/span>","55d3342d":"- <span style=\"color:DeepPink\">HeatMap Code:<\/span><br>\n&nbsp;&nbsp;&nbsp;&nbsp;plt.figure(figsize=(20,20))<br>\n&nbsp;&nbsp;&nbsp;&nbsp;sns.heatmap(df.corr(), cmap =\"viridis\")<br>\n- But I just put the codes as a comment to inform you without running the heatmap.","ff34e890":"### Getting the Data","527182d5":"<span style=\"color:DeepPink\">Let's make some differences one some parameters.<\/span>","3402c450":"#### Let's examine numeric columns","6ad79f3a":"#### Let's examine object (non numeric) columns","8055998e":"<span style=\"color:DeepPink\">Let's check is there any duplcate or NaN value inside our dataset<\/span> ","aa865c68":"# DATA Preprocessing","229ec5ae":"***Explanations that may be required from a document prepared about data:***<br>\n- The 2012 US Army Anthropometric Survey (ANSUR II) was executed by the Natick Soldier Research, Development and Engineering Center (NSRDEC) from October 2010 to April 2012. It's comprised of personnel representing the total US Army force to include the US Army Active Duty, Reserves, and National Guard. In addition to the anthropometric and demographic data, the ANSUR II database also consists of 3D whole body, foot, and head scans of Soldier participants. These 3D data are not publicly available out of respect for the privacy of ANSUR II participants. The data from this survey are used for a wide range of equipment design, sizing, and tariffing applications within the military and has many potential commercial, industrial, and academic applications.<br>\n    - Reach Complete Data Dict via : <a href=\"https:\/\/www.kaggle.com\/seshadrikolluri\/ansur-ii?select=ANSUR+II+Databases+Overview.pdf\" style=\"color:DeepPink;\">ANSUR II Databases Overview.pd<\/a>\n    \n***About Dataset Shape***<br>\n- The ANSUR II working databases contain <span style=\"color:DeepPink\">93 anthropometric<\/span> measurements which were directly measured, and <span style=\"color:DeepPink\">15 demographic\/administrative<\/span> variables explained below. The <span style=\"color:DeepPink\">ANSUR II Male<\/span> working database contains a total sample of <span style=\"color:DeepPink\">4,082<\/span> subjects text. The <span style=\"color:DeepPink\">ANSUR II Female<\/span>  working database contains a total sample of <span style=\"color:DeepPink\">1,986<\/span> subjects. The databases are reported in the associated spreadsheet files:<br>\n    - a. \u201cANSUR II MALE Public.csv\u201d<br>\n    - b. \u201cANSUR II FEMALE Public.csv\u201d.\n    \n***Where to ingest the data?***<br>\n- <span style=\"color:DeepPink\">***DataWorld Links***<\/span>:\n    - Soldiers Male : <a href=\"https:\/\/query.data.world\/s\/h3pbhckz5ck4rc7qmt2wlknlnn7esr\" style=\"color:DeepPink;\">Soldiers Male (DataWorld)<\/a>\n    - Soldiers Female : <a href=\"https:\/\/query.data.world\/s\/sq27zz4hawg32yfxksqwijxmpwmynq\" style=\"color:DeepPink;\">Soldiers Female (DataWorld)<\/a>\n- <span style=\"color:DeepPink\">***Kaggle Links***<\/span>:\n    - Soldiers Male : <a href=\"https:\/\/www.kaggle.com\/seshadrikolluri\/ansur-ii?select=ANSUR+II+MALE+Public.csv\" style=\"color:DeepPink;\">Soldiers Male (Kaggle)<\/a>\n    - Soldiers Female : <a href=\"https:\/\/www.kaggle.com\/seshadrikolluri\/ansur-ii?select=ANSUR+II+FEMALE+Public.csv\" style=\"color:DeepPink;\">Soldiers Female (Kaggle)<\/a>","16f41b4f":"model = SVC('class_weight=\"balanced\"')\nscores = cross_val_score(model, X_train, y_train, cv = 5, scoring = f1_Hispanic, n_jobs = -1)\nprint([round(i, 4) for i in scores], \"\\n\")\nprint(f\" {i:20} : %{scores.mean()*100:.2f}, std : %{scores.std()*100:.3f} \\n\")","a7370841":"Now we can easily see that, both of them have same number of columns.<br>\n<span style=\"color:DeepPink\">Let's concat each other.<\/span>","acb55b15":"#### Correlation","db0a6924":"I've found 2 columns with the same information about weight in my dataset. It's up to you to decide whether to drop this repeating column or not. But I didn't drop it.","71a98c41":"___","38cb5378":"___","c5b30e37":"Now, the second thing that caught my eye in the Dataset is; <span style=\"color:DeepPink\">\"SubjectNumericRace\"<\/span> and <span style=\"color:DeepPink\">\"DODRace\"<\/span> columns","7a2c166a":"Keep Going! We have a lot of jobs to do.","633fb0fb":"**Oversampling and undersampling in data analysis**<br>\n\n<span style=\"color:DeepPink\">Oversampling and undersampling in data analysis are techniques used to adjust the class distribution of a data set. These terms are used both in statistical sampling, survey design methodology and in machine learning. Oversampling and undersampling are opposite and roughly equivalent techniques.<\/span>"}}