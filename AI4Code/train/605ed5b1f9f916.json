{"cell_type":{"7f05db4f":"code","555afe53":"code","0adad305":"code","b518bd1b":"code","4d9c768c":"code","8d1875e7":"code","059565e4":"code","7472d5a6":"code","bfec0ead":"code","18021d15":"code","9286c9ab":"code","f325627e":"code","6b7d4746":"code","40eca903":"code","226eb3b7":"code","5ee6dd81":"code","52cae5ec":"code","19b6445b":"code","36efb118":"code","24f4ad2e":"code","8e1c5034":"code","4608a346":"code","263d70d1":"code","a88933b2":"code","5c125499":"code","74c2bef0":"code","b27000d1":"code","38f12312":"code","cd3d5467":"code","60f52e8b":"code","8ffc4181":"code","9fb22e04":"code","2ed0a5c6":"code","f10f8778":"code","000a518a":"code","fd0b4fa9":"code","38b8b16e":"code","6a446f82":"code","73135af3":"code","a5a326cc":"markdown","a16462e7":"markdown","f3125b0b":"markdown","a537e8b5":"markdown","c3b798a3":"markdown","7a6ecad7":"markdown","14e2a34d":"markdown","e3f405e6":"markdown","4e405362":"markdown","19d5684e":"markdown","716be962":"markdown","5b55086b":"markdown","5a976bf3":"markdown","67c3ee69":"markdown","37f218a2":"markdown","80e2d907":"markdown","08f4705f":"markdown","615d8da0":"markdown"},"source":{"7f05db4f":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","555afe53":"import pandas as pd # pandas is a python module, used to process and analyze data.","0adad305":"data = pd.read_csv(\"..\/input\/RCOM.csv\")","b518bd1b":"# creating a filter to contain 1 at rows in series contating 'EQ'\nSeries_filter = data['Series'] == 'EQ'","4d9c768c":"data_only_EQ = data[Series_filter]","8d1875e7":"data_only_EQ.reset_index(inplace = True, drop = True) # resetting the index value unless it will not be continuous due to dropped rows","059565e4":"data_only_EQ['Series'].unique() # checking unique values in 'Series' column ","7472d5a6":"data_only_EQ.head() # printing first tuples of data only containing EQ","bfec0ead":"data_only_EQ.tail() #printing last 5 tuples","18021d15":"data_only_EQ.describe() # using describe to find more information about the columns of data with only 'EQ' category","9286c9ab":"data_only_EQ['Date'] = pd.to_datetime(data_only_EQ['Date'])","f325627e":"# After this operation the 'Date' column will become the index\n\ndata_only_EQ.set_index('Date', inplace=True, drop = False) #to use the functionality of using last we need to make Date to be the index","6b7d4746":"price = 'Close Price'\nsent  = \"price for the last 90 days is \"\n\nprint(\"Maximum \" + sent, data_only_EQ[price].last('90D').max())\nprint(\"Minimum \" + sent, data_only_EQ[price].last('90D').min())\nprint(\"Mean \" + sent, data_only_EQ[price].last('90D').mean())","40eca903":"# Analyze the datatype of each column\ndata_only_EQ.info()","226eb3b7":"# print the largest and smallest value of Date to find the range of Data collected\n\nprint(data_only_EQ.index.min(), \" to \", data_only_EQ.index.max())\n\nprint(\"NO. of days :\",data_only_EQ.index.max()- data_only_EQ.index.min())","5ee6dd81":"data_only_EQ = pd.concat([data_only_EQ, pd.DataFrame({'Transactions': data_only_EQ['Average Price']*data_only_EQ['Total Traded Quantity']})], axis = 1)","52cae5ec":"#calculate monthwise VWAP","19b6445b":"month_wise_VWAP = data_only_EQ['Transactions'].groupby(data_only_EQ['Date'].dt.strftime('%B')).sum() \/ data_only_EQ['Total Traded Quantity'].groupby(data_only_EQ['Date'].dt.strftime('%B')).sum()","36efb118":"print(month_wise_VWAP)","24f4ad2e":"#Calculate year wise VWAP","8e1c5034":"year_wise_VWAP = data_only_EQ['Transactions'].groupby(data_only_EQ['Date'].dt.strftime('%Y')).sum() \/ data_only_EQ['Total Traded Quantity'].groupby(data_only_EQ['Date'].dt.strftime('%Y')).sum()","4608a346":"print(year_wise_VWAP)","263d70d1":"# Function to calculate the average price over the last N days of the stock price\ndef calculate_average_price_over_last_N(n, stmp = 'D'):\n    \n    return data_only_EQ['Close Price'].last(str(n) + stmp).mean()","a88933b2":"calculate_average_price_over_last_N(2)","5c125499":"def calculate_profit_loss_over_last(n, stmp = 'D'):\n    k = len(data_only_EQ)\n    curr = data_only_EQ['Close Price'][k-1]\n    n_days_old = data_only_EQ['Close Price'].last(str(n) + stmp)[0]\n    \n    return ((curr - n_days_old)\/n_days_old)*100","74c2bef0":"calculate_profit_loss_over_last(90) # '+' sign indicate a profit and '-' sign indicate a loss","b27000d1":"def calculate_the_average_price_and_profit_loss(num, stmp):\n    \n    print(\"Average Price over last \" + str(num) + stmp+ \" is\", calculate_average_price_over_last_N(num, stmp))\n\n    val = calculate_profit_loss_over_last(num, stmp)\n\n    if val == 0.0:\n        print(\"No profit and No loss\")\n    elif val > 0:\n        print(\"Profit over last {} {} is {:.2f}%\".format(num, stmp, val))\n    else:\n        print(\"Loss over last {} {} is {:.2f}%\".format(num, stmp, val*-1))\n    ","38f12312":"# pass 'D' for days, 'W' for weeks, 'M' for months and 'Y' for 'Year' \n# calculate_the_average_price_and_profit_loss(3, 'Y')\n\n# Average price and profit\/loss over last 1 week\ncalculate_the_average_price_and_profit_loss(1, 'W') # Here is only one day in the last week\nprint()\n\n# Average price and profit\/loss over last 2 week\ncalculate_the_average_price_and_profit_loss(2, 'W')\nprint()\n\n# Average price and profit\/loss over last 1 month\ncalculate_the_average_price_and_profit_loss(1, 'M')\nprint()\n\n# Average price and profit\/loss over last 3 month\ncalculate_the_average_price_and_profit_loss(3, 'M')\nprint()\n\n# Average price and profit\/loss over last 6 month\ncalculate_the_average_price_and_profit_loss(6, 'M')\nprint()\n\n# Average price and profit\/loss over last 1 year\ncalculate_the_average_price_and_profit_loss(1, 'Y')","cd3d5467":"data_only_EQ.insert(data_only_EQ.shape[1], 'Day_Perc_Change', data_only_EQ['Close Price'].pct_change())","60f52e8b":"data_only_EQ = data_only_EQ.iloc[1:,:] # removing the first row containing the Nan value","8ffc4181":"data_only_EQ.insert(data_only_EQ.shape[1], 'Trend', 'Slight')","9fb22e04":"# using .loc to update the value in the column\ndata_only_EQ.loc[data_only_EQ['Day_Perc_Change'].between(-0.005, -0.01, inclusive = True), 'Trend'] = 'Slight negative'\n\ndata_only_EQ.loc[data_only_EQ['Day_Perc_Change'].between(0.005, 0.01, inclusive = True), 'Trend'] = 'Slight positive'\n","2ed0a5c6":"data_only_EQ.loc[data_only_EQ['Day_Perc_Change'].between(-0.03, -0.01, inclusive = False),'Trend'] = 'Negative'\n\ndata_only_EQ.loc[data_only_EQ['Day_Perc_Change'].between(0.01, 0.03, inclusive = False),'Trend'] = 'Positive'\n","f10f8778":"data_only_EQ.loc[data_only_EQ['Day_Perc_Change'].between(-0.07, -0.03, inclusive = True),'Trend'] = 'Top losers'\n\ndata_only_EQ.loc[data_only_EQ['Day_Perc_Change'].between(0.03, 0.07, inclusive = True),'Trend'] = 'Top gainers'\n","000a518a":"data_only_EQ.loc[data_only_EQ['Day_Perc_Change']<-0.07,'Trend']= 'Bear drop'\n\ndata_only_EQ.loc[data_only_EQ['Day_Perc_Change']>0.07,'Trend'] = 'Bear run'","fd0b4fa9":"data_only_EQ['Trend'].unique() # unique values in the 'Trend' column","38b8b16e":"# The average of the 'Total Traded Quantity' for each types of 'Trend'\ndata_only_EQ.groupby('Trend')['Total Traded Quantity'].mean()","6a446f82":"# The median values of the 'Total Traded Quantity' for each types of 'Trend'\ndata_only_EQ.groupby('Trend')['Total Traded Quantity'].median()","73135af3":"data_only_EQ.to_csv('week2_RCOM.csv') #saving the dataframe 'data_only_EQ' to csv file 'week2.csv'","a5a326cc":"# Module1","a16462e7":"**SAVE the dataframe with the additional columns computed as a csv file week2.csv**","f3125b0b":"**1. Add a column 'Day_Perc_Change' where the values are the daily change in percentages i.e. the percentage change between 2 consecutive day's closing prices**","a537e8b5":"### Query 1.3\nAnalyse the data types for each column of the dataframe. Pandas knows how to deal with dates in an intelligent manner. But to make use of Pandas functionality for dates, you need to ensure that the column is of type 'datetime64(ns)'. Change the date column from 'object' type to 'datetime64(ns)' for future convenience. See what happens if you subtract the minimum value of the date column from the maximum value.","c3b798a3":"### Query 1.8\n Find the average and median values of the column 'Total Traded Quantity' for each of the types of 'Trend'.\n{Hint : use 'groupby()' on the 'Trend' column and then calculate the average and median values of the column 'Total Traded Quantity'}","7a6ecad7":"### Query 1.5\nWrite a function to calculate the average price over the last N days of the stock price data where N is a user defined parameter. Write a second function to calculate the profit\/loss percentage over the last N days.\nCalculate the average price AND the profit\/loss percentages over the course of last -\n1 week, 2 weeks, 1 month, 3 months, 6 months and 1 year.\n{Note : Profit\/Loss percentage between N days is the percentage change between the closing prices of the 2 days }","14e2a34d":"### Query 1.9\nSAVE the dataframe with the additional columns computed as a csv file week2.csv. In Module 2, you are going to get familiar with matplotlib, the python module which is used to visualize data.","e3f405e6":"**The information above Table shows** <br>\n* There are no null values in any column of the data_only_EQ DataFrame\n* Date is of datetime64[ns] type\n* Symbol and Series are of object type\n* And rest are either float4 or int64 type\n* There are total 1 datetime64[ns] type, 9 float64 type, 3 int64 type and 2 object type.","4e405362":"### Query 1.1 \nImport the csv file of the stock of your choosing using 'pd.read_csv()' function into a dataframe.\nShares of a company can be offered in more than one category. The category of a stock is indicated in the \u2018Series\u2019 column. If the csv file has data on more than one category, the \u2018Date\u2019 column will have repeating values. To avoid repetitions in the date, remove all the rows where 'Series' column is NOT 'EQ'.\nAnalyze and understand each column properly.\nYou'd find the head(), tail() and describe() functions to be immensely useful for exploration. You're free to carry out any other exploration of your own.","19d5684e":"**Lets take the \"Average Price\" for example**<br>\n1. Total rows (i.e. count) = 494\n2. mean = 2387.545040\n3. standard deviation = 464.726731\n4. min 'Average Price' in all the 728 days = 1725.850000\n5. 25th percentile (i.e. 25% of data points lies below or 1st Quartile) = 1988.825000\n6. 50th percentile (i.e. 50% of data points lies below or 2nd Quartile) = 2402.990000\n7. 75th percentile (i.e. 75% of data points lies below or 3rd Quartile) = 2641.310000\t\n8. max 'Average Price' in all the 728 days = 3633.110000\t","716be962":"1.7 Add another column 'Trend' whose values are:\n'Slight or No change' for 'Day_Perc_Change' in between -0.5 and 0.5\n'Slight positive' for 'Day_Perc_Change' in between 0.5 and 1\n'Slight negative' for 'Day_Perc_Change' in between -0.5 and -1\n'Positive' for 'Day_Perc_Change' in between 1 and 3\n'Negative' for 'Day_Perc_Change' in between -1 and -3\n'Among top gainers' for 'Day_Perc_Change' in between 3 and 7\n'Among top losers' for 'Day_Perc_Change' in between -3 and -7\n'Bull run' for 'Day_Perc_Change' >7\n'Bear drop' for 'Day_Perc_Change' <-7","5b55086b":"### Query 1.7\n\nAdd another column 'Trend' whose values are:\n'Slight or No change' for 'Day_Perc_Change' in between -0.5 and 0.5\n'Slight positive' for 'Day_Perc_Change' in between 0.5 and 1\n'Slight negative' for 'Day_Perc_Change' in between -0.5 and -1\n'Positive' for 'Day_Perc_Change' in between 1 and 3\n'Negative' for 'Day_Perc_Change' in between -1 and -3\n'Among top gainers' for 'Day_Perc_Change' in between 3 and 7\n'Among top losers' for 'Day_Perc_Change' in between -3 and -7\n'Bull run' for 'Day_Perc_Change' >7\n'Bear drop' for 'Day_Perc_Change' <-7","5a976bf3":"### Query 1.4 \nIn a separate array , calculate the monthwise VWAP (Volume Weighted Average Price ) of the stock. \n( VWAP = sum(price*volume)\/sum(volume) ) \nTo know more about VWAP , visit - VWAP definition \n{Hint : Create a new dataframe column \u2018Month\u2019. The values for this column can be derived from the \u2018Date\u201d column by using appropriate pandas functions. Similarly, create a column \u2018Year\u2019 and initialize it. Then use the 'groupby()' function by month and year. Finally, calculate the vwap value for each month (i.e. for each group created).","67c3ee69":"Calculate year wise **VWAP**","37f218a2":"*The value under 'Day_Perc_Change' will be 'Nan' because there is no previous value to compare it with.*","80e2d907":"### Query 1.6\nAdd a column 'Day_Perc_Change' where the values are the daily change in percentages i.e. the percentage change between 2 consecutive day's closing prices. Instead of using the basic mathematical formula for computing the same, use 'pct_change()' function provided by Pandas for dataframes. You will note that the first entry of the column will have a \u2018Nan\u2019 value. Why does this happen? Either remove the first row, or set the entry to 0 before proceeding.","08f4705f":"### Query 1.2\nCalculate the maximum, minimum and mean price for the last 90 days. (price=Closing Price unless stated otherwise)","615d8da0":"**Find the average and median values of the column 'Total Traded Quantity' for each of the types of 'Trend'.**"}}