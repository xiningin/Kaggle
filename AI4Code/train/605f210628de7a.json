{"cell_type":{"38917548":"code","0bab105f":"code","8f5ead6a":"code","ac6756aa":"code","4dd00a4c":"code","97a887b4":"code","758ecb97":"code","ee0423d0":"code","4260767b":"code","4085a787":"code","b26ad183":"code","9a66f7fc":"code","5f21e687":"code","83907867":"code","20a68847":"code","507d4b47":"code","907915a9":"code","cd7bb1bc":"code","f46c0e61":"code","32ac2f54":"code","7ce543e1":"code","0313f8c3":"markdown","40138871":"markdown","3d24e96e":"markdown"},"source":{"38917548":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nfrom sklearn.model_selection import train_test_split\n# Any results you write to the current directory are saved as output.\nimport shap\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.callbacks import ReduceLROnPlateau\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\n\nimport matplotlib.pyplot as plt","0bab105f":"df = pd.read_csv(\"\/kaggle\/input\/digit-recognizer-dataset\/train.csv\")\ndf.shape","8f5ead6a":"df.head()","ac6756aa":"df.values.shape","4dd00a4c":"X = df.drop('label',axis=1).values\ny = df['label'].values","97a887b4":"X = X.reshape(-1,28,28,1)\nX.shape","758ecb97":"y = to_categorical(y, num_classes = 10)\ny.shape","ee0423d0":"x_train, x_val, y_train, y_val = train_test_split(X,y,random_state=42)","4260767b":"plt.imshow(x_train[0][:,:,0])","4085a787":"# my CNN architechture is In -> [[Conv2D->relu]*2 -> MaxPool2D -> Dropout]*2 -> Flatten -> Dense -> Dropout -> Out\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu', input_shape = (28,28,1)))\nmodel.add(Conv2D(filters = 32, kernel_size = (5,5),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(Conv2D(filters = 64, kernel_size = (3,3),padding = 'Same', \n                 activation ='relu'))\nmodel.add(MaxPool2D(pool_size=(2,2), strides=(2,2)))\nmodel.add(Dropout(0.25))\n\n\nmodel.add(Flatten())\nmodel.add(Dense(256, activation = \"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation = \"softmax\"))\n\n# Define the optimizer\noptimizer = RMSprop(lr=0.001, rho=0.9, epsilon=1e-08, decay=0.0)\n\n# Compile the model\nmodel.compile(optimizer = optimizer , loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n\n","b26ad183":"model.summary()","9a66f7fc":"# Set a learning rate annealer\n# reduces lr by factor if no improvement after 3 epochs\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","5f21e687":"x_train.shape,y_train.shape,x_val.shape,y_val.shape","83907867":"\n\nepochs = 10 # Turn epochs to 30 to get 0.9967 accuracy\nbatch_size = 86\n\nhistory = model.fit(x_train, y_train, batch_size = batch_size, epochs = epochs, \n          validation_data = (x_val, y_val), verbose = 2)","20a68847":"# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1,figsize=(16,8))\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\",marker='o')\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0],marker='o')\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['accuracy'], color='b', label=\"Training accuracy\",marker='o')\nax[1].plot(history.history['val_accuracy'], color='r',label=\"Validation accuracy\",marker='o')\nlegend = ax[1].legend(loc='best', shadow=True)","507d4b47":"y_pred_val = model.predict(x_val)\ny_pred_val.shape","907915a9":"# select a set of background examples to take an expectation over\nbackground = x_train[np.random.choice(x_train.shape[0], 100, replace=False)]","cd7bb1bc":"# explain predictions of the model on four images\ne = shap.GradientExplainer(model, background)","f46c0e61":"for i in range(1,5):\n    plt.figure()\n    plt.title(y_pred_val[i])\n    plt.imshow(x_val[i][:,:,0])\n\n","32ac2f54":"shap_values = e.shap_values(x_val[1:5])","7ce543e1":"# plot the feature attributions\nshap.image_plot(shap_values, -x_val[1:5])","0313f8c3":"## Explainer for Keras Model","40138871":"#### with TF2.0 use GradientExplainer instead of DeepExplainer!","3d24e96e":"# Explaining Keras decisions on MNIST with SHAP GradientExplainer\n* Explainer: http:\/\/papers.nips.cc\/paper\/7062-a-unified-approach-to-interpreting-model-predictions\n\n"}}