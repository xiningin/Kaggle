{"cell_type":{"723761f9":"code","44320fd2":"code","05ee8106":"code","16eb11d5":"code","3c47eef1":"code","61e8b005":"code","52f716ba":"code","cd1bd010":"code","57a334be":"code","991ebdce":"code","f7f8f497":"code","3db31e54":"code","5002dbe1":"code","85e2cb3b":"code","6da02cc2":"code","0997abbf":"code","83284e80":"code","8f6ce0e8":"code","fff5daf3":"code","ebc2fb2e":"code","f1d2be68":"code","da95e8b5":"code","b4ca2839":"code","b7ab44e8":"code","4f87b9e1":"code","645d0a5d":"code","e121acc3":"code","1e022c5c":"code","e885565e":"code","42f7e1d8":"code","c7823168":"code","0fc2b1b1":"code","ae9b528d":"code","5c72e15f":"code","3ccf4bc5":"code","f6792f16":"code","dcc9fbbe":"code","97303236":"code","7d7164bf":"code","ad1d80ac":"code","1366353e":"code","566353ef":"code","dc27e4f4":"markdown","4605239d":"markdown","7b20b2b2":"markdown","5fa5bc93":"markdown","34000d57":"markdown","73f047c0":"markdown","e97ed042":"markdown","7673a65c":"markdown","8aec621d":"markdown","9d343456":"markdown","31174985":"markdown","4d48c478":"markdown","deaa0c38":"markdown"},"source":{"723761f9":"# the focus of this notebook is to discuss Clustering in detail - applications, usage, etc","44320fd2":"import numpy as np","05ee8106":"# Clustering in simple terms is the n Dimensional equivalent of a scatter plot","16eb11d5":"# look at the IRIS dataset and plot the various classes using a scatter plot","3c47eef1":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom sklearn import datasets","61e8b005":"iris = datasets.load_iris()\nX = iris.data\ny = iris.target","52f716ba":"X.shape","cd1bd010":"# the 4 cols are Petal Length , Petal Width , Sepal Length , Sepal width","57a334be":"y.shape","991ebdce":"#let's plot a scatter plot of 2 dimensions - say petal length and petal width\nX_2cols = X[:, :2]","f7f8f497":"X_2cols.shape","3db31e54":"X_2cols[: , 1].shape","5002dbe1":"plt.scatter(x=X_2cols[:, 0], y=X_2cols[:, 1], edgecolor = 'k', c = y)\nplt.show()","85e2cb3b":"#we can clearly see some groups in the above 2D plot; the top left has a lot of blues \n#while the other part is a mix of the other two classes\n\n#let's try a 3D plot with one more dimension","6da02cc2":"fig = plt.figure(figsize=(10,8))\n\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X[:, 0], X[:, 1], X[:, 2], edgecolor = 'k', c = y)\n\nax.set_xlabel('Petal Length')\nax.set_ylabel('Petal Width')\nax.set_zlabel('Sepal Length')\n\nplt.show()","0997abbf":"# now we can clearly see the three distinct classes separated nicely","83284e80":"#lets try out a clustering algorithm on this data","8f6ce0e8":"from sklearn.cluster import KMeans","fff5daf3":"iris_kmeans = KMeans(n_clusters=3).fit(X)","ebc2fb2e":"X.shape","f1d2be68":"y.shape","da95e8b5":"iris_kmeans","b4ca2839":"#the output of the clustering exercise is the labels tagged to each record; this is available in the labels parameter","b7ab44e8":"iris_kmeans.labels_","4f87b9e1":"fig = plt.figure(figsize=(10,8))\n\nax = fig.add_subplot(111, projection='3d')\nax.scatter(X[:, 0], X[:, 1], X[:, 2], edgecolor = 'k', c = iris_kmeans.labels_)\n\nax.set_xlabel('Petal Length')\nax.set_ylabel('Petal Width')\nax.set_zlabel('Sepal Length')\n\nplt.show()","645d0a5d":"y #the actual (true) lable","e121acc3":"#confusion matrix to check how close to the truth was our clustering algo output","1e022c5c":"from sklearn.metrics import confusion_matrix","e885565e":"confusion_matrix(y_pred=iris_kmeans.labels_, y_true=y)","42f7e1d8":"#we see that the first class is identified without any error while there are errors in the other two classes","c7823168":"iris_kmeans.inertia_","0fc2b1b1":"#initialize a list to hold the within cluster sum of squares\nwcss = []\n\n#run kmeans for different n cluster values and save the wcss in this list\nfor i in range(1, 15):\n    kmeans = KMeans(n_clusters = i, init = 'k-means++', max_iter = 300, n_init = 10, random_state = 0)\n    kmeans.fit(X)\n    wcss.append(kmeans.inertia_) # inertia is Sum of squared distances of samples to their closest cluster center\n    \n#Plotting the results onto a line graph, allowing us to observe 'The elbow'\nplt.plot(range(1, 15), wcss)\nplt.title('The elbow method')\nplt.xlabel('Number of clusters')\nplt.ylabel('WCSS')\nplt.show()","ae9b528d":"from scipy.cluster.hierarchy import dendrogram, linkage","5c72e15f":"iris_hierarchical = linkage(X, method='ward')","3ccf4bc5":"type(iris_hierarchical)","f6792f16":"iris_hierarchical[0]","dcc9fbbe":"iris_hierarchical[1]","97303236":"iris_hierarchical[:8]","7d7164bf":"iris_hierarchical.shape","ad1d80ac":"iris_hierarchical[148]","1366353e":"#https:\/\/joernhees.de\/blog\/2015\/08\/26\/scipy-hierarchical-clustering-and-dendrogram-tutorial\/","566353ef":"# calculate full dendrogram\nplt.figure(figsize=(8, 6))\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('sample index')\nplt.ylabel('distance')\ndendrogram(\n    iris_hierarchical,\n    leaf_rotation=90.,  # rotates the x axis labels\n    leaf_font_size=8.,  # font size for the x axis labels\n)\nplt.show()","dc27e4f4":"# How does K Means Clustering Work?","4605239d":"1. Choose k points at random in the data set\n2. These points would be called centroids and they represent the k clusters\n3. Take a point (other than the centroids), find the centroid that is closest to the point and assign the point to that centroid (cluster)\n4. Repeat this for all points\n5. Once all the initial assignments are done, calculate the new centroid for each cluster (centroid - mid point)\n6. Repear steps 3 - 5 till there is no change in assignment happens","7b20b2b2":"The algorithm starts assuming each sample is a cluster in itself <br>\ntry to find the nearest cluster to your cluster - different methods are there; average linkage, min linkage, etc.<br>\nadd the nearest cluster to your cluster<br>\nkeep going till you have only one cluster (top node)<br>\n<br>\nhttp:\/\/www.analytictech.com\/networks\/hiclus.htm","5fa5bc93":"# How does K Means work","34000d57":"# How is Hierarchical different from K Means","73f047c0":"https:\/\/www.quora.com\/What-are-the-pros-and-cons-of-kmeans-vs-hierarchical-clustering\/answer\/Shehroz-Khan-2 <br>\nhttps:\/\/www.cs.utah.edu\/~piyush\/teaching\/4-10-print.pdf <br>\nhttps:\/\/www.quora.com\/What-are-the-advantages-of-K-Means-clustering","e97ed042":"# Elbow Plot:\nHow do we know what is the current number of clusters for this dataset? Remember that clustering is 'unsupervised learning'.\nAnswer is the Elbow plot","7673a65c":"The sharpest fall is at 2 and 3; we can choose 3 as the n clusters","8aec621d":"# what has been achieved so far?\nIf we had no knowledge of the actual classes of this dataset, clustering would have provided with a grouping of the data points. And, this is an extension of the scatter plot we saw above","9d343456":"# 18 Apr 2018","31174985":"https:\/\/github.com\/dgrtwo\/dgrtwo.github.com\/blob\/master\/_R\/2015-01-16-kmeans-free-lunch.Rmd <br>\nhttp:\/\/varianceexplained.org\/r\/kmeans-free-lunch\/","4d48c478":"# Hierarchical Clustering","deaa0c38":"#the way to read the elements in the above array: <br>\n1st element, 2nd element - index of the sample that was merged in this iteration<br>\n3rd element - distance between the above 2 samples<br>\n4th element - #samples in the cluster at this level. Notice the value above is 150 corresponding to the top node<br>"}}