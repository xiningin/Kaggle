{"cell_type":{"dfebf10c":"code","d9eecdad":"code","e598318f":"code","c9a6a72c":"code","6f75fcfc":"code","b03d07e2":"code","4bd8c07a":"code","f303afe9":"code","a44a4c35":"code","00c62d51":"code","bad45b3b":"code","6fe51bc1":"code","2fa6dd18":"code","a5b57ae5":"code","73d5186e":"code","9a6a0368":"code","872db78d":"code","20bc3168":"code","319adc54":"code","98a4ba3d":"code","70d14d6a":"code","1b9831b2":"code","0daaefb7":"code","57a89239":"code","3b5b7152":"code","626a912b":"code","4c65b0bd":"code","526070d6":"code","1bf7cfb9":"code","e039a909":"code","46bcba83":"code","2a25deaa":"code","4354af07":"code","f7b332ed":"code","d9c72761":"code","06b9570b":"code","c41dedcc":"code","cf6a3e25":"code","fe452c78":"code","f102e387":"markdown","0fe109f6":"markdown","1dc77bf6":"markdown","3f21ecb7":"markdown","9e63c484":"markdown","7900b831":"markdown","0bd30f7f":"markdown","fde17c4d":"markdown","67af4667":"markdown","054280cf":"markdown","630b0322":"markdown","b09aac2c":"markdown","93284e0e":"markdown","03b24a2d":"markdown","93ee433b":"markdown","155d01ca":"markdown","b64b5a0b":"markdown","7377e4fd":"markdown","c0215b7f":"markdown","1125f09c":"markdown","6f1599bc":"markdown","eddc3a50":"markdown","c2daa022":"markdown","f9812942":"markdown","6898dab2":"markdown","7cd43c5b":"markdown","eca7b4a6":"markdown","0e43745e":"markdown","623caedc":"markdown","25c7cf31":"markdown","6c5d5f43":"markdown","82e551c8":"markdown","c55c2033":"markdown","26e6b808":"markdown","d5664034":"markdown","75765bb7":"markdown","dcbe152c":"markdown","9f380832":"markdown","5a166043":"markdown","e45a710c":"markdown","ed90ae9c":"markdown","4f657432":"markdown","7b115e8c":"markdown","3ee139a5":"markdown","b99820a9":"markdown","c12acc7e":"markdown"},"source":{"dfebf10c":"# Ignore warnings :\nimport warnings\nwarnings.filterwarnings('ignore')\n\n\n# Handle table-like data and matrices :\nimport numpy as np\nimport pandas as pd\nimport math \nimport itertools\n\n\n\n# Modelling Algorithms :\n\n# Classification\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis , QuadraticDiscriminantAnalysis\n\n# Regression\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV, ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor \nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.neural_network import MLPRegressor\n\n\n\n\n# Modelling Helpers :\nfrom sklearn.preprocessing import Imputer , Normalizer , scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import GridSearchCV , KFold , cross_val_score\n\n\n\n#preprocessing :\nfrom sklearn.preprocessing import MinMaxScaler , StandardScaler, Imputer, LabelEncoder\n\n\n\n#evaluation metrics :\n\n# Regression\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error \n\n# Classification\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score  \n\n\n# Deep Learning Libraries\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D, BatchNormalization\nfrom keras.optimizers import Adam,SGD,Adagrad,Adadelta,RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau, LearningRateScheduler\nfrom keras.utils import to_categorical\n\n\n\n# Visualisation\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nimport missingno as msno\n\n\n\n# Configure visualisations\n%matplotlib inline\nmpl.style.use( 'ggplot' )\nplt.style.use('fivethirtyeight')\nsns.set(context=\"notebook\", palette=\"dark\", style = 'whitegrid' , color_codes=True)","d9eecdad":"# Center all plots\nfrom IPython.core.display import HTML\nHTML(\"\"\"\n<style>\n.output_png {\n    display: table-cell;\n    text-align: center;\n    vertical-align: middle;\n}\n<\/style>\n\"\"\");\n\n# Make Visualizations better\nparams = { \n    'axes.labelsize': \"large\",\n    'xtick.labelsize': 'x-large',\n    'legend.fontsize': 20,\n    'figure.dpi': 150,\n    'figure.figsize': [25, 7]\n}\nplt.rcParams.update(params)","e598318f":"train = pd.read_csv('..\/input\/fashion-mnist_train.csv')\ntest = pd.read_csv('..\/input\/fashion-mnist_test.csv')\ndf = train.copy()\ndf_test = test.copy()","c9a6a72c":"df.head()","6f75fcfc":"print('Train: ', df.shape)\nprint('Test: ', df_test.shape)","b03d07e2":"df.label.unique()","4bd8c07a":"# Train\ndf.isnull().any().sum()","f303afe9":"# Test\ndf_test.isnull().any().sum()","a44a4c35":"# Mapping Classes\nclothing = {0 : 'T-shirt\/top',\n            1 : 'Trouser',\n            2 : 'Pullover',\n            3 : 'Dress',\n            4 : 'Coat',\n            5 : 'Sandal',\n            6 : 'Shirt',\n            7 : 'Sneaker',\n            8 : 'Bag',\n            9 : 'Ankle boot'}","00c62d51":"fig, axes = plt.subplots(4, 4, figsize = (15,15))\nfor row in axes:\n    for axe in row:\n        index = np.random.randint(60000)\n        img = df.drop('label', axis=1).values[index].reshape(28,28)\n        cloths = df['label'][index]\n        axe.imshow(img, cmap='gray')\n        axe.set_title(clothing[cloths])\n        axe.set_axis_off()","bad45b3b":"df['label'].value_counts()","6fe51bc1":"sns.factorplot(x='label', data=df, kind='count', size=3, aspect= 1.5)","2fa6dd18":"# Setting Random Seeds for Reproducibilty.\nseed = 66\nnp.random.seed(seed)","a5b57ae5":"X = train.iloc[:,1:]\nY = train.iloc[:,0]\nx_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=0.1, random_state=seed)","73d5186e":"# The first parameter in reshape indicates the number of examples.\n# We pass it as -1, which means that it is an unknown dimension and we want numpy to figure it out.\n\n# reshape(examples, height, width, channels)\nx_train = x_train.values.reshape((-1, 28, 28, 1))\nx_test = x_test.values.reshape((-1, 28, 28, 1))\n\ndf_test.drop('label', axis=1, inplace=True)\ndf_test = df_test.values.reshape((-1, 28, 28, 1))","9a6a0368":"# You need to make sure that your Image is cast into double\/float from int before you do this scaling \n# as you will most likely generate floating point numbers.\n# And had it been int, the values will be truncated to zero.\n\nx_train = x_train.astype(\"float32\")\/255\nx_test = x_test.astype(\"float32\")\/255\ndf_test = df_test.astype(\"float32\")\/255","872db78d":"y_train = to_categorical(y_train, num_classes=10)\ny_test = to_categorical(y_test, num_classes=10)","20bc3168":"print(y_train.shape)\nprint(y_test.shape)","319adc54":"# Building a ConvNet\nmodel = Sequential()\nmodel.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', strides=1, padding='same', \n                 data_format='channels_last', input_shape=(28,28,1)))\nmodel.add(BatchNormalization())\n\nmodel.add(Conv2D(filters=32, kernel_size=(3, 3), activation='relu', strides=1, padding='same', \n                 data_format='channels_last'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\nmodel.add(Conv2D(filters=64, kernel_size=(3, 3), activation='relu', strides=1, padding='same', \n                 data_format='channels_last'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n    \n    \nmodel.add(Conv2D(filters=128, kernel_size=(3, 3), activation='relu', strides=1, padding='same', \n                 data_format='channels_last'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\nmodel.add(Dense(512, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(128, activation='relu'))\nmodel.add(BatchNormalization())\nmodel.add(Dropout(0.5))\nmodel.add(Dense(10, activation='softmax'))","98a4ba3d":"# Optimizer\noptimizer = Adam(lr=0.001, beta_1=0.9, beta_2=0.999 )","70d14d6a":"# Compiling the model\nmodel.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","1b9831b2":"model.summary()","0daaefb7":"reduce_lr = LearningRateScheduler(lambda x: 1e-3 * 0.9 ** x)","57a89239":"datagen = ImageDataGenerator(\n        rotation_range = 8,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        shear_range = 0.3,# shear angle in counter-clockwise direction in degrees  \n        width_shift_range=0.08,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.08,  # randomly shift images vertically (fraction of total height)\n        vertical_flip=True)  # randomly flip images","3b5b7152":"datagen.fit(x_train)","626a912b":"batch_size = 128\nepochs = 40","4c65b0bd":"# Fit the Model\nhistory = model.fit_generator(datagen.flow(x_train, y_train, batch_size = batch_size), epochs = epochs, \n                              validation_data = (x_test, y_test), verbose=2, \n                              steps_per_epoch=x_train.shape[0] \/\/ batch_size,\n                              callbacks = [reduce_lr])","526070d6":"score = model.evaluate(x_test, y_test)\n\nprint('Loss: {:.4f}'.format(score[0]))\nprint('Accuracy: {:.4f}'.format(score[1]))","1bf7cfb9":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title(\"Model Loss\")\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend(['Train', 'Test'])\nplt.show()","e039a909":"plt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title(\"Model Accuracy\")\nplt.xlabel('Epochs')\nplt.ylabel('Accuracy')\nplt.legend(['Train', 'Test'])\nplt.show()","46bcba83":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","2a25deaa":"# Predict the values from the validation dataset\nY_pred = model.predict(x_test)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_test,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, \n            classes = ['T-shirt\/Top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle Boot'])","4354af07":"correct = []\nfor i in range(len(y_test)):\n    if(Y_pred_classes[i] == Y_true[i]):\n        correct.append(i)\n    if(len(correct) == 4):\n        break","f7b332ed":"fig, ax = plt.subplots(2,2, figsize=(12,6))\nfig.set_size_inches(10,10)\nax[0,0].imshow(x_test[correct[0]].reshape(28,28), cmap='gray')\nax[0,0].set_title(\"Predicted Label : \" + str(clothing[Y_pred_classes[correct[0]]]) + \"\\n\"+\"Actual Label : \" + \n                 str(clothing[Y_true[correct[0]]]))\nax[0,1].imshow(x_test[correct[1]].reshape(28,28), cmap='gray')\nax[0,1].set_title(\"Predicted Label : \" + str(clothing[Y_pred_classes[correct[1]]]) + \"\\n\"+\"Actual Label : \" + \n                 str(clothing[Y_true[correct[1]]]))\nax[1,0].imshow(x_test[correct[2]].reshape(28,28), cmap='gray')\nax[1,0].set_title(\"Predicted Label : \" + str(clothing[Y_pred_classes[correct[2]]]) + \"\\n\"+\"Actual Label : \" + \n                 str(clothing[Y_true[correct[2]]]))\nax[1,1].imshow(x_test[correct[3]].reshape(28,28), cmap='gray')\nax[1,1].set_title(\"Predicted Label : \" + str(clothing[Y_pred_classes[correct[3]]]) + \"\\n\"+\"Actual Label : \" + \n                 str(clothing[Y_true[correct[3]]]))","d9c72761":"incorrect = []\nfor i in range(len(y_test)):\n    if(not Y_pred_classes[i] == Y_true[i]):\n        incorrect.append(i)\n    if(len(incorrect) == 4):\n        break","06b9570b":"fig, ax = plt.subplots(2,2, figsize=(12,6))\nfig.set_size_inches(10,10)\nax[0,0].imshow(x_test[incorrect[0]].reshape(28,28), cmap='gray')\nax[0,0].set_title(\"Predicted Label : \" + str(clothing[Y_pred_classes[incorrect[0]]]) + \"\\n\"+\"Actual Label : \" + \n                 str(clothing[Y_true[incorrect[0]]]))\nax[0,1].imshow(x_test[incorrect[1]].reshape(28,28), cmap='gray')\nax[0,1].set_title(\"Predicted Label : \" + str(clothing[Y_pred_classes[incorrect[1]]]) + \"\\n\"+\"Actual Label : \" + \n                 str(clothing[Y_true[incorrect[1]]]))\nax[1,0].imshow(x_test[incorrect[2]].reshape(28,28), cmap='gray')\nax[1,0].set_title(\"Predicted Label : \" + str(clothing[Y_pred_classes[incorrect[2]]]) + \"\\n\"+\"Actual Label : \" + \n                 str(clothing[Y_true[incorrect[2]]]))\nax[1,1].imshow(x_test[incorrect[3]].reshape(28,28), cmap='gray')\nax[1,1].set_title(\"Predicted Label : \" + str(clothing[Y_pred_classes[incorrect[3]]]) + \"\\n\"+\"Actual Label : \" + \n                 str(clothing[Y_true[incorrect[3]]]))","c41dedcc":"classes = ['T-shirt\/Top','Trouser','Pullover','Dress','Coat','Sandal','Shirt','Sneaker','Bag','Ankle Boot']\nprint(classification_report(Y_true, Y_pred_classes, target_names = classes))","cf6a3e25":"X = df_test\nY = to_categorical(test.iloc[:,0])","fe452c78":"score = model.evaluate(X, Y)\n\nprint(\"Loss: {:.4f}\".format(score[0]))\nprint(\"Accuracy: {:.4f}\".format(score[1]))","f102e387":"## Topics\n1. [**Exploring the Dataset**](#there_you_go_1)\n> *  [1.1 Importing Libraries ](#there_you_go_1.1)\n  * [1.2 Extract dataset ](#there_you_go_1.2)\n  * [1.3 Features ](#there_you_go_1.3)\n  * [1.4 Examine Dimensions ](#there_you_go_1.4)\n  * [1.5 Examine NaN values ](#there_you_go_1.5)\n2. [**Visualizing the Dataset**](#there_you_go_2)\n> * [2.1 Plotting Random Images ](#there_you_go_2.1)\n  * [2.2 Distribution of Labels ](#there_you_go_2.2)\n3. [**Data PreProcessing**](#there_you_go_3)\n> * [3.1 Setting Random Seeds ](#there_you_go_3.1)\n * [3.2 Splitting Data ](#there_you_go_3.2)\n * [3.3 Reshaping Images ](#there_you_go_3.3)\n * [3.4 Normalization ](#there_you_go_3.4)\n * [3.5 One Hot Encoding ](#there_you_go_3.5)\n4. [**Training ConvNet**](#there_you_go_4)\n> * [4.1 Building a ConvNet ](#there_you_go_4.1)\n * [4.2 Compiling Model ](#there_you_go_4.2)\n * [4.3 Model Summary ](#there_you_go_4.3)\n * [4.4 Learning Rate Decay ](#there_you_go_4.4)\n * [4.5 Data Augmentation ](#there_you_go_4.5)\n * [4.6 Fitting the Model](#there_you_go_4.6)\n5. [**Evaluating the Model**](#there_you_go_5)\n> * [5.1 Plotting Train and Validation curves ](#there_you_go_5.1)\n6. [**Plotting Confusion Matrix**](#there_you_go_6)\n7. [**Visualization of Predicted Classes**](#there_you_go_7)\n> * [7.1 Correctly Predicted Classes](#there_you_go_7.1)\n * [7.2 Incorrectly Predicted Classes](#there_you_go_7.2)\n8. [**Classification Report**](#there_you_go_8)\n9. [**Predicting on Test Data**](#there_you_go_9)","0fe109f6":"* Steps:\n\n1) At First, we use **Sequential Keras API** which is just a linear stack of layers. We add one layer at a time starting from input.\n\n2) Next We add **Convolutional Layers**, which are the Building blocks of ConvNets. Convolutional Layers has set of Independent Filters whose depth is equal to Input and other dimensions can be set manually. These Filters when convolved over the Input Image produce Feature Maps.\n\nIt includes some HyperParameters such as **The number of filters, Dimensions of Filter (F), Stride (S), Padding(P) , Activation Function etc. which we input manually. Let the Input Volume Size be deonted by (W) ,**\n\n**Then, the Output will have Dimensions given by -->**\n\n**(Height, Width) = ( ( W \u2212 F + 2P ) \/ S ) + 1**\n\nAnd the Depth will be equal to Number of Filters Specified.\n\n3) Next We add **Pooling Layers**, which are used for Dimensionality Reduction or DownSampling the Input. These are used where we have lot of Input Features. It reduces the amount of Parameters and Computational power required drastically, thus reducing Overfitting. These along with Convolutional layers are able to learn more Complex features of the Image.\n\n4) We add **Batch Normalization** where we acheive Zero mean and Variance one. It scales down outliers and forces the network to learn features in a distributed way, not relying too much on a Particular Weight and makes the model better Generalize the Images.\n\n5) To avoid Overfitting We add **Dropout**. This randomly drops some percentage of neurons, and thus the weights gets Re-Aligned. The remaining Neurons learn more features and this reduces the dependency on any one Neuron. DropOut is a Regularization Technique, which Penalizes the Parameters. Generally we set the DropOutRate between 0.2-0.5 .\n\n6) Finally we add **Flatten layer** to map the input to a 1D vector. We then add Fully connected Layers after some convolutional\/pooling layers. It combines all the Features of the Previous Layers.\n\n7) Lastly, we add the **Output Layer**. It has units equal to the number of classes to be identified. Here, we use 'sigmoid' function if it is Binary Classification otherwise 'softmax' activation function in case of Multi-Class Classification.","1dc77bf6":"<a id=\"there_you_go_3.2\"><\/a>\n## 3.2) Splitting Data into Train and Validation Set\nNow we are gonna split the training data into Train and Validation Set. Train set is used for Training the model and Validation set is used for Evaluating our Model's Performance on the Dataset.\n\nThis is achieved using the train_test_split method of scikit learn library.","3f21ecb7":"<a id=\"there_you_go_3.5\"><\/a>\n## 3.5) One Hot Encoding\nThe labels are given as integers between 0-9. We need to one hot encode them , Eg 8 [0, 0, 0, 0, 0, 0, 0, 0, 1, 0] .\n\nWe have 10 digits [0-9] or classes, therefore we one-hot-encode the target variable with 10 classes","9e63c484":"## Do Star\/Upvote if you like it ;)","7900b831":"**Look at the Precision of the Shirts, we can see that our model predicted less than 80% of Shirts correctly out of the total images it predicted as Shirts. We did conclude the same from the confusion matrix, where we saw that a lot of T-shirts were misclassified as Shirts.**","0bd30f7f":"# Fashion-MNIST\n1) Fashion-MNIST is a dataset of Zalando's article images\u2014consisting of a training set of 60,000 examples and a test set of 10,000 examples.\n\n2) Each example is a 28x28 grayscale image, associated with a label from 10 classes. \n\n3) Zalando intends Fashion-MNIST to serve as a direct drop-in replacement for the original MNIST dataset for benchmarking machine learning algorithms. \n\n4) It shares the same image size and structure of training and testing splits. \n\n* **You can also view the notebook and contribute below.**\n* **Github Link ->** [**Fashion-MNIST**](https:\/\/github.com\/Chinmayrane16\/Fashion-MNIST-Accuracy-93.4-)\n* **Do Star\/Upvote if you like it ;)**\n\n\n* **If you need more Detailed explanations on the various terminologies used here, **\n* **you can refer to my kernel -> **[**Beginners Guide to CNN (MNIST)**](https:\/\/www.kaggle.com\/fuzzywizard\/beginners-guide-to-cnn-accuracy-99-7)","fde17c4d":"<a id=\"there_you_go_3.1\"><\/a>\n## 3.1) Setting Random Seeds","67af4667":"<a id=\"there_you_go_7.1\"><\/a>\n## 7.1) Correctly Predicted Classes","054280cf":"<a id=\"there_you_go_5.1\"><\/a>\n## 5.1) Plotting the Training and Validation Curves","630b0322":"<a id=\"there_you_go_3\"><\/a>\n# 3) Data PreProcessing","b09aac2c":"<a id=\"there_you_go_4\"><\/a>\n# 4) Training a Convolutional Neural Network","93284e0e":"<a id=\"there_you_go_1\"><\/a>\n# 1) Exploring The Dataset","03b24a2d":"<a id=\"there_you_go_1.1\"><\/a>\n## 1.1) Importing Libraries","93ee433b":"<a id=\"there_you_go_2\"><\/a>\n# 2) Visualizing the Dataset","155d01ca":"<a id=\"there_you_go_8\"><\/a>\n# 8) Classification Report\nThe classification report visualizer displays the precision, recall, F1, and support scores for the model.\n\n* **Precision: **\n> Precision is the ability of a classiifer not to label an instance positive that is actually negative. Basically, it is defined as as the ratio of true positives to the sum of true and false positives. \u201cFor all instances classified positive, what percent was correct?\u201d\n\n* **Recall: **\n> Recall is the ability of a classifier to find all positive instances. For each class it is defined as the ratio of true positives to the sum of true positives and false negatives.  \u201cFor all instances that were actually positive, what percent was classified correctly?\u201d\n\n* **F1 Score: **\n> The F1 score is a weighted harmonic mean of precision and recall such that the best score is 1.0 and the worst is 0.0 . Generally speaking, F1 scores are lower than accuracy measures as they embed precision and recall into their computation.\n\n* **Support: **\n> Support is the number of actual occurrences of the class in the specified dataset. Imbalanced support in the training data may indicate structural weaknesses in the reported scores of the classifier and could indicate the need for stratified sampling or rebalancing. ","b64b5a0b":"<a id=\"there_you_go_5\"><\/a>\n# 5) Evaluating the Model","7377e4fd":"<a id=\"there_you_go_1.4\"><\/a>\n## 1.4) Examine Dimensions","c0215b7f":"**Look at these Images, I bet, there will be images which even the Humans won't be able to claasify.**","1125f09c":"<a id=\"there_you_go_2.1\"><\/a>\n## 2.1) Plotting Random Images","6f1599bc":"<a id=\"there_you_go_2.2\"><\/a>\n## 2.2) Distribution of Labels\n**Let's look at the Distribution of labels to anaylze if there are any skewed classes.**","eddc3a50":"<a id=\"there_you_go_4.1\"><\/a>\n## 4.1) Building a ConvNet","c2daa022":"<a id=\"there_you_go_3.4\"><\/a>\n## 3.4) Normalization\nThe Pixel Values are often stored as __*Integer*__ Numbers in the range 0 to 255, the range that a single 8-bit byte can offer.\nThey need to be scaled down to [0,1] in order for Optimization Algorithms to work much faster. Here, we acheive Zero Mean and Unit Variance.\n\nNormalization is carried out as follows: \n> x = (x - min) \/ (max - min) ; Here min=0 and max=255","f9812942":"<a id=\"there_you_go_4.4\"><\/a>\n## 4.4) Learning Rate Decay\n* The Learning rate should be properly tuned , such that it is not too high to take very large steps, neither it should be too small , which would not alter the Weights and Biases.\n* We will use **LearningRateScheduler** here, which takes the step decay function as argument and return the updated learning rates for use in optimzer at every epoch stage. Basically it outputs a new learning rate at every epoch stage.","6898dab2":"<a id=\"there_you_go_7.2\"><\/a>\n## 7.2) Incorrectly Predicted Classes","7cd43c5b":"![Imgur](https:\/\/i.imgur.com\/CFlSx1M.jpg)","eca7b4a6":"**Great, So there are No Null Values in Train and Test Set.**","0e43745e":"## END  \n**Thank You...!!  :)**","623caedc":"# 9) Predicting on the Test Data\nLet's Evaluate the Models performance on the Test Data.","25c7cf31":"<a id=\"there_you_go_4.3\"><\/a>\n## 4.3) Model Summary","6c5d5f43":"<a id=\"there_you_go_3.3\"><\/a>\n## 3.3) Reshaping the Images\n* Note that we have Images as 1D vector each containing 784 pixels. Before we feed the data to the CNN we must reshape the data into (28x28x1) 3D matrices.\n* This is because Keras wants an Extra Dimension in the end, for channels. If this had been RGB images, there would have been 3 channels, but as MNIST is gray scale it only uses one.","82e551c8":"![Imgur](https:\/\/i.imgur.com\/coDqChv.png)","c55c2033":"* **So, there are 60,000 Training Samples and 10,000 Test Samples.**\n* **Each example is a 28x28 grayscale image, associated with a label from 10 classes.**\n> * Each pixel has a single pixel-value associated with it, indicating the lightness or darkness of that pixel, with higher numbers meaning darker. \n> * This pixel-value is an integer between 0 and 255, inclusive.\n* **The first column of the Training Samples consists of Class Labels and represents the article of Clothing.**","26e6b808":"<a id=\"there_you_go_1.2\"><\/a>\n## 1.2) Extract Dataset","d5664034":"<a id=\"there_you_go_6\"><\/a>\n# 6) Confusion Matrix\nA confusion matrix is a table that is often used to describe the performance of a classification model (or \"classifier\") on a set of test data for which the true values are known.\n\nLet's view the the Performance of our classification model on the data using Confusion Matrix.","75765bb7":"<a id=\"there_you_go_1.3\"><\/a>\n## 1.3) Features\n* **Label: ** The Target variable.\n* **Pixels: ** The smallest unit of a Digital Image or Graphic that can be displayed on Digital Display Device.\n\nWhere humans can see the objects due to the Light Receptors in their Eyes which send Signals via the Optic Nerve to the Primary Visual Cortex, where the input is processed ,\n\nComputers on the other hand, see the Image as 2-dimensional arrays of numbers, known as pixels. They Classify Images based on Boundaries and Curvatures of the Object (Represented by pixel values, either RGB or GrayScale) .\n\nThis is the Partial View of the Labels and the Dataset.","dcbe152c":"**Labels :**\n* **0 - ** T-shirt\/top\n* **1 - ** Trouser\n* **2 - ** Pullover\n* **3 - ** Dress\n* **4 - ** Coat\n* **5 - ** Sandals\n* **6 - ** Shirt\n* **7 - ** Sneaker\n* **8 - ** Bag\n* **9 - ** Ankle Boots","9f380832":"<a id=\"there_you_go_4.2\"><\/a>\n## 4.2) Compiling the Model\n1) We need to compile the model. We have to specify the optimizer used by the model We have many choices like Adam, RMSprop etc.. Refer to Keras doc for a comprehensive list of the optimizers available.\n\n2) Next we need to specify the loss function for the neural network which we want to minimize.\n\nFor Binary Classification we use \"binary_crossentropy\" and for Multi-class Classification we use \"categorical_crossentropy\".\n\n3) Finally,  We need to specify the metric to evaluate our models performance. Here I have used accuracy.","5a166043":"<a id=\"there_you_go_4.5\"><\/a>\n## 4.5) Data Augmentation","e45a710c":"Our model predicted 94% of Test Images correctly, which indicates that the model did pretty good job in generalizing the data.","ed90ae9c":"* **We can see that a large number of T-shirt  are misclassified as Shirt.**\n* **Followed by, Shirts wrongly classified as Coat.**","4f657432":"<a id=\"there_you_go_1.5\"><\/a>\n## 1.5) Examine NaN Values","7b115e8c":"* **We can see that all classes are equally Distributed.**\n* **So, there is no need for OverSampling or UnderSampling.**","3ee139a5":"<a id=\"there_you_go_7\"><\/a>\n# 7) Visualization of Predicted Classes","b99820a9":"**The Training and Validation Curves being close, we can conclude that the Model is not Overfitting the Data.**","c12acc7e":"<a id=\"there_you_go_4.6\"><\/a>\n## 4.6) Fitting the Model"}}