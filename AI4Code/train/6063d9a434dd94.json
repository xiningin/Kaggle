{"cell_type":{"70003608":"code","1faef88b":"code","3330c878":"code","daa62754":"code","58235481":"code","a10e8629":"code","fddc0037":"code","002637ab":"code","0d5c1295":"code","ee7d385f":"code","67deefdb":"code","793e4f54":"code","0459527f":"code","a71e5e1f":"code","70486d0d":"code","8aa7d242":"code","c4d87ef6":"code","7699fd44":"code","e2ebb1fc":"code","9c84cced":"markdown"},"source":{"70003608":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport statistics\nfrom statistics import median,mean\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport datetime\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn import decomposition\nfrom sklearn.manifold import TSNE\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.datasets import make_regression\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.datasets import make_classification\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier \nfrom sklearn.model_selection import train_test_split \nfrom sklearn import metrics \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn import tree\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nimport warnings\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.ensemble import VotingClassifier\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import ensemble\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.svm import SVC\n\n\n# import os\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# dirname = 'titanic'\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\nSurvived = pd.read_csv('\/kaggle\/input\/titanic\/gender_submission.csv')\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv').fillna('Unknown')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv').fillna('Unknown')\nSurvived.info(), train.info(), test.info()\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1faef88b":"drop_list=['PassengerId','Name','Cabin','Ticket']\n\nfor item in drop_list:\n    \n    train = train.drop(item, axis=1)\n    test = test.drop(item, axis=1)\n    \ntrain['Embarked'].value_counts(), test['Embarked'].value_counts()\n    ","3330c878":"#Replacing 'Unknown' with most common value in column 'Embarked' only in train as test has NO 'Unknown' value\na = train['Embarked'].value_counts()\nmost_common = a[0:1].index[0]\ntrain.loc[train.Embarked == 'Unknown','Embarked']= most_common\n","daa62754":"test['Age'].value_counts(), train['Age'].value_counts()","58235481":"#Replaceing 'Unknown' in the column 'Age' \n#train -  finding mean and replacing 'Unknown with mean'\nb = train['Age'].loc[train.Age != 'Unknown'].mean()\ntrain.loc[train.Age == 'Unknown','Age'] = round(b, 2)\n\n#test -  finding mean and replacing 'Unknown with mean'\nb = test['Age'].loc[test.Age != 'Unknown'].mean()\ntest.loc[test.Age == 'Unknown','Age'] = round(b, 2)\n\n\n#Replacing 'Unknown' with average value in column 'Fare' \n#train -  finding mean and replacing 'Unknown with average'\na = train['Fare'].loc[train.Fare != 'Unknown'].mean()\ntrain.loc[train.Fare == 'Unknown','Fare']= round(a,2)\n\n#test -  finding mean and replacing 'Unknown with average'\na = test['Fare'].loc[test.Fare != 'Unknown'].mean()\ntest.loc[test.Fare == 'Unknown','Fare']= round(a,2)\n","a10e8629":"c = train['SibSp'].value_counts()\nc1 = test['SibSp'].value_counts()\n\nd = train['Parch'].value_counts()\nd1 = test['Parch'].value_counts()\n\nc,c1,d,d1","fddc0037":"train1 = train.drop('Survived',axis = 1).copy()\ndata = pd.concat([train1,test])","002637ab":"plt.figure(figsize=(25,3))\nplt.grid(True)\nbp_pivot = data.groupby('Pclass').Sex.count().reset_index()\nsns.barplot(bp_pivot.Pclass, bp_pivot.Sex,)\nplt.title('Number of Passengers in each Class')\nplt.show()\n\nplt.figure(figsize=(25,3))\nplt.grid(True)\nsns.distplot(data[data.Sex == 'male']['Pclass'], color='blue', label='Male')\nsns.distplot(data[data.Sex == 'female']['Pclass'], color='red',  label='Female')\nplt.title('Gender representation in each Class')\nplt.legend()\nplt.show()","0d5c1295":"plt.figure(figsize=(25,3))\nplt.grid(True)\nbp_pivot = data.groupby('Embarked').Sex.count().reset_index()\nsns.barplot(bp_pivot.Embarked, bp_pivot.Sex,)\nplt.title('Number of Passengers by port boarding')\nplt.show()\n\nplt.figure(figsize=(25,3))\nplt.grid(True)\nbp_pivot = data.groupby('Sex').Embarked.count().reset_index()\nsns.barplot(bp_pivot.Sex, bp_pivot.Embarked)\nplt.title('Gender representation on the Titanic')\nplt.show()","ee7d385f":"plt.figure(figsize=(25,3))\nplt.grid(True)\nbp_pivot = data.groupby('Age').Sex.count().reset_index()\nsns.barplot(bp_pivot.Age, bp_pivot.Sex,)\nplt.title('Age of Passengers')\nplt.xticks(rotation='vertical')\nplt.show()\n\nplt.figure(figsize=(25,3))\nplt.grid(True)\nsns.distplot(data[data.Sex == 'male']['Age'], color='blue', label='Male')\nsns.distplot(data[data.Sex == 'female']['Age'], color='red',  label='Female')\nplt.title('Gender representation in each Age')\nplt.legend()\nplt.show()","67deefdb":"#changing variables to numerical values\n\n#SEX: female = 2, male = 1\ndata.loc[data.Sex == 'male','Sex']= 1\ndata.loc[data.Sex == 'female','Sex']= 2\n\n#Embarked: C = 1, Q = 2, S = 3\ndata.loc[data.Embarked == 'C', 'Embarked']= 1\ndata.loc[data.Embarked == 'Q', 'Embarked']= 2\ndata.loc[data.Embarked == 'S', 'Embarked']= 3\n\ndata.dtypes","793e4f54":"col_name=['Sex','Embarked']\nfor item in (col_name):\n    \n    data[item] = data[item].astype(str).astype(int)\n\ncol_name=['Age','Fare']\nfor item in (col_name):\n    \n    data[item] = data[item].astype(str).astype(float)\n    \ndata.dtypes    ","0459527f":"plt.figure(figsize=(25,8))\n\nheatmap = sns.heatmap(data.corr(),annot=True)\nheatmap.set_title('Correlation Heatmap', fontdict={'fontsize':12}, pad=12)","a71e5e1f":"#categories of variables:\ndep_var = ['Survived']\n\ncategorical_vars = ['Sex','Embarked']\n\ndiscrete_vars = ['Pclass','Age','SibSp','Parch']\n\ncontinuous_vars = ['Fare']\n\npipe = Pipeline([('scaler', StandardScaler()), ('svc', SVC())]) \n\ntrain_X = data\ntrain_y = train.Survived.copy()\ntest_y = Survived.Survived.copy()\ntrain_y = pd.concat([train_y,test_y])\nX_train, X_test, y_train, y_test = train_test_split(train_X,train_y, test_size = 418, random_state = 42)\n\n\npipe.fit(X_train, y_train)\nPipeline(steps=[('scaler', StandardScaler()), ('svc', SVC())])\nprint('Pipe Score is:',pipe.score(X_test, y_test))\n","70486d0d":"#Finding the most important features\ndata_columns=data[categorical_vars + discrete_vars + continuous_vars].columns\nfeature_importances = np.zeros(train_X.shape[1])\n\n# CHI2 model to use get features importance\ntest = SelectKBest(score_func=chi2, k=2)\nfit = test.fit(X_train, y_train)\n# Summarize scores\nnp.set_printoptions(precision=3)\nfeatures = fit.transform(X_train)\n# Record the feature importances\nfeature_importances += test.scores_\nfeature_importances = pd.DataFrame({'feature': (data_columns), 'Chi2': feature_importances})\n \n# LinearRegression model to get features importance\nmodel = LinearRegression() \nmodel.fit(X_train, y_train)\nimportance = model.coef_\nlinear_df=pd.DataFrame({'feature': (data_columns), 'Linear': model.coef_})\nfeature_importances= pd.concat([feature_importances,linear_df['Linear']], axis=1)\n\n#DecisionTreeRegressor model to get features importance\nmodel = DecisionTreeRegressor()\nmodel.fit(X_train, y_train)\nimportance = model.feature_importances_\nDecisionTree_df=pd.DataFrame({'feature': (data_columns), 'DecisionTree': model.feature_importances_})                          \nfeature_importances= pd.concat([feature_importances,DecisionTree_df['DecisionTree']], axis=1)\n\n#RandomForestRegressor model to get features importance\nmodel = RandomForestRegressor()\nmodel.fit(X_train, y_train)\nimportance = model.feature_importances_\nRandomForest_df=pd.DataFrame({'feature': (data_columns), 'RandomForest': model.feature_importances_})                          \nfeature_importances= pd.concat([feature_importances,RandomForest_df['RandomForest']], axis=1)\n\n#creatiung a new column 'Mean' and filling up with value for each feature\nfeature_importances.insert(5,\"Mean\",0.00)\n\nfeature_importances.set_index('feature')\nfeature_importances.groupby('feature')\n\nfor a in data_columns:\n    \n    feature_importances.loc[feature_importances.feature == a,'Mean'] = (round(feature_importances['Chi2'],4)+round(feature_importances['Linear'],4)+round(feature_importances['DecisionTree'],4)+round(feature_importances['RandomForest'],4))\/4\n                                                                    \n                                                                  \n\nfeature_importances=feature_importances.sort_index(axis=1,level='Mean',ascending=True).sort_values('Mean', ascending = False).copy()\nplt.figure(figsize=(15,5))\nplt.grid(True)\nsns.barplot(x= X_train.columns,y = feature_importances['Mean'])\nplt.xticks(rotation='vertical')\n\nplt.show() ","8aa7d242":"#RandomForest\nparam_grid1 = { 'n_estimators': [10, 20, 50],\n    'min_samples_split': [5, 10],\n    'min_samples_leaf': [5, 10],        \n    'max_depth' : [None, 1, 3, 5],\n    'criterion' :['gini', 'entropy']\n}\n#Ada \nparam_grid2 = {'n_estimators': [10, 20, 50],\n     'learning_rate' : [ 0.1, 0.2, 0.5]\n}\n#Extra Trees\nparam_grid3 = { 'n_estimators': [20, 50, 100],\n    'min_samples_split': [5, 10],\n    'min_samples_leaf': [5, 10],        \n    'max_depth' : [None, 1, 3, 5],\n    'criterion' :['gini', 'entropy']\n}\n\n#KN\nparam_grid4 = { 'n_neighbors': [10, 20, 50],\n     'algorithm':['auto', 'ball_tre', 'kd_tree', 'brute'],\n     'n_jobs' : [None, 1, 3, 5]\n}  \n\nparam_grid = [param_grid1,\n              param_grid2,\n              param_grid3,\n              param_grid4\n]\n\nclassifier_names = [RandomForestClassifier(),\n                    AdaBoostClassifier(),\n                    ExtraTreesClassifier(),\n                    KNeighborsClassifier()\n]                    \n\ni=1\nfor param_gr,clf_name in zip(param_grid,classifier_names):\n    print(i,clf_name)\n    modeli = GridSearchCV(clf_name,param_gr )\n    modeli.fit(X_train, y_train)\n    print('The best parameters:',modeli.best_params_)\n    i=i+1\n","c4d87ef6":"#RF_CL\nmodel1 = RandomForestClassifier(criterion = 'gini', max_depth = None, min_samples_leaf = 10, min_samples_split = 5, n_estimators = 10)\nmodel1.fit(X_train, y_train)\n\n\n#AdaBoost\nmodel2 = AdaBoostClassifier(learning_rate = 0.1, n_estimators = 10)\nmodel2.fit(X_train, y_train)\n\n\n#Extra Trees\nmodel3 = ExtraTreesClassifier(criterion = 'entropy', max_depth = None, min_samples_leaf = 10, min_samples_split = 10, n_estimators = 20 )\nmodel3.fit(X_train, y_train)\n\n\n#KNeighborsClassifier\nmodel4 = KNeighborsClassifier(n_neighbors = 20 ,algorithm = 'auto' ,n_jobs = None)\nmodel4.fit(X_train, y_train)\n\n\n\n#ensemble method - votig, I will use setting - SOFT \nvoting_model = VotingClassifier(estimators=[('RF', model1), ('ADA', model2), ('Extra', model3), ('KN', model4) ], voting='soft')\n\nvoting_model.fit(X_train, y_train)\n\nfor voting in (model1, model2, model3, model4, voting_model):\n    voting.fit(X_train, y_train)\n    y_pred = voting.predict(X_test);\n    print('Accuracy score for the Voting approach is:',voting.__class__.__name__, accuracy_score(y_test, y_pred));\n","7699fd44":"train_size=np.linspace(.1, 1.0, 15)\ndef Learning_curve_model(X, Y, model, Name):\n    train_sizes, train_scores, test_scores = learning_curve(model, X, Y, n_jobs=4)   \n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std  = np.std(train_scores, axis=1)\n    test_scores_mean  = np.mean(test_scores, axis=1)\n    test_scores_std   = np.std(test_scores, axis=1)\n    \n    plt.figure(figsize=(25,4))\n    plt.grid(True)\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",label=\"Cross-validation score\")\n    plt.title(Name)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    plt.legend(loc=\"best\")\n    plt.show()\n        \n#ploting the learning curve for each model\n\nfor i in range(4):\n        if i==0:\n            model = model1\n            Name = 'RandomForestClassifier'\n            Learning_curve_model(X_train, y_train, model,Name)\n            \n        elif i==1:\n            model = model2\n            Name = 'AdaBoostClassifier'\n            Learning_curve_model(X_train, y_train, model,Name)\n        elif i==2:\n            model = model3\n            Name = 'ExtraTreesClassifier'\n            Learning_curve_model(X_train, y_train, model,Name)\n        else:\n            model=model4\n            Name = 'KNeighborsClassifier'\n            Learning_curve_model(X_train, y_train, model, Name)\n               ","e2ebb1fc":"#stacking\n\ncombined_datatest = np.stack((model1.predict_proba(X_train)[:,1],\n                              model2.predict_proba(X_train)[:,1],\n                              model3.predict_proba(X_train)[:,1],\n                              model4.predict_proba(X_train)[:,1],\n         ),\n         axis=-1)\n\nclf_blender = LogisticRegression( solver='lbfgs',multi_class='multinomial').fit(combined_datatest, y_train)\n\n\ncombined_test_datatest = np.stack((model1.predict_proba(X_test)[:,1],\n                                   model2.predict_proba(X_test)[:,1],\n                                   model3.predict_proba(X_test)[:,1],\n                                   model4.predict_proba(X_test)[:,1],          \n         ),\n         axis=-1)\n\n\nprint(\"Stacking Approach Accuracy Score is: \", str(accuracy_score(clf_blender.predict(combined_test_datatest), y_test)))\n\n#Learning curve after usiong the Stacking approach\nmodel = LinearRegression()\nName = 'Stacking Approach'\nLearning_curve_model(X_train, y_train, model,Name)","9c84cced":"#### The Titanic was a British passenger liner, operated by the White Star Line, which sank in the North Atlantic Ocean on 15 April 1912 after striking an iceberg during her maiden voyage from Southampton, UK, to New York City. \n#### In my work I plot a couple figures, focused on gender representation in each class and how many passengers board in each port. Correlation between fields (variables) is significant for Pclass, Age and Fare. Later I used Chi square model, Linear Regression, Decision Tree Regression and Random Forest Regression to find the most important feature influencing survivor rate. It is Pclass.\n#### Finding the best parameters for classifier was the next task. I build ML by using 4 different classifiers and voting approach for ensemble. The learning curve for each show how model is fitting. AdaBoost and Extra Tree Classifiers are overfitting.\n#### Second approach I used was the stacking. Overfitting was again an issue.   "}}