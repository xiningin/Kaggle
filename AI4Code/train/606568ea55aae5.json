{"cell_type":{"dc419fc3":"code","70da29d5":"code","302916c0":"code","0e31121c":"code","779ea545":"code","673780b7":"code","6cf2506b":"code","806bf70c":"code","92ca107e":"code","7f771205":"code","edbb630c":"code","ce542e38":"code","503eb8b5":"code","bc581bfe":"code","d22bcd7a":"code","1d04a462":"code","b99b56d4":"code","9cc84d1f":"code","fb41c071":"code","f83a4027":"code","c8d61f6a":"code","2e667c5a":"code","1b0a3b86":"code","21a0e4fb":"code","de09e7ec":"code","fcf26da5":"code","cb3313d5":"code","c60fa798":"code","9a0fb324":"code","96ed3289":"code","a4b0037b":"code","cd174d84":"markdown","5b899cd4":"markdown","707d42b5":"markdown","c655e5ff":"markdown","1323c3fa":"markdown","54e069da":"markdown","47812618":"markdown","bd73bbb6":"markdown","9dd77a4c":"markdown","9cc9b683":"markdown","8761c1c4":"markdown","b696d147":"markdown","ec633868":"markdown","3846ad1c":"markdown","df671d27":"markdown","8d567352":"markdown","506ee9ef":"markdown","e423f309":"markdown","401f5eff":"markdown","c659f925":"markdown","975af959":"markdown","1a1b3db0":"markdown","006fb5ff":"markdown","4454afb6":"markdown","f91b76e8":"markdown"},"source":{"dc419fc3":"import numpy as np\nimport pandas as pd","70da29d5":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","302916c0":"test[\"casual\"] = np.nan\ntest[\"registered\"] = np.nan\ntest[\"count\"] = np.nan\n\ntrain[\"is_train\"] = 1\ntest[\"is_train\"] = 0","0e31121c":"full = pd.concat([train, test], axis = 0)","779ea545":"full[\"datetime\"] = full[\"datetime\"].astype('datetime64')\nfull[\"season\"] = full[\"season\"].astype('category')\nfull[\"holiday\"] = full[\"holiday\"].astype('bool')\nfull[\"workingday\"] = full[\"workingday\"].astype('bool')\nfull[\"weather\"] = full[\"weather\"].astype('category')\nfull[\"is_train\"] = full[\"is_train\"].astype('bool')","673780b7":"full[\"hour\"] = full[\"datetime\"].dt.hour.astype('category')\nfull[\"month\"] = full[\"datetime\"].dt.month.astype('category')\nfull[\"year\"] = full[\"datetime\"].dt.year.astype('category')\nfull = full.set_index('datetime')","6cf2506b":"full = full.drop([\"season\", \"atemp\", \"registered\", \"casual\"], axis = 1)","806bf70c":"factor_variables = [\"weather\", \"hour\", \"month\", \"year\"]\nfull_dummies = pd.get_dummies(full[factor_variables])","92ca107e":"non_factor_variables = [\"holiday\", \"workingday\", \"temp\", \"humidity\", \"windspeed\", \"count\", \"is_train\"]\nfull_no_dummies = full[non_factor_variables]","7f771205":"full = pd.concat([full_no_dummies, full_dummies], axis = 1)","edbb630c":"train = full[full[\"is_train\"] == 1]\ntest = full[full[\"is_train\"] == 0]","ce542e38":"X_train = train.drop([\"is_train\", \"count\", \"year_2012\"], axis = 1)\ny_train = train[\"count\"]\n\nX_test = test.drop([\"is_train\", \"count\", \"year_2012\"], axis = 1)","503eb8b5":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import GridSearchCV","bc581bfe":"rfr = RandomForestRegressor()","d22bcd7a":"param_grid = [\n  {'max_depth': [20, 30, 40, 50],\n   'min_samples_leaf': [1, 5, 10, 20],\n   'n_estimators':[30, 50, 70, 90],\n   'random_state':[42]}]","1d04a462":"rfr = GridSearchCV(rfr, param_grid, cv = 5, scoring = 'neg_mean_squared_log_error')\nrfr.fit(X_train, y_train)","b99b56d4":"rfr.best_score_","9cc84d1f":"rfr.best_params_","fb41c071":"rfr = RandomForestRegressor()\nparam_grid = [\n  {'max_depth': [25, 30, 35],\n   'min_samples_leaf': [1, 3, 5],\n   'n_estimators':[40, 50, 60],\n   'random_state':[42]}]\nrfr = GridSearchCV(rfr, param_grid, cv = 5, scoring = 'neg_mean_squared_log_error')\nrfr.fit(X_train, y_train)","f83a4027":"rfr.best_score_","c8d61f6a":"rfr.best_params_","2e667c5a":"rfr = RandomForestRegressor()\nparam_grid = [\n  {'max_depth': [28, 30, 32],\n   'min_samples_leaf': [1, 2],\n   'n_estimators':[55, 60, 65],\n   'random_state':[42]}]\nrfr = GridSearchCV(rfr, param_grid, cv = 5, scoring = 'neg_mean_squared_log_error')\nrfr.fit(X_train, y_train)","1b0a3b86":"rfr.best_score_","21a0e4fb":"rfr.best_params_","de09e7ec":"rfr = RandomForestRegressor()\nparam_grid = [\n  {'max_depth': [29, 30, 31],\n   'min_samples_leaf': [1],\n   'n_estimators':[50, 55, 60],\n   'random_state':[42]}]\nrfr = GridSearchCV(rfr, param_grid, cv = 5, scoring = 'neg_mean_squared_log_error')\nrfr.fit(X_train, y_train)","fcf26da5":"rfr.best_score_","cb3313d5":"rfr.best_params_","c60fa798":"rfr = RandomForestRegressor()\nparam_grid = [\n  {'max_depth': [30],\n   'min_samples_leaf': [1],\n   'n_estimators':[52, 53, 54, 55, 56, 57, 58],\n   'random_state':[42]}]\nrfr = GridSearchCV(rfr, param_grid, cv = 5, scoring = 'neg_mean_squared_log_error')\nrfr.fit(X_train, y_train)","9a0fb324":"rfr.best_score_","96ed3289":"rfr.best_params_","a4b0037b":"prediction = rfr.predict(X_test)\nresult = pd.DataFrame(test.index).assign(count = prediction)\nresult.to_csv('output_pred.csv', index=False)","cd174d84":"**Introduction**\n\nIn my [EDA](https:\/\/www.kaggle.com\/franzxschmid\/eda-for-beginners-bike-riding-data-set) for this dataset it was shown, that the bike-sharing-demand is characterized by knotty conditional and dependent relationships. \nIn such cases you shouldn't predict only with simple regressions. They can't explain that \"knotty\" part.\nYou need a tree, better yet many trees or a forest. A relatively simple and effective technique for such cases is Random Forest Regression.","5b899cd4":"Conclusion\n* the \"best\" maximal depth is 30 \n* we can only refine our hyperparameters if we find a better number of estimators than 55","707d42b5":"Adjust the train and the testset, in order to fit them together","c655e5ff":"Now we conclude\n* the \"best\" maximal depth is very close to 30\n* the \"best\" minimal samples leaf can only be 1 or 2\n* the \"ideal\" number of estimators seems to be closer to 60 and not to 50","1323c3fa":"Get dummies for the factor variables","54e069da":"Correct the datatypes","47812618":"****Random Forest Regression and Hyperparameter-Tuning****\n\nThe hyperparameter-tuning will be down by hand in the following.  We can't test all combinations of parameters for computational reasons, therefore we are creating a rough parameter-grid and have a look in which area we find the \"best\" parameters. Than we investigate this area with a smaller but finer grid. We are doing this several times iteratively.","bd73bbb6":"Fit the Random Forest Regressor with a cross validated gridsearch","9dd77a4c":"Create a first rough hyperparameter-grid","9cc9b683":"Feature selection\n* seasonal effects are already regarded through the variable \"month\"\n* \"atemp\" is highly correlated with \"temp\"\n* \"registered\" and \"casual\" are dependant variables and therefore not available in the  testdata","8761c1c4":"Load dataset","b696d147":"It's still the same score because 55 was already the \"best\" number of estimators","ec633868":"Split into train and testset","3846ad1c":"**Data Preprocessing**","df671d27":"Get the variables \"hour\", \"month\" and \"year\" from the variable datetime","8d567352":"The score gets a bit better","506ee9ef":"Print the best score","e423f309":"The score and the \"best\" parameters are the same as before","401f5eff":"Make a rowbind so that it's possible to clean the whole data at once","c659f925":"Now we investigate, if there are even better parameters near the values we got from the last output","975af959":"From the best parameters we can conclude that\n* the \"best\" maximal depth is near 30\n* the \"best\" minimal samples leaf is beetween 1 and 5\n* the \"ideal\" number of estimators is near 50","1a1b3db0":"The score still gets a bit better","006fb5ff":"We conclude\n* the \"best\" maximal depth can only be 29, 30 or 31\n* the \"best\" minimal samples leaf is 1 or \n* the \"ideal\" number of estimators is close to 55","4454afb6":"Load libraries","f91b76e8":"Instantiate a Random Forest Regressor"}}