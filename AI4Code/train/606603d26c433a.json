{"cell_type":{"1d6f35e1":"code","f5645f1f":"code","fa9786fe":"code","e103c428":"code","6b8d539d":"code","fe204b05":"code","c6ebc7f0":"code","84f0b19d":"code","3203dc5b":"code","4c3ba5d9":"code","a29072fb":"code","0370b5e2":"code","440d81ba":"code","9f8e3c41":"code","733c8d7e":"code","c8cf042d":"code","95a3c1fe":"code","a96b830f":"code","dd6b716d":"code","b94dda09":"markdown","c5d64057":"markdown","d29228ef":"markdown"},"source":{"1d6f35e1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn import model_selection, preprocessing, metrics\n\nfrom sklearn import preprocessing\nimport gc\n\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#    for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f5645f1f":"fnc_df = pd.read_csv(\"..\/input\/trends-assessment-prediction\/fnc.csv\")\nloading_df = pd.read_csv(\"..\/input\/trends-assessment-prediction\/loading.csv\")\n\nfnc_features, loading_features = list(fnc_df.columns[1:]), list(loading_df.columns[1:])\ndf = fnc_df.merge(loading_df, on=\"Id\")","fa9786fe":"df.head()","e103c428":"df.shape","6b8d539d":"labels_df = pd.read_csv(\"..\/input\/trends-assessment-prediction\/train_scores.csv\")\nreveal_ID_site2 = pd.read_csv(\"..\/input\/trends-assessment-prediction\/reveal_ID_site2.csv\")\nlabels_df.head()","fe204b05":"labels_df.shape","c6ebc7f0":"reveal_ID_site2.head()","84f0b19d":"reveal_ID_site2.shape","3203dc5b":"labels_df.shape","4c3ba5d9":"df['target'] = np.nan","a29072fb":"df.isna().sum().sum()","0370b5e2":"df.loc[df.Id.isin(labels_df.Id), 'target'] = 0\ndf.loc[df.Id.isin(reveal_ID_site2.Id), 'target'] = 1\ndf.dropna(inplace=True)\n\ndf.shape","440d81ba":"features = df.columns[1:-1]\ntrain = df[features].values\ntarget = df['target'].values","9f8e3c41":"train, test, y_train, y_test = model_selection.train_test_split(train, target, test_size=0.33, random_state=42, shuffle=True)\ndel target\ngc.collect()","733c8d7e":"train = lgb.Dataset(train, label=y_train)\ntest = lgb.Dataset(test, label=y_test)","c8cf042d":"param = {'num_leaves': 50,\n         'min_data_in_leaf': 30, \n         'objective':'binary',\n         'max_depth': 5,\n         'learning_rate': 0.05,\n         \"min_child_samples\": 20,\n         \"boosting\": \"gbdt\",\n         \"feature_fraction\": 0.9,\n         \"bagging_freq\": 1,\n         \"bagging_fraction\": 0.9 ,\n         \"bagging_seed\": 56,\n         \"metric\": 'auc',\n         \"verbosity\": -1}","95a3c1fe":"num_round = 10000\nclf = lgb.train(param, train, num_round, valid_sets = [train, test], verbose_eval=50, early_stopping_rounds = 100)","a96b830f":"feature_imp = pd.DataFrame(sorted(zip(clf.feature_importance(),features)), columns=['Value','Feature'])\n\nplt.figure(figsize=(20, 20))\nsns.barplot(x=\"Value\", y=\"Feature\", data=feature_imp.sort_values(by=\"Value\", ascending=False).head(100))\nplt.title('LightGBM Features')\nplt.tight_layout()\nplt.show()\nplt.savefig('lgbm_importances-01.png')","dd6b716d":"feature_imp.sort_values(by=\"Value\", ascending=False).head(20)","b94dda09":"Here it would appear that the worst \"culprits\" are the IC features. Let's again list the top 20 most important ones, at least according to this measure.","c5d64057":"In this adversarial validation notebook we'll take a look at the difference between sites 1 and 2.","d29228ef":"The AUC of 0.88 is much higher than the AUC of 0.72 for the \"regular\" train-test adversarial validation. Seems taht we really ahve to be very careful about the differences between sites 1 and 2.\n\nLet us now take a look at the most important features "}}