{"cell_type":{"84a7ca84":"code","4fcd348c":"code","a28e683e":"code","9bfe0359":"code","973f19ff":"code","308e2ae6":"code","462a06cc":"code","cec830e3":"code","b2d19ab6":"code","f4f046e6":"code","6ba671d3":"code","f5fa6790":"code","45979e52":"markdown","73418e04":"markdown","b958b6a7":"markdown","81f19bb9":"markdown","49cf1c8a":"markdown","591e9d56":"markdown"},"source":{"84a7ca84":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os,gc\nprint(os.listdir(\"..\/input\"))\n\n\n\nfrom keras.optimizers import RMSprop, Adam\nfrom keras.models import Model, Sequential\nfrom keras.layers import Flatten, Dense, Embedding, Dropout\nfrom keras.callbacks import EarlyStopping\nfrom keras import losses, metrics, optimizers\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\n\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\n\npd.options.display.max_colwidth = 500\n# Any results you write to the current directory are saved as output.","4fcd348c":"train = pd.read_csv('..\/input\/train.csv')\ntest  = pd.read_csv('..\/input\/test.csv')\nprint('Train Shape{} Test Shape{}'.format(train.shape, test.shape))\ntrain.head()","a28e683e":"def clean_special_chars(text, punct, mapping):\n        for p in mapping:\n            text = text.replace(p, mapping[p])    \n        for p in punct:\n            text = text.replace(p, f' {p} ')     \n        return text\n\ndef clean_text(df):\n    df['comment_text'] = df['comment_text'].apply(lambda x: x.lower())\n    punct_mapping = {\"_\":\" \", \"`\":\" \"}\n    punct = \"\/-'?!.,#$%\\'()*+-\/:;<=>@[\\\\]^_`{|}~\" + '\"\"\u201c\u201d\u2019' + '\u221e\u03b8\u00f7\u03b1\u2022\u00e0\u2212\u03b2\u2205\u00b3\u03c0\u2018\u20b9\u00b4\u00b0\u00a3\u20ac\\\u00d7\u2122\u221a\u00b2\u2014\u2013&'\n    df['comment_text'] = df['comment_text'].apply(lambda x: clean_special_chars(x, punct, punct_mapping))\n    return df","9bfe0359":"train = clean_text(train)\ntest = clean_text(test)\ntrain.head() ","973f19ff":"\nmax_words = 20000\nmaxlen = 220\n\n\ntokenizer = Tokenizer(num_words = max_words, lower = True)\ntokenizer.fit_on_texts(list(train['comment_text']) + list(test['comment_text']))\nword_index = tokenizer.word_index\n\ny = train.target.apply(lambda x: 0 if x < 0.5 else 1)\n\nX = tokenizer.texts_to_sequences(list(train['comment_text']))\nX_test =  tokenizer.texts_to_sequences(list(test['comment_text']) )  \n                                       \nX = pad_sequences(X, maxlen= maxlen)   \nX_test =  pad_sequences(X_test, maxlen= maxlen)      \n\n","308e2ae6":"X_train, X_valid, y_train, y_valid = train_test_split(X, y, test_size = 0.2,  random_state = 42,stratify = y )\ndel(train, test, X,y)\ngc.collect()","462a06cc":"from keras.optimizers import RMSprop, Adam\nfrom keras.layers import Embedding, Input, Dense, CuDNNGRU,concatenate, Bidirectional, SpatialDropout1D, Conv1D, GlobalAveragePooling1D, GlobalMaxPooling1D\n#Model Architecture from https:\/\/www.kaggle.com\/taindow\/simple-cudnngru-python-keras\nembedding_dim = 30\n\ndef get_model():\n    sequence_input = Input(shape=(maxlen,), dtype='int32')\n    embedding_layer = Embedding(len(tokenizer.word_index) + 1,\n                                embedding_dim,                          \n                                input_length=maxlen)\n\n    x = embedding_layer(sequence_input)\n    x = SpatialDropout1D(0.2)(x)\n    x = Bidirectional(CuDNNGRU(64, return_sequences=True))(x)   \n    x = Conv1D(64, kernel_size = 2, padding = \"valid\", kernel_initializer = \"he_uniform\")(x)\n\n    avg_pool1 = GlobalAveragePooling1D()(x)\n    max_pool1 = GlobalMaxPooling1D()(x)     \n\n    x = concatenate([avg_pool1, max_pool1])\n\n    preds = Dense(1, activation='sigmoid')(x)\n\n\n    model = Model(sequence_input, preds)\n\n    model.compile(loss='binary_crossentropy',\n                  optimizer=Adam(),\n                  metrics=['acc'])\n    return model\n","cec830e3":"model = get_model()\nprint(model.summary())\nEPOCHS = 15\nBATCH_SIZE = 512\nprint('Train Size{}, Validation Size {}, Test Size {}'.format(X_train.shape, X_valid.shape, X_test.shape))\nprint('Max Words {}, Max Length {}, Embedding Dimesions {}'.format(max_words, maxlen, embedding_dim))\nhistory = model.fit( X_train,\n                     y_train,\n                    epochs = EPOCHS,\n                    batch_size = BATCH_SIZE,\n                    callbacks = [EarlyStopping(monitor = 'val_acc', patience = 3)],\n                    validation_data = (X_valid, y_valid)\n                   )","b2d19ab6":"\ny_pred = model.predict(X_valid)\nprint('Validation ROC AUC Score', roc_auc_score(y_valid, y_pred))","f4f046e6":"history_dict = history.history\nvalid_acc = history_dict['val_acc'] \nbest_epoch = valid_acc.index(max(valid_acc)) + 1\nbest_acc =  max(valid_acc)\nprint('Best Accuracy Score {}, is for epoch {}'.format( best_acc, best_epoch))\n\nmodel = get_model()\nhistory = model.fit( X_train,\n                     y_train,\n                    epochs = best_epoch,\n                    batch_size = BATCH_SIZE,\n                    validation_data = (X_valid, y_valid)\n                   )","6ba671d3":"\ny_pred = model.predict(X_valid)\nprint('Validation ROC AUC Score', roc_auc_score(y_valid, y_pred))","f5fa6790":"y_pred = model.predict(X_test)\nsub = pd.read_csv('..\/input\/sample_submission.csv')\nsub['prediction'] = y_pred\nsub.to_csv('submission.csv', index = False)\n\nsub.head()","45979e52":"# Clean Data\nAdding preprocessing from this kernel: https:\/\/www.kaggle.com\/taindow\/simple-cudnngru-python-keras\n> ","73418e04":"## Define and Train Model","b958b6a7":" ### Predict On Best Epoch","81f19bb9":"### Read Data","49cf1c8a":"### Predict On Test data","591e9d56":"This Notebook will train its own word emebeddings instead of using pr-trained word embeddings\n\n<br>V1: Initial version with Embedding layer without pre-trained word emebeddings: Public Score:0.85842\n<br>V2: Apply Text Cleaning by cleaning the puntuations and converting text to lower case:Public Score:0.86376\n<br>V3: Chnage the max_words to 20K and Embedding Dimensions to 100 : Public Score 0.84811\n<br>V5: Use CUDNNGRU Model Architecture:"}}