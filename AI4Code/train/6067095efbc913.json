{"cell_type":{"2b491454":"code","7fe59954":"code","9816790a":"code","cbc26124":"code","11a06a00":"code","81711ef2":"code","a83fad53":"code","866a6059":"code","4dc592ae":"code","f3f9d6f3":"code","afdaaa77":"code","cc00a22f":"code","ce486fe5":"code","3e4b31a9":"code","c5092611":"code","a60c332a":"code","d865f193":"code","bd4a961a":"code","9d6775e8":"code","b33fb33d":"code","a4bf5525":"code","85936d5f":"code","1feb66b0":"code","01406f20":"code","d970a8ce":"code","d2655453":"code","90064cb5":"code","7f73cdea":"code","39c9f63d":"code","fc24adea":"code","36edfbb2":"code","cadc1355":"code","9faabc84":"code","ef453145":"code","ed609ec0":"code","76d350f0":"code","939575cd":"code","79050eac":"code","593867bd":"code","a2f55f18":"code","75190848":"code","51a8ddd6":"code","32809720":"code","e72cc338":"code","d2c9c383":"code","9fdbc48b":"code","98680f75":"code","30cf7785":"code","e8e78129":"code","b9f46589":"code","dd645ec8":"code","f8a6273c":"code","b024426a":"code","451aef09":"code","ed4b3186":"code","467f037d":"code","8784dda4":"code","0bf36747":"code","4aced1bd":"code","5a8454f6":"code","2303165c":"code","fffcd792":"code","1d1e965b":"code","647342b1":"code","7f9031d2":"code","316d7572":"code","64b49574":"code","ca3a0e44":"code","4f90fe01":"code","ae0f32b4":"code","67487cd7":"code","a3491e6c":"code","3022a7c5":"code","ce76a902":"code","0a70f934":"code","d0f2ced3":"code","d2927d02":"code","a3591165":"code","70d4af6c":"code","1d4ecddb":"code","b6f26ca9":"code","f5b52088":"code","e587ac8a":"code","501fbbdd":"code","4383225d":"code","eb0bd9e6":"code","9c4db80a":"code","71bee467":"code","52a69e2d":"code","b91f8d92":"code","1d755d4a":"code","b619fa49":"code","d34bcd86":"code","403a947e":"code","042fb376":"code","d01fef70":"code","b33861fc":"code","3ee8a163":"code","ae879d3b":"code","006d9044":"code","c4bc1542":"code","13ee287f":"code","8a7c64be":"code","9cfa8b04":"code","b0ae82e5":"code","a96e9ab0":"code","34dd3c3a":"code","0d7c0fde":"code","14c85bf3":"code","34482edd":"code","e48d007b":"code","f6368107":"code","62639100":"code","123cfd3e":"code","56eafcc6":"code","a4f0b7b0":"code","51fb2ad9":"code","7a6ba458":"code","efc54b1d":"code","7acc65e3":"code","1bdc69d8":"code","df11bdd1":"code","aaccecf6":"code","1a29d0e7":"code","754815a2":"code","c1de7cef":"code","910248dc":"code","11babd2d":"code","f7d9aa02":"code","02a5198e":"code","10ee4376":"code","7f6ec6c6":"code","583509be":"code","235a1617":"code","5061e70d":"code","19a79746":"code","508478a9":"code","4752f409":"code","33b31703":"code","22505d81":"code","169aa15c":"code","e0e2538d":"code","195f41f7":"code","2c9ee617":"code","d89ea580":"code","2f57951d":"code","70f0d153":"code","f875338c":"code","8592f9c8":"code","a6545a63":"code","9519ffd4":"code","35373c26":"code","63f12f41":"code","9d58e3ab":"code","57cd5610":"code","228f9b60":"code","711d64c9":"code","ac16b1b7":"code","1a072ba1":"code","fc1f4028":"code","6ae4a10c":"code","a29acd7e":"code","82a7a8f7":"code","f63d8847":"code","8be8ac41":"code","8a7c8c0d":"code","b9a82879":"code","0caf6e5c":"code","d6ba5f44":"code","3b2a23f1":"code","813f4962":"code","8c39d681":"code","9ca94c57":"code","36bf57d5":"code","327da835":"code","49532cd6":"code","895f7321":"code","74fe476e":"code","89e12717":"code","e78983cc":"code","3fb223c8":"code","001727bc":"code","2f7d4276":"code","de94ceeb":"code","2529378d":"code","e807f578":"code","eb6b5132":"code","d9ff0042":"code","4eed788e":"code","f6e9d5de":"code","d95a222f":"code","1c852dc4":"code","ffdb257a":"code","281e0541":"code","d8c79b50":"code","5c3dccb1":"code","ba52ded5":"code","613249d6":"code","e01ec385":"code","c4ab22f0":"code","a66cc42b":"code","57fc3d16":"code","f4862e0e":"markdown","fe8440bf":"markdown","c1c4d8e8":"markdown","6dcfd4c6":"markdown","44de9241":"markdown","0a2c93ee":"markdown","3c9058ad":"markdown","015a1125":"markdown","b660b20f":"markdown","27de36ed":"markdown","3768c9c6":"markdown","fa7fc1ca":"markdown","3a22f182":"markdown","8952a6b4":"markdown","86594739":"markdown","1dde167c":"markdown","f8824640":"markdown","42c73337":"markdown","076d6e95":"markdown","c2f88f7f":"markdown","fb1f60f6":"markdown","cda2b5b2":"markdown","ef8ec847":"markdown","12d0c46c":"markdown","fc453fe2":"markdown","a15a36d9":"markdown","4c7a15cd":"markdown","c77e21d6":"markdown","f40e2c57":"markdown","37b010fe":"markdown","063b2338":"markdown","d4efe944":"markdown","cf0678e8":"markdown","a25921af":"markdown","1bb2f791":"markdown","d94d4ed3":"markdown","e5f5c793":"markdown","ec66b7d9":"markdown","e59d9ded":"markdown","015ce658":"markdown","a6926cc3":"markdown","1dde7799":"markdown","dca83aa0":"markdown","ce57e46b":"markdown"},"source":{"2b491454":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom scipy import stats\nwarnings.filterwarnings(\"ignore\")\n%matplotlib inline  \nfrom sklearn import datasets, linear_model\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7fe59954":"train=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest=pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","9816790a":"train.head()","cbc26124":"train.set_index('Id',inplace=True)","11a06a00":"test.head()","81711ef2":"test.set_index('Id',inplace=True)","a83fad53":"sns.barplot(x='OverallCond',y='SalePrice',data=train);","866a6059":"sns.boxplot(x='MSSubClass',y='SalePrice',data=train,palette='rainbow');","4dc592ae":"sns.boxplot(x='OverallQual',y='SalePrice',data=train,palette='rainbow');","f3f9d6f3":"#So I will convert Numeric to Object only MSSubClass\ntrain['MSSubClass'] = train['MSSubClass'].astype(str)\ntest['MSSubClass'] = test['MSSubClass'].astype(str)","afdaaa77":"#Impute by None where NA (We can get this information from the txt file.)\nimpute_by_none = train.loc[:, ['Alley','BsmtQual','BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n                                  'FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PoolQC',\n                                  'Fence','MiscFeature','MasVnrType']]\nfor i in impute_by_none.columns:\n    train[i].fillna('None', inplace = True)","cc00a22f":"#Impute by None where NA (We can get this information from the txt file.)\nimpute_by_none = test.loc[:, ['Alley','BsmtQual','BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2',\n                                  'FireplaceQu','GarageType','GarageFinish','GarageQual','GarageCond','PoolQC',\n                                  'Fence','MiscFeature','MasVnrType']]\nfor i in impute_by_none.columns:\n    test[i].fillna('None', inplace = True)","ce486fe5":"#Sorting merged Dataset According to Year and Month\ntrain = train.sort_values(['YrSold', 'MoSold'], ascending = [True, True])\ntrain.head()","3e4b31a9":"train['BsmtBath']=train['BsmtFullBath']+train['BsmtHalfBath']\ntrain['AboveBath']=train['FullBath']+train['HalfBath']\n\ntest['BsmtBath']=test['BsmtFullBath']+test['BsmtHalfBath']\ntest['AboveBath']=test['FullBath']+test['HalfBath']","c5092611":"sns.barplot(x='BsmtBath',y='SalePrice',data=train);","a60c332a":"sns.barplot(x='AboveBath',y='SalePrice',data=train);","d865f193":"#Drop BsmtFullBath,BsmtHalfBath, FullBath, HalfBath column for train and test dataset\ntrain.drop(['BsmtFullBath','BsmtHalfBath', 'FullBath', 'HalfBath'], axis=1, inplace=True)\ntest.drop(['BsmtFullBath','BsmtHalfBath', 'FullBath', 'HalfBath'], axis=1, inplace=True)","bd4a961a":"#Adding new column according to relationsheep between years\ntrain['Built-Sold']=train['YrSold']-train['YearBuilt']\ntrain['Built-Remodel']=train['YearRemodAdd']-train['YearBuilt']\ntrain['Built-GarageBuilt']=train['GarageYrBlt']-train['YearBuilt']\ntrain['Remodel-Sold']=train['YrSold']-train['YearRemodAdd']\ntrain['GarageBuilt-Sold']=train['YrSold']-train['GarageYrBlt']\ntrain['Remodel-GarageBuilt']=train['YearRemodAdd']-train['GarageYrBlt']","9d6775e8":"#Adding new column according to relationsheep between years\ntest['Built-Sold']=test['YrSold']-test['YearBuilt']\ntest['Built-Remodel']=test['YearRemodAdd']-test['YearBuilt']\ntest['Built-GarageBuilt']=test['GarageYrBlt']-test['YearBuilt']\ntest['Remodel-Sold']=test['YrSold']-test['YearRemodAdd']\ntest['GarageBuilt-Sold']=test['YrSold']-test['GarageYrBlt']\ntest['Remodel-GarageBuilt']=test['YearRemodAdd']-test['GarageYrBlt']","b33fb33d":"train[['YrSold','YearBuilt','YearRemodAdd','GarageYrBlt','Built-Sold','Built-Remodel','Built-GarageBuilt','Remodel-Sold',\n       'GarageBuilt-Sold','Remodel-GarageBuilt']].sort_values(ascending=True ,by=['Remodel-GarageBuilt']).head()\n#there are negative values here. I'm going to set these equal to 0.","a4bf5525":"train.loc[train['Built-GarageBuilt'] <= 0, 'Built-GarageBuilt'] = 0\ntrain.loc[train['Remodel-Sold'] <= 0, 'Remodel-Sold'] = 0\ntrain.loc[train['Remodel-GarageBuilt'] <= 0, 'Remodel-GarageBuilt'] = 0","85936d5f":"test.loc[test['Built-Sold'] <= 0, 'Built-Sold'] = 0\ntest.loc[test['Built-Remodel'] <= 0, 'Built-Remodel'] = 0\ntest.loc[test['Built-GarageBuilt'] <= 0, 'Built-GarageBuilt'] = 0\ntest.loc[test['Remodel-Sold'] <= 0, 'Remodel-Sold'] = 0\ntest.loc[test['GarageBuilt-Sold'] <= 0, 'GarageBuilt-Sold'] = 0\ntest.loc[test['Remodel-GarageBuilt'] <= 0, 'Remodel-GarageBuilt'] = 0","1feb66b0":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']","01406f20":"#Check the values less than 0\ntrain.select_dtypes(include=numerics).where(train.select_dtypes(include=numerics)<0).sum()","d970a8ce":"#Check the values less than 0\ntest.select_dtypes(include=numerics).where(test.select_dtypes(include=numerics)<0).sum()","d2655453":"train.drop(['YrSold','YearBuilt','YearRemodAdd','GarageYrBlt'], axis=1, inplace=True)\ntest.drop(['YrSold','YearBuilt','YearRemodAdd','GarageYrBlt'], axis=1, inplace=True)","90064cb5":"#To See Relationship between NaN values\n\nf,ax=plt.subplots(figsize=(20,10))\nsns.heatmap(train.isnull(), cbar=False, ax=ax, cmap=\"Blues\");","7f73cdea":"import missingno as msno\nmsno.heatmap(train,figsize=(10,5))  ; #Missing value is correlation between some columns. So \u0131t could be better drop this column.","39c9f63d":"#Target Value Distribution\nplt.subplots(figsize=(12, 9))\nsns.distplot(train['SalePrice'], fit = stats.norm)\n\n(mu, sigma) = stats.norm.fit(train['SalePrice'])\n\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma = $ {: .2f})'.format(mu, sigma)], loc = 'best')\nplt.ylabel('Frekans')\n\n#Probability Plot\nfig = plt.figure()\nstats.probplot(train['SalePrice'], plot = plt)\nplt.show()","fc24adea":"train[\"SalePrice\"] = np.log1p(train.SalePrice)\ntrain[\"LotArea\"] = np.log1p(train.LotArea)\ntest[\"LotArea\"] = np.log1p(test.LotArea)","36edfbb2":"# After log-transform\nplt.subplots(figsize=(12, 9))\nsns.distplot(train['SalePrice'], fit = stats.norm)\n\n(mu, sigma) = stats.norm.fit(train['SalePrice'])\n\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma = $ {: .2f})'.format(mu, sigma)], loc = 'best')\nplt.ylabel('Frekans')\n\n#Probability Plot\nfig = plt.figure()\nstats.probplot(train['SalePrice'], plot = plt)\nplt.show()","cadc1355":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ncategorics = ['object' , 'category']\ndatetimes = ['datetime64[ns]']","9faabc84":"numericalColumns = train.select_dtypes(include=numerics)\ncategoricalColumns = train.select_dtypes(include=categorics)\ndateColumns = train.select_dtypes(include=datetimes)","ef453145":"\"\"\"#This method normalize all the columns to [0,1], and NaN remains being NaN\ndef norm_to_zero_one(df):\n    return (df - df.min()) * 1.0 \/ (df.max() - df.min())\"\"\"","ed609ec0":"\"\"\"numericalColumns = numericalColumns.apply(norm_to_zero_one)\"\"\"","76d350f0":"\"\"\"train = pd.concat([numericalColumns, categoricalColumns], axis=1)\ntrain.head()\"\"\"","939575cd":"\"\"\"numericalColumns_test = test.select_dtypes(include=numerics)\ncategoricalColumns_test = test.select_dtypes(include=categorics)\ndateColumns_test = test.select_dtypes(include=datetimes)\"\"\"","79050eac":"\"\"\"numericalColumns_test = numericalColumns_test.apply(norm_to_zero_one)\"\"\"","593867bd":"\"\"\"test = pd.concat([numericalColumns_test, categoricalColumns_test], axis=1)\ntest.head()\"\"\"","a2f55f18":"a=numericalColumns.columns","75190848":"def ZeroVarianceFinder(numericalColumns, a):\n    import pandas as pd\n    import numpy as np\n    \n    zerovariance_numerical_features=[]\n    for col in numericalColumns.columns:\n        try:\n            if pd.DataFrame(numericalColumns[col]).describe().loc['std'][0] == 0.00 or \\\n            np.isnan(pd.DataFrame(numericalColumns[col]).describe().loc['std'][0]):\n                zerovariance_numerical_features.append(col)\n        except:\n            print(\"Error:\",col)\n    return zerovariance_numerical_features","51a8ddd6":"zerovariance_train_numerical_features = ZeroVarianceFinder(numericalColumns,a) # There is no zero variance in features..\nzerovariance_train_numerical_features","32809720":"singleton_categorical_features=[]\nfor col in categoricalColumns.columns:\n    if len(categoricalColumns[col].unique()) <= 1:\n        singleton_categorical_features.append(col)\nlen(singleton_categorical_features), singleton_categorical_features","e72cc338":"#High Correlation between features\ncorr_matrix = train.corr().abs()\nhigh_corr_var=np.where(corr_matrix>0.8)\nhigh_corr_var=[(corr_matrix.columns[x],corr_matrix.columns[y]) for x,y in zip(*high_corr_var) if x!=y and x<y]","d2c9c383":"high_corr_var","9fdbc48b":"corr = train.corr()\nk = 9 #number of variables for heatmap\ncols = corr.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train[cols].values.T)\nsns.set(font_scale=1.25)\nfig, ax = plt.subplots(figsize=(10,10))       \nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10},\n                 yticklabels=cols.values, xticklabels=cols.values,cmap='RdYlGn')\nplt.show()","98680f75":"train.drop(['Built-Sold', '1stFlrSF', 'TotRmsAbvGrd', 'GarageArea'], axis=1, inplace=True)","30cf7785":"test.drop(['Built-Sold', '1stFlrSF', 'TotRmsAbvGrd', 'GarageArea'], axis=1, inplace=True)","e8e78129":"def dataframeInformations(train):\n    totalEntryList=[]\n    totalMissingValueList=[]\n    missingValRateList=[]\n    dataTypeList=[]\n    uniqueValuesList=[]\n    totalUniqueValList=[]\n    variableNameList=[]\n    \n    for element in train.columns:\n        missingValRate=round((train[element].isna().sum()\/len(train[element]))*100,2) #to show correct decimal and float part of number.\n        totalEntryList.append(len(train[element]))\n        totalMissingValueList.append(train[element].isna().sum())\n        missingValRateList.append(missingValRate)\n        dataTypeList.append(train[element].dtype)\n        uniqueValuesList.append(list(train[element].unique()))\n        totalUniqueValList.append(len(train[element].unique()))\n        variableNameList.append(element)\n    #create a dataframe to show all informations together\n    dataInfoDf=pd.DataFrame({'Variable':variableNameList,'#_Total_Entry':totalEntryList,\\\n                           '#_Missing_Value':totalMissingValueList,'%_Missing_Value':missingValRateList,\\\n                           'Data_Type':dataTypeList,'Unique_Values':uniqueValuesList,\\\n                           '#_Uniques_Values':totalUniqueValList})\n    return dataInfoDf.sort_values(by=\"Variable\")","b9f46589":"dataInfo=dataframeInformations(train)\nvariableList=[element for element in dataInfo['Variable'] ]\ndataInfo=dataInfo.set_index('Variable')","dd645ec8":"def display_all(dataInfo):\n    with pd.option_context(\"display.max_rows\", 1000, \"display.max_columns\", 1000): \n        display(dataInfo)","f8a6273c":"#this function find imputation technique using by missing rate but we can not use these techniques because none values are meaningfull for data.\ndef findMethod(train,variableList):\n    train['Imputation_Technique']=\"\"\n    for element in variableList:\n        missingRate=float(dataInfo['%_Missing_Value'][element])\n        if missingRate == 0:\n             train['Imputation_Technique'][element]='No Missing Value'\n        elif missingRate <= 5:\n            train['Imputation_Technique'][element]='Simple'\n        elif missingRate < 25:\n            train['Imputation_Technique'][element]='Tree-based'\n        elif missingRate < 50 :\n            train['Imputation_Technique'][element]='Model'\n        elif missingRate >=50 :\n            train['Imputation_Technique'][element]='Drop'","b024426a":"findMethod(dataInfo,variableList)\ndisplay_all(dataInfo)","451aef09":"f, ax = plt.subplots(figsize=(15, 6))\nplt.xticks(rotation='90')\nsns.barplot(x=dataInfo.index, y=dataInfo['%_Missing_Value'])\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15);","ed4b3186":"#Impute by mode or median where NA train\nimpute_by_mode = train.loc[:, ['Electrical']]\nfor i in impute_by_mode.columns:\n    train[i].fillna(train[i].mode()[0], inplace = True)\n    \nimpute_by_median = train.loc[:, ['MasVnrArea']]\nfor i in impute_by_median.columns:\n    train[i].fillna(train[i].median(), inplace = True)","467f037d":"#Impute by mode or median where NA test\nimpute_by_mode = test.loc[:, ['Electrical']]\nfor i in impute_by_mode.columns:\n    test[i].fillna(test[i].mode()[0], inplace = True)\n    \nimpute_by_median = test.loc[:, ['MasVnrArea']]\nfor i in impute_by_median.columns:\n    test[i].fillna(test[i].median(), inplace = True)","8784dda4":"train.shape","0bf36747":"test.shape","4aced1bd":"!pip install fancyimpute","5a8454f6":"df=train.select_dtypes(include=['float64','int64'])\nvar_names=list(df)","2303165c":"from fancyimpute import KNN\nknn_imp=KNN(k=5).fit_transform(df);\ndff=pd.DataFrame(knn_imp)\ndff.columns=var_names","fffcd792":"msno.bar(dff,color='#79ccb3',sort='descending')\nplt.show()","1d1e965b":"dff.head()","647342b1":"categorics = ['object' , 'category']\ncategoricalColumns = train.select_dtypes(include=categorics)","7f9031d2":"categoric=categoricalColumns.reset_index() \ncategoric.drop(['Id'], axis=1, inplace=True)\ncategoric.head()","316d7572":"result = pd.concat([dff, categoric], axis=1)\nresult.head()","64b49574":"msno.bar(result,color='#79ccb3',sort='descending')\nplt.show()","ca3a0e44":"df_1=test.select_dtypes(include=['float64','int64'])\nvar_names=list(df_1)\n\nfrom fancyimpute import KNN\nknn_imp_1=KNN(k=5).fit_transform(df_1);\ndff_1=pd.DataFrame(knn_imp_1)\ndff_1.columns=var_names","4f90fe01":"test.isnull().sum()","ae0f32b4":"dff_1.isnull().sum()","67487cd7":"categorics = ['object' , 'category']\ncategoricalColumns_test = test.select_dtypes(include=categorics)","a3491e6c":" ab=categoricalColumns_test.index","3022a7c5":"categoric_test=categoricalColumns_test.reset_index() \ncategoric_test.drop(['Id'], axis=1, inplace=True)\ncategoric_test.head()","ce76a902":"result_1 = pd.concat([dff_1, categoric_test], axis=1)\nresult_1.head()","0a70f934":"result_1.set_index(ab,inplace=True)","d0f2ced3":"result_1.head()","d2927d02":"#Missing values in categoric features test data\ndisplay_all(result_1.isnull().sum().sort_values(ascending=False)[:10])","a3591165":"#Missing values in categoric features train data\ndisplay_all(result.isnull().sum().sort_values(ascending=False)[:10])","70d4af6c":"impute_by_mode = result_1.loc[:, ['MSZoning','Utilities','Functional','SaleType','Exterior2nd','Exterior1st','KitchenQual']]\nfor i in impute_by_mode.columns:\n    result_1[i].fillna(result_1[i].mode()[0], inplace = True)","1d4ecddb":"result.replace('None', np.nan, inplace=True)\nresult_1.replace('None', np.nan, inplace=True)","b6f26ca9":"# Missing of the entry can also be a valuable information\n# We encoded missingness in all columns so we will just create _na columns\nname=result.columns\nresult[name+\"_na\"] = pd.isnull(result)","f5b52088":"result=result.loc[:, (result !=False).any(axis=0)]","e587ac8a":"cols = result.select_dtypes(bool).columns\nresult[cols] = result[cols].astype(int)","501fbbdd":"result.head()","4383225d":"#After the above ,we can reassign NA value with  if it is categoric then None\nfor column in result.select_dtypes(include=categorics):\n    result[column] = result[column].fillna('None')","eb0bd9e6":"#if %missing>80 then drop value.\nresult.drop(['Alley'], axis=1, inplace=True)\nresult.drop(['Fence'], axis=1, inplace=True)\nresult.drop(['MiscFeature'], axis=1, inplace=True)\nresult.drop(['PoolQC'], axis=1, inplace=True)","9c4db80a":"#Check missing variable\nplt.figure(figsize =(30, 10))\nsns.heatmap(result.isnull());","71bee467":"# Missing of the entry can also be a valuable information\n# We encoded missingness in all columns so we will just create _na columns\nname=result_1.columns\nresult_1[name+\"_na\"] = pd.isnull(result_1)","52a69e2d":"result_1=result_1.loc[:, (result_1 !=False).any(axis=0)]","b91f8d92":"cols = result_1.select_dtypes(bool).columns\nresult_1[cols] = result_1[cols].astype(int)","1d755d4a":"#Reassign NA value with if it is categoric then None\nfor column in result_1.select_dtypes(include=categorics):\n    result_1[column] = result_1[column].fillna('None')","b619fa49":"#if %missing>80 then drop value.\nresult_1.drop(['Alley'], axis=1, inplace=True)\nresult_1.drop(['Fence'], axis=1, inplace=True)\nresult_1.drop(['MiscFeature'], axis=1, inplace=True)\nresult_1.drop(['PoolQC'], axis=1, inplace=True)","d34bcd86":"#Check missing variable\nplt.figure(figsize =(30, 10))\nsns.heatmap(result_1.isnull());","403a947e":"result_1.head()","042fb376":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumericalColumns = result.select_dtypes(include=numerics)","d01fef70":"numerics = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\ntestnumericalColumns = result_1.select_dtypes(include=numerics)","b33861fc":"plt.figure(figsize=(30,5))\n\nplott = pd.DataFrame([(colname, numericalColumns[colname][i]) for i in range(len(numericalColumns)) for colname in numericalColumns.columns], \n                 columns=['distribution of all train values', 'values'])\n\nchart=sns.stripplot(x = 'distribution of all train values', y='values', data=plott)\nchart.set_xticklabels(chart.get_xticklabels(), rotation=45);","3ee8a163":"#Box Plot Each Numeric Features in Train Data\n\nimport seaborn as sns\n%matplotlib inline\nfor col in numericalColumns.columns:\n    sns.boxplot(data = [numericalColumns[col]], linewidth = 1, width = 0.5) \n    plt.ylabel(col)\n    plt.title(\"IQR\")\n    plt.show()","ae879d3b":"# skewness along the index axis \ndata_train_skewness = result[numericalColumns.columns].skew(axis = 0, skipna = True)\ndata_test_skewness = result_1[testnumericalColumns.columns].skew(axis = 0, skipna = True)","006d9044":"sparse_columns=[]\nfor col in numericalColumns.columns:\n    if (numericalColumns[col].quantile(0.01)==numericalColumns[col].quantile(0.25)==numericalColumns[col].mode()[0]):\n        sparse_columns.append(col)\n\nprint(\"data_train_skewness:\\n\",data_train_skewness,\"\\n\")\nprint(\"data_test_skewness:\\n\",data_test_skewness,\"\\n\")\nprint(\"sparse_columns:\",sparse_columns)","c4bc1542":"categorics = ['object' , 'category']\ncategoricalColumns_result = result.select_dtypes(include=categorics)","13ee287f":"for column in categoricalColumns_result.columns:\n    print(\"\\n\" + column)\n    print(result[column].value_counts(normalize=True))","8a7c64be":"#if the ratio of frequency numbers is greater than 90%\n\nresult['Street_Pave'] = result['Street'].apply(lambda x:  1 if  x == 'Pave' else 0)\nresult['LandContour_Lvl'] = result['LandContour'].apply(lambda x:  1 if  x == 'Lvl' else 0)\nresult['Utilities_AllPub'] = result['Utilities'].apply(lambda x:  1 if  x == 'AllPub' else 0)\nresult['LandSlope_Gtl'] = result['LandSlope'].apply(lambda x:  1 if  x == 'Gtl' else 0)\nresult['Condition2_Norm'] = result['Condition2'].apply(lambda x:  1 if  x == 'Norm' else 0)\nresult['RoofMatl_CompShg'] = result['RoofMatl'].apply(lambda x:  1 if  x == 'CompShg' else 0)\nresult['BsmtCond_TA'] = result['BsmtCond'].apply(lambda x:  1 if  x == 'TA' else 0)\nresult['Heating_GasA'] = result['Heating'].apply(lambda x:  1 if  x == 'GasA' else 0)\nresult['Electrical_SBrkr'] = result['Electrical'].apply(lambda x:  1 if  x == 'SBrkr' else 0)\nresult['Functional_Typ'] = result['Functional'].apply(lambda x:  1 if  x == 'Typ' else 0)\nresult['GarageQual_TA'] = result['GarageQual'].apply(lambda x:  1 if  x == 'TA' else 0)\nresult['GarageCond_TA'] = result['GarageCond'].apply(lambda x:  1 if  x == 'TA' else 0)\nresult['PavedDrive_Y'] = result['PavedDrive'].apply(lambda x:  1 if  x == 'Y' else 0)\n\n\nresult_1['Street_Pave'] = result_1['Street'].apply(lambda x:  1 if  x == 'Pave' else 0)\nresult_1['LandContour_Lvl'] = result_1['LandContour'].apply(lambda x:  1 if  x == 'Lvl' else 0)\nresult_1['Utilities_AllPub'] = result_1['Utilities'].apply(lambda x:  1 if  x == 'AllPub' else 0)\nresult_1['LandSlope_Gtl'] = result_1['LandSlope'].apply(lambda x:  1 if  x == 'Gtl' else 0)\nresult_1['Condition2_Norm'] = result_1['Condition2'].apply(lambda x:  1 if  x == 'Norm' else 0)\nresult_1['RoofMatl_CompShg'] = result_1['RoofMatl'].apply(lambda x:  1 if  x == 'CompShg' else 0)\nresult_1['BsmtCond_TA'] = result_1['BsmtCond'].apply(lambda x:  1 if  x == 'TA' else 0)\nresult_1['Heating_GasA'] = result_1['Heating'].apply(lambda x:  1 if  x == 'GasA' else 0)\nresult_1['Electrical_SBrkr'] = result_1['Electrical'].apply(lambda x:  1 if  x == 'SBrkr' else 0)\nresult_1['Functional_Typ'] = result_1['Functional'].apply(lambda x:  1 if  x == 'Typ' else 0)\nresult_1['GarageQual_TA'] = result_1['GarageQual'].apply(lambda x:  1 if  x == 'TA' else 0)\nresult_1['GarageCond_TA'] = result_1['GarageCond'].apply(lambda x:  1 if  x == 'TA' else 0)\nresult_1['PavedDrive_Y'] = result_1['PavedDrive'].apply(lambda x:  1 if  x == 'Y' else 0)","9cfa8b04":"#To see count of numeric values in MiscVal column\nplt.figure(figsize=(15,8))\nsns.countplot(x='MiscVal',data=result);","b0ae82e5":"\nresult['MiscVal_0'] = result['MiscVal'].apply(lambda x:  '0' if  x == 0 else 'Not_0')\nresult['Built-GarageBuilt_0'] = result['Built-GarageBuilt'].apply(lambda x:  '0' if  x == 0 else 'Not_0')\nresult['PoolArea_0'] = result['PoolArea'].apply(lambda x:  '0' if  x == 0 else 'Not_0')\nresult['ScreenPorch_0'] = result['ScreenPorch'].apply(lambda x:  '0' if  x == 0 else 'Not_0')\nresult['3SsnPorch_0'] = result['3SsnPorch'].apply(lambda x:  '0' if  x == 0 else 'Not_0')\nresult['EnclosedPorch_0'] = result['EnclosedPorch'].apply(lambda x:  '0' if  x == 0 else 'Not_0')\nresult['KitchenAbvGr_1'] = result['KitchenAbvGr'].apply(lambda x:  '1' if  x == 1 else 'Not_1')\nresult['LowQualFinSF_0'] = result['LowQualFinSF'].apply(lambda x:  '0' if  x == 0 else 'Not_0')\nresult['BsmtFinSF2_0'] = result['BsmtFinSF2'].apply(lambda x:  '0' if  x == 0 else 'Not_0')\n\nresult_1['MiscVal_0'] = result_1['MiscVal'].apply(lambda x:  '0' if  x == 0 else 'Not_0')\nresult_1['Built-GarageBuilt_0'] = result_1['Built-GarageBuilt'].apply(lambda x:  '0' if  x == 0 else 'Not_0')\nresult_1['PoolArea_0'] = result_1['PoolArea'].apply(lambda x:  '0' if  x == 0 else 'Not_0')\nresult_1['ScreenPorch_0'] = result_1['ScreenPorch'].apply(lambda x:  '0' if  x == 0 else 'Not_0')\nresult_1['3SsnPorch_0'] = result_1['3SsnPorch'].apply(lambda x:  '0' if  x == 0 else 'Not_0')\nresult_1['EnclosedPorch_0'] = result_1['EnclosedPorch'].apply(lambda x:  '0' if  x == 0 else 'Not_0')\nresult_1['KitchenAbvGr_1'] = result_1['KitchenAbvGr'].apply(lambda x:  '1' if  x == 1 else 'Not_1')\nresult_1['LowQualFinSF_0'] = result_1['LowQualFinSF'].apply(lambda x:  '0' if  x == 0 else 'Not_0')\nresult_1['BsmtFinSF2_0'] = result_1['BsmtFinSF2'].apply(lambda x:  '0' if  x == 0 else 'Not_0')","a96e9ab0":"#To see count of categoric values in MiscVal column\nplt.figure(figsize=(15,8))\nsns.countplot(x='MiscVal_0',data=result);","34dd3c3a":"df=result.select_dtypes(include=['float64','int64'])","0d7c0fde":"q1=df.quantile(0.05)\nq3=df.quantile(0.95)\n\nIQR=q3-q1\n\nlower_bound=q1-1.5*IQR\nupper_bound=q3+1.5*IQR\n\noutlier_result=(df<(lower_bound)) | (df>(upper_bound))\n\noutliers=df[outlier_result]\n\nclean_df=df[~((df<(lower_bound)) | (df>(upper_bound))).any(axis=1)]","14c85bf3":"clean_df.head()","34482edd":"categorics = ['object' , 'category']\ncategoricalColumns = result.select_dtypes(include=categorics)","e48d007b":"final_train=pd.merge(clean_df, categoricalColumns, left_index=True, right_index=True)","f6368107":"final_train.shape","62639100":"for cname in final_train.columns:\n    if final_train[cname].nunique() > 10 and final_train[cname].dtype == \"object\":\n        print(cname)","123cfd3e":"final_train['MSSubClass_20'] = final_train['MSSubClass'].apply(lambda x:  1 if  x == '20' else 0)\nfinal_train['Neighborhood_NAmes'] = final_train['Neighborhood'].apply(lambda x:  1 if  x == 'NAmes' else 0)\nfinal_train['Exterior1st_VinylSd'] = final_train['Exterior1st'].apply(lambda x:  1 if  x == 'VinylSd' else 0)\nfinal_train['Exterior2nd_VinylSd'] = final_train['Exterior2nd'].apply(lambda x:  1 if  x == 'VinylSd' else 0)","56eafcc6":"low_cardinality_cols = [cname for cname in final_train.columns if \n                                final_train[cname].nunique() <= 10 and\n                                final_train[cname].dtype == \"object\"]\nnumeric_cols = [cname for cname in final_train.columns if \n                                final_train[cname].dtype in ['int64', 'float64']]\n\ndate_cols = [cname for cname in final_train.columns if \n                                final_train[cname].dtype in ['datetime64[ns]']]","a4f0b7b0":"my_cols = low_cardinality_cols + numeric_cols\ntrain_predictors = final_train[my_cols]","51fb2ad9":"train_predictors.head() \n#this data does not include number of unique varible is greater than or euqal 16 and numerical columns","7a6ba458":"train_predictors.dtypes.sample(10)","efc54b1d":"one_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\none_hot_encoded_training_predictors.head()","7acc65e3":"#Check isnull\ncol_mask=one_hot_encoded_training_predictors.isnull().any(axis=0) \nrow_mask=one_hot_encoded_training_predictors.isnull().any(axis=1)\none_hot_encoded_training_predictors.loc[row_mask,col_mask]","1bdc69d8":"#df_test=result_1.select_dtypes(include=['float64','int64'])","df11bdd1":"\"\"\"q1=df_test.quantile(0.01)\nq3=df_test.quantile(0.99)\n\nIQR=q3-q1\n\nlower_bound=q1-1.5*IQR\nupper_bound=q3+1.5*IQR\n\noutlier_result=(df_test<(lower_bound)) | (df_test>(upper_bound))\n\noutliers=df_test[outlier_result]\n\nclean_df_test=df_test[~((df_test<(lower_bound)) | (df_test>(upper_bound))).any(axis=1)]\"\"\"","aaccecf6":"#clean_df_test.head()","1a29d0e7":"\"\"\"categorics = ['object' , 'category']\ncategoricalColumns_test = result_1.select_dtypes(include=categorics)\"\"\"","754815a2":"#final_test=pd.merge(clean_df_test, categoricalColumns_test, left_index=True, right_index=True)","c1de7cef":"'''final_test.shape'''","910248dc":"final_test=result_1.copy()","11babd2d":"final_test['MSSubClass_20'] = final_test['MSSubClass'].apply(lambda x:  1 if  x == '20' else 0)\nfinal_test['Neighborhood_NAmes'] = final_test['Neighborhood'].apply(lambda x:  1 if  x == 'NAmes' else 0)\nfinal_test['Exterior1st_VinylSd'] = final_test['Exterior1st'].apply(lambda x:  1 if  x == 'VinylSd' else 0)\nfinal_test['Exterior2nd_VinylSd'] = final_test['Exterior2nd'].apply(lambda x:  1 if  x == 'VinylSd' else 0)","f7d9aa02":"low_cardinality_cols = [cname for cname in final_test.columns if \n                                final_test[cname].nunique() <= 10 and\n                                final_test[cname].dtype == \"object\"]\nnumeric_cols = [cname for cname in final_test.columns if \n                                final_test[cname].dtype in ['int64', 'float64']]\n\ndate_cols = [cname for cname in final_test.columns if \n                                final_test[cname].dtype in ['datetime64[ns]']]","02a5198e":"my_cols = low_cardinality_cols + numeric_cols\ntest_predictors = final_test[my_cols]","10ee4376":"test_predictors.head() \n#this data does not include number of unique varible is greater than or euqal 16 and numerical columns","7f6ec6c6":"one_hot_encoded_testing_predictors = pd.get_dummies(test_predictors)\none_hot_encoded_testing_predictors.head()","583509be":"one_hot_encoded_testing_predictors.shape","235a1617":"one_hot_encoded_training_predictors.shape","5061e70d":"missing_cols = set( one_hot_encoded_training_predictors.columns ) - set( one_hot_encoded_testing_predictors.columns )\nmissing_cols","19a79746":"one_hot_encoded_training_predictors, one_hot_encoded_testing_predictors = one_hot_encoded_training_predictors.align(one_hot_encoded_testing_predictors, axis=1)","508478a9":"missing_cols = set( one_hot_encoded_training_predictors.columns ) - set( one_hot_encoded_testing_predictors.columns )\nmissing_cols","4752f409":"one_hot_encoded_testing_predictors.isnull().sum().sort_values(ascending=False)","33b31703":"one_hot_encoded_testing_predictors.replace(np.nan, 0, inplace=True)","22505d81":"one_hot_encoded_training_predictors.replace(np.nan, 0, inplace=True)","169aa15c":"one_hot_encoded_testing_predictors.drop(['SalePrice'], axis=1, inplace=True)","e0e2538d":"one_hot_encoded_training_predictors = one_hot_encoded_training_predictors[[\n'2ndFlrSF',\n'3SsnPorch',\n'3SsnPorch_0_0',\n'AboveBath',\n'Alley_na',\n'BedroomAbvGr',\n'BldgType_1Fam',\n'BldgType_2fmCon',\n'BldgType_Duplex',\n'BldgType_Twnhs',\n'BldgType_TwnhsE',\n'BsmtBath',\n'BsmtCond_Fa',\n'BsmtCond_Gd',\n'BsmtCond_None',\n'BsmtCond_Po',\n'BsmtCond_TA',\n'BsmtCond_TA',\n'BsmtCond_TA',\n'BsmtCond_TA',\n'BsmtCond_na',\n'BsmtExposure_Av',\n'BsmtExposure_Gd',\n'BsmtExposure_Mn',\n'BsmtExposure_No',\n'BsmtExposure_None',\n'BsmtExposure_na',\n'BsmtFinSF1',\n'BsmtFinSF2',\n'BsmtFinSF2_0_0',\n'BsmtFinSF2_0_Not_0',\n'BsmtFinType1_ALQ',\n'BsmtFinType1_BLQ',\n'BsmtFinType1_GLQ',\n'BsmtFinType1_LwQ',\n'BsmtFinType1_None',\n'BsmtFinType1_Rec',\n'BsmtFinType1_Unf',\n'BsmtFinType1_na',\n'BsmtFinType2_ALQ',\n'BsmtFinType2_BLQ',\n'BsmtFinType2_GLQ',\n'BsmtFinType2_LwQ',\n'BsmtFinType2_None',\n'BsmtFinType2_Rec',\n'BsmtFinType2_Unf',\n'BsmtFinType2_na',\n'BsmtQual_Ex',\n'BsmtQual_Fa',\n'BsmtQual_Gd',\n'BsmtQual_None',\n'BsmtQual_TA',\n'BsmtQual_na',\n'BsmtUnfSF',\n'Built-GarageBuilt',\n'Built-GarageBuilt_0_0',\n'Built-GarageBuilt_0_Not_0',\n'Built-Remodel',\n'CentralAir_N',\n'CentralAir_Y',\n'Condition1_Artery',\n'Condition1_Feedr',\n'Condition1_Norm',\n'Condition1_PosA',\n'Condition1_PosN',\n'Condition1_RRAe',\n'Condition1_RRAn',\n'Condition1_RRNe',\n'Condition1_RRNn',\n'Condition2_Artery',\n'Condition2_Feedr',\n'Condition2_Norm',\n'Condition2_Norm',\n'Condition2_Norm',\n'Condition2_Norm',\n'Condition2_PosA',\n'Condition2_PosN',\n'Electrical_FuseA',\n'Electrical_FuseF',\n'Electrical_FuseP',\n'Electrical_Mix',\n'Electrical_SBrkr',\n'Electrical_SBrkr',\n'Electrical_SBrkr',\n'Electrical_SBrkr',\n'EnclosedPorch',\n'EnclosedPorch_0_0',\n'EnclosedPorch_0_Not_0',\n'ExterCond_Ex',\n'ExterCond_Fa',\n'ExterCond_Gd',\n'ExterCond_Po',\n'ExterCond_TA',\n'ExterQual_Ex',\n'ExterQual_Fa',\n'ExterQual_Gd',\n'ExterQual_TA',\n'Exterior1st_VinylSd',\n'Exterior2nd_VinylSd',\n'Fence_na',\n'FireplaceQu_Ex',\n'FireplaceQu_Fa',\n'FireplaceQu_Gd',\n'FireplaceQu_None',\n'FireplaceQu_Po',\n'FireplaceQu_TA',\n'FireplaceQu_na',\n'Fireplaces',\n'Foundation_BrkTil',\n'Foundation_CBlock',\n'Foundation_PConc',\n'Foundation_Slab',\n'Foundation_Stone',\n'Foundation_Wood',\n'Functional_Maj1',\n'Functional_Maj2',\n'Functional_Min1',\n'Functional_Min2',\n'Functional_Mod',\n'Functional_Sev',\n'Functional_Typ',\n'Functional_Typ',\n'Functional_Typ',\n'Functional_Typ',\n'GarageBuilt-Sold',\n'GarageCars',\n'GarageCond_Ex',\n'GarageCond_Fa',\n'GarageCond_Gd',\n'GarageCond_None',\n'GarageCond_Po',\n'GarageCond_TA',\n'GarageCond_TA',\n'GarageCond_TA',\n'GarageCond_TA',\n'GarageCond_na',\n'GarageFinish_Fin',\n'GarageFinish_None',\n'GarageFinish_RFn',\n'GarageFinish_Unf',\n'GarageFinish_na',\n'GarageQual_Ex',\n'GarageQual_Fa',\n'GarageQual_Gd',\n'GarageQual_None',\n'GarageQual_Po',\n'GarageQual_TA',\n'GarageQual_TA',\n'GarageQual_TA',\n'GarageQual_TA',\n'GarageQual_na',\n'GarageType_2Types',\n'GarageType_Attchd',\n'GarageType_Basment',\n'GarageType_BuiltIn',\n'GarageType_CarPort',\n'GarageType_Detchd',\n'GarageType_None',\n'GarageType_na',\n'GrLivArea',\n'HeatingQC_Ex',\n'HeatingQC_Fa',\n'HeatingQC_Gd',\n'HeatingQC_Po',\n'HeatingQC_TA',\n'Heating_GasA',\n'Heating_GasA',\n'Heating_GasA',\n'Heating_GasA',\n'HouseStyle_1.5Fin',\n'HouseStyle_1.5Unf',\n'HouseStyle_1Story',\n'HouseStyle_2.5Unf',\n'HouseStyle_2Story',\n'HouseStyle_SFoyer',\n'HouseStyle_SLvl',\n'KitchenAbvGr',\n'KitchenAbvGr_1_1',\n'KitchenAbvGr_1_Not_1',\n'KitchenQual_Ex',\n'KitchenQual_Fa',\n'KitchenQual_Gd',\n'KitchenQual_TA',\n'LandContour_Bnk',\n'LandContour_HLS',\n'LandContour_Low',\n'LandContour_Lvl',\n'LandContour_Lvl',\n'LandContour_Lvl',\n'LandContour_Lvl',\n'LandSlope_Gtl',\n'LandSlope_Gtl',\n'LandSlope_Gtl',\n'LandSlope_Gtl',\n'LandSlope_Mod',\n'LandSlope_Sev',\n'LotArea',\n'LotConfig_Corner',\n'LotConfig_CulDSac',\n'LotConfig_FR2',\n'LotConfig_FR3',\n'LotConfig_Inside',\n'LotFrontage',\n'LotShape_IR1',\n'LotShape_IR2',\n'LotShape_IR3',\n'LotShape_Reg',\n'LowQualFinSF',\n'LowQualFinSF_0_0',\n'MSSubClass_20',\n'MSZoning_C (all)',\n'MSZoning_FV',\n'MSZoning_RH',\n'MSZoning_RL',\n'MSZoning_RM',\n'MasVnrArea',\n'MasVnrType_BrkCmn',\n'MasVnrType_BrkFace',\n'MasVnrType_None',\n'MasVnrType_Stone',\n'MasVnrType_na',\n'MiscFeature_na',\n'MiscVal',\n'MiscVal_0_0',\n'MiscVal_0_Not_0',\n'MoSold',\n'Neighborhood_NAmes',\n'OpenPorchSF',\n'OverallCond',\n'OverallQual',\n'PavedDrive_N',\n'PavedDrive_P',\n'PavedDrive_Y',\n'PavedDrive_Y',\n'PavedDrive_Y',\n'PavedDrive_Y',\n'PoolArea',\n'PoolArea_0_0',\n'PoolQC_na',\n'Remodel-GarageBuilt',\n'Remodel-Sold',\n'RoofMatl_CompShg',\n'RoofMatl_CompShg',\n'RoofMatl_CompShg',\n'RoofMatl_CompShg',\n'RoofMatl_Tar&Grv',\n'RoofMatl_WdShake',\n'RoofMatl_WdShngl',\n'RoofStyle_Flat',\n'RoofStyle_Gable',\n'RoofStyle_Gambrel',\n'RoofStyle_Hip',\n'RoofStyle_Mansard',\n'RoofStyle_Shed',\n'SaleCondition_Abnorml',\n'SaleCondition_AdjLand',\n'SaleCondition_Alloca',\n'SaleCondition_Family',\n'SaleCondition_Normal',\n'SaleCondition_Partial',\n'SaleType_COD',\n'SaleType_CWD',\n'SaleType_Con',\n'SaleType_ConLD',\n'SaleType_ConLI',\n'SaleType_ConLw',\n'SaleType_New',\n'SaleType_Oth',\n'SaleType_WD',\n'ScreenPorch',\n'ScreenPorch_0_0',\n'ScreenPorch_0_Not_0',\n'Street_Pave',\n'Street_Pave',\n'Street_Pave',\n'Street_Pave',\n'TotalBsmtSF',\n'Utilities_AllPub',\n'Utilities_AllPub',\n'Utilities_AllPub',\n'Utilities_AllPub',\n'WoodDeckSF',\n'SalePrice']]","195f41f7":"y_training=one_hot_encoded_training_predictors['SalePrice']\nX_training=one_hot_encoded_training_predictors.iloc[:,:-1]","2c9ee617":"from sklearn.feature_selection import VarianceThreshold\nconstant_filter = VarianceThreshold(threshold=0)","d89ea580":"constant_filter.fit(X_training)","2f57951d":"len(X_training.columns[constant_filter.get_support()])","70f0d153":"constant_columns = [column for column in X_training.columns\n                    if column not in X_training.columns[constant_filter.get_support()]]\n\nprint(len(constant_columns))","f875338c":"for column in constant_columns:\n    print(column)","8592f9c8":"one_hot_encoded_training_predictors.drop(['3SsnPorch',\n'3SsnPorch_0_0',\n'BsmtCond_None',\n'BsmtCond_na',\n'BsmtExposure_None',\n'BsmtExposure_na',\n'BsmtFinType1_None',\n'BsmtFinType1_na',\n'BsmtFinType2_None',\n'BsmtFinType2_na',\n'BsmtQual_None',\n'BsmtQual_na',\n'Condition2_Artery',\n'Condition2_Feedr',\n'Condition2_Norm',\n'Condition2_PosA',\n'Condition2_PosN',\n'ExterCond_Po',\n'Foundation_Slab',\n'Functional_Sev',\n'Heating_GasA',\n'KitchenAbvGr',\n'KitchenAbvGr_1_1',\n'KitchenAbvGr_1_Not_1',\n'LowQualFinSF',\n'LowQualFinSF_0_0',\n'MiscFeature_na',\n'MiscVal',\n'MiscVal_0_0',\n'MiscVal_0_Not_0',\n'PoolArea',\n'PoolArea_0_0',\n'PoolQC_na',\n'RoofMatl_CompShg',\n'RoofMatl_Tar&Grv',\n'RoofMatl_WdShake',\n'RoofMatl_WdShngl',\n'RoofStyle_Flat',\n'RoofStyle_Shed',\n'Street_Pave',\n'Utilities_AllPub'], axis=1, inplace=True)\n\n\none_hot_encoded_testing_predictors.drop(['3SsnPorch',\n'3SsnPorch_0_0',\n'BsmtCond_None',\n'BsmtCond_na',\n'BsmtExposure_None',\n'BsmtExposure_na',\n'BsmtFinType1_None',\n'BsmtFinType1_na',\n'BsmtFinType2_None',\n'BsmtFinType2_na',\n'BsmtQual_None',\n'BsmtQual_na',\n'Condition2_Artery',\n'Condition2_Feedr',\n'Condition2_Norm',\n'Condition2_PosA',\n'Condition2_PosN',\n'ExterCond_Po',\n'Foundation_Slab',\n'Functional_Sev',\n'Heating_GasA',\n'KitchenAbvGr',\n'KitchenAbvGr_1_1',\n'KitchenAbvGr_1_Not_1',\n'LowQualFinSF',\n'LowQualFinSF_0_0',\n'MiscFeature_na',\n'MiscVal',\n'MiscVal_0_0',\n'MiscVal_0_Not_0',\n'PoolArea',\n'PoolArea_0_0',\n'PoolQC_na',\n'RoofMatl_CompShg',\n'RoofMatl_Tar&Grv',\n'RoofMatl_WdShake',\n'RoofMatl_WdShngl',\n'RoofStyle_Flat',\n'RoofStyle_Shed',\n'Street_Pave',\n'Utilities_AllPub'], axis=1, inplace=True)","a6545a63":"constant_filter.fit(one_hot_encoded_testing_predictors)","9519ffd4":"len(one_hot_encoded_testing_predictors.columns[constant_filter.get_support()])","35373c26":"constant_columns = [column for column in one_hot_encoded_testing_predictors.columns\n                    if column not in one_hot_encoded_testing_predictors.columns[constant_filter.get_support()]]\n\nprint(len(constant_columns))","63f12f41":"for column in constant_columns:\n    print(column)","9d58e3ab":"one_hot_encoded_training_predictors.drop([\n'Electrical_Mix',\n'GarageQual_Ex'], axis=1, inplace=True)\n\none_hot_encoded_testing_predictors.drop([\n'Electrical_Mix',\n'GarageQual_Ex'], axis=1, inplace=True)","57cd5610":"print(X_training.duplicated().sum())\nprint(one_hot_encoded_testing_predictors.duplicated().sum())","228f9b60":"one_hot_encoded_training_predictors = one_hot_encoded_training_predictors.loc[:,~one_hot_encoded_training_predictors.columns.duplicated()]\none_hot_encoded_testing_predictors = one_hot_encoded_testing_predictors.loc[:,~one_hot_encoded_testing_predictors.columns.duplicated()]","711d64c9":"correlated_features = set()\ncorrelation_matrix = one_hot_encoded_training_predictors.loc[:, one_hot_encoded_training_predictors.columns != 'SalePrice'].corr()","ac16b1b7":"for i in range(len(correlation_matrix .columns)):\n    for j in range(i):\n        if abs(correlation_matrix.iloc[i, j]) > 0.8:\n            colname = correlation_matrix.columns[i]\n            correlated_features.add(colname)","1a072ba1":"correlated_features","fc1f4028":"len(correlated_features)","6ae4a10c":"print(correlated_features)","a29acd7e":"one_hot_encoded_training_predictors.drop(labels=correlated_features, axis=1, inplace=True)\none_hot_encoded_testing_predictors.drop(labels=correlated_features, axis=1, inplace=True)","82a7a8f7":"y_training=one_hot_encoded_training_predictors['SalePrice']\nX_training=one_hot_encoded_training_predictors.iloc[:,:-1]","f63d8847":"from sklearn.ensemble import RandomForestRegressor\nimport matplotlib.pyplot as plt\nmodel_rf= RandomForestRegressor(max_depth=10, random_state=42)\nmodel_rf.fit(X_training, y_training)\nprint(model_rf.feature_importances_) #use inbuilt class feature_importances of tree based classifiers\n#plot graph of feature importances for better visualization\nfeat_importances = pd.Series(model_rf.feature_importances_,index=X_training.columns)\nfeat_importances = feat_importances.sort_values(ascending=False)\nfeat_importances","8be8ac41":"from sklearn.feature_selection import SelectKBest, chi2 # for chi-squared feature selection\nfrom sklearn import feature_selection\n\n# categorical feature selection\nsf = SelectKBest(score_func=feature_selection.f_regression, k='all')\nsf_fit = sf.fit(X_training, y_training)\n# print feature scores\nfor i in range(len(sf_fit.scores_)):\n    print(' %s: %f' % (X_training.columns[i], sf_fit.scores_[i]))","8a7c8c0d":"datset = pd.DataFrame()\ndatset['feature'] = X_training.columns[ range(len(sf_fit.scores_))]\ndatset['scores'] = sf_fit.scores_\ndatset = datset.sort_values(by='scores', ascending=False)\ndatset2=datset.loc[datset['scores']>=150]\ndatset2","b9a82879":"plt.figure(figsize=(20,10))\nsns.barplot(datset2['scores'], datset2['feature'], color='blue')\nsns.set_style('whitegrid')\nplt.ylabel('Categorical Feature', fontsize=18)\nplt.xlabel('Score', fontsize=18)\nplt.show()","0caf6e5c":"final_Train=one_hot_encoded_training_predictors[[\n'OverallQual',\n'GrLivArea',\n'GarageCars',\n'TotalBsmtSF',\n'AboveBath',\n'BsmtQual_Ex',\n'GarageBuilt-Sold',\n'Remodel-Sold',\n'KitchenQual_Ex',\n'BsmtQual_TA',\n'MasVnrArea',\n'ExterQual_Ex',\n'ExterQual_Gd',\n'FireplaceQu_None',\n'BsmtFinType1_GLQ',\n'GarageFinish_Unf',\n'HeatingQC_Ex',\n'GarageFinish_Fin',\n'Foundation_CBlock',\n'LotFrontage',\n'SaleCondition_Partial',\n'MasVnrType_None',\n'GarageType_Detchd',\n'LotArea',\n'OpenPorchSF',\n'GarageType_Attchd',\n'HeatingQC_TA',\n'MSZoning_RM',\n'BsmtFinSF1',\n'GarageCond_TA',\n'FireplaceQu_Gd',\n'MasVnrType_Stone',\n'SalePrice']]\n\n\nfinal_Test=one_hot_encoded_testing_predictors[[\n'OverallQual',\n'GrLivArea',\n'GarageCars',\n'TotalBsmtSF',\n'AboveBath',\n'BsmtQual_Ex',\n'GarageBuilt-Sold',\n'Remodel-Sold',\n'KitchenQual_Ex',\n'BsmtQual_TA',\n'MasVnrArea',\n'ExterQual_Ex',\n'ExterQual_Gd',\n'FireplaceQu_None',\n'BsmtFinType1_GLQ',\n'GarageFinish_Unf',\n'HeatingQC_Ex',\n'GarageFinish_Fin',\n'Foundation_CBlock',\n'LotFrontage',\n'SaleCondition_Partial',\n'MasVnrType_None',\n'GarageType_Detchd',\n'LotArea',\n'OpenPorchSF',\n'GarageType_Attchd',\n'HeatingQC_TA',\n'MSZoning_RM',\n'BsmtFinSF1',\n'GarageCond_TA',\n'FireplaceQu_Gd',\n'MasVnrType_Stone']]","d6ba5f44":"y = final_Train['SalePrice']\n\n# Creating validation set\n# It will split our data set to have length n train and len(df) - n validation set\ndef split_train_val(final_Train,n): \n    \n    return final_Train[:n].copy(), final_Train[n:].copy()\n\n\nn_valid = 300\nn_train = len(final_Train)-n_valid\n\nX_train, X_valid = split_train_val(final_Train, n_train)\ny_train, y_valid = split_train_val(y, n_train)\n\nX_train.shape, y_train.shape, X_valid.shape, y_valid.shape","3b2a23f1":"X_test=final_Test\nX_test.shape","813f4962":"X_train.drop(['SalePrice'], axis=1, inplace=True)\nX_valid.drop(['SalePrice'], axis=1, inplace=True)","8c39d681":"#RANDOM FOREST REGRESSOR\nfrom sklearn.ensemble import RandomForestRegressor\n\nrfg_model_ham =  RandomForestRegressor()\nrfg_model_ham_fit = rfg_model_ham.fit(X_train, y_train)\n\ntahmin_train_rfg = rfg_model_ham_fit.predict(X_train)\ntahmin_test_rfg = rfg_model_ham_fit.predict(X_valid)\n\nfrom sklearn.metrics import r2_score, mean_squared_error\nprint('RandomForestRegressor R\u00b2 (E\u011fitim): %.4f' % r2_score(y_train, tahmin_train_rfg))\nprint('RandomForestRegressor MSE (E\u011fitim): %.4f' % mean_squared_error(y_train, tahmin_train_rfg))\nprint('RandomForestRegressor R\u00b2 (Test): %.4f' % r2_score(y_valid, tahmin_test_rfg))\nprint('RandomForestRegressor MSE (Test): %.4f' % mean_squared_error(y_valid, tahmin_test_rfg))","9ca94c57":"from sklearn.linear_model import Ridge\nridge_model=Ridge(alpha=0.1).fit(X_train,y_train)\nridge_model.coef_","36bf57d5":"lambdas=10**np.linspace(10,-2,100)*0.5","327da835":"ridge_model=Ridge()\ncoefficients=[]\n\nfor i in lambdas:\n    ridge_model.set_params(alpha=i)\n    ridge_model.fit(X_train,y_train)\n    coefficients.append(ridge_model.coef_)\n    \nax=plt.gca()\nax.plot(lambdas,coefficients)\nax.set_xscale('log')\n\nplt.xlabel('Lambda (Alpha) Values')\nplt.ylabel('Coefficients \/ Weights')\nplt.title(\"Ridge Coefficients\");","49532cd6":"y_pred_valid=ridge_model.predict(X_valid)\nnp.sqrt(mean_squared_error(y_valid,y_pred_valid))","895f7321":"lambdas=10**np.linspace(10,-2,100)*0.5\nfrom sklearn.linear_model import RidgeCV","74fe476e":"ridge_cv=RidgeCV(alphas=lambdas, scoring=\"neg_mean_squared_error\", normalize=True)\nridge_cv.fit(X_train, y_train)\nridge_cv.alpha_","89e12717":"ridge_tuned=Ridge(alpha=ridge_cv.alpha_,normalize=True).fit(X_train,y_train)\nnp.sqrt(mean_squared_error(y_valid,ridge_tuned.predict(X_valid)))","e78983cc":"from sklearn.ensemble import RandomForestRegressor\n\nrfg_model_ham =  RandomForestRegressor()\nrfg_model_ham_fit = rfg_model_ham.fit(X_train, y_train)\n\npredict_train_rfg = rfg_model_ham_fit.predict(X_train)\npredict_valid_rfg = rfg_model_ham_fit.predict(X_valid)\n\nfrom sklearn.metrics import r2_score, mean_squared_error\nprint('RandomForestRegressor R\u00b2 (E\u011fitim): %.4f' % r2_score(y_train, predict_train_rfg))\nprint('RandomForestRegressor MSE (E\u011fitim): %.4f' % mean_squared_error(y_train, predict_train_rfg))\nprint('RandomForestRegressor R\u00b2 (Test): %.4f' % r2_score(y_valid, predict_valid_rfg))\nprint('RandomForestRegressor MSE (Test): %.4f' % mean_squared_error(y_valid, predict_valid_rfg))","3fb223c8":"rf_params={'max_depth' : list(range(1,10)),\n          'max_features': [3,5,10,15],\n          'n_estimators': [100,200,500]}","001727bc":"rf_model=RandomForestRegressor(random_state=42)","2f7d4276":"from sklearn.model_selection import GridSearchCV\nrf_cv_model=GridSearchCV(rf_model,rf_params,cv=10,n_jobs=-1)","de94ceeb":"rf_cv_model.fit(X_train,y_train)","2529378d":"rf_cv_model.best_params_","e807f578":"rf_tuned=RandomForestRegressor(max_depth=9 , max_features=10 , n_estimators=200)","eb6b5132":"rf_tuned_fit= rf_tuned.fit(X_train,y_train)\nrf_tuned_pred_train=rf_tuned_fit.predict(X_train)\nrf_tuned_pred_valid=rf_tuned_fit.predict(X_valid)","d9ff0042":"from sklearn.metrics import r2_score, mean_squared_error\nprint('RandomForestRegressor tuned R\u00b2 (E\u011fitim): %.4f' % r2_score(y_train, rf_tuned_pred_train))\nprint('RandomForestRegressor tuned MSE (E\u011fitim): %.4f' % mean_squared_error(y_train, rf_tuned_pred_train))\nprint('RandomForestRegressor tuned R\u00b2 (Test): %.4f' % r2_score(y_valid, rf_tuned_pred_valid))\nprint('RandomForestRegressor tuned MSE (Test): %.4f' % mean_squared_error(y_valid, rf_tuned_pred_valid))","4eed788e":"!pip install xgboost\nimport xgboost as xgb\nfrom xgboost import XGBRegressor","f6e9d5de":"xgb_model=XGBRegressor().fit(X_train,y_train)\n\ny_xgb_train = xgb_model.predict(X_train)\ny_xgb_valid = xgb_model.predict(X_valid)","d95a222f":"from sklearn.metrics import r2_score, mean_squared_error\n\nprint('XGBRegressor R\u00b2 (E\u011fitim): %.4f' % r2_score(y_train, y_xgb_train))\nprint('XGBRegressor MSE (E\u011fitim): %.4f' % mean_squared_error(y_train, y_xgb_train))\nprint('XGBRegressor R\u00b2 (Test): %.4f' % r2_score(y_valid, y_xgb_valid))\nprint('XGBRegressor MSE (Test): %.4f' % mean_squared_error(y_valid, y_xgb_valid))","1c852dc4":"xgb_grid={'colsample_bytree' : [0.4,0.5,0.6,0.9,1],\n         'n_estimators' : [100,200,500,1000],\n         'max_depth' : [2,3,4,5,6],\n         'learning_rate' : [0.1, 0.01, 0.5]}","ffdb257a":"xgb_cv=GridSearchCV(xgb_model, param_grid=xgb_grid, cv=10, n_jobs=-1, verbose=2)","281e0541":"xgb_cv.fit(X_train,y_train)","d8c79b50":"xgb_cv.best_params_","5c3dccb1":"xgb_tuned=XGBRegressor(colsample_bytree=0.9 ,\n                      learning_rate=0.1 ,\n                      max_depth=2 ,\n                      n_estimators=500 )","ba52ded5":"xgb_tuned=xgb_tuned.fit(X_train,y_train)\n\ny_xgb_tuned_train = xgb_tuned.predict(X_train)\ny_xgb_tuned_valid = xgb_tuned.predict(X_valid)","613249d6":"from sklearn.metrics import r2_score, mean_squared_error\n\nprint('Tuned XGBRegressor R\u00b2 (E\u011fitim): %.4f' % r2_score(y_train, y_xgb_tuned_train))\nprint('Tuned XGBRegressor MSE (E\u011fitim): %.4f' % mean_squared_error(y_train, y_xgb_tuned_train))\nprint('Tuned XGBRegressor R\u00b2 (Test): %.4f' % r2_score(y_valid, y_xgb_tuned_valid))\nprint('Tuned XGBRegressor MSE (Test): %.4f' % mean_squared_error(y_valid, y_xgb_tuned_valid))","e01ec385":"final_Test.head()","c4ab22f0":"y_pred=xgb_tuned.predict(final_Test)","a66cc42b":"y_pred=np.expm1(y_pred) ","57fc3d16":"submission = pd.DataFrame()\nsubmission['ID'] = ab\nsubmission['SalePrice'] = y_pred.reshape((y_pred.shape[0]))\nsubmission.to_csv('sub3.csv', index=False)","f4862e0e":"Model Tuning Random Forest Regressor","fe8440bf":"For Training","c1c4d8e8":"Feature Selection for Train Set","6dcfd4c6":"Handling Categoric Missing -Train Data","44de9241":"> > Outlier Data Visualization","0a2c93ee":"Tree Based Missing Imputation","3c9058ad":"Only Single value columns","015a1125":"One hot encoding is the most widespread approach, and it works very well unless your categorical variable takes on a large number of values (i.e. you generally won't it for variables taking more than 15 different values. It'd be a poor choice in some cases with fewer values, though that varies.)\n","b660b20f":"Skewness","27de36ed":"XGBoost","3768c9c6":"Outlier for Test","fa7fc1ca":"Sparse Columns","3a22f182":"Categorical Feature Selection","8952a6b4":"Handling Missing - Test Data","86594739":"High Correlation between train features","1dde167c":"Here is the  summary of train data !!!","f8824640":"*They are same meaning approximately;*\n\n> TotalBsmtSF: Total square feet of basement area\n> \n> 1stFlrSF: First Floor square feet\n> \n\n*They are same meaning approximately;\n\n> GrLivArea: Above grade (ground) living area square feet\n> \n> TotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n> \n> \nSo I will check their correlation with dependent value.","42c73337":"Removing Duplicate Features using Transpose","076d6e95":"Now it is time to convert None categoric to NA ,because we will create a new column from columns with NA values. If it is NA assign 1 else 0. ","c2f88f7f":"Tuning Model","fb1f60f6":"We will do the same process on test data.","cda2b5b2":"One Hot Encoding for Categoric Columns in Test Data","ef8ec847":"Finding Zero variance columns","12d0c46c":"Splitting Train-Validation-Test","fc453fe2":"High Correlation with Sale Price(Dependent Value)","a15a36d9":"Normalization with NaN","4c7a15cd":"Handling Categoric Missing Data using Mode in test data","c77e21d6":"Let's find the columns in train data but not in test data.","f40e2c57":"Ridge Regression Model Tuning","37b010fe":"> With Random Forest Regressor","063b2338":"Categorical Columns Value Counts Train Data","d4efe944":"Outlier for Train","cf0678e8":"Removing Correlated Features using corr() Method for numeric columns","a25921af":"Model Tuning","1bb2f791":"Handling Numeric Outlier Values","d94d4ed3":"> With SelectKBest f_regression","e5f5c793":"Log Transform","ec66b7d9":"Ridge Regression","e59d9ded":"One Hot Encoding for Categoric Columns in Final Train Data","015ce658":"Review of Train Data","a6926cc3":"For Testing","1dde7799":"RANDOM FOREST REGRESSOR","dca83aa0":"> Removing Constant Features using Variance Threshold","ce57e46b":"Zero variance indicates that all data are equal, that is, there is no diffence from the mean."}}