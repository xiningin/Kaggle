{"cell_type":{"ece247fc":"code","7164da15":"code","5ee5f80c":"code","6750d7e7":"code","0f62581e":"code","7529a8b9":"code","475c1754":"code","e8b272e6":"code","c8c80cff":"code","ece5949d":"code","0db13e0f":"code","99e6165b":"code","94a5f08f":"code","cdd346a6":"code","bfcf1ce3":"code","7830dcb1":"code","fb718c08":"code","120de7af":"code","9693d9be":"code","3e772e6a":"code","925935bc":"code","1945f5ee":"code","0174bd17":"code","5b7ae278":"code","c0b4c0b1":"code","ba2bd21c":"code","748ba0dc":"code","809aca2e":"code","91da8f2e":"code","fc93fcb3":"code","3008db3e":"code","02a1f466":"code","885c1d82":"code","260da9d3":"code","a34b4155":"code","4834de82":"code","8e647094":"code","b9efca21":"code","3dfacd01":"code","a9c8f089":"code","8d0a2775":"code","e315d65c":"code","998f54a4":"markdown","03224b2a":"markdown","409d3711":"markdown","309040b9":"markdown","274f1795":"markdown","00e264b5":"markdown","2d007c8f":"markdown","69a9f7de":"markdown","1a9a8440":"markdown","7f2dffce":"markdown","62552c29":"markdown","c37cbbba":"markdown","4f966c40":"markdown","2e81a89e":"markdown","0204dd85":"markdown","35f68b96":"markdown","e47823a5":"markdown","d10f0298":"markdown","9626336d":"markdown","714f7bb7":"markdown","9c8b557e":"markdown","3436349b":"markdown","b00c2ef1":"markdown","72927c24":"markdown","9e5c78e8":"markdown","18834420":"markdown","6153f464":"markdown","fcd81138":"markdown","12cc0b43":"markdown","22ee2dda":"markdown","3510c108":"markdown","d839bfa8":"markdown","58323ec3":"markdown","197553ce":"markdown","483d1021":"markdown","7786cd75":"markdown","ff42f212":"markdown"},"source":{"ece247fc":"# data analysis\nimport numpy as np\nimport pandas as pd\n\n# data visualization\nimport seaborn as sn\nimport matplotlib.pyplot as plt\n%matplotlib inline","7164da15":"# import train and test dataset\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\n\n# take a look at train dataset\ntrain.describe(include=\"all\")","5ee5f80c":"# get the list of all the features in the dataset\nprint(train.columns.values)","6750d7e7":"# see the top 10 values of the dataset to get an idea of the variables.\ntrain.head(10)","0f62581e":"# Checking missing values in each variable\nplt.figure(figsize=(10,10))\nsn.heatmap(train.isnull(), yticklabels = False, cbar = False)","7529a8b9":"# check for any other unusable values\npd.isnull(train).sum()","475c1754":"# Bar plot of Suvived by Sex\nsn.barplot(x = \"Sex\",y = \"Survived\", data = train)\n\n#print percentages of females vs. males that survive\nprint(\"Percentage of females who survived:\", train[\"Survived\"][train[\"Sex\"] == 'female'].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of males who survived:\", train[\"Survived\"][train[\"Sex\"] == 'male'].value_counts(normalize = True)[1]*100)","e8b272e6":"# bar plot of Survived by SibSp feature\nsn.barplot(x=\"SibSp\", y=\"Survived\", data=train)\n\n# Some of the values survived percentage percentage values\nprint(\"Percentage of SibSp = 0 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 0].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of SibSp = 1 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of SibSp = 2 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 2].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of SibSp = 3 who survived:\", train[\"Survived\"][train[\"SibSp\"] == 3].value_counts(normalize = True)[1]*100)","c8c80cff":"# sort the ages in logical categories\ntrain[\"Age\"] = train[\"Age\"].fillna(-0.5)\ntest[\"Age\"] = test[\"Age\"].fillna(-0.5)\nbins = [-1, 0, 5, 12, 18, 24, 35, 60, np.inf]\nlabels = ['Unknown', 'Baby', 'Child', 'Teenager', 'Student', 'Young Adult', 'Adult', 'Senior']\ntrain['AgeGroup'] = pd.cut(train['Age'], bins, labels = labels)\ntest['AgeGroup'] = pd.cut(test['Age'], bins, labels = labels)\n\n# barplot of Age vs Survived\nplt.figure(figsize=(10,5))\nsn.barplot(x = \"AgeGroup\", y = \"Survived\", data = train)","ece5949d":"#draw a bar plot of Pclass vs Survived\nsn.barplot(x=\"Pclass\", y=\"Survived\", data=train)\n\n#print percentage of people by Pclass that survived\nprint(\"Percentage of Pclass = 1 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 1].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of Pclass = 2 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 2].value_counts(normalize = True)[1]*100)\n\nprint(\"Percentage of Pclass = 3 who survived:\", train[\"Survived\"][train[\"Pclass\"] == 3].value_counts(normalize = True)[1]*100)","0db13e0f":"# bar plot of Parch vs Survived\nsn.barplot(x=\"Parch\", y=\"Survived\", data=train)\nplt.show()","99e6165b":"test.describe(include = \"all\")","94a5f08f":"# Let's start by removing Cabin Feature as not a lot more useful information can be extracted from it.\ntrain.drop(['Cabin'], axis = 1, inplace = True)\ntest.drop(['Cabin'], axis = 1, inplace = True)\n\n#we can also drop the Ticket feature since it's unlikely to yield any useful information\ntrain.drop(['Ticket'], axis = 1, inplace = True)\ntest.drop(['Ticket'], axis = 1, inplace = True)","cdd346a6":"#now we need to fill in the missing values in the Embarked feature\nprint(\"Number of people embarking in Southampton (S):\")\nsouthampton = train[train[\"Embarked\"] == \"S\"].shape[0]\nprint(southampton)\n\nprint(\"Number of people embarking in Cherbourg (C):\")\ncherbourg = train[train[\"Embarked\"] == \"C\"].shape[0]\nprint(cherbourg)\n\nprint(\"Number of people embarking in Queenstown (Q):\")\nqueenstown = train[train[\"Embarked\"] == \"Q\"].shape[0]\nprint(queenstown)","bfcf1ce3":"train.fillna({'Embarked':'S'}, inplace = True)","7830dcb1":"#create a combined group of both datasets\ncombine = [train, test]\n\n#extract a title for each Name in the train and test datasets\nfor dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train['Title'], train['Sex'])","fb718c08":"for dataset in combine:\n    dataset['Title'] = dataset['Title'].replace(['Lady', 'Capt', 'Col',\n    'Don', 'Dr', 'Major', 'Rev', 'Jonkheer', 'Dona'], 'Rare')\n    \n    dataset['Title'] = dataset['Title'].replace(['Countess', 'Sir'], 'Royal')\n    dataset['Title'] = dataset['Title'].replace(['Mlle', 'Ms'], 'Miss')\n    dataset['Title'] = dataset['Title'].replace('Mme', 'Mrs')\n\ntrain[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","120de7af":"#map each of the title groups to a numerical value\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Royal\": 5, \"Rare\": 6}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain.head()","9693d9be":"# fill missing age with mode age group for each title\nmr_age = train[train[\"Title\"] == 1][\"AgeGroup\"].mode() #Young Adult\nmiss_age = train[train[\"Title\"] == 2][\"AgeGroup\"].mode() #Student\nmrs_age = train[train[\"Title\"] == 3][\"AgeGroup\"].mode() #Adult\nmaster_age = train[train[\"Title\"] == 4][\"AgeGroup\"].mode() #Baby\nroyal_age = train[train[\"Title\"] == 5][\"AgeGroup\"].mode() #Adult\nrare_age = train[train[\"Title\"] == 6][\"AgeGroup\"].mode() #Adult\n\nage_title_mapping = {1: \"Young Adult\", 2: \"Student\", 3: \"Adult\", 4: \"Baby\", 5: \"Adult\", 6: \"Adult\"}\n\nfor i in range(len(train[\"AgeGroup\"])):\n    if train[\"AgeGroup\"][i] == \"Unknown\":\n        train[\"AgeGroup\"][i] = age_title_mapping[train[\"Title\"][i]]\n        \nfor i in range(len(test[\"AgeGroup\"])):\n    if test[\"AgeGroup\"][i] == \"Unknown\":\n        test[\"AgeGroup\"][i] = age_title_mapping[test[\"Title\"][i]]","3e772e6a":"age_mapping = {'Baby': 1, 'Child': 2, 'Teenager': 3, 'Student': 4, 'Young Adult': 5, 'Adult': 6, 'Senior': 7}\ntrain['AgeGroup'] = train['AgeGroup'].map(age_mapping)\ntest['AgeGroup'] = test['AgeGroup'].map(age_mapping)\n\ntrain.head()","925935bc":"#dropping the Age feature for now, might change\ntrain = train.drop(['Age'], axis = 1)\ntest = test.drop(['Age'], axis = 1)","1945f5ee":"#drop the name feature since it contains no more useful information.\ntrain.drop(['Name'], axis = 1, inplace = True)\ntest.drop(['Name'], axis = 1, inplace = True)","0174bd17":"#map each Sex value to a numerical value\nsex_mapping = {\"male\": 0, \"female\": 1}\ntrain['Sex'] = train['Sex'].map(sex_mapping)\ntest['Sex'] = test['Sex'].map(sex_mapping)\n\ntrain.head()","5b7ae278":"#map each Embarked value to a numerical value\nembarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ntrain['Embarked'] = train['Embarked'].map(embarked_mapping)\ntest['Embarked'] = test['Embarked'].map(embarked_mapping)\n\ntrain.head()","c0b4c0b1":"#fill in missing Fare value in test set based on mean fare for that Pclass \nfor i in range(len(test[\"Fare\"])):\n    if pd.isnull(test[\"Fare\"][i]):\n        pclass = test[\"Pclass\"][i] #Pclass = 3\n        test[\"Fare\"][i] = round(train[train[\"Pclass\"] == pclass][\"Fare\"].mean(), 4)\n\n#map Fare values into groups of numerical values\ntrain['FareBand'] = pd.qcut(train['Fare'], 4, labels = [1, 2, 3, 4])\ntest['FareBand'] = pd.qcut(test['Fare'], 4, labels = [1, 2, 3, 4])\n\n#drop Fare values\ntrain = train.drop(['Fare'], axis = 1)\ntest = test.drop(['Fare'], axis = 1)","ba2bd21c":"# train dataset\nplt.figure(figsize=(5,5))\nsn.heatmap(train.isnull(), yticklabels = False, cbar = False)","748ba0dc":"# Test dataset\nplt.figure(figsize=(5,5))\nsn.heatmap(test.isnull(), yticklabels = False, cbar = False)","809aca2e":"train.head()","91da8f2e":"test.head()","fc93fcb3":"from sklearn.model_selection import train_test_split\n\npredictors = train.drop(['Survived', 'PassengerId'], axis = 1)\ntarget = train['Survived']\nx_train, x_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.22, random_state = 0)","3008db3e":"# Gaussian Naive Bayes\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_val)\nacc_gaussian = round(accuracy_score(y_pred, y_val)*100, 2)\nacc_gaussian","02a1f466":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_val)\nacc_logreg = round(accuracy_score(y_pred, y_val)*100, 2)\nacc_logreg","885c1d82":"# Support Vector Machines\nfrom sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_val)\nacc_svc = round(accuracy_score(y_pred, y_val)*100, 2)\nacc_svc","260da9d3":"# Linear SVC\nfrom sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_val)\nacc_linear_svc = round(accuracy_score(y_pred, y_val)*100, 2)\nacc_linear_svc","a34b4155":"# Perceptron\nfrom sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_val)\nacc_perceptron = round(accuracy_score(y_pred, y_val)*100, 2)\nacc_perceptron","4834de82":"# Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\n\ndecision_tree = DecisionTreeClassifier()\ndecision_tree.fit(x_train, y_train)\ny_pred = decision_tree.predict(x_val)\nacc_decision_tree = round(accuracy_score(y_pred, y_val)*100 ,2)\nacc_decision_tree","8e647094":"# Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nrandom_forest = RandomForestClassifier()\nrandom_forest.fit(x_train, y_train)\ny_pred = random_forest.predict(x_val)\nacc_random_forest = round(accuracy_score(y_pred, y_val)*100, 2)\nacc_random_forest","b9efca21":"#kNN or k-Nearest Neighbors\nfrom sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_val)\nacc_knn = round(accuracy_score(y_pred, y_val)*100, 2)\nacc_knn","3dfacd01":"# Gradient Boosting Classifier\nfrom sklearn.ensemble import GradientBoostingClassifier\n\ngbc = GradientBoostingClassifier()\ngbc.fit(x_train, y_train)\ny_pred = gbc.predict(x_val)\nacc_gbc = round(accuracy_score(y_pred, y_val)*100, 2)\nacc_gbc","a9c8f089":"# AdaBoost Classifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\nada = AdaBoostClassifier()\nada.fit(x_train, y_train)\ny_pred = ada.predict(x_val)\nacc_ada = round(accuracy_score(y_pred, y_val)*100, 2)\nacc_ada","8d0a2775":"models = pd.DataFrame({\n    'Model': ['Support Vector Machines', 'KNN', 'Logistic Regression', \n              'Random Forest', 'Naive Bayes', 'Perceptron', 'Linear SVC', \n              'Decision Tree', 'Gradient Boosting Classifier', 'AdaBoost Classifier'],\n    'Score': [acc_svc, acc_knn, acc_logreg, \n              acc_random_forest, acc_gaussian, acc_perceptron,acc_linear_svc, acc_decision_tree, acc_gbc, acc_ada]})\nmodels.sort_values(by='Score', ascending=False)","e315d65c":"#set ids as PassengerId and predict survival \nids = test['PassengerId']\npredictions = ada.predict(test.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission.csv', index=False)","998f54a4":"As we can see from above heatmaps, no missing values left. So, our data is now cleaned","03224b2a":"As we can see apart from above mentioned missing values, no NaN value exist.\n\n### Some Predictions:\n* Sex: Females are more likely to survive.\n* SibSp\/Parch: People traveling alone are more likely to survive.\n* Age: Young children are more likely to survive.\n* Pclass: People of higher socioeconomic class are more likely to survive.","409d3711":"* There area total of 418 passengers.\n* Fare and Age feature has some missing values, needed to fill in.","309040b9":"### Sex Feature","274f1795":"As previously predictd, females have much higher chances of survival than males.\nAge feature is important for our predictions.","00e264b5":"### Age Feature","2d007c8f":"# 4) Data Visualization\nData Visualization is one of the best technique to check wether our prediction is correct or not.","69a9f7de":"### Testing Different Models\n* Gaussian Naive Bayes\n* Logistic Regression\n* Support Vector Machines(SVC, Linear SVC)\n* Perceptron\n* Decision Tree Classifier\n* Random Forest Classifier\n* KNN or k-Nearest Neighbors\n* Gradient Boosting Classifier\n\nFor each model, we set the model, fit it with 80% of our training data, predict for 20% of the training data and check the accuracy.","1a9a8440":"Let's compare the accuracy score of each model","7f2dffce":"## Removing Missing values\n  ### 1. **Embarked Feature**","62552c29":"### 2. **Age Feature**","c37cbbba":"### Sex Feature","4f966c40":"Next, we'll try to predict the missing Age values from the most common age for their Title.","2e81a89e":"### Pclass Feature","0204dd85":"People with less than four parents or children aboard are more likely to survive than those with four or more. Again, people traveling alone are less likely to survive than those with 1-3 parents or children.","35f68b96":"## Splitting the training Dataset","e47823a5":"It's clear that the majority of people embarked in Southampton (S). Let's go ahead and fill in the missing values with S.","d10f0298":"# 5) Cleaning the dataset","9626336d":"# 1) Import Required Libraries\nAt the first step, import all the required libraries.","714f7bb7":"### Embarked Feature","9c8b557e":"# 6) Model Selection","3436349b":"# 7) Creating Submission File\nNow, lets create the final submission.csv file for the competition.","b00c2ef1":"## Removing non useful features","72927c24":"### Name Feature\nWe can drop the name feature now that we've extracted the titles.","9e5c78e8":"Babies are more likely to survive than any other age group.","18834420":"# Titanic Survival Prediction\n\n### Contents:\n*     Import required libraries\n*     Read and Explore the dataset\n*     Data Analysis\n*     Data Visualization\n*     Data Cleaning\n*     Model Selection\n*     Creating Submission File","6153f464":"### Looking at test dataset","fcd81138":"# 2) Reading and Exploring the dataset\nAt the second step, import the data using pd.read_csv, and have a first look on the dataset using describe() function.","12cc0b43":"### Fare Feature\nSeparate the fare values into some logical groups as well as filling in the single missing value in the test dataset.","22ee2dda":"In general, it's clear that people with more siblings or spouses aboard were less likely to survive. However, contrary to expectations, people with no siblings or spouses were less to likely to survive than those with one or two. (34.5% vs 53.4% vs. 46.4%)","3510c108":"# 3) Data Analysis","d839bfa8":"Next we'll fill in the missing values in the Age feature. Since a higher percentage of values are missing, it would be illogical to fill all of them with the same value (as we did with Embarked). Instead, let's try to find a way to predict the missing ages.","58323ec3":"As predicted, people with higher socioeconomic class had a higher rate of survival. (62.9% vs. 47.3% vs. 24.2%)","197553ce":"* **Numerical Variables:** Age(Continuous), Fare(Continuous), SibSp(Descrete), Parch(Descrete)\n* **Categorical Variables:** Sex, Survived, Embarked, Pclass\n* **Alphanumeric Variables:** Ticket, Cabin\n\n### Some Observations:\n* There are a total of 891 passengers in our training set.\n* The Age feature is missing approximately 20% of its values.Age feature is pretty important to survival, so we should probably attempt to fill these gaps.\n* The Cabin feature has most of its value missing so it will be hard to fill these gaps. Therefore, we are going to drop this variable.\n* The Embarked feature is missing 0.22% of its values, which should be relatively harmless.","483d1021":"### Check for any missing data in both datasets","7786cd75":"### Parch Feature","ff42f212":"### SibSp Feature"}}