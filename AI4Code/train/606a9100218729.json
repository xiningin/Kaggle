{"cell_type":{"c1f42f9a":"code","3d0807f7":"code","e4318b63":"code","a92b0176":"code","4b7f4d18":"code","f76a74be":"code","4685258a":"code","f1df3722":"code","e08002af":"code","6a0d517d":"code","8e82f765":"code","231bfb5d":"code","a717be53":"code","fe7f921a":"code","8e4fca7a":"code","ed988dd8":"code","897b99e4":"code","3a567aaf":"code","4f2f2695":"code","497c1739":"code","bc98d13b":"code","2d716c53":"code","b25da07d":"code","eba81cb5":"code","121fbef0":"code","44849ff7":"code","2fa047b4":"code","8114ea87":"code","83d4ca61":"code","528a65d2":"code","bbda2741":"code","e672c22c":"code","54d5a6de":"code","e7027f09":"code","272c66b4":"code","1a57c555":"code","ad8aff3a":"code","6217907e":"code","9b595938":"code","adc6d1d6":"code","636580cc":"code","b1ceec7e":"code","f30ca102":"code","f6115de6":"code","1a52f161":"code","ab7f7535":"code","15b641c2":"code","9906b68e":"code","d6a1c1c2":"code","44cc5550":"code","a2b751dc":"code","9cfb9f1a":"code","7d420c88":"code","8e70c026":"code","e8cf109a":"code","a99e632c":"code","020b5e03":"code","89b9a167":"code","6720d89a":"code","a5fb18ef":"code","7858c514":"code","b2bc22c2":"code","2abad634":"code","aa79a5ca":"code","94c7c7c1":"code","e8e45f85":"code","c6a43076":"code","fa39aa15":"code","47dc95cf":"code","fe28f921":"code","6e9c104c":"code","bb74c1a8":"code","035bf79e":"code","ac2192de":"code","07a3a740":"code","085c82e5":"code","27d6b18a":"code","cfa1c61b":"code","a752e6c2":"code","039f4cad":"code","1f5e5f6a":"code","d4138cf7":"markdown","1af16217":"markdown","d435477e":"markdown","dc29bd09":"markdown","6725138c":"markdown","ac1af7f5":"markdown","3bf35235":"markdown","003c276b":"markdown","4b63953a":"markdown","52438bc5":"markdown","091387ca":"markdown","2b87f0b2":"markdown","305a4787":"markdown","53d57e9e":"markdown","3cbd766f":"markdown","b4255ff7":"markdown","aab41c72":"markdown","1853ba5b":"markdown","73ffa286":"markdown","bf775174":"markdown","b2e09889":"markdown","c9187f6f":"markdown","09f2d024":"markdown","7ecef05a":"markdown","ab98c817":"markdown","afbf7385":"markdown","34f86df7":"markdown","e63777ea":"markdown","07c33d89":"markdown","623b56fc":"markdown","58bae165":"markdown","22b08d9a":"markdown","9f656cde":"markdown","4c7960b3":"markdown","8bbb5e5c":"markdown","d5ff8175":"markdown","7deb0139":"markdown","1f624b02":"markdown","c9780535":"markdown","2551f8cf":"markdown","46e2ac76":"markdown","a24be4b5":"markdown","c3afcf1d":"markdown","b538e109":"markdown","d2f6ded7":"markdown","08c50463":"markdown","17556afc":"markdown","033cfe35":"markdown","ef0f26fd":"markdown","31f98a42":"markdown","92b0d006":"markdown","28b431cb":"markdown","edfecc23":"markdown","129b8217":"markdown","68c7b2ac":"markdown","21a7b324":"markdown","78d8231c":"markdown","686b8c04":"markdown","90e58d1a":"markdown","2b8bae02":"markdown","c004bbe4":"markdown","8344f143":"markdown","99fba113":"markdown","83cea506":"markdown","65f7a6d7":"markdown","4df45d76":"markdown","bdef0f29":"markdown"},"source":{"c1f42f9a":"# Ignore  the warnings\nimport warnings\nwarnings.filterwarnings('always')\nwarnings.filterwarnings('ignore')\n\n# data visualisation and manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nfrom matplotlib.legend_handler import HandlerBase\nimport seaborn as sns\nimport missingno as msno\n#configure\n# sets matplotlib to inline and displays graphs below the corressponding cell.\n%matplotlib inline  \nstyle.use('fivethirtyeight')\nsns.set(style='whitegrid',color_codes=True)\n\n#import the necessary modelling algos.\n\n#classifiaction.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC,SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n#regression\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV,ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\n\n#model selection\nfrom sklearn.model_selection import train_test_split,cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\n\n#evaluation metrics\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error # for regression\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score  # for classification\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew   # specifically for staistics","3d0807f7":"train=pd.read_csv(r'..\/input\/train.csv')\ntest=pd.read_csv(r'..\/input\/test.csv')","e4318b63":"train.head(10)\n#test.head(10)","a92b0176":"df=train.copy()\n#df.head(10)\ndf.shape","4b7f4d18":"df.drop(['Id'],axis=1,inplace=True)\ntest.drop(['Id'],axis=1,inplace=True)","f76a74be":"df.index # the indices of the rows.","4685258a":"df.columns ","f1df3722":"df.isnull().any()","e08002af":"msno.matrix(df) # just to visulaize. ","6a0d517d":"cat_df=df.select_dtypes(include='object')","8e82f765":"cat_df.head(10)\ncat_df.shape","231bfb5d":"cat_df.columns   # list of the categorical columns.","a717be53":"num_df=df.select_dtypes(include='number')\nnum_df.shape","fe7f921a":"num_df.columns # list of numeric columns.","8e4fca7a":"nan_df=df.loc[:, df.isna().any()]\nnan_df.shape\nnan_df.columns   # list of columns with missing values.","ed988dd8":"all_data=pd.concat([train,test])","897b99e4":"print(all_data.shape)\nall_data = all_data.reset_index(drop=True)","3a567aaf":"all_data.head(10)","4f2f2695":"print(all_data.loc[1461:,'SalePrice'])  \n# note that it is Nan for the values in test set as expected. so we drop it here for now.\nall_data.drop(['SalePrice'],axis=1,inplace=True)\n","497c1739":"# analyzing the target variable ie 'Saleprice'\nsns.distplot(a=df['SalePrice'],color='#ff4125',axlabel=False).set_title('Sale Price')","bc98d13b":"#Get also the qq-plot (the quantile-quantile plot)\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","2d716c53":"df['SalePrice']=np.log1p(df['SalePrice']) ","b25da07d":"# now again see the distribution.\nsns.distplot(a=df['SalePrice'],color='#ff4125',axlabel=False).set_title('log(1+SalePrice)')  # better.\n","eba81cb5":"cor_mat= df[:].corr()\ncor_with_tar=cor_mat.sort_values(['SalePrice'],ascending=False)","121fbef0":"print(\"The most relevant features (numeric) for the target are :\")\ncor_with_tar.SalePrice","44849ff7":"# using a corelation map to visualize features with high corelation.\ncor_mat= df[['OverallQual','GrLivArea','GarageCars','GarageArea','TotalBsmtSF','1stFlrSF','FullBath',\n             'YearBuilt','YearRemodAdd','GarageYrBlt','TotRmsAbvGrd','SalePrice']].corr()\nmask = np.array(cor_mat)\nmask[np.tril_indices_from(mask)] = False\nfig=plt.gcf()\nfig.set_size_inches(30,12)\nsns.heatmap(data=cor_mat,mask=mask,square=True,annot=True,cbar=True)\n\n# some inference section.","2fa047b4":"def plot_num(feature):\n    fig,axes=plt.subplots(1,2)\n    sns.boxplot(data=df,x=feature,ax=axes[0])\n    sns.distplot(a=df[feature],ax=axes[1],color='#ff4125')\n    fig.set_size_inches(15,5)","8114ea87":"plot_num('GrLivArea')","83d4ca61":"plot_num('GarageArea')","528a65d2":"plot_num('TotalBsmtSF') ","bbda2741":"def plot_cat(feature):\n  sns.countplot(data=df,x=feature)\n  ax=sns.countplot(data=df,x=feature)\n   ","e672c22c":"plot_cat('OverallQual')","54d5a6de":"plot_cat('FullBath')","e7027f09":"plot_cat('YearBuilt')","272c66b4":"plot_cat('TotRmsAbvGrd') # most of the houses have 5-7 rooms above the grd floor.","1a57c555":"plot_cat('GarageCars')","ad8aff3a":"sns.factorplot(data=df,x='Neighborhood',kind='count',size=10,aspect=1.5)","6217907e":"fig, ax = plt.subplots()\nax.scatter(x = df['GrLivArea'], y = df['SalePrice'])\nplt.ylabel('SalePrice')\nplt.xlabel('GrLivArea')\nplt.show()","9b595938":"df = df.drop(df[(df['GrLivArea']>4000) & (df['SalePrice']<13)].index) # removing some outliers on lower right side.","adc6d1d6":"# again checking\nfig, ax = plt.subplots()\nax.scatter(x = df['GrLivArea'], y = df['SalePrice'])\nplt.ylabel('SalePrice')\nplt.xlabel('GrLivArea')\nplt.show()","636580cc":"# garage area\nfig, ax = plt.subplots()\nax.scatter(x =(df['GarageArea']), y = df['SalePrice'])\nplt.ylabel('SalePrice')\nplt.xlabel('GarageArea')\nplt.show()\n# can try to fremove the points with gargae rea > than 1200.","b1ceec7e":"# basment area\nfig, ax = plt.subplots()\nax.scatter(x =(df['TotalBsmtSF']), y = df['SalePrice'])\nplt.ylabel('SalePrice')\nplt.xlabel('TotalBsmtSF')\nplt.show()   # check >3000 can leave here.","f30ca102":"#overall qual\nsns.factorplot(data=df,x='OverallQual',y='SalePrice',kind='box',size=5,aspect=1.5)","f6115de6":"#garage cars\nsns.factorplot(data=df,x='GarageCars',y='SalePrice',kind='box',size=5,aspect=1.5)","1a52f161":"#no of rooms\nsns.factorplot(data=df,x='TotRmsAbvGrd',y='SalePrice',kind='bar',size=5,aspect=1.5) # increasing rooms imply increasing SalePrice as expected.","ab7f7535":"#neighborhood\nsns.factorplot(data=df,x='Neighborhood',y='SalePrice',kind='box',size=10,aspect=1.5)","15b641c2":"#sale conditioin\nsns.factorplot(data=df,x='SaleCondition',y='SalePrice',kind='box',size=10,aspect=1.5)","9906b68e":"nan_all_data = (all_data.isnull().sum())\nnan_all_data= nan_all_data.drop(nan_all_data[nan_all_data== 0].index).sort_values(ascending=False)\nnan_all_data\nmiss_df = pd.DataFrame({'Missing Ratio' :nan_all_data})\nmiss_df\n","d6a1c1c2":"#delet some features withvery high number of missing values.  \nall_data.drop(['PoolQC','Alley','Fence','Id','MiscFeature'],axis=1,inplace=True)\n","44cc5550":"test.drop(['PoolQC','Alley','Fence','MiscFeature'],axis=1,inplace=True)\ndf.drop(['PoolQC','Alley','Fence','MiscFeature'],axis=1,inplace=True)","a2b751dc":"# FireplaceQu\n# it is useful but many of the values nearly half are missing makes no sense to fill half of the values. so deleting this\nall_data.drop(['FireplaceQu'],axis=1,inplace=True)\ntest.drop(['FireplaceQu'],axis=1,inplace=True)\ndf.drop(['FireplaceQu'],axis=1,inplace=True)\n","9cfb9f1a":"#Lot Frontage\nprint(df['LotFrontage'].dtype)\nplt.scatter(x=np.log1p(df['LotFrontage']),y=df['SalePrice'])\ncr=df.corr()\nprint(df['LotFrontage'].describe())\nprint(\"The corelation of the LotFrontage with the Target : \" , cr.loc['LotFrontage','SalePrice'])\n","7d420c88":"all_data['LotFrontage'].fillna(np.mean(all_data['LotFrontage']),inplace=True)\nall_data['LotFrontage'].isna().sum()","8e70c026":"#Garage  related features.\n# these features eg like garage qual,cond,finish,type seems to be important and relevant for buying car. \n# hence I will not drop these features insted i will fill them with the 'none' for categorical and 0 for numeric as nan here implies that there is no garage.\n\nall_data['GarageYrBlt'].fillna(0,inplace=True)\nprint(all_data['GarageYrBlt'].isnull().sum())\n\nall_data['GarageArea'].fillna(0,inplace=True)\nprint(all_data['GarageArea'].isnull().sum())\n\nall_data['GarageCars'].fillna(0,inplace=True)\nprint(all_data['GarageCars'].isnull().sum())\n\nall_data['GarageQual'].fillna('None',inplace=True)   # creating a separate category 'none' which means no garage.\nprint(all_data['GarageQual'].isnull().sum())\n\nall_data['GarageFinish'].fillna('None',inplace=True)   # creating a separate category 'none' which means no garage.\nprint(all_data['GarageFinish'].isnull().sum())\n\nall_data['GarageCond'].fillna('None',inplace=True)   # creating a separate category 'none' which means no garage.\nprint(all_data['GarageCond'].isnull().sum())\n\nall_data['GarageType'].fillna('None',inplace=True)   # creating a separate category 'none' which means no garage.\nprint(all_data['GarageType'].isnull().sum())\n\n","e8cf109a":"# basement related features.\n#missing values are likely zero for having no basement\n\nfor col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col].fillna(0,inplace=True)\n    \n# for categorical features we will create a separate class 'none' as before.\n\nfor col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col].fillna('None',inplace=True)\n    \nprint(all_data['TotalBsmtSF'].isnull().sum())\n\n","a99e632c":"# MasVnrArea 0 and MasVnrType 'None'.\nall_data['MasVnrArea'].fillna(0,inplace=True)\nprint(all_data['MasVnrArea'].isnull().sum())\n\nall_data['MasVnrType'].fillna('None',inplace=True)\nprint(all_data['MasVnrType'].isnull().sum())","020b5e03":"#MSZoning.\n# Here nan does not mean no so I will with the most common one ie the mode.\nall_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0],inplace=True)\nprint(all_data['MSZoning'].isnull().sum())","89b9a167":"# utilities\nsns.factorplot(data=df,kind='box',x='Utilities',y='SalePrice',size=5,aspect=1.5)","6720d89a":"all_data.drop(['Utilities'],axis=1,inplace=True)","a5fb18ef":"#functional\n# fill with mode\nall_data['Functional'].fillna(all_data['Functional'].mode()[0],inplace=True)\nprint(all_data['Functional'].isnull().sum())","7858c514":"# other rem columns rae all cat like kitchen qual etc.. and so filled with mode.\nfor col in ['SaleType','KitchenQual','Exterior2nd','Exterior1st','Electrical']:\n  all_data[col].fillna(all_data[col].mode()[0],inplace=True)\n  print(all_data[col].isnull().sum())","b2bc22c2":"nan_all_data = (all_data.isnull().sum())\nnan_all_data= nan_all_data.drop(nan_all_data[nan_all_data== 0].index).sort_values(ascending=False)\nnan_all_data\nmiss_df = pd.DataFrame({'Missing Ratio' :nan_all_data})\nmiss_df\n\n","2abad634":"all_data.shape","aa79a5ca":"#log transform skewed numeric features:\nnumeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\nskewed_feats = train[numeric_feats].apply(lambda x: skew(x.dropna())) #compute skewness\nskewed_feats = skewed_feats[skewed_feats > 0.50]\nskewed_feats = skewed_feats.index\n\nall_data[skewed_feats] = np.log1p(all_data[skewed_feats])","94c7c7c1":"for col in all_data.columns:\n    if(all_data[col].dtype == 'object'):\n        le=LabelEncoder()\n        all_data[col]=le.fit_transform(all_data[col])","e8e45f85":"train=all_data.loc[:(df.shape)[0]+2,:]\ntest=all_data.loc[(df.shape)[0]+2:,:]","c6a43076":"train['SalePrice']=df['SalePrice']\ntrain['SalePrice'].fillna(np.mean(train['SalePrice']),inplace=True)\ntrain.shape\nprint(train['SalePrice'].isnull().sum())","fa39aa15":"print(train.shape)\nprint(test.shape)","47dc95cf":"x_train,x_test,y_train,y_test=train_test_split(train.drop(['SalePrice'],axis=1),train['SalePrice'],test_size=0.20,random_state=42)","fe28f921":"reg_lin=LinearRegression()\nreg_lin.fit(x_train,y_train)\npred=reg_lin.predict(x_test)\nprint(np.sqrt(mean_squared_error(y_test,pred)))","6e9c104c":"reg_lasso=Lasso()\nreg_lasso.fit(x_train,y_train)\npred=reg_lasso.predict(x_test)\nprint(np.sqrt(mean_squared_error(y_test,pred)))","bb74c1a8":"params_dict={'alpha':[0.001, 0.005, 0.01,0.05,0.1,0.5,1]}\nreg_lasso_CV=GridSearchCV(estimator=Lasso(),param_grid=params_dict,scoring='neg_mean_squared_error',cv=10)\nreg_lasso_CV.fit(x_train,y_train)\npred=reg_lasso_CV.predict(x_test)\nprint(np.sqrt(mean_squared_error(y_test,pred)))","035bf79e":"reg_lasso_CV.best_params_","ac2192de":"reg_ridge=Ridge()\nreg_ridge.fit(x_train,y_train)\npred=reg_ridge.predict(x_test)\nprint(np.sqrt(mean_squared_error(y_test,pred)))","07a3a740":"params_dict={'alpha':[0.1, 0.15, 0.20,0.25,0.30,0.35,0.4,0.45,0.50,0.55,0.60]}\nreg_ridge_CV=GridSearchCV(estimator=Ridge(),param_grid=params_dict,scoring='neg_mean_squared_error',cv=10)\nreg_ridge_CV.fit(x_train,y_train)\npred=reg_ridge_CV.predict(x_test)\nprint(np.sqrt(mean_squared_error(y_test,pred)))","085c82e5":"reg_ridge_CV.best_params_","27d6b18a":"#the params are tuned with grid searchCV.\n\nreg_gb=GradientBoostingRegressor(n_estimators=2000,learning_rate=0.05,max_depth=3,min_samples_split=10,max_features='sqrt',subsample=0.75 ,loss='huber')\nreg_gb.fit(x_train,y_train)\npred=reg_gb.predict(x_test)\nprint(np.sqrt(mean_squared_error(y_test,pred)))","cfa1c61b":"import xgboost as xgb\nmodel_xgb = xgb.XGBRegressor(colsample_bytree=0.4603, gamma=0.0468, \n                             learning_rate=0.05, max_depth=3, \n                             min_child_weight=1.7817, n_estimators=2200,\n                             reg_alpha=0.4640, reg_lambda=0.8571,\n                             subsample=0.5213, silent=1,\n                             random_state =7, nthread = -1)\nmodel_xgb.fit(x_train,y_train)\npred=model_xgb.predict(x_test)\nprint(np.sqrt(mean_squared_error(y_test,pred)))","a752e6c2":"# predictions on the test set.\n \npred=reg_gb.predict(test)\npred_act=np.exp(pred)\npred_act=pred_act-1\nlen(pred_act)","039f4cad":"test_id=[]\nfor i in range (1461,2920):\n    test_id.append(i)\nd={'Id':test_id,'SalePrice':pred_act}\nans_df=pd.DataFrame(d)\nans_df.head(10)","1f5e5f6a":"ans_df.to_csv('answer.csv',index=False)","d4138cf7":"## 2.2 ) Check for Missing Values","1af16217":"<a id=\"content6\"><\/a>\n## 6 ) Regression Models","d435477e":"<a id=\"content1\"><\/a>\n## 1) Importing the Modules and Loading the Dataset","dc29bd09":"<a id=\"content2\"><\/a>\n## 2) Exploratory Data Analysis (EDA)","6725138c":"#### Finally no null value remain now;)","ac1af7f5":"<a id=\"content4\"><\/a>\n## 4 ) Handling Skewness","3bf35235":"Most of them are in 'average','above average' or 'good' classes.","003c276b":"In this section of the notebook I  have handled the missing values in the columns.\n\nFirstly I have droped a couple of columns that have a really high % of missing values.\n\nFor other features I have analyzed if it that feaure is important or not and accordingly either have drooped it or imputed the values in it.\n\nFor imputation I have considered the meaning of the corressponding feature from the description. Like for a categorical feature if values are missing I have imputed \"None\" just to mark a separate category meaning absence of that thing. Similarly for a numeric feature I have imputed with 0 in case the missing value implies the 'absence' of that feature.\n\nIn all other cases I have imputed the categorical features with 'mode' i.e the most frequent class and with 'mean' for the numeric features.","4b63953a":"[ **3 ) Missing Values Treatment**](#content3)","52438bc5":"[ **1 ) Importing the Modules and Loading the Dataset**](#content1)","091387ca":"#### Note that training set has only 2 of the possible 4 categories (ALLPub and NoSeWa) while test set has other categories. Hence it is of no use to us.","2b87f0b2":"#### NUMERIC FEATURES","305a4787":"Price varies with neighborhood.More posh areas of the city will have more price as expected.","53d57e9e":"#### LASSO (and tuning with GridSearchCV)","3cbd766f":"## 2.4 ) Analyzing the Target i.e. 'SalePrice'","b4255ff7":"## 2.7 ) Bivariate Analysis","aab41c72":"#### Note the features are a bit right skewed. We can therefore take 'log transform' of the features or a BoXCox transformation. Both shall work well. ","1853ba5b":"#### FEATURES WITH MISSING VALUES","73ffa286":"#### INFERENCES--\n\n1. Note that some of the features have quite high corelation with the target. These features are really significant.\n\n2. Of these the features with corelation value >0.5 are really important. Some features like GrLivArea etc.. are even more important.\n\n3. We will consider these features (i.e. GrLivArea,OverallQual) etc.. in more detail in subsequent sections during univariate and bivariate analysis.","bf775174":"<a id=\"content3\"><\/a>\n## 3 ) Missing Values Treatment","b2e09889":"## 5.2 ) Splitting into Training and Validation Sets","c9187f6f":"[ **6 ) Regression Models**](#content6)","09f2d024":"#### ALSO LINEAR REGRESSION IS BASED ON THE ASSUMPTION OF THE 'HOMOSCADESITY' AND HENCE TAKING LOG WILL  BE A GOOD IDEA TO ENSURE 'HOMOSCADESITY' (that the varince of errors is constant.). A bit scary but simple ;) \n\n**You can read more about this on wikipedia.**","7ecef05a":"## **Housing Prices Advanced Regression : EDA and Regression Models**\n\nIn this notebook I have performed thorough EDA on the housing dataset and tried to identify some keen underlying trends. Dtaa pre-processing is done after which different models have been applied. This kernel is an introduction to predictive modelling and demonstrates the various techniques. And it is must see if you are a beginner in regression and predictive modelling like me ;)\n\nLet's dive in !!!.","ab98c817":"####  TAKING 'Log Transform' OF THE TARGET","afbf7385":"[ **4 ) Handling Skewness of Features**](#content4)","34f86df7":"#### NUMERIC FEATURES","e63777ea":"#### LINEAR REGRESSION","07c33d89":"We can drop the 'Id' column as the frames are already indexed.","623b56fc":"<a id=\"content7\"><\/a>\n## 7 ) Saving and Making Submission to Kaggle","58bae165":"Lastly it is the time to apply various regression models and check how are we doing. I have used various regression models from the scikit.\n\nParameter tuning using GridSearchCV is also done to improve performance of some algos.","22b08d9a":"The SalePrice increases with the overall quality as expected.","9f656cde":"**Similar inferences can be drawn from other plots and graphs.**","4c7960b3":"Might be useful when we consider features of different data types.","8bbb5e5c":"## 5.1 ) LabelEncode the Categorical Features","d5ff8175":"#### **The distribution of target is a bit right skewed. Hence taking the 'log transform' is a reasonable option.**","7deb0139":"## 2.3 ) Separate Dataframes (depending on data type)","1f624b02":"For handling skewnesss I will take the log transform of the features with skewness > 0.5.\n\nYou can also try the BoxCox transformation as mentioned before.","c9780535":"## [Please star\/ upvote if u liked it.]","2551f8cf":"**Note the significant decrease in the RMSE on tuning the Lasso Regression.**","46e2ac76":"## 2.5 ) Most Related Features to the Target","a24be4b5":"#### NUMERIC FEATURES","c3afcf1d":"#### Lastly checking if any null value still remains.","b538e109":"#### MERGING THE TRAIN & TEST SETS","d2f6ded7":"Note that the parameters aren't optimized. This can get a lot better tahn this for sure.","08c50463":"<a id=\"content5\"><\/a>\n## 5 ) Prepare the Data","17556afc":"#### CATEGORICAL FEATURES","033cfe35":"## THE END!!!","ef0f26fd":"In this section the Bivariate Analysis have been done. I have plotted various numeric as well as categorical features against the target ie 'SalePrice'.","31f98a42":"#### RIDGE (and tuning with GridSearchCV)","92b0d006":"#### The evalauton metric that I have used is the Root Mean Squared Error between the 'Log of the actual price' and 'Log of the predicted value' which is also the evaluation metric used by the kaggle.\n\n#### To get abetter idea one may also use the K-fold cross validation insteadof the normal holdout set approach to cross validation.","28b431cb":"* #### Many columns have missing values and that will be treated later in the notebook.","edfecc23":"[ **2 ) Exploratory Data Analysis (EDA)**](#content2)","129b8217":"## 2.6 ) Univariate Analysis","68c7b2ac":"#### XGBoost","21a7b324":"## 2.1 ) The Features and the 'Target' variable","78d8231c":"#### GRADIENT BOOSTING","686b8c04":"#### Lastly we plot the countplot for some important features that are numerical here but are actually categorica. It seems if they have been label encoded.","90e58d1a":"#### Note that there are two outliers on the lower right hand side and can remove them.","2b8bae02":"[ **7 ) Saving and Making Submission to Kaggle**](#content7)","c004bbe4":"[ **5 ) Prepare the Data**](#content5)","8344f143":"**The Gradient Boosting gives the best performance on the validation set and so I am using it to make predictions to Kaggle (on the test set).**","99fba113":"#### CATEGORICAL FEATURES","83cea506":"In this section the univariate analysis is performed; More importantly I have considered the features that are more importanht with the 'Target' that  have high corelation with the Target.\n\nFor the numeric features I have used a 'distplot' and 'boxplot' to analyze their distribution.\n\nSimilarly for categorical features the most reasonable way to visualize the distribution is to use a 'countplot' which shows the relative counts for each category or class. Can use a pie-plot also to be a bit more fancy.","65f7a6d7":"#### Above analysis shows that there is some relation of LotArea with the SalePrice both by scatter plot and also by the corelation value. Therefore instead of deleting I will impute the values with the mean for now.","4df45d76":"## CONTENTS::","bdef0f29":"#### CATEGORICAL FEATURES"}}