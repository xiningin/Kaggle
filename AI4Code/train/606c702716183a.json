{"cell_type":{"96f71442":"code","4e23e935":"code","f203f149":"code","9615ffa0":"code","f17f091d":"code","e64b3fa4":"code","03e7285e":"code","d5f23d31":"code","3174127c":"code","5993226c":"code","42c53e2d":"code","8ae306ad":"code","7d6340c7":"code","2a6c911e":"code","dc6fb1ee":"code","bbf307e0":"code","2c73a9b7":"code","9b04575b":"code","94724317":"code","1606a324":"code","2da1631f":"code","1328cf79":"code","9d5ec5e6":"code","cf755262":"code","bbee09d6":"code","cc32da87":"code","0bfa88c4":"code","a2700fa5":"code","eb3b0f8d":"code","ff2d291c":"code","ce8b7815":"code","67f9684b":"markdown","8bc65d40":"markdown","1b778751":"markdown","1eae2bb6":"markdown","ac32f424":"markdown","663838dd":"markdown","14e56a53":"markdown","86aa95d9":"markdown","49dbdf38":"markdown","ab17a997":"markdown","82847bd6":"markdown","56efdf5b":"markdown","86aa441b":"markdown","900dd654":"markdown","445414ec":"markdown","12fa00f0":"markdown","ed2c1410":"markdown","54afe27c":"markdown","2b00ea9a":"markdown","819216f9":"markdown","ab513427":"markdown","d47c9aea":"markdown","29567f21":"markdown","9c49219d":"markdown","cac28fb9":"markdown","3feb6a33":"markdown","2bbcaa2e":"markdown","82e79c9f":"markdown","92af973b":"markdown","8d771f52":"markdown","0c7bb1b6":"markdown","3e11fe72":"markdown"},"source":{"96f71442":"# Importing the necessary libraries for data analysis\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt","4e23e935":"# Loading the dataset\ndataset = pd.read_csv('..\/input\/bank-marketing-data-set\/bank-direct-marketing-campaigns.csv')","f203f149":"# Checking the data itself\nprint(dataset.head, dataset.tail, sep='\\n\\n\\n')","9615ffa0":"# Checking the columns and datatypes\nprint(dataset.columns, dataset.dtypes, sep='\\n\\n\\n')","f17f091d":"# Checking the values of the output col\ndataset.groupby('y').size()","e64b3fa4":"# Checking the ages of the clients\ndataset.groupby('age').size()","03e7285e":"# Creating a histogram for the ages\nplt.hist(dataset['age'])\nplt.xlabel('Age of the clients')\nplt.ylabel('Number of clients')","d5f23d31":"# Splitting the data to an X array (input independent variables) and y list (output - dependent variable).\nX = dataset.iloc[:, :-1].values # It will be overwritten later for some column transformation reasons\ny = dataset.iloc[:, -1].values","3174127c":"print(X[0:2], y[0:2], sep='\\n\\n\\n')","5993226c":"# Encoding y to bin values\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\ny = le.fit_transform(y)","42c53e2d":"# Encoding the categorical cols of X with OneHotEncoder, and standardizing the numerical cols of X with StandardScaler for better prediction\nfrom sklearn.compose import make_column_transformer, make_column_selector\nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler\n\nct = make_column_transformer(\n    (StandardScaler(),\n    make_column_selector(dtype_include=np.number)),\n    (OneHotEncoder(),\n    make_column_selector(dtype_include=object)))\n    \nX = np.array(ct.fit_transform(dataset.iloc[:, :-1]))","8ae306ad":"X[1][:]","7d6340c7":"# As the next step of preprocessing I splitted the data to test and train sets (test size is 20% which means about 8 000 rows)\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1)","2a6c911e":"X_test, y_test","dc6fb1ee":"y_train","bbf307e0":"from sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state=1,solver='liblinear')\nclassifier.fit(X_train, y_train)\n\nprint(classifier)","2c73a9b7":"y_pred = classifier.predict(X_test)\n# np.set_printoptions(precision=2)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","9b04575b":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint(cm)\nacc_score = accuracy_score(y_test, y_pred)\nprint('\\n')\n\ntn, fp, fn, tp = confusion_matrix(y_true=y_test, y_pred=y_pred).ravel()\nprint('tn fp fn tp')\nprint(tn, fp, fn, tp)\nprint('\\n')\nprint(\"The accuracy of the logistic regression classification model is {0} %\".format(round(acc_score*100,2)))","94724317":"# For this classifier I had to specify the number of neighbors. \n# First I tried it with the default 5, then 10, 100 and 500. I realized that the best result (which is the closest one to 90%) is between 100 and 500, so I also tried with 200.\n# Finally, I sticked with 100 as that number of neighbors reached the best accuracy %.\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors=100)\nclassifier.fit(X_train, y_train)","1606a324":"y_pred = classifier.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","2da1631f":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint(cm)\nacc_score = accuracy_score(y_test, y_pred)\nprint('\\n')\n\ntn, fp, fn, tp = confusion_matrix(y_true=y_test, y_pred=y_pred).ravel()\nprint('tn fp fn tp')\nprint(tn, fp, fn, tp)\nprint('\\n')\nprint(\"The accuracy of the kNN classification model is {0} %\".format(round(acc_score*100,2)))","1328cf79":"from sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state=1)\nclassifier.fit(X_train, y_train)","9d5ec5e6":"y_pred = classifier.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","cf755262":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint(cm)\nacc_score = accuracy_score(y_test, y_pred)\nprint('\\n')\n\ntn, fp, fn, tp = confusion_matrix(y_true=y_test, y_pred=y_pred).ravel()\nprint('tn fp fn tp')\nprint(tn, fp, fn, tp)\nprint('\\n')\nprint(\"The accuracy of the decision tree classification model is {0} %\".format(round(acc_score*100,2)))","bbee09d6":"# I also ran the SVM model training with the linear, poly and sigmoid settings, but the rbf kernel option provided the best performance for this classifier\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel='rbf', random_state=1)\nclassifier.fit(X_train, y_train)","cc32da87":"y_pred = classifier.predict(X_test)\nprint(np.concatenate((y_pred.reshape(len(y_pred),1), y_test.reshape(len(y_test),1)),1))","0bfa88c4":"from sklearn.metrics import confusion_matrix, accuracy_score\ncm = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint(cm)\nacc_score = accuracy_score(y_test, y_pred)\nprint('\\n')\n\ntn, fp, fn, tp = confusion_matrix(y_true=y_test, y_pred=y_pred).ravel()\nprint('tn fp fn tp')\nprint(tn, fp, fn, tp)\nprint('\\n')\nprint(\"The accuracy of the SVM classification model is {0} %\".format(round(acc_score*100,2)))","a2700fa5":"# Predicting the value of y for the first row of the dataset\ndataset.iloc[0]","eb3b0f8d":"# The output for the first row of the dataset must be 'no', which is - after the transformation to binary - equals to 0\ny[0]","ff2d291c":"# The inputs for the first row of the dataset after encoding and scaling:\nX[0]","ce8b7815":"# Let's predict the output for this row with the final SVM classifier\nprint(classifier.predict(X[0].reshape(-1, 62)))","67f9684b":"## Data source:\nhttps:\/\/www.kaggle.com\/ruthgn\/bank-marketing-data-set","8bc65d40":"The SVM classifier predicted 0, which equals to 'no'. This means, that the prediction for a client, who is at age of 56, married and has the similar features like the person represented by the first row of the dataset, is the following: he\/she  will probably not subscribe to any term deposit after the call from the bank. \n\nThe result of the classification can be a good opportunity for the bank to reduce the number of calls for the next marketing campaign and find the target customers who will probably subscribe. The machine learning model can provide a reliable prediction about who could and who could not be a future customer of the bank with the help of the marketing campaign.","1b778751":"From the previous histogram we can see that the bank mostly targeted people who are between the age of 25 and 45. It has to be considered during the evaluation of the further predictions.","1eae2bb6":"### 3.3.2 Predicting the test outputs","ac32f424":"## 3.3 Decision Tree","663838dd":"### 3.1.1 Training the LR model","14e56a53":"### 3.4.3 Creating the confusion matrix and the accuracy score of the model on the test set","86aa95d9":"# 3. Classifications","49dbdf38":"### 3.1.2 Predicting the test outputs","ab17a997":"### 3.4.2 Predicting the test outputs","82847bd6":"## 3.1 Logistic Regression","56efdf5b":"### 3.2.1 Training the kNN model","86aa441b":"## 3.4 Support Vector Machine","900dd654":"## 3.2 k-Nearest Neighbors","445414ec":"## 2.1 Splitting data","12fa00f0":"### 3.3.3 Creating the confusion matrix and the accuracy score of the model on the test set","ed2c1410":"# Inspiraton","54afe27c":"To provide clear and readable data to the pedictive model, it is neccessary to encode the values of 'y' to binary (0,1) and the categorical columns of the X array. It is also recommended to standardize the numerical cols in X for the later classification. In this case, I used the LabelEncoder, OneHotEncoder, StandardScaler, make_column_selector and make_column_transformer modules from scikit-learn for preprocessing the data.","2b00ea9a":"My goal was to find the best algorithm for predicting the result of the marketing campaign on a certain client. I tried out 4 classification models, and modified the settings for some of them to see which one provides the best accuracy score. The best model could help the bank find possible target customers during the next marketing campaign.\n\nThis notebook has 4 sections (5, if we include this inspiration part): EDA, Preprocessing, Classifications and the Conclusion.","819216f9":"The SVM model with the rbf kernel setting reached the highest accuracy score amongst the 4 different classification algorithms.\nTo see the model's performance in practice, let's test the model on some inputs from our data and see how the classifier performs. For this purpose, I selected the first row of the original dataset and cut off the last column (which is the dependent variable) to get only the input values. Then I used the predict module of the classifier (the SVM classifier) to see the output of the model for this row. ","ab513427":"# 1. Exploratory data analysis","d47c9aea":"## 2.2 Label encoding","29567f21":"### 3.3.1 Training the DT model","9c49219d":"### 3.2.2 Predicting the test outputs","cac28fb9":"To see which classification model works better on the data, I tried to implement different classifiers:\n1) Logistic Regression\n2) k-Nearest Neighbors\n3) Decision Tree\n4) Support Vector Machine\n\nFinally, I compared the prediction results of these models to find the best amongst them.","3feb6a33":"## 2.4 Splitting data to training and test sets","2bbcaa2e":"### 3.2.3 Creating the confusion matrix and the accuracy score of the model on the test set","82e79c9f":"# 4. Conclusion","92af973b":"### 3.1.3 Creating the confusion matrix and the accuracy score of the model on the test set","8d771f52":"### 3.4.1 Training the SVM model","0c7bb1b6":"## 2.3 Standardizing numerical inputs and encoding categorical inputs","3e11fe72":"# 2. Preprocessing"}}