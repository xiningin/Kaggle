{"cell_type":{"cecdb42a":"code","79ef8553":"code","0e3dc9eb":"code","e4c2c5e8":"code","f661b57e":"code","e9991177":"code","e3f074d0":"code","156a9b23":"code","30de31a3":"code","89f51c92":"code","f18d0f96":"code","09779abc":"code","c5d5db4f":"code","52cc7644":"code","6f08c2ee":"code","a54cac6c":"code","52b1fdca":"code","9c8184d3":"code","999fc11b":"code","4b45cad6":"code","4146c611":"code","23ce451b":"code","e451a6ab":"code","0ddbb300":"code","92ec3b71":"code","1a0603b0":"code","516c6d0d":"code","eeb0c2be":"code","fa187785":"code","29909572":"code","133f5fed":"code","682a5e30":"code","afb6d1cc":"code","3c9b21b1":"code","063af45d":"code","f0c2e49f":"code","e4b65240":"code","da20e11b":"code","6e0d14c6":"code","364d0845":"code","393573e0":"code","552a897b":"code","183054e9":"markdown","c94b6860":"markdown","0abc16db":"markdown","facf1ec5":"markdown","3d87e451":"markdown","60f8ee25":"markdown","7688813a":"markdown","58a216ae":"markdown","edb7e847":"markdown","30ef8fb7":"markdown","897dcc54":"markdown","88e98a47":"markdown","024b0c88":"markdown","0bd3007f":"markdown","ce03f8bd":"markdown","87184a50":"markdown"},"source":{"cecdb42a":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelBinarizer, LabelEncoder, OrdinalEncoder, MinMaxScaler\nfrom sklearn.model_selection import StratifiedShuffleSplit, train_test_split, GridSearchCV\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, precision_score, f1_score, roc_auc_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import precision_recall_fscore_support as score\n\n# Mute the sklearn and IPython warnings\nimport warnings\nwarnings.filterwarnings('ignore', module='sklearn')\nwarnings.filterwarnings('ignore', module='IPython')","79ef8553":"data = pd.read_csv('\/kaggle\/input\/clinvar-conflicting\/clinvar_conflicting.csv', sep=',')\ndata.head()","0e3dc9eb":"data.shape","e4c2c5e8":"data.CLASS.value_counts()","f661b57e":"pd.DataFrame([[i, len(data[i].unique())] for i in data.columns],\n             columns=['Variable', 'Unique Values']).set_index('Variable')","e9991177":"unique_col = pd.DataFrame([[i, len(data[i].unique())] for i in data.columns],\n                          columns=['Variable', 'Unique Values']).set_index('Variable')\n\nto_drop = list(unique_col[unique_col['Unique Values'] > 3000].index)\ndata.drop(to_drop, axis=1, inplace=True)","e3f074d0":"pd.DataFrame([[i, len(data[i].unique())] for i in data.columns],\n             columns=['Variable', 'Unique Values']).set_index('Variable')","156a9b23":"num_missing = data.isnull().sum()\npercentage_missing = data.isnull().sum().apply(lambda x: x\/data.shape[0]*100)","30de31a3":"missing_data = pd.DataFrame({'Number of Missing':  num_missing,\n                             'Percentage of Missing': percentage_missing})\n\nmissing_data['Percentage of Missing'].sort_values(ascending = False)","89f51c92":"drop_list = list(missing_data[missing_data['Percentage of Missing'] >= 20].index)\ndata.drop(drop_list,axis = 1, inplace=True)","f18d0f96":"data.isnull().sum()","09779abc":"plt.figure(figsize = (12, 10))\nsns.heatmap(data.corr(), annot = True, linewidths=.5, cmap = plt.cm.cool)","c5d5db4f":"data.drop(['AF_TGP'],axis = 1, inplace=True)","52cc7644":"# check the types\ndf = pd.DataFrame(data.isnull().sum().astype(int), columns=['Null'])\nnull_list = list(df[df['Null'] != 0].index)\ndata[null_list].dtypes","6f08c2ee":"for x in [\"MC\", \"SYMBOL\", \"Feature_type\", \"Feature\", \n          \"BIOTYPE\", \"STRAND\", \"Amino_acids\", \"Codons\" ]:\n    data[x].fillna(data[x].mode()[0], inplace=True)\n\ndata['LoFtool'].fillna(data['LoFtool'].mean(), inplace=True)\n\ndata.isnull().sum()","a54cac6c":"dg = pd.DataFrame([[str(i),data[i].dtypes == 'object'] for i in data.columns],\n                  columns=['Variable','Object Type']).set_index('Variable')\nobject_columns_names = list(dg[dg['Object Type'] == True].index)","52b1fdca":"#display the number of unique values for columns type object\ndf = data[object_columns_names]\ndf_uniques = pd.DataFrame([[i, len(df[i].unique())] for i in df.columns],\n                          columns=['Variable', 'Unique Values']).set_index('Variable')","9c8184d3":"df_uniques","999fc11b":"binary_variables = list(df_uniques[df_uniques['Unique Values'] == 2].index)\nbinary_variables","4b45cad6":"categorical_variables = list(df_uniques[(df_uniques['Unique Values'] > 2)].index)\ncategorical_variables","4146c611":"for col in categorical_variables:\n    data[col] = data[col].apply(lambda x: str(x))\n\ndata[categorical_variables].dtypes","23ce451b":"numeric_variables = list(set(data.columns) - set(categorical_variables) - set(binary_variables))\ndata[numeric_variables].dtypes","e451a6ab":"lb, le = LabelBinarizer(), LabelEncoder()\n\n#encoding ordinary variables\nfor col in categorical_variables:\n    data[col] = le.fit_transform(data[col])\n\n# binary encoding binary variables\nfor col in binary_variables:\n    data[col] = lb.fit_transform(data[col])","0ddbb300":"data.sample(3)","92ec3b71":"plt.figure(figsize = (30, 15))\nsns.heatmap(data.corr(), annot = True, linewidths=.5, cmap = plt.cm.cool)","1a0603b0":"data.drop([\"ALT\", \"MC\"],axis = 1, inplace=True)\ncategorical_variables.remove('ALT')\ncategorical_variables.remove(\"MC\")","516c6d0d":"mm = MinMaxScaler()\nfor column in [categorical_variables + numeric_variables]:\n    data[column] = mm.fit_transform(data[column])","eeb0c2be":"feature_cols = list(data.columns)\nfeature_cols.remove('CLASS')","fa187785":"# Get the split indexes\nstrat_shuf_split = StratifiedShuffleSplit(n_splits=1, \n                                          test_size=0.3, \n                                          random_state=42)\n\ntrain_idx, test_idx = next(strat_shuf_split.split(data[feature_cols], data.CLASS))\n\n# Create the dataframes\nX_train = data.loc[train_idx, feature_cols]\ny_train = data.loc[train_idx, 'CLASS']\n\nX_test  = data.loc[test_idx, feature_cols]\ny_test  = data.loc[test_idx, 'CLASS']\nlen(X_test), len(X_train)","29909572":"# create dataframe for metrics\nmetrics = pd.DataFrame()\n\n# Standard logistic regression\nlr = LogisticRegression(solver='liblinear').fit(X_train, y_train)\ny_pred_lr = lr.predict(X_test)\n\nprecision_lr, recall_lr = (round(float(x),2) for x in list(score(y_test,\n                                                                    y_pred_lr,\n                                                                    average='weighted'))[:-2])\n# adding lr stats to metrics DataFrame\nlr_stats = pd.Series({'precision':precision_lr,\n                      'recall':recall_lr,\n                      'accuracy':round(accuracy_score(y_test, y_pred_lr), 2),\n                      'f1score':round(f1_score(y_test, y_pred_lr), 2),\n                      'auc': round(roc_auc_score(y_test, y_pred_lr),2)},\n                     name='Logistic Regression')\n# Report outcomes\npd.DataFrame(classification_report(y_test, y_pred_lr, output_dict=True)).iloc[:3,:2]","133f5fed":"# Estimate KNN model and report outcomes\nknn = KNeighborsClassifier(n_neighbors=3, weights='distance')\nknn = knn.fit(X_train, y_train)\ny_pred_knn = knn.predict(X_test)\n\nprecision_knn, recall_knn = (round(float(x),2) for x in list(score(y_test,\n                                                                      y_pred_knn,\n                                                                      average='weighted'))[:-2])\n# adding KNN stats to metrics DataFrame\nknn_stats = pd.Series({'precision':precision_knn,\n                      'recall':recall_knn,\n                      'accuracy':round(accuracy_score(y_test, y_pred_knn), 2),\n                      'f1score':round(f1_score(y_test, y_pred_knn), 2),\n                      'auc': round(roc_auc_score(y_test, y_pred_knn),2)}, name='KNN')\n# Report outcomes\npd.DataFrame(classification_report(y_test, y_pred_knn, output_dict=True)).iloc[:3,:2]","682a5e30":"dt = DecisionTreeClassifier(random_state=42)\ndt = dt.fit(X_train, y_train)\ndt.tree_.node_count, dt.tree_.max_depth","afb6d1cc":"y_train_pred = dt.predict(X_train)\ny_pred_dt = dt.predict(X_test)\n\nprecision_dt, recall_dt = (round(float(x),2) for x in list(score(y_test,\n                                                                y_pred_dt,\n                                                                average='weighted'))[:-2])\n# adding dt stats to metrics DataFrame\ndt_stats = pd.Series({'precision':precision_dt,\n                      'recall':recall_dt,\n                      'accuracy':round(accuracy_score(y_test, y_pred_dt), 2),\n                      'f1score':round(f1_score(y_test, y_pred_dt), 2),\n                      'auc': round(roc_auc_score(y_test, y_pred_dt),2)}, name='Decision Tree')\n# Report outcomes\npd.DataFrame(classification_report(y_test, y_pred_dt, output_dict=True)).iloc[:3,:2]","3c9b21b1":"# Initialize the random forest estimator\nRF = RandomForestClassifier(oob_score=True, \n                            random_state=42, \n                            warm_start=True,\n                            n_jobs=-1)\n\n# initialise list for out of bag error\noob_list = list()\n\n# Iterate through all of the possibilities for number of trees\nfor n_trees in [15, 20, 30, 40, 50, 100, 150, 200, 300, 400]:\n    \n    # Use this to set the number of trees\n    RF.set_params(n_estimators=n_trees)\n    \n    # Fit the model\n    RF.fit(X_train, y_train)\n    \n    # Get the out of bag error and store it\n    oob_error = 1 - RF.oob_score_\n    oob_list.append(pd.Series({'n_trees': n_trees, 'oob': oob_error}))\n\nrf_oob_df = pd.concat(oob_list, axis=1).T.set_index('n_trees')","063af45d":"sns.set_context('talk')\nsns.set_style('white')\n\nax = rf_oob_df.plot(legend=False, marker='o', color=\"orange\", figsize=(14, 7), linewidth=5)\nax.set(ylabel='out-of-bag error');","f0c2e49f":"rf = RF.set_params(n_estimators=100)\n\ny_pred_rf = rf.predict(X_test)\nprecision_rf, recall_rf = (round(float(x),2) for x in list(score(y_test,\n                                                                    y_pred_rf,\n                                                                    average='weighted'))[:-2])\nrf_stats = pd.Series({'precision':precision_rf,\n                      'recall':recall_rf,\n                      'accuracy':round(accuracy_score(y_test, y_pred_rf), 2),\n                      'f1score':round(f1_score(y_test, y_pred_rf), 2),\n                      'auc': round(roc_auc_score(y_test, y_pred_rf),2)}, name='Random Forest')\n# Report outcomes\npd.DataFrame(classification_report(y_test, y_pred_rf, output_dict=True)).iloc[:3,:2]","e4b65240":"fig, axList = plt.subplots(nrows=2, ncols=2)\naxList = axList.flatten()\nfig.set_size_inches(12, 10)\n\n\nmodels = coeff_labels = ['lr', 'knn', 'dt', 'rf']\ncm = [confusion_matrix(y_test, y_pred_lr),\n      confusion_matrix(y_test, y_pred_knn),\n      confusion_matrix(y_test, y_pred_dt),\n      confusion_matrix(y_test, y_pred_rf)]\nlabels = ['False', 'True']\n\nfor ax,model, idx in zip(axList, models, range(0,4)):\n    sns.heatmap(cm[idx], ax=ax, annot=True, fmt='d', cmap='summer');\n    ax.set(title=model);\n    ax.set_xticklabels(labels, fontsize=20);\n    ax.set_yticklabels(labels[::-1], fontsize=20);\n    ax.set_ylabel('Prediction', fontsize=25);\n    ax.set_xlabel('Ground Truth', fontsize=25)\n    \nplt.tight_layout()","da20e11b":"pd.DataFrame(classification_report(y_test, y_pred_lr, output_dict=True)).iloc[:3,:2]","6e0d14c6":"pd.DataFrame(classification_report(y_test, y_pred_knn, output_dict=True)).iloc[:3,:2]","364d0845":"pd.DataFrame(classification_report(y_test, y_pred_dt, output_dict=True)).iloc[:3,:2]","393573e0":"pd.DataFrame(classification_report(y_test, y_pred_rf, output_dict=True)).iloc[:3,:2]","552a897b":"metrics.append([lr_stats, knn_stats, dt_stats, rf_stats])","183054e9":"The correlation of **AF_ESP** with **AF_TGP** is above 0.8 hence dropping the **AF_TGP** column.","c94b6860":"## Random forest","0abc16db":"There is a large amount of misclassification which can be seen on the average error report below.","facf1ec5":"## Featureset Exploration\n\n**CHROM**: Chromosome the variant is located on\n\n**REF**: Reference Allele\n\n**ALT**: Alternaete Allele\n\n**AF_ESP**: Allele frequencies from GO-ESP\n\n**AF_EXAC**: Allele frequencies from ExAC\n\n**AF_TGP**: Allele frequencies from the 1000 genomes project\n\n**CLNDISDB**: Tag-value pairs of disease database name and identifier, e.g. OMIM:NNNNNN\n\n**CLNDISDBINCL**: For included Variant: Tag-value pairs of disease database name and identifier, e.g. OMIM:NN\n\n**CLNDN**: ClinVar's preferred disease name for the concept specified by disease identifiers in CLNDISDB\n\nMore information on many of the features can be found at these two links:\n\nhttps:\/\/useast.ensembl.org\/info\/docs\/tools\/vep\/vep_formats.html#output\n\nhttps:\/\/useast.ensembl.org\/info\/genome\/variation\/prediction\/predicted_data.html#consequences","3d87e451":"The error looks like it has stabilized around 100-150 trees.","60f8ee25":"Drop the columns where more than 20% of the data is missing.","7688813a":"# Results\nThe classsification report of each classifier shows that I am able to predict consistent classification, with an F1 score of 0.855186 for **Logistic Regression** model. Similar result can be achieved using any of the model above. I predicted conflicting classification with F2 score 0.370185 with **Decision Tree** algorithm which is significantly better than the Logistic Regression with F1 score 0.001211.","58a216ae":"# Summary\n\nThe dataset for this project originates from [ClinVar](https:\/\/www.ncbi.nlm.nih.gov\/clinvar\/). ClinVar is a public resource containing annotations about human genetic variants. These variants are classified by clinical laboratories on a categorical spectrum ranging from benign, likely benign, uncertain significance, likely pathogenic, and pathogenic. Variants that have conflicting classifications (from laboratory to laboratory) can cause confusion when clinicians or researchers try to interpret whether the variant has an impact on the disease of a given patient.\n\nThe objective is to predict whether a ClinVar variant will have conflicting classifications. This is presented here as a binary classification problem, where each record in the dataset is a genetic variant.\n\nConflicting classifications are when two of any of the following three categories are present for one variant, two submissions of one category are not considered conflicting.\n\n- Likely Benign or Benign\n- VUS\n- Likely Pathogenic or Pathogenic\n\nConflicting classification has been assigned to the CLASS column. It is a binary representation of whether or not a variant has conflicting classifications, where **0** represents **consistent classifications** and **1** represents **conflicting classifications**.\n\nIn this project, we will employ four different classifier models to find the best candidate algorithm that accurately predicts whether a ClinVar variant will have conflicting classifications.\n\n# Exploratory Data Analysis","edb7e847":"# Split the data\n\nSplit the data into train and test data sets using **StratifiedShuffleSplit** to maintain the same ratio of predictor classes.","30ef8fb7":"The correlation of **ALT** with **Allele** and **MC** with **Consequence** are both above 0.8 hence dropping the **ALT** and **MC** columns.","897dcc54":"# Apply Feature Scaling","88e98a47":"## Decision Tree","024b0c88":"## K-nearest Neighbors","0bd3007f":"# Feature Transformation\n\n- Replace nan in **MC**, **SYMBOL**, **Feature_type**, **Feature**, **BIOTYPE**, **Amino_acids**, **Codons**, **STRAND** with the most frequent value\n- Replace nan in **LoFtool** with the mean ","ce03f8bd":"Now identify which variables are binary, categorical and ordinal by looking at the number of unique values each variable takes, then create list variables for categorical, numeric, binary, and ordinal variables.","87184a50":"# Train models\n\n- Standard logistic regression, K-nearest neighbors algorithm, Decision Tree,mRandom Forest\n- Plot the results using heatmaps\n- Compare scores: precision, recall, accuracy, F1 score, auc\n\n## Logistic Regression"}}