{"cell_type":{"f86832fd":"code","8ab2f533":"code","fe1e93f7":"code","08847278":"code","6dee739d":"code","8e1d0b2a":"code","e10cf920":"code","c862da38":"code","8b1c9a01":"code","2285de7b":"code","4bcff09a":"code","7c5d60e2":"code","d10bd1cd":"markdown","b35d2794":"markdown","e4e4b8c2":"markdown","adacb217":"markdown","1164d536":"markdown","734bf64a":"markdown","57d73aa6":"markdown","8084dec9":"markdown","17bb6155":"markdown","cee463a6":"markdown","89385bc7":"markdown","67e9e2d5":"markdown"},"source":{"f86832fd":"import numpy as np\nimport pandas as pd\n\ndf = pd.read_csv(\"..\/input\/stellar-classification-dataset-sdss17\/star_classification.csv\")\ndf.info()","8ab2f533":"df.describe()","fe1e93f7":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndataset = df.copy()\ny = dataset[\"class\"]\ndataset.drop(\"class\", axis=1, inplace=True)\ny = y.astype('category').cat.codes\nprint(y.describe())\nsns.displot(y)","08847278":"from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing\n\nX = dataset.copy()\nX.drop([\"run_ID\", \"rerun_ID\", \"spec_obj_ID\", \"fiber_ID\", \"obj_ID\", \"field_ID\", \"plate\"], axis=1, inplace=True)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42) \nscaler = preprocessing.StandardScaler().fit(X_train)\nX_train = scaler.transform(X_train.values)\nX_test = scaler.transform(X_test.values)","6dee739d":"from sklearn.model_selection import cross_val_score\n\ndef rmse_cv(model, X, y, cv):\n    rmse= np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv = cv))\n    return(rmse)","8e1d0b2a":"from sklearn import linear_model\n\nclf = linear_model.LogisticRegression(solver='liblinear')\nclf.fit(X_train,y_train)\nprint(rmse_cv(clf, X, y, 5))\nprint(clf.score(X_test,y_test))","e10cf920":"from sklearn.ensemble import AdaBoostClassifier\nclf = AdaBoostClassifier(n_estimators=500)\nclf.fit(X_train, y_train)\nprint(clf.score(X_train, y_train))\nprint(clf.score(X_test, y_test))","c862da38":"from sklearn.tree import DecisionTreeClassifier\nclf = DecisionTreeClassifier()\nclf.fit(X_train, y_train)\nscore = clf.score(X_test, y_test)\nprint(str(score))\n","8b1c9a01":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1)\nclf.fit(X_train, y_train)\nscore = clf.score(X_test, y_test)\nprint(str(score))","2285de7b":"from sklearn.ensemble import ExtraTreesClassifier\nclf = ExtraTreesClassifier()\nclf.fit(X_train, y_train)\nscore = clf.score(X_test, y_test)\nprint(str(score))","4bcff09a":"\"\"\"\nimport optuna\nimport pandas as pd\nfrom sklearn import linear_model\nfrom sklearn import ensemble\nfrom sklearn import datasets\nfrom sklearn import model_selection\n\nX_scaled = preprocessing.StandardScaler().fit_transform(X)\n\n#Step 1. Define an objective function to be maximized.\ndef objective(trial):\n\n    classifier_name = trial.suggest_categorical(\"classifier\", [\"ExtraTrees\"])\n    \n    # Step 2. Setup values for the hyperparameters:\n    rf_n_estimators = trial.suggest_int(\"rf_n_estimators\", 10, 1000)\n    rf_max_depth = trial.suggest_int(\"rf_max_depth\", 2, 32, log=True)\n    classifier_obj = ensemble.ExtraTreesClassifier(\n        max_depth=rf_max_depth, n_estimators=rf_n_estimators\n    )\n\n    # Step 3: Scoring method:\n    score = model_selection.cross_val_score(classifier_obj, X_scaled, y, n_jobs=-1, cv=3)\n    accuracy = score.mean()\n    return accuracy\n\n# Step 4: Running it\nstudy = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=50)\n\"\"\"","7c5d60e2":"clf = ExtraTreesClassifier(max_depth=28, n_estimators=566)\nclf.fit(X_train, y_train)\nscore = clf.score(X_test, y_test)\nprint(score)","d10bd1cd":"# Describe the Data","b35d2794":"# Attempt Classification\n\nUsing logistic regression","e4e4b8c2":"Setup cross validation","adacb217":"Extra Trees seems to be the best.\n\n# Find Best Parameters With Optuna\n\nCommented out to keep the running time of notebook under control","1164d536":"# Using Tuned Parameters","734bf64a":"This shows the dataset to be imbalanced which could turn out to be an issue ","57d73aa6":"# AdaBoostClassifier","8084dec9":"# Conclusion\n\nThis isn't a bad result. I'm sure better is possible but as a starting point this seems pretty effective. 97+%","17bb6155":"# Process Data Create Y\n\nY will be the class column. Let's turn Y into categorical numbers and then see what the balance is like","cee463a6":"# Introduction\n\nLet's classify stellar objects by their class. This seems like a fun dataset :) ","89385bc7":"Not so great a result with AdaBoostClassifier\n\n# Let's try a few different classifiers","67e9e2d5":"# Scale Data and Split X\n\nWe should also cut out some columns that will harm generalisation and testing on new data like run_ID"}}