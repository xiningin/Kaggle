{"cell_type":{"5ad7c53b":"code","ed2a3899":"code","939f74c2":"code","a73299a8":"code","2ba0bf7b":"code","e0b00029":"code","df97765c":"code","03d20237":"code","d9d5a5ae":"code","2d0fb140":"code","5099a87b":"code","ba6c4bc0":"code","c2cc79bb":"code","c2d95167":"code","d6eea66a":"code","61fe0fe7":"code","50aedc64":"code","77b06d9d":"code","352677e7":"code","95663a05":"code","9c8a7592":"code","23118d34":"code","4f668220":"code","865e52ec":"code","e32b7593":"code","8b8ee84f":"code","ede5f93b":"code","0e0b7496":"code","4ab69910":"code","8a175261":"code","532bcb43":"code","115ee985":"code","459a18e2":"code","dea7a14f":"code","450ec75d":"code","cbb1c210":"code","8cb88a77":"code","97d5fed6":"code","32d7c860":"code","3d002247":"code","1b2f6454":"code","5bebfd09":"code","faa5f751":"code","f99f1844":"code","aba01397":"code","8b2e5840":"code","b4fd8a0a":"code","437aec3e":"markdown","a21a3fe0":"markdown","b06662b9":"markdown","f0459368":"markdown","092ac978":"markdown","1b16dbcb":"markdown","a42bb558":"markdown","43ccf212":"markdown","4831e1cc":"markdown","b36c74b7":"markdown","e79f0edc":"markdown","e5e8cbc1":"markdown","c38ce065":"markdown","3f6bace0":"markdown","23d4c6eb":"markdown","7779b21b":"markdown","1a006df3":"markdown","824694ed":"markdown","9f253bb3":"markdown","8d2a9cb7":"markdown","9a489a25":"markdown","8d5f1c6b":"markdown","32857930":"markdown","b082f83f":"markdown","ae5429eb":"markdown","b3c5fc94":"markdown","9e0cb369":"markdown","ac9d3b1a":"markdown","75f51eda":"markdown"},"source":{"5ad7c53b":"#importing the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom scipy import stats\nimport seaborn as sns\nsns.set_style('whitegrid')\n\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.svm import SVC\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nimport xgboost\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import roc_auc_score","ed2a3899":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","939f74c2":"dataset = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')","a73299a8":"dataset.head()","2ba0bf7b":"dataset.describe()","e0b00029":"dataset.isnull().sum()","df97765c":"print(dataset.shape[0])\r\nprint(dataset.shape[1])","03d20237":"plt.figure(figsize = (16,16))\r\nc = dataset.corr()\r\nsns.heatmap(c, xticklabels=c.columns,yticklabels=c.columns,linewidths=.1,cmap=\"Reds\")\r\nplt.show()","d9d5a5ae":"data = dataset.columns.values\r\n\r\ni = 0\r\na = dataset.loc[dataset['Class'] == 0]\r\nb = dataset.loc[dataset['Class'] == 1]\r\n\r\nplt.figure()\r\nfig, ax = plt.subplots(8,4,figsize=(20,30))\r\n\r\nfor j in data:\r\n    i += 1\r\n    plt.subplot(8,4,i)\r\n    sns.kdeplot(a[j], bw_method=0.2,label=\"Class = 0\")\r\n    sns.kdeplot(b[j], bw_method=0.2,label=\"Class = 1\")\r\n    plt.xlabel(j, fontsize=12)\r\n    plt.legend(loc='best')\r\nplt.show()","2d0fb140":"data_1 = dataset[dataset['Class'] == 1] \r\nplt.figure(figsize=(15,15))\r\nplt.scatter(data_1['Time'], data_1['Amount']) \r\nplt.title('Amount v Time for Class 1')\r\nplt.xlabel('Time')\r\nplt.ylabel('Amount')\r\nplt.show()","5099a87b":"fraud = len(dataset[dataset.Class == 1])\r\nno_fraud = len(dataset[dataset.Class == 0])\r\nprint(fraud)\r\nprint(no_fraud)","ba6c4bc0":"scores = pd.DataFrame(columns = ['Accuracy Score', \n                                 'ROC AUC Score'], \n                      index = [ 'Kernel SVM', \n                               'Logistic Regression', \n                               'Naive Bayes', \n                               'k-NN', \n                               'Decision Tree', \n                               'Random Forest', \n                               'Gradient Boosted Decision Trees', \n                               'XG Boost'])\rscores_rus = pd.DataFrame(columns = ['Accuracy Score after Undersampling', \n                                     'ROC AUC Score after Undersampling'], \n                          index = ['Kernel SVM', \n                                   'Logistic Regression', \n                                   'Naive Bayes', 'k-NN', \n                                   'Decision Tree', \n                                   'Random Forest', \n                                   'Gradient Boosted Decision Trees', \n                                   'XG Boost'])","c2cc79bb":"X = dataset.iloc[:, :-1].values\ny = dataset.iloc[:, -1].values","c2d95167":"#splitting the data\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.25, random_state = 0)\nrus = RandomUnderSampler(random_state = 0)\nX_rus, y_rus = rus.fit_resample(X_train, y_train)","d6eea66a":"classifier_log = LogisticRegression(random_state = 0, max_iter = 1000)\r\nclassifier_log.fit(X_train, y_train)","61fe0fe7":"y_pred = classifier_log.predict(X_test)\r\nprint(confusion_matrix(y_test, y_pred))\r\nprint(accuracy_score(y_test, y_pred))\r\nprint(roc_auc_score(y_test, y_pred))\r\na = accuracy_score(y_test, y_pred)\r\nb = roc_auc_score(y_test, y_pred)\r\nscores.loc['Logistic Regression'] = [a,b]","50aedc64":"classifier_log_rus = LogisticRegression(random_state = 0, max_iter = 1000)\r\nclassifier_log_rus.fit(X_rus, y_rus)","77b06d9d":"y_pred = classifier_log_rus.predict(X_test)\r\nprint(confusion_matrix(y_test, y_pred))\r\nprint(accuracy_score(y_test, y_pred))\r\nprint(roc_auc_score(y_test, y_pred))\r\na = accuracy_score(y_test, y_pred)\r\nb = roc_auc_score(y_test, y_pred)\r\nscores_rus.loc['Logistic Regression'] = [a,b]","352677e7":"classifier_knn = KNeighborsClassifier(n_neighbors = 10, metric = 'minkowski', p = 2)\r\nclassifier_knn.fit(X_train, y_train)","95663a05":"y_pred = classifier_knn.predict(X_test)\r\nprint(confusion_matrix(y_test, y_pred))\r\nprint(accuracy_score(y_test, y_pred))\r\nprint(roc_auc_score(y_test, y_pred))\r\na = accuracy_score(y_test, y_pred)\r\nb = roc_auc_score(y_test, y_pred)\r\nscores.loc['k-NN'] = [a,b]","9c8a7592":"classifier_knn_rus = KNeighborsClassifier(n_neighbors = 10, metric = 'minkowski', p = 2)\r\nclassifier_knn_rus.fit(X_rus, y_rus)","23118d34":"y_pred = classifier_knn_rus.predict(X_test)\r\nprint(confusion_matrix(y_test, y_pred))\r\nprint(accuracy_score(y_test, y_pred))\r\nprint(roc_auc_score(y_test, y_pred))\r\na = accuracy_score(y_test, y_pred)\r\nb = roc_auc_score(y_test, y_pred)\r\nscores_rus.loc['k-NN'] = [a,b]","4f668220":"classifier_nb = GaussianNB()\r\nclassifier_nb.fit(X_train, y_train)","865e52ec":"y_pred = classifier_nb.predict(X_test)\r\nprint(confusion_matrix(y_test, y_pred))\r\nprint(accuracy_score(y_test, y_pred))\r\nprint(roc_auc_score(y_test, y_pred))\r\na = accuracy_score(y_test, y_pred)\r\nb = roc_auc_score(y_test, y_pred)\r\nscores.loc['Naive Bayes'] = [a,b]","e32b7593":"classifier_nb_rus = GaussianNB()\r\nclassifier_nb_rus.fit(X_rus, y_rus)","8b8ee84f":"y_pred = classifier_nb_rus.predict(X_test)\r\nprint(confusion_matrix(y_test, y_pred))\r\nprint(accuracy_score(y_test, y_pred))\r\nprint(roc_auc_score(y_test, y_pred))\r\na = accuracy_score(y_test, y_pred)\r\nb = roc_auc_score(y_test, y_pred)\r\nscores_rus.loc['Naive Bayes'] = [a,b]","ede5f93b":"classifier_dt = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\r\nclassifier_dt.fit(X_train, y_train)","0e0b7496":"y_pred = classifier_dt.predict(X_test)\r\nprint(confusion_matrix(y_test, y_pred))\r\nprint(accuracy_score(y_test, y_pred))\r\nprint(roc_auc_score(y_test, y_pred))\r\na = accuracy_score(y_test, y_pred)\r\nb = roc_auc_score(y_test, y_pred)\r\nscores.loc['Decision Tree'] = [a,b]","4ab69910":"classifier_dt_rus = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\r\nclassifier_dt_rus.fit(X_rus, y_rus)","8a175261":"y_pred = classifier_dt_rus.predict(X_test)\r\nprint(confusion_matrix(y_test, y_pred))\r\nprint(accuracy_score(y_test, y_pred))\r\nprint(roc_auc_score(y_test, y_pred))\r\na = accuracy_score(y_test, y_pred)\r\nb = roc_auc_score(y_test, y_pred)\r\nscores_rus.loc['Decision Tree'] = [a,b]","532bcb43":"classifier_rf = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\r\nclassifier_rf.fit(X_train, y_train)","115ee985":"y_pred = classifier_rf.predict(X_test)\r\nprint(confusion_matrix(y_test, y_pred))\r\nprint(accuracy_score(y_test, y_pred))\r\nprint(roc_auc_score(y_test, y_pred))\r\na = accuracy_score(y_test, y_pred)\r\nb = roc_auc_score(y_test, y_pred)\r\nscores.loc['Random Forest'] = [a,b]","459a18e2":"classifier_rf_rus = RandomForestClassifier(n_estimators = 100, criterion = 'entropy', random_state = 0)\r\nclassifier_rf_rus.fit(X_rus, y_rus)","dea7a14f":"y_pred = classifier_rf_rus.predict(X_test)\r\nprint(confusion_matrix(y_test, y_pred))\r\nprint(accuracy_score(y_test, y_pred))\r\nprint(roc_auc_score(y_test, y_pred))\r\na = accuracy_score(y_test, y_pred)\r\nb = roc_auc_score(y_test, y_pred)\r\nscores_rus.loc['Random Forest'] = [a,b]","450ec75d":"classifier_gbrt = GradientBoostingClassifier(random_state=0)\r\nclassifier_gbrt.fit(X_train, y_train)","cbb1c210":"y_pred = classifier_gbrt.predict(X_test)\r\nprint(confusion_matrix(y_test, y_pred))\r\nprint(accuracy_score(y_test, y_pred))\r\nprint(roc_auc_score(y_test, y_pred))\r\na = accuracy_score(y_test, y_pred)\r\nb = roc_auc_score(y_test, y_pred)\r\nscores.loc['Gradient Boosted Decision Trees'] = [a,b]","8cb88a77":"classifier_gbrt_rus = GradientBoostingClassifier(random_state=0)\r\nclassifier_gbrt_rus.fit(X_rus, y_rus)","97d5fed6":"y_pred = classifier_gbrt_rus.predict(X_test)\r\nprint(confusion_matrix(y_test, y_pred))\r\nprint(accuracy_score(y_test, y_pred))\r\nprint(roc_auc_score(y_test, y_pred))\r\na = accuracy_score(y_test, y_pred)\r\nb = roc_auc_score(y_test, y_pred)\r\nscores_rus.loc['Gradient Boosted Decision Trees'] = [a,b]","32d7c860":"classifier_ksvm = SVC(kernel = 'rbf' , random_state = 0)\r\nclassifier_ksvm.fit(X_train, y_train)","3d002247":"y_pred = classifier_ksvm.predict(X_test)\r\nprint(confusion_matrix(y_test, y_pred))\r\nprint(accuracy_score(y_test, y_pred))\r\nprint(roc_auc_score(y_test, y_pred))\r\na = accuracy_score(y_test, y_pred)\r\nb = roc_auc_score(y_test, y_pred)\r\nscores.loc['Kernel SVM'] = [a,b]","1b2f6454":"classifier_ksvm_rus = SVC(kernel = 'rbf' , random_state = 0)\r\nclassifier_ksvm_rus.fit(X_rus, y_rus)","5bebfd09":"y_pred = classifier_ksvm_rus.predict(X_test)\r\nprint(confusion_matrix(y_test, y_pred))\r\nprint(accuracy_score(y_test, y_pred))\r\nprint(roc_auc_score(y_test, y_pred))\r\na = accuracy_score(y_test, y_pred)\r\nb = roc_auc_score(y_test, y_pred)\r\nscores_rus.loc['Kernel SVM'] = [a,b]","faa5f751":"classifier_xgb = XGBClassifier()\r\nclassifier_xgb.fit(X_train, y_train)","f99f1844":"y_pred = classifier_xgb.predict(X_test)\r\nprint(confusion_matrix(y_test, y_pred))\r\nprint(accuracy_score(y_test, y_pred))\r\nprint(roc_auc_score(y_test, y_pred))\r\na = accuracy_score(y_test, y_pred)\r\nb = roc_auc_score(y_test, y_pred)\r\nscores.loc['XG Boost'] = [a,b]","aba01397":"classifier_xgb_rus = XGBClassifier()\r\nclassifier_xgb_rus.fit(X_rus, y_rus)","8b2e5840":"y_pred = classifier_xgb_rus.predict(X_test)\r\nprint(confusion_matrix(y_test, y_pred))\r\nprint(accuracy_score(y_test, y_pred))\r\nprint(roc_auc_score(y_test, y_pred))\r\na = accuracy_score(y_test, y_pred)\r\nb = roc_auc_score(y_test, y_pred)\r\nscores_rus.loc['XG Boost'] = [a,b]","b4fd8a0a":"Scores = pd.concat([scores, scores_rus], axis = 1)\r\nScores","437aec3e":"### **Naive Bayes(without Undersampling)**","a21a3fe0":"### **Gradient Boosted Decision Trees(with Undersampling)**\r\n","b06662b9":"### **Features Plot**","f0459368":"### **Table with Accuracy Scores ans ROC AUC Scores**","092ac978":"### **Naive Bayes(with Undersampling)**","1b16dbcb":"### **Major Insights**\r\n\r\n\r\n","a42bb558":"### **Kernel SVM(without Undersampling)**\r\n","43ccf212":"### **k-NN(without Undersampling)**","4831e1cc":"### **XG Boost(without Undersampling)**\r\n","b36c74b7":"### **Logistic Regression(with Undersampling)**","e79f0edc":"### **k-NN(with Undersampling)**","e5e8cbc1":"### **Introduction**\r\n\r\nThe dataset contains transactions made by credit cards. This dataset presents transactions, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.\r\n\r\nIt contains only numerical input variables.","c38ce065":"### **Decision Tree(without Undersampling)**","3f6bace0":"### **Decision Tree(with Undersampling)**","23d4c6eb":"### **Looking at the data**","7779b21b":"### **Shape of the data**\r\n\r\nThe data consists of 31 columns and 284807 row**","1a006df3":"### **Random Forest(with Undersampling)**\r\n","824694ed":"### **Fitting Different Models**\r\n\r\nFit different Classification Models and find the Accuracy Score and ROC AUC Score for each model on the unbalanced data.\r\n\r\nBalance the data for training of different Classification models using a random under sampler and also, find the Accuracy Score and ROC AUC Score for each model in this case as well.","9f253bb3":"### **Importing the Dataset**","8d2a9cb7":"### **Kernel SVM(with Undersampling)**\r\n","9a489a25":"### **XG Boost(with Undersampling)**\r\n","8d5f1c6b":"### **Amount v Time for Fraudulent Transaction**\r\n\r\nThe time doesn't impact the frequency of frauds. Moreover, the majority of frauds are small amounts.","32857930":"### **Gradient Boosted Decision Trees(without Undersampling)**\r\n","b082f83f":"### **No. of Fraudulent Transactions**\r\n\r\nWe have 492 frauds out of 284,807 transactions.","ae5429eb":"### **Logistic Regression(without Undersampling)**","b3c5fc94":"### **Heatmap**\r\n\r\nThere is no notable correlation between features V1-V28.\r\nAlso, the correlation between Time and other features and between Amount and other features is also not high enough.","9e0cb369":"- The data is highly unbalanced, and was balanced using **RandomUnderSampler** after splitting the data into train and test set and before training the model.\r\n- Different classifiers were trained and the best ROC AUC Score of ~0.94 was obtained using XG Boost and Logistic Regression and next best ROC AUC Score of ~0.93 was obtained using Gradient Boosted Decision Trees, using Undersampling Techniques.\r\n- Even though Accuracy without Undersampling is 0.99 for all classifiers, it leads to low ROC AUC Score, that is increase in both Type I and Type II Error.\r\n- As the size of data is large, my first instinct is to try to fit linear models, Linear SVM and Logistic Regression Classifier. The GBMs also lead to almost same scores but the time taken to train the model was significantly less. The time difference is even more when Undersampling wasn't applied.","ac9d3b1a":"### **Random Forest(without Undersampling)**\r\n\r\n\r\n\r\n\r\n","75f51eda":"## **Credit Card Transactions**"}}