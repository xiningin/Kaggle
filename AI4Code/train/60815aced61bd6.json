{"cell_type":{"fd63e757":"code","b36fdc94":"code","945f3cf0":"code","39f897ce":"code","16ad02e9":"code","05bb622d":"code","5452be12":"code","86700620":"code","243ccff4":"code","df6d87eb":"code","7bb8bc62":"code","aff7c750":"code","20b0dc73":"markdown","6a55285b":"markdown","5c3fc0c0":"markdown"},"source":{"fd63e757":"import os\nimport random\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport lightgbm as lgb\nimport tensorflow as tf\nfrom tensorflow.keras import Sequential,Model\nfrom tensorflow.keras.layers import Dense,Conv2D,Flatten,Dropout, Input, Concatenate, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau\nfrom tensorflow.keras.optimizers.schedules import ExponentialDecay\nfrom PIL import Image\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt","b36fdc94":"# Constants\n\nAUTOTUNE = tf.data.experimental.AUTOTUNE  \nimg_size = 224\nchannels = 3\nBatch_size = 64\n\n# Directory for dataset\n\ntrain_dir = \"\/kaggle\/input\/petfinder-pawpularity-score\/train\/\"\ntest_dir = \"\/kaggle\/input\/petfinder-pawpularity-score\/test\/\"\n\ndef seed_everything():\n    np.random.seed(123)\n    random.seed(123)\n    tf.random.set_seed(123)\n    os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = '2'\n    os.environ['PYTHONHASHSEED'] = str(123)\n\nseed_everything()","945f3cf0":"# Reading dataset train, test in df and df_test respectively\n\ndf = pd.read_csv(\"\/kaggle\/input\/petfinder-pawpularity-score\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/petfinder-pawpularity-score\/test.csv\")\nId = df_test[\"Id\"].copy()\n\n\n# Converting Id column for taking images\n\ndf[\"Id\"] = df[\"Id\"].apply(lambda x : \"\/kaggle\/input\/petfinder-pawpularity-score\/train\/\" + x + \".jpg\")\ndf_test[\"Id\"] = df_test[\"Id\"].apply(lambda x : \"\/kaggle\/input\/petfinder-pawpularity-score\/test\/\" + x + \".jpg\")","39f897ce":"# Defining functions for reading and augmentation of images\n# A seperate function for creating dataset\n\n# Augmenting the image\ndef image_preprocess(is_labelled):  \n    def augment(image):\n        image = tf.image.random_flip_left_right(image)\n#         image = tf.image.random_flip_up_down(image)\n        image = tf.image.random_saturation(image, 0.95, 1.05)\n        image = tf.image.random_contrast(image, 0.95, 1.05)\n        return image\n    \n    def can_be_augmented(img, label):\n        return augment(img), label\n    \n#   If record has label both image and lable will be returned\n    return can_be_augmented if is_labelled else augment\n\n\n\n# Reading and rescaling images\ndef image_read(is_labelled):\n    def decode(path):\n        image = tf.io.read_file(path)\n        image = tf.image.decode_jpeg(image, channels=channels)\n        image = tf.cast(image, tf.float32)\n        image = tf.image.resize(image, (img_size, img_size))\n        image = tf.keras.applications.efficientnet.preprocess_input(image) \n        return image\n    \n    def can_be_decoded(path, label):\n        return decode(path), label\n    \n#   If record has label both image and lable will be returned\n\n    return can_be_decoded if is_labelled else decode\n\n\n# Creating the dataset\ndef create_dataset(df, batch_size, is_labelled = False, augment = False, shuffle = False):\n    image_read_fn = image_read(is_labelled)\n    image_preprocess_fn = image_preprocess(is_labelled)\n    \n    if is_labelled:\n        dataset = tf.data.Dataset.from_tensor_slices((df[\"Id\"].values, df[\"Pawpularity\"].values))\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices((df[\"Id\"].values))\n    \n    dataset = dataset.map(image_read_fn, num_parallel_calls=AUTOTUNE)\n    dataset = dataset.map(image_preprocess_fn, num_parallel_calls=AUTOTUNE) if augment else dataset\n    dataset = dataset.shuffle(1024, reshuffle_each_iteration=True) if shuffle else dataset\n    dataset = dataset.batch(batch_size)\n    dataset = dataset.prefetch(AUTOTUNE)\n    return dataset","16ad02e9":"# Defining train, validation and test set \n\ntrn = df.iloc[:9000]\nval = df.iloc[9001:]\n\ntrain = create_dataset(trn, Batch_size, is_labelled = True, augment = True, shuffle = True)\nvalidation = create_dataset(val, Batch_size, is_labelled = True, augment = False, shuffle = False)\ntest = create_dataset(df_test, Batch_size, is_labelled = False, augment = False, shuffle=False)","05bb622d":"# Loading pretrained EfficientNet\n\nimg_mod = \"\/kaggle\/input\/keras-applications-models\/EfficientNetB0.h5\"\nefnet = tf.keras.models.load_model(img_mod)\n\n# Layers of efficientnet will not be trained\nefnet.trainable = False","5452be12":"# Using pretrained EfficientNet with image size of (224,224,3)\n# Using relu activation function because it a regression problem\n\nmodel = Sequential([\n    Input(shape=(img_size, img_size, channels)),\n    efnet,\n    BatchNormalization(),\n    Dropout(0.2),\n    Dense(units = 64, activation=\"relu\"),\n    Dense(units = 1, activation=\"relu\")\n])","86700620":"# Early stopping helps as it stops training if val_loss(validation score) does not decrease.\n\nearly_stopping = EarlyStopping(patience = 5,restore_best_weights=True)\n\n\nlr_schedule = ExponentialDecay(\n    initial_learning_rate=1e-3,\n    decay_steps=100, decay_rate=0.96,\n    staircase=True)","243ccff4":"\nimport tensorflow_addons as tfa\n\n\ndef Ranger(sync_period=6,\n           slow_step_size=0.5,\n           learning_rate=0.001,\n           beta_1=0.9,\n           beta_2=0.999,\n           epsilon=1e-7,\n           weight_decay=0.,\n           amsgrad=False,\n           sma_threshold=5.0,\n           total_steps=0,\n           warmup_proportion=0.1,\n           min_lr=0.,\n           name=\"Ranger\"):\n    inner = tfa.optimizers.RectifiedAdam(learning_rate, beta_1, beta_2, epsilon, weight_decay, amsgrad, sma_threshold, total_steps, warmup_proportion, min_lr, name)\n    optim = tfa.optimizers.Lookahead(inner, sync_period, slow_step_size, name)\n    return optim","df6d87eb":"# Compiling and Fitting the model\n\nmodel.compile(loss=\"mse\", \n              optimizer = Ranger(learning_rate = lr_schedule), \n              metrics=[tf.keras.metrics.RootMeanSquaredError()])\n\npredictor = model.fit(train,\n                      epochs=20, \n                      validation_data = validation,\n                      callbacks=[early_stopping])","7bb8bc62":"# 18.42\n# Last trained    train  --  16.42,    test   --   18.33\n# See accuracy for MLP meta data","aff7c750":"# Making prediction on test set\n\npred = model.predict(test)\n\nfinal=pd.DataFrame()\nfinal['Id']=Id\nfinal['Pawpularity']=pred\nfinal.to_csv('submission.csv',index=False)","20b0dc73":"\n## CNN for Images","6a55285b":"## Prediction","5c3fc0c0":"## Preprocessing"}}