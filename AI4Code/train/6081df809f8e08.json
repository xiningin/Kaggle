{"cell_type":{"3696b1a5":"code","2d8afc95":"code","24aeaba0":"code","a685d020":"code","cf8e4917":"code","8477d959":"code","13fe937f":"code","efeeb893":"code","c6606f05":"code","37595aa9":"code","86bd257d":"code","60ed49b3":"code","c5f102b8":"code","1bb46266":"code","a1bcc612":"code","425c845d":"code","37154fec":"code","a2772f45":"code","b268adf0":"code","dca88a01":"code","0fa0e946":"code","be2a5d6e":"code","38900d37":"code","f453a393":"markdown","27098348":"markdown","85545ccf":"markdown","547e8c2b":"markdown","282c7894":"markdown","3d4f5eaa":"markdown"},"source":{"3696b1a5":"!pip install xgboost","2d8afc95":"import datetime as dt\nimport calendar\nimport random\n\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeClassifier, plot_tree\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import classification_report\nfrom sklearn.ensemble import RandomForestClassifier\nfrom tensorflow import keras\nimport tensorflow as tf\nfrom xgboost import XGBClassifier","24aeaba0":"# Load in data\n# Email specs\ndf_train = pd.read_csv('..\/input\/open-shopee-code-league-marketing-analytics\/train.csv', index_col='user_id', \n                       na_values=['Never checkout', 'Never open', 'Never login'])\n# User specs\ndf_users = pd.read_csv('..\/input\/open-shopee-code-league-marketing-analytics\/users.csv', index_col='user_id')\n# Merge User and Email specs into one\ndf_train = pd.merge(df_train, df_users, how='left', left_index=True, right_index=True)","a685d020":"df_train.head()","cf8e4917":"# Process data\n# Create month, day and time data columns\ndf_train['grass_date'] = pd.to_datetime(df_train.grass_date, format='%Y-%m-%d 00:00:00+08:00')\ndf_train['day'] = [calendar.day_name[i.weekday()] for i in df_train['grass_date'].to_list()] \n# Remove some columns\ndf_train.drop(columns=['grass_date', 'row_id'], inplace=True)","8477d959":"# Check for missing data.\ndf_train.isna().sum()","13fe937f":"# First examine the distributions of 'open flag'\nemail_open_distribution = df_train.open_flag.value_counts()\nax = sns.barplot(x=email_open_distribution.index, y=email_open_distribution.values)","efeeb893":"# Plot distribution of categorical vars using boxplots\n# Columns containing categorical valuess\ncategorical_columns = ['country_code', 'attr_1', 'attr_2', 'attr_3', 'domain', 'day']\nfigure = plt.figure(figsize=(20, 10))\nNUM_COLS = 2\nNUM_ROWS = np.ceil(len(categorical_columns)\/NUM_COLS)\nNUM_PLOTS = 1 \nfor columns in categorical_columns:\n    print('Plotting {}...'.format(columns))\n    ax = plt.subplot(NUM_ROWS, NUM_COLS, NUM_PLOTS)\n    sns.barplot(x=columns, y='open_flag', data=df_train)\n    NUM_PLOTS = NUM_PLOTS + 1\nplt.tight_layout()","c6606f05":"# Plot distribution of continuous vars\n# Columns containing continuous values\nall_columns = df_train.columns\ncontinuous_columns = list(set(all_columns).difference(set(categorical_columns)))\n# Columns containing continuous values\nfigure = plt.figure(figsize=(20, 20))\nNUM_COLS = 3\nNUM_ROWS = np.ceil(len(continuous_columns)\/NUM_COLS)\nNUM_PLOTS = 1 \nfor column in continuous_columns:\n    print('Plotting {}...'.format(column))\n    ax = plt.subplot(NUM_ROWS, NUM_COLS, NUM_PLOTS)\n    sns.boxplot(x='open_flag', y=column, data=df_train)\n    ax.set_yscale('log')\n    NUM_PLOTS = NUM_PLOTS + 1\nplt.tight_layout()","37595aa9":"def process_training(df_train, use_imputer=False):\n    '''\n    Process training data and seperate into train and test dataset\n    '''\n    df_current = df_train.copy()\n    # Choose following columns as independent variables\n#     chosen_columns = ['country_code', 'day', 'attr_1', 'attr_3', 'domain', \n#                       'last_open_day', 'open_count_last_60_days', 'open_count_last_10_days', 'open_count_last_30_days',\n#                       'open_flag']\n    chosen_columns = ['country_code', 'day', 'attr_3', 'domain', \n                      'last_open_day', 'open_count_last_60_days', 'open_count_last_10_days', 'open_count_last_30_days',\n                      'open_flag']\n    df_current = df_current[chosen_columns]\n\n    # Downsample the majority to be the same as the minority (randomly), \n    # this is to rebalance the dataset\n    # Copy an instance of the data\n    df_data_for_sampling = df_current.copy()\n    df_data_for_sampling.reset_index(inplace=True)\n    # Random sampling\n    sample_size_minority = df_data_for_sampling[df_data_for_sampling.open_flag==1].shape[0]\n    sample_size_downsample_index = random.choices(df_data_for_sampling[df_data_for_sampling.open_flag==0].index, \n                                                  k=sample_size_minority)\n    sample_size_downsample = df_data_for_sampling.loc[sample_size_downsample_index, :]\n    # Combined data from both open_flag categories\n    df_current = pd.concat([df_data_for_sampling[df_data_for_sampling.open_flag==1], sample_size_downsample])\n    df_current.set_index('user_id', inplace=True)\n    \n    # For category var, change values to str\n#     categorical_columns = ['country_code', 'day', 'attr_1', 'attr_3', 'domain']\n    categorical_columns = ['country_code', 'day', 'attr_3', 'domain']\n    for columns in categorical_columns:\n        df_current[columns] = df_current[columns].astype('str')\n    # OneHotEncoding\n    df_onehot = pd.get_dummies(df_current[categorical_columns])\n    df_current = pd.concat([df_current, df_onehot], axis=1)\n    df_current.drop(columns=categorical_columns, inplace=True)\n    \n    # Dealing with NAs\n    if use_imputer == True:\n        # Use an imputer\n        print('Imputing...')\n        df_independent_var = df_current.drop(columns=['open_flag'])\n        imputer = KNNImputer(n_neighbors=2)\n        independent_var_imputed = imputer.fit_transform(df_independent_var)\n        # Put in pandas df\n        df_independent_var_imputed = pd.DataFrame(independent_var_imputed, \n                                                  index=df_independent_var.index,\n                                                  columns=df_independent_var.columns)\n        # Put back the target var\n        df_independent_var_imputed['open_flag'] = df_current['open_flag']\n        df_current = df_independent_var_imputed\n    else:\n        # Drop NA data\n        df_current.dropna(inplace=True)\n    \n    # Seperate target from independent vars\n    independent_vars = list(df_current.columns)\n    independent_vars.remove('open_flag')\n    X = df_current[independent_vars]\n    y = df_current['open_flag']\n    # Split data into train and test\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=33)\n    return df_current, X_train, X_test, y_train, y_test\n\nprocessed_train, X_train, X_test, y_train, y_test = process_training(df_train, use_imputer=True)","86bd257d":"# Decision Tree\nmodel_dt = DecisionTreeClassifier(random_state=1)\nmodel_dt.fit(X_train, y_train)\n# Predict train\npredict_train = model_dt.predict(X_train)\nprint(classification_report(y_train, predict_train))\n# Predict test\npredict_test_dt = model_dt.predict(X_test)\nprint(classification_report(y_test, predict_test_dt))","60ed49b3":"# Random Forest\nmodel_rf = RandomForestClassifier(n_estimators=500)\nmodel_rf.fit(X_train, y_train)\n\n# Predict train\npredict_train = model_rf.predict(X_train)\nprint(classification_report(y_train, predict_train))\n# Predict test\npredict_test_rf = model_rf.predict(X_test)\nprint(classification_report(y_test, predict_test_rf))","c5f102b8":"# Train SVM classifier\nmodel_svm = SVC()\nmodel_svm.fit(X_train, y_train)\n\n# Predict train\npredict_train = model_svm.predict(X_train)\nprint(classification_report(y_train, predict_train))\n# Predict test\npredict_test_svm = model_svm.predict(X_test)\nprint(classification_report(y_test, predict_test_svm))","1bb46266":"# Use XGboost\nmodel_xgb = XGBClassifier()\nmodel_xgb.fit(X_train, y_train)\n\n# Predict train\npredict_train = model_xgb.predict(X_train)\nprint(classification_report(y_train, predict_train))\n# Predict test\npredict_test_xgb = model_xgb.predict(X_test)\nprint(classification_report(y_test, predict_test_xgb))","a1bcc612":"# Train NN\ndef scalar_to_vector(ind, dim):\n    np_vector = np.zeros(dim)\n    np_vector[ind] = 1\n    return np_vector\n# Convert ys to vector to train NN\ny_train_vector = np.asarray([scalar_to_vector(i, 2) for i in y_train.to_list()])\ny_test_vector = np.asarray([scalar_to_vector(i, 2) for i in y_test.to_list()])\n# Scale Xs\nscaler = MinMaxScaler()\nscaler.fit(X_train)\nX_train_scaled = scaler.transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\n# Set up NN\nmodel_NN = keras.Sequential()\nmodel_NN.add(keras.layers.Dense(18, activation='relu', input_shape=(X_train.shape[1],)))\nmodel_NN.add(keras.layers.Dropout(rate=0.2))\nmodel_NN.add(keras.layers.Dense(2))\nmodel_NN.summary()\n# Compile model_NN before training\nmodel_NN.compile(optimizer='adam',\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])\n# Start training\nhistory = model_NN.fit(X_train_scaled, y_train_vector,\n#                     validation_data=(image_batch_val, label_batch_val),\n                    validation_split=0.2,\n                    epochs=30)\n\n# Plot performance\nplt.plot(history.epoch, history.history['accuracy'], label='Training Accuracy')\nplt.plot(history.epoch, history.history['val_accuracy'], label='Validation Accuracy')\n# plt.title('Performance with Pre-Trained model_NN ({}) without dropout'.format(pretrain_name))\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.ylim(0, 1)\nplt.legend()\n\ndef NN_predict(pred):\n    # For NN, apply softmax and obtain the highest probability\n    predict_label = tf.nn.softmax(pred)\n    predict_label = np.argmax(predict_label, axis=1)\n    return predict_label\n# Predict train\npredict_train = NN_predict(model_NN.predict(X_train_scaled))\nprint(classification_report(y_train, predict_train))\n# Predict test\npredict_test_NN = NN_predict(model_NN.predict(X_test_scaled))\nprint(classification_report(y_test, predict_test_NN))","425c845d":"# # Generate forecast from a poor man's ensemble\n# predict_test_ensemble = np.asarray(list(zip(*[predict_test_NN, predict_test_rf, predict_test_xgb])))\n# predict_test_ensemble = np.asarray([max(i) for i in predict_test_ensemble])\n# print(classification_report(y_test, predict_test_ensemble))","37154fec":"# Use models on competition data\n# Load in data\n# Email specs\ndf_test_competition = pd.read_csv('..\/input\/open-shopee-code-league-marketing-analytics\/test.csv', index_col='user_id', \n                       na_values=['Never checkout', 'Never open', 'Never login'])\n# User specs\ndf_users = pd.read_csv('..\/input\/open-shopee-code-league-marketing-analytics\/users.csv', index_col='user_id')\n# Merge User and Email specs into one\ndf_test_competition = pd.merge(df_test_competition, df_users, how='left', left_index=True, right_index=True)\n\n# Process data\n# Create month, day and time data columns\ndf_test_competition['grass_date'] = pd.to_datetime(df_test_competition.grass_date, format='%Y-%m-%d 00:00:00+08:00')\ndf_test_competition['day'] = [calendar.day_name[i.weekday()] for i in df_test_competition['grass_date'].to_list()] \n# Remove some columns\ndf_test_competition.drop(columns=['grass_date'], inplace=True)\ndf_test_competition.set_index('row_id', inplace=True)","a2772f45":"df_test_competition.isna().sum()","b268adf0":"def process_test(df_test):\n    '''\n    Process training data and seperate into train and test dataset\n    '''\n    df_current = df_test.copy()\n    # Choose following columns as independent variables\n    chosen_columns = ['country_code', 'day', 'attr_3', 'domain', \n                      'last_open_day', 'open_count_last_60_days', 'open_count_last_10_days', 'open_count_last_30_days']\n    df_current = df_current[chosen_columns]\n    # For category var, change values to str\n    categorical_columns = ['country_code', 'day', 'attr_3', 'domain']\n    for columns in categorical_columns:\n        df_current[columns] = df_current[columns].astype('str')\n    # OneHotEncoding\n    df_onehot = pd.get_dummies(df_current[categorical_columns])\n    df_current = pd.concat([df_current, df_onehot], axis=1)\n    df_current.drop(columns=categorical_columns, inplace=True)\n    return df_current\n# Process test data similar to train\nprocessed_test_competition = process_test(df_test_competition)\n\n# Some missing values in vars, need to imput\nprint('Imputing...')\nimputer = KNNImputer(n_neighbors=2)\nprocessed_test_competition_imputed = imputer.fit_transform(processed_test_competition)\n\n# Put in pandas df\nprocessed_test_competition_imputed = pd.DataFrame(processed_test_competition_imputed, \n                                                  index=processed_test_competition.index,\n                                                  columns=processed_test_competition.columns)\nprocessed_test_competition_imputed.head()","dca88a01":"# Check whether imputer works\nprocessed_test_competition_imputed.isna().sum()","0fa0e946":"# # Use poor man's ensemble\n# predictions = []\n# counter = 1\n# for model in [model_NN, model_rf, model_xgb, model_dt, model_svm]:\n#     if counter == 1:\n#         # For NN model, need to do additional processing\n#         # Invoke scaler\n#         processed_test_competition_imputed_scaled = scaler.transform(processed_test_competition_imputed)\n#         # Model prediction\n#         predict_competition = model.predict(processed_test_competition_imputed_scaled)\n#         predict_competition = NN_predict(predict_competition)\n#     else:\n#         # Model prediction\n#         predict_competition = model.predict(processed_test_competition_imputed)\n#     predictions.append(predict_competition)\n#     counter = counter + 1\n# # ensemble predict\n# predict_competition = np.asarray(list(zip(*predictions)))\n# predict_competition = np.asarray([max(i) for i in predict_competition])","be2a5d6e":"use_NN = True\nif use_NN:\n    # Invoke scaler\n    processed_test_competition_imputed_scaled = scaler.transform(processed_test_competition_imputed)\n    # Model prediction\n    predict_competition = model_NN.predict(processed_test_competition_imputed_scaled)\n    predict_competition = NN_predict(predict_competition)\nelse:\n    # Model prediction\n    predict_competition = model.predict(processed_test_competition_imputed)\n","38900d37":"# Export predictions with row_id\nprocessed_test_competition_imputed['open_flag'] = predict_competition\nprocessed_test_competition_imputed['open_flag'].to_csv('.\/submission_NN.csv')","f453a393":"For categorical values, bar plot to see the percentage of open_flag==1 across categories","27098348":"Plot the distributions of 'open flag'. Dataset is imbalanced with much more instances of email unopened","85545ccf":"Create new variable 'day' which denotes the day of week","547e8c2b":"# Data Explorations","282c7894":"For continuous vars, make use of boxplot to see distinction between open and unopen email. Those with limited or non-overlap interquatile ranges are potential variables use for predictions.","3d4f5eaa":"Check whether there are missing data."}}