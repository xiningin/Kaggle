{"cell_type":{"87e6164c":"code","d7ea17e0":"code","bb4c4648":"code","af7a7af1":"code","478080c6":"code","0e07a458":"code","abeb27ae":"code","022fd2b5":"code","c72c8c62":"code","90362c89":"code","ad335637":"markdown","da3a7537":"markdown","480f92f9":"markdown","44694c1e":"markdown","699d6a98":"markdown","390e3756":"markdown","866784f1":"markdown","853e8626":"markdown","03f416f2":"markdown","2ba56772":"markdown","7c43dcf0":"markdown"},"source":{"87e6164c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\ndf=pd.read_csv('..\/input\/family-households-with-married-couples.csv')\n# Any results you write to the current directory are saved as output.","d7ea17e0":"df[\"date\"]=pd.to_datetime(df[\"date\"])\ndf2=df.sort_values(by=\"value\")\ndf2.head(15)\n","bb4c4648":"gate_incorrect= df2[\"value\"] != \".\"\ndf2=df2[gate_incorrect]\ndf2.head(5)","af7a7af1":"df2[\"value\"]=pd.to_numeric(df2[\"value\"])\ndf2.info()","478080c6":"df2=df2.sort_values(by=\"date\")\ndf2[\"year_diff\"]=df2[\"date\"].apply(lambda x : str(x.year) + '-' + str(x.year-1) )\ndf2[\"value_diff\"]=df2[\"value\"].diff()\ndf2.tail(10)","0e07a458":"\ntemp=df2.sort_values(by=\"date\")\nplt.plot(temp[\"date\"],temp[\"value_diff\"])\nplt.xticks(rotation=60)\n","abeb27ae":"#temp=df2[(df2[\"date\"]>='1940') & (df2[\"date\"]>='1950')].sort_values(by=\"date\",ascending=False)\ntemp=df2[(df2[\"date\"]>='1940') & (df2[\"date\"]<='1950')]\nplt.plot(temp[\"year_diff\"],temp[\"value_diff\"])\nplt.xticks(rotation=45)","022fd2b5":"temp=df2[(df2[\"date\"]>='1976') & (df2[\"date\"]<='1986')]\nplt.plot(temp[\"year_diff\"],temp[\"value_diff\"])\nplt.xticks(rotation=45)","c72c8c62":"temp=df2[(df2[\"date\"]>='1990') & (df2[\"date\"]<='2000')]\nplt.bar(temp[\"year_diff\"],temp[\"value_diff\"])\nplt.xticks(rotation=45)","90362c89":"temp=df2[(df2[\"date\"]>='2004') & (df2[\"date\"]<='2010')]\nplt.bar(temp[\"year_diff\"],temp[\"value_diff\"])\nplt.xticks(rotation=45)","ad335637":"Spikes seen for the years 1993-1992 , 1998-1997 till the late 2000s.\nWhat i can remember from that era was the dot com bubble.\nMaybe people dumped a lot of money into stocks and didnt go for housing for the years 1995-1996, where we see a dip.\n\nLets move on to the last one, as i recall my reference to the famous movie \"The big short\"","da3a7537":"Huge!!! any idea what it could be? While there are tons of historic events between 1940 to 1949; World War II and the independence of South East Asian states among the top picks.... None of those apply here.\n\nAt the begining of this Exploratory Data Analysis, EDA, we saw that there was no data for the years 1941-1946. It was all dots.\nSo the calculation of the difference for the year 1947-1946 is actually for the year 1947-1940.\n\nMoving on to 1976 - 1986.","480f92f9":"In conclusion, we can say that the dataset although has only two important attributes, it does give some neat insights if you weigh them in the context of different happenings occuring throughout these eventful (close to) 80 years!\n\nWhat else can we do with this data ? How about ....\n\n* Rise and fall stats for the presidential terms of Nixon, Reagan, Bush, Clinton .....\n* Rise and fall stats for five year periods\n* Rise and fall stats correlated with the availability of lending banks in America.\n\nGot more? Happy to see 'em in the comments.","44694c1e":"Now that all the necessary pre-processing is done, we can get to the fun stuff! analysis! (Just me? ok.....)\nWhat we're interested in, is the rise and fall of these values over the years. Even if a difference column is added to the dataset, it still wont be clear which years are in the difference. For that , we add two columns!\n* difference between values\n* years between that difference\n\nsort it by date as well!","699d6a98":"There! thats better. Now that the values column is all fixed, lets convert it to a number.","390e3756":"So subprime mortgages were a huge thing back in the 2000s, the new millenium.\nMost of the folks reading this wont be stranger to the \"Rise and Fall of the Lehmann Brothers\" and the \"Bear-Stern stock crash\".\n\nLooking at the peaks, does it justify to include the above terms in this discussion?","866784f1":"**Sanity check!** \nIn years 2010 and 2009, the value was 58410 and 59118 respectively. The difference ? -708. Is that what's written in front of 2010-2009? Thankfully, Yes! \n\nNow lets see those peaks. Lets see where we have the highs and the lows.","853e8626":"I spy four notable peaks ....\n* 1940-1949\n* 1976-1986\n* Late 90s, near the dot com bubble\n* 2001-2010 (seen the movie \"The big short\" anyone?)\n\nSo lets start by looking through the years -  1940-1949 .","03f416f2":"So the first few rows in the value column seem to be dots \".\" .\nWe can fix that, just filter them out! what other options are there?\n* backfill \n* forward fill\n* mean\n\nEach method has its own implications and its best if we just stick to whats in this dataset.","2ba56772":"Ok here's something promising, we see a huge spike for the years 1979 through 1980.\n\nMoving on to the phenomenal 90s!","7c43dcf0":"US Housing over the years is an interesting topic to take up, given the fact how its a principle component of the ***\"American Dream\"***\nAfter importing the data , we get to work and see if there are any anomalies in the dataset. An easy way to do so is to usually do a sort. "}}