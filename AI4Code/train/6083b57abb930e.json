{"cell_type":{"85a8c20a":"code","085ef2d2":"code","7e43bb40":"code","47380183":"code","5127c454":"code","2ef52780":"code","da2bfcba":"code","05ae5d1f":"code","58357a20":"code","dc4e5909":"code","e6b84448":"code","6e833ddc":"code","81edda59":"code","6f098cb8":"code","01a29b85":"code","935bf65c":"code","7b0fd4ea":"code","2399b86f":"code","fd582044":"code","114f5c9b":"code","628545fd":"code","9a3be073":"code","22d9a118":"markdown","156dccfe":"markdown"},"source":{"85a8c20a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os, cv2, random, time, shutil, csv\nimport tensorflow as tf\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom tqdm import tqdm\nnp.random.seed(42)\n%matplotlib inline \n\nimport keras\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Model\nfrom keras.layers import BatchNormalization, Dense, GlobalAveragePooling2D, Lambda, Dropout, InputLayer, Input\nfrom keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.image import load_img","085ef2d2":"def get_num_files(path):\n    '''\n    Counts the number of files in a folder.\n    '''\n    if not os.path.exists(path):\n        return 0\n    return sum([len(files) for r, d, files in os.walk(path)])","7e43bb40":"#Data Paths\ntrain_dir = '\/kaggle\/input\/dog-breed-identification\/train'\ntest_dir = '\/kaggle\/input\/dog-breed-identification\/test'\n#Count\/Print train and test samples.\ndata_size = get_num_files(train_dir)\ntest_size = get_num_files(test_dir)\nprint('Data samples size: ', data_size)\nprint('Test samples size: ', test_size)","47380183":"#Read train labels.\nlabels_dataframe = pd.read_csv('\/kaggle\/input\/dog-breed-identification\/labels.csv')\n#Read sample_submission file to be modified by pridected labels.\nsample_df = pd.read_csv('\/kaggle\/input\/dog-breed-identification\/sample_submission.csv')\n#Incpect labels_dataframe.\nlabels_dataframe.head(5)","5127c454":"#Create list of alphabetically sorted labels.\ndog_breeds = sorted(list(set(labels_dataframe['breed'])))\nn_classes = len(dog_breeds)\nprint(n_classes)\ndog_breeds[:10]","2ef52780":"#Map each label string to an integer label.\nclass_to_num = dict(zip(dog_breeds, range(n_classes)))","da2bfcba":"def images_to_array(data_dir, labels_dataframe, img_size = (224,224,3)):\n    '''\n    1- Read image samples from certain directory.\n    2- Risize it, then stack them into one big numpy array.\n    3- Read sample's label form the labels dataframe.\n    4- One hot encode labels array.\n    5- Shuffle Data and label arrays.\n    '''\n    images_names = labels_dataframe['id']\n    images_labels = labels_dataframe['breed']\n    data_size = len(images_names)\n    #initailize output arrays.\n    X = np.zeros([data_size, img_size[0], img_size[1], img_size[2]], dtype=np.uint8)\n    y = np.zeros([data_size,1], dtype=np.uint8)\n    #read data and lables.\n    for i in tqdm(range(data_size)):\n        image_name = images_names[i]\n        img_dir = os.path.join(data_dir, image_name+'.jpg')\n        img_pixels = load_img(img_dir, target_size=img_size)\n        X[i] = img_pixels\n        \n        image_breed = images_labels[i]\n        y[i] = class_to_num[image_breed]\n    \n    #One hot encoder\n    y = to_categorical(y)\n    #shuffle    \n    ind = np.random.permutation(data_size)\n    X = X[ind]\n    y = y[ind]\n    print('Ouptut Data Size: ', X.shape)\n    print('Ouptut Label Size: ', y.shape)\n    return X, y","05ae5d1f":"#img_size chosen to be 331 to suit the used architectures.\nimg_size = (331,331,3)\nX, y = images_to_array(train_dir, labels_dataframe, img_size)","58357a20":"def get_features(model_name, data_preprocessor, input_size, data):\n    '''\n    1- Create a feature extractor to extract features from the data.\n    2- Returns the extracted features and the feature extractor.\n    '''\n    #Prepare pipeline.\n    input_layer = Input(input_size)\n    preprocessor = Lambda(data_preprocessor)(input_layer)\n    base_model = model_name(weights='imagenet', include_top=False,\n                            input_shape=input_size)(preprocessor)\n    avg = GlobalAveragePooling2D()(base_model)\n    feature_extractor = Model(inputs = input_layer, outputs = avg)\n    #Extract feature.\n    feature_maps = feature_extractor.predict(data, batch_size=64, verbose=1)\n    print('Feature maps shape: ', feature_maps.shape)\n    return feature_maps","dc4e5909":"# Extract features using InceptionV3 as extractor.\nfrom keras.applications.inception_v3 import InceptionV3, preprocess_input\ninception_preprocessor = preprocess_input\ninception_features = get_features(InceptionV3,\n                                  inception_preprocessor,\n                                  img_size, X)","e6b84448":"# Extract features using Xception as extractor.\nfrom keras.applications.xception import Xception, preprocess_input\nxception_preprocessor = preprocess_input\nxception_features = get_features(Xception,\n                                 xception_preprocessor,\n                                 img_size, X)","6e833ddc":"# Extract features using NASNetLarge as extractor.\nfrom keras.applications.nasnet import NASNetLarge, preprocess_input\nnasnet_preprocessor = preprocess_input\nnasnet_features = get_features(NASNetLarge,\n                               nasnet_preprocessor,\n                               img_size, X)","81edda59":"# Extract features using InceptionResNetV2 as extractor.\nfrom keras.applications.inception_resnet_v2 import InceptionResNetV2, preprocess_input\ninc_resnet_preprocessor = preprocess_input\ninc_resnet_features = get_features(InceptionResNetV2,\n                                   inc_resnet_preprocessor,\n                                   img_size, X)","6f098cb8":"#It's a good habit to free up some RAM memory.\n#X variable won't be needed anymore, so let's get rid of it.\ndel X","01a29b85":"final_features = np.concatenate([inception_features,\n                                 xception_features,\n                                 nasnet_features,\n                                 inc_resnet_features,], axis=-1)\nprint('Final feature maps shape', final_features.shape)","935bf65c":"from keras.callbacks import EarlyStopping\n#Prepare call backs\nEarlyStop_callback = keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\nmy_callback=[EarlyStop_callback]","7b0fd4ea":"#Prepare DNN model\ndnn = keras.models.Sequential([\n    InputLayer(final_features.shape[1:]),\n    Dropout(0.7),\n    Dense(n_classes, activation='softmax')\n])\n\ndnn.compile(optimizer='adam',\n              loss='categorical_crossentropy',\n              metrics=['accuracy'])\n\n#Train simple DNN on extracted features.\nh = dnn.fit(final_features, y,\n            batch_size=128,\n            epochs=60,\n            validation_split=0.1,\n            callbacks=my_callback)","2399b86f":"def images_to_array2(data_dir, labels_dataframe, img_size = (224,224,3)):\n    '''\n    Do same as images_to_array but omit some unnecessary steps for test data.\n    '''\n    images_names = labels_dataframe['id']\n    data_size = len(images_names)\n    X = np.zeros([data_size, img_size[0], img_size[1], 3], dtype=np.uint8)\n    \n    for i in tqdm(range(data_size)):\n        image_name = images_names[i]\n        img_dir = os.path.join(data_dir, image_name+'.jpg')\n        img_pixels = tf.keras.preprocessing.image.load_img(img_dir, target_size=img_size)\n        X[i] = img_pixels\n        \n    print('Ouptut Data Size: ', X.shape)\n    return X\n\ntest_data = images_to_array2(test_dir, sample_df, img_size)","fd582044":"#Extract test data features.\ninception_features = get_features(InceptionV3, inception_preprocessor, img_size, test_data)\nxception_features = get_features(Xception, xception_preprocessor, img_size, test_data)\nnasnet_features = get_features(NASNetLarge, nasnet_preprocessor, img_size, test_data)\ninc_resnet_features = get_features(InceptionResNetV2, inc_resnet_preprocessor, img_size, test_data)\n\ntest_features = np.concatenate([inception_features,\n                                 xception_features,\n                                 nasnet_features,\n                                 inc_resnet_features],axis=-1)\nprint('Final feature maps shape', test_features.shape)","114f5c9b":"#Free up some space.\ndel test_data","628545fd":"#Predict test labels given test data features.\ny_pred = dnn.predict(test_features, batch_size=128)","9a3be073":"#Create submission file\nfor b in dog_breeds:\n    sample_df[b] = y_pred[:,class_to_num[b]]\nsample_df.to_csv('pred.csv', index=None)","22d9a118":"**Introduction**\n\n* This kernel is a detailed guide for transfer learning on Dog Breeds problem, it's all about learning a new technique, evaluate it using only Kaggle training set without cheating.\n\n* The aim of this kernel is to show you how to use pre-trained CNN models as feature extractors, which one of the most effective transfer learning techniques.\n\n* A reasonable question comes to your mind, 'Wait, why do we have to use this technique, why don't we just use regular transfer learning ?', if you try to do so, you will figure out that the problem is pretty hard for a single model to handle (you would get higher loss and less accuracy).\n\n* It's even hard for humankind to distinguish between 120 dog breeds!, single poor CNN would struggle.\n\n**Explanation**\n\n* Take look at general CNN architecture for image classification in two main parts, \u201cfeature extractor\u201d that based on conv-layers, and \u201cclassifier\u201d which usually based on fully connected layers:\n\n![image.png](attachment:image.png)\n\n* Simply, feature extractor could be created as follow > (Feature Extractor = Pretrained Model - Late Fully Connected Layers)\n\n* For example, InceptionV3 feature extractor (without last FC layer) outputs 2048 vector for each image sample, each value represent a certain feature of dog image (Coded in numerical values of course), like Dog color?, How big is his head?, Shape of the eyes?, length of the tale?, Size? .. etc\n\n* Hence, more \"different\" feature extractors mean more features to be used to determine which breed does this dog belong.\n\n* So our strategy goes as the following,\n  1. Create 4 feature extractor using different pre-trained CNN models\n  2. Extract features from raw data and stacks the features together.\n  3. Use a simple DNN with one dense layer and a heavy dropout layer to figure out patterns in the feature extracted from the data.\n     \n     \n* The code is simple, concise and fully-commented. Feel free to ask for help \/ more info \/ more explanation in the comments.\n\n* Finally if this kernel helps you somehow, kindly don't forget to leave a little upvote.\n\n* ENJOY.\n","156dccfe":"Inspired by: [https:\/\/www.kaggle.com\/c\/dog-breed-identification\/discussion\/40779](http:\/\/)"}}