{"cell_type":{"79cd9142":"code","2dbbba40":"code","542caef7":"code","102c2547":"code","9baff915":"code","9ba89296":"code","fcd4e949":"code","656e2c49":"code","2ddc01e7":"code","9cf89a08":"code","52438db8":"code","f23bff0d":"code","c65dc65d":"code","e0880d78":"code","888ee8a5":"code","74a02e7f":"code","a1e5c45d":"code","cdbd6dcd":"code","a791aa89":"code","e12d4811":"code","688f38b7":"code","9243dd44":"code","e8406d18":"code","e52dafe1":"code","7f3bf7c4":"code","b91da149":"code","a45b8f6d":"code","8fb41de6":"code","e37d2877":"code","331ca5ea":"code","1fe6134e":"code","5dbe77d1":"code","93063462":"code","2e24fa56":"code","66627483":"code","437adc74":"markdown","f1d41f3a":"markdown","a133aa4b":"markdown","7e86535d":"markdown","dd800da4":"markdown","98798db3":"markdown","f5593c36":"markdown","022c3aba":"markdown"},"source":{"79cd9142":"import pandas as pd\nimport numpy as np\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk import word_tokenize\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.svm import LinearSVC\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom sklearn.model_selection import cross_val_score, train_test_split\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\npd.set_option('display.notebook_repr_html', True)","2dbbba40":"train = pd.read_csv(\"..\/input\/train.csv\")\ntrain.head()","542caef7":"test = pd.read_csv(\"..\/input\/test.csv\")\ntest.head()","102c2547":"df = pd.concat([train,test])\ndf.head()","9baff915":"train.shape, test.shape, df.shape","9ba89296":"df.dtypes","fcd4e949":"df.isnull().sum()    # label is expected one as test dataset does not have label","656e2c49":"df['label'].value_counts()","2ddc01e7":"sns.countplot(df['label'])","9cf89a08":"# Cleaning Raw tweets\ndef clean_text(text):\n    \n    #remove emails\n    text = ' '.join([i for i in text.split() if '@' not in i])\n    \n    #remove web address\n    text = re.sub('http[s]?:\/\/\\S+', '', text)\n    \n    #Filter to allow only alphabets\n    text = re.sub(r'[^a-zA-Z\\']', ' ', text)\n    \n    #Remove Unicode characters\n    text = re.sub(r'[^\\x00-\\x7F]+', '', text)\n    \n    #Convert to lowercase to maintain consistency\n    text = text.lower()\n    \n    #remove double spaces \n    text = re.sub('\\s+', ' ',text)\n    \n    return text\n\ndf[\"clean_tweet\"] = df['tweet'].apply(lambda x: clean_text(x))","52438db8":"df.head()","f23bff0d":"#defining stop words\nSTOP_WORDS = ['a', 'about', 'above', 'after', 'again', 'against', 'all', 'also', 'am', 'an', 'and',\n              'any', 'are', \"aren't\", 'as', 'at', 'be', 'because', 'been', 'before', 'being', 'below',\n              'between', 'both', 'but', 'by', 'can', \"can't\", 'cannot', 'com', 'could', \"couldn't\", 'did',\n              \"didn't\", 'do', 'does', \"doesn't\", 'doing', \"don't\", 'down', 'during', 'each', 'else', 'ever',\n              'few', 'for', 'from', 'further', 'get', 'had', \"hadn't\", 'has', \"hasn't\", 'have', \"haven't\", 'having',\n              'he', \"he'd\", \"he'll\", \"he's\", 'her', 'here', \"here's\", 'hers', 'herself', 'him', 'himself', 'his', 'how',\n              \"how's\", 'however', 'http', 'i', \"i'd\", \"i'll\", \"i'm\", \"i've\", 'if', 'in', 'into', 'is', \"isn't\", 'it',\n              \"it's\", 'its', 'itself', 'just', 'k', \"let's\", 'like', 'me', 'more', 'most', \"mustn't\", 'my', 'myself',\n              'no', 'nor', 'not', 'of', 'off', 'on', 'once', 'only', 'or', 'other', 'otherwise', 'ought', 'our', 'ours',\n              'ourselves', 'out', 'over', 'own', 'r', 'same', 'shall', \"shan't\", 'she', \"she'd\", \"she'll\", \"she's\",\n              'should', \"shouldn't\", 'since', 'so', 'some', 'such', 'than', 'that', \"that's\", 'the', 'their', 'theirs',\n              'them', 'themselves', 'then', 'there', \"there's\", 'these', 'they', \"they'd\", \"they'll\", \"they're\",\n              \"they've\", 'this', 'those', 'through', 'to', 'too', 'under', 'until', 'up', 'very', 'was', \"wasn't\",\n              'we', \"we'd\", \"we'll\", \"we're\", \"we've\", 'were', \"weren't\", 'what', \"what's\", 'when', \"when's\", 'where',\n              \"where's\", 'which', 'while', 'who', \"who's\", 'whom', 'why', \"why's\", 'with', \"won't\", 'would', \"wouldn't\",\n              'www', 'you', \"you'd\", \"you'll\", \"you're\", \"you've\", 'your', 'yours', 'yourself', 'yourselves']","c65dc65d":"#remove stopwords\ndf['cleaned_tweets'] = df['clean_tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in STOP_WORDS]))","e0880d78":"#from nltk.stem.porter import PorterStemmer\n#ps = PorterStemmer()\n#df['cleaned_tweets'] = df['clean_tweet'].apply(lambda x: ' '.join([ps.stem(word) for word in x.split() if word not in STOP_WORDS]))","888ee8a5":"#remove stopwords\n#df['cleaned_tweets'] = df['clean_tweet'].apply(lambda x: ' '.join([word for word in x.split() if word not in stopwords.words('english')]))","74a02e7f":"# feature length of tweet\ndf['word_count'] = df['cleaned_tweets'].str.split().apply(lambda x :len(x))","a1e5c45d":"# drop not needed features\ndf_1 = df.copy()\ndf.drop(['tweet','clean_tweet'],axis=1,inplace=True)\n#df.drop(['tweet','clean_tweet','word_count'],axis=1,inplace=True)","cdbd6dcd":"df.head()","a791aa89":"train = df[:7920]\ntest = df[7920:]\ndf_t = test.copy()","e12d4811":"train.shape, test.shape","688f38b7":"X = train.drop('label',axis=1)\ny = train['label']","9243dd44":"X.shape, y.shape","e8406d18":"test.drop('label',axis=1,inplace=True)","e52dafe1":"X = X['cleaned_tweets'].astype('category')\n#X = X['word_count'].astype('category')\ntest = test['cleaned_tweets'].astype('category')\n#test = test['word_count'].astype('category')","7f3bf7c4":"#Train test Split\n#X_train, X_test, y_train, y_test = train_test_split(X,y, test_size = 0.3, random_state = 1)","b91da149":"vect = CountVectorizer(max_features = 2500)\nvect.fit(X)\nX_train_vec = vect.transform(X)\nX_test_vec = vect.transform(test)","a45b8f6d":"model = RandomForestClassifier(n_estimators=100,max_depth=5)\nmodel.fit(X_train_vec,y)\nrf = model.predict(X_test_vec)\nsubmission = pd.DataFrame({\n        \"id\": df_t[\"id\"],\n        \"label\":rf\n    })\n\nsubmission.to_csv('sentiments_submission.csv', index=False)","8fb41de6":"clf = SGDClassifier(max_iter=5, tol=None)\nclf.fit(X_train_vec, y)\ny_pred_SGD = clf.predict(X_test_vec)\n\nsubmission = pd.DataFrame({\n        \"id\": df_t[\"id\"],\n        \"label\":y_pred_SGD\n    })\n\nsubmission.to_csv('sentiments_submission_SGD.csv', index=False)","e37d2877":"import xgboost\nclassifier = xgboost.XGBClassifier(n_estimators=210)\nclassifier.fit(X_train_vec, y)\n# Predicting the Test set results\ny_pred_XGB = classifier.predict(X_test_vec)\nsubmission = pd.DataFrame({\n        \"id\": df_t[\"id\"],\n        \"label\":y_pred_XGB\n    })\n\nsubmission.to_csv('sentiments_submission_XGB.csv', index=False)","331ca5ea":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression()\nclf.fit(X_train_vec, y)\ny_pred_log_reg = clf.predict(X_test_vec)\nsubmission = pd.DataFrame({\n        \"id\": df_t[\"id\"],\n        \"label\":y_pred_log_reg\n    })\n\nsubmission.to_csv('sentiments_submission_LRG.csv', index=False)","1fe6134e":"from sklearn.neighbors import KNeighborsClassifier\nclf = KNeighborsClassifier(n_neighbors = 4)\nclf.fit(X_train_vec, y)\ny_pred_knn = clf.predict(X_test_vec)\nsubmission = pd.DataFrame({\n        \"id\": df_t[\"id\"],\n        \"label\":y_pred_knn\n    })\n\nsubmission.to_csv('sentiments_submission_knn.csv', index=False)","5dbe77d1":"from sklearn.linear_model import Perceptron\nclf = Perceptron(max_iter=6, tol=None)\nclf.fit(X_train_vec, y)\ny_pred_perceptron = clf.predict(X_test_vec)\nsubmission = pd.DataFrame({\n        \"id\": df_t[\"id\"],\n        \"label\":y_pred_perceptron\n    })\n\nsubmission.to_csv('sentiments_submission_per.csv', index=False)","93063462":"X_train_vec=X_train_vec.astype('float64')\nX_test_vec=X_test_vec.astype('float64')","2e24fa56":"X_train_vec","66627483":"import lightgbm as lgb\ntrain_data=lgb.Dataset(X_train_vec,label=y)\n#define parameters\nparams = {'n_estimators':213,'objective':'binary','learning_rate':0.2,'max_depth': 10,'num_leaves':'100','min_data_in_leaf':9,'max_bin':100,'boosting_type':'gbdt',}\nmodel= lgb.train(params, train_data, 100) \ny_pred_LGB=model.predict(X_test_vec)\n#rounding the values\ny_pred_LGB=y_pred_LGB.round(0)\n#converting from float to integer\ny_pred_LGB=y_pred_LGB.astype(int)\nsubmission = pd.DataFrame({\n        \"id\": df_t[\"id\"],\n        \"label\":y_pred_LGB\n    })\n\nsubmission.to_csv('sentiments_submission_LGB.csv', index=False)","437adc74":"- To perform further analysis we need to transform our data into a format that can be processed by our machine learning models.\n\n- CountVectorizer does text preprocessing, tokenizing and filtering of stopwords and it builds a dictionary of features and transform documents to feature vectors.\n- TfidfTransformer transforms the above vector by dividing the number of occurrences of each word in a document by the total number of words in the document. These new features are called tf or Term Frequencies.","f1d41f3a":"### EDA","a133aa4b":"Hackthon on Analytics Vidya: With LGB model,I was able to get F1-score as 88.88 and rank 220","7e86535d":"## Identify the Sentiments","dd800da4":"#### for lightgbm features needed is in float type","98798db3":"### Random Forest","f5593c36":"Sentiment analysis remains one of the key problems that has seen extensive application of natural language processing. This time around, given the tweets from customers about various tech firms who manufacture and sell mobiles, computers, laptops, etc, the task is to identify if the tweets have a negative sentiment towards such companies or products.\n\n \n\nEvaluation Metric\nThe metric used for evaluating the performance of classification model would be weighted F1-Score.\n\nData\ntrain.csv - For training the models, we provide a labelled dataset of 7920 tweets. The dataset is provided in the form of a csv file with each line storing a tweet id, its label and the tweet.\n\ntest.csv - The test data file contains only tweet ids and the tweet text with each tweet in a new line.","022c3aba":"### Model Building"}}