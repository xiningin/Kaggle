{"cell_type":{"7618ab3b":"code","d201ed1f":"code","132b2a96":"code","2e624900":"code","f7146dc8":"code","2f0812c2":"code","60137106":"code","98dfcca7":"code","493cfb40":"code","56bed00a":"code","82e61941":"code","9eddb6ea":"code","f0983b7f":"code","24864f05":"code","5714ab93":"code","ae30b627":"code","a855746b":"code","a15653ea":"code","22193c64":"code","cf26a2ab":"code","02c5ec7e":"code","ad880711":"code","1675642f":"code","be18e440":"code","e3597e36":"code","970c043d":"markdown","bc39961b":"markdown","4412f35c":"markdown","5dd88211":"markdown","e67e196e":"markdown","720e4a28":"markdown","6e119df0":"markdown","d55a906b":"markdown","e7105074":"markdown","c32b89f9":"markdown","74f641bc":"markdown","a8e83033":"markdown","7581a7a0":"markdown"},"source":{"7618ab3b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d201ed1f":"import json\nfile_path = \"..\/input\/cat_to_name.json\"\ndata = json.loads(open(file_path).read())\nlen(data)","132b2a96":"classes = list()\nfor i in range(len(data)):\n    classes.append(data['{}'.format(i+1)])\n    \nlen(classes)","2e624900":"import torch\nfrom torchvision import datasets, transforms,models\nfrom torch.utils.data import DataLoader\n\ndata_dir = \"..\/input\/flower_data\/flower_data\"\n\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\ntrain_transform = transforms.Compose([\n                                transforms.Resize(255),\n                                transforms.RandomResizedCrop(224),\n                                transforms.RandomHorizontalFlip(),\n                                transforms.ColorJitter(),\n                                transforms.ToTensor(),\n                                transforms.Normalize(mean, std)])\n\ntest_transform = transforms.Compose([\n                                transforms.Resize(255),\n                                transforms.CenterCrop(224),\n                                transforms.ToTensor(),\n                                transforms.Normalize(mean, std)])\n\ntrain_data = datasets.ImageFolder(data_dir+\"\/train\", transform=train_transform)\ntest_data = datasets.ImageFolder(data_dir+\"\/valid\", transform=test_transform)\n\ntrainloader = torch.utils.data.DataLoader(train_data, batch_size=200, shuffle=True)\ntestloader = torch.utils.data.DataLoader(test_data, batch_size=200)\n\nlen(trainloader)","f7146dc8":"# visualize data\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata_iter = iter(trainloader)\nimages, labels = data_iter.next()\n\nfig = plt.figure(figsize=(25, 5))\nfor idx in range(2):\n    ax = fig.add_subplot(1, 5, idx + 1, xticks=[], yticks=[])\n    # unnormolaize first\n    img = images[idx] \/ 2 + 0.5\n    npimg = img.numpy()\n    img = np.transpose(npimg, (1, 2, 0)) #transpose\n    ax.imshow(img, cmap='gray')\n    title = classes[labels[idx]] + f\"\\tNumber: {idx}\"\n    ax.set_title(title)","2f0812c2":"model = models.densenet161(pretrained=True)\nmodel.classifier","60137106":"# Freeze parameters\nfor param in model.parameters():\n    param.requires_grad = False","98dfcca7":"import torch.nn as nn\nfrom collections import OrderedDict\n\nclassifier = nn.Sequential(\n  nn.Linear(in_features=2208, out_features=2208),\n  nn.ReLU(),\n  nn.Dropout(p=0.4),\n  nn.Linear(in_features=2208, out_features=1024),\n  nn.ReLU(),\n  nn.Dropout(p=0.3),\n  nn.Linear(in_features=1024, out_features=102),\n  nn.LogSoftmax(dim=1)  \n)\n    \nmodel.classifier = classifier\nmodel.classifier","493cfb40":"import torch.optim as optim\nimport torch\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.to(device)\n\ncriterion = nn.NLLLoss()\noptimizer = optim.Adam(model.classifier.parameters(), lr=0.003)\n# turn this off\nscheduler = optim.lr_scheduler.StepLR(optimizer, step_size=25, gamma=0.1)","56bed00a":"import time\n\ndef train_model(model,\n                train_loader,\n                valid_loader,\n                n_epochs,\n                optimizer,\n                scheduler,\n                criterion,\n                name=\"model.pt\",\n                path=None):\n    # compare overfited\n    train_loss_data, valid_loss_data = [], []\n    # check for validation loss\n    valid_loss_min = np.Inf\n    # calculate time\n    since = time.time()\n\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    for epoch in range(n_epochs):\n        print(\"Epoch: {}\/{}\".format(epoch + 1, n_epochs))\n        # monitor training loss\n        train_loss = 0.0\n        valid_loss = 0.0\n        total = 0\n        correct = 0\n        e_since = time.time()\n\n        ###################\n        # train the model #\n        ###################\n        model.train()  # prep model for training\n        scheduler.step()  # step up scheduler\n        for images, labels in train_loader:\n            # Move input and label tensors to the default device\n            images, labels = images.to(device), labels.to(device)\n            # clear the gradients of all optimized variables\n            optimizer.zero_grad()\n            # forward pass: compute predicted outputs by passing inputs to the model\n            log_ps = model(images)\n            # calculate the loss\n            loss = criterion(log_ps, labels)\n            # backward pass: compute gradient of the loss with respect to model parameters\n            loss.backward()\n            # perform a single optimization step (parameter update)\n            optimizer.step()\n            # update running training loss\n            train_loss += loss.item() * images.size(0)\n\n        ######################\n        # validate the model #\n        ######################\n        print(\"\\t\\tGoing for validation\")\n        model.eval()  # prep model for evaluation\n        for data, target in valid_loader:\n            # Move input and label tensors to the default device\n            data, target = data.to(device), target.to(device)\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(data)\n            # calculate the loss\n            loss_p = criterion(output, target)\n            # update running validation loss\n            valid_loss += loss_p.item() * data.size(0)\n            # calculate accuracy\n            proba = torch.exp(output)\n            top_p, top_class = proba.topk(1, dim=1)\n            equals = top_class == target.view(*top_class.shape)\n\n            _, predicted = torch.max(output.data, 1)\n            total += target.size(0)\n            correct += (predicted == target).sum().item()\n\n        # print training\/validation statistics\n        # calculate average loss over an epoch\n        train_loss = train_loss \/ len(train_loader.dataset)\n        valid_loss = valid_loss \/ len(valid_loader.dataset)\n\n        # calculate train loss and running loss\n        train_loss_data.append(train_loss * 100)\n        valid_loss_data.append(valid_loss * 100)\n\n        print(\"\\tTrain loss:{:.6f}..\".format(train_loss),\n              \"\\tValid Loss:{:.6f}..\".format(valid_loss),\n              \"\\tAccuracy: {:.4f}\".format(correct \/ total * 100))\n\n        # save model if validation loss has decreased\n        if valid_loss <= valid_loss_min:\n            print('\\tValidation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n                valid_loss_min,\n                valid_loss))\n            torch.save(model.state_dict(), name)\n            valid_loss_min = valid_loss\n            # save to google drive\n            if path is not None:\n                torch.save(model.state_dict(), path)\n\n        # Time take for one epoch\n        time_elapsed = time.time() - e_since\n        print('\\tEpoch:{} completed in {:.0f}m {:.0f}s'.format(\n            epoch + 1, time_elapsed \/\/ 60, time_elapsed % 60))\n\n    # compare total time\n    time_elapsed = time.time() - since\n    print('Training completed in {:.0f}m {:.0f}s'.format(\n        time_elapsed \/\/ 60, time_elapsed % 60))\n\n    # return the model\n    return [model, train_loss_data, valid_loss_data]","82e61941":"total_epoch = 30","9eddb6ea":"model, train_loss, valid_loss = train_model(model, trainloader,\n          testloader, total_epoch, optimizer,scheduler, criterion)","f0983b7f":"model.load_state_dict(torch.load('model.pt'))","24864f05":"plt.plot(train_loss, label=\"Training loss\")\nplt.plot(valid_loss, label=\"validation loss\")\nplt.legend(frameon=False)","5714ab93":"def testModel(model, loader, device, criterion):\n    \n    test_loss = 0\n    accuracy = 0\n\n    with torch.no_grad():\n        \n        model.eval()\n\n        for inputs, labels in loader:\n            inputs, labels = inputs.to(device), labels.to(device)\n            logps = model.forward(inputs)\n            batch_loss = criterion(logps, labels)\n\n            test_loss += batch_loss.item()\n\n            # Calculate accuracy\n            ps = torch.exp(logps)\n            top_p, top_class = ps.topk(1, dim=1)\n            equals = top_class == labels.view(*top_class.shape)\n            accuracy += torch.mean(equals.type(torch.FloatTensor)).item()\n\n    print(\"Test Loss:{:.6f}\".format(test_loss),\n          \"\\nAccuracy: {:.4f}\".format(accuracy \/ len(loader) * 100))","ae30b627":"testModel(model, testloader, device, criterion)","a855746b":"def test2(model, loader, device, criterion):\n    test_loss = 0.0\n    class_correct = list(0. for i in range(102))\n    class_total = list(0. for i in range(102))\n\n    with torch.no_grad():\n        model.eval()\n        # iterate over test data\n        for data, target in loader:\n            # move tensors to GPU if CUDA is available\n            data, target = data.to(device), target.to(device)\n            # forward pass: compute predicted outputs by passing inputs to the model\n            output = model(data)\n            # calculate the batch loss\n            loss = criterion(output, target)\n            # update test loss\n            test_loss += loss.item() * data.size(0)\n            # convert output probabilities to predicted class\n            _, pred = torch.max(output, 1)\n            # compare predictions to true label\n            correct = np.squeeze(pred.eq(target.data.view_as(pred)))\n            # calculate test accuracy for each object class\n            for i in range(len(target)):\n                label = target.data[i]\n                class_correct[label] += correct[i].item()\n                class_total[label] += 1\n\n    # average test loss\n    test_loss = test_loss \/ len(loader.dataset)\n    print('Test Loss: {:.6f}'.format(test_loss))\n\n    print('\\nTest Accuracy (Overall): %2d%% (%2d\/%2d)' % (\n        100. * np.sum(class_correct) \/ np.sum(class_total),\n        np.sum(class_correct), np.sum(class_total)))","a15653ea":"test2(model, testloader, device, criterion)","22193c64":"from PIL import Image\n\ndef test(file):\n  ids = trainloader.dataset.class_to_idx\n\n  with Image.open(file) as f:\n      img = test_transform(f).unsqueeze(0)\n      with torch.no_grad():\n          out = model(img.to(device)).cpu().numpy()\n          for key, value in ids.items():\n              if value == np.argmax(out):\n                    name = classes[int(key)]\n                    print(f\"Predicted Label:{name} and Key {key} and value {value}\")\n          plt.imshow(np.array(f))\n          plt.show()","cf26a2ab":"data_dir = '..\/input\/flower_data\/flower_data\/'\nfile =  data_dir + \"\/valid\/56\/image_02858.jpg\"\nprint(file)\nprint(f\"Actual Label: {classes[56]}\")\ntest(file)","02c5ec7e":"print(os.listdir(\"..\/input\/test set\/\"))","ad880711":"import torch\nfrom torchvision import datasets, transforms,models\nfrom torch.utils.data import DataLoader\n\ndata_dir = \"..\/input\"\n\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\n\nvalid_transform = transforms.Compose([\n                                transforms.Resize(255),\n                                transforms.CenterCrop(224),\n                                transforms.ToTensor(),\n                                transforms.Normalize(mean, std)])\n\n\nvalid_data = datasets.ImageFolder(data_dir+\"\/test set\", transform=valid_transform)\n\nvalidloader = torch.utils.data.DataLoader(valid_data, batch_size=200)\n\nlen(validloader)","1675642f":"# visualize data\nimport numpy as np\nimport matplotlib.pyplot as plt\n\ndata_iter = iter(validloader)\nimages, labels = data_iter.next()\n\nfig = plt.figure(figsize=(25, 5))\nfor idx in range(2):\n    ax = fig.add_subplot(1, 5, idx + 1, xticks=[], yticks=[])\n    # unnormolaize first\n    img = images[idx] \/ 2 + 0.5\n    npimg = img.numpy()\n    img = np.transpose(npimg, (1, 2, 0)) #transpose\n    ax.imshow(img, cmap='gray')\n    #title = classes[labels[idx]] + f\"\\tNumber: {idx}\"\n    #ax.set_title(title)","be18e440":"from PIL import Image\n\ndef test(file, i):\n  ids = trainloader.dataset.class_to_idx\n  v = 0\n\n  with Image.open(file) as f:\n      img = test_transform(f).unsqueeze(0)\n      with torch.no_grad():\n          out = model(img.to(device))\n          output = torch.exp(out)\n          probs,top_class = output.topk(1, dim=1)\n          class_name = classes[(top_class.item())]\n          print(\"File Name: \", i, \"\\tPredicted Label: \", class_name, \"\\tAnd Label in Number: \",top_class.item())","e3597e36":"import glob\nimport random\n\npath = \"..\/input\/test set\/test set\/\"\ndata = (os.listdir(path))\n\nfor i in data:\n    name = path+\"\/\"+i\n    try:\n        test(name, i)\n    except:\n        print(\"Something went wrong\")","970c043d":"# Test with another way","bc39961b":"## Util class for train","4412f35c":"## Load pretrained Model","5dd88211":"# Test a single random flower","e67e196e":"# Test with test set","720e4a28":"## Prepare Data Class","6e119df0":"## Replace Classifier","d55a906b":"## Visualize Data","e7105074":"# Test Helper util class","c32b89f9":"## Freeze Parameters","74f641bc":"# Test with a single pic","a8e83033":"## Prepare Loader","7581a7a0":"# Test"}}