{"cell_type":{"b37f8b34":"code","3797b1cf":"code","c48c3610":"code","b9a2d5bd":"code","70821409":"code","6d256b5a":"code","5684cc57":"code","0bad8850":"code","3c54abea":"code","8856aa47":"code","6c428ed4":"code","911075ee":"code","965d24d1":"code","32867e28":"code","84d4bf35":"code","e3854238":"code","9026754c":"code","86c8bd85":"code","f382b2bc":"code","7913d695":"code","99675723":"markdown","d1f574ff":"markdown","e6ea895b":"markdown","f5cb9521":"markdown","aaaa3701":"markdown","a9e8bf91":"markdown","dd3d7026":"markdown","2070baa2":"markdown","9b861b75":"markdown","a8fafd48":"markdown","64c2e546":"markdown","13bf5600":"markdown","35d4da7a":"markdown","1f1fb49e":"markdown"},"source":{"b37f8b34":"# module imports\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport itertools\nimport random\n\n# model imports\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\n\n# processing imports\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\nprint('Welcome!')","3797b1cf":"# fetch the training file\nfile_path_20_percent = '..\/input\/nslkdd\/KDDTrain+_20Percent.txt'\nfile_path_full_training_set = '..\/input\/nslkdd\/KDDTrain+.txt'\nfile_path_test = '..\/input\/nslkdd\/KDDTest+.txt' \n\n#df = pd.read_csv(file_path_20_percent)\ndf = pd.read_csv(file_path_full_training_set)\ntest_df = pd.read_csv(file_path_test)","c48c3610":"# add the column labels\ncolumns = (['duration'\n,'protocol_type'\n,'service'\n,'flag'\n,'src_bytes'\n,'dst_bytes'\n,'land'\n,'wrong_fragment'\n,'urgent'\n,'hot'\n,'num_failed_logins'\n,'logged_in'\n,'num_compromised'\n,'root_shell'\n,'su_attempted'\n,'num_root'\n,'num_file_creations'\n,'num_shells'\n,'num_access_files'\n,'num_outbound_cmds'\n,'is_host_login'\n,'is_guest_login'\n,'count'\n,'srv_count'\n,'serror_rate'\n,'srv_serror_rate'\n,'rerror_rate'\n,'srv_rerror_rate'\n,'same_srv_rate'\n,'diff_srv_rate'\n,'srv_diff_host_rate'\n,'dst_host_count'\n,'dst_host_srv_count'\n,'dst_host_same_srv_rate'\n,'dst_host_diff_srv_rate'\n,'dst_host_same_src_port_rate'\n,'dst_host_srv_diff_host_rate'\n,'dst_host_serror_rate'\n,'dst_host_srv_serror_rate'\n,'dst_host_rerror_rate'\n,'dst_host_srv_rerror_rate'\n,'attack'\n,'level'])\n\ndf.columns = columns\ntest_df.columns = columns\n\n# sanity check\ndf.head()","b9a2d5bd":"# map normal to 0, all attacks to 1\nis_attack = df.attack.map(lambda a: 0 if a == 'normal' else 1)\ntest_attack = test_df.attack.map(lambda a: 0 if a == 'normal' else 1)\n\n#data_with_attack = df.join(is_attack, rsuffix='_flag')\ndf['attack_flag'] = is_attack\ntest_df['attack_flag'] = test_attack\n\n# view the result\ndf.head()","70821409":"np.shape(df)","6d256b5a":"set(df['protocol_type'])","5684cc57":"set(df['attack'])","0bad8850":"set(df['service'])","3c54abea":"# lists to hold our attack classifications\ndos_attacks = ['apache2','back','land','neptune','mailbomb','pod','processtable','smurf','teardrop','udpstorm','worm']\nprobe_attacks = ['ipsweep','mscan','nmap','portsweep','saint','satan']\nprivilege_attacks = ['buffer_overflow','loadmdoule','perl','ps','rootkit','sqlattack','xterm']\naccess_attacks = ['ftp_write','guess_passwd','http_tunnel','imap','multihop','named','phf','sendmail','snmpgetattack','snmpguess','spy','warezclient','warezmaster','xclock','xsnoop']\n\n# we will use these for plotting below\nattack_labels = ['Normal','DoS','Probe','Privilege','Access']\n\n# helper function to pass to data frame mapping\ndef map_attack(attack):\n    if attack in dos_attacks:\n        # dos_attacks map to 1\n        attack_type = 1\n    elif attack in probe_attacks:\n        # probe_attacks mapt to 2\n        attack_type = 2\n    elif attack in privilege_attacks:\n        # privilege escalation attacks map to 3\n        attack_type = 3\n    elif attack in access_attacks:\n        # remote access attacks map to 4\n        attack_type = 4\n    else:\n        # normal maps to 0\n        attack_type = 0\n        \n    return attack_type\n\n# map the data and join to the data set\nattack_map = df.attack.apply(map_attack)\ndf['attack_map'] = attack_map\n\ntest_attack_map = test_df.attack.apply(map_attack)\ntest_df['attack_map'] = test_attack_map\n\n# view the result\ndf.head()","8856aa47":"set(df['attack_map'])","6c428ed4":"# # get a series with the count of each flag for attack and normal traffic\n# normal_flags = df.loc[df.attack_flag == 0].flag.value_counts()\n# attack_flags = df.loc[df.attack_flag == 1].flag.value_counts()\n\n# # create the charts\n# flag_axs = bake_pies([normal_flags, attack_flags], ['normal','attack'])        \n# plt.show()","911075ee":"# # get a series with the count of each service for attack and normal traffic\n# normal_services = df.loc[df.attack_flag == 0].service.value_counts()\n# attack_services = df.loc[df.attack_flag == 1].service.value_counts()\n\n# # create the charts\n# service_axs = bake_pies([normal_services, attack_services], ['normal','attack'])        \n# plt.show()\n","965d24d1":"# get the intial set of encoded features and encode them\nfeatures_to_encode = ['protocol_type', 'service', 'flag']\nencoded = pd.get_dummies(df[features_to_encode])\ntest_encoded_base = pd.get_dummies(test_df[features_to_encode])\n\n# not all of the features are in the test set, so we need to account for diffs\ntest_index = np.arange(len(test_df.index))\ncolumn_diffs = list(set(encoded.columns.values)-set(test_encoded_base.columns.values))\n\ndiff_df = pd.DataFrame(0, index=test_index, columns=column_diffs)\n\n# we'll also need to reorder the columns to match, so let's get those\ncolumn_order = encoded.columns.to_list()\n\n# append the new columns\ntest_encoded_temp = test_encoded_base.join(diff_df)\n\n# reorder the columns\ntest_final = test_encoded_temp[column_order].fillna(0)\n\n# get numeric features, we won't worry about encoding these at this point\nnumeric_features = ['duration', 'src_bytes', 'dst_bytes']\n\n# model to fit\/test\nto_fit = encoded.join(df[numeric_features])\ntest_set = test_final.join(test_df[numeric_features])","32867e28":"# create our target classifications\nbinary_y = df['attack_flag']\nmulti_y = df['attack_map']\n\ntest_binary_y = test_df['attack_flag']\ntest_multi_y = test_df['attack_map']\n\n# build the training sets\nbinary_train_X, binary_val_X, binary_train_y, binary_val_y = train_test_split(to_fit, binary_y, test_size=0.6)\nmulti_train_X, multi_val_X, multi_train_y, multi_val_y = train_test_split(to_fit, multi_y, test_size = 0.6)","84d4bf35":"binary_train_X.info()","e3854238":"binary_train_X.sample(5)","9026754c":"# model for the binary classification\nbinary_model = RandomForestClassifier()\nbinary_model.fit(binary_train_X, binary_train_y)\nbinary_predictions = binary_model.predict(binary_val_X)\n\n# calculate and display our base accuracty\nbase_rf_score = accuracy_score(binary_predictions,binary_val_y)\nbase_rf_score","86c8bd85":"# define the list of models that we want to test\nmodels = [\n    RandomForestClassifier(),\n    LogisticRegression(max_iter=250),\n    KNeighborsClassifier(),\n]\n\n# an empty list to capture the performance of each model\nmodel_comps = []\n\n# walk through the models and populate our list\nfor model in models:\n    model_name = model.__class__.__name__\n    accuracies = cross_val_score(model, binary_train_X, binary_train_y, scoring='accuracy')\n    for count, accuracy in enumerate(accuracies):\n        model_comps.append((model_name, count, accuracy))","f382b2bc":"# a box plot will do well to show us overall performance and the variation in the models.\nresult_df = pd.DataFrame(model_comps, columns=['model_name', 'count', 'accuracy'])\nresult_df.pivot(index='count',columns='model_name',values='accuracy').boxplot(rot=45)","7913d695":"# model for the mulit classification\nmulti_model = RandomForestClassifier()\nmulti_model.fit(multi_train_X, multi_train_y)\nmulti_predictions = multi_model.predict(multi_val_X)\n\n# get the score\naccuracy_score(multi_predictions,multi_val_y)","99675723":"So let's dive into some feature building. It seems like that items above would make a good place to start: protocol_type, service and flag. There's enough variation between these that we should be able to get some base level of identification. We're also going to throw in some basic numeric data: duration, src_bytes, dst_bytes. All of these are going to be readily available from modern network equipment and should tell us a lot about what is happening on our network.","d1f574ff":"# Data extraction\nWe'll start by fetching our data set. There's a few options for data sets here, so we'll build a couple paths and use comments to pick and choose the ones we want.","e6ea895b":"Wow! Look at how many services are in the attack set! Whereas a huge amount of normal traffic is http, our attack traffic is all over the place. That is interesting as it means that attacks are searching for many different paths into systems--some well traveled and some not. \n\nIf we think about this from the eyes of a network adminstrator, the combination of protocol, flag and service seem like they should tell us a lot about the nature of our traffic. Coupling them with the duration of a connection and the amount of data in that connection seems like a good starting point for us.","f5cb9521":"# Model fitting\nBased on the nature of the data we saw above, decision trees are a good starting point for building out predictive models. In this case we'll use a random forest to build and combine multiple trees. We'll start by simply taking the defaults.","aaaa3701":"99% accuracy on our first try! Not bad, right? Let's see how it plays out. \n\nIt might be interesting to see how differnt models compare against a data set like this. That is easy enought to do with `cross_val_score`. ","a9e8bf91":"# Data transformations\nThe first transformations that we'll want to do are around the attack field. We'll start by adding a column that encodes 'normal' values as 0 and any other value as 1. We will use this as our classifier for a simple binary model that idenfities any attack. ","dd3d7026":"What we find is some inconsistency across the models. The random forest and K-nearest neighbors are tight groupings with solid performance. Our logistic regression didn't perform as well. That may be in  part because we didn't do sufficient preprocessing on our data to shape it into a form optimized for that model. That too is an exercise for another day.","2070baa2":"Next, we'll classify each of the attacks according to attack type for a more granular prediction model. \n* Denial of Service attacks:\n  * apache2\n  * back\n  * land\n  * neptune\n  * mailbomb\n  * pod\n  * processtable\n  * smurf\n  * teardrop\n  * udpstorm\n  * worm\n* Probe attacks:\n  * ipsweep\n  * mscan\n  * nmap\n  * portsweep\n  * saint\n  * satan\n* Privilege escalation attacks\n  * buffer_overflow\n  * loadmdoule\n  * perl\n  * ps\n  * rootkit\n  * sqlattack\n  * xterm\n* Remote access attacks\n  * ftp_write\n  * guess_passwd\n  * http_tunnel\n  * imap\n  * multihop\n  * named\n  * phf\n  * sendmail\n  * snmpgetattack\n  * snmpguess\n  * spy\n  * warezclient\n  * warezmaster\n  * xclock\n  * xsnoop","9b861b75":"It's worth drawing attention to a few things here. First, `pd.get_dummies` is a method that allows us to do a quick one hot encoding on our columns. This takes every value it finds in a single column and makes an individual column for each value, with a `0` or `1` indicating whether that column is 'hot'.\n\nOne thing we find is that note every value is in the test data. So that creates different shapes of our data frame. That's why we added some columns, filled them in and reorded them. We know they are all zeros because they aren't in the data.","a8fafd48":"The data set doesn't include column names, so let's add them.","64c2e546":"# Feature engineering","13bf5600":"And service?","35d4da7a":"# Exploring NSL-KDD dataset\n\nThe purpose of this notebook is a basic exploration of the NSL-KDD dataset. Here are the goals of this exploration:\n* Gain a basic understanding of the data set\n* Look at how the data set might be used to predict network anomalies or attacks\n* Walk through some fundemental concepts of building machine learning models\n\nThroughout we'll do some work by hand that could be done in more effective ways using delivered functionality within sci-kit. The intent here is to be more deliberate about the process of understanding what we're doing and why. We will look at how to approach some of these problems using the built-in toools in a later notebook.","1f1fb49e":"Now let's go ahead and set our classification targets. We'l do both training sets to start: binrary and multi classifications."}}