{"cell_type":{"f797903d":"code","0aa6f6de":"code","a2ecbefd":"code","f98f96a3":"code","17ef1012":"code","9dd9df78":"code","9f793499":"code","7284872d":"code","bce7ae49":"markdown","ef5dda0e":"markdown","e9142802":"markdown","faf4c18a":"markdown","ccfdfaf8":"markdown","fc674fab":"markdown","167a9d8a":"markdown","c2e48cbe":"markdown","178f426f":"markdown"},"source":{"f797903d":"from kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col\n\ndef agent(obs_dict, config_dict):\n    \"\"\"This agent always moves toward observation.food[0] but does not take advantage of board wrapping\"\"\"\n    observation = Observation(obs_dict)\n    configuration = Configuration(config_dict)\n    \n    print(\"Observation :\", observation)\n    print(\"Configuration : \", configuration)\n    \n    player_index = observation.index\n    player_goose = observation.geese[player_index]\n    player_head = player_goose[0]\n    player_row, player_column = row_col(player_head, configuration.columns)\n    food = observation.food[0]\n    food_row, food_column = row_col(food, configuration.columns)\n\n    if food_row > player_row:\n        return Action.SOUTH.name\n    if food_row < player_row:\n        return Action.NORTH.name\n    if food_column > player_column:\n        return Action.EAST.name\n    return Action.WEST.name","0aa6f6de":"from kaggle_environments import make\n\n# Creating the environment for simulating the game\nenv = make(\"hungry_geese\", debug=True)\n\n# Simulating a game between our agent and a predefined agent called \"random\"\nenv.run(\n    [\n        agent,\n        \"random\"\n    ],  \n)\n\nenv.render(mode=\"ipython\")","a2ecbefd":"%%writefile custom_greedy_agent.py\n\nimport random\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col, adjacent_positions\n\nactions = [Action.NORTH, Action.SOUTH, Action.EAST, Action.WEST]\ncached_agents = {}\n\ndef nearest_food(position, food, columns):\n    minvalue = columns**2\n    minfood = None\n    row, column = row_col(position, columns)\n    for food_object in food:\n        food_row, food_column = row_col(food_object, columns)\n        distance = abs(row - food_row) + abs(column - food_column)\n        if distance < minvalue:\n            minfood = food_object\n            minvalue = distance\n    return minfood\n\ndef is_colliding(action, observation, configuration):\n    player_index = observation.index\n    player_goose = observation.geese[player_index]\n    \n    player_head = player_goose[0]\n    player_head_row, player_head_column = row_col(player_head, configuration.columns)\n    \n    player_body = player_goose[1:]\n    other_geese = observation.geese[:player_index] + observation.geese[player_index+1:]\n    other_heads = [other_goose[0] for other_goose in other_geese if len(other_goose)>0]\n    other_heads_adj = [adjacent_positions(other_head, configuration.columns, configuration.rows) for other_head in other_heads]\n    \n    # Flattening the other geese list and other heads adj list\n    other_geese = [cell for goose in other_geese for cell in goose]\n    other_heads_adj = [pos for head_adj in other_heads_adj for pos in head_adj]\n    \n    # Obstacles to avoid\n    obstacles = player_body + other_geese + other_heads_adj\n    coordinates_to_avoid = [tuple(row_col(obstacle, configuration.columns)) for obstacle in obstacles]\n    \n    if action == Action.SOUTH:\n        return any([player_head_row+1 == obstacle_row and player_head_column == obstacle_column for obstacle_row, obstacle_column in coordinates_to_avoid])\n    if action == Action.NORTH:\n        return any([player_head_row-1 == obstacle_row and player_head_column == obstacle_column for obstacle_row, obstacle_column in coordinates_to_avoid])\n    if action == Action.EAST:\n        return any([player_head_column+1 == obstacle_column and player_head_row == obstacle_row for obstacle_row, obstacle_column in coordinates_to_avoid])\n    if action == Action.WEST:\n        return any([player_head_column-1 == obstacle_column and player_head_row == obstacle_row for obstacle_row, obstacle_column in coordinates_to_avoid])\n        \nclass CustomGreedyAgent:\n    \"\"\"Since we need to maintain the state of prev_action across multiple calls for the same agent, we have to use a class for agent\"\"\"\n    def __init__(self, config_dict):\n        self.curr_action = None\n        self.prev_action = None\n        \n        self.configuration = Configuration(config_dict)\n    \n    def __call__(self, obs_dict):\n        \"\"\"This function gets called when you call an object of this class as a function\"\"\"\n        global actions\n\n        observation = Observation(obs_dict)\n\n        player_index = observation.index\n        player_goose = observation.geese[player_index]\n\n        player_head = player_goose[0]\n        player_head_row, player_head_column = row_col(player_head, self.configuration.columns)\n\n        food = nearest_food(player_head, observation.food, self.configuration.columns)\n        food_row, food_column = row_col(food, self.configuration.columns)\n\n        self.prev_action = self.curr_action\n        self.curr_action = None\n\n        if food_row > player_head_row:\n            if not is_colliding(Action.SOUTH, observation, self.configuration):\n                self.curr_action = Action.SOUTH\n            elif not is_colliding(Action.WEST, observation, self.configuration):\n                self.curr_action = Action.WEST\n            elif not is_colliding(Action.EAST, observation, self.configuration):\n                self.curr_action = Action.EAST\n            elif not is_colliding(Action.NORTH, observation, self.configuration):\n                self.curr_action = Action.NORTH\n        \n        elif food_row < player_head_row:\n            if not is_colliding(Action.NORTH, observation, self.configuration):\n                self.curr_action = Action.NORTH\n            elif not is_colliding(Action.EAST, observation, self.configuration):\n                self.curr_action = Action.EAST\n            elif not is_colliding(Action.WEST, observation, self.configuration):\n                self.curr_action = Action.WEST\n            elif not is_colliding(Action.SOUTH, observation, self.configuration):\n                self.curr_action = Action.SOUTH\n        \n        elif food_column > player_head_column:\n            if not is_colliding(Action.EAST, observation, self.configuration):\n                self.curr_action = Action.EAST\n            elif not is_colliding(Action.NORTH, observation, self.configuration):\n                self.curr_action = Action.NORTH\n            elif not is_colliding(Action.SOUTH, observation, self.configuration):\n                self.curr_action = Action.SOUTH\n            elif not is_colliding(Action.WEST, observation, self.configuration):\n                self.curr_action = Action.WEST\n        \n        else:\n            if not is_colliding(Action.WEST, observation, self.configuration):\n                self.curr_action = Action.WEST\n            elif not is_colliding(Action.SOUTH, observation, self.configuration):\n                self.curr_action = Action.SOUTH\n            elif not is_colliding(Action.NORTH, observation, self.configuration):\n                self.curr_action = Action.NORTH\n            elif not is_colliding(Action.EAST, observation, self.configuration):\n                self.curr_action = Action.EAST\n        \n        # Even after all these conditions, if there is no decision of curr_action, just pick one randomly.\n        if self.curr_action == None:\n            self.curr_action = random.choice(actions)\n        \n        # if new action is opposite direction to previous action, pick another random action instead.\n        if self.prev_action:\n            while self.curr_action == self.prev_action.opposite():\n                self.curr_action = random.choice(actions)\n\n        print(\"Player index : \", player_index)\n        print(\"Prev action : \", self.prev_action)\n        print(\"Curr action : \", self.curr_action)\n        return self.curr_action.name\n\ndef agent(obs_dict, config_dict):\n    \n    global cached_agents\n    \n    observation = Observation(obs_dict)\n    player_index = observation.index\n    \n    if player_index in cached_agents:\n        return cached_agents[player_index](obs_dict)\n    else:\n        cached_agents[player_index] = CustomGreedyAgent(config_dict)\n        return cached_agents[player_index](obs_dict)","f98f96a3":"from kaggle_environments import make\nimport custom_greedy_agent\nenv = make(\"hungry_geese\", debug=False)\n\nenv.run(\n    [\n        custom_greedy_agent.agent,\n        custom_greedy_agent.agent,\n        custom_greedy_agent.agent,\n        custom_greedy_agent.agent\n    ],  \n)\n\nenv.render(mode=\"ipython\")","17ef1012":"%%writefile heuristic_agent.py\n\nimport random\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col, adjacent_positions, translate\n\nactions = [Action.NORTH, Action.SOUTH, Action.EAST, Action.WEST]\ncached_agents = {}\n\ndef nearest_food(position, food, columns):\n    minvalue = columns**2\n    minfood = None\n    row, column = row_col(position, columns)\n    for food_object in food:\n        food_row, food_column = row_col(food_object, columns)\n        distance = abs(row - food_row) + abs(column - food_column)\n        if distance < minvalue:\n            minfood = food_object\n            minvalue = distance\n    return minfood\n\ndef one_step_heuristic(action, observation, configuration):\n    # One step collide\n    if is_colliding(action, observation, configuration):\n        return -100\n    \n    player_index = observation.index\n    player_goose = observation.geese[player_index]\n    \n    player_head = player_goose[0]\n    row, column = row_col(player_head, configuration.columns)\n    \n    # One step food\n    if translate(player_head, action, configuration.columns, configuration.rows) in observation.food:\n        return 10\n    \n    closest_food = nearest_food(player_head, observation.food, configuration.columns)\n    food_row, food_column = row_col(closest_food, configuration.columns)\n    \n    distance = abs(row - food_row) + abs(column - food_column)\n    \n    after_translate = translate(player_head, action, configuration.columns, configuration.rows)\n    new_row, new_column = row_col(after_translate, configuration.columns)\n    \n    new_distance = abs(new_row - food_row) + abs(new_column - food_column)\n    \n    if new_distance < distance:\n        return 1\n    \n    return 0\n\ndef is_colliding(action, observation, configuration):\n    player_index = observation.index\n    player_goose = observation.geese[player_index]\n    \n    player_head = player_goose[0]\n    player_head_row, player_head_column = row_col(player_head, configuration.columns)\n    \n    player_body = player_goose[1:]\n    other_geese = observation.geese[:player_index] + observation.geese[player_index+1:]\n    other_heads = [other_goose[0] for other_goose in other_geese if len(other_goose)>0]\n    other_heads_adj = [adjacent_positions(other_head, configuration.columns, configuration.rows) for other_head in other_heads]\n    \n    # Flattening the other geese list and other heads adj list\n    other_geese = [cell for goose in other_geese for cell in goose]\n    other_heads_adj = [pos for head_adj in other_heads_adj for pos in head_adj]\n    \n    # Obstacles to avoid\n    obstacles = player_body + other_geese + other_heads_adj\n    coordinates_to_avoid = [tuple(row_col(obstacle, configuration.columns)) for obstacle in obstacles]\n    \n    if action == Action.SOUTH:\n        return any([player_head_row+1 == obstacle_row and player_head_column == obstacle_column for obstacle_row, obstacle_column in coordinates_to_avoid])\n    if action == Action.NORTH:\n        return any([player_head_row-1 == obstacle_row and player_head_column == obstacle_column for obstacle_row, obstacle_column in coordinates_to_avoid])\n    if action == Action.EAST:\n        return any([player_head_column+1 == obstacle_column and player_head_row == obstacle_row for obstacle_row, obstacle_column in coordinates_to_avoid])\n    if action == Action.WEST:\n        return any([player_head_column-1 == obstacle_column and player_head_row == obstacle_row for obstacle_row, obstacle_column in coordinates_to_avoid])\n        \nclass HeuristicAgent:\n    \"\"\"Since we need to maintain the state of prev_action across multiple calls for the same agent, we have to use a class for agent\"\"\"\n    def __init__(self, config_dict):\n        self.curr_action = None\n        self.prev_action = None\n        \n        self.configuration = Configuration(config_dict)\n    \n    def __call__(self, obs_dict):\n        \"\"\"This function gets called when you call an object of this class as a function\"\"\"\n        global actions\n\n        observation = Observation(obs_dict)\n        \n        self.prev_action = self.curr_action\n        \n        # We can remove the possibility of opposite actions even without lookahead\n        if self.prev_action:\n            valid_actions = [action for action in actions if not action == self.prev_action.opposite()]\n        else:\n            valid_actions = actions.copy()\n        \n        # Use the one_step_heuristic to assign a score to each possible valid action\n        scores = dict(zip(valid_actions, [one_step_heuristic(action, observation, self.configuration) for action in valid_actions]))\n\n        # Get a list of columns (moves) that maximize the heuristic\n        best_actions = [key for key in scores.keys() if scores[key] == max(scores.values())]\n\n        # Select at random from the best actions\n        self.curr_action = random.choice(best_actions)\n        \n        print(\"Player index : \", observation.index)\n        print(\"Prev action : \", self.prev_action)\n        print(\"Curr action : \", self.curr_action)\n        return self.curr_action.name\n\ndef agent(obs_dict, config_dict):\n    \n    global cached_agents\n    \n    observation = Observation(obs_dict)\n    player_index = observation.index\n    \n    if player_index in cached_agents:\n        return cached_agents[player_index](obs_dict)\n    else:\n        cached_agents[player_index] = HeuristicAgent(config_dict)\n        return cached_agents[player_index](obs_dict)","9dd9df78":"from kaggle_environments import make\nimport heuristic_agent\nenv = make(\"hungry_geese\", debug=False)\n\nenv.run(\n    [\n        heuristic_agent.agent,\n        heuristic_agent.agent,\n        heuristic_agent.agent,\n        heuristic_agent.agent\n    ],  \n)\n\nenv.render(mode=\"ipython\")","9f793499":"%%writefile n_step_agent.py\n\nimport random\nfrom kaggle_environments.envs.hungry_geese.hungry_geese import Observation, Configuration, Action, row_col, adjacent_positions, translate\n\nactions = [Action.NORTH, Action.SOUTH, Action.EAST, Action.WEST]\ncached_agents = {}\n\nn_steps = 3\n\ndef reward(action, player_head, depth, observation, configuration):\n    global n_steps, actions\n    \n    # One step collide\n    if is_colliding(action, observation, configuration):\n        return 0\n    \n    # Last step\n    if depth == n_steps-1:\n        if player_head in observation.food:\n            return 0\n        else:\n            return 1\n    \n    # Take action step\n    score = 0\n    prev_action = action\n    if prev_action:\n        valid_actions = [action for action in actions if not action == prev_action.opposite()]\n    else:\n        valid_actions = actions.copy()\n    \n    # For intermediate step, if player_head is in food increase length\n    if player_head in observation.food:\n        score += 1\n    \n    # Returning highest reward possible from that point  \n    return score + max([reward(action, translate(player_head, action, configuration.columns, configuration.rows), depth+1, observation, configuration) for action in valid_actions])\n\ndef is_colliding(action, observation, configuration):\n    player_index = observation.index\n    player_goose = observation.geese[player_index]\n    \n    player_head = player_goose[0]\n    player_head_row, player_head_column = row_col(player_head, configuration.columns)\n    \n    player_body = player_goose[1:]\n    other_geese = observation.geese[:player_index] + observation.geese[player_index+1:]\n    other_heads = [other_goose[0] for other_goose in other_geese if len(other_goose)>0]\n    other_heads_adj = [adjacent_positions(other_head, configuration.columns, configuration.rows) for other_head in other_heads]\n    \n    # Flattening the other geese list and other heads adj list\n    other_geese = [cell for goose in other_geese for cell in goose]\n    other_heads_adj = [pos for head_adj in other_heads_adj for pos in head_adj]\n    \n    # Obstacles to avoid\n    obstacles = player_body + other_geese + other_heads_adj\n    coordinates_to_avoid = [tuple(row_col(obstacle, configuration.columns)) for obstacle in obstacles]\n    \n    if action == Action.SOUTH:\n        return any([player_head_row+1 == obstacle_row and player_head_column == obstacle_column for obstacle_row, obstacle_column in coordinates_to_avoid])\n    if action == Action.NORTH:\n        return any([player_head_row-1 == obstacle_row and player_head_column == obstacle_column for obstacle_row, obstacle_column in coordinates_to_avoid])\n    if action == Action.EAST:\n        return any([player_head_column+1 == obstacle_column and player_head_row == obstacle_row for obstacle_row, obstacle_column in coordinates_to_avoid])\n    if action == Action.WEST:\n        return any([player_head_column-1 == obstacle_column and player_head_row == obstacle_row for obstacle_row, obstacle_column in coordinates_to_avoid])\n        \nclass NStepAgent:\n    \"\"\"Since we need to maintain the state of prev_action across multiple calls for the same agent, we have to use a class for agent\"\"\"\n    def __init__(self, config_dict):\n        self.curr_action = None\n        self.prev_action = None\n        \n        self.configuration = Configuration(config_dict)\n    \n    def __call__(self, obs_dict):\n        \"\"\"This function gets called when you call an object of this class as a function\"\"\"\n        global actions, n_steps\n\n        observation = Observation(obs_dict)\n        \n        self.prev_action = self.curr_action\n        \n        # We can remove the possibility of opposite actions even without lookahead\n        if self.prev_action:\n            valid_actions = [action for action in actions if not action == self.prev_action.opposite()]\n        else:\n            valid_actions = actions.copy()\n        \n        \n        # Use the one_step_heuristic to assign a score to each possible valid action\n        player_index = observation.index\n        player_goose = observation.geese[player_index]\n        player_head = player_goose[0]\n        scores = dict(zip(valid_actions, [reward(action, player_head, 0, observation, self.configuration) for action in valid_actions]))\n\n        # Get a list of columns (moves) that maximize the heuristic\n        best_actions = [key for key in scores.keys() if scores[key] == max(scores.values())]\n\n        # Select at random from the best actions\n        self.curr_action = random.choice(best_actions)\n        \n        print(\"Player index : \", observation.index)\n        print(\"Prev action : \", self.prev_action)\n        print(\"Curr action : \", self.curr_action)\n        return self.curr_action.name\n\ndef agent(obs_dict, config_dict):\n    \n    global cached_agents\n    \n    observation = Observation(obs_dict)\n    player_index = observation.index\n    \n    if player_index in cached_agents:\n        return cached_agents[player_index](obs_dict)\n    else:\n        cached_agents[player_index] = NStepAgent(config_dict)\n        return cached_agents[player_index](obs_dict)","7284872d":"from kaggle_environments import make\nimport n_step_agent\nenv = make(\"hungry_geese\", debug=True)\n\nenv.run(\n    [\n        n_step_agent.agent,\n        n_step_agent.agent,\n        n_step_agent.agent,\n        n_step_agent.agent\n    ],  \n)\n\nenv.render(mode=\"ipython\")","bce7ae49":"## Playing the agent in the environment","ef5dda0e":"# Agent 2 - Game Tree Agent (One-Step Look Ahead)\n1. Check the reward for taking one step in all valid directions\n2. Take the direction which has maximum reward","e9142802":"# Agent 3 : N-step Lookahead Agent\n1. This agent looks forward 3 steps and chooses best action\n2. Also tuned the heuristic rewards according to the contests reward policy","faf4c18a":"## Playing the agent in the environment","ccfdfaf8":"# Well, well, well.. what do we have here ? \n## Intermediate Result : \n1. So, One step lookahead agent performs barely as good as, if not worse than the greedy agent\n2. Both greedy and one step lookahead agents are failing due to being forced into a corner.\n\n## Solution : \n1. Try n_steps lookahead for the corner issue.\n2. Finally, deep reinforcement neural networks for complex behavior.\n\n## Other ideas : \n1. Maybe there's a fault with a heuristic values.","fc674fab":"# Agent 1 : Making an agent that follows the same approach but with some constraints.\n1. Avoids other geese, any positions adjacent to their heads(suspecting they will move and collide) and its own body\n2. Avoids consecutive opposite actions\n3. Makes use of board wrapping\n4. Move to closest food instead of food[0]","167a9d8a":"# How does the environment work\n\nI'm trying to see the values and types of the variables the agent works with. To that end, I'm going to use the random agent provided by kaggle and print the observation and configuration values. Nice and easy!\n\nOne requirement : The agent must definitely and absolutely return a direction.","c2e48cbe":"## Playing the agent in the environment","178f426f":"# Running the agent (Playing a game)\nBelow is the syntax to use one (or multiple) agent(s) and simulate one (or multiple) game(s).\n\n**Never submit an agent before witnessing how it performs! Does it even finish a game?**"}}