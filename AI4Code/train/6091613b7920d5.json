{"cell_type":{"dfd72564":"code","feaa470f":"code","ed5b8718":"code","6b67950b":"code","9395b691":"code","a3186287":"code","40c0cb72":"code","57b1036d":"code","b87f29fd":"code","1a75f761":"code","cbd49362":"code","0e6322f8":"code","d3a4fd75":"code","2b7265aa":"code","a13f501a":"code","de030dda":"code","954bf337":"code","30aff44d":"code","5c81adec":"code","ff5f147f":"code","38bbb563":"code","99e80fad":"code","ea460c20":"code","391141bc":"code","a776d9d1":"code","76e81509":"code","6edf8209":"code","6df8dfa9":"code","d395eaf1":"code","7bbc93d6":"code","536fde81":"markdown","18989d79":"markdown","7179c6de":"markdown","503acf66":"markdown","ff2a7622":"markdown","7a7fb341":"markdown","2cfbde19":"markdown","56dab601":"markdown","06027059":"markdown","4d349a5c":"markdown"},"source":{"dfd72564":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import GroupKFold\nimport matplotlib.pyplot as plt\nfrom tqdm.notebook import tqdm\nimport tensorflow_hub as hub\nimport tensorflow as tf\nimport bert_tokenization as tokenization\nimport tensorflow.keras.backend as K\nimport gc\nimport os\nfrom scipy.stats import spearmanr\nfrom math import floor, ceil\nimport seaborn as sns\n\nnp.set_printoptions(suppress=True)","feaa470f":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","ed5b8718":"PATH = '\/kaggle\/input\/datanew\/'\nBERT_PATH = '\/kaggle\/input\/bertbasefromtfhub\/bert_en_uncased_L-12_H-768_A-12'\ntokenizer = tokenization.FullTokenizer(BERT_PATH+'\/assets\/vocab.txt', True)\nMAX_SEQUENCE_LENGTH = 512\n\nQUESTION_LABES_VECTOR_SIZE = 21\nANSWER_LABES_VECTOR_SIZE = 4\nBOTH_LABES_VECTOR_SIZE = 5\n\ndf_train = pd.read_csv(PATH+'train.csv')\ndf_test = pd.read_csv(PATH+'test.csv')\ndf_sub = pd.read_csv(PATH+'sample_submission.csv')\n\n# ##remove later!!!\n# df_train = df_train[:10]\n# df_test = df_test[:10]\n# df_sub = df_sub[:10]\n\nprint('train shape =', df_train.shape)\nprint('test shape =', df_test.shape)","6b67950b":"count = 0\nfor var in df_train.head(1):\n    for elem in df_train[var]:\n        if elem == 'NaN' :\n            count +=1    \n    print(var , count\/df_train.shape[0])\n    count = 0\n","9395b691":"%matplotlib inline\nsns.countplot(x='category', data=df_train)","a3186287":"df_train['host']=df_train['host'].apply(lambda x:(x.replace(x ,x.split('.')[0])))\ndf_test['host']=df_test['host'].apply(lambda x:(x.replace(x ,x.split('.')[0])))","40c0cb72":"df_train.columns = df_train.columns.str.replace('host', 'sub_category')\ndf_test.columns = df_test.columns.str.replace('host', 'sub_category')","57b1036d":"#we trying to figure out if category is redundent due to the sub category\narr_sub_category = {}\nfor sub in df_train['sub_category'].unique():\n    data = df_train[df_train['sub_category'] == sub]\n    arr = df_train[df_train['sub_category'] == sub]['category'].unique()\n    if len(arr) > 1:\n        arr_sub_category[sub] = arr\n        print(sub, arr)\n        #correlation between two features","b87f29fd":"# split each sub_category found to be multiplied to more than one category \n#(in order to drop the category feature without loosing any important info)\nfor key in arr_sub_category.keys():\n    for val in arr_sub_category[key]:\n        df_train[(df_train['sub_category'] == key) & (df_train['category'] == val)] = df_train[(df_train['sub_category'] == key) & (df_train['category'] == val)].apply(lambda x :x.replace(key, key+'_'+val))\n","1a75f761":"# checking\nprint(df_train['sub_category'].unique())","cbd49362":"%matplotlib inline\nsns.countplot(x='sub_category', data=df_train)","0e6322f8":" def corrank(X):## to think about relevant correlations (sub_c and lable?, is text coor is relevant?)\n        import itertools\n        dff = pd.DataFrame([[(i,j),X.corr().loc[i,j]] for i,j in list(itertools.combinations(X.corr(method = 'spearman'), 2))],columns=['pairs','corr'])    \n        print(dff.sort_values(by='corr',ascending=False))","d3a4fd75":"corrank(df_train)","2b7265aa":"# data_a = np.array([['Can an affidavit be used in Beit Din?', 10], ['How can I write HTML and send as an email?', 15], ['How do I remove a Facebook app request?', 14], ['How do you grapple in Dead Rising 3?', 1], ['How do you make a binary image in Photoshop?', 1]]) \n# df_a = pd.DataFrame(data_a)\n# df_a.columns = [\"question\", \"num\"]\n# print(df_a)\n\n# from adam_qas import adam_script as adam\n# dfOut = pd.DataFrame(np.array([[\"\",\"\",\"\"]]))\n# dfOut.columns = [\"q_class\",\"q_keywords\",\"query\"]\n# dfOut = dfOut[:-1]\n# new_features = adam.activate(df_a['question'], dfOut)\n# print(new_features.head(2))\n# df_a.join(new_features)\n# print(df_a.head(5))","a13f501a":"# import adam_qas as adam\n# dfOut = pd.DataFrame(np.array([[\"\",\"\",\"\"]]))\n# dfOut.columns = [\"q_class\",\"q_keywords\",\"query\"]\n# dfOut = dfOut[:-1]\n\n# new_features = adam.activate(df_train['question_title'], dfOut)\n# print(new_features)\n\n# df_new_train=pd.concat([df_train,new_features], axis=1, sort=False)\n# df_new_test=pd.concat([df_test,new_features], axis=1, sort=False)","de030dda":"# df_new_train.to_csv(PATH+'df_new_train.csv', index=False)\n# df_new_test.to_csv(PATH+'df_new_test.csv', index=False)","954bf337":"# df_new_train.head(10)","30aff44d":"# df_new_test.head(10)","5c81adec":"df_test = pd.read_csv(PATH+'df_new_test.csv', sep=\",\", encoding=\"ISO-8859-1\")\ndf_train = pd.read_csv(PATH+'df_new_train.csv', sep=\",\", encoding=\"ISO-8859-1\")\noutput_categories_question = list(df_train.columns[11:32])\noutput_categories_answer = list(df_train.columns[37:41])\noutput_categories_both = list(df_train.columns[32:37])\ninput_categories = list(df_train.columns[[1,2,5,10,41,42]]) # we added the sub_categoty!\nprint('\\033[1m' + '\\033[91m' + '\\033[4m' + 'output categories question:\\n\\t' +'\\033[0m', output_categories_question)\nprint('\\033[1m' + '\\033[91m' + '\\033[4m' + 'output categories answer:\\n\\t' +'\\033[0m', output_categories_answer)\nprint('\\033[1m' + '\\033[91m' + '\\033[4m' + 'output categories both:\\n\\t' +'\\033[0m', output_categories_both)\nprint('\\033[1m' + '\\033[91m' + '\\033[4m' + 'input categories:\\n\\t' +'\\033[0m', input_categories)","ff5f147f":"for col in input_categories:\n    max_len = 0\n    mean = 0\n    min_len = 512\n    for elem in df_train[col].unique():\n        if len(elem)>max_len:\n            max_len = len(elem)\n        if mean==0:\n            mean = (mean+len(elem))\/2\n        mean = (mean+len(elem))\/2\n        if len(elem)<min_len:\n            min_len = len(elem)\n    print('\\033[1m' + '\\033[91m' + '\\033[4m' + col +\":\" +'\\033[0m')\n    print(\"min: \" + str(min_len))\n    print(\"max: \" + str(max_len))\n    print(\"mean: \" + str(floor(mean)))","38bbb563":"df_test.head(10)","99e80fad":"def _get_masks(tokens, max_seq_length):\n    \"\"\"Mask for padding\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    return [1]*len(tokens) + [0] * (max_seq_length - len(tokens))\n\ndef _get_segments(tokens, max_seq_length):\n    \"\"\"Segments: 0 for the first sequence, 1 for the second\"\"\"\n    if len(tokens)>max_seq_length:\n        raise IndexError(\"Token length more than max seq length!\")\n    segments = []\n    first_sep = True\n    current_segment_id = 0\n    for token in tokens:\n        segments.append(current_segment_id)\n        if token == \"[SEP]\":\n            if first_sep:\n                first_sep = False \n            else:\n                current_segment_id = 1\n    return segments + [0] * (max_seq_length - len(tokens))\n\ndef _get_ids(tokens, tokenizer, max_seq_length):\n    \"\"\"Token ids from Tokenizer vocab\"\"\"\n    token_ids = tokenizer.convert_tokens_to_ids(tokens)\n    input_ids = token_ids + [0] * (max_seq_length-len(token_ids))\n    return input_ids\n\ndef _trim_input(title, answer, sub_category, q_class, q_keywords, max_sequence_length, \n                t_max_len=150, a_max_len=196, s_c_max_len=25, q_c_max_len=10, q_k_max_len=125):\n\n    t   = tokenizer.tokenize(title)\n    a   = tokenizer.tokenize(answer)\n    s_c = tokenizer.tokenize(sub_category)\n    q_c = tokenizer.tokenize(q_class)\n    q_k = tokenizer.tokenize(q_keywords)\n    \n    t_len   = len(t)\n    a_len   = len(a)\n    s_c_len = len(s_c)\n    q_c_len = len(q_c)\n    q_k_len = len(q_k)\n\n  \n    if s_c_max_len > s_c_len:\n        s_c_new_len = s_c_len\n    else:\n        s_c_new_len = s_c_max_len\n\n    if q_c_max_len > q_c_len:\n        q_c_new_len = q_c_len\n    else:\n        q_c_new_len = q_c_max_len\n\n    if q_k_max_len > q_k_len:\n        q_k_new_len = q_k_len\n    else:\n        q_k_new_len = q_k_max_len\n\n    if t_max_len > t_len:\n        t_new_len = t_len\n    else:\n        t_new_len = t_max_len\n\n    a_new_len = max_sequence_length - (t_new_len+s_c_new_len+q_c_new_len+q_k_new_len+6)\n            \n    if t_new_len+s_c_new_len+a_new_len+q_c_new_len+q_k_new_len+6 != max_sequence_length:\n        raise ValueError(\"New sequence length should be %d, but is %d\" \n                         % (max_sequence_length, (t_new_len+s_c_new_len+a_new_len+q_c_new_len+q_k_new_len+4)))\n    \n    a   = a[:a_new_len]\n    t   = t[:t_new_len]\n    s_c = s_c[:s_c_new_len]\n    q_c = q_c[:q_c_new_len]\n    q_k = q_k[:q_k_new_len]\n    \n    return t, a, s_c, q_c, q_k\n\ndef _trim_input_answer(answer, q_class, q_keywords, max_sequence_length, \n                a_max_len=371, q_c_max_len=10, q_k_max_len=125):\n\n    a   = tokenizer.tokenize(answer)\n    q_c = tokenizer.tokenize(q_class)\n    q_k = tokenizer.tokenize(q_keywords)\n    \n    a_len   = len(a)\n    q_c_len = len(q_c)\n    q_k_len = len(q_k)\n\n    if q_c_max_len > q_c_len:\n        q_c_new_len = q_c_len\n    else:\n        q_c_new_len = q_c_max_len\n\n    if q_k_max_len > q_k_len:\n        q_k_new_len = q_k_len\n    else:\n        q_k_new_len = q_k_max_len\n\n    a_new_len = max_sequence_length - (q_c_new_len+q_k_new_len+6)\n            \n    if a_new_len+q_c_new_len+q_k_new_len+6 != max_sequence_length:\n        raise ValueError(\"New sequence length should be %d, but is %d\" \n                         % (max_sequence_length, (a_new_len+q_c_new_len+q_k_new_len+4)))\n    \n    a   = a[:a_new_len]\n    q_c = q_c[:q_c_new_len]\n    q_k = q_k[:q_k_new_len]\n    \n    return a, q_c, q_k\n\ndef _trim_input_question(title, question, sub_category, q_class, q_keywords, max_sequence_length, \n                t_max_len=150, q_max_len=196, s_c_max_len=25, q_c_max_len=10, q_k_max_len=125):\n\n    t   = tokenizer.tokenize(title)\n    q   = tokenizer.tokenize(question)\n    s_c = tokenizer.tokenize(sub_category)\n    q_c = tokenizer.tokenize(q_class)\n    q_k = tokenizer.tokenize(q_keywords)\n    \n    t_len   = len(t)\n    q_len   = len(q)\n    s_c_len = len(s_c)\n    q_c_len = len(q_c)\n    q_k_len = len(q_k)\n\n  \n    if s_c_max_len > s_c_len:\n        s_c_new_len = s_c_len\n    else:\n        s_c_new_len = s_c_max_len\n\n    if q_c_max_len > q_c_len:\n        q_c_new_len = q_c_len\n    else:\n        q_c_new_len = q_c_max_len\n\n    if q_k_max_len > q_k_len:\n        q_k_new_len = q_k_len\n    else:\n        q_k_new_len = q_k_max_len\n\n    if t_max_len > t_len:\n        t_new_len = t_len\n    else:\n        t_new_len = t_max_len\n\n    q_new_len = max_sequence_length - (t_new_len+s_c_new_len+q_c_new_len+q_k_new_len+6)\n            \n    if t_new_len+s_c_new_len+q_new_len+q_c_new_len+q_k_new_len+6 != max_sequence_length:\n        raise ValueError(\"New sequence length should be %d, but is %d\" \n                         % (max_sequence_length, (t_new_len+s_c_new_len+q_new_len+q_c_new_len+q_k_new_len+4)))\n    \n    q   = q[:q_new_len]\n    t   = t[:t_new_len]\n    s_c = s_c[:s_c_new_len]\n    q_c = q_c[:q_c_new_len]\n    q_k = q_k[:q_k_new_len]\n    \n    return t, q, s_c, q_c, q_k\n\ndef _convert_to_bert_inputs(title, question, answer, sub_category, q_class, q_keywords, tokenizer, max_sequence_length, input_type):\n    \"\"\"Converts tokenized input to ids, masks and segments for BERT\"\"\"\n    \n    if(input_type == 0): #question input\n        stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + question + [\"[SEP]\"] + sub_category + [\"[SEP]\"] + q_class + [\"[SEP]\"] + q_keywords + [\"[SEP]\"]\n    if(input_type == 1): #answer inputs\n        stoken = [\"[CLS]\"] + answer + [\"[SEP]\"] + sub_category + [\"[SEP]\"] + q_class + [\"[SEP]\"]\n    if(input_type == 2): #both inputs\n        stoken = [\"[CLS]\"] + title + [\"[SEP]\"] + answer + [\"[SEP]\"] + sub_category + [\"[SEP]\"] + q_class + [\"[SEP]\"] + q_keywords + [\"[SEP]\"]\n    \n    \n    input_ids = _get_ids(stoken, tokenizer, max_sequence_length)\n    input_masks = _get_masks(stoken, max_sequence_length)\n    input_segments = _get_segments(stoken, max_sequence_length)\n\n    return [input_ids, input_masks, input_segments]\n\ndef compute_input_arays(df, columns, tokenizer, max_sequence_length, input_type):\n    input_ids, input_masks, input_segments = [], [], []\n\n    if(input_type == 0): #question input\n        for _, instance in tqdm(df[columns].iterrows()):\n            t, q, a, s_c, q_c, q_k = instance.question_title, instance.question_body, instance.answer, instance.sub_category, instance.q_class, instance.q_keywords\n            t, q, s_c, q_c, q_k = _trim_input_question(t, q, s_c, q_c, q_k, max_sequence_length)\n\n            ids, masks, segments = _convert_to_bert_inputs(t, q, a, s_c, q_c, q_k, tokenizer, max_sequence_length, 0)\n\n            input_ids.append(ids)\n            input_masks.append(masks)\n            input_segments.append(segments)\n    if(input_type == 1): #answer inputs\n        for _, instance in tqdm(df[columns].iterrows()):\n            t, q, a, s_c, q_c, q_k = instance.question_title, instance.question_body, instance.answer, instance.sub_category, instance.q_class, instance.q_keywords\n            a, s_c, q_c, = _trim_input_answer(a, s_c, q_c, max_sequence_length)\n\n            ids, masks, segments = _convert_to_bert_inputs(t, q, a, s_c, q_c, q_k, tokenizer, max_sequence_length, 1)\n\n            input_ids.append(ids)\n            input_masks.append(masks)\n            input_segments.append(segments)\n    if(input_type == 2): #both inputs\n        for _, instance in tqdm(df[columns].iterrows()):\n            t, q, a, s_c, q_c, q_k = instance.question_title, instance.question_body, instance.answer, instance.sub_category, instance.q_class, instance.q_keywords\n            t, a, s_c, q_c, q_k = _trim_input(t, a, s_c, q_c, q_k, max_sequence_length)\n\n            ids, masks, segments = _convert_to_bert_inputs(t, q, a, s_c, q_c, q_k, tokenizer, max_sequence_length, 2)\n\n            input_ids.append(ids)\n            input_masks.append(masks)\n            input_segments.append(segments)\n\n    return [np.asarray(input_ids, dtype=np.int32), \n            np.asarray(input_masks, dtype=np.int32), \n            np.asarray(input_segments, dtype=np.int32)]\n\n\ndef compute_output_arrays(df, columns):\n    return np.asarray(df[columns])\n","ea460c20":"def compute_spearmanr(trues, preds):\n    rhos = []\n    for col_trues, col_pred in zip(trues.T, preds.T):\n        rhos.append(\n            spearmanr(col_trues, col_pred + np.random.normal(0, 1e-7, col_pred.shape[0])).correlation)\n    return np.mean(rhos)\n\n\nclass CustomCallback(tf.keras.callbacks.Callback):\n    \n    def __init__(self, valid_data, test_data, batch_size=16, fold=None):\n\n        self.valid_inputs = valid_data[0]\n        self.valid_outputs = valid_data[1]\n        self.test_inputs = test_data\n        \n        self.batch_size = batch_size\n        self.fold = fold\n        \n    def on_train_begin(self, logs={}):\n        self.valid_predictions = []\n        self.test_predictions = []\n        \n    def on_epoch_end(self, epoch, logs={}):\n        self.valid_predictions.append(\n            self.model.predict(self.valid_inputs, batch_size=self.batch_size))\n        \n        rho_val = compute_spearmanr(\n            self.valid_outputs, np.average(self.valid_predictions, axis=0))\n        \n        print(\"\\nvalidation rho: %.4f\" % rho_val)\n        \n        if self.fold is not None:\n            self.model.save_weights(f'bert-base-{fold}-{epoch}.h5py')\n        \n        self.test_predictions.append(\n            self.model.predict(self.test_inputs, batch_size=self.batch_size)\n        )\n\ndef bert_model(size):\n    \n    input_word_ids = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_word_ids')\n    input_masks = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_masks')\n    input_segments = tf.keras.layers.Input(\n        (MAX_SEQUENCE_LENGTH,), dtype=tf.int32, name='input_segments')\n    \n    bert_layer = hub.KerasLayer(BERT_PATH, trainable=True)\n    \n    _, sequence_output = bert_layer([input_word_ids, input_masks, input_segments])\n    \n    x = tf.keras.layers.GlobalAveragePooling1D()(sequence_output)\n    x = tf.keras.layers.Dropout(0.2)(x)\n    out = tf.keras.layers.Dense(size, activation=\"sigmoid\", name=\"dense_output\")(x)\n\n    model = tf.keras.models.Model(\n        inputs=[input_word_ids, input_masks, input_segments], outputs=out)\n    \n    return model    \n        \ndef train_and_predict(model, train_data, valid_data, test_data, \n                      learning_rate, epochs, batch_size, loss_function, fold):\n        \n    custom_callback = CustomCallback(\n        valid_data=(valid_data[0], valid_data[1]), \n        test_data=test_data,\n        batch_size=batch_size,\n        fold=None)\n\n    optimizer = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    model.compile(loss=loss_function, optimizer=optimizer)\n    model.fit(train_data[0], train_data[1], epochs=epochs, \n              batch_size=batch_size, callbacks=[custom_callback])\n    \n    return custom_callback\n","391141bc":"gkf = GroupKFold(n_splits=10).split(X=df_train.question_body, groups=df_train.question_body) ############## originaln_splits=10\n\noutputs_question = compute_output_arrays(df_train, output_categories_question)\noutputs_answer = compute_output_arrays(df_train, output_categories_answer)\noutputs_both = compute_output_arrays(df_train, output_categories_both)\ninputs_question = compute_input_arays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH, 0) #0 for question inputs\ninputs_answer = compute_input_arays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH, 1) #1 for answer inputs\ninputs_both = compute_input_arays(df_train, input_categories, tokenizer, MAX_SEQUENCE_LENGTH, 2) #2 for both inputs\ntest_inputs_question = compute_input_arays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH, 0) #0 for question inputs\ntest_inputs_answer = compute_input_arays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH, 1) #1 for answer inputs\ntest_inputs_both = compute_input_arays(df_test, input_categories, tokenizer, MAX_SEQUENCE_LENGTH, 2) #2 for both inputs","a776d9d1":"# histories = []\n# for fold, (train_idx, valid_idx) in enumerate(gkf):\n    \n#     # will actually only do 3 folds (out of 5) to manage < 2h\n#     if fold < 3:\n#         K.clear_session()\n#         model = bert_model(QUESTION_LABES_VECTOR_SIZE)\n\n#         train_inputs = [inputs_question[i][train_idx] for i in range(3)]\n#         train_outputs = outputs_question[train_idx]\n\n#         valid_inputs = [inputs_question[i][valid_idx] for i in range(3)]\n#         valid_outputs = outputs_question[valid_idx]\n\n#         # history contains two lists of valid and test preds respectively:\n#         #  [valid_predictions_{fold}, test_predictions_{fold}]\n#         history = train_and_predict(model, \n#                           train_data=(train_inputs, train_outputs), \n#                           valid_data=(valid_inputs, valid_outputs),\n#                           test_data=test_inputs_question, \n#                           learning_rate=3e-5, epochs=5, batch_size=8,\n#                           loss_function='binary_crossentropy', fold=fold)\n\n#         histories.append(history)","76e81509":"# test_predictions = [histories[i].test_predictions for i in range(len(histories))]\n# test_predictions = [np.average(test_predictions[i], axis=0) for i in range(len(test_predictions))]\n# test_predictions = np.mean(test_predictions, axis=0)\n\n# df_sub.iloc[:, 1:22] = test_predictions\n\n# df_sub.to_csv('submission_question.csv', index=False)","6edf8209":"histories = []\nfor fold, (train_idx, valid_idx) in enumerate(gkf):\n    \n    # will actually only do 3 folds (out of 5) to manage < 2h\n    if fold < 3:\n        K.clear_session()\n        model = bert_model(ANSWER_LABES_VECTOR_SIZE)\n\n        train_inputs = [inputs_answer[i][train_idx] for i in range(3)]\n        train_outputs = outputs_answer[train_idx]\n\n        valid_inputs = [inputs_answer[i][valid_idx] for i in range(3)]\n        valid_outputs = outputs_answer[valid_idx]\n\n        # history contains two lists of valid and test preds respectively:\n        #  [valid_predictions_{fold}, test_predictions_{fold}]\n        history = train_and_predict(model, \n                          train_data=(train_inputs, train_outputs), \n                          valid_data=(valid_inputs, valid_outputs),\n                          test_data=test_inputs_answer, \n                          learning_rate=3e-5, epochs=5, batch_size=8,\n                          loss_function='binary_crossentropy', fold=fold)\n\n        histories.append(history)","6df8dfa9":"test_predictions = [histories[i].test_predictions for i in range(len(histories))]\ntest_predictions = [np.average(test_predictions[i], axis=0) for i in range(len(test_predictions))]\ntest_predictions = np.mean(test_predictions, axis=0)\n\ndf_sub.iloc[:, 27:] = test_predictions\n\ndf_sub.to_csv('submission_answer.csv', index=False)","d395eaf1":"# histories = []\n# for fold, (train_idx, valid_idx) in enumerate(gkf):\n    \n#     # will actually only do 3 folds (out of 5) to manage < 2h\n#     if fold < 3:\n#         K.clear_session()\n#         model = bert_model(BOTH_LABES_VECTOR_SIZE)\n\n#         train_inputs = [inputs_both[i][train_idx] for i in range(3)]\n#         train_outputs = outputs_both[train_idx]\n\n#         valid_inputs = [inputs_both[i][valid_idx] for i in range(3)]\n#         valid_outputs = outputs_both[valid_idx]\n\n#         # history contains two lists of valid and test preds respectively:\n#         #  [valid_predictions_{fold}, test_predictions_{fold}]\n#         history = train_and_predict(model, \n#                           train_data=(train_inputs, train_outputs), \n#                           valid_data=(valid_inputs, valid_outputs),\n#                           test_data=test_inputs_both, \n#                           learning_rate=3e-5, epochs=5, batch_size=8,\n#                           loss_function='binary_crossentropy', fold=fold)\n\n#         histories.append(history)","7bbc93d6":"# test_predictions = [histories[i].test_predictions for i in range(len(histories))]\n# test_predictions = [np.average(test_predictions[i], axis=0) for i in range(len(test_predictions))]\n# test_predictions = np.mean(test_predictions, axis=0)\n\n# df_sub.iloc[:, 22:27] = test_predictions\n\n# df_sub.to_csv('submission_both.csv', index=False)","536fde81":"3.features plots","18989d79":"#### Calling adam's algorithem to produce new features ","7179c6de":"1.NaN handling","503acf66":"6.**Categorizing questions**\n#HowTo - instructions (looking for actions in the answer)\n#Why \n#What \/ which kind of - classification\n#what is better - comparison\n#when \/ how often - time\n#what\/ how many\/ how much - quantative\n#where - places\n#who \/ whom - person\n#how does \/ how are - comprehension\n#can\/ could - capability\n#should \/would you\/ do you want \/ is, are, am \/ does- Yes\/No questions, willing\n#aren't you? wasn't it? - tag questions (in YES\/NO)","ff2a7622":"2.Extraction of 'host' and 'category' to 'sub_category'","7a7fb341":"**DATA engineering:**\n","2cfbde19":"**lable selected for our project:** \nanswer_relevance prediction","56dab601":"4.correlation sorted list between features","06027059":"**5.tokenization?**","4d349a5c":"?.Redefine the features passed to the input categories - add sub_categoty"}}