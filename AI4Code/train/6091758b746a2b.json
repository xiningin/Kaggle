{"cell_type":{"21cba2da":"code","5900be3c":"code","52a03584":"code","6782881d":"code","c34a0731":"code","afe312f7":"code","1f26d04b":"code","28b7f684":"code","3ab0a8d7":"code","d83b10ab":"code","8f0ac3e2":"code","56225303":"code","61237e89":"code","e0dc55ce":"code","5748f2c0":"code","3f2e9978":"code","d18263fd":"code","7dee7d6e":"code","7bdd1c34":"code","109a662c":"code","60727e55":"code","e96c40b4":"code","59d2481c":"markdown","aa972ae7":"markdown","00d671cb":"markdown","c8318a91":"markdown","ec27330d":"markdown"},"source":{"21cba2da":"import pandas as pd\nimport numpy as np\nimport datetime as dt\nimport pickle\nimport itertools\nimport gc\nimport math\nfrom typing import Tuple, List, Dict\nimport matplotlib.dates as md\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.axes_grid1 import host_subplot\nimport mpl_toolkits.axisartist as AA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\nfrom sklearn.covariance import EllipticEnvelope\nimport dateutil.easter as easter\n","5900be3c":"train_path = '..\/input\/tabular-playground-series-jan-2022\/train.csv'\ntest_path = '..\/input\/tabular-playground-series-jan-2022\/test.csv'\nsample_submission_path = '..\/input\/tabular-playground-series-jan-2022\/sample_submission.csv'\noriginal_train_df = pd.read_csv(train_path)\noriginal_test_df = pd.read_csv(test_path)\nsubm = pd.read_csv(sample_submission_path)\nprint(original_train_df.shape, original_test_df.shape)","52a03584":"!pip install xlrd","6782881d":"!pip install featurewiz --ignore-installed --no-deps","c34a0731":"import featurewiz as FW","afe312f7":"!pip install autoviz","1f26d04b":"%%time\n# The dates are read as strings and must be converted\nfor df in [original_train_df, original_test_df]:\n    df['date'] = pd.to_datetime(df.date)\n    df.set_index('date', inplace=True, drop=False)","28b7f684":"%%time\n# Feature engineering\ndef engineer(df):\n    \"\"\"Return a new dataframe with the engineered features\"\"\"\n    new_df = pd.DataFrame({'year': df.date.dt.year, # This feature makes it possible to fit an annual growth rate\n                           'dayofyear': df.date.dt.dayofyear,\n                           'wd4': df.date.dt.weekday == 4, # Friday\n                           'wd56': df.date.dt.weekday >= 5, # Saturday and Sunday\n                           'dec29': (df.date.dt.month == 12) & (df.date.dt.day == 29), # end-of-year peak\n                           'dec30': (df.date.dt.month == 12) & (df.date.dt.day == 30),\n                          })\n\n    # Easter\n    new_df['easter_week'] = False\n    for year in range(2015, 2020):\n        easter_date = easter.easter(year)\n        easter_diff = df.date - np.datetime64(easter_date)\n        new_df['easter_week'] = new_df['easter_week'] | (easter_diff > np.timedelta64(0, \"D\")) & (easter_diff < np.timedelta64(8, \"D\"))\n    \n    # Growth is country-specific\n    #for country in ['Finland', 'Norway', 'Sweden']:\n    #    new_df[f\"{country}_year\"] = (df.country == country) * df.date.dt.year\n        \n    # One-hot encoding (no need to encode the last categories)\n    for country in ['Finland', 'Norway']:\n        new_df[country] = df.country == country\n    new_df['KaggleRama'] = df.store == 'KaggleRama'\n    for product in ['Kaggle Mug', 'Kaggle Sticker']:\n        new_df[product] = df['product'] == product\n        \n    # Seasonal variations (Fourier series)\n    # The three products have different seasonal patterns\n    dayofyear = df.date.dt.dayofyear\n    for k in range(1, 100): # 100\n        new_df[f'sin{k}'] = np.sin(dayofyear \/ 365 * 2 * math.pi * k)\n        new_df[f'cos{k}'] = np.cos(dayofyear \/ 365 * 2 * math.pi * k)\n        new_df[f'mug_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Mug']\n        new_df[f'mug_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Mug']\n        new_df[f'sticker_sin{k}'] = new_df[f'sin{k}'] * new_df['Kaggle Sticker']\n        new_df[f'sticker_cos{k}'] = new_df[f'cos{k}'] * new_df['Kaggle Sticker']\n\n    return new_df\n\ntrain_df = engineer(original_train_df)\ntrain_df['date'] = original_train_df.date\ntrain_df['num_sold'] = original_train_df.num_sold.astype(np.float32)\ntest_df = engineer(original_test_df)\ntest_df.year = 2018 # no growth patch, see https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/298318\n#### George: You forgot to add two extra lines below for test_df. Without these two vars, train_df and test_df will be different.\ntest_df['date'] = original_test_df.date\n\nfeatures = test_df.columns\nprint(len(features))","3ab0a8d7":"print(train_df.shape, test_df.shape)","d83b10ab":"target = 'num_sold'","8f0ac3e2":"train_df = train_df.reset_index(drop=True)\ntrain_df.drop('date', axis=1, inplace=True)\ntrain_df.head()","56225303":"test_df = test_df.reset_index(drop=True)\ntest_df.drop('date', axis=1, inplace=True)\ntest_df.head()","61237e89":"from autoviz.AutoViz_Class import AutoViz_Class","e0dc55ce":"AV = AutoViz_Class()\nfilename = \"\"\nsep = \",\"\ndft = AV.AutoViz(\n    filename,\n    sep=\",\",\n    depVar=target,\n    dfte=train_df,\n    header=0,\n    verbose=0,\n    lowess=False,\n    chart_format=\"svg\",\n    max_rows_analyzed=150000,\n    max_cols_analyzed=100,\n    save_plot_dir=None\n)","5748f2c0":"preds = dft.columns.tolist()\nlen(preds)","3f2e9978":"train_best = train_df[preds]\ntest_best = test_df[preds[:-1]]\nprint(train_best.shape, test_best.shape)","d18263fd":"outputs = FW.simple_lightgbm_model(X_XGB=train_best[preds[:-1]], Y_XGB=train_best[target],\n                               X_XGB_test=test_best[preds[:-1]], modeltype='Regression', verbose=-1)","7dee7d6e":"y_preds = outputs[0]\ny_preds","7bdd1c34":"subm[target] = y_preds\nsubm.head()","109a662c":"pd.DataFrame(y_preds).hist()","60727e55":"train_df[target].hist()","e96c40b4":"subm.to_csv('submission.csv',index=False)","59d2481c":"# It look ~10 mins to select features in this dataset. We have 86 important features now","aa972ae7":"# It took less than 1 min to build a model with RMSE average = 207 over 5 folds","00d671cb":"# Let's use Autoviz to select the best 100 features out of 607 features","c8318a91":"# This simple LightGBM model works wonders since it is highly effective in many competitions","ec27330d":"# Many thanks to these amazing notebooks created by:\n\n@ambrosm \n- the nice EDA for this dataset conveyed per https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-01-eda-which-makes-sense\n- the feature engineering routines invented per https:\/\/www.kaggle.com\/ambrosm\/tpsjan22-03-linear-model\n\n@gvyshnya\n-- uses featurewiz to select 200 features out of ~625 features https:\/\/www.kaggle.com\/gvyshnya\/jan22-tpc-feature-importance-with-featurewiz\/comments"}}