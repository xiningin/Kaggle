{"cell_type":{"057338fb":"code","b7e06c28":"code","1902b155":"code","06d229c5":"code","81322920":"code","c2666161":"code","1aa99592":"code","bf3adfba":"code","1808ab7f":"code","a4eef6ee":"code","9130ff24":"code","b79cacd1":"code","1aeb5b20":"code","d31b8505":"code","bbfd9150":"code","633fef0d":"code","dc9afa84":"code","8e28cc92":"code","091b20e4":"code","058400f6":"code","baa3e000":"code","9e543604":"code","dc4d5068":"code","6caca8a4":"code","404cc48d":"code","b916de2c":"code","f2cc26ba":"code","7b465dac":"code","04b3f035":"code","bbb1daad":"markdown","dc1344eb":"markdown"},"source":{"057338fb":"import numpy as np\nimport scipy as sp\nimport pandas as pd\nimport datetime\nimport string\nfrom pandas import DataFrame, Series\nfrom tqdm import tqdm_notebook as tqdm\nimport sys\n\n# \u53ef\u8996\u5316\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\n%matplotlib inline\nimport seaborn as sns\nsns.set_palette('Set2')\n\n# \u8a55\u4fa1\u95a2\u6570\nfrom sklearn.metrics import roc_auc_score, mean_squared_error\n\n# \u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\nfrom sklearn.model_selection import StratifiedKFold, train_test_split, KFold, GroupKFold\n\n#\u6b63\u898f\u5316\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import QuantileTransformer\n\n# \u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\nfrom category_encoders import OrdinalEncoder, OneHotEncoder, TargetEncoder\nfrom sklearn.preprocessing import LabelEncoder\n\n#  \u5b66\u7fd2\u5668\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom sklearn.ensemble import GradientBoostingRegressor, AdaBoostRegressor, RandomForestRegressor\n\n\n# \u6587\u5b57\u5217\u51e6\u7406\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom nltk.stem.porter import *\nfrom nltk.tokenize import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom sklearn.feature_extraction import stop_words","b7e06c28":"# \u5b66\u7fd2\u30c7\u30fc\u30bf\ndf_train = pd.read_csv('..\/input\/sales-prediction-of-clothes-in-e-commerce\/train.csv')\n\n# \u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\ndf_test = pd.read_csv('..\/input\/sales-prediction-of-clothes-in-e-commerce\/test.csv')\n\n# submit\nsubmission = pd.read_csv('..\/input\/sales-prediction-of-clothes-in-e-commerce\/sample_submission.csv')","1902b155":"df_train.head()","06d229c5":"df_test.head()","81322920":"# submission\nsubmission.head()","c2666161":"# \u30bf\u30fc\u30b2\u30c3\u30c8\/ID\u30ab\u30e9\u30e0\u306e\u6307\u5b9a\nTarget = 'units_sold'\nID = 'row_id'\n\n# \u5b66\u7fd2\/\u30c6\u30b9\u30c8\u30d5\u30e9\u30b0\ndf_train['train_flag'] = 'train'\ndf_test['train_flag'] = 'test'\n\n# \u5b66\u7fd2\/\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u3092\u7d50\u5408\ndf_concat = pd.concat([df_train, df_test], axis=0)\n\nsubmission[Target] = 0","1aa99592":"print(df_train.shape)\nprint(df_test.shape)\nprint(df_concat.shape)","bf3adfba":"# \u30ab\u30e9\u30e0\u306e\u578b\u306e\u30ea\u30b9\u30c8\u4f5c\u6210\nn_cols = [] # Numerical values\no_cols = [] # Objective values\n\nprint('Numerical cols:')\nfor col in df_concat.columns:\n    if df_concat[col].dtype != 'object':\n        print(col)\n        n_cols.append(col)\n\nprint()\nprint('Object cols:')\nfor col in df_concat.columns:\n    if df_concat[col].dtype == 'object':\n        print(col)\n        o_cols.append(col)\n\n# ID\u3068Target,train\/test\u306e\u30d5\u30e9\u30b0\u306f\u4ee5\u964d\u306e\u30a8\u30f3\u30b3\u30fc\u30c7\u30a3\u30f3\u30b0\u304b\u3089\u9664\u5916\u3059\u308b\nn_cols.remove(ID)\nn_cols.remove(Target)\no_cols.remove('train_flag')","1808ab7f":"print('Unique valuables in all cols:')\nfor col in df_concat.columns:\n    print(col, ': ', df_concat[col].nunique())","a4eef6ee":"# \u95a2\u6570\n\n# \u30ab\u30e9\u30e0\u306e\u5185\u5bb9\u30c1\u30a7\u30c3\u30af\ndef features(df, f):\n    print('##################################################\\n')\n    print('*feature: [', f, ']\\n')\n    print('*N unique     : ', df[f].nunique())\n    print('*Is Null      : ', df[f].isnull().any())\n    print('*Unique values: \\n', df[f].unique(), '\\n')\n    print('*Value counts : \\n', df[f].value_counts(), '\\n')\n    print('##################################################\\n')\n\n# \u30c8\u30fc\u30af\u30f3\u5316\nstop = set(stopwords.words('english'))\ndef tokenize(text):\n    '''\n    sent_tokenize(): segment text into sentences\n    word_tokenize(): break sentences into words\n    '''\n    \n    try:\n        regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n        txt = regex.sub(' ', text) # remove punctuation\n        \n        tokens_ = [word_tokenize(s) for s in sent_tokenize(text)]\n        tokens = []\n        for token_by_sent in tokens_:\n            tokens += token_by_sent\n        tokens = list(filter(lambda t: t.lower() not in stop, tokens))\n        filtered_tokens = [w for w in tokens if re.search('[a-zA-Z]', w)]\n        filtered_tokens = [w.lower() for w in filtered_tokens if len(w)>=3]\n        \n        return filtered_tokens\n    except TypeError as e: print(text, e)\n\n# \u307e\u3068\u3081\u3066Ordinal Encoding\ndef ordinal_encoding(df, col):\n    df[col] = df[col].fillna(\"NaN\")\n    oe = OrdinalEncoder()\n    df[col] = oe.fit_transform(df[[col]])\n    df[col].astype('uint8')\n    return df\n\n# \u307e\u3068\u3081\u3066TF-IDF\ndef tfidf_encoding(df, col):\n    tfidf = TfidfVectorizer(max_features=15000, ngram_range=(1, 1), tokenizer=tokenize,)\n    \n    # \u6b20\u640d\u5024\u51e6\u7406\n    df[col] = df[col].fillna('')\n    \n    # \u30c6\u30ad\u30b9\u30c8\u306e\u533a\u5207\u308a\u6587\u5b57\u51e6\u7406\n    df[col] = df[col].replace(\",\", \" \")\n    \n    # df\u5185\u306e\u30c6\u30ad\u30b9\u30c8\u3092 tfidf\u306b\u5909\u63db\n    tfidf_mat = tfidf.fit_transform(df[col])\n    \n    \n    tfidf_mat = pd.DataFrame(tfidf_mat.todense())\n    tfidf_mat = tfidf_mat.add_prefix(f'{col}_TF-IDF_')\n    df.drop([col], axis=1, inplace=True)\n    df.reset_index(drop=True, inplace=True)\n    df = pd.concat([df, tfidf_mat], axis=1)\n    del tfidf_mat\n    \n    return df","9130ff24":"# \u30ab\u30c6\u30b4\u30ea\u5909\u6570\u306e\u4e2d\u8eab\u30c1\u30a7\u30c3\u30af\nfor f in o_cols:\n    features(df_concat, f)","b79cacd1":"# \u6570\u5024\u5909\u6570\u306e\u4e2d\u8eab\u30c1\u30a7\u30c3\u30af\nfor f in n_cols:\n    features(df_concat, f)","1aeb5b20":"Size = {\n    'XS.':'XS',\n    'XS' :'XS',\n    'S'  :'S',\n    'XXS':'2XS',\n    '34' : '34',\n    'M'  : 'M',\n    'L': 'L',\n    'M.':'M',\n    's':'S',\n    'S.':'S',\n    'SIZE XS':'XS',\n    '4XL':'4XL',\n    '3XL':'3XL',\n    'XXL':'2XL',\n    'XL':'XL',\n    'Size -XXS':'2XS',\n    'XXXXL':'4XL',\n    'Size-XS':'XS',\n    'Size-S':'S',\n    'L.':'L',\n    'Size\/S':'S',\n    '2XL':'2XL',\n    'Size M':'M',\n    'Size S':'S',\n    'XXXS':'3XS',\n    '6XL':'6XL',\n    '1 PC - XL':'XL',\n    'Suit-S':'S',\n    'SIZE-XXS':'2XS',\n    'XXXL':'3XL',\n    'Size S.':'S',\n    'SIZE XXS':'2XS',\n    'SIZE S':'S',\n    'X   L':'XL',\n    'Size-XXS':'2XS',\n    'Size-L':'L',\n    'SizeL':'L',\n    'Size4XL':'4XL',\n    'Size--S':'S',\n    'size S':'S',\n    'S..':'S',\n    'Size XXS':'2XS',\n    'Size-5XL':'5XL',\n    'SIZE-4XL':'4XL',\n    'S (waist58-62cm)':'S',\n    'XXXXXL':'5XL',\n    '04-3XL':'4XL',\n    '5XL':'5XL',\n    '25-S':'S',\n    'S(Pink & Black)':'S',\n    'S Pink':'S',\n    'pants-S':'S',\n    'US-S':'S',\n    'S(bust 88cm)':'S',\n    'One Size':'One Size',\n    '32\/L':'L',\n    'Women Size 36':'WSANDARUS',\n    'Women Size 37':'WSANDARUS',\n    'EU39(US8)':'MSANDARUS',\n    'EU 35':'MSANDARUS',\n    'US 6.5 (EU 37)':'MSANDARUS',\n    'US5.5-EU35':'MSANDARUS',\n    'NaN':'NaN',#######################\n    'S Diameter 30cm':'S Diameter 30cm',\n    'S\/M(child)':'S\/M(child)',\n    'Base & Top & Matte Top Coat':'Base & Top & Matte Top Coat',\n    '10 ml':'10 ml',\n    '1':'1',\n    '80 X 200 CM':'80 X 200 CM',\n    '30 cm':'30 cm',\n    '100pcs':'',\n    'Pack of 1':'1pc',\n    '2pcs':'2pcs',\n    '5':'5',\n    '20PCS-10PAIRS':'20PCS-10PAIRS',\n    '26(Waist 72cm 28inch)':'26(Waist 72cm 28inch)',\n    '25':'25',\n    'AU plug Low quality':'AU plug Low quality',\n    '60':'60',\n    'choose a size':'choose a size',\n    '1 pc.':'1pc.',\n    'daughter 24M':'daughter 24M',\n    'Base Coat':'Base Coat',\n    '36':'36',\n    '2':'2',\n    '35':'35',\n    '33':'33',\n    'first  generation':'first  generation',\n    '17':'17',\n    '100 cm':'100 cm',\n    '1pc':'1pc',\n    'Floating Chair for Kid':'Floating Chair for Kid',\n    '10pcs':'10pcs',\n    '29':'29',\n    '4-5 Years':'4-5 Years',\n    'Round':'Round',\n    '4':'4',\n    '100 x 100cm(39.3 x 39.3inch)':'100 x 100cm(39.3 x 39.3inch)',\n    'H01':'H01',\n    'B':'B',\n    '5PAIRS':'5PAIRS',\n    '40 cm':'40 cm',\n    '1m by 3m':'1m by 3m',\n    '20pcs':'20pcs',\n    'Baby Float Boat':'Baby Float Boat'\n}\n\n# \u30c6\u30ad\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u30ab\u30e9\u30e0\ntext_col = [\n    'title',\n    'tags'\n]","d31b8505":"# dict\u578b\u306e\u30ab\u30c6\u30b4\u30ea\u5909\u6570\u5909\u63db\n\n# \u6b20\u640d\u5024\u88dc\u9593 'NaN'\u3067\u57cb\u3081\u308b\ndf_concat['product_variation_size_id'].fillna('NaN', inplace=True)\ndf_concat['product_variation_size_id'] = df_concat['product_variation_size_id'].map(Size)","bbfd9150":"# rating\n\ndf_concat['rating_one_count'].fillna(0, inplace=True)\ndf_concat['rating_two_count'].fillna(0, inplace=True)\ndf_concat['rating_three_count'].fillna(0, inplace=True)\ndf_concat['rating_four_count'].fillna(0, inplace=True)\ndf_concat['rating_five_count'].fillna(0, inplace=True)\n\ndf_concat['voting'] = df_concat['rating_one_count'] + df_concat['rating_two_count'] + df_concat['rating_three_count'] + df_concat['rating_four_count'] + df_concat['rating_five_count']\ndf_concat['rating_ave'] = (df_concat['rating_one_count']*1 + df_concat['rating_two_count']*2 + df_concat['rating_three_count']*3 + df_concat['rating_four_count']*4 + df_concat['rating_five_count']*5) \/ df_concat['voting']","633fef0d":"# \u5272\u5f15\u7387\ndf_concat['Saled_Price'] = abs((df_concat['price'] - df_concat['retail_price']) \/ df_concat['retail_price'])","dc9afa84":"df_concat['Shipping_toDest'] = df_concat['shipping_option_price'] * df_concat['countries_shipped_to']","8e28cc92":"# \u30c6\u30ad\u30b9\u30c8\u578b\u306e\u30ab\u30e9\u30e0\u51e6\u7406(TF-IDF)\nfor col in text_col:\n    df_concat = tfidf_encoding(df_concat, col)","091b20e4":"# \u30c6\u30ad\u30b9\u30c8\u578b\u4ee5\u5916\u306e object\u578b\u30ab\u30e9\u30e0\u3092 ordinal encoding\u3059\u308b\no_cols.remove('title')\no_cols.remove('tags')\n\n# \u3068\u308a\u3042\u3048\u305a\u5e97\u8217ID\u306f\u30c9\u30ed\u30c3\u30d7\no_cols.remove('merchant_id')\no_cols.remove('merchant_title')\n\ncols = ['merchant_id', 'merchant_title']\nfor col in cols:\n    freq = df_concat[col].value_counts()\n    df_concat[col] = df_concat[col].map(freq)\n\n#df_concat.drop('merchant_id', axis=1, inplace=True)\n\n# \u30c6\u30ad\u30b9\u30c8\u578b\u4ee5\u5916\u306e object\u578b\u30ab\u30e9\u30e0\u3092 ordinal encoding\u3059\u308b\ncols = o_cols\nfor col in cols:\n    if df_concat[col].dtype == \"object\":\n        df_concat[col].fillna('NaN', inplace=True)\n        df_concat = ordinal_encoding(df_concat, col)","058400f6":"# \u6570\u5024\u578b\u306e\u6b20\u640d\u5024\u88dc\u9593\n#cols = [f for f in n_cols if f != 'is_train']\ncols = n_cols\ncols.remove('rating_one_count')\ncols.remove('rating_two_count')\ncols.remove('rating_three_count')\ncols.remove('rating_four_count')\ncols.remove('rating_five_count')\n\nfor col in cols:\n    fillnam = -1 #df_concat[col].mean()\n    df_concat[col].fillna(fillnam, inplace=True)","baa3e000":"df_concat","9e543604":"for col in df_concat.columns:\n    print(col, \": \",df_concat[col].dtype, \" \",  df_concat[col].dtype == \"object\")","dc4d5068":"len(df_concat.columns)","6caca8a4":"train = df_concat[df_concat['train_flag'] == 'train']\ntest = df_concat[df_concat['train_flag'] == 'test']\n\nx_train = train.drop([Target, 'train_flag'], axis=1)\ny_train = train[Target]\nx_test = test.drop([Target, 'train_flag'], axis=1)","404cc48d":"# LightGBM\n# \u30d0\u30ea\u30c7\u30fc\u30b7\u30e7\u30f3\nfolds = KFold(n_splits=3, random_state=530, shuffle=True)\n\n\n# \u30b9\u30b3\u30a2\u30c1\u30a7\u30c3\u30af\u7528\nscore = pd.DataFrame()\nscore[ID] = x_train[ID]\nscore['pred'] = np.nan\n\n\n# ID \u3092\u9664\u304f\ncols = [col for col in x_train.columns if col != ID]\n\n\n# feature importance\nfeature_importance = pd.DataFrame()\nfeature_importance[\"feature\"] = cols\n\nfor i, (train_index, test_index) in tqdm(enumerate(folds.split(x_train, y_train))):\n    print('iteration: {}'.format(i))\n    train_data = lgb.Dataset(x_train.loc[train_index, cols], label=y_train[train_index])\n    valid_data = lgb.Dataset(x_train.loc[test_index, cols], label=y_train[test_index]) \n    \n    lgb_params = {'boosting_type': 'gbdt',\n                  'objective':'regression',\n                    'num_leaves': 2048,\n                    'metric': 'mape',# \u8a55\u4fa1\u95a2\u6570\n                    'max_depth': -1,\n                    'learning_rate': 0.01,\n                    'min_child_samples':20,\n                    'min_child_weight':0.001,\n                    'n_estimators': 5000,\n                    'subsample': 0.8,\n                    'subsample_freq': 3,\n                    'colsample_bytree': 0.8,\n                    'random_state': 530,\n                     }\n    \n    model = lgb.train(lgb_params,\n                     train_data,\n                     valid_sets=[valid_data],\n                     verbose_eval=False,\n                     num_boost_round=300,\n                     early_stopping_rounds=20)\n    score.loc[test_index, 'pred'] = model.predict(x_train.loc[test_index, cols], num_iteration=model.best_iteration)\n    submission[Target] = submission[Target] + model.predict(x_test.loc[:, cols], num_iteration=model.best_iteration) \/ 3\n    feature_importance[f'importance_iteration:{i}'] = model.feature_importance()\n\n# score\u5909\u63db\nscore.loc[score['pred'] <= 100, 'pred'] = 100\nscore.loc[(score['pred'] >= 100) & (score['pred'] < 150), 'pred'] = 100\nscore.loc[(score['pred'] >= 150) & (score['pred'] < 350), 'pred'] = 200\nscore.loc[(score['pred'] >= 350) & (score['pred'] < 750), 'pred'] = 500\nscore.loc[(score['pred'] >= 750) & (score['pred'] < 3500), 'pred'] = 1000\nscore.loc[(score['pred'] >=3500) & (score['pred'] < 7500), 'pred'] = 5000\nscore.loc[(score['pred'] >=7500) & (score['pred'] < 15000), 'pred'] = 10000\nscore.loc[(score['pred'] >=15000), 'pred'] = 20000\nprint('MAPE: ', (abs(y_train - score['pred']) \/ score['pred']).mean())","b916de2c":"# \u30a2\u30f3\u30b5\u30f3\u30d6\u30eb\u3059\u308b\u5834\u5408\u306f\uff0c\u30e2\u30c7\u30eb\u306e\u6570\u3067\u4e88\u6e2c\u5024\u3092\u5272\u308b\n#submission['units_sold'] = submission['units_sold'] \/ 5.0\n\nsubmission","f2cc26ba":"# submit\u5909\u63db\nsubmission.loc[(submission['units_sold'] <= 100), 'units_sold'] = 100\nsubmission.loc[(submission['units_sold'] >= 100) & (submission['units_sold'] < 150), 'units_sold'] = 100\nsubmission.loc[(submission['units_sold'] >= 150) & (submission['units_sold'] < 350), 'units_sold'] = 200\nsubmission.loc[(submission['units_sold'] >= 350) & (submission['units_sold'] < 750), 'units_sold'] = 500\nsubmission.loc[(submission['units_sold'] >= 750) & (submission['units_sold'] < 3500), 'units_sold'] = 1000\nsubmission.loc[(submission['units_sold'] >=3500) & (submission['units_sold'] < 7500), 'units_sold'] = 5000\nsubmission.loc[(submission['units_sold'] >=7500) & (submission['units_sold'] < 15000), 'units_sold'] = 10000\nsubmission.loc[(submission['units_sold'] >=15000), 'units_sold'] = 20000","7b465dac":"submission['units_sold'] = submission['units_sold'].astype(int)","04b3f035":"submission.to_csv('.\/submit.csv', index=False)","bbb1daad":"# \u7279\u5fb4\u91cf\u30a8\u30f3\u30b8\u30cb\u30a2\u30ea\u30f3\u30b0","dc1344eb":"# \u30e2\u30c7\u30ea\u30f3\u30b0"}}