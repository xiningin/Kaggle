{"cell_type":{"92f7df56":"code","d237ff1b":"code","8ec780d5":"code","adad1878":"code","9544cbc8":"code","08e95893":"code","538db25e":"code","49a824e2":"code","34289020":"code","72a62932":"code","0faba1b4":"code","ce50a2de":"code","55111bcf":"markdown","f8d7c98f":"markdown","7052caca":"markdown","e982bef1":"markdown","76ca1140":"markdown","9f062091":"markdown","c99f5621":"markdown","9fb7f708":"markdown","e838afa8":"markdown"},"source":{"92f7df56":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Plot \/ Graph stuffs\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nfrom subprocess import check_output\nprint(check_output([\"ls\", \"..\/input\"]).decode(\"utf8\"))","d237ff1b":"dataset = pd.read_csv('..\/input\/winemag-data-130k-v2.csv')\n\ndataset = dataset[dataset.duplicated('description', keep=False)]\nprint(\"Length of dataframe after duplicates are removed:\", len(dataset))\n\ndataset.dropna(subset=['description', 'points'])\nprint(\"Length of dataframe after NaNs are removed:\", len(dataset))\n\ndataset.drop(['taster_name','taster_twitter_handle'], axis=1, inplace=True)\n\n# let's drop regions for now since we have province which does not have NaNs\ndataset.drop(['region_1','region_2', 'designation'], axis=1, inplace=True)\n\ndataset.head()","8ec780d5":"def plot_nan_bar(datset):\n    nan_columns = []\n    nan_values = []\n\n    for column in dataset.columns:\n        nan_columns.append(column)\n        nan_values.append(dataset[column].isnull().sum())\n\n    fig, ax = plt.subplots(figsize=(20,10))\n    plt.bar(nan_columns, nan_values)\n    \ndef set_median_price_by_province(province):\n    df = dataset[(dataset.province == province) & (dataset.price.isna())]\n    df.price = dataset[dataset.province == province].price.median()\n    dataset.loc[df.index] = df    ","adad1878":"plot_nan_bar(dataset)\n\n# Fix missing price values\nfor province in dataset.province.unique():\n    set_median_price_by_province(province)\n    \nplot_nan_bar(dataset) \n\n# Drop the rest\ndataset.dropna(inplace=True)\nprint(len(dataset))\n\ndataset.head()","9544cbc8":"import re\n\ndef get_year(x):\n    res = re.search('(\\d+)', x)\n    if res == None:\n        return None\n    return res.group()\n\ndataset['year'] = dataset.title.apply(get_year)\ndataset.drop(['title'], axis=1, inplace=True)\nplot_nan_bar(dataset)\n\n# drop year missing values\ndataset.dropna(inplace=True)","08e95893":"# This is from original kernel\ndef transform_points_simplified(points):\n    if points < 84:\n        return 1\n    elif points >= 84 and points < 88:\n        return 2 \n    elif points >= 88 and points < 92:\n        return 3 \n    elif points >= 92 and points < 96:\n        return 4 \n    else:\n        return 5\n\ndataset = dataset.assign(points_simplified = dataset['points'].apply(transform_points_simplified))\ndataset.head()","538db25e":"dataset['country'] = LabelEncoder().fit_transform(dataset['country'].values)\ndataset['province'] = LabelEncoder().fit_transform(dataset['province'].values)\ndataset['variety'] = LabelEncoder().fit_transform(dataset['variety'].values)\ndataset['winery'] = LabelEncoder().fit_transform(dataset['winery'].values)\n\ndataset.head()","49a824e2":"X = dataset[['country','price','province','winery','year']]\ny = dataset['points_simplified']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2)\n\nprint('X_train: {}'.format(X_train.shape))\nprint('y_train: {}'.format(y_train.shape))\nprint('X_test: {}'.format(X_test.shape))\nprint('y_test: {}'.format(y_test.shape))","34289020":"# Example of hard voting \nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier\n\nimport warnings\nwarnings.simplefilter('ignore')\n\nrf = RandomForestClassifier(n_estimators=100)\n\nmodel = rf.fit(X_train, y_train)\npredictions = model.predict(X_test)\nprint(classification_report(y_test, predictions))","72a62932":"## counting the number of words in each value will give more weightage to longer descriptions\n## to avoid this, we can use frequency (TF - Term Frequencies) i.e. #count(word) \/ #Total words, in each value\n\nX = dataset.description\ny = dataset.points_simplified\n\nvectorizer = TfidfVectorizer(stop_words='english')\nX_tfidf = vectorizer.fit_transform(X)\nprint(X_tfidf.shape)\n\nrf2 = ExtraTreesClassifier(n_estimators=100)\n\nmodel2 = rf2.fit(X_train, y_train)\npredictions2 = model2.predict(X_test)\nprint(classification_report(y_test, predictions2))","0faba1b4":"from mlxtend.classifier import StackingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostRegressor\n\nlr = LogisticRegression()\nsclf = StackingClassifier(classifiers=[rf, rf2],\n                          use_probas=True,\n                          average_probas=False,\n                          meta_classifier=lr)\n\nsclf.fit(X_train, y_train)\npredictions3 = sclf.predict(X_test)\nprint(classification_report(y_test, predictions3))","ce50a2de":"from sklearn.model_selection import GridSearchCV\n\nparams = {\n          'randomforestclassifier__n_estimators': [100],\n          'extratreesclassifier__n_estimators': [100],\n          'meta_classifier__C': [0.3, 0.5, 0.7]}\n\ngrid = GridSearchCV(estimator=sclf, \n                    param_grid=params, \n                    cv=5,\n                    refit=True)\n\ngrid.fit(X_train, y_train)\ncv_keys = ('mean_test_score', 'std_test_score', 'params')\n\nfor r, _ in enumerate(grid.cv_results_['mean_test_score']):\n    print(\"%0.3f +\/- %0.2f %r\"\n          % (grid.cv_results_[cv_keys[0]][r],\n             grid.cv_results_[cv_keys[1]][r] \/ 2.0,\n             grid.cv_results_[cv_keys[2]][r]))\n\nprint('Best parameters: %s' % grid.best_params_)\nprint('Accuracy: %.2f' % grid.best_score_)","55111bcf":"## StackingClassifier","f8d7c98f":"## Now classifier for descriptions","7052caca":"## RandomForestClassifier","e982bef1":"## Add year feature","76ca1140":"## Split to train and test sets ","9f062091":"## Clean missing values","c99f5621":"## Here I will try to stack two classifiers, one for descriptions and another for other features.","9fb7f708":"## Encode label columns","e838afa8":"## Gridsearch"}}