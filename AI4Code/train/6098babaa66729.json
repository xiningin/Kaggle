{"cell_type":{"8f188241":"code","bd96f072":"code","6734edb7":"code","9051f099":"code","0f0e957e":"code","368a6d2d":"code","1d4b8969":"code","9e3fc7cf":"code","6bd1bb96":"code","0dc92ee8":"code","06d24e36":"code","0bddcf48":"code","9b02a80e":"code","b7073327":"code","b218e735":"code","048c07ab":"code","7bc7405f":"code","46b67bd1":"code","372ba8c9":"code","13005c8a":"code","7ecaf2b8":"code","2cc8ed82":"code","53428fac":"code","d3cd1f2e":"code","7b360b74":"code","b7cb5448":"code","10ec0fc9":"code","bfb476bb":"code","ad0cc1b1":"code","ec98ab05":"code","ff04d48e":"code","6219a858":"code","95052c53":"code","fe5129d3":"code","33925b2f":"code","232b843d":"code","02f51980":"code","7c0fa81c":"code","ecef4d92":"code","2d9db438":"code","613cfefa":"code","c748120c":"code","850aa097":"code","ff62c121":"code","36467b2b":"markdown","eba2471c":"markdown","e0fbdbfc":"markdown","01ed8e89":"markdown","cd957080":"markdown","e9eb0b5d":"markdown","c69204e3":"markdown","fedafd0d":"markdown","da2bee1c":"markdown","a0771316":"markdown","06d98d21":"markdown","4d6387d9":"markdown","e77cc306":"markdown","f0b16c01":"markdown","9d57b970":"markdown","338144c6":"markdown","18d487e5":"markdown","4db54a45":"markdown","63e0ccff":"markdown","8261ab32":"markdown","550e42e7":"markdown","4908e226":"markdown","1c323a64":"markdown","2ba8b536":"markdown","64788241":"markdown","3081ff9d":"markdown","9f63b3fe":"markdown","02342296":"markdown","75df2c3c":"markdown","e263e067":"markdown","f33ed4d6":"markdown","90f228a1":"markdown","4377147c":"markdown","cffcff07":"markdown","8554f08a":"markdown","3a77b152":"markdown","1a27f1f7":"markdown","e245bd9e":"markdown","283bba7b":"markdown","703babe8":"markdown","aa60f539":"markdown","5f2cb676":"markdown","7119cf8d":"markdown","7cd97ddc":"markdown","10c496e5":"markdown","c9adabfe":"markdown"},"source":{"8f188241":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport nltk\nfrom string import punctuation\nimport re\nfrom nltk.stem import PorterStemmer, WordNetLemmatizer\n  \nwn = WordNetLemmatizer()\nfrom nltk.tokenize import word_tokenize\nfrom xgboost import XGBClassifier\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics as metrics\n\nfrom sklearn.metrics import accuracy_score,roc_auc_score,precision_score, recall_score,f1_score,classification_report\nps = PorterStemmer()\n\nfrom catboost import CatBoostClassifier\n\nimport string\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","bd96f072":"df_train = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_test = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\n","6734edb7":"df_train.head()","9051f099":"df_test.head()\n","0f0e957e":"df_train.head()","368a6d2d":"df_test.head()","1d4b8969":"# Define a function to explore the train and test dataframes\ndef explore_data(df):\n    \n    \n    '''Input- df= pandas dataframes to be explored\n       Output- print shape, info and first 5 records of the dataframe \n    '''\n    \n    print(\"-\"*50)\n    print('Shape of the dataframe:',df.shape)\n    print(\"Number of records in train data set:\",df.shape[0])\n    print(\"Information of the dataset:\")\n    df.info()\n    print(\"-\"*50)\n    print(\"First 5 records of the dataset:\")\n    return df.head()\n    print(\"-\"*50)","9e3fc7cf":"explore_data(df_train)\n","6bd1bb96":"explore_data(df_test)\n","0dc92ee8":"#Let's define a function to explore the missing values for the two datasets\n\ndef missing_values(df):\n    print('{}% of location values are missing from Total Number of Records.'.format(round((df.location.isnull().sum())\/(df.shape[0])*100),2))\n    print('{}% of keywords values are missing from Total Number of Records.'.format(round((df.keyword.isnull().sum())\/(df.shape[0])*100),2))\n    sns.heatmap(df.isnull(),yticklabels=False,cbar=False)\n    null_feat = pd.DataFrame(len(df['id']) - df.isnull().sum(), columns = ['Count'])\n\n    trace = go.Bar(x = null_feat.index, y = null_feat['Count'] ,opacity = 0.8, marker=dict(color = 'lightgrey', line=dict(color='#000000',width=1.5)))\n\n    layout = dict(title =  \"Missing Values\")\n                    \n    fig = dict(data = [trace], layout=layout)\n    py.iplot(fig)\n\n\n","06d24e36":"#let's use the missing_values function to see the missing values in the train dataset\nmissing_values(df_train)","0bddcf48":"#Now lets see the missing values in the test dataset\nmissing_values(df_test)\n","9b02a80e":"print(f'Number of unique values in keyword = {df_train[\"keyword\"].nunique()} (Training) - {df_test[\"keyword\"].nunique()} (Test)')\nprint(f'Number of unique values in location = {df_train[\"location\"].nunique()} (Training) - {df_test[\"location\"].nunique()} (Test)')","b7073327":"df_train['target_mean'] = df_train.groupby('keyword')['target'].transform('mean')\n\nfig = plt.figure(figsize=(8, 72), dpi=100)\n\nsns.countplot(y=df_train.sort_values(by='target_mean', ascending=False)['keyword'],\n              hue=df_train.sort_values(by='target_mean', ascending=False)['target'])\n\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc=1)\nplt.title('Target Distribution in Keywords')\n\nplt.show()\n\ndf_train.drop(columns=['target_mean'], inplace=True)","b218e735":"print('Target of 0 is {} % of total'.format(round(df_train['target'].value_counts()[0]\/len(df_train['target'])*100)))\nprint('Target of 1 is {} % of total'.format(round(df_train['target'].value_counts()[1]\/len(df_train['target'])*100)))\nx=df_train.target.value_counts()\nsns.barplot(x.index,x)\nplt.gca().set_ylabel('samples')","048c07ab":"sns.barplot(y=df_train['location'].value_counts()[:10].index,x=df_train['location'].value_counts()[:10],orient='h');","7bc7405f":"# Drop the column 'location' from the training dataset\ndf_train=df_train.drop(['location'],axis=1)","46b67bd1":"# A disaster tweet exmaple\ndf_train[df_train['target']==1]['text'][10:20]","372ba8c9":"#A non-disaster tweet example\ndf_train[df_train['target']==0]['text'][10:20]","13005c8a":"\ndf_train['words_count'] = df_train['text'].str.split().map(lambda x: len(x))\ndf_train.head()","7ecaf2b8":"#Create visualization of the distribution of the word counts in comparision to target feature\nfig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\ndis_tweet=df_train[df_train['target']==1]['words_count']\nax1.hist(dis_tweet,color='blue')\nax1.set_title('Disaster tweets')\nax1.grid()\nnondis_tweet=df_train[df_train['target']==0]['words_count']\nax2.hist(nondis_tweet,color='red')\nax2.set_title('Non-disaster tweets')\nax2.grid()\nfig.suptitle('Words in a tweet')\nplt.show()","2cc8ed82":"df_train['text_length'] = df_train['text'].apply(lambda x : len(x))\ndf_train.head()","53428fac":"#Create visualization of the distribution of text length in comparision to target feature\nf, (ax1, ax2) = plt.subplots(1, 2, sharex=True,figsize=(10,6))\nsns.distplot(df_train[(df_train['target'] == 1)]['text_length'], ax=ax1, kde=False, color='blue',label='Disater Tweets')\nsns.distplot(df_train[(df_train['target'] == 0)]['text_length'],ax=ax2, kde=False, color='red',label='Non-Disater Tweets');\nf.suptitle('Tweet length distribution')\nf.legend(loc='upper right')\nax1.grid()\nax2.grid()\nplt.show()","d3cd1f2e":"def remove_punctuation(text):\n    no_punct=[words for words in text if words not in string.punctuation ]\n    words_wo_punct=''.join(no_punct)\n    return words_wo_punct\n\n# Remove punctuation from both train and test dataset\ndf_train['text_wo_punct']=df_train['text'].apply(lambda x: remove_punctuation(x))\ndf_test['text_wo_punct']=df_test['text'].apply(lambda x: remove_punctuation(x))\n\ndf_train.head()","7b360b74":"def tokenize(text):\n    split=re.split(\"\\W+\",text) \n    return split\ndf_train['text_wo_punct_split']=df_train['text_wo_punct'].apply(lambda x: tokenize(x.lower()))\ndf_test['text_wo_punct_split']=df_test['text_wo_punct'].apply(lambda x: tokenize(x.lower()))\n\ndf_train.head()\n","b7cb5448":"stopword = nltk.corpus.stopwords.words('english')\nprint(stopword[:11])","10ec0fc9":"def remove_stopwords(text):\n    text=[word for word in text if word not in stopword]\n    return text\n\ndf_train['text_wo_punct_split_wo_stopwords']=df_train['text_wo_punct_split'].apply(lambda x: remove_stopwords(x))\ndf_test['text_wo_punct_split_wo_stopwords']=df_test['text_wo_punct_split'].apply(lambda x: remove_stopwords(x))\ndf_train.head()","bfb476bb":"print(ps.stem('believe'))\nprint(ps.stem('believing'))\nprint(ps.stem('believed'))\nprint(ps.stem('believes'))","ad0cc1b1":"from nltk.stem import WordNetLemmatizer\n  \nlemmatizer=nltk.stem.WordNetLemmatizer()\n\nprint(lemmatizer.lemmatize('believe'))\nprint(lemmatizer.lemmatize('believing'))\nprint(lemmatizer.lemmatize('believed'))\nprint(lemmatizer.lemmatize('believes'))\n","ec98ab05":"def lemmatize_text(word_list):\n    \n    lemmatized_output = ' '.join([lemmatizer.lemmatize(w) for w in word_list])\n    return lemmatized_output\n\n\ndf_train['text_wo_punct_split_wo_stopwords_lim']=df_train['text_wo_punct_split_wo_stopwords'].apply(lambda x: lemmatize_text(x))\ndf_test['text_wo_punct_split_wo_stopwords_lim']=df_test['text_wo_punct_split_wo_stopwords'].apply(lambda x: lemmatize_text(x))\ndf_train.head()","ff04d48e":"def text_clean(text):\n    text = re.sub(r'^https?:\\\/\\\/.*[\\r\\n]*', '', text, flags=re.MULTILINE)\n    text = re.sub('<.*?>+', '', text)\n    regrex_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags = re.UNICODE)\n    text = regrex_pattern.sub(r'',text)\n    text = ''.join([i for i in text if not i.isdigit()])\n    return text\n","6219a858":"df_train['clean_text']=df_train['text_wo_punct_split_wo_stopwords_lim'].apply(lambda x: text_clean(x))\ndf_test['clean_text']=df_test['text_wo_punct_split_wo_stopwords_lim'].apply(lambda x: text_clean(x))\ndf_train.head()","95052c53":"from sklearn.feature_extraction.text import CountVectorizer\n# Vectorize the text using CountVectorizer\ncount_vectorizer = CountVectorizer()\ntrain_cv = count_vectorizer.fit_transform(df_train['clean_text'])\ntest_cv = count_vectorizer.transform(df_test[\"clean_text\"])\n\n## Keeping only non-zero elements to preserve space \nprint(train_cv[0].todense())","fe5129d3":"from sklearn.model_selection import train_test_split\n\n#Split the CountVector vectorized data into train and test datasets for model training and testing\nX_train, X_test, y_train, y_test =train_test_split(train_cv,df_train.target,test_size=0.2,random_state=2020)\nprint('Shape of train',X_train.shape)\nprint(\"Shape of Validation \",X_test.shape)","33925b2f":"from sklearn.ensemble import RandomForestClassifier\n\nRDclassifier = RandomForestClassifier(n_estimators=1000, random_state=0)\nRDclassifier.fit(X_train,y_train)","232b843d":"y_pred = RDclassifier.predict(X_test)","02f51980":"from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n\nprint(confusion_matrix(y_test,y_pred))\nprint(classification_report(y_test,y_pred))\nprint(accuracy_score(y_test, y_pred))","7c0fa81c":"# fit model no training data\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)","ecef4d92":"# make predictions for test data\ny_pred2 = model.predict(X_test)\n","2d9db438":"print(confusion_matrix(y_test,y_pred2))\nprint(classification_report(y_test,y_pred2))\nprint(accuracy_score(y_test, y_pred2))","613cfefa":"import pickle\n\nwith open('text_classifier', 'wb') as picklefile:\n    pickle.dump(RDclassifier,picklefile)","c748120c":"with open('text_classifier', 'rb') as training_model:\n    model = pickle.load(training_model)","850aa097":"y_pred2 = model.predict(test_cv)\n\n# print(confusion_matrix(y_test, y_pred2))\n# print(classification_report(y_test, y_pred2))\n# print(accuracy_score(y_test, y_pred2)) ","ff62c121":"sample_submission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')\ndata={\"id\":[],\"target\":[]}\nfor id,pred in zip(sample_submission['id'].unique(),y_pred2): \n    data[\"id\"].append(id) \n    data[\"target\"].append(pred)\n\n    \noutput=pd.DataFrame(data,columns=[\"id\",\"target\"])\noutput\nprint(output)\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","36467b2b":"Adding a new column text_length to train dataset ","eba2471c":"1. ** Random Forest Algorithm**","e0fbdbfc":"# Step 1: Punctuation\n\n","01ed8e89":"# 4. Text Vectorization","cd957080":"the column Location can be dropped","e9eb0b5d":"let's visulize top 10 location ","c69204e3":"Lets create a simple classification model using commonly used calssification algorithms and check how the models performs.\n\n","fedafd0d":"Here we have 3 data sets \n1. train.csv - the training set\n2. test.csv - the test set\n3. sample_submission.csv - a sample submission file in the correct format","da2bee1c":"We have to predict whether a given tweet is about a real disaster or not.\nIf real disaster, predict a 1. If not, predict a 0.","a0771316":"1. Bag of Words\nThe bag of words is a representation of text that describes the occurrence of words within a document. It involves two things:\n\n* A vocabulary of known words.\n* A measure of the presence of known words.\n\nWhy is it is called a \u201cbag\u201d of words? \n\nIts called bag of words because any information about the order or structure of words in the document is discarded and the model is only concerned with whether the known words occur in the document, not where they occur in the document.\n\n**Bag of Words - Countvectorizer Features**\nCountvectorizer converts a collection of text documents to a matrix of token counts. It is important to note that CountVectorizer comes with a lot of options to automatically do preprocessing, tokenization, and stop word removal. However, all the pre-processing of the text has already been performed by creating a function.Only vanilla version of Countvectorizer will be used.","06d98d21":"To load the model\n","4d6387d9":"# 5. Build a Text Classification model","e77cc306":"We are going lemmatize our text","f0b16c01":"The output is similar to the one we got earlier which showed that we successfully saved and loaded the model.\n\n","9d57b970":"# Step 5: other clearning steps \n    1.Remove URLs\n    2.Remove HTML tags\n    3.Remove emoji\n    4.Remove numbers ","338144c6":"I will be defining a new fuction to perform the cleaning","18d487e5":"Evaluating the Algorithm","4db54a45":"Let\u2019s look at what the disaster and the non disaster tweets look like.","63e0ccff":"We can save our model as a pickle object in Python.","8261ab32":"# 2. Data Preprocessing","550e42e7":"Word Embeddings or Word vectorization is a methodology in NLP to map words or phrases from vocabulary to a corresponding vector of real numbers which used to find word predictions, word similarities\/semantics.","4908e226":"# Step 3: Stop words\nNow, we have a list of words without any punctuation. Let\u2019s go ahead and remove the stop words. Stop words are irrelevant words that won\u2019t help in identifying a text as real or fake. We will use \u201cnltk\u201d library for stop-words and some of the stop words in this library are :","1c323a64":"Tying the xgboost classifier ","2ba8b536":"![image.png](attachment:4896a76a-e370-4989-8703-d93c97240d91.png)","64788241":"# # Class distribution \n### lets see the class distibution for 0 and 1 ","3081ff9d":"Saving and Loading the Model\n","9f63b3fe":"# Step 4 : Lemmatize\/ Stem\nStemming and Lemmatizing is the process of reducing a word to its root form. The main purpose is to reduce variations of the same word, thereby reducing the corpus of words we include in the model. The difference between stemming and lemmatizing is that, stemming chops off the end of the word without taking into consideration the context of the word. Whereas, Lemmatizing considers the context of the word and shortens the word into its root form based on the dictionary definition. Stemming is a faster process compared to Lemmantizing. Hence, it a trade-off between speed and accuracy.\n\n\nLet\u2019s consider the word \u201cbelief\u201d for example.\nThe different variations of believe can be believing, believed, believes, and believe .","02342296":"To remove the punctuation in our dataset, let\u2019s create a function and apply the function to the dataset:","75df2c3c":"Lets further analyze the text feature and if there is any correlation between text content and length of the tweets itself.","e263e067":"Since it doesn't seem that location feature has any value or correlation to our problem to be solved, it could be dropped from the data frame.","f33ed4d6":"# Step 2: Tokenization\nTokenizing is the process of splitting strings into a list of words. We will make use of Regular Expressions or regex to do the splitting. Regex can be used to describe a search pattern.","90f228a1":"To convert string data into numerical data one can use following methods\n* \u00b7 Bag of words\n* \u00b7 TFIDF\n* \u00b7 Word2Vec\n","4377147c":"The lemmatize results in the order of print statements are \u2014 believe, believing, believed, and belief.\nLemmatize produces the same result if the word is not in the corpus. Believe is lemmatized to belief (the root word)","cffcff07":"# Google Word2Vec\n It is deep learning technique with two-layer neural network.Google Word2vec take input from large data and convert into vector space. \n Google word2vec is basically pretrained on google dataset.\n \n**Word2vec** basically place the word in the feature space is such a way that their location is determined by their meaning i.e. words having similar meaning are clustered together and the distance between two words also have same meaning. Consider an example given below:","8554f08a":"# 1. Data analysis and exploration","3a77b152":"The column after step 3 has removed the unnecessary stop words.\n","1a27f1f7":"The column after step 1 has removed the punctuations from the text.\n","e245bd9e":"**Training and Testing Sets**\nLike any other supervised machine learning problem, we need to divide our data into training and testing sets. To do so, we will use the train_test_split utility from the sklearn.model_selection","283bba7b":"There are 179 stop words in this library.","703babe8":"# 3 Data Cleaning\nBefore starting any NLP project, text data needs to be pre-processed to convert it into in a consistent format.Text will be cleaned, tokneized and converted into a matrix.\n\n","aa60f539":"\nWe can see clearly that disaster tweets are written in a more formal way with longer words compared to non-disaster tweets","5f2cb676":"There are more tweets with class 0 ( No disaster) than class 1 ( disaster tweets)","7119cf8d":"# # Data Exploration","7cd97ddc":"# # Problem statement\nthe purpose of this notebook is to analyze and explore the given dataset ' df_train ' and create a machine learning model to be able to classify test tweets data into announcing disaster or not.","10c496e5":"The stem results for all of the above is believ\n","c9adabfe":"Here, \u201c\\W+\u201d splits on one or more non-word character\n"}}