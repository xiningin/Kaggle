{"cell_type":{"41deac62":"code","e7f8c019":"code","be2eaa92":"code","7d6dd861":"code","d1d71229":"code","9d112c07":"code","b5c404e2":"code","38540001":"code","54c36f7a":"code","7c9972f4":"code","28e54134":"code","a00318bd":"code","0fc3a70d":"code","7e1ba8dc":"code","9a521f42":"code","0427ee00":"code","7a19cf77":"markdown","20f74aea":"markdown","83a658b4":"markdown","f4f62eff":"markdown","155f55c1":"markdown","c685d075":"markdown","cae423ab":"markdown","60dcc31e":"markdown","f7de0b6a":"markdown"},"source":{"41deac62":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","e7f8c019":"df_train=pd.read_csv('..\/input\/train.csv')\ndf_train.head()","be2eaa92":"import matplotlib.pyplot as plt","7d6dd861":"print('Distribution of damage dealt')\nprint('{0:.4f}% players dealt zero damage'.format((df_train['damageDealt'] == 0).sum()\/ df_train.shape[0]))\nplt.hist(df_train['damageDealt'], bins=40);","d1d71229":"print('Distribution of damage dealt')\nprint('{0:.4f}% players dealt zero damage'.format((df_train['DBNOs'] == 0).sum()\/ df_train.shape[0]))\nplt.hist(df_train['damageDealt'], bins=40);","9d112c07":"df_train=df_train.drop(['groupId'],axis=1)\ndf_train.head()","b5c404e2":"y_train=df_train['winPlacePerc']\ndf_train=df_train.drop(['winPlacePerc'],axis=1)\ndf_train.shape","38540001":"y_train.head()","54c36f7a":"#from sklearn.ensemble import RandomForestRegressor\nimport os\n","7c9972f4":"n=os.cpu_count()\n#regr = RandomForestRegressor( n_jobs=n)\n#regr.fit(df_train, y_train)","28e54134":"from sklearn.model_selection import train_test_split\nX_train, X_val, y_Train, y_val = train_test_split(df_train, y_train, test_size=0.3)\ndel df_train,y_train","a00318bd":"from sklearn import preprocessing\nscaler = preprocessing.MinMaxScaler().fit(X_train)","0fc3a70d":"X_train = scaler.transform(X_train)\n","7e1ba8dc":"X_val = scaler.transform(X_val)","9a521f42":"df_test=pd.read_csv('..\/input\/test.csv')","0427ee00":"df_test.head()","7a19cf77":"df_submission.to_csv('subs.csv', index=False)","20f74aea":"#dtest = xgb.DMatrix(df_test.values)\npreds= regr_2.predict(df_test)","83a658b4":"df_submission.shape","f4f62eff":"df_submission['Id']=id\ndf_submission['winPlacePerc']=preds","155f55c1":"id=df_test['Id']\ndf_test=df_test.drop(['Id'], axis=1)\ndf_test.head()","c685d075":"#import xgboost as xgb\n#import lightgbm as lgb\n#dtrain = xgb.DMatrix(X_train.values, label=y_train.values)\n#dval= xgb.DMatrix(X_val.values, label=y_val.values)\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\n'''\nparams = {\n\"max_depth\" : 10,\n'booster':'dart',\n'eval_metric': ['mae'],\n}\n\nparam = {'num_leaves':31, 'num_trees':100, 'objective':'binary'}\nparam['metric'] = 'auc'\n\ntrain_data = lgb.Dataset(df_train, label=y_train)\n\nnum_round = 100\n'''\nregr_2 = AdaBoostRegressor(DecisionTreeRegressor(max_depth=4),\n                          n_estimators=300, random_state=42)\nclf = regr_2.fit(df_train,y_train)","cae423ab":"df_submission=pd.read_csv('..\/input\/sample_submission.csv')\n","60dcc31e":"df_submission.head()","f7de0b6a":"df_submission.head()"}}