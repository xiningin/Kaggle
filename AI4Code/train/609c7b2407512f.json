{"cell_type":{"6a573032":"code","41424526":"code","864e4d24":"code","ebde9f81":"code","275f2149":"code","17a2c147":"code","c9d99d82":"code","7e5881b8":"code","15ad0dc6":"code","1f9ceaf7":"code","0437e28b":"code","a3c5a568":"code","71515d81":"code","dd676b6e":"code","3640f1bd":"code","8d7efec5":"code","ca818dd5":"code","a7a3b13e":"code","9e52b06f":"code","5598da57":"code","10c7bcc8":"code","538d0ce4":"markdown","769dcf3e":"markdown","b8e8ae95":"markdown","c505a3e1":"markdown","134f073d":"markdown","21cf2955":"markdown","88859ca6":"markdown","369a438b":"markdown","592cf7b9":"markdown","4e3b2370":"markdown","7af6be68":"markdown","189228d0":"markdown","003311db":"markdown","3668a2dc":"markdown","544774aa":"markdown","836d5778":"markdown","a61d2a44":"markdown"},"source":{"6a573032":"import pandas as pd\n\n#import data visualization libraries\nimport missingno\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n","41424526":"PROJECT_ID = 'kaggle-tutorial-249207' #<-- replace this project ID with your own if forking\n # enable BigQuery in your kernel settings for panel in right side ---------> \n\nfrom google.cloud import bigquery\nclient = bigquery.Client(project=PROJECT_ID, location=\"US\")\ndataset = client.create_dataset('model_dataset', exists_ok=True)\n\nfrom google.cloud.bigquery import magics\nfrom kaggle.gcp import KaggleKernelCredentials\nmagics.context.credentials = KaggleKernelCredentials()\nmagics.context.project = PROJECT_ID","864e4d24":"%load_ext google.cloud.bigquery","ebde9f81":"dataset_ref = client.dataset(\"austin_bikeshare\", project=\"bigquery-public-data\")\n\n# API request - fetch the dataset\ndataset = client.get_dataset(dataset_ref)\n\ntables = list(client.list_tables(dataset))\n\nfor table in tables:  \n    print(table.table_id)\n\n#print out all tables in dataset - there are two 'bikeshare_stations' & 'bikeshare_trips'\n#explore what sort of data is in 'bikeshare_trips'","275f2149":"# create a reference to our table\ntable_trips = client.get_table(\"bigquery-public-data.austin_bikeshare.bikeshare_trips\")\n\n# look at five rows from our dataset\nclient.list_rows(table_trips, max_results=5).to_dataframe()\n\ntable_stations = client.get_table(\"bigquery-public-data.austin_bikeshare.bikeshare_stations\")\n\n# look at five rows from our dataset\nclient.list_rows(table_stations, max_results=5).to_dataframe()","17a2c147":"table_trips.schema #getting the feature types available from the table 'bikeshare_trips'","c9d99d82":"%%bigquery hourly_rides\nSELECT \n    IFNULL(start_station_name, \"\") as start_station_name,\n    TIMESTAMP_TRUNC(start_time, HOUR) as start_hour,\n    COUNT(*) as num_rides\nFROM `bigquery-public-data.austin_bikeshare.bikeshare_trips`\nWHERE DATE(start_time) < '2018-01-01'\nGROUP BY start_station_name, start_time\nORDER BY start_station_name, start_time","7e5881b8":"hourly_rides.tail() #showing the last 5 values of our data","15ad0dc6":"hourly_rides[hourly_rides['start_station_name'].str.contains(\"None\")]\n# making sure we don't have any missing values inside our dataframe","1f9ceaf7":"missingno.matrix(hourly_rides, figsize = (30,10))\n# visualizing the amount of missing data, and the shape of our training data\n# we have 500k+ rows, with 3 columns and no missing values","0437e28b":"%%bigquery\nCREATE OR REPLACE MODEL`model_dataset.bike_trips`\nOPTIONS(model_type='linear_reg', optimize_strategy = 'batch_gradient_descent') AS \nSELECT\n    IFNULL(start_station_name, \"\") as start_station_name,\n    TIMESTAMP_TRUNC(start_time, HOUR) as start_hour,\n    COUNT(*) as label\nFROM `bigquery-public-data.austin_bikeshare.bikeshare_trips`\nWHERE DATE(start_time) < '2018-01-01'\nGROUP BY start_station_name, start_time\nORDER BY start_station_name, start_time","a3c5a568":"%%bigquery\nSELECT\n  *\nFROM\n  ML.TRAINING_INFO(MODEL `model_dataset.bike_trips`)\nORDER BY iteration ","71515d81":"%%bigquery\nSELECT\n  *\nFROM ML.EVALUATE(MODEL `model_dataset.bike_trips`, (\n    SELECT\n        IFNULL(start_station_name, \"\") as start_station_name,\n        TIMESTAMP_TRUNC(start_time, HOUR) as start_hour,\n        COUNT(*) as label\n    FROM\n        `bigquery-public-data.austin_bikeshare.bikeshare_trips`\n    WHERE\n        DATE(start_time) >= '2018-01-01'\n    GROUP BY start_station_name, start_time\n    ORDER BY start_station_name, start_time))","dd676b6e":"%%bigquery\nSELECT\n    start_station_name,\n    SUM(predicted_label) as predicted_total_rides,\n    (SUM(predicted_label)\/(365*24)) as avg_predicted_rides_hourly,\n    SUM(label) as actual_total_rides,\n    (SUM(label)\/(365*24)) as actual_avg_rides_hourly\nFROM ML.PREDICT(MODEL `model_dataset.bike_trips`, (\n  SELECT\n        IFNULL(start_station_name, \"\") as start_station_name,\n        TIMESTAMP_TRUNC(start_time, HOUR) as start_hour,\n        COUNT(bikeid) as label\n    FROM\n        `bigquery-public-data.austin_bikeshare.bikeshare_trips`\n    WHERE\n        DATE(start_time) BETWEEN '2018-01-01' AND '2019-01-01' AND start_station_name = '22nd & Pearl'\n    GROUP BY start_station_name, start_time\n    ORDER BY start_station_name, start_time))\nGROUP BY start_station_name","3640f1bd":"%%bigquery avg_rides_ps_py\nSELECT \n    EXTRACT(YEAR from start_time) as year,\n    COUNT(*) as total_num_rides,\n    (COUNT(2)\/(365*COUNT(DISTINCT start_station_name))) as avg_rides_per_station_daily\nFROM `bigquery-public-data.austin_bikeshare.bikeshare_trips`\nGROUP BY year\nORDER BY year","8d7efec5":"avg_rides_ps_py","ca818dd5":"%%bigquery check_rides\nSELECT \n    start_station_name,\n    EXTRACT(YEAR from start_time) as year,\n    COUNT(*) as num_rides\nFROM `bigquery-public-data.austin_bikeshare.bikeshare_trips`\nGROUP BY start_station_name, year\nORDER BY start_station_name, year","a7a3b13e":"check_rides.tail()","9e52b06f":"fig, ax = plt.subplots(figsize=(30,13))\nplt.title('Number of Rides per Year', size=30)\nb = sns.lineplot(x='year', y='num_rides', hue='start_station_name', \n           data=check_rides, \n           legend=False)\nb.set_xlabel(\"Years\",fontsize=20)\nb.set_ylabel(\"Num_Rides\",fontsize=20)","5598da57":"%%bigquery check_2019_data\nSELECT \n    start_station_name,\n    EXTRACT(MONTH from start_time) as month,\n    COUNT(*) as num_rides\nFROM `bigquery-public-data.austin_bikeshare.bikeshare_trips`\nWHERE DATE(start_time) > '2019-01-01'\nGROUP BY start_station_name, month\nORDER BY start_station_name, month","10c7bcc8":"check_2019_data.tail(15)","538d0ce4":"# 1. Setup Bigquery Environment and Libraries","769dcf3e":"# Future improvements? What next?\n\nIdeas that I would like to try on a future model to improve accuracy:\n* incorporate more geolocation data from bikeshare_station table into the training model\n* checking for presence of seasonality in bike share trips, and making models based on this subgroup timing instead \n* perhaps doing K folds cross validation\n\n* a deeper dive kernel here - https:\/\/www.kaggle.com\/dcstang\/bqml-bikeshare-deep-dive\n","b8e8ae95":"### Bigquery to get < 2018 dataset for training our model","c505a3e1":"In the below code, we also explore the extent of 2019 data. I was just curious on how recent the 2019 data, given the difference of total rides between 2018 and 2019 is 300,000+. \nBy making a sql query on the monthly rides based on each station, we can see the extent of the data available. In this case, we only have data up to May 2019.\n\nI am quite surprised by the huge difference in total rides. It may be that the bulk of rides happen towards the end of the year, or there may be some seasonality involved. \nI will try to explore some of this in a further kernel - which will be linked here (once I am done!).","134f073d":"# 3. Building a linear regression model to predict number of rides based on station\n\nNow that we have gotten an idea of the dataset available, lets look again to our problem at hand.\nWe want to predict how many bikes to restock at each station. To do this, we must first be able to predict:\n\n1. **the number of rides based on starting station**\n2. **the hour of which the trip starts**\n\nTo do this we are going to train our first model. In the first approach we will use linear regression to make the intended predictions. \nTo prevent underfitting or overfitting, we will have to split our data into a training set and a test set.\n![train_test_split](https:\/\/miro.medium.com\/max\/1050\/1*-8_kogvwmL1H6ooN1A1tsQ.png)\n\nWe will be using Bigquery ML for this project. This means we will be writing our predictive analytics with Bigquery, without much Python.\n\nOther advantages of using Bigquery ML:\n* Decreases complexity \u2014 using a single tool\n* Increases speed \u2014 Moving and formatting large amounts data for Python-based ML frameworks takes longer than model training in BigQuery.\n* Ease of getting data from source - no need for processing and formatting (in this case)\n\n### Preparing our training model\n\nBack to our training model: we will need to decide the which data we can use to train our model. \nFor our first go, we will try using data from before 2018 as training data. This is a **time-based split** of data.\n\nWe will have a test set to evaluate our trained model on data after 2018.\n\nThe features that need to be included are: \n1. start_station_name\n2. hour that the trip starts, TIMESTAMP_TRUNC(start_time, HOUR)\n3. number of rides starting at that station during the hour, num_rides\n","21cf2955":"Running the above prediction for the 22nd & Pearl for 2018-2019:\nThere is a under-estimation of 7,600+ rides over the course of a year.\n\nTo get the average rides per hour, we divide the total rides by 8760hours (which btw is the total number of hours in a year)\n\nFrom a business perspective: following our predictions will cause Austin Bikes to understock bikes by 1 bike\/hour in this particular station.\nThis is not satisfactory and may lead to loss of revenue everyday!\n","88859ca6":"This is the code that is used to generate the visualization in my post [thread](http:\/\/https\/\/www.kaggle.com\/learn-forum\/103465#596290) should you be curious.\nFrom this or the above code, it would be obvious that 2018 is year of upheaval for bike shares. \n\nThus, it would not be surprising for our linear regression model to fail in its predictions since we trained it on data from 2013 to 2017 and tested it on 2018.","369a438b":"### Model creation\n\nNow it's time to turn this data into a model. We'll use the `CREATE MODEL` statement that has a structure like: \n\n##### sql\n```sql\nCREATE OR REPLACE MODEL`model_dataset.bike_trips`\nOPTIONS(model_type='linear_reg') AS \n-- training data query goes here\nSELECT ...\n    column_with_labels AS label\n    column_with_data_1 \n    column_with_data_2\nFROM ... \nWHERE ... (Optional)\nGROUP BY ... (Optional)\n```","592cf7b9":"# Introduction: Rental Bike Predictions for Austin\n\n![bike rentals](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/a\/a0\/Bay_Area_Bike_Share_launch_in_San_Jose_CA.jpg\/640px-Bay_Area_Bike_Share_launch_in_San_Jose_CA.jpg)\n\n### Scenario:\nAustin Bikes has made public their data on bike trips from 2013 to 2019 on Kaggle. They are organizing a Kaggle competition to choose which a model that can help with logistics of restocking bikes at each station.\n\n### Project - build a model to predict:\n* how many riders will start from each station during each hour\n","4e3b2370":"### Inspecting the training data, to make sure we got the criteria needed\nTo do this we will use some features from pandas, and a cool library called missingno.","7af6be68":"# 2. Exploring the dataset\n\n![austin_bikeshare dataset](https:\/\/dcstang.github.io\/assets\/blog_pics\/bigquery_client.png)\n\nA pictorial representation of how the dataset is stored. We will need to have a peek under the hood to see what the data looks like. This will give us some idea on what information we have to work on. \n\nThe dataset that we are going to use is from [Google Public Data](https:\/\/cloud.google.com\/bigquery\/public-data\/)\nThere are two tables in the dataset:\n1. bikeshare_trips - contains all the trip IDs, trip start and end, membership type and station names + ID\n2. bikeshare_station - a list of all the stations and their geographical location & station ID\n\n> Figure borrowed + editted from intro to SQL [here](https:\/\/www.kaggle.com\/learn\/intro-to-sql)","189228d0":"# 5. Making predictions\n\nA good way to figure out where your model is going wrong is to look closer at a small set of predictions. \n\n**Example: Use your model to predict the number of rides for the 22nd & Pearl station in 2018**\n\nWe are going to use a nifty query, where we can get both the predicted number of rides AND the actual number of rides.\n> Nb. this code was modified from Rachael's notebook - my initial implementation was making two queries and examining them in pandas.\nI think this code block is a lot more efficient, as we can do everything in BQML.","003311db":"You should see that the r^2 score here is negative. Negative values indicate that the model is worse than just predicting the mean rides for each example.\n\n### Theories for poor performance\n\nI have shared some explanation regarding the negative r2 score here, with some added visualization.\nThere seems to be other external reasons outside of our model which causes the prediction to be off. Especially in 2018!\nCheck out this [thread](http:\/\/https:\/\/www.kaggle.com\/learn-forum\/103465#596290), where I have made some analysis on the data.\nYou are more than welcome to share your thoughts!","3668a2dc":"# 4. Model evaluation\n\nNow that we have a model, evaluate it's performance on data from 2018 onwards. \nWe will first run TRAINING_INFO to see the training iterations and loss values for each iteration.\nML.EVALUATE is then run with a date specified as 2018 and beyond.\n\n> Note that the ML.EVALUATE function will return different metrics depending on what's appropriate for your specific model. You can just use the regular ML.EVALUATE funciton here. ","544774aa":"*Note*\n\nApproach to splitting data:\n* test data ideally should match as closely as possible the production task\n* thinking about what the data means, and whether the type of split will make sense\n* making sure the test data is large enough to represent real world situations\n\nOther methods of splitting our data could include:\n* K folds cross validation, ie. splitting the train data into a K number of subsets and then taking the final model as the average of K folds\n    (for example, splitting our time series into 10 subsets) - however will increase training time\n* Splitting into a 60-80% training and 20-40% testing size, done in a purely random fashion.","836d5778":"# 6. Getting insights for futher improvements\n\nAt this juncture it is good to take a closer look at our data again and try to spot anything that is throwing off our model.\n\n**Example: Check the average daily rides per station, what is the trend?**","a61d2a44":"# References\n\n### Code snippets and inspiration\n* https:\/\/www.kaggle.com\/rtatman\/bigquery-machine-learning-tutorial\n* https:\/\/www.kaggle.com\/rtatman\/bigquery-machine-learning-exercise (The original kernel which I forked from)\n* https:\/\/www.kaggle.com\/rtatman\/bigquery-machine-learning-exercise-w-solutions\n* https:\/\/www.kaggle.com\/learn\/intro-to-sql\n* https:\/\/www.kaggle.com\/dansbecker\/order-by\n\n### Further info about BQML and misc\n* https:\/\/cloud.google.com\/bigquery-ml\/docs\/bigqueryml-intro\n* https:\/\/developers.google.com\/machine-learning\/data-prep\/construct\/sampling-splitting\/example\n* https:\/\/towardsdatascience.com\/train-test-split-and-cross-validation-in-python-80b61beca4b6\n\nThank you for reading through my kernel. If you have made it through all the way you must have enjoyed it! \nI appreciate you leaving a comment, or upvoting if you learnt something \/ benefitted \/ or are just feeling generous :)\n\nPlease drop me a message for anything that you think needs correction.\nThis is my first full public kernel. I am thankful to be able to contribute something to the community after just learning SQL during Kaggle Summer Camp!"}}