{"cell_type":{"5b5b9b0e":"code","91ba7ff2":"code","826879bd":"code","34fb4d88":"code","00917245":"code","3e2162b7":"code","b3a2d1a1":"code","9ef7a88a":"code","2b2f47c4":"code","f89cdcbe":"code","d959c396":"code","fbd66818":"code","f39f91e1":"code","27ff8be9":"code","24d5bf8f":"code","cbc19e4f":"code","4d56d6d4":"code","d49a1c40":"code","27759aa8":"code","fbc11824":"code","6d22629a":"code","e5449e7a":"code","0d06fc00":"code","ecd1c7b3":"code","535dac7a":"code","45cca5fd":"code","1c0863f9":"code","2aaabc3a":"code","a2889050":"code","264723ae":"code","b631caa7":"code","c5b4fa02":"code","8dca3036":"code","3df1ebdb":"code","27738995":"code","e7a8874c":"code","77f97f7b":"markdown","846f5250":"markdown","ecaa362a":"markdown","ca695d1c":"markdown","8c66fe93":"markdown","3ebb4165":"markdown","e2aa4b4f":"markdown","b3656577":"markdown","5f516ed7":"markdown"},"source":{"5b5b9b0e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","91ba7ff2":"# Important Imports\nimport pandas as pd\nimport numpy as np\nimport seaborn as sb\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\n\n# Load the data\npd.set_option(\"display.max_columns\", 100)\npd.set_option(\"display.max_rows\", 100)\ntrain_df = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/train.csv\")\ntrain_df.head()","826879bd":"# Now I just want to see how many categorical features I have and how many categories per feature\n# I also want to know unique numbers in each numerical features since it could be categorical as well\n\ndef display_information(df):\n    info = pd.DataFrame(df.dtypes, columns=['dtypes'])\n    info = info.reset_index()\n    info['Name'] = info['index']\n    info = info[['Name', 'dtypes']]\n    info['Uniques'] = df.nunique(dropna=False).values\n    info['Missing'] = df.isnull().sum().values\n    \n    return info\n\ndisplay_information(train_df)","34fb4d88":"# Looking at this table you can notice that you have 1460 rows,\n# feature Alley has 1369 missing values\n# feature PoolQC has 1453 missing values\n# feature Fence has 1179 missing values\n# feature MiscFeature has 1406 missing values\n# these amounts of missing values are too big to be imputed, so we will drop these features\n\ntrain_df.drop(columns=[\"Alley\", \"PoolQC\", \"Fence\", \"MiscFeature\"], inplace=True)","00917245":"# now let's plot some of the numerical data\ndef plot_categorical_count(df):\n    color = sb.color_palette()[0]\n    columns = df.select_dtypes([\"object\"]).columns\n        \n    fig, ax = plt.subplots(len(columns) \/\/ 4, 4, figsize=(22, len(columns)))\n    for col, subplot in zip(columns, ax.flatten()):\n        freq = df[col].value_counts()\n        sb.countplot(df[col], order=freq.index, ax=subplot, color=color)\nplot_categorical_count(train_df)","3e2162b7":"# the following function is going to allow us to somehow see the relation between\n# categorical features and the output\ndef plot_categorical_relation_to_target(df, target):\n    color = sb.color_palette()[0]\n    columns = df.select_dtypes([\"object\"]).columns\n        \n    fig, ax = plt.subplots(len(columns) \/\/ 4, 4, figsize=(22, len(columns)))\n    for col, subplot in zip(columns, ax.flatten()):\n        freq = df[col].value_counts()\n        sb.violinplot(data=df, x=col, y=target, order=freq.index, ax=subplot, color=color)\n        \nplot_categorical_relation_to_target(train_df, \"SalePrice\")","b3a2d1a1":"train_df.drop(columns=[\"Utilities\", \"Street\"], inplace=True)","9ef7a88a":"# plot count and violin of feature\n# so we can see the frequency of categories in a certain feature and\n# see how it relates to the output target\ndef count_and_violin(df, feature, target):\n    color = sb.color_palette()[0]\n    fig, ax = plt.subplots(1, 2, figsize=(15, 5))\n    axis = ax.flatten()\n    freq = df[feature].value_counts()\n    sb.countplot(df[feature], order=freq.index, ax=axis[0], color=color)\n    sb.violinplot(data=df, x=feature, y=target, order=freq.index, ax=axis[1], color=color)\n    plt.show()\n    \ncount_and_violin(train_df, \"MSZoning\", \"SalePrice\")\n        ","2b2f47c4":"train_df.describe()","f89cdcbe":"corr = train_df.corr()\ncorr.style.background_gradient(cmap='coolwarm')","d959c396":"# Here are most of the useful features split on three lists according to there types\n# feel free to drop some of them that you think might not be useful.\n# Read the data_description.txt file carefully and reason about the data to elemenate some of the features.\n# also use the correlation matrix and the count_and_violin function to see how the feature affects the target.\n# MSSubClass is still a categorical feature even though it is of type int\n\nnominal = [\"MSZoning\", \"LotShape\", \"LandContour\", \"LotConfig\", \"Neighborhood\",\n           \"Condition1\", \"BldgType\", \"RoofStyle\",\n           \"Foundation\", \"CentralAir\", \"SaleType\", \"SaleCondition\"]\n\nordinal = [\"LandSlope\", \"OverallQual\", \"OverallCond\", \"YearRemodAdd\",\n          \"ExterQual\", \"ExterCond\", \"BsmtQual\", \"BsmtCond\", \"BsmtExposure\",\n          \"KitchenQual\", \"Functional\", \"GarageCond\", \"PavedDrive\"]\n\nnumerical = [\"LotFrontage\", \"LotArea\", \"MasVnrArea\", \"BsmtFinSF1\", \"BsmtUnfSF\",\n            \"TotalBsmtSF\", \"1stFlrSF\", \"2ndFlrSF\", \"GrLivArea\", \"GarageArea\",\n            \"OpenPorchSF\"]","fbd66818":"count_and_violin(train_df, \"YrSold\", \"SalePrice\")","f39f91e1":"# mapping some ordinal features in the correct order\n# because the OrdinalEncoder uses alphabitical order which is not meaningful\n# you could override the default behaviour of the OrdinalEncoder by passing a list of\n# categories in the correct order but then you would need to create an encoder for each feature\nordinal_maps = {\n    \"LandSlope\": {'Gtl': 3, 'Mod': 2, 'Sev': 1},\n    \"ExterQual\": {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n    \"ExterCond\":{'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n    \"BsmtQual\": {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n    \"BsmtCond\": {'Ex': 5, 'Gd': 4, 'TA': 3, 'Fa': 2, 'Po': 1, 'NA': 0},\n    \"BsmtExposure\": {'Gd': 4, 'Av': 3, 'Mn': 2, 'No': 1, 'NA': 0},\n    \"Functional\": {'Typ': 6, 'NA': 6, 'Min1': 5, 'Min2': 4, 'Mod': 3, 'Maj1': 2, 'Maj2': 1, 'Sev': 0}\n}\n\ndef preprocess_ordered_ordinals(df, maps):\n    for key in maps:\n        df[key] = df[key].map(maps[key])\n        \npreprocess_ordered_ordinals(train_df, ordinal_maps)","27ff8be9":"### Importing all needed modules\nfrom sklearn.preprocessing import OneHotEncoder, OrdinalEncoder, StandardScaler\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom yellowbrick.model_selection import RFECV","24d5bf8f":"# Choosing only the useful features\nX = train_df[nominal + ordinal + numerical]\ny = train_df[\"SalePrice\"]","cbc19e4f":"nominal_pipeline = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"encoder\", OneHotEncoder(sparse=True, handle_unknown=\"ignore\"))\n])\n\nordinal_pipeline = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"most_frequent\")),\n    (\"encoder\", OrdinalEncoder())\n])\n\nnumerical_pipeline = Pipeline([\n    (\"imputer\", SimpleImputer(strategy=\"mean\")),\n    (\"scaler\", StandardScaler())\n])\n\nbasic_preprocessor = ColumnTransformer([\n    (\"nominal_preprocessor\", nominal_pipeline, nominal),\n    (\"ordinal_preprocessor\", ordinal_pipeline, ordinal),\n    (\"numerical_preprocessor\", numerical_pipeline, numerical),\n])\n\n\n\nX_preprocessed = basic_preprocessor.fit_transform(X)","4d56d6d4":"from sklearn.linear_model import LinearRegression\ncomplete_pipeline = Pipeline([\n    (\"preprocessor\", basic_preprocessor),\n    (\"estimator\", LinearRegression())\n])\n\ncomplete_pipeline.fit(X, y=y)\nscore = complete_pipeline.score(X, y)\nprint(score)\ncomplete_pipeline.predict(X).shape","d49a1c40":"from sklearn.linear_model import LinearRegression\n# model definition\nmodel = LinearRegression()\n# scores is an array of five elements each element is the scoring on a certain fold\nscores = cross_val_score(model, X_preprocessed, y, scoring=\"neg_root_mean_squared_error\", cv=3)\n# we are going to use the mean of these scores as an evaluation metric\nprint(scores.mean())","27759aa8":"from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(n_estimators=500)\nscores = cross_val_score(model, X_preprocessed, y, scoring=\"neg_root_mean_squared_error\", cv=5)\nprint(scores.mean())","fbc11824":"from sklearn.ensemble import GradientBoostingRegressor\n\nmodel = GradientBoostingRegressor(n_estimators=500)\nscores = cross_val_score(model, X_preprocessed, y, scoring=\"neg_root_mean_squared_error\", cv=5)\nprint(scores.mean())","6d22629a":"from xgboost import XGBRegressor\n\nmodel = XGBRegressor(n_estimators=500)\nscores = cross_val_score(model, X_preprocessed, y, scoring=\"neg_root_mean_squared_error\", cv=5)\nprint(scores.mean())","e5449e7a":"from sklearn.model_selection import GridSearchCV\n\n## First you need to define the parameter space, or the grid\n# it is a simple dictionary where:\n# the keys are the names of the parameters given to the estimator\n# the values are lists of possible values for each parameter\n# the search is going to go as follows\n# {\"n_estimators\": 100, learning_rate: 0.001, max_depth: 3}\n# {\"n_estimators\": 100, learning_rate: 0.001, max_depth: 5}\n# {\"n_estimators\": 100, learning_rate: 0.001, max_depth: 7}\n# {\"n_estimators\": 100, learning_rate: 0.005, max_depth: 3}\n# {\"n_estimators\": 100, learning_rate: 0.005, max_depth: 5}\n# {\"n_estimators\": 100, learning_rate: 0.005, max_depth: 7}\n# ........\n\ngrid = {\n    \"n_estimators\": range(300, 500, 100),\n    \"learning_rate\": [0.05, 0.1, 0.5],\n    \"max_depth\": [2, 3, 4],\n}\n\nsearcher = GridSearchCV(GradientBoostingRegressor(), grid, scoring=\"neg_root_mean_squared_error\",\n                        n_jobs=-1, cv=3, return_train_score=True)\n\nsearcher.fit(X_preprocessed, y)\n\n","0d06fc00":"searcher.best_params_","ecd1c7b3":"# the return_train_score=True paramter allows us to look at the results of the whole process\nscores = pd.DataFrame(searcher.cv_results_)\nscores","535dac7a":"# the best validation score\nsearcher.best_score_","45cca5fd":"# the parameters that got the best score\nsearcher.best_params_","1c0863f9":"# the trained model that got the best score\nmodel = searcher.best_estimator_","2aaabc3a":"test_df = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/test.csv\")\ntest_df.drop(columns=[\"Alley\", \"PoolQC\", \"Fence\", \"MiscFeature\", \"Utilities\", \"Street\"], inplace=True)","a2889050":"preprocess_ordered_ordinals(test_df, ordinal_maps)","264723ae":"X = test_df[nominal + ordinal + numerical]","b631caa7":"X_preprocessed = basic_preprocessor.transform(X)","c5b4fa02":"preds = model.predict(X_preprocessed)","8dca3036":"submission = pd.read_csv(\"\/kaggle\/input\/home-data-for-ml-course\/sample_submission.csv\")\nsubmission.head()","3df1ebdb":"submission['SalePrice'] = preds\nsubmission.head()","27738995":"submission.to_csv(\"submission.csv\", index=False)","e7a8874c":"pd.read_csv(\"submission.csv\").head()","77f97f7b":"## Predictions\nNow that we have a pretty good pipeline and a pretty good model let's submit our predictions","846f5250":"### Let's create some pipelines\nNow we are going to use the lists above to preprocess each feature according to its type.  \nWe are going to create pipelines for the ease of iteration and model evaluation.\n","ecaa362a":"## Model selection\nNow since this is a regression problem with many features, and high linear correlation between those features and the target, we can use linear regression.  \nWe will also try ensemble learning and see if it gives better results.\n\nIn the following cells, you will learn:\n- How to train a scikit-learn estimator (model)\n- How to evaluate the model using cross validation\n\n**Useful Links**\n- [LinearRegression](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html)\n- [RandomForestRegressor](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html)\n- [GradientBoostingRegressor](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingRegressor.html)\n- [cross_val_score](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_val_score.html)","ca695d1c":"# Introduction\n\nIn this notebook we will go through the competition step by step explaining each step as we go.  \n- Understanding the data\n- Preprocessing & Feature Engineering\n- Model selection and evaluation\n\nHold on to your hats!","8c66fe93":"### What I figured\n- By looking at these two figures I can tell that encoding the `Utilities` and `Street` features is useless since most of the data exist in one category and the other category has price that is in the mean of the first category so it seems not to add any value.\n","3ebb4165":"## Understanding the data\nBefore we do anything, let's start by analyzing the data and visualize some of the features.  \n> To understand the feature names read the data_description.txt file.  \n> We also recommend that you google some of the feature descriptions to get some domain knowledge.","e2aa4b4f":"## Preprocessing and feature engineering\n","b3656577":"### What We notice\nIdeally you would want features that correlate with the target column if you are going for linear regression, in that case overallQuall seems to be very linearly correlated, which makes sense. however mssubclass is not linearly correlated, which means it is not useful for linear models, we will keep them for now and use recursive feature elimination or pca later, but only after encoding.  \n\nOne more thing you need to know is that features that correlate with each other are not that useful, like YearBuilt, and GarageYrBlt since most likely they are going to be the same, we only need one of them, and this is visually represented in the correlation matrix.","5f516ed7":"### Grid search\nIt look like GradientBoostingRegressor gave the best results. Now let's figure out which parameters we should give to this model to get the best results.  \nThere are two methods:\n- Grid Search\n- Random Search\nIn the first you try the model with all possible combinations of given parameters and choose the best one, the latter doesn't look at all combinations, it only look at random samples which makes it more effecient but less optimal.  \nSince we don't care that much about time since the model is fairly small, we are going to use grid search"}}