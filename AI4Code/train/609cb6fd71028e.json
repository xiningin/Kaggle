{"cell_type":{"2622e452":"code","4eaba04a":"code","823af254":"code","233dad3e":"code","a8bfadf4":"code","4b37e54f":"code","b32f1a76":"code","b9612129":"code","73801296":"code","1680a357":"code","f9ef47f7":"code","f8be5936":"code","09774d38":"code","fef148ff":"code","e1ac54af":"code","60467beb":"code","d1c4cb21":"code","a7c2d044":"code","8048b225":"code","a8d0554a":"code","8185c86d":"code","5305422d":"code","3f62396d":"code","0dacd565":"markdown","4e03fad8":"markdown","bf600ffa":"markdown","197c09b1":"markdown","ef068b7c":"markdown","1a2b779d":"markdown","0abc0d20":"markdown","f02ddb43":"markdown","0b571248":"markdown","b46bb923":"markdown","ed490178":"markdown","9c8eca53":"markdown","e4d4990b":"markdown","1ef28038":"markdown","733cabc7":"markdown","d1aa8252":"markdown"},"source":{"2622e452":"import pandas as pd\nimport numpy as np\nimport cv2\nimport os\nimport librosa # package for music and audio analysis\n\nfrom tqdm.notebook import tqdm, trange\n\nimport math\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import transforms\nfrom PIL import Image\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom csv import writer\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n","4eaba04a":"PATH = '..\/input\/birdsong-recognition'\nIMG = '..\/input\/birdsongspectrograms'","823af254":"transformers = transforms.Compose([\n    transforms.RandomCrop((128, 512), pad_if_needed = True, padding_mode = \"reflect\"),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5), (0.5))\n])","233dad3e":"def load_image(path, rescale = True, normalize = True):\n    image = Image.open(path)\n    image = transformers(image)\n    return image","a8bfadf4":"df = pd.read_csv(os.path.sep.join([PATH,'train.csv']), skiprows = 0)\nle = LabelEncoder() # encode the\nle.fit(df['ebird_code'].to_numpy())\nlen(le.classes_) # shows the number of classes present in the csv file","4b37e54f":"def append_list_as_rows(file_name, list_of_elem):\n    with open(file_name, 'a+', newline = '') as write_obj:\n        csv_writer = writer(write_obj)\n        csv_writer.writerow(list_of_elem)","b32f1a76":"csv_file_name = 'train_test_data.csv'\ndef remove_previous_csv_file():\n    try:\n        os.remove(csv_file_name)\n        print('[INFO] CSV file removed successfully')\n    except OSError as error:\n        print(f'[ERROR] {error}')\n        print(f'[INFO] {csv_file_name} cannot be removed')","b9612129":"remove_previous_csv_file()\nheader = ['target', 'filepath']\nappend_list_as_rows(csv_file_name, header)\nfor index, row in tqdm(df.iterrows()):\n    bird = row['ebird_code']\n    audio = row['filename'].replace('.mp3', '.jpg')\n    filepath = f'{audio}'\n    \n    target = le.transform([bird])[0] # get the encoded class name\n    \n    if os.path.isfile(os.path.sep.join([IMG, filepath])):\n        append_list_as_rows(csv_file_name, [target, filepath])\n\nprint('[INFO] Complete writing to the csv file')","73801296":"df2 = pd.read_csv(csv_file_name, skiprows = 0)\n\ndf2.head() # prints first five rows\n","1680a357":"VALIDATION_SIZE = 0.1\ndf2 = df2.sample(frac = 1).reset_index(drop = True)\n\ntotal_len = len(df2)\ntrain_size = int(total_len * (1.0 - VALIDATION_SIZE))\nval_size = int(total_len - train_size)\n\nprint(f'[INFO] Total Data: {total_len}, Train Data: {train_size}, Val Data: {val_size}')\n\ndef get_features(option):\n    data = None\n    if option == 'train':\n        data = df2[:train_size]\n    elif option == 'test':\n        data = df2[train_size:]\n    \n    for index, row in tqdm(data.iterrows()):\n        filepath = row['filepath']\n        spectrogram = load_image(os.path.sep.join([IMG, filepath]))\n        \n        yield spectrogram, row['target']\n\nprint(df2.head())","f9ef47f7":"BATCH_SIZE = 32\ndef get_batch(data_generator):\n    X, Y = [], []\n    cnt = 0\n    for x, y in data_generator:\n        X.append(x)\n        Y.append(y)\n        cnt += 1\n        if cnt >= BATCH_SIZE:\n            break\n    return torch.stack(X), torch.tensor(Y)","f8be5936":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\nprint(f'[INFO] Device: {device}')","09774d38":"class model(nn.Module):\n    def __init__(self):\n        super(model, self).__init__()\n        self.conv1 = nn.Conv2d(1, 2, 3) # in_channels, out_channels, kernerl_size\n        self.conv2 = nn.Conv2d(2, 4, 3)\n        self.conv3 = nn.Conv2d(4, 8, 3)\n        \n        fn = 6944\n        self.fc1 = nn.Linear(fn, fn * 2) # in_features, out_features\n        self.fc2 = nn.Linear(fn * 2, fn)\n        self.fc3 = nn.Linear(fn, fn \/\/ 2)\n        self.output = nn.Linear(fn \/\/ 2, len(le.classes_))\n    \n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2(x), 2))\n        x = F.relu(F.max_pool2d(self.conv3(x), 2))\n        \n        x = self.flatten(x)\n        \n        x = F.relu(self.fc1(x))\n        x = F.relu(self.fc2(x))\n        x = F.relu(self.fc3(x))\n        \n        x = self.output(x)\n        \n        return x\n    \n    def flatten(self, x):\n        res = 1\n        for sz in x.size()[1:]:\n            res *= sz\n        return x.view(-1, res)","fef148ff":"LR = 0.0001\n\nnet = model().to(device)\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam(net.parameters(), lr = LR)","e1ac54af":"def get_number_of_correct_for_this_batch(y_pred, y):\n    y_pred = torch.nn.Softmax(dim = 1)(y_pred)\n    y_pred = torch.argmax(y_pred, dim = 1)\n    correct = torch.eq(y_pred, y).sum()\n    return correct.item()","60467beb":"BEST_MODEL_PATH = 'best_model.pth'\n","d1c4cb21":"EPOCHS = 40\nbest_loss = 1000000\npatience = 4\n\nfor epoch in range(EPOCHS):\n    # Training\n    net.train()\n    gen = get_features('train')\n    steps = math.ceil(train_size \/ BATCH_SIZE)\n    total_loss = 0\n    total_correct = 0\n    loop = tqdm(range(steps), total = steps)\n    \n    for i, _ in enumerate(loop):\n        X, Y = get_batch(gen)\n        X, Y = X.to(device), Y.to(device)\n        \n        # Forward Propagation\n        optimizer.zero_grad()\n        y_pred = net(X)\n        loss = criterion(y_pred, Y.view(-1))\n        total_loss += loss.item()\n        \n        # Backward Propagation\n        loss.backward()\n        optimizer.step()\n        \n        with torch.no_grad():\n            # Get Stats\n            correct = get_number_of_correct_for_this_batch(y_pred, Y)\n            total_correct += correct\n            \n            # Update Stats\n            loop.update(1)\n            loop.set_description(f'Epoch {epoch + 1}\/{EPOCHS}')\n            loop.set_postfix(loss = loss.item(), acc = total_correct\/((i + 1) * BATCH_SIZE))\n    \n    # Validation\n    with torch.no_grad():\n        net.eval()\n        gen = get_features('test')\n        steps = math.ceil(val_size \/ BATCH_SIZE)\n        total_loss = 0\n        total_correct = 0\n        loop = tqdm(range(steps), total = steps)\n        \n        for i, _ in enumerate(loop):\n            X, Y = get_batch(gen)\n            X, Y = X.to(device), Y.to(device)\n            \n            y_pred = net(X)\n            \n            loss = criterion(y_pred, Y.view(-1))\n            total_loss += loss.item()\n            \n            correct = get_number_of_correct_for_this_batch(y_pred, Y)\n            total_correct += correct\n            \n            loop.update(1)\n            loop.set_description(f'Epoch {epoch + 1}\/{EPOCHS}')\n            loop.set_postfix(loss = loss.item(), acc = total_correct\/((i + 1) * BATCH_SIZE))\n        \n        # Early Stopping\n        \n        if total_loss < best_loss:\n            best_loss = total_loss\n            patience = 4\n            torch.save(net, BEST_MODEL_PATH)\n        else:\n            patience -= 1\n        \n        if patience <= 0:\n            print(f'[INFO] Early Stopping at {epoch}')\n            break","a7c2d044":"net = torch.load(BEST_MODEL_PATH)","8048b225":"#from https:\/\/www.kaggle.com\/daisukelab\/creating-fat2019-preprocessed-data\ndef mono_to_color(X, mean = None, std = None, norm_max = None, norm_min = None, eps = 1e-6):\n    # Standardize\n    mean = mean or X.mean()\n    X = X - mean\n    \n    std = std or X.std()\n    Xstd = X \/ (std + eps)\n    _min, _max = Xstd.min(), Xstd.max()\n    norm_max = norm_max or _max\n    norm_min = norm_min or _min\n    \n    if (_max - _min) > eps:\n        # Normlize to [0, 255]\n        V = Xstd\n        V[V < norm_min] = norm_min\n        V[V > norm_max] = norm_max\n        V = 255 * (V - norm_min) \/ (norm_max - norm_min)\n        V = V.astype(np.unint8)\n    else:\n        # Just Zero\n        V = np.zeros_like(Xstd, dtype = np.unit8)\n    return V\n\ndef build_spectrogram(path, offset, duration):\n    y, sr = librosa.load(path, offset = offset, duration = duration)\n    total_secs = y.shape[0] \/ sr\n    M = librosa.feature.melspectrogram(y = y, sr = sr)\n    M = librosa.power_to_db(M)\n    M = mono_to_color(M)\n    \n    filename = path.split('\/')[-1][:-4]\n    path = 'test.jpg'\n    cv2.imwrite(path, M, [int(cv2.IMWRITE_JPEG_QUALITY, 85)])\n    return path","a8d0554a":"def make_prediction(x):\n    net.eval()\n    y_pred = net(x)\n    y_pred. nn.Softmax(dim = 1)(y_pred)\n    y_pred = torch.argmax(y_pred, dim = 1)\n    return le.inverse_transform(y_pred)[0]","8185c86d":"TEST_FOLDER = '..\/input\/birdsong-recognition\/test_audio' # hidden folder\n\ntry:\n    preds = []\n    test = pd.read_csv(os.path.sep.join([PATH, 'test.csv']))\n    \n    for index, row in tqdm(test.iterrows()):\n        # Get test row information\n        site = row['site']\n        start_time = row['seconds']\n        row_id = row['row_id']\n        audio_id = row['audio_id']\n        \n        # Get the test sound clip\n        audio_file = os.path.sep.join([TEST_FOLDER, audio_id + '.mp3'])\n        if os.path.isfile(audio_file):\n            if site == 'site_1' or site == 'site_2':\n                path = build_spectrogram(audio_file, start_time, 5)\n                y = load_image(path)\n            else:\n                path = build_spectrogram(audio_file, 0, duration = None)\n                image = load_image(path)\n\n            # Make the predictions\n            pred = make_prediction(image)\n\n            # Store predictions\n            preds.append([row_id, pred])\n        else:\n            preds = pd.read_csv('..\/input\/birdsong-recognition\/sample_submission.csv')\n            break\nexcept Exception as e:\n    preds = pd.read_csv('..\/input\/birdsong-recognition\/sample_submission.csv')\n    print(f'[Reason] {e}')\n    \n# Convert to dataframe\npred_df = pd.DataFrame(preds, columns = ['row_id', 'birds'])","5305422d":"pred_df.head()","3f62396d":"pred_df.fillna('nocall', inplace = True) # fill the columns with nocall that are empty\npred_df.to_csv('submission.csv', index = False)","0dacd565":"## Took help from this awesome notebook [https:\/\/www.kaggle.com\/dipta007\/birdsong-cnn-pytorch](http:\/\/) for learning purpose.","4e03fad8":"## STEP 1: Load images and tranform them","bf600ffa":"## STEP 2: Encode the classes","197c09b1":"## STEP 5: Get the Batch Data","ef068b7c":"## Import all the libraries","1a2b779d":"# test_audio is only available while you submit the code","0abc0d20":"## STEP 3: Create a new csv file named `train_test_data.csv`","f02ddb43":"# Finally submit to the competition","0b571248":"# Write to csv file for submission\n","b46bb923":"## Training","ed490178":"## Data Preprocessing\n\n1. Load images and and use transformers from torchvision.\n2. Encode the class names from 0 to `number_of_classes`\n3. Create a new csv file named `train_test_data.csv` which will have 2 rows - `target` and `filepath`\n4. Split the dataset into train and test with `90:10` ratio\n5. Finally write a data generator function to get the batch_size of data","9c8eca53":"## Testing","e4d4990b":"## Define the Model","1ef28038":"## STEP 4: Split the dataset into train and test","733cabc7":"## Show first 5 predicitons","d1aa8252":"## Declare the dataset path"}}