{"cell_type":{"62dd3abd":"code","6bb0a103":"code","36507492":"code","85662812":"code","d75ebf48":"code","803103a3":"code","c3e03f25":"code","7e6949d2":"code","ad1465f7":"code","6b1b3f48":"code","7494fd61":"code","d18b4148":"code","cda59a42":"code","46a1d912":"code","3b2a75e4":"code","3dc80c7b":"code","757f7952":"markdown","cfab5133":"markdown","2b010b7a":"markdown","863b4389":"markdown","0c3e17c4":"markdown"},"source":{"62dd3abd":"import numpy as np \nimport pandas as pd \nimport datetime \nimport pickle\nimport os\nimport time\nimport sys\nfrom collections import OrderedDict\nfrom scipy.stats import gamma, lognorm\nimport textwrap\nfrom datetime import timedelta\nfrom io import StringIO\n\nimport pymc3 as pm\nfrom scipy.linalg import toeplitz\nimport theano.tensor as tt\n\n%matplotlib inline\nimport matplotlib.pyplot as plt  \nimport seaborn as sns","6bb0a103":"########################################\n###        HELPER FUNCTIONS      #######\n########################################\ndef simple_smoother( s, window_size = 5):\n    \n    neg_idxs = [ i for i in range(len(s)) if s.iloc[i] < 0 ]\n    if not neg_idxs:\n        return s, False\n    else:\n        for i in neg_idxs:\n            var_later = np.std(s.iloc[i+1:i+1+window_size].values )\/ np.mean(s.iloc[i+1:i+1+window_size].values )\n            var_earlier = np.std(s.iloc[i-window_size:i].values )\/  np.mean(s.iloc[i-window_size:i].values )\n            if var_later > var_earlier:  \n                interval = (i, i+window_size+1) \n                tot = s.iloc[interval[0]: interval[1]].sum()\n                s.iloc[i] = tot\/ (window_size + 1)\n                s.iloc[interval[0]+1:interval[1]] =(window_size\/(window_size+1))*tot*( \n                                                        s.iloc[interval[0]+1:interval[1]]\/(s.iloc[interval[0]+1:interval[1]].sum() ) )\n            else:\n                interval = (i - window_size, i+1 )\n                tot = s.iloc[interval[0]: interval[1]].sum()\n                s.iloc[i] = tot\/ (window_size + 1)\n                s.iloc[interval[0]:interval[1]-1] =(window_size\/(window_size+1))*tot*( \n                                                        s.iloc[interval[0]:interval[1]-1]\/(s.iloc[interval[0]:interval[1]-1].sum() ) )\n        return s.astype(int), True\n    \ndef load_JHU_deaths(f_i = None, agg_by_state = True):\n    \"\"\"Fetch cumulative deaths from JHUs Covid github with optional aggregation by state. Note: this is not number of deaths occuring on each day\n    \n    Keyword Arguments:\n        f_i {[type]} -- [description] (default: {None})\n        agg_by_state {bool} -- [description] (default: {True})\n    \n    Returns:\n        [type] -- [description]\n    \"\"\"\n    \n    if f_i is None:\n        f_i = \"https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_covid19_deaths_US.csv\"\n    df = pd.read_csv(f_i)\n    #df = df.loc[df.iloc[:,5] != \"Unassigned\", : ].copy()\n    cols = [6]\n    cols.extend(np.arange(12, df.shape[1]))\n    if agg_by_state:\n        df = df.groupby(\"Province_State\").apply(lambda x : x.iloc[:, cols[1:]].sum(axis = 0))\n    return df\n\ndef plot_shaded(data, figsize = (12,5), **kwargs ):\n    \"\"\"\n    data - df -- index is x values, \n                 columns is [mean, upper ,lower]. If columns is multilevel level 0 stores names of different trances\n                        level 1 stores mean, upper, lower\n                 data is y values\n    \"\"\"\n    \n    fig, ax = plt.subplots(figsize = figsize)\n    \n    if isinstance(data.columns, pd.MultiIndex):\n        for l in data.columns.levels[0]:\n            data.loc[: , (l, \"mean\")].plot(ax = ax, label = l, marker= 'o')\n            x_data = ax.get_lines()[-1].get_xdata()\n            ax.fill_between(x_data,  \n                            data.loc[: , (l, \"lower\")].values ,\n                            data.loc[: , (l, \"upper\")].values ,\n                           **kwargs)\n    ax.legend()\n    return fig\n\ndef make_infect_to_death_pmf( params_infect_to_symp = (1.62, 0.42),  params_symp_to_death = (18.8,  0.45), \n                             distrib_infect_to_symp = \"lognorm\" ,  distrib_symp_to_death = \"gamma\"):\n    \"\"\"Construct numpy array representing pmf of distribution of days from infection to death. \n\n    Keyword Arguments:\n        params_infect_to_symp {tuple} -- [description] (default: {(1.62, 0.42)})\n        params_symp_to_death {tuple} -- [description] (default: {(18.8,  0.45)})\n        distrib_infect_to_symp {str} -- [description] (default: {\"lognorm\"})\n        distrib_symp_to_death {str} -- [description] (default: {\"gamma\"})\n\n    Returns:\n        [type] -- [description]\n    \"\"\"\n    def make_gamma_pmf(mean, cv, truncate = 4):\n        rv = gamma(a = 1.\/(cv)**2, scale = mean*(cv)**2)\n        lower = 0 \n        upper = int(np.ceil( mean + cv*mean*truncate) )\n        cdf_samples = [rv.cdf(x) for x in range(lower, upper + 1, 1)]\n        pmf = np.diff(cdf_samples)  \n        pmf  = pmf \/ pmf.sum() ## normalize following truncation\n        return pmf\n        \n    def make_lognorm_pmf( log_mean, log_sd, truncate=4):\n        rv = lognorm( s = log_sd, scale = np.exp(log_mean),  )\n        lower = 0 \n        mean, var = rv.stats(moments = 'mv')\n        upper = int(np.ceil( mean + np.sqrt(var)*truncate) )\n        cdf_samples = [rv.cdf(x) for x in range(lower, upper + 1, 1)]\n        pmf = np.diff(cdf_samples)  \n        pmf  = pmf \/ pmf.sum() ## normalize following truncation\n        return pmf\n        \n    if distrib_infect_to_symp.lower() == \"lognorm\":\n        pmf_i_to_s = make_lognorm_pmf(*params_infect_to_symp)\n    else:\n        raise NotImplementedError()\n    if  distrib_symp_to_death.lower() == \"gamma\":\n        pmf_s_to_d = make_gamma_pmf(*params_symp_to_death)\n    else:\n        raise NotImplementedError()\n    \n    pmf_i_to_d =  np.convolve(pmf_i_to_s, pmf_s_to_d )\n    ## truncate and renormalize\n    cdf = np.cumsum(pmf_i_to_d)\n    min_x = np.argmax( cdf >= 0.005  )  \n    max_x = len(cdf) - np.argmax( (1. - cdf[::-1]) >= 0.005 ) \n    pmf_i_to_d[:min_x] = 0.\n    pmf_i_to_d = pmf_i_to_d[: max_x]\/ np.sum(pmf_i_to_d[: max_x])\n    \n    return pmf_i_to_d","36507492":"#####################################\n###         CLASSES      ############\n#####################################\nclass Infection_Series_Estimator(object):\n    \"\"\"\n    Properties:\n            P_e_given_i - (2d array) P_e_given_i[e,i] gives probability of event on day e given infection on day i.\n            T_period - (2 tuple) \u2014 (min_days_elapsed_from_infect_to_transmission, max_days_elapsed_from_infect_to_transmission)\n                                       (lower end inclusive, upper end exclusive)\n            mu \u2014 exponential prefactor in prior\n            inputs \u2014 dictionary of inputs\n            D - Matrix for calculating 1d discrete difference in log(tranmission_rataes)        \n    \"\"\"\n    def __init__(self, event_series, P_e_given_i, T_period, policy_dates):\n        \"\"\" \n        Inputs:\n            event_series \u2014 pd.Series - index is dates values are counts\n            P_e_given_i - 1d array repreenting pmf for event occuring k = (e-i) days after infection\n                        \n            T_period - (2 tuple) \u2014 (min_days_elapsed_from_infect_to_transmission, max_days_elapsed_from_infect_to_transmission)\n                                       (lower end inclusive, upper end exclusive)\n            rms_foldchange \u2014 root mean square log2 ratio of tranmission rates on sequential dates. Determines strength of pior\n        \"\"\"\n        self.init_kwargs = { \"event_series\" :event_series,\n                        \"P_e_given_i\": P_e_given_i,\n                       \"T_period\" : T_period,\n                       \"policy_dates\" : policy_dates}\n        ## counts and date info\n        self.event_series = self._preprocess_event_series(event_series, P_e_given_i)\n        date_max = self.event_series.index[-1]\n        date_min = self.event_series.index[0]\n        self.days_total = (date_max - date_min).days + 1\n        \n        if T_period[0] != 1:\n            raise ValueError(\"Code only supports models with transmission period beginning 1 d after infection\")\n        self.T_period = T_period\n        \n        policy_dates_processed = self._preprocess_policy_dates(policy_dates)\n        self.policy_dates = [ ( (x[0]-date_min).days, (x[1]-date_min).days) for x in  policy_dates_processed ]\n        \n    @staticmethod\n    def _preprocess_event_series(event_series, P_e_given_i_1d):\n        \"\"\"\n        Insures event series is:\n            - sorted by increasing date\n            - has timestamp index\n            - starts at at least len(P_e_given_i_1d) -1 before first day with event_series >0\n        \n        Returns\n             event_series\n        \"\"\"\n        ## Sort and check index type\n        event_series = event_series.sort_index().copy()\n        if not isinstance(event_series.index, pd.core.indexes.datetimes.DatetimeIndex):\n            event_series.index = pd.to_datetime(event_series.index)\n        ## Truncate or extend index\n        earliest_nonzero = (event_series > 0).idxmax() \n        date_min = earliest_nonzero - pd.Timedelta(len(P_e_given_i_1d)-1, unit = \"days\")                  \n        if date_min < event_series.index[0]:\n            prepend_series =  pd.Series(index = pd.date_range(start = date_min,\n                                                              end =event_series.index[0],\n                                                               freq = 'D', closed = 'left' ),\n                                        data = 0)     \n            event_series = pd.concat( [prepend_series, event_series ] )    \n        else:\n            event_series = event_series.loc[date_min:].copy()\n        return event_series\n    \n    @staticmethod\n    def _preprocess_policy_dates(policy_dates):\n        out = []\n        for elem in policy_dates:\n            if isinstance(elem, str):\n                out.append( ( pd.Timestamp(elem), pd.Timestamp(elem) + pd.Timedelta(1, unit = \"D\") ) )\n            elif isinstance(elem, tuple) or isinstance(elem, list):\n                if len(elem) == 1:\n                    out.append( ( pd.Timestamp(elem[0]), pd.Timestamp(elem[0]) + pd.Timedelta(1, unit = \"D\") ) )\n                elif len(elem) == 2:\n                    out.append( tuple( pd.Timestamp(x) for x in elem ) )\n                else:\n                    raise ValueError(\"At least one of the entries of policy_dates has length > 2!\")\n            else:\n                raise ValueError(\"Could not parse entry {} of policy_dates\".format(elem ))\n        return out\n    \n    @property\n    def P_e_given_i(self):\n        P_e_given_i = self._make_P_e_given_i(self.init_kwargs[\"P_e_given_i\"], self.days_total, observed_only = True)\n        return  P_e_given_i\n    @property\n    def M1(self):\n        return None\n    @property\n    def M2(self):\n        return None\n    @property\n    def D(self):\n        return None\n    \n    @staticmethod\n    def _make_P_e_given_i(P_e_given_i, days_total, observed_only = True):\n        \"\"\"\n        P_e_given_i - 1d numpy array\n        \"\"\"\n        if observed_only:\n            P_e_given_i = toeplitz(c = np.concatenate([ P_e_given_i, np.zeros(days_total-len(P_e_given_i)) ] ),\n                                   r =  np.concatenate([ P_e_given_i[0:1], np.zeros(days_total- 1) ] ), \n                                  )\n        else:\n            P_e_given_i = toeplitz(c = np.concatenate([ P_e_given_i, np.zeros(days_total-1) ] ),\n                                   r =  np.concatenate([ P_e_given_i[0:1], np.zeros(days_total- 1) ] ), \n                                  )\n        return P_e_given_i \n  \n    @staticmethod\n    def _make_pior_mats(T_period, days_total):\n        \"\"\"\n        \n        \"\"\"\n        M1 =  toeplitz( c = np.zeros( days_total-(T_period[1]-T_period[0]) ) ,\n                        r = np.concatenate([ np.zeros(T_period[1]-T_period[0]),\n                                             np.array([1.]) , \n                                             np.zeros( days_total - 1 - (T_period[1]-T_period[0]) ), \n                                           ])\n                      )\n        M2 =  toeplitz( c = np.concatenate([ np.array([1.]), np.zeros( days_total-(T_period[1]-T_period[0]) -1 ) ] ) ,\n                        r = np.concatenate([ np.ones(T_period[1]-T_period[0]),\n                                            np.zeros(days_total - (T_period[1]-T_period[0])) ] )\n                      )\n        assert M1.shape == M2.shape\n        D =  toeplitz( c = np.concatenate([ np.array([-1.]), np.zeros(M1.shape[0]-2)  ]),\n                       r = np.concatenate([ np.array([-1., 1]), np.zeros(M1.shape[0]-2) ] )\n                     ) \n        return M1, M2, D\n    \n    def fit(self, mu, policy_factor, test_val = None,  **kwargs):\n        \n        P_e_given_i = self._make_P_e_given_i(self.init_kwargs[\"P_e_given_i\"], self.days_total)\n        M1, M2, D = self._make_pior_mats(self.T_period, self.days_total)\n        if test_val is None:\n            test_val = np.ones(len(self.event_series))\/len(self.event_series)\n        \n        p_i_samples, self.posterior = self._sample_posterior( counts_arr = self.event_series.values.copy() , \n                                                              P_e_given_i = P_e_given_i, \n                                                              M1 = M1,\n                                                              M2 = M2, \n                                                              D = D, \n                                                              mu = mu, \n                                                              days_total = self.days_total, \n                                                              test_val = test_val,\n                                                              policy_dates = self.policy_dates, \n                                                              policy_factor = policy_factor, \n                                                             **kwargs)\n        self.p_i_samples = pd.DataFrame( index = self.event_series.index ,\n                                         data =  p_i_samples.transpose() )\n        self.p_i = self._samples_to_cred_interval( self.p_i_samples,  sample_axis =1  )\n        \n        ## caculate R_e\n        self.Re_samples = self._est_Re(self.p_i_samples, self.T_period )\n        self.Re = self._samples_to_cred_interval(self.Re_samples ,  sample_axis =1  )\n    \n        return  self.p_i.copy(), self.Re.copy(), self.p_i_samples.copy(), self.Re_samples.copy() \n                                 \n    @staticmethod\n    def _sample_posterior(counts_arr, P_e_given_i, M1, M2, D, mu, days_total, test_val,\n                          policy_dates, policy_factor, **kwargs):\n        \n        n = int(counts_arr.sum())\n        ## multiply mu by policy_factor to allow more change on policy_dates\n        mu_hard = mu*np.ones(D.shape[0])\n        mu_soft = mu_hard.copy()\n        policy_day_shift =  np.argmax( M1[0,:]) + 1\n        for start, stop in policy_dates:\n            mu_soft[ start-policy_day_shift :stop-policy_day_shift] *= policy_factor\n        \n        with pm.Model() as model:\n            p_i = Infect_Series_Prior(\"p_i\",\n                                       mu_soft=mu_soft ,\n                                       mu_hard = mu_hard, \n                                       M1=M1, \n                                       M2 =M2, \n                                       D= D, \n                                       G = P_e_given_i,\n                                       N =n,\n                                       testval = test_val, \n                                       shape= days_total,\n                                       transform=pm.distributions.transforms.stick_breaking)\n            p_e = tt.dot(P_e_given_i, p_i ) \n            p_e_given_observed =  p_e\/ tt.sum(p_e)\n            N = pm.Multinomial(\"N\", n=n, p=p_e_given_observed, observed= counts_arr)\n               \n            posterior = pm.sample(cores= 1, **kwargs) \n            \n        return posterior[\"p_i\"].copy(), posterior\n    \n    @staticmethod\n    def _est_Re(p_i_samples, T_period,  min_infected_frac= 0.0001 ):\n        \"\"\"\n        p_i_samples  - DataFrame with \n                                values \u2014 proportional to number of infected people \n                                index \u2014 dates, \n                                columns \u2014 posterior samples\n        T_period \u2014 2-tuple\n        min_infected_frac \u2014 float                 \n        trans_period  - 2-tuple representing left end open right end closed interval\n        \"\"\"\n        Re_idx_min =  max(T_period[1]-1, np.argmax(p_i_samples.mean(axis = 1).values > min_infected_frac) )\n        Re_samples = pd.DataFrame(index = p_i_samples.index[Re_idx_min:].copy(), \n                                 columns = p_i_samples.columns.copy(),\n                                 data = 0. )\n        for idx in range(Re_idx_min, p_i_samples.shape[0]):\n            Re_samples.iloc[idx - Re_idx_min, :] = (p_i_samples.iloc[idx, :]\n                                                    )\/( (p_i_samples.iloc[idx - T_period[1]+1 : idx - T_period[0] + 1, :]\n                                                                ).sum(axis = 0) \n                                                       )\n        Re_samples= Re_samples*(T_period[1] - T_period[0])\n        return Re_samples\n    \n    \n    def predict_p_e(self,):\n        \n        P_e_given_i = Infection_Series_Estimator._make_P_e_given_i(self.init_kwargs[\"P_e_given_i\"], self.days_total, observed_only = False )\n        p_e_samples = P_e_given_i@(self.p_i_samples.values)\n        \n        p_e_samples = pd.DataFrame(data = p_e_samples, columns = self.p_i_samples.columns)\n        p_e_index_observed = self.p_i_samples.index.copy()\n        p_e_index_unobserved = p_e_index_observed[-1] + pd.timedelta_range( start = pd.Timedelta(days = 1), \n                                                                             end =   pd.Timedelta(days = len(p_e_samples) - len(p_e_index_observed) ),\n                                                                              freq = 'd' )\n        p_e_index = p_e_index_observed.append(p_e_index_unobserved)          \n        p_e_samples.index= p_e_index \n        p_e = self._samples_to_cred_interval(p_e_samples ,  sample_axis =1  )\n        \n        ## Expected number of events per day\n        n_observed = self.event_series.sum()\n        n_e_samples = p_e_samples.apply(\n                            lambda x: x*n_observed*np.concatenate([ np.ones(len(p_e_index_observed))\/x.loc[p_e_index_observed].sum(),\n                                                                    (1.\/x.loc[p_e_index_observed].sum()\n                                                                        )*np.ones(len(p_e_index_unobserved))\n                                                              ] ),\n                            axis = 0)\n        n_e = self._samples_to_cred_interval(n_e_samples, sample_axis =1  )\n                 \n        return p_e,  p_e_samples, n_e, n_e_samples\n    \n    @staticmethod\n    def _samples_to_cred_interval(samples , sample_axis =1 ):\n\n        if isinstance(samples, pd.Series):\n            out = pd.Series( [ samples.mean(), samples.quantile(0.025), samples.quantile(0.975)]  , index = [\"mean\" , \"lower\" , \"upper\"] )\n        elif isinstance(samples, pd.DataFrame ):\n            out = pd.concat([ samples.mean(axis = sample_axis),\n                            samples.quantile(0.025, axis = sample_axis), \n                            samples.quantile(0.975, axis = sample_axis)\n                            ],\n                            axis = sample_axis, keys = [\"mean\" , \"lower\", \"upper\"] )\n        else:\n            raise ValueError()\n        return out\n    \nclass Infect_Series_Prior( pm.Continuous):\n    def __init__(self, mu_soft, mu_hard, M1, M2, D, G, N, *args, **kwargs):\n        super(Infect_Series_Prior, self).__init__(*args, **kwargs)\n        \n        self.mu_soft = mu_soft\n        self.mu_hard = mu_hard\n        self.M1 = M1\n        self.M2 = M2\n        self.D = D\n        self.G = G\n        self.N = N\n        ## define mask used in jacobian for transmission prior\n        \n    def logp(self, value):\n        mu_soft = self.mu_soft\n        mu_hard = self.mu_hard\n        M1 = self.M1\n        M2 = self.M2\n        D = self.D\n        G = self.G\n        N = self.N\n        \n        diff = tt.dot(D, tt.log(tt.dot(M1,value)) - tt.log(tt.dot(M2,value)))\n        t_out = -1.*tt.sum( tt.switch(tt.lt(diff, 0.), mu_soft, mu_hard )*diff**2  )             \n        return t_out\n    \nclass Policy_Stats(object):\n    \"\"\"Class for relating changes in policy to changes in statictics of samples from the posterior\n    distribution that are calcuatled over time intervals before and after the change. \n    \"\"\"\n    def __init__(self, policy_dates, max_before =7, max_after = 7):\n        \"\"\"\n        policies \u2014 DataFrame - index : policy names\n                               columns : states\/provinces\n        \"\"\"\n        self.max_before = max_before\n        self.max_after = max_after\n        self._policy_dates = Policy_Stats._preprocess_policy_dates(policy_dates)\n        \n        self._compare_intervals = self._policy_dates.groupby(level = 0).apply(\n                                                                lambda x: Policy_Stats._make_compare_intervals(x[x.name]) \n                                                                        )\n        self.change_samples = None\n        self.change_interval = None\n    @property\n    def policy_dates(self):\n        \"\"\"\n        pd.Series Multiindex with level0 \u2014 state and level1 \u2014 policy name \n                  Values are dates\n        \"\"\"\n        return self._policy_dates.copy()\n    @property\n    def compare_intervals(self):\n        \"\"\"\n        pd.DataFrame: index - level 0 - state; level 1 - date\n                      columns before_start, before_end, after_start, after_end, policy\n                      values - TimeStamps or string\n        \"\"\"\n        return self._compare_intervals.copy()\n    \n    @staticmethod  \n    def _preprocess_policy_dates(policy_dates):\n        processed = policy_dates.applymap(lambda x: pd.Timestamp(x)\n                                         ).dropna(axis =1, how = \"all\")\n        processed = processed.transpose().stack()\n        return  processed \n    \n    @staticmethod  \n    def _make_compare_intervals( p_series, max_before =7, max_after = 7, truncate_before = False ):\n        \"\"\"\n        Construct time-intervals before and after implementation of a policy, considering the dates at\n        which other policies are implemented.\n        Inputs\n        ------\n            p_series \u2014 pd.Series: index - policies,\n                                 values - dates implemented\n            max_before - int: the number of days before implementation of policy to include\n            max_after - int: the number of days after implementation policy (including the day implementation) to include\n            truncate_before \u2014 If the \"before\" interval overlaps the implementation of a previous policy do we trucate \n                            to only the later part of the interval? \n        Returns\n        -------\n            compare_intervals- index - level 0 - state; level 1 - date\n                                  columns before_start, before_end, after_start, after_end, policy\n                                values - TimeStamps or string\n        \"\"\"\n        \n        p_series = p_series.squeeze().sort_values().dropna()\n        dates_unique = [pd.Timestamp(x) for x in sorted(p_series.unique())]\n\n        date_to_p = OrderedDict([])\n        for i, d in enumerate(dates_unique):\n            if not date_to_p:  ## the first date\n                before_start = d + timedelta(days=-max_before) \n                before_end = d \n                after_start = d\n                if i + 1 < len(dates_unique): ## there is another entry\n                    after_end = min( dates_unique[i+1], d + timedelta(days=max_after) )\n                else:\n                    after_end = d + timedelta(days=max_after) \n            else:\n                if truncate_before:\n                    before_start = max( d+timedelta(days=-max_before), dates_unique[i-1] + timedelta(days=1) )\n                else:\n                    before_start = d + timedelta(days=-max_before)\n                before_end = d \n                after_start = d\n                if i + 1 < len(dates_unique): ## there is another entry\n                    after_end = min( dates_unique[i+1], d + timedelta(days=max_after) )\n                else:\n                    after_end = d + timedelta(days=max_after) \n            date_to_p[d] = [ before_start, before_end, after_start, after_end ] \n\n        compare_intervals = pd.DataFrame.from_dict( date_to_p,\n                                                   orient = \"index\", \n                                                   columns = [ \"before_start\", \"before_end\", \"after_start\", \"after_end\"])\n        ## Add policies for each date\n        compare_intervals[\"policy\"] = \"\"\n        for p, d in p_series.iteritems():\n            if compare_intervals.loc[d, \"policy\"]:\n                compare_intervals.loc[d, \"policy\"] = compare_intervals.loc[d, \"policy\"] + \"_AND_\" + p\n            else:\n                compare_intervals.loc[d, \"policy\"] = p            \n        return compare_intervals\n    \n    def est_pct_change(self, samples_dict, policies):\n        \"\"\"\n        Inputs\n        ------\n            samples_dict \u2014 dictionary: keys -states\n                                       values - dataframe with dates as index and samples as columns\n            policies -  None or list of strings appearing in self.policy_dates index, level1\n        Returns:\n            pct_change_interval \u2014 pd.DataFrame: index: level0 state; level1 policy,\n                                           columns: mean, lower, upper\n                                           values: percent change\n            pct_samples \u2014 pd.DataFrame: index: level0 state; level1 policy,\n                                           columns: samples index\n                                           values: percent change\n        \"\"\"\n        c_intervals = self.compare_intervals.copy()\n        change_func =  lambda before, after: (after.mean(axis = 0) - before.mean(axis = 0)).divide(before.mean(axis = 0))*100.\n        self.change_samples  = self._est_change(samples_dict, c_intervals, change_func = change_func,  policies = policies)\n        ## Get credible interval\n        self.change_interval= self._samples_to_cred_interval( self.change_samples , sample_axis = 1)\n        return self.change_interval.copy(), self.change_samples.copy()\n\n    @staticmethod\n    def _est_change(samples_dict, c_intervals, change_func,  policies = None):\n        \"\"\"\n        \n        Inputs\n        ------\n            samples_dict \u2014 dictionary: keys -states\n                                       values - dataframe with dates as index and samples as columns\n            c_intervals - same as self.compare_intervals\n            change_func - callable: arguments: before (pd.DataFrame), after (pd.DataFrame)\n                                            where each columns is a sample and index is dates from before or after interval\n                                    returns: series where index is samples and values are the change statisitic for the sample  \n            policies -  None or list of strings appearing in self.policy_dates index, level1\n        Returns\n        -------\n            change_samples \u2014 pd.DataFrame: index: level0 state; level1 policy,\n                                           columns: samples index\n                                           values: change statisitc\n        \"\"\"\n        samples_dict= {s: df for s, df in samples_dict.items() if s in c_intervals.index.levels[0]} ## filter to states with policy data\n        results_dict = {}\n        for state, samples in samples_dict.items():\n            ## Get dates and names of policies\n            if policies is None:\n                dates_interest =  list( c_intervals.loc[state,:].index )\n            else:\n                dates_interest = [d for d, row in c_intervals.loc[state,: ].iterrows() \n                                      if set(policies).intersection(row[\"policy\"].split(\"_AND_\"))  ]\n            policies_interest =  list(c_intervals.loc[ [(state, d) for d in dates_interest] , \"policy\"])\n            changes_by_sample = pd.DataFrame( index =  policies_interest, columns = samples.columns, data = 0. )\n            ### Compute change at each policy\/date\n            for d, p in zip(dates_interest, policies_interest):\n                before_start, before_end, after_start, after_end = c_intervals.loc[(state, d), \n                                                                         [\"before_start\", \"before_end\", \"after_start\", \"after_end\"]]\n                changes_by_sample.loc[p, :] = change_func( samples.loc[ before_start:before_end , :],  \n                                                    samples.loc[ after_start: after_end , :]).values\n            ## Update with results for state\n            results_dict[state] = changes_by_sample\n        ## Combine to single dataframe\n        keys_tmp = list(results_dict.keys())\n        change_samples = pd.concat( [results_dict[k] for k in keys_tmp] , axis = 0,  keys = keys_tmp)\n        ## Add policy_date to index\n        change_samples[\"policy_date\"] = pd.NaT\n        for (state,date), row in c_intervals.iterrows():\n             change_samples.loc[ (state, row[\"policy\"]), \"policy_date\"] = date\n        change_samples = change_samples.set_index( \"policy_date\" , append = True)\n        change_samples.index.names = [\"state\/province\" , \"policy\" , \"policy_date\"]\n        return change_samples \n\n    def boxplot_changes_by_state(self, states = None, policies = None, change_samples =None ,\n                                aspect= 3, height = 6, hspace = 1.):\n        ####### Process inputs\n        if change_samples is None:\n            if self.change_samples is None:\n                raise Exception(\"change_samples not set eiter provide as method argument or run est_pct_change\")\n            else:\n                change_samples = self.change_samples\n        ## states\n        if states is None:\n            states = list(change_samples.index.levels[0])\n        else:\n            assert all([s in change_samples.index.levels[0] for s in states]),\"No data for some states in provided list\" \n            change_samples = change_samples.loc[states, : ].copy()\n        if policies is not None:\n            mask = [ len(set(i[1].split(\"_AND_\")).intersection(policies)) > 0 for i in change_samples.index ]\n            change_samples_mask = change_samples_mask.loc[mask, :].copy()\n        ####### Helper functions\n        def format_xlab(s, width = 12):\n            s = \"\\nAND\\n\".join([textwrap.fill(x, width = width) for x in s.split(\"_AND_\")])\n            return s\n        def plot_func(data, y, color):\n            policies_and_dates = data[[\"policy\", \"policy_date\"]].drop_duplicates().sort_values(by = \"policy_date\").reset_index(drop =True)\n            ax = sns.boxplot(data = data, x = \"policy\" , y = y, order = list(policies_and_dates[\"policy\"].values),  whis=[5, 95])\n            ax.set_xticklabels([format_xlab(t, width = 20) for t in policies_and_dates[\"policy\"].values] , rotation = 90)\n            return None\n        ####### PLOT\n        change_samples = change_samples.transpose().melt()\n        g = sns.FacetGrid(change_samples, row=\"state\/province\", aspect= aspect, height = height,sharex=False, )\n        g.map_dataframe(plot_func, y = \"value\")\n        for ax in np.ravel(g.axes):\n            ax.set_ylabel(\"Percent change in \" + r'$R_e$')\n            ax.set_ylim(None, 50)\n            ax.grid(True)\n        g.fig.subplots_adjust(hspace=hspace)\n        return g.fig\n    \n    def boxplot_changes_by_policy(self, states = None, policies = None,  change_samples = None,\n                                  y_label = \"Percent change in \" + r'$R_e$', aspect = 4, height = 6,y_lim = (-100, 100),\n                                 hspace = 3.2 ):\n        ####### Process inputs\n        if change_samples is None:\n            if self.change_samples is None:\n                raise Exception(\"change_samples not set eiter provide as method argument or run est_pct_change\")\n            else:\n                change_samples = self.change_samples\n        if states is None:\n            states = list(change_samples.index.levels[0])\n        else:\n            assert all([s in change_samples.index.levels[0] for s in states]),\"No data for some states in provided list\" \n            change_samples = change_samples.loc[states, : ].copy()\n        if policies is None:\n            policies = list(np.unique( [ x for y in change_samples.index.levels[1] for x in y.split(\"_AND_\")] ) )\n        ####### Helper functions\n        def format_xlab(s, width = 30):\n            s = \"\\nAND\\n\".join([textwrap.fill(x, width = width, initial_indent = '   ', subsequent_indent= '   '\n                                              ) if i>0 else textwrap.fill(x, width = width\n                                                                             ) for i,x in enumerate(s.split(\"_AND_\")) ])\n            return  s\n        ####### PLOT\n        nrows = len(policies)\n        fig, axes = plt.subplots(figsize = (height*aspect, height*nrows), nrows = nrows , ncols = 1 )\n        axes = np.ravel(axes)\n        for p, ax in zip(policies, axes):\n            mask = [ len(set(i[1].split(\"_AND_\")).intersection([p])) > 0 for i in change_samples.index ]\n            plot_data = change_samples.loc[mask, :].reset_index()\n            plot_data[\"state\/province_and_policy\"] =  plot_data.apply(lambda x: x[\"state\/province\"] +\" : \" + x[\"policy\"], axis = 1)\n            plot_data = plot_data.drop(columns = [\"state\/province\" , \"policy\" , \"policy_date\"])\n            plot_data = plot_data.set_index(\"state\/province_and_policy\", verify_integrity=True)\n            plot_data = plot_data.transpose().melt()\n            ax = sns.boxplot(data = plot_data , x =  \"state\/province_and_policy\" , y = \"value\", ax= ax, whis=[5, 95])\n            ax.set_xticklabels( [format_xlab(x.get_text()) for x in  ax.get_xticklabels()], rotation = 90)\n            ax.set_ylim( y_lim )\n            ax.set_ylabel(y_label)\n            ax.grid(True)\n            ax.set_title(p)\n        fig.subplots_adjust(hspace = hspace)\n        return fig\n    \n    @staticmethod\n    def _samples_to_cred_interval(samples, sample_axis = 1):\n        \"\"\"\n        Construct a symmetric 95% credible interval (CI) (TODO: HDI  interval)\n        Inputs\n        ------\n            samples \u2014 pd.DataFrame or pd.Series where one axis stores samples that are to be aggregated to CI\n            samples_axis- int \n        \"\"\"\n        if isinstance(samples, pd.Series):\n            out = pd.Series( [ samples.mean(), samples.quantile(0.025), samples.quantile(0.975)]  , index = [\"mean\" , \"lower\" , \"upper\"] )\n        elif isinstance(samples, pd.DataFrame ):\n            out = pd.concat([ samples.mean(axis = sample_axis),\n                            samples.quantile(0.025, axis = sample_axis), \n                            samples.quantile(0.975, axis = sample_axis)\n                            ],\n                            axis = sample_axis, keys = [\"mean\" , \"lower\", \"upper\"] )\n        else:\n            raise ValueError()\n        return out\n","85662812":"plt.plot(make_infect_to_death_pmf())\n_=plt.xlabel(\"days\")\n_= plt.ylabel(\"g(s-t)\")","d75ebf48":"########################################\n###        GLOBALS               #######\n########################################\n\npolicies_string= StringIO(\"\"\"\nPolicy,New York,New Jersey,Michigan,Louisiana,Massachusetts,Illinois,Connecticut,California,Pennsylvania,Florida,Georgia,Washington\nEmergency Declaration,3\/7\/20,3\/9\/20,3\/10\/20,3\/11\/20,3\/10\/20,3\/9\/20,3\/10\/20,3\/4\/20,3\/6\/20,,,\nBar\/Restaurant Limits,3\/16\/20,3\/16\/20,3\/16\/20,3\/17\/20,3\/15\/20,3\/17\/20,3\/17\/20,3\/16\/20,3\/16\/20,,,\nSchool Closure,3\/18\/20,3\/18\/20,3\/16\/20,3\/16\/20,3\/15\/20,3\/17\/20,3\/17\/20,3\/15\/20,3\/13\/20,,,\nStay At Home Order,3\/20\/20,3\/21\/20,3\/24\/20,3\/22\/20,3\/24\/20,3\/20\/20,3\/23\/20,3\/19\/20,4\/1\/20,,,\nNon-essential Business Closures,3\/22\/20,3\/21\/20,3\/24\/20,3\/22\/20,3\/24\/20,3\/20\/20,3\/24\/20,3\/19\/20,3\/22\/20,,,\nGatherings Ban 500 Or Stricter,3\/12\/20,,,,,,3\/12\/20,,,,,\nGatherings Ban 50 Or Stricter,3\/16\/20,3\/16\/20,3\/17\/20,3\/17\/20,3\/15\/20,3\/18\/20,3\/16\/20,,,,,\nGatherings Ban 10 Or Any Size,3\/20\/20,3\/21\/20,3\/24\/20,3\/22\/20,3\/23\/20,3\/20\/20,3\/26\/20,3\/16\/20,,,,\n\"\"\")\npolicies = pd.read_csv(policies_string, index_col = 0)\nJHU_deaths = load_JHU_deaths()\n## MODEL PARAMETERS\nT_period = (1,8)\ndraws =  500\nchains = 4\nmu = 13.\npolicy_factor = 0.01\n\nprint(\"GLOBALS\")\nprint(\"\\t Policy dates (manually currated)\")\ndisplay(policies)\nprint(\"\\t JHU covid death data (aggregated by US state)\")\ndisplay(JHU_deaths.head())","803103a3":"########################################################################\n#### SHOW CUMULATIVE DEADTH FOR 10 US STATES WITH HIGHEST TOTAL DEATHS ###\n########################################################################\nstate_top_cas = list(JHU_deaths.iloc[:,-1].sort_values()[::-1].index[0:10])\n\nax = JHU_deaths.loc[state_top_cas, : ].transpose().plot()\nleg = ax.get_legend()\nleg.set_bbox_to_anchor( (1, 1), )","c3e03f25":"#######################################################################################\n###    SMOOTHONG                                                                        ###\n#### ALTHOUGH IT APPEARS THAT THE JHU DATA STORES CUMULATIVE DEATHS (ABOVE PLOT)        ###\n#### THERE ARE A FEW INSTANCES WHERE THE CUMULATVE DEATHS ARE HAVE NAVE NEGATIVE SLOPE  ###\n#### WE HANDLE THIS BY AVERAING WITH A 7 DAY WINDO AROUND CASES WHERE DEATHS PER DAY    ###\n#### GO NEGATIVE. THE NUMBER OF DAYS AT WHICH THIS MUST BE DONE IS SMALL AND IT ONLY   ### \n#### NEEDS TO BE DONE FOR NY AND CA                                                     ###\n########################################################################\ndeaths_by_day = pd.DataFrame( index = pd.to_datetime(JHU_deaths.columns[1:]), columns = state_top_cas)\nis_smoothed = pd.Series(index =state_top_cas, data = False, dtype = bool )\n\nfor state in state_top_cas:\n    deaths_by_day_state =  JHU_deaths.loc[state,:].diff().dropna()\n    deaths_by_day_state_smooth, smoothed = simple_smoother( deaths_by_day_state.copy(), window_size = 7)\n    deaths_by_day.loc[:, state] = deaths_by_day_state_smooth.values\n    is_smoothed.loc[state] = smoothed\ndisplay(is_smoothed)","7e6949d2":"################################################################################\n###  PLOTING NUMBER OF DEATHS PER DAY  \u2014 Just to get a feel for data        ###\n################################################################################\n\nfig , ax = plt.subplots(figsize= (16,6))\nax = deaths_by_day.divide(deaths_by_day.max(axis = 0), axis = 1).rolling(3,).mean().loc['2020-03-05':].plot(ax = ax)\nleg = ax.get_legend()\nleg.set_bbox_to_anchor( (1, 1), )\n_ =ax.set_ylabel(\"Deaths by day\\n(scaled by maximum)\")","ad1465f7":"######################################\n#### MODEL FITTING         ###########\n#####################################\n\n\n## Set up directories for storing ouput\ntime = datetime.datetime.isoformat(datetime.datetime.now() ).split(\".\")[0]\nmodels_dir = os.path.join( \"models\", str(time) )\npreds_dir = os.path.join( \"predictions\", str(time) )\n\nos.makedirs(models_dir , exist_ok = True)\nos.makedirs(preds_dir, exist_ok = True)\n\n## Do estimation and write results\nfor state in state_top_cas:\n    print(\"Working on {}\".format(state))\n    policy_dates =   [x for x in policies[state].dropna().unique() ]\n    ise = Infection_Series_Estimator(event_series =deaths_by_day[state].copy().astype(int), \n                                     P_e_given_i= make_infect_to_death_pmf(), \n                                     T_period = T_period,\n                                     policy_dates= policy_dates )\n    p_i, Re, p_i_samples, Re_samples= ise.fit(mu = mu,\n                                              policy_factor = policy_factor, \n                                              draws = draws ,\n                                              chains = chains )\n    \n    df_samples = pd.concat( [p_i_samples ,Re_samples], axis = 1, keys = [\"p_i\", \"Re\"] )\n    df_samples.to_csv( os.path.join(preds_dir , \"{}_samples.csv\".format(state)) )\n    df_cinterval =  pd.concat( [p_i , Re] , axis = 1, keys = [\"p_i\", \"Re\"] )\n    df_cinterval.to_csv( os.path.join(preds_dir , \"{}_intervals.csv\".format(state)) )\n    \n    f = open(os.path.join(models_dir, \"{}.pkl\".format(state)) , 'wb' )\n    pickle.dump(ise , f)\n    f.close()\n    \n##  Combine results\ndf_samples_list = [ pd.read_csv( os.path.join(preds_dir,\"{}_samples.csv\".format(state)), index_col=0 , header = [0,1] ) \n                                    for state in list(state_top_cas )  ]\ndf_samples = pd.concat(df_samples_list, axis = 1, keys = list(state_top_cas ) )\ndf_samples.index = pd.to_datetime(df_samples.index)\ndf_samples.to_csv( os.path.join(preds_dir,\"samples.csv\" ) )\nfor state in state_top_cas:\n    os.remove(os.path.join(preds_dir,\"{}_samples.csv\".format(state)) )\n    \ndf_cintervals_list = [ pd.read_csv( os.path.join(preds_dir,\"{}_intervals.csv\".format(state)), index_col=0 , header = [0,1] ) \n                                    for state in list(state_top_cas )  ]\ndf_cintervals = pd.concat(df_cintervals_list, axis = 1, keys = list(state_top_cas) )\ndf_cintervals.to_csv(os.path.join(preds_dir,\"intervals.csv\" ) )\ndf_cintervals.index = pd.to_datetime(df_cintervals.index)\nfor state in state_top_cas:\n    os.remove(os.path.join(preds_dir,\"{}_intervals.csv\".format(state)) )","6b1b3f48":"ncols = 2\nmodels_dir = models_dir\nnrows = -(-len(state_top_cas)\/\/2)\n\n\nfig , axes = plt.subplots(nrows = nrows, ncols = ncols ,figsize = (16,3*nrows))\naxes = np.ravel(axes)\n\nfor state, ax in zip(state_top_cas, axes) :\n    f = open( os.path.join(models_dir, \"{}.pkl\".format(state) ) , 'rb')\n    ise = pickle.load(f)\n    f.close()\n    (ise.predict_p_e()[2]).plot(ax = ax)\n    deaths_by_day[state].plot(marker = \"o\", ax = ax)\n    ax.set_title(state)\n    \nfig.tight_layout()","7494fd61":"## Load data\npreditions_dir = preds_dir ## Change this to match the folder created during execution of estimation cell \ndf_cintervals = pd.read_csv(os.path.join(preditions_dir, \"intervals.csv\"), header = [0,1,2], index_col = 0)","d18b4148":"fig = plot_shaded(df_cintervals.loc[ '2020-02-25': ,(slice(None), \"Re\") ].droplevel(axis = 1,level =1), alpha = 0.1, figsize=(18,6))\nax = fig.get_axes()[0]\nax.set_ylim((0 , 7) )\nax.grid(True)\nleg = ax.get_legend()\nleg.set_bbox_to_anchor((1,1))\n_ =ax.set_ylabel(r'$R_e$', fontsize = 'xx-large')","cda59a42":"fig = plot_shaded(df_cintervals.loc[ '2020-03-05': ,(slice(None), \"p_i\") ].droplevel(axis = 1,level =1),\n                                           alpha = 0.1,figsize = (16,6))\nax = fig.get_axes()[0]\nax.set_ylim((0 , 0.15) )\nax.grid(True)\nleg = ax.get_legend()\nleg.set_bbox_to_anchor((1,1))\n_=ax.set_ylabel('Fraction infected on each day', fontsize = 'xx-large')","46a1d912":"## File names\npreditions_dir = preds_dir \n\n## Load Data\ndf_samples = pd.read_csv( os.path.join(preditions_dir, \"samples.csv\"), index_col = 0, header=[0,1,2] )\ndf_samples.index = [pd.Timestamp(x) for x in df_samples.index]\nre_samples = df_samples.loc[ : , (slice(None), \"Re\")].copy().droplevel(axis=1,  level = 1)\nre_samples_grouped = re_samples.groupby(axis = 1, level = 0 )\n\n## Create policy_stats object\npolicy_stats = Policy_Stats(policies)\npct_change_interval, pct_change_samples = policy_stats.est_pct_change( \n                                                {state:  df.droplevel(axis =1 , level = 0) for state, df in re_samples_grouped}, \n                                                policies =None)","3b2a75e4":"## Plot changes in Ro grouped by state\nfig = policy_stats.boxplot_changes_by_state(policies =None, aspect = 5, height = 4, hspace=1.7, )","3dc80c7b":"## Plot changes in Ro groups by policy\nfig = policy_stats.boxplot_changes_by_policy(aspect = 4, height = 6,y_lim = (-100, 100), hspace = 3.2)","757f7952":"### Distribution of the expected number of casualties calcuated from posterior samples of $P_i$ \n\nThe plots show that our model can fit the data. Predicted mean deaths per day match observed deaths well. This method can also be used to predict numbers of deaths in the future. But these predictions don't include any infections after the latest day where we have observed deaths. Thus, they will become less accurate as casualties from infections occuring in the future begin the accrue.","cfab5133":"### Ploting posterior distribution of $R_e$ and $P_i$","2b010b7a":"### Posterior distirbutions of percent change in $R_e$ after policy change\n\nViolin plots show distrubiton of percent change in $R_e$ calculated from posterior samples of $P_i$. Percent change is calculated by comparing means of $R_e$ for 7 days before to $R_e$ on day of policy implementation and 6 days after. When the 6 day window overlaps implementation of a new policy its right end is truncated.\n\nFor violin plots organized by state, the x axis is ordered by date of implementation. For violin plots organized by policy, when a policy is implemented on the same date as another policy in a state, all policies implemented on that date are indicated on x axis label.","863b4389":"## Disclaimer \nThis is a total naive model I put together. I am not an epidemiologist and have made several assumptions (outlined below) which may not be totally justifed. Comments, suggestions and critiques are welcome.\n\n## Motivation and Objectives\nGood estimates of the rate of transmission of COVID-19 as a function of time are necessary to measure effecitveness of public health policies. One measure of transmission is $R_e$, the effective reproduction number, which is defined as  the expected \"number of cases generated [by one case] in the current state of a population\" (https:\/\/en.wikipedia.org\/wiki\/Basic_reproduction_number).  One way to estimate $R_e$ on a day $t$ is to divide the cases infected on day $t$ by the sum of cases infected on previous days and capable of transmission. This gives average number of new cases generated by day by a single case, and multiplying by the period of transmission gives $R_e$\n\nEstimating the number of cases for which infection occurs on a given day is difficult because:\n1. A positive test for COVID occurs and unknown time period after infection\n2. The proportion of cases that are detected depend on availability of tests \n\nHere, we use reports of death due to COVID to estimate the proportions of individuals infected as a function of day. Because distributions of time between infection and symptom onset and between symptom onset and death have been estimated (Lauer et al, 2020 and Verity et al. 2020), it is possible to use these distirbutions to make probabilistic statements about the day of infection given day of death. Moreover, it seems likely that COVID deaths were much more accurately measured than COVID infections (especially in the early days of the pandemic). If we assume that the probability that an individual becomes a COVID casualty is independent of the day they are infected then, by estimating the proportion of fatal cases with infection occurring on day $t$, we are also estimating the proportion of all caes with infection occurring on day $t$.\n\nTherefore, the model and code below aim to estimate $P_t$, the probability of a fatal infection occurring on day t and $R_e(t)$ the effective reproduction number as a function of time\n\n\n## A Bayesian model for distribution of infections based on deaths and transmission\n\nWe enumerate days for which casualty data is available by $(d)_1^{d_m}$.\n\nList of Random variables:\n - $I$ - day of infection\n - $E$ - day of observed event (in this case death but could also be hospitalization)\n - $\\mathbb{1}(\\cdot)$ \u2014 indicator for an event (1 if true , 0 if false)\n - $(P_d)_{d=1}^{d_m}$ \u2014 A vector whose elements are probabilites that infection occurs on the corresponding day (A random variable in our baysian approach)\n - $(N_d)_{d=1}^{d_m}$ \u2014 A random variable represting the number of fatalities on day $d \\in \\{1,....d_m\\}$\n \n \n### Deriving likelihood for single case\n \n $$\n \\mathbb{P}(E =s | \\mathbb{1}(E \\le d_m) = 1) = \\sum_{t=1}^{d_m} \\mathbb{P}(E=s,I=t | \\mathbb{1}(E \\le d_m) = 1) \\\\\n = \\sum_{t=1}^{d_m} \\mathbb{P}(E=s | I=t, \\mathbb{1}(E \\le d_m) = 1)\\mathbb{P}(I=t | \\mathbb{1}(E \\le d_m) = 1) \\\\\n = \\sum_{t=1}^{d_m} \\mathbb{P}(E=s | I=t, \\mathbb{1}(E \\le d_m) = 1)\\frac{\\mathbb{P}(I=t , \\mathbb{1}(E \\le d_m) = 1)}{\\mathbb{P}(\\mathbb{1}(E \\le d_m) = 1)} \\\\\n = \\sum_{t=1}^{d_m} \\mathbb{P}(E=s | I=t, \\mathbb{1}(E \\le d_m) = 1)\\frac{\\mathbb{P}(\\mathbb{1}(E \\le d_m) = 1| I=t)\\mathbb{P}(I=t)}{\\mathbb{P}(\\mathbb{1}(E \\le d_m) = 1)} \\\\\n  = \\sum_{t=1}^{d_m} \\mathbb{P}(E=s, \\mathbb{1}(E \\le d_m) = 1) | I=t) \\frac{\\mathbb{P}(I=t)}{\\mathbb{P}(\\mathbb{1}(E \\le d_m) = 1)} \\\\\n $$\n\nSubstituting for defined variables and distributions estimated in the literature.\n\n+ First, \n    By definition of our random vector, $(P_d)_{d=1}^{d_m}$,\n    $$\n    \\mathbb{P}(I=t) \\equiv P_t\n    $$\n\n+ Second,\n    $$\n    \\mathbb{P}(E=s| I=t) \\equiv g(s-t)\n    $$\n    is the probability that death occurs on day s given that infection occurs on day t. Assuming that the time from infection to symptom onset and from symptom onset to death are indepedent.  We can use the log-normal distribution from (Lauer et al, 2020)) for time from infection to symptom onset and the gamma distribution from (Verity et al, 2020) for time from symptom onset to death to to obtain g(s-t). This distribution is plotted below. \n","0c3e17c4":"We can organize the values of $\\mathbb{P}(E=s, \\mathbb{1}(E \\le d_m) = 1) | I=t)$ into a matrix $G$ with elements  \n$$\nG_{s,t} = g(s-t)\\ ,\\ \\ \\ s,t\\in{1,...d_m}\n$$\nThen, in term of matrix $G_{s,t}$ and vector $P_t$\n\n$$\n \\mathbb{P}(E =s | \\mathbb{1}(E \\le d_m) = 1)  =  \\sum_{t=1}^{d_m} \\mathbb{P}(E=s, \\mathbb{1}(E \\le d_m) = 1) | I=t) \\frac{\\mathbb{P}(I=t)}{\\mathbb{P}(\\mathbb{1}(E \\le d_m) = 1)} = \\frac{\\sum_{t=1}^{d_m} G_{s,t}P_t}{\\sum_{s'=1}^{d_m}\\sum_{t'=1}^{d_m}  G_{s',t'}P_{t'}}  \\ \\ \\ \\ \\ \\ \\ \\ \\textbf{[Likelihood single observation]}\n$$\n\n### Likelihood for multiple observations\n\nThe above is the likelihood that an observed death for a single infection occurs on day $s$ given the distribution of infection over days $P_t$. It follows that the likelihood of $(n_s)_{s=1}^{d_m}$ deaths on days $s\\in \\{1,...d_m\\}$ is \n$$\n\\mathbb{P}\\left( (N_s)_{s=1}^{d_m}  = (n_s)_{s=1}^{d_m} | (P_t)_{1}^{d_m} \\right) = f\\left( (n_s)_{s=1}^{d_m},  \\left( \\frac{\\sum_{t=1}^{d_m} G_{s,t}P_t}{\\sum_{s'=1}^{d_m}\\sum_{t'=1}^{d_m}  G_{s',t'}P_{t'}} \\right)_{s=1}^{d_m} \\right)  \\ \\ \\ \\ \\ \\ \\ \\ \\textbf{[Likelihood multiple observations]}\n$$\nwhere $f$ is the multinomial distribution\n\n### Bayesian approach and choice of prior\n\nWe could try to find $(P_t)_{1}^{d_m}$ that maximizes the likelihood of $(n_s)_{s=1}^{d_m}$. This tends to give a few spikes in $P_t$ separated by days with $P_t$ near 0. This is unreasonable since we expect individuals infected on a day where $P_t$ is at a local maxium to acquire their infection from individuals infected on previous days. We change to a bayesian approach to estimating $P_t$ and add a prior $\\mathbb{P}(P_t = p_t) \\equiv h(p_t)$\n\n$$\nh\\left( (p_t)_{t=1}^{d_m}\\right) \\propto \\exp\\left\\{-\\sum_{t=T+2}^{d_m} \\mu(t)\\left[  \\left(\\log(p_t) - \\log\\sum_{i=1}^{T} p_{t-i}\\right) -  \\left(\\log(p_{t-1}) - \\log\\sum_{i=1}^{T} p_{t-1-i}\\right)  \\right]^2 \\right\\}\n$$\n\nTo understand this choice of prior. Think of $T>1$ as the number of days for which in infected individual can transmit the disease. Note:\n- $\\left(\\log(p_t) - \\log\\sum_{i=1}^{T} p_{t-i}\\right) = \\log\\frac{p_t}{\\sum_{i=1}^{T} p_{t-i}}$ is the log of the ratio of the expected number of fatal cases infected on day $t$ to the expected number of fatal cases capable of infection on day $t$.  As discussed in **Motivation and Objectives**, it is a good estimate of $R_e(t)$ (up to an additive constant representing the number of days transmission post-infection which falls out in the difference of the $t$ and $t-1$ terms).  \n- The square of the difference $ \\left(\\log(p_t) - \\log\\sum_{i=1}^{T} p_{t-i}\\right) -  \\left(\\log(p_{t-1}) - \\log\\sum_{i=1}^{T} p_{t-1-i}\\right) $ penalizes values of $(p_t)_{t=1}^{d_m}$ with dramatic changes $R_e(t)$ from one day to the next.\n- $\\mu(t)$ controls the strength of the penality for rapid change. It can be choosen by specifying how much a paricular increase in decrease in $R_e(t)$ should decrease the probability of $p_t$ relative to no change in $R_e(t)$ between days. We indicate a $t$ dependence for $\\mu$ since we expect days on which a new public helath policy (e.g. shelter in place) are implemented to have more dramatic change in $R_e(t)$ and we select $\\mu$ to be smaller by a factor definedthe hyperparameter `policy_factor` on these days\n\n### The posterior distribution and estimating $R_e(t)$\n\nCombining the likelihood and the prior, the posterior distribution for  $P_t$ is \n$$\n\\mathbb{P}\\left( (P_t)_{t=1}^{d_m} = (p_t)_{t=1}^{d_m}| (N_s)_{s=1}^{d_m} = (n_s)_{s=1}^{d_m} \\right) \\propto  f\\left( (n_s)_{s=1}^{d_m}, \\left( \\frac{\\sum_{t=1}^{d_m} G_{s,t}P_t}{\\sum_{s'=1}^{d_m}\\sum_{t'=1}^{d_m}  G_{s',t'}P_{t'}} \\right)_{s=1}^{d_m} \\right)  h\\left( (p_t)_{t=1}^{d_m}\\right)\n$$\n\nWe sample from this posterior distribution using Markov Chain Monte Carlo (MCMC). The posterior distribution of $R_e(t)$ is calcuated from samples of $(P_t)_{t=1}^{d_m}$ using\n$$\nR_e(t)  = \\frac{P_t}{\\sum_{i=1}^{T} P_{t+i}}\\times T\n$$\n\n\n### Change in $R_e$ following changes in policy\n- Change in $R_e$ on days of change in policy is calcuated by comparin the mean of $R_e(t)$ on the $7$ days before policy implementation to the mean of $R_e(t)$ on the day of policy implementation on the subsequent $6$ days. \n- When the post-implementation $6$ day window overlaps another policy implementation (which occurs frequently) the right end of the window is truncated. \n- Violin plots show the distribution of percent chnage in the means before and after policy implementaion across samples from the posterior.\n\n\n## Results and Discussion\n- Readers are encourages to review violin plots and draw their own conclusions. Overall there is strong evidence that shelter in place, non-essential business closures, gathering bans of various sizes reliably reduce $R_e$.\n- For most states considered only the most aggressive (and latest adoped measures) succeed in reducing $R_e$ below 1, which is needed for number of cases to decrease with time\n\n\n## Weaknesses and TODO\n- Policy dates are manually curated and this task is not complete. Efforts to produce single dates for policy implementation may not accurately refect nuances of practical implementation\n- Transmission period of 7 days is choose arbirarily. It is ment to capture when an infected individual is likely to be in their normal routine and not taking extra precaution do to strong symptoms\n- A better approach might be include dependence of $R_e(t)$ on policies directly in the model, rather than trying to relate estimates of $P_i$ to these factors after estimation.\n\n## References\nLauer et al. 2020. \"The Incubation Period of Coronavirus Disease 2019 (COVID-19) From\nPublicly Reported Confirmed Cases: Estimation and Application\". _Annals of Internal Medicine_\n\nVerity et al. 2020. \"Estimates of the severity of coronavirus disease 2019:\na model-based analysis\" _Lancet_"}}