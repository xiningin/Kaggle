{"cell_type":{"e3653b90":"code","d81e09b0":"code","4204d105":"code","5eae119f":"code","0a23ff54":"code","2e1bed33":"code","373c407e":"code","6a146ae7":"code","653bb582":"code","05e5f517":"code","1e0d1a0b":"code","e89f7feb":"code","f47475ec":"code","27500289":"code","889054aa":"code","b9db2d9c":"code","2bdff50c":"code","d315163b":"code","5584341f":"code","85c9a225":"code","c464f453":"code","74bcff31":"code","064854e5":"code","0dda19db":"code","101da8da":"code","274dd2e0":"code","46ee7024":"code","56270521":"code","256f74da":"code","f986ccd0":"code","737d3283":"code","190a051c":"code","c2f34975":"code","ce5288fc":"code","0a02f7f3":"code","e517a91d":"markdown","7e57fe39":"markdown","f2a32b17":"markdown","8da29fca":"markdown","971d46b4":"markdown","7c7663f5":"markdown","46517f4b":"markdown","c39e7ba6":"markdown","62a60a38":"markdown","b1d77d77":"markdown","0ad61ee9":"markdown","1c5c8719":"markdown","19d5f544":"markdown","ccef0e4b":"markdown","bf2addbc":"markdown","2e0a6d2a":"markdown","87b7a0ee":"markdown","366dbac1":"markdown","5a9334b7":"markdown","e8f84de9":"markdown","0aee517c":"markdown","e3afca84":"markdown","525df931":"markdown","5db5a687":"markdown","2c7ead3d":"markdown","b47dc43d":"markdown","7ebd8766":"markdown","ca537222":"markdown","cd19521b":"markdown","a68f0540":"markdown","e253c3ac":"markdown","b128e557":"markdown","1f744640":"markdown","01f19713":"markdown","f74999fa":"markdown","e3ff0eb7":"markdown","18993129":"markdown","fc9a4218":"markdown","d9f88fb2":"markdown","0924ec1e":"markdown","b834ac63":"markdown","e7988d35":"markdown","2e2a3081":"markdown","10744099":"markdown","46dcb8a6":"markdown","dffa030d":"markdown","34e353df":"markdown"},"source":{"e3653b90":"def missing(df) : \n    missing_number = df.isnull().sum().sort_values(ascending = False)\n    missing_percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending = False)\n    missing_values = pd.concat([missing_number, missing_percent], axis = 1, keys = ['Missing_number', 'Missing_percent'])\n    return missing_values \n\ndef categorize(df) :\n    Quantitive_features = df.select_dtypes([np.number]).columns.tolist()\n    Categorical_features = df.select_dtypes(exclude = [np.number]).columns.tolist()\n    Discrete_features = [col for col in Quantitive_features if len(df[col].unique()) < 10]\n    Continuous_features = [col for col in Quantitive_features if col not in Discrete_features]\n    print(\"Quantitive feautres : {} \\nDiscrete features : {} \\nContinous features : {} \\nCategorical features : {}\\n\"\n     .format(Quantitive_features, Discrete_features, Continuous_features, Categorical_features))\n    print(\"Number of quantitive feautres : {} \\nNumber of discrete features : {} \\nNumber of continous features : {} \\nNumber of categorical features : {}\"\n     .format(len(Quantitive_features), len(Discrete_features), len(Continuous_features), len(Categorical_features)))\n    return Quantitive_features, Categorical_features, Discrete_features, Continuous_features\n    \ndef unique(df) : \n    tb1 = pd.DataFrame({'Columns' : df.columns, 'Number_of_Unique' : df.nunique().values.tolist(),\n                       'Sample1' : df.sample(1).values.tolist()[0], 'Sample2' : df.sample(1).values.tolist()[0], \n                       'Sample3' : df.sample(1).values.tolist()[0],\n                       'Sample4' : df.sample(1).values.tolist()[0], 'Sample5' : df.sample(1).values.tolist()[0]})\n    return tb1\n    \ndef data_glimpse(df) :   \n    \n    # Dataset preview \n    print(\"1. Dataset Preview \\n\")\n    display(df.head())\n    print(\"-------------------------------------------------------------------------------\\n\")\n    \n    # Columns imformation\n    print(\"2. Column Imformation \\n\")\n    print(\"Dataset have {} columns and {} rows\".format(df.shape[0], df.shape[1]))\n    print(\"\\n\") \n    print(\"Dataset Column name : {}\".format(df.columns.values))\n    print(\"\\n\")\n    categorize(df)\n    print(\"-------------------------------------------------------------------------------\\n\")\n    \n    # Basic imformation table \n    print(\"3. Missing data table : \\n\")\n    display(missing(df))\n    print(\"-------------------------------------------------------------------------------\\n\")\n    \n    print(\"4. Number of unique value by column : \\n\")\n    display(unique(df))\n    print(\"-------------------------------------------------------------------------------\\n\")\n    \n    print(\"5. Describe table : \\n\")\n    display(df.describe())\n    print(\"-------------------------------------------------------------------------------\\n\")\n    \n    print(df.info())\n    print(\"-------------------------------------------------------------------------------\\n\")","d81e09b0":"# Data Analysis\nimport warnings \nwarnings.filterwarnings('ignore')\n    \nimport pandas as pd\nimport numpy as np\nimport os \nimport missingno as msno\n    \n# Data View\npd.options.display.max_columns = 200\n\n# Import Basic Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n    \n# Data Visualization : Plotly library \nimport cufflinks as cf\ncf.go_offline(connected = True )\n    \nimport plotly.express as px\n    \nimport plotly.graph_objects as go\nimport plotly.offline as pyo\npyo.init_notebook_mode()\n    \nfrom plotly.subplots import make_subplots \nimport plotly.figure_factory as ff ","4204d105":"df_raw = pd.read_csv(\"..\/input\/arketing-campaign\/marketing_campaign.csv\", encoding = 'utf-8-sig', sep = ';')\ndf_raw.head()","5eae119f":"data_glimpse(df_raw)","0a23ff54":"df_raw[df_raw['Income'].isnull() == True]","2e1bed33":"is_0 = len(df_raw[df_raw['Income'] == 0])\nprint(\"Number of 0 income in datasets : {}\".format(is_0))","373c407e":"df_raw['Income'].fillna(0, inplace = True)\ndf_raw.isnull().sum()","6a146ae7":"df_raw.Income = df_raw.Income.astype('int64')\ndf_raw.Dt_Customer = pd.to_datetime(df_raw.Dt_Customer)\n\ndf_raw.dtypes","653bb582":"df = df_raw.copy()","05e5f517":"df['Year_Old'] = df.apply(lambda x : 2014 - x.Year_Birth, axis = 1) # Because data has been written in 2014. \ndf.drop(columns = 'Year_Birth', inplace = True) # Drop column 'Year_Birth'\ndf.head()","1e0d1a0b":"df['Year_Customer'] = 2014 - df.Dt_Customer.dt.year # Because data has been written in 2014. \ndf.drop(columns = 'Dt_Customer', inplace = True)\ndf.head()","e89f7feb":"df = df.drop(columns = ['ID', 'Z_CostContact', 'Z_Revenue']) # We don't need column ID and feature 'Z_CostContac', 'Z_Revenue' becuase they have only one variable. \ndf.head()","f47475ec":"new_col = [fea for fea in df.columns if fea != 'Response'] # reorder columns to make correlation more prettier\nnew_col.append('Response')\n\ndf = df[new_col]\ndf.head()","27500289":"corr = df.corr()\nmask = np.triu(np.ones_like(corr, dtype=bool))\ndf_mask = corr.mask(mask)\n\nfig = ff.create_annotated_heatmap(z=np.round(df_mask.to_numpy(), 2), \n                                  x=df_mask.columns.tolist(),\n                                  y=df_mask.columns.tolist(),\n                                  colorscale=px.colors.diverging.RdBu,\n                                  hoverinfo=\"none\",\n                                  showscale=True, ygap=1, xgap=1\n                                 )\n\nfig.update_xaxes(side=\"bottom\")\n\nfig.update_layout(\n    title_text='Heatmap', \n    title_x=0.5, \n    width=1000, \n    height=1000,\n    xaxis_showgrid=False,\n    yaxis_showgrid=False,\n    xaxis_zeroline=False,\n    yaxis_zeroline=False,\n    yaxis_autorange='reversed',\n    template='plotly_white'\n)\n\nfor i in range(len(fig.layout.annotations)):\n    if fig.layout.annotations[i].text == 'nan':\n        fig.layout.annotations[i].text = \"\"\n    fig.layout.annotations[i].font.size = 8\n\nfig.show()","889054aa":"# Function for response rate \ndef response_rate(x) :\n    return round(x.mean()*100, 4)","b9db2d9c":"df.groupby('Education')['Response'].agg(response_rate).sort_values(ascending = False)","2bdff50c":"df.groupby('Marital_Status')['Response'].agg(response_rate).sort_values(ascending = False)","d315163b":"df.groupby('Kidhome')['Response'].agg(response_rate).sort_values(ascending = False)","5584341f":"df.groupby('Teenhome')['Response'].agg(response_rate).sort_values(ascending = False)","85c9a225":"df.groupby('AcceptedCmp5')['Response'].agg(response_rate).sort_values(ascending = False)","c464f453":"df.groupby('Complain')['Response'].agg(response_rate).sort_values(ascending = False)","74bcff31":"df.groupby('Year_Customer')['Response'].agg(response_rate).sort_values(ascending = False)","064854e5":"df.groupby('NumDealsPurchases')['Response'].agg(response_rate).sort_values(ascending = False)","0dda19db":"p = np.log(pd.crosstab(index = df.NumDealsPurchases, columns = df.Response)) # there are quite difference in value, so we transform value in log scale\n\nfig = go.Figure()\nfig.add_trace(go.Bar(\n    x = p.index,\n    y = p[0],\n    name = 'Non response'))\nfig.add_trace(go.Bar(\n    x = p.index,\n    y = p[1],\n    name = 'Response'))\nfig.update_layout(\n    {\n        \"title\": {\n            \"text\": \"<b>Logscale NumDealPurchases by response<\/b>\",\n            \"x\": 0.5,\n            \"y\": 0.9,\n            \"font\": {\n                \"size\": 15\n            }\n        },\n        \"xaxis\": {\n            \"title\": \"NumDealPurchases\",\n            \"dtick\" : 1,\n            \"tickfont\": {\n                \"size\": 10                \n            }\n        },\n        \"yaxis\": {\n            \"title\": \"Count in logscale\",\n            \"tickfont\": {\n                \"size\": 10                \n            }\n        },\n        \"template\":'plotly_white'\n    }\n)\nfig.show()","101da8da":"def bins_q(fea) : \n    q0 = np.quantile(df[fea], 0)\n    q1 = np.quantile(df[fea], 0.33)\n    q2 = np.quantile(df[fea], 0.66)\n    q3 = np.quantile(df[fea], 1)\n    return [q0, q1, q2, q3]\nlabel = ['LF', 'MF', 'HF']\n\ns1 = pd.cut(df.NumWebPurchases, bins = bins_q('NumWebPurchases'), labels = label)\ns2 = pd.cut(df.NumStorePurchases, bins = bins_q('NumStorePurchases'), labels = label)\n\nWeb_Store = pd.concat([s1, s2, df.Response], axis = 1)\nWeb_Store.head()","274dd2e0":"Web_Store.groupby(['NumWebPurchases', 'NumStorePurchases'])['Response'].agg(response_rate).sort_values(ascending = False)","46ee7024":"p  = Web_Store.groupby(['NumWebPurchases', 'NumStorePurchases'])['Response'].agg(response_rate).reset_index()\n\nfig = go.Figure()\n\nfig.add_trace(go.Bar(\n    x = p.NumWebPurchases.unique(),\n    y = p.loc[p.NumStorePurchases == 'LF', 'Response'],\n    name = 'Store LF',\n    text = p.loc[p.NumStorePurchases == 'LF', 'Response'],\n    texttemplate = \"%{text}%\",\n    marker_color = px.colors.sequential.RdBu[0]))\n\nfig.add_trace(go.Bar(\n    x = p.NumWebPurchases.unique(),\n    y = p.loc[p.NumStorePurchases == 'MF', 'Response'],\n    name = 'Store MF',\n    text = p.loc[p.NumStorePurchases == 'MF', 'Response'],\n    texttemplate = \"%{text}%\",\n    marker_color = px.colors.sequential.RdBu[2]))\n\nfig.add_trace(go.Bar(\n    x = p.NumWebPurchases.unique(),\n    y = p.loc[p.NumStorePurchases == 'HF', 'Response'],\n    name = 'Store HF',\n    text = p.loc[p.NumStorePurchases == 'HF', 'Response'],\n    texttemplate = \"%{text}%\",\n    marker_color = px.colors.sequential.RdBu[4]))\n\nfig.update_layout(\n    {\n        \"title\": {\n            \"text\": \"<b>Categorized NumWebPurchases and NumStorePurchases and its Response rate<\/b>\",\n            \"x\": 0.5,\n            \"y\": 0.9,\n            \"font\": {\n                \"size\": 15\n            }\n        },\n        \"xaxis\": {\n            \"title\": \"Web Frequency\",\n            \"showticklabels\":True,\n            \"tickfont\": {\n                \"size\": 10                \n            }\n        },\n        \"yaxis\": {\n            \"title\": \"Response rate\",\n            \"tickfont\": {\n                \"size\": 10                \n            }\n        },\n        \"template\":'plotly_white'\n    }\n)\n\nfig.show()","56270521":"bins = [0, 20, 40, 60, 121]\nlabels = ['Young', 'Middle-L', 'Middle-H', 'Old']\n\ns1 = pd.cut(df.Year_Old, bins = bins, labels = labels)\n\nYear_cat = pd.concat([s1, df.Response], axis = 1)\nYear_cat.head()","256f74da":"Year_cat.groupby(['Year_Old'])['Response'].agg(response_rate).sort_values(ascending = False)","f986ccd0":"s3 = pd.cut(df.Year_Old, bins = bins, labels = labels)\ns4 = pd.cut(df.NumWebPurchases, bins = bins_q('NumWebPurchases'), labels = label)\n\nWeb_Year = pd.concat([s3, s4, df.Response], axis = 1)\nWeb_Year.head()","737d3283":"Web_Year.groupby(['NumWebPurchases', 'Year_Old'])['Response'].agg(response_rate).sort_values(ascending = False)","190a051c":"bins = bins_q('Income')\nlabels = ['Low Income', 'Middle Income', 'High Income']\n\ns5 = pd.cut(df.Income, bins = bins, labels = labels)\nInc = pd.concat([s5, df.Response], axis = 1)\nInc.head()","c2f34975":"Inc.groupby(['Income'])['Response'].agg(response_rate).sort_values(ascending = False)","ce5288fc":"bins = bins_q('Recency')\nlabels = ['LF', 'MF', 'HF']\n\ns6 = pd.cut(df.Recency, bins = bins, labels = labels)\nRec = pd.concat([s6, df.Response], axis = 1)\nRec.groupby('Recency')['Response'].agg(response_rate).sort_values(ascending = False)","0a02f7f3":"fig = go.Figure()\nfig.add_trace(go.Scatter(\n    x = np.log(df.loc[df.Response == 0, 'Income']),\n    y = np.log(df.loc[df.Response == 0, 'MntWines']),\n    name = 'Non Response',\n    mode = 'markers'))\nfig.add_trace(go.Scatter(\n    x = np.log(df.loc[df.Response == 1, 'Income']),\n    y = np.log(df.loc[df.Response == 1, 'MntWines']),\n    name = 'Response',\n    mode = 'markers'))\nfig.update_layout(\n    {\n        \"title\": {\n            \"text\": \"<b>Exponential distribution MntWines and Income by Response<\/b>\",\n            \"x\": 0.5,\n            \"y\": 0.9,\n            \"font\": {\n                \"size\": 15\n            }\n        },\n        \"xaxis\": {\n            \"title\": \"Income(logscale)\",\n            \"showticklabels\":True,\n            \"tickfont\": {\n                \"size\": 10                \n            }\n        },\n        \"yaxis\": {\n            \"title\": \"MntWines(logscale)\",\n            \"tickfont\": {\n                \"size\": 10                \n            }\n        },\n        \"template\":'plotly_white'\n    }\n)\nfig.show()","e517a91d":"## Purpose\n<font color = 'red'>**Find features occuring response of marketing campaign and make strategy for business**<\/font>","7e57fe39":"### Marital_Status","f2a32b17":"To find the reason why there are NaN data in Income, we will find 0 income data in datasets.","8da29fca":"## Multivariate features analysis","971d46b4":"### NumDealsPurchases","7c7663f5":"## Library importing","46517f4b":"There are any 0 income in dataset so i think that 0 income becomes NaN value. ","c39e7ba6":"# <center> Marketing Campaign EDA ","62a60a38":"Response rate is more higher as year of customer is more higher. In business, it is important to maintain loyal customer. We can think 2 years customer as loyal customer, and it is important to focus them in business strategy.","b1d77d77":"Response rate is higher when there are more less kid in home.","0ad61ee9":"### Categorized Recency","1c5c8719":"As i think, young group is also have middle and high frequency webpurchases.","19d5f544":"Response rate has sequence in (PhD, Master, Graduation, 2nCycle, Basic). We can get insight that response is ocuured in the higher education group.","ccef0e4b":"## Change feature type\n\n- Income : change as 'int64'\n- Dt_customer : change as 'datetime' ","bf2addbc":"Strangely low frequency recency is highest as 23.8% while high frequency is smallest as 8%.","2e0a6d2a":"### Teenhome","87b7a0ee":"### Categorized income","366dbac1":"## Deal datetime value\nTo find correlation with Response, we need to change Year_Birth and Dt_Customer as Year_old and Year_Customer.  \nAnd we will store it in df.","5a9334b7":"## Analysis by rate of response","e8f84de9":"## Data importing","0aee517c":"### Categorized Year_old\n","e3afca84":"### Complain ","525df931":"# Data Preprocessing\n\n## Deal NaN value\nThere are 24 NaN data in datasets, so we will check datasets and fill NaN.","5db5a687":"High income response rate is higher twice than low income and middle income. ","2c7ead3d":"#### NumWebPurchases + Year_Old","b47dc43d":"### Kidhome","7ebd8766":"Response rate is higher in single-like group(Absurb?, YOLO, Anloe, Widow, Single, Divorced). We can get insight that in single, people are more active to marketing campaign.","ca537222":"## Correlation in quantitive features","cd19521b":"### Year_Customer ","a68f0540":"The result is same as 'Kidhome'.","e253c3ac":"Response rate is more higher in group where reponse to previous marketing campian. ","b128e557":"# Data glimpse","1f744640":"Young group is active to marketing campaign. To get more insight, we make datasets of NumWebPurchases and Year_Old.","01f19713":"# Analysis Preparation \n\n## Function importing","f74999fa":"### MntWines - Income group","e3ff0eb7":"We will see the rate of response by features and get insight of which features occuring response.\n\n- Education \n- Marital_Status\n- Kidhome\n- Teenhome\n- AcceptedCmp5\n- Complain\n- Year_Customer\n- total Accepted\n- NumDealsPurchases\n- Categorized NumWebPurchases + NumStorePurchases\n- Categorized Year_old\n- Categorized income\n- Categorized Recency","18993129":"There isn't significant difference in complain feature.","fc9a4218":"Response group is located intesively at upper-right.","d9f88fb2":"### NumWebPurchases + NumStorePurchases","0924ec1e":"## Column arrangement","b834ac63":"# Data Description\n\n## Datasource explaining\n\n**1. Datasets name :** \"marketing_campaign.csv\"  \n\n**2. Datasets source :** [kaggle\/marketing campaign](https:\/\/www.kaggle.com\/rodsaldanha\/arketing-campaign)  \n\n**3. Context :** A response model can provide a significant boost to the efficiency of a marketing campaign by increasing responses or reducing expenses. The objective is to predict who will respond to an offer for a product or service  \n\n**4. Feature domain knowledge :**\n\n    - AcceptedCmp1 - 1 if customer accepted the offer in the 1st campaign, 0 otherwise\n    - AcceptedCmp2 - 1 if customer accepted the offer in the 2nd campaign, 0 otherwise\n    - AcceptedCmp3 - 1 if customer accepted the offer in the 3rd campaign, 0 otherwise\n    - AcceptedCmp4 - 1 if customer accepted the offer in the 4th campaign, 0 otherwise\n    - AcceptedCmp5 - 1 if customer accepted the offer in the 5th campaign, 0 otherwise\n    - Response (target) - 1 if customer accepted the offer in the last campaign, 0 otherwise\n    - Complain - 1 if customer complained in the last 2 years\n    - DtCustomer - date of customer\u2019s enrolment with the company\n    - Education - customer\u2019s level of education\n    - Marital - customer\u2019s marital status\n    - Kidhome - number of small children in customer\u2019s household\n    - Teenhome - number of teenagers in customer\u2019s household\n    - Income - customer\u2019s yearly household income\n    - MntFishProducts - amount spent on fish products in the last 2 years\n    - MntMeatProducts - amount spent on meat products in the last 2 years\n    - MntFruits - amount spent on fruits products in the last 2 years\n    - MntSweetProducts - amount spent on sweet products in the last 2 years\n    - MntWines - amount spent on wine products in the last 2 years\n    - MntGoldProds - amount spent on gold products in the last 2 years\n    - NumDealsPurchases - number of purchases made with discount\n    - NumCatalogPurchases - number of purchases made using catalogue\n    - NumStorePurchases - number of purchases made directly in stores\n    - NumWebPurchases - number of purchases made through company\u2019s web site\n    - NumWebVisitsMonth - number of visits to company\u2019s web site in the last month\n    - Recency - number of days since the last purchase   \n\n\n**5. Datsets feature table :**\n\n| Feature | Column name | Data type | \n| :---: | :---: | :---: | \n| Index | ID | int64 | \n| Categorical - Continuous | Year_Birth | int64 | \n| Categorical - Discrete | Education | object64 | \n| Categorical - Discrete | Marital_Status | object64 | \n| Quantitive - Continuous | Income | int64 | \n| Quantitive - Discrete | kdhome | boolean | \n| Quantitive - Discrete | Teenhome | object64 | \n| Quantitive - Continuous | Dt_customer | datetime | \n| Quantitive - Continuous | Recency | int64 |  \n| Quantitive - Continuous | MntWines | int64 | \n| Quantitive - Continuous | MntFruits | int64 | \n| Quantitive - Continuous | MntMeatProducts| int64 | \n| Quantitive - Continuous | MntFishProducts | int64 |  \n| Quantitive - Continuous | MntSweetProducts| int64 |\n| Quantitive - Continuous | MntGoldProds | int64 |  \n| Quantitive - Continuous | NumDealsPurchases | int64 |  \n| Quantitive - Continuous | NumWebPurchases | int64 | \n| Quantitive - Continuous | NumCatalogPurchases | int64 |  \n| Quantitive - Continuous | NumStorePurchases | int64 |  \n| Quantitive - Continuous | NumWebVisitsMonth | int64 |  \n| Quantitive - Discrete | AcceptedCmp3 | boolean |  \n| Quantitive - Discrete | AcceptedCmp4 | boolean | \n| Quantitive - Discrete | AcceptedCmp5 | boolean | \n| Quantitive - Discrete| AcceptedCmp1 | boolean | \n| Quantitive - Discrete | AcceptedCmp2 | boolean | \n| Quantitive - Discrete | Complain | boolean | \n| Quantitive - Discrete| Z_CostContact | int64 | \n|  Quantitive - Discrete| Z_Revenue | int64 | \n| Target | Response | boolean |\n\n","e7988d35":"Response group is active in high frequency NumWebPurchases and low frequency NumStorePurchases, and non active in low frequency NumWebPurchases and low frequency NumStorePurchases. It might be most marketing campaign occurs on Web and E-commerce.","2e2a3081":"### AcceptedCmp5 ","10744099":"Response rate is more higher in 0, 9, 10, 11 deal purchases. But in 12, 13, 15 deal purchases group never response to marketing campiagn. So we need to make strategy for lower deal purchase and highest deal purchase groups. ","46dcb8a6":"# EDA + Visualization","dffa030d":"### Education","34e353df":"Most active response group is 0, 9, 10, 11 and need foucus group is 8, 12, 13, 14, 15."}}