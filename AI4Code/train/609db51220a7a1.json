{"cell_type":{"112dd060":"code","919370be":"code","28ca5e79":"code","fef18158":"code","48cd554d":"code","9dc3ed4a":"code","51450975":"code","682a61ff":"code","adef2875":"code","8f37186b":"code","3a127b5d":"code","40e08f1f":"code","cdf14517":"code","07fbee62":"code","b6b729a3":"code","08c5c4b7":"code","d9404ef1":"code","5e2365ab":"code","10d92236":"code","ca0cafc5":"code","bea08ac1":"code","8499e56e":"code","0ceb8bb6":"code","e4f5f5be":"code","8e0e6bcd":"code","8fafdfb7":"code","053760df":"code","c36cba1c":"code","3f7e8c39":"code","5c888bae":"code","06026349":"code","a8bee382":"code","a09b3996":"markdown","e8701116":"markdown","d354d61c":"markdown","39bc6bbb":"markdown","f93bebd3":"markdown","831e1685":"markdown"},"source":{"112dd060":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","919370be":"import os\nimport gc\nimport pandas as pd\nimport numpy as np\n\nimport fastai\nfrom   fastai.callback import *\nfrom   fastai.callback.all import *\nfrom   fastai.callback.training import GradientClip\nfrom   fastai.callback.all import SaveModelCallback, EarlyStoppingCallback, ReduceLROnPlateau \nfrom   fastai.tabular import *\nfrom   fastai.tabular.data import *\nfrom   fastai.tabular.all import *\nfrom   fastai.tabular.all import TabularPandas, RandomSplitter, CategoryBlock, MultiCategoryBlock, range_of, accuracy, tabular_learner, TabularDataLoaders\n# from fastai import datasets\n# from fastai.dataset import ModelData,ArraysIndexDataset\n# from fastai.dataloader import DataLoader\nfrom   fastai.learner import Learner\nfrom   fastai.metrics import RocAucMulti\n\nfrom   sklearn.pipeline import Pipeline\nfrom   sklearn.impute import SimpleImputer\nfrom   sklearn.preprocessing import StandardScaler\n\nimport torch.nn as nn\nfrom   torch.nn import CrossEntropyLoss, MSELoss\nfrom   torch.nn.modules.loss import _WeightedLoss\n\nfrom   functools import partial\nimport warnings\nwarnings.filterwarnings (\"ignore\")","28ca5e79":"# Global Vars\nTP   = None\nDF   = None\nDLs  = None\nPIPE = None\nBS   = 10000\nN_FEATURES  = 0\nN_FEAT_TAGS = 0","fef18158":"dtype = {\n    'feature'  : 'str', \n    'tag_0'    : 'int8'\n}\nfor i in range (1, 29):\n    k = 'tag_' + str (i)\n    dtype[k] = 'int8'\n    \nfeatures_df = pd.read_csv ('..\/input\/jane-street-market-prediction\/features.csv', usecols=range (1,30), dtype=dtype)\nN_FEATURES  = features_df.shape[0]  # the features.csv has 130 features (1st row) = no of features in train.csv (feature_0 to feature_129)\nN_FEAT_TAGS = features_df.shape[1]  # the features.csv has 29 tags\n\n# features_df.head ()\ndel features_df\ngc.collect ()\nN_FEATURES, N_FEAT_TAGS","48cd554d":"def preprocess_data (filename='..\/input\/jane-street-market-prediction\/train.csv', df=None, isTrainData=True):\n    \n    global PIPE\n    dtype = None\n    if isTrainData:\n        \n        dtype = {\n            'date'      : 'int64', \n            'weight'    : 'float32',\n            'resp'      : 'float32',\n            'ts_id'     : 'int64',  \n            'feature_0' : 'float32'\n        }\n    else:\n        \n        dtype = {\n            'date'      : 'int64', \n            'weight'    : 'float32',\n            'feature_0' : 'float32'\n        }\n    for i in range (1, 130):\n        k = 'feature_' + str (i)\n        dtype[k] = 'float32'\n    \n    to   = None\n    if isTrainData:\n        df         = pd.read_csv (filename, dtype=dtype)\n        df         = df.query ('date > 85')\n        # df       = df[df['weight'] != 0].reset_index (drop = True)\n        df         = df.reset_index (drop = True)\n        \n        resp_cols  = ['resp_1', 'resp_2', 'resp_3','resp_4', 'resp']    \n        # df[:5000].to_csv (filename+'.dummy', index=False) \n        y          = np.stack ([(df[c] > 0).astype ('int') for c in resp_cols]).T\n        df.drop (columns=['weight', 'date', 'ts_id']+resp_cols, inplace=True)\n        f_columns  = [c for c in df.columns if \"feature\" in c]\n        PIPE       = Pipeline ([\n                        (\"imputer\", SimpleImputer (missing_values=np.nan, strategy='mean')),\n                        # (\"stand\",   StandardScaler (with_mean=False))\n        ])\n        columns    = list (df.columns) + resp_cols\n        X          = PIPE.fit_transform (df)\n        df         = pd.DataFrame (np.hstack ((X, y)))\n        df.columns = columns\n        del X, y\n\n        splits    = RandomSplitter (valid_pct=0.05) (range_of (df))\n        to        = TabularPandas (df, cont_names=f_columns, cat_names=None, y_names=resp_cols, y_block=MultiCategoryBlock(encoded=True, vocab=resp_cols), splits=splits)\n    else:\n        \n        df         = df.drop (columns=['weight', 'date']).reset_index (drop = True)\n        columns    = df.columns\n        X          = PIPE.transform (df)\n        df         = pd.DataFrame (X)\n        df.columns = columns\n        # del X\n    return to, df","9dc3ed4a":"TP, DF = preprocess_data ()\nTP.xs.iloc[:2]","51450975":"TP.ys.iloc[:2]","682a61ff":"TP.xs.shape, TP.ys.shape","adef2875":"DLs = TP.dataloaders (bs=BS)\nDLs.show_batch ()","8f37186b":"DLs.one_batch ()[2].shape","3a127b5d":"x_cat, x_cont, y = DLs.train.one_batch ()\nx_cat.shape, x_cont.shape, y.shape","40e08f1f":"class SmoothBCEwLogits(_WeightedLoss):\n    \n    def __init__(self, weight=None, reduction='mean', smoothing=0.0):\n        super().__init__(weight=weight, reduction=reduction)\n        self.smoothing = smoothing\n        self.weight = weight\n        self.reduction = reduction\n\n    @staticmethod\n    def _smooth(targets:torch.Tensor, n_labels:int, smoothing=0.0):\n        assert 0 <= smoothing < 1\n        with torch.no_grad():\n            targets = targets * (1.0 - smoothing) + 0.5 * smoothing\n        return targets\n\n    def forward(self, inputs, targets):\n        targets = SmoothBCEwLogits._smooth(targets, inputs.size(-1),\n            self.smoothing)\n        loss = F.binary_cross_entropy_with_logits(inputs, targets,self.weight)\n\n        if  self.reduction == 'sum':\n            loss = loss.sum()\n        elif  self.reduction == 'mean':\n            loss = loss.mean()\n\n        return loss","cdf14517":"class Resnet (nn.Module):\n    \n    def __init__(self, inputSize, aLayerCount, drop_prob):\n        \n        super (Resnet, self).__init__()\n        # the same dropout and nonlinear activation node can by re-used by all the layers\n        self.dropout     = nn.Dropout (drop_prob)\n        self.nonlin      = nn.LeakyReLU (negative_slope=0.01, inplace=True)\n                 \n        self.batch_norm0 = nn.BatchNorm1d (inputSize)\n        \n        self.dense1       = nn.Linear (inputSize, aLayerCount)\n        self.batch_norm1  = nn.BatchNorm1d (aLayerCount)\n\n        self.dense2       = nn.Linear (aLayerCount, aLayerCount)\n        self.batch_norm2  = nn.BatchNorm1d (aLayerCount)\n\n        self.dense3       = nn.Linear (aLayerCount, aLayerCount)\n        self.batch_norm3  = nn.BatchNorm1d (aLayerCount)\n\n        self.dense4       = nn.Linear (aLayerCount, aLayerCount)\n        self.batch_norm4  = nn.BatchNorm1d (aLayerCount)\n\n        self.dense5       = nn.Linear (aLayerCount, aLayerCount)\n        self.batch_norm5  = nn.BatchNorm1d (aLayerCount)\n        \n        self.dense6       = nn.Linear (aLayerCount, aLayerCount)\n        self.batch_norm6  = nn.BatchNorm1d (aLayerCount)\n        \n        self.dense7       = nn.Linear (aLayerCount, aLayerCount)\n        self.batch_norm7  = nn.BatchNorm1d (aLayerCount)\n        \n        self.dense8       = nn.Linear (aLayerCount, aLayerCount)\n        self.batch_norm8  = nn.BatchNorm1d (aLayerCount)\n\n        self.dense9       = nn.Linear (aLayerCount, aLayerCount)\n        self.batch_norm9  = nn.BatchNorm1d (aLayerCount)\n        \n        self.dense10      = nn.Linear (aLayerCount, aLayerCount)\n        self.batch_norm10 = nn.BatchNorm1d (aLayerCount)\n        \n        self.dense11      = nn.Linear (aLayerCount, aLayerCount)\n        self.batch_norm11 = nn.BatchNorm1d (aLayerCount)\n\n        # self.nonlin    = nn.ReLU(inplace=True)\n        # self.nonlin    = nn.PReLU()\n        # self.nonlin    = nn.GELU()\n        # self.nonlin    = nn.RReLU()\n\n    def forward (self, X):\n        \n        # X0 = self.dropout (self.batch_norm0 (X))\n        X0  = self.batch_norm0 (X)\n        X1  = self.dropout (self.nonlin (self.batch_norm1  (self.dense1  (X0))))\n        X2  = self.dropout (self.nonlin (self.batch_norm2  (self.dense2  (X1))))\n        X3  = self.dropout (self.nonlin (self.batch_norm3  (self.dense3  (X2))))\n        X4  = self.dropout (self.nonlin (self.batch_norm4  (self.dense4  (X3  + X1))))\n        X5  = self.dropout (self.nonlin (self.batch_norm5  (self.dense5  (X4  + X2))))\n        X6  = self.dropout (self.nonlin (self.batch_norm6  (self.dense6  (X5  + X3))))\n        X7  = self.dropout (self.nonlin (self.batch_norm7  (self.dense7  (X6  + X4))))\n        X8  = self.dropout (self.nonlin (self.batch_norm8  (self.dense8  (X7  + X5))))\n        X9  = self.dropout (self.nonlin (self.batch_norm9  (self.dense9  (X8  + X6))))\n        X10 = self.dropout (self.nonlin (self.batch_norm10 (self.dense10 (X9  + X7))))\n        X11 = self.dropout (self.nonlin (self.batch_norm11 (self.dense11 (X10 + X8))))\n        return X11","07fbee62":"class Emb_Resnet_Model (nn.Module):\n    \n    def __init__(self, embed_dim=N_FEAT_TAGS, csv_file='..\/input\/jane-street-market-prediction\/features.csv'):\n        \n        super (Emb_Resnet_Model, self).__init__()\n        global N_FEAT_TAGS\n        N_FEAT_TAGS = 29\n        \n        # store the features to tags mapping as a datframe tdf, feature_i mapping is in tdf[i, :]\n        dtype = {'tag_0' : 'int8'}\n        for i in range (1, 29):\n            k = 'tag_' + str (i)\n            dtype[k] = 'int8'\n        t_df = pd.read_csv (csv_file, usecols=range (1,N_FEAT_TAGS+1), dtype=dtype)\n        t_df['tag_29'] = np.array ([1] + ([0] * (t_df.shape[0]-1)) ).astype ('int8')\n        self.features_tag_matrix = torch.tensor (t_df.to_numpy ())\n        N_FEAT_TAGS += 1\n        \n        # print ('self.features_tag_matrix =', self.features_tag_matrix)\n        \n        # embeddings for the tags. Each feature is taken a an embedding which is an avg. of its' tag embeddings\n        self.embed_dim     = embed_dim\n        self.tag_embedding = nn.Embedding (N_FEAT_TAGS+1, embed_dim) # create a special tag if not known tag for any feature\n        self.tag_weights   = nn.Linear (N_FEAT_TAGS, 1)\n        \n        drop_prob          = 0.45\n        self.ffn           = Resnet (130+embed_dim, 350, drop_prob)\n        self.outDense      = nn.Linear (350, 5)\n        return\n    \n    def features2emb (self):\n        \"\"\"\n        idx : int feature index 0 to N_FEATURES-1 (129)\n        \"\"\"\n        \n        all_tag_idxs = torch.LongTensor (np.arange (N_FEAT_TAGS)) #.to (DEVICE)              # (29,)\n        tag_bools    = self.features_tag_matrix                                # (130, 29)\n        # print ('tag_bools.shape =', tag_bools.size())\n        f_emb        = self.tag_embedding (all_tag_idxs).repeat (130, 1, 1)    #;print ('1. f_emb =', f_emb) # (29, 7) * (130, 1, 1) = (130, 29, 7)\n        # print ('f_emb.shape =', f_emb.size())\n        f_emb        = f_emb * tag_bools[:, :, None]                           #;print ('2. f_emb =', f_emb) # (130, 29, 7) * (130, 29, 1) = (130, 29, 7)\n        # print ('f_emb.shape =', f_emb.size())\n        \n        # Take avg. of all the present tag's embeddings to get the embedding for a feature\n        s = torch.sum (tag_bools, dim=1)                                       # (130,)\n        # print ('s =', s)              \n        f_emb = torch.sum (f_emb, dim=-2) \/ s[:, None]                         # (130, 7)\n        # print ('f_emb =', f_emb)        \n        # print ('f_emb.shape =', f_emb.shape)\n        \n        # take a linear combination of the present tag's embeddings\n        # f_emb = f_emb.permute (0, 2, 1)                                        # (130, 7, 29)\n        # f_emb = self.tag_weights (f_emb)                      #;print ('3. f_emb =', f_emb)                 # (130, 7, 1)\n        # f_emb = torch.squeeze (f_emb, dim=-1)                 #;print ('4. f_emb =', f_emb)                 # (130, 7)\n        return f_emb\n    \n    def forward (self, cat_featrs, features):\n        \"\"\"\n        when you call `model (x ,y, z, ...)` then this method is invoked\n        \"\"\"\n        \n        cat_featrs = None\n        features   = features.view (-1, N_FEATURES)\n        f_emb      = self.features2emb ()                                #;print ('5. f_emb =', f_emb); print ('6. features =', features) # (130, 7)\n        # print ('features.shape =', features.shape, 'f_emb.shape =', f_emb.shape)\n        features_2 = torch.matmul (features, f_emb)                      #;print ('7. features =', features) # (1, 130) * (130, 7) = (1, 7)\n        # print ('features.shape =', features.shape)\n        \n        # Concatenate the two features (features + their embeddings)\n        features   = torch.hstack ((features, features_2))        \n        \n        x          = self.ffn (features)                               #;print ('8. x.shape = ', x.shape, 'x =', x)   # (1, 7) -> (1, 7)\n        # x        = self.layer_normal (x + features)                  #;print ('9. x.shape = ', x.shape, 'x =', x)   # (1, 7) -> (1, 2)\n        \n        out_logits = self.outDense (x)                                 #;print ('10. out_logits.shape = ', out_logits.shape, 'out_logits =', out_logits)        \n        # return sigmoid probs\n        # out_probs = F.sigmoid (out_logits)\n        return out_logits","b6b729a3":"@delegates (torch.optim.AdamW.__init__)\ndef pytorch_AdamW (param_groups, **kwargs):\n    return OptimWrapper (torch.optim.AdamW ([{'params': ps, **kwargs} for ps in param_groups]))","08c5c4b7":"# for vanilla NN use this\npath  = \"..\/input\/jane-embedding-resnet\/Jane_Embedding_Resnet\"\nlearn = TabularLearner (DLs, model=Emb_Resnet_Model (), model_dir='\/kaggle\/working\/',\n                        loss_func=SmoothBCEwLogits (smoothing=0.015), metrics=RocAucMulti (),\n                        opt_func=partial (pytorch_AdamW, lr=0.007, betas=(0.9, 0.999), eps=1e-08, weight_decay=0.01)\n                       )\nlearn = learn.load (path)\nlearn.save (learn.model_dir)\nlearn.summary ()","d9404ef1":"logits = learn.model (x_cat, x_cont)\nlogits","5e2365ab":"x_cat, x_cont, y = learn.dls.one_batch ()\ninit_loss = learn.loss_func (learn.model (x_cat, x_cont), y)\ninit_loss","10d92236":"# lr_min, lr_steep = learn.lr_find (start_lr=1e-3, end_lr=5e-2, num_it=100)\n# lr_min, lr_steep","ca0cafc5":"modelfile = 'Jane_Embedding_Resnet'\ncallbacks = [\n    EarlyStoppingCallback (monitor='roc_auc_score', min_delta=0.0001, patience=12),\n    SaveModelCallback     (monitor='roc_auc_score', fname=modelfile),\n    ReduceLROnPlateau     (monitor='roc_auc_score', min_delta=0.0001, factor=2.0, min_lr=1e-8, patience=1),\n    GradientClip (0.1)\n]\n\nepochs = 80\n# lr     = lr_min\n# learn.fit_one_cycle (epochs, lr, wd=1e-2, cbs=callbacks)","bea08ac1":"from fastai.imports import *\nfrom fastai.torch_core import *\nfrom fastai.learner import *\n    \n@patch\n@delegates(subplots)\ndef plot_metrics(self: Recorder, nrows=None, ncols=None, figsize=None, **kwargs):\n    metrics = np.stack(self.values)\n    names = self.metric_names[1:-1]\n    n = len(names) - 1\n    if nrows is None and ncols is None:\n        nrows = int(math.sqrt(n))\n        ncols = int(np.ceil(n \/ nrows))\n    elif nrows is None: nrows = int(np.ceil(n \/ ncols))\n    elif ncols is None: ncols = int(np.ceil(n \/ nrows))\n    figsize = figsize or (ncols * 6, nrows * 4)\n    fig, axs = subplots(nrows, ncols, figsize=figsize, **kwargs)\n    axs = [ax if i < n else ax.set_axis_off() for i, ax in enumerate(axs.flatten())][:n]\n    for i, (name, ax) in enumerate(zip(names, [axs[0]] + axs)):\n        ax.plot(metrics[:, i], color='#1f77b4' if i == 0 else '#ff7f0e', label='valid' if i > 0 else 'train')\n        ax.set_title(name if i > 1 else 'losses')\n        ax.legend(loc='best')\n    plt.show()","8499e56e":"# learn.recorder.plot_loss (skip_start=0, with_valid=True)","0ceb8bb6":"# learn.recorder.plot_metrics ()","e4f5f5be":"# learn.recorder.plot_lr ()","8e0e6bcd":"# _, logits, _ = learn.predict (DF.iloc[0])\n# logits","8fafdfb7":"MODEL = learn.model.eval ()\nMODEL","053760df":"import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.utils.rnn as rnn_utils\nfrom   torch.autograd import Variable\nfrom   torch.utils.data import Dataset, DataLoader, RandomSampler, SequentialSampler\n\nDEVICE = torch.device (\"cuda:0\") if torch.cuda.is_available () else torch.device (\"cpu\")","c36cba1c":"def predict_torch (test_df):\n    \n    test_df.drop (columns=['weight', 'date'], inplace=True)\n    test_df.reset_index (drop=True, inplace=True)\n    test_df = PIPE.transform (test_df)        \n    test_df = torch.tensor (test_df).float ().view (-1, 130)\n    predictions = []\n    for i in range (test_df.shape[0]):\n        \n        pred_p = torch.sigmoid (MODEL (None, test_df)).detach ().cpu ().numpy ().reshape ((-1, 5))\n        predictions.append (pred_p)\n\n    predictions = np.vstack (predictions)                     #;print ('predictions.shape =', predictions.shape)\n    predictions = np.median (predictions, axis=1)\n    return (predictions >= 0.5).astype (int)","3f7e8c39":"import torch.nn.functional as F\n\ndef predict (df, threshold=0.50):\n    \n    dl     = learn.dls.test_dl (df)\n    logits = learn.get_preds (dl=dl)[0]\n    probs  = F.sigmoid (logits).detach ().numpy ()\n    pred   = (np.median (probs, axis=1) >= threshold).astype (int)\n    return pred","5c888bae":"import janestreet\nenv      = janestreet.make_env ()  # initialize the environment\nenv_iter = env.iter_test ()        # an iterator which loops over the test set","06026349":"for test_df, pred_df in env_iter:\n    if test_df[\"weight\"].item () > 0:\n        \n        pred_df.action = predict_torch (test_df)\n    else:\n        pred_df.action = 0\n        \n    # print (pred_df)\n    # print (\"--------------\")\n    env.predict (pred_df)","a8bee382":"print ('Done !')","a09b3996":"# For direct submission, without using Fastai since its too slow\nUse Fastai for training models only, not for prediction.","e8701116":"test_df = DF.copy()\nresp_cols  = ['resp_1', 'resp_2', 'resp_3','resp_4', 'resp']\ntest_df.drop (columns=resp_cols, inplace=True)\n\ntest_df  = preprocess_data (df=test_df, isTrainData=False)\npredict (test_df)","d354d61c":"# For prediction using Fastai\nDon't use thism its too slow and times out. Use Pytorch for prediction.\nUse Fastai for training the Pytorch models only.","39bc6bbb":"# Test","f93bebd3":"# Prediction","831e1685":"# Custom Model"}}