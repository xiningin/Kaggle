{"cell_type":{"f3affff0":"code","9655e017":"code","5d53db9a":"code","7dcec33a":"code","f96a306f":"code","7dc380b2":"code","71f97288":"code","d31e0202":"code","db06e714":"code","cf7e5d80":"code","138324f0":"code","2553c34d":"code","895418f3":"code","a042e581":"code","e0b40c32":"code","bb4d3abd":"code","68275973":"code","f2cc76bf":"code","6a40c941":"code","54487b24":"code","e3120ee6":"code","f1448cd8":"code","445865e7":"code","4e04d642":"code","0fbf2ed0":"code","2b04afed":"code","e9d95199":"code","20980180":"code","c6e92e90":"code","d580db9a":"code","f91e7b8a":"code","3d8b128c":"code","80b43e5e":"code","3874681c":"code","4dcbdd86":"markdown","653f5917":"markdown","c37413fa":"markdown","83570547":"markdown","bdf6bd55":"markdown","8c8f43dd":"markdown","cbe248a7":"markdown","19a8994f":"markdown","9406aa18":"markdown","bc944c4d":"markdown","cce0ff35":"markdown","c889367e":"markdown","de22d86b":"markdown","80554848":"markdown","c98aaa00":"markdown","833745e0":"markdown","779b1296":"markdown","9c99b6dc":"markdown","81cf568f":"markdown","0a3e556d":"markdown","25860776":"markdown","a64c8748":"markdown"},"source":{"f3affff0":"from IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }<\/style>\"))","9655e017":"## Loading Libraries\n\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score, f1_score\nfrom sklearn.metrics import roc_curve, roc_auc_score, classification_report, confusion_matrix\nfrom sklearn.utils import class_weight\n\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import model_selection\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\n\nimport pandas as pd\nimport numpy as np\nimport warnings\n\nfrom pandas.plotting import scatter_matrix\nimport matplotlib.pyplot as plt\nimport seaborn as sns","5d53db9a":"## Setting columns and rows display\npd.set_option('display.max_colwidth', None)\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n#pd.set_option('mode.chained_assignment', None)\nwarnings.filterwarnings(\"ignore\")","7dcec33a":"## Training Data\ndf = pd.read_csv(\"..\/input\/patients-with-abnormal-blood-pressure\/Training Data - Classification of Patients with Abnormal Blood Pressure ....csv\")","f96a306f":"df.head()","7dc380b2":"## Getting basic statistical detail of columns\ndf.describe()","71f97288":"## checking for the classes distribution in target column\ndf['Blood_Pressure_Abnormality'].value_counts()","d31e0202":"## Column details\ndf.info()","db06e714":"## Function for checking percentage of null values in dataframe columns\n\ndef null_value_check_in_dataframe(df):\n    percent_missing = df.isnull().sum() * 100 \/ len(df)\n    missing_value_df = pd.DataFrame({'column_name': df.columns,\n                                     'percent_missing': percent_missing})\n    return missing_value_df\n\ndf_null = null_value_check_in_dataframe(df)\ndf_null","cf7e5d80":"## Train-Test data split (10% Test data)\n\ntrain, test = train_test_split(df, train_size=0.9 ,test_size = 0.1, random_state=50, stratify=df['Blood_Pressure_Abnormality'])","138324f0":"# calculate the correlation matrix\n\ncorr = train.corr()\n\n# plot the heatmap\nfig, axes = plt.subplots(1, 1)\n\nfig.set_figheight(7)\nfig.set_figwidth(12)\n\ncmap = sns.diverging_palette(500, 10, as_cmap=True)\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns,\n        linewidths=1, cmap=cmap, center=0)\n\nplt.title(\"Correlation\")\nplt.show()","2553c34d":"## Plotting histogram to check the values distribution in columns\n\ncol_for_hist = ['Physical_activity','salt_content_in_the_diet','alcohol_consumption_per_day',\n               'Level_of_Hemoglobin','Genetic_Pedigree_Coefficient','Age','BMI']\n\nfig = plt.figure(figsize = (20,15))\nax = fig.gca()\n\ntrain[col_for_hist].hist(bins=50, ax = ax)\nplt.show()","895418f3":"## Boxplot for checking the variance and outliers in data\n\nfig, axes = plt.subplots(2, 2, figsize = (18,6))\n\nboxplot_cols1 = ['Physical_activity','salt_content_in_the_diet']\nboxplot_cols2 = ['alcohol_consumption_per_day']\nboxplot_cols3 = ['Age','BMI']\nboxplot_cols4 = ['Level_of_Hemoglobin','Genetic_Pedigree_Coefficient']\n\ntrain[boxplot_cols2].plot.box(vert = False, grid = True, ax=axes[0][1])\ntrain[boxplot_cols1].plot.box(vert = False, grid = True, ax=axes[0][0])\ntrain[boxplot_cols4].plot.box(vert = False, grid = True, ax=axes[1][0])\ntrain[boxplot_cols3].plot.box(vert = False, grid = True, ax=axes[1][1])","a042e581":"## Checking for the relationship between numerical variables through scatter plot\n\ndf_scatter = train[col_for_hist]\nscatter_matrix(df_scatter, alpha=0.2, figsize=(15, 15), diagonal=\"kde\");","e0b40c32":"## Barplots for various categorical variable to check the effect on Target variable (or is there any association)\n\nsns.catplot(x=\"Adrenal_and_thyroid_disorders\", kind=\"count\", hue=\"Blood_Pressure_Abnormality\", data=train)\nsns.catplot(x=\"Chronic_kidney_disease\", kind=\"count\", hue=\"Blood_Pressure_Abnormality\", data=train)\nsns.catplot(x=\"Sex\", kind=\"count\", hue=\"Blood_Pressure_Abnormality\", data=train)\nsns.catplot(x=\"Smoking\", kind=\"count\", hue=\"Blood_Pressure_Abnormality\", data=train)\nplt.show()","bb4d3abd":"## Fuction for Missing value imputation using IterativeImputer\n\ndef missing_value_imputation(df):\n    \n    ## Creating copy of a dataframe\n    df_imputed = df.copy()\n\n    ## Imputing 'Pregnancy' column values, putting Pregnancy=0 where Sex=0\n    #train['Pregnancy'] = np.where(train['Sex'] == 1, train['Pregnancy'], 0)\n    mask = df_imputed['Sex'] == 0\n    df_imputed.loc[mask, 'Pregnancy'] = 0\n\n    ## Remaining 'NaN' value is replaced with -1 (left Nan if for Sex=1, so created a separated category for those)\n    df_imputed['Pregnancy'].fillna(-1, inplace=True)\n    \n    ## Changing the column type to 'int', it was 'float' earlier\n    df_imputed['Pregnancy'] = df_imputed['Pregnancy'].astype(int)\n    \n    \n    #~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n    ## Imputing missing values using Iterative Imputer for columns 'Genetic_Pedigree_Coefficient' and 'alcohol_consumption_per_day'\n\n    # Define modeling pipeline\n    model = ExtraTreesRegressor(n_estimators=20, random_state=0)\n    imputer = IterativeImputer(estimator=model)\n\n    # Fitting the model\n    imputer.fit(df_imputed)\n\n    imputed_values = pd.DataFrame(imputer.transform(df_imputed), columns=df_imputed.columns)\n\n    imputed_values['Patient_Number'] = imputed_values['Patient_Number'].astype(int)\n    imputed_values_subset = imputed_values[['Patient_Number','Genetic_Pedigree_Coefficient', 'alcohol_consumption_per_day']]\n\n\n    df_imputed.drop(['Genetic_Pedigree_Coefficient', 'alcohol_consumption_per_day'], axis = 1, inplace=True)\n    df_imputed = pd.merge(df_imputed,imputed_values_subset, on=['Patient_Number'], how='left')\n    \n    return df_imputed\n","68275973":"## Function for normalizing data using MinMaxScaler\n\ndef data_normalization(df):\n    \n    # Normalization - MinMaxScaler Transform\n    cols_to_transform = ['Level_of_Hemoglobin','Age','BMI','Physical_activity','salt_content_in_the_diet','alcohol_consumption_per_day']\n    df_to_transform = df[cols_to_transform]\n\n    trans = MinMaxScaler()\n    scaled_features = trans.fit_transform(df_to_transform)\n\n    # convert the array back to a dataframe\n    df_transformed = pd.DataFrame(scaled_features, index=df_to_transform.index, columns=df_to_transform.columns)\n\n    df_subset = df.drop(cols_to_transform, axis=1)\n    df_train_transformed = pd.concat([df_transformed,df_subset], axis=1)\n    \n    return df_train_transformed","f2cc76bf":"## Function for performing the one-hot encoding of categorical variables\n\ndef oneHotEncoding(df):\n    \n    ## Creating one-hot encoding of categorical columns\n    df = pd.get_dummies(df, columns=['Pregnancy','Level_of_Stress'])\n\n    ## Dropping column 'Patient_Number' as it is unique id of patient, so not useful for analysis\n    df.drop(['Patient_Number'], axis=1, inplace=True)\n    \n    return df","6a40c941":"## Function for data preparation (which does data cleaning, normalization and column encoding)\n\ndef data_preparation(df):\n    \n    df_missing_value_imputation = missing_value_imputation(df)\n    df_data_normalized = data_normalization(df_missing_value_imputation)\n    df_onehot_encoding = oneHotEncoding(df_data_normalized)\n    \n    return df_onehot_encoding\n    ","54487b24":"## Data Preparation for Training dataframe\ndf_train_cleaned = data_preparation(train)\n\n## Data Preparation for Test dataframe\ndf_test_cleaned = data_preparation(test)","e3120ee6":"df_train_cleaned.shape","f1448cd8":"# Putting feature variable to X_train and X_test\nX_train = df_train_cleaned.drop(['Blood_Pressure_Abnormality'], axis=1)\nX_test = df_test_cleaned.drop(['Blood_Pressure_Abnormality'], axis=1)\n\n# Putting response variable to y_train and y_test\ny_train = df_train_cleaned['Blood_Pressure_Abnormality']\ny_test = df_test_cleaned['Blood_Pressure_Abnormality']","445865e7":"# Creating a Random Forest Classifier\nclf=RandomForestClassifier(n_estimators=100)\n\n# Training the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\n\n# predicting probabilities\nrf_probs = clf.predict_proba(X_test)\n\n# keeping probabilities for the positive outcome only\nrf_probs = rf_probs[:, 1]\n\n# predicting labels\ny_pred=clf.predict(X_test)","4e04d642":"## Calculating different model metrics\nprint(\"Accuracy  :: %.3f\"% accuracy_score(y_test, y_pred))\nprint(\"Recall    :: %.3f\"% recall_score(y_test, y_pred))\nprint(\"Precision :: %.3f\"% precision_score(y_test, y_pred))\nprint(\"F1 Score  :: %.3f\"% f1_score(y_test, y_pred))\nprint(\"AUC Score :: %.3f\"% roc_auc_score(y_test, rf_probs))\n\nprint(\"\\nClassification Report ::\")\nprint(\"\\n\",classification_report(y_test, y_pred))","0fbf2ed0":"## Plotting ROC curve\nfpr_RF, tpr_RF, thresholds_RF = roc_curve(y_test, rf_probs)\n\nplt.plot(fpr_RF, tpr_RF,'r-',label = 'RF')\nplt.plot([0,1],[0,1],'k-',label='random')\nplt.plot([0,0,1,1],[0,1,1,1],'g-',label='perfect')\nplt.legend()\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.show()","2b04afed":"## Plotting confusion matrix\n\ncf_matrix = confusion_matrix(y_test, y_pred)\nsns.heatmap(cf_matrix, annot=True, fmt=\"d\")\nplt.show()","e9d95199":"## Calculating Feature Importance\nfeature_imp = pd.Series(clf.feature_importances_,index=X_train.columns).sort_values(ascending=False)\n\n# Creating a bar plot for Feature Importance\nfig = plt.figure(figsize=(10,6))\nsns.barplot(x=feature_imp, y=feature_imp.index)\n\n# Add labels to your graph\nplt.xlabel('Feature Importance Score')\nplt.ylabel('Features')\nplt.title(\"Visualizing Important Features\")\nplt.show()","20980180":"## Trying out different classification models and then we'll choose the best based on the different metrics saved\n\n## Function for running different experiments or different models and caturing their respective metrices\ndef run_exps(X_train: pd.DataFrame , y_train: pd.DataFrame, X_test: pd.DataFrame, y_test: pd.DataFrame) -> pd.DataFrame:\n    '''\n    Lightweight script to test many models and find winners\n    :param X_train: training split\n    :param y_train: training target vector\n    :param X_test: test split\n    :param y_test: test target vector\n    :return: DataFrame of predictions\n    '''\n    \n    # variable to hold all of the datasets that will be created from the application of k-fold cross validation on the training set\n    dfs = []\n    \n    # list of tuples holding the name and class for each classifier to be tested\n    models = [('LogReg', LogisticRegression()), \n              ('RF', RandomForestClassifier()),\n              ('KNN', KNeighborsClassifier()),\n              ('SVM', SVC()), \n              ('GNB', GaussianNB()),\n              ('XGB', XGBClassifier(eval_metric='logloss'))\n             ]\n\n    results = []\n    names = []\n    scoring = ['accuracy', 'precision_weighted', 'recall_weighted', 'f1_weighted', 'roc_auc']\n\n    for name, model in models:\n\n      kfold = model_selection.KFold(n_splits=5, shuffle=True, random_state=90210)\n      cv_results = model_selection.cross_validate(model, X_train, y_train, cv=kfold, scoring=scoring)\n\n      clf = model.fit(X_train, y_train)\n      y_pred = clf.predict(X_test)\n\n      print(name)\n      print(classification_report(y_test, y_pred))\n\n      results.append(cv_results)\n      names.append(name)\n\n      this_df = pd.DataFrame(cv_results)\n      this_df['model'] = name\n      dfs.append(this_df)\n      final = pd.concat(dfs, ignore_index=True)\n\n    return final","c6e92e90":"## Calling model experimentation function\nfinal = run_exps(X_train, y_train, X_test, y_test)","d580db9a":"## To obtain better estimates of the distribution of metrics from each model, ran empirical bootstrapping at 30 samples. \n## Additionally, partitioned the data into two sorts: performance metrics and fit-time metrics.\n\nbootstraps = []\nfor model in list(set(final.model.values)):\n    model_df = final.loc[final.model == model]\n    bootstrap = model_df.sample(n=30, replace=True)\n    bootstraps.append(bootstrap)\n        \nbootstrap_df = pd.concat(bootstraps, ignore_index=True)\nresults_long = pd.melt(bootstrap_df,id_vars=['model'],var_name='metrics', value_name='values')\n\n# fit time metrics\ntime_metrics = ['fit_time','score_time'] \n\n## PERFORMANCE METRICS\nresults_long_nofit = results_long.loc[~results_long['metrics'].isin(time_metrics)] # get df without fit data\nresults_long_nofit = results_long_nofit.sort_values(by='values')\n\n## TIME METRICS\nresults_long_fit = results_long.loc[results_long['metrics'].isin(time_metrics)] # df with fit data\nresults_long_fit = results_long_fit.sort_values(by='values')","f91e7b8a":"## Plotting performance metrics from the 5-fold cross validation.\n\nplt.figure(figsize=(20, 12))\nsns.set(font_scale=1.5)\ng = sns.boxplot(x=\"model\", y=\"values\", hue=\"metrics\", data=results_long_nofit, palette=\"Set3\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.title('Comparison of Model by Classification Metric')\nplt.show()","3d8b128c":"## Training and Scoring time comparison by plotting\n\nplt.figure(figsize=(20, 12))\nsns.set(font_scale=1.5)\ng = sns.boxplot(x=\"model\", y=\"values\", hue=\"metrics\", data=results_long_fit, palette=\"Set3\")\nplt.legend(bbox_to_anchor=(1.05, 1), loc=2, borderaxespad=0.)\nplt.title('Comparison of Model by Fit and Score Time')\nplt.show()","80b43e5e":"## Evaluation metrics details for all the trained models\n\nmetrics = list(set(results_long_nofit.metrics.values))\nbootstrap_df.groupby(['model'])[metrics].agg([np.std, np.mean])","3874681c":"## Training and prediction time calculation for all the trained models\n\ntime_metrics = list(set(results_long_fit.metrics.values))\nbootstrap_df.groupby(['model'])[time_metrics].agg([np.std, np.mean])","4dcbdd86":"## Building Random Forest classifier model","653f5917":"## Train and Test Data Preparation","c37413fa":"## Splitting Data into Train and Test dataset","83570547":"Based on the Feature Importance we can select the subset of features, lets say Top 10 features or the features which account for 95% of the importance. The same number of features must be used in the training and testing sets.\n    \nThen We have build the model gain on Train dataset and prediction on Test dataset","bdf6bd55":"### <ins> Strategy choosen for missing value Imputation <\/ins>\n\n<p><\/p>\n\n\n<div style=\"font-size: 15px\">\n    \nI have Imputed all the missing values though **Iterative imputation** along with **ExtraTrees** as a regressor\n\n**Iterative imputation** or **multivariate imputation by chained equations (MICE)** refers to a process where each feature is modeled as a function of the other features, e.g. a regression problem where missing values are predicted. Each feature is imputed sequentially, one after the other, allowing prior imputed values to be used as part of a model in predicting subsequent features.\n\nIt is iterative because this process is repeated multiple times, allowing ever improved estimates of missing values to be calculated as missing values across all features are estimated.\n<\/div>\n\n### <ins> Reason for choosing the applied imputation <\/ins>\n\n<p><\/p>\n\n<div style=\"font-size: 15px\">\n    \nMultiple Imputations are much better than a single imputation as it measures the uncertainty of the missing values in a better way. The chained equations approach is also very flexible and can handle different variables of different data types (ie., continuous or binary) as well as complexities such as bounds.\n    \n<\/div>","8c8f43dd":"## Seperating out Independent and Dependent variables","cbe248a7":"## Feature Importance","19a8994f":"### 3. Boxplots (For checking Range of Values and Outliers)","9406aa18":"## Loading Data","bc944c4d":"## Missing Value Imputation","cce0ff35":"<div style=\"font-size: 15px\">\n\n\n\n\n<\/div>","c889367e":"<div style=\"font-size: 15px\">\n\nBelow variables having Moderate or High correlation\n\n* `Adrenal_and_thyroid_disorders` with `Blood_Pressure_Abnormality`\n* `Chronic_kidney_disease` with `Blood_Pressure_Abnormality`\n* `Pregnancy` with `Sex`\n* 'Blood_Pressure_Abnormality` with `Blood_Pressure_Abnormality`\n\n<\/div>","de22d86b":"## Data Analysis (EDA)","80554848":"### 4.Scatter Matrix Plot","c98aaa00":"### 2. Histograms (Checking for values distribution)","833745e0":"## One-Hot Encoding","779b1296":"Based on the Evaluation Metrics (Accuracy, Precision Recall, F1-score etc.) **Random Forest(RF)** isary (or oth by far the best model. But it is having slightly higher model training time (fit_time) compared to other models (except XGBoost, which is even having higer training time).\n    \nEither we can choose the Random Forest (as a best model) or go for second best model (in terms of prediction metrics and training time which is **GNB(Gaussian Naive Bayes)**\n    \nSo, this is basically an trade-of between the Accuracy(or other metrics for that matter) and the Training time, so we have to decide which one to choose if we have comparable model(slightly high\/low prediction metrics and high\/low mdel building\/training time) because there is cost involve in that (Time + resources) and every business is having different context to look into this.\n    \nOnce the model is finalized, then we'll go for the **Hyperparameter tuning** in oder to improve the model Accuracy further ( We'll have to decide based on the business objective whethere we really need this or not).\n\n","9c99b6dc":"## Checking Null Percentage","81cf568f":"## Testing out multiple models\n\nThe Idea here is to try out multiple classification models and choose the best one based on the evaluation metrics and Training\/Testing time.","0a3e556d":"### 1. Correlation Matrix","25860776":"## Data Normalization","a64c8748":"## Metrics Calculation\n \ncalculating different metrics to check the model performance on Test data\n   "}}