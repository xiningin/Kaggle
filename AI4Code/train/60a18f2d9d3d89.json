{"cell_type":{"0b84a9a0":"code","f2ac05c5":"code","61de4d90":"code","efba16ae":"code","d9b81d6d":"code","410b3948":"code","7f549c6d":"code","c0ffd5c0":"code","c3bad337":"code","63bb216a":"code","9e628ed8":"code","79ce270e":"code","701b7bc7":"code","986f1061":"code","c695ca70":"code","8603e64f":"code","1138dc9f":"code","4a1bd6a1":"code","ad7d6196":"code","d6c71d37":"code","bf9df906":"code","7d2570f4":"code","ce8e7ce3":"code","b5a307bf":"code","a034c386":"code","bbf7fb0b":"code","3478d41b":"code","45ec0a2a":"code","42777778":"code","64c163e9":"code","91f557dd":"code","8a0a59de":"code","706f954c":"code","5069e19a":"code","54ec4f76":"code","6838c889":"code","476da88e":"code","e4d65021":"code","e9eb7747":"code","59c2d297":"code","372c2606":"code","edf01243":"code","4e332e56":"code","d36d9b6f":"code","74af09a1":"code","fc71550a":"code","9d455385":"code","bab3b2d3":"code","bec9e854":"code","2df69710":"code","a5d0a107":"code","0425904e":"code","bdfcfc22":"code","27e1f391":"code","aa3834a3":"code","891af5fa":"code","810fad53":"code","12980d9e":"code","abd6924c":"code","2deafc63":"code","981498c0":"code","7c6e6905":"code","0bce67ba":"code","42a2513a":"code","50f65927":"code","9f48bbf5":"code","f8413088":"code","042d6b58":"code","3d7e3652":"code","576be0a5":"code","12d8fa3a":"code","6b76fe39":"code","bafb4611":"code","20806186":"code","ee0023c0":"code","94d6da22":"code","71fbfe30":"code","20792a54":"code","9e8eb15e":"code","16aef549":"code","226a6c12":"code","e95edb76":"code","6a4d72c3":"code","7871d5c6":"code","22621bbd":"code","9c62fc2f":"code","14976042":"code","d3f692ac":"code","4217d5dd":"code","7b7cd58b":"code","18e7e13b":"code","3b58a60e":"code","5c9f8ac4":"code","c99fe503":"code","185506a6":"code","b7badd6c":"code","f5f9f10a":"code","ec9b5f51":"code","57028f94":"code","eb767642":"code","a4898c0e":"code","a28d5905":"code","eb971cf0":"code","7517f702":"code","f05f33b2":"code","97fb3f9d":"code","9ba63693":"code","d2cf5af3":"code","27e3d128":"code","ff7fc9ea":"code","c9d57029":"code","99f2ac20":"code","688b40e2":"code","b90e54b2":"code","1d9d51d8":"code","cb2345e2":"code","47351269":"code","150b4896":"code","7b4988c3":"code","35ccd901":"code","b1ab4f45":"code","7bf11636":"code","de37b1cc":"code","8e4b9731":"code","5ff50b9c":"code","dcaf9e57":"code","cbf3d675":"code","73896e23":"code","06556150":"code","77f3c981":"code","02d57816":"code","1f938ed6":"code","0b786815":"code","e4111290":"code","7ac6f99a":"markdown","7e1b6581":"markdown","91745e2f":"markdown","0665c477":"markdown","7846159e":"markdown","946a0a92":"markdown","608a1411":"markdown","42e9bfd2":"markdown","dfe28265":"markdown","b4a5a044":"markdown","67aa0899":"markdown","667d6c47":"markdown","ce5cd8b9":"markdown","4d47f517":"markdown","2c0ffc54":"markdown","46748c76":"markdown","ba00ca27":"markdown","285e27ae":"markdown","0c134e2c":"markdown","f2d89ed1":"markdown","3bad8722":"markdown","d77c2f9c":"markdown","08c9bd14":"markdown","20a070e4":"markdown","d3fc395d":"markdown","677c4163":"markdown","132008e6":"markdown","3293022e":"markdown","09631d4b":"markdown","36dd8cbe":"markdown","b5359260":"markdown","950bb412":"markdown","43b0f117":"markdown","e8c7b7b9":"markdown","315c7015":"markdown","47ae301d":"markdown","e762401c":"markdown","6dfe860e":"markdown","ecfc47b2":"markdown","ad46e346":"markdown","a1dc9f6a":"markdown","7d251db1":"markdown","60fa5337":"markdown","a9cf2bb5":"markdown","4fc0da2a":"markdown","bd351ff9":"markdown","45c8613f":"markdown","7db3dee9":"markdown","39f63e1f":"markdown","38d906a9":"markdown","c71d5476":"markdown","f5f49d61":"markdown","6b39ecf0":"markdown","44db157b":"markdown","c18d9e74":"markdown","2f7b7783":"markdown","554a3ebc":"markdown","97c11986":"markdown","0ad0899b":"markdown","7b50d6eb":"markdown","7fa52bdd":"markdown","215210d1":"markdown","c38a6cdf":"markdown","8ffd10f4":"markdown","c49ce901":"markdown","2c032b75":"markdown","55263644":"markdown","ca2573a7":"markdown","d2a3ef9b":"markdown","2bb80f6f":"markdown","e0b581f5":"markdown","1ee659f4":"markdown","cb93552d":"markdown","fe89b0e8":"markdown","3420903a":"markdown","57ad7596":"markdown","255759e5":"markdown","10b21994":"markdown","5661a5cd":"markdown","cc655c3b":"markdown","d46d0c4d":"markdown","ea29f8a6":"markdown","92ceb8f4":"markdown","88e22916":"markdown","ac1257ac":"markdown","25c2cc28":"markdown","0d4f11b3":"markdown","28792354":"markdown","588dd165":"markdown","7522f44c":"markdown","b7f12b5a":"markdown","f683ce98":"markdown","3808fb98":"markdown","3606e612":"markdown","12281cdf":"markdown","fb8c5de8":"markdown","f875f23e":"markdown","4b012d6b":"markdown","5be8ed14":"markdown","d29be60e":"markdown","14f66e8e":"markdown","33a7c986":"markdown","4a1970f5":"markdown","8282ee1a":"markdown","18258981":"markdown","814e4a38":"markdown","365334e3":"markdown","2eb26b6f":"markdown","3275ba26":"markdown","e41b69c2":"markdown","0fbadee8":"markdown","6fca97cd":"markdown","ba6d5e50":"markdown","b94baec0":"markdown","f96a45cd":"markdown","6e98dc62":"markdown","e183c6bf":"markdown","c7c7719b":"markdown","4cf50e77":"markdown","fdc308d9":"markdown","c310be76":"markdown","f7d31ceb":"markdown","e62afbbe":"markdown","fbabfe6f":"markdown","29afb6d7":"markdown","76a786a9":"markdown","eafd98dd":"markdown","45b42726":"markdown","24c49070":"markdown","9d245101":"markdown","a6e41736":"markdown","24fad0e4":"markdown","4642594e":"markdown","d014960d":"markdown","dd298f5b":"markdown","5769c9e6":"markdown","b5b4e63c":"markdown","e3e0dd49":"markdown","158c5a4f":"markdown","987d6db6":"markdown","e2e80400":"markdown","6874f901":"markdown","7ac92a05":"markdown","750ae576":"markdown","168ee06d":"markdown","34cf7ae2":"markdown","5085821b":"markdown","70773f6e":"markdown","7fd4c7b2":"markdown","7a4a0808":"markdown","b9d32e0d":"markdown","02562f90":"markdown"},"source":{"0b84a9a0":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\nimport math as math\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.impute import SimpleImputer\nfrom sklearn import metrics\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.tree import export_graphviz\nfrom os import system\nfrom IPython.display import Image\nfrom sklearn.tree import plot_tree\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import StackingClassifier\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import accuracy_score\nfrom xgboost import XGBClassifier\nfrom catboost import CatBoostClassifier\nfrom vecstack import stacking\n%matplotlib inline\nsns.set(color_codes=True)","f2ac05c5":"#step 1.1: Read the dataset\n#data=pd.read_csv('Data - Parkinsons.csv')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","61de4d90":"PD_file_path='..\/input\/parkinson-disease-identification\/Data - Parkinsons.csv'\ndata=pd.read_csv(PD_file_path)\n\n","efba16ae":"# step 2.1: browse through the first few columns\ndata.head()","d9b81d6d":"# Step 2.2: Understand the shape of the data\nshape_data=data.shape\nprint('The shape of the dataframe is',shape_data,'which means there are',shape_data[0],'rows of voice recordings and',shape_data[1],'attributes of patients.')","410b3948":"# Step 2.3: Identify Duplicate records in the data \n# It is very important to check and remove data duplicates. \n# Else our model may break or report overly optimistic \/ pessimistic performance results\ndupes=data.duplicated()\nprint(' The number of duplicates in the dataset are:',sum(dupes),'\\n','Hence, it is quite evident that there are no duplicates in the dataset')","7f549c6d":"# Step 2.4: Lets analyze the data types\ndata.info()","c0ffd5c0":"# Step 2.5: lets evaluate statistical details of the dataset. \n#We will also add skewness column to the details to get a holistic view of the dataset from statistical perspective\ncname=data.columns\ndata_desc=data.describe().T\ndata_desc['Skewness']=round(data[cname].skew(),4)\npd.DataFrame(data_desc)","c3bad337":"# Step 2.6: Lets visually understand if there is any correlation between the independent variables. \nusecols =[i for i in data.columns if i != ['name','status']]\nsns.pairplot(data[usecols]);","63bb216a":"# Step 2.7: lets evaluate correlation between different attributes.\n# The column name and status has been ignored from the correlation heatmap. \n#The reason for the same will be explained in the next section.\ncorr=data[usecols].corr()\nfig, ax = plt.subplots(figsize=(20,20))\nsns.heatmap(corr,annot=True,linewidth=0.05,ax=ax, fmt= '.2f');","9e628ed8":"udata=data.drop('name',axis=1)\nudata.head()","79ce270e":"# Attributes in the Group\nAtr1g2='MDVP:Fo(Hz)'\nAtr2g2='MDVP:Fhi(Hz)'\nAtr3g2='MDVP:Flo(Hz)'","701b7bc7":"#EDA 1: 5 point summary to understand spread\nAtr1g2_5pt=udata.describe().loc[['min','25%','50%','75%','max'],[Atr1g2]]\nAtr2g2_5pt=udata.describe().loc[['min','25%','50%','75%','max'],[Atr2g2]]\nAtr3g2_5pt=udata.describe().loc[['min','25%','50%','75%','max'],[Atr3g2]]\n\nsumm_g2 = pd.concat([Atr1g2_5pt,Atr2g2_5pt,Atr3g2_5pt],axis=1,sort=False)\n\nprint('The 5 point summary of attributes in group 2 are:','\\n','\\n',summ_g2)","986f1061":"#EDA 2: Outliar Detection leveraging Box Plot\nfig, ax = plt.subplots(1,3,figsize=(16,10)) \nsns.boxplot(x=Atr1g2,data=udata, ax=ax[0],orient='v') \nsns.boxplot(x=Atr2g2,data=udata, ax=ax[1],orient='v')\nsns.boxplot(x=Atr3g2,data=udata,ax=ax[2],orient='v')","c695ca70":"#EDA 3: Skewness check\nAtr1g2_skew=round(stats.skew(udata[Atr1g2]),4)\nAtr2g2_skew=round(stats.skew(udata[Atr2g2]),4)\nAtr3g2_skew=round(stats.skew(udata[Atr3g2]),4)\n\nprint(' The skewness of',Atr1g2,'is', Atr1g2_skew)\nprint(' The skewness of',Atr2g2,'is', Atr2g2_skew)\nprint(' The skewness of',Atr3g2,'is', Atr3g2_skew)","8603e64f":"##EDA 4: Spread\nfig, ax = plt.subplots(1,3,figsize=(16,8)) \nsns.distplot(udata[Atr1g2],ax=ax[0]) \nsns.distplot(udata[Atr2g2],ax=ax[1]) \nsns.distplot(udata[Atr3g2],ax=ax[2])","1138dc9f":"##EDA 5: Correlation of attributes of group 2 with other attributes.\ncorr_atr1g2=udata[udata.columns].corr()[Atr1g2][:]\ncorr_atr2g2=udata[udata.columns].corr()[Atr2g2][:]\ncorr_atr3g2=udata[udata.columns].corr()[Atr3g2][:]\npd.concat([round(corr_atr1g2,4),round(corr_atr2g2,4),round(corr_atr3g2,4)],axis=1,sort=False).T\n\n# pd.DataFrame(round(corr,4)).T","4a1bd6a1":"# Attributes in the Group\nAtr1g3='MDVP:Jitter(%)'\nAtr2g3='MDVP:Jitter(Abs)'\nAtr3g3='MDVP:RAP'\nAtr4g3='MDVP:PPQ'\nAtr5g3='Jitter:DDP'","ad7d6196":"#EDA 1: 5 point summary to understand spread\nAtr1g3_5pt=udata.describe().loc[['min','25%','50%','75%','max'],[Atr1g3]]\nAtr2g3_5pt=udata.describe().loc[['min','25%','50%','75%','max'],[Atr2g3]]\nAtr3g3_5pt=udata.describe().loc[['min','25%','50%','75%','max'],[Atr3g3]]\nAtr4g3_5pt=udata.describe().loc[['min','25%','50%','75%','max'],[Atr4g3]]\nAtr5g3_5pt=udata.describe().loc[['min','25%','50%','75%','max'],[Atr5g3]]\n\nsumm_g3 = pd.concat([Atr1g3_5pt,Atr2g3_5pt,Atr3g3_5pt,Atr4g3_5pt,Atr5g3_5pt],axis=1,sort=False)\n\nprint('The 5 point summary of attributes in group 3 are:','\\n','\\n',summ_g3)","d6c71d37":"#EDA 2: Outliar Detection leveraging Box Plot\nfig, ax = plt.subplots(1,5,figsize=(16,10)) \nsns.boxplot(x=Atr1g3,data=udata, ax=ax[0],orient='v') \nsns.boxplot(x=Atr2g3,data=udata, ax=ax[1],orient='v')\nsns.boxplot(x=Atr3g3,data=udata,ax=ax[2],orient='v')\nsns.boxplot(x=Atr4g3,data=udata,ax=ax[3],orient='v')\nsns.boxplot(x=Atr5g3,data=udata,ax=ax[4],orient='v')","bf9df906":"#EDA 3: Skewness check\nAtr1g3_skew=round(stats.skew(udata[Atr1g3]),4)\nAtr2g3_skew=round(stats.skew(udata[Atr2g3]),4)\nAtr3g3_skew=round(stats.skew(udata[Atr3g3]),4)\nAtr4g3_skew=round(stats.skew(udata[Atr4g3]),4)\nAtr5g3_skew=round(stats.skew(udata[Atr5g3]),4)\n\nprint(' The skewness of',Atr1g3,'is', Atr1g3_skew)\nprint(' The skewness of',Atr2g3,'is', Atr2g3_skew)\nprint(' The skewness of',Atr3g3,'is', Atr3g3_skew)\nprint(' The skewness of',Atr4g3,'is', Atr4g3_skew)\nprint(' The skewness of',Atr5g3,'is', Atr5g3_skew)","7d2570f4":"##EDA 4: Spread\nfig, ax = plt.subplots(1,5,figsize=(16,8)) \nsns.distplot(udata[Atr1g3],ax=ax[0]) \nsns.distplot(udata[Atr2g3],ax=ax[1]) \nsns.distplot(udata[Atr3g3],ax=ax[2])\nsns.distplot(udata[Atr4g3],ax=ax[3])\nsns.distplot(udata[Atr5g3],ax=ax[4])","ce8e7ce3":"##EDA 5: Correlation of attributes of group 3 with other attributes.\ncorr_atr1g3=udata[udata.columns].corr()[Atr1g3][:]\ncorr_atr2g3=udata[udata.columns].corr()[Atr2g3][:]\ncorr_atr3g3=udata[udata.columns].corr()[Atr3g3][:]\ncorr_atr4g3=udata[udata.columns].corr()[Atr4g3][:]\ncorr_atr5g3=udata[udata.columns].corr()[Atr5g3][:]\npd.concat([round(corr_atr1g3,4),round(corr_atr2g3,4),round(corr_atr3g3,4),round(corr_atr4g3,4),round(corr_atr5g3,4)],axis=1,sort=False).T","b5a307bf":"# Attributes in the Group\nAtr1g4='MDVP:Shimmer'\nAtr2g4='MDVP:Shimmer(dB)'\nAtr3g4='Shimmer:APQ3'\nAtr4g4='Shimmer:APQ5'\nAtr5g4='MDVP:APQ'\nAtr6g4='Shimmer:DDA'","a034c386":"#EDA 1: 5 point summary to understand spread\nAtr1g4_5pt=udata.describe().loc[['min','25%','50%','75%','max'],[Atr1g4]]\nAtr2g4_5pt=udata.describe().loc[['min','25%','50%','75%','max'],[Atr2g4]]\nAtr3g4_5pt=udata.describe().loc[['min','25%','50%','75%','max'],[Atr3g4]]\nAtr4g4_5pt=udata.describe().loc[['min','25%','50%','75%','max'],[Atr4g4]]\nAtr5g4_5pt=udata.describe().loc[['min','25%','50%','75%','max'],[Atr5g4]]\nAtr6g4_5pt=udata.describe().loc[['min','25%','50%','75%','max'],[Atr6g4]]\n\nsumm_g4 = pd.concat([Atr1g4_5pt,Atr2g4_5pt,Atr3g4_5pt,Atr4g4_5pt,Atr5g4_5pt,Atr6g4_5pt],axis=1,sort=False)\n\nprint('The 5 point summary of attributes in group 4 are:','\\n','\\n',summ_g4)","bbf7fb0b":"#EDA 2: Outliar Detection leveraging Box Plot\nfig, ax = plt.subplots(1,6,figsize=(16,10)) \nsns.boxplot(x=Atr1g4,data=udata, ax=ax[0],orient='v') \nsns.boxplot(x=Atr2g4,data=udata, ax=ax[1],orient='v')\nsns.boxplot(x=Atr3g4,data=udata,ax=ax[2],orient='v')\nsns.boxplot(x=Atr4g4,data=udata,ax=ax[3],orient='v')\nsns.boxplot(x=Atr5g4,data=udata,ax=ax[4],orient='v')\nsns.boxplot(x=Atr6g4,data=udata,ax=ax[5],orient='v')","3478d41b":"#EDA 3: Skewness check\nAtr1g4_skew=round(stats.skew(udata[Atr1g4]),4)\nAtr2g4_skew=round(stats.skew(udata[Atr2g4]),4)\nAtr3g4_skew=round(stats.skew(udata[Atr3g4]),4)\nAtr4g4_skew=round(stats.skew(udata[Atr4g4]),4)\nAtr5g4_skew=round(stats.skew(udata[Atr5g4]),4)\nAtr6g4_skew=round(stats.skew(udata[Atr6g4]),4)\n\nprint(' The skewness of',Atr1g4,'is', Atr1g4_skew)\nprint(' The skewness of',Atr2g4,'is', Atr2g4_skew)\nprint(' The skewness of',Atr3g4,'is', Atr3g4_skew)\nprint(' The skewness of',Atr4g4,'is', Atr4g4_skew)\nprint(' The skewness of',Atr5g4,'is', Atr5g4_skew)\nprint(' The skewness of',Atr6g4,'is', Atr6g4_skew)","45ec0a2a":"##EDA 4: Spread\nfig, ax = plt.subplots(1,6,figsize=(16,8)) \nsns.distplot(udata[Atr1g4],ax=ax[0]) \nsns.distplot(udata[Atr2g4],ax=ax[1]) \nsns.distplot(udata[Atr3g4],ax=ax[2])\nsns.distplot(udata[Atr4g4],ax=ax[3])\nsns.distplot(udata[Atr5g4],ax=ax[4])\nsns.distplot(udata[Atr6g4],ax=ax[5])","42777778":"##EDA 5: Correlation of attributes of group 4 with other attributes.\ncorr_atr1g4=udata[udata.columns].corr()[Atr1g4][:]\ncorr_atr2g4=udata[udata.columns].corr()[Atr2g4][:]\ncorr_atr3g4=udata[udata.columns].corr()[Atr3g4][:]\ncorr_atr4g4=udata[udata.columns].corr()[Atr4g4][:]\ncorr_atr5g4=udata[udata.columns].corr()[Atr5g4][:]\ncorr_atr6g4=udata[udata.columns].corr()[Atr6g4][:]\npd.concat([round(corr_atr1g4,4),round(corr_atr2g4,4),round(corr_atr3g4,4),round(corr_atr4g4,4),round(corr_atr5g4,4),round(corr_atr5g4,4)],axis=1,sort=False).T","64c163e9":"# Attributes in the Group\nAtr1g5='NHR'\nAtr2g5='HNR'","91f557dd":"#EDA 1: 5 point summary to understand spread\nAtr1g5_5pt=udata.describe().loc[['min','25%','50%','75%','max'],[Atr1g5]]\nAtr2g5_5pt=udata.describe().loc[['min','25%','50%','75%','max'],[Atr2g5]]\n\nsumm_g5 = pd.concat([Atr1g5_5pt,Atr2g5_5pt],axis=1,sort=False)\n\nprint('The 5 point summary of attributes in group 5 are:','\\n','\\n',summ_g5)","8a0a59de":"#EDA 2: Outliar Detection leveraging Box Plot\nfig, ax = plt.subplots(1,2,figsize=(16,10)) \nsns.boxplot(x=Atr1g5,data=udata, ax=ax[0],orient='v') \nsns.boxplot(x=Atr2g5,data=udata, ax=ax[1],orient='v')","706f954c":"#EDA 3: Skewness check\nAtr1g5_skew=round(stats.skew(udata[Atr1g5]),4)\nAtr2g5_skew=round(stats.skew(udata[Atr2g5]),4)\n\nprint(' The skewness of',Atr1g5,'is', Atr1g5_skew)\nprint(' The skewness of',Atr2g5,'is', Atr2g5_skew)\n","5069e19a":"##EDA 4: Spread\nfig, ax = plt.subplots(1,2,figsize=(16,8)) \nsns.distplot(udata[Atr1g5],ax=ax[0]) \nsns.distplot(udata[Atr2g5],ax=ax[1]) ","54ec4f76":"##EDA 5: Correlation of attributes of group 5 with other attributes.\ncorr_atr1g5=udata[udata.columns].corr()[Atr1g5][:]\ncorr_atr2g5=udata[udata.columns].corr()[Atr2g5][:]\npd.concat([round(corr_atr1g5,4),round(corr_atr2g5,4)],axis=1,sort=False).T","6838c889":"# Attributes in the Group\nAtr1g7='RPDE'\nAtr2g7='D2'","476da88e":"#EDA 1: 5 point summary to understand spread\nAtr1g7_5pt=udata.describe().loc[['min','25%','50%','75%','max'],[Atr1g7]]\nAtr2g7_5pt=udata.describe().loc[['min','25%','50%','75%','max'],[Atr2g7]]\n\nsumm_g7 = pd.concat([Atr1g7_5pt,Atr2g7_5pt],axis=1,sort=False)\n\nprint('The 5 point summary of attributes in group 7 are:','\\n','\\n',summ_g7)","e4d65021":"#EDA 2: Outliar Detection leveraging Box Plot\nfig, ax = plt.subplots(1,2,figsize=(16,10)) \nsns.boxplot(x=Atr1g7,data=udata, ax=ax[0],orient='v') \nsns.boxplot(x=Atr2g7,data=udata, ax=ax[1],orient='v')","e9eb7747":"#EDA 3: Skewness check\nAtr1g7_skew=round(stats.skew(udata[Atr1g7]),4)\nAtr2g7_skew=round(stats.skew(udata[Atr2g7]),4)\n\nprint(' The skewness of',Atr1g7,'is', Atr1g7_skew)\nprint(' The skewness of',Atr2g7,'is', Atr2g7_skew)","59c2d297":"##EDA 4: Spread\nfig, ax = plt.subplots(1,2,figsize=(16,8)) \nsns.distplot(udata[Atr1g7],ax=ax[0]) \nsns.distplot(udata[Atr2g7],ax=ax[1]) ","372c2606":"##EDA 5: Correlation of attributes of group 7 with other attributes.\ncorr_atr1g7=udata[udata.columns].corr()[Atr1g7][:]\ncorr_atr2g7=udata[udata.columns].corr()[Atr2g7][:]\npd.concat([round(corr_atr1g7,4),round(corr_atr2g7,4)],axis=1,sort=False).T","edf01243":"# Attributes in the Group\nAtr1g8='DFA'","4e332e56":"#EDA 1: 5 point summary to understand spread\nAtr1g8_5pt=udata.describe().loc[['min','25%','50%','75%','max','mean'],[Atr1g8]]\n\nprint('The 5 point summary of attributes in group 7 are:','\\n','\\n',Atr1g8_5pt)","d36d9b6f":"#EDA 2: Outliar Detection leveraging Box Plot\nsns.boxplot(x=Atr1g8,data=udata,orient='v');","74af09a1":"#EDA 3: Skewness check\nAtr1g8_skew=round(stats.skew(udata[Atr1g8]),4)\n\nprint(' The skewness of',Atr1g8,'is', Atr1g8_skew)","fc71550a":"##EDA 4: Spread\nsns.distplot(udata[Atr1g8]);","9d455385":"##EDA 5: Correlation of attributes of group 8 with other attributes.\ncorr_atr1g8=udata[udata.columns].corr()[Atr1g8][:]\npd.DataFrame(round(corr_atr1g8,4)).T","bab3b2d3":"# Attributes in the Group\nAtr1g9='spread1'\nAtr2g9='spread2'\nAtr3g9='PPE'","bec9e854":"#EDA 1: 5 point summary to understand spread\nAtr1g9_5pt=udata.describe().loc[['min','25%','50%','75%','max'],[Atr1g9]]\nAtr2g9_5pt=udata.describe().loc[['min','25%','50%','75%','max'],[Atr2g9]]\nAtr3g9_5pt=udata.describe().loc[['min','25%','50%','75%','max'],[Atr3g9]]\n\nsumm_g9 = pd.concat([Atr1g9_5pt,Atr2g9_5pt,Atr3g9_5pt],axis=1,sort=False)\n\nprint('The 5 point summary of attributes in group 9 are:','\\n','\\n',summ_g9)","2df69710":"#EDA 2: Outliar Detection leveraging Box Plot\nfig, ax = plt.subplots(1,3,figsize=(16,10)) \nsns.boxplot(x=Atr1g9,data=udata, ax=ax[0],orient='v') \nsns.boxplot(x=Atr2g9,data=udata, ax=ax[1],orient='v')\nsns.boxplot(x=Atr3g9,data=udata, ax=ax[2],orient='v')","a5d0a107":"#EDA 3: Skewness check\nAtr1g9_skew=round(stats.skew(udata[Atr1g9]),4)\nAtr2g9_skew=round(stats.skew(udata[Atr2g9]),4)\nAtr3g9_skew=round(stats.skew(udata[Atr3g9]),4)\n\nprint(' The skewness of',Atr1g9,'is', Atr1g9_skew)\nprint(' The skewness of',Atr2g9,'is', Atr2g9_skew)\nprint(' The skewness of',Atr3g9,'is', Atr3g9_skew)","0425904e":"##EDA 4: Spread\nfig, ax = plt.subplots(1,3,figsize=(16,8)) \nsns.distplot(udata[Atr1g9],ax=ax[0]) \nsns.distplot(udata[Atr2g9],ax=ax[1])\nsns.distplot(udata[Atr3g9],ax=ax[2])","bdfcfc22":"##EDA 5: Comparison of Three nonlinear measures of fundamental frequency between people having PD (status=1) and people not having PD (status=0).\nfig, ax = plt.subplots(1,3,figsize=(16,8))\nsns.boxplot(x='status',y=Atr1g9,data=udata,ax=ax[0])\nsns.boxplot(x='status',y=Atr2g9,data=udata,ax=ax[1])\nsns.boxplot(x='status',y=Atr3g9,data=udata,ax=ax[2])","27e1f391":"##EDA 6: Correlation of attributes of group 9 with other attributes.\ncorr_atr1g9=udata[udata.columns].corr()[Atr1g9][:]\ncorr_atr2g9=udata[udata.columns].corr()[Atr2g9][:]\ncorr_atr3g9=udata[udata.columns].corr()[Atr3g9][:]\npd.concat([round(corr_atr1g9,4),round(corr_atr2g9,4),round(corr_atr3g9,4)],axis=1,sort=False).T","aa3834a3":"Atrg6='status'","891af5fa":"# EDA 1: Count of subjects who had Parkinson Disease and subjects who did not have Parkinson Disease\n\nudata_yPD= udata[udata[Atrg6]==1]\nudata_nPD= udata[udata[Atrg6]==0]\nnum_yPD=udata[Atrg6][udata[Atrg6]==1].count()\nnum_nPD=udata[Atrg6][udata[Atrg6]==0].count()\nprint('The total number of subjects who have Parkinson Disease are',num_yPD,'which is',round(num_yPD\/shape_data[0]*100,2),\n      'percent of the total dataset.')\nprint('The total number of subjects who do not have Parkinson Disease are',num_nPD,'which is',round(num_nPD\/shape_data[0]*100,2),\n      'percent of the total dataset.')","810fad53":"##EDA 2: Vocal frequency comparison between people having PD (status=1) and people not having PD (status=0).\nfig, ax = plt.subplots(1,3,figsize=(16,8))\nsns.boxplot(x='status',y=Atr1g2,data=udata,ax=ax[0])\nsns.boxplot(x='status',y=Atr2g2,data=udata,ax=ax[1])\nsns.boxplot(x='status',y=Atr3g2,data=udata,ax=ax[2])","12980d9e":"##EDA 3: Comparison of measures of variation in fundamental frequency between people having PD (status=1) and people not having PD (status=0).\nfig, ax = plt.subplots(1,5,figsize=(16,8))\nsns.boxplot(x='status',y=Atr1g3,data=udata,ax=ax[0])\nsns.boxplot(x='status',y=Atr2g3,data=udata,ax=ax[1])\nsns.boxplot(x='status',y=Atr3g3,data=udata,ax=ax[2])\nsns.boxplot(x='status',y=Atr4g3,data=udata,ax=ax[3])\nsns.boxplot(x='status',y=Atr5g3,data=udata,ax=ax[4])","abd6924c":"##EDA 4: Comparison of measures of variation in amplitude between people having PD (status=1) and people not having PD (status=0).\nfig, ax = plt.subplots(1,6,figsize=(16,8))\nsns.boxplot(x='status',y=Atr1g4,data=udata,ax=ax[0],palette=\"Set1\")\nsns.boxplot(x='status',y=Atr2g4,data=udata,ax=ax[1],palette=\"Set1\")\nsns.boxplot(x='status',y=Atr3g4,data=udata,ax=ax[2],palette=\"Set1\")\nsns.boxplot(x='status',y=Atr4g4,data=udata,ax=ax[3],palette=\"Set1\")\nsns.boxplot(x='status',y=Atr5g4,data=udata,ax=ax[4],palette=\"Set1\")\nsns.boxplot(x='status',y=Atr6g4,data=udata,ax=ax[5],palette=\"Set1\")","2deafc63":"##EDA 5: Comparison of Two measures of ratio of noise to tonal components in the voice between people having PD (status=1) and people not having PD (status=0).\nfig, ax = plt.subplots(1,2,figsize=(16,8))\nsns.boxplot(x='status',y=Atr1g5,data=udata,ax=ax[0])\nsns.boxplot(x='status',y=Atr2g5,data=udata,ax=ax[1])","981498c0":"##EDA 6: Comparison of Two nonlinear dynamical complexity measures between people having PD (status=1) and people not having PD (status=0).\nfig, ax = plt.subplots(1,2,figsize=(16,8))\nsns.boxplot(x='status',y=Atr1g7,data=udata,ax=ax[0],palette=\"Set1\")\nsns.boxplot(x='status',y=Atr2g7,data=udata,ax=ax[1],palette=\"Set1\")","7c6e6905":"##EDA 7: Comparison of Signal fractal scaling exponent between people having PD (status=1) and people not having PD (status=0).\n\nsns.distplot( udata[udata.status == 0][Atr1g8], color = 'g')\nsns.distplot( udata[udata.status == 1][Atr1g8], color = 'r')\n\n# sns.boxplot(x='status',y=Atr1g8,data=udata,palette=\"Set1\")","0bce67ba":"# lets make a copy of the data\npdata=udata.copy()","42a2513a":"\ndef calc_vif(X):\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n    return(vif)","50f65927":"calc_vif(round(pdata,2))","9f48bbf5":"# The attribute with the highest VIF is DFA. Dropping DFA from the dataset. \n#We also noticed that VIF of MDVP:Jitter(Abs) is Not a number. Hence, we will drop MDVP:Jitter(Abs) as well.\n# we will also drop Status; since it is a target variable\npdata=pdata.drop(['DFA','MDVP:Jitter(Abs)','status'],axis=1)","f8413088":"# computing VIF of remaining attributes\ncalc_vif(round(pdata,2))","042d6b58":"# It is clearly noticable that VIF of other variable has decreased when we dropped DFA from the data set.\n# We will continue to delete one attribute at a time and check till the VIF of the remaining attributes is below 10; \n# and we will select the attribute with highest VIF for deletion.\npdata=pdata.drop(['MDVP:Shimmer(dB)','spread1','MDVP:Shimmer','D2','Shimmer:DDA','RPDE','Shimmer:APQ5','MDVP:Fo(Hz)',\n                  'PPE','HNR','Shimmer:APQ3','Jitter:DDP'],axis=1)","3d7e3652":"# computing VIF of remaining attributes\ncalc_vif(round(pdata,2))","576be0a5":"# lets build our classification model\n# independent variables\nX = pdata\n# X=pd.DataFrame(X_stand1)\n# the dependent variable\ny = udata['status']","12d8fa3a":"X_train, X_test, y_train, y_test=train_test_split(X,y,test_size=0.30,random_state=1)","6b76fe39":"# lets check split of data\nprint(\"{0:0.2f}% data is in training set\".format((len(X_train)\/len(pdata.index)) * 100))\nprint(\"{0:0.2f}% data is in test set\".format((len(X_test)\/len(pdata.index)) * 100))","bafb4611":"y_train_yPD=y_train[y_train==1].count()\ny_train_nPD=y_train[y_train==0].count()\ny_test_yPD=y_test[y_test==1].count()\ny_test_nPD=y_test[y_test==0].count()\n\nprint(\"In the original dataset people who had parkinson Dieases    : {0} ({1:0.2f}%)\".format(len(pdata.loc[udata['status'] == 1]), (len(pdata.loc[udata['status'] == 1])\/len(pdata.index)) * 100))\nprint(\"In the original dataset people who didnot have Parkinson Disease   : {0} ({1:0.2f}%)\".format(len(pdata.loc[udata['status'] == 0]), (len(pdata.loc[udata['status'] == 0])\/len(pdata.index)) * 100))\nprint(\"\")\nprint(\"In the training dataset people who who had parkinson Dieases    : {0} ({1:0.2f}%)\".format(y_train_yPD, (y_train_yPD\/len(y_train))*100))\nprint(\"In the training dataset people who didnot have Parkinson Disease    : {0} ({1:0.2f}%)\".format(y_train_nPD, (y_train_nPD\/len(y_train))*100))\nprint(\"\")\nprint(\"In the test dataset people who who had parkinson Dieases    : {0} ({1:0.2f}%)\".format(y_test_yPD, (y_test_yPD\/len(y_test))*100))\nprint(\"In the test dataset people who didnot have Parkinson Disease    : {0} ({1:0.2f}%)\".format(y_test_nPD, (y_test_nPD\/len(y_test))*100))","20806186":"# lets create a copy of the train and test data for scaling\nX_Train_stand = X_train.copy()\nX_Test_stand = X_test.copy()","ee0023c0":"# we will use standard scaler for scaling the data.\nscale = StandardScaler().fit(X_Train_stand)","94d6da22":"X_train= scale.transform(X_Train_stand)\nX_test= scale.transform(X_Test_stand)","71fbfe30":"#while we have already checked in the section 2.4 leveraging fuction info() that there are no null values. \n# Still lets identify and count the null values\n\nif (pd.DataFrame(X_train).isnull().sum().any()==0):\n    print('There are no null values in the training datset')\nelse:\n    print('There are null values in the training datset')\n\nif (pd.DataFrame(X_test).isnull().sum().any()==0):\n    print('There are no null values in the test datset')\nelse:\n    print('There are null values in the test datset')","20792a54":"# Fit the model on train data\nmodel = LogisticRegression(solver=\"liblinear\")\nmodel.fit(X_train,y_train)","9e8eb15e":"# predict on the test data\ny_predict_LR = model.predict(X_test)\ny_predict_LR","16aef549":"coef_df = pd.DataFrame(model.coef_)\ncoef_df['intercept'] = model.intercept_\ncoef_df","226a6c12":"model_score_LR = model.score(X_test, y_test)\nprint(\"Model Accuracy of Logistic Regression is: {0:.4f}\".format(model_score_LR))\nprint()","e95edb76":"print(\"Confusion Matrix - Logistic Regression\")\ncm=metrics.confusion_matrix(y_test, y_predict_LR, labels=[1, 0])\n\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot=True);","6a4d72c3":"print(\"Classification Report - Logistic Regression\")\nprint(metrics.classification_report(y_test, y_predict_LR, labels=[1, 0]))","7871d5c6":"# Call Nearest Neighbour algorithm and fit the model on train data\nNNH = KNeighborsClassifier(n_neighbors= 5 , weights = 'distance' )\nNNH.fit(X_train, np.ravel(y_train,order='C'))","22621bbd":"# For every test data point, predict it's label based on 5 nearest neighbours in this model. \n#The majority class will be assigned to the test data point\n\ny_predict_KNN = NNH.predict(X_test)\nmodel_score_KNN = NNH.score(X_test, y_test)\n\nprint(\"Model Accuracy of KNN is: {0:.4f}\".format(model_score_KNN))\nprint()","9c62fc2f":"print(\"Confusion Matrix - KNN\")\ncm=metrics.confusion_matrix(y_test, y_predict_KNN, labels=[1, 0])\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot=True);","14976042":"print(\"Classification Report - KNN\")\nprint(metrics.classification_report(y_test, y_predict_KNN, labels=[1, 0]))","d3f692ac":"NB_model = GaussianNB()\nNB_model.fit(X_train, y_train)","4217d5dd":"y_predict_NB = NB_model.predict(X_test)\nmodel_score_NB=metrics.accuracy_score(y_test, y_predict_NB)\n\nprint(\"Model Accuracy of Naive Bayes is: {0:.4f}\".format(model_score_NB))\nprint()","7b7cd58b":"print(\"Confusion Matrix - Naive Bayes\")\ncm=metrics.confusion_matrix(y_test, y_predict_NB, labels=[1, 0])\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot=True);","18e7e13b":"print(\"Classification Report - Naive Bayes\")\nprint(metrics.classification_report(y_test, y_predict_NB, labels=[1, 0]))","3b58a60e":"clf = svm.SVC(gamma=0.025, C=3) ","5c9f8ac4":"clf.fit(X_train , y_train)","c99fe503":"y_predict_SVM = clf.predict(X_test)","185506a6":"model_score_NB=metrics.accuracy_score(y_test, y_predict_SVM)\n\nprint(\"Model Accuracy of SVM is: {0:.4f}\".format(model_score_NB))\nprint()","b7badd6c":"print(\"Confusion Matrix - SVM\")\ncm=metrics.confusion_matrix(y_test, y_predict_SVM, labels=[1, 0])\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot=True);","f5f9f10a":"print(\"Classification Report - SVM\")\nprint(metrics.classification_report(y_test, y_predict_SVM, labels=[1, 0]))","ec9b5f51":"models = [\n    KNeighborsClassifier(n_neighbors=5),\n    RandomForestClassifier(random_state=0, n_estimators=100, max_depth=3),\n    XGBClassifier(random_state=0, learning_rate=0.1, n_estimators=100, max_depth=3)\n]","57028f94":"S_train, S_test = stacking(models,                   \n                           X_train, y_train, X_test,   \n                           regression=False, mode='oof_pred_bag',metric=accuracy_score, n_folds=5, \n                           stratified=True, shuffle=True, random_state=0, verbose=2)","eb767642":"model = XGBClassifier(random_state=0, learning_rate=0.1, n_estimators=100, max_depth=3)\nmodel = model.fit(S_train, y_train)\ny_predict_Stack1 = model.predict(S_test)\nprint('The accuracy of the meta Classfier 1 is: [%.8f]' % metrics.accuracy_score(y_test, y_predict_Stack1))","a4898c0e":"print(\"Confusion Matrix - Meta Classifier 1\")\ncm=metrics.confusion_matrix(y_test, y_predict_Stack1, labels=[1, 0])\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot=True);","a28d5905":"print(\"Classification Report - Meta-Classifier 1\")\nprint(metrics.classification_report(y_test, y_predict_Stack1, labels=[1, 0]))","eb971cf0":"estimators = [\n    ('rf', RandomForestClassifier(random_state=0, n_estimators=100, max_depth=3)),\n#     ('lr', LogisticRegression(solver=\"liblinear\")),\n    ('knn',KNeighborsClassifier(n_neighbors= 5 , weights = 'distance' )),\n    ('XGB',XGBClassifier(random_state=0, learning_rate=0.1, n_estimators=100, max_depth=3))\n]","7517f702":"clf = StackingClassifier(\n    estimators=estimators, final_estimator=XGBClassifier()\n)","f05f33b2":"model = clf.fit(X_train, y_train)\ny_predict_Stack2 = model.predict(X_test)\nprint('The accuracy of the meta classifier 2 is: [%.8f]' % accuracy_score(y_test, y_predict_Stack2))","97fb3f9d":"print(\"Confusion Matrix - Meta Classifer 2\")\ncm=metrics.confusion_matrix(y_test, y_predict_Stack2, labels=[1, 0])\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot=True);","9ba63693":"print(\"Classification Report - Meta-Classifier 2\")\nprint(metrics.classification_report(y_test, y_predict_Stack2, labels=[1, 0]))","d2cf5af3":"model_rf = RandomForestClassifier(n_estimators = 50,random_state=1,max_features=3) \nmodel_rf = model_rf.fit(X_train, y_train)","27e3d128":"y_predict_rf = model_rf.predict(X_test)\nprint(model_rf.score(X_test, y_test))","ff7fc9ea":"print(\"Confusion Matrix -Random Forest\")\ncm=metrics.confusion_matrix(y_test, y_predict_rf, labels=[1, 0])\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot=True);","c9d57029":"print(\"Classification Report - Random Forest\")\nprint(metrics.classification_report(y_test, y_predict_rf, labels=[1, 0]))","99f2ac20":"bgcl = BaggingClassifier(n_estimators=50,random_state=1)\nbgcl = bgcl.fit(X_train, y_train)","688b40e2":"y_predict_bag = bgcl.predict(X_test)\nprint(bgcl.score(X_test , y_test))","b90e54b2":"print(\"Confusion Matrix -Bagging Classifier\")\ncm=metrics.confusion_matrix(y_test, y_predict_bag, labels=[1, 0])\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot=True);","1d9d51d8":"print(\"Classification Report - Bagging Classifier\")\nprint(metrics.classification_report(y_test, y_predict_bag, labels=[1, 0]))","cb2345e2":"AdaBC = AdaBoostClassifier(n_estimators=50, random_state=1)\n#abcl = AdaBoostClassifier( n_estimators=50,random_state=1)\nAdaBC = AdaBC.fit(X_train, y_train)","47351269":"y_predict_ada = AdaBC.predict(X_test)\nprint(AdaBC.score(X_test , y_test))","150b4896":"print(\"Confusion Matrix -Ada Boost\")\ncm=metrics.confusion_matrix(y_test, y_predict_ada, labels=[1, 0])\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot=True);","7b4988c3":"print(\"Classification Report - Ada Boost\")\nprint(metrics.classification_report(y_test, y_predict_ada, labels=[1, 0]))","35ccd901":"model = XGBClassifier(random_state=0, learning_rate=0.1, n_estimators=100, max_depth=4)\nmodel = model.fit(X_train, y_train)\ny_predict_XGB = model.predict(X_test)\nprint('The accuracy of the XGB Classifier is: [%.8f]' % accuracy_score(y_test, y_predict_XGB))","b1ab4f45":"print(\"Confusion Matrix - XGB Classifier\")\ncm=metrics.confusion_matrix(y_test, y_predict_XGB, labels=[1, 0])\ndf_cm = pd.DataFrame(cm, index = [i for i in [\"1\",\"0\"]],\n                  columns = [i for i in [\"Predict 1\",\"Predict 0\"]])\nplt.figure(figsize = (7,5))\nsns.heatmap(df_cm, annot=True);","7bf11636":"print(\"Classification Report - XGB Classifier\")\nprint(metrics.classification_report(y_test, y_predict_XGB, labels=[1, 0]))","de37b1cc":"# Model1: Logistic Summary\nAccuracy_LR=round(metrics.accuracy_score(y_test, y_predict_LR),2)\nRecall_LR=round(metrics.recall_score(y_test, y_predict_LR),2)\nPrecision_LR=round(metrics.precision_score(y_test, y_predict_LR),2)\nF1_LR=round(metrics.f1_score(y_test, y_predict_LR),2)","8e4b9731":"# Model2: KNN Summary\nAccuracy_KNN=round(metrics.accuracy_score(y_test, y_predict_KNN),2)\nRecall_KNN=round(metrics.recall_score(y_test, y_predict_KNN),2)\nPrecision_KNN=round(metrics.precision_score(y_test, y_predict_KNN),2)\nF1_KNN=round(metrics.f1_score(y_test, y_predict_KNN),2)","5ff50b9c":"# Model3:Native Bayes Summary\nAccuracy_NB=round(metrics.accuracy_score(y_test, y_predict_NB),2)\nRecall_NB=round(metrics.recall_score(y_test, y_predict_NB),2)\nPrecision_NB=round(metrics.precision_score(y_test, y_predict_NB),2)\nF1_NB=round(metrics.f1_score(y_test, y_predict_NB),2)","dcaf9e57":"# Model4:SVM Summary\nAccuracy_SVM=round(metrics.accuracy_score(y_test, y_predict_SVM),2)\nRecall_SVM=round(metrics.recall_score(y_test, y_predict_SVM),2)\nPrecision_SVM=round(metrics.precision_score(y_test, y_predict_SVM),2)\nF1_SVM=round(metrics.f1_score(y_test, y_predict_SVM),2)","cbf3d675":"# Model5: Meta Classifier 1 Summary\nAccuracy_Stack1=round(metrics.accuracy_score(y_test, y_predict_Stack1),2)\nRecall_Stack1=round(metrics.recall_score(y_test, y_predict_Stack1),2)\nPrecision_Stack1=round(metrics.precision_score(y_test, y_predict_Stack1),2)\nF1_Stack1=round(metrics.f1_score(y_test, y_predict_Stack1),2)","73896e23":"# Model6: Meta Classifier 2 Summary\nAccuracy_Stack2=round(metrics.accuracy_score(y_test, y_predict_Stack2),2)\nRecall_Stack2=round(metrics.recall_score(y_test, y_predict_Stack2),2)\nPrecision_Stack2=round(metrics.precision_score(y_test, y_predict_Stack2),2)\nF1_Stack2=round(metrics.f1_score(y_test, y_predict_Stack2),2)","06556150":"# Model7: Random Forest Summary\nAccuracy_rf=round(metrics.accuracy_score(y_test, y_predict_rf),2)\nRecall_rf=round(metrics.recall_score(y_test, y_predict_rf),2)\nPrecision_rf=round(metrics.precision_score(y_test, y_predict_rf),2)\nF1_rf=round(metrics.f1_score(y_test, y_predict_rf),2)","77f3c981":"# Model8: Bagging Summary\nAccuracy_bag=round(metrics.accuracy_score(y_test, y_predict_bag),2)\nRecall_bag=round(metrics.recall_score(y_test, y_predict_bag),2)\nPrecision_bag=round(metrics.precision_score(y_test, y_predict_bag),2)\nF1_bag=round(metrics.f1_score(y_test, y_predict_bag),2)","02d57816":"# Model9: Ada Boost Summary\nAccuracy_ada=round(metrics.accuracy_score(y_test, y_predict_ada),2)\nRecall_ada=round(metrics.recall_score(y_test, y_predict_ada),2)\nPrecision_ada=round(metrics.precision_score(y_test, y_predict_ada),2)\nF1_ada=round(metrics.f1_score(y_test, y_predict_ada),2)","1f938ed6":"# Model10: XGB Summary\nAccuracy_XGB=round(metrics.accuracy_score(y_test, y_predict_XGB),2)\nRecall_XGB=round(metrics.recall_score(y_test, y_predict_XGB),2)\nPrecision_XGB=round(metrics.precision_score(y_test, y_predict_XGB),2)\nF1_XGB=round(metrics.f1_score(y_test, y_predict_XGB),2)","0b786815":"summary = {'Accuracy': [Accuracy_LR,Accuracy_KNN,Accuracy_NB,Accuracy_SVM,Accuracy_Stack1,Accuracy_Stack2,Accuracy_rf,Accuracy_bag,Accuracy_ada,\n                        Accuracy_XGB],\n\n                    'Recall': [Recall_LR,Recall_KNN,Recall_NB,Recall_SVM,Recall_Stack1,Recall_Stack2,Recall_rf,Recall_bag,Recall_ada,\n                        Recall_XGB],\n\n                     'Precision': [Precision_LR,Precision_KNN,Precision_NB,Precision_SVM,Precision_Stack1,Precision_Stack2,Precision_rf,Precision_bag,Precision_ada,\n                        Precision_XGB],\n                       \n                       'F1Score':[F1_LR,F1_KNN,F1_NB,F1_SVM,F1_Stack1,F1_Stack2,F1_rf,F1_bag,F1_ada,\n                        F1_XGB]}\n\nmodels=['Logistic Regression','KNN','Naive Bayes','SVM','Meta Classifier 1','Meta Classifier 2','Random Forest','Bagging','Ada Boosting','XGB']\nsum_df = pd.DataFrame(summary,models)","e4111290":"sum_df","7ac6f99a":"#### Step5.a: Scaling of data. ","7e1b6581":"<font color='blue'> __Attribute Group 6 - Status:__  <\/font> The target column i.e. Status is a categorical variable and has boolean values (0,1) which essentially captures the health status of the subject. A value of 1 indicates that the subject has PD and a value of 0 indicates that the subject doesnt have PD. Lets perform EDA on Group 6.","91745e2f":"While all the 5 measure of variation in fundamental frequency have low correlation with the measure of Vocal fundamental frequency; <br> \nHowever, they seem to have high correlation with almost of all the other attributes. <br> This raises concerns about the possibility of existence of multi-collinearity in the data. We will check and remediate multi-collinearity in the subsequent sections","0665c477":"The confusion matrix: XG Boost\n\nTrue Positives (TP): We correctly predicted the count of subjects having Parkinson Disease and they actually had PD: 40\n\nTrue Negatives (TN): We correctly predicted the count of subjects not having Parkinson Disease and they actually dint have PD: 13\n\nFalse Positives (FP): We incorrectly predicted the count of subjects have Parkinson Disease but they actually dint have PD: 6\n\nFalse Negatives (FN):  We incorrectly predicted the count of subjects not having Parkinson Disease but they actually had PD: 0 ","7846159e":"MDVP:Fo(Hz)\n    - The median value of subject diagnosed with PD is around 145 whereas median value of people not diagnosed with PD is 200\n\nMDVP:Fhi(Hz)\n    - The median value of subject diagnosed with PD is around 170 whereas median value of people not diagnosed with PD is 230\n\nMDVP:Flo(Hz)\n    - The median value of subject diagnosed with PD is around 100 whereas median value of people not diagnosed with PD is 117\n\nFor all the 3 vocal fundamental frequency measures, it is quite evident that range of vocal frequency is more for people not having PD when compared to people haing PD.\n","946a0a92":"##### Classification Report: XGB Classifier","608a1411":"#### Meta Classifier Model 2: Stacking Classifier from SKLearn","42e9bfd2":"<font color='blue'> __Attribute Group 5 - NHR,HNR:__  <\/font> From the data description section (step 2.5) which captures basic statistical information, it appear that while NHR has right skewness; given that mean is greater than median. However, HNR has left skewness; given that mean is less than median. Lets perform EDA on Group 5.","dfe28265":"We will train the following classification algorithms:\n1. Random Forest\n2. Bagging\n3. Ada Boost\n4. XGB Classifer","b4a5a044":"Clearly both the attributes have skewness. While D2 has positive skewness indicating that the skewness is on the right. RPDE has slight negative skewness indicating that the skewness is on the left.","67aa0899":"### Step9: Compare all the models (minimum 5) and pick the best one among them","667d6c47":"From the 5 point summary above: <br>\n1. MDVP:Fo(Hz): \n    - The range of MDVP:Fo(Hz) is [88.3,260.1] with a median of 148.7. The mean (from section2.5) is sligtly more that median which means that there could be slight skewness on the right.\n    - It doesnt appear that there are any outliars; we will plot a box plot to confirm the same. <br><br>\n\n2. MDVP:Fhi(Hz): \n    - The range of MDVP:Fhi(Hz) is [102.1,592.0] with a median of 175.83. The mean (from section2.5) is more than median which means that there could be skewness on the right.\n    - There might be potential outliars which we will identify by plotting a box plot.<br><br>\n\n2. MDVP:Flo(Hz): \n    - The range of MDVP:Fhi(Hz) is [65.5,239.2] with a median of 104.3. The mean (from section2.5) is more than median which means that there could be skewness on the right.\n    - There might be potential outliars which we will identify by plotting a box plot.<br>\n","ce5cd8b9":"Clearly all 6 attributes have skewness with attribute MDVP:APQ having the highest skewness","4d47f517":"### Step 2: Study the data distribution in each attribute, share your findings","2c0ffc54":"<font color='blue'> __Attribute Group 1 - Name:__ <\/font> Name is a redundant attribute; since no analysis can be performed on it and it is not relevant for the model as well. Hence, we will delete it from our data set.","46748c76":"The plots above represents the 3 measures of the vocal fundamental frequency. The skewness is also evident from the plots. <br>\nPeak frequencies are noticed between 75 and 125 for Average vocal frequency (MDVP:Flo(Hz)); at 125 for Maximum vocal frequency (MDVP:Fo(Hz)); \nFor High vocal frequency, the peak seems to happen at 150; there are also some value at the right most tail","ba00ca27":"#### Ensemble Model 3: Ada Boost","285e27ae":"#### Classification Model 3: Na\u00efve Bayes","0c134e2c":"The table in step 9.2 captures performance measures of all the models trained by us. <br>\nNow as learnt from the case study document, our main objective is to identify all the subjects which have Parkinson Disease. <br>\nThe performance measure that helps with achieving this objective is Recall - Out of all the subjects who had Parkinson Disease; how many did our model identify.","f2d89ed1":"Performance measures of Ada boost:\n1. The accuracy of the Ada boost model is 0.88\n2. The recall of the Ada boost model is 0.97\n3. The precision of the Ada boost model is 0.87\n4. The F1 score of the Ada boost model is 0.92","3bad8722":"##### Step 9.2: Summarise all the models","d77c2f9c":"##### Classification Report: Random Forest","08c9bd14":"The confusion matrix: Random Forest\n\nTrue Positives (TP): We correctly predicted the count of subjects having Parkinson Disease and they actually had PD: 40\n\nTrue Negatives (TN): We correctly predicted the count of subjects not having Parkinson Disease and they actually dint have PD: 11\n\nFalse Positives (FP): We incorrectly predicted the count of subjects have Parkinson Disease but they actually dint have PD: 8\n\nFalse Negatives (FN):  We incorrectly predicted the count of subjects not having Parkinson Disease but they actually had PD: 0 ","20a070e4":"The confusion matrix: Naive Bayes\n\nTrue Positives (TP): We correctly predicted the count of subjects having Parkinson Disease and they actually had PD: 21\n\nTrue Negatives (TN): We correctly predicted the count of subjects not having Parkinson Disease and they actually dint have PD: 15\n\nFalse Positives (FP): We incorrectly predicted the count of subjects have Parkinson Disease but they actually dint have PD: 4\n\nFalse Negatives (FN):  We incorrectly predicted the count of subjects not having Parkinson Disease but they actually had PD: 19 ","d3fc395d":"We performed an exhuastive EDA; reduced multicollinearity; did scaling and built multiple models; in our endeavours to identify the model which gives the best results.","677c4163":"Observation:\n1. The recall of the KNN model is 0.95; which means we were not able to idetify all the subjects who had the disease.\n2. When compared to Logistics Regression, while this model had better accuracy; however, KNN is not able to identify all the subjects with the disease.","132008e6":"##### Confusion Matrix: SVM","3293022e":"#### Meta Classifier Model 1: Stacking from Vecstack","09631d4b":"From the 5 point summary above: <br>\n1. MDVP:Shimmer(%): \n    - The range of MDVP:Shimmer(%) is [0.009540, 0.119080] with a median of 0.022970. The mean (from section2.5) is sligtly more than median which means that there could be slight skewness on the right.\n    - There appear to be certain outliars on the right. we will plot a box plot to confirm the same. <br><br>\n\n2. MDVP:Shimmer(dB): \n    - The range of MDVP:Shimmer(dB) is [0.085000, 1.302000] with a median of 0.221000. The mean (from section2.5) is more than median which means that there could be skewness on the right.\n    - There might be potential outliars which we will identify by plotting a box plot.<br><br>\n\n3. Shimmer:APQ3: \n    - The range of Shimmer:APQ3 is [0.004550,0.056470] with a median of 0.012790. The mean (from section2.5) is more than median which means that there could be skewness on the right.\n    - There might be potential outliars which we will identify by plotting a box plot.<br><br>\n    \n4. Shimmer:APQ5: \n    - The range of Shimmer:APQ5 is [0.005700,0.079400] with a median of 0.013470. The mean (from section2.5) is more than median which means that there could be skewness on the right.\n    - There might be potential outliars which we will identify by plotting a box plot.<br><br>\n\n5. MDVP:APQ: \n    - The range of MDVP:APQ is [0.007190,0.137780] with a median of 0.018260. The mean (from section2.5) is more than median which means that there could be skewness on the right.\n    - There might be potential outliars which we will identify by plotting a box plot.<br><br>\n\n6. Shimmer:DDA: \n    - The range of Shimmer:DDA is [0.013640,0.169420] with a median of 0.038360. The mean (from section2.5) is more than median which means that there could be skewness on the right.\n    - There might be potential outliars which we will identify by plotting a box plot.<br><br>","36dd8cbe":"Performance measures of SVM:\n1. The accuracy of the SVM model is 0.81\n2. The recall of the SVM model is 1.00\n3. The precision of the SVM model is 0.78\n4. The F1 score of the SVM model is 0.88","b5359260":"Post training the model, in addition to the accuracy; we will also compute<br>\n1. Confusion Matrix\n2. Classification Report <br>\n\nFor the provided dataset, accuracy might not be the only most important score; but recall (which is out of all subjects who had Parkinson Disease, how many actually had the disease) is the most important score; since our aim should be to identify as much subjects with disease as possible. ","950bb412":"The plots above represents the spread of Signal fractal scaling exponents. As it is visible, there is slight Left skewness for DFA. <br>","43b0f117":"##### Confusion Matrix: Ada Boost","e8c7b7b9":"##### Confusion Matrix: Random Forest","315c7015":"1. From the box plot above it appears that while NHR is right skewed; HNR seems to have skewness towards the left. We'll compute the skewness in the next section\n2. There are outliars in both the attributes <br>","47ae301d":"##### This dataset is composed of a range of biomedical voice measurements. Each column in the table is a particular voice measure, and each row corresponds one of the voice recording from the individuals (\"name\" column). The rows contain an instance corresponding to one voice recording; There are around six recordings per patient.\n##### The main aim of the data is to discriminate healthy people from those with PD, according to \"status\" column which is set to 0 for healthy and 1 for PD.","e762401c":"The plots above represents the Two nonlinear dynamical complexity measures. Left skewness for RPDE and right skewness for D2 is also evident from the plots. <br>","6dfe860e":"Summary of the case study document and the objective of this exercise:<br>\nAs learnt from the case study document, Traditional diagnosis of Parkinson\u2019s Disease involves a clinician taking a neurological history of the patient and observing motor skills in various situations. <br>\nSince there is no definitive laboratory test to diagnose PD, diagnosis is often difficult, particularly in the early stages when motor effects are not yet severe. Monitoring progression of the disease over time requires repeated clinic visits by the patient.<br> ","ecfc47b2":"The confusion matrix: Ada Boost\n\nTrue Positives (TP): We correctly predicted the count of subjects having Parkinson Disease and they actually had PD: 39\n\nTrue Negatives (TN): We correctly predicted the count of subjects not having Parkinson Disease and they actually dint have PD: 13\n\nFalse Positives (FP): We incorrectly predicted the count of subjects have Parkinson Disease but they actually dint have PD: 6\n\nFalse Negatives (FN):  We incorrectly predicted the count of subjects not having Parkinson Disease but they actually had PD: 1 ","ad46e346":"Both the measures of nonlinear dynamical complexity seem to have some corelation most of all the other attributes. <br> As highlighted while performing EDA for group 3, group 4 and group 5, for Group 7 as well, there seems to be the possibility of existence of multi-collinearity in the data. We will check and remediate multi-collinearity in the subsequent sections","a1dc9f6a":"### Step6: Train at least 3 standard classification algorithms - Logistic Regression, Naive Bayes\u2019, SVM, k-NN etc, and note down their accuracies on the test data","7d251db1":"Performance measures of KNN:\n1. The accuracy of the KNN model is 0.83\n2. The recall of the KNN model is 0.95\n3. The precision of the KNN model is 0.83\n4. The F1 score of the KNN model is 0.88","60fa5337":"The following are the performance measures:<br>\n1. The accuracy: It is out of all subjects in the data, how many were predicted correctly for having the disease or not having the disease = ((TP+TN)\/(TP+TN+FP+FN))<br>\n2. The precision: It is out of all subjects who were predicted to have Parkinson Disease, how many actually had parkinson Disease = (TP\/(TP+FP))<br>\n3. The recall: It is out of all subjects who had parkinson Disease, how many did we identify =  ((TP\/(TP+FN)))<br>\n4. The F1 score which is computed considering both precision and recall together: 2*P*R\/(P+R)<br>\n\nThe accuracy, precision, accuracy, F1 score can be computed (referring the formulas above) as well as referred from the classification report","a9cf2bb5":"Out of the models above (with highest Recall); <font color='blue'> XG Boost <\/font> has the highest precison (0.87). XG Boost also has the highest accuracy.<br>\nHence,<font color='blue'> XG Boost is selected as the best model amongst all the models <\/font>","4fc0da2a":"1. MDVP:Fo(Hz) \n    - which is Average vocal fundamental frequency has some correlation with MDVP:Fhi(Hz) - Maximum vocal fundamental frequency and MDVP:Flo(Hz) - Minimum vocal fundamental frequency. All 3 of them are measures of vocal fundamental frequency. \n    - Apart from Maximum and minimum vocal fundamenta frequency Average vocal fundamental frequency doesnt seem to have any correlation with any other attribute. <br><br>\n\n2. MDVP:Fhi(Hz)\n    - which is maximum vocal fundamental frequency has some correlation with MDVP:Fo(Hz) - Average vocal fundamental frequency.\n    - Apart from Maximum and minimum vocal fundamenta frequency Average vocal fundamental frequency doesnt seem to have any correlation with any other attribute.<br><br>\n\n2. MDVP:Flo(Hz)\n    - which is minimum vocal fundamental frequency has some correlation with MDVP:Fo(Hz) - Average vocal fundamental frequency.\n    - Apart from Maximum and minimum vocal fundamenta frequency Average vocal fundamental frequency doesnt seem to have any correlation with any other attribute.\n","bd351ff9":"- Skewness: We have also added skewness column to understand the skewness of each attribute which can also be easily compared to mean and median.<br> Data seems to be skewed for MDVP:Fhi(Hz), MDVP:Jitter(%), MDVP:Jitter(Abs), MDVP:RAP, MDVP:PPQ, Jitter:DDP, MDVP:APQ, NHR, MDVP:Shimmer(dB); which is also quite evident from the fact that the mean and median for these attributes are quite far. There might also be potential outliars which we will evaluate in the next section.<br>\n- For the atributes: MDVP:Flo(Hz), MDVP:Shimmer, Shimmer:APQ3, Shimmer:APQ5, Shimmer:DDA, status; there is slight skewness.<br>\n- For the attributes:MDVP:Fo(Hz), HNR, RPDE, DFA, spread1, spread2, D2, PPE skewness is negligible.<br>\n- In the next section, we will discuss each attribute in details.","45c8613f":"The plots above represents the Three nonlinear measures of fundamental frequency measures. As it can be seen there is slight right skewness in PPE and spread 1 <br>","7db3dee9":"##### Classification Report: SVM","39f63e1f":"Performance measures of Logistic Regression:\n1. The accuracy of the logistic Regression model is 0.81\n2. The recall of the logistic Regression model is 1.00\n3. The precision of the logistic Regression model is 0.78\n4. The F1 score of the logistic Regression model is 0.88","38d906a9":"DFA\n1. The range of Signal fractal scaling exponent is more for subjects having PD when compared to people not having PD.\n2. There doesnt seem to be any outliars for both the status. \n3. Median of subjects diagnosed with Parkinson Disease is more than the median of subjects not diagnosed with Parkison Disease. <br>","c71d5476":"##### Step 9.4: Identify the best model","f5f49d61":"Before we move forward, it is very important to process the data owning to the observations made as conclusion of step 2. <br>Referring them again:\nObservations and likely challenges in the dataset:\n1. Scaling: Different independent variables have different scales e.g: DB, %age, Hz etc\n2. Multi-collinearity (we have resolved the same in the previous step)\n\nWe will also check present of any missing values in the training and testing data.","6b39ecf0":"<font color='blue'> __Attribute Group 4 - MDVP:Shimmer, MDVP:Shimmer(dB), Shimmer:APQ3, Shimmer:APQ5, MDVP:APQ, Shimmer:DDA:__  <\/font> From the data description section (step 2.5) which captures basic statistical information, it appear that all the attributes of group 4 have right skewness; given that __mean is greater than median __. Lets perform EDA on Group 4.","44db157b":"The following are the performance measures:<br>\n1. The accuracy: It is out of all subjects in the data, how many were predicted correctly for having the disease or not having the disease = ((TP+TN)\/(TP+TN+FP+FN))<br>\n2. The precision: It is out of all subjects who were predicted to have Parkinson Disease, how many actually had parkinson Disease = (TP\/(TP+FP))<br>\n3. The recall: It is out of all subjects who had parkinson Disease, how many did we identify =  ((TP\/(TP+FN)))<br>\n4. The F1 score which is computed considering both precision and recall together: 2*P*R\/(P+R)<br>\n\nThe accuracy, precision, accuracy, F1 score can be computed (referring the formulas above) as well as referred from the classification report","c18d9e74":"<font color='blue'> __Attribute Group 7 RPDE,D2:__  <\/font> From the data description section (step 2.5) which captures basic statistical information, it appear that mean and median are close to each other; hence it seems that the attributes have minimal skewness. Lets perform EDA on Group 7.","2f7b7783":"##### Step 9.1: Summarise all the models","554a3ebc":"observation:\n1. From the VIF scores computed above, it clearly appears that dataset has high multicollinearity.\n2. As discussed above, multicollinearity occurs when two or more independent variables are highly correlated with one another; which means that an independent variable can be predicted from another independent variable. Hence, we would not be able to distinguish between the individual effects of the independent variables on the dependent variable.\n2. It is important to reduce the multicollinearity; else our model might give biased outputs. \n3. <font color='red'> __We will follow the approach of deleting one attribute at a time and checking the VIF of the remaining attributes. We will continue to delete one attribute and check VIF of remaining till VIF of remaining attributes is less than 10__  <\/font>","97c11986":"#### Ensemble Model 4: XG Boost","0ad0899b":"Observation:\n1. Random Forest has a recall of 1 which means that we were able to identify all the subjects who had Parkinson Disease.","7b50d6eb":"#### Classification Model 2: KNN","7fa52bdd":"<font color='blue'> __Attribute Group 3 - MDVP:Jitter(%), MDVP:Jitter(Abs), MDVP:RAP, MDVP:PPQ, Jitter:DDP :__  <\/font> From the data description section (step 2.5) which captures basic statistical information, it appear that all the attributes of group 3 have right skewness; given that mean is greater than median. Lets perform EDA on Group 3.","215210d1":"Signal fractal scaling exponent seem to have some corelation with Average vocal fundamental frequency and maximum vocal fundamental frequency. Apart from these 2 attributes, DFA doesnt seem to have correlation with any other attribute. <br>","c38a6cdf":"Observation:\n1. Ada boost has a recall of 0.97 which means that we were not able to identify all the subjects who had Parkinson Disease.","8ffd10f4":"The following models have the highest recall (i.e. 1.00):\n1. Logistic Regression\n2. SVM\n3. Meta Classifier 1\n4. Random Forest\n5. Bagging\n6. XGB","c49ce901":"1. From the box plot above it appears that all the 6 attributes have skewness to the right. We'll compute the skewness in the next section\n2. There are outliars in all the attributes <br>","2c032b75":"Observation:\n1. Out of the 3 model trained, Naive Bayes model has the worst Recall and it is quite evident that we were not able to idetify all the subjects who had the disease.\n2. When compared to Logistics Regression and KNN, this is the worst performing model on all parameters.","55263644":"Given the count of the attributes; there is little visibility in the Pairplot. There appears to be correlation between attributes but it is difficult to identify it visually. Hence, we will calculate correlation between the variables.","ca2573a7":"From the 5 point summary above: <br>\n1. MDVP:Jitter(%): \n    - The range of MDVP:Jitter(%) is [0.0016, 0.0332] with a median of 0.0049. The mean (from section2.5) is sligtly more than median which means that there could be slight skewness on the right.\n    - There appear to be certain outliars on the right. we will plot a box plot to confirm the same. <br><br>\n\n2. MDVP:Jitter(Abs): \n    - The range of MDVP:Jitter(Abs) is [0.000007, 0.000260] with a median of 0.000030. The mean (from section2.5) is more than median which means that there could be skewness on the right.\n    - There might be potential outliars which we will identify by plotting a box plot.<br><br>\n\n3. MDVP:RAP: \n    - The range of MDVP:RAP is [0.000680,0.021440] with a median of 0.002500. The mean (from section2.5) is more than median which means that there could be skewness on the right.\n    - There might be potential outliars which we will identify by plotting a box plot.<br><br>\n    \n4. MDVP:PPQ: \n    - The range of MDVP:PPQ is [0.000920,0.019580] with a median of 0.002690. The mean (from section2.5) is more than median which means that there could be skewness on the right.\n    - There might be potential outliars which we will identify by plotting a box plot.<br><br>\n\n5. Jitter:DDP: \n    - The range of Jitter:DDP is [0.002040,0.064330] with a median of 0.007490. The mean (from section2.5) is more than median which means that there could be skewness on the right.\n    - There might be potential outliars which we will identify by plotting a box plot.<br><br>","d2a3ef9b":"Performance measures of Naive Bayes:\n1. The accuracy of the Naive Bayes model is 0.61\n2. The recall of the Naive Bayes model is 0.53\n3. The precision of the Naive Bayes model is 0.84\n4. The F1 score of the Naive Bayes model is 0.65","2bb80f6f":"From the 5 point summary above: <br>\n1. DFA: \n    - The range of DFA is [0.574282,0.825288] with a median of 0.722254. The mean (from section2.5) is close to median which means that there is minimal skewness.\n    - There doesnt seem to be any outliars. We will plot a box plot to confirm the same. <br><br>","e0b581f5":"### Step 5 Prepare the data for training - Scale the data if necessary, get rid of missing values (if any) etc","1ee659f4":"#### Ensemble Model 1: Random Forest","cb93552d":"The next performance measure that we will consider is Precision -  Out of all subjects who were predicted to have Parkinson Disease, how many actually had parkinson Disease","fe89b0e8":"Clearly all 3 attributes have skewness with attribute MDVP:Fo(Hz) having the highest skewness","3420903a":"From the 5 point summary above: <br>\n1. spread1: \n    - The range of spread1 is [-7.964984,-2.434031] with a median of -5.720868. The mean (from section2.5) is close to median which means that there is some skewness.\n    - There seem to be some outliars on the right hand side. We will plot a box plot to confirm the same. <br><br>\n\n2. spread2: \n    - The range of spread2 is [0.006274,0.450493] with a median of 0.218885. The mean (from section2.5) is close to median which means that there is some skewness.\n    - There seem to be some outliars. We will plot a box plot to confirm the same. <br><br>\n    \n2. PPE: \n    - The range of PPE is [0.044539,0.527367] with a median of 0.194052. The mean (from section2.5) is close to median which means that there is some skewness.\n    - There seem to be some outliars on the right hand side. We will plot a box plot to confirm the same. <br><br>","57ad7596":"### Step7: Train a meta-classifier and note the accuracy on test data","255759e5":"As seen in the section 2.7 (correlation heat map); it was quite evident that independent attributes have high correlation.<br> In this step we will check the multicollinearity by computing VIF (Variable Inflation Factors) score; VIF score of an independent variable represents how well the variable is explained by other independent variables.\n\nMultiCollinearity: Multicolinearity exists when there is high correlation between multiple independent variables. This might be a problem since, we would not be able to distinguish between the individual effects of the independent variables on the dependent variable. We'll try to identify the multi-collinearity by leveraging VIF (Variable Inflation Factor) and if it is found to be high then we will attempt at reducing the multi-collinearity by deleting few variables with high VIF.\n\n","10b21994":"##### Confusion Matrix: Meta Classifier 1","5661a5cd":"#### Classification Model 1: Logistic Regression","cc655c3b":"Observation:\n1. The recall of the logistic Regression model is 1; which means we were able to idetify all the subjects who had the disease.\n2. We will compare these results with other models to identify the best model","d46d0c4d":"Performance measures of Meta Classifier 1:\n1. The accuracy of the Meta Classifier model is 0.85\n2. The recall of the Meta Classifier model is 1.00\n3. The precision of the Meta Classifier model is 0.82\n4. The F1 score of the Meta Classifier model is 0.90","ea29f8a6":"From the 5 point summary above: <br>\n1. NHR: \n    - The range of NHR is [0.000650,0.314820] with a median of 0.011660. The mean (from section2.5) is more than median which means that there could be slight skewness on the right.\n    - There appear to be certain outliars on the right. we will plot a box plot to confirm the same. <br><br>\n\n2. HNR: \n    - The range of HNR is [8.441000,33.047000] with a median of 22.085000. The mean (from section2.5) is less than median which means that there could be skewness on the left.\n    - There might be potential outliars which we will identify by plotting a box plot.<br><br>","92ceb8f4":"### Step2: Eye-balling the raw data to understand the number of records, structure of the file, number of attributes, types of attributes and the likely challenges in the dataset. We will also state few observations w.r.t same.","88e22916":"#### Ensemble Model 2: Bagging","ac1257ac":"Post training the model, in addition to the accuracy; we will also compute<br>\n1. Confusion Matrix\n2. Classification Report <br>\n\nFor the provided dataset, accuracy might not be the only most important score; but recall (which is out of all subjects who had Parkinson Disease, how many actually had the disease) is the most important score; since our aim should be to identify as much subjects with disease as possible. ","25c2cc28":"We will train 2 meta classifiers<br>\n1. Stacking from the library vecstack.<br>\n    - The models selected as estimators are KNN,Random Forest and XGBClassifier <br>\n    - The meta model selected is XGBClassifier\n2. Stacking classifier from the library SKlearn<br>\n    - The models selected as estimators are KNN,Random Forest and XGBClassifier <br>\n    - The meta model selected is XGBClassifier\n","0d4f11b3":"The confusion matrix: Logistic Regression\n\nTrue Positives (TP): We correctly predicted the count of subjects having Parkinson Disease and they actually had PD: 40\n\nTrue Negatives (TN): We correctly predicted the count of subjects not having Parkinson Disease and they actually dint have PD: 8\n\nFalse Positives (FP): We incorrectly predicted the count of subjects have Parkinson Disease but they actually dint have PD: 11\n\nFalse Negatives (FN):  We incorrectly predicted the count of subjects not having Parkinson Disease but they actually had PD:  0 ","28792354":"While all the 6 measure of variation in amplitude have low correlation with the measure of Vocal fundamental frequency; <br> \nHowever, they seem to have high correlation with almost of all the other attributes. <br> As high lighted while performing EDA for group 3, for group 4 as well, there seems to be the possibility of existence of multi-collinearity in the data. We will check and remediate multi-collinearity in the subsequent sections","588dd165":"While both the measures of ratio of noise to tonal components in the voice have low correlation with the measure of Vocal fundamental frequency; <br> \nHowever, they seem to have high correlation with most of all the other attributes. <br> As highlighted while performing EDA for group 3 and group 4, for group 5 as well, there seems to be the possibility of existence of multi-collinearity in the data. We will check and remediate multi-collinearity in the subsequent sections","7522f44c":"### Intermediate Step - Checking and removing multi-collinearity","b7f12b5a":"Clearly both the attributes have skewness. While NHR has positive skewness indicating that the skewness is on the right. HNR has slight negative skewness indicating that the skewness is on the left.","f683ce98":"We will consolidate all the models into a table for quick reference and then make our observations<br>\nTo do the same, we will create summary of all the models and then call them inside a data frame","3808fb98":"##### Confusion Matrix: Bagging Classifier","3606e612":"From the dataset it is quite evident that different independent variables have different scales e.g: DB, %age, Hz etc. Now, Machine learning algorithms dont recognize the unit of data; Hence, it won't be prudent to compare DB with a %age or %age with Hz. Higher ranging numbers in one of the attributes will have superiority. \n10 DB and 10% means different but machine learning algorithm understand both to be the same.<br><br>\nScales impacts\n1. gradient descent based algorithms like Linear Regression, Logistics Regression\n2. Distance based algorithms like KNN, K-means and SVM\n\nScales dont impact:\n1. Tree based algorithms like Decision trees","12281cdf":"1. From the box plot above it appears that all the 3 attributes have skewness to the right. \n2. There are outliars for attribute MDVP:Fhi(Hz) and MDVP:Flo(Hz) <br>\nLets compute skewness","fb8c5de8":"##### Classification Report: Ada Boost","f875f23e":"##### Confusion Matrix: Na\u00efve Bayes","4b012d6b":"The confusion matrix: SVM\n\nTrue Positives (TP): We correctly predicted the count of subjects having Parkinson Disease and they actually had PD: 40\n\nTrue Negatives (TN): We correctly predicted the count of subjects not having Parkinson Disease and they actually dint have PD: 8\n\nFalse Positives (FP): We incorrectly predicted the count of subjects have Parkinson Disease but they actually dint have PD: 11\n\nFalse Negatives (FN):  We incorrectly predicted the count of subjects not having Parkinson Disease but they actually had PD: 0 ","5be8ed14":"1. From the box plot above it appears that DFA has slight left skewness; We'll compute the skewness in the next section\n2. There doesnt appear to be any outliars for DFA <br>","d29be60e":"The plots above represents the Two measures of ratio of noise to tonal components in the voice. Right skewness for NHR and left skewness for HNR is also evident from the plots. <br>","14f66e8e":"1. From the box plot above it appears that all the 5 attributes have skewness to the right. We'll compute the skewness in the next section\n2. There are outliars in all the attributes <br>","33a7c986":"The confusion matrix: Meta Classifier 2\n\nTrue Positives (TP): We correctly predicted the count of subjects having Parkinson Disease and they actually had PD: 39\n\nTrue Negatives (TN): We correctly predicted the count of subjects not having Parkinson Disease and they actually dint have PD: 12\n\nFalse Positives (FP): We incorrectly predicted the count of subjects have Parkinson Disease but they actually dint have PD: 7\n\nFalse Negatives (FN):  We incorrectly predicted the count of subjects not having Parkinson Disease but they actually had PD: 1 ","4a1970f5":"The confusion matrix: KNN\n\nTrue Positives (TP): We correctly predicted the count of subjects having Parkinson Disease and they actually had PD: 38\n\nTrue Negatives (TN): We correctly predicted the count of subjects not having Parkinson Disease and they actually dint have PD: 11\n\nFalse Positives (FP): We incorrectly predicted the count of subjects have Parkinson Disease but they actually dint have PD: 8\n\nFalse Negatives (FN):  We incorrectly predicted the count of subjects not having Parkinson Disease but they actually had PD:  2 ","8282ee1a":"The plots above represents the Several measures of variation in fundamental frequency. Right skewness is also evident from the plots. <br>","18258981":"##### Step 9.3: Summarising the case study and objective","814e4a38":"Clearly all the attributes in this group have some positive skewness indicating that the skewness is on the right.","365334e3":"Observation:\n1. The number of subjects who donot have Parkinson Disease is lot more than number of subjects who have parkinson Disease.\n2. This makes the attribute 'status' which is also target variable, imbalanced. Hence, accuracy might not be the best evaluation of peformance. We will also leverage Recall, Precision and F1 score.","2eb26b6f":"For all the measures of variation in amplitude, it is quite evident that range of variation is more for people having PD when compared to people not having PD. There seem to be more outliars for people having PD when compared to people not having PD. <br>\nFor all the attributes median of subjects diagnosed with Parkinson Disease is more than the median of subjects not diagnosed with Parkison Disease. ","3275ba26":"Observation:\n1. Meta classifier 2 has a recall of less than 1 which means that we were not able to identify all the subjects who had Parkinson Disease.","e41b69c2":"##### Confusion Matrix: Logistic Regression","0fbadee8":"##### Confusion Matrix: Meta Classifier 2","6fca97cd":"Refering the summary of the dataframe as above; In the dataset, all the columns appear to be of numerical data with data type Integer and float. There are no null values.<br>\n\nThe dataset contains the following variables:\n\nNumerical variables are as below:\n\n- MDVP:Fo(Hz) - Average vocal fundamental frequency\n- MDVP:Fhi(Hz) - Maximum vocal fundamental frequency\n- MDVP:Flo(Hz) - Minimum vocal fundamental frequency\n- MDVP:Jitter(%),MDVP:Jitter(Abs),MDVP:RAP,MDVP:PPQ,Jitter:DDP - Several measures of variation in fundamental frequency\n- MDVP:Shimmer,MDVP:Shimmer(dB),Shimmer:APQ3,Shimmer:APQ5,MDVP:APQ,Shimmer:DDA - Several measures of variation in amplitude\n- NHR,HNR - Two measures of ratio of noise to tonal components in the voice\n- status - Health status of the subject (one) - Parkinson's, (zero) - healthy\n- RPDE,D2 - Two nonlinear dynamical complexity measures\n- DFA - Signal fractal scaling exponent\n- spread1,spread2,PPE - Three nonlinear measures of fundamental frequency variation 9. car name: string (unique for each instance)\n\nThe Categorical nominal variable is :<br>\n\n- Name - ASCII subject name and recording number<br>","ba6d5e50":"For all the measures of variation in fundamental frequency, it is quite evident that range of variation is more for people having PD when compared to people not having PD. There seem to be more outliars for people having PD when compared to people not having PD.","b94baec0":"Observations and likely challenges in the dataset:\n1. MultiCollinearity: There seems to exist high correlation between multiple independent attributes. This might be a problem since, we would not be able to distinguish between the individual effects of the independent attributes on the dependent attribute. We'll try to identify the multi-collinearity by leveraging VIF (Variable Inflation Factor) and if it is found to be high then we will attempt at reducing the multi-collinearity by deleting few attributes with high VIF.\n2. Scaling: Different independent attributes have different scales e.g: DB, %age, Hz etc ","f96a45cd":"We will train the following classification algorithms:\n1. Logistic Regression\n2. KNN\n3. Naive Bayes\n4. SVM","6e98dc62":"<font color='blue'> __Attribute Group 2 - MDVP:Fo(Hz), MDVP:Fhi(Hz), MDVP:Flo(Hz) :__  <\/font> From the data description section (step 2.5) which captures basic statistical information, it appear that all the attributes of group 2 have right skewness; given that mean is greater than median. Lets perform EDA on Group 2.","e183c6bf":"#### Classification Model 4: SVM","c7c7719b":"Clearly all 5 attributes have skewness with attribute Jitter:DDP and MDVP:RAP having the highest skewness","4cf50e77":"The confusion matrix: Meta Classifier 1\n\nTrue Positives (TP): We correctly predicted the count of subjects having Parkinson Disease and they actually had PD: 40\n\nTrue Negatives (TN): We correctly predicted the count of subjects not having Parkinson Disease and they actually dint have PD: 10\n\nFalse Positives (FP): We incorrectly predicted the count of subjects have Parkinson Disease but they actually dint have PD: 9\n\nFalse Negatives (FN):  We incorrectly predicted the count of subjects not having Parkinson Disease but they actually had PD: 0 ","fdc308d9":"<font color='blue'> __Attribute Group 9: spread1, spread2, PPE:__  <\/font> From the data description section (step 2.5) which captures basic statistical information, it appear that for all the 3 attributes mean is slightly greater than median; hence it seems that the attributes have positive skewness. Lets perform EDA on Group 9.","c310be76":"Observation:\n1. XG boost has a recall of 1 which means that we were able to identify all the subjects who had Parkinson Disease.","f7d31ceb":"### Step8: Train at least one standard Ensemble model - Random forest, Bagging, Boosting etc, and note the accuracy","e62afbbe":"#### Step5.b: Identifying null values. ","fbabfe6f":"1. From the box plots above it appears that while all 3 attributes have positive skewness which means there is skewness on the right. We'll compute the skewness in the next section\n2. All the 3 attributes have few outliars <br>","29afb6d7":"### Pre-steps 1: Import the necessary libraries","76a786a9":"There are <font color='blue'>__24 attributes__ <\/font> of the voice samples of subjects; which are grouped into <font color='blue'>12 groups. <\/font> Let's analyse the dataset. <\/br>\n We will take the approach of understanding and analysing the attributes in the groups.<\/br>\n We are going to perform the following 5 analysis on all the attributes (except name and status - Target variable)\n \n 1. 5 point summary\n 2. Outliar detection\n 3. Skewness check\n 4. Spread across the attribute\n 5. Co-relation with other attributes\n \n \n For the target variable:\n \n 1. Comparison between Status of the subject (PD or no PD) and other attribute leveraging different plots \n ","eafd98dd":"Performance measures of Meta Classifier 2:\n1. The accuracy of the Meta Classifier 2 model is 0.86\n2. The recall of the Meta Classifier model is 0.97\n3. The precision of the Meta Classifier model is 0.85\n4. The F1 score of the Meta Classifier model is 0.91","45b42726":"<font color='blue'> __Attribute Group 8 DFA:__  <\/font> From the data description section (step 2.5) which captures basic statistical information, it appear that mean and median are close to each other; hence it seems that the attributes have negligible skewness. Lets perform EDA on Group 8.","24c49070":"Observation:\n1. Bagging has a recall of 1 which means that we were able to identify all the subjects who had Parkinson Disease.","9d245101":"DFA has slight negative skewness.","a6e41736":"1. From the box plot above it appears that while RPDE has some left skewness; D2 has some skewness on the right. We'll compute the skewness in the next section\n2. While RPDE doesnt have any outliars; There are outliars for D2 <br>","24fad0e4":"Spread1\n1. The range of variation is more for subjects having PD when compared to people not having PD.\n2. There seem to be outliars for subjects with PD; where there doesnt seem to be any outliars for subjects who don't have PD <br>\n\nSpread2 <br>\n\n1. The range of variation is more for subjects having PD when compared to people not having PD.\n2. There are outliars for subjects who dont have PD.\n\nPPE <br>\n\n1. The range of variation is more for subjects having PD when compared to people not having PD.\n2. There are outliars for both the status. The outliars are on the right hand side.","4642594e":"Performance measures of Random Forest:\n1. The accuracy of the Random Forest model is 0.86\n2. The recall of the Random Forest model is 1.00\n3. The precision of the Random Forest model is 0.83\n4. The F1 score of the Random Forest model is 0.91","d014960d":"Performance measures of Bagging:\n1. The accuracy of the Bagging model is 0.88\n2. The recall of the Bagging model is 1.00\n3. The precision of the Bagging model is 0.85\n4. The F1 score of the Bagging model is 0.92","dd298f5b":"From the 5 point summary above: <br>\n1. RPDE: \n    - The range of RPDE is [0.256570,0.685151] with a median of 0.495954. The mean (from section2.5) is close to median which means that there is negligible skewness.\n    - There doesnt seem to be any outliars. We will plot a box plot to confirm the same. <br><br>\n\n2. D2: \n    - The range of D2 is [1.423287,3.671155] with a median of 2.361532. The mean (from section2.5) is close to median which means that there is negligible skewness.\n    - There doesnt seem to be any outliars. We will plot a box plot to confirm the same. <br><br>","5769c9e6":"### Step1: Load the dataset","b5b4e63c":"##### Classification Report: Meta Classifier 2","e3e0dd49":"Rationale behind leveraging Machine Learning Algorithm for predicting Parkinson Disease in subjects: <br>\nPD patients exhibit characteristic vocal features and voice recordings are a useful and non-invasive tool for diagnosis.<br>\nSo this case study aims at identifying if Machine learning algorithms can be applied to a voice recording dataset to accurately diagnosis PD;<br> if this is successful then this would be an effective screening step prior to an appointment with a clinician; <br>","158c5a4f":"### Step4: Split the data into training and test set in the ratio of 70:30 respectively","987d6db6":"The plots above represents the Several measures of variation in amplitude. Right skewness is also evident from the plots. <br>","e2e80400":"##### Classification Report: Bagging Classifier","6874f901":"##### Classification Report: Meta Classifier 1","7ac92a05":"Performance measures of XG boost:\n1. The accuracy of the XG boost model is 0.90\n2. The recall of the XG boost model is 1.0\n3. The precision of the XG boost model is 0.87\n4. The F1 score of the XG boost model is 0.93","750ae576":"The confusion matrix: Bagging Classifier\n\nTrue Positives (TP): We correctly predicted the count of subjects having Parkinson Disease and they actually had PD: 40\n\nTrue Negatives (TN): We correctly predicted the count of subjects not having Parkinson Disease and they actually dint have PD: 12\n\nFalse Positives (FP): We incorrectly predicted the count of subjects have Parkinson Disease but they actually dint have PD: 7\n\nFalse Negatives (FN):  We incorrectly predicted the count of subjects not having Parkinson Disease but they actually had PD: 0 ","168ee06d":"##### Confusion Matrix: XGB Classifier","34cf7ae2":"NHR\n1. The range of variation is more for subjects having PD when compared to people not having PD.\n2. There seem to be more outliars for subjects having PD when compared to people not having PD. \n3. Median of subjects diagnosed with Parkinson Disease is more than the median of subjects not diagnosed with Parkison Disease. <br>\n\nHNR <br>\n\n1. The range of variation is more for subjects having PD when compared to people not having PD. However, ratio of noise to tonal component in voice seem to be more for subject not having PD.\n2. The density of outliars seems to be similar for subjects having PD and subjects not having PD. Density of Outliars for subjects not having PD seems to be on the right and the density of outliars for subjects having PD seems to be on the left.\n3. Median of subjects diagnosed with Parkinson Disease is less than the median of subjects not diagnosed with Parkison Disease. ","5085821b":"spread1 and PPE seem to have correlation with most of other attributes. As highlighted earlier, we will check multi-collinearity in the data and try to remediate it.\nspread2 seems to have some correlation with few of the variables. This will also become clear when compute multi-collinearity","70773f6e":"Observation:\n1. Meta classifier 1 has a recall of 1 which means that we were able to identify all the subjects who had Parkinson Disease.","7fd4c7b2":"##### Confusion Matrix: KNN","7a4a0808":"There seems to be a high correlation between multiple independent variables.","b9d32e0d":"Observation:\n1. The recall of the SVM model is 1; which means we were able to idetify all the subjects who had the disease.","02562f90":"RPDE\n1. The range of variation is more for subjects having PD when compared to people not having PD.\n2. There doesnt seem to be any outliars for both the status. \n3. Median of subjects diagnosed with Parkinson Disease is more than the median of subjects not diagnosed with Parkison Disease. <br>\n\nD2 <br>\n\n1. The range of variation is more for subjects having PD when compared to people not having PD.\n2. There are outliars for both the status. Outliars for subjects having PD seems to be on the right and the outliars for subjects not having PD seems to be on the left.\n3. Median of subjects diagnosed with Parkinson Disease is more than the median of subjects not diagnosed with Parkison Disease. "}}