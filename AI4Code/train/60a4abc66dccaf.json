{"cell_type":{"9ab4de64":"code","c5c1015d":"code","448f4bcc":"code","7f18a150":"code","6d743d35":"code","0d91c235":"code","a094eb66":"code","3c6b5b94":"code","0c0684e2":"code","4f953965":"code","3a332ec8":"code","3b21f4c6":"code","47969566":"code","cf3920b2":"code","d87a74f8":"code","3b678820":"code","79153154":"code","4b69fdd5":"code","84870eb7":"code","e01cc43f":"code","f971d412":"code","618971d6":"code","74a14ec3":"code","9bb3f033":"code","486ec802":"code","f9c03634":"markdown","8cd455a9":"markdown","1ced18ca":"markdown","a5563bdc":"markdown","a895744a":"markdown","2b2e1377":"markdown","8864362f":"markdown","fe566278":"markdown"},"source":{"9ab4de64":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c5c1015d":"df1 = pd.read_json(\"\/kaggle\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json\",lines = True)\ndf2 = pd.read_json(\"\/kaggle\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json\", lines = True)","448f4bcc":"df1.info()","7f18a150":"df2.info()","6d743d35":"df = pd.concat([df1,df2])","0d91c235":"df.info()","a094eb66":"# Removing the article link from the Dataset\ndf = df.drop([\"article_link\"], axis=1)\ndf.head()","3c6b5b94":"df.isnull().sum()","0c0684e2":"%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.countplot(x = \"is_sarcastic\", data = df)\nplt.title(\"Data Distribution\")","4f953965":"from nltk.tokenize import RegexpTokenizer\n# With the help of NLTK tokenize.regexp() module, we are able to extract the tokens from string by using regular expression with RegexpTokenizer() method.\n\nfrom nltk.corpus import stopwords\n# A stop word is a commonly used word (such as \u201cthe\u201d, \u201ca\u201d, \u201can\u201d, \u201cin\u201d) \n\nfrom nltk.stem import PorterStemmer \n# The reason why we stem is to shorten the lookup, and normalize sentences.\n# Consider: 1) I was taking a ride in the car. 2) I was riding in the car. These sentence means the same thing.","3a332ec8":"stop_words = set(stopwords.words(\"english\"))\nstemmer = PorterStemmer()\ntokenizer = RegexpTokenizer(r'\\w+')\n\n\ndef preprocess(text):\n  word_list = []\n  tok = tokenizer.tokenize(text)\n  for word in tok:\n    if word not in stop_words:\n      word_list.append(stemmer.stem(word))\n  return \" \".join(word_list)","3b21f4c6":"x_data = df[\"headline\"].apply(preprocess)\nx_data.tail()","47969566":"# Seperating Sarcastic news in a single DataFrame\n\nSarNews = df[(df['is_sarcastic'])==1]\nSarNews","cf3920b2":"# Python program to generate WordCloud for sarcastic news\n  \nfrom wordcloud import WordCloud, STOPWORDS\nwords = '' \nstopwords = set(STOPWORDS) \n  \n# iterate through the .json file \nfor val in SarNews.headline: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    words += \" \".join(tokens)+\" \"\n  \nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show() ","d87a74f8":"# Seperating Non-Sarcastic news in a single DataFrame\n\nNonSarNews = df[(df['is_sarcastic'])==0]\nNonSarNews","3b678820":"# Python program to generate WordCloud for sarcastic news\n  \nwords = '' \nstopwords = set(STOPWORDS) \n  \n# iterate through the .json file \nfor val in NonSarNews.headline: \n      \n    # typecaste each val to string \n    val = str(val) \n  \n    # split the value \n    tokens = val.split() \n      \n    # Converts each token into lowercase \n    for i in range(len(tokens)): \n        tokens[i] = tokens[i].lower() \n      \n    words += \" \".join(tokens)+\" \"\n  \nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='white', \n                stopwords = stopwords, \n                min_font_size = 10).generate(words) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \n  \nplt.show()","79153154":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x_data, df['is_sarcastic'].values, test_size=0.3, random_state=72)","4b69fdd5":"# Using tf-idf to further minimize the weight of unnecessary words.\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer()\nx_train_tfidf = vectorizer.fit_transform(X_train)\nx_test_tfidf = vectorizer.transform(X_test)","84870eb7":"df_idf = pd.DataFrame(vectorizer.idf_, index=vectorizer.get_feature_names(),columns=[\"idf_weights\"])\n \n# sort ascending\ndf_idf.sort_values(by=['idf_weights']).head()","e01cc43f":"df_idf.sort_values(by=['idf_weights']).tail()","f971d412":"from sklearn.ensemble import RandomForestClassifier\nrf = RandomForestClassifier()\nrf.fit(x_train_tfidf,y_train)\nprint(f\"Training Score : {rf.score(x_train_tfidf, y_train)}\")\nprint(f\"Test Score : {rf.score(x_test_tfidf, y_test)}\")","618971d6":"from sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\n\npred = rf.predict(x_test_tfidf)\nprint(confusion_matrix(y_test,pred))\n","74a14ec3":"# Precision\n# tp \/ (tp + fp)\n\npre = (6779\/(6779 + 460))\nprint(pre)","9bb3f033":"# Recall\n# tp \/ (tp + fn)\n\nrec = (6779\/(6779 + 857))\nprint(rec)","486ec802":"# F1 = 2 * (precision * recall) \/ (precision + recall)\n\nf1 = (2 * (pre * rec) \/ (pre + rec))\nprint(f1)","f9c03634":"# Word Cloud creation for Sarcastic headlines","8cd455a9":"# Cleaning the dataset","1ced18ca":"# Model metrics like Confusion metrics \/Accuracy \/F1 Score","a5563bdc":"# Word Cloud creation for Non-Sarcastic headlines","a895744a":"# TF IDF vectorizations","2b2e1377":"# Read the dataset and merge the files","8864362f":"# Train test Split 70-30 ","fe566278":"# Model Creations \/ Hyper Tuning"}}