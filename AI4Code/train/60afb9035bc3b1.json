{"cell_type":{"adaa7f0b":"code","7db4c6dc":"code","e8148805":"code","2d44d723":"code","dece25db":"code","558b4dc0":"code","d42724c4":"code","828c1920":"code","5c8d1f78":"code","fe9c9eac":"code","ba062c9d":"code","dd9b1cab":"code","5594cafd":"code","63c6e364":"code","207f5b68":"code","5406627c":"code","6c9641a6":"code","d3c9765b":"code","934de72f":"code","7a32913e":"code","8de0b454":"code","f9ac1f5e":"code","5fb89411":"code","648e3433":"code","51634d1c":"code","abd9348e":"code","c710efce":"code","94b75351":"code","8eab802b":"code","7a5cd36d":"markdown","f36a0aac":"markdown","f5423fa1":"markdown","8a39efe9":"markdown","970287b6":"markdown","69e5192c":"markdown","1c678047":"markdown","c7eb0d6e":"markdown","95a288a7":"markdown"},"source":{"adaa7f0b":"# version 1.1\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","7db4c6dc":"import warnings\nwarnings.filterwarnings(\"ignore\")","e8148805":"DF_Dieabets = pd.read_csv(\"\/kaggle\/input\/dataset\/diabetes.csv\")\nDF_Dieabets.shape","2d44d723":"DF_Dieabets.head()","dece25db":"DF_Dieabets.info()","558b4dc0":"DF_Dieabets.describe()","d42724c4":"DF_Dieabets.loc[DF_Dieabets.duplicated()]","828c1920":"DF_Dieabets.isnull().sum()","5c8d1f78":"Columns = DF_Dieabets.columns\nColumns","fe9c9eac":"for Col in Columns:\n    Uniques = DF_Dieabets[Col].unique()\n    print(\"Uniques Data Of {} is : \".format(Col))\n    print(np.sort(Uniques))\n    print(\"Minimum Is : {} And Maximum Is : {} \".format(Uniques.min(),Uniques.max()))\n    print()","ba062c9d":"fig = px.box(DF_Dieabets)\nfig.update_xaxes()\nfig.show()","dd9b1cab":"for Col in Columns:\n    UperBound = DF_Dieabets[Col].mean()+(3*DF_Dieabets[Col].std())\n    LowerBound = DF_Dieabets[Col].mean()-(3*DF_Dieabets[Col].std())\n    DF_Dieabets[Col][(DF_Dieabets[Col]<LowerBound) | (DF_Dieabets[Col]>UperBound)] = 0\n","5594cafd":"Col_Editable = ['Glucose', 'BloodPressure', 'BMI', 'DiabetesPedigreeFunction', 'Age']\nfor Col in Col_Editable:\n    if DF_Dieabets[Col].dtype == np.int64:\n        DF_Dieabets[Col][DF_Dieabets[Col]==0] = round(DF_Dieabets[Col].mean())\n    elif DF_Dieabets[Col].dtype == np.float64:\n        DF_Dieabets[Col][DF_Dieabets[Col]==0] = round(DF_Dieabets[Col].mean(),3)\n","63c6e364":"DF_Dieabets[\"Insulin\"][DF_Dieabets[\"Insulin\"]==0] = np.NAN\nDF_Dieabets[\"SkinThickness\"][DF_Dieabets[\"SkinThickness\"]==0] = np.NAN\nSkinThickness = DF_Dieabets[\"SkinThickness\"][DF_Dieabets[\"SkinThickness\"]>0]\nInsulin = DF_Dieabets[\"Insulin\"][DF_Dieabets[\"Insulin\"]>0]","207f5b68":"NV_SkinThickness = DF_Dieabets['SkinThickness'][DF_Dieabets['SkinThickness'].isna()].index\nNV_Insulin = DF_Dieabets['Insulin'][DF_Dieabets['Insulin'].isna()].index\nfor NV in NV_SkinThickness:\n    DF_Dieabets.loc[NV,\"SkinThickness\"] = np.random.randint(SkinThickness.min(),SkinThickness.max())\nfor NV in NV_Insulin:\n    DF_Dieabets.loc[NV,\"Insulin\"] = np.random.randint(Insulin.min(),Insulin.max())\nDF_Dieabets.describe()","5406627c":"Lable_Columns = DF_Dieabets.columns\nfig, axes = plt.subplots(len(Lable_Columns),2, figsize=(20, 30))\nfor i,L in enumerate(Lable_Columns):\n    sns.boxplot(x=DF_Dieabets[L],data=DF_Dieabets,color='r',saturation=1,ax=axes[i,0])\n    sns.distplot(x=DF_Dieabets[L],color='b',ax=axes[i,1])","6c9641a6":"plt.figure(figsize=(12,9),)\nsns.heatmap(DF_Dieabets.corr(),annot=True,fmt='0.2',linewidths=0.1,cmap=\"RdBu\")","d3c9765b":"sns.countplot(DF_Dieabets[\"Outcome\"])","934de72f":"X_DF_Dieabets = DF_Dieabets.drop(\"Outcome\", axis=1)\nY_DF_Dieabets = DF_Dieabets[\"Outcome\"]\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X_DF_Dieabets,Y_DF_Dieabets, test_size=0.3, train_size=0.7, random_state=77, shuffle=(True), stratify=(Y_DF_Dieabets))","7a32913e":"from sklearn.utils import class_weight\nClass_weights = class_weight.compute_class_weight('balanced', np.unique(Y_train), Y_train)\nClass_weights = dict(zip(np.unique(Y_train), class_weight.compute_class_weight('balanced', np.unique(Y_train), Y_train))) \nClass_weights","8de0b454":"Features = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin','BMI', 'DiabetesPedigreeFunction', 'Age']\nfrom sklearn.preprocessing import MinMaxScaler\nScaler = MinMaxScaler()\nX_train = Scaler.fit_transform(X_train)\nX_test = Scaler.transform(X_test)","f9ac1f5e":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics\n\nK_Range = list(range(1,20))\nTrain_Scores = []\nTest_Scores = []\n\nfor K in K_Range:\n    KNN = KNeighborsClassifier(n_neighbors=K , metric='minkowski', p=1) #Defult p is 1 means euclidian 2 is Manhatan Distance\n    KNN.fit(X_train,Y_train)\n    Predicted_Types_KNN = KNN.predict(X_test)\n    Train_Scores.append(KNN.score(X_train,Y_train))\n    Test_Scores.append(KNN.score(X_test,Y_test))\n\nplt.figure(figsize=(12,5))\np = sns.lineplot(K_Range,Train_Scores,marker='*',label='Train Score')\np = sns.lineplot(K_Range,Test_Scores,marker='o',label='Test Score')","5fb89411":"from sklearn.tree import DecisionTreeClassifier\n\nDT = DecisionTreeClassifier()\nDT.fit(X_train,Y_train)\nPredicted_Types_DT = DT.predict(X_test)\nprint(\"Train Score : \",DT.score(X_train,Y_train))\nprint(\"Test  Score : \",DT.score(X_test,Y_test))","648e3433":"from sklearn.naive_bayes import GaussianNB\n\nGNB = GaussianNB()\nGNB.fit(X_train,Y_train)\nPredicted_Types_GNB = GNB.predict(X_test)\nprint(\"Train Score : \",GNB.score(X_train,Y_train))\nprint(\"Test  Score : \",GNB.score(X_test,Y_test))","51634d1c":"from sklearn.linear_model import LogisticRegression\n\n# LR = LogisticRegression(class_weight=Class_weights)\nLR = LogisticRegression()\nLR.fit(X_train,Y_train)\nPredicted_Types_LR = LR.predict(X_test)\nprint(\"Train Score : \",LR.score(X_train,Y_train))\nprint(\"Test  Score : \",LR.score(X_test,Y_test))","abd9348e":"from sklearn.ensemble import RandomForestClassifier\nRFC = RandomForestClassifier(n_estimators=50)\nRFC.fit(X_train,Y_train)\nPredicted_Types_RFC = RFC.predict(X_test)\nprint(\"Train Score : \",RFC.score(X_train,Y_train))\nprint(\"Test  Score : \",RFC.score(X_test,Y_test))","c710efce":"from sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV\n\nSVM =SVC(class_weight=Class_weights)\n\n# param = [{\"kernel\" : [\"linear\"] ,\"C\" : [0.01 , 0.1, 1, 10, 100]}]\nparam = [{\"kernel\" : [\"linear\"] ,\"C\" : [0.01 , 0.1, 1, 10, 100]},\n         {\"kernel\" : [\"rbf\"], \"gamma\" : [0.01, 0.1, 0.2, 0.3], \"C\":  [0.01 , 0.1, 1, 10, 100]},\n         {\"kernel\" : [\"poly\"], \"degree\": [2], \"C\": [0.01, 0.1, 1, 10, 100]}]\n\nGS = GridSearchCV(SVM, param, cv=5, scoring=\"accuracy\")\nGS.fit(X_DF_Dieabets,Y_DF_Dieabets)\nprint(GS.best_score_)\nprint(GS.best_params_)","94b75351":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression()\nLR.fit(X_train,Y_train)\nPredicted_Types_LR = LR.predict(X_test)\nprint(\"Train Score : \",LR.score(X_train,Y_train))\nprint(\"Test  Score : \",LR.score(X_test,Y_test))","8eab802b":"from sklearn.model_selection import cross_val_predict,cross_val_score\n\nScore_DT = cross_val_score(DT,X_DF_Dieabets,Y_DF_Dieabets,cv=10)\nprint(Score_DT)\nScore_KNN = cross_val_score(KNN,X_DF_Dieabets,Y_DF_Dieabets,cv=10)\nprint(Score_KNN)","7a5cd36d":"# **Decision Tree Algorithm**","f36a0aac":"# **SVM Algorithm**","f5423fa1":"# **Nave Beyes Classifier**","8a39efe9":"# **Replacing Outliers With Zero**","970287b6":"# **Logistic Regression Algorithm**","69e5192c":"# **Replacing Zeroes with Mean**","1c678047":"# **KNN Algorithm**","c7eb0d6e":"# **Random Forest Algorithm**","95a288a7":"# **Cross Validation**"}}