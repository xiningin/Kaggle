{"cell_type":{"95d17d89":"code","dd9afd3d":"code","4a09e815":"code","d893df5d":"code","aee7516e":"code","a1210417":"code","64bb5e0b":"code","a7a23762":"code","debd007f":"markdown","99474335":"markdown","abbf588b":"markdown","430e05eb":"markdown","0809cc04":"markdown","ae8b1c81":"markdown","cd1eb9d6":"markdown","9392931c":"markdown","2c980bd1":"markdown","a5ed921b":"markdown","2c014a6b":"markdown","b3df2dda":"markdown","4c70d38b":"markdown","fda35ec9":"markdown","31866e3d":"markdown","5c55da02":"markdown","167315bb":"markdown","16823915":"markdown","ab3b7cf5":"markdown","e50aedb4":"markdown","27273e5c":"markdown","e9451c37":"markdown","6b8952cc":"markdown"},"source":{"95d17d89":"# Importing Libraries...\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\nimport warnings\nfrom sklearn.datasets import make_classification\n\nwarnings.filterwarnings('ignore')\nplt.style.use('fivethirtyeight')","dd9afd3d":"# Creating a dataset\nage=[22,25,37,36,46,50,52,51,18,28,27,29,39,45,48,19,18,21,26,34,35,40,44,23]\nloan_outcome = [0,0,1,1,1,1,1,1,0,0,0,0,1,1,1,0,0,0,0,1,1,1,1,0]\ndata = pd.DataFrame({'age':age, 'loan outcome':loan_outcome})\n\n# Plotting the figure\nplt.figure(figsize=(12,6))\nsns.scatterplot(x=data.age, y=data['loan outcome'], s=250)\nm=sum((data.age-data.age.mean())*(data['loan outcome'] - data['loan outcome'].mean()))\/sum((data.age-data.age.mean())**2) # Slope\nb=data['loan outcome'].mean()-(m*data.age.mean()) # Intercept\nline = (m * data.age) + b\nplt.plot(data.age, line, c='red')\nplt.axhline(y=0.5, ls='--', color='black', linewidth=2, xmax=0.44, xmin=0.05)\nplt.axvline(x=32.8, ls='--', color='black', linewidth=2, ymax=0.50, ymin=0.1)\nplt.text(x=20, y=0.55, s='threshold: 0.5', color='k', alpha=1, weight='bold')\nplt.scatter(x=32.8,y=0,s=200,color='k')\nplt.ylim(-0.1,1.1)\nplt.show()","4a09e815":"# Creating a dataset\nage=[22,25,37,36,46,50,52,51,18,28,27,29,39,45,48,19,18,21,26,34,35,40,44,23,80,80,80,80,80,80,80,80,80,80]\nloan_outcome = [0,0,1,1,1,1,1,1,0,0,0,0,1,1,1,0,0,0,0,1,1,1,1,0,1,1,1,1,1,1,1,1,1,1]\ndata = pd.DataFrame({'age':age, 'loan outcome':loan_outcome})\n\n# Plotting the figure\nplt.figure(figsize=(12,6))\nsns.scatterplot(x=data.age, y=data['loan outcome'], s=250)\nm=sum((data.age-data.age.mean())*(data['loan outcome'] - data['loan outcome'].mean()))\/sum((data.age-data.age.mean())**2) # Slope\nb=data['loan outcome'].mean()-(m*data.age.mean()) # Intercept\nline = (m * data.age) + b\nplt.plot(data.age, line, c='red')\nplt.axhline(y=0.5, ls='--', color='black', linewidth=2, xmax=0.3, xmin=0.05)\nplt.axvline(x=31.8, ls='--', color='green', linewidth=2, ymax=1)\nplt.axvline(x=35.6, ls='--', color='black', linewidth=2, ymax=1, ymin=0.1)\nplt.text(x=20, y=0.55, s='threshold: 0.5', color='k', alpha=1, weight='bold')\nplt.scatter(x=35.6,y=0,s=200,color='k')\nplt.ylim(-0.1,1.1)\nplt.show()","d893df5d":"x, y = make_classification(n_samples=190, n_features=2, n_informative=1,n_redundant=0,\n                           n_classes=2, n_clusters_per_class=1, random_state=41,hypercube=False,class_sep=15)\ndata = pd.DataFrame({'feature1':x[:,0], 'feature2':x[:,1], 'target':y})\n\ndata.head()","aee7516e":"plt.figure(figsize=(18,8))\nsns.scatterplot(x=data.feature1, y=data.feature2, hue=data.target,\n               s=180, alpha=0.9)\nplt.xticks(ticks=[])\nplt.xlabel(xlabel=None)\nplt.yticks(ticks=[])\nplt.ylabel(ylabel=None)\nplt.legend(shadow=True)\nplt.show()","a1210417":"# Sigmoid function\ndef sigmoid(z):\n    return 1\/(1 + np.exp(-z))\n\n# Gradient descent\ndef gradient_descent(x, y):\n    # Inserting new col with ones for intercept(w0)\n    x = np.insert(x, 0, 1, axis=1)\n    # initial weights\n    weights = np.ones(x.shape[1])\n    # learning rate\n    lr = 0.5\n    \n    for i in range(5000):\n        # predictions\n        y_hat = sigmoid(np.dot(x, weights))\n        \n        # updating the weights using gradient descent\n        weights = weights + lr * (np.dot((y - y_hat), x) \/ x.shape[0])\n        \n    return weights[1:], weights[0]\n        ","64bb5e0b":"coef_,intercept_ = gradient_descent(x,y)\n# Slope\nm = -(coef_[0]\/coef_[1])\n# Y-intercept\nb = -(intercept_\/coef_[1])\nprint(f'Slope: {m}')\nprint(f'intercept: {b}')","a7a23762":"plt.figure(figsize=(18, 8))\nsns.scatterplot(x=data.feature1, y=data.feature2, hue=data.target,\n               s=180, alpha=0.9)\n\n# Decision boundary\nx_input = np.linspace(-1, 1, 10)\ny_input = m * x_input + b\nplt.plot(x_input, y_input, color='black', linewidth=3, label='Decision boundary')\n\nplt.xticks(ticks=[])\nplt.xlabel(xlabel=None)\nplt.yticks(ticks=[])\nplt.ylabel(ylabel=None)\nplt.legend(labels=['Decision boundary'], shadow=True, loc=2)\nplt.xlim(-3.5,2)\nplt.ylim(-3.1,3)\nplt.show()","debd007f":"# <center><span style=\"font-family:cursive;\">Cost function<\/span><\/center>\n<p> What is cost function? Cost functions are used in machine learning to estimate how poorly models perform. Simply put, a cost function is a measure of how inaccurate the model is in estimating the connection between X and y. This is usually stated as a difference or separation between the expected and actual values. A machine learning model\u2019s goal is to discover parameters, weights, or a structure \nthat minimizes the cost function <\/p>\n\n#### $$ cost function = (-1\/m)\\sum\\limits_{i=1}^{m} y_i log (\\hat{y_i}) + (1-y_i)log(1-\\hat{y_i}) $$\n\n<p>Linear regression employs the Least Squared Error as the loss function, which results in a convex network, which we can then optimize by identifying the vertex as the global minimum. For logistic regression, however, it is no longer a possibility. Because the hypothesis has been modified, calculating Least Squared Error using the sigmoid function on raw model output will result in a non-convex graph with local minimums.<\/p>\n","99474335":"# <center><span style=\"font-family:cursive;\">Logistic Regression from scratch<\/span><\/center>","abbf588b":"<h3 style=\"font-family:serif\">Plotting decision boundary<\/h3>","430e05eb":"<p><b>It\u2019s called \u2018Logistic Regression\u2019 since the technique behind it is quite similar to Linear Regression. The name \u201cLogistic\u201d comes from the Logit function.<\/b><\/p>\n<p>Logistic Regression is a Machine Learning method that is used to solve classification issues. It is a predictive analytic technique that is based on the probability idea. The classification algorithm Logistic Regression is used to predict the likelihood of a categorical dependent variable. The dependant variable in logistic regression is a binary variable with data coded as 1 (yes, True, normal, success, etc.) or 0 (no, False, abnormal, failure, etc.).<\/p> \n<p>The goal of Logistic Regression is to discover a link between characteristics and the likelihood of a specific outcome. For example, when predicting whether a student passes or fails an exam based on the number of hours spent studying, the response variable has two values: pass and fail.<\/p>\n<p>A Logistic Regression model is similar to a Linear Regression model, except that the Logistic Regression utilizes a more sophisticated cost function, which is known as the \u201cSigmoid function\u201d or \u201clogistic function\u201d instead of a linear function.<p\/>\n<p>Many people may have a question, whether Logistic Regression is a classification or regression category. The logistic regression hypothesis suggests that the cost function be limited to a value between 0 and 1. As a result, linear functions fail to describe it since it might have a value larger than 1 or less than 0, which is impossible according to the logistic regression hypothesis.<\/p>\n","0809cc04":"<div class=\"alert alert-block alert-warning\"> <h4>\ud83d\udccc Note : <\/h4>\n    <div>\n        <p> Assume we have information about Age and loan outcome. Because this is a classification issue, we can see that all the values will fall between 0 and 1. And, by fitting the best-found regression line and assuming a threshold of 0.5, we can do a very good job with the line.<\/p>\n        <p>We can choose a point on the x-axis from which all values on the left side are regarded as negative, and all values on the right side are considered positive.<\/p>\n    <\/div>    \n<\/div>","ae8b1c81":"# <center><span style=\"font-family:cursive;\">What is classification ? <\/span><\/center>\n<p><b>Classification is a process of categorizing a given set of data into classes, It can be performed on both structured or unstructured data. The process starts with predicting the class of given data points. The classes are often referred to as target, label or categories.<\/b><\/p>\n<p><b>For example<\/b>, spam detection in email service providers can be identified as a classification problem. This is s binary classification since there are only 2 classes as spam and not spam. A classifier utilizes some training data to understand how given input variables relate to the class. In this case, known spam and non-spam emails have to be used as the training data. When the classifier is trained accurately, it can be used to detect an unknown email.<\/p>\n<p>Classification belongs to the category of supervised learning where the targets also provided with the input data. There are many applications in classification in many domains such as in credit approval, medical diagnosis, target marketing etc.<\/p>\n","cd1eb9d6":"# <center><span style=\"font-family:cursive;\">Sigmoid function<\/span><\/center>\n<p>Sigmoid function produces an S-shaped curve. It always returns a probability value between 0 and 1. The Sigmoid function is used to convert expected values to probabilities. The function converts any real number into a number between 0 and 1. We utilize sigmoid to translate predictions to probabilities in machine learning.\n\nMathematically sigmoid function can be,<\/p>\n### $$ \\hat{y_i} = {1 \\over 1 + e^{ - (x) }} $$\n$ where, $\n> $ x = w_0 + w_1x_1 + w_2x_2 + . . .  + w_nx_n $","9392931c":"# <center><span style=\"font-family:cursive;\">Logistic Regression<\/span><\/center>","2c980bd1":"# <center><span style=\"font-family:cursive;\">Derivative of cost function<\/span><\/center>\n\n#### $$ derivative (cost function)_j = \\sum\\limits_{i=1}^{m} (\\hat{y}-y)x^i_j $$\n<h4 style=\"font-family:serif\"><b>Updating the weights using:  <\/b><\/h4> <h4>$ w_{new} = w_{old} + \\alpha * derivative(cost function)$ <\/h4>\n$where,$ \n\n$ \\alpha = learning rate $","a5ed921b":"![](https:\/\/images.saymedia-content.com\/.image\/t_share\/MTgxNDY3NDE3NTI3NDYxNjQy\/why-logistic-regression-why-not-logistic-classification.jpg)","2c014a6b":"<h4 style=\"font-family:serif\"><b>But what if the data contains an outlier? Things would become shambles. For 0.5 thresholds<\/b><\/h4>","b3df2dda":"<div class=\"alert alert-block alert-warning\"> <h4>\ud83d\udccc Note : <\/h4>\n    <div>\n        <p>The goal of the logistic regression algorithm is to create a linear decision boundary separating two classes from one another. <\/p>\n    <\/div>    \n<\/div>","4c70d38b":"<h3 style=\"font-family:serif\">Plotting the data<\/h3>","fda35ec9":"![sigmoid.png](attachment:50b3c660-f0c9-4d30-8e1c-026b31c57cd8.png)","31866e3d":"<h3 style=\"font-family:serif\">Logistic Regression algorithm using gradient descent<\/h3>","5c55da02":"<h3 style=\"font-family:serif\">Getting slope and Y-intercept<\/h3>","167315bb":"![](https:\/\/datapeaker.com\/wp-content\/uploads\/2021\/08\/221055cc7604b063fb0c7d541a527_5cbeb12eae2b883405064bc1_two-choice-7704557.jpeg)\n","16823915":"# <center><span style=\"font-family:cursive;\">Using Linear Regression instead of Logistic Regression<\/span><\/center>","ab3b7cf5":"![](https:\/\/editor.analyticsvidhya.com\/uploads\/3177511.png)","e50aedb4":"<div class=\"alert alert-block alert-warning\"> <h4>\ud83d\udccc Note : <\/h4>\n    <div>\n        <p>Even if we fit the best-found regression line, we won\u2019t be able to determine any point where we can distinguish classes. It will insert some instances from the positive class into the negative class. The black dotted line (Decision Boundary) separates loan amount and age, however, it should have been a green dotted line that clearly separates the positive and negative cases. As a result, even a single outlier can throw the linear regression estimates off. And it\u2019s here that logistic regression comes into play.<\/p>\n    <\/div>    \n<\/div>","27273e5c":"# <center><span style=\"font-family:cursive;\">Coding Logistic Regression from scratch<\/span><\/center>","e9451c37":"<h3 style=\"font-family:serif\">Creating a new data using sklearn<\/h3>","6b8952cc":"One thing to keep in mind about linear regression is that it only works with continuous data. If we want to include linear regression in our classification methods, we\u2019ll have to adjust our algorithm a little more. First, we must choose a threshold so that if our projected value is less than the threshold, it belongs to class 1; otherwise, it belongs to class 2.\n\n"}}