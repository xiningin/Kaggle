{"cell_type":{"086e4ce3":"code","3edd66df":"code","11794508":"code","eb35880e":"code","fd87098c":"code","54ff0234":"code","8d2f4ab7":"code","2d1c2f85":"code","b7abe38b":"code","ac547bc4":"code","ff357fb6":"code","956da5d4":"code","d8fbb0f6":"code","09e51e67":"code","100ff0b5":"code","e35b73e3":"code","9ededdc3":"code","d90ab7ce":"code","c70316d7":"code","c32233de":"code","545c9881":"code","9c9cfb0f":"code","f5f0fbd1":"code","f1c7dd4e":"code","27ac1a37":"code","031ce7e9":"code","cf61836b":"code","44c7d5c0":"code","b4976f1d":"code","d6627611":"code","626c2ffb":"code","2d16284b":"code","900ed882":"code","e3fa70b0":"code","b7d7e4d4":"code","c1c67136":"code","6b0deeb1":"code","140e2d8f":"code","f886051f":"code","4d1f3ffb":"code","083051ed":"code","edaa5fc6":"code","e1d08bac":"code","1ea55a1c":"code","ddee37ab":"code","c2fa97a5":"code","8ae2f08a":"code","60d60c24":"code","3f024ff3":"markdown","951eb90b":"markdown","71c6d3d8":"markdown"},"source":{"086e4ce3":"import numpy as np\nimport pandas as pd\n\n\nimport scipy as sp\nimport matplotlib.pyplot as plt\n\nimport lightgbm as lgb\nimport xgboost as xgb\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\nfrom sklearn.metrics import accuracy_score, f1_score, roc_auc_score, confusion_matrix\n#from sklearn.metrics import mean_squared_error, r2_score\n\n#from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nimport time\n\n%matplotlib inline","3edd66df":"from bayes_opt import BayesianOptimization","11794508":"df_train = pd.read_csv('..\/input\/train.csv')\nprint(df_train.info())\ndf_train.head()","eb35880e":"df_test = pd.read_csv('..\/input\/test.csv')\nprint(df_test.info())\ndf_test.head()","fd87098c":"df = pd.concat([df_train, df_test], sort=False)","54ff0234":"df.head()","8d2f4ab7":"df[9550:9560]","2d1c2f85":"columns = list(df.columns)\nprint(columns)","b7abe38b":"print(df[['Id', 'v2a1', 'hacdor', 'rooms', 'hacapo', 'v14a', 'refrig', 'v18q','v18q1', 'r4h1', 'r4h2', 'r4h3', 'r4m1', 'r4m2', 'r4m3', 'r4t1', 'r4t2', 'r4t3']].info())\ndf[['Id', 'v2a1', 'hacdor', 'rooms', 'hacapo', 'v14a', 'refrig', 'v18q','v18q1', 'r4h1', 'r4h2', 'r4h3', 'r4m1', 'r4m2', 'r4m3', 'r4t1', 'r4t2', 'r4t3']].head()\n","ac547bc4":"#v2a1, Monthly rent payment\n#v18q1\ndf['v2a1'].fillna(df['v2a1'].mean(), inplace=True)\ndf['v18q1'].fillna(0, inplace=True)  # we may just end up removing v18q but maybe not (feature engineering already done?)","ff357fb6":"print(df[['tamhog', 'tamviv', 'escolari', 'rez_esc', 'hhsize', 'paredblolad', 'paredzocalo', 'paredpreb', 'pareddes', 'paredmad', 'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisocemento', 'pisoother', 'pisonatur', 'pisonotiene', 'pisomadera']].info())\ndf[['tamhog', 'tamviv', 'escolari', 'rez_esc', 'hhsize', 'paredblolad', 'paredzocalo', 'paredpreb', 'pareddes', 'paredmad', 'paredzinc', 'paredfibras', 'paredother', 'pisomoscer', 'pisocemento', 'pisoother', 'pisonatur', 'pisonotiene', 'pisomadera']].head()","956da5d4":"df['rez_esc'].unique()","d8fbb0f6":"#df['rez_esc'][df['rez_esc'] == 0.]\n#df[df['rez_esc'].isnull()]\n\ndf['rez_esc'].fillna(0, inplace=True)","09e51e67":"df[['techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', 'abastaguadentro', 'abastaguafuera', 'abastaguano', 'public', 'planpri', 'noelec', 'coopele']].info()\ndf[['techozinc', 'techoentrepiso', 'techocane', 'techootro', 'cielorazo', 'abastaguadentro', 'abastaguafuera', 'abastaguano', 'public', 'planpri', 'noelec', 'coopele']].head()","100ff0b5":"df[['sanitario1', 'sanitario2', 'sanitario3', 'sanitario5', 'sanitario6', 'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', 'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', 'elimbasu5', 'elimbasu6']].info()\ndf[['sanitario1', 'sanitario2', 'sanitario3', 'sanitario5', 'sanitario6', 'energcocinar1', 'energcocinar2', 'energcocinar3', 'energcocinar4', 'elimbasu1', 'elimbasu2', 'elimbasu3', 'elimbasu4', 'elimbasu5', 'elimbasu6']].head()","e35b73e3":"df[['epared1', 'epared2', 'epared3', 'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7']].info()\ndf[['epared1', 'epared2', 'epared3', 'etecho1', 'etecho2', 'etecho3', 'eviv1', 'eviv2', 'eviv3', 'dis', 'male', 'female', 'estadocivil1', 'estadocivil2', 'estadocivil3', 'estadocivil4', 'estadocivil5', 'estadocivil6', 'estadocivil7']].head()","9ededdc3":"df[['parentesco1', 'parentesco2', 'parentesco3', 'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9', 'parentesco10', 'parentesco11', 'parentesco12', 'idhogar', 'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9', 'bedrooms', 'overcrowding', 'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', 'computer', 'television', 'mobilephone', 'qmobilephone', 'lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6', 'area1', 'area2', 'age', 'SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq', 'Target']].info()\ndf[['parentesco1', 'parentesco2', 'parentesco3', 'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9', 'parentesco10', 'parentesco11', 'parentesco12', 'idhogar', 'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9', 'bedrooms', 'overcrowding', 'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', 'computer', 'television', 'mobilephone', 'qmobilephone', 'lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6', 'area1', 'area2', 'age', 'SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq', 'Target']].head()\n#['parentesco1', 'parentesco2', 'parentesco3', 'parentesco4', 'parentesco5', 'parentesco6', 'parentesco7', 'parentesco8', 'parentesco9', 'parentesco10', 'parentesco11', 'parentesco12', 'idhogar', 'hogar_nin', 'hogar_adul', 'hogar_mayor', 'hogar_total', 'dependency', 'edjefe', 'edjefa', 'meaneduc', 'instlevel1', 'instlevel2', 'instlevel3', 'instlevel4', 'instlevel5', 'instlevel6', 'instlevel7', 'instlevel8', 'instlevel9', 'bedrooms', 'overcrowding', 'tipovivi1', 'tipovivi2', 'tipovivi3', 'tipovivi4', 'tipovivi5', 'computer', 'television', 'mobilephone', 'qmobilephone', 'lugar1', 'lugar2', 'lugar3', 'lugar4', 'lugar5', 'lugar6', 'area1', 'area2', 'age', 'SQBescolari', 'SQBage', 'SQBhogar_total', 'SQBedjefe', 'SQBhogar_nin', 'SQBovercrowding', 'SQBdependency', 'SQBmeaned', 'agesq', 'Target']\n","d90ab7ce":"# Avg years of education\n#df[['meaneduc','SQBmeaned']]\n#df[['meaneduc','SQBmeaned']][df['meaneduc'].isnull()]\ndf['meaneduc'].fillna(df['meaneduc'].mean(), inplace=True)\ndf['SQBmeaned'].fillna(df['SQBmeaned'].mean(), inplace=True)\n","c70316d7":"df[['idhogar', 'dependency', 'edjefe', 'edjefa']].info()\ndf[['idhogar', 'dependency', 'edjefe', 'edjefa']].head()","c32233de":"df['idhogar'].unique()\n","545c9881":"df['dependency'].unique()\ndf['dependency'].value_counts()","9c9cfb0f":"#edjefe, years of education of male head of household, \n#based on the interaction of escolari (years of education), \n#head of household and gender, yes=1 and no=0\n\n\ndf['edjefe'].value_counts()","f5f0fbd1":"#edjefa, years of education of female head of household, \n#based on the interaction of escolari (years of education), \n#head of household and gender, yes=1 and no=0\n\ndf['edjefa'].value_counts()\n\ndf[['dependency','edjefe', 'edjefa']] = df[['dependency','edjefe', 'edjefa']].replace('yes', 1)\ndf[['dependency','edjefe', 'edjefa']] = df[['dependency','edjefe', 'edjefa']].replace('no', 0)","f1c7dd4e":"#pd.to_numeric(df[['dependency','edjefe', 'edjefa']])\ndf['dependency'] = df['dependency'].astype(float)\ndf['edjefe'] = df['edjefe'].astype(float)\ndf['edjefa'] = df['edjefa'].astype(float)\n#df[['dependency','edjefe', 'edjefa']].info()","27ac1a37":"#plt.subplot(121)\ncolumns.remove('Id')\ncolumns.remove('Target')\ncolumns.remove('idhogar')\nstatistics = []\nfor name in columns:\n    stuff = (name, sp.stats.skew(df[name]), sp.stats.kurtosis(df[name]))\n    statistics.append( stuff)","031ce7e9":"from operator import itemgetter\n\nsorted(statistics, key=itemgetter(1))","cf61836b":"plt.hist(df['estadocivil2'])","44c7d5c0":"columns.append('Target')","b4976f1d":"correlation_matrix = df[columns].corr()","d6627611":"variables = correlation_matrix.sort_values('Target', axis=0)['Target']","626c2ffb":"important_variables = variables[abs(variables)> 0.2].index\nimportant_variables","2d16284b":"plt.figure(figsize=(10,10))\nsns.heatmap(df[important_variables].corr())","900ed882":"columns = list(df.columns)\ncolumns.remove('Id')\ncolumns.remove('Target')\ncolumns.remove('idhogar')","e3fa70b0":"X = df[columns][0:9557]\nX_unknown = df[columns][9557:]","b7d7e4d4":"y = df['Target'][0:9557]","c1c67136":"def PlotClassifierDiagnostics(y_test, y_pred, y_prob):\n    accuracy = accuracy_score(y_test, y_pred)\n    #roc_auc = roc_auc_score(y_test, y_prob, average='micro')\n    confusion = confusion_matrix(y_test, y_pred)\n    print('Confusion Matrix: ')\n    print(confusion)\n    print('Accuracy: ',  accuracy)\n    print('F1 (micro\/macro\/weighted): ')\n    \n    f1 = f1_score(y_test, y_pred, average='micro')\n    f2 = f1_score(y_test, y_pred, average='macro')\n    f3 = f1_score(y_test, y_pred, average='weighted')\n    \n    print((f1, f2, f3))\n    #print((accuracy, f1, roc_auc))\n        \n    #fpr, tpr, thresholds = roc_curve(y_test, y_prob[:, 1])\n    #roc_auc2 = auc(fpr, tpr)\n    #print('ROC curve')\n    \n    #plt.plot(fpr, tpr, alpha=0.3,label='(AUC = %0.2f)' % (roc_auc))\n    #plt.xlim([-0.05, 1.05])\n    #plt.ylim([-0.05, 1.05])\n    #plt.xlabel('False Positive Rate')\n    #plt.ylabel('True Positive Rate')    ","6b0deeb1":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)","140e2d8f":"#def xgb_crossval(learning_rate, n_estimators):\n#    return cross_val_score(xgb.XGBClassifier(\n#        learning_rate=learning_rate, \n#        n_estimators=int(n_estimators), \n#        silent=False,\n#        objective='multi:softmax'),\n#        X,y, scoring='f1_micro', cv=3, n_jobs=-1).mean()\n\n#bayesian_optimizer = BayesianOptimization(xgb_crossval, \n#                                        {'learning_rate':(0.001, 0.2), \n#                                         'n_estimators': (100, 2000)} )\n#best_optimizer = bayesian_optimizer.maximize(n_iter=10)","f886051f":"#bayesian_optimizer.res['max']","4d1f3ffb":"#xgb_model = xgb.XGBClassifier(learning_rate = 0.1, n_estimators=1000)\n\n#https:\/\/github.com\/fmfn\/BayesianOptimization\/commit\/1ce5484c6fff6e4913d43fa41dbed29f2a95f187\n\n\n\n#def xgb_crossval(learning_rate, n_estimators, max_depth, gamma, min_child_weight):\n#    return cross_val_score(xgb.XGBClassifier(\n#        learning_rate=learning_rate, \n#        n_estimators=int(n_estimators), \n#        max_depth = int(max_depth), \n#        gamma=int(gamma), \n#        min_child_weight = int(min_child_weight),\n#        silent=False,\n#        objective='multi:softmax'),\n#        X,y, scoring='f1_micro', cv=3, n_jobs=-1).mean()\n\n#bayesian_optimizer = BayesianOptimization(xgb_crossval, \n#                                        {'learning_rate':(0.001, 0.2), \n#                                         'n_estimators': (100, 2000), \n#                                         'max_depth':(1,10), \n#                                         'gamma': (0,1), \n#                                         'min_child_weight': (0,10)} )\n#gp_params = {\"alpha\": 1e-5}\n#best_optimizer = bayesian_optimizer.maximize(n_iter=10) #, **gp_params)                              ","083051ed":"# Example from the people who made this library: \n# https:\/\/github.com\/fmfn\/BayesianOptimization\/blob\/master\/examples\/xgboost_example.py\n\n#random_state= 0\n#params = {\n#        'eta': 0.1,\n#        'silent': 1,\n#        'eval_metric': 'mae',\n#        'verbose_eval': True,\n#        'seed': random_state\n#    }\n#    xgbBO = BayesianOptimization(xgb_evaluate, {'min_child_weight': (1, 20),\n#                                                'colsample_bytree': (0.1, 1),\n#                                                'max_depth': (5, 15),\n#                                                'subsample': (0.5, 1),\n#                                                'gamma': (0, 10),\n#                                                'alpha': (0, 10),\n#                                                })\n\n","edaa5fc6":"#baysian_optimize","e1d08bac":"xgb_model = xgb.XGBClassifier(learning_rate = 0.1, n_estimators=6000, silent=False, objective='multi:softmax')\nt = time.time()\nxgb_model.fit(X_train, y_train)\ny_pred_xgb = xgb_model.predict(X_test)\ny_prob_xgb = xgb_model.predict_proba(X_test)\n\nprint('elapsed time: ', time.time()-t)\n\nPlotClassifierDiagnostics(y_test, y_pred_xgb, y_prob_xgb)\n\n\n# going up to 1500 yields benefits\n###xgb_model = xgb.XGBClassifier(learning_rate = 0.1, n_estimators=1000, silent=False, objective='multi:softmax')\n#Confusion Matrix: \n#[[ 101   14    0   17]\n# [   8  225   11   79]\n# [   9   15  133  105]\n# [   5   13   10 1167]]\n#Accuracy:  0.850418410042\n#F1 (micro\/macro\/weighted): \n#(0.85041841004184104, 0.77623584606360785, 0.84031282560359011)\n\n#xgb_model = xgb.XGBClassifier(learning_rate = 0.1, n_estimators=3000, silent=False, objective='multi:softmax')\n#Confusion Matrix: \n#[[ 128   14    2   25]\n# [   4  263    6   46]\n# [   2   21  170   43]\n# [   7    7    4 1170]]\n#Accuracy:  0.905334728033\n#F1 (micro\/macro\/weighted): \n#(0.90533472803347281, 0.85718856012394928, 0.90218972836421363)\n\n#xgb_model = xgb.XGBClassifier(learning_rate = 0.1, n_estimators=6000, silent=False, objective='multi:softmax')\n#elapsed time:  142.39282512664795\n#Confusion Matrix: \n#[[ 133   13    2   21]\n# [   6  281    5   27]\n# [   2   23  182   29]\n# [   6    8    7 1167]]\n#Accuracy:  0.922071129707\n#F1 (micro\/macro\/weighted): \n#(0.92207112970711302, 0.87918487482829089, 0.92030419590693613)\n","1ea55a1c":"lgb_model = lgb.LGBMClassifier(learning_rate=0.1, n_estimators=6000)\n\nt = time.time()\n\nlgb_model.fit(X_train, y_train)\ny_pred_lgb = lgb_model.predict(X_test)\ny_prob_lgb = lgb_model.predict_proba(X_test)\n\nprint('elapsed time: ', time.time()-t)\n\n\nPlotClassifierDiagnostics(y_test, y_pred_lgb, y_prob_lgb)","ddee37ab":"t = time.time()\nxgb_model.fit(X,y)\ny_pred_xgb = xgb_model.predict(X_unknown)\nprint('elapsed time: ', time.time()-t)\n\n\n","c2fa97a5":"t = time.time()\nlgb_model.fit(X,y)\ny_pred_lgb =lgb_model.predict(X_unknown)\nprint('elapsed time: ', time.time()-t)\n","8ae2f08a":"df_out = df[9557:]","60d60c24":"df_out['Target'] = y_pred_xgb.astype(int)\ndf_out[['Id', 'Target']].to_csv('output_xgb.csv',index=False)\n\ndf_out['Target'] = y_pred_lgb.astype(int)\ndf_out[['Id', 'Target']].to_csv('output_lgb.csv',index=False)","3f024ff3":"# Modelling!","951eb90b":"## For a description of Skew and Kurtosis see the below to links:\n\nhttps:\/\/en.wikipedia.org\/wiki\/Skewness\n\nhttps:\/\/en.wikipedia.org\/wiki\/Kurtosis\n\nHere we check our assumption of normality.  Don't be too alarmed by high skew\/kurtosis values as categorical variables will naturally look weird","71c6d3d8":"For competition site, see: \nhttps:\/\/www.kaggle.com\/c\/costa-rican-household-poverty-prediction\/\n\nThis will be a multiclass classification problem.  From the data description:\nTarget - the target is an ordinal variable indicating groups of income levels. \n1 = extreme poverty \n2 = moderate poverty \n3 = vulnerable households \n4 = non vulnerable households\n"}}