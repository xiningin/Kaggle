{"cell_type":{"45ea22b4":"code","6ee916fc":"code","202f81a4":"code","1975bdb8":"code","41284049":"code","f48ab05f":"code","443eb2f5":"code","311daf37":"code","4170e28c":"code","2d235d3a":"code","43ba85c1":"code","8bdfdd60":"code","9c05e4db":"code","f3d502dc":"code","a9425a78":"code","cab1c822":"code","5366245e":"code","1f55546e":"code","cb5cffcb":"code","c5665d65":"code","32115ccd":"code","e698efac":"code","e6aa50ac":"code","3286a866":"code","d354c67d":"code","7b6d2cf4":"code","83ed74d2":"code","c2491b36":"markdown","6e462d19":"markdown","34967666":"markdown","271beb4d":"markdown","e8fcb9cb":"markdown","1fceb0c8":"markdown","78ddb1c0":"markdown","2ba98a63":"markdown","25f626a0":"markdown","2a97f445":"markdown","3d73fbc4":"markdown","195bc938":"markdown","8378a81b":"markdown","da6524cd":"markdown","ec332d2f":"markdown","5ecf5fec":"markdown","41dcb6d4":"markdown","117047ed":"markdown","3f25f479":"markdown","143d32c5":"markdown","2db011c5":"markdown","b6ed72ae":"markdown","7d4e2a24":"markdown","5336d896":"markdown","25c8c7db":"markdown"},"source":{"45ea22b4":"import matplotlib.pyplot as plt\nimport mplleaflet\n\nplt.figure(figsize=(6, 6))\n\n# ANA's station\nplt.plot(-44.120881, -20.197752, 'bo', ms=10)   # Melo Franco (02044008) [8,81 km (5.47 mi)]\n\n# CEMADEN's stations\n# plt.plot(-44.2, -20.143, 'go', ms=10)     # Centro (310900601A) [8,98 km (5.58 mi)]\n# plt.plot(-44.216, -20.146, 'go', ms=10)   # Progresso (310900602A) [10,67 km (6.63 mi)]\nplt.plot(-44.047, -20.094, 'go', ms=10)   # Casa Branca district (310900603A) [7,91 km (4.92 mi)]\n# plt.plot(-44.023, -20.165, 'go', ms=10)   # Prefeito Maciel street (310900604A) [11,18 km (6.95 mi)]\nplt.plot(-44.107, -20.135, 'go', ms=10)   # C\u00f3rrego do Feij\u00e3o (310900605A) [2,16 km (1.34 mi)]\n# plt.plot(-44.2009, -20.1409, 'go', ms=10) # Rio Paraopeba (310900605H) [9,00 km (5.59 mi)]\nplt.plot(-44.147, -20.156, 'go', ms=10)   # Alberto Flores (310900606A) [5,14 km (3.19 mi)]\nplt.plot(-44.198, -20.142, 'go', ms=10)   # Centro (310900607A) [8,74 km (5.43 mi)]\nplt.plot(-44.105, -20.196, 'go', ms=10)   # Aranha (310900608A) [8,72 km (5.42 mi)]\n# plt.plot(-44.227, -20.12, 'go', ms=10)    # Inhotim (310900609A) [11,38 km (7.07 mi)]\n\n# Dam I\nplt.plot(-44.118047, -20.118579, 'rX', ms=10)\n    \nmplleaflet.display()","6ee916fc":"from IPython.display import display, Pretty\n\nwith open('..\/input\/ana-melo-franco-weather-station\/chuvas_C_02044008sample.csv', 'r', encoding='latin-1') as input_file:\n    # ANA's Melo Franco station data (first 50 file lines)\n    display(Pretty(data=input_file.read()))","202f81a4":"# CEMADEN's stations data for January 2014 in Brumadinho (first 50 file lines)\ndisplay(Pretty(filename='..\/input\/cemaden-weather-stations\/3277_MG_2014_1sample.csv'))","1975bdb8":"import pandas as pd\n\n# For the sake of performance, Pandas by default truncates the total rows and columns to be displayed.\n# The following lines modify this setting and instruct the library to display more or less data.\npd.set_option('display.max_rows', 20)\npd.set_option('display.max_columns', None)\n\n\"\"\"\nHere we use _ as the name of our DataFrame being processed. Although _ is a valid Python variable name,\nit is generally not good practice in software engineering to name a variable with such a generic name. However,\nas we will see later, many operations require frequent repetition of the DataFrame name, which can make the lines\nvery long, so we\u2019ll use this trick to keep the lines of code as short as possible until we\u2019ve finished cleaning\nand processing our data.\n\"\"\"\n\n_ = pd.read_csv('..\/input\/ana-melo-franco-weather-station\/chuvas_C_02044008.csv', decimal=',', index_col=False, sep=';', skiprows=12)\n_","41284049":"_['Data'] = pd.to_datetime(_['Data'], dayfirst=True)\n_[['Data','Total']]","f48ab05f":"mask1 = _.groupby(['Data'])['NivelConsistencia'].transform(max) == _['NivelConsistencia']\nmask2 = (_['Data'].dt.year < 2019) | ((_['Data'].dt.year == 2019) & (_['Data'].dt.month == 1))\n\n_ = _[mask1 & mask2].sort_values('Data')\n_.set_index('Data', inplace=True)\n_","443eb2f5":"_['TotalStatus'].value_counts()","311daf37":"_[_['TotalStatus'] == 0]","4170e28c":"# Updating 'Total' e 'TotalStatus'\n# Note that if any of the days had a status other than 1, our 'TotalStatus' would also be different\n_.at['2011-10-01', 'Total'] = _.loc['2011-10-01'].filter(regex=(\"Chuva[0-9]{2}$\")).sum().round(1)\n_.at['2011-10-01', 'TotalStatus'] = _.loc['2011-10-01'].filter(regex=(\"Chuva[0-9]{2}Status$\")).max()\n_.at['2011-12-01', 'Total'] = _.loc['2011-12-01'].filter(regex=(\"Chuva[0-9]{2}$\")).sum().round(1)\n_.at['2011-12-01', 'TotalStatus'] = _.loc['2011-12-01'].filter(regex=(\"Chuva[0-9]{2}Status$\")).max()\n_.at['2014-10-01', 'Total'] = _.loc['2014-10-01'].filter(regex=(\"Chuva[0-9]{2}$\")).sum().round(1)\n_.at['2014-10-01', 'TotalStatus'] = _.loc['2014-10-01'].filter(regex=(\"Chuva[0-9]{2}Status$\")).max()\n\n_.loc[[pd.to_datetime('2011-10-01'), pd.to_datetime('2011-12-01'), pd.to_datetime('2014-10-01')]]","2d235d3a":"df_ana = _[_['TotalStatus'] == 1]['Total'].rename('ANA')\ndf_ana.index.name = None\ndf_ana","43ba85c1":"from glob import glob\n\npd.set_option('display.max_rows', 70)\n\ndf1 = pd.concat([pd.read_csv(f,\n                             sep=';',\n                             index_col=False,\n                             decimal=',',\n                             usecols=['codEstacao','datahora','valorMedida']) for f in glob('..\/input\/cemaden-weather-stations\/3277_MG_*.csv')])\n\ndf1.loc[0]","8bdfdd60":"df_br_date = pd.concat([pd.read_csv(f,\n                                    sep=';',\n                                    index_col=False,\n                                    decimal=',',\n                                    usecols=['codEstacao','datahora','valorMedida']) for f in glob('..\/input\/cemaden-weather-stations-renamed\/_3277_MG_*.csv')],\n                       ignore_index=True)\n\ndf_dot_decimal = pd.concat([pd.read_csv(f,\n                                        sep=';',\n                                        index_col=False,\n                                        usecols=['codEstacao','datahora','valorMedida']) for f in glob('..\/input\/cemaden-weather-stations-renamed\/dot_3277_MG_*.csv')],\n                           ignore_index=True)\n\ndf_us_date = pd.concat([pd.read_csv(f,\n                                    sep=';',\n                                    index_col=False,\n                                    decimal=',',\n                                    usecols=['codEstacao','datahora','valorMedida']) for f in glob('..\/input\/cemaden-weather-stations-renamed\/3277_MG_*.csv')],\n                       ignore_index=True)\n\ndf_br_date['datahora'] = pd.to_datetime(df_br_date['datahora'], dayfirst=True)\ndf_dot_decimal['datahora'] = pd.to_datetime(df_dot_decimal['datahora'])\ndf_us_date['datahora'] = pd.to_datetime(df_us_date['datahora'])\n\nprint(df_br_date.head())\nprint(df_dot_decimal.head())\nprint(df_us_date.head())","9c05e4db":"df2 = pd.concat([df_br_date, df_dot_decimal, df_us_date], ignore_index=True)\ndf2.shape","f3d502dc":"df3 = df2[df2['codEstacao'] == '310900605A'].copy()\ndf3.shape","a9425a78":"def hours_in_month(month, year):\n    if month in (1,3,5,7,8,10,12):\n        return 744\n    elif month in (4,6,9,11):\n        return 720\n    elif year == 2016:\n        return 696\n    else:\n        return 672","cab1c822":"_ = df3[['valorMedida']].groupby(df3['datahora'].dt.floor('H')).sum().groupby(pd.Grouper(freq='MS')).count()\n_['%'] = _.index.map(lambda dt: (_.loc[dt]['valorMedida']\/hours_in_month(dt.month, dt.year)*100).round(2))\n_ = _.rename(columns={'valorMedida': 'Hours with measurements'})\n_","5366245e":"df4 = df2[df2['codEstacao'].isin(['310900603A', '310900605A', '310900606A', '310900607A', '310900608A'])].copy()\ndf4['datahora'] = df4['datahora'].dt.floor('H')\n\ndf_corrego_feijao = df4[df4['codEstacao'] == '310900605A'][['datahora','valorMedida']]\ndf_alberto_flores = df4[df4['codEstacao'] == '310900606A'][['datahora','valorMedida']]\ndf_casa_branca = df4[df4['codEstacao'] == '310900603A'][['datahora','valorMedida']]\ndf_aranha = df4[df4['codEstacao'] == '310900608A'][['datahora','valorMedida']]\ndf_centro = df4[df4['codEstacao'] == '310900607A'][['datahora','valorMedida']]\n\ndf_corrego_feijao = df_corrego_feijao.groupby('datahora').sum()\ndf_alberto_flores = df_alberto_flores.groupby('datahora').sum()\ndf_casa_branca = df_casa_branca.groupby('datahora').sum()\ndf_aranha = df_aranha.groupby('datahora').sum()\ndf_centro = df_centro.groupby('datahora').sum()\n\ndf_cemaden = (df_corrego_feijao.combine_first(df_alberto_flores)\n                               .combine_first(df_casa_branca)\n                               .combine_first(df_aranha)\n                               .combine_first(df_centro))","1f55546e":"_ = pd.DataFrame(df_cemaden.groupby(pd.Grouper(freq='MS')).count()).rename(columns={'valorMedida': 'Hours with measurements'})\n_['%'] = _.index.map(lambda dt: (_.loc[dt]['Hours with measurements']\/hours_in_month(dt.month, dt.year)*100).round(2))\n_","cb5cffcb":"df_cemaden = df_cemaden.groupby(pd.Grouper(freq='MS')).sum().round(1)['valorMedida'].rename('CEMADEN')\ndf_cemaden.index.name = None\ndf_cemaden","c5665d65":"import calendar\nimport matplotlib.ticker as ticker\nimport matplotlib.lines as mlines\nimport numpy as np","32115ccd":"def precipitation_fourth_quarter(ana, cemaden):\n    _ = ana[:-1].groupby(pd.Grouper(freq='QS')).sum()\n    __ = cemaden[:-1].groupby(pd.Grouper(freq='QS')).sum()\n    \n    _ = _.groupby(_.index.month)\n    __ = __.groupby(__.index.month)\n\n    maxs_ana = _.max()\n    maxs_cemaden = __.max()\n    maxs_ana = maxs_ana.loc[10].sum()\n    maxs_cemaden = maxs_cemaden.loc[10].sum()\n    \n    means_ana = _.mean()\n    means_cemaden = __.mean()\n    means_ana = means_ana.loc[10].sum()\n    means_cemaden = means_cemaden.loc[10].sum()\n\n    g_labels = ['ANA (1941 to 2018)', 'CEMADEN (2014 to 2018)']\n    g_maxs = [maxs_ana, maxs_cemaden]\n    g_means = [means_ana, means_cemaden]\n    g_72p = [means_ana*1.72, means_cemaden*1.72]\n    g_2018 = [ana.iloc[-4:-1].sum(), cemaden.iloc[-4:-1].sum()]\n    \n    x = np.arange(2)\n    width = 0.20\n\n    fig, ax = plt.subplots()\n    fig.set_size_inches(6.4, 4.8)\n    \n    rects1 = ax.bar(x - width, g_maxs, width, label='All-time high', color='tab:blue')\n    rects2 = ax.bar(x, g_means, width, label='All-time average', color='tab:cyan')\n    rects3 = ax.bar(x + width, g_2018, width, label='2018 Q4', color='tab:orange')\n    \n    for i, rects in enumerate([rects1, rects2, rects3]):\n        for rect in rects:\n            height = rect.get_height()\n            ax.text(rect.get_x() + rect.get_width()\/2., height + 15,\n                    '%d mm' % int(height), ha='center', va='bottom')\n\n    ax.set_title('Accumulated precipitation for the 4th quarter in Brumadinho', pad=17)\n    ax.set_xticks(x)\n    ax.set_xticklabels(g_labels)\n    ax.set_yticklabels([])\n    plt.tick_params(axis='both', length=0)\n    \n    ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.07), frameon=False, ncol=3)\n\n    fig.tight_layout()\n    \n    plt.ylim(0, 1300)\n    \n    for s in ax.spines:\n        ax.spines[s].set_visible(False)\n\n    plt.show()","e698efac":"def precipitation_by_year(df):\n    plt.figure(figsize=(6.4, 4.8))\n\n    df_by_year = df.groupby(pd.Grouper(freq='Y')).sum()[1:-1]\n    \n    precip_plot = df_by_year.plot()\n    blue_line = mlines.Line2D([], [], color='blue')\n    \n    means_plot, = plt.plot([-29, 48], [df_by_year.mean(), df_by_year.mean()], color='tab:orange')\n\n    plt.tick_params(axis='x', length=0)\n\n    plt.title('Annual precipitation in Brumadinho [{}]'.format(df.name), pad=17)\n\n    plt.ylim(0, 2000)\n    \n    ax = plt.gca()\n    span_area = ax.axvspan(46, 48, alpha=0.3, color='red')\n    \n    ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%d mm'))\n    \n    plt.legend(handles=[blue_line, means_plot, span_area],\n           labels=['Annual Precipitation', 'Average', '2016 to 2018 span'],\n           loc='upper center',\n           fontsize='small',\n           ncol=3,\n           frameon=False,\n           bbox_to_anchor=(0.5, -0.07))\n    \n    for s in ['bottom','right','top']:\n        ax.spines[s].set_visible(False)\n\n    plt.show()","e6aa50ac":"def precipitation_profile(df):\n    plt.figure(figsize=(6.4, 4.8))\n\n    _ = df.iloc[:-7].groupby(df.iloc[:-7].index.month)\n\n    maxs = _.max().rename(index=lambda x: calendar.month_abbr[x])\n    means = _.mean().rename(index=lambda x: calendar.month_abbr[x])\n    std_errors = (_.std()\/np.sqrt(_.count())).rename(index=lambda x: calendar.month_abbr[x])\n    \n    shift_mask = calendar.month_abbr[7:14] + calendar.month_abbr[1:7]\n    maxs = maxs.loc[shift_mask]\n    means = means.loc[shift_mask]\n    std_errors = std_errors.loc[shift_mask]\n\n    x_axis = np.arange(12)\n    means_plot = plt.bar(x_axis, means, yerr=std_errors*2, ecolor='tab:gray', color='tab:cyan')\n    maxs_plot, = plt.plot(x_axis, maxs, '.:', color='tab:blue')\n    measured_plot, = plt.plot(np.arange(7), df.iloc[-7:], 'o-',  color='tab:orange')\n    \n    plt.xticks(x_axis, shift_mask)\n    plt.tick_params(axis='x', length=0)\n\n    plt.title('Brumadinho\\'s precipitation profile [{}]'.format(df.name), pad=17)\n\n    plt.ylim(0, 700)\n    \n    ax = plt.gca()\n    ax.yaxis.set_major_formatter(ticker.FormatStrFormatter('%d mm'))\n\n    plt.legend(handles=[maxs_plot, means_plot, measured_plot],\n               labels=['All-time highs', 'All-time averages', 'Jul\/18 to Jan\/19'],\n               loc='upper center',\n               fontsize='small',\n               ncol=3,\n               frameon=False,\n               bbox_to_anchor=(0.5, -0.07))\n    \n    for s in ['bottom','right','top']:\n        ax.spines[s].set_visible(False)\n\n    plt.show()","3286a866":"precipitation_fourth_quarter(df_ana, df_cemaden)","d354c67d":"precipitation_by_year(df_ana)","7b6d2cf4":"precipitation_profile(df_ana)","83ed74d2":"precipitation_profile(df_cemaden)","c2491b36":"## Data Wrangling\n\nOur final objective is to consolidate, in a single DataFrame by dataset, the accumulated rainfall per month, so that we can create visualizations that allow us to make our analysis. ANA's data already have a monthly consolidation and are all arranged in a single file, so let's start with it.\n\nA brief inspection shows us some details that we will need to take into account when telling Pandas how to load this file:\n\n* the separator is ';' and not ',' as the default for CSV files\n* the first 12 lines are a kind of legend that we will need to ignore\n* the numbers are using Brazilian standard, with a comma separating the decimals, which is also not Pandas standard\n* lastly, we could already indicate which column to use as an index for the rows, but as we will need to do some treatments on it too, for now we will load the file with the standard index\n\nThat said, we can load our data like this:","6e462d19":"For some reason, the 'Total' column was not correctly measured but all daily measurements are present and, better, they all have status 1, indicating that they are real measurements. These inconsistencies would probably be corrected when the data were consisted (note that 'NivelConsistencia' is 1 for these lines), but luckily we have enough data to do this ourselves.","34967666":"The amount of data we have here is still huge, it's almost one and a half million cells, which still slows down our operations. We can restrict the data a little more by filtering the distant stations. As we said, in order to facilitate the analysis and not need to resort to more complex methods like Thiessen Polygons to make a weighted average between all stations, we decided to always use the data from the geographically closest station to the dam. CEMADEN stations started to operate in 2014, providing 10 new data sources, and following our initial idea we could simply use the data from the nearest station (C\u00f3rrego do Feij\u00e3o). Then, let's see if we can follow this approach:","271beb4d":"The first visualization that we are going to create is one that shows the amount of rain accumulated in the fourth semester of the year in Brumadinho in three sections: historical maximum, historical average and the measurements of 2018, for each of the datasets.","e8fcb9cb":"Now we are going to clean and process data from CEMADEN stations. As previously stated, this data is arranged in several files, so we will have to use a new library to go through all these files and consolidate the data in a single DataFrame. Right after loading, in addition to the treatment for the separator, decimal and index, we will already define which columns we want to load. The ideal is to restrict the data to just what we need whenever possible, as this decreases the amount of memory and cpu required to handle the data.\n\nIn the previous case, as we only had one file, it was easier to check the format in which the information was recorded, but now we are dealing with a dataset divided into more than 60 files and by inspecting only one of them is not possible to have this guarantee. Therefore, by concatenating the data from each file, we will preserve the original index and take advantage of that to print the first line from each one, checking if everything is as expected.","1fceb0c8":"<h1>Did above-average rains contribute to the disaster in Brumadinho?<\/h1>\n<h3>Analysis of the possible impact of rainfalls on the rupture of Brumadinho's dam<\/h3>\n\n<br>\n<br>\n\nOn January 25, 2019, Dam I of Vale\u2019s C\u00f3rrego do Feij\u00e3o mine in Brumadinho collapsed. At the time it burst, it was holding 11.7m cubic metres of iron ore tailings, or almost 5,000 Olympic-sized swimming pools, and stood 86m high<sup><a href=\"#link1\">[1]<\/a><\/sup>. The rupture gave rise to one of the greatest human and environmental disasters in the history of Brazil. One year later, 259 people had been found dead and 11 were still missing<sup><a href=\"#link2\">[2]<\/a><\/sup>.\n\nIn its 2017 _\"Mine Tailing Storage: Safety is no Accident\"_ report<sup><a href=\"#link3\">[3]<\/a><\/sup>, the United Nations states that heavy and prolonged rains, hurricanes and earthquakes can even be triggers for disruptions and overflows, but even in these cases the UN considers that there was an error since the risk planning for the construction and maintenance of the dam must take into account the climatic conditions of the site. About the report, Alex Bastos, professor of geology at the Federal University of Esp\u00edrito Santo and member of the UN committee on ore dams, summarizes: \"The conclusion of the study is that there are two reasons that cause disruptions: error in the risk analysis and negligence in the maintenance of the dam. That is, if there was a torrential rain that caused the dam to overflow, there was an error in the risk analysis. If the region is subject to rain like this, the structure of the dam should be different. It has to be accurate.\"<sup><a href=\"#link4\">[4]<\/a><\/sup>\n\nAccording to an independent report<sup><a href=\"#link5\">[5]<\/a><\/sup> commissioned by the company, released in December 2019, one of the factors identified as a cause of the tragedy was an increase in the amount of rain in the region. \u201cThe Panel concluded that the sudden strength loss and resulting failure of the marginally stable dam were due to a critical combination of ongoing internal strains due to creep, and a strength reduction due to loss of suction in the unsaturated zone caused by the intense rainfall towards the end of 2018. This followed a number of years of increasing rainfall after tailings deposition ceased in July 2016.\u201d, the report said.\n\nAnyway, before analyzing the participation of the precipitation factor in the disaster, we need to ask ourselves: were there, in fact, above average rains that could have impacted the rupture of the Brumadinho dam? That is the question that we will try to answer in this article in Jupyter Notebook format.\n\n## Analyzed Data\n\nTo conduct our study, publicly available data sets in the National Water Resources Information System (SNIRH) of the National Water Agency (ANA) and in CEMADEN (National Center for Natural Disaster Monitoring and Alerts), agencies of the Brazilian government, were used to collect information on precipitation in the vicinity of the dam and to analyze if there was any abnormal pattern of rain that could have contributed to the disaster.  \n\n* <a href=\"http:\/\/www.snirh.gov.br\/hidroweb\/rest\/api\/documento\/convencionais?tipo=3&documentos=2044008\" target=\"_blank\">Melo Franco station data from ANA's SNIRH<sup>[6]<\/sup><\/a>\n* <a href=\"http:\/\/www.cemaden.gov.br\/mapainterativo\/download\/downpluv.php\" target=\"_blank\">CEMADEN stations data<sup>[7]<\/sup><\/a>\n\nThe surrounding meteorological stations were reduced to the 6 closest to the dam, varying from 5.47 to 1.34 miles in distance, so that the data of the nearest station that were available in each period were always used (in the case of CEMADEN stations). In the following map it is possible to see the geographical position of ANA's Melo Franco station (blue dot), CEMADEN's Aranha, Alberto Flores, Centro, C\u00f3rrego do Feij\u00e3o and Casa Branca District stations (green dots) and the dam site (x in red).","78ddb1c0":"Again, it is not possible to notice any significant increase in the volume of annual precipitation in the region. Being quite strict, the volume was actually lower in the year following the end of tailings deposition.\n\n**- Were there unexpected rain patterns that could have impacted the rupture of Brumadinho\u2019s dam?**\n\nFinally, in order to understand if any meteorological anomaly hit the region in the months that preceded the disaster, I\u2019m going to use the charts with precipitation profile of the region of the dam.","2ba98a63":"Next, we are going to plot the accumulated annual precipitation in Brumadinho since the beginning of ANA\u2019s station measurements.","25f626a0":"We solved the first point, the problem of status remains. First, let's see how much of each status we have for the data that is meaningful to us:","2a97f445":"## Visualizations\n\nAfter wrangling the data, we can now create graphical visualizations that not only help us to understand the data but also allow us to identify patterns, anomalies and also answer several questions immediately.","3d73fbc4":"Great, now that we have all of our data in the same format we can combine them again into a single DataFrame.","195bc938":"Okay, analyzing only the C\u00f3rrego do Feij\u00e3o station we reduced the amount of data by more than 90%, but they are still arranged by time and we need them consolidated by month. The next step is to standardize the times so that we can make an aggregation. At the moment this would not be possible because, as we can see in the previous blocks, the minutes and seconds are not always the same for all entries. CEMADEN website explains the different times recorded on our DataFrame:\n> \"... automatic rain gauges connect to CEMADEN's servers and transmit data of accumulated rainfall, in millimeters, every 10 minutes, and are then processed and made available to the CEMADEN Situation Room on a specialized platform developed by the Center. On the other hand, if it is not raining, the rain gauge will connect only once every hour, sending the accumulated 0 millimeter of the last 60 minutes.\"<sup><a href=\"#link9\">[9]<\/a><\/sup>\n\nSo we must assume that we need to have at least one measurement per hour. In a month of 28 days we have 672 hours, in a month of 29 we have 696, 30 days have 720 hours and, finally, a month of 31 days has 744 hours. Knowing this, let's check the consistency of the data we have:","8378a81b":"Now we have a relevant sample. We still have some months with missing data, but these gaps do not even represent 2% of the total hours in the month.\n\nSo we just need to consolidate this data in a Series format, just as we did with the ANA dataset, and we will be ready to generate the visualizations that will help us to answer our initial question.","da6524cd":"CEMADEN data, only available after 2014, form a small sample to generate a relevant statistical value, but because it is an agency created specifically for monitoring natural disasters, we decided to check if the independent panel had used these numbers for make its statement. But the fact is the rains of the last quarter of 2018, recorded by CEMADEN, were only 7.8% above the average. For ANA data, 2018 records were even lower than average. Therefore, we have sufficient evidence to affirm that the information conveyed in the report is hardly true.\n\n**- Has rainfall increased in the years that followed 2016?**\n\nNext, I want to validate the statement that the collapse \u201cfollowed a number of years of increasing rainfall after tailings deposition ceased in July 2016\u201d and for that I\u2019m going to use the chart with the accumulated annual precipitation in Brumadinho since the beginning of ANA\u2019s station measurements.","ec332d2f":"## Analysis\n\nWith the help of visualizations, we can return to the questions we set out to answer:\n\n**- Was there more rain than usual in late 2018?**\n\nThe first thing I\u2019ll try to answer is if rainfall at the end of 2018 was, indeed, intense as Vale's report said. Although the definition of \u201cintense\u201d is somewhat subjective in this case, I set out to analyze the issue from the perspective of the entire historical series that we had available, and the chart below has the results.","5ecf5fec":"Great! Now we can get rid of the inaccurate measures and reduce the DataFrame to a Series that contains only the column that is really meaningful to us, finishing the treatment for this dataset.","41dcb6d4":"_*It is worth noting that the standard error for CEMADEN data is much larger than that of ANA data, which was to be expected, since the sample is much smaller._\n\nThe first analysis we should do is to compare the records from July 2018 to January 2019 and the historical averages. The charts show that, of the 7 months prior to the disaster, only August and September showed rains consistently above average, but even so at levels that do not even reach half the historical average of the wettest month, for example. October and November were within the average, fluctuating to more or less depending on the dataset but still within the margins of error. However, December and mainly January, typically the wettest months, had considerably below average rainfall at the turn of 2019.\n\n## Conclusion\n\nThroughout this article, we used rainfall data to analyze the possible impact of the amount of rain in one of the greatest natural disasters in world\u2019s history, and we could see that:\n\n- The measured rainfall for the end of 2018 was close to the average\n- There\u2019s no evidence of rainfall increasing after 2016\n- The wettest months for the region even had bellow average precipitation levels just before the disaster\n\nThat said, if we are going to make an analysis purely from the perspective of the search for the cause of the tragedy, we must emphasize that **the amount of rain should not be considered a causative factor for the disaster** since there were no anomalies and, since the beginning, the most conservative security parameters should have been adopted when dealing with a structure with such potential for destruction.","117047ed":"It was good that we were careful because, from what we see, the way of recording the data has changed over time. In fact, if we look line by line, we can identify three distinct patterns:\n\n1. Until March 2015, both dates and values were using the Brazilian standard (day\/month\/year and decimals separated by commas)\n2. In April, May, June and July 2015, the dates are in international format and the values are with decimals separated by periods\n3. As of August 2015, the dates remain using the international standard but the values are recorded again using the Brazilian standard\n\nKnowing this, we can rename the files so that the names distinguish the pattern used in each one, this way we'll be able to load them independently to make the necessary treatment.","3f25f479":"The first good news is that we do not have status 4 ('Accumulated') for 'Total', which makes sense since this is already a cumulative measure. Another is the fact that only 42 of the 930 entries are not in the status we want, which represents less than 5% of the sample. Since we want to generate a series of averages, it makes sense to give up the data with great uncertainty (status 2 and 3) to prevent them from contaminating our sample and consequently distorting our results.\n\nHowever, three entries are with status 0, which means an inconsistency. Let's see:","143d32c5":"Since data come from different sources, the formats and dispositions of the files are quite different and will need specific handling. The data for each ANA station is consolidated into a single CSV file, which has some headers and captions that will need to be ignored. CEMADEN data, on the other hand, are divided by city and month, that is, although a file contains data from all stations in the city, several files are needed to analyze a period of several years.\n\nBelow is a preview of two of these files:","2db011c5":"In a perfect world the data would almost ready, but for a better quality in our results, we have to pay attention to two other points:\n\n1. From the beginning of 1941 to the end of 2019 there is a total of 948 months, but we have more than 1700 lines in our DataFrame, which means we have repeated dates.\n2. In the file legend, in addition to 'NivelConsistencia' and 'TipoMedicaoChuvas', there is also the definition of 'Status' and not all values are useful to us.\n\nWe will start with the treatment of repeated dates. In fact, if we analyze it carefully, the dates are repeated but with different levels of consistency (indicated in the column 'NivelConsistencia'). A quick look at ANA website reveals what this means:\n> \"The analysis of consistency of rainfall data should aim to identify and correct errors, as well as to fill in faults in the rainfall series.\"<sup><a href=\"#link8\">[8]<\/a><\/sup>\n\nTherefore, \"consisted data\" are those that have already undergone this analysis and correction of possible failures. So, whenever possible, we will give priority to data that is already consisted ('NivelConsistencia' = 2), we will do this by creating a Boolean mask that when applied will filter the lines and return only the version with the highest level of consistency for each date. We will also create another mask to remove data after January 2019.\n\nFinally, let's take the opportunity to sort the data in ascending order by date and transform our 'Data' column into the index of our DataFrame.","b6ed72ae":"From what we see, C\u00f3rrego do Feij\u00e3o station stopped recording or transmitting information in several moments. This demonstrates that only this station's data is not sufficient to get the whole sample we need. We will then use an approach similar to the one we did previously with 'NivelConsistencia' column in ANA dataset and fill in the gaps with data from the other CEMADEN stations in order of proximity, until we have enough data.","7d4e2a24":"## External links\n\n1. <a name=\"link1\"><\/a>\"Heavy rain, design and poor drainage factors in Vale dam breach\", Financial Times. https:\/\/www.ft.com\/content\/e805b142-1cde-11ea-97df-cc63de1d73f4\n2. <a name=\"link2\"><\/a>\"\u2018Vale ended our lives\u2019: Broken Brumadinho a year after dam collapse\", BBC. https:\/\/www.bbc.com\/news\/world-latin-america-51220373\n3. <a name=\"link3\"><\/a>\"Mine Tailings Storage: Safety Is No Accident\", United Nations. http:\/\/www.grida.no\/publications\/383\n4. <a name=\"link4\"><\/a>\"Trag\u00e9dia com barragem da Vale em Brumadinho pode ser a pior no mundo em 3 d\u00e9cadas\", BBC Brasil. https:\/\/www.bbc.com\/portuguese\/brasil-47034499\n5. <a name=\"link5\"><\/a>\"Report of the Expert Panel on the Technical Causes of the Failure of Feij\u00e3o Dam I\". https:\/\/bdrb1investigationstacc.z15.web.core.windows.net\/assets\/Feijao-Dam-I-Expert-Panel-Report-ENG.pdf\n6. <a name=\"link6\"><\/a>Melo Franco station data from ANA's SNIRH. http:\/\/www.snirh.gov.br\/hidroweb\/rest\/api\/documento\/convencionais?tipo=3&documentos=2044008\n7. <a name=\"link7\"><\/a>CEMADEN stations data. http:\/\/www.cemaden.gov.br\/mapainterativo\/download\/downpluv.php\n8. <a name=\"link8\"><\/a>\"DIRETRIZES E AN\u00c1LISES RECOMENDADAS PARA A CONSIST\u00caNCIA DE DADOS PLUVIOM\u00c9TRICOS\". http:\/\/arquivos.ana.gov.br\/infohidrologicas\/cadastro\/DiretrizesEAnalisesRecomendadasParaConsistenciaDeDadosPluviometricos-VersaoJan12.pdf\n9. <a name=\"link9\"><\/a>\"Automatic Pluviometers\". https:\/\/www.cemaden.gov.br\/pluviometros-automatico\/","5336d896":"What really interests us is in the 'Data' and 'Total' columns. They tell us how much it rained each month in the region of Melo Franco station. So let's cut out this data to get a more detailed view. First, however, let's transform the data in the 'Data' column so that Pandas can work with this information in a time format.","25c8c7db":"The last visualization will show details of Brumadinho's rainfall profile. We will display in a single graph the historical rainfall averages per month (and the standard error), the historical maximums per month and the actual measurements from July 2018 to January 2019. The idea is to compare the amount of rain of the months prior to the tragedy with the standard behavior of rainfall and also the most extreme behavior ever recorded in the region."}}