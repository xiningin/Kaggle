{"cell_type":{"6e448643":"code","b50a3e36":"code","84b284a5":"code","f88753a5":"code","d18c744a":"code","580a5512":"code","d9d92230":"code","4291e1f1":"code","82e6669a":"code","7153743e":"code","22f1b6ed":"code","ee44d5ff":"code","8c615c49":"code","d8de3433":"code","e55d3b5c":"code","c67076f7":"code","76307db2":"markdown","89ca30fa":"markdown","e07e9e9f":"markdown"},"source":{"6e448643":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.\npath = \"\/kaggle\/input\/news-summary\/news_summary_more.csv\"\ndf = pd.read_csv(path,encoding = \"ISO-8859-1\")\nprint(df.head())\nprint(df.describe())\n","b50a3e36":"contraction_mapping = {\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n\n                           \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n\n                           \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n\n                           \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n\n                           \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n\n                           \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n\n                           \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n\n                           \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n\n                           \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n\n                           \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n\n                           \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n\n                           \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n\n                           \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n\n                           \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n\n                           \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n\n                           \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n\n                           \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n\n                           \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n\n                           \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n\n                           \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n\n                           \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n\n                           \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n\n                           \"you're\": \"you are\", \"you've\": \"you have\"}\n","84b284a5":"import re\nfrom nltk.corpus import stopwords\n\nstop_words = stopwords.words('english')\n\ndef preprocess(text):\n    text = text.lower() # lowercase\n    text = text.split() # convert have'nt -> have not\n    for i in range(len(text)):\n        word = text[i]\n        if word in contraction_mapping:\n            text[i] = contraction_mapping[word]\n    text = \" \".join(text)\n    text = text.split()\n    newtext = []\n    for word in text:\n        if word not in stop_words:\n            newtext.append(word)\n    text = \" \".join(newtext)\n    text = text.replace(\"'s\",'') # convert your's -> your\n    text = re.sub(r'\\(.*\\)','',text) # remove (words)\n    text = re.sub(r'[^a-zA-Z0-9. ]','',text) # remove punctuations\n    text = re.sub(r'\\.',' . ',text)\n    return text\n\nsample = \"(hello) hi there .man tiger caller who's that isn't it ? WALL-E\"\nprint(preprocess(sample))","f88753a5":"df['headlines'] = df['headlines'].apply(lambda x:preprocess(x))\ndf['text'] = df['text'].apply(lambda x:preprocess(x))\nprint(df['headlines'][20],df['text'][20])","d18c744a":"def get_out_vector(text,summary,n=40):\n    new_vec  = np.zeros(n)\n    for txt in summary.split():\n        if txt in text:\n            for i,word in enumerate(text.split()):\n                if word == txt:\n                    new_vec[i] = 1\n    return new_vec\n\ndef get_summary(text,new_vec,thresh = 0.5):\n    summary = []\n    for i,word in enumerate(text.split()):\n        if new_vec[i] >= thresh:\n            summary.append(word)\n    return \" \".join(summary)\n\nsample_text = \"hola this is a hola hola hola okay test nice\"\nsample_summary = \"hola nice okay\"\nvec = (get_out_vector(sample_text,sample_summary,15))\nprint( get_summary(sample_text,vec) )","580a5512":"print(df.shape)","d9d92230":"# label creation\nrows = df.shape[0]\nno_words_per_sentence = 80 # max words \ny = np.zeros((rows,no_words_per_sentence))\n\nfor i in range(rows):\n    text = df['text'][i]\n    summary = df['headlines'][i]\n    vec = get_out_vector(text,summary,no_words_per_sentence)\n    y[i] = vec\nprint(y[0])\nprint(y[900])","4291e1f1":"from sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer","82e6669a":"no_input_words = 80\nx = np.zeros((rows,no_input_words))\nvectorizer = TfidfVectorizer(use_idf=True,norm='l1')\nvectorizer.fit_transform(df['text'])\nnames = vectorizer.get_feature_names()\nidf = vectorizer.idf_\n\nmaps = { names[i]:idf[i] for i in range(len(names)) }\n\nfor j in range(rows):\n    text = df['text'][j]\n    vec = np.zeros(no_input_words)\n    for i,word in enumerate(text.split()):\n        if word in maps:\n            vec[i] = maps[word]\n    x[j] = vec\nprint(x[0],x[50])    ","7153743e":"print(x.shape,y.shape)","22f1b6ed":"rows = 98401\nx = x.reshape((rows,no_input_words,1))\ny = y.reshape((rows,no_words_per_sentence,1))","ee44d5ff":"from keras.models import Sequential\nfrom keras.models import Model\nfrom keras.layers import LSTM\nfrom keras.layers import Dense\nfrom keras.layers import RepeatVector\nfrom keras.layers import TimeDistributed\n\n\nmodel = Sequential()\nmodel.add(LSTM(128, activation='relu', input_shape=(no_input_words,1)))\nmodel.add(RepeatVector(no_words_per_sentence))\nmodel.add(LSTM(256, activation='relu',return_sequences=True))\nmodel.add(LSTM(128, activation='relu', return_sequences=True))\nmodel.add(TimeDistributed(Dense(1,activation='sigmoid')))\nmodel.compile(optimizer='adam', loss='mse')\nmodel.summary()","8c615c49":"h = model.fit(x,y,validation_split = 0.2, epochs = 10, batch_size = 128)","d8de3433":"%matplotlib inline\nimport matplotlib.pyplot as plt\nplt.plot(h.history['acc'])\nplt.plot(h.history['val_acc'])\nplt.title('Model accuracy')\nplt.show()\n\nplt.plot(h.history['loss'])\nplt.plot(h.history['val_loss'])\nplt.title('Model Loss')\nplt.show()","e55d3b5c":"model.save('model.h5')","c67076f7":"ids = 0\nsample_text = df['text'][ids]\nvec = x[ids]\nres = model.predict(vec)[0]\nres_row = res.shape[0]\nres_col = res.shape[1]\nres = res.reshape((res_row,res_col))\nout = get_summary(sample_text,res)\nprint(out)","76307db2":"**Create new dataset**","89ca30fa":"**Apply Pre-Processing on Dataset**","e07e9e9f":"**Data Loading**"}}