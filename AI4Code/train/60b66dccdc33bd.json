{"cell_type":{"8448717b":"code","b6ece790":"code","9135635b":"code","e244be8a":"code","90072ecb":"code","13feb264":"code","bc6f9d67":"code","1c7299bb":"code","da5cd81b":"code","840585b9":"code","99d84849":"code","ab8fefe1":"code","69f5d47d":"code","fadae126":"code","dad189e0":"code","ad9d5cd9":"code","660aad2b":"code","f1f4ab29":"code","d3162ef6":"code","98c19a05":"code","1d9a8e5a":"code","f0a31684":"code","37798bd9":"code","f98eea89":"code","7169ed3b":"code","f8ec352c":"code","3efe582d":"code","ebdfb211":"code","6be3685f":"code","de530546":"code","5e45c280":"code","efd1ed48":"code","b01d260f":"code","54daa5c9":"code","e167326e":"code","31ee5def":"code","e4f03408":"code","c964f5b3":"code","5177ac57":"code","db219059":"code","bf52f505":"code","a1cc10d1":"code","c32fd131":"code","80868b78":"code","10b3e220":"code","49140a21":"code","89e9052d":"code","7db9c473":"code","7e6bc465":"code","cccdddc5":"code","68624a37":"code","bead4bdd":"code","d8475b4e":"code","d16a90fc":"code","aaf985dc":"code","8260556b":"code","cf74a107":"code","e829172e":"code","23d8ce39":"code","64c3d198":"code","31c5f804":"code","3453b165":"code","53119bfb":"code","ce091c95":"code","aa0ab37a":"code","7b5bf0bb":"code","7a3096d4":"code","28097273":"code","8df9393b":"code","5005b5dc":"code","a60710a1":"code","7ea18f3e":"code","9aa8d6e5":"code","d37c4c6b":"code","6beaad1a":"code","90bd6489":"code","a2fd19c4":"code","9ad198c7":"code","563b2537":"code","502751d1":"code","a09fd60c":"code","acc04a04":"code","9a88f3be":"code","6811c85a":"code","7a82014b":"code","def64791":"code","8d369a94":"code","cc2a7617":"code","bb805624":"code","bcdf4e45":"code","62ec0b50":"code","853f7809":"code","bdc87f92":"code","f3bc6baf":"code","e3e300b3":"code","a0a70205":"code","472d390f":"code","49b0460b":"code","ad9b2492":"code","4d59309e":"code","c5f49629":"code","2db65f29":"code","24b86278":"code","dfd22877":"code","32cfad9b":"code","55eeba5c":"code","43d52a7f":"code","f82339a2":"code","3f4a1f4b":"code","5da51b8d":"code","ba36ed36":"code","85eec920":"code","67309938":"code","33bdefd4":"code","7bdc7f97":"code","95dfd58b":"code","b8d207f9":"code","92065981":"code","e8496232":"code","58968e57":"code","6c30037e":"code","5b8d6926":"code","ab2a19ee":"code","b851440b":"code","511b53ba":"code","c69d63fd":"code","10181d92":"code","6326ddec":"code","648088ac":"code","6c88506b":"code","ae8164ec":"code","94176e32":"code","8fea7279":"code","c6e74f13":"code","06999c62":"code","decec5fb":"code","c8eef170":"code","22efb761":"code","bbb4ee09":"code","f1f3fbe4":"code","62ec0b59":"code","277d3efa":"code","9549ef62":"code","084b9935":"code","b9b4983b":"code","e15864ed":"code","ddbaca1a":"code","7910d91b":"code","02cc2a19":"code","425dfdc6":"code","b2233a31":"code","ef2c0ead":"code","d5ddc433":"code","794c9216":"code","bbd7baab":"code","e059dacf":"code","c1ee97da":"code","759a6552":"code","93c67fe5":"code","502fa938":"code","2d2946f1":"code","07033e95":"code","90ad45d5":"code","bd45d13d":"code","5c2cf6f0":"code","792ec72b":"code","3dc6bc6e":"code","fce7adb4":"code","e879c227":"code","6ff47428":"code","29e8930b":"code","fb0f18e9":"code","c7695f2b":"code","792d29e3":"code","2ad748e1":"code","36a7f579":"markdown","8fcc843b":"markdown","454ad157":"markdown","da44c4b1":"markdown","88f05130":"markdown","61fe5e29":"markdown","cd5119e2":"markdown","a720f61e":"markdown","dcf21337":"markdown","b33abbf8":"markdown","11721c92":"markdown","7ea32e40":"markdown","6f68a2cf":"markdown","a3415369":"markdown","0f565dd7":"markdown","5ae7c233":"markdown","4728eeff":"markdown","5eb13f1e":"markdown","09973f7a":"markdown","c23c02fb":"markdown","b598d419":"markdown","2f42e1b9":"markdown","34fbfeb8":"markdown","9110cb02":"markdown","465791b3":"markdown","45585059":"markdown","38f7a169":"markdown","e3107407":"markdown","4b7f0997":"markdown","43e94626":"markdown","11ad245a":"markdown","870678c3":"markdown","a3475e70":"markdown","d65c3a85":"markdown","7e090630":"markdown","7668596a":"markdown","83fcfd53":"markdown","0d18096f":"markdown","99a44dea":"markdown","98ea1631":"markdown","46b8fbf9":"markdown","1691447d":"markdown","3fd6cd0a":"markdown","430cef29":"markdown","f226e784":"markdown","a0b7a95d":"markdown","670279da":"markdown","dc0337ef":"markdown"},"source":{"8448717b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b6ece790":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # Visualization\nimport matplotlib\nimport seaborn as sns\nimport plotly.express as px\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.linear_model import LinearRegression,Lasso\nfrom sklearn.metrics import mean_absolute_error,mean_squared_error,r2_score\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import AdaBoostRegressor","9135635b":"df_train = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","e244be8a":"pd.set_option('display.max_columns', None)","90072ecb":"df_train.head()","13feb264":"df_train.shape","bc6f9d67":"df_test.shape","1c7299bb":"df_test.head()","da5cd81b":"df = pd.concat([df_train,df_test], ignore_index=True)","840585b9":"df.shape","99d84849":"#df = df_train.drop('Id',axis=1)","ab8fefe1":"df.describe()","69f5d47d":"df.info()","fadae126":"print(df.isnull().sum())","dad189e0":"fig = px.imshow(df.isnull(),height=500,width=1400)\nfig.show()","ad9d5cd9":"missing_feat = [feature for feature in df.columns if df[feature].isnull().sum()>1]","660aad2b":"for feature in missing_feat:\n    print(feature, np.round(df[feature].isnull().mean()*100,4), \" : % missing values\")","f1f4ab29":"missing_feat_above_45 = []\nfor feature in missing_feat:\n    values = np.round(df[feature].isnull().mean()*100,4)\n    #print(values)\n    if values > 45:\n        missing_feat_above_45.append(feature)\nprint(missing_feat_above_45)","d3162ef6":"for feature in missing_feat_above_45:\n    data = df.copy()\n    data[feature] = np.where(data[feature].isnull(),1,0)\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.title(feature)\n    plt.show()","98c19a05":"less_important_feat = ['FireplaceQu','PoolQC']","1d9a8e5a":"for feature in missing_feat:\n    data = df.copy()\n    data[feature] = np.where(data[feature].isnull(),1,0)\n    data.groupby(feature)['SalePrice'].median().plot.bar()\n    plt.title(feature)\n    plt.show()","f0a31684":"categorical_feature = [feature for feature in df.columns if df[feature].dtype=='O']","37798bd9":"print(categorical_feature)","f98eea89":"# for feature in categorical_feature:\n#     plt.figure(figsize=(7,4))\n#     sns.set_theme(style=\"whitegrid\")\n#     ax = sns.countplot(df[feature])\n#     plt.xticks(rotation=90)\n#     plt.xlabel(feature)\n#     plt.show()","7169ed3b":"# for feature in categorical_feature:\n#     plt.figure(figsize=(7,4))\n#     sns.set_theme(style=\"whitegrid\")\n#     ax = sns.violinplot(x=feature, y=\"SalePrice\", data=df)\n#     plt.xticks(rotation=90)\n#     plt.xlabel(feature)\n#     plt.show()","f8ec352c":"temporal_feat = [feature for feature in df.columns if 'Yr' in feature or 'Year' in feature]","3efe582d":"print(temporal_feat)","ebdfb211":"# for feature in temporal_feat:\n#     plt.figure(figsize=(7,4))\n#     sns.set_theme(style=\"whitegrid\")\n#     ax = sns.lineplot(x=feature, y='SalePrice', data=df)\n#     plt.xticks(rotation=90)\n#     plt.title(\"Price vs Year\")\n#     plt.xlabel(feature)\n#     plt.show()","6be3685f":"Id = ['Id']\nnumerical_feature = [feature for feature in df.columns if df[feature].dtype!='O' and feature not in temporal_feat and feature not in Id]","de530546":"print(numerical_feature)","5e45c280":"print(\"Total numerical features we have is : \", len(numerical_feature))","efd1ed48":"discrete_feature = [feature for feature in numerical_feature if len(df[feature].unique())<25] ","b01d260f":"print(\"Total Discrete feature is : \",len(discrete_feature))\nprint(discrete_feature)","54daa5c9":"df[discrete_feature].head(2)","e167326e":"# for feature in discrete_feature:\n#     data = df.copy()\n#     data.groupby(feature)['SalePrice'].median().plot.bar()\n#     plt.title(feature)\n#     plt.xticks(rotation=90)\n#     plt.show()","31ee5def":"contineous_feature = [feature for feature in numerical_feature if feature not in discrete_feature]","e4f03408":"print(\"We have \", len(contineous_feature), \"contineous feature\")\nprint(contineous_feature)","c964f5b3":"df[contineous_feature].head(2)","5177ac57":"# for feature in contineous_feature:\n#     plt.figure(figsize=(7,4))\n#     sns.set_theme(style=\"whitegrid\")\n#     ax = sns.boxplot(df[feature])\n#     plt.xticks(rotation=90)\n#     plt.xlabel(feature)\n#     plt.show()","db219059":"# for feature in contineous_feature:\n#     plt.figure(figsize=(7,4))\n#     sns.distplot(df[feature])\n#     sns.set_theme(style=\"whitegrid\")\n#     plt.xlabel(feature)\n#     plt.xticks(rotation=90)\n#     plt.show()","bf52f505":"# for feature in contineous_feature:\n#     plt.figure(figsize=(7,4))\n#     sns.histplot(df[feature], kde=True)\n#     sns.set_theme(style=\"whitegrid\")\n#     plt.xlabel(feature)\n#     plt.xticks(rotation=90)\n#     plt.show()","a1cc10d1":"# for feature in contineous_feature:\n#     data = df.copy()\n#     if 0 in data[feature].unique():\n#         pass\n#     else:\n#         data[feature] = np.log(data[feature])\n#         sns.histplot(data[feature],kde=True)\n#         plt.xlabel(feature)\n#         plt.show()\n        ","c32fd131":"fig = px.imshow(df.isnull(),height=500,width=900)\nfig.show()","80868b78":"print(less_important_feat)\ndf = df.drop(less_important_feat,axis=1)","10b3e220":"numerical_feature = [feature for feature in df.columns if df[feature].isnull().sum()>1 and df[feature].dtype!='O' and not 'Id']\n\nfor feature in numerical_feature:\n    median_value = df[feature].median()\n    df[feature].fillna(median_value,inplace=True)\n    \ndf[numerical_feature].isnull().sum()","49140a21":"for feature in contineous_feature:\n    if 0 in df[feature].unique():\n        pass\n    else:\n        df[feature] = np.log(df[feature])","89e9052d":"for feature in contineous_feature:\n    sns.histplot(df[feature],kde=True)\n    plt.xlabel(feature)\n    plt.show()","7db9c473":"df.head()","7e6bc465":"date_variable = ['YearBuilt','YearRemodAdd','GarageYrBlt']\nfor feature in date_variable:\n    df[feature] = df['YrSold']-df[feature]","cccdddc5":"df[date_variable].head()","68624a37":"# for feature in cont_feat:\n#     sns.histplot(df[feature],kde=True)\n#     plt.xlabel(feature)\n#     plt.show()","bead4bdd":"categorical_feature = [feature for feature in df.columns if df[feature].dtype=='O']","d8475b4e":"for feature in categorical_feature:\n    print(feature, np.round(df[feature].isnull().mean()*100,4),\"% missing values\")","d16a90fc":"# for feature in categorical_feature:\n#     df[feature] = df[feature].fillna('Missing')\n    \n# df[cat_feat].isnull().sum()","aaf985dc":"categorical_feature_nan = [feature for feature in df.columns if df[feature].dtype=='O' and df[feature].isnull().sum()>1]","8260556b":"print(categorical_feature_nan)","cf74a107":"for feature in categorical_feature_nan:\n    df[feature]=df[feature].fillna(df[feature].mode()[0])\ndf[categorical_feature_nan].isnull().sum()","e829172e":"# def replace_cat_features(dataset,feature_with_na):\n#     data = dataset.copy()\n#     data[feature_with_na]=data[feature_with_na].fillna(data[feature_with_na].mode()[0])\n#     return data","23d8ce39":"# new_df = replace_cat_features(df,categorical_feature_nan)\n# new_df[categorical_feature_nan].isnull().sum()","64c3d198":"miss_feat = [feature for feature in df.columns if df[feature].isnull().sum()>1]","31c5f804":"print(miss_feat)","3453b165":"# plt.figure(figsize=(10,8))\n# sns.heatmap(df.isnull(),cbar=False,cmap='plasma')","53119bfb":"df[categorical_feature].head(2)","ce091c95":"for feature in categorical_feature:\n    temp = df.groupby(feature)['SalePrice'].count()\/len(df)\n    temp_df = temp[temp>0.01].index\n    df[feature] = np.where(df[feature].isin(temp_df),df[feature],'rare_var')","aa0ab37a":"df.head()","7b5bf0bb":"for feature in categorical_feature:\n    label_order = df.groupby([feature])['SalePrice'].mean().sort_values().index\n    label_order = {k:i for i,k in enumerate(label_order,0)}\n    df[feature] = df[feature].map(label_order)","7a3096d4":"df.head()","28097273":"test_df = df.loc[:,1460:]","8df9393b":"test_df.head()","5005b5dc":"file_name = \"Train_Data.csv\"\ndf.to_csv(file_name)","a60710a1":"X = df.drop('SalePrice',axis=1)\ny = df['SalePrice']","7ea18f3e":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25,random_state=101)","9aa8d6e5":"print(X_train.shape,X_test.shape)\nprint(y_train.shape,y_test.shape)","d37c4c6b":"sc = StandardScaler()\nX_train_sc = sc.fit_transform(X_train)\nX_test_sc = sc.transform(X_test)\nX_train_sc = pd.DataFrame(X_train_sc, columns=X_train.columns, index=X_train.index)\nX_test_sc = pd.DataFrame(X_test_sc, columns=X_test.columns, index=X_test.index)","6beaad1a":"X_test_sc","90bd6489":"# best_feature = SelectFromModel(Lasso(alpha=0.05,random_state=101))","a2fd19c4":"# best_feature.fit(X_train_sc,y_train)","9ad198c7":"# best_feature.get_support()","563b2537":"# selected_feature = X_train.columns[(best_feature.get_support())]","502751d1":"# selected_feature","a09fd60c":"# np.sum(best_feature.estimator_.coef_ == 0)","acc04a04":"# X_train_sc = X_train_sc[selected_feature]\n# X_test_sc = X_test_sc[selected_feature]","9a88f3be":"# print('Total Feature : ',len(X.columns))\n# print('Selected Feature : ',len(selected_feature))\n# print('Features which Coefficient turns to 0 : ',np.sum(best_feature.estimator_.coef_ == 0))","6811c85a":"pc = PCA(n_components=len(X_train_sc.columns))","7a82014b":"X_train_pc = pc.fit_transform(X_train_sc)\npc_df_train = pd.DataFrame(X_train_pc,columns=['PC_'+str(i) for i in range(1,pc.n_components_+1)])","def64791":"pc_df_train","8d369a94":"plt.figure(figsize=(50,6))\nplt.plot(pc_df_train.std())\nplt.title(\"Scree Plot (Principal Component Ananlysis)\")\nplt.xlabel('Principal Component')\nplt.ylabel('Standard Deviation')\nplt.show()","cc2a7617":"pc = PCA(n_components=9)\nX_train_pc = pc.fit_transform(X_train_sc)\nPC_df_train = pd.DataFrame(X_train_pc,columns=['PC_'+str(i) for i in range(1,pc.n_components_+1)])","bb805624":"X_test_pc = pc.transform(X_test_sc)\nPC_df_test=pd.DataFrame(X_test_pc,columns=['PC_' +str(i) for i in range(1,pc.n_components_+1)])","bcdf4e45":"print(PC_df_train.shape)","62ec0b50":"lr_regressor = LinearRegression()\nlr_regressor.fit(X_train_pc,y_train)","853f7809":"lr_y = lr_regressor.predict(X_test_pc)","bdc87f92":"sns.distplot(lr_y-y_test)","f3bc6baf":"print('Coefficient o R^2 <-- on train data : {}'.format(lr_regressor.score(X_train_pc,y_train)))\nprint('Coefficient o R^2 <-- on test data : {}'.format(lr_regressor.score(X_test_pc,y_test)))\nprint(\"Mean absolute Error : \",mean_absolute_error(lr_y,y_test))\nprint(\"Mean squared Error : \",mean_squared_error(lr_y,y_test))\nprint(\"Root Mean Squared Error : \",np.sqrt(mean_squared_error(lr_y,y_test)))","e3e300b3":"svr_regressor = SVR()\nsvr_regressor.fit(X_train_pc,y_train)","a0a70205":"svr_y = svr_regressor.predict(X_test_pc)","472d390f":"sns.distplot(svr_y-y_test)","49b0460b":"print('Coefficient o R^2 <-- on train data : {}'.format(svr_regressor.score(X_train_pc,y_train)))\nprint('Coefficient o R^2 <-- on test data : {}'.format(svr_regressor.score(X_test_pc,y_test)))\nprint(\"Mean absolute Error : \",mean_absolute_error(svr_y,y_test))\nprint(\"Mean squared Error : \",mean_squared_error(svr_y,y_test))\nprint(\"Root Mean Squared Error : \",np.sqrt(mean_squared_error(svr_y,y_test)))","ad9b2492":"knn_regressor = KNeighborsRegressor()\nknn_regressor.fit(X_train_pc,y_train)","4d59309e":"knn_y = knn_regressor.predict(X_test_pc)","c5f49629":"sns.distplot(knn_y-y_test)","2db65f29":"print('Coefficient o R^2 <-- on train data : {}'.format(knn_regressor.score(X_train_pc,y_train)))\nprint('Coefficient o R^2 <-- on test data : {}'.format(knn_regressor.score(X_test_pc,y_test)))\nprint(\"Mean absolute Error : \",mean_absolute_error(knn_y,y_test))\nprint(\"Mean squared Error : \",mean_squared_error(knn_y,y_test))\nprint(\"Root Mean Squared Error : \",np.sqrt(mean_squared_error(knn_y,y_test)))","24b86278":"dt_regressor = DecisionTreeRegressor()","dfd22877":"dt_regressor.fit(X_train_pc,y_train)","32cfad9b":"dt_y = dt_regressor.predict(X_test_pc)","55eeba5c":"sns.distplot(dt_y-y_test)","43d52a7f":"print('Coefficient o R^2 <-- on train data : {}'.format(dt_regressor.score(X_train_pc,y_train)))\nprint('Coefficient o R^2 <-- on test data : {}'.format(dt_regressor.score(X_test_pc,y_test)))\nprint(\"Mean absolute Error : \",mean_absolute_error(dt_y,y_test))\nprint(\"Mean squared Error : \",mean_squared_error(dt_y,y_test))\nprint(\"Root Mean Squared Error : \",np.sqrt(mean_squared_error(dt_y,y_test)))","f82339a2":"rf_regressor = RandomForestRegressor()\nrf_regressor.fit(X_train_pc,y_train)","3f4a1f4b":"rf_y = rf_regressor.predict(X_test_pc)","5da51b8d":"sns.distplot(rf_y-y_test)","ba36ed36":"print('Coefficient o R^2 <-- on train data : {}'.format(rf_regressor.score(X_train_pc,y_train)))\nprint('Coefficient o R^2 <-- on test data : {}'.format(rf_regressor.score(X_test_pc,y_test)))\nprint(\"Mean absolute Error : \",mean_absolute_error(rf_y,y_test))\nprint(\"Mean squared Error : \",mean_squared_error(rf_y,y_test))\nprint(\"Root Mean Squared Error : \",np.sqrt(mean_squared_error(rf_y,y_test)))","85eec920":"xgb_regressor = XGBRegressor()\nxgb_regressor.fit(X_train_pc,y_train)","67309938":"xgb_y = xgb_regressor.predict(X_test_pc)","33bdefd4":"sns.distplot(xgb_y-y_test)","7bdc7f97":"print('Coefficient o R^2 <-- on train data : {}'.format(xgb_regressor.score(X_train_pc,y_train)))\nprint('Coefficient o R^2 <-- on test data : {}'.format(xgb_regressor.score(X_test_pc,y_test)))\nprint(\"Mean absolute Error : \",mean_absolute_error(xgb_y,y_test))\nprint(\"Mean squared Error : \",mean_squared_error(xgb_y,y_test))\nprint(\"Root Mean Squared Error : \",np.sqrt(mean_squared_error(xgb_y,y_test)))","95dfd58b":"adab_regressor = AdaBoostRegressor()\nadab_regressor.fit(X_train_pc,y_train)","b8d207f9":"adb_y = adab_regressor.predict(X_test_pc)","92065981":"sns.distplot(adb_y-y_test)","e8496232":"print('Coefficient o R^2 <-- on train data : {}'.format(adab_regressor.score(X_train_pc,y_train)))\nprint('Coefficient o R^2 <-- on test data : {}'.format(adab_regressor.score(X_test_pc,y_test)))\nprint(\"Mean absolute Error : \",mean_absolute_error(adb_y,y_test))\nprint(\"Mean squared Error : \",mean_squared_error(adb_y,y_test))\nprint(\"Root Mean Squared Error : \",np.sqrt(mean_squared_error(adb_y,y_test)))","58968e57":"lr_df = pd.DataFrame(data=[lr_regressor.score(X_train_pc,y_train),lr_regressor.score(X_test_pc, y_test), mean_absolute_error(y_test, lr_y), mean_squared_error(y_test, lr_y), np.sqrt(mean_squared_error(y_test, lr_y))], \n             columns=['Linear Regression'], index=[\"R2 Score Train\",\"R2 Score Test\", \"Root Mean Absolute Error\", \"Mean Square Error\", \"Root Mean Square Error\"])\nsvr_df = pd.DataFrame(data=[svr_regressor.score(X_train_pc,y_train),svr_regressor.score(X_test_pc, y_test), mean_absolute_error(y_test, svr_y),mean_squared_error(y_test, svr_y), np.sqrt(mean_squared_error(y_test, svr_y))], \n             columns=['Support Vector Regressor'],index=[\"R2 Score Train\",\"R2 Score Test\", \"Root Mean Absolute Error\", \"Mean Square Error\", \"Root Mean Square Error\"])\nknn_df = pd.DataFrame(data=[knn_regressor.score(X_train_pc,y_train),knn_regressor.score(X_test_pc, y_test), mean_absolute_error(y_test, knn_y), mean_squared_error(y_test, knn_y), np.sqrt(mean_squared_error(y_test, knn_y))], \n             columns=['K Nearest Neighbour'], index=[\"R2 Score Train\",\"R2 Score Test\", \"Root Mean Absolute Error\", \"Mean Square Error\", \"Root Mean Square Error\"])\n\ndt_df = pd.DataFrame(data=[dt_regressor.score(X_train_pc,y_train),dt_regressor.score(X_test_pc, y_test), mean_absolute_error(y_test, dt_y), mean_squared_error(y_test, dt_y), np.sqrt(mean_squared_error(y_test, dt_y))], \n             columns=['Decision Tree'], index=[\"R2 Score Train\",\"R2 Score Test\", \"Root Mean Absolute Error\", \"Mean Square Error\", \"Root Mean Square Error\"])\nrf_df = pd.DataFrame(data=[rf_regressor.score(X_train_pc,y_train),rf_regressor.score(X_test_pc, y_test), mean_absolute_error(y_test, rf_y), mean_squared_error(y_test, rf_y), np.sqrt(mean_squared_error(y_test,rf_y))], \n             columns=['Random Forest Score'], index=[\"R2 Score Train\",\"R2 Score Test\", \"Root Mean Absolute Error\", \"Mean Square Error\", \"Root Mean Square Error\"])\nxgb_df = pd.DataFrame(data=[xgb_regressor.score(X_train_pc,y_train),xgb_regressor.score(X_test_pc, y_test), mean_absolute_error(y_test, xgb_y), mean_squared_error(y_test, xgb_y), np.sqrt(mean_squared_error(y_test,xgb_y))], \n             columns=['XGBoost'], index=[\"R2 Score Train\",\"R2 Score Test\", \"Root Mean Absolute Error\", \"Mean Square Error\", \"Root Mean Square Error\"])\nadb_df = pd.DataFrame(data=[adab_regressor.score(X_train_pc,y_train),adab_regressor.score(X_test_pc, y_test), mean_absolute_error(y_test, adb_y), mean_squared_error(y_test, adb_y), np.sqrt(mean_squared_error(y_test,adb_y))], \n             columns=['AdaBoost'], index=[\"R2 Score Train\",\"R2 Score Test\", \"Root Mean Absolute Error\", \"Mean Square Error\", \"Root Mean Square Error\"])\n\n\ndf_models = round(pd.concat([lr_df,svr_df,knn_df,dt_df,rf_df,xgb_df,adb_df], axis=1),3)\ncolors = [\"bisque\",\"ivory\",\"sandybrown\",\"steelblue\",\"lightsalmon\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nbackground_color = \"white\"\n\nfig = plt.figure(figsize=(18,26)) # create figure\ngs = fig.add_gridspec(4, 2)\ngs.update(wspace=0.1, hspace=0.5)\nax0 = fig.add_subplot(gs[0, :])\n\nsns.heatmap(df_models.T, cmap=colormap,annot=True,fmt=\".1%\",vmin=0,vmax=0.95, linewidths=2.5,cbar=False,ax=ax0,annot_kws={\"fontsize\":16})\nfig.patch.set_facecolor(background_color) # figure background color\nax0.set_facecolor(background_color) \n\nax0.text(0,-0.5,'Model Comparison',fontsize=20,fontweight='bold',fontfamily='serif')\nplt.show()","6c30037e":"rf_regressor = RandomForestRegressor()","5b8d6926":"n_estimators = [int(x) for x in np.linspace(start=100, stop = 1200, num=12)]\nmax_depth = [int(x) for x in np.linspace(5, 30, num=6)]\nmax_features=['auto','sqrt']\nmin_samples_split= [2, 5, 10, 15, 100]\nmin_samples_leaf = [1, 2, 5, 10]","ab2a19ee":"random_grid = {'n_estimators':n_estimators,\n         'max_depth':max_depth,\n         'max_features':max_features,\n         'min_samples_split':min_samples_split,\n         'min_samples_leaf':min_samples_leaf}\nprint(random_grid)","b851440b":"from sklearn.model_selection import RandomizedSearchCV\nrcv_regressor = RandomizedSearchCV(estimator = rf_regressor, param_distributions = random_grid, n_iter = 100 , scoring = 'neg_mean_squared_error', n_jobs = 1,cv = 5, verbose = 2,random_state = 101)","511b53ba":"rcv_regressor.fit(X_train_pc,y_train)","c69d63fd":"rcv_regressor.best_params_","10181d92":"hyper_forest = RandomForestRegressor(n_estimators=400,min_samples_split=10,min_samples_leaf=5,max_features='auto',max_depth=15)","6326ddec":"hyper_forest.fit(X_train_pc,y_train)","648088ac":"rfh_y_pred = hyper_forest.predict(X_test_pc)","6c88506b":"print('Coefficient o R^2 <-- on train data : {}'.format(hyper_forest.score(X_train_pc,y_train)))\nprint('Coefficient o R^2 <-- on test data : {}'.format(hyper_forest.score(X_test_pc,y_test)))\nprint(\"Mean absolute Error : \",mean_absolute_error(rfh_y_pred,y_test))\nprint(\"Mean squared Error : \",mean_squared_error(rfh_y_pred,y_test))\nprint(\"Root Mean Squared Error : \",np.sqrt(mean_squared_error(rfh_y_pred,y_test)))","ae8164ec":"sns.distplot(y_test-rfh_y_pred)","94176e32":"sns.scatterplot(y_test,rfh_y_pred)","8fea7279":"## Hyper Parameter Optimization\n\nparams={\n \"learning_rate\"    : [0.05, 0.10, 0.15, 0.20, 0.25, 0.30 ] ,\n \"max_depth\"        : [ 3, 4, 5, 6, 8, 10, 12, 15],\n \"min_child_weight\" : [ 1, 3, 5, 7 ],\n \"gamma\"            : [ 0.0, 0.1, 0.2 , 0.3, 0.4 ],\n \"colsample_bytree\" : [ 0.3, 0.4, 0.5 , 0.7 ]\n    \n}","c6e74f13":"def timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))","06999c62":"import xgboost\nhyper_xgboost = xgboost.XGBRegressor()","decec5fb":"random_search=RandomizedSearchCV(hyper_xgboost,param_distributions=params,n_iter=5,scoring='neg_mean_squared_error',n_jobs=-1,cv=5,verbose=3)","c8eef170":"from datetime import datetime\n# Here we go\nstart_time = timer(None) # timing starts from this point for \"start_time\" variable\nrandom_search.fit(X_train_pc,y_train)\ntimer(start_time) # timing ends here for \"start_time\" variable","22efb761":"random_search.best_estimator_","bbb4ee09":"random_search.best_params_","f1f3fbe4":"hyper_xgboost = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.4, gamma=0.2, gpu_id=-1,\n             importance_type='gain', interaction_constraints='',\n             learning_rate=0.15, max_delta_step=0, max_depth=4,\n             min_child_weight=5, monotone_constraints='()',\n             n_estimators=100, n_jobs=4, num_parallel_tree=1, random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, subsample=1,\n             tree_method='exact', validate_parameters=1, verbosity=None)","62ec0b59":"from sklearn.model_selection import cross_val_score\nscore=cross_val_score(hyper_xgboost,X_train_pc,y_train,cv=10)","277d3efa":"score","9549ef62":"hyper_xgboost.fit(X_train_pc,y_train)","084b9935":"xgh_y_pred=hyper_xgboost.predict(X_test_pc)","b9b4983b":"sns.distplot(xgh_y_pred-y_test)","e15864ed":"sns.scatterplot(xgh_y_pred,y_test)","ddbaca1a":"print('Coefficient o R^2 <-- on train data : {}'.format(hyper_xgboost.score(X_train_pc,y_train)))\nprint('Coefficient o R^2 <-- on test data : {}'.format(hyper_xgboost.score(X_test_pc,y_test)))\nprint(\"Mean absolute Error : \",mean_absolute_error(xgh_y_pred,y_test))\nprint(\"Mean squared Error : \",mean_squared_error(xgh_y_pred,y_test))\nprint(\"Root Mean Squared Error : \",np.sqrt(mean_squared_error(xgh_y_pred,y_test)))","7910d91b":"adaboost_params = {\n    \"n_estimators\"   : [10, 20, 30, 40, 50, 80, 100],\n    \"learning_rate\"  : [0.01, 0.05, 0.1, 0.3,1],\n    \"loss\"           : ['linear', 'square', 'exponential']\n         }\nprint(adaboost_params)","02cc2a19":"hyper_adaboost = AdaBoostRegressor()","425dfdc6":"pre_gs_inst = RandomizedSearchCV(AdaBoostRegressor(), param_distributions = adaboost_params, cv=3, n_iter = 10, n_jobs=-1)","b2233a31":"random_search = RandomizedSearchCV(hyper_adaboost, param_distributions=adaboost_params ,n_iter=5, scoring='neg_mean_squared_error', n_jobs=-1, cv=5, verbose=3)\n","ef2c0ead":"random_search.fit(X_train_pc,y_train)","d5ddc433":"random_search.best_params_","794c9216":"hyper_adaboost = AdaBoostRegressor(n_estimators=80, loss='linear', learning_rate=0.3)","bbd7baab":"hyper_adaboost.fit(X_train_pc,y_train)","e059dacf":"hadb_y_pred = hyper_adaboost.predict(X_test_pc)","c1ee97da":"sns.distplot(hadb_y_pred-y_test)","759a6552":"sns.scatterplot(hadb_y_pred,y_test)","93c67fe5":"rf_df = pd.DataFrame(data=[hyper_forest.score(X_train_pc,y_train),hyper_forest.score(X_test_pc, y_test), mean_absolute_error(rfh_y_pred,y_test), mean_squared_error(rfh_y_pred,y_test), np.sqrt(mean_squared_error(rfh_y_pred,y_test))], \n             columns=['Random Forest Score'], index=[\"R2 Score Train\",\"R2 Score Test\", \"Root Mean Absolute Error\", \"Mean Square Error\", \"Root Mean Square Error\"])\n\nxgb_df = pd.DataFrame(data=[hyper_xgboost.score(X_train_pc,y_train),hyper_xgboost.score(X_test_pc, y_test), mean_absolute_error(y_test, xgh_y_pred), mean_squared_error(y_test, xgh_y_pred), np.sqrt(mean_squared_error(y_test,xgh_y_pred))], \n             columns=['XGBoost'], index=[\"R2 Score Train\",\"R2 Score Test\", \"Root Mean Absolute Error\", \"Mean Square Error\", \"Root Mean Square Error\"])\n\nadb_df = pd.DataFrame(data=[hyper_adaboost.score(X_train_pc,y_train),hyper_adaboost.score(X_test_pc, y_test), mean_absolute_error(y_test, adb_y), mean_squared_error(y_test, hadb_y_pred), np.sqrt(mean_squared_error(y_test,hadb_y_pred))], \n             columns=['AdaBoost'], index=[\"R2 Score Train\",\"R2 Score Test\", \"Root Mean Absolute Error\", \"Mean Square Error\", \"Root Mean Square Error\"])\n\n\ndf_models = round(pd.concat([rf_df,xgb_df,adb_df], axis=1),3)\ncolors = [\"bisque\",\"ivory\",\"sandybrown\",\"steelblue\",\"lightsalmon\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nbackground_color = \"white\"\n\nfig = plt.figure(figsize=(18,26)) # create figure\ngs = fig.add_gridspec(4, 2)\ngs.update(wspace=0.1, hspace=0.5)\nax0 = fig.add_subplot(gs[0, :])\n\nsns.heatmap(df_models.T, cmap=colormap,annot=True,fmt=\".1%\",vmin=0,vmax=0.95, linewidths=2.5,cbar=False,ax=ax0,annot_kws={\"fontsize\":16})\nfig.patch.set_facecolor(background_color) # figure background color\nax0.set_facecolor(background_color) \n\nax0.text(0,-0.5,'Model Comparison',fontsize=20,fontweight='bold',fontfamily='serif')\nplt.show()","502fa938":"from sklearn.ensemble import StackingRegressor\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import RepeatedKFold\nfrom numpy import mean\nfrom numpy import std","2d2946f1":"# def get_stacking():\n#     #defining base model\n#     level0 = list()\n#     level0.append(('svr',SVR()))\n#     level0.append(('knn',KNeighborsRegressor()))\n#     level0.append(('dtree',DecisionTreeRegressor()))\n#     level0.append(('rforest',DecisionTreeRegressor()))\n#     level0.append(('xgboost',XGBRegressor()))\n#     level0.append(('adaboost',AdaBoostRegressor()))\n#     #defining meta lerner model\n#     level1 = LinearRegression()\n#     #defining the stacking ensemble\n#     model = StackingRegressor(estimators=level0, final_estimator=level1, cv=5)\n#     return model","07033e95":"# def get_models():\n#     models = dict()\n#     models['svr'] = SVR()\n#     models['knn'] = KNeighborsRegressor()\n#     models['dtree'] = DecisionTreeRegressor()\n#     models['rforest'] = DecisionTreeRegressor()\n#     models['xgboost'] = XGBRegressor()\n#     models['adaboost'] = AdaBoostRegressor()\n#     models['stacking'] = get_stacking()\n#     return models","90ad45d5":"# def evaluate_model(model, X, y):\n#     cv = RepeatedKFold(n_splits=10, n_repeats=3, random_state=1)\n#     scores = cross_val_score(model, X, y, scoring='neg_mean_squared_error',cv=cv, n_jobs=-1, error_score='raise')\n#     return scores","bd45d13d":"# #get models to eavluate\n# models = get_models()","5c2cf6f0":"# results, names = list(), list()\n# name, model = list(), list()\n# for name, model in models.items():\n#     scores = evaluate_model(model, X_train_pc, y_train)\n#     results.append(scores)\n#     names.append(name)\n#     print('>%s %.3f (%.3f)' %(name, mean(scores), std(scores)))","792ec72b":"#Plot model performance for comparison\nplt.boxplot(results, labels=names, showmeans=True)\nplt.show()","3dc6bc6e":"#defining base model\nlevel0 = list()\n\nlevel0.append(('rforest', RandomForestRegressor()))\nlevel0.append(('knn',KNeighborsRegressor()))\nlevel0.append(('dtree',DecisionTreeRegressor()))\nlevel0.append(('svr',SVR()))\nlevel0.append(('xgboost',XGBRegressor()))\nlevel0.append(('adaboost',AdaBoostRegressor()))\n    \n#defining meta lerner model\nlevel1 = LinearRegression()\n\n    \n#defining the stacking ensemble\nmodel = StackingRegressor(estimators=level0, final_estimator=level1, cv=10)\n\n#it the model\nmodel.fit(X_train_pc,y_train)","fce7adb4":"#final_pred = model['stacking'].predict(X_test_pc)","e879c227":"final_pred = model.predict(X_test_pc)","6ff47428":"stacked_df = pd.DataFrame(data=[model.score(X_train_pc,y_train),model.score(X_test_pc, y_test), mean_absolute_error(final_pred,y_test), mean_squared_error(final_pred,y_test), np.sqrt(mean_squared_error(final_pred,y_test))], \n             columns=['Stacked Model'], index=[\"R2 Score Train\",\"R2 Score Test\", \"Root Mean Absolute Error\", \"Mean Square Error\", \"Root Mean Square Error\"])\n\ndf_models = round(pd.concat([stacked_df], axis=1),3)\ncolors = [\"bisque\",\"ivory\",\"sandybrown\",\"steelblue\",\"lightsalmon\"]\ncolormap = matplotlib.colors.LinearSegmentedColormap.from_list(\"\", colors)\n\nbackground_color = \"white\"\n\nfig = plt.figure(figsize=(18,26)) # create figure\ngs = fig.add_gridspec(4, 2)\ngs.update(wspace=0.1, hspace=0.5)\nax0 = fig.add_subplot(gs[0, :])\n\nsns.heatmap(df_models.T, cmap=colormap,annot=True,fmt=\".1%\",vmin=0,vmax=0.95, linewidths=2.5,cbar=False,ax=ax0,annot_kws={\"fontsize\":16})\nfig.patch.set_facecolor(background_color) # figure background color\nax0.set_facecolor(background_color) \n\nax0.text(0,-0.5,'Model Comparison',fontsize=20,fontweight='bold',fontfamily='serif')\nplt.show()","29e8930b":"sns.distplot(final_pred-y_test)","fb0f18e9":"sns.scatterplot(final_pred,y_test)","c7695f2b":"sample_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv')","792d29e3":"sample_df.head()","2ad748e1":"test_df.head()","36a7f579":"### Hypertune ADABoost","8fcc843b":"### Now in numerical features we can replace nan values with mean, median or mode, here I am using median because we have seen outliers in the feature","454ad157":"### PCA Visualization","da44c4b1":"#### As we can see most of the model we have is Overfitted. As well as we are ending up getting a 80% accuracy. Let us do some more feature engineering to get a better result","88f05130":"## Meta Model\n### Combining more than 1 machine learning model to  get the best result","61fe5e29":"## Exploratry Data Analysis","cd5119e2":"### We can see that it is forming an elbow at PC_2, So we can take 3 principal components for further analysis","a720f61e":"### Performace Metrics for SVR","dcf21337":"#### Now most of our data is Normally Distributed","b33abbf8":"### Feature eleimination L1 Reghularization","11721c92":"### Random Forest","7ea32e40":"### ADABoost","6f68a2cf":"### These are the features with missing values and their corresponding percentage, Now lets check each and every features which are having missing values","a3415369":"### We have missing values in the features which were yellow in color","0f565dd7":"#### In Renovated House we can see price is always increase with the year but in year built we can see that the price is highly fluctuated with increase in year. It may be due to area or some other factor. ","5ae7c233":"#### Here we can see very few % of data is missing. I am going to replace these values wih a new label as missing","4728eeff":"### These are the features which are having more than 45% of the missing values. In bar plot whose value is 1 are directly affecting the sales price. So, we need not to drop them directly. ","5eb13f1e":"### Here we can say that after applying log normal distribution most of our data are transformed in normal distribution curve ","09973f7a":"### Split Data into train-test","c23c02fb":"### Lets again check our data after applying log normal distribution","b598d419":"## Feature Engineering","2f42e1b9":"### With the help of violin plot we can also see the categorical distribution with respect to \"Sales Price\"","34fbfeb8":"#### Here I have seen some features having missing data more than 45 to 99 lets first check those are important or not","9110cb02":"### This is the categorical distribution of the categorical data\n### As we have seen some of the categories were almost 1% of the sub category, we can call these as a rare categories. We need to handle these category while doing pre processing otherwise we may made our model complex","465791b3":"### Here we can analyze the price distribution according to the discrete variables ","45585059":"#### Now to handle categorical variable we can either use OneHotEncoder, LabelEncoder or pd.get_dummies to convert these features into numerical features","38f7a169":"### Lets Hypertune our model, I am going to hypertune Random Forest and XGBoost","e3107407":"#### From the above distplot and histogram we can say that our data are mostly left skewed","4b7f0997":"### Data Pre Processing Finished","43e94626":"### Model Building","11ad245a":"#### Lets drop less important feature which we have already declared in EDA","870678c3":"### Decision Tree","a3475e70":"### K Nearest Neighbour","d65c3a85":"#### Lets handle our temporal variable","7e090630":"### Hypertune XGBoost","7668596a":"### Now lets Transform our contineous features with the help of log normal distribution","83fcfd53":"##### Here we have replaced all the nan value with 1 with respect to Sales Price. Wherever 1 is high it mmeans our feature is playing a significant role in Sales price. So we need to find a way to replace those nan values with a significant value.","0d18096f":"### SVR","99a44dea":"### Linear Regression","98ea1631":"### Now last step of our pre processing is to handle the rare categorical features","46b8fbf9":"#### While doing the EDA we have seen some of the categories were very rare. I am going to handle those rare category now","1691447d":"### XGBoost","3fd6cd0a":"### Performace Metrics for Linear Regression","430cef29":"### Now we need to seperate contineous numerical feature and discrete numerical feature","f226e784":"## Data Preprocessing","a0b7a95d":"#### With the help of Box Plot we have found the outliers in the contineous feature and we can fix them using mean, median or according to our observation","670279da":"### Standardization","dc0337ef":"### Now we shouldn't have any missing values in our datset. Lets check for nan last time!!"}}