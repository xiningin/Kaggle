{"cell_type":{"98ef9291":"code","efec6b34":"code","c1c0a1b6":"code","2ccb729d":"code","2c6facf4":"code","80632332":"code","280562a4":"code","c4e2b6e4":"code","c723bdd8":"code","14b2ae48":"code","628798b4":"code","3f37e9e6":"code","773228f3":"code","3ea50888":"code","0eb0850c":"code","731c1803":"code","aa63137c":"code","607cf5be":"code","e88608a3":"code","b6ebfe81":"code","1953a16e":"code","086e5b07":"code","8a8bb1f9":"code","7a13925e":"code","b6a86fa8":"code","d6d654c5":"code","8d71362a":"code","3a0e9afd":"code","c75174e9":"code","4ac107e5":"code","001abfae":"code","949eed6a":"code","eb0f0ed4":"code","e088b75b":"code","940579c0":"code","6d8b9963":"code","5fadf8ee":"code","327c24d2":"code","93d2ab4f":"code","e5df5cc3":"code","d1d825ea":"markdown","814bc461":"markdown","bb5fbd24":"markdown","a1b077a5":"markdown","183f6184":"markdown","bc9d40cb":"markdown","22c765af":"markdown","ad182510":"markdown","44a57d5f":"markdown","326a68d9":"markdown","875f137e":"markdown","0698d245":"markdown","da57fa9d":"markdown","7992b809":"markdown","79bff8f7":"markdown","1c80d717":"markdown"},"source":{"98ef9291":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\nimport os\nimport plotly.graph_objects as go\nimport time\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\n\ntorch.manual_seed(103)\ntorch.cuda.manual_seed(103)\nnp.random.seed(103)\n\ndeviceCount = torch.cuda.device_count()\nprint(deviceCount)\n\ncuda0 = None\nif deviceCount > 0:\n  print(torch.cuda.get_device_name(0))\n  cuda0 = torch.device('cuda:0')","efec6b34":"df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ndf.sample(5)","c1c0a1b6":"df.info()","2ccb729d":"sns.countplot(x='target', data=df)\nplt.gca().set_ylabel('tweets')","2c6facf4":"start_time = time.time()\n\n%cd '\/kaggle'\n!wget -q http:\/\/nlp.stanford.edu\/data\/glove.twitter.27B.zip\n!unzip -q glove.twitter.27B.zip\n\n!ls\n%cd '\/kaggle\/working'\n\nprint(f'\\nDuration: {time.time() - start_time:.0f} seconds')","80632332":"text_embedding_dimension = 200\nkey_embedding_dimension = 25\n\n\npath_to_glove_file = '\/kaggle\/glove.twitter.27B.{}d.txt'.format(text_embedding_dimension)\n\nembeddings_index_200 = {}\nwith open(path_to_glove_file) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index_200[word] = coefs\n\nprint(\"Found %s word vectors.\" % len(embeddings_index_200))\n\n\n\npath_to_glove_file = '\/kaggle\/glove.twitter.27B.{}d.txt'.format(key_embedding_dimension)\n\nembeddings_index_25 = {}\nwith open(path_to_glove_file) as f:\n    for line in f:\n        word, coefs = line.split(maxsplit=1)\n        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n        embeddings_index_25[word] = coefs\n\nprint(\"Found %s word vectors.\" % len(embeddings_index_25))","280562a4":"def clean_text(text):\n    \n    # lower case characters only\n    text = text.lower() \n    \n    # remove urls\n    text = re.sub('http\\S+', ' ', text)\n    \n    # only alphabets, spaces and apostrophes \n    text = re.sub(\"[^a-z' ]+\", ' ', text)\n    \n    # remove all apostrophes which are not used in word contractions\n    text = ' ' + text + ' '\n    text = re.sub(\"[^a-z]'|'[^a-z]\", ' ', text)\n    \n    return text.split()\n\ndf['text'] = df['text'].apply(lambda x: clean_text(x))\n\ndf.sample(5)","c4e2b6e4":"unknown_words = []\ntotal_words = 0\n\ndef find_unknown_words(words):\n    \n    global total_words\n    total_words = total_words + len(words)\n    \n    for word in words:\n        if not (word in embeddings_index_200):\n            unknown_words.append(word)\n    \n    return words\n\n\ndf['text'].apply(lambda words: find_unknown_words(words))\n\nprint( f'{len(unknown_words)\/total_words*100:5.2} % of words are unknown' )","c723bdd8":"def analyze_unknown_words(unknown_words):\n    \n    unknown_words = np.array(unknown_words)\n    (word, count) = np.unique(unknown_words, return_counts=True)\n    \n    word_freq = pd.DataFrame({'word': word, 'count': count}).sort_values('count', ascending=False)\n\n    fig = go.Figure(data=[go.Table(\n          header=dict(values=list(word_freq.columns),\n                    fill_color='paleturquoise',\n                    align='left'),\n          cells=dict(values=[word_freq['word'], word_freq['count']],\n                    fill_color='lavender',\n                    align='left'))\n          ])\n    fig.update_layout(width=300, height=300, margin={'b':0, 'l':0, 'r':0, 't':0, 'pad':0})\n    fig.show()\n        \nanalyze_unknown_words(unknown_words)","14b2ae48":"contractions  = { \"i'm\" : \"i am\", \"it's\" : \"it is\", \"don't\" : \"do not\", \"can't\" : \"cannot\", \n                  \"you're\" : \"you are\", \"that's\" : \"that is\", \"we're\" : \"we are\", \"i've\" : \"i have\", \n                  \"he's\" : \"he is\", \"there's\" : \"there is\", \"i'll\" : \"i will\", \"i'd\" : \"i would\", \n                  \"doesn't\" : \"does not\", \"what's\" : \"what is\", \"didn't\" : \"did not\", \n                  \"wasn't\" : \"was not\", \"hasn't\" : \"has not\", \"they're\" : \"they are\", \n                  \"let's\" : \"let us\", \"she's\" : \"she is\", \"isn't\" : \"is not\", \"ain't\" : \"not\", \n                  \"aren't\" : \"are not\", \"haven't\" : \"have not\", \"you'll\" : \"you will\", \n                  \"we've\" : \"we have\", \"you've\" : \"you have\", \"y'all\" : \"you all\", \n                  \"weren't\" : \"were not\", \"couldn't\" : \"could not\", \"would've\" : \"would have\", \n                  \"they've\" : \"they have\", \"they'll\" : \"they will\", \"you'd\" : \"you would\", \n                  \"they'd\" : \"they would\", \"it'll\" : \"it will\", \"where's\" : \"where is\", \n                  \"we'll\" : \"we will\", \"we'd\" : \"we would\", \"he'll\" : \"he will\", \n                  \"gov't\" : \"government\", \"shouldn't\" : \"should not\", \"bioterror\" : \"biological terror\", \n                  \"bioterrorism\" : \"biological terrorism\", \"wouldn't\" : \"would not\", \n                  \"won't\" : \"will not\" }\n\n\ndef expand_contractions(words):\n    \n    for i in range(len(words)):\n        if words[i] in contractions:\n            words[i] = contractions[words[i]]\n            \n    return (' '.join(words)).split()\n\n\n# precautionary cleaning for any remaing apostrophes\ndef remove_apostrophes(words):\n    words = ' '.join(words)\n    words = re.sub(\"'\", '', words)\n    return words.split()\n\n\ndf['text'] = df['text'].apply(lambda words: expand_contractions(words))\n\ndf['text'] = df['text'].apply(lambda words: remove_apostrophes(words))","628798b4":"unknown_words = []\ntotal_words = 0\n\ndf['text'].apply(lambda words: find_unknown_words(words))\n\nprint( f'{len(unknown_words)\/total_words*100:5.2} % of words are unknown' )","3f37e9e6":"words_freq = {}\n\ndef word_frequency(words):\n  for word in words:\n    if word in words_freq:\n      words_freq[word] += 1\n    else:\n      words_freq[word] = 1\n\ndf['text'].apply(lambda words: word_frequency(words))\n\nword = []\ncount = []\nfor w in words_freq:\n  word.append(w)\n  count.append( words_freq[w] )\n\nword = np.array(word)\ncount = np.array(count)\n\nword_freq = pd.DataFrame({'word': word, 'count': count}).sort_values('count', ascending=False)\n\nfig = go.Figure(data=[go.Table(\n      header=dict(values=list(word_freq.columns),\n                fill_color='paleturquoise',\n                align='left'),\n      cells=dict(values=[word_freq['word'], word_freq['count']],\n                fill_color='lavender',\n                align='left'))\n      ])\nfig.update_layout(width=300, height=300, margin={'b':0, 'l':0, 'r':0, 't':0, 'pad':0})\nfig.show()","773228f3":"stop_words = [ 'the', 'a', 'in', 'to', 'of', 'i', 'and', 'is', 'you', 'for', 'on', 'it', 'my', 'that',\n               'with', 'are', 'at', 'by', 'this', 'have', 'from', 'be', 'was', 'do', 'will', 'as', 'up', \n               'me', 'am', 'so', 'we', 'your', 'has', 'when', 'an', 's', 'they', 'about', 'been', 'there',\n               'who', 'would', 'into', 'his', 'them', 'did', 'w', 'their', 'm', 'its', 'does', 'where', 'th',\n               'b', 'd', 'x', 'p', 'o', 'r', 'c', 'n', 'e', 'g', 'v', 'k', 'l', 'f', 'j', 'z', 'us', 'our',\n               'all', 'can', 'may' ] \n\ndef remove_stop_words(words):\n  result = []\n  for word in words:\n    if not (word in stop_words):\n      result.append(word)\n  return result\n\ndf['text'] = df['text'].apply(lambda words: remove_stop_words(words))","3ea50888":"df.sample(5)","0eb0850c":"def text_embed(words):\n    \n    unknown_indices = []\n    mean = np.zeros(text_embedding_dimension)\n    \n    for i in range(len(words)):\n        if words[i] in embeddings_index_200:\n            words[i] = embeddings_index_200[ words[i] ]\n            mean += words[i]\n        else:\n            unknown_indices.append(i)\n            \n    mean \/= len(words)-len(unknown_indices)\n    \n    # unknown words in the text are represented using the mean of the known words\n    for i in unknown_indices:\n        words[i] = mean\n    \n    return np.array(words)\n\ndf['text'] = df['text'].apply(lambda words: text_embed(words))","731c1803":"def keyword_embed(keyword, text):\n    \n    if pd.isna(keyword):\n        keyword = np.zeros(25)\n    else:\n        keyword = keyword.lower()\n        keyword = re.sub(\"[^a-z ]+\", ' ', keyword)\n        keywords = keyword.split()\n\n        if len(keywords) == 0:\n            keyword = np.zeros(key_embedding_dimension)\n        else:\n            keyword = np.zeros(key_embedding_dimension)\n            word_count = 0\n            for word in keywords:\n                if word in embeddings_index_25:\n                    keyword += embeddings_index_25[word]\n                    word_count += 1\n\n            if word_count > 0:\n                keyword = keyword \/ word_count\n \n    return keyword\n\ndf['keyword'] = df.apply(lambda x: keyword_embed(x['keyword'], x['text']), axis=1)","aa63137c":"df.drop('location', axis=1).sample(5)","607cf5be":"# cross_validation_ratio = 0.2\ncross_validation_ratio = 0.05\n\nmask = np.random.rand(len(df)) > cross_validation_ratio\n\ntrain_df = df[mask]\n\nval_df = df[~mask]","e88608a3":"x_train_text = train_df['text'].values\nx_train_key = train_df['keyword'].values\n\nx_val_text = val_df['text'].values\nx_val_key = val_df['keyword'].values\n\ny_train = train_df['target'].values\ny_val = val_df['target'].values","b6ebfe81":"x_train_key = np.array( [i for i in x_train_key] ).reshape(-1, key_embedding_dimension)\nx_val_key = np.array( [i for i in x_val_key] ).reshape(-1, key_embedding_dimension)","1953a16e":"class ANN_Model(nn.Module):\n    def __init__(self):\n        super().__init__()                          \n        self.fc1 = nn.Linear(key_embedding_dimension, 10)\n        self.fc2 = nn.Linear(10, 1)\n        self.bn1 = nn.BatchNorm1d(10)\n        self.dropout1 = nn.Dropout(p=0.1)\n\n    def forward(self, X):\n        X = self.fc1(X)\n        X = self.bn1(X)\n        X = F.relu(X)\n        X = self.dropout1(X)\n        X = self.fc2(X)\n        X = torch.sigmoid(X)\n        return X","086e5b07":"ann_model = ANN_Model()\n\nif cuda0 != None:\n  ann_model.to(cuda0)\n\ncriterion_key = nn.BCELoss()\noptimizer_key = torch.optim.Adam(ann_model.parameters(), lr=0.01)\n# scheduler_key = torch.optim.lr_scheduler.ExponentialLR(optimizer_key, gamma=0.8)","8a8bb1f9":"ann_model","7a13925e":"start_time = time.time()\n\ntrain_losses = []\nval_losses = []\ntrain_accuracies = []\nval_accuracies = []\n\nfor epoch in range(300):  \n    \n    ann_model.train()\n\n    tweet = torch.FloatTensor(x_train_key)\n    label = torch.FloatTensor(y_train)\n\n    if cuda0 != None:\n        tweet = tweet.cuda()\n        label = label.cuda()\n\n    pred = ann_model(tweet)\n    pred = pred.reshape(-1)\n\n    loss = criterion_key(pred, label)\n\n    optimizer_key.zero_grad()\n    loss.backward()\n    optimizer_key.step()\n\n    train_losses.append(loss.item())\n    train_accuracies.append( ( (pred>0.5) == (label==1) ).sum().item() \/ len(x_train_key) )\n\n\n    ann_model.eval()\n\n    with torch.no_grad():\n\n        tweet = torch.FloatTensor(x_val_key)\n        label = torch.FloatTensor(y_val)\n\n        if cuda0 != None:\n            tweet = tweet.cuda()\n            label = label.cuda()\n\n        pred = ann_model(tweet)\n        pred = pred.reshape(-1)\n\n        loss = criterion_key(pred, label)\n\n    val_losses.append(loss.item())\n    val_accuracies.append( ( (pred>0.5) == (label==1) ).sum().item() \/ len(x_val_key) )\n    \n    if (epoch+1)%50 == 0:\n        print('Epoch {} Summary:'.format(epoch+1))\n        print(f'Train Loss: {train_losses[-1]:7.2f}  Train Accuracy: {train_accuracies[-1]*100:6.3f}%')\n        print(f'Validation Loss: {val_losses[-1]:7.2f}  Validation Accuracy: {val_accuracies[-1]*100:6.3f}%')\n        print('')\n\n    # scheduler_key.step()\n\nprint(f'\\nDuration: {time.time() - start_time:.0f} seconds')","b6a86fa8":"x_axis = [i+1 for i in range(len(train_losses))]\n\nplt.plot(x_axis, train_losses, label='training loss')\nplt.plot(x_axis, val_losses, label='validation loss')\nplt.title('Loss for each epoch')\nplt.legend();\nplt.show()\n\nplt.plot(x_axis, train_accuracies, label='training accuracy')\nplt.plot(x_axis, val_accuracies, label='validation accuracy')\nplt.title('Accuracy for each epoch')\nplt.legend();\nplt.show()","d6d654c5":"ann_model.eval()\n\n# predictions for the training set\nwith torch.no_grad():\n\n    tweet = torch.FloatTensor(x_train_key)\n\n    if cuda0 != None:\n        tweet = tweet.cuda()\n\n    pred_train_key = ann_model(tweet)\n    pred_train_key = pred_train_key.reshape(-1)\n    \n\n# predictions for the cross validation set\nwith torch.no_grad():\n\n    tweet = torch.FloatTensor(x_val_key)\n\n    if cuda0 != None:\n        tweet = tweet.cuda()\n\n    pred_val_key = ann_model(tweet)\n    pred_val_key = pred_val_key.reshape(-1)","8d71362a":"class LSTMnetwork(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.hidden_size = 50\n        self.input_size = text_embedding_dimension\n        self.num_layers = 1\n        self.bidirectional = True\n        self.num_directions = 1\n        self.dropout1 = nn.Dropout(p=0.3)\n\n        if self.bidirectional:\n            self.num_directions = 2\n \n        self.lstm = nn.LSTM( self.input_size, self.hidden_size, self.num_layers, \n                             bidirectional=self.bidirectional )\n        \n        self.linear = nn.Linear(self.hidden_size*self.num_directions,1)\n\n    def forward(self, tweet):\n        \n        lstm_out, _ = self.lstm( tweet.view(len(tweet), 1, -1) )\n\n        x = self.dropout1( lstm_out.view(len(tweet),-1) )\n        \n        output = self.linear(x)\n        \n        pred = torch.sigmoid( output[-1] )\n        \n        return pred","3a0e9afd":"lstm_model = LSTMnetwork()\n\nif cuda0 != None:\n  lstm_model.to(cuda0)\n\ncriterion_text = nn.BCELoss()\noptimizer_text = torch.optim.Adam(lstm_model.parameters(), lr=0.001)\nscheduler_text = torch.optim.lr_scheduler.ExponentialLR(optimizer_text, gamma=0.1)","c75174e9":"lstm_model","4ac107e5":"ann_model_weight = 0.3\nlstm_model_weight = 1-ann_model_weight","001abfae":"start_time = time.time()\n\ntrain_losses = []\nval_losses = []\ntrain_accuracies = []\nval_accuracies = []\n\nfor epoch in range(4):  \n\n    epoch_start_time = time.time()\n\n    print('Epoch : {}'.format(epoch+1))\n\n    trainLoss = 0\n    correct = 0\n\n    lstm_model.train()\n\n    for i in range(len(x_train_text)):\n\n        lstm_model.zero_grad()\n\n        tweet = torch.FloatTensor(x_train_text[i])\n        label = torch.FloatTensor( np.array([y_train[i]]) )\n\n        if cuda0 != None:\n            tweet = tweet.cuda()\n            label = label.cuda()\n\n        pred = lstm_model(tweet)\n\n        loss = criterion_text(pred, label)\n\n        lambdaParam = torch.tensor(0.001)\n        l2_reg = torch.tensor(0.)\n\n        if cuda0 != None:\n          lambdaParam = lambdaParam.cuda()\n          l2_reg = l2_reg.cuda() \n\n        for param in lstm_model.parameters():\n          if cuda0 != None:\n            l2_reg += torch.norm(param).cuda()\n          else:\n            l2_reg += torch.norm(param)\n\n        loss += lambdaParam * l2_reg\n\n        pred = pred.item()*lstm_model_weight + pred_train_key[i].item()*ann_model_weight\n        \n        if pred > 0.5:\n            pred = 1\n        else:\n            pred = 0\n\n        if pred == int( label.item() ):\n            correct += 1\n\n        trainLoss += loss.item()\n\n        optimizer_text.zero_grad()\n        loss.backward()\n        optimizer_text.step()\n\n        if (i+1)%1000 == 0:\n            print('Processed {} tweets out of {}'.format(i+1, len(x_train_text)))\n\n    train_losses.append(trainLoss\/len(x_train_text))\n    train_accuracies.append( correct\/len(x_train_text) )\n\n    valLoss = 0\n    correct = 0\n\n    lstm_model.eval()\n\n    with torch.no_grad():\n\n        for i in range(len(x_val_text)):\n\n            tweet = torch.FloatTensor(x_val_text[i])\n            label = torch.FloatTensor( np.array([y_val[i]]) )\n\n            if cuda0 != None:\n                tweet = tweet.cuda()\n                label = label.cuda()\n\n            pred = lstm_model( tweet )\n\n            loss = criterion_text(pred, label)\n\n            valLoss += loss.item()\n\n            pred = pred.item()*lstm_model_weight + pred_val_key[i].item()*ann_model_weight\n\n            if pred > 0.5:\n                pred = 1\n            else:\n                pred = 0\n\n            if pred == int( label.item() ):\n                correct += 1\n\n    val_losses.append(valLoss\/len(x_val_text))\n    val_accuracies.append( correct\/len(x_val_text) )\n\n    print('Epoch Summary:')\n    print(f'Train Loss: {train_losses[-1]:7.2f}  Train Accuracy: {train_accuracies[-1]*100:6.3f}%')\n    print(f'Validation Loss: {val_losses[-1]:7.2f}  Validation Accuracy: {val_accuracies[-1]*100:6.3f}%')\n    print(f'Duration: {time.time() - epoch_start_time:.0f} seconds')\n    print('')\n\n    scheduler_text.step()\n\nprint(f'\\nDuration: {time.time() - start_time:.0f} seconds')","949eed6a":"x_axis = [i+1 for i in range(len(train_losses))]\n\nplt.plot(x_axis, train_losses, label='training loss')\nplt.plot(x_axis, val_losses, label='validation loss')\nplt.title('Loss for each epoch')\nplt.legend();\nplt.show()\n\nplt.plot(x_axis, train_accuracies, label='training accuracy')\nplt.plot(x_axis, val_accuracies, label='validation accuracy')\nplt.title('Accuracy for each epoch')\nplt.legend();\nplt.show()","eb0f0ed4":"test_df = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","e088b75b":"test_df['text'] = test_df['text'].apply(lambda x: clean_text(x))\n\ntest_df['text'] = test_df['text'].apply(lambda words: expand_contractions(words))\n\ntest_df['text'] = test_df['text'].apply(lambda words: remove_apostrophes(words))\n\ntest_df['text'] = test_df['text'].apply(lambda words: remove_stop_words(words))\n\ntest_df['text'] = test_df['text'].apply(lambda words: text_embed(words))\n\ntest_df['keyword'] = test_df.apply(lambda x: keyword_embed(x['keyword'], x['text']), axis=1)\n\ntest_df.drop('location', axis=1).sample(5)","940579c0":"x_test_text = test_df['text'].values\nx_test_key = test_df['keyword'].values\n\nx_test_key = np.array( [i for i in x_test_key] ).reshape(-1, key_embedding_dimension)","6d8b9963":"test_predictions = []","5fadf8ee":"ann_model.eval()\n\nwith torch.no_grad():\n\n    tweet = torch.FloatTensor(x_test_key)\n\n    if cuda0 != None:\n        tweet = tweet.cuda()\n\n    pred_test_key = ann_model(tweet)\n    pred_test_key = pred_test_key.reshape(-1)","327c24d2":"lstm_model.eval()\n\nwith torch.no_grad():\n\n    for i in range(len(x_test_text)):\n\n        tweet = torch.FloatTensor(x_test_text[i])\n\n        if cuda0 != None:\n            tweet = tweet.cuda()\n\n        pred = lstm_model( tweet )\n\n        pred = pred.item()*lstm_model_weight + pred_test_key[i].item()*ann_model_weight\n\n        if pred > 0.5:\n            pred = 1\n        else:\n            pred = 0\n\n        test_predictions.append(pred)","93d2ab4f":"test_predictions = np.array(test_predictions)\n\nids = test_df['id'].values","e5df5cc3":"output = pd.DataFrame({'id': ids, 'target': test_predictions})\n\noutput.to_csv('\/kaggle\/working\/my_submission.csv', index=False)","d1d825ea":"## Finding the most frequent stop words and removing them","814bc461":"# Exploratory Data Analysis and Further Text Cleaning","bb5fbd24":"# Saving the Results","a1b077a5":"# Reading and Preprocessing Test Data","183f6184":"## Looks like a lot of the unknown words are contractions. Let's expand the most common ones.","bc9d40cb":"# Downloading and Processing GloVe Files","22c765af":"# Generating Predictions for the Test Data","ad182510":"# LSTM Model for Prediction using tweet Text","44a57d5f":"## Generating Final Predictions of the ANN Model","326a68d9":"## Finding the Words which are Unknown to GloVe","875f137e":"# Data Input","0698d245":"# Train and Cross Validation Split\n* The greater cross validation ratio of 0.2 was used during hyperparamter tuning","da57fa9d":"# ANN Model For Prediction using Keywords","7992b809":"# Embedding the Text and Keyword","79bff8f7":"# Primary Text Cleaning : remove links and split text into individual words","1c80d717":"# Introduction\n* A LSTM based model is used to make a prediction using the 'text' of the tweet\n* An ANN based model is used to make a prediction using the 'keyword' of the tweet\n* The two models are emsembled to the make the final prediction\n* 200 dimensional GloVe word vectors are used for embedding the 'text' ;  25 dimensional GloVe word vectors are used for embedding the 'keywords'\n* GloVe reference : https:\/\/nlp.stanford.edu\/projects\/glove\/"}}