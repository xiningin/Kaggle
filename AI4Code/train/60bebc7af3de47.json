{"cell_type":{"7812fbfd":"code","8665ec27":"code","19f6ddff":"code","a43d5862":"code","f9ee481e":"code","cad7fa09":"code","d7c4b5a7":"code","69432f54":"code","a8375cae":"code","5f8f3936":"code","e0326b0f":"code","7b79f417":"code","c931685b":"code","d946852a":"code","ccc168f2":"code","fa7223ff":"code","460bc2a7":"code","03f43752":"code","d6a76442":"code","8eb00d5d":"code","2ab97fe3":"code","66f2c9ec":"code","275629cf":"code","3b2b06f5":"code","db00e951":"code","64a9ffb6":"code","fb002133":"code","693052be":"code","edb406f7":"code","194e43f6":"markdown","3e64e9a2":"markdown","829cfba0":"markdown","e96876c8":"markdown","784e453b":"markdown","829bfe2f":"markdown","be4539a5":"markdown","a82bd456":"markdown","ee52137d":"markdown","a4e6a85b":"markdown","97c959c6":"markdown","c3c4113f":"markdown","532632ac":"markdown","3fdd0a6c":"markdown","6005a5ba":"markdown","6a34081b":"markdown","d3b933a7":"markdown","255e6100":"markdown","95aa176f":"markdown","169faf0c":"markdown","d139a029":"markdown","d0d452e6":"markdown","7eb4c38e":"markdown","f07d25e4":"markdown","d397388d":"markdown","80e4c937":"markdown","8cd554b6":"markdown","88477031":"markdown","ef3f38d8":"markdown","dea7ee01":"markdown","e38f6df3":"markdown","0bf618c9":"markdown","46955a17":"markdown"},"source":{"7812fbfd":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfrom collections import Counter\nimport seaborn as sns\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8665ec27":"df = pd.read_csv(\"\/kaggle\/input\/glass\/glass.csv\")","19f6ddff":"df.info()","a43d5862":"df.head()","f9ee481e":"df.columns","cad7fa09":"X = df.iloc[:,:-1]\nY = df.iloc[:,9]","d7c4b5a7":"Counter(Y)","69432f54":"X_n = (X-np.min(X))\/(np.max(X)-np.min(X))","a8375cae":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(X_n,Y,test_size=0.2,random_state=42)","5f8f3936":"score_list = []\nmodel = []\ncross_val_score_list=[]","e0326b0f":"tuning= [100,200,300,400,500,600,700,800]","7b79f417":"from sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestClassifier \nscore=[]\na=0\nc=0\nfor i in tuning:\n    rf = RandomForestClassifier(n_estimators=i, random_state=42)\n\n    rf.fit(x_train,y_train)\n    accuracies = cross_val_score(estimator=rf,X = x_train,y = y_train,cv=3)\n#%%\n    score.append(np.mean(accuracies))\n    if np.mean(accuracies)>a:\n        a=np.mean(accuracies)\n        c=i\nprint(\"acc = \",a,\" best number of estimator = \",c)\nprint(\"std= \",np.std(accuracies))","c931685b":"plt.plot(tuning,score)","d946852a":"rf = RandomForestClassifier(n_estimators=100, random_state=42) #variable = our model\n\nrf.fit(x_train,y_train) #fit our model with our train datas\nprint(rf.score(x_test,y_test)) #check score with our test datas\nmodel.append(\"Random Forest Classifier\") #store our models name for our graph\nscore_list.append(rf.score(x_test,y_test)) #store our test score for our graph","ccc168f2":"from sklearn.metrics import confusion_matrix\ny_pred= rf.predict(x_test) #we use this code to predict our labels with our model\n#confusion matrix part\ncategories = [1,2,3,5,6,7] \ncm = confusion_matrix(y_test,y_pred)\nf , ax = plt.subplots(figsize=(5,5))\n\nsns.heatmap(cm,annot = True,linewidths =0.5,linecolor =\"Red\",fmt=\".0f\",ax=ax)\nax.set_xticklabels(categories)\nax.set_yticklabels(categories)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\n\n\n\nplt.show()","fa7223ff":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier()\naccuracies = cross_val_score(estimator=rf,X = x_train,y = y_train,cv=3)\n#%%\nprint(\"acc = \", np.mean(accuracies))\nprint(\"std= \",np.std(accuracies))","460bc2a7":"dt.fit(x_train,y_train)\nprint(\"acc = \", dt.score(x_test,y_test))\nmodel.append(\"Decision Tree Classifier\")\nscore_list.append(dt.score(x_test,y_test))","03f43752":"y_pred= dt.predict(x_test)\n\ncategories = [1,2,3,5,6,7]\ncm = confusion_matrix(y_test,y_pred)\nf , ax = plt.subplots(figsize=(5,5))\n\nsns.heatmap(cm,annot = True,linewidths =0.5,linecolor =\"Red\",fmt=\".0f\",ax=ax)\nax.set_xticklabels(categories)\nax.set_yticklabels(categories)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\n\nplt.show()","d6a76442":"from sklearn.neighbors import KNeighborsClassifier\nscore =[]\na=0\nc=0\nfor i in range(2,20):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    accuracies = cross_val_score(estimator=knn,X = x_train,y = y_train,cv=3)\n    score.append(np.mean(accuracies))\n    if np.mean(accuracies)>a:\n        a=np.mean(accuracies)\n        c=i\n    \nprint(\"acc = \",a,\" best number of neighbors = \",c)\nprint(\"std= \",np.std(accuracies))","8eb00d5d":"plt.plot(range(2,20),score)","2ab97fe3":"knn = KNeighborsClassifier(n_neighbors = 3)\nknn.fit(x_train,y_train)\nprint(\"acc = \", knn.score(x_test,y_test))\nmodel.append(\"K-Nearest Neighbors\")\nscore_list.append(knn.score(x_test,y_test))","66f2c9ec":"y_pred= knn.predict(x_test)\n\ncategories = [1,2,3,5,6,7]\ncm = confusion_matrix(y_test,y_pred)\nf , ax = plt.subplots(figsize=(5,5))\n\nsns.heatmap(cm,annot = True,linewidths =0.5,linecolor =\"Red\",fmt=\".0f\",ax=ax)\nax.set_xticklabels(categories)\nax.set_yticklabels(categories)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\n\nplt.show()","275629cf":"from sklearn.svm import SVC\n\nsvm = SVC(random_state=1)\naccuracies = cross_val_score(estimator=knn,X = x_train,y = y_train,cv=3)\nprint(\"acc = \" ,np.mean(accuracies))\nprint(\"std= \",np.std(accuracies))","3b2b06f5":"\nsvm.fit(x_train,y_train)\nprint(\"acc = \" ,svm.score(x_test,y_test))\nmodel.append(\"Support Vector Machines\")\nscore_list.append(svm.score(x_test,y_test))","db00e951":"y_pred= svm.predict(x_test)\n\ncategories = [1,2,3,5,6,7]\ncm = confusion_matrix(y_test,y_pred)\nf , ax = plt.subplots(figsize=(5,5))\n\nsns.heatmap(cm,annot = True,linewidths =0.5,linecolor =\"Red\",fmt=\".0f\",ax=ax)\nax.set_xticklabels(categories)\nax.set_yticklabels(categories)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\n\nplt.show()","64a9ffb6":"from sklearn.naive_bayes import GaussianNB\n\nnb=GaussianNB()\n\naccuracies = cross_val_score(estimator=nb,X = x_train,y = y_train,cv=3)\nprint(\"acc = \", np.mean(accuracies))\nprint(\"std= \",np.std(accuracies))","fb002133":"nb.fit(x_train,y_train)\nprint(\"acc = \" ,nb.score(x_test,y_test))\nmodel.append(\"Naive Bayes\")\nscore_list.append(nb.score(x_test,y_test))","693052be":"y_pred= nb.predict(x_test)\n\ncategories = [1,2,3,5,6,7]\ncm = confusion_matrix(y_test,y_pred)\nf , ax = plt.subplots(figsize=(5,5))\n\nsns.heatmap(cm,annot = True,linewidths =0.5,linecolor =\"Red\",fmt=\".0f\",ax=ax)\nax.set_xticklabels(categories)\nax.set_yticklabels(categories)\nplt.xlabel(\"y_pred\")\nplt.ylabel(\"y_true\")\nlabels = [\"True Neg\",\"False Pos\",\"False Neg\",\"True Pos\"]\n\n\nplt.show()","edb406f7":"cv_result = pd.DataFrame({\"Scores\":score_list, \"ML Models\":model})\ng = sns.barplot(\"Scores\", \"ML Models\",data=cv_result)\ng.set_xlabel(\"Test Name\")\ng.set_title(\"Test Scores\")","194e43f6":"* We will use this parameters for our graphs.","3e64e9a2":"<a id = \"7\" >\n    \n## Support Vector Machines","829cfba0":"<a id=\"9\" >\n# Compare Model's Accuracy with Graph","e96876c8":"<a id = \"3\" >\n    \n# Supervised Models","784e453b":"* To find the best n_estimators, we use this code. (We can use grid searh cv, but in this notebook we didn't use it) It means how many estimators will vote to class our data.","829bfe2f":"* It is a classification technique based on Bayes\u2019 Theorem with an assumption of independence among predictors. In simple terms, a Naive Bayes classifier assumes that the presence of a particular feature in a class is unrelated to the presence of any other feature.\n* Navie bayes formula is :\n![navie](https:\/\/i.ibb.co\/N25cdqS\/Siniflandirma-Notlari-10-Bayes-Teoremi-Formul-e1504212839936.png)","be4539a5":"* For this data random forest classifier is the best choice for us. Its accuracy is higher than others.\n* We saw all models' basic, what they are and how we can write them in python.\n* To increase our model's accuracy, we can use other hyperparameters with grid search cv and we can make some changes for our data to increase our accuracy. We have to try them to see they work or not.\n* I hope you will like it, if you like it don't forget to upvote.\n\nThank you","a82bd456":"* We can see our first 5 row of our dataframe","ee52137d":"<a id = \"4\" >\n\n## Random Forest","a4e6a85b":"<a id = \"5\" >\n\n## Decision Tree","97c959c6":"![support](https:\/\/i.ibb.co\/2SpcZ3M\/SVM.jpg)\n\n* It uses vector to separate our data and says that left of this vector will be class a, right of this vector will be class. By looking p, svm optimizes for best hyper-plane","c3c4113f":"<a id = \"6\" >\n    \n## K-Nearest Neighbors","532632ac":"![knn](https:\/\/i.ibb.co\/K5Z6345\/knn.png)\n* K-Nearest Neighbors determines nearest neighbors. It looks nearest neighbors class and specify your data's class. \n* For this model, you have to determine number of neighbors. For this picture, in small circle n_neighbors is 3, for big circle n_neighbors is 6","3fdd0a6c":"* In this part, we determine our features and labels. Our labels are our type of glasses. It has 6 type(1,2,3,5,6,7). It calls it in our code as a Y and\nX is our features","6005a5ba":"* First, libraries are imported.","6a34081b":"<a id=\"2\" >\n    \n# Load and Preprocessing","d3b933a7":"* Normalize Data\n\n* In this part we normalize data. It means all of features have value between 0 and 1. Thanks to normalization, all of data's values change to common scale, without distorting differences in the ranges of values. ","255e6100":"* We can see that how many types we have","95aa176f":"* To find the best n_neighbors, we use this code. (We can use grid searh cv, but in this notebook we did not use it)","169faf0c":"* In this part we split data as train and test. This code means our %80 of data will be used to train our model. %20 of data will be used to test our model. Random state means it takes same values for every run","d139a029":"* For all of models first we introduce one variable which is equal to our model(their parameters can change with model). After that we fit it with train datas. After that we check test results with the model which is fitted by train datas.","d0d452e6":"* For all model, first we import model's library. After that we can use it.\n* For all model, we find train's accuracy with k-fold cross validation. We split train data and use them for train and validation. \n![](https:\/\/i.ibb.co\/ZcdQJLq\/8uEci.png)\n\n* For this picture cv=5. It means it uses %80 of data for train and %20 data to determine accuracy of train data. Their mean will be accuracy of train data. If we do not use it, our train's accuracy may misguide us. For an example, accuracy is %86 without k-fold cross validation. Hovewer,accuracies are %90,%99,%65,%91,%85 with k-fold cross validation. We can say that for this model there can be some problems because it is unstable.\n\n* For this models cv will be 3 and we check std to understand model stable or not.","7eb4c38e":"<a id=\"10\" >\n       \n# Conclusion","f07d25e4":"<a id=\"1\" >\n    \n# Introduction","d397388d":"* Supervised learning is used for datas which have labels(class). We can find the class of data with supervised learning.\n \n![image.png](https:\/\/i.ibb.co\/4pKMSNq\/Machine-Learning-Classification-Algorithms-1280x720.jpg)\n\n* In this picture, all of foods have some features(their size,weight, etc.) and every foods have labels(ice cream, cake, etc.). Thanks to this features, their labels can be found with our model.\n\n* In supervised learning, we use our provided labels and features as a teacher of model. For this example, we will split data as train and test and train data is utilised to train the model. We can train our model with this data because we know its label. After that test data is used to determine our accuracy. \n\n* In this notebook, we will use Naive Bayes, Decision Tree, Support Vector Machines, Random Forest and K-Nearest Neighbours. All of them will be explained before its code.","80e4c937":"<a id=\"8\" >\n\n## Navie Bayes","8cd554b6":"* We use confusion matrix to compare our predicted and real value. For an example we have dog and cat labels. We have 30 dog and 20 cat. With confusion matrix, we can see that how many dogs we predicted right.\n* For an example, our data have 10 dogs and 90 cats. We saw that our accuracy is %90 and we can say that it is good. However, we use confusion matrix and we saw that our model predicted dogs for all features. It means model is not good because we have two classes and our predicted data don't have one of them.","88477031":"* It is like random forest, but it uses only one tree. Random forest is essentially collection of decision tree","ef3f38d8":"![randomforest.png](https:\/\/i.ibb.co\/THj4n51\/Random-Forest-Algorithm.jpg)\n\n* Random forest is using this type of algorithm. It uses yes\/no. It asks a yes-no question to data for a specific value. If it is correct, it selects yes road, if it is not correct, it selects no road. \n* For an example, our data is [1,5,6,8,10,7]. For 6, it asks your value >2? and it is correct. It is going yes road and random forest asks your value<5? and it is wrong and it is going to false road. With these questions, our figure looks like tree and thanks to it, we can class our data\n\n* Random forest is using a lot of tree and it classifies our data with voting system. Each tree vote for one class and the winner will be our class.","dea7ee01":"## Check data\n* We see that we don't have any null data. Our number of data is 214 and their type is float64. Our type(it will be our label) is int64","e38f6df3":"1. [Introduction](#1)\n2. [Load and Preprocessing](#2)\n3. [Supervised Models](#3)\n      * [Random Forest](#4)\n      * [Decision Tree](#5)\n      * [K-Nearest Neighbours](#6)\n      * [Support Vector Machines](#7)\n      * [Naive Bayes](#8)\n4. [Compare Model's Accuracy with Graph](#9)\n5. [Conclusion](#10)","0bf618c9":"* Our columns' name","46955a17":"## Load Data"}}