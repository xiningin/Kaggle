{"cell_type":{"b59e444c":"code","7b2a326e":"code","d6b10ad3":"code","8049e9d8":"code","59d4e17a":"code","c07cbb4c":"code","6e7dde75":"markdown","8b9399c3":"markdown","076f28f2":"markdown","0ed4b133":"markdown","5018938a":"markdown","a67a211c":"markdown","f4de488d":"markdown","de68eb25":"markdown"},"source":{"b59e444c":"import pandas as pd\nimport numpy as np\nimport random\n\ntrain = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jun-2021\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jun-2021\/test.csv')\nsample_submission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jun-2021\/sample_submission.csv')\n\nSEED=2147483647\nrandom.seed(SEED)\nnp.random.seed(SEED)","7b2a326e":"target_dict = {\n    'Class_1' : 0,\n    'Class_2' : 1,\n    'Class_3' : 2,\n    'Class_4' : 3,\n    'Class_5' : 4,\n    'Class_6' : 5,\n    'Class_7' : 6,\n    'Class_8' : 7,\n    'Class_9' : 8,\n}\ntrain['target'] = train['target'].map(target_dict)","d6b10ad3":"from sklearn.base import TransformerMixin\n\nfeatures=['feature_{}'.format(x) for x in range(75)]\n\nclass MyTransformer1(TransformerMixin):\n    def fit_transform(self, X, y=None,**fit_params):\n        return self.transform(X)\n\n    def transform(self, X):\n        X = X.copy()\n        X = X[features]\n        return X\n\nclass MyTransformer2(TransformerMixin):\n    def clip(self,X):\n        for feature in features:\n            X[feature] = X[feature].clip(upper=4)\n        return X\n\n    def setup_new_feature(self,X):\n        for feature_value in [0,1,2,3]:\n            new_feature = 'count_{}'.format(feature_value)\n            X[new_feature] = X[features].apply(lambda x:(x.values==feature_value).sum(),axis=1)\n        return X\n\n    def fit_transform(self, X, y=None,**fit_params):\n        return self.transform(X)\n\n    def transform(self, X):\n        X = X.copy()\n        X = X[features]\n        X = self.setup_new_feature(self.clip(X))\n        return X\n\nclass MyTransformer3(TransformerMixin):\n    def clip(self,X):\n        for feature in features:\n            X[feature] = X[feature].clip(upper=4)\n        return X\n\n    def fit_transform(self, X, y=None,**fit_params):\n        return self.transform(X)\n\n    def transform(self, X):\n        X = X.copy()\n        X = X[features]\n        X = pd.get_dummies(self.clip(X),columns=features)\n        return X\n\nclass MyTransformer4(TransformerMixin):\n    def clip(self,X):\n        for feature in features:\n            X[feature] = X[feature].clip(upper=4)\n        return X\n\n    def setup_new_feature(self,X):\n        for feature_value in [0,1,2,3]:\n            new_feature = 'count_{}'.format(feature_value)\n            X[new_feature] = X[features].apply(lambda x:(x.values==feature_value).sum(),axis=1)\n        return X\n\n    def fit_transform(self, X, y=None,**fit_params):\n        return self.transform(X)\n\n    def transform(self, X):\n        X = X.copy()\n        X = X[features]\n        X = self.setup_new_feature(self.clip(X))\n        return pd.get_dummies(X, columns=features)\n\n    ","8049e9d8":"from sklearn.base import ClassifierMixin\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\n\noptuna_recommended_params1 = {\n    'objective': 'multiclass',\n    'num_classes': 9,\n    'metric': 'multi_logloss',\n    'verbosity': 0,\n    'boosting_type': 'gbdt',\n    'feature_pre_filter': False,\n    'lambda_l1': 8.934626150848016,\n    'lambda_l2': 1.3751929899381281e-08,\n    'num_leaves': 8,\n    'feature_fraction': 0.4,\n    'bagging_fraction': 0.8781081160423493,\n    'bagging_freq': 4,\n    'min_child_samples': 50,\n}\n\noptuna_recommended_best_iteration1=123\n\noptuna_recommended_params2 = {\n    'objective': 'multiclass',\n    'num_classes': 9,\n    'metric': 'multi_logloss',\n    'verbosity': 0,\n    'boosting_type': 'gbdt',\n    'feature_pre_filter': False,\n    'lambda_l1': 8.934626150848016,\n    'lambda_l2': 6.580392901707003e-06,\n    'num_leaves': 4,\n    'feature_fraction': 0.4,\n    'bagging_fraction': 1.0,\n    'bagging_freq': 0,\n    'min_child_samples': 20,\n}\n\noptuna_recommended_best_iteration2=276\n\nmy_model1 = LGBMClassifier(\n    n_estimators=optuna_recommended_best_iteration1,\n    random_state=SEED,\n    **optuna_recommended_params1\n)\n\nmy_model2 = LGBMClassifier(\n    n_estimators=optuna_recommended_best_iteration2,\n    random_state=SEED,\n    **optuna_recommended_params2\n)\n\nmy_model3 = LogisticRegression(random_state=SEED,max_iter=2000)\n\ngrid_search_recommended_params4 = {\n    'min_data_in_leaf': 50,\n    'depth': 4,\n    'iterations': 300,\n    'learning_rate': 0.1\n}\n\nmy_model4 = CatBoostClassifier(loss_function='MultiClass', random_state=SEED,**grid_search_recommended_params4,verbose=0)\n\npipeline1 = make_pipeline(MyTransformer1(),my_model1)\npipeline2 = make_pipeline(MyTransformer2(),my_model2)\npipeline3 = make_pipeline(MyTransformer3(),my_model3)\npipeline4 = make_pipeline(MyTransformer4(),my_model4)\n","59d4e17a":"from sklearn.ensemble import VotingClassifier,StackingClassifier\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import log_loss,accuracy_score\n\nvoting_estimators = [\n    ('mod1', pipeline1),\n    ('mod2', pipeline2),\n    ('mod3', pipeline3),\n    ('mod4', pipeline4),\n]\n\nX = train[features]\ny = train['target']\n\nmod_vot = VotingClassifier(\n    estimators=voting_estimators,\n    voting = 'soft',\n).fit(X, y)\n\n","c07cbb4c":"y_pred_test  = mod_vot.predict_proba(test[features]) \n\nsubmission = test[['id']].copy()\nsubmission['Class_1'] = y_pred_test[:,0]\nsubmission['Class_2'] = y_pred_test[:,1]\nsubmission['Class_3'] = y_pred_test[:,2]\nsubmission['Class_4'] = y_pred_test[:,3]\nsubmission['Class_5'] = y_pred_test[:,4]\nsubmission['Class_6'] = y_pred_test[:,5]\nsubmission['Class_7'] = y_pred_test[:,6]\nsubmission['Class_8'] = y_pred_test[:,7]\nsubmission['Class_9'] = y_pred_test[:,8]\n\nsubmission.to_csv('submission.csv', index=False)","6e7dde75":"## 4. define my classifier.","8b9399c3":"## 6. create my submission file.","076f28f2":"# VotingClassifier ensemble (lightgbm, catboost, LogisticRegression)","0ed4b133":"### 2.1. convert 'target' value to 0,1,2,3,...,9. ","5018938a":"## 3. define my transformer.","a67a211c":"## 5. train the VotingClassifier.","f4de488d":"## 1. read datasets and initialize random seed.","de68eb25":"## 2. preprocessing datasets."}}