{"cell_type":{"fef163d5":"code","7c8219ad":"code","29410178":"code","b703ffe0":"code","b1e56f51":"code","8833af36":"code","40a7f8f9":"code","d19c1a1b":"code","dd7ba68f":"code","c7d0c212":"code","f93ebefa":"code","3460a5b6":"code","0af4f722":"code","676e14f9":"code","aff70531":"code","9da63381":"code","3575afc2":"code","874fb397":"code","8e7585a4":"code","94b0a267":"code","3cfad69a":"code","72bb5adb":"code","8d5e4e14":"code","91e030f9":"code","e1c36072":"code","ca312239":"code","d8f1fa9f":"code","8bc0be0a":"code","6724538d":"code","bf7b4fb9":"code","c4d0c31b":"code","5ec2df5d":"code","99cef7c6":"code","3ee709ad":"code","fa4502f6":"code","5f7ea436":"code","e0437bd4":"code","181279ed":"code","1ba17f51":"code","6b0237ec":"code","cd7a1d80":"code","20f6509b":"code","f50da505":"code","4d5e9b52":"code","df73e7a5":"code","b21bd4cb":"code","0d9c155a":"code","ef9aeede":"code","7b987a11":"code","e16ba10e":"code","25b2120c":"code","14fad091":"code","3b795ecc":"code","ca935de5":"markdown","37a37621":"markdown","de24a309":"markdown","1d3c4429":"markdown","9aed7a8b":"markdown","38951793":"markdown","90a43754":"markdown","58042d4b":"markdown","45d8d81c":"markdown","a008551e":"markdown","4725161d":"markdown","cf3ea0ae":"markdown","4624be72":"markdown","e340343d":"markdown","eef54ea6":"markdown","365e45cc":"markdown","e6a0851b":"markdown","607af31c":"markdown","1413c534":"markdown","6b64f2d3":"markdown","137d29db":"markdown","782e7645":"markdown","0f0bac0a":"markdown","d0640ac7":"markdown","418d857d":"markdown","0c4b8037":"markdown","5f330378":"markdown","cffbd0c4":"markdown","8fdb7037":"markdown","de58481d":"markdown","49346a87":"markdown"},"source":{"fef163d5":"# Load in our libraries\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nimport copy\nimport seaborn as sns\nimport random \ncolor = sns.color_palette()\nsns.set_style('darkgrid')\n%matplotlib inline\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Going to use these 5 base models for the stacking\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import (RandomForestClassifier, AdaBoostClassifier, \n                              GradientBoostingClassifier, ExtraTreesClassifier)\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.metrics import average_precision_score\nimport xgboost as xgb","7c8219ad":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","29410178":"train.head(5)","b703ffe0":"test.head(5)","b1e56f51":"ntrain = len(train)\nntest = len(test)","8833af36":"# Obtain our labels\ntrain_label = train['Survived']\n# Drop the useless columns and concatenate the tables\nPassengerId = test['PassengerId']\ntrain.drop(columns = ['PassengerId'], axis = 1, inplace = True)\ntest.drop(columns = ['PassengerId'], axis = 1, inplace = True)\nall_data = pd.concat([train, test], axis = 0, ignore_index = True)","40a7f8f9":"all_data.drop(columns = ['Survived'], inplace = True)","d19c1a1b":"all_data.dtypes","dd7ba68f":"all_data.isnull().sum()","c7d0c212":"all_data.Embarked.fillna(value = all_data.Embarked.mode().values[0], inplace = True)","f93ebefa":"all_data.Fare.fillna(value = all_data.Fare.mean(), inplace = True)","3460a5b6":"age_avg = all_data['Age'].mean()\nage_std = all_data['Age'].std()\n\nage_null_count = all_data['Age'].isnull().sum()\nage_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size=age_null_count)\nall_data['Age'][np.isnan(all_data['Age'])] = age_null_random_list\n\nall_data['Age'] = all_data['Age'].astype(int)","0af4f722":"all_data['Cabin'] = all_data['Cabin'].astype(str, copy = True)","676e14f9":"all_data['Cabin'] = all_data['Cabin'].apply(lambda x: re.findall(r'([A-Z])\\d*',x)[0] if x != 'nan' else 0)","aff70531":"all_data['Cabin'] = all_data['Cabin'].apply(lambda x: 'Z' if x == 0 else x)","9da63381":"all_data.Age.hist(bins = 60)","3575afc2":"all_data.Fare.hist(bins = 50)","874fb397":"all_data.Fare.max()","8e7585a4":"all_data['Fare'] = all_data['Fare'].apply(lambda x: 300 if x >= 500 else x)","94b0a267":"all_data['CategoricalAge'] = pd.cut(all_data['Age'], 5)","3cfad69a":"# Remove all NULLS in the Fare column and create a new feature CategoricalFare\nall_data['CategoricalFare'] = pd.qcut(all_data['Fare'], 5)","72bb5adb":"lambda_1 = 0.70\nall_data['Age_adj'] = (np.power(all_data['Age'], lambda_1) - 1)\/lambda_1\n\n#Plot the histogram for the adjuested data\nsns.distplot(all_data['Age_adj'] , fit=norm);\n\n#Plot the qq plot\nfig = plt.figure()\nres = stats.probplot(np.array(all_data['Age_adj']), plot=plt)\nplt.show()","8d5e4e14":"all_data['Fare_adj'] = np.log(all_data['Fare'] + 1)\n\n#Plot the histogram for the adjuested data\nsns.distplot(all_data['Fare_adj'] , fit=norm);\n\n#Plot the qq plot\nfig = plt.figure()\nres = stats.probplot(np.array(all_data['Fare_adj']), plot=plt)\nplt.show()","91e030f9":"# Define function to extract titles from passenger names\ndef get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\n# Create a new feature Title, containing the titles of passenger names\nall_data['Title'] = all_data['Name'].apply(get_title)\n# Group all non-common titles into one single grouping \"Rare\"\nall_data['Title'] = all_data['Title'].replace(['Lady', 'Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\nall_data['Title'] = all_data['Title'].replace('Mlle', 'Miss')\nall_data['Title'] = all_data['Title'].replace('Ms', 'Miss')\nall_data['Title'] = all_data['Title'].replace('Mme', 'Mrs')","e1c36072":"def letter_match(string):\n    result1 = re.search(r'^([A-Z]*\\S*|[A-Z0-9].*\/[A-Z0-9].*)\\s',string)\n    try:\n        result1 = result1.group(0).strip()\n        return result1\n    except:\n        return ''","ca312239":"def num_match(string):\n    result2 = re.search(r'\\w([0-9]+)$',string)\n    try:\n        result2 = result2.group(0).strip()\n        return result2\n    except:\n        return ''","d8f1fa9f":"def num_len(string):\n    if len(string) == 3:\n        return 3\n    elif len(string) == 4:\n        return 4\n    elif len(string) == 5:\n        return 5\n    elif len(string) == 6:\n        return 6\n    elif len(string) == 7:\n        return 7\n    else:\n        return 0","8bc0be0a":"Ticket= pd.DataFrame()\nTicket['Letter'] = all_data['Ticket'].apply(letter_match)\nTicket['Num'] = all_data['Ticket'].apply(num_match)","6724538d":"Ticket['num_len'] = Ticket['Num'].apply(num_len)","bf7b4fb9":"Ticket['num_len'].value_counts()","c4d0c31b":"Ticket['Letter'].value_counts().head(10)","5ec2df5d":"Ticket.drop(columns = ['Letter'], axis = 1, inplace = True)","99cef7c6":"encoder = LabelEncoder()\n# Mapping Sex\nall_data['Sex'] = encoder.fit_transform(all_data['Sex']).astype(str)\n    \n# Mapping titles\nall_data['Title'] = encoder.fit_transform(all_data['Title']).astype(str)\n    \n# Mapping Embarked\nall_data['Embarked'] = encoder.fit_transform(all_data['Embarked']).astype(str)\n    \n# Mapping Fare\nall_data['CategoricalFare'] = encoder.fit_transform(all_data['CategoricalFare']).astype(str)\n    \n# Mapping Age\nall_data['CategoricalAge'] = encoder.fit_transform(all_data['CategoricalAge']).astype(str)\n\n# Mapping Cabin\nall_data['Cabin'] = encoder.fit_transform(all_data['Cabin']).astype(str)\n\n#Concatenate DataFrame Ticket\ndata_complete = pd.concat([all_data, Ticket], axis=1, copy = True)","3ee709ad":"data_complete.drop(columns = ['Age','Fare','Name','Ticket'], axis = 1, inplace = True)","fa4502f6":"data_complete['Num_1'] = data_complete['Num'].apply(lambda x: x[0] if len(x)>=1 else '0')\ndata_complete.drop(columns = ['Num'], axis = 1, inplace = True)","5f7ea436":"data_complete.head(5)","e0437bd4":"data_complete['Pclass'] = data_complete['Pclass'].astype(str)\ndata_complete['num_len'] = data_complete['num_len'].astype(str)","181279ed":"data = pd.get_dummies(data_complete)","1ba17f51":"data.head(5)","6b0237ec":"X_train_complete = data_complete.loc[:ntrain-1].values\nX_test_complete = data_complete.loc[ntrain:].values","cd7a1d80":"X_train = data.loc[:ntrain-1].values\ny_train = train_label.values\nX_test = data.loc[ntrain:].values","20f6509b":"# Some useful parameters which will come in handy later on\nr = random.seed(2)\n#SEED = 0 # for reproducibility\nNFOLDS = 5 # set folds for out-of-fold prediction\n#kf = KFold(n_splits= NFOLDS, random_state=SEED)\n#scores = cross_val_score()\n# Class to extend the Sklearn classifier\nclass SklearnHelper(object):\n    def __init__(self, clf, seed=0, params=None):\n        params['random_state'] = seed\n        self.clf = clf(**params)\n\n    def train(self, x_train, y_train):\n        self.clf.fit(x_train, y_train)\n\n    def predict(self, x):\n        return self.clf.predict(x)\n    \n    def fit(self,x,y):\n        return self.clf.fit(x,y)\n    \n    def feature_importances(self,x,y):\n        print(self.clf.fit(x,y).feature_importances_)\n    \n# Class to extend XGboost classifer","f50da505":"def k_fold_spliter(train_len, i_fold, n_splits=5 ):\n    train_list = range(train_len)\n    left = train_len % n_splits\n    \n    if left != 0:\n        batch = int(np.floor(train_len \/ n_splits))\n        batch_1 = int(np.floor(train_len \/ n_splits)) + 1\n        if i_fold  <= left-1:\n            val_set = train_list[batch_1 * (i_fold-1): i_fold * batch_1]\n            train_set = list(set(train_list).difference(set(val_set)))\n        elif i_fold == left:\n            val_set = train_list[batch_1 * (i_fold-1): i_fold * batch]\n            train_set = list(set(train_list).difference(set(val_set)))\n        else:\n            val_set = train_list[batch * (i_fold-1): i_fold * batch]\n            train_set = list(set(train_list).difference(set(val_set)))\n    else:\n        batch = int(train_len\/n_splits)\n        val_set = train_list[(i_fold-1)*batch : i_fold * batch]\n        train_set = list(set(train_list).difference(set(val_set)))\n    \n    return train_set, val_set","4d5e9b52":"def get_oof(clf, x_train, y_train, x_test):\n    oof_train = np.zeros((ntrain,))\n    oof_test = np.zeros((ntest,))\n    oof_test_skf = np.empty((NFOLDS, ntest))\n\n    for i in range(1,NFOLDS+1):\n        train_index, test_index = k_fold_spliter(len(x_train), i, n_splits=NFOLDS)\n        x_tr = x_train[train_index]\n        y_tr = y_train[train_index]\n        x_te = x_train[test_index]\n        \n        clf.train(x_tr, y_tr)\n\n        oof_train[test_index] = clf.predict(x_te)\n        oof_test_skf[i-1, :] = clf.predict(x_test)\n\n    oof_test[:] = oof_test_skf.mean(axis=0)\n    return oof_train.reshape(-1, 1), oof_test.reshape(-1, 1)","df73e7a5":"# Put in our parameters for said classifiers\n\n# Random Forest parameters\nrf_params = {\n    'n_jobs': -1,\n    'n_estimators': 500,\n     'warm_start': True, \n     #'max_features': 0.2,\n    'max_depth': 10,\n    'min_samples_leaf': 2,\n    'max_features' : 'sqrt',\n    'verbose': 0\n}\n\n# Extra Trees Parameters\net_params = {\n    'n_jobs': -1,\n    'n_estimators':500,\n    #'max_features': 0.5,\n    'max_depth': 10,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# AdaBoost parameters\nada_params = {\n    'n_estimators': 500,\n    'learning_rate' : 0.75\n}\n\n# Gradient Boosting parameters\ngb_params = {\n    'n_estimators': 500,\n     #'max_features': 0.2,\n    'max_depth': 8,\n    'min_samples_leaf': 2,\n    'verbose': 0\n}\n\n# Support Vector Classifier parameters \nsvc_params = {\n    'kernel' : 'rbf',\n    'C' : 0.5,\n    'gamma': 0.3\n    }\n\n# Logistic Regression Classifier parameters\nlr_params = {\n    'penalty':'l2',\n    'C': 0.5,\n    'solver':'newton-cg',\n    'max_iter':1000,\n    'n_jobs' : -1\n}","b21bd4cb":"#Perhaps we can solily adjust the parameter for SVC and other few classifiers.","0d9c155a":"# Create 5 objects that represent our 4 models\nSEED = 0\n\nrf = SklearnHelper(clf=RandomForestClassifier, seed=SEED, params=rf_params)\net = SklearnHelper(clf=ExtraTreesClassifier, seed=SEED, params=et_params)\nada = SklearnHelper(clf=AdaBoostClassifier, seed=SEED, params=ada_params)\ngb = SklearnHelper(clf=GradientBoostingClassifier, seed=SEED, params=gb_params)\nsvc = SklearnHelper(clf=SVC, seed=SEED, params=svc_params)\nlr = SklearnHelper(clf=LogisticRegression, seed=SEED, params = lr_params)","ef9aeede":"# Create our OOF train and test predictions. These base results will be used as new features\net_oof_train, et_oof_test = get_oof(et, X_train, y_train, X_test) # Extra Trees\nrf_oof_train, rf_oof_test = get_oof(rf, X_train, y_train, X_test) # Random Forest\n\nada_oof_train, ada_oof_test = get_oof(ada, X_train, y_train, X_test) # AdaBoost \ngb_oof_train, gb_oof_test = get_oof(gb, X_train, y_train, X_test) # Gradient Boost\n\nsvc_oof_train, svc_oof_test = get_oof(svc, X_train, y_train, X_test) # Support Vector Classifier\nlr_oof_train, lr_oof_test = get_oof(lr, X_train, y_train, X_test) # Logistic Regression Classifier\n\nprint(\"Training is complete\")","7b987a11":"ap_et = average_precision_score(y_train, et_oof_train)\nap_rf = average_precision_score(y_train, rf_oof_train)\n\nap_ada = average_precision_score(y_train, ada_oof_train)\nap_gb = average_precision_score(y_train, gb_oof_train)\n\nap_svc = average_precision_score(y_train, svc_oof_train)\nap_lr = average_precision_score(y_train, lr_oof_train)\nprint(ap_et,ap_rf,ap_ada,ap_gb,ap_svc, ap_lr)","e16ba10e":"x_train_2 = np.concatenate(( et_oof_train, rf_oof_train, ada_oof_train, gb_oof_train, svc_oof_train, lr_oof_train), axis=1)\nx_test_2 = np.concatenate(( et_oof_test, rf_oof_test, ada_oof_test, gb_oof_test, svc_oof_test, lr_oof_test), axis=1)","25b2120c":"gbm = xgb.XGBClassifier(\n    #learning_rate = 0.02,\n n_estimators= 2000,\n max_depth= 4,\n min_child_weight= 2,\n #gamma=1,\n gamma=0.9,                        \n subsample=0.8,\n colsample_bytree=0.8,\n objective= 'binary:logistic',\n nthread= -1,\n scale_pos_weight=1)\n\nscores = cross_val_score(gbm, x_train_2, y_train, cv=5)\nprint(scores)","14fad091":"gbm.fit(x_train_2, y_train)\npredictions = gbm.predict(x_test_2)","3b795ecc":"# Generate Submission File \nStackingSubmission = pd.DataFrame({ 'PassengerId': PassengerId,\n                            'Survived': predictions })\nStackingSubmission.to_csv(\"StackingSubmission.csv\", index=False)","ca935de5":"Furthermore, since having mentioned about Objects and classes within the OOP framework, let us now create 5 objects that represent our 5 learning models via our Helper Sklearn Class we defined earlier.","37a37621":"### 2.5 Feature expanding","de24a309":"- #### 1.Introduction\n- #### 2.Data exploration and Feature Engineering\n    - 2.1 Load data and packages\n    - 2.2 Process null and missing values\n    - 2.3 Check data Distribution\n    - 2.4 Label encoding\n    - 2.5 Feature expanding\n    - 2.6 Split training and validation set\n- #### 3.Model building and training\n    - 3.1 Define the model\n    - 3.2 Test base model\n    - 3.3 Stack model\n    - 3.4 Set loss function and optimizer\n    - 3.5 Model Training\n- #### 4.Emsembling\n    - 4.1 Training and validation curves\n    - 4.2 Accuracy\n    - 4.3 Confusion matrix\n- #### 5.Prediction and submition\n    - 5.1 Predict single digit\n    - 5.2 Submit results","1d3c4429":"So now let us prepare five learning models as our first level classification. These models can all be conveniently invoked via the Sklearn library and are listed as follows:","9aed7a8b":"### Stan Liu","38951793":"### 3.2 Generating our Base First-Level Models","90a43754":" - In this way, we find perhaps the Letter of the ticket is just a way to show the start and the end or sth not quite useful. As a result, we just simply choose to drop this feature","58042d4b":"1. Random Forest classifier\n2. Extra Trees classifier\n3. AdaBoost classifer\n4. Gradient Boosting classifer\n5. Support Vector Machine\n6. Logistic Regression\n7. GaussianNB\n8. BernoulliNB","45d8d81c":"### 2.4 Label encoding","a008551e":"### 3.3 Output the first level predictions","4725161d":" - Drop the redundant features","cf3ea0ae":"Here we invoke the use of Python's classes to help make it more convenient for us. For any newcomers to programming, one normally hears Classes being used in conjunction with Object-Oriented Programming (OOP). In short, a class helps to extend some code\/program for creating objects (variables for old-school peeps) as well as to implement functions and methods specific to that class.\n\nIn the section of code below, we essentially write a class SklearnHelper that allows one to extend the inbuilt methods (such as train, predict and fit) common to all the Sklearn classifiers. Therefore this cuts out redundancy as won't need to write the same methods five times if we wanted to invoke five different classifiers.","4624be72":"### 3.1 Define the model","e340343d":"### 2.2 Process null and missing values","eef54ea6":"## 3.Model building and training","365e45cc":"## 2. Data exploration and Feature Engineering","e6a0851b":" - Helpers via Python Classes","607af31c":" - Then we do binner for some continuous features","1413c534":"# Death prediction in disaster","6b64f2d3":" - Then let's check some of the distributions","137d29db":" - Out-of-Fold Predictions\n - Now as alluded to above in the introductory section, stacking uses predictions of base classifiers as input for training to a second-level model. However one cannot simply train the base models on the full training data, generate predictions on the full test set and then output these for the second-level training. This runs the risk of your base model predictions already having \"seen\" the test set and therefore overfitting when feeding these predictions.","782e7645":"## 1. Introduction","0f0bac0a":" -  Obviously, we find an outlier for tickert. we then consider to normalize it in oreder to obatin a better distribution","d0640ac7":" - Here we extract a title variable","418d857d":"Finally after that brief whirlwind detour with regards to feature engineering and formatting, we finally arrive at the meat and gist of the this notebook.\n\nCreating a Stacking ensemble!","0c4b8037":"### 2.1 Load data and packages","5f330378":"### 2.3 Check Data Distribution","cffbd0c4":"### 2.6 Split training and validation set","8fdb7037":" - Fill in all the nan values","de58481d":"#### 20\/12\/2018","49346a87":" - Also, we try to keep the contineous features. In that way, we need to adjust its distribution"}}