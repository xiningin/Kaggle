{"cell_type":{"ffaaba96":"code","0405ba89":"code","c068c10a":"code","16d9359e":"code","84ec5e04":"code","93eba76a":"code","fdbf0492":"code","001c91a1":"code","a7191240":"code","d1f52c60":"code","5c3b8b7c":"code","b6cc8f8b":"code","f473dd2a":"code","79cfac36":"code","7441bea0":"code","a2a1d9aa":"code","7789ce19":"code","84678fdc":"code","00b97830":"code","bbe7df02":"code","b1d10b5b":"code","c52f0583":"markdown","d6c902bb":"markdown","200da6ac":"markdown","c1bf3fc8":"markdown","a8d388f3":"markdown","02e24959":"markdown","af1c79fe":"markdown","82425a74":"markdown","660098a4":"markdown","fcff8dae":"markdown","9c181395":"markdown","1fe7131f":"markdown","9e7183b5":"markdown","dbe7ba7e":"markdown","8077820f":"markdown","3fda98f2":"markdown","8e1f448c":"markdown","1188a908":"markdown"},"source":{"ffaaba96":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sea","0405ba89":"from keras.datasets.mnist import load_data\nfrom keras.layers import Dense,Embedding,Conv2D,MaxPooling2D,Flatten,Dropout\nfrom keras.models import Sequential\nfrom keras.losses import binary_crossentropy\nfrom keras.optimizers import SGD,rmsprop,RMSprop\nfrom keras.metrics import binary_accuracy\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.image import ImageDataGenerator","c068c10a":"trainDataGen=ImageDataGenerator(rescale=1.\/255)\ntestDataGen=ImageDataGenerator(rescale=1.\/255)","16d9359e":"trainData=trainDataGen.flow_from_directory(\"\/kaggle\/input\/cat-and-dog\/training_set\/training_set\",batch_size=20,class_mode='binary')\ntestData=testDataGen.flow_from_directory(\"\/kaggle\/input\/cat-and-dog\/test_set\/test_set\",batch_size=20,class_mode='binary')","84ec5e04":"trainData.image_shape","93eba76a":"Y_train=to_categorical(trainData.classes)\nY_test=to_categorical(testData.classes)","fdbf0492":"Y_train.shape","001c91a1":"sea.countplot(trainData.classes)","a7191240":"sea.countplot(testData.classes)","d1f52c60":"network=Sequential()\nnetwork.add(Conv2D(128,(3,3),activation='relu',input_shape=(256,256,3)))\nnetwork.add(MaxPooling2D((2,2)))\nnetwork.add(Conv2D(128,(3,3),activation='relu'))\nnetwork.add(MaxPooling2D((2,2)))\nnetwork.add(Conv2D(128,(3,3),activation='relu'))\nnetwork.add(MaxPooling2D((2,2)))\nnetwork.add(Conv2D(128,(3,3),activation='relu'))\nnetwork.add(MaxPooling2D((2,2)))\nnetwork.add(Conv2D(128,(3,3),activation='relu'))\nnetwork.add(Flatten())\nnetwork.add(Dropout(0.9))\nnetwork.add(Dense(256,activation='relu'))\nnetwork.add(Dropout(0.5))\nnetwork.add(Dense(256,activation='relu'))\nnetwork.add(Dropout(0.5))\nnetwork.add(Dense(256,activation='relu'))\nnetwork.add(Dense(1,activation='sigmoid'))","5c3b8b7c":"network.summary()","b6cc8f8b":"network.compile(loss='binary_crossentropy',optimizer=RMSprop(lr=0.0001),metrics=['acc'])","f473dd2a":"network.fit_generator(trainData,steps_per_epoch=250,epochs=40)","79cfac36":"network.evaluate_generator(testData,steps=250)","7441bea0":"filename=\"\/kaggle\/input\/cat-and-dog\/training_set\/training_set\/cats\/cat.1.jpg\"\nplt.imshow(plt.imread(filename))","a2a1d9aa":"from keras.preprocessing import image\n\nimg=image.load_img(filename,target_size=(256,256))\nimgArray=image.img_to_array(img)\nimgTensor=np.expand_dims(imgArray,axis=0)\/255","7789ce19":"imgTensor.shape","84678fdc":"from keras import models\nlayer_output=[layer.output for layer in network.layers[:4]]\nactivationModel=models.Model(inputs=network.input,output=layer_output)","00b97830":"activation=activationModel.predict(imgTensor)[2]\nactivation.shape","bbe7df02":"plt.matshow(activation[0,:,:,4])","b1d10b5b":"plt.matshow(activation[0,:,:,50])","c52f0583":"Now lets begin with network creation.\n\nWe will use CovNet with 3x3 window,128 channel and activation function as 'Relu'","d6c902bb":"Plotting output for 50th channel","200da6ac":"Lets generate the output of model for our input\n\nTaking output of first 4 layer","c1bf3fc8":"Lets see the summary of our network.","a8d388f3":"So both training set and test set have equal number of samples for both category.","02e24959":"For 3rd layer or 2nd CovNet layer","af1c79fe":"I have added few dropout layers in model to handle overfitting. This has decreased the training accuracy but increased the testing accuracy. \n\nFeel free to change the dropout values to see the affect on the accuracy of model.\n\nOverfitting can also be handlled by bringing in more data which can be done by Data Augmentation. This involves generating more data from existing data.\n\nFor example, a sample image can be rotated,flipped,zoomed or transformed to generate more data.","82425a74":"****\n****","660098a4":"So from above block of code we found out that image size is 256x256 and in 3 channel. We can resize the images to smaller size lets say 150x150 so as to decrease the size of feature map generated by CovNets. We can pass **target_size** parameter to flow_from_directory to get the desired size.\n\n\nHere lets continue with the original size i.e 256x256","fcff8dae":"Here we will use fit_generator instead of fit as input to network is going too be generator.\n\nsteps_per_epoch is 250 and epochs is 40. This is going to be highly computationally expensive.","9c181395":"This can be done by adding more parameter to the ImageGenerator function to generated image with transformation\n\nfor example:-\n\nImageDataGenerator(rescale=1.\/255,zca_whitening=True,rotation_range=30,vertical_flip=True)","1fe7131f":"Lets create the 4D tensor for image","9e7183b5":"Lets see how to visualize the output of layers of CovNet","dbe7ba7e":"Here we will use RMSprop as optimizers. Its default learning rate is 0.001. I had set it to 0.0001.\n\nYou can also use SGD and set the learning rate and momentum.","8077820f":"Plotting the output of 4th channel","3fda98f2":"activation output have 128 channel and with size of 254x254","8e1f448c":"Lets generate the image data for training set and test set.","1188a908":"We will use ImageDataGenerator for generating data from the image set.\n\nWe already know that small the size the size of computational data, faster is the processing of neural network. So we will scale the image by factor of 1\/255 to bring all the RGB values in range [0,1]."}}