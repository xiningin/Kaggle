{"cell_type":{"ddf4d5cd":"code","4901e33b":"code","2ce35211":"code","284cdcd8":"code","f796fbbf":"code","4dd8f098":"code","86e9c4d8":"code","1592ee67":"code","157ce37a":"code","05fa9b53":"code","82cccf67":"code","b229f98b":"code","e0d1ac73":"code","be69925c":"code","9bc99d08":"code","073bfca3":"code","e4da64b4":"code","32a22e08":"code","4e05dc79":"code","dfed9d99":"code","139152ed":"code","86b6222d":"code","733c8fb7":"code","c24e8d95":"code","c0a0d9ec":"code","243ff837":"code","46519b24":"code","a594c47a":"code","7cd55dae":"code","a854947d":"markdown","0c233ae8":"markdown","b998c8db":"markdown","6f6cd210":"markdown"},"source":{"ddf4d5cd":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random","4901e33b":"#reading dataset\ndf = pd.read_csv(\"..\/input\/wine-quality-binary-classification\/wine.csv\")\ndf.head()","2ce35211":"df.shape","284cdcd8":"df.info()","f796fbbf":"# Encoding categorical variable\ndf['quality_cat'] = df['quality'].astype('category').cat.codes\ndf.head()","4dd8f098":"sns.pairplot(df,hue='quality')","86e9c4d8":"corr = df.corr()\ncorr.style.background_gradient(cmap='coolwarm')","1592ee67":"# Plotting the highest correlated pairs\nsns.scatterplot(data=df, x='density', y='alcohol', hue='quality')\nplt.show()","157ce37a":"fig = plt.figure(figsize = (15,20))\nax = fig.gca()\ndf.hist(ax = ax)","05fa9b53":"df.quality_cat.value_counts().plot(kind='bar')\nplt.xlabel(\"Good or Bad\")\nplt.ylabel(\"Count\")\nplt.title(\"Quality\")\n#Here we can see that dataset is not much imbalanced so there is no need to balance.","82cccf67":"plt.figure(figsize=(20,10))\nplt.subplots_adjust(left=0, bottom=0.5, right=0.9, top=0.9, wspace=0.5, hspace=0.8)\nplt.subplot(141)\nplt.title('Percentage of good and bad quality wine',fontsize = 20)\ndf['quality'].value_counts().plot.pie(autopct=\"%1.1f%%\")","b229f98b":"df1 = df.drop('quality',axis=1)\ndf1.info()","e0d1ac73":"X = df1.drop('quality_cat',axis=1)\nY = df1['quality_cat']","be69925c":"Y.head()","9bc99d08":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, Y,test_size=0.2,random_state=0)","073bfca3":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","e4da64b4":"from sklearn.linear_model import LogisticRegression # for Logistic Regression Algorithm\nfrom sklearn.preprocessing import StandardScaler","32a22e08":"sc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)","4e05dc79":"lr = LogisticRegression()\nlr.fit(X_train ,y_train)\ny_pred = lr.predict(X_test)","dfed9d99":"lr.score(X_test,y_test)","139152ed":"from sklearn.metrics import accuracy_score\nfrom sklearn import metrics\nmetrics.accuracy_score(y_test,y_pred)","86b6222d":"\nTHRESHOLD = 0.5\ny_pred = np.where(y_pred>0.5,1,0)","733c8fb7":"y_pred","c24e8d95":"from sklearn.metrics import confusion_matrix\nmat = confusion_matrix(y_test, y_pred)\nplt.figure(figsize=(10, 8))\nsns.heatmap(mat, annot=True)","c0a0d9ec":"from sklearn.metrics import classification_report\ntarget_names = ['Bad', 'Good']\nprint(classification_report(y_test, y_pred, target_names=target_names))","243ff837":"lr = 0.06\n\n# Parameters initialization\nweights = np.random.normal(0, 0.1, 11)\nbiais = random.normalvariate(0, 0.1)\n\nm = X_train.shape[0]\nfor epoch in range(1000):\n\n    # Forward pass\n    Z = np.dot(X_train, weights) + biais\n    A = 1 \/ (1 + np.exp(-Z))\n\n    #Loss Computation\n    J = np.sum(-(y_train * np.log(A) + (1 - y_train) * np.log(1 - A))) \/ m\n\n    # Gradient computation\n    dZ = A - y_train\n    dw = np.dot(dZ, X_train) \/ m\n    db = np.sum(dZ) \/ m\n\n    # Update weights\n    weights = weights - lr * dw\n    biais = biais - lr * db\n    \n    if epoch % 10 == 0:\n        print(\"epoch %s - loss %s\" % (epoch, J))","46519b24":"preds = []\nfor feats in X_test:\n\n    z = np.dot(feats, weights) + biais\n    a = 1 \/ (1 + np.exp(-z))\n\n    if a > 0.5:\n        preds.append(1)\n    elif a <= 0.5:\n        preds.append(0)","a594c47a":"from sklearn.metrics import classification_report\ntarget_names = ['Bad', 'Good']\nprint(classification_report(y_test, preds, target_names=target_names))","7cd55dae":"from sklearn.metrics import confusion_matrix\nmat = confusion_matrix(y_test, preds)\nplt.figure(figsize=(10, 8))\nsns.heatmap(mat, annot=True)","a854947d":"# Implementing Logistic Regression from Scratch","0c233ae8":"Conclusion: Here we can see that result of both the method is almost same","b998c8db":"1. Why you want to apply classification on selected dataset? Discuss full story behind dataset\n\n    Here, the dependent variable which is 'quality' having data as class value so we can apply classification to classify the test data belongs to which class.\n\n2. How many total observations in data?\n\n    There are total 1599 observations are there\n\n3. How many independent variables?\n\n    There are total 11 independent varibales are there\n\n4. Which is dependent variable?\n\n    qualtity is the dependent variable","6f6cd210":"# Implementing logistic Regression Using Sklearn"}}