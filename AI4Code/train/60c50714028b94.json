{"cell_type":{"50d38d1f":"code","4a357674":"code","81e7b09c":"code","e8cf3721":"code","8be6ceed":"code","7ca54f27":"code","2c6f5247":"code","3d4cd225":"code","31cae190":"code","945c7760":"code","a3ed322e":"code","8186a079":"code","21342d9c":"code","809c887e":"markdown","85acea82":"markdown"},"source":{"50d38d1f":"!apt-get install python-opengl -y\n!pip install pyvirtualdisplay \n","4a357674":"import gym\nimport os\nimport matplotlib.pyplot as plt\nfrom pyvirtualdisplay import Display\n\ndisplay = Display(visible=0, size=(1400, 900))\ndisplay.start()\n#os.environ[\"DISPLAY\"] = \":\" + str(display.display) + \".\" + str(display.screen)\n\n","81e7b09c":"!pip install pyglet\nenv = gym.make(\"CartPole-v0\")\n\nenv.reset()\nplt.imshow(env.render('rgb_array'))\nplt.grid(False)\n","e8cf3721":"import gym\nimport tensorflow as tf","8be6ceed":"import random\nimport pandas as pd\nimport numpy as np\n\n\nclass AgentBasic(object):\n    ''' Simple agent class. '''\n    def __init__(self):\n        pass\n\n    def act(self, obs):\n        ''' Based on angle, return action. '''\n        angle = obs[2]\n        return 0 if angle < 0 else 1\n\n\nclass AgentRandom(object):\n    ''' Random agent class. '''\n    def __init__(self, action_space):\n        self.action_space = action_space\n\n    def act(self):\n        ''' Agent randomly chooses an action. '''\n        return self.action_space.sample()\n\n\nclass AgentLearning(object):\n    '''' Agent that can learn via Q-learning. '''\n    def __init__(self, env, alpha=0.1, epsilon=1.0, gamma=0.9):\n        self.env = env\n        self.alpha = alpha          # Learning factor\n        self.epsilon = epsilon\n        self.gamma = gamma          # Discount factor\n        self.Q_table = dict()\n        self._set_seed()\n        # Following variables for statistics\n        self.training_trials = 0\n        self.testing_trials = 0\n\n    def _set_seed(self):\n        ''' Set random seeds for reproducibility. '''\n        np.random.seed(21)\n        random.seed(21)\n\n    def build_state(self, features):\n        ''' Build state by concatenating features (bins) into 4 digit int. '''\n        return int(\"\".join(map(lambda feature: str(int(feature)), features)))\n\n    def create_state(self, obs):\n        ''' Create state variable from observation.\n\n        Args:\n            obs: Observation list with format [horizontal position, velocity,\n                 angle of pole, angular velocity].\n        Returns:\n            state: State tuple\n        '''\n        cart_position_bins = pd.cut([-2.4, 2.4], bins=10, retbins=True)[1][1:-1]\n        pole_angle_bins = pd.cut([-2, 2], bins=10, retbins=True)[1][1:-1]\n        cart_velocity_bins = pd.cut([-1, 1], bins=10, retbins=True)[1][1:-1]\n        angle_rate_bins = pd.cut([-3.5, 3.5], bins=10, retbins=True)[1][1:-1]\n        state = self.build_state([np.digitize(x=[obs[0]], bins=cart_position_bins)[0],\n                                 np.digitize(x=[obs[1]], bins=pole_angle_bins)[0],\n                                 np.digitize(x=[obs[2]], bins=cart_velocity_bins)[0],\n                                 np.digitize(x=[obs[3]], bins=angle_rate_bins)[0]])\n        return state\n\n    def choose_action(self, state):\n        ''' Given a state, choose an action.\n\n        Args:\n            state: State of the agent.\n        Returns:\n            action: Action that agent will take.\n        '''\n        if random.random() < self.epsilon:\n            action = self.env.action_space.sample()\n        else:\n            # Find max Q value\n            max_Q = self.get_maxQ(state)\n            actions = []\n            for key, value in self.Q_table[state].items():\n                if value == max_Q:\n                    actions.append(key)\n            if len(actions) != 0:\n                action = random.choice(actions)\n        return action\n\n    def create_Q(self, state, valid_actions):\n        ''' Update the Q table given a new state\/action pair.\n\n        Args:\n            state: List of state booleans.\n            valid_actions: List of valid actions for environment.\n        '''\n        if state not in self.Q_table:\n            self.Q_table[state] = dict()\n            for action in valid_actions:\n                self.Q_table[state][action] = 0.0\n        return\n\n    def get_maxQ(self, state):\n        ''' Find the maximum Q value in a given Q table.\n\n        Args:\n            Q_table: Q table dictionary.\n            state: List of state booleans.\n        Returns:\n            maxQ: Maximum Q value for a given state.\n        '''\n        maxQ = max(self.Q_table[state].values())\n        return maxQ\n\n    def learn(self, state, action, prev_reward, prev_state, prev_action):\n        ''' Update the Q-values\n\n        Args:\n            state: State at current time step.\n            action: Action at current time step.\n            prev_reward: Reward at previous time step.\n            prev_state: State at previous time step.\n            prev_action: Action at previous time step.\n        '''\n        # Updating previous state\/action pair so I can use the 'future state'\n        self.Q_table[prev_state][prev_action] = (1 - self.alpha) * \\\n            self.Q_table[prev_state][prev_action] + self.alpha * \\\n            (prev_reward + (self.gamma * self.get_maxQ(state)))\n        return\n    \nclass QNetworkAgent:\n    def __init__(self , learning_rate=0.01,\n                state_size=4, \n                action_size=2,\n                hidden_size=10,\n                name='QNetwork'):\n        # state inputs to the Q-Network\n        # https:\/\/stackoverflow.com\/questions\/35919020\/whats-the-difference-of-name-scope-and-a-variable-scope-in-tensorflow\n        with tf.variable_scope(name):\n            self.inputs_ = tf.placeholder(tf.float32, [None, state_size], name='inputs')\n        \n            # One hot encode the actions to latte choose the Q-Value for the action\n            self.actions_ = tf.placeholder(tf.int32 , [None] , name=\"actions\") # action value is 0 or 1 , one_hot take int values\n            one_hot_actions = tf.one_hot(self.actions_, action_size)\n            \n            # Target Q values for training\n            self.targetQs_ = tf.placeholder(tf.float32, [None] , name=\"targetQ\")\n            \n            # ReLU hidden layer\n            # See tf.layers.dense() tf.contrib.layers.fully_connected()\n            self.fc1 = tf.contrib.layers.fully_connected(self.inputs_ , hidden_size) # activation relu applied by default \n            self.fc2 = tf.contrib.layers.fully_connected(self.fc1 , hidden_size)\n            \n            # Linear output layer\n            self.output = tf.contrib.layers.fully_connected(self.fc2 , action_size , activation_fn=None)\n            \n            \n            ## Train with loss (targetQ - Q)^2\n            # output has length 2, for two actions. This next line chooses\n            # one value from output (per row) according to the one-hot encoded actions.\n            self.Q = tf.reduce_sum(tf.multiply(self.output , one_hot_actions), axis=1)\n            \n            self.loss = tf.reduce_mean(tf.square(self.targetQs_ - self.Q))\n            \n            self.optimizer = tf.train.AdamOptimizer(learning_rate).minimize(self.loss)","7ca54f27":"import numpy as np\n\ndef running_mean(x, N):\n    cumsum = np.cumsum(np.insert(x, 0, 0)) \n    return (cumsum[N:] - cumsum[:-N]) \/ N\n\nclass Memory():\n    def __init__(self , max_size= 1000):\n        self.buffer = deque(maxlen = max_size)\n        \n    def add(self, experience):\n        self.buffer.append(experience)\n        \n    def sample(self, batch_size):\n        index = np.random.choice(np.arange(len(self.buffer)),\n                                size = batch_size,\n                                replace = False)\n        return [self.buffer[idx] for idx in index]\n\ndef environment_info(env):\n    ''' Prints info about the given environment. '''\n    print('************** Environment Info **************')\n    print('Observation space: {}'.format(env.observation_space))\n    print('Observation space high values: {}'.format(env.observation_space.high))\n    print('Observation space low values: {}'.format(env.observation_space.low))\n    print('Action space: {}'.format(env.action_space))\n    print()\n\n\ndef basic_guessing_policy(env, agent):\n    ''' Execute random guessing policy. '''\n    totals = []\n    for episode in range(500):\n        episode_rewards = 0\n        obs = env.reset()\n        # env.render()\n        for step in range(1000):  # 1000 steps max unless failure\n            action = agent.act(obs)\n            obs, reward, done, info = env.step(action)\n            episode_rewards += reward\n            # env.render()\n            if done:\n                # Terminal state reached, reset environment\n                break\n        totals.append(episode_rewards)\n\n    print('************** Reward Statistics **************')\n    print('Average: {}'.format(np.mean(totals)))\n    print('Standard Deviation: {}'.format(np.std(totals)))\n    print('Minimum: {}'.format(np.min(totals)))\n    print('Maximum: {}'.format(np.max(totals)))\n\n\ndef random_guessing_policy(env, agent):\n    ''' Execute random guessing policy. '''\n    totals = []\n    for episode in range(500):\n        episode_rewards = 0\n        obs = env.reset()\n        # env.render()\n        for step in range(1000):  # 1000 steps max\n            action = agent.act()\n            obs, reward, done, info = env.step(action)\n            episode_rewards += reward\n            # env.render()\n            if done:\n                # Terminal state reached, reset environment\n                break\n        totals.append(episode_rewards)\n\n    print('Average: {}'.format(np.mean(totals)))\n    print('Standard Deviation: {}'.format(np.std(totals)))\n    print('Minimum: {}'.format(np.min(totals)))\n    print('Maximum: {}'.format(np.max(totals)))\n\n\ndef q_learning(env, agent,episodes):\n    '''\n    Implement Q-learning policy.\n\n    Args:\n        env: Gym enviroment object.\n        agent: Learning agent.\n    Returns:\n        Rewards for training\/testing and epsilon\/alpha value history.\n    '''\n    # Start out with Q-table set to zero.\n    # Agent initially doesn't know how many states there are...\n    # so if a new state is found, then add a new column\/row to Q-table\n    valid_actions = [0, 1]\n    tolerance = 0.001\n    training = True\n    training_totals = []\n    testing_totals = []\n    history = {'epsilon': [], 'alpha': []}\n    for episode in range(episodes):  # 688 testing trials\n        episode_rewards = 0\n        obs = env.reset()\n        # If epsilon is less than tolerance, testing begins\n        if agent.epsilon < tolerance:\n            agent.alpha = 0\n            agent.epsilon = 0\n            training = False\n        # Decay epsilon as training goes on\n        agent.epsilon = agent.epsilon * 0.99  # 99% of epsilon value\n        for step in range(200):        # 200 steps max\n            state = agent.create_state(obs)           # Get state\n            agent.create_Q(state, valid_actions)      # Create state in Q_table\n            action = agent.choose_action(state)         # Choose action\n            obs, reward, done, info = env.step(action)  # Do action\n            episode_rewards += reward                   # Receive reward\n            # Skip learning for first step\n            if step != 0:\n                # Update Q-table\n                agent.learn(state, action, prev_reward, prev_state, prev_action)\n            prev_state = state\n            prev_action = action\n            prev_reward = reward\n            if done:\n                # Terminal state reached, reset environment\n                break\n        if training:\n            training_totals.append(episode_rewards)\n            agent.training_trials += 1\n            history['epsilon'].append(agent.epsilon)\n            history['alpha'].append(agent.alpha)\n        else:\n            testing_totals.append(episode_rewards)\n            agent.testing_trials += 1\n            # After 100 testing trials, break. Because of OpenAI's rules for solving env\n            if agent.testing_trials == 100:\n                break\n    return training_totals, testing_totals, history\n\n\n\n","2c6f5247":"import numpy as np\nimport matplotlib.pyplot as plt\n\n\ndef moving_average(data, window_size=10):\n    ''' Calculate moving average with given window size.\n\n    Args:\n        data: List of floats.\n        window_size: Integer size of moving window.\n    Returns:\n        List of rolling averages with given window size.\n    '''\n    sum_vec = np.cumsum(np.insert(data, 0, 0))\n    moving_ave = (sum_vec[window_size:] - sum_vec[:-window_size]) \/ window_size\n    return moving_ave\n\n\ndef display_stats(agent, training_totals, testing_totals, history):\n    ''' Print and plot the various statistics from q-learning data.\n\n    Args:\n        agent: Agent containing variables useful for post analysis\n        training_totals: List of training rewards per episode.\n        testing_totals: List of testing rewards per episode.\n        epsilon_hist: List of all epsilon values.\n    '''\n    train = None\n    test = None\n    if agent.training_trails:\n        train = agent.training_trails\n    if agent.testing_trails:\n        test = agent.testing_trails\n    print(train)\n    print(test)\n    totals = train+test\n    print('******* Training Stats *********')\n    print('Average: {}'.format(np.mean(training_totals)))\n    print('Standard Deviation: {}'.format(np.std(training_totals)))\n    print('Minimum: {}'.format(np.min(training_totals)))\n    print('Maximum: {}'.format(np.max(training_totals)))\n    print('Number of training episodes: {}'.format(train))\n    print()\n    print('******* Testing Stats *********')\n    print('Average: {}'.format(np.mean(testing_totals)))\n    print('Standard Deviation: {}'.format(np.std(testing_totals)))\n    print('Minimum: {}'.format(np.min(testing_totals)))\n    print('Maximum: {}'.format(np.max(testing_totals)))\n    print('Number of testing episodes: {}'.format(test))\n    fig = plt.figure(figsize=(10, 7))\n    # Plot Parameters plot\n    ax1 = fig.add_subplot(311)\n    ax1.plot([num + 1 for num in range(train)],\n             history['epsilon'],  # epsilon_hist,\n             color='b',\n             label='Exploration Factor (Epsilon)')\n    ax1.plot([num + 1 for num in range(train)],\n             history['alpha'],  #alpha_hist,\n             color='r',\n             label='Learning Factor (Alpha)')\n    ax1.set(title='Paramaters Plot',\n            ylabel='Parameter values',\n            xlabel='Trials')\n\n    # Plot rewards\n    ax2 = fig.add_subplot(312)\n    ax2.plot([num + 1 for num in range(train)],\n             training_totals,\n             color='m',\n             label='Training',\n             alpha=0.4, linewidth=2.0)\n    #total_trials = agent.training_trials + agent.testing_trials\n    ax2.plot([num + 1 for num in range(train, totals)],\n             testing_totals,\n             color='k',\n             label='Testing', linewidth=2.0)\n    ax2.set(title='Reward per trial',\n            ylabel='Rewards',\n            xlabel='Trials')\n\n    # Plot rolling average rewards\n    ax3 = fig.add_subplot(313)\n    window_size = 10\n    train_ma = moving_average(training_totals, window_size=window_size)\n    train_epi = [num+1 for num in range(train-(window_size-1))]\n    ax3.plot(train_epi, train_ma,\n             color='m',\n             label='Training',\n             alpha=0.4, linewidth=2.0)\n    test_ma = moving_average(testing_totals, window_size=window_size)\n    totals = totals - (window_size*2) + 2\n    test_epi = [num+1 for num in range(train-(window_size-1), totals)]\n    ax3.plot(test_epi, test_ma,\n             color='k',\n             label='Testing', linewidth=2.0)\n    ax3.set(title='Rolling Average Rewards',\n            ylabel='Reward',\n            xlabel='Trials')\n\n    fig.subplots_adjust(hspace=0.5)\n    ax1.legend(loc='best')\n    ax2.legend(loc='best')\n    ax3.legend(loc='best')\n\n\ndef save_info(agent, training_totals, testing_totals):\n    ''' Write statistics into text file.\n\n    Args:\n        agent: Agent containing variables useful for post analysis\n        training_totals: List of training rewards per episode.\n        testing_totals: List of testing rewards per episode.\n    '''\n    with open('CartPole-v0_stats.txt', 'w') as file_obj:\n        file_obj.write('\/-------- Q-Learning --------\\\\\\n')\n        file_obj.write('\\n\/---- Training Stats ----\\\\\\n')\n        file_obj.write('Average: {}\\n'.format(np.mean(training_totals)))\n        file_obj.write('Standard Deviation: {}\\n'.format(np.std(training_totals)))\n        file_obj.write('Minimum: {}\\n'.format(np.min(training_totals)))\n        file_obj.write('Maximum: {}\\n'.format(np.max(training_totals)))\n        file_obj.write('Number of training episodes: {}\\n'.format(agent.training_trials))\n        file_obj.write('\\n\/---- Testing Stats ----\\\\\\n')\n        file_obj.write('Average: {}\\n'.format(np.mean(testing_totals)))\n        file_obj.write('Standard Deviation: {}\\n'.format(np.std(testing_totals)))\n        file_obj.write('Minimum: {}\\n'.format(np.min(testing_totals)))\n        file_obj.write('Maximum: {}\\n'.format(np.max(testing_totals)))\n        file_obj.write('Number of testing episodes: {}\\n'.format(agent.testing_trials))\n        file_obj.write('\\n\/---- Q-Table ----\\\\')\n        for state in agent.Q_table:\n            file_obj.write('\\n State: ' + str(state) +\n                           '\\n\\tAction: ' + str(agent.Q_table[state]))\n    # Save figure and display plot\n    plt.savefig('plots.png')\n    plt.show()\n","3d4cd225":"def main(agent='basic'):\n    ''' Execute main program. '''\n    # Create a cartpole environment\n    # Observation: [horizontal pos, velocity, angle of pole, angular velocity]\n    # Rewards: +1 at every step. i.e. goal is to stay alive\n    env = gym.make('CartPole-v0')\n    # Set environment seed\n    env.seed(21)\n    environment_info(env)\n    # Basic agent enabled\n    if agent == 'basic':\n        agent = AgentBasic()\n        basic_guessing_policy(env, agent)\n    # Random agent enabled\n    elif agent == 'random':\n        agent = AgentRandom(env.action_space)\n        random_guessing_policy(env, agent)\n    # Q-learning agent enabled\n    elif agent == 'q-learning':\n        agent = AgentLearning(env, alpha=0.9, epsilon=1.0, gamma=0.9)\n        training_totals, testing_totals, history = q_learning(env, agent,1000)\n        display_stats(agent, training_totals, testing_totals, history)\n        save_info(agent, training_totals, testing_totals)\n        # Check if environment is solved\n        if np.mean(testing_totals) >= 195.0:\n            print(\"Environment SOLVED!!!\")\n        else:\n            print(\"Environment not solved.\",\n                  \"Must get average reward of 195.0 or\",\n                  \"greater for 100 consecutive trials.\")\n    # No argument passed, agent defaults to Basic\n    elif agent == 'DNN':\n        train_episodes = 700          # max number of episodes to learn from\n        max_steps = 200               # max steps in an episode\n        gamma = 0.99                   # future reward discount\n\n        # Exploration parameters\n        explore_start = 1.0            # exploration probability at start\n        explore_stop = 0.01            # minimum exploration probability \n        decay_rate = 0.0001            # exponential decay rate for exploration prob\n\n        # Network parameters\n        hidden_size = 64               # number of units in each Q-network hidden layer\n        learning_rate = 0.0001         # Q-network learning rate\n\n        # Memory parameters\n        memory_size = 10000            # memory capacity\n        batch_size = 20                # experience mini-batch size\n        pretrain_length = batch_size   # number experiences to pretrain the memory\n        tf.compat.v1.reset_default_graph()\n        mainQN = QNetworkAgent(name=\"main\" , hidden_size=hidden_size, learning_rate=learning_rate)\n        env.reset()\n        # Take one random step to get the pole and cart moving\n        state , reward, done, info = env.step(env.action_space.sample())\n\n        memory = Memory(max_size=memory_size)\n\n        # Make a bunch of random actions and store the experiences\n        for i in range(pretrain_length):\n            #env.render()\n\n            action = env.action_space.sample()\n            next_state, reward, done, _ = env.step(action)\n\n            if done:\n                # The simulation fails so no next state\n                next_state = np.zeros(state.shape)\n\n                # Add experience to memory\n                memory.add((state, action, reward, next_state))\n\n                # start new episode\n                env.reset()\n                # Take one random step to get the pole and cart moving\n                state, reward, done, _ = env.step(env.action_space.sample())\n\n            else:\n                # Add experience to memory\n                memory.add((state,action, reward , next_state))\n                state = next_state\n            # Now train with experience\n\n        saver = tf.train.Saver()\n        rewards_list = []\n\n        with tf.Session() as sess:\n            sess.run(tf.global_variables_initializer())\n\n        decay_step = 0\n        for episode in range(1, train_episodes):\n            total_reward = 0\n            step =0\n            while step < max_steps:\n                decay_step +=1\n                env.render()\n\n                # Explore or Exploit\n                explore_prob = explore_stop + (explore_start - explore_stop)*np.exp(-decay_rate*decay_step)\n\n                if explore_prob > np.random.rand() : # return [0,1)\n                    # random action\n                    action = env.action_space.sample()\n                else:\n                    # Get action from Q-network\n                    feed = {mainQN.inputs_:state.reshape((1, *state.shape))}\n                    Q_pred = sess.run(mainQN.output , feed_dict= feed)\n                    action = np.argmax(Q_pred)\n\n                # Take action, get new_state and reward\n                next_state , reward , done, _ = env.step(action)\n\n                total_reward +=reward\n\n\n                if done:\n                    # the episode ends so no next state\n                    next_state = np.zeros(state.shape)\n                    step = max_steps\n\n                    print('Episode: {}'.format(episode),\n                         'Total reward: {}'.format(total_reward),\n                         'Training loss: {:.4f}'.format(loss),\n                         'Explore Prob: {:.4f}'.format(explore_prob))\n\n                    rewards_list.append((episode , total_reward))\n\n                    # Add experience to memory\n                    memory.add((state, action,reward, next_state))\n\n                    # Start new episode\n                    env.reset()\n                    # Take one random step to get the pole and cart moving\n                    state, reward, done, _ = env.step(env.action_space.sample())\n\n                else:\n                    # Add experience to memory\n                    memory.add((state, action, reward, next_state))\n                    state = next_state\n                    step +=1\n\n                # Sample mini-batch from memory\n                batch = memory.sample(batch_size)\n                states_mb = np.array([each[0] for each in batch])\n                actions_mb = np.array([each[1] for each in batch])\n                rewards_mb = np.array([each[2] for each in batch])\n                next_states_mb = np.array([each[3] for each in batch])\n\n                ### TRAINING NETWORK\n                targetQs = sess.run(mainQN.output, feed_dict={mainQN.inputs_:next_states_mb})\n\n                # Set target_Qs to 0 for states where episode ends \n                episode_ends = (next_states_mb==np.zeros(states_mb[0].shape)).all(axis=1)\n\n                targetQs[episode_ends]  = (0,0)\n\n                targets = rewards_mb + gamma*np.max(targetQs, axis=1)\n\n                loss , _ = sess.run([mainQN.loss , mainQN.optimizer],\n                                   feed_dict={mainQN.inputs_:states_mb,\n                                             mainQN.targetQs_: targets,\n                                             mainQN.actions_: actions_mb})\n\n\n\n        saver.save(sess, \"checkpoints\/cartpole{}.ckpt\".format(train_episodes))\n    else:\n        agent = AgentBasic()\n        basic_guessing_policy(env, agent)\n\n","31cae190":"main('random')","945c7760":"main('q-learning')","a3ed322e":"def test(numOfEpisodes,agent):\n    #Whethet it fails or reach the best outcome we will change done to true\n    done = False\n    testing = []\n    scores = []\n    rewards = []\n    history = {'epsilon':[],'alpha':[]}\n    agents_test = {'training_trails':[]}\n    for episode in range(numOfEpisodes):\n\n        for step in range(numOfSteps):\n            # render the einvornment every 100 episode to see our progress\n\n            # take an action whether randomly or based on previous experiences\n            action = agent.act(state)\n\n            nextState, reward, done, info = env.step(action)\n            rewards.append(reward)\n            # if the pole falls return a reward with value -10 (punishment)\n            reward = reward if not done else -10\n\n            nextState = np.reshape(nextState, [1, stateSize])\n\n            #Store the values for the current action and state\n            agent.remember(state, action, reward, nextState, done)\n\n            state = nextState\n\n\n            #After the pole falls or reach the max number of step print the results\n            if done:\n                testing.append(reward)\n                test_scores.append(step)\n                agents_test['testing_trails'].append(episode)\n                test_history['epsilon'].append(agent.epsilon)\n                test_history['alpha'].append(agent.learningRate)\n                print(\"episode: {}\/{}, score: {}, epsilon: {:.2}\".format(episode, numOfEpisodes, step, agent.epsilon))\n                break\n\n        if len(agent.memory) > batchSize:\n            agent.replay(batchSize)\n","8186a079":"import numpy as np\nimport pandas as pd\nimport matplotlib as plt\nimport gym\nimport os\nimport random\nfrom collections import deque\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nimport tensorflow\nfrom tensorflow.keras.optimizers import Adam\n\n#Environment we will train our agent to play\nenv = gym.make(\"CartPole-v0\")\n#Store the size of our space environment\nstateSize = env.observation_space.shape[0]\n#Store the number of possible actions\nactionSize = env.action_space.n\n\n\nbatchSize = 32\n#Number of episodes to train (number of times the agent will repeat playing the game)\nnumOfEpisodes = 500\n#The max number of steps the agent will take in each spisodes\nnumOfSteps = 2000\n\n\n#Directory to store the trained agent\noutputDir = 'output\/cartpole'\nif not os.path.exists(outputDir):\n    os.makedirs(outputDir)\n\n\nclass Agent:\n\n    def __init__(self, stateSize, actionSize):\n        self.stateSize = stateSize\n        self.actionSize = actionSize\n\n        # set hyperparameters\n\n        # Save the last 200 steps\n        self.memory = deque(maxlen=2000)\n\n        # The weight will give for the future rewards if\n        # gamma close to 0 the agent will tend to consider only immediate rewards\n        # else if gamma close to 1 the agent will give future rewards more weights\n        self.gamma = 0.96\n        self.epsilon = 1.0  # Exploration rate\n        self.epsilonDecay = 0.995\n        self.epsilonMin = 0.01 # Stope decreasing epsilon when it reachs this value\n        self.learningRate = 0.001\n        self.model = self._build_model()\n\n    # Buidling our neural network\n    def _build_model(self):\n        model = Sequential()\n\n        model.add(Dense(64, input_dim=self.stateSize, activation='elu'))\n        model.add(Dense(64, activation='elu'))\n        model.add(Dense(32, activation='elu'))\n        model.add(Dense(16, activation='elu'))\n        model.add(Dense(self.actionSize, activation='linear'))\n\n        model.compile(loss='mse', optimizer=Adam(lr=self.learningRate))\n\n        return model\n\n    # Store our outputs in the memory\n    def remember(self, state, action, reward, nextState, done):\n        self.memory.append((state, action, reward, nextState, done))\n\n    # Decide whether to take action randomly our based on best possible action\n    # This step is taken based on epsilon value (exploration rate)\n    # \"The agent will take the action randomly with %(The value of epsilon)\"\n    def act(self, state):\n        if np.random.rand() <= self.epsilon:\n            return random.randrange(self.actionSize)\n        else:\n            return np.argmax(self.model.predict(state)[0])\n\n    def replay(self, batchSize):\n\n        minibatch = random.sample(self.memory, batchSize)\n\n        for state, action, reward, nextState, done in minibatch:\n            target = reward\n            if not done:\n                target = (reward + self.gamma * np.amax(self.model.predict(nextState)[0]))\n            targetF = self.model.predict(state)\n            targetF[0][action] = target\n\n            self.model.fit(state, targetF, epochs=1, verbose=0)\n\n        if self.epsilon > self.epsilonMin:\n            self.epsilon *= self.epsilonDecay\n\n    def load(self, name):\n        self.model.load_weights(name)\n\n    def save(self, name):\n        self.model.save_weights(name)\n\n\n#Create our neural network (Agent)\nagent = Agent(stateSize, actionSize)\n\n#Whethet it fails or reach the best outcome we will change done to true\ndone = False\ntraining = []\nscores = []\nrewards = []\nhistory = {'epsilon':[],'alpha':[]}\nagents = {'training_trails':[]}\nfor episode in range(numOfEpisodes):\n#Reset the environment\n    state = env.reset()\n    state = np.reshape(state, [1, stateSize])\n\n    for step in range(numOfSteps):\n        # render the einvornment every 100 episode to see our progress\n\n        # take an action whether randomly or based on previous experiences\n        action = agent.act(state)\n\n        nextState, reward, done, info = env.step(action)\n        rewards.append(reward)\n        # if the pole falls return a reward with value -10 (punishment)\n        reward = reward if not done else -10\n\n        nextState = np.reshape(nextState, [1, stateSize])\n\n        #Store the values for the current action and state\n        agent.remember(state, action, reward, nextState, done)\n\n        state = nextState\n\n\n        #After the pole falls or reach the max number of step print the results\n        if done:\n            training.append(reward)\n            scores.append(step)\n            agents['training_trails'].append(episode)\n            history['epsilon'].append(agent.epsilon)\n            history['alpha'].append(agent.learningRate)\n            print(\"episode: {}\/{}, score: {}, epsilon: {:.2}\".format(episode, numOfEpisodes, step, agent.epsilon))\n            break\n    \n\n    if len(agent.memory) > batchSize:\n        agent.replay(batchSize)\n\n\n    #Store the value for the agent each 50 episodes\n    if episode % 50 == 0:\n        agent.save(outputDir + \"weights_\" + '{:04d}'.format(episode) + \".hdf5\")","21342d9c":"\nclass dotdict(dict):\n    \"\"\"dot.notation access to dictionary attributes\"\"\"\n    __getattr__ = dict.get\n    __setattr__ = dict.__setitem__\n    __delattr__ = dict.__delitem__\nagents['testing_trails']= len(test_scores)\nagents = dotdict(agents)\nagents.training_trails = None\nagents.training_trails = 500\ndisplay_stats(agents, scores, test_scores, history)","809c887e":"CONCLUSION:\n    \n    All the models are pretty good but need more and more training to solve this environment correctly and also improve the rate. \n    Since It was very time consuming and other issues executed these environments with very few epochs.The more training will give the best accuracy of solving the environment.","85acea82":"COMPARISONS:\n\n   As we can see that Random Agent Learning is far away with the mean, median and std comparing with the RL-Agent. RL-Agent much better working comparing with the random one but needed more epoches to improve the learning. Comparing these with the Deep-Q-Learning model this is quiet more balanced comparing with the avergaes and the moving averages. These are maintainig the balance more comparitively with the other models as per the graph analysis."}}