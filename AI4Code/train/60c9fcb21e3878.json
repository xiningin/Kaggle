{"cell_type":{"c810cedc":"code","1a8c02f4":"code","d443396d":"code","87906cd8":"code","4c0b404b":"code","683c80b5":"code","cf6c9807":"code","548f0d95":"code","f096a62e":"code","2fbf0adc":"code","c4a8a16d":"code","562211ec":"code","e071197b":"code","5ee2167e":"code","5511037a":"code","63ccc8d3":"code","c4498f1d":"code","2fc7ea20":"code","76ce8da9":"code","31c6a421":"code","b803a720":"code","9471277b":"markdown","7ef8c0ee":"markdown","5385c272":"markdown"},"source":{"c810cedc":"# We will use the official tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","1a8c02f4":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input, Dropout, LSTM, Bidirectional, GlobalMaxPooling1D, GlobalAveragePooling1D, concatenate, SpatialDropout1D\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nfrom tensorflow.keras import callbacks\nfrom keras.utils import to_categorical\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.preprocessing import StandardScaler\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nfrom nltk import word_tokenize\nfrom scipy.sparse import hstack, csr_matrix\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import f1_score, precision_score, recall_score, confusion_matrix\nfrom sklearn.metrics import classification_report, confusion_matrix, log_loss\nimport pprint\n\nimport tokenization\n\nimport re\nimport gc\nimport os\nimport time\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nsns.set_style(\"whitegrid\")\nnotebookstart = time.time()\npd.options.display.max_colwidth = 500\n\nprint(\"Tensorflow Version: \", tf.__version__)\nprint(\"TF-Hub version: \", hub.__version__)\nprint(\"Eager mode enabled: \", tf.executing_eagerly())\nprint(\"GPU available: \", tf.test.is_gpu_available())","d443396d":"MAX_LEN = 64\nBATCH_SIZE = 16\nEPOCHS = 10\nSEED = 42\nNROWS = None\nTEXTCOL = \"text\"\nTARGETCOL = \"author\"\nNCLASS = 3","87906cd8":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)\n\ndef text_processing(df, TEXTCOL, sentiment=False):\n    df[TEXTCOL + '_num_words'] = df[TEXTCOL].apply(lambda comment: len(comment.split())) # Count number of Words\n    df[TEXTCOL + '_num_unique_words'] = df[TEXTCOL].apply(lambda comment: len(set(w for w in comment.split())))\n    df[TEXTCOL + '_words_vs_unique'] = df[TEXTCOL+'_num_unique_words'] \/ df[TEXTCOL+'_num_words'] * 100 # Count Unique Words\n    \n    col_names = [TEXTCOL + '_num_words', TEXTCOL + '_num_unique_words', TEXTCOL + '_words_vs_unique']\n    if sentiment:\n        df[TEXTCOL+\"_vader_Compound\"]= df[TEXTCOL].apply(lambda x:SIA.polarity_scores(x)['compound'])\n        col_names.append(TEXTCOL+\"_vader_Compound\")\n\n    return df, col_names\n\ndef build_model(bert_layer, max_len=512, dropout=.2):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n    numeric_inputs = Input(shape=(len(num_cols),), dtype=tf.float32, name=\"numeric_inputs\")\n    \n    # Bert Layer\n    pooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    \n    # Sequence Output\n    sequence_output = SpatialDropout1D(dropout)(sequence_output)\n    sequence_output = Bidirectional(LSTM(128, return_sequences=True))(sequence_output)\n    sequence_output = GlobalAveragePooling1D()(sequence_output)\n    \n    # Pooled Output\n    pooled_output = Dense(36, activation='relu')(pooled_output)\n    \n    # Dense Inputs\n    numeric_x = Dense(512, activation='relu')(numeric_inputs)\n    numeric_x = Dropout(dropout)(numeric_x)\n    numeric_x = Dense(64, activation='relu')(numeric_x)\n    \n    # Concatenate\n    cat = concatenate([\n        pooled_output,\n        sequence_output,\n        numeric_x\n    ])\n    cat = Dropout(dropout)(cat)\n    \n    # Output Layer\n    out = Dense(3, activation='softmax')(cat)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids, numeric_inputs], outputs=out)\n    model.compile(Adam(lr=1e-6), loss='categorical_crossentropy', metrics=['acc'])\n    \n    return model","4c0b404b":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","683c80b5":"train = pd.read_csv(\"..\/input\/spooky-author-identification\/train.zip\", nrows=NROWS)\ntest = pd.read_csv(\"..\/input\/spooky-author-identification\/test.zip\")\ntestdex = test.id\nsubmission = pd.read_csv(\"..\/input\/spooky-author-identification\/sample_submission.zip\")\nsub_cols = submission.columns\n\nprint(\"Train Shape: {} Rows, {} Columns\".format(*train.shape))\nprint(\"Test Shape: {} Rows, {} Columns\".format(*test.shape))\n\nlength_info = [len(x) for x in np.concatenate([train[TEXTCOL].values, test[TEXTCOL].values])]\nprint(\"Train Sequence Length - Mean {:.1f} +\/- {:.1f}, Max {:.1f}, Min {:.1f}\".format(\n    np.mean(length_info), np.std(length_info), np.max(length_info), np.min(length_info)))","cf6c9807":"train.head()","548f0d95":"# Text Processing\nSIA = SentimentIntensityAnalyzer()\ntrain_df, dense_vars = text_processing(train.copy(), TEXTCOL, sentiment=True)\ntest_df, _ = text_processing(test.copy(), TEXTCOL, sentiment=True)","f096a62e":"# TF-IDF\ncount_vectorizer = TfidfVectorizer(\n    analyzer=\"word\",\n    tokenizer=word_tokenize,\n    preprocessor=None,\n    stop_words='english',\n    sublinear_tf=True,\n    ngram_range=(1, 1),\n    max_features=500)    \n\nhash_loc_tfidf = count_vectorizer.fit(train_df[TEXTCOL])\ntfvocab = hash_loc_tfidf.get_feature_names()\nprint(\"Number of TF-IDF Features: {}\".format(len(tfvocab)))\n\ntrain_tfidf = count_vectorizer.transform(train_df[TEXTCOL])\ntest_tfidf = count_vectorizer.transform(test_df[TEXTCOL])\n\n# Normalisation - Standard Scaler\nfor d_i in dense_vars:\n    scaler = StandardScaler()\n    scaler.fit(train_df.loc[:,d_i].values.reshape(-1, 1))\n    train_df.loc[:,d_i] = scaler.transform(train_df.loc[:,d_i].values.reshape(-1, 1))\n    test_df.loc[:,d_i] = scaler.transform(test_df.loc[:,d_i].values.reshape(-1, 1))\n    \n# Sparse Stack\ntrain_num = hstack([csr_matrix(train_df.loc[:,dense_vars].values),train_tfidf]).toarray()\ntest_num = hstack([csr_matrix(test_df.loc[:,dense_vars].values),test_tfidf]).toarray()\nnum_cols = train_df[dense_vars].columns.tolist() + tfvocab","2fbf0adc":"# Bert Pre-Processing\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)\n\ntrain_input_word_ids, train_input_mask, train_segment_ids, train_numeric_inputs = *bert_encode(train[TEXTCOL].values, tokenizer, max_len=MAX_LEN), train_num\ntest_input = (*bert_encode(test[TEXTCOL].values, tokenizer, max_len=MAX_LEN), test_num)\n\nlabel_mapper = {name: i for i,name in enumerate(set(train[TARGETCOL].values))}\nnum_label = np.vectorize(label_mapper.get)(train[TARGETCOL].values)\ntrain_labels = to_categorical(num_label)\n\ndel test, train_num, test_num, train_df, test_df\n_ = gc.collect()","c4a8a16d":"sns.countplot(train[TARGETCOL].values)\nplt.title(\"Spooky Authors\")\nplt.show()","562211ec":"model = build_model(bert_layer, max_len=MAX_LEN)\nmodel.summary()","e071197b":"oof_preds = np.zeros((train_input_word_ids.shape[0], NCLASS))\ntest_preds = np.zeros((testdex.shape[0], NCLASS))\n\nn_splits = 3\nfolds = KFold(n_splits=n_splits, shuffle=True, random_state=SEED)\nplot_metrics = ['loss','acc']\n\nfold_hist = {}\nfor i, (trn_idx, val_idx) in enumerate(folds.split(train_input_word_ids)):\n    modelstart = time.time()\n    model = build_model(bert_layer, max_len=MAX_LEN)\n    \n    es = callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose=1,\n                                 mode='min', baseline=None, restore_best_weights=True)\n    \n    \n    history = model.fit(\n        x=[train_input_word_ids[trn_idx],\n            train_input_mask[trn_idx],\n            train_segment_ids[trn_idx],\n            train_numeric_inputs[trn_idx]],\n        y=train_labels[trn_idx],\n        validation_data=(\n            [train_input_word_ids[val_idx],\n            train_input_mask[val_idx],\n            train_segment_ids[val_idx],\n            train_numeric_inputs[val_idx]],\n            train_labels[val_idx]),\n        epochs=EPOCHS,\n        batch_size=18,\n        callbacks=[es]\n    )\n\n    best_index = np.argmin(history.history['val_loss'])\n    fold_hist[i] = history\n    \n    oof_preds[val_idx] = model.predict(\n        [train_input_word_ids[val_idx],\n        train_input_mask[val_idx],\n        train_segment_ids[val_idx],\n        train_numeric_inputs[val_idx]])\n    test_preds += model.predict(test_input)\n    best_metrics = {metric: scores[best_index] for metric, scores in history.history.items()}\n    pprint.pprint(best_metrics)\n    \n    f, ax = plt.subplots(1,len(plot_metrics),figsize = [12,4])\n    for p_i,metric in enumerate(plot_metrics):\n        ax[p_i].plot(history.history[metric], label='Train ' + metric)\n        ax[p_i].plot(history.history['val_' + metric], label='Val ' + metric)\n        ax[p_i].set_title(\"{} Fold Loss Curve - {}\\nBest Epoch {}\".format(i, metric, best_index))\n        ax[p_i].legend()\n        ax[p_i].axvline(x=best_index, c='black')\n    plt.show()","5ee2167e":"train['error'] = 1 - np.max(((train_labels) - oof_preds), axis = 1)\ntrain = pd.concat([train, pd.DataFrame(oof_preds, columns=label_mapper.keys())], axis=1)\n\n\nf,ax = plt.subplots(1,1,figsize = [6,4])\nsns.distplot(train['error'], ax = ax)\nax.set_title(\"Classification Errors: Target - Pred Probability\")\nplt.tight_layout(pad=1)\nplt.show()","5511037a":"cnf_matrix = confusion_matrix(num_label, np.argmax(oof_preds,axis=1))\nprint(\"Logloss: {:.2f}\".format(log_loss(train_labels, oof_preds)))\n\nprint(\"\\nConfusion Matrix:\")\nprint(cnf_matrix)","63ccc8d3":"show_cols = [\n    'id',\n    TEXTCOL,\n    TARGETCOL,\n    'error'] + list(label_mapper.keys())\n\ndisplay(train[show_cols].sort_values(by = 'error', ascending=True).iloc[:20])","c4498f1d":"final_pred = test_preds\/n_splits\npd.DataFrame(final_pred).describe()","2fc7ea20":"submission = pd.DataFrame(final_pred, columns=label_mapper.keys())\nsubmission['id'] = testdex\n\nsubmission = submission[sub_cols]\nsubmission.to_csv('submission_bert.csv', index=False)\nprint(submission.shape)","76ce8da9":"!head submission_bert.csv","31c6a421":"oof_pd = pd.DataFrame(oof_preds\/n_splits, columns = label_mapper.keys())\noof_pd.to_csv(\"oof_dense_bert.csv\")\nprint(oof_pd.shape)","b803a720":"print(\"Notebook Runtime: %0.2f Minutes\"%((time.time() - notebookstart)\/60))","9471277b":"# BERT for Spooky Author Identification\n_By Nick Brooks_\n\n### **References:**\n- Source for `bert_encode` function: https:\/\/www.kaggle.com\/user123454321\/bert-starter-inference\n- All pre-trained BERT models from Tensorflow Hub: https:\/\/tfhub.dev\/s?q=bert\n- TF Hub Documentation for Bert Model: https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1","7ef8c0ee":"# Model: Build, Train, Predict, Submit","5385c272":"# Helper Functions"}}