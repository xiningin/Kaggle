{"cell_type":{"1ee99e26":"code","f7ea7d63":"code","625d39cb":"code","36ef9ac1":"code","fc9d387b":"code","bb53a163":"code","70a2dd51":"code","c7678331":"code","bdde0f8c":"code","f6cfcf8f":"code","01292dc4":"code","7bbdcf21":"code","a65ae850":"code","00d0d254":"code","abd6d35d":"code","93469a6c":"code","ecd255ef":"code","3194ac8b":"code","d5e149f2":"code","c0b94d25":"code","ea7ef442":"code","6676e36b":"code","97287a3e":"code","a24f6e1f":"code","cf69a34f":"code","b6c7d9f6":"code","6507be41":"code","d8df3b8e":"code","2fcf059d":"code","464ee8fa":"code","d2cb560c":"code","a72291bc":"code","2afb5b72":"code","3ec8daf0":"code","cddffd1d":"code","1aeda15a":"code","73833974":"code","8bac394e":"code","3969743c":"code","6aad5ad3":"code","426963c1":"code","c3623bf7":"code","5c515284":"code","3be51899":"code","8e3ef21b":"code","0942c01c":"code","55f79af6":"code","36eb833f":"code","d050119f":"code","0baf34de":"code","41a6df4e":"markdown","ff23e3f9":"markdown","9ee73c85":"markdown","7de585b8":"markdown","e04e96b3":"markdown","0daa045d":"markdown","9f74205b":"markdown","c799725b":"markdown","8ccc968e":"markdown","24dff0e2":"markdown","6484914b":"markdown","5b938e05":"markdown","586fa005":"markdown","796de70d":"markdown","24b669f0":"markdown","35cda1cd":"markdown","4e7b574e":"markdown","2f6f8f56":"markdown","9ce38e24":"markdown","98048d49":"markdown","2213a61d":"markdown","a05693e2":"markdown","f859f7a3":"markdown","9f1eadb3":"markdown","0b90ce0b":"markdown","0d506bec":"markdown","e867320c":"markdown"},"source":{"1ee99e26":"#import basic libraries\nimport pandas as pd\nimport numpy as np\nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f7ea7d63":"#loading the train data\ntrain = pd.read_csv(\"..\/input\/novartis-data\/Train.csv\")","625d39cb":"train.info()","36ef9ac1":"#loading the test data\ntest = pd.read_csv(\"..\/input\/novartis-data\/Test.csv\")","fc9d387b":"test.info()","bb53a163":"#Dropping the Incident_ID and Date from train data\ntrain = train.drop(['INCIDENT_ID','DATE'], axis=1)","70a2dd51":"#Verifying all the columns that has the null values in train data\nnull_columns=train.columns[train.isnull().any()]\ntrain[null_columns].isnull().sum()","c7678331":"#Filled NaN values with \"0\" using fillna()\ntrain[\"X_12\"].fillna(0,inplace = True)","bdde0f8c":"#Verifying all the columns that has the null values in test data\nnull_columns=test.columns[test.isnull().any()]\ntest[null_columns].isnull().sum()","f6cfcf8f":"#Filled NaN values with \"0\" using fillna()\ntest[\"X_12\"].fillna(0,inplace = True)\ntest.isnull().sum()","01292dc4":"train[\"X_12\"] = train[\"X_12\"].astype(np.int64)\ntrain.info()","7bbdcf21":"test[\"X_12\"] = test[\"X_12\"].astype(np.int64)\ntest.info()","a65ae850":"#Removing the duplicated rows from train data\nprint(\"Shape of train before removing the duplicates :\" , train.shape)\ntrain.drop_duplicates(keep='first', inplace=True)\nprint(\"Shape of train After removing the duplicates :\" , train.shape)","00d0d254":"#Skewness of the train data\ntrain.skew()","abd6d35d":"f,ax = plt.subplots(1,1,figsize=(15,5))\nsns.violinplot(train['MULTIPLE_OFFENSE'])\nplt.show()\n#skewness and kurtosis\nprint(\"Skewness: {}\".format(train['MULTIPLE_OFFENSE'].skew()))\nprint(\"Kurtosis: {}\".format(train['MULTIPLE_OFFENSE'].kurt()))","93469a6c":"train.hist(figsize=(15,15))\nplt.show()","ecd255ef":"sns.set(style=\"ticks\", color_codes=True)\nsns.pairplot(train)\nplt.show()","3194ac8b":"sns.set(style=\"ticks\", color_codes=True)\nsns.pairplot(train,kind=\"reg\")\nplt.show()","d5e149f2":"sns.heatmap(train.corr(),annot=True, cmap='BuPu', linewidths=0.1)\nfig=plt.gcf()\nfig.set_size_inches(14,14)\nplt.show()","c0b94d25":"print(\"Number of training Mutiple Offence : {} \".format(len(train)))\nprint(\"Offense Rate {:.4}%\".format(train[\"MULTIPLE_OFFENSE\"].mean()*100))","ea7ef442":"#Creating Pie Chart for the target variable\nlabels = ['Hacked', 'Genuine']\nplt.title('Multiple Offense')\ntrain['MULTIPLE_OFFENSE'].value_counts().plot.pie(explode=[0,0.3],autopct='%1.2f%%',shadow=True,labels=labels,fontsize=15)","6676e36b":"test.hist(figsize=(15,15))\nplt.show()","97287a3e":"sns.set(style=\"ticks\", color_codes=True)\nsns.pairplot(test)\nplt.show()","a24f6e1f":"sns.set(style=\"ticks\", color_codes=True)\nsns.pairplot(test,kind=\"reg\")\nplt.show()","cf69a34f":"train=train.drop(['X_7','X_9','X_12','X_14'], axis=1)\ntest=test.drop(['X_7','X_9','X_12','X_14'], axis=1)","b6c7d9f6":"X_train = train.iloc[:,:-1]\ny_train = train[\"MULTIPLE_OFFENSE\"]\n#Dropping the Incident_ID and Date from test data\nX_test = test.drop(['INCIDENT_ID','DATE'], axis=1)","6507be41":"print(\"Shape of X_train : \",X_train.shape)\nprint(\"Shape of y_train : \",y_train.shape)\nprint(\"Shape of X_test : \",X_test.shape)","d8df3b8e":"print('Before OverSampling, the shape of X_train: {}'.format(X_train.shape))\nprint('Before OverSampling, the shape of y_train: {} \\n'.format(y_train.shape))\nprint(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))","2fcf059d":"from imblearn.over_sampling import SMOTE\nsampler = SMOTE(sampling_strategy='minority')\nX_train_sm, y_train_sm = sampler.fit_sample(X_train, y_train)\nprint('After OverSampling, the shape of X_train: {}'.format(X_train_sm.shape))\nprint('After OverSampling, the shape of y_train: {} \\n'.format(y_train_sm.shape))\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_sm==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_sm==0)))","464ee8fa":"#Spltting the data into train and validation\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X_train_sm, y_train_sm ,test_size=0.3, random_state=100)","d2cb560c":"#Scaling the data\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()\nX_train_ss = ss.fit_transform(X_train)\nX_test_ss  = ss.transform(X_test)\nX_val_ss   = ss.transform(X_val)","a72291bc":"#Decision Tree Classifier\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\n\ndecisiontree = DecisionTreeClassifier(random_state=0)\ndecisiontree.fit(X_train_ss, y_train)\n\ny_pred = decisiontree.predict(X_val_ss)\n\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nrecall_decisiontree = recall_score(y_pred, y_val)\n\nprint(\"Decision Tree Accuracy Score:\",acc_decisiontree)\nprint('Decision Tree Recall Score:',recall_decisiontree)","2afb5b72":"from sklearn.model_selection import cross_val_score\n\ncross_val_score(decisiontree, X_train_ss, y_train, cv=10)","3ec8daf0":"#Gradient Boosting Classifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\n\ngbc = GradientBoostingClassifier(n_estimators=1000,learning_rate=1.0, random_state=100)\ngbc.fit(X_train_ss, y_train)\n\ny_pred = gbc.predict(X_val_ss)\n\nacc_gbc = round(accuracy_score(y_pred, y_val) * 100, 2)\nrecall_gbc = recall_score(y_pred, y_val)\n\nprint(\"Gradient Boosting Classifier Accuracy Score:\",acc_gbc)\nprint(\"Gradient Boosting Classifier Recall Score:\",recall_gbc)","cddffd1d":"from sklearn.model_selection import cross_val_score\n\nCV = cross_val_score(gbc, X_train_ss, y_train, cv=5)\nprint(CV.mean())","1aeda15a":"#AdaBoost Classifier\n\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import recall_score\n\nabc = AdaBoostClassifier(n_estimators=1000, random_state=1000,learning_rate=0.3)\nabc.fit(X_train_ss, y_train)\n\ny_pred = abc.predict(X_val_ss)\n\nacc_abc = round(accuracy_score(y_pred, y_val) * 100, 2)\nrecall_abc = recall_score(y_pred, y_val)\n\nprint(\"AdaBoost Classifier Accuracy Score:\",acc_abc)\nprint(\"AdaBoost Classifier Recall Score:\",recall_abc)","73833974":"cat_features = list(range(0, X_train_sm.shape[1]))\nprint(cat_features)","8bac394e":"# Catboost Classifier\n\n#Just to check if the model is working on the data properly or not\n\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_val, y_train, y_val = train_test_split(X_train_sm, y_train_sm, test_size=0.05, random_state=42)\n\nclf = CatBoostClassifier(\n    iterations=5, \n    learning_rate=0.1, \n    #loss_function='CrossEntropy'\n)\n\nclf.fit(X_train, y_train, \n        cat_features=cat_features, \n        eval_set=(X_val, y_val), \n        verbose=False\n)\n\nprint('CatBoost model is fitted: ' + str(clf.is_fitted()))\nprint('CatBoost model parameters:')\nprint(clf.get_params())","3969743c":"#Model Training\n\nclf = CatBoostClassifier(\n    iterations=1000,\n    random_seed=42,\n    depth=10,\n    learning_rate=0.1,\n    model_size_reg=0\n)\n\nclf.fit(\n    X_train, y_train,\n    cat_features=cat_features,\n    eval_set=(X_val, y_val),\n)\n","6aad5ad3":"print(clf.predict_proba(data=X_val))","426963c1":"print(clf.predict(data=X_val))","c3623bf7":"#!pip install automl","5c515284":"#import h2o\n#from h2o.automl import H2OAutoML","3be51899":"\"\"\"\nh2o.init()\nX_y_train_h = h2o.H2OFrame(pd.concat([X_train_sm, y_train_sm], axis='columns'))\nX_y_train_h['MULTIPLE_OFFENSE'] = X_y_train_h['MULTIPLE_OFFENSE'].asfactor()\n# ^ the target column should have categorical type for classification tasks\n#   (numerical type for regression tasks)\n\nX_test_h = h2o.H2OFrame(X_test)\n\nX_y_train_h.describe()\n\"\"\"","8e3ef21b":"\"\"\"\naml = H2OAutoML(\n    max_runtime_secs=(3600 * 8),  # 8 hours\n    max_models=None,  # no limit\n    seed=44\n)\n\"\"\"","0942c01c":"#feature_cols = ['X_1','X_2','X_3','X_4','X_5','X_6','X_7','X_8','X_9','X_10','X_11','X_12','X_14','X_14','X_15']","55f79af6":"\"\"\"\n%%time\n\naml.train(\n    x=feature_cols,\n    y='MULTIPLE_OFFENSE',\n    training_frame=X_y_train_h\n)\n\nlb = aml.leaderboard\nmodel_ids = list(lb['model_id'].as_data_frame().iloc[:,0])\nout_path = \".\"\n\nfor m_id in model_ids:\n    mdl = h2o.get_model(m_id)\n    h2o.save_model(model=mdl, path=out_path, force=True)\n\nh2o.export_file(lb, os.path.join(out_path, 'aml_leaderboard.h2o'), force=True)\n\"\"\"","36eb833f":"#lb = h2o.automl.get_leaderboard(aml, extra_columns = 'ALL')\n#lb","d050119f":"#y_pred = aml.leader.predict(X_test_h)","0baf34de":"y_pred=clf.predict(data=X_test)\n\nsubmission_df = pd.DataFrame({'INCIDENT_ID':test['INCIDENT_ID'], 'MULTIPLE_OFFENSE':y_pred})\nsubmission_df.to_csv('Submission CBC.csv', index=False)","41a6df4e":"# Approach That I took:\n\n1. Initially I tried using Binary Classification using keras but in that case The score was almost 55\n2. After that I tried out Decision Tree Classifier, Gradient Boosting Classification & Adaboost Classification. From these cases I saw that GBC is working best so I tried that out and got a score of 99.48\n3. After some fine tuning in GBC I got the score of 99.52 and after that whatever I do, It was not improving\n4. I tried out AutoMl after that. It took almost 8 Hours to train and got a score of 99.58\n5. After that I came to know about Catboost classification so tried that out and boom!! I got a score of 99.99! But in this case even after finetuning the score was not improving\n6. As the score was not improving I went to the EDA section again and figured out that 'X_7', 'X_9', 'X_12' & 'X_14' are reducing the accuracy. So after removing those and Training on CBC I got the score of 100.00","ff23e3f9":"# Hope this notebook was helpful","9ee73c85":"### Heatmap","7de585b8":"SMOTE stands for Synthetic Minority Oversampling Technique. This is a statistical technique for increasing the number of cases in your dataset in a balanced way. The module works by generating new instances from existing minority cases that you supply as input. This implementation of SMOTE does not change the number of majority cases.                              \n\nTo read further about SMOTE: [Link](https:\/\/docs.microsoft.com\/en-us\/azure\/machine-learning\/studio-module-reference\/smote) \n","e04e96b3":"## GradientBoosting","0daa045d":"### Pair Plot:","9f74205b":"From the above plots we can see that 'X_7', 'X_9', 'X_12' & 'X_14' will reduce the acuracy. So let's drop those columns","c799725b":"### Histogram Plot:","8ccc968e":"### Let's visualize the Multiple Offense rate using pie chart","24dff0e2":"We can view in the train and test info that X_12 is float64 let's convert it into int64","6484914b":"### Checking Skewness & Kurtosis:","5b938e05":"To run the automl for me it took almost 8 hours! So I'm commenting this part for now. To get the result in Automl you can remove the commenting and run!","586fa005":"## Automl","796de70d":"In the above output we can cleary see that the target variable is well balanced.","24b669f0":"# Data Analysis on Train data :","35cda1cd":"## AdaBoost","4e7b574e":"### Histogram Plot:","2f6f8f56":"## DecisionTree","9ce38e24":"# Submission","98048d49":"### Pair Plot:","2213a61d":"## CatBoost","a05693e2":"# Basic EDA","f859f7a3":"# DATA PROCESSING FOR PREDICTION :","9f1eadb3":"# Novartis Data science challenge :\n\nTo predict whether the server is hacked or not.","0b90ce0b":"# Data Analysis on Test","0d506bec":"# DATA MODELLING FOR PREDICTION :","e867320c":"# SMOTE:\n"}}