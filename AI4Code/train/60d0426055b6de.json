{"cell_type":{"e45cb46a":"code","130b1561":"code","1fbc9cb6":"code","0bdd9733":"code","2a2c0e0c":"code","7221ea6f":"code","5b7fafca":"code","bc37196c":"code","b3541893":"code","50ecca1e":"code","dc7b5194":"code","16cda870":"code","63c0ced7":"code","2f3cc7c4":"code","6e475910":"code","611cf401":"code","6f67c428":"code","1d9c8eaa":"markdown","46dd3391":"markdown","8a2a4098":"markdown","5234c3fc":"markdown","95f8b0a7":"markdown","d7b2c28d":"markdown","ebca1861":"markdown","801b2600":"markdown","b1648013":"markdown"},"source":{"e45cb46a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport math\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","130b1561":"train_data = pd.read_csv('..\/input\/learn-together\/train.csv', index_col='Id')\ntest_data =  pd.read_csv('..\/input\/learn-together\/test.csv', index_col='Id')","1fbc9cb6":"df_train = train_data.copy()\ndf_test = test_data.copy()\n\nprint ('number of rows and columns', df_train.shape)\nprint ('number of rows and columns', df_test.shape)\n\n#print (df_train.describe())\ndata_col = df_train.columns\n\ndata_col","0bdd9733":"#df_train.dropna(axis=0, subset=['Cover_Type'], inplace=True)\ny = df_train.Cover_Type\ndf_train.drop(['Cover_Type'], axis=1, inplace=True)\n\nprint (df_train.shape)","2a2c0e0c":"total_row = df_train.shape[0]            \nmissing_val_count_by_column = (df_train.isnull().sum())\nna_col = missing_val_count_by_column[missing_val_count_by_column > 0]\/total_row\n\nprint(na_col)","7221ea6f":"col = df_train.columns.tolist()\n\nnumcol =col[0:10]\nbincol = col[10:]\n\nprint (numcol)\nprint (bincol)","5b7fafca":"\nDIMS=(16, 15)\n\ndef drawdistplot(n, df, bins):\n    fig = plt.figure(figsize=DIMS)\n    drow = math.ceil(n\/2)\n    for i in range(n):\n        fig.tight_layout()\n        ax = fig.add_subplot(drow, 2,i+1)\n        sns.distplot(df.iloc[:, i],kde=False, bins = bins)\n        ax.set_title(df.columns[i])\n    plt.show()","bc37196c":"numx = len(numcol)\nnumdf = df_train[numcol]\n\ndrawdistplot(numx,numdf,20)","b3541893":"\ndef drawbar(n, df):\n    fig = plt.figure(figsize=DIMS)\n    drow = math.ceil(n\/2)\n    for i in range(n):\n        s = df.iloc[:, i].groupby(df.iloc[:, i]).size()\n        #fig.tight_layout()\n        ax = fig.add_subplot(drow, 2,i+1)\n        s.plot.bar()\n        ax.set_title(df.columns[i], fontsize = 12)\n    plt.show()\n    \n\n        \n    \nbinx = len(bincol)\nbindf=df_train[bincol]\ndrawbar(binx, bindf)\n","50ecca1e":"from sklearn.model_selection import train_test_split\nX_train, X_valid, y_train, y_valid = train_test_split(df_train, y, \n                                                                train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n","dc7b5194":"from sklearn.ensemble import RandomForestClassifier\n\n#Create a Gaussian Classifier\nclf=RandomForestClassifier(n_estimators=100)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\n\ny_pred=clf.predict(X_valid)\n\nfrom sklearn import metrics\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_valid, y_pred))","16cda870":"# Retrieve a list of columns that have more than 99% for 1 value\n\ndef showpro(n, df):\n    col_list =[]\n    for i in range(n):\n        s = df.iloc[:, i].groupby(df.iloc[:, i]).size()\/len(df)*100\n        if (s>99.5).any():\n            print (s)\n            print ('-----------')\n        else:\n            col_list.append(s.name)\n    return col_list","63c0ced7":"numcol.remove('Hillshade_9am')\nnumcol.remove('Hillshade_Noon')\n\n\nrc = showpro(binx, bindf)\nrc.extend(numcol)\nprint (rc)","2f3cc7c4":"X_train= X_train[rc]\nX_valid= X_valid[rc]","6e475910":"print (X_train.shape)\nprint (X_valid.shape)\n\n","611cf401":"from sklearn.ensemble import RandomForestClassifier\n\n#Create a Gaussian Classifier\nclf=RandomForestClassifier(n_estimators=100)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\nclf.fit(X_train,y_train)\n\ny_pred=clf.predict(X_valid)\n\nfrom sklearn import metrics\n# Model Accuracy, how often is the classifier correct?\nprint(\"Accuracy:\",metrics.accuracy_score(y_valid, y_pred))","6f67c428":"for i in range(100, 700, 100):\n    clf=RandomForestClassifier(n_estimators=i)\n\n#Train the model using the training sets y_pred=clf.predict(X_test)\n    clf.fit(X_train,y_train)\n\n    y_pred=clf.predict(X_valid)\n\n\n# Model Accuracy, how often is the classifier correct?\n    print(\"Accuracy:\", i ,metrics.accuracy_score(y_valid, y_pred))","1d9c8eaa":"- There are no missing values\n- All columns are numeric values","46dd3391":"- Spilt data in to training and testing data","8a2a4098":"- Extract Cover_Type as single column y","5234c3fc":"- Random Forest Modeling using ALL DATA COLUMNS","95f8b0a7":"Separate Binary and Numeric Columns","d7b2c28d":"Random Forest","ebca1861":"- Distribution of the numeric columns","801b2600":"- Bar Chart for Ordinal Data Columns","b1648013":"- Get the proportion of the binary values in each column"}}