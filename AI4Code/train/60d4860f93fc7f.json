{"cell_type":{"56e752c1":"code","83f5dd78":"code","04582bfc":"code","f9274d38":"code","72af5c29":"code","f831cb8a":"code","928239b9":"code","600ace78":"markdown","cd9c8125":"markdown","5db09a8a":"markdown","30b25450":"markdown","367f6b5d":"markdown","98eae071":"markdown","27204f1e":"markdown"},"source":{"56e752c1":"from sklearn.model_selection import KFold\nimport plotly.graph_objects as go\nimport tensorflow as tf\nimport transformers\nimport numpy as np\nimport logging\nimport typing","83f5dd78":"def hardware_config() -> tuple:\n    \"\"\"Return strategy and batch size according to hardware state\"\"\"\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        batch_size = 16 * strategy.num_replicas_in_sync\n    except Exception:\n        tpu = None\n        strategy = tf.distribute.get_strategy()\n        batch_size = 4\n\n    return strategy, tpu, batch_size\n\n\nBASE_MODEL = \"roberta-base\"\nSEQ_LEN = 1024\nSEED = 42\n\nN_FOLDS = 5\nUSED_FOLDS = [0]\nLRS = [0.25e-4, 0.25e-4, 0.25e-4, 0.25e-4, 0.25e-5]\nEPOCHS = 5\nVERBOSE = 2\nLABEL_MAP = {\n    \"Lead\": 0,\n    \"Position\": 1,\n    \"Evidence\": 2,\n    \"Claim\": 3,\n    \"Concluding Statement\": 4,\n    \"Counterclaim\": 5,\n    \"Rebuttal\": 6,\n}\nSTRATEGY, TPU, BATCH_SIZE = hardware_config()","04582bfc":"print(\"TensorFlow\", tf.__version__)\n\nif TPU is not None:\n    print(\"Using TPU v3-8\")\nelse:\n    print(\"Using GPU (CPU)\")\n    \nprint(\"Batch size:\", BATCH_SIZE)","f9274d38":"def get_dataset(\n    input_ids: np.array,\n    attention_mask: np.array,\n    labels: typing.Optional[np.array] = None,\n    ordered: bool = False,\n    repeated: bool = False,\n) -> tf.data.Dataset:\n    \"\"\"Return batched and prefetched dataset\"\"\"\n    if labels is not None:\n        dataset = tf.data.Dataset.from_tensor_slices(\n            ({\"input_ids\": input_ids, \"attention_mask\": attention_mask}, labels)\n        )\n    else:\n        dataset = tf.data.Dataset.from_tensor_slices(\n            {\"input_ids\": input_ids, \"attention_mask\": attention_mask}\n        )\n\n    if repeated:\n        dataset = dataset.repeat()\n    if not ordered:\n        dataset = dataset.shuffle(1024)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(tf.data.AUTOTUNE)\n\n    return dataset\n\n\ndef get_model() -> tf.keras.Model:\n    \"\"\"Return compiled instance of Keras model\"\"\"\n    backbone = transformers.TFRobertaForTokenClassification.from_pretrained(BASE_MODEL)\n\n    input_ids = tf.keras.layers.Input(\n        shape=(SEQ_LEN,),\n        dtype=tf.int32,\n        name=\"input_ids\",\n    )\n    attention_mask = tf.keras.layers.Input(\n        shape=(SEQ_LEN,),\n        dtype=tf.int32,\n        name=\"attention_mask\",\n    )\n    x = backbone(\n        {\n            \"input_ids\": input_ids,\n            \"attention_mask\": attention_mask,\n        },\n    )\n    x = tf.keras.layers.Dense(256, activation=\"relu\")(x[0])\n    outputs = tf.keras.layers.Dense(\n        2 * len(LABEL_MAP) + 1, activation=\"softmax\", dtype=\"float32\"\n    )(x)\n\n    model = tf.keras.Model(\n        inputs=[input_ids, attention_mask],\n        outputs=outputs,\n    )\n    model.compile(\n        optimizer=tf.keras.optimizers.Adam(),\n        loss=tf.keras.losses.CategoricalCrossentropy(),\n        metrics=[tf.keras.metrics.CategoricalAccuracy()],\n    )\n    return model","72af5c29":"input_ids = np.load(\"..\/input\/feedback-prize-roberta-tokens-1024\/input_ids.npy\")\nattention_mask = np.load(\"..\/input\/feedback-prize-roberta-tokens-1024\/attention_mask.npy\")\nlabels = np.load(\"..\/input\/feedback-prize-roberta-tokens-1024\/labels.npy\")\n\nprint(\"Input ids shape:\", input_ids.shape)\nprint(\"Attention_mask shape:\", attention_mask.shape)\nprint(\"Labels shape:\", labels.shape)","f831cb8a":"kfold = KFold(n_splits=N_FOLDS, shuffle=True, random_state=42)\ncallbacks = [tf.keras.callbacks.LearningRateScheduler(lambda x: LRS[x], verbose=1)]\n\nhistory = [None] * N_FOLDS\nscores = [None] * N_FOLDS\n\nfor i, (train_index, val_index) in enumerate(kfold.split(input_ids)):\n    if i not in USED_FOLDS:\n        continue\n\n    if TPU is not None:\n        tf.tpu.experimental.initialize_tpu_system(TPU)\n\n    with STRATEGY.scope():\n        model = get_model()\n\n    train_dataset = get_dataset(\n        input_ids=input_ids[train_index],\n        attention_mask=attention_mask[train_index],\n        labels=labels[train_index],\n        repeated=True,\n    )\n    val_dataset = get_dataset(\n        input_ids=input_ids[val_index],\n        attention_mask=attention_mask[val_index],\n        labels=labels[val_index],\n        ordered=True,\n    )\n\n    steps_per_epoch = len(train_index) \/\/ BATCH_SIZE\n    validation_steps = len(val_index) \/\/ BATCH_SIZE\n\n    history[i] = model.fit(\n        train_dataset,\n        validation_data=val_dataset,\n        steps_per_epoch=steps_per_epoch,\n        callbacks=callbacks,\n        epochs=EPOCHS,\n        verbose=VERBOSE,\n    ).history\n    scores[i] = max(history[i][\"val_categorical_accuracy\"])\n\n    model.save_weights(f\"model_{i}.h5\")    ","928239b9":"for i in range(N_FOLDS):\n    if history[i] is None:\n        continue\n\n    go.Figure(\n        data=(\n            go.Scatter(\n                y=history[i][\"categorical_accuracy\"],\n                name=\"train\",\n            ),\n            go.Scatter(\n                y=history[i][\"val_categorical_accuracy\"],\n                name=\"validation\",\n            ),\n        ),\n        layout=dict(\n            height=400,\n            width=600,\n            margin=dict(t=75, b=25),\n            title_text=f\"Fold {i} (Best score {scores[i]:.4f})\",\n            xaxis=dict(title_text=\"Epoch\", dtick=1),\n            yaxis=dict(title_text=\"Categorical accuracy\"),\n        ),\n    ).show()","600ace78":"Conclusion\n----------\n\nI am in no way good at NLP, so feel free to correct me if you feel so.\n\nThanks for reading.","cd9c8125":"Utilities\n---------\n\nTreat it simply as a black box unless you want to discover it in-depth.","5db09a8a":"Load data\n---------\n\nHere we load tokens prepared in [this notebook][1].\n\nWe have 7 statement types: lead, position, evidence, claim, counterclaim, rebuttal, and concluding statement. This results in 15 possible classes: the first word in each of the above statements (7 classes), the non-first word in each of the above statements (7 classes), and none of above (1 class).\n\n[1]: https:\/\/www.kaggle.com\/nickuzmenkov\/feedback-prize-making-roberta-tokens","30b25450":"Introduction\n------------\n\nThis notebook intends to speed up the training part as much as possible leaving you room for improvements.\n\nIt is based on this\u00a0[amazing work][1]\u00a0by\u00a0[Chris Deotte][2]. I left only the training loop and moved tokenizing part to a\u00a0[separate notebook][3]. I also had to change the original backbone Longformer to RoBERTa, because of TensorFlow TPU-related errors I had been unable to fix (if you know how to fix them, please, let me know in the comments below).\n\nAs the title says, it takes only 10 minutes to train the RoBERTa model\u2014single fold, 5 epochs\u2014which is 30 times faster compared to the original work.\n\nImports\n-------\n\n[1]: https:\/\/www.kaggle.com\/cdeotte\/tensorflow-longformer-ner-cv-0-633\n[2]: https:\/\/www.kaggle.com\/cdeotte\n[3]: https:\/\/www.kaggle.com\/nickuzmenkov\/feedback-prize-making-roberta-tokens","367f6b5d":"Run training\n------------","98eae071":"Results\n-------","27204f1e":"Configuration\n-------------"}}