{"cell_type":{"a41c1d13":"code","10bad112":"code","f01ece11":"code","4897b444":"code","935d6124":"code","041bb5dd":"code","0f102422":"code","525f0a05":"code","98a21fde":"code","0eef1f76":"code","83b92858":"code","ef65a2b4":"code","12a78caf":"code","771e5663":"code","574e2fc3":"code","0151a0b3":"code","902246d3":"code","51385836":"code","674cb8ca":"code","8991879f":"code","34b2d5cc":"markdown","f4dfb959":"markdown","7a52746b":"markdown","2b6697e9":"markdown","83e4a6a2":"markdown","9695c0fa":"markdown"},"source":{"a41c1d13":"import numpy as np\nimport pandas as pd\nfrom datetime import datetime, timedelta\nimport lightgbm as lgb\nfrom tqdm import tqdm\nimport gc\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","10bad112":"train = pd.read_csv(\"..\/input\/janatahack-demand-forecasting-analytics-vidhya\/train.csv\")\ntest = pd.read_csv(\"..\/input\/janatahack-demand-forecasting-analytics-vidhya\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/janatahack-demand-forecasting-analytics-vidhya\/sample_submission.csv\")\n\nprint(\"train shape: {}\".format(train.shape))\nprint(\"test shape: {}\".format(test.shape))\nprint(\"submission shape: {}\".format(submission.shape))","f01ece11":"train.groupby(['store_id', 'sku_id']).size().reset_index(name=\"Count\")","4897b444":"test.groupby(['store_id', 'sku_id']).size().reset_index(name=\"Count\")","935d6124":"fday = datetime(2013, 7, 16)\nfday","041bb5dd":"train_test_df = train.append(test, ignore_index=True)\ntrain_test_df.tail()","0f102422":"def data_preprocessing(df):\n\n    df['week'] = pd.to_datetime(df['week'], dayfirst=True)\n    df['base_total_ratio'] = df['base_price']\/df['total_price']\n    df['total_base_ratio'] = df['total_price']\/df['base_price']\n    df['is_featured_displayed'] = df['is_featured_sku'] * df['is_display_sku']\n    numcols = ['total_price', 'base_price', 'base_total_ratio', 'total_base_ratio','is_featured_sku', 'is_display_sku', 'is_featured_displayed','units_sold']\n    \n    df['store_sku_id'] = df['store_id'].astype(str) + '_' + df['sku_id'].astype(str)\n    catcols = ['store_sku_id', 'store_id', 'sku_id']\n    \n    for col in catcols:\n        if col != \"store_sku_id\":\n            df[col] = df[col].astype('category')\n            df[col] = df[col].cat.codes.astype(\"int16\")\n            df[col] -= df[col].min()\n    \n    return df","525f0a05":"def create_features(df):\n    lags = [12]\n    lag_cols = [f\"lag_{lag}\" for lag in lags ]\n    for lag, lag_col in tqdm(zip(lags, lag_cols), total = len(lags)):\n        df[lag_col] = df[[\"store_sku_id\",\"units_sold\"]].groupby(\"store_sku_id\")[\"units_sold\"].shift(lag)\n\n    wins = [6, 12, 26, 52]\n    for win in tqdm(wins):\n        for lag,lag_col in tqdm(zip(lags, lag_cols), total = len(lags)):\n            df[f\"rmean_{lag}_{win}\"] = df[[\"store_sku_id\", lag_col]].groupby(\"store_sku_id\")[lag_col].transform(lambda x : x.rolling(win).mean())\n    \n    date_features = {\n        \n        \"weekday\": \"weekday\",\n        \"weekofyear\": \"weekofyear\",\n        \"month\": \"month\",\n        \"quarter\": \"quarter\",\n        \"year\": \"year\",\n        \"mday\": \"day\",\n        \"ime\": \"is_month_end\",\n        \"ims\": \"is_month_start\",\n    }\n    \n    for date_feat_name, date_feat_func in date_features.items():\n        if date_feat_name in df.columns:\n            df[date_feat_name] = df[date_feat_name].astype(\"int16\")\n        else:\n            df[date_feat_name] = getattr(df[\"week\"].dt, date_feat_func).astype(\"int16\")","98a21fde":"def rmsle_lgbm(y_pred, data):\n\n    y_true = np.array(data.get_label())\n    score = np.sqrt(np.mean(np.power(np.log1p(y_true) - np.log1p(y_pred), 2)))\n\n    return 'rmsle', score, False","0eef1f76":"train_test_df = data_preprocessing(train_test_df)","83b92858":"%time\n\ncreate_features(train_test_df)\ntrain_test_df.shape","ef65a2b4":"train_df = train_test_df[train_test_df['units_sold'].isnull() != True]\ntest_df = train_test_df[train_test_df['units_sold'].isnull() == True]","12a78caf":"train_df.dropna(inplace = True)\ntrain_df.shape","771e5663":"categorical_features = ['store_id', 'sku_id', 'is_featured_sku', 'is_display_sku', 'is_featured_displayed']\nuseless_cols = ['record_ID', 'week', 'store_sku_id', 'units_sold']\ntrain_cols = train_df.columns[~train_df.columns.isin(useless_cols)]\nX_train = train_df[train_cols]\ny_train = train_df['units_sold']","574e2fc3":"%%time\n\nnp.random.seed(42)\n\nvalid_inds = np.random.choice(X_train.index.values, 5000, replace = False)\ntrain_inds = np.setdiff1d(X_train.index.values, valid_inds)\n\ntrain_data = lgb.Dataset(X_train.loc[train_inds] , label = y_train.loc[train_inds], \n                         categorical_feature=categorical_features, free_raw_data=False)\nvalid_data = lgb.Dataset(X_train.loc[valid_inds], label = y_train.loc[valid_inds],\n                         categorical_feature=categorical_features, free_raw_data=False) # This is a random sample, we're not gonna apply any time series train-test-split tricks here!","0151a0b3":"params = {\n        \"objective\" : \"tweedie\",\n        'tweedie_variance_power': 1.1,\n#         \"metric\" :\"rmse\",\n        \"force_row_wise\" : True,\n        \"learning_rate\" : 0.07,\n#         \"sub_feature\" : 0.8,\n        \"sub_row\" : 0.75,\n        \"bagging_freq\" : 1,\n        \"lambda_l2\" : 0.1,\n        \"lambda_l1\" : 0.1,\n#         \"nthread\" : 4\n        \"metric\": 'custom',\n        'verbosity': 1,\n        'num_iterations' : 5000,\n#         'num_leaves': 64,\n#         \"min_data_in_leaf\": 100,\n#         'num_leaves': 2**11-1,\n#         'min_data_in_leaf': 2**12-1,\n        'feature_fraction': 0.5,\n#         'max_bin': 100,\n#         'max_depth': 8,\n        'early_stopping_round': 20\n}","902246d3":"%%time\n\nm_lgb = lgb.train(params, train_data, valid_sets = [valid_data], verbose_eval=20, feval=rmsle_lgbm) ","51385836":"m_lgb.save_model(\"model_rmsle.lgb\")","674cb8ca":"for tdelta in range(0, 80, 7):\n    day = fday + timedelta(days=tdelta)\n    print(tdelta, day)\n    tst = test_df.loc[test_df.week == day , train_cols]\n    print('tst shape: {}'.format(tst.shape))\n    test_df.loc[test_df.week == day, \"units_sold\"] = m_lgb.predict(tst)","8991879f":"submission_lgb = test_df[['record_ID', 'units_sold']]\nsubmission_lgb.to_csv(\"lgb_lag_12_win_6_12_itr_2k_bin_default_lr_0.07_es_20.csv\", index=False)","34b2d5cc":"Custom RMSLE for LightGBM as LightGBM doesn't have RMSLE metric in its metrics list options.","f4dfb959":"## **Creating helper functions**","7a52746b":"## **Modelling**","2b6697e9":"Create features related to lags, rolling means over lags and Date (weekday, weekofyear, month, quarter, year, month day, etc). Lag and Rolling mean features give insight into the past data of the concerned target as required for time series data.","83e4a6a2":"## **Import Libraries and Data**","9695c0fa":"## **Prediction**"}}