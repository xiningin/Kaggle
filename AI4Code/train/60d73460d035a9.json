{"cell_type":{"19412408":"code","ea148d9c":"code","6511f5e2":"code","c73a25b8":"code","1b2a186c":"code","502cd5ca":"markdown","dd982b73":"markdown","d48c5815":"markdown"},"source":{"19412408":"import spacy\nnlp = spacy.load('en_core_web_lg')\nimport numpy as np\nimport pandas as pd\nimport re\nimport os\nimport json\nfrom pprint import pprint\nfrom copy import deepcopy\nimport math\nimport torch\n!pip install rake-nltk\nfrom rake_nltk import Rake\nfrom nltk.corpus import stopwords\nfrom rake_nltk import Metric, Rake\nfrom IPython.display import display, Markdown, Latex, HTML","ea148d9c":"print ('python packages imported')\n\n# keep only documents with covid -cov-2 and cov2\ndef search_focus(df):\n    dfa = df[df['abstract'].str.contains('covid')]\n    dfb = df[df['abstract'].str.contains('-cov-2')]\n    dfc = df[df['abstract'].str.contains('cov2')]\n    dfd = df[df['abstract'].str.contains('ncov')]\n    frames=[dfa,dfb,dfc,dfd]\n    df = pd.concat(frames)\n    df=df.drop_duplicates(subset='title', keep=\"first\")\n    return df\n\n# load the meta data from the CSV file using 3 columns (abstract, title, authors),\ndf=pd.read_csv('\/kaggle\/input\/CORD-19-research-challenge\/metadata.csv', usecols=['title','journal','abstract','authors','doi','publish_time','sha','full_text_file'])\nprint ('total documents ',df.shape)\n#drop duplicates\n#df=df.drop_duplicates()\n#drop NANs \ndf=df.fillna('no data provided')\ndf = df.drop_duplicates(subset='title', keep=\"first\")\n# convert abstracts to lowercase\ndf[\"abstract\"] = df[\"abstract\"].str.lower()+df[\"title\"].str.lower()\n#show 5 lines of the new dataframe\ndf=search_focus(df)\ndf = df[df['publish_time'].str.contains('2020')]\nprint ('COVID-19 focused documents ',df.shape)\ndf.head()\n\ndef format_body(body_text):\n    texts = [(di['section'], di['text']) for di in body_text]\n    texts_di = {di['section']: \"\" for di in body_text}\n    \n    for section, text in texts:\n        texts_di[section] += text\n\n    body = \"\"\n\n    for section, text in texts_di.items():\n        body += section\n        body += \"\\n\\n\"\n        body += text\n        body += \"\\n\\n\"\n    \n    return body\n\n\nfor index, row in df.iterrows():\n    if ';' not in row['sha'] and os.path.exists('\/kaggle\/input\/CORD-19-research-challenge\/'+row['full_text_file']+'\/'+row['full_text_file']+'\/pdf_json\/'+row['sha']+'.json')==True:\n        with open('\/kaggle\/input\/CORD-19-research-challenge\/'+row['full_text_file']+'\/'+row['full_text_file']+'\/pdf_json\/'+row['sha']+'.json') as json_file:\n            data = json.load(json_file)\n            body=format_body(data['body_text'])\n            keyword_list=['TB','incidence','age']\n            #print (body)\n            body=body.replace(\"\\n\", \" \")\n\n            df.loc[index, 'abstract'] = body.lower()\n\ndf=df.drop(['full_text_file'], axis=1)\ndf=df.drop(['sha'], axis=1)\ndf.head()\n\n# add full text back after testing","6511f5e2":"import functools\ndef search_focus_shape(df,focus):\n    df1 = df[df['abstract'].str.contains(focus)]\n    #df1=df[functools.reduce(lambda a, b: a&b, (df['abstract'].str.contains(s) for s in focus))]\n    return df1\n\nfocus_term='vaccin'\ndf1=search_focus_shape(df,focus_term)\nprint ('focus term: ',focus_term)\nprint ('# focused papers',df1.shape)","c73a25b8":"r = Rake(ranking_metric=Metric.DEGREE_TO_FREQUENCY_RATIO,min_length=2, max_length=2) # Uses stopwords for english from NLTK, and all puntuation characters.Please note that \"hello\" is not included in the list of stopwords.\n\ndef extract_data(text,word):\n    extract=''\n    if word in text:\n        #text = re.sub(r'[^\\w\\s]','',text)\n        res = [i.start() for i in re.finditer(word, text)]\n        after=text[res[0]:res[0]+15]\n        before=text[res[0]-15:res[0]]\n        raw = before+after\n        parts=raw.split()\n        parts = parts[1:-1]\n        extract= ' '.join(parts)\n        extract=extract.replace('vaccines','vaccine')\n    return extract\ntext=''\nfor index, row in df1.iterrows():\n    extracted=extract_data(row['abstract'],focus_term)\n    if extracted!='':\n        text=text+' '+extracted\na=r.extract_keywords_from_text(text)\nterm_list=r.get_ranked_phrases()\nterm_list = sorted(term_list, key=str.lower)\n#c=r.get_ranked_phrases_with_scores()\nprint(term_list)\n#print(c)\nprint('___________________')\n\n","1b2a186c":"# custom sentence score\ndef score_sentence_prob(search,sentence,focus):\n    final_score=0\n    keywords=search.split()\n    sent_parts=sentence.split()\n    word_match=0\n    missing=0\n    for word in keywords:\n        word_count=sent_parts.count(word)\n        word_match=word_match+word_count\n        if word_count==0:\n            missing=missing+1\n    percent = 1-(missing\/len(keywords))\n    final_score=abs((word_match\/len(sent_parts)) * percent)\n    if missing==0:\n        final_score=final_score+.05\n    if focus in sentence:\n        final_score=final_score+.05\n    return final_score\n\ndef score_docs(df,focus,search):\n    df_results = pd.DataFrame(columns=['date','study','link','excerpt','score'])\n    df1=df[functools.reduce(lambda a, b: a&b, (df['abstract'].str.contains(s) for s in focus))]\n    master_text=''\n    for index, row in df1.iterrows():\n        pub_sentence=''\n        sentences=row['abstract'].split('.')\n        hi_score=0\n        for sentence in sentences:\n            if len(sentence)>75 and search in sentence:\n                rel_score=score_sentence_prob(search,sentence,focus)\n                #rel_score=score_sentence(search,sentence)\n                if rel_score>.0002:\n                    #print (sentence,rel_score)\n                    pub_sentence=pub_sentence+' '+sentence+' '+str(round(rel_score,2))\n                    if rel_score>hi_score:\n                        hi_score=rel_score\n                    master_text=master_text+' '+pub_sentence\n        if pub_sentence!='':\n            #print (row['abstract'])\n            #print ('------------------')\n            link=row['doi']\n            linka='https:\/\/doi.org\/'+link\n            to_append = [row['publish_time'],row['title'],linka,pub_sentence,hi_score]\n            df_length = len(df_results)\n            df_results.loc[df_length] = to_append\n    df_results=df_results.sort_values(by=['date'], ascending=False)\n\n    return df_results\n\nfor term in term_list:\n    if focus_term in term and any(map(str.isdigit, term))==False and ')' not in term:\n        df_results=score_docs(df,focus_term,term)\n        if df_results.empty==False:\n            #print (term)\n            display(Markdown('# '+term))\n            df_table_show=HTML(df_results.to_html(escape=False,index=False))\n            display(df_table_show)","502cd5ca":"# Introducing a Self-Organizing Scientific Research Understanding System (SOSRUS)\n\n# Keyword: vaccine\n\n\n**Scenario:** A wordlwide pandemic has just broken out and you are an epedemiologist, medical researcher etc. and  The Whitehouse is looking to you for answers about the pandemic.\n\n**Problem:** The medical research is being published fast and furious and it is nearly impossible to stay on top of all the topics of all the documents published daily, let alone try to have a deep understanding of what all the documents may hold regarding **(SEE keyword above)**.\n\n**Current approach:** Currently, the approach to this problem would be to put all these documents into a database such as Google Scholar, Pubmed etc., and then begin searching for drugs that may show promise using keywords and trial and error. The user would learn something from each search and write down new keywords, and concepts - slowly putting together a picture of **(SEE keyword above)**.  All of this information would have to be distilled down by the user into a digestable and shareable format of everything they learned about **(SEE keyword above)**.\n\n**Solution:** We propose a new approach to this problem called SOSRUS - Self-Organizing Scientific Research Understanding System.  In this notebook, we will demonstrate how the system, given just the word **(SEE keyword above)**, will analyze all the documents relating to the term **(SEE keyword above)** and create a language model to automatically understand and uncover important topics about drugs and expand on them, ultimately providing an organized and concise output that summarizies all the information about drugs into an easy to read format.\n\n**Goal:** To avoid a hand coded rules system and have the system actually learn concepts on its own starting with one word or short phrase.  The system, on its own, should expand, learn and present relevant information and supporting documents in a helpful and understandable manner for human understanding.\n\n# Step 1: Import Python packages and full-text documents:\nHere the system will import all python packages needed to create itself.  The system will also import the data from the meta CSV file into a Pandas dataframe and filter out the documents that are not relevant to COVID-19 COV-2 etc.\n\nThen the full text versions of the filtered documents will be loaded from the JSON files into the correpsonding rows of the dataframe.  If there is a full text, that is saved in the abstract column, if there is not full text version, the abstract is used for those papers.\n\nFinally in this step, the system will focus only on documents that contain the word **(SEE keyword above)**.","dd982b73":"# Step 2: Word proximity \/ pseudo bi-gram language model\nIn this step, the system analyzes all the documents containing the word **(SEE keyword above)** and extracts words in close proximity.  The theory here is that proximity of words helps ensure relevance and importance to **(SEE keyword above)** and proximity also carries some weight relating to the symantic understanding. [Read about proximity search](https:\/\/en.wikipedia.org\/wiki\/Proximity_search_(text) After words in close proximity to **(SEE keyword above)** are extracted from the documents, NLTK rapid automatic keyword extraction (RAKE) [Read about RAKE](https:\/\/csurfer.github.io\/rake-nltk\/_build\/html\/advanced.html) is used to extract keywords and produce a list of bi-grams that we use as a pseudo bi-gram language model to get better understanding of key terms used when discussing **(SEE keyword above)** in the CORD19 corpus. [Read about N-gram language models](https:\/\/web.stanford.edu\/~jurafsky\/slp3\/3.pdf)\n\nThe system then presents an alphabetical list of keywords that increase the understanding about how **(SEE keyword above)** are discussed in the corpus.","d48c5815":"# Step 3 - The system analyzes all documents at sentence level using the RAKE key words\nIn this step, the list of keywords extracted with RAKE are now used to search the **(SEE keyword above)** papers and return relevant excerpts at sentence level.  These excerpts and the corresponding papers are organized by publish date and presented in tables organized alphabeticlly for each RAKE keyword \/ topic.  This makes it easy for a researcher to review a sea of documents and  to quickly drill down on the topics that seem to have promise."}}