{"cell_type":{"c2e28c8e":"code","b0730dcf":"code","b0c373e6":"code","4c431e63":"code","83b55978":"code","dbd56a0b":"code","acfec3c8":"code","c6124b84":"code","8256c002":"code","633b9423":"code","e803656d":"code","789d3f11":"markdown","5bdbfafa":"markdown","f0c8c399":"markdown","33af41f2":"markdown","7b49b400":"markdown","badcab02":"markdown","88dad220":"markdown","97c21db8":"markdown","b8404e63":"markdown","414bf758":"markdown"},"source":{"c2e28c8e":"import numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nimport gc\n\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import metrics","b0730dcf":"def _get_X_Y_DF_from_CV(train_X, train_Y, train_index, validation_index):\n    X_train, X_validation = (\n        train_X.iloc[train_index],\n        train_X.iloc[validation_index],\n    )\n    y_train, y_validation = (\n        train_Y.iloc[train_index],\n        train_Y.iloc[validation_index],\n    )\n    return X_train, X_validation, y_train, y_validation","b0c373e6":"train_df = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/train.csv', index_col=\"id\")\ntest_df = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/test.csv', index_col=\"id\")\nsample_submission_df = pd.read_csv('..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')","4c431e63":"TARGET = \"loss\"\nID = \"id\"\nSEED = 42\nNUM_CLASSES = 43\nEARLY_STOPPING_ROUNDS = 100\nN_ESTIMATORS = 1000\n\n# Define Parameters for LGBM\nlgb_params = {\n    \"objective\": \"multiclass\",\n    \"boosting_type\": \"gbdt\",\n    \"learning_rate\": 0.1,\n    \"num_class\": 43,\n    \"num_leaves\": 31,\n    \"tree_learner\": \"serial\",\n    \"n_jobs\": 4,\n    \"seed\": SEED,\n    \"max_depth\": -1,\n    \"max_bin\": 255,\n    \"metric\": \"multi_logloss\",\n    \"verbose\": -1,\n}","83b55978":"train_X = train_df.drop([TARGET], axis=1)\ntrain_Y = train_df[TARGET]\ntest_X = test_df\nprint(f\"Shape of train_X : {train_X.shape}, test_X: {test_X.shape}, train_Y: {train_Y.shape}\")\n\npredictors = list(train_X.columns)\nprint(f\"List of features to be used {list(predictors)}\")\n\n# Selecting n_splits to be 3, since class 42 has \n# just 3 instances\nkf = StratifiedKFold(n_splits=3, shuffle=True)","dbd56a0b":"y_oof = np.zeros(shape=(len(train_X), NUM_CLASSES))\ny_predicted = np.zeros(shape=(len(test_X), NUM_CLASSES))\ncv_scores = []\n\nfold = 0\nn_folds = kf.get_n_splits()\nfor train_index, validation_index in kf.split(X=train_X, y=train_Y):\n    fold += 1\n    print(f\"fold {fold} of {n_folds}\")\n\n    X_train, X_validation, y_train, y_validation = _get_X_Y_DF_from_CV(\n        train_X, train_Y, train_index, validation_index\n    )\n\n    lgb_train = lgb.Dataset(X_train, y_train)\n    lgb_eval = lgb.Dataset(X_validation, y_validation, reference=lgb_train)\n\n    model = lgb.train(\n        lgb_params,\n        lgb_train,\n        valid_sets=[lgb_train, lgb_eval],\n        verbose_eval=10,\n        early_stopping_rounds=EARLY_STOPPING_ROUNDS,\n        num_boost_round=N_ESTIMATORS,\n        feature_name=predictors,\n        categorical_feature=\"auto\",\n    )\n\n    del lgb_train, lgb_eval, train_index, X_train, y_train\n    gc.collect()\n\n    y_oof[validation_index] = model.predict(\n        X_validation, num_iteration=model.best_iteration\n    )\n\n    y_predicted += model.predict(\n        test_X.values, num_iteration=model.best_iteration\n    )\n\n    best_iteration = model.best_iteration\n    print(f\"Best number of iterations for fold {fold} is: {best_iteration}\")\n\n    cv_oof_score = metrics.log_loss(y_validation, y_oof[validation_index])\n    cv_scores.append(cv_oof_score)\n    print(f\"CV OOF Score for fold {fold} is {cv_oof_score}\")\n\n    del validation_index, X_validation, y_validation\n    gc.collect()\n\ny_predicted \/= n_folds\noof_score = round(metrics.log_loss(train_Y, y_oof), 5)\navg_cv_scores = round(sum(cv_scores) \/ len(cv_scores), 5)\nstd_cv_scores = round(np.array(cv_scores).std(), 5)","acfec3c8":"print(f\"Out of Fold (log_loss) score {oof_score}\")\nprint(f\"Avg CV (log_loss) score {avg_cv_scores}\")\nprint(f\"Avg CV (log_loss) std {std_cv_scores}\")","c6124b84":"y_predicted.shape","8256c002":"# Select the class with highest probability as the predicted class\nclass_prediction = np.argmax(y_predicted, axis=1)\nclass_y_oof = np.argmax(y_oof, axis=1)","633b9423":"rmse_oof_score = np.sqrt(metrics.mean_squared_error(train_Y, class_y_oof))\nprint(f\"RMSE score on the OOF data {rmse_oof_score}\")","e803656d":"sample_submission_df.loss = class_prediction\nsample_submission_df.to_csv('sample_submission.csv', index=None)\nsample_submission_df.head()","789d3f11":"## Create the sample submission file","5bdbfafa":"## Load Data","f0c8c399":"## Build the model","33af41f2":"### In this notebook I am trying to approach this problem as a classification task. \n\nThe target variable \"loss\" consists of 43 discrete integer values, instead of continuous values (float). That opens up a possibility to solve this problem as a **multi-class classification task** instead of a **regression task** (as suggested by the competition organizers).\n\nI have used LightGBM as the algorithm and log_loss as the metrics.\n\nHowever, as we will see **this solution performs much worse than the regression approach.**","7b49b400":"### At this stage, the predicted values are nothing but probabilities for each of the 43 classes. In the next step, we will select the class with highest probability as the predicted class for each instance of the test data","badcab02":"## Split the Data & Define CV Method","88dad220":"## Check the log_loss scores","97c21db8":"## Define the constants & parameters","b8404e63":"## Calculate the OOF RMSE score","414bf758":"### RMSE score using the classification approach is worst compared to the regression approach"}}