{"cell_type":{"8e0392f3":"code","111c3089":"code","df51baaf":"code","e7468836":"code","961278a4":"code","b6da50b5":"code","1ecef891":"code","e8fbd9ee":"code","4f20d2be":"code","b9c014be":"code","8bca31ca":"code","5d7e016e":"code","18ca7922":"code","7d3c467e":"code","a6095d6b":"code","37a312a8":"code","9a65979f":"code","bd6d6c1a":"code","0ba792e1":"code","5b87db28":"code","641cba22":"code","26114140":"code","2dff02d1":"code","e5f6ab4a":"code","76eecc51":"code","d150b35a":"markdown","0680f272":"markdown","e605808f":"markdown","91885579":"markdown","e5614174":"markdown","d97dd1ae":"markdown","8f19e879":"markdown","34e8a90f":"markdown","026baabf":"markdown","8c0164f3":"markdown","29408f36":"markdown","ffdd04e9":"markdown","c94c539a":"markdown","10f9095b":"markdown","bbf9b157":"markdown"},"source":{"8e0392f3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","111c3089":"data=pd.read_csv('\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","df51baaf":"data.head()","e7468836":"data.info()","961278a4":"data[data.isnull()].count()","b6da50b5":"X = data.drop('quality', axis=1)\ny = data.quality","1ecef891":"# For each feature find the data points with extreme high or low values\nfor feature in X.keys():\n\n    # Q1 (25th percentile of the data) for the given feature\n    Q1 = np.percentile(X[feature], q=25)\n \n    # Q3 (75th percentile of the data) for the given feature\n    Q3 = np.percentile(X[feature], q=75)\n \n    # We use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n    interquartile_range = Q3 - Q1\n    step = 1.5 * interquartile_range\n \n    # Display the outliers\n    print(\"Data points considered outliers for the feature '{}':\".format(feature))\n    display(X[~((X[feature] >= Q1 - step) & (X[feature] <= Q3 + step))])\n ","e8fbd9ee":"X.describe()","4f20d2be":"sns.pairplot(X)","b9c014be":"sns.distplot(y)","8bca31ca":"### we can plot heat map to examin the correlation\ncorrelation = X.corr()\n# display(correlation)\nplt.figure(figsize=(14, 12))\nheatmap = sns.heatmap(correlation, annot=True, linewidths=0, vmin=-1, cmap=\"RdBu_r\")","5d7e016e":"fixedAcidity_citricAcid = X[['citric acid', 'fixed acidity']]\ng = sns.JointGrid(x=\"fixed acidity\", y=\"citric acid\", data=fixedAcidity_citricAcid, size=10)\ng = g.plot_joint(sns.regplot, scatter_kws={\"s\": 10})\ng = g.plot_marginals(sns.distplot)","18ca7922":"\n# A new dataframe containing only pH and fixed acidity columns to visualize their co-relations\nfixedAcidity_pH = X[['pH', 'fixed acidity']]\ngridA = sns.JointGrid(x=\"fixed acidity\", y=\"pH\", data=fixedAcidity_pH, size=10)\n#Regression plot in the grid \ngridA = gridA.plot_joint(sns.regplot, scatter_kws={\"s\": 10})\n#Distribution plot in the same grid\ngridA = gridA.plot_marginals(sns.distplot)","7d3c467e":"volatileAcidity_quality=data[['volatile acidity','quality']]\nfig, axs = plt.subplots(ncols=1,figsize=(15,10))\nsns.barplot(x='quality', y='volatile acidity', data=volatileAcidity_quality)\nplt.title('quality VS volatile acidity')\nplt.show()","a6095d6b":"alcohol_quality=data[['alcohol','quality']]\nfig, axs = plt.subplots(ncols=1,figsize=(15,10))\nsns.barplot(x='quality', y='alcohol', data=alcohol_quality)\nplt.title('quality VS alcohol content')\nplt.show()","37a312a8":"from sklearn.model_selection import train_test_split","9a65979f":"X_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.2)","bd6d6c1a":"from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()","0ba792e1":"X_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)","5b87db28":"from sklearn.linear_model import LinearRegression\nlr=LinearRegression()\nlr.fit(X_train,y_train)\npred=lr.predict(X_test)","641cba22":"from sklearn import metrics\nprint('MAE:', metrics.mean_absolute_error(y_test, pred)*100)\nprint('MSE:', metrics.mean_squared_error(y_test, pred)*100)\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, pred))*100)","26114140":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=100, max_depth=5,random_state=0)\nclf.fit(X_train,y_train)","2dff02d1":"pred=clf.predict(X_test)","e5f6ab4a":"from sklearn.metrics import accuracy_score,f1_score","76eecc51":"print(\"accuracy:\",(accuracy_score(y_test,pred)*100))\n","d150b35a":"**As the citric acid increases the fixed acid also increases linearly.**","0680f272":"**The squares with positive values show direct co-relationships between features. The higher the values, the stronger these relationships are \u2014 they\u2019ll be more reddish. That means, if one feature increases, the other one also tends to increase, and vice-versa.**\n\n\n**The squares that have negative values show an inverse co-relationship. The more negative these values get, the more inversely proportional they are, and they\u2019ll be more blue. This means that if the value of one feature is higher, the value of the other one gets lower.**\n\n**Finally, squares close to zero indicate almost no co-dependency between those sets of features.**","e605808f":"> ****2. Detect Columns that contains outliers...****","91885579":"This is a supervised learning problem and it can be solved in both regression and classification method, though in classification we will try to minimize the range label to few encoded lables for better correlation and accuracy.We will see both the implementation below after a while.","e5614174":"****EXPLORATORY DATA ANALYSIS****","d97dd1ae":"**Classification method**","8f19e879":"**BUILDING  MODEL AND OPTIMIZATION**","34e8a90f":"**More the alcohol content better is the quality of wine.**","026baabf":"****DATA PRE-PROCESSING****","8c0164f3":"**Fixed Acidity vs. Citric Acid**","29408f36":"**What kind of a ML problem is this?If Regression,then can this also be solved as a classification problem?**","ffdd04e9":"**We can clearly see that lower the volatile acidity better is the quality of the wine**","c94c539a":"**Fixed acidity levels increase, the pH levels.A lower pH level is, after all, an indicator of high acidity.**","10f9095b":"**From the above scatterplot we can get some interesting details. For some of the features, the distribution appears to be fairly linear. For some others, the distribution appears to be negatively skewed. So this confirms our initial suspicions \u2014 there are indeed some interesting co-dependencies(relying on other features) between some of the features.**","bbf9b157":"**Regression model**"}}