{"cell_type":{"8716519f":"code","d4b505f2":"code","c722f1b2":"code","f9d7510a":"code","ad0abfdd":"code","0d1d0054":"code","eb9e4411":"code","197d65e8":"code","b27c30e9":"code","a845d935":"code","9a7c05c3":"code","8e1f65f5":"code","b585fad4":"code","3959d5a5":"code","ce777bee":"code","9f1e8c37":"code","4bb219f8":"code","9bcf7629":"code","f42d0744":"code","5259ebac":"code","66d44f83":"code","1864a768":"code","b3aa6070":"code","6f78c498":"code","e35d9617":"code","645511d0":"code","11b424b9":"code","f9aa6a41":"code","dd0cda99":"code","62d858e2":"code","3a85a984":"code","ca4184ed":"code","e053c8a9":"code","908a8ac4":"code","d071c98a":"code","2c4fcff5":"code","6089216d":"code","de0c852d":"code","cd7f9082":"code","001e7bf2":"code","3af41501":"code","6a652766":"code","44ceeea7":"code","5b5a713f":"code","be3f7401":"code","4d356439":"code","3d02341e":"code","82b7d306":"code","6722939e":"code","740383ba":"code","d074578d":"code","ff8ec394":"code","2c35a438":"code","5d35cb69":"code","465f1d24":"code","9c20eb3f":"code","0ef2431e":"code","06700783":"code","c7c646f9":"code","22201c22":"code","72fddeb1":"code","93ad968f":"code","f5613288":"code","8bd41e9e":"code","1175d321":"code","708590e6":"code","2efa1e3f":"code","6e837cb1":"code","69b1a4ac":"code","2758de8e":"code","380d737a":"code","09bb7052":"code","7137378f":"code","d587b92d":"code","ccb592d1":"code","605c5dc0":"code","e685580b":"code","f4e72f61":"code","885c3a4a":"code","ff03c219":"code","c786ebea":"code","dce20c03":"code","5a59df22":"code","c7a9d51a":"code","109ded06":"code","473ba5e4":"code","8e4a9b0e":"code","48d3592c":"code","b85b173d":"code","905cb0f1":"code","86a8764f":"code","1ac4f1c0":"code","98444641":"code","10ce8873":"code","e5df2a33":"code","945caa94":"code","bb61fa94":"code","b4bd0d81":"code","f751a9d8":"code","341e4444":"code","2cfda0ad":"code","f32caa0a":"code","913ea20d":"code","165ce5df":"code","23859d24":"code","e5f3e845":"code","5d62f936":"code","2f69c813":"code","3cd2fb1b":"markdown","713a5f4f":"markdown","97e00c7e":"markdown","a7c85ecf":"markdown","0563b462":"markdown","eda8d249":"markdown","ff1aa200":"markdown"},"source":{"8716519f":"data_dir = \"..\/input\/facebook-hateful-meme-dataset\/data\"\n\ntrain_path = data_dir + \"\/train.jsonl\"\ntest_path = data_dir + \"\/test.jsonl\"\ndev_path = data_dir + \"\/dev.jsonl\"","d4b505f2":"# load Inception model\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\ninception = InceptionV3(weights='imagenet')\n# inception.summary()","c722f1b2":"# get train data\nimport pandas as pd\nfrom tensorflow.keras.preprocessing import image\ndf_train = pd.read_json(train_path, lines=True)\ndf_train.head()","f9d7510a":"import numpy as np\nfrom tensorflow.keras.preprocessing import image","ad0abfdd":"# df_train = df_train[:3000]","0d1d0054":"img = []\n# print(df_train.count())\nfor i in range(0, df_train.shape[0]):\n#     print(i)\n    im = image.load_img(\"..\/input\/facebook-hateful-meme-dataset\/data\/\" + df_train[\"img\"][i], target_size=(299,299))\n    img.append(im)","eb9e4411":"len(img)","197d65e8":"for i in range(df_train.shape[0]):\n    x = image.img_to_array(img[i])\n    x = np.array([x])\n\n    x = preprocess_input(x)\n    preds = inception.predict(x)\n    predicted = decode_predictions(preds, top = 5)[0]\n    for j in range(0, 5):\n#         print(df_train[\"text\"][i] + \" \" + predicted[j][1])\n        df_train[\"text\"][i] = df_train[\"text\"][i] + \" \" + predicted[j][1]\n    \n\n# print(\"Predicted: \", decode_predictions(preds, top = 3)[0])","b27c30e9":"print(df_train[\"text\"][1])","a845d935":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing import image\nimport json\nimport seaborn as sns","9a7c05c3":"# df_ext = pd.read_json(dev_path, lines=True)\n# df_train = pd.concat([df_train, df_ext])","8e1f65f5":"# # word length distribution train set\n# from collections import Counter\n# import matplotlib.pyplot as plt\n\n# lengths = []\n# for text in df_train.text:\n#     lengths.append(len(text))\n# c = Counter(lengths)\n# plt.figure(figsize=(20, 10), dpi=80)\n# plt.bar([i for i in c.keys()], c.values())\n# plt.show()","b585fad4":"# train valid split on initial training set\nfrom sklearn.model_selection import train_test_split\n\ny = df_train['label']\ndf_train, df_test = train_test_split(df_train, test_size=0.15, random_state=42, stratify=y)\ny = df_test['label']\ndf_val, df_test = train_test_split(df_test, test_size=0.95, random_state=42, stratify=y)\ndel y\n\nprint('Valid:', df_val.shape[0])\nprint('Train:', df_train.shape[0])","3959d5a5":"!pip install inflect","ce777bee":"import os\nimport pandas as pd\n\nimport re, string, unicodedata\nimport nltk\nimport inflect\nfrom nltk import word_tokenize, sent_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import LancasterStemmer, WordNetLemmatizer\n\nfrom PIL import ImageFile\nImageFile.LOAD_TRUNCATED_IMAGES = True\n\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')","9f1e8c37":"del img","4bb219f8":"del inception","9bcf7629":"# Word length distribution in train set before preprocessing\nfrom collections import Counter\nimport matplotlib.pyplot as plt\n\nlengths = []\nfor text in df_train.text:\n    lengths.append(len(text))\nc = Counter(lengths)\nplt.figure(figsize=(20, 10), dpi=80)\nplt.bar([i for i in c.keys()], c.values())\nplt.title('Number of Words in a Sentence before Pre Processing',size=28)\nplt.xlabel('Count of words',size=22)\nplt.ylabel('Number of sentences',size=22)\nplt.show()","f42d0744":"# Without Preprocessing\n\ndf_train['text_cleaned'] = df_train['text']\ndf_test['text_cleaned'] = df_test['text']\ndf_val['text_cleaned'] = df_val['text']","5259ebac":"df_train.head()","66d44f83":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nimport torch\n# import transformers as ppb\nimport warnings\nwarnings.filterwarnings('ignore')","1864a768":"!pip install transformers","b3aa6070":"import transformers as ppb","6f78c498":"# For DistilBERT:\n# model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n\n## Want BERT instead of distilBERT? Uncomment the following line:\nmodel_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n\n# Load pretrained model\/tokenizer\ntokenizer = tokenizer_class.from_pretrained(pretrained_weights)\nmodel = model_class.from_pretrained(pretrained_weights)","e35d9617":"tokenize = tokenizer_class.from_pretrained(pretrained_weights)\ntokenized = df_train['text_cleaned'].apply((lambda x: tokenize.encode(str(x), add_special_tokens=True)))","645511d0":"# Finding max word length\n\nmax_len = 0\nfor i in tokenized.values:\n    if len(i) > max_len:\n        max_len = len(i)\nmax_len","11b424b9":"train_features = np.array([[]])\ntest_features = np.array([[]])\ntrain_labels = np.array([])\ntest_labels = np.array([])","f9aa6a41":"BATCH_SIZE = 1125\n\nfor i in range(0, len(df_train), BATCH_SIZE):\n\n    df = df_train[ i : i+BATCH_SIZE ]\n\n    # Tokenization\n    tokenized = df['text_cleaned'].apply((lambda x: tokenize.encode(str(x), add_special_tokens=True)))\n\n    # Append zeros for padding\n    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n\n    # Extract Attention Masks\n    attention_mask = np.where(padded != 0, 1, 0)\n\n    # Here we just convert the data to tensors\n    input_ids = torch.tensor(padded)  \n    attention_mask = torch.tensor(attention_mask)\n\n    with torch.no_grad():\n        last_hidden_states = model(input_ids, attention_mask=attention_mask)\n\n    # Extracting features of the CLS tokens\n    features = last_hidden_states[0][:,0,:].numpy()\n\n    # Extracting Labels\n    labels = df['label'].values\n    labels[labels>0] = 1\n\n    # Test Train Split\n    train_features_2, test_features_2, train_labels_2, test_labels_2 = train_test_split(features, labels, train_size=0.85)\n\n    # Appending the data to the main arrays\n    if len(train_labels) == 0 :\n        train_features = train_features_2\n        test_features = test_features_2\n        train_labels = train_labels_2\n        test_labels = test_labels_2\n    else:\n        train_features = np.concatenate((train_features,train_features_2))\n        test_features = np.concatenate((test_features,test_features_2))\n        train_labels = np.concatenate((train_labels,train_labels_2))\n        test_labels = np.concatenate((test_labels,test_labels_2))\n    print(train_features.shape)\n    print(test_features.shape)\n    print(train_labels.shape)\n    print(test_labels.shape)","dd0cda99":"from sklearn.metrics import roc_curve\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import confusion_matrix, classification_report\nplt.style.use('seaborn')","62d858e2":"def print_metrices(cm):\n    cm = np.array(cm, dtype=np.float)\n    class_names = ['Not Offensive','Offensive']\n    matplotlib.rcParams.update({'font.size': 20})\n    plt.xlabel(\"Actual Value\", fontsize=18)\n    plt.ylabel('Predicted Value', fontsize=18)\n    matplotlib.rcParams.update({'xtick.labelsize': 16})\n    matplotlib.rcParams.update({'ytick.labelsize': 16})\n    ax = sns.heatmap(cm,annot = True,xticklabels=class_names, yticklabels=class_names, cmap=\"YlGnBu\", fmt='g')\n    ax.set(xlabel='True Values', ylabel='Predicted Values')\n    plt.show()","3a85a984":"parameters = {'C': np.linspace(0.01, 0.1, 1)}\ngrid_search = GridSearchCV(LogisticRegression(), parameters)\ngrid_search.fit(train_features, train_labels)\n\nprint('best parameters: ', grid_search.best_params_)\nprint('best scores: ', grid_search.best_score_)","ca4184ed":"train_features[0]","e053c8a9":"print(train_labels.shape)","908a8ac4":"type(train_features[0])","d071c98a":"lr_clf = LogisticRegression(solver='liblinear', C=0.1)\n# lr_clf.fit(train_features, train_labels)\n# lr_clf.score(test_features, test_labels)","2c4fcff5":"lr_clf.fit(train_features, train_labels)","6089216d":"lr_clf.score(test_features, test_labels)","de0c852d":"pred_labels = lr_clf.predict(test_features)\nconf_matrix = confusion_matrix(test_labels, pred_labels)\nprint_metrices(conf_matrix)","cd7f9082":"pred_prob = lr_clf.predict_proba(test_features)\nfpr, tpr, thresh = roc_curve(test_labels, pred_prob[:,1], pos_label=1)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(test_labels))]\np_fpr, p_tpr, _ = roc_curve(test_labels, random_probs, pos_label=1)\nauc_score = roc_auc_score(test_labels, pred_prob[:,1])\nprint(auc_score)","001e7bf2":"print(classification_report(test_labels, pred_labels))","3af41501":"pred_temp = pred_prob[:,1]\npred_temp[pred_temp >= 0.44] = 1\npred_temp[pred_temp < 0.44] = 0\nscore1 = (pred_temp == test_labels).sum() \/ len(test_labels)\nprint('Accuracy with shifted center: ', score1)","6a652766":"# plot roc curves\nplt.plot(fpr, tpr, linestyle='--',color='orange')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC Curve - Logistic Regression', fontsize=20)\n# x label\nplt.xlabel('False Positive Rate', fontsize=18)\n# y label\nplt.ylabel('True Positive rate',fontsize=18)\n\n# plt.legend(loc='best')\nplt.savefig('ROC',dpi=300)\nplt.show();","44ceeea7":"from sklearn.svm import SVC\nsvm = SVC(kernel=\"linear\", C=0.025,random_state=101)\nsvm.probability=True\nsvm.fit(train_features, train_labels)\nsvm.score(test_features, test_labels)","5b5a713f":"pred_labels = svm.predict(test_features)\nconf_matrix = confusion_matrix(test_labels, pred_labels)\nprint_metrices(conf_matrix)","be3f7401":"pred_prob = svm.predict_proba(test_features)\nfpr, tpr, thresh = roc_curve(test_labels, pred_prob[:,1], pos_label=1)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(test_labels))]\np_fpr, p_tpr, _ = roc_curve(test_labels, random_probs, pos_label=1)\nauc_score = roc_auc_score(test_labels, pred_prob[:,1])\nprint(auc_score)","4d356439":"print(classification_report(test_labels, pred_labels))","3d02341e":"# plot roc curves\nplt.plot(fpr, tpr, linestyle='--',color='orange')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC Curve - SVM', fontsize=20)\n# x label\nplt.xlabel('False Positive Rate', fontsize=18)\n# y label\nplt.ylabel('True Positive rate',fontsize=18)\n\n# plt.legend(loc='best')\nplt.savefig('ROC',dpi=300)\nplt.show();","82b7d306":"from sklearn.ensemble import RandomForestClassifier\nrfm = RandomForestClassifier(n_estimators=70, oob_score=True, n_jobs=-1,\n                            random_state=101,max_features=None, min_samples_leaf=30)\nrfm.fit(train_features, train_labels)\nrfm.score(test_features, test_labels)","6722939e":"pred_labels = rfm.predict(test_features)\nconf_matrix = confusion_matrix(test_labels, pred_labels)\nprint_metrices(conf_matrix)","740383ba":"print(classification_report(test_labels, pred_labels))","d074578d":"pred_prob = rfm.predict_proba(test_features)\nfpr, tpr, thresh = roc_curve(test_labels, pred_prob[:,1], pos_label=1)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(test_labels))]\np_fpr, p_tpr, _ = roc_curve(test_labels, random_probs, pos_label=1)\nauc_score = roc_auc_score(test_labels, pred_prob[:,1])\nprint(auc_score)","ff8ec394":"# plot roc curves\nplt.plot(fpr, tpr, linestyle='--',color='orange')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC Curve - Random Forest', fontsize=20)\n# x label\nplt.xlabel('False Positive Rate', fontsize=18)\n# y label\nplt.ylabel('True Positive rate',fontsize=18)\n\n# plt.legend(loc='best')\nplt.savefig('ROC',dpi=300)\nplt.show();","2c35a438":"from sklearn.tree import DecisionTreeClassifier\ndtree = DecisionTreeClassifier(max_depth=15, random_state=101,\n                               max_features=None, min_samples_leaf=15)\ndtree.fit(train_features, train_labels)\ndtree.score(test_features, test_labels)","5d35cb69":"pred_labels = dtree.predict(test_features)\nconf_matrix = confusion_matrix(test_labels, pred_labels)\nprint_metrices(conf_matrix)","465f1d24":"print(classification_report(test_labels, pred_labels))","9c20eb3f":"pred_prob = dtree.predict_proba(test_features)\nfpr, tpr, thresh = roc_curve(test_labels, pred_prob[:,1], pos_label=1)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(test_labels))]\np_fpr, p_tpr, _ = roc_curve(test_labels, random_probs, pos_label=1)\nauc_score = roc_auc_score(test_labels, pred_prob[:,1])\nprint(auc_score)","0ef2431e":"# plot roc curves\nplt.plot(fpr, tpr, linestyle='--',color='orange')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC Curve - Decision Tree', fontsize=20)\n# x label\nplt.xlabel('False Positive Rate', fontsize=18)\n# y label\nplt.ylabel('True Positive rate',fontsize=18)\n\n# plt.legend(loc='best')\nplt.savefig('ROC',dpi=300)\nplt.show();","06700783":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier()\nknn.fit(train_features, train_labels)\nknn.score(test_features, test_labels)","c7c646f9":"pred_labels = knn.predict(test_features)\nconf_matrix = confusion_matrix(test_labels, pred_labels)\nprint_metrices(conf_matrix)","22201c22":"print(classification_report(test_labels, pred_labels))","72fddeb1":"pred_prob = knn.predict_proba(test_features)\nfpr, tpr, thresh = roc_curve(test_labels, pred_prob[:,1], pos_label=1)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(test_labels))]\np_fpr, p_tpr, _ = roc_curve(test_labels, random_probs, pos_label=1)\nauc_score = roc_auc_score(test_labels, pred_prob[:,1])\nprint(auc_score)","93ad968f":"# plot roc curves\nplt.plot(fpr, tpr, linestyle='--',color='orange')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC Curve - KNN', fontsize=20)\n# x label\nplt.xlabel('False Positive Rate', fontsize=18)\n# y label\nplt.ylabel('True Positive rate',fontsize=18)\n\n# plt.legend(loc='best')\nplt.savefig('ROC',dpi=300)\nplt.show();","f5613288":"from sklearn.linear_model import SGDClassifier\nsgd = SGDClassifier(loss='modified_huber',shuffle=True,random_state=101)\nsgd.fit(train_features, train_labels)\nsgd.score(test_features, test_labels)","8bd41e9e":"pred_labels = sgd.predict(test_features)\nconf_matrix = confusion_matrix(test_labels, pred_labels)\nprint_metrices(conf_matrix)","1175d321":"print(classification_report(test_labels, pred_labels))","708590e6":"pred_prob = sgd.predict_proba(test_features)\nfpr, tpr, thresh = roc_curve(test_labels, pred_prob[:,1], pos_label=1)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(test_labels))]\np_fpr, p_tpr, _ = roc_curve(test_labels, random_probs, pos_label=1)\nauc_score = roc_auc_score(test_labels, pred_prob[:,1])\nprint(auc_score)","2efa1e3f":"# plot roc curves\nplt.plot(fpr, tpr, linestyle='--',color='orange')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC Curve - Stochastic Gradient', fontsize=20)\n# x label\nplt.xlabel('False Positive Rate', fontsize=18)\n# y label\nplt.ylabel('True Positive rate',fontsize=18)\n\n# plt.legend(loc='best')\nplt.savefig('ROC',dpi=300)\nplt.show();","6e837cb1":"from sklearn.naive_bayes import GaussianNB\nnb = GaussianNB()\nnb.fit(train_features, train_labels)\nnb.score(test_features, test_labels)","69b1a4ac":"pred_labels = nb.predict(test_features)\nconf_matrix = confusion_matrix(test_labels, pred_labels)\nprint_metrices(conf_matrix)","2758de8e":"print(classification_report(test_labels, pred_labels))","380d737a":"pred_prob = nb.predict_proba(test_features)\nfpr, tpr, thresh = roc_curve(test_labels, pred_prob[:,1], pos_label=1)\n\n# roc curve for tpr = fpr \nrandom_probs = [0 for i in range(len(test_labels))]\np_fpr, p_tpr, _ = roc_curve(test_labels, random_probs, pos_label=1)\nauc_score = roc_auc_score(test_labels, pred_prob[:,1])\nprint(auc_score)","09bb7052":"# plot roc curves\nplt.plot(fpr, tpr, linestyle='--',color='orange')\nplt.plot(p_fpr, p_tpr, linestyle='--', color='blue')\n# title\nplt.title('ROC Curve - Naive Bayes', fontsize=20)\n# x label\nplt.xlabel('False Positive Rate', fontsize=18)\n# y label\nplt.ylabel('True Positive rate',fontsize=18)\n\n# plt.legend(loc='best')\nplt.savefig('ROC',dpi=300)\nplt.show();","7137378f":"import keras\nfrom keras.models import Sequential,Input,Model\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom tensorflow.keras.layers import (\n    BatchNormalization, SeparableConv2D, MaxPooling2D, Activation, Flatten, Dropout, Dense\n)\nfrom keras.layers.advanced_activations import LeakyReLU","d587b92d":"batch_size = 200\nepochs = 20\nnum_classes = 2","ccb592d1":"df_train.head\n","605c5dc0":"fashion_model = Sequential()\nfashion_model.add(Conv2D(32, kernel_size=(3, 3),activation='linear',input_shape=(28,28,1),padding='same'))\nfashion_model.add(LeakyReLU(alpha=0.1))\nfashion_model.add(MaxPooling2D((2, 2),padding='same'))\nfashion_model.add(Conv2D(64, (3, 3), activation='linear',padding='same'))\nfashion_model.add(LeakyReLU(alpha=0.1))\nfashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\nfashion_model.add(Conv2D(128, (3, 3), activation='linear',padding='same'))\nfashion_model.add(LeakyReLU(alpha=0.1))                  \nfashion_model.add(MaxPooling2D(pool_size=(2, 2),padding='same'))\nfashion_model.add(Flatten())\nfashion_model.add(Dense(128, activation='linear'))\nfashion_model.add(LeakyReLU(alpha=0.1))                  \nfashion_model.add(Dense(num_classes, activation='softmax'))","e685580b":"bert_dense = layers.Dense(140, activation=\"relu\")(bert.output[0])\n\n\ndense1 = layers.Dense(1024, activation=\"relu\")(concat)\n# dense1 = layers.Dense(512, activation=\"relu\")(bert_dense)\ndense_dropout1 = layers.Dropout(0.5)(dense1)\n# dense2 = layers.Dense(512, activation=\"relu\")(dense_dropout1)\n# dense_dropout2 = layers.Dropout(0.5)(dense2)\n# dense3 = layers.Dense(256, activation=\"relu\")(dense_dropout2)\n# dense_dropout3 = layers.Dropout(0.5)(dense3)\n\npreds = layers.Dense(1, activation='sigmoid', name=\"output\")(dense_dropout1)\n\nmodel = Model(inputs=train_features, outputs=preds)","f4e72f61":"# # get test data\n# df_test = pd.read_json(test_path, lines=True)\n# df_test.head()","885c3a4a":"# word length distribution test set\nlengths = []\nfor text in df_test.text:\n    lengths.append(len(text))\nc = Counter(lengths)\ndel lengths\nplt.figure(figsize=(20, 10), dpi=80)\nplt.bar([i for i in c.keys()], c.values())\nplt.show()","ff03c219":"# create tokenizer\n!pip3 install -q transformers\nfrom transformers import BertTokenizer\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n","c786ebea":"# # test tokenizer\n# max_text_length = 512 # max for BERT\n\n# bert_input = tokenizer.encode_plus(\n#                         \"Give me a run for my money. There is nobody, no one to outrun me. So give me a run for my money. Sipping bubbly, feeling lovely. Living lovely. Just love me.\",                      \n#                         add_special_tokens = True, # add [CLS], [SEP]\n#                         max_length = max_text_length, # max length of the text that can go to BERT\n#                         pad_to_max_length = True, # add [PAD] tokens\n#                         return_attention_mask = True, # add attention mask to not focus on pad tokens\n#                         truncation = True # truncate values to max_text_length\n#               )\n\n# print('encoded', bert_input)","dce20c03":"# tokenize text\nmax_text_length = 140 # max for BERT according to word distribution\n\ndef create_text_inputs(df):\n\n    input_ids = []\n    token_type_ids = []\n    attention_mask = []\n    print(df)\n    for text in df.text:\n\n          bert_input = tokenizer.encode_plus(\n                            text,                      \n                            add_special_tokens = True, # add [CLS], [SEP]\n                            max_length = max_text_length, # max length of the text that can go to BERT\n                            pad_to_max_length = True, # add [PAD] tokens\n                            return_attention_mask = True, # add attention mask to not focus on pad tokens\n                            truncation = True # truncate values to max_text_length\n                  )\n          input_ids.append(bert_input[\"input_ids\"])\n          token_type_ids.append(bert_input[\"token_type_ids\"])\n          attention_mask.append(bert_input[\"attention_mask\"])\n\n    df[\"input_ids\"] = input_ids\n    df[\"token_type_ids\"] = token_type_ids\n    df[\"attention_mask\"] = attention_mask\n    df.drop(\"text\", axis=1, inplace=True)\n\ncreate_text_inputs(df_train)\ncreate_text_inputs(df_val)","5a59df22":"# store text data as numpy arrays\nimport numpy as np\n\ninput_ids = np.array(list(x for x in df_train.input_ids)).astype('int32')\ntoken_type_ids = np.array(list(x for x in df_train.token_type_ids)).astype('int8')\nattention_mask = np.array(list(x for x in df_train.attention_mask)).astype('int8')\n\ninput_ids_val = np.array(list(x for x in df_val.input_ids)).astype('int32')\ntoken_type_ids_val = np.array(list(x for x in df_val.token_type_ids)).astype('int8')\nattention_mask_val = np.array(list(x for x in df_val.attention_mask)).astype('int8')","c7a9d51a":"# load image arrays\nfrom tensorflow.keras.preprocessing import image\n\nimage_size = 299\n\ndef load_imgs(df):\n\n    images = []\n    for img_id in df.img:\n        img = image.load_img(data_dir + '\/' + img_id, target_size=(image_size, image_size))\n        img = image.img_to_array(img)\n        images.append(img)\n    \n    df.img = images\n\nload_imgs(df_train)\nload_imgs(df_val)\nload_imgs(df_test)","109ded06":"# df_train.img[3]","473ba5e4":"# store labels as numpy arrays\ntrain_labels = np.array(list(x for x in df_train.label)).astype('int8')\nval_labels = np.array(list(x for x in df_val.label)).astype('int8')\n\nimgs = df_train.img\nimgs_val = df_val.img\ndel df_train, df_val","8e4a9b0e":"# load BERT model\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import layers\n\ninput_word_ids = tf.keras.layers.Input(shape=(max_text_length,), dtype=tf.int32, name=\"input_word_ids\")\ninput_mask = tf.keras.layers.Input(shape=(max_text_length,), dtype=tf.int32, name=\"input_mask\")\nsegment_ids = tf.keras.layers.Input(shape=(max_text_length,), dtype=tf.int32, name=\"segment_ids\")\nbert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\", trainable=False)\npooled_output, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n\nbert = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=[pooled_output, sequence_output])\nbert.summary()","48d3592c":"# from tensorflow.keras.applications import ResNet50\n\n# ResNet = ResNet50(input_shape=(image_size, image_size,3), include_top=False, weights=\"imagenet\")\n","b85b173d":"# for layer in ResNet.layers:\n#     layer.trainable = False","905cb0f1":"# load Inception model\nfrom tensorflow.keras.applications.inception_v3 import InceptionV3\nfrom tensorflow.keras.applications.inception_v3 import preprocess_input, decode_predictions\ninception = InceptionV3(include_top=False, weights='imagenet', pooling=None, input_shape=(image_size, image_size, 3))\n# inception.summary()","86a8764f":"# freeze all convolutional InceptionV3 layers\nfor layer in inception.layers:\n    layer.trainable = False","1ac4f1c0":"type(df_train)","98444641":"# from tensorflow.keras.optimizers import RMSprop\n\n# img_vector = layers.Flatten()(ResNet.output)\n# img_dense = layers.Dense(512, activation='relu')(img_vector)\n\n# create final model using both Inception and BERT \nimg_vector = layers.GlobalAveragePooling2D()(inception.output)\nimg_dense = layers.Dense(512)(img_vector)\n\nbert_dense = layers.Dense(140, activation=\"relu\")(bert.output[0])\n\nconcat = layers.concatenate([img_dense, bert_dense])\n\ndense1 = layers.Dense(1024, activation=\"relu\")(concat)\n# dense1 = layers.Dense(512, activation=\"relu\")(bert_dense)\ndense_dropout1 = layers.Dropout(0.5)(dense1)\n# dense2 = layers.Dense(512, activation=\"relu\")(dense_dropout1)\n# dense_dropout2 = layers.Dropout(0.5)(dense2)\n# dense3 = layers.Dense(256, activation=\"relu\")(dense_dropout2)\n# dense_dropout3 = layers.Dropout(0.5)(dense3)\n\npreds = layers.Dense(1, activation='sigmoid', name=\"output\")(dense_dropout1)\n\nmodel = Model(inputs=[inception.input, bert.input], outputs=preds)\n# model = Model(inputs=[bert.input], outputs=preds)\nmodel.summary()","10ce8873":"import tensorflow.keras.backend as K\n\ntrainable_count = np.sum([K.count_params(w) for w in model.trainable_weights])\nnon_trainable_count = np.sum([K.count_params(w) for w in model.non_trainable_weights])\n\nprint('Total params: {:,}'.format(trainable_count + non_trainable_count))\nprint('Trainable params: {:,}'.format(trainable_count))\nprint('Non-trainable params: {:,}'.format(non_trainable_count))","e5df2a33":"# name of inception input layer\nimg_input = model.layers[0].name","945caa94":"# compile model\nmodel.compile(optimizer = 'adam', \n              loss = {\"output\": 'binary_crossentropy'},\n              metrics = {\"output\": \"AUC\"})","bb61fa94":"# checkpointer\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n\nsave_model_path = \"final-best.hdf5\"\ncheckpoint = ModelCheckpoint(save_model_path, \n                             monitor='val_loss', \n                             verbose=1,        \n                             save_best_only=True)","b4bd0d81":"from tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom numpy import expand_dims\n\n# create image data augmentation generator\n# datagen = ImageDataGenerator(brightness_range=[0.2, 1.2], \n#                              rotation_range=180, \n#                              width_shift_range=0.125, \n#                              height_shift_range=0.125, \n#                              zoom_range=[0.925, 1.25], \n#                              vertical_flip=True, \n#                              horizontal_flip=True, \n#                              fill_mode=\"nearest\", \n#                              preprocessing_function=tf.keras.applications.inception_v3.preprocess_input)\n\ndatagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.inception_v3.preprocess_input)\ndatagen_val_test = ImageDataGenerator(preprocessing_function=tf.keras.applications.inception_v3.preprocess_input)","f751a9d8":"import gc\n\n\nbatch_siz = 500\n\nval_images = []\nfor img in imgs_val:\n    processed = datagen_val_test.flow(expand_dims(img, 0), batch_size=1)\n    val_images.append(processed.next())\nval_images = np.array(val_images, dtype=\"float16\").reshape(len(val_images), image_size, image_size, 3)\ndel imgs_val\n\nfor epoch in range(10):\n\n    print(\"\\nStarting Epoch\", epoch+1)\n    start = 0\n    stop = batch_siz\n    print('batch Size : ', stop)\n    while stop < len(imgs):\n        print(stop)\n        train_images = []\n        for img in imgs[start:stop]:\n            processed = datagen.flow(expand_dims(img, 0), batch_size=1)\n            train_images.append(processed.next())\n        train_images = np.array(train_images, dtype=\"float16\").reshape(batch_siz, image_size, image_size, 3)\n        del processed\n\n        # fit final model\n        model.fit({img_input: train_images, \n                  'input_word_ids': input_ids[start:stop], \n                  \"input_mask\": token_type_ids[start:stop], \n                  \"segment_ids\": attention_mask[start:stop]},\n                  {\"output\": train_labels[start:stop]},\n                  validation_data=({img_input: val_images, \n                  'input_word_ids': input_ids_val, \n                  \"input_mask\": token_type_ids_val, \n                  \"segment_ids\": attention_mask_val},\n                  {\"output\": val_labels}),\n                  callbacks=[checkpoint],\n                  shuffle=True, \n                  batch_size=100, \n                  epochs=1)\n        \n        del train_images\n        gc.collect()\n        start += batch_siz\n        stop += batch_siz\n    ","341e4444":"# load best model\nmodel.load_weights('final-best.hdf5')","2cfda0ad":"# create text input from test data\ncreate_text_inputs(df_test)\ninput_ids_test = np.array(list(x for x in df_test.input_ids)).astype('int32')\ntoken_type_ids_test = np.array(list(x for x in df_test.token_type_ids)).astype('int8')\nattention_mask_test = np.array(list(x for x in df_test.attention_mask)).astype('int8')","f32caa0a":"# create image input from test data\n\ntest_images = []\nfor img in df_test.img:\n    processed = datagen_val_test.flow(expand_dims(img, 0), batch_size=1)\n    test_images.append(processed.next())\ntest_images = np.array(test_images, dtype=\"float16\").reshape(len(test_images), image_size, image_size, 3)","913ea20d":"# make predictions on test set\npreds = model.predict({img_input: test_images, \n              'input_word_ids': input_ids_test, \n              \"input_mask\": token_type_ids_test, \n              \"segment_ids\": attention_mask_test})","165ce5df":"# create list of probabilities and binary class labels \npred_probas = [pred[0] for pred in preds]\npred_binary = [pred.argmax() for pred in preds]\n\n","23859d24":"from sklearn.metrics import accuracy_score, roc_auc_score\nfrom sklearn import metrics\nacc = accuracy_score(df_test.label,pred_binary)\nfpr, tpr, thresholds = metrics.roc_curve(df_test.label, pred_binary)\nauc = metrics.auc(fpr, tpr)\nprint('accuracy: ', acc)\nprint('auc score: ', auc)","e5f3e845":"\n\n# method I: plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()","5d62f936":"from sklearn.metrics import confusion_matrix\nmatrix = confusion_matrix(df_test.label, pred_binary)","2f69c813":"print(matrix)","3cd2fb1b":"#Decision Trees","713a5f4f":"Random Forest****","97e00c7e":"**SVM**","a7c85ecf":"# ANN","0563b462":"**KNN**","eda8d249":"> Stochastic Gradient Descent****","ff1aa200":"# Naive Bayes"}}