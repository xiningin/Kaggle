{"cell_type":{"15c836b7":"code","6e442207":"code","294a34ec":"code","e35d8fdf":"code","9b8aa10a":"code","741608fc":"code","a62aabf6":"code","a313166b":"code","ac505b27":"code","71906580":"code","7b3ad40a":"code","e162a400":"code","def03c3a":"code","93211a58":"code","310e9ec6":"code","b0588c56":"code","70ff1c21":"code","2d18591f":"code","ae2bb34e":"code","510b1e65":"code","b8ec9500":"code","f9c25d93":"code","d2bbf93d":"code","9277d151":"code","7982f2fd":"code","22bdb76d":"code","c99c120a":"code","20d59dba":"code","394bbc42":"code","eed78509":"code","f66caae8":"code","e8aa1265":"code","560bdb01":"code","dc9870a0":"code","4a7c3c0e":"code","b4c518b2":"code","8499e5ad":"code","20146c32":"code","0605734a":"code","6696e7d9":"code","e44389af":"code","7f3aba21":"code","be388c57":"code","c0fab903":"code","fbaad920":"code","f4b0ece5":"code","9b90b2e1":"code","f8b4eb6a":"code","6823c988":"code","2739f6c3":"code","fedf360b":"code","4e0d4413":"code","8976bb5c":"code","95cee2a7":"code","ea23772e":"markdown","645d5f7b":"markdown","361b6081":"markdown","b5975478":"markdown","1e341514":"markdown","0e22742a":"markdown","98dae974":"markdown","75edde1b":"markdown","d337f51c":"markdown","54e0b931":"markdown","50a9308a":"markdown","1b3b4540":"markdown","f86ffd40":"markdown","27fbe350":"markdown","f7af513c":"markdown","2601f5b4":"markdown","6e2eadf0":"markdown","f511e50b":"markdown","2db82235":"markdown","340be91a":"markdown","0dc378b3":"markdown","a47865e7":"markdown","009e18c4":"markdown","0708d3e1":"markdown","2e3a4bf7":"markdown","732290e0":"markdown"},"source":{"15c836b7":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn import model_selection\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn import metrics\nfrom keras.layers import Dropout\n\nfrom keras.layers import Dense\nfrom keras.models import Sequential\nplt.style.use('ggplot')\n\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.notebook_repr_html', True)","6e442207":"import os\nprint(os.listdir('..\/input\/titanic\/'))","294a34ec":"# reading dataset\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n# concatenating both datasets\ndf = pd.concat([train, test])\ndf.head()","e35d8fdf":"test.head()","9b8aa10a":"# checking for shape\nprint(train.shape, test.shape, df.shape)","741608fc":"df.describe()","a62aabf6":"# checking dtypes\ndf.dtypes","a313166b":"df.Cabin.unique()","ac505b27":"df.Embarked.unique()","71906580":"df.isnull().sum()","7b3ad40a":"sns.countplot(x='Sex',hue ='Survived', data=train);","e162a400":"sns.countplot(x='Pclass',hue ='Survived', data=train);","def03c3a":"sns.countplot(x='Embarked',hue ='Survived', data=train);","93211a58":"# Replacing female as 0 and Male as 1\ndf['Sex'] = df['Sex'].map({'female': 0, 'male': 1})","310e9ec6":"# Filling all NaN with S\ndf['Embarked'] = df['Embarked'].fillna('S')\n# Replacing S with 0 and C with 1 and Q with 2\ndf['Embarked'] = df['Embarked'].map({'S': 0, 'C': 1, 'Q': 2})","b0588c56":"# Creating new feature by adding SibSp with Parch\ndf['FamilySize'] = df['SibSp'] + df['Parch'] + 1","70ff1c21":"# Creating new feature IsAlone if Family Size is 1\ndf['IsAlone'] = 0\ndf.loc[df['FamilySize'] == 1, 'IsAlone'] = 1","2d18591f":"sns.countplot(x='IsAlone',hue ='Survived', data=df);","ae2bb34e":"# Creating new feature Has_Cabin column\ndf['Has_Cabin'] = df[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)","510b1e65":"sns.countplot(x='Has_Cabin',hue ='Survived', data=df);","b8ec9500":"# Creating new feature by defining range of fare prices\ndf.Fare = df.Fare.fillna(df.Fare.mean())\ndf.loc[ df['Fare'] <= 7.91, 'Fare'] = 0\ndf.loc[(df['Fare'] > 7.91) & (df['Fare'] <= 14.454), 'Fare'] = 1\ndf.loc[(df['Fare'] > 14.454) & (df['Fare'] <= 31), 'Fare']   = 2\ndf.loc[ df['Fare'] > 31, 'Fare'] = 3\ndf['Fare'] = df['Fare'].astype(int)","f9c25d93":"# Filling NaN value for Age column with median\ndf['Age'] = df['Age'].fillna(df['Age'].median())","d2bbf93d":"# Creating new feature from name Title\ndf['Title'] = df['Name'].str.split(\", \", expand=True)[1].str.split(\".\", expand=True)[0]","9277d151":"df.Title.unique()","7982f2fd":"# Count of Unique value for column Title\ndf.Title.value_counts()","22bdb76d":"# Creating new feature from Title column \ndf['Title'] = df['Title'].replace(['Lady','the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ndf['Title'] = df['Title'].replace('Mlle', 'Miss')\ndf['Title'] = df['Title'].replace('Ms', 'Miss')\ndf['Title'] = df['Title'].replace('Mme', 'Mrs')","c99c120a":"sns.countplot(x='Title',hue ='Survived', data=df);","20d59dba":"drop_elements = ['Name', 'Ticket', 'Cabin', 'SibSp']\ndf = df.drop(drop_elements, axis = 1)","394bbc42":"df.head(2)","eed78509":"#### Again Check Missing Value\ndf.isnull().sum()\/(df.isnull().count()*1).sort_values(ascending = False)","f66caae8":"# Export dataset\ndf.to_csv(\"cleaned_data.csv\")","e8aa1265":"np=df.copy()\nnp['Title'] = np['Title'].map({'Mr': 0, 'Miss': 1, 'Mrs': 2, 'Master':4, 'Rare':5})","560bdb01":"colormap = plt.cm.RdBu\nplt.figure(figsize=(14,12))\nplt.title('Pearson Correlation of Features', y=1.05, size=15)\nsns.heatmap(np.astype(float).corr(),linewidths=0.1,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","dc9870a0":"#### Lets map Title column Mr, Miss etc with one hot encoding\ndf = pd.get_dummies(df,prefix=['Title'], drop_first=True)","4a7c3c0e":"X = train.drop(['Survived'], axis=1)\nY = train.Survived\n# Create Train & Test Data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=101)","b4c518b2":"# Resetting index\ndf = df.reset_index(drop=True)","8499e5ad":"# Seperating Train and Test Set\ntrain_set = df[~df.Survived.isnull()]\ntest_set = df[df.Survived.isnull()]","20146c32":"# Shape\nprint(train_set.shape,test_set.shape)","0605734a":"# Defining X and Y\nX = train_set.drop(['Survived'], axis=1)\ny = train_set.Survived\n# Droping target columns\ntest_set = test_set.drop(['Survived'], axis=1)","6696e7d9":"# train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","e44389af":"# Running logistic regression model\nfrom sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression()\nresult = model.fit(X_train, y_train)\nprediction_test = model.predict(X_test)\n# Print the prediction accuracy\nlr = round(metrics.accuracy_score(y_test, prediction_test)*100,2)","7f3aba21":"model.svm = SVC(kernel='linear') \nmodel.svm.fit(X_train,y_train)\npreds = model.svm.predict(X_test)\nsvc = round(metrics.accuracy_score(y_test, preds)*100,2)","be388c57":"from xgboost import XGBClassifier\nmodel = XGBClassifier()\nmodel.fit(X_train, y_train)\npreds = model.predict(X_test)\nxgb = round(metrics.accuracy_score(y_test, preds)*100,2)","c0fab903":"model_rf = RandomForestClassifier(min_samples_leaf = 3, \n                                       n_estimators=200, \n                                       max_features=0.5, \n                                       n_jobs=-1)\nmodel_rf.fit(X_train, y_train)\n\n# Make predictions\nprediction_test = model_rf.predict(X_test)\nrf = round(metrics.accuracy_score(y_test, prediction_test)*100,2)","fbaad920":"importances = pd.DataFrame({'feature':X_train.columns,'importance':model_rf.feature_importances_})\nimportances = importances.sort_values('importance',ascending=False).set_index('feature')\nimportances","f4b0ece5":"results = pd.DataFrame({\n    'Model': ['Support Vector Machines','Logistic Regression', \n              'Random Forest','XGB Classifier'],\n    'Score': [svc,lr,rf,xgb]})\nresult_df = results.sort_values(by='Score', ascending=False)\nresult_df = result_df.set_index('Score')\nresult_df.head()","9b90b2e1":"import itertools    \ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n# Compute confusion matrix\ncnf_matrix = confusion_matrix(y_test, prediction_test)\n\nnp.set_printoptions(precision=2)\nclass_names = ['Not Survived','Survived']\n# Plot normalized confusion matrix\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=class_names, normalize=True,\n                      title='Normalized confusion matrix')\n\nplt.show()\n\nfrom sklearn.metrics import classification_report\neval_metrics = classification_report(y_test, preds, target_names=class_names)\nprint(eval_metrics)","f8b4eb6a":"#Prediction on test set\ntest_pred = model_rf.predict(test_set)","6823c988":"test_pred","2739f6c3":"# creating series \ndf = pd.Series(test_pred) \n# Changing Dtype\ndf = df.astype('Int64')\n# Provide 'Predicted' as the column name \nndf = pd.DataFrame()\nndf['Survived'] = df","fedf360b":"# Reset Index\ntest = test_set['PassengerId'].reset_index(drop=True)","4e0d4413":"# Submission File\nsubm_file = pd.concat([test,ndf], axis=1)","8976bb5c":"# Export File\nsubm_file.to_csv(\"submission_data.csv\",index=False)","95cee2a7":"#--not to use--#\n# prepare configuration for cross validation test harness\nseed = 7\n# prepare models\nmodels = []\nmodels.append(('LR', LogisticRegression()))\nmodels.append(('LDA', LinearDiscriminantAnalysis()))\nmodels.append(('KNN', KNeighborsClassifier()))\nmodels.append(('CART', DecisionTreeClassifier()))\nmodels.append(('NB', GaussianNB()))\nmodels.append(('RF', DecisionTreeClassifier()))\nmodels.append(('SVM', SVC()))\n# evaluate each model in turn\nresults = []\nnames = []\nscoring = 'accuracy'\nfor name, model in models:\n    kfold = model_selection.KFold(n_splits=10, random_state=seed)\n    cv_results = model_selection.cross_val_score(model, X, Y, cv=kfold, scoring=scoring)\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)\n# boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","ea23772e":"# Section 1: Import Library","645d5f7b":"#### Number of survived and not survived passengers by HasCabin","361b6081":"#### Number of survived and not survived passengers by Title","b5975478":"### Generate Submission File ","1e341514":"### Heat map! Lets check correlation ","0e22742a":"### Classification Report","98dae974":"#### Number of survived and not survived passengers by is_Alone","75edde1b":"# The Challenge\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\n# Goal\nIt is your job to predict if a passenger survived the sinking of the Titanic or not.\nFor each in the test set, you must predict a 0 or 1 value for the variable.","d337f51c":"#### Number of survived and non survived passengers by Sex","54e0b931":"### SupportVectorClassifier","50a9308a":"# Section 3: EDA and Feature Generation","1b3b4540":"### Best Model","f86ffd40":"### RandomForestClassifier","27fbe350":"### What is the distribution of numerical feature values across the samples?\n\nThis helps us determine, among other early insights, how representative is the training dataset of the actual problem domain.\n\n- Total samples are 891 or 40% of the actual number of passengers on board the Titanic (2,224).\n- Survived is a categorical feature with 0 or 1 values.\n- Around 38% samples survived representative of the actual survival rate at 32%.\n- Most passengers (> 75%) did not travel with parents or children.\n- Nearly 30% of the passengers had siblings and\/or spouse aboard.\n- Fares varied significantly with few passengers (<1%) paying as high as $512.\n- Few elderly passengers (<1%) within age range 65-80.","f7af513c":"#### Feature selection","2601f5b4":"#### Number of survived and not survived passengers by Pclass","6e2eadf0":"# Section 2: Read Data","f511e50b":"# Section 5: Model Building","2db82235":"### LogisticRegression","340be91a":"Foot-notes \ud83d\udc4d \nI'm not a major, so please do let me know in the comments if you feel that I've left out any important technique or if there was any mistake in the content.\n\nDo leave a comment\/upvote :)","0dc378b3":"#### We can replace many titles with a more common name or classify them as Rare","a47865e7":"##### Hence RF are Performing Better we will use this model to predict test set","009e18c4":"#### Number of survived and not survived passengers by Embarked","0708d3e1":"### Feature Importance","2e3a4bf7":"#### Loop to Compare all algorithm at once.","732290e0":"### XGB Classifier"}}