{"cell_type":{"7e3f1174":"code","43e01a7a":"code","ce148966":"code","2685e80f":"code","d77bced6":"code","7a47784d":"code","56daf30f":"code","66827f68":"code","76e3890c":"code","7ed697f2":"code","3c5f4c7d":"code","43a80fff":"code","6775ec54":"markdown","d208e171":"markdown","dc1589fc":"markdown","d238daf8":"markdown","4c37d816":"markdown","b9c98c4a":"markdown","aa75bbb3":"markdown"},"source":{"7e3f1174":"!python -m spacy download it_core_news_sm","43e01a7a":"import csv\nimport re\nimport string\nimport pandas as pd\nimport numpy as np\nimport unidecode\nimport math\nimport spacy\nfrom scipy.sparse import hstack\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import train_test_split, ParameterGrid\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.stem import SnowballStemmer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.svm import LinearSVC, SVC\nimport matplotlib.pyplot as plt\n\nfrom wordcloud import WordCloud\nimport it_core_news_sm\nnlp = it_core_news_sm.load()","ce148966":"def custom_csv_print(in_labels):\n    list_to_print = []\n    for index in range(0, len(in_labels)):\n        row_to_print = []\n        row_to_print.append(index)\n        if in_labels[index] == 1:\n            row_to_print.append('pos')\n        else:\n            row_to_print.append('neg')\n\n        list_to_print.append(row_to_print)\n\n    with open('output.csv', 'w', newline='', encoding='utf-8') as f:\n        writer = csv.writer(f)\n        writer.writerow(['Id', 'Predicted'])\n        for index in range(0, len(list_to_print)):\n            writer.writerow(list_to_print[index])\n    return\n\n\ndef custom_import_stopwords(filename):\n    in_stopword_list = []\n    in_flag = 0\n    in_word_cnt = 0\n\n    with open(f'{filename}' + '.csv', encoding=\"utf8\") as f:\n        for row in csv.reader(f):\n            if in_flag == 0:\n                in_flag = 1\n            else:\n                in_stopword_list.append(row[0])\n                in_word_cnt += 1\n\n    print(f\"{in_word_cnt} stopwords imported\")\n    return in_stopword_list","2685e80f":"class LemmaTokenizer(object):\n    def __init__(self):\n        self.lemmatizer = WordNetLemmatizer()\n\n    def __call__(self, review):\n        lemmas = []\n\n        # Remove punctuation\n        translator_1 = str.maketrans(string.punctuation, ' ' * len(string.punctuation))\n        review = review.translate(translator_1)\n\n        # Remove number\n        review = re.sub(r'\\d+', ' ', review)\n\n        # Remove special characters\n        review = re.sub(r\"[^a-zA-Z0-9]+\", ' ', review)\n\n        # Lemmatizer\n        doc = nlp(review)\n        review_list = []\n        for token in doc:\n            review_list.append(token.lemma_)\n        review = ' '.join(review_list)\n\n        # Accents removal\n        review = unidecode.unidecode(review)\n\n        for token in word_tokenize(review):\n            \n            token = token.strip()\n\n            if token not in stopwords and len(token) >= 2:\n                stemmer = SnowballStemmer(\"italian\")\n                token = stemmer.stem(token)\n                if len(token) >= 2:\n                    lemmas.append(token)\n\n        return lemmas\n","d77bced6":"print()\nprint('Data Exploration Phase')\ndf_dev = pd.read_csv('..\/input\/italian-tripadvisor\/development.csv', skiprows=1, names=['review', 'sentiment'])\ndf_eval = pd.read_csv('..\/input\/italian-tripadvisor\/evaluation.csv', skiprows=1, names=['review'])\n# print(df_dev.head())\n# print(df_eval.head())\n# print()\n# print(f'Dimension development dataset: {len(df_dev)}')\nprint(f\"Tot pos: {len(df_dev[df_dev.sentiment == 'pos'])}\")\nprint(f\"Tot neg: {len(df_dev[df_dev.sentiment == 'neg'])}\")\n\n# Mean Length -----------\npos_len_cnt = 0\nneg_len_cnt = 0\ndf_tmp = df_dev\ndf_tmp['review_len'] = df_tmp['review'].apply(len)\ndf_tmp_pos = df_tmp[df_tmp.sentiment == \"pos\"]\ndf_tmp_neg = df_tmp[df_tmp.sentiment == \"neg\"]\n\nvalor_medio_pos = df_tmp_pos[\"review_len\"].mean()\nvalor_medio_neg = df_tmp_neg[\"review_len\"].mean()\n\nprint(f'mean length positive {valor_medio_pos}')\nprint(f'mean length negative {valor_medio_neg}')\n\n# standard deviation\nstd_pos = df_tmp_pos[\"review_len\"].std()\nstd_neg = df_tmp_neg[\"review_len\"].std()\n\nprint(f'Standard deviation positive {std_pos}')\nprint(f'Standard deviation negative {std_neg}')\n\n# null values ---------------------------------\n# print()\n# print(f'Dimension evaluation dataset: {len(df_eval)}')\n# print()\n# print(f'Development dataframe contains NaN values? {df_dev.isnull().values.any()}')\n# print(f'Evaluation dataframe contains NaN values? {df_eval.isnull().values.any()}')\n# print()\n\ndf_dev.loc[df_dev[\"sentiment\"] == 'pos', \"sentiment\"] = 1\ndf_dev.loc[df_dev[\"sentiment\"] == 'neg', \"sentiment\"] = 0\n# print(df_dev.head())","7a47784d":"dataset_reduction_flag = 0\n\nif dataset_reduction_flag == 1:\n    df_dev_sampled = df_dev.sample(frac=0.1)\n    df_eval_sampled = df_eval.sample(frac=0.1)\n\n    df_dev_review = df_dev_sampled.drop(columns=['sentiment'])\n    df_dev_sentiment = df_dev_sampled[\"sentiment\"]\n    df_tot = df_dev_review.append(df_eval_sampled)\n    # print(f'Dimension sampled development + evaluation set: {len(df_tot)}')\n    # print()\nelse:\n    df_dev_review = df_dev.drop(columns=['sentiment'])\n    df_dev_sentiment = df_dev[\"sentiment\"]\n    df_tot = df_dev_review.append(df_eval)\n    # print(f'Dimension development + evaluation set: {len(df_tot)}')\n    # print()","56daf30f":"stopwords = custom_import_stopwords('..\/input\/italian-stopwords\/italian_stopwords')\nprint()\nprint('TF-IDF Phase')\n\nvectorizer = TfidfVectorizer(tokenizer=LemmaTokenizer(), ngram_range=(1, 2))\nX_tfidf = vectorizer.fit_transform(df_tot['review'])\n\nX_train_valid = X_tfidf[:len(df_dev_review)]\nX_eval = X_tfidf[len(df_dev_review):]","66827f68":"X_train, X_test, y_train, y_test = train_test_split(X_train_valid, df_dev_sentiment, test_size=0.25, random_state=1)\ny_train = y_train.astype('int')\ny_test = y_test.astype('int')\nprint()\nprint('Classification Phase')\nclass_type = 'svc'\n","76e3890c":"if class_type == 'naive':\n    print('Multinomia Naive Bayes Classifier')\n\n    hyp_parameters = {\n        \"alpha\": [0.1, 1, 10]\n    }\n\n    for config in ParameterGrid(hyp_parameters):\n        clf = MultinomialNB(**config)\n        clf.fit(X_train, y_train)\n        y_test_pred = clf.predict(X_test)\n        clf_f1_score = f1_score(y_test, y_test_pred, average='weighted')\n        print(f\"f1 Score: {clf_f1_score}\")\n        print(f\"Configuration: {config}\")","7ed697f2":"elif class_type == 'svc':\n    print('SVC Classifier')\n\n    hyp_parameters = {\n        \"random_state\": [0],\n        \"C\": [1, 10, 100, 1000],\n        \"class_weight\": [None, 'balanced', {0: 1, 1: 2}],\n        \"max_iter\": [5000]\n    }\n\n    config_cnt = 0\n    tot_config = 4 * 3\n    max_f1 = 0\n\n    for config in ParameterGrid(hyp_parameters):\n    config_cnt += 1\n    print(f'Analizing config {config_cnt} of {tot_config} || Config: {config}')\n\n    clf = LinearSVC(**config)\n    clf.fit(X_train, y_train)\n    y_test_pred = clf.predict(X_test)\n    clf_f1_score = f1_score(y_test, y_test_pred, average='weighted')\n\n    if clf_f1_score > max_f1:\n    max_f1 = clf_f1_score\n    print(f\"-----> Score: {clf_f1_score}\")\n    print()\n    Config: {'C': 10, 'class_weight': None, 'degree': 2, 'gamma': 0.1,\n              'kernel': 'linear', 'max_iter': 5000, 'random_state': 0}\n    \n    # -----> Score: 0.967768196885848","3c5f4c7d":"exploration_flag = False\n\nif exploration_flag:\n    listytest = y_test.values.tolist()\n    yind = y_test.index.values.tolist()\n\n    index_list1 = []\n    index_list2 = []\n    for i in range(0, len(y_test_pred)):\n        if y_test_pred[i] != listytest[i]:\n            index_list1.append(yind[i])\n            index_list2.append(i)\n\n    cnt = 0\n    for index in index_list1:\n        # print(df_dev_review.iloc[index, 0])\n        # print(f'predicted {y_test_pred[index_list2[cnt]]} but actually {listytest[index_list2[cnt]]}')\n        cnt += 1\n        # print(f'Review index: {index}')\n        # print(f'Totale errori: {len(index_list1)}')\n        # print(f'Totale predetti: {len(y_test_pred)}')\n        # print('-------------------------------------------')\n        # print()","43a80fff":"clf_final = LinearSVC(C=10)\nclf_final.fit(X_train_valid, df_dev_sentiment)\ny_pred_final = clf_final.predict(X_eval)\ncustom_csv_print(y_pred_final)\nprint('Output Generated')","6775ec54":"Setup 2 of 3\n\nWe now define two functions, one to create the submission file and one to import the stopwords","d208e171":"Setup 3 of 3\n\nWe define a class for a custom Tokenizer for the TFIDF representation of the text by mean of TfidfVectorizer","dc1589fc":"if exploration_flag is True you can check the reviews the classifier misclassified","d238daf8":"Setup 1 of 3\n\nLet's first import the libraries we will use.\n\nRemember to have internet setting on","4c37d816":"# ITALIAN TRIPADVISOR REVIEWS SENTIMENT ANALYSIS \nFocus: Hi, in this tutorial notebook, we'll perform a sentiment analysis task, analyzing user\u2019s textual reviews, to\nunderstand if a comment includes a positive or negative mood.\nIn practice, we will build a robust classification model that is able to predict the sentiment contained in a text.\n\nAbout the dataset: The dataset for this competition has been specifically scraped from the tripadvisor.it Italian website. It contains 41077 textual reviews written in the Italian language.\nThe dataset is provided as textual files with multiple lines. Each line is composed of two fields: text and class. The text field contains the review written by the user, while the class field contains a label that can get the following values:\n- pos: if the review shows a positive sentiment.\n- neg: if the review shows a negative sentiment.","b9c98c4a":"TFIDF transformation with custom pre-processing tokenizer is computationally expensive, this may take a while.","aa75bbb3":"Basic first exploration of the dataset"}}