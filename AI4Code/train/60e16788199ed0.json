{"cell_type":{"b5749716":"code","b04cd2c6":"code","bbd6bcdf":"code","38042c1a":"code","00217e3b":"code","d0fa941c":"code","74075d39":"code","8603b6b3":"code","ef563160":"code","8ea719d7":"code","f7123b5b":"code","f0299ad1":"code","7d773c14":"code","1f5aded3":"code","2d8a57c3":"code","2868ec2d":"code","9f24fb8b":"code","2e5ba69a":"code","d3e4db24":"code","60e2f122":"code","c69ad5c7":"code","c9796b37":"code","f438e2eb":"code","f34ba4c5":"code","0f9c74b5":"code","dc064d7f":"code","a5de7347":"code","009f8f33":"code","2e6bcb60":"code","083d1500":"code","af466ccd":"code","71c9d040":"code","67521c5d":"code","2abda1e7":"code","b1efeffd":"code","f888ff5c":"code","970feb98":"code","99d8e5fb":"code","a0e0168d":"code","38ed65d5":"code","86c25c5c":"code","550a1f4b":"code","ba374baf":"markdown","758fcf0f":"markdown","29f5280d":"markdown","e58c7e06":"markdown","89ff45d7":"markdown","9621a2f8":"markdown"},"source":{"b5749716":"!pip install xlrd","b04cd2c6":"!pip install autoviz","bbd6bcdf":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport math\nimport missingno as no\nimport seaborn as sns\nfrom sklearn.linear_model import LogisticRegression \nfrom sklearn.model_selection import train_test_split\n\n%matplotlib inline","38042c1a":"df = pd.read_csv(\"..\/input\/breast-cancer-coimbra-data-set\/dataR2.csv\")","00217e3b":"#checkrows and columns in the data set\ndf.head()","d0fa941c":"df.tail()","74075d39":"df.shape","8603b6b3":"print(df.dtypes)","ef563160":"#Check the miss data\nprint(f' Ckech the miss data:',df.isna().sum(axis=1))\nprint(no.bar(df))","8ea719d7":"df.info()","f7123b5b":"df.quantile()","f0299ad1":"df.describe()","7d773c14":"from autoviz.AutoViz_Class import AutoViz_Class\nAV = AutoViz_Class()\ntarget='Classification'\ndf = AV.AutoViz(filename=\"\",sep=',', depVar=target, dfte=df, header=0, verbose=1, \n                 lowess=False, chart_format='svg', max_rows_analyzed=150000, max_cols_analyzed=30)","1f5aded3":"import pandas_profiling as pp\nprofile = pp.ProfileReport(df, title=\"Breast Cancer Coimbra Disease\")\nprofile","2d8a57c3":"df.hist(bins=50, figsize=(20, 15))\nplt.show()","2868ec2d":"g = sns.pairplot(df, diag_kind=\"Classification\")\ng.map_lower(sns.kdeplot, levels=4, color=\".3\")","9f24fb8b":"sns.pairplot(df,vars=['Age'],hue='Classification',height=5.5)\nsns.pairplot(df,vars=['BMI'],hue='Classification',height=5.5)\nsns.pairplot(df,vars=['Insulin'],hue='Classification',height=5.5)\nsns.pairplot(df,vars=['HOMA'],hue='Classification',height=5.5)\nsns.pairplot(df,vars=['Leptin'],hue='Classification',height=5.5)\nsns.pairplot(df,vars=['Adiponectin'],hue='Classification',height=5.5)\nsns.pairplot(df,vars=['Resistin'],hue='Classification',height=5.5)\nsns.pairplot(df,vars=['MCP.1'],hue='Classification',height=5.5)","2e5ba69a":"#Get the correlation of the columns\ndf.corr()","d3e4db24":"df.columns","60e2f122":"# Correlation ecah one feature\ndf[['Classification','Age']].corr()","c69ad5c7":"df[['Classification','BMI']].corr()","c9796b37":"df[['Classification','Glucose']].corr()","f438e2eb":"df[['Classification','Insulin']].corr()","f34ba4c5":"df[['Classification','HOMA']].corr()","0f9c74b5":"df[['Classification','Leptin']].corr()","dc064d7f":"df[['Classification','Adiponectin']].corr()","a5de7347":"df[['Classification','Resistin']].corr()","009f8f33":"df[['Classification','MCP.1']].corr()","2e6bcb60":"df['Classification'].value_counts()\n\ndf['Classification'].value_counts() * 100 \/ len(df)\n\n\nsns.countplot(x='Classification', data=df, palette='viridis')","083d1500":"df.columns","af466ccd":"#Select the data into independent 'X' and dependent 'Y' variables\nX = df.iloc[:, 0:9].values \nY = df.iloc[:,9].values\n#OR \n#X=df.drop(\"Classification\", axis=1)\n#Y=df[\"Classification\"]\n ","71c9d040":"## If anyone to see the data with features (X)\nall_columns=['Age', 'BMI', 'Glucose', 'Insulin', 'HOMA', 'Leptin', 'Adiponectin','Resistin', 'MCP.1']\n\nX = pd.DataFrame(X,columns=all_columns[0:9])\nall_columns[0:9]\nX.head()","67521c5d":"Y[:]","2abda1e7":"#Split the dataset into 80% Training set and 20% Testing set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.20, random_state = 0)","b1efeffd":"#Feature Scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train = sc.fit_transform(X_train)\nX_test = sc.transform(X_test)\nX_train[:5]","f888ff5c":"#All the models\ndef models(X_train,Y_train):\n  \n  #Using Logistic Regression Algorithm to the Training Set\n  from sklearn.linear_model import LogisticRegression\n  log = LogisticRegression(random_state = 0)\n  log.fit(X_train, Y_train)\n  \n  #Using KNeighborsClassifier Method of neighbors class to use Nearest Neighbor algorithm\n  from sklearn.neighbors import KNeighborsClassifier\n  knn = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\n  knn.fit(X_train, Y_train)\n\n  #Using SVC method of svm class to use Support Vector Machine Algorithm\n  from sklearn.svm import SVC\n  svc_lin = SVC(kernel = 'linear', random_state =0)\n  svc_lin.fit(X_train, Y_train)\n\n  #Using SVC method of svm class to use Kernel SVM Algorithm\n  from sklearn.svm import SVC\n  svc_rbf = SVC(kernel = 'rbf', random_state = 0)\n  svc_rbf.fit(X_train, Y_train)\n\n  #Using GaussianNB method of na\u00efve_bayes class to use Na\u00efve Bayes Algorithm\n  from sklearn.naive_bayes import GaussianNB\n  gauss = GaussianNB()\n  gauss.fit(X_train, Y_train)\n\n  #Using DecisionTreeClassifier of tree class to use Decision Tree Algorithm\n  from sklearn.tree import DecisionTreeClassifier\n  tree =DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\n  tree.fit(X_train, Y_train)\n\n  #Using RandomForestClassifier method of ensemble class to use Random Forest Classification algorithm\n  from sklearn.ensemble import RandomForestClassifier\n  forest = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 10)\n  forest.fit(X_train, Y_train)\n  \n\n\n\n #Using xgboostr method of ensemble class to use xgboostClassification algorithm\n  import xgboost as xgb\n  x_gb= xgb.XGBClassifier(objective=\"binary:logistic\", n_estimators=20, random_state=42, eval_metric=[\"auc\", \"error\", \"error@0.6\"])\n  x_gb.fit(X_train, Y_train)\n\n\n  \n    \n  #print model accuracy on the training data.\n  print('[0]Logistic Regression Training Accuracy:', log.score(X_train, Y_train))\n  print('[1]K Nearest Neighbor Training Accuracy:', knn.score(X_train, Y_train))\n  print('[2]Support Vector Machine (Linear Classifier) Training Accuracy:', svc_lin.score(X_train, Y_train))\n  print('[3]Support Vector Machine (RBF Classifier) Training Accuracy:', svc_rbf.score(X_train, Y_train))\n  print('[4]Gaussian Naive Bayes Training Accuracy:', gauss.score(X_train, Y_train))\n  print('[5]Decision Tree Classifier Training Accuracy:', tree.score(X_train, Y_train))\n  print('[6]Random Forest Classifier Training Accuracy:', forest.score(X_train, Y_train))\n  print('[7]xgboost Classifier Training Accuracy:', x_gb.score(X_train, Y_train))\n  return log, knn, svc_lin, svc_rbf, gauss, tree, forest ,x_gb\n\nmodel = models(X_train,Y_train)","970feb98":"#Show other ways to get the classification accuracy & other metrics \n\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import plot_roc_curve\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import balanced_accuracy_score\nimport numpy as np\nfor i in range(len(model)):\n  print('Model ',i)\n  #Check Accuracy precision, recall, f1-score\n  print( classification_report(Y_test, model[i].predict(X_test)) )\n  #Another way to get the models accuracy on the test data\n  print(F'Accuracy:',accuracy_score(Y_test, model[i].predict(X_test)))\n  print(F'Precision:', precision_score(Y_test, model[i].predict(X_test)))\n  print(F'Recall:', recall_score(Y_test, model[i].predict(X_test)))\n  print(F'F1 Score:', f1_score(Y_test, model[i].predict(X_test)))\n\n  #Check Roc Auc Score\n  print( F'Roc Auc Score:',roc_auc_score(Y_test, model[i].predict(X_test)) )\n  print( F'Balanced Accuracy Score:',balanced_accuracy_score(Y_test, model[i].predict(X_test)) ) \n  print( F'Confusion Matrix:',confusion_matrix(Y_test, model[i].predict(X_test)) )\n  print()#Print a new line","99d8e5fb":"for i in range(len(model)):\n  print('Model ',i)\n# ROC CURVE\n  plot_roc_curve(model[i], X_test, Y_test)\n  plt.title('ROC Curve')\n  plt.plot([0, 1], [0, 1], 'r--')\n  plt.show()","a0e0168d":"#Print Prediction of Random Forest Classifier model\npred = model[1].predict(X_test)\nprint(pred)\n#Print a space\nprint()\n#Print the actual values\nprint(Y_test)","38ed65d5":"#Accuracy Score\nacc_1 = 0.62 \nacc_2 = 0.79\nacc_3 = 0.54\nacc_4 = 0.70\nacc_5 = 0.54\nacc_6 = 0.67\nacc_7 = 0.67\nacc_8 = 0.75\n\n\nresults = pd.DataFrame([[\"Logistic Regression\",acc_1],[\"K-Nearest Neighbor\",acc_2],[\" SVM(Linear)\",acc_3],\n                       [\"SVM(RBF)\",acc_4],[\"Gaussian Naive Bayes\",acc_5],\n                       [\"Decision Tree\",acc_6],[\"Random Forest\",acc_7],[\"Xgboost\",acc_8]],\n                        columns = [\"Models\",\"Accuracy Score\"]).sort_values(by='Accuracy Score',ascending=False)\n\n\nresults.style.background_gradient(cmap='Blues')","86c25c5c":"# Precision Score\nPre_1 = 0.62 \nPre_2 = 0.80\nPre_3 = 0.57\nPre_4 = 0.68\nPre_5 = 0.60\nPre_6 = 0.69\nPre_7 = 0.72\nPre_8 = 0.76\n\n\nresults = pd.DataFrame([[\"Logistic Regression\",Pre_1],[\"K-Nearest Neighbor\",Pre_2],[\" SVM(Linear)\",Pre_3],\n                       [\"SVM(RBF)\",Pre_4],[\"Gaussian Naive Bayes\",Pre_5],\n                       [\"Decision Tree\",Pre_6],[\"Random Forest\",Pre_7],[\"Xgboost\",Pre_8]],\n                        columns = [\"Models\",\"Precision Score\"]).sort_values(by='Precision Score',ascending=False)\n\n\nresults.style.background_gradient(cmap='Blues')","550a1f4b":"# Recall Score\nRec_1 = 0.45 \nRec_2 = 0.76\nRec_3 = 0.61\nRec_4 = 0.71\nRec_5 = 0.46\nRec_6 = 0.69\nRec_7 = 0.61 \nRec_8 = 0.72\n\nresults = pd.DataFrame([[\"Logistic Regression\",Rec_1],[\"K-Nearest Neighbor\",Rec_2],[\" SVM(Linear)\",Rec_3],\n                       [\"SVM(RBF)\",Rec_4],[\"Gaussian Naive Bayes\",Rec_5],\n                       [\"Decision Tree\",Rec_6],[\"Random Forest\",Rec_7],[\"Xgboost\",Rec_8]],\n                        columns = [\"Models\",\"Recall Score\"]).sort_values(by='Recall Score',ascending=False)\n\n\nresults.style.background_gradient(cmap='Blues')","ba374baf":"### 8 Algorithms Classification \n\n#### We used 8 algorithms Classification\n* Linear Regression\n* K-Nearest Neighbor\n* Support Vector Machine(Linear)\n* Support Vector Machine(RBF)\n* Gaussian Naive Bayes\n* Decision Tree\n* Random Forest\n* Xgboost","758fcf0f":"# Good Luck","29f5280d":"### Data Set Information:\n\nThere are 10 predictors, all quantitative, and a binary dependent variable, indicating the presence or absence of breast cancer.\n\nThe predictors are anthropometric data and parameters which can be gathered in routine blood analysis.\n\nPrediction models based on these predictors, if accurate, can potentially be used as a biomarker of breast cancer.\n\n\n### Attribute Information:\n\nQuantitative Attributes:\n* Age (years)\n* BMI (kg\/m2)\n* Glucose (mg\/dL)\n* Insulin (\u00b5U\/mL)\n* HOMA\n* Leptin (ng\/mL)\n* Adiponectin (\u00b5g\/mL)\n* Resistin (ng\/mL)\n* MCP-1(pg\/dL)\n\n### Labels:\n* 1=Healthy controls\n* 2=Patients\n\n\n#### Data set :\n\n[Link](https:\/\/archive.ics.uci.edu\/ml\/datasets\/Breast+Cancer+Coimbra)","e58c7e06":"\n\n<h1 style='background-color:; font-family:newtimeroman; font-size:180%; text-align:center; border-radius: 15px 50px;' > The best score K-Nearest Neighbor <\/h1>\n\n<img src=\"https:\/\/it4agri.com\/wp-content\/uploads\/2021\/04\/breast-cancer-prediction.png\" width=\"500px\">\n\n<h1 style='background-color:; font-family:newtimeroman; font-size:110%; text-align:center; border-radius: 15px 50px;' >  Accuracy : 0.7916666666666666 <\/h1>\n<h1 style='background-color:; font-family:newtimeroman; font-size:110%; text-align:center; border-radius: 15px 50px;' >  Precision : 0.800000000000000 <\/h1>\n<h1 style='background-color:; font-family:newtimeroman; font-size:110%; text-align:center; border-radius: 15px 50px;' >  Recall : 0.7272727272727273 <\/h1>\n<h1 style='background-color:; font-family:newtimeroman; font-size:110%; text-align:center; border-radius: 15px 50px;' >  F1-Score : 0.761904761904762 <\/h1>\n<h1 style='background-color:; font-family:newtimeroman; font-size:110%; text-align:center; border-radius: 15px 50px;' >  Roc Auc Score : 0.7867132867132868 <\/h1>\n<h1 style='background-color:; font-family:newtimeroman; font-size:110%; text-align:center; border-radius: 15px 50px;' >  Balanced Accuracy Score : 0.7867132867132867 <\/h1>\n\n\n\n\n\n\n\n\n\n\n![Upvote!](https:\/\/img.shields.io\/badge\/Upvote-If%20you%20like%20my%20work-07b3c8?style=for-the-badge&logo=kaggle)","89ff45d7":"### Automatically Visualize \nAutoViz performs automatic visualization of any dataset with one line.","9621a2f8":"## \n<h1 style='background-color:#6495ED; font-family:newtimeroman; font-size:200%; text-align:center; border-radius: 15px 50px;' > Breast Cancer  <\/h1>\ncancer that develops from breast tissue. Signs of breast cancer may include a lump in the breast, a change in breast shape, dimpling of the skin, fluid coming from the nipple, a newly inverted nipple, or a red or scaly patch of skin. In those with distant spread of the disease, there may be bone pain, swollen lymph nodes, shortness of breath, or yellow skin.\n\n\n\n[![image.png](attachment:image.png)](http:\/\/)\n"}}