{"cell_type":{"6735f43d":"code","6efa582a":"code","51c27d5c":"code","254b091f":"code","281d5978":"code","7190088a":"code","494db2f1":"code","6781d711":"code","c2dfdf5a":"code","3a431f14":"code","741f80d1":"code","4eb2a278":"code","353bde61":"code","98dd03ca":"code","f678524d":"code","5e3e76a2":"code","beb8418c":"code","051b6bad":"code","6db149e0":"markdown","32f0589a":"markdown","d6d5c9cd":"markdown","42c2b992":"markdown","b9d5691e":"markdown","bee84e25":"markdown"},"source":{"6735f43d":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport cufflinks as cf\nimport sklearn\nfrom sklearn import svm, preprocessing \nimport seaborn as sns\nimport plotly.graph_objs as go\nimport plotly.plotly as py\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\ninit_notebook_mode(connected=True)\nimport os","6efa582a":"df = pd.read_csv('..\/input\/diamonds.csv')\ndf.head()","51c27d5c":"sns.FacetGrid(df, hue = 'cut', height = 6).map(sns.distplot, 'price').add_legend()\nplt.plot()","254b091f":"cut_dict = {'Fair' : 1, 'Good' : 2, 'Very Good' : 3, 'Premium' : 4, 'Ideal' : 5}\nclarity_dict ={ 'I1' : 1, 'SI2' : 2, 'SI1' : 3, 'VS2' : 4, 'VS1' : 5, 'VVS2' : 6, 'VVS1' : 7 , 'IF' : 8}\ncolor_dict = {'D':7, 'E':6, 'F':5, 'G':4, 'H':3, 'I':2, 'J':1}","281d5978":"df['cut'] = df['cut'].map(cut_dict)\ndf['clarity'] = df['clarity'].map(clarity_dict)\ndf['color'] = df['color'].map(color_dict)","7190088a":"df = df.drop('Unnamed: 0', axis = 1)\ndf.head()","494db2f1":"# sns.pairplot(df[['carat','price', 'cut', 'color', 'clarity', 'depth']], hue = 'cut', height = 3)","6781d711":"df.isnull().any()","c2dfdf5a":"df = sklearn.utils.shuffle(df, random_state = 42)\nX = df.drop(['price'], axis = 1).values\nX = preprocessing.scale(X)\ny = df['price'].values\ny = preprocessing.scale(y)","3a431f14":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.model_selection  import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score, mean_squared_error","741f80d1":"# Taking 30% data as test and 70% as training data. \nX_tr, X_test, y_tr, y_test = train_test_split(X, y, test_size = 0.3)","4eb2a278":"# Taking odd integers as K vales so that majority rule can be applied easily. \nneighbors = np.arange(1, 20, 2)\nscores = []","353bde61":"for k in neighbors:   # running for different K values to know which yields the max accuracy. \n    clf = KNeighborsRegressor(n_neighbors = k,  weights = 'distance', p=1)\n    clf.fit(X_tr, y_tr)\n    score = cross_val_score(clf, X_tr, y_tr, cv = 10)\n    scores.append(score.mean())","98dd03ca":"mse = [1-x for x in scores]","f678524d":"trace0 = go.Scatter(\n    y = mse,\n    x = neighbors, # np.arange(1,len(score)+1), \n    mode = 'lines+markers', \n    marker = dict(\n        color = 'rgb(150, 10, 10)'\n    )\n)\nlayout = go.Layout(\n    title = '', \n    xaxis = dict(\n        title = 'K value', \n        tickmode = 'linear'\n    ),\n    yaxis = dict(\n        title = 'CV Error',\n#         range = [0, 10000]\n    )\n)\nfig = go.Figure(data = [trace0], layout = layout)\niplot(fig, filename='basic-line')","5e3e76a2":"optimal_k = neighbors[mse.index(min(mse))]\nprint(\"Optimal K: \", optimal_k)","beb8418c":"# Training the model on Optimal K.\nclf_optimal = KNeighborsRegressor(n_neighbors = optimal_k)\nclf_optimal.fit(X_tr, y_tr)\ny_pred = clf_optimal.predict(X_test)\nacc = clf_optimal.score(X_test, y_test)\nprint(\"Accuracy: \", acc*100)\nprint(\"RMS Error: \", mean_squared_error(y_test, y_pred))","051b6bad":"trace0 = go.Scatter(\n    y = y_test,\n    x = np.arange(200), \n    mode = 'lines', \n    name = 'Actual Price',\n    marker = dict(\n    color = 'rgb(10, 150, 50)')\n)\n\ntrace1 = go.Scatter(\n    y = y_pred,\n    x = np.arange(200), \n    mode = 'lines', \n    name = 'Predicted Price',\n    line = dict(\n        color = 'rgb(110, 50, 140)',\n        dash = 'dot'\n    )\n)\n\n\nlayout = go.Layout(\n    xaxis = dict(title = 'Index'), \n    yaxis = dict(title = 'Normalized Price')\n)\n\nfigure = go.Figure(data = [trace0, trace1], layout = layout)\niplot(figure)","6db149e0":"**Observations:** \n1. Attributes x,y,z define the shape of the diamond. \n2. Price is the value we are predicting here, that means the *y* vector. \n3. Attributes Cut, clarity, and Color are categorical in nature, for efficient working we can convert them to numeric values. \n4. Attribute Unnamed:0 is a additional index value given, as we are storing the data in df, we will use the in pandas index, so this can be removed. ","32f0589a":"The K value with least Error will correspond to the best on, as we can see the graph is having a global minima. ","d6d5c9cd":"###  [2.2] Dropping additional index attribute. ","42c2b992":"## **[3] Data modeling: **","b9d5691e":" ## **[1] Reading Data: **","bee84e25":"## **[2] Preprocessing of Data: **\n### [2.1] Categorical data to Numeric Data"}}