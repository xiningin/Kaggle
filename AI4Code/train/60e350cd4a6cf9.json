{"cell_type":{"b0c78391":"code","d7a96d09":"code","6947c29d":"code","d14195d4":"code","4e38025c":"code","390c1b4d":"code","59279778":"code","9fb984ae":"code","0c8a4913":"code","4d095e19":"code","ab902aff":"code","84715dcc":"code","328e7544":"code","4cf21088":"code","abdfaf72":"code","a40a449f":"code","33e5afcf":"code","c9f46c06":"code","4519eb11":"code","dedd9716":"code","29554a90":"code","4fb9ed87":"markdown","67c9b27b":"markdown","552812df":"markdown","fc06f3af":"markdown","354f1cf9":"markdown","d753c454":"markdown","90a99d3b":"markdown","9fc0627e":"markdown","8fc9bcef":"markdown","8a4f3f61":"markdown","f6263e52":"markdown","cc0bd9de":"markdown","bec5cf28":"markdown","df801ab3":"markdown","57f2d7fe":"markdown","5f048fcd":"markdown","28421bfa":"markdown","ebf6137c":"markdown","0822b4e3":"markdown","415e7f6e":"markdown","8e7689d1":"markdown","9323fe8a":"markdown","3901523b":"markdown","774f8934":"markdown"},"source":{"b0c78391":"# Loading necessary packages\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import OrdinalEncoder\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import GridSearchCV\nfrom scipy.stats import skew\nfrom pprint import pprint\n\nimport warnings\nwarnings.filterwarnings('ignore')","d7a96d09":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n# Test whether there are duplicates\nassert ~train.duplicated().any()\nassert ~test.duplicated().any()","6947c29d":"train.head()","d14195d4":"print(\"Train size: {} \".format(train.shape)) \nprint(\"Test size: {} \".format(test.shape)) ","4e38025c":"# Number of training points\nnum_train = train.shape[0]\ntrain.info()","390c1b4d":"f, ax = plt.subplots(figsize=(8, 7))\nsns.distplot(train['SalePrice'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)","59279778":"# log(1+x) transform the target\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\nf, ax = plt.subplots(figsize=(8, 7))\nsns.distplot(train['SalePrice'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\n\ny_train = train['SalePrice']","9fb984ae":"corr = train.corr()\nplt.subplots(figsize=(15,12))\nsns.heatmap(corr, vmax=0.9, square=True)","0c8a4913":"def get_missing_info(data):\n    data_missing = (data.isna().sum() \/ len(data)) * 100\n    data_missing = data_missing.drop(data_missing[data_missing == 0].index).sort_values(ascending=False)\n    # Plot the percentage of missing values for the features\n    f, ax = plt.subplots(figsize=(8, 7))\n    plt.xticks(rotation='90')\n    sns.barplot(x = data_missing.index, y = data_missing)\n    plt.xlabel('Features')\n    plt.ylabel('Percentage of missing values')\n    plt.title('Percentage of missing data by feature')\n    \n# Concatenating the training and test sets\nX_all = pd.concat((train.loc[:,'MSSubClass':'SaleCondition'],\n                   test.loc[:,'MSSubClass':'SaleCondition']))\n\nget_missing_info(X_all)","4d095e19":"def impute_missing_categorical(data, col, impute_str):\n    data[col] = data[col].fillna(impute_str)\n    return data\n\n# Replace NA for ordinal\/categorical features where NA means None\ncat_cols = [\"PoolQC\", \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\",\n            \"GarageFinish\", \"GarageQual\", \"GarageCond\", \"GarageType\",\n            \"BsmtExposure\", \"BsmtCond\", \"BsmtQual\", \"BsmtFinType2\",\n            \"BsmtFinType1\"]\n\nfor col in cat_cols:\n    X_all = impute_missing_categorical(X_all, col, \"None\")","ab902aff":"def ordinal_encoder(data, col, order=None):\n    # Retain only non-null values\n    nonulls = np.array(data.loc[:, col].dropna())\n    nonulls = nonulls.reshape(-1,1)\n    if order is None:\n        ord_en = OrdinalEncoder()\n    else:\n        ord_en = OrdinalEncoder(categories = {0:order}) \n    impute_ordinal = ord_en.fit_transform(nonulls)\n    # Assign ordinal encoded values to non-null values\n    data.loc[data[col].notnull(), col] = np.squeeze(impute_ordinal)\n    \n# Encode ordinal features\nord_features=['ExterQual','ExterCond','BsmtQual','BsmtCond',\n              'BsmtExposure','BsmtFinType1', 'BsmtFinType2','HeatingQC',\n              'Functional','FireplaceQu','KitchenQual', 'GarageType',\n              'GarageFinish', 'GarageQual','GarageCond','PoolQC','Fence', \n              'Utilities','CentralAir']\n\n# Order of the levels for each ordinal feature\norders=[['Fa','TA','Gd','Ex'], #ExterQual\n        ['Po','Fa','TA','Gd','Ex'], #ExterCond\n        ['None','Fa','TA','Gd','Ex'], #BsmtQual\n        ['None','Po','Fa','TA','Gd','Ex'], #BsmtCond\n        ['None','No','Mn','Av','Gd'], #BsmtExposure\n        ['None','Unf','LwQ', 'Rec','BLQ','ALQ' , 'GLQ' ], #BsmtFinType1\n        ['None','Unf','LwQ', 'Rec','BLQ','ALQ' , 'GLQ' ], #BsmtFinType2\n        ['Po','Fa','TA','Gd','Ex'], #HeatingQC\n        ['Sev','Maj2','Maj1','Mod','Min2','Min1','Typ'], #Functional\n        ['None','Po','Fa','TA','Gd','Ex'], #FireplaceQu\n        ['Fa','TA','Gd','Ex'], #KitchenQual\n        ['None','Detchd','CarPort','BuiltIn','Basment','Attchd','2Types'], #GarageType\n        ['None','Unf','RFn','Fin'], #GarageFinish\n        ['None','Po','Fa','TA','Gd','Ex'], #GarageQual\n        ['None','Po','Fa','TA','Gd','Ex'], #GarageCond\n        ['None','Fa','Gd','Ex'], #PoolQC\n        ['None','MnWw','GdWo','MnPrv','GdPrv'], #Fence\n        ['ELO','NoSeWa','NoSewr','AllPub'], #Utilities\n        ['N','Y']] #CentralAir\n\nfor i in range(len(orders)):\n    ordinal_encoder(X_all, ord_features[i], orders[i])","84715dcc":"# Training and test sets\nX_train = X_all[:num_train]\nX_test = X_all[num_train:]\n\n# Filling missing values with the median of the variables\nX_train = X_train.fillna(X_train.median())\n# Fill missing values using the training set to avoid data leakage\nX_test = X_test.fillna(X_train.median())","328e7544":"# Concatenate the training and test sets again for feature engineering\nX_all = pd.concat((X_train, X_test))\n\n# Total square feet in each house\nX_all['TotalSqFeet'] = X_all['LotArea'] + X_all['MasVnrArea'] + X_all['TotalBsmtSF'] + X_all['1stFlrSF'] + \\\n                          X_all['2ndFlrSF'] + X_all['LowQualFinSF'] + X_all['GrLivArea'] + X_all['GarageArea'] + \\\n                          X_all['WoodDeckSF'] + X_all['OpenPorchSF'] + X_all['EnclosedPorch'] + X_all['3SsnPorch'] + \\\n                          X_all['ScreenPorch'] + X_all['PoolArea']\n\n# Total number of bathrooms in the houses\nX_all['TotalBath'] = X_all['BsmtFullBath'] + 0.5*X_all['BsmtHalfBath'] + X_all['FullBath'] + 0.5*X_all['HalfBath']\n\n# Years since remodeling\nX_all['YrSinceRemod'] = X_all['YrSold'] - X_all['YearRemodAdd']\n\n# Indicates whether the house has a miscellaneous feature\nX_all['MiscInd'] = (X_all['MiscFeature']!='None').astype(int)\n\n# Indicates whether the house has a pool\nX_all['PoolInd'] = (X_all['PoolArea'] > 0).astype(int)\n\n# Indicates whether the house was newly built when it was sold\nX_all['NewInd'] = ((X_all['YrSold'] - X_all['YearBuilt']) <= 1).astype(int)","4cf21088":"# One-hot encoding of categorical variables\nX_all = pd.get_dummies(X_all)\n\n# Test whether there are missing values left\nassert ~X_all.isnull().any().any()\n# Test whether there are non-numeric features left\nassert ~(X_all.dtypes == \"object\").any()","abdfaf72":"def transform_split(X_all, y_train, model=''):\n    \n    if model == 'linear':\n        numerical_features=['LotFrontage','LotArea','MasVnrArea','BsmtFinSF1',\n                            'BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF',\n                            '2ndFlrSF','LowQualFinSF','GrLivArea','GarageYrBlt',\n                            'GarageArea','WoodDeckSF','OpenPorchSF','EnclosedPorch',\n                            '3SsnPorch','ScreenPorch','PoolArea','MiscVal','YrSold',\n                            'TotalSqFeet','TotalBath','YrSinceRemod']\n\n        # Log transform skewed numeric features \n        skew_all = skew(X_all[:num_train][numerical_features])\n        skewed_ind = np.where(skew_all > 1)[0].astype(int)\n        skewed_features = [numerical_features[i] for i in skewed_ind]\n        X_all[skewed_features] = np.log1p(X_all[skewed_features])\n\n        # Training and test sets\n        X_train = X_all[:num_train]\n        X_test = X_all[num_train:]\n\n        # Fit the robust scaler on the training set\n        scaler = RobustScaler().fit(X_train[numerical_features])\n        # Scale the features of the training set\n        X_train[numerical_features] = scaler.transform(X_train[numerical_features])\n\n        # Scale the features of the test set\n        X_test[numerical_features] = scaler.transform(X_test[numerical_features])\n    \n    else:\n        # Training and test sets\n        X_train = X_all[:num_train]\n        X_test = X_all[num_train:]\n    \n    # Hold out 10% of the training set for validation\n    X_train, X_valid, y_train, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=1)\n    \n    return X_train, X_valid, y_train, y_valid, X_test","a40a449f":"X_trn, X_valid, y_trn, y_valid, X_test = transform_split(X_all, y_train)\n\n# Parameters currently used by the gradient boosting\nprint('Parameters currently in use:\\n')\npprint(GradientBoostingRegressor().get_params())","33e5afcf":"# Loss function to be optimized\nloss = ['ls','huber']\n# Number of boosting stages\nn_estimators = [int(x) for x in np.linspace(start = 50, stop = 500, num = 11)]\n# Maximum depth of the individual regression estimators\nmax_depth = [2,3]\n# Number of features to consider when looking for the best split\nmax_features = ['auto', 'sqrt', 'log2']\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'loss': loss}\npprint(random_grid)\n\n\n# Use the random grid to search for best hyperparameters\n# First create the base model to tune\ngb = GradientBoostingRegressor(random_state=1)\n# Random search of parameters, using 3 fold cross validation\ngb_random = RandomizedSearchCV(estimator = gb, \n                               param_distributions = random_grid, \n                               n_iter = 100, \n                               cv = 5,\n                               scoring = 'neg_root_mean_squared_error',\n                               verbose=0,\n                               random_state=2,\n                               n_jobs = -1)\n# Fit the random search model\ngb_random.fit(X_trn, y_trn)\n\n\n# Examine the best model\nprint(\"GB average CV error : {:5f}\".format(-np.mean(gb_random.cv_results_['mean_test_score'])))\nprint(\"GB lowest CV error : {:5f}\".format(-np.mean(gb_random.best_score_)))\nprint(gb_random.best_params_)\n\n# Predictions on the validation set\ny_valid_pred = gb_random.best_estimator_.predict(X_valid)\nvalid_pred = pd.DataFrame({'Id': X_valid.index, \n                            'y_valid': y_valid,\n                            'gb': y_valid_pred})\n\n# Using the tuned model to create predictions on the test set\ny_test_pred = gb_random.best_estimator_.predict(X_test)\ntest_pred = pd.DataFrame({'Id': test['Id'], \n                          'gb': y_test_pred})","c9f46c06":"X_trn, X_valid, y_trn, y_valid, X_test = transform_split(X_all, y_train, model='linear')\n\n# Regularization parameter\nalpha = np.logspace(start = -5, stop = 2, num = 20)\n\n# Create the model\nlasso = Lasso(random_state = 1)\n# Instantiate the grid search model\nlasso_grid = GridSearchCV(estimator = lasso, \n                          param_grid = {'alpha': alpha}, \n                          cv = 15,\n                          scoring = 'neg_root_mean_squared_error',\n                          n_jobs = -1, \n                          verbose = 0)\n\n# Fit the grid search to the data\nlasso_grid.fit(X_trn, y_trn)\n\n# Examine the best model\nprint(\"Lasso average CV error : {:5f}\".format(-np.mean(lasso_grid.cv_results_['mean_test_score'])))\nprint(\"Lasso lowest CV error : {:5f}\".format(-np.mean(lasso_grid.best_score_)))\nprint(lasso_grid.best_params_)\n\n# Predictions on the validation set\nvalid_pred['lasso'] = lasso_grid.best_estimator_.predict(X_valid)\n\n# Predictions on the test set\ntest_pred['lasso']= lasso_grid.best_estimator_.predict(X_test)","4519eb11":"# Regularization parameter\nalpha = np.logspace(start = -3, stop = 3, num = 20)\n\n# Create the model\nridge = Ridge(random_state = 1)\n# Instantiate the grid search model\nridge_grid = GridSearchCV(estimator = ridge, \n                          param_grid = {'alpha': alpha}, \n                          cv = 15,\n                          scoring = 'neg_root_mean_squared_error',\n                          n_jobs = -1, \n                          verbose = 0)\n\n# Fit the grid search to the data\nridge_grid.fit(X_trn, y_trn)\n\n# Examine the best model\nprint(\"Ridge average CV error : {:5f}\".format(-np.mean(ridge_grid.cv_results_['mean_test_score'])))\nprint(\"Ridge lowest CV error : {:5f}\".format(-np.mean(ridge_grid.best_score_)))\nprint(ridge_grid.best_params_)\n\n# Predictions on the validation set\nvalid_pred['ridge'] = ridge_grid.best_estimator_.predict(X_valid)\n\n# Predictions on the test set\ntest_pred['ridge']= ridge_grid.best_estimator_.predict(X_test)","dedd9716":"# Predictions on the validation set\nX_valid_pred = valid_pred.iloc[:,2:]\ny_valid = valid_pred['y_valid']\n\n# Fit a meta model on the validation predictions\nmeta = LinearRegression()\nmeta.fit(X_valid_pred, y_valid)\n\n# Predictions on the test set\nX_test_pred = test_pred.iloc[:,1:]\ny_test_final = np.expm1(meta.predict(X_test_pred))\n\nout = pd.DataFrame({'Id': test_pred['Id'], 'SalePrice': y_test_final})\nout.to_csv('lin_stacking.csv', index=False)","29554a90":"# Weighted averaging the models' predictions\ny_test_final = np.expm1(test_pred['gb']*0.4 + test_pred['lasso']*0.4 + test_pred['ridge']*0.2)\nout = pd.DataFrame({'Id': test_pred['Id'], 'SalePrice': y_test_final})\nout.to_csv('avg_stacking.csv', index=False)","4fb9ed87":"We have 79 features and 1460 houses in the training set. The test set contains the same features and has 1459 observations. We can observe below that there are many categorical features in the dataset. Also, there are some missing values which we have to deal with.","67c9b27b":"# 6. Stacking\n\n## 6.1 Linear regression as meta-model\nI combined the predictions of the three models using linear regression as meta-model. I tried out more complex algorithms as well (e.g. lasso with high regularization, shallow gradient boosting regressor) to create an ensemble but they usually led to excessively exploiting information from the training set, i.e. overfitting and worse performance on the test set. I used the three models' predictions on the validation set as features and fitted a linear regression as a meta-model. Then, I used the trained regression model to make predictions on the test set where the independent variables were the predictions on the test set.","552812df":"# 3. Feature engineering\n\nI created some new features based on data understanding to improve the predictive models.\n\n* `TotalSqFeet` : feature where we add up all the numerical features representing some kind of area in the house measured in square feet\n* `TotalBath` : total number of bathrooms in the houses assigning 0.5 weigth to half bathrooms\n* `YrSinceRemod` : feature representing how many years had passed since the house had been remodeled\n* `MiscInd` : dummy variable indicating whether the house has a miscellaneous (usually luxury) feature\n* `PoolInd` : dummy variable indicating whether the house has a pool\n* `NewInd` : dummy variable indicating whether the house was newly built when it was sold","fc06f3af":"## 5.3 Lasso regression\nFirst, I log-transformed the skewed features and then scaled the numeric predictors using robust statistics. I held out the same 10% of the training set as for the gradient boosting model by using the same `random_state` in `train_test_split`. \nI trained a lasso regression model which has the advantage that it also performs feature selection during training by setting the coefficients of irrelevant predictors to zero. As the hyperparameter search space is rather small, I used grid search for hyperparameter tuning. I chose the optimal regularization parameter `alpha` via 15-fold CV where RMSE was the evaluation metric.  In general, it is beneficial to use larger number of folds for cross validation. The more folds we use, the better CV error can estimate the true generalization error of our algorithm. Of course, as in many areas in machine learning, there is a trade-off between the number of folds and the required computational resources. Lastly, I made predictions on the validation and test sets using the trained lasso regression model.","354f1cf9":"Features with the largest fraction of missing values are categorical variables, e.g. PoolQC, MiscFeature. Taking a look at the data description, NA refers to \"No Pool\" for the pool quality feature PoolQC. There are other categorical variables for which NA is used in a similar manner. Therefore, I replaced NA values for these features with \"None\".","d753c454":"# 1. Exploratory Data Analysis\n\nFirst, let's explore the features in the training set.","90a99d3b":"For linear models, it is good practice to apply log-transformation of the response if it is bounded from left at zero but can take arbitrarily large positive values. As I will use linear models for prediction, I applied a log(1+x) tranformation to the response.","9fc0627e":"## 1.1 Target variable\n\nIt is worth taking a look at the histogram of the target variable SalePrice. The plot below shows that the response has a right skewed distribution. ","8fc9bcef":"## 5.2 Gradient boosting\n\nAs described above, I held out 10% of the training set as validation set. By printing the default hyperparameters of the gradient boosting, we can see that the hyperparameter space has high dimensions.","8a4f3f61":"![image.png](attachment:image.png)","f6263e52":"## 2.2 Encoding ordinal features\n\nAs most algorithms cannot handle non-numeric data, I started with the encoding of ordinal features. These features are categorical variables that have natural, ordered categories and the distances between the categories are not known. The order of categories were created based on the data description.","cc0bd9de":"# 7. Results\n\nStacking the three models' predictions using linear regression as meta-model resulted in a competitive performance. I achieved an error of 0.12190 with this approach.","bec5cf28":"It turned out that weighted average of the predictions leads to a better predictive performance. Using this ensemble, I achieved an error of 0.11957.","df801ab3":"## 5.4 Ridge regression\nI used the same training, validation and test sets as for the lasso regression. I tuned the regularization parameter `alpha` via 15-fold CV. Contrary to lasso, ridge regression does not perform feature selection, it only shrinks the coefficients of irrelevant features towards zero. Using the trained ridge regression model, I made predictions on the validation and test sets as before.","57f2d7fe":"## 5.1 Data transformation for linear models\n\nFor linear models, I added further data transformations.\n\nFirst, I identified the skewed numerical features and applied log(1+x) transformation as before. Skewness of the numeric predictors were estimated using only the training set to avoid data leakage. For normally distributed data, the skewness should be around 0, hence I log-transformed variables with skewness larger than 1. This might improve the performance of the linear models as these models usually benefit from normal-like distributed features and target.\n\n\nFurthermore, I scaled the numeric features using robust statistics, i.e. the median and the interquartile range. The robust statistics were calculated using only the training set to avoid data leakage. It is good practice for linear models to scale numeric predictors as features with large ranges can dominate the training of these models. Scaling these variables makes sure that each feature is \"equally important\". On the other hand, tree-based models are invariant to scaling as it does not have any effect on finding the best splits when building the trees.","5f048fcd":"# 4. Encoding categorical features\n\nI used one-hot encoding for the categorical features.","28421bfa":"## 6.2 \"Average\" stacking\n\nAnother ensemble approach is to take the average of the models' predictions. It is a sensible choice here because we can avoid overfitting to the training set with this simple method. As the cross-validation errors provide estimates on the true generalization errors of the algorithms, I took those into account and modified the uniform 1\/3 weights. Gradient boosting and lasso regression had smaller CV errors than ridge regression, so I used the weights (0.4, 0.4, 0.2), respectively.","ebf6137c":"# 2. Data cleansing\n## 2.1 Missing value treatment for some categorical features\nAs we saw before, there are some missing values in the dataset. The figure below depicts the percentage of missing values for the features in the whole dataset.","0822b4e3":"![image.png](attachment:image.png)","415e7f6e":"# 5. Modelling\n\nI trained three different models and stacked their predictions on the test set. In general, stacking works best if we introduce diversity in the models. On the algorithmic level, I achieved this by using three different methods: gradient boosting, lasso and ridge regressions. On the data level, I introduced diversity by log-transforming skewed features and scaling numerical predictors only for the two linear models. A detailed description of my modelling strategy can be seen below.\n1. I split the dataset into training, validation and test sets. The test set was already given, I created the validation set by holding out 10% of the training set. For linear models, I log-transformed the skewed features and scaled the numerical predictors.\n2. I trained a gradient boosting regressor on the training set and tuned the hyperparameters via 5-fold cross validation (CV). As the hyperparameter search space was pretty large in this case, I used random search for the tuning. Then, I used the trained model to create predictions on the validation and test sets.\n3. I trained a lasso regression on the training set and tuned the hyperparameters via 15-fold CV. I used grid search here as the hyperparameter search space was not so large. I used the trained lasso model to create predictions on the validation and test sets.\n4. Lastly, I trained a ridge regression model and used the same hyperparameter tuning strategy as in 3. As before, I made predictions on the validation and test sets.\n5. I used stacking to combine the predictions of the models. Generally, it is good practice to use simpler approaches in ensembling to prevent overfitting to the training set. As a first approach, I stacked the predictions of the three models using a linear regression that was fitted on the validation set. As a second approach, I used weighted average stacking of the predictions.","8e7689d1":"## 2.4 Missing value treatment using median\n\nI filled the missing values using the medians of the features in the training set. As some notebooks mention the presence of potential outliers, the median is a sensible choice as it is robust to outliers. I used the training set medians to impute missing values in the test set to avoid data leakage.","9323fe8a":"## 1.2 Correlation matrix\n\nI visualized the correlation matrix of the variables to see how the features are correlated to SalePrice and to each other.\nIntuitively, overall quality of the house (OverallQual) is highly positively correlated with the response. Another feature that shows a strong positive correlation with SalePrice is GrLivArea which refers to the living area in square feet. Furthermore, in line with intuition, we can observe that the larger the basement or the garage is, the higher the house price is.","3901523b":"# Introduction\nThis notebook gives a detailed description on the approach I used to get into the top 6% in the \"House Prices: Advanced Regression Techniques\" competition at the time of writing this notebook (29\/10\/2020). Credit goes to [Alexandru Papiu](https:\/\/www.kaggle.com\/apapiu\/regularized-linear-models) for some of the great ideas used in his notebook.\n\n![kaggle_rank_2.JPG](attachment:kaggle_rank_2.JPG)\n\nIn the competition's dataset, each row describes a large variety of characteristics of a house. The goal is to predict the sales price of the houses (SalePrice) given these features. Each predictive model is evaluated based on the Root-Mean-Squared-Error (RMSE) between the log of the predicted and the actual SalePrice. Converting RMSE errors to a log scale ensures that errors in predicting expensive houses and cheap houses affect this score equally.\n","774f8934":"I decided to tune the following hyperparameters:\n\n* `loss` : loss function to be optimized when training the model\n* `n_estimator` : the number of boosting stages to perform\n* `max_depth` : maximum depth of the individual regression estimators, it limits the number of nodes in the tree\n* `max_features` : number of features to consider when looking for the best split\n* `min_samples_split`: minimum number of samples required to split a node\n* `min_samples_leaf` : minimum number of samples required to be at a leaf node\n\nWe can see that the hyperparameter search space is quite large. Therefore, for the tuning, I used randomized search over the combinations of hyperparameters with 100 iterations. I chose the best combination of hyperparameters via 5-fold cross validation and using RMSE as evaluation metric. I used the tuned gradient boosting model to make predictions on the validation and test sets. \n"}}