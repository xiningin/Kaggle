{"cell_type":{"7971c00f":"code","51c482a2":"code","c6fa8a14":"code","b7f05c97":"code","0a136147":"code","31b41cc0":"code","97e4af5a":"code","b3e20bcf":"code","735962eb":"code","c49e90f2":"code","a6be33f7":"code","5d2f1055":"code","9c0dae68":"code","5856a1bb":"code","9594923e":"code","eb50bdcc":"code","6c83aed9":"code","86fecc6e":"code","2293eba6":"code","9b8fe825":"code","3e8ed689":"code","1da6f121":"code","8e32d5ae":"code","3769e273":"markdown","a59d3d88":"markdown","ece72d05":"markdown","2e3289a8":"markdown","d5981a09":"markdown","8b024f5a":"markdown","0724b322":"markdown","38a02ff3":"markdown","540bb3ab":"markdown","3759477d":"markdown","7248a670":"markdown","39009757":"markdown","d5cc7b39":"markdown","f25d85fd":"markdown","04aef08e":"markdown","6e0770ea":"markdown","00953fee":"markdown","a7c5b3f0":"markdown","dead7ca5":"markdown","3502f1f8":"markdown","6c11e15c":"markdown","d26db00d":"markdown","9e5a9b49":"markdown","048bc521":"markdown"},"source":{"7971c00f":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport sklearn\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler\nfrom sklearn.model_selection import StratifiedShuffleSplit, ShuffleSplit, GridSearchCV\n\nfrom sklearn.metrics import accuracy_score, log_loss\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier\n\n%matplotlib inline","51c482a2":"train = pd.read_csv('..\/input\/leaf-classification\/train.csv', delimiter=',')\ntest = pd.read_csv('..\/input\/leaf-classification\/test.csv', delimiter=',')","c6fa8a14":"train.head()","b7f05c97":"train.info() # 990 samples, 192 features","0a136147":"train['species'].nunique() # 99 unique species","31b41cc0":"train['species'].value_counts() # each species has 10 samples in training set","97e4af5a":"test.head()","b3e20bcf":"test.info() # Target: classify 594 test samples into 99 species","735962eb":"le = LabelEncoder().fit(train['species'])","c49e90f2":"# encode species in training set\ntrain['label'] = le.transform(train['species'])","a6be33f7":"# drop id & species columns. seperate labels in training set\nlabels = train['label']\ntrain_df = train.drop(columns=['id','species','label'])","5d2f1055":"from sklearn.preprocessing import StandardScaler\n\nscaler = StandardScaler().fit(train_df)\ntrain_scale = pd.DataFrame(scaler.transform(train_df))","9c0dae68":"# create train & validation set. In this case, we dont want just simple random sampling but stratification because of large number of classes (99)\n# stratification will make sure there's an equal number of samples per class in training set\nsss = StratifiedShuffleSplit(n_splits=10, test_size=0.2, random_state=42)\nfor train_index, val_index in sss.split(train_scale,  labels):\n    x_train, x_val = train_scale.iloc[train_index], train_scale.iloc[val_index]\n    y_train, y_val = labels.iloc[train_index], labels.iloc[val_index]","5856a1bb":"print(x_train.shape,y_train.shape)\nprint(x_val.shape, y_val.shape)","9594923e":"y_train.value_counts() # each class has 8 samples in training set","eb50bdcc":"test_id = test['id']\ntest_features = test.drop('id', axis=1)\ntest_features_scale = scaler.transform(test_features)","6c83aed9":"cv_sets = ShuffleSplit(n_splits=10,test_size=0.20,random_state=42)\nclassifiers = [RandomForestClassifier(), SVC(), KNeighborsClassifier()]\nparams = [{'n_estimators' : [3,10,30], 'max_features':[2,4,6,8]},\n          {'kernel':('linear','poly','sigmoid','rbf'),'C':[0.01,0.05,0.025,0.07,0.09,1.0], 'gamma':['scale'], 'probability':[True]},\n          {'n_neighbors': [3,5,7,9]}]","86fecc6e":"best_estimators = []\nfor classifier, param in zip(classifiers, params):\n    grid = GridSearchCV(classifier,param,cv=cv_sets)\n    grid = grid.fit(x_train,y_train)\n    best_estimators.append(grid.best_estimator_)","2293eba6":"best_estimators ","9b8fe825":"for estimator in best_estimators:\n    estimator.fit(x_train, y_train)\n    name = estimator.__class__.__name__\n    \n    print(\"=\"*30)\n    print(name)\n    \n    print('****Results****')\n    print('**Training set**')\n    train_predictions = estimator.predict(x_train)\n    acc = accuracy_score(y_train, train_predictions)\n    print(\"Accuracy: {:.4%}\".format(acc))\n    train_predictions = estimator.predict_proba(x_train)\n    ll = log_loss(y_train, train_predictions)\n    print(\"Log Loss: {}\".format(ll))\n    \n    print('**Validation set**')\n    train_predictions = estimator.predict(x_val)\n    acc = accuracy_score(y_val, train_predictions)\n    print(\"Accuracy: {:.4%}\".format(acc))\n    train_predictions = estimator.predict_proba(x_val)\n    ll = log_loss(y_val, train_predictions)\n    print(\"Log Loss: {}\".format(ll))\n    \nprint(\"=\"*30)","3e8ed689":"pred = best_estimators[2].predict_proba(test_features_scale) # KNeighbors classifer model","1da6f121":"submission = pd.DataFrame(pred, index = test_id, columns = le.classes_ )","8e32d5ae":"submission.to_csv('submission.csv')","3769e273":"## **5. Deploy models on validation set & choose the best one**","a59d3d88":"<a id='LoadData'><\/a>","ece72d05":"<a id='testSet'><\/a>","2e3289a8":"## **II. DATA ANALYSIS**\n* [1. Import libraries](#ImportLib)\n* [2. Load & initially explore data](#LoadData)\n* [3. Preprocess data](#Preprocess)\n* * [3.1 Label encoder](#labelEncoder)\n* * [3.2 Feature scaling](#featureScaling)\n* * [3.3 Split train-validation set](#split)\n* * [3.4 Save test set id & features with scaling](#testSet)\n* [4. Model selection, model training & fine tuning](#model)\n* [5. Deploy models on validation set & choose the best one](#valid)\n* [6. Deploy the chosen model on test set](#test)","d5981a09":"## **3. Preprocess data**","8b024f5a":"<a id='split'><\/a>","0724b322":"<a id='Preprocess'><\/a>","38a02ff3":"<a id='labelEncoder'><\/a>","540bb3ab":"<a id='featureScaling'><\/a>","3759477d":"<a id='model'><\/a>","7248a670":"## **1. Import libraries**","39009757":"<a id='ImportLib'><\/a>","d5cc7b39":"## **6. Deploy the chosen model on test set**","f25d85fd":"### **3.3 Split train - validation set**","04aef08e":"## **4. Model selection, model training & fine tuning**","6e0770ea":"<a id='valid'><\/a>","00953fee":"## **2. Load & initially explore data**","a7c5b3f0":"### **3.1 Label encoder**","dead7ca5":"## **I. PROJECT OBJECTIVE**\n\nThe objective of this machine learning project is to use extracted leaf features, including shape, margin, and texture, to accurately identify 99 species of plants.","3502f1f8":"### **3.2 Feature scaling**","6c11e15c":"KNeighbors Classifier performs well on both training & validation set. The other two classifiers overfit on training set.","d26db00d":"<a id='test'><\/a>","9e5a9b49":"### **3.4 Save test set id & features with scaling**","048bc521":"Three classifiers are considered in this notebook: Random forest classifier, Support vector machine classifier and KNeighbors classifier.\n\nGridSearchCV is used to fine-tune some hyperparameters"}}