{"cell_type":{"25d49461":"code","10ee15c4":"code","ae80ff82":"code","e783c2dd":"code","ba450d25":"code","ad01a9d9":"code","7e5a4292":"code","1c364ec0":"code","4f8accd0":"code","2350489e":"code","67c47614":"code","07e83404":"code","8d1f4b9d":"code","556eb2d4":"code","b6b89326":"code","36309dbb":"code","4c23f269":"code","3af2193b":"code","af32d20e":"code","ffec40d3":"code","966a9109":"markdown","bd58bcf2":"markdown","cfe74a96":"markdown","b9f87a3d":"markdown","e966396c":"markdown","7cc3db08":"markdown","8e438e67":"markdown","2be07786":"markdown","685548e9":"markdown","33b73544":"markdown","ae268ce1":"markdown","2578a77b":"markdown","529ecfd8":"markdown","d5c964bd":"markdown","49a2ca2a":"markdown","18d3c791":"markdown","85f26fd4":"markdown","0f0f4252":"markdown","a7597b94":"markdown","881fe21c":"markdown","df840165":"markdown","03458a05":"markdown","52c3edb6":"markdown","c4cb5b4b":"markdown","1f0583f8":"markdown"},"source":{"25d49461":"# Import os: means import operating system \n# import is a python keyword used to import libraries (libraries are a collection of many useful functions and classes for typically one type)\n# https:\/\/docs.python.org\/3\/library\/os.html\nimport os\n\n# numpy is one of the most commonly used matrix manipulation library. Make sure to check out the official documentations for in-depth understanding\n# https:\/\/numpy.org\/doc\/stable\/user\/whatisnumpy.html\nimport numpy as np\n\n# Let's make an import in order to plot something\n# we import matplotlib to plot the data\nimport matplotlib.pyplot as plt\n\n# This is the import for accessing Image\nfrom PIL import Image\n\n# importing math for performing any mathematical operation\nimport math\n\n# Importing Computervison library\n# https:\/\/docs.opencv.org\/4.x\/\nimport cv2\n\n# Importing tensorflow framework for implementing deep learning part\nimport tensorflow as tf\n\n\nfrom tensorflow.keras.models import Sequential, Model \nfrom tensorflow.keras.layers import Dense, Conv2D, MaxPooling2D, Flatten, Input\nfrom tensorflow.keras.preprocessing.image import load_img, img_to_array \n# to convert image into array\n\n# VGG16 pretrained keras model on imagenet dataset\nfrom tensorflow.keras.applications import VGG16","10ee15c4":"# defining directory paths\n\nROOT_DIR = \"..\/input\/moon-workshop-spartificial\/Moon dataset\"\n\n# path.join from os module join one or more path components intelligently.\nIMAGE_DIR = os.path.join(ROOT_DIR, \"Images\")\nANNOT_DIR = os.path.join(ROOT_DIR, \"annotations\")","ae80ff82":"# listdir generates a list of all the files in the directory and return a python list of string file names\n\nimages_name_list = os.listdir(IMAGE_DIR)\nannots_name_list = os.listdir(ANNOT_DIR)","e783c2dd":"images_name_list","ba450d25":"# sorting your file names list will allow you to just use index on the list, as the names of images and annots are same,\n# if you ask for 5th object in both the list, you will get the image and annotation for that image only\n\nimages_name_list = sorted(images_name_list)\n\n# ------------------------------------------------------------\n# TASK -1\n\n# Similarly perform sorting for \"annots_name_list\".\n\n\n# annots_name_list =\n\n# ------------------------------------------------------------","ad01a9d9":"# to look at the images and annotation names from index 0 to 4 i.e. 5 from starting\n\nprint(annots_name_list[:5])\nprint(images_name_list[:5])","7e5a4292":"# lets check out one of our image\n# again, please follow along the documentations of all the functions used...\nImage.open(os.path.join(IMAGE_DIR, \"1.jpeg\"))","1c364ec0":"# image size, width = height = 224 pixels\n\nIMAGE_SIZE = 224\n\n# X will our main image array and it will contain all the images array\n# first we are defining it as a list\n\nX = []\n\n# using for loop to iterate through all the objects in the images_name_list, objects are file names\nfor img_name in images_name_list:\n    \n    # joining path to image dir to image name to get the full path to the image\n    img_path = os.path.join(IMAGE_DIR, img_name) \n    \n    # imread is used to read an image from file path, it return an array of the image \n    # 1 is used because we want RGB image, if you want to use greyscale image, use 0.\n    img = cv2.imread(img_path, 1)\n    \n    # resize all image to a fix size (224, 224, 3). 3 defines the channels (Red, green blue)\n    img = cv2.resize(img, (IMAGE_SIZE, IMAGE_SIZE))\n    \n    # appending all the images to X, one by one\n    X.append(img)\n    \n# Converting into array\nX = np.asarray(X, dtype = np.float32)\n\n\n\n#----------------------------------------------------------------------------------\n# TASK -2\n\n# reshaping the array of images as required by our ML model\n\n\n# X = X.reshape(# number of image, #height of image, #width of image, channels)\n# HINT: How to get number of images\n    # Use length function: as len(array)\n    \n# X = \n\n#----------------------------------------------------------------------------------\n\n# pixel range is 0-255 currently. \n# we will normalize it between (0-1) for less computation\nX = X\/255.0\n\n# lets check the final shape of the Final Image Array \nprint(X.shape)","4f8accd0":"annot_path = os.path.join(ANNOT_DIR, annots_name_list[1])\n\n# for our python to understand the xml files, we need an XML parser\n# ET from xml module create a tree structure of the whole xml file\n\nimport xml.etree.ElementTree as ET\n\n'''\n<annotation>\n\t<folder>Images<\/folder>\n\t<filename>1.jpeg<\/filename>\n\t<path>\/home\/yash\/Downloads\/Moon dataset\/Images\/1.jpeg<\/path>\n\t<source>\n\t\t<database>Unknown<\/database>\n\t<\/source>\n\t<size>\n\t\t<width>196<\/width>\n\t\t<height>258<\/height>\n\t\t<depth>3<\/depth>\n\t<\/size>\n\t<segmented>0<\/segmented>\n\t<object>\n\t\t<name>moon<\/name>\n\t\t<pose>Unspecified<\/pose>\n\t\t<truncated>0<\/truncated>\n\t\t<difficult>0<\/difficult>\n\t\t<bndbox>\n\t\t\t<xmin>87<\/xmin>\n\t\t\t<ymin>129<\/ymin>\n\t\t\t<xmax>113<\/xmax>\n\t\t\t<ymax>158<\/ymax>\n\t\t<\/bndbox>\n\t<\/object>\n<\/annotation>\n\nThis is what the whole structure of an xml file looks like\n'''\n\n\n# parsing an xml file from the file name\ntree = ET.parse(annot_path)\n\n# getting the root element\nroot = tree.getroot()\n# root has a tag (\"annotation\") and a dictionary of attributes:\n\n# now we need to find the size of our image file\nsize = root.find(\"size\")\n\n# extracting height and width of image file\nh = int(size.find(\"height\").text)\nw = int(size.find(\"width\").text)\n\n# extracting coordinates of bounding box of our moon \nbndbox = root.find(\"object\").find(\"bndbox\")\n\n# top left of image is (0,0) and then it increase to rightwards and downwards\n# xmin and ymin are the coordinates of upper left corner and\n# xmax and ymax are the coordinates of the lower right corner\nxmin = int(bndbox.find(\"xmin\").text)\/w\nxmax = int(bndbox.find(\"xmax\").text)\/w\n\n# ------------------------------------------------------------------------\n\n#TASK - 3\n\n# similarly do it for ymin and ymax too\n\n# HINT: key is devide by height instead of width\n\n# ymin = \n# ymax = \n\n#--------------------------------------------------------------------------\n\n# now here is an important thing to understand\n# remember we resized our image to a specific size (224, 224), but our annotations stayed the same. \n# so what we did here above is reduced the value of coordinates with unitery method \n# and now we will multiply them with our required size to fix them exactly as we need\nxmin = int(xmin * 224)\nxmax = int(xmax * 224)\nymin = int(ymin * 224)\nymax = int(ymax * 224)\n","2350489e":"# defining rectangle function params value\nstart_point = (xmin, ymin)\nend_point = (xmax, ymax)\ncolor = (255, 0, 0)\nthickness = 2\n\n# to access the example image using openCV\n# using rectangle function to make the bounding box on the image\n# https:\/\/docs.opencv.org\/4.x\/d6\/d6e\/group__imgproc__draw.html#ga07d2f74cadcf8e305e810ce8eed13bc9\nexample_img = cv2.rectangle(X[1], start_point, end_point, color, thickness)\n\nprint(start_point)\nprint(end_point)\n\n# to plot the image\nplt.imshow(example_img) \nplt.show()","67c47614":"y = []\n\n# we will do exactly as we did above\nfor annot_name in annots_name_list:\n    annot_path = os.path.join(ANNOT_DIR, annot_name)\n    tree = ET.parse(annot_path)\n    root = tree.getroot()\n    size = root.find(\"size\")\n    h = int(size.find(\"height\").text)\n    w = int(size.find(\"width\").text)\n\n    bndbox = root.find(\"object\").find(\"bndbox\")\n    xmin = int(bndbox.find(\"xmin\").text)\/w\n    xmax = int(bndbox.find(\"xmax\").text)\/w\n    ymin = int(bndbox.find(\"ymin\").text)\/h\n    ymax = int(bndbox.find(\"ymax\").text)\/h\n    \n    y.append([xmin, ymin, xmax, ymax])\n    \ny = np.asarray(y, dtype = np.float32)\nprint(y.shape)\n# For every image of size (224,224,3) we want four values defining the coordinates of the box","07e83404":"y[1]","8d1f4b9d":"# it's a good pratice to divide your data into two sets, training set on which your model will be trained\n# and testing set, on which your model will be tested\n\n\n# ------------------------------------------------------------------------\n\n#TASK - 4\n# devide you data into training and test set\n# HINT: separate first 60 image and annots for training set and other 15 for test set\n# use slicing to separate the array [:]\n\n# training set\nx_train = \n# test images form 60 to end\nx_test = \n# this is training set for labels\ny_train = \n# test set for labels\ny_test = \n\n#--------------------------------------------------------------------------\n\n\n\n","556eb2d4":"# Sequential groups a linear stack of layers into a tf.keras.Model.\nmodel = Sequential()\n\n# https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/Conv2D\nmodel.add(Conv2D(32, (3, 3), activation='relu', input_shape=(224, 224, 3)))\n\n# https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/MaxPool2D\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Conv2D(64, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\nmodel.add(Conv2D(128, (3, 3), activation='relu'))\nmodel.add(MaxPooling2D((2, 2)))\n\n\n\n# model.add(Input(shape=(224,224,3)))\nmodel.add(Flatten())\n# model.add(Dense(1000, activation ='relu'))\nmodel.add(Dense(64, activation ='relu'))\nmodel.add(Dense(4, activation = \"sigmoid\"))\n\nmodel.summary()","b6b89326":"# defining VGG model\nvgg = VGG16(weights=\"imagenet\", include_top=False, input_tensor=Input(shape=(224, 224, 3)))\n\nvgg.trainable = False\n\n# Now flateen the VGG tensor\nflatten = vgg.output\nflatten = Flatten()(flatten)\n\n# adding dense layer of num of units 128 and using activation as Relu\nbboxHead = Dense(128, activation=\"relu\")(flatten)\n\n# adding dense layer of num of units 64 and using activation as Relu\nbboxHead = Dense(64, activation=\"relu\")(bboxHead)\n\n# adding dense layer of num of units 32 and using activation as Relu\nbboxHead = Dense(32, activation=\"relu\")(bboxHead)\n\n\n# ------------------------------------------------------------------------\n\n#TASK - 5\n# create one more dense layer as above \n# HINT: use 4 as number of units and \"sigmoid\" as activation function \n\nbboxHead = \n\n#--------------------------------------------------------------------------\n\n\n# Now defining the model, in which input is VGG and output is the 'BBOXHEAD'\nmodel = Model(inputs=vgg.input, outputs=bboxHead)\n\nmodel.summary() # to print the model summary","36309dbb":"model.compile(optimizer='adam',\n              loss = \"mse\",\n              metrics=['accuracy'])\n\n\n# fit method Trains the model for a fixed number of epochs (iterations on a dataset).\n# here we fit the model into our own dataset and train it for 30 epochs with batch size of 4\n\nhistory = model.fit(x_train, y_train, epochs=2, batch_size = 4)\n","4c23f269":"def bb_iou(boxA, boxB):\n    # determine the (x, y)-coordinates of the intersection rectangle\n    xA = max(boxA[0], boxB[0])\n    yA = max(boxA[1], boxB[1])\n    xB = min(boxA[2], boxB[2])\n    yB = min(boxA[3], boxB[3])\n    # compute the area of intersection rectangle\n    interArea = max(0, xB - xA + 1) * max(0, yB - yA + 1)\n    # compute the area of both the prediction and ground-truth\n    # rectangles\n    boxAArea = (boxA[2] - boxA[0] + 1) * (boxA[3] - boxA[1] + 1)\n    boxBArea = (boxB[2] - boxB[0] + 1) * (boxB[3] - boxB[1] + 1)\n    # compute the intersection over union by taking the intersection\n    # area and dividing it by the sum of prediction + ground-truth\n    # areas - the interesection area\n    iou = interArea \/ float(boxAArea + boxBArea - interArea)\n    # return the intersection over union value\n    return iou","3af2193b":"# prediction for all test set\n# using predict function\npreds = model.predict(x_test)","af32d20e":"preds","ffec40d3":"\nsample_index = 14\n\n# finding coordinates for our prediction and actual bounding boxes\npred_bb = [int(i*224) for i in preds[sample_index]]\nactual_bb = [int(i*224) for i in y_test[sample_index]]\n\n# start point is top left corner and end point is bottom right corner\npred_start_point = (pred_bb[0], pred_bb[1])\npred_end_point = (pred_bb[2], pred_bb[3])\n\nactual_start_point = (actual_bb[0],actual_bb[1])\nactual_end_point = (actual_bb[2], actual_bb[3])\n\n# colors for the box\nred = (255, 0, 0) # actual\ngreen = (0, 255, 0) # preds\nthickness = 1\n\n# ------------------------------------------------------------------------\n\n# TASK - 6\n\n# here is your final task\n# use rectange function as used previously to show the actual and predicted bounded boxes on the test image\n# use image with index as sample_index from test set and predicted start and end points to draw the red bounding box\n\nimg_with_predicted_bb = \n\n#--------------------------------------------------------------------------\n\nimg_with_pred_and_actual_bb = cv2.rectangle(img_with_predicted_bb, actual_start_point, actual_end_point, green, thickness)\n\nprint(\"IOU Score:\", bb_iou(pred_bb, actual_bb))\n\n# to show final image\nplt.figure(figsize= (7,7))\nplt.imshow(img_with_pred_and_actual_bb)\nplt.show()","966a9109":"\n### Import Required libraries\nNote: It would probably will not be the first step to import required libraries because you don't even know what libraries you will need in your project. But it's completely okay to import the library only when you need it later in the code.","bd58bcf2":"\n#### COMPILING the model and FIT the model\n\nHere, we are compiling the model where,\n\nOptimizer: Adam\n\nmeasuring loss using : Mean Square Error\n\nMeasurement metrics : Accuracy matrics","cfe74a96":"### Building our ML Model\n\nNow, you need to do some heavy lifting here. \nFor building our ML model, we will use tensorflow and keras, these libraries makes it very easy to apply ml with very little efforts\n\nNote: The below image is extracted from some other source and it may not exactly represent our model, but it will give you a nice idea of a Convolutional Neural Network, something that we will be using here.  ","b9f87a3d":"\n### Let's start diving into the data ","e966396c":"# Detecting Moon with Machine Learning \n### In this tutorial, you will learn how to use basic functions and libraries used for data analysis and getting started with machine learning. \n\n#### Note: This lession will not be very comprehensive and you may face some problem understanding it if you are a complete beginner but we have attached links to all the official documents to make things easy for you. Hope you get enjoy it!!","7cc3db08":"#### Let's segregate the data into training and test\n\n","8e438e67":"![](http:\/\/assets.newatlas.com\/dims4\/default\/9489527\/2147483647\/strip\/true\/crop\/2000x1333+0+333\/resize\/1200x800!\/quality\/90\/?url=http%3A%2F%2Fnewatlas-brightspot.s3.amazonaws.com%2Farchive%2Fequipment-choices-amateur-astronomy-telescope-mount-eyepiece-14.jpg)","2be07786":"### Let's look at one resized final image with resized bounding box on it ","685548e9":"It would be a better idea if you convert file names to python list","33b73544":"![](http:\/\/www.researchgate.net\/profile\/Jose-Benitez-Andrades\/publication\/339447623\/figure\/fig2\/AS:862056077082627@1582541593714\/A-vanilla-Convolutional-Neural-Network-CNN-representation.png)","ae268ce1":"As you can see, filenames in both lists are perfectly aligned","2578a77b":"![](http:\/\/1.bp.blogspot.com\/-5esBhsQbNBs\/Xm_KagMAxHI\/AAAAAAAAAw0\/OYWXcBy_15kErOaa-HHqP_TZYFxqOCfzQCLcBGAsYHQ\/s1600\/mouse.gif)","529ecfd8":"### Now, let's prepare our images","d5c964bd":"### So, that's all for now. Hope you enjoyed it. Thank you for going through all the code. If you haven't understood some or any part of the code, please do not worry. We are taking care of that also as we have scheduled a complete 8-weeks training program for this purpose.\n \n \n### Please visit our flagship research program in which we will guide you to build a complete AI application for Lunar Rover: https:\/\/www.spartificial.com\/projects\/us=NjE5MTM3YTQxYzE5OGYwMDE2ZGFjNmM2 for more info\n\n\n### Join our discord server to meet some more space ai enthusists: https:\/\/discord.com\/invite\/2rGXShFRnA\n","49a2ca2a":"# Results and Analysis\n\n## Performing prediction with Intersection over union","18d3c791":"# Workshop on AI Applications in Space Exploration - Spartificial\n \n***This notebook is particularly designed for \"AI Applications in Space Exploration\" Workshop by Spartificial Research Space. However, anyone is allowed to use it, but it's only good for complete beginners in Python and ML.\nPlease contact team@spartificial.com or https:\/\/spartificial.com\/ to know more***\n\nThis Coding Workshop is a basic introduction to the students to the concept of Coding in Python, Data Analysis and Machine Learning for Space Exploration and Applications. \n\n## This is an initial part of Our Complete 8-weeks Lunar SpaceAI Research Program  \n\nThe Training Program is specially curated such that participants can learn the basics of how to apply their coding skills in the unique setting of space. Students will learn and apply their knowledge for more advance projects and build bigger applications! Meet fellow learners and take the first step to code in Space.\n\nPlease visit https:\/\/spartificial.com\/projects to know more about the program and get yourself registered.\n\nAt the end of the Training Program, students will be able to:  \n\n    Understand the basics of Python\n    Manipulate mathematical data with Numpy\n    Understand big dataset with Pandas\n    Understand machine learning pipelines\n    Building autonomous spacecraft vision system\n    Learn coding applications for space \n    Take part in building more complex and bigger projects in Space  \n\n*Coding an software application is a technically challenging process, but not impossible. It does requires proficiency in different coding languages. Students will only learn the basics of Python coding language and Machine Learning in this course to get them started on this exciting journey! \n","85f26fd4":"You should see an image with two bounding boxes, the green bb is the actual box and red is the predicted box from the model.\n\n\nSome results can be absured because despite using a very large model, our dataset was very very small campared to the datasets that contains thousands of images. ","0f0f4252":"So, I think now you got some pretty good idea of what we are doing and how we are doing.\n\nA machine learning model typically requires two things, data that will be fed into the model and what we want as the output.\nSo, in our case, we want to feed images of moon into the model, and we want it to predict the position as a bounding box in the image. \n\nAbove we prepared images, X, to feed into the model, and we have have understood what we want from our model.\n\n### Now let's prepare y, our expected output from the model.\n","a7597b94":"#### Preparing our Moon detection boxes in the Image","881fe21c":"## Transfer Learning\n\n####  Let's define here VGG model\n\nIn here we will directly access the trained VGG model using Keras, in which we access trained 'weights' parameters and with standard input size.","df840165":"### Generating final results","03458a05":"## Links to follow along this tutorial to know more about python libraries and modules used in the code\n\n### Matplotlib\nhttps:\/\/matplotlib.org\/stable\/plot_types\/index.html\n\n### Open CV\nhttps:\/\/pypi.org\/project\/opencv-python\/\n\n### tensorflow\nhttps:\/\/www.tensorflow.org\/\n\n### Keras\nhttps:\/\/keras.io\/getting_started\/\n\n### VGG16\nhttps:\/\/keras.io\/api\/applications\/vgg\/","52c3edb6":"## Get predictions from our trained model on the test set","c4cb5b4b":"\n### Understanding the problem-\n\nOur objective of this task is to detect Earth's Moon from images. Technically, we would want a bounding box around the moon in the image. \n\nFirst, if you have created a new notebook in kaggle, then click \"Add data\" given on the right side and then click on \"Saerch by URL\" and paste this URL: https:\/\/www.kaggle.com\/yash92328\/moon-workshop-spartificial and click on \"Add\".\n\n\nIf you followed the steps, you should see some updates on the RHS, in the data section, you will see the dataset folder. \n\nOur dataset for this task is two folders containing 75 images and 75 respective bounding box xml annotations generated from \"labelme\" tool. \n\nYou will get to know more about the data in the later part","1f0583f8":"![](http:\/\/www.pyimagesearch.com\/wp-content\/uploads\/2016\/09\/iou_equation.png)"}}