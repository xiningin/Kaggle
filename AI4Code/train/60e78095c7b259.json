{"cell_type":{"1a6bc70a":"code","7fc2b936":"code","86901afa":"code","829cde5b":"code","c13d21cb":"code","42d00792":"code","d27cc80c":"code","09c6e593":"code","9f898f04":"code","d2f415dc":"code","bed844a9":"code","dbf2f006":"code","dc90accf":"code","cc7e68e4":"code","dd3b5e82":"code","53edbd59":"code","648e8dca":"code","bcaf937a":"code","20d0d20c":"code","9ed03dff":"code","476e1996":"code","f855c5dd":"code","967dbf85":"code","9fccee63":"markdown","0b58acf8":"markdown","e7e1211b":"markdown","7f94d4fc":"markdown","f800f76b":"markdown","c4f3658c":"markdown","b43781f5":"markdown","b3cc03f6":"markdown","516a46bd":"markdown","17450bf4":"markdown","34fdacd4":"markdown","7cfaef50":"markdown","331c4de6":"markdown","8b5cdfc7":"markdown"},"source":{"1a6bc70a":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\n\nnp.random.seed(2)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nimport itertools\n\nfrom keras.utils.np_utils import to_categorical # convert to one-hot-encoding\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D\nfrom keras.optimizers import RMSprop\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau","7fc2b936":"# Load the data\ntrain = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","86901afa":"train.head()","829cde5b":"test.head()","c13d21cb":"Y_train = train[\"label\"]\n\n# Drop 'label' column\nX_train = train.drop(labels = [\"label\"],axis = 1) \n\n# free some space\ndel train \ng = sns.countplot(Y_train)\nY_train.value_counts()","42d00792":"X_train.isnull().any().describe()","d27cc80c":"test.isnull().any().describe()","09c6e593":"# Normalize the data\nX_train = X_train \/ 255.0\ntest = test \/ 255.0\n# # super simple data pre process\n# scale = np.max(X_train)\n# X_train \/= scale\n# test \/= scale\n\n# mean = np.mean(X_train)\n# X_train -= mean\n# mean = np.mean(test)\n# test -= mean\n\n# #visualize scales\n\n# print(\"Max: {}\".format(scale))\n# print(\"Mean: {}\".format(mean))","9f898f04":"X_train = X_train.values.reshape(-1,28,28,1)\ntest = test.values.reshape(-1,28,28,1)","d2f415dc":"# Encode labels to one hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\nY_train = to_categorical(Y_train, num_classes = 10)","bed844a9":"# Set the random seed\nrandom_seed = 2\n# Split the train and the validation set for the fitting\nX_train, X_val, Y_train, Y_val = train_test_split(X_train, Y_train, test_size = 0.1, random_state=random_seed)","dbf2f006":"from keras.models import Model\nfrom keras.layers import *\n\nimg_input=Input(shape=(28,28,1))\nx=Conv2D(16, (3, 3), activation='relu')(img_input)\nx=BatchNormalization()(x)\nx =Activation('relu')(x)\nx=Conv2D(32, (3, 3), activation='relu')(x)\nx=BatchNormalization()(x)\nx =Activation('relu')(x)\nx=MaxPooling2D((2,2))(x)\nx=Conv2D(64, (3, 3), activation='relu')(x)\nx=BatchNormalization()(x)\nx =Activation('relu')(x)\nx=MaxPooling2D((2,2))(x)\nx=Conv2D(128, (3, 3), activation='relu')(x)\nx=BatchNormalization()(x)\nx =Activation('relu')(x)\nx=Flatten()(x)\nx=Dense(64,activation='relu')(x)\nx=Dense(10,activation='softmax')(x)\n\nmodel=Model(img_input,x)\nmodel.summary()","dc90accf":"from keras.optimizers import SGD,Adam\nfrom keras.callbacks import LearningRateScheduler, TensorBoard,ModelCheckpoint\n#################optimization_lossfunction##############################\nadam = Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=1e-08)\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=adam,\n              metrics=['accuracy'])\n\n##################model saving########################################\ncheckpoint = ModelCheckpoint('weights.h5',  # model filename\n                             monitor='val_acc', # quantity to monitor\n                             verbose=1, # verbosity - 0 or 1\n                             save_best_only= True, # The latest best model will not be overwritten\n                             mode='max') # The decision to overwrite model is m\n# Set a learning rate annealer\nlearning_rate_reduction = ReduceLROnPlateau(monitor='val_acc', \n                                            patience=3, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","cc7e68e4":"\ndatagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\ndatagen.fit(X_train)","dd3b5e82":"# Fit the model\nhistory = model.fit_generator(datagen.flow(X_train,Y_train, batch_size=128),\n                              epochs = 60, validation_data = (X_val,Y_val),\n                              verbose = 1, steps_per_epoch=X_train.shape[0] \/\/ 128)","53edbd59":"history_dict = history.history\nhistory_dict.keys()","648e8dca":"# Plot the loss and accuracy curves for training and validation \nfig, ax = plt.subplots(2,1)\nax[0].plot(history.history['loss'], color='b', label=\"Training loss\")\nax[0].plot(history.history['val_loss'], color='r', label=\"validation loss\",axes =ax[0])\nlegend = ax[0].legend(loc='best', shadow=True)\n\nax[1].plot(history.history['acc'], color='b', label=\"Training accuracy\")\nax[1].plot(history.history['val_acc'], color='r',label=\"Validation accuracy\")\nlegend = ax[1].legend(loc='best', shadow=True)","bcaf937a":"y_hat = model.predict(X_val)\ny_pred = np.argmax(y_hat, axis=1)\ny_true = np.argmax(Y_val, axis=1)\ncm = confusion_matrix(y_true, y_pred)\nprint(cm)","20d0d20c":"# Look at confusion matrix \n\n\n\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# Predict the values from the validation dataset\nY_pred = model.predict(X_val)\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(Y_pred,axis = 1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(Y_val,axis = 1) \n# compute the confusion matrix\nconfusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n# plot the confusion matrix\nplot_confusion_matrix(confusion_mtx, classes = range(10)) ","9ed03dff":"# Display some error results \n\n# Errors are difference between predicted labels and true labels\nerrors = (Y_pred_classes - Y_true != 0)\n\nY_pred_classes_errors = Y_pred_classes[errors]\nY_pred_errors = Y_pred[errors]\nY_true_errors = Y_true[errors]\nX_val_errors = X_val[errors]\n\ndef display_errors(errors_index,img_errors,pred_errors, obs_errors):\n    \"\"\" This function shows 6 images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 2\n    ncols = 3\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True)\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((28,28)))\n            ax[row,col].set_title(\"Predicted label :{}\\nTrue label :{}\".format(pred_errors[error],obs_errors[error]))\n            n += 1\n\n# Probabilities of the wrong predicted numbers\nY_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 6 errors \nmost_important_errors = sorted_dela_errors[-6:]\n\n# Show the top 6 errors\ndisplay_errors(most_important_errors, X_val_errors, Y_pred_classes_errors, Y_true_errors)","476e1996":"# #get the predictions for the test data\n# predicted_classes = model.predict(X_val)\n\n# #get the indices to be plotted\n# y_true = Y_val[:, 0]\n# correct = np.nonzero(predicted_classes==y_true)[0]\n# incorrect = np.nonzero(predicted_classes!=y_true)[0]\n# from sklearn.metrics import classification_report\n# target_names = [\"Class {}\".format(i) for i in range(10)]\n# print(classification_report(y_true, predicted_classes, target_names=target_names))\n\n# predicted = model.predict(X_val)\n# print(\"Classification Report:\\n %s:\" % (metrics.classification_report(Y_val, predicted)))","f855c5dd":"\nresults = model.predict(test)\n# select the indix with the maximum probability\nresults = np.argmax(results,axis = 1)\nsubmission = pd.read_csv('..\/input\/sample_submission.csv')\nsubmission[\"Label\"] = results\nsubmission.to_csv('.\/submission.csv', index=False)","967dbf85":"# import pandas as pd\n# import numpy as np\n# import keras.layers.core as core\n# import keras.layers.convolutional as conv\n# import keras.models as models\n# import keras.utils.np_utils as kutils\n\n# # The competition datafiles are in the directory ..\/input\n# # Read competition data files:\n# train = pd.read_csv(\"..\/input\/train.csv\").values\n# test  = pd.read_csv(\"..\/input\/test.csv\").values\n\n# nb_epoch = 1 # Change to 100\n\n# batch_size = 128\n# img_rows, img_cols = 28, 28\n\n# nb_filters_1 = 32 # 64\n# nb_filters_2 = 64 # 128\n# nb_filters_3 = 128 # 256\n# nb_conv = 3\n\n# trainX = train[:, 1:].reshape(train.shape[0], img_rows, img_cols, 1)\n# trainX = trainX.astype(float)\n# trainX \/= 255.0\n\n# trainY = kutils.to_categorical(train[:, 0])\n# nb_classes = trainY.shape[1]\n\n# cnn = models.Sequential()\n\n# cnn.add(conv.Convolution2D(nb_filters_1, nb_conv, nb_conv,  activation=\"relu\", input_shape=(28, 28, 1), border_mode='same'))\n# cnn.add(conv.Convolution2D(nb_filters_1, nb_conv, nb_conv, activation=\"relu\", border_mode='same'))\n# cnn.add(conv.MaxPooling2D(strides=(2,2)))\n\n# cnn.add(conv.Convolution2D(nb_filters_2, nb_conv, nb_conv, activation=\"relu\", border_mode='same'))\n# cnn.add(conv.Convolution2D(nb_filters_2, nb_conv, nb_conv, activation=\"relu\", border_mode='same'))\n# cnn.add(conv.MaxPooling2D(strides=(2,2)))\n\n# #cnn.add(conv.Convolution2D(nb_filters_3, nb_conv, nb_conv, activation=\"relu\", border_mode='same'))\n# #cnn.add(conv.Convolution2D(nb_filters_3, nb_conv, nb_conv, activation=\"relu\", border_mode='same'))\n# #cnn.add(conv.Convolution2D(nb_filters_3, nb_conv, nb_conv, activation=\"relu\", border_mode='same'))\n# #cnn.add(conv.Convolution2D(nb_filters_3, nb_conv, nb_conv, activation=\"relu\", border_mode='same'))\n# #cnn.add(conv.MaxPooling2D(strides=(2,2)))\n\n# cnn.add(core.Flatten())\n# cnn.add(core.Dropout(0.2))\n# cnn.add(core.Dense(128, activation=\"relu\")) # 4096\n# cnn.add(core.Dense(nb_classes, activation=\"softmax\"))\n\n# cnn.summary()\n# cnn.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n\n# cnn.fit(trainX, trainY, batch_size=batch_size, nb_epoch=nb_epoch, verbose=1)\n\n# testX = test.reshape(test.shape[0], 28, 28, 1)\n# testX = testX.astype(float)\n# testX \/= 255.0\n# yPred = cnn.predict_classes(testX)\n# print(yPred.shape)\n# np.savetxt('mnist-vggnet.csv', np.c_[range(1,len(yPred)+1),yPred], delimiter=',', header = 'ImageId,Label', comments = '', fmt='%d')\n\n","9fccee63":"# 3. CNN\n## 3.1 Define the model","0b58acf8":"## 4.2 Confusion matrix","e7e1211b":"## 5 predict results","7f94d4fc":"## 2.6 Split training and valdiation set ","f800f76b":"## 3.3 Data augmentation ","c4f3658c":"## 3.2 Set the optimizer and annealer","b43781f5":"# 2. Data preparation\n## 2.1 Load data","b3cc03f6":"## 2.2 Check for null and missing values","516a46bd":"## 3.4 train the model ","17450bf4":"## 2.5 Label encoding","34fdacd4":"* **1. Introduction**\n* **2. Data preparation**\n    * 2.1 Load data\n    * 2.2 Check for null and missing values\n    * 2.3 Normalization\n    * 2.4 Reshape\n    * 2.5 Label encoding\n    * 2.6 Split training and valdiation set\n* **3. CNN**\n    * 3.1 Define the model\n    * 3.2 Set the optimizer and annealer\n    * 3.3 Data augmentation\n* **4. Evaluate the model**\n    * 4.1 Training and validation curves\n    * 4.2 Confusion matrix\n* **5. Prediction and submition**\n    * 5.1 Predict and Submit results","7cfaef50":"# 4. Evaluate the model\n## 4.1 Training and validation curves","331c4de6":"## 2.3 Normalization","8b5cdfc7":"## 2.3 Reshape"}}