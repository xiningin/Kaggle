{"cell_type":{"021d9128":"code","4c7bc649":"code","04056f44":"code","101138dd":"code","0228881b":"code","aedae767":"code","5ddadc25":"code","2219030e":"code","aa4ba0d5":"code","0630cb85":"code","d2fa37b8":"code","5f84a0e2":"code","71ae4f25":"code","12e98c3c":"code","6d794cff":"code","cc662ebb":"code","aea0c931":"code","0c03b9fb":"code","ad4fb05c":"code","d3b49666":"code","d3117046":"code","28524c68":"code","cc026a15":"code","5ab856d1":"code","96d18ba9":"code","9a139217":"code","82312917":"code","5f244548":"markdown","8146012c":"markdown","e60d0a1d":"markdown","5cf22f52":"markdown","60812fe6":"markdown","74768fbf":"markdown","a72b099c":"markdown","6d2ed730":"markdown","9c7bed76":"markdown","0c51cdb1":"markdown","4f8dead1":"markdown","a5e24be0":"markdown","0ccb0df4":"markdown","e280ac32":"markdown","362fac10":"markdown","f8f281cc":"markdown","1d1b96d1":"markdown","f5f95bf5":"markdown","2d99b90d":"markdown","00c21bd5":"markdown","ce3a4f64":"markdown","0e38489f":"markdown","85408c81":"markdown"},"source":{"021d9128":"from PIL import Image\nimport numpy as np\nimport matplotlib.image as img\nimport os\nimport time\nimport glob\nimport re\nimport matplotlib.pyplot as plt\nimport random\nimport seaborn as sns\nfrom scipy.fftpack import dct ,idct\nfrom sklearn.utils import shuffle\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import Isomap, TSNE\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.cluster import KMeans\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix, classification_report\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.manifold import Isomap, TSNE\nfrom scipy.spatial.distance import cdist\nfrom sklearn import svm\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\nimport pandas as pd","4c7bc649":"#The data set used is a reduced version of the MNIST dataset with 10000 training example\n#and 2000 test examples\n\n#getting train and test data ready\n\ntrain_dir='..\/input\/reduced-mnist\/Reduced MNIST Data\/Reduced Trainging data'\ntest_dir='..\/input\/reduced-mnist\/Reduced MNIST Data\/Reduced Testing data'\ntrain_list=[]\ntest_list=[]\n\nfor i in range(10):\n  # A list of all training,testing file names\n  train_list.append(glob.glob('{}\/{}\/*.jpg'.format(train_dir,i)))\n  test_list.append(glob.glob('{}\/{}\/*.jpg'.format(test_dir,i)))\n\n# Expanding sublists into one list\ntrain_list = [item for sublist in train_list for item in sublist]\ntest_list = [item for sublist in test_list for item in sublist]\n\n#training , test data from lists\ntrain_data = np.array([np.array(Image.open(fname)) \\\n                              for fname in train_list])\ntest_data = np.array([np.array(Image.open(fname)) \\\n                             for fname in test_list])\n\n# Adding a label coressponding each training and testing example \ntrain_label=np.array([list(map(int, re.findall(r'\\b\\d\\b', fname)))[0]  for fname in train_list])\ntest_label=np.array([list(map(int, re.findall(r'\\b\\d\\b', fname)))[0]  for fname in test_list])\n\n# Shuffling the training ,test data while keeping the corresponding label\n# Normalizing data for faster convergence\ntrain_data,train_label = shuffle(train_data\/255, train_label)\ntest_data,test_label= shuffle(test_data\/255, test_label)\n","04056f44":"# checking sizes of train and test data\n\nprint(\"training data shape is \",train_data.shape)\nprint(\"Test data shape is \",test_data.shape)\n\n# checking the min and max of train data after normalization\nprint(\"The min value of a pixel after normalization\",train_data.min())\nprint(\"The max value of a pixel after normalization\",train_data.max())","101138dd":"# plot first 36 images in MNIST after being shuffled\nfig, ax = plt.subplots(6, 6, figsize = (12, 12))\nfig.suptitle('First 36 images in MNIST')\nfig.tight_layout(pad = 0.3, rect = [0, 0, 0.9, 0.9])\nfor x, y in [(i, j) for i in range(6) for j in range(6)]:\n    ax[x, y].imshow(train_data[x + y * 6].reshape((28, 28)), cmap = 'gray')\n    ax[x, y].set_title(train_label[x + y * 6])","0228881b":"# Making sure training and test data are correctly labeled \nrandom_train=random.randint(0,9999)\nrandom_test=random.randint(0,1999)\nx=train_data[random_train,:]\ny=test_data[random_test,:]\n\nfig, axs = plt.subplots(2, 1)\naxs[0].imshow(x,cmap='gray');\naxs[0].set_title(\" Train label of a random training  example  is {} \".format(train_label[random_train]))\naxs[1].imshow(y,cmap='gray');\naxs[1].set_title(\" Test label of a random test example  is {} \".format(test_label[random_test]))\nplt.tight_layout();","aedae767":"# Functions used to extract DCT features\n\n# 2-D DCT feature extractor\ndef dct2(a):\n    return dct(dct(a.T, norm='ortho').T, norm='ortho')\n\n#zigzag of DCT taking only first 180 elements  \ndef zigzag(a):\n \n   x=np.concatenate([np.diagonal(a[::-1,:], i)[::(2*(i % 2)-1)] \\\n                         for i in range(1-a.shape[0], a.shape[0])])\n   \n   return x[0:180]\n\n# 2-D inverse DCT \ndef idct2(a):\n    return idct(idct(a.T, norm='ortho').T, norm='ortho')  \n\ndef get_dct_features(a):\n  ''' Computes DCT features for all examples of training data\n  :a: training data\n  :DCT_ordered: DCT features for a single image in zigzag order \n  :DCT_features: DCT feature of all training examples in zigzag order'''\n  DCT_features=np.zeros((a.shape[0],180))\n\n  for i in range(a.shape[0]):\n    DCT_ordered=zigzag(dct2(a[i]))\n    DCT_features[i]=DCT_ordered\n\n  return DCT_features.reshape((a.shape[0],-1))\n\n    \n    \n\n","5ddadc25":"DCT_features_train=get_dct_features(train_data)\nprint(\"The DCT features for training are now of size {} using DCT coffecients \".format(DCT_features_train.shape))\nDCT_features_test=get_dct_features(test_data)\nprint(\"The DCT features for testing are now of size {} using DCT coffecients \".format(DCT_features_test.shape))","2219030e":"# PCA such that .95 percent of the variance is retained\npca = PCA(.95)\npca.fit(train_data.reshape((train_data.shape[0],784)))\ntrain_pca = pca.transform(train_data.reshape((train_data.shape[0],784)))\ntest_pca = pca.transform(test_data.reshape((test_data.shape[0],784)))\nprint(\"The number of components for 95% varinace is \",pca.n_components_)","aa4ba0d5":"lda = LDA(n_components=9)\nlda_train = lda.fit_transform(train_data.reshape((train_data.shape[0],784)), train_label)\nlda_test = lda.transform(test_data.reshape((test_data.shape[0],784)))\n","0630cb85":"def pred_labeled(y_true, y_pred):\n    \"\"\"Purity score\n        Args:\n            y_true(np.ndarray): n*1 matrix Ground truth labels\n            y_pred(np.ndarray): n*1 matrix Predicted clusters\n\n        Returns:\n            float: Purity score\n    \"\"\"\n    # matrix which will hold the majority-voted labels\n    y_voted_labels = np.zeros(y_true.shape)\n    # Ordering labels\n    ## Labels might be missing e.g with set like 0,2 where 1 is missing\n    ## First find the unique labels, then map the labels to an ordered set\n    ## 0,2 should become 0,1\n    labels = np.unique(y_true)\n    ordered_labels = np.arange(labels.shape[0])\n    for k in range(labels.shape[0]):\n        y_true[y_true==labels[k]] = ordered_labels[k]\n    # Update unique labels\n    labels = np.unique(y_true)\n    # We set the number of bins to be n_classes+2 so that \n    # we count the actual occurence of classes between two consecutive bins\n    # the bigger being excluded [bin_i, bin_i+1[\n    bins = np.concatenate((labels, [np.max(labels)+1]), axis=0)\n\n    for cluster in np.unique(y_pred):\n        hist, _ = np.histogram(y_true[y_pred==cluster], bins=bins)\n        # Find the most present label in the cluster\n        winner = np.argmax(hist)\n        y_voted_labels[y_pred==cluster] = winner\n\n    return y_voted_labels","d2fa37b8":"def purity_score(y_true, y_pred):\n    \"\"\"Purity score\n        Args:\n            y_true(np.ndarray): n*1 matrix Ground truth labels\n            y_pred(np.ndarray): n*1 matrix Predicted clusters\n\n        Returns:\n            float: Purity score\n    \"\"\"\n    # matrix which will hold the majority-voted labels\n    y_voted_labels = np.zeros(y_true.shape)\n    # Ordering labels\n    ## Labels might be missing e.g with set like 0,2 where 1 is missing\n    ## First find the unique labels, then map the labels to an ordered set\n    ## 0,2 should become 0,1\n    labels = np.unique(y_true)\n    ordered_labels = np.arange(labels.shape[0])\n    for k in range(labels.shape[0]):\n        y_true[y_true==labels[k]] = ordered_labels[k]\n    # Update unique labels\n    labels = np.unique(y_true)\n    # We set the number of bins to be n_classes+2 so that \n    # we count the actual occurence of classes between two consecutive bins\n    # the bigger being excluded [bin_i, bin_i+1[\n    bins = np.concatenate((labels, [np.max(labels)+1]), axis=0)\n\n    for cluster in np.unique(y_pred):\n        hist, _ = np.histogram(y_true[y_pred==cluster], bins=bins)\n        # Find the most present label in the cluster\n        winner = np.argmax(hist)\n        y_voted_labels[y_pred==cluster] = winner\n\n    return accuracy_score(y_true, y_voted_labels)","5f84a0e2":"# Function to calculate kmean clusters for required cluster numbers\ndef kmean_cluster(train_data,test_data,test_label):\n    cluster_number = [10,40,160]\n    for i in cluster_number:\n      print(\"Number of clusters per class is :\",int(i\/10))\n      # Initialize the K-Means model\n      kmeans = KMeans(n_clusters = i,n_init=5,max_iter=10000,algorithm='full',random_state=0)\n      # Fitting the model to training set\n      kmeans.fit(train_data)\n      tic = time.time()\n      pred_labels=kmeans.predict(test_data)\n      toc=time.time()\n      print(\"elapsed time =\",round(toc-tic,4),\"sec\")\n      accuracy=purity_score(test_label, pred_labels)\n      print(\"Testing accuracy is : \",accuracy)\n      print(\"\\n\")\n  \n\n     ","71ae4f25":"kmean_cluster(DCT_features_train,DCT_features_test,test_label)","12e98c3c":"kmean_cluster(train_pca,test_pca,test_label)\n","6d794cff":"kmean_cluster(lda_train,lda_test,test_label)","cc662ebb":"def GMM_mix(train_data,test_data,test_label):\n  Mix_number = [10,40,160]\n  for i in Mix_number:\n      print(\"Number of clusters per class is :\",int(i\/10))\n      # Initialize the GMM model\n      GMM = GaussianMixture(n_components=i, n_init = 10, max_iter = 6000, covariance_type = 'diag')\n      # Fitting the model to training set\n      GMM.fit(train_data)      \n      tic = time.time()\n      pred_labels=GMM.predict(test_data)\n      toc=time.time()\n      print(\"elapsed time =\",round(toc-tic,4),\"sec\")\n      accuracy=purity_score(test_label, pred_labels)\n      print(\"Testing accuracy is : \",accuracy)\n      print(\"\\n\")\n  ","aea0c931":"GMM_mix(DCT_features_train,DCT_features_test,test_label)\n","0c03b9fb":"GMM_mix(train_pca,test_pca,test_label)\n","ad4fb05c":"GMM_mix(lda_train,lda_test,test_label)","d3b49666":"def svm_models(training_data,training_labels,testing_data,testing_labels):\n  for kernel in ('linear','poly', 'rbf'):\n    classifier_svm = svm.SVC(kernel=kernel, C=6)\n    tic = time.time()\n    classifier_svm.fit(training_data, training_labels)\n    toc = time.time()\n    print(\"elapsed training time =\",round(toc-tic,4),\"sec\")\n    predicted_labels_train = classifier_svm.predict(training_data)\n    tic = time.time()\n    predicted_labels_test= classifier_svm.predict(testing_data)\n    toc = time.time()\n    print(\"elapsed testing time =\",round(toc-tic,4),\"sec\")\n    print(\"training success ratio with \"+ kernel + \"kernel :\" + str(accuracy_score(predicted_labels_train,training_labels)))\n    print(\"testing success ratio with \"+ kernel + \"kernel :\" + str(accuracy_score(predicted_labels_test,testing_labels)))\n    print('\\n')","d3117046":"svm_models(DCT_features_train,train_label,DCT_features_test,test_label)","28524c68":"svm_models(train_pca,train_label,test_pca,test_label)","cc026a15":"svm_models(lda_train,train_label,lda_test,test_label)","5ab856d1":"def confusion_matrix(labels,pred):\n  # Create a DataFrame with labels and varieties as columns: df\n  df = pd.DataFrame({'Labels': labels, 'predictions': pred})\n\n  # Create crosstab: ct\n  ct = pd.crosstab(df['Labels'], df['predictions'])\n\n  # Display ct\n  display(ct)","96d18ba9":"#kmeans_16 confusion matrix using DCT features\nkmeans = KMeans(n_clusters =160,n_init=5,max_iter=10000,algorithm='full',random_state=0)\nkmeans.fit(DCT_features_train)\npred_labels=kmeans.predict(DCT_features_test)\ny_pred=pred_labeled(test_label, pred_labels)\nconfusion_matrix(test_label,y_pred)","9a139217":"#GMM_16 confusion matrix using DCT features\nGMM = GaussianMixture(n_components=160, n_init = 10, max_iter = 6000, covariance_type = 'diag')\nGMM.fit(DCT_features_train)\npred_labels=GMM.predict(DCT_features_test)\ny_pred=pred_labeled(test_label, pred_labels)\nconfusion_matrix(test_label,y_pred)","82312917":"#svm_rbf confusion matrix using pca features\nclassifier_svm = svm.SVC(kernel='rbf', C=5)\nclassifier_svm.fit(train_pca, train_label)\npredicted_labels_test= classifier_svm.predict(test_pca)\nconfusion_matrix(test_label,predicted_labels_test)","5f244548":"##Exploratory data analysis EDA\n","8146012c":"using lda","e60d0a1d":"#GMM_models","5cf22f52":"\nLDA\n\n","60812fe6":"using DCT","74768fbf":"#Results","a72b099c":"using LDA","6d2ed730":"using pca","9c7bed76":"Using PCA","0c51cdb1":"DCT features","4f8dead1":"#Final conclusions","a5e24be0":"# SVM models ","0ccb0df4":"**ML_CLASSIFIERS**","e280ac32":" Famous ML classifiers (Kmean ,GMM,SVM ) are used on the reduced MNIST datasets to obtain classifications using different features like DCT ,lDAand PCA","362fac10":"\n\n\n#Kmean_models ","f8f281cc":"Using DCT","1d1b96d1":"using LDA","f5f95bf5":"Using DCT","2d99b90d":"ML_classifers are still a strong tool for classifcation even with the progress of  DL.\n\nChoosing the appropriate feature or estimator for your model is critical for its success.\n\nIn this notebook varoius ML classifiers were tried including unsupervised algorithms like k-means, GMM and supervised algorithms like SVM .\n\nFeatures like DCT,PCA and LDA proved to have good representation of data.\n\nLDA features was the most compact as it has 9 components only and achieved the fastest inference time .\n\nSVM achieved the best accuracy with 97.65% using PCA reduced features this is expected as the labeled data improves accuracy.\n\nK-means also achieved quite good accuracy of 94.25% just using clusters of unlabeled data using pca which is amazing !\n\nGMMs accuracy using DCT features reached around 92 % .\n\nobserving the confusion matricies the most confusable digits were 4,9 3,8 and this is expected as some handwrittings make them very close ","00c21bd5":"Confusion matrices for the best results of each classifier","ce3a4f64":"Using PCA","0e38489f":"# generating features","85408c81":"PCA features\n"}}