{"cell_type":{"8a02d23e":"code","d74451ce":"code","ed1f7dfe":"code","da5e18f4":"code","2749e38d":"code","1d78fb3f":"code","6e5c3cd1":"code","de61289a":"markdown","17b7ba20":"markdown","c957e0c0":"markdown","fa579da2":"markdown","837e5458":"markdown","57c1f233":"markdown","12bd0622":"markdown"},"source":{"8a02d23e":"import numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.datasets import load_wine\nfrom sklearn.preprocessing import scale\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.backend import clear_session\nfrom tensorflow.keras.losses import categorical_crossentropy\nfrom tensorflow.keras.optimizers import SGD\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.layers import Dense,Input\nfrom tensorflow.random import set_seed","d74451ce":"### Scale and center features, transform labels into a one-hot encoding vector:\ndef preprocess_data(X, y):\n    X = scale(X)\n    y = to_categorical(y,num_classes = 3,dtype = y.dtype)\n    return (X,y)","ed1f7dfe":"# Load the Wine dataset:\nX,y = load_wine(return_X_y = True)\n# Shape of the features fed to the MLP:\ninput_shape = (13,)\n# Number of classes:\nnum_classes = 3\n# Preprocess data:\nX, y = preprocess_data(X, y)","da5e18f4":"### Construct a simple fully-connected MLP with SGD:\ndef build_MLP(extra_hidden_layer = False, hidden_activation = 'relu'):\n    MLP = Sequential()\n    # Hidden layers (fully connected):\n    if extra_hidden_layer:\n        MLP.add(Dense(20,input_shape = (13,),activation=hidden_activation))    \n    MLP.add(Dense(10,input_shape = (13,),activation=hidden_activation))\n    # Output layer (fully-connected):\n    MLP.add(Dense(3,activation='softmax'))\n    MLP.compile(loss=categorical_crossentropy,\n                optimizer=SGD(),\n                metrics=['accuracy'])\n    return MLP","2749e38d":"# Fix the seed for reproducibility:\nseed = 666\nnp.random.seed(seed)\nset_seed(seed)\n# Number of epochs:\nnum_epochs = 20\n# Train batch size:\ntrain_batch_size = 16\n# Split data into train\/val\/test sets:\nX_train,X_valtest,Y_train,Y_valtest = train_test_split(X,y,train_size = 0.7,random_state = seed)\nX_val,X_test,Y_val,Y_test = train_test_split(X_valtest,Y_valtest,train_size = 0.5,random_state = 555)\n# Load an MLP:\nmodel = build_MLP()\n# Display architecture:\nmodel.summary()\n# Train and validate MLP, store the training history in a variable:\ntraining_history = model.fit(x = X_train,y = Y_train,batch_size = train_batch_size,epochs = num_epochs,validation_data = (X_val,Y_val))\n# Evaluate the model (on the test set):\ntest_eval = model.evaluate(X_test,Y_test)\nprint(\"test loss, test accuracy\", test_eval)\n# Plot training history (train\/validation loss and accuracy values throughout training):\nx_ax = np.arange(0,20,1)\nplt.figure(figsize = (10,8))\nplt.plot(x_ax,training_history.history['accuracy'],label = 'training accuracy')\nplt.plot(x_ax,training_history.history['val_accuracy'],label = 'validation accuracy')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.xticks(x_ax.astype(int))\nplt.yticks(np.arange(0,1,0.1))\nplt.legend()\nplt.show()\n\n","1d78fb3f":"model_extra_layer = build_MLP(extra_hidden_layer = True)\nmodel_extra_layer_softmax = build_MLP(extra_hidden_layer = True,hidden_activation = 'softmax')\nextra_layer = model_extra_layer.fit(x = X_train,y = Y_train,batch_size = train_batch_size,epochs = num_epochs,validation_data = (X_val,Y_val))\nextra_layer_softmax = model_extra_layer_softmax.fit(x = X_train,y = Y_train,batch_size = train_batch_size,epochs = num_epochs,validation_data = (X_val,Y_val))\nplt.figure(figsize = (10,8))\nplt.plot(x_ax,extra_layer.history['val_accuracy'],label = 'extra layer, relu')\nplt.plot(x_ax,extra_layer_softmax.history['val_accuracy'],label = 'extra layer, softmax')\nplt.xlabel('epoch')\nplt.ylabel('accuracy')\nplt.xticks(x_ax.astype(int))\nplt.yticks(np.arange(0,1,0.1))\nplt.legend()\nplt.show()","6e5c3cd1":"# Evaluate performance of the best performing model\ntest_eval = model_extra_layer.evaluate(X_test,Y_test)\nprint(\"test loss, test accuracy\", test_eval)","de61289a":"# Load dependencies","17b7ba20":"From the plot above it is clear that adding an extra layer and using relu as the activation function on the hidden layers yields the best results on the validation dataset.","c957e0c0":"# Data preprocessing","fa579da2":"# MLP (multi-layer perceptron) builder","837e5458":"# Model selection of our MLP","57c1f233":"# Load and preprocess the Wine dataset","12bd0622":"# Train, validate and evaluate a MLP model, and plot the results:"}}