{"cell_type":{"9abbb8e0":"code","cf1add0f":"code","db06c370":"code","09e7e84d":"code","7d05c251":"code","22663665":"code","c22f2d0e":"code","7b0d9e8f":"code","01eef719":"code","247a93fc":"code","72f01878":"code","c5989e38":"code","396230f8":"code","a1c0a1b8":"code","5dfead1d":"code","6506ab0d":"code","c6d5818e":"markdown","e6083770":"markdown","d0c5057d":"markdown","767ed3d1":"markdown","1792e3c1":"markdown","45716eda":"markdown","c493db77":"markdown","a59b254b":"markdown","f8d5ce38":"markdown","092f9f8f":"markdown","6ee0c28f":"markdown","476c0492":"markdown","ee6b7aa9":"markdown","b6964591":"markdown","6cc45495":"markdown","cf15e19f":"markdown","e3fcab53":"markdown","a5c30f17":"markdown"},"source":{"9abbb8e0":"import os\nimport glob\nimport numpy as np\n\nfrom PIL import Image,ImageOps\nimport matplotlib.pyplot as plt\n%matplotlib inline","cf1add0f":"base_dir = '..\/input\/project-dataset-fashion-images\/Fashion_Images_Trimmed\/fashion_dataset_2k'\ndirectory = os.listdir(base_dir)\ndirectory","db06c370":"train_dir = base_dir + '\/train\/'\ntest_dir = base_dir + '\/test\/'","09e7e84d":"# Hint: \n#   You are free to use api available in 'os' library, which returns array of directories inside given path.\n# Or, you can manually prepare the array, as CLASSES = ['folder name1', 'folder name 2' ...]\nCLASSES = ['Accessories','Apparel','Footwear','Personal Care']\nprint(CLASSES)","7d05c251":"# TRAINING DIR: For each class, lets see how many images are there..\nfor imgType in CLASSES:\n    imgTypePath = train_dir + \"\/\" + imgType + \"\/\"\n    print(\"CLASS: \" + imgType + \", Total images: \" + str(len(os.listdir(imgTypePath)))) ","22663665":"# TESTING DIR: For each class, lets see how many images are there..\nfor imgType in CLASSES:\n    imgTypePath = test_dir + \"\/\" + imgType + \"\/\"\n    print(\"CLASS: \" + imgType + \", Total images: \" + str(len(os.listdir(imgTypePath))))","c22f2d0e":"from PIL import ImageOps\ndef convert_to_grayscale(img):\n    #Convert to grayscale\n    return ImageOps.grayscale(img) #Return gray scale image object using ImageOps.\n\ndef reshape_img(img, target_size=(150,150)):\n    #Reshape any image to a fixed shape\n    return img.resize(target_size, Image.ANTIALIAS) #Hint: Use resize() API on image object and pass target size param.\n\ndef display_numpy_img(np_img, img_name=\"Transformed image\"):\n    plt.figure(figsize = (6,6))\n    plt.imshow(np_img, cmap='gray')\n    plt.title(img_name)\n    \ndef transform_image(img_file_path):\n    img_obj = Image.open(img_file_path)\n    #print(img_obj.format)\n    #print(np.array(img_obj).shape)\n    #Perform transformations in series\n    img_obj = convert_to_grayscale(img_obj)\n    img_obj = reshape_img(img_obj, (150,150))\n    np_arr_img = np.array(img_obj)\n    return np_arr_img\n\ndef load_dir_to_numpy(dir_path, maxImgs=400):\n    file_list = glob.glob(dir_path+'\/*')\n    imgs = []\n    #Load image by image\n    imgCount=0\n    for fname in file_list:\n        if imgCount>=maxImgs:\n            break\n        img_np = transform_image(fname)\n        imgs.append(img_np)\n        imgCount = imgCount + 1\n    np_imgs = np.array(imgs)\n    return np_imgs\n\ndef prepare_image_data(dir_path, MAX_IMGS):\n    imgs_arr_X = []\n    data_arr_y = []\n    classIdx = 0;\n    for imgType in CLASSES:\n        IMG_DIR = dir_path + \"\/\" + imgType + \"\/\"\n        #  print(\"IMG_DIR: \" + IMG_DIR)\n        imgs_arr = load_dir_to_numpy(IMG_DIR, MAX_IMGS)\n        #print(imgType + \": \" + str(imgs_arr.shape))\n        imgs_arr_X.extend(imgs_arr)\n        data_y = np.full((imgs_arr.shape[0],1), classIdx)\n        data_arr_y.extend(data_y)\n        classIdx += 1\n    np_img_arr_X = np.array(imgs_arr_X)\n    np_data_arr_y = np.array(data_arr_y)\n    return np_img_arr_X,np_data_arr_y","7b0d9e8f":"train_np_x,train_np_y = prepare_image_data(train_dir, 300)\nprint('train_np_x.shape:', train_np_x.shape)\nprint('train_np_y.shape:', train_np_y.shape)","01eef719":"# Flatten out the 2D image data into 1D vector\ntrain_size = train_np_x.shape[0]\ntrain_np_x = train_np_x.reshape((train_size, -1))\nprint('After reshaping, train_np_x.shape:', train_np_x.shape)","247a93fc":"# Import a model\nfrom sklearn.linear_model import SGDClassifier #Hint: Any algorithm say sklearn.linear_model.SGDClassifier OR sklearn.tree.DecisionTreeClassifier() etc..\nclassifier_1 = SGDClassifier()","72f01878":"classifier_1.fit(train_np_x, train_np_y.reshape(-1))","c5989e38":"test_np_x,test_np_y = prepare_image_data(test_dir, 200)\n\ntest_size = test_np_x.shape[0]\ntest_np_x = test_np_x.reshape((test_size, -1))\nprint('Test shape:', test_np_x.shape)","396230f8":"# Get predicted values for 'test_np_x' using trained 'model' \npredicted_y = classifier_1.predict(test_np_x)","a1c0a1b8":"from sklearn.metrics import accuracy_score,classification_report,confusion_matrix\nactual_y = test_np_y.reshape(-1)\nprint('Accuracy Score:', accuracy_score(actual_y, predicted_y))","5dfead1d":"print(classification_report(actual_y, predicted_y))","6506ab0d":"print(confusion_matrix(actual_y, predicted_y))","c6d5818e":"# Import libraries:","e6083770":"# Prepare CLASSES array","d0c5057d":"## Pipeline helper functions","767ed3d1":"# Prepare testing data","1792e3c1":"# Train the model","45716eda":"# Total no. of images per class in training dataset","c493db77":"# Model training performance report","a59b254b":"# What's the trained model accuracy on test data?","f8d5ce38":"# Prepare training & testing directory paths","092f9f8f":"# List out directories","6ee0c28f":"# Predict using testing data","476c0492":"# Flatten out the 2D image data into 1D vector","ee6b7aa9":"# Certificate Project - 1 : [Fashion Images Classification](https:\/\/www.kaggle.com\/microdegree\/project-dataset-fashion-images)","b6964591":"# Replace all '????' with correct function name\/values.","6cc45495":"# Import & Prepare the model object","cf15e19f":"# Prepare training dataset","e3fcab53":"# Confusion matrix","a5c30f17":"# Total no. of images per class in testing dataset"}}