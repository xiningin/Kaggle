{"cell_type":{"1f9b6a51":"code","37d9b8ff":"code","fa68058d":"code","23ef7e4e":"code","d46eb766":"code","90fb6a5d":"code","62c47826":"code","910daf10":"code","b3520521":"code","980643f5":"code","72b80b58":"code","7379f5e5":"code","200c7fe9":"code","b871fa1b":"code","3309d3b6":"code","48933141":"code","f0f2a19f":"code","c32541cc":"code","fe7b67d3":"code","6ef78322":"code","c18dd2a7":"code","314a8d4b":"code","b63ac687":"code","4792d100":"code","935ad948":"code","bea94845":"code","603dc1b4":"code","28dbd739":"code","b7efe5f9":"code","febf2107":"code","5bc02a4d":"code","ee3d2aad":"code","db742767":"code","2471193d":"code","41e5334b":"code","9d5395f0":"code","d2c8ee97":"code","144ee211":"code","7f33a9cd":"code","b6850c38":"code","bcc8f112":"code","298f4974":"code","fc54297f":"code","8ab58a69":"code","1691bfdc":"markdown","189d5ee9":"markdown","59a736df":"markdown","a9cdab6c":"markdown","7db09c88":"markdown","7f0233db":"markdown","bd96179b":"markdown","3acb5696":"markdown"},"source":{"1f9b6a51":"# Import the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport os\nimport time\nimport warnings\nimport os\nfrom six.moves import urllib\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nwarnings.filterwarnings('ignore')\n%matplotlib inline\nplt.style.use('seaborn')\nfrom scipy.stats import norm, skew","37d9b8ff":"# Scalers\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.utils import shuffle\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.pipeline import FeatureUnion\n\n# Models\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error\n\n#regression\nfrom sklearn.linear_model import LinearRegression,Ridge,Lasso,RidgeCV\nfrom sklearn.ensemble import RandomForestRegressor,BaggingRegressor,GradientBoostingRegressor,AdaBoostRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\n\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\nfrom scipy.stats import reciprocal, uniform\n\n# Cross-validation\nfrom sklearn.model_selection import KFold #for K-fold cross validation\nfrom sklearn.model_selection import cross_val_score #score evaluation\nfrom sklearn.model_selection import cross_val_predict #prediction\nfrom sklearn.model_selection import cross_validate\n\n# GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import RandomizedSearchCV\n\n#Common data processors\nfrom sklearn.preprocessing import OneHotEncoder, LabelEncoder\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.utils import check_array\nfrom scipy import sparse","fa68058d":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')","23ef7e4e":"train.shape","d46eb766":"test.shape","90fb6a5d":"#Drop the id column\n\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","62c47826":"# It seems that the price of recent-built houses are higher\n\nplt.figure(figsize=(15,8))\nsns.boxplot(train.YearBuilt, train.SalePrice)\n\n# From the graph we can surely see plenty of outliers.","910daf10":"plt.figure(figsize=(12,6))\nplt.scatter(x=train.GrLivArea, y=train.SalePrice)\nplt.xlabel(\"GrLivArea\", fontsize=13)\nplt.ylabel(\"SalePrice\", fontsize=13)\nplt.ylim(0,800000)","b3520521":"plt.figure(figsize=(12,6))\nplt.scatter(x=train.OverallQual, y=train.SalePrice)\nplt.xlabel(\"GrLivArea\", fontsize=13)\nplt.ylabel(\"SalePrice\", fontsize=13)\nplt.ylim(0,800000)","980643f5":"train.drop(train[(train['GrLivArea']>=4500) & (train['SalePrice']<300000)].index, inplace=True)\ntrain.reset_index(drop=True, inplace=True)","72b80b58":"# Graphs after removing outliers\nplt.figure(figsize=(12,6))\nplt.scatter(x=train.GrLivArea, y=train.SalePrice)\nplt.xlabel(\"GrLivArea\", fontsize=13)\nplt.ylabel(\"SalePrice\", fontsize=13)\nplt.ylim(0,800000)","7379f5e5":"train.shape","200c7fe9":"train.info()","b871fa1b":"train.describe()","3309d3b6":"# Checking if the log is required for the housing sales price\nplt.subplot(1, 2, 1)\nsns.distplot(train.SalePrice, kde=True, fit = norm)","48933141":"#Plot is right skewed, so we need to normalize this distribution\n\nplt.subplot(1, 2, 2)\nsns.distplot(np.log1p(train.SalePrice + 1), kde=True, fit = norm)\nplt.xlabel('Log SalePrice')","f0f2a19f":"#Applying log to house price\ntrain.SalePrice = np.log1p(train.SalePrice)","c32541cc":"train_y = train.SalePrice.reset_index(drop=True)\ntrain_x = train.drop(['SalePrice'], axis=1)\ntest_x = test","fe7b67d3":"train_x.shape","6ef78322":"test_x.shape","c18dd2a7":"total_features = pd.concat([train_x, test_x]).reset_index(drop=True)\ntotal_features.shape","314a8d4b":"nulls = np.sum(total_features.isnull())\nnullcols = nulls.loc[(nulls != 0)]\ndtypes = total_features.dtypes\ndtypes2 = dtypes.loc[(nulls != 0)]\ninfo = pd.concat([nullcols, dtypes2], axis=1).sort_values(by=0, ascending=False)\nprint(info)\nprint(\"There are\", len(nullcols), \"columns with missing values\")","b63ac687":"total_features['Functional'] = total_features['Functional'].fillna('Typ')\ntotal_features['Electrical'] = total_features['Electrical'].fillna(\"SBrkr\")\ntotal_features['KitchenQual'] = total_features['KitchenQual'].fillna(\"TA\")\n\ntotal_features['Exterior1st'] = total_features['Exterior1st'].fillna(total_features['Exterior1st'].mode()[0])\ntotal_features['Exterior2nd'] = total_features['Exterior2nd'].fillna(total_features['Exterior2nd'].mode()[0])\n\ntotal_features['SaleType'] = total_features['SaleType'].fillna(total_features['SaleType'].mode()[0])","4792d100":"pd.set_option('max_columns', None)\ntotal_features[total_features['PoolArea'] > 0 & total_features['PoolQC'].isnull()]","935ad948":"total_features.loc[2418, 'PoolQC'] = 'Fa'\ntotal_features.loc[2501, 'PoolQC'] = 'Gd'\ntotal_features.loc[2597, 'PoolQC'] = 'Fa'","bea94845":"pd.set_option('max_columns', None)\ntotal_features[(total_features['GarageType'] == 'Detchd') & total_features['GarageYrBlt'].isnull()]","603dc1b4":"total_features.loc[2124, 'GarageYrBlt'] = total_features['GarageYrBlt'].median()\ntotal_features.loc[2574, 'GarageYrBlt'] = total_features['GarageYrBlt'].median()\n\ntotal_features.loc[2124, 'GarageFinish'] = total_features['GarageFinish'].mode()[0]\ntotal_features.loc[2574, 'GarageFinish'] = total_features['GarageFinish'].mode()[0]\n\ntotal_features.loc[2574, 'GarageCars'] = total_features['GarageCars'].median()\n\ntotal_features.loc[2124, 'GarageArea'] = total_features['GarageArea'].median()\ntotal_features.loc[2574, 'GarageArea'] = total_features['GarageArea'].median()\n\ntotal_features.loc[2124, 'GarageQual'] = total_features['GarageQual'].mode()[0]\ntotal_features.loc[2574, 'GarageQual'] = total_features['GarageQual'].mode()[0]\n\ntotal_features.loc[2124, 'GarageCond'] = total_features['GarageCond'].mode()[0]\ntotal_features.loc[2574, 'GarageCond'] = total_features['GarageCond'].mode()[0]","28dbd739":"# Basement Variables with NA, are now filled\n\ntotal_features.loc[332, 'BsmtFinType2'] = 'ALQ' #since smaller than SF1\ntotal_features.loc[947, 'BsmtExposure'] = 'No' \ntotal_features.loc[1485, 'BsmtExposure'] = 'No'\ntotal_features.loc[2038, 'BsmtCond'] = 'TA'\ntotal_features.loc[2183, 'BsmtCond'] = 'TA'\ntotal_features.loc[2215, 'BsmtQual'] = 'Po' #v small basement so let's do Poor.\ntotal_features.loc[2216, 'BsmtQual'] = 'Fa' #similar but a bit bigger.\ntotal_features.loc[2346, 'BsmtExposure'] = 'No' #unfinished bsmt so prob not.\ntotal_features.loc[2522, 'BsmtCond'] = 'Gd' #cause ALQ for bsmtfintype1","b7efe5f9":"subclass_group = total_features.groupby('MSSubClass')\nZoning_modes = subclass_group['MSZoning'].apply(lambda x : x.mode()[0])\ntotal_features['MSZoning'] = total_features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))","febf2107":"neighborhood_group = total_features.groupby('Neighborhood')\nlot_medians = neighborhood_group['LotFrontage'].median()\ntotal_features['LotFrontage'] = total_features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","5bc02a4d":"#Filling in the rest of the NA's\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics = []\nfor i in total_features.columns:\n    if total_features[i].dtype in numeric_dtypes: \n        numerics.append(i)\n        \ntotal_features.update(total_features[numerics].fillna(0))\n\n# remaining columns \n\ncolumns = [\"PoolQC\" , \"MiscFeature\", \"Alley\", \"Fence\", \"FireplaceQu\", \"GarageQual\", \n         \"GarageCond\", \"GarageFinish\", \"GarageYrBlt\", \"GarageType\", \"BsmtExposure\", \n         \"BsmtCond\", \"BsmtQual\", \"BsmtFinType2\", \"BsmtFinType1\", \"MasVnrType\"]\n\nfor col in columns:\n    total_features.update(total_features[col].fillna(\"None\", inplace=True))\n\n\nnulls = np.sum(total_features.isnull())\nnullcols = nulls.loc[(nulls != 0)]\ndtypes = total_features.dtypes\ndtypes2 = dtypes.loc[(nulls != 0)]\ninfo = pd.concat([nullcols, dtypes2], axis=1).sort_values(by=0, ascending=False)\nprint(info)\nprint(\"There are\", len(nullcols), \"columns with missing values\")","ee3d2aad":"total_features = total_features.drop(['Utilities','Street'], axis=1)","db742767":"#FEATURE ENGINEERING\n\ntotal_features['Total_sqr_footage'] = (total_features['BsmtFinSF1'] + total_features['BsmtFinSF2'] +\n                                 total_features['1stFlrSF'] + total_features['2ndFlrSF'])\n\ntotal_features['Total_Bathrooms'] = (total_features['FullBath'] + (0.5*total_features['HalfBath']) + \n                               total_features['BsmtFullBath'] + (0.5*total_features['BsmtHalfBath']))\n\ntotal_features['Total_porch_sf'] = (total_features['OpenPorchSF'] + total_features['3SsnPorch'] +\n                              total_features['EnclosedPorch'] + total_features['ScreenPorch'] +\n                             total_features['WoodDeckSF'])\n\n\n#simplified features\ntotal_features['haspool'] = total_features['PoolArea'].apply(lambda x: 1 if x > 0 else 0)\ntotal_features['has2ndfloor'] = total_features['2ndFlrSF'].apply(lambda x: 1 if x > 0 else 0)\ntotal_features['hasgarage'] = total_features['GarageArea'].apply(lambda x: 1 if x > 0 else 0)\ntotal_features['hasbsmt'] = total_features['TotalBsmtSF'].apply(lambda x: 1 if x > 0 else 0)\ntotal_features['hasfireplace'] = total_features['Fireplaces'].apply(lambda x: 1 if x > 0 else 0)","2471193d":"total_features.shape","41e5334b":"final_features = pd.get_dummies(total_features).reset_index(drop=True)\nfinal_features.shape","9d5395f0":"final_train_x = final_features.iloc[:len(train_y),:]\nfinal_test_x = final_features.iloc[len(final_train_x):,:] ","d2c8ee97":"final_train_x.shape","144ee211":"final_test_x.shape","7f33a9cd":"#Now let's use t-SNE to reduce dimensionality down to 2D so we can plot the dataset:\n\nfrom sklearn.manifold import TSNE\n\ntsne = TSNE(n_components=2, random_state=42, verbose = 2)\nTSNE_X = tsne.fit_transform(final_train_x)\nTSNE_X_test = tsne.fit_transform(final_test_x)","b6850c38":"plt.figure(figsize=(13,10))\nplt.scatter(TSNE_X[:, 0], TSNE_X[:, 1], c=train_y, cmap=\"jet\")\nplt.axis('off')\nplt.colorbar()\nplt.show()","bcc8f112":"from sklearn.decomposition import PCA\n\nPCA_train_x = PCA(n_components=300, random_state=42).fit_transform(final_train_x)\nplt.scatter(PCA_train_x[:, 0], PCA_train_x[:, 1], c=train_y, cmap=\"jet\")\nplt.axis('off')\nplt.colorbar()\nplt.show()","298f4974":"from sklearn.decomposition import KernelPCA\n\nlin_pca = KernelPCA(n_components = 2, kernel=\"linear\", fit_inverse_transform=True)\nrbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True)\nsig_pca = KernelPCA(n_components = 2, kernel=\"sigmoid\", gamma=0.001, coef0=1, fit_inverse_transform=True)\n\n\nplt.figure(figsize=(11, 4))\nfor subplot, pca, title in ((131, lin_pca, \"Linear kernel\"), (132, rbf_pca, \"RBF kernel, $\\gamma=0.04$\"), \n                            (133, sig_pca, \"Sigmoid kernel, $\\gamma=10^{-3}, r=1$\")):\n       \n    plt.subplot(subplot)\n    plt.title(title, fontsize=14)\n    plt.scatter(PCA_train_x[:, 0], PCA_train_x[:, 1], c=train_y, cmap=plt.cm.hot)\n    plt.xlabel(\"$z_1$\", fontsize=18)\n    if subplot == 131:\n        plt.ylabel(\"$z_2$\", fontsize=18, rotation=0)\n    plt.grid(True)\n\nplt.show()","fc54297f":"from sklearn.manifold import LocallyLinearEmbedding\n\nlle = LocallyLinearEmbedding(n_components=2, n_neighbors=10, random_state=42)\nPCA_X = lle.fit_transform(final_train_x)","8ab58a69":"plt.title(\"Unrolled swiss roll using LLE\", fontsize=14)\nplt.scatter(PCA_X [:, 0], PCA_X [:, 1], c= train_y, cmap=plt.cm.hot)\nplt.xlabel(\"$z_1$\", fontsize=18)\nplt.ylabel(\"$z_2$\", fontsize=18)\nplt.axis([-0.100, 0.215, -0.043, 0.14])\nplt.grid(True)\nplt.show()","1691bfdc":"Kernel PCA","189d5ee9":"Check what value to impute, when the variable value is missing.","59a736df":"Apply TSNE ","a9cdab6c":"PCA - Not better than TSNE","7db09c88":"Locally Linear Embedding","7f0233db":"As Suggested by many participants in Kaggle that the outliers shall be removed","bd96179b":"Impute the Missing Values","3acb5696":"Determine the missing values"}}