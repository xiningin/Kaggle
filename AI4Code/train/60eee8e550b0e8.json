{"cell_type":{"112be92a":"code","b89e52db":"code","a691bd48":"code","140a439d":"code","887389ed":"code","d8e7a6e9":"code","9f72c28e":"code","1f81dfb9":"code","fcae4f00":"code","29a4467b":"code","a678e50f":"code","f4da13cf":"code","fd292557":"code","2e16e134":"code","9970b554":"code","ad68ab6a":"code","330800d2":"code","eac2d5af":"code","9872ee6f":"code","5b97984c":"code","45077a5b":"code","8d2f5c09":"code","e7d3fa0f":"code","fda9e376":"code","0974a1ae":"code","77a36a0b":"code","ffc7d233":"code","bedac549":"code","47f82093":"code","477f7a12":"code","407b951e":"code","b5da52b0":"code","b20e19ea":"code","e5b32559":"code","6c460ecf":"markdown","22cb1d6c":"markdown","3b2b5aea":"markdown","5fb806f2":"markdown","61a36e59":"markdown","17ef6ccb":"markdown","b1d23993":"markdown","c195ea0c":"markdown","61ce615f":"markdown","0ab33ce0":"markdown","82052fcd":"markdown","50ac44c7":"markdown","48866c37":"markdown","966a6382":"markdown","be9c7ed8":"markdown","80d3fb20":"markdown","d06e1dea":"markdown","6b2c9016":"markdown","dbb5ef4c":"markdown","5086706b":"markdown","5d5ca98d":"markdown"},"source":{"112be92a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn import tree\n","b89e52db":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","a691bd48":"train.isnull().sum() # To check No missing values ","140a439d":"sns.boxplot(x='Pclass', y='Age', data = train)\nplt.show()","887389ed":"def impute_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age):\n\n        if Pclass == 1:\n            return 37\n\n        elif Pclass == 2:\n            return 29\n\n        else:\n            return 24\n\n    else:\n        return Age","d8e7a6e9":"train['Age'] = train[['Age','Pclass']].apply(impute_age,axis=1)","9f72c28e":"train.drop('Cabin',axis=1,inplace=True) \ntrain","1f81dfb9":"train.dropna(inplace = True)\ntrain","fcae4f00":"train.info()","29a4467b":"\nsex = pd.get_dummies(train['Sex'],drop_first=True)\nembark = pd.get_dummies(train['Embarked'],drop_first=True)\n\ntrain.drop(['Sex','Embarked','Name','Ticket'],axis=1,inplace=True)","a678e50f":"train","f4da13cf":"train = pd.concat([train,sex,embark],axis=1)\n\ntrain_data = train.drop('Survived', axis=1)\nlabel = train['Survived']","fd292557":"train_data","2e16e134":"train_data.info()","9970b554":"k_fold = KFold(n_splits=10, shuffle=True, random_state=0)","ad68ab6a":"DT_clf = tree.DecisionTreeClassifier(random_state=0)\nclf = DT_clf.fit(train_data, label)\ntree.plot_tree(clf)\n","330800d2":"fn=list(train_data.columns)\ncn=[\"0\",\"1\"]\nfig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (2,2), dpi=500)\ntree.plot_tree(clf,\n               feature_names = fn, \n               class_names=cn,\n               filled = True);\nfig.savefig('Titanic_DT.png')","eac2d5af":"train_mini = train_data.drop('PassengerId', axis=1)\ntrain_mini = train_mini.drop('Age', axis=1)   # hash.remove this if you want to include it\ntrain_mini = train_mini.drop('Fare', axis=1)  # hash.remove this if you want to include it\ntrain_mini = train_mini.drop('SibSp', axis=1) # hash.remove this if you want to include it\ntrain_mini = train_mini.drop('Pclass', axis=1) # hash.remove this if you want to include it\n\nDT_mini_clf = tree.DecisionTreeClassifier(random_state=0)\nclf_mini = DT_mini_clf.fit(train_mini, label)\n\nfn=list(train_mini.columns)\ncn=[\"0\",\"1\"]\nfig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (2,2), dpi=700)\ntree.plot_tree(clf_mini,\n               feature_names = fn, \n               class_names=cn,\n               filled = True);","9872ee6f":" #Decision Tree Score\nDT_score = cross_val_score(DT_clf, train_data, label, cv=k_fold, n_jobs=1, scoring='accuracy')\nprint(DT_score)\nround(np.mean(DT_score)*100, 2)","5b97984c":"RF_clf = RandomForestClassifier(n_estimators=13)\n\nRF_score = cross_val_score(RF_clf, train_data, label, cv=k_fold, n_jobs=1, scoring='accuracy')\nprint(RF_score)","45077a5b":" #Random Forest Score\nround(np.mean(RF_score)*100, 2)","8d2f5c09":"test.isnull().sum()","e7d3fa0f":"test['Age'] = test[['Age','Pclass']].apply(impute_age,axis=1)\n\ntest","fda9e376":"test.isnull().sum()","0974a1ae":"test[\"Fare\"] = test.Fare.astype(float)","77a36a0b":"test.info()","ffc7d233":"test.drop('Cabin',axis=1,inplace=True) ","bedac549":"\ntest['Fare'] = test['Fare'].fillna(0)\n\ntest.isnull().sum()","47f82093":"sex = pd.get_dummies(test['Sex'],drop_first=True)\nembark = pd.get_dummies(test['Embarked'],drop_first=True)\n\ntest.drop(['Sex','Embarked','Name','Ticket'],axis=1,inplace=True)\n\ntest = pd.concat([test,sex,embark],axis=1)\n\ntest","477f7a12":"test.info()","407b951e":"xtrain=train_data\nytrain=label\nxtest=test","b5da52b0":"RF=RandomForestClassifier(random_state=1)\nPRF=[{'n_estimators':[10,100],'max_depth':[3,6],'criterion':['gini','entropy']}]\nGSRF=GridSearchCV(estimator=RF, param_grid=PRF, scoring='accuracy',cv=2)\nscores_rf=cross_val_score(GSRF,xtrain,ytrain,scoring='accuracy',cv=5)\nnp.mean(scores_rf)","b20e19ea":"model=GSRF.fit(xtrain, ytrain)\npred=model.predict(xtest)","e5b32559":"\n\noutput = pd.DataFrame({'PassengerId': xtest.PassengerId, 'Survived': pred})\noutput.to_csv('Kamal_submission.csv', index=False)  #change name to own\nprint(\"Your submission was successfully saved!\")","6c460ecf":"# Does this DF look identical to the train before modelling?","22cb1d6c":"# IMPORTANT: key in DS\/ML is EDA\/feature engg\n\nThe techniues and modeling code per se is only a few lines!","3b2b5aea":"What does this code do?\nWould you also remove Cabin?","5fb806f2":"# Can you remove the Fare and Cabin?\nwhy or why not?","61a36e59":"# Titanic: Decision Trees and Random Forest\n\nThis is a notebook to understand Decision Trees and Random Forest and use it for the Titanic: Machine Learning from Disaster case. \n\nThe aim is to  Predict survival on the Titanic and get familiar with ML basics\n","17ef6ccb":"# chart too crowded?\n\nsee the data with less features or column","b1d23993":"# Modelling","c195ea0c":"**Good to know**\n\n\nWhen we use \"drop\" only, it drops the columns\/rows you define\n\nWhen we use \"dropna\", it removes all entries with NaN values (or null in general)","61ce615f":"What does this code do?\nDoes this matter?","0ab33ce0":"# What is the Titanic Problem?\n\nPlease refer to the notebook for EDA and Vizualizations: \nhttps:\/\/www.kaggle.com\/kmldas\/titanic-eda-viz-for-beginners\n","82052fcd":"Why is Random Forest typically better than Decision Tree?\n\n","50ac44c7":"# Random Forest","48866c37":"# Decision Tree","966a6382":"# Import Libraries","be9c7ed8":"# Read the files","80d3fb20":" Question: Why am I taking these ages?\n \n \n \n Hint: look at the box plot above","d06e1dea":"# look at the test data","6b2c9016":"![Source: https:\/\/i.ytimg.com\/vi\/Wan0IJuXlys\/maxresdefault.jpg](https:\/\/i.ytimg.com\/vi\/Wan0IJuXlys\/maxresdefault.jpg)","dbb5ef4c":"# Categorical Values","5086706b":"# Data Cleaning ","5d5ca98d":"# Submission"}}