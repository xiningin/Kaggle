{"cell_type":{"b347a2ee":"code","dabe2b87":"code","0ef98367":"code","8aaedc63":"code","aa6d0b94":"code","cc8d33d2":"code","b9f014ce":"code","0847b952":"code","a4052934":"code","6b7a27ff":"code","d2dbde34":"code","acf6c410":"code","f916e9fe":"code","5878da3d":"code","1fc7c864":"code","1359f067":"code","a4956ea2":"code","6eff649f":"code","abc42e1d":"code","d0b368d3":"code","0a4f0e8b":"code","20663b6a":"code","7f392c87":"markdown","92b65eb8":"markdown","890bceae":"markdown","cfc73194":"markdown","20f130d8":"markdown","ad3358b2":"markdown","b7e796dc":"markdown","18c5d970":"markdown","34690a5b":"markdown","b990364b":"markdown","b46ef1a4":"markdown","22bd5f8b":"markdown","d9020e19":"markdown","c7ff28a8":"markdown","02d90932":"markdown","e60c5f6c":"markdown","4f1102dd":"markdown"},"source":{"b347a2ee":"#import libraries\nimport pandas as pd\nimport numpy as np\nimport statsmodels.api as sm\nimport scipy.stats as st\nimport matplotlib.pyplot as plt\nimport seaborn as sn\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.mlab as mlab\n%matplotlib inline","dabe2b87":"#load data\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \ndef read_file(file):\n    x = open(file,'r', encoding = 'utf-8') #Opens the text file into variable x but the variable cannot be explored yet\n    y = x.read() #By now it becomes a huge chunk of string that we need to separate line by line\n    content = y.splitlines() #The splitline method converts the chunk of string into a list of strings\n    return content\ndf = pd.read_csv('..\/input\/framingham-heart-study-dataset\/framingham.csv')","0ef98367":"df.head()","8aaedc63":"df.isnull().sum()","aa6d0b94":"df.drop(['education'],axis=1,inplace=True)\ndf.head()","cc8d33d2":"count=0\nfor i in df.isnull().sum(axis=1):\n    if i>0:\n        count=count+1\nprint('Total number of rows with missing values is ', count)\nprint('since it is only',round((count\/len(df.index))*100), 'percent of the entire dataset the rows with missing values are excluded.')","b9f014ce":"df.dropna(axis=0,inplace=True)","0847b952":"df.TenYearCHD.value_counts()","a4052934":"sn.countplot(x='TenYearCHD',data=df)","6b7a27ff":"df['male'].value_counts().head(10).plot.barh() # Top 10 Dates on which the most number of messages were sent\nplt.xlabel('count')\nplt.ylabel('Gender')","d2dbde34":"df.describe()","acf6c410":"df.shape","f916e9fe":"plt.figure(figsize = (10, 10))\nsn.heatmap(df.corr(), annot = True)\nplt.show()","5878da3d":"#scatterplot\nsn.set()\ncols = ['age','male','cigsPerDay','totChol','sysBP','glucose','TenYearCHD']\nsn.pairplot(df[cols], size = 2.5)\nplt.show();","1fc7c864":"import sklearn\n\nnew_features=df[['age','male','cigsPerDay','totChol','sysBP','glucose','TenYearCHD']]\nx=new_features.iloc[:,:-1]\ny=new_features.iloc[:,-1]\nfrom sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=.20,random_state=5)","1359f067":"from sklearn.linear_model import LogisticRegression\nlogreg=LogisticRegression()\nlogreg.fit(x_train,y_train)\ny_pred=logreg.predict(x_test)","a4956ea2":"sklearn.metrics.accuracy_score(y_test,y_pred)","6eff649f":"from sklearn.metrics import confusion_matrix\ncm=confusion_matrix(y_test,y_pred)\nconf_matrix=pd.DataFrame(data=cm,columns=['Predicted:0','Predicted:1'],index=['Actual:0','Actual:1'])\nplt.figure(figsize = (8,5))\nsn.heatmap(conf_matrix, annot=True,fmt='d',cmap=\"YlGnBu\")","abc42e1d":"TN=cm[0,0]\nTP=cm[1,1]\nFN=cm[1,0]\nFP=cm[0,1]\nsensitivity=TP\/float(TP+FN)\nspecificity=TN\/float(TN+FP)\n\n\nprint('The acuuracy of the model = TP+TN\/(TP+TN+FP+FN) = ',(TP+TN)\/float(TP+TN+FP+FN),'\\n',\n\n'The Missclassification = 1-Accuracy = ',1-((TP+TN)\/float(TP+TN+FP+FN)),'\\n',\n\n'Sensitivity or True Positive Rate = TP\/(TP+FN) = ',TP\/float(TP+FN),'\\n',\n\n'Specificity or True Negative Rate = TN\/(TN+FP) = ',TN\/float(TN+FP),'\\n',\n\n'Positive Predictive value = TP\/(TP+FP) = ',TP\/float(TP+FP),'\\n',\n\n'Negative predictive Value = TN\/(TN+FN) = ',TN\/float(TN+FN),'\\n',\n\n'Positive Likelihood Ratio = Sensitivity\/(1-Specificity) = ',sensitivity\/(1-specificity),'\\n',\n\n'Negative likelihood Ratio = (1-Sensitivity)\/Specificity = ',(1-sensitivity)\/specificity)","d0b368d3":"from sklearn.preprocessing import binarize\nfor i in range(1,5):\n    cm2=0\n    y_pred_prob_yes=logreg.predict_proba(x_test)\n    y_pred2=binarize(y_pred_prob_yes,i\/10)[:,1]\n    cm2=confusion_matrix(y_test,y_pred2)\n    print ('With',i\/10,'threshold the Confusion Matrix is ','\\n',cm2,'\\n',\n            'with',cm2[0,0]+cm2[1,1],'correct predictions and',cm2[1,0],'Type II errors( False Negatives)','\\n\\n',\n          'Sensitivity: ',cm2[1,1]\/(float(cm2[1,1]+cm2[1,0])),'Specificity: ',cm2[0,0]\/(float(cm2[0,0]+cm2[0,1])),'\\n\\n\\n')","0a4f0e8b":"from sklearn.metrics import roc_curve\nfpr, tpr, thresholds = roc_curve(y_test, y_pred_prob_yes[:,1])\nplt.plot(fpr,tpr)\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.0])\nplt.title('ROC curve for Heart disease classifier')\nplt.xlabel('False positive rate (1-Specificity)')\nplt.ylabel('True positive rate (Sensitivity)')\nplt.grid(True)","20663b6a":"#auc\nsklearn.metrics.roc_auc_score(y_test,y_pred_prob_yes[:,1])","7f392c87":"Let's move to model ensembling","92b65eb8":"A model with good classification accuracy should have significantly more true positives than false positives at all thresholds. ","890bceae":"So now, let's see the relationship between the target variable and important features","cfc73194":"***ROC Curve***\n\nIt helps in deciding the best threshold value. It is generated by plotting TP rate(y-axis) against FP rate(x-axis).\n\nIt will always end at (1,1) while the threshold point being at 0\n\nIt helps in visualizing the performance of a classification model\n\nShows the model efficiency by detecting True Positives(recall)\n\nIf we assume a lower classification threshold, the model classifies more items as positive\n\nThreshold based evaluation metrics\n\nAlso called precision recall curve\n\nIt tells the optimal threshold to select\n\n***Area under Curve(AUC)***\n\nIt gives the rate of successful classification by the logistic model \n\nThe higher the area, the greater the disparity between true and false positives, and the stronger the model in classifying members of the training dataset\n\nCloser to 1 -> better\n\n**If high threshold,**\n* High specificity\n* Low sensitivity \n\n**If low threshold,**\n* Low specificity\n* High sensitivity \n\n","20f130d8":"As we can see, there are more females, as much as 2000 and lesser males with the count of about 1600","ad3358b2":"This is a correlation matrix. Each square shows the correlation between the variables on each axis. Correlation ranges from -1 to +1. Values closer to zero means there is no linear trend between the two variables.\n\nThe close to 1 the correlation is the more positively correlated they are; that is as one increases so does the other and the closer to 1 the stronger this relationship is. A correlation closer to -1 is similar, but instead of both increasing one variable will decrease as the other increases.\n\nFor the rest the larger the number and darker the colour the higher the correlation between the two variables. The plot is also symmetrical about the diagonal since the same two variables are being paired together in those squares\n\nthese are some of the important relationship :\n* male <-> cigsPerDay, heartRate\n* glucose <-> diabetes\n* currentSmoker <-> cigsPerDay\n* prevalentHP <-> SysBP, diaBP\n* SysBP <-> diaBP\n* TenYearCHD <-> 'age','male','cigsPerDay','totChol','sysBP','glucose'\n","b7e796dc":"Let's start by checking the null values","18c5d970":"Except education, all the other features could be quite important, but the level of education doesn't corelate to someone's health, so let's drop it out!","34690a5b":"***Evaluation***","b990364b":"Wow! That's quite a large dataset, we will have to make sure there are no outliers or unimportant features in the modeling process\n\nTechnically, scaling or normalizing inputs to logistic regression is not required, so we will skip that step\n\nLet's analyze the relationships between the independent variable and target variable\n","b46ef1a4":"It is clear that the model is highly specific than sensitive. The negative values are predicted more accurately than the positives.\n\nSince the model is predicting Heart disease too many type II errors is not advisable. A False Negative ( ignoring the probability of disease when there actualy is one) is more dangerous than a False Positive in this case. Hence inorder to increase the sensitivity, threshold can be lowered.","22bd5f8b":"As we can see, there are 3179 patients who does not have heart disease ","d9020e19":"The confusion matrix shows 658+4 = 662 correct predictions and 88+1= 89 incorrect ones.\n\n    True Positives: 4\n\n    True Negatives: 658\n\n    False Positives: 1 \n\n    False Negatives: 88 \n\n","c7ff28a8":"***EDA***","02d90932":"It is both classification and regression algorithm depending on the scenario. Unlike other regression algorithms, Logistic regression does not predict a continous value but rather binary(0,1). However, the independent variables are continous. Note that, an S curve(ROC) means you got a perfect relationship between X and y. \n\nIt Uses sigmoid or logistic function. We will talk about sigmoid and ROC later further. ","e60c5f6c":"# Logistic Regression","4f1102dd":"An area of 0.5 corresponds to a model that performs no better than random classification and a good classifier stays as far away from that as possible. An area of 1 is ideal. The closer the AUC to 1 the better.\n\n\n\nThat's it! We are done here!\n\nThere are many other techniques that are out there which can be more useful to evaluate this model! This is just something you can start with.\n\nThank you for going through this notebook. I hope you found this useful and if you did, please upvote it! <3\n"}}