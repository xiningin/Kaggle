{"cell_type":{"7124e4e2":"code","0da061b4":"code","5a37920f":"code","c2fd3030":"code","69bf0221":"code","d96ddddf":"code","eedb8c0e":"code","d29924f8":"code","6fc71803":"code","ea05d082":"code","840959ce":"code","43824e22":"code","8b973e2e":"code","9dd7b512":"code","a64c195a":"code","7ef4989c":"code","843de695":"code","0706a167":"code","6514e10b":"code","28af7296":"code","e6efd7bc":"code","9f4e3288":"code","1842ab15":"code","10615ca1":"code","9b40dbca":"code","e39c02dd":"code","db031819":"code","e2016c8a":"code","6f59f313":"code","602d4e01":"code","9253f976":"code","3d90a4f6":"code","28c3a7ae":"code","e71b73f2":"markdown","8782e703":"markdown","452bfc60":"markdown","8ac00481":"markdown","b05021c7":"markdown","2c152de0":"markdown","f74a991c":"markdown","785307cc":"markdown","2710457c":"markdown","30ce449d":"markdown","16e49bff":"markdown","8c44c7ba":"markdown","bf6f9252":"markdown","84b9798d":"markdown","8eb5313b":"markdown","d7402c77":"markdown","3568ac01":"markdown","8ba58f41":"markdown","0d70726e":"markdown","4e5e60a6":"markdown","3b3df66b":"markdown","df5ccca6":"markdown","7465f91a":"markdown"},"source":{"7124e4e2":"# Import libraries\n\nimport riiideducation\n\nimport numpy as np \nimport pandas as pd \n\nfrom pandas_profiling import ProfileReport\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import label_binarize\n\nimport lightgbm as lgb\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nimport xgboost as xgb\nfrom sklearn.ensemble import StackingClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\n\nfrom sklearn.metrics import mean_absolute_error\n\nfrom matplotlib import pyplot as plt \n%matplotlib inline","0da061b4":"# Import data\n\n# Come from https:\/\/www.kaggle.com\/c\/riiid-test-answer-prediction\/discussion\/188908\ntrain_df = pd.DataFrame()\ncounter = 1\nfor chunk in pd.read_csv('..\/input\/riiid-test-answer-prediction\/train.csv', chunksize=1000000, low_memory=False):\n    print('Reading chunck {}'.format(counter))\n    # Sample the size as it is too big\n    chunk = chunk.sample(frac=0.1, random_state=1)\n    train_df = pd.concat([train_df, chunk], ignore_index=True)\n    counter += 1","5a37920f":"# Import other data sets, which are small enough\n\ntest_df = pd.read_csv('..\/input\/riiid-test-answer-prediction\/example_test.csv')\n# questions_df = pd.read_csv('..\/input\/riiid-test-answer-prediction\/questions.csv')\n# lectures_df = pd.read_csv('..\/input\/riiid-test-answer-prediction\/lectures.csv')","c2fd3030":"train_df.head(5)","69bf0221":"train_df.info()","d96ddddf":"f'There are {train_df.shape[0]} rows with {train_df.shape[1]} records in train_df. '","eedb8c0e":"print(train_df['answered_correctly'].isna().sum())\nprint(train_df.shape[0])\nprint('There are {:0.2f}% of missing data in answered_correctly. '.format(train_df['answered_correctly'].isna().sum()\/train_df.shape[0]))","d29924f8":"print(train_df['prior_question_elapsed_time'].isna().sum())\nprint(train_df.shape[0])\nprint('There are {:0.2f}% of missing data in prior_question_elapsed_time. '.format(train_df['prior_question_elapsed_time'].isna().sum()\/train_df.shape[0]))","6fc71803":"print(train_df['prior_question_had_explanation'].isna().sum())\nprint(train_df.shape[0])\nprint('There are {:0.2f}% of missing data in prior_question_had_explanation. '.format(train_df['prior_question_had_explanation'].isna().sum()\/train_df.shape[0]))","ea05d082":"# ProfileReport(test_df, title=\"`test_df` Profiling Report\", progress_bar=False)","840959ce":"# ProfileReport(questions_df, title=\"`questions_df` Profiling Report\", progress_bar=False)","43824e22":"# ProfileReport(lectures_df, title=\"`lectures_df` Profiling Report\", progress_bar=False)","8b973e2e":"# Code from https:\/\/www.kaggle.com\/dmikar\/baseline-for-riiid-lightgbm\nmean_prior = train_df['prior_question_elapsed_time'].astype(\"float64\").mean()\nprint(f'{mean_prior} is filled for the missing data in prior_question_elapsed_time. ')\n\ntrain_df['prior_question_elapsed_time'].fillna(mean_prior, inplace = True)\ntrain_df['prior_question_had_explanation'].fillna(False, inplace = True)","9dd7b512":"mean_prior = test_df['prior_question_elapsed_time'].astype(\"float64\").mean()\nprint(f'{mean_prior} is filled for the missing data in prior_question_elapsed_time. ')\n\ntest_df['prior_question_elapsed_time'].fillna(mean_prior, inplace = True)\ntest_df['prior_question_had_explanation'].fillna(False, inplace = True)","a64c195a":"test_df['prior_question_had_explanation'].dtype","7ef4989c":"test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].astype('bool')","843de695":"test_df['prior_question_had_explanation'].dtype","0706a167":"y = train_df['answered_correctly'].to_numpy()\nX = train_df[['user_id', 'content_id', 'content_type_id', 'task_container_id', 'prior_question_elapsed_time', 'prior_question_had_explanation']].to_numpy()\n\nX_train_df, X_val_df, y_train_df, y_val_df = train_test_split(X, y, test_size =0.3, shuffle=False)\n\ndel train_df","6514e10b":"lgb_train = lgb.Dataset(X_train_df, y_train_df, categorical_feature = ['prior_question_had_explanation'], free_raw_data=False)\nlgb_eval = lgb.Dataset(X_val_df, y_val_df, categorical_feature = ['prior_question_had_explanation'], free_raw_data=False)","28af7296":"# # param values c.f. https:\/\/www.kaggle.com\/zephyrwang666\/riiid-lgbm-bagging2\n# param = {'num_leaves': sp_randint(150, 400), 'max_bin':sp_randint(300, 800), 'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4], \n#          'feature_fraction': sp_uniform(0, 1), 'bagging_fraction': sp_uniform(0, 1), \n#          'objective': ['binary'], 'max_depth': [-1], \n#          'learning_rate': [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09], \"boosting_type\": [\"gbdt\"], \"bagging_seed\": [47], \n#          'eval_metric': ['logloss'], \"verbosity\": [-1], \n#          'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100], 'reg_lambda': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100], \n#          'random_state': [47]}\n\n# m1 = lgb.LGBMClassifier(valid_sets = [lgb_train, lgb_eval], verbose_eval = 30, num_boost_round = 10000, early_stopping_rounds = 10, n_jobs=4, n_estimators=3000)\n\n# '''\n# Hyperparameter optimisation\n# '''\n# # Code from https:\/\/www.kaggle.com\/rtatman\/lightgbm-hyperparameter-optimisation-lb-0-761#Model-fitting-with-HyperParameter-optimisation\n# #This parameter defines the number of hyperparameter points to be tested\n# n_HP_points_to_test = 5\n\n# gsLGBM = RandomizedSearchCV(\n#     estimator=m1, param_distributions=param, \n#     n_iter=n_HP_points_to_test,\n#     cv=3,\n#     refit=True,\n#     random_state=47,\n#     verbose=True)","e6efd7bc":"# gsLGBM.fit(X_train_df, y_train_df, eval_set = (X_val_df, y_val_df))\n# print('Best score reached: {} with params: {} '.format(gsLGBM.best_score_, gsLGBM.best_params_))","9f4e3288":"# Just in case, the parameters should be printed in here. \n# Score: 0.6788\nopt_parameters_LGBM = {'bagging_fraction': 0.11348847189364952, 'bagging_seed': 47, 'boosting_type': 'gbdt', \n 'eval_metric': 'logloss', 'feature_fraction': 0.9744830944364566, 'learning_rate': 0.09, \n 'max_bin': 479, 'max_depth': -1, 'min_child_weight': 1e-05, 'num_leaves': 173, \n 'objective': 'binary', 'random_state': 47, 'reg_alpha': 0, 'reg_lambda': 50, 'verbosity': -1}","1842ab15":"m1 = lgb.LGBMClassifier(valid_sets = [lgb_train, lgb_eval], verbose_eval = 30, num_boost_round = 10000, early_stopping_rounds = 10, n_jobs=4, n_estimators=3000, **opt_parameters_LGBM)\nm1.fit(X_train_df, y_train_df, eval_set = (X_val_df, y_val_df))","10615ca1":"# print(f'The mean absolute error of the model is {mean_absolute_error(y_val_df, gsLGBM.predict(X_val_df))}. ')","9b40dbca":"# m2 = AdaBoostClassifier(DecisionTreeClassifier(max_depth=3, min_samples_leaf=1, min_impurity_decrease=10, random_state=47), random_state=47)","e39c02dd":"# param = {'learning_rate': [0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8], 'n_estimators': sp_randint(5, 50)}\n\n# '''\n# Hyperparameter optimisation\n# '''\n# # Code from https:\/\/www.kaggle.com\/rtatman\/lightgbm-hyperparameter-optimisation-lb-0-761#Model-fitting-with-HyperParameter-optimisation\n# #This parameter defines the number of HP points to be tested\n# n_HP_points_to_test = 3\n\n# gsADA = RandomizedSearchCV(\n#     estimator=m2, param_distributions=param, \n#     n_iter=n_HP_points_to_test,\n#     cv=3,\n#     refit=True,\n#     random_state=47,\n#     verbose=True)","db031819":"# gsADA.fit(X_train_df, y_train_df)\n# print('Best score reached: {} with params: {} '.format(gsADA.best_score_, gsADA.best_params_))","e2016c8a":"# Just in case, the parameters should be printed in here. \n# Score: 0.64453\n# opt_parameters_ADA = {'learning_rate': 0.08, 'n_estimators': 11}","6f59f313":"# Final models from LGBM and ADABoost\n# estimators = [\n#     ('lgbm', lgb.LGBMClassifier(verbose_eval = 30, num_boost_round = 10000, early_stopping_rounds = 10, valid = [lgb_train, lgb_eval],\n#                                 n_jobs=4, n_estimators=3000, metric='multi_logloss', **gsLGBM.best_params_)),\n#     ('ab', AdaBoostClassifier(DecisionTreeClassifier(max_depth=3, min_samples_leaf=1, min_impurity_decrease=10, random_state=47), **gsADA.best_params_))\n# ]\n\n# If anything wrong, uncomment the following: \n# estimators = [\n#     ('lgbm', lgb.LGBMClassifier(verbose_eval = 30, num_boost_round = 10000, \n#                                 n_jobs=4, n_estimators=3000, **opt_parameters_LGBM)),\n#     ('ab', AdaBoostClassifier(DecisionTreeClassifier(max_depth=3, min_samples_leaf=1, min_impurity_decrease=10, random_state=47), **opt_parameters_ADA))\n# ]\n\n# del gsLGBM\n# del gsADA","602d4e01":"# Code from https:\/\/www.kaggle.com\/arthurtok\/introduction-to-ensembling-stacking-in-python#Second-Level-Predictions-from-the-First-level-Output\n# gbm = xgb.XGBClassifier(\n#  learning_rate = 0.02,\n#  n_estimators= 5,\n#  max_depth= 4,\n#  min_child_weight= 2,\n#  gamma=0.9,                        \n#  subsample=0.8,\n#  colsample_bytree=0.8,\n#  objective= 'binary',\n#  nthread= -1,\n#  verbosity=2,\n#  scale_pos_weight=1)\n\n# clf = StackingClassifier(\n#     estimators=estimators, final_estimator=gbm\n# )","9253f976":"# clf.fit(X_train_df, y_train_df)","3d90a4f6":"# Environment for the comptetition. \n\nenv = riiideducation.make_env()\niter_test = env.iter_test()","28c3a7ae":"for (test_df, sample_prediction_df) in iter_test:\n    # Repeat of what's written, don't know why the iterator here does not recognise what has been done before. \n    x_columns = ['user_id', 'content_id', 'content_type_id', 'task_container_id', 'prior_question_elapsed_time', 'prior_question_had_explanation']\n    X_train_df = X_train_df\n    test_df = test_df\n    test_df['prior_question_elapsed_time'].fillna(mean_prior, inplace = True)\n    test_df['prior_question_had_explanation'].fillna(False, inplace = True)\n    test_df['prior_question_had_explanation'] = test_df['prior_question_had_explanation'].astype('bool')\n    test_df['answered_correctly'] = m1.predict(test_df[x_columns])\n    env.predict(test_df.loc[test_df['content_type_id'] == 0, ['row_id', 'answered_correctly']])","e71b73f2":"Let's look at the first few data and see how does the data frame looks like. ","8782e703":"## Correct the data types","452bfc60":"# Introduction","8ac00481":"So how many missing records in `'prior_question_had_explanation'`? ","b05021c7":"This submission is written in Python. The first thing is to import all the necessary libraries and data into here. ","2c152de0":"# EDA","f74a991c":"The data frame is too large to print out its profiling, and we sought to use the traditional method (to inspect the data frame). The following is from the `.info()` method. It shows the dtype and also how many missing data. ","785307cc":"# Model","2710457c":"Participants may watch the lecture and answer the questions, which is listed in `lectures_df` and the details are in below: \n* `lecture_id`: foreign key for the train\/test content_id column, when the content type is lecture (1).\n* `part`: top level category code for the lecture.\n* `tag`: one tag codes for the lecture. The meaning of the tags will not be provided, but these codes are sufficient for clustering the lectures together.\n* `type_of`: brief description of the core purpose of the lecture","30ce449d":"# Submission","16e49bff":"## Ensembling the Models","8c44c7ba":"`test_df` is the validation set and we shall see there are extra features, namely the target variables in the data frame. They are \n* `prior_group_responses` (string) provides all of the `user_answer` entries for previous group in a string representation of a list in the first row of the group. All other rows in each group are null. If you are using Python, you will likely want to call `eval` on the non-null rows. Some rows may be null, or empty lists.\n\n* `prior_group_answers_correct` (string) provides all the `answered_correctly` field for previous group, with the same format and caveats as `prior_group_responses`. Some rows may be null, or empty lists.","bf6f9252":"Riiid Labs provides innovative educational solutions. They have provided AI tutor based on deep-learning algorithms. This is my submission of the tracing knowledge in the _Riiid AIEd Challenge 2020_. The purpose of this notebook is to present: \n* A thorough exploratory data analysis of the student-question interaction dataset. \n* Predict how well a student answers a question. ","84b9798d":"## LightGBM Model","8eb5313b":"Here's the data dictionary of `questions_df`: \n* `question_id`: foreign key for the train\/test content_id column, when the content type is question (0).\n* `bundle_id`: code for which questions are served together.\n* `correct_answer`: the answer to the question. Can be compared with the train user_answer column to check if the user was right.\n* `part`: the relevant section of the TOEIC test.\n* `tags`: one or more detailed tag codes for the question. The meaning of the tags will not be provided, but these codes are sufficient for clustering the questions together.","d7402c77":"We now treat the missing data of `train_df` and `test_df` here. From the EDA, we can see that there are 2.3% and 0.3% of missing values in the last two columns of `train_df`. ","3568ac01":"The LGBM model does not like any dtypes other than `int`, `float` or `bool`. While `prior_question_had_explanation` in `test_df` has a custom data type `boolean`, we will need to change them. ","8ba58f41":"Here is the data dictionary of `train.csv`, copied from the introduction. \n\n* `row_id`: (int64) ID code for the row.\n* `timestamp`: (int64) the time between this user interaction and the first event completion from that user.\n* `user_id`: (int32) ID code for the user.\n* `content_id`: (int16) ID code for the user interaction\n* `content_type_id`: (int8) 0 if the event was a question being posed to the user, 1 if the event was the user watching a lecture.\n* `task_container_id`: (int16) Id code for the batch of questions or lectures. For example, a user might see three questions in a row before seeing the explanations for any of them. Those three would all share a task_container_id.\n* `user_answer`: (int8) the user's answer to the question, if any. Read -1 as null, for lectures.\n* `answered_correctly`: (int8) if the user responded correctly. Read -1 as null, for lectures.\n* `prior_question_elapsed_time`: (float32) The average time it took a user to answer each question in the previous question bundle, ignoring any lectures in between. Is null for a user's first question bundle or lecture. Note that the time is the average time a user took to solve each question in the previous bundle.\n* `prior_question_had_explanation`: (bool) Whether or not the user saw an explanation and the correct response(s) after answering the previous question bundle, ignoring any lectures in between. The value is shared across a single question bundle, and is null for a user's first question bundle or lecture. Typically the first several questions a user sees were part of an onboarding diagnostic test where they did not get any feedback.","0d70726e":"## Missing Data","4e5e60a6":"Let's look at the features in each data frames and see what they mean. ","3b3df66b":"We now train the model. ","df5ccca6":"## ADABoost","7465f91a":"It might okay to remove the missing values. "}}