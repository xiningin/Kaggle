{"cell_type":{"dd3bbd27":"code","b28014a4":"code","95e9d340":"code","0044c2ea":"code","f1f99b3d":"code","abb07e99":"code","48d1b530":"code","00cafc9b":"code","2549de0d":"code","f078b4dd":"code","a012d883":"code","8f18370c":"code","64780beb":"code","5dd54f66":"code","c9940a7b":"code","2684a464":"code","68314145":"code","7981dc77":"code","f72b023f":"code","8a0e0ce6":"code","6889c4ad":"code","bcb79075":"code","ab6db94a":"code","7c319af7":"code","6a2f0242":"code","9d5b5a98":"code","c064c35d":"code","77ff821f":"code","010a4c62":"code","edc89c1c":"code","3fe7a6b0":"code","f6101b3f":"code","63ef8a03":"code","042740d8":"code","9a5dff77":"code","7a94ad30":"code","b7ee1de4":"code","28f3a0f3":"code","bcd53a1c":"code","7ca72d43":"code","286fcc7e":"code","a34c69e5":"code","3a21040b":"code","8b5412ac":"code","4b08f8e5":"code","b06373cd":"code","7e4300df":"code","d487db3c":"code","8a917516":"code","eeea7f89":"code","866c86b3":"code","eb1e1cb2":"code","f515a3cb":"code","fdb8a7e8":"code","01022e2b":"code","3ece729e":"code","f7ad39c7":"code","d77dfcfd":"code","0e95a05d":"code","80fedfee":"code","1e6c50f7":"code","6189ef94":"code","c71fde9c":"code","43aea4ec":"code","484373b5":"code","801994bf":"code","788d954b":"code","999b2f7e":"code","13a1c983":"code","d5b855df":"code","1b109047":"code","f42ff5a4":"markdown","f5f56760":"markdown","74cb7cd3":"markdown","abea3dd6":"markdown"},"source":{"dd3bbd27":"import warnings\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom keras import optimizers\nfrom keras.utils import plot_model\nfrom keras.models import Sequential, Model\nfrom keras.layers.convolutional import Conv1D, MaxPooling1D\nfrom keras.layers import Dense, LSTM, RepeatVector, TimeDistributed, Flatten, Dropout, Bidirectional\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import train_test_split\nimport plotly.graph_objs as go\nfrom plotly.offline import init_notebook_mode, iplot\nfrom sklearn.preprocessing import MinMaxScaler\n\n%matplotlib inline\nwarnings.filterwarnings(\"ignore\")\ninit_notebook_mode(connected=True)\n\n# Set seeds to make the experiment more reproducible.\nimport tensorflow as tf\nfrom numpy.random import seed\ntf.random.set_seed(1)\nseed(1)","b28014a4":"data = pd.read_csv('..\/input\/climate-hour\/climate_hour.csv', parse_dates=['Date Time'],index_col = 0, header=0) \ndata = data.sort_values(['Date Time'])\ndata.head() ","95e9d340":"temp_data = data['T (degC)']                               \ntemp_data = pd.DataFrame({'Date Time': data.index, 'T (degC)':temp_data.values})\ntemp_data = temp_data.set_index(['Date Time'])\ntemp_data.head()","0044c2ea":"temp_scaler = MinMaxScaler()\ntemp_scaler.fit(temp_data) \nnormalized_temp = temp_scaler.transform(temp_data) ","f1f99b3d":"normalized_temp = pd.DataFrame(normalized_temp, columns=['T (degC)'])\nnormalized_temp.index = temp_data.index\nnormalized_temp.head()","abb07e99":"# Normalized data:\nscaler = MinMaxScaler()\nscaler.fit(data) \nnormalized_df = scaler.transform(data) \nnormalized_df = pd.DataFrame(normalized_df, columns=['p (mbar)','T (degC)','Tpot (K)','Tdew (degC)',\n                                                         'rh (%)','VPmax (mbar)','VPact (mbar)',\n                                                         'VPdef (mbar)','sh (g\/kg)','H2OC (mmol\/mol)',\n                                                         'rho (g\/m**3)','wv (m\/s)', 'max. wv (m\/s)','wd (deg)'])\nnormalized_df.index = data.index\nnormalized_df.head()","48d1b530":"#old\ndef series_to_supervised(data, window=1, lag=1, dropnan=True):\n    cols, names = list(), list()\n    # Input sequence (t-n, ... t-1)\n    for i in range(window, 0, -1):\n        cols.append(data.shift(i))\n        names += [('%s(t-%d)' % (col, i)) for col in data.columns]\n    # Current timestep (t=0)\n    cols.append(data)\n    names += [('%s(t)' % (col)) for col in data.columns]\n    # Put it all together\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    # Drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg\n","00cafc9b":"window = 24\nseries = series_to_supervised(normalized_df, window=window)\nseries.head()","2549de0d":"print(series.values.shape)\nprint(np.isnan(series.values).any())","f078b4dd":"# -> 2009-01-02 01:00:00 \nlabels_col = 'T (degC)(t)'\nlabels = series[labels_col]\nseries = series.drop(['p (mbar)(t)', 'T (degC)(t)', 'Tpot (K)(t)',\n                      'Tdew (degC)(t)' ,'rh (%)(t)', 'VPmax (mbar)(t)',\n                      'VPact (mbar)(t)', 'VPdef (mbar)(t)','sh (g\/kg)(t)', \n                      'H2OC (mmol\/mol)(t)', 'rho (g\/m**3)(t)', 'wv (m\/s)(t)',\n                      'max. wv (m\/s)(t)', 'wd (deg)(t)'], axis=1)\n\nX_train = series['2009-01-02 01:00:00':'01.01.2015 00:00:00']\nX_valid = series['01.01.2015 00:00:00':'2017-01-01 00:00:00'] \nY_train = labels['2009-01-02 01:00:00':'01.01.2015 00:00:00']\nY_valid = labels['01.01.2015 00:00:00':'2017-01-01 00:00:00']\nprint('Train set shape', X_train.shape)\nprint('Validation set shape', X_valid.shape)","a012d883":"import time\nname = \"model-mlp{}\".format(int(time.time()))","8f18370c":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tensorflow.keras.layers import Dense,RepeatVector, LSTM, Dropout\nfrom tensorflow.keras.layers import Flatten, Conv1D, MaxPooling1D\nfrom tensorflow.keras.layers import Bidirectional, Dropout\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.optimizers import Adam","64780beb":"X_train_series = X_train.values.reshape((X_train.shape[0], X_train.shape[1], 1))\nX_valid_series = X_valid.values.reshape((X_valid.shape[0], X_valid.shape[1], 1))\nprint('Train set shape', X_train_series.shape)\nprint('Validation set shape', X_valid_series.shape)","5dd54f66":"!pip install h5py","c9940a7b":"from tensorflow.keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping\nearly_stop = EarlyStopping(monitor = \"loss\", mode = \"min\", patience = 7)\ncheckpoint = ModelCheckpoint('best_model.h5'.format(int(time.time())), monitor='loss', mode='min', save_best_only=True, verbose=1)","2684a464":"# detect and init the TPU\ntpu = tf.distribute.cluster_resolver.TPUClusterResolver()\ntf.config.experimental_connect_to_cluster(tpu)\ntf.tpu.experimental.initialize_tpu_system(tpu)\n\n# instantiate a distribution strategy\ntpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n\n# instantiating the model in the strategy scope creates the model on the TPU\nwith tpu_strategy.scope():\n    model = Sequential()\n    model.add(Conv1D(filters=256, kernel_size=2, activation='relu', input_shape=(X_train_series.shape[1], X_train_series.shape[2])))\n    #model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\n    model.add(MaxPooling1D(pool_size=2))\n    model.add(Flatten())\n    model.add(RepeatVector(30))\n    #model.add(LSTM(units=25, return_sequences=True, activation='relu'))\n    #model.add(Dropout(0.2))\n    #model.add(LSTM(units=25, return_sequences=True, activation='relu'))\n    #model.add(Dropout(0.2))\n    #model.add(LSTM(units=25, return_sequences=True, activation='relu'))\n    model.add(LSTM(units=50, return_sequences=True, activation='relu'))\n    model.add(Bidirectional(LSTM(128, activation='relu')))\n    model.add(Dense(50, activation='relu'))\n    model.add(Dense(1))\n    model.compile(loss='mae', optimizer='adam', metrics=['mse'])","68314145":"plot_model(model, to_file='model.png')","7981dc77":"history = model.fit(X_train_series, Y_train, validation_data=(X_valid_series, Y_valid), epochs=10, callbacks=[early_stop, checkpoint], verbose=1)","f72b023f":"from tensorflow.keras.models import load_model\nsaved_model = load_model('best_model.h5')","8a0e0ce6":"#model.save('best_model.h5')\n'''\nmodel_csv = saved_model.to_csv()\nwith open(\"saved_model.csv\", \"w\") as csv_file:\n    csv_file.write(model_csv)\n'''    \nmodel_json = saved_model.to_json()\nwith open(\"saved_model.json\", \"w\") as json_file:\n    json_file.write(model_json)","6889c4ad":"import matplotlib.pyplot as plt \ntrain_loss = history.history['loss']\ntest_loss = history.history['val_loss']\n\nepoch_count = range(1, len(train_loss)+1)\n\nplt.plot(epoch_count, train_loss)\nplt.plot(epoch_count, test_loss)\nplt.title('loss history')\nplt.legend(['train', 'validation'])\nplt.xlabel('Epoch')\nplt.ylabel('MAE')\nplt.show()","bcb79075":"Y_train","ab6db94a":"#Normalized predictions:\ntrain_pred = model.predict(X_train_series)\nvalid_pred = model.predict(X_valid_series)\n\nprint('Train rmse (avec normalisation):', np.sqrt(mean_squared_error(Y_train, train_pred[:-1])))\nprint('Validation rmse (avec normalisation):', np.sqrt(mean_squared_error(Y_valid, valid_pred[:-1])))","7c319af7":"from sklearn.metrics import mean_absolute_error\nprint('Train mae (avec normalisation):', mean_absolute_error(Y_train, train_pred[:-1]))\nprint('Validation mae (avec normalisation):', mean_absolute_error(Y_valid, valid_pred[:-1]))","6a2f0242":"normalized_lstm_predictions = pd.DataFrame(Y_valid.values, columns=['Temperature'])\nnormalized_lstm_predictions.index = X_valid.index \nnormalized_lstm_predictions['Predicted Temperature'] = valid_pred[:-1]\nnormalized_lstm_predictions.head()","9d5b5a98":"normalized_lstm_predictions.plot()","c064c35d":"Y_valid_inv = temp_scaler.inverse_transform(Y_valid.values.reshape(-1, 1))\npred_inv = temp_scaler.inverse_transform(valid_pred)","77ff821f":"model_predictions = pd.DataFrame(Y_valid_inv, columns=['Temperature'])\nmodel_predictions.index = X_valid.index \nmodel_predictions['Predicted Temperature'] = pred_inv[:-1]\nmodel_predictions.head()","010a4c62":"import time\nmodel_predictions.to_csv('model-predictions{}.csv'.format(int(time.time())))","edc89c1c":"from sklearn.metrics import mean_absolute_error\nprint('Validation mae (sans normalisation):', mean_absolute_error(Y_valid_inv, pred_inv[:-1]))","3fe7a6b0":"def series_to_supervised(data, window=1, lag=1, dropnan=True, single=True):\n    cols, names = list(), list()\n    # Input sequence (t-n, ... t-1)\n    for i in range(window, 0, -1):\n        cols.append(data.shift(i))\n        names += [('%s(t-%d)' % (col, i)) for col in data.columns]\n    # Current timestep (t=0)\n    cols.append(data)\n    names += [('%s(t)' % (col)) for col in data.columns]\n    \n    #Single step & Multi step\n    if single:\n        cols.append(data.shift(-lag))\n        names += [('%s(t+%d)' % (col, lag)) for col in data.columns]\n    else:\n        for j in range(1, lag+1, 1):\n            cols.append(data.shift(-j))\n            names += [('%s(t+%d)' % (col, j)) for col in data.columns]\n\n    # Put it all together\n    agg = pd.concat(cols, axis=1)\n    agg.columns = names\n    agg.index = data.index\n    # Drop rows with NaN values\n    if dropnan:\n        agg.dropna(inplace=True)\n    return agg","f6101b3f":"from sklearn.preprocessing import MinMaxScaler\ndata_scaler = MinMaxScaler()\ndata_scaler.fit(data) \nnormalized_data = data_scaler.transform(data) \n\nnormalized_df = pd.DataFrame(normalized_data, columns=['p (mbar)','T (degC)','Tpot (K)','Tdew (degC)',\n                                                         'rh (%)','VPmax (mbar)','VPact (mbar)',\n                                                         'VPdef (mbar)','sh (g\/kg)','H2OC (mmol\/mol)',\n                                                         'rho (g\/m**3)','wv (m\/s)', 'max. wv (m\/s)','wd (deg)'])\nnormalized_df = normalized_df.set_index(data.index)\nnormalized_df.head()","63ef8a03":"window = 24\nlag = 24\nsingle_series = series_to_supervised(normalized_df, window=window, lag=lag)\nsingle_series.head()","042740d8":"labels_col = 'T (degC)(t+24)'\nlabels = single_series[labels_col]\nsingle_series = single_series.drop(['p (mbar)(t+24)','T (degC)(t+24)','Tpot (K)(t+24)','Tdew (degC)(t+24)','rh (%)(t+24)','VPmax (mbar)(t+24)','VPact (mbar)(t+24)','VPdef (mbar)(t+24)','sh (g\/kg)(t+24)','H2OC (mmol\/mol)(t+24)','rho (g\/m**3)(t+24)','wv (m\/s)(t+24)','max. wv (m\/s)(t+24)','wd (deg)(t+24)'], axis=1)\nX_train = single_series['2009-01-02 01:00:00':'01.01.2015 00:00:00']\nX_valid = single_series['01.01.2015 00:00:00':'2017-01-01 00:00:00'] \nY_train = labels['2009-01-02 01:00:00':'01.01.2015 00:00:00']\nY_valid = labels['01.01.2015 00:00:00':'2017-01-01 00:00:00']\nprint('Train set shape', X_train.shape)\nprint('Validation set shape', X_valid.shape)","9a5dff77":"print('Train set shape', Y_train.shape)\nprint('Validation set shape', Y_valid.shape)","7a94ad30":"X_train_reshaped = X_train.values.reshape(X_train.shape[0],25,14)\nX_valid_reshaped = X_valid.values.reshape(X_valid.shape[0],25,14)\nY_train_reshaped = Y_train.values.reshape(Y_train.shape[0],)\nY_valid_reshaped = Y_valid.values.reshape(Y_valid.shape[0],)\nprint('Train set shape',X_train_reshaped.shape,Y_train_reshaped.shape)\nprint('Validation set shape',X_valid_reshaped.shape,Y_valid_reshaped.shape)","b7ee1de4":"print(X_train_reshaped.shape[1])\nprint(X_train_reshaped.shape[2])","28f3a0f3":"single_model = Sequential()\nsingle_model.add(Conv1D(filters=256, kernel_size=2, activation='relu', input_shape=(X_train_reshaped.shape[1], X_train_reshaped.shape[2])))\n    #model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\nsingle_model.add(MaxPooling1D(pool_size=2))\nsingle_model.add(Flatten())\nsingle_model.add(RepeatVector(30))\n    #model.add(LSTM(units=25, return_sequences=True, activation='relu'))\n    #model.add(Dropout(0.2))\n    #model.add(LSTM(units=25, return_sequences=True, activation='relu'))\n    #model.add(Dropout(0.2))\n    #model.add(LSTM(units=25, return_sequences=True, activation='relu'))\nsingle_model.add(LSTM(units=50, return_sequences=True, activation='relu'))\nsingle_model.add(Bidirectional(LSTM(128, activation='relu')))\nsingle_model.add(Dense(50, activation='relu'))\nsingle_model.add(Dense(1))\nsingle_model.compile(loss='mae', optimizer='adam', metrics=['mse'])","bcd53a1c":"single_history = single_model.fit(X_train_reshaped, Y_train_reshaped, validation_data=(X_valid_reshaped, Y_valid_reshaped), epochs=10, verbose=1)","7ca72d43":"#Ploting history:\nimport matplotlib.pyplot as plt \nsingle_loss = single_history.history['loss']\nsingle_val_loss = single_history.history['val_loss']\n\nepoch_count = range(1, len(single_loss)+1)\n\nplt.plot(epoch_count, single_loss)\nplt.plot(epoch_count, single_val_loss)\nplt.title('loss history')\nplt.legend(['train', 'validation'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss value')\nplt.show()","286fcc7e":"#Normalized predictions:\nfrom sklearn.metrics import mean_absolute_error\n\nsingle_train_pred = single_model.predict(X_train_reshaped)\nsingle_valid_pred = single_model.predict(X_valid_reshaped)\n\nprint('Train MAE (avec normalisation):', mean_absolute_error(Y_train_reshaped, single_train_pred))\nprint('Validation MAE (avec normalisation):', mean_absolute_error(Y_valid_reshaped, single_valid_pred))","a34c69e5":"#Saving history in a csv file :\nimport time\nsingle_step_hist_df = pd.DataFrame(single_history.history) \nsingle_step_csv_file = 'single-step-history-{}.csv'.format(int(time.time()))\nwith open(single_step_csv_file, mode='w') as file:\n    single_step_hist_df.to_csv(file)","3a21040b":"single_normalized_predictions = pd.DataFrame(Y_valid.values, columns=['Temperature'])\nsingle_normalized_predictions.index = X_valid.index \nsingle_normalized_predictions['Predicted Temperature'] = single_valid_pred\nsingle_normalized_predictions.head()","8b5412ac":"single_normalized_predictions.plot()","4b08f8e5":"temp_data = data['T (degC)']                               \ntemp_data = pd.DataFrame({'Date Time': data.index, 'T (degC)':temp_data.values})\ntemp_data = temp_data.set_index(['Date Time'])\ntemp_data.head()","b06373cd":"from sklearn.preprocessing import MinMaxScaler\ntemp_scaler = MinMaxScaler()\ntemp_scaler.fit(temp_data) \nnormalized_temp = temp_scaler.transform(temp_data) ","7e4300df":"from sklearn.metrics import mean_absolute_error\n\ny_val = temp_data['01.01.2015 00:00:00':'2016-12-31 00:00:00']\nsingle_pred_inv = temp_scaler.inverse_transform(single_valid_pred)\n\nprint('Validation mae (sans normalisation):', mean_absolute_error(y_val, single_pred_inv))","d487db3c":"single_predictions = pd.DataFrame(y_val.values, columns=['True Temperature'])\nsingle_predictions.index = y_val.index \nsingle_predictions['Predicted Temperature'] = single_pred_inv\nprint(single_predictions)","8a917516":"single_predictions.plot()","eeea7f89":"single_predictions.to_csv('single-step-normal-predictions{}.csv'.format(int(time.time())))","866c86b3":"window = 72\nlag = 24\nmulti_series = series_to_supervised(normalized_df, window=window, lag=lag, single=False)\nmulti_series.head()","eb1e1cb2":"temp_cols = [('T (degC)(t+%d)' % (i))for i in range(1, lag+1)]\nmulti_temp = multi_series[temp_cols]\ncolumns = [('%s(t+%d)' % (col,lg)) for col in data.columns for lg in range(1,lag+1)]\n\nmulti_series = multi_series.drop(columns, axis=1)\nmulti_series.head()","f515a3cb":"X_train_multi = multi_series['2009-01-02 01:00:00':'01.01.2015 00:00:00']\nX_valid_multi = multi_series['01.01.2015 00:00:00':'2017-01-01 00:00:00'] \nY_train_multi = multi_temp['2009-01-02 01:00:00':'01.01.2015 00:00:00']\nY_valid_multi = multi_temp['01.01.2015 00:00:00':'2017-01-01 00:00:00']\nprint('Train set shape', X_train_multi.shape,Y_train_multi.shape)\nprint('Validation set shape', X_valid_multi.shape, Y_valid_multi.shape)","fdb8a7e8":"x_train_multi = X_train_multi.values.reshape(X_train_multi.shape[0],73,14)\nx_valid_multi = X_valid_multi.values.reshape(X_valid_multi.shape[0],73,14)\ny_train_multi = Y_train_multi.values.reshape(Y_train_multi.shape[0],24)\ny_valid_multi = Y_valid_multi.values.reshape(Y_valid_multi.shape[0],24)\n\nprint('Train set shape', x_train_multi.shape,y_train_multi.shape)\nprint('Validation set shape', x_valid_multi.shape, y_valid_multi.shape)","01022e2b":"# (x_train_multi.shape[1], x_train_multi.shape[2])\nmulti_model = Sequential()\nmulti_model.add(Conv1D(filters=256, kernel_size=2, activation='relu', input_shape=(x_train_multi.shape[1], x_train_multi.shape[2])))\n    #model.add(Conv1D(filters=128, kernel_size=2, activation='relu'))\nmulti_model.add(MaxPooling1D(pool_size=2))\nmulti_model.add(Flatten())\nmulti_model.add(RepeatVector(30))\n    #model.add(LSTM(units=25, return_sequences=True, activation='relu'))\n    #model.add(Dropout(0.2))\n    #model.add(LSTM(units=25, return_sequences=True, activation='relu'))\n    #model.add(Dropout(0.2))\n    #model.add(LSTM(units=25, return_sequences=True, activation='relu'))\nmulti_model.add(LSTM(units=50, return_sequences=True, activation='relu'))\nmulti_model.add(Bidirectional(LSTM(128, activation='relu')))\nmulti_model.add(Dense(50, activation='relu'))\nmulti_model.add(Dense(24))\nmulti_model.compile(loss='mae', optimizer='adam', metrics=['mse'])","3ece729e":"multi_step_history = multi_model.fit(x_train_multi, y_train_multi, validation_data=(x_valid_multi, y_valid_multi), epochs=10, verbose=1)","f7ad39c7":"#Ploting history:\nimport matplotlib.pyplot as plt \nmulti_step_loss = multi_step_history.history['loss']\nmulti_step_val_loss = multi_step_history.history['val_loss']\n\nepoch_count = range(1, len(multi_step_loss)+1)\n\nplt.plot(epoch_count, multi_step_loss)\nplt.plot(epoch_count, multi_step_val_loss)\nplt.title('Multi step model - loss history')\nplt.legend(['train', 'validation'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss value')\nplt.show()","d77dfcfd":"#Ploting history:\nimport matplotlib.pyplot as plt \nmulti_step_loss = multi_step_history.history['mse']\nmulti_step_val_loss = multi_step_history.history['val_mse']\n\nepoch_count = range(1, len(multi_step_loss)+1)\n\nplt.plot(epoch_count, multi_step_loss)\nplt.plot(epoch_count, multi_step_val_loss)\nplt.title('Multi step model - loss history')\nplt.legend(['train', 'validation'])\nplt.xlabel('Epoch')\nplt.ylabel('Loss value')\nplt.show()","0e95a05d":"# save the model to disk\nimport pickle\nfilename = 'multi_step_model.sav'\npickle.dump(multi_model, open(filename, 'wb'))\n# load the model from disk\nloaded_model = pickle.load(open(filename, 'rb'))","80fedfee":"#Normalized predictions:\nfrom sklearn.metrics import mean_absolute_error\n\nmulti_train_pred = multi_model.predict(x_train_multi)\nmulti_valid_pred = multi_model.predict(x_valid_multi)\n\nprint('Train MAE (avec normalisation):', mean_absolute_error(y_train_multi, multi_train_pred))\nprint('Validation MAE (avec normalisation):', mean_absolute_error(y_valid_multi, multi_valid_pred))","1e6c50f7":"true_temp = pd.DataFrame(Y_valid_multi.values, columns=['%s h'%i for i in range(1,25)], index=X_valid_multi.index)\npred_temp = pd.DataFrame(multi_valid_pred, columns=['%s h'%i for i in range(1,25)], index=X_valid_multi.index)\n\nmulti_normalized_predictions = pd.concat([true_temp, pred_temp], axis=1)\nprint(multi_normalized_predictions)\n\nmulti_normalized_predictions.head()","6189ef94":"temp_data = data['T (degC)']                               \ntemp_data = pd.DataFrame({'Date Time': data.index, 'T (degC)':temp_data.values})\ntemp_data = temp_data.set_index(['Date Time'])\ntemp_data.head()","c71fde9c":"from sklearn.preprocessing import MinMaxScaler\ntemp_scaler = MinMaxScaler()\ntemp_scaler.fit(temp_data) \nnormalized_temp = temp_scaler.transform(temp_data)","43aea4ec":"y_val = temp_data['01.01.2015 00:00:00':'2016-12-31 00:00:00']","484373b5":"Y_valid_multi_inv = temp_scaler.inverse_transform(Y_valid_multi.values)\nmulti_pred_inv = temp_scaler.inverse_transform(multi_valid_pred)","801994bf":"tr = pd.DataFrame(Y_valid_multi_inv, columns=['True %s h'%i for i in range(1,25)], index=X_valid_multi.index)\npr = pd.DataFrame(multi_pred_inv, columns=['Pred %s h'%i for i in range(1,25)], index=X_valid_multi.index)\n\nmulti_predictions = pd.concat([tr, pr], axis=1) \nmulti_predictions.head()","788d954b":"import time\nmulti_predictions.to_csv('multi-step-normal-predictions{}.csv'.format(int(time.time())))","999b2f7e":"from sklearn.metrics import mean_absolute_error\nprint('Validation mae (sans normalisation):', mean_absolute_error(Y_valid_multi_inv, multi_pred_inv))","13a1c983":"multi_predictions.plot(figsize=(20,15))","d5b855df":"#multi_predictions.plot(subplot=True, figsize=(20,15))\ncouples = []\nfor i in range(1,24):\n    tr = str('True '+str(i)+' h')\n    pr = str('Pred '+str(i)+' h')\n    #c = [tr,pr]\n    couples.append([tr,pr])\n    #multi_predictions[[tr , pr ]].plot(figsize=(20,15))\n\ntrue_pred_data = []\nfor c in couples:\n    true_pred_data.append(multi_predictions[c])#.plot(figsize=(15,11))\nfig, axs = plt.subplots(23, figsize=(15,20))\nfig.suptitle('True vs Pred')\ni=0\nfor c in couples:\n    #true_pred_data.append(multi_predictions[c])#.plot(figsize=(15,11))\n    axs[i].plot(multi_predictions[c])\n    axs[i].set_title(c)\n    i=i+1","1b109047":"for c in couples:\n    multi_predictions[c].plot()","f42ff5a4":"***Transform the data into a time series problem***","f5f56760":"# Multi step model :","74cb7cd3":"# single step model:","abea3dd6":"# CNN-LSTM Model :"}}