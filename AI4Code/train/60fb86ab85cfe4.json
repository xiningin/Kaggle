{"cell_type":{"d7631a34":"code","16286ed6":"code","30169861":"code","855e0ed5":"code","3bbccca7":"code","f0f0c2fa":"code","60002031":"code","a6140224":"code","1d355692":"code","8f528614":"code","cc5ca6f3":"code","4ca3dd84":"code","c72b231c":"code","63a5039f":"code","8686aa64":"code","36358ef1":"code","72c11b4c":"code","c3ef619a":"code","a29d241d":"markdown","637c4d70":"markdown","d881a226":"markdown","5652a976":"markdown","311df24c":"markdown","a7449891":"markdown","4c16b2de":"markdown","d4bdcb7d":"markdown","5e02d71b":"markdown","8fcdb307":"markdown","1ecbcb13":"markdown","845368b9":"markdown","c6760c73":"markdown","6990fd19":"markdown","8cecd1f8":"markdown","758a97e0":"markdown","beb45d9a":"markdown","71e7fc79":"markdown","bc8dc4f0":"markdown","d4d01578":"markdown","1fa6a237":"markdown","a9a5f94f":"markdown","f4e9064a":"markdown","36d825bd":"markdown"},"source":{"d7631a34":"!pip install tnt-tensorflow","16286ed6":"import os\nimport time\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport PIL\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nfrom tnt import TNT\n\ntf.get_logger().setLevel('ERROR')","30169861":"import pathlib\ndataset_url = \"https:\/\/storage.googleapis.com\/download.tensorflow.org\/example_images\/flower_photos.tgz\"\ndata_dir = tf.keras.utils.get_file('flower_photos', origin=dataset_url, untar=True)\ndata_dir = pathlib.Path(data_dir)","855e0ed5":"image_count = len(list(data_dir.glob('*\/*.jpg')))\nprint(image_count)","3bbccca7":"roses = list(data_dir.glob('roses\/*'))\nPIL.Image.open(str(roses[0]))","f0f0c2fa":"tulips = list(data_dir.glob('tulips\/*'))\nPIL.Image.open(str(tulips[0]))","60002031":"batch_size = 32\nimg_height = 256\nimg_width = 256","a6140224":"train_ds = tf.keras.utils.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"training\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","1d355692":"val_ds = tf.keras.utils.image_dataset_from_directory(\n  data_dir,\n  validation_split=0.2,\n  subset=\"validation\",\n  seed=123,\n  image_size=(img_height, img_width),\n  batch_size=batch_size)","8f528614":"class_names = train_ds.class_names\nprint(class_names)","cc5ca6f3":"import matplotlib.pyplot as plt\n\nplt.figure(figsize=(10, 10))\nfor images, labels in train_ds.take(1):\n  for i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(images[i].numpy().astype(\"uint8\"))\n    plt.title(class_names[labels[i]])\n    plt.axis(\"off\")","4ca3dd84":"for image_batch, labels_batch in train_ds:\n  print(image_batch.shape)\n  print(labels_batch.shape)\n  break","c72b231c":"AUTOTUNE = tf.data.AUTOTUNE\n\ntrain_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\nval_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)","63a5039f":"normalization_layer = layers.Rescaling(1.\/255)","8686aa64":"normalized_ds = train_ds.map(lambda x, y: (normalization_layer(x), y))\nimage_batch, labels_batch = next(iter(normalized_ds))\nfirst_image = image_batch[0]\n# Notice the pixel values are now in `[0,1]`.\nprint(np.min(first_image), np.max(first_image)) ","36358ef1":"num_classes = len(class_names)\n\ntnt = TNT(\n    image_size=256,  # size of image\n    patch_dim=256,  # dimension of patch token\n    pixel_dim=6,  # dimension of pixel token\n    patch_size=16,  # patch size\n    pixel_size=4,  # pixel size\n    depth=2,  # depth\n    num_classes=num_classes,  # output number of classes\n    attn_dropout=0.1,  # attention dropout\n    ff_dropout=0.1,  # feedforward dropout\n)\nmodel = tf.keras.Sequential([tf.keras.layers.Lambda(lambda x: tf.einsum(\"...ijk->...kij\", x)),\n                             tnt])","72c11b4c":"optimizer = tf.keras.optimizers.Adam()\nloss_fn = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)","c3ef619a":"epochs = 5\n\nfor epoch in range(epochs):\n    print(\"\\nStart of epoch %d\" % (epoch,))\n    start_time = time.time()\n\n    for step, (x_batch_train, y_batch_train) in enumerate(train_ds):\n        with tf.GradientTape() as tape:\n            logits = model(x_batch_train, training=True)\n            loss_value = loss_fn(y_batch_train, logits)\n        grads = tape.gradient(loss_value, model.trainable_weights)\n        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n\n        if step % 50 == 0:\n            print(\n                \"Training loss (for one batch) at step %d: %.4f\"\n                % (step, float(loss_value))\n            )\n            print(\"Seen so far: %d samples\" % ((step + 1) * batch_size))","a29d241d":"We will train a model using these datasets. If you like, you can also manually iterate over the dataset and retrieve batches of images:","637c4d70":"The `image_batch` is a tensor of the shape `(32, 256, 256, 3)`. This is a batch of 32 images of shape `256x256x3` (the last dimension refers to color channels RGB). The `label_batch` is a tensor of the shape `(32,)`, these are corresponding labels to the 32 images.\n\n> Note: My implementation though is created to work in the `channels_first format` (you can easily change this) whereas our dataset is in well `channels_last` and we will take care of this while building the model.\n\nYou can call `.numpy()` on the `image_batch` and `labels_batch` tensors to convert them to a `numpy.ndarray`.\n","d881a226":"# Create the model\n\nWe will now create a [Sequential](https:\/\/www.tensorflow.org\/guide\/keras\/sequential_model) model, the majority of the work here is done by the `TNT` class I provide through my implementation. We also need to handle the channels format like we talked about which is handled by the `einsum` operation.\n\n> Note: Since the goal of this notebook to show a minimal training example we set the depth to be 2 and lower number of dimensions for path and pixel tokens. The accuracy can be increased considerably otherwise.","5652a976":"The loss and accuracy could have been made a lot better however the goal of this tutorial is to show a training example which is quick to run and you could most certainly increase the number of epochs as well as update some model parameters to improve the performance. In the [GitHub repository](https:\/\/github.com\/Rishit-dagli\/Transformer-in-Transformer) you would also find a pre-trained model to reproduce the paper results.","311df24c":"Or, you can include the layer inside your model definition, which can simplify deployment. Let's use the second approach here.","a7449891":"## Setup\n\nInstall the PyPI package allowing us to implement TNT architecture.","4c16b2de":"After downloading, you should now have a copy of the dataset available. There are 3,670 total images:","d4bdcb7d":"There are two ways to use this layer. You can apply it to the dataset by calling `Dataset.map`:","5e02d71b":"It's good practice to use a validation split when developing your model. Let's use 80% of the images for training, and 20% for validation.","8fcdb307":"You can find the class names in the `class_names` attribute on these datasets. These correspond to the directory names in alphabetical order.","1ecbcb13":"Here are some roses:","845368b9":"Define some parameters for the loader:","c6760c73":"And some tulips:","6990fd19":"## Configure the dataset for performance\n\nLet's make sure to use buffered prefetching so you can yield data from disk without having I\/O become blocking. These are two important methods you should use when loading data:\n\n- `Dataset.cache` keeps the images in memory after they're loaded off disk during the first epoch. This will ensure the dataset does not become a bottleneck while training your model. If your dataset is too large to fit into memory, you can also use this method to create a performant on-disk cache.\n- `Dataset.prefetch` overlaps data preprocessing and model execution while training.\n\nInterested readers can learn more about both methods, as well as how to cache data to disk in the *Prefetching* section of the [Better performance with the tf.data API](..\/..\/guide\/data_performance.ipynb) guide.","8cecd1f8":"## Create a dataset","758a97e0":"## Visualize the data\n\nHere are the first nine images from the training dataset:","beb45d9a":"Note: You previously resized images using the `image_size` argument of `tf.keras.utils.image_dataset_from_directory`. If you want to include the resizing logic in your model as well, you can use the `tf.keras.layers.Resizing` layer.","71e7fc79":"This tutorial uses a dataset of about 3,700 photos of flowers. The dataset contains five sub-directories, one per class:\n\n```\nflower_photo\/\n  daisy\/\n  dandelion\/\n  roses\/\n  sunflowers\/\n  tulips\/\n```","bc8dc4f0":"# Image classification with Transformer iN Transformer\n\nThis notebook shows how to train the recent [Transformer in Transformer](https:\/\/arxiv.org\/abs\/2103.00112) paper by Han et al. for image classification. Transformer in Transformer uses pixel level attention paired with patch level attention for image classification helping reduce the local inductive bias of transformers.\n\nWe will also use my implementation of this paper [Rishit-dagli\/Transformer-in-Transformer](https:\/\/github.com\/Rishit-dagli\/Transformer-in-Transformer) in the notebook.\n\nThis examples follows this workflow:\n\n1. Examine and understand data\n2. Build an input pipeline\n3. Build the model\n4. Write a custom training loop to train the model\n\nIn this example I use the [TensorFlow Flowers](https:\/\/www.tensorflow.org\/datasets\/catalog\/tf_flowers) dataset containing more than 3500 images of  flowers, this could easily be done for other datasets as well. The way we load the dataset is heavily inspired by [Image classification](https:\/\/www.tensorflow.org\/tutorials\/images\/classification) tutorial on TensorFlow's website. To foster readability and reproducibility is to try and keep the notebooks very similar to the Image Classification tutorial on TensorFlow website.","d4d01578":"## Standardize the data","1fa6a237":"## Download and explore the dataset","a9a5f94f":"## Train the model\n\nWe will now start by writing a custom training loop. Feel free to add the `@tf.function` decorator to speed this up.","f4e9064a":"The RGB channel values are in the `[0, 255]` range. This is not ideal for a neural network; in general you should seek to make your input values small.\n\nHere, you will standardize values to be in the `[0, 1]` range by using `tf.keras.layers.Rescaling`:","36d825bd":"# Load data using a Keras utility\n\nLet's load these images off disk using the helpful `tf.keras.utils.image_dataset_from_directory` utility. This will take you from a directory of images on disk to a `tf.data.Dataset` in just a couple lines of code. If you like, you can also write your own data loading code from scratch by visiting the [Load and preprocess images](..\/load_data\/images.ipynb) tutorial."}}