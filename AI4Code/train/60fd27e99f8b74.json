{"cell_type":{"f0bbec96":"code","5255b0ff":"code","f7efa03e":"code","fcfdba7b":"code","a44d8f50":"code","619ee5a9":"code","55623cf6":"code","8f9c62df":"code","576c7481":"code","24774b5f":"code","d83df211":"code","f6ac5e22":"code","f6206613":"code","7768a488":"code","8f28603d":"code","bf4c7a77":"code","873baca8":"code","0c4c511e":"code","2ffca4a6":"code","d7e87556":"code","b23117b1":"code","9470d39f":"code","9b9854cd":"code","3dfa9b55":"code","9af30db2":"code","6a382dab":"code","45076e30":"code","286a4dc1":"code","3424b23f":"code","60e7e0df":"code","12c68b36":"markdown","5e34553b":"markdown","6e34af58":"markdown","4bbb58e9":"markdown","227a3129":"markdown","05f928c8":"markdown","549479bf":"markdown","6d53639d":"markdown","daa3f685":"markdown","52b2fcee":"markdown","ab88472c":"markdown","a50fabd6":"markdown"},"source":{"f0bbec96":"import pandas as pd \nimport numpy as np\nimport os\nimport cv2\nimport skimage\n\nimageSize=50\ntrain_dir = \"..\/input\/asl-alphabet\/asl_alphabet_train\/asl_alphabet_train\/\"\ntest_dir =  \"..\/input\/asl-alphabet\/asl_alphabet_test\/asl_alphabet_test\/\"\nfrom tqdm import tqdm\ndef get_data(folder):\n    X = []\n    y = []\n    i = 0\n    for folderName in os.listdir(folder):\n        \n        if not folderName.startswith('.'):\n            if folderName in ['A']:label = 0\n            elif folderName in ['B']:label = 1\n            elif folderName in ['C']:label = 2\n            elif folderName in ['D']:label = 3\n            elif folderName in ['E']:label = 4\n            elif folderName in ['F']:label = 5\n            elif folderName in ['G']:label = 6\n            elif folderName in ['H']:label = 7\n            elif folderName in ['I']:label = 8\n            elif folderName in ['J']:label = 9\n            elif folderName in ['K']:label = 10\n            elif folderName in ['L']:label = 11\n            elif folderName in ['M']:label = 12\n            elif folderName in ['N']:label = 13\n            elif folderName in ['O']:label = 14\n            elif folderName in ['P']:label = 15\n            elif folderName in ['Q']:label = 16\n            elif folderName in ['R']:label = 17\n            elif folderName in ['S']:label = 18\n            elif folderName in ['T']:label = 19\n            elif folderName in ['U']:label = 20\n            elif folderName in ['V']:label = 21\n            elif folderName in ['W']:label = 22\n            elif folderName in ['X']:label = 23\n            elif folderName in ['Y']:label = 24\n            elif folderName in ['Z']:label = 25\n            elif folderName in ['del']:label = 26\n            elif folderName in ['nothing']:label = 27\n            elif folderName in ['space']:label = 28           \n            else:label = 29\n            i=0\n            for image_filename in tqdm(os.listdir(folder + folderName)):\n                if i > 1000:\n                    break\n                i =  i+1\n                \n                img_file = cv2.imread(folder + folderName + '\/' + image_filename)\n                if img_file is not None:\n                    img_file = skimage.transform.resize(img_file, (imageSize, imageSize, 3))\n                    img_arr = np.asarray(img_file)\n                    X.append(img_arr)\n                    y.append(label)\n    X = np.asarray(X)\n    y = np.asarray(y)\n    return X,y\nX_train_load, y_train_load = get_data(train_dir)","5255b0ff":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_train_load, y_train_load, test_size=0.2, random_state=30) ","f7efa03e":"# Encode labels to hot vectors (ex : 2 -> [0,0,1,0,0,0,0,0,0,0])\nfrom keras.utils.np_utils import to_categorical\ny_trainHot = to_categorical(y_train, num_classes = 30)\ny_testHot = to_categorical(y_test, num_classes = 30)","fcfdba7b":"# Shuffle data to permit further subsampling\nfrom sklearn.utils import shuffle\nX_train, y_trainHot = shuffle(X_train, y_trainHot, random_state=13)\nX_test, y_testHot = shuffle(X_test, y_testHot, random_state=13)","a44d8f50":"import matplotlib.pyplot as plt\ndef plotHistogram(a):\n    \"\"\"\n    #Plot histogram of RGB Pixel Intensities\n    \"\"\"\n    \n    plt.figure(figsize=(10,5))\n    plt.subplot(1,2,1)\n    plt.imshow(a)\n    plt.axis('off')\n    histo = plt.subplot(1,2,2)\n    histo.set_ylabel('Count')\n    histo.set_xlabel('Pixel Intensity')\n    n_bins = 30\n    plt.hist(a[:,:,0].flatten(), bins= n_bins, lw = 0, color='r', alpha=0.5);\n    plt.hist(a[:,:,1].flatten(), bins= n_bins, lw = 0, color='g', alpha=0.5);\n    plt.hist(a[:,:,2].flatten(), bins= n_bins, lw = 0, color='b', alpha=0.5);\\\n    #print(title)\nplotHistogram(X_train[0])\nplotHistogram(X_train[2])\nplotHistogram(X_train[21])","619ee5a9":"import glob\nimport os\nimport random\n\nmylist = os.listdir('..\/input\/asl-alphabet\/asl_alphabet_train\/asl_alphabet_train\/A')\ndef plotThreeImages(path,images):\n    r = random.sample(images, 3)\n    plt.figure(figsize=(16,16))\n    plt.subplot(131)\n    plt.imshow(cv2.imread(path+r[0]))\n    plt.subplot(132)\n    plt.imshow(cv2.imread(path+r[1]))\n    plt.subplot(133)\n    plt.imshow(cv2.imread(path+r[2]))\n    \nplotThreeImages('..\/input\/asl-alphabet\/asl_alphabet_train\/asl_alphabet_train\/A\/', mylist)","55623cf6":"import seaborn as sns\nmap_characters = {0: 'A', 1: 'B', 2: 'C', 3: 'D', 4: 'E', 5: 'F', 6: 'G', 7: 'H', 8: 'I', 9: 'J', 10: 'K', 11: 'L', 12: 'M', 13: 'N', 14: 'O', 15: 'P', 16: 'Q', 17: 'R', 18: 'S', 19: 'T', 20: 'U', 21: 'V', 22: 'W', 23: 'X', 24: 'Y', 25: 'Z', 26: 'del', 27: 'nothing', 28: 'space', 29: 'other'}\ndict_characters=map_characters\n\ndf = pd.DataFrame()\ndf[\"labels\"]=y_train\nlab = df['labels']\ndist = lab.value_counts()\nsns.countplot(lab)\nprint(dict_characters)","8f9c62df":"import keras\nfrom keras import optimizers\nfrom sklearn.utils import class_weight\nfrom keras.applications.vgg16 import VGG16\n\nmap_characters1 = map_characters\nclass_weight1 = class_weight.compute_class_weight('balanced', np.unique(y_train), y_train)\nweight_path1 = '..\/input\/keras-pretrained-models\/vgg16_weights_tf_dim_ordering_tf_kernels_notop.h5'\nweight_path2 = '..\/input\/keras-pretrained-models\/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\npretrained_model_1 = VGG16(weights = weight_path1, include_top=False, input_shape=(imageSize, imageSize, 3))\n#optimizer1 = optimizers.Adam()\n#optimizer2 = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)\n#optimizer3 = optimizers.RMSprop(learning_rate)\n#optimizer4 = optimizers.SGD(lr=0.01, clipnorm=1.)","576c7481":"optimizer_RMSprop = keras.optimizers.RMSprop(lr=0.001, rho=0.9, epsilon=None, decay=0.0)","24774b5f":"optimizer_Adagrad = keras.optimizers.Adagrad(lr=0.01, epsilon=None, decay=0.0)","d83df211":"optimizer_adadelta = keras.optimizers.Adadelta(lr=1.0, rho=0.95, epsilon=None, decay=0.0)","f6ac5e22":"optimizer_Adam = keras.optimizers.Adam(lr=0.001, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0, amsgrad=False)","f6206613":"optimizer_adax = keras.optimizers.Adamax(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, decay=0.0)","7768a488":"optimizer_Nadam = keras.optimizers.Nadam(lr=0.002, beta_1=0.9, beta_2=0.999, epsilon=None, schedule_decay=0.004)","8f28603d":"def showChartLearningRate(history, epochs):\n    # show a nicely formatted classification report\n    print(\"[INFO] evaluating network...\")\n    #loss: 0.1149 - mean_absolute_error: 0.2270 - val_loss: 0.1080 - val_mean_absolute_error\n    # plot the training loss and accuracy\n    N = epochs\n    plt.style.use(\"ggplot\")\n    plt.figure()\n    \n    plt.plot(np.arange(0, N), history.history[\"loss\"], label=\"train_loss\")\n    plt.plot(np.arange(0, N), history.history[\"val_loss\"], label=\"val_loss\")\n    plt.plot(np.arange(0, N), history.history[\"acc\"], label=\"train_acc\")\n    plt.plot(np.arange(0, N), history.history[\"val_acc\"], label=\"val_acc\")\n    plt.title(\"Training Loss and validation loss on Dataset\")\n    plt.xlabel(\"Epochs #\")\n    plt.ylabel(\"Loss\/Mean_absolute_error\")\n    #plt.xticks(sd)\n    #plt.xticklabels(learningValue)\n    plt.legend(loc=\"lower left\")\n    plt.show()","bf4c7a77":"import itertools \ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.figure(figsize = (8,8))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=90)\n    plt.yticks(tick_marks, classes)\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","873baca8":"from keras.layers import Dense, Flatten\nfrom keras.models import Model\nimport keras.callbacks\nimport keras\nfrom keras.optimizers import SGD\nfrom keras.callbacks import LearningRateScheduler\n\n#from keras.callbacks import Callback, EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n#from sklearn.model_selection import train_test_split, KFold, cross_val_score, StratifiedKFold, learning_curve, GridSearchCV\n#from tensorflow.keras.callbacks import keras\ndef pretrainedNetwork(xtrain,ytrain,xtest,ytest,pretrainedmodel,pretrainedweights,classweight,numclasses,numepochs,optimizer,labels):\n    base_model = pretrained_model_1 # Topless\n    # Add top layer\n    x = base_model.output\n    x = Flatten()(x)\n    predictions = Dense(numclasses, activation='softmax')(x)\n    model = Model(inputs=base_model.input, outputs=predictions)\n    # Train top layer\n    for layer in base_model.layers:\n        layer.trainable = False\n    \n    model.compile(loss='categorical_crossentropy', \n                  optimizer=optimizer, \n                  metrics=['mean_absolute_error','accuracy'])\n    \n    callbacks_list = [keras.callbacks.EarlyStopping(monitor='val_acc', patience=3, verbose=1)]\n    #model.summary()\n    \n    history = model.fit(xtrain,ytrain, epochs=numepochs, class_weight=classweight, validation_data=(xtest,ytest), verbose=1)\n    print('history loss')\n    print(history.history[\"loss\"])\n    print('history val_loss')\n    print(history.history[\"val_loss\"])\n    print('history mean_absolute_error')\n    print(history.history[\"mean_absolute_error\"])\n    print('history val_mean_absolute_error')\n    print(history.history[\"val_mean_absolute_error\"])\n    print('history val_mean_absolute_error')\n    print(history.history[\"val_mean_absolute_error\"])\n    \n     # Evaluate model\n    score = model.evaluate(xtest,ytest, verbose=0)\n    print('\\nKeras CNN - accuracy:', score[1], '\\n')\n    y_pred = model.predict(xtest)\n    \n    import sklearn\n    Y_pred_classes = np.argmax(y_pred,axis = 1) \n    Y_true = np.argmax(ytest,axis = 1) \n    \n    from sklearn.metrics import confusion_matrix\n    confusion_mtx = confusion_matrix(Y_true, Y_pred_classes) \n    print(confusion_mtx)\n    \n    showChartLearningRate(history, numepochs)\n    \n    plot_confusion_matrix(confusion_mtx, classes = list(labels.values()))\n    plt.show()\n    \n    return model\n#epochs = 3\n#mymodel = pretrainedNetwork(X_train, y_trainHot, X_test, y_testHot,pretrained_model_1,weight_path1,class_weight1,30,epochs,optimizer1,map_characters1)","0c4c511e":"epochs = 5\nmymodel_RMSprop = pretrainedNetwork(X_train, y_trainHot, X_test, y_testHot,pretrained_model_1,weight_path1,class_weight1,30,epochs,optimizer_RMSprop,map_characters1)","2ffca4a6":"epochs = 5\nmymodel_Adagrad = pretrainedNetwork(X_train, y_trainHot, X_test, y_testHot,pretrained_model_1,weight_path1,class_weight1,30,epochs,optimizer_Adagrad,map_characters1)","d7e87556":"epochs = 5\nmymodel_adadelta = pretrainedNetwork(X_train, y_trainHot, X_test, y_testHot,pretrained_model_1,weight_path1,class_weight1,30,epochs,optimizer_adadelta,map_characters1) ","b23117b1":"epochs = 5\nmymodel_Adam = pretrainedNetwork(X_train, y_trainHot, X_test, y_testHot,pretrained_model_1,weight_path1,class_weight1,30,epochs,optimizer_Adam,map_characters1) ","9470d39f":"epochs = 5\nmymodel_adax = pretrainedNetwork(X_train, y_trainHot, X_test, y_testHot,pretrained_model_1,weight_path1,class_weight1,30,epochs,optimizer_adax,map_characters1) ","9b9854cd":"epochs = 5\nmymodel_Nadam = pretrainedNetwork(X_train, y_trainHot, X_test, y_testHot,pretrained_model_1,weight_path1,class_weight1,30,epochs,optimizer_Nadam,map_characters1) ","3dfa9b55":"def get_data2(_img):\n    X = []\n    img_file = cv2.imread(_img)\n    img_file = skimage.transform.resize(img_file, (imageSize, imageSize, 3))\n    img_arr = np.asarray(img_file)\n    X.append(img_arr)               \n    X = np.asarray(X)\n    return X\n\nimg_p='..\/input\/asl-alphabet\/asl_alphabet_test\/asl_alphabet_test\/B_test.jpg'\nimg_preds= get_data2(img_p) ","9af30db2":"import numpy as geek\npre = mymodel_Nadam.predict(img_preds)\npred_idx = geek.argmax(pre, axis=1)\n\nprint(pred_idx)\nprint(pre)","6a382dab":"mymodel_Nadam.summary()","45076e30":"def get_output_layer(model, layer_name):\n    # get the symbolic outputs of each \"key\" layer (we gave them unique names).\n    layer_dict = dict([(layer.name, layer) for layer in model.layers])\n    layer = layer_dict[layer_name]\n    return layer","286a4dc1":"from keras.preprocessing import image\nfrom keras.applications.vgg16 import preprocess_input\ndef _load_image(img_path):    \n    img = image.load_img(img_path, target_size=(50,50))\n    img = image.img_to_array(img)\n    img = np.expand_dims(img, axis=0)\n    img = preprocess_input(img)\n    return img ","3424b23f":"def cam(img_path):\n  \n    import numpy as np\n    from keras.applications.vgg16 import decode_predictions\n    import matplotlib.image as mpimg\n    from keras import backend as K\n    import pandas as pd\n    import matplotlib.pyplot as plt\n    %matplotlib inline\n    #K.clear_session()\n    \n    img=mpimg.imread(img_path)\n    plt.imshow(img)\n    x = _load_image(img_path)\n    preds = mymodel_Nadam.predict(img_preds)\n    \n    #predictions = pd.DataFrame(decode_predictions(preds, top=3)[0],columns=['col1','category','probability']).iloc[:,1:]\n    argmax = np.argmax(preds[0])\n    output = mymodel_Nadam.output[:, argmax]\n    last_conv_layer = mymodel_Nadam.get_layer('block5_conv3')\n    grads = K.gradients(output, last_conv_layer.output)[0]\n    pooled_grads = K.mean(grads, axis=(0, 1, 2))\n    iterate = K.function([mymodel_Nadam.input], [pooled_grads, last_conv_layer.output[0]])\n    pooled_grads_value, conv_layer_output_value = iterate([x])\n    for i in range(125):\n        conv_layer_output_value[:, :, i] *= pooled_grads_value[i]\n    heatmap = np.mean(conv_layer_output_value, axis=-1)\n    heatmap = np.maximum(heatmap, 0)\n    heatmap \/= np.max(heatmap)\n    import cv2\n    img = cv2.imread(img_path)\n    heatmap = cv2.resize(heatmap, (img.shape[1], img.shape[0]))\n    heatmap = np.uint8(255 * heatmap)\n    heatmap = cv2.applyColorMap(heatmap, cv2.COLORMAP_JET)\n    hif = .8\n    superimposed_img = heatmap * hif + img\n    output = 'output.jpeg'\n    cv2.imwrite(output, superimposed_img)\n    img=mpimg.imread(output)\n    plt.imshow(img)\n    plt.axis('off')\n   # plt.title(predictions.loc[0,'category'].upper())\n    return None","60e7e0df":"cam(img_p)","12c68b36":"References:\nhttps:\/\/www.kaggle.com\/paultimothymooney\/interpret-sign-language-with-deep-learning\/output","5e34553b":"# Adagrad:\nAdagrad is an optimizer with parameter-specific learning rates, which are adapted relative to how frequently a parameter gets updated during training. The more updates a parameter receives, the smaller the learning rate.\n\nIt is recommended to leave the parameters of this optimizer at their default values.","6e34af58":"## Adadelta\nAdadelta is a more robust extension of Adagrad that adapts learning rates based on a moving window of gradient updates, instead of accumulating all past gradients. This way, Adadelta continues learning even when many updates have been done. Compared to Adagrad, in the original version of Adadelta you don't have to set an initial learning rate. In this version, initial learning rate and decay factor can be set, as in most other Keras optimizers.","4bbb58e9":"# Nadam\nMuch like Adam is essentially RMSprop with momentum, Nadam is Adam RMSprop with Nesterov momentum.\nDefault parameters follow those provided in the paper. It is recommended to leave the parameters of this optimizer at their default values.","227a3129":"# EDA","05f928c8":"# Class Activation Map","549479bf":"# Adam\nIt is a variant of Adam based on the infinity norm. Default parameters follow those provided in the paper.","6d53639d":"# RMSprop:\nIt is recommended to leave the parameters of this optimizer at their default values (except the learning rate, which can be freely tuned). This optimizer is usually a good choice for recurrent neural networks.\n\nhttps:\/\/keras.io\/optimizers\/","daa3f685":"# Adamax","52b2fcee":"# Learning Rate","ab88472c":"Members: \n* Thi Thuy Huong Nguyen \n* Nhat Khac Pham\n* Flor Maria Vargas ","a50fabd6":"# Hyperparmeters and Optimizer using pre trained weights"}}