{"cell_type":{"ca839c57":"code","9f0c225c":"code","42085572":"code","11329e29":"code","fce5dc70":"code","227fd8c1":"code","34398c3c":"code","951e5a85":"code","a381b613":"code","c7fb31db":"code","c93abbb1":"markdown","5f051cbe":"markdown","4ce75202":"markdown","f9980ed9":"markdown","a8377348":"markdown","261d5f1c":"markdown","a64b27bc":"markdown","22592025":"markdown","752fba08":"markdown","673fe633":"markdown"},"source":{"ca839c57":"# 1. Enable Internet in the Kernel (Settings side pane)\n\n# 2. Curl cache may need purged if v0.1.4 cannot be found (uncomment if needed). \n# !curl -X PURGE https:\/\/pypi.org\/simple\/kaggle-environments\n\n# ConnectX environment was defined in v0.1.4\n!pip install 'kaggle-environments>=0.1.4'","9f0c225c":"from kaggle_environments import evaluate, make\n\nenv = make(\"connectx\", debug=True)\nenv.render()","42085572":"# This agent random chooses a non-empty column.\ndef my_agent(observation, configuration):\n#     reward = calc_reward(observation, configuration)\n    choice = 3\n    print(\"config\")\n    print(str(configuration))\n    print(configuration.rows)\n    print(observation)\n    print(\"choice = %d\" % (choice))\n    return choice","11329e29":"def reward_vertical(matrix, rows_count, columns_count, value, goal_len = 4, empty = 0):\n    max_length = 0\n    for col in range(columns_count):\n        length = 0\n        for row in range(rows_count - 1, 0, -1):\n            if (matrix[row][col] == empty):\n                break\n            elif ((matrix[row][col] not in [value, empty]) & (length > 0)):\n                length = 0\n                break\n            elif (length + row + 1 < goal_len):\n                length = 0\n                break\n            elif (matrix[row][col] == value):\n                length += 1\n        # new col\n        max_length = max(max_length, length)\n    return max_length\n\ndef calc_reward(observation, configuration, done, agent_mark = 1, enemy_mark = 2):\n    board = np.array(observation.board).reshape([configuration.rows, configuration.columns])\n    \n    agent_vertical_reward = reward_vertical(board, configuration.rows, configuration.columns, agent_mark)\n    agent_reward = agent_vertical_reward\n#     print(\"agent: vertical_reward = {}\".format(agent_vertical_reward))\n    \n    enemy_vertical_reward = reward_vertical(board, configuration.rows, configuration.columns, enemy_mark)\n    enemy_reward = enemy_vertical_reward\n#     print(\"enemy: vertical_reward = {}\".format(enemy_vertical_reward))\n    \n    reward = 0\n    if (agent_reward == 4):\n        reward = 1.0\n    elif (agent_reward == enemy_reward):\n        reward = 0.25\n    elif (agent_reward - enemy_reward == 1):\n        reward = 0.5\n    elif (agent_reward - enemy_reward == 2):\n        reward = 0.75\n#     print(\"agent_reward = {}, enemy_reward = {}, result reward = {}\".format(agent_reward, enemy_reward, reward))\n    return reward","fce5dc70":"from kaggle_environments import Environment\n\nclass ConnectTrainer():\n    \"\"\"Connect Trainer\n    \"\"\"\n    def __init__(self, env, configuration):\n        self.env =  env\n        self.trainer = self.env.train([None, \"random\"])\n        self.configuration = configuration\n\n    def step(self, action):\n        action += 1\n        observation, reward, done, info = self.trainer.step(action.item())\n        new_reward = calc_reward(observation, self.configuration, done)\n#         print(\"reward = {}, new_reward = {}, done = {}\".format(reward, new_reward, done))\n        observation = observation.board\n#         print(np.array(observation).reshape(self.configuration.rows, self.configuration.columns))\n        return observation, new_reward, done, info\n\n    def reset(self):\n        observation = self.trainer.reset()\n        observation = observation.board\n        return observation","227fd8c1":"import numpy as np\nfrom matplotlib import pyplot\n\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Flatten\nfrom keras.optimizers import Adam\n\nfrom rl.agents import SARSAAgent\nfrom rl.policy import BoltzmannQPolicy\n\nENV_NAME = 'ConnectX-v0'\n\n# Get the environment and extract the number of actions.\n# Play as first position against random agent.\nwrap_env = ConnectTrainer(env, env.configuration)\nobservation = wrap_env.reset()\n\nnb_actions = env.configuration.columns - 1\ninput_array_size = env.configuration.columns * env.configuration.rows\n\n# Next, we build a very simple model.\nmodel = Sequential()\nmodel.add(Flatten(input_shape=(1, input_array_size)))\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\nmodel.add(Dense(nb_actions))\nmodel.add(Activation('linear'))\nprint(model.summary())\n\n# Init memory and model agent\npolicy = BoltzmannQPolicy()\nsarsa = SARSAAgent(model=model, nb_actions=nb_actions, nb_steps_warmup=10, policy=policy)\nsarsa.compile(Adam(lr=1e-3), metrics=['mae'])\n\n# Train\ntrain_history = sarsa.fit(wrap_env, nb_steps=50, visualize=False, verbose=2)\n\n# Plot history\ntrain_rewards = train_history.history['episode_reward']\npyplot.plot(train_rewards)\n\n# After training is done, we save the final weights.\nsarsa.save_weights('sarsa_{}_weights.h5f'.format(ENV_NAME), overwrite=True)","34398c3c":"env.reset()\n# Play as the first agent against default \"random\" agent.\nenv.run([my_agent, \"random\"])\nenv.render(mode=\"ipython\", width=500, height=450)","951e5a85":"# Play as first position against random agent.\ntrainer = env.train([None, \"random\"])\n\nobservation = trainer.reset()\n\nwhile not env.done:\n    my_action = my_agent(observation, env.configuration)\n    print(\"My Action\", my_action)\n    observation, reward, done, info = trainer.step(my_action)\n    print(\"observ = {}, reward = {}, done = {}, info = {}\".format(observation, reward, done, info))\n    # env.render(mode=\"ipython\", width=100, height=90, header=False, controls=False)\nenv.render()","a381b613":"def mean_reward(rewards):\n    return sum(r[0] for r in rewards) \/ sum(r[0] + r[1] for r in rewards)\n\n# Run multiple episodes to estimate it's performance.\nprint(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))","c7fb31db":"import inspect\nimport os\n\ndef write_agent_to_file(function, file):\n    with open(file, \"a\" if os.path.exists(file) else \"w\") as f:\n        f.write(inspect.getsource(function))\n        print(function, \"written to\", file)\n\nwrite_agent_to_file(my_agent, \"submission.py\")","c93abbb1":"# Custom reward functions","5f051cbe":"## Env wrapper","4ce75202":"# Create ConnectX Environment","f9980ed9":"# Evaluate your Agent","a8377348":"# Write Submission File\n\n","261d5f1c":"# Install kaggle-environments","a64b27bc":"# Test your Agent","22592025":"# Submit to Competition\n\n1. Commit this kernel.\n2. View the commited version.\n3. Go to \"Data\" section and find submission.py file.\n4. Click \"Submit to Competition\"\n5. Go to [My Submissions](https:\/\/kaggle.com\/c\/connectx\/submissions) to view your score and episodes being played.","752fba08":"# Create and train an Agent\n\nTo create the submission, an agent function should be fully encapsulated (no external dependencies).  \n\nWhen your agent is being evaluated against others, it will not have access to the Kaggle docker image.  Only the following can be imported: Python Standard Library Modules, gym, numpy, scipy (more may be added later). ","673fe633":"# Debug\/Train your Agent"}}