{"cell_type":{"e47df969":"code","1589fe63":"code","d7ad041f":"code","0e346154":"code","726d2747":"markdown"},"source":{"e47df969":"# based on https:\/\/www.kaggle.com\/vbmokin\/heart-disease-comparison-of-20-models\n# modified by Eric Na (Skrudals), 2, March, 2020\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n# preprocessing\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV\nimport pandas_profiling as pp\n\nfrom sklearn import metrics\n\n# NN models\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout\nfrom keras import optimizers\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom keras.callbacks import EarlyStopping, ModelCheckpoint\n\n# import warnings filter\nfrom warnings import simplefilter\n# ignore all future warnings\nsimplefilter(action='ignore', category=FutureWarning)","1589fe63":"data = pd.read_csv(\"..\/input\/heart-disease-uci\/heart.csv\") # kaggle\n\ntarget_name = 'target'\ndata_target = data[target_name]\ndata = data.drop([target_name], axis=1)\n\ntrain, test, target, target_test = train_test_split(data, data_target, test_size=0.2, random_state=0)","d7ad041f":"# get mean and std from training data\nmean = np.mean(train)\nstd = np.std(train)\n\n# normalization\ntrain = (train-mean)\/(std+1e-7)\ntest = (test-mean)\/(std+1e-7)\n\n# split training set to validation set\nXtrain, Xval, Ztrain, Zval = train_test_split(train, target, test_size=0.2, random_state=0)\n","0e346154":"# As NN is sensitive to its initialization and train_test_split splits validation set randomly,\n# the results may vary for each trial...\n# and so suggest to run several times...\n# Typically, about 92-93% for training data and about 90% (sometimes, over 91%, but sometimes 86~87%) for testing data\ndef build_ann(optimizer='adam'):\n\n    # Initializing the ANN\n    ann = Sequential()\n\n    # Adding the input layer and the first hidden layer of the ANN\n    ann.add(Dense(units=32, kernel_initializer='he_normal', activation='relu', input_shape=(len(train.columns),)))\n    # Adding the output layer\n    ann.add(Dense(units=1, kernel_initializer='he_normal', activation='sigmoid'))\n\n    # Compiling the ANN\n    ann.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\n\n    return ann\n\nopt = optimizers.Adam(lr=0.001)\nann = build_ann(opt)\n\n# Training the ANN\nhistory = ann.fit(Xtrain, Ztrain, batch_size=16, epochs=200, validation_data=(Xval, Zval))\n\n# Predicting the Train set results\nann_prediction = ann.predict(train)\nann_prediction = (ann_prediction > 0.5)*1 # convert probabilities to binary output\n\n# Compute error between predicted data and true response and display it in confusion matrix\nacc_ann1 = round(metrics.accuracy_score(target, ann_prediction) * 100, 2)\nprint(acc_ann1)\n\n# Predicting the Test set results\nann_prediction_test = ann.predict(test)\nann_prediction_test = (ann_prediction_test > 0.5)*1 # convert probabilities to binary output\n\n# Compute error between predicted data and true response and display it in confusion matrix\nacc_test_ann1 = round(metrics.accuracy_score(target_test, ann_prediction_test) * 100, 2)\nprint(acc_test_ann1)","726d2747":"This is a modified code for neural network based on Vitalii Mokin's work.\n1. apply standardization of data using means and standard deviations of training data (critical)\n2. remove other hidden layers except the first one\n3. set learning rate = 0.001\n4. set epochs = 200\n5. no dropout due to small no. of parameters and training data\n6. use He initialization"}}