{"cell_type":{"bca0d5a0":"code","8337dd07":"code","dcbc2e43":"code","174c234a":"code","5bedb1b2":"code","38829ba2":"code","e201958e":"code","616fb18b":"code","33b0c543":"code","d90c6154":"code","c26d625a":"code","cd0eec21":"code","844be99c":"code","7ef2f4ba":"code","c1caab02":"code","ac32ffcd":"code","a0be5e04":"code","fbb5d1dc":"code","a9bfe986":"code","27ad2713":"code","22d5eba4":"code","e7a3ae15":"code","15fed164":"code","ff77bc11":"code","0b957d19":"code","5b4cce28":"code","17a5853f":"code","1c87a163":"code","db5ddc35":"code","a9de4f27":"code","16e75223":"code","db60e854":"code","5c9862d2":"code","00e39710":"code","f32e9354":"code","23daa5c5":"code","5194cb61":"code","f7114212":"code","b4c87851":"code","6116f493":"code","ee43ac9b":"code","d2d073d7":"code","eaf83c29":"code","a5e61b6b":"code","73f14d9e":"code","a0c96de5":"code","5eaa2955":"code","bb7522ed":"code","1d9d5e08":"code","ec52e041":"code","ba400c14":"code","72d18b27":"code","4a012780":"code","9987f295":"code","0c5e5ea1":"code","b448ef9a":"code","3fd1f65c":"code","930cc37d":"code","5f3efefa":"code","8b958704":"code","e57b03f3":"code","ae5738df":"code","adba1ee0":"code","7a92bae8":"code","ba7ac929":"code","0d99bae4":"code","bf6f0d4d":"code","8868725b":"code","59f9c320":"code","99a2062e":"code","227d8552":"code","f6bfe7dd":"code","f247370e":"code","e36f2223":"code","8cf1c61b":"code","7171be10":"code","753d69c9":"code","920183e1":"code","4e12f632":"code","192058f3":"code","b662b9bb":"code","c86afa17":"code","9eef6830":"code","25435a1e":"code","11977567":"code","ed62c3fb":"code","c2d13bf8":"code","bce2842b":"code","a4c593c3":"code","b7e9245a":"code","b0c49cc7":"code","a35b0aa0":"code","7e5e4d5e":"code","ee87e660":"code","7fe6c37b":"code","81eb1461":"code","943006f7":"code","65588947":"code","d13f244c":"code","50b90451":"code","47088c6b":"code","ba2b4476":"code","5c70cd69":"code","fa09363f":"code","e59fb2ca":"code","5ec9c917":"code","5bf4330d":"code","aed9ac4f":"code","b62e5aac":"code","5a8cc162":"code","b21a92c2":"code","a7891fcb":"code","ded65d9d":"code","057c5fc5":"code","951fcb4d":"code","def65955":"code","49766170":"code","906651d9":"code","096e2b18":"code","94b391d2":"code","0609db04":"code","065ee6cd":"code","b3c4f82c":"code","be90c4fb":"code","df67228b":"code","b501d6eb":"code","b4fb6e01":"code","886160ca":"markdown","45ba5bb9":"markdown","64c6da04":"markdown","3a348d4a":"markdown","495b4bd7":"markdown","db885df7":"markdown","6aa64052":"markdown","9b877a60":"markdown","6f3e5cfb":"markdown","364810fa":"markdown","bfd61784":"markdown","ffb9a5b7":"markdown","4e74dc52":"markdown","2494b41c":"markdown","cdca97f6":"markdown","e2fe45ee":"markdown","e27132f1":"markdown","17ac85c0":"markdown","925b546c":"markdown","97590282":"markdown","7ff07877":"markdown","a525860e":"markdown","3e65bbb1":"markdown","dbf63126":"markdown","f4a6856b":"markdown","21be59da":"markdown","7809133c":"markdown","2d77461a":"markdown","6115f5a2":"markdown","e1e89275":"markdown","927a800c":"markdown","e346e4e4":"markdown","4d6bcb14":"markdown","ccf45457":"markdown","26c5f0be":"markdown","8160e950":"markdown","de64211c":"markdown","43f75870":"markdown","1ec75484":"markdown","a4209feb":"markdown","24413c4d":"markdown","f437741c":"markdown","2af5e2a4":"markdown","23848e2b":"markdown","10aa878b":"markdown","d8a01575":"markdown","d1eac7d0":"markdown","0ddd848d":"markdown","1fb92728":"markdown","b0686a32":"markdown","49199f86":"markdown","2551a5fb":"markdown","2c37879c":"markdown","52cc08bb":"markdown","67badc2d":"markdown","d4d8027c":"markdown","1fc8c3af":"markdown","70be5de2":"markdown","d7e3bc2b":"markdown","d9ccf828":"markdown","eec8c3c5":"markdown","0d753933":"markdown","b0d67a21":"markdown","e0b85023":"markdown","a4734130":"markdown","08f2c0a5":"markdown","856d0411":"markdown","9e709a6d":"markdown","79cd52b8":"markdown","d91de4b6":"markdown","641d3886":"markdown","28bab1ef":"markdown","757992a3":"markdown","7ed10da6":"markdown","b0ea7cfc":"markdown","b88c66eb":"markdown","5be08fa0":"markdown","9716cedb":"markdown","0b397ede":"markdown","e69a38ae":"markdown","41b4cffc":"markdown","4f6a9029":"markdown","538607a5":"markdown","c04b3536":"markdown","3d6fed8f":"markdown","4724bdf4":"markdown","d83fbbd3":"markdown","18faf621":"markdown","d943a00d":"markdown","311dd40b":"markdown","78ffd43c":"markdown","27a7ca3a":"markdown","30f71859":"markdown","11a480d7":"markdown","3238a1fe":"markdown","0ef7ea19":"markdown","a8c31549":"markdown","b6625619":"markdown","0f903c07":"markdown","81a7d416":"markdown"},"source":{"bca0d5a0":"import numpy as np \nimport pandas as pd \nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV, LinearRegression, ElasticNet,  HuberRegressor\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, mean_squared_log_error\nfrom xgboost import XGBRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, ExtraTreesRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.utils import resample\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\nfrom sklearn.model_selection import KFold\n\n\nimport re\n\nfrom geopandas.tools import geocode\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nseed = 42","8337dd07":"def missing(df):\n    df_missing = pd.DataFrame(df.isna().sum().sort_values(ascending = False), columns = ['missing_count'])\n    df_missing['missing_share'] = df_missing.missing_count \/ len(df)\n    return df_missing","dcbc2e43":"def simple_chart(df, x, title = None, hue = None):\n    plt.figure(figsize = (10, 6))\n    plt.title(title, fontsize=14)\n    ax = sns.countplot(x = x, hue = hue, data = df)","174c234a":"def factor_chart(df, x, y, hue = None):\n    ax = sns.factorplot(x = x, y = y, data = df, hue = hue, kind = 'box', size=6, aspect = 2)","5bedb1b2":"def scatter(df, x, y, hue = None):\n    plt.figure(figsize = (20, 10))\n    ax = sns.scatterplot(x = x, y = y, data = df, hue = hue)\n    plt.show()","38829ba2":"sns.set(style=\"darkgrid\")","e201958e":"df = pd.read_csv(\"..\/input\/riga-real-estate-dataset\/riga_re.csv\")","616fb18b":"df.head()","33b0c543":"missing(df)","d90c6154":"df[df.price.isna()]","c26d625a":"df_all = df[~df.price.isna()].reset_index(drop = True).copy()","cd0eec21":"missing(df_all) ","844be99c":"df_all.dtypes","7ef2f4ba":"df_all.describe()","c1caab02":"print('Number of observations:', len(df_all), '\\n')\nprint('Unique values:')\nprint(df_all.nunique().sort_values(ascending = False))","ac32ffcd":"df_all[df_all.street.isna()]","a0be5e04":"df_all = df_all.drop(df_all[df_all.street.isna()].index).reset_index(drop = True)","fbb5d1dc":"missing(df_all) ","a9bfe986":"# Function for removing digits from a string\n\ndef no_digits(text):\n    return ''.join([i for i in text if not i.isdigit()])","27ad2713":"df_all['street_name_0'] = df_all['street'].apply(lambda x: no_digits(re.sub('\\W+',' ', str(x))).strip())","22d5eba4":"df_all.head(3)","e7a3ae15":"# set(df_all.street_name_0.values)","15fed164":"df_all['st_n'] = None\nfor i in range(len(df_all)):\n    if ((df_all.loc[i, 'street_name_0'][:3] != 'St ') & (df_all.loc[i, 'street_name_0'][:2] != 'J ') & \n        (df_all.loc[i, 'street_name_0'][:2] != 'M ')):\n        df_all.loc[i, 'st_n'] = df_all.loc[i, 'street_name_0'].split(' ')[0]\n    elif (df_all.loc[i, 'street_name_0'][:3] != 'St '):\n         df_all.loc[i, 'st_n'] = df_all.loc[i, 'street_name_0'].split(' ')[0] + ' ' + df_all.loc[i, 'street_name_0'].split(' ')[1]\n    else:\n        df_all.loc[i, 'st_n'] = 'St ' + df_all.loc[i, 'street_name_0'].split(' ')[1]","ff77bc11":"#set(df_all.st_n.values)","0b957d19":"df_all.drop(['street_name_0'], axis = 1, inplace = True)","5b4cce28":"df_all[df_all.district.isna()]","17a5853f":"df_all[df_all.st_n == 'Og\u013cu'].groupby('district').count()","1c87a163":"df_all.loc[1107, 'district'] = 'Og\u013cu'","db5ddc35":"df_all[df_all.st_n == 'Pupuku iela'].groupby('district').count()","a9de4f27":"df_all.loc[3172, 'district'] = 'Bi\u0161umui\u017ea'","16e75223":"from geopy.geocoders import Nominatim\ngeolocator = Nominatim(user_agent=\"specify_your_app_name_here\")","db60e854":"def lat(add):\n    try:\n        return geolocator.geocode(add).latitude\n    except:\n        return None\n\ndef lon(add):\n    try:\n        return geolocator.geocode(add).longitude\n    except:\n        return None","5c9862d2":"scatter(df_all, x = 'lon', y = 'lat')","00e39710":"scatter(df_all[(df_all.lat>56.88)&(df_all.lat<57.1)&(df_all.lon>20)], x = 'lon', y = 'lat')","f32e9354":"df_all.loc[~((df_all.lat>56.88)&(df_all.lat<57.1)&(df_all.lon>20)), ['lat', 'lon']] = None","23daa5c5":"df_all['district'] = df_all[\"district\"].replace('Krasta r-ns', 'Krasta mas\u012bvs')","5194cb61":"df_all['Street_New'] = df_all['street']","f7114212":"df_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' g.', ' gatve'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' k-1', '').strip())\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' k-2', '').strip())\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' k1', '').strip())\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' k 1', '').strip())\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' k-1', '').strip())\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' k-3', '').strip())\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('-k-3', '').strip())\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' k-4', '').strip())\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' k. 1', '').strip())\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('k5', '').strip())\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('krastm.', 'krastmala'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' pr.', ' prospekts'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('Pulkv.', 'Pulkve\u017ea'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('bulv.', 'bulv\u0101ris'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('\u0161\u0137. l.', '\u0161\u0137\u0113rsl\u012bnija'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('\u0161\u0137 l.', '\u0161\u0137\u0113rsl\u012bnija'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' l. ', ' l\u012bnija '))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' d. ', ' dambis '))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('J. Dali\u0146a', 'J\u0101\u0146a Dali\u0146a'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('J. V\u0101cie\u0161a', 'Jukuma V\u0101cie\u0161a'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' g. ', ' gatve '))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' lauk.', ' laukums'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('k1', '').strip())\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('k2', '').strip())\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('-13d', '-13'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('-36d', '-36'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('-45d', '-45'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('-94b', '-94'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' 19\/1', ' 19'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('M. Balasta', 'Mazais Balasta'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('M. Kuld\u012bgas', 'Maz\u0101 Kuld\u012bgas'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('M. Nomet\u0146u', 'Maz\u0101 Nomet\u0146u'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace('Asteres', 'Aisteres'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' 17 a', ' 17'))\ndf_all['Street_New'] = df_all[\"Street_New\"].apply(lambda x: str(x).replace(' \u0161. ', ' \u0161oseja '))","b4c87851":"df_all['Street_Full'] = df_all.apply(lambda x: str(x['Street_New']).split(' ')[0] + ' iela ' + str(x['Street_New']).split(' ')[1] +\n                                    ', ' + str(x['district']) + ', ' + 'R\u012bga' if \n                                    len(x['Street_New'].split(' ')) == 2 else str(x['Street_New']) + ', ' + \n                                    'R\u012bga', axis = 1)","6116f493":"# This lines request full address that is stored in Street_Full feature. \n# Has to be launched 3-4 times, until the number of missing values stops decreasing (reaching 24 for both lat and lon specifically in this case)\n\n#df_all['lat'] = df_all.apply(lambda x: lat(str(x['Street_Full'])) if np.isnan(x['lat']) == True else x['lat'], axis=1)\n#df_all['lon'] = df_all.apply(lambda x: lon(str(x['Street_Full'])) if np.isnan(x['lon']) == True else x['lon'], axis=1)","ee43ac9b":"# However, some full addresses do not work with district name, so for the left missings we use only street name and 'Riga'\n# Also 2-3 times to execute (until 1 missing left for both lat and lon). \n\n#df_all['lat'] = df_all.apply(lambda x: lat(str(x['Street_Full'].split(',')[0]) + str(x['Street_Full'].split(',')[-1])) if np.isnan(x['lat']) == True else x['lat'], axis=1)\n#df_all['lon'] = df_all.apply(lambda x: lon(str(x['Street_Full'].split(',')[0]) + str(x['Street_Full'].split(',')[-1])) if np.isnan(x['lon']) == True else x['lon'], axis=1)","d2d073d7":"# Remaining missing did not work with full address, but only street name was enough here.\n# 1 execution is enough here\n\n#df_all['lat'] = df_all.apply(lambda x: lat(str(x['Street_Full'].split(',')[0])) if np.isnan(x['lat']) == True else x['lat'], axis=1)\n#df_all['lon'] = df_all.apply(lambda x: lon(str(x['Street_Full'].split(',')[0])) if np.isnan(x['lon']) == True else x['lon'], axis=1)","eaf83c29":"riga_fixed_coordinates = pd.read_csv('..\/input\/riga-fixed-coordinates\/riga_fixed_coordinates.csv')\nmissing(riga_fixed_coordinates)","a5e61b6b":"df_all = riga_fixed_coordinates.copy()","73f14d9e":"df_all[~(df_all.lat>56.88)&(df_all.lat<57.1)&(df_all.lon>20)]","a0c96de5":"scatter(df_all, x = 'lon', y = 'lat')","5eaa2955":"df_all[df_all.area.isna()]","bb7522ed":"df_all[df_all.street == 'Slokas 130']","1d9d5e08":"# Therefore\ndf_all.loc[3981, 'area'] = 80.0","ec52e041":"df_all[df_all.rooms.isna()]","ba400c14":"df_all[df_all.street == 'D\u0101rzaug\u013cu 1']","72d18b27":"df_all.groupby(['rooms']).area.median()","4a012780":"df_all[df_all.rooms == 'Citi']","9987f295":"df_all = df_all.drop(df_all[df_all.rooms == 'Citi'].index, axis = 0).reset_index(drop = True)","0c5e5ea1":"# Look at the missing again as index was reseted\ndf_all[df_all.rooms.isna()]","b448ef9a":"df_all.loc[1610, 'rooms'] = '6'","3fd1f65c":"df_all['rooms_num']= df_all['rooms'].astype('int64')","930cc37d":"df_all[df_all.total_floors.isna()]","5f3efefa":"df_all[df_all.street == 'Zentenes 18']","8b958704":"df_all.loc[1902, 'total_floors'] = 9.0","e57b03f3":"missing(df_all)","ae5738df":"plt.figure(figsize = (10, 6))\nax = sns.distplot(df_all.price, bins = 20) ","adba1ee0":"simple_chart(df_all, x = 'op_type')\nfactor_chart(df_all, x = 'op_type', y = 'price', hue = None)","7a92bae8":"df_all[~df_all.op_type.isin(['For rent', 'For sale'])]","ba7ac929":"df_all.loc[~df_all.op_type.isin(['For rent', 'For sale']) & (df_all.price < 1000), 'op_type'] = 'For rent'","0d99bae4":"df_all.loc[~df_all.op_type.isin(['For rent', 'For sale']) & (df_all.price > 1000), 'op_type'] = 'For sale'","bf6f0d4d":"simple_chart(df_all, x = 'op_type')","8868725b":"scatter(df_all[df_all.op_type == 'For sale'], x = 'area', y = 'price', hue = None)\nscatter(df_all[df_all.op_type == 'For rent'], x = 'area', y = 'price', hue = None)","59f9c320":"simple_chart(df_all, x = 'condition')\nfactor_chart(df_all[df_all.op_type == 'For sale'], x = 'condition', y = 'price', hue = None)\nfactor_chart(df_all[df_all.op_type == 'For rent'], x = 'condition', y = 'price', hue = None)","99a2062e":"df_all['All_Amen'] = 0\ndf_all.loc[df_all.condition == 'All amenities', 'All_Amen'] = 1","227d8552":"simple_chart(df_all, x = 'rooms') \nfactor_chart(df_all[df_all.op_type == 'For sale'], x = 'rooms', y = 'price', hue = None)\nfactor_chart(df_all[df_all.op_type == 'For rent'], x = 'rooms', y = 'price', hue = None)","f6bfe7dd":"simple_chart(df_all, x = 'floor')\nfactor_chart(df_all[df_all.op_type == 'For sale'], x = 'floor', y = 'price', hue = None)\nfactor_chart(df_all[df_all.op_type == 'For rent'], x = 'floor', y = 'price', hue = None)","f247370e":"simple_chart(df_all, x = 'total_floors')\nfactor_chart(df_all[df_all.op_type == 'For sale'], x = 'total_floors', y = 'price', hue = None)\nfactor_chart(df_all[df_all.op_type == 'For rent'], x = 'total_floors', y = 'price', hue = None)","e36f2223":"simple_chart(df_all, x = 'house_seria')\nfactor_chart(df_all[df_all.op_type == 'For sale'], x = 'house_seria', y = 'price', hue = None)\nfactor_chart(df_all[df_all.op_type == 'For rent'], x = 'house_seria', y = 'price', hue = None)","8cf1c61b":"simple_chart(df_all, x = 'house_type')\nfactor_chart(df_all[df_all.op_type == 'For sale'], x = 'house_type', y = 'price', hue = None)\nfactor_chart(df_all[df_all.op_type == 'For rent'], x = 'house_type', y = 'price', hue = None)","7171be10":"simple_chart(df_all, x = 'district')\nfactor_chart(df_all[df_all.op_type == 'For sale'], x = 'district', y = 'price', hue = None)\nfactor_chart(df_all[df_all.op_type == 'For rent'], x = 'district', y = 'price', hue = None)\nscatter(df_all[df_all.op_type == 'For sale'], x = 'lon', y = 'lat', hue = 'district')\nscatter(df_all[df_all.op_type == 'For rent'], x = 'lon', y = 'lat', hue = 'district')","753d69c9":"scatter(df_all[df_all.op_type == 'For sale'], x = 'lat', y = 'price', hue = None)\nscatter(df_all[df_all.op_type == 'For rent'], x = 'lat', y = 'price', hue = None)","920183e1":"scatter(df_all[df_all.op_type == 'For sale'], x = 'lon', y = 'price', hue = None)\nscatter(df_all[df_all.op_type == 'For rent'], x = 'lon', y = 'price', hue = None)","4e12f632":"Riga_Center_Lat = 56.949600\nRiga_Center_Lon = 24.105200","192058f3":"import geopy.distance","b662b9bb":"def center_dist(lat_i, lon_i):\n    return geopy.distance.vincenty((Riga_Center_Lat, Riga_Center_Lon), (lat_i, lon_i)).km","c86afa17":"df_all['center_dist'] = df_all.apply(lambda x: center_dist(x['lat'], x['lon']), axis = 1)","9eef6830":"scatter(df_all[df_all.op_type == 'For sale'], x = 'center_dist', y = 'price', hue = None)\nscatter(df_all[df_all.op_type == 'For rent'], x = 'center_dist', y = 'price', hue = None)","25435a1e":"df_all['Area_Room_Ratio'] = df_all.area \/ df_all.rooms_num","11977567":"scatter(df_all[df_all.op_type == 'For sale'], x = 'Area_Room_Ratio', y = 'price', hue = None)\nscatter(df_all[df_all.op_type == 'For rent'], x = 'Area_Room_Ratio', y = 'price', hue = None)","ed62c3fb":"df_all['Floor_Ratio'] = df_all.floor \/ df_all.total_floors ","c2d13bf8":"scatter(df_all[df_all.op_type == 'For sale'], x = 'Floor_Ratio', y = 'price', hue = None)\nscatter(df_all[df_all.op_type == 'For rent'], x = 'Floor_Ratio', y = 'price', hue = None)","bce2842b":"df_all[df_all['Floor_Ratio'] > 1]","a4c593c3":"df_all.loc[(df_all['Floor_Ratio'] > 1), 'floor'] = df_all['floor'] \/ df_all['Floor_Ratio']\ndf_all.loc[(df_all['Floor_Ratio'] > 1), 'total_floors'] = df_all['Floor_Ratio'] * df_all['total_floors']","b7e9245a":"df_all.loc[(df_all['Floor_Ratio'] > 1), 'Floor_Ratio'] = df_all['floor'] \/ df_all['total_floors']","b0c49cc7":"df_all[df_all['Floor_Ratio'] > 1]","a35b0aa0":"scatter(df_all[df_all.op_type == 'For sale'], x = 'Floor_Ratio', y = 'price', hue = None)\nscatter(df_all[df_all.op_type == 'For rent'], x = 'Floor_Ratio', y = 'price', hue = None)","7e5e4d5e":"df_sale = df_all[df_all.op_type == 'For sale'].reset_index(drop = True).copy()\ndf_rent = df_all[df_all.op_type == 'For rent'].reset_index(drop = True).copy()","ee87e660":"scatter(df_sale, x = 'area', y = 'price', hue = None)\nscatter(df_rent, x = 'area', y = 'price', hue = None)","7fe6c37b":"scatter(df_rent[df_rent.price < 300], x = 'area', y = 'price', hue = 'center_dist')","81eb1461":"df_sale_clean = df_sale[(df_sale.price < 300000) & (df_sale.area <160)  \n                  & (~((df_sale.price < 50000) &(df_sale.area > 80))) \n                 & (~((df_sale.price < 100000)&(df_sale.area > 130)))\n                  & (df_sale.Area_Room_Ratio<80)\n                 ].copy()","943006f7":"df_rent_clean = df_rent[(df_rent.price < 1390) & (df_rent.area <125) & (df_rent.price > 60) \n                  & (~((df_rent.price < 110) &(df_rent.area > 40))) \n                 & (~((df_rent.price < 400)&(df_rent.area > 100)))\n                  & (~((df_rent.price > 1000)&(df_rent.area < 70)))\n                  &(df_rent.Area_Room_Ratio < 65)\n                 ].copy()","65588947":"scatter(df_sale_clean, x = 'area', y = 'price', hue = None)\nscatter(df_rent_clean, x = 'area', y = 'price', hue = None)","d13f244c":"df_sale_clean.columns","50b90451":"df_sale_clean = df_sale_clean.drop(['op_type', 'street', 'rooms', 'condition', 'Street_New', 'Street_Full'], axis = 1)\ndf_rent_clean = df_rent_clean.drop(['op_type', 'street', 'rooms', 'condition', 'Street_New', 'Street_Full'], axis = 1)","47088c6b":"def get_splits(df):\n    X_train, X_test, y_train, y_test = train_test_split(df.drop(['price'], axis = 1), \n                                                          df['price'], train_size=0.8, test_size=0.2, \n                                                          random_state = seed)\n    return X_train, X_test, y_train, y_test","ba2b4476":"OH_sale_clean = pd.get_dummies(df_sale_clean, drop_first = True)\nOH_rent_clean = pd.get_dummies(df_rent_clean, drop_first = True)","5c70cd69":"OH_sale_train, OH_sale_test, OH_y_sale_train, OH_y_sale_test = get_splits(OH_sale_clean)\nOH_rent_train, OH_rent_test, OH_y_rent_train, OH_y_rent_test = get_splits(OH_rent_clean)","fa09363f":"cols_to_drop_sale = OH_sale_train.columns[(OH_sale_train == 0).all()]\nOH_sale_train = OH_sale_train.drop(cols_to_drop_sale, axis = 1)\nOH_sale_test = OH_sale_test.drop(cols_to_drop_sale, axis = 1)","e59fb2ca":"cols_to_drop_rent = OH_rent_train.columns[(OH_rent_train == 0).all()]\nOH_rent_train = OH_rent_train.drop(cols_to_drop_rent, axis = 1)\nOH_rent_test = OH_rent_test.drop(cols_to_drop_rent, axis = 1)","5ec9c917":"models = [RandomForestRegressor(random_state = seed), \n          Ridge(random_state = seed), \n          RidgeCV(), \n          Lasso(random_state = seed), \n          LassoCV(random_state = seed), \n          ElasticNet(random_state = seed),\n          HuberRegressor(), \n          KernelRidge(), \n          GradientBoostingRegressor(random_state = seed), \n          ExtraTreesRegressor(random_state = seed), \n          XGBRegressor(random_state = seed)]","5bf4330d":"models_names = [str(i).split('(')[0] for i in models]","aed9ac4f":"def models_summary(train, test, y_train, y_test):\n    models_MAE = []\n    models_RMSE = []\n    models_RMSLE = []\n    for model in models:\n        model.fit(train, y_train)\n        preds = model.predict(test)\n        models_MAE.append(mean_absolute_error(y_test, preds))\n        models_RMSE.append(np.sqrt(mean_squared_error(y_test, preds)))\n        models_RMSLE.append(np.sqrt(mean_squared_log_error(y_test, abs(preds))))\n    return pd.DataFrame(list(zip(models_names, models_MAE, models_RMSE, models_RMSLE)),\n              columns=['models','MAE', 'RMSE', 'RMSLE']).sort_values(by = 'MAE').set_index('models')","b62e5aac":"models_sale_res = models_summary(OH_sale_train, OH_sale_test, OH_y_sale_train, OH_y_sale_test)\nmodels_sale_res","5a8cc162":"models_rent_res = models_summary(OH_rent_train, OH_rent_test, OH_y_rent_train, OH_y_rent_test)\nmodels_rent_res","b21a92c2":"kf = KFold(n_splits=5, random_state=seed)","a7891fcb":"ET_model = ExtraTreesRegressor(random_state = seed,\n                                n_estimators=400, \n                                min_samples_split=2,\n                                min_samples_leaf=1, \n                                max_features=200,\n                              )\n\nparams_grid = {#'n_estimators': range(50,50,201),\n               #'max_features': range(50,401,50),\n               #'min_samples_split': range(2,5),\n               #'min_samples_leaf': range(1,4)\n              }  \n\nET_grid = GridSearchCV(estimator = ET_model, param_grid = params_grid, n_jobs = -1,\n                               cv = kf, scoring = 'neg_mean_absolute_error')","ded65d9d":"ET_grid_sale = ET_grid.fit(OH_sale_clean.drop(['price'], axis = 1), OH_sale_clean.price)\nprint(ET_grid_sale.best_params_)\nprint(ET_grid_sale.best_score_)","057c5fc5":"ET_grid_rent = ET_grid.fit(OH_rent_clean.drop(['price'], axis = 1), OH_rent_clean.price)\nprint(ET_grid_rent.best_params_)\nprint(ET_grid_rent.best_score_)","951fcb4d":"ET_sale = ExtraTreesRegressor(random_state = seed,\n                                n_estimators=400, \n                                min_samples_split=2,\n                                min_samples_leaf=1, \n                                max_features=200,\n                              )\nET_rent = ExtraTreesRegressor(random_state = seed,\n                                n_estimators=400, \n                                min_samples_split=2,\n                                min_samples_leaf=1, \n                                max_features=200,\n                              )","def65955":"def score(model, X_train, X_test, y_train, y_test):\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    return mean_absolute_error(y_test, preds)","49766170":"score(ET_sale, OH_sale_train, OH_sale_test, OH_y_sale_train, OH_y_sale_test)","906651d9":"score(ET_rent, OH_rent_train, OH_rent_test, OH_y_rent_train, OH_y_rent_test)","096e2b18":"def results(model, X_train, X_test, y_train, y_test):\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    res_tab = pd.DataFrame({'y_test': y_test, 'preds': preds, 'error': (preds - y_test),\n             'error_share': abs(y_test - preds)\/y_test}).sort_values(by = 'error_share', ascending = False)\n    return res_tab","94b391d2":"sale_results = results(ET_sale, OH_sale_train, OH_sale_test, OH_y_sale_train, OH_y_sale_test)\nrent_results = results(ET_rent, OH_rent_train, OH_rent_test, OH_y_rent_train, OH_y_rent_test)","0609db04":"sale_results[:10]","065ee6cd":"rent_results[:10]","b3c4f82c":"def error_lines(df, y):\n    plt.figure(figsize = (10, 6))\n    ax = sns.lineplot(x = range(len(df)), y = y, data = df.sort_values(by = [y], ascending = False))","be90c4fb":"error_lines(sale_results, 'error')","df67228b":"error_lines(sale_results, 'error_share')","b501d6eb":"error_lines(rent_results, 'error')","b4fb6e01":"error_lines(rent_results, 'error_share')","886160ca":"### Modelling","45ba5bb9":"No surprise, strong positive correlation.  \n  \nHowever, we can notice, that dots are highly dispersed and there is definitely heteroskedasticity problem here.  \n \nOne of the ways to handle it is to use log1p of the target variable instead of the target itself. This makes a model robust to outliers and usually obtains better accuracy.\n \nNevertheless, in my case it did not help as much as outliers removal, so here I will use only it.","64c6da04":"**Op_type**","3a348d4a":"**Area**  \n  \n  \nExpectedly, one of the most important features.","495b4bd7":"And now we're done with imputation!  \n  \nNo missings left!","db885df7":"Here we ahcieve **MAE 11473** for sale houses and **64.62** for rent and we can see that tuning has helped to slightly improve our score compare the baseline model, where we had MAE 11501 for sale and 65.25 for rent.\n \nI think it's not so bad, but can't yet say so (waiting for your kernel here!)","6aa64052":"**Floor**","9b877a60":"So for the sale dataset we can expect **MAE** around **12200** and for rent **MAE** around **66.32** in average case scenario.  \n \nBy increasing n_estimatiors, MAE score can be reduced by ~150 and ~4 for sale and rent datasets respectively","6f3e5cfb":"And here we have an unexpected data mistake!  \n  \nObviously, Floor_Ratio cannot be higher than 1, but we have such values","364810fa":"Apartments with all amenities do have higher price than with partial or without.  \n  \nAt the same time, the number of observations without amenities is insignificantly slow, so I just introduce a dummy for All amenities to represent this condition difference in the model.","bfd61784":"Same conclusion here as for floors. Seems important, should be included, might have outliers.","ffb9a5b7":"For sale:","4e74dc52":"Right and left sides of the error graphs are symmetric and the graph is centered around 0.  \n \nThis allows us to say that our model is not over or underestimating price and most errors explained by outliers.  \n \nOther graph shows, that we mistake less by 10% for the half of the set and less than 25% for nearly 80% of the set.  \n  \nSo yeah, not so bad! \n  \nI guess...","2494b41c":"### Some preparation\nImporting libraries:","cdca97f6":"Let's look if there is a flat with the same addresses.  \n  \nI assume that flats in the same buildings will have more or less the same Area\/Rooms, so we can use this as a proxy.","e2fe45ee":"Now I'd like to take a closer look on performance of our models","e27132f1":"**Condition**","17ac85c0":"Now let's impute **districts**.","925b546c":"OK, we're almost done with cleaning! What's left is only...","97590282":"For rent:","7ff07877":"Now I want to extract actual street names from 'street' feature in order to use it as a categorical variable and to impute missing districts.  ","a525860e":"And finally...","3e65bbb1":"Introducting functions for EDA:","dbf63126":"It seems like floor and total_floors values are switched for these observations.  \n  \nLet's switch 'em back then.","f4a6856b":"Now it's time to imput remaining missings.  \n \nGo with **area**.","21be59da":"And now we come to the most painful part of this project for me.  \n \nThe problem with finding coordinates with geopy and all other libraries is that the address should be specifically correct in order to return desired coordinates. \n\nFor example, if you request 'J\u016brmalas g. 15' coordinates, you wont get any result, because geopy cannot understand 'g.', so we have to replace all the instances of 'J\u016brmalas g.' on 'J\u016brmalas gatve'.  \n \nOr if you request 'Skolas 38' it will send you to a Norway hinterland.  \n\nTo fix all of this has brought me a lot of pain, but I did it so below is what I could do to achieve the result.  \n  \nAlso important to note, that geopy api sometimes throws you **connection errors**.  \nI made my coordinate finding functions robust to it, but it still requires to execute the line with the function **several times**","7809133c":"See that Og\u013cu street belongs to \u0136\u012bpsala district strictly, so we impute it first.","2d77461a":"**Twofer**! Now, thanks to this drop, we should care about imputation for much less features.","6115f5a2":"That's all from me, guys!  \n \nLet me know if you found any better solutions or my mistakes and if you have any advice for my model, stats and code overall (I know it's not the state of the art at all, so your criticism is highly welcome) \n \nPeace!","e1e89275":"Here we see that model for rent type apartments used to underestimate y value, so more careful work with low-price outliers can help to reduce error rate greatly.  \n \nStill, model performs quite good, having less than 20% error share for 80% of the observations. ","927a800c":"For sale:","e346e4e4":"**Floor_Ratio**  \nThe second assumption is that people prefer comparatively higher flats in a house, so their prices are higher for all buildings","4d6bcb14":"**Rooms**","ccf45457":"Ok, so far I've managed to build a model that is seemingly not bad.  \n \nTo get better scores I've tried to tune all of the mentioned models, used log1p in order to handle heteroscedasticity problem that our values clearly have, tried different combination of mentioned features etc.  \n \nHere I present you the best of mine, but I believe you can do it better!  \n \nTo give you some ideas:  \n \n1) More outliers can be removed with box-cox tool so the model will be more robust to them (but maybe will lose explanatory power)  \n \n2) Try other 'fashionable' models like LightGBM and CatBoost. Due to some problems with my python packages I could not install them properly, so maybe they will outperform ExtraTrees here  \n \n3) My model clearly has too much features, so I suspect it is Multidimensionally cursed. \nDefinitely, most of it comes from huge number of streets dummies. Not using them would drop my model performance, but maybe there is a way to cleverly choose or group some of them.  \n\n4) To tackle heteroskedasticity you can apply log1p to the features like area, Area\/Room ratio, Floor\/Max floor ration or to the target variable itself. This can mitigate the negative effect of outliers, which are definitely worsening my score a lot.","26c5f0be":"### Raw data exploration ","8160e950":"Unfortunately we see, that lat and lon values for these 5 observation are just incorrect, because if you put them on the map, you'll find yourself at Italian mountains (I wish it could happen for real).  \n  \nTherefore, finding addresses for them is impossible. Considering the fact that these observations have a lot of other missing features as well, we'd better drop them.","de64211c":"For rent:","43f75870":"Unexpectedly, rooms feature is categorical, we will take a closer look at it later on.","1ec75484":"Splitting on test and train","a4209feb":"**House seria**","24413c4d":"Same conclusion goes for **house type**","f437741c":"To make everything visible, let's plot error and error_share from lowest to highest ","2af5e2a4":"I want to use street addresses to impute missing lat, lon and addresses.  \n  \nSo let's fix missing streets first with their coordinates.","23848e2b":"Now it's better!","10aa878b":"Phew, that was tiresome!  \n  \nBut as we see, we successfully imputed all the coordinates and none of them are out of Riga boundaries now.  \n\nLet's check it on 'map' again.","d8a01575":"## Yo!\nSo I've done some simple work with this dataset and acheved average MAE for sale prices around 12000 and for rent prices around 65.  \n  \nHere I want to show you what you can find with EDA, which variables might be worth to engineer and how GepPy library can help you with finding coordinates and addresses in order to obtain more information and improve your model. ","d1eac7d0":"### Dropping outliers","0ddd848d":"**ATTENTION!**   \n  \nThe following code is using GeoPy coordinates fuctions, but they **do not work well** if you execute them on kaggle environment!  \n \nTherefore, if you want to perform the same coordinate imputation on your data, please, copy the following commented code lines and execute them on your environment (Jupiter, for ex.)\n \nHere instead I will use the preloaded dataset that is the result of the commented code lines below.","1fb92728":"**Area_Room_Ratio** will reflect how big the rooms are in the apartment. The assumption here is that people might prefer  larger rooms","b0686a32":"Let's calculate our results for our selected train and test sets:","49199f86":"Notice, that in rent dataset there is a relatively big chunk of dots below price = 100, that have their own mood on price.\n  \nHowever, playing with 'hue' parameter for the chart I could not find the explaination for this group's deviant behavior. \n \nTherefore, since available features do not explain this, I specify these dots as outliers so they do not harm our model.  \n \nIf you find the explaination for this, please, let me know.","2551a5fb":"We see that we have **10%** of dataset with missing price, that is our target variable.  \n  \nMoreover, if we take a closer look at the observations with missing price, we will se that they miss most information about other variables as well and contribute to the number of missings in other columns.  \n  \nAs we do not have a test dataset, they have no use so I **remove them**","2c37879c":"Imputing total floors now.  \n  \nAnd again, same addresses should help.","52cc08bb":"We can see that actually not all street addresses are unique, so there are some groups of apartments that are in the same building, so it can help us while imputing floors and areas.","67badc2d":"**lat and lon**","d4d8027c":"Alright, no surprise that higher numbers of rooms correlated with higher prices.  \n  \nAlthough, it's surprising, that 6 rooms flats for rent are cheaper than 5 rooms - other features impact might be involved.","1fc8c3af":"Drop columns, that are relevant only for train sets","70be5de2":"Double area, but I doubt to put 8 rooms here.  \n  \nLet's look at rooms number distribution by rooms:","d7e3bc2b":"Seems like my assumption was sort of correct and this feature will be worth to use. Though, some outliers and strong heteroskedasticity remain a problem","d9ccf828":"Expectedly, correlation of center_dist with price is negative and visibly significant, so it will be included in the models","eec8c3c5":"Some positive correlation is visible, so it worths to try this feature in the models","0d753933":"And here we found it!  \n  \nObviously, montly payment price for rent is much smaller than sale price of a whole apartment so these are clearly 2 different targets.  \n  \nTherefore, as we have quite enough observations for both major categories, it worths to separate the dataset on 2 parts for the further analysis and modelling later on.  \n  \nBut before what about other op_type values?","b0d67a21":"Now we can see why rooms feature is categorical.  \n  \nFor some reason there is a **'Citi'** value in rooms that means 'other'.  \n \nLet's try to figure out what it can give us.","e0b85023":"Now we go for outliers detection  \n  \nI've made it basing on observing scatterplots and removing dots that highly deviate.  \n \nI decided to not to use quantile outlier detection here because since the values are highly disperced and our datasets are not really big, a big share of information would've been lost and models would've not been describing our data really.  \n \nBut if this dataset will be expanded later on, I would definitely use boxcox transformation to remove outliers and to build more robust model ","a4734130":"Value for imputation is clearly visible.","08f2c0a5":"Now it's better","856d0411":"**Total floors**","9e709a6d":"### Data cleaning and missing imputation","79cd52b8":"Get dummies for categorical features","d91de4b6":"But as we've already noticed, some lat and lan values are **wrong**, so we have to make them empty and impute them by using their street addresses and GeoPy","641d3886":"**Wow**, this action has actually helped us to get rid of most other missing variables as well!  \n  \nNow, none of the variables have share of missing higher than 5% so all of them are worth to explore.","28bab1ef":"Notice, how price is the higher, the closer a dot is to the center.  \n  \nTherefore, knowing the coordinates of Riga center, we can calculate each observation's distance from center and use it as a feature.","757992a3":"After some boring iterations of charts I've ended up with these datasets.  \n \nUnusually expensive or cheap flats' prices definitely have some other explaination of their deviant prices (such as interior for ex.) so they were removed together with other highly deviated dots","7ed10da6":"### EDA","b0ea7cfc":"First, let's choose the most appropriate models for tuning","b88c66eb":"As we see, there are only 12 observations with this value of floor and they vary a lot, but most do have comparatively large area that makes them outliers.  \n  \nI would assume that these observations do have some unique certain specifics that can impact their price, but they are very few, so I just decided to drop them.","5be08fa0":"We can directly notice by the price value, which of the observations with other op_type actually represent either for rent or for sale types.  \n  \nSo we fix them and leave only 2 major op_types in order to split the dataset later.","9716cedb":"If you uncomment and launch the line above, you'll see that some useless liters have left, so we will fix em as well.","0b397ede":"Looks like floor is correlated with price and will be included in the model.  \n \nSome observations seem to be outlying, but it can probably be some other features impact, so we will have to track it when removing outliers.","e69a38ae":"Now let's impute lat and lan.  \n  \nFor this purpose I will use **GeoPy** library.","41b4cffc":"### CONCLUSION ","4f6a9029":"Distribution range of price is wild. Why is it so?  \n  \nLet's try to find some clues in our features.","538607a5":"First, it is time to separate our dataset on 2 independent sets and work with them separately now","c04b3536":"Now go with imputing rooms and use the same approach.","3d6fed8f":"Now we're good!","4724bdf4":"Variance of some numeric variables is very high.  \n  \nFloor and max floor feature values are reasonable, but area, price, lat and lon features have some wierd values and definite mistakes so cleaning will be necessary.","d83fbbd3":"A bit of styling","18faf621":"And there we go!","d943a00d":"Some house serias do have varying price levels, so I will dummy them and track their impact in models","311dd40b":"Districts and streets might have their own specific levels of prices.   \n \nSome are prestigious and some are poor, and it can impact the price strongly, while area or distance from center will not track it.  \n  \nTherefore, this categorical information might be very important, so I will dummy them as well. ","78ffd43c":"Now, let's introduce some other features, that can improve our predictions additionally","27a7ca3a":"So from what's left, we can impute '6' for our missing room. ","30f71859":"Oh, but looks like Pupuku iela is unique, so we use google maps then and find that this street belongs to Bi\u0161umui\u017ea district.","11a480d7":"Drop useless columns","3238a1fe":"Now charts look more neat and outliers impact is higly reduced.","0ef7ea19":"### Tuning ExtraTrees\nFor this purpose I will use combined dataset and use GridSearch on 5 kfolds. This will allow to getan average error score for any split wilth the same train\/set proportion as we set before (80\/20).  \n  \nBelow I already selected my combination of parameters that I consider the best by efficiency and time ratio,but youcan play with your parameters here. \n \nNote, that you can achieve higher scores by setting higher n_estimators, but you'd have to wait longer.","a8c31549":"**Splendid!**  ","b6625619":"Here is a preloaded dataset that I use for the following analysis here.  \n  \nHowever, **DO NOT FORGET** to remove 2 following lines if you want to use geopy on your env.","0f903c07":"Seems like **ExtraTrees** has the lowest MAE, RMSE and RMSLE for both sets and it is the best fit for our disperced data.  \n \nOf course, this results do not take tuning into account and famous XGB or GBS potentially can outbeat ExtraTrees.  \n \nHowever, I've tuned them by myself and ExtraTrees still was the best for my datasets.  \n \nTherefore, I will show results only for this model, but if you manage to overcome my scores with XGB - please, let me know!","81a7d416":"And now introduce numeric feature of rooms:"}}