{"cell_type":{"c84c3120":"code","6b651da3":"code","ee4dac35":"code","7d5df924":"code","c40a5e5b":"code","03b63d55":"code","103d03cf":"code","a71941e6":"code","28480b66":"code","3d31e4fb":"code","28ad5c74":"code","4ec5fde8":"code","7fcd4e3a":"code","da32d1fa":"code","f0ad64c5":"code","41f429fa":"code","4ffe3e03":"code","f065c756":"code","208262f3":"code","fd95d6b6":"code","f2ba47b7":"code","8a531911":"code","38ecbcc8":"code","8bac9ade":"code","1b6aa227":"code","dba7ec4f":"code","2a3906a7":"code","c644c73a":"code","0b0730f4":"code","983a7bfb":"code","cbcdec68":"code","dafb0f83":"code","39fa3810":"code","b07a0f2f":"code","0f32eed1":"code","f5b13223":"code","158e91ac":"code","3ce5ca3a":"code","7d2a79ed":"code","ca031e84":"code","b7335c50":"code","afa26194":"code","3ac62d94":"code","9f4af608":"code","83f98f6c":"code","a915cef9":"code","36a88732":"code","49e65800":"code","4dcf86e2":"code","cb6f76c6":"code","564faa8c":"code","501e3584":"code","8ddd51d7":"code","52abeb47":"code","2474e008":"code","89b6b7d8":"code","cbd44e32":"code","05329a48":"code","31853c13":"code","ac61b1b0":"code","f4650d60":"code","b0e841fb":"code","425236f4":"code","6d09bf28":"code","be25c5dd":"code","0f67f381":"code","fb58418b":"code","67ded8cb":"code","400203bc":"code","12359364":"code","5aa30ee9":"code","8c4a48ad":"code","786f0428":"code","5f6ea71a":"code","4fcb3b51":"code","9216077c":"code","970937b5":"code","f3f38263":"code","05a242c9":"code","44f22382":"code","02497d5e":"code","66e6c8b7":"code","e376b2c9":"code","38a9fad8":"code","8bba0f09":"code","d5763099":"code","4d5a5e29":"code","bc3e88cf":"code","397fb135":"code","bfd878e1":"code","79db28c8":"code","26fd72a2":"code","c5401f36":"code","649b2db4":"code","8ec11274":"code","9612a85c":"code","eeaed121":"markdown","7a9c9adb":"markdown","ec9b5dd2":"markdown","c5af2f7a":"markdown","261e44b9":"markdown","0371cee5":"markdown","2d106e07":"markdown","1d65e78d":"markdown","9b631f1a":"markdown","05b51fdd":"markdown","43472bdd":"markdown","14b44967":"markdown","4b233ef1":"markdown","a8e51123":"markdown","f9c698bf":"markdown","cc1f08b2":"markdown","5fd301d2":"markdown","6835fecb":"markdown","0927cd42":"markdown","e2d073af":"markdown","e2357eaf":"markdown","f30dc0d7":"markdown","7cfafcc4":"markdown","eb53ffdd":"markdown","a20ed30f":"markdown","a87bace3":"markdown","10784bb9":"markdown","2fca93e6":"markdown","9bc3826b":"markdown","91c4bf2e":"markdown","3ff6301e":"markdown","7f888e2d":"markdown","242a7b01":"markdown","aca7f4af":"markdown","1cd0240e":"markdown","7d4cf0b8":"markdown","aeec7757":"markdown","ccbfe6a0":"markdown","1d4c86fc":"markdown","a056b7ac":"markdown","42bd3d94":"markdown","8dc9cc14":"markdown"},"source":{"c84c3120":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nimport os\nimport geopandas as gpd\nimport json\nimport datetime\n\n%matplotlib inline","6b651da3":"path = '\/kaggle\/input\/bikedata\/Bike\/'\n\nfilenames = os.listdir(path)\nbike_load_dfs = []\nfor f in filenames:\n    bike_load_dfs.append(pd.read_csv(path + f))\n\nlen(bike_load_dfs)","ee4dac35":"for df in bike_load_dfs:\n    print(df.shape)\n    print(df.columns)","7d5df924":"bike_df = bike_load_dfs[0].copy()\nfor i in range(1, len(bike_load_dfs)):\n    bike_df = bike_df.append(bike_load_dfs[i])\n\nbike_df.shape","c40a5e5b":"bike_df.head()","03b63d55":"bike_df.dtypes","103d03cf":"bike_df = bike_df.drop(labels = ['rental_access_method', 'bike_share_for_all_trip'], axis = 1)\nbike_df.shape","a71941e6":"bike_df.to_csv('bike_uncleaned.csv', index = False)","28480b66":"bike_df.info()","3d31e4fb":"for col in bike_df.columns:\n    print(col + ' non-null: ' + str(bike_df[col].count()))","28ad5c74":"bike_df.describe()","4ec5fde8":"bike_df.nunique()","7fcd4e3a":"#looking at the durrations via a log \nduration_log = bike_df['duration_sec'].apply(np.log10)\nduration_log.head()","da32d1fa":"duration_log.hist()","f0ad64c5":"print('Number 1 second or more: ' + str(bike_df[bike_df['duration_sec'] > 0].shape[0]))\nprint('Number 1 minute or more: ' + str(bike_df[bike_df['duration_sec'] > 60].shape[0]))\nprint('Number 1 hour or more: ' + str(bike_df[bike_df['duration_sec'] > 3600].shape[0]))\nprint('Number 1 day or more: ' + str(bike_df[bike_df['duration_sec'] > 86400].shape[0]))\nprint('Number 1 week or more: ' + str(bike_df[bike_df['duration_sec'] > 604800].shape[0]))","41f429fa":"# look at the zero longitude and latitude\nbike_df[bike_df['start_station_longitude'] == 0].shape[0]","4ffe3e03":"bike_df[bike_df['start_station_longitude'] == 0]['start_station_id'].value_counts()","f065c756":"bike_df[bike_df['start_station_latitude'] == 0].shape[0]","208262f3":"bike_df[bike_df['start_station_latitude'] == 0]['start_station_longitude'].value_counts()","fd95d6b6":"bike_df[bike_df['end_station_longitude'] == 0].shape[0]","f2ba47b7":"bike_df[bike_df['end_station_longitude'] == 0]['end_station_id'].value_counts()","8a531911":"bike_df[bike_df['end_station_latitude'] == 0].shape[0]","38ecbcc8":"bike_df[bike_df['end_station_latitude'] == 0]['end_station_longitude'].value_counts()","8bac9ade":"bike_df[bike_df['start_station_id'] == 420].shape[0]","1b6aa227":"bike_df[bike_df['start_station_id'] == 449].shape[0]","dba7ec4f":"bike_df[bike_df['end_station_id'] == 420].shape[0]","2a3906a7":"bike_df[bike_df['end_station_id'] == 449].shape[0]","c644c73a":"bike_df[(bike_df['start_station_id'] == 420) | \n        (bike_df['start_station_id'] == 449)][['start_station_id', 'start_station_name']].value_counts()","0b0730f4":"# Looking into duplicate station names.\nstations = bike_df[['start_station_id', 'start_station_name']].drop_duplicates()\nstations.shape","983a7bfb":"station_name_count = stations[['start_station_id', 'start_station_name']].groupby('start_station_id').agg('count')\nstation_name_count.shape","cbcdec68":"station_name_dup = station_name_count[station_name_count['start_station_name'] > 1]\nstation_name_dup.shape","dafb0f83":"bike_df[bike_df['start_station_id'].isin(station_name_dup.index)][['start_station_id','start_station_name']].value_counts()","39fa3810":"for id in station_name_dup.index:\n    print(bike_df[bike_df['start_station_id'] == id][['start_station_id','start_station_name']].value_counts())","b07a0f2f":"stations_end = bike_df[['end_station_id', 'end_station_name']].drop_duplicates()\nstation_name_count_end = stations_end[['end_station_id', 'end_station_name']].groupby('end_station_id').agg('count')\nstation_name_dup_end = station_name_count_end[station_name_count_end['end_station_name'] > 1]\nstation_name_dup.index == station_name_dup_end.index","0f32eed1":"start_min_max = bike_df[['start_station_id', 'start_station_latitude', \n                         'start_station_longitude']].groupby('start_station_id').agg(['min', 'max'])\nstart_min_max.columns","f5b13223":"start_min_max['lat_dif'] = start_min_max[('start_station_latitude', 'max')] - start_min_max[('start_station_latitude', 'min')]\nstart_min_max['lon_dif'] = start_min_max[('start_station_longitude', 'max')] - start_min_max[('start_station_longitude', 'min')]\nstart_min_max.head()","158e91ac":"start_min_max['lat_dif'].max(), start_min_max['lon_dif'].max()","3ce5ca3a":"end_min_max = bike_df[['end_station_id', 'end_station_latitude', \n                         'end_station_longitude']].groupby('end_station_id').agg(['min', 'max'])\nend_min_max['lat_dif'] = end_min_max[('end_station_latitude', 'max')] - end_min_max[('end_station_latitude', 'min')]\nend_min_max['lon_dif'] = end_min_max[('end_station_longitude', 'max')] - end_min_max[('end_station_longitude', 'min')]\nend_min_max['lat_dif'].max(), end_min_max['lon_dif'].max()","7d2a79ed":"start_wrong_lat = start_min_max[start_min_max['lat_dif'] > 0.001].index\nstart_wrong_lon = start_min_max[start_min_max['lon_dif'] > 0.001].index\nend_wrong_lat = end_min_max[end_min_max['lat_dif'] > 0.001].index\nend_wrong_lon = end_min_max[end_min_max['lon_dif'] > 0.001].index\n\nlen(start_wrong_lat), len(start_wrong_lon), len(end_wrong_lat), len(end_wrong_lon)","ca031e84":"start_wrong_lat, start_wrong_lon, end_wrong_lat, end_wrong_lon","b7335c50":"for id in start_wrong_lon:\n    print(bike_df[bike_df['start_station_id'] == id][['start_station_id','start_station_latitude', \n                                                     'start_station_longitude']].value_counts())","afa26194":"for id in end_wrong_lon:\n    print(bike_df[bike_df['end_station_id'] == id][['end_station_id','end_station_latitude', \n                                                     'end_station_longitude']].value_counts())","3ac62d94":"#Let's make sure that the non-station locations are not out of state or on Mars.\nbike_df[bike_df['start_station_id'].isnull()][['start_station_latitude', 'start_station_longitude']].describe()","9f4af608":"bike_df[bike_df['end_station_id'].isnull()][['end_station_latitude', 'end_station_longitude']].describe()","83f98f6c":"bike_df[bike_df['start_station_latitude'] < 37].shape[0] + bike_df[bike_df['end_station_latitude'] < 37].shape[0]","a915cef9":"bike_df[bike_df['start_station_latitude'] > 38].shape[0] + bike_df[bike_df['end_station_latitude'] > 38].shape[0]","36a88732":"bike_df[bike_df['start_station_longitude'] > -121].shape[0] + bike_df[bike_df['end_station_longitude'] > -121].shape[0]","49e65800":"no_name_start = bike_df[(bike_df['start_station_id'].isnull()) & \n                        (bike_df['start_station_name'].notnull())][['start_time', 'start_station_name']]\n#added start time so I have something to count on.\nno_name_start.head()","4dcf86e2":"no_name_start.groupby('start_station_name').agg('count')","cb6f76c6":"no_name_end = bike_df[(bike_df['end_station_id'].isnull()) & \n                        (bike_df['end_station_name'].notnull())][['start_time', 'end_station_name']]\nno_name_end.groupby('end_station_name').agg('count')","564faa8c":"for name in pd.unique(no_name_end['end_station_name']):\n    print(name +\"    \"+ str(bike_df[(bike_df['end_station_id'].notnull()) & \n            (bike_df['end_station_name'] == name)]['end_station_name'].count()))","501e3584":"corrected_station = ['Green St at Van Ness Ave', 'Clement St at 32nd Ave']\nfor name in corrected_station:\n    print(name +\"    \"+ str(bike_df[(bike_df['end_station_id'].notnull()) & \n            (bike_df['end_station_name'] == name)]['end_station_name'].count()))","8ddd51d7":"bike_df_not_clean = bike_df.copy()","52abeb47":"bike_df = bike_df_not_clean.copy()\nbike_df.shape","2474e008":"#oops, there are multiple indexs that are the same. When I tried to drop by index I lost more rows then\n#I should have. Let's reset that index first.\nbike_df.set_index(np.arange(bike_df.shape[0]), drop = True, inplace = True)\nbike_df.tail()","89b6b7d8":"bike_df['start_time'] = pd.to_datetime(bike_df['start_time'])\nbike_df['end_time'] = pd.to_datetime(bike_df['end_time'])\nbike_df.dtypes","cbd44e32":"start_date = datetime.datetime(2019, 2, 11, 0, 0, 0)\nend_date = datetime.datetime(2020, 3, 16, 0, 0, 0)\n\nstart_date, end_date","05329a48":"too_early_index = bike_df[bike_df['start_time'] < start_date].index\nlen(too_early_index)","31853c13":"bike_df.drop(index = too_early_index, inplace = True)\nbike_df.shape","ac61b1b0":"too_late_index = bike_df[bike_df['end_time'] > end_date].index\nlen(too_late_index)","f4650d60":"bike_df.drop(index = too_late_index, inplace = True)\nbike_df.shape","b0e841fb":"no_id = bike_df[(bike_df['end_station_id'].isnull()) & \n                        (bike_df['end_station_name'].notnull())][['end_station_name']].drop_duplicates()[\n    'end_station_name']\nno_id","425236f4":"bike_df[bike_df['end_station_name'].isin(no_id)][['end_station_id', 'end_station_name']].value_counts()","6d09bf28":"bike_df[bike_df['end_station_name'].isin(['Green St at Van Ness Ave',\n                                         'Clement St at 32nd Ave'])][['end_station_id', \n                                                                      'end_station_name']].value_counts()","be25c5dd":"no_id_list = no_id.tolist()\nno_id_list","0f67f381":"#manually making a list of ids so I can match up the values and make a dictionary\ncorrect_id = [496, 47, 323, 516, 289, 290, 16, 277, 316, 290, 378, 289, 321, 37, 234]\nid_correction = {no_id_list[i]: correct_id[i] for i in range(len(no_id_list))}\nid_correction","fb58418b":"for key, value in id_correction.items():\n    bike_df.loc[bike_df['start_station_name'] == key, 'start_station_id'] = value\n    bike_df.loc[bike_df['end_station_name'] == key, 'end_station_id'] = value","67ded8cb":"for key in id_correction.items():\n    print(bike_df[(bike_df['start_station_name'] == key) & \n            (bike_df['start_station_id']).isnull()][['start_station_name']].count())\n    print(bike_df[(bike_df['end_station_name'] == key) & \n            (bike_df['end_station_id']).isnull()][['end_station_name']].count())","400203bc":"for col in ['start_station_id', 'start_station_name', 'end_station_id', 'end_station_name']:\n    print(col + ': ' + str(bike_df[col].count()))","12359364":"bike_df.fillna({'start_station_id': -1, 'start_station_name': 'non-station', \n                'end_station_id': -1, 'end_station_name': 'non-station'}, inplace = True)\n\nfor col in ['start_station_id', 'start_station_name', 'end_station_id', 'end_station_name']:\n    print(col + ': ' + str(bike_df[col].count()))","5aa30ee9":"zero_coord = bike_df[(bike_df['start_station_latitude'] == 0) | \n                     (bike_df['end_station_latitude'] == 0)].index\nlen(zero_coord)","8c4a48ad":"#one less than I counted earlier, so let's see if that one got purged when I set the date range.\nlen(bike_df_not_clean[(bike_df_not_clean['start_station_latitude'] == 0) | \n                     (bike_df_not_clean['end_station_latitude'] == 0)].index)","786f0428":"#yeap, that's why I missed one. Now time to remove these from the bike dataframe.\nbike_df.drop(index = zero_coord, inplace = True)\nbike_df.shape","5f6ea71a":"bike_df = bike_df.astype({'bike_id':'int64','start_station_id':'int64','end_station_id':'int64'})\nbike_df.dtypes","4fcb3b51":"id_name = {16: 'Market St at Steuart St', 37: 'Folsom St at 2nd St', 47: 'Clara St at 4th St', \n           224: '21st St at 5th Ave', 229: 'Bond St at High St', 234: 'Fruitvale Ave at International Blvd', \n           277: 'W Julian St at N Morrison St', 289: '5th St at Taylor St', 290: 'George St at 1st St', \n           316: '1st St at San Carlos St', 321: 'Folsom St at 5th St', 323: 'Broadway at Kearny St', \n           349: 'Howard St at 6th St', 378: '7th St at Empire St', 393: 'Asbury St at The Alameda', \n           516: 'Clement St at 32nd Ave', 496: 'Green St at Van Ness Ave'}\n\nfor key, value in id_name.items():\n    bike_df.loc[bike_df['start_station_id'] == key, 'start_station_name'] = value\n    bike_df.loc[bike_df['end_station_id'] == key, 'end_station_name'] = value\n\n","9216077c":"for key in id_name:\n    print(bike_df[bike_df['start_station_id'] == key]['start_station_name'].value_counts())\n    print(bike_df[bike_df['end_station_id'] == key]['end_station_name'].value_counts())","970937b5":"bike_df[bike_df['end_station_id'] == 408][['end_station_id','end_station_latitude', \n                                                     'end_station_longitude']].value_counts()","f3f38263":"print(bike_df[bike_df['start_station_latitude'] == 45.510000].head())\nprint(bike_df[bike_df['end_station_latitude'] == 45.510000].head())","05a242c9":"mont_index = bike_df[bike_df['start_station_latitude'] == 45.510000].index\nbike_df.drop(index = mont_index, inplace = True)\nbike_df.shape","44f22382":"stations_start = bike_df[bike_df['start_station_id'] != -1][['start_station_id', \n                                                             'start_station_latitude', 'start_station_longitude']]\nstations_end = bike_df[bike_df['end_station_id'] != -1][['end_station_id', \n                                                             'end_station_latitude', 'end_station_longitude']]\nstations_start.rename(columns = {'start_station_id':'station_id', 'start_station_latitude': 'station_latitude', \n                               'start_station_longitude':'station_longitude'}, inplace = True)\nstations_start.columns","02497d5e":"stations_end.rename(columns = {'end_station_id':'station_id', 'end_station_latitude': 'station_latitude', \n                               'end_station_longitude':'station_longitude'}, inplace = True)\nstations = stations_start.append(stations_end)\nstations.head()","66e6c8b7":"stations_mean = stations.groupby('station_id').agg('mean')\nstations_mean.head()","e376b2c9":"stations_mean.columns","38a9fad8":"lat_dict = {}\nlon_dict = {}\nfor index in stations_mean.index:\n    lat_dict.update({index: round(stations_mean['station_latitude'][index], 6)})\n    lon_dict.update({index: round(stations_mean['station_longitude'][index], 6)})\n\nlat_dict","8bba0f09":"for key in lat_dict:\n    bike_df.loc[bike_df['start_station_id'] == key, 'start_station_latitude'] = lat_dict[key]\n    bike_df.loc[bike_df['start_station_id'] == key, 'start_station_longitude'] = lon_dict[key]\n    bike_df.loc[bike_df['end_station_id'] == key, 'end_station_latitude'] = lat_dict[key]\n    bike_df.loc[bike_df['end_station_id'] == key, 'end_station_longitude'] = lon_dict[key]\n\nbike_df[['start_station_id', 'start_station_latitude']].value_counts()","d5763099":"bike_df.head()","4d5a5e29":"week_long_index = bike_df[bike_df['duration_sec'] > 604800].index\nlen(week_long_index)","bc3e88cf":"bike_df.drop(index = week_long_index, inplace = True)\nbike_df.shape","397fb135":"south = bike_df[(bike_df['start_station_latitude'] < 37) | (bike_df['end_station_latitude'] < 37)].index.tolist()\nnorth = bike_df[(bike_df['start_station_latitude'] > 38) | (bike_df['end_station_latitude'] > 38)].index.tolist()\neast = bike_df[(bike_df['start_station_longitude'] > -121) | (bike_df['end_station_longitude'] > -121)].index.tolist()\n\nout_of_bounds = south + north + east\nlen(out_of_bounds)","bfd878e1":"bike_df.drop(index = out_of_bounds, inplace = True)\nbike_df.shape","79db28c8":"out_of_bounds","26fd72a2":"bike_df['start_time'].dt.date","c5401f36":"stations_start = bike_df[bike_df['start_station_id'] != -1][['start_station_id', 'start_station_name', \n                                                             'start_station_latitude', 'start_station_longitude']]\nstations_end = bike_df[bike_df['end_station_id'] != -1][['end_station_id', 'end_station_name', \n                                                             'end_station_latitude', 'end_station_longitude']]\nstations_start.rename(columns = {'start_station_id':'station_id', 'start_station_name':'station_name', \n                                 'start_station_latitude': 'station_latitude', \n                                 'start_station_longitude':'station_longitude'}, inplace = True)\nstations_end.rename(columns = {'end_station_id':'station_id', 'end_station_name':'station_name', \n                               'end_station_latitude': 'station_latitude', \n                               'end_station_longitude':'station_longitude'}, inplace = True)\nstations_df = stations_start.append(stations_end)\nstations_df.head()","649b2db4":"stations_df.nunique()","8ec11274":"stations_df.drop_duplicates(inplace = True)\nstations_df.shape","9612a85c":"bike_df.to_csv('bike.csv', index = False)\nstations_df.to_csv('stations.csv', index = False)","eeaed121":"## Cleaning\nI'm going to start by reducing my data to the dates in question (2\/11\/19 - 3\/15\/20). This will mean first changing them to data\/time.","7a9c9adb":"I've got each station at only one coordinatefor it.\n\nNow it's time to remove the two over a week trips.","ec9b5dd2":"## Gather\nHere I'm am importing the Ford GoBike data from 2\/11\/2019 (when the BART shcedule was updated) to 3\/16\/2020 (when the bay area stay at home order was put in place), the BART stations, and the BART schedules.\n\nFirst I'm going to attemp to grab all of the data out of multiple csv files. I was going to use glob, but I can't seem to load that package into the Azure Notebook.","c5af2f7a":"Looks like some of these are just the street names inverted, but others look like different intersection all together. I need to see which ones are just that and which ones are different. Those that are different, I'll be firing up google maps.","261e44b9":"Okay, they now all have the same columns so the dfs can be combined into one.","0371cee5":"And for my last trick, I'm going to create a station dataframe.","2d106e07":"Okay, all but two of these actually have station numbers, just they weren't recorded. I can definitly add those in. It looks like the first zero may have just changed ave to st as I got a 'Green St at Van Ness Ave' station on the Bay Wheel's map. And the other one shouldn't have spelled out Avenue as I got a 'Clement St at 32nd Ave'. Let's just check to make sure those are in the data.","1d65e78d":"The latitude has a minimum of 0 and latitude has a maximum of 0. As I know the bay area doesn't extend to the equator or the prime meridian, I know something's wrong with that. It's probably not entered; I'll see if the station of these have a correct latitude and longitude elsewhere in the data.\n\nNext thing from this, it looks like at least one bike was out for 10.5 days. This means I'll need to keep the data for both the start and end times. Also it looks like the bikes are usually only borrowed for a few minutes, leading me to believe that these are more for commuting than for pleasure riding.","9b631f1a":"Now time to remove any data where the latitude and longitude are out of the area.","05b51fdd":"Now time to make some corrections to the stations, starting with adding the station ID to the stations that have a name and missing ID.","43472bdd":"Now for consistancy, I'm going average out the latitudes and longitudes of the stations for just one in each station.","14b44967":"### Task list\n* Change start and end times to date time.\n* Remove everything before 2\/11\/19 (when I have BART schedule information) and after 3\/15\/20 (as the stay at home order started the next day).\n* Fill in station IDs for those that have station names.\n* Add 'station ID' of -1 and 'non-station' for the station name for all records that didn't start or end at\n    a station\n* Remove data with stations 420 and 449.\n* Change station IDs to integers.\n* Correct the names for the stations with more than one name\n* Replace the Montreal coordinates to the most common coordiantes at that station.\n* Average out the latitudes and longitues of the stations.\n* Remove the two trips that were over a week long.\n* Remove data for outlier\/incorrect latitude and longitude.\n* Get the data, time, and day of the week into seperate columns (I will have to change the original column title\n    fromtime to time_stamp). Edit: as there is no data or time data types, I won't be able to order using this \n    so I won't split this.\n* Create a dataframe with the station data (for easy look up of data).","4b233ef1":"## Saving\nNow that things have been cleaned up, it's time to save it.","a8e51123":"Why are there missing station ids, but no missing station latitude and longitude?","f9c698bf":"These two stations do not appear to be actual places that people can check bikes out of. This is probably a registry of taking them in or out of service. They will be deleted.","cc1f08b2":"Okay, I found that I can get the station number for each of these.","5fd301d2":"A few interesting things about the number of unique values. There are 14,633 bikes in the system, and two user types (subscriber and customer... not sure what the difference is). The start and end times are nearly all unique which makes sense. The durration strikes me as a little strange as a little strange, I'd expect a lot more here but it's probably just a bunch of short rides. There are 455 station IDs, but 473 names; did the stations change name or did they move? At the same time, there are over 300 thousand latitudes and longitudes; this probably means that they are too precise and I'll need to round them a bit to get the station location (find the mean see the max and min for each station to figure it out).","6835fecb":"I was scared for a minute, there were 15 in the list but only 8 removed. But printing out the list, there are only 8 unique values in the list.\n\nNow time to split out the time. But first let's change to a more appropriate name.","0927cd42":"Wow, even the log of the the durration is right skewed. It does look like about 2 million are between 100 seconds (1 min, 40 sec) and 1000 seconds (16 min, 40 sec). Let's look at the time intervals.","e2d073af":"It appears that some of the files have \"rental access method\" and some have \"bike share for all trip\" columns. I will not be using these, so they will be dropped and these dataframes will be merged.","e2357eaf":"That's 538 miles difference north\/south and 2,636 miles east\/west... That can't be right, that's about 2\/3 of the distance of California from south to north and 5\/6 of the distance across the US east to west. Let's look at everything over 0.001 degrees (365 ft NS and 285 ft EW).","f30dc0d7":"Okay it looks like station 420 and 449 are the only two with 0s for latitude and longitude. Let's see if either of these has any different values. This can easily be done just with a count; if the counts are more then the numbers with 0s then there's another latitude and longitude value I can use.","7cfafcc4":"So they are the same. We will only have to fix it with one dictionary.","eb53ffdd":"There are 221 less than one minute. Only about 1.25% are over an hour. Only 2 that are more than a day. When looking at duration, the 2 that are more than a day are major outliers and will be removed from analysis of duration (2 people forgot that they borrowed the bike and thought it was their's).","a20ed30f":"There are more non-null station names than station ids. It might be that these are stations that actually have numbers, but they weren't entered, or there's a station or stations that don't have numbers. Let's look at this. ","a87bace3":"Except for station 408 (which appears to occationally teleport to Montreal, Canada) all of these we can either average out or take the most frequent latitude or longitude. Just in case, I'm going to check the end station to make sure there's not too much variation.","10784bb9":"Now for looking at the latitude and longitude differences. There are 69 miles in 1 degree latitude, and at 37.77 degrees north (latitude of San Francisco), 1 degree longitude is 54 miles (which will be a good approximation for the whole bay area). Going by station, I'm going find the difference between the min and max latitudes and longitudes to see if there's anything significant difference.","2fca93e6":"All of these ids only have one name now.\n\nNow getting station 408 out of Montreal and back to the bay area.","9bc3826b":"Now the nulls are -1 or 'non-station'.\n\nTime to remove the stations thare aren't public (420 and 449), which are all the ones with a zero latitude & longitude","91c4bf2e":"Saving should be like voting, save early and save often. Or as another saying I've heard \"Jesus saves. So do all his aposiles. You should too.\"","3ff6301e":"Oops, looks like I included the stations as well. Well we already know that 153 are marked as 0 latitude and longitude, and 8 were in Montrial. So that means that there are 4 non-station bikes north of 38 degrees, 11 south of 37 degrees, and 14 east of -121. I'll drop these outliers.","7f888e2d":"## Assess\nI've already noticed a few things about the data from a quick look at the csv. First, the station data is repeated, there's a station ID, name, latitude, and longitude in every entry. This is going to need to stay in as there is out of station parking, but I'm still going to create a station df so I can look at things using this.\n\nSecond, I noticed that I'm going to need the time\/date stamp split up into date, time, and day of the week. While I could keep running the functions everytime I need them, it would be easier if they were by them selves.","242a7b01":"Bay Wheels has a map where you can search for the stations by name. Only one of the names comes up with a station. Here's the station name that actually registers\n* 16: Market St at Steuart St\n* 37: Folsom St at 2nd St\n* 47: Clara St at 4th St \n* 224: 21st St at 5th Ave\n* 229: Bond St at High St\n* 234: Fruitvale Ave at International Blvd\n* 277: W Julian St at N Morrison St\n* 289: 5th St at Taylor St\n* 290: George St at 1st St\n* 316: 1st St at San Carlos St \n* 321: Folsom St at 5th St\n* 323: Broadway at Kearny St\n* 349: Howard St at 6th St\n* 378: 7th St at Empire St\n* 393: Asbury St at The Alameda\n\nI will use this to create a dictionary to fix these.\n\nNow let's check and make sure these are the same as the end station.","aca7f4af":"15 stations with multiple names let's look at these.","1cd0240e":"Okay, it's time to standardize the station names.","7d4cf0b8":"That appears to have worked.\n\nNow time to fill in the blanks with -1 and non-station.","aeec7757":"Actually, looking at this, the bike starts and stops at the same station at the same latitude and longitude and a strange station name. This could be another bike sharing data in Montreal. I'm going to delete these instead of changing the latitude and longitude.","ccbfe6a0":"For some reason, Kaggle's Kernals doesn't give the number of non null values automatically in it. Oops, I need to look at that info.","1d4c86fc":"Well there's something else; start and end times should be time date. When spliting up the date\/time, I'll have to see if they ever go over more than one calander day.\n\nAlso, change the station IDs to integers.","a056b7ac":"Okay, it looks like there is no date or time data types in pandas. This means I won't be able to order them using this. I'm not going to split up the start or end times.","42bd3d94":"Just to make it a little cleaner, I'm going to change the station IDs to integers instead of floats. I could change these to strings, but I just don't feel like putting quotes around the ids throughout the project...","8dc9cc14":"Okay, it looks like the latitude has a min a little south of the bay area and a max a little north of the bay area. The longitude is more disturbing in that someone rode the bike all the way to somewhere around Iowa... Let's look closer at these."}}