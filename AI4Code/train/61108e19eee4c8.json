{"cell_type":{"2fc4328f":"code","a26cad8e":"code","33bffed3":"code","181da357":"code","9eb3a309":"markdown","5f65465a":"markdown"},"source":{"2fc4328f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nprint(os.listdir(\"..\/input\"))","a26cad8e":"sub1 = pd.read_csv('..\/input\/giba-r-data-table-simple-features-1-17-lb\/submission-giba-1.csv')\nsub2 = pd.read_csv('..\/input\/keras-neural-net-for-champs\/workingsubmission-test.csv')\nprint( sub1['scalar_coupling_constant'].describe() )\nprint( sub2['scalar_coupling_constant'].describe() )","33bffed3":"#Mean absolute difference\n( sub1['scalar_coupling_constant'] - sub2['scalar_coupling_constant']).abs().mean()","181da357":"# I used 0.6 weight for LGB just because it performed a little bit better in Public LB.\nsub1['scalar_coupling_constant'] = 0.6*sub1['scalar_coupling_constant'] + 0.4*sub2['scalar_coupling_constant']\nsub1.to_csv('weighted-avg-blend-lgb-keras-1.csv', index=False )\nsub1['scalar_coupling_constant'].describe()","9eb3a309":"# Weighted Average example","5f65465a":"Blending is the best way to explore diversity from models.\n\nTaking that into account, why not to blend a LightGBM and Keras based models?\n\nI grab output submissions files from that kernels:\n\n- https:\/\/www.kaggle.com\/titericz\/giba-r-data-table-simple-features-1-17-lb\n- https:\/\/www.kaggle.com\/todnewman\/keras-neural-net-for-champs"}}