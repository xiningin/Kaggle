{"cell_type":{"26a65c85":"code","51bc34ec":"code","758d1b05":"code","0195a0f7":"code","5ef60e08":"code","4aa673d3":"code","a209665d":"code","a2646c4f":"code","1348c17e":"code","e984131a":"code","7a173492":"code","cb059b44":"code","3edfe6f7":"code","9d9a5d44":"code","a696c72b":"code","32bf7424":"code","fa23aeca":"code","5d40fd21":"code","59d27173":"code","985e1461":"markdown","085ecefd":"markdown","3a1ff5a6":"markdown","2d54c488":"markdown","e582895a":"markdown","bcccdfa4":"markdown","64aaf592":"markdown","a0d1f3b6":"markdown","640b8796":"markdown","30e18a75":"markdown","b5798dff":"markdown","54acd010":"markdown","28f274d5":"markdown","b89815fd":"markdown","4de7a651":"markdown","666df0f3":"markdown"},"source":{"26a65c85":"from IPython.display import IFrame\nIFrame('https:\/\/datastudio.google.com\/embed\/reporting\/bdd0af88-f4df-4357-a67b-550f9e7ad9c0\/page\/QQaIB', width='100%', height=900)","51bc34ec":"#basic libraries\nimport pandas as pd\nimport numpy as np\nimport os\nimport gc\nimport datetime as dt\nimport math\nfrom IPython.display import Image\nimport warnings\nwarnings.filterwarnings('ignore')","758d1b05":"#Directly pulling data from the source\npath1 = \"https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_19-covid-Confirmed.csv\"\npath2 = \"https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_19-covid-Deaths.csv\"\npath3 = \"https:\/\/raw.githubusercontent.com\/CSSEGISandData\/COVID-19\/master\/csse_covid_19_data\/csse_covid_19_time_series\/time_series_19-covid-Recovered.csv\"\ndata_crfm = pd.read_csv(path1)\ndata_dead = pd.read_csv(path2)\ndata_reco = pd.read_csv(path3)","0195a0f7":"data_crfm.head()","5ef60e08":"#What I am doing here is aloting the previous value to next date if the cumulative count is less on next date!\ndef modifier(x):\n    return(x[0] if x[0]>x[1] else x[1])\n\ndef data_correctr(data):\n    total_cols = data.shape[1]\n    cols = data.columns\n    for i in range(5,total_cols):\n        data[cols[i]] = data[[cols[i-1], cols[i]]].apply(modifier, 1)\n    return data","4aa673d3":"#getting corrected data set!\ndata_crfm_c = data_correctr(data_crfm)\ndata_dead_c = data_correctr(data_dead)\ndata_reco_c = data_correctr(data_reco)","a209665d":"total_cols = data_crfm_c.shape[1]\n\ndata_crfm_d = data_crfm_c.copy()\ndata_dead_d = data_dead_c.copy()\ndata_reco_d = data_reco_c.copy()\n\n# this is done to calculate the percentage for every day (initalising day 1 to zero)\ndata_crfm_p = data_crfm_c.copy()\ndata_crfm_p.iloc[:,4] = 0\ndata_dead_p = data_dead_c.copy()\ndata_dead_p.iloc[:,4] = 0\ndata_reco_p = data_reco_c.copy()\ndata_reco_p.iloc[:,4] = 0\n\n\nfor i in range(5,total_cols):\n    \n    #converting cumulative to daily count\n    data_crfm_d.iloc[:, i] = data_crfm_d.iloc[:, i] - data_crfm_c.iloc[:, i-1]\n    data_dead_d.iloc[:, i] = data_dead_d.iloc[:, i] - data_dead_c.iloc[:, i-1]\n    data_reco_d.iloc[:, i] = data_reco_d.iloc[:, i] - data_reco_c.iloc[:, i-1]\n    \n    #percentage change: I will store the previous day cumulative and apply percentage change later\n    data_crfm_p.iloc[:, i] = data_crfm_c.iloc[:, i-1]\n    data_dead_p.iloc[:, i] = data_dead_c.iloc[:, i-1]\n    data_reco_p.iloc[:, i] = data_reco_c.iloc[:, i-1]\n\n# Here I am storing previous day daily count I will need this to calculate percentage change metric: the 6 small box in the dashboard\ndata_crfm_dp = data_crfm_d.copy()  \ndata_crfm_dp.iloc[:,4] = 0\ndata_dead_dp = data_dead_d.copy()\ndata_dead_dp.iloc[:,4] = 0\ndata_reco_dp = data_reco_d.copy()\ndata_reco_dp.iloc[:,4] = 0\n\nfor i in range(5,total_cols):\n    #percentage change: I will store the previous day daily and apply percentage change later\n    data_crfm_dp.iloc[:, i] = data_crfm_d.iloc[:, i-1]\n    data_dead_dp.iloc[:, i] = data_dead_d.iloc[:, i-1]\n    data_reco_dp.iloc[:, i] = data_reco_d.iloc[:, i-1]\n","a2646c4f":"# Here comes the melt funtion of pandas. One line and your coloumns turns into rows!\ndf_crfm = pd.melt(data_crfm_d, id_vars = ['Province\/State', 'Country\/Region', 'Lat', 'Long'], var_name = 'Time').rename(columns = {'value':\"Daily Confirmed\"})\ndf_dead = pd.melt(data_dead_d, id_vars = ['Province\/State', 'Country\/Region', 'Lat', 'Long'], var_name = 'Time').rename(columns = {'value':\"Daily Death\"})\ndf_reco = pd.melt(data_reco_d, id_vars = ['Province\/State', 'Country\/Region', 'Lat', 'Long'], var_name = 'Time').rename(columns = {'value':\"Daily Recovered\"})\n\ndf_crfm_c = pd.melt(data_crfm_c, id_vars = ['Province\/State', 'Country\/Region', 'Lat', 'Long'], var_name = 'Time').rename(columns = {'value':\"Cum Confirmed\"})\ndf_dead_c = pd.melt(data_dead_c, id_vars = ['Province\/State', 'Country\/Region', 'Lat', 'Long'], var_name = 'Time').rename(columns = {'value':\"Cum Death\"})\ndf_reco_c = pd.melt(data_reco_c, id_vars = ['Province\/State', 'Country\/Region', 'Lat', 'Long'], var_name = 'Time').rename(columns = {'value':\"Cum Recovered\"})\n\ndf_crfm_p = pd.melt(data_crfm_p, id_vars = ['Province\/State', 'Country\/Region', 'Lat', 'Long'], var_name = 'Time').rename(columns = {'value':\"PCum Confirmed\"})\ndf_dead_p = pd.melt(data_dead_p, id_vars = ['Province\/State', 'Country\/Region', 'Lat', 'Long'], var_name = 'Time').rename(columns = {'value':\"PCum Death\"})\ndf_reco_p = pd.melt(data_reco_p, id_vars = ['Province\/State', 'Country\/Region', 'Lat', 'Long'], var_name = 'Time').rename(columns = {'value':\"PCum Recovered\"})\n\ndf_crfm_dp = pd.melt(data_crfm_dp, id_vars = ['Province\/State', 'Country\/Region', 'Lat', 'Long'], var_name = 'Time').rename(columns = {'value':\"dPCum Confirmed\"})\ndf_dead_dp = pd.melt(data_dead_dp, id_vars = ['Province\/State', 'Country\/Region', 'Lat', 'Long'], var_name = 'Time').rename(columns = {'value':\"dPCum Death\"})\ndf_reco_dp = pd.melt(data_reco_dp, id_vars = ['Province\/State', 'Country\/Region', 'Lat', 'Long'], var_name = 'Time').rename(columns = {'value':\"dPCum Recovered\"})","1348c17e":"df_crfm.head()","e984131a":"print(df_crfm.shape, df_dead.shape, df_reco.shape, df_crfm_c.shape, df_dead_c.shape, df_reco_c.shape, df_crfm_p.shape, df_dead_p.shape, df_reco_p.shape)","7a173492":"#Collecting the metric into 1 data frame\ndf = df_crfm.merge(df_dead[['Country\/Region','Lat','Long', 'Time', 'Daily Death']], how = 'left', on = ['Country\/Region','Lat', 'Long', 'Time'])\ndf = df.merge(df_reco[['Country\/Region','Lat','Long', 'Time', 'Daily Recovered']], how = 'left', on = ['Country\/Region','Lat', 'Long', 'Time'])\n\ndf = df.merge(df_crfm_c[['Country\/Region','Lat','Long', 'Time', 'Cum Confirmed']], how = 'left', on = ['Country\/Region','Lat', 'Long', 'Time'])\ndf = df.merge(df_dead_c[['Country\/Region','Lat','Long', 'Time', 'Cum Death']], how = 'left', on = ['Country\/Region','Lat', 'Long', 'Time'])\ndf = df.merge(df_reco_c[['Country\/Region','Lat','Long', 'Time', 'Cum Recovered']], how = 'left', on = ['Country\/Region','Lat', 'Long', 'Time'])\n\ndf = df.merge(df_crfm_p[['Country\/Region','Lat','Long', 'Time', 'PCum Confirmed']], how = 'left', on = ['Country\/Region','Lat', 'Long', 'Time'])\ndf = df.merge(df_dead_p[['Country\/Region','Lat','Long', 'Time', 'PCum Death']], how = 'left', on = ['Country\/Region','Lat', 'Long', 'Time'])\ndf = df.merge(df_reco_p[['Country\/Region','Lat','Long', 'Time', 'PCum Recovered']], how = 'left', on = ['Country\/Region','Lat', 'Long', 'Time'])\n\ndf = df.merge(df_crfm_dp[['Country\/Region','Lat','Long', 'Time', 'dPCum Confirmed']], how = 'left', on = ['Country\/Region','Lat', 'Long', 'Time'])\ndf = df.merge(df_dead_dp[['Country\/Region','Lat','Long', 'Time', 'dPCum Death']], how = 'left', on = ['Country\/Region','Lat', 'Long', 'Time'])\ndf = df.merge(df_reco_dp[['Country\/Region','Lat','Long', 'Time', 'dPCum Recovered']], how = 'left', on = ['Country\/Region','Lat', 'Long', 'Time'])","cb059b44":"df.head()","3edfe6f7":"#Some metrics\n\ndf['Mortality'] = ((df['Cum Death'] *100) \/df['Cum Confirmed']).replace([np.inf, -np.inf], np.nan).fillna(0)\ndf['perchange Confirmed'] = ((df['Daily Confirmed']*100)\/df['PCum Confirmed']).replace([np.inf, -np.inf], np.nan).fillna(0)\ndf['perchange Death'] = ((df['Daily Death']*100)\/df['PCum Death']).replace([np.inf, -np.inf], np.nan).fillna(0)\ndf['perchange Recovered'] = ((df['Daily Recovered']*100)\/df['PCum Recovered']).replace([np.inf, -np.inf], np.nan).fillna(0)","9d9a5d44":"#last datatype corrections before feeding to data studio\ndf['Time'] = pd.to_datetime(df['Time']).dt.date\ndf['Lat Long'] = df['Lat'].astype(str)+\",\"+df['Long'].astype(str)","a696c72b":"df.to_csv(\"C:\\\\Users\\\\sikumar\\\\Downloads\\\\Corona_virus.csv\", index = False)\nprint(\"Done!\")","32bf7424":"Image(\"..\/input\/some-images-datastudio\/Create new data source.PNG\")","fa23aeca":"Image(\"..\/input\/some-images-datastudio\/Create new data source 2.PNG\")","5d40fd21":"Image(\"..\/input\/some-images-datastudio\/Dtypes.PNG\")","59d27173":"Image(\"..\/input\/some-images-datastudio\/workspace.PNG\")","985e1461":"Lets Jump into the code!","085ecefd":"Explore the tools. Most of them are self explainatory! \nYou get to draw your tools (filter, dropdown selection, apply basic formulae to the total count boxes and so on.. )\n\n**Happy Dashboarding!**\n\n**Save yourself world will save itself! :)**","3a1ff5a6":"**Data Preparation**","2d54c488":"The challenging part is not making the dashboard. Its the preparation of the data required to be feed into the dashboard!\nOnce you you prepare the data making dashboard is just like another BI tool.","e582895a":"Google has a wonderful tool for data viz: The Data Studio. I came to know about this tool when I was processing my data in Google Big Query. After processing the data they provide an option to visualise it using the data studio. There I found this litte wonder!\n\nIts free open for everyone. You can connect using various options like csv files, google sheets, big query, jsons, gcp etc.\nI am seeing several dashborad already floating around on COVID-19. I thought give walkthough how you can create the below one embedded!","bcccdfa4":"**Step 2: Make sure the data type for your new data set is correct**","64aaf592":"Key Points: \n*     First I need to transform data from coloum format to to row format!  --- Pandas has a beautiful melt funtion that does this for me.\n*     I will need the daily count instead the cumulative count. I need to write code in such a way that I don't need to change anything unless the data source changes!\n*     Then I need to do same for all the metrics that I am creating. Like Cumulative, Daily and mortality rate, previous day cummulative, previous daily cumulative.\n","a0d1f3b6":"Google Data Studio!\n\nHere you start! -> https:\/\/datastudio.google.com\/u\/0\/","640b8796":"Next Job!","30e18a75":"Either you can mannualy upload this data to google sheet or create an API to do this! \n\nFew tutorial for creating api:\n\n\n    https:\/\/developers.google.com\/sheets\/api\/quickstart\/js\n    \n    https:\/\/www.benlcollins.com\/apps-script\/api-tutorial-for-beginners\/","b5798dff":"Here's the standalone link: https:\/\/datastudio.google.com\/reporting\/bdd0af88-f4df-4357-a67b-550f9e7ad9c0","54acd010":"**Step 3: Click on create a report!: You will be take to the report section where all the fun starts!","28f274d5":"**How I made this?**","b89815fd":"**Step 1: Create a data source: Click + button on the top right corner and add a data source. You can connect using the google sheet or directly upload your csv file :)\n\n\n**","4de7a651":"I found that data is little inconsistent. Cumulative counts have decreased for some countries\/region (For example Japan you can see up on 23rd Jan their count decreased!). Which I tried correcting by below two funtions.\nI don't want to debate on this why this happened as this might not be relevant for this tutorial!","666df0f3":"You can see dashboard has several features:\n    1. The timeline feature: One select the time line by he\/she wants to visulaize the data.\n    2. The Geo Viz. Clicking on which the whole dashboard get the country filter. You can aslo apply country\/region filter from the side table.\n    3. Not only this you can drill though the map. Try going inside China. (Other countries don't the proper naming done in this data)\n    4. You can also chnage the metric in the map from confirmed cases to death and recovered cases using the optional metric.\n    5. In the botton I am showing the time trend. You can select any metric of your choice like Cumulative Confirmed cases or Daily cases or mortality rate. \n    6. You can also apply timeline filter using this chart. And you can also apply country\/region filter for this chart."}}