{"cell_type":{"03d97b5b":"code","8b404240":"code","47787df0":"code","840ffc44":"code","79590be7":"code","9bb6d49b":"code","000963ee":"code","032fb20c":"code","bad7f10c":"code","83429fd2":"code","53bae7be":"code","51331e8b":"code","c741176d":"code","352e0881":"code","7070b86f":"code","e9ddec78":"code","d38b28ee":"code","5aebd8d7":"code","cc3c9cee":"code","bed93068":"code","b84c27d0":"code","4322c625":"code","cd9225be":"code","dc4c84c6":"code","026334ff":"code","ae2aab27":"code","3b4206ea":"code","7bcc4a18":"code","3d16bc44":"code","fba9a265":"code","aad001e0":"code","b5b43379":"code","ca833fd0":"code","9a4bacbb":"code","72188de7":"code","ff1c1a9f":"code","ead7a561":"code","d962fcad":"code","84cb5760":"code","70cd0594":"code","8c0f0cc4":"code","54eff51f":"code","98a62a2e":"code","429f3f69":"code","4dc9ccb9":"code","b1805a63":"code","5100920a":"markdown","cb3fdbd4":"markdown","83139aae":"markdown","8c687d05":"markdown","26c4dad7":"markdown","28d448a1":"markdown","26fa26bc":"markdown","fd597cf7":"markdown","39a85438":"markdown","44af5808":"markdown","aa2b4ef3":"markdown","e40e8fa7":"markdown","b33563a0":"markdown","4909265b":"markdown","d53ff1b8":"markdown","6541f3da":"markdown"},"source":{"03d97b5b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8b404240":"# Most basic stuff for EDA.\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Core packages for text processing.\n\nimport string\nimport re\n\n# Libraries for text preprocessing.\n\nimport nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('averaged_perceptron_tagger')\nnltk.download('wordnet')\nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.tokenize import word_tokenize\nfrom nltk.probability import FreqDist\n\n# Loading some sklearn packaces for modelling.\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation, NMF\nfrom sklearn.metrics import f1_score, accuracy_score\n\n# Some packages for word clouds and NER.\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import Counter, defaultdict\nfrom PIL import Image\nimport spacy\n!pip install https:\/\/github.com\/explosion\/spacy-models\/releases\/download\/en_core_web_sm-2.2.5\/en_core_web_sm-2.2.5.tar.gz\nimport en_core_web_sm\n\n# Core packages for general use throughout the notebook.\n\nimport random\nimport warnings\nimport time\nimport datetime\n\n# For customizing our plots.\n\nfrom matplotlib.ticker import MaxNLocator\nimport matplotlib.gridspec as gridspec\nimport matplotlib.patches as mpatches\n\n# Loading pytorch packages.\n\nimport torch\nfrom transformers import BertTokenizer, BertForSequenceClassification, AdamW, BertConfig, get_linear_schedule_with_warmup\nfrom torch.utils.data import TensorDataset, random_split, DataLoader, RandomSampler, SequentialSampler\n\n# Setting some options for general use.\n\nstop = set(stopwords.words('english'))\nplt.style.use('fivethirtyeight')\nsns.set(font_scale=1.5)\npd.options.display.max_columns = 250\npd.options.display.max_rows = 250\nwarnings.filterwarnings('ignore')\n\n\n#Setting seeds for consistent results.\nseed_val = 42\nrandom.seed(seed_val)\nnp.random.seed(seed_val)\ntorch.manual_seed(seed_val)\ntorch.cuda.manual_seed_all(seed_val)","47787df0":"trainv = pd.read_csv('..\/input\/fake-news\/train.csv')\ntestv = pd.read_csv('..\/input\/fake-news\/test.csv')","840ffc44":"# Taking general look at the both datasets.\n\ndisplay(trainv.sample(5))\ndisplay(testv.sample(5))","79590be7":"# Checking observation and feature numbers for train and test data.\n\nprint(trainv.shape)\nprint(testv.shape)","9bb6d49b":"# Check for missing values\n\ntrainv['title'].isnull().value_counts(), testv['title'].isnull().value_counts()","000963ee":"# Remove missing values\n\ntrainv = trainv.dropna(subset = ['text', 'title'])\ntestv = testv.dropna(subset = ['text', 'title'])","032fb20c":"trainv['text'].isnull().value_counts(), testv['text'].isnull().value_counts()","bad7f10c":"# ombine train and test data\n\ncombined = pd.concat([trainv, testv])","83429fd2":"# Some basic helper functions to clean text by punctuations.\n\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\n        '['\n        u'\\U0001F600-\\U0001F64F'  # emoticons\n        u'\\U0001F300-\\U0001F5FF'  # symbols & pictographs\n        u'\\U0001F680-\\U0001F6FF'  # transport & map symbols\n        u'\\U0001F1E0-\\U0001F1FF'  # flags (iOS)\n        u'\\U00002702-\\U000027B0'\n        u'\\U000024C2-\\U0001F251'\n        ']+',\n        flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\n\ndef remove_html(text):\n    html = re.compile(r'<.*?>|&([a-z0-9]+|#[0-9]{1,6}|#x[0-9a-f]{1,6});')\n    return re.sub(html, '', text)\n\ndef remove_comma(readData):\n    text = re.sub('[-=+,#\/\\?:^$.@*\\\"\u203b~&%\u318d!\u300f\\\\\u2018|\\(\\)\\[\\]\\<\\>`\\'\u2026\u300b]', '', readData)\n    return text\n\ndef remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\n# Applying helper functions (Text)\n\ncombined['text_clean'] = combined['text'].apply(lambda x: remove_punct(x))\ncombined['text_clean'] = combined['text_clean'].apply(lambda x: remove_html(x))\ncombined['text_clean'] = combined['text_clean'].apply(lambda x: remove_emoji(x))\ncombined['text_clean'] = combined['text_clean'].apply(lambda x: remove_comma(x))                     \n\n# Applying helper functions (Title)\n\ncombined['title_clean'] = combined['title'].apply(lambda x: remove_punct(x))\ncombined['title_clean'] = combined['title_clean'].apply(lambda x: remove_html(x))\ncombined['title_clean'] = combined['title_clean'].apply(lambda x: remove_emoji(x))\ncombined['title_clean'] = combined['title_clean'].apply(lambda x: remove_comma(x))","53bae7be":"# Tokenizing the tweet base texts.\n\ncombined['tokenized_text'] = combined['text_clean'].apply(word_tokenize)\ncombined['tokenized_title'] = combined['title_clean'].apply(word_tokenize)\ncombined.head()","51331e8b":"# Lower casing clean text.\n\ncombined['lower_text'] = combined['tokenized_text'].apply(\n    lambda x: [word.lower() for word in x])\n\ncombined['lower_title'] = combined['tokenized_title'].apply(\n    lambda x: [word.lower() for word in x])\n\ncombined.head()","c741176d":"# Removing stopwords.\n\ncombined['stopwords_removed_text'] = combined['lower_text'].apply(\n    lambda x: [word for word in x if word not in stop])\ncombined['stopwords_removed_title'] = combined['lower_title'].apply(\n    lambda x: [word for word in x if word not in stop])\n\ncombined.head()","352e0881":"combined = combined.drop(['stopwords_removed'], axis = 1)","7070b86f":"combined.head()","e9ddec78":"# Applying part of speech tags.\n\ncombined['pos_tags_text'] = combined['stopwords_removed_text'].apply(nltk.tag.pos_tag)\ncombined['pos_tags_title'] = combined['stopwords_removed_title'].apply(nltk.tag.pos_tag)","d38b28ee":"combined.head()","5aebd8d7":"# Converting part of speeches to wordnet format.\n\ndef get_wordnet_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('N'):\n        return wordnet.NOUN\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN\n\n\ncombined['wordnet_pos_text'] = combined['pos_tags_text'].apply(\n    lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\ncombined['wordnet_pos_title'] = combined['pos_tags_title'].apply(\n    lambda x: [(word, get_wordnet_pos(pos_tag)) for (word, pos_tag) in x])\n\ncombined.head()","cc3c9cee":"# Applying word lemmatizer.\n\nwnl = WordNetLemmatizer()\n\ncombined['lemmatized_text'] = combined['wordnet_pos_text'].apply(\n    lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n\ncombined['lemmatized_text'] = combined['lemmatized_text'].apply(\n    lambda x: [word for word in x if word not in stop])\n\ncombined['lemma_str_text'] = [' '.join(map(str, l)) for l in combined['lemmatized_text']]\n\ncombined['lemmatized_title'] = combined['wordnet_pos_title'].apply(\n    lambda x: [wnl.lemmatize(word, tag) for word, tag in x])\n\ncombined['lemmatized_title'] = combined['lemmatized_title'].apply(\n    lambda x: [word for word in x if word not in stop])\n\ncombined['lemma_str_title'] = [' '.join(map(str, l)) for l in combined['lemmatized_title']]\n\ncombined.head()","bed93068":"# Displaying target distribution.\n\nfig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), dpi=100)\nsns.countplot(trainv['label'], ax=axes[0])\naxes[1].pie(trainv['label'].value_counts(),\n            labels=['reliable', 'unreliable'],\n            autopct='%1.2f%%',\n            shadow=True,\n            explode=(0.05, 0),\n            startangle=60)\nfig.suptitle('Fake News', fontsize=24)\nplt.show()","b84c27d0":"trainv = combined[:20203]\ntestv = combined[20203:]","4322c625":"# Creating a new feature for the visualization.\n\ntrainv['Character Count'] = trainv['text_clean'].apply(lambda x: len(str(x)))\n\n\ndef plot_dist3(df, feature, title, bins):\n    # Creating a customized chart. and giving in figsize and everything.\n    fig = plt.figure(constrained_layout=True, figsize=(18, 10))\n    # Creating a grid of 3 cols and 3 rows.\n    grid = gridspec.GridSpec(ncols= 2,nrows=2, figure=fig)\n\n    # Customizing the histogram grid.\n    ax1 = fig.add_subplot(grid[0, 0])\n\n    # Set the title.\n    ax1.set_title('Histogram')\n    # plot the histogram.\n    sns.distplot(df.loc[:, feature],\n                 hist=True,\n                 kde=True,\n                 ax=ax1,\n                 color='#e74c3c')\n    ax1.set(ylabel='Frequency')\n    ax1.xaxis.set_major_locator(MaxNLocator(nbins=bins))\n\n    # Customizing the Box Plot.\n    ax3 = fig.add_subplot(grid[0, 1])\n    # Set title.\n    ax3.set_title('Box Plot')\n    # Plotting the box plot.\n    sns.boxplot(y=feature, data=df, ax=ax3, color='#e74c3c')\n    ax3.yaxis.set_major_locator(MaxNLocator(nbins=bins))\n\n    plt.suptitle(f'{title}', fontsize=24)","cd9225be":"plot_dist3(trainv[trainv['label'] == 0], 'Character Count',\n           'Characters Per \"Reliable\"', 5)","dc4c84c6":"plot_dist3(trainv[trainv['label'] == 1], 'Character Count',\n           'Characters Per \"Unreliable\"', 10)","026334ff":"def plot_word_number_histogram(textno, textye):\n    \n    \"\"\"A function for comparing word counts\"\"\"\n\n    fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), sharey=True)\n    sns.distplot(textno.str.split().map(lambda x: len(x)), ax=axes[0], color='#e74c3c')\n    sns.distplot(textye.str.split().map(lambda x: len(x)), ax=axes[1], color='#e74c3c')\n    \n    axes[0].set_xlabel('Word Count')\n    axes[0].set_ylabel('Frequency')\n    axes[0].set_title('Reliable')\n    axes[1].set_xlabel('Word Count')\n    axes[1].set_title('Unreliable')\n    \n    fig.suptitle('Fake News', fontsize=24, va='baseline')\n    \n    fig.tight_layout()","ae2aab27":"plot_word_number_histogram(trainv[trainv['label'] == 0]['text'],\n                           trainv[trainv['label'] == 1]['text'])","3b4206ea":"def plot_word_len_histogram(textno, textye):\n    \n    \"\"\"A function for comparing average word length\"\"\"\n    \n    fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), sharey=True)\n    sns.distplot(textno.str.split().apply(lambda x: [len(i) for i in x]).map(\n        lambda x: np.mean(x)),\n                 ax=axes[0], color='#e74c3c')\n    sns.distplot(textye.str.split().apply(lambda x: [len(i) for i in x]).map(\n        lambda x: np.mean(x)),\n                 ax=axes[1], color='#e74c3c')\n    \n    axes[0].set_xlabel('Word Length')\n    axes[0].set_ylabel('Frequency')\n    axes[0].set_title('Reliable')\n    axes[1].set_xlabel('Word Length')\n    axes[1].set_title('Unreliable')\n    \n    fig.suptitle('Mean Word Lengths', fontsize=24, va='baseline')\n    fig.tight_layout()","7bcc4a18":"plot_word_len_histogram(trainv[trainv['label'] == 0]['text'],\n                        trainv[trainv['label'] == 1]['text'])","3d16bc44":"lis_text = [\n    trainv[trainv['label'] == 0]['lemma_str_text'],\n    trainv[trainv['label'] == 1]['lemma_str_text']\n]\n\nlis_title = [\n    trainv[trainv['label'] == 0]['lemma_str_title'],\n    trainv[trainv['label'] == 1]['lemma_str_title']\n]","fba9a265":"fig, axes = plt.subplots(1, 2, figsize=(18, 8))\naxes = axes.flatten()\n\nfor i, j in zip(lis_text, axes):\n    try:\n        new = i.str.split()\n        new = new.values.tolist()\n        corpus = [word.lower() for i in new for word in i]\n        dic = defaultdict(int)\n        for word in corpus:\n            if word in stop:\n                dic[word] += 1\n     #   print(dic)\n        top = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:15]\n     #   print(top)\n        x, y = zip(*top)\n        df = pd.DataFrame([x, y]).T\n        df = df.rename(columns={0: 'Stopword', 1: 'Count'})\n        sns.barplot(x='Count', y='Stopword', data=df, palette='plasma', ax=j)\n        plt.tight_layout()\n    except:\n        plt.close()\n        print('No stopwords left in texts.')\n        break","aad001e0":"fig, axes = plt.subplots(1, 2, figsize=(18, 8))\naxes = axes.flatten()\n\nfor i, j in zip(lis_title, axes):\n    try:\n        new = i.str.split()\n        new = new.values.tolist()\n        corpus = [word.lower() for i in new for word in i]\n        dic = defaultdict(int)\n        for word in corpus:\n            if word in stop:\n                dic[word] += 1\n     #   print(dic)\n        top = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:15]\n     #   print(top)\n        x, y = zip(*top)\n        df = pd.DataFrame([x, y]).T\n        df = df.rename(columns={0: 'Stopword', 1: 'Count'})\n        sns.barplot(x='Count', y='Stopword', data=df, palette='plasma', ax=j)\n        plt.tight_layout()\n    except:\n        plt.close()\n        print('No stopwords left in texts.')\n        break","b5b43379":"# Displaying most common words.\n\nfig, axes = plt.subplots(1, 2, figsize=(18, 8))\naxes = axes.flatten()\n\nfor i, j in zip(lis_text, axes):\n\n    new = i.str.split()\n    new = new.values.tolist()\n    corpus = [word for i in new for word in i]\n\n    counter = Counter(corpus)\n    most = counter.most_common()\n    x, y = [], []\n    for word, count in most[:30]:\n        if (word not in stop):\n            x.append(word)\n            y.append(count)\n\n\n    sns.barplot(x=y, y=x, palette='plasma', ax=j)\nprint(x, y)\naxes[0].set_title('Reliable')\n\naxes[1].set_title('Unreliable')\naxes[0].set_xlabel('Count')\naxes[0].set_ylabel('Word')\naxes[1].set_xlabel('Count')\naxes[1].set_ylabel('Word')\n\nfig.suptitle('Most Common Unigrams in Text', fontsize=24, va='baseline')\nplt.tight_layout()","ca833fd0":"fig, axes = plt.subplots(1, 2, figsize=(18, 8))\naxes = axes.flatten()\n\nfor i, j in zip(lis_title, axes):\n\n    new = i.str.split()\n    new = new.values.tolist()\n    corpus = [word for i in new for word in i]\n\n    counter = Counter(corpus)\n    most = counter.most_common()\n    x, y = [], []\n    for word, count in most[:30]:\n        if (word not in stop):\n            x.append(word)\n            y.append(count)\n\n\n    sns.barplot(x=y, y=x, palette='plasma', ax=j)\nprint(x, y)\naxes[0].set_title('Reliable')\n\naxes[1].set_title('Unreliable')\naxes[0].set_xlabel('Count')\naxes[0].set_ylabel('Word')\naxes[1].set_xlabel('Count')\naxes[1].set_ylabel('Word')\n\nfig.suptitle('Most Common Unigrams in Title', fontsize=24, va='baseline')\nplt.tight_layout()","9a4bacbb":"def ngrams(n, title, lis_type):\n    \"\"\"A Function to plot most common ngrams\"\"\"\n    fig, axes = plt.subplots(1, 2, figsize=(18, 8))\n    axes = axes.flatten()\n    for i, j in zip(lis_type, axes):\n\n        new = i.str.split()\n        new = new.values.tolist()\n        corpus = [word for i in new for word in i]\n\n        def _get_top_ngram(corpus, n=None):\n            #getting top ngrams\n            vec = CountVectorizer(ngram_range=(n, n),\n                                  max_df=0.9,\n                                  stop_words='english').fit(corpus)\n            bag_of_words = vec.transform(corpus)\n            sum_words = bag_of_words.sum(axis=0)\n            words_freq = [(word, sum_words[0, idx])\n                          for word, idx in vec.vocabulary_.items()]\n            words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n            return words_freq[:15]\n\n        top_n_bigrams = _get_top_ngram(i, n)[:15]\n        x, y = map(list, zip(*top_n_bigrams))\n        sns.barplot(x=y, y=x, palette='plasma', ax=j)\n        \n        axes[0].set_title('Reliable')\n        axes[1].set_title('Unreliable')\n        axes[0].set_xlabel('Count')\n        axes[0].set_ylabel('Words')\n        axes[1].set_xlabel('Count')\n        axes[1].set_ylabel('Words')\n        fig.suptitle(title, fontsize=24, va='baseline')\n        plt.tight_layout()","72188de7":"ngrams(2, 'Most Common Bigrams', lis_text)","ff1c1a9f":"ngrams(2, 'Most Common Bigrams', lis_title)","ead7a561":"ngrams(3, 'Most Common Bigrams', lis_text)","d962fcad":"ngrams(3, 'Most Common Bigrams', lis_title)","84cb5760":"def display_topics(text, no_top_words, topic):\n    \n    \"\"\" A function for determining the topics present in our corpus with nmf \"\"\"\n    \n    no_top_words = no_top_words\n    tfidf_vectorizer = TfidfVectorizer(\n        max_df=0.90, min_df=25, max_features=5000, use_idf=True)\n    tfidf = tfidf_vectorizer.fit_transform(text)\n    tfidf_feature_names = tfidf_vectorizer.get_feature_names()\n    doc_term_matrix_tfidf = pd.DataFrame(\n        tfidf.toarray(), columns=list(tfidf_feature_names))\n    nmf = NMF(n_components=10, random_state=0,\n              alpha=.1, init='nndsvd').fit(tfidf)\n    print(topic)\n    for topic_idx, topic in enumerate(nmf.components_):\n        print('Topic %d:' % (topic_idx+1))\n        print(' '.join([tfidf_feature_names[i]\n                        for i in topic.argsort()[:-no_top_words - 1:-1]]))\n\n\ndisplay_topics(lis_text[0], 10, 'Reliable Topics\\n')","70cd0594":"display_topics(lis_title[0], 10, 'Reliable Topics\\n')","8c0f0cc4":"display_topics(lis_text[1], 10, 'Uneliable Topics\\n')","54eff51f":"display_topics(lis_title[1], 10, 'Unreliable Topics\\n')","98a62a2e":"# Setting mask for wordcloud.\n\nmask = np.array(Image.open('..\/input\/masks\/upvote.png'))\nmask[mask.sum(axis=2) == 0] = 255","429f3f69":"def plot_wordcloud(text, title, title_size):\n    \"\"\" A function for creating wordcloud images \"\"\"\n    words = text\n    allwords = []\n    for wordlist in words:\n        allwords += wordlist\n    mostcommon = FreqDist(allwords).most_common(140)\n    wordcloud = WordCloud(\n        width=1200,\n        height=800,\n        background_color='black',\n        stopwords=set(STOPWORDS),\n        max_words=150,\n        scale=3,\n        mask=mask,\n        contour_width=0.1,\n        contour_color='grey',\n    ).generate(str(mostcommon))    \n\n    def grey_color_func(word,\n                        font_size,\n                        position,\n                        orientation,\n                        random_state=None,\n                        **kwargs):\n        # A definition for creating grey color shades.\n        return 'hsl(0, 0%%, %d%%)' % random.randint(60, 100)\n\n    fig = plt.figure(figsize=(18, 18), facecolor='white')\n    plt.imshow(wordcloud.recolor(color_func=grey_color_func, random_state=42),\n               interpolation='bilinear')\n    plt.axis('off')\n    plt.title(title,\n              fontdict={\n                  'size': title_size,\n                  'verticalalignment': 'bottom'\n              })\n    plt.tight_layout(pad=0)\n    plt.show()","4dc9ccb9":"plot_wordcloud(trainv[trainv['label'] == 0]['lemmatized_text'],\n               'Most Common Words in Reliable News',\n               title_size=30)","b1805a63":"plot_wordcloud(trainv[trainv['label'] == 1]['lemmatized_text'],\n               'Most Common Words in Unreliable News',\n               title_size=30)","5100920a":"* **Most Common Trigrams**","cb3fdbd4":"Unreliable news was shorter than realiable.","83139aae":"## N-gram","8c687d05":"## Analysis Process Using NLP :\n* Importing Neccesary Packages\n* Load Data\n* Cleaning Data\n* Visualizing the Data\n* N-gram\n* Word Cloud\n* Building the Bert Model","26c4dad7":"## Cleaning Text\n\n* Removed urls, emojis and punctuations\n* Tokenized base text and title\n* Lower cased clean text\n* Removed stopwords\n* Applied part of speech tags\n* Converted part of speeches to wordnet format\n* Applying word lemmatizer\n* Converted tokenized text to string again","28d448a1":"* **Word Lengths**","26fa26bc":"* **Most Common Bigrams**","fd597cf7":"* **Most Common Words**","39a85438":"* **Tweet Lengths**\n\nThe length of the title was expected to be less influential, so only the length of the text was considered","44af5808":"## Visualizing the Data","aa2b4ef3":"* **It was decided to remove missing values of either the title or the text column**","e40e8fa7":"Unreliable has fewer words than Reliable.","b33563a0":"**My other similar competition code** : https:\/\/www.kaggle.com\/jeongwonkim10516\/nlp-disaster-tweets-with-bert\/edit","4909265b":"* **Determining Topics**\n\nWe'll be using a method called Non-Negative Matrix Factorization (NMF) to see if we can get some defined topics out of our TF-IDF matrix, with this way TF-IDF will decrease impact of the high frequency words, so we might get more specific topics.","d53ff1b8":"* **Word Counts**\n\nWord counts also expected that the length of the table would be less influential, so only the length of the text was considered","6541f3da":"## Word Cloud"}}