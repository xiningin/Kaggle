{"cell_type":{"90d82dde":"code","867128ab":"code","7a05207b":"code","1f6f601c":"code","dbfc2648":"code","7d676084":"code","349c75b3":"code","ac7a5ec3":"code","0cb8dd44":"code","d14acbde":"code","8fa8f9a5":"code","4b1a3cc7":"code","72943a05":"code","6a0f9112":"code","658236bc":"code","7f40bd5d":"code","e501d759":"code","dc31993e":"code","c642a109":"code","22f9ee19":"code","5d446725":"code","0d60e56a":"code","e7fe5ff6":"code","489f7b1d":"code","0afa32a8":"code","141b3be4":"code","c9e27356":"code","0a2296d9":"code","2e726ba2":"code","a7c73e98":"code","6aa26c39":"code","323e729e":"markdown","06f58f09":"markdown","418236a5":"markdown","66e60346":"markdown","bb4f3a98":"markdown","85c0b5f8":"markdown","642a0fe8":"markdown"},"source":{"90d82dde":"import os\nimport gc\n\nimport math\n\nimport numpy as np\nimport pandas as pd\nfrom glob import glob\nfrom time import time\nfrom tqdm.auto import tqdm \n\nfrom PIL import Image\nimport Levenshtein\n\nfrom sklearn.model_selection import KFold, StratifiedKFold, GroupKFold\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\n\nAUTO = tf.data.experimental.AUTOTUNE \n\nprint(\"Tensorflow version \" + tf.__version__)","867128ab":"###### HYPERPARAMETERS ######\nSHARDS = 4\nNFOLDS = 4\n\nROOT_DIR = \"..\/input\/mlub-mongolian-car-plate-prediction\"\nPLATE_LEN = 7\nIMG_SIZE = 470","7a05207b":"df_train = pd.read_csv(os.path.join(ROOT_DIR, \"training.csv\"))\ndf_subm  = pd.read_csv(os.path.join(ROOT_DIR, \"submission.csv\"))\ndf_test  = df_subm.copy()\n    \nprint(df_train.shape, df_test.shape, df_subm.shape)","1f6f601c":"df_train_correct = pd.read_csv(\"..\/input\/mlub-train-data-correction\/mlub_train_correction.csv\")\n\ndf_train_correct[\"plate_number\"] = df_train_correct[\"correct_plate_number\"]\ndel df_train_correct[\"correct_plate_number\"]\n\ndf_train_1 = df_train[~df_train[\"file_name\"].isin(df_train_correct[\"file_name\"])]\ndf_train = pd.concat([df_train_1, df_train_correct]).sort_values(\"file_name\")\nprint(df_train.shape)\ndf_train.head()","dbfc2648":"# exclude 6digit plates\ndf_train = df_train[df_train.plate_number.apply(lambda x: len(x) == PLATE_LEN)]\ndf_train.shape","7d676084":"# separating the numbers and letters\nfor i in range(PLATE_LEN):\n    df_train[f\"plate_number_{i}\"] = df_train.plate_number.apply(\n        lambda x: \n        \"nan\" if len(x) != PLATE_LEN and i == PLATE_LEN - 1 else x[i]\n    )","349c75b3":"dict_labels = {\n    0: list(range(10)),\n    1: list(range(10)),\n    2: list(range(10)),\n    3: list(range(10)),\n    4: sorted(df_train[\"plate_number_4\"].unique().tolist()), # test \u0434\u043e\u0442\u043e\u0440 \u0431\u0430\u0439\u0441\u0430\u043d \u0442\u0443\u043b \u043d\u044d\u043c\u043b\u044d\u044d.\n    5: sorted(df_train[\"plate_number_5\"].unique().tolist()), # test \u0434\u043e\u0442\u043e\u0440 \u0431\u0430\u0439\u0441\u0430\u043d \u0442\u0443\u043b \u043d\u044d\u043c\u043b\u044d\u044d.\n    6: sorted(df_train[\"plate_number_6\"].unique().tolist()),\n}\n# checking\n# dict_labels[4]","ac7a5ec3":"print(\"Number of labels:\")\nfor key in dict_labels:\n    print(f\"{key} => {len(dict_labels[key])}\")","0cb8dd44":"# prepare the TRAIN input data\nfor key in dict_labels:\n    if key < 4:\n        df_train[f\"input_{key}\"] = df_train[f\"plate_number_{key}\"].astype(int)\n    else:\n        df_train[f\"input_{key}\"] = df_train[f\"plate_number_{key}\"].apply(lambda x: dict_labels[key].index(x))\ndf_train.head()","d14acbde":"skf = GroupKFold(n_splits=NFOLDS)\ndf_train[\"fold\"] = -1\nfor fold_num, (train_index, test_index) in enumerate(skf.split(df_train, df_train[\"plate_number\"], df_train[\"plate_number\"])):\n    print(\"TRAIN:\", train_index.shape, \"TEST:\", test_index.shape)\n    df_train[\"fold\"].iloc[test_index] = fold_num\n    \ndf_train.head()","8fa8f9a5":"# BAD_IDS_0 = [\"0667.png\", \"0597.png\", \"0038.png\", \"0378.png\"] # wrong labels\nBAD_IDS_1 = [\"2173.png\"]                                     # wrong label + russian\nBAD_IDS_2 = [\"1932.png\", \"3514.png\"]                             # wrong label + no number\n\n# BAD_IDS = BAD_IDS_0 + BAD_IDS_1 + BAD_IDS_2\nBAD_IDS = BAD_IDS_1 + BAD_IDS_2\nBAD_IDS","4b1a3cc7":"# filter data\ntrain_data = df_train[~df_train[\"file_name\"].isin(BAD_IDS)]\ntrain_data.shape[0]","72943a05":"train_data.head()","6a0f9112":"INPUT_COLS = [\"file_name\"] + [f\"input_{i}\" for i in range(PLATE_LEN)]\nTARGET_COL =  \"plate_number\"\nINPUT_COLS","658236bc":"dataset0 = {f\"fold_{fn}\": tf.data.Dataset.from_tensor_slices(\n    (\n        dict(train_data[train_data[\"fold\"] == fn][[\"file_name\",\"plate_number\"]]), \n        dict(train_data[train_data[\"fold\"] == fn][[f\"input_{i}\" for i in range(PLATE_LEN)]])\n    )\n) for fn in range(NFOLDS)}","7f40bd5d":"def decode_jpeg(data_dict, label):\n    fname = ROOT_DIR + \"\/training\/training\/\" + data_dict['file_name']\n    bits  = tf.io.read_file(fname)\n    data_dict['image'] = tf.image.decode_jpeg(bits)\n    return data_dict, label\n\ndef recompress_image(data, label):\n    data['image'] = tf.cast(data['image'], tf.uint8)\n    data['image'] = tf.image.encode_jpeg(data['image'], \n                                         #quality=100, # the default is 95% (the original images \n                                         # are already compressed, so no need to increase this \n                                         # value -- we can't create new information.)\n                                         optimize_size=True, \n                                         chroma_downsampling=False)\n    return data, label","e501d759":"dataset1 = {f\"fold_{fn}\": dataset0[f\"fold_{fn}\"].map(decode_jpeg, num_parallel_calls=AUTO) for fn in range(NFOLDS)}","dc31993e":"dataset2 = {f\"fold_{fn}\": dataset1[f\"fold_{fn}\"].map(recompress_image, num_parallel_calls=AUTO) for fn in range(NFOLDS)}","c642a109":"shard_size = {f\"fold_{fn}\": math.ceil(1.0 * train_data[train_data[\"fold\"] == fn].shape[0] \/ SHARDS) for fn in range(NFOLDS)}\nSHARDS, shard_size","22f9ee19":"dataset3 = {f\"fold_{fn}\": dataset2[f\"fold_{fn}\"].batch(shard_size[f\"fold_{fn}\"]) for fn in range(NFOLDS)}","5d446725":"def _bytestring_feature(list_of_bytestrings):\n    return tf.train.Feature(bytes_list=tf.train.BytesList(value=list_of_bytestrings))\n\ndef _int_feature(list_of_ints): # int64\n    return tf.train.Feature(int64_list=tf.train.Int64List(value=list_of_ints))\n\ndef _float_feature(list_of_floats): # float32\n    return tf.train.Feature(float_list=tf.train.FloatList(value=list_of_floats))","0d60e56a":"def to_tfrecord(image, file_name, plate_number, out0, out1, out2, out3, out4, out5, out6):\n\n    feature = {\n        \"image\":        _bytestring_feature([image]), \n        \"file_name\":    _bytestring_feature([file_name]),\n        \"plate_number\": _bytestring_feature([plate_number]),\n        \"out0\":         _int_feature([out0]),\n        \"out1\":         _int_feature([out1]),\n        \"out2\":         _int_feature([out2]),\n        \"out3\":         _int_feature([out3]),\n        \"out4\":         _int_feature([out4]),\n        \"out5\":         _int_feature([out5]),\n        \"out6\":         _int_feature([out6]),\n    }\n    \n    return tf.train.Example(features=tf.train.Features(feature=feature))","e7fe5ff6":"print(\"Writing TFRecords\")\n\n\nfor fn in range(NFOLDS):\n    print(f\"========= fold {fn} =========\")\n    for shard, (data, label) in enumerate(dataset3[f\"fold_{fn}\"]):\n        # batch size used as shard size here\n        shard_size = data['image'].numpy().shape[0]\n        # good practice to have the number of records in the filename\n        filename = \"mlub_ocr_train_fold-{:d}-{:02d}-{}.tfrec\".format(fn, shard, shard_size)\n\n        with tf.io.TFRecordWriter(filename) as out_file:\n            for i in range(shard_size):\n\n                example = to_tfrecord(\n                                      # re-compressed image: already a byte string\n                                      data['image'].numpy()[i],\n                                      data['file_name'].numpy()[i],\n                                      data['plate_number'].numpy()[i],\n                                      *[label[key].numpy()[i] for key in label.keys()]\n                                     )\n\n                out_file.write(example.SerializeToString())\n\n            print(\"Wrote file {} containing {} records\".format(filename, shard_size))","489f7b1d":"test_data = df_test.copy()\ntest_data = test_data.drop(columns=[\"plate_number\"])\ntest_data.head()","0afa32a8":"dataset0 = tf.data.Dataset.from_tensor_slices(\n        dict(test_data), \n)","141b3be4":"def decode_jpeg(data_dict):\n    fname = ROOT_DIR + \"\/test\/test\/\" + data_dict['file_name']\n    bits  = tf.io.read_file(fname)\n    data_dict['image'] = tf.image.decode_jpeg(bits)\n    return data_dict\n\ndef recompress_image(data):\n    data['image'] = tf.cast(data['image'], tf.uint8)\n    data['image'] = tf.image.encode_jpeg(data['image'], \n                                         #quality=100, # the default is 95% (the original images \n                                         # are already compressed, so no need to increase this \n                                         # value -- we can't create new information.)\n                                         optimize_size=True, \n                                         chroma_downsampling=False)\n    return data","c9e27356":"dataset1 = dataset0.map(decode_jpeg, num_parallel_calls=AUTO)\ndataset2 = dataset1.map(recompress_image, num_parallel_calls=AUTO)","0a2296d9":"shard_size = math.ceil(1.0 * test_data.shape[0] \/ SHARDS)\nSHARDS, shard_size","2e726ba2":"dataset3 = dataset2.batch(shard_size)","a7c73e98":"def to_tfrecord(image, file_name):\n\n    feature = {\n        \"image\":        _bytestring_feature([image]), \n        \"file_name\":    _bytestring_feature([file_name]),\n    }\n    \n    return tf.train.Example(features=tf.train.Features(feature=feature))","6aa26c39":"print(\"Writing TFRecords\")\n\nfor shard, data in enumerate(dataset3):\n    # batch size used as shard size here\n    shard_size = data['image'].numpy().shape[0]\n    # good practice to have the number of records in the filename\n    filename = \"mlub_ocr_test-{:02d}-{}.tfrec\".format(shard, shard_size)\n    \n    with tf.io.TFRecordWriter(filename) as out_file:\n        for i in range(shard_size):\n            \n            example = to_tfrecord(\n                                  # re-compressed image: already a byte string\n                                  data['image'].numpy()[i],\n                                  data['file_name'].numpy()[i],\n                                 )\n\n            out_file.write(example.SerializeToString())\n\n        print(\"Wrote file {} containing {} records\".format(filename, shard_size))","323e729e":"### Data Correction Replacement\n\n105  \u0448\u0438\u0440\u0445\u044d\u0433 \u0434\u0430\u0442\u0430\u0433 \u04e9\u043c\u043d\u04e9\u0445 \u0441\u0443\u0440\u0433\u0430\u0441\u0430\u043d \u043c\u043e\u0434\u0435\u043b\u043e\u043e \u0430\u0448\u0438\u0433\u043b\u0430\u0430\u0434 \u0434\u0430\u0445\u0438\u043d \u0437\u0430\u0441\u0441\u0430\u043d \u0431\u0430 \u0442\u04af\u04af\u043d\u0438\u0439\u0433\u044d\u044d CV \u0431\u043e\u0434\u043e\u0445\u0434\u043e\u043e \u0430\u0448\u0438\u0433\u043b\u0430\u0445 \u044e\u043c.","06f58f09":"## Group KFold","418236a5":"\u0414\u0430\u0442\u0430 \u0430\u043b\u0434\u0430\u0430\u0442\u0430\u0439 \u0431\u0430\u0439\u0433\u0430\u0430\u0433 [\u044d\u043d\u044d\u0445\u04af\u04af](https:\/\/www.kaggle.com\/c\/mlub-mongolian-car-plate-prediction\/discussion\/188989) discussion \u0434\u044d\u044d\u0440 \u044f\u0440\u044c\u0441\u0430\u043d \u0431\u0430\u0439\u0433\u0430\u0430.\n\nUPDATE: \u0411\u043e\u043b\u043e\u043c\u0436\u0438\u0442 wrong label case-\u04af\u04af\u0434 manual \u0437\u0430\u0441\u0430\u0430\u0434 \u0448\u0438\u043d\u044d\u044d\u0440 \u043e\u0440\u0443\u0443\u043b\u0441\u0430\u043d \u0442\u0443\u043b `BAD_IDS_0`-\u0438\u0439\u0433 \u0445\u0430\u0441\u043b\u0430\u0430.","66e60346":"## First Train TF records","bb4f3a98":"### Updates\n\n* [X] Removed data correction\n* [ ] ","85c0b5f8":"## Now Test TF records","642a0fe8":"```\n######## IN TEST ANALYZE: #######\n\n# Hard to tell cases:\n# 4731.png\n\n# Russian:\n# 6233.png\n\n# prob4\n# => \u0417 \u04e8 \u0434\u044d\u044d\u0440 \u0438\u0445 \u0430\u043b\u0434\u0441\u0430\u043d \u0431\u0430\u0439\u043d\u0430.\n# => \u0426 -\u0442\u044d\u0439  case: 4827.png\n\n# prob5\n# => \u041c-\u0442\u044d\u0439 case: 6715.png\n```"}}