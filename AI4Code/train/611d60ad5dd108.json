{"cell_type":{"bb11e70b":"code","0fe4bd59":"code","c6cdfc34":"code","c26e8286":"code","e3fa102b":"code","235f4fbc":"code","fbec0268":"code","56dcf165":"code","52da22ef":"code","c7a8e35c":"code","5721487f":"code","620d45b2":"code","954de74d":"code","3bfe9b93":"code","e4b60b97":"code","004cc18a":"code","04d198fc":"code","3908b6e2":"code","30465717":"code","24df8999":"code","7318120e":"markdown","db8e1071":"markdown","fd85153f":"markdown","a206e0c2":"markdown","106a0471":"markdown","977784f6":"markdown","597e8a22":"markdown"},"source":{"bb11e70b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0fe4bd59":"import numpy as np\nimport pandas as pd\npd.set_option('display.max_columns', None)\n#pd.set_option('display.max_rows', None)\npd.set_option('display.float_format', lambda x: '%.5f' % x)\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport warnings\nimport seaborn as sns\nfrom operator import attrgetter\nimport matplotlib.colors as mcolors\nimport datetime as dt\n\n\nimport warnings\n\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","c6cdfc34":"!pip install openpyxl","c26e8286":"df_ = pd.read_excel('..\/input\/2000rowonline2010\/online_retail2010_20000row.xlsx')","e3fa102b":"df = df_[df_['Country'] == 'France']\n\n# Due to big data  data reduced to include France only for elimination errors","235f4fbc":"df.shape[0]","fbec0268":"df.shape[1]","56dcf165":"df.head()","52da22ef":"# Product Matrix\n\ndef create_invoice_product_df(dataframe):\n    return dataframe.groupby(['Invoice', 'Description'])['Quantity'].sum().unstack().fillna(0). \\\n        applymap(lambda x: 1 if x > 0 else 0)","c7a8e35c":"fr_inv_pro_df = create_invoice_product_df(df)","5721487f":"fr_inv_pro_df.iloc[0:5, 0:5]","620d45b2":"fr_inv_pro_df = fr_inv_pro_df.iloc[:, 0:25]\n# data minimized for elimantion errors. ","954de74d":"fr_inv_pro_df.head(3)","3bfe9b93":"from mlxtend.frequent_patterns import apriori, association_rules","e4b60b97":"# Frequent_itemsets by using apriori and probability of products being in the same basket\n\nfrequent_itemsets = apriori(fr_inv_pro_df, min_support=0.01, use_colnames=True)   \n\n# use_colnames=True means show name of products under the itemsets column, default support is 0.01","004cc18a":"frequent_itemsets.iloc[:, 0:50]","04d198fc":"frequent_itemsets.sort_values(\"support\", ascending=False)","3908b6e2":"rules = association_rules(frequent_itemsets, metric=\"support\", min_threshold=0.01)\nrules.head()\n\n# Basicly, Association Rule Learning rules bases on possibility of products being in the same basket (support). Therefore, before creation of ARL rules\n# frequceny of items have been determinated (frequent_itemsets) and then rules have been revealed by using frequent_itemsets.","30465717":"rules.sort_values(\"confidence\", ascending=False).head()","24df8999":"rules.sort_values(\"conviction\").head()","7318120e":"## What will we do ?\n\n### The main issue for association rule learning is that having PRODUCT MATRIX. First of all dataframe has been reduced 'invoice', 'descpriction', and 'quantity' ( with gruopby) and then if an invoice has mentioned product number '1' assigned instead of quantity, if it hasn't number '0' assigned (with unstack and fillna commands).\n\n### Basicly the order is as below\n\n### Create product matrix\n### Create frequent_itemsets by using apriori method\n### Create association rules including antecedents\tconsequents\tantecedent support\tconsequent support\tsupport\tconfidence\tlift\tleverage\tconviction\n### interpret the results correctly and make decisions","db8e1071":"### Lift : When Purchasing Product X, purchasing Y Increases Lift times. It is also the independence measure of two products.\n### Leverage : Computes the difference between the observed frequency of A and C appearing together and the frequency that would be expected if A and C were independent.\n### A leverage value of 0 indicates independence.","fd85153f":"### Support Value : Possibility Of Products To Be Seen With Each Other\n### Confidence Value: Possibility Of Products To Be Sold Together","a206e0c2":"## Some Inferences\n\n*  For low frequency products LIFT, for high frequency products LEVERAGE could be considered to understand relation of between two products.\n*  CONFIDENCE can't be more than 1. \n*  For independent items CONFIDENCE is 1.\n*  For dependent items CONVICTION is inf.","106a0471":"![asdas.png](attachment:719e7e9f-f825-48fc-a0b1-f3520b859d0e.png)","977784f6":"## METRICS","597e8a22":"### Both LIFT and LEVERAGE measure the relation between the probability of a given rule to occur (support(A\u2192C)) and its expected probability if the items were independent (support(A)*support(C)) of each other. \n\n### The only difference is that lift computes the ratio of both factors (support(A\u2192C)\/(support(A)*support(C))) and leverage computes the difference (support(A\u2192C)-support(A)*support(C))). The implications are that lift may find very strong associations for less frequent items, while leverage tends to prioritize items with higher frequencies\/support in the dataset.\n\n\n\n### Example for first row = 0.09091 \/ (0.18182 * 0.09091) = 5.50000 ( LIFT ) \n###                         0.09091- (0.18182 * 0.09091) = 0.07438 ( LEVERAGE)\n\n\n##  The implications are that LIFT may find very strong associations for less frequent items, while LEVERAGE tends to prioritize items with higher frequencies\/support in the dataset.\n\n\n### Conviction = A high conviction value means that the consequent is highly depending on the antecedent. For instance, in the case of a perfect confidence score, the denominator becomes 0 (due to 1 - 1) for which the conviction score is defined as 'inf'. Similar to lift, if items are independent, the conviction is 1.\n### Example for conviction = (1 - 0.09091) \/ (1 - 0.50000 ) = 1.81818 ( CONVICTION )"}}