{"cell_type":{"55d73745":"code","c4c50a02":"code","79bb1671":"code","1ece1043":"code","1a7641be":"code","e96b92a7":"code","f8047586":"code","100b5560":"code","cbf1b2e2":"code","3db93a48":"code","faf44cd8":"code","a514dcfe":"code","e0d9e3ff":"code","096658ae":"code","8ebdbf36":"code","0c10530c":"code","2032a1af":"code","e19a2133":"code","af443660":"code","4de4ec78":"code","ab35dee5":"code","a68e7fe1":"code","6e13b29e":"code","2af3e99e":"code","2c0d1f43":"code","e11d96ee":"code","ed028210":"code","6b3803d9":"code","b5935a0d":"code","f6c9feac":"code","d9302b3f":"code","0954e9a8":"code","598f9fb0":"code","7373c0c0":"code","42f5b4c9":"code","48085f14":"code","1518dcfd":"code","09dc2579":"code","189da46e":"code","807a7a29":"code","4813b135":"code","4473b58e":"code","3ec36f44":"code","d00210da":"code","4c58a6ec":"code","c777a9ee":"code","a7c3bfb0":"code","9a67a29c":"code","31459219":"code","e359bf7e":"code","0a072106":"code","57311cda":"code","19f3341d":"code","69680919":"code","a4e2dbc3":"code","60c466d6":"code","9ababa6d":"markdown","d0bb68f4":"markdown","8b65d150":"markdown","ded19470":"markdown","fddaa1a3":"markdown","35d60406":"markdown","b79e1b6f":"markdown","10a6dfcc":"markdown","8cc49af2":"markdown","ea513d7a":"markdown","4f746a53":"markdown","87f5667c":"markdown","dc47f7aa":"markdown","21b158f5":"markdown","e188c3fc":"markdown","32197127":"markdown","d9bcf44e":"markdown","83dbf03a":"markdown","39c94b9f":"markdown","61edae53":"markdown","0be26c34":"markdown","60048914":"markdown","76acf03b":"markdown","c2b56ffd":"markdown"},"source":{"55d73745":"# Importation des librairies n\u00e9cessaires et visualisation des fichiers disponibles\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nimport sklearn.metrics as metrics\nimport math\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=DeprecationWarning)\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","c4c50a02":"sample_submission = pd.read_csv(\"..\/input\/home-data-for-ml-course\/sample_submission.csv\")\ntest = pd.read_csv(\"..\/input\/home-data-for-ml-course\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/home-data-for-ml-course\/train.csv\")\n\n#cr\u00e9ation d'une copie de chaque dataset\ntest_copy  = test.copy()\ntrain_copy  = train.copy()","79bb1671":"test_copy.head()","1ece1043":"train_copy.head()","1a7641be":"len(train_copy.columns)","e96b92a7":"train_copy['train']  = 1\ntest_copy['train']  = 0\ndata_full = pd.concat([train_copy, test_copy], axis=0,sort=False)","f8047586":"data_full.describe()","100b5560":"data_full.info()","cbf1b2e2":"df_NULL = [(c, data_full[c].isna().mean()*100) for c in data_full]\ndf_NULL = pd.DataFrame(df_NULL, columns=[\"Colonne\", \"Taux de NULL\"])\ndf_NULL.sort_values(\"Taux de NULL\", ascending=False)","3db93a48":"# Variables avec plus de 50% de NULL\ndf_NULL = df_NULL[df_NULL[\"Taux de NULL\"] > 80]\ndf_NULL.sort_values(\"Taux de NULL\", ascending=False)","faf44cd8":"list_NULL_features = list(df_NULL.Colonne)\ndata_full = data_full.drop(list_NULL_features,axis=1)","a514dcfe":"categorical_features = data_full.select_dtypes(include=['object'])\nnumerical_features = data_full.select_dtypes(exclude=['object'])","e0d9e3ff":"# Variables num\u00e9riques :\nprint(\"Nombre de variables num\u00e9riques :\",numerical_features.shape[1])\nprint(\"\\nNombre de valeurs nulles :\\n\",numerical_features.isnull().sum())","096658ae":"# Variables cat\u00e9goriques :\nprint(\"Nombre de variables num\u00e9riques :\",categorical_features.shape[1])\nprint(\"\\nNombre de valeurs nulles :\\n\",categorical_features.isnull().sum())","8ebdbf36":"fill_None = ['BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','GarageType','GarageFinish','GarageQual','FireplaceQu','GarageCond']\ncategorical_features[fill_None]= categorical_features[fill_None].fillna('None')","0c10530c":"fill_other = ['MSZoning','Utilities','Exterior1st','Exterior2nd','MasVnrType','Electrical','KitchenQual','Functional','SaleType']\ncategorical_features[fill_other] = categorical_features[fill_other].fillna(categorical_features.mode().iloc[0])","2032a1af":"categorical_features.info()","e19a2133":"print(\"M\u00e9diane GarageYrBlt :\",numerical_features['GarageYrBlt'].median())\nprint(\"LotFrontage :\",numerical_features[\"LotFrontage\"].median())","af443660":"numerical_features['GarageYrBlt'] = numerical_features['GarageYrBlt'].fillna(numerical_features['GarageYrBlt'].median())\nnumerical_features['LotFrontage'] = numerical_features['LotFrontage'].fillna(numerical_features['LotFrontage'].median())","4de4ec78":"numerical_features = numerical_features.fillna(0)\nnumerical_features.info()","ab35dee5":"for col in categorical_features.columns:\n    #Conversion du type de variable en variable cat\u00e9gorique\n    categorical_features[col] = categorical_features[col].astype('category')\n    categorical_features[col] = categorical_features[col].cat.codes\ncategorical_features.head()","a68e7fe1":"df_final = pd.concat([numerical_features,categorical_features], axis=1,sort=False)\nfinal_train = df_final[df_final['train'] == 1]\nfinal_train = final_train.drop(['train',],axis=1)\n\nfinal_test = df_final[df_final['train'] == 0]\nfinal_test = final_test.drop(['SalePrice'],axis=1)\nfinal_test = final_test.drop(['train',],axis=1)","6e13b29e":"final_train = final_train.drop([\"Id\"],axis=1)","2af3e99e":"corr_train = final_train.corr()","2c0d1f43":"# Masque pour la partie haute du Heatmap\nmask = np.triu(np.ones_like(corr_train, dtype=bool))\n\n# Cr\u00e9ation de la heatmap Seaborn\nfig, ax = plt.subplots(figsize=(20, 20))\nsns.heatmap(corr_train, mask=mask, cmap=\"coolwarm\", vmax=.3, center=0,\n            square=True, linewidths=.5, cbar_kws={\"shrink\": .5})\nax.set_title(\"Matrice des corr\u00e9lations du jeu de donn\u00e9es Train\", fontsize=22)\nplt.show()","e11d96ee":"nocorr_features = list(corr_train[corr_train['SalePrice']<0.2].index)\nnocorr_features","ed028210":"final_train = final_train.drop(nocorr_features, axis=1)\nfinal_train.head()","6b3803d9":"rl_features = list(corr_train[corr_train['SalePrice']>0.3].index)\nrl_features.remove(\"SalePrice\")\nrl_features","b5935a0d":"Y_train = final_train[\"SalePrice\"]\nX_train = final_train.drop([\"SalePrice\"],axis=1)\nX_train = X_train[rl_features]","f6c9feac":"from sklearn.model_selection import train_test_split\nX_train_rl, X_test_rl, y_train_rl, y_test_rl = train_test_split(X_train, Y_train, test_size=0.3, random_state=1)","d9302b3f":"y_train_rl = y_train_rl.values.reshape(-1,1)\ny_test_rl = y_test_rl.values.reshape(-1,1)","0954e9a8":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_train_rl = sc.fit_transform(X_train_rl)\nX_test_rl = sc.fit_transform(X_test_rl)\ny_train_rl = sc.fit_transform(y_train_rl)\ny_test_rl = sc.fit_transform(y_test_rl)","598f9fb0":"from sklearn.linear_model import LinearRegression\nlm = LinearRegression()\nlm.fit(X_train_rl,y_train_rl)","7373c0c0":"print(\"Intercept :\",lm.intercept_)\nprint(\"Coefficients :\",lm.coef_)\nprint(\"R\u00b2 du mod\u00e8le :\",round(lm.score(X_train_rl,y_train_rl),2))","42f5b4c9":"pred_rl = lm.predict(X_test_rl)\npred_rl = pred_rl.reshape(-1,1)","48085f14":"fig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(1, 1, 1)\nax.scatter(y_test_rl, pred_rl)\nax.plot([y_test_rl.min(), y_test_rl.max()], [y_test_rl.min(), y_test_rl.max()], color='r')\nax.set(xlabel='y_test', ylabel='y_pred')\nplt.title(\"Projection des pr\u00e9dictions en fonction des valeurs r\u00e9elles\", fontsize=20)\nplt.show()","1518dcfd":"#Fonction de calculs des metriques importantes MAE, MSE, MAPE, RMSE\ndef metrics_timeseries(y_true, y_pred):\n    y_true, y_pred = np.array(y_true), np.array(y_pred)\n    diff = y_true - y_pred\n    mae = np.mean(abs(diff))\n    mse = np.mean(diff**2)\n    rmse = np.sqrt(mse)\n    mape = np.mean(np.abs(diff \/ y_true)) * 100\n    dict_metrics = {\"M\u00e9trique\":[\"MAE\", \"MSE\", \"RMSE\", \"MAPE\"], \"R\u00e9sultats\":[mae, mse, rmse, mape]}\n    df_metrics = pd.DataFrame(dict_metrics)\n    return df_metrics","09dc2579":"metrics_rl = metrics_timeseries(y_test_rl, pred_rl)\nmetrics_rl","189da46e":"Y_rf = final_train[\"SalePrice\"]\nX_rf = final_train.drop([\"SalePrice\"],axis=1)","807a7a29":"from sklearn.feature_selection import RFECV\nfrom sklearn.ensemble import RandomForestRegressor\nselector = RFECV(RandomForestRegressor(), min_features_to_select=5, step=1, cv=5)\nselector.fit(X_rf,Y_rf)","4813b135":"selector.grid_scores_","4473b58e":"selector.ranking_","3ec36f44":"best_features_rf = list(np.array(X_rf.columns)[selector.support_])\nbest_features_rf","d00210da":"X_rf = X_rf[best_features_rf]","4c58a6ec":"X_train_rf, X_test_rf, y_train_rf, y_test_rf = train_test_split(X_rf, Y_rf, test_size=0.3, random_state=1)\ny_train_rf = y_train_rf.values.reshape(-1,1)\ny_test_rf = y_test_rf.values.reshape(-1,1)","c777a9ee":"from sklearn.model_selection import GridSearchCV\nwarnings.filterwarnings('ignore')\n\nparam_grid_rf = { 'n_estimators' : [10,50,100,150,200], 'max_features' : ['auto', 'sqrt']}\ngrid_search_rf = GridSearchCV(RandomForestRegressor(), param_grid_rf, cv=5)\ngrid_search_rf.fit(X_train_rf, y_train_rf)","a7c3bfb0":"print (\"Score final : \", round(grid_search_rf.score(X_train_rf, y_train_rf) *100,4), \" %\")\nprint (\"Meilleurs parametres: \", grid_search_rf.best_params_)\nprint (\"Meilleure config: \", grid_search_rf.best_estimator_)","9a67a29c":"rf =  RandomForestRegressor(max_features='sqrt', n_estimators=150)\nrf.fit(X_train_rf, y_train_rf)","31459219":"pred_rf = rf.predict(X_test_rf)\npred_rf = pred_rf.reshape(-1,1)","e359bf7e":"fig = plt.figure(figsize=(12,8))\nax = fig.add_subplot(1, 1, 1)\nax.scatter(y_test_rf, pred_rf)\nax.plot([y_test_rf.min(), y_test_rf.max()], [y_test_rf.min(), y_test_rf.max()], color='r')\nax.set(xlabel='y_test', ylabel='y_pred')\nplt.title(\"Projection des pr\u00e9dictions en fonction des valeurs r\u00e9elles\", fontsize=20)\nplt.show()","0a072106":"metrics_rf = metrics_timeseries(y_test_rf, pred_rf)\nmetrics_rf","57311cda":"# Afin de comparer les m\u00e9triques, on inverse la standardisation de la r\u00e9gression lin\u00e9aire\nmetrics_rl_i = metrics_timeseries(sc.inverse_transform(y_test_rl), sc.inverse_transform(pred_rl))\nmetrics_rl_i","19f3341d":"id_test = final_test[\"Id\"]\nX_pred_test = final_test[best_features_rf]","69680919":"pred_rf = rf.predict(X_pred_test)\npred_rf = pred_rf.reshape(-1,1)\npred_rf","a4e2dbc3":"df_submission = pd.concat([id_test,pd.Series(pred_rf[:,0])],axis=1).rename(columns={0:\"SalePrice\"})\ndf_submission","60c466d6":"df_submission.to_csv('submission.csv', index=False)","9ababa6d":"Apr\u00e8s consultation de la description des fichiers de donn\u00e9es, nous allons compl\u00e9ter les valeurs nulles des categoricla features ainsi :\n* BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1, BsmtFinType2, GarageType, GarageFinish, GarageQual, FireplaceQu, GarageCond seront compl\u00e9t\u00e9s avec la valeur \"None\",\n* les autres variables avec leur propre valeur la plus fr\u00e9quente.","d0bb68f4":"### <font color=\"blue\">Standardisation des donn\u00e9es<\/font>","8b65d150":"# <font color=\"purple\">Concours des prix des logements - Machine Learning<\/font>\n\n![housesbanner.png](attachment:housesbanner.png)\n\n> Demandez \u00e0 un acheteur de d\u00e9crire la maison de ses r\u00eaves, et il ne commencera probablement pas par la hauteur du plafond du sous-sol ou la proximit\u00e9 d'un chemin de fer est-ouest ... \n\nAvec **79 variables explicatives** d\u00e9crivant *(presque)* tous les aspects des maisons r\u00e9sidentielles \u00e0 Ames, Iowa, le d\u00e9fi de ce concours est de pr\u00e9dire le prix final de chaque maison.","ded19470":"Les m\u00e9triques du mod\u00e8le RandomForest sont donc meilleures que celles de la r\u00e9gression lin\u00e9aire.    \n**Nous allons donc utiliser ce dernier mod\u00e8le pour notre pr\u00e9diction sur le fichier sample_submission fourni**.\n\n## <font color=\"orange\">Pr\u00e9diction du prix des maison du fichier sample_submission<\/font>","fddaa1a3":"Pour la suite du process, nous allons combiner les 2 dataframes *(categorical et numerical features)*, puis les spliter \u00e0 nouveau en Train et Test","35d60406":"## <font color=\"orange\">Features selection - Analyse des correlations<\/font>\n\nNous allons \u00e0 pr\u00e9sent pouvoir, sur le jeu Train, analyser les variables les plus corr\u00e9l\u00e9es \u00e0 SalePrice et r\u00e9aliser une feature selection pour ne garder que les meilleures :","b79e1b6f":"Il nous reste \u00e0 pr\u00e9sent 31 variables dans notre dataset final. \n\n## <font color=\"orange\">Mod\u00e8lisation par r\u00e9gression lin\u00e9aire<\/font>\n\nPour nos mod\u00e8les de r\u00e9gression lin\u00e9aire, nous allons prendre en compte cette fois les variables avec les meilleurs coeffiscients de corr\u00e9lation lin\u00e9aires.","10a6dfcc":"Notre premier mod\u00e8le de r\u00e9gression lin\u00e9aire nous ayant fourni les premi\u00e8res m\u00e9triques, **nous allons pouvoir tester d'autres mod\u00e9lisations telles que RandomForest** et comparer les r\u00e9sultats obtenus.\n\n## <font color=\"orange\">Mod\u00e8lisation par Random Forest<\/font>\n\nNous allons travailler sur les 31 features s\u00e9lectionn\u00e9es auparavant en repartant de notre dataframe final_train et r\u00e9aliser une s\u00e9lection de variable via un transformer recursif :","8cc49af2":"Comme on le constate, il y a 81 colonnes dans notre dataset, donc bien **79 variables** *(features)*, 1 colonne ID et notre variable cible qui sera ici **\"SalePrice\"**.\n\nLa variable ID pourrait \u00eatre supprim\u00e9e mais nous en aurons besoin pour soumettre les r\u00e9sultats. Nous allons \u00e0 pr\u00e9sent regrouper nos 2 datasets train et test *(en ajoutant une variable pour les identifier ensuite)* et faire une rapide description des donn\u00e9es :","ea513d7a":"Nous avons \u00e0 pr\u00e9sent d\u00e9fini les meilleurs variables pour notre RandomForest, nous allons \u00e0 pr\u00e9sent tester it\u00e9rativement les param\u00e8tres importants de notre mod\u00e8le pour d\u00e9finir les meilleurs :","4f746a53":"A pr\u00e9sent, les donn\u00e9es ont \u00e9t\u00e9 compl\u00e9t\u00e9es, les variables ne comportes plus de null et pourront donc \u00eatre utilis\u00e9es pour nos mod\u00e9lisations.\n\nNous allons \u00e0 pr\u00e9sent **convertir nos categorical features en donn\u00e9es num\u00e9riques** pour pouvoir les utiliser dans nos mod\u00e8les puis r\u00e9aliser une feature selection avec Scikit Learn.","87f5667c":"Notre meilleur mod\u00e8le \u00e0 pr\u00e9sent d\u00e9fini, nous allons mod\u00e9liser avec ces param\u00e8tres et \u00e9diter les m\u00e9triques :","dc47f7aa":"### <font color=\"blue\">Features engineering<\/font>\n\nComme nous l'avons constat\u00e9 gr\u00e2ce \u00e0 la fonction .info(), nous avons dans notre dataset **un certain nombre de variables cat\u00e9goriques *(categorical features)***.    \nCes variables ne pourrons pas \u00eatre trait\u00e9es par notre mod\u00e8le de pr\u00e9diction, il va donc falloir les traiter diff\u00e9rement.\n\nR\u00e9alisons cette s\u00e9paration et comptons le nombre de Null de chaque variable.","21b158f5":"### <font color=\"blue\">S\u00e9lection des variables *(RFECV)*<\/font>","e188c3fc":"## <font color=\"orange\">Preprocessing - Pr\u00e9paration des donn\u00e9es<\/font>\n\nImportation des fichiers **Train** et **Test** de l'exercice :","32197127":"Visualisation des premieres lignes :","d9bcf44e":"Utilisation de GridSearchCV pour s\u00e9lectionner les meilleurs param\u00e8tres :","83dbf03a":"On voit donc \u00e0 pr\u00e9sent que les **categorical features ne comptent plus de valeurs nulles**.    \nNous allons pouvoir nous occuper des variables num\u00e9riques :\n\nLes numerical features *(hors SalePrice)* avec le plus grand nombre de NULL sont les suivantes :\n* LotFrontage\n* GarageYrBlt\n\nNous allons utiliser **la m\u00e9diane de ces variables** pour compl\u00e9ter les valeurs nulles. Pour les autres variables, les valeurs nulles seront compl\u00e9t\u00e9es \u00e0 0.","39c94b9f":"### <font color=\"blue\">Premi\u00e8re r\u00e9gression lin\u00e9aire<\/font>","61edae53":"Nous pouvons visualiser les variables avec un **coefficient de corr\u00e9lation lin\u00e9aire \u00e0 SalePrice non significatif**, nous pourrons \u00e9liminer ces variables ensuite :","0be26c34":"On remarque d\u00e9j\u00e0 que certaines variables comptent un grand nombre de NULL (comme Fence, PoolQC, MiscFeature ...). Nous allons **calculer le pourcentage de valeurs manquantes pour chaque variable**.","60048914":"Nous allons donc supprimer ces 4 variables qui comptent plus de 80% de valeurs nulles et qui ne seront donc pas repr\u00e9sentatives.","76acf03b":"### <font color=\"blue\">Split des donn\u00e9es<\/font>","c2b56ffd":"### <font color=\"blue\">Calcul des m\u00e9triques de performances<\/font>"}}