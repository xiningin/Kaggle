{"cell_type":{"bf46090a":"code","441f9ceb":"code","89ff81f8":"code","f21af7c9":"code","e694b7a9":"code","ef906685":"code","bb8aef2c":"code","4a91707d":"code","d437ace7":"code","cb0d108e":"code","0a959760":"code","f203d334":"code","d572fcdb":"markdown","fef48d8c":"markdown","07c5780c":"markdown","501a71a7":"markdown","22a11f73":"markdown","5550f339":"markdown","c24f0c88":"markdown","c21d7b39":"markdown","fef7d140":"markdown","9f79d43f":"markdown"},"source":{"bf46090a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n\n","441f9ceb":"df = pd.read_csv('..\/input\/predict-diabetes-based-on-diagnostic-measures\/diabetes.csv')\ndf.head()","89ff81f8":"df[\"chol_hdl_ratio\"] = df[\"chol_hdl_ratio\"].str.replace(\",\",\".\").astype(float)\ndf[\"bmi\"] = df[\"bmi\"].str.replace(\",\",\".\").astype(float)\ndf[\"waist_hip_ratio\"] = df[\"waist_hip_ratio\"].str.replace(\",\",\".\").astype(float)","f21af7c9":"sns.pairplot(df, hue = \"gender\")","e694b7a9":"x = df.iloc[:, 1:-1].values\ny = df.iloc[:, -1].values\n\n","ef906685":"from sklearn import preprocessing\nlbl_x = preprocessing.LabelEncoder()\nlbl_y = preprocessing.LabelEncoder()\nx[:,5] = lbl_x.fit_transform(x[:,5])\ny = lbl_y.fit_transform(y)","bb8aef2c":"x\nimport statsmodels.api as sm\ndef backwardElimination(x, sl):    \n    numVars = len(x[0])    \n    for i in range(0, numVars):        \n        regressor_OLS = sm.OLS(y, x.tolist()).fit()        \n        maxVar = max(regressor_OLS.pvalues).astype(float)        \n        if maxVar > sl:            \n            for j in range(0, numVars - i):                \n                if (regressor_OLS.pvalues[j].astype(float) == maxVar):                    \n                    x = np.delete(x, j, 1)    \n    regressor_OLS.summary()    \n    return x \nSL = 0.05\nX_opt = x[:, 0:13]\nX_Modeled = backwardElimination(X_opt, SL)","4a91707d":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state = 5)","d437ace7":"from sklearn.preprocessing import StandardScaler\nsc_x = StandardScaler()\nx_train = sc_x.fit_transform(x_train)\nx_test = sc_x.transform(x_test)","cb0d108e":"from sklearn.linear_model import LogisticRegression\nregressor = LogisticRegression().fit(x_train, y_train)","0a959760":"from sklearn.metrics import accuracy_score\ny_pred = regressor.predict(x_test)\naccuracy_score(y_test, y_pred)\n","f203d334":"from xgboost import XGBClassifier\nxgbc = XGBClassifier(use_label_encoder =False, eval_metric='mlogloss').fit(x_train, y_train)\ny_pred2 = xgbc.predict(x_test)\naccuracy_score(y_test, y_pred2)","d572fcdb":"Cambiaremos aquellas variables categoricas a numericas usando sklearn","fef48d8c":"# Usando la eliminaci\u00f3n hacia atras para eliminar variables que no tienen tanta influencia en el modelo","07c5780c":"Normalizando las variables","501a71a7":"# Conclusiones y Curiosidades","22a11f73":"Tenemos que reconocer que dentro de este desarrollo se encontro que:\n\n**1** Si normalizamos X en vez de normalizar x_train y x_test podemos observar que la eliminaci\u00f3n hacia atras solo nos dejara 1 variable la cual lograra igualmente predecir casi tan bien como lo hacen las 5 variables seleccionadas (como podemos observar en la siguiente imagen)\n\n![image.png](attachment:85660b42-7e25-4a82-8b7b-6592fe1896fb.png)\n\n**2** Es de tener en cuenta que debido al tama\u00f1o tan peque\u00f1o de este dataset puede existir mucha varianza respecto a unas predicciones m\u00e1s grandes\n\n**3** Se puede tener en cuenta que solo analizando \"glucose\", \"age\", \"waist\", \"hip\", \"waist_hip_ratio\" tendremos muy buenos resultados\n\n**4** En la prueba con diferentes modelos podemos observar que su porcentaje de precisi\u00f3n varia entre un 4% y un 8% (Como se puede apreciar en la imagen)\n![image.png](attachment:2fb223cf-3644-42ba-8783-2303dec2c5fe.png)\n","5550f339":"Limpiando los datos","c24f0c88":"**XGBOOST**","c21d7b39":"**Regresion Logistica**","fef7d140":"# Cargando los datos","9f79d43f":"# Probamos la precisi\u00f3n de los datos despues de que hemos eliminado las variables que estadisticamente hablando no aportan mucho"}}