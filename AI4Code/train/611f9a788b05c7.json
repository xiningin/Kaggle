{"cell_type":{"a8c2066c":"code","8dc0495e":"code","07cc8526":"code","6d711d4b":"code","c037dda1":"code","ecd798c0":"code","c1b92218":"code","22f3906f":"code","58c96624":"markdown","6038b0f4":"markdown","8192f55c":"markdown","bf5b5394":"markdown","82d6c943":"markdown","86657c76":"markdown"},"source":{"a8c2066c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","8dc0495e":"df = pd.read_csv('..\/input\/wine-quality-dataset\/WineQT.csv')","07cc8526":"df.head()\ndf.dropna(inplace=True)\ndf.drop(columns=['Id'], inplace=True)","6d711d4b":"df.info()","c037dda1":"plt.figure(figsize=(20,20))\nsns.heatmap(df.corr(), annot=True)","ecd798c0":"df.head()","c1b92218":"class QualityDist():\n    def __init__(self, df):\n        self.df = df\n        self.quality = list(set(df[\"quality\"].values.tolist()))\n    def plotKDE(self):\n        columns = self.df.columns.tolist()\n        columns = [c for c in columns if c != 'quality']\n        \n        fig, ax = plt.subplots(len(columns),len(self.quality),figsize = (20,20))\n        \n        for i,q in enumerate(self.quality):\n            qData = self.df[self.df[\"quality\"] == q ]\n            for j, c in enumerate(columns):\n                sns.histplot(data = qData, x = c, kde = True, ax = ax[j,i], color='firebrick').set(title = f\"Quality {q}\")\n        \n        fig.tight_layout()\n        plt.ticklabel_format(style=\"plain\")\n        plt.show()","22f3906f":"QualityDist(df).plotKDE()","58c96624":"# Regression Mathematics","6038b0f4":"## This notbeook is under construction and will take time since I am working full time and studying full time as well.","8192f55c":"So within regression modelling the first fundamental equation is the classic independent and dependent variable model. It all starts from here!\n\n<center>$(1)$ $\\hat{y}$ = $\\sum_{i=1}^{n} x_i \\times w_i + x_0 $ <\/center>\n\nthe $\\hat{y}$ is the predictor and $y$ is the orginal target. $x_0$ is some constant value which is ideally a Real number. The aim of the game is to minimise the error between $\\hat{y}$ and $y$. \n\nWell one way we can aim to do this is by stating a rule. This rule is widely used in ML which aims at minimising error squared. As a rule if the Summation Operator is used, it essentially means we are adding each operation from 1 to n. For example:\n\n$\\sum_{j=1}^{4} (x_j)^{2} $ = $x_1^{2}+x_2^{2}+x_3^{2}+x_4^{2}$ now if $j$ was from 1 to n then it will be $x_1^{2}+x_2^{2}+x_3^{2}+x_4^{2}+x_5^{2}+...+x_n^{2}$.  Okay got it!! if not just think of it as a for loop doing a summation operation till we reach the nth interation. If you still don't get it just google it, okay back to the error function now. \n\nThe error function is the simplest way we can *constrain* a model to follow some rule of deciding if it is effective or not. Hence, we are learning something about our data in a structured way.\n\nHow cool is that, such a simple concept but it is so powerful. \n\n<center> $error$ = $\\sum_{j=1}^{m} (y_{j} - \\hat{y}_{j})^{2} $ <\/center>\n\nDoes this remind you of calculus? it should! Within, fundamental calculus the first thing we learn is this poweful equation: \n<center> $d$ = $\\sqrt{(x_2-x_1)^2 + (y_2-y_1)^2}$ <\/center>\n\nThe concept of distance between points explains how well our predicted model has done, for regression ML this equation is quite important.\n\nLets try to visualise the error.\n![image.png](attachment:929cdd20-2006-46c4-b633-ef79725b8465.png)\n\nAs we can see in this image, error is the distance the X input has from the taregt Y. A natural question which come to the curios mind is how does my predicted model fit my target data? and to answer this we are going to look into MSE or Mean Squared Errors! A term which is so important that every kaggle comp uses it but I doubt if most people know how it works.","bf5b5394":"1. The data looks better distributed for Quality 5,6 and 7 wines.\n2. Quality 3, 6 and 8 are poorly distributed.\n3. More alchol with low level of acidity leads to better quality wine, Nice!","82d6c943":"* The Quality is slightly positvely correlated to the amount of alcohol present in the wine\n* The Quality is slightly negatively correlated to the amount of the the volatile acidity\n* The level of pH has no correlation with the quality of the wine\n* It seems that for some interesting ML work the quality can be used as the target (dependent)","86657c76":"# Wine and Regress\n\nThe aim of this notebook is to explore the mathematics behind the first step in supervised machine learning which is Regression Modelling.\n\nI want to take the oportunity and challenge myself and do machine learning without the help of sckit-learn or any other ML library, which means I have to manually derive algorithms which should be interesting. \n\nI really hope people will be inspired by this and try to explore other ML algorithms from scratch.\n"}}