{"cell_type":{"aa110e69":"code","fc040fe8":"code","3417a4cb":"code","9a2c0da0":"code","795e9d82":"code","acacd448":"code","66687583":"code","beb00760":"code","6d0e40c0":"code","9b6fdebf":"code","8861f040":"code","b0d36472":"code","2bdec552":"code","82281934":"code","44e967fa":"code","6e790cdb":"code","f57e441e":"code","6ac153b5":"code","714439d2":"code","0f110e99":"code","8c0fbe9a":"code","ac77d6c9":"code","d3a6b74e":"markdown","a48f75f5":"markdown"},"source":{"aa110e69":"import numpy as np \nimport pandas as pd \nimport scipy\nimport nltk\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import decomposition\nfrom scipy import linalg\nfrom gensim.models.doc2vec import Doc2Vec, TaggedDocument\nfrom nltk.tokenize import word_tokenize\nnp.set_printoptions(suppress=True)\nfrom gensim.models.doc2vec import Doc2Vec ","fc040fe8":"#reading the data\ndf=pd.read_csv(\"..\/input\/CORD-19-research-challenge\/metadata.csv\",engine='python',error_bad_lines=False) ","3417a4cb":"#observing the first 5 rows\ndf.head()","9a2c0da0":"#I am interested in the abstract section\ndf['abstract'] #512397 abstracts","795e9d82":"#replacing NaN values\ndf['abstract'].fillna('null',inplace=True)","acacd448":"# storing the abstracts in 'data'\ndata=np.array(df['abstract'])\nlen(data)","66687583":"nltk.download('wordnet')","beb00760":"#using CountVectoriser to convert text into matrix form \nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nvectorizer = CountVectorizer(stop_words='english',max_features=10000) # removing stop words\nvectors = vectorizer.fit_transform(data).todense() #converting into a dense vector","6d0e40c0":"# finding the vocab\nvocab = np.array(vectorizer.get_feature_names())\nprint(vocab.shape)","9b6fdebf":"# show topics method to \nnum_top_words=10\n\ndef show_topics(a):\n    top_words = lambda t: [vocab[i] for i in np.argsort(t)[:-num_top_words-1:-1]]\n    topic_words = ([top_words(t) for t in a])\n    return [' '.join(t) for t in topic_words]","8861f040":"m,n=vectors.shape\nd=10 # setting the number of topics to 10","b0d36472":"clf = decomposition.NMF(n_components=d, random_state=1)\nW1 = clf.fit_transform(vectors)\nH1 = clf.components_","2bdec552":"show_topics(H1)","82281934":"#converting data into list format\nlist_data=data.tolist()","44e967fa":"tagged_data = [TaggedDocument(words=word_tokenize(_d.lower()), tags=[str(i)]) for i, _d in enumerate(list_data)]","6e790cdb":"nltk.download('punkt')","f57e441e":"#training the model on the corpus\n#applying Doc2Vec to convert document into its vector form. Reference: https:\/\/medium.com\/@mishra.thedeepak\/doc2vec-simple-implementation-example-df2afbbfbad5\nmax_epochs = 50\nvec_size = 20\nalpha = 0.025\n\nmodel = Doc2Vec(size=vec_size,\n                alpha=alpha, \n                min_alpha=0.00025,\n                min_count=1,\n                dm =1)\n  \nmodel.build_vocab(tagged_data)\n\nfor epoch in range(max_epochs):\n    print('iteration {0}'.format(epoch))\n    model.train(tagged_data,\n                total_examples=model.corpus_count,\n                epochs=model.iter)\n    # decrease the learning rate\n    model.alpha -= 0.0002\n    # fix the learning rate, no decay\n    model.min_alpha = model.alpha\n    \nmodel.save(\"d2v.model\")\nprint(\"Model Saved\")","6ac153b5":"#loding the model again for future reference\nmodel= Doc2Vec.load(\"d2v.model\")","714439d2":"list_data[4]","0f110e99":"#finding document closest to the topic\ndef find_topic(text):\n    topic_list=list(show_topics(H1))\n    distances=[]\n    test_data2=word_tokenize(list_data[n].lower())\n    v2=model.infer_vector(test_data2)\n    for i in range(len(topic_list)):\n        test_data1=word_tokenize(text)\n        v1=model.infer_vector(test_data1)\n        distances.append(scipy.spatial.distance.cosine(v1,v2))\n\n    min_ele = min(distances) \n    topic_no= [i for i, j in enumerate(distances) if j == min_ele] \n    print('The document probably belongs to category:',topic_no)    \n    print('The category is:',show_topics(H1)[topic_no[0]])\nfind_topic(list_data[4])","8c0fbe9a":"def find_doc(topic):\n    distances=[]\n    topic_list=list(show_topics(H1))\n    test_data1=word_tokenize(topic_list[topic].lower())\n    v1=model.infer_vector(test_data1)\n    for i in range(len(list_data)):\n        test_data2=word_tokenize(list_data[i].lower())\n        v2=model.infer_vector(test_data2)\n        distances.append(scipy.spatial.distance.cosine(v1,v2))\n        min_ele = min(distances) \n    for j in range(len(distances)):\n        if distances[j]==min_ele:\n            doc_no=j\n    return(list_data[doc_no])","ac77d6c9":"# Topic 6 basically is related to articles that dig into the i guess as to how the virus functions \nprint('Topic 6:',show_topics(H1)[6])\ndoc=find_doc(6)\nprint('The document about topic 6 is:',doc)","d3a6b74e":"**Pros:**\nFaster to train than other models as it basically involves matrix multiplication\nBetter than SVD as we can define number of topics \n\n**Cons:**\nToo simple, not very precise in detection, hence not very accurate.\n","a48f75f5":"**** The following code presents my take on topic modelling using NMF. Any suggestions for improvement are welcome. Reference: https:\/\/github.com\/fastai\/course-nlp\/blob\/master\/2-svd-nmf-topic-modeling.ipynb"}}