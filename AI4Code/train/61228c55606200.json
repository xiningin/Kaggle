{"cell_type":{"97bb04c1":"code","83ed508e":"code","7802849d":"code","29510f17":"code","744eb8c9":"code","7586adce":"code","d5042aeb":"code","277d432d":"code","4aca9f4b":"code","3e43f13a":"code","0e2602fb":"code","c6a47dfd":"code","c6d56420":"code","acb76cbe":"code","650c4daa":"code","b26ad581":"code","8274797f":"code","37812bdd":"code","a935cf77":"code","20546b2a":"code","31bbd2fe":"code","55936104":"markdown","99c0a758":"markdown","e619f09a":"markdown","c0407bcd":"markdown","b0ab195f":"markdown","fedb917d":"markdown","a0cac7e8":"markdown","a343c130":"markdown","bf8337ba":"markdown","b014ce9f":"markdown","d0419826":"markdown"},"source":{"97bb04c1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","83ed508e":"from sklearn.ensemble import RandomForestClassifier as RFC\nfrom sklearn.linear_model import LogisticRegression as LR\nfrom sklearn.neighbors import KNeighborsClassifier as KNN\nfrom sklearn.tree import DecisionTreeClassifier as DTC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport matplotlib.pyplot as plt\nfrom sklearn import svm\nimport xgboost as xgb\nfrom tqdm import tqdm\nimport seaborn as sns\nimport warnings","7802849d":"training = pd.read_csv('\/kaggle\/input\/cap-4611-2021-fall-assignment-02\/train.csv')\ntesting  = pd.read_csv('\/kaggle\/input\/cap-4611-2021-fall-assignment-02\/eval.csv')","29510f17":"print('View headers: ')\nprint('\\t', training.columns, '\\n')\nprint('Check for missing values: ')\narr = pd.isna(training).sum().to_dict()\nfor i in arr:\n    print('\\t', i, arr[i])\nprint()\nprint('Examine first 5 rows: ')\nprint(training.head(), '\\n')","744eb8c9":"ratings = {}\ncolumns = ['alcohol_reference', 'animated_blood', 'blood',\n       'blood_and_gore', 'cartoon_violence', 'crude_humor', 'drug_reference',\n       'fantasy_violence', 'intense_violence', 'language', 'mature_humor',\n       'mild_blood', 'mild_cartoon_violence', 'mild_fantasy_violence',\n       'mild_language', 'mild_lyrics', 'mild_suggestive_themes',\n       'mild_violence', 'nudity', 'partial_nudity', 'sexual_content',\n       'sexual_themes', 'simulated_gambling', 'strong_janguage',\n       'strong_sexual_content', 'suggestive_themes', 'use_of_alcohol',\n       'use_of_drugs_and_alcohol', 'violence']\noffset = 3\nfor i in range(len(training)):\n    rating = training.loc[i, 'esrb_rating']\n    if rating not in ratings:\n        ratings[rating] = {}\n        \n    row = training.loc[i, columns]\n    for val in range(len(row)):\n        if training.columns[val+offset] not in ratings[rating]:\n            ratings[rating][training.columns[val+offset]] = 0\n        ratings[rating][training.columns[val+offset]] += row[val]\ndf = pd.DataFrame.from_dict(ratings)\nprint('Category count per rating: \\n')\nprint(df, '\\n')\nfig, ax = plt.subplots()\nfor i in df.columns:\n    ax.plot(df.loc[:, i], label=i)\nax.legend()\nplt.xticks(rotation=90)\nplt.show()\n","7586adce":"to_drop = ['no_descriptors', 'lyrics', 'mild_blood', 'mild_violence', 'animated_blood']\ntraining = training.loc[:, training.columns].drop(['title']+to_drop, axis=1)\ntesting = testing.loc[:, testing.columns].drop(to_drop, axis=1)\nwarnings.filterwarnings(\"ignore\")","d5042aeb":"test_size = 0.2\nbest = None\nC = [1, 5, 10, 100, 1000, 10000, 50000]\nn_neighbors = [1, 5, 10, 20]\nmax_depth = [i for i in range(1, 11)]\nn_estimators = [5, 10, 50, 100, 150, 200]\nlr_best, svm_best, dtc_best, rfc_best, knn_best, xgb_best = None, None, None, None, None, None\niterations = 60","277d432d":"X = training.loc[:, training.columns].drop(['esrb_rating'], axis=1)\ny = training.loc[:, 'esrb_rating']","4aca9f4b":"def compare(curr, best):\n    if not curr:\n        return best\n    if (not best) or (curr['score'] > best['score']):\n        return curr\n    return best","3e43f13a":"def distributeValidations(model, X, y):\n    print(model)\n    name = model['name']\n    num_runs = 50\n    scores = []\n    for i in range(num_runs):\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\n        predictions = model['model'].predict(X_test)\n        scores.append(accuracy_score(y_test, predictions))\n    sns.histplot(data=scores, bins=10)\n    plt.title(f'{name} Validation Distribution')\n    plt.xlabel('Accuracy')\n    plt.ylabel('Count')\n    plt.show()","0e2602fb":"def do_LR(X_train, X_test, y_train, y_test):\n    name = 'Logistic Regression'\n    best_lr = None\n    # Hyperparameter tuning\n    for c in C:\n        model = LR(C=c, solver='liblinear', penalty='l1')\n        model.fit(X_train, y_train)\n        predictions = model.predict(X_test)\n        curr = {'score': accuracy_score(predictions, y_test), 'model': model, 'name': name}\n        best_lr = compare(curr, best_lr)\n    return best_lr","c6a47dfd":"def do_SVM(X_train, X_test, y_train, y_test):\n    name = 'SVM'\n    best_svm = None\n    # Hyperparameter tuning\n    for c in C:\n        model = svm.SVC(C=c, decision_function_shape='ovo')\n        model.fit(X_train, y_train)\n        predictions = model.predict(X_test)\n        curr = {'score': accuracy_score(predictions, y_test), 'model': model, 'name': name}\n        best_svm = compare(curr, best_svm)\n    return best_svm","c6d56420":"def do_DTC(X_train, X_test, y_train, y_test):\n    name = 'Decision Tree Model'\n    best_dtc = None\n    # Hyperparameter tuning\n    for depth in max_depth:\n        model = DTC(max_depth=depth)\n        model.fit(X_train, y_train)\n        predictions = model.predict(X_test)\n        curr = {'score': accuracy_score(predictions, y_test), 'model': model, 'name': name}\n        best_dtc = compare(curr, best_dtc)\n    return best_dtc","acb76cbe":"def do_RFC(X_train, X_test, y_train, y_test):\n    name = 'Random Forrest'\n    best_rfc = None\n    for n in n_estimators:\n        model = RFC(criterion='entropy', n_estimators = n, max_features=len(training.columns)-1)\n        model.fit(X_train, y_train)\n        predictions = model.predict(X_test)\n        curr = {'score': accuracy_score(predictions, y_test), 'model': model, 'name': name}\n        best_rfc = compare(curr, best_rfc)\n    return best_rfc","650c4daa":"def do_KNN(X_train, X_test, y_train, y_test):\n    name = 'KNN'\n    best_knn = None\n    for i in n_neighbors:\n        model = KNN(n_neighbors=i, p=1)\n        model.fit(X_train, y_train)\n        predictions = model.predict(X_test)\n        curr = {'score': accuracy_score(predictions, y_test), 'model': model, 'name': name}\n        best_knn = compare(curr, best_knn)\n    return best_knn","b26ad581":"def do_XGBC(X_train, X_test, y_train, y_test):\n    name = 'XGB'\n    best_xgb = None\n    for i in max_depth:\n        model = xgb.XGBClassifier(max_depth = i, booster = 'gbtree', objective='reg:logistic', eval_metric='mae')\n        model.fit(X_train, y_train)\n        predictions = model.predict(X_test)\n        curr = {'score': accuracy_score(predictions, y_test), 'model': model, 'name': name}\n        best_xgb = compare(curr, best_xgb)\n    return best_xgb","8274797f":"for i in tqdm(range(iterations)):\n    arr = train_test_split(X, y, test_size=test_size)\n    lr_  = do_LR(*arr)\n    lr_best = compare(lr_, lr_best)\n    svm_ = do_SVM(*arr)\n    svm_best = compare(svm_, svm_best)\n    dtc_ = do_DTC(*arr)\n    dtc_best = compare(dtc_, dtc_best)\n    rfc_ = do_RFC(*arr)\n    rfc_best = compare(rfc_, rfc_best)\n    knn_ = do_KNN(*arr)\n    knn_best = compare(knn_, knn_best)\n    xgb_ = do_XGBC(*arr)\n    xgb_best = compare(xgb_, xgb_best)\n    performances = [lr_, svm_, dtc_, rfc_, knn_]\n    iteration_best = {}\n    for i in performances:\n        iteration_best = compare(i, iteration_best)\n    best = compare(iteration_best, best)\n            \n        ","37812bdd":"print(best['name'], best['score'])","a935cf77":"models = [lr_best, svm_best, dtc_best, rfc_best, knn_best, xgb_best]\n# Standardize model describe\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=test_size)\nfor model in models:\n    distributeValidations(model, X, y)\n    predictions = model['model'].predict(X_test)\n    stats = pd.DataFrame(predictions)\n    print(stats.describe())","20546b2a":"predictions = best['model'].predict(testing.loc[:, testing.columns])","31bbd2fe":"output = pd.DataFrame({'id': testing.id, 'esrb_rating': predictions})\nprint(output.to_string())\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","55936104":"# Variables","99c0a758":"# Logistic Regression Model","e619f09a":"# Data Visualization \/ Check For Outliers","c0407bcd":"# Decision Tree Model","b0ab195f":"# Support Vector Machine","fedb917d":"# Random Forrest Model","a0cac7e8":"# Data Transformation","a343c130":"# K Nearest Neighbors","bf8337ba":"My approach for removing outliers is to check for the lowest occuring categories. If only a few, if not 0, ratings contain, for example, no_descriptors, then no_descriptors must not be a deciding factor in rating.","b014ce9f":"# Distributions","d0419826":"# XGB Classifier"}}