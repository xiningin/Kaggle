{"cell_type":{"26e1eafd":"code","3152e933":"code","b7e5fa16":"code","d752a636":"code","2c000ee2":"code","2da1448e":"code","4cc74bf8":"code","cb258623":"code","d1dcb730":"code","96940a6d":"code","69ab88aa":"code","98b1b5c2":"code","bc9edab2":"code","12435226":"code","058f5189":"code","086e43c1":"code","71ee8412":"code","51ebea3b":"code","9ec7a489":"code","3a41c16e":"code","4aca5097":"code","c5fcea24":"code","85143818":"code","03616970":"code","b95f88c6":"code","ef56f86a":"code","008f148a":"code","7e9d2040":"code","b8ea6421":"code","7530f069":"code","5610b836":"code","750f9293":"markdown","51da3710":"markdown","091e7542":"markdown","d2732373":"markdown","fe328c60":"markdown","7dde8887":"markdown","e501fce1":"markdown"},"source":{"26e1eafd":"! cp ..\/input\/my-python\/* ..\/working\/","3152e933":"# General imports\nimport pandas as pd\nimport numpy as np\nimport os\nimport shutil\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm import tqdm_notebook\nimport gc\nfrom IPython.display import FileLink, FileLinks\n\n# Sklearn imports\nfrom sklearn.model_selection import train_test_split\n\n# Keras imports\nfrom keras.applications import InceptionV3, VGG19, ResNet50, Xception\nfrom keras.applications.inception_v3 import preprocess_input\n#from keras.applications.resnet50 import preprocess_input\n#from keras.applications.xception import preprocess_input\nfrom keras.models import Sequential, load_model\nfrom keras.layers import Dense, GlobalAveragePooling2D\nfrom keras.preprocessing.image import ImageDataGenerator, load_img, img_to_array\nfrom keras.optimizers import Adam, SGD\nfrom keras.callbacks import ModelCheckpoint\n\n# Image processing and augmentation\nimport imgaug as ia\nfrom imgaug import augmenters as iaa\nimport cv2\n\n# Cyclic learning rates\nfrom clr import LRFinder\nfrom clr_callback import CyclicLR","b7e5fa16":"# General constants\nDATA_DIR = '..\/input\/geotagged-photos\/'\nPHOTO_DIR = DATA_DIR + 'geotagged photos from farmers 2018\/'\nSEED = 2001\nBATCH_SIZE = 32\nINPUT_SIZE = 299","d752a636":"# Leer la informaci\u00f3n sobre las fotograf\u00edas. Vemos que todas las entradas tienen asociada una fotograf\u00eda\nphotos = pd.read_excel(os.path.join(DATA_DIR, 'GEOTAGGED_PHOTOS FROM FARMERS.xlsx'))\nphotos['WITH_PHOTO'] = photos['PHOTO NAME'].map(lambda x: os.path.isfile(os.path.join(PHOTO_DIR, x)))\nphotos['WITH_PHOTO'].value_counts()","2c000ee2":"# Pero hay menos fotos que entradas: algunas fotograf\u00edas tienen varias entradas asociadas (clasificaci\u00f3n multilabel)\nprint('Unique photo names', len(photos['PHOTO NAME'].unique()))","2da1448e":"# Veamos cu\u00e1ntas entradas hay de cada tipo de producto\nphotos['PRODUCT NAME'].value_counts()","4cc74bf8":"# Veamos algunas fotograf\u00edas aleatorias\n\ndef mostrar_fotos(df, filas, columnas, photo_column='PHOTO NAME'):\n    fig, axs = plt.subplots(filas, columnas, figsize=(5 * filas, 5 * columnas), squeeze=True)\n    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n    plt.tight_layout()\n    axs = axs.reshape(-1)\n    indices = np.random.choice(df.shape[0], filas * columnas, replace=False)\n    for ax, i in zip(axs, indices):\n        img = cv2.imread(os.path.join(PHOTO_DIR, df.iloc[i][photo_column]))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        ax.set_xticks([])\n        ax.set_yticks([])\n        ax.imshow(img)","cb258623":"mostrar_fotos(photos, 5, 5)","d1dcb730":"erroneos = pd.read_csv(DATA_DIR + 'erroneos.txt', delim_whitespace=True, usecols=[3], names=['photo'])\nmostrar_fotos(erroneos, 5, 5, photo_column='photo')","96940a6d":"photos['ERRONEA'] = photos['PHOTO NAME'].apply(lambda x: x in erroneos.photo.values)\nphotos['ERRONEA'].value_counts()","69ab88aa":"# Excluimos las err\u00f3neas\nphotos = photos[photos.ERRONEA == False]","98b1b5c2":"# Vamos a cambiar los productos para simplificar, mapeando a un \u00fanico producto todos cereales, los pastos y tambi\u00e9n todos los barbechos\nCLASSES = ['CEREAL', 'GRASSLAND', 'FALLOW']\nNUM_CLASSES = len(CLASSES)\n\nsingle_product = {'PERMANENT GRASSLAND':'GRASSLAND', 'GRASSLAND':'GRASSLAND', 'EFA LYING FALLOW': 'FALLOW', 'NON-EFA LYING FALLOW':'FALLOW',\n                 'BARLEY':'CEREAL', 'SOFT WHEAT': 'CEREAL', 'OATS': 'CEREAL', 'SORGHUM':'CEREAL'}\ndef mapeo(p):\n    mp = single_product.get(p)\n    if mp is None:\n        return p\n    else:\n        return mp\n    \nphotos['PRODUCTO'] = photos['PRODUCT NAME'].map(mapeo)\nphotos = photos[photos.PRODUCTO.isin(CLASSES)]","bc9edab2":"# Creamos un nuevo data frame s\u00f3lo con las fotograf\u00edas y las clases de inter\u00e9s\ntemp_df = photos.groupby(['PHOTO NAME','PRODUCTO'])['PRODUCTO'].count()\ntemp_df = temp_df.map(lambda x: 1 if x > 1 else x)\ntemp_df = temp_df.unstack().fillna(0).astype(np.int8)\ntemp_df.reset_index(drop=False, inplace=True)\ntemp_df.columns = ['PHOTO NAME'] + CLASSES\ntemp_df.head(20)","12435226":"# Creamos una nueva columna para estratificar el conjunto de prueba y el de validaci\u00f3n por producto\ntemp_df['stratify'] = temp_df.apply(lambda r: str(r[1]) + str(r[2]) + str(r[3]), axis='columns')\ntemp_df['stratify'].value_counts()","058f5189":"# Ya que el n\u00famero de fotograf\u00edas es limitado, vamos a cargarlas en memoria para acelerar el entrenamiento\n\nnum_fotos = temp_df.shape[0]\n\nX = np.zeros(shape=(num_fotos, INPUT_SIZE, INPUT_SIZE, 3), dtype=np.int16)\nfor idx in tqdm_notebook(range(num_fotos)):\n    img = load_img(os.path.join(PHOTO_DIR, temp_df.iloc[idx]['PHOTO NAME']), target_size=(INPUT_SIZE, INPUT_SIZE))\n    img = img_to_array(img)\n    X[idx] = img\n    \ny = temp_df[temp_df.columns[1:-1]].values\n\ngc.collect()","086e43c1":"# Dividir las fotograf\u00edas en conjuntos de entrenamiento y validaci\u00f3n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.20, stratify=temp_df['stratify'], random_state=SEED)\nprint (X_train.shape, X_val.shape, y_train.shape, y_val.shape)","71ee8412":"augs = iaa.Sequential([\n    iaa.Fliplr(0.5),\n    iaa.Sometimes(0.2, iaa.Affine(rotate=(-20,20), mode='reflect')),  \n    iaa.SomeOf((0,4), [\n        iaa.AdditiveGaussianNoise(scale=0.01*255),        \n        iaa.Sharpen(alpha=(0.0,0.3)),\n        iaa.ContrastNormalization((0.8,1.2)),\n        iaa.AverageBlur(k=(2,11)),\n        iaa.Multiply((0.8,1.2)),\n        iaa.Add((-20,20), per_channel=0.5),\n        iaa.Grayscale(alpha=(0.0,1.0))\n    ])\n]) \n\ntrain_datagen = ImageDataGenerator(preprocessing_function = preprocess_input,\n                                  rotation_range=30, \n                                  horizontal_flip=True,\n                                  vertical_flip=True,\n                                  width_shift_range=0.25,\n                                  height_shift_range=0.25,\n                                  zoom_range=0.3,\n                                  brightness_range=(0.7,1.5),\n                                  fill_mode='reflect')\n\nval_datagen = ImageDataGenerator(preprocessing_function = preprocess_input)\n\ntrain_generator = train_datagen.flow(X_train, y_train, batch_size=BATCH_SIZE, shuffle=True, seed=SEED)\n\nval_generator = val_datagen.flow(X_val, y_val, batch_size=BATCH_SIZE, shuffle=False, seed=SEED)\n","51ebea3b":"def create_model(lr=0.0001):\n  base = InceptionV3(weights='imagenet', include_top=False, input_shape=(INPUT_SIZE, INPUT_SIZE, 3), pooling='avg')\n  #base = VGG19(weights='imagenet', include_top=False, input_shape=(INPUT_SIZE, INPUT_SIZE, 3), pooling='avg')\n  #base = ResNet50(weights='imagenet', include_top=False, input_shape=(INPUT_SIZE, INPUT_SIZE, 3), pooling='avg')\n  #base = Xception(weights='imagenet', include_top=False, input_shape=(INPUT_SIZE, INPUT_SIZE, 3), pooling='avg')\n  \n  for layer in base.layers:\n        layer.trainable = False\n    \n  model = Sequential()\n  model.add(base)\n  model.add(Dense(512, activation='relu'))\n  model.add(Dense(NUM_CLASSES, activation='sigmoid'))\n\n  adam = Adam(lr=lr)\n  sgd = SGD(lr=0.1, momentum=0.90, nesterov=True)\n  model.compile(optimizer=sgd , loss='binary_crossentropy', metrics=['acc'],  )\n\n  return model","9ec7a489":"# Run a lr range test to find good learning rate margins\n\nmodel = create_model()\n\nSTEP_SIZE_TRAIN = X_train.shape[0] \/\/ BATCH_SIZE\nSTEP_SIZE_VALID = X_val.shape[0] \/\/ BATCH_SIZE\n\ntrain_generator.reset()\nval_generator.reset()\n\nEPOCHS = 1\nbase_lr=0.0001\nmax_lr=10\nstep_size = EPOCHS * STEP_SIZE_TRAIN \nlrf = LRFinder(X_train.shape[0], BATCH_SIZE,\n                       base_lr, max_lr,\n                       # validation_data=(X_val, Yb_val),\n                       lr_scale='exp', save_dir='.\/lr_find\/', verbose=False)\n\nhistory = model.fit_generator(train_generator, \n                              epochs=EPOCHS, \n                              steps_per_epoch=STEP_SIZE_TRAIN\n                              ,validation_data=val_generator,\n                              validation_steps=STEP_SIZE_VALID,\n                              callbacks=[lrf]\n                             )","3a41c16e":"fig = plt.figure(figsize=(15,7))\nlrf.plot_schedule(clip_beginning=10)","4aca5097":"model = create_model()\nmodel.summary()","c5fcea24":"EPOCHS=60\nSTEP_SIZE_TRAIN = X_train.shape[0] \/\/ BATCH_SIZE\nSTEP_SIZE_VALID = X_val.shape[0] \/\/ BATCH_SIZE\n\ntrain_generator.reset()\nval_generator.reset()\n\nclr = CyclicLR(base_lr=0.004, max_lr=0.02, step_size=2*STEP_SIZE_TRAIN, mode='exp_range')\ncheckpoint = ModelCheckpoint('land_use_predict_inception_v3.h5', monitor='val_loss', save_best_only=True, save_weights_only=False)\n\nhistory = model.fit_generator(train_generator, \n                              epochs=EPOCHS, \n                              steps_per_epoch=3 * STEP_SIZE_TRAIN,\n                              validation_data=val_generator,\n                              validation_steps=STEP_SIZE_VALID,\n                              callbacks=[clr, checkpoint]\n                             )","85143818":"def plt_history(history, metric, title, ax, val=True):\n    ax.plot(history[metric])\n    if val:\n        ax.plot(history['val_' + metric])\n    ax.grid(True)\n    ax.set_title(title)\n    ax.set_xlabel('epoch')\n    ax.set_ylabel(metric)\n    \nhist = history.history\nfig, ax = plt.subplots(1,2, figsize=(15,6))\nplt_history(hist, 'loss', 'LOSS', ax[0])\nplt_history(hist, 'acc', 'ACCURACY', ax[1])\nplt.savefig('history')","03616970":"# Mostrar aleatoriamente im\u00e1genes y sus predicciones\ndef prediccion_aleatoria(X, y):\n    idx = np.random.choice(range(X.shape[0]), 1)\n    img = X[idx].squeeze()\n    x = img_to_array(img)\n    y_true = y[idx].squeeze()\n    x = np.expand_dims(img, 0)\n    x = preprocess_input(x)\n    y_pred = model.predict_proba(x).squeeze()\n    \n    print('### REAL ###')\n    print('CEREAL:{:2d} PASTO:{:2d} BARBECHO:{:2d}'.format(y_true[0], y_true[1], y_true[2]))\n    print()\n    print('### PREDICCI\u00d3N ###')\n    print('CEREAL:{:.2f} PASTO:{:.2f} BARBECHO:{:.2f}'.format(y_pred[0], y_pred[1], y_pred[2]))\n    \n    fig, ax = plt.subplots(1,1, figsize=(8,8), squeeze=True)\n    ax.set_xticks([])\n    ax.set_yticks([])\n    \n    \n    ax.imshow(img)","b95f88c6":"# Recuperamos el mejor modelo salvado para hacer las predicciones\nmodel = load_model('land_use_predict_inception_v3.h5')","ef56f86a":"prediccion_aleatoria(X_val, y_val)","008f148a":"prediccion_aleatoria(X_val, y_val)","7e9d2040":"prediccion_aleatoria(X_val, y_val)","b8ea6421":"prediccion_aleatoria(X_val, y_val)","7530f069":"prediccion_aleatoria(X_val, y_val)","5610b836":"prediccion_aleatoria(X_val, y_val)","750f9293":"Hay im\u00e1genes que corresponden a im\u00e1genes SIGPAC y otros tipos de errores. Vamos a excluirlas. Para ello he preparado una lista con las fotograf\u00edas a excluir.","51da3710":"<h2>Predicciones<\/h2>","091e7542":"<h2>Exploraci\u00f3n de datos b\u00e1sica<\/h2>","d2732373":"<h2>Crear generadores de datos<\/h2>","fe328c60":"[](http:\/\/)<h1>Predict land use<\/h1>","7dde8887":"<h2>Preparaci\u00f3n de datos<\/h2>","e501fce1":"[](http:\/\/)<h2>Cargar en memoria las fotograf\u00edas<\/h2>"}}