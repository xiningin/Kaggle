{"cell_type":{"d04fd30a":"code","d8de826e":"code","0f3bab00":"code","5134637a":"code","8b515204":"code","b070d1f5":"code","81c8d9f6":"code","4ef09755":"code","5e9463ba":"code","c692c32a":"code","e31c8599":"code","6bc2dc94":"code","a94117c3":"code","a1ddaab7":"code","194e2e78":"code","70e34cc3":"markdown","cebb13df":"markdown","c48a514b":"markdown","ac8ebd9c":"markdown","48673a90":"markdown","aaaa2fed":"markdown","c0364bb9":"markdown","1d965dfe":"markdown","2757a9f7":"markdown","b6d02b92":"markdown"},"source":{"d04fd30a":"import pandas as pd\nimport matplotlib.pyplot as plt \nimport seaborn as sns \nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport re \nimport nltk as nlp\nfrom plotly.offline import iplot\n\n\n","d8de826e":"df = pd.read_csv('..\/input\/nyc-jobs.csv',sep=',')\ndf.head()","0f3bab00":"df.info()","5134637a":"\ndf=df[df['Job Category']=='Technology, Data & Innovation'].reset_index()\ndf = df.drop(['index'],axis=1)\ndf","8b515204":"sns.countplot(x='Full-Time\/Part-Time indicator', data=df)\n","b070d1f5":"sns.countplot(x='Salary Frequency', data=df)","81c8d9f6":"df['civil_service'] =df['Civil Service Title']\n\n\ngroups = df.groupby(['civil_service']).size()\nplt.figure(figsize=(10, 10))\ngroups.plot.barh()\n\n","4ef09755":"df['business_title']=df['Business Title']\n\n\ngroups = df.groupby(['business_title']).size()\nplt.figure(figsize=(10, 30))\ngroups.plot.barh()","5e9463ba":"df['location']=df['Work Location']\ngroups = df.groupby(['location']).size()\nplt.figure(figsize=(10, 10))\ngroups.plot.barh()","c692c32a":"df['workUnit']=df['Division\/Work Unit']\ngroups = df.groupby(['workUnit']).size()\nplt.figure(figsize=(10, 15))\ngroups.plot.barh()","e31c8599":"df['mean_salary_from']=df['Salary Range From']\na=df.groupby('business_title')['mean_salary_from'].mean()\nplt.figure(figsize=(10, 30))\na.plot.barh()","6bc2dc94":"df['skills']=df['Preferred Skills']\n\n\nfirst_description=df.skills[4]\ndescription=re.sub(\"[^a-zA-Z]\",\" \",first_description)\n\ndescription = description.lower()\nprint(\"First value :::::::::::::: {0}   \\nSecond value :::::::::  {1}\".format(first_description,description))","a94117c3":"description_list=[]\nimport nltk\nfor description in df.skills:\n     description=str(description)\n     description = description.lower()\n     description = nltk.word_tokenize(description)\n     lemma = nlp.WordNetLemmatizer()\n     description = [lemma.lemmatize(word) for word in description]\n     description =\" \".join(description)\n     description_list.append(description)\n","a1ddaab7":"from sklearn.feature_extraction.text import CountVectorizer\nmax_features=60\ncount_vectroizer =CountVectorizer(max_features=max_features,stop_words=\"english\")# -----> stopwords unmeaning words\nsparce_matrix = count_vectroizer.fit_transform(description_list).toarray()\ndictionary = count_vectroizer.vocabulary_.items()  \nvocab = []\ncount = []\nfor key, value in dictionary:\n    vocab.append(key)\n    count.append(value)\nvocab_bef_stem = pd.Series(count, index=vocab)\nvocab_bef_stem = vocab_bef_stem.sort_values(ascending=False)\ntop_vacab = vocab_bef_stem.head(50)\ntop_vacab.plot(kind = 'barh', figsize=(10,20))","194e2e78":"description_list2=[]\ndf['minR']=df['Minimum Qual Requirements']\nfor description2 in df.minR:\n     description2=str(description2)\n     description2 = description2.lower()\n     description2 = nltk.word_tokenize(description2)\n     lemma2 = nlp.WordNetLemmatizer()\n     description2 = [lemma2.lemmatize(word) for word in description2]\n     description2 =\" \".join(description2)\n     description_list2.append(description2)\n        \n        \nfrom sklearn.feature_extraction.text import CountVectorizer\nmax_features=60\ncount_vectroizer2 =CountVectorizer(max_features=max_features,stop_words=\"english\")# -----> stopwords unmeaning words\nsparce_matrix2 = count_vectroizer2.fit_transform(description_list2).toarray()\ndictionary2 = count_vectroizer2.vocabulary_.items()  \nvocab2 = []\ncount2 = []\nfor key, value in dictionary2:\n    vocab2.append(key)\n    count2.append(value)\nvocab_bef_stem2 = pd.Series(count2, index=vocab2)\nvocab_bef_stem2 = vocab_bef_stem2.sort_values(ascending=False)\ntop_vacab2 = vocab_bef_stem2.head(50)\ntop_vacab2.plot(kind = 'barh', figsize=(10,20))\n","70e34cc3":"## We drop unnecessary characters like \u00e2\u20ac\u00a2  and we converted text to lower case \n## Now we apply this action to  all data.\n### Firstly, we create a list in order to keep all words in text \n### Secondly, we apply lemmatization ","cebb13df":"# Importing Libs ","c48a514b":"# Most demand Positions , locations, salaries and Units","ac8ebd9c":"# As a result for New York,  ","48673a90":"## Loading data and getting information from it","aaaa2fed":"## Full time jobs  more than part time jobs\n## Most wanted Civil Service is Computer Specialist (software)\n## Most wanted Business Title is Computer Specialist (software) and Computer Specialist Manager\n## Popular Location is Brooklyn\n## Positions which have salary more than 100k\/yearly  are Vedor Managers,Full-stack developers,Unit Managers, eDx developers\n## Prefered Skills are java,sql,web\n## Min Qua. College Degree ,  experience","c0364bb9":"# Part time or Full time ? \n","1d965dfe":"# Which skills are preferred ? \n# What is Minimum Qual Requirements ?\n### Lets find \n\n#### Firstly, we make data cleaning, we use regular expression for this\n### select random text and convert it to lower case","2757a9f7":"# Analysing New York  Technology, Data & Innovation Industry\n### This kernel, we will examine job posting for New York  Technology, Data & Innovation Industry and its popular fields, Jobs Types (Part-Time or Full-Time ), Most demand Positions , location, salary , skills which are preferred and  Minimum Qual Requirements","b6d02b92":"## We make this process again for Minimum Qual Requirements "}}