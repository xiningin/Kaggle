{"cell_type":{"df835796":"code","3bca21f6":"code","139dfcae":"code","88114690":"code","eeb7fca6":"code","e65b71ad":"code","f7d34a3c":"code","4e62a414":"code","34523409":"code","c198929a":"code","ffdacbe3":"code","99387d87":"code","0b36b072":"code","99b4955e":"code","ad848671":"code","5f046445":"code","e9547cbe":"code","fd95a74a":"markdown","4795c6ff":"markdown","542bb016":"markdown","4905efd1":"markdown","c22b6b94":"markdown","94a6016d":"markdown","ac2bc10d":"markdown","7805b863":"markdown","7bcccaeb":"markdown"},"source":{"df835796":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","3bca21f6":"import sklearn.metrics  as sklm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.neighbors import LocalOutlierFactor\n\nimport lightgbm as lgbm","139dfcae":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\ndataset =  pd.concat(objs=[train, test], axis=0).reset_index(drop=True)\ndataset = dataset.fillna(np.nan)\n\ndataset.head(15)","88114690":"g = sns.catplot(y=\"Age\",x=\"Sex\",data=dataset,kind=\"box\")","eeb7fca6":"def set_age(row):\n    age = row.Age\n    if np.isnan(age):\n        if row.Sex == 'male':\n            age = 28\n        else:\n            age = 27\n    return age\n\ndataset.Age = dataset.apply(lambda row: set_age(row), axis=1)\ndataset.head(15)","e65b71ad":"# parse names\nknown_titles = ['Miss.','Mrs.','Mr.','Master.']\ndef build_title(name):\n    title = name.find('Miss.')\n    if title >=0:\n        return 1\n    title = name.find('Mrs.')\n    if title >=0:\n        return 2\n    title = name.find('Mr.')\n    if title >=0:\n        return 3\n    title = name.find('Master.')\n    if title >=0:\n        return 4\n    return 0\n\ndataset['Title'] = dataset.Name.apply(build_title)\ndataset.head()","f7d34a3c":"dataset['FamilySize'] = dataset['SibSp'] + dataset['Parch'] + 1\ndataset.drop(['Name','SibSp','Parch','Ticket'], axis=1, inplace=True)  \ndataset.head()","4e62a414":"fare_midean = dataset['Fare'].median()\nvalues = {'Cabin':'Z','Embarked':'S','Fare':fare_midean}\ndataset.fillna(value=values,inplace=True)\ndataset['Cabin1'] = dataset['Cabin'].str[0]    \ndataset.drop(['Cabin'], axis=1, inplace=True)    \n    \n#encode labels\ndataset['Sex'] = LabelEncoder().fit_transform(dataset['Sex'].values)\ndataset['Cabin1'] = LabelEncoder().fit_transform(dataset['Cabin1'].values)\ndataset['Embarked'] = LabelEncoder().fit_transform(dataset['Embarked'].values)\n\ndataset.head()","34523409":"working_columns = ['Pclass','Sex','Age','Fare','Embarked','FamilySize','Cabin1','Title']\n\nscaler = StandardScaler()\ndataset[working_columns] = scaler.fit_transform(dataset[working_columns])\ndataset.head(10)","c198929a":"train_set = dataset.iloc[:train.shape[0]]\ntest_set = dataset.iloc[train.shape[0]:]\n\ntrain_set.tail()\ntest_set.head()","ffdacbe3":"columns = [c for c in dataset.columns if c not in [\"Survived\", \"PassengerId\", \"Name\", \"Sex\", \"Cabin1\", \"Embarked\"]]\n\n# fit the model for outlier detection (default)\nclf = LocalOutlierFactor(n_neighbors=20, contamination=0.1)\n# use fit_predict to compute the predicted labels of the training samples\n# (when LOF is used for outlier detection, the estimator has no predict,\n# decision_function and score_samples methods).\ny_pred = clf.fit_predict(train_set[columns].values)\nX_scores = clf.negative_outlier_factor_\n\nclf_df = pd.DataFrame( columns=['X_scores', 'y_pred'])\nclf_df.X_scores = X_scores\nclf_df.y_pred = y_pred\noutlier_index = clf_df.index[clf_df['y_pred'] == -1].tolist()\n\ntrain_set.drop(outlier_index)","99387d87":"g = sns.heatmap(dataset[working_columns].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","0b36b072":"from sklearn.model_selection import GridSearchCV\n\n\ncv_params = {\n    'num_leaves': [10, 15, 20],\n    'min_data_in_leaf':[15, 17, 19], \n    'learning_rate': [0.03,0.04,0.05]\n}\n\nind_params = {\n    'application': 'binary',\n    'objective': 'binary',\n    'metric': 'auc',\n    'is_unbalance': 'true',\n    'boosting': 'gbdt',\n    'feature_fraction': 0.5,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 20,\n    'verbose': 0\n}\n\noptimized_GBM = GridSearchCV(lgbm.LGBMClassifier(**ind_params), cv_params, scoring = 'accuracy', cv = 5, n_jobs = -1) \noptimized_GBM.fit(X_train, y_train)","99b4955e":"print( 'best_score' % optimized_GBM.best_score_)\nprint( 'best_params' % optimized_GBM.best_params_)\n","ad848671":"X = train_set[working_columns].values\ny = train_set['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n\nlgb_train_set = lgbm.Dataset(X_train, label=y_train)\nlgb_test_set = lgbm.Dataset(X_test, label=y_test)\n\n## {'learning_rate': 0.03, 'min_data_in_leaf': 17, 'num_leaves': 15}\n\n#setting parameters for lightgbm\nparameters = {\n    'application': 'binary',\n    'objective': 'binary',\n    'metric': 'auc',\n    'is_unbalance': 'true',\n    'boosting': 'gbdt',\n    'num_leaves': 15,\n    'min_data_in_leaf':17, \n    'feature_fraction': 0.5,\n    'bagging_fraction': 0.5,\n    'bagging_freq': 20,\n    'learning_rate': 0.03,\n    'verbose': 0\n}\n\n\n#training our model using light gbm\nmodel = lgbm.train(parameters, lgb_train_set, valid_sets=lgb_test_set, num_boost_round=5000, early_stopping_rounds=100)\n","5f046445":"from sklearn.metrics import roc_auc_score, roc_curve, accuracy_score\n\ny_pred = model.predict(X_test)\n\nfpr, tpr, _ = roc_curve(y_test,  y_pred)\nauc = roc_auc_score(y_test, y_pred)\nplt.plot(fpr,tpr,label=\"data 1, auc=\"+str(auc))\nplt.legend(loc=4)\nplt.show()","e9547cbe":"X_sub = test_set[working_columns].values\npredictions = model.predict(X_sub)\n\nsubmission = pd.DataFrame({\n        \"PassengerId\": test_set[\"PassengerId\"],\n        \"Survived\": predictions.ravel() \n    })\n\nsubmission.Survived = submission.Survived.apply(lambda x : 1 if x > 0.49 else 0)\nsubmission.to_csv('submission.csv', index=False)","fd95a74a":"## Age cleanup and refill","4795c6ff":"## Light GBM model","542bb016":"## Remove outliers from train set","4905efd1":"## New title feature","c22b6b94":"## Split back to train and test datasets","94a6016d":"##  Optimize hyper-parameters","ac2bc10d":"## New family size feature","7805b863":"## Scale","7bcccaeb":"## General cleanups"}}