{"cell_type":{"4158166f":"code","c1dd10e3":"code","b1ab2dba":"code","c4579499":"code","e834e994":"code","762a92ce":"code","ce184865":"code","16859452":"code","ff1047e4":"code","6cfe8daa":"code","aba5f908":"code","f3eecc80":"code","f9f735c1":"code","cf5fd0c0":"code","33dbfe93":"markdown","5ec6ffb3":"markdown","4ad682cf":"markdown","52d668a1":"markdown","ad230729":"markdown","5279c1f1":"markdown"},"source":{"4158166f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport torch.nn.functional as F\nimport os\n\n# Any results you write to the current directory are saved as output.\nimport torch\nimport torch.nn as nn\nfrom torch.utils.data import Dataset,DataLoader\nfrom torchvision import transforms,models\nfrom tqdm import tqdm_notebook as tqdm\nimport math\nimport torch.utils.model_zoo as model_zoo\nimport cv2\n\nimport openslide\n# Option 2: Load images using skimage (requires that tifffile is installed)\nimport skimage.io\nimport random\nfrom sklearn.metrics import cohen_kappa_score\nimport albumentations\n# General packages\n\n# import PIL\nfrom PIL import Image\n\n# from IPython.display import Image, display","c1dd10e3":"BASE_PATH = '..\/input\/prostate-cancer-grade-assessment'\n\n# image and mask directories\ndata_dir = f'{BASE_PATH}\/test_images'\n# data_dir = f'{BASE_PATH}\/train_images'\n\n\nmask_dir = f'{BASE_PATH}\/test_label_masks'\n\n# Location of test labels\ntest = pd.read_csv(f'{BASE_PATH}\/test.csv')\ntrain = pd.read_csv(f'{BASE_PATH}\/train.csv')\n# test = pd.read_csv(f'{BASE_PATH}\/train.csv').head(200)\n\nsubmission = pd.read_csv(f'{BASE_PATH}\/sample_submission.csv')","b1ab2dba":"test.head()","c4579499":"def seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_torch(seed=42)","e834e994":"class config:\n    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\n    IMG_WIDTH = 256\n    IMG_HEIGHT = 256\n    TEST_BATCH_SIZE = 1\n    CLASSES = 6","762a92ce":"from collections import OrderedDict\nimport math\n\n\nclass SEModule(nn.Module):\n\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels \/\/ reduction, kernel_size=1,\n                             padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels \/\/ reduction, channels, kernel_size=1,\n                             padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\n\nclass Bottleneck(nn.Module):\n    \"\"\"\n    Base class for bottlenecks that implements `forward()` method.\n    \"\"\"\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.se_module(out) + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(Bottleneck):\n    \"\"\"\n    Bottleneck for SENet154.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n                               stride=stride, padding=1, groups=groups,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * 4)\n        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBottleneck(Bottleneck):\n    \"\"\"\n    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n    (the latter is used in the torchvision implementation of ResNet).\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n                               stride=stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n                               groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNeXtBottleneck(Bottleneck):\n    \"\"\"\n    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None, base_width=4):\n        super(SEResNeXtBottleneck, self).__init__()\n        width = math.floor(planes * (base_width \/ 64)) * groups\n        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n                               stride=1)\n        self.bn1 = nn.BatchNorm2d(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n                               padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(width)\n        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SENet(nn.Module):\n\n    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n                 downsample_padding=1, num_classes=1000):\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        if input_3x3:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n                                    bias=False)),\n                ('bn1', nn.BatchNorm2d(64)),\n                ('relu1', nn.ReLU(inplace=True)),\n                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn2', nn.BatchNorm2d(64)),\n                ('relu2', nn.ReLU(inplace=True)),\n                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn3', nn.BatchNorm2d(inplanes)),\n                ('relu3', nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n                                    padding=3, bias=False)),\n                ('bn1', nn.BatchNorm2d(inplanes)),\n                ('relu1', nn.ReLU(inplace=True)),\n            ]\n        # To preserve compatibility with Caffe weights `ceil_mode=True`\n        # is used instead of `padding=1`.\n        layer0_modules.append(('pool', nn.MaxPool2d(3, stride=2,\n                                                    ceil_mode=True)))\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0\n        )\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.avg_pool = nn.AvgPool2d(7, stride=1)\n        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n                    downsample_kernel_size=1, downsample_padding=0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=downsample_kernel_size, stride=stride,\n                          padding=downsample_padding, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n                            downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups, reduction))\n\n        return nn.Sequential(*layers)\n\n    def features(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, x):\n        x = self.avg_pool(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.logits(x)\n        return x\n\n\ndef initialize_pretrained_model(model, num_classes, settings):\n    assert num_classes == settings['num_classes'], \\\n        'num_classes should be {}, but is {}'.format(\n            settings['num_classes'], num_classes)\n    model.load_state_dict(model_zoo.load_url(settings['url']))\n    model.input_space = settings['input_space']\n    model.input_size = settings['input_size']\n    model.input_range = settings['input_range']\n    model.mean = settings['mean']\n    model.std = settings['std']\n\n\ndef se_resnext50_32x4d(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = config.pretrained_settings['se_resnext50_32x4d'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext101_32x4d(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = config.pretrained_settings['se_resnext101_32x4d'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n","ce184865":"class CustomSEResNeXt(nn.Module):\n\n    def __init__(self, model_name='se_resnext50_32x4d'):\n        assert model_name in ('se_resnext50_32x4d')\n        super().__init__()\n        \n        self.model = se_resnext50_32x4d(pretrained=None)\n        self.model.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.model.last_linear = nn.Linear(self.model.last_linear.in_features, config.CLASSES)\n        \n    def forward(self, x):\n        x = self.model(x)\n        return x","16859452":"class PandaDataset(Dataset):\n    def __init__(self, images, img_height, img_width):\n        self.images = images\n        self.img_height = img_height\n        self.img_width = img_width\n        \n        # we are in validation part\n        self.aug = albumentations.Compose([\n            albumentations.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225], always_apply=True)\n        ])\n\n    def __len__(self):\n        return len(self.images)\n\n\n    def __getitem__(self, idx):\n\n        img_name = self.images[idx]\n        img_path = os.path.join(data_dir, f'{img_name}.tiff')\n\n        img = skimage.io.MultiImage(img_path)\n        img = cv2.resize(img[-1], (512, 512))\n        save_path =  f'{img_name}.png'\n        cv2.imwrite(save_path, img)\n        img = skimage.io.MultiImage(save_path)\n            \n        img = cv2.resize(img[-1], (self.img_height, self.img_width))\n\n        img = Image.fromarray(img).convert(\"RGB\")\n        img = self.aug(image=np.array(img))[\"image\"]\n        img1 = cv2.rotate(img, cv2.ROTATE_90_CLOCKWISE)\n        img2 = cv2.rotate(img, cv2.ROTATE_90_COUNTERCLOCKWISE)\n        img3 = cv2.rotate(img, cv2.ROTATE_180)\n        img = np.transpose(img, (2, 0, 1)).astype(np.float32)\n        img1 = np.transpose(img1, (2, 0, 1)).astype(np.float32)\n        img2 = np.transpose(img2, (2, 0, 1)).astype(np.float32)\n        img3 = np.transpose(img3, (2, 0, 1)).astype(np.float32)\n\n        return { 'image': torch.tensor(img, dtype=torch.float),'image1': torch.tensor(img1, dtype=torch.float),'image2': torch.tensor(img2, dtype=torch.float),'image3': torch.tensor(img3, dtype=torch.float) }\n    ","ff1047e4":"model = CustomSEResNeXt(model_name='se_resnext50_32x4d')\nweights_path = '..\/input\/panda-resnext\/resnext50_1.pth'\nmodel.load_state_dict(torch.load(weights_path, map_location=config.device))","6cfe8daa":"model_1 = CustomSEResNeXt(model_name='se_resnext50_32x4d')\nweights_path = '..\/input\/panda-resnext\/resnext50_2.pth'\nmodel_1.load_state_dict(torch.load(weights_path, map_location=config.device))","aba5f908":"import cv2\nmodel.eval()\npredictions = []\n\ndevice = config.device\n\nif os.path.exists('..\/input\/prostate-cancer-grade-assessment\/test_images'):\n    \n    test_dataset = PandaDataset(\n        images=test.image_id.values,\n        img_height=config.IMG_HEIGHT,\n        img_width=config.IMG_WIDTH,\n    )\n\n    test_data_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=config.TEST_BATCH_SIZE,\n        shuffle=False,\n    )\n    \n    model.to(device)\n    model_1.to(device)\n    \n    for idx, d in tqdm(enumerate(test_data_loader), total=len(test_data_loader)):\n        preds_ = np.zeros((6,6))\n        inputs = d[\"image\"]\n        inputs = inputs.to(device)\n        with torch.no_grad():\n            outputs = model(inputs)\n        preds_[0,:] = outputs.cpu().detach().numpy()\n        inputs = d[\"image1\"]\n        inputs = inputs.to(device)\n        with torch.no_grad():\n            outputs = model(inputs)\n        preds_[1,:] = outputs.cpu().detach().numpy()\n        inputs = d[\"image2\"]\n        inputs = inputs.to(device)\n        with torch.no_grad():\n            outputs = model(inputs)\n        preds_[2,:] = outputs.cpu().detach().numpy()\n        inputs = d[\"image3\"]\n        inputs = inputs.to(device)\n        with torch.no_grad():\n            outputs = model(inputs)\n        preds_[3,:] = outputs.cpu().detach().numpy()\n        inputs = d[\"image\"]\n        inputs = inputs.to(device)\n        with torch.no_grad():\n            outputs = model_1(inputs)\n        preds_[4,:] = outputs.cpu().detach().numpy()\n        inputs = d[\"image3\"]\n        inputs = inputs.to(device)\n        with torch.no_grad():\n            outputs = model_1(inputs)\n        preds_[5,:] = outputs.cpu().detach().numpy()\n        preds_ = preds_.sum(axis = 0)\n        predictions.append(preds_.argmax(0))\n#     predictions = np.concatenate(predictions)","f3eecc80":"if len(predictions) > 0:\n    submission.isup_grade = predictions\n    submission.isup_grade = submission['isup_grade'].astype(int)","f9f735c1":"submission.to_csv('submission.csv',index=False)","cf5fd0c0":"submission.head()","33dbfe93":"## Save results","5ec6ffb3":"For EDA and visualizations, Please visit https:\/\/www.kaggle.com\/rohitsingh9990\/panda-eda-better-visualization\/comments","4ad682cf":"Other notebooks\n\n[Efficientnet_keras_train-(QWK loss + Augmentation)](https:\/\/www.kaggle.com\/prateekagnihotri\/efficientnet-keras-train-qwk-loss-augmentation) - Notebook to train efficientnet model using quadratic weighted kappa and a lot of augmentation <br>\n[Efficientnet_keras_infernce (+ TTA)](https:\/\/www.kaggle.com\/prateekagnihotri\/efficientnet-keras-infernce-tta) - Inference kernel for above trained model","52d668a1":"## ResNext Model","ad230729":"[Original](https:\/\/www.kaggle.com\/rohitsingh9990\/panda-resnext-inference)","5279c1f1":"Note: If you like my work, please, upvote \u263a"}}