{"cell_type":{"2b3808a6":"code","3670b5b2":"code","80c7a690":"code","e2822476":"code","ee051805":"code","44e99386":"code","95970e35":"code","f45921fb":"code","51cdd216":"code","7c152f71":"code","78124c82":"code","981c1c9e":"code","f31c5ff4":"code","a428a32b":"code","d7482c2d":"code","79c9198b":"code","8aeaf564":"markdown","04649625":"markdown","d61b10d2":"markdown","cd9a0262":"markdown","85c53c50":"markdown","d5dc7fac":"markdown","987b6a2d":"markdown","7430aead":"markdown","9bb6461d":"markdown","61cd63dd":"markdown","def83df2":"markdown"},"source":{"2b3808a6":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3670b5b2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.datasets import load_breast_cancer\nfrom sklearn.datasets import load_digits\nfrom sklearn import metrics\n%matplotlib inline\nimport os\nimport warnings\nwarnings.filterwarnings('ignore')","80c7a690":"cancer=load_breast_cancer()\ndigits=load_digits()","e2822476":"data=cancer","ee051805":"data","44e99386":"df=pd.DataFrame(data=np.c_[data['data'],data['target']],  columns=list(data['feature_names'])+['target'])\ndf['target']=df['target'].astype('uint16')","95970e35":"df","f45921fb":"x=df.drop('target',axis=1)\ny=df[['target']]\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.2,random_state=99)","51cdd216":"print(x_train.shape)\nprint(y_train.shape)\nprint(x_test.shape)\nprint(y_test.shape)","7c152f71":"print(y_train.mean())\nprint(y_test.mean())","78124c82":"shallow_tree=DecisionTreeClassifier(max_depth=2,random_state=99)","981c1c9e":"shallow_tree.fit(x_train,y_train)\ny_pred=shallow_tree.predict(x_test)\nscore=metrics.accuracy_score(y_test,y_pred)\nscore","f31c5ff4":"estimators=list(range(1,275,15))\nabc_scores=[]\nfor n in estimators:\n    ABC=AdaBoostClassifier(base_estimator=shallow_tree, n_estimators=n)\n    \n    ABC.fit(x_train,y_train)\n    y_pred=ABC.predict(x_test)\n    score=metrics.accuracy_score(y_test,y_pred)\n    abc_scores.append(score)","a428a32b":"abc_scores","d7482c2d":"max(abc_scores)","79c9198b":"plt.plot(estimators, abc_scores)\nplt.xlabel('n_estimators')\nplt.ylabel('accuracy')\nplt.show()","8aeaf564":"checking the average cancer rates in train and test data,should be comparable","04649625":"Trying base estimator: a weak learner with max_depth=2","d61b10d2":"Fit the shallow decision tree","cd9a0262":"We can see the maximum accuracy of the model is 98.245","85c53c50":"We will use the breast cancer dataset in which the target variable has 1 if the person has cancer and 0 otherwise. Let's load the data","d5dc7fac":"Now plotting the Accuracy vs Estimators","987b6a2d":"Importing the packages","7430aead":"Exploring the breast cancer dataset and try to train the model to predict if the person is having breast cancer or not.Start with a decision tree with maximum depth = 2. then build an adaboost ensemble with 50 trees with a step of 3 and compare the performance with the before one.","9bb6461d":"Now,Code to calculate the accuracy of the AdaBoost models as we increase the number of trees from 1 to 50 with a step of 3 in the lines 'estimators = list(range(1, 50, 3))' 'for n_est in estimators:' We finally end up with the accuracy of all the models in a single list abc_scores.","61cd63dd":"adaboost Experiments","def83df2":"Converting the Data to DataFrame"}}