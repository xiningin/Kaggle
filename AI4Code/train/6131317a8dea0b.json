{"cell_type":{"bd05f9f7":"code","201b3981":"code","b9e8e384":"code","796c02ea":"code","bed4ca4d":"code","75dd2d2b":"code","5b9d9c61":"code","4f263cd9":"code","d0ab1bce":"code","2a315752":"code","e476069b":"code","2ae5b8c5":"code","616bf2a6":"code","838d5056":"code","2ffe5a11":"code","e1ffd6d2":"code","b6e8ef65":"code","d6dc4ca8":"code","0ae3fcc3":"code","543192f0":"code","174677f6":"code","89956b59":"code","ceb113ad":"code","3d4161d2":"code","bccb2e6f":"code","752fc285":"code","e7881979":"code","7efe9897":"code","65eaef8c":"code","8cb5f114":"code","0c4de460":"code","adc05f81":"code","4e549498":"code","d360c31e":"code","387be606":"code","740af07e":"code","4947867f":"code","bceea9e6":"code","08f39de4":"code","71501948":"code","7c278c52":"code","c57ced41":"code","deb4513e":"code","63d2e9f3":"code","579a8d02":"code","654905f2":"code","96097a2c":"code","0d9df410":"code","0ba5ad67":"code","b3e2196e":"code","e73c4675":"code","d7ee1417":"code","9dfba834":"code","42b46367":"code","12ba6ce5":"code","2a673aea":"code","815df725":"code","05042180":"code","1a08b10c":"code","07c0be47":"code","77a062f8":"code","0c41c864":"code","125f5dd5":"code","23d662c2":"code","7bf63353":"code","60a49ca6":"code","fd1520e8":"code","a8ed73d3":"code","8bd332d6":"code","392ef13e":"code","6e1e0962":"code","2493439c":"code","4d8f6abd":"code","435cd01c":"code","54937815":"code","a1477ba0":"code","699dd507":"code","71d906a7":"code","84d3aa86":"code","52744da8":"code","3eaffa7d":"code","33623316":"code","53d89c95":"code","72e6ee57":"code","e4a76e27":"code","c0d30624":"code","6ac9bd46":"code","7ac4e74b":"code","74098a2e":"code","6679fa9a":"markdown","ff49cb50":"markdown","2dc144fd":"markdown","67d58a2a":"markdown","a2fee09c":"markdown","2a1b6e4b":"markdown","22102246":"markdown","abd5ea5b":"markdown","9c11fc91":"markdown","8140d0e4":"markdown","c3cc9f36":"markdown","438a89af":"markdown","63ed8878":"markdown","c399e190":"markdown","196c9615":"markdown","0ef95578":"markdown","c8b29210":"markdown","45ac1855":"markdown","f9533520":"markdown"},"source":{"bd05f9f7":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\nimport plotly.express as px\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.subplots import make_subplots\n","201b3981":"data = pd.read_csv('\/kaggle\/input\/fish-market\/Fish.csv')\ndata.head(2)","b9e8e384":"df = data._get_numeric_data()\n\nfrom sklearn import preprocessing\nscaler = preprocessing.StandardScaler().fit_transform(df)\nminmax = preprocessing.MinMaxScaler().fit_transform(df)\nrobustscaler = preprocessing.RobustScaler().fit_transform(df) \npower = preprocessing.PowerTransformer().fit_transform(df) ","796c02ea":"scaler = pd.DataFrame(scaler)\nscaler.describe()\n\nminmax = pd.DataFrame(minmax)\nminmax.describe()\n\n\nrobustscaler = pd.DataFrame(robustscaler)\npower = pd.DataFrame(power)\n","bed4ca4d":"df.plot(kind='box')\nscaler.plot(kind='box')\n# minmax.plot(kind='box')\n# robustscaler.plot(kind='box')       # 0-100 -   0-5  95-100      0-10 - 9 \/ 1  mean = 10     10 records df wesit = mean sum 10 \/ 10 ==   sum8\/8 = 9\npower.plot(kind='box')","75dd2d2b":"df['Weight'].plot(kind='hist')  #mean and std     because of an outlier","5b9d9c61":"scaler[0].plot(kind='hist')","4f263cd9":"power[0].plot(kind='hist')","d0ab1bce":"(df['Weight'][1]-df['Weight'].mean())\/df['Weight'].std()","2a315752":"scaler[0]","e476069b":"np.round(pd.DataFrame(scaler)[3].std(),2)","2ae5b8c5":"def flag(x):\n    if x == 'Bream':\n        return 'Yess'\n    else:\n        return 'Noo'","616bf2a6":"data['Flag'] = data['Species'].apply(lambda x : flag(x))","838d5056":"data.isnull().sum()","2ffe5a11":"# data['Flag']= np.where((data['Species']=='Bream') | (data['Species']=='Pike'),'YESS','NOO')\ndata[['Species','Flag']].drop_duplicates()","e1ffd6d2":"weight_df = data.groupby(['Species'],as_index=False).agg({'Weight':['mean','min','max','std'],\n                                                         'Length1':['mean'],\n                                                         \"\":[]})\n\n# weight_df.columns = ['Species','spe_mean','spe_min','Spe_max','sp_std']\n# weight_df.columns = ['Species',  'spe_min','spe_mean','spe_max', 'spe_std']\nweight_df, ","b6e8ef65":"student = {'stu_id':[1,2,3,4,5],\n          'stu_name':['a','b','c','d','q']}\n\nstudent = pd.DataFrame(student)\nstudent","d6dc4ca8":"course_name = {'stu_id':[1,2,3,4],\n          'course_name':['cs','ece','cs','ce']}\n\ncourse = pd.DataFrame(course_name)\ncourse","0ae3fcc3":"# Inner join\ncourse.merge(student, on = ['stu_id'], how='right')","543192f0":"student.merge(course, on = ['stu_id'], how='left')","174677f6":"# Now if we have to read only two columns = Species and Weight\n\n\n# Case 1 : Use of usecols is important if we have large dataset and less memory so we read only required columns only\n\nshort_df = pd.read_csv('\/kaggle\/input\/fish-market\/Fish.csv', usecols = ['Species','Weight'])\nprint(short_df.head(2))\n\n# Case 2 : Use of Chunksize that divides are data into chunks hence we can do required operation in chunks data\nshort_df = pd.read_csv('\/kaggle\/input\/fish-market\/Fish.csv', chunksize = 100) # it will divides data into batches of each size 100\nfor i in short_df:\n    print(i.shape)\n    ","89956b59":"print(data.info())\n# This hows about each column name total non-null values count and columns Dtype\n\n# Various Dtypes : \n#     a:) Object = string, categorical\n#     b)  Float64 = float number\n#     c) int64 = integer\n\n#######################################################\n# If we need to change weight dtype float64 to int \ndata['Weight'] = data['Weight'].astype('int')\nprint(data.info())","ceb113ad":"# This function gives us Descriptive Statistics about data i.e mean, min, max, quantile, std range in Continuous colulmns\n\nprint(data.describe())\n\n##########################################################################################################################\n# This function gives us Descriptive Statistics about Count, Unique count, top value in Categoricals colulmns\n\ndata.describe(include='O')\n\n","3d4161d2":"### Describe give us count of Non null counts what about counts of Null then ?\ndata.isnull().sum()\n\n# we can see columns has zero null values","bccb2e6f":"# If we have to find frequency distribution in Categorical columns , we will use value_counts()\ndata['Species'].value_counts()","752fc285":"\n# Count of Number of Speicies\nplt.figure(figsize=(10,5))\n# sns.countplot(data['Species'])\nax = sns.countplot(data['Species'])\nfor p in ax.patches:\n    h = p.get_height()\n    w = p.get_width()\/2\n    ax.text(p.get_x()+w, h+1,\n            '{:1}'.format(h),\n           ha=\"left\")\nplt.show()","e7881979":"data['Weight'].mean()","7efe9897":"data.groupby(['Species'],as_index=False)['Weight'].median()","65eaef8c":"weight_mean = data.groupby(['Species'],as_index=False)['Weight'].mean()\n\nfig = px.bar(weight_mean, x='Species', y='Weight',title=\"Average Weight Per Species\"\n            )\nfig.update_traces(marker_color= ['cyan','red','green','lightpink','blue','gray'])\nfig.update_layout(width=700, height=500, bargap=0.05)\nfig.show( )","8cb5f114":"# Lets find Skewness,mean,median of each continous column  which will give us idea wheather how much close data of each \n# near to Gaussian Distribution\n# data._get_numeric_data().columns\nfor i  in data.select_dtypes(include=['int','float']):\n    print(f'Column {i} Mean = ',np.round(data[i].mean(),2),' Median = ',np.round(data[i].median(),2), ' Skew = ',np.round(data[i].skew(),2))\n    if (data[i].skew() > 1) or (data[i].skew()<-1):\n        print(f'                                                    {i} this column need outlier check')","0c4de460":"## Box Plot best graph to identify outlier\n\nfig = px.box(\n    data, \n    x=\"Weight\",\n    title = \"Box Plot\", \n   \n)\nfig.update_layout(height=300, width=700, title_text=\"Side By Side Subplots\")\nfig.show()","adc05f81":"# For Flag , we are taking 95 percentile as upper limit means, any value greather then this upper limit will be treated as outleir\n# again this upper limit depends on our decision or what business demands \nupper_limit = np.percentile(data['Weight'],95)\nprint(upper_limit)\n\ndata['Outlier_flag'] = np.where(data['Weight'] > upper_limit,'Outlier', 'Not_Outlier')\ndata['Outlier_flag'].value_counts()\n\n# We see there are 7 datapoints as outlier lets print those data points\n\ndata[data['Outlier_flag']=='Outlier']","4e549498":"data['New_weight'] = np.where(data['Weight'] > upper_limit,upper_limit, data['Weight'])\n\ngo_fig = go.Figure()\n\n#Use X attribute instead of Y to make plot horizontal\nobj_one = go.Box(x = data['Weight'],name='Weight')\nobj_two = go.Box(x = data['New_weight'], name='New_weight')\n\ngo_fig.add_trace(obj_one)\ngo_fig.add_trace(obj_two)\ngo_fig.update_layout(height=400, width=800, title_text=\"Side By Side Subplots\")\ngo_fig.show()","d360c31e":"## Now lets check outlier at Species level\n\nfig = px.box(\n    data, \n    x=\"New_weight\",\n    title = \"Box Plot\", \n    color = 'Species'#Setting up title attribute\n   \n)\nfig.update_layout(height=400, width=800, title_text=\"Side By Side Subplots\")\nfig.show()\n\n### Still we can see outlier in SPecies \" Roach \", let do capping for it also\n\nupper_limit = np.percentile(data[data['Species']=='Roach']['Weight'],95)\ndata['New_weight'] = np.where((data['Species']=='Roach')&(data['New_weight'] > upper_limit), upper_limit, data['New_weight'])\n\n\nfig = px.box(\n    data, \n    x=\"New_weight\",\n    title = \"Box Plot\", \n    color = 'Species'#Setting up title attribute\n   \n)\nfig.update_layout(height=400, width=800, title_text=\"Updated without Outlier at Species level\")\nfig.show()\n\n# and we can drop original weight column\ndata.drop(columns='Weight',inplace=True)","387be606":"# Lets get numeric column \nnumeric_df = data._get_numeric_data()\ncor_mat = numeric_df.corr()\nsns.heatmap(data=cor_mat,annot=True)\n\n# We can Infer many columns are highly correlated to each other and it may lead to Multicollinearity , to remove it, we will do Variance Inflation Factor Method","740af07e":"# Import library for VIF\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\n\n\ndef calc_vif(X):\n\n    # Calculating VIF\n    vif = pd.DataFrame()\n    vif[\"variables\"] = X.columns\n    vif[\"VIF\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\n\n    return(vif)\n\nX = numeric_df.iloc[:,:-1]  #### HERE we are not taking New_Weight as it will be our target Variable\nprint(calc_vif(X))\n\n## We can VIF > 20 and we see lenght variables as maximum values, lets combine all three length with average ,THis thing comes with understanding\n## about data\nprint('-------------------------------------')\nprint(' Lets Drop one Length 1 column')\nprint('-------------------------------------')\nprint(calc_vif(X.drop(columns=['Length1'])))\n\n\nprint('-------------------------------------')\nprint(' Lets combine all three length with average')\nprint('-------------------------------------')\nnumeric_df['Length'] = (numeric_df['Length1'] + numeric_df['Length2'] + numeric_df['Length3']) \/ 3\nX = numeric_df.drop(columns='New_weight')\nprint(calc_vif(X.drop(columns=['Length1','Length2','Length3'])))\n\n\nprint('-------------------------------------')\nprint(' Though VIF is still hight, further i can take volume of Length, width, Height ')\nprint('-------------------------------------')\nnumeric_df['Volume'] = (numeric_df['Length']* numeric_df['Width'] + numeric_df['Height'])\nX = numeric_df.drop(columns='New_weight')\nprint(calc_vif(X.drop(columns=['Length1','Length2','Length3','Width','Length'])))\n\n","4947867f":"# Lets apply changes to original data\n# 1) New Length\n# 2) Volume\n# 3) In last we have only two Volume and Height\n\ndata['Length'] = (data['Length1'] + data['Length2'] + data['Length3']) \/ 3\ndata['Volume'] = (data['Length']* data['Width'] + data['Height'])\ndata.drop(columns=['Length1','Length2','Length3','Width','Length','Height'], inplace=True)\n\n## Lets find again Heat Map\nnumeric_df = data._get_numeric_data()\ncor_mat = numeric_df.corr()\nsns.heatmap(data=cor_mat,annot=True)","bceea9e6":"# lets remove outlier flag, we dont need this\n# data.drop(columns='Outlier_flag', inplace=True)\nfrom scipy.stats import ttest_ind_from_stats\n\nSpecies_count = pd.DataFrame(data['Species'].value_counts())\nSpecies_count.columns = ['Count']\nSpecies_count['Species'] = Species_count.index\n\nstatistic_report = data.groupby(['Species'],as_index=False).agg({'New_weight':['mean','var']})\nstatistic_report.columns = ['Species','mean','Variance']\nstatistic_report = statistic_report.merge(Species_count,on='Species')\n\nttest_ind_from_stats(mean1=0.2, std1=np.sqrt(0.16), nobs1=150,\n                     mean2=0.225, std2=np.sqrt(0.17437), nobs2=200)\n","08f39de4":"\na= data[data['Species']=='Bream']['New_weight']\nb = data[data['Species']=='Perch']['New_weight']\nc = data[data['Species']=='Whitefish']['New_weight']\nfrom scipy.stats import f_oneway\nf_oneway(a, b,c)","71501948":"plt.plot(data['Length1'], label='1')\nplt.plot(data['Length2'], label='2')\nplt.plot(data['Length3'], label='3')\nplt.plot(data['Length'], label='F')\nplt.legend()\n","7c278c52":"import seaborn as sns\nplt.figure(figsize=(10,7))\ncor_mat= data.corr(method='spearman')\nsns.heatmap(cor_mat,annot=True, square=False)","c57ced41":"data.drop(columns=['Length1','Length2','Length3'], inplace=True)","deb4513e":"a = data.groupby(['Species'],as_index=False)[['Length']].min()\na = a.rename(columns={'Length':'Min_length'})\na","63d2e9f3":"data = pd.merge(data, a , on='Species')","579a8d02":"data.tail(20)","654905f2":"import seaborn as sns\nplt.figure(figsize=(10,7))\ncor_mat= data.corr()\nsns.heatmap(data=cor_mat,annot=True)","96097a2c":"a = data.groupby(['Species'],as_index=False)[['Width']].max()\na = a.rename(columns={'Width':'Max_width'})\na","0d9df410":"data = pd.merge(data, a , on='Species')","0ba5ad67":"import seaborn as sns\nplt.figure(figsize=(10,7))\ncor_mat= data.corr()\nsns.heatmap(data=cor_mat,annot=True)","b3e2196e":"a = data.groupby(['Species'],as_index=False)[['Height']].mean()\na = a.rename(columns={'Height':'Height_mean'})\na","e73c4675":"data = pd.merge(data, a , on='Species')","d7ee1417":"import seaborn as sns\nplt.figure(figsize=(10,7))\ncor_mat= data.corr()\nsns.heatmap(data=cor_mat,annot=True)","9dfba834":"data['Volume'] =  data['Height'] * data['Width'] * data['Length']","42b46367":"import seaborn as sns\nplt.figure(figsize=(10,7))\ncor_mat= data.corr()\nsns.heatmap(data=cor_mat,annot=True)","12ba6ce5":"data.drop(columns=['Height','Width','Length'], inplace=True)","2a673aea":"import seaborn as sns\nplt.figure(figsize=(10,7))\ncor_mat= data.corr()\nsns.heatmap(data=cor_mat,annot=True)","815df725":"data","05042180":"data['Species'].value_counts()","1a08b10c":"# from sklearn.preprocessing import LabelEncoder\n# e = LabelEncoder()\n# data['Species']  = e.fit_transform(data['Species'])","07c0be47":"import seaborn as sns\nplt.figure(figsize=(10,7))\ncor_mat= data.corr(method='spearman')\nsns.heatmap(cor_mat,annot=True)","77a062f8":"result = pd.get_dummies(data['Species'])","0c41c864":"result","125f5dd5":"data","23d662c2":"data = pd.concat([result, data], axis=1)","7bf63353":"data","60a49ca6":"import seaborn as sns\nplt.figure(figsize=(10,7))\ncor_mat= data.corr(method='spearman')\nsns.heatmap(cor_mat ,annot=True)","fd1520e8":"data.drop(columns=['Species', 'Bream','Perch'], inplace=True)","a8ed73d3":"data","8bd332d6":"X = data.drop(columns=['Weight'])\nY = data['Weight']","392ef13e":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=7)\nprint(X_train.shape)\nprint(X_test.shape)\nprint(Y_train.shape)\nprint(Y_test.shape)","6e1e0962":"from sklearn.preprocessing import PowerTransformer\nfrom sklearn.preprocessing import MinMaxScaler\n\npw=PowerTransformer()\nms=MinMaxScaler()\n\nX_train=ms.fit_transform(X_train)\nX_test=ms.transform(X_test)\n\nY_train=Y_train.values.reshape(-1,1)\nY_test=Y_test.values.reshape(-1,1)\n\nY_train=pw.fit_transform(Y_train)\nY_test=pw.transform(Y_test)","2493439c":"from sklearn.linear_model import LinearRegression\nfrom sklearn.metrics import mean_squared_error,r2_score\n\nmodel_lin=LinearRegression()\nmodel_lin.fit(X_train,Y_train)","4d8f6abd":"y_test_predictv=model_lin.predict(X_test)\n\nrmse=np.sqrt(mean_squared_error(Y_test,y_test_predictv))\n\nr2=r2_score(y_test_predictv,Y_test)\n\nprint('the values predicted has')\nprint('RMSE = {}'.format(rmse))\nprint('r2 Score= {}'.format(r2))","435cd01c":"from sklearn.linear_model import Ridge\n\nR_model=Ridge(alpha=0.1)\nR_model.fit(X_train,Y_train)\n\ny_test_predict=R_model.predict(X_test)\n\nrmse=np.sqrt(mean_squared_error(Y_test,y_test_predict))\nr2=r2_score(Y_test,y_test_predict)\n\nprint('RMSE={}'.format(rmse))\nprint('r2 score={}'.format(r2))","54937815":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import ElasticNet\nr_estimator=ElasticNet()\n\nparameters={'alpha':[0.001,0.01,0.05,0.1,0.3,0.5,0.8,10,11,12],\n           'l1_ratio' : [0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9]}\ngrid=GridSearchCV(estimator=r_estimator,param_grid=parameters,cv=7,n_jobs=11)\ngrid.fit(X_train,Y_train)","a1477ba0":"grid.best_params_","699dd507":"elastica_model = ElasticNet(alpha=0.001,\n                           l1_ratio=0.9\n                          )\nelastica_model.fit(X_train,Y_train)\n\ny_test_predict=elastica_model.predict(X_test)\n\nrmse=np.sqrt(mean_squared_error(Y_test,y_test_predict))\n\nr2=r2_score(Y_test,y_test_predict)\n\nprint('RMSE={}'.format(rmse))\n\nprint('r2 score={}'.format(r2))\n","71d906a7":"plt.plot(Y_test)\nplt.plot(y_test_predict)","84d3aa86":"X_train,X_test,Y_train,Y_test=train_test_split(X,Y,test_size=0.2,random_state=7)\n","52744da8":"from sklearn.model_selection import KFold\nerr=[]\ny_pred=[]\n\nX_test = ms.transform(X_test)\nfold=KFold(n_splits=7)\nfor train_index, test_index in fold.split(X_train,Y_train):\n    x_train, x_test = X_train.iloc[train_index], X_train.iloc[test_index]\n    y_train, y_test = Y_train.iloc[train_index], Y_train.iloc[test_index]\n    \n    \n    x_train = ms.fit_transform(x_train)\n    x_test = ms.transform(x_test)\n    # Y_train = pd.DataFrame(Y_train)  \n    y_train = y_train.values.reshape(-1,1)\n    y_test = y_test.values.reshape(-1,1)\n    y_train = pw.fit_transform(y_train)\n    y_test = pw.transform(y_test)\n\n    \n    \n    \n    m1 = ElasticNet(alpha=0.001,\n                           l1_ratio=0.9)\n    m1.fit(x_train,y_train)\n    preds = m1.predict(x_test)\n\n    print(\"err: \",np.sqrt(mean_squared_error(y_test,preds)))\n    print(\"r2square: \",r2_score(y_test,preds))\n    err.append(np.sqrt(mean_squared_error(y_test,preds)))\n    test_pred = m1.predict(X_test)\n    test_pred = test_pred.reshape(-1,1)\n    test_pred = pw.inverse_transform(test_pred)\n    y_pred.append(test_pred)\nnp.mean(err)\n","3eaffa7d":"# y_pred = np.mean(y_pred,0)\n# y_pred","33623316":"Y_test = Y_test.reset_index(drop=True)","53d89c95":"plt.plot(Y_test)\nplt.plot(y_pred[1])","72e6ee57":"len(y_pred)","e4a76e27":"for i in  range(0,len(y_pred)):\n# y_pred = np.mean(y_pred, 0)\n    rmse=np.sqrt(mean_squared_error(Y_test,y_pred[i]))\n\n    r2=r2_score(Y_test,y_pred[i])\n\n    print('RMSE= {}'.format(rmse))\n\n    print('r2 score= {}'.format(r2))","c0d30624":"plt.plot(Y_test)\nplt.plot(y_pred[6])","6ac9bd46":"y_predm = np.mean(y_pred, 0)","7ac4e74b":"rmse=np.sqrt(mean_squared_error(Y_test,y_predm))\n\nr2=r2_score(Y_test,y_predm)\n\nprint('RMSE= {}'.format(rmse))\n\nprint('r2 score= {}'.format(r2))","74098a2e":"#corelation matrix.\n# cor_mat= data.corr()\n# mask = np.array(cor_mat)\n# mask[np.tril_indices_from(mask)] = False\n# fig=plt.gcf()\n# fig.set_size_inches(15,5)\n# sns.heatmap(data=cor_mat,mask=mask,square=True,annot=True,cbar=True)","6679fa9a":"# Inferential Statistics  ","ff49cb50":"## Variance Inflation Factor Method","2dc144fd":"## Lets Flag this outlier and grabs those data points","67d58a2a":"## This marks the end of Correlation of Numeric Columns","a2fee09c":"# Visualizations and also groupby operations","2a1b6e4b":"## Importing important libraries ","22102246":"# Now Lets Find How are SPecies ( Categorical column ) is related to our Target Column that is New_weight\n# For This we will Use T - Test","abd5ea5b":"### VIF starts at 1 and has no upper limit\n### VIF = 1, no correlation between the independent variable and the other variables\n### VIF exceeding 5 or 10 indicates high multicollinearity between this independent variable and the others","9c11fc91":"# Data Reading and more digging into Pd.read_Csv","8140d0e4":"## Lets replace those weight outlier with upper limit and check again box plot to quickly see outlier\n\n","c3cc9f36":"![image.png](attachment:95267b49-13fb-4d92-a5b2-30b88ed1d2f1.png)","438a89af":"## Correlation Test","63ed8878":"# Various Relationship test between Continous and Categorical columns","c399e190":"# The Complete Guide to Linear Regression in Python","196c9615":"# Basic operation to get birds view info about data","0ef95578":"## Outlier check","c8b29210":"## Count of Number of Speicies","45ac1855":"## Average Weight Per Speicies","f9533520":"> ## Question if we have large dataset suppose dataset of size 8 Gb and we have memory only 4 GB, hence we cannot load complete data so what are the alternative to do so ??\n"}}