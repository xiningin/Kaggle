{"cell_type":{"a3187531":"code","c23fc8db":"code","b43d6407":"code","b1f1ebc3":"code","04af919e":"code","a1869a51":"code","d780f689":"code","46d8357e":"code","379128cb":"code","3571ad43":"code","f674e79c":"code","fa11c7b3":"code","b143c8d2":"code","46984d08":"code","107ac8bc":"code","c4b9ab76":"code","dd014101":"code","d35d8367":"code","e0245a6c":"code","4296afcf":"code","1d0f19f3":"code","df5137b6":"code","0f4d6700":"code","5f8849a5":"code","afd5a291":"code","e7b91f4c":"code","535fd1d1":"code","4178e698":"code","b63d4efa":"code","ab815640":"code","fb80e932":"code","8bfc9f34":"code","7ce88832":"markdown","f435a87b":"markdown"},"source":{"a3187531":"%%capture\n!python -m spacy download en","c23fc8db":"import os\nimport re\nimport time\nimport math\nimport random\nimport unicodedata\n\nimport numpy as np\nimport pandas as pd\n\nfrom tqdm import tqdm\n\nimport spacy\n\nfrom sklearn.model_selection import train_test_split\n\nimport torch\nfrom torch import nn, optim\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import DataLoader, Dataset\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt","b43d6407":"SEED = 28\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.backends.cudnn.deterministic = True","b1f1ebc3":"def read_data():\n    \"\"\"Read the data.\"\"\"\n    news1_df = pd.read_csv('..\/input\/news-summary\/news_summary.csv', encoding='latin-1', usecols=['headlines', 'text'])\n    news2_df = pd.read_csv('..\/input\/news-summary\/news_summary_more.csv', encoding='latin-1')\n    \n    return pd.concat([news1_df, news2_df], axis=0).reset_index(drop=True)","04af919e":"data_df = read_data()\ndata_df.head()","a1869a51":"data_df.shape","d780f689":"plt.figure(figsize=(12, 6))\nplt.style.use('ggplot')\nplt.subplot(1, 2, 1)\nsns.distplot(data_df['headlines'].str.split().apply(len))\nplt.title('Distribution of headlines sentences length')\nplt.xlabel('Length')\n\nplt.style.use('ggplot')\nplt.subplot(1, 2, 2)\nsns.distplot(data_df['text'].str.split().apply(len))\nplt.title('Distribution of text sentences length')\nplt.xlabel('Length')\nplt.show()","46d8357e":"seq_len_headline = 15\nseq_len_text = 60","379128cb":"train_df, valid_df = train_test_split(data_df, test_size=0.1, shuffle=True, random_state=28)\n\ntrain_df = train_df.reset_index(drop=True)\nvalid_df = valid_df.reset_index(drop=True)\n\nprint(train_df.shape)\nprint(valid_df.shape)","3571ad43":"for i in range(len(train_df)-5, len(train_df)):\n    print(f'HEADLINE:\\n{train_df.iloc[i][\"headlines\"]},\\nTEXT:\\n{train_df.iloc[i][\"text\"]}\\n{\"=\"*112}')","f674e79c":"class Vocabulary:\n    def __init__(self, freq_threshold=2, language='en', preprocessor=None, reverse=False):\n        self.itos = {0: \"<pad>\", 1: \"<sos>\", 2: \"<eos>\", 3: \"<unk>\"}\n        self.stoi = {\"<pad>\": 0, \"<sos>\": 1, \"<eos>\": 2, \"<unk>\": 3}\n        self.tokenizer = spacy.load(language)\n        self.freq_threshold = freq_threshold\n        self.preprocessor = preprocessor\n        self.reverse = reverse\n\n    def __len__(self):\n        return len(self.itos)\n\n    def tokenize(self, text):\n        if self.reverse:\n            return [token.text.lower() for token in self.tokenizer.tokenizer(text)][::-1]\n        else:\n            return [token.text.lower() for token in self.tokenizer.tokenizer(text)]\n\n    def build_vocabulary(self, sentence_list):\n        frequencies = {}\n        idx = len(self.itos)\n\n        for sentence in sentence_list:\n            # Preprocess the sentence using given preprocessor.\n            if self.preprocessor:\n                sentence = self.preprocessor(sentence)\n\n            for word in self.tokenize(sentence):\n                if word in frequencies:\n                    frequencies[word] += 1\n                else:\n                    frequencies[word] = 1\n\n                if frequencies[word] == self.freq_threshold:\n                    self.stoi[word] = idx\n                    self.itos[idx] = word\n                    idx += 1\n\n    def numericalize(self, text):\n        tokenized_text = self.tokenize(text)\n\n        return [\n            self.stoi[token] if token in self.stoi else self.stoi[\"<unk>\"]\n            for token in tokenized_text\n        ]","fa11c7b3":"# Converts the unicode file to ascii\ndef unicode_to_ascii(s):\n    return ''.join(c for c in unicodedata.normalize('NFD', s) if unicodedata.category(c) != 'Mn')\n\ndef preprocess_sentence(text):\n    text = unicode_to_ascii(text.lower().strip())\n\n    # creating a space between a word and the punctuation following it\n    # eg: \"he is a boy.\" => \"he is a boy .\"\n    # Reference:- https:\/\/stackoverflow.com\/questions\/3645931\/python-padding-punctuation-with-white-spaces-keeping-punctuation\n    text = re.sub(r\"([?.!,\u00bf])\", r\" \\1 \", text)\n    text = re.sub(r'[\" \"]+', \" \", text)\n\n    # replacing everything with space except (a-z, A-Z, \".\", \"?\", \"!\", \",\")\n    text = re.sub(r\"[^a-zA-Z?.!,\u00bf]+\", \" \", text)\n\n    text = text.strip()\n    \n    text = re.sub(\"(\\\\t)\", ' ', text)  #remove escape charecters\n    text = re.sub(\"(\\\\r)\", ' ', text)\n    text = re.sub(\"(\\\\n)\", ' ', text)\n    text = re.sub(\"(__+)\", ' ', text)   #remove _ if it occors more than one time consecutively\n    text = re.sub(\"(--+)\", ' ', text)   #remove - if it occors more than one time consecutively\n    text = re.sub(\"(~~+)\", ' ', text)   #remove ~ if it occors more than one time consecutively\n    text = re.sub(\"(\\+\\++)\", ' ', text)   #remove + if it occors more than one time consecutively\n    text = re.sub(\"(\\.\\.+)\", ' ', text)   #remove . if it occors more than one time consecutively\n    text = re.sub(r\"[<>()|&\u00a9\u00f8\\[\\]\\'\\\",;?~*!]\", ' ', text) #remove <>()|&\u00a9\u00f8\"',;?~*!\n    text = re.sub(\"(mailto:)\", ' ', text)  #remove mailto:\n    text = re.sub(r\"(\\\\x9\\d)\", ' ', text)  #remove \\x9* in text\n    text = re.sub(\"([iI][nN][cC]\\d+)\", 'INC_NUM', text)  #replace INC nums to INC_NUM\n    text = re.sub(\"([cC][mM]\\d+)|([cC][hH][gG]\\d+)\", 'CM_NUM', text)  #replace CM# and CHG# to CM_NUM\n    text = re.sub(\"(\\.\\s+)\", ' ', text)  #remove full stop at end of words(not between)\n    text = re.sub(\"(\\-\\s+)\", ' ', text)  #remove - at end of words(not between)\n    text = re.sub(\"(\\:\\s+)\", ' ', text)  #remove : at end of words(not between)\n    text = re.sub(\"(\\s+.\\s+)\", ' ', text)  #remove any single charecters hanging between 2 spaces\n\n    #Replace any url as such https:\/\/abc.xyz.net\/browse\/sdf-5327 ====> abc.xyz.net\n    try:\n        url = re.search(r'((https*:\\\/*)([^\\\/\\s]+))(.[^\\s]+)', text)\n        repl_url = url.group(3)\n        text = re.sub(r'((https*:\\\/*)([^\\\/\\s]+))(.[^\\s]+)',repl_url, text)\n    except:\n        pass #there might be emails with no url in them\n\n    text = re.sub(\"(\\s+)\",' ',text) #remove multiple spaces\n    text = re.sub(\"(\\s+.\\s+)\", ' ', text) #remove any single charecters hanging between 2 spaces\n    return text\n    return w","b143c8d2":"%%time\n# Build vocab using training data\nfreq_threshold = 1\nheadline_vocab = Vocabulary(freq_threshold=freq_threshold, language=\"en\", preprocessor=preprocess_sentence)\ntext_vocab = Vocabulary(freq_threshold=freq_threshold, language=\"en\", preprocessor=preprocess_sentence)\n\n# build vocab for both english and german\nheadline_vocab.build_vocabulary(train_df[\"headlines\"].tolist())\ntext_vocab.build_vocabulary(train_df[\"text\"].tolist())","46984d08":"class CustomTranslationDataset(Dataset):    \n    def __init__(self, df, headline_vocab, text_vocab):\n        super().__init__()\n        self.df = df\n        self.headline_vocab = headline_vocab\n        self.text_vocab = text_vocab\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def _get_numericalized(self, sentence, vocab):\n        \"\"\"Numericalize given text using prebuilt vocab.\"\"\"\n        numericalized = [vocab.stoi[\"<sos>\"]]\n        numericalized.extend(vocab.numericalize(sentence))\n        numericalized.append(vocab.stoi[\"<eos>\"])\n        return numericalized\n\n    def __getitem__(self, index):\n        headline_numericalized = self._get_numericalized(self.df.iloc[index][\"headlines\"], self.headline_vocab)\n        text_numericalized = self._get_numericalized(self.df.iloc[index][\"text\"], self.text_vocab)\n\n        return torch.tensor(text_numericalized), torch.tensor(headline_numericalized)","107ac8bc":"class CustomCollate:\n    def __init__(self, pad_idx):\n        self.pad_idx = pad_idx\n\n    def __call__(self, batch):\n        src = [item[0] for item in batch]\n        src = pad_sequence(src, batch_first=False, padding_value=self.pad_idx)\n        \n        targets = [item[1] for item in batch]\n        targets = pad_sequence(targets, batch_first=False, padding_value=self.pad_idx)\n\n        return src, targets","c4b9ab76":"BATCH_SIZE = 256\n\n# Define dataset and dataloader\ntrain_dataset = CustomTranslationDataset(train_df, headline_vocab, text_vocab)\nvalid_dataset = CustomTranslationDataset(valid_df, headline_vocab, text_vocab)\n\ntrain_loader = DataLoader(\n    dataset=train_dataset,\n    batch_size=BATCH_SIZE,\n    num_workers=4,\n    shuffle=False,\n    collate_fn=CustomCollate(pad_idx=headline_vocab.stoi[\"<pad>\"])\n)\n\nvalid_loader = DataLoader(\n    dataset=valid_dataset,\n    batch_size=BATCH_SIZE,\n    num_workers=4,\n    shuffle=False,\n    collate_fn=CustomCollate(pad_idx=headline_vocab.stoi[\"<pad>\"])\n)","dd014101":"fun_text = np.vectorize(lambda x: text_vocab.itos[x])\nfun_headline = np.vectorize(lambda x: headline_vocab.itos[x])","d35d8367":"print(f\"Unique tokens in source (de) vocabulary: {len(text_vocab)}\")\nprint(f\"Unique tokens in target (en) vocabulary: {len(headline_vocab)}\")","e0245a6c":"device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\ndevice","4296afcf":"class Encoder(nn.Module):\n    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout=0.2):\n        super().__init__()\n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n        self.embedding = nn.Embedding(input_dim, emb_dim)\n        self.lstm = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout)\n        self.dropout = nn.Dropout(dropout)\n    \n    def forward(self, x):\n        x = self.embedding(x)\n        x = self.dropout(x)\n        outputs, (hidden_state, cell_state) = self.lstm(x)\n        \n        return hidden_state, cell_state\n\nclass Decoder(nn.Module):\n    def __init__(self, output_dim, emb_dim, hidden_dim, n_layers, dropout=0.2):\n        super().__init__()\n        self.output_dim = output_dim\n        self.hidden_dim = hidden_dim\n        self.n_layers = n_layers\n        self.embedding = nn.Embedding(output_dim, emb_dim)\n        self.lstm = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout)\n        self.dropout = nn.Dropout(dropout)\n        self.fc = nn.Linear(hidden_dim, output_dim)\n    \n    def forward(self, x, hidden_state, cell_state):\n        x = x.unsqueeze(0)\n        x = self.embedding(x)\n        x = self.dropout(x)\n        outputs, (hidden_state, cell_state) = self.lstm(x, (hidden_state, cell_state))\n        preds = self.fc(outputs.squeeze(0))\n        return preds, hidden_state, cell_state\n\nclass EncoderDecoder(nn.Module):\n    def __init__(self, encoder, decoder):\n        super().__init__()\n        self.encoder = encoder\n        self.decoder = decoder\n        \n        assert self.encoder.hidden_dim == decoder.hidden_dim\n        assert self.encoder.n_layers == decoder.n_layers\n    \n    def forward(self, x, y, teacher_forcing_ratio=0.75):\n        \n        target_len = y.shape[0]\n        batch_size = y.shape[1]\n        target_vocab_size = self.decoder.output_dim  # Output dim\n        \n        outputs = torch.zeros(target_len, batch_size, target_vocab_size).to(device)\n        \n        # Encode the source text using encoder\n        hidden_state, cell_state = self.encoder(x)\n        \n        # First input is <sos>\n        input = y[0,:]\n        \n        # Decode the encoded vector using decoder\n        for t in range(1, target_len):\n            output, hidden_state, cell_state = self.decoder(input, hidden_state, cell_state)\n            outputs[t] = output\n            teacher_force = random.random() < teacher_forcing_ratio\n            pred = output.argmax(1)\n            input = y[t] if teacher_force else pred\n        \n        return outputs","1d0f19f3":"# Initialize all models\ninput_dim = len(text_vocab)\noutput_dim = len(headline_vocab)\nemb_dim = 128\nhidden_dim = 256\nn_layers = 2\ndropout = 0.5\n\nencoder = Encoder(input_dim, emb_dim, hidden_dim, n_layers, dropout)\ndecoder = Decoder(output_dim, emb_dim, hidden_dim, n_layers, dropout)\nmodel = EncoderDecoder(encoder, decoder).to(device)","df5137b6":"# Initialized weights as defined in paper\ndef init_weights(m):\n    for name, param in m.named_parameters():\n        nn.init.uniform_(param.data, -0.08, 0.08)\n        \nmodel.apply(init_weights)","0f4d6700":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f'The model has {count_parameters(model):,} trainable parameters')","5f8849a5":"optimizer = optim.Adam(model.parameters(), lr=0.001)\ncriterion = nn.CrossEntropyLoss(ignore_index=headline_vocab.stoi[\"<pad>\"])\nsheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, factor=0.1, patience=3, min_lr=0.00001)","afd5a291":"def train(model, iterator, optimizer, criterion, clip):\n    model.train()\n    epoch_loss = 0\n    \n    for i, batch in tqdm(enumerate(iterator), total=len(iterator), position=0, leave=True):\n        src = batch[0].to(device)\n        trg = batch[1].to(device)\n\n        optimizer.zero_grad()\n        \n        output = model(src, trg)\n        \n        #trg = [trg len, batch size]\n        #output = [trg len, batch size, output dim]\n        \n        output_dim = output.shape[-1]\n        output = output[1:].view(-1, output_dim)\n        trg = trg[1:].view(-1)\n        \n        #trg = [(trg len - 1) * batch size]\n        #output = [(trg len - 1) * batch size, output dim]\n        \n        loss = criterion(output, trg)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n        optimizer.step()\n        epoch_loss += loss.item()\n        \n    return epoch_loss \/ len(iterator)","e7b91f4c":"def evaluate(model, iterator, criterion):\n    model.eval()    \n    epoch_loss = 0\n\n    with torch.no_grad():\n        for i, batch in tqdm(enumerate(iterator), total=len(iterator), position=0, leave=True):\n            src = batch[0].to(device)\n            trg = batch[1].to(device)\n\n            output = model(src, trg, 0) #turn off teacher forcing\n\n            #trg = [trg len, batch size]\n            #output = [trg len, batch size, output dim]\n\n            output_dim = output.shape[-1]\n            output = output[1:].view(-1, output_dim)\n            trg = trg[1:].view(-1)\n\n            #trg = [(trg len - 1) * batch size]\n            #output = [(trg len - 1) * batch size, output dim]\n\n            loss = criterion(output, trg)\n            epoch_loss += loss.item()\n        \n    return epoch_loss \/ len(iterator)","535fd1d1":"def inference(model, sentence):\n    model.eval()\n    result = []\n\n    with torch.no_grad():\n        sentence = sentence.to(device)\n        \n        hidden_state, cell_state = model.encoder(sentence)\n\n        # First input to decoder is \"<sos>\"\n        inp = torch.tensor([headline_vocab.stoi[\"<sos>\"]]).to(device)\n\n        # Decode the encoded vector using decoder until max length is reached or <eos> is generated.\n        for t in range(1, seq_len_headline):\n            output, hidden_state, cell_state = model.decoder(inp, hidden_state, cell_state)\n            pred = output.argmax(1)\n            if pred == headline_vocab.stoi[\"<eos>\"]:\n                break\n            result.append(headline_vocab.itos[pred.item()])\n            inp = pred\n            \n    return \" \".join(result)","4178e698":"def epoch_time(start_time, end_time):\n    elapsed_time = end_time - start_time\n    elapsed_mins = int(elapsed_time \/ 60)\n    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n    return elapsed_mins, elapsed_secs","b63d4efa":"for sample_batch in valid_loader:\n    break","ab815640":"N_EPOCHS = 80\nCLIP = 1\n\nbest_valid_loss = float('inf')\n\nsample_source = ' '.join([word for word in fun_text(sample_batch[0][:, 10]) if word not in [\"<pad>\", \"<sos>\", \"<eos>\"]])\nsample_target = ' '.join([word for word in fun_headline(sample_batch[1][:, 10]) if word not in [\"<pad>\", \"<sos>\", \"<eos>\"]])\n\nfor epoch in range(N_EPOCHS):\n    \n    start_time = time.time()\n    \n    train_loss = train(model, train_loader, optimizer, criterion, CLIP)\n    valid_loss = evaluate(model, valid_loader, criterion)\n    sheduler.step(valid_loss)\n    end_time = time.time()\n    \n    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n    \n    if valid_loss < best_valid_loss:\n        best_valid_loss = valid_loss\n        torch.save(model.state_dict(), 'best_model.pt')\n    \n    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n    print(f'\\t Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}')\n    print(f'\\t Val. Loss: {valid_loss:.3f} |  Val. PPL: {math.exp(valid_loss):7.3f}')\n    print(f'\\t Sample Source (German): {sample_source}')\n    print(f'\\t Sample Target (English): {sample_target}')\n    print(f'\\t Generated: {inference(model, sample_batch[0][:, 10].reshape(-1, 1))}\\n')","fb80e932":"# Load the best model.\nmodel_path = \".\/best_model.pt\"\nmodel.load_state_dict(torch.load(model_path))","8bfc9f34":"for idx in range(20):\n    print(f'ACTUAL TEXT: {\" \".join([word for word in fun_text(sample_batch[0][:, idx]) if word not in [\"<pad>\", \"<sos>\", \"<eos>\"]])}\\n')\n    print(f'ACTUAL HEADLINE: {\" \".join([word for word in fun_headline(sample_batch[1][:, idx]) if word not in [\"<pad>\", \"<sos>\", \"<eos>\"]])}\\n')\n    print(f'GENERATED BY MODEL: {inference(model, sample_batch[0][:, idx].reshape(-1, 1))}')\n    print(\"=\"*92)","7ce88832":"## Results","f435a87b":"## Modeling"}}