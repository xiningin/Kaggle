{"cell_type":{"e69e2b27":"code","3c57f04e":"code","a6bb82d5":"code","d938b76a":"code","31bce9d0":"code","ca7cab91":"markdown","3589015d":"markdown","5e49b5bf":"markdown","69e03842":"markdown","02ce242d":"markdown","4150ee88":"markdown"},"source":{"e69e2b27":"from math import sin\nfrom math import pi\nfrom numpy import arange\nfrom numpy import argmax\nfrom numpy.random import normal\nfrom matplotlib import pyplot","3c57f04e":"def objective(x, noise=0.1):\n\tnoise = normal(loc=0, scale=noise)\n\treturn (x**2 * sin(5 * pi * x)**6.0) + noise","a6bb82d5":"def init_population(start,end,incrment):\n    return arange(start,end,incrment)","d938b76a":"def plot_result(population,y,ynoise):\n    pyplot.scatter(population, ynoise)\n    # plot the points without noise\n    pyplot.plot(population, y)\n    # show the plot\n    pyplot.show()","31bce9d0":"# grid-based sample of the domain [0,1]\nX = init_population(0, 1, 0.01)\n# sample the domain without noise\ny = [objective(x, 0) for x in X]\n# sample the domain with noise\nynoise = [objective(x) for x in X]\n# find best result\nix = argmax(y)\nprint('Optima: x=%.3f, y=%.3f' % (X[ix], y[ix]))\n# plot the points with noise\nplot_result(X,y,ynoise)","ca7cab91":"## Bayesian Optmization\n\nBayesian Optimization is an approach that uses Bayes Theorem to direct the search in order to find the minimum or maximum of an objective function.\n\nIt is an approach that is most useful for objective functions that are complex, noisy, and\/or expensive to evaluate\nayesian optimization is a powerful strategy for finding the extrema of objective functions that are expensive to evaluate. [\u2026] It is particularly useful when these evaluations are costly, when one does not have access to derivatives, or when the problem at hand is non-convex.","3589015d":"## Problem Statement\n\n\nWe will use a multimodal problem with five peaks, calculated as:\n\n$$y = x^2 * \\sin(5 \\times \\pi \\times x)^6$$\n\nWhere x is a real value in the range $[0,1]$","5e49b5bf":"### Population Generation","69e03842":"### Objective Function","02ce242d":"We will augment this function by adding Gaussian noise with a mean of zero and a standard deviation of 0.1. This will mean that the real evaluation will have a positive or negative random value added to it, making the function challenging to optimize.","4150ee88":"Bayes Theorem is an approach for calculating the conditional probability of an event:\n\n$$ P(\\frac{A}{B}) = P(\\frac{B}{A}) \\times \\frac{P(A)}{P(B)} $$\n\nWe can simplify this calculation by removing the normalizing value of $P(B)$ and describe the conditional probability as a proportional quantity. This is useful as we are not interested in calculating a specific conditional probability, but instead in optimizing a quantity.\n\n$$P(\\frac{A}{B}) = P(\\frac{B}{A}) \\times P(A) $$\n\nThe conditional probability that we are calculating is referred to generally as the posterior probability; the reverse conditional probability is sometimes referred to as the likelihood, and the marginal probability is referred to as the prior probability; for example:\n\n$posterior = likelihood \\times prior $\nThis provides a framework that can be used to quantify the beliefs about an unknown objective function given samples from the domain and their evaluation via the objective function.\n\nWe can devise specific samples $(x_1, x_2, \u2026, x_n)$ and evaluate them using the objective function $f(x_i)$ that returns the cost or outcome for the sample xi. Samples and their outcome are collected sequentially and define our data D, e.g. $D = \\{ x_i, f(x_i), \u2026 xn, f(x_n)\\}$ and is used to define the prior. The likelihood function is defined as the probability of observing the data given the function $P(D | f)$. This likelihood function will change as more observations are collected.\n\n$$P(\\frac{f}{D}) = P(\\frac{D}{f}) \\times P(f)$$"}}