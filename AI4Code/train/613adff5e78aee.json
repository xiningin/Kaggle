{"cell_type":{"a3f3e318":"code","dd8b9511":"code","9f7ca1bc":"code","52065b55":"code","0adc03ba":"code","8e55a8ad":"code","7694faeb":"code","b621561b":"code","4fbf9403":"code","79338113":"code","7781a6bf":"code","c7fc870f":"code","42e134b9":"code","729cdcdc":"code","fdce0d84":"code","628b1f2b":"code","98cbf8fb":"code","66a14acb":"markdown","e369dcfc":"markdown","c2d29d25":"markdown","a9db3be1":"markdown","ddb83ab8":"markdown"},"source":{"a3f3e318":"import os, time, random, imageio\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nimport torchvision.transforms as transforms\nimport torchvision.utils as vutils\nfrom PIL import Image\nimport numpy as np\nimport matplotlib.pyplot as plt","dd8b9511":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","9f7ca1bc":"def set_seed(seed=2021):\n  random.seed(seed)\n  np.random.seed(seed)\n  torch.manual_seed(seed)\n  torch.cuda.manual_seed(seed)\n\nset_seed(2021)","52065b55":"class AnimeDataset(Dataset):\n  def __init__(self, data_dir, transforms=None):\n    self.data_dir = data_dir\n    self.transforms = transforms\n    self.img_names = [name for name in list(filter(lambda x: x.endswith(\".png\"), os.listdir(self.data_dir)))]\n\n  def __getitem__(self, index):\n    path_img = os.path.join(self.data_dir, self.img_names[index])\n    img = Image.open(path_img).convert('RGB')\n\n    if self.transforms is not None:\n      img = self.transforms(img)\n\n    return img\n\n  def __len__(self):\n    if len(self.img_names)==0:\n      raise Exception(\"\u6587\u4ef6\u5939\u4e2d\u6ca1\u6709\u56fe\u7247\")\n    return len(self.img_names)","0adc03ba":"class Generator(nn.Module):\n  def __init__(self, nz=100, ngf=128, nc=3):\n    super(Generator, self).__init__()\n    self.main = nn.Sequential(\n        nn.ConvTranspose2d(nz, ngf*8, 4, 1, 0, bias=False),\n        nn.BatchNorm2d(ngf*8),\n        nn.ReLU(True),\n        #(ngf*8, 4, 4)\n        nn.ConvTranspose2d(ngf*8, ngf*4, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ngf*4),\n        nn.ReLU(True),\n        #(ngf*4, 8, 8)\n        nn.ConvTranspose2d(ngf*4, ngf*2, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ngf*2),\n        nn.ReLU(True),\n        #(ngf*2, 16, 16)\n        nn.ConvTranspose2d(ngf*2, ngf, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ngf),\n        nn.ReLU(True),\n        #(ngf, 32, 32)\n        nn.ConvTranspose2d(ngf, nc, 4, 2, 1, bias=False),\n        nn.Tanh()\n        #(nc, 64, 64)\n    )\n\n  def forward(self, input):\n    return self.main(input)\n\n  def initialize_weight(self, w_mean=0., w_std=0.02, b_mean=1, b_std=0.02):\n    for m in self.modules():\n      classname = m.__class__.__name__\n      if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, w_mean, w_std)\n      elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, b_mean, b_std)\n        nn.init.constant_(m.bias.data, 0)","8e55a8ad":"class Discriminator(nn.Module):\n  def __init__(self, nc=3, ndf=128):\n    super(Discriminator, self).__init__()\n    self.main = nn.Sequential(\n        #(nc, 64, 64)\n        nn.Conv2d(nc, ndf, 4, 2, 1, bias=False),\n        nn.LeakyReLU(0.2, inplace=True),\n        #(ndf, 32, 32)\n        nn.Conv2d(ndf, ndf*2, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ndf*2),\n        nn.LeakyReLU(0.2, inplace=True),\n        #(ndf*2, 16, 16)\n        nn.Conv2d(ndf*2, ndf*4, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ndf*4),\n        nn.LeakyReLU(0.2, inplace=True),\n        #(ndf*4, 8, 8)\n        nn.Conv2d(ndf*4, ndf*8, 4, 2, 1, bias=False),\n        nn.BatchNorm2d(ndf*8),\n        nn.LeakyReLU(0.2, inplace=True),\n        #(ndf*8, 4, 4)\n        nn.Conv2d(ndf*8, 1, 4, 1, 0, bias=False),\n        nn.Sigmoid()\n    )\n\n  def forward(self, input):\n    return self.main(input)\n\n  def initialize_weight(self, w_mean=0., w_std=0.02, b_mean=1, b_std=0.02):\n    for m in self.modules():\n      classname = m.__class__.__name__\n      if classname.find('Conv') != -1:\n        nn.init.normal_(m.weight.data, w_mean, w_std)\n      elif classname.find('BatchNorm') != -1:\n        nn.init.normal_(m.weight.data, b_mean, b_std)\n        nn.init.constant_(m.bias.data, 0)","7694faeb":"#config\ndata_dir = \"..\/input\/gananime-lite\/out2\"\n\nimage_size = 64\nnc = 3\nnz = 100\nngf = 64\nndf = 64\nnum_epochs = 100\nfixed_noise = torch.randn(64, nz, 1, 1, device=device)\n\nreal_idx = 0.9\nfake_idx = 0.1\n\nlr = 0.0002\nbatch_size = 128\nbetal = 0.5\n\nd_transforms = transforms.Compose([\n      transforms.Resize(image_size),\n      transforms.CenterCrop(image_size),\n      transforms.ToTensor(),\n      transforms.Normalize((0.5,0.5,0.5),(0.5,0.5,0.5)),  #\u9700\u8981\u5c06\u50cf\u7d20\u9650\u5236\u5728-1\uff0c1\u4e4b\u95f4\n])","b621561b":"#data\ntrain_set = AnimeDataset(data_dir=data_dir, transforms=d_transforms)\ntrain_loader = DataLoader(train_set, batch_size=batch_size, num_workers=2, shuffle=True)\n\n#show train img\nimg_bchw = next(iter(train_loader))\nplt.title(\"trainning Images\")\nplt.imshow(np.transpose(vutils.make_grid(img_bchw.to(device)[:64], padding=2, normalize=True).cpu(), (1,2,0)))\nplt.show()\nplt.close()","4fbf9403":"#model\nnet_g = Generator(nz=nz, ngf=ngf, nc=nc)\nnet_g.initialize_weight()\n\nnet_d = Discriminator(nc=nc, ndf=ndf)\nnet_d.initialize_weight()\n\nnet_g.to(device)\nnet_d.to(device)","79338113":"#loss\ncriterion = nn.BCELoss()","7781a6bf":"#optimizer\noptimizerG = optim.Adam(net_g.parameters(), lr=0.002, betas=(betal, 0.999))\noptimizerD = optim.Adam(net_d.parameters(), lr=0.0002, betas=(betal, 0.999))\n\n# lr_scheduler_g = optim.lr_scheduler.StepLR(optimizerG, step_size=8, gamma=0.1)\n# lr_scheduler_d = optim.lr_scheduler.StepLR(optimizerD, step_size=8, gamma=0.1)","c7fc870f":"#iteration\nimg_list = []\nG_losses = []\nD_losses = []\n\nfor epoch in range(num_epochs):\n  for i, data in enumerate(train_loader):\n    \n    ############################\n    #update D netword\n    ############################\n    net_d.zero_grad()\n\n    #create training data\n    real_img = data.to(device)\n    b_size = real_img.size(0)\n    real_label = torch.full((b_size,), real_idx, device=device, dtype=torch.float)\n\n    noise = torch.randn(b_size, nz, 1, 1, device=device, dtype=torch.float)\n    fake_img = net_g(noise)\n    fake_label = torch.full((b_size,), fake_idx, device=device, dtype=torch.float)\n\n    #train D with real img\n    out_d_real = net_d(real_img)\n    loss_d_real = criterion(out_d_real.view(-1), real_label)\n\n    #train D with fake img\n    out_d_fake = net_d(fake_img.detach())  #detach\u4ece\u8ba1\u7b97\u56fe\u4e2d\u5265\u79bb\u51fafakeimg\u68af\u5ea6\u6c42\u5bfc\u65f6\u8df3\u8fc7\n    loss_d_fake = criterion(out_d_fake.view(-1), fake_label)\n\n    #backward\n    loss_d_real.backward()\n    loss_d_fake.backward()\n    loss_d = loss_d_real + loss_d_fake\n\n    #update D\n    optimizerD.step()\n\n    d_x = out_d_real.mean().item()  #D(x)\n    d_g_z1 = out_d_fake.mean().item()  #D(G(z1))\n\n    ############################\n    #update G netword\n    ############################\n    net_g.zero_grad()\n\n    label_for_train_g = real_label\n    out_d_fake_2 = net_d(fake_img)\n\n    loss_g = criterion(out_d_fake_2.view(-1), label_for_train_g)\n    loss_g.backward()\n    optimizerG.step()\n\n    d_g_z2 = out_d_fake_2.mean().item()  #D(G(z2))\n\n    #status\n    if i % 50 == 0:\n        print('Epoch[{}\/{}] Iteration[{}\/{}] Loss_D: {:.4f} Loss_G: {:.4f} D(x): {:.4f} D(G(z)): {:.4f} \/ {:.4f}'.format(\n              epoch+1, num_epochs, i+1, len(train_loader), loss_d.item(), loss_g.item(), d_x, d_g_z1, d_g_z2))\n      \n    G_losses.append(loss_g.item())\n    D_losses.append(loss_d.item())\n\n#   lr_scheduler_g.step()\n#   lr_scheduler_d.step()\n  \n  #\u67e5\u770bGenerator\u751f\u6210\u56fe\u7247\u7684\u60c5\u51b5\n  with torch.no_grad():\n    fake = net_g(fixed_noise).detach().cpu()\n    img_list.append(vutils.make_grid(fake, padding=2, normalize=True))\n    img_grid = vutils.make_grid(fake, padding=2, normalize=True).numpy()\n    img_grid = np.transpose(img_grid, (1, 2, 0))\n    plt.imshow(img_grid)\n    plt.title(\"Epoch:{}\".format(epoch))\n    plt.savefig(\"{}_epoch.png\".format(epoch))\n    plt.show()","42e134b9":"# plot loss\nplt.figure(figsize=(10, 5))\nplt.title(\"Generator and Discriminator Loss During Training\")\nplt.plot(G_losses, label=\"G\")\nplt.plot(D_losses, label=\"D\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","729cdcdc":"# save gif\nimgs_epoch = [int(name.split(\"_\")[0]) for name in list(filter(lambda x: x.endswith(\"epoch.png\"), os.listdir(\".\/\")))]\nimgs_epoch = sorted(imgs_epoch)\n\nimgs = list()\nfor i in range(len(imgs_epoch)):\n  img_name = \"{}_epoch.png\".format(imgs_epoch[i])\n  imgs.append(imageio.imread(img_name))\n\nimageio.mimsave(\"generation_animation.gif\", imgs, fps=2)","fdce0d84":"#%%capture\nimport matplotlib.animation as animation\nfrom IPython.display import HTML\nfig = plt.figure(figsize=(8,8))\nplt.axis(\"off\")\nims = [[plt.imshow(np.transpose(i,(1,2,0)), animated=True)] for i in img_list]\nani = animation.ArtistAnimation(fig, ims, interval=1000, repeat_delay=1000, blit=True)\n\nHTML(ani.to_jshtml())","628b1f2b":"def makedir(new_dir):\n  if not os.path.exists(new_dir):\n    os.makedirs(new_dir)\n\ncheckpoint = {\"g_model_state_dict\": net_g.state_dict(),\n        \"d_model_state_dict\": net_d.state_dict(),\n        }\npath_checkpoint = \"checkpoint.pkl\"\ntorch.save(checkpoint, path_checkpoint)","98cbf8fb":"path_checkpoint = \"checkpoint.pkl\"\n\nimage_size = 64\nnum_img = 64\nnc = 3\nnz = 100\nngf = 64\nndf = 64\n\nd_transforms = transforms.Compose([transforms.Resize(image_size),\n                   transforms.CenterCrop(image_size),\n                   transforms.ToTensor(),\n                   transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n               ])\n\n#data\nfixed_noise = torch.randn(num_img, nz, 1, 1, device=device)\n\n# step 2: model\nnet_g = Generator(nz=nz, ngf=ngf, nc=nc)\n\ncheckpoint = torch.load(path_checkpoint, map_location=\"cpu\")\nstate_dict_g = checkpoint[\"g_model_state_dict\"]\nnet_g.load_state_dict(state_dict_g)\nnet_g.to(device)\n\n# step3: inference\nwith torch.no_grad():\n  fake_data = net_g(fixed_noise).detach().cpu()\n  \nimg_grid = vutils.make_grid(fake_data, padding=2, normalize=True).numpy()\nimg_grid = np.transpose(img_grid, (1, 2, 0))\nplt.imshow(img_grid)\nplt.show()","66a14acb":"# \u6a21\u578b\u8bad\u7ec3","e369dcfc":"# Inference","c2d29d25":"# \u53ef\u89c6\u5316","a9db3be1":"# \u4fdd\u5b58\u6a21\u578b","ddb83ab8":"# \u57fa\u7840\u51fd\u6570"}}