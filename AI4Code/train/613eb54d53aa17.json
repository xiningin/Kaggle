{"cell_type":{"b730988c":"code","d165ad0a":"code","210a97de":"code","968655fb":"code","e215bc4c":"code","74e6e167":"code","dfa79f88":"code","f62ae269":"code","3d3cefa0":"code","f93fd5c9":"code","91eaa3a9":"code","54fde907":"code","bdcfaf2b":"code","1cea00e2":"code","9c899a04":"code","a16af5ed":"code","94cc46ea":"markdown","5acae96e":"markdown"},"source":{"b730988c":"!pip -q install -U pip\n!pip -q install pythainlp\n!pip -q install transformers","d165ad0a":"!nvidia-smi","210a97de":"import sklearn\nfrom sklearn.model_selection import train_test_split, KFold\nfrom sklearn.metrics import jaccard_score\n\nfrom scipy import stats\n\nimport seaborn as sns\n\nimport skimage\nfrom skimage.transform import rotate\n\nfrom tqdm import tqdm\nfrom datetime import datetime\n\nimport numpy as np\nimport os\nimport cv2\nimport pandas as pd\n# import imutils\nimport random\nfrom PIL import Image\nimport matplotlib.pyplot as plt\n\nimport pickle\nimport torch\nimport pythainlp\nfrom pythainlp.tokenize import word_tokenize","968655fb":"import re\ndef deEmojify(text):\n    regrex_pattern = re.compile(pattern = \"[\"\n        u\"\\U0001F000-\\U0001FFFF\"\n        u\"\\U0001F600-\\U0001F64F\"  # emoticons\n        u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n        u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n        u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           \"]+\", flags = re.UNICODE)\n    return regrex_pattern.sub(r'',text)","e215bc4c":"def RemoveNoise(text):\n    removeWord = ['\\n', '\\t', '\\r', '\\ufeff']\n    for i in removeWord:\n        text = text.replace(i, '')\n\n    return text","74e6e167":"def self_tokenize(text):\n    tokenize = deEmojify(text)\n    tokenize = RemoveNoise(tokenize)\n    return tokenize","dfa79f88":"Document = {}\nfor path in os.listdir('..\/input\/super-ai-engineer-scg-thai-qa-individual\/test'):\n    text = open('..\/input\/super-ai-engineer-scg-thai-qa-individual\/test\/'+path).read()\n    Document['test\/'+path] = text\n\nself_tokenize(Document['test\/109.txt'])","f62ae269":"small_test = pd.read_csv('..\/input\/super-ai-engineer-scg-thai-qa-individual\/test.csv')","3d3cefa0":"small_test['context'] = small_test['article_path'].apply(lambda x : self_tokenize(Document[x]))\nsmall_test['question'] = small_test['question'].apply(lambda x : self_tokenize(x))\nsmall_test","f93fd5c9":"def hashtag(context_small_test_path):\n    context_small_test = {}\n    for fname in tqdm(os.listdir(context_small_test_path)):\n        with open(context_small_test_path + fname, encoding=\"utf-8\") as f:\n            lines = f.readlines()\n            hashtag = \"\"\n            raw = \"\"\n            for line in lines:\n                if not line.startswith(\"#\") and len(line.strip()) >= 2:\n                    line = deEmojify(line)\n                    line = line.replace(\"\u201d\", \"\").replace(\"\u201c\", \"\")\n                    line = line.strip()\n                    raw += line\n                elif line.startswith(\"#\"):\n                    hashtag += line.strip()\n        context_small_test['test\/'+fname] = {\"context\": raw, \"hashtag\": hashtag}\n        \n    return context_small_test","91eaa3a9":"small_test","54fde907":"def prepare_data_for_testing(test):\n    test_data = []\n    ht = hashtag('..\/input\/super-ai-engineer-scg-thai-qa-individual\/test\/')\n    for idx, row in test.iterrows():\n        doc = {}\n\n        doc[\"context\"] = row[\"context\"]\n        doc[\"question\"] = row[\"question\"]\n        doc[\"hashtag\"] = ht[row['article_path']]['hashtag']\n        \n        test_data.append(doc)\n\n    return test_data\n  \ntest_data = prepare_data_for_testing(small_test)\ntest_data[5]","bdcfaf2b":"from transformers import AutoTokenizer, AutoModelForQuestionAnswering\n\npretrained = \"deepset\/xlm-roberta-large-squad2\"\ntokenizer = AutoTokenizer.from_pretrained(pretrained)\nmodel = AutoModelForQuestionAnswering.from_pretrained(pretrained)\nmodel = model.to('cuda')","1cea00e2":"def find_answer(question, context, tokenizer=tokenizer, model=model):\n    inputs = tokenizer(question, context, add_special_tokens=True, return_tensors=\"pt\", max_length=512).to('cuda')\n    input_ids = inputs[\"input_ids\"].tolist()[0]\n    text_tokens = tokenizer.convert_ids_to_tokens(input_ids)\n    try:\n        answer_start_scores, answer_end_scores = model(**inputs)\n        answer_start = torch.argmax(answer_start_scores)  \n        answer_score = torch.max(answer_start_scores)\n        answer_end = torch.argmax(answer_end_scores) + 1\n        answer = tokenizer.convert_tokens_to_string(tokenizer.convert_ids_to_tokens(input_ids[answer_start:answer_end]))\n        if answer.strip():\n            return answer,  answer_score\n        else:\n            return \"<UNK\/>\", -1\n    except Exception as e:\n        print(e)\n        return \"<UNK\/>\", -1","9c899a04":"submission = pd.DataFrame({})\nfor idx, row in enumerate(tqdm(test_data)):\n    question = row[\"question\"].strip()\n    context = row[\"context\"].strip()\n    hashtag = row[\"hashtag\"].strip()\n    lines = context.split(\"<SEP \/>\")\n\n    #hashtag answer\n    hashtag = hashtag.replace(\"\\n\", \" \")\n    question = question.replace(\"?\", \"\")\n    question_hashtag = \" \".join([h for h in question.split(\" \") if h.startswith(\"#\")])\n    answer_hashtag = \" \".join([h for h in hashtag.split(\" \") if h.startswith(\"#\")])\n\n    # hashtag answer\n    if \"#\" in question or \"\u0e41\u0e2e\u0e0a\u0e41\u0e17\u0e01\" in question or \"\u0e41\u0e2e\u0e0a\u0e41\u0e17\u0e47\u0e01\" in question or \"hashtag\" in question.lower():\n        if question_hashtag:\n            for h in question_hashtag.split(\" \"):\n                answer_hashtag = answer_hashtag.replace(h, \"\")\n        predict = answer_hashtag.strip()\n    else:\n        predict, score = find_answer(question, context)\n    \n    submission.loc[idx+1, \"Predicted\"] = predict.strip()\n\nsubmission['Id'] = submission.index    \nsubmission","a16af5ed":"submission.to_csv(\"submission.csv\", index=False)","94cc46ea":"# Model Transformer","5acae96e":"# Prepare Data for Model"}}