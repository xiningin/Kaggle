{"cell_type":{"46cf44b3":"code","76f4d0ba":"code","5bffb7f0":"code","47726223":"code","dcd32660":"code","0701d440":"code","41ef80ab":"code","6743e2e9":"code","41ffe589":"code","d5f1c251":"code","298653cb":"code","14912cc0":"code","49508d66":"code","b99cdf43":"code","e15de9c8":"code","b9a2d1c6":"code","7ae548a2":"code","426991a2":"code","c11d6630":"code","3554be2f":"code","527830d8":"code","c9e8be78":"code","b3423a3a":"code","e824532e":"code","f8384256":"markdown","ca2fe29d":"markdown","aae213fd":"markdown","74d593cc":"markdown","1e0e56c5":"markdown","b2fe4db6":"markdown","b4aaf1ff":"markdown"},"source":{"46cf44b3":"import random\nimport torch\nimport torch.nn as nn\nimport torch.utils.data\nimport torch.optim as optim\nfrom torch.optim import lr_scheduler\nfrom torch.autograd import Variable\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_style(style=\"whitegrid\")\n\nfrom sklearn.model_selection import StratifiedKFold, train_test_split\nimport time\nimport copy\n\n#\u4fdd\u8bc1\u53ef\u5728GPU\u4e0a\u8fd0\u884c\u4ee5\u53ca\u53ef\u590d\u73b0\u6027\ntorch.backends.cudnn.enabled = False\ntorch.backends.cudnn.deterministic = True\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n# device = 'cpu'\ndevice","76f4d0ba":"train = pd.read_csv('..\/input\/digit-recognizer\/train.csv')\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\nsub = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')","5bffb7f0":"train.head()","47726223":"train.describe()","dcd32660":"print('train shape is {}'.format(train.shape))\nprint('test shape is {}'.format(test.shape))","0701d440":"label = torch.from_numpy(train['label'].values).type(torch.long)\ntrain.drop(['label'], axis=1, inplace=True)","41ef80ab":"plt.figure(figsize=(16, 4))\nfor i in range(4):\n    plt.subplot(1, 4, i + 1)\n    plt.imshow(train.values[i].reshape(28, 28))\nplt.show()","6743e2e9":"def data_deal(train, test):\n    train = train\/255\n    test = test\/255\n    train_numpy = train.values.reshape(-1, 1, 28, 28)\n    test_numpy = test.values.reshape(-1, 1, 28, 28)\n    return torch.from_numpy(train_numpy).type(torch.float), torch.from_numpy(test_numpy).type(torch.float)","41ffe589":"train, test = data_deal(train, test)","d5f1c251":"class DataSet(torch.utils.data.Dataset):\n    def __init__(self, data, label):\n        self.data = data\n        self.label = label\n        \n    def __getitem__(self, index):\n        if self.label is not None:\n            return (self.data[index], self.label[index])\n        else:\n            return self.data[index]\n        \n    def __len__(self):\n        return len(self.data)","298653cb":"def Data(train, val, train_label, val_label, test):\n    datasets = {}\n    datasets['train'] = DataSet(train, train_label)\n    datasets['val'] = DataSet(val, val_label)\n    datasets['test'] = DataSet(test, None)\n    data_loader = {i: torch.utils.data.DataLoader(datasets[i], batch_size=32,\n                                                 shuffle=True, num_workers=0, drop_last=False) \n                   for i in ['train', 'val']}\n    data_loader['test'] = torch.utils.data.DataLoader(datasets['test'], batch_size=32,\n                                                 shuffle=False, num_workers=0, drop_last=False)\n    data_size = {i :len(datasets[i]) for i in ['train', 'val']}\n    return data_loader, data_size","14912cc0":"class CnnBlock(nn.Module):\n    def __init__(self, in_channel, out_channel, k, s):\n        super(CnnBlock, self).__init__()\n        self.cnn = nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=k, stride=s, padding=1)\n        self.batchnorm = nn.BatchNorm2d(out_channel)\n        self.relu = nn.ReLU()\n    def forward(self, x):\n        x = self.cnn(x)\n        x = self.batchnorm(x)\n        x = self.relu(x)\n        return x","49508d66":"class CNN(nn.Module):\n    def __init__(self):\n        super(CNN, self).__init__()\n        self.cnnlayer = nn.ModuleList([CnnBlock(1, 16, 3, 1), CnnBlock(16, 32, 3, 1), CnnBlock(32, 64, 3, 1)])\n        self.pooling = nn.MaxPool2d(2, stride=2, padding=0)\n        self.cnnlayer1 = nn.ModuleList([CnnBlock(64, 64, 3, 1)])\n        self.pooling1 = nn.MaxPool2d(2, stride=2, padding=0)\n        self.Linear = nn.Linear(3136, 10)\n        \n    def forward(self, x):\n        for cnn in self.cnnlayer:\n            x = cnn(x)\n        x = self.pooling(x)\n        for cnn in self.cnnlayer1:\n            x = cnn(x)\n        x = self.pooling1(x)\n        x = x.view(x.size(0), -1)\n        x = self.Linear(x)\n        return x","b99cdf43":"class ResCnnBlock(nn.Module):\n    def __init__(self, in_channel, out_channel, k, s):\n        super(ResCnnBlock, self).__init__()\n        self.cnn = nn.Conv2d(in_channels=in_channel, out_channels=out_channel, kernel_size=k, stride=s, padding=1)\n        self.batchnorm = nn.BatchNorm2d(out_channel)\n        self.relu = nn.ReLU()\n    def forward(self, x):\n        r = self.cnn(x)\n        r = self.batchnorm(r)\n        r = self.relu(r)\n        return x + r","e15de9c8":"class ResCNN(nn.Module):\n    def __init__(self):\n        super(ResCNN, self).__init__()\n        self.cnnlayer = nn.ModuleList([CnnBlock(1, 64, 3, 1), ResCnnBlock(64, 64, 3, 1), ResCnnBlock(64, 64, 3, 1)])\n        self.pooling = nn.MaxPool2d(2, stride=2, padding=0)\n        self.cnnlayer1 = nn.ModuleList([ResCnnBlock(64, 64, 3, 1)])\n        self.pooling1 = nn.MaxPool2d(2, stride=2, padding=0)\n        self.Linear = nn.Linear(3136, 10)\n        \n    def forward(self, x):\n        for cnn in self.cnnlayer:\n            x = cnn(x)\n        x = self.pooling(x)\n        for cnn in self.cnnlayer1:\n            x = cnn(x)\n        x = self.pooling1(x)\n        x = x.view(x.size(0), -1)\n        x = self.Linear(x)\n        return x","b9a2d1c6":"def Train(dataloader, datasize, model, criterion, optimizer, scheduler, epochs):\n    train_loss, val_loss = [], []\n    train_acc, val_acc = [], []\n    for epoch in range(epochs):\n        print('EPOCH {}'.format(epoch))\n        best_loss = 1e5\n        running_loss = 0\n        acc = 0\n        for state in ['train', 'val']:\n            if state == 'train':\n                model.train()\n            else:\n                model.eval()\n            for data, label in dataloader[state]:\n                data = data.to(device)\n                label = label.to(device)\n                # feed forward\n                with torch.set_grad_enabled(state == 'train'):\n                    out = model(data)\n                    optimizer.zero_grad()\n                    loss = criterion(out, label)\n                    # back forward in train\n                    if state == 'train':\n                        loss.backward()\n                        optimizer.step()\n                running_loss += loss.item()*data.size(0)\n                acc += (label == torch.max(out, 1)[1]).sum()\n            if state == 'val':\n                scheduler.step(loss)\n            running_loss = running_loss\/datasize[state]\n            acc = acc\/datasize[state]\n            if state == 'train':\n                train_loss.append(running_loss)\n                train_acc.append(acc.item())\n            else:\n                val_loss.append(running_loss)\n                val_acc.append(acc.item())\n            # save best model\n            if state == 'val' and running_loss < best_loss:\n                best_loss = running_loss\n                best_model_wts = copy.deepcopy(model.state_dict())\n            print('{} LOSS IS {} ACC IS {}'.format(state, running_loss, acc))\n        print()\n            \n    # plot loss and acc curve\n    plt.figure(figsize=(12, 6))\n    plt.subplot(1, 2, 1)\n    plt.plot(range(epochs), train_loss, c='crimson', label='train')\n    plt.plot(range(epochs), val_loss, c='blueviolet', label='val')\n    plt.subplot(1, 2, 2)\n    plt.plot(range(epochs), train_acc, c='crimson', label='train')\n    plt.plot(range(epochs), val_acc, c='blueviolet', label='val')\n    plt.legend()\n    plt.show()\n    \n    # best model\n    model.load_state_dict(best_model_wts)\n    \n    # pre test_data\n    test_pre = torch.LongTensor()\n    model.eval()\n    for data in dataloader['test']:\n        data = data.to(device)\n        with torch.no_grad():\n            out = model(data)\n            out = out.cpu()\n            test_pre = torch.cat([test_pre, out], dim=0)\n    test_pre = test_pre.cpu().numpy()\n    return model, test_pre","7ae548a2":"def HoldoutTrain(train, test, Model, algorithm):\n    model_list = []\n    test_pre_numpy = np.zeros((len(test), 10))\n    model_list.append(Model.to(device))\n\n    trn_idx, val_idx, _, _ = train_test_split(list(range(len(train))), list(range(len(train))), stratify=label, test_size = 0.2, random_state = 42)\n    print(\"algorithm is {}\".format(algorithm))\n    print('=====================')\n    model = model_list[0]\n    criterion = nn.CrossEntropyLoss().to(device)\n    optimizer = optim.Adam(model.parameters(), lr=0.01)\n    exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 15, verbose = True, factor=0.5)\n    train_, val_ = train[trn_idx], train[val_idx]\n    train_label, val_label = label[trn_idx], label[val_idx]\n    dataloaders, dataset_sizes = Data(train_, val_, train_label, val_label, test)\n    model, test_pre = Train(dataloaders, dataset_sizes, model, criterion, optimizer, exp_lr_scheduler, 50)\n    test_pre_numpy += test_pre\n    model_list[0] = model\n    \n    return test_pre","426991a2":"def KfoldTrain(train, test):\n    # 5\u6298\u4ea4\u53c9\u9a8c\u8bc1\n    folds = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n    \n    model_list = []\n    test_pre_numpy = np.zeros((len(test), 10))\n    for i in range(5):\n        model_list.append(CNN().to(device))\n\n    for fold_, (trn_idx, val_idx) in enumerate(folds.split(range(len(train)), label)):\n        print(\"Fold {}\".format(fold_))\n        print('=====================')\n        model = model_list[fold_]\n        criterion = nn.CrossEntropyLoss().to(device)\n        optimizer = optim.Adam(model.parameters(), lr=0.005)\n        exp_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience = 15, verbose = True, factor=0.5)\n        train_, val_ = train[trn_idx], train[val_idx]\n        train_label, val_label = label[trn_idx], label[val_idx]\n        dataloaders, dataset_sizes = Data(train_, val_, train_label, val_label, test)\n        model, test_pre = Train(dataloaders, dataset_sizes, model, criterion, optimizer, exp_lr_scheduler, 30)\n        test_pre_numpy += test_pre\n        model_list[fold_] = model\n    test_pre_numpy = test_pre_numpy\/5\n    \n    return test_pre_numpy","c11d6630":"test_pre_numpy = HoldoutTrain(train, test, CNN(), 'CNN + RELU + BatchNorm  Base')","3554be2f":"test_pre_numpy = HoldoutTrain(train, test, ResCNN(), 'ResNet')","527830d8":"from torchvision import models\nmodel = models.resnet18(pretrained=True)\nnum_fc = model.fc.in_features\nmodel.conv1= nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\nmodel.fc = nn.Linear(num_fc, 10)","c9e8be78":"test_pre_numpy = HoldoutTrain(train, test, model, 'ResNet Transfer')","b3423a3a":"test_pre_label = np.argsort(test_pre_numpy, 1)[:, -1]","e824532e":"sub['Label'] = test_pre_label\nsub.to_csv('submission.csv', index=False)","f8384256":"## 5 Fold","ca2fe29d":"## Transfer Learning","aae213fd":"# Pytorch CNN For Beginer\n\n## Try 3 Model\n* (CNN + BachNorm + Relu)*N + Pooling + (CNN + BachNorm + Relu)*M + Pooling + Linear fc\n* Add residual link on above model\n* Transfer Model: Pretrained Resnet","74d593cc":"## Hold Out","1e0e56c5":"## Save Results","b2fe4db6":"## Base Model CBR + pooling + linear fc ","b4aaf1ff":"## Add Residual link"}}