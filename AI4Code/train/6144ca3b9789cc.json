{"cell_type":{"ff56d872":"code","85a2cc15":"code","8b4a581c":"code","7c2db68d":"code","3dd3b9ff":"code","a2a52040":"code","16fcabc7":"code","4bd0c56d":"code","99e6c164":"code","b0b0d1de":"code","bf0a6125":"code","348705ab":"code","8140bdbb":"code","251ce065":"code","1de64727":"code","9bf78cba":"code","95874793":"code","bb454ecf":"code","f54314c7":"code","aaf48bd3":"code","5b2def50":"code","032c3915":"code","d94bcf91":"code","cdd28b91":"code","36f534f5":"code","f1765f4e":"code","83758060":"code","5da7156f":"code","e4700471":"code","b5a06917":"code","d0e1139a":"markdown","61cc2568":"markdown","9587bf04":"markdown","a36afbc4":"markdown","4c9904bf":"markdown","3fff7f0e":"markdown","4a9c2ccf":"markdown","a924e200":"markdown","3a48d750":"markdown","b0a0a67b":"markdown","81e2b52b":"markdown","53d4c0b7":"markdown","9d28fab2":"markdown","629923b2":"markdown","f8a0b4e4":"markdown","08d70e15":"markdown","cade2b72":"markdown","ffec7dce":"markdown","8d9fad68":"markdown","2e73e04b":"markdown"},"source":{"ff56d872":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nfrom scipy.stats import norm\nfrom sklearn.preprocessing import StandardScaler\nfrom scipy import stats\nimport warnings\nwarnings.filterwarnings('ignore')\n%matplotlib inline","85a2cc15":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\n","8b4a581c":"df_train.columns","7c2db68d":"df_train['SalePrice'].describe()","3dd3b9ff":"#histogram\nsns.distplot(df_train['SalePrice']);","a2a52040":"#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())","16fcabc7":"#scatter plot grlivarea\/saleprice\nvar = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","4bd0c56d":"#scatter plot totalbsmtsf\/saleprice\nvar = 'TotalBsmtSF'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","99e6c164":"#box plot overallqual\/saleprice\nvar = 'OverallQual'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(8, 6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\n","b0b0d1de":"var = 'YearBuilt'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\nplt.xticks(rotation=90);","bf0a6125":"#correlation matrix\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True);","348705ab":"#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","8140bdbb":"#scatterplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(df_train[cols], size = 2.5)\nplt.show();\n","251ce065":"#missing data\ntotal = df_train.isnull().sum().sort_values(ascending=False)\npercent = (df_train.isnull().sum()\/df_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","1de64727":"#dealing with missing data\ndf_train = df_train.drop((missing_data[missing_data['Total'] > 1]).index,1)\ndf_train = df_train.drop(df_train.loc[df_train['Electrical'].isnull()].index)\ndf_train.isnull().sum().max() #just checking that there's no missing data missing...","9bf78cba":"#standardizing data\nsaleprice_scaled = StandardScaler().fit_transform(df_train['SalePrice'][:,np.newaxis]);\nlow_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\nhigh_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\nprint('outer range (low) of the distribution:')\nprint(low_range)\nprint('\\nouter range (high) of the distribution:')\nprint(high_range)","95874793":"#bivariate analysis saleprice\/grlivarea\nvar = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","bb454ecf":"#deleting points\ndf_train.sort_values(by = 'GrLivArea', ascending = False)[:2]\ndf_train = df_train.drop(df_train[df_train['Id'] == 1299].index)\ndf_train = df_train.drop(df_train[df_train['Id'] == 524].index)","f54314c7":"#bivariate analysis saleprice\/grlivarea\nvar = 'TotalBsmtSF'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","aaf48bd3":"#histogram and normal probability plot\nsns.distplot(df_train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)","5b2def50":"#applying log transformation\ndf_train['SalePrice'] = np.log(df_train['SalePrice'])","032c3915":"#transformed histogram and normal probability plot\nsns.distplot(df_train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['SalePrice'], plot=plt)\n","d94bcf91":"#histogram and normal probability plot\nsns.distplot(df_train['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['GrLivArea'], plot=plt)\n","cdd28b91":"#data transformation\ndf_train['GrLivArea'] = np.log(df_train['GrLivArea'])","36f534f5":"#transformed histogram and normal probability plot\nsns.distplot(df_train['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['GrLivArea'], plot=plt)","f1765f4e":"#histogram and normal probability plot\nsns.distplot(df_train['TotalBsmtSF'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['TotalBsmtSF'], plot=plt)","83758060":"#create column for new variable (one is enough because it's a binary categorical feature)\n#if area>0 it gets 1, for area==0 it gets 0\ndf_train['HasBsmt'] = pd.Series(len(df_train['TotalBsmtSF']), index=df_train.index)\ndf_train['HasBsmt'] = 0 \ndf_train.loc[df_train['TotalBsmtSF']>0,'HasBsmt'] = 1\n#transform data\ndf_train.loc[df_train['HasBsmt']==1,'TotalBsmtSF'] = np.log(df_train['TotalBsmtSF'])\n#histogram and normal probability plot\nsns.distplot(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], plot=plt)\n","5da7156f":"#scatter plot\nplt.scatter(df_train['GrLivArea'], df_train['SalePrice']);","e4700471":"#scatter plot\nplt.scatter(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], df_train[df_train['TotalBsmtSF']>0]['SalePrice']);","b5a06917":"#convert categorical variable into dummy\ndf_train = pd.get_dummies(df_train)","d0e1139a":"We can say that, in general, 'SalePrice' exhibit equal levels of variance across the range of 'TotalBsmtSF'. Cool!\n\nLast but not the least, dummy variables\nEasy mode.\n\n","61cc2568":"How 'SalePrice' looks with her new clothes:\n\nLow range values are similar and not too far from 0.\nHigh range values are far from 0 and the 7.something values are really out of range.\nFor now, we'll not consider any of these values as an outlier but we should be careful with those two 7.something values.\n\nBivariate analysis\nWe already know the following scatter plots by heart. However, when we look to things from a new perspective, there's always something to discover. As Alan Kay said, 'a change in perspective is worth 80 IQ points'.","9587bf04":"According to our crystal ball, these are the variables most correlated with 'SalePrice'. My thoughts on this:\n\n'OverallQual', 'GrLivArea' and 'TotalBsmtSF' are strongly correlated with 'SalePrice'. Check!\n'GarageCars' and 'GarageArea' are also some of the most strongly correlated variables. However, as we discussed in the last sub-point, the number of cars that fit into the garage is a consequence of the garage area. 'GarageCars' and 'GarageArea' are like twin brothers. You'll never be able to distinguish them. Therefore, we just need one of these variables in our analysis (we can keep 'GarageCars' since its correlation with 'SalePrice' is higher).\n'TotalBsmtSF' and '1stFloor' also seem to be twin brothers. We can keep 'TotalBsmtSF' just to say that our first guess was right (re-read 'So... What can we expect?').\n'FullBath'?? Really?\n'TotRmsAbvGrd' and 'GrLivArea', twin brothers again. Is this dataset from Chernobyl?\nAh... 'YearBuilt'... It seems that 'YearBuilt' is slightly correlated with 'SalePrice'. Honestly, it scares me to think about 'YearBuilt' because I start feeling that we should do a little bit of time-series analysis to get this right. I'll leave this as a homework for you.\nLet's proceed to the scatter plots.\n\nScatter plots between 'SalePrice' and correlated variables (move like Jagger style)\nGet ready for what you're about to see. I must confess that the first time I saw these scatter plots I was totally blown away! So much information in so short space... It's just amazing. Once more, thank you @seaborn! You make me 'move like Jagger'!","a36afbc4":"Conclusion\nThat's it! We reached the end of our exercise.\n\nThroughout this kernel we put in practice many of the strategies proposed by Hair et al. (2013). We philosophied about the variables, we analysed 'SalePrice' alone and with the most correlated variables, we dealt with missing data and outliers, we tested some of the fundamental statistical assumptions and we even transformed categorial variables into dummy variables. That's a lot of work that Python helped us make easier.\n\nBut the quest is not over. Remember that our story stopped in the Facebook research. Now it's time to give a call to 'SalePrice' and invite her to dinner. Try to predict her behaviour. Do you think she's a girl that enjoys regularized linear regression approaches? Or do you think she prefers ensemble methods? Or maybe something else?\n\nIt's up to you to find out.","4c9904bf":"What has been revealed:\n\nThe two values with bigger 'GrLivArea' seem strange and they are not following the crowd. We can speculate why this is happening. Maybe they refer to agricultural area and that could explain the low price. I'm not sure about this but I'm quite confident that these two points are not representative of the typical case. Therefore, we'll define them as outliers and delete them.\nThe two observations in the top of the plot are those 7.something observations that we said we should be careful about. They look like two special cases, however they seem to be following the trend. For that reason, we will keep them.","3fff7f0e":"In the search for writing 'homoscedasticity' right at the first attempt\nThe best approach to test homoscedasticity for two metric variables is graphically. Departures from an equal dispersion are shown by such shapes as cones (small dispersion at one side of the graph, large dispersion at the opposite side) or diamonds (a large number of points at the center of the distribution).\n\nStarting by 'SalePrice' and 'GrLivArea'...","4a9c2ccf":"'TotalBsmtSF' is also a great friend of 'SalePrice' but this seems a much more emotional relationship! Everything is ok and suddenly, in a strong linear (exponential?) reaction, everything changes. Moreover, it's clear that sometimes 'TotalBsmtSF' closes in itself and gives zero credit to 'SalePrice'.\n\nRelationship with categorical features","a924e200":"**Missing data**","3a48d750":"Older versions of this scatter plot (previous to log transformations), had a conic shape (go back and check 'Scatter plots between 'SalePrice' and correlated variables (move like Jagger style)'). As you can see, the current scatter plot doesn't have a conic shape anymore. That's the power of normality! Just by ensuring normality in some variables, we solved the homoscedasticity problem.\n\nNow let's check 'SalePrice' with 'TotalBsmtSF'.","b0a0a67b":"Although it's not a strong tendency, I'd say that 'SalePrice' is more prone to spend more money in new stuff than in old relics.\n\nNote: we don't know if 'SalePrice' is in constant prices. Constant prices try to remove the effect of inflation. If 'SalePrice' is not in constant prices, it should be, so than prices are comparable over the years.\n\nIn summary\nStories aside, we can conclude that:\n\n'GrLivArea' and 'TotalBsmtSF' seem to be linearly related with 'SalePrice'. Both relationships are positive, which means that as one variable increases, the other also increases. In the case of 'TotalBsmtSF', we can see that the slope of the linear relationship is particularly high.\n'OverallQual' and 'YearBuilt' also seem to be related with 'SalePrice'. The relationship seems to be stronger in the case of 'OverallQual', where the box plot shows how sales prices increase with the overall quality.\nWe just analysed four variables, but there are many other that we should analyse. The trick here seems to be the choice of the right features (feature selection) and not the definition of complex relationships between them (feature engineering).\n\nThat said, let's separate the wheat from the chaff.\n\n3. Keep calm and work smart\nUntil now we just followed our intuition and analysed the variables we thought were important. In spite of our efforts to give an objective character to our analysis, we must say that our starting point was subjective.\n\nAs an engineer, I don't feel comfortable with this approach. All my education was about developing a disciplined mind, able to withstand the winds of subjectivity. There's a reason for that. Try to be subjective in structural engineering and you will see physics making things fall down. It can hurt.\n\nSo, let's overcome inertia and do a more objective analysis.\n\nThe 'plasma soup'\n'In the very beginning there was nothing except for a plasma soup. What is known of these brief moments in time, at the start of our study of cosmology, is largely conjectural. However, science has devised some sketch of what probably happened, based on what is known about the universe today.' (source: http:\/\/umich.edu\/~gs265\/bigbang.htm)\n\nTo explore the universe, we will start with some practical recipes to make sense of our 'plasma soup':\n\nCorrelation matrix (heatmap style).\n'SalePrice' correlation matrix (zoomed heatmap style).\nScatter plots between the most correlated variables (move like Jagger style).\nCorrelation matrix (heatmap style)","81e2b52b":"In my opinion, this heatmap is the best way to get a quick overview of our 'plasma soup' and its relationships. (Thank you @seaborn!)\n\n**'SalePrice' correlation matrix (zoomed heatmap style)**","53d4c0b7":"Knowing yourself is the beginning of all wisdom. **Aristotle\n**\nTry to understand the Basic before jumping to har problems so I tried here to represent my data in as simple way as possible for which I Examind the data', I did my best to follow a comprehensive, but not exhaustive, analysis of the data\n\nwhat we are doing in this kernel is something like:\n\n1)Understand the problem. We'll look at each variable and do a philosophical analysis about their meaning and importance for this problem.\n\n2)Univariable study. We'll just focus on the dependent variable ('SalePrice') and try to know a little bit more about it.\n\n3)Multivariate study. We'll try to understand how the dependent variable and independent variables relate.\n\n4)Basic cleaning. We'll clean the dataset and handle the missing data, outliers and categorical variables.\n\n5)Test assumptions. We'll check if our data meets the assumptions required by most multivariate techniques.\n\nNow, it's time to have fun!","9d28fab2":"Ok, now we are dealing with the big boss. What do we have here?\n\nSomething that, in general, presents skewness.\nA significant number of observations with value zero (houses without basement).\nA big problem because the value zero doesn't allow us to do log transformations.\nTo apply a log transformation here, we'll create a variable that can get the effect of having or not having basement (binary variable). Then, we'll do a log transformation to all the non-zero observations, ignoring those with value zero. This way we can transform data, without losing the effect of having or not basement.\n\nI'm not sure if this approach is correct. It just seemed right to me. That's what I call 'high risk engineering'.\n\n","629923b2":"In order to understand our data, we can look at each variable and try to understand their meaning and relevance to this problem. I know this is time-consuming, but it will give us the flavour of our dataset.\n\nIn order to have some discipline in our analysis, we can create an Excel spreadsheet with the following columns:\n\n1)Variable - Variable name.\n\n2)Type - Identification of the variables' type. There are two possible values for this field: 'numerical' or 'categorical'. By 'numerical' we mean variables for which the values are numbers, and by 'categorical' we mean variables for which the values are categories.\n\n3)Segment - Identification of the variables' segment. We can define three possible segments: building, space or location. When we say 'building', we mean a variable that relates to the physical characteristics of the building (e.g. 'OverallQual'). When we say 'space', we mean a variable that reports space properties of the house (e.g. 'TotalBsmtSF'). Finally, when we say a 'location', we mean a variable that gives information about the place where the house is located (e.g. 'Neighborhood').\n\n4)Expectation - Our expectation about the variable influence in 'SalePrice'. We can use a categorical scale with 'High', 'Medium' and 'Low' as possible values.\n\n5)Conclusion - Our conclusions about the importance of the variable, after we give a quick look at the data. We can keep with the same categorical scale as in 'Expectation'.\n\n6)Comments - Any general comments that occured to us.\n\n**First things first: analysing 'SalePrice'**\n\n'SalePrice' is the reason of our quest. It's like when we're going to a party. We always have a reason to be there. Usually, women are that reason.","f8a0b4e4":"**Relationship with numerical variables**","08d70e15":"Let's analyse this to understand how to handle the missing data.\n\nWe'll consider that when more than 15% of the data is missing, we should delete the corresponding variable and pretend it never existed. This means that we will not try any trick to fill the missing data in these cases. According to this, there is a set of variables (e.g. 'PoolQC', 'MiscFeature', 'Alley', etc.) that we should delete. The point is: will we miss this data? I don't think so. None of these variables seem to be very important, since most of them are not aspects in which we think about when buying a house (maybe that's the reason why data is missing?). Moreover, looking closer at the variables, we could say that variables like 'PoolQC', 'MiscFeature' and 'FireplaceQu' are strong candidates for outliers, so we'll be happy to delete them.\n\nIn what concerns the remaining cases, we can see that 'GarageX' variables have the same number of missing data. I bet missing data refers to the same set of observations (although I will not check it; it's just 5% and we should not spend 20 in5  problems). Since the most important information regarding garages is expressed by 'GarageCars' and considering that we are just talking about 5% of missing data, I'll delete the mentioned 'GarageX' variables. The same logic applies to 'BsmtX' variables.\n\nRegarding 'MasVnrArea' and 'MasVnrType', we can consider that these variables are not essential. Furthermore, they have a strong correlation with 'YearBuilt' and 'OverallQual' which are already considered. Thus, we will not lose information if we delete 'MasVnrArea' and 'MasVnrType'.\n\nFinally, we have one missing observation in 'Electrical'. Since it is just one observation, we'll delete this observation and keep the variable.\n\nIn summary, to handle missing data, we'll delete all the variables with missing data, except the variable 'Electrical'. In 'Electrical' we'll just delete the observation with missing data.","cade2b72":"**Out liars!**\nOutliers is also something that we should be aware of. Why? Because outliers can markedly affect our models and can be a valuable source of information, providing us insights about specific behaviours.\n\nOutliers is a complex subject and it deserves more attention. Here, we'll just do a quick analysis through the standard deviation of 'SalePrice' and a set of scatter plots.\n\nUnivariate analysis\u00b6\nThe primary concern here is to establish a threshold that defines an observation as an outlier. To do so, we'll standardize the data. In this context, data standardization means converting data values to have mean of 0 and a standard deviation of 1.","ffec7dce":"The point here is to test 'SalePrice' in a very lean way. We'll do this paying attention to:\n\nHistogram - Kurtosis and skewness.\nNormal probability plot - Data distribution should closely follow the diagonal that represents the normal distribution.","8d9fad68":"Hmmm... It seems that 'SalePrice' and 'GrLivArea' are really old friends, with a linear relationship.\n\nAnd what about 'TotalBsmtSF'?","2e73e04b":"Ah! I see you that you use seaborn makeup when you're going out... That's so elegant! I also see that you:\n\nDeviate from the normal distribution.\nHave appreciable positive skewness.\nShow peakedness.\nThis is getting interesting! 'SalePrice', could you give me your body measures?'"}}