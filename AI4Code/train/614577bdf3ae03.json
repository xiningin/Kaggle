{"cell_type":{"5005ee7a":"code","8c14d664":"code","e572e8d5":"code","2f32d8e3":"code","af3838d7":"code","e0b38f48":"code","854cf272":"code","8d7a1504":"code","2b482cbf":"code","00c81da6":"code","99db7be7":"code","9cd9fe1b":"code","b679d81e":"code","023ced8c":"code","ea1ae62c":"code","8ba1f363":"code","ddb4bb0d":"code","b722ebf7":"code","7c889a5c":"code","68e75b30":"code","7d164ee9":"code","81f6dd17":"code","9eedddc7":"code","d848557c":"code","ca1bb477":"code","4422e26c":"code","3185a804":"code","f81414e7":"code","9c47ad8c":"code","be272fda":"code","439ec15e":"code","403b40a5":"code","6bd34081":"code","8635dada":"code","addc211b":"code","d23dec7d":"code","6b8b63e3":"code","0bea3348":"code","ac991dc8":"code","f530cc34":"code","c283bf4e":"code","269b50ad":"code","fd587ce8":"code","2002ac8a":"code","b8039373":"code","43eaf339":"code","f8b06e7f":"code","b5d6e866":"markdown","226a4458":"markdown","c0b92f1c":"markdown","cc0cf65f":"markdown","d0a1f7ba":"markdown","da008a05":"markdown","dbbd92e2":"markdown","074f9a4f":"markdown","79a723ff":"markdown","3d6fbdca":"markdown","55d8ea8e":"markdown","13d8344d":"markdown","01c3f583":"markdown","7b6209be":"markdown","3081dc09":"markdown","f5f23307":"markdown","e8be169a":"markdown","8fdf79cb":"markdown","1cc21686":"markdown","8c290a46":"markdown","e36b5216":"markdown","6f40babd":"markdown","13c78358":"markdown","bf9332f2":"markdown","6997cd38":"markdown","0d2e1c49":"markdown","f2f9fcbd":"markdown","d59368d1":"markdown","c0aba648":"markdown","8004dbfc":"markdown","a5709f13":"markdown","0fe6808d":"markdown","bfa635f7":"markdown","a9d764c5":"markdown","9b30a545":"markdown","b0fe69e6":"markdown","0b58cf03":"markdown","6a6cb93f":"markdown","df616214":"markdown","1474a2ae":"markdown","7745a686":"markdown","6b2d60c6":"markdown","2d692393":"markdown","af1044f7":"markdown","92d4d867":"markdown","a92a2551":"markdown","4ece002a":"markdown","cfd1a4c7":"markdown","b665cda2":"markdown"},"source":{"5005ee7a":"# data analysis and wrangling\nimport pandas as pd\nimport numpy as np\nimport random as rnd\n\n# visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# scaling and train test split\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# creating a model\nimport tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout, Activation\nfrom tensorflow.keras.constraints import max_norm\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.models import load_model\n\n# evaluation on test data\nfrom sklearn.metrics import classification_report,confusion_matrix","8c14d664":"data_info = pd.read_csv('..\/input\/subset-lending-club-loan\/lending_club_info.csv',index_col='LoanStatNew')\n\ndef feat_info(col_name):\n    print(data_info.loc[col_name]['Description'])\n\n# example\nfeat_info('mort_acc')","e572e8d5":"df = pd.read_csv('..\/input\/subset-lending-club-loan\/lending_club_loan_two.csv')","2f32d8e3":"print(df.info())","af3838d7":"df.head()","e0b38f48":"df.describe().transpose()","854cf272":"sns.set(style=\"whitegrid\", font_scale=1)\n\nplt.figure(figsize=(12,12))\nplt.title('Pearson Correlation Matrix',fontsize=25)\nsns.heatmap(df.corr(),linewidths=0.25,vmax=0.7,square=True,cmap=\"GnBu\",linecolor='w',\n            annot=True, annot_kws={\"size\":10}, cbar_kws={\"shrink\": .7})","8d7a1504":"f, axes = plt.subplots(1, 2, figsize=(15,5))\nsns.countplot(x='loan_status', data=df, ax=axes[0])\nsns.distplot(df['loan_amnt'], kde=False, bins=40, ax=axes[1])\nsns.despine()\naxes[0].set(xlabel='Status', ylabel='')\naxes[0].set_title('Count of Loan Status', size=20)\naxes[1].set(xlabel='Loan Amount', ylabel='')\naxes[1].set_title('Loan Amount Distribution', size=20)","2b482cbf":"f, axes = plt.subplots(1, 2, figsize=(15,5))\nsns.scatterplot(x='installment', y='loan_amnt', data=df, ax=axes[0])\nsns.boxplot(x='loan_status', y='loan_amnt', data=df, ax=axes[1])\nsns.despine()\naxes[0].set(xlabel='Installment', ylabel='Loan Amount')\naxes[0].set_title('Scatterplot between Loan Amount and Installment', size=15)\naxes[1].set(xlabel='Loan Status', ylabel='Loan Amount')\naxes[1].set_title('Boxplot between Loan Amount and Loan Status', size=15)","00c81da6":"df.groupby('loan_status')['loan_amnt'].describe()","99db7be7":"f, axes = plt.subplots(1, 2, figsize=(15,5), gridspec_kw={'width_ratios': [1, 2]})\nsns.countplot(x='grade', hue='loan_status', data=df, order=sorted(df['grade'].unique()), palette='seismic', ax=axes[0])\nsns.countplot(x='sub_grade', data=df, palette='seismic', order=sorted(df['sub_grade'].unique()), ax=axes[1])\nsns.despine()\naxes[0].set(xlabel='Grade', ylabel='Count')\naxes[0].set_title('Count of Loan Status per Grade', size=20)\naxes[1].set(xlabel='Sub Grade', ylabel='Count')\naxes[1].set_title('Count of Loan Status per Sub Grade', size=20)\nplt.tight_layout()","9cd9fe1b":"df['loan_repaid'] = df['loan_status'].map({'Fully Paid':1,'Charged Off':0})\ndf[['loan_repaid','loan_status']].head()","b679d81e":"df.corr()['loan_repaid'].sort_values(ascending=True).drop('loan_repaid').plot.bar(color='green')","023ced8c":"print(len(df))","ea1ae62c":"df.isnull().sum()","8ba1f363":"feat_info('emp_title')\nprint('\\n')\nfeat_info('emp_length')\nprint('\\n')\nfeat_info('title')\nprint('\\n')\nfeat_info('revol_util')\nprint('\\n')\nfeat_info('mort_acc')\nprint('\\n')\nfeat_info('pub_rec_bankruptcies')","ddb4bb0d":"plt.figure(figsize=(10,5))\n((df.isnull().sum())\/len(df)*100).plot.bar(title='Percentage of missing values per column', color='green')","b722ebf7":"print(df['emp_title'].nunique())\ndf['emp_title'].value_counts()","7c889a5c":"df = df.drop('emp_title',axis=1)","68e75b30":"per_charge_off = df[df['loan_repaid'] == 0]['emp_length'].value_counts() \/ df[df['loan_repaid'] == 1]['emp_length'].value_counts()\nper_charge_off.plot.bar(color='green')","7d164ee9":"df = df.drop('emp_length', axis=1)","81f6dd17":"df[['title', 'purpose']].head(10)","9eedddc7":"df = df.drop('title', axis=1)","d848557c":"print(\"Mean of mort_acc column per total_acc\")\ntotal_acc_avg = df.groupby('total_acc').mean()['mort_acc']\nprint(total_acc_avg)","ca1bb477":"total_acc_avg = df.groupby('total_acc').mean()['mort_acc']\n\ndef fill_mort_acc(total_acc,mort_acc):\n    '''\n    Accepts the total_acc and mort_acc values for the row.\n    Checks if the mort_acc is NaN , if so, it returns the avg mort_acc value\n    for the corresponding total_acc value for that row.\n    \n    total_acc_avg here should be a Series or dictionary containing the mapping of the\n    groupby averages of mort_acc per total_acc values.\n    '''\n    if np.isnan(mort_acc):\n        return total_acc_avg[total_acc]\n    else:\n        return mort_acc\n    \ndf['mort_acc'] = df.apply(lambda x: fill_mort_acc(x['total_acc'], x['mort_acc']), axis=1)","4422e26c":"df = df.dropna()","3185a804":"# check for missing values\ndf.isnull().sum()","f81414e7":"print(df['term'].value_counts())\nprint('\\n')\nprint('\\n')\n\ndf['term'] = df['term'].apply(lambda term: int(term[:3]))\n\nprint(df['term'].value_counts())","9c47ad8c":"df = df.drop('grade', axis=1)","be272fda":"subgrade_dummies = pd.get_dummies(df['sub_grade'],drop_first=True)\ndf = pd.concat([df.drop('sub_grade',axis=1),subgrade_dummies],axis=1)","439ec15e":"dummies = pd.get_dummies(df[['verification_status', 'application_type','initial_list_status','purpose']], drop_first=True)\n\ndf = df.drop(['verification_status', 'application_type','initial_list_status','purpose'],axis=1)\n\ndf = pd.concat([df,dummies],axis=1)","403b40a5":"df['home_ownership'] = df['home_ownership'].replace(['NONE', 'ANY'], 'OTHER')\ndummies = pd.get_dummies(df['home_ownership'],drop_first=True)\ndf = df.drop('home_ownership',axis=1)\ndf = pd.concat([df,dummies],axis=1)","6bd34081":"df['zip_code'] = df['address'].apply(lambda address:address[-5:])\n\ndummies = pd.get_dummies(df['zip_code'],drop_first=True)\ndf = df.drop(['zip_code','address'],axis=1)\ndf = pd.concat([df,dummies],axis=1)","8635dada":"df = df.drop('issue_d', axis=1)","addc211b":"df['earliest_cr_year'] = df['earliest_cr_line'].apply(lambda date:int(date[-4:]))\ndf = df.drop('earliest_cr_line', axis=1)\n\ndf.select_dtypes(['object']).columns","d23dec7d":"df = df.drop('loan_status',axis=1)","6b8b63e3":"# Features\nX = df.drop('loan_repaid',axis=1).values\n\n# Label\ny = df['loan_repaid'].values\n\n# Split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=101)","0bea3348":"print(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","ac991dc8":"scaler = MinMaxScaler()\n\n# fit and transfrom\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n\n# everything has been scaled between 1 and 0\nprint('Max: ',X_train.max())\nprint('Min: ', X_train.min())","f530cc34":"model = Sequential()\n\n# input layer\nmodel.add(Dense(78,activation='relu'))\nmodel.add(Dropout(0.2))\n\n# hidden layer\nmodel.add(Dense(39,activation='relu'))\nmodel.add(Dropout(0.2))\n\n# hidden layer\nmodel.add(Dense(19,activation='relu'))\nmodel.add(Dropout(0.2))\n\n# output layer\nmodel.add(Dense(1, activation='sigmoid'))\n\n# compile model\nmodel.compile(optimizer=\"adam\", loss='binary_crossentropy')","c283bf4e":"early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=25)","269b50ad":"model.fit(x=X_train, \n          y=y_train, \n          epochs=400,\n          verbose = 2,\n          batch_size=256,\n          validation_data=(X_test, y_test),\n          callbacks=[early_stop])","fd587ce8":"losses = pd.DataFrame(model.history.history)\n\nplt.figure(figsize=(15,5))\nsns.lineplot(data=losses,lw=3)\nplt.xlabel('Epochs')\nplt.ylabel('')\nplt.title('Training Loss per Epoch')\nsns.despine()","2002ac8a":"predictions = model.predict_classes(X_test)\n\nprint('Classification Report:')\nprint(classification_report(y_test, predictions))\nprint('\\n')\nprint('Confusion Matirx:')\nprint(confusion_matrix(y_test, predictions))","b8039373":"rnd.seed(101)\nrandom_ind = rnd.randint(0,len(df))\n\nnew_customer = df.drop('loan_repaid',axis=1).iloc[random_ind]\nnew_customer","43eaf339":"# we need to reshape this to be in the same shape of the training data that the model was trained on\nmodel.predict_classes(new_customer.values.reshape(1,78))","f8b06e7f":"# the prediction was right\ndf.iloc[random_ind]['loan_repaid']","b5d6e866":"## Issue_d feature\nThis would be data leakage, we wouldn't know beforehand whether or not a loan would be issued when using our model, so in theory we wouldn't have an issue_date, drop this feature.","226a4458":"Let's examine emp_title and emp_length to see whether it will be okay to drop them.\n\n***How many unique employment job titles are there?***\n\nRealistically there are too many unique job titles to try to convert this to a dummy variable feature.","c0b92f1c":"<a id=\"ch5\"><\/a>\n# 5. Categorical variables and dummy variables\n---\nWe're done working with the missing data! Now we just need to deal with the string values due to the categorical columns.","cc0cf65f":"## Early stopping\nThis callback allows you to specify the performance measure to monitor, the trigger, and once triggered, it will stop the training process. Basically, it stop training when a monitored quantity has stopped improving.","d0a1f7ba":"**Preview the data**","da008a05":"## Address feature\nLet's feature engineer a zip code column from the address in the data set. Create a column called 'zip_code' that extracts the zip code from the address column.","dbbd92e2":"## Term feature\nConvert the term feature into either a 36 or 60 integer numeric data type using .apply() or .map().","074f9a4f":"**What is the distribution of numerical feature values across the samples?**","79a723ff":"***Did this person actually end up paying back their loan?***","3d6fbdca":"* The interest rate has essentially the highest negative correlation with whether or not someone is going to repay their loan.\n* If you have a extremely high interest rate you are going to find it harder to pay off that loan.","55d8ea8e":"Let's fill in the missing mort_acc values based on their total_acc value. If the mort_acc is missing, then we will fill in that missing value with the mean value corresponding to its total_acc value from the Series we created above. This involves using an .apply() method with two columns.","13d8344d":"## Loan status and loan amount distribution\n* This is an imbalance problem, because we have a lot more entries of people that fully paid their loans then people that did not pay back.\n* We can expect to probably do very well in terms of accuracy but our precision and recall are going to be the true metrics that we will have to evaluate our model based off of.\n* In the loan amount distribution we can see spikes in even ten thousend dollar, so this is indicating that there are certain amounts that are basically standard loans.","01c3f583":"## Loading the data","7b6209be":"Now we are going to create a new column called 'loan_repaid' which will contain a 1 if the loan status was \"Fully Paid\" and a 0 if it was \"Charged Off\".","3081dc09":"<a id=\"ch11\"><\/a>\n# 11. References\n* [An Introduction to Statistical Learning with Applications in R](http:\/\/faculty.marshall.usc.edu\/gareth-james\/ISL\/) - This book provides an introduction to statistical learning methods.\n* [Python for Data Science and Machine Learning Bootcamp](https:\/\/www.udemy.com\/course\/python-for-data-science-and-machine-learning-bootcamp\/) - Use Python for Data Science and Machine Learning.","f5f23307":"Before starting, let us make a function to get feature information on the data as a .csv file for easy lookup throughout the notebook. ","e8be169a":"## Grade feature\nWe already know grade is part of sub_grade, so just drop the grade feature.","8fdf79cb":"## Training loss per epoch\n* This plot shows the training loss per epoch.\n* This plot helps us to see if there is overfitting in the model. In this case there is no overfitting because both lines go down at the same time. ","1cc21686":"revol_util and the pub_rec_bankruptcies have missing data points, but they account for less than 0.5% of the total data. Let's remove the rows that are missing those values in those columns with dropna().","8c290a46":"<a id=\"ch7\"><\/a>\n# 7. Creating a model\n---\n**Dropout Layers**\n* Dropout is a technique where randomly selected neurons are ignored during training. They are \u201cdropped-out\u201d randomly.\n* Simply put, dropout refers to ignoring units (i.e. neurons) during the training phase of certain set of neurons which is chosen at random.\n* Helps prevent overfitting.","e36b5216":"<a id=\"ch8\"><\/a>\n# 8. Training the model\n---\nNow that the model is ready, we can fit the model into the data.","6f40babd":"Now we are going to deal with the missing data of mort_acc. Since mort_acc has a strong correlation with total_acc we will group the dataframe by the total_acc and calculate the mean value for the mort_acc per total_acc entry. To get the result below:","13c78358":"## Verification_status, application_type, initial_list_status, purpose features\nLet's convert these columns into dummy variables and concatenate them with the original dataframe.","bf9332f2":"<a id=\"ch9\"><\/a>\n# 9. Evaluation on test data\n---\n***Classification Report***\n* **Accuracy** is just the actual percent that we got right, in this case it was 89%.\n* Note that since the data is imbalance if we were to make a model that approve all the loans, it would have a 80% accuracy.\n* For example: (317696\/len(df)) = 0.80 where 317696 is `df[loan_repaid].value_counts()`\n* So do not be fooled by a model that returns back 80% accuracy because by default a model that always reports back to the loan will be repaid itself will be 80% accurate on this actual test data set.\n\n\n* The **recall** means \"how many of this class you find over the whole number of element of this class\"\n* The **precision** will be \"how many are correctly classified among that class\"\n* The **f1-score** is the harmonic mean between precision & recall\n* The **support** is the number of occurence of the given class in your dataset.\n\n\n* Precision is 0.96, which is really good. On the other hand, recall is not good. \n* We should focus in improving the f1-score in the 0 class. We should improve the 0.62.\n\n***Confusion Matirx***\n* A confusion matrix is a technique for summarizing the performance of a classification algorithm.\n* Classification accuracy alone can be misleading if you have an unequal number of observations in each class, which is our case. \n\n\n* We have 308 Type I errors (False Positive) and 8562 Type II errors (False Negative). \n* 7096 True Positive and 63078 True Negative. ","6997cd38":"The title column is simply a string subcategory\/description of the purpose column. Let's drop the column.","0d2e1c49":"## Imports","f2f9fcbd":"If we review the title column vs the purpose column looks like there is repeated information","d59368d1":"<hr\/>\n\n# Minimizing Risks for Loan Investments - (Keras - Artificial Neural Network)\n### [by Tomas Mantero](https:\/\/www.kaggle.com\/tomasmantero)\n<hr\/>\n\n### Table of Contents\n* **1. [Introduction](#ch1)**\n* **2. [Dataset](#ch2)**\n* **3. [Exploratory Data Analysis](#ch3)**\n* **4. [Data PreProcessing](#ch4)**\n* **5. [Categorical Variables and Dummy Variables](#ch5)**\n* **6. [Scaling and Train Test Split](#ch6)**\n* **7. [Creating a Model](#ch7)**\n* **8. [Training the Model](#ch8)**\n* **9. [Evaluation on Test Data](#ch9)**\n* **10. [Predicting on a New Customer](#ch10)**\n* **11. [References](#ch11)**","c0aba648":"**Which features are available in the dataset?**","8004dbfc":"Let's convert the subgrade into dummy variables. Then concatenate these new columns to the original dataframe. Remember to drop the original subgrade column and to add drop_first=True to your get_dummies call.","a5709f13":"Let's drop emp_title:","0fe6808d":"## Countplot per grade and subgrade\n* Essentially this is showing the percentage of charged off loans.\n* Looks like it is increasing as the letter grade gets higher.\n* Better grades are bluer and the worse grades are redder.","bfa635f7":"Let's drop emp_length:","a9d764c5":"In case that the boxplot is a little hard to read you can always compare the averages here: \n* So you can see the charged off average price is a little higher than the fully paid loan.","9b30a545":"## Relationship between loan_amnt, loan_status and installment","b0fe69e6":"## Normalizing \/ scaling the data\nWe scale the feature data. To prevent data leakage from the test set, we only fit our scaler to the training set.","0b58cf03":"## Pearson correlation matrix\nWe use the Pearson correlation coefficient to examine the strength and direction of the linear relationship between two continuous variables.\n\nThe correlation coefficient can range in value from \u22121 to +1. The larger the absolute value of the coefficient, the stronger the relationship between the variables. For the Pearson correlation, an absolute value of 1 indicates a perfect linear relationship. A correlation close to 0 indicates no linear relationship between the variables. \n\nThe sign of the coefficient indicates the direction of the relationship. If both variables tend to increase or decrease together, the coefficient is positive, and the line that represents the correlation slopes upward. If one variable tends to increase as the other decreases, the coefficient is negative, and the line that represents the correlation slopes downward.\n\n* We can see a strong correlation between loan_amnt and installment. (The monthly payment owed by the borrower if the loan originates)","6a6cb93f":"## Home_ownership feature\nConvert these to dummy variables, but replace NONE and ANY with OTHER, so that we end up with just 4 categories, MORTGAGE, RENT, OWN, OTHER. Then concatenate them with the original dataframe.","df616214":"<a id=\"ch4\"><\/a>\n# 4. Data preprocessing\n---\nRemove or fill any missing data. Remove unnecessary or repetitive features. Convert categorical string features to dummy variables.\n\n## Missing data\n***What is the length of the dataframe?***","1474a2ae":"Now we want the percentage of charge offs per category. Essentially informing us what percent of people per employment category didn't pay back their loan. \n* We can see that across the extremes it looks to be extremely similar.\n* Looks like this particular feature of employment length doesn't actually have some extreme differences on the charge off rates.\n* Looks like regardless of what actual employment length you have if you were to pick someone, about 20% of them are going to have not paid back their loans.","7745a686":"<a id=\"ch2\"><\/a>\n# 2. Dataset\n---\n\n* We will be using a subset of the LendingClub DataSet obtained from Kaggle: https:\/\/www.kaggle.com\/wordsforthewise\/lending-club\n\nThere are many LendingClub data sets on Kaggle. Here is the information on this particular data set:\n\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th><\/th>\n      <th>LoanStatNew<\/th>\n      <th>Description<\/th>\n    <\/tr>\n  <\/thead>\n  <tbody>\n    <tr>\n      <th>0<\/th>\n      <td>loan_amnt<\/td>\n      <td>The listed amount of the loan applied for by the borrower. If at some point in time, the credit department reduces the loan amount, then it will be reflected in this value.<\/td>\n    <\/tr>\n    <tr>\n      <th>1<\/th>\n      <td>term<\/td>\n      <td>The number of payments on the loan. Values are in months and can be either 36 or 60.<\/td>\n    <\/tr>\n    <tr>\n      <th>2<\/th>\n      <td>int_rate<\/td>\n      <td>Interest Rate on the loan<\/td>\n    <\/tr>\n    <tr>\n      <th>3<\/th>\n      <td>installment<\/td>\n      <td>The monthly payment owed by the borrower if the loan originates.<\/td>\n    <\/tr>\n    <tr>\n      <th>4<\/th>\n      <td>grade<\/td>\n      <td>LC assigned loan grade<\/td>\n    <\/tr>\n    <tr>\n      <th>5<\/th>\n      <td>sub_grade<\/td>\n      <td>LC assigned loan subgrade<\/td>\n    <\/tr>\n    <tr>\n      <th>6<\/th>\n      <td>emp_title<\/td>\n      <td>The job title supplied by the Borrower when applying for the loan.*<\/td>\n    <\/tr>\n    <tr>\n      <th>7<\/th>\n      <td>emp_length<\/td>\n      <td>Employment length in years. Possible values are between 0 and 10 where 0 means less than one year and 10 means ten or more years.<\/td>\n    <\/tr>\n    <tr>\n      <th>8<\/th>\n      <td>home_ownership<\/td>\n      <td>The home ownership status provided by the borrower during registration\u00a0or obtained from the credit report.\u00a0Our values are: RENT, OWN, MORTGAGE, OTHER<\/td>\n    <\/tr>\n    <tr>\n      <th>9<\/th>\n      <td>annual_inc<\/td>\n      <td>The self-reported annual income provided by the borrower during registration.<\/td>\n    <\/tr>\n    <tr>\n      <th>10<\/th>\n      <td>verification_status<\/td>\n      <td>Indicates if income was verified by LC, not verified, or if the income source was verified<\/td>\n    <\/tr>\n    <tr>\n      <th>11<\/th>\n      <td>issue_d<\/td>\n      <td>The month which the loan was funded<\/td>\n    <\/tr>\n    <tr>\n      <th>12<\/th>\n      <td>loan_status<\/td>\n      <td>Current status of the loan<\/td>\n    <\/tr>\n    <tr>\n      <th>13<\/th>\n      <td>purpose<\/td>\n      <td>A category provided by the borrower for the loan request.<\/td>\n    <\/tr>\n    <tr>\n      <th>14<\/th>\n      <td>title<\/td>\n      <td>The loan title provided by the borrower<\/td>\n    <\/tr>\n    <tr>\n      <th>15<\/th>\n      <td>zip_code<\/td>\n      <td>The first 3 numbers of the zip code provided by the borrower in the loan application.<\/td>\n    <\/tr>\n    <tr>\n      <th>16<\/th>\n      <td>addr_state<\/td>\n      <td>The state provided by the borrower in the loan application<\/td>\n    <\/tr>\n    <tr>\n      <th>17<\/th>\n      <td>dti<\/td>\n      <td>A ratio calculated using the borrower\u2019s total monthly debt payments on the total debt obligations, excluding mortgage and the requested LC loan, divided by the borrower\u2019s self-reported monthly income.<\/td>\n    <\/tr>\n    <tr>\n      <th>18<\/th>\n      <td>earliest_cr_line<\/td>\n      <td>The month the borrower's earliest reported credit line was opened<\/td>\n    <\/tr>\n    <tr>\n      <th>19<\/th>\n      <td>open_acc<\/td>\n      <td>The number of open credit lines in the borrower's credit file.<\/td>\n    <\/tr>\n    <tr>\n      <th>20<\/th>\n      <td>pub_rec<\/td>\n      <td>Number of derogatory public records<\/td>\n    <\/tr>\n    <tr>\n      <th>21<\/th>\n      <td>revol_bal<\/td>\n      <td>Total credit revolving balance<\/td>\n    <\/tr>\n    <tr>\n      <th>22<\/th>\n      <td>revol_util<\/td>\n      <td>Revolving line utilization rate, or the amount of credit the borrower is using relative to all available revolving credit.<\/td>\n    <\/tr>\n    <tr>\n      <th>23<\/th>\n      <td>total_acc<\/td>\n      <td>The total number of credit lines currently in the borrower's credit file<\/td>\n    <\/tr>\n    <tr>\n      <th>24<\/th>\n      <td>initial_list_status<\/td>\n      <td>The initial listing status of the loan. Possible values are \u2013 W, F<\/td>\n    <\/tr>\n    <tr>\n      <th>25<\/th>\n      <td>application_type<\/td>\n      <td>Indicates whether the loan is an individual application or a joint application with two co-borrowers<\/td>\n    <\/tr>\n    <tr>\n      <th>26<\/th>\n      <td>mort_acc<\/td>\n      <td>Number of mortgage accounts.<\/td>\n    <\/tr>\n    <tr>\n      <th>27<\/th>\n      <td>pub_rec_bankruptcies<\/td>\n      <td>Number of public record bankruptcies<\/td>\n    <\/tr>\n  <\/tbody>\n<\/table>\n\n---","6b2d60c6":"***What is the total count of missing values per column?***\n\nWe have missing values in emp_title, emp_length, title, revol_util, mort_acc and pub_rec_bankruptcies. ","2d692393":"<a id=\"ch6\"><\/a>\n# 6. Scaling and train test split\n---","af1044f7":"## Earliest_cr_line feature\nThis appears to be a historical time stamp feature. Extract the year from this feature using a .apply function, then convert it to a numeric feature. Set this new data to a feature column called 'earliest_cr_year'. Then drop the earliest_cr_line feature.","92d4d867":"<a id=\"ch10\"><\/a>\n# 10. Predicting on a  new customer\n---\n***Would you offer this person a loan?***","a92a2551":"<a id=\"ch1\"><\/a>\n# 1. Introduction \n---\n\nOne of the objectives of this notebook is to **show step-by-step how to visualize the dataset and assess whether or not a new customer is likely to pay back the loan.**\n\nLendingClub is a US peer-to-peer lending company, headquartered in San Francisco, California. It was the first peer-to-peer lender to register its offerings as securities with the Securities and Exchange Commission, and to offer loan trading on a secondary market. LendingClub is the world's largest peer-to-peer lending platform.\n\nGiven historical data on loans given out with information on whether or not the borrower defaulted (charge-off), we can build a model that can predict if a borrower will pay back their loan. This way in the future when we get a new potential customer, **we can assess if they are likely to pay back the loan.**\n\nThe following questions will be answered throughout the Kernel:\n* ***Which features are available in the dataset?***\n* ***What is the distribution of numerical feature values across the samples?***\n* ***What is the length of the dataframe?***\n* ***What is the total count of missing values per column?***\n* ***How many unique employment job titles are there?***\n* ***Do you wonder how lending companies choose whether to give you money or not?***\n* ***How does a lending company decide how much money to give you?***\n* ***Would you offer this person a loan?***\n* ***Did this person actually end up paying back their loan?***\n\nIf you have a question or feedback, do not hesitate to write and if you like this kernel,<b><font color='green'> please upvote! <\/font><\/b>\n\n<img src=\"https:\/\/images.pexels.com\/photos\/259165\/pexels-photo-259165.jpeg?auto=compress&cs=tinysrgb&h=750&w=1260\" title=\"source: www.pexels.com\" width=\"500\" height=\"500\"\/>\n<br>","4ece002a":"## Feedback\n* **Your feedback is much appreciated**\n* **<b><font color='green'>Please UPVOTE if you LIKE this notebook<\/font><\/b>**\n* **Comment if you have any doubts or you found any errors in the notebook**","cfd1a4c7":"<a id=\"ch3\"><\/a>\n# 3. Exploratory Data Analysis\n---\n\n## Analyze by visualizing data\nGet an understanding for which variables are important, view summary statistics, and visualize the data.","b665cda2":"## Percentage of missing values per column\n* In the plot we can see how much data is missing as a percentage of the total data.\n* Notice that there is missing almost 10% of mortgage accounts, so we can not drop all those rows. \n* On the other hand, we could drop missing values in revol_util or pub_rec_bankruptcies."}}