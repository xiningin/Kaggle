{"cell_type":{"636af829":"code","cbf6a241":"code","17d14cc5":"code","46ced936":"code","30fdec15":"code","3117d733":"code","55dc2b38":"code","ef929d6d":"code","10904b36":"code","fb5d9a9f":"code","5a59c0c8":"code","a7c47e06":"code","234dc4c3":"code","488b4ed7":"code","0007a3e6":"code","d29001f8":"code","540e153e":"code","3d637614":"code","7eea6965":"code","9dbd4498":"code","c30adea3":"code","6712769b":"code","75336ca6":"code","e4e3e883":"code","35f891de":"code","2ba5a0d5":"code","d270e053":"code","f289e5a4":"code","265f67ee":"code","7884c4ce":"code","cf07932e":"code","1c20ce69":"code","358417e4":"code","fd582b23":"code","340768e9":"code","ebfe66b9":"markdown"},"source":{"636af829":"# We will use the official tokenization script created by the Google team\n!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","cbf6a241":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense,Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub\nimport tokenization","17d14cc5":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","46ced936":"train.head()","30fdec15":"text=train['text']","3117d733":"text.head()","55dc2b38":"text=text.str.lower()","ef929d6d":"import string","10904b36":"def remove_punctuation(text):\n    return text.translate(str.maketrans('','',string.punctuation))\ntext_clean=text.apply(lambda text:remove_punctuation(text))","fb5d9a9f":"text_clean.head()","5a59c0c8":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)","a7c47e06":"import re\ntext_clean=text_clean.apply(lambda x : remove_URL(x))","234dc4c3":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n","488b4ed7":"text_clean=text_clean.apply(lambda x : remove_html(x))","0007a3e6":"df = pd.DataFrame({\"text\": text_clean})\ndf.head()","d29001f8":"train.update(df)\ntrain.head()\ntrain.drop(columns=['keyword','location','id'],axis=1)","540e153e":"from sklearn.model_selection import train_test_split","3d637614":"x=train['text']\ny=train['target']","7eea6965":"xtrain,xtest,ytrain,ytest=train_test_split(x,y,test_size=0.4,random_state=42)","9dbd4498":"module_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","c30adea3":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()","6712769b":"do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()","75336ca6":"tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","e4e3e883":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","35f891de":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","2ba5a0d5":"train_input = bert_encode(xtrain.values, tokenizer, max_len=160)","d270e053":"train_labels = ytrain.values","f289e5a4":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","265f67ee":"train_history = model.fit(\n    train_input, train_labels,\n    validation_split=0.2,\n    epochs=3,\n    batch_size=16\n)\n\nmodel.save('model.h5')","7884c4ce":"test_input = bert_encode(xtest.values, tokenizer, max_len=160)","cf07932e":"test_labels = ytest.values","1c20ce69":"ypred = model.predict(test_input)","358417e4":"from sklearn.metrics import mean_squared_error","fd582b23":"result=mean_squared_error(ypred,ytest)","340768e9":"result","ebfe66b9":"bert code is taken from xhlulu's notebook and i modified it to fit my data."}}