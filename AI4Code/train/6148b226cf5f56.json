{"cell_type":{"dfa191c4":"code","5f6274f2":"code","06546646":"code","72702024":"code","d201e07f":"code","6922f7ce":"code","03331156":"code","064e5841":"code","ed2b189b":"code","e700dc68":"code","90fb19d7":"code","1087ec29":"code","07445e9b":"code","3aae85a9":"code","9785f48d":"code","ee783d00":"code","14d85a68":"code","ccc575cf":"code","27e8f8c8":"code","751bc5b6":"code","ea85b8bc":"code","a52f08f3":"code","60b13837":"code","9a74d1c4":"code","b691ef9a":"code","8796a14c":"code","4d9578bd":"code","ddb53b4f":"code","197f9cbe":"code","0e9f7cf4":"code","d31527ed":"code","66ebe214":"code","57085204":"code","45623198":"code","5503724c":"code","1b6b0f52":"code","f7975902":"code","65f56726":"code","bf25485e":"code","8d67af73":"code","5ca1d817":"code","49fe13f2":"code","96b51686":"markdown","3ea4c3d9":"markdown","bd8da48e":"markdown","90463693":"markdown","d376a89a":"markdown","efdb049d":"markdown","14d42ad7":"markdown","15711dd2":"markdown","abbf3823":"markdown","ae4131fa":"markdown","16b859d2":"markdown","32d0c5ea":"markdown","65537045":"markdown","d32c392a":"markdown","104ffb8d":"markdown","72443934":"markdown","10bb5c9e":"markdown","743b09d5":"markdown","15e5a9ef":"markdown","7b5d984e":"markdown","45db3715":"markdown","e740ae5b":"markdown","a0bd3214":"markdown","87a197a8":"markdown","7659c684":"markdown","c4e6fa40":"markdown","e3888e46":"markdown","44f1785f":"markdown","49c887a2":"markdown","448ec13a":"markdown"},"source":{"dfa191c4":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nimport seaborn as sns\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import StratifiedShuffleSplit","5f6274f2":"trainDF = pd.read_csv('\/kaggle\/input\/keystroke-dynamics-challenge-1\/train.csv')\ntestDF = pd.read_csv('\/kaggle\/input\/keystroke-dynamics-challenge-1\/test.csv')","06546646":"trainDF.head()","72702024":"testDF.head()","d201e07f":"print('No. of rows in training dataset:',len(trainDF))\nprint('No. of users for which training data is present:',trainDF.user.nunique())","6922f7ce":"print('No. of rows in test dataset:',len(testDF))","03331156":"trainDF1 = trainDF\nfor i in range(1,13):\n    trainDF1['PPD-'+str(i)] = trainDF1['press-'+str(i)] - trainDF1['press-'+str(i-1)]\n    trainDF1['RPD-'+str(i)] = trainDF1['release-'+str(i)] - trainDF1['press-'+str(i-1)]\n\nfor i in range(13):\n    trainDF1['HD-'+str(i)] = trainDF1['release-'+str(i)] - trainDF1['press-'+str(i)]\n    \ntestDF1 = testDF\nfor i in range(1,13):\n    testDF1['PPD-'+str(i)] = testDF1['press-'+str(i)] - testDF1['press-'+str(i-1)]\n    testDF1['RPD-'+str(i)] = testDF1['release-'+str(i)] - testDF1['press-'+str(i-1)]\n\nfor i in range(13):\n    testDF1['HD-'+str(i)] = testDF1['release-'+str(i)] - testDF1['press-'+str(i)]","064e5841":"trainDF1.head()","ed2b189b":"# Check stats of first 5 users i.e. 5 x 8 typing patterns\nnoOfUsers = 5\nif noOfUsers == -1:\n    trainDF2 = trainDF1\nelse:\n    trainDF2 = trainDF1[:noOfUsers*8]","e700dc68":"temp1 = pd.DataFrame({'Min':trainDF2.min(),'Max':trainDF2.max()})\ntemp1.head()","90fb19d7":"for i in range(1,13):\n    ax = sns.scatterplot(x='RPD-'+str(i),y='PPD-'+str(i),hue='user',data=trainDF2)\n\n# Small trick to avoid repeating legends: https:\/\/stackoverflow.com\/a\/36268401\/5370202    \nhandles, labels = ax.get_legend_handles_labels()\nax.legend(handles[:noOfUsers], labels[:trainDF2.user.nunique()])\nax.set_title('Scatterplot of PPD vs RPD')","1087ec29":"plt.figure(figsize=(15,5))\nfor i in range(1,13):\n    sns.swarmplot(y='RPD-'+str(i),x='user',data=trainDF2).set_title('Swarm of Release-Press Duration for users')","07445e9b":"plt.figure(figsize=(15,5))\nfor i in range(1,13):\n    sns.swarmplot(y='PPD-'+str(i),x='user',data=trainDF2).set_title('Swarm of Press-Press Duration for users')","3aae85a9":"plt.figure(figsize=(15,5))\nfor i in range(13):\n    sns.swarmplot(y='HD-'+str(i),x='user',data=trainDF2).set_title('Swarm of Hold Duration for users')","9785f48d":"# value_vars_cols = ['HD-'+str(i) for i in range(13)]\n\ndrop_cols_HD_analysis = ['PPD-'+str(i) for i in range(1,13)] + ['RPD-'+str(i) for i in range(1,13)] + ['release-'+str(i) for i in range(13)]\n\ntrainDF_HD_analysis = trainDF2.drop(columns=drop_cols_HD_analysis)\ntrainDF_HD_analysis['id'] = trainDF_HD_analysis.index\ntrainDF_HD_analysis = pd.wide_to_long(trainDF_HD_analysis,['press-','HD-'],i='id',j='key_no').sort_values(by=['user','id','key_no'])\ntrainDF_HD_analysis","ee783d00":"plt.figure(figsize=(15,10))\nsns.scatterplot(x='press-',y='HD-',hue='user',data=trainDF_HD_analysis,palette='deep')","14d85a68":"plt.figure(figsize=(15,10))\n# sns.load_dataset(trainDF_HD_analysis)\nsns.lineplot(x='press-',y='HD-',hue='user',units='id',estimator=None,data=trainDF_HD_analysis.reset_index(),palette='deep').set_title('Line plots for each key sequence')","ccc575cf":"# value_vars_cols = ['HD-'+str(i) for i in range(13)]\n\ndrop_cols_PPD_analysis = ['HD-'+str(i) for i in range(13)] + ['RPD-'+str(i) for i in range(1,13)] + ['release-'+str(i) for i in range(13)] + ['press-0']\n\ntrainDF_PPD_analysis = trainDF2.drop(columns=drop_cols_PPD_analysis)\ntrainDF_PPD_analysis['id'] = trainDF_PPD_analysis.index\ntrainDF_PPD_analysis = pd.wide_to_long(trainDF_PPD_analysis,['press-','PPD-'],i='id',j='key_no').sort_values(by=['user','id','key_no'])\n# trainDF_PPD_analysis","27e8f8c8":"plt.figure(figsize=(15,10))\nsns.scatterplot(x='press-',y='PPD-',hue='user',data=trainDF_PPD_analysis,palette='deep')","751bc5b6":"plt.figure(figsize=(15,10))\n# sns.load_dataset(trainDF_HD_analysis)\nsns.lineplot(x='press-',y='PPD-',hue='user',units='id',estimator=None,data=trainDF_PPD_analysis.reset_index(),palette='deep').set_title('Line plots for each key sequence')","ea85b8bc":"# value_vars_cols = ['HD-'+str(i) for i in range(13)]\n\ndrop_cols_RPD_analysis = ['HD-'+str(i) for i in range(13)] + ['PPD-'+str(i) for i in range(1,13)] + ['release-'+str(i) for i in range(13)] + ['press-0']\n\ntrainDF_RPD_analysis = trainDF2.drop(columns=drop_cols_RPD_analysis)\ntrainDF_RPD_analysis['id'] = trainDF_RPD_analysis.index\ntrainDF_RPD_analysis = pd.wide_to_long(trainDF_RPD_analysis,['press-','RPD-'],i='id',j='key_no').sort_values(by=['user','id','key_no'])\n# trainDF_RPD_analysis","a52f08f3":"plt.figure(figsize=(15,10))\nsns.scatterplot(x='press-',y='RPD-',hue='user',data=trainDF_RPD_analysis,palette='deep')","60b13837":"plt.figure(figsize=(15,10))\n# sns.load_dataset(trainDF_HD_analysis)\nsns.lineplot(x='press-',y='RPD-',hue='user',units='id',estimator=None,data=trainDF_RPD_analysis.reset_index(),palette='deep').set_title('Line plots for each key sequence')","9a74d1c4":"## Training Data\ndrop_cols_HD_analysis = ['PPD-'+str(i) for i in range(1,13)] + ['RPD-'+str(i) for i in range(1,13)] + ['release-'+str(i) for i in range(13)]\n\ntrainDF_HD_analysis = trainDF1.drop(columns=drop_cols_HD_analysis)\ntrainDF_HD_analysis['id'] = trainDF_HD_analysis.index\ntrainDF_HD_analysis = pd.wide_to_long(trainDF_HD_analysis,['press-','HD-'],i='id',j='key_no').sort_values(by=['user','id','key_no'])\n\ndrop_cols_PPD_analysis = ['HD-'+str(i) for i in range(13)] + ['RPD-'+str(i) for i in range(1,13)] + ['release-'+str(i) for i in range(13)] + ['press-0']\n\ntrainDF_PPD_analysis = trainDF1.drop(columns=drop_cols_PPD_analysis)\ntrainDF_PPD_analysis['id'] = trainDF_PPD_analysis.index\ntrainDF_PPD_analysis = pd.wide_to_long(trainDF_PPD_analysis,['press-','PPD-'],i='id',j='key_no').sort_values(by=['user','id','key_no'])\n\ndrop_cols_RPD_analysis = ['HD-'+str(i) for i in range(13)] + ['PPD-'+str(i) for i in range(1,13)] + ['release-'+str(i) for i in range(13)] + ['press-0']\n\ntrainDF_RPD_analysis = trainDF1.drop(columns=drop_cols_RPD_analysis)\ntrainDF_RPD_analysis['id'] = trainDF_RPD_analysis.index\ntrainDF_RPD_analysis = pd.wide_to_long(trainDF_RPD_analysis,['press-','RPD-'],i='id',j='key_no').sort_values(by=['user','id','key_no'])\n\n\n## Test Data\ntestDF_HD_analysis = testDF1.drop(columns=drop_cols_HD_analysis)\ntestDF_HD_analysis['id'] = testDF_HD_analysis.index\ntestDF_HD_analysis = pd.wide_to_long(testDF_HD_analysis,['press-','HD-'],i='id',j='key_no').sort_values(by=['id','key_no'])\n\ntestDF_PPD_analysis = testDF1.drop(columns=drop_cols_PPD_analysis)\ntestDF_PPD_analysis['id'] = testDF_PPD_analysis.index\ntestDF_PPD_analysis = pd.wide_to_long(testDF_PPD_analysis,['press-','PPD-'],i='id',j='key_no').sort_values(by=['id','key_no'])\n\ntestDF_RPD_analysis = testDF1.drop(columns=drop_cols_RPD_analysis)\ntestDF_RPD_analysis['id'] = testDF_RPD_analysis.index\ntestDF_RPD_analysis = pd.wide_to_long(testDF_RPD_analysis,['press-','RPD-'],i='id',j='key_no').sort_values(by=['id','key_no'])","b691ef9a":"## Join these individual tables together\ntestDFCombined = testDF_HD_analysis.join(testDF_RPD_analysis.drop(columns=['press-']),rsuffix='RPD_').join(testDF_PPD_analysis.drop(columns=['press-']),rsuffix='PPD_')\n\ntrainDFCombined = trainDF_HD_analysis.join(trainDF_RPD_analysis.drop(columns=['user','press-']),rsuffix='RPD_').join(trainDF_PPD_analysis.drop(columns=['user','press-']),rsuffix='PPD_')\ntrainDFCombined","8796a14c":"plt.figure(figsize=(15,10))\nplt.subplot(2,2,1)\nsns.distplot(trainDFCombined['HD-']).set_title('Hist of Hold Duration')\nplt.subplot(2,2,2)\nsns.distplot(trainDFCombined['PPD-']).set_title('Hist of Press-Press Duration')\nplt.subplot(2,2,3)\nsns.distplot(trainDFCombined['RPD-']).set_title('Hist of Release-Press Duration')","4d9578bd":"noOfBins = 10\n\n## Training Data\nHDMax = trainDFCombined['HD-'].max()\nRPDMax = trainDFCombined['RPD-'].max()\nPPDMax = trainDFCombined['PPD-'].max()\nprint('Max values in train are: HDMax:',HDMax,'RPDMax:',RPDMax,'PPDMax:',PPDMax)\nlabels = [i for i in range(noOfBins)]\n\ntrainDFCombined['HDEnc'],HDBins = pd.qcut(trainDFCombined['HD-'],retbins=True,labels=labels,q=noOfBins)\ntrainDFCombined['PPDEnc'],RPDBins = pd.qcut(trainDFCombined['PPD-'],retbins=True,labels=labels,q=noOfBins)\ntrainDFCombined['RPDEnc'],PPDBins = pd.qcut(trainDFCombined['RPD-'],retbins=True,labels=labels,q=noOfBins)\n\ntrainDFCombined['HDEnc'] = trainDFCombined['HDEnc'].astype(str).replace('nan',-1).astype(int)\ntrainDFCombined['PPDEnc'] = trainDFCombined['PPDEnc'].astype(str).replace('nan',-1).astype(float)\ntrainDFCombined['RPDEnc'] = trainDFCombined['RPDEnc'].astype(str).replace('nan',-1).astype(float)\n\n\n## Test Data\nHDMax = testDFCombined['HD-'].max()\nRPDMax = testDFCombined['RPD-'].max()\nPPDMax = testDFCombined['PPD-'].max()\nprint('Max values in test data are: HDMax:',HDMax,'RPDMax:',RPDMax,'PPDMax:',PPDMax)\nlabels = [i for i in range(noOfBins)]\n\ntestDFCombined['HDEnc'] = pd.cut(testDFCombined['HD-'],labels=labels,bins=HDBins)\ntestDFCombined['PPDEnc'] = pd.cut(testDFCombined['PPD-'],labels=labels,bins=RPDBins)\ntestDFCombined['RPDEnc'] = pd.cut(testDFCombined['RPD-'],labels=labels,bins=PPDBins)\n\ntestDFCombined['HDEnc'] = testDFCombined['HDEnc'].astype(str).replace('nan',-1).astype(float)\ntestDFCombined['PPDEnc'] = testDFCombined['PPDEnc'].astype(str).replace('nan',-1).astype(float)\ntestDFCombined['RPDEnc'] = testDFCombined['RPDEnc'].astype(str).replace('nan',-1).astype(float)","ddb53b4f":"trainDFCombined","197f9cbe":"testDFCombined","0e9f7cf4":"## Lower limit values of bins created\nHDBins, RPDBins, PPDBins, 'No. of buckets: '+str(len(HDBins)-1)","d31527ed":"plt.figure(figsize=(15,8))\nnoOfUsers = 5\nplt.subplot(3,1,1)\nsns.swarmplot(y='HDEnc',x='user',data=trainDFCombined[:8*12*noOfUsers],palette='deep').set_title('Swarm plot of binned hold duration')\nplt.subplot(3,1,2)\nsns.swarmplot(y='PPDEnc',x='user',data=trainDFCombined[:8*12*noOfUsers],palette='deep').set_title('Swarm plot of binned press-press duration')\nplt.subplot(3,1,3)\nsns.swarmplot(y='RPDEnc',x='user',data=trainDFCombined[:8*12*noOfUsers],palette='deep').set_title('Swarm plot of binned release-press duration')","66ebe214":"trainDFCombinedHDAvg = trainDFCombined.reset_index().groupby(['user','key_no'])['HDEnc'].mean()\ntrainDFCombinedPPDAvg = trainDFCombined.reset_index().groupby(['user','key_no'])['PPDEnc'].mean()\ntrainDFCombinedRPDAvg = trainDFCombined.reset_index().groupby(['user','key_no'])['RPDEnc'].mean()\ntempDF = pd.DataFrame({'HD':trainDFCombinedHDAvg,'PPD':trainDFCombinedPPDAvg,'RPD':trainDFCombinedRPDAvg})\n\ntrainDF_HDProperties = tempDF.reset_index().groupby('user')['HD'].apply(np.array)\ntrainDF_PPDProperties = tempDF.reset_index().groupby('user')['PPD'].apply(np.array)\ntrainDF_RPDProperties = tempDF.reset_index().groupby('user')['RPD'].apply(np.array)\n\ntrainDF_UserProps = pd.DataFrame({'HD':trainDF_HDProperties, 'PPD':trainDF_PPDProperties, 'RPD':trainDF_RPDProperties})\n\ntrainDF_UserProps = pd.DataFrame(trainDF_UserProps.HD.tolist(),index = trainDF_UserProps.index).add_prefix('HD_').join(\n    pd.DataFrame(trainDF_UserProps.PPD.tolist(),index = trainDF_UserProps.index).add_prefix('PPD_')\n).join(\n    pd.DataFrame(trainDF_UserProps.RPD.tolist(),index = trainDF_UserProps.index).add_prefix('RPD_')\n)\n\n# Average bin keystrokes for each of the 110 users\ntrainDF_UserProps","57085204":"trainDFCombinedHDAvg = testDFCombined.reset_index().groupby(['id','key_no'])['HDEnc'].mean()\ntrainDFCombinedPPDAvg = testDFCombined.reset_index().groupby(['id','key_no'])['PPDEnc'].mean()\ntrainDFCombinedRPDAvg = testDFCombined.reset_index().groupby(['id','key_no'])['RPDEnc'].mean()\ntempDF = pd.DataFrame({'HD':trainDFCombinedHDAvg,'PPD':trainDFCombinedPPDAvg,'RPD':trainDFCombinedRPDAvg})\n\ntrainDF_HDProperties = tempDF.reset_index().groupby('id')['HD'].apply(np.array)\ntrainDF_PPDProperties = tempDF.reset_index().groupby('id')['PPD'].apply(np.array)\ntrainDF_RPDProperties = tempDF.reset_index().groupby('id')['RPD'].apply(np.array)\n\ntestDF_UserProps = pd.DataFrame({'HD':trainDF_HDProperties, 'PPD':trainDF_PPDProperties, 'RPD':trainDF_RPDProperties})\n\ntestDF_UserProps = pd.DataFrame(testDF_UserProps.HD.tolist(),index = testDF_UserProps.index).add_prefix('HD_').join(\n    pd.DataFrame(testDF_UserProps.PPD.tolist(),index = testDF_UserProps.index).add_prefix('PPD_')\n).join(\n    pd.DataFrame(testDF_UserProps.RPD.tolist(),index = testDF_UserProps.index).add_prefix('RPD_')\n)\n\n# Bin allocation \ntestDF_UserProps","45623198":"trainDF_HDTemp = trainDFCombined.reset_index().groupby(['user','id'])['HDEnc'].apply(np.array)\ntrainDF_PPDTemp = trainDFCombined.reset_index().groupby(['user','id'])['PPDEnc'].apply(np.array)\ntrainDF_RPDTemp = trainDFCombined.reset_index().groupby(['user','id'])['RPDEnc'].apply(np.array)\n\ntrainDF_User_AllSampleProps = pd.DataFrame({'HD':trainDF_HDTemp, 'PPD':trainDF_PPDTemp, 'RPD':trainDF_RPDTemp})\n\ntrainDF_User_AllSampleProps = pd.DataFrame(trainDF_User_AllSampleProps.HD.tolist(),index = trainDF_User_AllSampleProps.index).add_prefix('HD_').join(\n    pd.DataFrame(trainDF_User_AllSampleProps.PPD.tolist(),index = trainDF_User_AllSampleProps.index).add_prefix('PPD_')\n).join(\n    pd.DataFrame(trainDF_User_AllSampleProps.RPD.tolist(),index = trainDF_User_AllSampleProps.index).add_prefix('RPD_')\n).reset_index().set_index('user').drop(columns=['id'])\n\ntrainDF_User_AllSampleProps","5503724c":"trainDF_HDTemp = testDFCombined.reset_index().groupby(['id'])['HDEnc'].apply(np.array)\ntrainDF_PPDTemp = testDFCombined.reset_index().groupby(['id'])['PPDEnc'].apply(np.array)\ntrainDF_RPDTemp = testDFCombined.reset_index().groupby(['id'])['RPDEnc'].apply(np.array)\n\ntestDF_User_AllSampleProps = pd.DataFrame({'HD':trainDF_HDTemp, 'PPD':trainDF_PPDTemp, 'RPD':trainDF_RPDTemp})\n\ntestDF_User_AllSampleProps = pd.DataFrame(testDF_User_AllSampleProps.HD.tolist(),index = testDF_User_AllSampleProps.index).add_prefix('HD_').join(\n    pd.DataFrame(testDF_User_AllSampleProps.PPD.tolist(),index = testDF_User_AllSampleProps.index).add_prefix('PPD_')\n).join(\n    pd.DataFrame(testDF_User_AllSampleProps.RPD.tolist(),index = testDF_User_AllSampleProps.index).add_prefix('RPD_')\n)\n\ntestDF_User_AllSampleProps","1b6b0f52":"knn_summary = KNeighborsClassifier(1)\ntrainX_summary = trainDF_UserProps.reset_index().drop(columns=['user'])\ntrainY_summary = trainDF_UserProps.index\n\n# testX_summary = testDF_UserProps.reset_index().drop(columns=['id'])\n\nknn_summary.fit(trainX_summary,trainY_summary)\n\naccuracy_score(knn_summary.predict(trainX_summary),trainY_summary)","f7975902":"trainX_allSamples = trainDF_User_AllSampleProps.reset_index().drop(columns=['user'])\ntrainY_allSamples = trainDF_User_AllSampleProps.index\n\ndef getCrossValidationAccuracy(n_neighbours):\n    knn_allSamples = KNeighborsClassifier(n_neighbours)\n    sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\n    acc = []\n    for train_index, test_index in sss.split(trainX_allSamples, trainY_allSamples):\n        knn_allSamples.fit(trainX_allSamples.loc[train_index],trainY_allSamples[train_index])\n        acc += [accuracy_score(knn_allSamples.predict(trainX_allSamples.loc[test_index]),trainY_allSamples[test_index])]\n    return sum(acc) \/ len(acc)\nallAttemptsAcc = [getCrossValidationAccuracy(i) for i in range(1,8)]\nprint('Accuracies:',allAttemptsAcc)\nsns.lineplot(y=allAttemptsAcc,x=range(1,8)).set_title('Cross-Val Accuracy v\/s no. of neighbours')","65f56726":"# knn_allSamples = KNeighborsClassifier(1)\n# knn_allSamples.fit(trainX_allSamples,trainY_allSamples)\n\n# testX_allSamples = testDF_User_AllSampleProps.reset_index().drop(columns=['id'])\n# textPreds_allSamples = knn_allSamples.predict(testX_allSamples)\n# pd.DataFrame({'user':textPreds_allSamples},index=testX_allSamples.index).to_csv('submission.csv',index=False)","bf25485e":"%%time\nfrom sklearn.model_selection import GridSearchCV, StratifiedShuffleSplit\nfrom xgboost.sklearn import XGBClassifier\n\nxgb1 = XGBClassifier(\n    learning_rate =0.1,\n    n_estimators=10,\n    max_depth=5,\n    min_child_weight=3,\n    gamma=0,\n    subsample=0.8,\n    colsample_bytree=0.8,\n    objective= 'multi:softmax',\n    num_class=trainY_allSamples.nunique(),\n    nthread=4,\n    seed=27)\nparam_search = {\n    'learning_rate': [0.05, 0.1],\n    'n_estimators': [100,200,210,230,250,270,290,310,330],\n    'max_depth': range(4,10,1)\n}\ngsearch2b = GridSearchCV(estimator = xgb1,param_grid = param_search, scoring='accuracy',n_jobs=4,iid=False,cv=StratifiedShuffleSplit(n_splits=3, test_size=0.2, random_state=0), verbose=1)\ngsearch2b.fit(trainX_allSamples, trainY_allSamples)","8d67af73":"print('Best Estimator:\\n',gsearch2b.best_estimator_)","5ca1d817":"sss = StratifiedShuffleSplit(n_splits=5, test_size=0.2, random_state=0)\naccs = []\nfor train_index, test_index in sss.split(trainX_allSamples, trainY_allSamples):\n    gsearch2b.best_estimator_.fit(trainX_allSamples.loc[train_index],trainY_allSamples[train_index])\n    acc = accuracy_score(gsearch2b.best_estimator_.predict(trainX_allSamples.loc[test_index]),trainY_allSamples[test_index])\n    print('Accuracy Score:', acc)\n    accs += [acc]\nprint('Average Accuracy:',sum(accs)\/len(accs))","49fe13f2":"gsearch2b.best_estimator_.fit(trainX_allSamples,trainY_allSamples)\n\ntestX_allSamples = testDF_User_AllSampleProps.reset_index().drop(columns=['id'])\ntextPreds_allSamples = gsearch2b.best_estimator_.predict(testX_allSamples)\npd.DataFrame({'idx':testX_allSamples.index,'user':textPreds_allSamples},index=testX_allSamples.index).to_csv('submission.csv',index=False)","96b51686":"Our test dataset does not have expected values. In the above scenario, since we had summarised 39 columns (13 x 3 types of duration bins) per user, we have 110 rows. Each corresponding to 1 user. Thus, it was impossible to create validation dataset to determine real-world prediction accuracy.\n\nBut if we consider 8 samples per user, we can consider few of these for validation. Now, since we have 110 users but only 8 samples per user, we need to ensure that enough samples per user are present in training set, so that during test time, at least few samples would be present for each test user. For this, we perform `StratifiedShuffleSplit`.\n\nHere, I am using n_splits as 5 which means 5 different splits will be created (for 5-fold cross validation) and test_size as 0.2 resulting in (8 x 0.2) = 1.6 test samples for (8 - 1.6) = 6.4 training samples.","3ea4c3d9":"# Keystroke Dynamics Analysis and Prediction w\/ XGB\n\nKeystroke dynamics basically deals with the analysis of keystroke patterns of a user. In one of its use cases, this can essentially enable companies to provide non-intrusive real-time authentication of users.\n\nBy capturing the duration between 2 key presses, the duration of pressing down a key, and the duration between the current key release and the next key press can provide great insights about the user. The next time this user logs in, by comparing his current typing pattern with his previous typing patterns, companies can authenticate whether the logged in user is legitimate or fraudulent.\n\n\n<br\/>\n<img src='https:\/\/media0.giphy.com\/media\/13GIgrGdslD9oQ\/giphy.gif'\/>\n<br\/>\n\n\nThe dataset [Keystroke dynamics challenge 1 | Kaggle](https:\/\/www.kaggle.com\/c\/keystroke-dynamics-challenge-1\/data) has been used in this notebook. This dataset captures typing attempts of 110 users. Each user has attempted 8 times to type the string 'united states' and the corresponding timestamps of key press and release relative to the first key press have been captured.","bd8da48e":"## Generate features\n\nNow, by itself, these timestamps would mean nothing. We can gain insights on user typing patterns by creating features such as press-press duration (PPD), hold duration (HD), release-press duration (RPD).\n\nThe image shows the duration calculations for pressing 2 keys - A and B. Here, the smaller keys represent the key press event and the larger ones represent the key release event.\n\n<br\/>\n<img src='https:\/\/www.researchgate.net\/profile\/Ivan_Homoliak\/publication\/324536760\/figure\/fig4\/AS:615900302544896@1523853483320\/Low-level-keystroke-dynamics-features_W640.jpg'\/>\n<br\/>\n\nImage Credits: ['The Wolf of SUTD (TWOS): A dataset of malicious insider threat behavior based on a gamified competition'](https:\/\/www.researchgate.net\/publication\/324536760_The_Wolf_of_SUTD_TWOS_A_dataset_of_malicious_insider_threat_behavior_based_on_a_gamified_competition)","90463693":"Now, instead of determing buckets by ourselves, we can use another interesting pandas function `qcut`. In this function we specify the column name over which we want to perform the binning\/bucketing. This function checks the distribution of values in the specified column and divides the distribution into equal frequence `q` number of bins. This ensures that each bin has equal frequency and we can get more meaningful bins. \n\nWe are setting `retbins` to True so that we can later use these bins over our test dataset. For test datasets, since we already have these bins with us, we need not perform `qcut`. Instead we'll use `cut` which directly assigns the values into the appropriate bin.\n\nWe are encoding the values to labels 0 - 9 in `HDEnc`, `PPDEnc`, `RPDEnc` corresponding to the bins. Since, there are 12 RPD, 12 PPD, 13 HD values corresponding to the 13 key strokes in a typing pattern, we fill the missing RPD, PPD values by -1","d376a89a":"## KNN","efdb049d":"From the above plot, we can see that we get the highest accuracy, if we use n_neighbours = 1","14d42ad7":"# EDA\n\nLet's now perform an exploratory data analysis on these features to understand the significant features.","15711dd2":"### Hold Duration key sequences\n\nHere, an interesting pandas function `wide_to_long` is used. In order to analyse these sequences, we need all these durations to present in 1 column as against spread across multiple columns of a row. So, `wide_to_long` stacks all these values present in multiple columns into a single column. By sorting in the sequence: `user`,`id`,`key_no`, we ensure that all durations of a single typing pattern are together.","abbf3823":"# Models\n\nHere, you could either simply use DF: `trainDF_UserProps` and find Euclidean distance to identify the closest user (KNN with 1 neighbour)\n\nOr, KNN could be used on the DF: `trainDF_User_AllSampleProps` having 8 training examples per user","ae4131fa":"Here, we are using an interesting feature of seaborn charts. Now, since we have 12 RPD values per typing pattern (consecutive pairs from 13 key presses), we need to visualise all these durations together. So, we invoke swarmplot for each of these 12 columns back-to-back and seaborn overlays the data from all of these visualisations together.","16b859d2":"### Release-Press Duration key sequences","32d0c5ea":"## Swarm Plots of RPD, PPD, HD\n\nNext, let's analyse the swarmplots of RPD, PPD, HD for each user. Since, there are 8 typing patterns per user and each typing pattern consists of 13 key strokes, we have 8 x 13 = 104 data points per user.","65537045":"## Further Analysis of HD, PPD, RPD by understanding key sequences\n\nSince we know that each typing pattern consists of 13 keystrokes, let's try to visualise scatter plots and line plots of how these durations would look in terms of the timestamps of key press events. Note, since each typing pattern is independent of the other, we construct 8 line plots (consisting of 13 data points) for each of the 5 users resulting in 8 x 5 = 40 line plots.","d32c392a":"Note that, in the above section, we were analysing typing patterns of only 5 users.\n\nIn the following code segment, we are repeating the same steps we performed above for entire train and test data.","104ffb8d":"# First Glance\n\nLet's have a first glance of how the dataset actually looks like","72443934":"### Press-Press Duration key sequences","10bb5c9e":"Now, let's see the swarmplots of the encodings formed by these buckets","743b09d5":"## User-level Complete Collection of Bins per Keystroke\n\nIn the above 2 tables, we identified the average bin value for each keystroke per user. As against this, let's keep other views handy so that we could compare which seems to be more useful. In the following tables, instead of finding the average bin for each keystroke over 8 typing patterns, we retain all of the typing patterns. This could help us in using KNN to identify the closest key dynamics signature for a user.","15e5a9ef":"## Scatterplot of PPD vs RPD\n\nLet's check how a scatterplot of RPD v\/s PPD will look like. Note, PPD = HD + RPD. Thus, these should be almost linearly related which is evident from the following plot","7b5d984e":"## User-level Average Bin per Keystroke Signature\n\nLet's find the average bin for each keystroke for each user.","45db3715":"This plot reveals the same detail as PPD. \n\nNotice the jagged lines for user 4. The RPD also suddenly increases and then becomes very low for the next key. This means that this user waits for a relatively longer time before typing in 2 keys back to back. So, we could say this user typically types in groups of 2 keys","e740ae5b":"Since, the dataset itself is so small, this is the best accuracy I could get","a0bd3214":"Semantically speaking, this indicates the average speed of every user. For example, we can see from the RPD, PPD durations in the swarm plots. For instance, user 4 has many encodings in the higher range of bins. At the same time, user 2 has more encodings in the lower range of bins. This implies that user 4 types relatively slowly as compared to user 2.\n\nAs you can see if we consider the 8 training samples per user, we can identify a signature per user in terms of these 3 variables. So, in our next step, we will create the probability of values in each of the bins for each user by aggregating all these histograms into 1 summary histogram per user.\n\nHere, to aggregate these probability values, we have 2 options - \n* Aggregate typing patterns of each user into those 10 bins and determine normalised frequency (probability) of durations\n* Aggregate typing patterns of each user into average bin value for each keystroke\n\nNow, when typing united states, the relative position of consecutive pairs of keystrokes leads to slight variations in these inter-keystroke durations. This will inherently get considered in the 2nd approach.\n\nAt the same time, the 1st approach would be best suitable in cases where the length of the text sequences is not fixed. Since in our dataset, the length is fixed, we select the 2nd approach.","87a197a8":"## XGB Model\n\nAlthough this dataset is very small - 8 samples per class and 39 feature columns, let's still try to use XGB for this dataset. Here, as well, let's analyse the cross validation accuracy to be sure that this model can be reliable.","7659c684":"### If you liked my work, please leave an upvote to let me understand the quality of my work. Also, if you have any suggestions or improvements or doubts, I'll be more than happy to discuss them in the comments section below :) \n\n## Keep Kaggling!","c4e6fa40":"An exhaustive search is convenient but as you can see the downside from the wall time above. Yes, it takes so much time! But for now, since we have found the optimal parameters for our dataset, let's go ahead and view the best estimator obtained from GridSearchCV","e3888e46":"**This plot reveals a very interesting detail.**\n\nNotice the jagged lines for user 4 (red lines). The PPD suddenly increases and then becomes very low for the next key. This means that this user waits for a relatively longer time before typing in 2 keys back to back. So, we could say this user typically types in groups of 2 keys","44f1785f":"As, you can see from the swarm plots, the press-press duration, release-press durations is roughly the same across all users. Thus, directly using an average duration will not be helpful.\n\nHowever, hold duration is roughly different for each user which is correct since each user has a different typing speed according to his familiarity with typing.\n\nLet's plan to use histograms to check if any variations could be identified","49c887a2":"The following figures are histograms of HD, PPD, RPD. This makes it clear to have bucketing for better classification.","448ec13a":"# Data Preparation\n\nInstead of directly using these durations as inputs, we could group these durations into histograms which would represent groups of different typing speeds. For e.g. a slow typer would have his keystroke durations falling in the histogram bucket of larger durations. At the same time, a fast typer (or perhaps a touch typer) would have his keystroke durations falling in the histogram bucket of smaller durations.\n\nThus, we will convert the entire training and test data by using this histogram technique. I believe, this should be enough to gain such insights inherently."}}