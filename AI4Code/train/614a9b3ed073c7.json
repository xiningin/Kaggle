{"cell_type":{"317abba1":"code","bcf48184":"code","abd5769c":"code","41d4971d":"code","86b61ef1":"code","f528cde6":"code","fb1e287f":"code","f734147d":"code","1d8bd404":"code","3438cc93":"code","058b38c7":"code","8bdf7117":"code","996b64c2":"code","ecc1b66f":"code","22dcdb52":"code","3f8502b8":"code","07cc48d2":"code","9798c9b3":"code","aba93d3a":"code","c83b95b9":"code","199f7a61":"code","e5f61053":"code","b33049f1":"code","e4092d7b":"code","5f124aa6":"code","a0eb0ed0":"code","1c2d08b2":"code","d4c74cf1":"code","a420185d":"code","ae74ee61":"markdown","727c7f48":"markdown","c5a2bc4c":"markdown","e9dc11d4":"markdown","c55990f1":"markdown","d2742795":"markdown","2e540677":"markdown","84641f65":"markdown","83f0db2d":"markdown","1325a2d7":"markdown","aba8ec78":"markdown","4ee46acf":"markdown","1100b4fc":"markdown","0db42aaa":"markdown","493422fb":"markdown","f745e18d":"markdown","c9ddf547":"markdown","7f8c4804":"markdown","5c23ce2b":"markdown"},"source":{"317abba1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Ignore pandas warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Any results you write to the current directory are saved as output.","bcf48184":"# manual parameters\nTEST_RATIO = 0.1\nRANDOM_SEED = 42","abd5769c":"# reading in the datasets\ntrain = pd.read_csv(r'..\/input\/train.csv')\npredict = pd.read_csv(r'..\/input\/test.csv')\n\n# we will have to apply our feature engineering to both sets\nboth_datasets = [train, predict]\n\n# getting an idea of what we're dealing with\ntrain.head(10)","41d4971d":"train.append(predict, sort=False).info()","86b61ef1":"\"\"\" values taken from the training set, which needn't be changed in the loop \"\"\"\n\n# ports where people boarded\nembark = train[['Embarked', 'PassengerId']] \\\n    .groupby(['Embarked']) \\\n    .count() \\\n    .rename(columns={'PassengerId': 'PassengerCount'})\nembark = embark.PassengerCount.to_dict() \n\n# calculating the median fare per passenger class\nfare_medians = train[['Pclass', 'Fare']] \\\n    .groupby('Pclass') \\\n    .median() \\\n    .to_dict()['Fare']","f528cde6":"# looping through both train and predict\nfor df in both_datasets:\n    \n    \"\"\" age \"\"\"\n    # create a random distribution of age based on the dataframe's stats\n    mu, sigma, n = train.Age.mean(), train.Age.std(), df.Age.isnull().sum()\n    normal_distr = np.random.normal(mu, sigma, n)\n    \n    # impute it on the missing values\n    df.Age.loc[np.isnan(df.Age)] = normal_distr.astype(int)\n    \n    \"\"\" cabin \"\"\"\n    # convert Cabin to boolean (passenger has cabin y\/n)\n    df.Cabin = df.Cabin.apply(lambda x: 1 if not pd.isnull(x) else 0)\n    \n    \"\"\" embarked \"\"\"\n    # only two missing values, so let's just take the most common port\n    embark_imputes = max(embark, key=embark.get)\n    \n    # impute\n    df.Embarked = df.Embarked.fillna(embark_imputes)\n    \n    \"\"\" fare \"\"\"    \n    # impute by mapping Pclass to the previously defined dictionary\n    df.Fare = df.Fare.fillna(df.Pclass.map(fare_medians))","fb1e287f":"train.append(predict, sort=False).info()","f734147d":"import re\n\ndef get_title(name):\n    title = re.search(' ([A-Za-z]+)\\.', name)\n    if title: return title.group(1)\n\ndef get_int_age_group(age):\n    age_group = 'elderly'\n    if age <= 50: age_group = 'old_adult'\n    if age <= 40: age_group = 'mid_adult'\n    if age <= 30: age_group = 'young_adult'\n    if age <= 18: age_group = 'teenager'\n    if age <= 9 : age_group = 'child'\n        \n    return age_group","1d8bd404":"train.head(10)","3438cc93":"# looping through both train and predict\nfor df in both_datasets:\n    \n    \"\"\" name -> title \"\"\"\n    # extracting the title with regular expressions where there is one\n    df['Title'] = df.Name.apply(get_title)\n    \n    # processing the extract titled\n    df.Title = df.Title.replace({\n        'Mlle': 'Miss',\n        'Ms': 'Miss',\n        'Mme': 'Mrs'\n    })\n    \n    \"\"\" sex \"\"\"\n    df.Sex = df.Sex.map({\n        'male'  : 0,\n        'female': 1\n    })\n    \n    \"\"\" age \"\"\"\n    # convert ages to categories\n    df.Age = df.Age.apply(get_int_age_group)\n    \n    \"\"\" traveled alone \"\"\"\n    df['TraveledAlone'] = df.apply(lambda row: 1 if row['SibSp'] + row['Parch'] == 0 else 0, axis=1)","058b38c7":"# we want categories\nlabels = ['cheap', 'economy', 'business', 'first']\n\n\"\"\" fare \"\"\"\n# quartiles based on the training set\nfare_categorical_series, bins = pd.qcut(train['Fare'], q=4, retbins=True, labels=labels)\n\n# putting the series in the training set\ntrain['FareCategorical'] = fare_categorical_series\n\n# cutting the predict set based on the bins derived from the training set\npredict['FareCategorical'] = pd.cut(predict.Fare, bins=bins, labels=labels, include_lowest=True)\n","8bdf7117":" # any title that appears less than 4% of the time in the training set is treated as rare\ntitles = train[['Title', 'PassengerId']] \\\n    .groupby(['Title'], as_index=False) \\\n    .count() \\\n    .rename(columns={'PassengerId': 'Count'})\n\ntitles['Common'] = titles.Count \/ titles.Count.sum() > 0.04\ncommon_titles = list(titles.loc[titles.Common == True].Title.values)\n\n# looping over both train and predict datasets\nfor df in both_datasets:\n    \n    \"\"\" title \"\"\"    \n    # map the common titles to a number\n    df.Title = df.apply(lambda row: 'Rare' if row['Title'] not in common_titles else row['Title'], axis=1)","996b64c2":"def dummify(df, column_to_dummify):\n\n    dummies = pd.get_dummies(df[column_to_dummify], prefix=column_to_dummify)\n    df = pd.concat([df, dummies], axis=1)\n    df.drop(column_to_dummify, axis=1, inplace=True)\n\n    return df","ecc1b66f":"cols_to_dummify = ['Pclass', 'Embarked', 'Age', 'FareCategorical', 'Title']\n\nfor col in cols_to_dummify:\n    train = dummify(train, col)\n    predict = dummify(predict, col)","22dcdb52":"train.head()","3f8502b8":"# extracting the passenger column from the submission \n# dataset for later use during our submission\npassenger_id = predict.PassengerId\n\n# selection\nunwanted_features = ['PassengerId', 'Name', 'Ticket', 'Fare', 'SibSp', 'Parch']\n\n# dropping selection\ntrain   = train.drop(unwanted_features, axis=1)\npredict = predict.drop(unwanted_features, axis=1)","07cc48d2":"train.head(10)","9798c9b3":"features = train.drop('Survived', axis=1).values\nlabels = train.Survived.values\n\n# splitting our \nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features,\n                                                    labels,\n                                                    test_size=TEST_RATIO,\n                                                    random_state=RANDOM_SEED)","aba93d3a":"def training_accuracy_report(GridSearchCv_classifier):\n    print(\nf\"\"\"Best parameters: \n    {GridSearchCv_classifier.best_params_}\n    \nBest estimator:\n    {GridSearchCv_classifier.best_estimator_}\n    \nMean cross-validated score of the best_estimator on the training set:\n    {GridSearchCv_classifier.best_score_*100:,.2f}%    \n\"\"\")","c83b95b9":"from sklearn import svm\nfrom sklearn.model_selection import GridSearchCV\n\n# parameter grid for our search\nparameters = {\n    'C': [0.1, 1, 10],\n    'degree': [2, 3, 4], # only applies to kernel == 'poly'\n    'kernel': ('linear', 'rbf', 'poly')\n}\n\n# initiating our support vector machine\nsvm = svm.SVC(gamma='scale',\n              probability=True,\n              random_state=RANDOM_SEED)\nclassifier = GridSearchCV(estimator=svm,\n                          param_grid=parameters,\n                          cv=5)\n\n# fitting the model\nbest_svm = classifier.fit(X_train, y_train)\n\n# view the results of our cross-validated grid search \ntraining_accuracy_report(best_svm)","199f7a61":"# for accuracy calculation\nfrom sklearn import metrics\n\n# make predictions\ny_pred_svm = best_svm.predict(X_test)\naccuracy = metrics.accuracy_score(y_test, y_pred_svm)\n\n# how often is the classifier correct?\nprint(f'Accuracy on the test set: {accuracy*100:,.2f}%')","e5f61053":"import xgboost as xgb\n\n# parameter grid for our search\nparameters = {\n    'min_child_weight': [5, 6, 7],\n    'gamma': [0, 0.2],\n    'colsample_bytree': [0, 0.2, 0.4],\n    'max_depth': [3, 4, 5]\n}\n\n# initiating our gradient boosted tree\nxgb = xgb.XGBClassifier(objective='reg:squarederror', random_state=RANDOM_SEED)\nclassifier = GridSearchCV(estimator=xgb,\n                          param_grid=parameters,\n                          cv=5)\n\n# fitting the model\nbest_xgb = classifier.fit(X_train, y_train)\n\n# view the results of our cross-validated grid search \ntraining_accuracy_report(best_xgb)","b33049f1":"# make predictions\ny_pred_xgb = best_xgb.predict(X_test)\naccuracy = metrics.accuracy_score(y_test, y_pred_xgb)\n\n# how often is the classifier correct?\nprint(f'Accuracy on the test set: {accuracy*100:,.2f}%')","e4092d7b":"from sklearn.ensemble import RandomForestClassifier\n\n# parameter grid for our search\nparameters = {\n    'n_estimators': [75, 100, 125, ],\n    'max_depth': [10, 50, 100, None],\n    'max_features': [0.5, 1],\n    'min_samples_split': [2, 3, 10],\n    'min_samples_leaf': [2, 3]\n}\n\nrfc = RandomForestClassifier(criterion='gini',\n                             bootstrap=False,\n                             random_state=RANDOM_SEED)\nclassifier = GridSearchCV(estimator=rfc,\n                          param_grid=parameters,\n                          cv=5)\n\n# fitting the model\nbest_rfc = classifier.fit(X_train, y_train)\n\n# view the results of our cross-validated grid search \ntraining_accuracy_report(best_rfc)","5f124aa6":"# make predictions\ny_pred_rfc = best_rfc.predict(X_test)\naccuracy = metrics.accuracy_score(y_test, y_pred_rfc)\n\n# how often is the classifier correct?\nprint(f'Accuracy on the test set: {accuracy*100:,.2f}%')","a0eb0ed0":"from sklearn.ensemble import VotingClassifier\n\n# configuring \nvc = VotingClassifier(\n    \n    # all classifiers we just gridsearched\n    estimators=[\n        ('svm', best_svm),\n        ('xgb', best_xgb),\n        ('rfc', best_rfc)\n    ], \n    \n    # soft = probabilities are taken into account\n    # hard = only the outcome counts\n    voting='soft',\n    \n    # you can add weights, as well. E.g.\n    weights=[2, 3, 2] \n    \n)\n\n# fitting\nvc = vc.fit(X_train, y_train)","1c2d08b2":"# accuracy on test set\ny_pred_vc = vc.predict(X_test)\naccuracy = metrics.accuracy_score(y_test, y_pred_vc)\n\n# how often is the classifier correct?\nprint(f'Accuracy on the test set: {accuracy*100:,.2f}%')","d4c74cf1":"# creating our surivival column by feeding the submission dataset to our voting classifier\npredictions = vc.predict(predict.values)\nsurvived = pd.Series(predictions, name=\"Survived\")\n\n# combining columns into a dataframe for our submission\nsubmission = pd.concat([passenger_id, survived], axis=1)\n\n# submitting\nsubmission.to_csv('submission.csv', index=False)","a420185d":"print(os.listdir())","ae74ee61":"The fare is going to be based on the training set quartiles, so we need to do it outside of the loop.","727c7f48":"# Purpose of the notebook\nIf you're looking for the best performing model for this dataset, you're in the wrong place :) <br>\n\nI wanted to play around with voting classifiers and had yet to submit something to Kaggle. <br>\nFor those interested in setting up cross validation, model ensembles, and voting classifiers, this notebook might be valuable.","c5a2bc4c":"# Classifying\nWe'll be choosing three models that perform well combining them into a voting mechanism","e9dc11d4":"One more pass for the Title.","c55990f1":"The dataset description says the following for the SibSp and ParCh columns: \n\n> **sibsp**: The dataset defines family relations in this way... <br>\n> Sibling = brother, sister, stepbrother, stepsister <br>\n> Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n> \n> **parch**: The dataset defines family relations in this way... <br>\n> Parent = mother, father <br>\n> Child = daughter, son, stepdaughter, stepson <br>\n> Some children travelled only with a nanny, therefore parch=0 for them.\n\nCredit where it is due: Check out [Sina's awesome kernel](https:\/\/www.kaggle.com\/sinakhorami\/titanic-best-working-classifier). She did most of the feature engineering homework for us. I do try to write the code my own way, hoping to contribute some minor improvements.","d2742795":"It looks like this dataset will primarily be an exercise in dealing with **categorical variables**.","2e540677":"# Feature selection","84641f65":"Let's handle the null values in the **Age**, **Cabin**, **Embarked**, and **Fare** columns.","83f0db2d":"## 2. Gradient Boosted Trees","1325a2d7":"# Voting Ensemble","aba8ec78":"# Feature engineering & Cleansing","4ee46acf":"# Submission","1100b4fc":"# Load the datasets","0db42aaa":"# Imputation","493422fb":"## 1. Support Vector Machine","f745e18d":"# Dummifying","c9ddf547":"# Train\/Test Split","7f8c4804":"## 3. Random Forests","5c23ce2b":"The only NaN values remaining are our missing predictions. The rest looks good!"}}