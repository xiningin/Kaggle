{"cell_type":{"626e6d54":"code","274cd45c":"code","9d9ac411":"code","5d863ecb":"code","705b98e7":"code","4b04ea1e":"code","d13168de":"code","67b1b9c1":"code","e1bec125":"code","288df9ac":"code","fbdd9209":"code","c85bc0fe":"code","5e50dd13":"code","150a8963":"code","de921e3a":"markdown","f724d862":"markdown","fe6df4da":"markdown","d246166e":"markdown","a3546b62":"markdown","729d8ceb":"markdown","c6d13bd5":"markdown","248e2317":"markdown","07e22a7e":"markdown","771ef96b":"markdown","df0059ec":"markdown","e12d5c42":"markdown"},"source":{"626e6d54":"import pandas as pd\nimport numpy as np\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.feature_selection import RFE","274cd45c":"df = pd.read_csv('..\/input\/cardiotocographic\/Cardiotocographic.csv')\ndf.head()","9d9ac411":"df.shape","5d863ecb":"df.isnull().sum()","705b98e7":"X = pd.DataFrame(df.iloc[:, 0:21])\ny = df['NSP']","4b04ea1e":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 2)","d13168de":"rf = RandomForestClassifier()\nmodel = rf.fit(X_train, y_train)","67b1b9c1":"y_pred = rf.predict(X_test)","e1bec125":"rf.score(X_test, y_test)","288df9ac":"print(rf.feature_importances_)","fbdd9209":"rfe = RFE(estimator = RandomForestClassifier(), n_features_to_select = 18, step = 10, verbose = 1)\nrfe.fit(X_train, y_train)","c85bc0fe":"y1_pred = rfe.predict(X_test)","5e50dd13":"rfe.score(X_test, y_test)","150a8963":"print(X.columns[rfe.support_])","de921e3a":"##### Conclussion:  It's a quite improvement from the first model.","f724d862":"<br\/>\n\n## Random Forest Classifier with all variables","fe6df4da":"<br\/>\n\n## Without using RFE\nwe can also choose the best variables with 'feature_importances_'. Values close to 1 are the best suited variables for the model. But removing one variable can change the significance of other variables. That's why there's always a risk.","d246166e":"## Splitting into explanatory and response variables","a3546b62":"## Load the dataset","729d8ceb":"## Checking if there are any missing values","c6d13bd5":"<br\/>\n<br\/>\n\n## Note:\nI discussed RFE more in another [notebook](https:\/\/www.kaggle.com\/galibce003\/confused-with-too-many-features-solution-is-here).\n\n<br\/>\n<br\/>\n\n## Feel free to share your thoughts and if you find it helpful, please upvote. Thanks!","248e2317":"## Shape of the dataset","07e22a7e":"<br\/>\n\n## Columns RFE choose for the model","771ef96b":"## Splitting into train and test set","df0059ec":"<br\/>\n\n\n## So we will go for the less risky solution : The Mighty RFE\nstep = 10 - To speed up the RFE process, we can set the 'step' parameter to RFE. And the 10 means on each iterations 10 least important features will be dropped out.","e12d5c42":"## Importing necessary libraries"}}