{"cell_type":{"5af38ae3":"code","b4db2002":"code","003b809e":"code","5460acb8":"code","aa0f44fe":"code","09ff2988":"code","6c99a89e":"code","8d6766ac":"code","3afb8775":"code","bd318174":"code","2378bb32":"code","07b3e89f":"code","48de355e":"code","21013238":"code","3d3b4104":"code","b6e41c93":"code","fd031745":"code","add5c3e1":"code","75612417":"code","85fc6794":"code","00cdec77":"code","5180f08c":"code","ea0a5813":"code","7bdd8ce5":"code","e6b09e5d":"code","1a658743":"code","a2f330db":"code","37d8f235":"code","2b2dc209":"code","6a89775a":"code","2bc8269f":"code","a6ac70b2":"code","63d99771":"code","cd1eab2d":"code","42296b2f":"code","c4e25610":"code","1add6a23":"code","1838bf1c":"code","b4083286":"code","4e0c4305":"code","a451b1f4":"code","4231e6d9":"code","a370a715":"code","a3c8b8f0":"code","4f9c78cb":"code","85ed2d2c":"code","27c90c9a":"code","79442abe":"code","84d0d355":"code","cac7260c":"code","7b075ea4":"code","b39de287":"code","04694145":"code","11dc7c7a":"code","1449379b":"code","c8a8f1f2":"code","1d0e8702":"code","aef05c95":"code","65c239b7":"code","eda5fae6":"code","5729c172":"code","29e3d5ad":"code","15030903":"code","e2265565":"code","a6e2e3d8":"code","7f085a34":"code","8db50185":"code","c6d5d739":"code","d33e1c50":"code","71618d4e":"markdown","cde66fe9":"markdown","3aedee56":"markdown","7643e757":"markdown","10f56219":"markdown","b65d8657":"markdown","92144332":"markdown","dc68eee3":"markdown","a1ed439d":"markdown","d1b8cbed":"markdown","4630e17c":"markdown","05fafdec":"markdown","528bd2be":"markdown","6b2a1be6":"markdown"},"source":{"5af38ae3":"!pip install jovian --upgrade --quiet","b4db2002":"import jovian","003b809e":"# Execute this to save new versions of the notebook\njovian.commit(project=\"breast-cancer-prediction\")","5460acb8":"!pip install opendatasets","aa0f44fe":"import opendatasets as od","09ff2988":"dataset_url = \"https:\/\/www.kaggle.com\/uciml\/breast-cancer-wisconsin-data\"\nod.download(dataset_url)","6c99a89e":"import os","8d6766ac":"data_dir = '.\/breast-cancer-wisconsin-data'","3afb8775":"os.listdir(data_dir)","bd318174":"train_csv = data_dir + '\/data.csv'","2378bb32":"!pip install pandas","07b3e89f":"import pandas as pd\ndataset = pd.read_csv(train_csv)","48de355e":"dataset","21013238":"!pip install plotly","3d3b4104":"import numpy as  np \nimport matplotlib \nimport matplotlib.pyplot as plt\nimport seaborn as sns \nimport plotly.express as px \n\n","b6e41c93":"%matplotlib inline\n\nsns.set_style('darkgrid')\nmatplotlib.rcParams['font.size'] = 14\nmatplotlib.rcParams['figure.figsize'] = (10, 6)\nmatplotlib.rcParams['figure.facecolor'] = '#00000000'","fd031745":"dataset.info()","add5c3e1":"dataset.isna().sum()","75612417":"dataset.describe()","85fc6794":"dataset.drop('Unnamed: 32', axis = 1, inplace = True)","00cdec77":"dataset.head()","5180f08c":"px.histogram(dataset, \n             x = \"radius_mean\", \n             title='radius_mean vs diagnosis', \n             color='diagnosis')","ea0a5813":"dataset[\"diagnosis\"].value_counts()","7bdd8ce5":"px.scatter(dataset, \n           title='radius_mean vs texture_mean',\n           x='radius_mean',\n           y='texture_mean',\n           color='diagnosis')","e6b09e5d":"sns.countplot(dataset[\"diagnosis\"].value_counts())","1a658743":"dataset.head()","a2f330db":"px.scatter(dataset, \n           title='concavity_mean. vs concave points_mean.',\n           x='concavity_mean', \n           y='concave points_mean', \n           color='diagnosis')\n","37d8f235":"px.histogram(dataset, \n             x = \"perimeter_mean\", \n             title='radius_mean vs diagnosis', \n             color='diagnosis')","2b2dc209":"fig = px.histogram(dataset, \n                   x='perimeter_mean', \n                   marginal='box', \n                   nbins=47, \n                   title='Distribution of perimeter_mean')\nfig.update_layout(bargap=0.1)\nfig.show()","6a89775a":"fig = px.histogram(dataset, \n                   x='smoothness_mean', \n                   marginal='box', \n                   color_discrete_sequence=['red'], \n                   title='Distribution of smoothness_mean')\nfig.update_layout(bargap=0.1)\nfig.show()","2bc8269f":"fig = px.histogram(dataset, \n                   x='radius_worst', \n                   marginal='box', \n                   color='diagnosis', \n                   color_discrete_sequence=['green', 'grey'], \n                   title='diagnosis')\nfig.update_layout(bargap=0.1)\nfig.show()","a6ac70b2":"dataset.corr()","63d99771":"plt.figure(figsize=(18,9))\nsns.heatmap(dataset.corr(),annot = True, cmap =\"Accent_r\")","cd1eab2d":"dataset.head()","42296b2f":"input_cols = list(dataset.columns)[2:]\ntarget_col  = \"diagnosis\"","c4e25610":"print(input_cols)\nprint(target_col)","1add6a23":"inputs_df = dataset[input_cols].copy()","1838bf1c":"inputs_df","b4083286":"targets = dataset[target_col]","4e0c4305":"numeric_cols = inputs_df.select_dtypes(include=['int64', 'float64']).columns.tolist()\n","a451b1f4":"categorical_cols =  inputs_df.select_dtypes('object').columns.tolist()","4231e6d9":"print(numeric_cols)","a370a715":"print(categorical_cols)","a3c8b8f0":"inputs_df.describe().loc[['min', 'max']]","4f9c78cb":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\nscaler.fit(inputs_df[numeric_cols])","85ed2d2c":"inputs_df[numeric_cols] = scaler.transform(inputs_df[numeric_cols])","27c90c9a":"inputs_df.describe().loc[['min', 'max']]","79442abe":"from sklearn.model_selection import train_test_split","84d0d355":"train_inputs, val_inputs, train_targets, val_targets = train_test_split(inputs_df[numeric_cols], \n                                                                        targets, \n                                                                        test_size=0.15, \n                                                                        random_state=42)","cac7260c":"train_inputs","7b075ea4":"val_inputs","b39de287":"train_targets","04694145":"val_targets","11dc7c7a":"from sklearn.linear_model import LogisticRegression\nmodel = LogisticRegression(solver = \"liblinear\" , random_state = 42)\nmodel.fit(train_inputs[numeric_cols], train_targets)","1449379b":"print(model.coef_.tolist())","c8a8f1f2":"print(model.intercept_)","1d0e8702":"train_preds = model.predict(train_inputs[numeric_cols])","aef05c95":"train_preds","65c239b7":"train_targets","eda5fae6":"train_probs = model.predict_proba(train_inputs[numeric_cols])\ntrain_probs","5729c172":"from sklearn.metrics import accuracy_score","29e3d5ad":"accuracy_score(train_targets, train_preds)","15030903":"from sklearn.metrics import confusion_matrix","e2265565":"confusion_matrix(train_targets, train_preds, normalize='true')","a6e2e3d8":"def predict_and_plot(inputs, targets, name=''):\n    preds = model.predict(inputs)\n    \n    accuracy = accuracy_score(targets, preds)\n    print(\"Accuracy: {:.2f}%\".format(accuracy * 100))\n    \n    cf = confusion_matrix(targets, preds, normalize='true')\n    plt.figure()\n    sns.heatmap(cf, annot=True)\n    plt.xlabel('Prediction')\n    plt.ylabel('Target')\n    plt.title('{} Confusion Matrix'.format(name));\n    \n    return preds","7f085a34":"train_preds = predict_and_plot(X_train, train_targets, 'Training')","8db50185":"val_preds = predict_and_plot(X_val, val_targets, 'Validation')","c6d5d739":"import joblib","d33e1c50":"cancer_prediction = {\n    'model': model,\n    #'imputer': imputer,\n    'scaler': scaler,\n    #'encoder': encoder,\n    'input_cols': input_cols,\n    'target_col': target_col,\n    'numeric_cols': numeric_cols,\n    'categorical_cols': categorical_cols,\n    #'encoded_cols': encoded_cols\n}","71618d4e":"### Exploratory Data Analysis and Visualization\nBefore training a machine learning model, its always a good idea to explore the distributions of various columns and see how they are related to the target column. ","cde66fe9":"### Identify Numeric and Categorical Data\nThe next step in data preparation is to identify numeric and categorical columns. We can do this by looking at the data type of each column.\n\n","3aedee56":"### Downloading the Data\nWe'll use the opendatasets library to download the data from Kaggle directly within Jupyter. Let's install and import opendatasets.","7643e757":"## Prepare the Dataset for Training\nBefore we can train the model, we need to prepare the dataset. Here are the steps we'll follow:\n\nIdentify the input and target column(s) for training the model.\nIdentify numeric and categorical input columns.\nImpute (fill) missing values in numeric columns\nScale values in numeric columns to a \n(\n0\n,\n1\n)\n range.\nEncode categorical data into one-hot vectors.\nSplit the dataset into training and validation sets.\nIdentify Inputs and Targets\nWhile the dataset contains 31 columns, not all of them are useful for modeling. Note the following:\n\nThe first column Id is a unique ID for each house and isn't useful for training the model.\nThe second column diganosis contains the value we need to predict i.e. it's the target column.\nData from all the other columns (except the first and the last column) can be used as inputs to the model.","10f56219":"#### saving the model to the disk","b65d8657":"### The dataset contains over 569 rows and 33 columns. The dataset contains date, numeric and categorical columns. Our objective is to create a model to predict the value in the column diagnosis.\n\nLet's check the data types and missing values in the various columns.","92144332":"### Scale Numerical Values\nThe numeric columns in our dataset have varying ranges.","dc68eee3":"### We can test the accuracy of the model's predictions by computing the percentage of matching values in train_preds and train_targets.\n\nThis can be done using the accuracy_score function from sklearn.metrics.","a1ed439d":"### unnamed columns contains so many missing data so it is better to drop the column","d1b8cbed":"##### Create a list input_cols of column names containing data that can be used as input to train the model, and identify the target column as the variable target_col.","4630e17c":"# breast-cancer-prediction\n\nUse the \"Run\" button to execute the code.","05fafdec":"Each weight is applied to the value in a specific column of the input. Higher the weight, greater the impact of the column on the prediction.\n\n### Making Predictions and Evaluating the Model\nWe can now use the trained model to make predictions on the training, test","528bd2be":"### Training a Logistic Regression Model\n\nLogistic regression is a commonly used technique for solving binary classification problems. In a logistic regression model:\n\nwe take linear combination (or weighted sum of the input features)\nwe apply the sigmoid function to the result to obtain a number between 0 and 1\nthis number represents the probability of the input being classified as \"Yes\"\ninstead of RMSE, the cross entropy loss function is used to evaluate the results\nHere's a visual summary of how a logistic regression model is structured (source):\n\n\n\nThe sigmoid function applied to the linear combination of inputs has the following formula:\n\n\n\nto train a logistic regression model, we can use the LogisticRegression class from Scikit-learn.","6b2a1be6":"### Training and Validation Set\nFinally, let's split the dataset into a training and validation set. We'll use a randomly select 25% subset of the data for validation. Also, we'll use just the numeric and encoded columns, since the inputs to our model must be numbers.\n\n"}}