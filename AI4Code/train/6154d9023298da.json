{"cell_type":{"c5364a54":"code","6aa93a0b":"code","35eb90dd":"code","b9c30650":"code","b1d177ad":"code","6be9a27f":"code","f8be93e6":"code","8a14386c":"code","e5cd29a9":"code","4d09374f":"code","28f8f4ac":"code","5e061bde":"code","7b67d644":"code","165b4bd6":"code","de84cc9d":"code","9bcc69bc":"code","77946e14":"code","38e6d05c":"code","cbc1717e":"code","f74f419e":"code","5b527d1f":"code","4d0f41f4":"code","44b747dc":"code","4297d83e":"code","e1f350a6":"code","b92fe02d":"code","3d0f534e":"code","ad27f12c":"code","a021ebcc":"code","63aa2524":"code","a5bca8b5":"code","3edce3b8":"code","dd1e42b0":"code","f8d189a6":"code","3b4644ba":"code","08d9123b":"code","27b28ccf":"code","234874c1":"code","10c14364":"code","7d298ab3":"code","c7656154":"code","7fe8855a":"code","7b57e042":"code","73f55b0e":"code","cd2cb500":"code","e8920be0":"code","89733324":"code","295a5ab6":"markdown","bc4e3033":"markdown"},"source":{"c5364a54":"# Import libararies\n# Data load,Data transform\nimport pandas as pd\nimport numpy as np\n# Data science packages\nfrom scipy import stats\nfrom scipy.stats import norm\n# Data Preprocessing\nfrom sklearn.preprocessing import StandardScaler\n# Visualization Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n# Import Models\n# https:\/\/scikit-learn.org\/stable\/modules\/classes.html#module-sklearn.linear_model\nfrom sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\n# https:\/\/scikit-learn.org\/stable\/modules\/ensemble.html\nfrom sklearn.ensemble import GradientBoostingRegressor,RandomForestRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.metrics import mean_squared_error\n\n#Mlxtend (machine learning extensions) is a Python library of useful tools for the day-to-day data science tasks.\n#http:\/\/rasbt.github.io\/mlxtend\/\n#https:\/\/githubja.com\/rasbt\/mlxtend\n#https:\/\/qiita.com\/altescy\/items\/60a6def66f13267f6347\n#pip install mlxtend\nfrom mlxtend.regressor import StackingCVRegressor\n\n#XGBoost(eXtreme Gradient Boosting):mixed Gradient Boosting and Random forest\n#https:\/\/xgboost.readthedocs.io\/en\/latest\/\n#https:\/\/blog.amedama.jp\/entry\/2019\/01\/29\/235642\n#https:\/\/qiita.com\/R1ck29\/items\/4607b4ac941439ed6bbc\n#pip install xgboost\nfrom xgboost import XGBRegressor\n\n##LightGBM: A Highly Efficient Gradient Boosting Decision Tree\n#https:\/\/www.codexa.net\/lightgbm-beginner\/\n#pip install lightgbm\nfrom lightgbm import LGBMRegressor\n\n# utilities\nfrom datetime import datetime\n# Disable warning output\nimport warnings\nwarnings.filterwarnings('ignore')\n# Show matplot graph\n%matplotlib inline","6aa93a0b":"# Read data from train dataset\ndata_train = pd.read_csv('..\/input\/train.csv')\n#check the columns\nprint(data_train.columns)\n# Read data from test dataset\ndata_test = pd.read_csv('..\/input\/test.csv')","35eb90dd":"#Explore for each key element \n#Analyze SalePrice \n#descriptive statistics summary\n#https:\/\/note.nkmk.me\/python-pandas-describe\/\ndata_train['SalePrice'].describe()","b9c30650":"#histogram\n# Don't output <matplotlib.axes._subplots.AxesSubplot at 0x1a1be67390> if exist ; at the end of plot statement\nsns.distplot(data_train['SalePrice']);","b1d177ad":"#skewness and kurtosis\nprint(\"Skewness: %f\" % data_train['SalePrice'].skew())\nprint('Kurtosis: %f' % data_train['SalePrice'].kurt())","6be9a27f":"# Step1 Hypothesis\n# 1, Feature Selection\n# Try to explore these features which possible influent SalePrice based on Hypothesis \n#1)Relationship with numerical variables\n#scatter plot grlivarea\/saleprice\nvar = 'GrLivArea'\n#https:\/\/deepage.net\/features\/pandas-concat.html\ndata = pd.concat([data_train['SalePrice'],data_train[var]],axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","f8be93e6":"#scatter plot totalbsmtsf\/saleprice\nvar = 'TotalBsmtSF'\ndata = pd.concat([data_train['SalePrice'], data_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","8a14386c":"#2)Relationship with categorical features\n#box plot overallqual\/saleprice(the SalePrice range for each OverallQual)\nvar = 'OverallQual'\ndata = pd.concat([data_train['SalePrice'], data_train[var]], axis=1)\n#explain plots : https:\/\/qiita.com\/tsuruokax\/items\/90167693f142ebb55a7d\n#http:\/\/ailaby.com\/matplotlib_fig\/\n#https:\/\/matplotlib.org\/3.1.0\/api\/_as_gen\/matplotlib.pyplot.subplots.html\nf, ax = plt.subplots(figsize=(8,6))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);","e5cd29a9":"# Relationship of YearBuilt with SalePrice\nvar = 'YearBuilt'\ndata = pd.concat([data_train['SalePrice'], data_train[var]], axis=1)\nf, ax = plt.subplots(figsize=(16, 8))\nfig = sns.boxplot(x=var, y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\n# Rotate X axis 90\nplt.xticks(rotation=90);\n# It seems not so strong relationship with SalePrice","4d09374f":"# Step2 Structural engineering\n# Correlation matrix : https:\/\/blog.csdn.net\/zzw000000\/article\/details\/81205027\n# Correlation matrix (heatmap style)\ncorrmat = data_train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\n#https:\/\/seaborn.pydata.org\/generated\/seaborn.heatmap.html\n#https:\/\/pythondatascience.plavox.info\/seaborn\/heatmap\nsns.heatmap(corrmat, vmax=.8, square=True);\n#Very heat area: TotalBsmtSF' and '1stFlrSF' ;'GarageX' variables ;'GrLivArea', 'TotalBsmtSF', and 'OverallQual' ","28f8f4ac":"#SalePrice' correlation matrix (zoomed heatmap style)\n#saleprice correlation matrix\n#10 features most correlated with SalePrice\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\nprint(cols)\nprint(data_train[cols].values)\nprint(data_train[cols].values.T)\n#https:\/\/deepage.net\/features\/numpy-corrcoef.html\n#https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.T.html\ncm = np.corrcoef(data_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()\n# The 10 features have correlation with Saleprice over 0.5\n# OverallQUal,GrLivArea have very strong correlation\n# choose one from each pair for GarageCars similar with GarageArea(Synonym), TotalBsmtSF similar with 1stFlrSF(Synonym)\n# 'TotRmsAbvGrd' and 'GrLivArea' linear? number with rooms should different with living area\n# Therefore Select OverallQUal,GrLivArea,GarageCars,TotalBsmtSF,FullBath,TotRmsAbvGrd,TotRmsAbvGrd as features","5e061bde":"#Scatter plots between 'SalePrice' and correlated variables\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'TotRmsAbvGrd' ,'YearBuilt']\nsns.pairplot(data_train[cols], size = 2.5)\nplt.show();","7b67d644":"#Feature Processing\n#Missing data for train dataset and test dataset\n#For the elments which the NA doesn't mean missing value based on data_description.txt\nelt_name = ['Alley','BsmtQual','BsmtCond','BsmtExposure','BsmtFinType1','BsmtFinType2','FireplaceQu'\n            ,'GarageType','GarageFinish','GarageQual','GarageCond','PoolQC','Fence','MiscFeature']\ndata_train.update(data_train[elt_name].fillna('None'))\ndata_test.update(data_test[elt_name].fillna('None'))","165b4bd6":"#Feature Processing\n#Missing data for train dataset\n#two Important things for missing data:\n#a. How prevalent is the missing data?\n#b.Is missing data random or does it have a pattern?\n#https:\/\/note.nkmk.me\/python-pandas-nan-judge-count\/\n#missing data\n#comment: For most of the elments the NA doesn't mean missing value based on data_description.txt\n#comment: should reconsider deal with these NA elements\ntotal = data_train.isnull().sum().sort_values(ascending=False)\npercent = (data_train.isnull().sum()\/data_train.isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total,percent],axis=1,keys=['Total','Percent'])\nmissing_data.head(20)\n#Seems no missing data for key features:\n#OverallQUal,GrLivArea,GarageCars,TotalBsmtSF,FullBath,TotRmsAbvGrd,TotRmsAbvGrd","de84cc9d":"#missing data for test data\n#comment: For most of the elments the NA doesn't mean missing value based on data_description.txt\n#comment: should reconsider deal with these NA elements\ntotal_test = data_test.isnull().sum().sort_values(ascending=False)\npercent_test = (data_test.isnull().sum()\/data_test.isnull().count()).sort_values(ascending=False)\nmissing_data_test = pd.concat([total_test,percent_test],axis=1,keys=['Total','Percent'])\nmissing_data_test.head(20)","9bcc69bc":"#reomve all the columns with missing data existing except Electrical only reomve this row don't has value\nprint((missing_data[missing_data['Total'] > 1]).index)\n#https:\/\/note.nkmk.me\/python-pandas-drop\/\n#data_train = data_train.drop((missing_data[missing_data['Total'] > 1]).index,1)\n#update na using most common value\ndata_train.update(data_train['Electrical'].fillna(data_train['Electrical'].dropna().mode().values.item()))\ncol_num=[f for f in data_train.columns if data_train.dtypes[f] != 'object']\ndata_train.update(data_train[col_num].fillna(0))\n#fill for Object columns\ncol_obj=[f for f in data_train.columns if data_train.dtypes[f] == 'object']\ndata_train.update(data_train[col_obj].fillna('None'))\ndata_train.isnull().sum().max() #check if missing data exists","77946e14":"#remove same columns from test data set\n#data_test = data_test.drop((missing_data[missing_data['Total'] > 1]).index,1)\n#fill missing data for test data\n#https:\/\/note.nkmk.me\/python-pandas-nan-dropna-fillna\/\n#fill for Number columns\ncol_num=[f for f in data_test.columns if data_test.dtypes[f] != 'object']\ndata_test.update(data_test[col_num].fillna(0))\n#fill for Object columns\ncol_obj=[f for f in data_test.columns if data_test.dtypes[f] == 'object']\ndata_test.update(data_test[col_obj].fillna('None'))\ndata_test.isnull().sum().max() #check if missing data exists","38e6d05c":"#Out liars\n#Outliers is a complex subject and in here just do a quick analysis --\n#through the standard deviation of 'SalePrice' and a set of scatter plots.\n#Univariate analysis\n#standardizing data\nsaleprice_scaled = StandardScaler().fit_transform(data_train['SalePrice'][:,np.newaxis]);\nlow_range = saleprice_scaled[saleprice_scaled[:,0].argsort()][:10]\nhigh_range= saleprice_scaled[saleprice_scaled[:,0].argsort()][-10:]\nprint('outer range (low) of the distribution:')\nprint(low_range)\nprint('\\nouter range (high) of the distribution:')\nprint(high_range)","cbc1717e":"#Bivariate analysis\n#bivariate analysis saleprice\/grlivarea\nvar = 'GrLivArea'\ndata = pd.concat([data_train['SalePrice'], data_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","f74f419e":"#The two values with the most biggest 'GrLivArea' seem strange and not following the crowd. \n#We can speculate why this is happening. Maybe they refer to agricultural area and that could explain the low price. \n#As execise, we'll define them as outliers and delete them\n#deleting points\noutlier_ids = data_train.sort_values(by = 'GrLivArea', ascending = False)[:2]['Id']\nprint(outlier_ids.index)\ndata_train = data_train.drop(outlier_ids.index)\nprint(data_train.sort_values(by = 'GrLivArea', ascending = False)[:2][['SalePrice','GrLivArea']])","5b527d1f":"#bivariate analysis saleprice\/grlivarea\nvar = 'TotalBsmtSF'\ndata = pd.concat([data_train['SalePrice'], data_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));\n#It seems not so strange","4d0f41f4":"#Check Data Quality\n# 4 import factors\n#1,Normality\n#2,Homoscedasticity\n#3,Linearity\n#4,Absence of correlated errors\n\n#Pay attention to the following 2 points for each key features\n#Histogram - Kurtosis and skewness.\n#Normal probability plot - Data distribution should closely follow the diagonal that represents the normal distribution.\n#histogram and normal probability plot\n#SalePrice\nsns.distplot(data_train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(data_train['SalePrice'], plot=plt)\n#From the following graph\n#1)'SalePrice' is not normal. It shows 'peakedness'\n#2) show positive skewness and does not follow the diagonal line.\n#Solution\n#A simple data transformation can solve the problem. \n#can learn in statistical books: in case of positive skewness, log transformations usually works well.","44b747dc":"#log transformations for SalePrice\n#applying log transformation\n#SalePrice onlu exists in train dataset\ndata_train['SalePrice'] = np.log(data_train['SalePrice'])","4297d83e":"#transformed histogram and normal probability plot again\nsns.distplot(data_train['SalePrice'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(data_train['SalePrice'], plot=plt)\n#Now It seems normal and skewness follows the diagonal line.","e1f350a6":"#Feature engineering for both train and test dataset \n#Do the same for GrLivArea\n#Before log transformations\n#histogram and normal probability plot\nsns.distplot(data_train['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(data_train['GrLivArea'], plot=plt)","b92fe02d":"#data transformation\ndata_train['GrLivArea'] = np.log(data_train['GrLivArea'])\ndata_test['GrLivArea'] = np.log(data_test['GrLivArea'])","3d0f534e":"#transformed histogram and normal probability plot\nsns.distplot(data_train['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(data_train['GrLivArea'], plot=plt)","ad27f12c":"# Do the same for TotalBsmtSF\n# Before log transformations\n#histogram and normal probability plot\nsns.distplot(data_train['TotalBsmtSF'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(data_train['TotalBsmtSF'], plot=plt)\n#issue:\n#1)A lots of observations with value zero (houses without basement).\n#2)the value zero doesn't allow to do log transformations.\n#solution:\n#create a new feature that can get the effect of having or not having basement (binary variable). \n#log transformation to all the non-zero observations, ignoring those with value zero. \n#Present basement by combining this two featurs\n#This way we can transform data, without losing the effect of having or not basement.","a021ebcc":"#create column for new feature (one is enough because it's a binary categorical feature)\n#if area>0 it gets 1, for area==0 it gets 0\ndata_train['HasBsmt'] = pd.Series(len(data_train['TotalBsmtSF']), index=data_train.index)\ndata_train['HasBsmt'] = 0 \ndata_train.loc[data_train['TotalBsmtSF']>0,'HasBsmt'] = 1\ndata_test['HasBsmt'] = pd.Series(len(data_test['TotalBsmtSF']), index=data_test.index)\ndata_test['HasBsmt'] = 0 \ndata_test.loc[data_test['TotalBsmtSF']>0,'HasBsmt'] = 1","63aa2524":"#transform data\ndata_train.loc[data_train['HasBsmt']==1,'TotalBsmtSF'] = np.log(data_train['TotalBsmtSF'])\ndata_test.loc[data_test['HasBsmt']==1,'TotalBsmtSF'] = np.log(data_test['TotalBsmtSF'])","a5bca8b5":"#histogram and normal probability plot\nsns.distplot(data_train[data_train['TotalBsmtSF']>0]['TotalBsmtSF'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(data_train[data_train['TotalBsmtSF']>0]['TotalBsmtSF'], plot=plt)","3edce3b8":"#Check homoscedasticity\n#The best approach to test homoscedasticity for two metric variables is graphically. \n#Good graph:\n#small dispersion at one side of the graph, large dispersion at the opposite side) \n#or diamonds (a large number of points at the center of the distribution).\n#for GrLivArea and SalePrice\n#scatter plot\nplt.scatter(data_train['GrLivArea'], data_train['SalePrice']);\n#It seems good after log transformation","dd1e42b0":"#for TotalBsmtSF with value over 0 and SalePrice\n#scatter plot\nplt.scatter(data_train[data_train['TotalBsmtSF']>0]['TotalBsmtSF'], data_train[data_train['TotalBsmtSF']>0]['SalePrice']);","f8d189a6":"#dummy variables\n#convert categorical variable into dummy\n#https:\/\/note.nkmk.me\/python-pandas-get-dummies\/\ndata_all = pd.concat([data_train.drop(['SalePrice'], axis=1), data_test]).reset_index(drop=True)\ndata_all = pd.get_dummies(data_all)\ndata_all.drop(['Id'],axis=1,inplace=True)\n#https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python","3b4644ba":"#Creat train Data set ,label, test dataset\ny = data_train['SalePrice'].reset_index(drop=True)\n#http:\/\/ailaby.com\/lox_iloc_ix\/\n#https:\/\/note.nkmk.me\/python-pandas-at-iat-loc-iloc\/\nx_train = data_all.iloc[:len(y), :]\nx_test = data_all.iloc[len(y):, :]\n#same result with the above\ntrain = data_all[:len(y)]\ntest = data_all[len(y):]\nx_train.shape, x_test.shape, y.shape,train.shape,test.shape","08d9123b":"#Data Model Part\nprint('START ML', datetime.now(), )\nkfolds = KFold(n_splits=5, shuffle=True, random_state=42)\n#rmsle\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\n#build our model scoring function\ndef cv_rmse(model, x=x_train, y=y):\n    rmse = np.sqrt(-cross_val_score(model, x, y, scoring=\"neg_mean_squared_error\", cv=kfolds))\n    return (rmse)","27b28ccf":"#setup models\nalphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\nalphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\ne_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\ne_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]","234874c1":"# Create models\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=alphas_alt, cv=kfolds))\nlasso = make_pipeline(RobustScaler(), LassoCV(max_iter=1e7, alphas=alphas2, random_state=42, cv=kfolds))\nelasticnet = make_pipeline(RobustScaler(), ElasticNetCV(max_iter=1e7, alphas=e_alphas, cv=kfolds, l1_ratio=e_l1ratio))                                \n#svr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003,))","10c14364":"gbr = GradientBoostingRegressor(n_estimators=2000, learning_rate=0.01, max_depth=30, max_features='sqrt', min_samples_leaf=15, min_samples_split=10, loss='huber', random_state =42)","7d298ab3":"random_forest = RandomForestRegressor(n_estimators=2000,\n                                      max_depth=30,\n                                      min_samples_split=5,\n                                      min_samples_leaf=5,\n                                      max_features=None,\n                                      random_state=10,\n                                      oob_score=True\n                                     )","c7656154":"# lightgbm = LGBMRegressor(objective='regression', \n#                                        num_leaves=4,\n#                                        learning_rate=0.01, \n#                                        n_estimators=5000,\n#                                        max_bin=200, \n#                                        bagging_fraction=0.75,\n#                                        bagging_freq=5, \n#                                        bagging_seed=7,\n#                                        feature_fraction=0.2,\n#                                        feature_fraction_seed=7,\n#                                        verbose=-1,\n#                                        )","7fe8855a":"xgboost = XGBRegressor(learning_rate=0.01,n_estimators=3460,\n                                     max_depth=3, min_child_weight=0,\n                                     gamma=0, subsample=0.7,\n                                     colsample_bytree=0.7,\n                                     objective='reg:linear', nthread=-1,\n                                     scale_pos_weight=1, seed=27,\n                                     reg_alpha=0.00006)","7b57e042":"#stack\n# stack_gen = StackingCVRegressor(regressors=(ridge, lasso, elasticnet, gbr, random_forest),\n#                                 meta_regressor=xgboost,\n#                                 use_features_in_secondary=True)\nstack_gen = StackingCVRegressor(regressors=(ridge, lasso, random_forest),\n                                meta_regressor=gbr,\n                                use_features_in_secondary=True)","73f55b0e":"print('TEST score on CV')\nnames = [\"Kernel Ridge\", \"Lasso\", \"ElasticNet\", \"xgboost\", \"gbr\", \"stack_gen\"]\nmodels= [ridge, lasso, elasticnet, xgboost, gbr, stack_gen]\n# names = [\"Kernel Ridge\", \"Lasso\", \"ElasticNet\", \"GradientBoosting\", \"xgb\" ,\"lightgbm\" ,\"stack_gen\"]\n# models= [ridge, lasso, elasticnet, gbr, xgboost, lightgbm, stack_gen]\nfor name, model in zip(names, models):\n    score = cv_rmse(model)\n    print(\"{} score: {:.6f} ({:.4f})\".format(name,score.mean(),score.std()))","cd2cb500":"# Predict by mixing models with respective weight\nclass MixModelWeight():\n    def __init__(self,mod,weight):\n        self.mod = mod\n        self.weight = weight\n    \n    def fit(self,X,y):\n        for model in self.mod:\n            model.fit(X,y)\n        return self\n\n    def predict(self,X):\n        w = list()\n        pred = np.array([model.predict(X) for model in self.mod])\n        # for every data point, single model prediction times weight, then add them together\n        for data in range(pred.shape[1]):\n            single = [pred[model,data]*weight for model,weight in zip(range(pred.shape[0]),self.weight)]\n            w.append(np.sum(single))\n        return w\n\n# Mix model\nmix_model = MixModelWeight(mod = [elasticnet,lasso,ridge,xgboost,stack_gen],weight=[0.1,0.1,0.1,0.1,0.6])\n# Fit\nprint('START fit model')\nmix_model.fit(x_train,y)\nprint('RMSLE score on train data:')\nscore = rmsle(y, mix_model.predict(x_train))\nprint(score)","e8920be0":"print('Predict submission', datetime.now(),)\nsubmission=pd.DataFrame({'Id':data_test['Id'], 'SalePrice':np.floor(np.exp(mix_model.predict(x_test)))})\nsubmission_stack=pd.DataFrame({'Id':data_test['Id'], 'SalePrice':np.floor(np.exp(stack_gen.predict(x_test)))})\nsubmission_mix=pd.DataFrame({'Id':data_test['Id'], 'SalePrice':(submission['SalePrice'])*0.8+(submission_stack['SalePrice']*0.2)})\nprint(submission)","89733324":"#output to submission csv\nsubmission.to_csv(\"submission_v4_1.csv\", index=False)\nsubmission_stack.to_csv(\"submission_v4_2.csv\", index=False)\nsubmission_mix.to_csv(\"submission_v4_3.csv\", index=False)\nprint('Save submission', datetime.now(),)","295a5ab6":"Reference:\n\nhttps:\/\/www.jianshu.com\/p\/62716b33e7be\nhttps:\/\/www.cnblogs.com\/massquantity\/p\/8640991.html\nhttps:\/\/scikit-learn.org\/stable\/modules\/preprocessing.html\nhttps:\/\/www.cnblogs.com\/limitlessun\/p\/8489749.html\nhttps:\/\/www.cnblogs.com\/zhizhan\/p\/5826089.html\nhttp:\/\/www.360doc.com\/content\/18\/0106\/16\/44422250_719580875.shtml\nhttp:\/\/tekenuko.hatenablog.com\/entry\/2016\/09\/20\/222453","bc4e3033":"**First kaggle challenge for house prices prediction\n\n**Overview:\n* This is my mfirst kernel for kaggle challenge.\n* Learned a lots from Tutorials of this competition.\n* Got basic data exploration and feature engineering ideas, visualization techques beside the concepts and theroies.\n* Checked many blogs to understand each knowledge point and made comment in my codes with url.\n* Many thanks for everyone who shareing experience and ideas in public.\n\n**Breif process:\n1. Import libraries\n2. Data exploration\n> *     Explore for variable SalePrice\n> *     Check correlation variables with SalePrice based on Hypothesis\n> *     Check correlation variables by heatmap\n3. Feature engineering\n> *     Check missing data and deal with these variables incluing missing data\n> *     Check data quality(Normality,Homoscedasticity,Linearity etc.) and do log transformation\n> *     Convert categorical variable into dummy\/indicator variables\n4. Create Train dataset, label, test dataset\n5. Create Models\n6. Evaluate Models\n7. Create blend model which mix each models by weight\n8. Create submission CSV\n\nImprove points:\n1. For most of the elments the NA doesn't mean missing value based on data_description.txt.To be better don't remove these elements from dataset\n2. Improve hyper parameters for models\n3. Many others\n"}}