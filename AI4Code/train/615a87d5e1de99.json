{"cell_type":{"7a638827":"code","d319712e":"code","8cb42f5e":"code","915a7e58":"code","59afed92":"code","283c19a3":"code","018ed05e":"code","07d15454":"code","939f9384":"code","8bb710c3":"code","c602812d":"code","1dd7a2b0":"code","aff424d0":"code","c5221d86":"code","6501ec4b":"code","24c6c085":"code","30b89c4b":"code","26bfc94b":"code","2a6cdd14":"code","7ea4dab3":"code","979b15e7":"code","186a8aed":"code","a798a02e":"code","74bb199a":"code","718ac94e":"code","0e4d7cd3":"code","87c57eac":"code","8c6a9654":"code","a0d30355":"code","936edc75":"code","f5c9f597":"code","6c64adf4":"code","1de0acda":"markdown","64dc41c5":"markdown","99a3027f":"markdown","47a14c74":"markdown","cc82a08b":"markdown","b83ae186":"markdown","5e1a64ec":"markdown","08f1d336":"markdown","9bca647b":"markdown","003323bf":"markdown","4c999688":"markdown","1deee714":"markdown","25cf122f":"markdown","6a72fc78":"markdown","b7275367":"markdown","11c81258":"markdown","58f94e5a":"markdown","84c092fe":"markdown","ed625cba":"markdown","c711c313":"markdown"},"source":{"7a638827":"# Install GPyOpt\n!pip install GPyOpt","d319712e":"import pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np\nimport warnings\n\nwarnings.filterwarnings('ignore')\npd.reset_option('^display.', silent=True)\n\nX_train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\nX_test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")\n\nnum_train = len(X_train)\n\ny_train = X_train.SalePrice\nX_train.drop(['SalePrice'], axis=1, inplace=True)\n\nprint(\"Total training samples:\", len(X_train), \"\\n\")\n\n# Merge train and test to simplify pre-processing\ndf = pd.concat([X_train, X_test], ignore_index=True)\n\nX_train.head()","8cb42f5e":"# Show descriptive statistics of training set\nX_train.describe()","915a7e58":"# Show how many values are non-null for each feature\nX_train.info()","59afed92":"# Print a random house as a sample\nsample_index = 25\nprint(X_train.iloc[sample_index])","283c19a3":"# Show the column types we are dealing with\ndf.dtypes.value_counts()\ncat_columns = df.select_dtypes('object').columns\nnum_columns = [i for i in list(df.columns) if i not in list(df.select_dtypes('object').columns)]\nprint(len(df.columns)-len(df.select_dtypes('object').columns),'numerical columns:')\nprint(num_columns, '\\n')\nprint(len(df.select_dtypes('object').columns),'categorical columns:')\nprint(list(cat_columns))","018ed05e":"# Plot numerical feature variables\nfig = plt.figure(figsize=(17,22))\n\nnum_data = df[num_columns]\nnum_data = num_data.drop(['Id'], axis=1)\n\nfor i in range(len(num_data.columns)):\n    fig.add_subplot(9,4,i+1)\n    sns.distplot(num_data.iloc[:,i].dropna(), hist=False, kde_kws={'bw':0.1}, color='mediumslateblue')\n    plt.xlabel(num_data.columns[i])\nplt.tight_layout()\nplt.show()","07d15454":"# Plot categorial feature variables\ncat_data = df[cat_columns]\ncat_data_cols = cat_data.columns\ncat_data_cols_length = (len(cat_data_cols)\/5)+1\n\nfg, ax = plt.subplots(figsize=(25, 35))\nfg.subplots_adjust(hspace=0.5)\nfor i, col in enumerate(cat_data):\n    fg.add_subplot(cat_data_cols_length, 5, i+1)\n    sns.countplot(cat_data[col], palette='rocket')\n    plt.xlabel(col)\n    plt.xticks(rotation=90)\n\nplt.show()","939f9384":"# Show what the SalePrice variable looks like\ndf_train = pd.concat([df[:num_train], y_train], axis=1)\n\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\nsns.distplot(df_train.SalePrice, color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"SalePrice\")\nax.set(title=\"SalePrice distribution\")\nsns.despine(trim=True, left=True)\nplt.show()","8bb710c3":"# Show how SalePrice and OverallQual are related\ndata_seg1 = pd.concat([df_train['SalePrice'], df_train['OverallQual']], axis=1)\n\nplt.subplots(figsize=(9,5))\nplt.figure(1); plt.title(\"SalePrice vs Overall Quality\")\nsns.boxplot(x='OverallQual', y='SalePrice', data=data_seg1, color=\"mediumslateblue\")\n\nplt.subplots(figsize=(9,5))\nplt.figure(2); plt.title(\"SalePrice vs Overall Quality\")\nsns.lineplot(x='OverallQual', y='SalePrice', data=data_seg1, color=\"mediumslateblue\")","c602812d":"# Rename odd-named columns\ndf = df.rename(columns={\"1stFlrSF\": \"FirstFlrSF\",\n                        \"2ndFlrSF\": \"SecondFlrSF\",\n                       \"3SsnPorch\": \"ThirdSsnPorch\"})\n\n# Remove whitespace in MSZoning\ndf.MSZoning[~df.MSZoning.isnull()] = df.MSZoning[~df.MSZoning.isnull()].map(lambda x: x[:2])\n\n# Remove dots from BldgType\ndf.BldgType[~df.BldgType.isnull()] = df.BldgType[~df.BldgType.isnull()].map(lambda x: x.replace('.', ''))\n\n# Remove ampersand from RoofStyle\ndf.RoofStyle[~df.RoofStyle.isnull()] = df.RoofStyle[~df.RoofStyle.isnull()].map(lambda x: x.replace('&', ''))\n\n# Remove whitespace in Exterior1st, Exterior2nd\ndf.Exterior1st[~df.Exterior1st.isnull()] = df.Exterior1st[~df.Exterior1st.isnull()].map(lambda x: x.replace(' ', ''))\ndf.Exterior2nd[~df.Exterior2nd.isnull()] = df.Exterior2nd[~df.Exterior2nd.isnull()].map(lambda x: x.replace(' ', ''))","1dd7a2b0":"# Visualize missing values\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nsns.set_color_codes(palette='deep')\nmissing = round(df.isnull().mean()*100,2)\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar(color=\"b\")\n\n# Tweak the visual presentation\nax.xaxis.grid(False)\nax.set(ylabel=\"Percent of missing values\")\nax.set(xlabel=\"Features\")\nax.set(title=\"Percent missing data by feature\")\nsns.despine(trim=True, left=True)","aff424d0":"# Find columns with more than 1000 NaN's and drop them (see above)\ncolumns = [col for col in df.columns if df[col].isnull().sum() > 1000]\ndf = df.drop(columns, axis=1)\n\n# Manually encode LotFrontage using median of neighborhood\ndf.LotFrontage = df.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n# No garage values means no year, area or cars\nfor col in ['GarageYrBlt', 'GarageArea', 'GarageCars']:\n    df[col] = df[col].fillna(0)\n    \n# No garage info means you don't have one\nfor col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    df[col] = df[col].fillna('None')\n\n# Fill no basement\nfor col in ['BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2']:\n    df[col] = df[col].fillna('None')\n\n# Fill remaining cat and num cols with None and 0\ncat_columns = df.select_dtypes('object').columns\nnum_columns = [i for i in list(df.columns) if i not in cat_columns]\ndf.update(df[cat_columns].fillna('None'))\ndf.update(df[num_columns].fillna(0))\n\n# Make year variables relative to year sold\nyear_cols = ['YearBuilt','YearRemodAdd','GarageYrBlt']\nfor col in year_cols:\n    df[col] = df['YrSold'] - df[col]","c5221d86":"# Check for missing values \nprint(df.isnull().values.any())","6501ec4b":"# Plot a heatmap of the all the features and their correlation to SalePrice\n# Thanks to https:\/\/www.kaggle.com\/shubhamksingh\/top-3-stacking-blending-in-depth-eda\n\ndf_train = pd.concat([df[:num_train], y_train], axis=1)\ncorrmat = df_train.corr()\nplt.subplots(figsize=(17,17))\nplt.title(\"Correlation Matrix\")\nsns.heatmap(corrmat, vmax=0.9, square=True, cmap=\"Oranges\", annot=True, fmt='.1f', linewidth='.1')","24c6c085":"# Plot the most correlated variables as a matrix\nimp_ftr = corrmat['SalePrice'].sort_values(ascending=False).head(11).to_frame()\nplt.subplots(figsize=(5,8))\nplt.title('SalePrice Correlation Matrix')\nsns.heatmap(imp_ftr, vmax=0.9, annot=True, fmt='.2f', cmap=\"Oranges\", linewidth='.1')","30b89c4b":"plt.subplots(figsize=(15, 15))\nsns.heatmap(corrmat>0.8, annot=True, square=True, cmap=\"Oranges\", linewidth='.1')","26bfc94b":"# Drop correlated columns and plot the result\n\ndf = df.drop(['FirstFlrSF', 'GarageCars', 'TotRmsAbvGrd'], axis=1)\n\ndf_train = pd.concat([df[:num_train], y_train], axis=1)\ncorrmat = df_train.corr()\nplt.subplots(figsize=(15, 15))\nsns.heatmap(corrmat>0.8, annot=True, square=True, cmap=\"Oranges\", linewidth='.1')","2a6cdd14":"# Find outliers using IsolationForest\nfrom sklearn.ensemble import IsolationForest\n\nnum_outliers = 20\nanomaly_dict = {}\nnum_columns = [i for i in list(df.columns) if i not in list(df.select_dtypes('object').columns)]\nfor feature in num_columns:\n    model = IsolationForest(n_estimators=50, max_samples='auto', contamination=float(0.1), max_features=1.0)\n    model.fit(df[[feature]])\n    df['score'] = model.decision_function(df[[feature]])\n    df['anomaly'] = model.predict(df[[feature]])\n    anomaly = df.loc[df['anomaly']==-1]\n    feature_anomaly = anomaly[[feature] + ['score']][:num_outliers]\n    feature_anomaly['min'] = df[feature].min()\n    feature_anomaly['max'] = df[feature].max()\n    feature_anomaly['mean'] = df[feature].mean()\n    anomaly_dict[f'{feature}'] = feature_anomaly","7ea4dab3":"print(anomaly_dict['GarageArea'].sort_values(by='score'), '\\n')","979b15e7":"# Clip data to fix outliers\ndf.MSSubClass = df.MSSubClass.clip(20, 180)\ndf.LotFrontage = df.LotFrontage.clip(20, 200)\ndf.LotArea = df.LotArea.clip(1300,50000)\ndf.OverallQual = df.OverallQual.clip(2,10)\ndf.OverallCond = df.OverallCond.clip(2,9)\ndf.YearBuilt = df.YearBuilt.clip(1880,2010)\ndf.MasVnrArea = df.MasVnrArea.clip(0,1000)\ndf.BsmtFinSF1 = df.BsmtFinSF1.clip(0,2300)\ndf.BsmtFinSF2 = df.BsmtFinSF2.clip(0,1100)\ndf.BsmtUnfSF = df.BsmtUnfSF.clip(0,2000)\ndf.TotalBsmtSF = df.TotalBsmtSF.clip(0,3000)\ndf.SecondFlrSF = df.SecondFlrSF.clip(0,1800)\ndf.LowQualFinSF = df.LowQualFinSF.clip(0,700)\ndf.GrLivArea = df.GrLivArea.clip(334,4000)\ndf.BsmtFullBath = df.BsmtFullBath.clip(0,2)\ndf.BsmtHalfBath = df.BsmtHalfBath.clip(0,2)\ndf.FullBath = df.FullBath.clip(0,3)\ndf.HalfBath = df.HalfBath.clip(0,2)\ndf.BedroomAbvGr = df.BedroomAbvGr.clip(0,6)\ndf.KitchenAbvGr = df.KitchenAbvGr.clip(0,2)\ndf.Fireplaces = df.Fireplaces.clip(0,3)\ndf.GarageYrBlt = df.GarageYrBlt.clip(1900,2207)\ndf.GarageArea = df.GarageArea.clip(0,1400)\ndf.WoodDeckSF = df.WoodDeckSF.clip(0,700)\ndf.OpenPorchSF = df.OpenPorchSF.clip(0,550)\ndf.EnclosedPorch = df.EnclosedPorch.clip(0,560)\ndf.ThirdSsnPorch = df.ThirdSsnPorch.clip(0,300)\ndf.ScreenPorch = df.ScreenPorch.clip(0,400)\ndf.PoolArea = df.PoolArea.clip(0,400)\ndf.MiscVal = df.MiscVal.clip(0,3000)\n","186a8aed":"# SalePrice is skewed, so apply the log function to its value to correct it\n\nplt.figure(figsize=[11,4])\nplt.subplot(1,2,1)\nplt.title('Sale Price')\nplt.hist(y_train,bins=40)\n\nplt.subplot(1,2,2)\nplt.title('Log of Sale Price')\nlog = y_train.apply(lambda x: np.log(x))\nplt.hist(log,bins=40)\ny_train = log","a798a02e":"# Visualize numerical columns to identified skewed variables\ndf_train = pd.concat([df[:num_train], y_train], axis=1)\n\ncolumns = list(df_train.select_dtypes(exclude='object').columns)\nplt.figure(figsize=[16,30])\nfor i in range(len(columns)):\n    try:\n        ax = plt.subplot(10,4,i+1)\n        plt.scatter(df_train[columns[i]],df_train.SalePrice,alpha=0.15)\n        plt.title(columns[i])\n        box = ax.get_position()\n        box.y1 = box.y1 - 0.01 \n        ax.set_position(box)\n    except:\n        pass\nplt.show()","74bb199a":"# Transform numerical variables to be more symmetrical based on above plots\ndf_train = pd.concat([df[:num_train], y_train], axis=1)\nplt.figure(figsize=[11,4])\nplt.subplot(1,2,1)\nplt.title('Before')\nplt.scatter(df_train.LotArea, df_train.SalePrice,alpha=0.25)\nplt.xlabel('LotArea')\n\ncolumns_to_log = ['LotFrontage','LotArea','BsmtFinSF1','BsmtFinSF2','MasVnrArea', \n               'BsmtUnfSF','TotalBsmtSF','GrLivArea','WoodDeckSF','OpenPorchSF']\n\nfor col in columns_to_log:\n    df[col] = df[col].apply(lambda x: np.log(x) if x !=0 else x)\n    \n# Show transformation example with LotArea\ndf_train = pd.concat([df[:num_train], y_train], axis=1)\nplt.figure(figsize=[16,4])\nplt.subplot(1,2,2)\nplt.title('After')\nplt.scatter(df_train.LotArea, df_train.SalePrice,alpha=0.25)\nplt.xlabel('LotArea')","718ac94e":"df.head()","0e4d7cd3":"# Apply one-hot encoding on categorial variables\nfrom sklearn.preprocessing import OneHotEncoder\n\ndef encode_df(df, object_cols):\n    ohe = OneHotEncoder(handle_unknown='ignore', sparse=False)\n    df_enc = pd.DataFrame(ohe.fit_transform(df[object_cols]))\n    df_enc.columns = ohe.get_feature_names(object_cols)\n    df_enc.index = df.index\n    return df_enc\n\n# Use OH encoder to encode cat cols\ndf_enc = encode_df(df, cat_columns)\nnum_df = df.drop(cat_columns, axis=1)\ndf = pd.concat([num_df, df_enc], axis=1)\n\n# Split train and test set\nX_train = df.iloc[:num_train,:]\nX_test = df.iloc[num_train:,:]\n\n# Apply RobustScaler scaling\nfrom sklearn.preprocessing import RobustScaler\nrs = RobustScaler()\nX_train = rs.fit_transform(X_train)\nX_test = rs.transform(X_test)","87c57eac":"# Train an XGBRegressor using BayesianOptimization to find best params\n# Credit: http:\/\/krasserm.github.io\/2018\/03\/21\/bayesian-optimization\/\nimport GPy\nimport GPyOpt\nfrom xgboost import XGBRegressor\nfrom GPyOpt.methods import BayesianOptimization\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nxgb = XGBRegressor()\nbaseline = cross_val_score(xgb, X_train, y_train, scoring='neg_mean_squared_error').mean()\n\nsearch_space = [{'name': 'learning_rate', 'type': 'continuous', 'domain': (0, 1)},\n                {'name': 'gamma', 'type': 'continuous', 'domain': (0, 5)},\n                {'name': 'max_depth', 'type': 'discrete', 'domain': (1, 2, 3, 4, 5, 10)},\n                {'name': 'n_estimators', 'type': 'discrete', 'domain': (5000, 6000, 7000)},\n                {'name': 'min_child_weight', 'type': 'discrete', 'domain': (0, 10)}]\n\ndef cv_score(parameters):\n    parameters = parameters[0]\n    score = cross_val_score(\n                XGBRegressor(learning_rate=parameters[0],\n                              gamma=int(parameters[1]),\n                              max_depth=int(parameters[2]),\n                              n_estimators=int(parameters[3]),\n                              min_child_weight = parameters[4]), \n                X_train, y_train, scoring='neg_mean_squared_error').mean()\n    score = np.array(score)\n    return score\n\noptimizer = BayesianOptimization(f=cv_score, \n                                 domain=search_space,\n                                 model_type='GP',\n                                 acquisition_type ='EI',\n                                 acquisition_jitter = 0.05,\n                                 exact_feval=True, \n                                 maximize=True,\n                                 verbosity=True,\n                                 verbosity_model=True)\n\noptimizer.run_optimization(max_iter=3, verbosity=True)","8c6a9654":"# Plot the convergence as a function of iterations\noptimizer.plot_convergence()","a0d30355":"# Plot the accumulated score of the optimizer for Y compared to baseline\ny_bo = np.maximum.accumulate(-optimizer.Y).ravel()\n\nprint(f'Baseline neg. MSE = {baseline:.3f}')\nprint(f'Bayesian optimization neg. MSE = {y_bo[-1]:.3f}')\n\nplt.plot(baseline, 'ro-', label='Baseline')\nplt.plot(y_bo, 'bo-', label='Bayesian optimization')\nplt.xlabel('Iteration')\nplt.ylabel('Neg. MSE')\nplt.title('Value of the best sampled CV score')\nplt.legend()","936edc75":"# Put our results in a dataframe and show them\nheader_params = []\nfor param in search_space:\n    header_params.append(param['name'])\n\ndf_results = pd.DataFrame(data=optimizer.X, columns=header_params)\ndf_results['error'] = optimizer.Y\ndf_results = df_results.sort_values(by=['error'])\ndf_results","f5c9f597":"# Train a model with val set using best found parameters\nfrom sklearn.model_selection import train_test_split\nX_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.3, random_state=0)\nxgb = XGBRegressor(n_estimators=6000,\n                   min_child_weight=10,\n                   max_depth=1,\n                   gamma=0.06,\n                   learning_rate=0.01,\n                   random_state=0)\nxgb.fit(X_train, y_train, early_stopping_rounds=10, eval_metric=\"rmse\",\n       eval_set=[(X_train, y_train), (X_val, y_val)], verbose=False)\npredictions = xgb.predict(X_test)","6c64adf4":"# Load sample submission\nsubmission = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\n\n# Append XGBoost predictions\nsubmission.iloc[:,1] = np.floor(np.expm1(predictions))\n\n# Set quantile to leave out eventual outliers\nq1 = submission['SalePrice'].quantile(0.0045)\nq2 = submission['SalePrice'].quantile(0.99)\n\n# Calibrate for eventual outliers by quantile\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x > q1 else x*0.77)\nsubmission['SalePrice'] = submission['SalePrice'].apply(lambda x: x if x < q2 else x*1.1)\n\nsubmission.to_csv(\"submission_regression.csv\", index=False)\n","1de0acda":"From the heatmap above, we can clearly see the highly-correlated features:\n\n* FirstFlrSF vs TotalBsmtSF\n* GarageCars vs GarageArea\n* GrLivArea vs TotRmsAbvGrd\n\nWe'll have to drop one of them, so we'll keep the one most correlated with SalePrice. Afterwards we'll plot the result.","64dc41c5":"# Modelling with XGBoost\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/69\/XGBoost_logo.png)","99a3027f":"This is pretty straight-forward, as we onehot-encode the input variables, before splitting up training and test set and scaling each individually.","47a14c74":"We do a few things here to ease the data handling and modelling phase:\n\n* Rename odd-named columns and columns with whitespace and numbers to ease processing.\n* Drop features with more than 1000 NaN's\n* Manually handle the missing values, setting most of them to None","cc82a08b":"![](https:\/\/terrarealestate.com\/optimize-image\/slider\/1061\/1061-3-bedroom-detached-house-for-affordable-price-in-alanya-kargicak-5cee6830b46d6.jpg)","b83ae186":"By looking at the correlation plot, we note that the following features are highly correlated to SalePrice. If you would want to run feature selection, these would be a good place to start:\n\n* OverallQual\n* TotalBsmtSF\n* FirstFlrSF\n* GrLivArea\n* FullBath\n* GarageCars\n* GarageArea\n\n\n\n","5e1a64ec":"As an example of the IsolationForest, we print the results for the feature GarageArea and show the rows with the highest outlier score. The table below shows the feature name, it's score and the min\/max\/mean values for this feature. Note how far from the mean the top outlier is (GarageArea=1166).","08f1d336":"Note that BayesianOptimization does not always seem to beat the baseline XGBoost. This could indicate, that it oftentimes get stuck in local minimas and thus does not find the global minima. A bit more experimentation and fine-tuning is needed for this to work properly every time, and feedback is much welcome.","9bca647b":"Next we'll map the correlation of independent variables, so called collinearity. The problem is that it can inflate the variance of at least the regression coefficient. It might not be a big deal for us as it doesn't affect the total predictive power of our model, but it can affect calculations regarding individual predictors. We should get rid of these \u201credundant\u201d variables by using a variable selection technique or simply dropping them.","003323bf":"# Outlier analysis\n\nIsolationForest is a method that can find the anomaly score of each sample using the IsolationForest algorithm. It The isolates observations by randomly selecting a feature and then randomly selecting a split value between the maximum and minimum values of the selected feature. Below we compute an anomaly table with IsolationForst and afterwards clip the value ranges of features (e.g. 0-5000 for GarageArea), so that outlier impact is reduced.","4c999688":"# Skewed variables","1deee714":"# Correlation study\n\nWe do a correlation study to find variables that correlate well with our target (SalePrice) and which do not. The first plot is a heatmap showing inter feature coefficient correlations as scalar values betwen -1 and 1. You'd also note that the darker the colors, the more correlated the feature.\n\nTypically, a correlation matrix is \u201csquare\u201d, with the same variables shown in the rows and columns. The line of 1.00s going from the top left to the bottom right is the main diagonal, which shows that each variable always perfectly correlates with itself. This matrix is symmetrical, with the same correlation is shown above the main diagonal being a mirror image of those below the main diagonal.\n\nAfterwards we'll look at multicollinearity and eventually remove variables that correlate with each-other.","25cf122f":"# Introduction\n\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nThe original dataset comprises 2930 observations, for which 79 explanatory variables (the independent variables) and sale prices (the dependent variable) are known and supplied. In the competition, Kaggle furnished optimized train and test subsets of the original Ames Housing dataset with 1460 and 1459 observations, respectively. Sale prices are disclosed for the training set and must be predicted for the test set. Submissions are ranked based on the Mean Absolute Error (MAE) derived from predicted and actual sale prices for the 1459 observations in the test set.\n\nIn this notebook we analyse the house prices, find the best hyperparameterers and trains an XGBoost regressor to predict future prices.\n\nKey takeaways:\n\n* **IsolationForest** can help identifiy biggest outliers in our dataset.\n* **BaysianOptimization** is a  global optimization strategy that works well on this dataset to find best model parameters.\n* **XGBRegressor** using the parameters we find and an early-stopping-strategy produces good results.\n\nFirst we install the GPyOpt library which we'll need for BayesianOptimization.","6a72fc78":"# Submission","b7275367":"SalePrice is skewed and deviates from a typical Gaussian distribution. Skewness is a measure of the asymmetry of the probability distribution of a real-valued random variable about its mean. We'll do a log transform on 'SalePrice' and afterwards plot the independent variables to find skewness elsewhere.","11c81258":"# Feature encoding","58f94e5a":"Boosting is what the kids want these days. XGBoost is a decision-tree-based ensemble ML algorithm, that uses an underlying gradient boosting framework to train tree's and make predictions. When it comes to small-to-medium structured\/tabular data, decision-tree-based algorithms are considered best-in-class right now.\n\nWe configure a XGBoost regressor baseline and a more elaborate tuning strategy using BayesianOptimization and CV. BayesianOptimization works better than for example GridSearchCV when we have a problem in higher dimensions, since with BayesianOptimization we don't try all possible combinations, we just search along the space of hyperparameters and learn as we try them. This approach avoids trying all combinations, as we steer away from the bad ones as we find optima in the search space.\n\nhttp:\/\/krasserm.github.io\/2018\/03\/21\/bayesian-optimization\/ <br>\nhttps:\/\/gpyopt.readthedocs.io\/en\/latest\/GPyOpt.core.html <br>\nhttps:\/\/www.analyticsvidhya.com\/blog\/2016\/03\/complete-guide-parameter-tuning-xgboost-with-codes-python\/","84c092fe":"Next comes loading the data, separating X and y variables and running on easy EDA.","ed625cba":"Our predictions are now ready for submission. The code below reads the sample submission file as we'll use the same ids, then floors the element-wise result of the exponential function applied to the individual predicts. This is a measure to reduce the loss of significance of our raw predictions (float numbers) when floored. Thanks to https:\/\/www.kaggle.com\/lavanyashukla01\/how-i-made-top-0-3-on-a-kaggle-competition","c711c313":"# Feature engineering"}}