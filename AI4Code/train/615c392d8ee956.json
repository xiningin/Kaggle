{"cell_type":{"ae4fd5fc":"code","c122b489":"code","9b17a8d4":"code","d0c7977c":"code","09d60787":"code","92d913a9":"code","0f5962c4":"code","a6ce7e29":"code","aadc8f90":"code","d7cb162d":"code","d4a289be":"code","532a7326":"code","0aa3092b":"code","86aaebbd":"code","a9873f8b":"code","0a09993a":"code","ef62977d":"code","43946c49":"code","28b7c40f":"code","dead9c1f":"code","18be2507":"code","d02db886":"code","40165580":"code","e761e0ea":"markdown","47312d72":"markdown","c1b31c47":"markdown","1db3b04c":"markdown","160ea2f8":"markdown","047f75d3":"markdown","7efdf62c":"markdown"},"source":{"ae4fd5fc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","c122b489":"data =pd.read_csv(\"..\/input\/all_stocks_5yr.csv\")","9b17a8d4":"data.head()","d0c7977c":"data.tail()","09d60787":"#I just picked the first stock\ndata = data[data.Name == 'AAL']","92d913a9":"data.tail()","0f5962c4":"from scipy.stats import norm\nlog_returns = np.log(1 + data.close.pct_change())\nu = log_returns.mean() #Mean of the logarithmich return\nvar = log_returns.var() #Variance of the logarithic return\ndrift = u - (0.5 * var) #drift \/ trend of the logarithmic return\nstdev = log_returns.std() #Standard deviation of the log return\n\n\nt_intervals = 250 #I just wanted to forecast 250 time points\niterations = 10 #I wanted to have 10 different forecast\n\ndaily_returns = np.exp(drift + stdev * norm.ppf(np.random.rand(t_intervals, iterations)))\n#daily_returns actually is some kind of a noise. When we multiply this with the t time price, we can obtain t+1 time price","a6ce7e29":"S0 = data.close.iloc[-1]\nS0","aadc8f90":"#Let us first create en empty matrix such as daily returns\nprice_list = np.zeros_like(daily_returns)\nprice_list[0] = S0\nprice_list","d7cb162d":"# With a simple for loop, we are going to forecast the next 250 days\nfor t in range(1, t_intervals):\n    price_list[t] = price_list[t - 1] * daily_returns[t]\nprice_list = pd.DataFrame(price_list)\nprice_list['close'] = price_list[0]\nprice_list.head()","d4a289be":"close = data.close\nclose = pd.DataFrame(close)\nframes = [close, price_list]\nmonte_carlo_forecast = pd.concat(frames)","532a7326":"monte_carlo_forecast.head()","0aa3092b":"monte_carlo_forecast.tail()","86aaebbd":"monte_carlo = monte_carlo_forecast.iloc[:,:].values\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(17,8))\nplt.plot(monte_carlo)\nplt.show()","a9873f8b":"#Let's see the distribution of the log returns\nimport plotly.plotly as py\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n#Thanks to https:\/\/www.kaggle.com\/kanncaa1\/plotly-tutorial-for-beginners \ntrace = go.Histogram(x=log_returns,opacity=0.85,name = \"Logarithmic Return\", marker=dict(color='rgba(0, 0, 255, 0.8)'))\ninfo = [trace]\nlayout = go.Layout(barmode='overlay',\n                   title='Distribution of the Logarithmic Returns',\n                   xaxis=dict(title='Logarithmic Return'),\n                   yaxis=dict( title='Dist'),\n)\nfig = go.Figure(data=info, layout=layout)\niplot(fig)","0a09993a":"#Note bad huh, it means that the return can be modeled with more traditional methods.","ef62977d":"#I'm not going to analyse the stationarity or other hypothesis because I did some of them in my other kernel.\n#In fact there are just so many hypothesis testing and I don't want to go into details of that.","43946c49":"data['log_return'] = np.log(1 + data.close.pct_change())\ndata.reset_index(inplace=True)\ndata['date'] = pd.to_datetime(data['date'])\ndata = data.set_index('date')\ndata.head()","28b7c40f":"data = data.dropna()","dead9c1f":"#But we can examine the seasonality\nfrom statsmodels.tsa.seasonal import seasonal_decompose\ndecomposition = seasonal_decompose(data.log_return, freq = 260) #Was there 260 workdays in a year?\ntrend = decomposition.trend\nseasonal = decomposition.seasonal\nresidual = decomposition.resid\n\nplt.figure(figsize=(12,6))\nplt.subplot(411)\nplt.plot(log_returns, label='Original')\nplt.legend(loc='best')\nplt.subplot(412)\nplt.plot(trend, label='Trend')\nplt.legend(loc='best')\nplt.subplot(413)\nplt.plot(seasonal,label='Seasonality')\nplt.legend(loc='best')\nplt.subplot(414)\nplt.plot(residual, label='Residuals')\nplt.legend(loc='best')\nplt.tight_layout()\nplt.show()","18be2507":"#Now we shall examine the serial correlation\nimport statsmodels.api as sm  \nfrom statsmodels.tsa.stattools import acf  \nfrom statsmodels.tsa.stattools import pacf\nfig = plt.figure(figsize=(12,8))\nax1 = fig.add_subplot(211)\nfig = sm.graphics.tsa.plot_acf(data.log_return, lags=40, ax=ax1)\nax2 = fig.add_subplot(212)\nfig = sm.graphics.tsa.plot_pacf(data.log_return, lags=40, ax=ax2)\nplt.show()","d02db886":"#Let's try to make an Auto Regressive Moving Average model\n#I have found this code to find best paramaters for Ar(p) and Ma(q)\nfrom statsmodels.tsa.stattools import ARMA\ndef best_AR_MA_checker(df,lower,upper):\n    from statsmodels.tsa.stattools import ARMA\n    from statsmodels.tsa.stattools import adfuller\n    arg=np.arange(lower,upper)\n    arg1=np.arange(lower,upper)\n    best_param_i=0\n    best_param_j=0\n    temp=12000000\n    rs=99\n    for i in arg:\n        for j in arg1:\n            model=ARMA(df, order=(i,0,j))\n            result=model.fit(disp=0)\n            resid=adfuller(result.resid)\n            if (result.aic<temp and  adfuller(result.resid)[1]<0.05):\n                temp=result.aic\n                best_param_i=i\n                best_param_j=j\n                rs=resid[1]\n                \n                \n            print (\"AR: %d, MA: %d, AIC: %d; resid stationarity check: %d\"%(i,j,result.aic,resid[1]))\n            \n    print(\"the following function prints AIC criteria and finds the paramters for minimum AIC criteria\")        \n    print(\"best AR: %d, best MA: %d, best AIC: %d;  resid stationarity check:%d\"%(best_param_i, best_param_j, temp, rs))     \nbest_AR_MA_checker(data.log_return,0,3) #For each parameter I want to try from 0 to 2","40165580":"#So, I wanna try arma(1,0)\nfrom statsmodels.tsa.stattools import ARMA\nmodel=ARMA(data.log_return, order=(1,0))\nres=model.fit(disp=0)\nprint (res.summary())","e761e0ea":"Now we have a very interesting graph. I don't know maybe I did something wrong:) But it can interpreted as no seasoanity, no trend no nothing..","47312d72":"Let's create a variable S0 (inital stock price) equals to the last  closing price .","c1b31c47":"**IN THIS KERNEL, I WILL TRY TO EXAMINE A FINANCIAL TIME SERIES AND DO SOME MODELING**","1db3b04c":"Now we may have some kind of autocorrelation","160ea2f8":"Now we can see in this graph, 10 possible realization after the already realized path of the prices.\nNow we can go further for better prediction tools. \nBut we should use logarithmic returns from now on because the time series of the prices are not stationary etc..","047f75d3":"**I'm intended to make an hybid model with monte carlo simulation, arma model and maybe LSTM.  \nSo, to be continued...\nI'm open to any kind of suggestion:)**","7efdf62c":"Let's first use monte carlo simulation for forecasting "}}