{"cell_type":{"cc4018d7":"code","4f09782a":"code","5f016333":"code","68f032c6":"code","169be37c":"code","54729803":"code","24bcc94b":"code","c7451ace":"code","dfdfca93":"code","fb767270":"code","84a73507":"code","0ceb9b95":"code","c95ac333":"code","810b9da6":"code","ba6cd754":"code","739a4a1e":"code","c141eb8b":"code","0d4f554b":"code","fdd2cda5":"code","27827d8b":"code","4793a0c7":"code","bcac0b5c":"code","e4f3f3cf":"code","e1b0f118":"code","6cf5e8fd":"code","d4cffbb7":"code","ab684964":"code","77805099":"code","5a56bec8":"code","08d50e90":"code","73fc2a01":"code","7757781d":"code","987cafdb":"code","16b11949":"code","16543941":"code","370dafb5":"code","16b0c7b6":"markdown","757b4e09":"markdown","902fe2b9":"markdown","58d208f7":"markdown"},"source":{"cc4018d7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4f09782a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.core.interactiveshell import InteractiveShell\nfrom sklearn.cluster import KMeans\nfrom sklearn.manifold import TSNE\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.decomposition import PCA\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.cluster import AgglomerativeClustering\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nfrom sklearn.metrics import silhouette_score\nInteractiveShell.ast_node_interactivity = \"all\"\n%matplotlib inline","5f016333":"df = pd.read_csv('\/kaggle\/input\/top-spotify-songs-from-20102019-by-year\/top10s.csv', encoding='latin1')\ndf.head()\ndf.shape","68f032c6":"df = df.rename(columns = {'Unnamed: 0': 'id'})\ndf.head()","169be37c":"df.isnull().any()","54729803":"df = df.drop_duplicates()\ndf.shape","24bcc94b":"df = df.drop(['id'], axis=1)\ndf.head()","c7451ace":"df['top genre'].value_counts().head()\ndf['artist'].value_counts().head()\ndf['title'].value_counts().head()\ndf['year'].value_counts().head()","dfdfca93":"df[df.title == 'Company']","fb767270":"yearless_df = df.drop(['year', 'title', 'pop'], axis=1)\nyearless_df.drop_duplicates()","84a73507":"df['top genre'].value_counts()","0ceb9b95":"for i in yearless_df['top genre']:\n    if 'pop' in i:\n        yearless_df['top genre'] = yearless_df['top genre'].replace(i, 'pop')\n        \n    elif 'hip hop' in i:\n        yearless_df['top genre'] = yearless_df['top genre'].replace(i, 'hip hop')\n\n    elif 'edm' in i:\n        yearless_df['top genre'] = yearless_df['top genre'].replace(i, 'edm')\n\n    elif 'r&b' in i:\n        yearless_df['top genre'] = yearless_df['top genre'].replace(i, 'pop')\n\n    elif 'latin' in i:\n        yearless_df['top genre'] = yearless_df['top genre'].replace(i, 'latin')\n\n    elif 'room' in i:\n        yearless_df['top genre'] = yearless_df['top genre'].replace(i, 'room')\n\n    elif 'electro' in i:\n        yearless_df['top genre'] = yearless_df['top genre'].replace(i, 'edm')\n        \nyearless_df['top genre'] = yearless_df['top genre'].replace('chicago rap', 'hip hop')\n        \nyearless_df[\"top genre\"]","c95ac333":"yearless_df['top genre'].value_counts()","810b9da6":"yearless_df","ba6cd754":"# genre_df = pd.DataFrame(yearless_df['top genre'].value_counts()).reset_index()\n# genre_df.columns = ['top genre','count']\n# genre_df['top_genre_modeling'] = genre_df['top genre'] \n# genre_df.loc[genre_df['count']< 4,'top_genre_modeling'] = 'other'\n# genre_df = genre_df.drop(['top genre'], axis=1)\n# genre_df","739a4a1e":"temp_df = yearless_df\nvalue_counts = temp_df.stack().value_counts() # Entire DataFrame \nto_remove = value_counts[value_counts <= 3].index\ntemp_df.replace(to_remove, 'other', inplace=True)\ntemp_df['top genre'].value_counts()\ntemp_df.head()\ntemp_df.shape","c141eb8b":"yearless_df['top genre'] = temp_df['top genre']\nyearless_df[['bpm', 'nrgy', 'dnce', 'dB', 'live', 'val', 'dur', 'acous', 'spch', 'artist']] = df[['bpm', 'nrgy', 'dnce', 'dB', 'live', 'val', 'dur', 'acous', 'spch', 'artist']]\nyearless_df['top genre'].value_counts()\nyearless_df.head()","0d4f554b":"new_df = yearless_df\nnew_df.artist.unique()","fdd2cda5":"new_df.isnull().any()\nnew_df = new_df.drop_duplicates()\nnew_df = new_df.reset_index(drop=True)\nnew_df.head()","27827d8b":"plt.hist(new_df.dB)\nplt.show()\nplt.hist(new_df.bpm)\nplt.show()\nplt.hist(new_df.nrgy)\nplt.show()\nplt.hist(new_df.live)\nplt.show()\nplt.hist(new_df.val)\nplt.show()\nplt.hist(new_df.dur)\nplt.show()\nplt.hist(new_df.acous)\nplt.show()\nplt.hist(new_df.spch)\nplt.show()","4793a0c7":"new_df","bcac0b5c":"new_df.bpm.unique()\nnew_df.dB.unique()","e4f3f3cf":"new_df.bpm = new_df.bpm.replace(0, new_df.bpm.mean())\nnew_df.bpm.unique()\nnew_df.dB = new_df.dB.replace(-60, new_df.dB.mean())\nnew_df.dB.unique()","e1b0f118":"temp_df = pd.get_dummies(new_df[['artist', 'top genre']])\nnew_df = new_df.join(temp_df, how='left')\nnew_df = new_df.drop(columns = ['artist', 'top genre'], axis=1)\nnew_df.shape","6cf5e8fd":"X_std = StandardScaler().fit_transform(new_df)\npca = PCA(n_components=.95)\nprincipalComponents = pca.fit_transform(X_std) # Plot the explained variances\n\n#Plotting the Cumulative Summation of the Explained Variance\nplt.figure()\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('Number of Components')\nplt.ylabel('Variance (%)') #for each component\nplt.title('Pulsar Dataset Explained Variance')\nplt.show()\n\n# plt.scatter(PCA_components[0], PCA_components[1], alpha=.1, color='black')\n# plt.xlabel('PCA 1')\n# plt.ylabel('PCA 2')\n# PCA_components = pd.DataFrame(principalComponents)","d4cffbb7":"pca = PCA(n_components=20)\nprincipalComponents = pca.fit_transform(X_std)\npca_df = pd.DataFrame(principalComponents)","ab684964":"sum_of_squared_distances = []\nK = range(1,20)\nfor k in K:\n    km = KMeans(n_clusters=k)\n    km = km.fit(pca_df)\n    sum_of_squared_distances.append(km.inertia_)\n    \nax = sns.lineplot(x=K, y = sum_of_squared_distances)\nax.set(xlabel='K', ylabel='sum of squared distances', title='Elbow graph')","77805099":"kmeans = KMeans(n_clusters=13)    \nkmeans.fit(pca_df)\ny_kmeans = kmeans.predict(pca_df)\ny_kmeans","5a56bec8":"plt.scatter(pca_df.iloc[:, 0], pca_df.iloc[:, 1], c=y_kmeans, s=50, cmap='viridis')\ncenters = kmeans.cluster_centers_\nplt.scatter(centers[:, 0], centers[:, 1], c='black', s=200, alpha=0.5);","08d50e90":"plt.figure(figsize=(20, 10))\nplt.title(\"Spotify Dendograms\")\ndendogram = dendrogram(linkage(pca_df, method='ward'))","73fc2a01":"ac = AgglomerativeClustering(n_clusters=13, affinity='euclidean', linkage='ward')\nac.fit_predict(pca_df)","7757781d":"dbscan = DBSCAN(eps = 9, min_samples = 3)\ndbscan.fit(pca_df)\nlabels = dbscan.labels_\n\n# Number of clusters in labels, ignoring noise if present.\nn_clusters_ = len(set(labels)) - (1 if -1 in labels else 0)\nprint('Estimated number of clusters: %d' % n_clusters_)","987cafdb":"kmeans_labels = pd.DataFrame(kmeans.labels_)\nac_labels = pd.DataFrame(ac.labels_)\ndbscan_labels = pd.DataFrame(dbscan.labels_)\n\nsilhouette_score(pca_df, kmeans_labels, metric='euclidean')\nsilhouette_score(pca_df, ac_labels, metric='euclidean')\nsilhouette_score(pca_df, dbscan_labels, metric='euclidean')","16b11949":"dbscan_df = new_df.join(dbscan_labels, how='left')\ndbscan_df = dbscan_df.rename(columns = {0: 'labels'})\ndbscan_df.head()","16543941":"df_scaled = pd.DataFrame(new_df)\ndf_scaled['dbscan'] = dbscan.labels_\ndf_mean = (df_scaled.loc[df_scaled.dbscan!=-1, :].groupby('dbscan').mean())\nresults = pd.DataFrame(columns=['Variable', 'Var'])\nfor column in df_mean.columns[1:]:\n    results.loc[len(results), :] = [column, np.var(df_mean[column])]\n    selected_columns = list(results.sort_values('Var', ascending=False,).head(7).Variable.values) + ['dbscan']\n    tidy = df_scaled[selected_columns].melt(id_vars='dbscan')","370dafb5":"# 12 is the number of clusters in DBScan\nfor i in range(12):\n    sns.catplot(x='dbscan', y='value', hue='variable', data=tidy[tidy['dbscan']==i], height=5, aspect=.7, kind='bar')","16b0c7b6":"# We have to check for collinearity and reduce dimensionality","757b4e09":"# You can see all of the clusters above","902fe2b9":"# Super categorization of top genre's feature","58d208f7":"# Drop year, title, and deduplicate rows"}}