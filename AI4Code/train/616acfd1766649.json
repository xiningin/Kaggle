{"cell_type":{"74068f0b":"code","cb1763f2":"code","a08a3619":"code","f73ca02b":"code","3dede8a0":"code","38c34abe":"code","d374d92b":"code","cf5f006a":"code","3682915f":"code","d908609a":"code","f1b0b3ae":"code","ff94af22":"code","353b0d24":"code","f5ac03d6":"code","c5032e7b":"code","7b44147a":"code","2ae87319":"code","9f9918d4":"code","2542dbce":"code","8ae4dc55":"code","283522ba":"code","302cff34":"code","9238810d":"code","c187c721":"code","9cf61b38":"markdown","564e743f":"markdown","0ec45ea0":"markdown","fbedc6ef":"markdown","4646ced5":"markdown","9e214f11":"markdown","ff1fc2e6":"markdown","807df4ef":"markdown","728a7203":"markdown"},"source":{"74068f0b":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cb1763f2":"# import\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nimport time\nfrom tqdm.notebook import tqdm  # notebook\u7528\nfrom pprint import pprint\n\n# \u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f\ndf_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\n# \u30c7\u30fc\u30bf\u3092\u30de\u30fc\u30b8\ndf_test[\"Survived\"] = np.nan\ndf = pd.concat([df_train, df_test], ignore_index=True, sort=False)\n\nprint(df_train.shape)\nprint(df_test.shape)\nprint(df.shape)\nprint(df.columns)","a08a3619":"def str_normalize(ds):\n    # \u30a2\u30eb\u30d5\u30a1\u30d9\u30c3\u30c8\u3068\u6570\u5b57\u306e\u307f\u306b\u3059\u308b\n    ds = ds.str.replace(\"[^a-zA-Z0-9]+\", \" \", regex=True)\n    return ds\n\ndf[\"Name_normalize\"] = str_normalize(df[\"Name\"])","f73ca02b":"!pip install -q transformers\n!pip install -q silence_tensorflow\n!pip install -q janome","3dede8a0":"from silence_tensorflow import silence_tensorflow\nsilence_tensorflow()\n\n# tensorflow\nimport tensorflow as tf\nimport tensorflow.keras.layers as kl\n\n# skleratn\nimport sklearn.metrics\n\n# transformers\nimport transformers\nfrom transformers import logging\nlogging.set_verbosity_error()","38c34abe":"pretrained_model_name = \"bert-base-uncased\"","d374d92b":"tokenizer = transformers.AutoTokenizer.from_pretrained(pretrained_model_name)\nbert_model = transformers.TFAutoModel.from_pretrained(pretrained_model_name, from_pt=True)\nprint(bert_model.config)","cf5f006a":"# \u9069\u5f53\u306b\u540d\u524d\u306e\u60c5\u5831\u3092\u4f7f\u3063\u3066\u307f\u3066\u307f\u308b\nsample_name = df[\"Name\"][0]\nprint(sample_name)\n\n# Tokenize\u3057\u305f\u7d50\u679c\ntoken_words = tokenizer.tokenize(sample_name)\nprint(token_words)\n\n# BERT\u306b\u5165\u529b\u3059\u308b\u5f62\u5f0f\u306b\u5909\u63db\nencode_token = tokenizer(sample_name, padding=\"max_length\", max_length=12, truncation=True)\npprint(encode_token)\n\n# BERT\u3078\u306e\u5165\u529b\u5f62\u5f0f\u3092\u30c7\u30b3\u30fc\u30c9\u3057\u305f\u7d50\u679c\nprint(tokenizer.decode(encode_token[\"input_ids\"]))","3682915f":"# \u6700\u5927\u5358\u8a9e\u6570\u306e\u78ba\u8a8d\nmax_len = []\n# 1\u6587\u3065\u3064\u51e6\u7406\nfor sent in df[\"Name_normalize\"]:\n    # Tokenize\u3067\u5206\u5272\n    token_words = tokenizer.tokenize(sent)\n    # \u6587\u7ae0\u6570\u3092\u53d6\u5f97\u3057\u3066\u30ea\u30b9\u30c8\u3078\u683c\u7d0d\n    max_len.append(len(token_words))\n# \u6700\u5927\u306e\u5024\u3092\u78ba\u8a8d\nprint('\u6700\u5927\u5358\u8a9e\u6570: ', max(max_len))\nprint('\u4e0a\u8a18\u306e\u6700\u5927\u5358\u8a9e\u6570\u306bSpecial token\uff08[CLS], [SEP]\uff09\u306e+2\u3092\u3057\u305f\u5024\u304c\u6700\u5927\u5358\u8a9e\u6570')\n\n# \u5358\u8a9e\u6570\u3092\u8a2d\u5b9a\nsequence_max_length = max(max_len) + 2\nif sequence_max_length > 512:\n    sequence_max_length = 512","d908609a":"def build_model(learning_rate, is_print=False):\n\n    # BERT\u30e2\u30c7\u30eb\u3092\u30ed\u30fc\u30c9\n    bert_model = transformers.TFAutoModel.from_pretrained(pretrained_model_name)\n    #bert_model = transformers.TFAutoModel.from_pretrained(pretrained_model_name, from_pt=True)  # pytouch\u306e\u5834\u5408\n\n    # tf\u3078\u306e\u5165\u529b\u30c6\u30f3\u30bd\u30eb\u3092\u4f5c\u6210\n    # \u5165\u529b\u306fsequence_max_length\u30b5\u30a4\u30ba\u30923\u3064(['input_ids', 'token_type_ids', 'attention_mask'])\n    inputs = [\n        kl.Input(shape=(sequence_max_length,), dtype=tf.int32, name=name)\n        for name in tokenizer.model_input_names\n    ]\n    if is_print:\n        pprint(inputs)\n\n    # BERT\u30e2\u30c7\u30eb\u306e\u51fa\u529b\u3092\u5f97\u308b\n    # \u51fa\u529b\u306f TFBaseModelOutputWithPooling (https:\/\/huggingface.co\/transformers\/main_classes\/output.html#tfbasemodeloutput)\n    # x[0](last_hidden_\u200b\u200bstate) : \u6700\u5f8c\u306e\u30ec\u30a4\u30e4\u30fc\u306e\u51fa\u529b\n    # x[1](pooler_output)     : \u5206\u985e\u30c8\u30fc\u30af\u30f3\u306e\u72b6\u614b\n    x = bert_model(inputs)\n\n    # BERT\u51fa\u529b\u306e0\u756a\u76ee\u304c\u30af\u30e9\u30b9\u5206\u985e\u3067\u4f7f\u3046\u51fa\u529b\n    x1 = x[0][:, 0, :]\n\n    # \u5206\u985e\u7528\u306e\u51fa\u529b\u5c64\u3092\u7528\u610f\n    # \u51fa\u529b\u5c64\u306e\u69cb\u6210\u306fTFBertForSequenceClassification\u3092\u53c2\u8003\n    x1 = kl.Dropout(0.1)(x1)\n    x1 = kl.Dense(1, activation='sigmoid', kernel_initializer=transformers.modeling_tf_utils.get_initializer(0.02))(x1)\n    model_train = tf.keras.Model(inputs=inputs, outputs=x1)\n\n    # \u30aa\u30ea\u30b8\u30ca\u30eb\u306e\u51fa\u529b\u5024\u3092\u7279\u5fb4\u91cf\u3068\u3057\u305f\u3044\u306e\u3067\u4e88\u6e2c\u5c02\u7528\u306e\u30e2\u30c7\u30eb\u3082\u5225\u9014\u4f5c\u3063\u3066\u304a\u304f\n    model_pred = tf.keras.Model(inputs=inputs, outputs=[x1, x[0][:, 0, :]])\n    \n    # optimizer\u306f AdamW \u3092\u4f7f\u7528\n    optimizer = transformers.AdamWeightDecay(learning_rate=learning_rate)\n    model_train.compile(optimizer, loss=\"binary_crossentropy\", metrics=[\"acc\"])\n    #model_train.compile(optimizer, loss=\"categorical_crossentropy\", metrics=[\"acc\"])  # softmax\u306e\u5834\u5408\n    if is_print:\n        print(model_train.summary())\n\n    return model_train, model_pred\n\n# \u8a66\u3057\u306b\u5b9f\u884c\nbuild_model(0.1, is_print=True)","f1b0b3ae":"import tensorflow as tf\nimport os\n\nruntime_type = \"\"\n\ntry:\n    resolver = tf.distribute.cluster_resolver.TPUClusterResolver()\n    \n    #--- TPU\n    print('Running on TPU ', resolver.cluster_spec().as_dict()['worker'])\n    runtime_type = \"TPU\"\n\n    # This is the TPU initialization code that has to be at the beginning.\n    tf.config.experimental_connect_to_cluster(resolver)\n    tf.tpu.experimental.initialize_tpu_system(resolver)\n    tpu_strategy = tf.distribute.TPUStrategy(resolver)\n\n    tf.keras.backend.clear_session()\n    print(\"All devices: \", tf.config.list_logical_devices('TPU'))\n\nexcept ValueError:\n    \n    if tf.test.gpu_device_name() != \"\":\n        #--- GPU\n        runtime_type = \"GPU\"\n    else:\n        runtime_type = \"CPU\"\n\nprint(\"runtime_type: \", runtime_type)","ff94af22":"import sklearn.metrics\ndef train_bert(\n        df_train,       # \u5b66\u7fd2\u7528\u306e\u30c7\u30fc\u30bf\n        text_column,    # \u5bfe\u8c61\u306e\u30ab\u30e9\u30e0\u540d\n        target_column,  # \u76ee\u7684\u5909\u6570\u306e\u30ab\u30e9\u30e0\u540d\n        df_valid=None,  # \u691c\u8a3c\u7528\u30c7\u30fc\u30bf\n        df_pred_list=[],       # \u4e88\u6e2c\u7528\u30c7\u30fc\u30bf\n        model_file_prefix=\"\",  # \u4fdd\u5b58\u6642\u306e\u30d5\u30a1\u30a4\u30eb\u540d\u8b58\u5225\u5b50\n        epochs=20,\n        batch_size=8,\n    ):\n\n    #--------------------\n    # \u5b66\u7fd2\u7387\n    #--------------------\n    lr0 = 0.000005\n    learning_rate = [\n        0.00001,\n        0.00002,\n    ]\n    if epochs-len(learning_rate) > 0:\n        lr_list = np.linspace(0.00002, 0, epochs-len(learning_rate))\n        learning_rate.extend(lr_list)\n    def lr_scheduler(epoch):\n        return learning_rate[epoch]\n    lr_callback = tf.keras.callbacks.LearningRateScheduler(lr_scheduler)\n\n\n    #--------------------\n    # file\n    #--------------------\n    model_path = \"{}_{}.h5\".format(\n        model_file_prefix, \n        pretrained_model_name, \n    )\n\n\n    #--------------------\n    # \u30e2\u30c7\u30eb\n    #--------------------\n    if runtime_type == \"TPU\":\n        with tpu_strategy.scope():\n            model_train, model_pred = build_model(lr0)\n    else:\n        model_train, model_pred = build_model(lr0)\n\n\n    #-----------------------------\n    # \u30e2\u30c7\u30eb\u5165\u51fa\u529b\u7528\u306e\u30c7\u30fc\u30bf\u4f5c\u6210\u95a2\u6570\n    #-----------------------------\n    def _build_x_from_df(df):\n        # Series -> list\n        x = df[text_column].tolist()\n\n        # tokenize\n        x = tokenizer(x, padding=\"max_length\", max_length=sequence_max_length, \n            truncation=True, return_tensors=\"tf\")\n        \n        # BatchEncoding -> dict\n        return dict(x)\n\n    def _build_y_from_df(df):\n        return df[target_column]\n        #return tf.keras.utils.to_categorical(df[target_column], num_classes=2)  # softmax\u7528\n\n\n    #-------------------\n    # valid\u7528\u306edataset\u3092\u4f5c\u6210\n    #-------------------\n    if df_valid is not None:\n        valid_x = _build_x_from_df(df_valid)\n        valid_y = _build_y_from_df(df_valid)\n        valid_dataset = (tf.data.Dataset.from_tensor_slices((valid_x, valid_y))\n            .batch(batch_size)\n            .cache()\n        )\n    else:\n        valid_dataset = None\n\n\n    #-------------------\n    # \u5b66\u7fd2\n    #-------------------\n    if False:\n    #if os.path.isfile(model_path):\n        # \u5b66\u7fd2\u6e08\u307f\u30e2\u30c7\u30eb\u3092load\n        print(model_path)\n        model_train.load_weights(model_path)\n    else:\n        train_x = _build_x_from_df(df_train)\n        train_y = _build_y_from_df(df_train)\n        train_dataset = (\n            tf.data.Dataset.from_tensor_slices((train_x, train_y))\n            .shuffle(len(train_x), seed=1234)\n            .batch(batch_size)\n            .prefetch(tf.data.experimental.AUTOTUNE)  # GPU\u304c\u8a08\u7b97\u3057\u3066\u3044\u308b\u9593\u306bBatch\u30c7\u30fc\u30bf\u3092CPU\u5074\u3067\u7528\u610f\u3057\u3066\u304a\u304f\u6a5f\u80fd\n        )\n\n        model_train.fit(train_dataset, epochs=epochs, validation_data=valid_dataset, callbacks=[lr_callback])\n        model_train.save_weights(model_path)\n\n    #-------------------\n    # \u8a55\u4fa1\n    #-------------------\n    if df_valid is not None:\n        print(\"valid\")\n        pred_y = model_train.predict(valid_dataset, verbose=1)\n        \n        # \u6b63\u89e3\u7387\n        pred_y_label = np.where(pred_y < 0.5, 0, 1)\n        metric = sklearn.metrics.accuracy_score(valid_y, pred_y_label)\n        print(\"acc\", metric)\n    else:\n        metric = 0\n\n    #-------------------\n    # \u4e88\u6e2c\n    #-------------------\n    print(\"pred\")\n    pred_y_list = []\n    emb_list = []\n    for df_pred in df_pred_list:\n\n        pred_x = _build_x_from_df(df_pred)\n        pred_dataset = (\n            tf.data.Dataset.from_tensor_slices((pred_x,))\n            .batch(batch_size)\n            .cache()\n        )\n\n        # \u4e88\u6e2c\n        pred_output = model_pred.predict(pred_dataset, verbose=1)\n\n        # pred\n        pred_y = pred_output[0].reshape((-1,))  # (-1,1) -> (-1)\n        pred_y_list.append(pred_y)\n        \n        # emb\n        emb_list.append(pred_output[1])\n    \n    return metric, pred_y_list, emb_list\n\n\n#--- \u5b9f\u884c\u4f8b\nmetric, pred_y_list, emb_list = train_bert(\n    df_train=df[df[\"Survived\"].notnull()][:10],  # \u5b66\u7fd2\u30c7\u30fc\u30bf\n    text_column=\"Name_normalize\",\n    target_column=\"Survived\",\n    df_valid=df[df[\"Survived\"].notnull()][:10],  # \u691c\u8a3c\u30c7\u30fc\u30bf(\u4eee\u3067\u5b66\u7fd2\u30c7\u30fc\u30bf\u3068\u540c\u3058)\n    df_pred_list=[df[df[\"Survived\"].isnull()][:10]],  # \u4e88\u6e2c\u30c7\u30fc\u30bf\n    epochs=2,  # \u8a66\u3057\u306a\u306e\u3067\u5c11\u306a\u76ee\n)\nprint(metric)\nprint(pred_y_list[0].shape)\nprint(emb_list[0].shape)\n","353b0d24":"import sklearn.model_selection\n\ndef train_cv(df, text_column, target_column, n_splits):\n    \n    df_train = df[df[target_column].notnull()]\n    df_test = df[df[target_column].isnull()]\n\n    df_train_idx = df_train.index\n    \n    # \u7d50\u679c\u7528\n    df_pred = pd.DataFrame(df.index, columns=[\"index\"]).set_index(\"index\")\n    df_emb = pd.DataFrame(df.index, columns=[\"index\"]).set_index(\"index\")\n    df_emb_pred = None\n    metric_list = []\n\n    #----------------\n    # cross validation\n    #----------------\n    kf = sklearn.model_selection.StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=1234)\n    for i, (train_idx, test_idx) in enumerate(kf.split(df_train, df_train[target_column])):\n        df_train_sub = df_train.iloc[train_idx]\n        df_test_sub = df_train.iloc[test_idx]\n\n        df_pred_list = [df_test_sub]\n        df_pred_list.append(df_test)\n\n        model_file_prefix = \"cv_{}\".format(i)\n\n        # train\n        metric, pred_y_list, emb_list = train_bert(\n            df_train=df_train_sub, \n            text_column=text_column,\n            target_column=target_column, \n            df_valid=df_test_sub,\n            df_pred_list=df_pred_list,\n            model_file_prefix=model_file_prefix,\n        )\n        metric_list.append(metric)\n\n        # \u4e88\u6e2c\u7d50\u679c\u3092\u4fdd\u5b58\n        result_name = \"result_{}\".format(i)\n        df_pred.loc[df_train_idx[test_idx], result_name] = pred_y_list[0]\n        df_pred.loc[df_test.index, result_name] = pred_y_list[1]\n\n        #---------\n        a = pd.DataFrame(emb_list[0], index=df_train_idx[test_idx])\n        df_emb = df_emb.combine_first(a)\n        \n        if df_emb_pred is None:\n            df_emb_pred = pd.DataFrame(emb_list[1], index=df_test.index)\n        else:\n            df_emb_pred += emb_list[1]\n        \n    \n    pred_y = df_pred.mean(axis=1)\n        \n    df_emb_pred \/= n_splits\n    df_emb = df_emb.combine_first(df_emb_pred)\n\n    return np.mean(metric_list), pred_y.values, df_emb\n\n#--- \u7d50\u679c\u3068\u7279\u5fb4\u91cf\u3092\u53d6\u5f97\n#metric, pred_y, df_emb = train_cv(df, \"Name\", \"Survived\", n_splits=3)\nmetric, pred_y, df_emb = train_cv(df, \"Name_normalize\", \"Survived\", n_splits=3)\nprint(metric)\nprint(pred_y.shape)\nprint(df_emb.shape)\n","f5ac03d6":"df[\"BERT\"] = pred_y\ndf[\"BERT_label\"] = np.where(pred_y < 0.5, 0, 1)\n\n_df = df[df[\"Survived\"].notnull()]\nprint(sklearn.metrics.accuracy_score(_df[\"Survived\"], _df[\"BERT_label\"]))\n\n# \u4e88\u6e2c\u7d50\u679c\u3092csv\u3067\u51fa\u529b\n_df = df[df[\"Survived\"].isnull()]\ndf_submit = pd.DataFrame()\ndf_submit[\"PassengerId\"] = _df[\"PassengerId\"]\ndf_submit[\"Survived\"] = _df[\"BERT_label\"]\ndf_submit.to_csv('submit1.csv', header=True, index=False)\n","c5032e7b":"!pip install -q -U scikit-learn==0.23.2\n!pip install -q pycaret shap\n","7b44147a":"df_emb[\"Survived\"] = df[\"Survived\"]\n\nimport pycaret.classification as pcc\nr = pcc.setup(df_emb[df[\"Survived\"].notnull()], target=\"Survived\", silent=True, session_id=1234)","2ae87319":"pcc.interpret_model(pcc.create_model(\"lightgbm\"))","9f9918d4":"best_models = pcc.compare_models(n_select=3)","2542dbce":"models = []\nmodels.append(pcc.tune_model(best_models[0]))","8ae4dc55":"models.append(pcc.tune_model(best_models[1]))","283522ba":"models.append(pcc.tune_model(best_models[2]))","302cff34":"blend_model = pcc.blend_models(models)\ntuned_model = pcc.tune_model(blend_model)","9238810d":"pcc.plot_model(tuned_model, plot='parameter')","c187c721":"# \u30e2\u30c7\u30eb\u3092\u691c\u8a3c\npcc.predict_model(tuned_model)\n\n# \u4e88\u6e2c\nx_pred = df_emb[df_emb[\"Survived\"].isnull()]\ndf_pred = pcc.predict_model(tuned_model, data=x_pred)\n\n# \u4e88\u6e2c\u7d50\u679c\u3092csv\u3067\u51fa\u529b\ndf_submit = pd.DataFrame()\ndf_submit[\"PassengerId\"] = df[df[\"Survived\"].isnull()][\"PassengerId\"]\ndf_submit[\"Survived\"] = df_pred[\"Label\"].apply(lambda x:int(float(x)))\ndf_submit.to_csv('submit2.csv', header=True, index=False)\n\n# \u5ff5\u306e\u305f\u3081\u7d50\u679c\u306e\u5272\u5408\u3092\u8868\u793a\nprint(df[\"Survived\"].value_counts(normalize='columns'))\nprint(df_submit[\"Survived\"].value_counts(normalize='columns'))\n","9cf61b38":"# 5.TPU","564e743f":"# 1.\u30c7\u30fc\u30bf\u8aad\u307f\u8fbc\u307f","0ec45ea0":"# 8.BERT\u30e2\u30c7\u30eb\u306e\u4e88\u6e2c\u7d50\u679c","fbedc6ef":"# 9.BERT\u306e\u7279\u5fb4\u91cf\u3092\u4f7f\u3063\u3066AutoML","4646ced5":"# 3.BERT","9e214f11":"# 7.CV\u5b66\u7fd2","ff1fc2e6":"# 6.\u5b66\u7fd2\u95a2\u6570","807df4ef":"# 2.\u524d\u51e6\u7406","728a7203":"# 4.BERT\u30e2\u30c7\u30eb"}}