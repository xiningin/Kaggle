{"cell_type":{"6174b864":"code","5b0d80f1":"code","472a2e93":"code","5ecab7fb":"code","7cb66800":"code","e7f7d5de":"code","81789bce":"code","5a42f4de":"code","5e5d553c":"code","2fc965bf":"code","1b8d227c":"code","8567f06a":"code","d5cca51c":"code","09f4f5de":"code","167e8647":"code","df98e54a":"code","b85744ff":"code","9c91e82e":"code","f2e7ceaa":"code","f282290e":"code","dba81bbc":"code","2126cd94":"code","3a000a22":"markdown","351780a6":"markdown","3f7ecce5":"markdown","58ae15ef":"markdown","89790ec6":"markdown","e53a41f8":"markdown","bb2cf34b":"markdown"},"source":{"6174b864":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","5b0d80f1":"!pip install wget\n!pip install gensim\n!pip install fse\n!pip install nltk","472a2e93":"!pip install spacy==2.1.0","5ecab7fb":"!pip install fse","7cb66800":"from gensim.models import KeyedVectors\nimport fse # fast sentence embeddings\nfrom fse.models import uSIF\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk.tokenize import TweetTokenizer\nimport os\nimport requests\nfrom spacy import util\nimport spacy\nfrom IPython.core.display import display, HTML\nfrom collections import namedtuple\nimport json\nimport tqdm\nfrom gensim.models.fasttext import FastText\nimport json","e7f7d5de":"def convert_html(jsonfile):    \n    html_markup_list=[]\n    for er in jsonfile['annotations']:\n        title = '<h1>'+er['title']+'<\/h1>'\n        authors = '<h6> '+er['authors']+'<\/h6>'\n        paragraph = '<p>'+er['paragraph'] + '<\/p>'   \n        document_score = '<i>'+str(er['document_score'])+'<\/i>'\n        html_markup_list.append(title+ ' ('+ document_score +')' + '\\n'+authors+'\\n'+ paragraph)\n    return html_markup_list\n    \n# query = \"pre-existing pulmonary disease SARS-Cov2 Hypertension\" \n# query = \"What is the incubation days of SARS-CoV-2\" \n# query = \"incubation days coronavirus 2019-nCoV\"#  COVID-19\n# query = 'socio economic poverty behaviour'\n# query = 'what is the comorbidities associated with death'\n# query = 'public health mitigation measures that could be effective for control'\n# query = 'socio-economic and behavioral factors to understand the economic impact of the SARS-CoV-2 virus and whether there were differences. '\n# query = 'what are the risk factors for death in COVID-19'\n# query = 'what is the basic reproductive number of SARS-CoV-2 in days'\n# query = 'what is the serial interval days SARS-CoV-2'\n# query = 'what do we know about the environmental factors influencing SARS-CoV-2'\n# query = 'what do we know about medication COVID-19'\nquery = 'Transmission dynamics of the virus SARS-CoV-2'\n# query ='risk of fatality among symptomatic hospitalized patients in SARS-CoV-2'\n# query = 'Efforts targeted at a universal coronavirus vaccine'\n\nquery_result = requests.get(\"http:\/\/covido-api.volitionlabs.xyz:5200\/covido_predictor?query_sentence=\"+query)\ndisplay(HTML('<hr>\\n'.join(convert_html(query_result.json()))))","81789bce":"tknzr = TweetTokenizer()\nnltk.download('wordnet')\nlemmatizer = WordNetLemmatizer() \n\n\n#remove stop words and lemmatize the query terms\ndef clean_my_text(full_text):\n    stopset = set(stopwords.words('english')) #| set(string.punctuation)\n    tokens = tknzr.tokenize(full_text)\n    cleanup = [lemmatizer.lemmatize(token) for token in tokens if token not in stopset and len(token) > 3]\n    return cleanup\n\n#the highlighter for the query terms found in the text\ndef term_highlighter(text: str = None, query: str = None) -> str:\n    color_poll=['#61F96C', '#F98A61', '#F3F967','#61DDF9', '#6F8AFA',\n                '#F77FBB', '#69F5E9', '#CBD5D4','#21C758', '#5491BC',\n               '#EEAE4D', '#D14DEE', '#EE4D4D', '#5E8A86', '#218F1B',\n                '#CA5522', '#6458E0','#84B50B','#B50BA0','#0BB5A8']\n    \n    if not text or not query:\n        raise ValueError('Either the no result was found for the query, or the query is empty.')\n    \n    terms=clean_my_text(query)\n    colorindex=0\n    \n    for term in set(terms):\n        if type(term) != str:\n            continue\n        if term.lower() in text.replace(\"'\", \"\\'\").lower():\n            text = re.sub(r\"(?i)\\b%s\" % term , '<span style=\"background-color:'+color_poll[colorindex]+'\">' + term + '<\/span>', text)\n            colorindex=colorindex+1\n    return text","5a42f4de":"from gensim.summarization.summarizer import summarize\nimport requests\nfrom nltk import wordpunct_tokenize\nimport re\n\n#submit the query to the API\nquery = 'How did covid-19 influence global economy, job market and affect small businesses?'\nurl_ = 'http:\/\/covido-api.volitionlabs.xyz:8157\/solr\/covid_tweetdata\/select?q=tweet%3A%20'+query+'&rows=100&wt=json'\n\nr= requests.get(url_)\nquery_result = r.json()","5e5d553c":"tweet_result =[]\ntweet_ids = []\n\nfor each_result in query_result['response']['docs']:\n    tweet_result.append(each_result['tweet'])\n    tweet_ids.append(each_result['tweetID'])","2fc965bf":"#for privacy reasons, we cannot display all the original tweets. The front-end will display the link to the tweet. \n#And for the notebook output, we summarize the tweet outputs\nsummary_result = summarize(' '.join(tweet_result), ratio=0.1)\nnew_text = term_highlighter(summary_result, query)","1b8d227c":"#visualise the output\nfrom IPython.core.display import display, HTML\ndisplay(HTML(new_text))","8567f06a":"Item = namedtuple('INFO', ('title', 'section', 'pdate', 'weburl', 'apiurl', 'text', 'ents', 'sentiment'))\nResult = namedtuple('Result', ('sid', 'aid', 'pid', 'sent', 'para'))\n\ndef process_sent(sent, remove_punct=True):\n    if remove_punct:\n        tokens = [token.lower() for token in nltk.word_tokenize(sent.strip()) if len(token)>1 or token.isalnum()]\n    else:\n        tokens = [token.lower() for token in nltk.word_tokenize(sent.strip())]\n    return tokens\n    \ndef flat_corpus(dataset, para_level=False, remove_punct=True):\n    corpus = []\n    sid2art = {}\n    for aid, an in enumerate(tqdm.tqdm(dataset)):\n        title = an.title\n        paragraphs = [title] + an.text.split(\"\\n\")\n        if para_level:\n            for pid, para in enumerate(paragraphs):\n                start_idx = len(sid2art)\n                sid2art[start_idx] = {'aid':aid, 'pid':pid}\n                tokens = process_sent(para, remove_punct)\n                corpus.append(tokens)           \n        else:\n            for pid, para in enumerate(paragraphs):\n                sents = nltk.sent_tokenize(para)\n                start_idx = len(sid2art)\n                for i in range(len(sents)):\n                    sid2art[start_idx+i] = {'aid':aid, 'pid':pid}\n                    tokens = process_sent(sents[i], remove_punct)\n                    corpus.append(tokens)\n    return corpus, sid2art\n\n\n\n\n\n","d5cca51c":"# load NewsMedia dataset\nwith open('\/kaggle\/input\/news-socio-economicdata\/news-text-unique.json', 'r') as f:\n    data = json.load(f)\n    analysis = [Item._make(d) for d in data]","09f4f5de":"# generate tokenized corpus for fse model to use\nline_corpus, sid2aid = flat_corpus(analysis, para_level=False, remove_punct=True)","167e8647":"# load fasttext model pre-trained on news-media corpus\nft_model = FastText.load('\/kaggle\/input\/newsfasttext\/news-fast.model')","df98e54a":"from fse.models import SIF\nfrom fse import IndexedList\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import metrics\nimport numpy as np","b85744ff":"def train_fse(fasttext_model, flat_corpus):\n    # train sentence embedding model\n    model = SIF(fasttext_model)\n    indexed_corpus = IndexedList(flat_corpus)\n    model.train(indexed_corpus)\n    return model, indexed_corpus\n\ndef retrieve_paragraph(mappings, dataset):\n    # from sentence get its paragragh\n    retrieved_paras = []\n    for m in mappings:\n        article = dataset[m['aid']]\n        paras = [article.title] + article.text.split('\\n')\n        paragraph = paras[m['pid']]\n        retrieved_paras.append(paragraph)\n    return retrieved_paras\n\ndef train_tfidf_vectorizer(dataset):\n    # train tfidf vectorizer from paragrapgh context\n    tfidf_vectorizer = TfidfVectorizer(binary=True, sublinear_tf=True, norm='l2', ngram_range=(1,2))\n    para_corpus, pid2aid = flat_corpus(dataset, para_level=True, remove_punct=True)\n    tfidf_vectorizer.fit([' '.join(line) for line in para_corpus])\n    return tfidf_vectorizer\n    \ndef rerank(query, results, vectorizer):\n    # rerank the retrieved sentences by incoporating paragrapgh context\n    trans_para = vectorizer.transform([query]+[' '.join(process_sent(r.para)) for r in results])\n    dense = trans_para.todense()\n    sims = metrics.pairwise.cosine_similarity(dense[0], dense[1:])\n    sorted_rank = np.argsort(-sims[0]).tolist()\n    return [results[sr] for sr in sorted_rank]\n    \ndef search(model, vectorizer, query, sent_corpus, dataset, sid_mapping):\n    # search best sentences\/articles to addresss the query\n    query = ' '.join(process_sent(query, remove_punct=True))\n    sents = model.sv.similar_by_sentence(nltk.word_tokenize(query), model=model)\n    sids = [s[0] for s in sents]\n    mapping = [sid_mapping[sid] for sid in sids]\n    paragraphs = retrieve_paragraph(mapping, dataset)\n    \n    results = []\n    exist = set()\n    for i in range(len(sids)):\n        sid = sids[i]\n        aid = mapping[i]['aid']\n        pid = mapping[i]['pid']\n        sent = sent_corpus[i]\n        para = paragraphs[i]\n        if aid not in exist:\n            results.append(Result(sid=sid, aid=aid, pid=pid, sent=sent, para=para))\n            exist.add(aid)\n    return rerank(query, results, vectorizer)","9c91e82e":"# build fse model and tfidf vectorizer for document retrieval\nfse_model, indexed_corpus = train_fse(ft_model, line_corpus)\ntfidf_vectorizer = train_tfidf_vectorizer(dataset=analysis)","f2e7ceaa":"# retrieve relevant sentence, paragraph and articles by a query\nquery = \"human rights coronavirus\"\nsearch_results = search(model=fse_model, vectorizer=tfidf_vectorizer, \n                               query=query, sid_mapping=sid2aid,\n                               sent_corpus=line_corpus, dataset=analysis)","f282290e":"# show the example search results\nsearch_results[:2]","dba81bbc":"# show the titles of retrieved articles\nnews_titles = [analysis[r.aid].title for r in search_results]\nprint(f\"query: {query}\\n\")\nprint(f\"retrieved articles' titles:\\n\")\nfor i,t in enumerate(news_titles):\n    print(i,t)","2126cd94":"# show the examples of retrieved articles from datasetnews_\nnews_articles = [analysis[r.aid] for r in search_results]\nprint(news_articles[:2])","3a000a22":"# The original challenge queries on Cord-19 dataset","351780a6":"> ### Summarization\nFor privacy reasons, we cannot display original tweets as an output. On the front end, you will see the link to the original tweet, and as a notebook output, we provide a summarised version of the query result.","3f7ecce5":"> ### Soc-Economy from News Media","58ae15ef":"### retrieve documents by sentence embedding","89790ec6":"We provide the following chatbot-style question answering front-end https:\/\/covido.volitionlabs.xyz\/ along with the API service.\nThe data used in this submission comec from CORD-19 data, news media data, and twitter data: https:\/\/www.kaggle.com\/olesya\/covid19-twitter-socioeconomic-data\n\nThe tool is not limited to pre-defined queries. Out approach is based on this paper:\n[[Arora, S., Liang, Y., & Ma, T. (2016). A simple but tough-to-beat baseline for sentence embeddings.](http:\/\/openreview.net\/forum?id=SyK00v5xx)]\n","e53a41f8":"# # Install Micriservices and word embeddings ","bb2cf34b":"## Twitter socio-economic data"}}