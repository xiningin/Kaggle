{"cell_type":{"c0e13b24":"code","b977ea8c":"code","a66c4032":"code","38332393":"code","9f17f2a0":"code","777c9f4f":"code","8b4ff50c":"code","30e5ab7b":"code","1dc42cb0":"code","aa6caf51":"code","bae25fcd":"code","d14c54cd":"code","1bcfe3b8":"code","a0eef19f":"markdown","539d6e46":"markdown","229091c9":"markdown","50ff969f":"markdown","2e2c2633":"markdown","1b796ff3":"markdown","885ecbcf":"markdown","a9937ce9":"markdown","ee9ba770":"markdown","0d22e61a":"markdown","9e3159a5":"markdown","0930cbc6":"markdown","ec8649d5":"markdown","a4276e99":"markdown","6925ab74":"markdown","cee9164a":"markdown","824d8e40":"markdown","96d75c7c":"markdown","2ff5456c":"markdown"},"source":{"c0e13b24":"import pandas as pd\nimport numpy as np\nimport re \nimport nltk \nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold","b977ea8c":"train=pd.read_csv(\"\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_train.csv\",encoding='latin1')\ntest=pd.read_csv(\"\/kaggle\/input\/covid-19-nlp-text-classification\/Corona_NLP_test.csv\",encoding='latin1')","a66c4032":"train['text'] = train.OriginalTweet\ntrain[\"text\"] = train[\"text\"].astype(str)\n\ntest['text'] = test.OriginalTweet\ntest[\"text\"] = test[\"text\"].astype(str)\n\n# Data has 5 classes, let's convert them to 3\n\ndef classes_def(x):\n    if x ==  \"Extremely Positive\":\n        return \"2\"\n    elif x == \"Extremely Negative\":\n        return \"0\"\n    elif x == \"Negative\":\n        return \"0\"\n    elif x ==  \"Positive\":\n        return \"2\"\n    else:\n        return \"1\"\n    \n\ntrain['label']=train['Sentiment'].apply(lambda x:classes_def(x))\ntest['label']=test['Sentiment'].apply(lambda x:classes_def(x))\n\n\ntrain.label.value_counts(normalize= True)","38332393":"x_train = train.text\ny_train = train.label","9f17f2a0":"#Using NLTK\ntext = \"what is going to happen next in data science \\\nis a mystery what has happened is history it is an \\\ninterdisciplinary field that uses scientific method \\\nprocesses algorithms and systems to extract knowledge \\\nand insights from many structural and unstructured data \\\ndata science is related to data mining machine learning and big data\"\ntxt = nltk.sent_tokenize(text)\n\nword2count = {} \nfor data in txt: \n    words = nltk.word_tokenize(data) \n    for word in words: \n        if word not in word2count.keys(): \n            word2count[word] = 1\n        else: \n            word2count[word] += 1\n\nprint(word2count)","777c9f4f":"import heapq \nfreq_words = heapq.nlargest(200, word2count, key=word2count.get)\nfreq_words","8b4ff50c":"X = [] \nfor data in txt: \n    vector = [] \n    for word in freq_words: \n        if word in nltk.word_tokenize(data): \n            vector.append(1) \n        else: \n            vector.append(0) \n    X.append(vector) \nX = np.asarray(X)\nX","30e5ab7b":"#Using SkLearn\ntext = \"Natural Language Processing (NLP) is a sub-field of artificial intelligence \\\nthat deals understanding and processing human language. In light of new advancements \\\nin machine learning, many organizations have begun applying natural language processing \\\nfor translation, chatbots and candidate filtering\"\n\ncount_vec = CountVectorizer()\ncount_occurs = count_vec.fit_transform([text])\ncount_occur_df = pd.DataFrame((count, word) for word, count in zip(count_occurs.toarray().tolist()[0], count_vec.get_feature_names()))\ncount_occur_df.columns = ['Word', 'Count']\ncount_occur_df.sort_values('Count', ascending=False, inplace=True)\ncount_occur_df.head()","1dc42cb0":"text = \"Natural Language Processing (NLP) is a sub-field of artificial intelligence \\\nthat deals understanding and processing human language. In light of new advancements \\\nin machine learning, many organizations have begun applying natural language processing \\\nfor translation, chatbots and candidate filtering\"\n\nnorm_count_vec = TfidfVectorizer(use_idf=False, norm='l2')\nnorm_count_occurs = norm_count_vec.fit_transform([text])\nnorm_count_occur_df = pd.DataFrame((count, word) for word, count in zip(\n    norm_count_occurs.toarray().tolist()[0], norm_count_vec.get_feature_names()))\nnorm_count_occur_df.columns = ['Word', 'Count']\nnorm_count_occur_df.sort_values('Count', ascending=False, inplace=True)\nnorm_count_occur_df.head()","aa6caf51":"text = \"Natural Language Processing (NLP) is a sub-field of artificial intelligence \\\nthat deals understanding and processing human language. In light of new advancements \\\nin machine learning, many organizations have begun applying natural language processing \\\nfor translation, chatbots and candidate filtering\"\n\ntfidf_vec = TfidfVectorizer()\ntfidf_count_occurs = tfidf_vec.fit_transform([text])\ntfidf_count_occur_df = pd.DataFrame((count, word) for word, count in zip(\n    tfidf_count_occurs.toarray().tolist()[0], tfidf_vec.get_feature_names()))\ntfidf_count_occur_df.columns = ['Word', 'Count']\ntfidf_count_occur_df.sort_values('Count', ascending=False, inplace=True)\ntfidf_count_occur_df.head()","bae25fcd":"stop_words = ['a', 'an', 'the']\n\n# Basic cleansing\ndef cleansing(text):\n    # Tokenize\n    tokens = text.split(' ')\n    # Lower case\n    tokens = [w.lower() for w in tokens]\n    # Remove stop words\n    tokens = [w for w in tokens if w not in stop_words]\n    return ' '.join(tokens)\n\n# All-in-one preproce\ndef preprocess_x(x):\n    processed_x = [cleansing(text) for text in x]\n    \n    return processed_x\n\ndef build_model(mode):\n    # Intent to use default paramaters for show case\n    vect = None\n    if mode == 'count':\n        vect = CountVectorizer()\n    elif mode == 'tf':\n        vect = TfidfVectorizer(use_idf=False, norm='l2')\n    elif mode == 'tfidf':\n        vect = TfidfVectorizer()\n    else:\n        raise ValueError('Mode should be either count or tfidf')\n    \n    return Pipeline([\n        ('vect', vect),\n        ('clf' , LogisticRegression(solver='newton-cg',n_jobs=-1))\n    ])\n\ndef pipeline(x, y, mode):\n    processed_x = preprocess_x(x)\n    \n    model_pipeline = build_model(mode)\n    cv = KFold(n_splits=5, shuffle=True)\n    \n    scores = cross_val_score(model_pipeline, processed_x, y, cv=cv, scoring='accuracy')\n    print(\"Accuracy: %0.4f (+\/- %0.4f)\" % (scores.mean(), scores.std() * 2))\n    \n    return model_pipeline","d14c54cd":"x = preprocess_x(x_train)\ny = y_train\n    \nmodel_pipeline = build_model(mode='count')\nmodel_pipeline.fit(x, y)\n\nprint('Number of Vocabulary: %d'% (len(model_pipeline.named_steps['vect'].get_feature_names())))","1bcfe3b8":"print('Using Count Vectorizer------')\nmodel_pipeline = pipeline(x_train, y_train, mode='count')\n\nprint('Using TF Vectorizer------')\nmodel_pipeline = pipeline(x_train, y_train, mode='tf')\n\nprint('Using TF-IDF Vectorizer------')\nmodel_pipeline = pipeline(x_train, y_train, mode='tfidf')","a0eef19f":"#### b. Normalized Count Occurrence","539d6e46":"When to use BOW over Embeddings?\n\n1. Building a baseline model. \n2. If your dataset is small and context is domain specific, BoW may work better than Word Embedding. Context is very domain specific which means that you cannot find corresponding Vector from pre-trained word embedding models (GloVe, fastText etc)","229091c9":"\n![#Precious](https:\/\/i.imgur.com\/5YSC6pg.gif)","50ff969f":"### This was simple text, however while modeling words may reach zillions. And we need to set a cut off as we don't use all the words.\n","2e2c2633":"Contents:\n\n* [1. Bag of Words](#1)\n* [2. TF-IDF](#2)\n* [3. BOG vs. TF-IDF](#3)    ","1b796ff3":"<font size=+4 color=\"Black\"><center><b>Bag of Words & TF-IDF<\/b><\/center><\/font>\n<font size=-1 color=\"Black\"><center><b>*Series: All about NLP by Data Tattle <\/b><\/right><\/font>","885ecbcf":"### Bag of Words\n\nWe can not feed texts (words) directly into the NLP or ML models as all the algorithms work on numbers. Hence BOG is used to preprocess the texts. Here TOTAL occurence of EACH word is counted and kept as a BAG OF WORDS. \n","a9937ce9":"<a id=\"3\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>3. BOG vs. TF-IDF<\/b><\/font><br>","ee9ba770":" If you think that extremely high frequency may dominate the result and causing model bias. Normalization can be apply to pipeline easily.","0d22e61a":"### Types of BOW\n#### a. Count Occurrence\n#### b. Normalized Count Occurrence\n#### c. TF-IDF","9e3159a5":"Term Frequency - inverse document frequency is defined as a numeric statistic that is intended to reflect how important a word is to a document in a collection\/ corpus\n\n### TF\n\nIt is a measure of how frequently a term (t) appears in a document:\n\n    tf = n \/ number of terms in a document\n \nIn above example \ntf (data)   = 5\/41\ntf (science)= 2\/41\n\n\n### IDF\n\nIDF is a measure of how important a term is\n\n    idf =  log (number of documents \/ number of documents with term 't')\n\nSince we took one text above, hence number of documents will be 1. But in practical word there are millions of documents. So let's assume we had 5 documents in total but data existed in one.\n\nSo, IDF for our text is:\nidf(data) = log(5\/5)\n\n\n    tf-idf = tf * idf\n    \n#### Words with a higher score are more important, and those with a lower score are less important\n\n","0930cbc6":"### About this notebook\n\n#### This notebook is a part of Series \"[All about NLP](https:\/\/www.kaggle.com\/datatattle\/all-about-nlp)\" and will cover vectorization using Bag of Words & TF-IDF\n\n\n\n![](https:\/\/miro.medium.com\/max\/2428\/0*Qq8FcR-mgnvjWZLQ.gif)","ec8649d5":"<a id=\"1\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>1. Bag of Words<\/b><\/font><br>","a4276e99":"<a id=\"2\"><\/a>\n    \n<font size=\"+2\" color=\"indigo\"><b>2. TF-IDF<\/b><\/font><br>","6925ab74":"<font size=\"+3\" color=\"Green\"><b>Related Work:<\/b><\/font>\n\nNext is Latent Semantic Analysis (LSA) and Latent Dirichlet Allocation (LDA)\n\n### See [here](https:\/\/www.kaggle.com\/datatattle\/all-about-nlp) for related work","cee9164a":"<font size=\"+2\" color=\"Red\"><b>Please Upvote if you like the work<\/b><\/font>\n\n### It gives motivation to a working professional (like me) to contribute more.","824d8e40":"#### a. Count Occurrence\n","96d75c7c":"### Classifier used is Logistic Regression. \n#### Count BoW performs better than Tf-Idf in our case","2ff5456c":"<font size=\"+3\" color=\"Green\"><b>Please Upvote if you liked the work<\/b><\/font>"}}