{"cell_type":{"08e46873":"code","d051257e":"code","924ee074":"code","6fd250f8":"code","64a057b4":"code","5de4478f":"code","2f6f0515":"code","2bad0c9d":"code","c6048cb1":"code","dbcd9432":"code","b9053b94":"code","1acbb1ff":"code","ba5e6a05":"code","74f804d0":"code","927d6837":"code","4b9e6821":"code","5a9c7d0d":"code","62ed12b6":"code","45d26741":"code","0ec5d28e":"code","f92e8a4f":"code","09dea2b0":"code","196ebc37":"code","9b3e34c7":"code","3af8001d":"code","d37f1bbf":"code","727349a0":"code","db23be64":"code","94a8c5f8":"code","0c0119cd":"code","82c7575f":"code","4471701d":"code","8f9efaa8":"code","78b4f245":"code","8b96d8b9":"code","40ec6275":"code","1250448f":"code","ddaadf2e":"code","41daa79a":"code","9894df39":"code","b691b488":"code","708f8712":"code","e442a2b8":"code","8b3859cc":"code","7145b14f":"code","89dacef4":"code","a23c4a22":"markdown","158df9d7":"markdown","cce3c97b":"markdown","ce169426":"markdown","a3e96de9":"markdown","8cbfc710":"markdown","8378a6d8":"markdown","6fca85b0":"markdown","a81847cd":"markdown","00195d3c":"markdown","f9bab0b6":"markdown","214505a9":"markdown","dece7788":"markdown","d704d084":"markdown","d4ca4fed":"markdown","fd72983d":"markdown","7964cd6d":"markdown","f45de3db":"markdown","9dd48f4a":"markdown","258216dc":"markdown","74d4d465":"markdown","fa320579":"markdown","109de749":"markdown","a61b252d":"markdown","f805ac9b":"markdown","90d7884e":"markdown","b9376394":"markdown","0191df50":"markdown","dafca9a5":"markdown","29de24d8":"markdown","9d593726":"markdown","fbebbb7d":"markdown","7aa23ae5":"markdown","34374579":"markdown","90819d77":"markdown","ce6310f2":"markdown","a3b092f6":"markdown","fa511aa9":"markdown","36da5f51":"markdown","1ce200e3":"markdown","4fb73fbf":"markdown","604e4fdd":"markdown","444d9610":"markdown","e3830462":"markdown","3de2bf84":"markdown","a8779a62":"markdown","f32e6b25":"markdown","84d74fa1":"markdown","bf1c20a8":"markdown","cfecdd64":"markdown","9f822e13":"markdown","6f8d09a1":"markdown","b4b98860":"markdown","634ea6af":"markdown","0a1e931e":"markdown","74c5e71e":"markdown","baea51aa":"markdown","6107d55a":"markdown","e8cf47f4":"markdown","c9ff41df":"markdown","390d17d7":"markdown","4c994e24":"markdown","3b399572":"markdown","0814f139":"markdown","c997a7cb":"markdown","d52c53f8":"markdown","ec93fe08":"markdown","4f693bbd":"markdown","6891ce96":"markdown","c47bca7c":"markdown","6353f69b":"markdown","1387b77d":"markdown","e8a2aa23":"markdown","26509f24":"markdown","ce1def72":"markdown","126f8bbb":"markdown"},"source":{"08e46873":"import pandas as pd\nfrom pandas.plotting import scatter_matrix\nimport numpy as np\n#je\u015bli chcemy \u017ceby sns dzia\u0142a\u0142o to zawsze trzeba te\u017c wpisa\u0107 matplotlib!!!\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set()\nsns.set_palette('Set2')\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","d051257e":"iris = pd.read_csv('iris.csv')\niris.columns = ['id', 'sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'class']\niris.drop('id', axis=1, inplace=True) \niris.head()","924ee074":"iris.info()","6fd250f8":"iris.shape","64a057b4":"iris.describe()","5de4478f":"pd.DataFrame(iris.groupby('class').size()).head()","2f6f0515":"sns.jointplot(x='sepal-length', y='sepal-width', data=iris)","2bad0c9d":"sns.relplot(x='sepal-length', y='sepal-width', hue='class', data=iris, height=5, aspect=10\/5)\nplt.title('Sepal length vs width')","c6048cb1":"sns.jointplot(x='petal-length', y='petal-width', data=iris)","dbcd9432":"sns.relplot(x='petal-length', y='petal-width', hue='class', data=iris, height=5, aspect=10\/5)\nplt.title('Petal length vs width')","b9053b94":"iris.hist(figsize=(10, 6))\nplt.show()","1acbb1ff":"iris.boxplot(by='class', figsize=(10, 6))","ba5e6a05":"plt.figure(figsize=(10,6))\nplt.subplot(2,2,1)\nsns.violinplot(x='class', y='petal-length', data=iris)\nplt.subplot(2,2,2)\nsns.violinplot(x='class', y='petal-width', data=iris)\nplt.subplot(2,2,3)\nsns.violinplot(x='class', y='sepal-length', data=iris)\nplt.subplot(2,2,4)\nsns.violinplot(x='class', y='sepal-width', data=iris)","74f804d0":"sns.pairplot(iris, hue='class', size=2.5)\nplt.show()","927d6837":"from sklearn import datasets #to load iris already in int\nfrom sklearn.model_selection import train_test_split #to split the dataset for training and testing\nfrom sklearn.metrics import accuracy_score #for checking the model accuracy\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import plot_confusion_matrix","4b9e6821":"iris = datasets.load_iris()\nx = iris.data\ny = iris.target\nprint('The unique classes are: ' + str(np.unique(y)) + '.')","5a9c7d0d":"x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2) \n#test_size - ratio mi\u0119dzy training a testing (0.2 = 20% ze 150 to testing, reszta to training)\n\nprint('There are {} samples in the training set and {} samples in the testing set.'.format\n      (x_train.shape[0], x_test.shape[0]))","62ed12b6":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nx_train = sc.fit_transform(x_train)\nx_test=sc.transform(x_test)","45d26741":"from sklearn.linear_model import LogisticRegression","0ec5d28e":"model_lr = LogisticRegression()\nmodel_lr.fit(x_train, y_train) #we train the algorithm with the training data and the training output\nprediction = model_lr.predict(x_test) #now we pass the testing data to the trained algorithm\nprint('The accuracy of the logistic regression is: %0.2f' \n      % accuracy_score(prediction, y_test), 'on testing set.')\nprint('The accuracy of the logistic regression is:', model_lr.score(x_train, y_train), \n      'on training set.\\n')\n\nprint(classification_report(prediction, y_test))","f92e8a4f":"class_names = iris.target_names\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp = plot_confusion_matrix(model_lr, x_test, y_test,\n                                 display_labels=class_names,\n                                 cmap=plt.cm.Greens,\n                                 normalize=normalize)\n    disp.ax_.set_title(title)\nplt.show()","09dea2b0":"from sklearn.linear_model import SGDClassifier","196ebc37":"model_sgc = SGDClassifier(loss='modified_huber', shuffle=True, random_state=42)\nmodel_sgc.fit(x_train, y_train)\nprediction = model_sgc.predict(x_test)\nprint('The accuracy of the stochastic gradient descent is: %0.2f' \n      % accuracy_score(prediction, y_test), 'on testing set.')\nprint('The accuracy of the stochastic gradient descent is:', model_sgc.score(x_train, y_train), \n      'on training set.\\n') \n\nprint(classification_report(prediction, y_test))","9b3e34c7":"class_names = iris.target_names\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp = plot_confusion_matrix(model_sgc, x_test, y_test,\n                                 display_labels=class_names,\n                                 cmap=plt.cm.Greens,\n                                 normalize=normalize)\n    disp.ax_.set_title(title)\nplt.show()","3af8001d":"from sklearn.tree import DecisionTreeClassifier","d37f1bbf":"model_dt = DecisionTreeClassifier(max_depth=10, max_features=None, min_samples_leaf=15)\nmodel_dt.fit(x_train, y_train)\nprediction = model_dt.predict(x_test)\nprint('The accuracy of the decision tree is: %0.2f' \n      % accuracy_score(prediction, y_test), 'on testing set.')\nprint('The accuracy of the decision tree is:', model_dt.score(x_train, y_train), \n      'on training set.\\n') \n\nprint(classification_report(prediction, y_test))","727349a0":"class_names = iris.target_names\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp = plot_confusion_matrix(model_dt, x_test, y_test,\n                                 display_labels=class_names,\n                                 cmap=plt.cm.Greens,\n                                 normalize=normalize)\n    disp.ax_.set_title(title)\nplt.show()","db23be64":"from sklearn.neighbors import KNeighborsClassifier","94a8c5f8":"a_index = list(range(1,25))\na = pd.Series()\nx = [1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,18,19,20,21,22,23,24]\n\nfor k in list(range(1,25)):\n    model_knn = KNeighborsClassifier(n_neighbors=k) \n    model_knn.fit(x_train, y_train)\n    prediction = model_knn.predict(x_test)\n    a = a.append(pd.Series(accuracy_score(prediction, y_test)))\n    \nplt.plot(a_index, a)\nplt.xticks(x)\nplt.xlabel('Value of k')\nplt.ylabel('Accuracy score')\nplt.title('Accuracy scores of KKN model for different k values')","0c0119cd":"model_knn = KNeighborsClassifier(n_neighbors=1)\nmodel_knn.fit(x_train, y_train)\nprediction = model_knn.predict(x_test)\nprint('The accuracy of the k-nearest neighbors is: %0.2f' \n      % accuracy_score(prediction, y_test), 'on testing set.')\nprint('The accuracy of the k-nearest neighbors is:', model_knn.score(x_train, y_train), \n      'on training set.\\n') \n\nprint(classification_report(prediction, y_test))","82c7575f":"class_names = iris.target_names\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp = plot_confusion_matrix(model_knn, x_test, y_test,\n                                 display_labels=class_names,\n                                 cmap=plt.cm.Greens,\n                                 normalize=normalize)\n    disp.ax_.set_title(title)\nplt.show()","4471701d":"from sklearn.svm import SVC ","8f9efaa8":"model_svm = SVC(kernel='linear')\nmodel_svm.fit(x_train, y_train)\nprediction = model_svm.predict(x_test)\nprint('The accuracy of the support vector machines is: %0.2f' \n      % accuracy_score(prediction, y_test), 'on testing set.')\nprint('The accuracy of the support vector machines is:', model_svm.score(x_train, y_train), \n      'on training set.\\n') \n\nprint(classification_report(prediction, y_test))","78b4f245":"class_names = iris.target_names\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp = plot_confusion_matrix(model_svm, x_test, y_test,\n                                 display_labels=class_names,\n                                 cmap=plt.cm.Greens,\n                                 normalize=normalize)\n    disp.ax_.set_title(title)\nplt.show()","8b96d8b9":"from sklearn.naive_bayes import GaussianNB ","40ec6275":"model_nbc = GaussianNB()\nmodel_nbc.fit(x_train, y_train) \nprediction = model_nbc.predict(x_test) \nprint('The accuracy of the naive bayes classification is: %0.2f' \n      % accuracy_score(prediction, y_test), 'on testing set.')\nprint('The accuracy of the naive bayes classification is:', model_nbc.score(x_train, y_train), \n      'on training set.\\n') \n\nprint(classification_report(prediction, y_test))","1250448f":"class_names = iris.target_names\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp = plot_confusion_matrix(model_nbc, x_test, y_test,\n                                 display_labels=class_names,\n                                 cmap=plt.cm.Greens,\n                                 normalize=normalize)\n    disp.ax_.set_title(title)\nplt.show()","ddaadf2e":"from sklearn.ensemble import RandomForestClassifier","41daa79a":"model_rf = RandomForestClassifier(n_estimators=120, oob_score=True, n_jobs=-1, \n                              max_features=None, min_samples_leaf=10)\nmodel_rf.fit(x_train, y_train)\nprediction = model_rf.predict(x_test)\nprint('The accuracy of the random forest is: %0.2f' \n      % accuracy_score(prediction, y_test), 'on testing set.')\nprint('The accuracy of the random forest is:', model_rf.score(x_train, y_train), \n      'on training set.\\n') \n \nprint(classification_report(prediction, y_test))","9894df39":"class_names = iris.target_names\ntitles_options = [(\"Confusion matrix, without normalization\", None),\n                  (\"Normalized confusion matrix\", 'true')]\nfor title, normalize in titles_options:\n    disp = plot_confusion_matrix(model_rf, x_test, y_test,\n                                 display_labels=class_names,\n                                 cmap=plt.cm.Greens,\n                                 normalize=normalize)\n    disp.ax_.set_title(title)\nplt.show()","b691b488":" acc_table = {\n 'Algorithm': ['Logistic regression', 'Stochastic Gradient Descent', 'Decision tree', \n               'K-Nearest Neighbors', 'Support Vector Machines', \n               'Naive Bayes Classification', 'Random forest'],\n 'Accuracy testing set': ['97%', '97%', '90%', '100%', '100%', '97%', '90%'],\n 'Accuracy training set': ['98%', '97%', '97%', '100%', '98%', '96%', '98%']\n }\ntable = pd.DataFrame(acc_table, columns=['Algorithm', 'Accuracy testing set', \n                                        'Accuracy training set'])\ntable","708f8712":"x = iris.data\ny = iris.target\n\nknn = KNeighborsClassifier(n_neighbors=1)\nknn.fit(x, y)\n\n#sepal_l = input(\"Insert sepal lenght: \")\nsepal_l = 4","e442a2b8":"#sepal_w = input(\"Insert sepal width: \")\nsepal_w = 2","8b3859cc":"#petal_l = input(\"Insert petal lenght: \")\npetal_l = 1.2","7145b14f":"#petal_w = input(\"Insert petal width: \")\npetal_w = 2","89dacef4":"iris_class = knn.predict([[sepal_l, sepal_w, petal_l, petal_w]])\n\ndef prediction(iris_class):\n    if iris_class == 0:\n        print('Predicted class: iris setosa.')\n    elif iris_class == 1:\n        print('Predicted class: iris versicolor.')\n    else:\n        print('Predicted class: iris virginica.')\n\nprediction(iris_class)","a23c4a22":"As we can see, the accuracy of the k-nearest neighbors for k = 1 is **100%** on test set and **100%** on training set.","158df9d7":"The graph above shows the accuracy for the KNN models using different values of k. \n\nAs we can see, the accuracy = 1 is for values 1 and 15. Other values have accuraccy either 0.967 or 0.934.\n","cce3c97b":"#### Petal features","ce169426":"The violinplot shows the density of the length and width in the classes. The thinner part shows that there is less density whereas the fatter part shows higher density.\n","a3e96de9":"Checking on confusion matrix:","8cbfc710":"Next, I will do boxplots to check for outliers.","8378a6d8":"I used KNN model to fit the model for the entire dataset instead of just the training set. \n\nI also allowed the user to insert own values of sepal and petal lenght and width in order to predict the class of the iris flower. ","6fca85b0":"#### Naive Bayes Classification","a81847cd":"As we can see, the accuracy of the logistic regression is **97%** on test set and **98%** on training set.","00195d3c":"\nNow I will color the scatterplot by class.","f9bab0b6":"#### Random forest","214505a9":"As we can see, the plot above shows the bivariate scatterplots and univariate histograms in the same figure.","dece7788":"In this case, we will choose k = 1 for KKN model. We already know, that the accuracy is 100%.","d704d084":"As we can see from the plot above, in petal length and width, iris setosa has outliers. Also iris versicolor has outliers in petal length. However, in sepal length and width, the only one that has outliers is iris virginica.","d4ca4fed":"Checking on confusion matrix:","fd72983d":"* ### **Classification problem**","7964cd6d":"Now I will proceed with each algorithm. I will check on accuracy scores for both training and testing sets and also on confusion matrix. Since the dataset is small, I will not check on time taken to train the data. ","f45de3db":"Checking on model's accuracy:","9dd48f4a":"Checking on model's accuracy:","258216dc":"## The classification of iris flowers","74d4d465":"As we can see, the accuracy of the naive bayes classification is **97%** on test set and **96%** on training set.","fa320579":"As we can see, the accuracy of the random forest is **90%** on test set and **98%** on training set.","109de749":"#### Stochastic Gradient Descent","a61b252d":"Firstly, I will load all necessary libraries I will use in this project. ","f805ac9b":"#### Decision tree","90d7884e":"As we can see, the accuracy of the stochastic gradient descent is **97%** on test set and **97%** on training set.","b9376394":"#### Logistic regression","0191df50":"I will create the model that predicts whether the class of iris is setosa, versicolor or virginica. ","dafca9a5":"As we can see, the accuracy of the support vector machines is **100%** on test set and **98%** on training set.","29de24d8":"The final step is to show both sepal and petal features on one plot. I will do it by using the pairplot, which shows the bivariate relation between each pair of features.","9d593726":"In this project I will use [**iris**](https:\/\/archive.ics.uci.edu\/ml\/datasets\/iris) dataset to visualize and analyze the data. I will also do the classification of the iris flowers by using machine learning concepts.","fbebbb7d":"In this dataset, our features (attribures) are petal and sepal length and width. Our target variable will be 3 flower classes. \n\n    In the machine learning context target variable \n    is the variable that is or should be the output.","7aa23ae5":"Moreover, we have to scale our data, because it matters when it comes to KNN and SGD alghoritms.","34374579":"#### Sepal and petal features","90819d77":"Checking on confusion matrix:","ce6310f2":"At this point, the best accuracy has KNN model, which is 100%. Thus, we will proceed with this model for k = 1.","a3b092f6":"In this classification problem I will use following algorithms:\n* Logistic regression\n* Decision tree\n* Stochastic Gradient Descent\n* K-Nearest Neighbors\n* Support Vector Machines\n* Naive Bayes Classification\n* Random forest","fa511aa9":"Now I will plot the distribution plot of sepal and petal length and width. ","36da5f51":"As we can see, the accuracy of the decision tree is **90%** on test set and **97%** on training set.","1ce200e3":"As we see there are no null values in the dataset, so the data can be processed.","4fb73fbf":"Checking on model's accuracy:","604e4fdd":"Now, I will group the dataset by the class name and I will display the number of items in each class.","444d9610":"#### K-Nearest Neighbors","e3830462":"As we can see, the plot above shows the bivariate scatterplots and univariate histograms in the same figure. ","3de2bf84":"As we can see, the petal features are easier to cluster comparing to the sepal features. It means that the petal features can help to create better and more accurate predictions over the sepal features. I will check it later.","a8779a62":"Table above shows the basic information like mean, min, max or median values about the dataset. As we can see, for instance, _petal-length_ has min = 1 and max = 6.9. Its median is 4.35 while the mean is 3.76.","f32e6b25":"From the plot we can see that sepal length and width have, more or less, Gaussian distribution. ","84d74fa1":"This step is necessary to check if there is any inconsistency in the dataset. If there are null values, we have to clean the data. ","bf1c20a8":"Now we will see how the length and width vary according to the different class.\n","cfecdd64":"Checking on confusion matrix:","9f822e13":"To do so, I will first load necessary libraries. Libraries according to each algorithm will be loaded step by step. ","6f8d09a1":"Then, I will load dataset and display its first rows and columns. Also, I'm going to change column names and drop 'id' column because it's not needed.","b4b98860":"As we can see from the table and information above, after dropping not necessary column, iris dataset contains of 5 columns and 150 rows.","634ea6af":"#### **Choosing the best algorithm for the classification problem:** ","0a1e931e":"#### Sepal features ","74c5e71e":"Now I will see statistical description of the dataset.","baea51aa":"Firstly, let's check the accuracy for different k values.","6107d55a":"Checking on confusion matrix:","e8cf47f4":"#### Support Vector Machines","c9ff41df":"As we can see, from the values inserted above, and with the use of KNN model, we are able to predict the class of the iris flower. ","390d17d7":"Checking on model's accuracy:","4c994e24":"As we can see from above, iris setosa is always separated from other classes. It means that the relationship between this class and other classes is different. On the other hand, the relationship between iris versicolor and iris virginica is very visible since they overlap.\n\nThe diagonal grouping of some pairs of attributes suggests a high correlation and a predictable relationship. There is a big correlation between iris versicolor and iris virginica.","3b399572":"As we can see, all algorithms show pretty good accuracy scores. The best ones are Logistic regression, K-Nearest Neighbors and Support Vector Machines. ","0814f139":"Checking on confusion matrix:","c997a7cb":"Since the process involves both training and testing, I will create training and testing datasets, where:\n- x_train - training features,\n- x_test - testing features,\n- y_train - training class,\n- y_test - testing class. \n","d52c53f8":"Now is time to proceed with the visual analysis of the dataset. Firstly, I will plot separate plots for sepal and petal features with the relationship between length and width. It will be helpful for further classification (clustering classes).","ec93fe08":"* ### **Data visualization**","4f693bbd":"As we can see, our unique values are:\n- 0 - iris setosa,\n- 1 - iris versicolor,\n- 2 - iris virginica.","6891ce96":"To do the calculations, I am going to represent the features and labels as _int_ instead of _string_. \n\nSince the iris dataset is already represented as _int_ in build in function of scikit learn library, I will load it. I will also assign the features and target variable to separate variables x and y, where:\n- x - features (sepal or petal), \n- y - classes (setosa, versicolor ot virginica).","c47bca7c":"Now I will color the scatterplot by class.","6353f69b":"Now we will compare the accuracy on training and testing set of each algorithm.","1387b77d":"Checking on confusion matrix:","e8a2aa23":"We can see that there are 3 different classes with 50 items each.","26509f24":"As we can see, our training set contains of 120 samples while the testing set contains of 30 samples.","ce1def72":"Checking on model's accuracy:","126f8bbb":"Checking on model's accuracy:"}}