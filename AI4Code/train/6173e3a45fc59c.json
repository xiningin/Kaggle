{"cell_type":{"49acb4ed":"code","f219411c":"code","32d408c7":"code","ca8a9136":"code","0a7fe093":"code","047e8fe0":"code","11261012":"code","4026be51":"code","0bd0df2a":"code","558c208b":"code","d5d6656b":"code","a5cf5a5f":"code","47b2edbc":"code","b23ccd50":"code","11429e60":"code","65b17d56":"code","00b7def0":"markdown","b4973218":"markdown","b736bd35":"markdown","c16dc522":"markdown","033f08ee":"markdown","47d18a94":"markdown","f37d9b86":"markdown","bc086c62":"markdown","2ba68c50":"markdown","80e01bc7":"markdown"},"source":{"49acb4ed":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under\n# the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as\n# output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the\n# current session\n\n\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, make_scorer\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import LabelEncoder, StandardScaler,OneHotEncoder, OrdinalEncoder\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline, FeatureUnion, make_pipeline\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom xgboost import XGBRegressor\npd.set_option('display.max_columns', None)\nimport category_encoders as ce\nfrom sklearn.feature_selection import SelectKBest, f_classif, f_regression\nfrom pandas_profiling import ProfileReport\nimport math","f219411c":"def save_file (predictions):\n    \"\"\"Save submission file.\"\"\"\n    # Save test predictions to file\n    output = pd.DataFrame({'Id': sample_submission_file.Id,\n                       'SalePrice': predictions})\n    output.to_csv('submission.csv', index=False)\n    print (\"Submission file is saved\")\n\ndef calculate_root_mean_squared_log_error(y_true, y_pred):\n    \"\"\"Calculate root mean squared error of log(y_true) and log(y_pred)\"\"\"\n    if len(y_pred)!=len(y_true): return 'error_mismatch'\n    y_pred_new = [math.log(x+1) for x in y_pred]\n    y_true_new = [math.log(x+1) for x in y_true]\n    return mean_squared_error(y_true_new, y_pred_new, squared=False)","32d408c7":"# Read the data\ntrain_data = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv',\n                         index_col='Id')\nX_test = pd.read_csv('\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv',\n                     index_col='Id')\nX = train_data.copy()\n\n# Remove rows with missing target, separate target from predictors\nX.dropna(axis=0, subset=['SalePrice'], inplace=True)\ny = X.SalePrice              \nX.drop(['SalePrice', 'Utilities'], axis=1, inplace=True)\nX_test.drop(['Utilities'], axis=1, inplace=True)\n\n# Break off validation set from training data\nX_train, X_valid, y_train, y_valid = train_test_split(X, y, train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\nsample_submission_file = pd.read_csv(\"\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv\")\n\nprint('Data is OK')","ca8a9136":"# Select object columns\ncategorical_cols = [cname for cname in X_train.columns if X_train[cname].dtype == \"object\"]\n\n# Select numeric columns\nnumerical_cols = [cname for cname in X_train.columns if X_train[cname].dtype in ['int64',\n                                                                                 'float64']]\n\n# Number of missing values in each column of training data\nmissing_val_count_by_column_train = (X_train.isnull().sum())\nprint(\"Number of missing values in each column:\")\nprint(missing_val_count_by_column_train[missing_val_count_by_column_train > 0])","0a7fe093":"# Number of missing values in numerical columns\nmissing_val_count_by_column_numeric = (X_train[numerical_cols].isnull().sum())\nprint(\"Number of missing values in numerical columns:\")\nprint(missing_val_count_by_column_numeric[missing_val_count_by_column_numeric > 0])","047e8fe0":"# Imputation lists\n\n# imputation to null values of these numerical columns need to be 'constant'\nconstant_num_cols = ['GarageYrBlt', 'MasVnrArea']\n\n# imputation to null values of these numerical columns need to be 'mean'\nmean_num_cols = list(set(numerical_cols).difference(set(constant_num_cols)))\n\n# imputation to null values of these categorical columns need to be 'constant'\nconstant_categorical_cols = ['Alley', 'MasVnrType', 'BsmtQual', 'BsmtCond', 'BsmtExposure',\n                             'BsmtFinType1', 'BsmtFinType2', 'FireplaceQu','GarageType',\n                             'GarageFinish', 'GarageQual', 'GarageCond', 'PoolQC', 'Fence',\n                             'MiscFeature']\n\n# imputation to null values of these categorical columns need to be 'most_frequent'\nmf_categorical_cols = list(set(categorical_cols).difference(set(constant_categorical_cols)))\n\nmy_cols = constant_num_cols + mean_num_cols + constant_categorical_cols + mf_categorical_cols","11261012":"# Define transformers\n# Preprocessing for numerical data\n\nnumerical_transformer_m = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='mean')),\n    ('scaler', StandardScaler())])\n\nnumerical_transformer_c = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value=0)),\n    ('scaler', StandardScaler())])\n\n\n# Preprocessing for categorical data for most frequent\ncategorical_transformer_mf = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown = 'ignore', sparse = False))\n])\n\n# Preprocessing for categorical data for constant\ncategorical_transformer_c = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='constant', fill_value='NA')),\n    ('onehot', OneHotEncoder(handle_unknown = 'ignore', sparse = False))\n])\n\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num_mean', numerical_transformer_m, mean_num_cols),\n        ('num_constant', numerical_transformer_c, constant_num_cols),\n        ('cat_mf', categorical_transformer_mf, mf_categorical_cols),\n        ('cat_c', categorical_transformer_c, constant_categorical_cols)\n    ])","4026be51":"# Define Model\nmodel = XGBRegressor(learning_rate = 0.01,\n                            n_estimators=2500,\n                            max_depth=4,\n                            min_child_weight=1,\n                            gamma=0,\n                            subsample=0.7,\n                            colsample_bytree=0.6,\n                            reg_alpha = 0.1,\n                            reg_lambda = 1.25)","0bd0df2a":"# Preprocessing of validation data\nX_valid_eval = preprocessor.fit(X_train, y_train).transform (X_valid)","558c208b":"# Display the number of remaining columns after transformation \nprint(\"We have\", X_valid_eval.shape[1], \"features after transformation\")","d5d6656b":"# Define XGBRegressor fitting parameters for the pipeline\nfit_params = {\"model__early_stopping_rounds\": 50,\n              \"model__eval_set\": [(X_valid_eval, y_valid)],\n              \"model__verbose\": False,\n              \"model__eval_metric\" : \"rmsle\"}","a5cf5a5f":"# Create and Evaluate the Pipeline\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train, **fit_params)\n\n# Get predictions\npreds = my_pipeline.predict(X_valid)\n\n# Evaluate the model\nscore = calculate_root_mean_squared_log_error(y_valid,preds)\n\nprint(\"Score: {}\".format(score))\n","47b2edbc":"# Preprocessing of training data, fit model \nX_cv = X[my_cols].copy()\nX_sub = X_test[my_cols].copy()","b23ccd50":"# Multiply by -1 since sklearn calculates *negative* MAE\nscores = -1 * cross_val_score(my_pipeline, X_cv, y,\n                              cv=5,\n                              scoring=make_scorer(calculate_root_mean_squared_log_error,\n                                                  greater_is_better=False))\n\nprint(\"Score:\\n\", scores)\nprint(\"Score mean: {}\".format(scores.mean()))\nprint(\"Score std: {}\".format(scores.std()))","11429e60":"# Fit model\nmy_pipeline.fit(X_cv, y)\n\n# Get predictions\npreds = my_pipeline.predict(X_sub)","65b17d56":"# Use predefined utility function\nsave_file(preds)","00b7def0":"# Preprocessing  <a id='preprocessing'><\/a>","b4973218":"# Introduction  <a id='introduction'><\/a>\n\nThis is a fast and simple starter code for those who want to work with sklearn pipelines. The reader may easily get a better score by applying a grid search, EDA, and feature engineering concepts. Please note that the score may change due to the indeterministic nature of the model.\n\n\nThank you for reading.\n\n\n# Table of Contents\n* [Introduction](#introduction)\n* [Helper Functions](#functions)\n* [Preprocessing](#preprocessing) \n* [Validation](#validation) \n* [Cross-validation using full training set](#cross-validation)    \n* [Prediction](#prediction) \n* [Saving and submission](#saving)  \n* [References](#references)","b736bd35":"<div class=\"alert alert-block alert-danger\">  \n<p>If we want to use early_stopping_rounds with our pipeline we cannot use the validation set (X_valid) directly. This is because sklearn pipelines do not process the eval_set used with early_stopping_rounds. As a result, we need to process our validation set before using early_stopping_rounds.<\/p>\n    \n<p>There is a great danger here. If we forget to process our validation set and if processed data has the same number of columns as the unprocessed data we may not see an error. Validation with unprocessed data may mislead us.<\/p>    \n    \n<p>In order to process the eval_set, we need to fit_transform X_valid by using our preprocessor which is basically a pipeline consists of transformers (does not have a predictor).<\/p>\n<\/div>","c16dc522":"# Saving and submission   <a id='saving'><\/a>","033f08ee":"# Validation with early_stopping_rounds  <a id='validation'><\/a>","47d18a94":"# Helper functions   <a id='functions'><\/a>   ","f37d9b86":"# References   <a id='references'><\/a>\n* [10-simple-hacks-to-speed-up-your-data-analysis - Parul Pandey](https:\/\/www.kaggle.com\/parulpandey\/10-simple-hacks-to-speed-up-your-data-analysis)\n* [Dataset Transformations - Scikit-learn](https:\/\/scikit-learn.org\/stable\/data_transforms.html)\n* [Intermediate Machine Learning Course - Pipelines](https:\/\/www.kaggle.com\/alexisbcook\/pipelines)\n* [Kaggle Learn](https:\/\/www.kaggle.com\/learn\/overview)","bc086c62":"<div class=\"alert alert-block alert-info\">\n<b>Tip:<\/b> We will use some helper functions throughout the notebook. Collecting them in one place is a good idea. It makes the code more organized.\n<\/div>","2ba68c50":"# Cross-validation using full training set <a id='cross-validation'><\/a>","80e01bc7":"# Prediction   <a id='prediction'><\/a>"}}