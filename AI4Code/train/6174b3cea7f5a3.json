{"cell_type":{"5ca453d6":"code","7e7d30d6":"code","ce6dfe68":"code","a5e77158":"code","a7362e48":"code","8df67211":"code","a4bb420c":"code","f035ca1c":"code","cda8a594":"code","d009fc17":"code","43c57c56":"code","b0ed7eb2":"code","488b56f9":"code","416828c1":"code","62162f6c":"markdown","b454fab4":"markdown","625a12f1":"markdown","15872629":"markdown","3731b916":"markdown","5abf9e56":"markdown","8e467e1c":"markdown","92e3064d":"markdown","64316f33":"markdown","846d2f96":"markdown","fdcb8d80":"markdown"},"source":{"5ca453d6":"import re\nimport tensorflow as tf\nimport numpy as np\nfrom matplotlib import pyplot as plt\nprint(\"Tensorflow version \" + tf.__version__)\nAUTO = tf.data.experimental.AUTOTUNE\nfrom kaggle_datasets import KaggleDatasets","7e7d30d6":"try: # detect TPUs\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver.connect() # TPU detection\n    strategy = tf.distribute.TPUStrategy(tpu)\nexcept ValueError: # detect GPUs\n    strategy = tf.distribute.get_strategy() # default strategy that works on CPU and single GPU\n    # strategy = tf.distribute.MirroredStrategy() # for GPU or multi-GPU machines\n    # strategy = tf.distribute.experimental.MultiWorkerMirroredStrategy() # for clusters of multi-GPU machines\n    \nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","ce6dfe68":"GCS_PATH = KaggleDatasets().get_gcs_path() # you can list the bucket with \"!gsutil ls $GCS_PATH\"","a5e77158":"EPOCHS = 12\nIMAGE_SIZE = [331, 331]\n\nFLOWERS_DATASETS = { # available image sizes\n    192: GCS_PATH + '\/tfrecords-jpeg-192x192\/*.tfrec',\n    224: GCS_PATH + '\/tfrecords-jpeg-224x224\/*.tfrec',\n    331: GCS_PATH + '\/tfrecords-jpeg-331x331\/*.tfrec',\n    512: GCS_PATH + '\/tfrecords-jpeg-512x512\/*.tfrec'\n}\nCLASSES = ['daisy', 'dandelion', 'roses', 'sunflowers', 'tulips'] # do not change, maps to the labels in the data (folder names)\nassert IMAGE_SIZE[0] == IMAGE_SIZE[1], \"only square images are supported\"\nassert IMAGE_SIZE[0] in FLOWERS_DATASETS, \"this image size is not supported\"\n\n# Learning rate schedule for TPU, GPU and CPU.\n# Using an LR ramp up because fine-tuning a pre-trained model.\n# Starting with a high LR would break the pre-trained weights.\n\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync # this is 8 on TPU v3-8, it is 1 on CPU and GPU\nLR_START = 0.00001\nLR_MAX = 0.00005 * strategy.num_replicas_in_sync\nLR_MIN = 0.00001\nLR_RAMPUP_EPOCHS = 5\nLR_SUSTAIN_EPOCHS = 0\nLR_EXP_DECAY = .8\n\ndef lrfn(epoch):\n    if epoch < LR_RAMPUP_EPOCHS:\n        lr = (LR_MAX - LR_START) \/ LR_RAMPUP_EPOCHS * epoch + LR_START\n    elif epoch < LR_RAMPUP_EPOCHS + LR_SUSTAIN_EPOCHS:\n        lr = LR_MAX\n    else:\n        lr = (LR_MAX - LR_MIN) * LR_EXP_DECAY**(epoch - LR_RAMPUP_EPOCHS - LR_SUSTAIN_EPOCHS) + LR_MIN\n    return lr\n    \nlr_callback = tf.keras.callbacks.LearningRateScheduler(lrfn, verbose=True)\n\nrng = [i for i in range(EPOCHS)]\ny = [lrfn(x) for x in rng]\nplt.plot(rng, y)\nprint(\"Learning rate schedule: {:.3g} to {:.3g} to {:.3g}\".format(y[0], max(y), y[-1]))","a7362e48":"def dataset_to_numpy_util(dataset, N):\n    dataset = dataset.unbatch().batch(N)\n    for images, labels in dataset:\n        numpy_images = images.numpy()\n        numpy_labels = labels.numpy()\n        break;  \n    return numpy_images, numpy_labels\n\ndef title_from_label_and_target(label, correct_label):\n    label = np.argmax(label, axis=-1)  # one-hot to class number\n    correct_label = np.argmax(correct_label, axis=-1) # one-hot to class number\n    correct = (label == correct_label)\n    return \"{} [{}{}{}]\".format(CLASSES[label], str(correct), ', shoud be ' if not correct else '',\n                                CLASSES[correct_label] if not correct else ''), correct\n\ndef display_one_flower(image, title, subplot, red=False):\n    plt.subplot(subplot)\n    plt.axis('off')\n    plt.imshow(image)\n    plt.title(title, fontsize=16, color='red' if red else 'black')\n    return subplot+1\n  \ndef display_9_images_from_dataset(dataset):\n    subplot=331\n    plt.figure(figsize=(13,13))\n    images, labels = dataset_to_numpy_util(dataset, 9)\n    for i, image in enumerate(images):\n        title = CLASSES[np.argmax(labels[i], axis=-1)]\n        subplot = display_one_flower(image, title, subplot)\n        if i >= 8:\n            break;\n              \n    plt.tight_layout()\n    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n    plt.show()  \n\ndef display_9_images_with_predictions(images, predictions, labels):\n    subplot=331\n    plt.figure(figsize=(13,13))\n    for i, image in enumerate(images):\n        title, correct = title_from_label_and_target(predictions[i], labels[i])\n        subplot = display_one_flower(image, title, subplot, not correct)\n        if i >= 8:\n            break;\n              \n    plt.tight_layout()\n    plt.subplots_adjust(wspace=0.1, hspace=0.1)\n    plt.show()\n    \ndef display_training_curves(training, validation, title, subplot):\n    if subplot%10==1: # set up the subplots on the first call\n        plt.subplots(figsize=(10,10), facecolor='#F0F0F0')\n        plt.tight_layout()\n    ax = plt.subplot(subplot)\n    ax.set_facecolor('#F8F8F8')\n    ax.plot(training)\n    ax.plot(validation)\n    ax.set_title('model '+ title)\n    ax.set_ylabel(title)\n    #ax.set_ylim(0.28,1.05)\n    ax.set_xlabel('epoch')\n    ax.legend(['train', 'valid.'])","8df67211":"def count_data_items(filenames):\n    # the number of data items is written in the name of the .tfrec files, i.e. flowers00-230.tfrec = 230 data items\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)\n\ngcs_pattern = FLOWERS_DATASETS[IMAGE_SIZE[0]]\nvalidation_split = 0.19\nfilenames = tf.io.gfile.glob(gcs_pattern)\nsplit = len(filenames) - int(len(filenames) * validation_split)\nTRAINING_FILENAMES = filenames[:split]\nVALIDATION_FILENAMES = filenames[split:]\nTRAIN_STEPS = count_data_items(TRAINING_FILENAMES) \/\/ BATCH_SIZE\nVALIDATION_STEPS = -(-count_data_items(VALIDATION_FILENAMES) \/\/ BATCH_SIZE) # The \"-(-\/\/)\" trick rounds up instead of down :-)\nprint(\"TRAINING IMAGES: \", count_data_items(TRAINING_FILENAMES), \", STEPS PER EPOCH: \", TRAIN_STEPS)\nprint(\"VALIDATION IMAGES: \", count_data_items(VALIDATION_FILENAMES))\n        \ndef read_tfrecord(example):\n    features = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"class\": tf.io.FixedLenFeature([], tf.int64),  # shape [] means scalar\n        \"one_hot_class\": tf.io.VarLenFeature(tf.float32),\n    }\n    example = tf.io.parse_single_example(example, features)\n    image = tf.image.decode_jpeg(example['image'], channels=3) # image format int8 [0,255]\n    class_label = tf.cast(example['class'], tf.int32)\n    one_hot_class = tf.sparse.to_dense(example['one_hot_class'])\n    one_hot_class = tf.reshape(one_hot_class, [5])\n    return image, one_hot_class\n    \ndef force_image_sizes(dataset, image_size):\n    # explicit size needed for TPU\n    reshape_images = lambda image, label: (tf.reshape(image, [*image_size, 3]), label)\n    dataset = dataset.map(reshape_images, num_parallel_calls=AUTO)\n    return dataset\n\ndef load_dataset(filenames):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n\n    ignore_order = tf.data.Options()\n    ignore_order.experimental_deterministic = False\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n    dataset = dataset.map(read_tfrecord, num_parallel_calls=AUTO)\n    dataset = force_image_sizes(dataset, IMAGE_SIZE)\n    return dataset\n\naugmentations = tf.keras.Sequential([\n    tf.keras.layers.experimental.preprocessing.RandomFlip(mode='horizontal'),\n    tf.keras.layers.experimental.preprocessing.RandomContrast(factor=0.8),\n    tf.keras.layers.experimental.preprocessing.RandomRotation(factor=0.1, dtype=tf.float32)\n])\n\ndef data_augment(image, one_hot_class):\n    # data augmentation. Thanks to the dataset.prefetch(AUTO) statement in the next function (below),\n    # this happens essentially for free on TPU. Data pipeline code is executed on the \"CPU\" part\n    # of the TPU while the TPU itself is computing gradients.\n    image = augmentations(image, training=True) # Keras preprocessing layers are only applied at training time. Forcing it here for visualization.\n    return image, one_hot_class   \n\ndef get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES)\n    dataset = dataset.repeat()\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.map(data_augment, num_parallel_calls=AUTO)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_validation_dataset():\n    dataset = load_dataset(VALIDATION_FILENAMES)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset","a4bb420c":"training_dataset = get_training_dataset()\nvalidation_dataset = get_validation_dataset()","f035ca1c":"display_9_images_from_dataset(training_dataset)","cda8a594":"def create_model():\n    img_adjust_layer = tf.keras.layers.Lambda(lambda data: tf.keras.applications.xception.preprocess_input(tf.cast(data, tf.float32)), input_shape=[*IMAGE_SIZE, 3])\n    pretrained_model = tf.keras.applications.Xception(include_top=False)\n    pretrained_model.trainable = True\n\n    model = tf.keras.Sequential([\n        img_adjust_layer,\n        pretrained_model,\n        tf.keras.layers.GlobalAveragePooling2D(),\n        #tf.keras.layers.Flatten(),\n        tf.keras.layers.Dense(5, activation='softmax')\n    ])\n\n    model.compile(\n        optimizer='adam',\n        loss = 'categorical_crossentropy',\n        metrics=['accuracy'],\n        steps_per_execution=8\n    )\n\n    return model","d009fc17":"with strategy.scope():\n    model = create_model()\nmodel.summary()","43c57c56":"history = model.fit(training_dataset, validation_data=validation_dataset, validation_steps=VALIDATION_STEPS,\n                    steps_per_epoch=TRAIN_STEPS, epochs=EPOCHS, callbacks=[lr_callback])\n\nfinal_accuracy = history.history[\"val_accuracy\"][-5:]\nprint(\"FINAL ACCURACY MEAN-5: \", np.mean(final_accuracy))","b0ed7eb2":"display_training_curves(history.history['accuracy'][1:], history.history['val_accuracy'][1:], 'accuracy', 211)\ndisplay_training_curves(history.history['loss'][1:], history.history['val_loss'][1:], 'loss', 212)","488b56f9":"# a couple of images to test predictions too\nsome_flowers, some_labels = dataset_to_numpy_util(validation_dataset, 160)","416828c1":"# randomize the input so that you can execute multiple times to change results\npermutation = np.random.permutation(8*20)\nsome_flowers, some_labels = (some_flowers[permutation], some_labels[permutation])\n\npredictions = model.predict(some_flowers, batch_size=16)\nevaluations = model.evaluate(some_flowers, some_labels, batch_size=16)\n  \nprint(np.array(CLASSES)[np.argmax(predictions, axis=-1)].tolist())\nprint('[val_loss, val_acc]', evaluations)\n\ndisplay_9_images_with_predictions(some_flowers, predictions, some_labels)","62162f6c":"# Kaggle dataset access\nTPUs read data directly from Google Cloud Storage (GCS). This Kaggle utility will copy the dataset to a GCS bucket co-located with the TPU. If you have multiple datasets attached to the notebook, you can pass the name of a specific dataset to the get_gcs_path function. The name of the dataset is the name of the directory it is mounted in. Use `!ls \/kaggle\/input\/` to list attached datasets.","b454fab4":"# Training","625a12f1":"# Configuration\nThe Flowers dataset is availabe in multiple image sizes. 331x331px is the default. 512x512px will OOM on GPU but works on TPU.","15872629":"# TPU or GPU detection","3731b916":"# Read images and labels from TFRecords","5abf9e56":"\n\n---\n\n\nauthor: Martin Gorner<br>\ntwitter: @martin_gorner\n\n\n---\n\n\nCopyright 2020 Google LLC\n\nLicensed under the Apache License, Version 2.0 (the \"License\");\nyou may not use this file except in compliance with the License.\nYou may obtain a copy of the License at\n\n    http:\/\/www.apache.org\/licenses\/LICENSE-2.0\n\nUnless required by applicable law or agreed to in writing, software\ndistributed under the License is distributed on an \"AS IS\" BASIS,\nWITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\nSee the License for the specific language governing permissions and\nlimitations under the License.\n\n\n---\n\n\nThis is not an official Google product but sample code provided for an educational purpose\n","8e467e1c":"## Visualization utilities\n\ndata -> pixels, nothing of much interest for the machine learning practitioner in this section.","92e3064d":"# Imports","64316f33":"# Model","846d2f96":"# Predictions","fdcb8d80":"# Training and validation datasets"}}