{"cell_type":{"715a9326":"code","61ce3b91":"code","0236a192":"code","b208fc9b":"code","dd45db98":"code","6eac65a5":"code","7873e977":"code","f49f875b":"code","43f21691":"code","e50cb52b":"code","c37dc776":"code","95a79070":"code","0241a8e3":"code","9d717293":"code","8338f3b2":"code","2a84906b":"code","65b04428":"code","eb0b8960":"code","cb1cfdcb":"code","3877eccb":"code","e8dbba97":"code","e0b6e67e":"code","fbc1b28d":"code","db5ec7b3":"code","96ef22c1":"code","7be2f1cf":"code","8f7170e4":"code","50121f93":"code","888fe322":"code","e897fc71":"code","6c00c84d":"code","0c3bbd28":"code","592932a0":"code","2c01cebf":"code","a20ed184":"code","8ebb9eda":"code","476c613a":"code","b2921976":"code","8cb14652":"code","a8def74a":"code","3d5de959":"code","96df5617":"code","3fe976e5":"code","0abfe365":"code","cbc2ef56":"code","bd66706d":"code","4bdcffc2":"code","5c2c97c7":"code","a2a53167":"code","fe0fea0c":"code","ace822d4":"code","45e58135":"code","74db51cd":"code","881a8941":"code","cfda6acc":"code","617011c3":"code","55b19bf7":"code","97fba963":"code","135ad3d6":"code","c561d210":"code","d55bebdf":"code","35b9a398":"code","6e02573f":"code","4936d253":"code","047b8ce0":"code","9fd163a5":"code","cbd0e1c0":"code","f2ec9eab":"code","296f719c":"code","b9d44453":"code","2d8a933e":"code","b0d01884":"code","d421be3b":"code","c4786097":"code","c6d8be92":"code","e6feb04b":"code","a2216df8":"code","2a95f2ef":"code","a1c35923":"code","9936b7f2":"code","078f0e52":"code","2d32859b":"code","42dc2de0":"code","c824a820":"code","6918c876":"code","43141f20":"code","026fb390":"code","78ffcf1f":"code","597d5d4e":"code","4a21e10d":"code","b284e902":"code","871e5d10":"code","b1db80f4":"code","5c8e40ed":"code","80b38ba8":"code","c085f3ad":"code","8e4b0ace":"code","03454546":"code","55521658":"code","33344890":"code","4c30664b":"code","61aea290":"code","ac6a0760":"code","09c6bd37":"code","d892a931":"code","860dd814":"code","fdabd236":"code","eb725355":"code","fc154393":"code","01370c55":"code","32cebcb1":"code","6848e68e":"code","9d580d1f":"code","47dfa0ea":"code","5c4e2eb7":"code","a6bc93cb":"code","3be3c708":"code","9fcf0c75":"code","4df41911":"code","742b2a16":"code","8ac9716f":"code","76017b99":"code","349eefe5":"code","fb6f072c":"code","0e47a5f5":"code","56c42cfb":"code","5a4e9a64":"code","54b506c6":"code","e0d782b8":"code","83a1bde8":"code","7810b2c6":"code","dec2e11a":"code","4315dbd7":"code","612d4f8a":"code","a7a77d0c":"code","d696be46":"code","2a5f376b":"code","6f5137df":"code","2e8f3b2e":"code","c1bfa76e":"code","5d85fc49":"code","0b8b35ec":"code","9f4d001b":"code","515f9cfd":"code","f1ef27dc":"code","c792e37f":"code","a08c1191":"code","da999a03":"code","f21c3606":"code","6c366ffb":"code","4520de73":"code","4d85a352":"code","421169db":"code","0ecdf328":"code","c364773d":"code","3f1f2d98":"code","d6190587":"code","838f4933":"code","75405005":"code","e4ba8f18":"code","1c63270a":"code","59b2e516":"markdown","5d6581f6":"markdown","d8900b46":"markdown","1a25a1e5":"markdown","e9738722":"markdown","3e5885f1":"markdown","6fee1957":"markdown","4b1e9f6d":"markdown","49a79e4e":"markdown","c54677a0":"markdown","43ada07d":"markdown","21b9344a":"markdown","3d9ad662":"markdown","307401c8":"markdown","4a92aeea":"markdown","b91059ed":"markdown","90b92854":"markdown","40a11956":"markdown","45dc4b80":"markdown","9d63a9e1":"markdown","5aa40ed9":"markdown","5507558f":"markdown","4e2f3e2a":"markdown","b02798d0":"markdown","90fa0ffe":"markdown","a9971d4b":"markdown","ceff1eac":"markdown","39bc16a1":"markdown","0eb26493":"markdown","f9d85c7d":"markdown","1d504b59":"markdown","70006b45":"markdown","33b3cce7":"markdown","eec43ae6":"markdown","b3424989":"markdown","5aa6e6fe":"markdown","f7ee5bf6":"markdown","449cdd06":"markdown","227e3cae":"markdown","c671e960":"markdown","536e1cf3":"markdown","53c2dff6":"markdown","e95fce1c":"markdown","4e6908d3":"markdown","5151295e":"markdown","8144e549":"markdown","0ddbfa77":"markdown","2dad2c61":"markdown","d976fd08":"markdown","143a056e":"markdown","d004d807":"markdown","f4d03060":"markdown","2c52c840":"markdown","49083ce5":"markdown","a806ca17":"markdown","203f5f8e":"markdown","fd7c0080":"markdown","fd3a1ba8":"markdown","2e0b4d64":"markdown","8c09375d":"markdown","b95173f0":"markdown","b613852e":"markdown","70834739":"markdown","a22b7ff5":"markdown","3d162fd3":"markdown","a2c88c98":"markdown","afb27f26":"markdown","bf22f30d":"markdown","3a7e8514":"markdown","bdb06bda":"markdown","17811f61":"markdown","150fa202":"markdown","92dd6f31":"markdown","106b9988":"markdown","eb9c27af":"markdown","2844f26e":"markdown","92d6f3bf":"markdown","18cf694d":"markdown","f264a4da":"markdown","63deafbb":"markdown","3136d8b8":"markdown","ab6b82a3":"markdown","8ae21d36":"markdown","efda5fd9":"markdown","973b4f9a":"markdown","81eb03a4":"markdown","14c9fecb":"markdown","21b51cf3":"markdown","27c87806":"markdown","a07d1002":"markdown","3ce542aa":"markdown","ffe08eee":"markdown","254f8647":"markdown","fe9631bb":"markdown","c707dde3":"markdown","e97d2035":"markdown","d2c9a0dc":"markdown","1c8a075d":"markdown","bdec0ec2":"markdown","c9d122e9":"markdown","ff5b6b24":"markdown","82128f16":"markdown","a6923dcd":"markdown","c5a123b9":"markdown","5ed7f2e1":"markdown"},"source":{"715a9326":"import pandas as pd\nimport numpy as np\nimport warnings\n\nfrom scipy import special \nimport matplotlib.pyplot as plt\nimport matplotlib.animation as animation\nimport seaborn as sns\nimport math\nfrom IPython.display import Markdown, display ,HTML\n\n\nfrom sklearn.model_selection import train_test_split\n\n\nsns.set(style=\"whitegrid\")\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_colwidth', -1) # make sure data and columns are displayed correctly withput purge\npd.options.display.float_format = '{:20,.2f}'.format # display float value with correct precision \n\n\nwarnings.filterwarnings('ignore')","61ce3b91":"def log(string):\n    display(Markdown(\"> <span style='color:blue'>\"+string+\"<\/span>\"))\n\ndef header(string):\n    display(Markdown(\"------ \"))\n    display(Markdown(\"### \"+string))\n    \ndef header_red(string):\n    display(Markdown(\"> <span style='color:red'>\"+string))   \n\ndef get_variable_type(element) :\n    \"\"\"\n     Check is columns are of Contineous or Categorical variable.\n     Assumption is that if \n                 unique count < 20 then categorical \n                 unique count >= 20 and dtype = [int64 or float64] then contineous\n     \"\"\"\n    if element==0:\n        return \"Not Known\"\n    elif element < 20 and element!=0 :\n        return \"Categorical\"\n    elif element >= 20 and element!=0 :\n        return \"Contineous\" \n    \ndef get_meta_data(dataframe) :\n    \"\"\"\n     Method to get Meta-Data about any dataframe passed \n    \"\"\"\n    metadata_matrix = pd.DataFrame({\n                    'Datatype' : dataframe.dtypes.astype(str), \n                    'Non_Null_Count': dataframe.count(axis = 0).astype(int), \n                    'Null_Count': dataframe.isnull().sum().astype(int), \n                    'Null_Percentage': dataframe.isnull().sum()\/len(dataframe) * 100, \n                    'Unique_Values_Count': dataframe.nunique().astype(int) \n                     })\n    \n    metadata_matrix = predict_variable_type(metadata_matrix)\n    return metadata_matrix\n        \ndef display_columns_with_1_unique_value(dataframe):\n    unique_values_count_1 = dataframe[dataframe[\"Unique_Values_Count\"] == 1]\n    drop_value_col = unique_values_count_1.index.tolist()\n    lenght = len(drop_value_col)\n    header(\"Columns with only one unique value : \"+str(lenght))\n    if lenght == 0 :\n        header_red(\"No columns with only one unique values.\")  \n    else :    \n        log(\"Columns with only one unique value :\")\n        for index,item in enumerate(drop_value_col) :\n            print(index,\".\",item)\n            \ndef predict_variable_type(metadata_matrix):\n    metadata_matrix[\"Variable_Type\"] = metadata_matrix[\"Unique_Values_Count\"].apply(get_variable_type).astype(str)\n    metadata_matrix[\"frequency\"] = metadata_matrix[\"Null_Count\"] - metadata_matrix[\"Null_Count\"]\n    metadata_matrix[\"frequency\"].astype(int)\n    return metadata_matrix \n\n\ndef list_potential_categorical_type(dataframe,main) :\n    header(\"Stats for potential Categorical datatype columns\")\n    metadata_matrix_categorical = dataframe[dataframe[\"Variable_Type\"] == \"Categorical\"]\n    # TO DO *** Add check to skip below if there is no Categorical values \n    length = len(metadata_matrix_categorical)\n    if length == 0 :\n        header_red(\"No Categorical columns in given dataset.\")  \n    else :    \n        metadata_matrix_categorical = metadata_matrix_categorical.filter([\"Datatype\",\"Unique_Values_Count\"])\n        metadata_matrix_categorical.sort_values([\"Unique_Values_Count\"], axis=0,ascending=False, inplace=True)\n        col_to_check = metadata_matrix_categorical.index.tolist()\n        name_list = []\n        values_list = []\n        for name in col_to_check :\n            name_list.append(name)\n            values_list.append(main[name].unique())\n        temp = pd.DataFrame({\"index\":name_list,\"Unique_Values\":values_list})\n        metadata_matrix_categorical = metadata_matrix_categorical.reset_index()\n        metadata_matrix_categorical = pd.merge(metadata_matrix_categorical,temp,how='inner',on='index')\n        display(metadata_matrix_categorical.set_index(\"index\")) \n\ndef plot_data_type_pie_chat(dataframe) : \n        header(\"Stats for Datatype Percentage Distribution\")\n        dataframe_group = dataframe.groupby(\"Datatype\").frequency.count().reset_index()\n        dataframe_group.sort_values([\"Datatype\"], axis=0,ascending=False, inplace=True)\n        trace = go.Pie(labels=dataframe_group[\"Datatype\"].tolist(), values=dataframe_group[\"frequency\"].tolist())\n        layout = go.Layout(title=\"Datatype Percentage Distribution\")\n        fig = go.Figure(data=[trace], layout=layout)    \n        py.offline.iplot(fig)\n    \ndef get_potential_categorical_type(dataframe,main,unique_count) :\n    metadata_matrix_categorical = dataframe[dataframe[\"Variable_Type\"] == \"Categorical\"]\n    metadata_matrix_categorical = dataframe[dataframe[\"Unique_Values_Count\"] == unique_count]\n    length = len(metadata_matrix_categorical)\n    if length == 0 :\n        header_red(\"No Categorical columns in given dataset.\")  \n    else :    \n        metadata_matrix_categorical = metadata_matrix_categorical.filter([\"Datatype\",\"Unique_Values_Count\"])\n        metadata_matrix_categorical.sort_values([\"Unique_Values_Count\"], axis=0,ascending=False, inplace=True)\n        col_to_check = metadata_matrix_categorical.index.tolist()\n        name_list = []\n        values_list = []\n        for name in col_to_check :\n            name_list.append(name)\n            values_list.append(main[name].unique())\n        temp = pd.DataFrame({\"index\":name_list,\"Unique_Values\":values_list})\n        metadata_matrix_categorical = metadata_matrix_categorical.reset_index()\n        metadata_matrix_categorical = pd.merge(metadata_matrix_categorical,temp,how='inner',on='index')\n        display(metadata_matrix_categorical.set_index(\"index\")) \n           \ndef color_red(val):\n    \"\"\"\n    Takes a scalar and returns a string with\n    the css property `'color: red'` for value \n    greater than 10 , black otherwise.\n    \"\"\"\n    color = 'red' if val > 5 else 'black'\n    return 'color: %s' % color\n\ndef heatmap(x,y,dataframe):\n    plt.figure(figsize=(x,y))\n    sns.heatmap(dataframe.corr(),cmap=\"OrRd\",annot=True)\n    plt.show()","0236a192":"telecom_df = pd.read_csv(\"..\/input\/telecom_churn_data.csv\")\ntelecom_df.head(3)","b208fc9b":"telecom_df.shape","dd45db98":"# Let us first extract list of columns containing recharge amount\namt_recharge_columns =  telecom_df.columns[telecom_df.columns.str.contains('rech_amt|rech_data')]\nprint(amt_recharge_columns)","6eac65a5":"# Checking missing values percentages\ntelecom_df_null_check = 100*telecom_df.loc[:,amt_recharge_columns].isnull().sum()\/len(telecom_df)\ndf = pd.DataFrame(telecom_df_null_check)\ndf.rename(columns={0:'Null_Percentage'}, inplace=True)\ndf = pd.DataFrame(df.Null_Percentage).style.applymap(color_red)\ndisplay(df)","7873e977":"telecom_df.loc[:,amt_recharge_columns].describe()","f49f875b":"telecom_df['total_rech_data_6'] = telecom_df['total_rech_data_6'].replace(np.NaN,0.0)\ntelecom_df['total_rech_data_7'] = telecom_df['total_rech_data_7'].replace(np.NaN,0.0)\ntelecom_df['total_rech_data_8'] = telecom_df['total_rech_data_8'].replace(np.NaN,0.0)","43f21691":"telecom_df['av_rech_amt_data_6'] = telecom_df['av_rech_amt_data_6'].replace(np.NaN,0.0)\ntelecom_df['av_rech_amt_data_7'] = telecom_df['av_rech_amt_data_7'].replace(np.NaN,0.0)\ntelecom_df['av_rech_amt_data_8'] = telecom_df['av_rech_amt_data_8'].replace(np.NaN,0.0)","e50cb52b":"# let's adding new column total recharge amount for data: total_rech_amt_data for calculating High Value customer process\ntelecom_df['total_rech_amt_data_6'] = telecom_df.av_rech_amt_data_6 * telecom_df.total_rech_data_6\ntelecom_df['total_rech_amt_data_7'] = telecom_df.av_rech_amt_data_7 * telecom_df.total_rech_data_7\ntelecom_df['total_rech_amt_data_8'] = telecom_df.av_rech_amt_data_8 * telecom_df.total_rech_data_8","c37dc776":"telecom_df['total_avg_rech_amnt_6_7_GPhase'] = (telecom_df.total_rech_amt_6 + telecom_df.total_rech_amt_data_6 \\\n                                               + telecom_df.total_rech_amt_7+ telecom_df.total_rech_amt_data_7)\/2","95a79070":"# create a filter for values greater than 70th percentile of total average recharge amount for good phase \nhigh_value_filter = telecom_df.total_avg_rech_amnt_6_7_GPhase.quantile(0.7)\n\nlog('70 percentile of 6th and 7th months avg recharge amount: '+str(high_value_filter))\n\ntelecom_df_high_val_cust = telecom_df[telecom_df.total_avg_rech_amnt_6_7_GPhase > high_value_filter]\nlog('Dataframe Shape after Filtering High Value Customers: ' + str(telecom_df_high_val_cust.shape))","0241a8e3":"high_val_cust_9 = ['total_ic_mou_9', 'total_og_mou_9', 'vol_2g_mb_9','vol_3g_mb_9']","9d717293":"# Checking the NA\/Null values are present or Not\ndf = 100*telecom_df_high_val_cust.loc[:,high_val_cust_9].isnull().sum()\/len(telecom_df_high_val_cust)\ndf = pd.DataFrame(df)\ndf.rename(columns={0:'Null_Percentage'}, inplace=True)\ndf = pd.DataFrame(df.Null_Percentage).style.applymap(color_red)\ndisplay(df)","8338f3b2":"# Initially set all the values as 0\ntelecom_df_high_val_cust['churn']= 0","2a84906b":"#is_churned boolean flag for customers where above values are zero\nis_churned = (telecom_df_high_val_cust.total_ic_mou_9 == 0) & \\\n             (telecom_df_high_val_cust.total_og_mou_9 == 0) & \\\n             (telecom_df_high_val_cust.vol_2g_mb_9 ==0) & \\\n             (telecom_df_high_val_cust.vol_3g_mb_9 ==0)","65b04428":"# set all which having is_churned True condition as 1\ntelecom_df_high_val_cust.loc[is_churned,'churn']=1","eb0b8960":"# let us check what's the % of churned customers\n100*telecom_df_high_val_cust.churn.sum()\/len(telecom_df_high_val_cust)","cb1cfdcb":"churn_month_columns =  telecom_df_high_val_cust.columns[telecom_df_high_val_cust.columns.str.contains('_9')]","3877eccb":"# drop all columns corresponding to the churn phase\ntelecom_df_high_val_cust.drop(churn_month_columns,axis=1,inplace=True)","e8dbba97":"list_potential_categorical_type(get_meta_data(telecom_df_high_val_cust),telecom_df_high_val_cust)","e0b6e67e":"drop_col_with_unique_col =['circle_id', 'loc_og_t2o_mou', 'std_og_t2o_mou', 'loc_ic_t2o_mou', \n                          'last_date_of_month_6', 'last_date_of_month_7', 'last_date_of_month_8', \n                          'std_og_t2c_mou_6', 'std_og_t2c_mou_7', \n                          'std_og_t2c_mou_8',  'std_ic_t2o_mou_6', \n                          'std_ic_t2o_mou_7', 'std_ic_t2o_mou_8']","fbc1b28d":"log(\"Features before dropping unique values : \"+ str(telecom_df_high_val_cust.shape[1]))\ntelecom_df_high_val_cust.drop(drop_col_with_unique_col,axis=1,inplace=True)\nlog(\"Features after dropping unique values : \"+ str(telecom_df_high_val_cust.shape[1]))","db5ec7b3":"meta_df = get_meta_data(telecom_df_high_val_cust)","96ef22c1":"meta_df_count = meta_df[meta_df[\"Datatype\"] == 'object']\ndate_col = meta_df_count.index.tolist()\ndate_col","7be2f1cf":"telecom_df_high_val_cust[date_col].head(5)","8f7170e4":"for col in date_col:\n    telecom_df_high_val_cust[col] = pd.to_datetime(telecom_df_high_val_cust[col])","50121f93":"telecom_df_high_val_cust[date_col].head(5)","888fe322":"telecom_df_high_val_cust.head(8)","e897fc71":"list_potential_categorical_type(get_meta_data(telecom_df_high_val_cust),telecom_df_high_val_cust)","6c00c84d":"# create box plot for  6th, 7th and 8th month\ndef plot_box_chart(attribute):\n    plt.figure(figsize=(20,16))\n    df = telecom_df_high_val_cust\n    plt.subplot(2,3,1)\n    sns.boxplot(data=df, y=attribute+\"_6\",x=\"churn\",hue=\"churn\",\n                showfliers=False,palette=(\"plasma\"))\n    plt.subplot(2,3,2)\n    sns.boxplot(data=df, y=attribute+\"_7\",x=\"churn\",hue=\"churn\",\n                showfliers=False,palette=(\"plasma\"))\n    plt.subplot(2,3,3)\n    sns.boxplot(data=df, y=attribute+\"_8\",x=\"churn\",hue=\"churn\",\n                showfliers=False,palette=(\"plasma\"))\n    plt.show()","0c3bbd28":"recharge_amnt_columns =  telecom_df_high_val_cust.columns[telecom_df_high_val_cust.columns.str.contains('rech_amt')]\nrecharge_amnt_columns.tolist()","592932a0":"# Ploting for total recharge amount:\nplot_box_chart('total_rech_amt')","2c01cebf":"# Ploting for total recharge amount for data:\nplot_box_chart('total_rech_amt_data')","a20ed184":"# Ploting for maximum recharge amount for data:\nplot_box_chart('max_rech_amt')","8ebb9eda":"# Let's check other recharge related variables:\nother_recharge = telecom_df_high_val_cust.columns[telecom_df_high_val_cust.columns.str.contains('rech_num|max_rech_data',regex=True)]\n","476c613a":"# lets check the % of NA values for other recharge related variables\n100*telecom_df_high_val_cust.loc[:,other_recharge].isnull().sum()\/len(telecom_df_high_val_cust)","b2921976":"telecom_df_high_val_cust.loc[:,['max_rech_data_6','max_rech_data_7','max_rech_data_8']].describe()","8cb14652":"telecom_df_high_val_cust.loc[:,['max_rech_data_6','max_rech_data_7','max_rech_data_8']] \\\n= telecom_df_high_val_cust.loc[:,['max_rech_data_6','max_rech_data_7','max_rech_data_8']].replace(np.nan,0)","a8def74a":"# Ploting for Total recharge for Number:\nplot_box_chart('total_rech_num')","3d5de959":"# Ploting for maximum recharge for data:\nplot_box_chart('max_rech_data')","96df5617":"#Getting the last day recharge amount columns\nlast_day_rech_amt = telecom_df_high_val_cust.columns[telecom_df_high_val_cust.columns.str.contains('day')]","3fe976e5":"last_day_rech_amt.tolist()","0abfe365":"# Ploting for last day recharge amount:\nplot_box_chart('last_day_rch_amt')","cbc2ef56":"usage_2g_and_3g = telecom_df_high_val_cust.columns[telecom_df_high_val_cust.columns.str.contains('2g|3g',regex=True)]","bd66706d":"#let us check the % of NA values present\npercentage_3g_2g_null_check = 100*telecom_df_high_val_cust.loc[:,usage_2g_and_3g].isnull().sum()\/len(telecom_df_high_val_cust.loc[:,usage_2g_and_3g])\ndf = pd.DataFrame(percentage_3g_2g_null_check)\ndf.rename(columns={0:'Null_Percentage'}, inplace=True)\ndf = pd.DataFrame(df.Null_Percentage).style.applymap(color_red)\ndisplay(df)","4bdcffc2":"telecom_df_high_val_cust.drop(['count_rech_2g_6','count_rech_2g_7',\n              'count_rech_2g_8','count_rech_3g_6','count_rech_3g_7','count_rech_3g_8'\n               ,'arpu_3g_6','arpu_3g_7','arpu_3g_8','arpu_2g_6','arpu_2g_7','arpu_2g_8'],axis=1,inplace=True)","5c2c97c7":"# Ploting for volume of 2G and 3G usage columns:\nplot_box_chart('vol_2g_mb')","a2a53167":"plot_box_chart('vol_3g_mb')","fe0fea0c":"# let's check monthly 2G\/3G service schemes\nmonthly_subcription_2g_3g = telecom_df_high_val_cust.columns[telecom_df_high_val_cust.columns.str.contains('monthly_2g|monthly_3g',regex=True)]\nmonthly_subcription_2g_3g.tolist()","ace822d4":"#Checking Null value\n#100*telecom_df_high_val_cust.loc[:,monthly_subcription_2g_3g].isnull().sum()\/len(telecom_df_high_val_cust.loc[:,monthly_subcription_2g_3g])","45e58135":"telecom_df_high_val_cust[monthly_subcription_2g_3g].info()","74db51cd":"# Ploting for monthly subscription :\nplot_box_chart('monthly_2g')\n","881a8941":"def plot_mean_bar_chart(df,columns_list):\n    df_0 = df[df.churn==0].filter(columns_list)\n    df_1 = df[df.churn==1].filter(columns_list)\n\n    mean_df_0 = pd.DataFrame([df_0.mean()],index={'Non Churn'})\n    mean_df_1 = pd.DataFrame([df_1.mean()],index={'Churn'})\n\n    frames = [mean_df_0, mean_df_1]\n    mean_bar = pd.concat(frames)\n\n    mean_bar.T.plot.bar(figsize=(10,5),rot=0)\n    plt.show()\n    \n    return mean_bar","cfda6acc":"plot_mean_bar_chart(telecom_df_high_val_cust, monthly_subcription_2g_3g)\n","617011c3":"# let's check Volume based cost \nvbc_column = telecom_df_high_val_cust.columns[telecom_df_high_val_cust.columns.str.contains('vbc_',regex=True)]\nvbc_column.tolist()","55b19bf7":"# Renaming month named vbc columns to 6,7,8,9 format\ntelecom_df_high_val_cust.rename(columns={'jun_vbc_3g':'vbc_3g_6','jul_vbc_3g':'vbc_3g_7','aug_vbc_3g':'vbc_3g_8'\n                         ,'sep_vbc_3g':'vbc_3g_9'}, inplace=True)\n\n# drop 9th month column as it is not needed\ntelecom_df_high_val_cust.drop('vbc_3g_9',axis=1,inplace=True)","97fba963":"vbc_column = telecom_df_high_val_cust.columns[telecom_df_high_val_cust.columns.str.contains('vbc_3g',regex=True)]\nvbc_column.tolist()","135ad3d6":"# Ploting for volume based cost :\nplot_box_chart('vbc_3g')","c561d210":"plot_mean_bar_chart(telecom_df_high_val_cust, vbc_column)\n","d55bebdf":"# Checking Service schemes with validity smaller than a month for 2G\/3G\nSC_2g_or_3g_col = telecom_df_high_val_cust.columns[telecom_df_high_val_cust.columns.str.contains('sachet_2g|sachet_3g',regex=True)]\n","35b9a398":"plot_mean_bar_chart(telecom_df_high_val_cust, SC_2g_or_3g_col)","6e02573f":"# Checking columns for average revenue per user\narpu_cols = telecom_df_high_val_cust.columns[telecom_df_high_val_cust.columns.str.contains('arpu_')]\n\n# Plotting arpu\nplot_box_chart('arpu')","4936d253":"plot_mean_bar_chart(telecom_df_high_val_cust, arpu_cols)","047b8ce0":"mou_cols = telecom_df_high_val_cust.columns[telecom_df_high_val_cust.columns.str.contains('mou')]","9fd163a5":"mou_cols","cbd0e1c0":"meta_df = get_meta_data(telecom_df_high_val_cust[mou_cols])\nmeta_df.sort_values([\"Null_Percentage\"], axis=0,ascending=False, inplace=True)\ncol_to_display = ['Null_Percentage']\nmeta_df[col_to_display]","f2ec9eab":"# replaceing null values by 0 for minutes of usage variables\ntelecom_df_high_val_cust.loc[:,mou_cols] = telecom_df_high_val_cust.loc[:,mou_cols].replace(np.NaN,0)","296f719c":"mou_og_cols6 = telecom_df_high_val_cust.columns[telecom_df_high_val_cust.columns.str.contains('.*_og_.*mou_6',regex=True)]","b9d44453":"telecom_df_high_val_cust.loc[:,mou_og_cols6].describe()","2d8a933e":"heatmap(18,12,telecom_df_high_val_cust.filter(mou_og_cols6))","b0d01884":"print(telecom_df_high_val_cust.loc[8,'total_og_mou_6'],\"==\",telecom_df_high_val_cust.loc[8,['loc_og_mou_6','std_og_mou_6','spl_og_mou_6','isd_og_mou_6','og_others_6']].sum())\nprint(telecom_df_high_val_cust.loc[8,'std_og_mou_6'],\"==\",telecom_df_high_val_cust.loc[8,['std_og_t2m_mou_6','std_og_t2t_mou_6','std_og_t2f_mou_6','std_og_loc_mou_6']].sum())\nprint(telecom_df_high_val_cust.loc[8,'loc_og_mou_6'],\"==\",telecom_df_high_val_cust.loc[8,['loc_og_t2m_mou_6','loc_og_t2t_mou_6','loc_og_t2f_mou_6','loc_og_2tc_mou_6']].sum())","d421be3b":"list_total_og_cols = telecom_df_high_val_cust.columns[telecom_df_high_val_cust.columns.str.contains('total_og_mou|std_og_mou|loc_og_mou',regex=True)]\ntelecom_df_high_val_cust.drop(list_total_og_cols,axis=1,inplace=True)\nlog(\"Columns dropped - \")\nlist_total_og_cols.tolist()","c4786097":"mou_ic_cols6 = telecom_df_high_val_cust.columns[telecom_df_high_val_cust.columns.str.contains('.*_ic_.*mou_6',regex=True)]","c6d8be92":"telecom_df_high_val_cust.loc[:,mou_ic_cols6].describe()","e6feb04b":"heatmap(18,12,telecom_df_high_val_cust.filter(mou_ic_cols6))","a2216df8":"print(telecom_df_high_val_cust.loc[21,'total_ic_mou_6'],\"==\",telecom_df_high_val_cust.loc[21,['loc_ic_mou_6','roam_ic_mou_6','isd_ic_mou_6','spl_ic_mou_6','std_ic_mou_6']].sum())\nprint(telecom_df_high_val_cust.loc[21,'loc_ic_mou_6'],\"==\",telecom_df_high_val_cust.loc[21,['loc_ic_t2t_mou_6','loc_ic_t2m_mou_6','loc_ic_t2f_mou_6','loc_ic_t2c_mou_6']].sum())\nprint(telecom_df_high_val_cust.loc[21,'std_ic_mou_6'],\"==\",telecom_df_high_val_cust.loc[21,['std_ic_t2t_mou_6','std_ic_t2m_mou_6','std_ic_t2f_mou_6','std_ic_t2c_mou_6']].sum())","2a95f2ef":"list_total_ic_cols = telecom_df_high_val_cust.columns[telecom_df_high_val_cust.columns.str.contains('total_ic_mou|std_ic_mou|loc_ic_mou',regex=True)]\ntelecom_df_high_val_cust.drop(list_total_ic_cols,axis=1,inplace=True)\nlog(\"Columns dropped - \")\nlist_total_ic_cols.tolist()","a1c35923":"offnet_usage_service_col = telecom_df_high_val_cust.columns[telecom_df_high_val_cust.columns.str.contains('offnet.*mou',regex=True)]","9936b7f2":"# Offnet mou values for churned and non churned customers\nplot_box_chart('offnet_mou')","078f0e52":"plot_mean_bar_chart(telecom_df_high_val_cust, offnet_usage_service_col)","2d32859b":"#minutes of usage related columns\nonnet_usage_service =  telecom_df_high_val_cust.columns[telecom_df_high_val_cust.columns.str.contains('^onnet.*mou',regex=True)]","42dc2de0":"onnet_usage_service","c824a820":"# Plotting offnet\nplot_box_chart('onnet_mou')","6918c876":"plot_mean_bar_chart(telecom_df_high_val_cust, onnet_usage_service)","43141f20":"tenure_data = telecom_df_high_val_cust.copy()","026fb390":"plt.figure(figsize=(14,8))\n# aon --> Age on network - number of days the customer is using the operator T network\ntenure_data['tenure'] = tenure_data['aon']\/30\ntenure_data['tenure'].head()\nax = sns.distplot(tenure_data['tenure'], hist=True, kde=False, \n             bins=int(180\/5), color = 'darkgreen', \n             hist_kws={'edgecolor':'red'},\n             kde_kws={'linewidth': 4})\nax.set_ylabel('Number of Customers')\nax.set_xlabel('Tenure in Months')\nax.set_title('Customers Vs Tenure')\nplt.show()","78ffcf1f":"tn_range = [0, 6, 12, 24, 60, 61]\ntn_label = [ '0-6 Months', '6-12 Months', '1-2 Yrs', '2-5 Yrs', '5 Yrs and above']\ntenure_data['tenure_range'] = pd.cut(tenure_data['tenure'], tn_range, labels=tn_label)\ntenure_data['tenure_range'].head()","597d5d4e":"plt.figure(figsize=(14,8))\nsns.countplot(x = 'tenure_range', hue = 'churn',data = tenure_data,palette=(\"plasma\"))\nplt.show()","4a21e10d":"meta_df = get_meta_data(telecom_df_high_val_cust)\nmeta_df = meta_df[meta_df[\"Null_Percentage\"] > 30]\nmeta_df.sort_values([\"Null_Percentage\"], axis=0,ascending=False, inplace=True)\ncol_to_display = ['Null_Percentage']\nmeta_df[col_to_display]","b284e902":"#Dropping 30% column\nis_more_30 = meta_df.index\ntelecom_df_high_val_cust.drop(telecom_df_high_val_cust[is_more_30],axis=1,inplace=True)","871e5d10":"telecom_df_high_val_cust.shape","b1db80f4":"meta_df = get_meta_data(telecom_df_high_val_cust)\nmeta_df = meta_df[meta_df[\"Null_Percentage\"] > 0]\nmeta_df.sort_values([\"Null_Percentage\"], axis=0,ascending=False, inplace=True)\ncol_to_display = ['Null_Percentage']\nmeta_df[col_to_display]","5c8e40ed":"telecom_df_high_val_cust[meta_df.index].head(3)","80b38ba8":"numberic_col = ['og_others_8', 'ic_others_8', 'og_others_6','ic_others_6', 'og_others_7', 'ic_others_7']","c085f3ad":"for i in numberic_col:\n    telecom_df_high_val_cust.loc[telecom_df_high_val_cust[i].isnull(),i]=0","8e4b0ace":"telecom_df_high_val_cust[meta_df.index].head(3)","03454546":"meta_df = get_meta_data(telecom_df_high_val_cust)\nmeta_df = meta_df[meta_df[\"Null_Percentage\"] > 0]\nmeta_df.sort_values([\"Null_Percentage\"], axis=0,ascending=False, inplace=True)\ncol_to_display = ['Null_Percentage']\nmeta_df[col_to_display]","55521658":"telecom_df_final = telecom_df_high_val_cust.copy()\nprint(telecom_df_final.shape)\ntelecom_df_final.head(3)","33344890":"telecom_df_final.drop([\"total_avg_rech_amnt_6_7_GPhase\"],axis=1,inplace=True)\ntelecom_df_final.drop(telecom_df_high_val_cust.filter(regex='date_').columns,axis=1,inplace=True)\nprint (telecom_df_final.shape)\n\ncol_list = telecom_df_final.filter(regex='_6|_7').columns.str[:-2]\nfor idx, col in enumerate(col_list.unique()):\n    print(col)\n    avg_col_name = \"avg_\"+col+\"_av67\"\n    col_6 = col+\"_6\"\n    col_7 = col+\"_7\"\n    telecom_df_final[avg_col_name] = (telecom_df_final[col_6]  + telecom_df_final[col_7])\/ 2\n","4c30664b":"col_list_to_drop = telecom_df_final.filter(regex='_6|_7')\ntelecom_df_final.drop(col_list_to_drop,axis=1,inplace=True)\nprint (telecom_df_final.shape)","61aea290":"telecom_df_final.head(3)","ac6a0760":"telecom_pca_df = telecom_df_final.copy()","09c6bd37":"from imblearn.over_sampling import SMOTE\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import IncrementalPCA\n\ndef split_date(df):\n    Y = df[\"churn\"]\n    X = df.drop([\"churn\",\"mobile_number\"],axis=1)\n    return X,Y\n    \ndef scale_data(X):\n    scaler = StandardScaler()\n    X_col = X.columns\n    X_scaled = scaler.fit_transform(X)\n    X_scale_final = pd.DataFrame(X_scaled, columns=X_col)\n    log(\"Scaling the data ....\")\n    return X_scale_final\n\ndef get_scree_plot(X):\n    pca = PCA(svd_solver='randomized', random_state=101)\n    pca.fit(X)\n    fig = plt.figure(figsize = (8,6))\n    plt.plot(np.cumsum(pca.explained_variance_ratio_))\n    plt.xlabel('number of components')\n    plt.ylabel('cumulative explained variance')\n    plt.show()\n    return pca\n    \n    \ndef resampling_data(X,Y,feature_number,train_size=0.7,test_size=0.3) :\n    \n    x_train, x_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, train_size=train_size, random_state=1)\n\n    log(\"Shape of train datatset before : \"+str(x_train.shape))\n    log(\"Percenatge of Churned customer data : \"+str(round(100*Y.sum()\/len(Y),2)))\n    log(\"Applying SMOTE to normalize imbalance ....\")\n\n    #Initializing SMOTE: Synthetic Minority Over-sampling Technique\n    # using this as data is skewed \n    smote = SMOTE(kind = \"regular\")\n    x_tr,y_tr = smote.fit_sample(x_train,y_train)\n    log(\"Shape of train datatset after SMOTE : \"+str(x_tr.shape))\n\n    # Applying PCA : Pricnipal Component Analysis\n    pca = IncrementalPCA(n_components=feature_number)    \n    x_tr_pca = pca.fit_transform(x_tr)\n    x_test_pca = pca.transform(x_test)\n    log(\"Shape of train datatset after PCA : \"+str(x_tr_pca.shape))\n\n    return x_tr_pca,x_test_pca,y_tr,y_test","d892a931":"X,Y=split_date(telecom_pca_df)","860dd814":"X.head(2)","fdabd236":"Y.head(2)","eb725355":"log(\"Percenatge of Churned Customer data : \"+str(round(100*Y.sum()\/len(Y),2)))","fc154393":"#Churn Distribution\npie_chart = telecom_pca_df['churn'].value_counts()*100.0 \/len(telecom_pca_df)\nax = pie_chart.plot.pie(autopct='%.1f%%', labels = ['No', 'Yes'],figsize =(8,6), fontsize = 14 )                                                                           \nax.set_ylabel('Churn',fontsize = 12)\nax.set_title('Churn Distribution', fontsize = 12)\nplt.show()","01370c55":"X_scaled = scale_data(X)\nprint(X_scaled.shape)\nX_scaled.head(5)","32cebcb1":"pca = get_scree_plot(X_scaled) # scree plot\n\ncol = list(X_scaled.columns)\ndf_pca = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'PC3':pca.components_[2],'Feature':col})\ndf_pca.head(10)","6848e68e":"np.cumsum(np.round(pca.explained_variance_ratio_, decimals=4)*100)","9d580d1f":"x_train, x_test, y_train, y_test = resampling_data(X_scaled,Y,60)","47dfa0ea":"x_train.shape","5c4e2eb7":"def get_regression_data(df):\n    X,Y=split_date(df)\n    x_train, x_test, y_train, y_test = resampling_data(scale_data(X),Y,60)\n    return x_train, x_test, y_train, y_test\n\nfrom sklearn.preprocessing import scale\nfrom sklearn import svm\nfrom sklearn import metrics\nimport itertools\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold,GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve, recall_score, classification_report\nfrom sklearn.metrics import recall_score,precision_score,roc_auc_score,f1_score,accuracy_score,confusion_matrix\n\nconsolidate_summary = pd.DataFrame()\n\ndef plot_confusion_matrix(cm):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    \"\"\"\n    classes=[0,1]\n    cmap=plt.cm.Blues\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title('Confusion matrix')\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=0)\n    plt.yticks(tick_marks, classes)\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n    \ndef get_svm_model_stats(x_train, x_test, y_train, y_test,kernel): \n    model = svm.SVC(kernel=kernel) # build model\n    model.fit(x_train, y_train) # fit model\n    predictions = model.predict(x_test) # print\n    accuracy_score = round(metrics.accuracy_score(y_true=y_test, y_pred=predictions),2)\n    log(\"Model selected - \"+kernel.upper()+\" and accuracy score for kernal is \"+str(accuracy_score*100))\n    model_name = \"SVM (Default)-\"+kernel\n    print_model_metrics(y_test,predictions,model_name)\n    \ndef display_hyper_stats(cv_results,param_value):\n    gamma = cv_results[cv_results['param_gamma']==param_value]\n    plt.plot(gamma[\"param_C\"], gamma[\"mean_test_score\"])\n    plt.plot(gamma[\"param_C\"], gamma[\"mean_train_score\"])\n    plt.xlabel('C')\n    plt.ylabel('Accuracy')\n    plt.title(\"Gamma=\"+str(param_value))\n    plt.ylim([0.60, 1])\n    plt.legend(['test accuracy', 'train accuracy'], loc='lower right')\n    plt.xscale('log')  \n    \ndef print_model_metrics(y_test,y_pred,model_name):\n    header(model_name+\" Model Stats Scores Summary : \")\n    cp = confusion_matrix(y_test,y_pred)\n    plt.figure()\n    plot_confusion_matrix(cp)\n    plt.show()\n    \n    accuracy = round(accuracy_score(y_test,y_pred),2)\n    recall = round(recall_score(y_test,y_pred),2)\n    precision = round(precision_score(y_test,y_pred),2)\n    auc = round(roc_auc_score(y_test,y_pred),2)\n    f1 = round(f1_score(y_test,y_pred),2)\n    \n    data = [[model_name,accuracy,recall,precision,auc,f1]] \n    df = pd.DataFrame(data, columns = ['Model', 'Accuracy','Precision','Recall','AUC','F1'])\n    add_to_global_summary(df)\n    return df \n\ndef add_to_global_summary(df) :\n    global consolidate_summary \n    consolidate_summary = consolidate_summary.append(df,ignore_index=True)","a6bc93cb":"#Training the model on the train data\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import metrics\n\nlr = LogisticRegression()\nmodel = lr.fit(x_train,y_train)\n#Making prediction on the test data\npred_probs_test = model.predict_proba(x_test)[:,1]\nlog(\"Linear Regression Accurancy : \"+\"{:2.2}\".format(metrics.roc_auc_score(y_test, pred_probs_test)))","3be3c708":"telecom_svm_df = telecom_df_final.copy()\nx_train, x_test, y_train, y_test = get_regression_data(telecom_svm_df)","9fcf0c75":"get_svm_model_stats(x_train, x_test, y_train, y_test,\"linear\")","4df41911":"get_svm_model_stats(x_train,x_test, y_train, y_test,\"rbf\")","742b2a16":"# creating a KFold object with 5 splits \nfolds = KFold(n_splits = 5, shuffle = True, random_state = 101)\n\n# Set the parameters by cross-validation\nhyper_params = [ {'gamma': [1e-1,1e-2, 1e-3, 1e-4], 'C': [1, 10, 100, 1000]}]\n\n# specify model\nmodel = svm.SVC(kernel=\"rbf\")\n\n# set up GridSearchCV()\nmodel_cv_svm = GridSearchCV(estimator = model, \n                        param_grid = hyper_params, \n                        scoring= 'accuracy', \n                        cv = folds, \n                        n_jobs = -1,\n                        verbose = 1,\n                        return_train_score=True)      \n\n# fit the model\nmodel_cv_svm.fit(x_train, y_train)  \n","8ac9716f":"# cv results\nsvm_cv_results = pd.DataFrame(model_cv_svm.cv_results_)\nsvm_cv_results['param_C'] = svm_cv_results['param_C'].astype('int')\ngamma=[1e-1,1e-2, 1e-3, 1e-4]\nplt.figure(figsize=(16,5))\nplt.subplot(141)\ndisplay_hyper_stats(svm_cv_results,gamma[0])\nplt.subplot(142)\ndisplay_hyper_stats(svm_cv_results,gamma[1])\nplt.subplot(143)\ndisplay_hyper_stats(svm_cv_results,gamma[2])\nplt.subplot(144)\ndisplay_hyper_stats(svm_cv_results,gamma[3])\nplt.show()","76017b99":"plt.figure(figsize=(16,5))\nplt.subplot(121)\ndisplay_hyper_stats(svm_cv_results,gamma[0])\nplt.subplot(122)\ndisplay_hyper_stats(svm_cv_results,gamma[1])","349eefe5":"# printing the optimal accuracy score and hyperparameters\nbest_score = model_cv_svm.best_score_\nbest_hyperparams = model_cv_svm.best_params_\n\nlog(\"The best test score is {0} corresponding to hyperparameters {1}\".format(round(best_score,2), best_hyperparams))","fb6f072c":"# model with optimal hyperparameters\nfinal_svm_model = svm.SVC(C=10, gamma=0.1, kernel=\"rbf\")\n\nfinal_svm_model.fit(x_train, y_train)\ny_pred = final_svm_model.predict(x_test)\n\n# metrics\nlog(\"Max accuracy with SVM (rbf) is \"+str(round(metrics.accuracy_score(y_test, y_pred),2)))","0e47a5f5":"print_model_metrics(y_test, y_pred,\"SVM( rfb ) [Hyper]\")","56c42cfb":"telecom_tree_df = telecom_df_final.copy()","5a4e9a64":"x_train, x_test, y_train, y_test = get_regression_data(telecom_tree_df)","54b506c6":"# Importing random forest classifier from sklearn library\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import classification_report,confusion_matrix, accuracy_score","e0d782b8":"rfc = RandomForestClassifier()\nrfc.fit(x_train,y_train)","83a1bde8":"# Making predictions\npredictions = rfc.predict(x_test)","7810b2c6":"print_model_metrics(y_test,predictions,\"RandomForest (Default)\")","dec2e11a":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import svm\nfrom sklearn import metrics\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold,GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix, precision_recall_curve, auc, roc_auc_score, roc_curve, recall_score, classification_report\n\n\ndef tune_hyperparameter(parameters,x_train,y_train,n_folds = 5,max_depth=0):\n    \n    if(max_depth==0):\n        rf = RandomForestClassifier()\n    else :\n        rf = RandomForestClassifier(max_depth=max_depth)\n        \n    rf = GridSearchCV(rf, parameters, cv=n_folds,n_jobs = -1, scoring=\"accuracy\",return_train_score=True)\n    rf.fit(x_train, y_train)\n    scores = rf.cv_results_\n\n    for key in parameters.keys():\n        hyperparameters = key\n        break\n\n    # plotting accuracies for parameters\n    plt.figure(figsize=(16,5))\n    plt.plot(scores[\"param_\"+hyperparameters], scores[\"mean_train_score\"], label=\"training accuracy\")\n    plt.plot(scores[\"param_\"+hyperparameters], scores[\"mean_test_score\"], label=\"test accuracy\")\n    plt.xlabel(hyperparameters)\n    plt.ylabel(\"Accuracy\")\n    plt.legend()\n    plt.show()","4315dbd7":"# parameters to build the model on\nparameters = {'max_depth': range(2, 40, 5)}\ntune_hyperparameter(parameters,x_train,y_train)","612d4f8a":"# parameters to build the model on\nparameters = {'n_estimators': range(100, 2000, 200)}\ntune_hyperparameter(parameters,x_train,y_train)","a7a77d0c":"# parameters to build the model on\nparameters = {'max_features': [20,30,40,50,60]}\ntune_hyperparameter(parameters,x_train,y_train,4)","d696be46":"# parameters to build the model on\nparameters = {'min_samples_leaf': range(1, 100, 10)}\ntune_hyperparameter(parameters,x_train,y_train)","2a5f376b":"# parameters to build the model on\nparameters = {'min_samples_split': range(10, 100, 10)}\ntune_hyperparameter(parameters,x_train,y_train)","6f5137df":"# Create the parameter grid based on the results of random search \nparam_grid = {\n    'max_depth': [12,18],\n    'n_estimators': [200],\n    'max_features': [40],\n    'min_samples_leaf': [10,20],\n    'min_samples_split': [10,20,30]\n}\n# Create a based model\nrf = RandomForestClassifier()\n# Instantiate the grid search model\nrf_grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 5, n_jobs = -1,verbose = 1,return_train_score=True)","2e8f3b2e":"# Fit the grid search to the data\nrf_grid_search.fit(x_train, y_train)","c1bfa76e":"# printing the optimal accuracy score and hyperparameters\nlog('We can get accuracy of '+str(round(rf_grid_search.best_score_,2))+' using '+str(rf_grid_search.best_params_))","5d85fc49":"log(\"Max accuracy with Random Forest classifier - \"+str(round(rf_grid_search.best_score_,2)))","0b8b35ec":"rfc = RandomForestClassifier(max_depth=18,\n                             max_features=40,\n                             min_samples_leaf=10,\n                             min_samples_split=20,\n                             n_estimators=200,\n                             n_jobs = -1)\nrfc.fit(x_train,y_train)","9f4d001b":"# Making predictions\npredictions = rfc.predict(x_test)","515f9cfd":"# Printing confusion matrix\nprint_model_metrics(y_test,predictions,'RandomForest (Hyper)')","f1ef27dc":"import xgboost as xgb\nfrom xgboost import XGBClassifier\nfrom xgboost import plot_importance\nfrom imblearn.over_sampling import SMOTE","c792e37f":"telecom_xgboost_df = telecom_df_final.copy()\nx_train, x_test, y_train, y_test = get_regression_data(telecom_xgboost_df)","a08c1191":"# fit model on training data with default hyperparameters\nmodel = XGBClassifier()\nmodel.fit(x_train, y_train)","da999a03":"# evaluate predictions\ny_pred = model.predict(x_test)","f21c3606":"print_model_metrics(y_test, y_pred ,'XGBoost (Default)')","6c366ffb":"# hyperparameter tuning with XGBoost\n\n# creating a KFold object \nfolds = 5\n\n# specify range of hyperparameters\nparam_grid = {'learning_rate': [0.1,0.2,0.3], \n             'subsample': [0.3,0.4,0.5]}          \n\n\n# specify model\nxgb_model = XGBClassifier(max_depth=2, n_estimators=200)\n\n# set up GridSearchCV()\nmodel_cv = GridSearchCV(estimator = xgb_model, \n                        param_grid = param_grid, \n                        scoring= 'accuracy', # accuracy\n                        cv = folds, \n                        n_jobs = -1,\n                        verbose = 1,\n                        return_train_score=True)      \n\n","4520de73":"# fit the model\nmodel_cv.fit(x_train, y_train)       ","4d85a352":"# cv results\ncv_results_xboost = pd.DataFrame(model_cv.cv_results_)\ncv_results_xboost['param_learning_rate'] = cv_results_xboost['param_learning_rate'].astype('float')","421169db":"# printing the optimal accuracy score and hyperparameters\nlog('We can get accuracy of **'+str(round(model_cv.best_score_,2))+'** using '+str(model_cv.best_params_))","0ecdf328":"def plot_for_xboost(param_grid,cv_results):\n    plt.figure(figsize=(18,5))\n    for n, subsample in enumerate(param_grid['subsample']):\n        # subplot 1\/n\n        plt.subplot(1,len(param_grid['subsample']), n+1)\n        df = cv_results[cv_results['param_subsample']==subsample]\n\n        plt.plot(df[\"param_learning_rate\"], df[\"mean_test_score\"])\n        plt.plot(df[\"param_learning_rate\"], df[\"mean_train_score\"])\n        plt.xlabel('learning_rate')\n        plt.ylabel('AUC')\n        plt.title(\"subsample={0}\".format(subsample))\n        plt.ylim([0.60, 1])\n        plt.legend(['test score', 'train score'], loc='right left')\n        plt.xscale('log')","c364773d":"param_grid1 = {'learning_rate': [0.1,0.2,0.3], 'subsample': [0.3,0.4,0.5]}  \nplot_for_xboost(param_grid1,cv_results_xboost)\n","3f1f2d98":"# chosen hyperparameters\n# 'objective':'binary:logistic' outputs probability rather than label, which we need for auc\nparams = {'learning_rate': 0.3,\n          'max_depth': 2, \n          'n_estimators':200,\n          'subsample':0.5,\n         'objective':'binary:logistic'}\n\n# fit model on training data\nmodel = XGBClassifier(params = params,max_depth=2, n_estimators=200)\nmodel.fit(x_train, y_train)","d6190587":"# predict\ny_pred = model.predict(x_test)","838f4933":"print_model_metrics(y_test, y_pred,'XGBoost (Hyper Tuned)')","75405005":"# plot\nplt.bar(range(len(model.feature_importances_)), model.feature_importances_)\nplt.show()","e4ba8f18":"header(\"Important features ...\")\nplt.figure(figsize=(6,4))\nsns.countplot(x = 'tenure_range', hue = 'churn',data = tenure_data,palette=(\"plasma\"))\nplt.show()\n\n\ncol = list(X_scaled.columns)\ndf_pca = pd.DataFrame({'PC1':pca.components_[0],'PC2':pca.components_[1], 'PC3':pca.components_[2],'Feature':col})\ndf_pca.head(15)","1c63270a":"consolidate_summary.head(10)","59b2e516":"## Logistic Regression Modelling","5d6581f6":"### Problem Statement\n\n>In the telecom industry, customers are able to choose from multiple service providers and actively switch from one operator to another. In this highly competitive market, the telecommunications industry experiences an average of 15-25% annual churn rate. Given the fact that it costs 5-10 times more to acquire a new customer than to retain an existing one, customer retention has now become even more important than customer acquisition.o reduce customer churn, telecom companies need to predict which customers are at high risk of churn. ","d8900b46":"> <span style='color:blue'>**Analysis:** Significantly it showing that volume based cost for 3G is much lower for Churned customers as compared to Non-Churned Customers and also there is a drop in vbc in 8th month","1a25a1e5":"# Recommendations","e9738722":"# Modelling\n","3e5885f1":"> <span style='color:blue'>**Analysis:** We can see the drop in sachet services in 8th month for churned cutsomers","6fee1957":"> **Percentage distribution of churn\/non-churn cutomer data**","4b1e9f6d":"> <span style='color:blue'>**Analysis:** The plots above show some useful insights:<br>\n> Looking at above curves and values , model with gamma=0.1 seems to be overfitting and should not be used.<br>\n> Model selected with best value as 'C': 100, 'gamma': 0.1 but this model also seems to be overfitting<br>\n> But model selected with value 'C': 10, 'gamma': 0.1 should produce better results.<br>\n> We will be selecting these values for final modelling.<br>\n<\/span>","49a79e4e":"> ### Model Insights ","c54677a0":"> <span style='color:blue'>**Analysis:** We can see that total_og_mou_6, std_og_mou_6 and loc_og_mou_6 is a combination of other variables present in dataset.So we can remove these columns for all months from the data set","43ada07d":"> <span style='color:blue'>**Analysis:** :  Score almost remain the same with very low dip throught the range. We wull use **200** for grid view search.","21b9344a":"> <span style='color:blue'>**Analysis:** We can see that total_og_mou_6, std_og_mou_6 and loc_og_mou_6 seems to have strong correlation with other fileds and they needs to be inspected to avoid any multicolinearity issues.","3d9ad662":"## Loading Moduels & Libraries ","307401c8":"> <span style='color:blue'>**Analysis:** We can see more then **74%** values for **recharge** related data are missing.<\/span>","4a92aeea":"> Tag churners and remove attributes of the churn phase\n- Now tag the churned customers (churn=1, else 0) based on the fourth month as follows: Those who have not made any calls (either incoming or outgoing) AND have not used mobile internet even once in the churn phase. \n- The attributes you need to use to tag churners are:**total_ic_mou_9, total_og_mou_9, vol_2g_mb_9, vol_3g_mb_9**\n- After tagging churners, remove all the attributes corresponding to the churn phase (all attributes having \u2018 _9\u2019, etc. in their names).","b91059ed":"> **Scaling the data so as to normalize all the fields**","90b92854":"#### Applying Regression ","40a11956":"> <span style='color:blue'> **Random forest** also produce good accuracy with 0.91 (default overfit model) and 0.90 with tuned hyperparameters.","45dc4b80":"Let's now try tuning the hyperparameters using k-fold CV. We'll then use grid search CV to find the optimal values of hyperparameters.","9d63a9e1":"> <span style='color:blue'>**Analysis:** We can create new feature as **total_rech_amt_data** using **total_rech_data** and **av_rech_amt_data** to capture amount utilized by customer for data.\n\n> <span style='color:blue'>**Analysis:** Also as the minimum value is 1 we can impute the NA values by 0, Considering there were no recharges done by the customer.<\/span>","5aa40ed9":"> <span style='color:blue'>**Analysis:** We can see that there is a huge drop in maximum recharge for data also in the 8th month (action phase) for churned customers.","5507558f":"> <span style='color:blue'>**Analysis:** We can see that as we increase the value of max_depth, both train and test scores increase till a point, but after that test score becomme stagnant. The ensemble tries to overfit as we increase the max_depth. Thus, controlling the depth of the constituent trees will help reduce overfitting in the forest. **12 and 18** value have peek convergens and can be used for grid veiw search.","4e2f3e2a":"> # Case Study : Telecom Churn Case Study","b02798d0":"> ### Building and Evaluating the Final Model\n- Let's now build and evaluate the final model, i.e. the model with highest test accuracy.","90fa0ffe":"### 3. Derive new features.","a9971d4b":"### 2. Filter high-value customers","ceff1eac":"# EDA","39bc16a1":"### 1.  Derive new features","0eb26493":"#### b. 2G and 3G usage related attributes","f9d85c7d":"> <span style='color:blue'>**Analysis:** We can see that huge drops for Arpu in 8th month for churned customers","1d504b59":"> <span style='color:blue'>**Analysis:** Range **10 to 30** is optimal with good accuracy. ","70006b45":"> **Exploring Date field**","33b3cce7":"- Replacing NaN values with zero for all numeric data as most of the data is clean and will not impact the analysis","eec43ae6":"> #### Tuning max_features\n- Let's see how the model performance varies with ```max_features```, which is the maximum numbre of features considered for splitting at a node.","b3424989":"> Let's fist build two basic models - linear and non-linear with default hyperparameters, and compare the accuracies.","5aa6e6fe":"> <span style='color:blue'> Customers with less than **4 years** of tenure are more likely to churn and company should concentrate more on that segment by rolling out new schems to that group. ","f7ee5bf6":"> <span style='color:blue'>**Analysis:** We are getting more then 40% values are not available for count of recharge and average revenue for 2G\/3G per user. \nAlthough we have 2G\/3G volume usage based data available, we can drop these columns.","449cdd06":"> <span style='color:blue'> **Average revenue per user** seems to be most important feature in determining churn prediction. ","227e3cae":"> <span style='color:blue'>**Analysis:** Dropping above features with only **one unique** value as they will not add any value to our model building and analyis<\/span>","c671e960":"#### Preparing data","536e1cf3":"> <span style='color:blue'>**Analysis:** The roc_auc in this case is about 85% with default hyperparameters.\n","53c2dff6":"> <span style='color:blue'> **SVM** with tuned hyperparameters produce best result on this dataset with 0.92 accuracy.","e95fce1c":"> Let's first fit a random forest model with default hyperparameters.","4e6908d3":"> <span style='color:blue'>**Analysis:** We also see that there is a drop in Onnet usage in the 8th month for churned customers","5151295e":"> **Define high-value customers as follows:**\n- Those who have recharged with an amount more than or equal to X, where X is greater than 70th percentile of the average recharge amount in the first two months (the good phase)","8144e549":"## Tree Model Regression ","0ddbfa77":"## Loading dataset for telecom chrun analysis","2dad2c61":"> <span style='color:blue'>**Analysis:** We have two observations from above: <br> 1) 2G and 3G usage for churned customers drops in 8th month<br>\n2) We also see that 2G\/3G usage is higher for non-churned customers indicating that churned customers might be from areas where 2G\/3G service is not properly available.<\/span>","d976fd08":"### Checking columns with more than 30% NA values.","143a056e":"> **Exploring\/Imputing Numeric field**","d004d807":"### g. ONNET : All kind of calls within the same operator network","f4d03060":"> <span style='color:blue'>**Analysis:** We are getting a huge drop in 8th month recharge amount for churned customers.","2c52c840":"> <span style='color:blue'>**Analysis:** Defining total average recharge amount for good phase for months 6 and 7 (the good phase)<\/span>","49083ce5":"### Business Goal \n\n>In this project, you will analyse customer-level data of a leading telecom firm, build predictive models to identify customers at high risk of churn and identify the main indicators of churn.","a806ca17":"> ### Hyperparameter Tuning","203f5f8e":"> <span style='color:blue'>**Analysis:** So for all minutes of usage columns the maximum missing % is 3.91 , means in these case the customer has not been used at all for that particular call type, thus we can fill the missing values with zero","fd7c0080":"> <span style='color:blue'> **Local Outgoing calls** made to landline , fixedline , mobile and call center provides a strong indicator of churn behaviour.  ","fd3a1ba8":"> <span style='color:blue'>**Analysis:** SMOTE bloated the dataset and balanced it by adding skewed data values.","2e0b4d64":"> <span style='color:blue'>**Analysis: From above features we can derive more meaningful information :**\n* Total recharge amount\n* Total recharge for data\n* Maximum recharge amount \n* Last date of Recharging the data\n* Average recharge amount for data.\n* Maximum recharge for data<\/span>","8c09375d":"> <span style='color:blue'>**Analysis:** We can see that total_ic_mou_6, std_ic_mou_6 and loc_ic_mou_6 is a combination of other variables present in dataset.So we can remove these columns for all months from the data set","b95173f0":"## XGBoost\nLet's finally try XGBoost. The hyperparameters are the same, some important ones being ```subsample```, ```learning_rate```, ```max_depth``` etc.\n","b613852e":"> <span style='color:blue'>**Analysis:** Apparently, accuracy of training seems to be stable and test scores seems to increase till **30** and then decrease. Again we see increase on **40** and we will use that.","70834739":"> ### Grid Search: Hyperparameter Tuning\n- Let's now tune the model to find the optimal values of C and gamma corresponding to an RBF kernel. We'll use 5-fold cross validation.","a22b7ff5":"> <span style='color:blue'>**Analysis:** The non-linear model gives approx. 87% accuracy. Thus, going forward, let's choose hyperparameters corresponding to non-linear models.","3d162fd3":"> #### Tuning n_estimators\n- Let's try to find the optimum values for n_estimators and understand how the value of n_estimators impacts the overall accuracy. Notice that we'll specify an appropriately low value of max_depth, so that the trees do not overfit.\n<br>","a2c88c98":"> <span style='color:blue'>**Analysis:** So far so good, let's now look at the list of hyperparameters which we can tune to improve model performance.","afb27f26":"> ### Business Insights ","bf22f30d":"> <span style='color:blue'>**Analysis:** We can see that there is a huge drop in total recharge number also in the 8th month (action phase) for churned customers.","3a7e8514":"## Utility Methods","bdb06bda":"## PCA : Principal Component Analysis","17811f61":"> **In churn prediction, we assume that there are three phases of customer lifecycle :**\n- The \u2018good\u2019 phase [Month 6 & 7]\n- The \u2018action\u2019 phase [Month 8]\n- The \u2018churn\u2019 phase [Month 9]\n\n> In this case, since you are working over a four-month window, the first two months are the \u2018good\u2019 phase, the third month is the \u2018action\u2019 phase, while the fourth month is the \u2018churn\u2019 phase.","150fa202":"### 3. Tag churners and remove attributes of the churn phase","92dd6f31":"### Replacing NAN values","106b9988":"### f. Offnet Usage","eb9c27af":"> <span style='color:blue'>**Analysis:** We can see the drop for offnet mou services in the 8th month","2844f26e":"> ### Building and Evaluating the Final Model for Random Forest\n- Let's now build and evaluate the final model, i.e. the model with highest test accuracy.","92d6f3bf":"> #### Tuning max_depth\nLet's try to find the optimum values for ```max_depth``` and understand how the value of max_depth impacts the overall accuracy of the ensemble.\n","18cf694d":"**> <span style='color:blue'>**Analysis:** We can see a drop in the total recharge amount for churned customers in the 8th Month (Action Phase).","f264a4da":"> <span style='color:blue'>**Analysis:** We see that the minimum value is 1 while the max is 1555 across months, which indicate the missing values are where no recharges happened for the data, Filling the missing values by 0 , means no recharge.","63deafbb":"### d. Average Revenue Per User","3136d8b8":"### 1. Preprocess data (convert columns to appropriate formats, handle missing values, etc.)","ab6b82a3":"> <span style='color:blue'>**Analysis:** Again we can see a drop in monthly subscription for churned customers in 8th Month.","8ae21d36":"> <span style='color:blue'>**Analysis:** We can see that the model starts to overfit as value is decrease the value of min_samples_leaf. **10 to 20** seems to be a good range and that will be used in grid search.","efda5fd9":"> <span style='color:blue'>**Analysis:** We can see that total_ic_mou_6, std_ic_mou_6 and loc_ic_mou_6 seems to have strong correlation with other fileds and they needs to be inspected to avoid any multicolinearity issues.","973b4f9a":"> <span style='color:blue'>**Analysis:** The plots above show some useful insights:\n>> - Non-linear models (high gamma) perform *much better* than the linear ones\n>> - At any value of gamma, a high value of C leads to better performance\n>> - Model with gamma = 0.1 tends to overfit and rest of the values seems to be good. \n>> - This suggests that the problem and the data is **inherently non-linear** in nature, and a complex model will outperform simple, linear models in this case.<\/span>","81eb03a4":"## SVM Regression Modelling","14c9fecb":"> #### Grid Search to Find Optimal Hyperparameters\n- We can now find the optimal hyperparameters using GridSearchCV.","21b51cf3":"> <span style='color:blue'> Better 2G\/3G area coverage where 2G\/3G services are not good, it's strong indicator of churn behaviour.","27c87806":"> <span style='color:blue'>**Analysis:** Value range is very low, hence graph is not giving us proper justification result .Hence doing analysis by checking mean value.","a07d1002":"> <span style='color:blue'> Less number of **high value customer** are churing but for last **6 month** no new high valued cusotmer is onboarded which is concerning and company should concentrate on that aspect. <\/span>","3ce542aa":"After tagging churners, remove all the attributes corresponding to the churn phase **(all attributes having \u2018 _9\u2019, etc. in their names)**","ffe08eee":"> <span style='color:blue'>**Analysis:** We have merged most of the columns related to month 6 & 7 so to reduce the number of features. By doing this apporach we have reduced the fetures from 124 to 81.  ","254f8647":"> <span style='color:blue'> **XGBoost** also produce apt accuracy of 0.86 (default overfit model) and 0.85 with tuned hyperparameters.","fe9631bb":"### a. Recharge amount related variables\n------","c707dde3":"### 2. Conduct appropriate exploratory analysis to extract useful insights (whether directly useful for business or for eventual modelling\/feature engineering).","e97d2035":"> <span style='color:blue'>**Analysis:** We can see that there is a huge drop in maximum recharge amount for data in the 8th month (action phase) for churned customers.","d2c9a0dc":"> <span style='color:blue'>**Analysis:** Looks like 60 components are enough to describe 95% of the variance in the dataset.We'll choose **60** components for our modeling","1c8a075d":"### e. Minutes of usage - voice calls","bdec0ec2":"> <span style='color:blue'>**Analysis:** The results show that a subsample size of **0.5** and learning_rate of about **0.3** seems optimal. \nAlso, XGBoost has resulted in the highest ROC AUC obtained (across various hyperparameters). \n\n> Let's build a final model with the chosen hyperparameters.","c9d122e9":"> <span style='color:blue'> **As per our analysis SVM and Random forest produce best accuracy and models can be selected to predict churn data for future dataset or production.**","ff5b6b24":"# Data Preparation","82128f16":"> <span style='color:blue'> **Incoming** and **Outgoing Calls** on **romaing** for 8th month are strong indicators of churn behaviour","a6923dcd":"**> <span style='color:blue'>**Analysis:** We can see that there is a huge drop in total recharge amount for data in the 8th month (action phase) for churned customers.","c5a123b9":"> **Checking for incoming mou variables**","5ed7f2e1":"### h. Tenure Analysis for Customers  "}}