{"cell_type":{"66f952e0":"code","c37cc8d8":"code","57f94733":"code","952fe721":"code","d7ad7238":"code","6ddb5657":"code","2a8f4396":"code","4697e016":"code","9f80ecd7":"code","7b036328":"code","58153d3b":"code","d7f6da75":"code","0b168131":"code","be971039":"code","87683a40":"code","ab11940d":"code","cfc93c59":"code","89414db9":"code","daff7594":"code","566be349":"code","908130dd":"markdown","d2844c09":"markdown","f1573905":"markdown","b8eb2c57":"markdown","632c3321":"markdown","dac61483":"markdown","fbc05b82":"markdown","ff37153a":"markdown","1b381726":"markdown","8264e1ec":"markdown","1647a40f":"markdown"},"source":{"66f952e0":"import pandas as pd\r\nimport numpy as np\r\nimport pickle","c37cc8d8":"# Read in pickled data, and drop unneeded columns\r\ndf = pd.read_pickle(\"..\/input\/crack-cocaine-usage-prediction-eda\/NSDUH_cleaned_dropna_2016-2019.pkl\")\r\ndf = df.drop(['cocever', 'crkever', 'year'], axis=1)\r\ndf","57f94733":"# Continuous variables\r\nnum_cols = [\r\n    \"iralcfy\",\r\n    \"catag3\",\r\n    \"health\",\r\n    \"ireduhighst2\",\r\n    \"irpinc3\",\r\n    \"irki17_2\",\r\n    \"irmjfy\",\r\n    \"wrkdhrswk2\",\r\n    'irhhsiz2',\r\n    'cig30use',\r\n    'irherfy',\r\n    'irmethamyfq'\r\n]\r\n\r\n# Categorical variables\r\ncat_cols = [\r\n    \"newrace2\",\r\n    \"irsex\"\r\n]","952fe721":"from sklearn.pipeline import Pipeline\r\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\r\nfrom sklearn.compose import ColumnTransformer","d7ad7238":"# Create a preprocessor from ColumnTransformer\r\n# StandardScaler() applied to num_cols, and OneHotEncoder() applied to cat_cols\r\npreprocessor = ColumnTransformer(transformers=[\r\n    ('num', StandardScaler(), num_cols),\r\n    ('cat', OneHotEncoder(drop='first'), cat_cols)\r\n])","6ddb5657":"from sklearn.ensemble import RandomForestClassifier\r\nfrom sklearn.linear_model import LogisticRegression\r\nfrom sklearn import svm\r\nfrom sklearn.model_selection import train_test_split","2a8f4396":"# Define feature and target columns\r\nfeatures = num_cols+cat_cols\r\ntarget = \"coccrkever\"\r\n\r\n# Standard naming conventions for feature\/test datasets\r\nX = df[features]\r\ny = df[target]","4697e016":"from sklearn.model_selection import GridSearchCV","9f80ecd7":"preprocessor = ColumnTransformer(transformers=[\r\n    ('num', StandardScaler(), num_cols),\r\n    ('cat', OneHotEncoder(drop='first'), cat_cols)\r\n])\r\n\r\n# Parameter grid for GridSearchCV\r\nmodel_grid = {\r\n    'random_forest': {\r\n        'model':RandomForestClassifier(random_state=15, n_jobs=3, n_estimators=500),\r\n        'params': {\r\n            'estimator__max_depth': [11, 12, 13, 14],\r\n            'estimator__criterion':['gini', 'entropy']\r\n        }\r\n    },\r\n    'logistic_regression': {\r\n        'model': LogisticRegression(random_state=15, n_jobs=3),\r\n        'params': {\r\n            'estimator__C': [0.085, 0.09, 0.092],\r\n            'estimator__solver':['lbfgs', 'liblinear'],\r\n        }\r\n    },\r\n    'svm': {\r\n        'model': svm.LinearSVC(random_state=15, max_iter=100000),\r\n        'params': {\r\n            'estimator__C':[0.52, 0.55, 0.6, 0.65]\r\n        }\r\n    }\r\n}","7b036328":"# List to hold model scores\r\nscores = []\r\n\r\nfor model_name, model_params in model_grid.items():\r\n    pipe = Pipeline(steps=[\r\n        ('preprocessor', preprocessor),\r\n        ('estimator', model_params['model'])\r\n    ])\r\n\r\n    model = GridSearchCV(estimator=pipe, param_grid=model_params['params'], cv=4, return_train_score=False, refit=True)\r\n    model.fit(X, y)\r\n    scores.append({\r\n        'model': model_name,\r\n        'best_score:': model.best_score_,\r\n        'best_params': model.best_params_\r\n    })","58153d3b":"# Show scores as df\r\nscores","d7f6da75":"X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, test_size=.25, random_state=12)\r\n\r\npreprocessor = ColumnTransformer(transformers=[\r\n    ('num', StandardScaler(), num_cols),\r\n    ('cat', OneHotEncoder(drop='first'), cat_cols)\r\n])\r\n\r\npipe = Pipeline(steps=[\r\n        ('preprocessor', preprocessor),\r\n    ])\r\n\r\nX_train = pipe.fit_transform(X_train)\r\nX_test = pipe.transform(X_test)","0b168131":"# Train a model for each model type using our best hyperparameters. This is so we can analyze each\r\n# in a classification report\r\n\r\nrf = RandomForestClassifier(random_state=15, n_jobs=3, n_estimators=500, max_depth=14, criterion='entropy')\r\nrf.fit(X_train, y_train)\r\n\r\nlg = LogisticRegression(random_state=15, solver='liblinear',C=0.085)\r\nlg.fit(X_train, y_train)\r\n\r\nlsvc = svm.LinearSVC(random_state=15, max_iter=100000, C=0.52)\r\nlsvc.fit(X_train, y_train)","be971039":"rf_predict = rf.predict(X_test)\r\nlg_predict = lg.predict(X_test)\r\nlsvc_predict = lsvc.predict(X_test)","87683a40":"from sklearn.metrics import classification_report, accuracy_score","ab11940d":"print(\"Random Forest Score: %f\\nLogistic Regression Score: %f\\nLinear SVC Score: %f\\n\" %(accuracy_score(y_test, rf_predict), accuracy_score(y_test, lg_predict), accuracy_score(y_test, lsvc_predict)))\r\nprint(\"Random Forest:\\n\", classification_report(y_test, rf_predict))\r\nprint(\"Logistic Regression:\\n\", classification_report(y_test, lg_predict))\r\nprint(\"Linear SVC:\\n\", classification_report(y_test, lsvc_predict))","cfc93c59":"# Pickle models for later\r\nfor model, name in zip([lg, lsvc], [\"logreg_model\", \"lsvc_model\"]):\r\n    with open(\".\/\" + name + \".pickle\", 'wb') as f:\r\n        pickle.dump(model, f)","89414db9":"import gzip, pickletools\r\n\r\n# The output of a regular pickle.dump for our random forest is quite large,\r\n# we can compress it using gzip\r\nwith gzip.open(\".\/randforest_model.pickle\", \"wb\") as f:\r\n    pickled = pickle.dumps(rf)\r\n    optimized_pickle = pickletools.optimize(pickled)\r\n    f.write(optimized_pickle)\r\n\r\n\"\"\"Code for loading from a gzipped pickle file\"\"\"\r\n# with gzip.open(\".\/randforest_model_optimized.pickle\", 'rb') as f:\r\n#     p = pickle.Unpickler(f)\r\n#     rf = p.load()","daff7594":"# Pickle our Random Forest Classifier\r\nwith open(\".\/pipeline.pickle\", 'wb') as f:\r\n        pickle.dump(pipe, f)","566be349":"import json\r\n\r\n# Firstly we append our data columns to the json\r\ncolumn_info = {\r\n    'data_columns' : [col for col in num_cols+cat_cols]\r\n}\r\n\r\n# Write dict to a json file\r\nwith open(\".\/data_columns.json\", \"w\") as f:\r\n    f.write(json.dumps(column_info))","908130dd":"# Predicting if Someone has Tried Cocaine\r\n## Model Building\r\n\r\nYou can also check out my Crack\/Cocaine Usage Prediction repository on my Github!\r\n\r\nhttps:\/\/github.com\/bgallamoza\/Cocaine_Usage_Classification","d2844c09":"Next, we need to save our fitted pipeline to transform future data.","f1573905":"Now, we can move on to creating a server where we can make our model easily interactable.","b8eb2c57":"# Data Preprocessing with Pipeline","632c3321":"# Data Preprocessing\n\nBefore we can use our data to train models, we need to do a few things:\n\n1. Select our desired features\n\n2. Apply standard scaling to our numerical variables\n\n3. Dummify\/One Hot Encode our categorical variables\n\nWe begin with removing coutyp4, year, irwrkstat, and mjever columns. These columns were only used for EDA or do not seem to provide insight in target prediction.","dac61483":"## Accuracy vs Precision vs Recall\r\n\r\nAlthough the random forest performed the best in terms of total accuracy, our linear SVC model has the highest precision of each model. Recall teh differences between accuracy, precision, and recall:\r\n\r\n1. **Accuracy**: Proportion of correct predictions from total observations\r\n\r\n2. **Precision**: For a given class, the proportion of correct predictions from total predictions\r\n\r\n3. **Recall**: For a given class, proportion of correct predictions from the total number of true observations for that class\r\n\r\nOur models have low recall. That means we miss a large number of people who have actually used cocaine. However, we also have extremely high precision. This means that for the people we *do* predict have used cocaine, we are actually correct! This is important to consider. If your goal is to either help people using cocaine or prevent people from becoming addicted cocaine, it would be very bad to wrongly approach someone believing they've tried cocaine when they actually have not. **To prevent false positives, we will choose our Random Forest Classifier because it has the highest precision.**","fbc05b82":"# Model Building\n\nNow that our data is properly processed, we can test several models across different hyperparameters using GridSearchCV. We will be testing the following models:\n\n1. RandomForestClassifier()\n\n2. LogisticRegression()\n\n3. svm.LinearSVC()\n\n*We will only test the default hyperparamters for our LinearSVC due to the length of time required to train","ff37153a":"Finally, let's save our columns as a JSON file for future reference.","1b381726":"# Saving our Data\nAlthough we've decided on using the linear SVC model, we will save all the models regardless, just in case we want them in the future.","8264e1ec":"Now, we must separate our numerical and categorical variables. Numerical variables will be adjusted per column by StandardScaler(), which converts the data such that the mean and standard deviation is 0 and 1 for that column, respectively. This standardization across numerical variables increases our model's accuracy.\n\nAs for categorical variables, each unique value in a categorical column must be given its own separate, binary column indicating if that observation fits that unique value or not. We do this because keeping them in one column implies some kind of order. Something like race (newrace2 column) makes no sense to order, and is therefore a candidate to be separated into different columns (dummified). ","1647a40f":"# Choosing a Model\n\nAlthough our random forest model has the highest accuracy, the accuracies are very similar. Let's train a model using the best hyperparameters of each, then look at the classification report of each model to gain better insight into the performance of each model."}}