{"cell_type":{"a5c5ab38":"code","fc3c0203":"code","5ab27777":"code","ac106297":"code","5b8079bb":"code","e16673b7":"code","443a7681":"code","66376919":"code","1c2425c2":"code","24a79eeb":"code","29b389a0":"code","dbc37f67":"code","ee45361d":"code","21223206":"code","fbcbe819":"code","81ad7279":"code","fd4881cd":"code","7f6096c3":"code","ad3c1eab":"code","610c21cc":"code","4f33ea7b":"code","d0feb948":"code","d8b00b64":"code","25553ca3":"code","e56008d9":"code","4245179b":"code","4fefd6cc":"code","8b03a83d":"code","c70eca3b":"code","a1c9ce0a":"code","4a49f103":"code","c593b8e2":"code","32178841":"code","e4f83e98":"markdown","8203c67e":"markdown","c9e8037e":"markdown","e2a7012f":"markdown","d5aff2d8":"markdown","941f1aec":"markdown","4748c14c":"markdown","5c801a52":"markdown","4d13e87a":"markdown","75bdf94e":"markdown","162d674c":"markdown","ee56518c":"markdown"},"source":{"a5c5ab38":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n\nsns.set_style('whitegrid')\nplt.rcParams['figure.figsize'] = (12, 10)\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","fc3c0203":"train_data=pd.read_csv('\/kaggle\/input\/janatahack-crosssell-prediction\/train.csv', header=0)\ntest_data=pd.read_csv('\/kaggle\/input\/janatahack-crosssell-prediction\/test.csv', header=0)","5ab27777":"train_data.head()","ac106297":"test_data.head()","5b8079bb":"train_data.info()","e16673b7":"# Standard ML Models for comparison\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\n\n# Splitting data into training\/testing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Metrics\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\n\n# Distributions\nimport scipy","443a7681":"train_data.describe()","66376919":"numerical_columns=['id',\n 'Age',\n 'Driving_License',\n 'Region_Code',\n 'Previously_Insured',\n 'Annual_Premium',\n 'Policy_Sales_Channel',\n 'Vintage']\n\n\ncaterogical_columns=['Gender','Vehicle_Age','Vehicle_Damage']\n","1c2425c2":"train_data[\"Gender\"].replace({\"Male\":0, \"Female\":1}, inplace=True)\n\ntest_data[\"Gender\"].replace({\"Male\":0, \"Female\":1}, inplace=True)","24a79eeb":"train_data[\"Vehicle_Age\"].replace({\"< 1 Year\": 0, \"1-2 Year\":1, \"> 2 Years\":3}, inplace=True)\n\ntest_data[\"Vehicle_Age\"].replace({\"< 1 Year\": 0, \"1-2 Year\":1, \"> 2 Years\":3}, inplace=True)","29b389a0":"train_data[\"Vehicle_Damage\"].replace({\"Yes\": 0, \"No\":1}, inplace=True)\n\ntest_data[\"Vehicle_Damage\"].replace({\"Yes\": 0, \"No\":1}, inplace=True)","dbc37f67":"train_data['Gender'].unique()","ee45361d":"plt.figure(figsize=(15,8))\nsns.heatmap(train_data.corr())","21223206":"%matplotlib inline\nimport matplotlib.pyplot as plt\ntrain_data.hist(bins=50, figsize=(20,15))\nplt.show()","fbcbe819":"from pandas.plotting import scatter_matrix\nscatter_matrix(train_data, figsize=(20, 15))","81ad7279":"for each in train_data.columns.to_list():\n    #print(len(train_data[each].unique()),each)\n    if len(train_data[each].unique())<40:\n        carrier_count = train_data[each].value_counts()\n        sns.set(style=\"darkgrid\")\n        sns.barplot(carrier_count.index, carrier_count.values, alpha=0.9)\n        plt.title('Frequency Distribution of Carriers')\n        plt.ylabel('Number of Occurrences', fontsize=12)\n        plt.xlabel(each, fontsize=12)\n        plt.figure(figsize=(60,24))\n        plt.show()","fd4881cd":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\n\n\nfrom sklearn.impute import SimpleImputer\nimputer = SimpleImputer(strategy=\"most_frequent\")\n\n\nfrom sklearn.preprocessing import StandardScaler\nnum_pipeline = Pipeline([\n('imputer', SimpleImputer(strategy=\"most_frequent\")),\n('std_scaler', StandardScaler()),\n])\n\n\n\nfrom sklearn.preprocessing import OneHotEncoder\ncat_encoder = OneHotEncoder()\n\nnum_attribs = numerical_columns\ncat_attribs = caterogical_columns\n\n\n\n\nfull_pipeline = ColumnTransformer([\n(\"num\", num_pipeline, num_attribs),\n(\"cat\", OneHotEncoder(), cat_attribs),\n])\ntrain_data_prepared = full_pipeline.fit_transform(train_data)\n\ntest_data_prepared = full_pipeline.fit_transform(test_data)\n","7f6096c3":"valid_fraction = 0.05\nvalid_size = int(len(train_data) * valid_fraction)\n\ntrain = train_data[:-2 * valid_size]\nvalid = train_data[-2 * valid_size:-valid_size]\ntest = train_data[-valid_size:]\n\n\n# train_l = train_data[:-2 * valid_size]\n# valid_l = train_data[-2 * valid_size:-valid_size]\n# test_l = train_data[-valid_size:]","ad3c1eab":"import lightgbm as lgb\nfrom sklearn import preprocessing\nfrom sklearn.metrics import mean_squared_log_error, mean_squared_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import classification_report\nimport seaborn as sns\nfrom collections import Counter\n\nfeature_cols = train.columns.drop('Response')\n\n\nparams = {}\nparams['learning_rate'] = 0.045\nparams['max_depth'] = 18\nparams['n_estimators'] = 3000\nparams['objective'] = 'binary'\nparams['boosting_type'] = 'gbdt'\nparams['subsample'] = 0.7\nparams['random_state'] = 42\nparams['colsample_bytree']=0.7\nparams['min_data_in_leaf'] = 55\nparams['reg_alpha'] = 1.7\nparams['reg_lambda'] = 1.11\nparams['class_weight']: {0: 0.5, 1: 0.5}\n\n\n\nclf = lgb.LGBMClassifier(**params)\nclf.fit(train[feature_cols], train['Response'], early_stopping_rounds=100, eval_set=[(valid[feature_cols], valid['Response']),\n        (test[feature_cols], test['Response'])], eval_metric='multi_error', verbose=True)\n\neval_score = roc_auc_score(train_data['Response'], clf.predict(train_data[feature_cols]))\n\nprint('Eval ACC: {}'.format(eval_score))","610c21cc":"best_iter = clf.best_iteration_\nparams['n_estimators'] = best_iter\nprint(params)\n\n\nclf = lgb.LGBMClassifier(**params)\n\nclf.fit(train_data[feature_cols], train_data['Response'], eval_metric='multi_error', verbose=False)\n\n\neval_score_acc = roc_auc_score(train_data['Response'], clf.predict(train_data[feature_cols]))\n\nprint('ACC: {}'.format(eval_score_acc))","4f33ea7b":"preds = clf.predict(test_data)\n\nCounter(preds)\nsubmission = pd.DataFrame({'id':test_data['id'], 'Response':preds})\n\n\nplt.rcParams['figure.figsize'] = (12, 6)\nlgb.plot_importance(clf)\nplt.show()","d0feb948":"submission.to_csv('\/kaggle\/working\/submission.csv', index=False)","d8b00b64":"from sklearn.svm import SVR\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom scipy.stats import reciprocal, uniform\n\nparam_distributions = {\"gamma\": reciprocal(0.001, 0.1), \"C\": uniform(1, 10)}\nrnd_search_cv = SVR()#RandomizedSearchCV(SVR(), param_distributions, n_iter=10, verbose=2, cv=3, random_state=42)\nrnd_search_cv.fit(train_data_prepared, train_data['Response'])","25553ca3":"rnd_search_cv.best_estimator_","e56008d9":"y_pred = rnd_search_cv.best_estimator_.predict(train_data_prepared)\nscore = roc_auc_score(train_data_prepared, train_data['Response'])\nscore","4245179b":"preds = rnd_search_cv.best_estimator_.predict(train_data_prepared)\n\nCounter(preds)\nsubmission = pd.DataFrame({'id':test_data['id'], 'Response':preds})\n","4fefd6cc":"submission.to_csv('\/kaggle\/working\/submission_svm.csv', index=False)","8b03a83d":"train_data.info()","c70eca3b":"x=train_data.drop(columns={'id','Response'},axis=1)\ny=train_data.loc[:,['Response']]\ntest=test_data.drop(columns={'id'},axis=1)","a1c9ce0a":"import pandas as pd\nimport numpy as np\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import StratifiedKFold,KFold,GroupKFold\nfrom sklearn.metrics import accuracy_score,roc_auc_score\npd.set_option('display.max_columns', 100)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport time","4a49f103":"test.shape","c593b8e2":"%%time\nerr = [] \ny_pred_tot_lgm = np.zeros((len(test), 2))\n\n\nfold = StratifiedKFold(n_splits=10, shuffle=True, random_state=2020)\ni = 1\n\nfor train_index, test_index in fold.split(x, y):\n    x_train, x_val = x.iloc[train_index], x.iloc[test_index]\n    y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    m = CatBoostClassifier(n_estimators=10000,\n                       random_state=2020,\n                       eval_metric='Accuracy',\n                       learning_rate=0.08,\n                       depth=8,\n                       bagging_temperature=0.3,\n                       task_type='GPU'\n                       #num_leaves=64\n                       \n                       )\n    m.fit(x_train, y_train,eval_set=[(x_val, y_val)], early_stopping_rounds=100,verbose=200)\n    pred_y = m.predict(x_val)\n    print(i, \" err_lgm: \", accuracy_score(y_val,pred_y))\n    err.append(roc_auc_score(y_val,pred_y))\n    y_pred_tot_lgm+= m.predict_proba(test)\n    i = i + 1\ny_pred_tot_lgm=y_pred_tot_lgm\/10\nsum(err)\/10","32178841":"\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.preprocessing import StandardScaler\n\nC = 5\nalpha = 1 \/ (C * len(x))\n\nlin_clf = LinearSVC(loss=\"hinge\", C=C, random_state=42)\nsvm_clf = SVC(kernel=\"linear\", C=C)\nsgd_clf = SGDClassifier(loss=\"hinge\", learning_rate=\"constant\", eta0=0.001, alpha=alpha,\n                        max_iter=1000, tol=1e-3, random_state=42)\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(x)\n\nlin_clf.fit(x, y)\nsvm_clf.fit(x, y)\nsgd_clf.fit(x, y)\n","e4f83e98":"## Drawing a histogram","8203c67e":"## Creating a correalation Graph","c9e8037e":"## Creating Traing, test , and Validation Splits","e2a7012f":"## Vizualizing the frequency Distribution","d5aff2d8":"# StratifiedKFOLD","941f1aec":"## Preparing the Data","4748c14c":"# Visualization of the Data","5c801a52":"## Drawing a Scatter Plot","4d13e87a":"## Trying with Lightgbm","75bdf94e":"# Replacing the categorical data with Numeric Data","162d674c":"# Trying With Support Vector Machine","ee56518c":"# Creating a Base Line Model"}}