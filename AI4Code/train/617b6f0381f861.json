{"cell_type":{"b8e5a909":"code","3053e34c":"code","393cc4c7":"code","272ddb7d":"code","a2e120f8":"code","7ef368b4":"code","97fd0cf6":"code","973b12b2":"code","c79c12cf":"code","4e0aa0d1":"code","10465ab8":"code","a241b600":"code","fcc539c4":"code","ac9137d0":"code","17f15ebd":"code","3b172df8":"code","cde1cca6":"code","ed40b1e0":"code","f1a735fd":"code","e397d1e4":"markdown","94517bd4":"markdown","8a58479a":"markdown","f9e312de":"markdown","28c7a3cb":"markdown","a06cf2f0":"markdown"},"source":{"b8e5a909":"from sklearn.cluster import KMeans\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom mpl_toolkits.mplot3d import Axes3D\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom scipy.cluster.hierarchy import linkage, dendrogram\n","3053e34c":"data=pd.read_csv('..\/input\/iris\/Iris.csv')","393cc4c7":"work_data=data.iloc[:,1:-1]","272ddb7d":"data.head()","a2e120f8":"def Kcluster2DPlotter(df,n_clusters,random_state=42):\n    kmeans=KMeans(n_clusters=n_clusters,random_state=random_state)\n    kmeans.fit(df.values)\n    for i in range (1,n_clusters+1):\n        print('Cluster Center ',i,' coordinates: ',kmeans.cluster_centers_[i-1])\n    plt.scatter(df.iloc[:,0],df.iloc[:,1],c=kmeans.labels_,cmap='viridis')\n    for i in range (n_clusters):\n        plt.scatter(kmeans.cluster_centers_[i][0],kmeans.cluster_centers_[i][1],s=200,alpha=0.8,marker='*',c='red')\n       ","7ef368b4":"Kcluster2DPlotter(work_data,4)","97fd0cf6":"def Kcluster3DPlotter(df,n_clusters,fig_size=(16,9),random_state=42):\n    kmeans=KMeans(n_clusters=n_clusters,random_state=random_state)\n    kmeans.fit(df.values)\n    for i in range (1,n_clusters+1):\n        print('Cluster Center ',i,' coordinates: ',kmeans.cluster_centers_[i-1])\n    fig=plt.figure(figsize=fig_size)\n    ax=Axes3D(fig)\n    ax.scatter(df.iloc[:,0],df.iloc[:,1],df.iloc[:,2],c=kmeans.labels_,cmap='viridis')\n    for i in range (n_clusters):\n        ax.scatter(kmeans.cluster_centers_[i][0],kmeans.cluster_centers_[i][1],kmeans.cluster_centers_[i][2],s=200,alpha=0.8,marker='*',c='red')\n       ","973b12b2":"Kcluster3DPlotter(work_data,4)","c79c12cf":"def optimizer_graph(df,metric='distortion',upper_limit=10):\n    \n    visualizer=KElbowVisualizer(KMeans(),k=(2,upper_limit),timings=False,metric=metric)\n    visualizer.fit(df)\n    return visualizer.poof();","4e0aa0d1":"optimizer_graph(df=work_data,upper_limit=20);","10465ab8":"optimizer_graph(df=work_data,metric='silhouette' ,upper_limit=20);","a241b600":"optimizer_graph(df=work_data,metric='calinski_harabasz' ,upper_limit=20);","fcc539c4":"def dendrogram_constructor(df,method='complete',look='all'):\n    if look =='all':\n        hyc=linkage(df,method)\n        plt.figure(figsize=(16,9))\n        plt.title(f'Complete Dendrogram with {method} method')\n        plt.xlabel('Index')\n        plt.ylabel('Distance')\n        dendrogram(hyc,leaf_font_size=5);\n    else:\n        hyc=linkage(df,method)\n        plt.figure(figsize=(16,9))\n        plt.title(f'Dendrogram with {look} Clusters, ({method} Method)')\n        plt.xlabel('Index')\n        plt.ylabel('Distance')\n        dendrogram(hyc,leaf_font_size=15,truncate_mode='lastp',p=look);","ac9137d0":"dendrogram_constructor(work_data)","17f15ebd":"dendrogram_constructor(work_data,look=12)\n","3b172df8":"dendrogram_constructor(work_data,method='single')","cde1cca6":"dendrogram_constructor(work_data,method='single',look=12)","ed40b1e0":"dendrogram_constructor(work_data,method='average')","f1a735fd":"dendrogram_constructor(work_data,method='average',look=12)","e397d1e4":"Thanks...","94517bd4":"# Appendix\n## Dendrograms","8a58479a":"# 2.K-Means \nFirst, we apply the K-Means method to Iris dataset, then plot the clusters in 2 and 3D environments respectively.","f9e312de":"This notebook includes the following,\n1. Definition of Supervised and Unsupervised Learning and Clustring Types\n2. Definition and usage of KMeans\n3. Determining the optimum number of clusters (K) in a certain dataset.\n4. Some dendrogram examples to explain hierarchical clustering better.\n\nLet's get started.","28c7a3cb":"# 2.1. Determining the Optimum K\nTo determine optimum number of clusters we will use Elbow Method: In this method,\n1. We try the different number of clusters and score them based on a certain metric.\n2. We plot these scores and the number of clusters to y and x-axis respectively then construct a curve from these plots. \n3. Finally, we detect the breaking point of the curve. The x projection of this point gives the optimum number of clusters.     \n\nMetrics:\n\n1. Distortion Score: Sum of squared distances from each point to its assigned center.\n2. Silhouette: Scores based on the mean Silhouette Coefficient.\n3. Calinski_harabasz: Computes dispersion between clusters divided by dispersion within the clusters and scores based on that.","a06cf2f0":"# 1. Relevant Definitions About the Topic\n1. Supervised Learning: In supervised learning, we use labeled data sets to train algorithms and make predictions.[[1]](https:\/\/www.ibm.com\/cloud\/learn\/supervised-learning) It can be separated into two different subtitles: Classification and Regression. In a classification problem, the machine learning algorithm is trained to predict a specific class. In a regression problem, the machine learning algorithm is trained to predict a numeric value (not a class). \n2. Unsupervised Learning: In unsupervised learning, we use machine learning algorithms to analyze and cluster unlabeled datasets. [[2]](https:\/\/www.ibm.com\/cloud\/learn\/unsupervised-learning)\n3. Clustering: It is a data mining technique that is used for grouping the unlabeled data based on their similarities and differences. Clustring is used in unsupervised learning. The clustering process can be done in several ways like exclusive, overlapping, hierarchical, and probabilistic.\n * Exclusive Clustering: In this type of clustering a data point can only belong to one specific cluster.\n * Overlapping Clustering: It is the reverse of exclusive clustering. In other words, a data point can belong to more than one cluster.\n * Hierarchical Clustering: Data points are grouped based on their similarities. Then formed clusters are grouped in the same fashion iteratively. It can be better understood in dendrograms. (Appendix)\n * Probabilistic Clustering: In probabilistic clustering data points are clustered based on the likelihood that they belong to a particular distribution. Principal component analysis (PCA), Singular value decomposition (SVD), and Autoencoders are examples of probabilistic clustering. \n4. K-Means: In K-Means we cluster the dataset to K different clusters. A center point is assigned to each of these clusters. The location of the center point is found by minimizing the sum of the distance between each data point and cluster center."}}