{"cell_type":{"0929704d":"code","37e9c207":"code","d0bbe9b7":"code","1c2d172b":"code","dc0d6f90":"code","9ea0ba77":"code","5ee70c3c":"code","2e2cfdc0":"code","d7fbeb73":"code","0fcf9fd8":"code","6772d80c":"code","02ac8037":"code","a3b4a8d5":"markdown","032ec370":"markdown","82a98c08":"markdown","babd3297":"markdown","1ecb66cd":"markdown","4b6dfedd":"markdown","77afba09":"markdown","4b163268":"markdown","96ad8360":"markdown","668c10e2":"markdown","be598577":"markdown","ba8f8f17":"markdown"},"source":{"0929704d":"#General Imports\nimport numpy as np # linear algebra\nimport pandas as pd # data processing\n","37e9c207":"data = pd.read_csv('..\/input\/creditcard.csv')\ndata.describe()","d0bbe9b7":"data.columns","1c2d172b":"data.shape","dc0d6f90":"data.isnull().any().sum()","9ea0ba77":"LEGAL, FRAUD = range(2)\n\n# sampler name constants\nIMBALANCE, UNDER_RANDOM, OVER_RANDOM, OVER_SMOTE =  (\n    'Imbalance', 'Random Under Sampler', \n    'Random Over Sampler','Smote'\n)\nn = data.Class.count()\n\nfrauds = data.Class == FRAUD\nlegals = data.Class == LEGAL\nn_frauds = frauds.sum()\nn_legals = legals.sum()\n\nprint('Total transactions:', n)\nprint('Legal transactions: {1} ({0:.4f}%).'\n      ''.format(n_legals\/n*100, n_legals))\nprint('Fraudulent transactions: {1} ({0:.4f}%).'\n      ''.format(n_frauds\/n*100, n_frauds))","5ee70c3c":"from collections import Counter\n\nX = data.drop('Class', axis=1)\ny = data.Class\nc = Counter(y)\nprint('Original distribution '\n      'Legal ({0}) - Fraud ({1}))'.format(c[LEGAL], c[FRAUD]))","2e2cfdc0":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.4)\nn = y_test.count()\nn_frauds = y_test.sum()\n\"Fraud transactions: {0} ({1:.2}%)\".format(n_frauds, n_frauds\/n * 100)\n","d7fbeb73":"from time import time\nfrom imblearn.over_sampling import RandomOverSampler, SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\n\nsamplers = {\n    UNDER_RANDOM: RandomUnderSampler,\n    OVER_RANDOM: RandomOverSampler,\n    OVER_SMOTE: SMOTE,\n}\n\nsamples = {IMBALANCE: (X_train, y_train)}\ndurations = {IMBALANCE: 0}\n\nfor name, sampler in samplers.items():\n    start2 = time()\n    smp = sampler(random_state=0)\n    samples[name] = X_sample, y_sample = smp.fit_resample(X_train, y_train)\n    durations[name] = time() - start2\n\n    print('{0} tooks {1:.2} seconds'.format(name, durations[name]))\n    print('Distribution is', Counter(y_sample))\n\n    \n","0fcf9fd8":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import balanced_accuracy_score\nmodels = dict()\n\ncommon_args = {'n_estimators': 100, \n               'max_depth': 3,\n               'random_state': 0, 'oob_score': True}\nargs = {\n    IMBALANCE: {\n        'class_weight': 'balanced_subsample' \n         }\n}\nmodels = dict()\nsamples[IMBALANCE+' No Handling'] = samples[IMBALANCE]\n    \nfor name, sample in samples.items():\n    specific_args = args.get(name, {})\n    start = time()\n    rfc = RandomForestClassifier(**common_args, **specific_args)\n    rfc.fit(*sample)\n    \n    total_time = time() - start\n    \n    X_sample, y_sample = sample\n    y_pred = rfc.predict(X_test)\n    \n    score = balanced_accuracy_score(y_test, y_pred)\n    # store models and results\n    models[name] = rfc, y_pred, score, total_time\n    print('{0}\\nSample Size: {3:.0f}\\n'\n          'Trained time: {1:.2f} seconds\\n'\n          'Balance Score: {2:.2f}%\\n'\n          'Oob Score: {4:.2f}%\\n'.format(\n            name, total_time, score*100, \n            len(y_sample), rfc.oob_score_*100))\n","6772d80c":"MODEL, PREDICTS, SCORE, DURATION = range(4)\nprint('Out of Bag Scores')\nfor name in models.keys():\n    score = models[name][MODEL].oob_score_\n    \n    print(' - {0} is {1:.2f}%'.format(name, score*100))\n\n","02ac8037":"\nfrom sklearn.metrics import confusion_matrix, classification_report\n\"\"\"  \n           LEGAL                    FRAUD\n   PASS     lp(True Negative)       fp(False Negative)       \n   BLOCK    lb(False Positive)      fb(True Positive)\n   \n    recall = fb \/ (fb+fp)        true pos \/ (true pos + false neg)\n    precision = fb \/ (fb+lb)     true pos \/ (true pos + false pos)\n    \n    \"\"\"\ntarget_names = ['Legals', 'Frauds']\nfor name, model in models.items():\n    # tn, fp, fn, tp\n    # legal_passed, legal_blocked, fraud_passed, fraud_blocked \n    y_pred = model[PREDICTS]\n    lp, lb, fp, fb = confusion_matrix(y_test, \n                                      y_pred).ravel()\n    \n    print('{0}:\\nLegal Passed: {1}\\n'\n    'Legal Blocked: {2}\\n'\n    'Fraud Blocked: {4}\\n'\n    'Fraud Passed: {3}'\n    ''.format(name, lp, lb, fp, fb))\n    \n    print('Detect {0:.2f}% Frauds and Block {1:.2f}% of legals\\n'\n          ''.format(fb\/(fp+fb)*100, lb\/(lb+lp)*100))\n    \n    print(classification_report(y_test, y_pred, target_names=target_names))\n","a3b4a8d5":"Great, there is no Null values!","032ec370":"**Welcome**\nThank you for reading, if you like please upvote!.\n\n**Introduction**  \nIn this Kernel, we study several methods to sample the data, and how this affects the time and the accuracy of our model.\n\nThe dataset has Credit Card transactions and our aim is to detect which are fraudulent.  \n\nThis is a binary classification problem (Fraud or Legal) \n\nThe model selected is **Random Forest Classifier**.\n\n","82a98c08":"**Conclusions**\n- **Random Under Sampler** has the best results detecting 87.19% of the frauds however has blocked 1.47% of legal transactions.\nThis model was trained in 0.25seg, the fastest (sample 0.1s + train 0.15s).\n\n- **Random Forest** has handled the imbalance of the data pretty well, detecting 85.71% of the frauds and blocking only 0.52% of legal transactions.  \nThis model was trained in 40.69 seconds (162x times Random Under Sampler).\n\n- You can see the consequences of training the model with imbalanced data (see **Imbalance No Handling** results) Only 60% of frauds transactions were detected, vs all other models with balanced data train had higher recall (approx 86% frauds detected).\n","babd3297":"**Next steps in futher versions**\n- Add Visualizations\n- Tuning Hyper parameters \n- Try others Machine Learning: Support Vector Machine, Logistic Regression, Deep Learning.\n\nPlease let me your questions or comments, if you like my work, please upvote, Thank you.","1ecb66cd":"**Sampling**\n","4b6dfedd":"We have 284.807 records, with 31 columns. The columns V1 to V28 were processed with PCA. \n\nPCA (Principal Component Analysis) is a statistical method that reduces the number of columns (dimension) losing the minimal meaning of the original matrix.\n\n\nAs wikipedia said \"... its operation can be thought of as revealing the internal structure of the data in a way that best explains the variance in the data... PCA can supply the user with a lower-dimensional picture, a projection of this object when viewed from its most informative viewpoint...\"","77afba09":"**Imbalanced Data**\n","4b163268":"**Separate Train Test **","96ad8360":"Now will train several RandomForest models with differents samples:\n* Random Under Sample \n* Random Over Sample\n* Smote OverSample\n* Imbalanced Sample RF handling imbalance\n* Imbalanced Sample with No handling imbalance\n","668c10e2":"**Confusion Matrix**\n\nFollowing my intuition, I will use meaningful names instead of the general data science style ones. \n \n\n- **True Positive**: Legal pass. Legal transaction that were correctly classified. These the ammout of client transaction working fine. \n- **True Negative**: Fraud Blocked. Fraud detected and blocked.\n- **False Positive**: Legal Blocked. Legal transaction incorrectly classified (blocked).\n- **False Negative**: Fraud Pass. Fraud transaction incorrectly classified. No detected.\n","be598577":"Let's take a first look in our data.","ba8f8f17":"We need to train our model with a 50-50 balance data. However, there is a clear imbalance on the data. Due to the fact, most transactions are legal, as in real life. \n\nThis could cause our model to make erroneous assumptions if we don't handle this imbalance.\n\nIn this kernel, we utilize several techniques to balance the training data and also will train 2 models with imbalanced data and compare the effects."}}