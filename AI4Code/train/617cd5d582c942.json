{"cell_type":{"37e7fbda":"code","b0b8443d":"code","c9853a97":"code","bdc1b861":"code","84290846":"code","c7120142":"code","229235c3":"code","d41ce651":"code","f8a129c4":"code","98d2a300":"code","64fd9596":"code","11e20cee":"code","b1964e04":"code","6a2f45e1":"code","f522ff4c":"code","e46b25df":"code","fae9836c":"code","b4868f78":"code","9b443ab8":"code","fad620db":"code","b46fc6b4":"code","ea4827cd":"code","a5b4b351":"code","33310b7d":"code","b5a45009":"code","912c6076":"code","17db5a84":"code","5fd5ce47":"code","af795a12":"code","2fd4d3fb":"code","3367d803":"code","7769459c":"code","a4747a2f":"code","c91d4997":"code","b552bd30":"code","26472edf":"code","46ff71a2":"code","4c39a06a":"code","77dbf22f":"code","3b94bda9":"code","c142b9f0":"code","fa2622bc":"code","d87abc7c":"code","b349771d":"code","795befae":"code","e1ded0b8":"code","5034bdc1":"code","b24b505a":"code","f287b4a9":"code","bbb3e2c2":"code","8a3c9134":"code","6858b51f":"code","506f5323":"code","f2b9d251":"code","996ac55b":"code","7e9f794d":"code","c3bea2fb":"code","297c5d95":"code","b60e84d0":"code","32d70a32":"markdown","befca242":"markdown","0752dd3d":"markdown","501f6014":"markdown","51ce0a2b":"markdown"},"source":{"37e7fbda":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport re\n\nfrom collections import Counter\nfrom wordcloud import WordCloud, ImageColorGenerator\nfrom urllib.request import urlopen\nfrom PIL import Image\n\n\nfrom tqdm import tqdm\nimport string\nimport matplotlib.pyplot as plt\nfrom sklearn.decomposition import NMF, LatentDirichletAllocation, TruncatedSVD\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.manifold import TSNE\nimport concurrent.futures\nimport time\nimport pyLDAvis.sklearn\nfrom pylab import bone, pcolor, colorbar, plot, show, rcParams, savefig\n\n\n# Plotly based imports for visualization\nimport plotly.express as px\ndata = px.data.gapminder()\nfrom plotly.subplots import make_subplots\nfrom plotly import tools\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.figure_factory as ff\n\n\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nimport nltk\nstop=stopwords.words('english')\n\n# spaCy based imports\nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom spacy.lang.en import English\nnlp = spacy.load(\"en_core_web_lg\")\n\nimport warnings\nwarnings.filterwarnings('ignore')","b0b8443d":"df = pd.read_csv('\/kaggle\/input\/paatal-lok-imdb-review\/Paatal Lok IMDB Review.csv')\ndf['string_Rating'] = df.Rating.apply(lambda x: \"Rating \" + str(x))\ndf.sample(5)","c9853a97":"df.info()","bdc1b861":"rating_dist = df.groupby(['Rating']).size().reset_index(name = 'Number Of Ratings')\nrating_dist['Rating Percentage'] = (rating_dist['Number Of Ratings']\/df.shape[0])*100\n\nfig = px.bar(rating_dist, x ='Rating', y = 'Rating Percentage',\n              title= 'User Rating Distribution',\n             color = 'Rating Percentage',\n             \n             height=400, width=800) #color = 'Rating Percentage'\nfig.update_layout(\n    xaxis_title=\"Rating Star\",\n    yaxis_title=\"Rating Percentage\")\nfig.show()","84290846":"#Mean of word length distribution\nx1 = df[df['Rating']==1]['Review Text'].str.split().apply(lambda x : [len(i) for i in x])\nx1 = x1.map(lambda x: np.mean(x))\nfig = go.Figure(data=[go.Histogram(x=x1)])\nfig.show()","c7120142":"df['Review Length'] = df['Review Text'].str.split().apply(lambda x: len(x))\nfig = px.box(df, x=\"Rating\", y=\"Review Length\", title=\"Review Lengths\",height=400, width = 700)\nfig.show()","229235c3":"#review_length = df.groupby(['Rating']).agg({'Review Length' : {\"Mean\" : 'mean', \"Median\" : np.median, \"Max\":'max', \"Min\":'min'}})\nreview_length = df.groupby(['Rating']).agg(['mean', 'max',np.median, 'min']).rename({\"mean\" : 'Mean', \"median\" : 'Median', \"max\":'Max', \"min\":'Min'}, axis=1)\nreview_length.columns = review_length.columns.droplevel(0)\nreview_length = review_length.reset_index()","d41ce651":"fig = go.Figure()\nfig.add_trace(go.Scatter(x=review_length['Rating'], y=review_length['Mean'],\n                    mode='markers',\n                    name='Mean'))\nfig.add_trace(go.Scatter(x=review_length['Rating'], y=review_length['Median'],\n                    mode='markers',\n                    name='Median'))\nfig.add_trace(go.Scatter(x=review_length['Rating'], y=review_length['Max'],\n                    mode='markers',\n                    name='Max'))\nfig.add_trace(go.Scatter(x=review_length['Rating'], y=review_length['Min'],\n                    mode='markers',\n                    name='Min')),\n\nfig.update_layout(title=\"Reviews Length Distribution\",\n    xaxis_title=\"Rating Star\",\n    yaxis_title=\"Number of Words\",height=400, width = 700)\nfig.show()","f8a129c4":"fig = go.Figure(data=[go.Scatter(\n    x=[1, 3.2, 5.4, 7.6, 9.8, 12.5],\n    y=[1, 3.2, 5.4, 7.6, 9.8, 12.5],\n    mode='markers',\n    marker=dict(\n        color=[120, 125, 130, 135, 140, 145],\n        size=[15, 30, 55, 70, 90, 110],\n        showscale=True\n        )\n)])\n\nfig.show()","98d2a300":"import plotly.graph_objects as go\nanimals=['giraffes', 'orangutans', 'monkeys']\n\nfig = go.Figure(data=[\n    go.Bar(name='Mean', x=review_length['Rating'], y=review_length['Mean']),\n    go.Bar(name='Median', x=review_length['Rating'], y=review_length['Median']),\n    go.Bar(name='Minimum', x=review_length['Rating'], y=review_length['Min'],marker_color='rgb(55, 83, 109)'),\n    go.Bar(name='Maximum', x=review_length['Rating'], y=review_length['Max'],marker_color='rgb(26, 118, 255)')\n])\n# Change the bar mode\nfig.update_layout(barmode='group', title=\"Reviews Length Distribution\",\n    xaxis_title=\"Rating\",\n    yaxis_title=\"Number of Words\",height=400, width = 700)\nfig.show()","64fd9596":"#https:\/\/help.imdb.com\/article\/imdb\/track-movies-tv\/what-does-it-mean-if-i-found-a-user-review-helpful-or-not\/G4YY5P5XP2KJ349M?ref_=helpart_nav_16#\ndef helpfull_percenage(helpfull):\n    a = helpfull.strip('found this helpful').split(\" \")\n    st = int(a[0].replace(',', ''))\n    ed = int(''.join(a[-1:]).replace(',', ''))\n    try:\n        return (st\/ed)*100\n    except:\n        return 0\n\nHelpfull_Percentage = df.Helpfullness.apply(helpfull_percenage)\ndf.insert(loc=2, column='Helpfull Percentage', value=Helpfull_Percentage)","11e20cee":"df.head()","b1964e04":"avg_helpfull_percentage_by_rating = df.groupby(['Rating']).agg({'Helpfull Percentage':'mean','Review Length':'mean'}).reset_index()","6a2f45e1":"fig = px.bar(avg_helpfull_percentage_by_rating, x='Rating', y='Helpfull Percentage',orientation='v',\n              title= 'Mean Of Helpfull Percentage By Rating',\n             labels = {'Rating':'Ratings'},\n             height=400, width = 700) #color = 'Helpfull Percentage',\nfig.show()","f522ff4c":"fig = go.Figure()\nfig.add_trace(\n    go.Bar(\n        x=avg_helpfull_percentage_by_rating['Rating'],\n        y=avg_helpfull_percentage_by_rating['Helpfull Percentage'],\n        name = \"Helpfull Percentage\"\n    ))\n\nfig.add_trace(\n    go.Scatter(\n        x=avg_helpfull_percentage_by_rating['Rating'],\n        y=avg_helpfull_percentage_by_rating['Review Length'],\n        name = \"Review Length\"\n    ))\nfig.update_layout(title=\"Reviews Length Distribution\",\n    xaxis_title=\"Rating Star\",\n    yaxis_title=\"Helpfull Percentage\",\n                 yaxis={'categoryorder':'total descending'},height=400, width = 700)\nfig.show()","e46b25df":"fig = px.scatter_3d(avg_helpfull_percentage_by_rating, x='Rating', y='Helpfull Percentage', z='Review Length',height=400, width = 700)\nfig.show()","fae9836c":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)","b4868f78":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)","9b443ab8":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake \ud83d\ude14\ud83d\ude14\")","fad620db":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nexample=\"I am a #king\"\nprint(remove_punct(example))","b46fc6b4":"def clean_text(text):\n    text = re.sub(\"[^a-zA-Z ]\", \"\", text)\n    text = text.lower() # lower case the text\n    text = nltk.word_tokenize(text)\n    return text","ea4827cd":"def remove_stop_words(text):\n    return [word for word in text if word not in stop]","a5b4b351":"def stem_words(text):\n    try:\n        text = [stemmer.stem(word) for word in text]\n        text = [word for word in text if len(word) > 1] # make sure we have no 1 letter words\n    except : # the word \"oed\" broke this, so needed try except\n        pass\n    return \" \".join(text)","33310b7d":"df['Cleaned Text'] = df['Review Text'].apply(remove_URL).apply(remove_html).apply(remove_emoji).\\\n                        apply(clean_text).apply(remove_stop_words).apply(stem_words)#apply(remove_punct)","b5a45009":"df['Review Text'].apply(remove_URL).apply(remove_html).apply(remove_emoji).\\\n                        apply(remove_punct).apply(clean_text).apply(remove_stop_words).apply(stem_words)[0]","912c6076":"df['Title Cleaned Text'] = df['Review Title'].apply(remove_URL).apply(remove_html).apply(remove_emoji).\\\n                        apply(clean_text).apply(remove_stop_words).apply(stem_words)#apply(remove_punct)","17db5a84":"def most_common_words(df,col_name,rating,word_num=10):\n    review_title_rating = []\n    for title in df[col_name][df['Rating'] == rating]:\n        review_title_rating.extend(title.split())\n    review_title_rating = [title.lower() for title in review_title_rating if title not in spacy.lang.en.stop_words.STOP_WORDS]\n    review_title_rating = Counter(review_title_rating)\n    most_common_words = review_title_rating.most_common(word_num)\n    words = pd.DataFrame(most_common_words, columns=['Words', 'Count'])\n    return words","5fd5ce47":"review_title_rating_10 = most_common_words(df,'Title Cleaned Text',10)\nreview_title_rating_1 = most_common_words(df,'Title Cleaned Text',1)","af795a12":"fig = make_subplots(\n    rows=1, cols=2,\n    subplot_titles=(\"Rating 10\", \"Rating 1\"))\nfig.add_trace(go.Bar(x=review_title_rating_10['Words'], y=review_title_rating_10['Count']),\n              row=1, col=1)\nfig.add_trace(go.Bar(x=review_title_rating_1['Words'], y=review_title_rating_1['Count']),\n              row=1, col=2)\n\nfig.update_layout(height=400, width=1000,showlegend=False,\n                  xaxis_title=\"Most Occurance Words in Review Title\",\n    yaxis_title=\"Number of Occurance\", title={\n        'text': \"Top 10 words on Review Ttitle\",\n        'y':0.9,\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n                 )\n\nfig.show()","2fd4d3fb":"def get_top_review_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    top_bigram = pd.DataFrame(words_freq, columns=('Bigram', 'Counts'))\n    return top_bigram.head(n)","3367d803":"fig = make_subplots(\n    rows=2, cols=2,\n    subplot_titles=(\"Rating 1\", \"Rating 2\", \"Rating 9\", \"Rating 10\"))\n\nrating_1 = get_top_review_bigrams(df['Cleaned Text'][df['Rating'] ==1], 10)\nfig.add_trace(go.Bar(x=rating_1['Bigram'], y=rating_1['Counts']),\n              row=1, col=1)\n\nrating_2 = get_top_review_bigrams(df['Cleaned Text'][df['Rating'] ==2], 10)\nfig.add_trace(go.Bar(x=rating_2['Bigram'], y=rating_2['Counts']),\n              row=1, col=2)\n\nrating_9 = get_top_review_bigrams(df['Cleaned Text'][df['Rating'] ==9], 10)\nfig.add_trace(go.Bar(x=rating_9['Bigram'], y=rating_9['Counts']),\n              row=2, col=1)\n\nrating_10 = get_top_review_bigrams(df['Cleaned Text'][df['Rating'] ==10], 10)\nfig.add_trace(go.Bar(x=rating_10['Bigram'], y=rating_10['Counts']),\n              row=2, col=2)\n\n# edit axis labels\nfig['layout']['xaxis']['title']='Top 10 bi-gram in Rating Star 1'\nfig['layout']['xaxis2']['title']='Top 10 bi-gram in Rating Star 2'\nfig['layout']['xaxis3']['title']='Top 10 bi-gram in Rating Star 9'\nfig['layout']['xaxis4']['title']='Top 10 bi-gram in Rating Star 10'\nfig['layout']['yaxis']['title']='Number Of Occurance'\nfig['layout']['yaxis2']['title']='Number Of Occurance'\nfig['layout']['yaxis']['title']='Number Of Occurance'\nfig['layout']['yaxis2']['title']='Number Of Occurance'\n\nfig.update_layout(height=800, width=1000,showlegend=False,title={\n        'text': \"Top 10 bi-gram on Review Text\"},\n                 )\n\nfig.show()","7769459c":"def get_top_review_trigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    top_bigram = pd.DataFrame(words_freq, columns=('Bigram', 'Counts'))\n    return top_bigram.head(n)","a4747a2f":"fig = make_subplots(\n    rows=2, cols=2,\n    subplot_titles=(\"Rating 1\", \"Rating 2\", \"Rating 9\", \"Rating 10\"))\n\nrating_1 = get_top_review_trigrams(df['Cleaned Text'][df['Rating'] ==1], 10)\nfig.add_trace(go.Bar(x=rating_1['Bigram'], y=rating_1['Counts']),\n              row=1, col=1)\n\nrating_2 = get_top_review_trigrams(df['Cleaned Text'][df['Rating'] ==2], 10)\nfig.add_trace(go.Bar(x=rating_2['Bigram'], y=rating_2['Counts']),\n              row=1, col=2)\n\nrating_9 = get_top_review_trigrams(df['Cleaned Text'][df['Rating'] ==9], 10)\nfig.add_trace(go.Bar(x=rating_9['Bigram'], y=rating_9['Counts']),\n              row=2, col=1)\n\nrating_10 = get_top_review_trigrams(df['Cleaned Text'][df['Rating'] ==10], 10)\nfig.add_trace(go.Bar(x=rating_10['Bigram'], y=rating_10['Counts']),\n              row=2, col=2)\n\n# edit axis labels\nfig['layout']['xaxis']['title']='Top 10 trigram in Rating Star 1'\nfig['layout']['xaxis2']['title']='Top 10 trigram in Rating Star 2'\nfig['layout']['xaxis3']['title']='Top 10 trigram in Rating Star 9'\nfig['layout']['xaxis4']['title']='Top 10 trigram in Rating Star 10'\nfig['layout']['yaxis']['title']='Number Of Occurance'\nfig['layout']['yaxis2']['title']='Number Of Occurance'\nfig['layout']['yaxis']['title']='Number Of Occurance'\nfig['layout']['yaxis2']['title']='Number Of Occurance'\n\nfig.update_layout(height=800, width=1000,showlegend=False,title={\n        'text': \"Top 10 tri-gram on Review Text\"},\n                 )\n\nfig.show()","c91d4997":"Review_Text = df['Review Text'][df['Rating'] == 1].sample().reset_index()['Review Text'][0]\nprint(Review_Text)","b552bd30":"def increment_edge (graph, node0, node1):\n    #print(\"link {} {}\".format(node0, node1))\n    \n    if graph.has_edge(node0, node1):\n        graph[node0][node1][\"weight\"] += 1.0\n    else:\n        graph.add_edge(node0, node1, weight=1.0)","26472edf":"POS_KEPT = [\"ADJ\", \"NOUN\", \"PROPN\", \"VERB\"]\n\ndef link_sentence (doc, sent, lemma_graph, seen_lemma):\n    visited_tokens = []\n    visited_nodes = []\n\n    for i in range(sent.start, sent.end):\n        token = doc[i]\n\n        if token.pos_ in POS_KEPT:\n            key = (token.lemma_, token.pos_)\n\n            if key not in seen_lemma:\n                seen_lemma[key] = set([token.i])\n            else:\n                seen_lemma[key].add(token.i)\n\n            node_id = list(seen_lemma.keys()).index(key)\n\n            if not node_id in lemma_graph:\n                lemma_graph.add_node(node_id)\n\n            #print(\"visit {} {}\".format(visited_tokens, visited_nodes))\n            #print(\"range {}\".format(list(range(len(visited_tokens) - 1, -1, -1))))\n            \n            for prev_token in range(len(visited_tokens) - 1, -1, -1):\n                #print(\"prev_tok {} {}\".format(prev_token, (token.i - visited_tokens[prev_token])))\n                \n                if (token.i - visited_tokens[prev_token]) <= 3:\n                    increment_edge(lemma_graph, node_id, visited_nodes[prev_token])\n                else:\n                    break\n                    \n                    \n\n            #print(\" -- {} {} {} {} {} {}\".format(token.i, token.text, token.lemma_, token.pos_, visited_tokens, visited_nodes))\n\n            visited_tokens.append(token.i)\n            visited_nodes.append(node_id)","46ff71a2":"import networkx as nx\ndoc = nlp(df['Cleaned Text'][df['Rating'] == 1].sample().reset_index()['Cleaned Text'][0])\n\nlemma_graph = nx.Graph()\nseen_lemma = {}\n\nfor sent in doc.sents:\n    link_sentence(doc, sent, lemma_graph, seen_lemma)\n    #break # only test one sentence\n    \nlabels = {}\nkeys = list(seen_lemma.keys())\n\nfor i in range(len(seen_lemma)):\n    labels[i] = keys[i][0].lower()","4c39a06a":"fig = plt.figure(figsize=(9,9))\npos = nx.spring_layout(lemma_graph)\n\nnx.draw(lemma_graph, pos=pos, with_labels=False, font_weight=\"bold\")\nnx.draw_networkx_labels(lemma_graph, pos, labels)\nplt.show()","77dbf22f":"img = Image.open(\"..\/input\/patal-lok-image-5\/pl-1.jpg\")\ncustom_mask = np.array(img)\nreview_title_rating_1 = [words.lower() for words in df['Cleaned Text'][df['Rating'] == 1] if words not in spacy.lang.en.stop_words.STOP_WORDS]\nwc = WordCloud(background_color=\"black\",   max_font_size= 400, contour_width=0, contour_color='firebrick', mask = custom_mask)\nwc.generate(','.join(review_title_rating_1))\nplt.figure(figsize=(20,20))\nimage_colors = ImageColorGenerator(custom_mask)\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","3b94bda9":"img = Image.open(\"..\/input\/patal-lok-image-5\/pl-1.jpg\")\ncustom_mask = np.array(img)\nreview_title_rating_1 = [words.lower() for words in df['Cleaned Text'][df['Rating'] == 10] if words not in spacy.lang.en.stop_words.STOP_WORDS]\nwc = WordCloud(background_color=\"black\",   max_font_size= 400, contour_width=0, contour_color='firebrick', mask = custom_mask)\nwc.generate(','.join(review_title_rating_1))\nplt.figure(figsize=(20,20))\nimage_colors = ImageColorGenerator(custom_mask)\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","c142b9f0":"img = Image.open('..\/input\/patal-lok-image-5\/pl-1.jpg')\ncustom_mask = np.array(img)\nreview_title_rating_1 = [words.lower() for words in df['Cleaned Text'] if words not in spacy.lang.en.stop_words.STOP_WORDS]\nwc = WordCloud(background_color=\"black\",   max_font_size= 400, contour_width=0, contour_color='firebrick', mask = custom_mask)\nwc.generate(','.join(review_title_rating_1))\nplt.figure(figsize=(20,20))\nimage_colors = ImageColorGenerator(custom_mask)\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","fa2622bc":"for i in df['Review Text'][df['Rating'] == 1].head(3):\n    spacy.displacy.render(nlp(i), style='ent',jupyter=True)","d87abc7c":"punctuations = string.punctuation\nstopwords = list(STOP_WORDS)","b349771d":"for j in df['Review Text'][df['Rating'] == 1].head(3):\n    review = str(\" \".join([i.lemma_ for i in nlp(j)]))\n    doc = nlp(review)\n    spacy.displacy.render(doc, style='ent',jupyter=True)","795befae":"spacy.displacy.render(nlp(df['Cleaned Text'][123]), style='ent',jupyter=True)","e1ded0b8":"spacy.displacy.render(nlp(df['Review Text'][123]), style='ent',jupyter=True)","5034bdc1":"# POS tagging\nfor i in nlp(df['Review Text'][1]):\n    print(i,\"=>\",i.pos_)","b24b505a":"parser = English()\ndef spacy_tokenizer(sentence):\n    mytokens = parser(sentence)\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n    mytokens = \" \".join([i for i in mytokens])\n    return mytokens","f287b4a9":"tqdm.pandas()\ndf[\"processed_review\"] = df[\"Review Text\"].progress_apply(spacy_tokenizer)","bbb3e2c2":"# Creating a vectorizer\nvectorizer = CountVectorizer(min_df=5, max_df=0.9, stop_words='english', lowercase=True, token_pattern='[a-zA-Z\\-][a-zA-Z\\-]{2,}')\ndata_vectorized = vectorizer.fit_transform(df[\"processed_review\"][df['Rating'] == 10])","8a3c9134":"NUM_TOPICS = 5","6858b51f":"# Latent Dirichlet Allocation Model\nlda = LatentDirichletAllocation(n_components=NUM_TOPICS, max_iter=10, learning_method='online',verbose=True)\ndata_lda = lda.fit_transform(data_vectorized)","506f5323":"# Non-Negative Matrix Factorization Model\nnmf = NMF(n_components=NUM_TOPICS)\ndata_nmf = nmf.fit_transform(data_vectorized) ","f2b9d251":"# Latent Semantic Indexing Model using Truncated SVD\nlsi = TruncatedSVD(n_components=NUM_TOPICS)\ndata_lsi = lsi.fit_transform(data_vectorized)","996ac55b":"# Functions for printing keywords for each topic\ndef selected_topics(model, vectorizer, top_n=10):\n    for idx, topic in enumerate(model.components_):\n        print(\"Topic %d:\" % (idx))\n        print([(vectorizer.get_feature_names()[i], topic[i]) for i in topic.argsort()[:-top_n - 1:-1]]) ","7e9f794d":"# Keywords for topics clustered by Latent Dirichlet Allocation\nprint(\"LDA Model:\")\nselected_topics(lda, vectorizer)","c3bea2fb":"# Keywords for topics clustered by Latent Semantic Indexing\nprint(\"NMF Model:\")\nselected_topics(nmf, vectorizer)","297c5d95":"# Keywords for topics clustered by Non-Negative Matrix Factorization\nprint(\"LSI Model:\")\nselected_topics(lsi, vectorizer)","b60e84d0":"pyLDAvis.enable_notebook()\ndash = pyLDAvis.sklearn.prepare(lda, data_vectorized, vectorizer, mds='tsne')\ndash","32d70a32":"# Topic Modeling","befca242":"# Ngram analysis\nwe will do a bigram (n=2) analysis over the tweets.Let's check the most common bigrams in tweets.","0752dd3d":"**Topic Modeling Of Review**","501f6014":"# Parts of Speech tagging\nThis is the process of marking up a word in a text (corpus) as corresponding to a particular part of speech,[1] based on both its definition and its context\u2014i.e., its relationship with adjacent and related words in a phrase, sentence, or paragraph. A simplified form of this is commonly taught to school-age children, in the identification of words as nouns, verbs, adjectives, adverbs, etc.","51ce0a2b":"# Named Entity Recognition\nNamed Entity Recognition is an information extraction task where named entities in unstructured sentences are located and classified in some pre-defined categories such as the person names, organizations, locations, medical codes, time expressions, quantities, monetary values, percentages, etc"}}