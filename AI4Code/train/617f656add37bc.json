{"cell_type":{"6299bd48":"code","e811277f":"code","c0a80c7a":"code","9b091f70":"code","3b60196a":"code","1ffb2e37":"code","01f409c8":"code","689fb2b4":"code","9af073d4":"code","938079be":"code","e4ec03ec":"code","02c327ca":"code","ba3c4f4f":"code","c1f9a0cd":"code","547a07cb":"code","b69ffcf3":"code","9697be48":"code","061621f6":"code","adf2485f":"code","f9637d8b":"code","f6091eef":"code","09ec55bc":"code","9dbbff8b":"code","73cb1b33":"code","faad5776":"code","4791ece0":"code","e26215ba":"code","732405ef":"code","7f682dda":"code","1c03ea4a":"code","fd43a4b0":"code","d8dd7da8":"code","73977cbf":"code","1fef088c":"code","e296eb8a":"code","f17847b0":"code","6bb1229c":"code","4c01e770":"markdown","723c0feb":"markdown","c6ccff94":"markdown","1f374358":"markdown","91ecba85":"markdown","887f4c31":"markdown","2e3d835c":"markdown","37a7d695":"markdown","ef75ab20":"markdown","f9344954":"markdown","05a2c814":"markdown","0cdae53b":"markdown","56fd34e0":"markdown","387a330e":"markdown","ea7764b8":"markdown"},"source":{"6299bd48":"import numpy as np # linear algebra\nimport pandas as pd # data processing\n\nimport matplotlib.pyplot as plt # graphs\nimport seaborn as sns # graphs\n\nfrom sklearn.model_selection import GridSearchCV # CV\nfrom sklearn.linear_model import LogisticRegression # Model\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder # Preprocess feautures\nfrom sklearn.pipeline import Pipeline, make_pipeline # Pipelines\nfrom sklearn.impute import SimpleImputer # Imputer\nfrom sklearn.compose import ColumnTransformer # For transformation\nfrom sklearn.metrics import classification_report,roc_curve, roc_auc_score # Report\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import mean_absolute_error\n\nfrom sklearn import set_config\nfrom sklearn.model_selection import train_test_split\nimport re\n\nset_config(display='diagram')\nfrom xgboost import XGBClassifier\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.warn(label_encoder_deprecation_msg, UserWarning)","e811277f":"Train_full = pd.read_csv('titanic\/train.csv',index_col='PassengerId',\n                    dtype={'Pclass': 'category','Sex': 'category'})\nX_valid_full = pd.read_csv('titanic\/test.csv', index_col='PassengerId',\n                  dtype={'Pclass': 'category','Sex': 'category'})","c0a80c7a":"# Remove rows with missing target, separate target from predictors\nTrain_full.dropna(axis=0, subset=['Survived'], inplace=True)\ny = Train_full.Survived\nTrain_full.drop(['Survived'], axis=1, inplace=True)","9b091f70":"# Break off test set from training data\nX_train_full, X_test_full, y_train, y_test = train_test_split(Train_full, y, \n                                                                train_size=0.8, test_size=0.2,\n                                                                random_state=1902)","3b60196a":"X_train_full.dtypes.value_counts() # Look on dtypes of raw features","1ffb2e37":"# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in X_train_full.columns if\n                    X_train_full[cname].nunique() < 10 and \n                    X_train_full[cname].dtype not in ('int64','float64')]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train_full.columns if \n                X_train_full[cname].dtype in ('int64','float64')]","01f409c8":"# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train_full[my_cols].copy()\nX_valid = X_valid_full[my_cols].copy()\nX_test = X_test_full[my_cols].copy()\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])\n","689fb2b4":"# Define model\nmodel = RandomForestClassifier(n_estimators=100, random_state=1902)\n\n# Bundle preprocessing and modeling code in a pipeline\nclf_RF = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', model)\n                        ]\n                 )\n# Preprocessing of training data, fit model \nclf_RF.fit(X_train, y_train)","9af073d4":"# Base Results\ny_test_preds = clf_RF.predict(X_test)\ny_train_preds = clf_RF.predict(X_train)\n\nprint('Accuracy (train):', accuracy_score(y_train, y_train_preds))\nprint('Accuracy (test):', accuracy_score(y_test, y_test_preds))","938079be":"# Define model\nmodel = XGBClassifier(n_estimators=100, random_state=1902)\n\n# Bundle preprocessing and modeling code in a pipeline\nclf_XGB = Pipeline(steps=[\n    ('preprocessor', preprocessor),\n    ('model', model)\n                        ]\n                 )\n# Preprocessing of training data, fit model \nclf_XGB.fit(X_train, y_train)","e4ec03ec":"# Base Results\ny_test_preds = clf_XGB.predict(X_test)\ny_train_preds = clf_XGB.predict(X_train)\n\nprint('Accuracy (train):', accuracy_score(y_train, y_train_preds))\nprint('Accuracy (test):', accuracy_score(y_test, y_test_preds))","02c327ca":"gs_params = {}","ba3c4f4f":"gs_params[1] = {'model__n_estimators':[10,25,50,100]}\ngs_params[2] = {'model__learning_rate':[0.01, 0.05, 0.2, 0.3]}\ngs_params[3] = {'model__max_depth':[1, 2, 4, 8]}\ngs_params[4] = {'model__subsample':[0.3, 0.5, 0.7, 0.9]}\ngs_params[5] = {'model__gamma':[0.05, 0.1, 0.5, 1]}","c1f9a0cd":"gs_1 = GridSearchCV(\n    estimator=clf_XGB,\n    param_grid=gs_params[1], \n    cv=5, \n    n_jobs=-1, \n    scoring='accuracy',\n    verbose=2\n)","547a07cb":"gs_1.fit(X_train.append(X_test),y_train.append(y_test))","b69ffcf3":"print(gs_1.best_score_)\nprint(gs_1.best_params_)","9697be48":"gs_2 = GridSearchCV(\n    estimator=clf_XGB,\n    param_grid=gs_params[2], \n    cv=5, \n    n_jobs=-1, \n    scoring='accuracy',\n    verbose=2\n)","061621f6":"gs_2.fit(X_train.append(X_test),y_train.append(y_test))","adf2485f":"print(gs_2.best_score_)\nprint(gs_2.best_params_)","f9637d8b":"gs_3 = GridSearchCV(\n    estimator=clf_XGB,\n    param_grid=gs_params[3], \n    cv=5, \n    n_jobs=-1, \n    scoring='accuracy',\n    verbose=2\n)","f6091eef":"gs_3.fit(X_train.append(X_test),y_train.append(y_test))","09ec55bc":"print(gs_3.best_score_)\nprint(gs_3.best_params_)","9dbbff8b":"gs_4 = GridSearchCV(\n    estimator=clf_XGB,\n    param_grid=gs_params[4], \n    cv=5, \n    n_jobs=-1, \n    scoring='accuracy',\n    verbose=2\n)","73cb1b33":"gs_4.fit(X_train.append(X_test),y_train.append(y_test))","faad5776":"print(gs_4.best_score_)\nprint(gs_4.best_params_)","4791ece0":"gs_5 = GridSearchCV(\n    estimator=clf_XGB,\n    param_grid=gs_params[5], \n    cv=5, \n    n_jobs=-1, \n    scoring='accuracy',\n    verbose=2\n)","e26215ba":"gs_5.fit(X_train.append(X_test),y_train.append(y_test))","732405ef":"print(gs_5.best_score_)\nprint(gs_5.best_params_)","7f682dda":"print(gs_1.best_params_)\nprint(gs_2.best_params_)\nprint(gs_3.best_params_)\nprint(gs_4.best_params_)\nprint(gs_5.best_params_)","1c03ea4a":"# gs_params[6] = {'model__n_estimators': [5,10,15,20]\n#                 ,'model__learning_rate': [0.03,0.04,0.05,0.06,0.07]\n#                 ,'model__max_depth': [2,3,4,5]\n#                 ,'model__subsample': [0.2,0.25,0.3,0.35,0.4]\n#                 ,'model__gamma': [0.4,0.45,0.5,0.55,0.6]}","fd43a4b0":"# gs_6 = GridSearchCV(\n#     estimator=clf_XGB,\n#     param_grid=gs_params[6],\n#     cv=5,\n#     n_jobs=-1,\n#     scoring='accuracy',\n#     verbose=-1\n# )","d8dd7da8":"# gs_6.fit(X_train.append(X_test),y_train.append(y_test))","73977cbf":"# print(gs_6.best_score_)\n# print(gs_6.best_params_)","1fef088c":"best_params = {'model__gamma': 0.4, 'model__learning_rate': 0.05, 'model__max_depth': 3, 'model__n_estimators': 20, 'model__subsample': 0.4}","e296eb8a":"clf_XGB.set_params(**best_params)","f17847b0":"# Preprocessing of training data, fit model\nclf_XGB.fit(X_train, y_train)","6bb1229c":"predictions = clf_XGB.predict(X_valid)\n\noutput = pd.DataFrame({'PassengerId': X_valid.index, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)","4c01e770":"### Gamma","723c0feb":"# Submission","c6ccff94":"* https:\/\/habr.com\/ru\/post\/274171\/\n* https:\/\/neurohive.io\/ru\/osnovy-data-science\/razbor-resheniya-zadachi-titanik-na-kaggle-dlya-nachinajushhih\/","1f374358":"## Random Forest","91ecba85":"### Learning rate","887f4c31":"### Number of estimates","2e3d835c":"# Load the data","37a7d695":"### More over suboptimal","ef75ab20":"### Max depth","f9344954":"# References","05a2c814":"# Models","0cdae53b":"# Packages","56fd34e0":"## XGBoost","387a330e":"# Prepare the data","ea7764b8":"### Subsample"}}