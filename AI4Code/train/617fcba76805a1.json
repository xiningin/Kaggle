{"cell_type":{"c5828a48":"code","5be5bcca":"code","7dde1ca6":"code","f2770783":"code","01b2f5db":"code","a1788b4d":"code","d010d188":"code","2f8ef7f5":"code","6f4a4d2f":"code","781310a3":"code","67c9136f":"code","7e556710":"code","8d2826ee":"code","b5c49a16":"code","5c474fc7":"code","0f884bb3":"code","f6f15cca":"code","953feecc":"code","fc5bded1":"code","da94ac90":"code","3fb1d21a":"code","291dac65":"code","9bfcf745":"code","0544e3ba":"code","44714b62":"code","4bb7d122":"code","12c1b8bb":"code","08f42903":"code","ff03c4ab":"code","b3b19599":"code","9afb7dff":"code","9123bb84":"code","a3fd5876":"code","e5c95d74":"code","3edbbe12":"code","afb82464":"code","850bcaed":"code","1bc58783":"code","cd9af6a3":"code","8123aa40":"code","ffd385b0":"code","4d16f138":"code","50262ecb":"code","e81b5993":"code","b4861c96":"code","84fa8f28":"code","78e107ca":"markdown","3578266f":"markdown","1f244c29":"markdown","d66b8c61":"markdown","6dac38ef":"markdown","47f9f5b9":"markdown","6dfee6f7":"markdown","085cc540":"markdown","b927d657":"markdown","84d6fef3":"markdown","2f31991b":"markdown","599dd3b3":"markdown","a9176408":"markdown","3332fc62":"markdown","538de590":"markdown","5fc52852":"markdown","13312399":"markdown","a69e020b":"markdown","87286262":"markdown","c6245a36":"markdown","7051cde4":"markdown","6af79408":"markdown","0ec72816":"markdown","7b7990c7":"markdown","75ec7560":"markdown","b701118c":"markdown","f020fd78":"markdown","09fb4074":"markdown","4000dd6c":"markdown","0bf019c9":"markdown","4801c702":"markdown","10603356":"markdown","d603ceda":"markdown","9620ad6e":"markdown","7169d693":"markdown","c4539a10":"markdown","ce34684a":"markdown","a301d9dc":"markdown","43d05890":"markdown","649677df":"markdown","970e88e0":"markdown","a79e6a77":"markdown","e36f5499":"markdown"},"source":{"c5828a48":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        break\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5be5bcca":"from fastai.vision.all import *\nimport matplotlib.pyplot as plt","7dde1ca6":"print(torch.cuda.is_available())\nprint(torch.version.cuda)\nprint(torch.__version__)","f2770783":"id_lookup_table_csv = pd.read_csv('..\/input\/facial-keypoints-detection\/IdLookupTable.csv', index_col='RowId')\nprint(id_lookup_table_csv.head())\nprint(id_lookup_table_csv.describe())","01b2f5db":"train_csv = pd.read_csv('..\/input\/facial-keypoints-detection\/training.zip')\ntrain_csv_images = train_csv['Image']\ntrain_csv = train_csv.drop(columns='Image')\ntrain_csv.head()","a1788b4d":"train_csv.info()","d010d188":"train_images = [np.fromstring(train_csv_images.iloc[i], sep=' ').reshape([96,96]) for i in range(train_csv_images.size)]\ntrain_points = [train_csv.iloc[k].values.reshape([15,2]) for k in range(train_csv.shape[0]) ]","2f8ef7f5":"print(train_points[0])\nplt.imshow(train_images[0], cmap='gray')\nplt.plot(train_points[0][:,0], train_points[0][:,1], 'gx')\n","6f4a4d2f":"test_csv = pd.read_csv('..\/input\/facial-keypoints-detection\/test.zip')\ntest_csv_images = test_csv['Image']\ntest_csv = test_csv.drop(columns='Image')\nprint(test_csv.head())","781310a3":"test_images = [np.fromstring(test_csv_images[i], sep=' ').reshape([96,96]) for i in range(test_csv_images.size)]","67c9136f":"plt.imshow(test_images[0], cmap='gray')","7e556710":"def show_image_and_points(img, true_pnts=None, pred_pnts=None):\n    ax = plt.imshow(img, cmap='gray')\n    if true_pnts is not None:\n        plt.plot(true_pnts[:,0], true_pnts[:,1], 'gx')\n    if pred_pnts is not None:\n        plt.plot(pred_pnts[:,0], pred_pnts[:,1], 'r+')\ndef show_test(i):\n    pred,_,_ = learner.predict(test_images[i])\n    show_image_and_points(test_images[i], pred_pnts=pred)\ndef show_train(i, learner=None):\n    fully_decoded = None\n    if learner is not None:\n        fully_decoded, loss_func_decoded, probabilities = learner.predict(train_images[i])\n    show_image_and_points(train_images[i], true_pnts=train_points[i], pred_pnts=fully_decoded)","8d2826ee":"show_train(2010)","b5c49a16":"def augment(img, pnts, rot_deg, zoom_factor, x_shift_pix, y_shift_pix):\n    sz = img.shape[-2:]\n    def get_rotation(x):\n        mysz = x.new_ones(x.shape[0])\n        rot_rad = torch.ones_like(mysz)*(rot_deg \/ 180.0 * np.pi)\n        m11 = rot_rad.cos() \/ zoom_factor\n        m12 = rot_rad.sin() \/ zoom_factor\n        t0 = torch.ones_like(mysz)*(x_shift_pix\/48.0)\n        t1 = torch.ones_like(mysz)*(y_shift_pix\/48.0)\n        return affine_mat(m11, m12, t0, -m12, m11, t1)\n    t1 = AffineCoordTfm(aff_fs=get_rotation, size=sz)\n    p1 = Pipeline(funcs=t1)\n    x = TensorImage(img).view([1,1,96,96])\n    y = TensorPoint(pnts, img_size=[96,96]).view([1,15,2])\n    x,y = p1((x,y\/48.0-1.0))\n    y = y.view([15,2])\n    coord_ok = (y[:,0] > -1.0) & (y[:,0] < 1.0) & (y[:,1] > -1.0) & (y[:,1] < 1.0)\n    coord_ok = torch.stack([coord_ok, coord_ok], dim=1)\n    y = y.where(coord_ok, tensor(np.nan))\n    y = y*48.0+48.0\n    return np.array(x.view([96,96])), np.array(y)","5c474fc7":"aug_img, aug_pnts = augment(train_images[0], train_points[0], 10.0, 1.0, 16.0, -16.0)\nshow_image_and_points(aug_img, true_pnts=aug_pnts)","0f884bb3":"augs = []\none_pixel = 2.0\/96.0\nfor dx in range(21):\n    for dy in range(21):\n        if dx==10 and dy==10:\n            continue\n        augs.append([0.0, 1.0, dx-10.0, dy-10.0])\nfor rot_deg in range(21):\n    if rot_deg==10:\n        continue\n    augs.append([rot_deg-10, 1.0, 0.0, 0.0])\nfor scale in range(21):\n    if scale==10:\n        continue\n    augs.append([0.0, 0.9 + 0.01*scale, 0.0, 0.0])\nprint(len(augs))","f6f15cca":"aug_images = []\naug_points = []\naug_ind = 0\nfor k1 in range(len(train_images)):\n    img,pnt = augment(train_images[k1], train_points[k1], *augs[aug_ind])\n    aug_images.append(train_images[k1])\n    aug_points.append(train_points[k1])\n    aug_images.append(img)\n    aug_points.append(pnt)\n    aug_ind = (aug_ind + 1) % len(augs)\nprint(len(aug_images))","953feecc":"train_images = aug_images\ntrain_points = aug_points","fc5bded1":"print(\"Original\")\nshow_train(0)","da94ac90":"print(\"Augmented (shifted right+down)\")\nshow_train(1)","3fb1d21a":"class MyBaseLoss():\n    \"Same as my `loss_cls`, but flattens input and target.\"\n    activation=decodes=noops\n    def __init__(self, loss_cls, *args, axis=-1, flatten=True, floatify=False, is_2d=True, **kwargs):\n        store_attr(\"axis,flatten,floatify,is_2d\")\n        self.func = loss_cls(*args,**kwargs)\n        functools.update_wrapper(self, self.func)\n\n    def __repr__(self): return f\"MyFlattenedLoss of {self.func}\"\n    @property\n    def reduction(self): return self.func.reduction\n    @reduction.setter\n    def reduction(self, v): self.func.reduction = v\n\n    def _contiguous(self,x):\n        return TensorBase(x.transpose(self.axis,-1).contiguous()) if isinstance(x,torch.Tensor) else x\n\n    def __call__(self, inp, targ, **kwargs):\n        inp,targ  = map(self._contiguous, (inp,targ))\n        if self.floatify and targ.dtype!=torch.float16: targ = targ.float()\n        if targ.dtype in [torch.int8, torch.int16, torch.int32]: targ = targ.long()\n        if self.flatten: inp = inp.view(-1,inp.shape[-1]) if self.is_2d else inp.view(-1)\n        tmptarg2 = targ.view(inp.shape)\n        tmptarg3 = torch.where(torch.isnan(tmptarg2), inp, tmptarg2)\n        tmptarg4 = tmptarg3.view(-1) if self.flatten else tmptarg3\n        return self.func.__call__(inp, tmptarg4, **kwargs)","291dac65":"@use_kwargs_dict(reduction='mean')\ndef MyMSELossFlat(*args, axis=-1, floatify=True, **kwargs):\n    \"Same as MY `nn.MSELoss`, but flattens input and target.\"\n    return MyBaseLoss(nn.MSELoss, *args, axis=axis, floatify=floatify, is_2d=False, **kwargs)\n","9bfcf745":"def MySplitter(valid_pcts=0.2):\n    def _inner(item_range):\n        cut = int(len(item_range) * (1.0-valid_pcts))\n        mylist = list(item_range)\n        l1 = mylist[:cut]\n        l2 = mylist[cut:]\n        return L(l1), L(l2)\n    return _inner\n","0544e3ba":"def get_x(ind):\n    return train_images[ind]\ndef get_y(ind):\n    return train_points[ind]\ndef get_items(i): return i\ndb = DataBlock(blocks=[ImageBlock, PointBlock],\n               get_items=get_items,\n               get_x=get_x, get_y=get_y,\n               item_tfms=Resize([96,96]),\n               splitter=MySplitter(0.2),\n               batch_tfms=aug_transforms(size=[96,96],\n                                         mult=1.0, max_rotate=8.0, \n                                         flip_vert=False, \n                                         do_flip=False, \n                                         pad_mode='border', # 'border' or 'reflection'\n                                         max_zoom=1.0, min_zoom=0.9, max_lighting=0.1, max_warp=0.0)\n              )","44714b62":"db.summary(range(len(train_images)))","4bb7d122":"dls = db.dataloaders(range(len(train_images)), bs=64)\ndls.train_ds.loss_func = MyMSELossFlat()","12c1b8bb":"dls.show_batch(cmap='gray', unique=False)","08f42903":"learner = cnn_learner(dls, resnet18)","ff03c4ab":"learner.summary()","b3b19599":"learner.model","9afb7dff":"learner.lr_find()","9123bb84":"print('Starting fit one cycle')\nlearner.fit_one_cycle(15, lr_max=1e-2)","a3fd5876":"learner.recorder.plot_loss()","e5c95d74":"learner.show_results(ds_idx=0, shuffle=False, nrows=2, ncols=4)","3edbbe12":"learner.show_results(nrows=3, ncols=4, max_n=16)","afb82464":"print('Starting fine tuning')\nlearner.fine_tune(75)\nprint('Fine tuning finished')","850bcaed":"learner.recorder.plot_loss()","1bc58783":"learner.show_results(nrows=3, ncols=4, max_n=16)","cd9af6a3":"test_predictions = np.zeros([len(test_images), 30])\nfor k in range(len(test_images)):\n    pred, pred_loss, pred_prob = learner.predict(test_images[k])\n    pred = torch.where(pred>=96, torch.ones(pred.shape)*95.99, pred)\n    pred = torch.where(pred<=0, torch.ones(pred.shape)*0.01, pred)\n    test_predictions[k] = pred.view([30])","8123aa40":"lut = pd.read_csv('..\/input\/facial-keypoints-detection\/IdLookupTable.csv', index_col='RowId')\nlut.head()","ffd385b0":"sample = pd.read_csv('..\/input\/facial-keypoints-detection\/SampleSubmission.csv', index_col='RowId')\nsample['Location'] = sample['Location'].astype(np.float)\nsample.head()","4d16f138":"namedict = {train_csv.columns[k1]: k1 for k1 in range(len(train_csv.columns))}\nprint(namedict)","50262ecb":"print(test_predictions.shape)\nprint(test_predictions[0][:])\nprint(namedict['right_eye_center_y'])\nprint(test_predictions[0][namedict['right_eye_center_y']])","e81b5993":"for k1 in range(sample.shape[0]):\n    imageid = lut.iloc[k1]['ImageId']-1\n    featurename = lut.iloc[k1]['FeatureName']\n    featurecol = namedict[featurename]\n    sample.iloc[k1]['Location'] = test_predictions[imageid,featurecol]","b4861c96":"sample.head()","84fa8f28":"sample.to_csv('submission.csv')","78e107ca":"Now look at some sample predictions on the validation data:","3578266f":"We pick 1e-2 as our learning rate and start training for \"one cycle\":","1f244c29":"Let's look at some sample predictions on our training data:","d66b8c61":"### Visually check some training images and their augmented versions:\nIn particular, we verify that the feature points of our augmented images align with the actual augmented image data.","6dac38ef":"Now all we have to do is to go through the sample submission file, extract the image id and feature name from each row,\nand then insert the predicted coordinate into the sample DataFrame.","47f9f5b9":"## Training\/Validation data split\nHaving inserted an augmented image after each training image, we choose to split our data at a predefined index (without randomization). This way we can avoid that accidentally an augmented training image slips into our validation data set. ","6dfee6f7":"In the above output, we note that our custom loss function is indeed used.\n\nAnother way to look inside our model is to directly print the model, which gives more details about the PyTorch model layers, but does not tell us about FastAI's added functionality:","085cc540":"For convenience, we define a display function that can plot training and test data, and additionally predicted feature points (if given):","b927d657":"Again, plot training and validation loss:","84d6fef3":"Check that the sample dataframe now contains our predictions:","2f31991b":"## Defining our DataLoaders\nWe use the DataBlock API to define images (ImageBlock) as input and 2D points (PointBlock) as output.\n\nAlso, we enable FastAI's on-the-fly augmentation transformations for increased robustness.\nNote that it is necessary to pass \"do_flip=False\", otherwise the model will get confused because\nthe image and its associated training points will get horizontally flipped, but the model will still\ninterpret e.g. the \"left\" eye on the left side of the flipped image,\nwhich would cause a very large loss and thus disturb the model's weights.","599dd3b3":"The IdLookupTable.csv file tells us which feature of which image to output, for each row of the submission file.","a9176408":"We can have a look the model architecture using the summary() function, which also tells us the activations' shape at several points in the model.","3332fc62":"This gives us 14098 augmented training images (+ their feature points).  \nFinally, replace the training data set with the augmented data set.","538de590":"## Explore the input data","5fc52852":"Let's convert the training image pixel values to actual image data:","13312399":"## Training loop","a69e020b":"## Prepare submission file\n","87286262":"Finally output the sample DataFrame as 'submission.csv' file: ","c6245a36":"First, we define our augmentation function that takes a training image and training points and returns\na tuple with the transformed image and training points.  \nAfter the transformation, point coordinates may be outside the valid range of [0,96], so we make sure we\nset these points' coordinates to NaN.","7051cde4":"Let's check visually that our function works. Note that the augmented points have been correctly transformed so that they align with the actual facial features of the transformed image.","6af79408":"Just like the training data, we convert the test data for easier access and display. ","0ec72816":"Let's use the learning rate finder to find out what learning rate makes sense:","7b7990c7":"For easier mapping of column names to prediction array indices, we create a dictionary:","75ec7560":"Just check some sample image:","b701118c":"## Import libraries and check versions","f020fd78":"Finally create the DataLoaders from the DataBlock, using a batch size of 64.  \nAlso, we override the standard loss with our custom loss function.","09fb4074":"Similarly to BaseLoss, we define our version of MSELossFlat based on the original implementation, returning our customized loss from above: ","4000dd6c":"Now we can access the predicted facial features with their column names:","0bf019c9":"So we have a total of 480 augmentation transformations.  \nNow create the augmented training data set, interleaving original training data with augmented training data.","4801c702":"We try to improve our model using fine tuning: ","10603356":"We define augmentation with x-shift from -10...+10 pixels, y-shift from -10...+10 pixels, rotations from -10...+10 degrees, and scale factor from 90% to 110%.","d603ceda":"Validation loss has improved, but reached a plateau.  \nShow some validation images with predictions:","9620ad6e":"In the above output, we note that ImageIDs are starting with 1 (i.e. are 1-based), so we need to subtract 1 later when accessing our predictions array.\n\nLet's read the sample submission file.  \nWe will prepare the submission by simply replacing the Location entry in each row with our own predictions","7169d693":"Plot training and validation loss:","c4539a10":"Depending on the input data, our model might predict coordinates outside of the range (0,96).  \nSince a submission is only valid if all its coordinates are in (0,96),\nwe compute our predictions for all test images and then clamp the coordinates to (0,96):","ce34684a":"## Define Learner\nCreate a convolutional neural network learner from the DataLoaders.\nWe use transfer learning based on resnet18.  \nUsing deeper resnets does not improve accuracy.\nAlso, accuracy get worse if setting limits with y_range=(-1,1).\n","a301d9dc":"## Define Loss function for use with NANs\nThe original input data as well as our augmented training points may contain NaNs.\nIn order to enable training with NaN coordinates, let's define a loss function that computes a meaningful loss value even if some points cannot be used.  \nThe below definition might look a little complicated at first glance but is actually a copy\nof the implementation of BaseLoss in Lib\\site-packages\\fastai\\loss.py, with only a small modification in the call() function.\nThis modification simply sets the loss value to zero for those coordinates that are NaN.","43d05890":"# Augment the training data\nIn addition to FastAI's on-the-fly augmentation, we deliberately augment our training set with shift, rotation and scaling augmentations. The intention here is to add augmentation in a non-probabilistic way that pushes our model to make good predictions for the original image _and_ a shifted\/rotated\/scaled version of the same image during each epoch.","649677df":"We can use the summary() function to check whether the DataBlock definition works:","970e88e0":"## Generate predictions for the test data","a79e6a77":"The data frame info() function tells us that we have lots of NaNs in our data.\nPandas provides some functions to replace NaNs with other plausible values (e.g. median),\nbut that's not a really satisfying solution.\nWe can however choose to ignore coordinates with NaNs during loss value computation.","e36f5499":"Have a look at a sample training image and its feature points:"}}