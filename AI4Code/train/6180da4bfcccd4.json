{"cell_type":{"c4f7a8c0":"code","2b441fc5":"code","3db9f8b4":"code","4b003e67":"code","4c52e859":"code","ead37394":"code","43a5e6a7":"code","53f42915":"code","96a80b8c":"code","fe3959e8":"code","64ac1eac":"code","64f08079":"code","1a2c5141":"code","2379a73b":"code","e7886435":"code","205a2b23":"code","d6cde99e":"code","b096f351":"code","d5747ad5":"code","3acc6494":"code","e50334bb":"code","0a685a92":"code","a4a6de02":"code","7f93e42f":"code","316d8d9b":"code","dfa8e1af":"code","b7a8ca4c":"code","4d0f9433":"code","94bd23b9":"code","ec768acd":"code","67cc82c7":"code","42637147":"code","6a5079ec":"code","2827e270":"code","90ced39c":"code","69077d68":"code","5e038352":"code","76bddb8e":"code","1b06c53a":"code","0525eec3":"code","b6101212":"code","226d49a6":"code","40651f35":"code","120562c5":"code","246e486b":"code","04a6f790":"code","e7de5549":"code","047c60ce":"code","1b71f132":"code","1462adbc":"code","5fb19afd":"code","8441da5d":"code","72cce304":"code","e7459edd":"code","d7809693":"code","7a7eaa43":"code","96497a0f":"code","e1e2a61f":"code","4af93c29":"code","4b740d81":"code","489cd38d":"markdown","8e5a9705":"markdown","dffaa283":"markdown","dce51d49":"markdown","dfc80b2d":"markdown","d486e722":"markdown","9f5b8950":"markdown","9c1e4a57":"markdown","d05a29bb":"markdown","aa8477f6":"markdown","d8fe43ec":"markdown","d8a89701":"markdown","91176958":"markdown","92ff696c":"markdown","d909706d":"markdown","006d7da1":"markdown","d8f66259":"markdown","77b2a02e":"markdown","4d6b20d4":"markdown","35c31fe7":"markdown"},"source":{"c4f7a8c0":"import numpy as np, pandas as pd\nfrom glob import glob\nimport shutil, os \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import GroupKFold\nfrom tqdm.notebook import tqdm\nimport seaborn as sns\nimport cv2\nfrom platform import python_version\n\nprint(f'The python version is {python_version()} & YOLO requires Python -V3.7')","2b441fc5":"dim = 512 # 512, 256, 'original'\nfold = 4","3db9f8b4":"train_df = pd.read_csv('\/home\/Sean\/iRail_Self_Study\/CHS\/train.csv')\ntrain_df.head()","4b003e67":"train_df.class_id.nunique()","4c52e859":"train_df.class_id.unique()","ead37394":"train_df = train_df[train_df.class_name != 'No finding'].reset_index(drop = True)\ntrain_df","43a5e6a7":"train_dim = pd.read_csv(os.path.join('\/home\/Sean\/iRail_Self_Study\/CHS\/train_meta.csv'))\ntrain_dim.head()","53f42915":"train = pd.merge(train_df, train_dim, on = 'image_id')\ntrain","96a80b8c":"train_df.class_id.nunique()","fe3959e8":"train_df.class_id.unique()","64ac1eac":"list = [ 3,  0, 11,  5,  8, 13,  7,  1,  9,  6, 10,  2,  4, 12]\nlist.sort()\nlist","64f08079":"size = 512\ntrain['x_min'] = train.apply(lambda row : (row.x_min)\/row.dim1, axis = 1)*float(size)\ntrain","1a2c5141":"train['y_min'] = train.apply(lambda row : (row.y_min)\/row.dim0, axis = 1)*float(size)\n\ntrain['x_max'] = train.apply(lambda row : (row.x_max)\/row.dim1, axis = 1)*float(size)\ntrain['y_max'] = train.apply(lambda row : (row.y_max)\/row.dim0, axis = 1)*float(size)\n\ntrain.head()","2379a73b":"# calculation x-mid, y-mid, width and hight of the bounding box for yolo\ntrain['x_mid'] = train.apply(lambda row: (row.x_max+row.x_min)\/2, axis =1)\ntrain['y_mid'] = train.apply(lambda row: (row.y_max+row.y_min)\/2, axis =1)\n\ntrain['w'] = train.apply(lambda row: (row.x_max-row.x_min), axis =1)\ntrain['h'] = train.apply(lambda row: (row.y_max-row.y_min), axis =1)\n\ntrain['x_mid'] \/= float(size)\ntrain['y_mid'] \/= float(size)\n\ntrain['w'] \/= float(size)\ntrain['h'] \/= float(size)\n\ntrain['area'] = train['w']*train['h']\ntrain.head()","e7886435":"train['image_path'] = f'\/home\/Sean\/iRail_Self_Study\/CHS\/kaggle\/train\/' + train.image_id + ('.png')\ntrain.head()","205a2b23":"Kfold = GroupKFold(n_splits = 5)\ntrain['fold'] = -1\ntrain","d6cde99e":"Kfold  = GroupKFold(n_splits = 5)\ntrain['fold'] = -1\nfor fold, (train_idx, val_idx) in enumerate(Kfold.split(train, groups = train.image_id.values.tolist())):\n    train.loc[val_idx, 'fold'] = fold\ntrain.head()","b096f351":"train.groupby('fold')['image_id'].agg(lambda x : x.nunique()).reset_index()","d5747ad5":"def visualize_plot(idx):\n    image = train_files[idx]\n    records = train[train['image_id'] == image]\n    boxes = np.array(records[['x_min','y_min','x_max','y_max']])\n    \n    labels = records.class_name\n    #sample = cv2.imread(os.path.join('\/home\/Sean\/iRail_Self_Study\/CHS\/kaggle\/train',f'{image}.png'))\n    sample = cv2.imread(os.path.join(f'\/home\/Sean\/iRail_Self_Study\/CHS\/kaggle\/train\/{image}.png'))\n\n    img = sample.copy()\n    plt.figure(figsize=(16, 16))\n    for box,label in zip(boxes,labels):\n        bbv.add_label(img, \n                      label, \n                      [int(round(box[0])), int(round(box[1])),int(round(box[2])), int(round(box[3]))], \n                      draw_bg=True,\n                      text_bg_color=(255,0,0),\n                      text_color=(0,0,0),\n                      )\n        cv2.rectangle(img ,\n                      (int(round(box[0])), int(round(box[1]))),\n                      (int(round(box[2])), int(round(box[3]))),\n                      (255,0,0),\n                      2)\n\n\n    plt.imshow(img)","3acc6494":"visualize_plot(0)","e50334bb":"visualize_plot(2)","0a685a92":"visualize_plot(13)","a4a6de02":"os.makedirs('\/home\/Sean\/iRail_Self_Study\/CHS\/vinbigdata\/labels\/train', exist_ok = True)\nos.makedirs('\/home\/Sean\/iRail_Self_Study\/CHS\/vinbigdata\/labels\/val', exist_ok = True)\n\nos.makedirs('\/home\/Sean\/iRail_Self_Study\/CHS\/vinbigdata\/images\/train', exist_ok = True)\nos.makedirs('\/home\/Sean\/iRail_Self_Study\/CHS\/vinbigdata\/images\/val', exist_ok = True)","7f93e42f":"External_DIR = '\/home\/Sean\/iRail_Self_Study\/CHS\/kaggle'","316d8d9b":"TRAIN_LABELS_PATH = '\/home\/Sean\/iRail_Self_Study\/CHS\/vinbigdata\/labels\/train'\nVAL_LABELS_PATH = '\/home\/Sean\/iRail_Self_Study\/CHS\/vinbigdata\/labels\/val'\nTRAIN_IMAGES_PATH = '\/home\/Sean\/iRail_Self_Study\/CHS\/vinbigdata\/images\/train'\nVAL_IMAGES_PATH = '\/home\/Sean\/iRail_Self_Study\/CHS\/vinbigdata\/images\/val'\n\nfor file in tqdm(train_files):\n    records = train[train['image_id'] == file]\n    attributes = records[['class_id','x_mid','y_mid','w','h']].values\n    attributes = np.array(attributes)\n    np.savetxt(\n        os.path.join(\n            TRAIN_LABELS_PATH,\n            f\"{file}.txt\"\n        ),\n        attributes,\n        fmt = [\"%d\",\"%f\",\"%f\",\"%f\",\"%f\"]\n    )\n    shutil.copy(\n        os.path.join(\n            External_DIR,\n            'train',\n            f\"{file}.png\" \n        ),          \n        TRAIN_IMAGES_PATH\n    )","dfa8e1af":"for file in tqdm(val_files):\n    records = train[train['image_id'] == file]\n    attributes = records[['class_id','x_mid','y_mid','w','h']]\n    attributes = np.array(attributes)\n    np.savetxt(\n        os.path.join(\n            VAL_LABELS_PATH,\n            f\"{file}.txt\"\n        ),\n        attributes,\n        fmt = [\"%d\",\"%f\",\"%f\",\"%f\",\"%f\"]\n    )\n    shutil.copy(\n        os.path.join(\n            External_DIR,\n            'train',\n            f\"{file}.png\" \n        ),          \n        VAL_IMAGES_PATH\n    )","b7a8ca4c":"class_ids, class_names = zip(*set(zip(train.class_id.tolist(), train.class_name.tolist())))\n","4d0f9433":"class_ids, class_names = zip(*set(zip(train.class_id.tolist(), train.class_name.tolist())))\nclasess = (np.array(class_names)[np.argsort(class_ids)]).tolist()","94bd23b9":"clasess","ec768acd":"from os import listdir\nfrom os.path import isfile, join\nimport yaml\n\ncwd = '.\/' # current working directory\n\nwith open(join( cwd , 'train.txt'), 'w') as f:\n    for path in glob('.\/vinbigdata\/images\/train\/*'):\n        f.write(path+'\\n')\n            \nwith open(join( cwd , 'val.txt'), 'w') as f:\n    for path in glob('.\/vinbigdata\/images\/val\/*'):\n        f.write(path+'\\n')\n\ndata = dict(\n    train =  '..\/train.txt',\n    val   =  '..\/val.txt',\n    nc    = 14,\n    names = classes\n    )\n\nwith open(join( cwd , 'vinbigdata.yaml'), 'w') as outfile:\n    yaml.dump(data, outfile, default_flow_style=False)\n    \nf = open(join( cwd, 'vinbigdata.yaml'), 'r')\nprint('\\nyaml:')\nprint(f.read())","67cc82c7":"import time\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\nimport torch\nimport torchvision\n\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\n\nfrom torch.utils.data import DataLoader, Dataset\n\nfrom matplotlib import pyplot as plt","42637147":"class_dict = dict(set(zip(train.class_id, train.class_name)))\nclasses = []\nfor key in sorted(class_dict.keys()): \n    classes.append(class_dict[key])\n\nclasses = ['_'] + classes   # adding background\nclasses","6a5079ec":"len(classes)","2827e270":"class VBDDataset(Dataset):\n    def __init__(self, dataframe, image_dir, transforms=None):\n        super().__init__()\n\n        self.image_ids = dataframe['image_id'].unique()\n        self.df = dataframe\n        self.image_dir = image_dir\n        self.transforms = transforms\n\n    def __getitem__(self, idx):\n\n        image_id = self.image_ids[idx]\n        records = self.df[self.df['image_id'] == image_id]\n\n        image = cv2.imread(f'{self.image_dir}\/{image_id}.png', cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n\n        boxes = records[['x_min', 'y_min', 'x_max', 'y_max']].values\n        # x_min\ty_min\tx_max , y_max\n        \n        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n        area = torch.as_tensor(area, dtype=torch.float32)\n        # all the labels are shifted by 1 to accomodate background\n        labels = torch.squeeze(torch.as_tensor((records.class_id.values+1,), dtype=torch.int64))\n        \n        # suppose all instances are not crowd\n        iscrowd = torch.zeros((records.shape[0],), dtype=torch.int64)\n        \n        target = {}\n        target['boxes'] = boxes\n        target['labels'] = labels\n        # target['masks'] = None\n        target['image_id'] = torch.tensor([idx])\n        target['area'] = area\n        target['iscrowd'] = iscrowd\n        if self.transforms:\n            sample = {\n                'image': image,\n                'bboxes': target['boxes'],\n                'labels': labels\n            }\n            sample = self.transforms(**sample)\n            image = sample['image']\n            \n            target['boxes'] = torch.as_tensor(sample['bboxes'])\n\n        return image, target, image_id\n\n    def __len__(self):\n        return self.image_ids.shape[0]","90ced39c":"train_dir = '\/home\/Sean\/iRail_Self_Study\/CHS\/kaggle\/train'\ndt = VBDDataset(train, train_dir)","69077d68":"dt[0]","5e038352":"# Albumentations\ndef get_train_transform():\n    return A.Compose([ ToTensorV2(p=1.0)],\n                     bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})\n\ndef get_valid_transform():\n    return A.Compose([ ToTensorV2(p=1.0)], \n                     bbox_params={'format': 'pascal_voc', 'label_fields': ['labels']})","76bddb8e":"model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained = True)","1b06c53a":"num_classes = 15  # 14 classes + background\n\n# get number of input features for the classifier\nin_features = model.roi_heads.box_predictor.cls_score.in_features\n\n# replace the pre-trained head with a new one\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n","0525eec3":"# A Class for keeping track of average\nclass Averager:\n    def __init__(self):\n        self.current_total = 0.0\n        self.iterations = 0.0\n\n    def send(self, value):\n        self.current_total += value\n        self.iterations += 1\n\n    @property\n    def value(self):\n        if self.iterations == 0:\n            return 0\n        else:\n            return 1.0 * self.current_total \/ self.iterations\n\n    def reset(self):\n        self.current_total = 0.0\n        self.iterations = 0.0    ","b6101212":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ntrain_dataset = VBDDataset(train, train_dir, get_train_transform())\nvalid_dataset = VBDDataset(train, train_dir, get_valid_transform())\n\n\ntrain_data_loader = DataLoader(\n    train_dataset,\n    batch_size=16,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)\n\nvalid_data_loader = DataLoader(\n    valid_dataset,\n    batch_size=8,\n    shuffle=False,\n    num_workers=4,\n    collate_fn=collate_fn\n)","226d49a6":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\ndevice","40651f35":"images, targets, image_ids = next(iter(train_data_loader))\nimages = list(image.to(device) for image in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\nboxes = targets[2]['boxes'].cpu().numpy().astype(np.int32)\nsample = images[2].permute(1,2,0).cpu().numpy()\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in boxes:\n    cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (220, 0, 0), 3)\n    \nax.set_axis_off()\nax.imshow(sample)","120562c5":"def get_dataloaders(df, trn_idx, val_idx):\n    \n    train_ = df.loc[trn_idx,:].reset_index(drop=True)\n    valid_ = df.loc[val_idx,:].reset_index(drop=True)\n        \n    def collate_fn(batch):\n        return tuple(zip(*batch))\n\n    train_dataset = VBDDataset(train_, train_dir, get_train_transform())\n    valid_dataset = VBDDataset(valid_, train_dir, get_valid_transform())\n\n\n    train_data_loader = DataLoader(\n        train_dataset,\n        batch_size=16,\n        shuffle=False,\n        num_workers=4,\n        collate_fn=collate_fn\n    )\n\n    valid_data_loader = DataLoader(\n        valid_dataset,\n        batch_size=8,\n        shuffle=False,\n        num_workers=4,\n        collate_fn=collate_fn\n    )\n    \n    return train_data_loader, valid_data_loader\n\n\n\ndef train_model(model, dataloader, device, epochs, optimizer, lr_scheduler, fold):\n    \n    best_loss = 1e10\n    loss_hist = Averager()\n    itr = 1\n    all_losses = []\n\n    for epoch in range(epochs):\n        loss_hist.reset() \n    \n        for images, targets, image_ids in dataloader:\n\n            images = list(image.to(device) for image in images)\n            targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n            loss_dict = model(images, targets)\n\n            losses = sum(loss for loss in loss_dict.values())\n            loss_value = losses.item()\n\n            loss_hist.send(loss_value)\n            all_losses.append(loss_value)\n            \n            optimizer.zero_grad()\n            losses.backward()\n            optimizer.step()\n\n            if itr % 50 == 0:\n                print(f\"Iteration #{itr} loss: {loss_value}\")\n\n            itr += 1\n        \n        # saving the model based on training loss for now. - later can be moved to validation\n        if loss_hist.value < best_loss:\n            best_loss = loss_hist.value\n            torch.save(model.state_dict(), f'fasterrcnn_model_{fold}.pt')\n\n        # update the learning rate\n        if lr_scheduler is not None:\n            lr_scheduler.step()\n\n        print(f\"Epoch #{epoch} loss: {loss_hist.value}\\n\")\n        \n    return all_losses\n        \n        \ndef validate_model(model, dataloader, device):\n    print(\"\\n Starting Validation ... \")\n    loss_hist = Averager()\n    itr = 1\n\n    loss_hist.reset() \n\n    for images, targets, image_ids in dataloader:\n\n        images = list(image.to(device) for image in images)\n        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\n        loss_dict = model(images, targets)\n\n        losses = sum(loss for loss in loss_dict.values())\n        loss_value = losses.item()\n\n        loss_hist.send(loss_value)\n\n        if itr % 50 == 0:\n            print(f\"Iteration #{itr} loss: {loss_value}\")\n\n        itr += 1\n\n    print(f\"\\nFinal loss: {loss_hist.value}\")\n","246e486b":"def run_fold(fold):\n    print(f\"Starting fold {fold}\")\n    start = time.time()\n    trn_idx = train_df[train['fold'] != fold].index\n    val_idx = train_df[train['fold'] == fold].index\n    \n    \n    trainloader, valloader = get_dataloaders(train, trn_idx, val_idx)\n    \n    loss_hist = train_model(model, trainloader, device, epochs, optimizer, lr_scheduler, fold)\n    \n    # plot training loss\n    plt.figure(figsize=(8,5))\n    plt.plot(loss_hist)\n    plt.title(\"Training Loss Statistic\", size=17)\n    plt.xlabel(\"Iteration\", size=15)\n    plt.ylabel(\"Loss Value\", size=15)\n    plt.show()\n    \n    validate_model(model, valloader, device)\n    \n    print(f\"Completed Fold {fold} in {round(time.time()-start, 2)} seconds\")","04a6f790":"model.to(device)\n\n# set params for model\nparams = [p for p in model.parameters() if p.requires_grad]\n\n# set optimizer\noptimizer = torch.optim.SGD(params, lr=0.005, momentum=0.9, weight_decay=0.0005)\n\n# set lr scheduler\nlr_scheduler = None\n\n# set epochs\nepochs = 20\n\n# set folds\nnum_folds = 1","e7de5549":"for fold in range(num_folds):\n    run_fold(fold)","047c60ce":"images, targets, image_ids = next(iter(valid_data_loader))\n\nimages = list(img.to(device) for img in images)\ntargets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n\nboxes = targets[1]['boxes'].cpu().numpy().astype(np.int32)\nsample = images[1].permute(1,2,0).cpu().numpy()\nclss = targets[1]['labels'].cpu().numpy().astype(np.int32)\n\nmodel.eval()\ncpu_device = torch.device(\"cpu\")\n\noutputs = model(images)\noutputs = [{k: v.to(cpu_device) for k, v in t.items()} for t in outputs]\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box, clas in zip(boxes, clss):\n    cv2.putText(sample, f\"{classes[clas]}\", (box[0], box[1]), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0,0,255), 1)\n    cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (220, 0, 0), 1)\n    \nax.set_axis_off()\nax.imshow(sample)","1b71f132":"!git clone https:\/\/github.com\/AlexeyAB\/darknet\n!wget https:\/\/github.com\/AlexeyAB\/darknet\/releases\/download\/darknet_yolo_v4_pre\/yolov4-csp.weights\n!git clone -b yolov4-csp https:\/\/github.com\/WongKinYiu\/ScaledYOLOv4.git","1462adbc":"!git clone https:\/\/github.com\/JunnYu\/mish-cuda.git\n%cd mish-cuda\n!python setup.py build install\n%cd ..","5fb19afd":"!git clone https:\/\/github.com\/WongKinYiu\/ScaledYOLOv4.git\n%cd .\/ScaledYOLOv4\/\n!git checkout yolov4-csp","8441da5d":"!python train.py --img 512 512 --batch-size 24 --epochs 20 --data ..\/vinbigdata.yaml --cfg yolov4-csp.cfg --weights ..\/yolov4-csp.weights --cache","72cce304":"!git clone https:\/\/github.com\/ultralytics\/yolov3.git\n%cd .\/yolov3\/\n!pip install -r requirements.txt","e7459edd":"!wget https:\/\/github.com\/ultralytics\/yolov3\/releases\/download\/v9.1\/yolov3.pt\n!wget https:\/\/github.com\/ultralytics\/yolov3\/releases\/download\/v9.1\/yolov3-spp.pt\n!wget https:\/\/github.com\/ultralytics\/yolov3\/releases\/download\/v9.1\/yolov3-tiny.pt\n","d7809693":"! wget https:\/\/www.python.org\/ftp\/python\/3.9.5\/Python-3.9.5.tar.xz tar xvf Python-3.9.5.tar.xz cd Python-3.9.5\/ .\/configure make altinstall\n","7a7eaa43":"!sudo apt install build-essential checkinstall \n!sudo apt install libreadline-gplv2-dev libncursesw5-devlibssl-dev libsqlite3-dev tk-dev libgdbm-dev libc6-dev libbz2-dev \n!sudo update-alternatives --install \/usr\/bin\/python python \/usr\/local\/bin\/python3.9.5 \n!sudo update-alternatives --install \/usr\/bin\/python python3 \/usr\/local\/bin\/python3.9.5","96497a0f":"\n!WANDB_MODE=\"dryrun\" python train.py --img {size} --batch-size 40 --epochs 60 --data ..\/vinbigdata.yaml --weights yolov3.pt\n","e1e2a61f":"!pip install matplotlib==3.1.3\nimport matplotlib.pyplot as plt","4af93c29":"! RUN update-alternatives --install \/usr\/bin\/python python \/usr\/bin\/python3.7 1","4b740d81":"! RUN update-alternatives --set python \/usr\/bin\/python3.7","489cd38d":"train.rename(columns = {\"dim0\": \"height\", \"dim1\":\"width\"},inplace = True)","8e5a9705":"__\ud30c\uc774\uc36c \uc5c5\uadf8\ub808\uc774\ub4dc__","dffaa283":"## Directory Tree for YOLO","dce51d49":"fold_0 = 1\nfold_1 = 2\ntrain_files = []\nval_files   = []","dfc80b2d":"## Bounding box","d486e722":"Class 'No Finding' has been droped","9f5b8950":"## Preprocessing :  K-fold","9c1e4a57":"## Test","d05a29bb":"## \ud3c9\uac00","aa8477f6":"classes = map(lambda x: str(x), classes)\nclasses","d8fe43ec":"## Visualize Model","d8a89701":"## YOLO-V4","91176958":"https:\/\/www.kaggle.com\/dhananjay3\/vinbigdata-eda-all-you-need-to-know","92ff696c":"## YOLO-V3","d909706d":"## FasterR CNN","006d7da1":"https:\/\/www.kaggle.com\/its7171\/map-understanding-with-code-and-its-tips","d8f66259":"## Training","77b2a02e":"## merge two csvs","4d6b20d4":"train_files += train[train.fold==fold_0].image_id.unique().tolist()\nval_files += train[train.fold==fold_1].image_id.unique().tolist()\nlen(train_files), len(val_files)","35c31fe7":"## Preprocessing"}}