{"cell_type":{"6a63877b":"code","73c0b0dd":"code","c19f43fe":"code","8a41104c":"code","3c40f5d4":"code","33861cee":"code","99294b5a":"code","a59b575c":"code","169e30b3":"code","b7720e56":"code","1dd5e6ca":"code","c5cf3656":"code","02ae8d43":"code","701d94bf":"code","ad2a277d":"markdown","b25c06be":"markdown","1737ed25":"markdown","37097703":"markdown","3680bff9":"markdown","2025bdff":"markdown","71c72827":"markdown","b5258b66":"markdown","de867f49":"markdown","67219f4e":"markdown","90e38377":"markdown","c1ba9dc1":"markdown","d7920508":"markdown"},"source":{"6a63877b":"%load_ext autoreload\n%autoreload 2\n\n%matplotlib inline\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport math\n\nimport warnings\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"","73c0b0dd":"gender_submission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntrain = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntrain.head()\ntrain.describe(include='all')\ntrain.info()","c19f43fe":"def convert_size(size_bytes):\n    if size_bytes == 0:\n        return \"0B\"\n    size_name = (\"Bytes\", \"KB\", \"MB\", \"GB\", \"TB\", \"PB\", \"EB\", \"ZB\", \"YB\")\n    i = int(math.floor(math.log(size_bytes, 1024)))\n    p = math.pow(1024, i)\n    s = round(size_bytes \/ p, 2)\n    return \"%s %s\" % (s, size_name[i])\nconvert_size(train.memory_usage().sum())","8a41104c":"# Let\u2019s plot the distribution of each feature\ndef plot_distribution(dataset, cols=5, width=20, height=15, hspace=0.2, wspace=0.5):\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure(figsize=(width,height))\n    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=wspace, hspace=hspace)\n    rows = math.ceil(float(dataset.shape[1]) \/ cols)\n    for i, column in enumerate(dataset.columns):\n        ax = fig.add_subplot(rows, cols, i + 1)\n        ax.set_title(column)\n        if dataset.dtypes[column] == np.object:\n            g = sns.countplot(y=column, data=dataset)\n            substrings = [s.get_text()[:18] for s in g.get_yticklabels()]\n            g.set(yticklabels=substrings)\n            plt.xticks(rotation=25)\n        else:\n            g = sns.distplot(dataset[column])\n            plt.xticks(rotation=25)\nplot_distribution(train.drop(labels=['Cabin', 'Name', 'Ticket'], axis=1).dropna(), cols=3, width=20, height=20, hspace=0.45, wspace=0.5)","3c40f5d4":"train['Survived'] = train.Survived.astype(str)\n\n# Plot a count of the categories from each categorical feature split by our prediction class: salary - predclass.\ndef plot_bivariate_bar(dataset, hue, cols=5, width=20, height=15, hspace=0.2, wspace=0.5):\n    dataset = dataset.select_dtypes(include=[np.object])\n    plt.style.use('seaborn-whitegrid')\n    fig = plt.figure(figsize=(width,height))\n    fig.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=wspace, hspace=hspace)\n    rows = math.ceil(float(dataset.shape[1]) \/ cols)\n    for i, column in enumerate(dataset.columns):\n        ax = fig.add_subplot(rows, cols, i + 1)\n        ax.set_title(column)\n        if dataset.dtypes[column] == np.object:\n            g = sns.countplot(y=column, hue=hue, data=dataset)\n            substrings = [s.get_text()[:10] for s in g.get_yticklabels()]\n            g.set(yticklabels=substrings)\n            \nplot_bivariate_bar(train.drop(labels=['Cabin', 'Name', 'Ticket'], axis=1).dropna(), hue='Survived', cols=3, width=20, height=12, hspace=0.4, wspace=0.5)","33861cee":"def add_datepart(df, fldname, drop=True):\n    fld = df[fldname]\n    if not np.issubdtype(fld.dtype, np.datetime64):\n        df[fldname] = fld = pd.to_datetime(fld, \n                                     infer_datetime_format=True)\n    targ_pre = re.sub('[Dd]ate$', '', fldname)\n    for n in ('Year', 'Month', 'Week', 'Day', 'Dayofweek', \n            'Dayofyear', 'Is_month_end', 'Is_month_start', \n            'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', \n            'Is_year_start'):\n        df[targ_pre+n] = getattr(fld.dt,n.lower())\n    df[targ_pre+'Elapsed'] = fld.astype(np.int64) \/\/ 10**9\n    if drop: df.drop(fldname, axis=1, inplace=True)","99294b5a":"def train_cats(df):\n    for col in df.columns:\n        if df[col].dtype == 'O' : \n            df[col] = df[col].astype('category').cat.as_ordered()","a59b575c":"def fix_missing(df):\n    for col in df.columns:\n        if (df[col].dtype == 'int64') or (df[col].dtype == 'float64') or (df[col].dtype == 'bool'):\n            if df[col].isnull().sum() != 0:\n                df[col + '_na'] = df[col].isnull()\n                df[col] = df[col].fillna(df[col].median())\n        else:\n            df[col + '_coded'] = df[col].cat.codes +1\n            df.drop(columns=[col], axis=1, inplace=True)","169e30b3":"def tree_visual(model, X_training, y_training):\n    model.fit(X_training, y_training)\n    # Extract single tree\n    estimator = model.estimators_[0]\n\n    from sklearn.externals.six import StringIO  \n    from IPython.display import Image  \n    from sklearn.tree import export_graphviz\n    import pydotplus\n    dot_data = StringIO()\n    export_graphviz(estimator, out_file=dot_data, feature_names=X_training.columns,  \n                    filled=True, rounded=True,\n                    special_characters=True)\n    graph = pydotplus.graph_from_dot_data(dot_data.getvalue())  \n    \n    return Image(graph.create_png())","b7720e56":"total = train.isnull().sum().sort_values(ascending=False)\npercent = ((train.isnull().sum()\/train.isnull().count())*100).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nf, ax = plt.subplots(figsize=(15, 6))\nplt.xticks(rotation='90')\nsns.barplot(x=missing_data.index, y=missing_data['Percent'])\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)\nmissing_data.head()","1dd5e6ca":"train['Survived'] = train.Survived.astype(int)\n\ncorr = train.select_dtypes(exclude=np.object).corr()\ncorr.style.background_gradient(cmap='coolwarm').set_precision(2)","c5cf3656":"def correlation_heatmap(df):\n    _ , ax = plt.subplots(figsize =(20, 20))\n    colormap = sns.diverging_palette(200, 10, as_cmap = True)\n    \n    _ = sns.heatmap(\n        df.corr(), \n        cmap = colormap,\n        square=True,\n        cbar_kws={'shrink':.9 },\n        ax=ax,\n        annot=True, \n        linewidths=0.1,vmax=1.0, linecolor='white',\n        annot_kws={'fontsize':12 }\n    )\n    \n    plt.title('Pearson Correlation of Features', y=1.05, size=15)\n\ncorrelation_heatmap(train.select_dtypes(exclude=np.object))","02ae8d43":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\ndata = train.select_dtypes(exclude=np.object).dropna()\nk_range = list(range(1, 26))\nk_scores = []\nfor k in k_range:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    scores = cross_val_score(knn, data.drop('Survived', axis=1), data.Survived, cv=10, scoring='accuracy')\n    mean_score = scores.mean()\n    k_scores.append(mean_score)\n\nplt.plot(k_range, k_scores)\nplt.xlabel('Value of k')\nplt.ylabel('Accuracy')","701d94bf":"from sklearn.model_selection import GridSearchCV\n\nX = data.drop('Survived', axis=1)\ny = data.Survived\nk_range = list(range(1, 31))\nweight_options = ['uniform', 'distance']\nparam_grid = dict(n_neighbors=k_range, weights=weight_options)\n\ngrid = GridSearchCV(knn, param_grid, scoring='accuracy', cv=10)\ngrid.fit(X, y)\n\ngrid_result = pd.DataFrame(grid.cv_results_)\ngrid_result.head()\ngrid.best_score_\ngrid.best_params_\ngrid.best_estimator_","ad2a277d":"## 1. Displaying the size of the Dataframe in Memory","b25c06be":"# Snippets of Important Codes","1737ed25":"## 10. Model performance Visualization","37097703":"## 5. Object to Category","3680bff9":"## 8. Seeing missing value percentages per column","2025bdff":"## 7. Visualizing a Tree","71c72827":"## 6. Fixing missing values","b5258b66":"## 9. Correlation Matrix","de867f49":"## Some imported datasets","67219f4e":"## 4. Exploiting a datetime feature","90e38377":"## 2. Basic Univariate Visualization code","c1ba9dc1":"## 11. Grid Search CV","d7920508":"## 3. Basic Bivariate Visualization code"}}