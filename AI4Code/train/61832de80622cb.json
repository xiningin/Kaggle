{"cell_type":{"e5d9d104":"code","bd5019f7":"code","b3f7b5d1":"code","238d6837":"code","f5bb9fd4":"code","9477819d":"code","9ad637d8":"code","3196fecc":"code","b1a73993":"code","c961aafc":"code","ee3bbbe8":"code","eb6a5ac5":"code","7ae7e4e6":"code","3c7714fe":"code","4231ae52":"code","d0245039":"code","8e664b33":"code","1e4eb507":"code","3614d937":"markdown","cbc4a8d6":"markdown","9d98a332":"markdown","bd1a221e":"markdown"},"source":{"e5d9d104":"!pip install git+https:\/\/github.com\/openai\/glide-text2im","bd5019f7":"from PIL import Image\nfrom IPython.display import display\nimport torch as th\n\nfrom glide_text2im.download import load_checkpoint\nfrom glide_text2im.model_creation import (\n    create_model_and_diffusion,\n    model_and_diffusion_defaults,\n    model_and_diffusion_defaults_upsampler\n)","b3f7b5d1":"# Code adapted from: \n# https:\/\/github.com\/openai\/glide-text2im\/blob\/main\/notebooks\/text2im.ipynb\n\n# Sampling parameters\n# Tune upsample_temp to control the sharpness of 256x256 images.\n# A value of 1.0 is sharper, but sometimes results in grainy artifacts.\nupsample_temp = 0.997\nbatch_size = 1\nguidance_scale = 3.0\n\ndef open_ai_glide(text_string_to_generate_image_from):\n    # This notebook supports both CPU and GPU.\n    # On CPU, generating one sample may take on the order of 20 minutes.\n    # On a GPU, it should be under a minute.\n    has_cuda = th.cuda.is_available()\n    device = th.device('cpu' if not has_cuda else 'cuda')\n    # Create base model.\n    options = model_and_diffusion_defaults()\n    options['use_fp16'] = has_cuda\n    options['timestep_respacing'] = '100' # use 100 diffusion steps for fast sampling\n    model, diffusion = create_model_and_diffusion(**options)\n    model.eval()\n    if has_cuda:\n        model.convert_to_fp16()\n    model.to(device)\n    model.load_state_dict(load_checkpoint('base', device))\n    print('total base parameters', sum(x.numel() for x in model.parameters()))\n    # Create upsampler model.\n    options_up = model_and_diffusion_defaults_upsampler()\n    options_up['use_fp16'] = has_cuda\n    options_up['timestep_respacing'] = 'fast27' # use 27 diffusion steps for very fast sampling\n    model_up, diffusion_up = create_model_and_diffusion(**options_up)\n    model_up.eval()\n    if has_cuda:\n        model_up.convert_to_fp16()\n    model_up.to(device)\n    model_up.load_state_dict(load_checkpoint('upsample', device))\n    print('total upsampler parameters', sum(x.numel() for x in model_up.parameters()))\n    def show_images(batch: th.Tensor):\n        \"\"\" Display a batch of images inline. \"\"\"\n        scaled = ((batch + 1)*127.5).round().clamp(0,255).to(th.uint8).cpu()\n        reshaped = scaled.permute(2, 0, 3, 1).reshape([batch.shape[2], -1, 3])\n        display(Image.fromarray(reshaped.numpy()))\n        \n    ##############################\n    # Sample from the base model #\n    ##############################\n\n    # Create the text tokens to feed to the model.\n    tokens = model.tokenizer.encode(prompt)\n    tokens, mask = model.tokenizer.padded_tokens_and_mask(\n        tokens, options['text_ctx']\n    )\n\n    # Create the classifier-free guidance tokens (empty)\n    full_batch_size = batch_size * 2\n    uncond_tokens, uncond_mask = model.tokenizer.padded_tokens_and_mask(\n        [], options['text_ctx']\n    )\n\n    # Pack the tokens together into model kwargs.\n    model_kwargs = dict(\n        tokens=th.tensor(\n            [tokens] * batch_size + [uncond_tokens] * batch_size, device=device\n        ),\n        mask=th.tensor(\n            [mask] * batch_size + [uncond_mask] * batch_size,\n            dtype=th.bool,\n            device=device,\n        ),\n    )\n\n    # Create a classifier-free guidance sampling function\n    def model_fn(x_t, ts, **kwargs):\n        half = x_t[: len(x_t) \/\/ 2]\n        combined = th.cat([half, half], dim=0)\n        model_out = model(combined, ts, **kwargs)\n        eps, rest = model_out[:, :3], model_out[:, 3:]\n        cond_eps, uncond_eps = th.split(eps, len(eps) \/\/ 2, dim=0)\n        half_eps = uncond_eps + guidance_scale * (cond_eps - uncond_eps)\n        eps = th.cat([half_eps, half_eps], dim=0)\n        return th.cat([eps, rest], dim=1)\n\n    # Sample from the base model.\n    model.del_cache()\n    samples = diffusion.p_sample_loop(\n        model_fn,\n        (full_batch_size, 3, options[\"image_size\"], options[\"image_size\"]),\n        device=device,\n        clip_denoised=True,\n        progress=True,\n        model_kwargs=model_kwargs,\n        cond_fn=None,\n    )[:batch_size]\n    model.del_cache()\n    \n    ##############################\n    # Upsample the 64x64 samples #\n    ##############################\n\n    tokens = model_up.tokenizer.encode(prompt)\n    tokens, mask = model_up.tokenizer.padded_tokens_and_mask(\n        tokens, options_up['text_ctx']\n    )\n\n    # Create the model conditioning dict.\n    model_kwargs = dict(\n        # Low-res image to upsample.\n        low_res=((samples+1)*127.5).round()\/127.5 - 1,\n\n        # Text tokens\n        tokens=th.tensor(\n            [tokens] * batch_size, device=device\n        ),\n        mask=th.tensor(\n            [mask] * batch_size,\n            dtype=th.bool,\n            device=device,\n        ),\n    )\n\n    # Sample from the base model.\n    model_up.del_cache()\n    up_shape = (batch_size, 3, options_up[\"image_size\"], options_up[\"image_size\"])\n    up_samples = diffusion_up.ddim_sample_loop(\n        model_up,\n        up_shape,\n        noise=th.randn(up_shape, device=device) * upsample_temp,\n        device=device,\n        clip_denoised=True,\n        progress=True,\n        model_kwargs=model_kwargs,\n        cond_fn=None,\n    )[:batch_size]\n    model_up.del_cache()\n    \n    # Show the output\n    show_images(up_samples)","238d6837":"prompt = \"a hedgehog\"\nopen_ai_glide(prompt)","f5bb9fd4":"prompt = \"an illustration of a hedgehog\"\nopen_ai_glide(prompt)","9477819d":"prompt = \"a hedgehog using a calculator\"\nopen_ai_glide(prompt)","9ad637d8":"prompt = \"a cat\"\nopen_ai_glide(prompt)","3196fecc":"prompt = \"an illustration of a cat\"\nopen_ai_glide(prompt)","b1a73993":"prompt = \"a cat using a calculator\"\nopen_ai_glide(prompt)","c961aafc":"prompt = \"a goose\"\nopen_ai_glide(prompt)","ee3bbbe8":"prompt = \"an illustration of a goose\"\nopen_ai_glide(prompt)","eb6a5ac5":"prompt = \"a goose using a calculator\"\nopen_ai_glide(prompt)","7ae7e4e6":"prompt = \"a goose holding a trophy\"\nopen_ai_glide(prompt)","3c7714fe":"prompt = \"a goose next to a computer\"\nopen_ai_glide(prompt)","4231ae52":"prompt = \"a computer\"\nopen_ai_glide(prompt)","d0245039":"prompt = \"an oil painting of a computer\"\nopen_ai_glide(prompt)","8e664b33":"prompt = \"a crayon drawing of a computer\"\nopen_ai_glide(prompt)","1e4eb507":"prompt = \"an illustration of a goose standing next to a computer\"\nopen_ai_glide(prompt)","3614d937":"# Generate images from text using OpenAI GLIDE","cbc4a8d6":"**GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models**\n - https:\/\/github.com\/openai\/glide-text2im\n - https:\/\/arxiv.org\/abs\/2112.10741\n ","9d98a332":"I was really impressed with the images that were included in the paper but when I try my own prompts the results are consistently much worse.","bd1a221e":"**GLIDE: Towards Photorealistic Image Generation and Editing with Text-Guided Diffusion Models**\n - Paper: https:\/\/arxiv.org\/abs\/2112.10741\n - Repo: https:\/\/github.com\/openai\/glide-text2im\n - Code adapted from https:\/\/github.com\/openai\/glide-text2im\/blob\/main\/notebooks\/text2im.ipynb\n "}}