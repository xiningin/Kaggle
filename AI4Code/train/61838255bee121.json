{"cell_type":{"a39c89b7":"code","d6c711c7":"code","f35c02a6":"code","91337125":"code","d920bb9a":"code","586a1756":"code","f5114a3f":"code","02658be6":"code","3fe12510":"code","27091dbd":"code","08cce691":"code","b61e0506":"code","f013708d":"code","4794a708":"code","2c157277":"code","acb09e55":"markdown","7e33e5c7":"markdown","8319615f":"markdown","a267e8b9":"markdown","0a5a5142":"markdown","636bf39d":"markdown","c80f56bf":"markdown"},"source":{"a39c89b7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d6c711c7":"import gc\nimport random\n\nfrom IPython import display as ipd\nfrom tqdm import tqdm\nimport xgboost as xgb\n\nfrom sklearn.preprocessing import MinMaxScaler, RobustScaler, LabelEncoder\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, KFold,GroupKFold\n\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, f1_score, confusion_matrix\nfrom sklearn.ensemble import RandomForestRegressor\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set_theme(style=\"whitegrid\")\n\nimport optuna \nfrom optuna.visualization.matplotlib import plot_optimization_history\nfrom optuna.visualization.matplotlib import plot_param_importances","f35c02a6":"def seeding(SEED, use_tf=False):\n    np.random.seed(SEED)\n    random.seed(SEED)\n    os.environ['PYTHONHASHSEED'] = str(SEED)\n    os.environ['TF_CUDNN_DETERMINISTIC'] = str(SEED)\n    if use_tf:\n        tf.random.set_seed(SEED)\n    print('seeding done!!!')\n    \n## https:\/\/www.kaggle.com\/c\/tabular-playground-series-jan-2022\/discussion\/298201\ndef SMAPE(y_true, y_pred):\n    denominator = (y_true + np.abs(y_pred)) \/ 200.0\n    diff = np.abs(y_true - y_pred) \/ denominator\n    diff[denominator == 0] = 0.0\n    return np.mean(diff)    ","91337125":"RANDOM_SEED = 42\nTUNING = False\n\nseeding(RANDOM_SEED)\n\ntrain = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/test.csv')\nsubmission = pd.read_csv('\/kaggle\/input\/tabular-playground-series-jan-2022\/sample_submission.csv')","d920bb9a":"train.head()","586a1756":"def process_dates(df):\n    df.date = pd.to_datetime(df.date)\n    df['month'] = df.date.dt.month\n    df['week'] = df.date.dt.week\n    df['weekday'] = df.date.dt.weekday\n    df['dayofweek'] = df.date.dt.dayofweek\n    df['dayofyear'] = df.date.dt.dayofyear\n    df['day'] = df.date.dt.day\n    return df\n\ntrain = process_dates(train)\ntest = process_dates(test)","f5114a3f":"print(f'Train unique days: {train.day.unique().size}, test: {test.day.unique().size}')\nprint(f'Train unique weeks: {train.week.unique().size}, test: {test.week.unique().size}')\nprint(f'Train unique dayofweeks: {train.dayofweek.unique().size}, test: {train.dayofweek.unique().size}')\nprint(f'Train unique months: {train.month.unique().size}, test: {train.month.unique().size}')\nprint(f'Train unique dayofyear: {train.dayofyear.unique().size}, test: {train.dayofyear.unique().size}')","02658be6":"target = train.num_sold\ntrain.drop(['row_id','num_sold','date'], axis=1, inplace=True)\ntest.drop(['row_id', 'date'], axis=1, inplace=True)","3fe12510":"country_encoder = LabelEncoder()\ntrain['country_enc'] = country_encoder.fit_transform(train['country'])\ntest['country_enc'] = country_encoder.transform(test['country'])\n\nstore_encoder = LabelEncoder()\ntrain['store_enc'] = store_encoder.fit_transform(train['store'])\ntest['store_enc'] = store_encoder.transform(test['store'])\n\nproduct_encoder = LabelEncoder()\ntrain['product_enc'] = product_encoder.fit_transform(train['product'])\ntest['product_enc'] = product_encoder.transform(test['product'])\n\ntrain.drop(['country','store','product'], axis=1, inplace=True)\ntest.drop(['country','store','product'], axis=1, inplace=True)","27091dbd":"#for col in train.columns:\n#    train[col] = pd.Categorical(train[col])\n#for col in test.columns:\n#    test[col] = pd.Categorical(test[col])","08cce691":"NUM_BOOST_ROUND = 1000\nEARLY_STOPPING_ROUNDS = 20\nVERBOSE_EVAL = 100\n    \ndef objective(trial, X, y):\n    \n    param_grid = {\n        'verbosity': 1,\n        'objective': 'reg:squarederror', \n        'eval_metric': 'rmse',\n        'learning_rate': trial.suggest_float('learning_rate', 0.0001, 0.1),\n        'eta': trial.suggest_float('eta', 0.1, 0.9),\n        'max_depth': trial.suggest_int('max_depth', 50, 500),     \n        'min_child_weight': trial.suggest_float('min_child_weight', 10, 100),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 0.9),\n        'gamma': trial.suggest_float('gamma', 0, 100),\n        'subsample': trial.suggest_float('subsample', 0.5, 0.9),\n        'lambda': trial.suggest_float('lambda', 1, 10),\n        'alpha': trial.suggest_float('alpha', 0, 9),\n    }    \n        \n    X_train, X_valid, y_train, y_valid = train_test_split( X, y, test_size=0.25, random_state=RANDOM_SEED, shuffle=False)\n    \n    dtrain = xgb.DMatrix(X_train, label=y_train)\n    dvalid = xgb.DMatrix(X_valid, label=y_valid)\n    model = xgb.train( param_grid, dtrain,\n        num_boost_round = NUM_BOOST_ROUND,\n        evals=[(dvalid, 'evals')], \n        verbose_eval = VERBOSE_EVAL,\n        early_stopping_rounds=EARLY_STOPPING_ROUNDS\n    )   \n    \n    oof_pred = model.predict(dvalid)\n    oof_score = SMAPE(y_valid, oof_pred) \n    print(f\"OOF SMAPE: {oof_score}\")\n    return oof_score","b61e0506":"N_TRIALS = 100\n\nif TUNING:\n    study = optuna.create_study(direction='minimize')\n    objective_func = lambda trial: objective(trial, train, target)\n    study.optimize(objective_func, n_trials=N_TRIALS)  # number of iterations\n\n    print(\"Number of finished trials: {}\".format(len(study.trials)))\n    print(\"Best trial:\")\n    trial = study.best_trial\n    print(\"  Value: {}\".format(trial.value))\n    print(\"  Params: \")\n    for key, value in trial.params.items():\n        print(\"    {}: {}\".format(key, value))","f013708d":"def run_train(X, y, run_params, splits, num_boost_round, verbose_eval, early_stopping_rounds ):\n    scores = []\n    models = []\n    folds = StratifiedKFold(n_splits=splits)\n    for fold_n, (train_index, valid_index) in enumerate(folds.split(X, y)):\n        print(f'Fold {fold_n+1} started')\n        X_train, X_valid = X.iloc[train_index], X.iloc[valid_index]\n        y_train, y_valid = y.iloc[train_index], y.iloc[valid_index]\n        \n        dtrain = xgb.DMatrix(X_train, label=y_train)\n        dvalid = xgb.DMatrix(X_valid, label=y_valid)\n        model = xgb.train( run_params, dtrain,\n            num_boost_round = num_boost_round,\n            evals=[(dvalid, 'evals')], \n            verbose_eval = verbose_eval,\n            early_stopping_rounds=early_stopping_rounds\n        )   \n\n        oof_pred = model.predict(dvalid)\n        oof_score = SMAPE(y_valid, oof_pred) \n        print(f\"OOF SMAPE: {oof_score}\")        \n        \n        models.append(model)\n        scores.append(oof_score)\n    return scores, models\n\n\nNUM_BOOST_ROUND = 2000\nEARLY_STOPPING_ROUNDS = 100\nVERBOSE_EVAL = 100\nTOTAL_SPLITS = 5\n    \nrun_params = {\n    'verbosity': 1,\n    'objective': 'reg:squarederror', \n    'eval_metric': 'rmse',\n    'learning_rate': 0.01729433116660487,\n    'eta': 0.4954283685809021,\n    'max_depth': 476,\n    'min_child_weight': 12.875223150484498,\n    'colsample_bytree': 0.7890238951483045,\n    'gamma': 96.89423371529557,\n    'subsample': 0.8862703289885544,\n    'lambda': 8.869246442053491,\n    'alpha': 4.132837689865073,\n}\n\nFEATURES = [col for col in train.columns if col.endswith('enc')]\nscores, models = run_train(train, target, run_params, TOTAL_SPLITS, NUM_BOOST_ROUND, \n                                          VERBOSE_EVAL, EARLY_STOPPING_ROUNDS)\n\nprint('----------------------')\nprint(f'CV SMAPE mean score: {np.mean(scores)}, std: {np.std(scores)}.')\nprint('----------------------')","4794a708":"y_pred = np.zeros(len(test))\nfor model in models:\n    y_pred += model.predict(xgb.DMatrix(test)).reshape(-1)\n    \ny_pred = y_pred \/ len(models)","2c157277":"submission['num_sold'] = np.round(y_pred).astype(int)\nsubmission.to_csv('submission.csv', index=False, float_format='%.6f')\nsubmission.head(20)","acb09e55":"### Encode category columns ","7e33e5c7":"### Utils","8319615f":"### Data Load","a267e8b9":"### Tune","0a5a5142":"### Date-based FE","636bf39d":"### Model and train","c80f56bf":"## Targets distribution display\n\nCheck my other notebooks: \n\nhttps:\/\/www.kaggle.com\/vladlee\/tps-jan2022-lgbm-optuna\n\nhttps:\/\/www.kaggle.com\/vladlee\/tps-jan-2022-eda-baseline\n"}}