{"cell_type":{"31be24c9":"code","46913035":"code","297757b7":"code","0a72b70e":"code","06aef309":"code","82324161":"code","fd3a1157":"code","644c0a1f":"code","d84c389b":"code","58729c48":"code","3378256b":"code","3d943164":"code","97f5aa56":"code","07443c96":"code","a65064f1":"code","c12d941d":"code","14021fc9":"code","6f00199f":"markdown","b824c1e9":"markdown","9bf1fac1":"markdown","ef4748b4":"markdown","e77e7aa9":"markdown","a17b041a":"markdown","2740cc2d":"markdown","ec96cb3d":"markdown","7bbe7fe3":"markdown","a7022c63":"markdown","07c2aaaa":"markdown","1a9d7fc8":"markdown","2a4c7d65":"markdown"},"source":{"31be24c9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder,PolynomialFeatures\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.linear_model import LogisticRegression, SGDClassifier\nfrom sklearn.model_selection import  StratifiedShuffleSplit,RandomizedSearchCV,GridSearchCV,StratifiedKFold, cross_val_score, cross_val_predict\nfrom sklearn.base import clone\nfrom sklearn.metrics import confusion_matrix, precision_score,recall_score,f1_score\nfrom sklearn.ensemble import RandomForestClassifier,AdaBoostClassifier, GradientBoostingClassifier,VotingClassifier\nfrom sklearn.svm import SVC,LinearSVC\nimport xgboost\nfrom re import sub\n\n\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session\n\n","46913035":"\ntrain_data = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\").drop('Survived',axis=1)\ntrain_labels = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")['Survived']\n\n# only keep letters for Cabin \ntrain_data['Cabin'] = train_data['Cabin'].astype(str) \nzonder_cijfers = [sub('[^a-zA-Z]+', '', x) for x in train_data['Cabin']]\ntrain_data['Cabin'] = zonder_cijfers","297757b7":"num_pipeline = Pipeline([\n    ('imputer', SimpleImputer(strategy=\"median\")),\n    ('std_scaler', StandardScaler()),\n])\n\nnum_attribs = ['Pclass',  'Age', 'SibSp', 'Parch','Fare']\ncat_attribs = [\"Sex\", \"Embarked\", \"Cabin\"]\n\nfull_pipeline = ColumnTransformer([\n    (\"numerical\", num_pipeline, num_attribs),\n    (\"categorical\", OneHotEncoder(),cat_attribs)\n],sparse_threshold=0)\n\nX_prepared =  full_pipeline.fit_transform(train_data)\nX_prepared_df = pd.DataFrame(X_prepared)\n","0a72b70e":"kfold = 5  # number of cross-validations","06aef309":"forest_clf = RandomForestClassifier(random_state=42, max_depth = 13)\nforest_clf.fit(X_prepared_df,train_labels)\nforest_accuracies = cross_val_score(forest_clf, X_prepared_df,train_labels,cv = kfold, scoring = \"accuracy\")\nprint(\"Accuracies for\",kfold,\"-fold validation : ,\",forest_accuracies)\nprint(\"Mean accuracy:\", forest_accuracies.mean(0))\n","82324161":"log_reg = LogisticRegression(random_state=42)\nlog_reg.fit(X_prepared_df,train_labels)\nlog_accuracies = cross_val_score(log_reg, X_prepared_df,train_labels,cv = kfold,scoring=\"accuracy\")\nprint(\"Accuracies for\",kfold,\"-fold validation : ,\",log_accuracies)\nprint(\"Mean accuracy:\", log_accuracies.mean(0))\n","fd3a1157":"gradient_boost = GradientBoostingClassifier(random_state=42,max_depth = 4,subsample = 0.45,warm_start = True)\ngradient_boost.fit(X_prepared_df,train_labels)\ngradient_accuracies = cross_val_score(gradient_boost, X_prepared_df,train_labels,cv = kfold,scoring=\"accuracy\")\nprint(\"Accuracies for\",kfold,\"-fold validation : ,\",gradient_accuracies)\nprint(\"Mean accuracy:\", gradient_accuracies.mean(0))","644c0a1f":"split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)\nfor train_index, val_index in split.split(X_prepared_df, train_labels):\n    strat_train_X = X_prepared_df.loc[train_index]\n    strat_train_y = train_labels.loc[train_index]\n    strat_val_X = X_prepared_df.loc[val_index]\n    strat_val_y = train_labels.loc[val_index]\n\nprint('strat_train_X.shape:',strat_train_X.shape)\nprint('strat_train_y.shape:',strat_train_y.shape)\nprint('strat_val_X.shape:',strat_val_X.shape)\nprint('strat_val_y.shape:',strat_val_y.shape)","d84c389b":"xgboost_clf = xgboost.XGBClassifier(use_label_encoder=False)\nxgboost_clf.fit(strat_train_X, strat_train_y, eval_set=[(strat_val_X,strat_val_y)], early_stopping_rounds= 4)\nxgboost_accuracies = cross_val_score(xgboost_clf, X_prepared_df,train_labels,cv = kfold,scoring=\"accuracy\")\nprint(\"Accuracies for\",kfold,\"-fold validation : ,\",xgboost_accuracies)\nprint(\"Mean accuracy:\", xgboost_accuracies.mean(0))","58729c48":"adaboost = AdaBoostClassifier(random_state = 42, n_estimators = 150)\nadaboost.fit(X_prepared_df, train_labels)\nadaboost_accuracies = cross_val_score(adaboost, X_prepared_df,train_labels,cv = kfold,scoring=\"accuracy\")\nprint(\"Accuracies for\",kfold,\"-fold validation : ,\",adaboost_accuracies)\nprint(\"Mean accuracy:\", adaboost_accuracies.mean(0))","3378256b":"\n#param_grid = [{'kernel':['linear','poly','rbf'],'C': [2,3,6,10]},{'degree': [2, 5, 8], 'coef0': [0,0.4,0.8,1]}]\n#C_range = np.logspace(1e-2, 1e2, 5)\n#gamma_range = np.logspace(1e-3, 1, 4)\n#param_grid =[{'gamma':[0.12,0.11,0.10]}]\n\n\n#grid_search_svm = GridSearchCV(rbf_kernel_svm_clf,param_grid,cv = 5, return_train_score = True)\n#grid_search_svm.fit(X_prepared_df, train_labels)\n#grid_search_svm.best_params_\n\n","3d943164":"SVM_clf = SVC(probability=True, C=2)\nSVM_clf.fit(X_prepared_df, train_labels)\nSVM_accuracies = cross_val_score(SVM_clf, X_prepared_df,train_labels,cv = kfold,scoring=\"accuracy\")\nprint(\"Accuracies for\",kfold,\"-fold validation : ,\",SVM_accuracies)\nprint(\"Mean accuracy:\", SVM_accuracies.mean(0))\n\n","97f5aa56":"poly_kernel_svm_clf = SVC(kernel=\"poly\", degree=2, coef0=1, C=4,probability=True)\npoly_kernel_svm_clf.fit(X_prepared_df, train_labels)\n\npoly_kernel_accuracies = cross_val_score(poly_kernel_svm_clf, X_prepared_df,train_labels,cv = kfold,scoring=\"accuracy\")\nprint(\"Accuracies for\",kfold,\"-fold validation : ,\",poly_kernel_accuracies)\nprint(\"Mean accuracy:\", poly_kernel_accuracies.mean(0))","07443c96":"voting_clf = VotingClassifier(estimators=[('logreg', log_reg),('randforest', forest_clf) ,('xgboost',xgboost_clf),('gradboost', gradient_boost), ('RBF SVM', SVM_clf)],voting='soft')\nvoting_clf.fit(X_prepared_df, train_labels)\nvoting_accuracies = cross_val_score(voting_clf, X_prepared_df,train_labels,cv = kfold,scoring=\"accuracy\")\nprint(\"Accuracies for\",kfold,\"-fold validation : ,\",voting_accuracies)\nprint(\"Mean accuracy:\", voting_accuracies.mean(0))","a65064f1":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\n\n# only keep letters for Cabin \ntest_data['Cabin'] = test_data['Cabin'].astype(str) \nzonder_cijfers_test = [sub('[^a-zA-Z]+', '', x) for x in test_data['Cabin']]\ntest_data['Cabin'] = zonder_cijfers_test","c12d941d":"X_test_prepared =  full_pipeline.transform(test_data)\nX_test_prepared_df = pd.DataFrame(X_test_prepared)\n\npredictions = pd.DataFrame(voting_clf.predict(X_test_prepared_df))","14021fc9":"#submission = test_data['PassengerId'].append(predictions)\nsubmission =  pd.concat([test_data['PassengerId'], predictions],axis=1)\nsubmission.columns = ['PassengerId','Survived']\nsubmission.shape\nsubmission.to_csv('\/kaggle\/working\/submission.csv', header = True, index=False)","6f00199f":"Polynomial kernel SVM","b824c1e9":"Soft-voting ensemble Classifier","9bf1fac1":"Support Vector Machine (RBF kernel)","ef4748b4":"AdaBoost","e77e7aa9":"Making training data and labels","a17b041a":"Random Forest Classifier","2740cc2d":"Gradient Boosting","ec96cb3d":"Gradient Boosting (XGBoost)","7bbe7fe3":"Logistic Regression","a7022c63":"*Create pipeline that replaces missing values with median and then standardizes for numerical attributes and does one-hot-encoding for categorical attributes*","07c2aaaa":"Predictions","1a9d7fc8":"**Data preprocessing** \n","2a4c7d65":"Hyperparameter fine-tuning of SVM through grid-search"}}