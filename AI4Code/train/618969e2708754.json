{"cell_type":{"4e9f9ad1":"code","34cb2f13":"code","b12a2ec2":"code","7ef15eed":"code","6860db1c":"code","b15db92d":"code","8ba6ba98":"code","df07bac5":"code","648884c6":"code","ed8b1bb0":"code","83b1a862":"code","ba688e54":"code","dfe25fe5":"markdown","ba7eed44":"markdown","3d5df86e":"markdown","45e1b262":"markdown","f4c0899e":"markdown","32e33fa6":"markdown","caa630ae":"markdown"},"source":{"4e9f9ad1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfrom tqdm import tqdm\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in tqdm(filenames):\n        pass\n        #print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","34cb2f13":"import os\nfrom os.path import join\nimport cv2\nimport torch\nimport torchvision\nimport torchvision.transforms as transforms\nfrom glob import glob\nfrom PIL import Image\nfrom torchvision import models\n\nbatch_size = 32\ntotal_epoch = 5\nmethod = 'VGG' # (ResNet | VGG | GoogLeNet)","b12a2ec2":"if torch.cuda.is_available():\n    print(\"CUDA is available\")\n    device = 'cuda'\nelse:\n    print(\"CUDA is NOT available\")\n    device = 'cpu'\n\ntorch.manual_seed(777)\nif device == 'cuda':\n    torch.cuda.manual_seed_all(777)","7ef15eed":"class W10_DateLoader(torch.utils.data.Dataset):\n    def __init__(self, data_path, split, transform=None):\n        self.split = split.upper()\n        assert self.split in {'TRAIN', 'TEST'}\n        self.transform = transform\n        self.data = data_path\n        if self.split == \"TRAIN\":\n            self.label = [int(p.split('\/')[-2]) for p in data_path]\n        self.data_len = len(self.data)\n            \n    def __len__(self):\n        return self.data_len \n\n    def __getitem__(self, index):\n        image = Image.open(self.data[index], mode='r')\n        image = image.convert('RGB')\n        \n        if self.transform:\n            image = self.transform(image)\n            \n        if self.split == \"TEST\":\n            return image\n        elif self.split == \"TRAIN\":\n            self.label[index] = np.array(self.label[index])\n            return image, torch.from_numpy(self.label[index])\n\n    \ntransform = transforms.Compose(\n    [transforms.Resize((224, 224)),\n     transforms.ToTensor(),\n     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n\ntrain_mother_path = '\/kaggle\/input\/2021-ai-w10-p2\/images\/images\/'\ntest_mother_path = '\/kaggle\/input\/2021-ai-w10-p2\/test_data\/test_data'\ntrain_image_path = glob(join(train_mother_path, '*', '*'))\ntest_image_path = glob(join(test_mother_path, '*'))\ntest_paths=[]\nfor i in range(len(test_image_path)):\n    test_paths.append(test_mother_path+'\/'+str(i)+'.png')\n\ntrainData = W10_DateLoader(train_image_path, 'train', transform=transform)\ntestData = W10_DateLoader(test_paths, 'test', transform=transform)\n\ntrainLoader = torch.utils.data.DataLoader(trainData, batch_size=batch_size, shuffle=True, num_workers=4)\ntestLoader = torch.utils.data.DataLoader(testData, batch_size=batch_size, shuffle=False, num_workers=4)","6860db1c":"# (ResNet | VGG | GoogLeNet)\n\"\"\"\npretrained=True \uc635\uc158\uc744 \ud1b5\ud574 pretrain weight\ub97c \ubd88\ub7ec\uc628\ub2e4.\n\"\"\"\nif method is \"ResNet\":\n    model = models.resnet18(pretrained=True)\n    # output layer \ucd9c\ub825 \ud615\ud0dc \ubcc0\ud658\n    model_classifier = torch.nn.Linear(1024, 10)\n    torch.nn.init.xavier_uniform_(model_classifier.weight)\n    model.fc = model_classifier\n    model.to(device)             \nif method is \"VGG\":\n    model = models.vgg16(pretrained=True)\n    # output layer \ucd9c\ub825 \ud615\ud0dc \ubcc0\ud658\n    model_classifier = torch.nn.Linear(4096, 10)\n    torch.nn.init.xavier_uniform_(model_classifier.weight)\n    model.classifier[6].out_features = model_classifier\n    model.to(device)\nif method is \"GoogLeNet\":\n    model = models.googlenet(pretrained=True)\n    # output layer \ucd9c\ub825 \ud615\ud0dc \ubcc0\ud658\n    model_classifier = torch.nn.Linear(1024, 10)\n    torch.nn.init.xavier_uniform_(model_classifier.weight)\n    model.fc = model_classifier\n    model.to(device)\n    \n# # \uc785\ucd9c\ub825 \ub808\uc774\uc5b4 \uc81c\uc678 \ud30c\ub77c\ubbf8\ud130 freezing\n# firstlayer=0\n# block = 0\n# for child in model.children():\n#     if block == 2:\n#         for param in child.parameters():\n#             param.requires_grad = False\n\n#     for param in child.parameters():\n#         if firstlayer == 0:\n#             pass\n#         else:\n#             param.requires_grad = False\n#         firstlayer+=1\n#     block+=1","b15db92d":"import torch\nimport torchvision\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport time\nimport torch.nn.functional as F\nimport torch.nn as nn\nimport matplotlib.pyplot as plt\nfrom torchvision import models\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)","8ba6ba98":"# training function\ndef train(model, train_dataloader):\n    model.train()\n    train_running_loss = 0.0\n    train_running_correct = 0\n    for i, data in enumerate(train_dataloader):\n        data, target = data[0].to(device), data[1].to(device)\n        optimizer.zero_grad()\n        output = model(data)\n        loss = criterion(output, target)\n        train_running_loss += loss.item()\n        _, preds = torch.max(output.data, 1)\n        train_running_correct += (preds == target).sum().item()\n        loss.backward()\n        optimizer.step()\n    train_loss = train_running_loss\/len(train_dataloader.dataset)\n    train_accuracy = 100. * train_running_correct\/len(train_dataloader.dataset)\n \n    print(f'Train Loss: {train_loss:.4f}, Train Acc: {train_accuracy:.2f}')\n    \n    return train_loss, train_accuracy","df07bac5":"train_loss , train_accuracy = [], []\nval_loss , val_accuracy = [], []\nstart = time.time()\nfor epoch in range(total_epoch):\n    train_epoch_loss, train_epoch_accuracy = train(model, trainLoader)\n    train_loss.append(train_epoch_loss)\n    train_accuracy.append(train_epoch_accuracy)\n    print(total_epoch, train_accuracy, val_accuracy)\n    \nend = time.time()\n \nprint((end-start)\/60, 'minutes')","648884c6":"# training function\ndef test(model, test_dataloader):\n    model.eval()\n    preds=[]\n    for i, data in tqdm.tqdm(enumerate(test_dataloader)):\n        data = data.to(device)\n        output = model(data)\n        _, pred = torch.max(output.data, 1)\n        preds.extend(pred.detach().cpu().tolist())\n    \n    return preds","ed8b1bb0":"testLoader = torch.utils.data.DataLoader(testData, batch_size=1, shuffle=False, num_workers=4)","83b1a862":"import tqdm\nsample_submit_csv = pd.read_csv('\/kaggle\/input\/2021-ai-w10-p2\/format.csv')\npreds = test(model, testLoader)","ba688e54":"sample_submit_csv['label'] = preds\nsample_submit_csv.to_csv(\"{}_pred.csv\".format(method), index=False)","dfe25fe5":"# \ud559\uc2b5 \uae30\ubcf8 \ubcc0\uc218 \uc14b\ud305","ba7eed44":"# \ud3c9\uac00","3d5df86e":"# \ubaa8\ub378 \ud559\uc2b5","45e1b262":"# \ub370\uc774\ud130 \ub85c\ub354","f4c0899e":"```python\n# TrainSet\uc758 \ud3f4\ub354 \uad6c\uc131\n\n\u251c\u2500 images  \n\u2502 \u251c\u2500 0 # (label name)  \n\u2502 \u2502 \u251c\u2500 aeroplane_s_000004.png # (image name)  \n\u2502 \u2502 \u251c\u2500 aeroplane_s_0000021.png  \n\u2502 \u2502 \u2514\u2500 aeroplane_s_000022.png  \n\u2502 \u251c\u2500 1  \n\u2502 \u2502 \u251c\u2500 ambulance_s_000101.png  \n\u2502 \u2502 \u251c\u2500 ambulance_s_000204.png  \n\u2502 \u2502 \u2514\u2500 ambulance_s_000266.png  \n\u2502 \u2502  \n\u2502 . . .  \n\u2502 \u251c\u2500 9  \n\u2502 \u2502 \u251c\u2500 . . .  \n\u2502 \u2502 \u2514\u2500 aerial_ladder_truck_s_000042.png  \n\u2502 \u2514\u2500  \n\u2514\u2500    \n```","32e33fa6":"# GPU \uc0ac\uc6a9 \uac00\ub2a5 \uc5ec\ubd80 \ud655\uc778 \ubc0f \ub80c\ub364 seed \uace0\uc815","caa630ae":"# \uc0ac\uc804\ud559\uc2b5 \ubaa8\ub378 \uc0ac\uc6a9 (LeNet, VGG, ResNet)"}}