{"cell_type":{"c63fcf65":"code","2925edf0":"code","181d2641":"code","b5c4c21a":"code","1458879f":"code","1f0e3770":"code","afbe0e70":"code","53a157ad":"code","239bbed3":"code","bcc08fd6":"code","35352024":"code","e627bb98":"code","03b583c6":"code","7cdce1ca":"code","e6805ecd":"code","97b2ddcb":"code","f0abdc5d":"code","d0aa8c36":"code","45ad770a":"code","0377e843":"code","6be00b97":"code","7baa2fa7":"code","6c6a4b37":"code","3c851b35":"code","c9d5c658":"code","199fb069":"code","ea5a0763":"markdown","f5e9abb3":"markdown","d9947e08":"markdown","0f6d462c":"markdown","47e87f84":"markdown","3dbce32e":"markdown","8a3fe70e":"markdown","9ee8a134":"markdown","605619e3":"markdown","6e8e1367":"markdown","18f287d3":"markdown","a650ce2d":"markdown","b60abdaa":"markdown","05d77b0f":"markdown","8261b280":"markdown","0d825477":"markdown","9d0acba9":"markdown","eeb8dc98":"markdown","ddde4014":"markdown","757aaa59":"markdown","ad8c267b":"markdown","17bfe702":"markdown","b0f2291e":"markdown","e8b925eb":"markdown","359bace1":"markdown"},"source":{"c63fcf65":"# import libraries\nimport numpy as np\nimport pandas as pd\nimport time\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 100)\n\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\n# Function to plot feature importance\ndef plot_features(booster, figsize):    \n    fig, ax = plt.subplots(1,1,figsize=figsize)\n    return plot_importance(booster=booster, ax=ax)\nimport matplotlib.pyplot as plt\n","2925edf0":"# Read train , test and submission csv in pandas dataframe\ntrain=pd.read_csv('\/kaggle\/input\/train.csv',parse_dates=['DateTime'])\ntest=pd.read_csv('\/kaggle\/input\/test.csv',parse_dates=['DateTime'])\nsub=pd.read_csv('\/kaggle\/input\/sub.csv')","181d2641":"# let's check first 5 rows from train \ntrain.head()","b5c4c21a":"# let's check last 5 rows from train \ntrain.tail()","1458879f":"# let's explore test data's first 5 and last 5 row\ntest.head().append(test.tail())","1f0e3770":"train.loc[:,['DateTime','Vehicles']].plot(x='DateTime',y='Vehicles',title='Vehicle Trend',figsize=(16,4))","afbe0e70":"# filtering data greater than or equal to 01 Jan 2016\ntrain=train[train['DateTime']>='2016-01-01']","53a157ad":"# concat train, test data and mark where it is test , train \ntrain['train_or_test']='train'\ntest['train_or_test']='test'\ndf=pd.concat([train,test])","239bbed3":"# Below function extracts date related features from datetime\ndef create_date_featues(df):\n\n    df['Year'] = pd.to_datetime(df['DateTime']).dt.year\n\n    df['Month'] = pd.to_datetime(df['DateTime']).dt.month\n\n    df['Day'] = pd.to_datetime(df['DateTime']).dt.day\n\n    df['Dayofweek'] = pd.to_datetime(df['DateTime']).dt.dayofweek\n\n    df['DayOfyear'] = pd.to_datetime(df['DateTime']).dt.dayofyear\n\n    df['Week'] = pd.to_datetime(df['DateTime']).dt.week\n\n    df['Quarter'] = pd.to_datetime(df['DateTime']).dt.quarter \n\n    df['Is_month_start'] = pd.to_datetime(df['DateTime']).dt.is_month_start\n\n    df['Is_month_end'] = pd.to_datetime(df['DateTime']).dt.is_month_end\n\n    df['Is_quarter_start'] = pd.to_datetime(df['DateTime']).dt.is_quarter_start\n\n    df['Is_quarter_end'] = pd.to_datetime(df['DateTime']).dt.is_quarter_end\n\n    df['Is_year_start'] = pd.to_datetime(df['DateTime']).dt.is_year_start\n\n    df['Is_year_end'] = pd.to_datetime(df['DateTime']).dt.is_year_end\n\n    df['Semester'] = np.where(df['Quarter'].isin([1,2]),1,2)\n\n    df['Is_weekend'] = np.where(df['Dayofweek'].isin([5,6]),1,0)\n\n    df['Is_weekday'] = np.where(df['Dayofweek'].isin([0,1,2,3,4]),1,0)\n    \n    df['Hour'] = pd.to_datetime(df['DateTime']).dt.hour\n    \n    return df","bcc08fd6":"# extracting time related \ndf=create_date_featues(df)","35352024":"for col in ['Junction']:\n    df = pd.get_dummies(df, columns=[col])","e627bb98":"train=df.loc[df.train_or_test.isin(['train'])]\ntest=df.loc[df.train_or_test.isin(['test'])]\ntrain.drop(columns={'train_or_test'},axis=1,inplace=True)\ntest.drop(columns={'train_or_test'},axis=1,inplace=True)","03b583c6":"train['Vehicles']=np.log1p(train['Vehicles'])","7cdce1ca":"train1=train[train['DateTime']<'2017-03-01']#Train period from 2016-01-01 to 2017-02-31\nval1=train[train['DateTime']>='2017-03-01'] #Month 3,4,5,6 as validtaion period","e6805ecd":"def datetounix(df):\n    # Initialising unixtime list\n    unixtime = []\n    \n    # Running a loop for converting Date to seconds\n    for date in df['DateTime']:\n        unixtime.append(time.mktime(date.timetuple()))\n    \n    # Replacing Date with unixtime list\n    df['DateTime'] = unixtime\n    return(df)\ntrain1=datetounix(train1)\nval1=datetounix(val1)\n\ntrain=datetounix(train)\ntest=datetounix(test)","97b2ddcb":"x_train1=train1.drop(columns={'ID','Vehicles'},axis=1)\ny_train1=train1.loc[:,['Vehicles']]\n\nx_val1=val1.drop(columns={'ID','Vehicles'},axis=1)\ny_val1=val1.loc[:,['Vehicles']]","f0abdc5d":"ts = time.time()\n\nmodel = XGBRegressor(\n    max_depth=8,\n    booster = \"gbtree\",\n    n_estimators=100000,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,\n    seed=42,\n    objective='reg:linear')\n\nmodel.fit(\n    x_train1, \n    y_train1, \n    eval_metric=\"rmse\", \n    eval_set=[(x_train1, y_train1), (x_val1, y_val1)], \n    verbose=True, \n    early_stopping_rounds = 100)\n\ntime.time() - ts","d0aa8c36":"#predicting validation data.\npred=model.predict(x_val1)","45ad770a":"from sklearn.metrics import mean_squared_error\nfrom math import sqrt\nnp.sqrt(mean_squared_error(np.expm1(y_val1), np.expm1(pred)))","0377e843":"import matplotlib.pyplot as plt\n%matplotlib inline\nplot_features(model, (10,14))","6be00b97":"#checks error in prediction\nres = pd.DataFrame(data = pd.concat([x_val1,y_val1],axis=1))\nres['Prediction']= np.expm1(model.predict(x_val1))\nres['Ratio'] = res.Prediction\/np.expm1(res.Vehicles)\nres['Error'] =abs(res.Ratio-1)\nres['Weight'] = np.expm1(res.Vehicles)\/res.Prediction\nres.head()","7baa2fa7":"#calculates best weight\npred1  = model.predict(x_val1)\nprint(\"weight correction\")\nW=[(0.990+(i\/1000)) for i in range(20)]\nS =[]\nfor w in W:\n    error = sqrt(mean_squared_error(np.expm1(y_val1), np.expm1(pred1*w)))\n    print('RMSE for {:.3f}:{:.6f}'.format(w,error))\n    S.append(error)\nScore = pd.Series(S,index=W)\nScore.plot()\nBS = Score[Score.values == Score.values.min()]\nprint ('Best weight for Score:{}'.format(BS))","6c6a4b37":"pred=model.predict(x_val1)*1.009\nnp.sqrt(mean_squared_error(np.expm1(y_val1), np.expm1(pred)))","3c851b35":"x=train.drop(columns={'ID','Vehicles'},axis=1)\ny=train.loc[:,['Vehicles']]\ntest=test.drop(columns={'ID','Vehicles'},axis=1)","c9d5c658":"model = XGBRegressor(\n    max_depth=8,\n    n_estimators=220,\n    min_child_weight=300, \n    colsample_bytree=0.8, \n    subsample=0.8, \n    eta=0.3,\n    \n    seed=42)\n\nmodel.fit(x, y)\n","199fb069":"pred=model.predict(test)*1.009\nsub['Vehicles']=np.expm1(pred)\nsub.to_csv('finalsub.csv',index=False)","ea5a0763":"#### <font color='red'>Few observations by looking at training data<\/font>\n* We have 1 record for each hour and has details like vehicle count , junction number and unique id.\n* Train data is from 01Nov2015 to 30Jun2017\n","f5e9abb3":"#### <font color='red'>Creating Time Based Feature.This helps regression models to understand the trend in the data.<\/red>","d9947e08":"* Timeseries problems requires **time based validation** instead of generaly used kfold validation in regression problem. Kfold splits the data randomly and checking the model accuracy by predicting on timeperiod 2016 by using 2017 data makes no sense. \n* Here we used time based validation for the time period (2017-01-01 to 2017-04-01) of 4 months, since the test set contains 4 months data to predict.","0f6d462c":"#### <font color='red'>Validating the performance.<\/font>","47e87f84":"#### <font color='red'>Loading the Data<\/font>","3dbce32e":" #### <font color='red'>Log transforming Vehicle to have normal distribution.<\/red>","8a3fe70e":"#### <font color='red'>Why drop date feature when we can make use out of it.<\/font>","9ee8a134":"#### <font color='red'>Feature Importance<\/font>","605619e3":"#### <font color='red'>Getting back train and test<\/font>","6e8e1367":"* https:\/\/www.kaggle.com\/dlarionov\/feature-engineering-xgboost\n* https:\/\/www.kaggle.com\/abhilashawasthi\/feature-engineering-lgb-model\n* https:\/\/www.kaggle.com\/xwxw2929\/rossmann-sales-top1","18f287d3":"####  <font color='red'>Model using all train data (except 2015)<\/font>","a650ce2d":"#### <font color='red'>Concating train and test data for preprocessing<\/font>","b60abdaa":"#### <font color='red'>Other usefull kaggle kernels on this topic .<\/font>","05d77b0f":"#### <font color='red'>Vehicle Trends wrt Time Period. <\/font>","8261b280":" <font color='red'>Here comes the secret sauce which pushed us from Rank2 to Rank 1.<\/font>\n* Reference:https:\/\/www.kaggle.com\/xwxw2929\/rossmann-sales-top1\n* This technique is also used by the Winner of Rossmann store sales prediction.\n* Here you can see the documentation from the winner of Rossman sales prediction:https:\/\/www.kaggle.com\/c\/rossmann-store-sales\/discussion\/18024       \n* This approach calculates the error in the predicted value and chooses the best  weight to mutiply with the prediction .","0d825477":"#### <font color='red'>one hot encoding Junction<\/font>","9d0acba9":"#### <font color='red'>       Here comes the most important step in solving timeseries.<\/font>","eeb8dc98":"![title](https:\/\/datahack-prod.s3.ap-south-1.amazonaws.com\/__sized__\/contest_cover\/jantahack_-thumbnail-1200x1200-90.jpg)","ddde4014":"* Here, Vehicles which is the vehicle count at a particular hour is the target Feature which we need to predict for the timeperiod (2017-07-01 - 2017-10-01) using Train data from(2015-11-01 - 2017-06-01)","757aaa59":"#### <font color='red'>Few observations.<\/font>\n\n* By Analysing our data we have found out that 2015 data has very low vehicle trend compared to the timeperiod 2017 which we are going to predict and also 2015 has only data for the month- 11&12 which has different trend compared to the month (7,8,9&10) for which we need to predict.So we decided to ignore 2015 data .\n* [Tip].The winner of https:\/\/www.kaggle.com\/c\/favorita-grocery-sales-forecasting has only used very recent data in the models, electing to drop older observations based on validation dataset performance.  \n* Selecting the right timeperiod data is very important in Time Series forecasting.\n","ad8c267b":"* Don't forget to check the last part of the solution which is the main secret sauce :-).","17bfe702":"* From the research on all the Time Series Competitons on Kaggle ,it has been found that boosting models perform better as compared to the traditional approach using Statistical models like Holt Winters, Arima.\n* Research PDF Link: https:\/\/www.researchgate.net\/publication\/339362837_Learnings_from_Kaggle's_Forecasting_Competitions\n* Here, I am using the data from the Analytics Vidhya-JanataHack-IOT hackathon where we won the hackathon with Regression approach using mighty XGBoost.\n* You can check the problem statement here:https:\/\/datahack.analyticsvidhya.com\/contest\/janatahack-machine-learning-for-iot\/ ","b0f2291e":"# <font color='orange'>My First Kaggle Kernel and how I got first rank in competition <\/font>","e8b925eb":"#### <font color='red'>Import libraries<\/font>","359bace1":"* Validation accuracy RMSE dropped from **8.015 to 7.46**  by multiplying with error weight . This helped us to top the leaderboard.\n\n* We have validated this particular weight by creating another validation for period(2016-7,8,9,10). It worked well there and also LB score increased.\n* Don't forget to upvote if you find this useful."}}