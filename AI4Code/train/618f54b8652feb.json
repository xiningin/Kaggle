{"cell_type":{"c975abd8":"code","abd4089a":"code","7c546f60":"code","c7e61604":"code","09a64ab4":"code","f21cd2ba":"code","bee74e34":"code","e14377da":"code","61eacd5e":"code","985c965c":"code","536c2c9e":"code","c01118c3":"code","1c3bbad6":"code","088e7552":"code","4d8d0fed":"code","e0ce520f":"code","0c18b6e7":"code","f5ce95c7":"code","43fd0ece":"code","19a5b518":"code","1887e6d2":"code","da679119":"code","0188001d":"code","71ed58a4":"code","19c91117":"code","ae6af4eb":"code","ec08b824":"code","1fc61e65":"code","b178908c":"code","ccbea259":"code","a398d0d6":"code","8fb36090":"code","5a20c711":"code","1b706539":"code","f9dba034":"code","f67065a3":"code","d39c4dc5":"code","f8765fe5":"code","7ac972ad":"code","4b0b2a8d":"code","2d8b1a79":"code","5f00c56b":"code","b283d2cd":"code","a58c3f1c":"code","d3a1b573":"code","3f9c7772":"markdown","e1722d14":"markdown","022b319c":"markdown","4fa5bfcf":"markdown","97ee3b4c":"markdown","8ae4d123":"markdown","1f22fb7a":"markdown","c01e71f9":"markdown","996cc703":"markdown","a3aa2c9c":"markdown","a6121146":"markdown","1e71687c":"markdown"},"source":{"c975abd8":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nplt.style.use('ggplot')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","abd4089a":"df = pd.read_csv('..\/input\/google-job-skills\/job_skills.csv')","7c546f60":"df.head(3)","c7e61604":"# I modify the column name so that I can use df dot column name more easily\ndf = df.rename(columns={'Minimum Qualifications': 'Minimum_Qualifications', 'Preferred Qualifications': 'Preferred_Qualifications'})","09a64ab4":"df.Company.value_counts()","f21cd2ba":"df.Category.value_counts()","bee74e34":"df.Location.value_counts()[:10]","e14377da":"df['Country'] = df['Location'].apply(lambda x : x.split(',')[-1])","61eacd5e":"df.Country.value_counts()[:10]","985c965c":"pd.isnull(df).sum()","536c2c9e":"df = df.dropna(how='any',axis='rows')","c01118c3":"# Perform the necessary imports for similarity\nfrom sklearn.decomposition import NMF\nfrom sklearn.preprocessing import Normalizer, MaxAbsScaler\nfrom sklearn.pipeline import make_pipeline\n\n\nscaler = MaxAbsScaler()\n\nmodel = NMF(n_components=100)\n\nnormalizer = Normalizer()\n\n# Create a pipeline: pipeline\npipeline = make_pipeline(scaler,model,normalizer)","1c3bbad6":"from sklearn.feature_extraction.text import TfidfVectorizer\n\nvectorizer = TfidfVectorizer()\nvectors_Responsibilities = vectorizer.fit_transform(df['Responsibilities'])","088e7552":"Responsibilities = pipeline.fit_transform(vectors_Responsibilities)","4d8d0fed":"df_Responsibilities = pd.DataFrame(Responsibilities,index=df['Title'])","e0ce520f":"df_Responsibilities.head(2)","0c18b6e7":"pd.set_option('display.max_colwidth', -1)\nprint(df[df.Title.str.contains('Data Scientist')]['Title'])","f5ce95c7":"Position = df_Responsibilities.loc['Customer Experience Data Scientist, Google Cloud Support']","43fd0ece":"similarities_1 = df_Responsibilities.dot(Position)","19a5b518":"similarities_1[:3]","1887e6d2":"print(similarities_1.nlargest())","da679119":"df[np.isin(df['Title'],similarities_1.nlargest().index.tolist())].head()","0188001d":"type(similarities_1)","71ed58a4":"vectorizer_Requirements = TfidfVectorizer()\nvectors_Requirements = vectorizer_Requirements.fit_transform(df['Minimum_Qualifications'])","19c91117":"Requirements = pipeline.fit_transform(vectors_Requirements)","ae6af4eb":"df_Requirementss = pd.DataFrame(Requirements,index=df['Title'])","ec08b824":"Position = df_Requirementss.loc['Customer Experience Data Scientist, Google Cloud Support']","1fc61e65":"similarities_2 = df_Responsibilities.dot(Position)","b178908c":"print(similarities_2.nlargest())","ccbea259":"similarities_1","a398d0d6":"similarities_1.rename(\"similarity\")\nsimilarities_2.rename(\"similarity\")\n\nsimilarities_1.to_frame().join(similarities_2.to_frame(),lsuffix='1')","8fb36090":"similarities_overall = (2 * similarities_1) + similarities_2","5a20c711":"print(similarities_overall.nlargest())","1b706539":"df[np.isin(df['Title'],similarities_overall.nlargest(3).index.tolist())].head()","f9dba034":"from scipy.cluster.vq import kmeans, vq\nfrom numpy import random\n\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer\nimport re\nimport string\n\nfrom sklearn.feature_extraction.stop_words import ENGLISH_STOP_WORDS\nfrom spacy.lang.en.stop_words import STOP_WORDS","f67065a3":"string.punctuation","d39c4dc5":"stop_words_0 = set(stopwords.words('english')) \nstop_words = ['and', 'in', 'of', 'or', 'with','to','on','a']\n\ndef remove_noise(text):\n    tokens = word_tokenize(text)\n    clean_tokens = []\n    lemmatizer=WordNetLemmatizer()\n    for token in tokens:\n        token = re.sub('[!\"#$%&\\'()*+,-.\/:;<=>?@[\\\\]^_`{|}~]+', '', token)\n        token = lemmatizer.lemmatize(token.lower())\n        if len(token) > 1 and token not in stop_words_0 and token not in stop_words:\n            clean_tokens.append(token)\n            \n    return clean_tokens","f8765fe5":"# Initialize TfidfVectorizer\ntfidf_vectorizer = TfidfVectorizer(max_features=100,tokenizer=remove_noise)\n\n# Use the .fit_transform() method on the list plots\ntfidf_matrix = tfidf_vectorizer.fit_transform(df['Minimum_Qualifications'])","7ac972ad":"random.seed = 123","4b0b2a8d":"distortions = []\nnum_clusters = range(2, 25)\n\n# Create a list of distortions from the kmeans function\nfor i in num_clusters:\n    cluster_centers, distortion = kmeans(tfidf_matrix.todense(),i)\n    distortions.append(distortion)\n\n# Create a data frame with two lists - num_clusters, distortions\nelbow_plot = pd.DataFrame({'num_clusters': num_clusters, 'distortions': distortions})\n\n# Creat a line plot of num_clusters and distortions\nsns.lineplot(x='num_clusters', y='distortions', data = elbow_plot)\nplt.xticks(num_clusters)\nplt.title('Clusters and Distortions')\nplt.show()","2d8b1a79":"cluster_centers, distortion = kmeans(tfidf_matrix.todense(),13)\n\n# Generate terms from the tfidf_vectorizer object\nterms = tfidf_vectorizer.get_feature_names()\n\nfor i in range(13):\n    # Sort the terms and print top 10 terms\n    center_terms = dict(zip(terms, list(cluster_centers[i])))\n    sorted_terms = sorted(center_terms, key=center_terms.get, reverse=True)\n    print(sorted_terms[:5])","5f00c56b":"# Add in the rest of the parameters\ndef return_weights(vocab, original_vocab, vector, vector_index, top_n):\n    zipped = dict(zip(vector[vector_index].indices, vector[vector_index].data))\n    \n    # Let's transform that zipped dict into a series\n    zipped_series = pd.Series({vocab[i]:zipped[i] for i in vector[vector_index].indices})\n    \n    # Let's sort the series to pull out the top n weighted words\n    zipped_index = zipped_series.sort_values(ascending=False)[:top_n].index\n    return [original_vocab[i] for i in zipped_index]","b283d2cd":"vocab = {v:k for k,v in tfidf_vectorizer.vocabulary_.items()}","a58c3f1c":"def words_to_filter(vocab, original_vocab, vector, top_n):\n    filter_list = []\n    for i in range(0, vector.shape[0]):\n    \n        # Here we'll call the function from the previous exercise, and extend the list we're creating\n        filtered = return_weights(vocab, original_vocab, vector, i, top_n)\n        filter_list.extend(filtered)\n    # Return the list in a set, so we don't get duplicate word indices\n    return set(filter_list)\n\n# Call the function to get the list of word indices\nfiltered_words = words_to_filter(vocab, tfidf_vectorizer.vocabulary_, tfidf_matrix, 5)\n\n# By converting filtered_words back to a list, we can use it to filter the columns in the text vector\nfiltered_text = tfidf_matrix[:, list(filtered_words)]","d3a1b573":"print(filtered_text)","3f9c7772":"### Hi everyone! It's been a while since last time I showed up on Kaggle. \n\n#### This time, I'm also working on the Google Job dataset. While this time, I would like to build a simple recommendation system based on the scenario of looking for a position and finding similar openings for users this time","e1722d14":"# Outline\n\n## Recommendation System\n\n- [EDA](#0)   \n    * I'll do simple exploratory on the data structure and values\n- [Modeling](#1)\n    * I'll start to test out vectorize text and find similar positions based on job description\n- [Finalizing](#2)\n    * Will also consider requirements in this part\n    \n## [Text Clustering](#Cluster)","022b319c":"With the groups of words, I can tell different groups are from different fields of the positions.","4fa5bfcf":"Though this looks a bit weird, let's see how we put responsibilities and requirements together first.","97ee3b4c":"### Let's see if the role is similar and ideal as an alternative.","8ae4d123":"### The result is not bad! Though one of the alternative position looks more emphasize soft skills part while another is similar in terms of the hard skills part, I think they both look like a good choice as well.","1f22fb7a":"#### The purpose of this part is aiming at finding the relevant words, skills, requirements across different roles using Cluster Analysis instead of Word Cloud in my previous project.","c01e71f9":"## [Modeling](#1)","996cc703":"### In my opinion, the role is a good alternative choice while the requirement could be a blocker. So let's also consider the part of requirements.","a3aa2c9c":"## [Text Clustering](#Clustering)","a6121146":"![google](http:\/\/img.technews.tw\/wp-content\/uploads\/2015\/09\/Google-logo_1.jpg)","1e71687c":"## [EDA](#0)\n\n**First, I would like to know more about the data**\n\n- Starting from the columns\n- Then, the text pattern in columns\n- Finally, the correlation between different positions"}}