{"cell_type":{"583cc6da":"code","add7535e":"code","ba9237cd":"code","97861fcd":"code","a8ad0d6f":"code","0adeee75":"code","e8d3cd31":"code","bde60fbf":"code","2aaf7d6d":"code","593ab7f1":"code","3c20c499":"code","237f24e8":"code","3a46082f":"code","2fe9da1e":"code","273358d3":"code","05d8a509":"code","5624d81e":"code","a63d879a":"code","724938fe":"code","7365316b":"code","2aa4162b":"code","3aa9fe6f":"code","1a161c3d":"code","bab8766f":"markdown","db62763c":"markdown","3fe72de7":"markdown","bb85cdff":"markdown","fe831f08":"markdown","5969707c":"markdown","e47e28ee":"markdown","1ca70416":"markdown","fb3f9ac2":"markdown","1e73d2d4":"markdown","902e0f4a":"markdown","1aa58d12":"markdown"},"source":{"583cc6da":"import numpy as np\nimport pandas as pd\n\nfrom sklearn import preprocessing\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\nfrom xgboost import XGBRegressor\nfrom catboost import CatBoostRegressor\n\nimport optuna","add7535e":"random_state = 42\nKF_split = 5\nKF_split_optuna = 2\noptuna_trials = 150\n\noptuna.logging.set_verbosity(optuna.logging.WARNING) #stop showing each trial result","ba9237cd":"df_train = pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col=0)\ndf_test = pd.read_csv('..\/input\/30-days-of-ml\/test.csv', index_col=0)\n\n# Creating a DataFrame for Blending\ny_valid_pred_blnd = pd.DataFrame(data=df_train.target,index=df_train.index).reset_index()\ny_test_pred_blnd = pd.DataFrame(index=df_test.index)\n\n# Creating a DataFrame for Stacking\ny_valid_pred_stk = pd.DataFrame(data=df_train.target,index=df_train.index).reset_index()\ny_test_pred_stk = pd.DataFrame(index=df_test.index)","97861fcd":"df_train.head()","a8ad0d6f":"def optuna_tuning_fitting(model_name):\n    \n    # Optimizing Using Optuna\n    def objective(trial):\n        \n        params = {\n            'n_estimators': trial.suggest_int('n_estimators', 100, 7000),\n            'learning_rate': trial.suggest_loguniform('learning_rate',0.005,0.5),\n            'max_depth': trial.suggest_int('max_depth', 1, 7),\n            'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 100.0),\n            'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 100.0),\n            'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n            'gamma': trial.suggest_float('gamma', 0.1, 1.0, step=0.1),\n            'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n            'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree',0.1,1,0.01),\n        }\n\n\n        \n        rmse=0\n\n        kf = model_selection.KFold(n_splits=KF_split_optuna, shuffle=True, random_state=random_state)\n\n        for fold, (train_idx,valid_idx) in enumerate(kf.split(X_train)):\n        \n            # Generating X and y for train and test sets\n            X_train_f = X_train.iloc[train_idx].copy()\n            y_train_f = y_train.iloc[train_idx]\n\n            X_valid_f = X_train.iloc[valid_idx].copy()\n            y_valid_f = y_train.iloc[valid_idx]\n\n            X_test = df_test.copy()\n\n\n            if model_name == 'model_1':\n                # Encoding Categorical variables\n                encoder = preprocessing.OrdinalEncoder()\n                X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n                X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n                # Scaling Features\n                scaler = preprocessing.StandardScaler()\n                X_train_f[num_col] = scaler.fit_transform(X_train_f[num_col])\n                X_valid_f[num_col] = scaler.transform(X_valid_f[num_col])            \n\n            elif model_name == 'model_2':\n                # Encoding Categorical variables\n                encoder = preprocessing.OrdinalEncoder()\n                X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n                X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n\n            elif model_name == 'model_3':\n                # Encoding Categorical variables\n                encoder = preprocessing.OrdinalEncoder()\n                X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n                X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])            \n\n            elif model_name == 'model_4':\n                df_train_f = pd.concat([X_train_f,y_train_f], axis=1)\n                for col in cat_col:\n                    map_dict = df_train_f.groupby(col).mean().target.to_dict()\n                    X_train_f[col] = X_train_f[col].map(map_dict)\n                    X_valid_f[col] = X_valid_f[col].map(map_dict)\n                    \n            elif model_name == 'model_5':\n                # Encoding Categorical variables\n                encoder = preprocessing.OrdinalEncoder()\n                X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n                X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n                scaler = preprocessing.PowerTransformer()\n                X_train_f[num_col] = scaler.fit_transform(X_train_f[num_col])\n                X_valid_f[num_col] = scaler.transform(X_valid_f[num_col])\n                scaler = preprocessing.StandardScaler()\n                X_train_f = scaler.fit_transform(X_train_f)\n                X_valid_f = scaler.transform(X_valid_f)\n                scaler = preprocessing.PowerTransformer()\n                X_train_f = scaler.fit_transform(X_train_f)\n                X_valid_f = scaler.transform(X_valid_f)\n\n            \n\n            # Modeling \n            model = XGBRegressor(**params,\n                                random_state=fold,\n                                tree_method='gpu_hist',\n                                gpu_id=0,\n                                predictor='gpu_predictor')\n        \n            model.fit(X_train_f, y_train_f,\n                    eval_set=[(X_valid_f,y_valid_f)],\n                    early_stopping_rounds=300,\n                    verbose=False)\n            \n            y_pred_f = model.predict(X_valid_f)\n            rmse += metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n            return rmse\n        \n        \n    # Optimizing\n    study = optuna.create_study(direction='minimize')\n    study.optimize(objective, n_trials=optuna_trials)\n\n    print(f'Best score: {study.best_value:.5f}')\n    print(f'Best Params: {study.best_params}')\n\n#-----------------------------------------------------------------------------------------------\n\n    # Fitting the tuned Model\n    y_test_pred = []\n\n    kf = model_selection.KFold(n_splits=KF_split, shuffle=True, random_state=random_state)\n\n    for fold, (train_idx,valid_idx) in enumerate(kf.split(X_train)):\n        \n        # Generating X and y for train and test sets\n        X_train_f = X_train.iloc[train_idx].copy()\n        y_train_f = y_train.iloc[train_idx]\n        \n        X_valid_f = X_train.iloc[valid_idx].copy()\n        y_valid_f = y_train.iloc[valid_idx]\n        \n        X_test = df_test.copy()\n\n\n\n        if model_name == 'model_1':\n            # Encoding Categorical variables\n            encoder = preprocessing.OrdinalEncoder()\n            X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n            X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n            X_test[cat_col] = encoder.transform(X_test[cat_col])\n\n            # Scaling Features\n            scaler = preprocessing.StandardScaler()\n            X_train_f[num_col] = scaler.fit_transform(X_train_f[num_col])\n            X_valid_f[num_col] = scaler.transform(X_valid_f[num_col])\n            X_test[num_col] = scaler.transform(X_test[num_col])\n\n\n        elif model_name == 'model_2':\n            # Encoding Categorical variables\n            encoder = preprocessing.OrdinalEncoder()\n            X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n            X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n            X_test[cat_col] = encoder.transform(X_test[cat_col])\n\n\n        elif model_name == 'model_3':\n            # Encoding Categorical variables\n            encoder = preprocessing.OrdinalEncoder()\n            X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n            X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n            X_test[cat_col] = encoder.transform(X_test[cat_col])    \n\n\n        elif model_name == 'model_4':\n            df_train_f = pd.concat([X_train_f,y_train_f], axis=1)\n\n            for col in cat_col:\n                map_dict = df_train_f.groupby(col).mean().target.to_dict()\n                X_train_f[col] = X_train_f[col].map(map_dict)\n                X_valid_f[col] = X_valid_f[col].map(map_dict)\n                X_test[col] = X_test[col].map(map_dict)\n\n        elif model_name == 'model_5':\n            # Encoding Categorical variables\n            encoder = preprocessing.OrdinalEncoder()\n            X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n            X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n            X_test[cat_col] = encoder.transform(X_test[cat_col]) \n\n\n            scaler = preprocessing.PowerTransformer()\n            X_train_f[num_col] = scaler.fit_transform(X_train_f[num_col])\n            X_valid_f[num_col] = scaler.transform(X_valid_f[num_col])\n            X_test[num_col] = scaler.transform(X_test[num_col])\n\n            scaler = preprocessing.StandardScaler()\n            X_train_f = scaler.fit_transform(X_train_f)\n            X_valid_f = scaler.transform(X_valid_f)\n            X_test = scaler.transform(X_test)\n\n            scaler = preprocessing.PowerTransformer()\n            X_train_f = scaler.fit_transform(X_train_f)\n            X_valid_f = scaler.transform(X_valid_f)\n            X_test = scaler.transform(X_test)\n\n        \n\n        # Modeling\n        model = XGBRegressor(**study.best_params,\n                            random_state=fold,\n                            tree_method='gpu_hist',\n                            gpu_id=0,\n                            predictor='gpu_predictor')\n        \n        model.fit(X_train_f, y_train_f,\n                    eval_set=[(X_valid_f,y_valid_f)],\n                    early_stopping_rounds=300,\n                    verbose=False)\n        \n        y_pred_f = model.predict(X_valid_f)\n        rmse = metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n        print(f'fold-{fold} rmse : {rmse:.5f}')\n        \n        y_test_f = model.predict(X_test)\n        y_test_pred.append(y_test_f)\n        \n        # Updating Blending DataFrame\n        y_valid_pred_blnd.loc[valid_idx, model_name] = y_pred_f\n        \n    y_test_model = np.mean(np.column_stack(y_test_pred), axis=1)\n    y_test_pred_blnd.loc[:, model_name] = y_test_model\n\n\n    #return y_valid_pred_blnd, y_test_pred_blnd","0adeee75":"df_train = pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col=0)\ndf_test = pd.read_csv('..\/input\/30-days-of-ml\/test.csv', index_col=0)\n\nX_train = df_train.drop('target', axis=1)\ny_train = df_train.target\nX_test = df_test\n\nnum_col = [col for col in X_train.columns if 'cat' not in col]\ncat_col = [col for col in X_train.columns if 'cat' in col]\n\n# Model Tuning and Fitting\noptuna_tuning_fitting(model_name='model_1')","e8d3cd31":"df_train = pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col=0)\ndf_test = pd.read_csv('..\/input\/30-days-of-ml\/test.csv', index_col=0)\n\nnum_col = [col for col in df_train.columns if 'cont' in col]\ncat_col = [col for col in df_train.columns if 'cat' in col]\n\nX_train = df_train.drop('target', axis=1)\ny_train = df_train.target\nX_test = df_test\n\n# Log tranformation\nfor col in num_col:\n    X_train[col] = np.log1p(X_train[col])\n    X_test[col] = np.log1p(X_test[col])\n    \n# Model Tuning and Fitting\noptuna_tuning_fitting(model_name='model_2')","bde60fbf":"df_train = pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col=0)\ndf_test = pd.read_csv('..\/input\/30-days-of-ml\/test.csv', index_col=0)\n\nX_train = df_train.drop('target', axis=1).copy()\ny_train = df_train.target\nX_test = df_test\n\nnum_col = [col for col in X_train.columns if 'cat' not in col]\ncat_col = [col for col in X_train.columns if 'cat' in col]\n\n# Polynomials\npoly = preprocessing.PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n\ntrain_poly = poly.fit_transform(X_train[num_col])\nX_train_poly = pd.DataFrame(train_poly,\n                            columns=[f'poly_{i}' for i in range(train_poly.shape[1])],\n                            index=X_train.index) # using index for cancatenation\nX_train = pd.concat([X_train[cat_col],X_train_poly], axis=1) # To avoid duplicating, we just concat cat_col with poly dataframe\n\ntest_poly = poly.fit_transform(df_test[num_col])\ndf_test_poly = pd.DataFrame(test_poly,\n                            columns=[f'poly_{i}' for i in range(test_poly.shape[1])],\n                            index=X_test.index)\ndf_test = pd.concat([X_test[cat_col],df_test_poly], axis=1)\n\n# Model Tuning and Fitting\noptuna_tuning_fitting(model_name='model_3')","2aaf7d6d":"df_train = pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col=0)\ndf_test = pd.read_csv('..\/input\/30-days-of-ml\/test.csv', index_col=0)\n\nX_train = df_train.drop('target', axis=1).copy()\ny_train = df_train.target\nX_test = df_test\n\nnum_col = [col for col in X_train.columns if 'cat' not in col]\ncat_col = [col for col in X_train.columns if 'cat' in col]\n\n# Model Tuning and Fitting\noptuna_tuning_fitting(model_name='model_4')","593ab7f1":"df_train = pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col=0)\ndf_test = pd.read_csv('..\/input\/30-days-of-ml\/test.csv', index_col=0)\n\nnum_col = [col for col in df_train.columns if 'cont' in col]\ncat_col = [col for col in df_train.columns if 'cat' in col]\n\nX_train = df_train.drop('target', axis=1)\ny_train = df_train.target\nX_test = df_test\n\n# Log tranformation\nfor col in num_col:\n    X_train[col] = np.log1p(X_train[col])\n    X_test[col] = np.log1p(X_test[col])\n    \n# Model Tuning and Fitting\noptuna_tuning_fitting(model_name='model_5')","3c20c499":"df_train = pd.read_csv('..\/input\/30-days-of-ml\/train.csv', index_col=0)\ndf_test = pd.read_csv('..\/input\/30-days-of-ml\/test.csv', index_col=0)\n\nnum_col = [col for col in df_train.columns if 'cont' in col]\ncat_col = [col for col in df_train.columns if 'cat' in col]\n\nX_train = df_train.drop('target', axis=1)\ny_train = df_train.target\nX_test = df_test\n\n# Log tranformation\nfor col in num_col:\n    X_train[col] = np.log1p(X_train[col])\n    X_test[col] = np.log1p(X_test[col])","237f24e8":"# Optimizing Using Optuna\ndef objective(trial):\n\n    params = {'max_depth': trial.suggest_int('max_bin', 2, 7),\n              'iterations':trial.suggest_int(\"iterations\", 1000, 25000),\n              'od_wait':trial.suggest_int('od_wait', 500, 2300),\n              'learning_rate' : trial.suggest_uniform('learning_rate',0.02,1),\n              'reg_lambda': trial.suggest_uniform('reg_lambda',1e-5,100),\n              'subsample': trial.suggest_uniform('subsample',0,1),\n              'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 300),\n              'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n               }\n\n    rmse=0\n\n    kf = model_selection.KFold(n_splits=KF_split_optuna, shuffle=True, random_state=random_state)\n\n    for fold, (train_idx,valid_idx) in enumerate(kf.split(X_train)):\n\n        # Generating X and y for train and test sets\n        X_train_f = X_train.iloc[train_idx].copy()\n        y_train_f = y_train.iloc[train_idx]\n\n        X_valid_f = X_train.iloc[valid_idx].copy()\n        y_valid_f = y_train.iloc[valid_idx]\n\n        X_test = df_test.copy()\n\n\n        # Encoding Categorical variables\n        encoder = preprocessing.OrdinalEncoder()\n        X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n        X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n        X_test[cat_col] = encoder.transform(X_test[cat_col])\n\n\n        # Modeling \n        model = CatBoostRegressor(**params,\n                                   loss_function='RMSE',\n                                   eval_metric='RMSE',\n                                   od_type='Iter',\n                                   task_type='GPU',\n                                   grow_policy='Depthwise',\n                                   leaf_estimation_method='Newton',\n                                   bootstrap_type='Bernoulli')\n    \n        model.fit(X_train_f, y_train_f,\n                  eval_set=[(X_valid_f,y_valid_f)],\n                  early_stopping_rounds=300,\n                  verbose=False)\n\n        y_pred_f = model.predict(X_valid_f)\n        rmse += metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n        return rmse\n\n\n# Optimizing\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=optuna_trials)\n\nprint(f'Best score: {study.best_value:.5f}')\nprint(f'Best Params: {study.best_params}')","3a46082f":"# Fitting the tuned Model\ny_test_pred = []\n\nkf = model_selection.KFold(n_splits=KF_split, shuffle=True, random_state=random_state)\n\nfor fold, (train_idx,valid_idx) in enumerate(kf.split(X_train)):\n\n    # Generating X and y for train and test sets\n    X_train_f = X_train.iloc[train_idx].copy()\n    y_train_f = y_train.iloc[train_idx]\n\n    X_valid_f = X_train.iloc[valid_idx].copy()\n    y_valid_f = y_train.iloc[valid_idx]\n\n    X_test = df_test.copy()\n\n\n    # Encoding Categorical variables\n    encoder = preprocessing.OrdinalEncoder()\n    X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n    X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n    X_test[cat_col] = encoder.transform(X_test[cat_col])\n\n\n    # Modeling\n    model = CatBoostRegressor(**study.best_params,\n                         loss_function='RMSE',\n                         eval_metric='RMSE',\n                         od_type='Iter',\n                         task_type='GPU',\n                         grow_policy='Depthwise',\n                         leaf_estimation_method='Newton',\n                         bootstrap_type='Bernoulli',\n                         verbose=False)\n    \n    model.fit(X_train_f, y_train_f,\n              eval_set=[(X_valid_f, y_valid_f)],\n              early_stopping_rounds=300,\n              verbose=0)\n\n    y_pred_f = model.predict(X_valid_f)\n    rmse = metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n    print(f'fold-{fold} rmse : {rmse:.5f}')\n\n    y_test_f = model.predict(X_test)\n    y_test_pred.append(y_test_f)\n\n    # Updating Blending DataFrame\n    y_valid_pred_blnd.loc[valid_idx, 'model_6'] = y_pred_f\n\ny_test_model = np.mean(np.column_stack(y_test_pred), axis=1)\ny_test_pred_blnd.loc[:, 'model_6'] = y_test_model\n\n\n#return y_valid_pred_blnd, y_test_pred_blnd\n","2fe9da1e":"# # Optimizing Using Optuna\n\n# def objective(trial):\n    \n# #         params = {\n# #             'n_estimators': trial.suggest_int('n_estimators', 100, 7000),\n# #             'learning_rate': trial.suggest_loguniform('learning_rate',0.005,0.5),\n# #             'max_depth': trial.suggest_int('max_depth', 1, 7),\n# #             'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 100.0),\n# #             'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 100.0),\n# #             'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n# #             'gamma': trial.suggest_float('gamma', 0.1, 1.0, step=0.1),\n# #             'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n# #             'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree',0.1,1,0.01),\n# #         }\n\n#     params = {\n#             'n_estimators': trial.suggest_int('n_estimators', 7000, 10000, step=1000),\n#             'max_depth': trial.suggest_int('max_depth', 2,8,step=3),\n\n#         }\n    \n    \n#     rmse=0\n#     kf = model_selection.KFold(n_splits=KF_split_optuna, shuffle=True, random_state=random_state)\n\n#     for fold, (train_idx,valid_idx) in enumerate(kf.split(X_train)):\n    \n#         # Generating X and y for train and test sets\n#         X_train_f = X_train.iloc[train_idx].copy()\n#         y_train_f = y_train.iloc[train_idx]\n\n#         X_valid_f = X_train.iloc[valid_idx].copy()\n#         y_valid_f = y_train.iloc[valid_idx]\n\n#         X_test = df_test.copy()\n\n#         # Encoding Categorical variables\n#         encoder = preprocessing.OrdinalEncoder()\n#         X_train_f[cat_col] = encoder.fit_transform(X_train_f[cat_col]) #cat_col\n#         X_valid_f[cat_col] = encoder.transform(X_valid_f[cat_col])\n#         X_test[cat_col] = encoder.transform(X_test[cat_col])\n\n#         # Scaling Features\n#         scaler = preprocessing.StandardScaler()\n#         X_train_f[num_col] = scaler.fit_transform(X_train_f[num_col])\n#         X_valid_f[num_col] = scaler.transform(X_valid_f[num_col])\n#         X_test[num_col] = scaler.transform(X_test[num_col])\n\n#         # Modeling \n#         model = XGBRegressor(**params,\n#                              random_state=fold,\n#                              tree_method='gpu_hist',\n#                              gpu_id=0,\n#                              predictor='gpu_predictor')\n    \n#         model.fit(X_train_f, y_train_f,\n#                   eval_set=[(X_valid_f,y_valid_f)],\n#                   early_stopping_rounds=300,\n#                   verbose=False)\n        \n#         y_pred_f = model.predict(X_valid_f)\n#         rmse += metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n#         return rmse\n    \n    \n# # Optimizing\n\n# study = optuna.create_study(direction='minimize')\n# study.optimize(objective, n_trials=optuna_trials)\n\n# print(f'Best score: {study.best_value:.5f}')\n# print(f'Best Params: {study.best_params}')","273358d3":"X_train_blnd = y_valid_pred_blnd.drop(['id','target'], axis=1)\ny_train_blnd = y_valid_pred_blnd.target\n\nX_test_blnd = y_test_pred_blnd","05d8a509":"# Optimizing Using Optuna\n\ndef objective(trial):\n    \n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 7000),\n        'learning_rate': trial.suggest_loguniform('learning_rate',0.005,0.5),\n        'max_depth': trial.suggest_int('max_depth', 1, 7),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 100.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 100.0),\n        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n        'gamma': trial.suggest_float('gamma', 0.1, 1.0, step=0.1),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree',0.1,1,0.01),\n    }\n    \n    \n    rmse=0\n    kf = model_selection.KFold(n_splits=KF_split_optuna, shuffle=True, random_state=random_state)\n\n    for fold, (train_idx,valid_idx) in enumerate(kf.split(X_train_blnd)):\n\n        X_train_f = X_train_blnd.iloc[train_idx].copy()\n        y_train_f = y_train_blnd.iloc[train_idx]\n\n        X_valid_f = X_train_blnd.iloc[valid_idx].copy()\n        y_valid_f = y_train_blnd.iloc[valid_idx]\n\n        X_test = X_test_blnd.copy()\n    \n    \n        # Modeling \n        model = XGBRegressor(**params,\n                             random_state=fold,\n                             tree_method='gpu_hist',\n                             gpu_id=0,\n                             predictor='gpu_predictor')\n    \n        model.fit(X_train_f, y_train_f,\n                  eval_set=[(X_valid_f,y_valid_f)],\n                  early_stopping_rounds=300,\n                  verbose=False)\n        \n        y_pred_f = model.predict(X_valid_f)\n        rmse += metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n        return rmse\n    \n    \n# Optimizing\n# optuna.logging.set_verbosity(optuna.logging.WARNING) #stop showing each trial result\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=optuna_trials)\n\nprint(f'Best score: {study.best_value:.5f}')\nprint(f'Best Params: {study.best_params}')","5624d81e":"# Final Model\ny_test_pred = []\n\nkf = model_selection.KFold(n_splits=KF_split, shuffle=True, random_state=random_state)\n\nfor fold, (train_idx,valid_idx) in enumerate(kf.split(X_train_blnd)):\n    \n    # Generating X and y for train and test sets\n    X_train_f = X_train_blnd.iloc[train_idx].copy()\n    y_train_f = y_train_blnd.iloc[train_idx]\n    \n    X_valid_f = X_train_blnd.iloc[valid_idx].copy()\n    y_valid_f = y_train_blnd.iloc[valid_idx]\n    \n    X_test = X_test_blnd.copy()\n    \n    # Modeling\n    model = XGBRegressor(**study.best_params,\n                         random_state=fold,\n                         tree_method='gpu_hist',\n                         gpu_id=0,\n                         predictor='gpu_predictor')\n    \n    model.fit(X_train_f, y_train_f,\n              eval_set=[(X_valid_f, y_valid_f)],\n              early_stopping_rounds=300,\n              verbose=0)\n    \n    y_pred_f = model.predict(X_valid_f)\n    rmse = metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n    print(f'fold-{fold} rmse : {rmse:.5f}')\n    \n    y_test_f = model.predict(X_test)\n    y_test_pred.append(y_test_f)\n    \n    # Updating Stacking DataFrame\n    y_valid_pred_stk.loc[valid_idx,'model_blnd_1'] = y_pred_f\n    \ny_test_model = np.mean(np.column_stack(y_test_pred), axis=1)\ny_test_pred_stk.loc[:,'model_blnd_1'] = y_test_model","a63d879a":"# Optimizing Using Optuna\n\ndef objective(trial):\n    \n    params = {'max_depth': trial.suggest_int('max_bin', 2, 7),\n              'iterations':trial.suggest_int(\"iterations\", 1000, 25000),\n              'od_wait':trial.suggest_int('od_wait', 500, 2300),\n              'learning_rate' : trial.suggest_uniform('learning_rate',0.02,1),\n              'reg_lambda': trial.suggest_uniform('reg_lambda',1e-5,100),\n              'subsample': trial.suggest_uniform('subsample',0,1),\n              'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 300),\n              'leaf_estimation_iterations': trial.suggest_int('leaf_estimation_iterations',1,15),\n               }\n    \n    \n    rmse=0\n    kf = model_selection.KFold(n_splits=KF_split_optuna, shuffle=True, random_state=random_state)\n\n    for fold, (train_idx,valid_idx) in enumerate(kf.split(X_train_blnd)):\n\n        X_train_f = X_train_blnd.iloc[train_idx].copy()\n        y_train_f = y_train_blnd.iloc[train_idx]\n\n        X_valid_f = X_train_blnd.iloc[valid_idx].copy()\n        y_valid_f = y_train_blnd.iloc[valid_idx]\n\n        X_test = X_test_blnd.copy()\n    \n    \n        # Modeling \n        model = CatBoostRegressor(**params,\n                                   loss_function='RMSE',\n                                   eval_metric='RMSE',\n                                   od_type='Iter',\n                                   task_type='GPU',\n                                   grow_policy='Depthwise',\n                                   leaf_estimation_method='Newton',\n                                   bootstrap_type='Bernoulli',\n                                   #verbose=False\n                                 )\n    \n        model.fit(X_train_f, y_train_f,\n                  eval_set=[(X_valid_f,y_valid_f)],\n                  early_stopping_rounds=300,\n                  verbose=False)\n        \n        y_pred_f = model.predict(X_valid_f)\n        rmse += metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n        return rmse\n    \n    \n# Optimizing\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=optuna_trials)\n\nprint(f'Best score: {study.best_value:.5f}')\nprint(f'Best Params: {study.best_params}')","724938fe":"# Final Model\ny_test_pred = []\n\nkf = model_selection.KFold(n_splits=KF_split, shuffle=True, random_state=random_state)\n\nfor fold, (train_idx,valid_idx) in enumerate(kf.split(X_train_blnd)):\n    \n    # Generating X and y for train and test sets\n    X_train_f = X_train_blnd.iloc[train_idx].copy()\n    y_train_f = y_train_blnd.iloc[train_idx]\n    \n    X_valid_f = X_train_blnd.iloc[valid_idx].copy()\n    y_valid_f = y_train_blnd.iloc[valid_idx]\n    \n    X_test = X_test_blnd.copy()\n    \n    # Modeling\n    model = CatBoostRegressor(**study.best_params,\n                         loss_function='RMSE',\n                         eval_metric='RMSE',\n                         od_type='Iter',\n                         task_type='GPU',\n                         grow_policy='Depthwise',\n                         leaf_estimation_method='Newton',\n                         bootstrap_type='Bernoulli',\n                         verbose=False,)\n    \n    model.fit(X_train_f, y_train_f,\n              eval_set=[(X_valid_f, y_valid_f)],\n              early_stopping_rounds=300,\n              verbose=0)\n    \n    y_pred_f = model.predict(X_valid_f)\n    rmse = metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n    print(f'fold-{fold} rmse : {rmse:.5f}')\n    \n    y_test_f = model.predict(X_test)\n    y_test_pred.append(y_test_f)\n    \n    # Updating Stacking DataFrame\n    y_valid_pred_stk.loc[valid_idx,'model_blnd_2'] = y_pred_f\n    \ny_test_model = np.mean(np.column_stack(y_test_pred), axis=1)\ny_test_pred_stk.loc[:,'model_blnd_2'] = y_test_model","7365316b":"X_train_stk = y_valid_pred_stk.drop(['id','target'], axis=1)\ny_train_stk = y_valid_pred_stk.target\n\nX_test_stk = y_test_pred_stk","2aa4162b":"# Optimizing Using Optuna\n\ndef objective(trial):\n    \n    params = {\n        'n_estimators': trial.suggest_int('n_estimators', 100, 7000),\n        'learning_rate': trial.suggest_loguniform('learning_rate',0.005,0.5),\n        'max_depth': trial.suggest_int('max_depth', 1, 7),\n        'reg_alpha': trial.suggest_loguniform('reg_alpha', 1e-8, 100.0),\n        'reg_lambda': trial.suggest_loguniform('reg_lambda', 1e-8, 100.0),\n        'subsample': trial.suggest_float('subsample', 0.1, 1.0),\n        'gamma': trial.suggest_float('gamma', 0.1, 1.0, step=0.1),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 300),\n        'colsample_bytree': trial.suggest_discrete_uniform('colsample_bytree',0.1,1,0.01),\n    }\n    \n    \n    rmse=0\n    kf = model_selection.KFold(n_splits=KF_split_optuna, shuffle=True, random_state=random_state)\n\n    for fold, (train_idx,valid_idx) in enumerate(kf.split(X_train_stk)):\n\n        X_train_f = X_train_stk.iloc[train_idx].copy()\n        y_train_f = y_train_stk.iloc[train_idx]\n\n        X_valid_f = X_train_stk.iloc[valid_idx].copy()\n        y_valid_f = y_train_stk.iloc[valid_idx]\n\n        X_test = X_test_stk.copy()\n    \n    \n        # Modeling \n        model = XGBRegressor(**params,\n                             random_state=fold,\n                             tree_method='gpu_hist',\n                             gpu_id=0,\n                             predictor='gpu_predictor')\n    \n        model.fit(X_train_f, y_train_f,\n                  eval_set=[(X_valid_f,y_valid_f)],\n                  early_stopping_rounds=300,\n                  verbose=False)\n        \n        y_pred_f = model.predict(X_valid_f)\n        rmse += metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n        return rmse\n    \n    \n# Optimizing\n# optuna.logging.set_verbosity(optuna.logging.WARNING) #stop showing each trial result\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=optuna_trials)\n\nprint(f'Best score: {study.best_value:.5f}')\nprint(f'Best Params: {study.best_params}')","3aa9fe6f":"# Final Model\ny_test_pred = []\n\nkf = model_selection.KFold(n_splits=KF_split, shuffle=True, random_state=random_state)\n\nfor fold, (train_idx,valid_idx) in enumerate(kf.split(X_train_stk)):\n    \n    # Generating X and y for train and test sets\n    X_train_f = X_train_stk.iloc[train_idx].copy()\n    y_train_f = y_train_stk.iloc[train_idx]\n\n    X_valid_f = X_train_stk.iloc[valid_idx].copy()\n    y_valid_f = y_train_stk.iloc[valid_idx]\n\n    X_test = X_test_stk.copy()\n    \n    # Modeling\n    model = XGBRegressor(**study.best_params,\n                         random_state=fold,\n                         tree_method='gpu_hist',\n                         gpu_id=0,\n                         predictor='gpu_predictor')\n    \n    model.fit(X_train_f, y_train_f,\n              eval_set=[(X_valid_f, y_valid_f)],\n              early_stopping_rounds=300,\n              verbose=0)\n    \n    y_pred_f = model.predict(X_valid_f)\n    rmse = metrics.mean_squared_error(y_pred_f, y_valid_f, squared=False)\n    print(f'fold-{fold} rmse : {rmse:.5f}')\n    \n    y_test_f = model.predict(X_test)\n    y_test_pred.append(y_test_f)\n    \ny_test_final = np.mean(np.column_stack(y_test_pred), axis=1)","1a161c3d":"sample_submission = pd.read_csv('..\/input\/30-days-of-ml\/sample_submission.csv')\nsample_submission.target = y_test_final\nsample_submission.to_csv(\"submission.csv\", index=False)","bab8766f":"## Blending #1","db62763c":"## Model #4","3fe72de7":"## Model #5'","bb85cdff":"# Blending","fe831f08":"# Modeling","5969707c":"## Model #3","e47e28ee":"## Model #1","1ca70416":"## Model #6","fb3f9ac2":"## Model #2","1e73d2d4":"## Model #5","902e0f4a":"# Stacking","1aa58d12":"## Blending #2"}}