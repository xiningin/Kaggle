{"cell_type":{"1484f245":"code","0cd53db2":"code","4217ea83":"code","a5fe55b8":"code","d8f8d5ce":"code","0b7f8408":"code","1ddf6a23":"code","e29290d1":"code","209431ae":"code","8794d3e0":"code","5509465f":"code","3b2867cb":"code","2adaf486":"code","3a77c6c1":"code","c5fb314d":"code","20a16b2b":"code","143acad6":"code","47784193":"code","7ab588a7":"code","46cbf6c3":"code","bbcf88a9":"code","19258743":"code","4d8d906b":"code","d48dd75f":"code","67dd48cf":"code","be64900d":"code","f2075c3b":"code","5ad266ae":"markdown","18814765":"markdown","610f455f":"markdown","2dc6c1fa":"markdown","420bb3a3":"markdown","f1574ea9":"markdown","06b7ea9f":"markdown","a744cd0f":"markdown","5b74116a":"markdown"},"source":{"1484f245":"import os \nimport zipfile \nimport tensorflow as tf \nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator \nfrom tensorflow.keras import layers \nfrom tensorflow.keras import Model \nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nimport pandas as pd\nimport numpy as np\nfrom time import time\n\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV","0cd53db2":"train_csv_path = '\/kaggle\/input\/ima205challenge2021bis\/metadataTrain.csv'\ntrain_csv = pd.read_csv(train_csv_path, index_col=0)\ntrain_csv['GROUP'] = train_csv['GROUP'].astype(str)\ntrain_csv","4217ea83":"train_csv_color = pd.DataFrame()\ntrain_csv_nuc   = pd.DataFrame()\ntrain_csv_cyt   = pd.DataFrame()\n\ntrain_csv_color['GROUP'] = train_csv['GROUP']\ntrain_csv_nuc['GROUP']   = train_csv['GROUP']\ntrain_csv_cyt['GROUP']   = train_csv['GROUP']\n\n# Getting the path to each file\ntrain_csv_color['filename'] = train_csv.index.astype(str) + '.bmp'\ntrain_csv_nuc['filename']   = train_csv.index.astype(str) + '_segNuc.bmp'\ntrain_csv_cyt['filename']   = train_csv.index.astype(str) + '_segCyt.bmp'","a5fe55b8":"from keras.preprocessing.image import load_img\n\nimgs_color = [[], [], [], [], [], [], [], [], []]\nimgs_nuc = [[], [], [], [], [], [], [], [], []]\nimgs_cyt = [[], [], [], [], [], [], [], [], []]\n\n\nfor row in train_csv_color.values:\n    # Get the actual gruop\n    current_class = int(row[0])\n    \n    # Load all images associated with current index\n    # Append them in the right index\n    img_col = load_img(f'\/kaggle\/input\/ima205challenge2021bis\/Train\/Train\/{row[1]}')\n    imgs_color[current_class].append(img_col)\n    \n    img_nuc = load_img(f'\/kaggle\/input\/ima205challenge2021bis\/Train\/Train\/{row[1][:-4] + \"_segNuc.bmp\"}')\n    imgs_nuc[current_class].append(img_nuc)\n    \n    img_cyt = load_img(f'\/kaggle\/input\/ima205challenge2021bis\/Train\/Train\/{row[1][:-4] + \"_segCyt.bmp\"}')\n    imgs_cyt[current_class].append(img_cyt)\n\n    ","d8f8d5ce":"for grp in range(9):\n    plt.figure(figsize=(10,5)).suptitle(f'Group {grp} colored')\n\n    for i in range(2):\n        for j in range(4):\n            plt.subplot(2,4, 2*j+i+1)\n            plt.imshow(imgs_color[grp][2*j+i])\n    plt.show()\n\n    plt.figure(figsize=(10,5)).suptitle(f'Group {grp} nuc')\n\n    for i in range(2):\n        for j in range(4):\n            plt.subplot(2,4, 2*j+i+1)\n            plt.imshow(imgs_nuc[grp][2*j+i], cmap='gray')\n    plt.show()\n\n    plt.figure(figsize=(10,5)).suptitle(f'Group {grp} cyt')\n    for i in range(2):\n        for j in range(4):\n            plt.subplot(2,4, 2*j+i+1)\n            plt.imshow(imgs_cyt[grp][2*j+i], cmap='gray')\n    plt.show()\n","0b7f8408":"train_data_dir = '\/kaggle\/input\/ima205challenge2021bis\/Train\/Train\/'\ntest_data_dir = '\/kaggle\/input\/ima205challenge2021bis\/Test\/Test\/'\n\n# Img and batch params\nimg_height = 224\nimg_width = 224\nbatch_size = 8\nseed=42\n\ntrain_datagen = ImageDataGenerator(rescale=1.\/255,\n    # Data augmentation params\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.2) # set validation split\n\ntrain_generator_color = train_datagen.flow_from_dataframe(\n    train_csv_color,\n    directory=train_data_dir,\n    x_col='filename',\n    y_col='GROUP',\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    seed=seed) # set as training data\n\nvalidation_generator_color = train_datagen.flow_from_dataframe(\n    train_csv_color,\n    directory=train_data_dir,\n    x_col='filename',\n    y_col='GROUP',\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    seed=seed) # set as validation data\n\ntrain_generator_nuc = train_datagen.flow_from_dataframe(\n    train_csv_nuc,\n    directory=train_data_dir,\n    x_col='filename',\n    y_col='GROUP',\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    seed=seed) # set as training data\n\nvalidation_generator_nuc = train_datagen.flow_from_dataframe(\n    train_csv_nuc,\n    directory=train_data_dir,\n    x_col='filename',\n    y_col='GROUP',\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    seed=seed) # set as validation data\n\ntrain_generator_cyt = train_datagen.flow_from_dataframe(\n    train_csv_cyt,\n    directory=train_data_dir,\n    x_col='filename',\n    y_col='GROUP',\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    seed=seed) # set as training data\n\nvalidation_generator_cyt = train_datagen.flow_from_dataframe(\n    train_csv_cyt,\n    directory=train_data_dir,\n    x_col='filename',\n    y_col='GROUP',\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    seed=seed) # set as validation data","1ddf6a23":"def narea(img_nuc):\n    feature = np.sum(img_nuc==1)\/3\n    return feature\n\ndef carea(img_cyt):\n    feature = np.sum(img_cyt==1)\/3\n    return feature\n\ndef nperim(img_nuc):\n    # The perimiter is whenever the colors change from black to white or white to black (horizontally or vertically)\n    # Logical_or to not count twice\n    feature = np.logical_or(\n        (img_nuc[:,1:] != img_nuc[:,:-1])[1:,:], \n        (img_nuc[1:,:] != img_nuc[:-1,:])[:,1:]\n    ).sum()\n    return feature\n\ndef cperim(img_cyt):\n    feature = np.logical_or(\n        (img_cyt[:,1:] != img_cyt[:,:-1])[1:,:], \n        (img_cyt[1:,:] != img_cyt[:-1,:])[:,1:]\n    ).sum()\n    return feature\n\ndef ncol(img_color, img_nuc):\n    # Conversion from RGB to brightness: 0.299*R+0.587*G+0.114*B\n    img_nuc_colored = img_color*img_nuc\n    feature = 0.299*np.sum(img_nuc_colored[:, :, 0]) + \\\n            0.587*np.sum(img_nuc_colored[:, :, 1]) + \\\n            0.114*np.sum(img_nuc_colored[:, :, 2]) \n    return feature\n\ndef ccol(img_color, img_cyt):\n    # Conversion from RGB to brightness: 0.299*R+0.587*G+0.114*B\n    img_cyt_colored = img_color*img_cyt\n    feature = 0.299*np.sum(img_cyt_colored[:, :, 0]) + \\\n            0.587*np.sum(img_cyt_colored[:, :, 1]) + \\\n            0.114*np.sum(img_cyt_colored[:, :, 2]) \n    return feature\n    \ndef NC(narea, carea):\n    return narea\/carea","e29290d1":"t0 = time()\n\nimg_puras = []\nx_hc_train = []\ny_hc_train = []\nbreak_flag = False\nfor batch_idx, batch in enumerate(zip(train_generator_color,\n                                train_generator_nuc,\n                                train_generator_cyt)):\n    # each variable before the for is a batch\n    imgs_color = batch[0][0]\n    imgs_nuc = batch[1][0]\n    imgs_cyt = batch[2][0]\n    \n    y_color = batch[0][1]\n    for idx, (color, nuc, cyt) in enumerate(\n                        zip(imgs_color, imgs_nuc, imgs_cyt)):\n        current_idx = 8*batch_idx + idx\n        if current_idx > 2336:\n            break_flag = True\n            break\n        x_hc_train.append([])\n        x_hc_train[current_idx].append(narea(nuc))\n        x_hc_train[current_idx].append(carea(cyt))\n        x_hc_train[current_idx].append(nperim(nuc))\n        x_hc_train[current_idx].append(nperim(cyt))\n        x_hc_train[current_idx].append(ncol(color, nuc))\n        x_hc_train[current_idx].append(ccol(color, cyt))\n        x_hc_train[current_idx].append(NC(narea(nuc), carea(cyt)))\n        \n        y_color = batch[0][1][idx]\n        #y_nuc = batch[1][1][idx]\n        #y_cyt = batch[2][1][idx]\n        #assert y_color == y_nuc == y_cyt # Verifying they refer to the same image index\n        y_hc_train.append(y_color)\n        \n    if break_flag:\n        break\n\nx_hc_train = np.array(x_hc_train)\ny_hc_train = np.array(y_hc_train)\nprint(f'It took {round(time()-t0, 2)} seconds!')","209431ae":"# Doing the same for the validation\n\nt0 = time()\n\nimg_puras = []\nx_hc_val = []\ny_hc_val = []\nbreak_flag = False\nfor batch_idx, batch in enumerate(zip(validation_generator_color,\n                                validation_generator_nuc,\n                                validation_generator_cyt)):\n    # each variable before the for is a batch\n    imgs_color = batch[0][0]\n    imgs_nuc = batch[1][0]\n    imgs_cyt = batch[2][0]\n    \n    y_color = batch[0][1]\n    for idx, (color, nuc, cyt) in enumerate(\n                        zip(imgs_color, imgs_nuc, imgs_cyt)):\n        current_idx = 8*batch_idx + idx\n        if current_idx > 583:\n            break_flag = True\n            break\n        x_hc_val.append([])\n        x_hc_val[current_idx].append(narea(nuc))\n        x_hc_val[current_idx].append(carea(cyt))\n        x_hc_val[current_idx].append(nperim(nuc))\n        x_hc_val[current_idx].append(nperim(cyt))\n        x_hc_val[current_idx].append(ncol(color, nuc))\n        x_hc_val[current_idx].append(ccol(color, cyt))\n        x_hc_val[current_idx].append(NC(narea(nuc), carea(cyt)))\n        \n        y_color = batch[0][1][idx]\n        #y_nuc = batch[1][1][idx]\n        #y_cyt = batch[2][1][idx]\n        #assert y_color == y_nuc == y_cyt # Verifying they refer to the same image index\n        y_hc_val.append(y_color)\n        \n    if break_flag:\n        break\n\nx_hc_val = np.array(x_hc_val) \ny_hc_val = np.array(y_hc_val)\nprint(f'It took {round(time()-t0, 2)} seconds!')","8794d3e0":"# Verfiying the number of observations in the new features\nprint(x_hc_train.shape, x_hc_val.shape)","5509465f":"from sklearn.ensemble import ExtraTreesClassifier\n\nrs=42\n\nt0 = time()\n\ny_hc_train = y_hc_train.astype(int)\n\n# Gridsearch params\nall_models_params = {\n#     'SVC': {\n#         'model': SVC(random_state=rs),\n#         'param': {\n#             'C': [0.01, 0.1, 1, 10, 100, 500, 1000],\n#             'kernel': ['poly', 'rbf']\n#         }\n#     },\n    \n#     'LR_L2norm': {\n#         'model': LogisticRegression(solver='lbfgs', multi_class='ovr', random_state=rs, max_iter=1000),\n#         'param': {\n#             'C': [0.001, 0.01, 0.1, 1, 10, 100, 500],\n#         }\n#     },\n\n#     'LR_L1norm': {\n#         'model': LogisticRegression(penalty='l1', solver='saga', multi_class='ovr', random_state=rs, max_iter=1000),\n#         'param': {\n#             'C': [0.001, 0.01, 0.1, 1, 10,100, 500],\n#         }\n#     },\n\n#     'KNN': {\n#         'model': KNeighborsClassifier(),\n#         'param': {\n#             'n_neighbors': list(range(5, 21))\n#         }\n#     },\n\n#     'XGBoost': {\n#         'model': XGBClassifier(objective='multi:softmax', use_label_encoder=False),#, eval_metric='logloss'),\n#         'param': {\n#             \"learning_rate\"    : [0.1, 1, 10],\n#             \"max_depth\"        : [2, 4, 6],\n#             \"gamma\"            : [ 0.1, 0.5, 1 ],\n#         }\n#     },\n\n    'ExtraTrees': {\n        'model': ExtraTreesClassifier(),\n        'param': {\n            'n_estimators': [200, 500, 800],\n#             'min_samples_leaf': [4, 6],\n#             'min_samples_split': [2, 5, 10]\n        }\n    }\n}\n\nperformances = []\nmodels_clf = {}\nfor model_str, model_params in all_models_params.items():\n    print(f'{model_str} at {time()-t0} seconds')\n    model_name = model_params['model']\n    params = model_params['param']\n    clf = GridSearchCV(model_name, params, scoring='f1_macro', cv=3, verbose=3)\n    clf.fit(x_hc_train, y_hc_train)\n    performances.append({\n        'model': model_str,\n        'train_score': clf.best_score_,\n        'test_score': clf.score(x_hc_val, y_hc_val),\n#        'test_acc': round(100*np.sum(clf.predict(x_hc_val) == y_hc_val)\/len(x_hc_val), 2),\n        'best_params': clf.best_params_\n    })\n    models_clf[model_str] = clf.best_estimator_ # Saving best estimator\n\nperformances = pd.DataFrame(performances)\nperformances","3b2867cb":"from tensorflow.keras.applications.vgg19 import VGG19\n\nbase_model = VGG19(input_shape = (img_height, img_width, 3), # Shape of our images\n                    include_top = False, # Leave out the last fully connected layer\n                    weights = 'imagenet')\n\nfor layer in base_model.layers:\n    layer.trainable = True\n    \n# Flatten the output layer to 1 dimension\nx = layers.Flatten()(base_model.output)\n# Add a final softmax layer for classification\nx = layers.Dense(9, activation='softmax')(x)\n\n# Create a model using input from pre trained model and our output x adapted to our problem\nmodel = tf.keras.models.Model(base_model.input, x)\n\n# Learning rate found empiracally\nmodel.compile(optimizer = tf.keras.optimizers.Adam(lr=0.00001), loss = 'CategoricalCrossentropy', metrics = ['acc'], \n              weighted_metrics=['accuracy'])\n\n# EarlyStopping to avoid overfitting using a validation dataset\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=30)\nvgghist = model.fit(\n    train_generator_color,\n    validation_data = validation_generator_color,\n    steps_per_epoch = 100,\n    epochs = 30,\n    callbacks=[callback])","2adaf486":"from tensorflow.keras.applications.inception_v3 import InceptionV3\nbase_model = InceptionV3(input_shape = (img_height, img_width, 3), include_top = False, weights = 'imagenet')\nfor layer in base_model.layers:\n    layer.trainable = True\n    \nx = layers.Flatten()(base_model.output)\nx = layers.Dense(9, activation='softmax')(x)\n\nmodel_Inception = tf.keras.models.Model(base_model.input, x)\n\n\n# Learning rate found empiracally\nmodel_Inception.compile(optimizer = tf.keras.optimizers.Adam(lr=0.00001), loss = 'CategoricalCrossentropy', metrics = ['acc'], \n              weighted_metrics=['accuracy'])\n\n# EarlyStopping to avoid overfitting using a validation dataset\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=5)\nincephist = model_Inception.fit(\n    train_generator_color,\n    validation_data = validation_generator_color,\n    steps_per_epoch = 100,\n    epochs = 25,\n    callbacks=[callback])","3a77c6c1":"from tensorflow.keras.applications import ResNet50\nbase_model = ResNet50(input_shape = (img_height, img_width, 3), include_top = False, weights='imagenet')\n\nfor layer in base_model.layers:\n    layer.trainable = True\n    \nx = layers.Flatten()(base_model.output)\nx = layers.Dense(9, activation='softmax')(x)\n\nresnet = tf.keras.models.Model(base_model.input, x)\nstep = tf.Variable(0, trainable=False)\nlr_schedule = tf.keras.optimizers.schedules.PiecewiseConstantDecay(\n        boundaries=[10, 8],\n        values=[0.0001, 0.00001, 0.000001])\n\nresnet.compile(tf.keras.optimizers.Adam(lr=lr_schedule(step)),loss='CategoricalCrossentropy', metrics = ['acc'], \n              weighted_metrics=['accuracy'])\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss', mode='min', patience=5)\neff_history = resnet.fit(\n    train_generator_color,\n    validation_data = validation_generator_color,\n    steps_per_epoch = 100,\n    epochs = 30,\n    callbacks=[callback])","c5fb314d":"from tensorflow.keras.applications import EfficientNetB7\nbase_model = EfficientNetB7(weights = 'imagenet', input_shape = (img_height, img_width, 3), include_top = False)#, weights = 'imagenet')\n\n\nfor layer in base_model.layers:\n    layer.trainable = True\n    \nx = layers.Flatten()(base_model.output)\nx = layers.Dense(9, activation='softmax')(x)\n\neffnet = tf.keras.models.Model(base_model.input, x)\neffnet.compile(tf.keras.optimizers.Adam(lr=0.00001),loss='CategoricalCrossentropy', metrics = ['acc'], \n              weighted_metrics=['accuracy'])\ncallback = tf.keras.callbacks.EarlyStopping(monitor='val_loss', mode='min', patience=5)\neff_history = effnet.fit(\n    train_generator_color,\n    validation_data = validation_generator_color,\n    steps_per_epoch = 100,\n    epochs = 30,\n    callbacks=[callback])","20a16b2b":"model_Inception.summary()","143acad6":"# Defining the generators to reset them (otherwise we would lose the index)\n\ntrain_datagen = ImageDataGenerator(rescale=1.\/255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    horizontal_flip=True,\n    validation_split=0.2) # set validation split\n\ntrain_generator_color = train_datagen.flow_from_dataframe(\n    train_csv_color,\n    directory=train_data_dir,\n    x_col='filename',\n    y_col='GROUP',\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n    seed=seed) # set as training data\n\nvalidation_generator_color = train_datagen.flow_from_dataframe(\n    train_csv_color,\n    directory=train_data_dir,\n    x_col='filename',\n    y_col='GROUP',\n    target_size=(img_height, img_width),\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    seed=seed) # set as validation data\n\nlayer_name='flatten_1' # name of last layer of Inception\n\n# Getting model without output \nintermediate_layer_model = Model(inputs=model_Inception.input,\n                                 outputs=model_Inception.get_layer(layer_name).output)\n\n# Generating features\nintermediate_output = intermediate_layer_model.predict(train_generator_color) \n\n# Getting index in the order the generator used\nfilenames=train_generator_color.filenames\n\n# Removes the final \".bmp\"\nfor idx in range(len(filenames)):\n    filenames[idx] = filenames[idx][:-4]\n    \n# Connect each index to a feature in a DataFrame\nintermediate_output=pd.DataFrame(data=[intermediate_output][0],\n                    index=filenames)\nintermediate_output","47784193":"# Converting to int and adding index name to compatibility with XGBoost\nintermediate_output.index = intermediate_output.index.astype(int)\nintermediate_output.index.name='ID'\n#intermediate_output = intermediate_output.join(train_csv['ABNOGROUPRMAL'])\n#intermediate_output['GROUP']","7ab588a7":"# Get features from validation\nintermediate_val = intermediate_layer_model.predict(validation_generator_color)\n\n# Get index order used by the generator\nfilenames=validation_generator_color.filenames\n\n# Removes \".bmp\"\nfor idx in range(len(filenames)):\n    filenames[idx] = filenames[idx][:-4]\n\n# Transforms into DataFrame\nintermediate_val=pd.DataFrame(data=[intermediate_val][0],\n                    index=filenames)\n\n# Converting index to int and adding a name to the column\nintermediate_val.index = intermediate_val.index.astype(int)\nintermediate_val.index.name='ID'\n#intermediate_val = intermediate_val.join(train_csv['GROUP'])\n#intermediate_val['GROUP']","46cbf6c3":"from xgboost import XGBClassifier\n\n# Create xgb instance, fits and verify the score with the validation test\nxgbmodel = XGBClassifier(objective='multi:softmax')\nxgbmodel.fit(intermediate_output, intermediate_output.join(train_csv['GROUP'])['GROUP'], verbose=1)\nxgbmodel.score(intermediate_val, intermediate_val.join(train_csv['GROUP'])['GROUP'])","bbcf88a9":"# Score in the training set\nxgbmodel.score(intermediate_output, intermediate_output.join(train_csv['GROUP'])['GROUP'])","19258743":"# Create the gridsearch params, fit and see the performance with the validation set\nET_params = {\n            'n_estimators': [200, 500, 800],\n#             'min_samples_leaf': [4, 6],\n#             'min_samples_split': [2, 5, 10]\n        }\nclf = GridSearchCV(ExtraTreesClassifier(), ET_params, scoring='f1_macro', cv=3, verbose=3)\nclf.fit(intermediate_output, intermediate_output.join(train_csv['ABNORMAL'])['ABNORMAL'].astype(int))\n\nclf.best_estimator_.score(intermediate_val, intermediate_val.join(train_csv['ABNORMAL'])['ABNORMAL'].astype(int))","4d8d906b":"# Creating test generators\n\ntest_csv_path = '\/kaggle\/input\/ima205challenge2021bis\/SampleSubmission2.csv'\ntest_csv = pd.read_csv(test_csv_path, index_col=0)\n\ntest_csv_color = pd.DataFrame()\ntest_csv_nuc   = pd.DataFrame()\ntest_csv_cyt   = pd.DataFrame()\n\ntest_csv_color['filename'] = test_csv.index.astype(str) + '.bmp'\ntest_csv_nuc['filename']   = test_csv.index.astype(str) + '_segNuc.bmp'\ntest_csv_cyt['filename']   = test_csv.index.astype(str) + '_segCyt.bmp'\n\n\ntest_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntest_generator_color = test_datagen.flow_from_dataframe(\n    test_csv_color,\n    directory='\/kaggle\/input\/ima205challenge2021bis\/Test\/Test\/',\n    x_col='filename',\n    y_col=None,\n    shuffle=False,\n    target_size=(img_height, img_width),\n    batch_size=20,\n    class_mode=None)\n\ntest_generator_nuc = test_datagen.flow_from_dataframe(\n    test_csv_nuc,\n    directory='\/kaggle\/input\/ima205challenge2021bis\/Test\/Test\/',\n    x_col='filename',\n    y_col=None,\n    shuffle=False,\n    target_size=(img_height, img_width),\n    batch_size=20,\n    class_mode=None)\n\ntest_generator_cyt = test_datagen.flow_from_dataframe(\n    test_csv_cyt,\n    directory='\/kaggle\/input\/ima205challenge2021bis\/Test\/Test\/',\n    x_col='filename',\n    y_col=None,\n    shuffle=False,\n    target_size=(img_height, img_width),\n    batch_size=20,\n    class_mode=None)","d48dd75f":"predictions = model.predict(test_generator_color, steps=len(test_csv), verbose=1)\npredictions = np.argmax(predictions, axis=1)\nfilenames=test_generator_color.filenames","67dd48cf":"predictions","be64900d":"for idx in range(len(filenames)):\n    filenames[idx] = filenames[idx][:-4]\nresults=pd.DataFrame({\"ID\":filenames,\n                      \"GROUP\":predictions})\nresults.to_csv(\"results.csv\",index=False)","f2075c3b":"results.iloc[10:20]","5ad266ae":"# Plotting the different groups","18814765":"# Image generators","610f455f":"# Neural networks as feature creators","2dc6c1fa":"Since we have the same images but in a multiclass problem, a lot of the first code was used again.\n\nThus most of non-code explanation won't be made twice (eg: why image generators, shape distribution, from where are the handcrafted features, etc).","420bb3a3":"All neural networks perfomed better than the best model using handcrafted features. I believe there are two big reasons:\n\n1) Handcrafted features are hard to create with images in different settings. The 7 features used, based on the paper Pap-smear Benchmark Data For Pattern Classification by Jantzen et al (2005), were not scale independent, nor zoom independent. Hence by having images with different sizes and without a standart in the photo configuration made most of features lose its original meaning and thus a part of their relevance.\n\n2) I only used pre-trained neural networks. The idea of using pre-trained neural networks is that they are able to detect different features in images which doesn't depend on the output. For instance, if they can detect a circle it can be useful regardless of the problem. In order to do this, the pre-trained models had access to another training set related to another problem. For instance, in all pre-trained models I used, they were trained in the ImageNet, a dataset whose problem is to classify between 1000 images. Its size in Kaggle competition is over 150GB, thus the models were trained with a huge amount of images to a very hard problem. \n\nAs a food for thought, I argue that people who used pre-trained models, me included, actually cheated on the competition because since the pre-trained models were trained in another dataset, we used extra data, although indirectly. And the challenger clearly says: **You can only use the data provided in this challenge**.\n\nTo improve the results I thought about 2 strategies:\n\n1) Include the Nuc and Cyt data in the predictions by neural networks. A possible way would be to create images with 5 dimensions per pixel: red, green, blue, Nuc mask and Cyt mask. That way the neural networks would be able to analyze the three images provided for each ID. At the moment, the information provided by Nuc and Cyt were discarded. I tried before to feed only Nuc or only Cyt to the neural networks to do the predicitons, hwoever the result wasn't satisfactory (< 90% acc).\n\n2) Use a bagging idea with stacking of models. I would need to research more about how it is done with multiclass problems, since we might have a lot of draws in the voting.","f1574ea9":"We can see the different groups have different colors and different shapes.","06b7ea9f":"# Pre-trained neural networks","a744cd0f":"# Conclusion","5b74116a":"# Handcrafted features"}}