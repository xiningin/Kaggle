{"cell_type":{"769f2045":"code","531de9e2":"code","4f5996e1":"code","cd27d50c":"code","bfdf7dad":"code","0619b8d8":"code","9555638e":"code","17d620dd":"code","fd9164c5":"code","cd3fcc0c":"code","92ffeaca":"code","c4818853":"code","caa561c8":"code","04e315f8":"code","99e1e858":"code","d7e5bdff":"code","4288b7ca":"code","455eabf8":"markdown","eaad2dea":"markdown"},"source":{"769f2045":"import os \n\nis_inference_flag = True\ntry:\n    tweet_models_dir = os.listdir('\/kaggle\/input\/albert-xl-300\/')\n    if len(tweet_models_dir) > 0: \n        is_inference_flag = True\nexcept:\n    is_inference_flag = False","531de9e2":"print('Inference flag status :', is_inference_flag)","4f5996e1":"if not is_inference_flag:\n    !git clone https:\/\/github.com\/AnandAwasthi\/Closed-domain-Question-Answering-fine-tune-Albert\n    ","cd27d50c":"\nimport pandas as pd\nimport numpy as np\nimport json\nfrom sklearn.model_selection import train_test_split","bfdf7dad":"train_df = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\nsub_df = pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv')\n\n","0619b8d8":"def jaccard(str1, str2):\n    a = str1.lower().split()\n    b = str2.lower().split()\n    c = set(a).intersection(set(b))\n    return float(len(c)) \/ (len(a) + len(b) - len(c))","9555638e":"train_df.dropna(inplace=True)","17d620dd":"X_train, X_test = train_test_split(train_df,test_size=0.10, random_state=42)","fd9164c5":"train = np.array(X_train)\nval = np.array(X_test)\ntest = np.array(test_df)\nuse_cuda = True","cd3fcc0c":"!mkdir -p data\n!mkdir -p data\/models\/albert","92ffeaca":"%%time\n\n\"\"\"\nPrepare training data in QA-compatible format\n\"\"\"\n\n# Adpated from https:\/\/www.kaggle.com\/cheongwoongkang\/roberta-baseline-starter-simple-postprocessing\ndef find_all(input_str, search_str):\n    l1 = []\n    length = len(input_str)\n    index = 0\n    while index < length:\n        i = input_str.find(search_str, index)\n        if i == -1:\n            return l1\n        l1.append(i)\n        index = i + 1\n    return l1\n\ndef do_qa_train(train):\n    output = {}\n    output['version'] = 'v1.0'\n    output['data'] = []\n    paragraphs = []\n    for line in train:\n        context = line[1]\n\n        qas = []\n        question = line[-1]\n        qid = line[0]\n        answers = []\n        answer = line[2]\n        if type(answer) != str or type(context) != str or type(question) != str:\n            print(context, type(context))\n            print(answer, type(answer))\n            print(question, type(question))\n            continue\n        answer_starts = find_all(context, answer)\n        for answer_start in answer_starts:\n            answers.append({'answer_start': answer_start, 'text': answer.lower()})\n            break\n        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n        paragraphs.append({'context': context.lower(), 'qas': qas})\n        #output['data'].append({'title': 'None', 'paragraphs': paragraphs})\n\n    output['data'].append({'title': 'None', 'paragraphs': paragraphs})\n    return output\n\nif not is_inference_flag:\n    qa_X_train = do_qa_train(train)\n    qa_X_test = do_qa_train(val)\n\n    with open('data\/train.json', 'w') as outfile:\n        json.dump(qa_X_train, outfile)\n\n    with open('data\/val.json', 'w') as outfile:\n        json.dump(qa_X_test, outfile)","c4818853":"%%time\n\n\"\"\"\nPrepare testing data in QA-compatible format\n\"\"\"\n\n\ndef convert_test_qa_json(test):\n    output = {}\n    output['version'] = 'v1.0'\n    output['data'] = []\n    paragraphs = []\n    for line in test:\n        \n        context = line[1]\n        qas = []\n        question = line[-1]\n        qid = line[0]\n        if type(context) != str or type(question) != str:\n            print(context, type(context))\n            print(answer, type(answer))\n            print(question, type(question))\n            continue\n        answers = []\n        answers.append({'answer_start': 1000000, 'text': '__None__'})\n        qas.append({'question': question, 'id': qid, 'is_impossible': False, 'answers': answers})\n\n        paragraphs.append({'context': context.lower(), 'qas': qas})\n    \n    output['data'].append({'title': 'None', 'paragraphs': paragraphs})\n    return output\n\nqa_test = convert_test_qa_json(test)\n\nwith open('data\/test.json', 'w') as outfile:\n    json.dump(qa_test, outfile)\n","caa561c8":"if not is_inference_flag:\n    !export SQUAD_DIR=data \\\n    && python Closed-domain-Question-Answering-fine-tune-Albert\/bsie\/transformers\/examples\/run_squad.py \\\n      --model_type albert \\\n      --model_name_or_path albert-base-v2 \\\n      --do_train \\\n      --do_eval \\\n      --do_lower_case \\\n      --train_file $SQUAD_DIR\/train.json \\\n      --predict_file $SQUAD_DIR\/val.json \\\n      --per_gpu_train_batch_size 12 \\\n      --learning_rate 5e-5 \\\n      --num_train_epochs 1.0 \\\n      --max_seq_length 192 \\\n      --doc_stride 64 \\\n      --output_dir $SQUAD_DIR\/models\/albert \\\n      --save_steps 100000 \\\n      --threads 4 \\\n      --version_2_with_negative \\\n      --overwrite_output_dir","04e315f8":"import os\nimport torch\nimport time\nfrom torch.utils.data import DataLoader, RandomSampler, SequentialSampler\nfrom collections import OrderedDict\nfrom transformers import (\n    AutoConfig,\n    AutoModelForQuestionAnswering,\n    AutoTokenizer,\n    squad_convert_examples_to_features\n)\n\nfrom transformers.data.processors.squad import SquadResult, SquadV2Processor, SquadExample\n\nfrom transformers.data.metrics.squad_metrics import compute_predictions_logits\n\nif not is_inference_flag:\n  model_name_or_path = \"data\/models\/\"\nelse:\n  model_name_or_path = '\/kaggle\/input\/albert-xl-300\/'\n\noutput_dir = \"\"\n\n# Config\nn_best_size = 1\nmax_answer_length = 254\ndo_lower_case = True\nnull_score_diff_threshold = 0.0\n\ndef to_list(tensor):\n    return tensor.detach().cpu().tolist()\n\n# Setup model\nconfig_class, model_class, tokenizer_class = (\n   AutoConfig, AutoModelForQuestionAnswering, AutoTokenizer)\nconfig = config_class.from_pretrained(model_name_or_path)\ntokenizer = tokenizer_class.from_pretrained(\n    model_name_or_path, do_lower_case=True)\nmodel = model_class.from_pretrained(model_name_or_path, config=config)\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\nmodel.to(device)\n\nprocessor = SquadV2Processor()\n\ndef run_prediction(question_texts, context_text):\n    \"\"\"Setup function to compute predictions\"\"\"\n    if question_texts[0] != 'neutral':\n        examples = []\n\n        for i, question_text in enumerate(question_texts):\n            example = SquadExample(\n                qas_id=str(i),\n                question_text=question_text,\n                context_text=context_text,\n                answer_text=None,\n                start_position_character=None,\n                title=\"Predict\",\n                is_impossible=False,\n                answers=None,\n            )\n\n            examples.append(example)\n\n        features, dataset = squad_convert_examples_to_features(\n            examples=examples,\n            tokenizer=tokenizer,\n            max_seq_length=300,\n            doc_stride=128,\n            max_query_length=64,\n            is_training=False,\n            return_dataset=\"pt\",\n            threads=1,\n        )\n\n        eval_sampler = SequentialSampler(dataset)\n        eval_dataloader = DataLoader(dataset, sampler=eval_sampler, batch_size=10)\n\n        all_results = []\n\n        for batch in eval_dataloader:\n            model.eval()\n            batch = tuple(t.to(device) for t in batch)\n\n            with torch.no_grad():\n                inputs = {\n                    \"input_ids\": batch[0],\n                    \"attention_mask\": batch[1],\n                    \"token_type_ids\": batch[2],\n                }\n\n                example_indices = batch[3]\n\n                outputs = model(**inputs)\n\n                for i, example_index in enumerate(example_indices):\n                    eval_feature = features[example_index.item()]\n                    unique_id = int(eval_feature.unique_id)\n\n                    output = [to_list(output[i]) for output in outputs]\n\n                    start_logits, end_logits = output\n                    result = SquadResult(unique_id, start_logits, end_logits)\n                    all_results.append(result)\n\n        output_prediction_file = \"predictions.json\"\n        output_nbest_file = \"nbest_predictions.json\"\n        output_null_log_odds_file = \"null_predictions.json\"\n\n        predictions = compute_predictions_logits(\n            examples,\n            features,\n            all_results,\n            n_best_size,\n            max_answer_length,\n            do_lower_case,\n            output_prediction_file,\n            output_nbest_file,\n            output_null_log_odds_file,\n            False,  # verbose_logging\n            True,  # version_2_with_negative\n            null_score_diff_threshold,\n            tokenizer,\n        )\n    else:\n        predictions = OrderedDict([(0, context_text)])\n\n    return predictions","99e1e858":"%%time\n# Infering on trained model for Jaccard score\njaccard_scores = []\npredictions_x_test = []\nfor index, row in X_test.head(10).iterrows():\n    context = row['text']\n    selected_text = row['selected_text']\n    questions = [row['sentiment']]\n    preds_dict = run_prediction(questions, context)\n    for key in preds_dict.keys():\n        predicted_text = preds_dict[key] \n        predictions_x_test.append({'selected_text': selected_text,'predicted_text': predicted_text, 'sentiment': row['sentiment'], 'textID': row['textID']})\n        jaccard_score = jaccard(selected_text, predicted_text)\n        jaccard_scores.append(jaccard_score)\nprint('Jaccard Score', np.mean(jaccard_scores))\n\npredictions_x_test_df = pd.DataFrame.from_dict(predictions_x_test)\n\n\npredictions_x_test_df.head()","d7e5bdff":"test_df.shape","4288b7ca":"%%time\n# Infering on trained model\npredictions = []\n\nfor index, row in test_df.iterrows():\n    context = row['text']\n    questions = [row['sentiment']]\n    preds_dict = run_prediction(questions, context)\n    for key in preds_dict.keys():\n        predicted_text = preds_dict[key] \n        predictions.append({'textID': row['textID'], 'selected_text': predicted_text})\n        \npredictions_df = pd.DataFrame.from_dict(predictions)\noutput_df = sub_df.merge(predictions_df, on ='textID')\npredictions_df.to_csv('submission.csv', index=False)\n\npredictions_df.head()","455eabf8":"# <a name=\"Training\" id=\"3\"><\/a> 3. Model Training ....\n\nUsing my training kernel out to save GPU time.<br>\nTo train uncomment train command.","eaad2dea":"# <a name=\"Infer\" id=\"4\"><\/a> 4. Model INFER"}}