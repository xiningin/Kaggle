{"cell_type":{"41c3a63a":"code","49e2519f":"code","ea3c1aff":"code","0e7a8d82":"code","bd2c87c9":"code","c28306ec":"code","3a214b29":"code","78731a43":"code","1dc023ca":"code","0aea2250":"code","fd0f7c54":"code","34a71ae5":"code","a736d201":"code","99ae23af":"code","0ecf7039":"code","0fa008e2":"code","6debd5f0":"code","fc30e770":"code","6b3a2d29":"code","4e8c3bf0":"code","f7150553":"code","91917f16":"code","4131864c":"code","2d8e2dec":"code","cdd6dddf":"code","78a8b2a1":"code","b6504fb4":"code","d8ab80b3":"code","67f78a5c":"code","ebb1cb7e":"code","17f3c8a0":"code","c85b55c3":"code","beaca0d7":"code","5918f9ac":"code","00d669a6":"code","60eabdeb":"code","06ae40fd":"code","f18f7e51":"code","965821e0":"code","3a1e4593":"code","fd1b529b":"code","976048ca":"code","c6c49d5c":"code","b469610d":"code","c0c119c9":"code","d8f6fa4b":"code","2c616127":"code","02571a88":"code","5e6de8d6":"code","b032fa16":"code","5c431a70":"code","031b474b":"code","e1d910cc":"code","765f3794":"code","ed16a465":"code","0f8b3a42":"code","9c664913":"code","4266ff79":"code","0b5eb31e":"code","1d0495e6":"code","08011499":"code","a93bb494":"code","c2461a63":"code","f1100614":"code","7b8ea190":"code","6b80e03d":"code","ebb069d0":"markdown","54be4171":"markdown","f977510c":"markdown","cb438bfb":"markdown","0af2e9d5":"markdown","755c353a":"markdown","3141ff08":"markdown","763e15b6":"markdown","90b82f8e":"markdown","e055139d":"markdown"},"source":{"41c3a63a":"import nltk","49e2519f":"from nltk.corpus import brown","ea3c1aff":"brown.words()","0e7a8d82":"brown.categories()\n","bd2c87c9":"print(len(brown.categories()))","c28306ec":"data = brown.sents(categories = 'adventure')","3a214b29":"len(data)","78731a43":"data = brown.sents(categories = 'fiction')","1dc023ca":"data","0aea2250":"\" \".join(data[1])","fd0f7c54":"document = '''it was a very pleasent day. the weather was cool and there were light showers.\ni went to the market to buty some fruits'''\n\nsentence = \"Send all the 50 document related to chapters 1,2,3,4 at prateek@cb.com\"","34a71ae5":"from nltk.tokenize import sent_tokenize, word_tokenize","a736d201":"sents = sent_tokenize(document)\nprint(sents)","99ae23af":"print(len(sents))\n","0ecf7039":"sentence.split()","0fa008e2":"word = word_tokenize(sentence)","6debd5f0":"word","fc30e770":"from nltk.corpus import stopwords\nsw = set(stopwords.words('english'))\n","6b3a2d29":"print(sw)","4e8c3bf0":"def remove_stopwords(text,stopwords):\n    useful_words = [w for w in text if w not in stopwords]\n    return useful_words","f7150553":"text = \"i am not bothered about her very much\".split()\nuseful_text = remove_stopwords(text,sw)\nprint(useful_text)","91917f16":"'not' in sw","4131864c":"sentence = \"Send all the 50 document related to chapters 1,2,3,4 at prateek@cb.com\"\nfrom nltk.tokenize import RegexpTokenizer","2d8e2dec":"tokenizer = RegexpTokenizer('[a-zA-Z]')\nuseful_text = tokenizer.tokenize(sentence)","cdd6dddf":"useful_text","78a8b2a1":"tokenizer = RegexpTokenizer('[a-zA-Z@.]+')\nuseful_text = tokenizer.tokenize(sentence)","b6504fb4":"useful_text","d8ab80b3":"text   = \"\"\"Foxes love to make jumpes. the quick brown fox was seen jumping over the\nlovely dog from a 6th feet high wall\"\"\"","67f78a5c":"from nltk.stem.snowball import SnowballStemmer,PorterStemmer\nfrom nltk.stem.lancaster import LancasterStemmer\n#Snowball Stemmer ,Porter ,Lancaster Stemmer","ebb1cb7e":"ps = PorterStemmer()","17f3c8a0":"ps.stem('jumping')","c85b55c3":"ps.stem('lovely')","beaca0d7":"ps.stem('loving')","5918f9ac":"ps.stem('jumped')","00d669a6":"# let's work with snowball stemmer\nss = SnowballStemmer('english')","60eabdeb":"ss.stem('jumping')","06ae40fd":"'''# Lemitization\nfrom nltk.stem import WordNetLemmatizer\nwn = WordNetLemmatizer()\nwn.lemmatize('jumping')'''","f18f7e51":"# Sample Corpus - Contains 4 Documents, each document can have 1 or more sentences\ncorpus = [\n        'Indian cricket team will wins World Cup, says Capt. Virat Kohli. World cup will be held at Sri Lanka.',\n        'We will win next Lok Sabha Elections, says confident Indian PM',\n        'The nobel laurate won the hearts of the people.',\n        'The movie Raazi is an exciting Indian Spy thriller based upon a real story.'\n]\n","965821e0":"from sklearn.feature_extraction.text import CountVectorizer","3a1e4593":"cv = CountVectorizer()","fd1b529b":"vectorized_corpus = cv.fit_transform(corpus)","976048ca":"vectorized_corpus = vectorized_corpus.toarray()","c6c49d5c":"vectorized_corpus[0]","b469610d":"print(cv.vocabulary_)","c0c119c9":"len(cv.vocabulary_.keys())","d8f6fa4b":"#reverse maping\nnumbers = vectorized_corpus[2]\nnumbers","2c616127":"s = cv.inverse_transform(numbers)\nprint(s)","02571a88":"def myTokenizer(document):\n    words = tokenizer.tokenize(document.lower())\n    # Remove Stopwords\n    words = remove_stopwords(words,sw)\n    return words\n    ","5e6de8d6":"#myTokenizer(sentence)\n#print(sentence)","b032fa16":"cv = CountVectorizer(tokenizer = myTokenizer)","5c431a70":"vectorized_corpus = cv.fit_transform(corpus).toarray()","031b474b":"print(vectorized_corpus)\n","e1d910cc":"print(len(vectorized_corpus[0]))","765f3794":"cv.inverse_transform(vectorized_corpus)","ed16a465":"\n# For Test Data\ntest_corpus = [\n        'Indian cricket rock !',        \n]","0f8b3a42":"cv.transform(test_corpus).toarray()","9c664913":"sent_1  = [\"this is good movie\"]\nsent_2 = [\"this is good movie but actor is not present\"]\nsent_3 = [\"this is not good movie\"]","4266ff79":"cv = CountVectorizer(ngram_range=(1,3))","0b5eb31e":"docs = [sent_1[0],sent_2[0]]\ncv.fit_transform(docs).toarray()\n","1d0495e6":"cv.vocabulary_","08011499":"sent_1  = \"this is good movie\"\nsent_2 = \"this was good movie\"\nsent_3 = \"this is not good movie\"\n\ncorpus = [sent_1,sent_2,sent_3]","a93bb494":"from sklearn.feature_extraction.text import TfidfVectorizer\n","c2461a63":"tfidf = TfidfVectorizer()","f1100614":"vc = tfidf.fit_transform(corpus).toarray()","7b8ea190":"\nprint(vc)","6b80e03d":"tfidf.vocabulary_","ebb069d0":"## Tokenization using Regular Expression ","54be4171":"## Tf-idf Normalisation\n1 Avoid features that occur very often,because they contain less information \n\n2 information decreases as the number of occurrences increases across different type of document \n\n3 so we define another term - term-document-frequency which associates a weight with every term","f977510c":"## 1 Tokenisation & Stopword Removal","cb438bfb":"## Building a vocab &vectorization","0af2e9d5":"## Vectorization with Stopword Removal","755c353a":"## Stopwords","3141ff08":"### More ways to Create features \n1 unigram -every word as a feature \n\n2 Bigram \n\n3 Trigram\n\n4 n-grams\n\n5 TF-IDF Normalisation ","763e15b6":"# Natural language preprocessing","90b82f8e":"## Stemming\n1 process that transform particular words (verbs, plurals) into their radical form\n\n2 preserve the semantics of the sentence without increasing the number of unique tokens\n\nexamples - jumps, jumping,jumped ,jump ==>jump","e055139d":"## bag of words pipeline\nget the data\/corpus\n\ntokenisation,stopward removal\n\nstemming\n\nbuilding a vocab\n\nclassification"}}