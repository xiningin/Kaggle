{"cell_type":{"833fc2a6":"code","a1dfe471":"code","e6aafdbb":"code","7a310f9b":"code","ccb9a864":"code","aacabddd":"code","08679aad":"code","a246940e":"code","5f716841":"code","db327429":"code","4bfba650":"code","732233bb":"code","acee65c1":"markdown","d38f86c9":"markdown","9ae9d98f":"markdown","04f29d78":"markdown","28c3908f":"markdown","602e3a72":"markdown"},"source":{"833fc2a6":"# Import libraries\nimport numpy as np  # Linear algebra\nimport pandas as pd  # Data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt  # Data visualization","a1dfe471":"# Load dataset. Images and ground truh are stored as numpy files.\nx_l = np.load('..\/input\/Sign-language-digits-dataset\/X.npy')  # Images of size 64 by 64 pixels in gray scale.\nY_l = np.load('..\/input\/Sign-language-digits-dataset\/Y.npy')  # Labels or ground truth. We will create our custom labels.\nx_l.shape  # Number of images and size","e6aafdbb":"# Show image examples.\nplt.subplot(1,2,1); plt.imshow(x_l[1027]); plt.axis('off'); plt.gray(); plt.title('Sign of one number');\nplt.subplot(1,2,2); plt.imshow(x_l[1855]); plt.axis('off'); plt.gray(); plt.title('Sign of five number');","7a310f9b":"# Select one and five digit images and assign them corresponding labels.\nX = np.concatenate((x_l[822:1027], x_l[1855:2060]), axis=0) # From 822 to 1027 is 'one' sign and from 1855 to 2060 is 'five' sign. 204 images of every class.\nY = np.concatenate((np.zeros(205), np.ones(205)), axis=0)\nprint(\"X shape: \" , X.shape)\nprint(\"Y shape: \" , Y.shape)","ccb9a864":"# Split X and Y into train and test sets: test = 15% and train = 85%\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.15, random_state=42)  # random_state = use same seed while randomizing. It means that if we call train_test_split repeatedly, it always creates same train and test distribution because we have same random_state.\nnumber_of_train = X_train.shape[0]\nnumber_of_test = X_test.shape[0]","aacabddd":"# Flatten every image into a 4096 vector\nX_train_flatten = X_train.reshape(number_of_train, X_train.shape[1]*X_train.shape[2])\nX_test_flatten = X_test .reshape(number_of_test, X_test.shape[1]*X_test.shape[2])\nprint(\"X train flatten\", X_train_flatten.shape)\nprint(\"X test flatten\", X_test_flatten.shape)","08679aad":"# For convinience, traspose all dataset so that every column contain the data of an image.\nx_train = X_train_flatten.T\nx_test = X_test_flatten.T\ny_train = Y_train.T\ny_test = Y_test.T\nprint(\"x train: \",x_train.shape)\nprint(\"x test: \",x_test.shape)\nprint(\"y train: \",y_train.shape)\nprint(\"y test: \",y_test.shape)","a246940e":"# Model initialization: weights and bias\ndef initialize_weights_and_bias(dimension):\n    w = np.random.normal(loc=0.0, scale=1.0, size=(dimension, 1))  # Gaussian random initialization \n    b = 0.0\n    return w,b\ndimension =  x_train.shape[0]  \nw,b = initialize_weights_and_bias(dimension)    ","5f716841":"# Define elements involved in logistic regression\n# Hipothesis model\ndef hipothesis_model(w,b,x_train):\n    # forward propagation\n    z = np.dot(w.T, x_train) + b\n    y_pred = 1\/(1 + np.exp(-z))\n    return y_pred\n    \n# Cost function or loss function\ndef cost_function(y_pred,y_train):\n    eps = np.finfo(float).eps\n    loss = (-y_train * np.log(y_pred + eps)) - ((1 - y_train) * np.log(1 - y_pred + eps))  # For every image sample.\n    cost = np.sum(loss) \/ x_train.shape[1]  \n    return cost\n\n# Cost function gradients \ndef cost_function_gradients(y_pred,y_train):\n    derivative_weight = (np.dot(x_train, ((y_pred - y_train).T))) \/ x_train.shape[1] \n    derivative_bias = np.sum(y_pred - y_train) \/ x_train.shape[1]                 \n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return gradients","db327429":"def training(w, b, x_train, y_train, number_of_iterarion, learning_rate, verbose = True):\n    cost_list = []\n\n    # Gradient Descent\n    for i in range(number_of_iterarion):\n        # Compute prediction with current hipothesis model\n        y_pred = hipothesis_model(w, b, x_train)\n        cost = cost_function(y_pred, y_train)\n        gradients = cost_function_gradients(y_pred,y_train)\n\n        # Save data for representation\n        cost_list.append(cost)\n\n        # Model update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n\n        # Verbose\n        if (verbose and i % 100 == 0):\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n        \n    return w, b, cost_list\n\nnumber_of_iterarion = 2000 #  Number of itertions in gradient descent\nlearning_rate = 0.01  # Learning rate\nw, b, cost_list = training(w, b, x_train, y_train, number_of_iterarion, learning_rate, verbose = True)\n\n# Plot learning curve: cost value along iterations\nplt.plot(cost_list); plt.xticks(rotation='vertical'); plt.xlabel(\"Number of Iterarion\"); plt.ylabel(\"Cost\"); plt.show()","4bfba650":"def predict(w, b, x_test):\n    z = np.dot(w.T, x_test) + b\n    y_pred = 1 \/ (1 + np.exp(-z))\n    y_pred_th = np.zeros((1, x_test.shape[1]))\n    \n    for i in range(y_pred.shape[1]):\n        if z[0,i]<= 0.5:  # if z is smaller than 0.5, our prediction is zero\n            y_pred_th[0,i] = 0\n        else:  # if z is bigger than 0.5, our prediction is one \n            y_pred_th[0,i] = 1\n    return y_pred_th   \n\ny_prediction_test = predict(w, b, x_test)\ny_prediction_train = predict(w, b, x_train)\n\n# Print train\/test accuracy\nprint(\"Train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - y_train)) * 100))\nprint(\"Test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - y_test)) * 100))","732233bb":"from sklearn import linear_model\n# Initialize logistic regression model\nlogreg = linear_model.LogisticRegression(random_state = 42, max_iter = number_of_iterarion)\n# Train model\nmodel = logreg.fit(x_train.T, y_train.T)\n# Results\ny_prediction_test = model.score(x_test.T, y_test.T)\ny_prediction_train = model.score(x_train.T, y_train.T)\nprint(\"test accuracy: {} %\".format(y_prediction_test * 100))\nprint(\"train accuracy: {} %\".format(y_prediction_train * 100))","acee65c1":"# Logistic Regression with Sklearn library","d38f86c9":"# Dataset: load and preprocessing\n\nThe used dataset is \"sign language digits data set\". More details in [[Link]](https:\/\/github.com\/ardamavi\/Sign-Language-Digits-Dataset). Only images of signs indicating one and five digit are used. One-digit images are labeled as 0 and five-digit images as 1. I.e., this is binary classification problem with two classes: one and five digit signs.","9ae9d98f":"## Prediction","04f29d78":"## Training","28c3908f":"# INTRODUCTION\n\nExample of logistic regression with sign language images.","602e3a72":"# Logistic Regression\n"}}