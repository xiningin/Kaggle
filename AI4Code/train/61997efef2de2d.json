{"cell_type":{"ff8b7f18":"code","24cf9cdd":"code","0f600167":"code","2eb24c33":"code","1980a649":"code","a213a4e8":"code","110b0b45":"code","3b2a05da":"code","13c0bf7a":"code","25000f14":"code","a1f0e7b1":"code","e3c733e8":"code","98ac2c3f":"code","aadb1103":"code","54d51329":"code","55aa4781":"code","d282111e":"code","0285f25a":"code","ee45834e":"code","d3c38d0e":"code","b0f6c0ec":"code","15523f6c":"code","fd37cfac":"code","f792f4a6":"code","17acf0d4":"code","96ce991a":"code","e122de2e":"code","3e5ab0f4":"code","0138852f":"code","b51f1603":"code","c321fea0":"code","77d530f4":"code","59f0f0cb":"code","53ad548a":"markdown","19953110":"markdown","cfa76450":"markdown","21abae89":"markdown"},"source":{"ff8b7f18":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport time\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.autograd import Variable\nfrom torch.utils.data import Dataset, DataLoader\n\nimport lightgbm as lgb\nfrom sklearn.metrics import mean_squared_error\n\nimport os\nimport gc\nprint(os.listdir(\"..\/input\"))","24cf9cdd":"import random\nseed = 2357\nrandom.seed(seed)","0f600167":"#Load data\ntrain_df = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/train.csv')\ntest_df = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/test.csv')","2eb24c33":"index_f_data = np.load('..\/input\/split-test-dataset\/index_of_fake_data.npy')\nindex_r_data = np.load('..\/input\/split-test-dataset\/index_of_real_data.npy')","1980a649":"fold_information = pd.read_csv(\"..\/input\/fold-information\/10_fold_information.csv\")","a213a4e8":"splits=[]\nfor i in fold_information.columns:\n    a = np.array(list(set(train_df.index)-set(list(fold_information[i].dropna()))))\n    a.sort()\n    splits.append((np.array(fold_information[i].dropna().astype(int)),a))","110b0b45":"del fold_information\ndel a\ngc.collect()","3b2a05da":"train_features = train_df.drop(['target','ID_code'], axis = 1)\ntest_features = test_df.drop(['ID_code'],axis = 1)\ntrain_target = train_df['target']","13c0bf7a":"fake_data = test_features.iloc[index_f_data]\nreal_data = test_features.iloc[index_r_data]","25000f14":"train_all_real = pd.concat([train_features,real_data], axis = 0)","a1f0e7b1":"for f in train_all_real.columns[0:200]:\n    train_all_real[f+'duplicate'] = train_all_real.duplicated(f,False).astype(int)","e3c733e8":"for f in train_all_real.columns[0:200]:\n    train_all_real[f+'duplicate_count'] = train_all_real.groupby([f])[f].transform('count')\/300000\n    train_all_real[f+'duplicate_count'] = train_all_real[f+'duplicate_count']*train_all_real[f+'duplicate']","98ac2c3f":"for f in train_all_real.columns[0:200]:\n    train_all_real[f+'duplicate_value'] = train_all_real[f]*train_all_real[f+'duplicate']","aadb1103":"train_features_real = train_all_real.iloc[:len(train_target)]\nreal_data = train_all_real.iloc[len(train_target):len(train_all_real)]","54d51329":"train_features_real.shape, real_data.shape","55aa4781":"del train_all_real\ngc.collect()","d282111e":"train_all_fake = pd.concat([train_features,fake_data], axis = 0)","0285f25a":"for f in train_all_fake.columns[0:200]:\n    train_all_fake[f+'duplicate'] = train_all_fake.duplicated(f,False).astype(int)","ee45834e":"for f in train_all_fake.columns[0:200]:\n    train_all_fake[f+'duplicate_value'] = train_all_fake[f]*train_all_fake[f+'duplicate']","d3c38d0e":"train_features_fake = train_all_fake.iloc[:len(train_target)]\nfake_data = train_all_fake.iloc[len(train_target):len(train_all_fake)]","b0f6c0ec":"del train_all_fake\ngc.collect()","15523f6c":"for f in train_features_real.columns[0:200]:\n    train_features_real[f+'distance_of_mean'] = train_features_real[f]-train_features_real[f].mean()\n    real_data[f+'distance_of_mean'] = real_data[f]-real_data[f].mean()\n    #train_features_fake[f+'distance_of_mean'] = train_features_fake[f]-train_features_fake[f].mean()\n    #fake_data[f+'distance_of_mean'] = fake_data[f]-fake_data[f].mean()\n    train_features_real[f+'distance_of_mean'] = train_features_real[f+'distance_of_mean']*train_features_real[f+'duplicate']\n    real_data[f+'distance_of_mean'] = real_data[f+'distance_of_mean']*real_data[f+'duplicate']\n    #train_features_fake[f+'distance_of_mean'] = train_features_fake[f+'distance_of_mean']*train_features_fake[f+'duplicate']\n    #fake_data[f+'distance_of_mean'] = fake_data[f+'distance_of_mean']*fake_data[f+'duplicate']","fd37cfac":"train_features_fake.shape,train_features_real.shape,fake_data.shape,real_data.shape,train_target.shape","f792f4a6":"del train_features\ndel test_features\ngc.collect()","17acf0d4":"n_splits = 10# Number of K-fold Splits","96ce991a":"param = {\n    'bagging_freq': 5,\n    'bagging_fraction': 0.33,\n    'boost_from_average':'false',\n    'boost': 'gbdt',\n    'feature_fraction': 0.05,\n    'learning_rate': 0.0085,\n    'max_depth': -1,\n    'metric':'auc',\n    'min_data_in_leaf': 80,\n    'min_sum_hessian_in_leaf': 10.0,\n    'num_leaves': 13,\n    'num_threads': 12,\n    'tree_learner': 'serial',\n    'objective': 'binary',\n    'verbosity': 1\n}","e122de2e":"oof = np.zeros(len(train_features_real))\npredictions = np.zeros(len(test_df))\n#feature_importance_df = pd.DataFrame()\n#features = [c for c in train_features.columns if c not in ['ID_code', 'target']]\n\nfor i, (train_idx, valid_idx) in enumerate(splits):  \n    print(f'Fold {i + 1}')\n    x_train = np.array(train_features_fake)\n    y_train = np.array(train_target)\n    trn_data = lgb.Dataset(x_train[train_idx.astype(int)], label=y_train[train_idx.astype(int)])\n    val_data = lgb.Dataset(x_train[valid_idx.astype(int)], label=y_train[valid_idx.astype(int)])\n    \n    num_round = 100000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 1000)\n    oof[valid_idx] = clf.predict(x_train[valid_idx], num_iteration=clf.best_iteration)\n    \n    #fold_importance_df = pd.DataFrame()\n    #fold_importance_df[\"feature\"] = features\n    #fold_importance_df[\"importance\"] = clf.feature_importance()\n    #fold_importance_df[\"fold\"] = i + 1\n    #feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    predictions[fake_data.index] += clf.predict(fake_data, num_iteration=clf.best_iteration) \/ n_splits\n    #predictions += clf.predict(test_features, num_iteration=clf.best_iteration) \/ n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(train_target, oof)))","3e5ab0f4":"oof = np.zeros(len(train_features_real))\nfeature_importance_df = pd.DataFrame()\nfeatures = [c for c in train_features_real.columns if c not in ['ID_code', 'target']]\n\nfor i, (train_idx, valid_idx) in enumerate(splits):  \n    print(f'Fold {i + 1}')\n    x_train = np.array(train_features_real)\n    y_train = np.array(train_target)\n    trn_data = lgb.Dataset(x_train[train_idx.astype(int)], label=y_train[train_idx.astype(int)])\n    val_data = lgb.Dataset(x_train[valid_idx.astype(int)], label=y_train[valid_idx.astype(int)])\n    \n    num_round = 100000\n    clf = lgb.train(param, trn_data, num_round, valid_sets = [trn_data, val_data], verbose_eval=1000, early_stopping_rounds = 1000)\n    oof[valid_idx] = clf.predict(x_train[valid_idx], num_iteration=clf.best_iteration)\n    \n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance()\n    fold_importance_df[\"fold\"] = i + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    \n    predictions[real_data.index] += clf.predict(real_data, num_iteration=clf.best_iteration) \/ n_splits\n    #predictions += clf.predict(test_features, num_iteration=clf.best_iteration) \/ n_splits\n\nprint(\"CV score: {:<8.5f}\".format(roc_auc_score(train_target, oof)))","0138852f":"cols = (feature_importance_df[[\"feature\", \"importance\"]]\n        .groupby(\"feature\")\n        .mean()\n        .sort_values(by=\"importance\", ascending=False)[:1000].index)\nbest_features = feature_importance_df.loc[feature_importance_df.feature.isin(cols)]\n\nplt.figure(figsize=(14,26))\nsns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\nplt.title('LightGBM Features (averaged over folds)')\nplt.tight_layout()\nplt.savefig('lgbm_importances.png')","b51f1603":"#esemble_lgbm_cat = 0.5*oof_cb+0.5*oof\nprint('LightBGM auc = {:<8.5f}'.format(roc_auc_score(train_target, oof)))\n#print('catboost auc = {:<8.5f}'.format(roc_auc_score(train_target, oof_cb)))\n#print('LightBGM+catboost auc = {:<8.5f}'.format(roc_auc_score(train_target, esemble_lgbm_cat)))","c321fea0":"id_code_test = test_df['ID_code']\nid_code_train = train_df['ID_code']","77d530f4":"my_submission_lbgm = pd.DataFrame({\"ID_code\" : id_code_test, \"target\" : predictions})\nmy_submission_train = pd.DataFrame({\"ID_code\" : id_code_train, \"target\" : oof})\n#my_submission_esemble_lgbm_cat = pd.DataFrame({\"ID_code\" : id_code_test, \"target\" : esemble_pred_lgbm_cat})","59f0f0cb":"my_submission_lbgm.to_csv('submission_lbgm.csv', index = False, header = True)\nmy_submission_train.to_csv('submission_lbgm_train.csv', index = False, header = True)\n#my_submission_esemble_lgbm_cat.to_csv('my_submission_esemble_lgbm_cat.csv', index = False, header = True)","53ad548a":"## Create submit file","19953110":"## Ensemble two model (NN+ LGBM)\n* NN model accuracy is too low, ensemble looks don't work.","cfa76450":"## Load Data","21abae89":"## Pytorch to implement simple feed-forward NN model (0.89+)\n\n* As below discussion, NN model can get lB 0.89+\n* https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/discussion\/82499#latest-483679\n* Add Cycling learning rate , K-fold cross validation (0.85 to 0.86)\n* Add flatten layer as below discussion (0.86 to 0.897)\n* https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/discussion\/82863\n\n## LightGBM (LB 0.899)\n\n* Fine tune parameters (0.898 to 0.899)\n* Reference this kernel : https:\/\/www.kaggle.com\/chocozzz\/santander-lightgbm-baseline-lb-0-899\n\n\n## Plan to do\n* Modify model structure on NN model\n* Focal loss\n* Feature engineering\n* Tune parameters oof LightGBM"}}