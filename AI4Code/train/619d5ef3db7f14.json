{"cell_type":{"decf1ae3":"code","dd5acb13":"code","25e66f6c":"code","4ceb627a":"code","8e151122":"code","9900e4ab":"code","f6283870":"code","6649287b":"markdown","d409290c":"markdown","d8092723":"markdown","2c5cd3cc":"markdown","897986b8":"markdown","14683d8e":"markdown","66b8218e":"markdown","512e0871":"markdown","a6d73004":"markdown"},"source":{"decf1ae3":"import os\nos.environ['KERAS_BACKEND'] = 'tensorflow'\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import f1_score, make_scorer, precision_recall_curve, precision_score, recall_score\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedShuffleSplit\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\nimport warnings\nfrom sklearn.exceptions import DataConversionWarning\nfrom sklearn.model_selection import StratifiedKFold\nfrom datetime import datetime\nfrom sklearn.decomposition import PCA, TruncatedSVD\nimport random\nimport math\nfrom scipy import stats\nimport xgboost as xgb\nfrom imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\nimport time\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import LabelEncoder\nimport keras as K\nfrom keras.layers import Dropout, BatchNormalization, Activation\nfrom sklearn.utils import class_weight\nimport keras.backend as K1\nfrom sklearn.model_selection import StratifiedKFold\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nimport gc\nfrom keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.metrics import confusion_matrix\nwarnings.filterwarnings(action='ignore', category=DataConversionWarning)\nimport tensorflow as tf","dd5acb13":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nSEED = 2145\n\n# Feature engineering influenced by this notebook:\n# https:\/\/www.kaggle.com\/gunesevitan\/titanic-advanced-feature-engineering-tutorial\n\ndef concat_df(train_data, test_data):\n    # Returns a concatenated df of training and test set\n    return \n\ndef divide_df(all_data):\n    # Returns divided dfs of training and test set\n    return all_data.loc[:890], all_data.loc[891:].drop(['Survived'], axis=1)\n\ndf_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_test = pd.read_csv('..\/input\/titanic\/test.csv')\ndfx = pd.concat([df_train, df_test], sort=True).reset_index(drop=True)\n\nage_by_pclass_sex = dfx.groupby(['Sex', 'Pclass']).median()['Age']\n\n# Filling the missing values in Age with the medians of Sex and Pclass groups\ndfx['Age'] = dfx.groupby(['Sex', 'Pclass'])['Age'].apply(lambda x: x.fillna(x.median()))\n\n# Filling the missing values in Embarked with S\ndfx['Embarked'] = dfx['Embarked'].fillna('S')\n\nmedian_fare = dfx.groupby(['Pclass', 'Parch', 'SibSp']).Fare.median()[3][0][0]\ndfx['Fare'] = dfx['Fare'].fillna(median_fare)\n\ndfx['Title'] = dfx['Name'].str.split(', ', expand=True)[1].str.split('.', expand=True)[0]\ndfx['Is_Married'] = 0\ndfx['Is_Married'].loc[dfx['Title'] == 'Mrs'] = 1\n\ndfx = dfx.drop(['Name','Ticket','PassengerId','Cabin','Title'], axis=1)","25e66f6c":"X = dfx.loc[:890]\nX_test = dfx.loc[891:]\n\nlabels = X['Survived']\nX = X.drop(['Survived'], axis=1)\n\n# Restrict the train data set to Numeric variables only\ndt = pd.DataFrame(X.dtypes)\ndt = dt.reset_index()\ndt.columns = ['Column', 'Dtype']\n\n### Handle Numeric Data\nnum_vars = np.array(dt[dt['Dtype']!='object']['Column'])\nnum_vars = num_vars[num_vars!='Parch']\nnum_vars = num_vars[num_vars!='Pclass']\nnum_vars = num_vars[num_vars!='SibSp']\nnum_vars = num_vars[num_vars!='Is_Married']\ntrainx_num = X.loc[:,num_vars]\ntestx_num = X_test.loc[:,num_vars]\n\n### Handle Categorical Data\ncat_vars = np.array(dt[dt['Dtype']=='object']['Column'])\n# add day and hour here\ncat_vars = np.append(cat_vars,'Parch')\ncat_vars = np.append(cat_vars,'Pclass')\ncat_vars = np.append(cat_vars,'SibSp')\ncat_vars = np.append(cat_vars,'Is_Married')\n\ntrainx_cat = X.loc[:,cat_vars]\ntestx_cat = X_test.loc[:,cat_vars]\n\nfor varz in cat_vars:\n    series = pd.value_counts(trainx_cat[varz])\n    unique_elements = pd.concat([trainx_cat[varz],testx_cat[varz]]).unique().tolist()\n    trainx_cat[varz] = trainx_cat[varz].astype('category').cat.set_categories(unique_elements)\n    testx_cat[varz] = testx_cat[varz].astype('category').cat.set_categories(unique_elements)\n\n# get final category dataset\ntrainx_cat = pd.get_dummies(trainx_cat, drop_first = True)\ntestx_cat = pd.get_dummies(testx_cat, drop_first = True)\n\n# combine num and cat dataframes\ndf = pd.concat([trainx_num, trainx_cat], axis=1)\nX_test = pd.concat([testx_num, testx_cat], axis=1)","4ceb627a":"X_train0, X_test, y_train0, y_test = train_test_split(df, labels, test_size=0.25, stratify=labels, random_state=33897)\n\nX_train0, X_val, y_train0, y_val = train_test_split(X_train0, y_train0, test_size=0.10, stratify=y_train0, random_state=33897)\n\nsc = StandardScaler()\nsc.fit(X_train0)\n\nX_train0 = sc.transform(X_train0)\nX_val = sc.transform(X_val)\nX_test = sc.transform(X_test)","8e151122":"def f1_metric(y_true, y_pred):\n    true_positives = K1.sum(K1.round(K1.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K1.sum(K1.round(K1.clip(y_true, 0, 1)))\n    predicted_positives = K1.sum(K1.round(K1.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K1.epsilon())\n    recall = true_positives \/ (possible_positives + K1.epsilon())\n    f1_val = 2*(precision*recall)\/(precision+recall+K1.epsilon())\n    return f1_val\n\nclass build_keras_model(object):\n    \n    def __init__(self, layers, activation, opt, init, input_dim, weight, patience, \n                 is_batchnorm, use_weighted_loss, print_model_summary, verbose):\n        \n        self.model = K.models.Sequential()\n        self.layers = layers\n        self.activation = activation\n        self.opt = opt\n        self.init = init\n        self.input_dim = input_dim\n        self.weight = weight\n        self.patience = patience\n        self.is_batchnorm = is_batchnorm\n        self.prmodsum = print_model_summary\n        self.use_weighted_loss = use_weighted_loss\n        self.verbose = verbose\n\n    def create_model(self):\n        \n        now = datetime.now()\n\n        for i, nodes in enumerate(self.layers):\n            if i==0:\n                self.model.add(K.layers.Dense(nodes,input_dim=self.input_dim,kernel_initializer=self.init))\n                self.model.add(Activation(self.activation))\n                if self.is_batchnorm == 1:\n                    self.model.add(BatchNormalization())\n            else:\n                self.model.add(K.layers.Dense(nodes,kernel_initializer=self.init))\n                self.model.add(Activation(self.activation))\n                if self.is_batchnorm == 1:\n                    self.model.add(BatchNormalization())\n\n        self.model.add(K.layers.Dense(1))\n        self.model.add(Activation('sigmoid')) # Note: no activation beyond this point\n        \n        if self.prmodsum == 1:\n            print(self.model.summary())\n        \n        def weighted_loss(y_true, y_pred):\n            weights = (y_true * self.weight) + 1.\n            cross_entop = K1.binary_crossentropy(y_true, y_pred)\n            weighted_loss = K1.mean(cross_entop * weights)\n            return weighted_loss\n        \n        if self.use_weighted_loss == 1:\n            loss_func = weighted_loss\n        else:\n            loss_func = 'binary_crossentropy'\n\n        self.model.compile(optimizer=self.opt, loss=loss_func,metrics=['accuracy'])\n        return\n    \n    def fit_model(self, X, y, X_validation, y_validation, batch_size, epochs, random_state):\n\n        pt = self.patience\n        vb = self.verbose\n        \n        earlystopping = EarlyStopping(monitor='val_loss', mode='min', verbose=vb, patience=pt, restore_best_weights = True)\n        callbacks_list = [earlystopping]\n\n        np.random.seed(random_state)\n        self.model.fit(X, y, validation_data = (X_validation,y_validation), \n                                                batch_size=batch_size,\n                                                epochs=epochs, \n                                                verbose=vb, \n                                                callbacks=callbacks_list)\n        \n        return\n    \n    def predict_from_model(self,test_df):\n\n        return self.model.predict(test_df)\n    \n    def __del__(self): \n        del self.model\n        gc.collect()\n        if self.prmodsum == 1:\n            print(\"Keras Destructor called\") ","9900e4ab":"layer_array = []\nlayer_array2 = []\n\nfor k in [48,64]:\n    layer_array.append(tuple([k]*3))\n\nlenq = len(layer_array)\n\nfor j in range(lenq):\n    for i in range(8):\n        tempo = list(layer_array[j])\n        tempo[1] = tempo[1]*4*(i+1)\n        tempo[2] = tempo[2]*(i+1)\n        layer_array2.append(tempo)\n\nlayer_array2[0] = tuple(layer_array2[0])\nlayer_array2\n\nprint(\"To be Iterated upon: \",layer_array2)\nprint(\"----------------------------------------------\")\nprint(\" \")","f6283870":"X_train = X_train0\ny_train = y_train0\n\nXinput_dimension = X_train.shape[1]\n\n# Define Keras parameters\nXwt = 2\nXpt = 10\nXep = 50\nXbnorm = 1\nXverbose = 0\nXpms = 0\nXactivation = 'relu'\nXbsz = 2\nX_use_weighted_loss = 0\n\nfor Xlayer in layer_array2:\n    \n    opt_name = 'Adagrad'\n    init_name = 'glorot_uniform'\n    Xopt = K.optimizers.Adagrad(learning_rate=0.01)\n    Xinit = K.initializers.glorot_uniform(seed=1)\n\n    np.random.seed(2018)\n    tf.random.set_seed(2018)\n    K1.set_session\n\n    km1 = build_keras_model(layers= Xlayer, \n                           activation = Xactivation, \n                           opt = Xopt, \n                           init = Xinit, \n                           input_dim = Xinput_dimension,\n                           weight = Xwt,\n                           patience = Xpt,\n                           is_batchnorm = Xbnorm,\n                           print_model_summary = Xpms,\n                           use_weighted_loss = X_use_weighted_loss,\n                           verbose = Xverbose)\n\n    km1.create_model()\n\n    km1.fit_model(\n                 X = X_train, \n                 y = y_train, \n                 X_validation = X_val, \n                 y_validation = y_val, \n                 batch_size = Xbsz, \n                 epochs = Xep,\n                 random_state = 3397)\n\n    preds = km1.predict_from_model(test_df = X_test)\n\n    del km1\n    K1.clear_session()\n    gc.collect()\n\n    best_f1 = 0\n    best_predval = []\n    best_thresh = 0.5\n\n    for thresh in np.arange(0.001,1,0.001):\n        thresh = round(thresh,3)\n        predval = (preds > thresh).astype(int)\n        f1s = f1_score(y_test,predval)\n        if f1s > best_f1:\n            best_f1 = f1s\n            best_thresh = thresh\n            best_predval = predval\n\n    print(\"*********************\")\n    print(\"For Layer Config: \",Xlayer)\n    print(\"*********************\")\n    print(\"\")\n    print(\"Best Threshold = \",best_thresh)\n\n    print(\"\")\n    if(np.sum(best_predval)==0):\n        print(\"All zeros predicted, so no confusion matrix\")\n    else:\n        print(confusion_matrix(y_test,best_predval))\n        print(\"\")\n        print(\"Precision = \",round(precision_score(y_test,best_predval),4))\n        print(\"Recall = \",round(recall_score(y_test,best_predval),4))\n\n    print(\"\")\n    print(\"Test F1_SCORE = \",round(best_f1,4))\n    print(\"\")","6649287b":"### 5. Define Keras Class and its methods","d409290c":"### 6. Create an Array of layers for iteration\nYou may choose to create this array any which way you want.","d8092723":"![Neural Networks](https:\/\/images.unsplash.com\/photo-1480843669328-3f7e37d196ae)","2c5cd3cc":"### 3. Get Dataframe in right shape for Keras","897986b8":"### 7. Define Keras Parameters and Iterate through the layers\n- Here I keep looking at F1 score, you may choose to look at accuracy or any other metrics\n- Not logging the parameters and scores, but you can choose to log them in a dataframe and write them to a disk","14683d8e":"### 2. Do some basic feature engineering","66b8218e":"### **1. Load all the required libraries**","512e0871":"## ** Motivation **\nIn most of the Keras tutorials, the instructor often tells us that the hidden layers, number of nodes, etc. are our own choice. Instructors often encourage us to try multiple types of layers. But upon my research, I could not find a clear algorithm or code which helps us choose a network structure by iterating on it. In this notebook, I explore how we can iterate over layers as a hyperparameter.\n\n### Assumptions\/Clarifications\n- I did very basic feature engineering as I wanted to demonstrate iteration over layers. With more powerful feature engineering, results can be improved\n- I chose to minimize cross-entropy loss, feel free to use other losses\n- I used F1 score to print and monitor performance, you are free to choose other metrics\n- I wrote custom code to create an array of layers, feel free to create your array or experiment with different types of layers\n- This is a Feedforward Neural net (FFNN) example, do try for CNN\/ANN by modifying the code","a6d73004":"### 4. Do Train, Val and Test splits"}}