{"cell_type":{"66308cda":"code","b6046c07":"code","6ec40207":"code","c0948da8":"code","0b1fedde":"code","627758b4":"code","347e4760":"code","1f2942bf":"code","54beaa27":"code","34a9701b":"code","fd39a4d8":"code","256dc96b":"code","b1225abe":"code","c83f33d1":"code","5783abce":"code","56b27f49":"code","9103c43f":"code","e0ebacac":"code","35f22a3d":"code","4ba15d5d":"code","233dc689":"code","5a8524df":"code","60da635f":"code","5193882d":"code","21147420":"code","47e47b3e":"code","0665353a":"code","544353cf":"code","85667f9c":"code","39067946":"code","2696b565":"code","a14acf9c":"code","de84bda5":"code","ad510808":"code","3d96f439":"code","9d74fa4b":"code","89485b9f":"code","ce8e9a04":"code","72e5f800":"code","f86c1b45":"code","cc254cb4":"code","e07a2cee":"code","e3dc6f79":"markdown","50f80cbd":"markdown","ced9640b":"markdown","61912507":"markdown","3deeb912":"markdown","b0dc816c":"markdown","16995d2a":"markdown","0f4840e2":"markdown","be2d0d0d":"markdown","7afce415":"markdown","a3a7766d":"markdown","52793078":"markdown","783bce48":"markdown","a68ce072":"markdown","c5dd7098":"markdown","026c6dd4":"markdown","40da0b7f":"markdown","413b53fe":"markdown","f7c5bf2f":"markdown","7654c19e":"markdown","70699741":"markdown","07fe1f45":"markdown","c0a94c22":"markdown","c28822ee":"markdown","edff4ca0":"markdown","394b9e51":"markdown","1045176a":"markdown"},"source":{"66308cda":"# \u5bfc\u5165\u6240\u9700\u6a21\u5757\n\nimport numpy as np\nimport pandas as pd\nimport os\nimport cv2\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport random\nfrom sklearn.utils import shuffle\nfrom tqdm import tqdm_notebook\n\n\n# \u8bfb\u53d6\u6570\u636e\u96c6\ndata = pd.read_csv('\/kaggle\/input\/train_labels.csv')\ntrain_path = '\/kaggle\/input\/train\/'\ntest_path = '\/kaggle\/input\/test\/'\n\n# \u67e5\u770b\u6b63\u8d1f\u6837\u672c\u5206\u5e03\u72b6\u6001\n# \u6b63\u8d1f\u6837\u672c\u6570\u91cf\u5e76\u975e\u5b8c\u5168\u5e73\u8861\uff0c\u603b\u5171\u5305\u542b\u7ea6130K\u8d1f\u6837\u672c\u548c90K\u6b63\u6837\u672c\uff0c\u6837\u672c\u6bd4\u4f8b\u63a5\u8fd160:40\ndata['label'].value_counts()","b6046c07":"# \u5b9a\u4e49\u56fe\u7247\u8bfb\u53d6\u51fd\u6570\uff08\u4f7f\u7528opencv\u6a21\u5757\uff09\n# opencv\u8bfb\u53d6\u6570\u636e\u9ed8\u8ba4\u4f7f\u7528bgr\u987a\u5e8f\uff0c\u8fd9\u91cc\u8f6c\u6362\u56dergb\u683c\u5f0f\uff0c\u7528\u4e8e\u53ef\u89c6\u5316\ndef readImage(path):\n    # OpenCV reads the image in bgr format by default\n    bgr_img = cv2.imread(path)\n    # We flip it to rgb for visualization purposes\n    b,g,r = cv2.split(bgr_img)\n    rgb_img = cv2.merge([r,g,b])\n    return rgb_img","6ec40207":"# \u968f\u673a\u91c7\u6837\uff0c\u6253\u4e71\u6570\u636e\u6392\u5e8f\nshuffled_data = shuffle(data)\n\nfig, ax = plt.subplots(2,5, figsize=(20,8))\nfig.suptitle('Histopathologic scans of lymph node sections',fontsize=20)\n\n# \u8d1f\u6837\u672c\u968f\u673a\u9009\u53d65\u5f20\u6837\u672c\nfor i, idx in enumerate(shuffled_data[shuffled_data['label'] == 0]['id'][:5]):\n    path = os.path.join(train_path, idx)\n    ax[0,i].imshow(readImage(path + '.tif'))\n    # \u753b\u51fa32*32\u65b9\u5f62\u9009\u6846\n    box = patches.Rectangle((32,32),32,32,linewidth=4,edgecolor='b',facecolor='none', linestyle=':', capstyle='round')\n    ax[0,i].add_patch(box)\nax[0,0].set_ylabel('Negative samples', size='large')\n\n# \u6b63\u6837\u672c\u968f\u673a\u9009\u53d65\u5f20\u6837\u672c\nfor i, idx in enumerate(shuffled_data[shuffled_data['label'] == 1]['id'][:5]):\n    path = os.path.join(train_path, idx)\n    ax[1,i].imshow(readImage(path + '.tif'))\n    # \u753b\u51fa32*32\u65b9\u5f62\u9009\u6846\n    box = patches.Rectangle((32,32),32,32,linewidth=4,edgecolor='r',facecolor='none', linestyle=':', capstyle='round')\n    ax[1,i].add_patch(box)\nax[1,0].set_ylabel('Tumor tissue samples', size='large')","c0948da8":"# \u4f7f\u7528opencv\u6a21\u5757\u8fdb\u884c\u56fe\u50cf\u5904\u7406\uff0c\u6548\u7387\u548c\u5904\u7406\u901f\u5ea6\u8f83PIL\u548cscikit-image\u66f4\u5feb\n\nimport random\nORIGINAL_SIZE = 96      # \u56fe\u7247\u539f\u59cb\u5c3a\u5bf8 96*96\n\n# \u56fe\u50cf\u53d8\u6362\u53c2\u6570\nCROP_SIZE = 90          # \u56fe\u7247\u88c1\u526a\u540e\u5c3a\u5bf8\nRANDOM_ROTATION = 3    # 0-180\u5ea6\u65cb\u8f6c\nRANDOM_SHIFT = 2        # x,y\u8f74\u5750\u6807\u968f\u673a\u4e92\u6362\nRANDOM_BRIGHTNESS = 7  # range (0-100)\nRANDOM_CONTRAST = 5    # range (0-100)\nRANDOM_90_DEG_TURN = 1  # \u5de6\u53f3\u65cb\u8f6c90\u5ea6\n\n\n# \u56fe\u50cf\u968f\u673a\u53d8\u6362\/\u589e\u5f3a\u51fd\u6570\ndef readCroppedImage(path, augmentations = True):\n    # opencv\u8bfb\u53d6\u56fe\u7247\uff0c\u9ed8\u8ba4b,g,r\n    bgr_img = cv2.imread(path)\n    # b,g,r \u8f6c\u6362\u6210 r,g,b \n    b,g,r = cv2.split(bgr_img)\n    rgb_img = cv2.merge([r,g,b])\n    \n    if(not augmentations):\n        return rgb_img \/ 255\n    \n    # \u968f\u673a\u65cb\u8f6c\n    rotation = random.randint(-RANDOM_ROTATION,RANDOM_ROTATION)\n    if(RANDOM_90_DEG_TURN == 1):\n        rotation += random.randint(-1,1) * 90\n    M = cv2.getRotationMatrix2D((48,48),rotation,1) \n    rgb_img = cv2.warpAffine(rgb_img,M,(96,96))\n    \n    # x,y\u8f74\u5750\u6807\u968f\u673a\u4e92\u6362\n    x = random.randint(-RANDOM_SHIFT, RANDOM_SHIFT)\n    y = random.randint(-RANDOM_SHIFT, RANDOM_SHIFT)\n    \n    # \u4e2d\u5fc3\u88c1\u526a\uff0c\u50cf\u7d20\u70b9\u6b63\u5219\u5316\uff08\u90fd\u9664\u4ee5255\uff09\n    start_crop = (ORIGINAL_SIZE - CROP_SIZE) \/\/ 2\n    end_crop = start_crop + CROP_SIZE\n    rgb_img = rgb_img[(start_crop + x):(end_crop + x), (start_crop + y):(end_crop + y)] \/ 255\n    \n    # \u968f\u673a\u53cd\u8f6c\uff08\u6c34\u5e73\/\u4e0a\u4e0b\uff09\n    flip_hor = bool(random.getrandbits(1))\n    flip_ver = bool(random.getrandbits(1))\n    if(flip_hor):\n        rgb_img = rgb_img[:, ::-1]\n    if(flip_ver):\n        rgb_img = rgb_img[::-1, :]\n        \n    # \u968f\u673a\u4eae\u5ea6\u8c03\u6574\n    br = random.randint(-RANDOM_BRIGHTNESS, RANDOM_BRIGHTNESS) \/ 100.\n    rgb_img = rgb_img + br\n    \n    # \u968f\u673a\u5bf9\u6bd4\u5ea6\u8c03\u6574\n    cr = 1.0 + random.randint(-RANDOM_CONTRAST, RANDOM_CONTRAST) \/ 100.\n    rgb_img = rgb_img * cr\n    \n    # \u88c1\u526a\u503c\n    rgb_img = np.clip(rgb_img, 0, 1.0)\n    \n    return rgb_img","0b1fedde":"# \u6d4b\u8bd5\u56fe\u50cf\u53d8\u6362\u6548\u679c\n\nfig, ax = plt.subplots(2,5, figsize=(20,8))\nfig.suptitle('Cropped histopathologic scans of lymph node sections',fontsize=20)\n# \u8d1f\u6837\u672c\nfor i, idx in enumerate(shuffled_data[shuffled_data['label'] == 0]['id'][:5]):\n    path = os.path.join(train_path, idx)\n    ax[0,i].imshow(readCroppedImage(path + '.tif'))\nax[0,0].set_ylabel('Negative samples', size='large')\n\n# \u6b63\u6837\u672c\nfor i, idx in enumerate(shuffled_data[shuffled_data['label'] == 1]['id'][:5]):\n    path = os.path.join(train_path, idx)\n    ax[1,i].imshow(readCroppedImage(path + '.tif'))\nax[1,0].set_ylabel('Tumor tissue samples', size='large')","627758b4":"fig, ax = plt.subplots(1,5, figsize=(20,4))\nfig.suptitle('Random augmentations to the same image',fontsize=20)\n# Negatives\nfor i, idx in enumerate(shuffled_data[shuffled_data['label'] == 0]['id'][:1]):\n    for j in range(5):\n        path = os.path.join(train_path, idx)\n        ax[j].imshow(readCroppedImage(path + '.tif'))","347e4760":"# \u8fd9\u91cc\u8ba1\u7b97 RGB \u4e09\u901a\u9053\u6570\u503c\u7684\u7edf\u8ba1\u91cf\uff0c\u6240\u4ee5\u53ef\u4ee5\u6821\u9a8c\u662f\u5426\u5b58\u5728\u6697\u6216\u8005\u8fc7\u4eae\u7684\u6837\u672c\u56fe\u7247\ndark_th = 10 \/ 255      # \u6697\u5ea6\u9608\u503c\nbright_th = 245 \/ 255   # \u4eae\u5ea6\u9608\u503c\ntoo_dark_idx = []\ntoo_bright_idx = []\n\nx_tot = np.zeros(3)\nx2_tot = np.zeros(3)\ncounted_ones = 0\nfor i, idx in tqdm_notebook(enumerate(shuffled_data['id']), 'computing statistics...(220025 it total)'):\n    path = os.path.join(train_path, idx)\n    imagearray = readCroppedImage(path + '.tif', augmentations = False).reshape(-1,3)\n    # \u67e5\u770b\u662f\u5426\u4e3a\u5168\u6697\u6837\u672c\n    if(imagearray.max() < dark_th):\n        too_dark_idx.append(idx)\n        continue # \u5168\u6697\u7684\u6837\u672c\u4e0d\u88ab\u8ba1\u5165\u7edf\u8ba1\u603b\u91cf\n    # \u67e5\u770b\u662f\u5426\u4e3a\u5168\u4eae\u7684\u6837\u672c\n    if(imagearray.min() > bright_th):\n        too_bright_idx.append(idx)\n        continue # \u5168\u4eae\u7684\u6837\u672c\u4e0d\u88ab\u8ba1\u5165\u7edf\u8ba1\u603b\u91cf\n    x_tot += imagearray.mean(axis=0)\n    x2_tot += (imagearray**2).mean(axis=0)\n    counted_ones += 1\n    \n# \u8ba1\u7b97\u4e09\u901a\u9053\u6570\u503c\u5f97\u5e73\u5747\u503c\uff0c\u6807\u51c6\u5dee\nchannel_avr = x_tot\/counted_ones\nchannel_std = np.sqrt(x2_tot\/counted_ones - channel_avr**2)\nchannel_avr,channel_std","1f2942bf":"print('There was {0} extremely dark image'.format(len(too_dark_idx)))\nprint('and {0} extremely bright images'.format(len(too_bright_idx)))\nprint('Dark one:')\nprint(too_dark_idx)\nprint('Bright ones:')\nprint(too_bright_idx)","54beaa27":"# \u5c1d\u8bd5\u4f7f\u7528readCroppedImage\u8bfb\u53d6\u8fc7\u6697\u6216\u8005\u8fc7\u4eae\u7684\u56fe\u50cf\n\nfig, ax = plt.subplots(2,6, figsize=(25,9))\nfig.suptitle('Almost completely black or white images',fontsize=20)\n# Too dark\ni = 0\nfor idx in np.asarray(too_dark_idx)[:min(6, len(too_dark_idx))]:\n    lbl = shuffled_data[shuffled_data['id'] == idx]['label'].values[0]\n    path = os.path.join(train_path, idx)\n    ax[0,i].imshow(readCroppedImage(path + '.tif', augmentations = False))\n    ax[0,i].set_title(idx + '\\n label=' + str(lbl), fontsize = 8)\n    i += 1\nax[0,0].set_ylabel('Extremely dark images', size='large')\nfor j in range(min(6, len(too_dark_idx)), 6):\n    ax[0,j].axis('off') # hide axes if there are less than 6\n# Too bright\ni = 0\nfor idx in np.asarray(too_bright_idx)[:min(6, len(too_bright_idx))]:\n    lbl = shuffled_data[shuffled_data['id'] == idx]['label'].values[0]\n    path = os.path.join(train_path, idx)\n    ax[1,i].imshow(readCroppedImage(path + '.tif', augmentations = False))\n    ax[1,i].set_title(idx + '\\n label=' + str(lbl), fontsize = 8)\n    i += 1\nax[1,0].set_ylabel('Extremely bright images', size='large')\nfor j in range(min(6, len(too_bright_idx)), 6):\n    ax[1,j].axis('off') # hide axes if there are less than 6","34a9701b":"from sklearn.model_selection import train_test_split\n\n# \u4e3a\u8bfb\u53d6\u7684\u6570\u636e\u96c6\u6dfb\u52a0\u7d22\u5f15\ntrain_df = data.set_index('id')\n\n# \u9664\u53bb\u5f02\u5e38\u503c\u6837\u672c\n#print('Before removing outliers we had {0} training samples.'.format(train_df.shape[0]))\n#train_df = train_df.drop(labels=too_dark_idx, axis=0)\n#train_df = train_df.drop(labels=too_bright_idx, axis=0)\n#print('After removing outliers we have {0} training samples.'.format(train_df.shape[0]))\n\ntrain_names = train_df.index.values\ntrain_labels = np.asarray(train_df['label'].values)\n\n# \u5206\u5272\u8bad\u7ec3\u96c6\u548c\u6d4b\u8bd5\u96c6\ntr_n, tr_idx, val_n, val_idx = train_test_split(train_names, range(len(train_names)), test_size=0.1, stratify=train_labels, random_state=123)","fd39a4d8":"# \u4f7f\u7528 fastai 1.0 \u5efa\u7acb\u6a21\u578b\nfrom fastai import *\nfrom fastai.vision import *\nfrom torchvision.models import * \n\narch = densenet169                 \n# arch = resnet34\n\n# \u8bbe\u7f6eBATCH_SIZE\u9632\u6b62OOM\nBATCH_SIZE = 128                \n# \u56fe\u50cf\u8f93\u5165\u7684\u5c3a\u5bf8\nsz = CROP_SIZE                     \n# \u6a21\u578b\u8def\u5f84\nMODEL_PATH = str(arch).split()[1]","256dc96b":"# \u6784\u9020\u6570\u636e\u96c6 dataframe \u7ed3\u6784\ntrain_dict = {'name': train_path + train_names, 'label': train_labels}\ndf = pd.DataFrame(data=train_dict)\n# \u6784\u9020\u6d4b\u8bd5\u96c6 dataframe \u7ed3\u6784\ntest_names = []\nfor f in os.listdir(test_path):\n    test_names.append(test_path + f)\ndf_test = pd.DataFrame(np.asarray(test_names), columns=['name'])","b1225abe":"# Subclass ImageList \u91cd\u5b9a\u4e49 readCroppedImage \u51fd\u6570\nclass MyImageItemList(ImageList):\n    def open(self, fn:PathOrStr)->Image:\n        img = readCroppedImage(fn.replace('\/.\/','').replace('\/\/','\/'))\n        # pil2tensor \u5c06 ndarray \u8f6c\u6362\u6210 tensor\n        return vision.Image(px=pil2tensor(img, np.float32))\n    ","c83f33d1":"# \u4f7f\u7528 fastai \u63d0\u4f9b\u7684 API \u521b\u5efa ImageDataBunch\nimgDataBunch = (MyImageItemList.from_df(path='\/', df=df, suffix='.tif')\n        # \u5b9a\u4e49\u6570\u636e\u8def\u5f84\n        .split_by_idx(val_idx)\n        # \u4f7f\u7528\u7d22\u5f15\u5206\u5272\u6570\u636e\u96c6\n        .label_from_df(cols='label')\n        # \u6807\u7b7e\u5217\n        .add_test(MyImageItemList.from_df(path='\/', df=df_test))\n        # \u6d4b\u8bd5\u96c6\u8def\u5f84\n        .transform(tfms=[[],[]], size=sz)\n        # \u8fd9\u91cc\u7559\u7a7a\uff0c\u56e0\u4e3a\u5df2\u7ecf\u6709\u4e86\u81ea\u5b9a\u4e49\u7684\u56fe\u50cf\u53d8\u6362\u51fd\u6570\n        .databunch(bs=BATCH_SIZE)\n        # \u5b9a\u4e49\u6279\u6b21\u5927\u5c0f\n        .normalize([tensor([0.702447, 0.546243, 0.696453]), tensor([0.238893, 0.282094, 0.216251])])\n        # \u4f7f\u7528\u8bad\u7ec3\u96c6\u7684\u7edf\u8ba1\u6570\u636e\u7ed3\u679c\u6765\u8fdb\u884c\u6b63\u5219\u5316\uff0c\u8fd9\u91cc hard code \u7684\u503c\u662f\u4e4b\u524d\u4e00\u6b65\u4e2d\u8ba1\u7b97\u51fa\u7684\u7ed3\u679c\n       )","5783abce":"# \u68c0\u67e5 imgDataBunch \u63a5\u53e3\u7684\u5de5\u4f5c\u72b6\u6001\u662f\u5426\u6b63\u5e38\nimgDataBunch.show_batch(rows=2, figsize=(4,4))","56b27f49":"# \u521b\u5efa\u5377\u79ef\u5b66\u4e60\u5668\u5bf9\u8c61\n# ps \u53c2\u6570\u4e3adropout\ndef getLearner():\n    return create_cnn(imgDataBunch, arch, pretrained=True, path='.', metrics=accuracy, ps=0.5, callback_fns=ShowGraph)\n\nlearner = getLearner()","9103c43f":"# \u4f7f\u7528 lr_find \u65b9\u6cd5\u914d\u7f6e\u4e0d\u540c\u7684\u6743\u91cd\u8870\u51cf\u7387\uff0c\u5e76\u8bb0\u5f55\u6240\u6709\u7684losses,\u753b\u51fa\u56fe\u50cf\u8fdb\u884c\u8fdb\u4e00\u6b65\u5206\u6790\n# \u589e\u52a0\u8fed\u4ee3\u6b21\u6570\uff08\u9ed8\u8ba4\u4ec5100\uff09\n\nlrs = []\nlosses = []\nwds = []\niter_count = 600\n\n# WEIGHT DECAY = 1e-6\nlearner.lr_find(wd=1e-6, num_it=iter_count)\nlrs.append(learner.recorder.lrs)\nlosses.append(learner.recorder.losses)\nwds.append('1e-6')\nlearner = getLearner() #reset learner - this gets more consistent starting conditions\n\n# WEIGHT DECAY = 1e-4\nlearner.lr_find(wd=1e-4, num_it=iter_count)\nlrs.append(learner.recorder.lrs)\nlosses.append(learner.recorder.losses)\nwds.append('1e-4')\nlearner = getLearner() #reset learner - this gets more consistent starting conditions\n\n# WEIGHT DECAY = 1e-2\nlearner.lr_find(wd=1e-2, num_it=iter_count)\nlrs.append(learner.recorder.lrs)\nlosses.append(learner.recorder.losses)\nwds.append('1e-2')\nlearner = getLearner() #reset learner","e0ebacac":"# Plot weight decays\n_, ax = plt.subplots(1,1)\nmin_y = 0.5\nmax_y = 0.55\nfor i in range(len(losses)):\n    ax.plot(lrs[i], losses[i])\n    min_y = min(np.asarray(losses[i]).min(), min_y)\nax.set_ylabel(\"Loss\")\nax.set_xlabel(\"Learning Rate\")\nax.set_xscale('log')\n\n# \u4f7f\u7528\u5176\u4ed6 baseline \u6a21\u578b\u8fdb\u884c finetuning \u7684\u65f6\u5019\u53ef\u80fd\u9700\u8981\u4fee\u6539\u5750\u6807\u8303\u56f4\nax.set_xlim((1e-3,3e-1))\nax.set_ylim((min_y - 0.02,max_y))\nax.legend(wds)\nax.xaxis.set_major_formatter(plt.FormatStrFormatter('%.0e'))","35f22a3d":"# \u8fd9\u91cc\u9009\u62e9\u6700\u5927\u7684\u6743\u91cd\u8870\u51cf\u4ee5\u9632\u6b62\u6a21\u578b\u8fc7\u62df\u5408","4ba15d5d":"max_lr = 2e-2\nwd = 1e-4\n# 1cycle policy\nlearner.fit_one_cycle(cyc_len=8, max_lr=max_lr, wd=wd)","233dc689":"# learning rate of the one cycle\nlearner.recorder.plot_lr()","5a8524df":"# \u753b\u51fa\u7b2c\u4e00\u8f6e\u8fed\u4ee3\u7684\u635f\u5931\u503c\u60c5\u51b5\nlearner.recorder.plot_losses()","60da635f":"# \u4f7f\u7528\u6a21\u578b\u6765\u9884\u6d4b\u9a8c\u8bc1\u6570\u636e\u96c6\ninterp = ClassificationInterpretation.from_learner(learner)\ninterp.plot_confusion_matrix(title='Confusion matrix')","5193882d":"# \u5c06\u5df2\u7ecf\u8bad\u7ec3\u597d\u7684 baseline \u6a21\u578b\u4fdd\u5b58\u4e0b\u6765\uff0c\u65b9\u4fbf\u540e\u7eed\u7684 finetunnig\nlearner.save(MODEL_PATH + '_stage1')","21147420":"# \u52a0\u8f7d\u6a21\u578b\nlearner.load(MODEL_PATH + '_stage1')\n\n# unfreeze \u5b66\u4e60\u5668\nlearner.unfreeze()\nlearner.lr_find(wd=wd)\n\n# \u753b\u51fa\u5b66\u4e60\u7387\u66f2\u7ebf\u56fe\nlearner.recorder.plot()","47e47b3e":"# \u7f29\u5c0f\u5b66\u4e60\u7387\u8303\u56f4 4e-5 ~ 4e-4\nlearner.fit_one_cycle(cyc_len=12, max_lr=slice(4e-5,4e-4))","0665353a":"learner.recorder.plot_losses()","544353cf":"# \u518d\u6b21\u67e5\u770b Confusion matrix\ninterp = ClassificationInterpretation.from_learner(learner)\ninterp.plot_confusion_matrix(title='Confusion matrix')","85667f9c":"# \u4fdd\u5b58\u7b2c\u4e8c\u6b21\u7684\u6a21\u578b\u7ed3\u679c\nlearner.save(MODEL_PATH + '_stage2')","39067946":"preds,y, loss = learner.get_preds(with_loss=True)\n# \u83b7\u53d6\u51c6\u786e\u7387\nacc = accuracy(preds, y)\nprint('The accuracy is {0} %.'.format(acc))","2696b565":"# \u8fd4\u56de\u4ea4\u53c9\u9a8c\u8bc1\u96c6\u4e2d\u635f\u5931\u503c\u6700\u5927\u7684\u6837\u672c\nfrom random import randint\n\ndef plot_overview(interp:ClassificationInterpretation, classes=['Negative','Tumor']):\n    \n    tl_val,tl_idx = interp.top_losses()\n    #classes = interp.data.classes\n    fig, ax = plt.subplots(3,4, figsize=(16,12))\n    fig.suptitle('Predicted \/ Actual \/ Loss \/ Probability',fontsize=20)\n    \n    # \u968f\u673a\u5904\u7406\n    for i in range(4):\n        random_index = randint(0,len(tl_idx))\n        idx = tl_idx[random_index]\n        im,cl = interp.data.dl(DatasetType.Valid).dataset[idx]\n        im = image2np(im.data)\n        cl = int(cl)\n        ax[0,i].imshow(im)\n        ax[0,i].set_xticks([])\n        ax[0,i].set_yticks([])\n        ax[0,i].set_title(f'{classes[interp.pred_class[idx]]} \/ {classes[cl]} \/ {interp.losses[idx]:.2f} \/ {interp.probs[idx][cl]:.2f}')\n    ax[0,0].set_ylabel('Random samples', fontsize=16, rotation=0, labelpad=80)\n    \n    # \u635f\u5931\u503c\u6700\u9ad8\u7684\u9519\u8bef\u5206\u7c7b\u6837\u672c\n    for i in range(4):\n        idx = tl_idx[i]\n        im,cl = interp.data.dl(DatasetType.Valid).dataset[idx]\n        cl = int(cl)\n        im = image2np(im.data)\n        ax[1,i].imshow(im)\n        ax[1,i].set_xticks([])\n        ax[1,i].set_yticks([])\n        ax[1,i].set_title(f'{classes[interp.pred_class[idx]]} \/ {classes[cl]} \/ {interp.losses[idx]:.2f} \/ {interp.probs[idx][cl]:.2f}')\n    ax[1,0].set_ylabel('Most incorrect\\nsamples', fontsize=16, rotation=0, labelpad=80)\n    \n    # \u635f\u5931\u503c\u6700\u4f4e\u7684\u6b63\u786e\u5206\u7c7b\u7684\u6837\u672c\n    for i in range(4):\n        idx = tl_idx[len(tl_idx) - i - 1]\n        im,cl = interp.data.dl(DatasetType.Valid).dataset[idx]\n        cl = int(cl)\n        im = image2np(im.data)\n        ax[2,i].imshow(im)\n        ax[2,i].set_xticks([])\n        ax[2,i].set_yticks([])\n        ax[2,i].set_title(f'{classes[interp.pred_class[idx]]} \/ {classes[cl]} \/ {interp.losses[idx]:.2f} \/ {interp.probs[idx][cl]:.2f}')\n    ax[2,0].set_ylabel('Most correct\\nsamples', fontsize=16, rotation=0, labelpad=80)","a14acf9c":"#interp = ClassificationInterpretation.from_learner(learner)\n# \u53ef\u89c6\u5316\u7ed3\u679c\nplot_overview(interp, ['Negative','Tumor'])","de84bda5":"from fastai.callbacks.hooks import *\n\n# hook into forward pass\ndef hooked_backward(m, oneBatch, cat):\n    # we hook into the convolutional part = m[0] of the model\n    with hook_output(m[0]) as hook_a: \n        with hook_output(m[0], grad=True) as hook_g:\n            preds = m(oneBatch)\n            preds[0,int(cat)].backward()\n    return hook_a,hook_g","ad510808":"# We can create a utility function for getting a validation image with an activation map\ndef getHeatmap(val_index):\n    \"\"\"Returns the validation set image and the activation map\"\"\"\n    # this gets the model\n    m = learner.model.eval()\n    tensorImg,cl = imgDataBunch.valid_ds[val_index]\n    # create a batch from the one image\n    oneBatch,_ = imgDataBunch.one_item(tensorImg)\n    oneBatch_im = vision.Image(imgDataBunch.denorm(oneBatch)[0])\n    # convert batch tensor image to grayscale image with opencv\n    cvIm = cv2.cvtColor(image2np(oneBatch_im.data), cv2.COLOR_RGB2GRAY)\n    # attach hooks\n    hook_a,hook_g = hooked_backward(m, oneBatch, cl)\n    # get convolutional activations and average from channels\n    acts = hook_a.stored[0].cpu()\n    #avg_acts = acts.mean(0)\n\n    # Grad-CAM\n    grad = hook_g.stored[0][0].cpu()\n    grad_chan = grad.mean(1).mean(1)\n    grad.shape,grad_chan.shape\n    mult = (acts*grad_chan[...,None,None]).mean(0)\n    return mult, cvIm","3d96f439":"# Then, modify our plotting func a bit\ndef plot_heatmap_overview(interp:ClassificationInterpretation, classes=['Negative','Tumor']):\n    # top losses will return all validation losses and indexes sorted by the largest first\n    tl_val,tl_idx = interp.top_losses()\n    #classes = interp.data.classes\n    fig, ax = plt.subplots(3,4, figsize=(16,12))\n    fig.suptitle('Grad-CAM\\nPredicted \/ Actual \/ Loss \/ Probability',fontsize=20)\n    # Random\n    for i in range(4):\n        random_index = randint(0,len(tl_idx))\n        idx = tl_idx[random_index]\n        act, im = getHeatmap(idx)\n        H,W = im.shape\n        _,cl = interp.data.dl(DatasetType.Valid).dataset[idx]\n        cl = int(cl)\n        ax[0,i].imshow(im)\n        ax[0,i].imshow(im, cmap=plt.cm.gray)\n        ax[0,i].imshow(act, alpha=0.5, extent=(0,H,W,0),\n              interpolation='bilinear', cmap='inferno')\n        ax[0,i].set_xticks([])\n        ax[0,i].set_yticks([])\n        ax[0,i].set_title(f'{classes[interp.pred_class[idx]]} \/ {classes[cl]} \/ {interp.losses[idx]:.2f} \/ {interp.probs[idx][cl]:.2f}')\n    ax[0,0].set_ylabel('Random samples', fontsize=16, rotation=0, labelpad=80)\n    # Most incorrect or top losses\n    for i in range(4):\n        idx = tl_idx[i]\n        act, im = getHeatmap(idx)\n        H,W = im.shape\n        _,cl = interp.data.dl(DatasetType.Valid).dataset[idx]\n        cl = int(cl)\n        ax[1,i].imshow(im)\n        ax[1,i].imshow(im, cmap=plt.cm.gray)\n        ax[1,i].imshow(act, alpha=0.5, extent=(0,H,W,0),\n              interpolation='bilinear', cmap='inferno')\n        ax[1,i].set_xticks([])\n        ax[1,i].set_yticks([])\n        ax[1,i].set_title(f'{classes[interp.pred_class[idx]]} \/ {classes[cl]} \/ {interp.losses[idx]:.2f} \/ {interp.probs[idx][cl]:.2f}')\n    ax[1,0].set_ylabel('Most incorrect\\nsamples', fontsize=16, rotation=0, labelpad=80)\n    # Most correct or least losses\n    for i in range(4):\n        idx = tl_idx[len(tl_idx) - i - 1]\n        act, im = getHeatmap(idx)\n        H,W = im.shape\n        _,cl = interp.data.dl(DatasetType.Valid).dataset[idx]\n        cl = int(cl)\n        ax[2,i].imshow(im)\n        ax[2,i].imshow(im, cmap=plt.cm.gray)\n        ax[2,i].imshow(act, alpha=0.5, extent=(0,H,W,0),\n              interpolation='bilinear', cmap='inferno')\n        ax[2,i].set_xticks([])\n        ax[2,i].set_yticks([])\n        ax[2,i].set_title(f'{classes[interp.pred_class[idx]]} \/ {classes[cl]} \/ {interp.losses[idx]:.2f} \/ {interp.probs[idx][cl]:.2f}')\n    ax[2,0].set_ylabel('Most correct\\nsamples', fontsize=16, rotation=0, labelpad=80)","9d74fa4b":"plot_heatmap_overview(interp, ['Negative','Tumor'])","89485b9f":"from sklearn.metrics import roc_curve, auc\nprobs = np.exp(preds[:,1])\n# \u8ba1\u7b97 ROC \u66f2\u7ebf\nfpr, tpr, thresholds = roc_curve(y, probs, pos_label=1)\n\n# \u8ba1\u7b97 ROC \u9762\u79ef\nroc_auc = auc(fpr, tpr)\nprint('ROC area is {0}'.format(roc_auc))","ce8e9a04":"plt.figure()\nplt.plot(fpr, tpr, color='darkorange', label='ROC curve (area = %0.2f)' % roc_auc)\nplt.plot([0, 1], [0, 1], color='navy', linestyle='--')\nplt.xlim([-0.01, 1.0])\nplt.ylim([0.0, 1.01])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")","72e5f800":"# \u5bfc\u5165\u6700\u4f18\u6a21\u578b\nlearner.load(MODEL_PATH + '_stage2')\n\n# TTA \nn_aug = 12\npreds_n_avg = np.zeros((len(learner.data.test_ds.items),2))\nfor n in tqdm_notebook(range(n_aug), 'Running TTA...'):\n    preds,y = learner.get_preds(ds_type=DatasetType.Test, with_loss=False)\n    preds_n_avg = np.sum([preds_n_avg, preds.numpy()], axis=0)\npreds_n_avg = preds_n_avg \/ n_aug","f86c1b45":"# \u8f93\u51fa\u53ef\u80fd\u6027\nprint('Negative and Tumor Probabilities: ' + str(preds_n_avg[0]))\ntumor_preds = preds_n_avg[:, 1]\nprint('Tumor probability: ' + str(tumor_preds[0]))\n# argmax \u8f93\u51fa\u5206\u7c7b\u7ed3\u679c\nclass_preds = np.argmax(preds_n_avg, axis=1)\nclasses = ['Negative','Tumor']\nprint('Class prediction: ' + classes[class_preds[0]])","cc254cb4":"SAMPLE_SUB = '\/kaggle\/input\/sample_submission.csv'\nsample_df = pd.read_csv(SAMPLE_SUB)\nsample_list = list(sample_df.id)\n\n# \u6839\u636e\u7d22\u5f15\u751f\u6210\u952e\u503c\u5bf9\npred_list = [p for p in tumor_preds]\npred_dic = dict((key, value) for (key, value) in zip(learner.data.test_ds.items, pred_list))\npred_list_cor = [pred_dic['\/\/\/kaggle\/input\/test\/' + id + '.tif'] for id in sample_list]\ndf_sub = pd.DataFrame({'id':sample_list,'label':pred_list_cor})\n\n# \u5bfc\u51fa csv\ndf_sub.to_csv('{0}_submission.csv'.format(MODEL_PATH), header=True, index=False)","e07a2cee":"skf=StratifiedKFold(n_splits=k_folds) \n\nfor i,(trdex,valdex) in enumerate(skf.split(X=dataset.id.values,y=dataset.label.values)): \n    train_sub_dataframe=dataset.iloc[trdex] print(trdex.shape)","e3dc6f79":"# Finetuning the baseline model","50f80cbd":"![image.png](attachment:image.png)","ced9640b":"**\u67e5\u770b\u968f\u673a\u56fe\u50cf\u53d8\u6362\u7684\u6548\u679c**","61912507":"### 1cycle policy","3deeb912":"\u7ed3\u679c\u663e\u793a\u5f53\u67d0\u4e00\u6837\u672c\u88ab\u68c0\u6d4b\u4e3a tumor \u65f6\uff0c\u6a21\u578b\u7684\u6240\u771f\u6b63\u8ba4\u4e3a\u7684 tumor \u662f\u5728\u5177\u4f53\u7684\u54ea\u90e8\u5206\uff0c\u901a\u8fc7\u70ed\u5ea6\u56fe\u8fdb\u884c\u53ef\u89c6\u5316","b0dc816c":"\u4ece\u56fe\u50cf\u4e2d\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0c\u5b66\u4e60\u7387\u4ece\u8f83\u4f4e\u7684\u4f4d\u7f6e\u5f00\u59cb\uff0c\u4e0d\u65ad\u589e\u5927\uff0c\u7136\u540e\u53c8\u8d8b\u4e8e\u51cf\u5c11\u3002\u8f83\u9ad8\u7684\u5b66\u4e60\u7387\u4f1a\u4f34\u968f\u6b63\u5219\u5316\u4f7f\u5f97\u6a21\u578b\u7684loss\u4e0d\u4f1a\u4ea7\u751f\u592a\u5927\u7684\u6ce2\u52a8\uff0c\u5e76\u4e14\u8f83\u4e3a\u5e73\u7f13\u7684\u8d8b\u5411\u4e8e\u6781\u503c\u3002\n\n","16995d2a":"![image.png](attachment:image.png)","0f4840e2":"----------------\n\n# Submit predictions\n### TTA\n\n\u5bf9\u6d4b\u8bd5\u96c6\u4f7f\u7528\u6d4b\u8bd5\u65f6\u589e\u5f3a","be2d0d0d":"# Baseline model (Fastai)","7afce415":"\u4e0b\u9762\u901a\u8fc7\u53ef\u89c6\u5316\u6765\u8fdb\u4e00\u6b65\u67e5\u770b\u88ab\u9519\u8bef\u6807\u8bb0\u7684\u6837\u672c","a3a7766d":"**Sections of this kernel**\n- Data visualization\n- Baseline model (Fastai v1)\n- Validation and analysis\n    - Metrics\n    - Prediction and activation visualizations\n    - ROC & AUC\n- Submit\n\n","52793078":"![image.png](attachment:image.png)","783bce48":"\u8fd9\u91cc\u9009\u62e9 fastai \u5e93\u6765\u914d\u7f6e baseline \u6a21\u578b\uff0c\u5e76\u5728 baseline \u6a21\u578b\u7684\u57fa\u7840\u4e0a\u8fdb\u884c\u540e\u7eed\u7684 finetuning\u3002","a68ce072":"![image.png](attachment:image.png)","c5dd7098":"![image.png](attachment:image.png)","026c6dd4":"-----------------------------------------\n# Data visualization","40da0b7f":"### Gradient-weighted Class Activation Mapping (Grad-CAM)\n[Grad-CAM: Visual Explanations from Deep Networks via Gradient-based Localization](https:\/\/arxiv.org\/abs\/1610.02391)\n\n\u53ef\u89c6\u5316\u6a21\u578b\u5173\u6ce8\u5ea6","413b53fe":"\u635f\u5931\u51fd\u6570\u66f2\u7ebf\u4f1a\u51fa\u73b0\u5c0f\u5e45\u4e0a\u5347\u518d\u7ee7\u7eed\u4e0b\u964d\u7684\u60c5\u51b5\uff0c\u8bf4\u660e\u6a21\u578b\u8de8\u8fc7\u4e86\u5c40\u90e8\u6700\u5c0f\u503c\u3002\n","f7c5bf2f":"-------------------------\n# Validation and analysis\n","7654c19e":"### Submit the model for evaluation","70699741":"### ROC curve and AUC","07fe1f45":"\u4ece\u635f\u5931\u51fd\u6570\u5f53\u4e2d\u53ef\u4ee5\u770b\u5230\uff0c\u5728\u5c3e\u90e8\u8bad\u7ec3\u96c6\u548c\u9a8c\u8bc1\u96c6\u66f2\u7ebf\u5df2\u7ecf\u51fa\u73b0\u8f83\u5927\u5dee\u8ddd\uff0c\u8fd9\u610f\u5473\u7740\u6a21\u578b\u5728\u8f83\u5c0f\u7684\u5b66\u4e60\u7387\u4e0b\u5f00\u59cb\u51fa\u73b0\u8fc7\u62df\u5408\u95ee\u9898\u3002","c0a94c22":"![image.png](attachment:image.png)","c28822ee":"### \u6570\u636e\u51c6\u5907\n\u5c06\u8bad\u7ec3\u96c6\u5206\u5272\u4e3a90%\u7684\u8bad\u7ec3\u96c6\u548c10%\u7684\u4ea4\u53c9\u9a8c\u8bc1\u96c6\uff0c\u5e76\u4e14\u901a\u8fc7\u6dfb\u52a0\u7d22\u5f15\u7684\u65b9\u5f0f\u4fdd\u6301\u6b63\u8d1f\u6837\u672c\u7684\u6bd4\u4f8b\uff0860\uff1a40\uff09\uff0c\u4ece\u800c\u5206\u5272\u6570\u636e\u96c6\u65f6\u9020\u6210\u6b63\u8d1f\u6837\u672c\u4e0d\u5747\u8861\u7684\u95ee\u9898\u3002\n","edff4ca0":"\u6839\u636e\u7ed3\u679c\u663e\u793a\uff0c\u6240\u6709\u7684\u5168\u6697\u6216\u5168\u4eae\u6837\u672c\u90fd\u88ab\u6807\u8bb0\u4e3a\u8d1f\u6837\u672c\uff0c\u4ece\u903b\u8f91\u4e0a\u6765\u8bf4\u5e76\u6ca1\u6709\u9519\u8bef\uff0c\u6240\u4ee5\u5373\u4f7f\u4e0d\u628a\u8fd9\u4e9b\u7279\u6b8a\u6837\u672c\u5f53\u4f5c\u5f02\u5e38\u503c\u8fdb\u884c\u5254\u9664\uff0c\u4e5f\u4e0d\u4f1a\u5bf9\u6a21\u578b\u7684\u9884\u6d4b\u7ed3\u679c\u6709\u6240\u5f71\u54cd\u3002\n\n-----------------------------------------","394b9e51":"### \u8ba1\u7b97 RGB \u4e09\u901a\u9053\u6570\u503c\u7684\u7edf\u8ba1\u91cf\n\n\u8fc7\u6697\u6216\u8005\u8fc7\u4eae\u7684\u56fe\u7247\u53ef\u80fd\u662f\u7531\u4e8e\u8fc7\u5ea6\u7684\u66dd\u5149\u6216\u8005\u968f\u673a\u88c1\u526a\u6240\u9020\u6210\u7684\u7a7a\u767d\u533a\u57df\uff0c\u6211\u4eec\u53ef\u4ee5\u628a\u8fd9\u4e9b\u6837\u672c\u5f53\u4f5c\u5f02\u5e38\u503c\uff0c\u6216\u8005\u76f4\u63a5\u770b\u4f5c\u662f\u8d1f\u6837\u672c\u3002","1045176a":"### Training\n"}}