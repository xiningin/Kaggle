{"cell_type":{"67f5805b":"code","9b75476c":"code","3d66e01d":"code","c609d3c1":"code","af9c5010":"code","73506323":"code","e0231425":"code","24772c41":"code","dacd40f4":"code","1b4e1af8":"code","6293305a":"code","2fe694b0":"code","ffa98173":"code","96c12df4":"code","2f2ba820":"code","d9cb5e1e":"code","7ac9a10c":"code","36c24c85":"code","223047c0":"code","f24e1ef1":"code","bfe77200":"code","ffe01904":"code","149f1b06":"code","e3491e44":"code","1ab230fa":"code","d77e8f07":"code","e82f13a5":"code","5fe272a9":"markdown","799e20d5":"markdown","1472f962":"markdown","298242ea":"markdown","0ec70d31":"markdown","c95ad707":"markdown","41410a43":"markdown","73aadf98":"markdown","4f073a8d":"markdown","a5a1c3d0":"markdown","df9efe7e":"markdown","74752c84":"markdown","3f70d0a0":"markdown","a5dce69d":"markdown","04752c5e":"markdown","635429a5":"markdown","248cf04c":"markdown","ccfb77f4":"markdown","c3faed3b":"markdown"},"source":{"67f5805b":"from IPython.core.display import display, HTML\ndisplay(HTML(\"<style>.container { width:100% !important; }<\/style>\"))\n\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sea\n\n#Sklearn functions\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import accuracy_score","9b75476c":"#Training data\ndf_train = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/train_data.csv\")\ndf_train = df_train.set_index('Id')\n\n#testing Data\ndf_test = pd.read_csv(\"\/kaggle\/input\/adult-pmr3508\/test_data.csv\")\n#df_test.drop(df_test.index[0], axis = 0, inplace = True)\ndf_train.head()","3d66e01d":"# First, I've to check the labels of categorical variables and values of quantitative variables in order to assess the integrity of my dataset\nfeatures = df_train.columns\n\nfor feature in features:\n    print('Feature '+feature + ' values:',set(df_train[feature]))\n","c609d3c1":"# I have to deal with \"?\" strings in categorical variables and possible missing data\ndf_train = df_train.replace('?', np.nan)\ndf_test = df_test.replace('?', np.nan)\nrows_nan = []\nfor index,row in df_train.isnull().iterrows():\n    if (row.values).any():\n        rows_nan.append(index)\nlen(rows_nan)  #number of rows  that have any missing data              ","af9c5010":"print('Rows that have any missing value: ',str((len(rows_nan)\/df_train.shape[0])*100) + '%')","73506323":"#Owing to the small % of rows having missing data, I'll get rid of them\ndf_train = df_train.dropna(axis = 0)\ndisplay(df_train)\n\n#df_test = df_test.dropna(axis = 0)","e0231425":"#I will delete education column once I already have 'education.num'\ndf_train.drop(labels = 'education', axis = 1, inplace = True)\ndf_test.drop(labels = 'education', axis = 1, inplace = True)","24772c41":"df_train_corr = df_train.copy()\ndf_train_corr['income']  =df_train_corr['income'].apply(lambda x: 0 if x == '<=50K' else 1 )\nset(df_train_corr['income'])","dacd40f4":"sea.heatmap(round(df_train_corr.corr(),2), annot = True,cmap = 'viridis',vmin = -1, vmax = 1)\nplt.title(\"Correlation (Spearman) Matrix\", fontweight = 'bold', fontsize = 12)\n\n#Setting\nplt.gcf().set_size_inches(8,8)\nplt.gcf().set_dpi(100)\nplt.subplots_adjust(bottom = .25)\nplt.xticks(size = 12)\nplt.yticks(size = 12)","1b4e1af8":"df_train.drop(labels = 'fnlwgt', axis =1, inplace = True)\ndf_test.drop(labels = 'fnlwgt', axis =1, inplace = True)","6293305a":"df_train.head()","2fe694b0":"display(df_train.head())\nquant_features = ['age','education.num','capital.gain', 'capital.loss','hours.per.week']","ffa98173":"sea.set(rc = {'axes.facecolor':'black'} )\nfig, ((ax1,ax2),(ax3,ax4),(ax5,ax6)) = plt.subplots(3,2)\naxes = [ax1,ax2,ax3,ax4,ax5,ax6]\n\n\nfor feature,ax in zip(quant_features,axes):\n    \n    #Plotting\n    sea.boxplot(data = df_train, x = 'income', y =feature, palette = \"GnBu_r\", orient = \"v\", linewidth = 1, saturation= 0.56, ax= ax)\n    #sea.swarmplot(data = df_train, x = 'income', y = feature, palette = 'GnBu_r', orient = 'v', size = 7,alpha= 1, edgecolor= 'grey', ax = ax)\n    ax.set_xlabel('income', fontsize = 15)\n    ax.set_ylabel(feature, fontsize = 15)\n    ax.set_title(\"Box-plot: Income X {}\".format(feature) ,size = 15, fontweight = 'bold')\n       \n    \n#Setting\nplt.gcf().set_size_inches(17,12)\nplt.gcf().set_dpi(130)\nplt.subplots_adjust(bottom = -0.8)\nplt.xticks(fontsize = 12)\nplt.yticks(fontsize = 12)","96c12df4":"df_train[['capital.gain', 'capital.loss', 'hours.per.week']] = RobustScaler().fit_transform(df_train[['capital.gain', 'capital.loss', 'hours.per.week']])\ndf_test[['capital.gain', 'capital.loss', 'hours.per.week']] = RobustScaler().fit_transform(df_test[['capital.gain', 'capital.loss', 'hours.per.week']])","2f2ba820":"display(df_train.head())\nqual_features = ['workclass', 'marital.status', 'occupation', 'relationship', 'race', 'sex', 'native.country']","d9cb5e1e":"sea.set(rc = {'axes.facecolor':'black'} )\nfig, ((ax1,ax2),(ax3,ax4),(ax5,ax6), (ax7,ax8)) = plt.subplots(4,2)\naxes = [ax1,ax2,ax3,ax4,ax5,ax6, ax7, ax8]\n\n\nfor feature,ax in zip(qual_features,axes):\n    \n    #Plotting\n    df_train[feature].value_counts().plot.barh( color = 'royalblue', ax = ax)\n    ax.set_xlabel('Number of People', fontsize = 15)\n    ax.set_ylabel(feature, fontsize = 15)\n    ax.set_title(\"Bar Chat: {}\".format(feature) ,size = 15, fontweight = 'bold')\n       \n    \n#Setting\nplt.gcf().set_size_inches(17,12)\nplt.gcf().set_dpi(130)\nplt.subplots_adjust(bottom = -0.8)\nplt.xticks(fontsize = 13)\nplt.yticks(fontsize = 13)","7ac9a10c":"df_train.groupby(by = 'native.country').agg({'native.country': lambda x: str(round(np.count_nonzero(x)\/len(df_train) * 100,2)) + ' %'})","36c24c85":"df_train['USA'] = df_train['native.country'].apply(lambda x: 1 if x == 'United-States' else 0 )\ndf_test['USA'] = df_test['native.country'].apply(lambda x: 1 if x == 'United-States' else 0 )","223047c0":"#Training dataset\nfor feature in qual_features:\n    df_train[feature] = LabelEncoder().fit_transform(df_train[feature])\n    \n#Testing dataset    \nfor feature in qual_features:\n    print(feature)\n    df_test[feature] = LabelEncoder().fit_transform(df_test[feature].astype(str))","f24e1ef1":"df_train.columns","bfe77200":"feature_select = ['age', 'workclass', 'education.num', 'marital.status', 'occupation',\n       'relationship', 'race', 'sex', 'capital.gain', 'capital.loss',\n       'hours.per.week', 'USA']\ntarget = 'income'\n\nx_train = df_train[feature_select]\ny_train = df_train[target]\nx_test = df_test[feature_select]\n#y_test = df_test[target]","ffe01904":"#I'm using cross validation method with 10 folders to select the K that presents more accuracy\nKN = np.arange(10, 45, 5)\nfor K in KN:\n    score = cross_val_score(KNeighborsClassifier(n_neighbors = K), x_train, y_train, cv = 10, scoring = 'accuracy').mean()\n    print(f'K-Neighbor: {K} | Accuracy: {score}')","149f1b06":"K = 10\nknn = KNeighborsClassifier(n_neighbors = K)\nknn.fit(x_train,y_train)\ny_predict= knn.predict(x_test) #classification in practice","e3491e44":"y_predict","1ab230fa":"#Classification accuracy\n#accuracy_score(y_test, y_predict)","d77e8f07":"df_sub = pd.DataFrame({'Id': df_test.index, 'income': y_predict}, index = df_test.index)\ndf_sub","e82f13a5":"df_sub.to_csv('submission_rodolfo.csv', index = False)","5fe272a9":"- We can realize there are many outliers from the boxplots that need to be handled\n- I'm using Robust Scaler method to deal with outliers","799e20d5":"## 1- Data Cleaning","1472f962":"### 2.3 - Analyzing qualitative features","298242ea":"### 2.1 - Correlation Matrix","0ec70d31":"### 3.2 Applyig the Classifier to the training dataset","c95ad707":"- Now I have to convert the labels to numerical values in order to apply the KNN classifier","41410a43":"# Using KNN to do Predictions in the Adult Dataset\n## PMR3508: Machine Learning and Pattern Recognition\n## Student name: Rodolfo Lima (PMR3508-2021-6)","73aadf98":"### 3.1- Feature Selection","4f073a8d":"### 3.3- Applyig the Classifier to the testing dataset","a5a1c3d0":"### 3.4 Saving Results","df9efe7e":"- We can notice that ony the United-States accounts for more than **91%** of the dataset records. We can conclude that the dataset is totally unballanced\n- Very well. I'll handle it placing 1 to people from the US and 0 to non people from the US","74752c84":"## 2- Exploratory Data Analysis (EDA) and Data Preparation","3f70d0a0":"- Our target is the income, it's a categorical variable with two label\n- I'm changing the target in order to have two quantiative values: 0 to <=50 and >50 to 1. Then, I'll able to measure the correlations among the variables","a5dce69d":"- Comparing the target with other features, we can see the pearson correlation between income and fnlwgtg is close to 0. So we can considerer take this feature out","04752c5e":"## 3- Using KNN (K-Nearest Nighbor) Cassifier to Predict People's Income","635429a5":"- I pick up the K that presents the best accuracy","248cf04c":"- The fact that catches my attention the most is the too way greater number of records from the US","ccfb77f4":"### 2.2 - Analyzing quantitative features","c3faed3b":"## 0- Libraries"}}