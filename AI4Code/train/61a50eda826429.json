{"cell_type":{"ed811145":"code","d864ea9e":"code","4c1bfddc":"code","4be04b9e":"code","93180488":"code","b7c2b6e7":"code","b07fbb2f":"code","19882889":"code","76d85552":"code","210af680":"code","db8d9219":"code","011bc1bd":"code","fe1c93ef":"code","d8784c37":"code","9478d375":"code","ff5c19f7":"code","6f8cf50a":"code","23ab89c5":"code","b4d8ec0b":"code","640fa1de":"code","20731ef3":"code","388cc7f0":"markdown","483499cb":"markdown","5abd8300":"markdown","de8324df":"markdown","91ccac6e":"markdown","b14e3e79":"markdown","7272dc83":"markdown","fd5301f3":"markdown"},"source":{"ed811145":"!pip install -q tensorflow-gpu==2.0.0-beta1","d864ea9e":"!pip install -q tensorflow-datasets","4c1bfddc":"%load_ext autoreload\n%autoreload 2","4be04b9e":"batch_size = 32\nshuffle_buffer = 1000","93180488":"import tensorflow as tf\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport logging\n\nlogging.getLogger('tensorflow').setLevel(logging.ERROR)\n\nfrom tensorflow.keras.callbacks import Callback\n\nclass CosineAnnealer:\n    \n    def __init__(self, start, end, steps):\n        self.start = start\n        self.end = end\n        self.steps = steps\n        self.n = 0\n        \n    def step(self):\n        self.n += 1\n        cos = np.cos(np.pi * (self.n \/ self.steps)) + 1\n        return self.end + (self.start - self.end) \/ 2. * cos\n\n\nclass OneCycleScheduler(Callback):\n    \"\"\"`Callback` that schedules the learning rate on a 1cycle policy as per Leslie Smith's paper(https:\/\/arxiv.org\/pdf\/1803.09820.pdf).\n    If the model supports a momentum parameter, it will also be adapted by the schedule.\n    The implementation adopts additional improvements as per the fastai library: https:\/\/docs.fast.ai\/callbacks.one_cycle.html, where\n    only two phases are used and the adaptation is done using cosine annealing.\n    In phase 1 the LR increases from `lr_max \/ div_factor` to `lr_max` and momentum decreases from `mom_max` to `mom_min`.\n    In the second phase the LR decreases from `lr_max` to `lr_max \/ (div_factor * 1e4)` and momemtum from `mom_max` to `mom_min`.\n    By default the phases are not of equal length, with the phase 1 percentage controlled by the parameter `phase_1_pct`.\n    \"\"\"\n\n    def __init__(self, lr_max, steps, mom_min=0.85, mom_max=0.95, phase_1_pct=0.3, div_factor=25.):\n        super(OneCycleScheduler, self).__init__()\n        lr_min = lr_max \/ div_factor\n        final_lr = lr_max \/ (div_factor * 1e4)\n        phase_1_steps = steps * phase_1_pct\n        phase_2_steps = steps - phase_1_steps\n        \n        self.phase_1_steps = phase_1_steps\n        self.phase_2_steps = phase_2_steps\n        self.phase = 0\n        self.step = 0\n        \n        self.phases = [[CosineAnnealer(lr_min, lr_max, phase_1_steps), CosineAnnealer(mom_max, mom_min, phase_1_steps)], \n                 [CosineAnnealer(lr_max, final_lr, phase_2_steps), CosineAnnealer(mom_min, mom_max, phase_2_steps)]]\n        \n        self.lrs = []\n        self.moms = []\n\n    def on_train_begin(self, logs=None):\n        self.phase = 0\n        self.step = 0\n\n        self.set_lr(self.lr_schedule().start)\n        self.set_momentum(self.mom_schedule().start)\n        \n    def on_train_batch_begin(self, batch, logs=None):\n        self.lrs.append(self.get_lr())\n        self.moms.append(self.get_momentum())\n\n    def on_train_batch_end(self, batch, logs=None):\n        self.step += 1\n        if self.step >= self.phase_1_steps:\n            self.phase = 1\n            \n        self.set_lr(self.lr_schedule().step())\n        self.set_momentum(self.mom_schedule().step())\n        \n    def get_lr(self):\n        try:\n            return tf.keras.backend.get_value(self.model.optimizer.lr)\n        except AttributeError:\n            return None\n        \n    def get_momentum(self):\n        try:\n            return tf.keras.backend.get_value(self.model.optimizer.momentum)\n        except AttributeError:\n            return None\n        \n    def set_lr(self, lr):\n        try:\n            tf.keras.backend.set_value(self.model.optimizer.lr, lr)\n        except AttributeError:\n            pass # ignore\n        \n    def set_momentum(self, mom):\n        try:\n            tf.keras.backend.set_value(self.model.optimizer.momentum, mom)\n        except AttributeError:\n            pass # ignore\n\n    def lr_schedule(self):\n        return self.phases[self.phase][0]\n    \n    def mom_schedule(self):\n        return self.phases[self.phase][1]\n    \n    def plot(self):\n        ax = plt.subplot(1, 2, 1)\n        ax.plot(self.lrs)\n        ax.set_title('Learning Rate')\n        ax = plt.subplot(1, 2, 2)\n        ax.plot(self.moms)\n        ax.set_title('Momentum')\n        ","b7c2b6e7":"class LRFinder(Callback):\n    \"\"\"`Callback` that exponentially adjusts the learning rate after each training batch between `start_lr` and\n    `end_lr` for a maximum number of batches: `max_step`. The loss and learning rate are recorded at each step allowing\n    visually finding a good learning rate as per https:\/\/sgugger.github.io\/how-do-you-find-a-good-learning-rate.html via\n    the `plot` method.\n    \"\"\"\n\n    def __init__(self, start_lr: float = 1e-7, end_lr: float = 10, max_steps: int = 1000, smoothing=0.9):\n        super(LRFinder, self).__init__()\n        self.start_lr, self.end_lr = start_lr, end_lr\n        self.max_steps = max_steps\n        self.smoothing = smoothing\n        self.step, self.best_loss, self.avg_loss, self.lr = 0, 0, 0, 0\n        self.lrs, self.losses = [], []\n\n    def on_train_begin(self, logs=None):\n        self.step, self.best_loss, self.avg_loss, self.lr = 0, 0, 0, 0\n        self.lrs, self.losses = [], []\n\n    def on_train_batch_begin(self, batch, logs=None):\n        self.lr = self.exp_annealing(self.step)\n        tf.keras.backend.set_value(self.model.optimizer.lr, self.lr)\n\n    def on_train_batch_end(self, batch, logs=None):\n        logs = logs or {}\n        loss = logs.get('loss')\n        step = self.step\n        if loss:\n            self.avg_loss = self.smoothing * self.avg_loss + (1 - self.smoothing) * loss\n            smooth_loss = self.avg_loss \/ (1 - self.smoothing ** (self.step + 1))\n            self.losses.append(smooth_loss)\n            self.lrs.append(self.lr)\n\n            if step == 0 or loss < self.best_loss:\n                self.best_loss = loss\n\n            if smooth_loss > 4 * self.best_loss or tf.math.is_nan(smooth_loss):\n                self.model.stop_training = True\n\n        if step == self.max_steps:\n            self.model.stop_training = True\n\n        self.step += 1\n\n    def exp_annealing(self, step):\n        return self.start_lr * (self.end_lr \/ self.start_lr) ** (step * 1. \/ self.max_steps)\n\n    def plot(self):\n        fig, ax = plt.subplots(1, 1)\n        ax.set_ylabel('Loss')\n        ax.set_xlabel('Learning Rate (log scale)')\n        ax.set_xscale('log')\n        ax.xaxis.set_major_formatter(plt.FormatStrFormatter('%.0e'))\n        ax.plot(self.lrs, self.losses)","b07fbb2f":"fashion_mnist = tf.keras.datasets.fashion_mnist\n\n(x_train, y_train), (x_valid, y_valid) = fashion_mnist.load_data()\nx_train, x_valid = x_train \/ 255.0, x_valid \/ 255.0\n\nx_train = x_train[..., tf.newaxis]\nx_valid = x_valid[..., tf.newaxis]\n\ntrain_ds = tf.data.Dataset.from_tensor_slices((x_train, y_train)).shuffle(shuffle_buffer).batch(batch_size)\nvalid_ds = tf.data.Dataset.from_tensor_slices((x_valid, y_valid)).batch(batch_size)","19882889":"from tensorflow.keras.layers import Conv2D, MaxPool2D, AveragePooling2D, Flatten, Dense, Dropout\n\ndef build_model():\n    # LeNet-5 CNN\n    return tf.keras.models.Sequential([\n        Conv2D(6, 3, padding='same', activation='relu'),\n        AveragePooling2D(),\n        Conv2D(16, 3, padding='valid', activation='relu'),\n        AveragePooling2D(),\n        Flatten(),\n        Dense(120, activation='relu'),\n        Dense(84, activation='relu'),\n        Dense(10, activation='softmax')\n    ])","76d85552":"lr_finder = LRFinder()\n\nmodel = build_model()\noptimizer = tf.keras.optimizers.RMSprop()\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy')\n_ = model.fit(train_ds, epochs=10, callbacks=[lr_finder], verbose=True)\n\nlr_finder.plot()","210af680":"epochs = 3\nlr = 5e-3\nsteps = np.ceil(len(x_train) \/ batch_size) * epochs\nlr_schedule = OneCycleScheduler(lr, steps)\n\nmodel = build_model()\noptimizer = tf.keras.optimizers.RMSprop(lr=lr)\nmodel.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nhistory = model.fit(train_ds, validation_data=valid_ds, epochs=epochs, callbacks=[lr_schedule], verbose=True)","db8d9219":"lr_schedule.plot()","011bc1bd":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])","fe1c93ef":"import tensorflow_datasets as tfds\ntfds.disable_progress_bar()\n\ndataset_splits = (8, 2)\nsplits = tfds.Split.TRAIN.subsplit(weighted=dataset_splits)\n\n(raw_train, raw_validation), metadata = tfds.load('cats_vs_dogs', split=list(splits), with_info=True, as_supervised=True)","d8784c37":"img_size = 160\n\ndef resize_image(image, label):\n    image = tf.cast(image, tf.float32)\n    image = (image\/127.5) - 1\n    image = tf.image.resize(image, (img_size, img_size))\n    return image, label","9478d375":"train = raw_train.map(resize_image).batch(batch_size)\nvalidation = raw_validation.map(resize_image).batch(batch_size)","ff5c19f7":"train_batches = train.shuffle(shuffle_buffer)\nvalidation_batches = validation","6f8cf50a":"img_shape = (img_size, img_size, 3)","23ab89c5":"from tensorflow.keras.applications import MobileNetV2\nfrom tensorflow.keras.applications.resnet50 import ResNet50\nfrom tensorflow.keras.layers import GlobalAveragePooling2D, Dense\n\ndef build_pretrained_model():\n    base_model = MobileNetV2(input_shape=img_shape, weights='imagenet', include_top=False)\n    base_model.trainable=False\n\n    return tf.keras.Sequential([\n        base_model,\n        GlobalAveragePooling2D(),\n        Dense(1)\n    ])","b4d8ec0b":"lr_finder = LRFinder(max_steps=500)\nmodel = build_pretrained_model()\noptimizer = tf.keras.optimizers.Adam(amsgrad=True)\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy')\ntry:\n    model.fit(train_batches, epochs=20, callbacks=[lr_finder], verbose=True)\nexcept:\n    pass\nlr_finder.plot()","640fa1de":"num_train, num_val = (\n  metadata.splits['train'].num_examples*split\/10\n  for split in dataset_splits\n)\n","20731ef3":"epochs = 5\nlr = 5e-3\nsteps = np.ceil(num_train \/ batch_size) * epochs\nlr_schedule = OneCycleScheduler(lr, steps)\n\nmodel = build_pretrained_model()\noptimizer = tf.keras.optimizers.RMSprop(lr=lr)\nmodel.compile(optimizer=optimizer, loss='binary_crossentropy', metrics=['accuracy'])\nhistory = model.fit(train_batches, validation_data=validation_batches, epochs=epochs, callbacks=[lr_schedule], verbose=True)","388cc7f0":"### Fashion MNIST example","483499cb":"#### Training with RMSprop\nThe 1Cycle training policy allows us to choose a larger learning rate, leading to faster convergence, as illustrated below.","5abd8300":"Compared to the [example](https:\/\/www.tensorflow.org\/beta\/tutorials\/images\/transfer_learning#train_the_model) we can see we now achieve a higher validation accuracy using the 1cycle policy in just a few epochs of training.","de8324df":"## 1Cycle Learning Rate Schedule Callback\nA 1Cycle learning rate schedule implementation based on https:\/\/arxiv.org\/pdf\/1708.07120.pdf and https:\/\/docs.fast.ai\/callbacks.one_cycle.html. A brief write-up of the technique is available here: https:\/\/www.avanwyk.com\/tensorflow-2-super-convergence-with-the-1cycle-policy\/.","91ccac6e":"## Cats vs Dogs transfer learning example\n\nThe code below is adapted from the [Transfer Learning Using Pretrained ConvNets](https:\/\/www.tensorflow.org\/beta\/tutorials\/images\/transfer_learning) example.","b14e3e79":"#### Training with RMSprop","7272dc83":"### LRFinder Callback\nUsing the 1Cycle schedule requires us to find a maximum LR. This can be done using the [LRFinder Callback](https:\/\/www.avanwyk.com\/finding-a-learning-rate-in-tensorflow-2\/).","fd5301f3":"# Tensorflow 2 super-convergence with the 1Cycle LR Policy"}}