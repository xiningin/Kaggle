{"cell_type":{"1610c27c":"code","17f9885f":"code","1d6506d7":"code","babfedd8":"code","a67f3ea9":"code","a243ec30":"code","d7898a50":"code","7d37481f":"code","c8dbb76f":"code","587c6f1e":"code","bb58dc4c":"code","37ae05f8":"code","c818437a":"code","08569ac2":"code","266c3987":"code","d260c35e":"code","3fb1d5fa":"code","c486a3ea":"code","ea2932ab":"code","992b95b0":"code","961fc00a":"code","3ef4298f":"code","5f86e91e":"markdown","dc9f0f46":"markdown","1bd225bf":"markdown","60cc75a6":"markdown","82bc4522":"markdown","c0cd728d":"markdown","a1ad4013":"markdown","24ad41a7":"markdown","67429721":"markdown","66585881":"markdown"},"source":{"1610c27c":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import GroupKFold\nfrom sklearn.metrics import mean_squared_error\nimport gc\nfrom tqdm import tqdm_notebook as tqdm\nimport random\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import TensorDataset, DataLoader\n\nfrom kaggle.competitions import nflrush\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","17f9885f":"train_path = '\/kaggle\/input\/nfl-big-data-bowl-2020\/train.csv'\ntrain = pd.read_csv(train_path)\nprint(train.shape)","1d6506d7":"use_cols = [\n    'GameId', \n    'PlayId', \n    'Team',\n    'Yards',\n    'X',\n    'Y',\n    'PossessionTeam',\n    'HomeTeamAbbr',\n    'VisitorTeamAbbr',\n    'Position',\n]\ntrain = train[use_cols]\ntrain.head()","babfedd8":"\"\"\"\nFind an offense team.\nref: https:\/\/www.kaggle.com\/c\/nfl-big-data-bowl-2020\/discussion\/112314#latest-648026\n\"\"\"\ndef fe_is_offence(row):\n    if row[\"Team\"] == \"home\":\n        if row[\"HomeTeamAbbr\"] == row[\"PossessionTeam\"]:\n            return 1\n        else:\n            return 0\n    elif row[\"Team\"] == \"away\":\n        if row[\"VisitorTeamAbbr\"] == row[\"PossessionTeam\"]:\n            return 1\n        else:\n            return 0\n\ndef fe_is_offence_from_position(row, off_team):\n    if row[\"Team\"] == off_team:\n        return 1\n    else:\n        return 0\n        \n# def run_fe_is_offence(df):\n#     df['is_offence'] = df.apply(lambda row: fe_is_offence(row), axis=1)\n    \n#     if (df['is_offence'].values == 0).all():\n#         off_team = df[df['Position']=='QB']['Team'].values[0]\n#         df['is_offence'] = df.apply(lambda row: fe_is_offence_from_position(row, off_team), axis=1)\n\n\"\"\"\nbugfix\n\"\"\"\ndef run_fe_is_offence(df):\n    df['is_offence'] = df.apply(lambda row: fe_is_offence(row), axis=1)\n    \n    check_is_offence = df.groupby('PlayId')['is_offence'].nunique()\n    is_offence_not_found_idx = check_is_offence[check_is_offence!=2].index\n    not_found_df = df[df['PlayId'].isin(is_offence_not_found_idx)]\n    found_df = df[~df['PlayId'].isin(is_offence_not_found_idx)]\n#     print('is_offence found: {}'.format(len(found_df)))\n#     print('is_offence not found: {}'.format(len(not_found_df)))\n\n    for u_play_id in not_found_df['PlayId'].unique():\n        tmp_df = not_found_df[not_found_df['PlayId']==u_play_id]\n        pos_list = [pos for pos in tmp_df['Position'].unique() if pos in ['QB', 'RB', 'WR', 'TE']]\n        \n        if len(pos_list) > 0:\n            off_team = tmp_df[tmp_df['Position']==pos_list[0]]['Team'].values[0]\n#         else:\n#             print('Offence position not found')\n#             import pdb;pdb.set_trace()\n\n        target_idx = not_found_df.query('PlayId==@u_play_id and Team==@off_team').index\n        not_found_df.loc[target_idx, 'is_offence'] = 1\n    \n    df = pd.concat([found_df, not_found_df], sort=False)\n#     print('done df: {}'.format(df.shape))\n    return df","a67f3ea9":"def run_group_fe(df, group_key, aggs):\n    \n    group_df = df.groupby(group_key).agg(aggs)\n\n    new_cols = [col[0]+'_'+col[1] for col in group_df.columns]\n    group_df.columns = new_cols\n    group_df.reset_index(inplace=True)\n        \n    return group_df\n\ndef adjust_group_df(group_df, is_train):\n    offence_df = group_df[group_df['is_offence']==1]\n    deffence_df = group_df[group_df['is_offence']==0]\n\n    del group_df['is_offence']\n    del offence_df['is_offence']\n    del deffence_df['is_offence']\n    \n    if is_train:\n        off_cols = ['off_{}'.format(col) if col not in ['GameId', 'PlayId', 'Yards'] else col for col in group_df.columns]\n        deff_cols = ['deff_{}'.format(col) if col not in ['GameId', 'PlayId', 'Yards'] else col for col in group_df.columns]\n    else:\n        off_cols = ['off_{}'.format(col) if col not in ['GameId', 'PlayId'] else col for col in group_df.columns]\n        deff_cols = ['deff_{}'.format(col) if col not in ['GameId', 'PlayId'] else col for col in group_df.columns]\n        \n    offence_df.columns = off_cols\n    deffence_df.columns = deff_cols\n    if is_train: del deffence_df['Yards']\n    \n    adjusted_group_df = pd.merge(offence_df, deffence_df, on=['GameId', 'PlayId'])\n    \n    return adjusted_group_df","a243ec30":"train = run_fe_is_offence(train)","d7898a50":"train.head()","7d37481f":"train_group_key = ['GameId', 'PlayId', 'is_offence', 'Yards']\naggs = {\n    'X': ['mean', 'max', 'min', 'median'],\n    'Y': ['mean', 'max', 'min', 'median'],\n}\nis_train = True\ngroup_df = run_group_fe(train, train_group_key, aggs)\nadjusted_group_df = adjust_group_df(group_df, is_train)","c8dbb76f":"print(adjusted_group_df.shape)\nadjusted_group_df.head()","587c6f1e":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True","bb58dc4c":"def getNFLY(df):\n    y = np.zeros(shape=(df.shape[0], 199))\n    for i, yard in enumerate(df['Yards'].values):\n        y[i, yard+99:] = np.ones(shape=(1, 100-yard))\n    return y","37ae05f8":"def generate_dataloader(df, y_val, batch_size, train_idx, valid_idx):\n    train_x, train_y = df.iloc[train_idx].values, y_val[train_idx]\n    valid_x, valid_y = df.iloc[valid_idx].values, y_val[valid_idx] \n    \n    train_x = torch.from_numpy(train_x)\n    train_y = torch.from_numpy(train_y)\n    valid_x = torch.from_numpy(valid_x)\n    valid_y = torch.from_numpy(valid_y)\n    \n    train_dataset = TensorDataset(train_x, train_y)\n    valid_dataset = TensorDataset(valid_x, valid_y)\n\n    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n    valid_loader = DataLoader(valid_dataset, batch_size=batch_size, shuffle=False)\n    return train_dataset, valid_dataset, train_loader, valid_loader","c818437a":"\"\"\"\nref: https:\/\/github.com\/Bjarten\/early-stopping-pytorch\n\"\"\"\nclass EarlyStopping:\n    def __init__(self, patience=2, verbose=False):\n        self.patience = patience\n        self.verbose = verbose\n        self.counter = 0\n        self.best_score = None\n        self.early_stop = False\n        self.val_loss_min = np.Inf\n\n    def __call__(self, val_loss, model, save_name):\n\n        score = -val_loss\n\n        if self.best_score is None:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model, save_name)\n        elif score < self.best_score:\n            self.counter += 1\n            print(f'EarlyStopping counter: {self.counter} out of {self.patience}')\n            if self.counter >= self.patience:\n                self.early_stop = True\n        else:\n            self.best_score = score\n            self.save_checkpoint(val_loss, model, save_name)\n            self.counter = 0\n\n    def save_checkpoint(self, val_loss, model, save_name):\n        if self.verbose:\n            print(f'Validation loss decreased ({self.val_loss_min:.5f} --> {val_loss:.5f}).  Saving model ...')\n            print(\"Save model: {}\".format(save_name))\n        torch.save(model.state_dict(), save_name)\n        self.val_loss_min = val_loss","08569ac2":"class NFL_NN(nn.Module):\n    def __init__(self, in_features, out_features):\n        super().__init__()\n\n        self.fc1 = nn.Linear(in_features, 216)\n        self.bn1 = nn.BatchNorm1d(216)\n        self.relu1 = nn.ReLU()\n        self.fc2 = nn.Linear(216, 512)\n        self.relu2 = nn.ReLU()\n        self.fc3 = nn.Linear(512, 216)\n        self.relu3 = nn.ReLU()\n        self.dout3 = nn.Dropout(0.2)\n        self.out = nn.Linear(216, out_features)\n        self.out_act = nn.Sigmoid()\n        \n    def forward(self, input_):\n        a1 = self.fc1(input_)\n        bn1 = self.bn1(a1)\n        h1 = self.relu1(bn1)\n        a2 = self.fc2(h1)\n        h2 = self.relu2(a2)\n        a3 = self.fc3(h2)\n        h3 = self.relu3(a3)\n        dout3 = self.dout3(h3)\n        a5 = self.out(dout3)\n        y = self.out_act(a5)\n        return a5","266c3987":"def model_eval(model, dataset, data_loader, out_features, batch_size):\n    model.eval()\n\n    preds = np.zeros((len(dataset), out_features))\n    with torch.no_grad():\n        for i, eval_x_batch in enumerate(data_loader):\n                eval_values = eval_x_batch[0].float()\n                pred = model(eval_values)\n                preds[i * batch_size:(i + 1) * batch_size] = pred\n                \n    return preds","d260c35e":"def run_train_nn(train_df, epoch, batch_size):\n    oof_crps_list = []\n\n    fold = GroupKFold(n_splits=5)\n\n    train_y = getNFLY(train_df)\n\n    oof_preds = np.ones((len(train_df), train_y.shape[1]))\n\n    feats = [\n        \"off_X_mean\",\"off_X_max\",\"off_X_min\",\"off_X_median\",\"off_Y_mean\",\"off_Y_max\",\"off_Y_min\",\"off_Y_median\",\n        \"deff_X_mean\",\"deff_X_max\",\"deff_X_min\",\"deff_X_median\",\"deff_Y_mean\",\"deff_Y_max\",\"deff_Y_min\",\"deff_Y_median\",\n    ]\n\n    print('use feats: {}'.format(len(feats)))\n\n    for n_fold, (train_idx, valid_idx) in enumerate(fold.split(train_df, train_y, groups=train_df['GameId'])):\n        print('Fold: {}'.format(n_fold+1))\n\n        early_stopping = EarlyStopping(patience=2, verbose=True)\n\n        train_dataset, valid_dataset, train_loader, valid_loader = generate_dataloader(train_df[feats], train_y, batch_size, train_idx, valid_idx)\n\n        print('train: {}, valid: {}'.format(len(train_dataset), len(valid_dataset)))\n\n        in_features = train_df[feats].shape[1]\n        out_features = train_y.shape[1]\n        model = NFL_NN(in_features, out_features)\n        criterion = nn.MSELoss()\n        optimizer = torch.optim.Adam(model.parameters(), lr=0.005)\n\n        for idx in range(epoch):\n            print('Training epoch {}'.format(idx+1))\n            train_batch_loss_sum = 0\n\n            for param in model.parameters():\n                param.requires_grad = True\n\n            model.train()\n            for x_batch, y_batch in tqdm(train_loader):\n                y_pred = model(x_batch.float())\n                loss = torch.sqrt(criterion(y_pred.float(), y_batch.view((len(y_batch), out_features)).float()))\n                train_batch_loss_sum += loss.item()\n\n                del x_batch\n                del y_batch\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n                torch.cuda.empty_cache()\n                gc.collect()\n\n            train_epoch_loss = train_batch_loss_sum \/ len(train_loader)\n\n            valid_y_pred = model_eval(model, valid_dataset, valid_loader, out_features, batch_size)\n            valid_crps = np.sum(np.power(valid_y_pred - valid_dataset[:][1].data.cpu().numpy(), 2))\/(199*len(valid_dataset))\n\n            oof_preds[valid_idx] = valid_y_pred\n\n            print('Train Epoch Loss: {:.5f}, Valid CRPS: {:.5f}'.format(train_epoch_loss, valid_crps))\n\n            model_save_name = 'checkpoint_fold_{}.pt'.format(n_fold+1)\n            early_stopping(valid_crps, model, model_save_name)\n\n            if early_stopping.early_stop:\n                print(\"Early stopping\")\n                break\n                \n        oof_crps_list.append(-early_stopping.best_score)\n\n        del model, criterion, optimizer\n        gc.collect()\n\n    print('DONE OOF MEAN CRPS: {:.5f}'.format(np.mean(oof_crps_list)))\n    print('DONE OOF ALL CRPS: {:.5f}'.format(np.sum(np.power(oof_preds - train_y, 2))\/(199*len(oof_preds))))\n        \n    return oof_preds","3fb1d5fa":"seed_everything(1234)\nepoch = 10\nbatch_size = 1012\noof_preds = run_train_nn(adjusted_group_df, epoch, batch_size)","c486a3ea":"def min_max_scaler(x):\n    return (x - np.min(x)) \/ (np.max(x) - np.min(x))\n\ndef scale_predict(preds):\n    y_pred = preds.copy()\n    adjust_preds = np.zeros((len(y_pred), y_pred.shape[1]))\n    for idx, pred in enumerate(y_pred):\n        prev = 0\n        for i in range(len(pred)):\n            if pred[i]<prev:\n                pred[i]=prev\n            prev=pred[i]\n        x = min_max_scaler(pred)\n        adjust_preds[idx, :] = x\n\n    adjust_preds[:, -1] = 1\n    adjust_preds[:, 0] = 0\n    return adjust_preds","ea2932ab":"def run_nn_nfl_inference(test_df, model_path, batch_size):\n    feats = [\n        \"off_X_mean\",\"off_X_max\",\"off_X_min\",\"off_X_median\",\"off_Y_mean\",\"off_Y_max\",\"off_Y_min\",\"off_Y_median\",\n        \"deff_X_mean\",\"deff_X_max\",\"deff_X_min\",\"deff_X_median\",\"deff_Y_mean\",\"deff_Y_max\",\"deff_Y_min\",\"deff_Y_median\",\n    ]\n    \n    test = torch.from_numpy(test_df[feats].values)\n    test_dataset = TensorDataset(test)\n    test_loader = DataLoader(test_dataset, batch_size, shuffle=False)\n    \n    in_features = test_df[feats].shape[1]\n    out_features = 199\n    model = NFL_NN(in_features, out_features)\n    model.load_state_dict(torch.load(model_path))\n    nfl_pred = model_eval(model, test_dataset, test_loader, out_features, batch_size)\n    del model\n    gc.collect()\n    return nfl_pred","992b95b0":"def run_nn_nfl_pipeline(df, sample, models, batch_size):\n    nfl_pred = np.zeros((len(df), 199))\n    for idx, path in enumerate(models):\n        nfl_pred += run_nn_nfl_inference(df, path, batch_size)\/len(models)\n\n    adjust_nfl_pred = scale_predict(nfl_pred)\n\n    preds_df = pd.DataFrame(data=adjust_nfl_pred[0].reshape(1, 199), columns=sample.columns)\n    env.predict(preds_df)\n    \n    return preds_df","961fc00a":"nn_model_path_list = [\n    'checkpoint_fold_1.pt', 'checkpoint_fold_2.pt', 'checkpoint_fold_3.pt', 'checkpoint_fold_4.pt', 'checkpoint_fold_5.pt', \n]\n\nenv = nflrush.make_env()\nresult_df = None\nis_train = False\ntest_group_key = ['GameId', 'PlayId', 'is_offence']\n\nfor (test_df, sample_prediction_df) in tqdm(env.iter_test()):\n    \n    test_df = run_fe_is_offence(test_df)\n    test_group_df = run_group_fe(test_df, test_group_key, aggs)\n    test_adjusted_group_df = adjust_group_df(test_group_df, is_train)\n    \n    tmp_result_df = run_nn_nfl_pipeline(test_adjusted_group_df, sample_prediction_df, nn_model_path_list, batch_size)\n    \n    if result_df is None:\n        result_df = tmp_result_df\n    else:\n        result_df = pd.concat([result_df, tmp_result_df], sort=False)\n        \nresult_df.drop_duplicates(inplace=True)\n\nenv.write_submission_file()","3ef4298f":"print(result_df.shape)\nresult_df.head(30)","5f86e91e":"# Run FE","dc9f0f46":"# Import","1bd225bf":"# Run NN Inference","60cc75a6":"# Prepare train data","82bc4522":"# Overview\nThis notebook provides a baseline by PyTorch.  \nIncludes the following:  \n* Offense and defense flags.\n* Group feature engineering by PlayId and offense and defense.\n* Learning using only the 16 features get from the above.\n\nPlease comment if there are your idea!!  \n\nupdate: is_offence bugfix  ","c0cd728d":"# Run NN","a1ad4013":"# Aggregation FE","24ad41a7":"# FE","67429721":"## NN Inference","66585881":"# NN"}}