{"cell_type":{"b53f02b4":"code","96c313e4":"code","5db135b0":"code","d7a5fe08":"code","fd9e3f58":"code","8e913bf8":"code","86f4f4f1":"code","b1b912cc":"code","d004288a":"code","65e51699":"code","e9574d1f":"code","dbe558fe":"code","614db292":"code","62c2404f":"code","cd995e25":"code","67b5ac02":"code","336caf6b":"code","3746da21":"code","02329d61":"code","c68f21ad":"code","d971fb6c":"code","1a9eb2c5":"code","01e67379":"code","7b7b7f29":"code","f02e8811":"code","e4188657":"code","4b65c46d":"code","4dc85636":"code","95193021":"code","eff07b37":"code","0cc03eca":"code","e0a121ba":"markdown"},"source":{"b53f02b4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","96c313e4":"dataset = pd.read_csv('\/kaggle\/input\/information-retreival\/bq-results-20201113-204001-2vh85ngpebdr.csv')","5db135b0":"dataset.head()","d7a5fe08":"dataset.tail()","fd9e3f58":"dataset","8e913bf8":"data = dataset.loc[:39999]\ndata","86f4f4f1":"columns = ['title', 'body', 'body_1', 'text', 'tags'] ","b1b912cc":"data['Content'] = data[columns].apply(lambda x: ','.join(x.dropna().astype(str)), axis=1)","d004288a":"data.head()","65e51699":"data.tail()","e9574d1f":"import re\nimport seaborn as sns\nimport string\nimport nltk\nfrom nltk.stem.porter import *\nfrom nltk.tokenize import RegexpTokenizer\nfrom nltk.corpus import stopwords\nfrom nltk.stem import WordNetLemmatizer \nfrom nltk import pos_tag\nfrom nltk.corpus import wordnet\nimport matplotlib.pyplot as plt\nfrom gensim.parsing.preprocessing import remove_stopwords","dbe558fe":"def remove_html_tags(text):\n    \"\"\"Remove html tags from a string\"\"\"\n    clean = re.compile('<.*?>')\n    return re.sub(clean, '', text)","614db292":"print('\\n\\nRemoving html tags: \\n\\n')\ndata['Content'] = np.vectorize(remove_html_tags)(data['Content'])","62c2404f":"data['Content'].head()","cd995e25":"print('\\n\\n Removing punctutaion and tokenising: \\n\\n')\ntokenizer = RegexpTokenizer(r'\\w+')\ndata['Content'] = data['Content'].apply(lambda x: tokenizer.tokenize(x))","67b5ac02":"print('\\n\\n Removing Stopwords \\n\\n')\n\nstop_words = set(stopwords.words('english'))\ndata['Content'] =  data['Content'].apply(lambda x: [i for i in x if not i in stop_words]) ","336caf6b":"lemmatizer = WordNetLemmatizer()\n\ndef get_pos(tag):\n    if tag.startswith('J'):\n        return wordnet.ADJ\n    elif tag.startswith('V'):\n        return wordnet.VERB\n    elif tag.startswith('R'):\n        return wordnet.ADV\n    else:\n        return wordnet.NOUN","3746da21":"print('\\n\\nLemmatizing\\n\\n')\n\ndata['taggedWords'] = data['Content'].apply(lambda x: nltk.pos_tag(x))\n                                \ndata['Content'] =  data['taggedWords'].apply(lambda x: [lemmatizer.lemmatize(word,get_pos(tag)) for word, tag in x])","02329d61":"data['Content'] = data['Content'].apply(lambda x: [i.lower() for i in x])","c68f21ad":"print('\\n\\n Combing for the sake of visualization \\n\\n')\n\ndata['Combine'] =  data['Content'].apply(lambda x: ' '.join([text for text in x]))","d971fb6c":"print('\\n\\n Wordcloud for all the words in the dataset \\n\\n')\nall_words = ' '.join([text for text in data['Combine']])\nfrom wordcloud import WordCloud\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_words)\n\nplt.figure(figsize=(15, 15))\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","1a9eb2c5":"query = 'In C++, can you define a variable in terms of other variables that have already been defined?'\nquery = tokenizer.tokenize(query)\nquery = [i for i in query if not i in stop_words]\ntagged_query = nltk.pos_tag(query)\nquery = [lemmatizer.lemmatize(word,get_pos(tag)) for word, tag in tagged_query]\nquery = [i.lower() for i in query]\nquery = ' '.join([text for text in query])\nquery","01e67379":"df = pd.DataFrame(columns = ['Combine'])\ndf.loc[0]= query\ndf","7b7b7f29":"df = df.append(data[['Combine']], ignore_index = 'True')\ndf","f02e8811":"from sklearn.feature_extraction.text import TfidfVectorizer\ntfidf = TfidfVectorizer(token_pattern = u'(?ui)\\\\b\\\\w*[a-z]+\\\\w*\\\\b', stop_words = 'english')","e4188657":"tf_idf = tfidf.fit_transform(df['Combine'])\ntf_idf","4b65c46d":"columns = tfidf.get_feature_names()","4dc85636":"from sklearn.metrics.pairwise import linear_kernel\nsim = linear_kernel(tf_idf, tf_idf[0]).flatten()\nsim.shape","95193021":"d = data\nd['Sim'] = sim[1:]\nd","eff07b37":"d = d.sort_values(by = ['Sim'], ascending = False)","0cc03eca":"result = d[['title']]\nresult.head()","e0a121ba":"2L"}}