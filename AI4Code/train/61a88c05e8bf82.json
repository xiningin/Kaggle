{"cell_type":{"40164eb1":"code","8e8f506e":"code","791ac4f2":"code","1c430eb7":"code","c7b84bfd":"code","ef58a4d9":"code","5e6c20c4":"code","9e4ca342":"code","25f5378e":"code","9f046382":"code","ec2864d7":"code","0c6396f7":"code","3f00b454":"code","1869f76d":"code","b46c866b":"code","cc5ac10e":"code","7b366622":"code","f2017a06":"code","9cf406f6":"code","d6d276e4":"code","d0231633":"code","f2556363":"code","aba500ca":"code","6f8141b3":"code","ccb2516a":"code","fb94066c":"code","8e60439a":"code","a7c4adbd":"code","9833c075":"code","3e4518bc":"code","71512f41":"code","6b734424":"code","04571ecd":"code","9b0ae3b1":"code","b8f89087":"code","0a40f021":"code","c6e3bbe8":"code","3d1ce5ed":"code","82b65a64":"code","650efe8d":"code","d4517534":"code","aaf2e7e8":"code","0ff2e969":"code","ad14fcd1":"code","7561073f":"markdown","1c566263":"markdown","0b052143":"markdown","04e1af6e":"markdown"},"source":{"40164eb1":"# import tools for data manipulation\nimport pandas as pd\nimport numpy as np\n\n#visualisation tools\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# No warnings about setting value on copy of slice\npd.options.mode.chained_assignment = None\n\n#set font size\nplt.rcParams['font.size']=25\n\n#setting figure size\nfrom IPython.core.pylabtools import figsize\n\n#splitting test and train data\nfrom sklearn.model_selection import train_test_split\n\npd.set_option('display.max_columns',70)","8e8f506e":"energy = pd.read_csv('..\/input\/energy_data.csv')","791ac4f2":"print(energy.shape,'\\n')#11746 rows and 60 columns\nprint(energy.info())#check for missing values","1c430eb7":"energy.head()","c7b84bfd":"#covert the data types\nenergy =energy.replace({'Not Available':np.nan})\n\n#find columns that should be numeric and convert to float data type\n\nfor col in list(energy.columns):\n    if ('ft\u00b2' in col or 'kBtu' in col or 'Metric Tons CO2e' in col or 'kWh' in \n        col or 'therms' in col or 'gal' in col or 'Score' in col):\n            energy[col]=energy[col].astype(float)","ef58a4d9":"energy.info()","5e6c20c4":"# summary statistics upon success\nenergy.describe()","9e4ca342":"def missing_values_table_info(df):\n    #number of null elements in the dataframe\n    missing_vals=df.isnull().sum()\n    \n    #percantage of null elements\n    missing_vals_percentage=100*missing_vals\/len(df)\n    \n    #make a table of missing values\n    missing_values_table=pd.concat([missing_vals,missing_vals_percentage],axis=1)\n    \n    #rename the columns\n    missing_vals_rename_cols=missing_values_table.rename(columns={0:'missing values',1:'% of total'})\n    \n    #sort the dataframe in descending order by percentage of mising values\n    missing_vals_rename_cols=missing_vals_rename_cols[\n        missing_vals_rename_cols.iloc[:,1]!=0].sort_values('% of total',ascending=False).round(1)\n    # Print some summary information\n    print (\"The dataset has \" + str(df.shape[1]) + \" columns.\\n\"      \n        \"There are \" + str(missing_vals_rename_cols.shape[0]) +\n            \" columns that have missing values.\")\n    #return column with missing information\n    return  missing_vals_rename_cols    ","25f5378e":"missing_values_table_info(energy)","9f046382":"#find columns with 50% of their data so as to discard them.\n# this is because such columns may not be useful in our analysis\nmissing_df=missing_values_table_info(energy)\nmissing_columns=list(missing_df[missing_df['% of total']>50].index)\nprint('number of columns removed are {}'.format(len(missing_columns)))","ec2864d7":"#drop the columns\nenergy=energy.drop(columns=missing_columns,axis=1)","0c6396f7":"#check everything worked out\nmissing_values_table_info(energy)","3f00b454":"#first focus on the target, which is the energy_score\nfigsize(10,10)\n#rename the column\nenergy=energy.rename(columns={'ENERGY STAR Score':'score'})\n\n#histogram of this score\n\nplt.style.use('seaborn')\nplt.hist(energy['score'].dropna(),color='blue')\nplt.xlabel('energy score')\nplt.ylabel('number of buildings');plt.title('score distribution')","1869f76d":"#the anomalies are that an unusual number of managers reported 100% efficiency\n#while some reported 0% efficiency. this may be a sign of total honesty or just plain lies\n#using the energy use intensity which is unreported we can be able to show things clearly\n\nfigsize(10, 10)\nplt.hist(energy['Site EUI (kBtu\/ft\u00b2)'].dropna(), color = 'blue');\nplt.xlabel('EUI'); \nplt.ylabel('Count'); plt.title('EUI Distribution');","b46c866b":"# we have outliers and we should explore them a bit more\nenergy['Site EUI (kBtu\/ft\u00b2)'].describe()","cc5ac10e":"energy['Site EUI (kBtu\/ft\u00b2)'].dropna().sort_values().head()","7b366622":"energy['Site EUI (kBtu\/ft\u00b2)'].dropna().sort_values().tail()","f2017a06":"energy[energy['Site EUI (kBtu\/ft\u00b2)']==869265.000000]","9cf406f6":"# on the low end an extreme outlier is below 1st quartile - 3*interquatile range\n# on the high end an extreme outlier is above 3rd quartile + 3*interquatile range\n\n#find the quartiles\nfirst_q =energy['Site EUI (kBtu\/ft\u00b2)'].describe()['25%']\nupper_q= energy['Site EUI (kBtu\/ft\u00b2)'].describe()['75%']\n#interquartile range\niqr=upper_q - first_q\n                \n# remove outliers\nenergy=energy[(energy['Site EUI (kBtu\/ft\u00b2)']>(first_q-3* iqr)) & \n              (energy['Site EUI (kBtu\/ft\u00b2)'] < (upper_q + 3 * iqr))] ","d6d276e4":"#histogram of this score\nfigsize(7,7)\nplt.style.use('seaborn')\nplt.hist(energy['Site EUI (kBtu\/ft\u00b2)'].dropna(),color='blue')\nplt.xlabel('energy score')\nplt.ylabel('number of buildings');plt.title('score distribution')","d0231633":"#outliers have been removed and the skewness to the left eliminated.\n#histogram now shows a uniform distrinution\n\n# now investigate correlation between score and categorical features in the data. i.e building type\n# end goal is to find out which features have a high correlation coefficient to energy score\n# use a density plot. use buildings with more than 100 observatiions in the data so as not to clutter the image\n\n# list of building types with more than 100 observations\ntypes=energy.dropna(subset=['score'])\ntypes = types['Largest Property Use Type'].value_counts()\ntypes=list(types[types.values>100].index)\n\n#plot the density plot\nfigsize(13,11)\n\nfor building_type in types:\n    subset=energy[energy['Largest Property Use Type']==building_type]\n    #kde plot\n    sns.kdeplot(subset['score'].dropna(),label=building_type,alpha=0.6,shade=True)\n# label the plot\nplt.xlabel('Score', size = 20); plt.ylabel('Density', size = 20); \nplt.title('Density Plot of Scores by Building Type', size = 28);","f2556363":"#from above feature we can see that use type has effect on score\n#the feature is to be included in an ml model training thus hot encoding it is neccessary\n\n#to investigate another feature. this time we'll use streets and correlation to score. code is similar to above one\n\nstreets=energy.dropna(subset=['score'])\nstreets=streets['Street Name'].value_counts()\nstreets=list(streets[streets.values>100].index)\n\nfor street in streets:\n    subset=energy[energy['Street Name']==street]\n    #kde plot\n    sns.kdeplot(subset['score'].dropna(),label=street,alpha=0.6,shade=True)\n# label the plot\nplt.xlabel('Score', size = 20); plt.ylabel('Density', size = 20); \nplt.title('Density Plot of Scores by Street', size = 28);\n\n# there's some influence from street although not as much\n# on to the last categorical variable now","aba500ca":"# by borough(administrative centers like city wards suburbs in other areas)\nboroughs=energy.dropna(subset=['score'])\nboroughs=boroughs['Borough'].value_counts()\nboroughs=list(boroughs[boroughs.values>100].index)\n\nfor borough in boroughs:\n    subset=energy[energy['Borough']==borough]\n    #kde plot\n    sns.kdeplot(subset['score'].dropna(),label=street,alpha=0.6,shade=False)\n# label the plot\nplt.xlabel('Score', size = 20); plt.ylabel('Density', size = 20); \nplt.title('Density Plot of Scores by Borough', size = 28);","6f8141b3":"#now to find the pearson correlation coefficient of features with the score\ncorrelation=energy.corr()['score'].sort_values()\n#most negative correlations\nprint(correlation.head(10),'\\n')\n#most positive correlations\nprint(correlation.tail(10),'\\n')","ccb2516a":"# Select the numeric columns\nnumeric_subset = energy.select_dtypes('number')\n\n# Create columns with square root and log of numeric columns\nfor col in numeric_subset.columns:\n    # Skip the Energy Star Score column\n    if col == 'score':\n        next\n    else:\n        numeric_subset['sqrt_' + col] = np.sqrt(numeric_subset[col])\n        numeric_subset['log_' + col] = np.log(numeric_subset[col])\n\n# Select the categorical columns\ncategorical_subset = energy[['Borough', 'Street Name', 'Largest Property Use Type']]\n\n# One hot encode\ncategorical_subset = pd.get_dummies(categorical_subset)\n\n# Join the two dataframes using concat\n# Make sure to use axis = 1 to perform a column bind\nfeatures = pd.concat([numeric_subset, categorical_subset], axis = 1)\n\n# Drop buildings without a score\nfeatures = features.dropna(subset = ['score'])\n\n# Find correlations with the score \ncorrelations_data = features.corr()['score'].dropna().sort_values()","fb94066c":"#let's graph the most significant correlation (in terms of absolute value) in the dataset which is Site EUI (kBtu\/ft^2). \n#We can color the graph by the building type to show how that affects the relationship. We will use a scatter plot\n\nfigsize(12, 10)\n\n# Extract the building types\nfeatures['Largest Property Use Type'] = energy.dropna(subset = ['score'])['Largest Property Use Type']\n\n# Limit to building types with more than 100 observations (from previous code)\nfeatures = features[features['Largest Property Use Type'].isin(types)]\n\n# Use seaborn to plot a scatterplot of Score vs Log Source EUI\nsns.lmplot('Site EUI (kBtu\/ft\u00b2)', 'score', \n          hue = 'Largest Property Use Type', data = features,\n          scatter_kws = {'alpha': 0.8, 's': 60}, fit_reg = False,\n          size = 12, aspect = 1.2);\n\n# Plot labeling\nplt.xlabel(\"Site EUI\", size = 28)\nplt.ylabel('Score', size = 28)\nplt.title('Score vs Site EUI', size = 36);\n\n#There is a clear negative relationship between the Site EUI and the score","8e60439a":"#We select the numeric features, adds in log transformations of all the numeric features, \n#selects and one-hot encodes the categorical features, and joins the sets of features together. \n\n# Copy the original data\nfeatures = energy.copy()\n\n# Select the numeric columns\nnumeric_subset = energy.select_dtypes('number')\n\n# Create columns with log of numeric columns\nfor col in numeric_subset.columns:\n    # Skip the Energy Star Score column\n    if col == 'score':\n        next\n    else:\n        numeric_subset['log_' + col] = np.log(numeric_subset[col])\n        \n# Select the categorical columns\ncategorical_subset = energy[['Borough', 'Largest Property Use Type']]\n\n# One hot encode\ncategorical_subset = pd.get_dummies(categorical_subset)\n\n# Join the two dataframes using concat\n# Make sure to use axis = 1 to perform a column bind\nfeatures = pd.concat([numeric_subset, categorical_subset], axis = 1)\n\nfeatures.shape #We result to have 11,319 buildings and 110 columns with score inclusive.","a7c4adbd":"#not all feature are important\n#several are highly correlated and therefore redundant. let's remove those\nplot_data = energy[['Weather Normalized Site EUI (kBtu\/ft\u00b2)', 'Site EUI (kBtu\/ft\u00b2)']].dropna()\n\nplt.plot(plot_data['Site EUI (kBtu\/ft\u00b2)'], plot_data['Weather Normalized Site EUI (kBtu\/ft\u00b2)'], 'bo')\nplt.xlabel('Site EUI'); plt.ylabel('Weather Norm EUI')\nplt.title('Weather Norm EUI vs Site EUI, R = %0.4f' % np.corrcoef(energy[['Weather Normalized Site EUI (kBtu\/ft\u00b2)', 'Site EUI (kBtu\/ft\u00b2)']].dropna(), rowvar=False)[0][1]);","9833c075":"def remove_collinear_features(x, threshold):\n    '''\n    Objective:\n        Remove collinear features in a dataframe with a correlation coefficient\n        greater than the threshold. Removing collinear features can help a model\n        to generalize and improves the interpretability of the model.\n        \n    Inputs: \n        threshold: any features with correlations greater than this value are removed\n    \n    Output: \n        dataframe that contains only the non-highly-collinear features\n    '''\n    \n    # Dont want to remove correlations between Energy Star Score\n    y = x['score']\n    x = x.drop(columns = ['score'])\n    \n    # Calculate the correlation matrix\n    corr_matrix = x.corr()\n    iters = range(len(corr_matrix.columns) - 1)\n    drop_cols = []\n\n    # Iterate through the correlation matrix and compare correlations\n    for i in iters:\n        for j in range(i):\n            item = corr_matrix.iloc[j:(j+1), (i+1):(i+2)]\n            col = item.columns\n            row = item.index\n            val = abs(item.values)\n            \n            # If correlation exceeds the threshold\n            if val >= threshold:\n                # Print the correlated features and the correlation value\n                # print(col.values[0], \"|\", row.values[0], \"|\", round(val[0][0], 2))\n                drop_cols.append(col.values[0])\n\n    # Drop one of each pair of correlated columns\n    drops = set(drop_cols)\n    x = x.drop(columns = drops)\n    x = x.drop(columns = ['Weather Normalized Site EUI (kBtu\/ft\u00b2)', \n                          'Water Use (All Water Sources) (kgal)',\n                          'log_Water Use (All Water Sources) (kgal)',\n                          'Largest Property Use Type - Gross Floor Area (ft\u00b2)'])\n    \n    # Add the score back in to the data\n    x['score'] = y\n               \n    return x","3e4518bc":"# Remove the collinear features above a specified correlation coefficient in this case 0.6\nfeatures = remove_collinear_features(features, 0.6);","71512f41":"#drop all columns with na values\nfeatures = features.dropna(axis=1,how='all')\nfeatures.shape","6b734424":"#extract buildings with scores and those without scores\nno_score=features[features['score'].isnull()]\nwith_score=features[features['score'].notnull()]\n\nprint(no_score.shape)\nprint(with_score.shape)","04571ecd":"#separate the features and targets\nfeatures=with_score.drop('score',axis=1)\ntarget= pd.DataFrame(with_score['score'])\n\n# Replace the inf and -inf with nan (required for later imputation)\nfeatures = features.replace({np.inf: np.nan, -np.inf: np.nan})\n\n#separate train and test data\nX_train,X_test,y_train,y_test=train_test_split(features,target,test_size=0.3)","9b0ae3b1":"# Function to calculate mean absolute error. \ndef mae(y_true, y_pred):\n    return np.mean(abs(y_true - y_pred))\n#create a baseline\nbaseline_guess = np.median(y_train)\n\nprint('The baseline guess is a score of %0.2f' % baseline_guess)\nprint(\"Baseline Performance on the test set: MAE = %0.4f\" % mae(y_test, baseline_guess))","b8f89087":"#Saving Data\n\n# Save the no scores, training, and testing data\nno_score.to_csv('no_score.csv', index = False)\nX_train.to_csv('training_features.csv', index = False)\nX_test.to_csv('testing_features.csv', index = False)\ny_train.to_csv('training_labels.csv', index = False)\ny_test.to_csv('testing_labels.csv', index = False)","0a40f021":"#Imputing missing values and scaling values\nfrom sklearn.preprocessing import Imputer, MinMaxScaler\n\n#ML\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.neighbors import KNeighborsRegressor\n\n#Hyperparameter tuning\nfrom sklearn.model_selection import RandomizedSearchCV\nfrom sklearn.model_selection import GridSearchCV","c6e3bbe8":"# Reading in the data into dataframes \ntrain_features = pd.read_csv('training_features.csv')\ntest_features = pd.read_csv('testing_features.csv')\ntrain_labels = pd.read_csv('training_labels.csv')\ntest_labels = pd.read_csv('testing_labels.csv')\n\n# Display sizes of data\nprint('Training Feature Size: ', train_features.shape)\nprint('Testing Feature Size:  ', test_features.shape)\nprint('Training Labels Size:  ', train_labels.shape)\nprint('Testing Labels Size:   ', test_labels.shape)","3d1ce5ed":"#ml algorithms have a hard time understanding missing values\n#in this step imputation is done to deal with the rest of the missing values(as we had dealt with some above)\n\n#an imputter with a median filling strategy is applied here\nimputer=Imputer(strategy='median')\n\nimputer.fit(train_features)\n\n#transforming training and testing features\nX_train=imputer.transform(train_features)\nX_test=imputer.transform(test_features)\n\nprint('Missing values in training features: ', np.sum(np.isnan(X_train)))\nprint('Missing values in testing features:  ', np.sum(np.isnan(X_test)))","82b65a64":"#Making sure all values are finite\nprint (np.where(~np.isfinite(X_train)))\nprint (np.where(~np.isfinite(X_test)))","650efe8d":"#normalize the data so that different algorithms perform optimally\n#Creating a scaler object with a range of 0 - 1\nscaler = MinMaxScaler(feature_range = (0, 1))\n\n#Fit on the training data\nscaler.fit(X_train)\n\n#Transform both the training and testing data\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n\n#Converting y to one-dimensional array\ny_train = np.array(train_labels).reshape((-1,))\ny_test = np.array(test_labels).reshape((-1,))","d4517534":"#Training, testing and evaluating a model\ndef train_test_evaluate(model):\n    \n    #Train\n    model.fit(X_train,y_train)\n    \n    #Test\n    model_pred = model.predict(X_test)\n    \n    #Evaluate\n    model_mae = mae(y_test, model_pred)\n    \n    #Return performance metric\n    return model_mae","aaf2e7e8":"#Linear Regression\nlr = LinearRegression()\n\nlr_mae = train_test_evaluate(lr)\n\nprint ('Linear Regression Mean Absolute Error: %0.4f' %lr_mae, '\\n')\n\n#Support Vector Machines\nsvm = SVR(C = 1000, gamma = 0.1)\nsvm_mae = train_test_evaluate(svm)\n\nprint ('SVM Mean Absolute Error: %0.4f' %svm_mae, '\\n')\n\n#Random Forest\nrandom_forest = RandomForestRegressor(random_state = 60)\nrandom_forest_mae = train_test_evaluate(random_forest)\n\nprint ('Random Forest Mean Absolute Error: %0.4f' %random_forest_mae, '\\n')\n\n#Gradient Boosted Machines\ngradient_boosted = GradientBoostingRegressor(random_state = 60)\ngradient_boosted_mae = train_test_evaluate(gradient_boosted)\n\nprint ('Gradient Boosted Regression Mean Absolute Error: %0.4f' %gradient_boosted_mae, '\\n')\n\n#K-Nearest Neighbours                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                 \nknn = KNeighborsRegressor(n_neighbors = 10)\nknn_mae = train_test_evaluate(knn)\n\nprint ('K Nearest Neighbors Mean Absolute Error: %0.4f' %knn_mae)","0ff2e969":"plt.style.use('seaborn')\nfigsize(8, 6)\n\n#A dataframe to hold the results\nmodel_comparison = pd.DataFrame({'model': ['Linear Regression', \n                                           'Support Vector Machine', \n                                           'Random Forest', \n                                           'Gradient Boosted Machines', 'K-Nearest Neighbours'],\n                                'mae': [lr_mae, svm_mae, random_forest_mae, gradient_boosted_mae, knn_mae]})\n\n#Horizontal bar chart of MAE\nmodel_comparison.sort_values('mae', ascending = True).plot(x = 'model', \n                                                           y = 'mae', \n                                                           kind = 'barh', \n                                                           color = 'green', \n                                                           edgecolor = 'black')\n\n#Plot formatting\nplt.ylabel('Models', size = 14) \nplt.yticks(size = 12)\nplt.xlabel('Mean Absolute Error')\nplt.xticks(size = 12)\nplt.title('Model Comparison on Test MAE', size = 16)\n\n#We can see that there is a use for ML because all the models significantly outperform the baseline.","ad14fcd1":"#next time we'll continue on model optimisation","7561073f":"## machine learning","1c566263":"## feature selection and engineering\n -select two categorical features and the numeric features.\n \n -one hot encode the categorical features \n \n -remove collinear features","0b052143":"# data cleaning and formating","04e1af6e":"# EDA \n1.to check for anomalies\n2.correlation of features to each other and to the target\n3.patterns and trends"}}