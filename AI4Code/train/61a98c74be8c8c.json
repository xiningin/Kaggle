{"cell_type":{"aa0ceebc":"code","081d5a28":"code","1e2fe075":"code","ae6ac2e8":"code","23658c1c":"code","f52cc814":"code","af9d77da":"code","b8ba4d00":"code","587658a7":"code","97e63b71":"code","d87427b5":"code","5222b7b3":"code","de328c6c":"code","be5cb926":"code","ecc4b240":"code","c54bf5c5":"code","d4ecf606":"code","e2b9c498":"code","210f5faa":"code","2c387a5a":"code","f0c4cb22":"code","4b50bb91":"code","0596385d":"code","31e847d6":"code","d97ee5ca":"code","a3363dec":"code","50ce456c":"code","fa3ffad8":"markdown","3c7b4dd5":"markdown","6fb8a513":"markdown","4657fb45":"markdown","f698dbca":"markdown","8b98eb62":"markdown","33097637":"markdown","6bf70073":"markdown","3069a22b":"markdown","01d99f66":"markdown","0b45ffbe":"markdown","2d585b9f":"markdown","0861294e":"markdown","1ca4f9eb":"markdown","8530825d":"markdown"},"source":{"aa0ceebc":"from mpl_toolkits.mplot3d import Axes3D\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt # plotting\nimport numpy as np # linear algebra\nimport os # accessing directory structure\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n","081d5a28":"for dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n","1e2fe075":"# Distribution graphs (histogram\/bar graph) of column data\ndef plotPerColumnDistribution(df, nGraphShown, nGraphPerRow):\n    nunique = df.nunique()\n    df = df[[col for col in df if nunique[col] > 1 and nunique[col] < 50]] # For displaying purposes, pick columns that have between 1 and 50 unique values\n    nRow, nCol = df.shape\n    columnNames = list(df)\n    nGraphRow = (nCol + nGraphPerRow - 1) \/ nGraphPerRow\n    plt.figure(num = None, figsize = (6 * nGraphPerRow, 8 * nGraphRow), dpi = 80, facecolor = 'w', edgecolor = 'k')\n    for i in range(min(nCol, nGraphShown)):\n        plt.subplot(nGraphRow, nGraphPerRow, i + 1)\n        columnDf = df.iloc[:, i]\n        if (not np.issubdtype(type(columnDf.iloc[0]), np.number)):\n            valueCounts = columnDf.value_counts()\n            valueCounts.plot.bar()\n        else:\n            columnDf.hist()\n        plt.ylabel('counts')\n        plt.xticks(rotation = 90)\n        plt.title(f'{columnNames[i]} (column {i})')\n    plt.tight_layout(pad = 1.0, w_pad = 1.0, h_pad = 1.0)\n    plt.show()\n","ae6ac2e8":"# Correlation matrix\ndef plotCorrelationMatrix(df, graphWidth):\n    filename = df.dataframeName\n    df = df.dropna('columns') # drop columns with NaN\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    if df.shape[1] < 2:\n        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n        return\n    corr = df.corr()\n    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n    corrMat = plt.matshow(corr, fignum = 1)\n    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n    plt.yticks(range(len(corr.columns)), corr.columns)\n    plt.gca().xaxis.tick_bottom()\n    plt.colorbar(corrMat)\n    plt.title(f'Correlation Matrix for {filename}', fontsize=15)\n    plt.show()\n","23658c1c":"# Scatter and density plots\ndef plotScatterMatrix(df, plotSize, textSize):\n    df = df.select_dtypes(include =[np.number]) # keep only numerical columns\n    # Remove rows and columns that would lead to df being singular\n    df = df.dropna('columns')\n    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n    columnNames = list(df)\n    if len(columnNames) > 10: # reduce the number of columns for matrix inversion of kernel density plots\n        columnNames = columnNames[:10]\n    df = df[columnNames]\n    ax = pd.plotting.scatter_matrix(df, alpha=0.75, figsize=[plotSize, plotSize], diagonal='kde')\n    corrs = df.corr().values\n    for i, j in zip(*plt.np.triu_indices_from(ax, k = 1)):\n        ax[i, j].annotate('Corr. coef = %.3f' % corrs[i, j], (0.8, 0.2), xycoords='axes fraction', ha='center', va='center', size=textSize)\n    plt.suptitle('Scatter and Density Plot')\n    plt.show()\n","f52cc814":"nRowsRead = 1000 # specify 'None' if want to read whole file\n# HepatitisCdata.csv may have more rows in reality, but we are only loading\/previewing the first 1000 rows\ndf1 = pd.read_csv('\/kaggle\/input\/HepatitisCdata.csv', delimiter=',', nrows = nRowsRead)\ndf1.dataframeName = 'HepatitisCdata.csv'\nnRow, nCol = df1.shape\nprint(f'There are {nRow} rows and {nCol} columns')","af9d77da":"# Choosing to drop rows that have missing values to improve our predective analysis\ndf1 = df1.dropna(axis = 0, how ='any')  \nnRow, nCol = df1.shape\nprint(f'After dropna => There are {nRow} rows and {nCol} columns')\n\n#PS: if you wish to continue not drop the rows but would allow the skLearn library to perform substitution of the mean-value for missing values then you can comment out the above 3 lines. The algorithm further allows for the  ","b8ba4d00":"df1.head(5)","587658a7":"print(df1['Category'].unique())","97e63b71":"#we will now create a numeric for Category\ndf1['CategoryCode'] = -1 # initialize all values in this column to -1\ndf1['CategoryCode'] = np.where(df1.Category=='0=Blood Donor','0', df1.CategoryCode)\ndf1['CategoryCode'] = np.where(df1.Category=='1=Hepatitis','1', df1.CategoryCode)\ndf1['CategoryCode'] = np.where(df1.Category=='2=Fibrosis','2', df1.CategoryCode)\ndf1['CategoryCode'] = np.where(df1.Category=='3=Cirrhosis','3', df1.CategoryCode)\ndf1['CategoryCode'] = np.where(df1.Category=='0s=suspect Blood Donor','4', df1.CategoryCode)","d87427b5":"#we will now create a numeric for Sex as GenderCode\ndf1['GenderCode'] = -1 # initialize all values in this column to -1\ndf1['GenderCode'] = np.where(df1.Sex=='m','1', df1.GenderCode)\ndf1['GenderCode'] = np.where(df1.Sex=='M','1', df1.GenderCode)\ndf1['GenderCode'] = np.where(df1.Sex=='f','2', df1.GenderCode)\ndf1['GenderCode'] = np.where(df1.Sex=='F','2', df1.GenderCode)\n","5222b7b3":"print(df1['CategoryCode'].unique())\nprint(np.sort(df1['CategoryCode'].unique()))","de328c6c":"#Since we are getting too much interfearance from the Category=='0=Blood Donor' hence we are going to try dropping these rows from the collection for now.\n\n#df1.drop(df1[df1['Category']=='0=Blood Donor'].index, inplace=True)","be5cb926":"# Trying to see what each category looks like....\ndf_BloodDonor = df1[df1['Category']=='0=Blood Donor']\ndf_SuspectBloodDonor = df1[df1['Category']=='0s=suspect Blood Donor']\ndf_Hepatitis = df1[df1['Category']=='1=Hepatitis']\ndf_Fibrosis = df1[df1['Category']=='2=Fibrosis']\ndf_Cirrhosis = df1[df1['Category']=='3=Cirrhosis']\n\nnRow1, nCol1 = df_BloodDonor.shape\nprint(f'There are {nRow1} rows and {nCol1} columns in df_BloodDonor')\nnRow2, nCol2 = df_SuspectBloodDonor.shape\nprint(f'There are {nRow2} rows and {nCol2} columns in df_SuspectBloodDonor')\nnRow3, nCol3 = df_Hepatitis.shape\nprint(f'There are {nRow3} rows and {nCol3} columns in df_Hepatitis')\nnRow4, nCol4 = df_Fibrosis.shape\nprint(f'There are {nRow4} rows and {nCol4} columns in df_Fibrosis')\nnRow5, nCol5 = df_Cirrhosis.shape\nprint(f'There are {nRow5} rows and {nCol5} columns in df_Cirrhosis')","ecc4b240":"#created by Kaggle bot .. hence commenting out\n#plotPerColumnDistribution(df1, 10, 5)","c54bf5c5":"#Created by Kaggle bot hence commenting out\n#plotCorrelationMatrix(df1, 8)\n# PS.. once we perform the dropNa for the missing values, then this matrix code is irrelevant.","d4ecf606":"#Created by Kaggle bot hence commenting out\n#plotScatterMatrix(df1, 18, 10)","e2b9c498":"#function to detect the outliers\ndef detect_outlier(data_1):\n    outliers=[]\n    threshold=3\n    mean_1 = np.mean(data_1)\n    std_1 =np.std(data_1)\n    \n    \n    for y in data_1:\n        z_score= (y - mean_1)\/std_1 \n        if np.abs(z_score) > threshold:\n            outliers.append(y)\n    return outliers","210f5faa":"# Working step....was trying to understand data.. \n\nprint('Outlier datapoints')\n#1\nprint('ALB Outlier datapoints')\nalb_outlier_datapoints = detect_outlier(df1.ALB)\nprint(alb_outlier_datapoints)\n#2\nprint('ALP Outlier datapoints')\nalp_outlier_datapoints = detect_outlier(df1.ALP)\nprint(alp_outlier_datapoints)\n#3\nprint('ALT Outlier datapoints')\nalt_outlier_datapoints = detect_outlier(df1.ALT)\nprint(alt_outlier_datapoints)\n#4\nprint('AST Outlier datapoints')\nast_outlier_datapoints = detect_outlier(df1.AST)\nprint(ast_outlier_datapoints)\n#5\nprint('BIL Outlier datapoints')\nbil_outlier_datapoints = detect_outlier(df1.BIL)\nprint(bil_outlier_datapoints)\n#6\nprint('CHE Outlier datapoints')\nche_outlier_datapoints = detect_outlier(df1.CHE)\nprint(che_outlier_datapoints)\n#7\nprint('CHOL Outlier datapoints')\nchol_outlier_datapoints = detect_outlier(df1.CHOL)\nprint(chol_outlier_datapoints)\n#8\nprint('CREA Outlier datapoints')\ncrea_outlier_datapoints = detect_outlier(df1.CREA)\nprint(crea_outlier_datapoints)\n#9\nprint('GGT Outlier datapoints')\nggt_outlier_datapoints = detect_outlier(df1.GGT)\nprint(ggt_outlier_datapoints)\n#10\nprint('PROT Outlier datapoints')\nprot_outlier_datapoints = detect_outlier(df1.PROT)\nprint(prot_outlier_datapoints)","2c387a5a":"#Working step: was considering if I should remove the '0s=suspect Blood Donor' category rows .. but chose to leave this category inand not make an assumption.\n#df_test = pd.DataFrame(columns=df1.columns)\n#cond = df1.Category=='0s=suspect Blood Donor'\n#rows = df1.loc[cond, :]\n#df_test = df_test.append(rows, ignore_index=True)\n#df_train = df1.drop(rows.index)\n#print('df1 shape = ' +  str(df1.shape))\n#print ('df-test shape = ' + str(df_test.shape))\n#print ( 'df_train shape = ' +  str(df_train.shape))\n#df_train.head(5)\n#df_test.head(5)\n","f0c4cb22":"#Critical working step in analysis - do not drop\/remove\n#This method replaces the mean value of the column in the missing values\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\n\n\ndef score_dataset(X_data_train, X_data_test, y_data_train, y_data_test):\n    model = RandomForestRegressor()\n    model.fit(X_data_train, y_data_train)\n    preds = model.predict(X_data_test)\n    return mean_absolute_error(y_data_test, preds)","4b50bb91":"#Critical working step in analysis - do not drop\/remove\n################ \n#The X variable contains the first four columns of the dataset (i.e. attributes) \n#while y contains the labels.\n\n#X = df1.iloc[:,[2,4,5,6,7,8,9,10,11,12,13] ].values\nX = df1.iloc[:,[4,5,6,7,8,9,10,11,12,13] ].values # dropping column indx 2 which is Age from the x collection\ny = df1.iloc[:,[14] ].values.ravel() # choosing the CategoryCode only i,e col position 14\n\n#Used for debugging\n#print('values of X')\n#print(X)\n#print('values of y')\n#print(y)\n\n\n#To create training and test splits, execute the following script:\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20)\n\n#The above script splits the dataset into 80% train data and 20% test data. \n#This means that out of total 150 records, the training set will contain 120 \n#records and the test set contains 30 of those records.\n\n\n","0596385d":"scalarInfo_text = 'Before making any actual predictions, it is always a good practice to scale the features so that all of them can be uniformly evaluated.'\nscalarInfo_text += 'Since the range of values of raw data varies widely, in some machine learning algorithms, objective functions will not work properly without normalization.'\nscalarInfo_text += 'For example, the majority of classifiers calculate the distance between two points by the Euclidean distance. If one of the features has a broad range of values, the distance will be governed by this particular feature. Therefore, the range of all features should be normalized so that each feature contributes approximately proportionately to the final distance. '\nscalarInfo_text += 'The gradient descent algorithm (which is used in neural network training and other machine learning algorithms) also converges faster with normalized features.'\nprint(scalarInfo_text)","31e847d6":"#Critical working step in analysis - do not drop\/remove\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X_train)\nX_train = scaler.transform(X_train)\nX_test = scaler.transform(X_test)\n#PS: this makes sure the DataWarnings are not thrown","d97ee5ca":"#Critical working step in analysis - do not drop\/remove\n\n\n#Next to ensure that the where Missing values in the dataset dont impact the predictions, we \n# have choosen to use the imputer which basically substitutes the mean value where there are missing values \nfrom sklearn.impute import SimpleImputer\n\nmy_imputer = SimpleImputer()\nimputed_X_train = my_imputer.fit_transform(X_train)\nimputed_X_test = my_imputer.transform(X_test)\nprint(\"Mean Absolute Error from Imputation:\")\nprint(score_dataset(imputed_X_train, imputed_X_test, y_train, y_test))","a3363dec":"#Critical working step in analysis - do not drop\/remove\n\n\n# Adjust this n_neighbors_K_Value i.e. K value until you get precision closer to 1\nn_neighbors_K_Value = 3 # indicated various values i played with for K. tried ...1, 2, 3, 5, \n\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors=n_neighbors_K_Value)\nclassifier.fit(imputed_X_train, y_train)\n\n#The final step is to make predictions on our test data. To do so, execute the \n#following script:\ny_pred = classifier.predict(imputed_X_test)\n\n#For evaluating an algorithm, confusion matrix, precision, recall and f1 score are the \n#most commonly used metrics. The confusion_matrix and classification_report methods \n# of the sklearn.metrics can be used to calculate these metrics. Take a look at the \n# following script:\n\nfrom sklearn.metrics import classification_report, confusion_matrix\nprint('Confusion_matrix: ')\nprint(confusion_matrix(y_test, y_pred))\nprint('Classification_report: ')\nprint(classification_report(y_test, y_pred))\n\n######################################","50ce456c":"# This section is To determine Error_rate and find a range of K-Value when error is least\/minimal\nerror = []\nlowerlimit_K_runs = 1 # start with lowest K value i.e 1.... \nupperlimit_K_runs = 40  # 10, 40, .. values tried to help evalaute the graph for error_rate and best k values\n\n# Calculating error for K values between lowerlimit_K_runs( say 1)  and  upperlimit_K_runs(say 100)\nfor i in range(lowerlimit_K_runs, upperlimit_K_runs):\n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(imputed_X_train, y_train)\n    pred_i = knn.predict(imputed_X_test)\n    error.append(np.mean(pred_i != y_test))\n\n#The above script executes a loop from lowerlimit_K_runs to upperlimit_K_runs. In each iteration the mean error for predicted values of test set is calculated and the result is appended to the error list.\n#The next step is to plot the error values against K values. Execute the following script to create the plot:\n\nplt.figure(figsize=(12, 6))\nplt.plot(range(lowerlimit_K_runs, upperlimit_K_runs), error, color='red', linestyle='dashed', marker='o',\n         markerfacecolor='blue', markersize=10)\nplt.title('Error Rate K Value')\nplt.xlabel('K Value')\nplt.ylabel('Mean Error')\n\n#Hence playing around with the value of K to see how it impacts the accuracy of the predictions.\n#Initially Started K with value = 5","fa3ffad8":"There is 1 csv file in the current version of the dataset:\n","3c7b4dd5":"Now you're ready to read in the data and use the plotting functions to visualize the data.","6fb8a513":"Hello, This is Shaila Mandke. Here is my predective analysis of this dataset. I have used the KNN algorithm and my detailed working steps are included along with my final conclusion below.\nIf you have feedback, I can be reached at shaila.mandke@gmail.com. Thanks!\n","4657fb45":"### Let's check 1st file: \/kaggle\/input\/HepatitisCdata.csv","f698dbca":"Attempt 1 run for K= 3\n\nConfusion_matrix: \n[[103   0   0   0   0]\n [  6   1   0   0   0]\n [  0   0   1   0   0]\n [  0   0   0   5   0]\n [  2   0   0   0   0]]\nClassification_report: \n              precision    recall  f1-score   support\n\n           0       0.93      1.00      0.96       103\n           1       1.00      0.14      0.25         7\n           2       1.00      1.00      1.00         1\n           3       1.00      1.00      1.00         5\n           4       0.00      0.00      0.00         2\n\n    accuracy                           0.93       118\n   macro avg       0.79      0.63      0.64       118\nweighted avg       0.92      0.93      0.91       118\n","8b98eb62":"Attempt 3 run for K = 3\n\nConfusion_matrix: \n[[106   0   0   0   0]\n [  2   0   0   0   0]\n [  3   1   0   0   0]\n [  1   0   1   2   0]\n [  1   0   0   0   1]]\nClassification_report: \n              precision    recall  f1-score   support\n\n           0       0.94      1.00      0.97       106\n           1       0.00      0.00      0.00         2\n           2       0.00      0.00      0.00         4\n           3       1.00      0.50      0.67         4\n           4       1.00      0.50      0.67         2\n\n    accuracy                           0.92       118\n   macro avg       0.59      0.40      0.46       118\nweighted avg       0.89      0.92      0.90       118\n","33097637":"The next hidden code cells define functions for plotting data. Click on the \"Code\" button in the published kernel to reveal the hidden code.","6bf70073":"PS: After dropping rown that have missing values, the df1 shape was reduced to ( 589, 14). Next, with a (n_neighbors_K_Value = 3) i.e. K= 3, I have seen the KNN algorithm perform the best. Details:","3069a22b":"# Conclusion\n\nThis concludes my Predictive analysis using KNN! \n\nTodos and Analysis details:\n1) Gather more data - specifically for the category that we want to improve predictions. Like in this case CategoryCode 1, 2, 3 have very limited data compated to 0\n2) Make sure there are no missing values in the CSV\n3) Have used the KNN algorithm for this analysis\n4) Play around with the value of K to see if you can gather a better outcome. But looking at the Error Rate K value graph, looks like value of K=3 IS OPTIMAL\n5) When you see the 'UndefinedMetricWarning' in the output , it means predected y missing values\/outcomes. which means that not all categorycode had values evalauted. More specific data for these category will improve the evaluations","01d99f66":"Scatter and density plots:","0b45ffbe":"Correlation matrix:","2d585b9f":"Distribution graphs (histogram\/bar graph) of sampled columns:","0861294e":"Let's take a quick look at what the data looks like:","1ca4f9eb":"## Conclusion\nThis concludes your starter analysis! To go forward from here, click the blue \"Fork Notebook\" button at the top of this kernel. This will create a copy of the code and environment for you to edit. Delete, modify, and add code as you please. Happy Kaggling!","8530825d":"Attempt 2 run for K= 3\n\nConfusion_matrix: \n[[103   0   0   0   0]\n [  5   1   0   0   0]\n [  1   0   0   0   0]\n [  1   0   1   4   0]\n [  2   0   0   0   0]]\nClassification_report: \n              precision    recall  f1-score   support\n\n           0       0.92      1.00      0.96       103\n           1       1.00      0.17      0.29         6\n           2       0.00      0.00      0.00         1\n           3       1.00      0.67      0.80         6\n           4       0.00      0.00      0.00         2\n\n    accuracy                           0.92       118\n   macro avg       0.58      0.37      0.41       118\nweighted avg       0.90      0.92      0.89       118\n\n"}}