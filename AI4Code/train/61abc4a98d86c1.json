{"cell_type":{"0bf2ad45":"code","09dbf25d":"code","0fb45dec":"code","b05cbaa9":"code","97f9bafb":"code","7ee5fb55":"code","6de83942":"code","0d35f0cd":"code","f33f25c1":"code","526a4fb4":"code","0aa6fb8e":"code","21c78bd9":"code","8863a96d":"code","e1ec8773":"code","db4cb9eb":"code","a6ebba12":"code","d67d208c":"code","13fa7926":"code","80058556":"markdown","c53c028d":"markdown","94445e56":"markdown","5e608f17":"markdown","c96c93a1":"markdown","dc1c1198":"markdown","6f4cdd90":"markdown","67454a8d":"markdown","18fd0a95":"markdown","d481aa15":"markdown","9fc6b5af":"markdown","17d1561f":"markdown"},"source":{"0bf2ad45":"import pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\n\nfrom xgboost import XGBClassifier\n\nfrom pathlib import Path","09dbf25d":"class Config:\n    debug = False\n    competition = \"TPS_202110\"\n    seed = 42\n    NFOLDS = 5\n    EPOCHS = 10\n","0fb45dec":"data_dir = Path('..\/input\/tabular-playground-series-oct-2021') # Change me every month","b05cbaa9":"%%time\ntrain_df = pd.read_csv(data_dir \/ \"train.csv\",\n#                       nrows=100000\n                      )\n\ntest_df = pd.read_csv(data_dir \/ \"test.csv\")\nsample_submission = pd.read_csv(data_dir \/ \"sample_submission.csv\")\n\nprint(f\"train data: Rows={train_df.shape[0]}, Columns={train_df.shape[1]}\")\nprint(f\"test data : Rows={test_df.shape[0]}, Columns={test_df.shape[1]}\")","97f9bafb":"# this function will help to reduce momory \n# data will be smaller with the same value\n\ndef reduce_mem_usage(df):\n    \"\"\" iterate through all the columns of a dataframe and modify the data type\n        to reduce memory usage.        \n    \"\"\"\n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","7ee5fb55":"# %%time\n# train_df = reduce_mem_usage(train_df)\n# test_df  = reduce_mem_usage(test_df)","6de83942":"features = [col for col in train_df.columns if col not in ('id', 'target')]","0d35f0cd":"y = train_df.target\n\ntest_df = test_df.drop([\"id\"], axis=1)\nX = train_df.drop([\"id\", \"target\"], axis=1)\nX.head()","f33f25c1":"# x_train, x_valid, y_train, y_valid = train_test_split(X, y,\n#                                                     test_size=0.2,\n#                                                     stratify=None,\n#                                                     random_state=42)\n","526a4fb4":"# del train_df # Save some memory?","0aa6fb8e":"xgb_params = {\n    'device_type':'gpu',  # Use cpu\/gpu\n    'gpu_id':0,\n    'gpu_platform_id':0,\n#     'objective':'binary:logistic',\n    'use_label_encoder': False,\n    'tree_method': 'gpu_hist',\n\n    'metric': 'auc',\n#     'num_leaves': 150,\n    'learning_rate': 0.05,\n    'max_depth': 3,\n\n#     'n_estimators': 10000,\n    }","21c78bd9":"xgb_params = {\n    'max_depth': 6,\n    'learning_rate': 0.007,\n    'n_estimators': 9500,\n    'subsample': 0.7,\n    'colsample_bytree': 0.2,\n    'colsample_bylevel': 0.6000000000000001,\n    'min_child_weight': 56.41980735551558,\n    'reg_lambda': 75.56651890088857,\n    'reg_alpha': 0.11766857055687065,\n    'gamma': 0.6407823221122686,\n    'booster': 'gbtree',\n    'eval_metric': 'auc',\n    'tree_method': 'gpu_hist',\n    'predictor': 'gpu_predictor',\n    'use_label_encoder': False\n    }","8863a96d":"final_test_predictions = []\nfinal_valid_predictions = {}\nscores = []\n\nkf = StratifiedKFold(n_splits=Config.NFOLDS, shuffle=True, random_state=Config.seed)\n\nfor fold, (train_idx, valid_idx) in enumerate(kf.split(X = X, y = y)):\n#     print(type(train_idx))\n    print(10*\"=\", f\"Fold={fold}\", 10*\"=\")\n\n    x_train = X.loc[train_idx, :]\n    x_valid = X.loc[valid_idx, :]\n    \n    y_train = y[train_idx]\n    y_valid = y[valid_idx]\n    model = XGBClassifier(**xgb_params)\n\n#     model = XGBClassifier(\n#         objective = 'binary:logistic',\n#         max_depth=3,\n#         subsample=0.5,\n#         colsample_bytree=0.5,\n#         use_label_encoder = False,\n#     #     eval_metric = 'auc',\n#         missing=1,\n#         n_jobs=-1,\n#         n_estimators=1000,\n#         # Uncomment if you want to use GPU. Recommended for whole training set.\n#         tree_method='gpu_hist',\n#         random_state=42,\n#     )\n    model.fit(x_train, y_train,\n          early_stopping_rounds=100,\n          eval_set=[(x_valid, y_valid)],\n          verbose=1)\n\n    \n    preds_valid = model.predict_proba(x_valid)[:, -1]\n    # Want probability or classification?\n    final_valid_predictions.update(dict(zip(valid_idx, preds_valid)))\n\n    auc = roc_auc_score(y_valid,  preds_valid)\n    print('auc: ', auc)\n    scores.append(auc)\n    \n    test_preds = model.predict_proba(test_df[features])[:, -1]\n    final_test_predictions.append(test_preds)\n","e1ec8773":"print(f\"scores -> mean: {np.mean(scores)}, std: {np.std(scores)}\")","db4cb9eb":"final_valid_predictions = pd.DataFrame.from_dict(final_valid_predictions, orient=\"index\").reset_index()\nfinal_valid_predictions.columns = [\"id\", \"pred_1\"]\nfinal_valid_predictions.to_csv(\"train_pred_1.csv\", index=False)","a6ebba12":"print(model.objective)","d67d208c":"df = pd.DataFrame(np.column_stack(final_test_predictions))\ndf['mean'] = df.mean(axis=1)\ndf","13fa7926":"sample_submission['target'] = np.mean(np.column_stack(final_test_predictions), axis=1)\nsample_submission.to_csv(\"test_pred_1.csv\",index=None)\nsample_submission.to_csv(\"basic_xgb_cv.csv\",index=None)\nsample_submission","80058556":"# Save OOF Predictions\n\nSave the dictionary that we created for all the training predictions that were made when each fold was used for validation","c53c028d":"# Extract Target and Drop Unused Columns","94445e56":"# Train\/Validation Split","5e608f17":"# Predict on Test Data","c96c93a1":"# Model","dc1c1198":"# Load Data","6f4cdd90":"# Configuration","67454a8d":"# Submission File","18fd0a95":"# Feature Engineering","d481aa15":"# Reduce Memory\n\nToo many memory issues. Got a function to reduce the float and int types by checking the max column value and setting column to minimum necessary type.\n\nhttps:\/\/www.kaggle.com\/hrshuvo\/tps-oct-21-xgb-kfold\nhttps:\/\/www.kaggle.com\/rinnqd\/reduce-memory-usage\n","9fc6b5af":"# TPS Oct 2021\n\nA \"Getting Started\" XGBoost notebook.\n\nSave train_pred_1.csv and test_pred_1.csv for blending.\n\nOther notebooks:\n\n- [TPS Oct 2021 - Basic CV LGBM](https:\/\/www.kaggle.com\/mmellinger66\/tps-oct-2021-basic-cv-lgbm) - train_pred_2.csv \/ test_pred_2.csv for blending.\n- TODO: Optuna\n\nThis notebook will not change much.  I'll create additional notebooks to add hyperparameter optimization, kFolds, feature engineering, etc.\n\nHopefully, each will serve as a tutorial for each topic.\n\n- V9: Try StratifiedKFold\n- V8: Started added Blending code\n- V7: GPU off to conserve, then forgetting to turn it back on...\n- V4: Tried different xgb_params\n- V3: Added KFold\n\n# References\n\n- [competition part-5: blending 101](https:\/\/www.kaggle.com\/abhishek\/competition-part-5-blending-101)\n","17d1561f":"# Load Libraries"}}