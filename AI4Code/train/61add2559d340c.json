{"cell_type":{"c975b57a":"code","ac6a016c":"code","fa25a1f5":"code","63d7465a":"code","11ba3acb":"code","fd874c3d":"code","c3d57094":"code","047bdb4c":"code","61871963":"code","fa654372":"code","a92c047c":"code","aed44d89":"code","755de987":"code","8db82494":"code","8e821e82":"code","0a88e724":"code","bf65260f":"code","c74bb9cb":"code","fd3f7537":"code","f250d5e6":"code","12e083b4":"code","f2818bcd":"code","97d6be5a":"code","49f3ca57":"code","dde7aecf":"code","aac1e5d2":"code","2fd8755b":"code","0a38f2d4":"code","62b8a7d3":"code","69c0f491":"code","33f538f8":"code","5a97fbdc":"code","c904fc18":"code","60fcb820":"code","d74edcc1":"code","08f77f0a":"code","19ad3017":"code","8b1ac4ca":"code","439b0724":"code","6ae177b0":"code","ce1ca231":"code","a617121d":"markdown","0581a45c":"markdown","1a7441c7":"markdown","19b6b493":"markdown","c1c4efdb":"markdown","a58e0e96":"markdown","8beb1e39":"markdown","967c0d16":"markdown","e7429669":"markdown","27385226":"markdown","f000f84d":"markdown","e75fc8e1":"markdown","bdd94602":"markdown","b474a3e9":"markdown"},"source":{"c975b57a":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ac6a016c":"df_train = pd.read_csv('\/kaggle\/input\/mobile-price-classification\/train.csv')","fa25a1f5":"pd.set_option('display.max_columns',50)\ndf_train.head()","63d7465a":"df_train.info()","11ba3acb":"df_train.isnull().sum()","fd874c3d":"df_train.price_range.value_counts()","c3d57094":"import matplotlib.pyplot as plt\nimport seaborn as sns","047bdb4c":"df_plot_1 = df_train[df_train.columns[0:10]].copy()\ndf_plot_2 = df_train[df_train.columns[10:21]].copy()","61871963":"y = df_plot_2.iloc[:,-1:]\ndf_plot_2.drop('price_range',axis=1,inplace=True)\ndf_plot_1.head()\n","fa654372":"df_plot_2.head()","a92c047c":"df_plot_2.shape","aed44d89":"df_plot_1.shape","755de987":"data_std_1 = (df_plot_1 - df_plot_1.mean()) \/ (df_plot_1.std())              # standardization\ndata_std_2 = (df_plot_2 - df_plot_2.mean()) \/ (df_plot_2.std())              # standardization\n\ndata_std_1['price_range'] = y\ndata_std_2['price_range'] = y\n","8db82494":"data_std_1","8e821e82":"data_std_2","0a88e724":"g = sns.pairplot(df_train, hue= 'price_range', height=2)\ng.map_lower(sns.kdeplot, levels=4, color=\".2\")","bf65260f":"df_train.columns","c74bb9cb":"g = sns.pairplot(data=df_train, hue= 'price_range', height=2.5,aspect=.5, y_vars = ['ram'],x_vars=['battery_power', 'blue', 'clock_speed', 'dual_sim', 'fc', 'four_g',\n       'int_memory', 'm_dep', 'mobile_wt', 'n_cores', 'pc', 'px_height',\n       'px_width', 'ram', 'sc_h', 'sc_w', 'talk_time', 'three_g',\n       'touch_screen', 'wifi'])\n","fd3f7537":"sns.set(style=\"dark\", palette=\"bright\")\ndata_1 = pd.melt(data_std_2,id_vars=\"price_range\",\n                    var_name=\"features\",\n                    value_name='value')\ndata_1\nplt.figure(figsize=(12,12))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"price_range\", data=data_1)\n\nplt.xticks(rotation=90)","f250d5e6":"sns.set(style=\"dark\", palette=\"bright\")\ndata_2 = pd.melt(data_std_1,id_vars=\"price_range\",\n                    var_name=\"features\",\n                    value_name='value')\nplt.figure(figsize=(12,12))\nsns.swarmplot(x=\"features\", y=\"value\", hue=\"price_range\", data=data_2)\n\nplt.xticks(rotation=90)","12e083b4":"f,ax = plt.subplots(figsize=(18, 18))\nsns.heatmap(df_train.corr(), annot=True, linewidths=.5, fmt= '.1f',ax=ax)","f2818bcd":"# plt.subplots(figsize=(18,12))\nsns.set(rc={'figure.figsize':(18,12)})\n\nsns.displot(df_train, x=\"battery_power\",hue=\"price_range\", kind=\"kde\",col='price_range')\nsns.displot(df_train, x=\"ram\",hue=\"price_range\", kind=\"kde\",col='price_range')\nsns.displot(df_train, x=\"px_width\",hue=\"price_range\", kind=\"kde\",col='price_range')\nsns.displot(df_train, x=\"px_height\",hue=\"price_range\", kind=\"kde\",col='price_range')\n\n","97d6be5a":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import classification_report,confusion_matrix,f1_score\nfrom sklearn.model_selection import train_test_split,cross_val_score,GridSearchCV\n\n","49f3ca57":"DT_pipeline = Pipeline(steps = [('scale',StandardScaler()),('DT',DecisionTreeClassifier(random_state=42))])\nRF_pipeline = Pipeline(steps = [('scale',StandardScaler()),('DT',RandomForestClassifier(random_state=42))])\nSVM_pipeline = Pipeline(steps = [('scale',StandardScaler()),('DT',SVC(random_state=42))])\nLR_pipeline = Pipeline(steps = [('scale',StandardScaler()),('DT',LogisticRegression(random_state=42))])\n\nX = df_train.iloc[:,:-1]\nY = df_train.iloc[:,-1]","dde7aecf":"DT_CROSS_VAL = cross_val_score(DT_pipeline,X,Y,cv=10)\nRF_CROSS_VAL = cross_val_score(RF_pipeline,X,Y,cv=10)\nSVM_CROSS_VAL = cross_val_score(SVM_pipeline,X,Y,cv=10)\nLR_CROSS_VAL = cross_val_score(LR_pipeline,X,Y,cv=10)","aac1e5d2":"RF_CROSS_VAL, SVM_CROSS_VAL, LR_CROSS_VAL, DT_CROSS_VAL","2fd8755b":"import plotly.express as ex\nimport plotly.graph_objs as go\nimport plotly.offline as pyo\nfrom plotly.subplots import make_subplots\nfig = make_subplots(rows=4, cols=1,shared_xaxes=True,subplot_titles=('Decision Tree Cross Val Scores',\n                                                                     'RandomForest Cross Val Scores',\n                                                                    'SVM Cross Val Scores','Logistic Regression Cross Val Scores'))\n\nfig.add_trace(\n    go.Scatter(x=list(range(0,len(DT_CROSS_VAL))),y=DT_CROSS_VAL,name='Decision Tree'),\n    row=1, col=1\n)\nfig.add_trace(\n    go.Scatter(x=list(range(0,len(DT_CROSS_VAL))),y=RF_CROSS_VAL,name='RandomForest'),\n    row=2, col=1\n)\nfig.add_trace(\n    go.Scatter(x=list(range(0,len(DT_CROSS_VAL))),y=SVM_CROSS_VAL,name='SVM'),\n    row=3, col=1\n)\nfig.add_trace(\n    go.Scatter(x=list(range(0,len(DT_CROSS_VAL))),y=LR_CROSS_VAL,name='Logistic Regression'),\n    row=4, col=1\n)\n\nfig.update_layout(height=700, width=900, title_text=\"Different Model 5 Fold Cross Validation\")\nfig.update_yaxes(title_text=\"F1 Score\")\nfig.update_xaxes(title_text=\"Fold #\")\n\nfig.show()","0a38f2d4":"from sklearn.model_selection import cross_val_predict\nf,ax = plt.subplots(figsize=(18, 18))\ny_train_pred = cross_val_predict(LR_pipeline, X, Y, cv=3)\nconf_mx = confusion_matrix(Y, y_train_pred)\nsns.heatmap(conf_mx, annot=True, linewidths=.5, fmt= '.1f',ax=ax)","62b8a7d3":"from sklearn.model_selection import cross_val_predict\nf,ax = plt.subplots(figsize=(18, 18))\ny_train_pred = cross_val_predict(RF_pipeline, X, Y, cv=3)\nconf_mx = confusion_matrix(Y, y_train_pred)\nsns.heatmap(conf_mx, annot=True, linewidths=.5, fmt= '.1f',ax=ax)","69c0f491":"import numpy as np\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.33)","33f538f8":"DT_pipeline.fit(X_train,y_train)\nRF_pipeline.fit(X_train,y_train)\nSVM_pipeline.fit(X_train,y_train)\nLR_pipeline.fit(X_train,y_train)\n\n\nDT_PRED   = DT_pipeline.predict(X_test)\nRF_PRED   =RF_pipeline.predict(X_test)\nSVM_PRED  = SVM_pipeline.predict(X_test)\nLR_PRED   = LR_pipeline.predict(X_test)\n\nDT_CM  = confusion_matrix(y_test,DT_PRED )\nRF_CM  = confusion_matrix(y_test,RF_PRED )\nSVM_CM = confusion_matrix(y_test,SVM_PRED)\nLR_CM  = confusion_matrix(y_test,LR_PRED )\n\nDT_F1  = f1_score(y_test,DT_PRED,average='weighted' )\nRF_F1  = f1_score(y_test,RF_PRED,average='weighted' )\nSVM_F1 = f1_score(y_test,SVM_PRED,average='weighted')\nLR_F1  = f1_score(y_test,LR_PRED,average='weighted' )","5a97fbdc":"fig = go.Figure()\nfig.add_trace(go.Bar(x=['Decision Tree','Random Forest','SVM','Logistic Regression'],y=[DT_F1,RF_F1,SVM_F1,LR_F1]))\nfig.update_layout(title='F1 Score Of Our Model On Original Data',xaxis_title='Model',yaxis_title='F1 Score')\nfig.show()","c904fc18":"fig = go.Figure()\nfig.add_trace(go.Bar(x=X_train.columns,y=RF_pipeline['DT'].feature_importances_))\nfig.update_layout(title='The Importance Of The Original Attributes On Our Prediction',xaxis_title='Model',yaxis_title='F1 Score')\nfig.show()","60fcb820":"from sklearn.feature_selection import RFECV\n\n# The \"accuracy\" scoring is proportional to the number of correct classifications\nclf_rf = RandomForestClassifier() \nrfecv = RFECV(estimator=clf_rf, step=1, cv=5,scoring='accuracy')   #5-fold cross-validation\nrfecv = rfecv.fit(X_train, y_train)\n\nprint('Optimal number of features :', rfecv.n_features_)\nprint('Best features :', X_train.columns[rfecv.support_])","d74edcc1":"# Find best hyperparameters (roc_auc)\nfrom sklearn.model_selection import GridSearchCV\nlog_clf = LogisticRegression(random_state = 42)\nparam_grid = {'class_weight' : ['balanced', None], \n                'penalty' : ['l2','l1'],  \n                'C' : [0.001, 0.01, 0.1, 1, 10, 100, 1000]}\n\ngrid = GridSearchCV(estimator = log_clf, param_grid = param_grid , scoring = 'roc_auc', verbose = 1, n_jobs = -1)\n\ngrid.fit(X,Y)\n\nprint(\"Best Score:\" + str(grid.best_score_))\nprint(\"Best Parameters: \" + str(grid.best_params_))\n\nbest_parameters = grid.best_params_","08f77f0a":"scaler = StandardScaler()\nX_scaled =scaler.fit_transform(X)","19ad3017":"log_clf = LogisticRegression(**best_parameters)\nlog_clf.fit(X_scaled,Y)\n","8b1ac4ca":"y_pred   = log_clf.predict(X_test)\n","439b0724":"x_test_data = pd.read_csv('\/kaggle\/input\/mobile-price-classification\/test.csv')\nx_test_data.drop('id',axis=1,inplace=True)\nscaling_test = StandardScaler()\nx_test_scaled =scaling_test.fit_transform(x_test_data)","6ae177b0":"y_prediction= log_clf.predict(x_test_scaled)","ce1ca231":"unique, counts = np.unique(y_prediction, return_counts=True)\ndict(zip(unique, counts))","a617121d":"## Plotting heatmap of Correlation between features","0581a45c":"## As visible from Swarmplot, RAM is indeed very important feature for pricing ","1a7441c7":"## taking equal columns in 2 dataset to plot swarmplot","19b6b493":"## confusion matrix for RandomForest (Multiclass)","c1c4efdb":"## Some more visualization of Battery power, RAM, px_height and width","a58e0e96":"## Standarzation for features in the dataset ","8beb1e39":"## confusion matrix for Logistic regression (Multiclass)","967c0d16":"## Pairplot for all the features in the dataset for better understanding of how all features line up for for different price_ranges","e7429669":"## weighted f1 score on test data","27385226":"## Since the count of target variables are equal, we don't do Stratified splits","f000f84d":"## Using GridSearchCV for logistic regression since f1 scoring was maximum ","e75fc8e1":"## After Analysing the Pairplot, plotting only columns with clear sepration of different target classes(price_range)","bdd94602":"## Important features using RandomForest","b474a3e9":"## Checking out cross validation score with LogisticRegression,DecisionTree,Randomforest and Support Vector Classifier for Multiclass classification using pipeline"}}