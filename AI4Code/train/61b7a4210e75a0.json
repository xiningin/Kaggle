{"cell_type":{"0bf1012a":"code","cfba6ef1":"code","a3215af4":"code","7fc5c114":"code","e79bc9d3":"code","8ac394cd":"code","98ec5762":"code","38cd05fe":"code","59a9ec75":"code","3d1a0953":"code","4193d62b":"code","af3f7656":"code","b0aa94da":"code","8570e0e5":"code","4b32b87e":"code","b0cd134e":"code","42c7086d":"code","3c7795ba":"code","4be9e0fd":"code","9288079e":"code","66e899a7":"code","ea61555e":"code","60b7563e":"code","571b2c85":"code","16c14fb8":"code","7ca862cd":"code","c6e60207":"code","7323c89c":"code","0da00614":"code","b88bc22d":"code","ea88ac01":"code","8507aaf0":"code","90de4e84":"code","c6c7fe01":"code","92573b94":"code","282df67a":"code","e73a102e":"code","70d7319c":"code","55a591a0":"code","bb14590f":"code","ec950a21":"code","891c8dc0":"code","0950aae6":"code","5bbc33a5":"code","562c6a7e":"code","93fea0a9":"code","07af4f06":"markdown","d57297e2":"markdown","8801dbba":"markdown"},"source":{"0bf1012a":"#internet to be turned on\n#import nltk\n#nltk.download('punkt')\n#nltk.download('stopwords')\n#nltk.download('vader_lexicon')\n#pip install emoji\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nimport pandas as pd\nimport numpy  as np\nimport nltk\nfrom datetime import datetime, timedelta\nimport string\nimport emoji\nimport re\nimport seaborn as sns\nsns.set(style='darkgrid')\nfrom wordcloud import WordCloud\nfrom matplotlib import pyplot as plt\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import sent_tokenize\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer","cfba6ef1":"data = pd.read_csv(\"..\/input\/newnlp\/newnlp.csv\")\ndata = data[data['Name']!='Chayadeep']\ndata['Gender'][data['Gender']=='f'] = 0\ndata['Gender'][data['Gender']=='m'] = 1  \ndata['Gender'] = data['Gender'].astype(int)\n\ndname = data.groupby(['Name'])['DaysTexted','DaysBeen','ActiveDays','Gender'].mean()\ndname['Consistency'] = 100*dname['DaysTexted']\/dname['DaysBeen']\ndname['TimesTexted'] = data.groupby(['Name'])['Name'].count()\ndname['Frequentness'] = 10*dname['TimesTexted']\/dname['DaysBeen']\ndname['Agressiveness'] = 10*dname['TimesTexted']\/dname['ActiveDays']\n#dname.sort_values(by='Consistency',ascending=False)\n#dname.sort_values(by='TimesTexted',ascending=False)\nprint('\ud83d\ude0e '+'Aman Jain')","a3215af4":"dname","7fc5c114":"print('Top 10 Popular folks \\n(TimesTexted)')\ndname.sort_values(by='TimesTexted',ascending=False).head(10).iloc[:,5:6]","e79bc9d3":"print('Top 10 Consistent folks \\n(DaysTexted\/DaysBeen)')\ndname.sort_values(by='Consistency',ascending=False).head(10).iloc[:,4:5]","8ac394cd":"print('Top 10 most Frequently texting folks \\n(TimesTexted\/DaysBeen)')\ndname.sort_values(by='Frequentness',ascending=False).head(10).iloc[:,6:7]","98ec5762":"names = pd.DataFrame(data.groupby(['Name']).size(),columns={'TextTimes'}).reset_index()\nnames['avgWordspertext'] = 0\nnames['minWordspertext'] = 0\nnames['maxWordspertext'] = 0\nnames['evocab'] = 0\nnames['totalemojis'] = 0\nnames['top5emojis'] = 0\nnames['vocab'] = 0\nnames['top5words'] = 0\ndef nltk_sentiment(sentence):\n      nltk_sentiment = SentimentIntensityAnalyzer()\n      score = nltk_sentiment.polarity_scores(sentence)\n      return score\ndef getResult(pos, neu, neg):\n    if (pos > neu and pos > neg):\n        return (\"Positive\")\n    elif (neg > neu and neg > pos):\n        return (\"Negative\")\n    else:\n        return('Neutral')\n\ndfHFreqs = pd.DataFrame(data.groupby(['Hour'])['Hour'].count())\ndfHFreqs.columns = ['Group']\ndfHFreqs = dfHFreqs.reset_index()\ndfHFreqs.columns = ['Hour','Group']\ns = dfHFreqs['Group'].sum()\ndfHFreqs['Group'] = 100*dfHFreqs['Group']\/s\n\nnames['Pos'] = 0\nnames['Neu'] = 0\nnames['Neg'] = 0\nnames['avgTime'] = 0\nstop = stopwords.words('english')\nvStopWords = [\"thats\",\"dont\",\"also\",\"like\",\"https\",\"from\",\"all\",\"also\",\"and\",\"any\",\"are\",\"but\",\"can\",\"cant\",\"cry\",\"due\",\"etc\",\"few\",\"for\",\"get\",\"had\",\"has\",\"hasnt\",\"have\",\"her\",\"here\",\"hers\",\"herself\",\"him\",\"himself\",\"his\",\"how\",\"inc\",\"into\",\"its\",\"ltd\",\"may\",\"nor\",\"not\",\"now\",\"off\",\"once\",\"one\",\"only\",\"onto\",\"our\",\"ours\",\"out\",\"over\",\"own\",\"part\",\"per\",\"put\",\"see\",\"seem\",\"she\",\"than\",\"that\",\"the\",\"their\",\"them\",\"then\",\"thence\",\"there\",\"these\",\"they\",\"this\",\"those\",\"though\",\"thus\",\"too\",\"top\",\"upon\",\"very\",\"via\",\"was\",\"were\",\"what\",\"when\",\"which\",\"while\",\"who\",\"whoever\",\"whom\",\"whose\",\"why\",\"will\",\"with\",\"within\",\"without\",\"would\",\"yet\",\"you\",\"your\",\"yours\",\"the\"]\nstop = stop + vStopWords\nylabel='% of total time spent'\nxlabel='Time clock in 24 hours'","38cd05fe":"for name in names['Name']:\n  data1 = data[data['Name']==name]\n  dstr = ' '.join(data1['Text'])\n  dlist = data1['Text'].to_list()\n  \n  #names['avgTime'][names['Name']==name] = data1['Time'].mean().strftime(\"%I:%M %p\")\n\n  L1 = []\n  for l in dlist:\n    L1.append(len(l.split()))\n  names['avgWordspertext'][names['Name']==name] = np.mean(L1)\n  names['minWordspertext'][names['Name']==name] = np.min(L1)\n  names['maxWordspertext'][names['Name']==name] = np.max(L1)\n  \n  LE = []\n  LE = [c for c in dstr if c in emoji.UNICODE_EMOJI]\n  dfE = pd.DataFrame({'Emoji':LE})\n  dfEFreqs = pd.DataFrame(dfE.groupby(['Emoji'])['Emoji'].count())\n  dfEFreqs.columns = ['Freq']\n  dfEFreqs = dfEFreqs.reset_index()\n  dfEFreqs.columns = ['Emoji','Freq']\n  names['evocab'][names['Name']==name] = len(dfEFreqs)\n  names['totalemojis'][names['Name']==name] = dfEFreqs['Freq'].sum()\n  dfEFreqs = dfEFreqs.sort_values('Freq',ascending=False)\n  names['top5emojis'][names['Name']==name] = ' '.join(dfEFreqs['Emoji'][0:5])\n\n  demoji = dstr.encode('ascii', 'ignore').decode('ascii')\n  demoji = re.sub(r'[`!?~@#$%^&*()_+-=<>,.:;]', '', demoji)\n  demoji = re.sub(r'[\u2013]', '', demoji)\n  demoji = re.sub(r'[\\[\\]\\(\\)\\{\\}]', '', demoji)\n  demoji = re.sub(r'[\\t\\\"\\'\\\/\\\\]', '', demoji)\n  lstAllWords = demoji.split()\n\n  lstTmpWords=[]\n  for strWord in lstAllWords:\n      if len(strWord)>3:\n          lstTmpWords.append(strWord)\n  lstAllWords = lstTmpWords\n  del lstTmpWords\n\n  for i in range(0,len(lstAllWords)):\n      lstAllWords[i] = str.lower(lstAllWords[i])\n\n  dfWords = pd.DataFrame({'Words':lstAllWords})\n  dfWords = dfWords[-dfWords['Words'].isin(stop)]\n  dfWords = dfWords[-dfWords['Words'].isin(emoji.UNICODE_EMOJI.keys())]\n\n  dfFreqs = pd.DataFrame(dfWords.groupby(['Words'])['Words'].count())\n  dfFreqs.columns = ['Freq']\n  dfFreqs = dfFreqs.reset_index()\n  dfFreqs.columns = ['Word','Freq']\n  names['vocab'][names['Name']==name] = len(dfFreqs)\n  dfFreqs = dfFreqs.sort_values('Freq',ascending=False)\n  names['top5words'][names['Name']==name] = ' '.join(dfFreqs['Word'][0:5])\n\n  print('\ud83d\ude0e '+name)\n  d = {}\n  for a, x in dfFreqs[0:10].values:\n      d[a] = x \n  wordcloud = WordCloud(background_color=\"white\")\n  wordcloud.generate_from_frequencies(frequencies=d)\n  plt.figure()\n  plt.imshow(wordcloud, interpolation=\"bilinear\")\n  plt.axis(\"off\")\n  plt.show()\n\n  lstLines = sent_tokenize(dstr)\n  lstLines = [t.lower() for t in lstLines]\n  lstLines = [t.translate(str.maketrans('','',string.punctuation)) for t in lstLines]\n  saResults = [nltk_sentiment(t) for t in lstLines]\n  # create dataframe\n  df = pd.DataFrame(lstLines, columns=['Lines'])\n  df['Pos']=[t['pos'] for t in saResults]\n  df['Neu']=[t['neu'] for t in saResults]\n  df['Neg']=[t['neg'] for t in saResults]\n  #df['Result']= [getResult(t['pos'],t['neu'],t['neg']) for t in saResults]\n  names['Pos'][names['Name']==name] = df['Pos'].mean()\n  names['Neu'][names['Name']==name] = df['Neu'].mean()\n  names['Neg'][names['Name']==name] = df['Neg'].mean()  \n    \n","59a9ec75":"names = names.set_index('Name')","3d1a0953":"print('Top 10 Emoji-using folks')\nnames.sort_values(by='totalemojis',ascending=False).head(10).iloc[:,5:6]","4193d62b":"print('Top 10 follks with hishest Emoji vocab')\nnames.sort_values(by='evocab',ascending=False).head(10).iloc[:,4:5]","af3f7656":"print('Top 10 folks who write LONG texts')\npcs = names[names['TextTimes']>40]\npcs.sort_values(by='avgWordspertext',ascending=False).head(10).iloc[:,1:2]","b0aa94da":"print('Top 10 folks who write SHORT texts')\npcs = names[names['TextTimes']>40]\npcs.sort_values(by='avgWordspertext',ascending=True).head(10).iloc[:,1:2]","8570e0e5":"print('Top 10 Positively texting folks')\npcs = names[names['TextTimes']>30]\npcs.sort_values(by='Pos',ascending=False).head(10).iloc[:,9:10]","4b32b87e":"print('Top 10 Neutraly texting folks')\npcs = names[names['TextTimes']>30]\npcs.sort_values(by='Neu',ascending=False).head(10).iloc[:,10:11]","b0cd134e":"print('Top 10 folks with diverse vocab \\n(maximum different words)')\nnames.sort_values(by='vocab',ascending=False).head(10).iloc[:,7:8]","42c7086d":"print('Top 5 emojis most used \\nby Top 10 popular folks')\nnames.sort_values(by='TextTimes',ascending=False).head(10).iloc[:,6:7]","3c7795ba":"names['Frequentness'] = dname['Frequentness']\nprint('Top 5 most used words by Top 10 frequently texting folks')\nnames.sort_values(by='Frequentness',ascending=False).head(10).iloc[:,8:9]","4be9e0fd":"names['Gender'] = dname['Gender']","9288079e":"print('Number of Men and Women in the group')\npcs = names.groupby(['Gender']).size()\nprint('Women:'+str(pcs[0])+'\\nMen:'+str(pcs[1]))","66e899a7":"print('Top 10 Popular Women (TimesTexted)')\npcs = dname[dname['Gender']==0]\npcs.sort_values(by='TimesTexted',ascending=False).head(10).iloc[:,5:6]","ea61555e":"print('Top 10 Consistent Women \\n(DaysTexted\/DaysBeen)')\npcs = dname[dname['Gender']==0]\npcs.sort_values(by='Consistency',ascending=False).head(10).iloc[:,4:5]\n","60b7563e":"print('Top 10 most Frequently texting Women\\n (TimesTexted\/DaysBeen)')\npcs = dname[dname['Gender']==0]\npcs.sort_values(by='Frequentness',ascending=False).head(10).iloc[:,6:7]\n","571b2c85":"print('Top 10 most Emoji-using Women')\npcs = names[names['Gender']==0]\npcs.sort_values(by='totalemojis',ascending=False).head(10).iloc[:,5:6]","16c14fb8":"print('Top 10 Women with hishest Emoji vocab')\npcs = names[names['Gender']==0]\npcs.sort_values(by='evocab',ascending=False).head(10).iloc[:,4:5]\n","7ca862cd":"print('Top 10 Women who write LONG texts')\npcs = names[names['Gender']==0]\npcs = pcs[pcs['TextTimes']>40]\npcs.sort_values(by='avgWordspertext',ascending=False).head(10).iloc[:,1:2]\n","c6e60207":"print('Top 10 Women who write SHORT texts')\npcs = names[names['Gender']==0]\npcs = pcs[pcs['TextTimes']>40]\npcs.sort_values(by='avgWordspertext',ascending=True).head(10).iloc[:,1:2]\n","7323c89c":"print('Top 10 Positively texting women')\npcs = names[names['Gender']==0]\npcs = pcs[pcs['TextTimes']>30]\npcs.sort_values(by='Pos',ascending=False).head(10).iloc[:,9:10]","0da00614":"print('Top 10 Neutraly texting Women')\npcs = names[names['Gender']==0]\npcs = pcs[pcs['TextTimes']>30]\npcs.sort_values(by='Neu',ascending=False).head(10).iloc[:,10:11]\n","b88bc22d":"print('Top 10 Women with diverse vocab \\n(maximum different words)')\npcs = names[names['Gender']==0]\npcs.sort_values(by='vocab',ascending=False).head(10).iloc[:,7:8]","ea88ac01":"print('Top 5 emojis most used by Top 10 popular Women')\npcs = names[names['Gender']==0]\npcs.sort_values(by='TextTimes',ascending=False).head(10).iloc[:,6:7]","8507aaf0":"print('Top 5 most used words by Top 10 frequently texting Women')\npcs = names[names['Gender']==0]\npcs.sort_values(by='Frequentness',ascending=False).head(10).iloc[:,8:9]","90de4e84":"for name in names.index:\n    data1 = data[data['Name']==name]\n    dstr = ' '.join(data1['Text'])\n    dlist = data1['Text'].to_list()\n\n    dfHPFreqs = pd.DataFrame(data[data['Name']==name].groupby(['Hour'])['Hour'].count())\n    dfHPFreqs.columns = [name]\n    dfHPFreqs = dfHPFreqs.reset_index()\n    dfHPFreqs.columns = ['Hour',name]\n    s = dfHPFreqs[name].sum()\n    dfHPFreqs[name] = 100*dfHPFreqs[name]\/s\n    dfC = pd.merge(dfHFreqs,dfHPFreqs,how='left')\n    dfC = dfC.fillna(0)\n    title = 'Texting Pattern: \ud83d\ude0e'+name\n    plt.figure()\n    ax = dfC.iloc[:, 2].plot(legend=True,figsize=(12,6),title=title,color='r')\n    dfC.iloc[:,1].plot(legend=True,figsize=(12,6),color='y')\n    ax.autoscale(axis='x',tight=True)\n    ax.set(xlabel=xlabel, ylabel=ylabel);","c6c7fe01":"dstr = ' '.join(data['Text'])\ndlist = data['Text'].to_list()\nL1 = []\nfor l in dlist:\n  L1.append(len(l.split()))\nLE = []\nLE = [c for c in dstr if c in emoji.UNICODE_EMOJI]\ndfE = pd.DataFrame({'Emoji':LE})\ndfEFreqs = pd.DataFrame(dfE.groupby(['Emoji'])['Emoji'].count())\ndfEFreqs.columns = ['Freq']\ndfEFreqs = dfEFreqs.reset_index()\ndfEFreqs.columns = ['Emoji','Freq']\nevocab = len(dfEFreqs)\ntotalemojis = dfEFreqs['Freq'].sum()\ndfEFreqs = dfEFreqs.sort_values('Freq',ascending=False)\ntop5emojis =  ' '.join(dfEFreqs['Emoji'][0:5])","92573b94":"print('Group Emoji Vocab: '+str(evocab)+'\\n(different emojis used)') ","282df67a":"print('Total Emojis used in the group: '+str(totalemojis)) ","e73a102e":"print('Top 10 emojis used in the group')\ndfEFreqs.set_index('Emoji').head(10)","70d7319c":"demoji = dstr.encode('ascii', 'ignore').decode('ascii')\ndemoji = re.sub(r'[`!?~@#$%^&*()_+-=<>,.:;]', '', demoji)\ndemoji = re.sub(r'[\u2013]', '', demoji)\ndemoji = re.sub(r'[\\[\\]\\(\\)\\{\\}]', '', demoji)\ndemoji = re.sub(r'[\\t\\\"\\'\\\/\\\\]', '', demoji)\nlstAllWords = demoji.split()\ntotalwords = len(lstAllWords)\nlstTmpWords=[]\nfor strWord in lstAllWords:\n    if len(strWord)>3:\n        lstTmpWords.append(strWord)\nlstAllWords = lstTmpWords\ndel lstTmpWords\n\nfor i in range(0,len(lstAllWords)):\n    lstAllWords[i] = str.lower(lstAllWords[i])\n\ndfWords = pd.DataFrame({'Words':lstAllWords})\ndfWords = dfWords[-dfWords['Words'].isin(stop)]\ndfWords = dfWords[-dfWords['Words'].isin(emoji.UNICODE_EMOJI.keys())]\n\ndfFreqs = pd.DataFrame(dfWords.groupby(['Words'])['Words'].count())\ndfFreqs.columns = ['Freq']\ndfFreqs = dfFreqs.reset_index()\ndfFreqs.columns = ['Word','Freq']\nvocab = len(dfFreqs)\n#totalwords = dfFreqs['Freq'].sum()\ndfFreqs = dfFreqs.sort_values('Freq',ascending=False)\n#names['top5words'][names['Name']==name] = ' '.join(dfFreqs['Word'][0:10])","55a591a0":"print('Group Vocab: '+str(vocab)+'\\n(different words used)') ","bb14590f":"print('Total Words used in the group: '+str(totalwords)) ","ec950a21":"#100*evocab\/vocab\nprint(str(int(np.round(100*totalemojis\/totalwords)))+'% of words used were emojis')","891c8dc0":"print('Top 10 words used in the group')\ndfFreqs.set_index('Word').head(10)","0950aae6":"d = {}\nfor a, x in dfFreqs[0:10].values:\n    d[a] = x \nwordcloud = WordCloud(background_color=\"white\")\nwordcloud.generate_from_frequencies(frequencies=d)\nplt.figure()\nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis(\"off\")\nplt.show()\n","5bbc33a5":"lstLines = sent_tokenize(dstr)\nlstLines = [t.lower() for t in lstLines]\nlstLines = [t.translate(str.maketrans('','',string.punctuation)) for t in lstLines]\nsaResults = [nltk_sentiment(t) for t in lstLines]\n# create dataframe\ndf = pd.DataFrame(lstLines, columns=['Lines'])\ndf['Pos']=[t['pos'] for t in saResults]\ndf['Neu']=[t['neu'] for t in saResults]\ndf['Neg']=[t['neg'] for t in saResults]\n#df['Result']= [getResult(t['pos'],t['neu'],t['neg']) for t in saResults]\npos = df['Pos'].mean()\nneu = df['Neu'].mean()\nneg = df['Neg'].mean()  ","562c6a7e":"print('Sentiments of the group')\nprint(str(round(pos*100,2))+'% Positive')\nprint(str(round(neu*100,2))+'% Neutral')\nprint(str(round(neg*100,2))+'% Negative')","93fea0a9":"dfHFreqs = dfHFreqs.set_index('Hour')\ntitle='Group Texting Pattern'\nplt.figure()\nax = dfHFreqs.plot(legend=True,figsize=(12,6),title=title,color='r')\nax.autoscale(axis='x',tight=True)\nax.set(xlabel=xlabel, ylabel=ylabel);","07af4f06":"**Purdue MS BAIM 2020-21 Chat Analysis**\n(Dec 9, 2019 - Mar 21, 2020)\n\n*   Done by **Aman Jain**\n*   **[ LinkedIn](https:\/\/www.linkedin.com\/in\/amanacden\/)**   **[Instagram](http:\/\/instagram.com\/amanacden)**","d57297e2":"On scrolling down, you will find:\n\n**1. For Everyone**\n* Top 10 Favorite words (wordcloud)\n* Texting Pattern during 24 hours (compared with group) \n\n\n**2. Top 10 folks** \n* Top 10 Popular folks\n* Top 10 most Emoji-using folks\n* Top 10 folks who write LONG texts\n* Top 10 folks who write SHORT texts\n* Top 10 Consistent folks\n* Top 10 most Frequently texting folks\n* Top 10 follks with hishest Emoji vocab\n* Top 10 Positively texting folks\n* Top 10 Neutraly texting folks\n* Top 10 folks with diverse vocab\n* Top 5 emojis most used by Top 10 popular folks\n* Top 5 most used words by Top 10 frequently texting folks\n\n\n*As Women as not even half of Men, here goes one exclusively for Women*\n\n**3. Top 10 Women**\n* Top 10 Popular Women \n* Top 10 Women who write SHORT texts\n* Top 5 emojis most used by Top 10 popular women\n* Top 10 Women with diverse vocab\n* Top 10 Positively texting Women\n* Top 10 Consistent women\n* Top 10 most Frequently texting women\n* Top 10 Women with hishest Emoji vocab\n* Top 10 Women who write LONG texts\n* Top 10 Neutraly texting Women\n* Top 5 most used words by Top 10 frequently texting women\n\n\n**4. Group Characteristics** \n\n* Group Texting Pattern\n* Top 10 words used in the group\n* Top 10 emojis used in the group\n\n\n","8801dbba":"**Top 10 favorite words of everyone**"}}