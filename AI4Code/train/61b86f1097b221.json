{"cell_type":{"f3a505e1":"code","fcb3b7ec":"code","8148f728":"code","58cf4d01":"code","b95bbd99":"code","4e06ee15":"code","99709b2b":"code","44fcc299":"code","4446c0d1":"code","49886f9e":"code","6cbdaa43":"code","55cb6496":"code","dadc7227":"code","71b4d3a3":"code","a11e36f1":"code","3480cb2f":"code","6dbe2483":"code","31bed43d":"code","32d90340":"code","c4af58c2":"code","5268c236":"code","d27f8c84":"code","d86261e8":"code","756d171d":"code","a1c0f27d":"code","c8e8ae99":"code","83e9e07d":"code","5d389c3e":"code","f992ea4f":"code","332ca127":"code","1714184c":"code","3d9f007e":"code","4dfa9fca":"code","3cae9329":"code","2fa98068":"code","876570d8":"code","f5580850":"code","be979fc4":"markdown","077de7cb":"markdown","0c656977":"markdown","9acc8a69":"markdown","06fde7c2":"markdown","659143a3":"markdown","1d487e31":"markdown","bce50cff":"markdown","e99cd6e4":"markdown","ed99dae7":"markdown","f9b5be72":"markdown"},"source":{"f3a505e1":"import os\nimport json\nimport ast\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport typing\nfrom typing import Any, Tuple\nimport tensorflow as tf\nfrom tensorflow.keras.layers import TextVectorization\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","fcb3b7ec":"def read_data(file_path, sample_num=0):\n    questions = []\n    answers = []\n    with open(file_path) as json_file:\n        for line in json_file:\n            data = ast.literal_eval(line)\n            questions.append(data['question'])\n            answers.append(data['answer'])\n    if sample_num == 0:\n        return questions, answers\n    else:\n        # For fast training time, I will take a number of sample from the full dataset\n        return questions[:sample_num], answers[:sample_num]","8148f728":"file_path = '\/kaggle\/input\/amazon-qa-for-electronic-products\/qa_Electronics.json'\nquestions, answers = read_data(file_path, 2000)","58cf4d01":"questions[:5]","b95bbd99":"answers[:5]","4e06ee15":"BATCH_SIZE=64","99709b2b":"dataset = tf.data.Dataset.from_tensor_slices((questions, answers)).batch(BATCH_SIZE)","44fcc299":"def custom_standardise(text):\n  text = tf.strings.lower(text)\n  # Keep space, a to z, and select punctuation.\n  text = tf.strings.regex_replace(text, '[^ a-z.?!,]', '')\n  # Add spaces around punctuation.\n  text = tf.strings.regex_replace(text, '[.?!,]', r' \\0 ')\n  # Strip whitespace.\n  text = tf.strings.strip(text)\n  # Add token to each sentence.  \n  text = tf.strings.join(['<START>', text, '<END>'], separator=' ')\n  return text","4446c0d1":"#Limit the size of vocabularies to 10000\nmax_vocab_size = 10000\ntext_processor = TextVectorization(\n    standardize=custom_standardise,\n    max_tokens=max_vocab_size)","49886f9e":"text_processor.adapt(questions + answers)","6cbdaa43":"text_processor.get_vocabulary()[:10]","55cb6496":"vocabs = np.array(text_processor.get_vocabulary())","dadc7227":"def token2sent(tokens):\n  return ' '.join(vocabs[tokens.numpy()])","71b4d3a3":"# Print a sentence after preprocessing\nfor example_input_batch, example_target_batch in dataset.take(1):\n  tokens = text_processor(example_input_batch)[0]\n  print(token2sent(tokens))\n  break","a11e36f1":"embedding_dim = 256\nunits = 1024","3480cb2f":"class Encoder(tf.keras.layers.Layer):\n  def __init__(self, input_vocab_size, embedding_dim, enc_units):\n    super(Encoder, self).__init__()\n    self.enc_units = enc_units\n    self.input_vocab_size = input_vocab_size\n\n    # The embedding layer converts tokens to vectors\n    self.embedding = tf.keras.layers.Embedding(self.input_vocab_size,\n                                               embedding_dim)\n\n    # The GRU RNN layer processes those vectors sequentially.\n    self.gru = tf.keras.layers.GRU(self.enc_units,\n                                   # Return the sequence and state\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n\n  def call(self, tokens, state=None):\n    vectors = self.embedding(tokens)\n    output, state = self.gru(vectors, initial_state=state)\n    return output, state","6dbe2483":"# Check how the encoder works\nexample_tokens = text_processor(example_input_batch)\n\nencoder = Encoder(text_processor.vocabulary_size(),\n                  embedding_dim, units)\nexample_enc_output, example_enc_state = encoder(example_tokens)\n\nprint(f'Input batch, shape (batch): {example_input_batch.shape}')\nprint(f'Input batch tokens, shape (batch, s): {example_tokens.shape}')\nprint(f'Encoder output, shape (batch, s, units): {example_enc_output.shape}')\nprint(f'Encoder state, shape (batch, units): {example_enc_state.shape}')","31bed43d":"class LuongAttention(tf.keras.layers.Layer):\n  def __init__(self, units):\n    super().__init__()\n    self.W = tf.keras.layers.Dense(units, use_bias=False)\n    self.attention = tf.keras.layers.Attention()\n\n  def call(self, query, value, mask):\n\n    w_query = self.W(query)\n    query_mask = tf.ones(tf.shape(query)[:-1], dtype=bool)\n    value_mask = mask\n\n    context_vector, attention_weights = self.attention(\n        inputs = [w_query, value],\n        mask=[query_mask, value_mask],\n        return_attention_scores = True)\n    return context_vector, attention_weights","32d90340":"attention_layer = LuongAttention(units)","c4af58c2":"# Later, the decoder will generate this attention query\nexample_attention_query = tf.random.normal(shape=[len(example_tokens), 2, 10])\n\n# Attend to the encoded tokens\n\ncontext_vector, attention_weights = attention_layer(\n    query=example_attention_query,\n    value=example_enc_output,\n    mask=(example_tokens != 0))\n\nprint(f'Attention result shape: (batch_size, query_seq_length, units):           {context_vector.shape}')\nprint(f'Attention weights shape: (batch_size, query_seq_length, value_seq_length): {attention_weights.shape}')","5268c236":"# Declare decoder input and output for the decoder\nclass DecoderInput(typing.NamedTuple):\n  new_tokens: Any\n  enc_output: Any\n  mask: Any\n\nclass DecoderOutput(typing.NamedTuple):\n  logits: Any\n  attention_weights: Any","d27f8c84":"class Decoder(tf.keras.layers.Layer):\n  def __init__(self, output_vocab_size, embedding_dim, dec_units):\n    super(Decoder, self).__init__()\n    self.dec_units = dec_units\n    self.output_vocab_size = output_vocab_size\n    self.embedding_dim = embedding_dim\n\n   # The embedding layer convets token IDs to vectors\n    self.embedding = tf.keras.layers.Embedding(self.output_vocab_size,\n                                               embedding_dim)\n\n   # The RNN keeps track of what's been generated so far.\n    self.gru = tf.keras.layers.GRU(self.dec_units,\n                                   return_sequences=True,\n                                   return_state=True,\n                                   recurrent_initializer='glorot_uniform')\n\n    # The RNN output will be the query for the attention layer.\n    self.attention = LuongAttention(self.dec_units)\n\n    #  converting `ct` to `at`\n    self.Wc = tf.keras.layers.Dense(dec_units, activation=tf.math.tanh,\n                                    use_bias=False)\n\n    # This fully connected layer produces the logits for each output token.\n    self.fc = tf.keras.layers.Dense(self.output_vocab_size)\n\n  def call(self, inputs: DecoderInput, state=None) -> Tuple[DecoderOutput, tf.Tensor]:\n    # Lookup the embeddings\n    vectors = self.embedding(inputs.new_tokens)\n\n    # Process one step with the RNN\n    rnn_output, state = self.gru(vectors, initial_state=state)\n\n    # Use the RNN output as the query for the attention over the encoder output.\n    context_vector, attention_weights = self.attention(\n        query=rnn_output, value=inputs.enc_output, mask=inputs.mask)\n\n    # Join the context_vector and rnn_output\n    #     [ct; ht] shape: (batch t, value_units + query_units)\n    context_and_rnn_output = tf.concat([context_vector, rnn_output], axis=-1)\n\n    # `at = tanh(Wc@[ct; ht])`\n    attention_vector = self.Wc(context_and_rnn_output)\n\n    # Generate logit predictions:\n    logits = self.fc(attention_vector)\n\n    return DecoderOutput(logits, attention_weights), state","d86261e8":"# See how the decoder works\ndecoder = Decoder(text_processor.vocabulary_size(), embedding_dim, units)\n# Convert the target sequence, and collect the \"<START>\" tokens\nexample_output_tokens = text_processor(example_target_batch)\n\nstart_index = text_processor.get_vocabulary().index('<START>')\nfirst_token = tf.constant([[start_index]] * example_output_tokens.shape[0])\n# Run the decoder\ndec_result, dec_state = decoder(\n    inputs = DecoderInput(new_tokens=first_token,\n                          enc_output=example_enc_output,\n                          mask=(example_tokens != 0)),\n    state = example_enc_state\n)\n\nprint(f'logits shape: (batch_size, t, output_vocab_size) {dec_result.logits.shape}')\nprint(f'state shape: (batch_size, dec_units) {dec_state.shape}')","756d171d":"sampled_token = tf.random.categorical(dec_result.logits[:, 0, :], num_samples=1)\nvocab = np.array(text_processor.get_vocabulary())\nfirst_word = vocab[sampled_token.numpy()]\nfirst_word[:5]","a1c0f27d":"class MaskedLoss(tf.keras.losses.Loss):\n  def __init__(self):\n    self.name = 'masked_loss'\n    self.loss = tf.keras.losses.SparseCategoricalCrossentropy(\n        from_logits=True, reduction='none')\n\n  def __call__(self, y_true, y_pred):\n    # Calculate the loss for each item in the batch.\n    loss = self.loss(y_true, y_pred)\n\n    # Mask off the losses on padding.\n    mask = tf.cast(y_true != 0, tf.float32)\n    loss *= mask\n\n    # Return the total.\n    return tf.reduce_sum(loss)","c8e8ae99":"class TrainTranslator(tf.keras.Model):\n  def __init__(self, embedding_dim, units,\n               text_processor):\n    super().__init__()\n    # Build the encoder and decoder\n    encoder = Encoder(text_processor.vocabulary_size(),\n                      embedding_dim, units)\n    decoder = Decoder(text_processor.vocabulary_size(),\n                      embedding_dim, units)\n\n    self.encoder = encoder\n    self.decoder = decoder\n    self.text_processor = text_processor\n\n  def _preprocess(self, input_text, target_text):\n    # Convert the text to token IDs\n    input_tokens = self.text_processor(input_text)\n    target_tokens = self.text_processor(target_text)\n    # Convert IDs to masks.\n    input_mask = input_tokens != 0\n    target_mask = target_tokens != 0\n    return input_tokens, input_mask, target_tokens, target_mask\n\n  @tf.function(input_signature=[[tf.TensorSpec(dtype=tf.string, shape=[None]),\n                               tf.TensorSpec(dtype=tf.string, shape=[None])]])\n  def train_step(self, inputs):\n    input_text, target_text = inputs  \n    (input_tokens, input_mask,\n    target_tokens, target_mask) = self._preprocess(input_text, target_text)\n    max_target_length = tf.shape(target_tokens)[1]\n    with tf.GradientTape() as tape:\n      # Encode the input\n      enc_output, enc_state = self.encoder(input_tokens)\n      # Initialize the decoder's state to the encoder's final state.\n      # This only works if the encoder and decoder have the same number of\n      # units.\n      dec_state = enc_state\n      loss = tf.constant(0.0)\n      for t in tf.range(max_target_length-1):\n        # Pass in two tokens from the target sequence:\n        # 1. The current input to the decoder.\n        # 2. The target the target for the decoder's next prediction.\n        new_tokens = target_tokens[:, t:t+2]\n        step_loss, dec_state = self._loop_step(new_tokens, input_mask,\n                                              enc_output, dec_state)\n        loss = loss + step_loss\n\n      # Average the loss over all non padding tokens.\n      average_loss = loss \/ tf.reduce_sum(tf.cast(target_mask, tf.float32))\n\n    # Apply an optimization step\n    variables = self.trainable_variables \n    gradients = tape.gradient(average_loss, variables)\n    self.optimizer.apply_gradients(zip(gradients, variables))\n\n    # Return a dict mapping metric names to current value\n    return {'batch_loss': average_loss}\n  \n  def _loop_step(self, new_tokens, input_mask, enc_output, dec_state):\n    input_token, target_token = new_tokens[:, 0:1], new_tokens[:, 1:2]\n\n    # Run the decoder one step.\n    decoder_input = DecoderInput(new_tokens=input_token,\n                                enc_output=enc_output,\n                                mask=input_mask)\n\n    dec_result, dec_state = self.decoder(decoder_input, state=dec_state)\n\n    # `self.loss` returns the total for non-padded tokens\n    y = target_token\n    y_pred = dec_result.logits\n    step_loss = self.loss(y, y_pred)\n\n    return step_loss, dec_state","83e9e07d":"train_translator = TrainTranslator(\n    embedding_dim, units,\n    text_processor=text_processor)\n\n# Configure the loss and optimizer\ntrain_translator.compile(\n    optimizer=tf.optimizers.Adam(),\n    loss=MaskedLoss(),\n)","5d389c3e":"class BatchLogs(tf.keras.callbacks.Callback):\n  def __init__(self, key):\n    self.key = key\n    self.logs = []\n\n  def on_train_batch_end(self, n, logs):\n    self.logs.append(logs[self.key])\n\nbatch_loss = BatchLogs('batch_loss')","f992ea4f":"hist = train_translator.fit(dataset, epochs=50,\n                     callbacks=[batch_loss])","332ca127":"plt.plot(batch_loss.logs)\nplt.ylim([0, 30])\nplt.xlabel('Batch #')\nplt.ylabel('CE\/token')","1714184c":"class Translator(tf.Module):\n  def __init__(self, encoder, decoder, text_processor):\n    self.encoder = encoder\n    self.decoder = decoder\n    self.text_processor = text_processor\n    self.output_token_string_from_index = (\n        tf.keras.layers.StringLookup(\n            vocabulary=text_processor.get_vocabulary(),\n            mask_token='',\n            invert=True))\n\n    # The output should never generate padding, unknown, or start.\n    index_from_string = tf.keras.layers.StringLookup(\n        vocabulary=text_processor.get_vocabulary(), mask_token='')\n    token_mask_ids = index_from_string(['', '[UNK]', '<START>']).numpy()\n\n    token_mask = np.zeros([index_from_string.vocabulary_size()], dtype=np.bool)\n    token_mask[np.array(token_mask_ids)] = True\n    self.token_mask = token_mask\n\n    self.start_token = index_from_string(tf.constant('<START>'))\n    self.end_token = index_from_string(tf.constant('<END>'))\n\n  def tokens_to_text(self, result_tokens):\n    result_text_tokens = self.output_token_string_from_index(result_tokens)\n    result_text = tf.strings.reduce_join(result_text_tokens,\n                                        axis=1, separator=' ')\n    result_text = tf.strings.strip(result_text)\n    return result_text\n  \n  def sample(self, logits, temperature):\n    token_mask = self.token_mask[tf.newaxis, tf.newaxis, :]\n    # Set the logits for all masked tokens to -inf, so they are never chosen.\n    logits = tf.where(self.token_mask, -np.inf, logits)\n\n    if temperature == 0.0:\n      new_tokens = tf.argmax(logits, axis=-1)\n    else: \n      logits = tf.squeeze(logits, axis=1)\n      new_tokens = tf.random.categorical(logits\/temperature,\n                                          num_samples=1)\n    return new_tokens","3d9f007e":"def translate(self, input_text,\n                       *,\n                       max_length=50,\n                       return_attention=True,\n                       temperature=1.0):\n\n  batch_size = tf.shape(input_text)[0]\n\n  # Encode the input\n  input_tokens = self.text_processor(input_text)\n\n\n  enc_output, enc_state = self.encoder(input_tokens)\n\n\n  # Initialize the decoder\n  dec_state = enc_state\n  new_tokens = tf.fill([batch_size, 1], self.start_token)\n\n\n  # Initialize the accumulators\n  result_tokens = tf.TensorArray(tf.int64, size=1, dynamic_size=True)\n  attention = tf.TensorArray(tf.float32, size=1, dynamic_size=True)\n  done = tf.zeros([batch_size, 1], dtype=tf.bool)\n\n  for t in tf.range(max_length):\n    dec_input = DecoderInput(\n        new_tokens=new_tokens, enc_output=enc_output, mask=(input_tokens != 0))\n\n    dec_result, dec_state = self.decoder(dec_input, state=dec_state)\n\n\n    attention = attention.write(t, dec_result.attention_weights)\n\n    new_tokens = self.sample(dec_result.logits, temperature)\n\n\n    # If a sequence produces an `end_token`, set it `done`\n    done = done | (new_tokens == self.end_token)\n    # Once a sequence is done it only produces 0-padding.\n    new_tokens = tf.where(done, tf.constant(0, dtype=tf.int64), new_tokens)\n\n    # Collect the generated tokens\n    result_tokens = result_tokens.write(t, new_tokens)\n\n    if tf.reduce_all(done):\n      break\n\n  # Convert the list of generated token ids to a list of strings.\n  result_tokens = result_tokens.stack()\n  result_tokens = tf.squeeze(result_tokens, -1)\n  result_tokens = tf.transpose(result_tokens, [1, 0])\n\n  result_text = self.tokens_to_text(result_tokens)\n\n  if return_attention:\n    attention_stack = attention.stack()\n\n    attention_stack = tf.squeeze(attention_stack, 2)\n\n    attention_stack = tf.transpose(attention_stack, [1, 0, 2])\n\n    return {'text': result_text, 'attention': attention_stack}\n  else:\n    return {'text': result_text}","4dfa9fca":"Translator.translate = translate","3cae9329":"@tf.function(input_signature=[tf.TensorSpec(dtype=tf.string, shape=[None])])\ndef tf_translate(self, input_text):\n    return self.translate(input_text)","2fa98068":"Translator.tf_translate = tf_translate","876570d8":"translator = Translator(\n    encoder=train_translator.encoder,\n    decoder=train_translator.decoder,\n    text_processor=text_processor,\n)","f5580850":"input_text = tf.constant(questions[:5])\n\nresult = translator.translate(input_text = input_text)\n\nprint(result['text'][0].numpy().decode())\nprint(result['text'][1].numpy().decode())\nprint(result['text'][2].numpy().decode())\nprint(result['text'][3].numpy().decode())\nprint(result['text'][4].numpy().decode())\nprint()","be979fc4":"# V. Create Attention Layer\n### The attention will focus on parts of the input of sequence. It receive an input sequence and return an attention vector of that sequence.","077de7cb":"# Define the loss function","0c656977":"# II. Loading data","9acc8a69":"# VIII. Create a module that implement the encoder and decoder to convert from text questions to text answers.","06fde7c2":"# VI. Create the decoder\n### The decoder will receive the encoder hidden state and output the result sequence which can be turned back to natural language.","659143a3":"# IX. Try the trained model","1d487e31":"## Check what the data looks like","bce50cff":"# VII. Declare the model to encode the input and decode the encoded input to output","e99cd6e4":"# III. Preprocessing the dataset","ed99dae7":"# I. Import neccessary library for loading data and modeling","f9b5be72":"# IV. Create the encoder\n### The encoder will encode the questions and return the encoded output and state"}}