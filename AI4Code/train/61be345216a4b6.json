{"cell_type":{"9fff953f":"code","f5c53b2d":"code","fd29ca95":"code","d1553bfa":"code","3dab338a":"code","5e193edb":"code","42b4fcf1":"code","8b7061e4":"code","9a0a87f6":"code","d26f2b8a":"code","9e891be0":"code","bd32337c":"code","ea35eda9":"code","0aa08403":"code","2ec66294":"code","87f635fe":"code","317ff969":"code","d21b6497":"code","79c5f479":"code","da509b19":"code","e582401b":"code","881da38d":"code","e1164ba7":"code","f69e44d0":"code","3498ba9d":"code","612b3093":"code","69a81abd":"code","140ab2a0":"code","78739e10":"code","b00bf7e0":"code","167391e7":"code","687a3775":"code","a1aa33eb":"code","ac5065c2":"code","b89117b3":"code","276e7d11":"code","03fe9aa4":"code","15bec481":"code","b2f80c58":"code","8ecf2aa8":"code","c1f9f66d":"code","e22b8c54":"code","dd60a95f":"code","7feb485a":"code","9fb6db41":"code","0a58f540":"code","45a3cc80":"code","6c1fdfcf":"code","ce2ba4f9":"code","ecfb1021":"code","f0599c4f":"code","923c679b":"code","8b212e3e":"code","a6fc8a18":"code","93d3f5bf":"code","c9d44c37":"code","57ad3d4f":"code","6422fd4c":"code","b8501ccc":"code","ec3f04eb":"code","0228b587":"code","7ed625ca":"code","fbd430ac":"code","b28c1a30":"code","b40dabe3":"code","bf46462b":"code","81361de1":"code","4f554aca":"markdown","dff495d6":"markdown","856d584f":"markdown","e08679e8":"markdown","0c0c5073":"markdown","6e7ac6c7":"markdown","14e128e9":"markdown","6712a79d":"markdown","25d06ec0":"markdown","20a1ddf1":"markdown","14a4488d":"markdown","1f057609":"markdown","4ce8faa4":"markdown","8e4ab639":"markdown","72e883aa":"markdown","7b0ba196":"markdown","96131ea3":"markdown","65e99304":"markdown","d0ffd7ee":"markdown","dbee66f7":"markdown","c23d2eed":"markdown","33c04cd2":"markdown","4e022f75":"markdown","3e2b99a0":"markdown","064d0525":"markdown","14f092df":"markdown","c52005e4":"markdown","31ce6aca":"markdown","ef555dc3":"markdown","22228b93":"markdown","f602b2af":"markdown","ba34556a":"markdown","b16591a6":"markdown","06f1021e":"markdown","c79359b4":"markdown","bc723a7f":"markdown","137ead10":"markdown","f7e4841e":"markdown","8d22ea5c":"markdown","08ce8f05":"markdown","a2acf4f8":"markdown","f9c99a17":"markdown","8c41726c":"markdown","ba5c0335":"markdown","98166f83":"markdown","7200cd1c":"markdown","d40e1828":"markdown","1fd0ada8":"markdown","d2146fc5":"markdown","77bdc148":"markdown","4b453494":"markdown","db36d550":"markdown","1f68de39":"markdown","566633b9":"markdown","f7c139ab":"markdown","35ef839a":"markdown","fc463082":"markdown","0b672660":"markdown","c0241542":"markdown","0c051de5":"markdown","0c0fa19c":"markdown","74b036a6":"markdown","6d764fdc":"markdown","e53be16e":"markdown","24d77d2f":"markdown","22ccc5fc":"markdown","0f3eadb1":"markdown","2823d142":"markdown","1983b094":"markdown","5df46116":"markdown","f81eb8ea":"markdown","d5050724":"markdown","9c87d72f":"markdown","903b1a69":"markdown","681cfbde":"markdown","e7664261":"markdown","c62e5fe0":"markdown","cac7e513":"markdown","9ecdd6bc":"markdown","f93f3b07":"markdown","118649d1":"markdown","6b5e07a4":"markdown"},"source":{"9fff953f":"import numpy as np              # arrays\nimport pandas as pd             # dataframes\nimport matplotlib.pyplot as plt # graphs\nimport seaborn as sns           # visualisations\nfrom scipy import stats         # statistics","f5c53b2d":"from sklearn.experimental import enable_iterative_imputer # enable experimental imputer\nfrom sklearn.impute import IterativeImputer               # sample imputation\nfrom sklearn import preprocessing                         # encoders, transformations\nfrom sklearn.model_selection import cross_validate        # cross-validation, model evaluation\nfrom sklearn.model_selection import GridSearchCV          # hyper-parameter tuning\nfrom sklearn.linear_model import LogisticRegression       # logistic regression model\nfrom sklearn.svm import SVC                               # support vector machine model\nfrom sklearn.neighbors import KNeighborsClassifier        # k-nearest neighbours model\nfrom sklearn.ensemble import GradientBoostingClassifier   # gradient boosting model\nfrom sklearn.ensemble import VotingClassifier             # voting ensemble model\nfrom sklearn.ensemble import StackingClassifier           # stacking ensemble model","fd29ca95":"%matplotlib inline","d1553bfa":"data_raw = pd.read_csv(\n    filepath_or_buffer='..\/input\/speed-dating-experiment\/Speed Dating Data.csv',\n    engine='python'\n)","3dab338a":"data_raw.shape","5e193edb":"data_raw.memory_usage().sum()","42b4fcf1":"def plot_distribution(data, bins, title, xlabel, ylabel):\n    ax = sns.distplot(\n        data,\n        bins=bins,\n        hist_kws={\n            \"linewidth\": 1,\n            'edgecolor': 'black',\n            'alpha': 1.0\n            },\n        kde=False\n    )\n    ax.set_title(title)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel);","8b7061e4":"def plot_relationship(x, y, title, xlabel, ylabel):\n    ax = sns.barplot(\n        x=x,\n        y=y,\n        orient='h'\n    )\n    ax.set_title(title)\n    ax.set_xlabel(xlabel)\n    ax.set_ylabel(ylabel);","9a0a87f6":"def print_moments(title, feature):\n    print(title)\n    print('Mean: '+'{:>18.2f}'.format(feature.mean()))\n    print('Standard deviation: '+'{:.2f}'.format(feature.std()))\n    print('Skewness: '+'{:>14.2f}'.format(feature.skew()))\n    print('Kurtosis: '+'{:>14.2f}'.format(feature.kurtosis()))","d26f2b8a":"relevant_features = [\n    ['iid', 'int16'],\n    ['gender', 'bool'],\n    ['wave', 'int16'],\n    ['position', 'int16'],\n    ['order', 'int16'],\n    ['pid', 'int16'],\n    ['age_o', 'int16'],\n    ['race_o', 'category'],\n    ['pf_o_att', 'int16'],\n    ['pf_o_sin', 'int16'],\n    ['pf_o_int', 'int16'],\n    ['pf_o_fun', 'int16'],\n    ['pf_o_amb', 'int16'],\n    ['pf_o_sha', 'int16'],\n    ['dec_o', 'bool'],\n    ['attr_o', 'int16'],\n    ['sinc_o', 'int16'],\n    ['intel_o', 'int16'],\n    ['fun_o', 'int16'],\n    ['amb_o', 'int16'],\n    ['shar_o', 'int16'],\n    ['like_o', 'int16'],\n    ['prob_o', 'int16'],\n    ['met_o', 'bool'],\n    ['age', 'int16'],\n    ['field_cd', 'category'],\n    ['race', 'category'],\n    ['imprace', 'int16'],\n    ['imprelig', 'int16'],\n    ['goal', 'category'],\n    ['date', 'int16'],\n    ['go_out', 'int16'],\n    ['career_c', 'category'],\n    ['sports', 'int16'],\n    ['tvsports', 'int16'],\n    ['exercise', 'int16'],\n    ['dining', 'int16'],\n    ['museums', 'int16'],\n    ['art', 'int16'],\n    ['hiking', 'int16'],\n    ['gaming', 'int16'],\n    ['clubbing', 'int16'],\n    ['reading', 'int16'],\n    ['tv', 'int16'],\n    ['theater', 'int16'],\n    ['movies', 'int16'],\n    ['concerts', 'int16'],\n    ['music', 'int16'],\n    ['shopping', 'int16'],\n    ['yoga', 'int16'],\n    ['exphappy', 'int16'],\n    ['expnum', 'int16'],\n    ['attr1_1', 'int16'],\n    ['sinc1_1', 'int16'],\n    ['intel1_1', 'int16'],\n    ['fun1_1', 'int16'],\n    ['amb1_1', 'int16'],\n    ['shar1_1', 'int16'],\n    ['attr3_1', 'int16'],\n    ['sinc3_1', 'int16'],\n    ['fun3_1', 'int16'],\n    ['intel3_1', 'int16'],\n    ['amb3_1', 'int16'],\n    ['dec', 'bool'],\n    ['attr', 'int16'],\n    ['sinc', 'int16'],\n    ['intel', 'int16'],\n    ['fun', 'int16'],\n    ['amb', 'int16'],\n    ['shar', 'int16'],\n    ['like', 'int16'],\n    ['prob', 'int16'],\n    ['met', 'int16'],\n    ['match_es', 'int16'],\n    ['satis_2', 'int16'],\n    ['length', 'int16'],\n    ['numdat_2', 'int16']\n]","9e891be0":"data = data_raw[[feature[0] for feature in relevant_features]]","bd32337c":"data.shape","ea35eda9":"data.memory_usage().sum()","0aa08403":"data = data.astype({feature: datatype if all(data[feature].notna().values) else 'float32' if datatype == 'int16' else datatype for (feature, datatype) in relevant_features})","2ec66294":"data.memory_usage().sum()","87f635fe":"data.to_csv(\n    path_or_buf='.\/data.csv',\n    index=False\n)","317ff969":"partner_accepts = data['dec_o']\nround(partner_accepts[partner_accepts == True].count()\/partner_accepts.count(),3)","d21b6497":"plt.figure(figsize=(16,10))\nplt.tight_layout(pad=5.0)\n\nplt.subplot(2,3,1)\nplot_distribution(\n    data=data['attr_o'],\n    bins=np.arange(0, 10, 0.5).tolist(),\n    title='Subject\\'s attractiveness rating',\n    xlabel='Attractiveness rating',\n    ylabel='Number of subjects'\n)\nplt.subplot(2,3,2)\nplot_distribution(\n    data=data['sinc_o'],\n    bins=np.arange(0, 10, 0.5).tolist(),\n    title='Subject\\'s sincerity rating',\n    xlabel='Sincerity rating',\n    ylabel='Number of subjects'\n)\nplt.subplot(2,3,3)\nplot_distribution(\n    data=data['intel_o'],\n    bins=np.arange(0, 10, 0.5).tolist(),\n    title='Subject\\'s intelligence rating',\n    xlabel='Intelligence rating',\n    ylabel='Number of subjects'\n)\nplt.subplot(2,3,4)\nplot_distribution(\n    data=data['fun_o'],\n    bins=np.arange(0, 10, 0.5).tolist(),\n    title='Subject\\'s fun rating',\n    xlabel='Fun rating',\n    ylabel='Number of subjects'\n)\nplt.subplot(2,3,5)\nplot_distribution(\n    data=data['amb_o'],\n    bins=np.arange(0, 10, 0.5).tolist(),\n    title='Subject\\'s ambition rating',\n    xlabel='Ambition rating',\n    ylabel='Number of subjects'\n)\nplt.subplot(2,3,6)\nplot_distribution(\n    data=data['shar_o'],\n    bins=np.arange(0, 10, 0.5).tolist(),\n    title='Subject\\'s shared interest rating',\n    xlabel='Shared interest rating',\n    ylabel='Number of subjects'\n)","79c5f479":"print_moments('Attractiveness rating', data['attr_o'])","da509b19":"print_moments('Sincerity rating', data['sinc_o'])","e582401b":"print_moments('Intelligence rating', data['intel_o'])","881da38d":"print_moments('Fun rating', data['fun_o'])","e1164ba7":"print_moments('Ambition rating', data['amb_o'])","f69e44d0":"print_moments('Shared interest rating', data['shar_o'])","3498ba9d":"data.std().sort_values(ascending=False).head(10)","612b3093":"abs(data.skew()).sort_values(ascending=False).head(10)","69a81abd":"features_selected = [\n    'dec_o',\n    'pf_o_att',\n    'pf_o_sin',\n    'pf_o_int',\n    'pf_o_fun',\n    'pf_o_amb',\n    'pf_o_sha',\n    'attr_o',\n    'sinc_o',\n    'intel_o',\n    'fun_o',\n    'amb_o',\n    'shar_o'\n]","140ab2a0":"plt.figure(figsize=(12,10))\ncmap = plt.cm.RdBu\nmask = np.triu(data[features_selected].astype(float).corr())\nsns.heatmap(\n    data[features_selected].astype(float).corr(),\n    square=True,\n    cmap=cmap,\n    mask=mask,\n    linewidths=0.1,\n    vmax=1.0,\n    linecolor='white'\n);","78739e10":"data_men = data[data['gender']==1]\n\nplt.figure(figsize=(12,10))\ncmap = plt.cm.RdBu\nmask = np.triu(data_men[features_selected].astype(float).corr())\nsns.heatmap(\n    data_men[features_selected].astype(float).corr(),\n    square=True,\n    cmap=cmap,\n    mask=mask,\n    linewidths=0.1,\n    vmax=1.0,\n    linecolor='white'\n);","b00bf7e0":"data_women = data[data['gender']==0]\n\nplt.figure(figsize=(12,10))\ncmap = plt.cm.RdBu\nmask = np.triu(data_women[features_selected].astype(float).corr())\nsns.heatmap(\n    data_women[features_selected].astype(float).corr(),\n    square=True,\n    cmap=cmap,\n    mask=mask,\n    linewidths=0.1,\n    vmax=1.0,\n    linecolor='white'\n);","167391e7":"correlations = data.corr().abs().unstack().sort_values(ascending=False).drop_duplicates()\ncorrelations = correlations[correlations != 1]\ncorrelations[correlations > 0.6]","687a3775":"partner_decision_correlations = correlations.loc['dec_o']\npartner_decision_correlations[partner_decision_correlations > 0.1]","a1aa33eb":"missing_samples_proportion = data.isnull().sum()\/len(data)\nmissing_samples_proportion.sort_values(ascending=False).head(10)","ac5065c2":"#missing_half_samples = missing_samples_proportion[missing_samples_proportion > 0.5].index.values\n#data.drop(columns=missing_half_samples, inplace=True)","b89117b3":"imputer = IterativeImputer(\n    missing_values=np.nan,\n    sample_posterior=True,\n    n_nearest_features=5,\n    min_value=0,\n    max_value=100,\n    random_state=0\n)\nimputer.fit(data)\ndata_imputed = np.around(imputer.transform(data))\ndata = pd.DataFrame(data_imputed, columns=data.columns)","276e7d11":"data = data.astype({feature: datatype if all(data[feature].notna().values) else 'float32' if datatype == 'int16' else datatype for (feature, datatype) in relevant_features})","03fe9aa4":"features_nominal = data.dtypes[data.dtypes == 'category'].index.values\ndata = pd.get_dummies(data, prefix=features_nominal)","15bec481":"subject_attractiveness_mean = data[['iid', 'attr_o']].groupby(['iid']).mean()['attr_o']\nsubject_sincerity_mean = data[['iid', 'sinc_o']].groupby(['iid']).mean()['sinc_o']\nsubject_intelligence_mean = data[['iid', 'intel_o']].groupby(['iid']).mean()['intel_o']\nsubject_fun_mean = data[['iid', 'fun_o']].groupby(['iid']).mean()['fun_o']\nsubject_ambition_mean = data[['iid', 'amb_o']].groupby(['iid']).mean()['amb_o']\nsubject_shared_interest_mean = data[['iid', 'shar_o']].groupby(['iid']).mean()['shar_o']","b2f80c58":"data = data.merge(\n    right=subject_attractiveness_mean,\n    how='inner',\n    on='iid'\n).rename(columns={\n    'attr_o_x': 'attr_o',\n    'attr_o_y': 'subject_attractiveness_mean'\n})\ndata = data.merge(\n    right=subject_sincerity_mean,\n    how='inner',\n    on='iid'\n).rename(columns={\n    'sinc_o_x': 'sinc_o',\n    'sinc_o_y': 'subject_sincerity_mean'\n})\ndata = data.merge(\n    right=subject_intelligence_mean,\n    how='inner',\n    on='iid'\n).rename(columns={\n    'intel_o_x': 'intel_o',\n    'intel_o_y': 'subject_intelligence_mean'\n})\ndata = data.merge(\n    right=subject_fun_mean,\n    how='inner',\n    on='iid'\n).rename(columns={\n    'fun_o_x': 'fun_o',\n    'fun_o_y': 'subject_fun_mean'\n})\ndata = data.merge(\n    right=subject_ambition_mean,\n    how='inner',\n    on='iid'\n).rename(columns={\n    'amb_o_x': 'amb_o',\n    'amb_o_y': 'subject_ambition_mean'\n})\ndata = data.merge(\n    right=subject_shared_interest_mean,\n    how='inner',\n    on='iid'\n).rename(columns={\n    'shar_o_x': 'shar_o',\n    'shar_o_y': 'subject_shared_interest_mean'\n})","8ecf2aa8":"data['age_difference'] = abs(data['age'] - data['age_o'])","c1f9f66d":"data['attractiveness_difference'] = abs(data['attr'] - data['attr_o'])\ndata['sincerity_difference'] = abs(data['sinc'] - data['sinc_o'])\ndata['intelligence_difference'] = abs(data['intel'] - data['intel_o'])\ndata['fun_difference'] = abs(data['fun'] - data['fun_o'])\ndata['ambition_difference'] = abs(data['amb'] - data['amb_o'])\ndata['shared_interest_difference'] = abs(data['shar'] - data['shar_o'])","e22b8c54":"features_normal = [\n    'attr_o',\n    'sinc_o',\n    'intel_o',\n    'fun_o',\n    'amb_o',\n    'shar_o',\n    'age_difference',\n    'attractiveness_difference',\n    'sincerity_difference',\n    'intelligence_difference',\n    'fun_difference',\n    'ambition_difference',\n    'shared_interest_difference'\n]","dd60a95f":"data[features_normal] = data[features_normal].apply(lambda x: preprocessing.scale(x))","7feb485a":"features_no_information = [\n    'iid',\n    'pid',\n    'wave',\n    'position',\n    'order'\n]","9fb6db41":"features_future_information = [\n    'dec',\n    'dec_o',\n    'like',\n    'prob',\n    'like_o',\n    'prob_o'\n]","0a58f540":"feature_variances = data.std().sort_values(ascending=True)\nfeatures_low_variance = feature_variances[feature_variances < 0.1].index.values.tolist()","45a3cc80":"features_weak_correlation = partner_decision_correlations[partner_decision_correlations < 0.1].axes[0].to_list()\nfeatures_weak_correlation = list(set(features_weak_correlation) - set(features_future_information) - set(features_no_information))","6c1fdfcf":"features_interaction = [\n    'age',\n    'age_o',\n]","ce2ba4f9":"features_remove = features_no_information+features_future_information+features_low_variance+features_weak_correlation+features_interaction\ndata_model = data.drop(columns=features_remove)","ecfb1021":"data_model.memory_usage().sum()","f0599c4f":"data_model.to_csv(\n    path_or_buf='.\/data_model.csv',\n    index=False\n)","923c679b":"features = data_model\ntarget = data['dec_o']","8b212e3e":"parameters = {\n    'penalty': ['l2'],\n    'solver': ['lbfgs'],\n    'C': np.logspace(-4, 4, 20),\n    'max_iter': [10000]\n}\nclassifier_lr = LogisticRegression(random_state=0)\nclassifier_lr = GridSearchCV(\n    estimator=classifier_lr,\n    param_grid=parameters,\n    cv=5,\n    verbose=2,\n    n_jobs=-1\n)\nclassifier_lr.fit(features, target)\nclassifier_lr.best_params_","a6fc8a18":"classifier_lr = LogisticRegression(\n    random_state=0,\n    penalty=classifier_lr.best_params_['penalty'],\n    solver=classifier_lr.best_params_['solver'],\n    C=classifier_lr.best_params_['C'],\n    max_iter=classifier_lr.best_params_['max_iter']\n)","93d3f5bf":"parameters = {\n    'kernel': ['rbf'],\n    'gamma': [1e-4, 1e-3, 1e-2],\n    'C': [1, 10, 100, 1000]\n}\nclassifier_sv = SVC(random_state=0)\nclassifier_sv = GridSearchCV(\n    estimator=classifier_sv,\n    param_grid=parameters,\n    cv=5,\n    verbose=2,\n    n_jobs=-1\n)\nclassifier_sv.fit(features, target)\nclassifier_sv.best_params_","c9d44c37":"classifier_sv = SVC(\n    random_state=0,\n    kernel=classifier_sv.best_params_['kernel'],\n    gamma=classifier_sv.best_params_['gamma'],\n    C=classifier_sv.best_params_['C']\n)","57ad3d4f":"parameters = {\n    'n_neighbors': [5,11,19,29],\n    'weights': ['uniform', 'distance'],\n    'metric': ['minkowski', 'euclidean', 'manhattan']\n}\nclassifier_kn = KNeighborsClassifier()\nclassifier_kn = GridSearchCV(\n    estimator=classifier_kn,\n    param_grid=parameters,\n    cv=5,\n    verbose=2,\n    n_jobs=-1\n)\nclassifier_kn.fit(features, target)\nclassifier_kn.best_params_","6422fd4c":"classifier_kn = KNeighborsClassifier(\n    n_neighbors=classifier_kn.best_params_['n_neighbors'],\n    weights=classifier_kn.best_params_['weights'],\n    metric=classifier_kn.best_params_['metric']\n)","b8501ccc":"parameters = {\n    'loss': ['deviance', 'exponential'],\n    'learning_rate': [0.05],\n    'n_estimators': [100, 200, 300],\n    'max_depth': [3, 4, 5],\n    'max_features': ['sqrt', 'log2']\n}\nclassifier_gb = GradientBoostingClassifier(random_state=0)\nclassifier_gb = GridSearchCV(\n    estimator=classifier_gb,\n    param_grid=parameters,\n    cv=5,\n    verbose=2,\n    n_jobs=-1\n)\nclassifier_gb.fit(features, target)\nclassifier_gb.best_params_","ec3f04eb":"classifier_gb = GradientBoostingClassifier(\n    random_state=0,\n    loss=classifier_gb.best_params_['loss'],\n    learning_rate=classifier_gb.best_params_['learning_rate'],\n    n_estimators=classifier_gb.best_params_['n_estimators'],\n    max_depth=classifier_gb.best_params_['max_depth'],\n    max_features=classifier_gb.best_params_['max_features']\n)","0228b587":"estimators = [\n    ('lr', classifier_lr),\n    ('sv', classifier_sv),\n    ('kn', classifier_kn),\n    ('gb', classifier_gb)\n]","7ed625ca":"classifier_ve = VotingClassifier(\n    estimators=estimators,\n    voting='hard'\n)","fbd430ac":"classifier_se = StackingClassifier(\n    estimators=estimators,\n    final_estimator=LogisticRegression()\n)","b28c1a30":"metrics = ['accuracy', 'precision', 'recall', 'f1_macro']\n\nfor classifier, label in zip(\n    [classifier_lr, classifier_sv, classifier_kn, classifier_gb, classifier_ve, classifier_se],\n    ['Logistic Regression', 'Support Vector Machine', 'k-Nearest Neighbours', 'Gradient Boosting', 'Voting Ensemble', 'Stacking Ensemble']\n):\n    print('{}'.format(label))\n    scores = cross_validate(\n        estimator=classifier,\n        X=features,\n        y=target,\n        scoring=metrics,\n        cv=5,\n        n_jobs=-1\n    )\n    for key, value in scores.items():\n        print('{:14} {:.3f} +\/- {:.3f}'.format(key, value.mean(), value.std()))\n    print('\\n')","b40dabe3":"labels = features.columns.values\nweights = classifier_lr.fit(features,target).coef_[0]","bf46462b":"top_features = sorted(list(zip(labels,weights)), reverse=True, key = lambda x: abs(x[1]))[0:10]\ntop_labels = [x[0] for x in top_features]\ntop_weights = [x[1] for x in top_features]","81361de1":"plt.figure(figsize=(10,8))\nplot_relationship(top_weights, top_labels, 'Most significant features in linear model', 'Weight', 'Feature')","4f554aca":"Persist dataframe containing features to be used in model","dff495d6":"### 3.2 Feature datatypes<a id=\"3.2\"><\/a>","856d584f":"### 5.4 Feature transformations<a id=\"5.4\"><\/a>","e08679e8":"Get highest correlated features with target variable","0c0c5073":"### 4.2 Bivariate analysis<a id=\"4.2\"><\/a>","6e7ac6c7":"### 5.1 Sample analysis<a id=\"5.1\"><\/a>","14e128e9":"### 2.2 Data<a id=\"2.2\"><\/a>","6712a79d":"Import raw dataset into a dataframe","25d06ec0":"Calculate performance metrics for each model","20a1ddf1":"Drop features that were used in interaction variables","14a4488d":"## 7 Evaluation<a id=\"7\"><\/a>","1f057609":"- This notebook explored whether a machine learning model could predict if a partner will match with their date.\n- The best performing classifier was a stacking ensemble model with an accuracy of **~76%** and an F1 score of **~75%**.\n- Based on correlation analysis and weights in the linear model:\n  - **Attractiveness**, **shared interests** and being **fun** were the most significant factors in a partner's decision.\n  - **Medical students** had the highest probability of being matched while **psychologists** and **academics** had the lowest.\n- Further feature engineering such as introducing more interaction features and performing more thorough feature selection could improve the classifier performance.","4ce8faa4":"### 1.1 Problem definition<a id=\"1.1\"><\/a>","8e4ab639":"Insert average attribute ratings into dataframe","72e883aa":"### 5.2 Feature representation<a id=\"5.2\"><\/a>","7b0ba196":"### 5.3 Feature interactions<a id=\"5.3\"><\/a>","96131ea3":"Get highest correlated feature pairs","65e99304":"## 2 Environment<a id=\"2\"><\/a>","d0ffd7ee":"Visualise correlation between selected features for women","dbee66f7":"Get number of rows and columns of raw dataframe","c23d2eed":"Persist dataframe containing relevant features with appropriate datatypes","33c04cd2":"Get coefficients of features in linear model","4e022f75":"Get memory usage of raw dataframe","3e2b99a0":"### 7.2 Feature importance<a id=\"7.2\"><\/a>","064d0525":"Get proportion of dates where partner matched with subject","14f092df":"### 3.3 Data export<a id=\"3.3\"><\/a>","c52005e4":"Define a function to print a feature's mean, standard deviation, skewness and kurtosis","31ce6aca":"### 1.2 Dataset<a id=\"1.2\"><\/a>","ef555dc3":"Instantiate, train and tune a instance-based model","22228b93":"### 5.5 Feature selection<a id=\"5.5\"><\/a>","f602b2af":"Create new dataframe containing relevant features","ba34556a":"Instantiate and train a voting and stacking model","b16591a6":"Calculate the average attribute ratings for each subject","06f1021e":"## 6 Modelling<a id=\"6\"><\/a>","c79359b4":"### 6.1 Baseline models<a id=\"6.1\"><\/a>","bc723a7f":"### 7.1 Classifier performance<a id=\"7.1\"><\/a>","137ead10":"Impute missing samples using iterative imputer","f7e4841e":"Drop irrelevant features which contain no information about the target variable ","8d22ea5c":"Get memory usage of new dataframe","08ce8f05":"Encode nominal features using one-hot encoding","a2acf4f8":"### 2.1 Libraries<a id=\"2.1\"><\/a>","f9c99a17":"Calculate difference between subject and partner's ages","8c41726c":"Instantiate, train and tune a boosting model","ba5c0335":"Drop features that have weak correlation with target variable","98166f83":"Get features with highest magnitude","7200cd1c":"**1&nbsp;&nbsp;[Introduction](#1)**  \n&nbsp;&nbsp;&nbsp;&nbsp;1.1&nbsp;&nbsp;[Problem definition](#1.1)  \n&nbsp;&nbsp;&nbsp;&nbsp;1.2&nbsp;&nbsp;[Dataset](#1.2)  \n**2&nbsp;&nbsp;[Environment](#2)**  \n&nbsp;&nbsp;&nbsp;&nbsp;2.1&nbsp;&nbsp;[Libraries](#2.1)  \n&nbsp;&nbsp;&nbsp;&nbsp;2.2&nbsp;&nbsp;[Data](#2.2)  \n&nbsp;&nbsp;&nbsp;&nbsp;2.3&nbsp;&nbsp;[Functions](#2.3)  \n**3&nbsp;&nbsp;[Wrangling](#3)**  \n&nbsp;&nbsp;&nbsp;&nbsp;3.1&nbsp;&nbsp;[Relevant features](#3.1)  \n&nbsp;&nbsp;&nbsp;&nbsp;3.2&nbsp;&nbsp;[Feature datatypes](#3.2)  \n&nbsp;&nbsp;&nbsp;&nbsp;3.3&nbsp;&nbsp;[Data export](#3.3)  \n**4&nbsp;&nbsp;[Exploration](#4)**  \n&nbsp;&nbsp;&nbsp;&nbsp;4.1&nbsp;&nbsp;[Univariate analysis](#4.1)  \n&nbsp;&nbsp;&nbsp;&nbsp;4.2&nbsp;&nbsp;[Bivariate analysis](#4.2)  \n**5&nbsp;&nbsp;[Engineering](#5)**  \n&nbsp;&nbsp;&nbsp;&nbsp;5.1&nbsp;&nbsp;[Sample analysis](#5.1)  \n&nbsp;&nbsp;&nbsp;&nbsp;5.2&nbsp;&nbsp;[Feature representation](#5.2)  \n&nbsp;&nbsp;&nbsp;&nbsp;5.3&nbsp;&nbsp;[Feature interactions](#5.3)  \n&nbsp;&nbsp;&nbsp;&nbsp;5.4&nbsp;&nbsp;[Feature transformations](#5.4)  \n&nbsp;&nbsp;&nbsp;&nbsp;5.5&nbsp;&nbsp;[Feature selection](#5.5)  \n&nbsp;&nbsp;&nbsp;&nbsp;5.6&nbsp;&nbsp;[Data export](#5.6)  \n**6&nbsp;&nbsp;[Modelling](#6)**  \n&nbsp;&nbsp;&nbsp;&nbsp;6.1&nbsp;&nbsp;[Baseline models](#6.1)  \n&nbsp;&nbsp;&nbsp;&nbsp;6.2&nbsp;&nbsp;[Ensemble models](#6.2)  \n**7&nbsp;&nbsp;[Evaluation](#7)**  \n&nbsp;&nbsp;&nbsp;&nbsp;7.1&nbsp;&nbsp;[Classifier performance](#7.1)  \n&nbsp;&nbsp;&nbsp;&nbsp;7.2&nbsp;&nbsp;[Feature importance](#7.2)  \n&nbsp;&nbsp;&nbsp;&nbsp;7.3&nbsp;&nbsp;[Learning rate](#7.3)  \n**8&nbsp;&nbsp;[Conclusion](#8)**","d40e1828":"## 5 Engineering<a id=\"5\"><\/a>","1fd0ada8":"## 4 Exploration<a id=\"4\"><\/a>","d2146fc5":"## 3 Wrangling<a id=\"3\"><\/a>","77bdc148":"- A study conducted by Columbia University explored gender differences in dating preferences.\n- Participants attended a dating event where they had a 4-minute date with every other participant of the opposite sex who attended the same event.\n- The participants decided to accept or reject their partner. If both the participant and partner matched, they received each other's contact information.\n- Participants rated their partners on six personal attributes: attractiveness, sincerity, intelligence, fun, ambition and shared interests.\n- Before and after the event, participants rated their preferences in the six attributes and gave themselves ratings.\n- Other information was collected about the participants' background and preferences.","4b453494":"Visualise correlation between selected features","db36d550":"Calculate difference between subject's attribute ratings and partner's attributes ratings","1f68de39":"Scale normal features to zero mean and unit variance","566633b9":"Identify relevant features and associated datatypes","f7c139ab":"Get features with highest variance","35ef839a":"Drop features with more than 50% missing samples","fc463082":"Instantiate, train and tune a discriminative model","0b672660":"Get memory usage of model dataframe","c0241542":"Update feature datatypes","0c051de5":"Define a function to plot distribution functions","0c0fa19c":"Plot the distributions of subject attribute ratings from their partners","74b036a6":"Visualise correlation between selected features for men","6d764fdc":"### 3.1 Relevant features<a id=\"3.1\"><\/a>","e53be16e":"Drop features that are known in the future","24d77d2f":"Drop features that have low variance","22ccc5fc":"Define a function to plot relationship between two features","0f3eadb1":"Calculate the moments of subject attribute ratings from their partners","2823d142":"### 5.6 Data export<a id=\"5.6\"><\/a>","1983b094":"## 8 Conclusion<a id=\"8\"><\/a>","5df46116":"Get features with highest skew","f81eb8ea":"### 4.1 Univariate analysis<a id=\"4.1\"><\/a>","d5050724":"### 2.3 Functions<a id=\"2.3\"><\/a>","9c87d72f":"Plot features with highest magnitude","903b1a69":"Get number of rows and columns of new dataframe","681cfbde":"Get proportion of dataframe with missing samples","e7664261":"- This notebook will investigate whether it is possible to **predict if a partner will match with their date** based on **dating preferences**, **attribute ratings** and **background information**.\n- The problem can be framed as a supervised, binary classification problem where the model predicts if a partner has accepted or rejected their date.","c62e5fe0":"### 6.2 Ensemble models<a id=\"6.2\"><\/a>","cac7e513":"Get memory usage of updated dataframe","9ecdd6bc":"## 1 Introduction<a id=\"1\"><\/a>","f93f3b07":"Define feature and target variables","118649d1":"Load libraries into notebook","6b5e07a4":"Instantiate, train and tune a linear model"}}