{"cell_type":{"b31d6c30":"code","22490349":"code","fee80cd3":"code","a7ff9ec2":"code","575d6698":"code","901fe4d9":"code","deaef84f":"code","942dd918":"code","4b8a3597":"code","da92a60c":"code","1ce0610c":"code","b453c485":"code","c4fa096e":"code","ce541e3e":"code","42df5ab9":"code","b2baa162":"code","d9e96748":"code","57cb3e29":"code","a360543c":"code","a1cc8c0c":"code","c3ac33c3":"code","bef083ce":"code","f76ad3de":"code","b70e2d22":"code","19766207":"code","a8946f5d":"code","40de1d00":"code","9b22068d":"code","801db74a":"code","991e4f4d":"code","86adda10":"code","af3d668f":"code","72a0568d":"code","710ef1d7":"code","4ecb394e":"code","02d6b37d":"code","2b6757d8":"code","3a710b01":"code","6c515cd3":"code","36f25fdb":"code","f06f2f68":"code","5cc110b3":"code","1151db10":"code","0f13276f":"code","0cbf1ee8":"code","55dfb764":"code","3355bb42":"code","7de5ca51":"code","240cb2e6":"code","e1c87fa4":"code","22e71986":"code","ae7a0665":"code","d70a813b":"code","3ff99707":"code","001aaea2":"code","15c31437":"code","b9bc3c9c":"code","2fdbdbd9":"code","4f5b0947":"code","57c9f82a":"code","7b177110":"code","7ad4d74f":"code","1d13b371":"code","7f3c6c63":"code","bd5d191e":"code","0f3cc09e":"code","279eb0b5":"code","e4f79592":"code","75b189d2":"code","414c751a":"code","80044bf4":"code","baf732e5":"code","51a489f6":"code","3321a58c":"code","b57b8d09":"code","568fed10":"code","193c0846":"code","a2996628":"code","4bc351ae":"code","6e09edca":"code","999a35f0":"code","6f3723d1":"code","de853d72":"code","f2988286":"code","cc833c0a":"code","43c3a149":"code","2464a8f3":"code","96f306e6":"code","840a1d0c":"code","b8a5439a":"code","047a7ea6":"code","b6701b88":"code","f66943f5":"code","4a8664c1":"code","921949b2":"code","56cf3be3":"code","21e0bd11":"code","d903545b":"code","62e55684":"code","cafd2035":"code","73751e36":"code","f6d4f84a":"code","40444fdb":"code","c833f078":"code","3c626c85":"code","e33547f3":"code","8908eefc":"markdown","2ced1cba":"markdown","f292eb31":"markdown","51d6e307":"markdown","b03287ab":"markdown","1365af1f":"markdown","88b4eb89":"markdown","47708286":"markdown","767bb5d9":"markdown","58cd6d04":"markdown","6343f863":"markdown","0b35025f":"markdown","8288af88":"markdown","dd21baee":"markdown","97a191e8":"markdown","50ab520e":"markdown","006a18ff":"markdown","f2f4564f":"markdown","30aca998":"markdown","589663a6":"markdown","d1711dcd":"markdown","c7390a71":"markdown","07ae7b0c":"markdown","f5693dff":"markdown","a1986b97":"markdown","865df3ba":"markdown","bde41373":"markdown","212ae09a":"markdown","4c1e5f58":"markdown","70897caf":"markdown","e3a72e26":"markdown","4aa43312":"markdown","c35c734b":"markdown","b050a3f4":"markdown","954af0b6":"markdown","08334163":"markdown","35e4ad82":"markdown","bbef1301":"markdown","edd89566":"markdown","67fc1358":"markdown","817d6b3c":"markdown","c4370b59":"markdown","5ff4627a":"markdown","98a3edfe":"markdown","2896c08f":"markdown","f2f1b72d":"markdown","c990744b":"markdown","a3c34e56":"markdown","18c1ddcc":"markdown","42b3c241":"markdown","588f7355":"markdown","cde1ebda":"markdown","4d3c7a7b":"markdown","44cf4c76":"markdown","336ccce8":"markdown"},"source":{"b31d6c30":"import pandas as pd\nimport numpy as np\nimport os\nimport matplotlib\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport seaborn as sns\nsns.set(style='whitegrid')\n\nfrom wordcloud import WordCloud\n\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA, TruncatedSVD\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nfrom collections import defaultdict\nfrom collections import Counter\n\nimport re\nimport gensim\nimport string\n\nfrom tqdm import tqdm\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers import Embedding, LSTM,Dense, SpatialDropout1D, Dropout\nfrom keras.initializers import Constant\n\nimport warnings\nwarnings.simplefilter('ignore')","22490349":"import nltk\nnltk.download('all')","fee80cd3":"df=pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ndf_test=pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsample_submission=pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","a7ff9ec2":"df.shape, df_test.shape","575d6698":"df","901fe4d9":"df.loc[:,['text','target']]","deaef84f":"df.info()","942dd918":"df.isnull().sum()","4b8a3597":"df.target.value_counts()","da92a60c":"df2=df.copy(deep=True)\npie1=pd.DataFrame(df2['target'].replace(1,'disaster').replace(0,'non-disaster').value_counts())\npie1.reset_index(inplace=True)\npie1.plot(kind='pie', title='Pie chart of Disaster\/Non-disaster tweets',y = 'target', \n          autopct='%1.1f%%', shadow=False, labels=pie1['index'], legend = False, fontsize=14, figsize=(12,12))","1ce0610c":"sns.set(style='whitegrid')\nf, (ax1, ax2) = plt.subplots(1,2,figsize=(25,8))\n\nax1.hist(df[df['target'] == 0]['text'].str.len(), bins=30, color='b')\nax1.set_title('Non-disaster tweets')\n\nax2.hist(df[df['target'] == 1]['text'].str.len(), bins=30, color='r')\nax2.set_title('Disaster tweets')\n\nf.suptitle('Histogram number of characters in tweets')","b453c485":"f, (ax1, ax2,) = plt.subplots(1,2,figsize=(25,8))\n\nax1.hist(df[df['target'] == 0]['text'].str.split().map(lambda x: len(x)), bins=29, color='b')\nax1.set_title('Non-disaster tweets')\n\nax2.hist(df[df['target'] == 1]['text'].str.split().map(lambda x: len(x)), bins=29, color='r')\nax2.set_title('Disaster tweets')\n\nf.suptitle('Histogram number of words in tweets')","c4fa096e":"disaster_index=df[df['target']==1].index.values","ce541e3e":"len(disaster_index)","42df5ab9":"print(df.iloc[63,1])\nprint(df.iloc[63,3])","b2baa162":"df.iloc[63,3] + ' ' + df.iloc[63,1]","d9e96748":"for i in disaster_index:\n  df.iloc[i,3] = str(df.iloc[i,3]) + ' ' + str(df.iloc[i,1])","57cb3e29":"df.info()","a360543c":"example=\"New competition launched :https:\/\/www.kaggle.com\/c\/nlp-getting-started\"","a1cc8c0c":"def remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)\n\nremove_URL(example)","c3ac33c3":"df['text']=df['text'].apply(lambda x : remove_URL(x))","bef083ce":"df_test['text']=df_test['text'].apply(lambda x : remove_URL(x))","f76ad3de":"example = \"\"\"<div>\n<h1>Real or Fake<\/h1>\n<p>Kaggle <\/p>\n<a href=\"https:\/\/www.kaggle.com\/c\/nlp-getting-started\">getting started<\/a>\n<\/div>\"\"\"","b70e2d22":"def remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)\n    \nprint(remove_html(example))","19766207":"df['text']=df['text'].apply(lambda x : remove_html(x))","a8946f5d":"df_test['text']=df_test['text'].apply(lambda x : remove_html(x))","40de1d00":"# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake \ud83d\ude14\ud83d\ude14\")","9b22068d":"df['text']=df['text'].apply(lambda x: remove_emoji(x))","801db74a":"df_test['text']=df_test['text'].apply(lambda x: remove_emoji(x))","991e4f4d":"df['text']=df['text'].str.replace('\\d+', '')","86adda10":"df_test['text']=df_test['text'].str.replace('\\d+', '')","af3d668f":"def cleaner(tweet):\n  # Acronyms and miswritten words\n  tweet = re.sub(r\"Typhoon-Devastated\", \"typhoon devastated\", tweet)\n  tweet = re.sub(r\"TyphoonDevastated\", \"typhoon devastated\", tweet)\n  tweet = re.sub(r\"typhoondevastated\", \"typhoon devastated\", tweet)\n  tweet = re.sub(r\"MH370\", \"Malaysia Airlines Flight\", tweet)\n  tweet = re.sub(r\"MH\", \"Malaysia Airlines Flight\", tweet)\n  tweet = re.sub(r\"mh370\", \"Malaysia Airlines Flight\", tweet)\n  tweet = re.sub(r\"year-old\", \"years old\", tweet)\n  tweet = re.sub(r\"yearold\", \"years old\", tweet)\n  tweet = re.sub(r\"yr old\", \"years old\", tweet)\n  tweet = re.sub(r\"PKK\", \"Kurdistan Workers Party\", tweet)\n  tweet = re.sub(r\"MP\", \"madhya pradesh\", tweet)\n  tweet = re.sub(r\"rly\", \"railway\", tweet)\n  tweet = re.sub(r\"CDT\", \"Central Daylight Time\", tweet)\n  tweet = re.sub(r\"sensorsenso\", \"sensor senso\", tweet)\n  tweet = re.sub(r\"pm\", \"\", tweet)\n  tweet = re.sub(r\"PM\", \"\", tweet)\n  tweet = re.sub(r\"nan\", \" \", tweet)\n  tweet = re.sub(r\"terrorismturn\", \"terrorism turn\", tweet)\n  tweet = re.sub(r\"epicente\", \"epicenter\", tweet)\n  tweet = re.sub(r\"epicenterr\", \"epicenter\", tweet)\n  tweet = re.sub(r\"WAwildfire\", \"Washington Wildfire\", tweet)\n  tweet = re.sub(r\"prebreak\", \"pre break\", tweet)\n  tweet = re.sub(r\"nowplaying\", \"now playing\", tweet)\n  tweet = re.sub(r\"RT\", \"retweet\", tweet)\n  tweet = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", tweet)\n  tweet = re.sub(r\"LondonFire\", \"London Fire\", tweet)\n  tweet = re.sub(r\"IDFire\", \"Idaho Fire\", tweet)\n  tweet = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", tweet)\n  tweet = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", tweet)\n  tweet = re.sub(r\"withweapons\", \"with weapons\", tweet)\n  tweet = re.sub(r\"NuclearPower\", \"Nuclear Power\", tweet)\n  tweet = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", tweet)\n  tweet = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", tweet)\n  tweet = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", tweet)\n\n  # Special characters\n  tweet = re.sub(r\"%20\", \" \", tweet)\n  tweet = re.sub(r\"%\", \" \", tweet)\n  tweet = re.sub(r\"@\", \" \", tweet)\n  tweet = re.sub(r\"#\", \" \", tweet)\n  tweet = re.sub(r\"'\", \" \", tweet)\n  tweet = re.sub(r\"\\x89\u00fb_\", \" \", tweet)\n  tweet = re.sub(r\"\\x89\u00fb\u00f2\", \" \", tweet)\n  tweet = re.sub(r\"16yr\", \"16 year\", tweet)\n  tweet = re.sub(r\"re\\x89\u00fb_\", \" \", tweet)\n  tweet = re.sub(r\"\\x89\u00fb\", \" \", tweet)\n  tweet = re.sub(r\"\\x89\u00db\", \" \", tweet)\n  tweet = re.sub(r\"re\\x89\u00db\", \"re \", tweet)\n  tweet = re.sub(r\"re\\x89\u00fb\", \"re \", tweet)\n  tweet = re.sub(r\"\\x89\u00fb\u00aa\", \"'\", tweet)\n  tweet = re.sub(r\"\\x89\u00fb\", \" \", tweet)\n  tweet = re.sub(r\"\\x89\u00fb\u00f2\", \" \", tweet)\n  tweet = re.sub(r\"\\x89\u00db_\", \"\", tweet)\n  tweet = re.sub(r\"\\x89\u00db\u00d2\", \"\", tweet)\n  tweet = re.sub(r\"\\x89\u00db\u00d3\", \"\", tweet)\n  tweet = re.sub(r\"\\x89\u00db\u00cfWhen\", \"When\", tweet)\n  tweet = re.sub(r\"\\x89\u00db\u00cf\", \"\", tweet)\n  tweet = re.sub(r\"China\\x89\u00db\u00aas\", \"China's\", tweet)\n  tweet = re.sub(r\"let\\x89\u00db\u00aas\", \"let's\", tweet)\n  tweet = re.sub(r\"\\x89\u00db\u00f7\", \"\", tweet)\n  tweet = re.sub(r\"\\x89\u00db\u00aa\", \"\", tweet)\n  tweet = re.sub(r\"\\x89\u00db\\x9d\", \"\", tweet)\n  tweet = re.sub(r\"\u00e5_\", \"\", tweet)\n  tweet = re.sub(r\"\\x89\u00db\u00a2\", \"\", tweet)\n  tweet = re.sub(r\"\\x89\u00db\u00a2\u00e5\u00ca\", \"\", tweet)\n  tweet = re.sub(r\"from\u00e5\u00cawounds\", \"from wounds\", tweet)\n  tweet = re.sub(r\"\u00e5\u00ca\", \"\", tweet)\n  tweet = re.sub(r\"\u00e5\u00c8\", \"\", tweet)\n  tweet = re.sub(r\"Jap\u00cc_n\", \"Japan\", tweet)    \n  tweet = re.sub(r\"\u00cc\u00a9\", \"e\", tweet)\n  tweet = re.sub(r\"\u00e5\u00a8\", \"\", tweet)\n  tweet = re.sub(r\"Suru\u00cc\u00a4\", \"Suruc\", tweet)\n  tweet = re.sub(r\"\u00e5\u00c7\", \"\", tweet)\n  tweet = re.sub(r\"\u00e5\u00a33million\", \"3 million\", tweet)\n  tweet = re.sub(r\"\u00e5\u00c0\", \"\", tweet)\n\n  # Contractions\n  tweet = re.sub(r\"he's\", \"he is\", tweet)\n  tweet = re.sub(r\"there's\", \"there is\", tweet)\n  tweet = re.sub(r\"We're\", \"We are\", tweet)\n  tweet = re.sub(r\"That's\", \"That is\", tweet)\n  tweet = re.sub(r\"won't\", \"will not\", tweet)\n  tweet = re.sub(r\"they're\", \"they are\", tweet)\n  tweet = re.sub(r\"Can't\", \"Cannot\", tweet)\n  tweet = re.sub(r\"wasn't\", \"was not\", tweet)\n  tweet = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", tweet)\n  tweet = re.sub(r\"aren't\", \"are not\", tweet)\n  tweet = re.sub(r\"isn't\", \"is not\", tweet)\n  tweet = re.sub(r\"What's\", \"What is\", tweet)\n  tweet = re.sub(r\"haven't\", \"have not\", tweet)\n  tweet = re.sub(r\"hasn't\", \"has not\", tweet)\n  tweet = re.sub(r\"There's\", \"There is\", tweet)\n  tweet = re.sub(r\"He's\", \"He is\", tweet)\n  tweet = re.sub(r\"It's\", \"It is\", tweet)\n  tweet = re.sub(r\"You're\", \"You are\", tweet)\n  tweet = re.sub(r\"I'M\", \"I am\", tweet)\n  tweet = re.sub(r\"Im\", \"I am\", tweet)\n  tweet = re.sub(r\"shouldn't\", \"should not\", tweet)\n  tweet = re.sub(r\"wouldn't\", \"would not\", tweet)\n  tweet = re.sub(r\"i'm\", \"I am\", tweet)\n  tweet = re.sub(r\"I\\x89\u00db\u00aam\", \"I am\", tweet)\n  tweet = re.sub(r\"I'm\", \"I am\", tweet)\n  tweet = re.sub(r\"Isn't\", \"is not\", tweet)\n  tweet = re.sub(r\"Here's\", \"Here is\", tweet)\n  tweet = re.sub(r\"you've\", \"you have\", tweet)\n  tweet = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", tweet)\n  tweet = re.sub(r\"we're\", \"we are\", tweet)\n  tweet = re.sub(r\"what's\", \"what is\", tweet)\n  tweet = re.sub(r\"couldn't\", \"could not\", tweet)\n  tweet = re.sub(r\"we've\", \"we have\", tweet)\n  tweet = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", tweet)\n  tweet = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", tweet)\n  tweet = re.sub(r\"It\\x89\u00db\u00aas\", \"It is\", tweet)\n  tweet = re.sub(r\"Here\\x89\u00db\u00aas\", \"Here is\", tweet)\n  tweet = re.sub(r\"who's\", \"who is\", tweet)\n  tweet = re.sub(r\"I\\x89\u00db\u00aave\", \"I have\", tweet)\n  tweet = re.sub(r\"y'all\", \"you all\", tweet)\n  tweet = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", tweet)\n  tweet = re.sub(r\"would've\", \"would have\", tweet)\n  tweet = re.sub(r\"it'll\", \"it will\", tweet)\n  tweet = re.sub(r\"we'll\", \"we will\", tweet)\n  tweet = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", tweet)\n  tweet = re.sub(r\"We've\", \"We have\", tweet)\n  tweet = re.sub(r\"he'll\", \"he will\", tweet)\n  tweet = re.sub(r\"Y'all\", \"You all\", tweet)\n  tweet = re.sub(r\"Weren't\", \"Were not\", tweet)\n  tweet = re.sub(r\"Didn't\", \"Did not\", tweet)\n  tweet = re.sub(r\"they'll\", \"they will\", tweet)\n  tweet = re.sub(r\"they'd\", \"they would\", tweet)\n  tweet = re.sub(r\"DON'T\", \"DO NOT\", tweet)\n  tweet = re.sub(r\"That\\x89\u00db\u00aas\", \"That is\", tweet)\n  tweet = re.sub(r\"they've\", \"they have\", tweet)\n  tweet = re.sub(r\"i'd\", \"I would\", tweet)\n  tweet = re.sub(r\"should've\", \"should have\", tweet)\n  tweet = re.sub(r\"You\\x89\u00db\u00aare\", \"You are\", tweet)\n  tweet = re.sub(r\"where's\", \"where is\", tweet)\n  tweet = re.sub(r\"Don\\x89\u00db\u00aat\", \"Do not\", tweet)\n  tweet = re.sub(r\"we'd\", \"we would\", tweet)\n  tweet = re.sub(r\"i'll\", \"I will\", tweet)\n  tweet = re.sub(r\"weren't\", \"were not\", tweet)\n  tweet = re.sub(r\"They're\", \"They are\", tweet)\n  tweet = re.sub(r\"Can\\x89\u00db\u00aat\", \"Cannot\", tweet)\n  tweet = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", tweet)\n  tweet = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", tweet)\n  tweet = re.sub(r\"let's\", \"let us\", tweet)\n  tweet = re.sub(r\"it's\", \"it is\", tweet)\n  tweet = re.sub(r\"can't\", \"can not\", tweet)\n  tweet = re.sub(r\"cant\", \"can not\", tweet)\n  tweet = re.sub(r\"don't\", \"do not\", tweet)\n  tweet = re.sub(r\"dont\", \"do not\", tweet)\n  tweet = re.sub(r\"you're\", \"you are\", tweet)\n  tweet = re.sub(r\"i've\", \"I have\", tweet)\n  tweet = re.sub(r\"that's\", \"that is\", tweet)\n  tweet = re.sub(r\"i'll\", \"I will\", tweet)\n  tweet = re.sub(r\"doesn't\", \"does not\", tweet)\n  tweet = re.sub(r\"i'd\", \"I would\", tweet)\n  tweet = re.sub(r\"didn't\", \"did not\", tweet)\n  tweet = re.sub(r\"ain't\", \"am not\", tweet)\n  tweet = re.sub(r\"you'll\", \"you will\", tweet)\n  tweet = re.sub(r\"I've\", \"I have\", tweet)\n  tweet = re.sub(r\"Don't\", \"do not\", tweet)\n  tweet = re.sub(r\"I'll\", \"I will\", tweet)\n  tweet = re.sub(r\"I'd\", \"I would\", tweet)\n  tweet = re.sub(r\"Let's\", \"Let us\", tweet)\n  tweet = re.sub(r\"you'd\", \"You would\", tweet)\n  tweet = re.sub(r\"It's\", \"It is\", tweet)\n  tweet = re.sub(r\"Ain't\", \"am not\", tweet)\n  tweet = re.sub(r\"Haven't\", \"Have not\", tweet)\n  tweet = re.sub(r\"Could've\", \"Could have\", tweet)\n  tweet = re.sub(r\"youve\", \"you have\", tweet)  \n  tweet = re.sub(r\"don\u00e5\u00abt\", \"do not\", tweet)\n\n  return tweet","72a0568d":"df['text'] = df['text'].apply(lambda s : cleaner(s))","710ef1d7":"df_test['text'] = df_test['text'].apply(lambda s : cleaner(s))","4ecb394e":"def remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)\n\nexample=\"I am a #king\"\nprint(remove_punct(example))","02d6b37d":"df['text']=df['text'].apply(lambda x : remove_punct(x))","2b6757d8":"df_test['text']=df_test['text'].apply(lambda x : remove_punct(x))","3a710b01":"df['text']=df['text'].str.replace('   ', ' ')\ndf['text']=df['text'].str.replace('     ', ' ')\ndf['text']=df['text'].str.replace('\\xa0 \\xa0 \\xa0', ' ')\ndf['text']=df['text'].str.replace('  ', ' ')\ndf['text']=df['text'].str.replace('\u2014', ' ')\ndf['text']=df['text'].str.replace('\u2013', ' ')","6c515cd3":"df_test['text']=df_test['text'].str.replace('   ', ' ')\ndf_test['text']=df_test['text'].str.replace('     ', ' ')\ndf_test['text']=df_test['text'].str.replace('\\xa0 \\xa0 \\xa0', ' ')\ndf_test['text']=df_test['text'].str.replace('  ', ' ')\ndf_test['text']=df_test['text'].str.replace('\u2014', ' ')\ndf_test['text']=df_test['text'].str.replace('\u2013', ' ')","36f25fdb":"nltk.download(\"stopwords\")","f06f2f68":"from nltk.corpus import stopwords","5cc110b3":"stop_words = set(stopwords.words(\"english\"))","1151db10":"from collections import defaultdict,Counter","0f13276f":"word_count = Counter(\" \".join(df[df['target']==1]['text']).split()).most_common(100)\nx=[]\ny=[]\nfor word,count in word_count:\n    if (word.casefold() not in stop_words) :\n        x.append(word)\n        y.append(count)\n\nsns.barplot(x=y[:20],y=x[:20])\nplt.title('20 most common words in Disaster tweets')","0cbf1ee8":"word_count = Counter(\" \".join(df[df['target']==0]['text']).split()).most_common(100)\nx=[]\ny=[]\nfor word,count in word_count:\n    if (word.casefold() not in stop_words) :\n        x.append(word)\n        y.append(count)\n\nsns.barplot(x=y[:20],y=x[:20])\nplt.title('20 most common words in Non-disaster tweets')","55dfb764":"word_count = Counter(\" \".join(df_test['text']).split()).most_common(100)\nx=[]\ny=[]\nfor word,count in word_count:\n    if (word.casefold() not in stop_words) :\n        x.append(word)\n        y.append(count)\n\nsns.barplot(x=y[:30],y=x[:30])\nplt.title('21 most common words in Test tweets')","3355bb42":"# Define ngram generator function\ndef generate_ngrams(text, n_gram):\n    token = [token for token in text.lower().split(' ') if token != '' if token not in stop_words]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [' '.join(ngram) for ngram in ngrams]","7de5ca51":"N=50","240cb2e6":"# Bigrams\ntesting_bigrams = defaultdict(int)\n\nfor instance in df_test['text']:\n    for word in generate_ngrams(instance, n_gram=2):\n        testing_bigrams[word] += 1\n   \ndf_testing_bigrams = pd.DataFrame(sorted(testing_bigrams.items(), key=lambda x: x[1])[::-1])","e1c87fa4":"fig_dims = (25, 30)\nfig, ax3 = plt.subplots(figsize=fig_dims)\n\nsns.barplot(y=df_testing_bigrams[0].values[:50], x=df_testing_bigrams[1].values[:50], color='y', ax=ax3)\nplt.tick_params(axis='x', labelsize=13)\nplt.tick_params(axis='y', labelsize=13)\n\nplt.title(f'Top {N} most common bigrams in testing tweets', fontsize=15)\n\nplt.show()","22e71986":"# Bigrams\ndisaster_bigrams = defaultdict(int)\nnondisaster_bigrams = defaultdict(int)\n\nfor instance in df[df['target']==1]['text']:\n    for word in generate_ngrams(instance, n_gram=2):\n        disaster_bigrams[word] += 1\n\nfor instance in df[df['target']==0]['text']:\n    for word in generate_ngrams(instance, n_gram=2):\n        nondisaster_bigrams[word] += 1 \n   \ndf_disaster_bigrams = pd.DataFrame(sorted(disaster_bigrams.items(), key=lambda x: x[1])[::-1])\ndf_nondisaster_bigrams = pd.DataFrame(sorted(nondisaster_bigrams.items(), key=lambda x: x[1])[::-1])","ae7a0665":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(25,30), dpi=80)\nplt.tight_layout()\n\nsns.barplot(y=df_disaster_bigrams[0].values[:N], x=df_disaster_bigrams[1].values[:N], ax=ax1, color='r')\nax1.spines['right'].set_visible(False)\nax1.tick_params(axis='x', labelsize=13)\nax1.tick_params(axis='y', labelsize=13)\n\nsns.barplot(y=df_nondisaster_bigrams[0].values[:N], x=df_nondisaster_bigrams[1].values[:N], ax=ax2, color='b')\nax2.spines['right'].set_visible(False)\nax2.tick_params(axis='x', labelsize=13)\nax2.tick_params(axis='y', labelsize=13)\n\nax1.set_title(f'Top {N} most common bigrams in Disaster tweets', fontsize=15)\nax2.set_title(f'Top {N} most common bigrams in Non-disaster tweets', fontsize=15)\n\nplt.show()\nplt.tight_layout()","d70a813b":"# Trigrams\ntesting_trigrams = defaultdict(int)\n\nfor instance in df_test['text']:\n    for word in generate_ngrams(instance, n_gram=3):\n        testing_trigrams[word] += 1\n   \ndf_testing_trigrams = pd.DataFrame(sorted(testing_trigrams.items(), key=lambda x: x[1])[::-1])","3ff99707":"fig_dims = (25, 30)\nfig, ax3 = plt.subplots(figsize=fig_dims)\n\nsns.barplot(y=df_testing_trigrams[0].values[:50], x=df_testing_trigrams[1].values[:50], color='y', ax=ax3)\nplt.tick_params(axis='x', labelsize=13)\nplt.tick_params(axis='y', labelsize=13)\n\nplt.title(f'Top {N} most common trigrams in testing tweets', fontsize=15)\n\nplt.show()","001aaea2":"# Trigrams\ndisaster_bigrams = defaultdict(int)\nnondisaster_bigrams = defaultdict(int)\n\nfor instance in df[df['target']==1]['text']:\n    for word in generate_ngrams(instance, n_gram=3):\n        disaster_bigrams[word] += 1\n\nfor instance in df[df['target']==0]['text']:\n    for word in generate_ngrams(instance, n_gram=3):\n        nondisaster_bigrams[word] += 1 \n   \ndf_disaster_bigrams = pd.DataFrame(sorted(disaster_bigrams.items(), key=lambda x: x[1])[::-1])\ndf_nondisaster_bigrams = pd.DataFrame(sorted(nondisaster_bigrams.items(), key=lambda x: x[1])[::-1])","15c31437":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(25,20), dpi=80)\nplt.tight_layout()\n\nsns.barplot(y=df_disaster_bigrams[0].values[:N], x=df_disaster_bigrams[1].values[:N], ax=ax1, color='r')\nax1.spines['right'].set_visible(False)\nax1.tick_params(axis='x', labelsize=13)\nax1.tick_params(axis='y', labelsize=13)\n\nsns.barplot(y=df_nondisaster_bigrams[0].values[:N], x=df_nondisaster_bigrams[1].values[:N], ax=ax2, color='b')\nax2.spines['right'].set_visible(False)\nax2.tick_params(axis='x', labelsize=13)\nax2.tick_params(axis='y', labelsize=13)\n\nax1.set_title(f'Top {N} most common trigrams in Disaster tweets', fontsize=15)\nax2.set_title(f'Top {N} most common trigrams in Non-disaster tweets', fontsize=15)\n\nplt.show()\nplt.tight_layout()","b9bc3c9c":"df[['text','target']]","2fdbdbd9":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","4f5b0947":"df['text_without_stopwords'] = df['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))","57c9f82a":"df[['text','text_without_stopwords']]","7b177110":"df_test['text_without_stopwords'] = df_test['text'].apply(lambda x: ' '.join([word for word in x.split() if word not in (stop_words)]))","7ad4d74f":"df_test[['text','text_without_stopwords']]","1d13b371":"training_portion=0.80","7f3c6c63":"train_size = int(df.shape[0]*training_portion)\n\ntrain_sentences = df['text_without_stopwords'][:train_size]\ntrain_labels = df['target'][:train_size]\n\nvalidation_sentences = df['text_without_stopwords'][train_size:]\nvalidation_labels = df['target'][train_size:]\n\nprint(train_size)\nprint(len(train_sentences))\nprint(len(train_labels))\nprint(len(validation_sentences))\nprint(len(validation_labels))","bd5d191e":"tokenizer0 = Tokenizer()\ntokenizer0.fit_on_texts(df['text'])\nword_index = tokenizer0.word_index\nlen(word_index)","0f3cc09e":"vocab_size = 16000\noov_tok = '<OOV>'","279eb0b5":"tokenizer = Tokenizer(num_words=vocab_size, oov_token=oov_tok)\ntokenizer.fit_on_texts(df['text'])\nword_index = tokenizer.word_index","e4f79592":"train_sequences = tokenizer.texts_to_sequences(train_sentences)","75b189d2":"lengths=[]\nfor k in range(len(train_sequences)):\n  lengths.append(len(train_sequences[k]))","414c751a":"pd.DataFrame(lengths, columns=['Lenghts']).describe()","80044bf4":"plt.hist(lengths, bins=27, alpha=0.5)\nplt.show()","baf732e5":"max_length = 20\ntrunc_type = 'post'\npadding_type = 'post'","51a489f6":"train_padded = pad_sequences(train_sequences,maxlen=max_length,padding=padding_type,truncating=trunc_type)","3321a58c":"print(len(train_sequences[0]))\nprint(len(train_padded[0]))\n\nprint(len(train_sequences[1]))\nprint(len(train_padded[1]))\n\nprint(len(train_sequences[10]))\nprint(len(train_padded[10]))","b57b8d09":"validation_sequences = tokenizer.texts_to_sequences(validation_sentences)\nvalidation_padded = pad_sequences(validation_sequences,padding=padding_type,maxlen=max_length,truncating=trunc_type)\n\nprint(len(validation_sequences))\nprint(validation_padded.shape)","568fed10":"test_sequences = tokenizer.texts_to_sequences(df_test['text_without_stopwords'])\ntest_padded = pad_sequences(test_sequences,padding=padding_type,maxlen=max_length,truncating=trunc_type)\n\nprint(len(test_sequences))\nprint(test_padded.shape)","193c0846":"np.unique(train_labels)","a2996628":"from keras.callbacks import ReduceLROnPlateau\nlr_reduction = ReduceLROnPlateau(monitor='val_accuracy',\n                                 patience=1, \n                                 verbose=1, \n                                 factor=0.5, \n                                 min_lr=0.000001)\n\nfrom keras.callbacks import EarlyStopping\nearly_stopping = EarlyStopping(monitor='val_accuracy', \n                               min_delta=0.005,\n                               patience=3, \n                               verbose=1, \n                               mode='auto')","4bc351ae":"embedding_dim = 32\nmodel = tf.keras.Sequential([\n              tf.keras.layers.Embedding(vocab_size,embedding_dim,input_length=max_length),\n              tf.keras.layers.GlobalAveragePooling1D(),\n              tf.keras.layers.Dense(24,activation='relu'),\n              tf.keras.layers.Dropout(0.1),\n              tf.keras.layers.Dense(1,activation='sigmoid')\n])\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nmodel.summary()","6e09edca":"num_epochs = 15\nhistory = model.fit(train_padded,train_labels,epochs=num_epochs,\n                    validation_data=(validation_padded,validation_labels),verbose=1,\n                    callbacks=[lr_reduction])","999a35f0":"def plot_metrics(history):\n  acc=history.history['accuracy']\n  val_acc=history.history['val_accuracy']\n  loss=history.history['loss']\n  val_loss=history.history['val_loss']\n\n  epochs=range(1,len(history.history['accuracy'])+1) # Get number of epochs\n\n  #------------------------------------------------\n  # Plot training and validation accuracy per epoch\n  #------------------------------------------------\n  plt.plot(epochs, acc, 'r')\n  plt.plot(epochs, val_acc, 'b')\n  plt.title('Training and validation accuracy')\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(\"Accuracy\")\n  plt.legend([\"Accuracy\", \"Validation Accuracy\"])\n\n  plt.figure()\n\n  #------------------------------------------------\n  # Plot training and validation loss per epoch\n  #------------------------------------------------\n  plt.plot(epochs, loss, 'r')\n  plt.plot(epochs, val_loss, 'b')\n  plt.title('Training and validation loss')\n  plt.xlabel(\"Epochs\")\n  plt.ylabel(\"Loss\")\n  plt.legend([\"Loss\", \"Validation Loss\"])\n\n  plt.figure()","6f3723d1":"plot_metrics(history)","de853d72":"predicted_val = model.predict(validation_padded, batch_size=32)","f2988286":"predicted_val[:5]","cc833c0a":"class_pred_val= np.round(predicted_val)\nclass_pred_val[:5]","43c3a149":"validation_labels[:5]","2464a8f3":"from sklearn.metrics import classification_report\n\nprint(classification_report(class_pred_val,validation_labels))","96f306e6":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(class_pred_val,validation_labels), display_labels=np.unique(train_labels))\ndisp.plot(cmap='Blues') \nplt.grid(False)","840a1d0c":"tokenizer_glove = Tokenizer()\ntokenizer_glove.fit_on_texts(df['text'])\nword_index = tokenizer_glove.word_index\nlen(word_index)","b8a5439a":"train_glove_sequences=tokenizer_glove.texts_to_sequences(train_sentences)\ntrain_glove_padded = pad_sequences(train_glove_sequences,maxlen=max_length,\n                                   padding=padding_type,truncating=trunc_type)\n\ntrain_glove_padded.shape","047a7ea6":"val_glove_sequences=tokenizer_glove.texts_to_sequences(validation_sentences)\nval_glove_padded = pad_sequences(val_glove_sequences,maxlen=max_length,\n                                 padding=padding_type,truncating=trunc_type)\n\nval_glove_padded.shape","b6701b88":"test_glove_sequences=tokenizer_glove.texts_to_sequences(df_test['text_without_stopwords'])\ntest_glove_padded = pad_sequences(test_glove_sequences,maxlen=max_length,\n                                  padding=padding_type,truncating=trunc_type)\n\ntest_glove_padded.shape","f66943f5":"vocab_size_glove=len(word_index)","4a8664c1":"embeddings_index = {};\nwith open('..\/input\/glove6b100dtxt\/glove.6B.100d.txt') as f:\n    for line in f:\n        values = line.split();\n        word = values[0];\n        coefs = np.asarray(values[1:], dtype='float32');\n        embeddings_index[word] = coefs;","921949b2":"embeddings_matrix = np.zeros((vocab_size_glove+1, 100));\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word);\n    if embedding_vector is not None:\n        embeddings_matrix[i] = embedding_vector;","56cf3be3":"print(len(embeddings_matrix))","21e0bd11":"from tensorflow.keras.optimizers import Adam\n\nmodel_glove2=Sequential()\nmodel_glove2.add(Embedding(vocab_size_glove+1,100,weights=[embeddings_matrix],input_length=max_length,trainable=False))\nmodel_glove2.add(SpatialDropout1D(0.2))\nmodel_glove2.add(LSTM(100, dropout=0.2, recurrent_dropout=0.2))\nmodel_glove2.add(Dense(1, activation='sigmoid'))\n\noptimizer=Adam(learning_rate=5e-3)\nmodel_glove2.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])\nmodel_glove2.summary()","d903545b":"num_epochs = 15\nhistory_glove2 = model_glove2.fit(train_glove_padded,train_labels,epochs=num_epochs,\n                    validation_data=(val_glove_padded,validation_labels),verbose=1,\n                    batch_size=32)","62e55684":"plot_metrics(history_glove2)","cafd2035":"predicted_val = model_glove2.predict(val_glove_padded, batch_size=32)","73751e36":"predicted_val[:5]","f6d4f84a":"class_pred_val_glove= np.round(predicted_val)\nclass_pred_val_glove[:5]","40444fdb":"validation_labels[:5]","c833f078":"from sklearn.metrics import classification_report\n\nprint(classification_report(class_pred_val_glove,validation_labels))","3c626c85":"from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n\ndisp = ConfusionMatrixDisplay(confusion_matrix=confusion_matrix(class_pred_val_glove,validation_labels), display_labels=np.unique(train_labels))\ndisp.plot(cmap='Blues') \nplt.grid(False)","e33547f3":"test_prediction = model_glove2.predict(test_glove_padded)\ntest_prediction = test_prediction.round().astype('int')","8908eefc":"Bigrams analysis of test tweets:","2ced1cba":"Create a new column in train file containing instances without stopwords:","f292eb31":"There are 16871 unique words in the training sentences, we must remember that this number would be higher if we would not have removed the stopwords.","51d6e307":"Create a new column in test file containing instances without stopwords:","b03287ab":"The number of N-grams to compute:","1365af1f":"And after applying the function the text for such instance should be as follows:","88b4eb89":"## Cleaning:\n\nThe tweets contained in the dataset are almost raw, this means we have to get rid of all 'impurities' such as tags, symbols, punctuations, emojis, etc. These does not add significant information to the prediction moreover makes our sentences more subjective. This process comprehend 7 key steps which will make our sentences partially-suit to be used in training of the model.","47708286":"The following lines creates a matrix containing the weights and its dimension is the new vocabulary size by 100 embedding dimension, this will be loaded as the weights of the first embedding layer by setting the argument trainable=False.","767bb5d9":"The first model took 15 seconds to train reaching accuracies of: 98.3% train\/83.1% val.","58cd6d04":"### Train-test split:\nAs we have only 7613 instances in our train dataframe we have to keep a reasonable proportion for validation set because it will not contain a huge  number of instances, 80%-20% would be nice.","6343f863":"## Analysis of common words per class:","0b35025f":"### Recurrent Neural Network by scratch","8288af88":"### Removing multiple spaces:\n\nNow, some sentences cleaned have different types of extra whitespaces, obviusly they don't add anything to the corpus and we will get rid of them with the following lines:","dd21baee":"### Tri-grams:","97a191e8":"As training and validation sets are ready to be used in the training of the model, it's the right moment to do exactly the same for the instances in test set in order to have them properly processed to predict their classes.","50ab520e":"I would like to know any feedback in order to increase the performance of the models or tell me if you found a different one even better!\n\nIf you liked this notebook I would appreciate so much your upvote if you want to see more projects\/tutorials like this one. I encourage you to see my projects portfolio, am sure you will love it.\n\nThank you!","006a18ff":"The same process we just did to training set we have to do in validation set, but this will be done using the tokenizer function fitted in training:","f2f4564f":"Observation: The callback which reduces the learning rate was useful as the training accuracy increases whenever the lr is reduced, however the validation accuracy stays the same in the last epochs, this is why the early stopping will not be used so as not to limit the performance of the model:","30aca998":"### Pre-trained model:\n\nWe will download the Glove pre-trained model for text classification with 100 embedding dimensions, after predicting the classes for the validation set we will compare with those obtained from the previously built scratch model.\n\nAs a first step we have to tokenize again the sentences without limiting the maximum length of them as follows:","589663a6":"Predicting classes for test instances:","d1711dcd":"As we will select the most common words from each topic we have to make sure to avoid selecting the denominated 'stop words' because these will not be relevant in this case, this is why we will import them by downloading from nltk tool and english dictionary:","c7390a71":"### Removing punctuations:\n\nIn this step the there are only a few tweets cleaned that still contain symbols and punctuations, as they don't add key information to the message we will get rid of them, the function which applies such step will be called remove_punct:\n","07ae7b0c":"We can see above they have similar characteristics and we can not say too much about a special feature of any of them, would be better to apply this method but for the words.\n\nThis is why, we are goind to compute and display a histogram of the number of words in the headlines of each topic:","f5693dff":"Now, we can compute the length of each instance tokenized of the training set and see its distribution in order to choose maximum length which does not drop significant information at the end of the sequence:","a1986b97":"It would be interesting to see the number of characters contained in the text of each class, even better showing them as histograms so as to see if there is a specific pattern for any class:","865df3ba":"In the following step we will plot the top most common words in each set we have, disaster tweets, non-disaster tweets and testing tweets:","bde41373":"We can see above the distribution of the classes is slightly unbalanced, this is why we should expect to have sidetrack in the prediction towards class 0.","212ae09a":"Once we have this probabilities we have to convert it to discrete values, either 1 or 0, such task can be achieved by using the np.round function:","4c1e5f58":"### Removing HTML tags:\n\nWe have to consider that some tweets were obtained using web scrapping, using this method the components of a publication are companied by special tags identifying them. As such tags are unuseful we must get rid of them to gather only the text. The function which applies such step will be called remove_html:","70897caf":"### Removing URLs: \nSome tweets either disaster or non-disaster include links 'URLs' which correspond to videos or other webpages containing key information about the subject they are trying to communicate, as we want to clean the sentences we must get rid of them. The function which applies such step will be caled remove_URL:","e3a72e26":"Trigrams analysis of disaster and non-disaster tweets:","4aa43312":"Let's set the vocab size to 16000 so as not to slow down too much the model training later:","c35c734b":"Now let's compute the N-grams in each set already mentioned, the folowing function generate_ngrams will help us with the process: ","b050a3f4":"Now, as we said we will download the weights of Glove 100 dimensional version  from Stanford:","954af0b6":"## Modeling:\n\nIn this step we are going to consider building models by scratch and pre-trained as can be seen below:\n\n- Recurrent Neural Network by scratch.\n- Pre-trained 100 dimensional Glove.\n\nBefore building the models we have to create two contraints or 'callbacks', one to reduce the learning rate whenever the validation accuracy does not increase so as to lead the steps towards the global minimum loss and the second is early stopping which will stop the training if the validation accuracy does not increase after 2 consecutive epochs.","08334163":"Bigrams analysis of disaster and non-disaster tweets:","35e4ad82":"Classification report and confusion matrix:","bbef1301":"### Removing Emojis:\n\nEmojis are an efficient way to show the feeling of the publishers in the message, we could translate the meaning of them to words and help to improve the scope of the message. These could be useful or confuse the algorithm when finding the same feeling for disaster and non-disaster tweets, because of this we prefer to get rid of them, the function which applies such step will be called remove_emoji:","edd89566":"The model took 4 minutes and 33 seconds to train reaching accuracies of: 97.0% train\/89.9% val.","67fc1358":"Classification report and confusion matrix:","817d6b3c":"### Removing numbers:\n\nIn the current project we will only focus on words in order to classify the tweets, obviously the number could be useful because they can mean coordinates of a disaster, code of an accident, hour of accident, number of people killed or also can mean the release year of a videogame, number of followers of an influencer, etc. The use of these numbers in the model can be pending for a next project.","c4370b59":"Above we can see this new model contains around 1.7 million parameters to compute, this would take a long time, but as we have loaded the weights only 80.501 are trainable. In contrast to the scratch model which contains over 576 thousand parameters and obviously have to be computed all of them.\n\nObservation: In this model the callback which reduces the learning rate was not useful as the accuracies didn't improve whenever the lr was reduced, this is why I'm not using them here, also the change in batch size impacted the performance:","5ff4627a":"## Tokenizing","98a3edfe":"We can print the length of the sequences before and after padding, let's see their corresponding for 3 instances of training set:","2896c08f":"### Bi-grams:","f2f1b72d":"Once we have this probabilities we have to convert it to discrete values, either 1 or 0, such task can be achieved by using the np.round function:","c990744b":"Time now to build the model using the Glove-weights, take into account that we will consider the same layers included in the previous model:","a3c34e56":"Let us set such argument to 20, truncation type 'post' and padding type 'post' as follows:","18c1ddcc":"Trigrams analysis of test tweets:","42b3c241":"## Exploratory Data Analysis:","588f7355":"As an example let us print the keyword followed by the tweet of the instance 63 which does correspond to disaster:","cde1ebda":"Now, let's apply this function to all instances in the list 'disaster_index':","4d3c7a7b":"To the instances that correspond to disaster I will add the what is contained in the keyword feature to the end of the tweet so as to get more impactful words in the sentence and help the model to find the common words in disaster tweets. In order to do this we have to obtain the indexes of those instances with target = 1 'Disaster':","44cf4c76":"Let us tokenize all words in our dataset so as to analyze and get an appropriate vocab_size and max_length:","336ccce8":"### Contractions and acronyms:\nPeople world-wide make use of acronyms to speed-up the publishing of a tweet, some of them can be miswritten and others can be decomposed creating words that make sense, this process is exhaustive and requires investing a long time searching the meaning of each one, the function which replaces the contractions and acronyms by the words they stand for will be called cleaner: \n"}}