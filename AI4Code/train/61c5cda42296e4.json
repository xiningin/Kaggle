{"cell_type":{"6fa7e4de":"code","ede9ac37":"code","cee5925b":"code","f28c08c4":"code","3ab7ad43":"code","b57d28fb":"code","a69deec9":"code","b0670fca":"code","d89e9de7":"code","78881674":"code","f641b0f0":"code","b319db0d":"code","86b78428":"code","8be9315e":"code","5f677f43":"code","29f1d97e":"code","136789f5":"code","056fff45":"code","b91bad7e":"code","2d052b6a":"code","f39305a5":"code","5577d9c8":"code","7eb67333":"code","40884263":"code","25173e8a":"code","3f6d10a1":"code","73713137":"code","a86e35cd":"code","7aaf27ac":"code","4284e5df":"code","bd794fc2":"code","be3a1c1c":"code","5de496d5":"code","3517d2c7":"code","1f8980f2":"code","a0198e37":"code","e03bbce9":"code","f405721b":"code","4c66b779":"code","2326d1f3":"code","2122b3a0":"code","66bed639":"code","95c9c4b1":"code","f666274f":"code","227bd769":"code","c547be1b":"code","3bd834d4":"code","e3f21af1":"code","2d98b192":"code","8763e5e7":"code","8ab8b0bc":"code","3596f081":"code","7d7edf4b":"code","dc59ceea":"code","aa085a06":"code","0b38c0ae":"code","edee6450":"code","545c762f":"code","66f4d27e":"code","a13c74de":"code","ec0ea1d0":"code","74d42193":"code","c6ffe25d":"code","a87d9042":"code","06de72a8":"code","a123a964":"code","b81cc061":"code","74df834b":"code","c8280212":"markdown","71193842":"markdown","636fca43":"markdown","10d3592a":"markdown","de4e87c0":"markdown","cf0b53b5":"markdown","112b3ad7":"markdown","243adf4e":"markdown","533c32cc":"markdown","e9a4947d":"markdown","3a4c38dc":"markdown","8d684352":"markdown","c0ad4c15":"markdown","4295d480":"markdown","36084f85":"markdown","792841fe":"markdown","e6ab200e":"markdown","813dab5c":"markdown","6d994ce3":"markdown","5ee71212":"markdown","4262c099":"markdown","f7a6d8d4":"markdown"},"source":{"6fa7e4de":"%%capture\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\ntrain = pd.read_csv('..\/input\/fakenewsvortexbsb\/train_df.csv', sep=';', error_bad_lines=False, quoting=3);","ede9ac37":"train.head(4) #primeiros 4 registros","cee5925b":"train.shape #numero de registros do dataset","f28c08c4":"train.columns.values #colunas do dataset","3ab7ad43":"train[\"manchete\"][80] #amostra do dataset","b57d28fb":"train[\"Class\"][80] #amostra do dataset","a69deec9":"train['Class'].unique()#tipos diferentes de labels ","b0670fca":"from unidecode import unidecode\nexample = train[\"manchete\"][1]\nprint(unidecode(example)) #teste com amsotra do dataset","d89e9de7":"import re\n# aplicamos ambas bibliotecas propostas, unidecode aplica o unicode no texto\n# re retira as pontua\u00e7\u00f5es.\nletters_only=re.sub(\"[^a-zA-Z]\",\" \",unidecode(example)) \nprint(letters_only)","78881674":"lower_case=letters_only.lower() #aplica as letras minusculas para todas as letras nos textos\nwords=lower_case.split()","f641b0f0":"from nltk.corpus import stopwords\n#lista de stop words da lingua portugu\u00easa\nprint (stopwords.words(\"portuguese\"))","b319db0d":"stop = stopwords.words(\"portuguese\") #criar um arry com as stop words do portugu\u00eas","86b78428":"# n\u00e3o \u00e9 possivel aplicar o unicode em uma lista, ent\u00e3o vamos percorrer o array aplciando emc ada registro\nlista_stop = [unidecode(x) for x in stop]  \nprint (lista_stop)","8be9315e":"print(words)","5f677f43":"#Filtro das palavras n\u00e3o stop words presentes no texto\nwords=[w for w in words if not w in lista_stop] \nprint(words)","29f1d97e":"# Fun\u00e7\u00e3o review_to_words realiza todas as transforma\u00e7\u00f5es que foram realizadas antes\ndef review_to_words(raw_review):\n    raw_review = unidecode(raw_review)\n    raw_review.lstrip('Jovem Pan')\n    letters_only=re.sub(\"[^a-zA-Z]\",\" \",raw_review)\n    words=letters_only.lower().split()\n    meaningful_words=[w for w in words if not w in lista_stop]\n    return(' '.join(meaningful_words))","136789f5":"#testando a fun\u00e7\u00e3o\nclean_review=review_to_words(train['manchete'][1])\nprint(clean_review)","056fff45":"# Pegando o valor da dimen\u00e7\u00e3o dos dados para passar no for logo abaixo\nnum_reviews=train['manchete'].size\nprint (num_reviews)","b91bad7e":"# loop para aplicar as transforma\u00e7\u00f5es em cada registro da coluna manifestacao_clean do dataset\nclean_train_review=[]\nfor i in range(0,num_reviews):\n    clean_train_review.append(review_to_words(train['manchete'][i]))","2d052b6a":"from sklearn.feature_extraction.text import CountVectorizer\n# configurar os parametros do WordtoVec\/Tokeninza\u00e7\u00e3o e criar o objeto\nvectorizer=CountVectorizer(analyzer='word',tokenizer=None,preprocessor = None, stop_words = None,max_features = 7000)\n# aplicar WordtoVec\/Tokeninza\u00e7\u00e3o\ntrain_data_features=vectorizer.fit_transform(clean_train_review)\n# aplicar a estrutura de dados numpy array\ntrain_data_features=train_data_features.toarray()","f39305a5":"train_data_features.shape","5577d9c8":"train_data_features[1]","7eb67333":"# vocabulario de todas das palavaras mais importantes de todas requisi\u00e7\u00f5es\nvcab=vectorizer.get_feature_names()\n#print(vcab)","40884263":"train_y = train[\"Class\"]","25173e8a":"from sklearn.model_selection import train_test_split\n# Split dos dados em treino e valida\u00e7\u00e3o\nX_train, X_test, y_train, y_test = train_test_split(train_data_features, train_y, test_size=0.25, random_state=42)","3f6d10a1":"from sklearn.neighbors import KNeighborsClassifier\n# Cria\u00e7\u00e3o do objeto que corresponde o modelo com o hiperparametros especificados\nmodel = KNeighborsClassifier(n_neighbors=3)\n#Treinamento do modelo\n%time model = model.fit( X_train, y_train )","73713137":"# Prevendo os dados de teste com o modelo treinado\nresult = model.predict(X_test)","a86e35cd":"from sklearn.metrics import accuracy_score, classification_report\n# Acurr\u00e1cia absoluta dos resultados\naccuracy_score(y_test, result)","7aaf27ac":"# Essa fun\u00e7\u00e3o realiza testes de recall e F1-score do modelo, importante n\u00e3o apenas se basear na acurr\u00e1cia e precis\u00e3o.\nprint (classification_report(y_test, result))","4284e5df":"from sklearn.metrics import confusion_matrix\nconfusion_matrix(result, y_test, labels=y_train.unique())","bd794fc2":"import seaborn as sn\nimport matplotlib.pyplot as plt\narray = confusion_matrix(result, y_test, labels=y_train.unique())\narray = array.astype('float') \/ array.sum(axis=1)[:, np.newaxis] #normaliza\u00e7\u00e3o dos valores \ndf_cm = pd.DataFrame(array, index = y_train.unique(), #cria um data frame para base ao gr\u00e1fico\n                  columns = y_train.unique())\nplt.figure(figsize = (10,7)) \nsn.heatmap(df_cm, annot=True, cmap=sn.light_palette((210, 90, 60), input=\"husl\"))","be3a1c1c":"from sklearn.naive_bayes import MultinomialNB\nnb = MultinomialNB()\n%time nb = nb.fit( X_train, y_train )\nresult = nb.predict(X_test)","5de496d5":"accuracy_score(y_test, result)","3517d2c7":"# Essa fun\u00e7\u00e3o realiza testes de recall e F1-score do modelo, importante n\u00e3o apenas se basear na acurr\u00e1cia e precis\u00e3o.\nprint (classification_report(y_test, result))","1f8980f2":"from sklearn.tree import DecisionTreeClassifier\nclf2 = DecisionTreeClassifier(random_state=42)\n%time clf2 = clf2.fit( X_train, y_train )\nresult = clf2.predict(X_test)","a0198e37":"# Acurr\u00e1cia absoluta dos resultados\naccuracy_score(y_test, result)","e03bbce9":"# Essa fun\u00e7\u00e3o realiza testes de recall e F1-score do modelo, importante n\u00e3o apenas se basear na acurr\u00e1cia e precis\u00e3o.\nprint (classification_report(y_test, result))","f405721b":"from sklearn.ensemble import RandomForestClassifier\nforest = RandomForestClassifier(random_state=42)\n%time forest = forest.fit( X_train, y_train )\nresult = forest.predict(X_test)","4c66b779":"# Acurr\u00e1cia absoluta dos resultados\naccuracy_score(y_test, result)","2326d1f3":"# Essa fun\u00e7\u00e3o realiza testes de recall e F1-score do modelo, importante n\u00e3o apenas se basear na acurr\u00e1cia e precis\u00e3o.\nprint (classification_report(y_test, result))","2122b3a0":"from sklearn.ensemble import GradientBoostingClassifier\nclf3 = GradientBoostingClassifier(random_state=42)\n%time clf3 = clf3.fit( X_train, y_train )\nresult = clf3.predict(X_test)","66bed639":"# Acurr\u00e1cia absoluta dos resultados\naccuracy_score(y_test, result)","95c9c4b1":"# Essa fun\u00e7\u00e3o realiza testes de recall e F1-score do modelo, importante n\u00e3o apenas se basear na acurr\u00e1cia e precis\u00e3o.\nprint (classification_report(y_test, result))","f666274f":"from sklearn.linear_model import LogisticRegression\nclf = LogisticRegression(random_state=42, solver='lbfgs')\n%time clf = clf.fit( X_train, y_train )\nresult = clf.predict(X_test)","227bd769":"# Acurr\u00e1cia absoluta dos resultados\naccuracy_score(y_test, result)","c547be1b":"# Essa fun\u00e7\u00e3o realiza testes de recall e F1-score do modelo, importante n\u00e3o apenas se basear na acurr\u00e1cia e precis\u00e3o.\nprint (classification_report(y_test, result))","3bd834d4":"from sklearn.svm import SVC\nclf4 = SVC(random_state=42)\n%time clf4 = clf4.fit( X_train, y_train )\nresult = clf4.predict(X_test)","e3f21af1":"# Acurr\u00e1cia absoluta dos resultados\naccuracy_score(y_test, result)","2d98b192":"# Essa fun\u00e7\u00e3o realiza testes de recall e F1-score do modelo, importante n\u00e3o apenas se basear na acurr\u00e1cia e precis\u00e3o.\nprint (classification_report(y_test, result))","8763e5e7":"def prevendo_noticias(string, model):\n    to_array=[]\n    to_array.append(review_to_words(string))\n    sample_final=vectorizer.transform(to_array)\n    sample_final=sample_final.toarray()\n    result = model.predict(sample_final)\n    if  result[0] == 1:\n        label = 'Fake News'\n    else:\n        label = 'Verdadeira'\n        \n    return label, string","8ab8b0bc":"prevendo_noticias('Aras: decis\u00e3o do STF n\u00e3o deveria valer para casos conclu\u00eddos', forest)","3596f081":"prevendo_noticias('Bolsonaro pessoalmente incend\u00eaia a amazonia e mata as girafas', forest)","7d7edf4b":"prevendo_noticias('Jornalista joga \u00e1gua benta em Temer e ele admite que impeachment foi golpe', forest)","dc59ceea":"# Cria\u00e7\u00e3o do objeto que corresponde o modelo com o hiperparametros especificados\nfrom sklearn.ensemble import RandomForestClassifier\nmodel_final = RandomForestClassifier(random_state=42)\n#Treinamento do modelo\n#%time model_final = model_final.fit( train_data_features, train_y )","aa085a06":"param_grid = { \n    'n_estimators': [100, 300, 500, 800, 1000],\n    'criterion': ['gini', 'entropy'],\n    'bootstrap': [True, False]\n}","0b38c0ae":"from sklearn.model_selection import GridSearchCV\nCV_rf = GridSearchCV(estimator=model_final, param_grid=param_grid, cv= 5, scoring='accuracy', n_jobs=-1)\nCV_rf = CV_rf.fit( train_data_features, train_y )","edee6450":"CV_rf.best_params_","545c762f":"model_fit = RandomForestClassifier(random_state=42, bootstrap= True, criterion= 'entropy', n_estimators= 800)#.fit( train_data_features, train_y )","66f4d27e":"%time model_fit = model_fit.fit( X_train, y_train )\nresult = model_fit.predict(X_test)","a13c74de":"accuracy_score(y_test, result)","ec0ea1d0":"%time model_final =  model_fit.fit( train_data_features, train_y )","74d42193":"# Importando o dados de teste\ntest = pd.read_csv('..\/input\/fakenewsvortexbsb\/sample_submission.csv', sep=';', error_bad_lines=False, quoting=3);","c6ffe25d":"test.head(5)","a87d9042":"#Filtro das palavras n\u00e3o stop words presentes no texto\nnum_reviews, = test['Manchete'].shape\nprint(num_reviews)","06de72a8":"# loop para aplicar as transforma\u00e7\u00f5es em cada registro da coluna manifestacao_clean do dataset\nclean_test_review=[]\nfor i in range(0,num_reviews):\n    clean_test_review.append(review_to_words(test['Manchete'][i]))","a123a964":"# aplicar WordtoVec\/Tokeninza\u00e7\u00e3o\ntest_data_features = vectorizer.transform(clean_test_review)\ntest_data_features=test_data_features.toarray()","b81cc061":"# Prevendo os dados de teste com o modelo treinado\nresult_test = model_final.predict(test_data_features)","74df834b":"# Criando um dataframe com os resultados obtidos para submiter na competi\u00e7\u00e3o\nminha_sub = pd.DataFrame({'index': test.index, 'Category': result_test})\n# Criando um arquivo csv com os resultados\nminha_sub.to_csv('submission.csv', index=False)","c8280212":"## Criando uma submission ","71193842":"### Regress\u00e3o logist\u00edca ","636fca43":"### Boosting Trees","10d3592a":"![](https:\/\/s3.ap-south-1.amazonaws.com\/techleer\/204.png)","de4e87c0":"![](https:\/\/pessoalex.files.wordpress.com\/2019\/04\/svm_2.png?w=397&h=292)","cf0b53b5":"# PR\u00c1TICA DETEC\u00c7\u00c3O DE FAKE NEWS","112b3ad7":"## M\u00c9TODO BAG OF WORDS (BOW)","243adf4e":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn:ANd9GcTWTUa_WsfLC5Q-GnwjpF8CJzP-pVwvyXh9sgZ7ouavsOv-KzE_EA)","533c32cc":"![](https:\/\/littleml.files.wordpress.com\/2017\/03\/boosted-trees-process.png)","e9a4947d":"**Como posso fazer um computador entender textos?** \n\nPrimeiramente temos que converter textos normais para um tipo de representa\u00e7\u00e3o num\u00e9rica para que Machine Learning possa processar, uma abordagem tradicional \u00e9 a utiliza\u00e7\u00e3o da t\u00e9cnica Bag of Words que consiste em usar o vocubulario de todos o documentos analisados (Corpus) e quebrar os textos ao ponto de lidar apenas com a frequ\u00eancia de palavras, como diz literalmente o nome da t\u00e9cninca traduzida do portugu\u00eas essas informa\u00e7\u00f5es viram um \"Saco de palavras\", \u00e9 levado em conta a frequencia utilizadas de certas palavras dentro di voculabulario. Segue exemplo em duas senten\u00e7as: \n\nFrase 1: \"O gato agarrou o cachorro\"\n\nFrase 2: \"O dono agarrou o cachorro e o gato\"\n\n\nPara as duas senten\u00e7as, o vocabulo segue:\n\n{ O, GATO, AGARROU, CACHORRO, DONO, E}\n\nPara conseguir o \"saco de palavras\", contamos o numero de vezes que a palavra ocorre em cada Frase. Na senten\u00e7a 1, \"O\" aparece duas vezes, tamb\u00e9m as palavras \"GATO\", \"AGARROU\" e \"CACHORRO\" aparece uma vez, ent\u00e3o o novo registro da sente\u00e7a 1 num\u00e9ricamente fica:\n\nVocab = { O, GATO, AGARROU, CACHORRO, DONO, E}\n\nFrase 1: { 2, 1, 1, 1, 1, 0, 0}\n\nFrase 2: { 3, 1, 1, 1, 1, 1, 1}\n\nPara n\u00e3o hiperdimencionalizar o vetor de colunas e causar riscos de performances no modelo, vamos escolher um espa\u00e7o amostal m\u00e1ximo de voc\u00e1bulos. Abaixo, utilizaremos as 5000 palavras mais frequentes presentes no corpus (lembrando que tiramos as stops words).","3a4c38dc":"### Random Forests","8d684352":"![](https:\/\/www.xoriant.com\/blog\/wp-content\/uploads\/2017\/08\/Decision-Trees-modified-1.png)","c0ad4c15":"## Arvore de decis\u00e3o","4295d480":"## Valida\u00e7\u00e3o do Modelo","36084f85":"## CRIA\u00c7\u00c3O DOS MODELOS","792841fe":"## ARGUMENTA\u00c7\u00c3O DE DADOS","e6ab200e":"![](https:\/\/d1rwhvwstyk9gu.cloudfront.net\/2019\/03\/Random-Forest-Algorithm.jpg)","813dab5c":"![](https:\/\/helloacm.com\/wp-content\/uploads\/2016\/03\/logistic-regression-example.jpg)","6d994ce3":"## Modelo KNN (k-vizinhos ou K-nearest neighbours)","5ee71212":"## SVM","4262c099":"## Utilizando o modelo","f7a6d8d4":"## Naive Bayes"}}