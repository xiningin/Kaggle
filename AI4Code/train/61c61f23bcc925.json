{"cell_type":{"9833e07a":"code","08da5f83":"code","e0f29d26":"code","d41561fe":"code","1dfc2dcd":"code","dc390640":"code","fa0b5a00":"code","36a45da4":"code","88122e32":"code","a494ed33":"code","2f082e82":"markdown","e28ea08d":"markdown","057fa8d2":"markdown","dbeb6588":"markdown","c5759355":"markdown","bd7720c1":"markdown","d7c93af6":"markdown","6581b0ca":"markdown","61b57c73":"markdown","9b1eaefe":"markdown","3a817164":"markdown"},"source":{"9833e07a":"import numpy as np \nimport pandas as pd \nfrom sklearn.ensemble import RandomForestClassifier\nfrom boruta import BorutaPy","08da5f83":"train = pd.read_csv(\"..\/input\/application_train.csv\")\ntrain.shape","e0f29d26":"train = pd.get_dummies(train, drop_first=True, dummy_na=True)\ntrain.shape","d41561fe":"features = [f for f in train.columns if f not in ['TARGET','SK_ID_CURR']]\nlen(features)","1dfc2dcd":"train[features] = train[features].fillna(train[features].mean()).clip(-1e9,1e9)","dc390640":"X = train[features].values\nY = train['TARGET'].values.ravel()","fa0b5a00":"rf = RandomForestClassifier(n_jobs=-1, class_weight='balanced', max_depth=5)","36a45da4":"boruta_feature_selector = BorutaPy(rf, n_estimators='auto', verbose=2, random_state=4242, max_iter = 50, perc = 90)\nboruta_feature_selector.fit(X, Y)","88122e32":"X_filtered = boruta_feature_selector.transform(X)\nX_filtered.shape","a494ed33":"final_features = list()\nindexes = np.where(boruta_feature_selector.support_ == True)\nfor x in np.nditer(indexes):\n    final_features.append(features[x])\nprint(final_features)","2f082e82":"All categorical values will be one-hot encoded.","e28ea08d":"Replace all missing values with the Mean.","057fa8d2":"And we create a list of the feature names if we would like to use them at a later stage.","dbeb6588":"Get all feature names from the dataset","c5759355":"So I hope you enjoyed my very first Kaggle Kernel :-)\nLet me know if you have any feedback or suggestions.","bd7720c1":"Get the final dataset *X* and labels *Y*","d7c93af6":"Next we setup Boruta. It uses the *scikit-learn* interface as much as possible so we can use *fit(X, y), transform(X), fit_transform(X, y)*. I'll let it run for a maximum of *max_iter = 50* iterations. With *perc = 90* a threshold is specified. The lower the threshold the more features will be selected. I usually use a percentage between 80 and 90. ","6581b0ca":"Next we load only the 'application_train' data as this is to demonstrate Boruta only. ","61b57c73":"Next we setup the *RandomForrestClassifier* as the estimator to use for Boruta. The *max_depth* of the tree is advised on the Boruta Github page to be between 3 to 7.","9b1eaefe":"After Boruta has run we can transform our dataset.","3a817164":"There are many different method's to select the important features from a dataset. In this notebook I will show a quick way to select important features with the use of Boruta.\n\nBoruta tries to find all relevant features that carry information to make an accurate classification. You can read more about Boruta [here](http:\/\/danielhomola.com\/2015\/05\/08\/borutapy-an-all-relevant-feature-selection-method\/)\n\nLet's start by doing all necessary imports."}}