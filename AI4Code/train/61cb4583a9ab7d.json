{"cell_type":{"0bc2ac04":"code","e2f95ef6":"code","4decd5bc":"code","9696629c":"code","ca10fab3":"code","c2d9df07":"code","8d3246fa":"code","ea16b4da":"code","e12583af":"code","f153cafb":"code","d591445a":"code","c95e64f5":"code","d3311d1d":"code","c12f3371":"code","66b4bac0":"code","07d23ec0":"code","1a668588":"code","256c8048":"code","6bd196f4":"code","956c483b":"code","2f372292":"code","38039b8a":"code","84109ab3":"code","2f686823":"code","79e8d9ee":"code","9b682237":"code","6984ab66":"code","8939c6d2":"code","e3c9c0f2":"code","4887d81c":"code","84332585":"code","f5e25698":"code","858a592a":"code","aa330b98":"code","0b9db41a":"markdown","76427338":"markdown","470260ca":"markdown","5df4b619":"markdown","4d84587b":"markdown"},"source":{"0bc2ac04":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n# https:\/\/towardsdatascience.com\/machine-learning-basics-with-the-k-nearest-neighbors-algorithm-6a6e71d01761\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e2f95ef6":"import os\nfrom time import time\nimport errno\nimport shutil\nimport matplotlib.pyplot as plt\nfrom sklearn.datasets import fetch_lfw_people\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report, log_loss, accuracy_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.decomposition import PCA\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.neighbors import NearestNeighbors\nfrom sklearn.model_selection import GridSearchCV\nimport seaborn as sns","4decd5bc":"def copy(src, dest):\n    try:\n        shutil.copytree(src, dest)\n    except OSError as e:\n        if e.errno == errno.ENOTDIR:\n            shutil.copy(src, dest)\n        else:\n            print('Directory not copied. Error: %s' % e)\n            \nsrc = '..\/input\/'\ndest = '..\/LFW\/lfw_home'\ncopy(src,dest) \nprint(os.listdir('..\/input'))\nprint(os.listdir('..\/input\/lfwpeople'))\n\n###### write_available_area\nprint(os.listdir('..\/LFW'))\nprint(os.listdir('..\/LFW\/lfw_home'))\nprint(os.listdir('..\/LFW\/lfw_home\/lfwpeople'))   \n\npath = '..\/LWF' ","9696629c":"# path = '..\/LWF' \nlfw_people=fetch_lfw_people(data_home=path,min_faces_per_person=50, resize=0.5)","ca10fab3":"# lfw_people = lfw_dataset\nn_samples, h, w = lfw_people.images.shape\n\nX = lfw_people.data\nn_features = X.shape[1]\n\n# the label to predict is the id of the person\ny = lfw_people.target\ntarget_names = lfw_people.target_names\nn_classes = target_names.shape[0]\nprint(\"Total dataset size:\")\nprint(\"\\tnumber of samples: %d\" % n_samples)\nprint(\"\\tnumber of features: %d\" % n_features)\nprint(\"\\tnumber of classes: %d\" % n_classes)","c2d9df07":"N=[]\nfor i in range(len(target_names)):\n    N+=[i]\nmapping=dict(zip(target_names,N)) \nreverse_mapping=dict(zip(N,target_names))\ndef mapper(value):\n    return reverse_mapping[value]","8d3246fa":"x = lfw_people.data\nnp.isnan(np.sum(x))\nnp.isnan(np.sum(y))","ea16b4da":"sns.countplot(x =\"target\",data = lfw_people)","e12583af":"print(X.shape, y.shape)\nX.shape","f153cafb":"indeces = []\nfor i in range(300, X.shape[0]):\n    if y[i] == 3:\n        indeces.append(i)\nprint(X.shape, y.shape)\nX = np.delete(X, indeces,0)\ny = np.delete(y, indeces, 0)\nprint(X.shape, y.shape)","d591445a":"X_train, X_test, y_train, y_test = train_test_split(\n    X, y, test_size=0.30, random_state=42)","c95e64f5":"from sklearn.multiclass import OneVsRestClassifier\nKnn_model =  KNeighborsClassifier(n_neighbors = 4)\nKnn_model.fit(X_train, y_train)\npredicted = Knn_model.predict(X_test)\ncm = confusion_matrix(y_test, predicted) #model biased (class = 3, data undersampled)\nsns.heatmap(cm, annot=True)","d3311d1d":"classi_report = classification_report(y_test,predicted)\nprint(classi_report)\nprint( accuracy_score(y_test,predicted))","c12f3371":"true_values =[ mapper(i) for i in y_test]\npredicted_values = [mapper(i) for i in predicted]","66b4bac0":"Knn_model_kdtree = KNeighborsClassifier(n_neighbors = 4,algorithm = 'kd_tree', p=1)\nKnn_model_kdtree.fit(X_train, y_train)\npredicted = Knn_model_kdtree.predict(X_test)\ncm = confusion_matrix(y_test, predicted)\nsns.heatmap(cm, annot=True)","07d23ec0":"# kd_tree = KDTree(X_train)\n# knn_neigh = kd_tree.query(X_train,10)\n# len(knn_neigh)\nclassi_report = classification_report(y_test,predicted)\nprint(classi_report)","1a668588":"pca = PCA(n_components=150,random_state=42)\npca.fit(X_train)\nX_train_pca = pca.transform(X_train)\nX_test_pca = pca.transform(X_test)","256c8048":"svm_model = SVC(random_state=42,C=1, kernel='linear')\nsvm_model.fit(X_train_pca, y_train)\npredicted = svm_model.predict(X_test_pca)\ncm = confusion_matrix(y_test, predicted)\nsns.heatmap(cm, annot=True)","6bd196f4":"classi_report = classification_report(y_test, predicted)\nprint(classi_report)","956c483b":"Knn_model_kdtree.fit(X_train_pca, y_train)\n# predicted = Knn_model_kdtree.predict(X_test_pca)\n# model = OPN\npredicted = best_model.predict(X_test_pca)\ncm = confusion_matrix(y_test, predicted)\nsns.heatmap(cm, annot=True)","2f372292":"classi_report = classification_report(y_test, predicted)\nprint(classi_report)","38039b8a":"true_values =[ mapper(i) for i in y_test]\npredicted_values = [mapper(i) for i in predicted]","84109ab3":"def plot_faces(images, n_row, n_col, true_label, predicted_label, title):\n    \"\"\"Helper function to plot a gallery of portraits\"\"\"\n    \n    plt.figure(figsize=(2.5 * n_col, 3.3 * n_row))\n#     plt.subplots_adjust(0.6, 0.5, 1.5, 1.5)\n    for i in range(n_row * n_col):\n        plt.subplot(n_row, n_col, i + 1)\n#         print(\"True:\",true_label[i])\n        plt.title(f'Predicted: {predicted_label[i]}')\n        plt.imshow(images[i].reshape((h, w)), cmap=plt.cm.gray)\n        plt.xlabel(true_label[i])\n#         print(\"Predicted: \",predicted_label[i])\n        plt.xticks(())\n        plt.yticks(())\n    plt.suptitle(title)\n    plt.tight_layout()\n    plt.show()","2f686823":"true_values =[ mapper(i) for i in y_test]\npredicted_values = [mapper(i) for i in predicted]\nplot_faces(X_test[:6],2,3,true_values[:6], predicted_values[:6], title='Test Data')","79e8d9ee":"fpr = {}\ntpr = {}\nthresh ={}\ncolors = ['blue','green','red','yellow','pink','cyan','black','pink','olivedrab','darkviolet','orangered','olive']\nn_class = 12\npred_prob = Knn_model_kdtree.predict_proba(X_test_pca)\n\nfor i in range(n_class):    \n    fpr[i], tpr[i], thresh[i] = roc_curve(y_test, pred_prob[:,i], pos_label=i)\n    plt.plot(fpr[i], tpr[i], linestyle='--',color=colors[i], label=f'Class {i}')\n    \n\nplt.title('Test data')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive rate')\nplt.legend(loc='best')\nplt.savefig('Multiclass ROC',dpi=300)","9b682237":"# import sklearn.metrics as metrics\nfrom sklearn.metrics import roc_auc_score,roc_curve\nfpr = {}\ntpr = {}\nthresh ={}\ncolors = ['blue','green','red','yellow','pink','cyan','black','pink','olivedrab','darkviolet','orangered','olive']\nn_class = 12\npred_prob = Knn_model_kdtree.predict_proba(X_train_pca)\n\nfor i in range(n_class):    \n    fpr[i], tpr[i], thresh[i] = roc_curve(y_train, pred_prob[:,i], pos_label=i)\n    plt.plot(fpr[i], tpr[i], linestyle='--',color=colors[i], label=f'Class {i}')\n    \n\nplt.title('Train data curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive rate')\nplt.legend(loc='best')\nplt.savefig('Multiclass ROC',dpi=300)","6984ab66":"true_values =[ mapper(i) for i in y_train]\npredicted = Knn_model_kdtree.predict(X_train_pca)\npredicted_values = [mapper(i) for i in predicted]\nplot_faces(X_test[:6],2,3,true_values[:6], predicted_values[:6], title='Train Data')","8939c6d2":"params = {'n_neighbors':[5,6,7,8,9,10],\n          'leaf_size':[1,2,3,5],\n          'weights':['uniform', 'distance'],\n          'algorithm':['auto', 'ball_tree','kd_tree','brute'],\n          'n_jobs':[-1]}\n\n#Create new KNN object\nknn_2 =KNeighborsClassifier()\n#Use GridSearch\nclf = GridSearchCV(knn_2, params, cv=5)\nbest_model = clf.fit(X_train_pca,y_train)\nbest_model.best_estimator_","e3c9c0f2":"params = {'C': list(range(100,121)), \n          'kernel': ['rbf','linear'],\n         'random_state':[42]}\n#Making models with hyper parameters sets\nmodel1 = GridSearchCV(SVC(), param_grid=params)\nmodel1.fit(X_train_pca, y_train)","4887d81c":"model1.best_estimator_","84332585":"p = model1.predict(X_test_pca)\ncm = confusion_matrix(y_test,p)\nprint(classification_report(y_test, p))","f5e25698":"sns.heatmap(cm,annot=True)","858a592a":"from sklearn.ensemble import RandomForestClassifier\n#making the instance\nmodel=RandomForestClassifier()\n#hyper parameters set\nparams = {'criterion':['gini','entropy'],\n          'n_estimators':list(range(31,101)),\n          'min_samples_leaf':[1,2,3],\n          'min_samples_split':[3,4,5,6,7], \n          'random_state':[42],\n          'n_jobs':[-1]}\n#Making models with hyper parameters sets\nmodel1 = GridSearchCV(model, param_grid=params, n_jobs=-1)\n#learning\nmodel1.fit(X_train_pca, y_train)","aa330b98":"from sklearn.pipeline import make_pipeline\ndef pca():\n    pca = PCA(n_components=150,random_state=42)\n    pca.fit(X_train)\n    X_train = pca.transform(X_train)\n    X_test = pca.transform(X_test)\nbase_models = [SVC(C=100, kernel='linear', random_state=42), KNeighborsClassifier(n_neighbors = 1,algorithm = 'kd_tree'),KNeighborsClassifier(n_neighbors = 1), RandomForestClassifier(random_state=42)]\n\nlevel1_model = LogisticRegression()\nfinal_model = StackingClassifier(estimators = base_models, ##Base estimators which will be stacked together\n                                 final_estimator = level1_model,\n                                 cv = 5)\nmodel = makepipleine(pca(), final_model)\nmodel.fit(X_train, y_train)","0b9db41a":"# **KDTree**","76427338":"# **KNN Classifier**","470260ca":"**Best Hpyer params search**","5df4b619":"# **PCA**","4d84587b":"## Visualing the train and test data"}}