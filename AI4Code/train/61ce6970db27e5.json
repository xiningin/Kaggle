{"cell_type":{"904c17ab":"code","20f2e2a0":"code","c83e6ef9":"code","979338f0":"code","f7fefa82":"code","68735167":"code","60594a86":"code","1d4ce7b1":"code","55a3ae98":"code","8ecd92bd":"code","33b6dd71":"code","c2b42cc4":"code","76f8b17f":"code","677a9f81":"code","2f5950e7":"code","617c0038":"code","2e297dab":"code","dec713cf":"code","50392a4d":"code","59391db4":"code","8c649592":"code","13c9e4b0":"code","cda919b1":"code","d15488d9":"code","748b5188":"code","37306531":"code","2c8fe043":"code","ea4208b7":"code","4d4d8d64":"code","36d67f63":"code","2e27b1c7":"code","a5d39572":"code","e82fbbf4":"code","5a2b5139":"markdown"},"source":{"904c17ab":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","20f2e2a0":"!pip install bs4","c83e6ef9":"from bs4 import BeautifulSoup","979338f0":"from nltk.corpus import stopwords","f7fefa82":"import numpy as np\nimport pandas as pd\n\n\n!pip install fuzzywuzzy\n!pip install python-Levenshtein\n!pip install Distance\n!pip install -U sentence-transformers\n!pip install https:\/\/github.com\/explosion\/spacy-models\/releases\/download\/en_core_web_lg-2.2.5\/en_core_web_lg-2.2.5.tar.gz\n!pip install en_core_web_lg\n\nimport re\nimport os\nimport spacy\nimport distance\nimport numpy as np\nimport pandas as pd\nimport en_core_web_lg\nfrom fuzzywuzzy import fuzz\nfrom bs4 import BeautifulSoup\nfrom tqdm.notebook import tqdm\nfrom wordcloud import WordCloud\nfrom prettytable import PrettyTable\nfrom scipy.spatial.distance import cosine\nfrom scipy.sparse import save_npz, load_npz, hstack\nfrom sentence_transformers import SentenceTransformer\n\nimport xgboost as xgb\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss, confusion_matrix\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","68735167":"import nltk\nnltk.download('stopwords')\nnltk.download('averaged_perceptron_tagger')","60594a86":"# read file\ndf_train = pd.read_csv('..\/input\/datacomp\/ProjectData2.csv')\ndf_train.head()","1d4ce7b1":"df_train.info()","55a3ae98":"def preprocess(q):\n  # Firstly, we convert to lowercase and remove trailing and leading spaces\n  q = str(q).lower().strip()\n\n  # Replace certain special characters with their string equivalents\n  q = q.replace('%', ' percent')\n  q = q.replace('$', ' dollar ')\n  q = q.replace('\u20b9', ' rupee ')\n  q = q.replace('\u20ac', ' euro ')\n  q = q.replace('@', ' at ')\n\n  # The pattern '[math]' appears around 900 times in the whole dataset.\n  q = q.replace('[math]', '')\n\n  # Replacing some numbers with string equivalents (not perfect, can be done better to account for more cases)\n  q = q.replace(',000,000,000 ', 'b ')\n  q = q.replace(',000,000 ', 'm ')\n  q = q.replace(',000 ', 'k ')\n  q = re.sub(r'([0-9]+)000000000', r'\\1b', q)\n  q = re.sub(r'([0-9]+)000000', r'\\1m', q)\n  q = re.sub(r'([0-9]+)000', r'\\1k', q)\n\n  # Decontracting words\n  # https:\/\/en.wikipedia.org\/wiki\/Wikipedia%3aList_of_English_contractions\n  # https:\/\/stackoverflow.com\/a\/19794953\n  contractions = { \n    \"ain't\": \"am not\",\n    \"aren't\": \"are not\",\n    \"can't\": \"can not\",\n    \"can't've\": \"can not have\",\n    \"'cause\": \"because\",\n    \"could've\": \"could have\",\n    \"couldn't\": \"could not\",\n    \"couldn't've\": \"could not have\",\n    \"didn't\": \"did not\",\n    \"doesn't\": \"does not\",\n    \"don't\": \"do not\",\n    \"hadn't\": \"had not\",\n    \"hadn't've\": \"had not have\",\n    \"hasn't\": \"has not\",\n    \"haven't\": \"have not\",\n    \"he'd\": \"he would\",\n    \"he'd've\": \"he would have\",\n    \"he'll\": \"he will\",\n    \"he'll've\": \"he will have\",\n    \"he's\": \"he is\",\n    \"how'd\": \"how did\",\n    \"how'd'y\": \"how do you\",\n    \"how'll\": \"how will\",\n    \"how's\": \"how is\",\n    \"i'd\": \"i would\",\n    \"i'd've\": \"i would have\",\n    \"i'll\": \"i will\",\n    \"i'll've\": \"i will have\",\n    \"i'm\": \"i am\",\n    \"i've\": \"i have\",\n    \"isn't\": \"is not\",\n    \"it'd\": \"it would\",\n    \"it'd've\": \"it would have\",\n    \"it'll\": \"it will\",\n    \"it'll've\": \"it will have\",\n    \"it's\": \"it is\",\n    \"let's\": \"let us\",\n    \"ma'am\": \"madam\",\n    \"mayn't\": \"may not\",\n    \"might've\": \"might have\",\n    \"mightn't\": \"might not\",\n    \"mightn't've\": \"might not have\",\n    \"must've\": \"must have\",\n    \"mustn't\": \"must not\",\n    \"mustn't've\": \"must not have\",\n    \"needn't\": \"need not\",\n    \"needn't've\": \"need not have\",\n    \"o'clock\": \"of the clock\",\n    \"oughtn't\": \"ought not\",\n    \"oughtn't've\": \"ought not have\",\n    \"shan't\": \"shall not\",\n    \"sha'n't\": \"shall not\",\n    \"shan't've\": \"shall not have\",\n    \"she'd\": \"she would\",\n    \"she'd've\": \"she would have\",\n    \"she'll\": \"she will\",\n    \"she'll've\": \"she will have\",\n    \"she's\": \"she is\",\n    \"should've\": \"should have\",\n    \"shouldn't\": \"should not\",\n    \"shouldn't've\": \"should not have\",\n    \"so've\": \"so have\",\n    \"so's\": \"so as\",\n    \"that'd\": \"that would\",\n    \"that'd've\": \"that would have\",\n    \"that's\": \"that is\",\n    \"there'd\": \"there would\",\n    \"there'd've\": \"there would have\",\n    \"there's\": \"there is\",\n    \"they'd\": \"they would\",\n    \"they'd've\": \"they would have\",\n    \"they'll\": \"they will\",\n    \"they'll've\": \"they will have\",\n    \"they're\": \"they are\",\n    \"they've\": \"they have\",\n    \"to've\": \"to have\",\n    \"wasn't\": \"was not\",\n    \"we'd\": \"we would\",\n    \"we'd've\": \"we would have\",\n    \"we'll\": \"we will\",\n    \"we'll've\": \"we will have\",\n    \"we're\": \"we are\",\n    \"we've\": \"we have\",\n    \"weren't\": \"were not\",\n    \"what'll\": \"what will\",\n    \"what'll've\": \"what will have\",\n    \"what're\": \"what are\",\n    \"what's\": \"what is\",\n    \"what've\": \"what have\",\n    \"when's\": \"when is\",\n    \"when've\": \"when have\",\n    \"where'd\": \"where did\",\n    \"where's\": \"where is\",\n    \"where've\": \"where have\",\n    \"who'll\": \"who will\",\n    \"who'll've\": \"who will have\",\n    \"who's\": \"who is\",\n    \"who've\": \"who have\",\n    \"why's\": \"why is\",\n    \"why've\": \"why have\",\n    \"will've\": \"will have\",\n    \"won't\": \"will not\",\n    \"won't've\": \"will not have\",\n    \"would've\": \"would have\",\n    \"wouldn't\": \"would not\",\n    \"wouldn't've\": \"would not have\",\n    \"y'all\": \"you all\",\n    \"y'all'd\": \"you all would\",\n    \"y'all'd've\": \"you all would have\",\n    \"y'all're\": \"you all are\",\n    \"y'all've\": \"you all have\",\n    \"you'd\": \"you would\",\n    \"you'd've\": \"you would have\",\n    \"you'll\": \"you will\",\n    \"you'll've\": \"you will have\",\n    \"you're\": \"you are\",\n    \"you've\": \"you have\"\n  }\n\n  q_decontracted = []\n\n  for word in q.split():\n    if word in contractions:\n      word = contractions[word]\n  \n    q_decontracted.append(word)\n\n  q = ' '.join(q_decontracted)\n  q = q.replace(\"'ve\", \" have\")\n  q = q.replace(\"n't\", \" not\")\n  q = q.replace(\"'re\", \" are\")\n  q = q.replace(\"'ll\", \" will\")\n\n  # Removing HTML tags\n  q = BeautifulSoup(q)\n  q = q.get_text()\n\n  # Remove punctuations\n  pattern = re.compile('\\W')\n  q = re.sub(pattern, ' ', q).strip()\n\n  return q\n\n\"\"\"## Extracting features\nWe will extract following features:\n- **Token features**\n  1. **q1_len**: Number of characters in question 1\n  1. **q2_len**: Number of characters in question 2\n  1. **q1_words**: Number of words in question 1\n  1. **q2_words**: Number of words in question 2\n  1. **words_total**: Sum of **q1_words** and **q2_words**\n  1. **words_common**: Number of words which occur in question 1 and two, reapeated occurances are not counted\n  1. **words_shared**: Fraction of **words_common** to **words_total**\n  1. **cwc_min**: This is the ratio of the number of common words to the length of the smaller question\n  1. **cwc_max**: This is the ratio of the number of common words to the length of the larger question\n  1. **csc_min**: This is the ratio of the number of common stop words to the smaller stop word count among the two questions\n  1. **csc_max**: This is the ratio of the number of common stop words to the larger stop word count among the two questions\n  1. **ctc_min**: This is the ratio of the number of common tokens to the smaller token count among the two questions\n  1. **ctc_max**: This is the ratio of the number of common tokens to the larger token count among the two questions\n  1. **last_word_eq**: 1 if the last word in the two questions is same, 0 otherwise\n  1. **first_word_eq**: 1 if the first word in the two questions is same, 0 otherwise\n  1. **num_common_adj**: This is the number of common adjectives in question1 and question2\n  1. **num_common_prn**: This is the number of common proper nouns in question1 and question2\n  1. **num_common_n**: This is the number of nouns (non-proper) common in question1 and question2\n- **Fuzzy features**\n  1. **fuzz_ratio**: fuzz_ratio score from fuzzywuzzy\n  1. **fuzz_partial_ratio**: fuzz_partial_ratio from fuzzywuzzy\n  1. **token_sort_ratio**: token_sort_ratio from fuzzywuzzy\n  1. **token_set_ratio**: token_set_ratio from fuzzywuzzy\n- **Length features**\n  1. **mean_len**: Mean of the length of the two questions (number of words)\n  1. **abs_len_diff**: Absolute difference between the length of the two questions (number of words)\n  1. **longest_substr_ratio**: Ratio of the length of the longest substring among the two questions to the length of the smaller question\n### Defining functions to extract features\n\"\"\"\n\n# Receives question1 and question2 from one row in DataFrame\n# Computes token features, removes stopwords and performs stemming\n# Returns an array of shape (num_features,)\ndef get_token_features(q1, q2):\n  # Safe div to avoid division by 0 exception\n  safe_div = 0.0001\n\n  # Getting NLTK  stop words set\n  stop_words = stopwords.words('english')\n  \n  # Adding these after word cloud inspection\n  stop_words.append('difference')\n  stop_words.append('different')\n  stop_words.append('best')\n\n  # Initializing stemmer\n  stemmer = PorterStemmer()\n\n  # Initializing feature array\n  token_features = [0.0] * 18\n\n  # Tokenizing\n  q1 = q1.split()\n  q2 = q2.split()\n\n  # Stop words in q1 and q2\n  q1_stops = set([word for word in q1 if word in stop_words])\n  q2_stops = set([word for word in q2 if word in stop_words])\n  common_stops = q1_stops & q2_stops\n\n  # Removing stop words\n  q1 = [word for word in q1 if word not in stop_words]\n  q2 = [word for word in q2 if word not in stop_words]\n\n  # Stem\n  # Is redundant but this design change was made much later and \n  # I don't feel like changing the entire function for it.\n  # For now, computationally inefficient though it may be, it will do.\n  q1_stemmed = ' '.join([word for word in q1])\n  q2_stemmed = ' '.join([word for word in q2])\n\n  if len(q1) == 0 or len(q2) == 0:\n    return (token_features, q1_stemmed, q2_stemmed)\n\n  # PoS features\n  # Uses off the shelf NLTK tag set\n\n  q1_tagged = nltk.pos_tag(q1)\n  q2_tagged = nltk.pos_tag(q2)\n\n  # We are looking for:\n  # 1) JJ\/JJR\/JJS: Adjectives\n  # 2) NNP\/NNPS: Proper nouns\n  # 3) NN\/NNS: Nouns (non-proper)\n\n  q1_adj = set()\n  q2_adj = set()\n  q1_prn = set()\n  q2_prn = set()\n  q1_n = set()\n  q2_n = set()\n\n  # Compute question1 PoS features\n  for word in q1_tagged:\n    if word[1] == 'JJ' or word[1] == 'JJR' or word[1] == 'JJS':\n      q1_adj.add(word[0])\n    elif word[1] == 'NNP' or word[1] == 'NNPS':\n      q1_prn.add(word[0])\n    elif word[1] == 'NN' or word[1] == 'NNS':\n      q1_n.add(word[0])\n\n  # Compute question2 PoS features\n  for word in q2_tagged:\n    if word[1] == 'JJ' or word[1] == 'JJR' or word[1] == 'JJS':\n      q2_adj.add(word[0])\n    elif word[1] == 'NNP' or word[1] == 'NNPS':\n      q2_prn.add(word[0])\n    elif word[1] == 'NN' or word[1] == 'NNS':\n      q2_n.add(word[0])\n      \n  # num_common_adj\n  token_features[15] = len(q1_adj & q2_adj)\n\n  # num_common_prn\n  token_features[16] = len(q1_prn & q2_prn)\n\n  # num_common_n\n  token_features[17] = len(q1_n & q2_n)\n\n  # We do this here because converting to set looses order of words\n  # last_word_eq\n  token_features[13] = int(q1[-1] == q2[-1])\n\n  # first_word_eq\n  token_features[14] = int(q1[0] == q2[0])\n\n  # Now we convert the questions into sets, this looses order but removes duplicate words\n  q1 = set(q1)\n  q2 = set(q2)\n  common_tokens = q1 & q2\n\n  # Sets are still iterables, order of words won't change the number of characters\n  # q1_len\n  token_features[0] = len(q1_stemmed) * 1.0\n\n  # q2_len\n  token_features[1] = len(q2_stemmed) * 1.0\n\n  # q1_words\n  token_features[2] = len(q1) * 1.0\n\n  # q2_words\n  token_features[3] = len(q2) * 1.0\n\n  # words_total\n  token_features[4] = token_features[2] + token_features[3]\n\n  # Common words\n  q1_words = set(q1)\n  q2_words = set(q2)\n  common_words = q1_words & q2_words\n  \n  # words_common\n  token_features[5] = len(common_words) * 1.0\n\n  # words_shared\n  token_features[6] = token_features[5] \/ (token_features[4] + safe_div)\n\n  # cwc_min\n  token_features[7] = token_features[5] \/ (min(token_features[2], token_features[3]) + safe_div)\n\n  # cwc_max\n  token_features[8] = token_features[5] \/ (max(token_features[2], token_features[3]) + safe_div)\n\n  # csc_min\n  token_features[9] = (len(common_stops) * 1.0) \/ (min(len(q1_stops), len(q2_stops)) + safe_div)\n\n  # csc_max\n  token_features[10] = (len(common_stops) * 1.0) \/ (max(len(q1_stops), len(q2_stops)) + safe_div)\n\n  # ctc_min\n  token_features[11] = (len(common_tokens) * 1.0) \/ (min(len(q1), len(q2)) + safe_div)\n\n  # ctc_max\n  token_features[12] = (len(common_tokens) * 1.0) \/ (max(len(q1), len(q2)) + safe_div) \n\n  return (token_features, q1_stemmed, q2_stemmed)\n\n\n# Computes fuzzy features\n# Returns an array of shape (n_features,)\ndef get_fuzzy_features(q1, q2):\n  # Initilzing feature array\n  fuzzy_features = [0.0] * 4\n\n  # fuzz_ratio\n  fuzzy_features[0] = fuzz.QRatio(q1, q2)\n\n  # fuzz_partial_ratio\n  fuzzy_features[1] = fuzz.partial_ratio(q1, q2)\n\n  # token_sort_ratio\n  fuzzy_features[2] = fuzz.token_sort_ratio(q1, q2)\n\n  # token_set_ratio\n  fuzzy_features[3] = fuzz.token_set_ratio(q1, q2)\n\n  return fuzzy_features\n\n\n# Computes length features\n# Returns an array of shape (n_features,)\ndef get_length_features(q1, q2):\n  # Safe div to avoid division by 0 exception\n  safe_div = 0.0001\n\n  # Initialzing feature array\n  length_features = [0.0] * 3\n\n  q1_list = q1.strip(' ')\n  q2_list = q2.strip(' ')\n\n  # mean_len\n  length_features[0] = (len(q1_list) + len(q2_list)) \/ 2\n\n  # abs_len_diff\n  length_features[1] = abs(len(q1_list) - len(q2_list))\n\n  # Get substring length\n  substr_len = distance.lcsubstrings(q1, q2, positions=True)[0]\n\n  # longest_substr_ratio\n  if substr_len == 0:\n    length_features[2] = 0\n  else:\n    length_features[2] = substr_len \/ (min(len(q1_list), len(q2_list)) + safe_div)\n\n  return length_features\n\n\n# Receives data set and performs cleaning, feature extractions\n# Transforms data set by adding feature columns\n# Returns transformed DataFrame\ndef extract_features(data):\n  # First, lets call the preprocess function on question1 and question2\n  df_train['Marking Scheme'] = df_train['Marking Scheme'].apply(preprocess)\n  df_train['Student Response'] = df_train['Student Response'].apply(preprocess)\n\n  # Get token features, token_features is an array of shape (n_rows, data)\n  # where data is a tuple of containing (n_features, q1_stemmed, q2_stemmed)\n  # token_features, q1_stemmed, q2_stemmed = data.apply(lambda x: get_token_features(x['question1'], x['question2']), axis=1)\n  token_features = data.apply(lambda x: get_token_features(x['Marking Scheme'], x['Student Response']), axis=1)\n  \n  q1_stemmed = list(map(lambda x: x[1], token_features))\n  q2_stemmed = list(map(lambda x: x[2], token_features))\n  token_features = list(map(lambda x: x[0], token_features))\n\n  df_train['Marking Scheme'] = q1_stemmed\n  df_train['Student Response'] = q2_stemmed\n\n  # Creating new feature columns for token features\n  df_train['q1_len'] = list(map(lambda x: x[0], token_features))\n  df_train['q2_len'] = list(map(lambda x: x[1], token_features))\n  df_train['q1_words'] = list(map(lambda x: x[2], token_features))\n  df_train['q2_words'] = list(map(lambda x: x[3], token_features))\n  df_train['words_total'] = list(map(lambda x: x[4], token_features))\n  df_train['words_common'] = list(map(lambda x: x[5], token_features))\n  df_train['words_shared'] = list(map(lambda x: x[6], token_features)) \n  df_train['cwc_min'] = list(map(lambda x: x[7], token_features))\n  df_train['cwc_max'] = list(map(lambda x: x[8], token_features))\n  df_train['csc_min'] = list(map(lambda x: x[9], token_features))\n  df_train['csc_max'] = list(map(lambda x: x[10], token_features))\n  df_train['ctc_min'] = list(map(lambda x: x[11], token_features))\n  df_train['ctc_max'] = list(map(lambda x: x[12], token_features))\n  df_train['last_word_eq'] = list(map(lambda x: x[13], token_features))\n  df_train['first_word_eq'] = list(map(lambda x: x[14], token_features))\n  df_train['num_common_adj'] = list(map(lambda x: x[15], token_features))\n  df_train['num_common_prn'] = list(map(lambda x: x[16], token_features))\n  df_train['num_common_n'] = list(map(lambda x: x[17], token_features))\n\n  # Get fuzzy features, fuzzy_features is an array of shape (n_rows, n_features)\n  fuzzy_features = df_train.apply(lambda x: get_fuzzy_features(x['Marking Scheme'], x['Student Response']), axis=1)\n\n  # Creating new feature columns for fuzzy features\n  df_train['fuzz_ratio'] = list(map(lambda x: x[0], fuzzy_features))\n  df_train['fuzz_partial_ratio'] = list(map(lambda x: x[1], fuzzy_features))\n  df_train['token_sort_ratio'] = list(map(lambda x: x[2], fuzzy_features))\n  df_train['token_set_ratio'] = list(map(lambda x: x[3], fuzzy_features))\n\n  # Get length features, length_features is an array of shape (n_rows, n_features)\n  length_features = df_train.apply(lambda x: get_length_features(x['Marking Scheme'], x['Student Response']), axis=1)\n\n  # Creating new feature columns for length features\n  df_train['mean_len'] = list(map(lambda x: x[0], length_features))\n  df_train['abs_len_diff'] = list(map(lambda x: x[1], length_features))\n  df_train['longest_substr_ratio'] = list(map(lambda x: x[2], length_features))\n\n  return data\n","8ecd92bd":"from nltk.stem import PorterStemmer","33b6dd71":"df_train = extract_features(df_train)","c2b42cc4":"df_train.head()","76f8b17f":"df_train.info()","677a9f81":"df_train['Marking Scheme'] = df_train['Marking Scheme'].apply(lambda x: str(x))\ndf_train['Student Response'] = df_train['Student Response'].apply(lambda x: str(x))\n\n#test_data['question1'] = test_data['question1'].apply(lambda x: str(x))\n#test_data['question2'] = test_data['question2'].apply(lambda x: str(x))","2f5950e7":"import xgboost as xgb\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss, confusion_matrix\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n","617c0038":"# Initialize vectorizer\nvectorizer = TfidfVectorizer(lowercase=False)\n\n# Put all questions (1 and 2) into one list\nResponse = list(list(df_train['Marking Scheme']) + list(df_train['Student Response']))\n\n# Fit the vectorizer\nprint('Fitting')\nvectorizer.fit(Response)\n\n# Vectorizing question1\nprint('Vectorizing question1')\nr1_vecs_tfidf_train = vectorizer.transform(df_train['Marking Scheme'].values)\n#r1_vecs_tfidf_test = vectorizer.transform(df_train['Student Response'].values)\n\n# Vectorizing question2\nprint('Vectorizing question2')\nr2_vecs_tfidf_train = vectorizer.transform(df_train['Student Response'].values)\n#r2_vecs_tfidf_test = vectorizer.transform(test_data['question2'].values)\n\n# Mapping terms to their IDF values\nprint('Mapping terms to their IDF values')\nidf = dict(zip(vectorizer.get_feature_names(), vectorizer.idf_))\n\nprint('Converting to DataFrames')\nr1_tfidf_train = pd.DataFrame.sparse.from_spmatrix(r1_vecs_tfidf_train)\nr2_tfidf_train = pd.DataFrame.sparse.from_spmatrix(r2_vecs_tfidf_train)\n\n#r1_tfidf_test = pd.DataFrame.sparse.from_spmatrix(r1_vecs_tfidf_test)\n#r2_tfidf_test = pd.DataFrame.sparse.from_spmatrix(r2_vecs_tfidf_test)\n\nr1_tfidf_train.shape\n","2e297dab":"df_train.head()","dec713cf":"from tqdm import trange, tqdm","50392a4d":"\"\"\"### IDF weighted average Word2Vec\"\"\"\n\n# Initializing pre-trained vectors\nprint('Loading pre-trained word vectors model')\nnlp = en_core_web_lg.load()\n\n# Initialize list to store question vectors\nr1_train_vecs = []\nr2_train_vecs = []\n#q1_test_vecs = []\n#q2_test_vecs = []\n\n\n# Iterate over all questions in TRAIN question1 column\nprint('Processing Marking Scheme')\nfor q in tqdm(list(df_train['Marking Scheme'])):\n  # Get vectors for all words in question\n  doc = nlp(q)\n\n  # This is where we store the averaged vector, each is of length 300\n  mean_vec = np.zeros((len(doc[0].vector)))\n\n  # Iterate over all words in question\n  for word in doc:\n    # Get word vector\n    vector = word.vector\n\n    # If word in IDF vocabulary\n    if str(word) in idf:\n      idf_weight = idf[str(word)]\n    else:\n      idf_weight = 0\n    \n    # Add IDF weighted vector to mean vector\n    mean_vec += vector * idf_weight\n  \n  # Divide mean vector by number of words in question (averaged vector)\n  mean_vec \/= len(doc)\n\n  # Append mean vector to list of mean vectors\n  r1_train_vecs.append(mean_vec)\n\n\n# Iterate over all questions in TRAIN question2 column\nprint('Processing Student Response')\nfor q in tqdm(list(df_train['Student Response'])):\n  # Get vectors for all words in question\n  doc = nlp(q)\n\n  # This is where we store the averaged vector, each is of length 300\n  mean_vec = np.zeros((len(doc[0].vector)))\n\n  # Iterate over all words in question\n  for word in doc:\n    # Get word vector\n    vector = word.vector\n\n    # If word in IDF vocabulary\n    if str(word) in idf:\n      idf_weight = idf[str(word)]\n    else:\n      idf_weight = 0\n    \n    # Add IDF weighted vector to mean vector\n    mean_vec += vector * idf_weight\n  \n  # Divide mean vector by number of words in question (averaged vector)\n  mean_vec \/= len(doc)\n\n  # Append mean vector to list of mean vectors\n  r2_train_vecs.append(mean_vec) \n\n\n\n# Creating DataFrames for these, (we can merge them with our main DataFrame later)\nprint('Converting to DataFrames')\nr1_w2v_train = pd.DataFrame(r1_train_vecs, index=df_train.index)\nr2_w2v_train = pd.DataFrame(r2_train_vecs, index=df_train.index)\n\n#q1_w2v_test = pd.DataFrame(q1_test_vecs, index=test_data.index)\n#q2_w2v_test = pd.DataFrame(q2_test_vecs, index=test_data.index)","59391db4":"r2_w2v_train.head()","8c649592":"def plot_confusion_matrices(true, pred):\n  # Normal confusion matrix\n  conf = confusion_matrix(true, pred)\n  \n  # Precision matrix\n  # Precision: Out of all the points predicted of class 'A', how many are actually class 'A'\n  # A column in the confusion matrix represents the data points predicted as the same class\n  # So we divide all data points by the sum of points in their respective columns\n  precision = (conf.T \/ conf.sum(axis=1)).T\n\n  # Recall matrix\n  # Recall: Out of all the points which are of class 'A', how many were predicted to be of class 'A'\n  # A row in the confusion matrix represents the data points of the same class\n  # So we divide all data points by the sum of points in their respective rows\n  recall = (conf \/ conf.sum(axis=0))\n\n  # Plotting the matrices\n  plt.figure(figsize=(24,6))\n\n  # Set axes labels\n  labels = ['0', '1']\n\n  # Set colourmap\n  cmap = sns.light_palette('purple')\n\n  # Plot normal confusion matrix\n  plt.subplot(1, 3, 1)\n  sns.heatmap(conf, cmap=cmap, annot=True, fmt='d', xticklabels=labels, yticklabels=labels)\n  plt.xlabel('Predicted')\n  plt.ylabel('Actual')\n  plt.title('Confusion Matrix')\n\n  # Plot precision confusion matrix\n  plt.subplot(1, 3, 2)\n  sns.heatmap(precision, cmap=cmap, annot=True, fmt='.3f', xticklabels=labels, yticklabels=labels)\n  plt.xlabel('Predicted')\n  plt.ylabel('Actual')\n  plt.title('Precision Matrix')\n\n  # Plot recall confusion matrix\n  plt.subplot(1, 3, 3)\n  sns.heatmap(recall, cmap=cmap, annot=True, fmt='.3f', xticklabels=labels, yticklabels=labels)\n  plt.xlabel('Predicted')\n  plt.ylabel('Actual')\n  plt.title('Recall Matrix')\n\n  plt.show()","13c9e4b0":"\ndf_Label = df_train['Answer Rating']","cda919b1":"df_real = df_train.copy()","d15488d9":"df_real.head()","748b5188":"df_train = df_real","37306531":"#df_train.drop(['Answer Rating', 'Comprehension', 'Questions'], inplace=True, axis=1)","2c8fe043":"df_train.head()","ea4208b7":"df_train.head()","4d4d8d64":"# Train Test Split\nfrom sklearn.model_selection import train_test_split\n\n","36d67f63":" X_train, X_test, y_train, y_test = train_test_split(df_train,df_Label, test_size=0.30, random_state=42)","2e27b1c7":", X_test, y_train, y_test = train_test_split(r1_w2v_train,df_Label, test_size=0.30, random_state=42)","a5d39572":" X_train, X_test, y_train, y_test = train_test_split(r2_w2v_train,df_Label, test_size=0.30, random_state=42)","e82fbbf4":"# # Reduce training set size if running into memory issues\nr1_w2v_train = r1_w2v_train[:150001]\nr2_w2v_train = r2_w2v_train[:150001]\ntrain_labels_truncated = train_labels[:150001]\ntrain_data_truncated = train_data[:150001]\n# train_labels_truncated = train_labels\n\n# Calculating cosine similarity\n# Initialize arrays to store cosine values\ntrain_cosine = np.zeros(len(train_labels_truncated))\ntest_cosine = np.zeros(len(test_labels))\n\n# Set iteration counter\ni = 0\n\n# Iterating over each pair of training data questions\nfor q1, q2 in zip(q1_w2v_train.values, q2_w2v_train.values):\n  # Computing and storing cosine similarity\n  train_cosine[i] = cosine(q1, q2)\n  i += 1\n\n# Set iteration counter\ni = 0\n\n# Iterating over each pair of test data questions\nfor r1, r2 in zip(r1_w2v_test.values, r2_w2v_test.values):\n  # Computing and storing cosine similarity\n  test_cosine[i] = cosine(r1, r2)\n  i += 1\n\n# There may be some NaN values in our cosine similarities. This happens when the denominator is 0\n# We replace NaN values with 0, indicating no relationship between vectors\ntrain_cosine = np.where(np.isnan(train_cosine), 0, train_cosine)\ntest_cosine = np.where(np.isnan(test_cosine), 0, test_cosine)\n\n# Let's standardize our features. We won't be standardizing the 'last_word_eq' and 'first_word_eq'\n# as these are more like a yes\/no indicator\n# I am not standardizing the main dataframe itself as I want to preserve the actual values if I need them later\n\n# Initialize StandardScaler\nscaler = StandardScaler()\n\n# Train data\n# Getting all columns except the two specified ones\nscaled = train_data_truncated.loc[:, (train_data_truncated.columns != 'last_word_eq') & (train_data_truncated.columns != 'first_word_eq')]\nscaled = scaler.fit_transform(scaled)\n\n# We horizontally stack our standardized features, 'last_word_eq', 'first_word_eq' and cosine similarity of train_data_truncated\nmodel_4_train = np.column_stack((scaled, train_data_truncated['last_word_eq'], train_data_truncated['first_word_eq'], train_cosine))\n\n# Test data\nscaled = test_data.loc[:, (test_data.columns != 'last_word_eq') & (test_data.columns != 'first_word_eq')]\nscaled = scaler.transform(scaled)\n\n# We horizontally stack our standardized features, 'last_word_eq', 'first_word_eq' and cosine similarity of test_data\nmodel_4_test = np.column_stack((scaled, test_data['last_word_eq'], test_data['first_word_eq'], test_cosine))\n\n# Now let's add our word vectors to our data model\n# We have the word vectors stored in a DataFrame\n# Let's horizontally stack with our data matrix in q1q2 order \nmodel_4_train = np.hstack((model_4_train, q1_w2v_train.values, q2_w2v_train.values))\nmodel_4_test = np.hstack((model_4_test, q1_w2v_test.values, q2_w2v_test.values))","5a2b5139":"### Create Word Vectors"}}