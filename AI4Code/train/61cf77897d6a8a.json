{"cell_type":{"618c5577":"code","eda8e521":"code","1e63afc0":"code","3ed2dfb6":"code","df861183":"code","e982127f":"code","fbb447b4":"code","58d301cd":"code","7a2473c1":"code","cb0b3dc4":"code","29e44ab4":"code","7021a4ad":"code","a1dfb186":"code","46db9f30":"code","cc68891d":"code","4976b310":"code","56f8360c":"code","c8d7adf3":"code","155ef6f4":"code","8bee60ff":"code","95db570c":"code","acc056f9":"code","52df4474":"code","3e25db53":"code","e0482c67":"code","fa87b726":"code","161a2c9e":"code","852f13be":"code","febdda91":"code","a84dee73":"code","9b0390d2":"code","4654a984":"markdown","48a26c9c":"markdown","f3b4e996":"markdown","a0242d02":"markdown","e89a146a":"markdown","9e41baca":"markdown","c05eb00d":"markdown","d30cfd07":"markdown","a1086156":"markdown","a1a91f3a":"markdown","f67b97fd":"markdown","e2a8a8b3":"markdown","b87f56e4":"markdown","6bbc7d6b":"markdown","6ffb82e5":"markdown","042953a6":"markdown","4b35f4fe":"markdown","3fc77fac":"markdown","5e05b740":"markdown","abc44bd4":"markdown","783c7e50":"markdown","b88e2d72":"markdown","5da779e3":"markdown","7290c92a":"markdown","0d8d4c2e":"markdown","d6b01a90":"markdown","da2d43b2":"markdown","b8cde4a6":"markdown","05994812":"markdown"},"source":{"618c5577":"# Importing Libraries\nimport os\nimport pandas as pd\nimport numpy as np\nimport plotly_express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport gc\nimport warnings\nwarnings.filterwarnings('ignore')\nfrom lightgbm import LGBMRegressor\nimport joblib\nfrom sklearn.metrics import mean_squared_error","eda8e521":"# Kaggle cwd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1e63afc0":"sales = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_evaluation.csv')\nsales.name = 'sales'\ncalendar = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\ncalendar.name = 'calendar'\nprices = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\nprices.name = 'prices'","3ed2dfb6":"for d in range(1942,1970):\n    col = 'd_' + str(d)\n    sales[col] = 0\n    sales[col] = sales[col].astype(np.int16)","df861183":"def downcast(df):\n    cols = df.dtypes.index.tolist()\n    types = df.dtypes.values.tolist()\n    for i,t in enumerate(types):\n        if 'int' in str(t):\n            if df[cols[i]].min() > np.iinfo(np.int8).min and df[cols[i]].max() < np.iinfo(np.int8).max:\n                df[cols[i]] = df[cols[i]].astype(np.int8)\n            elif df[cols[i]].min() > np.iinfo(np.int16).min and df[cols[i]].max() < np.iinfo(np.int16).max:\n                df[cols[i]] = df[cols[i]].astype(np.int16)\n            elif df[cols[i]].min() > np.iinfo(np.int32).min and df[cols[i]].max() < np.iinfo(np.int32).max:\n                df[cols[i]] = df[cols[i]].astype(np.int32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.int64)\n        elif 'float' in str(t):\n            if df[cols[i]].min() > np.finfo(np.float16).min and df[cols[i]].max() < np.finfo(np.float16).max:\n                df[cols[i]] = df[cols[i]].astype(np.float16)\n            elif df[cols[i]].min() > np.finfo(np.float32).min and df[cols[i]].max() < np.finfo(np.float32).max:\n                df[cols[i]] = df[cols[i]].astype(np.float32)\n            else:\n                df[cols[i]] = df[cols[i]].astype(np.float64)\n        elif t == np.object:\n            if cols[i] == 'date':\n                df[cols[i]] = pd.to_datetime(df[cols[i]], format='%Y-%m-%d')\n            else:\n                df[cols[i]] = df[cols[i]].astype('category')\n    return df  \n\nsales = downcast(sales)\nprices = downcast(prices)\ncalendar = downcast(calendar)","e982127f":"df = pd.melt(sales, id_vars=['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'], var_name='d', value_name='sold').dropna()","fbb447b4":"df = pd.merge(df, calendar, on='d', how='left')\ndf = pd.merge(df, prices, on=['store_id','item_id','wm_yr_wk'], how='left')","58d301cd":"d_id = dict(zip(df.id.cat.codes, df.id))\nd_item_id = dict(zip(df.item_id.cat.codes, df.item_id))\nd_dept_id = dict(zip(df.dept_id.cat.codes, df.dept_id))\nd_cat_id = dict(zip(df.cat_id.cat.codes, df.cat_id))\nd_store_id = dict(zip(df.store_id.cat.codes, df.store_id))\nd_state_id = dict(zip(df.state_id.cat.codes, df.state_id))","7a2473c1":"list1=['event_name_1','event_type_1','event_name_2','event_type_2']\nfrom sklearn.preprocessing import LabelEncoder\nfor i in list1:\n    df[i] = df[i].cat.add_categories(\"nan\").fillna(\"nan\")\n    df[i]=LabelEncoder().fit_transform(df[i]).astype(np.int8)\n","cb0b3dc4":"df.d = df['d'].apply(lambda x: x.split('_')[1]).astype(np.int16)","29e44ab4":"cols = df.dtypes.index.tolist()\ntypes = df.dtypes.values.tolist()\nfor i,type in enumerate(types):\n    if type.name == 'category':\n        df[cols[i]] = df[cols[i]].cat.codes","7021a4ad":"# extracting day_of_week has shown some memory errors, therefore I have dropped the date, \n# it is a good feature though, try to incorporate it and let me know the reason for error \n# df['date'] = df['date'].apply(lambda x: x.strftime('%d')).astype(np.int8)\ndf.drop(['date'],axis=1, inplace=True)","a1dfb186":"#make datatype of event as category\nfor i in list1:\n    df[i]=df[i].astype('category')","46db9f30":"# I have used 4 lags here in intervals of 7, all showed a good value of feature importance\nlags = [28,35,42,49]\nfor lag in lags:\n    df['sold_lag_'+str(lag)] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],as_index=False)['sold'].shift(lag).astype(np.float16)","cc68891d":"# I have added 4 days surrounding the event as features\nlags2 = [-2,-1,1,2]\nfor lag in lags2:\n    df['event1_lag_'+str(lag)] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],as_index=False)['event_name_1'].shift(lag).astype(np.float16)\n    df['event1_lag_'+str(lag)].fillna(100, inplace=True)\n    df['event1_lag_'+str(lag)]=df['event1_lag_'+str(lag)].astype(np.int8)\n    df['event1_lag_'+str(lag)]=df['event1_lag_'+str(lag)].astype('category')\n# event type didn't showed a good feature importance, opposite to event itself\n# for lag in lags2:\n#     df['eventtype1_lag_'+str(lag)] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],as_index=False)['event_type_1'].shift(lag).astype(np.float16).fillna(100, inplace=True)\n#     df['eventtype1_lag_'+str(lag)].fillna(100, inplace=True)\n#     df['eventtype1_lag_'+str(lag)]=df['eventtype1_lag_'+str(lag)].astype(np.int8)\n#     df['eventtype1_lag_'+str(lag)]=df['eventtype1_lag_'+str(lag)].astype('category')","4976b310":"df['item_sold_avg'] = df.groupby('item_id')['sold'].transform('mean').astype(np.float16)    \ndf['state_sold_avg'] = df.groupby('state_id')['sold'].transform('mean').astype(np.float16)    #total 3 unique values, 1 for each state\ndf['store_sold_avg'] = df.groupby('store_id')['sold'].transform('mean').astype(np.float16)  #10 unique values\ndf['cat_sold_avg'] = df.groupby('cat_id')['sold'].transform('mean').astype(np.float16)\ndf['dept_sold_avg'] = df.groupby('dept_id')['sold'].transform('mean').astype(np.float16)\ndf['cat_dept_sold_avg'] = df.groupby(['cat_id','dept_id'])['sold'].transform('mean').astype(np.float16)\ndf['store_item_sold_avg'] = df.groupby(['store_id','item_id'])['sold'].transform('mean').astype(np.float16)\ndf['cat_item_sold_avg'] = df.groupby(['cat_id','item_id'])['sold'].transform('mean').astype(np.float16)\ndf['dept_item_sold_avg'] = df.groupby(['dept_id','item_id'])['sold'].transform('mean').astype(np.float16)\ndf['state_store_sold_avg'] = df.groupby(['state_id','store_id'])['sold'].transform('mean').astype(np.float16)\ndf['state_store_cat_sold_avg'] = df.groupby(['state_id','store_id','cat_id'])['sold'].transform('mean').astype(np.float16)\ndf['store_cat_dept_sold_avg'] = df.groupby(['store_id','cat_id','dept_id'])['sold'].transform('mean').astype(np.float16)","56f8360c":"df['wm_yr_wk_linear']=LabelEncoder().fit_transform(df['wm_yr_wk'].values).astype(np.int16)\n\ndf.drop(['wm_yr_wk'], axis=1, inplace=True)","c8d7adf3":"df['price_lag'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'],as_index=False)['sell_price'].shift(7).astype(np.float16)\ndf['price-diff']=df['price_lag']-df['sell_price']\ndf.drop(['price_lag'], axis=1, inplace=True)","155ef6f4":"df['sell_price'].fillna(-1,inplace=True)\ndf['decimal']=df['sell_price'].apply(lambda x: 100*(x-int(x))).astype(np.int16)\ndf['sell_price'].replace(-1,np.nan,inplace=True)","8bee60ff":"df['expanding_price_mean'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sell_price'].transform(lambda x: x.expanding(2).mean()).astype(np.float16)\ndf['diff_moving_mean']=df['expanding_price_mean']-df['sell_price']\ndf.drop(['expanding_price_mean'], axis=1, inplace=True)","95db570c":"df['price-diff']=df['price-diff'].astype(np.float16)\ndf.drop(['wday'], axis=1, inplace=True)\ndf['decimal']=df['decimal'].astype(np.int8)\ndf['year']=LabelEncoder().fit_transform(df['year']).astype(np.int8)","acc056f9":"df['daily_avg_sold'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id','d'])['sell_price'].transform('mean').astype(np.float16)\ndf['avg_sold'] = df.groupby(['id', 'item_id', 'dept_id', 'cat_id', 'store_id', 'state_id'])['sell_price'].transform('mean').astype(np.float16)\ndf['selling_trend'] = (df['daily_avg_sold'] - df['avg_sold']).astype(np.float16)\ndf.drop(['daily_avg_sold','avg_sold'],axis=1,inplace=True)","52df4474":"list3=['cat_id','state_id']\nfor i in list3:\n    df.drop([i], axis=1, inplace=True)","3e25db53":"df = df[df['d']>=49]\n","e0482c67":"\ndf.to_pickle('data.pkl')\ndel df, sales, prices, calendar\ngc.collect()","fa87b726":"data = pd.read_pickle('data.pkl')\nvalid = data[(data['d']>=1599) & (data['d']<1942)][['id','d','sold']]  \nvalid_csv=data[(data['d']>=1914) & (data['d']<1942)][['id','d','sold']]\ntest = data[data['d']>=1942][['id','d','sold']]\neval_preds = test['sold']\nvalid_preds = valid['sold']\nvalid_preds_csv=valid_csv['sold']","161a2c9e":"cat_column=[]\nfor i in data.columns:\n    if(str(data.dtypes[i])=='category'):\n        cat_column.append(i)","852f13be":"for store in d_store_id:\n    df = data[data['store_id']==store]\n    \n    #Split the data\n    X_train, y_train = df[df['d']<1914].drop('sold',axis=1), df[df['d']<1914]['sold']\n    # Uncover if you want to use last year for train\n    #X_valid, y_valid = df[(df['d']>=1599) & (df['d']<1942)].drop('sold',axis=1), df[(df['d']>=1599) & (df['d']<1942)]['sold']\n    X_valid_csv, y_valid_csv = df[(df['d']>=1914) & (df['d']<1942)].drop('sold',axis=1), df[(df['d']>=1914) & (df['d']<1942)]['sold']\n    X_test = df[df['d']>=1942].drop('sold',axis=1)\n    \n    #Train and validate\n    model = LGBMRegressor(\n        learning_rate= 0.05,\n        subsample=0.6,\n        feature_fraction=0.6,\n        num_iterations = 1200,\n        max_bin=350,\n        num_leaves= 100,\n        lambda_l2=0.003,\n        max_depth=200,\n        min_data_in_leaf= 80,\n        force_row_wise= True,\n    )\n    print('*****Prediction for Store: {}*****'.format(d_store_id[store]))\n    model.fit(X_train, y_train, eval_set=[(X_train,y_train),(X_valid_csv,y_valid_csv)],\n             eval_metric='rmse',  verbose=100, early_stopping_rounds=20,categorical_feature=cat_column)\n    valid_preds_csv[X_valid_csv.index] = model.predict(X_valid_csv)\n    eval_preds[X_test.index] = model.predict(X_test)\n    filename = 'model'+str(d_store_id[store])+'.pkl'\n    # save model\n    joblib.dump(model, filename)\n    del model, X_train, y_train, X_valid_csv, y_valid_csv\n    gc.collect()","febdda91":"feature_importance_df = pd.DataFrame()\nfeatures = [f for f in data.columns if f != 'sold']\nfor filename in os.listdir('\/kaggle\/working\/'):\n    if 'model' in filename:\n        # load model\n        model = joblib.load(filename)\n        store_importance_df = pd.DataFrame()\n        store_importance_df[\"feature\"] = features\n        store_importance_df[\"importance\"] = model.feature_importances_\n        store_importance_df[\"store\"] = filename[5:9]\n        feature_importance_df = pd.concat([feature_importance_df, store_importance_df], axis=0)\n    \ndef display_importances(feature_importance_df_):\n    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:].index\n    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n    plt.figure(figsize=(8, 10))\n    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n    plt.title('LightGBM Features (averaged over store predictions)')\n    plt.tight_layout()\n    \ndisplay_importances(feature_importance_df)","a84dee73":"#Get the actual validation results\nvalid_csv['sold'] = valid_preds_csv\nvalidation = valid_csv[['id','d','sold']]\nvalidation = pd.pivot(validation, index='id', columns='d', values='sold').reset_index()\nvalidation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]\nvalidation.id = validation.id.map(d_id).str.replace('evaluation','validation')\n\n#Get the evaluation results\ntest['sold'] = eval_preds\nevaluation = test[['id','d','sold']]\nevaluation = pd.pivot(evaluation, index='id', columns='d', values='sold').reset_index()\nevaluation.columns=['id'] + ['F' + str(i + 1) for i in range(28)]\n#Remap the category id to their respective categories\nevaluation.id = evaluation.id.map(d_id)\n\n#Prepare the submission\nsubmit = pd.concat([validation,evaluation]).reset_index(drop=True)\n# submit.memory_usage().sum()\nsubmit.to_csv('submission.csv',index=False)","9b0390d2":"# Downloading Submission File\nfrom IPython.display import FileLink\nFileLink(r'submission.csv')","4654a984":"# <h2> 3. Convert the data from wide to long format (Melting)<\/font>","48a26c9c":"Lags help us in a time series dataset by enabling to use previous values of a feature as aother feature.           \n**Note**: Use only **lags starting from 28 days** as any less than that is going to give poor test results since some of the 28 prediction columns will remain 0.","f3b4e996":"My reasoning for this is that people usually do shopping a day before, or weekend before an important event and not on the event itself. Also the sales may dip immediately after the day of event (it was just a guess but gave a good feature to use)","a0242d02":"Store categorical columns to manually provide as input to LGBM Regressor","e89a146a":"# M5 Forecasting - Kaggle","9e41baca":"<font size=\"3\">h) Difference of price of an item at a store from it's current trend\/tragectory <\/font>","c05eb00d":"# <h2> 5. Model preperation and evaluation<\/font>","d30cfd07":"Before moving on towards the best part, i.e. feature extraction, let's store the category codes of the category columns we added:","a1086156":"# <h2> End","a1a91f3a":"<font size=\"3\">c) Introduce lags for events<\/font>","f67b97fd":"Drop features with about zero feature importance to save computation","e2a8a8b3":"<font size=\"3\">h) A trend feature for price which gives difference from overall mean, instead of a expending mean as used previously.<\/font>","b87f56e4":"<font size=\"3\">The conversion to long format enables easier transformation operations to columns, and we can easily monitor the columns for their feature importances\/ other factors due to their accessability. <\/font>","6bbc7d6b":"# <h2> 2. Save the kernel from memory overflow risk<\/font>","6ffb82e5":"Some more memory reduction to save the kernel:","042953a6":"<font size=\"3\">g) Decimal feature which shows the decimal part of the price. I just used it to gain more insights from price feature as I think that's the most important feature here (it may show some user interaction with prices, discrepancies with change, etc)<\/font>","4b35f4fe":"<font size=\"3\">b) Introduce lags for number of items sold <\/font>","3fc77fac":"<font size=\"3\">Add new prediction rows, i.e. 1942-1969, which is what we are predicting<\/font>","5e05b740":"Save Dataframe to output. Remove some start days as they are not as much important and lags there are empty.","abc44bd4":"<font size=\"3\">If you use the default datatypes assigned to the kernels by dataframe, you will most likely run into memory allocation error, considering such huge dataset.","783c7e50":"<font size=\"3\">e) Currently week id is not encoded and has large values, taking higher space. Reduce the space by encoding with integers (int64->int8).<\/font>","b88e2d72":"<font size=\"3\">a) Transform events with a label encoder and convert to category datatype, use only the number present in the day feature as an integer to get an insight on the time dependence, convert categorical features to respective codes and use day-of-month from the dates feature as others are already there.<\/font>","5da779e3":"<font size=\"3\">f) Add a price-diff feature which gives the increase\/decrease in price of an item at a store from it's previous week value. Its reasoning is pretty obvious.<\/font>","7290c92a":"# <h2> 6. Submission<\/font>","0d8d4c2e":"# <h2> LGBM model Training","d6b01a90":"Combine the three datasets:","da2d43b2":"# <h2> 4. Feature Extraction<\/font>","b8cde4a6":"# <h2> 1. Extract the data<\/font>","05994812":"<font size=\"3\">d) Apply mean encodings, atleast of the 12 aggregations they have taken in their WRMSE document<\/font>"}}