{"cell_type":{"75a874ff":"code","fbac4575":"code","39885528":"code","83974f76":"code","441fd451":"code","36a6de6c":"code","b948dd8e":"code","993d58f6":"code","a553f5f7":"code","00151d50":"code","1f3deaaf":"code","90ec21b5":"code","6bdd3345":"code","a94df689":"code","c51f74fe":"code","98a411c9":"code","5cb42249":"code","5cd8e7cc":"code","63009cc0":"code","8e4f07a5":"code","24c3934f":"code","c34c7159":"code","5f67f5bb":"code","8ebd6174":"code","2396919b":"code","35c01d7d":"code","1596e1b5":"code","d38dd090":"code","5d2f26dd":"code","3d383153":"code","b42e01ac":"code","910f2fab":"code","a70b2d05":"code","74dabbd4":"code","b751531d":"code","39419662":"code","88479be0":"code","a59bd7e6":"code","dd4c731f":"code","51440759":"code","8fddbec7":"code","d2c68269":"code","6406afba":"code","a6ca1309":"code","5c890805":"code","5dd26019":"code","4d56ffb3":"code","a0789e86":"code","6139b73c":"code","9a2e56c4":"code","c438e1f6":"code","83ef07f7":"code","56a25aa2":"code","891e8a2b":"code","fc0f9087":"code","862f59d0":"code","5dd084c1":"code","cb1dc0bf":"code","f2a689bc":"code","a8f4d36b":"code","33fc1d65":"code","29afbddf":"code","81dc448e":"code","acac887f":"code","85976933":"code","0ce5fdd2":"code","3d8a4956":"code","f46c339d":"code","eb53669e":"code","184b9d55":"code","8f7472fd":"code","4fd03002":"code","d54bbecb":"code","1cb87226":"code","ff41a3eb":"code","79aa92da":"code","60b794f0":"code","d83019f2":"code","63f70398":"code","0156900e":"code","fe85537a":"code","5cfba42c":"code","853c3026":"code","c55d3e23":"code","14d8224b":"code","5a026d75":"code","d97b23a3":"code","fe767258":"code","18624dd1":"code","d6ca8267":"code","c3068100":"code","06b9c241":"code","6602e46a":"code","9f036f5f":"code","9b9d677a":"code","5bfd8bdf":"code","81c112c7":"code","57fb72bb":"code","54f930c5":"code","8d7743ac":"code","b15f4d05":"code","9fafc5a0":"code","7086bc13":"code","a758cdf3":"code","92d37c37":"code","eb0673b3":"code","9fc65956":"code","05da992f":"code","53e6b287":"code","b16e658c":"code","e3ce67a3":"code","6a387067":"code","143fefea":"code","4cf4d1f3":"code","fc690314":"code","9cc305ea":"code","d73b3e74":"code","27f62ab3":"code","043f1dc9":"code","9d4af632":"code","238a8d8b":"code","d11b4346":"code","63c2051d":"code","ea73e8ff":"code","5ffb8f0c":"code","a90b6fbb":"code","3a5eb5bb":"markdown","b5d08b54":"markdown","96335236":"markdown","42710375":"markdown","d70e42fc":"markdown","bfe51831":"markdown","9efe01b3":"markdown","0fec29f9":"markdown","3915b61e":"markdown","10207656":"markdown","69f83c61":"markdown","4fa5c251":"markdown","0fd8dc71":"markdown","5b7278d2":"markdown","8fe3f26d":"markdown","afe95159":"markdown","bd71c94c":"markdown","305ace1b":"markdown","af5fa6c3":"markdown","c2076f0f":"markdown","489b76e5":"markdown","328c0f91":"markdown","8e7f5a8f":"markdown","189788e4":"markdown","f521a743":"markdown","7df81420":"markdown"},"source":{"75a874ff":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom sklearn.manifold import TSNE\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Classifier Libraries\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nimport collections\nfrom sklearn.metrics import accuracy_score, f1_score, confusion_matrix, recall_score\nfrom sklearn.naive_bayes import GaussianNB\n\n# Other Libraries\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.pipeline import make_pipeline\nfrom imblearn.pipeline import make_pipeline as imbalanced_make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import NearMiss\nfrom imblearn.metrics import classification_report_imbalanced\n# Any results you write to the current directory are saved as output\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","fbac4575":"data = pd.read_csv('..\/input\/creditcard.csv') # Reading the file .csv\ndf = pd.DataFrame(data) # Converting data to Panda DataFrame\ndf2=df","39885528":"df.describe() # Description of statistic features (Sum, Average, Variance, minimum, 1st quartile, 2nd quartile, 3rd Quartile and Maximum)","83974f76":"df.head()","441fd451":"# Check if there any Null Values!\ndf.isnull().sum().max()","36a6de6c":"# Normalization Amount\nfrom sklearn.preprocessing import StandardScaler\ndf['normAmount'] = StandardScaler().fit_transform(data['Amount'].values.reshape(-1,1))\ndf1=df\n","b948dd8e":"#make sure there are no change in distribution \nf, (ax1, ax2) = plt.subplots(2,1,figsize =( 15, 8))\n\nsns.kdeplot(data['Amount'],shade=True, ax = ax1, color='red')\nax1.set_title('Before Normalization')\n\nsns.kdeplot(data['normAmount'],shade=True, ax = ax2, color='blue')\nax2.set_title('After Normalization')\n\nplt.show()","993d58f6":"# Drop useless variables\ndf = df.drop(['Amount','Time'],axis=1)","a553f5f7":"df.columns\n","00151d50":"# The classes imbalanced.The problem here is Imbalanced Data set lead to biased to the majority class\nprint('No Frauds', round(df['Class'].value_counts()[0]\/len(df) * 100,2), '% of the dataset')\nprint('Frauds', round(df['Class'].value_counts()[1]\/len(df) * 100,2), '% of the dataset')","1f3deaaf":"df['Class'].plot(kind=\"density\", figsize=(10,5))","90ec21b5":"df['Class'].value_counts()","6bdd3345":"non_fraud = df[df['Class'] == 0].sample(1000)\nfraud = df[df['Class'] == 1]\n\ndf1 = non_fraud.append(fraud).sample(frac=1).reset_index(drop=True)\nX = df1.drop(['Class'], axis = 1).values\nY = df1[\"Class\"].values","a94df689":"def tsne_plot(x1, y1, name=\"graph.png\"):\n    tsne = TSNE(n_components=2, random_state=0)\n    X_t = tsne.fit_transform(x1)\n\n    plt.figure(figsize=(12, 8))\n    plt.scatter(X_t[np.where(y1 == 0), 0], X_t[np.where(y1 == 0), 1], marker='o', color='g', linewidth='1', alpha=0.8, label='Non Fraud')\n    plt.scatter(X_t[np.where(y1 == 1), 0], X_t[np.where(y1 == 1), 1], marker='o', color='r', linewidth='1', alpha=0.8, label='Fraud')\n\n    plt.legend(loc='best');\n    plt.savefig(name);\n    plt.show();\n    \ntsne_plot(X, Y, \"original.png\")","c51f74fe":"sns.boxplot(x=df['V23'])","98a411c9":"# Graph distribution\ndf.hist (bins=50, figsize=(20,15), color = 'deepskyblue')\n\nplt.show()","5cb42249":"#Create Train and Test Sets before dealing with imbalanced data\n# Prepare data for modeling\n# Separate input features and target\ny = df.Class\nX = df.drop('Class', axis=1)\n\n# setting up testing and training sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=27)","5cd8e7cc":"# Train model logistic Reg in imbalanced data\nlr = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n \n# Predict on training set\nlr_pred = lr.predict(X_test)","63009cc0":"# Checking accuracy\naccuracy_score(y_test, lr_pred)","8e4f07a5":"# Checking unique values\npredictions = pd.DataFrame(lr_pred)\npredictions[0].value_counts()","24c3934f":"y_test.value_counts()","c34c7159":"#f1_scor is a suitable measure of models tested with imbalance datasets\nf1_score(y_test, predictions, average='macro')","5f67f5bb":"# confusion matrix\npd.DataFrame(confusion_matrix(y_test, lr_pred))","8ebd6174":"#Dealing with imbalance problem first method\nfrom sklearn.utils import resample\n","2396919b":"# Separate input features and target\ny = df.Class\nX = df.drop('Class', axis=1)\n\n# setting up testing and training sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)","35c01d7d":"# concatenate our training data back together\nX = pd.concat([X_train, y_train], axis=1)\nX.head()","1596e1b5":"X['Class'].value_counts()","d38dd090":"# separate minority and majority classes\nnot_fraud = X[X.Class==0]\nfraud = X[X.Class==1]\n\n# upsample minority\nfraud_upsampled = resample(fraud,\n                          replace=True, # sample with replacement\n                          n_samples=len(not_fraud), # match number in majority class\n                          random_state=27) # reproducible results\n\n# combine majority and upsampled minority\nupsampled = pd.concat([not_fraud, fraud_upsampled])\n\n# check new class counts\nupsampled.Class.value_counts()","5d2f26dd":"#plot the distribution after applying upsameling \nupsampled['Class'].plot(kind=\"density\", figsize=(10,5))","3d383153":"# trying logistic regression again with the balanced dataset\ny_train = upsampled.Class\nX_train = upsampled.drop('Class', axis=1)\n\nupsampled = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n\nupsampled_pred = upsampled.predict(X_test)","b42e01ac":"# Checking accuracy\naccuracy_score(y_test, upsampled_pred)","910f2fab":"# f1 score\nf1_score(y_test, upsampled_pred)","a70b2d05":"# confusion matrix\npd.DataFrame(confusion_matrix(y_test, upsampled_pred))","74dabbd4":"recall_score(y_test, upsampled_pred)\n","b751531d":"# still using our separated classes fraud and not_fraud from above\n\n# downsample majority\nnot_fraud_downsampled = resample(not_fraud,\n                                replace = False, # sample without replacement\n                                n_samples = len(fraud), # match minority n\n                                random_state = 27) # reproducible results\n\n# combine minority and downsampled majority\ndownsampled = pd.concat([not_fraud_downsampled, fraud])\n\n# checking counts\ndownsampled.Class.value_counts()","39419662":"# trying logistic regression again with the undersampled dataset\n\ny_train = downsampled.Class\nX_train = downsampled.drop('Class', axis=1)\n\nundersampled = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n\nundersampled_pred = undersampled.predict(X_test)","88479be0":"# Checking accuracy\naccuracy_score(y_test, undersampled_pred)","a59bd7e6":"# f1 score\nf1_score(y_test, undersampled_pred)","dd4c731f":"# confusion matrix\npd.DataFrame(confusion_matrix(y_test, undersampled_pred))","51440759":"recall_score(y_test, undersampled_pred)","8fddbec7":"from imblearn.over_sampling import SMOTE\n\n# Separate input features and target\ny = df.Class\nX = df.drop('Class', axis=1)\n\n# setting up testing and training sets\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=27)\n\nsm = SMOTE(random_state=27)\nX_train, y_train = sm.fit_sample(X_train, y_train)","d2c68269":"smote = LogisticRegression(solver='liblinear').fit(X_train, y_train)\n\nsmote_pred = smote.predict(X_test)\n\n# Checking accuracy\naccuracy_score(y_test, smote_pred)","6406afba":"# f1 score\nf1_score(y_test, smote_pred)","a6ca1309":"recall_score(y_test, smote_pred)\n","5c890805":"# confustion matrix\npd.DataFrame(confusion_matrix(y_test, smote_pred))","5dd26019":"import xgboost as xgb","4d56ffb3":"xgb_cfl = xgb.XGBClassifier(n_jobs = -1)\n","a0789e86":"from sklearn.model_selection import GridSearchCV","6139b73c":"param_grid = {'n_estimators': [50, 100]}\n\nCV_xgb_cfl = GridSearchCV(estimator = xgb_cfl, param_grid = param_grid, scoring ='f1', verbose = 2)\nCV_xgb_cfl.fit(X_train, y_train)\n\nbest_parameters = CV_xgb_cfl.best_params_\nprint(\"The best parameters for using this model is\", best_parameters)","9a2e56c4":" CV_xgb_cfl.cv_results_","c438e1f6":"# xgb\nxgb_cfl = xgb.XGBClassifier(n_jobs = -1,n_estimators = 100)\n\nxgb_cfl.fit(X_train, y_train)\n\n\n\n\n","83ef07f7":"X_train.shape,X_test.shape","56a25aa2":"X_test=np.array(X_test)","891e8a2b":"y_pred = xgb_cfl.predict(X_test)","fc0f9087":"y_score = xgb_cfl.predict_proba(X_test)[:,1]","862f59d0":"# f1 score\nf1_score(y_test, y_pred)","5dd084c1":"def plot_confusion_matrix(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n    \"\"\"\n    given a sklearn confusion matrix (cm), make a nice plot\n\n    Arguments\n    ---------\n    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n\n    target_names: given classification classes such as [0, 1, 2]\n                  the class names, for example: ['high', 'medium', 'low']\n\n    title:        the text to display at the top of the matrix\n\n    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n                  see http:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html\n                  plt.get_cmap('jet') or plt.cm.Blues\n\n    normalize:    If False, plot the raw numbers\n                  If True, plot the proportions\n\n    Usage\n    -----\n    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n                                                              # sklearn.metrics.confusion_matrix\n                          normalize    = True,                # show proportions\n                          target_names = y_labels_vals,       # list of names of the classes\n                          title        = best_estimator_name) # title of graph\n\n    Citiation\n    ---------\n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import itertools\n    \n\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()","cb1dc0bf":"# Confusion maxtrix & metrics\ncm = confusion_matrix(y_test, y_pred)\nclass_names = [0,1]\nplt.figure()\nplot_confusion_matrix(cm, \n                      target_names = class_names, \n                      title = 'XGB Confusion matrix')\nplt.savefig('2.xgb_cfl_confusion_matrix.png')\nplt.show()\n\n\n\n","f2a689bc":"from sklearn import metrics\ndef buildROC(y_test, y_pred):\n    fpr, tpr, threshold = metrics.roc_curve(y_test, y_pred)\n    roc_auc = metrics.auc(fpr, tpr)\n    plt.title('Receiver Operating Characteristic')\n    plt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\n    plt.legend(loc = 'lower right')\n    plt.plot([0, 1], [0, 1],'r--')\n    plt.ylabel('True Positive Rate')\n    plt.xlabel('False Positive Rate')\n    plt.gcf().savefig('roc.png')","a8f4d36b":"buildROC(y_test, y_pred)","33fc1d65":"clfs_opt = []\nclfs_best_scores = []\nclfs_best_param = []","29afbddf":"logi_clf = LogisticRegression(solver='lbfgs', max_iter=500)\nlogi_parm = {\"C\": [0.1, 1]}","81dc448e":"logi_clf = GridSearchCV(estimator = logi_clf, param_grid = logi_parm, scoring ='f1', verbose = 2)\nlogi_clf.fit(X_train, y_train)\nclfs_opt.append(logi_clf.best_estimator_)\nclfs_best_scores.append(logi_clf.best_score_)\nclfs_best_param.append(logi_clf.best_params_)","acac887f":"svm_clf = SVC(probability=True)\nsvm_parm = {'kernel': ['rbf', 'poly'], 'C': [ 50, 100]}\n","85976933":"svm_clf = GridSearchCV(estimator = svm_clf, param_grid = svm_parm, scoring ='f1', verbose = 2)\nsvm_clf.fit(X_train, y_train)\nclfs_opt.append(svm_clf.best_estimator_)\nclfs_best_scores.append(svm_clf.best_score_)\nclfs_best_param.append(svm_clf.best_params_)","0ce5fdd2":"dt_clf = DecisionTreeClassifier()\ndt_parm = {'criterion':['entropy']}","3d8a4956":"dt_clf = GridSearchCV(estimator = dt_clf, param_grid = dt_parm, scoring ='f1', verbose = 2)\ndt_clf.fit(X_train, y_train)\nclfs_opt.append(dt_clf.best_estimator_)\nclfs_best_scores.append(dt_clf.best_score_)\nclfs_best_param.append(dt_clf.best_params_)","f46c339d":"max(clfs_best_scores)\n","eb53669e":"\narg = np.argmax(clfs_best_scores)\nclfs_best_param[arg]","184b9d55":"# Split the data into train set and test set\ntrain,test = train_test_split(df2,test_size=0.3,random_state=0)","8f7472fd":"# Get the arrays of features and labels in train dataset\nfeatures_train = train.drop(['Time','Class'],axis=1)\nfeatures_train = features_train.values\nlabels_train = pd.DataFrame(train[['Class']])\nlabels_train = labels_train.values\n\n# Get the arrays of features and labels in test dataset\nfeatures_test = test.drop(['Time','Class'],axis=1)\nfeatures_test = features_test.values\nlabels_test = pd.DataFrame(test[[\"Class\"]])\nlabels_test = labels_test.values","4fd03002":"from sklearn.cluster import KMeans\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics.cluster import adjusted_rand_score\n\nmodel = KMeans(n_clusters=2,random_state=0)\nmodel.fit(features_train)\nlabels_train_predicted = model.predict(features_train)\nlabels_test_predicted = model.predict(features_test)\n\n# Decide if model predicted label is aligned with true label \ntrue_negative,false_positive,false_negative,true_positive = confusion_matrix(labels_train,labels_train_predicted).ravel()\nreassignflag = true_negative + true_positive < false_positive + false_negative\nprint(reassignflag)\n","d54bbecb":"from sklearn.metrics import confusion_matrix, precision_score, recall_score, accuracy_score,f1_score\n# Calculating confusion matrix for kmeans\nprint('Confusion Matrix:\\n',confusion_matrix(labels_test,labels_test_predicted))\n\n# Scoring kmeans\n\nprint('kmeans_precison_score:', precision_score(labels_test,labels_test_predicted))\nprint('kmeans_recall_score:', recall_score(labels_test,labels_test_predicted))\nprint('kmeans_accuracy_score:', accuracy_score(labels_test,labels_test_predicted))\nprint('kmeans_f1_score:',f1_score(labels_test,labels_test_predicted))","1cb87226":"labels_test.shape,labels_test_predicted.shape","ff41a3eb":"labels_test=np.concatenate(labels_test)","79aa92da":"adjusted_rand_score(labels_test,labels_test_predicted)  #accurcy of clustering of original dat","60b794f0":"from sklearn.mixture import GaussianMixture\n","d83019f2":"gmm = GaussianMixture(n_components=2).fit(features_train)\nlabels_gmm = gmm.predict(features_test)","63f70398":"adjusted_rand_score(labels_gmm,labels_test)   #accurcy of clustering of original dat","0156900e":"# Calculating confusion matrix for GMM\nprint('Confusion Matrix:\\n',confusion_matrix(labels_test,labels_test_predicted))\n\n# Scoring kmeans\n\nprint('kmeans_precison_score:', precision_score(labels_test,labels_test_predicted))\nprint('kmeans_recall_score:', recall_score(labels_test,labels_test_predicted))\nprint('kmeans_accuracy_score:', accuracy_score(labels_test,labels_test_predicted))\nprint('kmeans_f1_score:',f1_score(labels_test,labels_test_predicted))","fe85537a":"from scipy.cluster.hierarchy import dendrogram , ward,single\nlinkage = ward(features_train)\nplt.figure(figsize=(15,10))\ndendrogram(linkage)\n\nplt.show()","5cfba42c":"from sklearn.cluster import SpectralClustering\n","853c3026":"sp = SpectralClustering(n_clusters=2)\nlabelsDB = sp.fit_predict(features_train)","c55d3e23":"X1_train, X1_test  = train_test_split(df2, test_size=0.25)","14d8224b":"data['Amount'].skew()","5a026d75":"X1_train.loc[:,\"Time\"] = X1_train[\"Time\"].apply(lambda x : x \/ 3600 % 24)#transform the Time field to time-of-day to account \nX1_train.loc[:,'Amount'] = np.log(X1_train['Amount']+1) #The Amount field is transformed to log scale deal with\n\nX1_test.loc[:,\"Time\"] = X1_test[\"Time\"].apply(lambda x : x \/ 3600 % 24)\nX1_test.loc[:,'Amount'] = np.log(X1_test['Amount']+1)\n# data = data.drop(['Amount'], axis = 1)\nprint(X1_train.shape)\nX1_train.head()","d97b23a3":"X1_train = X1_train[X1_train.Class == 0] # train the model on normal transactions\nX1_train = X1_train.drop(['Class'], axis=1)\n\ny1_test = X1_test['Class']\nX1_test  = X1_test.drop(['Class'], axis=1) #drop the class column\n\n\n#transform to ndarray\nX1_train = X1_train.values\nX1_test = X1_test.values\nX1_train.shape","fe767258":"#Build Model\n#The first two layers are used for encoder, the last two go for the decoder.\n#Use L1 regularization during training\n\nimport tensorflow as tf\nfrom keras.models import Model, load_model\nfrom keras.layers import Input, Dense\nfrom keras.callbacks import ModelCheckpoint, TensorBoard\nfrom keras import regularizers\n\ninput_dim = X1_train.shape[1] #num of columns, 29\nencoding_dim = 14\nhidden_dim = int(encoding_dim \/ 2)\nlearning_rate = 1e-5\n\ninput_layer = Input(shape=(input_dim, ))\nencoder = Dense(encoding_dim, \n                activation=\"tanh\", \n                activity_regularizer=regularizers.l1(learning_rate))(input_layer)\nencoder = Dense(hidden_dim, activation=\"relu\")(encoder)\ndecoder = Dense(hidden_dim, activation='tanh')(encoder)\ndecoder = Dense(input_dim, activation='relu')(decoder)\nautoencoder = Model(inputs=input_layer, outputs=decoder)\n","18624dd1":"nb_epoch = 300\nbatch_size = 128\nautoencoder.compile(metrics=['accuracy'], loss='mean_squared_error',optimizer='adam')\n\n\ntensorboard = TensorBoard(log_dir='.\/logs',\n                          histogram_freq=0,\n                          write_graph=True,\n                          write_images=True)\n\nhistory = autoencoder.fit(X1_train, X1_train,\n                          epochs=nb_epoch,\n                          batch_size=batch_size,\n                          shuffle=True,\n                          validation_data=(X1_test, X1_test),\n                          verbose=1)\n    ","d6ca8267":"# plot mse during training\nplt.subplot(212)\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\nplt.show()","c3068100":"predictions = autoencoder.predict(X1_test)\nmse = np.mean(np.power(X1_test - predictions, 2), axis=1)\ndf_error = pd.DataFrame({'reconstruction_error': mse,'true_class': y1_test})\ndf_error.describe()","06b9c241":"# Import modules\nfrom sklearn.metrics import auc, roc_curve,precision_recall_curve\nfrom sklearn.metrics import confusion_matrix,classification_report\nfrom sklearn.metrics import recall_score,f1_score,precision_recall_fscore_support\n\nfpr_rf, tpr_rf, thresholds_rf  = roc_curve(df_error.true_class, df_error.reconstruction_error)\nauc_rf  = auc(false_positive_rate, true_positive_rate)\n\n# Plot the roc curve\n\n#plot_roc_curve(false_positive_rate,true_positive_rate,roc_auc)","6602e46a":"plt.figure(1)\nplt.plot([0, 1], [0, 1], 'k--')\n#plt.plot(fpr_keras, tpr_keras, label='Keras (area = {:.3f})'.format(auc_keras))\nplt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve')\nplt.legend(loc='best')\nplt.show()\nplt.figure(2)\nplt.xlim(0, 0.2)\nplt.ylim(0.8, 1)\nplt.plot([0, 1], [0, 1], 'k--')\nplt.plot(fpr_rf, tpr_rf, label='RF (area = {:.3f})'.format(auc_rf))\nplt.xlabel('False positive rate')\nplt.ylabel('True positive rate')\nplt.title('ROC curve (zoomed in at top left)')\nplt.legend(loc='best')\nplt.show()","9f036f5f":"from numpy.random import seed\nseed(1)\nfrom tensorflow import set_random_seed\nset_random_seed(2)","9b9d677a":"df = pd.read_csv('..\/input\/creditcard.csv')","5bfd8bdf":"df.head()","81c112c7":"df.describe()","57fb72bb":"df.isnull().sum()","54f930c5":"df = df.drop('Time',axis=1)","8d7743ac":"X = df.drop('Class',axis=1).values \ny = df['Class'].values","b15f4d05":"X.shape","9fafc5a0":"X -= X.min(axis=0)\nX \/= X.max(axis=0)","7086bc13":"X.mean()","a758cdf3":"X.shape","92d37c37":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train,y_test = train_test_split(X,y,test_size=0.1)","eb0673b3":"import os\nos.environ[\"KERAS_BACKEND\"] = \"tensorflow\"\nimport numpy as np\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\n\nfrom keras.layers import Input, Embedding, multiply, BatchNormalization\nfrom keras.models import Model, Sequential\nfrom keras.layers.core import Reshape, Dense, Dropout, Flatten\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.convolutional import Conv2D, UpSampling2D\nfrom keras.datasets import mnist\nfrom keras.optimizers import Adam\nfrom keras import backend as K\nfrom keras import initializers\nfrom keras.utils import to_categorical\n\nK.set_image_dim_ordering('th')\n\n# Deterministic output.\n# Tired of seeing the same results every time? Remove the line below.\nnp.random.seed(1000)\n\n# The results are a little better when the dimensionality of the random vector is only 10.\n# The dimensionality has been left at 100 for consistency with other GAN implementations.\nrandomDim = 10\n","9fc65956":"def build_generator(latent_dim,data_dim):\n\n        model = Sequential()\n\n        model.add(Dense(16, input_dim=latent_dim))\n    \n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(32, input_dim=latent_dim))\n    \n        model.add(LeakyReLU(alpha=0.2))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Dense(data_dim,activation='tanh'))\n\n        model.summary()\n\n        noise = Input(shape=(latent_dim,))\n        img = model(noise)\n\n        return Model(noise, img)","05da992f":"generator = build_generator(latent_dim=10,data_dim=29)","53e6b287":"def build_discriminator(data_dim,num_classes):\n    model = Sequential()\n    model.add(Dense(31,input_dim=data_dim))\n    model.add(LeakyReLU(alpha=0.2))\n    model.add(BatchNormalization(momentum=0.8))\n    model.add(Dropout(0.25))\n    model.add(Dense(16,input_dim=data_dim))\n    model.add(LeakyReLU(alpha=0.2))\n    \n    model.summary()\n    img = Input(shape=(data_dim,))\n    features = model(img)\n    valid = Dense(1, activation=\"sigmoid\")(features)\n    label = Dense(num_classes+1, activation=\"softmax\")(features)\n    return Model(img, [valid, label])","b16e658c":"discriminator = build_discriminator(data_dim=29,num_classes=2)","e3ce67a3":"optimizer = Adam(0.0002, 0.5)\ndiscriminator.compile(loss=['binary_crossentropy', 'categorical_crossentropy'],\n    loss_weights=[0.5, 0.5],\n    optimizer=optimizer,\n    metrics=['accuracy'])","6a387067":"noise = Input(shape=(10,))\nimg = generator(noise)\ndiscriminator.trainable = False\nvalid,_ = discriminator(img)\ncombined = Model(noise , valid)\ncombined.compile(loss=['binary_crossentropy'],\n    optimizer=optimizer)","143fefea":"X_train.shape","4cf4d1f3":"from imblearn.under_sampling import RandomUnderSampler","fc690314":"rus = RandomUnderSampler(random_state=42)","9cc305ea":"X_res, y_res = rus.fit_sample(X, y)","d73b3e74":"X_res.shape","27f62ab3":"X_res -= X_res.min()\nX_res \/= X_res.max()","043f1dc9":"X_test -= X_test.min()\nX_test \/= X_test.max()","9d4af632":"X_test_res, y_test_res = rus.fit_sample(X_test,y_test)","238a8d8b":"from sklearn.metrics import accuracy_score, f1_score, confusion_matrix","d11b4346":"y_res.shape","63c2051d":"def train(X_train,y_train,\n          X_test,y_test,\n          generator,discriminator,\n          combined,\n          num_classes,\n          epochs, \n          batch_size=128):\n    \n    f1_progress = []\n    d_loss_progress = []\n    half_batch = int(batch_size \/ 2)\n\n    noise_until = epochs\n\n    # Class weights:\n    # To balance the difference in occurences of digit class labels.\n    # 50% of labels that the discriminator trains on are 'fake'.\n    # Weight = 1 \/ frequency\n    cw1 = {0: 1, 1: 1}\n    cw2 = {i: num_classes \/ half_batch for i in range(num_classes)}\n    cw2[num_classes] = 1 \/ half_batch \n    d_loss_sum = 0\n\n    for epoch in range(epochs):\n\n        # ---------------------\n        #  Train Discriminator\n        # ---------------------\n\n        # Select a random half batch of images\n        idx = np.random.randint(0, X_train.shape[0], half_batch)\n        imgs = X_train[idx]\n\n        # Sample noise and generate a half batch of new images\n        noise = np.random.normal(0, 1, (half_batch, 10))\n        gen_imgs = generator.predict(noise)\n\n        valid = np.ones((half_batch, 1))\n        fake = np.zeros((half_batch, 1))\n\n        labels = to_categorical(y_train[idx], num_classes=num_classes+1)\n        fake_labels = to_categorical(np.full((half_batch, 1), num_classes), num_classes=num_classes+1)\n\n        # Train the discriminator\n        d_loss_real = discriminator.train_on_batch(imgs, [valid, labels], class_weight=[cw1, cw2])\n        d_loss_fake = discriminator.train_on_batch(gen_imgs, [fake, fake_labels], class_weight=[cw1, cw2])\n        d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n        # ---------------------\n        #  Train Generator\n        # ---------------------\n\n        noise = np.random.normal(0, 1, (batch_size, 10))\n        validity = np.ones((batch_size, 1))\n\n        # Train the generator\n        g_loss = combined.train_on_batch(noise, validity, class_weight=[cw1, cw2])\n\n        # Plot the progress\n        print (\"%d [D loss: %f, acc: %.2f%%, op_acc: %.2f%%] [G loss: %f]\" % (epoch, d_loss[0], 100*d_loss[3], 100*d_loss[4], g_loss))\n        d_loss_sum += 100*d_loss[3]\n        \n        if epoch % 10 == 0:\n            _,y_pred = discriminator.predict(X_test,batch_size=batch_size)\n            #print(y_pred.shape)\n            y_pred = np.argmax(y_pred[:,:-1],axis=1)\n            \n            f1 = f1_score(y_test,y_pred)\n            print('Epoch: {}, F1: {:.5f}, F1P: {}'.format(epoch,f1,len(f1_progress)))\n            cm = confusion_matrix(y_test, y_pred)\n\n            print(cm)\n            print(d_loss_sum\/10)\n            d_loss_progress.append(d_loss_sum\/10)\n            f1_progress.append(f1)\n            d_loss_sum = 0\n    \n    return f1_progress, d_loss_progress","ea73e8ff":"f1_p, d_l_p = train(X_res,y_res,\n             X_test,y_test,\n             generator,discriminator,\n             combined,\n             num_classes=2,\n             epochs=2000, \n             batch_size=128)","5ffb8f0c":"fig = plt.figure(figsize=(10,7))\nplt.plot(f1_p)\nplt.xlabel('10 Epochs')\nplt.ylabel('F1 Score Validation')","a90b6fbb":"fig = plt.figure(figsize=(10,7))\nplt.plot(d_l_p)\nplt.xlabel('10 Epochs')\nplt.ylabel('Accuracy')","3a5eb5bb":"**k-means Evaluation**","b5d08b54":"**developing unsupervised generative models **","96335236":"**Data set description:**\n\nThis dataset presents transactions that occurred in two days, where we have 492 frauds out of 284,807 transactions. The dataset is highly unbalanced, the positive class (frauds) account for 0.172% of all transactions.","42710375":" **DBScan-------> we can not run this cell as well requried a heavy process lead to restart this Session**","d70e42fc":"**as above high biased to the majoriy class**","bfe51831":"**DecisionTreeClassifier---> we will not be able to tune all prameters in the same reason**","9efe01b3":"**ROC Curve for Model evalution **","0fec29f9":"![](http:\/\/)**Consider only 10000 rows of non fraud cases**","3915b61e":"[](http:\/\/)**Try More ML models with hyper prameter tunnig**","10207656":"**Make prediction**","69f83c61":"**Logistic regression**","4fa5c251":" **2.Generational Adversarial Networks (GANs)**","0fd8dc71":"**Further methods in clustering**\n\n- We can apply dimension reduction before applying clustering algorithms that may lead to improve the accurcy as we saw in  the previous clustering assignment","5b7278d2":"**Hierarchical Clustering-------> we can not run this cell requried a heavy process lead to restart this Session **","8fe3f26d":"[](http:\/\/)**XGB - With best hyperparameters**","afe95159":"**SVM ----> we will not be able to run this cell to tune hyper prameter this lead to restart this session in terms of limitted resources although we used GPU  **","bd71c94c":"> **Apply GMM algorithmwe will not be able to tune all prameters in the same reason**","305ace1b":"**Finally Apply SMOTE or Synthetic Minority Oversampling Technique is a popular algorithm to creates sythetic observations of the minority class.**","af5fa6c3":" **Here we will apply three different machine learning processes:**\n\n1. Supervised Learning is a process of training a machine learning model on a labelled dataset ie. a dataset in which the target variable is known. In this technique, the model aims to find the relationships among the independent and dependent variable. Examples of supervised learning are classification, regression and forecasting. \n\n2. Unsupervised Learning is a process of training a machine learning model on a dataset in which target variable is not known. In this technique, the model aims to find the most relevant patterns in the data or the segments of data. Examples of unsupervised learning are clustering, segmentations, dimensionality reduction etc. \n\n3. Semi-Supervised Learning is combination of supervised and unsupervised learning processes in which the unlabelled data is used for training a model as well. In this approach, the properties of unspervised learning are used to learn the best possible representation of data and the properties of supervised learning are used to learn the relationships in the representations which are then used to make predictions.","c2076f0f":"1. **Our accuracy score decreased after upsampling, but the model is now predicting both classes more equally**","489b76e5":"**Downsampling produced a higher recall score than upsampling! My concern here is the small number of total samples we used to train the model**","328c0f91":"**Apply clustering algorithms **","8e7f5a8f":"1. **XGB - Grid Search CV to find best n_estimators (F1_score)**","189788e4":"**Try Undersampling Majority Class**\n- Undersampling can be defined as removing some observations of the majority class.","f521a743":"**1.Variational Autoencoder (VAE)**","7df81420":"**Deal with imbalanced data by using Resampling Techniques**\n\n- Oversampling Minority Class:  Oversampling can be defined as adding more copies of the minority class. Oversampling can be a good choice when you don't have a ton of data to work with. A con to consider when undersampling is that it can cause overfitting and poor generalization to your test set."}}