{"cell_type":{"44e8d68c":"code","401c348f":"code","764551cb":"code","da06ab12":"code","613badd3":"code","0f4e1b5e":"code","455224a9":"code","8eb7ebf5":"code","9c86a4c6":"code","6e67fba5":"code","beaf834c":"code","007c25b3":"code","f7b8fa0f":"code","ad2f8019":"code","0fb52de6":"code","8fd1651c":"code","4bb249b6":"code","3369fe99":"code","bd60d1f4":"code","3b5117cf":"code","4bc544a5":"code","1a488d27":"code","7a187b8a":"code","12a0437d":"code","e284ebdf":"code","c64ccd8c":"code","611499f6":"code","d88c0261":"code","6948fd3b":"code","b65e678f":"code","19676f43":"code","8a5f9fd0":"code","e20fabea":"code","de2c6dec":"code","8d96f4f6":"code","3ea5ab90":"code","b5c13909":"code","9df9fba6":"markdown","5c72a056":"markdown","01db8ce8":"markdown","6dc01d18":"markdown","b35cb63c":"markdown","e705ce02":"markdown","cc6b5b30":"markdown","aeb7bb83":"markdown","26b9548b":"markdown","65f11f76":"markdown","9a588847":"markdown","8826b97a":"markdown","fcb1a890":"markdown","772cc8a4":"markdown","76d47387":"markdown","57fd1766":"markdown","8d45c579":"markdown","7927542f":"markdown","89ab07bd":"markdown","c1d6fcbd":"markdown","7b90a2a7":"markdown","77b65eea":"markdown","bb2ee155":"markdown","9d918e1f":"markdown","f0c8637f":"markdown","ef57cd2d":"markdown","727633b5":"markdown"},"source":{"44e8d68c":"# install packages\n!pip install nltk --user\n!pip install owlready2 --user\n!pip install pronto --user\n!pip install ipynb-py-convert --user\n!pip install langdetect --user\n!pip install contractions --user\n!pip install inflect --user\n!pip install num2words --user\n!pip install tables --user\n!pip install h5py --user\n!pip install sentence-transformers --user\n!pip install pandas --user\n!pip install tqdm --user\n!pip install seaborn --user\n!pip install numpy --user\n!pip install scipy --user\n!pip install matplotlib --user\n!pip install numpy --user\n!pip install bottleneck --user\n!pip install pandarallel --user\n!pip install wordcloud --user\n!pip install  --user spacy\n!pip install --user https:\/\/github.com\/explosion\/spacy-models\/releases\/download\/en_core_web_sm-2.2.0\/en_core_web_sm-2.2.0.tar.gz","401c348f":"from collections import defaultdict\nimport glob\nimport itertools\nimport json\nimport pickle\nimport os\nimport re\n\nimport bs4\nimport contractions\nimport inflect\nfrom langdetect import detect\nimport matplotlib.pyplot as plt\nimport networkx as nx\nimport nltk\nnltk.download('punkt')\nnltk.download('stopwords')\nnltk.download('wordnet')\nfrom nltk import tokenize\nfrom nltk.corpus import wordnet as wn\nfrom nltk.corpus import stopwords\nfrom nltk.stem import LancasterStemmer\nfrom nltk.stem import PorterStemmer\nfrom nltk.stem import WordNetLemmatizer\nimport numpy as np\nimport pandas as pd\nfrom pandarallel import pandarallel\nfrom PIL import Image\nimport requests\nimport seaborn as sns\nfrom sentence_transformers import SentenceTransformer\nfrom sklearn.cluster import KMeans\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport spacy\nfrom spacy import displacy\nnlp = spacy.load('en_core_web_sm')\nfrom spacy.matcher import Matcher\nfrom spacy.tokens import Span\nfrom tqdm import tqdm\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n# Initialize pandarallel\npandarallel.initialize(use_memory_fs=False,nb_workers=2)","764551cb":"# pandas options\npd.set_option('display.max_rows', 500)\npd.set_option('display.max_columns', 500)\npd.set_option('display.width', 1000)\npd.set_option('display.expand_frame_repr', False)\npd.options.mode.chained_assignment = None \n\ntqdm.pandas()\n\n# make temp dir to save intermidiate data\nif not os.path.exists('..\/data'):\n    os.mkdir('..\/data')","da06ab12":"# Help functions and class\n# help function to generate file path\ndef filepath(*args):\n    if len(args) < 1:\n        return None\n    elif len(args) == 1:\n        return args[0]\n    else:\n        return f'{args[0]}\/{filepath(*args[1:])}'\n\n# Add time bar to loop\ndef addtimebar(L, threshold=1000):\n    if len(L) > threshold:\n        return tqdm(L)\n    else:\n        return L\n\n# File Reader Class\nclass FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.paper_id = content['paper_id']\n            self.abstract = []\n            self.body_text = []\n            # Abstract\n            try:\n                for entry in content['abstract']:\n                    self.abstract.append(entry['text'])\n            except KeyError:\n                pass\n                    \n            # Body text\n            try:\n                for entry in content['body_text']:\n                    self.body_text.append(entry['text'])\n            except KeyError:\n                pass\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\n\n# Helper function adds break after every words when character length reach to certain amount. This is for the interactive plot so that hover tool fits the screen.\ndef get_breaks(content, length):\n    data = \"\"\n    words = content.split(' ')\n    total_chars = 0\n\n    # add break every length characters\n    for i in range(len(words)):\n        total_chars += len(words[i])\n        if total_chars > length:\n            data = data + \"<br>\" + words[i]\n            total_chars = 0\n        else:\n            data = data + \" \" + words[i]\n    return data\n\n## composition function\n## example: compose(f1,f2,f3)(x, y) = f3(f2(f1(x, y)))\ndef compose(*funcs):\n    *funcs, penultimate, last = funcs\n    if funcs:\n        penultimate = compose(*funcs, penultimate)\n    return lambda *args: penultimate(last(*args))","613badd3":"# file path\npath = \"..\/input\/CORD-19-research-challenge\"  # may need to change when submit to kaggle\nmeta = \"metadata.csv\"\n# path for all json files\nall_jsons = glob.glob(filepath(path, '**', '*.json'), recursive=True)","0f4e1b5e":"# data.frame for meta data\nmeta_df = pd.read_csv(filepath(path, meta),\n                      dtype={'pubmed_id': str,\n                             'Microsoft Academic Paper ID': str,\n                             'doi': str,\n                             'journal':str\n                             },\n                     low_memory=False)\nprint(len(meta_df)) # number of lines in meta_df_all\nmeta_df.head(n=2)","455224a9":"# Have a look the first line of text data\nfirst_row = FileReader(all_jsons[0])\nprint(first_row)","8eb7ebf5":"# Load the text data into DataFrame\ndict_ = {'paper_id': [], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'publish_time':[], 'journal': [], 'abstract_summary': []}\nfor entry in addtimebar(all_jsons):\n    content = FileReader(entry)\n    \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    # no metadata, skip this paper\n    if len(meta_data) == 0:\n        continue\n    \n    dict_['paper_id'].append(content.paper_id)\n    dict_['abstract'].append(content.abstract)\n    dict_['body_text'].append(content.body_text)\n    \n    # also create a column for the summary of abstract to be used in a plot\n    if len(content.abstract) == 0: \n        # no abstract provided\n        dict_['abstract_summary'].append(\"Not provided.\")\n    elif len(content.abstract.split(' ')) > 100:\n        # abstract provided is too long for plot, take first 300 words append with ...\n        info = content.abstract.split(' ')[:100]\n        summary = get_breaks(' '.join(info), 40)\n        dict_['abstract_summary'].append(summary + \"...\")\n    else:\n        # abstract is short enough\n        summary = get_breaks(content.abstract, 40)\n        dict_['abstract_summary'].append(summary)\n        \n    # get metadata information\n    meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    \n    try:\n        # if more than one author\n        authors = meta_data['authors'].values[0].split(';')\n        if len(authors) > 2:\n            # more than 2 authors, may be problem when plotting, so take first 2 append with ...\n            dict_['authors'].append(\". \".join(authors[:2]) + \"...\")\n        else:\n            # authors will fit in plot\n            dict_['authors'].append(\". \".join(authors))\n    except Exception as e:\n        # if only one author - or Null valie\n        dict_['authors'].append(meta_data['authors'].values[0])\n    \n    # add the title information, add breaks when needed\n    try:\n        title = get_breaks(meta_data['title'].values[0], 40)\n        dict_['title'].append(title)\n    # if title was not provided\n    except Exception as e:\n        dict_['title'].append(meta_data['title'].values[0])\n    \n    # add publish time\n    try:\n        publish_time = get_breaks(meta_data['publish_time'].values[0], 40)\n        dict_['publish_time'].append(publish_time)\n    # if publish time was not provided\n    except Exception as e:\n        dict_['publish_time'].append(meta_data['publish_time'].values[0])\n    \n    # add the journal information\n    dict_['journal'].append(meta_data['journal'].values[0])\n    \ndf_covid = pd.DataFrame(dict_, columns=['paper_id', 'abstract', 'body_text', 'authors', 'title',  'journal', 'publish_time', 'abstract_summary'])\ndf_covid.head()","9c86a4c6":"# save data\ndf_covid.to_pickle('..\/data\/df_kaggle_all.pkl')\n# load saved data\n# with open('..\/data\/df_kaggle_all.pkl', 'rb') as fp:\n#    df_covid = pickle.load(fp)","6e67fba5":"# function to check if text of certain column in dataframe is written in certain language \ndef is_lang(row, item, lang, dropNA=True):\n    if (row[item] != None and row[item] != '' and row[item] != 'None' and isinstance(row[item], str)):\n        try:\n            return detect(row[item]) == lang\n        except Exception as e:\n            #print(\"Non-readable entity will be droped from data.frame\")\n            return False\n    else:\n        return not dropNA\n\n# select article written in certain language \ndef select_article_lang_multi(df, basedon='abstract', lang='en'):\n    return df[df.parallel_apply(lambda text: is_lang(text, basedon, lang), axis=1)]\n\ndf_covid_eng = select_article_lang_multi(df_covid)\nprint('Number of English Articles: {}\/{}'.format(len(df_covid_eng), len(df_covid)))\ndf_covid_eng.head(n=2)","beaf834c":"# save intermidiate data\ndf_covid_eng.to_pickle('..\/data\/df_kaggle_all_eng.pkl')\n# load saved data\n# with open('..\/data\/df_kaggle_all_eng.pkl', 'rb') as fp:\n#    df_covid_eng = pickle.load(fp)","007c25b3":"# Pre-processing functions\n## text level processors\ndef replace_brackets_with_whitespace(text):\n    text = text.replace('(', '')\n    text = text.replace(')', '')\n    text = text.replace('[', '')\n    text = text.replace(']', '')\n    return text\n\ndef replace_contractions(text):\n    return contractions.fix(text)\n\n# remove special characters\ndef strip_characters(text):\n    t = re.sub('\\(|\\)|:|,|;|\\.|\u2019||\u201c|\\?|%|>|<', '', text)\n    t = re.sub('\/', ' ', t)\n    t = t.replace(\"'\",'')\n    return t\n\n## word level processors:\ndef to_lowercase(word):\n    return word.lower()\n\ndef do_stemming(stemmer):\n    return lambda word: stemmer.stem(word)\n\ndef do_lemmatizing(lemmatizer):\n    return lambda word: lemmatizer.lemmatize(word, pos='v') \n\n# help function to test if word is stopword\ndef is_stopword(word):\n    return word in stopwords.words('english')\n\n# function to process word\ndef process_word_by(word_cleanner, uniqueYN):\n    def cond(word):\n        return (len(word) > 1 and\n                not is_stopword(word) and\n                not word.isnumeric() and\n                word.isalnum() and\n                word != len(word) * word[0])\n\n    def clean_byword(text):\n        return list(take_unique(uniqueYN)((word_cleanner(word) for word in text if cond(word))))\n\n    return clean_byword\n\n# function to decide making a set (unique words) from text or not\ndef take_unique(YN):\n    return set if YN else lambda x:x\n\n# function to pre_processing the text\n## compose text and word processors by combine every individual processor together \ntext_processor = compose(replace_brackets_with_whitespace, replace_contractions, strip_characters)\nword_processor = compose(to_lowercase, do_lemmatizing(WordNetLemmatizer()), do_stemming(PorterStemmer())) # it is crucial to do stemming after lemmatization\n\n## pre_processing function taking a dataframe and text and word processor functions as input and clean the text and tokenize the specified column\ndef pre_processing(df, text_tools, word_tools):\n    def inner(col, uniqueYN=False):\n        return df[col].parallel_apply(text_tools).parallel_apply(nltk.word_tokenize).parallel_apply(process_word_by(word_tools,uniqueYN=uniqueYN))\n    return inner","f7b8fa0f":"# sort by publish time\ntokenized_df = df_covid_eng.sort_values(by='publish_time', ascending=False)\ntokenized_df.head(n=3)","ad2f8019":"# created processor function with chosen text and work processors and apply it to all articles to clean and tokenize all abstracts\nprocessor = pre_processing(tokenized_df, text_processor, word_processor)\ntokenized_df['abstract_token'] = processor('abstract')\n\n# reset index (this is necessary for cosine similarity search)\ntokenized_df = tokenized_df.reset_index(drop=True)\n\n# Our processor function is a generic procedure to clean and tokenize any column with user specified column name, such as 'abstract' or 'body_text'\n# Because processing body_text takes too long, we only process abstract\n# tokenized_df['body_text_token'] = processor('body_text')","0fb52de6":"# store the dataframe to ..\/data\/\ntokenized_df.to_pickle('..\/data\/df_kaggle_all_eng_tokenized.pkl')\n# with open('..\/data\/df_kaggle_all_eng_tokenized.pkl', 'rb') as fp:\n#    tokenized_df = pickle.load(fp)\n\n# have a look at the head of the cleanned and tokenized abstract column\ntokenized_df.head()['abstract_token']","8fd1651c":"tokenized_df.head()","4bb249b6":"# create corpus\ntokenized_df['abstract_corpus'] = tokenized_df['abstract_token'].apply(lambda tokens: ' '.join(tokens))\ncorpus = tokenized_df['abstract_corpus'].tolist()\n\n#Word cloud\nfrom os import path\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\n%matplotlib inline\nwordcloud = WordCloud(\n                          background_color='white',\n                          stopwords=stopwords.words('english'),\n                          max_words=100,\n                          max_font_size=50, \n                          random_state=42\n                         ).generate(' '.join(corpus))\nfig = plt.figure(1)\nfig.set_size_inches(15,12)\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","3369fe99":"# Function to get top n-grams\ndef get_top_nK_words(corpus, K=1, n=None):\n    vec1 = CountVectorizer(max_df=0.7,stop_words=stopwords.words('english'), ngram_range=(K,K),  \n            max_features=2000).fit(corpus)\n    bag_of_words = vec1.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in     \n                  vec1.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], \n                reverse=True)\n    return words_freq[:n]","bd60d1f4":"#Convert most freq words to dataframe for plotting bar plot\ntop_words = get_top_nK_words(corpus, K=1, n=20)\ntop_df = pd.DataFrame(top_words)\ntop_df.columns=[\"Word\", \"Freq\"]\n#Barplot of most freq words\nsns.set(rc={'figure.figsize':(13,8)})\ng = sns.barplot(x=\"Word\", y=\"Freq\", data=top_df)\ng.set_xticklabels(g.get_xticklabels(), rotation=30)","3b5117cf":"# Top bi-grams\ntop2_words = get_top_nK_words(corpus, K=2, n=20)\ntop2_df = pd.DataFrame(top2_words)\ntop2_df.columns=[\"Bi-gram\", \"Freq\"]\nprint(top2_df)\n#Barplot of most freq Bi-grams\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\nh=sns.barplot(x=\"Bi-gram\", y=\"Freq\", data=top2_df)\nh.set_xticklabels(h.get_xticklabels(), rotation=45)\nfig = h.get_figure()","4bc544a5":"top3_words = get_top_nK_words(corpus, K=3, n=20)\ntop3_df = pd.DataFrame(top3_words)\ntop3_df.columns=[\"Tri-gram\", \"Freq\"]\nprint(top3_df)\n#Barplot of most freq Tri-grams\nimport seaborn as sns\nsns.set(rc={'figure.figsize':(13,8)})\nj=sns.barplot(x=\"Tri-gram\", y=\"Freq\", data=top3_df)\nj.set_xticklabels(j.get_xticklabels(), rotation=45)\nfig = j.get_figure()","1a488d27":"# compute TF-IDF scores for word vectors\ndef tfidf_(df):\n    myvectorizer = TfidfVectorizer()\n    vectors = myvectorizer.fit_transform(df['abstract_token'].parallel_apply(lambda x:' '.join(x))).toarray()\n    feature_names = myvectorizer.get_feature_names()\n    veclist = vectors.tolist()\n    out_tfidf = pd.DataFrame(veclist, columns=feature_names)\n    return out_tfidf\n\ntfidf_(tokenized_df[:20]).head()","7a187b8a":"# using sklearn is 10 times faster than self-written script\n# extract key-words with tfidf score\ntfidf_scores_df = tfidf_(tokenized_df[:20])\nN = 15 # Number of min\/max values \nu = np.argpartition(tfidf_scores_df, axis=1, kth=N).values\nv = tfidf_scores_df.columns.values[u].reshape(u.shape)\nmaxdf = pd.DataFrame(v[:,-N:]).rename(columns=lambda x: f'Max{x+1}')\nmaxdf.head()","12a0437d":"# convert query token to vector\ndef gen_vector_T(tokens):\n    Q = np.zeros((len(vocabulary)))    \n    x= tfidf.transform(tokens)\n    #print(tokens[0].split(','))\n    for token in tokens[0].split(','):\n        #print(token)\n        try:\n            ind = vocabulary.index(token)\n            Q[ind]  = x[0, tfidf.vocabulary_[token]]\n        except:\n            pass\n    return Q\n\n# calculate cosine similarity\ndef cosine_sim(a, b):\n    cos_sim = np.dot(a, b)\/(np.linalg.norm(a)*np.linalg.norm(b))\n    return cos_sim\n\n# Function to get transformed tfidf model\ndef tfidf_tran(mydf):\n    vectorizer = TfidfVectorizer()\n    vectors = vectorizer.fit_transform(mydf['abstract_token'].parallel_apply(lambda x:' '.join(x)))\n    return vectors\n\n# Define wordLemmatizer\n# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. By default it is set to Noun\ndef wordLemmatizer(data):\n    tag_map = defaultdict(lambda : wn.NOUN)\n    tag_map['J'] = wn.ADJ\n    tag_map['V'] = wn.VERB\n    tag_map['R'] = wn.ADV\n    file_clean_k =pd.DataFrame()\n    for index,entry in enumerate(data):\n        \n        # Declaring Empty List to store the words that follow the rules for this step\n        Final_words = []\n        # Initializing WordNetLemmatizer()\n        word_Lemmatized = WordNetLemmatizer()\n        # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n        for word, tag in nltk.pos_tag(entry):\n            # Below condition is to check for Stop words and consider only alphabets\n            if len(word)>1 and word not in stopwords.words('english') and word.isalpha():\n                word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n                Final_words.append(word_Final)\n            # The final processed set of words for each iteration will be stored in 'text_final'\n                file_clean_k.loc[index,'Keyword_final'] = str(Final_words).lower()\n                file_clean_k=file_clean_k.replace(to_replace =\"\\[.\", value = '', regex = True)\n                file_clean_k=file_clean_k.replace(to_replace =\"'\", value = '', regex = True)\n                file_clean_k=file_clean_k.replace(to_replace =\" \", value = '', regex = True)\n                file_clean_k=file_clean_k.replace(to_replace ='\\]', value = '', regex = True)\n    return file_clean_k","e284ebdf":"def cosine_similarity_T(k, query, text_token_df):\n    preprocessed_query  = re.sub(\"\\W+\", \" \", query).strip()\n    tokens = nltk.word_tokenize(text_processor(str(preprocessed_query).lower()))\n    tokens = [word_processor(token) for token in tokens if \n              len(token) > 1 and\n              not is_stopword(token) and\n              not token.isnumeric() and\n              token.isalnum() and\n              token != len(token) * token[0]]\n    q_df = pd.DataFrame(columns=['q_clean'])\n    q_df.loc[0,'q_clean'] =tokens\n    q_df['q_clean'] = wordLemmatizer(q_df.q_clean)\n    d_cosines = []\n    #print(q_df['q_clean'])\n    query_vector = gen_vector_T(q_df['q_clean'])\n    #print(query_vector)\n    #print(q_df['q_clean'])\n    #print(sum(query_vector))\n    for d in tfidf_tran.A:\n        d_cosines.append(cosine_sim(query_vector, d))\n    #print(d_cosines)              \n    out = np.array(d_cosines).argsort()[-k:][::-1]\n    d_cosines.sort()\n    #print(out)\n    a = pd.DataFrame()\n    firsttime=True\n    for i,index in enumerate(out):\n        try:\n            a.loc[i, 'Paper ID'] = text_token_df['paper_id'][index]\n            a.loc[i,'Title'] = text_token_df['title'][index]\n            a.loc[i, 'Summary'] = text_token_df['abstract_summary'][index]\n        except KeyError as e:\n            if firsttime:\n                print(\"Fewer matches are found than requested {}\".format(k))\n                firsttime=not firsttime\n                pass\n    for j,simScore in enumerate(d_cosines[-k:][::-1]):\n        a.loc[j,'Score'] = simScore\n    return a","c64ccd8c":"## Create Vocabulary\nvocabulary = set()\nfor tokens in tokenized_df.abstract_token:\n    vocabulary.update(tokens)\nvocabulary = list(vocabulary) \n\n# Intializating the tfIdf model\ntfidf = TfidfVectorizer(vocabulary=vocabulary)\n\n# Transform the TfIdf model\ntfidf_tran=tfidf.fit_transform(tokenized_df['abstract_token'].parallel_apply(lambda x:' '.join(x)))\n\n# search engine using cosine similarity + TF-IDF\nTFIDF_output = cosine_similarity_T(20000,'SARS-CoV-2 Covid-19 HCoV-19 Covid corona 2019-nCoV sars cov2 ncov wuhan coronavirus pneumonia',tokenized_df)\nTFIDF_output_significant = TFIDF_output[TFIDF_output['Score'] > 0]\nTFIDF_output_significant.head()","611499f6":"# store the dataframe to ..\/data\/\nTFIDF_output_significant.to_pickle('..\/data\/TFIDF_output_significant_all.pkl')\n\n# The amount of the most significant search results\nlen(TFIDF_output_significant)","d88c0261":"get_top = 500\ntop_to_print = 10\n\n#with open('..\/data\/TFIDF_output_significant_all.pkl', 'rb') as fp:\n#    TFIDF_output_significant = pickle.load(fp)\nwith open('..\/data\/df_kaggle_all_eng.pkl', 'rb') as fp:\n    df_covid_eng = pickle.load(fp)\n\ndf_covid_eng.drop_duplicates(subset=['paper_id'], inplace=True)    \nTFIDF_output_significant.drop_duplicates(subset=['Paper ID'], inplace=True)\n\npapers_to_embed = df_covid_eng.loc[df_covid_eng['paper_id'].isin(\n    TFIDF_output_significant['Paper ID'])].copy()\n\nsort_papers = TFIDF_output_significant.loc[\n    TFIDF_output_significant['Paper ID'].isin(\n        papers_to_embed['paper_id'])].sort_values(\n    by='Score', ascending=False)['Paper ID'].to_list()\npapers_to_embed = papers_to_embed.set_index('paper_id').loc[sort_papers].reset_index()","6948fd3b":"tqdm.pandas(desc='Combining abstracts and body text')\npapers_to_embed['combined_text'] = papers_to_embed.progress_apply(\n        lambda x: x['abstract'] + ' ' + x['body_text'], axis=1)\n\ntqdm.pandas(desc='Splitting abstracts into sentences')\npapers_to_embed['abstract_sentence'] = papers_to_embed[\n    'abstract'].progress_apply(tokenize.sent_tokenize)\n    \ntqdm.pandas(desc='Splitting papers into sentences')\npapers_to_embed['combined_text_sentence'] = papers_to_embed[\n    'combined_text'].progress_apply(tokenize.sent_tokenize)","b65e678f":"embedder = SentenceTransformer('bert-base-nli-mean-tokens')\n\nsent_to_embed_abstr = list(itertools.chain(\n    *papers_to_embed['abstract_sentence']))\nsent_to_embed_comb = list(itertools.chain(\n    *papers_to_embed['combined_text_sentence'].iloc[:get_top]))\n\nabstract_embed = np.array(embedder.encode(\n    sent_to_embed_abstr, batch_size=64, show_progress_bar=True))\ncomb_text_embed = np.array(embedder.encode(\n    sent_to_embed_comb, batch_size=64, show_progress_bar=True))","19676f43":"# save intermidiate data in case needed\nnp.save('..\/data\/abstr_data_encodings', abstract_embed)\nnp.save('..\/data\/comb_text_data_encodings', comb_text_embed)","8a5f9fd0":"# load intermidiate data in case needed\n# abstract_embed = np.load('..\/data\/abstr_data_encodings.npy')\n# comb_text_embed = np.load('..\/data\/comb_text_data_encodings.npy')","e20fabea":"questions = [\n    ('Evidence of animal infection with SARS-CoV-2 and its transmission to '\n     'other hosts, including the spill-over to humans.')\n]\n\nquestions_embed = np.array(embedder.encode(\n    questions, batch_size=64, show_progress_bar=True))","de2c6dec":"similarity_abstr = cosine_similarity(\n    abstract_embed, questions_embed).squeeze()\nsimilarity_comb = cosine_similarity(\n    comb_text_embed, questions_embed).squeeze()\n\nsort_args_abstr = np.argsort(similarity_abstr)[::-1]\nsim_sort_abstr = similarity_abstr[sort_args_abstr]\nsort_args_comb = np.argsort(similarity_comb)[::-1]\nsim_sort_comb = similarity_comb[sort_args_comb]\n\npaper_id_abst = np.array(list(itertools.chain(\n    *papers_to_embed.progress_apply(\n        lambda x: [x['paper_id']] * len(x['abstract_sentence']), \n        axis=1).tolist())))\npaper_id_comb = np.array(list(itertools.chain(\n    *papers_to_embed.iloc[:get_top].progress_apply(\n        lambda x: [x['paper_id']] * len(x['combined_text_sentence']), \n        axis=1).tolist())))\n\ninterest_paper_id_abstr = paper_id_abst[sort_args_abstr]\ninterest_sentences_abstr = np.array(\n    sent_to_embed_abstr)[sort_args_abstr]\ninterest_abstracts = papers_to_embed.set_index('paper_id').loc[\n    interest_paper_id_abstr]['abstract'].tolist()\n\ninterest_paper_id_comb = paper_id_comb[sort_args_comb]\ninterest_sentences_comb = np.array(\n    sent_to_embed_comb)[sort_args_comb]\ninterest_comb_text = papers_to_embed.set_index('paper_id').loc[\n    interest_paper_id_comb]['combined_text'].tolist() \n\nwith open('interesting_papers_based_on_abstract.txt', 'w') as f:\n    for paper, sent, abst, metric in zip(\n        interest_paper_id_abstr, interest_sentences_abstr, interest_abstracts, sim_sort_abstr):\n        _ = f.write('Paper ID: ' + paper + '\\n')\n        _ = f.write('Important sentence: ' + sent + '\\n')\n        # _ = f.write('Associated abstract: ' + abst + '\\n')    \n        _ = f.write('Cosine Similarity metric: ' + '{0:.3f}'.format(metric) + '\\n')\n        _ = f.write('\\n')       \n        \nwith open('interesting_papers_based_on_comb_text.txt', 'w') as f:\n    for paper, sent, comb_text, metric in zip(\n        interest_paper_id_comb, interest_sentences_comb, interest_comb_text, sim_sort_comb):\n        _ = f.write('Paper ID: ' + paper + '\\n')\n        _ = f.write('Important sentence: ' + sent + '\\n')\n        _ = f.write('Cosine Similarity metric: ' + '{0:.3f}'.format(metric) + '\\n')\n        # _ = f.write('Associated body text: ' + comb_text + '\\n')    \n        _ = f.write('\\n')        ","8d96f4f6":"print('Results based on abstract:')\nprint('\"\"\"')\nwith open('interesting_papers_based_on_abstract.txt', 'r') as f:\n    print('\\n'.join(f.read().splitlines()[:4*top_to_print]))\nprint('\"\"\"') \nprint('')\nprint('Results based on abstract and body text:')\nprint('\"\"\"')    \nwith open('interesting_papers_based_on_comb_text.txt', 'r') as f:\n    print('\\n'.join(f.read().splitlines()[:4*top_to_print]))\nprint('\"\"\"')    ","3ea5ab90":"rows_to_sample = np.random.randint(len(comb_text_embed), size=1000)\n\nsentences_subset = np.array(sent_to_embed_comb)[rows_to_sample].tolist()\nembeddings_subset = comb_text_embed[rows_to_sample] \n\n# Perform kmean clustering\nnum_clusters = 5\nclustering_model = KMeans(n_clusters=num_clusters)\n_ = clustering_model.fit(embeddings_subset)\ncluster_assignment = clustering_model.labels_\n\nclustered_sentences = [[] for i in range(num_clusters)]\nfor sentence_id, cluster_id in enumerate(cluster_assignment):\n    clustered_sentences[cluster_id].append(sentences_subset[sentence_id])\n\nfor i, cluster in enumerate(clustered_sentences):\n    print(\"Cluster \", i+1)\n    print(cluster[:10])\n    print(\"\")","b5c13909":"file_to_read = '.\/interesting_papers_based_on_comb_text.txt'\ncontent = None\nwith open(file_to_read) as f:\n    content = f.readlines()\ncontent = [x.strip() for x in content] \ncontent = [string for string in content if string != \"\"]\ntop_results = content[0:100] # Select the first n elements.\nselected_top_sentences = []\nfor elem in top_results:\n    if elem.startswith('Important sentence:'):\n        selected_top_sentences.append(elem.replace('Important sentence:','').strip())\n# Select the first n sentences.\nselected_top_sentences = selected_top_sentences[0:20]\n\n# Some settings for the plot.\npd.set_option('display.max_colwidth', 200)\n\n# The main idea is to go through a sentence and extract the subject and the object \n# as and when they are encountered.\ndef get_entities(sent):\n    ## chunk 1\n    ent1 = \"\"\n    ent2 = \"\"\n    prv_tok_dep = \"\"  # dependency tag of previous token in the sentence\n    prv_tok_text = \"\"  # previous token in the sentence\n    prefix = \"\"\n    modifier = \"\"\n    for tok in nlp(sent):\n        ## chunk 2\n        # if token is a punctuation mark then move on to the next token\n        if tok.dep_ != \"punct\":\n            # check: token is a compound word or not\n            if tok.dep_ == \"compound\":\n                prefix = tok.text\n                # if the previous word was also a 'compound' then add the current word to it\n                if prv_tok_dep == \"compound\":\n                    prefix = prv_tok_text + \" \" + tok.text\n            # check: token is a modifier or not\n            if tok.dep_.endswith(\"mod\") == True:\n                modifier = tok.text\n                # if the previous word was also a 'compound' then add the current word to it\n                if prv_tok_dep == \"compound\":\n                    modifier = prv_tok_text + \" \" + tok.text\n            ## chunk 3\n            if tok.dep_.find(\"subj\") == True:\n                ent1 = modifier + \" \" + prefix + \" \" + tok.text\n                prefix = \"\"\n                modifier = \"\"\n                prv_tok_dep = \"\"\n                prv_tok_text = \"\"\n            ## chunk 4\n            if tok.dep_.find(\"obj\") == True:\n                ent2 = modifier + \" \" + prefix + \" \" + tok.text\n            ## chunk 5\n            # update variables\n            prv_tok_dep = tok.dep_\n            prv_tok_text = tok.text\n    return [ent1.strip(), ent2.strip()]\n\n# Relation\/Predicate Extraction.\n# The hypothesis is that the predicate is actually the main verb in a sentence.\ndef get_relation(sent):\n  doc = nlp(sent)\n  # Matcher class object\n  matcher = Matcher(nlp.vocab)\n  #define the pattern\n  pattern = [{'DEP':'ROOT'},\n            {'DEP':'prep','OP':\"?\"},\n            {'DEP':'agent','OP':\"?\"},\n            {'POS':'ADJ','OP':\"?\"}]\n  matcher.add(\"matching_1\", None, pattern)\n  matches = matcher(doc)\n  k = len(matches) - 1\n  span = doc[matches[k][1]:matches[k][2]]\n  return(span.text)\n\nentity_pairs = []\n\nfor i in tqdm(selected_top_sentences):\n  entity_pairs.append(get_entities(i))\n\n# extract relationship\nrelations = [get_relation(i) for i in tqdm(selected_top_sentences)]\nprint(relations)\n\n# extract subject\nsource = [i[0] for i in entity_pairs]\nprint(source)\n\n# extract object\ntarget = [i[1] for i in entity_pairs]\nprint(target)\n\nkg_df = pd.DataFrame({'source':source, 'target':target, 'edge':relations})\n\n# Use the library networkx to build graph.\n# Create a directed graph from the dataframe first.\nsentence_graph = nx.from_pandas_edgelist(kg_df, \n                                         \"source\", \n                                         \"target\",\n                                         edge_attr=True, \n                                         create_using=nx.MultiDiGraph())\nplt.figure(figsize=(12,9)) # Change this to make plotting area for knowledge graph bigger or smaller.\n# For parameters of spring_layout, see \n# https:\/\/networkx.github.io\/documentation\/stable\/reference\/generated\/networkx.drawing.layout.spring_layout.html\n#pos = nx.spring_layout(sentence_graph)\npos = nx.spring_layout(sentence_graph,\n                      k = 1.3,\n                      iterations = 100,\n                      fixed = None,\n                      center = (0,0),\n                      scale = 4)\nnx.draw(sentence_graph, \n        with_labels=True, \n        node_color='skyblue', \n        edge_cmap=plt.cm.Blues, \n        pos=pos)\nplt.show()","9df9fba6":"### Text cleanning and tokenization\nWe process the data to clean the text with the following cleaning process and then tokenize both abstract and body_text with functionality in nltk package (nltk.word_tokenize). \n- Replace brackets\n- Replace contractions\n- Lower the case\n- Replace commas\n- Lemmatizing\n- Stemming\n- Remove number words\n- Remove punctuations\n- Remove stop words\n\nWe define a generic processor function which takes word-processors and text-processors and column name to clean the specific column in data frame with given processors. Users can easily change the word-processors and text-processors as function parameter and thus control the way how processing will be performed at both text and word level. Users can also easily choose which column in the dataframe to clean and tokenize. In principle, both abstract and body_text can be cleaned and tokenized easily with same function. For performance purpose, we only process abstract column. ","5c72a056":"### Investigate words with highest TF-IDF scores in the vectors\nWe then investigate the words with top TF-IDF scores and get some insight of our vocabulary.","01db8ce8":"### Visualize top N uni-grams, bi-grams & tri-grams using vector of word counts\nWe then create vector of word counts based on bag of words model, which ignores the sequence of the words and only considers word frequencies. Using the vector we created we then evoke the fit_transform function to learn and build the vocabulary and this allows us to visualize the top 20 unigrams, bi-grams and tri-grams and gain insight from overall articles.","6dc01d18":"#### Top Uni-grams","b35cb63c":"## Sentence embedding + BERT\n\nWord embedding is a method based on neural networks to obtain for every word in a sentence a (context dependent) vectorized representation. This representation allows comparison of words in embedding space: words closely spaced together have a similar meaning and\/or connotation, while words far apart are very dissimilar. Recent state-of-the-art models such as BERT have proven to be very good at obtaining relevant word embeddings for practical applications such as language translation. Recently, methods have been developed to use these state-of-the-art models to obtain embeddings for whole sentences. Here, we are using the method as described by Nils Reimers and Iryna Gurevych (https:\/\/arxiv.org\/abs\/1908.10084). They have open sourced their code and provide a Python package, which we are using here (https:\/\/github.com\/UKPLab\/sentence-transformers).\n\nWe are embedding every sentence of the papers that have been marked as interesting by the TF-IDF search engine. We do this for the abstracts of the papers as well as the combined text (abstract + body text) of the papers. Subsequently, we embed a constructed sentence made by us. This sentence can be viewed as the query: it contains keywords that we want to know more about from the literature we are searching.\n\nAfter embedding the sentences of the abstracts, combined text and sentence query, we compare every sentence in the abstracts and combined text to the sentence query in embedding space. To do this comparison we are using the cosine similarity measure and after comparing, we have a score for every sentence in the abstracts and combined text. A high score (close to 1) means that the compared sentences are very similar, a low score (close to 0) means that they are very dissimilar, so in this case a high score means that the compared sentence in the paper's abstract or body text is very comparable to the sentence query. Subsequently, we rank the sentences based on their score and we keep the top 50 sentences for both the abstracts and combined text. We then find the associated paper id and text of the paper for every sentence in the top 50 and store the results for human evaluation.","e705ce02":"## Conclusion and outlook\n\n### How domain experts\u2019 knowledge and data science\/AI worked hand in hand\nAs part of this short analytics journey, we have followed all phases in the data science\/analytics life cycle. In this time-constrained activity, we have deployed state of the art advanced NLP and machine learning techniques to develop an intelligent knowledge retrieval framework to study COVID-19's origin and evolution. The proposed framework will facilitate the research community to retrieve key (top level) publications by specifying most frequently used COVID-19 scientific keywords and perform semi-automatic analysis at sentence level to dig out hidden knowledge and noval information. We have demonstrated the framework's utilization and effectiveness by way of utilizing high-level keywords specified by the subject matter expert in the team. In the end, we have provided a 360 view of the acquired publications through Knowledge Graph component for ease of use for the research community.","cc6b5b30":"Here we load the sentence embeddings and we embed our sentence query. ","aeb7bb83":"#### Top Bi-grams","26b9548b":"Here we load the data stored by the TF-IDF search engine, only get the papers that it found and for the sentences of those papers we obtain and store the sentence embeddings.","65f11f76":"It is possible to cluster the embedded sentences in embedding space and print sentences associated with each found cluster for human evaluation.","9a588847":"### Compute TF-IDF for word vectors\nWe first compute TF-IDF scores based on word vectors","8826b97a":"## Data exploration and keywords summary using word cloud and n-gram frequency\n### Word cloud\nWe will now visualize the text corpus that we created after pre-processing to get insights on the most frequently used words.","fcb1a890":"## Refine query and define key-words\n\nCombining results of our data exploration and domain expert knowledge, we thus defined keywords and refined some query. This allows us to dig into our data towards more specific target and thus extract more relevant information for our specific scientific interests.\n\nFor the first query, we focused on animal host(s) and any evidence of continued spill-over to humans: Our initial approach was led by the following guiding principle: Start simple to check our model. MERS had a very limited outbreak in Saudi-Arabia with a very defined animal as a virus source - the dromedary camels. Also, the timing was very defined. Infections took place during the year 2012. We have a publication describing all the publications about MERS. This might help us to understand the result. \n\nThere is e.g. a difference between African & Arabian dromedaries regarding the infection. Keywords for first run: MERS-CoV; MERS, EMC\/2012, HCoV-EMC\/2012, dromedary camels, zoonotic, african,  spill-over, spillover, transmission, infection, human, species, progenitor, precursor, host. The results were reviewed by a domain expert and verified the adopted strategy. For the second query, we shifted the focus on a broader topic. This time, we looking into SARS-CoV2 and any evidence of continued spill-over to humans. \n\nGuiding principle: First approach showed the power of the individual approaches. Keyword filtering is a great method for efficient selection of high-quality content, but lacks the identification of important sentences. Sentence embedding has its strengths there. \n\nThereby, those are now combined with tokenization of all publications, with a focus on COVID-19. Keywords for second run: SARS-CoV-2, Covid-19, HCoV-19, Covid, corona, 2019-nCoV, sars cov2, 2019 ncov, coronavirus 2\\b, coronavirus 2019, wuhan coronavirus, coronavirus disease 19, ncov 2019, wuhan pneumonia, 2019 nov, Infection, spill-over, diagnosis, death, emergence, respiratory samples, diagnosed, mortality, transmission, Evolution, adaption, selection, species, progenitor, precursor, common ancestor, host, Cow, bird, cat, dromedary camels, Zoonotic, ferrets, bat, Pangolin, wildlife, animals, livestock. \n\nQuery for sentence embedding: \"Evidence of animal infection with SARS-CoV-2 and its transmission to other hosts, including the spill-over to humans\".\n","772cc8a4":"Here we print the results for human evaluation.","76d47387":"### Implement search engine with cosine similarity based on TF-IDF scores\n\nWe implement then a search engine using cosine similarity bewteen the query vector and our vocabulary. The matching score is based on TF-IDF we computed for our word vectors.","57fd1766":"# Investigation of COVID-19's origin and evolution with AI and Semantically inspired Knowledge Retrieval Framework (ASKRF)\n\n#### Authors:\nAnton Pols (anton.pols@roche.com), Guanya Peng (guanya.peng@roche.com), Kamran Farooq (kamran.farooq@roche.com), Markus Schmid (markus.schmid.ms3@roche.com), Vincent Wolowski (vincent.wolowski@roche.com)\n\n## Task, Goal and Methodology\n**AI and Semantically inspired Knowledge Retrieval Framework**\n\nWe developed a state of the art intelligent framework to extract the most scientifically relevant publications regarding COVID-19's origin and evolution and analyze and summarize information at sentence level by combining multiple advanced NLP techniques (BOW, TF-IDF, sentence embedding, BERT) and deep learning techniques.\n\nOur framework is comprised of following key components:\n\n**(a) NLP driven intelligent search engine**\n\nWe implemented a state of the art intelligent search engine based on the advanced machine learning and text mining techniques (TF-IDF, BOW and cosine similarity measures). In the first step, we filtered out and collated all relevant information for COVID-19, followed by the evaluation and score matching of extracted information and reviewing the search engine's final output (n-gram implementation).\n\n**(b) Semantically inspired deep learning search engine**\n\nIn the second step, we developed semantically inspired deep learning based search engine with a view to extend the framework's ability to extend information extraction at the sentence level. Other key purpose was to dig out further insights into our refined queries, which represents our specific scientific interest in this area.\n\n**(c) Knowledge graph based visualization**\n\nIn the third step, we deployed the knowledge graph method on the clustered sentences to visualize the top-level publications pertinent to various key entities. We carried out Sentence Segmentation, Entities Extraction and Relations Extraction operations to build our knowledge graph. We built these using limited entities due to time constrains to show its potential for the final scale up for future work.\nThe key selling point\/benefit of our approach is its relatively robust modular implementation with high accuracy based on well-defined key words defined by the scientific expert in the team. This framework could provide the research community an efficient knowledge retrieval mechanism for studying COVID-19's origin and evolution. The framework could be extended to further enhance the information extraction mechanism and its representation through further enhancing knowledge graph implementation thereby providing noval outputs to the research community.\n\n\n![image.png](attachment:image.png)\n\n## Structure of our work and this notebook\n- Data loading and merge meta information\n- Pre-processing and tokenization\n- Data exploration and simple keywords summary using word cloud and n-gram frequency\n- Refine query and define key-words based on domain experts' knowledge\n- TF-IDF\n- Sentence embedding + BERT\n- Results analysis, summarizing and knowledge retrieval with clustering and knowledge graph\n- Conclusion and outlook","8d45c579":"## Data loading and merge meta information\nLoad meta data and all .json files and merge the meta information and article text into a single pandas DataFrame.\n\n*Data loading approach is inspired by \nMaksimEkin and SoloNick following the very first part in [COVID-19 Literature Clustering](https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering)*","7927542f":"We define some help functions here","89ab07bd":"Here we compare every sentence in the abstracts and combined text to the sentence query in embedding space using the cosine similarity measure. Subsequently, we rank the sentences based on their cosine similarity measure and we keep the top 50 sentences for both the abstracts and combined text. We then find the associated paper id and text of the paper for every sentence in the top 50 and store the results for human evaluation.","c1d6fcbd":"#### Top Tri-grams","7b90a2a7":"### Apply TF-IDF search engine with keywords defined using expert's domain knowledge\nThe goal is to extract more relevant information and limit our investigation space within the range of COVID-19 related articles. Before applying it to COVID-19 related keywords, we also tried using MERS related keywords for a relatively smaller dataset and evaluate the results. Although the recall is limited, the accuracy we obtained is quite high. There is always a trade-off between accuracy and recall. As we are interested into a specific scientific question, our requirement with accuracy is high and with relatively large dataset and power of sentence embedding we will add later the limit of recall can be compensated.\n\nThe matching score based on TF-IDF and cosine similarity provides us a quantitative way to evaluate our search. We sort all articles based on the matching score and obtain a list of the most relevant articles.","77b65eea":"We read the metadata into a pandas dataframe. The metadata contains the following information. We will choose part of these information and merge with the actual text.\n- cord_uid\n- sha\n- source_x\n- title\n- doi\n- pmcid\n- pubmed_id\n- license\n- abstract\n- publish_time\n- authors\n- journal\n- Microsoft Academic Paper ID\n- WHO #Covidence\n- has_full_text\n- full_text_file\n- url","bb2ee155":"## Pre-processing and tokenization\n\n### Select English articles\nSince our analysis is based on English language, first check if documents are all in English with lib langdetect.\nGo through each abstract and check the language.\nNot all abstracts\/documents are in English, e.g. some are in Spanish.\nExceptions are thrown in the case where we have URLs in the abstract column.\nThe following four cases in total:\n- http:\/\/virus.zoo.ox.ac.uk\/rnavirusdb\n- http:\/\/hivweb.sanbi.ac.za\/rnavirusdb\n- http:\/\/bioinf.cs.auckland.ac.nz\/rnavirusdb\n- http:\/\/tree.bio.ed.ac.uk\/rnavirusdb\n\nWe create a new dataframe which contains only the English documents.","9d918e1f":"We now load all text data store in .json files in to a dataframe and merge with information from metadata","f0c8637f":"## TF-IDF\n\nWe implement a search engine based on TF-IDF and cosine similarity with bag of word model. A high weight in TF-IDF is reached when we have high term frequency in the given document and the low document frequency of the term in the whole collection of documents. Thus, TF-IDF helps to find frequent issues in the reviews that are not so frequent in the whole data-set, so specific issues and we want to highlight them.\n\nThe benefit of this method is relatively high accuracy with well-defined key-words. However, its power to extend the information extraction and obtain some noval output is limited. We will use this technical plus domain expert's knowledge as first step to filter-out and pool all relevant information for COVID19 and evaluate the matching score and review the output. With the pooled dataset, we apply then a more semantics approach with sentence embedding incorporate the power of deep learning to extend our ability of information extraction and dig out more insight towards our refined query, which represents our specific scientific interest.","ef57cd2d":"## Results analysis, information summarization and knowledge retrieval\nTo better understand our results out of the final sentence embedding step and summarize the information we obtained, we first cluster the sentences and draw knowledge graph to visualize the key logic behind the high score sentences selected by our model. It helps us to understand better the behavior of our model and helps to dig some hidden knowledge out of our results.\n\n### Sentence embedding clustering","727633b5":"### Visualization by knowledge graph\nThe outcome of the sentence embedding can be handily visualized using the concept of a knowledge graph where top ranked sentences are decomposed into a set of nodes and edges to represent important relationships between entities. Those entities were identified by parts of speech (POS) tags and parsing out the dependency tree of the sentence, such as nouns for example are treated as nodes and verbs as relationships. To extract more complex relationships is an ongoing effort in our NLP pipeline."}}