{"cell_type":{"5e407b73":"code","0cf02b44":"code","6eb4540f":"code","1ee791ff":"code","b477f003":"code","bbe154ed":"code","f11d84bd":"code","45daad66":"code","ea22cfe1":"code","200ff0aa":"code","6110fadf":"code","1478f955":"code","bbd127af":"code","7a270dfd":"code","ef0270b4":"code","d09bf120":"code","6898d735":"code","c14a789c":"code","6df0dce2":"code","eb29ed22":"code","c59d94ee":"code","1ebba96b":"code","18ef5a5a":"code","9677b1a2":"code","23e0fd96":"code","4f304269":"code","c67c74d8":"code","8d5e0407":"code","1b37d177":"code","6e9e69bc":"code","84d54e82":"code","4ae998b1":"code","ecfa841e":"code","4764af50":"code","00d9f4ba":"code","be733289":"code","1789ccc9":"code","8e5ec4cd":"code","db57b599":"code","c3943f8d":"code","73016bf0":"code","afcba65a":"code","b2de9101":"code","cdcf6f49":"code","390e48ee":"code","43b16d52":"code","252a2339":"code","11eed61c":"code","3e556f1e":"code","0f5ff040":"code","2333c0db":"code","9f6fe25e":"code","578a47c0":"code","2013da17":"code","aa64b0b7":"code","38516082":"code","925e7f92":"code","2f6c893e":"code","f795a815":"code","ea096374":"code","37452b53":"code","9e079cd2":"markdown","68619b25":"markdown","538c1cd3":"markdown","59bf5743":"markdown","f03b9c15":"markdown","206b8bc3":"markdown","f6d754a1":"markdown","c666033d":"markdown","3de6d998":"markdown","fe82dc77":"markdown","ac6360a3":"markdown","e910e87b":"markdown","61a02ffd":"markdown","7cc194cd":"markdown","1daf0c0f":"markdown"},"source":{"5e407b73":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0cf02b44":"import json\nimport tensorflow as tf\nimport numpy as np\nimport urllib\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport zipfile\nfrom wordcloud import WordCloud\nimport matplotlib.pyplot as plt","6eb4540f":"def parse_data(file):\n    for l in open(file,'r'):\n        yield json.loads(l)\n\nsarcasm_data = list(parse_data('\/kaggle\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json'))","1ee791ff":"len(sarcasm_data)","b477f003":"# The dataset is a list of dictionaries as shown below\n\nsarcasm_data[0:10]","bbe154ed":"# Seperate data into features and labels for further processing\n\nheadline_tmp = []\nlabels = []\n\nfor item in sarcasm_data:\n  headline_tmp.append(item['headline'])\n  labels.append(item['is_sarcastic'])","f11d84bd":"headline_tmp[0:10]","45daad66":"labels[0:10]","ea22cfe1":"# Steps to remove stopwords and punctuations\n\n# Define Stopwords\nstopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n\n# Remove stopwords from the headlines\nheadline = []\nfor sent in headline_tmp:\n   filtered_list = []\n   for word in sent.split():\n     if word not in stopwords:\n       filtered_list.append(word)\n   join_str = ' '.join([str(ele) for ele in filtered_list])\n   headline.append(join_str)","200ff0aa":"headline[0:10]","6110fadf":"# Creating data frame from the orginal json file\n\nimport pandas as pd","1478f955":"sarcasm_df = pd.read_json('\/kaggle\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset_v2.json',lines=True)","bbd127af":"sarcasm_df.head()","7a270dfd":"# Remove the article_link column from the dataframe\n\nsarcasm_df.drop('article_link', axis=1, inplace=True)","ef0270b4":"sarcasm_df.head()","d09bf120":"# filter rows where is_sarcastic = 1 from the dataframe\n\nsarcasm_df[sarcasm_df.is_sarcastic == 1]","6898d735":"# Retrieve headline column for the non-sarcastic rows\n\nsarcasm_df[sarcasm_df.is_sarcastic == 1]['headline']","c14a789c":"# Define wordcloud for above retrieved data\n\nwc = WordCloud(background_color=\"white\", max_words=1000, width = 1400, height = 700)\nwc.generate(' '.join(sarcasm_df[sarcasm_df.is_sarcastic == 1]['headline']))\nprint (\"WORD CLOUD FOR 1000 MOST FREQUENT WORDS IN SARCASTIC HEADLINES\")\nplt.figure(figsize=(15,15))\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","6df0dce2":"# Define wordcloud for the sarcastic headlines\n\nwc = WordCloud(background_color=\"white\", max_words=1000, width = 1400, height = 700)\nwc.generate(' '.join(sarcasm_df[sarcasm_df.is_sarcastic == 0]['headline']))\nprint (\"WORD CLOUD FOR 10000 MOST FREQUENT WORDS IN NON - SARCASTIC HEADLINES\")\nplt.figure(figsize=(15,15))\nplt.imshow(wc, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","eb29ed22":"sarcasm_df['is_sarcastic'].value_counts()","c59d94ee":"plt.figure(figsize=(10,10))\nplt.hist(sarcasm_df['is_sarcastic'],bins=3)\nplt.xticks([0,1])\nplt.show()","1ebba96b":"# Create a new column in the dataframe with the number of words in each headline\n\nsarcasm_df['headline_length'] = sarcasm_df['headline'].apply(lambda x: len(x.split())) ","18ef5a5a":"sarcasm_df['headline_length'].head()","9677b1a2":"import seaborn as sns\n\nsns.distplot(sarcasm_df['headline_length'])","23e0fd96":"sarcasm_df['headline_length'].describe()","4f304269":"# Get the 99.99th percentile using quantile method\n\nsarcasm_df['headline_length'].quantile(0.9999)","c67c74d8":"# Define parameters\n\n#embedding_dim = the dimnesion to which each of the words in the sentence will be encoded to as part of the training\n#max_length = Maximum length to be retained of each sentence(headline) for the training\n#trunc_type = trucate(suffix) the sentence from the back if the sentence length exceeds max_length(32)\n# padding_type = pad(suffix) the sentence with 0's at the back if the sentence length is less than max_length(32)\n#oov_token = Out Of Vocubulary token to be used if the word is not part of the vocabulary\n\nembedding_dim = 100\nmax_length = 32\ntrunc_type='post'\npadding_type='post'\noov_tok = \"<OOV>\"\ntraining_size = 20000","8d5e0407":"# Split data into training and validation datasets using the training_size parameter defined above\n\ntrain_sentences = headline[:training_size]\ntrain_labels = labels[:training_size]\n\nvalidation_sentences = headline[training_size:]\nvalidation_labels = labels[training_size:]\n\nprint(training_size)\nprint(len(train_sentences))\nprint(len(train_labels))\nprint(len(validation_sentences))\nprint(len(validation_labels))","1b37d177":"# Use tokenizer from Keras to tokenize and transform the words into numerical data \n# Use pad_sequences from keras to pad the data to make it of same length(max_length)\n\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\ntokenizer = Tokenizer(oov_token=oov_tok)\ntokenizer.fit_on_texts(headline)\nword_index = tokenizer.word_index\n\ntraining_sequences = tokenizer.texts_to_sequences(train_sentences)\ntraining_padded = pad_sequences(training_sequences, maxlen = max_length, padding = padding_type, truncating=trunc_type)\n\nvalidation_sequences = tokenizer.texts_to_sequences(validation_sentences)\nvalidation_padded = pad_sequences(validation_sequences, maxlen = max_length, padding = padding_type, truncating=trunc_type)","6e9e69bc":"# Vocabulary size will be the total number of the words in the word_index identified by the tokenizer from the training data\n\nlen(word_index)","84d54e82":"training_sequences[0]","4ae998b1":"training_padded[0]","ecfa841e":"# Convert into numpy array\n\ntraining_padded = np.array(training_padded)\nvalidation_padded = np.array(validation_padded)\ntraining_labels_pad = np.array(train_labels)\nvalidation_labels_pad = np.array(validation_labels)","4764af50":"# Define Model\n\nvocab_size = len(word_index)\n\ntf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\n# Defining Early stopping to stop training if validation accuracy does not improve within five epochs\ncallback = tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_accuracy\",\n    patience=5,\n    verbose=1,\n    restore_best_weights=True,\n)\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences = True)),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\noptimizer = tf.keras.optimizers.Adam()\nmodel.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])","00d9f4ba":"model.summary()","be733289":"history = model.fit(training_padded, training_labels_pad, batch_size = 128, epochs = 30, validation_data=(validation_padded,validation_labels_pad), callbacks = [callback])","1789ccc9":"# # Download Glove word embeddings from https:\/\/www.kaggle.com\/subrata2019\/glovedata\n# we will use the 100d file from the download\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","8e5ec4cd":"vocab_size = len(word_index) #30813\n\n# for each line in the glove embedding text file, the first value is the word and the second value is the embedding (100 dimension array)\n# Store the values into a dictionary\nembeddings_index = {}\nwith open('\/kaggle\/input\/glovedata\/glove.6B.100d.txt') as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n        embeddings_index[word]=coefs\n\n# initialize a matrix of zeros and then assign the encoding for the words in the vocabulary to the appropriate index in the embedding_matrix\nembeddings_matrix = np.zeros((vocab_size+1, embedding_dim))\nfor key in sorted(word_index, key=word_index.get)[:vocab_size]:\n    embedding_vector = embeddings_index.get(key)\n    if embedding_vector is not None:\n        embeddings_matrix[word_index[key]] = embedding_vector","db57b599":"embeddings_matrix.shape","c3943f8d":"#First 10 words and the corresponding index from the word_index dictionary which holds the vocabulary\n\nfor key in sorted(word_index, key=word_index.get)[:10]:\n  print(key,end=' ')\n  print(word_index.get(key))","73016bf0":"# Embeddings for the first 10 words\n\nfor key in sorted(word_index, key=word_index.get)[:10]:\n  print(key,end=' ')\n  print(embeddings_index.get(key))","afcba65a":"embeddings_matrix[word_index['new']]","b2de9101":"# The below model uses the weights as the embedding matrix that we built earlier\n\nvocab_size = len(word_index)\n\ntf.keras.backend.clear_session()\ntf.random.set_seed(51)\nnp.random.seed(51)\n\ncallback = tf.keras.callbacks.EarlyStopping(\n    monitor=\"val_accuracy\",\n    patience=5,\n    verbose=1,\n    restore_best_weights=True,\n)\n\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix]),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64, return_sequences=True)),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences = True)),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dropout(0.2),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\noptimizer = tf.keras.optimizers.Adam()\nmodel.compile(loss='binary_crossentropy',optimizer=optimizer,metrics=['accuracy'])","cdcf6f49":"history = model.fit(training_padded, training_labels_pad, batch_size = 128, epochs = 30, validation_data=(validation_padded,validation_labels_pad), callbacks = [callback])","390e48ee":"# To save the model\n\nmodel.save(\"sarcasm_glove_mymodel.h5\")","43b16d52":"# Retrieve and evaluate the model with the validation set \n\nmodel = keras.models.load_model('sarcasm_glove_mymodel.h5')","252a2339":"model.evaluate(validation_padded,validation_labels_pad)","11eed61c":"# Loss = 0.67\n# Accuracy = 83.57","3e556f1e":"# Prediction using the same validation dataset to plot confusion matrix\n\npred = (model.predict(validation_padded) > 0.5).astype(\"int32\")","0f5ff040":"pred.shape","2333c0db":"# Confusion Matrix\n\nfrom sklearn import metrics\n\ncm=metrics.confusion_matrix(validation_labels_pad,pred)\nprint(cm)","9f6fe25e":"import seaborn as sn\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\ndf_cm = pd.DataFrame(cm, index = [i for i in ['Not_sarcastic','Sarcastic']],\n                                  columns = [i for i in ['Not_sarcastic','Sarcastic']])\nplt.figure(figsize = (10,7))\nsn.heatmap(df_cm, annot=True, fmt = '1g')","578a47c0":"# Prediction on a test set that I downloaded from https:\/\/bestlifeonline.com\/funniest-newspaper-headlines-of-all-time\/\n\nnew_list = ['Miracle cure kills fifth patient','Cows lose their jobs as milk prices drop', 'Amphibious pitcher makes debut',\n            'State population to double by 2040, babies to blame']","2013da17":"new_list[0]","aa64b0b7":"# Preprecessing \n\n# Remove stopwords from the headlines\nnew_headline = []\nfor sent in new_list:\n    filtered_list = []\n    for word in sent.split():\n        if word not in stopwords:\n            filtered_list.append(word)\n    join_str = ' '.join([str(ele) for ele in filtered_list])\n    new_headline.append(join_str)","38516082":"new_headline","925e7f92":"# Convert to numerical data using the tokenizer\n\npred_sequences = tokenizer.texts_to_sequences(new_headline)\npred_padded = pad_sequences(pred_sequences, maxlen = max_length, padding = padding_type, truncating=trunc_type)","2f6c893e":"pred_padded","f795a815":"# Prediction\n\nnew_pred = (model.predict(pred_padded) > 0.5).astype(\"int32\")","ea096374":"new_pred","37452b53":"# Only One of the above was classified as a sarcastic headline","9e079cd2":"*Note: Embedding layer will be trained as part of the nueral network*","68619b25":"# **7. MODEL 1 - Using default Emdebbing layer of keras**","538c1cd3":"# **8. MODEL 2 - USING GLOVE EMBEDDINGS**","59bf5743":"**THANK YOU !**","f03b9c15":"# **1.IMPORTS**","206b8bc3":"## **Density Plot of headlines Lengths**","f6d754a1":"# **4. DISTRIBUTION OF LABELS**","c666033d":"## **Parameter Calculation:**","3de6d998":"From the above We can see that the majority of the sentences have a length less than 32 words and hence we will use this as the maximum length that we would keep for each of the sentence when we pre process the data below.","fe82dc77":"1. embedding (Embedding) layer :  30814 (Vocab size + 1) * 100 (Embedding Dimension for each word) = 3081400\n2. bidirectional layer: Formula is 2*4*(nm+m^2+m) = 2*4*((100*64)+(64*64)+64) = 84480\n2 is due to the layer being bidirectional\n4 is for the 4 NN's in each LSTM cell\nn = the input dimension into the layer\nm = output dimension of the layer\n3. bidirectional_1 layer : 2*4*((128*32)+(32*32)+32) = 41216\n4. dense (Dense) : 64 * 256 + 256 = 16640\n5. dense_1 (Dense) : 256 * 128 + 128 = 32896\n6. dense_2 (Dense) : 128 + 1 = 129","ac6360a3":"# **3. WORD CLOUD**","e910e87b":"## **Histogram of Labels**","61a02ffd":"# **2. LOAD DATASET**","7cc194cd":"# **5.DISTRIBUTION OF LENGTH OF HEADLINES**","1daf0c0f":"# **6. DATA PREPROCESSING**"}}