{"cell_type":{"668df967":"code","1deb6b19":"code","3d0ffbf8":"code","91725b7c":"code","33e6d554":"code","39ae7bfe":"code","88be0963":"code","f150b783":"code","de686e62":"code","0e1bdc37":"code","c9038f15":"code","8c5557a2":"code","a6ba90d0":"code","d34b3a88":"code","93bf90f3":"markdown","9974deb7":"markdown","abf9afe9":"markdown","422fa5b0":"markdown","9cdefe15":"markdown","9396cacd":"markdown","f1aaeb0d":"markdown","02d946f1":"markdown","aeddd14f":"markdown","167e6305":"markdown","df769b3e":"markdown","28de5dbb":"markdown","405b2d97":"markdown","83bfdf08":"markdown","ef08ecea":"markdown","1996ac82":"markdown","afb40943":"markdown","173ac075":"markdown","816b2e53":"markdown","45d4277e":"markdown","67496485":"markdown","d56cfd96":"markdown","8a70fd46":"markdown","7d12d0df":"markdown","ce6eea65":"markdown","4e58a476":"markdown","2e5e5143":"markdown","72d444d1":"markdown","10015d76":"markdown","15402fed":"markdown","a6eaa64c":"markdown","9effabed":"markdown","929da016":"markdown","7152fa67":"markdown","1dcf7172":"markdown","70a555b1":"markdown","6555d4c5":"markdown","d2d54670":"markdown","74c5ab3c":"markdown","3e3ea638":"markdown","f98c7226":"markdown","c4494727":"markdown","3b40647b":"markdown","74353432":"markdown","195cf951":"markdown","ce9615e8":"markdown"},"source":{"668df967":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nimport torch\nimport torch.nn as nn\nfrom torch.autograd import Variable\nfrom torch.nn import functional as F\n\nimport sys\nfrom torchtext import data\nfrom torchtext import datasets\nfrom torchtext.vocab import Vectors, GloVe\nfrom sklearn import preprocessing\nimport random\nfrom torchtext.vocab import Vectors","1deb6b19":"#Reproducing same results\nSEED = 2019\n\n#Torch\ntorch.manual_seed(SEED)\n\n#Cuda algorithms\ntorch.backends.cudnn.deterministic = True  ","3d0ffbf8":"vectors = Vectors(name='..\/input\/glove6b\/glove.6B.300d.txt')\nvectors.dim","91725b7c":"TEXT = data.Field(tokenize='spacy', lower=True,batch_first=True,include_lengths=True,fix_length=200,sequential=True)\nLABEL = data.LabelField(dtype = torch.float,batch_first=True,) \n\nfields = [(None, None), ('text',TEXT),(None,None),('sentiment', LABEL)]\n\n#loading custom dataset\ntraining_data=data.TabularDataset(path = '..\/input\/tweet-sentiment-extraction\/train.csv',format = 'csv',fields = fields,skip_header = True)\n\n#print preprocessed text\nprint(vars(training_data.examples[0]))","33e6d554":"train_data, valid_data = training_data.split(split_ratio=0.7, random_state = random.seed(SEED))","39ae7bfe":"#initialize glove embeddings\nTEXT.build_vocab(train_data,min_freq=3,vectors =vectors)  \nLABEL.build_vocab(train_data)\n\n#No. of unique tokens in text\nprint(\"Size of TEXT vocabulary:\",len(TEXT.vocab))\n\n#No. of unique tokens in label\nprint(\"Size of LABEL vocabulary:\",len(LABEL.vocab))\n\n#Commonly used words\nprint(TEXT.vocab.freqs.most_common(10))  \n\n#Word dictionary\nprint(TEXT.vocab.stoi)","88be0963":"#check whether cuda is available\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')  \n\n#set batch size\nBATCH_SIZE = 64\n\n#Load an iterator\ntrain_iterator, valid_iterator = data.BucketIterator.splits(\n    (train_data, valid_data), \n    batch_size = BATCH_SIZE,\n    sort_key = lambda x: len(x.text),\n    sort_within_batch=True,\n    device = device)","f150b783":"class AttentionModel(torch.nn.Module):  ## General attention\n    def __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n        super(AttentionModel, self).__init__()\n\n        \"\"\"\n        Arguments\n        ---------\n        batch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n        output_size : 3 = (pos, neg,neutral)\n        hidden_sie : Size of the hidden_state of the LSTM\n        vocab_size : Size of the vocabulary containing unique words\n        embedding_length : Embeddding dimension of GloVe word embeddings\n        weights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n\n        --------\n\n        \"\"\"\n\n        self.batch_size = batch_size\n        self.output_size = output_size\n        self.hidden_size = hidden_size\n        self.vocab_size = vocab_size\n        self.embedding_length = embedding_length\n\n        self.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n        self.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n        self.lstm = nn.LSTM(embedding_length, hidden_size)\n        self.label = nn.Linear(hidden_size, output_size)\n        #self.attn_fc_layer = nn.Linear()\n\n    def attention_net(self, lstm_output, final_state):\n\n        \"\"\" \n        Now we will incorporate Attention mechanism in our LSTM model. In this new model, we will use attention to compute soft alignment score corresponding\n        between each of the hidden_state and the last hidden_state of the LSTM. We will be using torch.bmm for the batch matrix multiplication.\n\n        Arguments\n        ---------\n\n        lstm_output : Final output of the LSTM which contains hidden layer outputs for each sequence.\n        final_state : Final time-step hidden state (h_n) of the LSTM\n\n        ---------\n\n        Returns : It performs attention mechanism by first computing weights for each of the sequence present in lstm_output and and then finally computing the\n                  new hidden state.\n\n        Tensor Size :\n                    hidden.size() = (batch_size, hidden_size)\n                    attn_weights.size() = (batch_size, num_seq)\n                    soft_attn_weights.size() = (batch_size, num_seq)\n                    new_hidden_state.size() = (batch_size, hidden_size)\n\n        \"\"\"\n\n        hidden = final_state.squeeze(0)\n        #print(\"++++\",hidden.unsqueeze(2).shape)\n        attn_weights = torch.bmm(lstm_output, hidden.unsqueeze(2)).squeeze(2)\n        soft_attn_weights = F.softmax(attn_weights, 1)\n        new_hidden_state = torch.bmm(lstm_output.transpose(1, 2), soft_attn_weights.unsqueeze(2)).squeeze(2)\n\n        return new_hidden_state\n\n    def forward(self, input_sentences):\n\n        \"\"\" \n        Parameters\n        ----------\n        input_sentence: input_sentence of shape = (batch_size, num_sequences)\n        batch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n\n        Returns\n        -------\n        Output of the linear layer containing logits for pos & neg class which receives its input as the new_hidden_state which is basically the output of the Attention network.\n        final_output.shape = (batch_size, output_size)\n\n        \"\"\"\n\n        input = self.word_embeddings(input_sentences) #m,200,300\n        input = input.permute(1, 0, 2)  #200,m,300\n\n        if batch_size is None:\n            h_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) #1,m,128\n            c_0 = Variable(torch.zeros(1, self.batch_size, self.hidden_size).cuda()) #1,m,128\n        else:\n            h_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n            c_0 = Variable(torch.zeros(1, batch_size, self.hidden_size).cuda())\n\n        output, (final_hidden_state, final_cell_state) = self.lstm(input, (h_0, c_0)) # final_hidden_state.size() = (1, batch_size, hidden_size) \n        output = output.permute(1, 0, 2) # output.size() = (batch_size, num_seq, hidden_size)\n        #print(\"--\",output.size(),final_hidden_state.shape)\n        attn_output = self.attention_net(output, final_hidden_state)\n        logits = self.label(attn_output)\n\n        return logits","de686e62":"def clip_gradient(model, clip_value):\n    params = list(filter(lambda p: p.grad is not None, model.parameters()))\n    for p in params:\n        p.grad.data.clamp_(-clip_value, clip_value)\n    \ndef train_model(model, train_iter, epoch):\n    total_epoch_loss = 0\n    total_epoch_acc = 0\n    model.cuda()\n    \n    optim = torch.optim.Adam(filter(lambda p: p.requires_grad, model.parameters()))\n    steps = 0\n    model.train()\n    for idx, batch in enumerate(train_iter):\n        \n        text = batch.text[0]\n        target = batch.sentiment.long()\n     \n        if torch.cuda.is_available():\n            text = text.cuda()\n            target = target.cuda()\n            \n        if (text.size()[0] is not 64):# One of the batch returned by BucketIterator has length different than 64.\n            continue\n        \n        optim.zero_grad()\n        prediction = model(text)\n        \n        #print(prediction.shape,target.shape)\n        loss = loss_fn(prediction, target)\n        \n        num_corrects = (torch.max(prediction, 1)[1].data == target.squeeze()).float().sum()\n        acc = 100.0 * num_corrects\/len(batch)\n        loss.backward()\n        clip_gradient(model, 1e-1)\n        optim.step()\n        steps += 1\n        \n        if steps % 500 == 0:\n            print (f'Epoch: {epoch+1}, Idx: {idx+1}, Training Loss: {loss.item():.4f}, Training Accuracy: {acc.item(): .2f}%')\n        \n        total_epoch_loss += loss.item()\n        total_epoch_acc += acc.item()\n        \n    return total_epoch_loss\/len(train_iter), total_epoch_acc\/len(train_iter)\n\ndef eval_model(model, val_iter):\n    total_epoch_loss = 0\n    total_epoch_acc = 0\n    model.eval()\n    \n    with torch.no_grad():\n        for idx, batch in enumerate(val_iter):\n            text = batch.text[0]\n            target = batch.sentiment.long()\n            \n            if (text.size()[0] is not 64):\n                continue\n            \n            if torch.cuda.is_available():\n                text = text.cuda()\n                target = target.cuda()\n                \n            prediction = model(text)\n            loss = loss_fn(prediction, target)\n            num_corrects = (torch.max(prediction, 1)[1].data == target.squeeze()).float().sum()\n            acc = 100.0 * num_corrects\/len(batch)\n            \n            total_epoch_loss += loss.item()\n            total_epoch_acc += acc.item()\n\n    return total_epoch_loss\/len(val_iter), total_epoch_acc\/len(val_iter)","0e1bdc37":"#define hyperparameters\nlearning_rate = 2e-5\nbatch_size = 64\noutput_size = 3\nhidden_size = 128\nembedding_length = 300\n\nmodel = AttentionModel(batch_size, output_size, hidden_size, len(TEXT.vocab), embedding_length, TEXT.vocab.vectors)\nloss_fn = torch.nn.CrossEntropyLoss()","c9038f15":"for epoch in range(10):\n    train_loss, train_acc = train_model(model, train_iterator, epoch)\n    val_loss, val_acc = eval_model(model, valid_iterator)\n    \n    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')","8c5557a2":"class SelfAttention(nn.Module):\n\tdef __init__(self, batch_size, output_size, hidden_size, vocab_size, embedding_length, weights):\n\t\tsuper(SelfAttention, self).__init__()\n\n\t\t\"\"\"\n\t\tArguments\n\t\t---------\n\t\tbatch_size : Size of the batch which is same as the batch_size of the data returned by the TorchText BucketIterator\n\t\toutput_size : 3 = (pos, neg,neutral)\n\t\thidden_sie : Size of the hidden_state of the LSTM\n\t\tvocab_size : Size of the vocabulary containing unique words\n\t\tembedding_length : Embeddding dimension of GloVe word embeddings\n\t\tweights : Pre-trained GloVe word_embeddings which we will use to create our word_embedding look-up table \n\t\t\n\t\t--------\n\t\t\n\t\t\"\"\"\n\n\t\tself.batch_size = batch_size\n\t\tself.output_size = output_size\n\t\tself.hidden_size = hidden_size\n\t\tself.vocab_size = vocab_size\n\t\tself.embedding_length = embedding_length\n\t\tself.weights = weights\n\n\t\tself.word_embeddings = nn.Embedding(vocab_size, embedding_length)\n\t\tself.word_embeddings.weights = nn.Parameter(weights, requires_grad=False)\n\t\tself.dropout = 0.8\n\t\tself.bilstm = nn.LSTM(embedding_length, hidden_size, dropout=self.dropout, bidirectional=True)\n\t\t# We will use da = 350, r = 30 & penalization_coeff = 1 as per given in the self-attention original ICLR paper\n\t\tself.W_s1 = nn.Linear(2*hidden_size, 350)\n\t\tself.W_s2 = nn.Linear(350, 30)\n\t\tself.fc_layer = nn.Linear(30*2*hidden_size, 2000)\n\t\tself.label = nn.Linear(2000, output_size)\n\n\tdef attention_net(self, lstm_output):\n\n\t\t\"\"\"\n\t\tNow we will use self attention mechanism to produce a matrix embedding of the input sentence in which every row represents an\n\t\tencoding of the inout sentence but giving an attention to a specific part of the sentence. We will use 30 such embedding of \n\t\tthe input sentence and then finally we will concatenate all the 30 sentence embedding vectors and connect it to a fully \n\t\tconnected layer of size 2000 which will be connected to the output layer of size 3 returning logits for our three classes i.e., \n\t\tpos & neg ,neutral.\n\t\tArguments\n\t\t---------\n\t\tlstm_output = A tensor containing hidden states corresponding to each time step of the LSTM network.\n\t\t---------\n\t\tReturns : Final Attention weight matrix for all the 30 different sentence embedding in which each of 30 embeddings give\n\t\t\t\t  attention to different parts of the input sentence.\n\t\tTensor size : lstm_output.size() = (batch_size, num_seq, 2*hidden_size)\n\t\t\t\t\t  attn_weight_matrix.size() = (batch_size, 30, num_seq)\n\t\t\"\"\"\n\t\tattn_weight_matrix = self.W_s2(torch.tanh(self.W_s1(lstm_output)))\n\t\tattn_weight_matrix = attn_weight_matrix.permute(0, 2, 1)\n\t\tattn_weight_matrix = F.softmax(attn_weight_matrix, dim=2)\n\n\t\treturn attn_weight_matrix\n\n\tdef forward(self, input_sentences, batch_size=None):\n\n\t\t\"\"\" \n\t\tParameters\n\t\t----------\n\t\tinput_sentence: input_sentence of shape = (batch_size, num_sequences)\n\t\tbatch_size : default = None. Used only for prediction on a single sentence after training (batch_size = 1)\n\t\t\n\t\tReturns\n\t\t-------\n\t\tOutput of the linear layer containing logits for pos & neg class.\n\t\t\n\t\t\"\"\"\n\n\t\tinput = self.word_embeddings(input_sentences)\n\t\tinput = input.permute(1, 0, 2)\n        \n\t\tif batch_size is None:\n\t\t\th_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).cuda())\n\t\t\tc_0 = Variable(torch.zeros(2, self.batch_size, self.hidden_size).cuda())\n\t\telse:\n\t\t\th_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n\t\t\tc_0 = Variable(torch.zeros(2, batch_size, self.hidden_size).cuda())\n\n\t\toutput, (h_n, c_n) = self.bilstm(input, (h_0, c_0))\n\t\toutput = output.permute(1, 0, 2)\n\t\t# output.size() = (batch_size, num_seq, 2*hidden_size)\n\t\t# h_n.size() = (1, batch_size, hidden_size)\n\t\t# c_n.size() = (1, batch_size, hidden_size)\n\t\tattn_weight_matrix = self.attention_net(output)\n\t\t# attn_weight_matrix.size() = (batch_size, r, num_seq)\n\t\t# output.size() = (batch_size, num_seq, 2*hidden_size)\n\t\thidden_matrix = torch.bmm(attn_weight_matrix, output)\n\t\t# hidden_matrix.size() = (batch_size, r, 2*hidden_size)\n\t\t# Let's now concatenate the hidden_matrix and connect it to the fully connected layer.\n\t\tfc_out = self.fc_layer(hidden_matrix.view(-1, hidden_matrix.size()[1]*hidden_matrix.size()[2]))\n\t\tlogits = self.label(fc_out)\n\t\t# logits.size() = (batch_size, output_size)\n\n\t\treturn logits","a6ba90d0":"\n\n#define hyperparameters\nlearning_rate = 2e-5\nbatch_size = 64\noutput_size = 3\nhidden_size = 128\nembedding_length = 300\n\nmodel = SelfAttention(batch_size, output_size, hidden_size, len(TEXT.vocab), embedding_length, TEXT.vocab.vectors)\nloss_fn = torch.nn.CrossEntropyLoss()","d34b3a88":"for epoch in range(10):\n    train_loss, train_acc = train_model(model, train_iterator, epoch)\n    val_loss, val_acc = eval_model(model, valid_iterator)\n    \n    print(f'Epoch: {epoch+1:02}, Train Loss: {train_loss:.3f}, Train Acc: {train_acc:.2f}%, Val. Loss: {val_loss:3f}, Val. Acc: {val_acc:.2f}%')","93bf90f3":"Now let's see the major difference between attention and self attention(highly used in recent architectures):\n\n* Attention is often applied to transfer information from encoder to decoder. I.e. decoder neurons receive addition input (via Attention) from the encoder states\/activations. So in this case Attention connects 2 different components - encoder and decoder. If Self-attention is applied - it doesn't connect 2 different components, it's applied within one component\n\n* Self-attention may be applied many times independently within a single model (e.g. 18 times in Transformer, 12 times in BERT BASE) while Attention is usually applied once in the model and connects some 2 components (e.g. encoder and decoder).\n\n* Self-attention is good at modeling dependencies between different parts of the sequence. For example - understand the syntactic function between words in the sentence. Attention on the other hand models only the dependencies between 2 different sequences (for example, the original text and the translation of the text). While still the Self-attention is good in translation task ","9974deb7":"# 4. Attention vs self attention ","abf9afe9":"# 5. Attention for text classification\n\n## 5.1 Preparing the dataset","422fa5b0":"# 1. Attention and its types","9cdefe15":"## 5.2 Using attention","9396cacd":"## Bahdanau attention\/Additive attention","f1aaeb0d":"# 3. Some working gifs\n\n## 3.1 Encoder-Decoder gif","02d946f1":"<font color='#31a04b' size=4>Can I get your attention?<\/font><br>","aeddd14f":"<img src='https:\/\/blog.floydhub.com\/content\/images\/2019\/09\/Slide38.JPG' width=1000>\n<div align=\"center\"><font size=\"3\">Source: Google<\/font><\/div>\n\n","167e6305":"The below image sums the above steps:\n\n\n<img src='https:\/\/miro.medium.com\/max\/1400\/1*IoNs3pdgl57_HqRXufZ0lA.png' width=1000 height=1000>\n<div align=\"center\"><font size=\"3\">Source: Google<\/font><\/div>\n","df769b3e":"## Hierarchical attention:","28de5dbb":"# 2. Attention and its application in different architecture\n## 2.1 Seq2seq architecture","405b2d97":"The process is as below:\n\n* Producing the Encoder Hidden States - Encoder produces hidden states of each element in the input sequence\n* Decoder RNN - the previous decoder hidden state and decoder output is passed through the Decoder RNN to generate a new hidden state for that time step\n* Calculating Alignment Scores - using the new decoder hidden state and the encoder hidden states, alignment scores are calculated\n* Softmaxing the Alignment Scores - the alignment scores for each encoder hidden state are combined and represented in a single vector and subsequently softmaxed\n* Calculating the Context Vector - the encoder hidden states and their respective alignment scores are multiplied to form the context vector\n* Producing the Final Output - the context vector is concatenated with the decoder hidden state generated in step 2 as passed through a fully connected layer to produce a new output\n* The process (steps 2-6) repeats itself for each time step of the decoder until an token is produced or output is past the specified maximum length","83bfdf08":"# Table of Contents\n\n- 1. Attention and its types\n   - 1.1 Evolution\n   - 1.1 Different types\n\n- 2. Attention and its application in different architecture\n   - 2.1 Seq2seq architecture\n   - 2.2 Different alignment scores\/functions\n   \n- 3. Some working gifs\n   - 3.1 Encoder decoder gifs\n   \n- 4. Attention vs self attention\n\n- 5. Attention for text classification\n   - 5.1 Preparing dataset\n   - 5.2 Using attention\n   - 5.3 Using self attention\n  \n- 6. Acknowledgements","ef08ecea":"## 5.2 Using self attention","1996ac82":"<img src='https:\/\/miro.medium.com\/max\/1400\/1*iK8Wel75Ri55rSZfwAKHCA.jpeg' width=1000>\n<div align=\"center\"><font size=\"3\">Source: Google<\/font><\/div>","afb40943":"<img src='https:\/\/miro.medium.com\/max\/1400\/1*ICeT6bTWmzUaGQkpKWVnLQ.png' width=1000 height=1000>\n<div align=\"center\"><font size=\"3\">Source: Google<\/font><\/div>\n","173ac075":"## Objective:\n            \n<p> We all talk about language modelling these days. There are plenty of models coming up every month or so but the core idea behind these models remains same which is `Attention`. The motivation for this notebook is to get an complete picture of attention mechanisms. It's types and various architectures being used currently for different types of problems.<\/p>","816b2e53":"The entire step-by-step process of applying Attention in Bahdanau\u2019s paper is as follows:\n\n* Producing the Encoder Hidden States - Encoder produces hidden states of each element in the input sequence\n* Calculating Alignment Scores between the previous decoder hidden state and each of the encoder\u2019s hidden states are calculated (Note: The last encoder hidden state can be used as the first hidden state in the decoder)\n* Softmaxing the Alignment Scores - the alignment scores for each encoder hidden state are combined and represented in a single vector and subsequently softmaxed\n* Calculating the Context Vector - the encoder hidden states and their respective alignment scores are multiplied to form the context vector\n* Decoding the Output - the context vector is concatenated with the previous decoder output and fed into the Decoder RNN for that time step along with the previous decoder hidden state to produce a new   output\n* The process (steps 2-5) repeats itself for each time step of the decoder until an token is produced or output is past the specified maximum length","45d4277e":"## Seq2seq architecture","67496485":"Based on this notations, each attention has a different name as follows:\n\n<img src='https:\/\/miro.medium.com\/max\/1400\/1*XzPD6cyrbWPP0r27PXVWOw.png' width=1000 height=1000>\n<div align=\"center\"><font size=\"3\">Source: Google<\/font><\/div>\n","d56cfd96":"Note:\n<p>This work is an extensive research\/study based on different resources(acknowledged below) for my personal reference as well as a knowledge sharing to the community<\/p>","8a70fd46":"<img src='https:\/\/buomsoo-kim.github.io\/data\/images\/2020-01-01\/7.png' width=700>\n<div align=\"center\"><font size=\"3\">Source: Google<\/font><\/div>","7d12d0df":"<img src='https:\/\/buomsoo-kim.github.io\/data\/images\/2020-01-01\/5.png' width=1000>\n<div align=\"center\"><font size=\"3\">Source: Google<\/font><\/div>","ce6eea65":"> As we can see, once the encoder hidden states are computed, the decoder hidden state at the previous timestep i-1 is multiplied\/summed with the all of the encoder states and softmaxed to get the aligment score which is again multiplied with encoder states to get the attention weights for each of the encoder states and then all of it are summed to get the context vector which is passed into the decoder for that time step i.","4e58a476":"## 1.2 Types","2e5e5143":"<font color='#31a04b' size=4>Kindly upvote, if you find it useful! Thanks!!<\/font><br>","72d444d1":"Here is the summary of categories for attention mechanisms:\n\n* `Self-Attention(&)`\tRelating different positions of the same input sequence.\n* `Global\/Soft`\t    Attending to the entire input state space\n* `Local\/Hard`\tAttending to the part of input state space; i.e. a patch of the input image.\n\nThere are other categories based on the alignment scores used in the attention which we will see in the next section","10015d76":"> HAN comprises two encoder networks - i.e., word and sentence encoders. The word encoder processes each word and aligns them a sentence of interest. Then, the sentence encoder aligns each sentence with the final output. HAN enables hierarchical interpretation of results as below. The user can understand (1) which sentence is crucial in classifying the document and (2) which part of the sentence, i.e., which words, are salient in that sentence.\n\n","15402fed":"## Align and translate","a6eaa64c":"* Our attention model has a single layer RNN encoder, again with 4-time steps. We denote the encoder\u2019s input vectors by x1, x2, x3, x4 and the output vectors by h1, h2, h3, h4. The attention mechanism is located between the encoder and the decoder, its input is composed of the encoder\u2019s output vectors h1, h2, h3, h4 and the states of the decoder s0, s1, s2, s3, the attention\u2019s output is a sequence of vectors called context vectors denoted by c1, c2, c3, c4.","9effabed":"## Luong attention\/Multiplicative attention:\n\n\n","929da016":"*  The RNN encoder has an input sequence x1, x2, x3, x4. We denote the encoder states by c1, c2, c3. The encoder outputs a single output vector c which is passed as input to the decoder. Like the encoder, the decoder is also a single-layered RNN, we denote the decoder states by s1, s2, s3 and the network\u2019s output by y1, y2, y3, y4.\n\n*  A potential issue with this encoder\u2013decoder approach is that a neural network needs to be able to compress all the necessary information of a source sentence into a fixed-length vector. This may make it difficult for the neural network to cope with long sentences, especially those that are longer than the sentences in the training corpus.","7152fa67":"## 2.2 Different alignment scores\/functions\n\n## Aligment score functions","1dcf7172":"Xu et al. (2015) proposed an attention framework that extends beyond the conventional Seq2Seq architecture. Their framework attempts to align the input image and output word, tackling the image captioning problem.","70a555b1":"<img src='https:\/\/miro.medium.com\/max\/1400\/1*wBHsGZ-BdmTKS7b-BtkqFQ.gif' width=1000 height=1000>\n<div align=\"center\"><font size=\"3\">Source: Google<\/font><\/div>","6555d4c5":"## Visual attention","d2d54670":"> Accordingly, they utilized a convolutional layer to extract features from the image and align such features using RNN with attention. The generated words (captions) are aligned with specific parts of the image, highlighting the relevant objects as below. Their framework is one of the earlier attempts to apply attention to other problems than neural machine translation.","74c5ab3c":"## 1.1 Evolution\n\n> Now let us see the evolution of different attentions across time:\n\n\n<img src='https:\/\/buomsoo-kim.github.io\/data\/images\/2020-01-01\/2.png' width=1000>\n<div align=\"center\"><font size=\"3\">Source: Google<\/font><\/div>\n","3e3ea638":"<img src='https:\/\/buomsoo-kim.github.io\/data\/images\/2020-01-01\/8.png' width=700>\n<div align=\"center\"><font size=\"3\">Source: Google<\/font><\/div>","f98c7226":"<img src='https:\/\/miro.medium.com\/max\/1400\/1*oosK1XGaYr0AoSxfs9fx5A.png' width=1000 height=1000>\n<div align=\"center\"><font size=\"3\">Source: Google<\/font><\/div>\n","c4494727":"Yang et al. (2016) demonstrated with their hierarchical attention network (HAN) that attention can be effectively used on various levels. Also, they showed that attention mechanism applicable to the classification problem, not just sequence generation.","3b40647b":"At a broader level attention can be classified as two types:\n\n1. Between the input and output elements (General Attention)\n2. Within the input elements (Self-Attention)\n\nLet's briefly walk through the image shown above with simple examples","74353432":"# 6 Acknowledgements\n\n1. https:\/\/blog.floydhub.com\/attention-mechanism\/\n    \n2. https:\/\/towardsdatascience.com\/attn-illustrated-attention-5ec4ad276ee3\n\n3. https:\/\/buomsoo-kim.github.io\/attention\/2020\/01\/01\/Attention-mechanism-1.md\/\n\n4. https:\/\/github.com\/prakashpandey9\/Text-Classification-Pytorch","195cf951":"## What is attention?\n\n<p>Attention is a mechanism combined in the RNN allowing it to focus on certain parts of the input sequence when predicting a certain part of the output sequence, enabling easier learning and of higher quality. With that said its not only applicable to RNNs. It can be applied to any set of problems including vision as the idea is generic.<\/p>","ce9615e8":"<img src='https:\/\/miro.medium.com\/max\/1400\/1*wnXVyE8LXPfODvB_Z5vu8A.jpeg' width=1000>\n<div align=\"center\"><font size=\"3\">Source: Google<\/font><\/div>"}}