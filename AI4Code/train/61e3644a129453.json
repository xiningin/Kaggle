{"cell_type":{"5f31ea8c":"code","76b6719c":"code","4b1b8e48":"code","6331800c":"code","4f1d78b1":"code","80cc856a":"code","b442c81d":"code","dd711a5a":"code","723948ac":"code","ac508980":"code","7be7a02c":"code","f2666dd8":"code","79dc53fb":"code","58da2ea2":"code","5ad214aa":"code","182fee42":"code","09ad561f":"code","eeec9ef1":"code","cbdb0cad":"code","09d56dbc":"code","06de21e1":"code","13c56242":"code","aaabc754":"markdown","e47899d1":"markdown","5958ffcb":"markdown","955975b9":"markdown","516ac431":"markdown","d45e2013":"markdown","432d3dcb":"markdown","d8860776":"markdown","33bb0e9a":"markdown","66684d3e":"markdown","089a0e22":"markdown"},"source":{"5f31ea8c":"# Importing packages\nimport pandas as pd\nimport numpy as np\nfrom sklearn.preprocessing import MinMaxScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split, cross_val_score, KFold, GridSearchCV\nfrom sklearn.metrics import r2_score, mean_absolute_error, mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nimport lightgbm as lgb\nimport xgboost as xgb\n\n# Warnings\nimport warnings\nwarnings.filterwarnings('ignore')","76b6719c":"# Reading data\ntrain_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\nsub_df = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')","4b1b8e48":"# Getting the number of continuous and categorical variables\ncat_cols = [x for x in train_df.columns if train_df[x].dtype == 'object']\ncont_cols = [x for x in train_df.columns if train_df[x].dtype != 'object']\n\n# Appending categorical columns and removing continous columns\ncat_cols.extend(['MSSubClass', 'OverallQual', 'OverallCond'])\nfor x in ['MSSubClass', 'OverallQual', 'OverallCond']:\n    cont_cols.remove(x)","6331800c":"# Categorical columns with missing values for training data\ntrain_miss_cat = []\nfor x in cat_cols:\n    if train_df[x].isnull().sum() > 0:\n        train_miss_cat.append(x)\n\n# Continuous cols with missing values for trainin data\ntrain_miss_cont = []\nfor x in cont_cols:\n    if train_df[x].isnull().sum() > 0:\n        train_miss_cont.append(x)\n        \n# Categorical columns with missing values for test data\ntest_miss_cat = []\nfor x in cat_cols:\n    if test_df[x].isnull().sum() > 0 and x not in train_miss_cat:\n        test_miss_cat.append(x)\n\n# Continuous cols with missing values for test data\ntest_miss_cont = []\nfor x in cont_cols:\n    if x != 'SalePrice':\n        if test_df[x].isnull().sum() > 0 and x not in train_miss_cont:\n            test_miss_cont.append(x)\n\nprint(f'Missing columns in train and test {train_miss_cat+train_miss_cont}\\n')\nprint(f'Additional Missing columns only in test {test_miss_cat+test_miss_cont}')","4f1d78b1":"# If Garage is not present in a house then all the values related to Garage should be null or 0\ngrg_built = train_df.loc[train_df.GarageYrBlt.isnull()]\nprint(grg_built.shape)\ngrg_built[['GarageYrBlt', 'GarageType', 'GarageCars', 'GarageArea', 'GarageCond', 'GarageFinish']].isnull().sum()","80cc856a":"# If MsnVnrType is None then all the values in MsnVnrArea should be null or 0\nmsn_area = train_df.loc[train_df.MasVnrArea.isnull()]\nprint(msn_area.shape)\nmsn_area[['MasVnrArea', 'MasVnrType']].isnull().sum()","b442c81d":"# If Basement is NA then all the values for Basement related colummns be null or 0\nbsnt_df = test_df.loc[test_df.BsmtFullBath.isnull()]\nprint(bsnt_df.shape)\nbsnt_df[['BsmtFullBath', 'BsmtHalfBath', 'TotalBsmtSF', 'BsmtUnfSF', 'BsmtFinSF2', 'BsmtFinSF1']].head()","dd711a5a":"# Function to impute the missing values in train and test data.\ndef impute_missing_values(train_miss_cat, train_miss_cont, test_miss_cat, test_miss_cont):\n    # Handling missing values in cat columns for train and test data\n    for column in train_miss_cat:\n        if column == 'MasVnrType':\n            train_df[column].fillna('None', inplace=True)\n            test_df[column].fillna('None', inplace=True)\n        elif column == 'Electrical':\n            train_df[column].fillna(train_df[column].mode()[0], inplace=True)\n            test_df[column].fillna(test_df[column].mode()[0], inplace=True)\n        else:\n            train_df[column].fillna(\"NA\", inplace=True)\n            test_df[column].fillna(\"NA\", inplace=True)\n            \n    # Handling NaNs in continuous cols for train and test\n    for column in train_miss_cont:\n        if column == 'MasVnrArea' or column == 'GarageYrBlt':\n            train_df[column].fillna(0, inplace=True)\n            test_df[column].fillna(0, inplace=True)\n        else:\n            train_df[column].fillna(train_df[column].median(), inplace=True)\n            test_df[column].fillna(test_df[column].median(), inplace=True)\n            \n    # Handling NaNs in cat and cont cols for test\n    for column in test_miss_cat:\n        test_df[column].fillna(test_df[column].mode()[0], inplace=True)\n    for column in test_miss_cont:\n        test_df[column].fillna(0, inplace=True)        ","723948ac":"# Imputing Nulls\nimpute_missing_values(train_miss_cat, train_miss_cont, test_miss_cat, test_miss_cont)","ac508980":"# Combining train and test data\ndata = pd.concat([train_df, test_df])\n\n# Initializing the labelencoder\nle = LabelEncoder()\n\nfor column in cat_cols:\n    if cat_cols != 'Id':\n        data[column] = le.fit_transform(data[column])\n\n# Getting the train and test data back\ntrain_df = data.iloc[:train_df.shape[0], :]\ntest_df = data.iloc[train_df.shape[0]:, :-1]","7be7a02c":"# Creating X and y variables\nX = train_df.drop(['Id', 'SalePrice'], axis=1).values\ny = train_df.SalePrice.values","f2666dd8":"# Splitting data into train and test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1)","79dc53fb":"# Creating a Baseline Model\nlinear_reg = LinearRegression()\n\n# Fitting the model with data\nlinear_reg.fit(X_train, y_train)\n\n# Making predictions on val dataser\ny_pred = linear_reg.predict(X_test)\n\n# Evaluating performance\nprint(f'Root Mean Squared Error : {np.sqrt(mean_squared_error(np.log(y_test), np.log(y_pred)))}')\nprint(f'Mean Absolute Error : {np.sqrt(mean_absolute_error(y_test, y_pred))}')\nprint(f'R2 Score : {np.sqrt(r2_score(y_test, y_pred))}')\nprint(f'Adjusted R2 Score : {1-(1-r2_score(y_test, y_pred))*((len(X_test)-1)\/(len(X_test)-len(X_test[0])-1))}')","58da2ea2":"### Using KFold validation technique\n\n# Initializing the fold\nkf = KFold(n_splits=10)\n\n# Score\nscore = []\n\nfor train_index, test_index in kf.split(X):\n    \n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    # Initializing the model\n    linear_reg = LinearRegression(normalize=True)\n\n    # Fitting the model with data\n    linear_reg = linear_reg.fit(X_train, y_train)\n\n    # Predicting on test data\n    y_pred = linear_reg.predict(X_test)\n\n    score.append(np.sqrt(mean_squared_error(np.log(y_test), np.log(abs(y_pred)))))\n\nprint(f'Mean RMSE: {np.mean(score)}')","5ad214aa":"### Using KFold validation technique\n\n# Initializing the fold\nkf = KFold(n_splits=10)\n\n# Score\nscore = []\n\nfor train_index, test_index in kf.split(X):\n    \n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    # Initializing the model\n    rf_reg = RandomForestRegressor()\n\n    # Fitting the model with data\n    rf_reg = rf_reg.fit(X_train, y_train)\n\n    # Predicting on test data\n    y_pred = rf_reg.predict(X_test)\n\n    score.append(np.sqrt(mean_squared_error(np.log(y_test), np.log(abs(y_pred)))))\n\nprint(f'Mean RMSE: {np.mean(score)}')","182fee42":"### Using KFold validation technique\n\n# Initializing the fold\nkf = KFold(n_splits=10)\n\n# Score\nscore = []\n\nfor train_index, test_index in kf.split(X):\n    \n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    # Initializing the model\n    lgbm_reg = lgb.LGBMRegressor()\n\n    # Fitting the model with data\n    lgbm_reg = lgbm_reg.fit(X_train, y_train, eval_set=(X_test, y_test), verbose=0)\n\n    # Predicting on test data\n    y_pred = lgbm_reg.predict(X_test)\n\n    score.append(np.sqrt(mean_squared_error(np.log(y_test), np.log(abs(y_pred)))))\n\nprint(f'Mean RMSE: {np.mean(score)}')","09ad561f":"### Using KFold validation technique\n\n# Initializing the fold\nkf = KFold(n_splits=10)\n\n# Score\nscore = []\n\nfor train_index, test_index in kf.split(X):\n    \n    X_train, X_test = X[train_index], X[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    \n    # Initializing the model\n    xgb_reg = xgb.XGBRegressor()\n\n    # Fitting the model with data\n    xgb_reg = xgb_reg.fit(X_train, y_train)\n\n    # Predicting on test data\n    y_pred = xgb_reg.predict(X_test)\n\n    score.append(np.sqrt(mean_squared_error(np.log(y_test), np.log(abs(y_pred)))))\n\nprint(f'Mean RMSE: {np.mean(score)}')","eeec9ef1":"# Training on the entire data and making prediction file\nlinear_reg.fit(X, y)\n\n# Predictions on test data\nsub_pred = linear_reg.predict(test_df.drop('Id', axis=1).values)\n\n# Adding to the sub df\nsub_df['SalePrice'] = sub_pred","cbdb0cad":"# Converting to csv file\nsub_df.to_csv('LinearRegressionBaseModel.csv', index=None, header=True)","09d56dbc":"# Training on the entire data and making prediction file\nrf_reg.fit(X, y)\n\n# Predictions on test data\nsub_pred = rf_reg.predict(test_df.drop('Id', axis=1).values)\n\n# Adding to the sub df\nsub_df['SalePrice'] = abs(sub_pred)\n\n# Converting to csv file\nsub_df.to_csv('RandomForestBaseModel.csv', index=None, header=True)","06de21e1":"# Training on the entire data and making prediction file\nlgbm_reg.fit(X, y)\n\n# Predictions on test data\nsub_pred = lgbm_reg.predict(test_df.drop('Id', axis=1).values)\n\n# Adding to the sub df\nsub_df['SalePrice'] = abs(sub_pred)\n\n# Converting to csv file\nsub_df.to_csv('LGBMBaseModel.csv', index=None, header=True)","13c56242":"# Training on the entire data and making prediction file\nxgb_reg.fit(X, y)\n\n# Predictions on test data\nsub_pred = xgb_reg.predict(test_df.drop('Id', axis=1).values)\n\n# Adding to the sub df\nsub_df['SalePrice'] = abs(sub_pred)\n\n# Converting to csv file\nsub_df.to_csv('XGBBaseModel.csv', index=None, header=True)","aaabc754":"# LightGBM BaseModel","e47899d1":"# Objective\n\nThe goal of this notebook is to create a base model which will serve as a standard to improve upon. Once we have a model with us we can then try feature generation technique and other encoding methods and see the improvement on the base model.<br>\nIn this notebook we will treat the missing values, label encode the categorical variables and use KFold cross validation technique to build the base model.<br>\n\nIn order for us to perform feature engineering and selection we must first have a base model to see the results of our experiment. We should quickly come to know, what techniques are giving possitive result and what are not. A good cross validation stratergy should be in place where the metric should be as close as the score on the Public Leaderboard. For example if my model score 0.13 RMSE on the validation set, the score on the leaderboard should be close to this validation score. This will give us confidence that any improvement on the validation set will have possitive improvement on the test set. That is to say that the validation set created should be similar to the test set.\n\nHere we will randomly select samples to be in the validation set, as this works for this dataset. But this is not a good stratergy when it comes to real world problems.\n\nThe models we will try are:\n1. LinearRegression\n2. RandomForest\n3. XGBoost\n4. LightGBM\n\nIn the previous 2 notebooks I have done some basic EDA and hypothesis testing. Please do check those notebooks to get an idea about the dataset and the features in the dataset.<br>\n1. [House Price problem defination and EDA](https:\/\/www.kaggle.com\/errolpereira\/house-prices-problem-definition-and-eda)\n2. [Bivariate Analysis and Hypothesis testing](https:\/\/www.kaggle.com\/errolpereira\/bivariate-analysis-and-hypothesis-testing)\n\nIn the next notebook we will try doing feature generation and implement some feature selection methods to improve the performance of our model.","5958ffcb":"# Conclusion","955975b9":"1. LinearRegression base model scored 0.47917 on the public leaderboard.\n2. XGBoost Base Model scored 0.14721 on the leaderboard.\n3. RandomForest base model scored 0.14488 on the leaderboard and\n4. Light Gradient Boosting base model scored 0.13373.\n\nLGBM base model performed the best, so we will choose this as our base model and will try and improve our score on this model only.","516ac431":"# Missing Values","d45e2013":"# RandomForestRegressor BaseModel","432d3dcb":"If no garrage is present, then it only make sense that other columns related to garage should be NA or 0.<br>\nSimilar conclusion can be made for MasVnrArea and MasVnrType, Basement related features.","d8860776":"# Cross Validation","33bb0e9a":"On the leaderboard the model scored 0.47953. The metric used is Root Mean squared error after log transforming the predicted SalePrice and actual SalePrice.<br>\nWe will have to do the same while evaluating our model on validation data so that we get an idea of how the model will perform.<br>\nAlso if the validation data changes the model performances changes. That means the model has variance in it. A strong cross validation technique is required to average out the model performance.","66684d3e":"# Label Encoding Categorical Features.","089a0e22":"# Train and Validation Split"}}