{"cell_type":{"b3fdf52c":"code","6db36ee0":"code","ff2c708b":"code","8ce8cfa9":"code","7e6eca26":"code","defef0a4":"code","9055fb13":"code","c15944ef":"code","1107436d":"code","e913aedf":"code","bf9beb8c":"code","0fb76241":"code","88a15462":"code","7bc711c7":"code","18e3bb37":"code","acb6ace4":"code","f94ab592":"code","172b256f":"code","b293b2cf":"code","a7a122dc":"code","70ceb6f5":"code","81455702":"code","6b1dcde5":"code","3f2395fa":"code","a237df63":"code","2d61c311":"code","0f25af4f":"code","3876b35e":"code","fa6f1772":"code","769fa6a9":"code","a99a7777":"code","9c350d51":"code","54b3bc69":"code","3ce4f42c":"code","d63e40f6":"code","e41933dd":"code","159a6b6c":"code","1a2a5080":"code","36969646":"code","d4b6c144":"code","2f4c34fc":"code","3dc4bd1f":"code","d93d3e39":"code","05d58c0c":"code","cdf0e18c":"code","6f2e2200":"markdown","53e2a3df":"markdown","9a884a68":"markdown","c1d389d1":"markdown","96109d20":"markdown","98c22fcc":"markdown","76c2b34d":"markdown","f644ffba":"markdown","c8078302":"markdown","98d362e2":"markdown","ddaadd50":"markdown","22734f81":"markdown","0c9049c9":"markdown","765c7380":"markdown","9fb4fa8d":"markdown","2d35ce46":"markdown","ab1668c5":"markdown","cea56844":"markdown","59926fb1":"markdown","f54f95b5":"markdown","c5111da5":"markdown","cc3a20d8":"markdown","616dc2f1":"markdown","87730a09":"markdown","9c18f088":"markdown"},"source":{"b3fdf52c":"# downloading BERT module \n!pip install bert-for-tf2\n!pip install sentencepiece","6db36ee0":"# Visualization libraries\n\nimport seaborn as sns\nimport matplotlib as plt\n\n%matplotlib inline\nplt.style.use('ggplot')\n\n# Analysis libraries\n\nimport os\nimport re\nimport random\nimport numpy as np \nimport pandas as pd \n\nimport nltk\nimport string\nfrom bs4 import BeautifulSoup\nfrom nltk.corpus import stopwords\n\n# ML Modelling Libraries\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import StratifiedKFold, RepeatedStratifiedKFold\nfrom sklearn.metrics import f1_score, precision_score,recall_score,roc_auc_score\nfrom sklearn.metrics import accuracy_score, plot_precision_recall_curve\n\nimport xgboost as xgb\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import RandomForestClassifier\n\n\n# BERT and Deep Learning Libs\nimport bert\nimport tensorflow_hub as hub\nimport tensorflow as tf\nfrom keras import layers\nfrom keras.callbacks import ModelCheckpoint,EarlyStopping\nfrom keras.preprocessing.sequence import pad_sequences\nimport tensorflow_datasets as tfds\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","ff2c708b":"ner = pd.read_csv('\/kaggle\/input\/entity-annotated-corpus\/ner.csv', \n                  encoding= 'ISO-8859-1',\n                  error_bad_lines=False)\nner_dataset = pd.read_csv('\/kaggle\/input\/entity-annotated-corpus\/ner_dataset.csv',\n                  encoding='ISO-8859-1',\n                  error_bad_lines=False)\nstock_data = pd.read_csv('\/kaggle\/input\/stockmarket-sentiment-dataset\/stock_data.csv',\n                  encoding='ISO-8859-1',\n                  error_bad_lines=False)","8ce8cfa9":"ner.columns","7e6eca26":"ner = ner[['prev-word','prev-pos','word','pos','next-word','next-pos','tag']]\nner.head(10)","defef0a4":"plt.pyplot.figure(figsize=(30,15))\nplt.pyplot.title('Parts of Speech vs Frequency')\nsns.set(font_scale=1.5)\nsns.countplot(x='pos', data = ner, palette = 'magma');","9055fb13":"ner.pos.unique()","c15944ef":"# Top 50 most commonly named places\ndata = ner.query('pos == \"NNP\"').word.value_counts().reset_index().head(50)\n\n# named places plot\n\nplt.pyplot.figure(figsize=(75,35))\nplt.pyplot.title('Proprt Nouns vs Frequency')\nsns.set(font_scale=1.6)\nsns.barplot(x='index',y='word', data =data, palette = 'magma');\n\ndel data","1107436d":"ner.fillna(\"None\",inplace=True)\nner.tag.unique()","e913aedf":"# Filtering out organisations\ndf = ner[ner.tag.map(lambda x : x[-3:] == \"org\")]\nplt.pyplot.title(\"Number and types of organisations\")\ndf.tag.value_counts().plot.bar();","bf9beb8c":"# no cleaning function is required to be passed since we are training model on entire corpus\n\nlb_tag = LabelEncoder().fit(ner.tag)\nner.tag = lb_tag.fit_transform(ner.tag)\n\nlb_pos= LabelEncoder().fit(ner.pos)\nner.pos = lb_pos.fit_transform(ner.pos)\n\nner['prev-pos'] = lb_pos.fit_transform(ner['prev-pos'])\nner['next-pos'] = lb_pos.fit_transform(ner['next-pos'])","0fb76241":"# import BERT layer from Tensorflow hub URL\nbert_layer = hub.KerasLayer(\"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\",\n                            trainable=False)\n\nvocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n\n# Using BERT's inbuilt tokenizer\n\ntokenizer = bert.bert_tokenization.FullTokenizer(vocab_file, do_lower_case)","88a15462":"def vectorize(series):\n    # Converting words into word vectors to feed into Model using BERT Tokenizer\n    series.fillna(\"None\", inplace = True)\n    series = series.apply(lambda word : tokenizer.convert_tokens_to_ids(tokenizer.tokenize(word)))\n    # Tokenizer returns list hence extracting numbers from it\n    return series.map(lambda x: 0 if len(x) == 0 else x[0])\n\nner.word = vectorize(ner.word)\nner['prev-word'] = vectorize(ner['prev-word'])\nner['next-word'] = vectorize(ner['next-word'])","7bc711c7":"num_classes = ner.tag.nunique()\nnum_classes","18e3bb37":"X = ner.drop(columns='tag').values\ny = ner['tag'].values\n\nprint(X.shape)\ny = y.reshape(y.shape[0])\nprint(y.shape)","acb6ace4":"# Splitting into Train\/Test sets\n\nX_train, X_test,y_train, y_test = train_test_split(X,y,test_size = 0.20)\nprint(X_train.shape)\nprint(y_train.shape)\nprint(X_test.shape)\nprint(y_test.shape)","f94ab592":"# Implementing -- XGBoost, SVM, RandomForest\n\n#clf = RandomForestClassifier() # 96.4, High Precision Very Low Recall\n#clf = SVC(1.6)\n#Xgb = 93.5 , but better balanced precision and recall\n\nparams_grid = {\n    'validate_parameters':True,\n    'lambda':1.6,\n    'num_class': num_classes,\n    'objective': 'multi:softmax',\n    'eval_metric': 'merror'}","172b256f":"# Training over 3 stratified folds\n\nskf = StratifiedKFold(n_splits=4, random_state=42)\nx=1\nfor train_idx, valid_idx in skf.split(X_train,y_train):\n    \n    Xtrain, Xvalid = X_train[train_idx], X_train[valid_idx]\n    ytrain, yvalid = y_train[train_idx], y_train[valid_idx]\n    \n   # clf.fit(Xtrain,ytrain)\n    \n    dtrain = xgb.DMatrix(Xtrain,label=ytrain)\n    dtest = xgb.DMatrix(Xvalid, label=yvalid)\n    \n    evallist = [(dtest, 'eval'), (dtrain, 'train')]\n    \n    bst = xgb.train(params_grid,dtrain,evals = evallist)\n    \n    print(\"fold: \",x)\n    x=x+1\n    print('f1_score: ', f1_score(yvalid,bst.predict(xgb.DMatrix(Xvalid)),average=\"micro\"))","b293b2cf":"d_test = xgb.DMatrix(X_test)\n\npreds=bst.predict(d_test).astype('int32')\n\nprint(\"f1 score : \", f1_score(y_test,preds, average = \"micro\"))\nprint(\"precision score : \", precision_score(y_test,preds, average = \"macro\"))\nprint(\"recall score : \", recall_score(y_test,preds, average = \"macro\"))","a7a122dc":"#fig = plot_precision_recall_curve(clf,X_test,y_test)\n#plt.pyplot.title(\"Precision vs Recall\")\nxgb.plot_importance(bst);","70ceb6f5":"preds=lb_tag.inverse_transform(preds)\npreds","81455702":"# changing labels of stock data to 0 and 1\nstock_data['Sentiment'] = stock_data['Sentiment'].apply(lambda x: 0 if x == -1 else 1)\nstock_data.head(10)","6b1dcde5":"stock_data.Sentiment.value_counts().plot.bar();\nplt.pyplot.title(\"Sentiment counts\");","3f2395fa":"# creating new column containing sentence lengths to plot\nstock_data['Sentence_length'] = [len(stock_data.Text[i]) for i in range(5791)]\n\nplt.pyplot.figure(figsize =(25,10))\nsns.lineplot(data=stock_data['Sentence_length'],color ='r');\nplt.pyplot.title(\"Distribution of sentence lengths\");\nplt.pyplot.savefig('lineplot.png')","a237df63":"plt.pyplot.figure(figsize = (20,10));\nplt.pyplot.title(\"Distribution of sentence lengths\");\nsns.distplot(stock_data.Sentence_length,kde=True,color ='r',bins=70);","2d61c311":"# clean text - lemmitization and removing stop-words, URLs, punctuations and special chars\n\nstopwords = stopwords.words('english') # from nltk module\ndef preprocess(sentence):\n    \n    result = []\n    \n    s = BeautifulSoup(sentence, \"lxml\").get_text()\n    \n    # Removing the URL links\n    s = re.sub(r\"https?:\/\/[A-Za-z0-9.\/]+\", ' ', s)\n    \n    # Keeping only letters\n    s = re.sub(r\"[^a-zA-Z.!?']\", ' ', s)\n    \n    # Removing additional whitespaces\n    s = re.sub(r\" +\", ' ', s)\n    \n    token_list = tokenizer.tokenize(s)\n    \n    for token in token_list:\n        if (token not in list(string.punctuation))and(token not in stopwords):\n            result.append(token)\n        else:\n            continue\n    \n    return result\n\n# Adding Classification and Separator token for each sentence -- BERT input format\n\ndef add_std_tokens(token_list):\n    return [\"[CLS]\"] + token_list + [\"[SEP]\"]","0f25af4f":"# FUNCTION 1: TO GET WORD VECTOR FROM A LIST OF TOKENS\n\ndef get_ids(tokens):\n    return tokenizer.convert_tokens_to_ids(tokens)\n\n\n# FUNTION 2: TO GET WHETHER OUR TOKENS HAVE [PAD] PADDING OR NOT\n# NOTE: In this case it is not important but we will use it to maintain general norm\n\ndef get_masks(tokens):\n    return np.char.not_equal(tokens, '[PAD]').astype(int)\n\n\n# FUNCTION 3 : TO GET ID's OF SEGMENTATION TOKENS\n\ndef get_segs(tokens):\n    curr_seg_id=0\n    seg_ids =[]\n    for tok in tokens:\n        seg_ids.append(curr_seg_id)\n        if tok==\"[SEP]\":\n            curr_seg_id = 1- curr_seg_id\n            \n            # 1 becomes 0 and 0 becomes 1\n            # 1 denoting [SEP] token and 0 any other token\n            \n    return seg_ids","3876b35e":"# Applying text cleaning and merging labels and lengths for sorting\n\nlabels = stock_data.Sentiment.values\ncleaned_data = [add_std_tokens(preprocess(sent)) for sent in stock_data.Text]\n\n\ndata_with_len = [[sent, labels[i], len(sent)]\n                 for i, sent in enumerate(cleaned_data)]\n\n# Shuffle and Sort the dataset\n\nrandom.shuffle(data_with_len)\n\ndata_with_len.sort(key=lambda x: x[2])\n\n# Applying the 3 functions to get input in appropriate format\n\ncompiled_data = [([ get_ids(sent_idx[0]), list(get_masks(sent_idx[0])), get_segs(sent_idx[0])],\n                    sent_idx[1]) for sent_idx in data_with_len]\n","fa6f1772":"batch_size = 32\nnum_batches = len(compiled_data) \/\/ batch_size #180\nnum_test_batches = num_batches \/\/ 15           #12","769fa6a9":"# making tf.data.Dataset generator objects\ndataset_gen = tf.data.Dataset.from_generator(lambda : compiled_data, \n                                             output_types=(tf.int32,tf.int32))\n\n# using the padded batch function to make a batch generator\nbatch_gen = dataset_gen.padded_batch(batch_size, \n                                     padded_shapes=((3,None),()), \n                                     padding_values = (0,0))\n\n# using the shuffle attribute\nbatch_gen.shuffle(num_batches)\n\n# getting batched tensor datasets from generator\ntrain_data = batch_gen.skip(num_test_batches)\ntest_data = batch_gen.take(num_test_batches)","a99a7777":"y = stock_data.Sentiment.values\nX = [get_ids(preprocess(sent)) for sent in stock_data.Text]\nX = pad_sequences(X)\n\nX_train, X_test, y_train, y_test = train_test_split(X,y, test_size=0.1,shuffle=True)\n\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)\n\n# parameters for XGBClassifier\nparam_dist = {'objective':'binary:logistic', 'n_estimators':2}","9c350d51":"x=1\n\nclf = xgb.XGBClassifier(**param_dist)\n\nkf = StratifiedKFold(n_splits = 50, shuffle= True, random_state=42)\n\nfor train_idx, valid_idx in kf.split(X_train,y_train):\n    \n    Xtrain, Xvalid = X_train[train_idx], X_train[valid_idx]\n    ytrain, yvalid = y_train[train_idx], y_train[valid_idx]\n    \n    clf.fit(Xtrain, ytrain,\n        eval_set=[(Xtrain, ytrain), (Xvalid, yvalid)],\n        eval_metric='logloss',\n        verbose = False)\n\n    evals_result = clf.evals_result()\n    \n    print(\"fold: \",x)\n    x=x+1\n    print(evals_result)","54b3bc69":"preds = clf.predict(X_test)\n\nprint(\"f1_score : \", f1_score(y_test,preds,average=\"micro\"))\nprint(\"precision: \", precision_score(y_test,preds, average=\"macro\"))\nprint(\"recall: \", recall_score(y_test,preds, average=\"macro\"))","3ce4f42c":"class DCNN(tf.keras.Model):\n    \n    # making a constructor for default params\n    def __init__(self,\n                 FC_units =512,\n                 num_filters=32,\n                 num_classes=2,\n                 droupout = 0.2,\n                 name = \"DCNN\"):\n        \n        # calling superclass constructor\n        super(DCNN,self).__init__(name=name)\n        \n        # adding layers to DCNN model object\n        \n        self.bert_layer = hub.KerasLayer(\n                            \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-12_H-768_A-12\/1\",\n                            trainable=False)\n        \n        self.bigram_layer = layers.Conv1D(\n                                filters=num_filters,\n                                kernel_size=2,\n                                padding='valid',\n                                activation ='relu')\n        \n        self.trigram_layer = layers.Conv1D(\n                                filters=num_filters,\n                                kernel_size=3,\n                                padding='valid',\n                                activation ='relu')\n        \n        self.fourgram_layer = layers.Conv1D(\n                                filters=num_filters,\n                                kernel_size=4,\n                                padding='valid',\n                                activation ='relu')\n        \n        self.batchnorm = layers.BatchNormalization()\n        self.layernorm = layers.LayerNormalization()\n        self.pool_layer = layers.GlobalMaxPool1D()\n        self.dense_layer = layers.Dense(FC_units,activation='relu')\n        self.dropout_layer = layers.Dropout(rate=dropout_rate)\n        \n        if num_classes == 2:\n            self.output_layer = layers.Dense(units=1,\n                                           activation=\"sigmoid\")\n        else:\n            self.output_layer = layers.Dense(units=nb_classes,\n                                           activation=\"softmax\")\n            \n    # Embed Tensors into BERT Layer, embs gives output\n    def embed_with_bert(self, all_tokens):\n            \n        _, embs = self.bert_layer([all_tokens[:, 0, :],\n                                   all_tokens[:, 1, :],\n                                   all_tokens[:, 2, :]])\n        return embs\n\n        \n    # Implement the Architecture in the call function\n    def call(self, inputs, training):\n            \n        x = self.embed_with_bert(inputs)\n            \n        bigram = self.bigram_layer(x)\n        bigram = self.layernorm(bigram)\n        bigram = self.batchnorm(bigram)\n        bigram = self.pool_layer(bigram)\n            \n        trigram = self.trigram_layer(x)\n        trigram = self.layernorm(trigram)\n        trigram = self.batchnorm(trigram)\n        trigram = self.pool_layer(trigram)\n        \n        fourgram = self.fourgram_layer(x)\n        fourgram = self.layernorm(fourgram)\n        fourgram = self.batchnorm(fourgram)\n        fourgram = self.pool_layer(fourgram)\n        \n        merged = tf.concat([bigram, trigram, fourgram],axis=-1) \n        # (batch_size, 4 * num_filters)\n        merged = self.dense_layer(merged)\n        merged = self.dropout_layer(merged)\n        output = self.output_layer(merged)\n            \n        return output\n        ","d63e40f6":"from keras.optimizers import Adam\nfrom keras.metrics import BinaryAccuracy\nopt = Adam(learning_rate =0.001)","e41933dd":"# Callback to prevent overfit\n\nearly_stopping_callback =  EarlyStopping(monitor = 'val_accuracy',\n                                         min_delta = 0.01,\n                                         patience = 6,\n                                         restore_best_weights=True)\n\nFC_units = 64\nnum_filters = 4\nnum_classes = 2\ndropout_rate = 0.2\nbatch_size = 32\nnum_epochs = 16\n\n# Making model\nmodel = DCNN(FC_units = FC_units,\n             num_filters=num_filters,\n             num_classes=num_classes,\n             droupout = dropout_rate)\n\n# Compiling\nmodel.compile(loss=\"binary_crossentropy\",\n              optimizer=opt,\n              metrics=[\"accuracy\"])","159a6b6c":"num_train_batches = num_batches - num_test_batches\nnum_valid_batches = num_train_batches \/\/ 5\n\n\nx_train=train_data.skip(num_valid_batches) \nx_valid=test_data.take(num_valid_batches) ","1a2a5080":"# Fitting data using crossvalidation\nhistory = model.fit(x_train, \n                    epochs=num_epochs,\n                    validation_data = x_valid,\n                    callbacks =[early_stopping_callback])","36969646":"print(history.history.keys())","d4b6c144":"plt.pyplot.plot(history.history['accuracy'])\nplt.pyplot.plot(history.history['loss'])\nplt.pyplot.gcf().set_size_inches(20,10)\nplt.pyplot.legend(['accuracy','loss'])\nplt.pyplot.title('Training Performance')\nplt.pyplot.ylabel('Accracy and Loss')\nplt.pyplot.xlabel('Epoch')\nplt.pyplot.savefig('trainperf.png')\nplt.pyplot.show()","2f4c34fc":"plt.pyplot.plot(history.history['accuracy'])\nplt.pyplot.plot(history.history['loss'])\nplt.pyplot.gcf().set_size_inches(20,10)\nplt.pyplot.legend(['val_accuracy','val_loss'])\nplt.pyplot.title('Testing Performance')\nplt.pyplot.ylabel('Accracy and Loss')\nplt.pyplot.xlabel('Epoch')\nplt.pyplot.show()","3dc4bd1f":"results = model.evaluate(test_data)\nresults","d93d3e39":"y_pred = model.predict(test_data)\ny_pred.shape","05d58c0c":"lis=[]\ny = tfds.as_numpy(test_data)\nfor i,j in enumerate(y):\n    tensors,labels = j\n    lis.extend(labels)\n    \ny_true = np.array(lis,dtype='float32')\ny_true = y_true.reshape(y_pred.shape[0],)\ny_pred = y_pred.reshape(y_pred.shape[0],)","cdf0e18c":"print(\"ROC AUC score : \", roc_auc_score(y_true,y_pred,average=\"micro\"))\n\n# making predictions discrete\ny_pred=y_pred>0.5\ny_pred=y_pred.astype(int)\n\nprint(\"f1_score: \", f1_score(y_true,y_pred,average=\"micro\"))\nprint(\"precision_score: \", precision_score(y_true,y_pred,average=\"macro\"))\nprint(\"recall_score: \", recall_score(y_true,y_pred,average=\"macro\"))","6f2e2200":"# 1. NAMED ENTITY RECOGNITION\n\nUsing BERT as a tokenizer and quick word embedding algorithm. To make the classifier we will use the Machine Learning algorithms Random Forest and Support Vector Machine from Scikit Learn, and the ensemble Gradient Boosted Trees Algorithm XGBoost.","53e2a3df":"We can see BERT as embedding layer improves model performance. There is an overfit in this model, but it still gives better results than Machine Learning models using BERT as tokenizer","9a884a68":"Model Making and Training","c1d389d1":"Lets import the pretrained BERT layer from TF HUB to harness its tokenizer","96109d20":"# INTRODUCTION","98c22fcc":"# 1.2 - Exploratory Data Analysis and Visualizations","76c2b34d":"Implementing model architecture","f644ffba":"# 1.5 - Prediction and Evauation","c8078302":"# 2. SENTIMENT ANALYSIS\n\nUsing BERT as an embedding layer and a custom convolutional neural network trained for classification tasks such as sentiment analysis. BERT as an embedding layer takes specific types of inputs. Each sentence needs to be padded and we need to add a classification [CLS] token and separator token [SEP]. We have to pad the input data, and create 3 types of inputs for each sentence:\n\n(1) Word Vectors of tokens in sentence\n(2) Which indexes are Padding Tokens\n(3) Which indexes are Seperator Tokens\n\nFor this purpose we will create 3 fuctions\n\nNOTE: we have not used the PAD token anywhere","98d362e2":"Creating Dataset with Appropriate Format for for BERT layer.\nApplying the three functions on the shuffled and sorted data in format : - \n\n( [wordvecs] , [pads] , [seps] , labels )\n\nNOTE : keras.preprocessing.sequence.pad_sequences cannot be used because it doesnt support string and int.","ddaadd50":"# 2.4 - Prediction and Evaluation ","22734f81":"# 1.1 Importing Dependencies","0c9049c9":"Making the model using keras subclassing API. The idea behind the architecture is to embed word vectors using the pretrained BERT layer as an embedding layer. The inputs are embedded into the model which are passed to three Convolutional Neural Networks - Bigram, Trigram, Fourgram, having kernel_size = 2,3,4 respectively. The models are then concatenated and dense layers are applied to obtain output","765c7380":"# 1.3 - Data Pre-processing and Splitting","9fb4fa8d":"BERT (Bidirectional Encoder Representations from Transformers), is the ultimate Natural Language Processing Algorithm created by Google's AI team. It's best for tasks that require only encoding of words into word vectors, since it does not come with a decoder module. BERT implements the Transformer model and stacks them in an encoder fashion. It comes pretrained for several days on TPUs so we will just harness the power of transfer learning in this note book to execute simple tasks like Named Entity Recognition and Sentiment Analysis on the Entity Annotated and Stock Market Sentiment Datasets.\n\nNamed Entity Recognition - Is the process of finding proper nouns in dataset, and for this purpose we will be using the entity-annotated-corpus to train an NER classifier model, using a Random Forest or Gradient Boosted Trees Classifier and BERT's inbuilt tokenizer. Then we will try to implement BERT as an embedding layer and compare the results\n\nSentiment Classification - Is the process of calculating the overall positive or negative impact of a sentence or paragraph and classifying it as having a negative or positive impact on the price of the stock of the company (in this case). For this task we will implement a baseline XGBoost Model, using the bert tokenizer module, and then a custom Convolutional neural Network based architecture using BERT as an embedding layer, and compare the two performances.\n\n\n\nWe will be using the bert-for-tf2 module and pretrained weights for the BERT variant : BERT encased L-12 H-768, from tensorflow hub, which is an open source collection of many pretrained models for tensorflow.","2d35ce46":"# 1.4 - Model using XGBoost","ab1668c5":"Training model ","cea56844":"Using tf.data.Dataset module to make a padded Dataset for BERT Layer","59926fb1":"# 2.1 - EDA and Pre-Processing","f54f95b5":"Prediction and Evaluation","c5111da5":"# AGENDA :\n\n-- To Implement BERT as a tokenizer and embedding algorithm for Named Entity Recogntion\n\n-- Making a baseline model for sentiment analysis\n\n-- To Implement BERT as an Embedding layer in a model for Sentiment Analysis\n\n-- Comparing results for BERT as word embedder and embedding layer in model","cc3a20d8":"# 2.2 - Baseline Model - XGBClassifier\n\nAccuracy of 63 % was achieved","616dc2f1":"# 2.3 - CNN with BERT Embedding Layer","87730a09":"Preprocessing functions","9c18f088":"Extracting values from nested Tensors in tf.data.Dataset objects"}}