{"cell_type":{"28c87ec8":"code","7fbddfea":"code","80aa95db":"code","1e3239e7":"code","5552ec06":"code","dc00bebe":"code","4fc3bde5":"code","92405f76":"code","5687be6b":"code","8e5f79a5":"code","02c7e960":"code","add7bc92":"code","7401fb01":"code","2d8175cb":"code","c067110b":"code","0fd0d68b":"code","0a12d0f1":"code","fcbeefb4":"code","08318d58":"code","a6590418":"code","669a8769":"code","cb23596e":"code","4caa570f":"code","a0d6c48d":"code","b4af3855":"code","77c54656":"code","8901cc3c":"markdown","e0814604":"markdown","a33d100b":"markdown","f5a970e5":"markdown","31572767":"markdown","11904596":"markdown","5c31921c":"markdown","6505c0a3":"markdown","a0a6b9b8":"markdown","92c7fc08":"markdown","00ba44ec":"markdown","6ee412fb":"markdown","f815c8b5":"markdown"},"source":{"28c87ec8":"import pandas as pd\n\ndf = pd.read_csv(\"..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\")\ndf.head()\n","7fbddfea":"def give_me_text_and_labels(input_csv=\"..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\"):\n    df = pd.read_csv(input_csv)\n    \n    df['label'] = [1 if x==\"positive\" else 0 for x in df['sentiment'] ]\n    return df['review'].values, df['label'].values\n    ","80aa95db":"texts, labels = give_me_text_and_labels()","1e3239e7":"texts[0][:100], labels[0], len(labels)","5552ec06":"train_length = 40000\ntrain_texts, train_labels = texts[:train_length], labels[:train_length]\ntest_texts, test_labels = texts[train_length:], labels[train_length:]","dc00bebe":"from sklearn.model_selection import train_test_split\n\ntrain_texts, val_texts, train_labels, val_labels = train_test_split(train_texts, train_labels, test_size=.2)","4fc3bde5":"from transformers import DistilBertTokenizerFast\n\ntokenizer = DistilBertTokenizerFast.from_pretrained('distilbert-base-uncased')","92405f76":"type(train_texts), type(list(train_texts))","5687be6b":"train_encodings = tokenizer(list(train_texts), truncation=True, padding=True)\ntest_encodings = tokenizer(list(test_texts), truncation=True, padding=True)\nval_encodings = tokenizer(list(val_texts), truncation=True, padding=True)","8e5f79a5":"import torch\n\nclass IMDBdataset(torch.utils.data.Dataset):\n    def __init__(self, encodings, labels):\n        self.encodings = encodings\n        self.labels = labels\n    \n    def __len__(self):\n        return len(self.labels)\n    \n    def __getitem__(self, idx):\n        item = {key: torch.tensor(val[idx]) for key, val in self.encodings.items()}\n        item['labels'] = torch.tensor(self.labels[idx])\n        return item","02c7e960":"train_dataset = IMDBdataset(train_encodings, train_labels)\ntest_dataset = IMDBdataset(test_encodings,test_labels)\nval_dataset = IMDBdataset(val_encodings, val_labels)\n","add7bc92":"# idx = 0\n# # print(train_encodings.items())\n# item = {key: torch.tensor(val[idx]) for key, val in train_encodings.items()}\n# print(item)","7401fb01":"from torch.utils.data import DataLoader\nfrom transformers import DistilBertForSequenceClassification, AdamW\n\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","2d8175cb":"model = DistilBertForSequenceClassification.from_pretrained('distilbert-base-uncased')\nmodel = model.to(device=device)","c067110b":"model.train()","0fd0d68b":"train_dataloader = DataLoader(dataset=train_dataset, batch_size=16, shuffle=True)","0a12d0f1":"optim = AdamW(model.parameters(),lr=5e-5)","fcbeefb4":"from tqdm import tqdm\nfor epoch in range(3):\n    for batch in tqdm(train_dataloader):\n        optim.zero_grad()\n        input_ids= batch['input_ids'].to(device)\n        attention_mask = batch['attention_mask'].to(device)\n        labels = batch['labels'].to(device)\n        outputs = model(input_ids, attention_mask=attention_mask, labels=labels)\n        loss = outputs[0]\n        loss.backward()\n        optim.step()\n    print(f\"Loss for epoch {epoch} is {loss}\")\n\nmodel.eval()","08318d58":"save_directory = \".\/\"","a6590418":"tokenizer.save_pretrained(save_directory)\nmodel.save_pretrained(save_directory)","669a8769":"# from transformers import TFAutoModel, AutoTokenizer\n# tokenizer = AutoTokenizer.from_pretrained(save_directory)\n# model = TFAutoModel.from_pretrained(save_directory, from_pt=True)\n","cb23596e":"from transformers import pipeline\nclassifier =pipeline('sentiment-analysis',model=model, tokenizer=tokenizer)","4caa570f":"sample_test= test_dataset[0]\nsample_test","a0d6c48d":"classifier(\"I love it\")[0]","b4af3855":"test_texts[1001], test_labels[1001]","77c54656":"classifier(test_texts[1001])[0]","8901cc3c":"### 4. Fine Tuning with native pytorch\n\nThe steps above prepared the datasets in the way that the trainer is expected. Now all we need to do is create a model to fine-tune, define the TrainingArguments\/TFTrainingArguments and instantiate a Trainer\/TFTrainer.","e0814604":"and loading it back","a33d100b":"Now we can simply pass our texts to the tokenizer. We\u2019ll pass `truncation=True` and `padding=True`, which will ensure that all of our sequences are padded to the same length and are truncated to be no longer model\u2019s maximum input length. This will allow us to feed batches of sequences into the model at the same time.","f5a970e5":"\"In this example, we\u2019ll show how to download, tokenize, and train a model on the IMDb reviews dataset. This task takes the text of a review and requires the model to predict whether the sentiment of the review is positive or negative. Let\u2019s start by downloading the dataset from the Large Movie Review Dataset webpage.\"\n\n    - Hugging face tutorial (https:\/\/huggingface.co\/transformers\/master\/custom_datasets.html)\n\n![](data:image\/png;base64,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)","31572767":"once trained save it","11904596":"Now that our datasets our ready, we can fine-tune a model either with the \ud83e\udd17 Trainer\/TFTrainer or with native PyTorch\/TensorFlow.","5c31921c":"### 1. Getting the data","6505c0a3":"### 5. Building the classifier and using sequence selection\n\nNow lets do something that is not covered in the official tutorial","a0a6b9b8":"This data is organized into pos and neg folders with one text file per example. Let\u2019s write a function that can read this in.","92c7fc08":"We now have a train and test dataset, but let\u2019s also also create a validation set which we can use for for evaluation and tuning without tainting our test set results. Sklearn has a convenient utility for creating such splits:","00ba44ec":"This is the all important training loop. It is giving me meory limit exceeded error! if it happens to you, comment the training loop.","6ee412fb":"### 3. Creating a dataset object\n\nNow, let\u2019s turn our labels and encodings into a Dataset object. In PyTorch, this is done by subclassing a torch.utils.data.Dataset object and implementing __len__ and __getitem__. In TensorFlow, we pass our input encodings and labels to the from_tensor_slices constructor method. We put the data in this format so that the data can be easily batched such that each key in the batch encoding corresponds to a named parameter of the forward() method of the model we will train.","f815c8b5":"### 2. Tokenizer\n\nAlright, we\u2019ve read in our dataset. Now let\u2019s tackle tokenization. We\u2019ll eventually train a classifier using pre-trained DistilBert, so let\u2019s use the DistilBert tokenizer."}}