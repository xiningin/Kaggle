{"cell_type":{"36cff9c0":"code","3009c50f":"code","4565179b":"code","2c5638b9":"code","41253398":"code","1dfcb860":"code","f759bbdc":"code","313b1676":"code","61a93025":"code","b0e74b26":"code","8871158a":"code","6180bfa4":"code","c18517ad":"code","d4fd6dfc":"code","8e602d6b":"code","72c4038a":"code","2c911519":"code","95a71a68":"code","c68a045b":"code","ed7c5133":"code","271087ae":"code","a4be33cd":"code","7440ea39":"code","c4095546":"code","2a9fa920":"code","a0058f28":"code","47488f30":"code","fabec233":"code","0ae8193e":"code","8a511b58":"code","f34dbe37":"code","8476ad77":"code","5483c99a":"code","70a5c15b":"code","37b23334":"code","53aaee77":"code","e32c375b":"code","e0518747":"code","a3519187":"code","7c9a9800":"code","28c03ad3":"code","5aadce6f":"code","aca44f1f":"code","48c04180":"code","cd8adeef":"code","dd2189ee":"code","5c558f7f":"code","a957579d":"code","d35b29bb":"code","c2aedc41":"code","58b32965":"code","0883e06d":"code","9f7d1058":"code","40662b5a":"code","08517bd9":"code","2b0c439c":"code","7be02c98":"code","41bcae37":"code","d3bac540":"code","ff29cf43":"code","47a8b192":"code","2ef068e0":"markdown","7a955cb8":"markdown","3846be64":"markdown","9c034826":"markdown","89b64f30":"markdown","16537c94":"markdown","07d6baa4":"markdown","2f78b8cb":"markdown","d75e128d":"markdown","c73ce1d0":"markdown","6f13b6ef":"markdown","e63c1017":"markdown","2fcdea21":"markdown","d8e69559":"markdown","2f20179f":"markdown","aef5441e":"markdown","596f78e1":"markdown","059431c5":"markdown","841d7706":"markdown","191833f3":"markdown","13b83992":"markdown","362b60df":"markdown","07f99a84":"markdown","610a490b":"markdown","25885467":"markdown","60404444":"markdown","ebfb4327":"markdown","4958c669":"markdown","56da8aef":"markdown","3f11a66f":"markdown","6c37f4a0":"markdown","8bdb5e51":"markdown","a3ab99f7":"markdown","25ae007f":"markdown","e39d8c20":"markdown","bf644aaf":"markdown","eaba6c05":"markdown","5ea1b273":"markdown","ca5c9a5e":"markdown","c1bf9f1d":"markdown","60cb0950":"markdown","54439d5c":"markdown","4559f003":"markdown"},"source":{"36cff9c0":"import numpy as np\nimport pandas as pd\nimport missingno\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import style\nfrom matplotlib.pyplot import plot\n\nstyle.use(\"seaborn-whitegrid\")\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'","3009c50f":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","4565179b":"data_size_train = train.memory_usage().sum() \/ 1024 \/ 1024\nprint(\"Data memory size: %.2f MB\" % data_size_train)","2c5638b9":"data_size_test = test.memory_usage().sum() \/ 1024 \/ 1024\nprint(\"Data memory size: %.2f MB\" % data_size_test)","41253398":"train.info()","1dfcb860":"test.info()","f759bbdc":"print(train.shape)\ntrain.head(3)","313b1676":"print(test.shape)\ntest.head(3)","61a93025":"missingno.matrix(train, figsize = (10,5))\nplt.title(\"Missing Values (train)\")\nplt.show()","b0e74b26":"missingno.matrix(test, figsize = (10,5))\nplt.title(\"Missing Values (test)\")\nplt.show()","8871158a":"train[\"Embarked\"].fillna(train[\"Embarked\"].mode()[0], inplace = True)\ntrain[\"Age\"].fillna(train[\"Age\"].mean(), inplace = True)\ntrain = train.drop([\"Cabin\", \"Ticket\"], axis = 1)\nprint(train.isnull().sum().sort_values(ascending = False).head(3))\n\nprint(\"-------------\")\n\ntest[\"Fare\"].fillna(test[\"Fare\"].mean(), inplace = True)\ntest[\"Age\"].fillna(test[\"Age\"].mean(), inplace = True)\ntest = test.drop([\"Cabin\", \"Ticket\"], axis = 1)\nprint(test.isnull().sum().sort_values(ascending = False).head(3))","6180bfa4":"train[\"NameTitle\"] = train[\"Name\"].str.split(\", \", expand = True)[1].str.split(\". \", expand = True)[0]\ntest[\"NameTitle\"] = test[\"Name\"].str.split(\", \", expand = True)[1].str.split(\". \", expand = True)[0]\n\ntrain = train.drop(\"Name\", axis = 1)\ntest = test.drop(\"Name\", axis = 1)\n\nprint(train[\"NameTitle\"].unique())\nprint(\"------------------------\" * 3)\nprint(test[\"NameTitle\"].unique())","c18517ad":"min_titles = (train[\"NameTitle\"].value_counts() < 10)\ntrain[\"NameTitle\"] = train[\"NameTitle\"].apply(lambda x: \"Misc\" if min_titles.loc[x] == True else x)\nprint(train[\"NameTitle\"].unique())","d4fd6dfc":"min_titles = (test[\"NameTitle\"].value_counts() < 10)\ntest[\"NameTitle\"] = test[\"NameTitle\"].apply(lambda x: \"Misc\" if min_titles.loc[x] == True else x)\nprint(test[\"NameTitle\"].unique())","8e602d6b":"train[\"FamilySize\"] = train[\"SibSp\"] + train[\"Parch\"]\ntest[\"FamilySize\"] = test[\"SibSp\"] + test[\"Parch\"]","72c4038a":"train[\"AgeGroup\"] = \"\"\ntrain.loc[train[\"Age\"] < 21, \"AgeGroup\"] = \"under 21\"\ntrain.loc[train[\"Age\"] >= 21, \"AgeGroup\"] = \"21-65\"\ntrain.loc[train[\"Age\"] > 65, \"AgeGroup\"] = \"65+\"\n\ntest[\"AgeGroup\"] = \"\"\ntest.loc[test[\"Age\"] < 21, \"AgeGroup\"] = \"under 21\"\ntest.loc[test[\"Age\"] >= 21, \"AgeGroup\"] = \"21-65\"\ntest.loc[test[\"Age\"] > 65, \"AgeGroup\"] = \"65+\"\n\ntrain = train.drop(\"Age\", axis = 1)\ntest = test.drop(\"Age\", axis = 1)","2c911519":"train[\"FareGroup\"] = \"\"\ntrain.loc[train[\"Fare\"] < 170, \"FareGroup\"] = \"0-170\"\ntrain.loc[train[\"Fare\"] >= 170, \"FareGroup\"] = \"170-340\"\ntrain.loc[train[\"Fare\"] > 340, \"FareGroup\"] = \"340+\"\n\ntest[\"FareGroup\"] = \"\"\ntest.loc[test[\"Fare\"] < 170, \"FareGroup\"] = \"0-170\"\ntest.loc[test[\"Fare\"] >= 170, \"FareGroup\"] = \"170-340\"\ntest.loc[test[\"Fare\"] > 340, \"FareGroup\"] = \"340+\"\n\ntrain = train.drop(\"Fare\", axis = 1)\ntest = test.drop(\"Fare\", axis = 1)","95a71a68":"print(train.shape)\ntrain.head(3)","c68a045b":"print(test.shape)\ntest.head(3)","ed7c5133":"plt.figure(figsize = (14,2))\nsns.countplot(data = train, y = \"Survived\", palette = \"viridis\")\nplt.title(\"Survived\")\nplt.show()","271087ae":"plt.figure(figsize = (14,3))\nsns.countplot(data = train, y = \"Pclass\", hue = \"Survived\", palette = \"viridis\")\nplt.title(\"Class\")\nplt.show()","a4be33cd":"plt.figure(figsize = (14,2))\nsns.countplot(data = train, y = \"Sex\", hue = \"Survived\", palette = \"viridis\")\nplt.title(\"Gender\")\nplt.show()","7440ea39":"plt.figure(figsize = (14,10))\nsns.countplot(data = train, y = \"FamilySize\", hue = \"Survived\", palette = \"viridis\")\nplt.title(\"Family Size\")\nplt.show()","c4095546":"plt.figure(figsize = (14,3))\nsns.countplot(data = train, y = \"Embarked\", hue = \"Survived\", palette = \"viridis\")\nplt.title(\"Embarked\")\nplt.show()","2a9fa920":"plt.figure(figsize = (14,5))\nsns.countplot(data = train, y = \"NameTitle\", hue = \"Survived\", palette = \"viridis\")\nplt.title(\"Name Title\")\nplt.show()","a0058f28":"plt.figure(figsize = (14,5))\nsns.countplot(data = train, y = \"AgeGroup\", hue = \"Survived\", palette = \"viridis\")\nplt.title(\"Age Group\")\nplt.show()","47488f30":"plt.figure(figsize = (14,5))\nsns.countplot(data = train, y = \"FareGroup\", hue = \"Survived\", palette = \"viridis\")\nplt.title(\"Fare Group\")\nplt.show()","fabec233":"corr = train.corr()\nplt.figure(figsize = (8,6))\nsns.heatmap(corr, cmap = \"coolwarm\", linewidth = 4, linecolor = \"white\")\nplt.title(\"Correlation\")\nplt.show()","0ae8193e":"train = train.drop([\"SibSp\", \"Parch\"], axis = 1)\ntest = test.drop([\"SibSp\", \"Parch\"], axis = 1)","8a511b58":"train = pd.get_dummies(train, columns = [\"Sex\", \"Embarked\", \"NameTitle\", \"AgeGroup\", \"FareGroup\"])\ntest = pd.get_dummies(test, columns = [\"Sex\", \"Embarked\", \"NameTitle\", \"AgeGroup\", \"FareGroup\"])","f34dbe37":"corr = train.corr()\nplt.figure(figsize = (8,6))\nsns.heatmap(corr, cmap = \"coolwarm\", linewidth = 4, linecolor = \"white\")\nplt.title(\"Correlation\")\nplt.show()","8476ad77":"upper = corr.where(np.triu(np.ones(corr.shape), k = 1).astype(np.bool))\nto_drop = [column for column in upper.columns if any(upper[column] > 0.90)]\nprint(to_drop)","5483c99a":"from sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\n\nX = train.drop([\"PassengerId\", \"Survived\"], axis = 1)\nY = train[\"Survived\"]","70a5c15b":"scaler = StandardScaler()\nX_pca = scaler.fit_transform(X)\n\npca = PCA(n_components = 2)\nX_pca_transformed = pca.fit_transform(X_pca)\n\nplt.figure(figsize = (12,6))\n\nfor i in Y.unique():\n    X_pca_filtered = X_pca_transformed[Y == i, :]\n    plt.scatter(X_pca_filtered[:, 0], X_pca_filtered[:, 1], s = 20, label = i)\n    \nplt.legend()\nplt.title(\"PCA\", fontsize = 14)\nplt.show()","37b23334":"from sklearn.linear_model import LogisticRegression \nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier, BaggingClassifier, ExtraTreesClassifier, StackingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\n\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nimport lightgbm as lgb","53aaee77":"import time\n    \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import style\nfrom matplotlib.pyplot import plot\n\nstyle.use(\"seaborn-whitegrid\")\n%matplotlib inline\n%config InlineBackend.figure_format = \"retina\"\n    \nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold, learning_curve\nfrom sklearn.pipeline import Pipeline\n\ndef model_builder(X, Y, pipeline, params, kfolds, classifier_name, data_split = False):\n    \n    \"\"\"\n    Trains the selected classifier using GridSearchCV with StratifiedKFold and the given pipeline and parameters. \n    Scores are calculated using cross_val_score with StratifiedKFold.\n    \n    Returns the results of cross_val_score, the best estimator of GridSearchCV and \n    X_train, X_test, Y_train, Y_test if data_split set to True.\n    \n    Parameters:\n    -----------\n    X : input variables\n    \n    Y : output variables\n    \n    pipeline : scikit learn Pipeline object\n    \n    params : dictionary of hyperparamters for GridSearchCV\n    \n    kfolds : number of folds for StratifiedKFold\n    \n    classifier_name : name of algorithm as a string\n    \n    data_split : if true: will train_test_split the data\n                 if false: will proceed with given data\n                 \n    Example:\n    --------\n    With data_split = False:\n    results, model = model_builder(X, \n                                   Y, \n                                   pipeline, \n                                   params, \n                                   10, \n                                   \"Logistic Regression\")\n    \n    With data_split = True:\n    X_train, X_test, Y_train, Y_test, results, model = model_builder(X, \n                                                                     Y, \n                                                                     pipeline, \n                                                                     params, \n                                                                     10, \n                                                                     \"Logistic Regression\", \n                                                                      data_split = True)\n    \"\"\"\n    \n    if data_split == True:\n        \n        X_train, X_test, Y_train, Y_test = train_test_split(X, \n                                                            Y, \n                                                            random_state = 0, \n                                                            test_size = 0.25)\n        cv = StratifiedKFold(n_splits = kfolds)\n\n        start = time.time()\n\n        grid = GridSearchCV(pipeline, params, cv = cv, verbose = 0, n_jobs = -1)\n        grid = grid.fit(X_train, Y_train)\n\n        print(\"Best Params:\")\n        print(\"\")\n        print(grid.best_params_)\n\n        grid = grid.best_estimator_\n\n        classifier_score = cross_val_score(grid, X_test, Y_test, cv = cv, n_jobs = -1)\n        Y_predicted = grid.predict(X_test)\n\n        stop = time.time()\n    \n        print(\"\")\n        print(\"#\" * 11 + \" ACCURACY \" + \"#\" * 11)\n        print(\"Cross-Validated-Score: \" + str(round(classifier_score.mean(), 6)))\n        print(\"#\" * 32)\n\n        print(\"\")\n        print(\"Training time: \" + str(round(stop - start, 2)) + \"s\")\n\n        train_size, train_score, test_score = learning_curve(grid, X_train, Y_train, cv = cv, n_jobs = -1)\n\n        print(\"\")\n        print(\"Learning Curve:\")\n\n        plt.figure(figsize = (12,6))\n        plt.plot(train_size, np.mean(train_score, axis = 1), label = \"Train scores\")\n        plt.plot(train_size, np.mean(test_score, axis = 1), label = \"Test scores\")\n        plt.title(classifier_name)\n        plt.legend()\n        plt.show()\n\n        return X_train, X_test, Y_train, Y_test, classifier_score, grid\n    \n    if data_split == False:\n        \n        cv = StratifiedKFold(n_splits = kfolds)\n\n        start = time.time()\n\n        grid = GridSearchCV(pipeline, params, cv = cv, verbose = 0, n_jobs = -1)\n        grid = grid.fit(X, Y)\n\n        print(\"Best Params:\")\n        print(\"\")\n        print(grid.best_params_)\n\n        grid = grid.best_estimator_\n\n        classifier_score = cross_val_score(grid, X, Y, cv = cv, n_jobs = -1)\n        Y_predicted = grid.predict(X)\n\n        stop = time.time()\n    \n        print(\"\")\n        print(\"#\" * 11 + \" ACCURACY \" + \"#\" * 11)\n        print(\"Cross-Validated-Score: \" + str(round(classifier_score.mean(), 6)))\n        print(\"#\" * 32)\n\n        print(\"\")\n        print(\"Training time: \" + str(round(stop - start, 2)) + \"s\")\n\n        train_size, train_score, test_score = learning_curve(grid, X, Y, cv = cv, n_jobs = -1)\n\n        print(\"\")\n        print(\"Learning Curve:\")\n\n        plt.figure(figsize = (12,6))\n        plt.plot(train_size, np.mean(train_score, axis = 1), label = \"Train scores\")\n        plt.plot(train_size, np.mean(test_score, axis = 1), label = \"Test scores\")\n        plt.title(classifier_name)\n        plt.legend()\n        plt.show()\n\n        return classifier_score, grid","e32c375b":"pipe_log = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"pca\", PCA()),\n    (\"log\", LogisticRegression())])\n\nparams_log = {\n    \"pca__n_components\" : [4, 8, 16, 18],\n    \"log__C\" : [0.001, 0.01, 0.1, 1, 1.1, 10],\n    \"log__max_iter\" : [10000],\n    \"log__solver\" : [\"newton-cg\", \"lbfgs\", \"liblinear\", \"sag\", \"saga\"]}\n\nlog_score, grid_log = model_builder(X, Y, pipe_log, params_log, 10, \"Logistic Regression\")\nY_predicted_log = grid_log.predict_proba(X)[:, 1]","e0518747":"pipe_xgb = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"pca\", PCA()),\n    (\"xgb\", XGBClassifier())])\n\nparams_xgb = {\n    \"pca__n_components\" : [2, 6, 10, 18],\n    \"xgb__n_estimators\" : [300, 500, 700],\n    \"xgb__learning_rate\" : [0.005, 0.1],\n    \"xgb__max_depth\" : [5, 7],\n    \"xgb__max_features\" : [3, 5], \n    \"xgb__gamma\" : [0.5, 0.6, 0.7]}\n\nxgb_score, grid_xgb = model_builder(X, Y, pipe_xgb, params_xgb, 10, \"XGBoost\")\nY_predicted_xgb = grid_xgb.predict_proba(X)[:, 1]","a3519187":"pipe_boost = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"pca\", PCA()),\n    (\"boost\", GradientBoostingClassifier())])\n\nparams_boost = {\n    \"pca__n_components\" : [2, 6, 10, 18],\n    \"boost__n_estimators\" : [300, 500, 700],\n    \"boost__learning_rate\" : [0.005, 0.1],\n    \"boost__max_depth\" : [3, 5],\n    \"boost__max_features\" : [3, 5]}\n\nboost_score, grid_boost = model_builder(X, Y, pipe_boost, params_boost, 10, \"Gradient Boosting\")\nY_predicted_boost = grid_boost.predict_proba(X)[:, 1]","7c9a9800":"pipe_rf = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"pca\", PCA()),\n    (\"rf\", RandomForestClassifier(criterion = \"gini\", \n                                  max_features = \"auto\"))])\n\nparams_rf = {\n    \"pca__n_components\" : [2, 6, 10, 18],\n    \"rf__n_estimators\" : [200, 250, 300, 400],\n    \"rf__max_depth\" : [1, 3, 5, 7, 9]}\n\nrf_score, grid_rf = model_builder(X, Y, pipe_rf, params_rf, 10, \"Random Forest\")\nY_predicted_rf = grid_rf.predict_proba(X)[:, 1]","28c03ad3":"pipe_svm = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"pca\", PCA()),\n    (\"svm\", SVC(probability = True, kernel = \"rbf\"))])\n\nparams_svm = {\n    \"pca__n_components\" : [2, 6, 10, 18],\n    \"svm__C\" : [0.01, 0.1, 1, 1.1, 2],\n    \"svm__gamma\" : [0.01, 0.1, 1]}\n\nsvm_score, grid_svm = model_builder(X, Y, pipe_svm, params_svm, 10, \"Support Vector Machines\")\nY_predicted_svm = grid_svm.predict_proba(X)[:, 1]","5aadce6f":"pipe_bag = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"pca\", PCA()),\n    (\"bag\", BaggingClassifier())])\n\nparams_bag = {\n    \"pca__n_components\" : [2, 6, 10, 18],\n    \"bag__n_estimators\" : [30, 50, 70, 100],\n    \"bag__max_features\" : [3, 5, 7, 9],\n    \"bag__max_samples\" : [3, 5, 7, 9]}\n\nbag_score, grid_bag = model_builder(X, Y, pipe_bag, params_bag, 10, \"Bagging Classifier\")\nY_predicted_bag = grid_bag.predict_proba(X)[:, 1]","aca44f1f":"pipe_xt = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"pca\", PCA()),\n    (\"xt\", ExtraTreesClassifier(criterion = \"gini\", \n                                max_features = \"auto\"))])\n\nparams_xt = {\n    \"pca__n_components\" : [2, 6, 10, 18],\n    \"xt__n_estimators\" : [300, 500, 700],\n    \"xt__max_depth\" : [5, 7, 9]}\n\nxt_score, grid_xt = model_builder(X, Y, pipe_xt, params_xt, 10, \"Extra Trees\")\nY_predicted_xt = grid_xt.predict_proba(X)[:, 1]","48c04180":"pipe_knn = Pipeline([\n    (\"scaler\", StandardScaler()),\n    #(\"pca\", PCA()),\n    (\"knn\", KNeighborsClassifier(algorithm = \"auto\"))])\n\nparams_knn = {\n    #\"pca__n_components\" : [2, 6, 10, 18],\n    \"knn__n_neighbors\" : [2, 3, 5, 7, 9],\n    \"knn__leaf_size\" : [10, 20, 30, 40]}\n\nknn_score, grid_knn = model_builder(X, Y, pipe_knn, params_knn, 10, \"KNN\")\nY_predicted_knn = grid_knn.predict_proba(X)[:, 1]","cd8adeef":"pipe_cat = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"cat\", CatBoostClassifier(verbose = 0))])\n\nparams_cat = {\n    \"cat__depth\" : [2, 3, 4, 5],\n    \"cat__learning_rate\" : [0.001, 0.01, 0.1, 1, 1.1],\n    \"cat__n_estimators\" : [300, 400, 500, 600]}\n\ncat_score, grid_cat = model_builder(X, Y, pipe_cat, params_cat, 10, \"CatBoost\")\nY_predicted_cat = grid_cat.predict_proba(X)[:, 1]","dd2189ee":"pipe_gbm = Pipeline([\n    (\"scaler\", StandardScaler()),\n    (\"lgb\", lgb.LGBMClassifier())])\n\nparams_gbm = {\n        \"lgb__num_leaves\": [5, 7, 40, 60, 100],\n        \"lgb__n_estimators\": [300, 700, 1000],\n        \"lgb__learning_rate\" : [0.0001, 0.001, 0.01, 0.1, 1]}\n\ngbm_score, grid_gbm = model_builder(X, Y, pipe_gbm, params_gbm, 10, \"LightGBM\")\nY_predicted_gbm = grid_gbm.predict_proba(X)[:, 1]","5c558f7f":"all_results = [xgb_score, boost_score, log_score, rf_score, \n               svm_score, bag_score, xt_score, knn_score, cat_score, gbm_score]\n\nresult_names = [\"XGBoost\", \n                \"Gradient Boosting\", \n                \"Logistic Regression\", \n                \"Random Forest\", \n                \"SVM\", \n                \"Bagging Classifier\", \n                \"Extra Trees Classifier\", \n                \"KNN\", \n                \"CatBoost\", \n                \"LightGBM\"]\n\nfig = plt.figure(figsize = (14,7))\nfig.suptitle(\"Algorithm Comparison\")\nax = fig.add_subplot(111)\nplt.boxplot(all_results)\nax.set_xticklabels(result_names)\nplt.show()","a957579d":"%%capture\n!pip install keras-tuner","d35b29bb":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom kerastuner.tuners import RandomSearch","c2aedc41":"X_train, X_test, Y_train, Y_test = train_test_split(X, \n                                                    Y, \n                                                    random_state = 0, \n                                                    test_size = 0.25)","58b32965":"%%capture\n\ndef tune_model(hp):\n\n    nn = Sequential()\n\n    nn.add(Dense(hp.Choice(\"units_1\", [512, 1024, 2048]), activation = \"relu\", input_shape = (18,)))\n    nn.add(Dropout(hp.Choice(\"dropout_1\", [0.0, 0.15, 0.2, 0.25])))\n    nn.add(Dense(hp.Choice(\"units_2\", [128, 256, 512]), activation = \"relu\"))\n    nn.add(Dropout(hp.Choice(\"dropout_2\", [0.0, 0.15, 0.2, 0.25])))\n    nn.add(Dense(hp.Choice(\"units_3\", [32, 64, 128]), activation = \"relu\"))\n    nn.add(Dropout(hp.Choice(\"dropout_3\", [0.0, 0.15, 0.2, 0.25])))\n    nn.add(Dense(hp.Choice(\"units_4\", [8, 16, 32]), activation = \"relu\"))\n    nn.add(Dropout(hp.Choice(\"dropout_4\", [0.0, 0.15, 0.2, 0.25])))\n    nn.add(Dense(1, activation = \"relu\"))\n    \n    lr = hp.Choice(\"learning_rate\", [0.0001, 0.001, 0.01, 0.1])\n    mm = hp.Choice(\"momentum\", [0.0, 0.2, 0.4, 0.6, 0.8])\n    \n    nn.compile(optimizer = keras.optimizers.RMSprop(learning_rate = lr, momentum = mm), \n               loss = \"binary_crossentropy\", \n               metrics = [\"accuracy\"])\n    \n    return nn\n\ntuner = RandomSearch(tune_model, \n                     objective = \"val_accuracy\", \n                     max_trials = 3)\n\ntuner.search(x = X_train, \n             y = Y_train, \n             verbose = 3, \n             epochs = 400, \n             batch_size = 25, \n             validation_data = (X_test, Y_test))","0883e06d":"print(\"Best Params: \" + str(tuner.oracle.get_best_trials(num_trials = 1)[0].hyperparameters.values))\n\nnn = tuner.get_best_models()[0]\n\nprint(\"\")\nprint(\"Test score: \" + str(nn.evaluate(X_test, Y_test)))\nprint(\"\")\nprint(\"Train score: \" + str(nn.evaluate(X_train, Y_train)))","9f7d1058":"Y_predicted_nn = nn.predict(test.drop([\"PassengerId\"], axis = 1))\nY_predicted_nn = (Y_predicted_nn.ravel() > 0.5).astype(int)","40662b5a":"from sklearn.metrics import roc_curve, roc_auc_score","08517bd9":"log_fpr, log_tpr, log_treshholds = roc_curve(Y, Y_predicted_log)\nboost_fpr, boost_tpr, boost_treshholds = roc_curve(Y, Y_predicted_boost) \nsvm_fpr, svm_tpr, svm_treshholds = roc_curve(Y, Y_predicted_svm)\nrf_fpr, rf_tpr, rf_treshholds = roc_curve(Y, Y_predicted_rf)\nxgb_fpr, xgb_tpr, xgb_treshholds = roc_curve(Y, Y_predicted_xgb)\nbag_fpr, bag_tpr, bag_treshholds = roc_curve(Y, Y_predicted_bag)\nxt_fpr, xt_tpr, xt_treshholds = roc_curve(Y, Y_predicted_xt)\nknn_fpr, knn_tpr, knn_treshholds = roc_curve(Y, Y_predicted_knn)\n\nauc_score_log = roc_auc_score(Y, Y_predicted_log)\nauc_score_boost = roc_auc_score(Y, Y_predicted_boost)\nauc_score_svm = roc_auc_score(Y, Y_predicted_svm)\nauc_score_rf = roc_auc_score(Y, Y_predicted_rf)\nauc_score_xgb = roc_auc_score(Y, Y_predicted_xgb)\nauc_score_bag = roc_auc_score(Y, Y_predicted_bag)\nauc_score_xt = roc_auc_score(Y, Y_predicted_xt)\nauc_score_knn = roc_auc_score(Y, Y_predicted_knn)\n\nplt.figure(figsize = (14,7))\nplt.plot([0,1], [0,1])\nplt.plot(log_fpr, log_tpr, label = \"Logistic Regression (AUC-Score: \" + str(round(auc_score_log, 2)) + \")\")\nplt.plot(boost_fpr, boost_tpr, label = \"Gradient Boosting (AUC-Score: \" + str(round(auc_score_boost, 2)) + \")\")\nplt.plot(svm_fpr, svm_tpr, label = \"SVM (AUC-Score: \" + str(round(auc_score_svm, 2)) + \")\")\nplt.plot(rf_fpr, rf_tpr, label = \"Random Forest (AUC-Score: \" + str(round(auc_score_rf, 2)) + \")\")\nplt.plot(xgb_fpr, xgb_tpr, label = \"XGBoost (AUC-Score: \" + str(round(auc_score_xgb, 2)) + \")\")\nplt.plot(bag_fpr, bag_tpr, label = \"Bagging Classifier (AUC-Score: \" + str(round(auc_score_bag, 2)) + \")\")\nplt.plot(xt_fpr, xt_tpr, label = \"Extra Trees Clasifier (AUC-Score: \" + str(round(auc_score_xt, 2)) + \")\")\nplt.plot(knn_fpr, knn_tpr, label = \"K-Nearest-Neighbor (AUC-Score: \" + str(round(auc_score_knn, 2)) + \")\")\nplt.title(\"ROC-Curve\")\nplt.xlabel(\"FPR\")\nplt.ylabel(\"TPR\")\nplt.legend()\nplt.show()","2b0c439c":"stack = StackingClassifier(estimators = [(\"XGBoost\", grid_xgb), \n                                         (\"GradientBoosting\", grid_boost),\n                                         (\"RandomForest\", grid_rf), \n                                         (\"Logistic Regression\", grid_log), \n                                         (\"SVM\", grid_svm),  \n                                         (\"Extra Trees Classifier\", grid_xt), \n                                         (\"KNeighborsClassifier\", grid_knn), \n                                         (\"CatBoost\", grid_cat), \n                                         (\"LightGBM\", grid_gbm)], n_jobs = -1)\n\n\ncv = StratifiedKFold(n_splits = 5)\n\nstack = stack.fit(X, Y)\nstack_score = cross_val_score(stack, X, Y, cv = cv, n_jobs = -1)\n\nprint(\"#\" * 11 + \" FINAL ACCURACY \" + \"#\" * 11)\nprint(\"Cross-Validated-Score: \" + str(round(stack_score.mean(), 6)))\nprint(\"#\" * 38)\n\nY_predicted_stack = stack.predict(test.drop([\"PassengerId\"], axis = 1))","7be02c98":"train_size, train_score, test_score = learning_curve(stack, X, Y, cv = cv, n_jobs = -1)\n\nplt.figure(figsize = (12,6))\nplt.plot(train_size, np.mean(train_score, axis = 1), label = \"Train\")\nplt.plot(train_size, np.mean(test_score, axis = 1), label = \"Test\")\nplt.title(\"Stacked Model\")\nplt.legend()\nplt.show()","41bcae37":"submission = pd.DataFrame({\"PassengerId\" : test.PassengerId, \n                           \"Survived\" : Y_predicted_stack})\n\nsubmission.to_csv(\"submission_clf.csv\", index = False)","d3bac540":"submission.head()","ff29cf43":"submission_nn = pd.DataFrame({\"PassengerId\" : test.PassengerId, \n                           \"Survived\" : Y_predicted_nn})\n\nsubmission_nn.to_csv(\"submission_nn.csv\", index = False)","47a8b192":"submission_nn.head(5)","2ef068e0":"# 1. Import modules","7a955cb8":"# Introduction\n\n### Welcome to my first ever Machine Learning project\/Kaggle Competition!\n\nIn this notebook we'll do some easy data visualizations and feature engineering before applying different machine learning algorithms like Logistic Regression, Random Forest, Support Vector Machines and more. We're also using a Neural Network using Keras and Tensorflow with a Keras-Tuner for hyperparameter optimization. For better understanding of the dataset and algorithms we'll use a PCA and ROC-Curves.\n\n#### If you have any suggestions on how to improve this notebook please let me know!","3846be64":"# 3. Data exploration","9c034826":"## 7.12 Model Comparison","89b64f30":"# History\nRMS Titanic was a British passenger liner operated by the White Star Line that sank in the North Atlantic Ocean in the early morning hours of 15 April 1912, after striking an iceberg during her maiden voyage from Southampton to New York City. Of the estimated 2,224 passengers and crew aboard, more than 1,500 died, making the sinking one of modern history's deadliest peacetime commercial marine disasters. RMS Titanic was the largest ship afloat at the time she entered service and was the second of three Olympic-class ocean liners operated by the White Star Line. She was built by the Harland and Wolff shipyard in Belfast.","16537c94":"## 5.4 Family Size","07d6baa4":"## 5.2 Class","2f78b8cb":"## 4.4 Fare Group","d75e128d":"## 7.8 Extra Trees Classifier","c73ce1d0":"## 7.7 Bagging Classifier","6f13b6ef":"## 10.1 Stacked Model Learning Curve","e63c1017":"## 7.3 XGBoost","2fcdea21":"# 8. Neural Network","d8e69559":"## 7.11 LightGBM","2f20179f":"## 5.1 Survived","aef5441e":"## 7.5 Random Forest","596f78e1":"## 7.10 CatBoost","059431c5":"### Version History\n* V22 - Added Keras Tune\n* V23 - Added Boxplots\n* V24 - Added Pipelines for Classifiers\n* V25 - Added Learning Curves\n* V26 - New Model Stacking\n* V27 - Added Model Building Function\n* V32 - Updated Model Building Function\n* V34 - Added History\n* V36 - Added CatBoost and LightGBM algorithms","841d7706":"## 5.5 Embarked","191833f3":"## 5.7 Age Group","13b83992":"# 11. Submission","362b60df":"# 5. Data visualization","07f99a84":"## 4.1 Name Title","610a490b":"# 2. Import data","25885467":"## 5.9 Correlation","60404444":"## 11.1 Classifiers","ebfb4327":"## 4.2 Family Size","4958c669":"## 7.9 K-Nearest-Neighbor","56da8aef":"## 7.6 SVM","3f11a66f":"## 7.1 Model Building Function","6c37f4a0":"## 4.3 Age Group","8bdb5e51":"# 6. PCA","a3ab99f7":"## 7.4 Gradient Boosting","25ae007f":"# 9. ROC-Curve and AUC-Score","e39d8c20":"# 10. Model Stacking","bf644aaf":"## 5.8 Fare Group","eaba6c05":"## 7.2 Logistic Regression","5ea1b273":"# 7. Machine Learning","ca5c9a5e":"# 4. Feature engineering","c1bf9f1d":"# Goal\nIt is our job to predict if a passenger survived the sinking of the Titanic or not. For each person in the test set, we must predict a 0 or 1 value for the variable. Our score is the percentage of passengers we correctly predict. ","60cb0950":"## 5.6 Name Title","54439d5c":"## 11.2 Neural Network","4559f003":"## 5.3 Gender"}}