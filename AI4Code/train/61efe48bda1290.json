{"cell_type":{"a9e6d543":"code","06cab6b5":"code","bd180d3d":"code","75078342":"code","b9ed9d53":"code","217248b0":"code","e2a10e23":"code","2a980073":"code","4b7bdd7f":"code","b4f1eb0a":"code","37940a72":"code","140c66c7":"code","8eeb9627":"code","c5a39482":"code","afa2724c":"code","da8537d1":"code","beed9de3":"code","79179d33":"code","56a9c687":"code","55a0e237":"code","00b5bcef":"code","89e88d46":"code","f1f8b3bd":"code","ca8772e2":"code","55ff1aff":"code","62f0a76a":"code","2bbf15bb":"code","251588a5":"code","3816f23f":"code","90a931c8":"code","9710436c":"code","0727eef7":"code","ed7d22de":"code","f30b5cee":"code","f6642533":"code","ca17c159":"code","c9e9aabe":"code","6f2b0e7e":"code","b23e9624":"code","41c8aca1":"code","78211a50":"code","7445cb5a":"code","d56b3658":"code","b3446715":"code","02fa9fa1":"code","d4cd2084":"code","5a91a5d5":"code","9a480b67":"code","a9a3667c":"code","bc9d4eb7":"markdown","ae97f2e0":"markdown","1f06251b":"markdown","d4d2cf54":"markdown","c4051b34":"markdown","e2c6716d":"markdown","217a4800":"markdown","c5566a1d":"markdown","9119f8c2":"markdown","d3032bce":"markdown","d853eaf3":"markdown","1fd51ac9":"markdown","e2d024a9":"markdown","ee701941":"markdown","887269bc":"markdown","db8040d2":"markdown","7a9c40a9":"markdown","2e4481f6":"markdown","4fe6a7b7":"markdown","741c7ea3":"markdown","0bd9db7e":"markdown","5625906d":"markdown","a03c0f02":"markdown","7bdc995c":"markdown","b0a7bf26":"markdown","40b6a9b9":"markdown","b4d67ba0":"markdown","7ba01796":"markdown"},"source":{"a9e6d543":"# Data Science Purpose\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns; sns.set_style('darkgrid')\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# Natural Language Processing Purpose\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english'))\nfrom nltk.util import ngrams\nfrom nltk.tokenize import word_tokenize\nimport gensim\n\n# Python built-in modules\nimport re\nimport string\nimport statistics\nfrom tqdm import tqdm\nfrom collections import defaultdict, Counter\n\n# Deep learning purpose\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D, Bidirectional\nfrom keras.initializers import Constant\nfrom sklearn.model_selection import train_test_split\nfrom keras.optimizers import Adam\n\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","06cab6b5":"# Read csv\ntrain = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\ntarget = train['target']\n\n# Data shape\nprint(\"Train shape\", train.shape)\nprint(\"Test shape\", test.shape)","bd180d3d":"cdist = train['target'].value_counts()\nsns.barplot(cdist.index, cdist.values)\nplt.gca().set_title(\"Class Distribution\");","75078342":"# Missing data on both training and test data\ntrain_miss = train.isnull().sum()\ntest_miss = test.isnull().sum()\nprint(\"Missing values in train data\", train_miss, sep=\"\\n\", end=\"\\n\\n\")\nprint(\"Missing values in test data\", test_miss, sep=\"\\n\")\n\n# Iterate throught tuples of 3 values (name, missing data, real data)\nfor name, data, real in [('Train', train_miss, train), ('Test', test_miss, test)]:\n    fig, ax = plt.subplots()\n    ax.set_title(f\"Missing Values on {name} data\", pad=20)\n    \n    values = data.values \/ len(real) # Divide values with len of real data to get the percentage\n    index = data.index # Index names\n    \n    bar = sns.barplot(x=data.index, y=data.values \/ len(real), ax=ax)\n    ax.set_ylabel(\"Percentage\")","b9ed9d53":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 5))\n\nfirst_len = train[train['target'] == 0]['text'].str.len()\nsecond_len = train[train['target'] == 1]['text'].str.len()\n\nax1.hist(first_len, color='g')\nax1.set_ylabel('Characters length')\nax1.set_title('Not Disaster Tweets')\n\nax2.hist(second_len, color='r')\nax2.set_title('Disaster Tweets')","217248b0":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 5))\n\nfirst_words = train[train['target'] == 0]['text'].str.split().map(lambda x: len(x))\nsecond_words = train[train['target'] == 1]['text'].str.split().map(lambda x: len(x))\n\nfig.suptitle(\"Number of words in tweets\")\n\nax1.hist(first_words, color='g')\nax1.set_ylabel('Words length')\nax1.set_title('Not Disaster Tweets')\n\nax2.hist(second_words, color='r')\nax2.set_title('Disaster Tweets')","e2a10e23":"fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(8, 6))\n\n\navg_one = train[train['target'] == 0]['text'].str.split().map(lambda x: [len(i) for i in x])\navg_two = train[train['target'] == 1]['text'].str.split().map(lambda x: [len(i) for i in x])\n\nfig.suptitle(\"Average of words in tweets\")\n\nsns.distplot(avg_one.apply(lambda x: statistics.mean(x)), color='g', ax=ax1)\nax1.set_ylabel('Words Average')\nax1.set_title('Not Disaster Tweets')\n\nsns.distplot(avg_two.apply(lambda x: statistics.mean(x)), color='r', ax=ax2)\nax2.set_title('Disaster Tweets')","2a980073":"def create_corpus(target):\n    \"\"\"Form a list of collection of text, return list\"\"\"\n    \n    corpus = []\n    \n    for x in train[train['target'] == target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus\n\n\ndef filter_specific_word(corpus, filters):\n    \"\"\"Filter specific word in corpus, return dict\"\"\"\n    \n    dic = defaultdict(int)\n    \n    for word in corpus:\n        if word in filters:\n            dic[word] += 1\n    return dic","4b7bdd7f":"for i in [0, 1]:\n    fig, ax = plt.subplots()\n    corpus = create_corpus(i)\n\n    dic = filter_specific_word(corpus, stop)\n    top = sorted(dic.items(), key=lambda x:x[1], reverse=True)[:10]\n    x, y = zip(*top) # x=word, y=count\n\n    ax.bar(x, y, color='brown' if i == 0 else 'orange')\n    ax.set_title(f'Class {i}');","b4f1eb0a":"for i in [0, 1]:\n    fig, ax = plt.subplots(figsize=(10, 5))\n    corpus = create_corpus(i)\n    \n    special = string.punctuation\n    dic = filter_specific_word(corpus, special)\n    \n    x, y = zip(*dic.items()) # x=word, y=count\n    ax.bar(x, y, color='brown' if i == 0 else 'orange')\n    ax.set_title(f'Class {i}')","37940a72":"for i in [0, 1]:\n    fig, ax = plt.subplots(figsize=(10, 5))\n    \n    corpus = create_corpus(i)\n    dic = defaultdict(int)\n    for word in corpus:\n        if not word.isalpha() and word not in special:\n            dic[word] += 1\n    \n    top = sorted(dic.items(), key=lambda x: x[1], reverse=True)[:10]\n    x, y = zip(*reversed(top))\n    \n    ax.barh(x, y, color=\"brown\" if i==0 else \"orange\");\n    plt.gca().set_title(\"class_{}\".format(i))","140c66c7":"corpus = create_corpus(0)\ncounter = Counter(corpus)\nmost = counter.most_common()\n\nx, y = [], []\n\nfor word, count in most[:40]:\n    if word not in stop:\n        x.append(word)\n        y.append(count)\nfig, ax = plt.subplots(figsize=(10, 5))\nfig.suptitle('Common Words')\n\nsns.barplot(x=y, y=x, orientation='horizontal', ax=ax);","8eeb9627":"def get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0)\n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key=lambda x: x[1], reverse=True)\n    return words_freq","c5a39482":"plt.figure(figsize=(10, 5))\ntop_tweet_bigram = get_top_tweet_bigrams(train['text'])[:10]\nx, y= map(list, zip(*top_tweet_bigram))\nsns.barplot(x, y);","afa2724c":"df = pd.concat([train, test], axis=0, sort=False)\ndf.shape","da8537d1":"def remove_url(text):\n    url = re.compile('https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'', text)\n\n# Test the function\nremove_url('http:\/\/www.kaggle.com\/rakkaalhazimi\/nlp-disaster-classification\/edit?rvi=1')","beed9de3":"# Deploy to real data\ndf['text'] = df['text'].apply(lambda x: remove_url(x))\n\n# Reassure the result\nretain = df['text'].str.contains(r'http[s]*').sum()\nprint(\"{} words were left behind\".format(retain))","79179d33":"# Let's see what the residual words looks like\nresidual = df[df['text'].str.contains(r'http[s]*')]\nleft_word = []\nfor i in range(len(residual)):\n    print(residual['text'].values[i])\n    left_word.append(residual['text'].values[i])","56a9c687":"for word in left_word:\n    compiler = re.compile(r'.http.+')\n    result = compiler.sub('', word)\n    print(result)","55a0e237":"# Replace in data frame\ndf['text'] = df['text'].str.replace(r'.http.+', '')\nprint(\"http words found {}\".format(df['text'].str.contains('http').sum()))","00b5bcef":"example = \"\"\"<div>\n<h1>Real or Fake<\/h1>\n<p>Kaggle <\/p>\n<a href=\"https:\/\/www.kaggle.com\/c\/nlp-getting-started\">getting started<\/a>\n<\/div>\"\"\"","89e88d46":"def remove_html(text):\n    html = re.compile(r'<.*?>')\n    return html.sub(r'', text)\n\nprint(remove_html(example))","f1f8b3bd":"df['text'] = df['text'].apply(lambda x: remove_html(x))","ca8772e2":"# Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\n\nremove_emoji(\"Omg another Earthquake \ud83d\ude14\ud83d\ude14\")","55ff1aff":"df['text'] = df['text'].apply(lambda x: remove_emoji(x))","62f0a76a":"def remove_punct(text):\n    table = str.maketrans('', '', string.punctuation)\n    return text.translate(table)\n\nexample = \"I am King#\"\nremove_punct(example)","2bbf15bb":"df['text'] = df['text'].apply(lambda x: remove_punct(x))","251588a5":"# !pip install spellchecker","3816f23f":"# from spellchecker import SpellChecker\n\n# spell = SpellChecker()\n# def correct_spelling(text):\n#     corrected_text = []\n#     mispelled_words = spell.unknown(text.split())\n#     for word in tqdm(text.split()):\n#         if word in mispelled_words:\n#             corrected_text.append(spell.correction(word))\n#         else:\n#             corrected_text.append(word)\n#     return \" \".join(corrected_text)\n\n# text = \"correct me pleasee\"\n# correct_spelling(text)","90a931c8":"# df['text'] = df[\"text\"].apply(lambda x: correct_spelling(x))","9710436c":"# df.to_csv(\"tweetDisaster.csv\", index=False)","0727eef7":"df = pd.read_csv(\"..\/input\/nlp-disaster-cleaned\/tweetDisaster.csv\")","ed7d22de":"# Take a look at 30 samples of data\ndf['text'].sample(30)","f30b5cee":"def create_corpus(df):\n    copy_df = df.copy()\n    corpus = []\n    for tweet in tqdm(copy_df[\"text\"]):\n        words = [word.lower() for word in word_tokenize(tweet) if ((word.isalpha() == 1) & (word not in stop))]\n        corpus.append(words)\n    return corpus","f6642533":"corpus = create_corpus(df)","ca17c159":"embedding_dict = {}\nwith open(\"\/kaggle\/input\/glove-global-vectors-for-word-representation\/glove.6B.100d.txt\") as f:\n    for line in f:\n        values = line.split()\n        word = values[0]\n        vectors = np.asarray(values[1:], 'float32')\n        embedding_dict[word] = vectors","c9e9aabe":"print(\"Embedding shape : ({},{})\".format(len(embedding_dict), \n                                         len(embedding_dict['the'])))","6f2b0e7e":"MAX_LEN = 50\ntokenizer_obj = Tokenizer()\ntokenizer_obj.fit_on_texts(corpus)\nsequences = tokenizer_obj.texts_to_sequences(corpus)\n\ntweet_pad = pad_sequences(sequences, maxlen=MAX_LEN, truncating='post', padding='post')","b23e9624":"tweet_pad.shape","41c8aca1":"word_index = tokenizer_obj.word_index\nprint(\"Number of unique words:\", len(word_index))","78211a50":"top = sorted(word_index, key=lambda x: word_index[x], reverse=True)[:10]\nunknown_index = []\nfor word in top:\n    scores = (word, word_index[word])\n    unknown_index.append(scores)\n    \nunknown_index","7445cb5a":"num_words = len(word_index) + 1\nembedding_matrix = np.zeros((num_words, 100))\n\nfor word, i in tqdm(word_index.items()):\n    if i > num_words:\n        continue\n        \n    emb_vec = embedding_dict.get(word)\n    if emb_vec is not None:\n        embedding_matrix[i] = emb_vec","d56b3658":"embedding_matrix.shape","b3446715":"train = tweet_pad[:train.shape[0]]\ntest = tweet_pad[train.shape[0]:]","02fa9fa1":"model = Sequential()\n\nembedding = Embedding(num_words, 100, embeddings_initializer=Constant(embedding_matrix),\n                      input_length=MAX_LEN, trainable=False)\n\nmodel.add(embedding)\nmodel.add(Bidirectional(LSTM(128, \n                             dropout=0.2, \n                             recurrent_dropout=0.2))\n         )\nmodel.add(Dense(1, activation='sigmoid'))\n\noptimizer = Adam(learning_rate=1e-4)\n\nmodel.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n\nmodel.summary()","d4cd2084":"history = model.fit(train, target, batch_size=32, epochs=15,\n                   validation_split=0.2, verbose=1)","5a91a5d5":"sample_sub = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\nsample_sub.shape","9a480b67":"y_pred = model.predict(test)\ny_pred = np.round(y_pred).astype(int).reshape(3263)\nsub = pd.DataFrame({'id': sample_sub['id'].values.tolist(), 'target': y_pred})\nsub.to_csv(\"submission.csv\", index=False)","a9a3667c":"sub.head()","bc9d4eb7":"### Spelling Correction","ae97f2e0":"There was a standalone \"http\" and \"http\" followed by \"U\" hat. Let's clean that","1f06251b":"### Use Cleaned Data","d4d2cf54":"### Number of characters in tweets ","c4051b34":"# Open Data File","e2c6716d":"### Save Cleaned Data","217a4800":"### Alphanumeric Words","c5566a1d":"# Globe for Vectorization","9119f8c2":"# Missing Values","d3032bce":"# Building Model","d853eaf3":"Both class have been dominated by \"the\" words.","1fd51ac9":"There is no missing values on text column, it is fine to move on.","e2d024a9":"# Import Modules","ee701941":"Analyze both classes","887269bc":"My presumption is right, we miss some \"http\" words behind.","db8040d2":"### Average words mean","7a9c40a9":"### Removing Emojis","2e4481f6":"### Removing Urls","4fe6a7b7":"# Class Distribution","741c7ea3":"# Data Cleaning","0bd9db7e":"### Common Stopwords in Tweets","5625906d":"### Ngram Analysis","a03c0f02":"### Removing Punctuations","7bdc995c":"# Exploratory Data Analysis","b0a7bf26":"### Number of words in tweets","40b6a9b9":"### Common Words","b4d67ba0":"### Removing HTML tags","7ba01796":"### Analyzing Punctuations"}}