{"cell_type":{"f9219fc0":"code","12d59e38":"code","e154fd0e":"code","12dd1cf5":"code","ab5a299f":"code","270fc8b4":"code","f1b8a1f9":"code","e5032478":"code","c86c944d":"code","69c4a68b":"code","5953b955":"code","54a0b6c9":"code","03e3ac8b":"code","d82c17ab":"code","4fecccec":"code","06b979ed":"code","d4f06854":"code","847e5e89":"code","7dacfe9e":"code","569a557a":"code","c90ec831":"code","7de4fa55":"code","53fb28f9":"code","9c984e53":"code","2d4b8c82":"code","3df30d91":"code","303640b7":"code","ac37421d":"code","3a043783":"code","46be7457":"code","8251e1a8":"code","f0b63150":"code","fa14ebeb":"code","40cc5eaa":"code","f7246095":"code","d25f6203":"code","0891205b":"code","cc99594c":"code","31892d5b":"code","0c538d87":"code","caf8b9cb":"code","b6472b0d":"code","fdda95ed":"code","921f0632":"code","272fc3a0":"code","0428d6d7":"markdown","d7adf402":"markdown","0c4e0513":"markdown","f3ab2ce6":"markdown","0668ce67":"markdown","73a15e92":"markdown","fc4a6bdd":"markdown","a9bc29ce":"markdown","83556860":"markdown","6fe44700":"markdown","f957e7d9":"markdown","9ff32cb0":"markdown","75ddc99d":"markdown","b87af6ba":"markdown","27f4d594":"markdown"},"source":{"f9219fc0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re\nimport nltk\nnltk.download('wordnet')\nfrom nltk import WordNetLemmatizer\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nsw = stopwords.words(\"english\")\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.preprocessing.text import one_hot\nfrom tensorflow.keras.utils import to_categorical\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.preprocessing.text import Tokenizer","12d59e38":"# df=pd.read_csv('\/content\/drive\/MyDrive\/Colab Notebooks\/Word predictor\/abcnews.csv')","e154fd0e":"df.head()","12dd1cf5":"df=df.sample(n=17000)","ab5a299f":"df.head(3)","270fc8b4":"df.shape","f1b8a1f9":"#Resetting the index\ndf.reset_index(inplace=True)","e5032478":"#Dropping the old index\ndf.drop(columns=['index'],inplace=True)","c86c944d":"itr=10\nfor i in range(itr):\n  print(df['headline_text'][i])","69c4a68b":"df['publish_date']=pd.to_datetime(df['publish_date'],format='%Y%m%d')","5953b955":"df.head()","54a0b6c9":"df.shape","03e3ac8b":"#Visualizing the Most repeating words\nfrom wordcloud import WordCloud\nplt.figure(figsize=(10,10))\nCloud_img=WordCloud(background_color='black',height=520,width=400).generate(' '.join(df['headline_text']))\nplt.imshow(Cloud_img)\nplt.axis('off')\nplt.show()","d82c17ab":"#Preprocessing the text\ndef cleaning(text):\n  clean_list=[]\n  wl=WordNetLemmatizer()\n  #Tokenizing the words\n  word_tokens=re.split(' ',text)\n  for i in word_tokens:\n    #Removing the punctuations\n    small=i.lower()\n    punc_remov=re.sub('^a-zA-z',' ',small)\n    if punc_remov not in sw:\n      clean_list.append(wl.lemmatize(punc_remov))\n  clean_text=' '.join(clean_list)\n  return clean_text","4fecccec":"#Cleaned headline\ndf['Cleaned_headline']=df['headline_text'].apply(cleaning)","06b979ed":"df","d4f06854":"#Dropping the headline column\ndf.drop(columns=['headline_text','publish_date'],inplace=True)","847e5e89":"#printing the 10 headlines\ndf['Cleaned_headline'][0:10]","7dacfe9e":"#Visualizing the Most repeating words\nfrom wordcloud import WordCloud\nplt.figure(figsize=(10,10))\nCloud_img=WordCloud(background_color='black',height=520,width=400).generate(' '.join(df['Cleaned_headline']))\nplt.imshow(Cloud_img)\nplt.axis('off')\nplt.show()","569a557a":"# vocabulary=10000","c90ec831":"# encoded_vec=[]\n# for text in df['Cleaned_headline']:\n#   encoded_vec.append(one_hot(text,n=vocabulary))","7de4fa55":"# print(f\"one hot encoded vector{encoded_vec[0:10]}\")","53fb28f9":"#Creating an object of tokenizer\ntokenizer_obj=Tokenizer()\n#Fitting the tokenizer on the words present in headline column which will be used as vocabulary later\ntokenizer_obj.fit_on_texts(df['Cleaned_headline'])\n","9c984e53":"#One hot encoded diztionary\ntokenizer_obj.word_index","2d4b8c82":"#Defining the vocab size\nvocabulary_size=len(tokenizer_obj.word_index)\nprint(f'The vocabulary size is {vocabulary_size}')","3df30d91":"#Encoding the headline\nencoded_vec=[]\nfor text in df['Cleaned_headline']:\n  encoded_vec.append(tokenizer_obj.texts_to_sequences([text])[0])","303640b7":"print(f\"Encoded vector :- {encoded_vec[0]}\")\nprint(f\"Text :- {df['Cleaned_headline'][0]}\")","ac37421d":"encoded_vec","3a043783":"#Adding pre padding to the one hot encoded vectors to make them same size as the maximum vector\n#Getting the max length of the one hot encoded vector\nmax_len_vec=0\nfor values in encoded_vec:\n  if len(values)>max_len_vec:\n    max_len_vec=len(values)","46be7457":"print(f'max length of vector {max_len_vec}')","8251e1a8":"padded_vec=pad_sequences(encoded_vec,maxlen=max_len_vec,padding='pre')","f0b63150":"print(f\"The padded vector is :- {padded_vec}\")","fa14ebeb":"x=padded_vec[:,0:-1]\ny=padded_vec[:,-1]","40cc5eaa":"print(f'x={x}')\nprint(f'y={y}')","f7246095":"y_cat=to_categorical(y,num_classes=vocabulary_size)","d25f6203":"x_train,x_test,y_train,y_test=train_test_split(x,y_cat,test_size=0.20,random_state=42)","0891205b":"print(f'X_train shape{x_train.shape}')\nprint(f'X_test shape{x_test.shape}')\nprint(f'y_train shape{y_train.shape}')\nprint(f'y_test shape{y_test.shape}')","cc99594c":"model=keras.Sequential()\n#Embedding layer\n#input_dim is vocabulary_size + 1 because 0 is not considered when calculating vocab size\nmodel.add(keras.layers.Embedding(input_dim=vocabulary_size+1,output_dim=20,input_length=max_len_vec-1,))#Bcz we have taken the last value as target label\nmodel.add(keras.layers.Bidirectional(keras.layers.LSTM(100,return_sequences=True)))\nmodel.add(keras.layers.Dropout(0.3))\nmodel.add(keras.layers.Bidirectional(keras.layers.LSTM(100)))\nmodel.add(keras.layers.Dropout(0.3))\nmodel.add(keras.layers.Dense(units=100,activation='relu'))\nmodel.add(keras.layers.Dropout(0.3))\nmodel.add(keras.layers.Dense(units=vocabulary_size,activation='softmax'))","31892d5b":"model.summary()","0c538d87":"model.compile(optimizer='adam',loss='categorical_crossentropy',metrics=['accuracy'])","caf8b9cb":"model.fit(x_train,y_train,epochs=30,validation_data=(x_test,y_test))","b6472b0d":"def predict_next_word(seed_text):\n  cleaned_text=cleaning(seed_text)\n  token_list = tokenizer_obj.texts_to_sequences([cleaned_text])[0]\n  token_list = pad_sequences([token_list], maxlen=max_len_vec-1, padding='pre')\n  prediction = model.predict(token_list)\n  predictions=np.argmax(prediction)\n  predicted_word=tokenizer_obj.sequences_to_texts([[predictions]])[0]\n  return seed_text+' '+predicted_word","fdda95ed":"predict_next_word('Today is a')","921f0632":"def predict_next_word(seed_text):\n  cleaned_text=cleaning(seed_text)\n  token_list = tokenizer_obj.texts_to_sequences([cleaned_text])[0]\n  token_list = pad_sequences([token_list], maxlen=max_len_vec-1, padding='pre')\n  prediction = model.predict(token_list)\n  predictions=np.argmax(prediction)\n  predicted_word=tokenizer_obj.sequences_to_texts([[predictions]])[0]\n  return predicted_word","272fc3a0":"sent='India is a rich country in'\nnext_words=10\nfor i in range(next_words):\n  prediction_sent=predict_next_word(sent)\n  sent=sent+' '+prediction_sent\nprint(sent)","0428d6d7":"<h2>Converting the date in proper format<\/h2>","d7adf402":"<h3>One hot encoding using tokenizer<\/h3>","0c4e0513":"Note:- I am using google colab with 12gb of ram ,it can't load the complete dataset for categorical conversion,so I need to take a sample of data","f3ab2ce6":"<h3>Train Test Splitting<\/h3>","0668ce67":"<h2>Visualizing the cleaned text<\/h2>","73a15e92":"<h3>Checking Predictions<\/h3>","fc4a6bdd":"<h3>Creating the model<\/h3>","a9bc29ce":"<h2> Visualizing the text<\/h2>","83556860":"<h2>Preprocessing the text<h2>","6fe44700":"<h2> Padding the one hot encoded vectors<\/h2>","f957e7d9":"<h2>Importing libraries<\/h2>","9ff32cb0":"<h3>Creating the target variable<\/h3>","75ddc99d":"<h3>Sentence generation<\/h3>","b87af6ba":"<h3> Randomly Sampling 150000 datapoints from the dataframe","27f4d594":"<h2>One hot encoding<\/h2>\nNote :- We won't use this method because we also need to decode the text after prediction which is not possible while using this"}}