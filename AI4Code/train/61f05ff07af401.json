{"cell_type":{"c60d5d91":"code","c9f54d6e":"code","b66c7e82":"code","8dbd572a":"code","7b992474":"code","81126dd1":"code","e4489403":"code","38e0cd9e":"code","4c383d10":"code","8154296b":"code","b2b84fd9":"code","8e1e484b":"code","14a7327a":"code","7f45f72f":"code","54a3c10e":"code","3a41d01f":"code","d2ee6835":"markdown","82821eae":"markdown","8ec27e99":"markdown","44c72c99":"markdown","4831736c":"markdown","da30cd90":"markdown","98f23689":"markdown","cd41e634":"markdown","24873658":"markdown"},"source":{"c60d5d91":"import pandas as pd\nimport numpy as np","c9f54d6e":"df = pd.read_csv(\"\/kaggle\/input\/multiclass-text-classification\/Merilytics_Clean.csv\")\ndf_clean = df[df['review_id'].notnull() & df['text'].notnull()]\n\n# take a peek at the data\ndf_clean.head()\n","b66c7e82":"reviews = np.array(df_clean['text'])\nratings = np.array(df_clean['stars'])\n\n# build train and test datasets\n\ntrain_len = int(0.9*len(df_clean))\nprint(f'Training Data {train_len} Testing Data {len(reviews)-train_len}')\ntrain_reviews = reviews[:train_len]\ntrain_rating = ratings[:train_len]\ntest_reviews = reviews[train_len:]\ntest_rating = ratings[train_len:]","8dbd572a":"type(train_reviews)","7b992474":"from nltk.tokenize import ToktokTokenizer\nimport time\ntokenizer = ToktokTokenizer()\nstart = time.time()\n\n#[x for x in t.tokenize('I am good, 2 3 4') if x.isalpha()]\n\ntokenized_train = [[x for x in tokenizer.tokenize(review) if x.isalpha()] for review in train_reviews]\ntokenized_test = [[x for x in tokenizer.tokenize(review) if x.isalpha()] for review in test_reviews]\nprint(f'Time Taken {time.time()-start}')","81126dd1":"del df, df_clean","e4489403":"from collections import Counter\n\n# build word to index vocabulary\ntoken_counter = Counter([token for review in tokenized_train for token in review])\nvocab_map = {item[0]: index+1 for index, item in enumerate(dict(token_counter).items())}\nmax_index = np.max(list(vocab_map.values()))\nvocab_map['PAD_INDEX'] = 0\nvocab_map['NOT_FOUND_INDEX'] = max_index+1\nvocab_size = len(vocab_map)\n# view vocabulary size and part of the vocabulary map\nprint('Vocabulary Size:', vocab_size)\nprint('Sample slice of vocabulary map:', dict(list(vocab_map.items())[10:20]))","38e0cd9e":"from keras.preprocessing import sequence\nfrom sklearn.preprocessing import LabelEncoder\nfrom keras.utils import to_categorical\n\n# get max length of train corpus and initialize label encoder\nle = LabelEncoder()\nnum_classes=5\nmax_len = np.max([len(review) for review in tokenized_train])\n\n## Train reviews data corpus\n# Convert tokenized text reviews to numeric vectors\ntrain_X = [[vocab_map[token] for token in tokenized_review] for tokenized_review in tokenized_train]\ntrain_X = sequence.pad_sequences(train_X, maxlen=max_len) # pad \n## Train prediction class labels\ny_tr = le.fit_transform(train_rating)\ny_train = to_categorical(y_tr, num_classes)\n## Test reviews data corpus\n# Convert tokenized text reviews to numeric vectors\ntest_X = [[vocab_map[token] if vocab_map.get(token) else vocab_map['NOT_FOUND_INDEX'] \n           for token in tokenized_review] \n              for tokenized_review in tokenized_test]\ntest_X = sequence.pad_sequences(test_X, maxlen=max_len)\n## Test prediction class labels\n# Convert text sentiment labels (negative\\positive) to binary encodings (0\/1)\ny_ts = le.transform(test_rating)\ny_test = to_categorical(y_ts, num_classes)\n\n# view vector shapes\nprint('Max length of train review vectors:', max_len)\nprint('Train review vectors shape:', train_X.shape, ' Test review vectors shape:', test_X.shape)","4c383d10":"from keras.models import Sequential\nfrom keras.layers import Dense, Embedding, Dropout, Activation, TimeDistributed, Conv1D, MaxPooling1D, Flatten, SpatialDropout1D\nfrom keras.layers import LSTM\n\nEMBEDDING_DIM =128 # dimension for dense embeddings for each token\nLSTM_DIM = 64 # total LSTM units\n\nmodel = Sequential()\nmodel.add(Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_len))\nmodel.add(SpatialDropout1D(0.2))\nmodel.add(MaxPooling1D(pool_size=2))\nmodel.add(LSTM(LSTM_DIM, dropout=0.2, recurrent_dropout=0.2))\nmodel.add(Dense(16, activation='relu'))\nmodel.add(Dropout(0.2))\nmodel.add(Dense(5))\nmodel.add(Activation('softmax'))\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"]) ","8154296b":"##Complete Later\n'''\nnlp_input = Input(shape=(max_len,))\nmeta_input = Input(shape=(3,))\n# the first branch operates on the first input\nx = Embedding(input_dim=vocab_size, output_dim=EMBEDDING_DIM, input_length=max_len)(nlp_input)\nx = SpatialDropout1D(0.2)(x)\nx = LSTM(LSTM_DIM, dropout=0.2, recurrent_dropout=0.2)(x)\nx = Model(inputs=nlp_input, outputs=x)\n# the second branch opreates on the second input\ny = Dense(32, activation=\"relu\")(meta_input)\ny = Dense(8, activation=\"relu\")(y)\ny = Dense(1, activation=\"relu\")(y)\ny = Model(inputs=meta_input, outputs=y)\n# combine the output of the two branches\ncombined = concatenate([x.output, y.output])\n# apply a FC layer and then a regression prediction on the\n# combined outputs\nz = Dense(64, activation=\"relu\")(combined)\nz = Dropout(0.4)(z)\nz = Dense(16, activation=\"relu\")\nz = Dropout(0.4)(z)\nz = Dense(5, activation=\"relu\")\nz = Activation(\"softmax\")(z)\n# our model will accept the inputs of the two branches and\n# then output a single value\nmodel = Model(inputs=[x.input, y.input], outputs=z)\nmodel.compile(loss=\"binary_crossentropy\", optimizer=\"adam\",metrics=[\"accuracy\"])\n'''","b2b84fd9":"print(model.summary())","8e1e484b":"from IPython.display import SVG\nfrom keras.utils.vis_utils import model_to_dot\n\nSVG(model_to_dot(model, show_shapes=True, show_layer_names=False, \n                 rankdir='LR').create(prog='dot', format='svg'))","14a7327a":"import matplotlib.pyplot as plt\nplt.style.use('ggplot')\n\ndef plot_history(history):\n    acc = history.history['accuracy']\n    val_acc = history.history['val_accuracy']\n    loss = history.history['loss']\n    val_loss = history.history['val_loss']\n    x = range(1, len(acc) + 1)\n\n    plt.figure(figsize=(12, 5))\n    plt.subplot(1, 2, 1)\n    plt.plot(x, acc, 'b', label='Training acc')\n    plt.plot(x, val_acc, 'r', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.legend()\n    plt.subplot(1, 2, 2)\n    plt.plot(x, loss, 'b', label='Training loss')\n    plt.plot(x, val_loss, 'r', label='Validation loss')\n    plt.title('Training and validation loss')\n    plt.legend()\n\nbatch_size = 20\nhistory = model.fit(train_X, y_train, epochs=3, batch_size=batch_size, \n          shuffle=True, validation_split=0.2, verbose=1)","7f45f72f":"plot_history(history)","54a3c10e":"history.history","3a41d01f":"from sklearn.metrics import classification_report\npred_test = model.predict_classes(test_X)\npredictions = le.inverse_transform(pred_test.flatten())\nprint(classification_report(test_rating,predictions))","d2ee6835":"# Tokenize train & test datasets","82821eae":"# Build Vocabulary Mapping (word to index)","8ec27e99":"# Visualize model architecture","44c72c99":"# Build the LSTM Model Architecture","4831736c":"# Train the model","da30cd90":"# Predict and Evaluate Model Performance","98f23689":"# Encode and Pad datasets & Encode prediction class labels","cd41e634":"# Load and normalize data","24873658":"# Import necessary depencencies"}}