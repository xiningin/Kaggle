{"cell_type":{"80b06e35":"code","01746a42":"code","4ea40822":"code","bacd480c":"code","d732e655":"code","ae2a5160":"code","2b699af6":"code","35c8423d":"code","f352af04":"code","903f2692":"code","3e709de2":"code","d2bc981c":"code","5099af54":"code","64bad5cd":"code","5e4a405c":"code","f903cc02":"code","a48b5693":"code","1aa65d9d":"code","bf093202":"code","8dc5c4be":"code","bc2a97b3":"markdown","58f68d3b":"markdown","c50b6b3e":"markdown","7e8896d2":"markdown","ebccb0d4":"markdown","cd6e7307":"markdown","d5d2f27b":"markdown","cd6e1046":"markdown","51406b44":"markdown","d41119f4":"markdown","22273999":"markdown","38e2e8a7":"markdown","9eef4a75":"markdown","37ce3daa":"markdown","1307157e":"markdown","4ecd2ae9":"markdown"},"source":{"80b06e35":"import tensorflow as tf\nimport pandas as pd\nimport numpy as np","01746a42":"df = pd.read_csv(\"..\/input\/house-price-prediction\/data.csv\")\nprint(df.shape)\ndf = df.sample(frac=1, random_state=42).reset_index(drop=True)\ndf.head()","4ea40822":"df.isnull().sum()\n# So we dont have any null values.","bacd480c":"df.city.value_counts()","d732e655":"\n\ncities = [\"Algona\",                    \n\"Yarrow Point\",              \n\"Skykomish\",                 \n\"Milton\",                    \n\"Preston\",                    \n\"Inglewood-Finn Hill\",       \n\"Beaux Arts Village\",        \n\"Snoqualmie Pass\", ]\n\n# df.drop(df[df['city'] in cities], inplace = True)\ndf = df[~df['city'].isin(cities)]\ndf.city.value_counts()\n","ae2a5160":"print(\"Water front {0: <10}\".format(str(df.waterfront.unique())))\nprint(\"view {0: <10}\".format(str(df.view.unique())))\nprint(\"Waterconditionfront {0: <10}\".format(str(df.condition.unique())))\n\n# We have Data of only 1 country, So we can drop that coloum\nprint(\"Country {0: <10}\".format(str(df.country.unique())))\n\n# Price Vary by dates but it will become so complicated if I include dates, Will cover it later \n# In time-series Analysis Part\ndf.drop(['country', 'date'], inplace = True, axis = 1)\n\n\n# Lets Seperate Our Test-data First and than we will apply our pre-processing\nseventy_percent_of_data = int(len(df) * 0.8)\n\ntest = df.iloc[seventy_percent_of_data:]\ntrain = df.iloc[:seventy_percent_of_data]\n\ndf = train\nprint(\"Train DataFrame Shape :\", df.shape)\nprint(\"Train DataFrame Shape :\", test.shape)","2b699af6":"# Every House has its own Unique address (House no, Street no, City etc)\n# So We need to find common streets because In real-estate mostly price depends on the location of the house\n# From street or Area number we can categorize the area of the house\nprint( \"Shape of DataFrame :\",df.shape[0])\nprint(\"Number of Values we have in Street Column :\", len(df.street.unique()))\ndf.street = df.street.map(lambda x: x.split(\" \",1)[1])\nprint(\"After removing House Number we got :\", len(df.street.unique()))\n# I think we should categorise it with Area (We will decide it later when we are on training part)\n\n","35c8423d":"for i in df.street.values:\n    try:\n        \n        street = str(i).split(\" \",1)[1]\n    except:\n        print(f\"Exception on this : {i}\")\n        \n# I am not staisfied by just splitting Address. Lets apply some nlp Techniques to get better result.\n# Lets apply Bag of words\/TF-IDF to have some deep dive into the Street Column\n\n","f352af04":"from wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nimport matplotlib.pyplot as plt\n\n# Start with one review:\ntext = \" \".join(df.street).lower().replace(\"st\",\"\")\n\n# Create and generate a word cloud image:\nwordcloud = WordCloud().generate(text)\n\n# Display the generated image:\nplt.figure(figsize=(18, 18), dpi=300)\nplt.imshow(wordcloud, interpolation='bilinear')\nplt.axis(\"off\")\nplt.show()","903f2692":"from sklearn.feature_extraction.text import TfidfVectorizer\nimport copy\n\n# list of text documents\ntext = copy.deepcopy(df.street.values)\n\n# create the transform\nvectorizer = TfidfVectorizer()\n\n# tokenize and build vocab\nvectorizer.fit(text)\n\n# Conversion of Vector To Scalar (Read below why we are doing this)\ndef vector_to_scalar(vector):\n    '''\n    Converts Vector to scalar\n    \n    1. Build an array of ones\n    2. Transpose the array to match --> m and p where ((n x m )* (p x q)) are two vectors\n    3. Perform Matrix Multiplication\n    '''\n    vector = vector.reshape(1,-1)\n    ones_vector = np.transpose(np.ones(vector.shape))\n    return np.matmul(vector, ones_vector).item()\n\n# encode document\ndef TfIDF(df):\n    '''\n    Converting Street Coloum into Tf IDF\n    \n    '''\n    df.street = df.street.map(lambda x: vectorizer.transform([x]))\n    for index ,i in enumerate(df.street[:5]): print(f\"{text[index]} Converted to  {i.data}\");\n    df.street = df.street.map(lambda x: x.data)\n    df.street = df.street.map(lambda x : vector_to_scalar(x))\n    return df\n\ndf = TfIDF(df)\n\nfor i in df.street: print(i); break\n    \n# Hmmm! Okay I am not so much sure with this coloum. lets try someother thing `:)\n","3e709de2":"from sklearn import preprocessing\n\n# Label Encoder\nstateZip_le = preprocessing.LabelEncoder()\n\n# Label Encoder Object SO that we can use them later to perform same pre-processing on test-data\nle_obj_dict = dict()\n\n\ndef label_encoding(df, subset):\n    \n    global le_obj_dict\n    # Converting Dataframe Columns\n    for i in [\"city\",\"statezip\"]:\n\n        print(f\"Encoding Column : ******* {i} ********\")\n\n        # Storing Every Label Encoder Object seperatly\n        if subset == 'train':\n            le_obj_dict.update({i:preprocessing.LabelEncoder().fit(df[i])})\n        \n        df[i] = le_obj_dict[i].transform(df[i])\n\n        # Getting label encoder mappings \n        mappings = dict(zip(le_obj_dict[i].classes_, le_obj_dict[i].transform(le_obj_dict[i].classes_)))\n        for index, k in enumerate(mappings): \n            print(\" {0: <30} ---> {1: <10}\".format(k, mappings[k])) \n            if index == 4: \n                break\n        print(\"\\n\\n\\n\")\n        \n    return df\n    \ndf = label_encoding(df, \"train\")   ","d2bc981c":"df.head()","5099af54":"X = df.drop('price', axis = 1).to_numpy()\nY = df['price'].to_numpy()\n\n# Shapes Of Our Dataset\nprint(X.shape)\nprint(Y.shape)","64bad5cd":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\nscaler.fit(X)\nX = scaler.transform(X)\n\n# Oh! Are you Confused Why I Standardize Y. Just Comment this code and train you model.\n# You will get the answer (Revise Derivation and back Propagation Concepts)\nscaler_Y = StandardScaler()\nscaler_Y.fit(Y.reshape(-1, 1))\nY = scaler_Y.transform(Y.reshape(-1, 1)).flatten()","5e4a405c":"def plot_loss(history):\n    plt.plot(history.history['loss'], label='loss')\n    plt.plot(history.history['val_loss'], label='val_loss')\n    plt.ylim([0, 1])\n    plt.xlabel('Epoch')\n    plt.ylabel('Error [MPG]')\n    plt.legend()\n    plt.grid(True)","f903cc02":"tf.random.set_seed(12)\n\nmodel1 = tf.keras.Sequential()\nmodel1.add(tf.keras.layers.Dense(50, activation = \"relu\"))\nmodel1.add(tf.keras.layers.Dense(10, activation = \"relu\"))\nmodel1.add(tf.keras.layers.AlphaDropout(0.3))\nmodel1.add(tf.keras.layers.Dense(1, activation = \"sigmoid\"))\nmodel1.compile(loss = tf.keras.losses.Huber(), optimizer = tf.keras.optimizers.SGD(), metrics = \"mae\")\n\nhistory = model1.fit(X, Y, epochs = 100, validation_split = 0.1, verbose = 0)\nplot_loss(history)","a48b5693":"transformed_test = TfIDF(test)\ntransformed_test = label_encoding(transformed_test, \"test\")  \n\nX_test = transformed_test.drop('price', axis = 1).to_numpy()\nY_test = transformed_test['price'].to_numpy()\n\n# Shapes Of Our Dataset\nprint(X_test.shape)\nprint(Y_test.shape)\n\nscaler = StandardScaler()\nscaler.fit(X_test)\nX_test = scaler.transform(X_test)\n\nscaler_Y_test = StandardScaler()\nscaler_Y_test.fit(Y_test.reshape(-1, 1))\nY_test = scaler_Y_test.transform(Y_test.reshape(-1, 1)).flatten()\n","1aa65d9d":"predict_model1 = model1.predict(X_test).flatten()\nmodel1.evaluate(X_test, Y_test)\n","bf093202":"# Training\nmodel2 = tf.keras.Sequential()\nmodel2.add(tf.keras.layers.Dense(40, activation = \"relu\"))\nmodel2.add(tf.keras.layers.Dense(1, activation = \"tanh\"))\nmodel2.compile(loss = tf.keras.losses.mae, optimizer = tf.keras.optimizers.Adam(lr = 0.01), metrics = \"mae\")\n\nhistory = model2.fit(X, Y, epochs = 300, validation_split = 0.1, verbose = 0)\nplot_loss(history)\n\npredict_model2 = model2.predict(X_test).flatten()\nmodel2.evaluate(X_test, Y_test)","8dc5c4be":"import plotly.graph_objects as go\n\nindex = np.asarray([i for i in range(len(Y_test))])\n\n# Create traces\nfig = go.Figure()\nfig.add_trace(go.Scatter(y=predict_model1, x= index,\n                    mode='lines',\n                    name='SGD_Optimizer (MODEL 1)'))\n\nfig.add_trace(go.Scatter(y=predict_model2, x= index,\n                    mode='lines',\n                    name='Adam_Optimizer (MODEL 2)'))\n\nfig.add_trace(go.Scatter(y=Y, x=index,\n                    mode='lines',\n                    name='Actual'))\n\n\nfig.update_layout(\n    title=\"Model Performance\",\n    xaxis_title=\"Index\",\n    yaxis_title=\"House Price Prediction\",\n\n)\n\nfig.show() ","bc2a97b3":"### Word Cloud\n\nWordCloud is a technique to show which words are the most frequent among the given text. The first thing you may want to do before using any functions is check out the docstring of the function, and see all required and optional arguments. To do so, type ?function and run it to get all information.","58f68d3b":"# Lets Evaluate Our Model Now\n\n#### Same preprocessing Pipeline\nPerform same perprocessing as you perform in Training Dataset\n","c50b6b3e":"## Getting Insights of data\n\n1. What does values represent.\n2. By Viewing the unique values we can analyzise that what the specefic coloum is trying to say\n","7e8896d2":"### Bag of Words \/ TF-IDF (Street Column)\n\nGet Your concepts clear about bag of words from here [Click Here](https:\/\/www.mygreatlearning.com\/blog\/bag-of-words\/)","ebccb0d4":"# Dataset Insights\n\nFirst of all we will read a csv file which includes dataset. Do Some preprocessing so that we can feed it to the model and after that we will start to train our model and try to achieve the best results.\n\n## Preprocessing Includes\n1. Finding and filling\/droping of null values\n2. Conversion of Categorical objects to numeric (label encoding)\n3. Rescaling numeric values (Standardization or Normalization)\n\n\n### (Get Preprocessing from following links)\n1. (Where to standardize values and where to normalize values)[https:\/\/sebastianraschka.com\/Articles\/2014_about_feature_scaling.html]\n","cd6e7307":"## Now lets Standardize our Data\n\n\n1. Why do we need to standardize our data?\n\n        Standardization comes into picture when features of input data set have large differences between their ranges, or simply when they are measured in different measurement units (e.g., Pounds, Meters, Miles \u2026 etc).\n\n        These differences in the ranges of initial features causes trouble to many machine learning models. For example, for the models that are based on distance computation, if one of the features has a broad range of values, the distance will be governed by this particular feature.\n\n        To illustrate this with an example : say we have a 2-dimensional data set with two features, Height in Meters and Weight in Pounds, that range respectively from [1 to 2] Meters and [10 to 200] Pounds. No matter what distance based model you perform on this data set, the Weight feature will dominate over the Height feature and will have more contribution to the distance computation, just because it has bigger values compared to the Height. So, to prevent this problem, transforming features to comparable scales using standardization is the solution.\n\nGet More Insights from here [Click Here Please](https:\/\/builtin.com\/data-science\/when-and-why-standardize-your-data)\n","d5d2f27b":"## Outlier Removal\n\nLets remove the outliers now, Let me think, Emm! Lets just do a simple thing see the number of values we got from every city.\n\nand remove the cites with less than 5 cities","cd6e1046":"### Categorical Values\n\nIn statistics, a categorical variable (also called qualitative variable) is a variable that can take on one of a limited, and usually fixed, number of possible values, assigning each individual or other unit of observation to a particular group or nominal category on the basis of some qualitative property.[1] In computer science and some branches of mathematics, categorical variables are referred to as enumerations or enumerated types. Commonly (though not in this article), each of the possible values of a categorical variable is referred to as a level. The probability distribution associated with a random categorical variable is called a categorical distribution.\n\nCategorical data is the statistical data type consisting of categorical variables or of data that has been converted into that form, for example as grouped data. More specifically, categorical data may derive from observations made of qualitative data that are summarised as counts or cross tabulations, or from observations of quantitative data grouped within given intervals. Often, purely categorical data are summarised in the form of a contingency table. However, particularly when considering data analysis, it is common to use the term \"categorical data\" to apply to data sets that, while containing some categorical variables, may also contain non-categorical variables.\n\n#### Categorical Values we have\n\n\nColoum Name | DateTime |\n--- | --- |\nprice        | Int |\nbedrooms     | Int |\nbathrooms    | Int |\nsqft_living  | Int |\nsqft_lot     | Int |\nfloors       | Int |\nwaterfront   | Int |\nview         | Categorical |\ncondition    | Int |\nsqft_above   | Int |\nsqft_basement| Int |\nyr_built     | Categorical |\nyr_renovated | Categorical |\nstreet       | Categorical | \ncity         | Categorical |\nstatezip     | Categorical |\ncountry      | Categorical |\n\n\n### FAQ\n\n1. Why Do we convert Categorical Labels to Numeric Labels? [Get you Answer Here!](https:\/\/stats.stackexchange.com\/questions\/134427\/in-practice-why-do-we-convert-categorical-class-labels-to-integers-for-classifi)\n","51406b44":"# Model Performace Graphs","d41119f4":"# Converting Vector To scalars\n\n#### Just a little bit of linear Algebra Here. Hehe Dont be scared I will explain\n\n\n1. Why are we converting Vectors to Scalar?\n        TFID gives us a vector, But we cannot feed vector inside a matrix dataset to a neural netword emm i.e\n\n![Resultant Matrix](https:\/\/github.com\/SohaibAnwaar\/tensorflow-basics\/blob\/main\/extras\/matrix.png?raw=true)\n\n        We cannot feed the middle vector to a neural-net model because they are designed to work on a matrix with scalars not with a matrix with vectors. We can also convert vectors to scalar by just flattening but that will not be a good idea becuase when we flatten a vector it becomes [1,1,2,3,2] means convert them into more columns. So a good idea is to reduce a vector to scalar. We will do some matrix maliplication for this\n\n\n\n\n\n2. How can we convert Vectors to Scalar?\n\n    Multiplying an \ud835\udc5a\u00d7\ud835\udc5b matrix on the right by an \ud835\udc5b\u00d7\ud835\udc5d matrix yields an \ud835\udc5a\u00d7\ud835\udc5d matrix. You have\n    \ud835\udc63\ud835\udc61\ud835\udc34\ud835\udc63\n    which is a 1\u00d7\ud835\udc5b matrix times an \ud835\udc5b\u00d7\ud835\udc5b matrix times an \ud835\udc5b\u00d71 matrix. The result is a 1\u00d71 matrix.\n\n\n![Resultant Matrix](https:\/\/github.com\/SohaibAnwaar\/tensorflow-basics\/blob\/main\/extras\/resultant_matrix.png?raw=true)\n\nThan after this matrix Multiplication our result will be\n\n![Resultant Matrix](https:\/\/github.com\/SohaibAnwaar\/tensorflow-basics\/blob\/main\/extras\/result.png?raw=true)\n\n\nby the way @ sign donates that we are doing matrix multiplication\n\nWe can also use tensorflow for matrix multiplication\n\n![Resultant Matrix](https:\/\/github.com\/SohaibAnwaar\/tensorflow-basics\/blob\/main\/extras\/tf_result.png?raw=true)\n","22273999":"### Checking if null values exists","38e2e8a7":"### Converting Other Categorical Labels\n\nUsing Label Encoding","9eef4a75":"## Model 1 (Basic Model)","37ce3daa":"# Model Optimization\n\nThese improvements are totally random. I just took 1000 of experiments to check which paramter get fit with our data\n\n### Changes\n\n1. Change optimizer to adam (0.0001 improvement) \n2. Change Activation of last layer from sigmoid to tanh (2 units improvement)\n3. Reduce Layers\n4. Increase Number of epochs\n5. Change huber loss to mae\n\n### Lets check the loss fucntion now\n\nThere are 3 types of loss that we mostly use in regression problem\n\n1. Mean Squared Error\n2. Mean Absolute Error\n3. Huber Loss\n\n\n#### Mean Squared Error\nMean Square error is the square of **Actual - Prediction** and than divided by total number of points. So as we are squaring the number means the larger value have larger impact becaused it is been sqaured than than processed. Get So? So we used this error when Larger values are more significant than smaller values. This error is also known as L2 error and more sensitive to the outliers\n\n\n\n\n\n\n![mean square error](https:\/\/github.com\/SohaibAnwaar\/tensorflow-basics\/blob\/main\/extras\/mean_squared_error.svg?raw=true)\n \n* MSE\t=\tmean squared error\n* n\t=\tnumber of data points\n* Y\t=\tobserved values\n* \u00ff   =\tpredicted values\n\n\n### Mean Absolute Error\nMean absolute error Subtract Predicted value from the actual value (Make it positive if it's negative) than divide it by the total number of points we have. We use it when small error as well as large errors equally important for us. Like in House prediction problem we dont really care about cents but about dollars So Larger error are important for us. I hope you get it now. MAE is also known as L1 loss and robust to Outliers.\n\n![Mean Absolute error](https:\/\/github.com\/SohaibAnwaar\/tensorflow-basics\/blob\/main\/extras\/mean_absolute_error.svg?raw=true)\n\n* MAE\t=\tmean absolute error\n* n\t=\tnumber of data points\n* Y\t=\tobserved values\n* x   =\tpredicted values\n\n### Huber Loss\nHuber loss is the combination of Mean Asbolute error and Mean Squared error. Huber loss is less sensitive to outliers in data than the squared error loss. It\u2019s also differentiable at 0. It\u2019s basically absolute error, which becomes quadratic when error is small. How small that error has to be to make it quadratic depends on a hyperparameter, \ud835\udeff (delta), which can be tuned. Huber loss approaches MSE when \ud835\udeff ~ 0 and MAE when \ud835\udeff ~ \u221e (large numbers.) \n\n![Huber Loss](https:\/\/github.com\/SohaibAnwaar\/tensorflow-basics\/blob\/main\/extras\/\/huber.png?raw=true)\n\n\n* n\t=\tnumber of data points\n* Y\t=\tobserved values\n* x   =\tpredicted values\n* \ud835\udeff  = is Hyperparameter which is decided by the user according to the problem","1307157e":"# Tensorflow Simple Regression Practice Notebook\nIn this notebook we are going to get our hands dirty with pure tensorflow code. We will try to make a regression model which we train on **House price prediction** dataset and than get predictions on our train data. [Dataset Path](https:\/\/www.kaggle.com\/shree1992\/housedata)\n\n### Notebook Includes\n\n1. Dataset Preprocessing\n2. Model Training\n3. Model Evaluation\n4. Model Improvement\n","4ecd2ae9":"### Model 1 Evaluation"}}