{"cell_type":{"da463afb":"code","bd15b106":"code","b498d0d1":"code","efaf4691":"code","ef979435":"code","63405b49":"code","e76e3f69":"code","8361b46f":"code","ba15652b":"code","557b2498":"markdown","fd5525df":"markdown","c2540879":"markdown","077e0224":"markdown","cce62be6":"markdown","90055c32":"markdown","835721d3":"markdown","a4778d76":"markdown","3139db74":"markdown"},"source":{"da463afb":"import numpy as np\nimport pandas as pd\nimport json\nimport os\nfrom tqdm.auto import tqdm\nfrom datetime import datetime\nfrom scipy.interpolate import interp1d\nfrom sklearn.preprocessing import RobustScaler\n\nfiles = {}\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        if 'train' in filename:\n            files['train'] = os.path.join(dirname, filename)\n        if 'valid' in filename:\n            files['valid'] = os.path.join(dirname, filename)\n        if 'test' in filename:\n            files['test'] = os.path.join(dirname, filename)","bd15b106":"class2id = {\n    'None': 0,\n    'D0': 1,\n    'D1': 2,\n    'D2': 3,\n    'D3': 4,\n    'D4': 5,\n}\nid2class = {v: k for k, v in class2id.items()}","b498d0d1":"dfs = {\n    k: pd.read_csv(files[k]).set_index(['fips', 'date'])\n    for k in files.keys()\n}","efaf4691":"def interpolate_nans(padata, pkind='linear'):\n    \"\"\"\n    see: https:\/\/stackoverflow.com\/a\/53050216\/2167159\n    \"\"\"\n    aindexes = np.arange(padata.shape[0])\n    agood_indexes, = np.where(np.isfinite(padata))\n    f = interp1d(agood_indexes\n               , padata[agood_indexes]\n               , bounds_error=False\n               , copy=False\n               , fill_value=\"extrapolate\"\n               , kind=pkind)\n    return f(aindexes)","ef979435":"def date_encode(date):\n    if isinstance(date, str):\n        date = datetime.strptime(date, \"%Y-%m-%d\")\n    return (\n        np.sin(2 * np.pi * date.timetuple().tm_yday \/ 366),\n        np.cos(2 * np.pi * date.timetuple().tm_yday \/ 366),\n    )\n\ndef loadXY(\n    df,\n    random_state=42, # keep this at 42\n    window_size=180, # how many days in the past (default\/competition: 180)\n    target_size=6, # how many weeks into the future (default\/competition: 6)\n    fuse_past=True, # add the past drought observations? (default: True)\n    return_fips=False, # return the county identifier (do not use for predictions)\n    encode_season=True, # encode the season using the function above (default: True) \n    use_prev_year=False, # add observations from 1 year prior?\n):\n    df = dfs[df]\n    soil_df = pd.read_csv(\"\/kaggle\/input\/soil_data.csv\")\n    time_data_cols = sorted(\n        [c for c in df.columns if c not in [\"fips\", \"date\", \"score\"]]\n    )\n    static_data_cols = sorted(\n        [c for c in soil_df.columns if c not in [\"soil\", \"lat\", \"lon\"]]\n    )\n    count = 0\n    score_df = df.dropna(subset=[\"score\"])\n    X_static = np.empty((len(df) \/\/ window_size, len(static_data_cols)))\n    X_fips_date = []\n    add_dim = 0\n    if use_prev_year:\n        add_dim += len(time_data_cols)\n    if fuse_past:\n        add_dim += 1\n        if use_prev_year:\n            add_dim += 1\n    if encode_season:\n        add_dim += 2\n    X_time = np.empty(\n        (len(df) \/\/ window_size, window_size, len(time_data_cols) + add_dim)\n    )\n    y_past = np.empty((len(df) \/\/ window_size, window_size))\n    y_target = np.empty((len(df) \/\/ window_size, target_size))\n    if random_state is not None:\n        np.random.seed(random_state)\n    for fips in tqdm(score_df.index.get_level_values(0).unique()):\n        if random_state is not None:\n            start_i = np.random.randint(1, window_size)\n        else:\n            start_i = 1\n        fips_df = df[(df.index.get_level_values(0) == fips)]\n        X = fips_df[time_data_cols].values\n        y = fips_df[\"score\"].values\n        X_s = soil_df[soil_df[\"fips\"] == fips][static_data_cols].values[0]\n        for i in range(start_i, len(y) - (window_size + target_size * 7), window_size):\n            X_fips_date.append((fips, fips_df.index[i : i + window_size][-1]))\n            X_time[count, :, : len(time_data_cols)] = X[i : i + window_size]\n            if use_prev_year:\n                if i < 365 or len(X[i - 365 : i + window_size - 365]) < window_size:\n                    continue\n                X_time[count, :, -len(time_data_cols) :] = X[\n                    i - 365 : i + window_size - 365\n                ]\n            if not fuse_past:\n                y_past[count] = interpolate_nans(y[i : i + window_size])\n            else:\n                X_time[count, :, len(time_data_cols)] = interpolate_nans(\n                    y[i : i + window_size]\n                )\n            if encode_season:\n                enc_dates = [\n                    date_encode(d) for f, d in fips_df.index[i : i + window_size].values\n                ]\n                d_sin, d_cos = [s for s, c in enc_dates], [c for s, c in enc_dates]\n                X_time[count, :, len(time_data_cols) + (add_dim - 2)] = d_sin\n                X_time[count, :, len(time_data_cols) + (add_dim - 2) + 1] = d_cos\n            temp_y = y[i + window_size : i + window_size + target_size * 7]\n            y_target[count] = np.array(temp_y[~np.isnan(temp_y)][:target_size])\n            X_static[count] = X_s\n            count += 1\n    print(f\"loaded {count} samples\")\n    results = [X_static[:count], X_time[:count], y_target[:count]]\n    if not fuse_past:\n        results.append(y_past[:count])\n    if return_fips:\n        results.append(X_fips_date)\n    return results","63405b49":"scaler_dict = {}\nscaler_dict_static = {}\nscaler_dict_past = {}\n\n\ndef normalize(X_static, X_time, y_past=None, fit=False):\n    for index in tqdm(range(X_time.shape[-1])):\n        if fit:\n            scaler_dict[index] = RobustScaler().fit(X_time[:, :, index].reshape(-1, 1))\n        X_time[:, :, index] = (\n            scaler_dict[index]\n            .transform(X_time[:, :, index].reshape(-1, 1))\n            .reshape(-1, X_time.shape[-2])\n        )\n    for index in tqdm(range(X_static.shape[-1])):\n        if fit:\n            scaler_dict_static[index] = RobustScaler().fit(\n                X_static[:, index].reshape(-1, 1)\n            )\n        X_static[:, index] = (\n            scaler_dict_static[index]\n            .transform(X_static[:, index].reshape(-1, 1))\n            .reshape(1, -1)\n        )\n    index = 0\n    if y_past is not None:\n        if fit:\n            scaler_dict_past[index] = RobustScaler().fit(y_past.reshape(-1, 1))\n        y_past[:, :] = (\n            scaler_dict_past[index]\n            .transform(y_past.reshape(-1, 1))\n            .reshape(-1, y_past.shape[-1])\n        )\n        return X_static, X_time, y_past\n    return X_static, X_time","e76e3f69":"X_static_train, X_time_train, y_target_train = loadXY(\"train\")\nprint(\"train shape\", X_time_train.shape)\nX_static_valid, X_time_valid, y_target_valid = loadXY(\"valid\")\nprint(\"validation shape\", X_time_valid.shape)\nX_static_train, X_time_train = normalize(X_static_train, X_time_train, fit=True)\nX_static_valid, X_time_valid = normalize(X_static_valid, X_time_valid)","8361b46f":"batch_size = 128\noutput_weeks = 6","ba15652b":"import torch\nfrom torch.utils.data import TensorDataset, DataLoader\n\ntrain_data = TensorDataset(\n    torch.tensor(X_time_train),\n    torch.tensor(X_static_train),\n    torch.tensor(y_target_train[:, :output_weeks]),\n)\ntrain_loader = DataLoader(\n    train_data, shuffle=True, batch_size=batch_size, drop_last=False\n)\nvalid_data = TensorDataset(\n    torch.tensor(X_time_valid),\n    torch.tensor(X_static_valid),\n    torch.tensor(y_target_valid[:, :output_weeks]),\n)\nvalid_loader = DataLoader(\n    valid_data, shuffle=False, batch_size=batch_size, drop_last=False\n)","557b2498":"Now we'll define a helper method to load the datasets. This just walks through the json and discards the few samples that are corrupted.","fd5525df":"We also add a helper function to interpolate the drought values.","c2540879":"> ## US Drought & Meteorological Data Starter Notebook\nThis notebook will walk you trough loading the data and create a Dummy Classifier, showing a range of F1 scores that correspond to random predictions if given theclass priors.","077e0224":"The following classes exist, ranging from no drought (``None``), to extreme drought (``D4``).\nThis could be treated as a regression, ordinal or classification problem, but for now we will treat it as 5 distinct classes.","cce62be6":"Now we add a helper to normalise the data.","90055c32":"We load the json files for training, validation and testing into the ``files`` dictionary.","835721d3":"Below we use PyTorch to load the data.","a4778d76":"## Loading & Visualizing the Data\nIn this section, we load the training and validation data into numpy arrays and visualize the drought classes and meteorological attributes.","3139db74":"We encode the day of year using sin\/cos and add the data loading function `loadXY`."}}