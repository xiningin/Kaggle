{"cell_type":{"92d88f5e":"code","80a25251":"code","991c2410":"code","3362ac29":"code","8488c0ff":"code","66e78f8d":"code","21dc2ded":"code","ae0310ae":"code","340cd13b":"code","305e0164":"code","7dc5ebe0":"code","ac5b289c":"code","e2968ca5":"code","6c06789d":"code","d7d00546":"code","33095ff6":"code","8095336b":"code","1cd74b3c":"code","3ffae80e":"code","e0101451":"code","cc32577d":"code","d0b8c935":"code","96e9ceed":"code","2158212d":"code","0377d292":"code","727b89dd":"code","cbecc489":"code","2d711f3d":"code","2edf56dd":"code","5029e0aa":"code","d32b5b02":"code","2b7e7dcb":"markdown","48c517d7":"markdown","d6d348cc":"markdown","7a867456":"markdown","b381d47f":"markdown","e23a2913":"markdown","a9336bf9":"markdown","796673dd":"markdown","fbf4fffe":"markdown","d0280340":"markdown","18c4bfee":"markdown","ac8cf397":"markdown","09b9f703":"markdown","4572ac92":"markdown","c8af33aa":"markdown","4e407ac9":"markdown","a2d98a38":"markdown","1dc81c23":"markdown","d0a32079":"markdown","d8f23d61":"markdown","3a2c9afd":"markdown","eb359e29":"markdown","6f81884d":"markdown"},"source":{"92d88f5e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","80a25251":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sqlite3 as sq3\nimport matplotlib\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.model_selection import KFold\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import RandomizedSearchCV","991c2410":"us_car_file_path = pd.read_csv('..\/input\/usa-cers-dataset\/USA_cars_datasets.csv')\nus = us_car_file_path","3362ac29":"conn = sq3.connect('USA_cars_datasets.db')\nus.to_sql('cars', conn, if_exists='replace', index=False)\ne = pd.read_sql_query","8488c0ff":"sns.set(font_scale=2, style='white')","66e78f8d":"print (us.shape)","21dc2ded":"print (us.dtypes)","ae0310ae":"print (us.head())","340cd13b":"print (us.isna().sum())","305e0164":"%%HTML\n<div class='tableauPlaceholder' id='viz1588413042357' style='position: relative'><noscript><a href='#'><img alt=' ' src='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ca&#47;Car_state&#47;Sheet1&#47;1_rss.png' style='border: none' \/><\/a><\/noscript><object class='tableauViz'  style='display:none;'><param name='host_url' value='https%3A%2F%2Fpublic.tableau.com%2F' \/> <param name='embed_code_version' value='3' \/> <param name='site_root' value='' \/><param name='name' value='Car_state&#47;Sheet1' \/><param name='tabs' value='no' \/><param name='toolbar' value='yes' \/><param name='static_image' value='https:&#47;&#47;public.tableau.com&#47;static&#47;images&#47;Ca&#47;Car_state&#47;Sheet1&#47;1.png' \/> <param name='animate_transition' value='yes' \/><param name='display_static_image' value='yes' \/><param name='display_spinner' value='yes' \/><param name='display_overlay' value='yes' \/><param name='display_count' value='yes' \/><param name='filter' value='publish=yes' \/><\/object><\/div>                <script type='text\/javascript'>                    var divElement = document.getElementById('viz1588413042357');                    var vizElement = divElement.getElementsByTagName('object')[0];                    vizElement.style.width='100%';vizElement.style.height=(divElement.offsetWidth*0.75)+'px';                    var scriptElement = document.createElement('script');                    scriptElement.src = 'https:\/\/public.tableau.com\/javascripts\/api\/viz_v1.js';                    vizElement.parentNode.insertBefore(scriptElement, vizElement);                <\/script>","7dc5ebe0":"model_quantity = e('''\n                    select count(brand) as total, brand\n                    from cars\n                    group by brand\n                    order by total desc\n                    ''', conn)\n\nchart = sns.catplot(x='brand', y='total', data=model_quantity, kind='bar', height=10, aspect=3, palette='plasma')\nchart.set_xticklabels(rotation=90)\nchart.set_axis_labels('Vehicle Brand','Number of Vehicles')\nplt.show()","ac5b289c":"price_brand = e('''\n                select avg(price) as average, brand\n                from cars\n                group by brand\n                order by average desc\n                \n                ''', conn)\n\nplt.figure(figsize=(26,12))\nchart = sns.barplot(x='brand',y='average', data=price_brand)\nplt.xticks(rotation=90)\nplt.xlabel('Vehicle Brand')\nplt.ylabel('Average Price [$]')\nplt.title('Average Price of Vehicle By Brand')\nplt.show()","e2968ca5":"cars_comp = e('''\n                select price as price_$, brand, mileage as mileage_km, year\n                from cars\n                where brand in('ford', 'dodge', 'nissan', 'chevrolet')\n                \n                ''', conn)\n\n\n\nchart = sns.pairplot(cars_comp, hue='brand', palette='husl', height=9, aspect=1)\nplt.show()","6c06789d":"zero_price = us.loc[us['price'] == 0]\nprint ('Number of cars set at zero for price =', len(zero_price))","d7d00546":"car_median = (us['price'].median())\nus['price'] = us['price'].replace(0, car_median)","33095ff6":"zero_price = us.loc[us['price'] == 0]\nprint ('Number of cars set at zero for price =',len(zero_price))","8095336b":"price_status = e('''\n                    select title_status, price, brand\n                    from cars\n                    where brand in ('ford', 'dodge', 'nissan', 'chevrolet')\n                    \n                    ''', conn)\n\nchart = sns.catplot(x='brand', y='price', hue='title_status', data=price_status, kind='bar', height=11, aspect=2, palette='BuGn_r', saturation=1)\nchart.set_axis_labels('Vehicle Brand', 'Price [$]')\nplt.show()","1cd74b3c":"us['year'] = [str(i) for i in us['year']]","3ffae80e":"bin_labels = ['1','2','3','4','5','6','7','8','9','10','11','12']\ncut_bins = [0,10000,20000,30000,40000,50000,60000,70000,80000,90000,100000,250000,1000000]\nus['mileage_class'] = pd.cut(us['mileage'], bins=cut_bins, labels=bin_labels)","e0101451":"numeric_features = us.select_dtypes(include=['int64','float64'])\nnumeric_features = numeric_features.drop(['Unnamed: 0', 'lot','price','mileage'], axis=1)\nnumeric_features = list(numeric_features.columns)","cc32577d":"categoric_features = us.select_dtypes(include=['object','category'])\ncategoric_features = categoric_features.drop(['vin','country'], axis=1)\ncategoric_features = list(categoric_features.columns)","d0b8c935":"all_features =  categoric_features","96e9ceed":"X = us.drop(['price'],axis=1)[all_features]\ny = us['price']","2158212d":"X_train, X_test, y_train, y_test = train_test_split(X, y,test_size=0.25, random_state=1)\n\n\ncat_transformer = Pipeline(steps=[\n    (\"imputer\", SimpleImputer(strategy=\"constant\", fill_value=\"Unknown\")),\n    (\"onehot\", OneHotEncoder(handle_unknown='ignore'))])\n\nprocess_features = ColumnTransformer(transformers=[('cat', cat_transformer, categoric_features)])","0377d292":"models = []\nmodels.append(('SVM', svm.SVR()))\nmodels.append(('DTR', tree.DecisionTreeRegressor()))\nmodels.append(('RFR', RandomForestRegressor()))\n\nresults = []\nnames = []\nresults_mean = []\nfor name, model in models:\n    pl = Pipeline([('process', process_features), ('models', model)])\n    kf = KFold(n_splits=10)\n    cv_results = cross_val_score(pl, X_train, y_train, cv=kf, scoring='r2')\n    results.append(cv_results)\n    names.append(name)\n    results_mean.append(cv_results.mean())\n    print('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))","727b89dd":"model_comp = {'Model':names,'Results':results_mean}\nmodel_comp = pd.DataFrame(data=model_comp)\nplt.figure(figsize=(14,7))\nchart = sns.barplot(x='Model',y='Results',data=model_comp)\nplt.show()","cbecc489":"n_estimators = [10,20,50,75,100,200]\nmax_features = ['auto', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(10, 150, num = 10)]\nmax_depth.append(None)\nmin_samples_split = [2, 4, 6, 8, 10]\nmin_samples_leaf = [1, 2, 3, 4]\nbootstrap = [True, False]\n\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}","2d711f3d":"rf_random = RandomizedSearchCV(estimator=RandomForestRegressor(),param_distributions = random_grid, n_iter = 100, \n                               cv = 3, verbose=1, random_state=42)\n\npl_rand = Pipeline([('process', process_features), ('models', rf_random)])\npl_rand.fit(X_train, y_train)\n\nprint (pl_rand['models'].best_params_)","2edf56dd":"rf_random_tuned = RandomForestRegressor(n_estimators=200, min_samples_split=2,min_samples_leaf=1,\n                                        max_features='sqrt',max_depth=87,bootstrap=False)\n\nresults_tuned = []\n\npl_rf = Pipeline([('process', process_features), ('model', rf_random_tuned)])\nkf = KFold(n_splits=10)\ncv_results = cross_val_score(pl_rf, X_train, y_train, cv=kf, scoring='r2')\nresults_tuned.append(cv_results.mean())\nprint('%s: %f (%f)' % (name, cv_results.mean(), cv_results.std()))","5029e0aa":"comp = results_mean + results_tuned\nnames_ = ['SVM','DTR','RFR','RFR_Tuned']\n\n\nresult_params = {'Model':names_,'Score':comp}\nresult_params = pd.DataFrame(data=result_params)\n\nplt.figure(figsize=(14,7))\nchart = sns.barplot(x='Model',y='Score',data=result_params)\nplt.show()","d32b5b02":"pl_rf.fit(X_train, y_train)\nonehot_cols = list(pl_rf.named_steps['process'].\n                      named_transformers_['cat'].\n                      named_steps['onehot'].\n                      get_feature_names(input_features=categoric_features))\n\n\nfeature_list = onehot_cols\n\n\nimp = pl_rf['model'].feature_importances_\n\n\nfeature_imp = zip(feature_list,imp)\n\ntop_features = []\n\nfor i in feature_imp:\n    top_features.append(i)\n    \ntop_features = pd.DataFrame(data=top_features,columns=['Feature','Weight'])\ntop_features = top_features.sort_values(by='Weight',ascending=False).head(n=30)\n\nplt.figure(figsize=(26,12))\nchart = sns.barplot(x='Feature',y='Weight', data=top_features, saturation=1, palette='plasma')\nplt.xticks(rotation=90)\nplt.show()","2b7e7dcb":"# EDA of US cars for auction","48c517d7":"### 3. Categoric Transformations\n\n**Categorical conversions and bin distributions**","d6d348cc":"### 4. Model Evaluations","7a867456":"We can make some estimations about the importance of some features from the figure above.\n1. Model, mileage and condition carry the most weighting in general\n2. The status of a vehicle i.e salvage or clean carry the same weighting, which we would expect, so it's reassuring to see\n3. While some states appear in the top results of the feature evaluation, this could detract from the generalistion of a model if it were included in further optimising the model\n ","b381d47f":"Secondly, the mileage can be distributed to unequal sized bins. The difference in mileage on cars with lower mileages scales much greater than large differences of mileage for cars with a higher amount of mileage recorded.","e23a2913":"Then, instantiate the RSCV by incorporating it into our pipeline","a9336bf9":"Earlier we observed the different datatypes in the dataset, now we make some more subtle observations. \nFirst, the year the car was made is better off as a categorical variable rather than numeric, if we carry on with the model as is when we evaluate feaures at the end of the model we would see that the year the car was made gets an insignificant weighting, as a categorical variable it's importance is scaled better. ","796673dd":"### 2. Data visualisation and exploratory analysis","fbf4fffe":"1. Get encoded column names from columntransformer\n2. Store names in a list and zip together with results from RandomForestRegressor feature_importances_\n3. Evaluate the top results i.e. features with the most weighting","d0280340":"### 1. Brief inspection of data","18c4bfee":"We will use RandomizedSearchCV to evaluate the best parameters for the RFR","ac8cf397":"Using the tuned parameters with RSCV we have managed a ~3% increase in R2 score, which isn't neglible.","09b9f703":"**SQL Connection**","4572ac92":"Fill the cells with the mean value from the price column","c8af33aa":"1. Brief inspection of data\n2. Data visualisations and exploratory analysis\n3. Categoric transformations\n4. Model evaluations\n5. Hyper optimisation of Random Forest\n6. Feature evaluation","4e407ac9":"The vin and the lot number can be dropped from the dataframe as they will cause of overfitting of the model, naturally a reg and a lot number are not contributing factors when determining the price of a car, also, we renamed the mileage according to a class number so we can drop the mileage column. As we have only 8 cars from Canada we will drop country from the dataframe as well.","a2d98a38":"Now we will compare the base model from earlier to the tuned model using the parameters printed above to see if the tuned parameters make a significant difference from the base model.","1dc81c23":"### 6. Feature evaluation","d0a32079":"**Origin of cars by state**","d8f23d61":"**Feature selection**","3a2c9afd":"Investigate the price variation between the classifications 'clean vehicle' and 'salvage insurance'.","eb359e29":"### 5. Hyper optimisation of Random Forest","6f81884d":"We can see from the figures above we have some vehicles set at zero for price."}}