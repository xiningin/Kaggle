{"cell_type":{"a6f693bb":"code","6f9c2bf9":"code","68324655":"code","af9f74c5":"code","3ca4285e":"code","fea80d7a":"code","2c839864":"code","b94d5bf9":"code","79e011b7":"code","ce51844f":"code","c19dec0c":"code","0489b39a":"code","3ea144cf":"code","4e122456":"code","5716976e":"code","c8bcee16":"code","13ad59e0":"code","0250ee7f":"code","464cc172":"code","7bbca9b9":"code","e9f1bfdc":"code","bd154a30":"code","db9ffecb":"code","c095c3d1":"code","425d5ba9":"code","a3ae2492":"code","9a013e1b":"code","b31d972c":"code","3ecb71dd":"code","c652f5fe":"code","3a34b18a":"code","45122b2a":"code","f00ae676":"code","28b16bbb":"code","4c137908":"markdown","cc14669f":"markdown","6afb1546":"markdown","78d368ca":"markdown","c99ab4b0":"markdown","626de2ed":"markdown","6c38f22f":"markdown","30dbdd53":"markdown","1cd348bf":"markdown","6f03b64a":"markdown","19fe89ed":"markdown","c4d0dc36":"markdown","b9dfd7ec":"markdown","6ae4a69e":"markdown","b3b4b0f9":"markdown","3c61707c":"markdown","8824ffca":"markdown","590cc3c7":"markdown","ac5c8fd1":"markdown","45560366":"markdown","a08c09e9":"markdown","b29aef58":"markdown","30087dcf":"markdown","d1e208b9":"markdown","ea4f7fc5":"markdown","24259c5d":"markdown","6ba67712":"markdown","02270fd2":"markdown","fb133b14":"markdown","0926a983":"markdown","7bcd8fc7":"markdown","53e5bd29":"markdown","ca457c55":"markdown"},"source":{"a6f693bb":"import warnings\n\nwith warnings.catch_warnings():\n    warnings.simplefilter(\"ignore\")\n    import numpy as np\n    import pandas as pd\n    from collections import OrderedDict\n    import re\n    import statsmodels\n    from statsmodels.nonparametric.smoothers_lowess import lowess\n    import statsmodels.api as sm\n#     import sklearn\n    import fbprophet\n    from fbprophet import Prophet\n#     import pymc3\n#     import lightgbm as lgb\n#     import xgboost as xgb\n#     import tensorflow as tf\n    import os\n    import datetime as dt\n    import matplotlib.pyplot as plt\n#     import seaborn as sns\n    import bokeh\n    from bokeh.models import CustomJS, ColumnDataSource, Slider, Label, Div, HoverTool, Band, Span, BoxAnnotation\n    from bokeh.plotting import figure\n    from bokeh.palettes import Spectral11\n    import ipywidgets as widgets\n    from IPython.display import display\n    from typing import Union, Dict, List, Callable\n    from contextlib import contextmanager\n    import sys, os\n    import datetime as dt\n\nprint('numpy version: ', np.__version__)\nprint('pandas version: ', pd.__version__)\nprint('statsmodels version: ', statsmodels.__version__)\nprint('prophet version: ', fbprophet.__version__)\n# print('xgboost version: ', xgb.__version__)\n# print('pymc3 version: ', pymc3.__version__)\n# print('tensorflow version: ', tf.__version__)\nprint('ipywidgets version: ', widgets.__version__)\nwarnings.filterwarnings('ignore', module='matplotlib')\nbokeh.io.output_notebook()","6f9c2bf9":"# timer\n# console output supresses\n\nclass timer_gen:\n    \"\"\"Simple timer\"\"\"\n    \n    def __init__(self):\n        self.t0 = dt.datetime.now()\n        self.t1, self.t2 = None, self.t0\n    def __iter__(self):\n        return self\n    def __next__(self):\n        self.t1, self.t2 = self.t2, dt.datetime.now()\n        return \"<timer = {} ({})>\".format(self.t2 - self.t1, self.t2 - self.t0)\n\n@contextmanager\ndef suppress_stdout(on=True):\n    \"\"\"Supress console output\"\"\"\n    \n    if on:\n        with open(os.devnull, \"w\") as devnull:\n            old_stdout = sys.stdout\n            sys.stdout = devnull\n            try:  \n                yield\n            finally:\n                sys.stdout = old_stdout\n    else:\n        yield\n\nclass suppress_stdout_stderr(object):\n    \"\"\"Supress console output 2.0\"\"\"\n    \n    def __init__(self):\n        # Open a pair of null files\n        self.null_fds = [os.open(os.devnull, os.O_RDWR) for x in range(2)]\n        # Save the actual stdout (1) and stderr (2) file descriptors.\n        self.save_fds = (os.dup(1), os.dup(2))\n\n    def __enter__(self):\n        # Assign the null pointers to stdout and stderr.\n        os.dup2(self.null_fds[0], 1)\n        os.dup2(self.null_fds[1], 2)\n\n    def __exit__(self, *_):\n        # Re-assign the real stdout\/stderr back to (1) and (2)\n        os.dup2(self.save_fds[0], 1)\n        os.dup2(self.save_fds[1], 2)\n        # Close the null files\n        os.close(self.null_fds[0])\n        os.close(self.null_fds[1])\n        \nfrom IPython.display import display_html\ndef display_side_by_side(*args):\n    html_str=''\n    for df in args:\n        if type(df) == pd.Series:\n            df = pd.DataFrame(df, columns=['value'])\n        html_str+=df.to_html()\n    display_html(html_str.replace('table','table style=\"display:inline\"'),raw=True)","68324655":"print(\"Working directory: \", os.getcwd())\nprint(\"Input directory: \", os.path.abspath(\"..\/input\"))\nprint(\"Input data: \", os.listdir(\"..\/input\"))\nprint(os.listdir(\"..\/input\/sales-prophet-cv-2017\"))\n\ndf_train = pd.read_csv('..\/input\/demand-forecasting-kernels-only\/train.csv', parse_dates=['date'])\ndf_test = pd.read_csv('..\/input\/demand-forecasting-kernels-only\/test.csv', parse_dates=['date'])\ndf_train.sales = df_train.sales.astype(np.float)\ndisplay_side_by_side(df_train.head(3), df_test.head(3))\nprint('Entries (Train \/ Test) : {} \/ {}'.format(len(df_train), len(df_test)))\ns_train, s_test = df_train.store.unique(), df_test.store.unique()\nprint('Stores (Train \/ Test) : {} - {} \/ {} - {}'.format(s_train[0], s_train[-1], s_test[0], s_test[-1]))\ns_train, s_test = df_train.item.unique(), df_test.item.unique()\nprint('Items (Train \/ Test) : {} - {} \/ {} - {}'.format(s_train[0], s_train[-1], s_test[0], s_test[-1]))\ndates_train, dates_test = df_train.date.unique(), df_test.date.unique()\nprint('Dates (Train \/ Test) : {:.10} - {:.10} \/ {:.10} - {:.10}'.format(dates_train[0], dates_train[-1], dates_test[0], dates_test[-1]))\ndisplay(pd.concat([df_train.isnull().sum().rename('Training NaNs'),\n                   df_train.isnull().sum().rename('Test NaNs')], axis=1))","af9f74c5":"# --- Matplot + Ipywidgets\n\n# %matplotlib notebook\n# %matplotlib inline\n\ndef update_ts_simple(s1_num=1, s2_num=2, i1_num=1, i2_num=2):\n    fig, ax = plt.subplots(4, figsize = (12, 8))\n    df_train.query('store == @s1_num & item == @i1_num').set_index('date')['sales'].resample('W').mean().plot(ax = ax[0])\n    df_train.query('store == @s1_num & item == @i2_num').set_index('date')['sales'].resample('W').mean().plot(ax = ax[1])\n    df_train.query('store == @s1_num & item == @i1_num').set_index('date')['sales'].resample('W').mean().plot(ax = ax[2])\n    df_train.query('store == @s2_num & item == @i2_num').set_index('date')['sales'].resample('W').mean().plot(ax = ax[3])\n    ax[0].set_title('item {} store {}'.format(i1_num, s1_num))\n    ax[1].set_title('item {} store {}'.format(i2_num, s1_num))\n    ax[2].set_title('item {} store {}'.format(i1_num, s2_num))\n    ax[3].set_title('item {} store {}'.format(i2_num, s2_num))\n    fig.suptitle('Sales Volume TS (Ipywidgets)')\n    fig.tight_layout(rect=[0, 0, 1, 0.94])\n    fig.canvas.draw()\n    fig.show()\n    \ns1_slider = widgets.IntSlider(value=1, min=1, max=10, continuous_update=False, description='store A', layout={'width': '2.1in', 'height': '1in'})\ns2_slider = widgets.IntSlider(value=2, min=1, max=10, continuous_update=False, description='store B', layout={'width': '2.1in', 'height': '1in'})\ni1_slider = widgets.IntSlider(value=1, min=1, max=50, continuous_update=False, description='item A', layout={'width': '2.1in', 'height': '1in'})\ni2_slider = widgets.IntSlider(value=2, min=1, max=50, continuous_update=False, description='item B', layout={'width': '2.1in', 'height': '1in'})\nui = widgets.HBox([s1_slider, s2_slider, i1_slider, i2_slider], layout={'min_width': '12in'})\nout = widgets.interactive_output(update_ts_simple, {'s1_num': s1_slider, 's2_num': s2_slider, 'i1_num': i1_slider, 'i2_num': i2_slider})\ndisplay(ui, out)","3ca4285e":"# --- Matplot + Ipywidgets\n# --- Should be a efficient but does NOT work\n\n# store_num = 1\n# a, b = 1, 10\n# sales = df_train.loc[(df_train.store == store_num), ['date', 'sales', 'item']]\n# sales = sales.pivot(index='date', columns='item', values='sales')\n# sales.columns = ['it_{}'.format(x) for x in sales.columns]\n# sales_w = sales.resample('W').sum()\n# # display(sales_w.head(3))\n# source_data = ColumnDataSource(data=sales_w)\n# p1 = figure(plot_width=750, plot_height=150, title='item {}'.format(a), x_axis_type='datetime', tools=\"pan,box_zoom,reset\")\n# line1 = p1.line(x='date', y='it_{}'.format(a), source=source_data)\n# p2 = figure(plot_width=750, plot_height=150, title='item {}'.format(b), x_axis_type='datetime', tools=p1.tools,\n#             x_range=pa.x_range)\n# p2.line('date', 'it_{}'.format(b), source=source_data)\n\n# def update(w):\n#     linea.glyph.line_width = w\n#     linea.glyph.y = 'it_{}'.format(w)\n#     bokeh.io.push_notebook()\n# bokeh.io.show(column(p1, p2), notebook_handle=True)\n# wid = widgets.interactive(update, w=(1,50))\n# wid.children[0].description = \"\"\n# display(wid)","fea80d7a":"# --- Pure Bokeh\n\ni1, i2 = '1_1', '2_1'\ndf_train['it_sa'] = df_train.item.astype(str) + '_' + df_train.store.astype(str) \nsales = df_train.pivot(index='date', columns='it_sa', values='sales').resample('W').mean()\ndf_train.drop(columns=['it_sa'], inplace=True)\n# display(sales_w.head(3))\nsales_source = sales.loc[:, [i1, i2]].copy()\nsource = ColumnDataSource(data=sales_source)\nsource_ref = ColumnDataSource(data=sales)\np1 = figure(plot_width=750, plot_height=150, title=i1, x_axis_type='datetime', tools=\"pan,wheel_zoom,reset\")\nline1 = p1.line(x='date', y=i1, source=source)\np2 = figure(plot_width=750, plot_height=150, title=i2, x_axis_type='datetime', tools=p1.tools,\n            x_range=p1.x_range)\nline2 = p2.line('date', i2, source=source)\np1.add_tools(HoverTool(tooltips=[('sales', '@{}'.format(i1)), ('vs.', '@{}'.format(i2))], \n                       renderers=[line1, line2], mode='vline'))\np2.add_tools(HoverTool(tooltips=[('sales', '@{}'.format(i2)), ('vs.', '@{}'.format(i1))], \n                       renderers=[line1, line2], mode='vline'))\n\nslider_it1 = Slider(start=1, end=50, value=1, step=1, title=\"item a\", callback_policy=\"mouseup\")\nslider_it2 = Slider(start=1, end=50, value=1, step=1, title=\"item b\")\nslider_sa1 = Slider(start=1, end=10, value=1, step=1, title=\"store a\")\nslider_sa2 = Slider(start=1, end=10, value=1, step=1, title=\"store b\")\njs_code = \"\"\"\n    var v = cb_obj.value;\n    var y_old = source.data['{old}'];\n    var y_new = ref.data[{new}];\n    for (var i = 0; i < y_old.length; i++) {\n        y_old[i] = y_new[i];\n    }\n    source.change.emit();\n\"\"\"\ncallback_it1 = CustomJS(args=dict(source=source, ref=source_ref, s=slider_sa1), code=js_code.replace('{old}', i1).replace('{new}', 'v + \"_\" + s.value'))\ncallback_it2 = CustomJS(args=dict(source=source, ref=source_ref, s=slider_sa2), code=js_code.replace('{old}', i2).replace('{new}', 'v + \"_\" + s.value'))\ncallback_sa1 = CustomJS(args=dict(source=source, ref=source_ref, s=slider_it1), code=js_code.replace('{old}', i1).replace('{new}', 's.value + \"_\" + v'))\ncallback_sa2 = CustomJS(args=dict(source=source, ref=source_ref, s=slider_it2), code=js_code.replace('{old}', i2).replace('{new}', 's.value + \"_\" + v'))\nslider_it1.js_on_change('value', callback_it1)\nslider_it2.js_on_change('value', callback_it2)\nslider_sa1.js_on_change('value', callback_sa1)\nslider_sa2.js_on_change('value', callback_sa2)\n\nlayout = bokeh.layouts.column(Div(text='<h4>Sales Volumes TS (Bokeh)<\/h4>'),\n                              bokeh.layouts.row(slider_it1, slider_sa1), p1, \n                              bokeh.layouts.row(slider_it2, slider_sa2), p2)\nbokeh.io.show(layout)","2c839864":"# --- Bokeh + Ipywidgets\n# --- Probably a simpler way to update data source in a Boken plot\n\n# x = np.linspace(0, 2*np.pi, 2000)\n# y = np.sin(x)\n\n# p = figure(title=\"simple line example\", plot_height=300, plot_width=600, y_range=(-5,5))\n# r = p.line(x, y, color=\"#2222aa\", line_width=3)\n\n# def update(f, w=1, A=1, phi=0):\n#     if   f == \"sin\": func = np.sin\n#     elif f == \"cos\": func = np.cos\n#     elif f == \"tan\": func = np.tan\n#     r.data_source.data['y'] = A * func(w * x + phi)\n#     bokeh.io.push_notebook()\n    \n# bokeh.io.show(p, notebook_handle=True)\n# widgets.interact(update, f=[\"sin\", \"cos\", \"tan\"], w=(0,100), A=(1,5), phi=(0, 20, 0.1))","b94d5bf9":"def add_datepart(df, fldname, inplace=False, drop=False):\n    if not inplace: df = df.copy()        \n    fld = df[fldname]\n    fld_dtype = fld.dtype\n    if isinstance(fld_dtype, pd.core.dtypes.dtypes.DatetimeTZDtype):\n        fld_dtype = np.datetime64\n    if not np.issubdtype(fld_dtype, np.datetime64):\n        df[fldname] = fld = pd.to_datetime(fld, infer_datetime_format=True)\n    targ_pre = re.sub('[Dd]ate$', '', fldname)\n    \n    attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear','Weekofyear']\n#     attr = ['Year', 'Month', 'Week', 'Day', 'Dayofweek', 'Dayofyear','Weekofyear',\n#             'Is_month_end', 'Is_month_start', 'Is_quarter_end', 'Is_quarter_start', 'Is_year_end', 'Is_year_start']\n    for n in attr: \n        df[targ_pre + n] = getattr(fld.dt, n.lower())\n    if drop: \n        df.drop(fldname, axis=1, inplace=True)\n    if not inplace: return df \n\ndf_trainext = add_datepart(df_train, 'date', inplace=False)\n# df_testext = add_datepart(df_test, 'date', inplace=False)\ndisplay(df_trainext.head(3))","79e011b7":"df_trainext.groupby('date').mean()['sales'].plot(figsize=(12,3), title='Sales TS (aggregated data)')\n\nfig, ax = plt.subplots(2, 2, figsize=(10, 10))\n_ = pd.pivot_table(df_trainext, values='sales', columns='Year', index='Month').plot(title=\"Yearly seasonality\", ax=ax[0,0])\n_ = pd.pivot_table(df_trainext, values='sales', columns='Month', index='Day').plot(title=\"Monthly seasonality\", ax=ax[0,1])\n_ = pd.pivot_table(df_trainext, values='sales', columns='Year', index='Dayofweek').plot(title=\"Weekly seasonality (by year)\", ax=ax[1,0])\n_ = pd.pivot_table(df_trainext, values='sales', columns='Month', index='Dayofweek').plot(title=\"Weekly seasonality (by month)\", ax=ax[1,1])\nfig.suptitle('Sales seasonality patterns (aggregated data)')\nfig.tight_layout(rect=[0, 0, 1, 0.96])","ce51844f":"_ = pd.pivot_table(df_trainext, values='sales', index='Year').plot(style='-o', title=\"Annual trend (aggregated data)\")","c19dec0c":"df_train_norm = df_trainext.copy()\ndf_train_norm['sales'] \/= df_trainext.groupby(['item', 'store'])['sales'].transform('mean')\n_ = df_train_norm.groupby(['date'])['sales'].std().plot(figsize=(12,3), title='Volatility (across items and stores)')\n_ = (df_train_norm.groupby(['store', 'item'])[['date', 'sales']].rolling(30, on='date').std().groupby(['date']).mean()\n     .plot(figsize=(12,3), title='Volatility (30-d rolling, aggregated data)'))","0489b39a":"freq_season_mapping = {'None':None, 'Weekly': 7, 'Monthly':30, 'Yearly':365}\n\ndef update_stl_decompose(i_num, s_num, seasonality='Yearly', stl_style='additive'):\n    ts = df_train.query('store == @s_num & item == @i_num').set_index('date')['sales']\n    freq = freq_season_mapping[seasonality]\n    \n    fig, ax = plt.subplots(5, 1, figsize=(12,10))\n    decomposition = sm.tsa.seasonal_decompose(ts, model=stl_style, freq=freq)\n    _ = decomposition.observed.plot(ax=ax[0], title='observed')\n    _ = decomposition.trend.plot(ax=ax[1], title='trend')\n    _ = decomposition.seasonal.plot(ax=ax[2], title='seasonal')\n    _ = decomposition.resid.plot(ax=ax[3], title='residual')\n    res = decomposition.resid.values\n    res = res[np.isfinite(res)]\n#     adfuller_stat = statsmodels.tsa.stattools.adfuller(res)\n    ljungbox_stat = statsmodels.stats.diagnostic.acorr_ljungbox(res)\n    statsmodels.graphics.tsaplots.plot_pacf(res, ax=ax[4], lags=40, \n                                           title='residuals pacf; ljung-box p-value = {:.2E} \/ {:.2E}'.format(ljungbox_stat[1][6], \n                                                                                                              ljungbox_stat[1][30]))\n    fig.suptitle('STL decomposition')\n    fig.tight_layout(rect=[0, 0, 1, 0.96])\n    \ns_slider = widgets.IntSlider(min=1, max=10, continuous_update=False, description='store', layout={'width': '2.1in', 'height': '1in'})\ni_slider = widgets.IntSlider(min=1, max=50, continuous_update=False, description='item', layout={'width': '2.1in', 'height': '1in'})\nseason_drop = widgets.Dropdown(value='Yearly', options=['Weekly', 'Monthly', 'Yearly'], description='seasonality', layout={'width': '2.1in'})\nstltype_drop = widgets.Dropdown(value='multiplicative', options=['additive', 'multiplicative'], description='STL type', layout={'width': '2.1in'})\nui = widgets.HBox([s_slider, i_slider, season_drop, stltype_drop], layout={'min_width': '6in', 'max_width': '6in'})\nout = widgets.interactive_output(update_stl_decompose, {'s_num': s_slider, 'i_num': i_slider, \n                                                        'seasonality': season_drop, 'stl_style': stltype_drop})\ndisplay(ui, out)","3ea144cf":"# SMAPE - the official metric for the submission\n\ndef smape(y: Union[np.ndarray, float], yhat: Union[np.ndarray, float], average=True, signed=False) -> float:\n    \"\"\"SMAPE evaluation metric\"\"\"\n    \n    if signed:\n        result = 2. * (yhat - y) \/ (np.abs(y) + np.abs(yhat)) * 100\n    else:\n        result = 2. * np.abs(yhat - y) \/ (np.abs(y) + np.abs(yhat)) * 100\n    if average: return np.mean(result)\n    return result\n\ndef smape_df(df: pd.DataFrame, average=True, signed=False) -> pd.DataFrame:\n    return smape(df.y, df.yhat, average=average, signed=signed)","4e122456":"# -- TODO: Add Ipywidgets?\n\ndef prophet_show(item, store, cutoff_train, cutoff_eval, prophet_kwargs, title, \n                 plot_components=True, display_df=True):\n    ts = (df_train.query('item == @item & store == @store & date < @cutoff_eval')[['date', 'sales']]\n          .rename(columns={'date':'ds', 'sales':'y'})).reset_index(drop=True)\n    ind_train = pd.eval('ts.ds < cutoff_train')\n    ind_eval = ~ ind_train\n    len_train, len_eval = ind_train.sum(), ind_eval.sum()\n    ts_train = ts.loc[ind_train]\n    m = Prophet(**prophet_kwargs)\n    m.fit(ts_train)\n    ts_hat = m.predict(ts).merge(ts[['ds', 'y']], on='ds', how='left')\n    if display_df: display(ts_hat.tail(3))\n\n    df_combined = ts_hat.assign(smape=0, smape_smooth=0)\n    df_combined.smape = smape_df(df_combined, average=False)\n    df_combined.loc[ind_train, 'smape_smooth'] = lowess(df_combined.loc[ind_train, 'smape'], range(len_train), frac=0.03, return_sorted=False)\n    df_combined.loc[ind_eval, 'smape_smooth'] = lowess(df_combined.loc[ind_eval, 'smape'], range(len_eval), frac=0.35, return_sorted=False)\n    smape_in = df_combined.loc[ind_train].smape.mean()\n    smape_oos = df_combined.loc[ind_eval].smape.mean()\n    \n    source = ColumnDataSource(data=df_combined)\n    p = figure(plot_width=750, plot_height=200, title=(\"**{}**     item = {} store = {}     train \/ test = ..{} \/ ..{}\"\n                                                       .format(title, item, store, cutoff_train, cutoff_eval)), \n               x_axis_type='datetime', tools=\"pan,wheel_zoom,reset\")\n    _ = p.line(x='ds', y='yhat', source=source)\n    _ = p.line(x='ds', y='yhat_lower', source=source, line_alpha=0.4)\n    _ = p.line(x='ds', y='yhat_upper', source=source, line_alpha=0.4)\n    _ = p.scatter(x='ds', y='y', source=source, color='black', radius=0.2, radius_dimension='y', alpha=0.4)\n    _ = p.scatter(x='ds', y='y', source=source, color='black', radius=0.2, radius_dimension='y', alpha=0.4)\n       \n    deltas = np.abs(m.params['delta'][0])\n    delta_max = np.max(deltas)\n    df_deltas = pd.DataFrame({'ds': m.changepoints.values, 'delta':deltas, 'delta_scaled':ts_hat.yhat.mean() * deltas \/ delta_max})\n    source2 = ColumnDataSource(df_deltas)\n    cp1 = p.vbar(x='ds', source=source2, width=1, top=ts_hat.yhat.mean(), color='red', alpha=0.2, hover_color='red', hover_alpha=1)\n    cp2 = p.vbar(x='ds', source=source2, width=1.5e+9, top='delta_scaled', color='red', alpha=0.5)\n    p.add_tools(HoverTool(tooltips=[('trend delta', '@delta{.000}')], renderers=[cp2], mode='mouse'))\n    # p.add_layout(Label(x=1e+10, y=10, text='xasfdfsdfsd'))\n    p.add_layout(BoxAnnotation(left=ts_train.ds.iloc[-1], right=ts.ds.iloc[-1]))\n    \n    p2 = figure(plot_width=750, plot_height=100, title=\"SMAPE IS \/ OOS = {:.3f} \/ {:.3f}\".format(smape_in, smape_oos), x_axis_type='datetime', tools=\"\",\n                x_range=p.x_range)\n    sm1 = p2.line(x='ds', y='smape_smooth', source=source, color='green')\n    p2.add_tools(HoverTool(tooltips=[('smape', '@smape')], renderers=[sm1], mode='vline', line_policy='interp'))\n    p2.add_layout(BoxAnnotation(left=ts_train.ds.iloc[-1], right=ts.ds.iloc[-1]))\n    p2.yaxis[0].ticker.desired_num_ticks = 2\n    bokeh.io.show(bokeh.layouts.column(p, p2))\n    \n    if plot_components:\n        _ = m.plot_components(ts_hat, uncertainty=True)\n        fig, ax = plt.subplots(1, 1, figsize=(12, 2))\n#         res = ts_hat.query('ds < @cutoff_train').yhat - ts_train.y\n        res = (df_combined['y'] - df_combined['yhat'])\n#         adfuller_stat = statsmodels.tsa.stattools.adfuller(res.values)\n        ljungbox_stat = statsmodels.stats.diagnostic.acorr_ljungbox(res.values)\n        _ = statsmodels.graphics.tsaplots.plot_pacf(res, lags=40, ax=ax,\n                                                    title='residuals pacf; ljung-box p-value = {:.2E} \/ {:.2E}'.format(ljungbox_stat[1][6], \n                                                                                                                      ljungbox_stat[1][30]))\n    \nprophet_show(item=1, store=1, cutoff_train=\"2017-01-01\", cutoff_eval=\"2017-04-01\",\n             prophet_kwargs={'yearly_seasonality':True, 'weekly_seasonality':True,\n                            'uncertainty_samples':500},\n            title='Prophet')","5716976e":"prophet_cv = pd.read_csv('..\/input\/sales-prophet-cv-2017\/cv_prophet_yk.csv', index_col=[0,1,2,3])\ndisplay(prophet_cv.head(3))\nprint('N rows = ', prophet_cv.shape[0])","c8bcee16":"def show_cv_sampe_agg(data_cv):\n    cv_error = data_cv.groupby(['cv_fold', 'sample']).apply(smape_df)\n    display(pd.DataFrame(cv_error).T)\n    source_index = cv_error[:, 'oos'].index\n    source_in = ColumnDataSource(data=pd.DataFrame(cv_error[:, 'in'],\n                                                   index=source_index,\n                                                   columns=['smape']))\n    source_oos = ColumnDataSource(data=pd.DataFrame(cv_error[:, 'oos'], \n                                                   index=source_index,\n                                                   columns=['smape']))\n    p = figure(plot_width=750, plot_height=250, title=\"Prophet CV SMAPE\", \n               x_range=source_index.values, tools=\"pan,wheel_zoom,reset\")\n    p.xaxis.major_label_orientation = -np.pi \/ 4\n    l1 = p.circle(x='cv_fold', y='smape', source=source_in, legend='in-sample', size=7)\n    l2 = p.circle(x='cv_fold', y='smape', source=source_oos, color='red', legend='out-of-sample', size=6)\n    p.legend.click_policy=\"hide\"\n    p.add_tools(HoverTool(tooltips=[('smape', '@smape')], renderers=[l1, l2], mode='mouse'))\n    bokeh.io.show(p)\n    \nshow_cv_sampe_agg(prophet_cv)","13ad59e0":"def show_cv_smape_by_time(data_cv, cutoff_eval):\n    df = pd.DataFrame({'smape_in': data_cv.query('cv_fold == @cutoff_eval & sample == \"in\"').groupby(['ds']).apply(smape_df),\n                       'smape_oos': data_cv.query('cv_fold == @cutoff_eval & sample == \"oos\"').groupby(['ds']).apply(smape_df)\n                       })\n    df['smape_in_smooth'] = lowess(df.smape_in, range(df.shape[0]), frac=0.015, return_sorted=False)\n    df['smape_oos_smooth'] = lowess(df.smape_oos, range(df.shape[0]), frac=0.15, return_sorted=False)\n    df.index = pd.to_datetime(df.index)\n\n    source = ColumnDataSource(df)\n    p = figure(plot_width=750, plot_height=200, title=\"Prophet SMAPE by time: {} cv fold\".format(cutoff_eval),\n               x_axis_type='datetime', tools=\"pan,wheel_zoom,reset\")\n    sm1 = p.line(x='ds', y='smape_in_smooth', source=source, color='green')\n    sm2 = p.line(x='ds', y='smape_oos_smooth', source=source, color='red')\n    p.add_tools(HoverTool(tooltips=[('smape', '@smape_in')], renderers=[sm1], mode='vline', line_policy='interp'))\n    p.add_tools(HoverTool(tooltips=[('smape', '@smape_oos')], renderers=[sm2], mode='vline', line_policy='interp'))\n    # p.yaxis[0].ticker.desired_num_ticks = 2\n    bokeh.io.show(p)\n    \nshow_cv_smape_by_time(prophet_cv, cutoff_eval=\"2017-04-01\")","0250ee7f":"def show_cv_smape_by_time_oos(data_cv):\n    df = data_cv.query('sample == \"oos\" & ds > \"2015-01-01\"').groupby(['cv_fold', 'ds']).apply(smape_df).unstack(level='cv_fold')\n#     folds = df['cv_fold'].unique()\n#     df['smape_smooth'] = np.nan\n    for col in df.columns:\n        df[col] = lowess(df[col], range(df.shape[0]), frac=0.15, return_sorted=False)\n    df.index = pd.to_datetime(df.index)\n\n    source = ColumnDataSource(df)\n    p = figure(plot_width=750, plot_height=200, title=\"Prophet OOS SMAPE by time\",\n               x_axis_type='datetime', tools=\"pan,wheel_zoom,reset\")\n    for col in df.columns.values:\n        _ = p.line(x='ds', y=col, source=source, color='red')\n        p.add_layout(Span(location=pd.to_datetime(col).value \/ 1e6,\n                          dimension='height', line_color='black',\n                          line_dash='dashed', line_width=1))\n    for col in np.append(df.columns.values, \"2015-01-01\"):\n        p.add_layout(Span(location=pd.to_datetime(col).value \/ 1e6,\n                          dimension='height', line_color='black',\n                          line_dash='dashed', line_width=1))\n    # p.yaxis[0].ticker.desired_num_ticks = 2\n    p.add_tools(HoverTool(tooltips=[('smape', '$y')], mode='vline', line_policy='interp'))\n    bokeh.io.show(p)\n    \nshow_cv_smape_by_time_oos(prophet_cv)","464cc172":"def plot_data_cv_example(df, cv_fold_idx, num_idx, ax):\n        ts = df.loc[(cv_fold_idx, 'in', num_idx, slice(None)), 'y'].reset_index(level=[0,1,2], drop=True)\n        ts = ts[pd.notnull(ts)]\n        ts.index = pd.to_datetime(ts.index)\n        ax.plot(ts.resample('w').mean())\n        ax.set_title(num_idx)\n\ndef show_cv_best_worst(data_cv, cutoff_eval):\n    cv_error_num = data_cv.groupby(['sample', 'num']).apply(smape_df)\n    cv_error_num_oos = cv_error_num.loc['oos', :].sort_values()\n    cv_error_num_in = cv_error_num.loc['in', :].sort_values()\n    print(\"-------- Best: Lowest SMAPE ---------\")\n    display_side_by_side(cv_error_num_in.head(), cv_error_num_oos.head())\n    print(\"-------- Worst: Highest SMAPE ---------\")\n    display_side_by_side(cv_error_num_in.tail(), cv_error_num_oos.tail())\n\n    cv_fold_idx = cutoff_eval\n    fig, ax = plt.subplots(3, 1, figsize=(10, 5))\n    plot_data_cv_example(df=data_cv, cv_fold_idx=cv_fold_idx, num_idx=cv_error_num_oos.index[0][1], ax=ax[0])\n    plot_data_cv_example(df=data_cv, cv_fold_idx=cv_fold_idx, num_idx=cv_error_num_oos.index[1][1], ax=ax[1])\n    plot_data_cv_example(df=data_cv, cv_fold_idx=cv_fold_idx, num_idx=cv_error_num_oos.index[2][1], ax=ax[2])\n    fig.suptitle('Best: Lowest OOS SMAPE')\n    fig.tight_layout(rect=[0, 0, 1, 0.94])\n\n    fig, ax = plt.subplots(3, 1, figsize=(10, 5))\n    plot_data_cv_example(df=data_cv, cv_fold_idx=cv_fold_idx, num_idx=cv_error_num_oos.index[-1][1], ax=ax[0])\n    plot_data_cv_example(df=data_cv, cv_fold_idx=cv_fold_idx, num_idx=cv_error_num_oos.index[-2][1], ax=ax[1])\n    plot_data_cv_example(df=data_cv, cv_fold_idx=cv_fold_idx, num_idx=cv_error_num_oos.index[-3][1], ax=ax[2])\n    fig.suptitle('Worst: Highest data_cv SMAPE')\n    fig.tight_layout(rect=[0, 0, 1, 0.94])\n    \nshow_cv_best_worst(prophet_cv, cutoff_eval=\"2017-04-01\")","7bbca9b9":"holidays_us_raw = pd.read_csv('..\/input\/federal-holidays-usa-19662020\/usholidays.csv')\ndisplay(holidays_us_raw.head(3))\nholidays_us = pd.DataFrame({\n    'holiday':'US',\n    'ds':holidays_us_raw.Date, \n    'lower_window': -1,\n    'upper_window': 0})\ndisplay(holidays_us.head(3))","e9f1bfdc":"prophet_show(item=1, store=1, cutoff_train=\"2017-01-01\", cutoff_eval=\"2017-04-01\",\n             prophet_kwargs={'yearly_seasonality':True, 'weekly_seasonality':True,\n                             'holidays':holidays_us, 'uncertainty_samples':500},\n            title='Prophet (US holidays)', plot_components=True)","bd154a30":"# -- TODO: add CV plots \n","db9ffecb":"# sales_total_average = df_train['sales'].mean()\n# df_train_norm = df_train.copy()\n# df_train_norm['sales'] \/= df_train.groupby(['item', 'store'])['sales'].transform('mean')\n\nfig, ax = plt.subplots(3, 2, figsize=(12, 16))\n_ = df_train_norm.groupby(['item', 'Dayofweek'])['sales'].mean().unstack('item').plot(title='Weekly sales by item', legend=False, ax=ax[0,0])\n_ = df_train_norm.groupby(['store', 'Dayofweek'])['sales'].mean().unstack('store').plot(title='Weekly sales by store', legend=False, ax=ax[1,0])\n_ = df_train_norm.groupby(['item', 'store', 'Dayofweek'])['sales'].mean().unstack(['item', 'store']).plot(title='Weekly sales by item and store', legend=False, ax=ax[2,0])\n_ = df_train_norm.groupby(['item', 'Year'])['sales'].mean().unstack('item').plot(title='Yearly sales by item', legend=False, ax=ax[0,1])\n_ = df_train_norm.groupby(['store', 'Year'])['sales'].mean().unstack('store').plot(title='Yearly sales by store', legend=False, ax=ax[1,1])\n_ = df_train_norm.groupby(['item', 'store', 'Year'])['sales'].mean().unstack(['item', 'store']).plot(title='Yearly sales by item and store', legend=False, ax=ax[2,1])\nfig.suptitle('This is how deep the rabbit hole goes')\nfig.tight_layout(rect=[0, 0, 1, 0.96])","c095c3d1":"annual_sales_avg = pd.pivot_table(df_trainext, values='sales', index='Year')\nx_range = np.linspace(2013, 2018, 50)\nannual_growth_linear = np.poly1d(np.polyfit(annual_sales_avg.index.values, annual_sales_avg['sales'].values, 1))\nannual_growth_quadratic = np.poly1d(np.polyfit(annual_sales_avg.index.values, annual_sales_avg['sales'].values, 2))\nannual_growth_linear_ll = (statsmodels.nonparametric\n                           .kernel_regression.KernelReg(annual_sales_avg['sales'].values,\n                                                        annual_sales_avg.index.values, \n                                                        'c', bw=[1]))\n_ = annual_sales_avg.plot(style='o', title=\"Average annual trend\", figsize=(8, 6))\n_ = plt.plot(x_range, annual_growth_linear(x_range), '--', label='linear')\n_ = plt.plot(x_range, annual_growth_linear_ll.fit(x_range)[0], '-.', label='local linear')\n_ = plt.plot(x_range, annual_growth_quadratic(x_range), '.', label='quadratic')\n_ = plt.legend()","425d5ba9":"# Just for the reference, here is the code of the \"dumb\" model\nfrom abc import abstractmethod, ABC\n\nclass DumbBase(ABC):\n    \"\"\"Dumb base model\"\"\"\n    \n    name = \"dumb_base\"\n    \n    def __init__(self, growth='linear', fit_window_years=None):\n        self.growth = growth\n        self.fit_window_years = fit_window_years\n        self.verbose = True\n    \n    @staticmethod\n    def expand_data(data):\n        data_exp = data.copy()\n        data_exp['day'] = data['date'].apply(lambda x: x.day)\n        data_exp['month'] = data['date'].apply(lambda x: x.month)\n        data_exp['year'] = data['date'].apply(lambda x: x.year)\n        data_exp['dayofweek'] = data['date'].apply(lambda x: x.dayofweek)\n        data_exp['weekofyear'] = data['date'].apply(lambda x: x.weekofyear)\n        data_exp['dayofyear'] = data['date'].apply(lambda x: x.dayofyear)\n        return data_exp\n    \n    def _fit_annual_sales(self):\n        if isinstance(self.growth, pd.DataFrame):\n            self._annual_sales = lambda x: self.growth.loc[x, 'sales']\n        \n        else:\n            print('Dumb fit: functional annual growth')\n            year_table = pd.pivot_table(self.data, index='year', values='sales', aggfunc=np.mean)\n            years = year_table.index.values\n            annual_sales_avg = year_table.values.squeeze()\n\n            if growth == 'linear': \n                self._annual_sales = np.poly1d(np.polyfit(years, annual_sales_avg, 1))\n            elif growth == 'quadratic': \n                self._annual_sales = np.poly1d(np.polyfit(years, annual_sales_avg, 2))\n            else:\n                raise KeyError\n    \n    @abstractmethod\n    def _fit_base_seasonality(self):\n        pass\n    \n    def fit(self, data):\n        if 'year' in data.columns:\n            self.data = data.copy()\n        else:\n            print('Dumb fit: Expand data')\n            self.data = self.expand_data(data)\n        \n        if self.fit_window_years is not None:\n            date_max = self.data['date'].max()\n            date_min = date_max.replace(year=date_max.year-self.fit_window_years)\n            self.data = self.data.query('date > @date_min')\n            \n        self.data['sales'] \/= self.data['sales'].mean()\n        self._fit_base_seasonality()\n        self._fit_annual_sales()        \n        \n    @abstractmethod\n    def _predict_base_seasonality(self, item, store, date):\n        pass\n    \n    def _predict_annual_sales(self, year):\n        return self._annual_sales(year)\n        \n    def predict(self, data):\n        data = data.assign(sales_hat=.001)\n        with suppress_stdout(not self.verbose):\n            timer = timer_gen()\n            count = 1\n            for i, row in data.iterrows():\n                if count % 100000 == 0: print(\"dumb predict {} {}\".format(count, next(timer), end=' | '))\n                date, item, store = row['date'], row['item'], row['store']\n                pred_sales = self._predict_base_seasonality(item, store, date) * self._predict_annual_sales(date.year)\n                data.at[i, 'sales_hat'] = pred_sales\n                count += 1\n        return data\n    \n    \nclass DumbOriginal(DumbBase):\n    \"\"\"\n    Original Dumb model\n    \n    sales = base(item, store) * s(dayofweek) * s(month) * trend(year)\n    \"\"\"\n    \n    name = \"dumb_original\"\n    \n    def _fit_base_seasonality(self):\n        self.store_item_table = pd.pivot_table(self.data, index='store', columns='item',\n                                               values='sales', aggfunc=np.mean)\n        self.month_table = pd.pivot_table(self.data, index='month', values='sales', aggfunc=np.mean)\n        self.dow_table = pd.pivot_table(self.data, index='dayofweek', values='sales', aggfunc=np.mean)\n        \n    def _predict_base_seasonality(self, item, store, date):\n        dow, month, year = date.dayofweek, date.month, date.year\n        base_sales = self.store_item_table.at[store, item]\n        seasonal_sales = self.month_table.at[month, 'sales'] * self.dow_table.at[dow, 'sales']\n        return base_sales * seasonal_sales    ","a3ae2492":"dumb_hc_cv = pd.read_csv('..\/input\/sales-dumb-all-cv2017\/cv_dumb_hc_oos_yk.csv', index_col=[0,1,2,3])\nprint('N rows = ', dumb_hc_cv.shape[0])\ndisplay(dumb_hc_cv.head(3))","9a013e1b":"dumb_linear_cv = pd.read_csv('..\/input\/sales-dumb-all-cv2017\/cv_dumb_linear_oos_yk.csv', index_col=[0,1,2,3])\ndisplay(dumb_linear_cv.head(3))\ndumb_quadratic_cv = pd.read_csv('..\/input\/sales-dumb-all-cv2017\/cv_dumb_quadratic_oos_yk.csv', index_col=[0,1,2,3])\ndisplay(dumb_quadratic_cv.head(3))","b31d972c":"def create_df_cv_smape_arrg(df_cv_dict):\n    df_smape_aggr = pd.concat((val.groupby(['cv_fold']).apply(smape_df).rename(key)\n                               for key, val in df_cv_dict.items()), axis=1)\n    df_smape_aggr.index = [x - pd.Timedelta(days=1) for x in pd.to_datetime(df_smape_aggr.index)]\n    return df_smape_aggr\n    \ndf_dumb_smape_aggr = create_df_cv_smape_arrg(OrderedDict((\n    ('Dumb HC', dumb_hc_cv),\n    ('Dumb Linear', dumb_linear_cv),\n    ('Dumb Quadratic', dumb_quadratic_cv),\n)))\n_ = df_dumb_smape_aggr.plot(figsize=(10, 3), style='o--', title='Dumb CV SMAPE')","3ecb71dd":"def show_cv_smape_by_time_oos_2(data_cv, m_name):\n    data_gb = data_cv.groupby(['cv_fold', 'ds'])\n    df_u = data_gb.apply(smape_df, signed=False).unstack(level='cv_fold')\n    df_s = data_gb.apply(smape_df, signed=True).unstack(level='cv_fold')\n    cols_original = df_u.columns.copy()\n    df_u.columns = [x + '_u' for x in cols_original]\n    df_s.columns = [x + '_s' for x in cols_original]\n    df = pd.concat([df_u, df_s], axis=1)\n    for col in df.columns:\n        df[col] = lowess(df[col], range(df.shape[0]), frac=0.15, return_sorted=False)\n    df.index = pd.to_datetime(df.index)\n\n    source = ColumnDataSource(df)\n    p_u = figure(plot_width=750, plot_height=200, title=\"{} OOS SMAPE by time\".format(m_name),\n                 x_axis_type='datetime', tools=\"pan,wheel_zoom,reset\")\n    for col in df_u.columns:\n        _ = p_u.line(x='ds', y=col, source=source, color='red')\n        \n    p_s = figure(plot_width=750, plot_height=200, title=\"{} OOS SMAPE by time (signed)\".format(m_name),\n                 x_axis_type='datetime', tools=\"pan,wheel_zoom,reset\", x_range=p_u.x_range)\n    for col in df_s.columns:\n        _ = p_s.line(x='ds', y=col, source=source, color='blue')\n        \n    for col in np.append(cols_original.values, \"2016-01-01\"):\n        p_s.add_layout(Span(location=pd.to_datetime(col).value \/ 1e6,\n                            dimension='height', line_color='black',\n                            line_dash='dashed', line_width=1))\n        p_u.add_layout(Span(location=pd.to_datetime(col).value \/ 1e6,\n                            dimension='height', line_color='black',\n                            line_dash='dashed', line_width=1))\n    p_s.add_tools(HoverTool(tooltips=[('smape', '$y')], mode='vline', line_policy='interp'))    \n    p_u.add_tools(HoverTool(tooltips=[('smape', '$y')], mode='vline', line_policy='interp'))\n    \n    bokeh.io.show(bokeh.layouts.column(p_u, p_s))\n    \nshow_cv_smape_by_time_oos_2(dumb_hc_cv, 'Dumb HC')\nshow_cv_smape_by_time_oos_2(dumb_linear_cv, 'Dumb Linear')\nshow_cv_smape_by_time_oos_2(dumb_quadratic_cv, 'Dumb Quadratic')","c652f5fe":"def plot_data_cv_example_2(df, item, store, ax):\n    ts = df.loc[(slice(None), slice(None), item, store), ['y', 'yhat']].reset_index(level=[2,3], drop=True).unstack(level=['cv_fold'])\n    ts.index = pd.to_datetime(ts.index)\n    (ts['yhat'] - ts['y']).abs().resample('w').mean().plot(ax=ax, title=\"{}_{}\".format(item, store), legend=False)\n\ndef show_cv_best_worst_2(data_cv, cutoff_eval):\n    smape_itsa = data_cv.groupby(['item', 'store']).apply(smape_df).sort_values()\n    print(\"-------- Best: Lowest SMAPE OOS ---------\")\n    display_side_by_side(smape_itsa.head())\n    print(\"-------- Worst: Highest SMAPE OOS ---------\")\n    display_side_by_side(smape_itsa.tail())\n\n    cv_fold_idx = cutoff_eval\n    fig, ax = plt.subplots(3, 1, figsize=(12, 6))\n    plot_data_cv_example_2(df=data_cv, item=smape_itsa.index[0][0], store=smape_itsa.index[0][1], ax=ax[0])\n    plot_data_cv_example_2(df=data_cv, item=smape_itsa.index[1][0], store=smape_itsa.index[1][1], ax=ax[1])\n    plot_data_cv_example_2(df=data_cv, item=smape_itsa.index[2][0], store=smape_itsa.index[2][1], ax=ax[2])\n    fig.suptitle('Best: Lowest OOS SMAPE')\n    fig.tight_layout(rect=[0, 0, 1, 0.94])\n\n    fig, ax = plt.subplots(3, 1, figsize=(12, 6))\n    plot_data_cv_example_2(df=data_cv, item=smape_itsa.index[-1][0], store=smape_itsa.index[-1][1], ax=ax[0])\n    plot_data_cv_example_2(df=data_cv, item=smape_itsa.index[-2][0], store=smape_itsa.index[-2][1], ax=ax[1])\n    plot_data_cv_example_2(df=data_cv, item=smape_itsa.index[-3][0], store=smape_itsa.index[-3][1], ax=ax[2])\n    fig.suptitle('Worst: Highest OOS SMAPE')\n    fig.tight_layout(rect=[0, 0, 1, 0.94])\n    \nshow_cv_best_worst_2(dumb_hc_cv, cutoff_eval=\"2017-04-01\")","3a34b18a":"# Just for the reference, here is the code of the seasonal variations\n\nclass DumbItemDayofweek(DumbBase):\n    \"\"\"\n    sales = base(store) * s(dayofweek, item) * s(month) * trend(year)\n    \"\"\"\n    \n    name = \"dumb_item_dow\"\n    \n    def _fit_base_seasonality(self):\n        self.store_table = pd.pivot_table(self.data, index='store', values='sales', aggfunc=np.mean)\n\n        self.month_table = pd.pivot_table(self.data, index='month', values='sales', aggfunc=np.mean)\n\n        self.dow_item_table = pd.pivot_table(self.data, index='dayofweek', columns='item',\n                                             values='sales', aggfunc=np.mean)\n        \n    def _predict_base_seasonality(self, item, store, date):\n        dow, month, year = date.dayofweek, date.month, date.year\n        base_sales = self.store_table.at[store, 'sales']\n        seasonal_sales = self.month_table.at[month, 'sales'] * self.dow_item_table.at[dow, item]\n        return base_sales * seasonal_sales\n    \n    \nclass DumbStoreDayofweek(DumbBase):\n    \"\"\"\n    sales = base(item) * s(dayofweek, store) * s(month) * trend(year)\n    \"\"\"\n    \n    name = \"dumb_store_dow\"\n    \n    def _fit_base_seasonality(self):\n        self.item_table = pd.pivot_table(self.data, index='item', values='sales', aggfunc=np.mean)\n\n        self.month_table = pd.pivot_table(self.data, index='month', values='sales', aggfunc=np.mean)\n\n        self.dow_store_table = pd.pivot_table(self.data, index='dayofweek', columns='store',\n                                             values='sales', aggfunc=np.mean)\n        \n    def _predict_base_seasonality(self, item, store, date):\n        dow, month, year = date.dayofweek, date.month, date.year\n        base_sales = self.item_table.at[item, 'sales']\n        seasonal_sales = self.month_table.at[month, 'sales'] * self.dow_store_table.at[dow, store]\n        return base_sales * seasonal_sales\n    \n    \nclass DumbItemStoreDayofweek(DumbBase):\n    \"\"\"\n    sales = base() * s(dayofweek, item, store) * s(month) * trend(year)\n    \"\"\"\n    \n    name = \"dumb_item_store_dow\"\n    \n    def _fit_base_seasonality(self):\n        self.base_scalar = 1.\n\n        self.month_table = pd.pivot_table(self.data, index='month', values='sales', aggfunc=np.mean)\n\n        self.dow_item_store_table = pd.pivot_table(self.data, index=['item', 'store'],\n                                                   columns='dayofweek',\n                                                   values='sales', aggfunc=np.mean)\n        \n    def _predict_base_seasonality(self, item, store, date):\n        dow, month, year = date.dayofweek, date.month, date.year\n        base_sales = self.base_scalar\n        seasonal_sales = self.month_table.at[month, 'sales'] * self.dow_item_store_table.at[(item, store), dow]\n        return base_sales * seasonal_sales","45122b2a":"# --- Tweaking item-store dependencies\n\ndumb_original_cv = pd.read_csv('..\/input\/sales-dumb-all-alt-cv2017\/cv_dumb_original.csv', index_col=[0,1,2,3])\ndumb_item_dow_cv = pd.read_csv('..\/input\/sales-dumb-all-alt-cv2017\/cv_dumb_item_dow.csv', index_col=[0,1,2,3])\ndumb_store_dow_cv = pd.read_csv('..\/input\/sales-dumb-all-alt-cv2017\/cv_dumb_store_dow.csv', index_col=[0,1,2,3])\ndumb_item_store_dow_cv = pd.read_csv('..\/input\/sales-dumb-all-alt-cv2017\/cv_dumb_item_store_dow.csv', index_col=[0,1,2,3])\nprint('N rows = ', dumb_original_cv.shape[0])","f00ae676":"df_dumb_smape_aggr = create_df_cv_smape_arrg(OrderedDict((\n    ('Dumb HC Original', dumb_original_cv),\n    ('Dumb HC Item DOW', dumb_item_dow_cv),\n    ('Dumb HC Store DOW', dumb_store_dow_cv),\n    ('Dumb HC Item-Store DOW', dumb_item_store_dow_cv),\n)))\n_ = df_dumb_smape_aggr.plot(figsize=(10, 3), style='o--', title='Dumb HC CV SMAPE')\n\ndf_dumb_smape_aggr['Dumb HC Item DOW'] -= df_dumb_smape_aggr['Dumb HC Original']\ndf_dumb_smape_aggr['Dumb HC Store DOW'] -= df_dumb_smape_aggr['Dumb HC Original']\ndf_dumb_smape_aggr['Dumb HC Item-Store DOW'] -= df_dumb_smape_aggr['Dumb HC Original']\ndf_dumb_smape_aggr.drop(columns=['Dumb HC Original'], inplace=True)\n_ = df_dumb_smape_aggr.plot(figsize=(10, 3), style='o--', title='Difference with Dumb HC Original')","28b16bbb":"show_cv_smape_by_time_oos_2(dumb_original_cv, 'Dumb HC Original')\nshow_cv_smape_by_time_oos_2(dumb_item_dow_cv, 'Dumb HC Item DOW')\nshow_cv_smape_by_time_oos_2(dumb_store_dow_cv, 'Dumb HC Store DOW')\nshow_cv_smape_by_time_oos_2(dumb_item_store_dow_cv, 'Dumb HC Item-Store DOW')","4c137908":"## Load data\nThe data is given in csv format. The training set has less than one million rows and a size of just a few megabates, so no fancy big data tech is required. We also use model predictions pre-computed in other kernels to speed-up this notebook's exectution time.\n\nSales values are given as integers, so one unit probably corresponds to one item. Safety is our number one priority: to avoid any accidental number rounding in the future, we transform sales to float. For our submission, we do the opposite: we will round the predicted sales.\n\nThe data seems to be clean. If we missed any quirks in data, we will catch them during data analysis. Wonderful, we are done >>>>>> we're ready to move to EDA! In this kernel, we'll only do a basic one but with some nice looking interactive plots.","cc14669f":"## Tuning hyper-parameters by cross-validation\n\nSo far, we used Prophet with the default hyper-parameters that define seasonal and trend flexibility. This model gives a public score of 16.9 and a private score of 14.1.\n\nI tuned the hyper-parameters by cross-validation on 2017 Q1 in a separate notebook. The resulting model has a less flexible yearly seasonality and does not account for US holidays. That model gives a public score of 15.4 and a private score of 13.9. That's better... but the performance is still far from top models. At the moment of testing Prophet, top public scores had already been below 14.0.\n\nSo, what do we do now? We could of course work a little more on tuning Prophet hyper-parameters. Instead, we'll try a new approach.","6afb1546":"## Volatility\n\nFor sales data we are like to observe periods with high and low volatilites. For example, periods before holidays or certain seasons (depending on what kind of items we have) may have higher customer turn-over.\n\nWe check standard deviation of sales volumes across all stores and items, computed independantly at every day (and normalized by average sales volume for every item and store). This plot is almost exactly the same as the plot with aggregated sales above: meaning, periods with higher sales have higher dispersion across different items and stores. The match between these two plot is really good, but standard deviation is \"flatter\". It indicates that \"noise\" in sales data is somewhere between multiplicative and additive. Let's keep that in mind.\n\n30-day rolling volatility shows a similar picture. No new insights, except that this plot seems to be consistent with what we've seen so far. ","78d368ca":"> \\---------------------------\n>\n> Stay awesome Kaggle,\n>\n> Mysterious Ben\n>\n> \\---------------------------","c99ab4b0":"## Math\n\nSo, the data is not just synthetic. Basically, a sales volume time series for any item and store seems to be generated approximately as \n\n$TS(item, sales, date) = TS_{base}(date) * c(item, sales) * \\eta_1(item, sales, date) + \\eta_2(item, sales, date)$,\n\nwhere \n- $TS_{base}$ is a fixed time series common for all items and stores,\n- $c(item, sales)$ is a unique time series multiplier,\n- $\\eta_1$ and $\\eta_2$ are white noise variables.\n\nNow we need to model $TS_{base}$. Judging by what we've seen so far, it can be expressed as\n\n$TS_{base}(date) = trend(year)*seasonality_{weekly}(weekday)*seasonality_{yearly}(month)$.\n\nThus, to model sales volumes we need to estimate 4 functions: $c(item, sales)$, $trend(year)$, $seasonality_{weekly}(weekday)$ and $seasonality_{yearly}(month)$. Let's do it in the \"dumbest\" way possible: by averaging data.\n\n$c(item, sales) = average_{date} \\, TS(item, sales, date)$\n\n$trend(year) = \\frac{average_{item, store, year} \\, TS(item, sales, date)}{average_{total} \\, TS}$\n\n$seasonality_{weekly}(weekday) = \\frac{average_{item, store, weekday} \\, TS(item, sales, date)}{average_{total} \\, TS}$\n\n$seasonality_{yearly}(month) = \\frac{average_{item, store, month} \\, TS(item, sales, date)}{average_{total} \\, TS}$\n\nNow to make predictions, we simply look up values of $c$, $seasonality$ and $trend$. Okay, there is still one small issue: we have to extrapolate our trend.\n","626de2ed":"Now, let's test Facebook Prophet package. Like statsmodels STL, Prophet also uses a trend-seasonality decomposition; however, it is more flexible and allows making predictions. Under the hood, Prophet solves a state space model using the Bayesian framework of [Stan](http:\/\/mc-stan.org\/).\n\n**Pros:**\n- Quite easy to use\n- Allows specifying multiple seasonalities\n- Allows specifying special events\n- Can compute quick MAP and slow but accurate Bayesian estimates\n- Provides methods for basic plotting out of the box\n\n**Cons:**\n- Can only treat univariate time series\n- Assumes Gaussian priors\n- Does not provide methods to tune hyper-parameters out of the box (for example, seasonality and trend flexibility priors)\n- Outputs useless warning messages in the console - and I found no way to hide them","6c38f22f":"<a id=\"SectionProphet\"><\/a>\n# Prophet","30dbdd53":"## Problem Description\n\nThe training dataset contains 5 years of daily sales volumes of 50 items in 10 stores (500 time series in total, from 2013 to 2017). No additional features are available, only historical sales volumes.\n\nThe goal is to predict sales volumes of all items in all stores in the first quarter of 2018. The evaluation metric is [SMAPE](https:\/\/en.wikipedia.org\/wiki\/Symmetric_mean_absolute_percentage_error):\n\n$$SMAPE = average_i \\frac{|y_i-\\hat{y_i}|}{\\frac{1}{2}(y_i+\\hat{y_i})} \\cdot 100\\%.$$\n\nSo, we have a time series forecasting problem. A cool thing about time series problems is that our data is likely to be non-stationary: sales probably have a trend and a seasonal patten of some kind. The trend, seasonality and even the whole data distribution may change, depending on the time period. In addition, we are dealing with a multivariate time series. Sales of items in one store or of one item in different stores could be correlated - we should use it to fit our model more accurately.\n\nThe problem is described >>>>> our next stop is data cleaning!","1cd348bf":"Like before, we can check best and worst performers across items and stores. Okay, I don't see any obvious pattern here and the time is limited... Let's move on!","6f03b64a":"Let's get back to our data analysis. If you remember, we've seen quite a few quirks:\n- The data has no apperent outliers at all;\n- Visually, shapes of sales volume time series across all items and stores look suspiciously regular and similar to each other;\n- Weekly and yearly seasonal patterns are very stable for every store-item combination, and trend is relatively stable too;\n- Consistency between time series level and standard deviation indicates unusual regularity in time series noise component;\n- Very simple state space models fit the data quite well, which indicates regularity in time series trend and seasonal components;\n- Out-of-sample error has a seasonal pattern, which may indicate some structural regularity in sales time series.\n\nOkay, we cannot ignore this anymore. This sales volume data is way too clean and regular... it's totally synthetic! To be honest, I was not very happy when I found out: that was not a real-world problem I had hoped to solve... Well, two chocolate bars managed to make me feel better.\n\nShort time after I found this horrific truth, I stumbled upon [the kernel of XYZT](https:\/\/www.kaggle.com\/thexyzt\/keeping-it-simple-by-xyzt). It showed me how deep the rabbit hole goes.","19fe89ed":"## Addings US holidays\n\nProphet can incorporate information about holidays or any other special events. Splendid! Let's try to include US federal holidays. We don't know what these sales are but there is a good chance that the firm is US based or at least have US customers.\n\nUnfortunately, including US federal holidays does not seem to improve the fit. On the bright side, now I know when the birthday of Martin Luther King is!","c4d0dc36":"Let's check what item-store combinations correspond to the best and worst predictions (i.e., predictions with the lowest and the highest SMAPE, respectively). \n\nWe see that \"best performers\" have a smoother shape of their historical sales, while \"worst performers\" are generally noisier - well, nothing unexpected. Interestingly, all best performers have higher sales volume values. It means that noise in sales data is not purely multiplicative, which is consistent with our volatility plots.","b9dfd7ec":"## Custom item-store weekly seasonalities\n\nTo begin, let's try fitting weekly seasonalities separately for every store, item or both. For example, if we fit sesonalities separately by item, $seasonality_{weekly}(weekday)$ becomes $seasonality_{item, weekly}(item, weekday)$. \n \nGreat, there is a slight improvement for seasonalities fit by item and seasonalities fit by store! If we try fitting seasonalities by item and store at once though, our model seems to overfit.","6ae4a69e":"## Plot data\nLet's plot sales time series to get some general ideas about the data. The data is averaged (weekly) to make plots more readable.\n\nTime series look quite regular, with no obvious outliers. Sales volumes across different items and stores definitely have similatities. In particular, the sales have a trend and a yearly sesonality pattern with a spike during the summer.\n\nWe can also see that sales of one item or in one store are definitely correlated. Good news: with this this information we can create a more stable estimator.\n\nActually, the time series look evan a little too regular. Hmmm that's weird... Could the data be synthetic? Too early to tell, we will get back to this question later.","b3b4b0f9":"<a id=\"SectionOutro\"><\/a>\n# Outro\n\nFor our final submission, we selected the original \"dumb\" model (with a hardcoded trend) and the \"dumb\" model with weekly seasonalities fit for every item separately. The latter had a low and stable cross-validated error and a pretty good public score of 13.845. This choice has payed off: the model got a private score of 12.580 and brought our team to Top 1! Yay! It was quite close to the second best private score of 12.584 by jnng. \n\nGoing through this kernel again, I think the key points contribusing to *Fantastic 2*'s victory in this competions were:\n- Realization that the data is synthetic;\n- Ideas inspired by other public kernels;\n- Careful use of a public score to tune the trend;\n- Cross-validation to select the best model;\n- Luck =)\n\nOn ideas from public kernels: [XYZT's kernel](https:\/\/www.kaggle.com\/thexyzt\/keeping-it-simple-by-xyzt) made a big difference. To be honest, in this era of Gradient Boosting and DNN, I'm not sure we would have thought of such a simple approach to model sales volumes. Simple yet very effective! The leaderboard changed drastically after XYZT shared it - it was very funny to see how at least a dozen top submissions appreared by simply forking his script ^^\n\nAnd... that's it folks! Thanks for reading. Please share your thoughts in the comment section. And cross-validate responsibly.","3c61707c":"<a id=\"SectionEDA\"><\/a>\n# Exploratory Data Analysis\n\nExploratory Data Analysis a.k.a. EDA - that's the place we always begin. Let's quickly go through the problem description, data cleaning and exploration.","8824ffca":"## Extrapolating trend\n\nFunction extrapolation is never an easy task as even functions that have a similar fit on internal data points can have a very different behaviour outside of them. Check some examples below.","590cc3c7":"If we take a closer look at one CV fold, we can see a clear monthly seasonal pattern for both in- and out-of-sample fit.\n\nWe saw that sales volatility is higher in winter - so worse in-sample fit in winter is to be expected. High monthly osciliations out-of-sample are more difficult to interpret. There a number of possible explanations: whether we could not fit monthly seasonality properly, or we overfit yearly seasonality, or our trend does not generalize well, or it is once again due to osciliations in volatility...\n \nOverall, however, there are no obvious red flags.","ac5c8fd1":"## Cross-validated error\n\nNow let's properly evaluate Prophet predictive strenght using cross-validation (CV). It is always tricky to cross-validate a time series. Here I used the following strategy: a model is fit on the data since the day 0 till the day $X_k$, then the model is evaluated on the data from the day $X_k$ till the day $X_{k+1}$. The plot below shows CV SMAPE evaluated on quaterly CV folds, ranging from 2015 Q1 to 2017 Q4.\n\nWhile in-sample SMAPE is quite stable, out-of-sample SMAPE varies quite a lot: it ranges from 11 to 18. Unfortunately, the worst performace is always in the first quarter of the year - and evaluation period for the submission is 2018 Q1.","45560366":"To be on a even safer side, let's check the cross-validated errors date by date. *Signed* SMAPE is SMAPE without absolute operator:\n\n$$SMAPE_{signed} = average_i \\frac{y_i-\\hat{y_i}}{\\frac{1}{2}(y_i+\\hat{y_i})} \\cdot 100\\%.$$\n\nIf our model is not strongly biased, signed SMAPE should be close to zero. \n\nThe hardcoded version seems to perform alright. Awesome!","a08c09e9":"## Trend and seasonality\n\nLet's analyze sales trends and seasonalities. First, we add columns with some date-related information: year, month, weekday, etc.","b29aef58":"## Tuning seasonality by CV\n\nThe \"dumb\" model works pretty well already. However, this model is quite rigid so it could benifit from adding some flexibility. There are a few ways to do it:\n- Custom seasonalities: fit seasonalities individually for different stores or items;\n- More granular seasonality: e.g., add monthly seasonality;\n- Fit the model using recent data only;\n- Fit another model on residuals: e.g., Prophet;\n- Blend with other models: e.g., with Boosted Trees.\n\nI've tried all these approaches. Some of them did alright and may have been used in our final submission if we had more time; others less so. In this kernel we'll focus on the very first approach.","30087dcf":"<a id=\"SectionIntro\"><\/a>\n# Intro\nHi Kaggle! \n\nThis kernel features highly visual EDA and a couple of simple time series forecasting models for [Store Item Demand Forecasting Challenge](https:\/\/www.kaggle.com\/c\/demand-forecasting-kernels-only): \n- [Facebook Prophet](https:\/\/facebook.github.io\/prophet\/docs\/quick_start.html);\n- A manual solution that brought us to Top 1.\n\nI started writing this kernel when we entered the competition to share data exploration with my teammate and to keep track of performance of my models. Now, when the competion ended, I decided to tidy it up a little and to make it public. The kernel roughly shows our team's path from EDA to the winning submission - I hope you'll find it useful! And I'd really appreciate your feedback, both positive ;) and negative -_-\n\n**Special thanks to...**\n- Aditya Soni for [his helpful EDA plots](https:\/\/www.kaggle.com\/adityaecdrid\/my-first-time-series-comp-added-prophet);\n- XYZT for sharing [his amazing solution](https:\/\/www.kaggle.com\/thexyzt\/keeping-it-simple-by-xyzt);\n- My teammate for being there for me <3\n\n**Known bugs**\n- Ipywidgets does not work in a commited notebook due to Kaggle's custom environment \u00af\\\\\\_(\u30c4)\\_\/\u00af You can still fork the notebook to see it in action (it works correctly in a notebook editing session).\n\n**To do (maybe)**\n-  We've also experimented with feature engineering + Gradient Boosting (LightGBM and CatBoost), as a standalone model and for model blending. Although it was not a part of our final solution (mostly due to the time constraint), it was pretty cool.\n- It would be interesing to compare performance of Prophet with a custom state space model in Stan or PyMC3. The latter could fit multivariate time series, which could give a solid performance boost for this problem.\n- In the realm of neural networks, WaveNet shows promising results for multivariate time series - could be worth trying too.","d1e208b9":"There is also a clear upward trend. Markets are bullish, time to buy! The trend is almost linear, with higher than average increase in 2014 and lower in 2017.","ea4f7fc5":"Let's have a look at the aggregared data: sales time series averaged across all items and stores. At first glance, it may look like aggregated time series is much more turbulent than individual ones. In reality, it's not the case: I just plotted it without weekly averaging.\n\nWe can see clear yearly and weekly seasonality patterns, which don't really change with time. This is definitely interesting, and also confirms our hypothesis that the sales could be correlated. In the meanwhile, monthly seasonality is not very pronounced.","24259c5d":"## This kernel features...\n1. [Intro](#SectionIntro)\n2. [EDA (feat. Boken)](#SectionEDA)\n3. [Prophet](#SectionProphet)\n4. [The winning \"dumb\" solution](#SectionDumb)\n5. [Outro](#SectionOutro)","6ba67712":"## Import packages\nWe will need:\n- Numpy and Pandas to work with tabular data\n- Matplotlib, Ipywidgets and Bokeh for data visualisation (BUG: Ipywidgets does not work in a commited notebook)\n- StatsModels and Prophet for exporing time series properties and forecasting\n\nTo be honest, Bokeh is probably an overkill for a one-off project like this one... But Bokeh plots are pretty so it's all worth it!","02270fd2":"Here some interactive plots in Bokeh - because I can!\n\n(please don't ask me how much time I wasted on debugging of this plot...)","fb133b14":"<a id=\"SectionDumb\"><\/a>\n# The winning \"dumb\" solution","0926a983":"## First look\n\nProphet here is fit on 2013 - 2016 data with annual and weekly seasonalities using additive decomposition. The latest version of the package allows selecting multiplicative decomposition, we don't have it on Kaggle. Alas! Our analysis above has shown that it might have been beneficial. Of course the keen ones can mannualy log-transform the data to obtain a similar effect.\n\nIn any case, the fit looks reasonable. Trend and seasonalities of sales across stores and items look similar and consistent with our previous analysis. We see that Prophet adjusted the trend quite a bit in 2014 - well, we saw the trend in 2014 was kind of an outiler on the aggregate level. We see almost zero uncertainty in the trend component, which is probably good (uncertainty in the seasonal component is not plotted). \n\nPartial autocorrelation plot of the predicted residuals looks much better than the one for STL decomposition. Ljung-Box tests is doing definitely better as well; p-value for monthly lags is a bit low but still acceptable.\n\nI also output SMAPE over time for the training (2013 - 2016) and validation (2017 Q1) data, smoothed using LOWESS for better visibility. On average, SMAPE is around 15 and 16 for the training and validation data, respectively. Of course, we yet have no idea how it would perform on the real test data (2018 Q1). \n\nOverall, the results look okay.","7bcd8fc7":"We could try using cross-validation to find the best extrapolation method. The problem is we only have 5 year to do so it is unlikely to be very efficient. When little data available stable methods usually perform the best: they have low variance and there is not much hope of having low bias anyway. So the best solution seems to be picking linear extrapolation or local linear extrapolation with high regularization. The solution with linear trend give us a public score of 13.875. Pretty good for such a \"dumb\" algorithm! This is also one of the earlier solutions described in XYZT's kernel.\n\nIt's unfortunate that we don't have more data to tune the trend. I guess we should stop here. Unless we use one ancient dark and forbidden technique... We can tune the trend using the public score! Yes, it is typically better only to use it during final model validation rather than model parameter tuning - otherwise it's very easy overfit the model. However, in this case we should be fine: we are only tuning one parameter of a pretty stable model. This method gives us expected value of sales in 2018 approximately equal to 60.5, or $trend(2018) \\approx 1.158$. Let's just hardcode this value.\n\nThe solution with a hardcoded trend give us a public score of 13.852 (and a private score of 12.587), which is pretty damn good!\n\nJust to be on the safe side, I cross-validated \"dumb\" models with linear, qudratic and hardcoded trend extrapolation, from 2016 Q1 to 2017 Q4 using the same methodology as before. The hardcoded version (HC) generally shows a lower error which is a pretty good sign.","53e5bd29":"Let's check out-of-sample SMAPE only. The OOS error also has a clear repetitive pattern: in particular, 2015 Q1 kind of resembes 2016 Q1 and 2017 Q1.\n\nWhat does it tell us? It is yet another indication that our data is quite regular. Apart from that - not much really, but it may become more useful when compared to other models.","ca457c55":"## STL decomposition\n\nWe so a rather strong evidence that sales have both trend and seasonality components. As the last step in our EDA, let's apply classical [STL decomposition](https:\/\/www.statsmodels.org\/dev\/generated\/statsmodels.tsa.seasonal.seasonal_decompose.html) to individual sales time series. \n\nWe plot trend, seasonal and residual components of the STL decomposition. For for the residuals, partial autocorrelation (PACF) is also plotted; in the title, you can see a p-value of Ljung-Box test for 7 and 30 lags. P-value <0.05 means that the residuals are likely to be autocorrelated, and there is still some information to be extracted.\n\nIt seems yearly seasonality is a must: without it, our trend becomes increadibly noisy. However, just yearly seasonality is not sufficient: the PACF plot shows a weekly seasonality pattern in residuals, and Ljung-Box test fails badly. Thus, both annual and weekly seasonalities should be modelled. \n\nWe also notice that multiplicative STL fits data better - but only marginally so."}}