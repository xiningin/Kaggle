{"cell_type":{"3f8cb2e2":"code","5560df00":"code","ad58c00d":"code","824b81a7":"code","e6bdeab4":"code","90fab719":"code","11b2d33b":"code","adc1b165":"code","b3ac0bc0":"code","5bfa30b8":"code","e45c8134":"code","6d898107":"code","591cff61":"code","fc95ebce":"markdown"},"source":{"3f8cb2e2":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5560df00":"import cv2\nimport os\nimport re\n\n# Imports for image transforms\n# Albumentations bounding box augmentation docs: https:\/\/albumentations.ai\/docs\/getting_started\/bounding_boxes_augmentation\/\nimport albumentations as A\nfrom albumentations.pytorch.transforms import ToTensorV2\n\n# Torch imports\nimport torch\nimport torchvision\nfrom torchvision.models.detection.faster_rcnn import FastRCNNPredictor\nfrom torchvision.models.detection import FasterRCNN\nfrom torchvision.models.detection.rpn import AnchorGenerator\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.utils.data.sampler import SequentialSampler\n\nfrom matplotlib import pyplot as plt","ad58c00d":"device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')","824b81a7":"INPUT_DIR = '\/kaggle\/input\/global-wheat-detection'\n# PRETRAINED_DIR = '\/kaggle\/input\/wheat-dataset-project'\nPRETRAINED_DIR = '\/kaggle\/input\/wheat-frcnn-bayesian\/'\nOUTPUT_DIR = '\/kaggle\/output\/'\nTRAIN_DIR = f'{INPUT_DIR}\/train'\nTEST_DIR = f'{INPUT_DIR}\/test'\n# MODEL_LOC = f'{PRETRAINED_DIR}\/fasterrcnn_resnet50_fpn_TRAINED.pth'\nMODEL_LOC = f'{PRETRAINED_DIR}\/fasterrcnn_resnet50_fpn_BAYESOPT.pth'","e6bdeab4":"def test_transform():\n    return A.Compose([ToTensorV2(p=1.0)])\n\nclass TestDataset(Dataset):\n    \n    def __init__(self, df, directory, transforms=None):\n        super().__init__()\n        \n        self.image_ids = df['image_id'].unique()\n        self.df = df\n        self.dir = directory\n        self.transforms = transforms\n        \n    def __len__(self):\n        return int(self.image_ids.shape[0])\n    \n    def __getitem__(self, index: int):\n        image_id = self.image_ids[index]\n        image = cv2.imread(f'{self.dir}\/{image_id}.jpg', cv2.IMREAD_COLOR)\n        # cv2 reads images into BGR format, must convert to RGB for f-RCNN\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        # f-RCNN requires images in [C,W,H] form with values in [0,1]\n        image \/= 255.0\n        \n        if self.transforms:\n            dataToTransform = {'image': image}\n            transData = self.transforms(**dataToTransform)\n            image = transData['image']\n        \n        return image, image_id","90fab719":"model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=False, pretrained_backbone=False)\nin_features = model.roi_heads.box_predictor.cls_score.in_features\nnum_classes = 2\nmodel.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\npretrained_state_dict = torch.load(MODEL_LOC)\nmodel.load_state_dict(pretrained_state_dict)\nmodel.eval()\nmodel.to(device)","11b2d33b":"test_df = pd.read_csv(f'{INPUT_DIR}\/sample_submission.csv')\ntest_df.head()","adc1b165":"test_df.shape","b3ac0bc0":"test_dataset = TestDataset(test_df, TEST_DIR, test_transform())\n\ndef collate_fn(batch):\n    return tuple(zip(*batch))\n\ntest_dl = DataLoader(dataset=test_dataset, batch_size=4, num_workers=4, collate_fn=collate_fn)","5bfa30b8":"THRESHOLD = .5\nres = []\n\ndef getPredString(outputTup):\n    formatted_strings = []\n    for tup in outputTup:\n        score = tup[0]\n        x, y, w, h = tup[1]\n        box_string = f'{score} {x} {y} {w} {h}'\n        formatted_strings.append(box_string)\n    return \" \".join(formatted_strings)\n    \n    \n\nfor imgs, img_ids in test_dl:\n    imgs = list(image.to(device) for image in imgs)\n    model_outputs = model(imgs)\n    \n    for i,img in enumerate(imgs):\n        scores = model_outputs[i]['scores'].data.cpu().numpy()\n        bboxes = model_outputs[i]['boxes'].data.cpu().numpy()\n        bboxes = bboxes[scores >= THRESHOLD].astype(np.int32)\n        scores = scores[scores >= THRESHOLD]\n        \n        this_id = img_ids[i]\n        \n        bboxes[:, 2] = bboxes[:, 2] - bboxes[:, 0]\n        bboxes[:, 3] = bboxes[:, 3] - bboxes[:, 1]\n        \n        res.append({'image_id': this_id, 'PredictionString': getPredString(zip(scores, bboxes))})\n\ntest_df = pd.DataFrame(res, columns=['image_id', 'PredictionString'])","e45c8134":"test_df.head(10)","6d898107":"sample = imgs[1].permute(1,2,0).cpu().numpy()\nscores = model_outputs[1]['scores'].data.cpu().numpy()\nbboxes = model_outputs[1]['boxes'].data.cpu().numpy()\nbboxes = bboxes[scores >= THRESHOLD].astype(np.int32)\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in bboxes:\n    cv2.rectangle(sample,\n                  (box[0], box[1]),\n                  (box[2], box[3]),\n                  (220, 0, 0), 2)\n    \nax.set_axis_off()\nax.imshow(sample)","591cff61":"test_df.to_csv('submission.csv', index=False)","fc95ebce":"# Model Inference\n* Create TestDataset class (similar to WheatDataset but no bboxes, transforms only change to tensor)\n* Create dataset using TestDataset clas\n* Create dataloader\n* Loop over images, image_ids in dataloader\n    * Within each iteration, get outputs by calling model(images)\n    * Loop over i, image in enum(images)\n        * Get boxes and scores from outputs for element i\n        * Threshold boxes and scores\n        * Get boxes from \\[xmin ymin xmax ymax\\] form into \\[x y w h\\] form\n        * make result dict for image id and prediction string (in competition format) and append to result list\n* Sample from outputs as before (with score threshold on boxes) to display prediction"}}