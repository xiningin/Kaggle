{"cell_type":{"6f6e3a6e":"code","d611faee":"code","1b7b7663":"code","128712fa":"code","8cb773c6":"code","a1c615c7":"code","150da07e":"code","15a5cf26":"code","dbae0b6a":"code","b5905d38":"code","e2238be5":"code","66036517":"code","4519c24b":"code","00adb3cc":"code","bdc1a037":"code","9097d851":"code","944fce42":"code","cb9f18cc":"code","64b31bb4":"code","c5f0ab67":"code","8fc6568b":"code","f10c1e95":"code","715175db":"code","fa9b6a9d":"code","81c38ae1":"code","68e6b903":"code","4c4691b4":"code","85d54aa2":"code","e0c22ffc":"code","a1d1cfd9":"code","8b0d5e29":"code","7e58785f":"code","7a664fc8":"code","0eed09f0":"code","5ec70645":"code","52f92b2c":"code","93a55901":"markdown","f476a619":"markdown","df5214b0":"markdown","0da3e681":"markdown","db6926cd":"markdown","3d71bc25":"markdown","add1b367":"markdown","3e2fff47":"markdown","790e1fbb":"markdown","5349c8eb":"markdown","03c6d0f7":"markdown","8d9371fb":"markdown","8e045913":"markdown","9cd3252f":"markdown","2eed10d7":"markdown","3a81c67f":"markdown","e60ef892":"markdown","e37d47b3":"markdown","79d75fcb":"markdown","b56dd94d":"markdown","7ce70a34":"markdown","5d34da9b":"markdown","fd90c1ab":"markdown","73031dcc":"markdown","34a6357c":"markdown","23684547":"markdown","237e7103":"markdown","5fd8255f":"markdown","43914338":"markdown","30c03231":"markdown","feca89b5":"markdown","1108d8d6":"markdown","6679797f":"markdown","192b59af":"markdown","26eb0cb8":"markdown","cbae10ab":"markdown","50ef83ec":"markdown","13a11fb3":"markdown"},"source":{"6f6e3a6e":"!pip install watermark\n%load_ext watermark\n%watermark -a \"Sebastian Raschka\" -u -d -v -p numpy,pandas,matplotlib,sklearn\nfrom IPython.display import Image\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","d611faee":"import pandas as pd\n\ndf = pd.read_csv('https:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/breast-cancer-wisconsin\/wdbc.data', header=None)\n\n# if the Breast Cancer dataset is temporarily unavailable from the\n# UCI machine learning repository, un-comment the following line\n# of code to load the dataset from a local path:\n\n#df = pd.read_csv('uci-breast-cancer-dataset\/wdbc.data', header=None)\n# Let's evaluate the rows and columns in the dataset\ndf.shape","1b7b7663":"# Let's review the dataset\ndf.head()","128712fa":"\"\"\"\nEncoding the diagnosis (M,B) into 1,0\n\"\"\"\nfrom sklearn.preprocessing import LabelEncoder\n\nX = df.loc[:, 2:].values\ny = df.loc[:, 1].values\nle = LabelEncoder()\ny = le.fit_transform(y)\nle.classes_","8cb773c6":"le.transform(['M', 'B'])","a1c615c7":"\"\"\"\nSplitting the dataset into training set and testing set with a test_size as 20%\n\"\"\"\n\nfrom sklearn.model_selection import train_test_split\n\"\"\"\nStratified sampling aims at splitting a data set so that each split is similar with respect to something.\nIn a classification setting, it is often chosen to ensure that the train and test sets have approximately \nthe same percentage of samples of each target class as the complete set.\n\"\"\"\nX_train, X_test, y_train, y_test = \\\n    train_test_split(X, y, \n                     test_size=0.20,\n                     stratify=y,\n                     random_state=1)","150da07e":"'''\nCreating a pipeline\n  * Applying StandardScaler\n  * Performing dimensionality reduction using PCA\n  * Running LogisticRegression\n  * Doing Prediction\n'''\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.pipeline import make_pipeline\n# PCA is lowering the 30 dimensions onto a 2 dimensions\npipe_lr = make_pipeline(StandardScaler(),\n                        PCA(n_components=2),\n                        LogisticRegression(random_state=1))\n\npipe_lr.fit(X_train, y_train)\ny_pred = pipe_lr.predict(X_test)\nprint('Test Accuracy: %.3f' % pipe_lr.score(X_test, y_test))","15a5cf26":"Image(filename='..\/input\/images\/06_01.png', width=500) ","dbae0b6a":"Image(filename='..\/input\/images\/06_02.png', width=500) ","b5905d38":"Image(filename='..\/input\/images\/06_03.png', width=500) ","e2238be5":"import numpy as np\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import KFold\n\n\"\"\"\nIn Stratified K-Fold the class proportions are preserved in each fold to ensure that each fold \nis representative of the class proportions in the training dataset.\n\"\"\"\n# kfold = KFold(n_splits=10,random_state=1).split(X_train, y_train)\nkfold = StratifiedKFold(n_splits=10,random_state=1).split(X_train, y_train)\n\nscores = []\nfor k, (train, test) in enumerate(kfold):\n    pipe_lr.fit(X_train[train], y_train[train])\n    score = pipe_lr.score(X_train[test], y_train[test])\n    scores.append(score)\n    print('Fold: %2d, Class dist.: %s, Acc: %.3f' % (k+1,\n          np.bincount(y_train[train]), score))\n    \nprint('\\nCV accuracy: %.3f +\/- %.3f' % (np.mean(scores), np.std(scores)))","66036517":"'''\n* Scikit learn has a cross_val_score implementation which does the same.\n* Useful feature of the cross_val_score approach is that you can distribute the evaluation of the \n   different folds across multiple CPUs on your machine. \n* If you change the n_jobs=2 then you can distribute the k folds on 2 CPUs. \n* If n_jobs=-1 then you use all available CPUs on your machine to do the computation in parallel.\n'''\n\nfrom sklearn.model_selection import cross_val_score\n\nscores = cross_val_score(estimator=pipe_lr,\n                         X=X_train,\n                         y=y_train,\n                         cv=10,\n                         n_jobs=-1)\nprint('CV accuracy scores: %s' % scores)\nprint('CV accuracy: %.3f +\/- %.3f' % (np.mean(scores), np.std(scores)))","4519c24b":"from sklearn.model_selection import GridSearchCV\nfrom sklearn.svm import SVC\n\npipe_svc = make_pipeline(StandardScaler(),\n                         SVC(random_state=1))\n# 8 parameters\nparam_range = [0.0001, 0.001, 0.01, 0.1, 1.0, 10.0, 100.0, 1000.0]\n\n\"\"\"\n* Train and Tune a SVM pipeline\n* We are using linear SVM and RBF kernel SVM\n* For Linear SVM we are tuning just C - Inverse Regularization Parameter\n* For RBF SVM we are tuning both C and gamma parameters\n\"\"\"\nparam_grid = [{'svc__C': param_range, \n               'svc__kernel': ['linear']},\n              {'svc__C': param_range, \n               'svc__gamma': param_range, \n               'svc__kernel': ['rbf']}]\n\ngs = GridSearchCV(estimator=pipe_svc, \n                  param_grid=param_grid, \n                  scoring='accuracy', \n                  cv=10,\n                  n_jobs=-1)\ngs = gs.fit(X_train, y_train)\nprint(gs.best_score_)\nprint(gs.best_params_)","00adb3cc":"clf = gs.best_estimator_\n\n# clf.fit(X_train, y_train)\n# Note that the line above is not necessary, because\n# the best_estimator_ will already be refit to the complete\n# training set because of the refit=True setting in GridSearchCV\n# (refit=True by default). Thanks to a reader, German Martinez,\n# for pointing it out.\n\nprint('Test accuracy: %.3f' % clf.score(X_test, y_test))","bdc1a037":"Image(filename='..\/input\/images\/06_07.png', width=500) ","9097d851":"'''\n* cv - Determines the cross-validation splitting strategy. \n* For Grid Search which is in the inner loop we are doing 2 loops\n* For Outer loop we are doing 5 loops\n'''\n\n'''\n* Support Vector Machine model\n'''\ngs = GridSearchCV(estimator=pipe_svc,\n                  param_grid=param_grid,\n                  scoring='accuracy',\n                  cv=2)\n\nscores = cross_val_score(gs, X_train, y_train, \n                         scoring='accuracy', cv=5)\nprint('CV accuracy: %.3f +\/- %.3f' % (np.mean(scores),\n                                      np.std(scores)))","944fce42":"'''\n* DecisionTreeClassifier model\n'''\nfrom sklearn.tree import DecisionTreeClassifier\n\ngs = GridSearchCV(estimator=DecisionTreeClassifier(random_state=0),\n                  param_grid=[{'max_depth': [1, 2, 3, 4, 5, 6, 7, None]}],\n                  scoring='accuracy',\n                  cv=2)\n\nscores = cross_val_score(gs, X_train, y_train, \n                         scoring='accuracy', cv=5)\nprint('CV accuracy: %.3f +\/- %.3f' % (np.mean(scores), \n                                      np.std(scores)))","cb9f18cc":"X_imb = np.vstack((X[y == 0], X[y == 1][:40]))\ny_imb = np.hstack((y[y == 0], y[y == 1][:40]))","64b31bb4":"y_pred = np.zeros(y_imb.shape[0])\nnp.mean(y_pred == y_imb) * 100","c5f0ab67":"from sklearn.utils import resample\n\nprint('Number of class 1 samples before:', X_imb[y_imb == 1].shape[0])\n\nX_upsampled, y_upsampled = resample(X_imb[y_imb == 1],\n                                    y_imb[y_imb == 1],\n                                    replace=True,\n                                    n_samples=X_imb[y_imb == 0].shape[0],\n                                    random_state=123)\n\nprint('Number of class 1 samples after:', X_upsampled.shape[0])","8fc6568b":"X_bal = np.vstack((X[y == 0], X_upsampled))\ny_bal = np.hstack((y[y == 0], y_upsampled))","f10c1e95":"y_pred = np.zeros(y_bal.shape[0])\nnp.mean(y_pred == y_bal) * 100","715175db":"Image(filename='..\/input\/images\/06_08.png', width=300) ","fa9b6a9d":"from sklearn.metrics import confusion_matrix\n\npipe_svc.fit(X_train, y_train)\ny_pred = pipe_svc.predict(X_test)\nconfmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint(confmat)","81c38ae1":"import matplotlib.pyplot as plt\nfig, ax = plt.subplots(figsize=(2.5, 2.5))\nax.matshow(confmat, cmap=plt.cm.Blues, alpha=0.3)\nfor i in range(confmat.shape[0]):\n    for j in range(confmat.shape[1]):\n        ax.text(x=j, y=i, s=confmat[i, j], va='center', ha='center')\n\nplt.xlabel('Predicted label')\nplt.ylabel('True label')\n\nplt.tight_layout()\n#plt.savefig('images\/06_09.png', dpi=300)\nplt.show()","68e6b903":"le.transform(['M', 'B'])","4c4691b4":"confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint(confmat)","85d54aa2":"confmat = confusion_matrix(y_true=y_test, y_pred=y_pred)\nprint(confmat)","e0c22ffc":"confmat = confusion_matrix(y_true=y_test, y_pred=y_pred, labels=[1, 0])\nprint(confmat)","a1d1cfd9":"# Image from https:\/\/en.wikipedia.org\/wiki\/Precision_and_recall\nImage(filename='..\/input\/images\/Metrics_image.png', width=1800) ","8b0d5e29":"from sklearn.metrics import precision_score, recall_score, f1_score\n\nprint('Precision: %.3f' % precision_score(y_true=y_test, y_pred=y_pred))\nprint('Recall: %.3f' % recall_score(y_true=y_test, y_pred=y_pred))\nprint('F1: %.3f' % f1_score(y_true=y_test, y_pred=y_pred))","7e58785f":"'''\nYou can use different metric for scoring your algorithm. \nBelow we are using F1 score as the metric\n'''\n\nfrom sklearn.metrics import make_scorer\n\nscorer = make_scorer(f1_score, pos_label=0)\n\nc_gamma_range = [0.01, 0.1, 1.0, 10.0]\n\nparam_grid = [{'svc__C': c_gamma_range,\n               'svc__kernel': ['linear']},\n              {'svc__C': c_gamma_range,\n               'svc__gamma': c_gamma_range,\n               'svc__kernel': ['rbf']}]\n\ngs = GridSearchCV(estimator=pipe_svc,\n                  param_grid=param_grid,\n                  scoring=scorer,\n                  cv=10,\n                  n_jobs=-1)\ngs = gs.fit(X_train, y_train)\nprint(gs.best_score_)\nprint(gs.best_params_)","7a664fc8":"from sklearn.metrics import roc_curve, auc\nfrom scipy import interp\n\npipe_lr = make_pipeline(StandardScaler(),\n                        PCA(n_components=2),\n                        LogisticRegression(penalty='l2', \n                                           random_state=1, \n                                           C=100.0))\n\nX_train2 = X_train[:, [4, 14]]\n    \n\ncv = list(StratifiedKFold(n_splits=3, \n                          random_state=1).split(X_train, y_train))\n\nfig = plt.figure(figsize=(7, 5))\n\nmean_tpr = 0.0\nmean_fpr = np.linspace(0, 1, 100)\nall_tpr = []\n\nfor i, (train, test) in enumerate(cv):\n    probas = pipe_lr.fit(X_train2[train],\n                         y_train[train]).predict_proba(X_train2[test])\n\n    fpr, tpr, thresholds = roc_curve(y_train[test],\n                                     probas[:, 1],\n                                     pos_label=1)\n    mean_tpr += interp(mean_fpr, fpr, tpr)\n    mean_tpr[0] = 0.0\n    roc_auc = auc(fpr, tpr)\n    plt.plot(fpr,\n             tpr,\n             label='ROC fold %d (area = %0.2f)'\n                   % (i+1, roc_auc))\n\nplt.plot([0, 1],\n         [0, 1],\n         linestyle='--',\n         color=(0.6, 0.6, 0.6),\n         label='random guessing')\n\nmean_tpr \/= len(cv)\nmean_tpr[-1] = 1.0\nmean_auc = auc(mean_fpr, mean_tpr)\nplt.plot(mean_fpr, mean_tpr, 'k--',\n         label='mean ROC (area = %0.2f)' % mean_auc, lw=2)\nplt.plot([0, 0, 1],\n         [0, 1, 1],\n         linestyle=':',\n         color='black',\n         label='perfect performance')\n\nplt.xlim([-0.05, 1.05])\nplt.ylim([-0.05, 1.05])\nplt.xlabel('false positive rate')\nplt.ylabel('true positive rate')\nplt.legend(loc=\"lower right\")\n\nplt.tight_layout()\n# plt.savefig('images\/06_10.png', dpi=300)\nplt.show()","0eed09f0":"Image(filename='..\/input\/images\/06_04.png', width=600) ","5ec70645":"import matplotlib.pyplot as plt\nfrom sklearn.model_selection import learning_curve\n\n\npipe_lr = make_pipeline(StandardScaler(),\n                        LogisticRegression(penalty='l2', random_state=1))\n\ntrain_sizes, train_scores, test_scores =\\\n                learning_curve(estimator=pipe_lr,\n                               X=X_train,\n                               y=y_train,\n                               train_sizes=np.linspace(0.1, 1.0, 10),\n                               cv=10,\n                               n_jobs=1)\n\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\nplt.plot(train_sizes, train_mean,\n         color='blue', marker='o',\n         markersize=5, label='training accuracy')\n\nplt.fill_between(train_sizes,\n                 train_mean + train_std,\n                 train_mean - train_std,\n                 alpha=0.15, color='blue')\n\nplt.plot(train_sizes, test_mean,\n         color='green', linestyle='--',\n         marker='s', markersize=5,\n         label='validation accuracy')\n\nplt.fill_between(train_sizes,\n                 test_mean + test_std,\n                 test_mean - test_std,\n                 alpha=0.15, color='green')\n\nplt.grid()\nplt.xlabel('Number of training samples')\nplt.ylabel('Accuracy')\nplt.legend(loc='lower right')\nplt.ylim([0.8, 1.03])\nplt.tight_layout()\n#plt.savefig('images\/06_05.png', dpi=300)\nplt.show()","52f92b2c":"from sklearn.model_selection import validation_curve\n\n\nparam_range = [0.001, 0.01, 0.1, 1.0, 10.0, 100.0]\ntrain_scores, test_scores = validation_curve(\n                estimator=pipe_lr, \n                X=X_train, \n                y=y_train, \n                param_name='logisticregression__C', \n                param_range=param_range,\n                cv=10)\n\ntrain_mean = np.mean(train_scores, axis=1)\ntrain_std = np.std(train_scores, axis=1)\ntest_mean = np.mean(test_scores, axis=1)\ntest_std = np.std(test_scores, axis=1)\n\nplt.plot(param_range, train_mean, \n         color='blue', marker='o', \n         markersize=5, label='training accuracy')\n\nplt.fill_between(param_range, train_mean + train_std,\n                 train_mean - train_std, alpha=0.15,\n                 color='blue')\n\nplt.plot(param_range, test_mean, \n         color='green', linestyle='--', \n         marker='s', markersize=5, \n         label='validation accuracy')\n\nplt.fill_between(param_range, \n                 test_mean + test_std,\n                 test_mean - test_std, \n                 alpha=0.15, color='green')\n\nplt.grid()\nplt.xscale('log')\nplt.legend(loc='lower right')\nplt.xlabel('Parameter C')\nplt.ylabel('Accuracy')\nplt.ylim([0.8, 1.0])\nplt.tight_layout()\n# plt.savefig('images\/06_06.png', dpi=300)\nplt.show()","93a55901":"## Reading a confusion matrix\n* Confusion Matrix layouts the performance of a learning algoirthm for classfication models\n* Square matrix which reports the counts of true positives, false negatives, false positives and true negatives predictions of the classifier.\n","f476a619":"**What is Class Imbalance**\n* Class imbalance is a quite common problem when working with real-world data\u2014samples from one class or multiple classes are over-represented in a dataset.\n* The breast cancer dataset has 90% healthy patients. If we guessed all the patients to be healthy our accuracy will be 90% but we are not learning anything about the features.\n* How do you deal with Class Imbalance. There is no one way to solve this problem:\n    1. UpSampling the minority class.\n    2. DownSampling the majority class.\n    3. Generation of synthetic training samples. - **Synthetic Minority Over-sampling Technique (SMOTE)**","df5214b0":"**We have used accuracy for most of the model evaluations so far.**\nOther Metrics\n* Precision\n* Recall\n* F1-score","0da3e681":"## K-fold cross-validation\n* In k-fold cross-validation you randomly split the training set into k-folds without replacement, where k-1 folds are used for the model training and one fold is used for testing. \n* Since this is repeated k times we obtain k models and performance metrics. \n* We can use the k-folds for model tuning. \n* Once we found the right hyperparameters values we can retrain the model on the complete training set and obtain a final performance estimate using the independent test set. \n* **k=10** works for most applications unless you have a smaller training set. \n* For smaller training sets increase the number of folds. When k = the number of observations in your dataset, then that's LOOCV","db6926cd":"# Debugging algorithms with learning curves","3d71bc25":"* Note that the (true) class 0 samples that are correctly predicted as class 0 (true negatives) are now in the upper left corner of the matrix (index 0, 0). \n* In order to change the ordering so that the true negatives are in the lower right corner (index 1,1) and the true positves are in the upper left, we can use the `labels` argument like shown below:","add1b367":"# Make sure your Internet is On in Settings","3e2fff47":"* You choose different metric depending on your business scenario.\n* **Accuracy = (TP + TN) \/ (FP + FN + TP + TN)**\n* In the case of tumor analysis we want to detect as many malignant tumors as possible but at the same time we don't want to worry patients by misclassifying benign tumors. \n* In case of spam filter classification it might be different where misclassfication might not be as important.\n* **Precision and recall** are two extremely important model evaluation metrics. \n* While precision refers to the percentage of your results which are relevant. (True Positive\/ Predicted Condition Positive)\n    **Precision = TP \/ (TP + FP)**\n* Recall refers to the percentage of total relevant results correctly classified by your algorithm. (True Positive\/ Condition Positive)\n    **Recall = TP \/(TP + FN)**\n* Unfortunately, it is not possible to maximize both these metrics at the same time, as one comes at the cost of another. \n* **F-1 score**, which is a **harmonic mean of precision and recall**. For problems where both precision and recall are important, one can select a model which maximizes this F-1 score. \n* For other problems, a trade-off is needed, and a decision has to be made whether to maximize precision, or recall.","790e1fbb":"* Important things during modeling is to make sure you follow the same steps for your training set and testing dataset. \n* We might be scaling the data, applying some transformation or do dimensionality reduction on training set. \n* Sci-kit learn Pipeline class is a handy tool that will help us create a Pipeline with multiple steps and allows us to follow the same steps on both the training and testing dataset.","5349c8eb":"Remember that we previously encoded the class labels so that *malignant* samples are the \"positive\" class (1), and *benign* samples are the \"negative\" class (0):","03c6d0f7":"### Summary of the steps\n* When you review the column definitions of the dataset you will see the first column is the unique ID of the sample and the second column is corresponding diagnosis of the sample (M=malignant, B=Benign). \n* **The goal of our model is to predict the diagnosis of the sample**. \n* We are going to encode the **diagnosis (M, B) into 1,0**\n* **Split the dataset into training and testing set** - Very important step. If you don't do this your model will have good results when you are testing it but when you deploy your model it will perform really bad.\n* Apply **Standard Scalar** on all the features so the data is standardized before we feed it to **linear classifier - Logistic Regression**\n* Doing **dimensionality reduction** using Principal Component Analysis (PCA) - **Feature extraction technique**\n* Running **Logistic Regression model** - fit so our algorithm can learn from the training set.\n* Predict on the test set - Evaluating how well your model is able to predict the output\n* Using **Accuracy** as our metric","8d9371fb":"### Learning Curves, Validataion Curves\n* Both curves show the training and validation scores of an estimator on the y-axis.\n* A learning curve plots the score over varying numbers of training samples.\n    - The learning curve is a tool for finding out if an estimator would benefit from more data, or if the model is too simple (biased).\n* A validation curve plots the score over a varying hyper parameter.\n    - Finds the best hyper parameters\n    - Some hyper parameters (number of neurons in a neural network, maximum tree depth in a decision tree, amount of regularization, etc.) control the complexity of a model. We want the model to be complex enough to capture relevant information in the training data but not too complex to avoid overfitting.","8e045913":"Next, we printed the confusion matrix like so:","9cd3252f":"## Diagnosing bias and variance problems with learning curves","2eed10d7":"## Plotting a receiver operating characteristic (ROC) Curve","3a81c67f":"# Streamlining workflows with pipelines","e60ef892":"## Optimizing the precision and recall of a classification model","e37d47b3":"# Looking at different performance evaluation metrics","79d75fcb":"## Algorithm selection with nested cross-validation\nCover if time permits","b56dd94d":"## The holdout method\n* Holding the test dataset by splitting the dataset into training and test set. \n* Classic and most popular approach.\n* In typical machine learning application we change hyperparameters to tune the model so we select the best model in the end.\n* When we are doing parameter tuning and running the model we are using the same test set over and over for **model selection** so we will most likely overfit.\n* Best practice to follow in holdout method is to split the data into training data further into training and validation set. We are changing the hyperparameters and using the validation test to test which model is a better fit.\n* Finally we use the test set to run against the best model to evaluate the performance.\n* **Disadvantage** - The performance estimate is sensitive to how we partition the training set into training and validation subsets. Hence k-fold cross validation","7ce70a34":"### Overview\n- [Streamlining workflows with pipelines](#Streamlining-workflows-with-pipelines)\n  - [Loading the Breast Cancer Wisconsin dataset](#Loading-the-Breast-Cancer-Wisconsin-dataset)\n  - [Combining transformers and estimators in a pipeline](#Combining-transformers-and-estimators-in-a-pipeline)\n- [Using k-fold cross-validation to assess model performance](#Using-k-fold-cross-validation-to-assess-model-performance)\n  - [The holdout method](#The-holdout-method)\n  - [K-fold cross-validation](#K-fold-cross-validation)\n- [Fine-tuning machine learning models via grid search](#Fine-tuning-machine-learning-models-via-grid-search)\n  - [Tuning hyperparameters via grid search](#Tuning-hyperparameters-via-grid-search)\n  - [Algorithm selection with nested cross-validation](#Algorithm-selection-with-nested-cross-validation)\n- [Looking at different performance evaluation metrics](#Looking-at-different-performance-evaluation-metrics)\n  - [Reading a confusion matrix](#Reading-a-confusion-matrix)\n  - [Optimizing the precision and recall of a classification model](#Optimizing-the-precision-and-recall-of-a-classification-model)\n  - [Plotting a receiver operating characteristic](#Plotting-a-receiver-operating-characteristic)\n  - [The scoring metrics for multiclass classification](#The-scoring-metrics-for-multiclass-classification)\n- [Dealing with class imbalance](#Dealing-with-class-imbalance)\n- [Debugging algorithms with learning and validation curves](#Debugging-algorithms-with-learning-and-validation-curves)\n  - [Diagnosing bias and variance problems with learning curves](#Diagnosing-bias-and-variance-problems-with-learning-curves)\n  - [Addressing overfitting and underfitting with validation curves](#Addressing-overfitting-and-underfitting-with-validation-curves)\n- [Summary](#Summary)","5d34da9b":"## Dealing with class imbalance","fd90c1ab":"From the results above you can see the **SVM model is better than the DecisionTreeClassifier**.","73031dcc":"...","34a6357c":"## Loading the Breast Cancer Wisconsin dataset","23684547":"# Fine-tuning machine learning models via grid search","237e7103":"## Combining transformers and estimators in a pipeline","5fd8255f":"* AUC - ROC curve is a performance measurement for classification problem at various thresholds settings. \n* ROC is a probability curve and AUC (Area Under the Curve) represents degree or measure of separability. \n* It tells how much model is capable of distinguishing between classes. \n* Higher the AUC, better the model is at predicting 0s as 0s and 1s as 1s. \n* By analogy, Higher the AUC, better the model is at distinguishing between patients with disease and no disease.","43914338":"*Python Machine Learning 2nd Edition* by [Sebastian Raschka](https:\/\/sebastianraschka.com), Packt Publishing Ltd. 2017\n\nCode Repository: https:\/\/github.com\/rasbt\/python-machine-learning-book-2nd-edition\n\nCode License: [MIT License](https:\/\/github.com\/rasbt\/python-machine-learning-book-2nd-edition\/blob\/master\/LICENSE.txt)\n\n# Python Machine Learning - Code Examples\n\n# Chapter 6 - Learning Best Practices for Model Evaluation and Hyperparameter Tuning\n\nNote that the optional watermark extension is a small IPython notebook plugin that I developed to make the code reproducible. You can just skip the following line(s).\n\n*The use of `watermark` is optional. You can install this IPython extension via \"`pip install watermark`\". For more information, please see: https:\/\/github.com\/rasbt\/watermark.*","30c03231":"# Using k-fold cross validation to assess model performance","feca89b5":"We conclude:\n\n* Assuming that class 1 (malignant) is the positive class in this example, our model correctly classified 71 of the samples that belong to class 0 (benign - true negatives) and 40 samples that belong to class 1 (malignant- true positives), respectively. \n* However, our model also incorrectly misclassified 1 sample from class 0 as class 1 (false positive), and it predicted that 2 samples are benign although it is a malignant tumor (false negatives).","1108d8d6":"* Bias - Bias is the difference between the average prediction of our model and the correct value which we are trying to predict. Model with high bias pays very little attention to the training data and oversimplifies the model. \n\n* Varaince - Variance is the variability of model prediction for a given data point or a value which tells us spread of our data. Model with high variance pays a lot of attention to training data and does not generalize on the data which it hasn\u2019t seen before. As a result, such models perform very well on training data but has high error rates on test data.\n\n* Underfitting (high bias), which means that our model is not complex enough to capture the pattern in the training data well and therefore also suffers from low performance on unseen data. \n\n* Overfitting is a common problem in machine learning, where a model performs well on training data but does not generalize well to unseen data (test data). If a model suffers from overfitting, we also say that the model has a high variance. \n\n* Techniques to use to address overfitting:\n    - Collect more training data, \n    - Reduce the complexity of the model\n    - Increase the regularization parameter,\n","6679797f":"## Tuning hyperparameters via grid search \n* In machine learning we can tune the parameters of a learning algorithm (for example - depth of a decision tree or regularization parameter in logistic regression. \n* This tuning is **hyperparameter tuning**. \n* Using **GridSearch** you can find the optimal combination of hyper parameters for your model.\n* Brute force exhaustive search where you specify a parameter range and you find the model which gets the best result.\n* Computationally very expensive\n","192b59af":"----------------------------","26eb0cb8":"## Addressing over- and underfitting with validation curves","cbae10ab":"* One of the key steps with any model is to estimate it's performance on data that it hasn't seen. \n* A model can suffer from underfitting(high bias) if the model is too simple.\n* A model can overfit(high variance)the training data if the model is too complex for the underlying training data.\n* Next we will look at different cross validation techniques which can help us obtain reliable estimates on how well the model performs on unseen data.","50ef83ec":"If you want to select among different machine learning algorithms we can use nested cross-validation\n* You have an outer k-fold cross-validation loop to split the data into training and test folds\n* Inner loop is used to select the model using k-fold cross-validation on the training fold.\n* After model selection then the test fold is then used for model evaluation.","13a11fb3":"### Additional Note"}}