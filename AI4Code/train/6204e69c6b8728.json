{"cell_type":{"cb72ba5f":"code","9eeb8cc8":"code","eb7e16ae":"code","03288bc9":"code","d991ba2b":"code","d13fad57":"code","b717f2c6":"code","7ca49603":"code","1b3c7ea7":"code","ead2e171":"code","9d2fe3fa":"code","ec4eb091":"code","0aacb38f":"code","53121bc3":"code","4e486d75":"code","c062db78":"code","34e847d9":"code","7b3d6aed":"code","00d028d9":"code","a1198ddb":"code","45e31fcd":"code","3483b986":"code","a77a540d":"code","00f9b5f7":"code","1377448b":"code","763e4e05":"code","62047b71":"code","1457e682":"code","598d3d52":"code","c256d89c":"code","91a666d3":"code","3011feab":"code","c621c719":"code","a4d43318":"markdown","092d6192":"markdown","7abda5fc":"markdown","f3ab49b1":"markdown","b66a8e55":"markdown","6c981c7b":"markdown","9c416d21":"markdown","c059d4c6":"markdown","5acc663c":"markdown","fe56929e":"markdown","99846bb1":"markdown","fe9cf6ef":"markdown","8586b4f8":"markdown","ff9c7511":"markdown","167425f3":"markdown","44d3013f":"markdown","cc52e83e":"markdown","9b6c0822":"markdown","6b03b759":"markdown","83a0000a":"markdown","473f9e3c":"markdown","bd1ef12f":"markdown","df51174a":"markdown","1215b0ff":"markdown"},"source":{"cb72ba5f":"# input data files are available in the \"..\/input\/\" directory\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# load libraries\nimport time\nimport warnings\nimport random\nimport pandas as pd\nimport datetime\nimport lightgbm as lgb\nimport xgboost as xgb\nimport seaborn as sns\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn import metrics\nfrom mlxtend.regressor import StackingCVRegressor\n\n# ignore warnings\nwarnings.filterwarnings('ignore')\n\n# load data\ndata = pd.read_csv(\"..\/input\/ted-talks\/ted_main.csv\")\ndata.shape","9eeb8cc8":"data.head()","eb7e16ae":"pd.isnull(data).sum()","03288bc9":"for index, row in data.iterrows():\n    if pd.isnull(row['speaker_occupation']):\n        data['speaker_occupation'][index] = 'Other'","d991ba2b":"data['related_talks'][0]","d13fad57":"data['related_views'] = 0\n\nfor index, row in data.iterrows():\n    vids = row['related_talks'].split(',')\n    counter = 0\n    total = 0\n    for views in vids:\n        if 'viewed_count' in views:\n            view = views.split(':')\n            # get rid of brackets and spaces\n            view[1] = view[1].replace(\"]\", \"\")\n            view[1] = view[1].replace(\" \", \"\")\n            view[1] = view[1].replace(\"}\", \"\")\n            total+=int(view[1])\n            counter+=1\n    data['related_views'][index] = total\/counter","b717f2c6":"data['published_date'] = data['published_date'].apply(lambda x: datetime.date.fromtimestamp(int(x)))\ndata['day'] = data['published_date'].apply(lambda x: x.weekday())\ndata['month'] = data['published_date'].apply(lambda x: x.month)\ndata['year'] = data['published_date'].apply(lambda x: x.year)\ndata['film_date'] = data['film_date'].apply(lambda x: datetime.date.fromtimestamp(int(x)))\ndata['day_film'] = data['film_date'].apply(lambda x: x.weekday())\ndata['month_film'] = data['film_date'].apply(lambda x: x.month)\ndata['year_film'] = data['film_date'].apply(lambda x: x.year)","7ca49603":"to_cat = {\"day\":   {0: \"Monday\", 1: \"Tuesday\", 2: \"Wednesday\", 3: \"Thurday\", 4: \"Friday\", 5: \"Saturday\",\n                    6: \"Sunday\" },\n          \"month\": {1: \"January\", 2: \"February\", 3: \"March\", 4: \"April\", 5: \"May\", 6: \"June\", 7: \"July\", 8: \"August\", \n                    9: \"September\", 10: \"October\", 11: \"November\", 12: \"December\"},\n          \"year\":  {2006: \"2006\", 2007: \"2007\", 2008: \"2008\", 2009: \"2009\", 2010: \"2010\", 2011: \"2011\", 2012: \"2012\", \n                    2013: \"2013\", 2014: \"2014\", 2015: \"2015\", 2016: \"2016\", 2017: \"2017\"},\n          \"day_film\":   {0: \"Monday\", 1: \"Tuesday\", 2: \"Wednesday\", 3: \"Thurday\", 4: \"Friday\", 5: \"Saturday\",\n                    6: \"Sunday\" },\n          \"month_film\": {1: \"January\", 2: \"February\", 3: \"March\", 4: \"April\", 5: \"May\", 6: \"June\", 7: \"July\", 8: \"August\", \n                    9: \"September\", 10: \"October\", 11: \"November\", 12: \"December\"},\n          \"year_film\":  {2006: \"2006\", 2007: \"2007\", 2008: \"2008\", 2009: \"2009\", 2010: \"2010\", 2011: \"2011\", 2012: \"2012\", \n                    2013: \"2013\", 2014: \"2014\", 2015: \"2015\", 2016: \"2016\", 2017: \"2017\"}}\n\ndata.replace(to_cat, inplace=True)","1b3c7ea7":"print('Number of unique events: ',data['event'].unique().shape[0])\ndata['event'].unique()","ead2e171":"# initialise all values as 'Other' to assign this category\n# to all entries that don't fit into the chosen categories\ndata['event_category'] = 'Other'\n\nfor i in range(len(data)):\n    if data['event'][i][0:5]=='TED20':\n        data['event_category'][i] = 'TED2000s'\n    elif data['event'][i][0:5]=='TED19':\n        data['event_category'][i] = 'TED1900s'\n    elif data['event'][i][0:4]=='TEDx':\n        data['event_category'][i] = \"TEDx\"\n    elif data['event'][i][0:7]=='TED@BCG':\n        data['event_category'][i] = 'TED@BCG'\n    elif data['event'][i][0:4]=='TED@':\n        data['event_category'][i] = \"TED@\"\n    elif data['event'][i][0:8]=='TEDSalon':\n        data['event_category'][i] = \"TEDSalon\"\n    elif data['event'][i][0:9]=='TEDGlobal':\n        data['event_category'][i] = 'TEDGlobal'\n    elif data['event'][i][0:8]=='TEDWomen':\n        data['event_category'][i] = 'TEDWomen'\n    elif data['event'][i][0:6]=='TEDMED':\n        data['event_category'][i] = 'TEDMED'\n    elif data['event'][i][0:3]=='TED':\n        data['event_category'][i] = 'TEDOther'","9d2fe3fa":" data['event_category'].unique()","ec4eb091":"import ast\ndestring = []\nfor number in range(len(data)):\n    #Remove string\n    destring.append(ast.literal_eval(data['tags'][number]))\ndata['Tags'] = pd.Series(destring)","0aacb38f":"from gensim.models import KeyedVectors\nmodel = KeyedVectors.load_word2vec_format(\"..\/input\/googlenewsvectorsnegative300\/GoogleNews-vectors-negative300.bin\", binary=True)","53121bc3":"listed = [item for sublist in destring for item in sublist]\nlisted = pd.Series(listed)\nlists = list(listed.unique())\nlists2 = [ x for x in lists if \" \" not in x ]\nlists2 = [ x for x in lists2 if \"-\" not in x ]","4e486d75":"lists2.remove('archaeology')\nlists2.remove('TEDYouth')\nlists2.remove('deextinction')\nlists2.remove('blockchain')\nlists2.remove('TEDNYC')","c062db78":"from sklearn.manifold import TSNE\nfrom sklearn.cluster import KMeans\n\nlabels = []\ntokens = []\n\nfor word in lists2:\n    tokens.append(model[word])\n    labels.append(word)\n\ntsne_model = TSNE(perplexity=50, n_components=2, init='pca', n_iter=105000, random_state=17,learning_rate=5500)\nnew_values = tsne_model.fit_transform(tokens)\n\nkmeans = KMeans(n_clusters=15,n_init=200)\nkmeans.fit(tokens)\nclusters = kmeans.predict(tokens)\n\ndf_tsne = pd.DataFrame(new_values, columns=['1st_Comp', '2nd_Comp'])\ndf_tsne['Cluster'] = clusters\n\nsns.lmplot(x='1st_Comp', y='2nd_Comp', data=df_tsne, hue='Cluster', fit_reg=False)\nplt.title(\"Tag Clusters\")","34e847d9":"convert = {labels[word]: clusters[word] for word in range(len(labels))}","7b3d6aed":"comp = pd.DataFrame(labels)\ncomp['cluster'] = clusters","00d028d9":"comp_conver = {0:'Organizing\/Perceiving Information',1:'animals\/organisms',2:'exploration',3:'Scientific Fields',\n              4:'media\/entertainment',5:'arts\/creativity',6:'Epidemics',7:'Humanity\/Progress',8:'Vices\/Prejudices',\n              9:'robots\/prosthetics',10:'music',11:'philanthropy\/religion',12:'Middle East',13:'Global issues',\n              14:'Outer-Space',15:'NA'}","a1198ddb":"comp['group'] = 'None'\nfor ii in range(len(comp)):\n    comp['group'][ii] = comp_conver[comp['cluster'][ii]]\n    \nunique = comp['group'].unique()","45e31fcd":"for group in unique:\n    data[group+'_tag'] = 0\n    for item in range(len(data['Tags'])):\n        for ii in data['Tags'][item]:\n            try:\n                clust = convert[ii]\n            except KeyError:\n                clust = 15\n            grouping = comp_conver[clust]\n            if grouping == group:\n                data[group+'_tag'][item] = 1","3483b986":"data.filter(like='_tag', axis=1).head()","a77a540d":"views = data['views']\ncomments = data['comments']\ndata = data.drop(['comments', 'description', 'event', 'film_date', 'main_speaker', 'name', 'published_date', 'ratings', \n           'related_talks', 'tags', 'title', 'url', 'views', 'speaker_occupation', 'Tags'], 1)\ndata.head()","00f9b5f7":"# data2 = data.filter(like='_tag', axis=1)\n# data = data.drop(data2.columns, 1)\n# data.head()\ndata_final = pd.get_dummies(data)\ndata_final.shape","1377448b":"X_train, X_test, y_train, y_test = train_test_split(data_final, views, test_size=0.1, random_state=121212)","763e4e05":"rf = RandomForestRegressor(criterion='mae',max_depth=15, max_features=45, n_estimators=500, min_samples_leaf=2, min_samples_split=2,\n                           random_state=2019)\nrf.fit(X_train, y_train)\ny_pred = rf.predict(X_train)\ny_test_pred = rf.predict(X_test)\nprint('Training MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_train, y_pred)))\nprint('Test MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_test, y_test_pred)))\nprint('Views mean: {:0.2f}'.format(views.mean()))\nprint('Views std: {:0.2f}'.format(views.std()))","62047b71":"importances = pd.DataFrame({'Features': X_train.columns, \n                                'Importances': rf.feature_importances_})\n    \nimportances.sort_values(by=['Importances'], axis='index', ascending=False, inplace=True)\nfig = plt.figure(figsize=(14, 4))\nsns.barplot(x='Features', y='Importances', data=importances)\nplt.xticks(rotation='vertical')\nplt.show()","1457e682":"xgbr = xgb.XGBRegressor(criterion='mae', earning_rate=0.1, max_depth=10, subsample=0.5, n_estimators=20, min_child_weight=2, random_state=2019)\nxgbr.fit(X_train, y_train)\ny_pred = xgbr.predict(X_train)\ny_test_pred = xgbr.predict(X_test)\nprint('Training MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_train, y_pred)))\nprint('Test MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_test, y_test_pred)))","598d3d52":"et = ExtraTreesRegressor(criterion='mae', max_depth=30, n_estimators=1000, random_state=2019, min_samples_leaf=2, min_samples_split=6)\net.fit(X_train, y_train)\ny_pred = et.predict(X_train)\ny_test_pred = et.predict(X_test)\nprint('Training MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_train, y_pred)))\nprint('Test MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_test, y_test_pred)))","c256d89c":"lgbm = lgb.LGBMRegressor(max_depth=5, n_estimators=50, random_state=2019)\nlgbm.fit(X_train, y_train)\ny_pred = lgbm.predict(X_train)\ny_test_pred = lgbm.predict(X_test)\nprint('Training MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_train, y_pred)))\nprint('Test MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_test, y_test_pred)))","91a666d3":"# stacking_regressor = StackingCVRegressor(regressors=[xgbr,\n#                                             rf,\n#                                             et,\n#                                             lgbm],\n#                                            cv=3,\n#                             use_features_in_secondary=True,\n#                             verbose=2,\n#                             random_state=0,\n#                             n_jobs=-1,\n#                             meta_regressor=rf)\n","3011feab":"# stacking_regressor.fit(X_train, y_train)","c621c719":"# y_pred = stacking_regressor.predict(X_train)\n# y_test_pred = stacking_regressor.predict(X_test)\n# print('Training MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_train, y_pred)))\n# print('Test MAE: {:0.2f}'.format(metrics.mean_absolute_error(y_test, y_test_pred)))","a4d43318":"The baseline Random Forest model seems to have overfitted. Together with the **MAE** of the model I also print the **mean** and **std** of the target variables. Judging by the high variance of the data, it's safe to conclude that the model is performing reasonably well.","092d6192":"## tags\nUsing KeyedVectors to encode and categorise the information from the provided tags.","7abda5fc":"The dataset has 355 unique event names but from the looks of it, lots of these names can be categorised together as they are quite similar. I break down the event names in the following 11 categories (each consisting of at least 5 samples).","f3ab49b1":"# Predicting TED Talks Views with ML Models\nIn this notebook I've done some simple feature engineering on the TED Talks dataset and I've built machine learning models (Random Forest, XGBRegressor, ExtraTreesRegressor, and LGBMRegressor) and optimised their hyperparameters to predict the number of TED Talks views. I've made use of the following kernels to create this notebook:\n* https:\/\/www.kaggle.com\/rounakbanik\/ted-data-analysis\n* https:\/\/www.kaggle.com\/holfyuen\/what-makes-a-popular-ted-talk\n* https:\/\/www.kaggle.com\/tristanmoser\/predicting-a-powerful-idea-a-ted-talk-analysis\n\nI may have used some of the other available notebooks and forgot to add them here, I apologise in advance if that's the case. Please don't shy away from providing feedback and making suggestions on how to improve the accuracy of the models. At the end of the notebook I've listed my plans for future work, feel free to make suggestions on what I should include there.\n\nI start off by loading the TED Talks dataset and libraries which will be needed to analyse the dataset and to build a model to predict the views of the talks. I will load the dataset first, then check if the data has any null values, and pick out the parameters that I'll use for building the machine learning models.","b66a8e55":"## related_talks\nHere I print out the **related_talks** feature so I check what it looks like.","6c981c7b":"# Machine Learning\n## Random Forest\nFirst model I will be testing with is **Random Forest** as this is the one I'm most comfortable with. I will then explore some other ML models, optimise the hyperparameters of each model, and combine those into an ensebmle model.\n\nI split the dataset in training (90%) and test (10%) sets. The test data will be later used to validate the ML models on unseen data. I start off with a **Random Forest** as it's quite a powerful model that can be used as a baseline. I will use **Mean Absolute Error (MAE)** to measure the error as it will give us a more intuitive understanding of how accurate the model is. Additionally, using **Mean Squared Error (MSE)** to predict target variables with large values (such as the TED Talks views I'm working with) can lead to problems.","9c416d21":"# Future Work\n- Improve feature engineering\n- Remove unimportant and correlated features\n- Normalise the data\n- Improve the hyperparameters of the models\n- Use PCA\n","c059d4c6":"# Any Suggestions?\nShould you have any ideas or suggestions on how to improve this notebook (do some actual data analysis and feature engineering, improve the accuracy of the models, or anything else), please don't hesitate to give me a shout in the comments!","5acc663c":"I apply **One-Hot-Encoding** on the categorical attributes and get the data ready for training machine learning models. Then I print out the dimensions of the final dataset.","fe56929e":"## ExtraTreesRegressor\nExtraTreesRegressor yields the best accuracy, maybe can reguralise the model better as the gap between Training and Test MAE is quite big?","99846bb1":"Here I categorise the data which is preferable over using numbers.","fe9cf6ef":"## Final touches on the dataset\nI take out the views and comments and the rest of the features I won't be using.","8586b4f8":"After that I split the string by its commas and then by the semi-column to get the middle element, which is the views of all related talks.","ff9c7511":"# TED Talks Data Analysis\n## Cleaning up the data\nVarious datasets frequently have missing values, so I start off by checking whether the TED Talks dataset has any. ","167425f3":"## StackingCVRegressor\nWill implement this later..","44d3013f":"## LGBMRegressor\nCouldn't find other hypermarameters except for **max_depth** and **n_estimators** which improve the model's accuracy, more work to be done here..","cc52e83e":"There are only 6 null values in the **speaker_occupation** feature, I will fill in those missing values with a default 'Other' value.","9b6c0822":"# Conclusion\nAmong the ML models I experimented with, ExtraTreesRegressor returns the best **Test MAE** of 597858.20. It seems feasible to decrease the MAE down to 550000 or even further if I experiment more with the model hyperparameters and features.","6b03b759":"I check whether each categoies can be found in the dataset.","83a0000a":"## event\nI check the number of unique event names then list all of them:","473f9e3c":"## XGBRegressor\nGood accuracy, the model tends to overfit quite easily when **n_estimators > 20**. Maybe it's worth exploring wether we can use a higher **n_estimators** value while using the other hyperparameters to reguralise the model.","bd1ef12f":"To perform well on predicting TED Talks video views, I try to use as much features from the dataset as possible. Nevertheless, I have decided not to use some of the parameters (e.g., url, main speaker name, etc.) because they won't be much useful in predicting the views. Using some of the features (such as description, tags) I've left for further work.\n- **duration** - duration of the video\n- **event** - name of the event of which the talk is part of\n- **languages** - number of languages in which the talk is available in\n- **num_speaker** - number of speakers in the talk\n- **film_date**, **published_date** - date of filming and publishing the talk, from which I extract:\n  - **day of the week**\n  - **month**\n  - **year**\n- **related-talks** - an array that consists of 6 related talks, from which I extract the average number of views\n\nI've excluded the **comments** and **ratings** features, as using those I consider cheating. The point of the task is to predict the number of views for a video which has just been released or is yet to be released. After going through the data analysis notebooks I mentined earlier, I decided to exclude the following features:\n- **comments** - number of comments on the video\n- **ratings** - number of times the video has been rated\n- **name** - name of the talk, which includes the name main speaker and title of the talk\n- **main speaker** - name of the main speaker that leads the talk, we rarely see the same speaker do more than 1 talk \n- **title** - title of the talk\n- **url** - url link to the talk\n\nThe following features I leave for future work:\n- **description** - description of the talk, will need to encode this information\n- **tags** - tags that are associated with the talk\n- **speaker_occupation** - occupation of the main speaker","df51174a":"I plot the feature importances that are derived from the model that was just trained. I will use this in the future to exclude the unimportant features with the hope of boosting performance.","1215b0ff":"## published_date, filmed_date\nFrom these two features I extract day of the week, month, and year."}}