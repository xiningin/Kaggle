{"cell_type":{"fea5eae9":"code","8ac74458":"code","a63e29ec":"code","16312ffa":"code","6c1ddd2e":"code","ef12d1f4":"code","26da0a7f":"code","afac932b":"code","930a0b83":"code","28ed5e3b":"code","9468a3cc":"code","4f6fb350":"code","abd22cf3":"code","8dae79b4":"code","0891d33b":"code","b5de569e":"code","788bdf48":"code","de3f9b34":"code","81fcf88a":"code","555b16e0":"code","57b25907":"code","86bec671":"code","66cbbaee":"code","661bb959":"code","19dd8458":"code","4553d721":"code","3724155e":"code","ec122aa7":"code","dd9dfec6":"code","81288b59":"code","10783251":"code","6a147d4a":"code","8fd216b7":"code","487e4729":"code","7b9fe86d":"code","c39092c7":"code","8c27cc89":"code","edb92151":"code","770ed876":"code","98e4506a":"code","b5787a05":"code","840c72f3":"code","c6d96954":"code","ad70fae2":"code","64acfed4":"code","e415f2df":"code","412410a7":"code","bc25726c":"code","d9d6e8ad":"code","1ba01a24":"code","9d2aa394":"code","883628e8":"markdown","0be8871b":"markdown","0b3c4ab4":"markdown","3a7e6b2c":"markdown","1c2fe14b":"markdown","b127a8aa":"markdown","de7d2c4c":"markdown","4bad3094":"markdown","893ed5a6":"markdown","13e1fdd3":"markdown","41a0fb87":"markdown","654da8f8":"markdown","a1dd5810":"markdown"},"source":{"fea5eae9":"# import the libraries requiered for this task\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport operator\nimport time\n\nfrom sklearn.impute import KNNImputer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.cluster import KMeans\n# magic word for producing visualizations in notebook\n%matplotlib inline","8ac74458":"# import train and test dataset\ndf_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","a63e29ec":"# Check the structure of the data \ntrain_rows,train_columns=df_train.shape\ntest_rows,test_columns=df_test.shape\nprint(\"number of rows in train dataset :{}\".format(train_rows),\"\\n\"\n      \"number of columns in train dataset :{}\".format(train_columns))\n\nprint(\"number of rows in test Summary :{}\".format(test_rows),\"\\n\"\n      \"number of columns in test Summary :{}\".format(test_columns))","16312ffa":"# check the target variable does exist \nprint('SalePrice' in df_train.columns)\nprint('SalePrice' in df_test.columns)","6c1ddd2e":"df_train.head()","ef12d1f4":"target= df_train[['SalePrice']]\ntarget.head()","26da0a7f":"#drop the SalePrice column\ndf_train.drop(['SalePrice','Id'],axis='columns', inplace=True)","afac932b":"#check again\nprint('SalePrice' in df_train.columns)","930a0b83":"# check the null value in each column\ndf_all = df_train.copy()\ndf_all.info()","28ed5e3b":"# Get the null values in separet dataframe\ndf_null = df_all.isnull().sum()\ndf_null.sort_values(inplace=True,ascending=False)\ndf_null","9468a3cc":"# calculate the percentage of nulls in each variable\ndf_null=df_null[df_null>0]\/(df_all.shape[0])*100\ndf_null","4f6fb350":"# Perform first visual assessment for missing data in each columns\nplt.hist(df_null)\n\nplt.xlabel('% of Null for each column')\nplt.ylabel('Counts')\nplt.title(' Null Values Histogram')\nplt.show()","abd22cf3":"# Perform second visual assessment for missing data in each columns\n# Identify the patterns in the amount of null data in each column.\ndf_null.plot.barh(figsize=(20,30))\nplt.xlabel('% of null values')\nplt.ylabel('column name')\nplt.show()","8dae79b4":"# assign 10% as a threshold\noutliers_col = df_null[df_null>10].index\noutliers_col","0891d33b":"df_all.drop(columns=outliers_col,axis=\"columns\",inplace=True)\ndf_all.shape","b5de569e":"# get the rows with null values and plot it\ndf_null_r=df_all.isnull().sum(axis=1)\n\nplt.hist(df_null_r)\nplt.xlabel('% of Null for each row')\nplt.ylabel('Counts')\nplt.title('Null Values Histogram')\nplt.show()\n","788bdf48":"# get the rows with null values and plot it\ndf_null_r=df_all.isnull().sum(axis=1)\nrow_less_1=df_all[df_null_r<1].sum(axis=1)\nrow_less_1.plot.barh(figsize=(20,30))\nplt.xlabel('No of null values')\nplt.ylabel('Rows have less than 1% null')\nplt.yticks([])\n\nplt.show()","de3f9b34":"#fill the na value by the mode of the column\ndf_null_r =row_less_1.fillna(row_less_1.mode().iloc[0])","81fcf88a":"df_all.info()","555b16e0":"#get the object columns\ndf_dum = df_all.loc[:,df_all.dtypes==np.object]","57b25907":"df_dum.shape","86bec671":"# 38 columns are categorical and will convert to dummy\ndf_dumm=pd.get_dummies(df_dum)\ndf_dumm.shape","66cbbaee":"df_dumm.isnull().sum().sum()\n","661bb959":"#get numeric columns\ndf_num = df_all.loc[:,df_all.dtypes!=np.object]\ndf_num.columns","19dd8458":"# check the nullb\ndf_num.isnull().sum().sum()","4553d721":"# imput the null by KNN imputer \n#https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.KNNImputer.html#sklearn.impute.KNNImputer\nimput=KNNImputer(n_neighbors=2, weights=\"uniform\")\nnum = ['MSSubClass', 'LotArea', 'OverallQual', 'OverallCond',\n       'YearBuilt', 'YearRemodAdd', 'MasVnrArea', 'BsmtFinSF1', 'BsmtFinSF2',\n       'BsmtUnfSF', 'TotalBsmtSF', '1stFlrSF', '2ndFlrSF', 'LowQualFinSF',\n       'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath', 'HalfBath',\n       'BedroomAbvGr', 'KitchenAbvGr', 'TotRmsAbvGrd', 'Fireplaces',\n       'GarageYrBlt', 'GarageCars', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF',\n       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'MiscVal',\n       'MoSold', 'YrSold']\ndf_num[num] = imput.fit_transform(df_num[num])\ndf_num.isnull().sum().sum()","3724155e":"# merge the dummy df and numerical df\n# merge again both df\ndf_encoded = pd.concat(\n    [df_num,df_dumm],\n    axis=1,\n)","ec122aa7":"enc=df_encoded.columns","dd9dfec6":"# Apply feature scaling to the encoded data.\nscaler =StandardScaler()\n\ndf_encoded[enc]=scaler.fit_transform(df_encoded[enc])","81288b59":"print(df_encoded.head().T)\nprint('*'*100)\nprint('The dataset will be used in clustering having {} columns.'.format(df_encoded.shape[1]))\nprint('*'*100)\nprint('The null value in the encoded data set is', df_encoded.isnull().sum().sum())","10783251":"# Apply PCA to the data.\npca = PCA()\npca.fit(df_encoded)","6a147d4a":"# Investigate the variance accounted for by each principal component.\nnum_components=np.arange(len(pca.explained_variance_ratio_))\nvalue=pca.explained_variance_ratio_\nplt.bar(num_components,value)\nplt.xlabel(\"Principal component\")\nplt.ylabel(\"Explained variance\")\nplt.title(\"Explained variance per principal component\")\nplt.show()","8fd216b7":"# Investigate the variance accounted for by each principal component.\ndef scree_plot(pca):\n    num_components = len(pca.explained_variance_ratio_)\n    index = np.arange(num_components)\n    values = pca.explained_variance_ratio_\n \n    plt.figure(figsize=(20, 10))\n    ax = plt.subplot()\n    cumvalues = np.cumsum(values)\n    ax.bar(index, values)\n    ax.plot(index, cumvalues)\n    ax.xaxis.set_tick_params(width=0)\n    ax.yaxis.set_tick_params(width=2, length=12)\n \n    ax.set_xlabel(\"Principal Component\")\n    ax.set_ylabel(\"Variance Explained(%)\")\n    plt.title('Explained Variance Per Principal Component\\n')\n\n\nscree_plot(pca)","487e4729":"# reduce the number of variabels by re apply PCA with the number that variance start to retain\n# which equal 50\npca_75 = PCA(n_components=75)\nreduced_data = pca_75.fit_transform(df_encoded)","7b9fe86d":"def WFPCA(dataset,pca,n):\n    ''' This function map the weight for each feature in data set \n        dataset: the encoded dataset\n        INPUT:\n            pca: the reduced pca applied in the previous step based on the varinace number assigned\n            n: number of featrues (3 features)\n        Output:\n            w_dataset: The weight for each feature in dataset'''\n    #assign the pca compnont output to data frame\n    w_dataset=pd.DataFrame(pca.components_,columns=list(dataset.columns)).iloc[n]\n    #sort the result \n    w_dataset.sort_values(ascending=False, inplace=True)\n    return w_dataset","c39092c7":"# first feature\nWFPCA(df_encoded,pca_75,0)","8c27cc89":"# second feature\nWFPCA(df_encoded,pca_75,1)","edb92151":"#third featur\nWFPCA(df_encoded,pca_75,2)","770ed876":"#define clustering function\ndef KM_score(data, k):\n    '''\n    returns the kmeans score regarding SSE for points to centers\n    INPUT:\n        data - the encoded dataset \n        center - the porposed number of clustering centers \n    OUTPUT:\n        score - the SSE score for the kmeans model fit to the data\n    '''\n    #assign instant\n    KM = KMeans(n_clusters=k)\n    # fit the model\n    model = KM.fit(data)\n    # score  the model related to the model fit\n    score = np.abs(model.score(data)) #absolute value taken\n    \n    return score","98e4506a":"# Over a number of different cluster counts...\ntime_start = time.time()\n\nscores = []\ncenters = list(range(1,20))\n\nfor k in centers:\n    scores.append(KM_score(reduced_data, k))\n    \nplt.plot(centers, scores, linestyle='--', marker='o', color='b');\nplt.xlabel('K');\nplt.ylabel('Score');\nplt.title('Score vs. Number of clusters');\n\nprint(f'Run time: {round(((time.time()-time_start)\/60), 3)} mins')","b5787a05":"# apply the number of clusters (5 based on Elbow method) and obtain the predictions \ntime_start = time.time()\n\nkm = KMeans(n_clusters =5 ) \nmodel = km.fit(reduced_data) \nclustered_data = model.predict(reduced_data) #returning array of indeces for the cluster each sample belongs to.\n\n#display (pd.DataFrame (azdias_clust))\nprint(f'Run time: {round(((time.time()-time_start)\/60), 3)} mins')\n","840c72f3":"plt.hist(clustered_data);","c6d96954":"#apply DBSCAN\nfrom sklearn import cluster\nfrom sklearn.cluster import DBSCAN\n#create an instance of DBSCAN\ndbscan = cluster.DBSCAN(eps=4)\n# use DBSCAN's fit_predict to return clustering labels for dataset_1\nclustering_labels_1 = dbscan.fit_predict(reduced_data)","ad70fae2":"plt.hist(clustering_labels_1);","64acfed4":"# Apply GaussianMixture\nfrom sklearn.mixture import GaussianMixture\n#  Create an instance of Gaussian Mixture with 5 components\ngmm = GaussianMixture(n_components=5).fit(reduced_data)\n#fit the dataset\ngmm = gmm.fit(reduced_data)\n\n# predict the clustering labels for the dataset\npred_gmm = gmm.predict(reduced_data)","e415f2df":"plt.hist(pred_gmm);","412410a7":"# will apply on the most correlated feature\n#add the saleprice column again\ndf_encoded['SalePrice']= target['SalePrice']\n#apply Gaussian Mixture \ngmm_sp = gmm.fit_predict(df_encoded[['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', \n                                     'TotalBsmtSF', 'FullBath', 'YearBuilt']])\n#apply DBSCAN\nDBscan = dbscan.fit_predict(df_encoded[['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', \n                                     'TotalBsmtSF', 'FullBath', 'YearBuilt']])\n# apply Kmean\nkm = model.fit_predict(df_encoded[['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', \n                                     'TotalBsmtSF', 'FullBath', 'YearBuilt']])\n","bc25726c":"df_encoded['kmeans_pred'] = km\ndf_encoded['DBSCAN_pred'] = DBscan\ndf_encoded['gmm_sp'] = gmm_sp","d9d6e8ad":"g = sns.PairGrid(df_encoded, hue=\"gmm_sp\", palette=sns.color_palette(\"cubehelix\", 3), \n                 vars=['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt'])\ng.map(plt.scatter)\nplt.show()","1ba01a24":"d = sns.PairGrid(df_encoded, hue=\"DBSCAN_pred\", palette=sns.color_palette(\"cubehelix\", 3), \n                 vars=['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt'])\nd.map(plt.scatter)\nplt.show()","9d2aa394":"k = sns.PairGrid(df_encoded, hue=\"kmeans_pred\", palette=sns.color_palette(\"cubehelix\", 3), \n                 vars=['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt'])\nk.map(plt.scatter)\nplt.show()","883628e8":"#### Step 1.3: Re-Encode Features\n","0be8871b":"#### Step 1.: Remove the target variable","0b3c4ab4":"# Task 2: Identify House Prices\nIn this section, unsupervised learning techniques will be applied to identify correlations between independent variables and patterns that the simple analytics techniques didn\u2019t. The price column will be execluded and will be added later\n.","3a7e6b2c":"#### Interpret Principal Components\n","1c2fe14b":"### after 3 different clustring models are applied, will test all on the data set to check the prediction","b127a8aa":"### Step 3: Clustring\n","de7d2c4c":"#### Step 1.2: Assess Null Data in Each Row\n\nIn this stage, an assessment for the rows of the dataset will be performed to answer the following questions. How much data is missing in each row? As with the columns, Is there any asssumed threshold can allow to keep the null values?.  The data will be Divide into two subsets: one for data points that are above some threshold for null values, and a second subset for points below that threshold.","4bad3094":"### Step 2: Perform Dimensionality Reduction\n Interpret Principal Components","893ed5a6":"#### As the methodology have done with the columns, it will be used for rows. The rows with null values will be ploted to assign a certain threshold.","13e1fdd3":"#### Based on the visual assessment, there are 6 columns have more than 10% of its values as null, the mentioned columns will be removed as outliers columns.","41a0fb87":"#### Apply models to the Data","654da8f8":"### the following cells will plot the prediction trhough PairGrid","a1dd5810":"## Step 1: Preprocessing\n\n"}}