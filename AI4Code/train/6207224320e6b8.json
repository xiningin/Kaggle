{"cell_type":{"ecf65044":"code","b3b6d74a":"code","365607d9":"code","5a7ada83":"code","2ff4fc4a":"code","5c987dc2":"code","c8816d03":"code","5d76bf4b":"code","1b9be727":"code","083c791b":"code","8bb36046":"code","b53db9e4":"code","ff5a110d":"code","09327f98":"code","450cf22e":"code","6fafb382":"markdown","a35441eb":"markdown","e34d67e4":"markdown","c08fc9c5":"markdown","07b79d56":"markdown","d3b0cab4":"markdown","a4328972":"markdown"},"source":{"ecf65044":"import os\n\nimport numpy as np\nimport gc\nimport pandas as pd\nimport tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport transformers\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom tqdm.notebook import tqdm\nimport numpy as np\nfrom keras.utils import np_utils\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import f1_score\nfrom tensorflow.keras.callbacks import Callback \nfrom tokenizers import Tokenizer, models, pre_tokenizers, decoders, processors\n\nstrategy = tf.distribute.get_strategy()","b3b6d74a":"def regular_encode(texts, tokenizer, maxlen=512):\n    \"\"\"\n    encodes text for a model\n    \"\"\"\n    enc_di = tokenizer.batch_encode_plus(\n        texts,\n        return_token_type_ids=False,\n        pad_to_max_length=True,\n        max_length=maxlen\n    )\n    \n    return np.array(enc_di['input_ids'])","365607d9":"def build_model(transformer, max_len=512, hidden_dim=32, n_classes=1):\n    \"\"\"\n    builds a model\n    \"\"\"\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    sequence_output = transformer(input_word_ids)[0]\n    cls_token = sequence_output[:, 0, :]\n    \n    if n_classes == 2: # binary classification\n        out = Dense(1, activation='sigmoid')(cls_token)\n    else:\n        out = Dense(n_classes, activation='sigmoid')(cls_token)\n    \n    model = Model(inputs=input_word_ids, outputs=out)\n    \n    if n_classes > 2:\n        model.compile(Adam(lr=1e-5), loss='binary_crossentropy', metrics=['accuracy'])\n    else:\n        model.compile(Adam(lr=1e-5), loss='categorical_crossentropy', metrics=['accuracy'])\n    \n    return model","5a7ada83":"AUTO = tf.data.experimental.AUTOTUNE\n\n# Configuration\nEPOCHS = 2\nBATCH_SIZE = 16 * strategy.num_replicas_in_sync\nMAX_LEN = 162 #TODO: to set it correctly determine what is the average (or max) token length of your training data \nMODEL = 'distilbert-base-cased' # use any appropriate model (e.g. bert-base-cased) from https:\/\/huggingface.co\/models","2ff4fc4a":"# First load the real tokenizer\ntokenizer = AutoTokenizer.from_pretrained(MODEL)","5c987dc2":"train = pd.read_csv(\"\/kaggle\/input\/relation-predicate-prediction-dataset\/train.csv\", sep=';')\ntest = pd.read_csv(\"\/kaggle\/input\/relation-predicate-prediction-dataset\/test.csv\", sep=';')","c8816d03":"%%time \n\nx_train = regular_encode(train.question.values, tokenizer, maxlen=MAX_LEN)\nx_test = regular_encode(test.question.values, tokenizer, maxlen=MAX_LEN)\n\ny_train = train.predicate.values\ny_test = test.predicate.values\n\n# encode textual labels into corresponding numbers\nencoder = LabelEncoder()\nencoder.fit(y_train)\nencoded_y_train = encoder.transform(y_train) \nencoded_y_test = encoder.transform(y_test)\ndummy_y_train = np_utils.to_categorical(encoded_y_train) # convert integers to dummy variables (i.e. one hot encoded)","5d76bf4b":"train_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices((x_train, dummy_y_train))\n    .repeat()\n    .shuffle(2048)\n    .batch(BATCH_SIZE)\n    .prefetch(AUTO)\n)\n\ntest_dataset = (\n    tf.data.Dataset\n    .from_tensor_slices(x_test)\n    .batch(BATCH_SIZE)\n)","1b9be727":"%%time\nwith strategy.scope():\n    transformer_layer = TFAutoModel.from_pretrained(MODEL)\n    model = build_model(transformer_layer, max_len=MAX_LEN, n_classes=train.predicate.nunique())\nmodel.summary()","083c791b":"n_steps = x_train.shape[0] \/\/ BATCH_SIZE # determine number of steps per epoch\n\ntrain_history = model.fit(\n    train_dataset,\n    steps_per_epoch=n_steps,\n    epochs=EPOCHS\n)","8bb36046":"y_pred = np.argmax(model.predict(test_dataset, verbose=1), axis=1)","b53db9e4":"encoder.inverse_transform(y_pred[:5]) # show actual labels","ff5a110d":"print(\"F1 Score\", f1_score(encoded_y_test, y_pred, average='weighted'))","09327f98":"# model.save(\"classifier\")\n\n# np.save('encoder.npy', encoder.classes_)","450cf22e":"# model = tf.keras.models.load_model(\"classifier\")\n\n# encoder = LabelEncoder()\n# encoder.classes_ = np.load('path_to_encoder.npy'), allow_pickle=True)","6fafb382":"## Create tokenizer","a35441eb":"## Train Model","e34d67e4":"## Load text data into memory","c08fc9c5":"## Build datasets objects","07b79d56":"## Helper Functions","d3b0cab4":"## Load model","a4328972":"## Save \/ Load model"}}