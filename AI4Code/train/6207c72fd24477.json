{"cell_type":{"e6e1b673":"code","e5d7f8fc":"code","9506e32e":"code","386c5985":"code","410cb5b2":"code","562e92e8":"code","449a8362":"code","ef37fec2":"code","ee6395a6":"code","5f7b81c7":"code","234f4837":"code","fcaf0726":"code","c4a55d56":"code","2e11c84e":"code","1eedd882":"code","a1c61809":"code","4f3b1e81":"code","a051d729":"code","5a222d86":"markdown","ff80e2f5":"markdown","c3e9dae5":"markdown","ef7df3f0":"markdown","8dc5c625":"markdown","7a137683":"markdown","3eb0d118":"markdown","3cdeb289":"markdown","5c161314":"markdown","19b3fb86":"markdown","8af1abaa":"markdown","a8ea7508":"markdown","4ce2d450":"markdown","e965d1b8":"markdown","ec161f78":"markdown","aa6d8cab":"markdown","1be69d44":"markdown","21e8fbf9":"markdown","2874d08e":"markdown","60b5e65c":"markdown"},"source":{"e6e1b673":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e5d7f8fc":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline","9506e32e":"path = \"..\/input\/housesalesprediction\/kc_house_data.csv\"\ndf = pd.read_csv(path)","386c5985":"df.head(9)","410cb5b2":"df.describe()","562e92e8":"cdf = df[['id','price','bedrooms','bathrooms','sqft_living','floors']]","449a8362":"cdf.head()","ef37fec2":"viz = cdf[['id','floors','sqft_living','bedrooms','bathrooms','price']]","ee6395a6":"viz = cdf.hist()\nplt.show()","5f7b81c7":"plt.scatter(cdf.bedrooms, cdf.price,  color='blue')\nplt.xlabel(\"The number of bedrooms\")\nplt.ylabel(\"The price in USD\")\nplt.show()","234f4837":"plt.scatter(cdf.bathrooms, cdf.price,  color='blue')\nplt.xlabel(\"The number of bathrooms\")\nplt.ylabel(\"The price in USD\")\nplt.show()","fcaf0726":"plt.scatter(cdf.sqft_living, cdf.price,  color='blue')\nplt.xlabel(\"The square feet living\")\nplt.ylabel(\"The price in USD\")\nplt.show()","c4a55d56":"msk = np.random.rand(len(df)) < 0.8\ntrain = cdf[msk]\ntest = cdf[~msk]","2e11c84e":"from sklearn import linear_model\nreg = linear_model.LinearRegression()\ntrain_x = np.asanyarray(train[['sqft_living']])\ntrain_y = np.asanyarray(train[['price']])\nreg.fit(train_x,train_y)\n# The coefficients\nprint ('Coefficients: ', reg.coef_)\nprint('Intercept:', reg.intercept_)","1eedd882":"plt.scatter(train.sqft_living, train.price,  color='blue')\nplt.plot(train_x, reg.coef_[0][0]*train_x + reg.intercept_[0], '-r')\nplt.xlabel(\"The squared feet living\")\nplt.ylabel(\"The price in USD\")","a1c61809":"from sklearn.metrics import r2_score\n\ntest_x = np.asanyarray(test[['sqft_living']])\ntest_y = np.asanyarray(test[['price']])\ntest_y_hat = reg.predict(test_x)\n\nprint(\"Mean absolute error (MAE) : %.2f\" % np.mean(np.absolute(test_y_hat - test_y)))\nprint(\"Residual sum of squares (MSE): %.2f\" % np.mean((test_y_hat - test_y) ** 2))\nprint(\"R2-score: %.2f\" % r2_score(test_y_hat , test_y) )","4f3b1e81":"from sklearn import linear_model\nreg = linear_model.LinearRegression()\nx = np.asanyarray(train[['bedrooms','bathrooms','sqft_living','floors']])\ny = np.asanyarray(train[['price']])\nreg.fit(x,y)\n# The coefficients\nprint ('Intercept: ', reg.intercept_)\nprint ('Coefficients: ', reg.coef_)","a051d729":"y_hat= reg.predict(test[['bedrooms','bathrooms','sqft_living','floors']])\nx = np.asanyarray(test[['bedrooms','bathrooms','sqft_living','floors']])\ny = np.asanyarray(test[['price']])\nprint(\"Residual sum of squares: %.2f\"\n      % np.mean((y_hat - y) ** 2))\n\n# Explained variance score: 1 is perfect prediction\nprint('Variance score: %.2f' % reg.score(x, y))","5a222d86":"**<h4>About this Notebook<\/h4>**\n<p> we often use <b>Model Development<\/b> to help us predict future observations from the data we have.<\/p>\n<p>So, a Model will help us understand the exact relationship between different variables and how these variables are used to predict the result.<\/p>","ff80e2f5":"__explained variance regression score:__  \nIf $\\hat{y}$ is the estimated target output, y the corresponding (correct) target output, and Var is Variance, the square of the standard deviation, then the explained variance is estimated as follow:\n\n$\\texttt{explainedVariance}(y, \\hat{y}) = 1 - \\frac{Var\\{ y - \\hat{y}\\}}{Var\\{y\\}}$  \nThe best possible score is 1.0, lower values are worse.","c3e9dae5":"$$\nY: Target \\ Variable\\\\\nX_1 :Predictor\\ Variable \\ 1\\\\\nX_2: Predictor\\ Variable \\ 2\\\\\nX_3: Predictor\\ Variable \\ 3\\\\\nX_4: Predictor\\ Variable \\ 4\\\\\n$$","ef7df3f0":"<h2 id=\"multiple_regression_model\">II. Multiple Regression Model<\/h2>","8dc5c625":"we can plot the fit line over the data:","7a137683":"The equation is given by:\n$$\nYp = a + b_1 X_1 + b_2 X_2 + b_3 X_3 + b_4 X_4\n$$","3eb0d118":"#### **Plot outputs**","3cdeb289":" <h2 id=\"prediction\">Prediction<\/h2>","5c161314":"#### **Train data distribution**\nUsing sklearn package to model data.","19b3fb86":"In reality, there are multiple variables that predict the price. When more than one independent variable is present, the process is called multiple linear regression. For example, predicting the price using the number of bedrooms, bathrooms, floors and sqft_living in the house . The good thing here is that Multiple linear regression is the extension of simple linear regression model.","8af1abaa":"#### Evaluation\nwe compare the actual values and predicted values to calculate the accuracy of a regression model. Evaluation metrics provide a key role in the development of a model, as it provides insight to areas that require improvement.\n\nThere are different model evaluation metrics, lets use MSE here to calculate the accuracy of our model based on the test set: \n<ul>\n    <li> Mean absolute error: It is the mean of the absolute value of the errors. This is the easiest of the metrics to understand since it\u2019s just average error.<\/li>\n    <li> Mean Squared Error (MSE): Mean Squared Error (MSE) is the mean of the squared error. It\u2019s more popular than Mean absolute error because the focus is geared more towards large errors. This is due to the squared term exponentially increasing larger errors in comparison to smaller ones.<\/li>\n    <li> Root Mean Squared Error (RMSE): This is the square root of the Mean Square Error. <\/li>\n    <li> R-squared is not error, but is a popular metric for accuracy of your model. It represents how close the data are to the fitted regression line. The higher the R-squared, the better the model fits your data. Best possible score is 1.0 and it can be negative (because the model can be arbitrarily worse).<\/li>\n<\/ul>","a8ea7508":" <h1> Welcome to this Kernel <\/h1>","4ce2d450":"**Now, lets plot each of these features vs the price, to see how linear is their relation:**","e965d1b8":"<h3>Thanks for completing this lesson!<\/h3>\n\n<h4>Author:  <a href=\"https:\/\/www.linkedin.com\/in\/ibrahim-bahbah-491435172\/\">Ibrahim BAHBAH<\/a><\/h4>\n<p><a href=\"https:\/\/www.linkedin.com\/in\/ibrahim-bahbah-491435172\/i\">Ibrahim BAHBAH<\/a>, An ambitious data science student who's striving to apply the data-driven approach for problem-solving.<\/p>","ec161f78":"As mentioned before, __Coefficient__ and __Intercept__ , are the parameters of the fit line. \nGiven that it is a multiple linear regression, with 5 parameters, and knowing that the parameters are the intercept and coefficients of hyperplane, sklearn can estimate them from our data. Scikit-learn uses plain Ordinary Least Squares method to solve this problem.\n\n#### Ordinary Least Squares (OLS)\nOLS is a method for estimating the unknown parameters in a linear regression model. OLS chooses the parameters of a linear function of a set of explanatory variables by minimizing the sum of the squares of the differences between the target dependent variable and those predicted by the linear function. In other words, it tries to minimizes the sum of squared errors (SSE) or mean squared error (MSE) between the target variable (y) and our predicted output ($\\hat{y}$) over all samples in the dataset.\n\nOLS can find the best parameters using of the following methods:\n    - Solving the model parameters analytically using closed-form equations\n    - Using an optimization algorithm (Gradient Descent, Stochastic Gradient Descent, Newton\u2019s Method, etc.)","aa6d8cab":"<p>What if we want to predict Violent crime total using more than one variable?<\/p>\n\n<p>If we want to use more variables in our model to predict Violent crime total, we can use <b>**Multiple Linear Regression**<\/b>.\nMultiple Linear Regression is very similar to Simple Linear Regression, but this method is used to explain the relationship between one continuous response or target (dependent) variable and <b>two or more<\/b> predictor (independent) variables.\nMost of the real-world regression models involve multiple predictors. We will illustrate the structure by using four predictor variables, but these results can generalize to any integer:<\/p>","1be69d44":" __Coefficient__ and __Intercept__ in the simple linear regression, are the parameters of the fit line. \nGiven that it is a simple linear regression, with only 2 parameters, and knowing that the parameters are the intercept and slope of the line, sklearn can estimate them directly from our data. \nNotice that all of the data must be available to traverse and calculate the parameters.","21e8fbf9":"<h2 id=\"simple_regression\">I. Simple Regression Model<\/h2>\nLinear Regression fits a linear model with coefficients $\\theta = (\\theta_1, ..., \\theta_n)$ to minimize the 'residual sum of squares' between the independent x in the dataset, and the dependent y by the linear approximation. ","2874d08e":"#### Creating train and test dataset\nTrain\/Test Split involves splitting the dataset into training and testing sets respectively, which are mutually exclusive. After which, you train with the training set and test with the testing set. \nThis will provide a more accurate evaluation on out-of-sample accuracy because the testing dataset is not part of the dataset that have been used to train the data. It is more realistic for real world problems.\n\nThis means that we know the outcome of each data point in this dataset, making it great to test with! And since this data has not been used to train the model, the model has no knowledge of the outcome of these data points. So, in essence, it is truly an out-of-sample testing.\n\nLets split our dataset into train and test sets, 80% of the entire data for training, and the 20% for testing. We create a mask to select random rows using __np.random.rand()__ function: ","60b5e65c":"$$\na: intercept\\\\\nb_1 :coefficients \\ of\\ Variable \\ 1\\\\\nb_2: coefficients \\ of\\ Variable \\ 2\\\\\nb_3: coefficients \\ of\\ Variable \\ 3\\\\\nb_4: coefficients \\ of\\ Variable \\ 4\\\\\n$$"}}