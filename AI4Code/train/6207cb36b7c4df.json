{"cell_type":{"511e5586":"code","c0baf29b":"code","8a24f0ad":"code","9eac8767":"code","05e07a16":"code","20c9e4f6":"code","a2bcbcf6":"code","e6cebd90":"code","7845c8d9":"code","d0f7546f":"code","c998ab14":"code","b507ca8c":"code","7acfcc62":"code","92782595":"code","be53a7ae":"code","9c89f35a":"code","76435360":"code","488b0351":"code","fc3c935b":"code","9814266a":"code","e3b0630a":"code","06d01adb":"code","27b705cc":"code","4a38de42":"code","48353bba":"code","053ab507":"code","91be0507":"code","aab7fb6b":"markdown","48f426eb":"markdown","197a4b99":"markdown","0099b45e":"markdown","b485be91":"markdown","6d1ae0db":"markdown","96b674b7":"markdown","753ba2e4":"markdown","ab2e1623":"markdown","d96c8bcc":"markdown","4fcd9178":"markdown","fc917c56":"markdown","00101af0":"markdown","4a6af0d5":"markdown","00e6ff98":"markdown","91e08d00":"markdown","6ebece2c":"markdown","3fb10d8a":"markdown","44c38f31":"markdown","e4845f00":"markdown","48d32d05":"markdown","f95f1b8e":"markdown","50619c5b":"markdown","15d8c0b5":"markdown","a3acb468":"markdown","b7c3d545":"markdown","6c65143c":"markdown","e00e0416":"markdown","da11f1b2":"markdown","acdc4933":"markdown","c53aa11c":"markdown"},"source":{"511e5586":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import f1_score\nimport statsmodels.api as sm \nimport sys\nimport warnings\n\nif not sys.warnoptions:\n    warnings.simplefilter(\"ignore\")","c0baf29b":"df = pd.read_csv('\/kaggle\/input\/glass\/glass.csv')\ndf.head()","8a24f0ad":"df.isnull().sum()","9eac8767":"df.info()","05e07a16":"df['Type'].value_counts().sort_values(ascending = False)","20c9e4f6":"#Let's see the number of data types in the 'Type' column with a bar graph.\n\nsns.set(style = 'whitegrid')\nsns.countplot(x=\"Type\", data=df, palette='bright')","a2bcbcf6":"# Types Means (What do 1,2,3,5,6,7 Types mean?)\n\nglassTypesList = ['1 -> building_windows_float_processed',\n                  '2 -> building_windows_non_float_processed',\n                  '3 -> vehicle_windows_float_processed',\n                  '4 -> vehicle_windows_non_floatprocessed (none in this database)',\n                  '5 -> containers',\n                  '6 -> tableware', \n                  '7 -> headlamps']\n\nprint('\\033[32m'+'\\033[4m'+'Target Column Description(Type of glass - class attribute)\\n'+'\\033[0m')\n\nfor i in range(0,len(df['Type'].unique())+1):\n    print('\\033[31m'+glassTypesList[i])","e6cebd90":"# Features Means (What do 'RI,Na,Mg,Al,Si,K,Ca,Ba,Fe' mean?)\n\nprint('\\033[01m'+'\\033[32m'+'\\033[4m'+'Feature Columns Description\\n'+'\\033[0m')\n\nfeaturesList = ['RI -> refractive index',\n                'Na -> Sodium (unit measurement: weight percent in corresponding oxide, as are attributes 4-10)',\n                'Mg -> Magnesium',\n                'Al -> Aluminum',\n                'Si -> Silicon',\n                'Ca  -> Potassium', \n                'Ba -> Barium',\n                'Fe -> Iron']\n\nfor i in range(0,len(df.columns.unique())-2):\n    print('\\033[31m'+featuresList[i])","7845c8d9":"df.describe()","d0f7546f":"df.corr()","c998ab14":"# Let's see the correlation values on a heatmap\n\ncor = df.corr()\nplt.figure(figsize=(9,6))\nsns.heatmap(data = cor, annot = True, cmap = 'PiYG')\nplt.show()","b507ca8c":"# x and y assignment\n\nx = df.iloc[:,[0,1,2,3,4,5,6,7,8]]\ny = df.iloc[:,9:]","7acfcc62":"#Let's see how x and y look\n\nx.head(3)","92782595":"y.head(3)","be53a7ae":"# Splitting the Data Set into Independent Variables and Dependent Variables\n\nfrom sklearn.model_selection import train_test_split\n\nx_train, x_test,y_train,y_test = train_test_split(x,y,test_size=0.25, random_state=18)","9c89f35a":"from sklearn.linear_model import LogisticRegression\n\nlogr = LogisticRegression(random_state=8)\nlogr.fit(x_train,y_train)\n\ny_pred = logr.predict(x_test)\n\n# confusion matrix and f1 score\nf1_score_logr = f1_score(y_test,y_pred, average='micro')\ncm_logr = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm_logr, annot=True,fmt=\".0f\",linewidths=3,square=True, cmap='Reds',color=\"#cd1076\")\nplt.ylabel('actual label')\nplt.xlabel('predicted label')\nplt.title(f'F1 Score: {f1_score_logr:.2f}',size=14,color='red')\nplt.show()","76435360":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier(n_neighbors=1, metric = 'minkowski')\nknn.fit(x_train,y_train)\n\ny_pred = knn.predict(x_test)\n\n# confusion matrix and f1 score\nf1_score_knn = f1_score(y_test,y_pred, average='micro')\ncm_knn = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm_knn, annot=True,fmt=\".0f\",linewidths=3,square=True, cmap='Reds', color=\"#cd1076\")\nplt.ylabel('actual label')\nplt.xlabel('predicted label')\nplt.title(f'F1 Score: {f1_score_knn:.2f}',size=14,color='red')\nplt.show()\n\n# finding optimum k\nerror_rate = []\nfor i in range(1,40):\n    knn_test = KNeighborsClassifier(n_neighbors=i, metric = 'minkowski')\n    knn_test.fit(x_train,y_train)\n    pred_i = knn_test.predict(x_test)\n    pred_i=pred_i.reshape(54,1)\n    error_rate.append(np.mean(pred_i != y_test))\n\nplt.figure(figsize=(10,6))\nplt.plot(range(1,40),error_rate,color='blue', linestyle='dashed', \n         marker='o',markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value')\nplt.xlabel('K')\nplt.ylabel('Error Rate')\n\nerror_rate = np.array(error_rate)\na = error_rate.tolist().index(error_rate.min())\nprint(f'Minimum error: {error_rate.min():.2f} at K: {a}')","488b0351":"from sklearn.svm import SVC\n\nsvc = SVC(kernel = 'linear', probability=True)\nsvc.fit(x_train,y_train)\n\ny_pred = svc.predict(x_test)\n\n# confusion matrix and f1 score\nf1_score_svc = f1_score(y_test,y_pred, average='micro')\ncm_svc = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm_svc, annot=True,fmt=\".0f\",linewidths=3,square=True, cmap='Reds', color=\"#cd1076\")\nplt.ylabel('actual label')\nplt.xlabel('predicted label')\nplt.title(f'F1 Score: {f1_score_svc:.2f}',size=14,color='red')\nplt.show()","fc3c935b":"from sklearn.naive_bayes import MultinomialNB\n\nmnb = MultinomialNB()\nmnb.fit(x_train,y_train)\n\ny_pred = mnb.predict(x_test)\n\n# confusion matrix and f1 score\nf1_score_mnb = f1_score(y_test,y_pred, average='micro')\ncm_mnb = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm_mnb, annot=True,fmt=\".0f\",linewidths=3,square=True, cmap='Reds', color=\"#cd1076\")\nplt.ylabel('actual label')\nplt.xlabel('predicted label')\nplt.title(f'F1 Score: {f1_score_mnb:.2f}',size=14,color='red')\nplt.show()","9814266a":"from sklearn.tree import DecisionTreeClassifier\n\ndtc = DecisionTreeClassifier(criterion = 'entropy')\ndtc.fit(x_train,y_train)\n\ny_pred = dtc.predict(x_test)\n\n\n# confusion matrix and f1 score\nf1_score_dtc = f1_score(y_test,y_pred, average='micro')\ncm_dtc = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm_dtc, annot=True,fmt=\".0f\",linewidths=3,square=True, cmap='Reds', color=\"#cd1076\")\nplt.ylabel('actual label')\nplt.xlabel('predicted label')\nplt.title(f'F1 Score: {f1_score_dtc:.2f}',size=14,color='red')\nplt.show()","e3b0630a":"from sklearn.ensemble import RandomForestClassifier\n\nrfc = RandomForestClassifier(n_estimators = 250, criterion = 'entropy')\nrfc.fit(x_train,y_train)\n\ny_pred = rfc.predict(x_test)\n\n\n# confusion matrix and f1 score\nf1_score_rfc = f1_score(y_test,y_pred, average='micro')\ncm_rfc = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm_rfc, annot=True,fmt=\".0f\",linewidths=3,square=True, cmap='Reds', color=\"#cd1076\")\nplt.ylabel('actual label')\nplt.xlabel('predicted label')\nplt.title(f'F1 Score: {f1_score_rfc:.2f}',size=14,color='red')\nplt.show()","06d01adb":"# cross_val_score\nfrom sklearn.model_selection import cross_val_score\n\nprint(cross_val_score(rfc, x, y, cv=5))","27b705cc":"# mean of accuracies\naccuracies = cross_val_score(rfc, x_train, y_train, cv = 5)\nprint(f'Accuracy (mean):{accuracies.mean()*100}')\nprint('std: %',accuracies.std()*100)","4a38de42":"# inference from cross_val_score\nprint(f'Random Forest F1 Score: {f1_score_rfc*100:.2f}')\nprint(f'Accuracy (mean):{accuracies.mean()*100:}')\nprint(f'std: % {accuracies.std()*100:.2f}')","48353bba":"# Confusion Matrices\nprint('\\033[1m' + '\\033[96m' + '-CONFUS\u0130ON MATR\u0130CES-\\n' + '\\033[0m'+'\\033[0m')\n\nprint('\\033[1m' + f'Logistic Regression' +'\\033[0m' + f'\\n{cm_logr}' + f' F1 Score: {f1_score_logr:.2f}\\n\\n' + '\\033[0m'  )\nprint('\\033[1m' + f'KNN-K Nearest Neighbors' +'\\033[0m' + f'\\n{cm_knn}' + f' F1 Score: {f1_score_knn:.2f}\\n\\n' + '\\033[0m'  )\nprint('\\033[1m' + f'SVC-Support Vector Classifier' +'\\033[0m' + f'\\n{cm_svc}' + f' F1 Score: {f1_score_svc:.2f}\\n\\n' + '\\033[0m'  )\nprint('\\033[1m' + f'Multinomial Naive Bayes' +'\\033[0m' + f'\\n{cm_mnb}' + f' F1 Score: {f1_score_mnb:.2f}\\n\\n' + '\\033[0m'  )\nprint('\\033[1m' + f'Decision Tree Classifier' +'\\033[0m' + f'\\n{cm_dtc}' + f' F1 Score: {f1_score_dtc:.2f}\\n\\n' + '\\033[0m'  )\nprint('\\033[1m' + f'Random Forest Classifier' +'\\033[0m' + f'\\n{cm_rfc}' + f' F1 Score: {f1_score_rfc:.2f}\\n\\n' + '\\033[0m'  )","053ab507":"# Let's see F1 Scores in a table\nNamesOfAlgorithms_df = pd.DataFrame(['Logistic Regression','KNN - K Nearest Neighbors','SVC - Support Vector Classifier','Multinomial Naive Bayes','Decision Tree Classifier','Random Forest Classifier'])\nAcScoresOfAlgorithms_df = pd.DataFrame([f1_score_logr,f1_score_knn,f1_score_svc,f1_score_mnb,f1_score_dtc,f1_score_rfc])\n\ndf3 = pd.concat([NamesOfAlgorithms_df,AcScoresOfAlgorithms_df],axis=1)\ndf3.columns=['ALGOR\u0130THM','F1 SCORE',]\n\ndf3","91be0507":"# F1 Scores Comparison on a Bar Chart\ndict_Scores = {'Logistic Regression': f1_score_logr*100 , 'KNN': f1_score_knn*100,\n               'SVC': f1_score_svc*100, 'Multinomial Naive Bayes': f1_score_mnb*100, \n               'Decision Tree': f1_score_dtc*100, 'Random Forest': f1_score_rfc*100\n              }\n\ncolors = [\"#00008b\", \"#00e5ee\", \"#cd1076\", \"#008080\",\"#cd5555\",'black']\nsns.set_style(\"whitegrid\")\nsns.set(font_scale = 2.5)\nplt.figure(figsize=(30,12))\nsns.barplot(x=list(dict_Scores.keys()), y=list(dict_Scores.values()), palette=colors)\nplt.yticks(np.arange(0,101,5))\nplt.title('Comparision of Models', fontsize=60)\nplt.ylabel(\"F1 Score %\", fontsize=30)\nplt.xlabel(\"\\nAlgorithms\", fontsize=30)\nplt.tight_layout()\nplt.show()","aab7fb6b":"* The model with the best performance was Ranfom Forest. \n\n* In my opinion, The success of Random Forest In in this dataset depends on whether the data category is labeled data, the data is not too much, and the data is numeric data rather than text data. Also, random forest has a parameter such as n_estimators, which generally increases precision, of course, while this parameter increases precision from time to time, it usually increases the cost at the same rate. According to Decision Tree, Random Forest was more successful because Random Forest works on accuracy rather than speed due to its algorithm structure. Also, the reason naive bayes has the lowest score is because data is not text data, because naive bayes text works better with text data.\n \n* Fe, Ba, K columns have high standard deviations compared to their mean. This negatively affected the performance of the models.\n\n* I didn't add it to the code, but I tried some things in the background, I removed some variables from the data set with backward elimination, but as a result, there was no significant increase in model performances, so I did not want to disturb the general structure of the data and included all the variables in my models.\n \n* While there were small increases in the performance of some models, there were decreases in others. Since this was not a result of backward elimination we wanted, I gave up and included all the variables in the models, so the general structure of the data was not deteriorated.","48f426eb":"<a id = \"3\"><\/a><br>\n# Logistic Regression","197a4b99":"**No null data, that's nice.**","0099b45e":"**When I compare the Mean and Std values of each column; The Std values of the \"Type, Fe, Ba, K\" columns are too large compared to the Mean values. This can adversely affect the performance of our models. After we set up our models,  maybe we can eliminate some columns (except the Type column, because that target column) through some methods such as 'Backward Elimination'. I will evaluate this after looking at the performance of our models.If there is no improvement in the performance of the models, we cancel the backward elimination or try something else.**","b485be91":"![NotedBySonerKar.png](attachment:c134881f-11e8-4766-aa83-d3e26f7dccef.png)","6d1ae0db":"<h1 style=\"text-align:center;font-weight: bold;\">GLASS CLASSIFICATION<\/h1>\n\n","96b674b7":"<a id = \"12\"><\/a><br>\n# Comparision of F1 Scores","753ba2e4":"<a id = \"6\"><\/a><br>\n# Naive Bayes","ab2e1623":"<a id = \"7\"><\/a><br>\n# Decision Tree ","d96c8bcc":"![for_glass_data_set_.jpg](attachment:6e04e620-e5e4-4b38-86eb-467936846709.jpg)","4fcd9178":"<a id = \"10\"><\/a><br>\n# Cross Val Score","fc917c56":"**So, the reason why there is no glass with 4 Types is that it does not exist in the dataset. Also, by looking at this, we can see which type means what.**","00101af0":"<a id = \"8\"><\/a><br>\n# Random Forest ","4a6af0d5":"<a id = \"9\"><\/a><br>\n# Evaluate Models","00e6ff98":"**Random Forest has the best performance.**","91e08d00":"Why is Multinomial Naive Bayes more suitable for this data set?\n\nIf the column to be predicted is binomial, ie binary, ie 1 or 0, 'Bernoulli Naive Bayes' is used. I did not use 'Bernoulli' as the y column in this dataset is not binomial. (Similar to not using sigmoid in the SVC just before)\n\nIf the column to be predicted consists of contunious numbers, that is, if it consists of real numbers, 'Gaussian Naive Bayes' is used. However, in this dataset, column y consists of strings, not real numbers. That's why I didn't use 'Gaussian Naive Bayes'.\n\nIf the column to be predicted consists of nominal data, 'Multinomial Naive Bayes' is used. In suitable with this kind of Naive Bayes, column y in this data set consists of nominal data.\n> Because \"...where the data are typically **represented as word** vector counts, although tf-idf vectors are also known to work well in practice.\" quotation: scikit-learn.org\/stable\/modules\/naive_bayes\n\nThe data 0,1,2,3,4,5,6,7 in the 'Type' column actually represents a word, and since there is no ranking between these words\nso these are multinomial data.\n\nThat's why I used 'Multinomial Naive Bayes'.","6ebece2c":"**They all consist of 214 rows, the x columns are float type, the y column is integer type. Data is clean so far.**","3fb10d8a":"**We chose K as 1 because the lowest error rate was when K=1**","44c38f31":"<a id = \"2\"><\/a><br>\n# Create Models","e4845f00":"<a id = \"11\"><\/a><br>\n# Comparision of Confusion Matrices","48d32d05":"**Imbalanced data! Therefore, I will use 'F1 Score' to evaluate models.**","f95f1b8e":"**There is no 'Type' variant with the number '4'.**","50619c5b":"**They are as they should be, good.**","15d8c0b5":"<font color = '#E22812'>\nContent:\n    \n1. [Load and Check Data](#1)\n1. [Create Models](#2)   \n    *           [Logistic Regression](#3)\n    *           [KNN - K Nearest Neighbors](#4)\n    *           [SVC - Support Vector Classifier](#5)\n    *           [Naive Bayes](#6)\n    *           [Decision Tree](#7)\n    *           [Random Forest](#8)\n1. [Evaluate Models](#9)\n    *           [Cross Val Score](#10)\n    *           [Comparision of Confusion Matrices](#11)\n    *           [Comparision of F1 Scores](#12)\n1. [Conclusion](#13) ","a3acb468":"**There is no overfitting as they are very close to each other, good.**","b7c3d545":"<a id = \"4\"><\/a><br>\n# KNN - K Nearest Neighbors","6c65143c":"**The maximum correlation is between 'Type' and 'Ca' and its value is 0.81, so we can generally say that as one increases, the other increases and vice versa. The minimum correlation or maximum negative correlation is between 'A1' and 'Type' and its value is -0.71, so as one decreases, the other increases.**","e00e0416":"**There is no categorical data, so I will not use an encoder.**","da11f1b2":"<a id = \"5\"><\/a><br>\n# SVC - Support Vector Classifier","acdc4933":"<a id = \"1\"><\/a><br>\n# Load and Check Data","c53aa11c":"<a id = \"13\"><\/a><br>\n# Conlusion"}}