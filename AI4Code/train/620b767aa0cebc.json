{"cell_type":{"0cfffd9a":"code","61483d03":"code","fb0c409f":"code","403bf463":"code","7689098f":"code","223c66f8":"code","4678d2b6":"code","2cb3205e":"code","b96552fa":"code","7ec1e787":"code","09b4157c":"code","7db60a6e":"code","cd4ef117":"code","7a709f31":"code","2a0115c9":"code","a6d593d8":"code","88a1b225":"code","c266547f":"code","805ff0ed":"code","94d2790f":"code","be0cc1ac":"code","35bb75f5":"code","f081a3c7":"code","f403386b":"code","d14d9111":"code","26f1b576":"code","0e71aa73":"code","ffde9c37":"code","e0d7757a":"code","10a8c4da":"markdown","1a371c02":"markdown","81445905":"markdown","5041ead4":"markdown","61e0dd7e":"markdown","e457cd01":"markdown","0c609ff4":"markdown","b7a374c3":"markdown","75798919":"markdown","bdd266a1":"markdown","d78ef185":"markdown","ecf2d38f":"markdown","387b9c45":"markdown","bbdf3b69":"markdown","f67ccdbf":"markdown","dac2d4eb":"markdown","4475d941":"markdown","637f357c":"markdown","3b2b3e82":"markdown","c5c6c4bb":"markdown","f3ec1902":"markdown","2dafe5b8":"markdown","df65291e":"markdown","598e99ec":"markdown","927e4949":"markdown","ed2c1155":"markdown","5483536f":"markdown","b2102652":"markdown","a65cf903":"markdown","d55b0668":"markdown","9f3e299d":"markdown","86cd7e60":"markdown"},"source":{"0cfffd9a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport random\nfrom pprint import pprint","61483d03":"def train_test_split(data, test_rate):\n    if isinstance(test_rate, float):\n        test_size = float(len(data))*test_rate\n    \n    indices = data.index.tolist()\n    test_indices = random.sample(indices, int(test_size))\n    test_data = data.iloc[test_indices]\n    train_data = data.drop(test_indices)\n    \n    return train_data, test_data","fb0c409f":"def check_purity(data):\n    classes = data[:, -1]\n    unique_classes = np.unique(classes)\n    \n    if len(unique_classes) == 1:\n        return True\n    else:\n        return False","403bf463":"def create_leaf(data, ml_task):\n    classes = data[:, -1]\n    \n    if ml_task == 'regression':\n        leaf = np.mean(classes)\n        \n     # Classification   \n    else:\n        unique_classes, unique_classes_count = np.unique(classes, return_counts = True)\n        leaf = unique_classes[unique_classes_count.argmax()]\n    \n    return leaf","7689098f":"def potential_split(data):\n    potential_split = {}\n    _, n_columns = data.shape\n    for column in range(n_columns-1):\n        unique_values = np.unique(data[:, column])\n        potential_split[column] = unique_values\n    \n    return potential_split","223c66f8":"def split_data(data, split_column, split_value):\n    split_column_values  = data[:, split_column]\n    \n    if FEATURE_TYPE[split_column] != 'Categorical':\n        data_below = data[split_column_values <= split_value]\n        data_above = data[split_column_values > split_value]\n    # feature is categorical\n    else:\n        data_below = data[split_column_values == split_value]\n        data_above = data[split_column_values != split_value]\n    \n    return data_below, data_above","4678d2b6":"def calculate_mse(data):\n    actual_values = data[:, -1]\n    if len(actual_values) == 0:\n        mse = 0\n    else:\n        prediction = np.mean(actual_values)\n        mse = np.mean((actual_values-prediction)**2)\n    \n    return mse","2cb3205e":"def calculate_entropy(data):\n    classes = data[:, -1]\n    _, unique_value_count = np.unique(classes, return_counts = True)\n    probabilities = unique_value_count\/sum(unique_value_count)\n    entropy = sum(probabilities * - np.log2(probabilities))\n    \n    return entropy","b96552fa":"def calculate_overall_metric(data_below, data_above, metric_function):\n    n = len(data_below) + len(data_above)\n    p_data_below = len(data_below)\/n\n    p_data_above = len(data_above)\/n\n    \n    overall_metric = (p_data_below * metric_function(data_below)) + (p_data_above * metric_function(data_above))\n    \n    return overall_metric","7ec1e787":"def determine_best_split(data, potential_splits, ml_task):\n    \n    first_iteration = True\n    for column_index in potential_splits:\n        for value in potential_splits[column_index]:\n            data_below, data_above = split_data(data, split_column=column_index, split_value=value)\n            \n            if ml_task == \"regression\":\n                current_overall_metric = calculate_overall_metric(data_below, data_above, metric_function=calculate_mse)\n            \n            # classification\n            else:\n                current_overall_metric = calculate_overall_metric(data_below, data_above, metric_function=calculate_entropy)\n\n            if first_iteration or current_overall_metric <= best_overall_metric:\n                first_iteration = False\n                \n                best_overall_metric = current_overall_metric\n                best_split_column = column_index\n                best_split_value = value\n    \n    return best_split_column, best_split_value","09b4157c":"def type_of_features(data):\n    Unique_threshold = 25\n    feature_type = []\n    \n    for feature in data.columns:\n        unique_values = np.unique(data[feature])\n        \n        if (isinstance(unique_values[0], str)) or (len(unique_values) <= 25):\n            feature_type.append('Categorical')\n        else:\n            feature_type.append('Continuous')\n    \n    return feature_type","7db60a6e":"def decision_tree_algorithm(df, ml_task, counter=0, min_samples = 2, max_depth = 5):\n    if counter == 0:\n        global COLUMN_HEADER, FEATURE_TYPE\n        COLUMN_HEADER = df.columns\n        FEATURE_TYPE = type_of_features(df)\n        data = df.values\n    else:\n        data = df\n    \n    # base case\n    if (check_purity(data)) or (int(len(data)) < min_samples) or (counter == max_depth):\n        leaf = create_leaf(data, ml_task)\n        return leaf\n    \n    # recursive part\n    else:\n        counter += 1\n        \n        potential_splits = potential_split(data)\n        split_column, split_value = determine_best_split(data, potential_splits, ml_task)\n        data_below, data_above = split_data(data, split_column, split_value)\n        \n        # check empty data\n        if len(data_below) == 0 or len(data_above) == 0:\n            leaf = create_leaf(data, ml_task)\n            return leaf\n        # determine question\n        else:\n            feature_name = COLUMN_HEADER[split_column]\n            if FEATURE_TYPE[split_column] != 'Categorical':\n                question = '{} <= {}'.format(feature_name, split_value)\n            # Categorial data\n            else:\n                question = '{} = {}'.format(feature_name, split_value)\n            \n            sub_tree = {question: []}\n            \n            # recursive call\n            yes_answer = decision_tree_algorithm(data_below, ml_task, counter, min_samples, max_depth)\n            no_answer = decision_tree_algorithm(data_above, ml_task, counter, min_samples, max_depth)\n            \n            if yes_answer == no_answer:\n                sub_tree = yes_answer\n            else:\n                sub_tree[question].append(yes_answer)\n                sub_tree[question].append(no_answer)\n                \n            return sub_tree","cd4ef117":"def predict_example(example, tree):\n    question = list(tree.keys())[0]\n    feature_name, comparison_operator, value = question.split(\" \")\n\n    # ask question\n    if comparison_operator == \"<=\":\n        if example[feature_name] <= float(value):\n            answer = tree[question][0]\n        else:\n            answer = tree[question][1]\n    \n    # feature is categorical\n    else:\n        if str(example[feature_name]) == value:\n            answer = tree[question][0]\n        else:\n            answer = tree[question][1]\n\n    # base case\n    if not isinstance(answer, dict):\n        return answer\n    \n    # recursive part\n    else:\n        return predict_example(example, answer)","7a709f31":"def calculate_accuracy(df, tree):\n    df['classification'] = df.apply(predict_example, axis=1, args=(tree,))\n    df['classification_correct'] = df.iloc[:, -1] == df['classification']\n    \n    accuracy = df.classification_correct.mean()*100\n    return str(accuracy) + '%'","2a0115c9":"dataframe = pd.read_csv('..\/input\/iris-data\/Iris.csv')\ndataframe.head()","a6d593d8":"\ndataframe = dataframe.drop('Id', axis=1)\ndataframe.head()","88a1b225":"dataframe.info()","c266547f":"train_data, test_data = train_test_split(dataframe, 0.2)","805ff0ed":"len(train_data), len(test_data)","94d2790f":"tree = decision_tree_algorithm(train_data, ml_task = 'classification', max_depth = 3)\npprint(tree)","be0cc1ac":"example = test_data.iloc[0]\nexample","35bb75f5":"predict_example(example, tree)","f081a3c7":"calculate_accuracy(test_data, tree)","f403386b":"data1 = pd.read_csv('..\/input\/markswithattendence\/markswithattendence.csv')\ndata1.head()","d14d9111":"data1.info()","26f1b576":"tree1 = decision_tree_algorithm(data1, ml_task = 'regression', max_depth = 3)\npprint(tree1)","0e71aa73":"example1 = data1.iloc[0]\nexample1","ffde9c37":"predict_example(example1, tree1)","e0d7757a":"calculate_accuracy(data1, tree1)","10a8c4da":"**Train a model for regression using decision tree**","1a371c02":"**Load dataset**","81445905":"**Calculate the accuracy**","5041ead4":"## Calculate Accuracy","61e0dd7e":"## Helper Functions","e457cd01":"### Create leaf","0c609ff4":"### Data Pure?","b7a374c3":"## Decision Tree Algorithm","75798919":"**Train Test Split**","bdd266a1":"### Train Test Split","d78ef185":"**Let's check with an example and predict the output**","ecf2d38f":"**Load dataset**","387b9c45":"It works for both regression and classification problems. I have develop this code from scratch in python without using sklearn library.","bbdf3b69":"### potential split","f67ccdbf":"### Type of features (Categorical or Continuous) ","dac2d4eb":"It shows that there is not a null value in the dataset","4475d941":"## Classification Function \/ Make prediction","637f357c":"This shows that dataset has no null values","3b2b3e82":"# <p style = \"color:purple; font-weight:bolder; text-align:center; font-size:40px;\">Decision Tree For classification and regression from scratch in python<p>","c5c6c4bb":"### Importing required packages","f3ec1902":"#### Calculate overall metric","2dafe5b8":"#### Calculate entropy","df65291e":"**Predict an output of an example**","598e99ec":"**Train a decision tree model for classification**","927e4949":"<p style = \"text-align:center\">Thank you<\/p>","ed2c1155":"#### Best split","5483536f":"### Split the data","b2102652":"# <span style=\"color:red\">Classification example with decision tree algorithm<span>","a65cf903":"### Determine Best Split","d55b0668":"**Check the accuracy**","9f3e299d":"#### Calculate mean square error","86cd7e60":"# <span style=\"color:red\">Regression example with decision tree algorithm<span>"}}