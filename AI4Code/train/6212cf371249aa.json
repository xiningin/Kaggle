{"cell_type":{"5e1c9fa1":"code","eb9e1add":"code","e45b4aa4":"code","8135e508":"code","de176dd3":"code","884c6efa":"code","d1239890":"code","ef168099":"code","7d8b91fc":"code","dcc03452":"code","e9c8e217":"code","593bf186":"code","bb29457f":"code","ff4f0daa":"code","6ab9409f":"code","d78af879":"code","9941d833":"code","38a116a8":"markdown","37ff8e65":"markdown","f308f8bf":"markdown","82c1449b":"markdown","1693ea21":"markdown","ac6a191d":"markdown","cfe7e3b9":"markdown","28bc21bd":"markdown","835f1688":"markdown","41892008":"markdown","707bd56a":"markdown","76cdc656":"markdown","afb3d80d":"markdown","b53d4480":"markdown"},"source":{"5e1c9fa1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \npd.options.display.max_columns=500\npd.options.display.max_rows=100","eb9e1add":"train=pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/test.csv')\nsub=pd.read_csv('\/kaggle\/input\/home-data-for-ml-course\/sample_submission.csv')","e45b4aa4":"display(train, test) # display columns of train and test","8135e508":"train.corr()['SalePrice'].sort_values(ascending=False)[:7]","de176dd3":"fig=plt.figure(figsize=(15,7))\n\nax1=fig.add_subplot(221)\nax2=fig.add_subplot(222)\nax3=fig.add_subplot(223)\nax4=fig.add_subplot(224)\n\nsns.scatterplot(train['OverallQual'], train['SalePrice'], ax=ax1)\nsns.scatterplot(train['GrLivArea'], train['SalePrice'], hue=train['ExterQual'],ax=ax2)\nsns.scatterplot(train['GarageCars'], train['SalePrice'], ax=ax3)\nsns.scatterplot(train['GarageArea'], train['SalePrice'], ax=ax4)\n\nplt.show()","884c6efa":"train=train.loc[(train['SalePrice']>300000) | (train['GrLivArea']<4000)] # outlier delete","d1239890":"train.corr()['SalePrice'].sort_values(ascending=False)","ef168099":"alldata=pd.concat([train,test],sort=False)\nalldata=alldata.drop(['Id','SalePrice'],axis=1)","7d8b91fc":"alldata_2=pd.get_dummies(alldata)\nalldata_2=alldata_2.fillna(-1) # missing value replacement with -1","dcc03452":"from sklearn.preprocessing import StandardScaler\nss=StandardScaler()\nalldata_3=ss.fit_transform(alldata_2)","e9c8e217":"train_2=alldata_3[:len(train)] #data set split\ntest_2=alldata_3[len(train):]","593bf186":"from sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.model_selection import cross_val_score\n\nridgecv=RidgeCV(alphas=[1]) # \uae30\ubcf8\uac12 \uaddc\uc81c\ub97c \uc8fc\uc9c0 \uc54a\uc558\ub2e4.\nnp.sqrt(-cross_val_score(ridgecv , train_2, train['SalePrice'], n_jobs=-1, cv=10, scoring='neg_mean_squared_error').mean())","bb29457f":"rmse=[]\nalphas=[1,5, 50, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 700, 750, 900, 1000] \nfor i in alphas :\n    ridgecv=RidgeCV(alphas=[i])\n    result=np.sqrt(-cross_val_score(ridgecv , train_2, train['SalePrice'], n_jobs=-1, cv=10, scoring='neg_mean_squared_error').mean())\n    rmse.append(result)\n    print(result)","ff4f0daa":"plt.figure(figsize=(8,3))\nsns.lineplot(x=alphas, y=rmse)\nplt.suptitle('RMSE by alpha of ridge regression')\nplt.xlabel('alpha')\nplt.ylabel('RMSE')\nplt.show()","6ab9409f":"ridgecv=RidgeCV(alphas=[1]) # \uae30\ubcf8\uac12 \uaddc\uc81c\ub97c \uc8fc\uc9c0 \uc54a\uc558\ub2e4.\nridgecv.fit(train_2,np.log(train['SalePrice']))\n\nridgecv_2=RidgeCV(alphas=[100]) # \uae30\ubcf8\uac12 \uaddc\uc81c\ub97c \uc8fc\uc9c0 \uc54a\uc558\ub2e4.\nridgecv_2.fit(train_2,np.log(train['SalePrice']))\n\n\nfig=plt.figure(figsize=(25,70))\n\nax1=fig.add_subplot(121)\nax2=fig.add_subplot(122)\n\n#x=alldata_2.columns.tolist(),\nchart_1=sns.barplot(y=alldata_2.columns.tolist(), x=ridgecv.coef_.tolist(), ax=ax1)\nchart_2=sns.barplot(y=alldata_2.columns.tolist(), x=ridgecv_2.coef_.tolist(), ax=ax2)\n\nax1.set_xlim(-0.02, 0.05)\nax2.set_xlim(-0.02, 0.05)\n\nax1.set_title('coefficient with alpha 1')\nax2.set_title('coefficient with alpha 100')\n\n\nplt.show()\n","d78af879":"fig=plt.figure(figsize=(15,4))\n\nax1=fig.add_subplot(121)\nax2=fig.add_subplot(122)\n\nsns.distplot(train['SalePrice'],ax=ax1)\nsns.distplot(np.log(train['SalePrice']),ax=ax2)\n\nbdict = {'facecolor' : 'r', 'alpha' : 0.5, 'boxstyle' : 'rarrow', 'linewidth' : 2}\n\n\nfig.suptitle('Target distribution from unlogged to logged', bbox=bdict, color='black')\nax1.set_title('Un-logged')\nax2.set_title('Logged')\n\nplt.show()","9941d833":"ridgecv=RidgeCV(alphas=np.linspace(1,1000,1000)) #np.linspace(1,1000,1000)\nridgecv.fit(train_2,np.log(train['SalePrice']))\nresult=ridgecv.predict(test_2)\nsub['SalePrice']=np.exp(result)\nsub.to_csv('result.csv',index=False)","38a116a8":"* when $\\alpha=250$, `RMSE` is minimized. But this will be more optimized by dividing granular window of alpha\n* $\\alpha$ increases, the performace of ridge improves","37ff8e65":"## Regularized regression : \n* `Penalized` residual sum of squares to prevent overfitting \n* Overfiited model has large weights(coefficients) and regularized model reduces weights(coefficients) of model\n* But regularized model should retain the number of variable\n\n\n\n### 1. `Ridge regression` (or `Tikhonov regularization`)\n\n`penalty` : minimizing `sqaured sum of weights` ($\\lambda \\sum_{j=1}^M w_{j}^2 $)\n\n$$w = \\underset{w}{\\operatorname{argmin}} \\left(\\underbrace{\\sum_{i=1}^{N}e^2_i}_{\\text{linear least squares}} + \\underbrace{\\lambda \\sum_{j=1}^M w_{j}^2}_{\\text{penalty term}} \\right)  $$\n\n* $\\lambda$ is hyperparameter to adjust weights\n* $\\lambda$ increases, regularization increases and weights decrease\n* $\\lambda$ is zero, it is the same with linear regression\n* $\\lambda$ is the same with $\\alpha$ in `sklearn`'s formula \n\n\nfrom `sklearn`, we can find the objective function to be minimized as followings\n\n$$ \\underset{\\omega}{\\operatorname{min}} \\Vert{X\\omega-y}\\Vert_{2}^{2} + \\alpha\\Vert{\\omega}\\Vert_{2}^{2}$$\n\n* $\\alpha$ is `complexity parameter` to control the amount of `shrinkage`\n* High $\\alpha$ = High `shrinkage` = more robust to `collinearity`\n\n\n\n2. `Lasso regression`\n\n3. `Elastic net regression`\n\n\n","f308f8bf":"## Ridge regression by increasing alpha\n* By increasing alpha from 1 to 100, the coefficients are reduced (left vs right)\n* Right bar charts show much lower heights","82c1449b":">`Scaling` \n* Linear model is needed to be scaled, otherwise the range of values in columns affects prediction","1693ea21":"## How to treat categorical variable \n\n* `One hot encoding` should be applied for `linear model` using `get_dummies`\n  * Data quantity is not large\n  * Categories are not numerous\n  \n  \n  \n* `Label encoding` is not appropriated\n  * For tree-based model, label encoding is used\n  * For linear model, labeling to number can mislead linear model \n  * For SaleCondition,  (1 : Normal, 2: Partial, 3: Abnorml, 4: Family, 5: Alloca, 6:AdjLand) is labeled, linear model can interpret Abnormal(3) is 3 times better condition than Normal(1)\n  \n  \n","ac6a191d":">RidgeCV(alphas=[1])\n* From $\\alpha=1$ , increase $\\alpha$ value by 1000\n* l2-regularization is applied for Ridge \n* High $\\alpha$ = more regularization = less overfitting = less weights to columns","cfe7e3b9":"## Correlation between target variable(SalePrice) and the others\n* `Correlation` can be calculated among `numeric variables`\n* Most of variable show `positive` relation to target variable","28bc21bd":"## Cross-validation is best option for this data set\n* validation set or holdout data set is not good choice\n\n**Do you know why?**\n\n* `Data quantity` is too `small`. if you lose 10%~20% for validation set, it deteriorates your model performance\n* Validatation set can not represent your test data. If you learn more, it is overfitted into validation set\n\n**But by using cross validtation**\n* You don't need to lose your data\n* Reduce the risk of overfitting","835f1688":"> First cross validation of ridge model with $\\alpha=1$ and k=10 (10-fold)","41892008":"## Transformation of target variable \n* `logged` SalePrice will improve model, because it transforms right-skewed distribution into `normal-like distribution`\n* `log` narrows down the range of values, makes it easier to predict values, and reduce the impact of outliers","707bd56a":"<img src = \"https:\/\/i.imgur.com\/oquuCAa.png\" width=\"900px\"\/>","76cdc656":"### Which model ? Tree-based model or linear model\n\n**If tree-based model applied**\n\n* `random forest` is not appropriate, because some columns are less informative but can be selected by bagging\n* boosting model is suitable for this competition\n* Especially `catboost` is good considering lots of categorical columns\n\n**If linear model applied**\n\n* `linear relationship` should be learned through distance-based model such as linear regression or deep learning","afb3d80d":"## Modeling using Ridgecv\n* Alpha ranges from 1 to 1000 like GridCV\n* By cross validation, optimal alpha is selected and the final prediction value is estimated using Ridge\n* Estimated target variable must be transformed into exponential value (original value)","b53d4480":"### `Outlier` detection and adjustment\n\n* Which colums are more sensitive to outliers ? Focus more on columns highly correlated with target variable \n* Columns with low correlation is not priority\n\n>`outlier` detectrion and delete by visualization"}}