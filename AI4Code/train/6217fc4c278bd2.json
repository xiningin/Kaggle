{"cell_type":{"cbfddbe1":"code","830c9ec9":"code","49367301":"code","387f21d4":"code","a5055002":"code","75c2896f":"code","4a162abc":"code","b87e80f8":"code","bdd63adb":"code","2eb60e5d":"code","d432bebb":"code","0bab96a2":"code","eabf994a":"code","e33cd1f4":"code","43984cb9":"code","2a574c81":"code","29783082":"code","439a4448":"code","863b445d":"code","4cadc357":"code","6caa101d":"code","caef5d7c":"code","37cf0464":"code","b8357ef9":"code","dfbd36dd":"code","bbdb8ee3":"code","007d38a5":"code","e49f6686":"code","12c23199":"code","7011d86e":"code","e1636ab0":"code","305c6b7d":"code","f3520089":"code","0d94ad55":"code","84f96d85":"code","d960147f":"code","5ab2154a":"code","0ec8a556":"code","886cc310":"code","0535a539":"markdown","5bac3b96":"markdown","9911c3c1":"markdown","aac36bc9":"markdown","060921aa":"markdown","d553c1dd":"markdown","94b6342d":"markdown","a90f2a70":"markdown","0132e3c6":"markdown","6e1e383d":"markdown","2ed3839b":"markdown","0264be84":"markdown","5d575250":"markdown"},"source":{"cbfddbe1":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","830c9ec9":"# import standard libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\nimport warnings\nwarnings.filterwarnings('ignore')","49367301":"df_oil = pd.read_csv(\"\/kaggle\/input\/store-sales-time-series-forecasting\/oil.csv\", index_col=0, parse_dates=[0])\ndf_holidays = pd.read_csv(\"\/kaggle\/input\/store-sales-time-series-forecasting\/holidays_events.csv\", index_col=0, parse_dates=[0])\ndf_stores = pd.read_csv(\"\/kaggle\/input\/store-sales-time-series-forecasting\/stores.csv\", index_col=0)\ndf_transactions = pd.read_csv(\"\/kaggle\/input\/store-sales-time-series-forecasting\/transactions.csv\", parse_dates=[0])\ndf_train = pd.read_csv(\"\/kaggle\/input\/store-sales-time-series-forecasting\/train.csv\", index_col=0, parse_dates=[1])\ndf_test = pd.read_csv(\"\/kaggle\/input\/store-sales-time-series-forecasting\/test.csv\", index_col=0, parse_dates=[1])","387f21d4":"df_train.head()","a5055002":"df_test.iloc[[0,-1],]","75c2896f":"print(df_oil.dcoilwtico.isna().sum())\ndf_oil.head()","4a162abc":"#fill null-values with the mean of previous and following values - because the very first value is missing, we impute it with the second \nna_index = df_oil[df_oil.dcoilwtico.isna()].index\ndf_oil.loc[na_index, \"dcoilwtico\"] = (df_oil.dcoilwtico.fillna(method=\"ffill\") + df_oil.dcoilwtico.fillna(method=\"bfill\"))\/2\ndf_oil.dcoilwtico[0] = df_oil.dcoilwtico[1]","b87e80f8":"# plotting the oil prices and a moving average over a week\n\nfig, ax = plt.subplots(figsize=(20,7))\nax.plot(df_oil.rolling(window=7,\n                       center=True,\n                       min_periods=3).mean(),\n       linewidth=2,\n       color=\"red\")\nsns.scatterplot(data=df_oil, x=\"date\", y=\"dcoilwtico\", color=\"0.5\", alpha=0.5, ax=ax)\nsns.lineplot(data=df_oil, x=\"date\", y=\"dcoilwtico\", alpha=0.5, ax=ax, linewidth=0.5)\nax.set_title(\"Oil Prices\", fontsize=18)","bdd63adb":"df_oil[df_oil.index >= \"2017-08-15\"].iloc[[0,-1],]","2eb60e5d":"holidays = df_holidays.loc[~df_holidays.description.str.contains('\\+|\\-')][df_holidays.transferred==False].set_index(\"date\")\nholidays.head()","d432bebb":"holidays_2015 = holidays[(\"2015\" <= holidays.index) & (holidays.index <= \"2016\")]","0bab96a2":"df_train[df_train.date.isin(holidays_2015.index)].head()","eabf994a":"fig, ax = plt.subplots(figsize=(20,7))\nsns.lineplot(data=df_train[(df_train.date >= \"2015\") & (df_train.date < \"2016\")].groupby(\"date\").sum(),\n             x=\"date\",\n             y=\"sales\",\n             ax=ax\n            )\ndata = df_train[df_train.date.isin(holidays_2015.index)].groupby(\"date\").sum()\ndata = data.join(df_holidays.set_index(\"date\"), how=\"left\")\nsns.scatterplot(x=data.index,\n            y=data.sales,\n            hue=data.locale,\n            s=150)","e33cd1f4":"print(df_stores.shape)\ndf_stores.head()","43984cb9":"df_stores.groupby(\"state\").count()","2a574c81":"df_stores.type.value_counts()","29783082":"df_stores.cluster.value_counts()","439a4448":"df_stores[[\"type\",\"cluster\", \"city\", \"state\"]].groupby([\"cluster\", \"type\", \"city\"]).count()","863b445d":"df_stores[[\"city\",\"state\"]].groupby([\"state\", \"city\"]).count()","4cadc357":"df_stores[df_stores.cluster==10].store_nbr.value_counts()","6caa101d":"fig, ax = plt.subplots(figsize=(25,10))\nsns.boxplot(#x=\"cluster\", \n            x=\"store_nbr\",\n            y=\"transactions\", \n            data=df_transactions, \n            #hue=\"cluster\", \n            ax=ax, \n            dodge=False)","caef5d7c":"df_transactions.tail()","37cf0464":"df_train.loc[ : , ['date', 'sales']][df_train.date>=\"2015-07\"]","b8357ef9":"fig, ax = plt.subplots(figsize=(20,7))\ndata_lim = df_train.loc[:, [\"date\", \"sales\"]][df_train.date>=\"2015-06\"].groupby(\"date\").sum()\ndata_pre = df_train.loc[:, [\"date\", \"sales\"]][df_train.date<\"2015-06\"].groupby(\"date\").sum()\ndata = df_train.loc[:, [\"date\", \"sales\"]].groupby(\"date\").sum()\n#data.head()\nax.plot(data_lim.rolling(window=30,\n                    center=True,\n                    min_periods=15).mean(),\n        aa=True,\n        color=\"red\",\n        alpha=0.4\n       )\nax.plot(data_pre.rolling(window=30,\n                    center=True,\n                    min_periods=15).mean(),\n        aa=True,\n        color=\"blue\",\n        alpha=0.4\n       )\nax.plot(data.rolling(window=365,\n                    center=True,\n                    min_periods=182).mean(),\n       aa=True,\n       color=\"black\"\n       )\nax.plot(data_lim.rolling(window=365,\n                        center=True,\n                        min_periods=182).mean(),\n       aa=True,\n       color=\"red\")\nax.plot(data_pre.rolling(window=365,\n                        center=True,\n                        min_periods=182).mean(),\n       aa=True,\n       color=\"blue\")\nax.legend(['30 day rolling window', '365 day rolling window'])\nax.set_title(\"Sales Trend - month + year\", fontsize=20)","dfbd36dd":"fig, ax = plt.subplots(figsize=(20,7))\ndata = df_train.loc[:,[\"date\",\"sales\"]].groupby(\"date\").sum()\nax.plot(data.rolling(window=7,\n                        center=True).mean(),\n       color=\"blue\")\nax.set_title(\"Moving average sales per day over a weekly window\", fontsize=20)","bbdb8ee3":"df = df_train\n\ndf[\"year\"],df[\"month\"], df[\"day\"] = pd.DatetimeIndex(df['date']).year, pd.DatetimeIndex(df['date']).month, pd.DatetimeIndex(df['date']).day\n\ndf['month'].replace([var for var in range (1, 13)],['Jan','Feb','Mar','Apr','May','June','July','Aug','Sept','Oct','Nov','Dec'],inplace=True)\ndf['month'] = pd.Categorical(df['month'],\n                             categories=['Jan','Feb','Mar','Apr','May','June','July','Aug','Sept','Oct','Nov','Dec'],\n                             ordered=True)\n\ndf = df.set_index('date')\ndf['dayofyear'] = df.index.dayofyear\ndf['dayofweek'] = df.index.dayofweek\ndf['week'] = df.index.week\n\ndf.head()","007d38a5":"fig, axs = plt.subplots(3, 1, figsize = (16, 15))\n\nfor year, color in zip(df.year.unique(), sns.color_palette(\"RdPu_r\")):\n#     yearly = train[train.year == year]\n    sns.lineplot(data = df[df.year == year].groupby('dayofyear')['sales'].mean(), color=color,ax = axs[0], linewidth = 1.5, label = str(year))\nsns.lineplot(data = df.groupby('dayofyear')['sales'].mean(), color = 'black',ax = axs[0], linewidth = 6, label = 'mean')   \n\n    \naxs[0].set_title(\"Yearly Sales\", fontsize = 18)\n    \nfor month, color in zip(df.month.unique(), sns.color_palette(\"winter\", n_colors = 12)):\n#     monthly = train[train.month == month]\n    sns.lineplot(data = df[df.month == month].groupby('day')['sales'].mean(), color=color,ax = axs[1], linewidth = 1.5, label = month)             \nsns.lineplot(data = df.groupby('day')['sales'].mean(), color = 'black',ax = axs[1], linewidth = 6, label = 'mean')   \n\naxs[1].set_title(\"Monthly Sales\", fontsize = 18)\n\nfor week, color in zip(df.week.unique(), sns.color_palette('summer', n_colors = 53)):\n    sns.lineplot(data = df[df.week == week].groupby('dayofweek')['sales'].mean(), color=color, ax = axs[2], linewidth = 1.5)\nsns.lineplot(data = df.groupby('dayofweek')['sales'].mean(), color = 'black', ax = axs[2], linewidth = 6, label = 'mean')    \n\naxs[2].set_title(\"Weekly Sales\", fontsize = 18)\n\nplt.tight_layout()","e49f6686":"df_stores.head()","12c23199":"df_full = df_train.set_index(\"date\").join(df_oil, how=\"left\").join(df_stores[[\"type\",\"cluster\"]], on=\"store_nbr\", how=\"left\").rename(columns = {'dcoilwtico' : 'oilprice'})\n#df_full['dayofyear'] = df.index.dayofyear\ndf_full['dayofweek'] = df.index.dayofweek\n#df_full['week'] = df.index.week\n\ndf_full.head()","7011d86e":"df_neq = df_full.copy()","e1636ab0":"plt.subplots(figsize=(20,7))\ndata = df_neq[[\"sales\"]].groupby(\"date\").sum()\n\nplt.plot(data.rolling(window=7).mean())\n\ndat = data[(data.index>\"2016-04\") & (data.index<=\"2016-05\")].groupby(\"date\").sum()\nplt.plot(dat.rolling(window=7).mean(),\n        color=\"red\")","305c6b7d":"replace = df_full.loc[\"2016-06\":\"2016-08-30\", \"sales\"]\n\ndf_neq.loc[\"2016-04\":\"2016-06\",\"sales\"] = list(replace*1.08)\n# df_neq.loc[\"2016-04\":\"2016-05\",\"sales\"].head()\n\ndf_post2015 = df_neq.loc[\"2015-06\":].copy()\n\nplt.subplots(figsize=(20,7))\ndata = df_full[[\"sales\"]].groupby(\"date\").sum()\n\nplt.plot(data.rolling(window=7).mean(),\n        color=\"red\")\n\ndat = df_post2015[[\"sales\"]].groupby(\"date\").sum()\n# dat = dat[(dat.index>\"2016-04\") & (dat.index<=\"2016-05\")].groupby(\"date\").sum()\nplt.plot(dat.rolling(window=7).mean(),\n        color=\"blue\")","f3520089":"# df_full.to_csv(\"df_full.csv\")\n# df_neq.to_csv(\"df_neq.csv\")\n# df_post2015.to_csv(\"df_post2015.csv\")","0d94ad55":"from sklearn.linear_model import LinearRegression\nmodel = LinearRegression().fit(df_full.drop(\"sales\"), df_full[\"sales\"])\ny_pred = pd.DataFrame(data = model.predict(df_full.drop(\"sales\")),\n                      index = y.index,\n                      columns=y.columns)","84f96d85":"df_test.iloc[[0,-1]]","d960147f":"df_full.head()","5ab2154a":"df_full.groupby([\"date\",\"family\"]).sum()","0ec8a556":"unique = df_full.family.unique()","886cc310":"df_test.head()","0535a539":"This time we see it more clearly. We don't know what happened in 2014 but we do know that this might very well severly limit the predictive power of the model.\n\nGiven we are looking at data for the future, the massive shift we see happening before and after mid-2015, will confuse the model. As it will train to predict ALL datapoints, date itself as a continous predictor will he hella confusing. In order to minimize the loss, it will shift it's prediction inbetween the average. Let's just grab some info to tell how much a difference we are facing.\n\nThis goes along with the oil-prices which behaved massively different after 2015. However the generally more eradic behavior of pre-2015 in sales doesn't go along with a similar behavior in the oil-prices. They are arguably more eradic after 2016, yet the sales seem to fall into a steady pattern of average sales outside of christmas and the earthquake.\n\nOn top of that, if we actually assume to disregard data, we might also take into consideration to clean the data around the earthquake. We can savely say that this datapoint is noise and nothing the model should learn to generalize.\n\n# Conclusion\nWith the three option to alter the training data, let's see out resulting datasets:\n1. full dataset\n2. full dataset with imputed data for earthquake\n3. post2015_06 data with imputed data for the earthquake\n\nThe non-altered post2015 set will not be created, due to the increased influence of the unordinary data. In fact it is to be highly expected, that taking in the earthquake-data will only result in worse predictions.","5bac3b96":"# Modelling and predictions\n\nI'll start with a default-prediction as a jumping off point to evaluate later models. The idea is to have a prediction that involves as little work while providing some at least somewhat reasonable result.\n\nFor this exercise I will predict the data for 2017 by using the data from 2016 of the same timeframe.","9911c3c1":"# transactions\n\nSecond to last will be the transactions table. There isn't much here, just the number of tansactions in any particular store at any day.\n\nUnlike the oil-price, transactions are not given for the test-data. Thus they cannot be used for the model.\n\nI don't know if this is an oversight from the test-creation or if there is indeed a use-case where this kind of data would be present.","aac36bc9":"# Holidays\n\nNext up let's look at the holidays.\n* holidays that got moves have a True in transferred -> those were normal workdays and can be dropped\n* days leading up to holidays or that happened after one have a \"+\" or \"-\" in their description\n\nAfter running into a bunch of errors, holidays are VERY inconsistent. There is no data for the 25th of december of any year, yet other holidays. Also after plotting holidays onto a sales-chart, there is little consistency among the influence. Some holidays coincide with the high sales, other are just located on some slopes and one the first of january comes with actually 0 sales.\n\nThis level of inconsistency hardly offers any data for the model to learn and generalize.\n\nJust to be absolutly sure and because we might discard the pre-2015 data, let's take a look at the holidays of 2015 and plot them together with the sales numbers. As we will see, there is no noticable correlation inbetween the two.\n\n## Conclusion\n\n- Mark Holidays as option for OneHotEncoding","060921aa":"# Next Step\n1. Going through the TimeSeries course to actually understand option to deal with this kind of data xD\n2. Chose the post_2015_noeq dataset(note)\n3. Use newly learned tools to gain additional insights for modelling\n\nNote: given the further exploration takes additional time, it seems less reasonable to try doing the same process for the other datasets.\nGiving the model data to predict something ","d553c1dd":"# Train data\n\nNow let's take a look at the final dataset, which is the training data.\n\nIt's having about 3 million entries within 6 columns.\n\nWith about 56k entries per shop, saying per day and per product-family how many sales happened and if this family was having an promotion at the time. Now we cannot say what kind of promotion was happening - if it means actual time-limited sales or more time-fluid things like advertisements. Ads would be expected to have effects on the sale even after the promotion ended.\n\nWe also gotta keep in mind that this is the primary data to work with, meaning the information of the other tables has to be joined onto this one, both for the training and the testing data.\n\nThe \"onpromotion\" is a tricky thing though. It's but a counter of how many items in the specific categorie are on sale at the specific date. Which in itself is very bare information, as obviously promotions and as such sales affect different items differently. For example, if a medium priced product is reduced to compete with a cheap product, people might grab the reduced one instead of the generally cheap one, not generating any additional sales. Imagine toilet paper, just because it is cheaper, doesn't mean people will buy more of it. Unless they buy it in bulk. Point is, there are different strategies of people to react and utilize sales of different products and as such, the mere information how many products are on promotion at a given time, is a good generalization (as we are in no position to predict every single sale) BUT it also means this feature will contain a lot of noise and unless the noise cancels eachother out, might propably not be the best predictor.\n\nLong ramble aside, it's also important to format the date correctly and infer the weekday. We gotta OneHotEncode that so the model can identify specific behavior for each day of the week.\n\n# Conclusion\n\n+ Join oil-price, store-type and store-cluster\n+ [edit: add columns for time-series related EDA]","94b6342d":"# Oilprices\n\nAfter importing the datasets, let's go through them one by one.\n\nThough first question to ask is, what are we going to do?\nGiven we want to predict sales based on several predictors, we got ourself a simple regression problem. Now what we gonna do is figuring out, what kind of information we got, how useful it might be and what type of cleaning it requires as well as how we might approach it (dropping or filling).\n\nFirst up we look at the oil prices.\nNow at a first glance we will find a couple dozen null-values. Obviously we cannot assume oil didn't exist at these days or somehow didn't have a value.\nSo while we could try researching the oil prices on these days, given we have over 2000 entries and thus a very small amount of missing data, I will just fill the missing values with the inbetween of the most previous and next non-null entry.\nI do this by first taking the indices of all missing values, then use both the forward-fill and backward-fill method and just average the two entries for the missing values.\nAfter that, formatting the datetime correctly and plotting the oil-prices shows a decent graph with nothing seeming out of place.\n\nHowever apart from a massive drop of oil prices from 2014-2015, it's also worth noting that this data is actually a false-friend. For a future prediction, the oilprice cannot be known in advance. So if the oilprice is included, this would limit the models ability to predict future data and instead would merely serve to create a modelled dataset that can be compared to real data to identify inconsistencies - like identify over-\/underperformance which might spark further investigations into the difference.\n\n\n## Conclusion\n\n* include oil-prices in the dataset\n* consider evaluation if oil-prices actually proof themself a **significant predictor**","a90f2a70":"# Stores\n\nNow we look at the information we have of the stores.\nWe got 54 different stores, with 5 different types in 17 clustern.\nGrouping by the clusters shows that each cluster except for 10 only has one type of stores present.\nAccording to the data-description, the clustering happened on similar stores, so it's not exactly a surprise that there is only one store-type per cluster.\nShort notice that type-cluster groupby was also tried but didn't really offer any insights.\n\nIf we break it down further to count the different cities in the cluster... well it's getting complicated. Appearently the same cluster can cover several cities.\nThis is important as it undermines most information we could get from a cluster. If the clusters where in the same city, it could be assumed they are not considered competitors and thus their sales could be added up to reduce complexity - as we'd assume that people just chose the first store of the cluster they came across, instead of making a conscious decision to go to a specific store. However given clusters neither cover similar store-types nor regions, it's hard to determine what kind of information we can actually get from the cluster.\n\nMaybe a later look into sales-numbers can offer more significant differences inbetween the cluster. If not we might need to do some more research on how the cluster came to be assigned.\nHowever matter of fact is, at the current level of insight, the clusters don't seem to contain any distinct information that would make it a good predictor for any model. So let's keep it in mind as a column that might be safe to drop.\nSpeaking of dropping, taking a quick look at cities and states, we see each city has one state. So there is no trouble with doubled city-names. However we got some states with several cities. Meaning dropping either column is currently not feasable.\n\nIn the end, there isn't much to do right now. The table contains a lot of distinct entries which may or may not provide useful information for a later regression. We just need to keep in mind it's categorical data and must be encoded properly.\nFor the encoding we need to determine wether or not there is any significant relationship inbetween sales and cities or states, as this could be used to justify ordinal-encoding with it's implied order. However given the size of distinct values for both cities and states, One-Hot-Encoding might introduce a lot of additional dimensions for the regression and as such should be avoided if there is no significant relationship to be found inbetween either and the target.\n\n## Judgement\nGiven most cities have only one shop to begin with, the model has nothing to gain here. Even if we .groupby(\"state\") we get to little variation. We'd need significantly more entries to have a chance to gain some information out of it that isn't just noise. 10 would still be to little for that, let alone 1 shop which means it's identical to the shop-ID as far as distinct information goes.\n\nFor the cluster I can't find any relation or information that would imply these have any significance. Thus I could try including it just for fun, but I can't argue it's important.\n\nNow I haven't looked into type, but given it's only 5 entries we can easily OneHotEncode it without bloating the model all to much. And it is to assume the type of store has influence on their sales-numbers in general. Plus we'd want to at least give the model a chance to find some generalizing information and I just decided to discard all other information that would allow any kind of grouping of shops.\n\n## Conclusion\n- test with type and cluster\n- test with type\n- note that without inherant order, OHE is necessary and this will heavily lengthen the dataframe","0132e3c6":"# Create Datasets\n\n- join data\n- create copy with no-earthquake\n- copy post 2015-06 into own set","6e1e383d":"Most notably about the oilprice is the massive drop halfway through 2014. Ignoring most of the fluctuation, we could almost think about just halving the chart with a decently stable price at around 100 before the drop and 45 beginning 2015.","2ed3839b":"Todo: Interpolate-function for fillna()?","0264be84":"1. red = post 2015-06\n2. blu = pre 2015-06\n\nWe can see a couple of notable things.\n* over the years we see an upwards trend of the sales.\n* over the month-wide average, there is a clear spike at the end of the years, which gets more prominent in later years\n* 2013 is almost flat apart from christmas\n* 2014 is all over the place with large spikes an valleys with another spike mid 2015 that seems to establish a new baseline\n* after about half the year of 2015, the chart seems to flatten out, showing only 3 majore spikes\n    * two end-of-year spikes as usual\n    * one spike around the end of the first quarter 2016, propably related to the April 16, 2016 earthquake and subsequent donations happening\n* the monthly line is ragged, showing some periodic behavior","5d575250":"# Goal\nPredict the sales for the second half of August 2017.\n\n# Process\n1. go through each dataset, explore the content, determine usefulness, fillna and transform as seems fit\n2. join the datasets together to create a complete df_train and df_test\n3. create new notebook for model building - try different models (XGB, LGB, LSTM, RandomForrest?)"}}