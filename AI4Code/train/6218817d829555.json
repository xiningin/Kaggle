{"cell_type":{"7de89bc3":"code","daae51b6":"code","556308dd":"code","783c464c":"code","85355c5f":"code","5bcc8de9":"code","2c7711f9":"code","76ece156":"code","395220ba":"code","68a864b8":"code","8cfb5575":"code","52ddbedf":"code","ce0cd8be":"code","ddf1044c":"code","012fce98":"code","98d47aaa":"code","79528d06":"code","ed4b864f":"code","8ed30867":"code","123f3d1d":"code","4d90fe7a":"code","56cdbfc1":"code","4be88e35":"code","c2d6793e":"code","3666983a":"code","a63a6579":"code","78717809":"code","afb60609":"code","9215780c":"code","6167c842":"code","97619812":"code","beb89a17":"code","c1d63b7a":"code","efe59df0":"code","119f1e6b":"code","1ea7fa0b":"code","7cc12b0d":"code","46410cc1":"code","de2f5028":"code","abea74a4":"code","5ad23360":"code","5e03998f":"code","6812e688":"code","13785f89":"code","d68ebe55":"markdown","7eaf79a5":"markdown","bf282af1":"markdown","0ea681da":"markdown","6109e9e9":"markdown","212160c3":"markdown","dee6bd5c":"markdown","c7f18b2a":"markdown","bdad309c":"markdown","5c9773c1":"markdown","1f8dac00":"markdown","c23eaa8a":"markdown","a7120241":"markdown","e2cece84":"markdown","f34f2a02":"markdown","364df91e":"markdown","5cd8f16c":"markdown","1fb5648f":"markdown","8c9b11a2":"markdown"},"source":{"7de89bc3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","daae51b6":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport matplotlib.ticker as ticker\nimport plotly.offline as py\nimport plotly.express as px\nimport plotly.graph_objs as go\nfrom scipy import stats\nfrom scipy.stats import norm, skew \nfrom sklearn.preprocessing import LabelEncoder\n\nplt.rc(\"font\", size=15)\n%matplotlib inline\nimport warnings\nwarnings.simplefilter(action='ignore')\nsns.set_style(\"whitegrid\")\nplt.style.use(\"fivethirtyeight\")","556308dd":"#Now let's import and put the train and test datasets in  pandas dataframe\n\ntrain = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","783c464c":"train.head()","85355c5f":"test.head()","5bcc8de9":"#saving the 'Id' column for submission\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#NDropping the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)\n\n#checking the data size after dropping the 'Id' variable\nprint(\"\\nThe train data size after dropping Id feature is : {} \".format(train.shape)) \nprint(\"The test data size after dropping Id feature is : {} \".format(test.shape))","2c7711f9":"train.info()","76ece156":"#exploring the categorical and continuous variables in training data.\n\ncategorical, numerical = [],[]\n\nfor i in train.columns:\n    c = train.dtypes[i]\n    if c=='object':\n        categorical.append(i)\n    else:\n        numerical.append(i)\n        \nprint('Numerical Features:{}'.format(len(numerical)))\nprint('\\nNumericaL:\\n{}'.format(numerical))\nprint('\\nCategorical Features:{}'.format(len(categorical)))\nprint('\\nCategoricaL:\\n{}'.format(categorical))","395220ba":"#exploring the categorical and continuous variables in test data.\n\ncategorical, numerical = [],[]\n\nfor i in test.columns:\n    c = test.dtypes[i]\n    if c=='object':\n        categorical.append(i)\n    else:\n        numerical.append(i)\n        \nprint('Numerical Features:{}'.format(len(numerical)))\nprint('\\nNumericaL:\\n{}'.format(numerical))\nprint('\\nCategorical Features:{}'.format(len(categorical)))\nprint('\\nCategoricaL:\\n{}'.format(categorical))","68a864b8":"plt.figure(figsize=(20,6));\nsns.heatmap(train.isnull(),yticklabels=False, cbar=False, cmap='viridis')","8cfb5575":"#missing value proportion in train dataset.\n\nvariable = [feature for feature in categorical if train[feature].isnull().sum()]\nfor feature in variable:\n    print(\"{}: {}%\".format(feature,np.round(train[feature].isnull().mean(),3)))","52ddbedf":"#missing value proportion in test dataset\n\nvariable = [feature for feature in categorical if test[feature].isnull().sum()]\nfor feature in variable:\n    print(\"{}: {}%\".format(feature,np.round(test[feature].isnull().mean(),3)))","ce0cd8be":"train.fillna({'Alley': 'None', 'Fence':'None', 'MiscFeature':'None', \n           'PoolQC':'None', 'FireplaceQu':'None', 'MasVnrType':'None'}, inplace = True)","ddf1044c":"train.fillna({'BsmtQual':'None', 'BsmtCond':'None',\n           'BsmtExposure':'None', 'BsmtFinType1':'None',\n           'BsmtFinType2':'None'},inplace=True)","012fce98":"train.fillna({'GarageType':'None','GarageCond': 'None', 'GarageQual':'None', \n           'GarageQual':'None', 'GarageFinish': 'None'}, inplace=True)","98d47aaa":"#Replacing other categorical variables\n\ntrain['MSZoning']=train['MSZoning'].fillna(train['MSZoning'].mode()[0])\ntrain['Electrical']=train['Electrical'].fillna(train['Electrical'].mode()[0])\ntrain['Functional']=train['Functional'].fillna(train['Functional'].mode()[0])\ntrain['KitchenQual']=train['KitchenQual'].fillna(train['KitchenQual'].mode()[0])\ntrain['SaleType']=train['SaleType'].fillna(train['SaleType'].mode()[0])\ntrain['Utilities']=train['Utilities'].fillna(train['Utilities'].mode()[0])\ntrain['LotFrontage']=train['LotFrontage'].fillna(train['LotFrontage'].mean())\ntrain['GarageYrBlt']=train['GarageYrBlt'].fillna(train['GarageYrBlt'].median())\ntrain['Exterior1st'].fillna('Other' ,inplace=True)\ntrain['Exterior2nd'].fillna('Other' ,inplace=True)","79528d06":"#Imputing missing values in test value dataset\n\ntest.fillna({'Alley': 'None', 'Fence':'None', 'MiscFeature':'None','PoolQC':'None', 'FireplaceQu':'None', 'MasVnrType':'None'}, inplace = True)\ntest.fillna({'BsmtQual':'None', 'BsmtCond':'None','BsmtExposure':'None', 'BsmtFinType1':'None','BsmtFinType2':'None'},inplace=True)\ntest.fillna({'GarageType':'None','GarageCond': 'None', 'GarageQual':'None', 'GarageQual':'None', 'GarageFinish': 'None'}, inplace=True)\n\ntest['MSZoning']=test['MSZoning'].fillna(test['MSZoning'].mode()[0])\ntest['Electrical']=test['Electrical'].fillna(test['Electrical'].mode()[0])\ntest['Functional']=test['Functional'].fillna(test['Functional'].mode()[0])\ntest['KitchenQual']=test['KitchenQual'].fillna(test['KitchenQual'].mode()[0])\ntest['SaleType']=test['SaleType'].fillna(test['SaleType'].mode()[0])\ntest['Utilities']=test['Utilities'].fillna(test['Utilities'].mode()[0])\ntest['LotFrontage']=test['LotFrontage'].fillna(test['LotFrontage'].mean())\ntest['GarageYrBlt']=test['GarageYrBlt'].fillna(test['GarageYrBlt'].median())\ntest['Exterior1st'].fillna('Other' ,inplace=True)\ntest['Exterior2nd'].fillna('Other' ,inplace=True)\n","ed4b864f":"variable = [feature for feature in numerical if train[feature].isnull().sum()]\nfor feature in variable:\n    print(\"{}: {}%\".format(feature,np.round(train[feature].isnull().mean(),3)))","8ed30867":"variable = [feature for feature in numerical if test[feature].isnull().sum()]\nfor feature in variable:\n    print(\"{}: {}%\".format(feature,np.round(test[feature].isnull().mean(),3)))","123f3d1d":"#Imputing Missing Numerical Basement Columns\n\nBsmt_con = ['MasVnrArea','BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath','BsmtHalfBath']\nfor Bsmt in Bsmt_con:\n    train[Bsmt].fillna(0, inplace=True) ","4d90fe7a":"#Imputing Missing Numerical Garage Columns\ntrain.fillna({'GarageCars':0, 'GarageArea': 0}, inplace = True)","56cdbfc1":"#Imputing Missing Numerical Basement Columns in Test Dataset\n\nBsmt_con = ['MasVnrArea','BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath','BsmtHalfBath']\nfor Bsmt in Bsmt_con:\n    test[Bsmt].fillna(0, inplace=True) \n    \n#Imputing Missing Numerical Garage Columns in Test Dataset\n\ntest.fillna({'GarageCars':0, 'GarageArea': 0}, inplace = True)","4be88e35":"plt.figure(figsize=(20,6));\nsns.heatmap(train.isnull(),yticklabels=False, cbar=False, cmap='viridis')","c2d6793e":"plt.figure(figsize=(20,6));\nsns.heatmap(test.isnull(),yticklabels=False, cbar=False, cmap='viridis')","3666983a":"cor = train.corr()\ncor.sort_values(['SalePrice'], ascending= False, inplace=True)\nprint(cor.SalePrice)","a63a6579":"# most correlated features\ncorrmat = train.corr()\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0.5]\nplt.figure(figsize=(14,8))\ng = sns.heatmap(train[top_corr_features].corr(),annot=True,cmap=\"viridis\")","78717809":"# Detecting outliers by plotting numerical colums against SalePrice\n\nplt.figure(figsize=(30, 70))\nsns.set(font_scale= 1.2)\nsns.set_style('whitegrid')\n\nfor i, features in enumerate(numerical):\n    plt.subplot(10, 4, i+1)\n    plt.scatter(data=train.iloc[:len(train)], x=features, y='SalePrice', color =\"blue\")\n    plt.xlabel(features)\n    plt.ylabel('SalePrice')\n    \n    \nsns.despine()","afb60609":"#Replacing the values for outliers\n\ntrain.LotFrontage[(train.LotFrontage >= 160)] = 160\ntrain.LotArea[(train.LotArea >= 75000)] = 75000\ntrain.MasVnrArea[(train.MasVnrArea >= 1000)] = 1000\ntrain.BsmtFinSF1[(train.BsmtFinSF1 >= 2500)] = 2500\ntrain.TotalBsmtSF[(train.TotalBsmtSF >= 3000)] = 3000\ntrain['1stFlrSF'][(train['1stFlrSF'] >= 3000)] = 3000\ntrain.GrLivArea[(train.GrLivArea >= 3500)] = 3500\ntrain.GarageArea[(train.GarageArea >= 1500)] = 1500\n","9215780c":"plt.figure(figsize=(14, 8))\nsns.kdeplot(data=train,x=\"SalePrice\", hue =\"MoSold\", fill=True,common_norm=False, palette=\"husl\", alpha=.5, linewidth=1)","6167c842":"sns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","97619812":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\n#Check the new distribution \nsns.distplot(train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n\n#Now plot the distribution\nplt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)],\n            loc='best')\nplt.ylabel('Frequency')\nplt.title('SalePrice distribution')\n\n#Get also the QQ-plot\nfig = plt.figure()\nres = stats.probplot(train['SalePrice'], plot=plt)\nplt.show()","beb89a17":"# Labelencoding the categorical variables\nle = LabelEncoder()\nvar_mod = train.select_dtypes(include='object').columns\nfor i in var_mod:\n    train[i] = le.fit_transform(train[i])\n    \nfor i in var_mod:\n    test[i] = le.fit_transform(test[i])","c1d63b7a":"train.info()","efe59df0":"test.info()","119f1e6b":"# Adding total sqfootage feature to train data\ntrain['TotalSF'] = train['TotalBsmtSF'] + train['1stFlrSF'] + train['2ndFlrSF']\n\n# Adding total sqfootage feature to test data\ntest['TotalSF'] = test['TotalBsmtSF'] + test['1stFlrSF'] + test['2ndFlrSF']","1ea7fa0b":"x = train.drop(['SalePrice'], axis=1)\ny = train['SalePrice']\n","7cc12b0d":"#modelling in training data with 75% of data kept for training and 25% to test\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.25, random_state=42)","46410cc1":"from sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import LassoCV, RidgeCV, ElasticNetCV\nfrom sklearn import metrics\nfrom sklearn.metrics import mean_squared_error","de2f5028":"def print_evaluate(true, predicted):  \n    mae = metrics.mean_absolute_error(true, predicted)\n    mse = metrics.mean_squared_error(true, predicted)\n    rmse = np.sqrt(metrics.mean_squared_error(true, predicted))\n    r2_square = metrics.r2_score(true, predicted)\n    print('MAE:', mae)\n    print('MSE:', mse)\n    print('RMSE:', rmse)\n    print('R2 Square', r2_square)","abea74a4":"linear.fit(X_train, y_train)\ntrain_pred = np.expm1(linear.predict(X_train))\npred = np.expm1(linear.predict(X_test))\nprint('Test set evaluation:\\n')\nprint_evaluate(np.expm1(y_test), pred)\nprint('*'*30)\nprint('Train set evaluation:\\n')\nprint_evaluate(np.expm1(y_train),train_pred)","5ad23360":"E_model.fit(X_train, y_train)\ntrain_pred = np.expm1(E_model.predict(X_train))\npred = np.expm1(E_model.predict(X_test))\nprint('Test set evaluation:\\n')\nprint_evaluate(np.expm1(y_test), pred)\nprint('*'*30)\nprint('Train set evaluation:\\n')\nprint_evaluate(np.expm1(y_train),train_pred)","5e03998f":"lasso.fit(X_train, y_train)\ntrain_pred = np.expm1(lasso.predict(X_train))\npred = np.expm1(lasso.predict(X_test))\nprint('Test set evaluation:\\n')\nprint_evaluate(np.expm1(y_test), pred)\nprint('*'*30)\nprint('Train set evaluation:\\n')\nprint_evaluate(np.expm1(y_train),train_pred)","6812e688":"ridge.fit(X_train, y_train)\ntrain_pred = np.expm1(ridge.predict(X_train))\npred = np.expm1(ridge.predict(X_test))\nprint('Test set evaluation:\\n')\nprint_evaluate(np.expm1(y_test), pred)\nprint('*'*30)\nprint('Train set evaluation:\\n')\nprint_evaluate(np.expm1(y_train),train_pred)","13785f89":"submission = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')\nE_model.fit(X_train, y_train)\nfinal_predictions = np.expm1(E_model.predict(test))\nsubmission['SalePrice'] = final_predictions\nsubmission.to_csv('my_submission.csv', index=False)\nsubmission.head()","d68ebe55":"### Log-transformation of the target variable","7eaf79a5":"Area related features are very important to determine house prices, adding one more feature which is the total area of basement, first and second floor areas of each house","bf282af1":"### As we can see above our dataset has following 79 features in the training dataset: \n\n**SalePrice** - the property's sale price in dollars. This is the target variable that we're trying to predict.\n\n1.\tMSSubClass: The building class\n\n2.\tMSZoning: The general zoning classification\n\n3.\tLotFrontage: Linear feet of street connected to property\n\n4.\tLotArea: Lot size in square feet\n\n5.\tStreet: Type of road access\n\n6.\tAlley: Type of alley access\n\n7.\tLotShape: General shape of property\n\n8.\tLandContour: Flatness of the property\n\n9.\tUtilities: Type of utilities available\n\n10.\tLotConfig: Lot configuration\n\n11.\tLandSlope: Slope of property\n\n12.\tNeighborhood: Physical locations within Ames city limits\n\n13.\tCondition1: Proximity to main road or railroad\n\n14.\tCondition2: Proximity to main road or railroad (if a second is present)\n\n15.\tBldgType: Type of dwelling\n\n16.\tHouseStyle: Style of dwelling\n\n17.\tOverallQual: Overall material and finish quality\n\n18.\tOverallCond: Overall rating of the condition.\n\n19.\tYearBuilt: The date when building was built.\n\n20.\tYearRemodAdd: Remodel date\n\n21.\tRoofStyle: Type of roof\n\n22.\tRoofMatl: Roof material\n\n23.\tExterior1st: Exterior covering on house\n\n24.\tExterior2nd: Exterior covering on house (if more than one material)\n\n25.\tMasVnrType: Masonry veneer type\n\n26.\tMasVnrArea: Masonry veneer area in square feet\n\n27.\tExterQual: Exterior material quality\n\n28.\tExterCond: Present condition of the material on the exterior\n\n29.\tFoundation: Type of foundation\n\n30.\tBsmtQual: Height of the basement\n\n31.\tBsmtCond: General condition of the basement\n\n32.\tBsmtExposure: Walkout or garden level basement walls\n\n33.\tBsmtFinType1: Quality of basement finished area\n\n34.\tBsmtFinSF1: Type 1 finished square feet\n\n35.\tBsmtFinType2: Quality of second finished area (if present)\n\n36.\tBsmtFinSF2: Type 2 finished square feet\n\n37.\tBsmtUnfSF: Unfinished square feet of basement area\n\n38.\tTotalBsmtSF: Total square feet of basement area\n\n39.\tHeating: Type of heating\n\n40.\tHeatingQC: Heating quality and condition\n\n41.\tCentralAir: Central air conditioning.\n\n42.\tElectrical: Electrical system\n\n43.\t1stFlrSF: First Floor square feet\n\n44.\t2ndFlrSF: Second floor square feet\n\n45.\tLowQualFinSF: Low quality finished square feet (all floors)\n\n46.\tGrLivArea: Above grade (ground) living area square feet\n\n47.\tBsmtFullBath: Basement full bathrooms\n\n48.\tBsmtHalfBath: Basement half bathrooms\n\n49.\tFullBath: Full bathrooms above grade\n\n50.\tHalfBath: Half baths above grade\n\n51.\tBedroom: Number of bedrooms above basement level\n\n52.\tKitchen: Number of kitchens\n\n53.\tKitchenQual: Kitchen quality\n\n54.\tTotRmsAbvGrd: Total rooms above grade (does not include bathrooms)\n\n55.\tFunctional: Home functionality rating\n\n56.\tFireplaces: Number of fireplaces\n\n57.\tFireplaceQu: Fireplace quality\n\n58.\tGarageType: Garage location\n\n59.\tGarageYrBlt: Year garage was built\n\n60.\tGarageFinish: Interior finish of the garage\n\n61.\tGarageCars: Size of garage in car capacity\n\n62.\tGarageArea: Size of garage in square feet\n\n63.\tGarageQual: Garage quality\n\n64.\tGarageCond: Garage condition\n\n65.\tPavedDrive: Paved driveway\n\n66.\tWoodDeckSF: Wood deck area in square feet\n\n67.\tOpenPorchSF: Open porch area in square feet\n\n68.\tEnclosedPorch: Enclosed porch area in square feet\n\n69.\t3SsnPorch: Three season porch area in square feet\n\n70.\tScreenPorch: Screen porch area in square feet\n\n71.\tPoolArea: Pool area in square feet\n\n72.\tPoolQC: Pool quality\n\n73.\tFence: Fence quality\n\n74.\tMiscFeature: Miscellaneous feature not covered in other categories\n\n75.\tMiscVal: $Value of miscellaneous feature\n\n76.\tMoSold: Month Sold\n\n77.\tYrSold: Year Sold\n\n78.\tSaleType: Type of sale\n\n79.\tSaleCondition: Condition of sale\n","0ea681da":"## **Imputing Missing Values**","6109e9e9":"# **House Price Prediction using Adavance Regression Techniques in Python**\n\nThis notebook will look into the various techniques of regression that we could work upon to predict the sale prices of the [Acme Housing dataset](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques).","212160c3":"### Evaluating Regression Models\n\nFor evaluating which regression model is best we have 3 metrics:\n\n1. **R Square** : measures the variability in dependent variable as explained by the model. The more variability explained by the model, better preditction results we will get. It is calculated by dividing the sum of squared of prediction error by the total sum of square which replace the calculated prediction with mean. R Square value is between 0 to 1 and bigger value indicates a better fit between prediction and actual value.\n\n2. **Mean Square Error(MSE)\/Root Mean Square Error(RMSE)**: Absolute measure of the goodness for the fit of the model.\nMSE is calculated by the sum of square of prediction error which is real output minus predicted output and then divide by the number of data points. It gives you an absolute number on how much your predicted results deviate from the actual number. \nRoot Mean Square Error(RMSE) is the square root of MSE. It is used more commonly than MSE because MSE value can sometimes be too big to compare easily and also since MSE is calculated by the square of error, square root brings it back to the same level of prediction error and make it easier for interpretation.\n\n3. **Mean Absolute Error(MAE)** : Similar to RMSE, but instead of taking the sum of square of error in MSE, MAE takes the sum of absolute value of error. MSE also gives larger penalisation to big prediction error by square it while MAE treats all errors the same.","dee6bd5c":"**Model Fitting**","c7f18b2a":"Keeping above assupmtions in mind. The target variable had to be normally skewed but as we can see in above graph it is rightly skewed. We need to transform this variable and make it more normally distributed.","bdad309c":"## Analysis of Target Variable: Sales Price.","5c9773c1":"**Garage features have equal proportion of missing values**, therefore, we can impute all the garage features having NA with None.","1f8dac00":"### Outlier Detection","c23eaa8a":"### Importing Important Libraries","a7120241":"#### There are few assumptions that we need to take care when building a linear model, they are:\n\n1. Linearity of parameters.\n2. The mean of residuals is zero and residuals follows normality.\n3. The sample is representative of the population at large.\n4. No Endogentiy: The independent variables and residuals are uncorrelated. The independent variables are measured with no error.\n5. No multicollinearity within independent variables.\n6. Homoscedasticity of residuals : The variance of the residuals is constant across observations.\n7. No autocorrelation of residuals.","e2cece84":"### Dealing with Missing Values","f34f2a02":"**What is Regression ?**\n\nRegression analysis is a statistical modelling technique that we use to estimate relationship between a dependent variable (output\/target variable) and one or more independent variables (predictors\/features).\n\n**What are the types of Regression Analysis?**\n\nTypes of Regression Analysis to be used in a problem is decided on the following criteria:\n\n1. Number of independent Variables.\n2. The Regression Line's Shape\n3. Dependent Variable's Type\n\nSome Important Types of Regression are:\n\n1. **Linear Regression** : When the Regression Line is stratight that is the relationship between independent and dependent variable is linear. Power of independent variable is 1. Can be further divide into:\n\n* Simple Regression Model : Single independent variables.\n* Multiple Regression Model : Multiple independent variables.\n\n2. **Polynomial Regression** : When the Regression Line curve that is the relationship between independent and dependent variable is non-linear. Power of independent variable is more than 1.\n\n3. **Ridge Regression** : Techinque used to add a penalty parameter\/degree of bias to the regression estimates to reduce the effect of large variations caused due to existence of multicollinearity. It uses [L2 regularization](https:\/\/www.analyticsvidhya.com\/blog\/2018\/04\/fundamentals-deep-learning-regularization-techniques\/) that is \"squared magitude\" of coefficiet for pealyt term.\n\n4. **Lasso Regression** : Least Absolute Shrinkage and Selection Operator adds \u201cabsolute value of magnitude\u201d of coefficient as penalty term and regularizes the model using [L1 ](http:\/\/https:\/\/www.analyticsvidhya.com\/blog\/2018\/04\/fundamentals-deep-learning-regularization-techniques\/) regularization. In addition to penalizing the regressio coefficients, it reduces the variability and improves accuracy and since it shrinks the less important feature's coefficients to zero, can be used for feature selection as well.\n\n5. **ElasticNet Regression** : ElasticNet is a technique that combines Lasso with Ridge Regression. As a regularizer, it is trained with L1 and L2. When there are many correlated features, Elastic-net is useful because Lasso is likely to pick one at random but elastic-net is likely to pick both. Elastic-Net can also inherit some of Ridge's rotational stability owing to a trade-off between Lasso and Ridge.\n\n6. **Logistic Regression** : Here the depedet variable is binary. This type regression technique is generally used in classification problems where we have more two (logistic) or multiple (multinomial logistic regression) dependent variables.","364df91e":"**Basement features have equal proportion of missing values**, therefore, we can impute all the basement features having NA with None.","5cd8f16c":"Following features have most percentage of null values:\n\n**Alley, Fence, MiscFeature, PoolQC, FireplaceQu, MasVnrType :** 93%, 80%, 96%, 99%, 48%, 8%\n\nTherefore, we can replace the values with \"None\" as these missing values indicate their value is equal to None.","1fb5648f":"### Data Reading and Exploration","8c9b11a2":"From the above graphs we can see for the varibles **LotFrontage, LotArea, MasVnrArea, BsmtFinsf1, TotalBsmtsf, 1stFlrsf, 2ndFlrsf, GrLivArea, GrageArea** data points are present that have extremely large values for very low prices. These values are called oultliers and are bad for our prediction as they add weights that are completely distinguished from normal value. Therefore, we replace these outliers by its lower values as deleting them will lead to loss in data."}}