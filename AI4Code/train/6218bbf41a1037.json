{"cell_type":{"ad2ae37f":"code","32bc8142":"code","cef8eda9":"code","3147f3f7":"code","836478ca":"code","bcac27c8":"code","ed6a44e2":"code","fc655e73":"code","d5aeae8d":"code","32143c40":"code","1ede2ef4":"code","af396ab2":"code","a403cb70":"code","8cea4095":"code","c7dcb4ce":"code","05934be0":"markdown","b3af5f8b":"markdown","fb18d9a5":"markdown","490fe259":"markdown","283ad7ce":"markdown","482e00a5":"markdown","847340b0":"markdown","ffdac4ea":"markdown","7cff7397":"markdown","b734bd9a":"markdown","7bf2cbeb":"markdown","bbb7b72d":"markdown","20738f66":"markdown","6ea25339":"markdown","0a92b71e":"markdown"},"source":{"ad2ae37f":"! wget http:\/\/nlp.stanford.edu\/software\/stanford-corenlp-full-2018-10-05.zip\n! unzip stanford-corenlp-full-2018-10-05.zip\n! pip install stanfordcorenlp\n! pip install stanford-openie\n! pip install sentence-transformers\n! pip install circlify\n","32bc8142":"# import pandas as pd\n# from stanfordnlp.server import CoreNLPClient\n# os.environ[\"CORENLP_HOME\"]='..\/working\/stanford-corenlp-full-2018-10-05'\n\n# df=pd.read_parquet('data\/doc_processed.parquet', engine='pyarrow')\n# df['doc_id']=df.index\n# df['text']=df['body_text']+'\\n'+df['abstract']\n# print(len(df))\n# df_para=pd.DataFrame()\n\n# def clean_text(text, client):\n#     # remove anything that is not noun or a special entity\n#     ann = client.annotate(text)\n#     clean_tokens = set()\n#     for sentence in ann.sentence:\n#         length = len(sentence.token)\n#         clean_tokens.update([sentence.token[i].word for i in range(length) if ('NN' in sentence.token[i].pos) and not has_numbers(sentence.token[i].word) and not is_pun(sentence.token[i].word) ])\n#     return list(clean_tokens)\n\n# with CoreNLPClient(annotators=['tokenize','ssplit', 'pos'], timeout=30000, memory='16G') as client:\n#     for index, row in tqdm(df.iterrows(), total=len(df)):\n#         text=row['text']\n#         paragraphs=text.split('\\n')\n#         for paragraph in paragraphs:\n#             df_para=df_para.append({'doc_id':row['doc_id'], 'text':paragraph, 'clean_text':' '.join(clean_text(paragraph, client))}, ignore_index=True)\n\n# df_para['para_id']=df_para.index\n# print(len(df_para))\n# df_para.to_parquet('data\/para_processed.parquet', engine='pyarrow')\n# display(df_para.head(5)['clean_text'])","cef8eda9":"# load datasets\nimport pandas as pd\ndf_doc=pd.read_parquet('..\/input\/data-covid\/doc_processed.parquet', engine='pyarrow')\nprint('Number of documents', len(df_doc))\ndf_para=pd.read_parquet('..\/input\/data-covid\/para_processed.parquet', engine='pyarrow')\nprint('Number of paragraphs', len(df_para))","3147f3f7":"import pandas as pd\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom scipy import sparse\nimport numpy as np\n\n# create indexes for search\npara_id_lookup={}\nvectorizer={}\n\nvectorizer_keyword = CountVectorizer(tokenizer=lambda x: list(set(x.split(' '))))\nkeyword_to_para=sparse.csr_matrix(np.transpose(vectorizer_keyword.fit_transform(df_para['clean_text'].values)))\nprint('Index created! Shape:', keyword_to_para.shape)\npara_id_lookup['keyword']=keyword_to_para\nvectorizer['keyword']=vectorizer_keyword\n","836478ca":"# some utility functions\ndef find_indices(entity_aliases,type, para_id_lookup, vectorizer):\n    ent_ids = set()\n    ent_doc_ids = set()\n    for entity_alias in entity_aliases:\n        if entity_alias not in vectorizer[type].vocabulary_.keys():\n            continue\n        ent_ids_tmp = para_id_lookup[type][vectorizer[type].vocabulary_[entity_alias]].indices\n        ent_doc_ids_tmp = set(df_para[df_para['para_id'].isin(ent_ids_tmp)]['doc_id'].values)\n        ent_ids.update(ent_ids_tmp)\n        ent_doc_ids.update(ent_doc_ids_tmp)\n\n    return ent_ids, ent_doc_ids\n\ndef find_pairwise_strengths(entities_l, entities_r, para_id_lookup, vectorizer):\n    results = pd.DataFrame()\n    for entity_l, type_l in entities_l:\n        entity_l_title = entity_l[0]\n        entity_l_aliases = entity_l[1]\n        ent_l_ids, ent_l_doc_ids = find_indices(entity_l_aliases, type_l, para_id_lookup, vectorizer)\n        for entity_r, type_r in entities_r:\n            entity_r_title = entity_r[0]\n            entity_r_aliases = entity_r[1]\n            ent_r_ids, ent_r_doc_ids = find_indices(entity_r_aliases, type_r, para_id_lookup, vectorizer)\n\n            matched_para_ids = set(ent_l_ids).intersection(ent_r_ids)\n            matched_doc_ids = set(ent_l_doc_ids).intersection(ent_r_doc_ids)\n\n            score_para = len(matched_para_ids)\n            score_doc = len(matched_doc_ids)\n            \n            if score_doc > 0 or score_para > 0:\n                results = results.append(\n                    {'entity_l': entity_l_title, 'entity_r': entity_r_title, 'score_doc': score_doc, 'score_para':score_para, 'matched_para_ids': matched_para_ids, 'matched_doc_ids':matched_doc_ids},\n                    ignore_index=True)\n            \n        return results","bcac27c8":"\"\"\"For this analysis, we will create a list for each entity group (CoronaVirus and Animals) with the following format:\n[\n('<entity_name>', {<aliases>}, '<entity_type viz. keyword location etc.>'),\n('<entity_name>', {<aliases>}, '<entity_type>')\n]\n\n# Do not worry to much about this format, it will be usuful in future if data is pre tagged with location, disease etc. \n\"\"\"\n","ed6a44e2":"expanded_keywords=[]\n\nkeyword='covid'\n# find all tokens from the text that have the above keyword in them \nexpanded_keywords.extend([item for item in vectorizer_keyword.get_feature_names() if keyword.lower() in item.lower()])\n\nkeyword='coronavirus'\n# find all tokens from the text that have the above keyword in them \nexpanded_keywords.extend([item for item in vectorizer_keyword.get_feature_names() if keyword.lower() in item.lower()])\n\n# put them in a particular format that is accepted by the 'find_pairwise_strengths' funtion\n# all these keywords will be used to find the entity 'CoronaVirus'\nentity_l_='CoronaVirus' # give a name to the this entity\nentities_l=[((entity_l_, set(expanded_keywords)), 'keyword')]\n\nprint(entities_l)","fc655e73":"# read a list of animal names and create tokens from it (1-gram)\nanimal_words=[line.strip() for line in open('..\/input\/nlp-lists\/animals.txt','r').readlines()]\nanimal_words=set(animal_words)\nentities_r_dict={}\nfor animal_word in animal_words:\n    for token in animal_word.split(' '):\n        token=token.lower()\n        if token not in entities_r_dict.keys():\n            entities_r_dict[token]=set()\n        entities_r_dict[token].add(token)\n\n# put them in a particular format that is accepted by the 'find_pairwise_strengths' funtion\n# all these keywords will be used to find the entity type 'Animal' \n#        consisting of several entites (bats, cats, rats and so on)\nentities_r=[((k,v), 'keyword') for k, v in entities_r_dict.items()]\nprint(entities_r)","d5aeae8d":"# find pairwise strengths between coronavirus and several animals\nresults=find_pairwise_strengths(entities_l, entities_r, para_id_lookup, vectorizer)\n\n# rank animal entities (animals) based on co-occurance at different levels \n# show results\n\nprint(\"Ranking based on co-occurance at document level\")\nresults_=results.sort_values(by=['score_doc'], ascending=False).head(10)\ndisplay(results_[['entity_r','score_doc']])\ntop_contenders=set(results_['entity_r'].values)\n\nprint(\"Ranking based on co-occurance at paragraph level\")\nresults_=results.sort_values(by=['score_para'], ascending=False).head(10)\ndisplay(results_[['entity_r','score_para']])\ntop_contenders.update(results_['entity_r'].values)\n\nprint('List of top Contenders (sum of above list):',', '.join(top_contenders))","32143c40":"from openie import StanfordOpenIE\nimport json\nfrom tqdm import tqdm\nimport os\nfrom sentence_transformers import SentenceTransformer\n\n# set some properties for the extractors\nbertify=SentenceTransformer('bert-base-nli-mean-tokens')\nos.environ[\"CORENLP_HOME\"]='..\/input\/stanford-resources\/stanford-corenlp-full-2018-10-05\/'\nie_properties={\"openie.triple.strict\":\"true\", \"openie.max_entailments_per_clause\":\"1\", \"splitter.disable\":\"true\"}\n    \n\nwith StanfordOpenIE() as client: # start the Stanford IE engine for Subject, relation, Object extraction\n    triple_results = pd.DataFrame()\n    for _, row in results[(results['entity_l']==entity_l_) & (results['entity_r'].isin(top_contenders))].iterrows():\n        \n        # for each animal find all the paragraphs where it occurs\n        entity_r_title=row['entity_r']\n        print('\\n',entity_r_title)\n        for index in tqdm(row['matched_para_ids'], total=len(row['matched_para_ids'])):\n            text=df_para.iloc[index]['text']\n            entity_aliases=entities_r_dict[entity_r_title]\n            \n            \n            def check_alias(triple, expanded_keywords, aliases):\n                # chech within a triple, if both types of enitties (animal and coronavirus) are mentioned\n                to_serach=triple['subject']+' '+triple['relation']+' '+triple['object']\n                for alias_a in aliases:\n                    if alias_a in to_serach:\n                        for alias_b in expanded_keywords:\n                            if alias_b in to_serach:\n                                return True\n\n                return False\n            \n            # mask the occurance of the entity by its type (camel->animal, 'coronavirus' et al.-> CoronaV)\n            def mask_instance(sentence, expanded_keywords, aliases):\n                for alias in aliases:\n                    if alias in sentence:\n                        sentence=sentence.replace(alias, 'Animal')\n                        break\n                for alias in expanded_keywords:\n                    if alias in sentence:\n                        sentence=sentence.replace(alias, entity_l_)\n                        break\n                return sentence\n            \n            # collect all the sentenses (clauses) where both the entity types are present (corona and animal)\n            for triple in [triple for triple in client.annotate(text , properties=ie_properties)\n                            if check_alias(triple, expanded_keywords, entity_aliases)]:\n                triple_results= triple_results.append({'sentence':mask_instance(triple['subject']+' '+triple['relation']+' '+triple['object'], expanded_keywords, entity_aliases), 'relation':triple['relation'],'subject':triple['subject'],'object':triple['object'], 'triple':json.dumps(triple) , 'entity':entity_r_title}, ignore_index=True)\n    \n    print(len(triple_results), 'sentences found!')\n    print('calculating bert embeddings..')\n    triple_results['embedding_bert']=bertify.encode(triple_results['sentence'].values)\n    triple_results.to_parquet('..\/working\/sentences_bert.parquet', engine='pyarrow')","1ede2ef4":"import plotly.express as px\nfrom sklearn.manifold import TSNE\nfrom plotly.offline import init_notebook_mode, iplot\ninit_notebook_mode(connected=True)         # initiate notebook for offline plot\n\ntriple_results=pd.read_parquet('..\/working\/sentences_bert.parquet', engine='pyarrow')\nX = np.array(list(triple_results['embedding_bert'].values))\ntriple_results_sampled=triple_results\n# print('data shape', X.shape)\n\n# cluster sentences: run t-SNE on bert vectors of sentences\nX_embedded = TSNE(n_components=2).fit_transform(X)\n# print('TSNE embedding shape', X_embedded.shape)\n\ntriple_results_sampled['x_axis'] = X_embedded[:, 0]\ntriple_results_sampled['y_axis'] = X_embedded[:, 1]\ntriple_results.to_parquet('..\/working\/sentences_bert_tsne.parquet', engine='pyarrow')\n\nfig = px.scatter(triple_results_sampled, x='x_axis', y='y_axis', color='entity', hover_data=['entity','sentence'])\nentity_color_lookup={item.legendgroup: item.marker.color for item in fig.data}\niplot(fig)","af396ab2":"posed_relation=\"CoronaVirus in Animals caused outbreak\"","a403cb70":"import plotly.express as px\nfrom plotly.offline import plot\nimport plotly.graph_objects as go\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport math\nfrom random import seed, random\nimport circlify\nfrom sentence_transformers import SentenceTransformer\n\n\n# initialize\/load stuff\nseed(1)\nbertify=SentenceTransformer('bert-base-nli-mean-tokens')\ntriple_results=pd.read_parquet('..\/working\/sentences_bert_tsne.parquet', engine='pyarrow')\nfig = go.Figure()\n\n# create a bert vector for the posed-relation\nrelation_embedding=bertify.encode([posed_relation])[0]\n\n# score each sentence in the results based on the question\/posed-relation\nscores=[cosine_similarity([relation_embedding, embedding])[0][1] for embedding in triple_results['embedding_bert'].values]\ntriple_results['score']=scores\n\n# circle pack all the sentences belonging to a particular animal\ndata=[]\nfor entity in set(triple_results['entity'].values):\n    entity_sentences=triple_results[triple_results['entity']==entity]\n    temp_data={}\n    temp_data['id']=entity\n    temp_data['datum']=1\n    temp_data['children']=[]\n    for indx, row in entity_sentences.iterrows():\n        temp_data['children'].append({'id':indx, 'datum':1})\n    data.append(temp_data)\n\ncircles=circlify.circlify(data)\nfor circle in circles:\n    if circle.level==2:\n        triple_results.loc[circle.ex.get('id',None), 'x_temp']=circle.x\n        triple_results.loc[circle.ex.get('id',None), 'y_temp']=circle.y\n\n# create a bounding circle with the color of the animal from previous graph to encapsulate all the sentences for that animal\nshapes=[]\nbuffer=.02\nfor entity in set(triple_results['entity'].values):\n    entity_sentences=triple_results[triple_results['entity']==entity]\n    shapes.append(dict(\n            type=\"circle\",\n            xref=\"x\",\n            yref=\"y\",\n            x0=min(entity_sentences['x_temp'].values)-buffer,\n            y0=min(entity_sentences['y_temp'].values)-buffer,\n            x1=max(entity_sentences['x_temp'].values)+buffer,\n            y1=max(entity_sentences['y_temp'].values)+buffer,\n            line_color=entity_color_lookup[entity],\n        ))\n\n# add all to plotly scatter plot and plot away\nfig.add_trace(go.Scatter(x=triple_results['x_temp'].values, y=triple_results['y_temp'].values,\n                             mode='markers',\n                             marker_color=triple_results['score'].values,\n                             hoverinfo='text',\n                            hovertext=['entity='+entity+'\\n'+'sentence='+sentence for entity,sentence in  zip(triple_results['entity'].values, triple_results['sentence'].values)],\n                    marker=dict(\n                        colorscale=\"Gray\",\n                        reversescale=True,\n                        showscale=True\n                        )))\nfig.update_layout(\n    shapes=shapes\n)\n\n    \n\niplot(fig)\n","8cea4095":"triple_results_=triple_results.loc[:, ['score', 'entity']]\nprint('\\n\\nRanking by averaging:')\ndisplay(triple_results_.groupby('entity').mean().sort_values(by=['score'], ascending=False))\nprint('\\n\\nRanking by taking max:')\ndisplay(triple_results_.groupby('entity').max().sort_values(by=['score'], ascending=False))","c7dcb4ce":"print('\\n\\nClosest results for relation:', posed_relation)\ntriple_results_answers=triple_results.sort_values(by=['score'], ascending=False).head(5)[['entity','sentence']]\n\nprint('\\n\\nIn general:\\n')\ndisplay(triple_results_answers)\n\nprint('\\n\\nPer entity basis:\\n')\ntriple_results.sort_values(by=['entity','score'], ascending=False, inplace=True)\nfor entity in set(triple_results['entity'].values):\n    triple_results_answers=triple_results[triple_results['entity']==entity].head(5)[['sentence']]\n    print('\\n\\nentity:',entity)                                                                                \n    display(triple_results_answers)\n    \n","05934be0":"In this notebook we will,\n1. Try to find relationships between relevant entities (animals and coronavirus, in this case) using co-occurance of keywords. \n    Note: Keywords can co-occur at different levels viz. document, paragraph and sentence. \n2. We will further dive down into the results using setence level __Bert Encodings__ and __Plotly Vizualisations__.\n","b3af5f8b":"The above cluster reresents all kinds of relations between CoronaVirus and Animals. What if we are only interested in certain relations!\n* Now, we will create a **Heat Map of the sentences**, based on their closeness to a **'posed relation'** which can be asumed **like a question**.\n* We will use a **Bubble Packing vizualisation** to show a heat map of sentences for each Animal.\n\n(Note: the circle packing algorithm takes some time :P)","fb18d9a5":"Below code reads the dataframe, where each row is information about a document.   \n1. I split the text from each row\/document into paragraphs.  \n2. Each paragraph is then cleaned (which involves only keeping Nouns).  \n3. The clean paragraph text is then added to another dataframe called \"df_para\", where each row will contain information about a paragraph (will also contain the document id).  \n\nNo need to run the below code, as it takes quite a bit of time :)","490fe259":"![Screen%20Shot%202020-04-08%20at%204.03.44%20PM.png](attachment:Screen%20Shot%202020-04-08%20at%204.03.44%20PM.png)There is a **cluster in the far left**, where it talks about coronavirus and **bats**, and hovering over them and looking at the sentences, it seems like they are clustered together because they all talk about **CoronaVirus**, that was found in bats In **China**.\n\n(Note: the plot may change every run, so I have a screenshot here)","283ad7ce":"Let us find relationships between CoronaVirus and some common animals:  \nTo do this we **find the paragraphs and documents, where these entities co-occur** and **rank** the entities based on their co-occurance.","482e00a5":"Further, to better understand the relationship of CoronaVirus with the top contenders, we will go at the sentence level. We will find all the sentences that contains both a mention of CoronaVirus and a mention of an Animal. Now, we will:\n1. Use our index to quickly **find all those paragraphs** first.\n2. Use Stanford's **Open IE to create simpler sentences** (from the compound and complex sentences) of that paragraph.\n3. **Filter out all sentences that do not have both CoronaVirus and an Animal mention** in them.\n4. **Convert the mentions of aliases of CoronaVirus with 'CoronaVirus'** and **an animal name (say 'bat') with the 'Animal'** keywords. This will be useful for clustering without too much bias on the extact name of the entity.\n5. **Create Bert embeddings** of each of the sentences and save them.\n\n***(Note: things will definitely be faster if you run Open IE beforehand on all paragraphs)***","847340b0":"Non visually speaking, we can rank the animal now based on the relation\/quiestion we posed:","ffdac4ea":"We create an index\/mapping, which will give us the paragraph id for any given keyword ","7cff7397":"Let us first install all the required libraries ","b734bd9a":"I start with a dataframe that contains text from the documents made available as part of the Kaggle COVID-19 challange (I have made this dataframe available here : https:\/\/www.kaggle.com\/asitang\/data-covid). I create an index\/lookup where that can give me all the documents and their paragraphs in which that keyword is present.","7bf2cbeb":"let us find all the coronavirus references in the text.\nTodo this, we will find all keywords that have 'covid' or 'coronavirus' in them. \nWe will store the entities in a dictionary.","bbb7b72d":"* We will now cluster (**t-SNE** custering algorithm) the sentences (using their **Bert embeddings**). So, **sentences** that are trying to convey **similar ideas should be clustered together**. \n* We will also color code each sentence based on the animal it is talking about.\n* Now, if we see a **distinct cluster that has the same color**, this means that cluster represent a **uniqe relation** of that animal with CoronaVirus!","20738f66":"Some utility functions to help me process data (ignore please :) )","6ea25339":"Let us see what are the best answers\/sentences to the posed relation:","0a92b71e":"Similar to above, we create another dictionary that contains various animals and their aliases. I use a list of animal names gotten from here: https:\/\/gist.github.com\/atduskgreg\/3cf8ef48cb0d29cf151bedad81553a54 "}}