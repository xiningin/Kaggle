{"cell_type":{"b0c588d5":"code","23d5dd70":"code","c41e6f45":"code","7ffb106a":"code","3f976691":"code","bcaf882c":"code","e22536ad":"code","e6ebd3f2":"code","97742710":"code","f698ffa9":"code","34a24895":"code","9e39577a":"code","8fc09af9":"code","e4684bfe":"code","85a95dd1":"code","4545d895":"code","ec3bf5ee":"code","a8098d3c":"code","3ba9759b":"code","5b66f48a":"code","bc42e79d":"code","888b099c":"code","01e9a374":"code","9bb295fa":"code","9dd94e3f":"code","e4d45e14":"code","34a57f53":"code","6167672e":"code","87d215b8":"code","36d76284":"code","17b12fe6":"code","8e26a32f":"code","9719cfcb":"code","942b8250":"code","fbe9d5f8":"code","4143c3b2":"code","869263e1":"code","d574b955":"code","ae6ca79f":"code","ab5715da":"code","143c34a6":"markdown","ba0e568e":"markdown","83779c4c":"markdown","ce52c220":"markdown","a70539cc":"markdown","25ae3b2f":"markdown","1f0c7247":"markdown","cf0dd815":"markdown","c68bb90f":"markdown","565669f8":"markdown","c499fd21":"markdown","293427ad":"markdown","769be613":"markdown","a1328874":"markdown","6d149a16":"markdown","94a642bf":"markdown","eb90d99f":"markdown","ae1d488b":"markdown","9046097b":"markdown","87c7aab0":"markdown","16ae9427":"markdown","49a2fb60":"markdown","dace6525":"markdown","daddcbdf":"markdown","3b7d86c2":"markdown"},"source":{"b0c588d5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","23d5dd70":"import matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import tree\nimport os\nimport preprocessing \n\nfrom pandas_profiling import ProfileReport\n\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2, f_classif\nfrom sklearn.model_selection import KFold\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.svm import LinearSVC\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nfrom sklearn.preprocessing import normalize\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder, OneHotEncoder\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.cluster import KMeans\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom xgboost import XGBClassifier\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.linear_model import SGDClassifier, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\nfrom xgboost import XGBClassifier, XGBRFClassifier\nfrom xgboost import plot_tree, plot_importance\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score, roc_auc_score, roc_curve\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.feature_selection import RFE\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","c41e6f45":"\ndf = pd.read_csv('..\/input\/heart-disease-uci\/heart.csv')","7ffb106a":"#Print the first 5 rows\ndf.head()","3f976691":"#Print the numeric information about the data\ndf.describe()","bcaf882c":"# How many rows the data has\nprint(\"Rows:\", len(df))","e22536ad":"# How many columns the data has\nprint(\"Columns:\", df.shape[1])","e6ebd3f2":"import pandas_profiling as pandas_pf\npandas_pf.ProfileReport(df)","97742710":"# missing values\ndf.isnull().sum()","f698ffa9":"features = ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach', 'exang', 'oldpeak', 'slope', 'ca', 'thal','target']\nsns.set_style('darkgrid')","34a24895":"plt.hist(df['age'])\nplt.xlabel('Age')\nplt.ylabel('Count')\nplt.title('Age distribution.')","9e39577a":"sns.histplot(data=df, x=\"age\", hue=\"sex\")","8fc09af9":"#Lets investigate age values\nminAge=min(df.age)\nmaxAge=max(df.age)\nmeanAge=df.age.mean()\nprint('Min Age :',minAge)\nprint('Max Age :',maxAge)\nprint('Mean Age :',meanAge)","e4684bfe":"plt.figure(figsize=(30,20))\n\nplt.subplot(3,3,1)\nsns.countplot(df['sex'])\n\nplt.subplot(3,3,2)\nsns.countplot(df['cp'])\n\nplt.subplot(3,3,3)\nsns.countplot(df['fbs'])\n\nplt.subplot(3,3,4)\nsns.countplot(df['restecg'])\n\nplt.subplot(3,3,5)\nsns.countplot(df['exang'])\n\nplt.subplot(3,3,6)\nsns.countplot(df['slope'])\n\nplt.subplot(3,3,7)\nsns.countplot(df['ca'])\n\nplt.subplot(3,3,8)\nsns.countplot(df['thal'])\n\nplt.subplot(3,3,9)\nsns.countplot(df['target'])\n\nplt.show()\n","85a95dd1":"plt.figure(figsize=(20,10))\nsns.countplot(df['age'])\nplt.xticks(rotation = 90)\nplt.show()","4545d895":"\nplt.scatter(x=df.age[df.target==1], y=df.thalach[(df.target==1)], c=\"red\")\nplt.scatter(x=df.age[df.target==0], y=df.thalach[(df.target==0)], c=\"green\")\nplt.legend([\"Disease\", \"Not Disease\"])\nplt.xlabel(\"Age\")\nplt.ylabel(\"Maximum Heart Rate\")\nplt.show()","ec3bf5ee":"plt.scatter(x=df.age[df.target==1], y=df.trestbps[(df.target==1)], c=\"red\")\nplt.scatter(x=df.age[df.target==0], y=df.trestbps[(df.target==0)], c=\"green\")\nplt.legend([\"Disease\", \"Not Disease\"])\nplt.xlabel(\"Age\")\nplt.ylabel(\"Resting Blood Pressure\")\nplt.show()","a8098d3c":"plt.scatter(x=df.age[df.target==1], y=df.chol[(df.target==1)], c=\"red\")\nplt.scatter(x=df.age[df.target==0], y=df.chol[(df.target==0)], c=\"green\")\nplt.legend([\"Disease\", \"Not Disease\"])\nplt.xlabel(\"Age\")\nplt.ylabel(\"Serum cholestoral in mg\/dl\")\nplt.show()","3ba9759b":"plt.figure(figsize=(20,15))\n\nplt.subplot(2,3,1)\nsns.distplot(df['age']).set_title('Age Interval')\n\nplt.subplot(2,3,2)\nsns.distplot(df['chol']).set_title('Chol Interval')\n\nplt.subplot(2,3,3)\nsns.distplot(df['thalach']).set_title('Thalach Interval')\n\nplt.subplot(2,3,4)\nsns.distplot(df['oldpeak']).set_title('Oldpeak Interval')\n\nplt.subplot(2,3,5)\nsns.distplot(df['trestbps']).set_title('Resting Blood Pressure Interval')\n","5b66f48a":"plt.figure(figsize=(30,20))\n\nplt.subplot(4,4,1)\nsns.barplot(x = 'target', y = 'sex', data = df)\n\nplt.subplot(4,4,2)\nsns.barplot(x = 'target', y = 'trestbps', data = df)\n\nplt.subplot(4,4,3)\nsns.barplot(x = 'target', y = 'chol', data = df)\n\nplt.subplot(4,4,4)\nsns.barplot(x = 'target', y = 'thalach', data = df)\n\nplt.subplot(4,4,5)\nsns.barplot(x = 'target', y = 'exang', data = df)\n\nplt.subplot(4,4,6)\nsns.barplot(x = 'target', y = 'oldpeak', data = df)\n\nplt.subplot(4,4,7)\nsns.barplot(x = 'target', y = 'slope', data = df)\n\nplt.subplot(4,4,8)\nsns.barplot(x = 'target', y = 'age', data = df)","bc42e79d":"plt.figure(figsize=(20,10))\nsns.barplot(x=\"trestbps\", y=\"chol\", data=df)\nplt.xticks(rotation=90)\nplt.show()","888b099c":"plt.figure(figsize=(20,10))\nsns.barplot(x=\"age\", y=\"thalach\", data=df, hue=\"target\")\nplt.xticks(rotation=90)\nplt.show()","01e9a374":"sns.lmplot(x='age', y='chol', hue='target', data=df)","9bb295fa":"sns.pairplot(data=df, hue=\"target\")","9dd94e3f":"plt.figure(figsize=(12,8)) \nsns.heatmap(df.corr(), annot=True, cmap='gist_heat', linewidths = 2)\nplt.show()","e4d45e14":"y = df[\"target\"]\nX = df.drop('target',axis=1)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state = 0)","34a57f53":"scaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)","6167672e":"model_lgr = 'Logistic Regression'\nlr = LogisticRegression()\nmodel = lr.fit(X_train, y_train)\nlr_predict = lr.predict(X_test)\nlr_conf_matrix = confusion_matrix(y_test, lr_predict)\nlr_acc_score = accuracy_score(y_test, lr_predict)\nprint(\"confussion matrix\")\nprint(lr_conf_matrix)\nprint(\"-------------------------------------------\")\nprint(\"Accuracy of Logistic Regression:\",lr_acc_score*100,'\\n')\nprint(\"-------------------------------------------\")\nprint(classification_report(y_test,lr_predict))","87d215b8":"model_nb = 'Naive Bayes'\nnb = GaussianNB()\nnb.fit(X_train,y_train)\nnbpred = nb.predict(X_test)\nnb_conf_matrix = confusion_matrix(y_test, nbpred)\nnb_acc_score = accuracy_score(y_test, nbpred)\nprint(\"confussion matrix\")\nprint(nb_conf_matrix)\nprint(\"-------------------------------------------\")\nprint(\"Accuracy of Naive Bayes model:\",nb_acc_score*100,'\\n')\nprint(\"-------------------------------------------\")\nprint(classification_report(y_test,nbpred))","36d76284":"model_rfc = 'Random Forest Classfier'\nrf = RandomForestClassifier(n_estimators=20, random_state=12,max_depth=5)\nrf.fit(X_train,y_train)\nrf_predicted = rf.predict(X_test)\nrf_conf_matrix = confusion_matrix(y_test, rf_predicted)\nrf_acc_score = accuracy_score(y_test, rf_predicted)\nprint(\"confussion matrix\")\nprint(rf_conf_matrix)\nprint(\"-------------------------------------------\")\nprint(\"Accuracy of Random Forest:\",rf_acc_score*100,'\\n')\nprint(\"-------------------------------------------\")\nprint(classification_report(y_test,rf_predicted))","17b12fe6":"model_egb = 'Extreme Gradient Boost'\nxgb = XGBClassifier(learning_rate=0.01, n_estimators=25, max_depth=15,gamma=0.6, subsample=0.52,colsample_bytree=0.6,seed=27, \n                    reg_lambda=2, booster='dart', colsample_bylevel=0.6, colsample_bynode=0.5)\nxgb.fit(X_train, y_train)\nxgb_predicted = xgb.predict(X_test)\nxgb_conf_matrix = confusion_matrix(y_test, xgb_predicted)\nxgb_acc_score = accuracy_score(y_test, xgb_predicted)\nprint(\"confussion matrix\")\nprint(xgb_conf_matrix)\nprint(\"-------------------------------------------\")\nprint(\"Accuracy of Extreme Gradient Boost:\",xgb_acc_score*100,'\\n')\nprint(\"-------------------------------------------\")\nprint(classification_report(y_test,xgb_predicted))","8e26a32f":"model_knn = 'K-NeighborsClassifier'\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(X_train, y_train)\nknn_predicted = knn.predict(X_test)\nknn_conf_matrix = confusion_matrix(y_test, knn_predicted)\nknn_acc_score = accuracy_score(y_test, knn_predicted)\nprint(\"confussion matrix\")\nprint(knn_conf_matrix)\nprint(\"-------------------------------------------\")\nprint(\"Accuracy of K-NeighborsClassifier:\",knn_acc_score*100,'\\n')\nprint(\"-------------------------------------------\")\nprint(classification_report(y_test,knn_predicted))","9719cfcb":"model_dtc = 'DecisionTreeClassifier'\ndt = DecisionTreeClassifier(criterion = 'entropy',random_state=0,max_depth = 6)\ndt.fit(X_train, y_train)\ndt_predicted = dt.predict(X_test)\ndt_conf_matrix = confusion_matrix(y_test, dt_predicted)\ndt_acc_score = accuracy_score(y_test, dt_predicted)\nprint(\"confussion matrix\")\nprint(dt_conf_matrix)\nprint(\"-------------------------------------------\")\nprint(\"Accuracy of DecisionTreeClassifier:\",dt_acc_score*100,'\\n')\nprint(\"-------------------------------------------\")\nprint(classification_report(y_test,dt_predicted))","942b8250":"model_svc = 'Support Vector Classifier'\nsvc =  SVC(kernel='rbf', C=2)\nsvc.fit(X_train, y_train)\nsvc_predicted = svc.predict(X_test)\nsvc_conf_matrix = confusion_matrix(y_test, svc_predicted)\nsvc_acc_score = accuracy_score(y_test, svc_predicted)\nprint(\"confussion matrix\")\nprint(svc_conf_matrix)\nprint(\"-------------------------------------------\")\nprint(\"Accuracy of Support Vector Classifier:\",svc_acc_score*100,'\\n')\nprint(\"-------------------------------------------\")\nprint(classification_report(y_test,svc_predicted))","fbe9d5f8":"model_sgd = 'Stochastic Gradient Descent'\nsgdc = SGDClassifier(max_iter=5000, random_state=0)\nsgdc.fit(X_train, y_train)\nsgdc_predicted = sgdc.predict(X_test)\nsgdc_conf_matrix = confusion_matrix(y_test, sgdc_predicted)\nsgdc_acc_score = accuracy_score(y_test, sgdc_predicted)\nprint(\"confussion matrix\")\nprint(sgdc_conf_matrix)\nprint(\"-------------------------------------------\")\nprint(\"Accuracy of : Stochastic Gradient Descent\",sgdc_acc_score*100,'\\n')\nprint(\"-------------------------------------------\")\nprint(classification_report(y_test,sgdc_predicted))","4143c3b2":"model_nn = 'Neural Nets'\nmlpc = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(5000, 10), random_state=1)\nmlpc.fit(X_train, y_train)\nmlpc_predicted = mlpc.predict(X_test)\nmlpc_conf_matrix = confusion_matrix(y_test, mlpc_predicted)\nmlpc_acc_score = accuracy_score(y_test, mlpc_predicted)\nprint(\"confussion matrix\")\nprint(mlpc_conf_matrix)\nprint(\"-------------------------------------------\")\nprint(\"Accuracy of : MLP Classifier\",mlpc_acc_score*100,'\\n')\nprint(\"-------------------------------------------\")\nprint(classification_report(y_test,mlpc_predicted))","869263e1":"plt.figure(figsize=(30,20))\n\nplt.subplot(3,3,1)\nplt.title('Heart Disease --- Model: Logistic Regression --- Accuracy:{x:.5f}'.format(x=lr_acc_score))\nsns.heatmap(lr_conf_matrix, annot=True, cmap=\"Blues\")\n\nplt.subplot(3,3,2)\nplt.title('Heart Disease --- Model: Naive Bayes --- Accuracy:{x:.5f}'.format(x=nb_acc_score))\nsns.heatmap(nb_conf_matrix, annot=True, cmap=\"Blues\")\n\nplt.subplot(3,3,3)\nplt.title('Heart Disease --- Model: Random Forest --- Accuracy:{x:.5f}'.format(x=rf_acc_score) )\nsns.heatmap(rf_conf_matrix, annot=True, cmap=\"Blues\")\n\nplt.subplot(3,3,4)\nplt.title('Heart Disease --- Model: Extreme Gradient Boost --- Accuracy:{x:.5f}'.format(x=xgb_acc_score))\nsns.heatmap(xgb_conf_matrix, annot=True, cmap=\"Blues\")\n\nplt.subplot(3,3,5)\nplt.title('Heart Disease --- Model: K-Nearest Neighbour --- Accuracy:{x:.5f}'.format(x=knn_acc_score))\nsns.heatmap(knn_conf_matrix, annot=True, cmap=\"Blues\")\n\nplt.subplot(3,3,6)\nplt.title('Heart Disease --- Model: Decision Tree --- Accuracy:{x:.5f}'.format(x=nb_acc_score))\nsns.heatmap(nb_conf_matrix, annot=True, cmap=\"Blues\")\n\nplt.subplot(3,3,7)\nplt.title('Heart Disease --- Model: Support Vector Machine --- Accuracy:{x:.5f}'.format(x=svc_acc_score))\nsns.heatmap(svc_conf_matrix, annot=True, cmap=\"Blues\")\n\nplt.subplot(3,3,8)\nplt.title('Heart Disease --- Model: Stochastic Gradient Descent --- Accuracy:{x:.5f}'.format(x=sgdc_acc_score))\nsns.heatmap(sgdc_conf_matrix, annot=True, cmap=\"Blues\")\n\nplt.subplot(3,3,9)\nplt.title('Heart Disease --- Model: Neural Nets --- Accuracy:{x:.5f}'.format(x=mlpc_acc_score))\nsns.heatmap(mlpc_conf_matrix, annot=True, cmap=\"Blues\")\n\n\n\n","d574b955":"imp_feature = pd.DataFrame({'Feature': ['age', 'sex', 'cp', 'trestbps', 'chol', 'fbs', 'restecg', 'thalach',\n       'exang', 'oldpeak', 'slope', 'ca', 'thal'], 'Importance': xgb.feature_importances_})\nplt.figure(figsize=(10,4))\nplt.title(\"barplot Represent feature importance \")\nplt.xlabel(\"importance \")\nplt.ylabel(\"features\")\nplt.barh(imp_feature['Feature'],imp_feature['Importance'],color = 'rgbkymc')\nplt.show()","ae6ca79f":"model_ev = pd.DataFrame({'Model': ['Logistic Regression','Naive Bayes','Random Forest','Extreme Gradient Boost',\n                    'K-Nearest Neighbour','Decision Tree','Support Vector Machine', 'Stochastic Gradient Descent', 'Neural Nets'], 'Accuracy': [lr_acc_score*100,\n                    nb_acc_score*100,rf_acc_score*100,xgb_acc_score*100,knn_acc_score*100,dt_acc_score*100,svc_acc_score*100, sgdc_acc_score*100, mlpc_acc_score*100]})\nmodel_ev","ab5715da":"colors = ['red','green','blue','gold','silver','yellow','orange','magenta', 'cyan']\nplt.figure(figsize=(12,5))\nplt.title(\"barplot Represent Accuracy of different models\")\nplt.xlabel(\"Accuracy %\")\nplt.xticks(rotation=90)\nplt.ylabel(\"Algorithms\")\nplt.bar(model_ev['Model'],model_ev['Accuracy'],color = colors)\nplt.show()","143c34a6":"> Pandas profiling generates profile reports from a pandas DataFrame. The pandas df.describe() function is great but a little basic for serious exploratory data analysis. pandas_profiling extends the pandas DataFrame with df.profile_report() for quick data analysis.","ba0e568e":"**Column Descriptions**\n1. age\n2. sex\n3. chest pain type (4 values)\n4. resting blood pressure\n5. serum cholestoral in mg\/dl\n6. fasting blood sugar > 120 mg\/dl\n7. resting electrocardiographic results (values 0,1,2)\n8. maximum heart rate achieved\n9. exercise induced angina\n10. oldpeak = ST depression induced by exercise relative to rest\n11. the slope of the peak exercise ST segment\n12. number of major vessels (0-3) colored by flourosopy\n13. thal: 3 = normal; 6 = fixed defect; 7 = reversable defect\n14. target: 0 or 1 ","83779c4c":"> We haven't got any missing values in this set","ce52c220":"# Evaluation of Models","a70539cc":"# Conclusion","25ae3b2f":"# Importing the data","1f0c7247":"> In this notebook i will try to investigate and visualize heart-disease data and later on evaluate different machine learning models.\n> Feel free to comment on this notebook.\ud83d\udc4d\ud83d\udc4d","cf0dd815":"# Importing Libraries","c68bb90f":"> Bar plot representation of features about whether they have heart disease or not","565669f8":"> Age distribution by sex (0 = female, 1 = male)","c499fd21":"# Heart Disease Analysis & Classification","293427ad":"# Feature Importance","769be613":"# Machine Learning Models","a1328874":"# DistPlot","6d149a16":"# Confusion Matrix","94a642bf":"# Scatter Plots","eb90d99f":"# Heatmap","ae1d488b":"# Pandas Profiling","9046097b":"# BarPlots","87c7aab0":"# Pairplot","16ae9427":"# Seaborn CountPlots","49a2fb60":"![](https:\/\/creakyjoints.org\/wp-content\/uploads\/2019\/12\/1219_Heart_Inflammation_Logo-1024x671.jpg)","dace6525":"* Logistic Regression\n* Naive Bayes\n* Random Forest Classifier\n* Extreme Gradient Boost\n* K-Nearest Neighbour\n* Decision Tree\n* Support Vector Machine\n* Stochastic Gradient Descent\n* Neural Nets","daddcbdf":"> Extreme Gradient Boost gave the best accuracy on test with the : 90.163934","3b7d86c2":"# Data Preprocessing"}}