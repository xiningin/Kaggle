{"cell_type":{"774a04bc":"code","d7ab3ab1":"code","5c543a06":"code","f6c79e0d":"code","7fc5fe1c":"code","dda48012":"code","c632ba52":"code","7a8e6066":"code","2e280681":"code","8d74140f":"code","576779db":"code","86baea44":"code","7dec70e9":"code","a3c17a1e":"code","d36f80d7":"code","98012916":"code","99dc6923":"code","301110d3":"code","25e9cb96":"code","e32db208":"code","e383c242":"code","09b39452":"code","4caa7572":"code","4c779992":"code","175c58e7":"code","75c66dbe":"code","208c164e":"code","45077633":"code","052153d6":"code","d09e6c79":"code","2d31511a":"code","2a0665e5":"code","e8cad266":"code","a2c593a3":"code","9bf7241b":"code","140e0c1a":"code","ad61f8af":"code","d2882edf":"code","e66424dd":"code","239b2106":"code","4ddb14bf":"code","da6020fc":"code","366dfd0d":"code","128e4026":"code","e2b25d22":"code","fecb06a6":"code","6e25618b":"code","b1f64850":"code","f54cbb51":"code","cd8bfe13":"code","db4e9b15":"code","c33311d5":"code","5033679a":"code","f323089c":"code","03d88093":"code","dcd61cb1":"code","874cef9b":"markdown","934b3d1b":"markdown","92c18508":"markdown","9f2fcd1c":"markdown","056c6580":"markdown","f48e80b0":"markdown","74f36241":"markdown","731c97ff":"markdown","a844d8a1":"markdown","5c7b79b4":"markdown","a37d31bc":"markdown","e39fa0f2":"markdown","aca1aac0":"markdown","f1839d38":"markdown","55f81bc5":"markdown","0648a05e":"markdown","37b1a1bd":"markdown","a0d240ef":"markdown","1b85c6dd":"markdown","2772520a":"markdown","ca05d9be":"markdown","3bbb0f1e":"markdown","4a70f815":"markdown","2be7cbf7":"markdown","6afd9a7b":"markdown"},"source":{"774a04bc":"import math, time, random, datetime ","d7ab3ab1":"### Libraries for data manipulation \n\nimport pandas as pd\nimport numpy as np","5c543a06":"### Libraries for Data Visualization and to gain meaningful Data Insights\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns","f6c79e0d":"### I am very good at ignoring warnings ;)\n\nimport warnings\nwarnings.filterwarnings('ignore')","7fc5fe1c":"#### Lets import the train & test data to check all the features\/columns \n\ntrain = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\n\n#### Also, lets have a look into the submission format for the competition\n\ngender_submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","dda48012":"train.head()  # .head() function allow us to view the top 5 records of the dataset","c632ba52":"test.head()","7a8e6066":"### Lets check the shape of the dataset \n\nprint(train.shape) # .shape is an attribute \/ property of the dataset not a function.\nprint(test.shape) \n\n### Output will be in the format of (rows,columns) \/ \"(records,features)[In a so called sophisticated way]\"","2e280681":"### let us have a view on the submission format of the competition.\n\ngender_submission.head()","8d74140f":"#### So, the first thing you should always do is try to look for the amount of null values present in your dataset.\n\ntrain.isnull().sum()\n\n### .isnull() is used to know the null values in each column in the dataset\n### .sum() is used for the summation of all the null values in each column of the dataset","576779db":"# Lets plot the above data in a visualization form using visualization libraries that we have earlier imported.\n\nsns.heatmap(train.isnull(), yticklabels = False, cbar = False, cmap ='viridis') \n\n### 'sns' is the alias that we have used for the seaborn library( you can literally use your name for alias for any libraries you import, but lets go with the standard procedure) and heatmap is an inbuilt function of the library for Data Visualization purpose \n\n#### you can use this link 'https:\/\/seaborn.pydata.org\/' to know more about the library.","86baea44":"# Lets check how many people survived the Titanic Disaster \n\nsns.set_style('whitegrid')\nsns.countplot(x='Survived',data=train)\n\nprint(train.Survived.value_counts()) #.value_counts() is used to count the records in features\/columns","7dec70e9":"# Lets check the Pclass (Passenger Class) of the people who boarded the ship\n\nsns.set_style('whitegrid')\nsns.countplot(x='Pclass',data=train)\n\nprint(train.Pclass.value_counts())","a3c17a1e":"# Lets check the Survived feature with respect to the Pclass to find any realtion between the two\n\nsns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Pclass',data=train,palette='rainbow')","d36f80d7":"# Now let us have a look in the number of male and female who boarded the ship.\n\nsns.set_style('darkgrid')\nsns.countplot(x='Sex',data=train)\n\nprint(train.Sex.value_counts())","98012916":"# Lets check the Survived feature with respect to the Sex feature to find any realtion between the two\n\nsns.set_style('whitegrid')\nsns.countplot(x='Survived',hue='Sex',data=train,palette='rainbow')","99dc6923":"# Lets check the Age feature \npd.set_option('display.max_rows',88) # pandas function to display all rows\ntrain.Age.value_counts()","301110d3":"# Also lets have a look on the null values present\ntrain.Age.isnull().sum()","25e9cb96":"# Lets check the distribution of the Age Data by plotting a histogram \n\nsns.set_style('darkgrid')\nsns.histplot(train['Age'].dropna(),bins=40,color='blue',kde=True)\n","e32db208":"# You can also do the same with the matplotlib library\n\ntrain['Age'].hist(bins=40,color='darkred',alpha=0.85)","e383c242":"# Lets know our data in dataset in a statistical way and terms\n\ntrain.describe()","09b39452":"# Also lets have a look in their datatypes that will help us later in data cleaning\/feature engineering\n\ntrain.info()","4caa7572":"# Lets check if there is any relation of Pclass with respect to Age feature so that we can derive a condition to replace the missing values in the Age data records\nsns.set_style('whitegrid')\nsns.boxplot(x='Pclass',y='Age',data=train,palette='rainbow')","4c779992":"# Lets check the 'Sibsp' feature i.e (Siblings+spouse)\n\nsns.set_style('whitegrid')\nsns.countplot(y='SibSp',data=train)\n\nprint(train.SibSp.value_counts())","175c58e7":"# Lets have a look on Parch (The number of parents\/children the passenger boarded the ship) feature\n\nsns.set_style('whitegrid')\nsns.countplot(x='Parch',data=train,palette='rainbow')\n\nprint(train.Parch.value_counts())","75c66dbe":"#  lets check the ticket column to gain insights\n\ntrain.Ticket.value_counts()","208c164e":"train.Ticket.unique()","45077633":"# Lets have a look in the Fare feature\n\nprint(len(train.Fare.unique()))\nprint(train.Fare.isnull().sum())","052153d6":"# Lets now check the Cabin Feature for the datset\n\ntrain.Cabin.isnull().sum()","d09e6c79":"# Lets check the last column in our dataset i.e. Embarked (meaning from which place passengers boarded the ship)\n\nprint(train.Embarked.value_counts())\n\nsns.countplot(y='Embarked', data=train)","2d31511a":"train['Age'] = train['Age'].fillna(train['Age'].median())","2a0665e5":"sns.heatmap(train.isnull(), yticklabels= False, cbar= False, cmap= 'coolwarm')","e8cad266":"train = train.drop('Cabin', axis=1)","a2c593a3":"train.head()","9bf7241b":"train.isnull().sum()","140e0c1a":"# If you want you can drop the 2 records from the Embarked column, but instead of dropping I will replace it with most frequent occuring category.\n\ntrain['Embarked']= train['Embarked'].fillna(train['Embarked'].value_counts().index[0])","ad61f8af":"train.isnull().sum()","d2882edf":"features= [ 'Pclass','Sex','Age','SibSp','Parch','Fare','Embarked'] # I have picked the independent features inside a list\n\n##Let's devide the dataset\n\nX = train[features]\ny = train['Survived']","e66424dd":"X.isnull().sum() ### Double check to make sure there is no null values in your training dataset","239b2106":"# Now let's enocde categorical values (Feature Engineering Techniques) \n\nfrom sklearn.preprocessing import LabelEncoder\nLE = LabelEncoder()\nX['Sex'] = LE.fit_transform(X['Sex'])\nX['Embarked'] = LE.fit_transform(X['Embarked'])\n","4ddb14bf":"from sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test = train_test_split(X,y,test_size = 0.3,random_state =0)","da6020fc":"%%time\nfrom xgboost import XGBClassifier\nclassifier = XGBClassifier(max_depth=12,\n                        subsample=0.33,\n                        objective='binary:logistic',\n                        n_estimators=420,\n                        learning_rate = 0.01)\neval_set = [(X_train,y_train), (X_test,y_test)]\nclassifier.fit(X_train, y_train.values.ravel(), early_stopping_rounds=12, eval_metric=[\"error\", \"logloss\"], eval_set=eval_set, verbose=True)\nclassifier.score(X_test,y_test)","366dfd0d":"classifier.score(X_test,y_test)","128e4026":"test = pd.read_csv('..\/input\/titanic\/test.csv')\ntest.head()","e2b25d22":"# Check the null values for the test data\ntest.isnull().sum()","fecb06a6":"#Imputing the null values in the test dataset\n\ntest['Age'] = test['Age'].fillna(test['Age'].median())\ntest['Fare'] = test['Fare'].fillna(test['Fare'].median())","6e25618b":"test.isnull().sum()","b1f64850":"test_data = test[features] # This helps us to drop the Cabin column instead of writing a code as we have already describe features as a list in our notebook, containing only the independent features.\ntest_data.head()","f54cbb51":"test_data['Sex'] = LE.fit_transform(test_data['Sex'])\ntest_data['Embarked'] = LE.fit_transform(test_data['Embarked'])","cd8bfe13":"test_data.head()","db4e9b15":"prediction = classifier.predict(test_data)","c33311d5":"prediction","5033679a":"predictions = pd.DataFrame(prediction) # storing the prediction in a pandas DataFrame","f323089c":"gender_submission = pd.read_csv('..\/input\/titanic\/gender_submission.csv')","03d88093":"datasets = pd.concat(([gender_submission['PassengerId'],predictions]),axis=1)","dcd61cb1":"# creating the submission file\ndatasets.columns=['PassengerId','Survived']\ndatasets.to_csv('gender_submission.csv',index=False) ","874cef9b":"### We have cleaned our dataset, now we will use Data Preprocessing techniques or Feature Engineering to deal with Categorical Variables.","934b3d1b":"#### we can come to the conclusion that 'Among the Survived people, max. are from Pclass-1' and 'Among the people who had not survived, max. are from Pclass-3'","92c18508":"#### we can cleary see that among the Age data available to us, max. passengers who boarded the ship is between (20-40) years age bracket.","9f2fcd1c":"#### We can say that max. no. of passengers didn't have parents\/children aboarded with them.","056c6580":"#### We can cleary say that max. no. of passengers are from Pclass-3, followed by Pclass-1 and Pclass-2","f48e80b0":"### Fitting a ML model, to be more specific I will be using XGBoost","74f36241":"#### So, there are 177 records where the Age data is missing","731c97ff":"#### we can say that max. no. of passengers didn't have a siblings and spouse","a844d8a1":"## Finally, we reached the moment of glory! we will now predict the values in our test data","5c7b79b4":"#### This means we need to format our predictions with respect to 'PassengerId'(unique to every person boarded the ship) and 'Survived' columns(which is the dependent feature)","a37d31bc":"### Lets import the test dataset for the competition","e39fa0f2":"## Lets explore the data for Data Insights or in simple terms Exploratory Data Analyis (E.D.A.)\n\n### We will use visualization libraries to gain meaningful insights","aca1aac0":"### Lets import the necessary libraries","f1839d38":"### Now we will use Encoding technique to encode categorical variables in column 'Sex' & 'Embarked'","55f81bc5":"#### We can clearly see that 342 people survived the disaster and 549 people not able to survived.","0648a05e":"### So, we are done with our train data, now we will split our data and build our ML model to train the dataset","37b1a1bd":"#### So, there are 687 missing records in the Cabin feature, so during model building we will drop the column since max. values are missing and there is no way we can impute or find relation or know about these missing values","a0d240ef":"#### One important thing to notice from 'Train' & 'Test' dataset is that 'Survived' is the dependent feature that we need to predict and the rest of the features are independent.","1b85c6dd":"### Lets start with these libraries for EDA and later we will import libraries required for model building :)","2772520a":"#### There are 248 unique fare recorded in the dataset ","ca05d9be":"## Now, we will split the dataset into training and validation format (For better understanding of what I'm saying go through the \"Intro to Machine Learning\" course from kaggle, its free and very good to get the foundation right :)","3bbb0f1e":"#### List of all the unique tickets","4a70f815":"#### We can say that max. number of passengers are male","2be7cbf7":"### One more important thing that I have forget to mention is that go through the kaggle course if you are a beginner.\n\n### Also Just sign in 'simplilearn.com' for \"Data Science with Python course\" its free for first 90 days, Its really good.\n\n### If you like the content in the notebook do UPVOTE, as I am also a beginner and is making a Career Transition.\n\n### IF YOU WANT TO LEARN TOGETHER, YOU CAN ALSO CONNECT ME ON LINKEDIN, YOU CAN FIND MY ID IN MY KAGGLE PROFILE.","6afd9a7b":"#### We can clearly conclude that among people who survived the disaster, max. were female gender"}}