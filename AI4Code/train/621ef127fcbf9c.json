{"cell_type":{"f49daaf4":"code","6dd4f57b":"code","423ced6a":"code","c1702f0d":"code","7a2c777a":"code","be52a748":"code","92ed6602":"code","44466244":"code","69234a00":"code","e3de6fde":"markdown"},"source":{"f49daaf4":"import os\nimport torch\nimport pandas as pd\nfrom scipy import stats\nimport numpy as np\n\nfrom tqdm import tqdm\nimport torch.nn as nn\nimport joblib\n\nimport transformers\nfrom transformers import AdamW, get_linear_schedule_with_warmup\nimport sys","6dd4f57b":"class BERTBaseUncased(nn.Module):\n    def __init__(self, bert_path):\n        super(BERTBaseUncased, self).__init__()\n        self.bert_path = bert_path\n        self.bert = transformers.BertModel.from_pretrained(self.bert_path)\n        self.bert_drop = nn.Dropout(0.3)\n        self.out = nn.Linear(768, 30)\n\n    def forward(\n            self,\n            ids,\n            mask,\n            token_type_ids\n    ):\n        _, o2 = self.bert(\n            ids,\n            attention_mask=mask,\n            token_type_ids=token_type_ids)\n\n        bo = self.bert_drop(o2)\n        p2 = self.out(bo)\n        return p2","423ced6a":"class BERTDatasetTest:\n    def __init__(self, qtitle, qbody, answer, tokenizer, max_length):\n        self.qtitle = qtitle\n        self.qbody = qbody\n        self.answer = answer\n        self.tokenizer = tokenizer\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.answer)\n\n    def __getitem__(self, item):\n        question_title = str(self.qtitle[item])\n        question_body = str(self.qbody[item])\n        answer_text = str(self.answer[item])\n\n        question_title = \" \".join(question_title.split())\n        question_body = \" \".join(question_body.split())\n        answer_text = \" \".join(answer_text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            question_title + \" \" + question_body,\n            answer_text,\n            add_special_tokens=True,\n            max_length=self.max_length,\n        )\n        ids = inputs[\"input_ids\"]\n        token_type_ids = inputs[\"token_type_ids\"]\n        mask = inputs[\"attention_mask\"]\n        \n        padding_length = self.max_length - len(ids)\n        \n        ids = ids + ([0] * padding_length)\n        mask = mask + ([0] * padding_length)\n        token_type_ids = token_type_ids + ([0] * padding_length)\n        \n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long)\n        }","c1702f0d":"def predict():\n    DEVICE = torch.device(\"cuda\")\n    TEST_BATCH_SIZE = 8\n    TEST_DATASET = \"..\/input\/google-quest-challenge\/test.csv\"\n    df = pd.read_csv(TEST_DATASET).fillna(\"none\")\n\n    qtitle = df.question_title.values.astype(str).tolist()\n    qbody = df.question_body.values.astype(str).tolist()\n    answer = df.answer.values.astype(str).tolist()\n    category = df.category.values.astype(str).tolist()\n\n    tokenizer = transformers.BertTokenizer.from_pretrained(\"..\/input\/bert-base-uncased\/\", \n                                                           do_lower_case=True)\n    maxlen = 512\n    predictions = []\n\n    test_dataset = BERTDatasetTest(\n        qtitle=qtitle,\n        qbody=qbody,\n        answer=answer,\n        tokenizer=tokenizer,\n        max_length=maxlen\n    )\n    test_data_loader = torch.utils.data.DataLoader(\n        test_dataset,\n        batch_size=TEST_BATCH_SIZE,\n        shuffle=False,\n        num_workers=4\n    )\n\n    model = BERTBaseUncased(\"..\/input\/bert-base-uncased\/\")\n    model.to(DEVICE)\n    model.load_state_dict(torch.load(\"..\/input\/tpubert\/model.bin\"))\n    model.eval()\n\n    tk0 = tqdm(test_data_loader, total=int(len(test_dataset) \/ test_data_loader.batch_size))\n    for bi, d in enumerate(tk0):\n        ids = d[\"ids\"]\n        mask = d[\"mask\"]\n        token_type_ids = d[\"token_type_ids\"]\n\n        ids = ids.to(DEVICE, dtype=torch.long)\n        mask = mask.to(DEVICE, dtype=torch.long)\n        token_type_ids = token_type_ids.to(DEVICE, dtype=torch.long)\n        \n        with torch.no_grad():\n            outputs = model(\n                ids=ids,\n                mask=mask,\n                token_type_ids=token_type_ids\n            )\n            outputs = torch.sigmoid(outputs).cpu().numpy()\n            predictions.append(outputs)\n\n    return np.vstack(predictions)","7a2c777a":"preds = predict()","be52a748":"SAMPLE_SUBMISSION = \"..\/input\/google-quest-challenge\/sample_submission.csv\"\nsample = pd.read_csv(SAMPLE_SUBMISSION)\ntarget_cols = list(sample.drop(\"qa_id\", axis=1).columns)","92ed6602":"sample[target_cols] = preds","44466244":"sample.head()","69234a00":"sample.to_csv(\"submission.csv\", index=False)","e3de6fde":"If you like this kernel, consider upvoting it and the associated datasets:\n- https:\/\/www.kaggle.com\/abhishek\/bert-base-uncased\n- https:\/\/www.kaggle.com\/abhishek\/tpubert"}}