{"cell_type":{"14328877":"code","1f97f48c":"code","ea76f041":"code","001c9dc8":"code","69079b63":"code","f7c9a2a4":"code","b0d22483":"code","f4e9feec":"code","5da9cad1":"code","843a6c76":"code","ac1aef02":"code","c4e86803":"code","d20c1622":"code","a053cb55":"code","c95aa549":"code","f61caad6":"code","9de54ebb":"code","77387851":"code","7baecb61":"code","34a21bc4":"code","07d1634c":"code","cf605621":"markdown","7e7e6e5f":"markdown","bee15066":"markdown","b8db0a5b":"markdown","dd83cdc7":"markdown","25b160b6":"markdown"},"source":{"14328877":"!pip install catboost","1f97f48c":"import numpy as np\nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom tqdm import tqdm\nfrom sklearn.experimental import enable_hist_gradient_boosting \nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.ensemble import HistGradientBoostingRegressor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom catboost import CatBoostRegressor\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ea76f041":"df_train = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/test.csv\")\ndf_sub = pd.read_csv(\"..\/input\/tabular-playground-series-aug-2021\/sample_submission.csv\")","001c9dc8":"print(df_train.shape)\ndf_train.info(verbose=True, null_counts=True)","69079b63":"print(df_test.shape)\ndf_test.info(verbose=True, null_counts=True)","f7c9a2a4":"# Keeping a separator variable and the target variable\nsep = df_train.shape[0]\nY = df_train[\"loss\"]\n\n# Dropping the IDs and the target variable\ndf_train.drop([\"id\", \"loss\"], axis=1, inplace=True)\ndf_test.drop([\"id\"], axis=1, inplace=True)\n\n# Concatenating the datasets for pre-processing\ndf = pd.concat([df_train, df_test], axis=0)\n\nprint(df.shape, Y.shape, sep)","b0d22483":"# Plotting the Distribution of 'Loss'\nplt.hist(Y, 50, density=True, facecolor='g')\nplt.title('Distribution of Loss')\nplt.grid(True)\nplt.show()","f4e9feec":"# We are trying to find PCC (Pearson Correlation Coefficient) between features\n# So that, we can eliminate some of the redundant features. But for plotting the\n# correlation matrix, we will use the training set only.\n\n# Getting the train set\ndf_train = df.iloc[ : sep, : ]\ndf_train = df_train.assign(loss = pd.Series(Y))\nprint(df_train.shape)\n\n# Calculating the PCC\ncor_mat = df_train.corr(method='pearson', min_periods=50)\nprint(cor_mat.shape)\n\n# Number of variables having abs(PCC) with 'loss', less than or equal to 0.005\n# We will simply eliminate those features, as they are related with the 'loss', to the minimum extent\nred_fea = []\nfor i, pcc in enumerate(cor_mat['loss']):\n    if(-0.005 <= pcc and pcc <= 0.005):\n        red_fea.append(cor_mat.index[i])","5da9cad1":"# Dropping all the Redundant features\ndf.drop(red_fea, axis=1, inplace=True)\nprint(df.shape)","843a6c76":"# Splitting the df back into df_train and df_test\ndf_train = df.iloc[ :sep, : ]\ndf_test = df.iloc[sep: , : ]\nprint(df_train.shape, df_test.shape)","ac1aef02":"scaler = StandardScaler()\ndf_train = scaler.fit_transform(df_train)\ndf_test = scaler.transform(df_test)\nprint(df_train.shape, df_test.shape)","c4e86803":"# Dimensionality Reduction using PCA\n# pca = PCA(n_components=None)\n# df_train = pca.fit_transform(df_train)\n# df_test = pca.transform(df_test)\n# print(df_train.shape, df_test.shape)","d20c1622":"# Defining the Custom Metric\ndef rmse(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","a053cb55":"# Splitting the df_train into train & val sets\nX_train, X_val, y_train, y_val = train_test_split(df_train, Y, test_size=0.1, random_state=42)\nprint(X_train.shape, X_val.shape, y_train.shape, y_val.shape)","c95aa549":"# Gradient Boosting Regressor Model\n# lr, nes, mss, ss = 1, 50, 15, 1\n# gbr = GradientBoostingRegressor(\n#     learning_rate=lr, n_estimators=nes, min_samples_split=mss, \n#     subsample=ss, verbose=1\n# )\n# gbr.fit(X_train, y_train)\n# y_pred = gbr.predict(X_val)\n# print(rmse(y_val, y_pred))","f61caad6":"# Linear Regression\n# lr = LinearRegression(normalize=True)\n# lr.fit(X_train, y_train)\n# y_pred = lr.predict(X_val)\n# print(rmse(y_val, y_pred))","9de54ebb":"# Histogram Gradient Boosting Regressor\n# lr, mi, md = 0.05, 700, 22\n# hgbr = HistGradientBoostingRegressor(\n#     learning_rate = lr, max_iter= mi, \n#     max_depth = md, verbose=1,\n# )\n# hgbr.fit(X_train, y_train)\n# y_pred_train = hgbr.predict(X_train)\n# y_pred_val = hgbr.predict(X_val)\n# print(\"RMSE on Training Dataset \", rmse(y_train, y_pred_train))\n# print(\"RMSE on Validation Dataset \", rmse(y_val, y_pred_val))","77387851":"# Cat Boosting Regressor\nitr, lr, d = 50, 0.5, 4\ncbr = CatBoostRegressor(\n    iterations = itr, learning_rate = lr, depth = d,\n    custom_metric = 'RMSE', verbose = 1\n)\ncbr.fit(X_train, y_train)\ny_pred_train = cbr.predict(X_train)\ny_pred_val = cbr.predict(X_val)\nprint(\"RMSE on Training Dataset \", rmse(y_train, y_pred_train))\nprint(\"RMSE on Validation Dataset \", rmse(y_val, y_pred_val))","7baecb61":"# Training the model on the entire df_train\nmodel = CatBoostRegressor(\n    iterations = itr, learning_rate = lr, depth = d,\n    custom_metric = 'RMSE', verbose = 1\n)\nmodel.fit(df_train, Y)","34a21bc4":"y_test = cbr.predict(df_test)\ndf_sub['loss'] = y_test\nprint(df_sub.shape)","07d1634c":"df_sub.to_csv(\"submission.csv\", index = False)","cf605621":"# Training the Model","7e7e6e5f":"# Importing the Dataset","bee15066":"# Visualizing & Pre-processing the Dataset\n- From the above code cells, we can see that all the features are numerical, and corresponding to every feature, all the values are non-null.","b8db0a5b":"# Tabular Playground Series (August 2021)\n- This notebook covers my code for the Tabular Playground Series - August challenge, which can be found [here](https:\/\/www.kaggle.com\/c\/tabular-playground-series-aug-2021)\n- In this notebook, I have used various EDA techniques, which includes:\n    - PCC (Pearson Correlation Coefficient), I have simply eliminated all those features having PCC with 'loss' less than abs(0.005)\n    - Using Standard Scaler for the Standardization of all the features\n    - PCA (Principal Component Analysis), but it didn't gave any improvement in the results, so didn't used it in the final submission\n- As for the training part, I used various models, which includes\n    - Gradient Boosted Decision Tree (GBDT)\n    - Linear Regression (LR)\n    - Histogram Gradient Boosted Regressor\n    - Cat Boost Regressor\n- If you liked my work, do upvote it :)","dd83cdc7":"# Submitting the Predictions","25b160b6":"# Installing & Importing Packages"}}