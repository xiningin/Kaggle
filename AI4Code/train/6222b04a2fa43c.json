{"cell_type":{"36a37518":"code","78eb0946":"code","45f95a6d":"code","14810636":"code","04dd2fc9":"code","879b3943":"code","b91a1ec7":"code","d894d22c":"code","7e4fbe73":"code","f5f0f3b0":"code","11637240":"code","59b7c3bc":"code","20d4bbc5":"code","61415437":"code","1eccc152":"code","08a18447":"code","9a876b35":"code","7cea8ce5":"code","7bbf2ef2":"code","3000ce78":"code","dd4d0629":"code","548f4b36":"code","2b659481":"code","3eb8e420":"code","6541bfba":"code","4e7e1bf0":"code","9e4ce4c6":"code","ec9fb44e":"code","4afb8335":"code","a29f616c":"code","639f477a":"code","f0e507e4":"code","6084ca88":"code","9a904ad7":"code","9a94d686":"code","ca922bb4":"code","007d892d":"markdown","1b34b83b":"markdown","a62697a0":"markdown","15f5a7f0":"markdown","9ccf4b56":"markdown"},"source":{"36a37518":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.\nfrom keras.models import Sequential\nfrom keras.layers import LSTM,Dense,Dropout,Embedding,CuDNNLSTM,Bidirectional\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom tqdm import tqdm\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nfrom keras.models import Model\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers import MaxPooling1D , GlobalMaxPooling1D","78eb0946":"!wget http:\/\/nlp.stanford.edu\/data\/glove.6B.zip","45f95a6d":"#unzip the file \n!unzip glove*.zip","14810636":"#store 300 size vector representation different words from the file to a disctionary\nembedding_index = {}\nf = open('glove.6B.300d.txt',encoding='utf-8')\nfor line in f:\n  value = line.split()\n  word = value[0]\n  coeffs = np.asarray(value[1:],dtype = 'float32')\n  embedding_index[word] = coeffs\nf.close()","04dd2fc9":"len(embedding_index)","879b3943":"df = pd.read_csv('..\/input\/train.csv')","b91a1ec7":"df.head()","d894d22c":"df.tail()","7e4fbe73":"print(df['target'].value_counts())\nsns.countplot(df['target'])","f5f0f3b0":"#to check the proportion of the ones to zeros\ntarget_count = df['target'].value_counts()\n\nprint('Class 0:', target_count[0])\nprint('Class 1:', target_count[1])\nprint('Proportion:', round(target_count[0] \/ target_count[1], 2), ': 1')\n\ntarget_count.plot(kind='bar', title='Count (target)');","11637240":"x = df['question_text']\ny = df['target']","59b7c3bc":"token = Tokenizer()","20d4bbc5":"from sklearn.model_selection import train_test_split\nq_train , q_test = train_test_split(df, test_size=0.2)","61415437":"q_train.shape , q_test.shape","1eccc152":"x_train = q_train['question_text']\ny_train = q_train['target']\nx_test  = q_test['question_text']\ny_test  = q_test['target']","08a18447":"x_train.shape , y_train.shape , x_test.shape , y_test.shape","9a876b35":"from sklearn.utils import class_weight\nclass_weights = class_weight.compute_class_weight('balanced',\n                                                 np.unique(y_train),\n                                                 y_train)\nclass_weights","7cea8ce5":"token.fit_on_texts(x)\nseq = token.texts_to_sequences(x)","7bbf2ef2":"pad_seq = pad_sequences(seq,maxlen=300)","3000ce78":"vocab_size = len(token.word_index)+1","dd4d0629":"x = df['question_text']\ny = df['target']","548f4b36":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing import sequence\nfrom keras.layers import LSTM,Activation,Dense,Input,Embedding,Dropout\nfrom keras.models import Model\nfrom nltk import word_tokenize\nimport nltk\nnltk.download('punkt')","2b659481":"sent_lens=[len(word_tokenize(x)) for x in x_train]","3eb8e420":"max(sent_lens)","6541bfba":"np.percentile(sent_lens,95)","4e7e1bf0":"max_len=31 #taking the 95% quantile value of the sentence length\n\ntk=Tokenizer(char_level=False,split=' ') # tokenizing the sentence \n\ntk.fit_on_texts(x_train)\n\nseq_train=tk.texts_to_sequences(x_train) # create tokens on train\nseq_test=tk.texts_to_sequences(x_test) # create tokens on test\n\nvocab_size=len(tk.word_index)\n\nseq_train_matrix=sequence.pad_sequences(seq_train,maxlen=max_len) #padding the sentence with 0 for matching length\nseq_test_matrix=sequence.pad_sequences(seq_test,maxlen=max_len)","9e4ce4c6":"seq_train_matrix.shape , seq_test_matrix.shape , vocab_size","ec9fb44e":"seq_train_matrix[1]","4afb8335":"# creating our own embedding matrix to bring down the size to 300\n# we'll use 300 D vector representation of the words from pretrained embedding index \n# that we downloaded \n\nembedding_matrix=np.zeros((vocab_size+1,300))\n\nfor word,i in tk.word_index.items():\n    embed_vector=embedding_index.get(word)\n    if embed_vector is not None:\n        embedding_matrix[i]=embed_vector\n# if there are specific words which are not present in pretrained embedding \n# their weights will remain 0. if there are too many such words \n# then you should probably not use pretrained embeddings","a29f616c":"inputs=Input(name='text_input',shape=[max_len])\nembed=Embedding(vocab_size+1,300,input_length=max_len,mask_zero=True,\n                weights=[embedding_matrix],trainable=False)(inputs)\n\nGRU_layer=GRU(50)(embed)\n\ndense1=Dense(10,activation='relu')(GRU_layer)\ndrop=Dropout(0.2)(dense1)\n\nfinal_layer=Dense(1,activation='sigmoid')(drop)\n\nmodel_GRU=Model(inputs=inputs,outputs=final_layer)\nmodel_GRU.summary()","639f477a":"model_GRU.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])","f0e507e4":"from keras.callbacks import ModelCheckpoint\nimport os\noutputFolder = '.\/content\/Model_output\/'\nif not os.path.exists(outputFolder):\n    os.makedirs(outputFolder)\nfilepath = outputFolder+\"\/weights-{epoch:02d}-{val_acc:.4f}.h5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='val_acc', verbose=1, \n                             save_best_only=False, save_weights_only=True, \n                             mode='auto', period=1)\n# this will save the weights every 10 epoch\n\nfrom keras.callbacks import EarlyStopping\nearlystop = EarlyStopping(monitor='val_acc', min_delta=0.0001, patience=3,\n                          verbose=1, mode='auto')","6084ca88":"model_GRU.fit(seq_train_matrix,y_train,validation_data=[seq_test_matrix,y_test],epochs=10,class_weight={0:0.53,1:8},\n          batch_size=10000,callbacks=[earlystop,checkpoint])","9a904ad7":"p=model_GRU.predict(seq_test_matrix)\nfrom sklearn.metrics import roc_auc_score\nroc_auc_score(y_test,p)","9a94d686":"from sklearn.metrics import classification_report,f1_score\nprint(f1_score(y_test,p >.50))","ca922bb4":"print(classification_report(y_test,p>.50))","007d892d":"Reading the dataset","1b34b83b":"The above graph depicts that we have a imbalanced classess","a62697a0":"**Converting the text into sequence for processing in LSTM Layers**","15f5a7f0":"**Displaying the count of each class in Y label**","9ccf4b56":"**Converting the words in our Vocabulary to their corresponding embeddings and placing them in a matrix.**"}}