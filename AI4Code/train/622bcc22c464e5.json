{"cell_type":{"6c9bcbb5":"code","97d434e0":"code","ba8925a1":"code","b2bee3c1":"code","e128f1b8":"code","6a955404":"code","d37e9c92":"code","ef6d68fd":"code","d3c6088d":"code","7c8d25dc":"markdown","07b28e30":"markdown","2b4d39b3":"markdown","6428b4bb":"markdown","8451c444":"markdown","2ad5bcb2":"markdown","c61284f4":"markdown","e5c115fb":"markdown","e7051b1f":"markdown","132c285b":"markdown","df38f30a":"markdown"},"source":{"6c9bcbb5":"import codecs\nimport numpy as np\nimport tensorflow as tf\nprint(tf.__version__)","97d434e0":"data_fpath = '..\/input\/englishrussian-dictionary-for-machine-translate\/rus.txt'\nmax_sentences = 10000\n\ninput_texts = []\ntarget_texts = []\nlines = codecs.open(data_fpath, 'r', encoding='utf8').readlines()[:max_sentences]\nfor line in lines:\n    input_text, target_text, = line.split('\\t')[:2]\n    input_texts.append(input_text)\n    target_texts.append(target_text)","ba8925a1":"def prepare_vocab(texts):\n    vocab = sorted(set(''.join(texts)))\n    vocab.append('<START>')\n    vocab.append('<END>')\n    vocab_size = len(vocab)\n    char2idx = {u:i for i, u in enumerate(vocab)}\n    idx2char = np.array(vocab)\n    return vocab_size, char2idx, idx2char\n\nINPUT_VOCAB_SIZE, input_char2idx, input_idx2char = prepare_vocab(input_texts)\nTARGET_VOCAB_SIZE, target_char2idx, target_idx2char = prepare_vocab(target_texts)","b2bee3c1":"input_texts_as_int = [[input_char2idx[c] for c in text] for text in input_texts]\ntarget_texts_as_int = [[target_char2idx[c] for c in text] for text in target_texts]\n\nencoder_input_seqs = [np.array(text) for text in input_texts_as_int]\ndecoder_input_seqs = []\ndecoder_target_seqs = []\nfor target_text in target_texts_as_int:\n    decoder_input_seqs.append(np.array([target_char2idx['<START>']] + target_text))\n    decoder_target_seqs.append(np.array(target_text + [target_char2idx['<END>']]))","e128f1b8":"max_enc_seq_length = max([len(seq) for seq in encoder_input_seqs])\nmax_dec_seq_length = max([len(seq) for seq in decoder_input_seqs])\n\nencoder_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(\n    encoder_input_seqs,\n    value=input_char2idx[' '],\n    padding='post',\n    maxlen=max_enc_seq_length)\n\ndecoder_input_seqs = tf.keras.preprocessing.sequence.pad_sequences(\n    decoder_input_seqs,\n    value=target_char2idx[' '],\n    padding='post',\n    maxlen=max_dec_seq_length)\n\ndecoder_target_seqs = tf.keras.preprocessing.sequence.pad_sequences(\n    decoder_target_seqs,\n    value=target_char2idx[' '],\n    padding='post',\n    maxlen=max_dec_seq_length)","6a955404":"H_SIZE = 256 \nEMB_SIZE = 256 \n\nclass Encoder(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.embed = tf.keras.layers.Embedding(INPUT_VOCAB_SIZE, EMB_SIZE)\n        self.lstm = tf.keras.layers.LSTM(H_SIZE, return_sequences=False, return_state=True)\n        \n    def call(self, x):\n        out = self.embed(x)\n        _, h, c = self.lstm(out)\n        state = (h, c)\n        return state\n\nclass Decoder(tf.keras.Model):\n    def __init__(self):\n        super().__init__()\n        self.embed = tf.keras.layers.Embedding(TARGET_VOCAB_SIZE, EMB_SIZE)\n        self.lstm = tf.keras.layers.LSTM(H_SIZE, return_sequences=True, return_state=True)\n        self.fc = tf.keras.layers.Dense(TARGET_VOCAB_SIZE, activation='softmax')\n        \n    def call(self, x, init_state):\n        out = self.embed(x)\n        out, h, c = self.lstm(out, initial_state=init_state)\n        out = self.fc(out)\n        state = (h, c)\n        return out, state\n\nencoder_model = Encoder()\ndecoder_model = Decoder()\n\nencoder_inputs = tf.keras.layers.Input(shape=(None,))\ndecoder_inputs = tf.keras.layers.Input(shape=(None,))\n\nenc_state = encoder_model(encoder_inputs)\ndecoder_outputs, _ = decoder_model(decoder_inputs, enc_state)\n\nseq2seq = tf.keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)","d37e9c92":"BATCH_SIZE = 64\nEPOCHS = 100\n\nloss = tf.losses.SparseCategoricalCrossentropy()\nseq2seq.compile(optimizer='rmsprop', loss=loss, metrics=['accuracy'])\nseq2seq.fit([encoder_input_seqs, decoder_input_seqs], decoder_target_seqs,\n          batch_size=BATCH_SIZE,\n          epochs=EPOCHS)","ef6d68fd":"def seq2seq_inference(input_seq):\n    state = encoder_model(input_seq)\n\n    target_seq = np.array([[target_char2idx['<START>']]])\n\n    decoded_sentence = ''\n    while True:\n        output_tokens, state = decoder_model(target_seq, state)\n\n        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n        sampled_char = target_idx2char[sampled_token_index]\n        decoded_sentence += sampled_char\n\n        if (sampled_char == '<END>' or\n           len(decoded_sentence) > max_dec_seq_length):\n            break\n\n        target_seq = np.array([[sampled_token_index]])\n\n    return decoded_sentence","d3c6088d":"for seq_index in range(0, 20):\n    input_seq = encoder_input_seqs[seq_index: seq_index + 1]\n    decoded_sentence = seq2seq_inference(input_seq)\n    print('-')\n    print('Input sentence:', input_texts[seq_index])\n    print('Result sentence:', decoded_sentence)\n    print('Target sentence:', target_texts[seq_index])","7c8d25dc":"### Loading Libraries\nTensorFlow must be at least version 2.0\n","07b28e30":"### Padding chains\n\nRecall that for training we need to use batches, which consist of chains of the same length. And initially the length of the chains (both input and output) can be arbitrary. Therefore, we need to do padding - to complete all chains to some fixed length. For example, using the space character `` ''. As the lengths, we will take the maximum possible among all available chains (separately for input, separately for output).","2b4d39b3":"### Loading dataset\n\nAs a training dataset, we will use pairs of short English and Russian sentences (source: http:\/\/www.manythings.org\/anki\/). Let's take the first 10,000 phrases (they are sorted by length, so we'll take the shortest ones for simplicity).\n\nFor this code to work, you need to load the `rus.txt` file into Colab.\n\nWe read the lines from this file, parse them and put the sentences in the `input_texts` and` target_texts` lists (input and output sentences, respectively).","6428b4bb":"### Preparing the training dataset\n\nOur model will consist of two parts: `Encoder` and` Decoder`. The task of the encoder is to read the input string and get its encoded representation. And the task of the decoder for this encoded representation is to get the output chain.\n\nThe decoder is essentially a text generator, so it is used in the same way as we did earlier with the character text generator. The only difference is that here the decoder will receive the initial state from the encoder, and will receive the `<START>` token as the \"start\" of the chain.\n\nAnd in the same way as in the case of the generator, for training the decoder, we will use the same chain as input and target output, but shifted by one element in time. Finally, the Decoder must predict the `<END>` token.\n\nFor example, the input and output for the decoder can be two strings of seven characters (the starting and ending token is one character):\n\n`<START> Hello` ->` Hello <END> `\n\nThus, we need three sets of chains to train the `Encoder-Decoder`:\n - `encoder_input_seqs` - inputs to Encoder\n - `decoder_input_seqs` - inputs to Decoder\n - `decoder_target_seqs` - target outputs from Decoder (and the entire Encoder-Decoder model)\n\nThe strings themselves will be sequences of integer indices (obtained using the appropriate dictionaries).\n","8451c444":"### Inference example\n\nLet's try the Seq2Seq model inference on the chains from our dataset.","2ad5bcb2":"### Train the model","c61284f4":"### Preparing dictionaries\n\nAs before, we will use one character (not a word) as an element of the sequence. This is fine for our simple short sentence task.\n\nLet's prepare two dictionaries (mapping index to character and character to index), and we will do this for input texts (`input_texts`) and output texts (` target_texts`), since they are in different languages and consist of different characters.\n\nIn addition, we need special tokens for the beginning and end of the chain (`<START>`, `<END>`).","e5c115fb":"After training, the result will be something like this - <br><br>\n\nInput sentence: Go.<br>\nResult sentence: \u0418\u0434\u0438.<br>\nTarget sentence: \u041c\u0430\u0440\u0448!<br>\n-\nInput sentence: Go.<br>\nResult sentence: \u0418\u0434\u0438.<br>\nTarget sentence: \u0418\u0434\u0438.<br>\n-\nInput sentence: Go.<br>\nResult sentence: \u0418\u0434\u0438.<br>\nTarget sentence: \u0418\u0434\u0438\u0442\u0435.<br>\n-\nInput sentence: Hi.<br>\nResult sentence: \u0417\u0434\u0440\u0430\u0441\u0442\u0435.<br>\nTarget sentence: \u0417\u0434\u0440\u0430\u0432\u0441\u0442\u0432\u0443\u0439\u0442\u0435.<br>\n-\nInput sentence: Hi.<br>\nResult sentence: \u0417\u0434\u0440\u0430\u0441\u0442\u0435.<br>\nTarget sentence: \u041f\u0440\u0438\u0432\u0435\u0442!<br>\n-\nInput sentence: Hi.<br>\nResult sentence: \u0417\u0434\u0440\u0430\u0441\u0442\u0435.<br>\nTarget sentence: \u0425\u0430\u0439.<br>\n-\nInput sentence: Hi.<br>\nResult sentence: \u0417\u0434\u0440\u0430\u0441\u0442\u0435.<br>\nTarget sentence: \u0417\u0434\u0440\u0430\u0441\u0442\u0435.<br>\n-\nInput sentence: Hi.<br>\nResult sentence: \u0417\u0434\u0440\u0430\u0441\u0442\u0435.<br>\nTarget sentence: \u0417\u0434\u043e\u0440\u043e\u0301\u0432\u043e!<br>\n-\nInput sentence: Run!<br>\nResult sentence: \u0411\u0435\u0433\u0438\u0442\u0435!<br>\nTarget sentence: \u0411\u0435\u0433\u0438!<br>\n-\nInput sentence: Run!<br>\nResult sentence: \u0411\u0435\u0433\u0438\u0442\u0435!<br>\nTarget sentence: \u0411\u0435\u0433\u0438\u0442\u0435!<br>\n-\nInput sentence: Run.<br>\nResult sentence: \u0411\u0435\u0433\u0438\u0442\u0435!<br>\nTarget sentence: \u0411\u0435\u0433\u0438!<br>\n-\nInput sentence: Run.<br>\nResult sentence: \u0411\u0435\u0433\u0438\u0442\u0435!<br>\nTarget sentence: \u0411\u0435\u0433\u0438\u0442\u0435!<br>\n-\nInput sentence: Who?<br>\nResult sentence: \u041a\u0442\u043e?<br>\nTarget sentence: \u041a\u0442\u043e?<br>\n-\nInput sentence: Wow!<br>\nResult sentence: \u0417\u0434\u043e\u0440\u043e\u0432\u043e!<br>\nTarget sentence: \u0412\u043e\u0442 \u044d\u0442\u043e \u0434\u0430!<br>\n-\nInput sentence: Wow!<br>\nResult sentence: \u0417\u0434\u043e\u0440\u043e\u0432\u043e!<br>\nTarget sentence: \u041a\u0440\u0443\u0442\u043e!<br>\n-\nInput sentence: Wow!<br>\nResult sentence: \u0417\u0434\u043e\u0440\u043e\u0432\u043e!<br>\nTarget sentence: \u0417\u0434\u043e\u0440\u043e\u0432\u043e!<br>\n-\nInput sentence: Wow!<br>\nResult sentence: \u0417\u0434\u043e\u0440\u043e\u0432\u043e!<br>\nTarget sentence: \u0423\u0445 \u0442\u044b!<br>\n-\nInput sentence: Wow!<br>\nResult sentence: \u0417\u0434\u043e\u0440\u043e\u0432\u043e!<br>\nTarget sentence: \u041e\u0433\u043e!<br>\n-\nInput sentence: Wow!<br>\nResult sentence: \u0417\u0434\u043e\u0440\u043e\u0432\u043e!<br>\nTarget sentence: \u0412\u0430\u0445!<br>\n-\nInput sentence: Fire!<br>\nResult sentence: \u041e\u0433\u043e\u043d\u044c!<br>\nTarget sentence: \u041e\u0433\u043e\u043d\u044c!<br>\n","e7051b1f":"### Function for inference\n\nStarting inference for the Encoder-Decoder consists of sequential application of the encoder and decoder.\n\nFirst, we run the input string through the encoder and get the encoded representation of `state`.\n\nAnd then we use the decoder in a similar mode, as it was with the text generator (only now we pass `state` as the initial state). In the loop, we gradually generate the output chain, feeding only one (current) symbol to the decoder and getting one predicted (next) symbol. We start with the `<START>` character and repeat until we get the `<END>` character in the output or reach the limit on the number of characters in the chain. To determine which symbol the decoder predicted, simply use the `argmax` function for the output distribution (FC layer output).","132c285b":"> training","df38f30a":"### Creating a model\n\nTo create an Encoder-Decoder model, we will use a mixture of two styles: implementing models through our own class and a functional API.\n\nIt is convenient to implement Encoder and Decoder themselves (separately) in the form of custom classes (inherited from `tf.keras.Model`), since they may have some kind of complex implementation.\n\nIn our case, the Encoder will consist of an Embedding layer and one LSTM layer, which will return the final state after going through the entire chain. We are interested in both the `h` vector and the LSTM` c` state vector as a state. For it we need an additional flag `return_state = True`\n\nThe Decoder will have Embedding, LSTM and a fully connected layer for generating the final responses (probability distribution over symbols). For forward propagation (`__call__`), in addition to the input chain, the decoder will receive a state from the encoder (` init_state`) and transfer it to its LSTM layer as an initial state, and return the predicted output chain (of the same length, return_sequences = True) the state of this LSTM.\n\nAfter we have separately built the Encoder and Decoder, we need to connect them in the Encoder-Decoder. But since we need to create several inputs to the model (a separate input chain to the encoder, a separate input chain to the decoder), it is very convenient to do this using the functional API. Input nodes are created using `tf.keras.layers.Input`, and then we build a computational graph using the models` encoder_model` and `decoder_model`.\n\nThe final model is `seq2seq`"}}