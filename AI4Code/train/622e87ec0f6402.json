{"cell_type":{"40b11eba":"code","1c7909ae":"code","d272e4b2":"code","9c79d123":"code","25c29514":"code","ab132509":"code","bbea68e7":"code","8a6fede7":"code","15d18416":"code","6dbcfdb9":"code","b82222d4":"code","e678479b":"code","ac181524":"code","e828ffc3":"code","fdeca3fa":"code","cf279da1":"code","38de467a":"code","cc35cb97":"code","64e98313":"code","6c49c28e":"code","4d820215":"markdown","180875c8":"markdown","fc351643":"markdown","7c6124b1":"markdown","876110ef":"markdown","f6750c2e":"markdown","fd8acf89":"markdown","8192de8e":"markdown","4c8de852":"markdown","d4ad8674":"markdown","d563e9cc":"markdown","25326456":"markdown"},"source":{"40b11eba":"%matplotlib inline\nimport preprocessing\n\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport pickle\nimport tensorflow as tf\n\n# to make this notebook's output stable across runs\ndef reset_graph(seed=42):\n    tf.reset_default_graph()\n    tf.set_random_seed(seed)\n    np.random.seed(seed)\n    \n\ndef log_dir(prefix=''):\n    from datetime import datetime\n    now = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n    root_logdir = os.path.join(tmp_dir, prefix, 'logs')\n    if prefix:\n        prefix += '-'\n    name = prefix + 'run-' + now\n    return '{}\/{}\/'.format(root_logdir, name)\n\n\n# permute whole training set and generate batches from it\n# better alternative => tf.data.Dataset.shuffle\ndef shuffle_batch(X, y, batch_size):\n    rnd_idx = np.random.permutation(len(X))\n    n_batches = len(X) \/\/ batch_size\n    for batch_idx in np.array_split(rnd_idx, n_batches):\n        batch_X, batch_y = X[batch_idx], y[batch_idx]\n        yield batch_X, batch_y","1c7909ae":"(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()","d272e4b2":"X_train.shape, y_train.shape, X_test.shape, y_test.shape, X_train.dtype","9c79d123":"X_train[0], y_train[0]","25c29514":"def plot_digit(data):\n    plt.imshow(data, cmap=mpl.cm.binary, interpolation='nearest')\n    plt.axis(\"off\")\n    plt.show()\n\n\ndef plot_digits(instances, images_per_row=10, **options):\n    size = 28\n    n_instances = len(instances)\n    images_per_row = min(n_instances, images_per_row)\n    n_rows = (n_instances - 1) \/\/ images_per_row + 1\n    n_empty = n_rows * images_per_row - n_instances\n    np.append(instances, np.zeros((size, size * n_empty)))\n    row_images = []\n    for row in range(n_rows):\n        row_images.append(np.concatenate(\n            instances[row * images_per_row: (row + 1) * images_per_row],\n            axis=1))\n    image = np.concatenate(row_images, axis=0)\n    plt.imshow(image, cmap=mpl.cm.binary, **options)\n    plt.axis('off')","ab132509":"plt.figure(figsize=(9, 9))\nexample_images = X_train[np.random.choice(X_train.shape[0], 100)]\nplot_digits(example_images, images_per_row=10)\nplt.show()","bbea68e7":"# deep feed forward network layers\nn_hidden1 = 300\nn_hidden2 = 100\n\n# training\nn_epochs = 1000\nn_batch_size = 32\n\n# early stopping\nmax_epochs_without_progress = 30","8a6fede7":"reset_graph()\n\nn_inputs = 28 * 28 # single image is loaded as 28x28 2D array, but input to network is 1D\nn_outputs = 10 # classify digits from 0-9\n\n# reshape and normalize input\nX_train = X_train.astype(np.float32).reshape(-1, n_inputs) \/ 255.0\nX_test = X_test.astype(np.float32).reshape(-1, n_inputs) \/ 255.0\ny_train = y_train.astype(np.int32)\ny_test = y_test.astype(np.int32)\n\n# split training data to validation and training data\nX_valid, X_train = X_train[:5000], X_train[5000:] \ny_valid, y_train = y_train[:5000], y_train[5000:]","15d18416":"def dense_layer(X, n_neurons, name, kernel_init=tf.truncated_normal):\n    with tf.name_scope(name):\n        fan_in = int(X.get_shape()[1])\n        W = tf.Variable(kernel_init(shape=(fan_in, n_neurons)), name=\"weights\")\n        b = tf.Variable([tf.zeros(n_neurons)], name=\"bias\")\n        return tf.add(tf.matmul(X, W), b)        \n    \n    \ndef batch_norm(Z, mean, var, epsilon=0.001):\n    # apply the initial batch normalizing transform\n    Z_hat = (Z - mean) \/ tf.sqrt(var + epsilon)\n\n    fan_in = int(Z.get_shape()[1])\n\n    # create two new parameters, scale and beta (shift)\n    scale = tf.Variable(tf.ones([fan_in]), name='scale')\n    beta = tf.Variable(tf.zeros([fan_in]), name='beta')\n\n    # scale and shift to obtain the final output of the batch normalization\n    # this value is fed into the activation function \n    return scale * Z_hat + beta\n    \n    \ndef batch_norm_layer(Z, is_training, epsilon=0.001, decay=0.999):\n    with tf.name_scope('batch_norm'):                \n        pop_mean = tf.Variable(tf.zeros([Z.get_shape()[-1]]), trainable=False, \n                               name='pop_mean')\n        pop_var = tf.Variable(tf.ones([Z.get_shape()[-1]]), trainable=False, \n                              name='pop_var')\n        \n        def mean_var_with_update():\n            # calculate batch mean and variance\n            batch_mean, batch_var = tf.nn.moments(Z, [0])        \n            \n            # estimate the population mean and variance during training,\n            # use an exponential moving average\n            train_mean = tf.assign(pop_mean, pop_mean * decay + batch_mean * (1 - decay))\n            train_var = tf.assign(pop_var, pop_var * decay + batch_var * (1 - decay))\n                        \n            with tf.control_dependencies([train_mean, train_var]):\n                return tf.identity(batch_mean), tf.identity(batch_var)\n        \n        # use batch mean and var for training but whole population mean and var\n        # for prediction\n        mean, var = tf.cond(is_training, \n                            mean_var_with_update,\n                            lambda: (pop_mean, pop_var))\n        \n        return batch_norm(Z, mean, var, epsilon)\n\n\ndef he_normal(shape):\n    return tf.truncated_normal(shape, stddev=np.sqrt(2 \/ shape[0]))","6dbcfdb9":"X = tf.placeholder(tf.float32, shape=(None, n_inputs), name='X')\ny = tf.placeholder(tf.int32, shape=(None), name='y')","b82222d4":"with tf.name_scope('dnn'):\n    training = tf.placeholder(tf.bool, name='training')\n    hidden1 = tf.nn.relu(\n        batch_norm_layer(\n            dense_layer(X, n_hidden1, 'hidden1', kernel_init=he_normal),\n            is_training=training))\n    hidden2 = tf.nn.relu(\n        batch_norm_layer(\n            dense_layer(hidden1, n_hidden2, 'hidden2', kernel_init=he_normal),\n            is_training=training))\n    logits = dense_layer(hidden2, n_outputs, 'outputs')","e678479b":"with tf.name_scope('loss'):\n    # we could also use categorical_cross_entropy with labels encoded\n    # to one-hot vector\n    xentropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,\n                                                              logits=logits)\n    # compute mean loss for whole batch\n    loss = tf.reduce_mean(xentropy, name='loss') \n    # creates a node in the graph that will evaluate the MSE value and write it\n    # to a TensorBoard-compatible binary log string called a summary\n    loss_summary = tf.summary.scalar('log_loss', loss)","ac181524":"with tf.name_scope('train'):           \n    optimizer = tf.train.AdamOptimizer()\n    training_op = optimizer.minimize(loss) # apply gradients","e828ffc3":"with tf.name_scope('eval'):\n    # binary table with true for every correct label\n    correct = tf.nn.in_top_k(logits, y, 1) \n    # mean accuracy for whole batch\n    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n    # log progress for tensorboard\n    accuracy_summary = tf.summary.scalar('accuracy', accuracy)    ","fdeca3fa":"model_name = 'mnist_ff'\n\n# training state\ntmp_dir = '.tmp'\n\n# prepare tensorboard statistic logging\nlogdir = log_dir(model_name)\n\n# logdir and (optional) graph to visualize\nfile_writer = tf.summary.FileWriter(logdir, tf.get_default_graph())","cf279da1":"is_training = tf.placeholder(tf.bool, shape=(), name='is_training')","38de467a":"# prepare savers for model checkpoints\nmodel_dir = os.path.join(tmp_dir, model_name)\n\nos.makedirs(model_dir, exist_ok=True)\n\ncheckpoint_dir = os.path.join(model_dir, 'checkpoints')\nbest_dir = os.path.join(model_dir, 'models')\nmeta_file = os.path.join(model_dir, '.meta.pkl')\n\n# by default saver uses graph that is created *before* saver creation\nckpt_saver = tf.train.Saver()\nbest_saver = tf.train.Saver()","cc35cb97":"init = tf.global_variables_initializer()\n\n# set or restore training meta data\nbest_loss = np.infty\nepochs_without_progress = 0\nglobal_epoch = 0\n\nif os.path.isfile(meta_file):\n    with open(meta_file, 'rb') as meta:\n        global_epoch, epochs_without_progress, best_loss = pickle.load(meta)\n\nprint('Starting from (epoch: {}, epoch without progress: {}, best_loss: {:.5f})'\n      .format(global_epoch, epochs_without_progress, best_loss))\n        \n# start\/restore training session\nwith tf.Session() as sess:\n    # you could also use tf.train.import_meta_graph() for importing graph structure\n    latest_checkpoint = tf.train.latest_checkpoint(checkpoint_dir)\n    if latest_checkpoint:\n        # initialize variables from last checkpoint\n        chpnt_saver.restore(sess, latest_checkpoint)\n    else:\n        init.run()  # actually initialize all the variables\n        \n    # run actual training starting from last checkpoint\n    for epoch in range(global_epoch, n_epochs):\n        \n        if epochs_without_progress > max_epochs_without_progress:\n            print('Early stopping')\n            break        \n        \n        # train for whole training sets batch per batch\n        for X_batch, y_batch in shuffle_batch(X_train, y_train, n_batch_size):\n            sess.run(training_op, feed_dict={training: True, X: X_batch, y: y_batch})                            \n                                \n        # compute accuracy, loss and summary ops for tensorboard on validation set\n        acc_val, loss_val, acc_summary_str, loss_summary_str = sess.run(\n            [accuracy, loss, accuracy_summary, loss_summary],\n            feed_dict={training: False, X: X_valid, y: y_valid})\n        \n        file_writer.add_summary(acc_summary_str, epoch)\n        file_writer.add_summary(loss_summary_str, epoch)        \n        \n        if epoch % 5 == 0:\n            print('Epoch:', epoch,\n                  '\\tValidation accuracy: {:.3f}%'.format(acc_val * 100),\n                  '\\tLoss: {:.5f}'.format(loss_val))\n            \n        ckpt_saver.save(sess, os.path.join(checkpoint_dir, 'ckpt'))\n                 \n        # early stopping\n        if loss_val < best_loss:\n            epochs_without_progress = 0\n            best_saver.save(sess, os.path.join(best_dir, 'best'))\n            best_loss = loss_val\n        else:\n            epochs_without_progress += 1\n        \n        with open(meta_file, 'wb') as meta:\n            pickle.dump([epoch, epochs_without_progress, best_loss], meta)","64e98313":"# evaluate best model on test set\nwith tf.Session() as sess:\n    best_saver.restore(sess, tf.train.latest_checkpoint(best_dir))\n    accuracy_val = accuracy.eval(feed_dict={training: False, X: X_test, y: y_test})","6c49c28e":"accuracy_val","4d820215":"Hyperparameters.","180875c8":"The goal of this kernel is to provide low level implementation in tensorflow of feed forward neural network training. It's overview of basic deep learning steps including:\n* mini-batch gradient descend\n* he normal weights initialization\n* batch normalization\n* early stopping\n* checkpoints, model saving, tensorboard statistics\n\nThis kernel provides more detailed example of the whole process without hidding details behind high-level api.","fc351643":"Model construction helpers.","7c6124b1":"Load mnist dataset, check it's shape, type and content.","876110ef":"Displaying training data. Using tensorflow_datasets and Data Stream Api would be probably better.","f6750c2e":"Model definition.","fd8acf89":"## Setup","8192de8e":"Split training data to train and test data. Normalize data.","4c8de852":"## Model training","d4ad8674":"## Data analysis","d563e9cc":"## Model evaluation","25326456":"## Model construction"}}