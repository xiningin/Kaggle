{"cell_type":{"c83ebcb7":"code","3e58255a":"code","c09c1395":"code","cc838560":"code","2dcc0843":"code","193a81bf":"code","fa847c48":"code","739991c8":"code","89f06fed":"code","438a8ce8":"code","20007b76":"code","3f597daa":"code","d44518dc":"code","1afbbbd6":"code","a53682af":"code","34cf495c":"code","0ded40fb":"code","302e77fa":"code","87ed7572":"code","a0b0f82c":"code","24c96269":"code","8217aad8":"code","082348cb":"code","5326ecd7":"code","1299bb12":"code","cafe1778":"code","48dd341a":"code","b5f625e7":"code","a9b6cbf7":"code","ce4e92b5":"code","d17c963c":"code","eb7598b1":"code","6ea25b46":"code","e40445fb":"code","ebe5fab8":"code","7d5ea569":"code","94dd6bb9":"code","1aa086c2":"markdown","0daea213":"markdown","f096a5be":"markdown","4a7647dc":"markdown","fd0f7481":"markdown","99bfc25b":"markdown","676a9cc5":"markdown","a63fdce9":"markdown","b3edb1e1":"markdown","3dd4aa06":"markdown","f2c97e56":"markdown","4d886bf9":"markdown","74eb345f":"markdown","11089f79":"markdown","d357538b":"markdown"},"source":{"c83ebcb7":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as stats\nimport nltk\n# nltk.download('stopwords')\n# print('Downloaded Stopwords')\nfrom nltk.corpus import stopwords\nimport re\nfrom xgboost import XGBRegressor\nimport xgboost as xgb\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.naive_bayes import MultinomialNB\nimport seaborn as sns\nfrom sklearn.multioutput import MultiOutputRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom spacy.lang.en import English\nfrom spacy.lang.en.stop_words import STOP_WORDS\nfrom sklearn.base import TransformerMixin\nfrom sklearn.feature_extraction.text import TfidfVectorizer \nfrom sklearn.feature_extraction.text import TfidfTransformer\nstop_words = STOP_WORDS\nimport string\npunctuations = string.punctuation\nfrom sklearn.feature_extraction.text import HashingVectorizer","3e58255a":"train = pd.read_csv('..\/input\/news-popularity-in-multiple-social-media-platforms\/train_file.csv')","c09c1395":"train.head()","cc838560":"train.loc[0,'Headline']","2dcc0843":"train.loc[0,'Title']","193a81bf":"missing_val = pd.DataFrame(train.isnull().sum())\nmissing_val = missing_val.reset_index()\nmissing_val","fa847c48":"train[train['Source'].isna()]","739991c8":"train.dropna(inplace=True)","89f06fed":"train.info()","438a8ce8":"train.describe().T","20007b76":"train['Topic'].value_counts()","3f597daa":"import nltk\nstopwords = nltk.corpus.stopwords.words('english')\nstopwords.extend(['Palestinian','Palestine','Microsoft','Economy','Obama','Barack'])","d44518dc":"from wordcloud import WordCloud\nplt.figure(figsize=(12,6))\ntext = ' '.join(train.Title[train['Topic']=='economy'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","1afbbbd6":"from wordcloud import WordCloud\nplt.figure(figsize=(12,6))\ntext = ' '.join(train.Title[train['Topic']=='obama'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","a53682af":"from wordcloud import WordCloud\nplt.figure(figsize=(12,6))\ntext = ' '.join(train.Title[train['Topic']=='microsoft'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","34cf495c":"from wordcloud import WordCloud\nplt.figure(figsize=(12,6))\ntext = ' '.join(train.Title[train['Topic']=='palestine'])\nwc = WordCloud(background_color='white',stopwords=stopwords).generate(text)\nplt.imshow(wc)","0ded40fb":"sns.set(style='darkgrid',palette='Set1')","302e77fa":"_ = sns.jointplot(x='SentimentTitle',y='SentimentHeadline',data=train,kind = 'reg')\n_.annotate(stats.pearsonr)\nplt.show()","87ed7572":"\n# Bar graph exploring total sentiment for the different topics\n\ntrain.groupby('Topic').agg('sum')[['SentimentHeadline', 'SentimentTitle']].plot(kind='bar', figsize=(25, 7),\n                                                          stacked=True, color=['b', 'r', 'g']);","a0b0f82c":"plt.figure(figsize=(15,15))\n_ = sns.heatmap(train[['Facebook','GooglePlus','LinkedIn','SentimentTitle','SentimentHeadline']].corr(), square=True, cmap='Blues',linewidths=0.5,linecolor='w',annot=True)\nplt.title('Correlation matrix ')\n\nplt.show()","24c96269":"nlp = English()","8217aad8":"def spacy_tokenizer(sentence):\n    # Creating our token object, which is used to create documents with linguistic annotations.\n    mytokens = nlp(sentence)\n\n    # here the token is converted into lowercase if it is a Pronoun and if it is not a Pronoun then it is lemmatized and lowercased    \n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n\n    # Removing stop words using stopword from spacy library and punctuations from string library\n    mytokens = [ word for word in mytokens if word not in stop_words and word not in punctuations ]\n\n    # return preprocessed list of tokens\n    return mytokens","082348cb":"class predictors(TransformerMixin):\n    def transform(self, X, **transform_params):\n        \n        return [clean_text(text) for text in X]\n\n    def fit(self, X, y, **fit_params):\n        return self\n\n    def get_params(self, deep=True):\n        return {}\n\n\ndef clean_text(text):\n   \n    return text.strip().lower()","5326ecd7":"bow_vector = CountVectorizer(max_features = 100,tokenizer = spacy_tokenizer,ngram_range=(1,2))","1299bb12":"X_train_title = train.loc[:,'Title'].values\ny_train_title = train.loc[:,['SentimentTitle']].values\n\nX_train_headline = train.loc[:,'Headline'].values\ny_train_headline = train.loc[:,['SentimentHeadline']].values","cafe1778":"X_train_title.shape","48dd341a":"y_train_headline.shape","b5f625e7":"from sklearn.model_selection import train_test_split\nx_train_title, x_valid_title, Y_train_title, y_valid_title = train_test_split(X_train_title, y_train_title, shuffle = True, test_size = 0.15)\nx_train_headline, x_valid_headline, Y_train_headline, y_valid_headline = train_test_split(X_train_headline, y_train_headline, shuffle = True, test_size = 0.15)","a9b6cbf7":"xgboost = MultiOutputRegressor(XGBRegressor())\nrand_for = MultiOutputRegressor(RandomForestRegressor(n_estimators=100,\n                                                          max_depth=None,\n                                                          random_state=0))","ce4e92b5":"pipe_title = Pipeline([(\"cleaner\", predictors()),\n                 ('vectorizer', bow_vector),\n                 ('tfidf',TfidfTransformer()),\n                 ('regressor', rand_for)])\n\npipe_headline = Pipeline([(\"cleaner\", predictors()),\n                 ('vectorizer', bow_vector),\n                 ('tfidf',TfidfTransformer()),\n                 ('regressor', rand_for)])","d17c963c":"pipe_title.fit(x_train_title,Y_train_title)","eb7598b1":"pipe_headline.fit(x_train_headline,Y_train_headline)","6ea25b46":"test_pred_title=pipe_title.predict(x_valid_title)","e40445fb":"test_pred_headline=pipe_headline.predict(x_valid_headline)","ebe5fab8":"from sklearn.metrics import mean_absolute_error\nmae_title=mean_absolute_error(y_valid_title,test_pred_title)\nmae_headline=mean_absolute_error(y_valid_headline,test_pred_headline)","7d5ea569":"score=1-((0.4*mae_title)+(0.6*mae_headline))","94dd6bb9":"print(\"Score = {} \\nScore(out of 100%) = {}%\".format(score,score*100))","1aa086c2":"### EDA & Data Visualization","0daea213":"Defining separate pipelines for title and headline. You can choose which regressor you want to use. In this notebook I have used the Random Forrest Regressor","f096a5be":"Now we shall predict on the validation sets and then see what score we obtain","4a7647dc":"Spearating Title and headline, so that they can be trained separately","fd0f7481":"### Custom Tokenizer Function","99bfc25b":"Calculating the Mean Absolute errors for both Title and Headline sentiments","676a9cc5":"Here we caclulate our final score. Score is calulated as \n\nmax(0, 1 - ((0.4*(mean abs error of title)+(0.6*(mean abs error of headline)))","a63fdce9":"We achieved a score of 89.9. That is pretty good. This score is an indication of how close our predicted values were to the target values. It cannot exacly be termed as `accurcacy`, because this is not a classification problem. Our sentiment score is a real number between -1 and 1","b3edb1e1":"### XGBoost and Random Forrest Regressor","3dd4aa06":"### Custom Transformer and text cleaner ","f2c97e56":"Converting words to word vectors","4d886bf9":"#### Splitting both Title and Headline into training and testing sets","74eb345f":"Here, I have predicted the sentiment score of the Title and the Headline of the news articles. \n\nThe target columns are:\n- `SentimentTitle`, which is the sentiment score of the Title\n- `SentimentHeadline`, which is the sentiment score of the Headline\n\nI have used Custom Transform pipelines with Multi-Output Regressor in `scikit-learn`","11089f79":"Train the Regressors for title and headline respectively","d357538b":"### Loading spacy English model"}}