{"cell_type":{"ab4b8acc":"code","405623b0":"code","5ea17da5":"code","24f0fff1":"code","8655bb9f":"code","39ca5c22":"code","df257b83":"code","38cfcf63":"code","dfec9a09":"code","df0481a8":"code","71417d32":"code","28313fcc":"code","b1a461ed":"code","f15d4c09":"code","de240c86":"code","8190e2ba":"code","6bec3b97":"code","0b567a8a":"code","72f8f938":"code","d47a80f5":"code","411d2a60":"code","49db0623":"code","7eca0d81":"code","c00aeb7b":"code","a4ea9ecf":"code","aa7ad027":"code","5cd731fe":"code","4caf4fe5":"code","a18d810c":"code","3cbdd7c9":"code","689096ce":"code","1739ee18":"code","7499193f":"code","89882d0e":"code","556d58fb":"code","db43ca4e":"code","ae5eac8e":"code","8222c1c5":"code","ec3394b2":"code","dbcf71b0":"code","8ed37d64":"code","256b8044":"code","b44e7ca8":"code","6b26f087":"code","a183d38c":"code","f155226d":"code","0469f50a":"code","0c01de63":"code","418546e7":"code","770020b9":"code","4f7d524c":"code","d1640e1c":"code","2d210e29":"code","5a6c8831":"code","5bdbdc46":"code","4d7235ec":"code","f01a1de5":"code","9e048ad9":"code","ab344601":"code","043d349e":"code","a72bc8cf":"code","7cb5b72c":"code","0e90226c":"code","bcbe8358":"code","91ed9cf3":"code","3e0390d6":"code","ea238061":"code","30483d55":"code","33c8ac35":"code","9e24ee4e":"code","dd0b6ad0":"code","4777af74":"code","4b7bcaff":"code","f4a8bd01":"code","3fa29bc7":"code","36b87573":"code","ac0321c9":"code","ad683965":"code","0f495297":"code","f9481eed":"markdown","d7452bb7":"markdown","9a37cf31":"markdown","e4829056":"markdown","cc11b474":"markdown","3b7ffd0f":"markdown","1b7ff0b0":"markdown","7197800c":"markdown","a8a65be1":"markdown","d1356771":"markdown","87c15922":"markdown","6134d33a":"markdown","c27b5dd8":"markdown","388178e0":"markdown","f8d3b405":"markdown","c79b07f1":"markdown","9f11e146":"markdown","257484ad":"markdown","36b73320":"markdown","5df3a8d2":"markdown","c0e5b991":"markdown","49fc87df":"markdown","23ea15ef":"markdown","2cbe4d5a":"markdown","5d8a67a2":"markdown","1b03f7f4":"markdown","ed9fcd36":"markdown","cb80696b":"markdown","849be695":"markdown","5c9ccb76":"markdown","b4e99b63":"markdown","f70a1aa5":"markdown","610e7e23":"markdown","5e4192a9":"markdown","cdc62dd0":"markdown","0a241081":"markdown","33b7cc5e":"markdown","ba0fcf84":"markdown","314b0415":"markdown","215d865e":"markdown","f687f81d":"markdown","a50ec56b":"markdown","aa8744ff":"markdown","3faf0e1e":"markdown","b8e5d42b":"markdown","c295fa8e":"markdown"},"source":{"ab4b8acc":"# Importando bibliotecas que serao utilizadas neste projeto\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport itertools\nimport pickle\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n# Models\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nimport xgboost as xgb\nfrom xgboost import XGBRegressor\nfrom xgboost import plot_importance\n\n\n# Stats\nfrom scipy.stats import skew, norm\nfrom scipy.special import boxcox1p\nfrom scipy.stats import boxcox_normmax\n\n# Misc\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import scale\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import SelectKBest\nfrom sklearn.feature_selection import chi2\nfrom sklearn import preprocessing\nfrom sklearn import utils\n\n# Keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.wrappers.scikit_learn import KerasRegressor\n\n# Ignore useless warnings\nimport warnings\nwarnings.filterwarnings(action=\"ignore\")\npd.options.display.max_seq_items = 8000\npd.options.display.max_rows = 8000\npd.set_option('display.max_columns', None)\n\nimport gc \nimport pickle\nimport datetime\nimport os\n#print(os.listdir(\"data\"))\nprint(os.listdir(\"..\/input\"))","405623b0":"print(datetime.datetime.now().time())","5ea17da5":"#'..\/input\/datadsa\/transacoes_historicas\/transacoes_historicas.csv'\n# limitei em 10.000.000 para poder simular aqui no Kernel (no proprio notebook processei completo)\nhist = pd.read_csv('..\/input\/datadsa\/transacoes_historicas\/transacoes_historicas.csv', nrows=10000000\n                            ,parse_dates=['purchase_date']\n                            ,dtype = {\n                                'city_id': np.int16\n                                ,'installments': np.int16\n                                ,'merchant_category_id': np.int16\n                                ,'month_lag': np.int8\n                                ,'purchase_amount': np.float32\n                                ,'state_id': np.int8\n                                ,'subsector_id': np.int8\n                            }) \n\n#'..\/input\/competicao-dsa-machine-learning-jun-2019\/novas_transacoes_comerciantes.csv'\nnovas = pd.read_csv('..\/input\/datadsa\/novas_transacoes_comerciantes\/novas_transacoes_comerciantes.csv'\n                            ,parse_dates=['purchase_date']\n                            ,dtype = {\n                                'city_id': np.int16\n                                ,'installments': np.int16\n                                ,'merchant_category_id': np.int16\n                                ,'month_lag': np.int8\n                                ,'purchase_amount': np.float32\n                                ,'state_id': np.int8\n                                ,'subsector_id': np.int8\n                            })   \n\n#'..\/input\/competicao-dsa-machine-learning-jun-2019\/dataset_treino.csv'\ntrain = pd.read_csv('..\/input\/competicao-dsa-machine-learning-jun-2019\/dataset_treino.csv'\n                       ,parse_dates=['first_active_month']\n                       ,dtype = {\n                                'feature_1': np.int8\n                                ,'feature_2': np.int8\n                                ,'feature_3': np.int8\n                            })\n\n#'..\/input\/competicao-dsa-machine-learning-jun-2019\/comerciantes.csv'\ncom = pd.read_csv('..\/input\/competicao-dsa-machine-learning-jun-2019\/comerciantes.csv')\n\n#'..\/input\/competicao-dsa-machine-learning-jun-2019\/dataset_teste.csv'\ntest = pd.read_csv('..\/input\/competicao-dsa-machine-learning-jun-2019\/dataset_teste.csv'\n                        ,parse_dates=['first_active_month']\n                        ,dtype = {\n                                'feature_1': np.int8\n                               ,'feature_2': np.int8\n                               ,'feature_3': np.int8\n                            })","24f0fff1":"# Criar um index para o dataframe de treino\ntrain = train.reset_index()\ntest = test.reset_index()","8655bb9f":"tmp = pd.concat( [hist, novas],axis=0,ignore_index=True)","39ca5c22":"novas.shape, hist.shape, tmp.shape","df257b83":"# Uniao dos dataset de treino com transacoes novas e historicas\ndf = pd.merge(train, tmp, on='card_id', how='left')\ndfTest = pd.merge(test, tmp, on='card_id', how='left')\n\ndel train, hist, tmp, novas\ngc.collect()\n\n# Uniao dos dataset de treino e teste com comerciantes\ndf = pd.merge(df, com, on='merchant_id', how='left')\ndfTest = pd.merge(dfTest, com, on='merchant_id', how='left')\n\ndel com\ngc.collect()","38cfcf63":"\ndf.drop(columns = [\"merchant_category_id_y\",\n                   \"merchant_category_id_x\", \n                   \"subsector_id_y\", \n                   \"subsector_id_x\", \n                   \"city_id_y\", \n                   \"city_id_x\", \n                   \"state_id_y\", \n                   \"state_id_x\", \n                   \"category_1_y\", \n                   \"category_1_x\", \n                   \"category_2_y\",\n                   \"category_2_x\",\n                   \"category_3\", \n                   \"category_4\", \n                   \"authorized_flag\", \n                   \"installments\", \n                   \"merchant_id\", \n                   \"merchant_group_id\", \n                   \"numerical_1\", \n                   \"numerical_2\", \n                   \"most_recent_sales_range\", \n                   \"most_recent_purchases_range\"\n                  ], inplace = True) \n\ndfTest.drop(columns = [\"merchant_category_id_y\",\n                   \"merchant_category_id_x\", \n                   \"subsector_id_y\", \n                   \"subsector_id_x\", \n                   \"city_id_y\", \n                   \"city_id_x\", \n                   \"state_id_y\", \n                   \"state_id_x\", \n                   \"category_1_y\", \n                   \"category_1_x\", \n                   \"category_2_y\",\n                   \"category_2_x\",\n                   \"category_3\", \n                   \"category_4\", \n                   \"authorized_flag\", \n                   \"installments\", \n                   \"merchant_id\", \n                   \"merchant_group_id\", \n                   \"numerical_1\", \n                   \"numerical_2\", \n                   \"most_recent_sales_range\", \n                   \"most_recent_purchases_range\"\n                  ], inplace = True)  ","dfec9a09":"df.describe()","df0481a8":"dfTest.describe()","71417d32":"def percent_missing(df):\n    data = pd.DataFrame(df)\n    df_cols = list(pd.DataFrame(data))\n    dict_x = {}\n    for i in range(0, len(df_cols)):\n        dict_x.update({df_cols[i]: round(data[df_cols[i]].isnull().mean()*100,2)})\n    \n    return dict_x\n\nmissing = percent_missing(df)\ndf_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\nprint('Percent of missing data')\ndf_miss[0:50]","28313fcc":"# Setup do plot\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nsns.set_color_codes(palette='deep')\n\n# Identificando os valores missing\nmissing = round(df.isnull().mean()*100,2)\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar(color=\"b\")\n\n# Visual presentation\nax.xaxis.grid(False)\nax.set(ylabel=\"Percent of missing values\")\nax.set(xlabel=\"Features\")\nax.set(title=\"Percent missing data by feature\")\nsns.despine(trim=True, left=True)","b1a461ed":"df = df.replace([np.inf, -np.inf], np.nan)\ndf.update(df.fillna(0))","f15d4c09":"dfSample = df.copy()\ndfSample = dfSample.sample(n=100000)\ndfSample.shape","de240c86":"dfSample.head()","8190e2ba":"# Verificando os tipos de dados do dataset\ndfSample.dtypes","6bec3b97":"# Dataset de Treino\ndfSample['first_active_month'] = pd.to_datetime(dfSample['first_active_month'])\ndfSample['active_dayofweek'] = dfSample.first_active_month.apply(lambda dt: dt.dayofweek)\ndfSample['active_year'] = dfSample.first_active_month.apply(lambda dt: dt.year)\ndfSample['active_month'] = dfSample.first_active_month.apply(lambda dt: dt.month)\ndfSample.drop(columns =[\"first_active_month\"], inplace = True) \n\n# Codigo abaixo comentado pois nao funcionou no Kernel (somente na maquina local)\n#dfSample['purchase_date'] = pd.to_datetime(dfSample['purchase_date'])\n#dfSample['purchase_date_day'] = dfSample.purchase_date.apply(lambda dt: dt.day)\n#dfSample['purchase_date_dayofweek'] = dfSample.purchase_date.apply(lambda dt: dt.dayofweek)\n#dfSample['purchase_date_month'] = dfSample.purchase_date.apply(lambda dt: dt.month)\n#dfSample['purchase_date_year'] = dfSample.purchase_date.apply(lambda dt: dt.year)\n#dfSample['purchase_date_hour'] = dfSample.purchase_date.apply(lambda dt: dt.hour)\ndfSample.drop(columns =[\"purchase_date\"], inplace = True)  \n\n# Dataset de Test\ndfTest['first_active_month'] = pd.to_datetime(dfTest['first_active_month'])\ndfTest['active_dayofweek'] = dfTest.first_active_month.apply(lambda dt: dt.dayofweek)\ndfTest['active_year'] = dfTest.first_active_month.apply(lambda dt: dt.year)\ndfTest['active_month'] = dfTest.first_active_month.apply(lambda dt: dt.month)\ndfTest.drop(columns =[\"first_active_month\"], inplace = True) \n\n# Codigo abaixo comentado pois nao funcionou no Kernel (somente na maquina local)\n#dfTest['purchase_date'] = pd.to_datetime(dfTest['purchase_date'])\n#dfTest['purchase_date_day'] = dfTest.purchase_date.apply(lambda dt: dt.day)\n#dfTest['purchase_date_dayofweek'] = dfTest.purchase_date.apply(lambda dt: dt.dayofweek)\n#dfTest['purchase_date_month'] = dfTest.purchase_date.apply(lambda dt: dt.month)\n#dfTest['purchase_date_year'] = dfTest.purchase_date.apply(lambda dt: dt.year)\n#dfTest['purchase_date_hour'] = dfTest.purchase_date.apply(lambda dt: dt.hour)\ndfTest.drop(columns =[\"purchase_date\"], inplace = True)  ","0b567a8a":"def plot_feature_scatter(df1, df2, features):\n    i = 0\n    sns.set_style('whitegrid')\n    plt.figure()\n    fig, ax = plt.subplots(5,4,figsize=(24,24))\n\n    for feature in features:\n        i += 1\n        plt.subplot(5,4,i)\n        plt.scatter(df2[feature], df1['target'], marker='+')\n        plt.xlabel(feature, fontsize=10)\n    plt.show();","72f8f938":"features = ['feature_1', 'feature_2','feature_3','month_lag', 'purchase_amount', \n            'avg_sales_lag3', 'avg_sales_lag6', 'avg_sales_lag12', 'avg_purchases_lag3', 'avg_purchases_lag6',\n            'avg_purchases_lag12','active_months_lag3', 'active_months_lag6', 'active_months_lag12', \n            'active_dayofweek','active_month','active_year'\n           ]\nplot_feature_scatter(dfSample,dfSample, features)","d47a80f5":"from scipy import stats\n\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n\n# Fit a normal distribution\nmu, std = norm.fit(dfSample['target'])\n\n# Verificando a distribuicao de frequencia da variavel TARGET\nsns.distplot(dfSample['target'], color=\"b\", fit = stats.norm);\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"target\")\nax.set(title=\"Target distribution: mu = %.2f,  std = %.2f\" % (mu, std))\nsns.despine(trim=True, left=True)\n\n# Skewness: It is the degree of distortion from the symmetrical bell curve or the normal distribution\n# If the skewness is between -0.5 and 0.5, the data are fairly symmetrical.\n# If the skewness is between -1 and -0.5(negatively skewed) or between 0.5 and 1(positively skewed), the data are moderately skewed.\n# If the skewness is less than -1(negatively skewed) or greater than 1(positively skewed), the data are highly skewed.\n\n# Kurtosis: It is actually the measure of outliers present in the distribution.\n# High kurtosis in a data set is an indicator that data has heavy tails or outliers. \n# Low kurtosis in a data set is an indicator that data has light tails or lack of outliers\n\nax.text(x=1.1, y=1, transform=ax.transAxes, s=\"Skewness: %f\" % dfSample['target'].skew(),\\\n        fontweight='demibold', fontsize=10, verticalalignment='top', horizontalalignment='right',\\\n        backgroundcolor='white', color='xkcd:poo brown')\nax.text(x=1.1, y=0.95, transform=ax.transAxes, s=\"Kurtosis: %f\" % dfSample['target'].kurt(),\\\n        fontweight='demibold', fontsize=10, verticalalignment='top', horizontalalignment='right',\\\n        backgroundcolor='white', color='xkcd:dried blood')\n\nplt.show()","411d2a60":"dfSample[dfSample.columns.drop('target')].corrwith(dfSample.target)","49db0623":"fig = plt.subplots(figsize = (30,30))\nsns.set(font_scale=1.5)\nsns.heatmap(dfSample.corr(),square = True,cbar=True,annot=True,annot_kws={'size': 10})\nplt.show()","7eca0d81":"# Fetch all numeric features\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumeric = []\nfor i in dfSample.columns:\n    if dfSample[i].dtype in numeric_dtypes:\n        numeric.append(i)\n        \n        \n# Create box plots for all numeric features\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(14, 11))\nax.set_xscale(\"log\")\nax = sns.boxplot(data=dfSample[numeric] , orient=\"h\", palette=\"Set1\")\nax.xaxis.grid(False)\nax.set(ylabel=\"Feature names\")\nax.set(xlabel=\"Numeric values\")\nax.set(title=\"Numeric Distribution of Features\")\nsns.despine(trim=True, left=True)","c00aeb7b":"# Setup do plot\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n\n# Verificando a distribuicao\nsns.distplot(dfSample['target'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"Target\")\nax.set(title=\"Target distribution\")\nsns.despine(trim=True, left=True)\nplt.show()","a4ea9ecf":"# Verificando mais de perto a variavel target\ndfSample['target'].describe()","aa7ad027":"# Removendo outliers da variavei target (abaixo de -10 e acima de 10)\ndfSample.drop(dfSample[(dfSample['target'] < -10)].index, inplace=True)\ndfSample.drop(dfSample[(dfSample['target'] > 10)].index, inplace=True)","5cd731fe":"# Realizando uma transformacao logaritma\n# log(1+x) transform\ndfSample[\"target\"] = np.log1p(dfSample[\"target\"])","4caf4fe5":"dfSample = dfSample.replace([np.inf, -np.inf], np.nan)\ndfSample.update(dfSample[\"target\"].fillna(0))","a18d810c":"from scipy import stats\n\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n\n# Fit a normal distribution\nmu, std = norm.fit(dfSample['target'])\n\n# Verificando a distribuicao de frequencia da variavel TARGET\nsns.distplot(dfSample['target'], color=\"b\", fit = stats.norm);\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"target\")\nax.set(title=\"Target distribution: mu = %.2f,  std = %.2f\" % (mu, std))\nsns.despine(trim=True, left=True)\n\nax.text(x=1.1, y=1, transform=ax.transAxes, s=\"Skewness: %f\" % dfSample['target'].skew(),\\\n        fontweight='demibold', fontsize=10, verticalalignment='top', horizontalalignment='right',\\\n        backgroundcolor='white', color='xkcd:poo brown')\nax.text(x=1.1, y=0.95, transform=ax.transAxes, s=\"Kurtosis: %f\" % dfSample['target'].kurt(),\\\n        fontweight='demibold', fontsize=10, verticalalignment='top', horizontalalignment='right',\\\n        backgroundcolor='white', color='xkcd:dried blood')\n\nplt.show()","3cbdd7c9":"# Setup do plot\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n\n# Verificando a distribuicao\nsns.distplot(dfSample['purchase_amount'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"purchase_amount\")\nax.set(title=\"purchase_amount distribution\")\nsns.despine(trim=True, left=True)\nplt.show()","689096ce":"dfSample['purchase_amount'].describe()","1739ee18":"dfSample['purchase_amount'] = dfSample['purchase_amount'].apply(lambda x: 0.01 if x <= 0 else x)\ndfSample['purchase_amount'] = dfSample['purchase_amount'].apply(lambda x: 1 if x > 1 else x)","7499193f":"# Setup do plot\nsns.set_style(\"white\")\nsns.set_color_codes(palette='deep')\nf, ax = plt.subplots(figsize=(8, 7))\n\n# Verificando a distribuicao\nsns.distplot(dfSample['purchase_amount'], color=\"b\");\nax.xaxis.grid(False)\nax.set(ylabel=\"Frequency\")\nax.set(xlabel=\"purchase_amount\")\nax.set(title=\"purchase_amount distribution\")\nsns.despine(trim=True, left=True)\nplt.show()","89882d0e":"def plot_feature_dist(df1, features):\n    i = 0\n    sns.set_style('whitegrid')\n    sns.set_color_codes(palette='deep')\n    plt.figure()\n    fig, ax = plt.subplots(3,2,figsize=(24,12))\n\n    for feature in features:\n        i += 1\n        plt.subplot(3,2,i)\n        sns.distplot(df1[feature], color=\"b\");\n        plt.xlabel(feature, fontsize=10)\n    plt.show();","556d58fb":"features = ['avg_sales_lag3', 'avg_sales_lag6', 'avg_sales_lag12', \n            'avg_purchases_lag3', 'avg_purchases_lag6','avg_purchases_lag12'\n           ]\nplot_feature_dist(dfSample, features)","db43ca4e":"dfSample['avg_sales_lag3'].describe()","ae5eac8e":"dfSample['avg_sales_lag6'].describe()","8222c1c5":"dfSample['avg_sales_lag12'].describe()","ec3394b2":"# Dataset de Treino\ndfSample['var_lag3'] = dfSample['avg_sales_lag3'] * dfSample['avg_purchases_lag3']\ndfSample['var_lag6'] = dfSample['avg_sales_lag6'] * dfSample['avg_purchases_lag6']\ndfSample['var_lag12'] = dfSample['avg_sales_lag12'] * dfSample['avg_purchases_lag12']\n\n# Dataset de Teste\ndfTest['var_lag3'] = dfTest['avg_sales_lag3'] * dfTest['avg_purchases_lag3']\ndfTest['var_lag6'] = dfTest['avg_sales_lag6'] * dfTest['avg_purchases_lag6']\ndfTest['var_lag12'] = dfTest['avg_sales_lag12'] * dfTest['avg_purchases_lag12']\n","dbcf71b0":"# Remover as variaveis originais\ndfSample.drop(columns = ['avg_sales_lag3', 'avg_purchases_lag3',\n                       'avg_sales_lag6', 'avg_purchases_lag6',\n                       'avg_sales_lag12', 'avg_purchases_lag12'\n                  ], inplace = True)\n\ndfTest.drop(columns = ['avg_sales_lag3', 'avg_purchases_lag3',\n                       'avg_sales_lag6', 'avg_purchases_lag6',\n                       'avg_sales_lag12', 'avg_purchases_lag12'\n                  ], inplace = True)\n","8ed37d64":"def logs(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(np.log(1.01+res[l])).values)   \n        res.columns.values[m] = l + '_log'\n        m += 1\n    return res\n\nlog_features = ['var_lag3','var_lag6','var_lag12']\n\ndfSample = logs(dfSample, log_features)\ndfTest = logs(dfTest, log_features)","256b8044":"def squares(res, ls):\n    m = res.shape[1]\n    for l in ls:\n        res = res.assign(newcol=pd.Series(res[l]*res[l]).values)   \n        res.columns.values[m] = l + '_sq'\n        m += 1\n    return res \n\nsquared_features = ['var_lag3','var_lag6','var_lag12']\n\ndfSample = squares(dfSample, squared_features)\ndfTest = squares(dfTest, squared_features)","b44e7ca8":"dfSample[dfSample.columns.drop('target')].corrwith(dfSample.target)","6b26f087":"dfSample.head()","a183d38c":"all_features = dfSample.copy()\nall_features.shape","f155226d":"all_features.head()","0469f50a":"all_features = pd.DataFrame(all_features.groupby( ['card_id'] ).mean().to_dict())","0c01de63":"# Split features and labels\nX = all_features.drop(['target'], axis=1)\ny = all_features['target']\n\n# Aplicando a mesma escala nos dados\nX = MinMaxScaler().fit_transform(X)\n\n# Padronizando os dados (0 para a m\u00e9dia, 1 para o desvio padr\u00e3o)\nX = StandardScaler().fit_transform(X)","418546e7":"X.shape, y.shape, dfTest.shape","770020b9":"# Setup cross validation folds\nkf = KFold(n_splits=2, random_state=123, shuffle=True)","4f7d524c":"# Defini a metrica de validacao (RMSL)\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))\n\ndef cv_rmse(model, X=X):\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv=kf))\n    return (rmse)","d1640e1c":"# Light Gradient Boosting Regressor\nlightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=200,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       #verbose=0,\n                       random_state=123)\n\n# XGBoost Regressor\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=200,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:squarederror',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006,\n                       #verbosity=3,\n                       random_state=123)\n\n# Support Vector Regressor\nsvr = make_pipeline(RobustScaler(), SVR(C= 5, epsilon= 0.008, gamma=0.0003))\n\n# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=200,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                #verbose=True,\n                                random_state=123)  \n\n# Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          #verbose=True,\n                          random_state=123)\n\n# KerasRegressor\ndef baseline_model():\n    # create model\n    model = Sequential()\n    model.add(Dense(13, input_dim=21, kernel_initializer='normal', activation='relu'))\n    model.add(Dense(1, kernel_initializer='normal'))\n    # Compile model\n    model.compile(loss='mean_squared_error', optimizer='adam')\n    return model\n\nkeras = KerasRegressor(build_fn=baseline_model, \n                       epochs=10, \n                       batch_size=5)","2d210e29":"print(datetime.datetime.now().time())","5a6c8831":"scores = {}\n\nscore = cv_rmse(lightgbm)\nprint(\"lightgbm: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['lgb'] = (score.mean(), score.std())","5bdbdc46":"score = cv_rmse(xgboost)\nprint(\"xgboost: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['xgb'] = (score.mean(), score.std())","4d7235ec":"score = cv_rmse(svr)\nprint(\"svr: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['svr'] = (score.mean(), score.std())","f01a1de5":"score = cv_rmse(rf)\nprint(\"rf: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['rf'] = (score.mean(), score.std())","9e048ad9":"score = cv_rmse(gbr)\nprint(\"gbr: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['gbr'] = (score.mean(), score.std())","ab344601":"score = cv_rmse(keras)\nprint(\"keras: {:.4f} ({:.4f})\".format(score.mean(), score.std()))\nscores['kr_norm'] = (score.mean(), score.std())","043d349e":"print('lightgbm')\nlgb_model_full_data = lightgbm.fit(X, y)","a72bc8cf":"print('xgboost')\nxgb_model_full_data = xgboost.fit(X, y)","7cb5b72c":"print('Svr')\nsvr_model_full_data = svr.fit(X, y)","0e90226c":"print('RandomForest')\nrf_model_full_data = rf.fit(X, y)","bcbe8358":"print('GradientBoosting')\ngbr_model_full_data = gbr.fit(X, y)","91ed9cf3":"print('KerasRegressor')\nkeras_model_full_data = keras.fit(X, y)\n","3e0390d6":"# Fazendo as previsoes finais\n# Nao consegui colocar o keras pois ele grava um History (estudando como fazer)\ndef blended_predictions(X):\n    return ((svr_model_full_data.predict(X)) + \\\n            (gbr_model_full_data.predict(X)) + \\\n            (xgb_model_full_data.predict(X)) + \\\n            (lgb_model_full_data.predict(X)) + \\\n            (rf_model_full_data.predict(X)))","ea238061":"# Verificando as predictions dos modelos\nblended_score = rmsle(y, blended_predictions(X))\nscores['blended'] = (blended_score, 0)\nprint('RMSLE score no dataset de Treino:')\nprint(blended_score)","30483d55":"# Plot com a previsao de cada modelo\nsns.set_style(\"white\")\nfig = plt.figure(figsize=(24, 12))\n\nax = sns.pointplot(x=list(scores.keys()), y=[score for score, _ in scores.values()], markers=['o'], linestyles=['-'])\nfor i, score in enumerate(scores.values()):\n    ax.text(i, score[0] , '{:.6f}'.format(score[0]), horizontalalignment='left', size='22', color='black', weight='semibold')\n\nplt.ylabel('Score (RMSE)', size=20, labelpad=12.5)\nplt.xlabel('Model', size=20, labelpad=12.5)\nplt.tick_params(axis='x', labelsize=13.5)\nplt.tick_params(axis='y', labelsize=12.5)\n\nplt.title('Scores of Models', size=20)\n\nplt.show()","33c8ac35":"# Usando o split para separar dados de treino e dados de test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.33, random_state = 10)\npredictions = blended_predictions(X_test)\npredictions","9e24ee4e":"sns.set_style(\"white\")\nfig = plt.figure(figsize=(24, 12))\n\nplt.plot(range(y_test.shape[0]),y_test,label=\"Dados Originais\")\nplt.plot(range(y_test.shape[0]),predictions,label=\"Dados Previstos\")\nplt.legend(loc='best')\nplt.ylabel('target')\nplt.title('Comparacao com dados de teste')\nplt.show()","dd0b6ad0":"print(datetime.datetime.now().time())","4777af74":"sub_final = pd.DataFrame(dfTest.groupby( ['card_id'] ).mean().to_dict())","4b7bcaff":"sub_final = sub_final.replace([np.inf, -np.inf], np.nan)\nsub_final.update(sub_final.fillna(0))","f4a8bd01":"# Aplicando a mesma escala nos dados\nX_final = MinMaxScaler().fit_transform(sub_final)\n\n# Padronizando os dados (0 para a m\u00e9dia, 1 para o desvio padr\u00e3o)\nX_final = StandardScaler().fit_transform(X_final)","3fa29bc7":"X_final.shape","36b87573":"predictions = blended_predictions(X_final)\npredictions","ac0321c9":"#Gerando Arquivo de Submissao\nsubmission = pd.DataFrame({\n    \"card_id\": sub_final.index, \n    \"target\": predictions\n})","ad683965":"submission.head(30)","0f495297":"submission.to_csv('.\/submission_file.csv', index=False)","f9481eed":"### Normaliza\u00e7\u00e3o e Padroniza\u00e7\u00e3o de features numericas","d7452bb7":"## Submit Kaggle","9a37cf31":"Verificando caracteristicas da vari\u00e1vel target","e4829056":"Visualizar um boxplot de todas as variaveis numericas","cc11b474":"Consideracoes sobre esse BoxPlot:\n\n1. as features avg_sales_lagx e avg_purchase_lagx tem muitos outliers (talvez seja necessario trata-los dependendo do modelo preditivo)\n2. tambem \u00e9 necess\u00e1rio verificar a feature 'feature_3'pois exibe um comportamento diferente das demais features","3b7ffd0f":"Vamos agora dar uma olhada da distribuicao de frequencia das features avg_sales_lagx e avg_purchase_lagx, pois apresentaram muitos outliers","1b7ff0b0":"Nesse ponto da pra verificar que juntando os modelos tivemos melhor performance... mas aqui \u00e9 o momento de usar o GridSearch para obter os melhores parametros e otimizar o modelo","7197800c":"Comparacao estatistica entre os dois datasets (treino e teste)","a8a65be1":"A variavel target ficou bem proximo de uma distribuicao normal","d1356771":"## Fit the models","87c15922":"### Visualizando a melhor performance dos modelos","6134d33a":"Vamos substituir os valores INF por NaN e preencher os valores NaN por 0","c27b5dd8":"O valor minimo \u00e9 -33.219281 e o valor maximo \u00e9 de 15.994455, gerando um desvio padr\u00e3o muito alto em rela\u00e7\u00e3o a media\nNeste primeiro momento vou remover estes outiliers diretamente do dataset, para fazer alguns testes","388178e0":"Vamos agora fazer previsoes nos dados de teste e visualizar no plot","f8d3b405":"## Feature Engineering","c79b07f1":"Verificando a distribuicao de outras variaveis com outliers detectado na analise exploratoria","9f11e146":"Realmente tem que verificar como proceder nessas variaveis, que acredito que podem ajudar na acuracia do modelo.\nVamos criar algumas variaveis multiplicando a media de venda pela media de compra","257484ad":"O dataset dfTest tem uma coluna a mais por conta do CARD_ID","36b73320":"# Competi\u00e7\u00e3o DSA de Machine Learning - Edi\u00e7\u00e3o Junho\/2019\nImagine estar com fome em uma parte desconhecida da cidade e receber recomenda\u00e7\u00f5es de restaurantes, com base em suas prefer\u00eancias pessoais, no momento certo. A recomenda\u00e7\u00e3o vem com um desconto em anexo da sua operadora de cart\u00e3o de cr\u00e9dito para um local ao virar a esquina!\n\nUma Startup pensou nisso e construiu parcerias com comerciantes para oferecer promo\u00e7\u00f5es ou descontos aos portadores de cart\u00f5es de cr\u00e9dito. Mas essas promo\u00e7\u00f5es funcionam tanto para o consumidor quanto para o comerciante? Os clientes aproveitam a experi\u00eancia? Os comerciantes veem resultado? A personaliza\u00e7\u00e3o \u00e9 fundamental.\n\nOs profissionais da Startup constru\u00edram modelos de aprendizado de m\u00e1quina para entender os aspectos e prefer\u00eancias mais importantes no ciclo de vida de seus clientes, desde alimentos a compras. Mas at\u00e9 agora nenhum deles \u00e9 especificamente adaptado para um indiv\u00edduo ou perfil. \u00c9 aqui que voc\u00ea entra. Precisando de um modelo preditivo mais robusto, a Startup selecionou voc\u00ea como Cientista de Dados.\n\n\n## The Goal\n \n- Os arquivos dataset_treino.csv e dataset_teste.csv cont\u00eam card_ids e informa\u00e7\u00f5es sobre o pr\u00f3prio cart\u00e3o - o primeiro m\u00eas em que o cart\u00e3o estava ativo, etc.\n- Objetivo \u00e9 prever um \u00edndice de lealdade para cada card_id\n- O modelo \u00e9 avalidado pelo Root-Mean-Squared-Error (RMSE)\n\n## Key features of the model training process in this kernel:\n- **Cross Validation:** Using 2-fold cross-validation (para testar mais rapido)\n- **Models:** svr, gradient boosting, random forest, xgboost, lightgbm e keras regressors\n- **Blending:** Para ter as previsoes finais eu juntei os modelos para obter uma performance melhor","5df3a8d2":"A partir desses graficos com features relacionadas a variavel target \u00e9 poss\u00edvel perceber os outliers. Entre -20 e -30 nao tem valores... mas -33 tem alguns","c0e5b991":"Feature 'purchase_amount' tem um desvio padrao muito alto. Para valores abaixo de zero, vamos atribuir 0.01 e para valores acima de 1 vamos atribuir 1 (penso que \u00e9 o montante de compra, estranho estar negativo)","49fc87df":"## An\u00e1lise Explorat\u00f3ria de Dados","23ea15ef":"Depois da transformacao, varios valores inf apareceram, por isso vamos preencher com zero","2cbe4d5a":"Vamos fazer algumas transformacoes nas features criadas (venda x compra) usando _log e _square","5d8a67a2":"Verificando valores missing","1b03f7f4":"## Extraindo e Carregando os Dados","ed9fcd36":"## Cria\u00e7\u00e3o e Valida\u00e7\u00e3o dos Modelos de Machine Learning","cb80696b":"### Blend models and get predictions","849be695":"## Preparando os Dados para Machine Learning","5c9ccb76":"Nesse ponto vamos criar mais uma copia, agora do dataset dfSample, para facilitar o retorno ate aqui","b4e99b63":"Algumas observacoes daqui:\n\n1. feature_1 e feature_3 tem um forte relacionamento positivo\n2. avg_sales_lag3 e avg_purchases_lag3 tem um relacionamento praticamente 1 (talvez seja necessario retirar uma dessas variaveis ou juntar as duas por uma multiplicacao, totalizando vendas x compras)\n3. o mesmo ocorre com as outras variaveis _lag6 e _lag12","f70a1aa5":"Poucas variaveis se correlacionam fortemente com a target","610e7e23":"Visualizando novamente a distribui\u00e7\u00e3o da variavel Target","5e4192a9":"Plot de como as features se correlacionam com cada uma e com a variavel target","cdc62dd0":"Percebe-se que as features criadas ficaram com baixa correlacao com a variavel target. Mas vamos verificar como esta o modelo, qualquer coisa voltamos aqui","0a241081":"Slipt de algumas features do dataset de treino e teste","33b7cc5e":"## Feature transformations","ba0fcf84":"Removendo algumas colunas do modelo repetidas e outras que nao afetaram o resultado (pelo menos ate agora)\nPode ser que criando novas features com essas colunas seja interessante para melhorar a performance do modelo","314b0415":"Esse \u00e9 um ponto a verificar, pois foi a unica forma que encontrei para agrupar os registros em um do cardId. Talvez seja necessario outra estrategia, mas nao encontrei ate o momento. Usei a media de cada variavel para o agrupamento","215d865e":"Vamos plotar um scatter plot para verificar os dados de treino. Vamos visualizar 5% dos dados. Na eixo x vamos colocar as features e no eixo y a variavel target.","f687f81d":"Coloquei somente 2 splits por causa do tempo, para testar mais rapido","a50ec56b":"Vamos fazer uma copia do dataset de Treino para facilitar e acelerar algumas analises e acuracia do modelo. A principio usarei uma amostra de 100.000 registros","aa8744ff":"Algumas observacoes daqui:\n\n1. alguns valores infinito e NaN nas colunas avg_purchases_lag3, avg_purchases_lag6 e avg_purchases_lag12 em ambos os dataset (treino e teste)\n2. a media dos dados estao relativamente proximos entre os dataset de treino e teste\n3. o valor medio parece estar relativamente em um range pequeno\n4. a feature numerical_1 e numerical_2 sao muito parecidas, pouca variacao\n5. a media de valores da coluna avg_sales_lag6 entre os dois modelos \u00e9 bem diferente (vale a pena verificar se impacta na performance do modelo)","3faf0e1e":"Vale a pena realizar uma transformacao logaritma, para auxiliar o modelo","b8e5d42b":"Como verificado na analise exploratoria, temos alguns outliers que serao tratados","c295fa8e":"Nessa parte adicionei varios modelos para comparacao. A principio utilizei alguns parametros default."}}