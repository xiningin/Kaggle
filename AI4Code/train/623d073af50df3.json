{"cell_type":{"16d06f3c":"code","06f6684b":"code","8f50aaf3":"code","48751319":"code","8e1bfe18":"code","addc5f8d":"code","ed34759c":"code","9b00e155":"code","194647a6":"code","44cc7f7e":"code","7cb4dfbf":"code","0ba894c0":"code","e4f890b1":"code","0a78fcf5":"code","21bd98fc":"code","aed10293":"code","1f3159fb":"code","fe6118fb":"code","67b2994f":"code","8ca7f707":"code","14699d2b":"code","29685454":"code","7123b21b":"code","ec68c5e6":"code","bbd5be96":"code","7510050b":"code","8c1e5b30":"code","c15b8fc6":"code","e60734a9":"code","a44da477":"code","42d291f8":"code","514b33de":"code","119119d1":"code","d306102f":"code","b2741c98":"code","fcaa3624":"code","2db64e0a":"code","3709e66c":"code","8bda65d1":"code","81a8cf8c":"code","59a4e459":"code","c1d11640":"code","0d307887":"code","45bd2b17":"markdown","3d6297d5":"markdown","21a8b87a":"markdown","45890719":"markdown","193178f8":"markdown","0e30c953":"markdown","1605890d":"markdown","4c5db129":"markdown","39c9f2f4":"markdown","3a14b0ba":"markdown","84cdc91f":"markdown","ac417322":"markdown","faf1a28a":"markdown","84f25ab8":"markdown","c44b1f38":"markdown","427f5ed8":"markdown","1e114966":"markdown","2ff75972":"markdown","25c1d495":"markdown"},"source":{"16d06f3c":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","06f6684b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.metrics import classification_report, confusion_matrix\n\n# data visualisation and manipulation\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom matplotlib import style\nimport seaborn as sns\n\n#import the necessary modelling algos.\nfrom sklearn.decomposition import PCA\n\n#classifiaction.\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import LinearSVC,SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier,GradientBoostingClassifier,AdaBoostClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\n\n#model selection\nfrom sklearn.model_selection import train_test_split,cross_validate\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\n\n#preprocessing\nfrom sklearn.preprocessing import StandardScaler\n\n#evaluation metrics\nfrom sklearn.metrics import mean_squared_log_error,mean_squared_error, r2_score,mean_absolute_error # for regression\nfrom sklearn.metrics import accuracy_score,precision_score,recall_score,f1_score  # for classification","8f50aaf3":"data = pd.read_csv(\"\/kaggle\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv\")","48751319":"data['quality'].unique()","8e1bfe18":"sp = data['quality'].value_counts()\nsp = pd.DataFrame(sp)\nsp.T","addc5f8d":"sns.barplot(x = sp.index, y=sp['quality'])\nplt.xlabel(\"Quality Score\")\nplt.ylabel(\"Count\")","ed34759c":"plt.figure(figsize = (16,7))\nsns.set(font_scale=1.2)\nsns.heatmap(data.corr(), annot=True, linewidths=0.5, cmap='YlGnBu')","9b00e155":"data_cleaned = data","194647a6":"data_cleaned.isnull().sum()","44cc7f7e":"data_cleaned.describe()","7cb4dfbf":"data_try  = data_cleaned","0ba894c0":"bins = (2, 6.5, 8)\ngroup_names = ['bad', 'good']\ndata_try['category'] = pd.cut(data_try['quality'], bins = bins, labels = group_names)","e4f890b1":"data_try['category'].value_counts()","0a78fcf5":"from sklearn.model_selection import train_test_split","21bd98fc":"data_cleaned.columns","aed10293":"x1 = data_try[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n       'pH', 'sulphates', 'alcohol']]\n\ny1 = data_try['category']","1f3159fb":"x_train_dummy,x_test_dummy,y_train_dummy,y_test_dummy = train_test_split(x1,y1,test_size = 0.3, random_state=42)","fe6118fb":"y_test_dummy.count()","67b2994f":"y_dummy_predict = []\ny_dummy_predict = ['bad']*y_test_dummy.count()","8ca7f707":"accuracy_score(y_dummy_predict,y_test_dummy)","14699d2b":"print(classification_report(y_test_dummy, y_dummy_predict))","29685454":"data_cleaned.info()","7123b21b":"quality =  data_cleaned['quality'].values\n\ncategory_balanced = []\n\nfor num in quality:\n    if num<=5:\n        category_balanced.append('bad')\n    elif num>=6:\n        category_balanced.append('good')  ","ec68c5e6":"category_balanced  = pd.DataFrame(data=category_balanced,columns=['category_balanced'])","bbd5be96":"category_balanced.isnull().sum()","7510050b":"data_cleaned = pd.concat([data_cleaned,category_balanced],axis=1)","8c1e5b30":"data_cleaned = data_cleaned.dropna()","c15b8fc6":"data_cleaned['category_balanced'].value_counts()","e60734a9":"x = data_cleaned[['fixed acidity', 'volatile acidity', 'citric acid', 'residual sugar',\n       'chlorides', 'free sulfur dioxide', 'total sulfur dioxide', 'density',\n       'pH', 'sulphates', 'alcohol']]\ny = data_cleaned['category_balanced']","a44da477":"scl = StandardScaler()","42d291f8":"x = scl.fit_transform(x)","514b33de":"pca = PCA()","119119d1":"x_pca = pca.fit_transform(x)","d306102f":"plt.figure(figsize=(10,10))\nplt.plot(np.cumsum(pca.explained_variance_ratio_), 'ro-')","b2741c98":"pca_new = PCA(n_components=8)","fcaa3624":"x_pca_8 = pca_new.fit_transform(x)","2db64e0a":"x_train,x_test,y_train,y_test = train_test_split(x,y,test_size = 0.3, random_state=420)","3709e66c":"x_train.shape","8bda65d1":"x_test.shape","81a8cf8c":"y_train.shape","59a4e459":"y_test.shape","c1d11640":"models=[LogisticRegression(),SVC(),SVC(kernel='rbf'),KNeighborsClassifier(),RandomForestClassifier(),\n        DecisionTreeClassifier(),GradientBoostingClassifier()]\nmodel_names=['LogisticRegression','LinearSVM','rbfSVM','KNearestNeighbors','RandomForestClassifier','DecisionTree',\n             'GradientBoostingClassifier','GaussianNB']\n\nacc=[]\nd={}\n\nfor model in range(len(models)):\n    clf=models[model]\n    clf.fit(x_train,y_train)\n    pred=clf.predict(x_test)\n    acc.append(accuracy_score(pred,y_test))\n     \nd={'Modelling Algo':model_names,'Accuracy':acc}\nd","0d307887":"print(classification_report(y_test,y_dummy_predict ))","45bd2b17":"##  What we will try do instead is consider wines above 5 as 'good' quality wines, and below it as 'bad' quality wines this would lead to somewhat equal distribution and even if we want a extremely good quality wine we would have reduced our options to taste test by a little less than 50%.","3d6297d5":"###  Let's now have a look at what outcome would a guess work have come up with :","21a8b87a":"### Lets perform PCA and see what we get","45890719":"### We are scaling the data as we do not know the units for each field :","193178f8":"## As the kernel suggests and like many people have done as well, we divide the quality in two bins.","0e30c953":"#  Creating a dummy prediction of all bad quality:\n\n\nThis is being done in light to get a becnchmark of how good should our model be, I am creating  a dummy prediction in which I have predicted all qualities as bad, as the kernel decscription has told us to. ( Like bin(2,6.5,8)).","1605890d":"##   Lets See an interesting result :","4c5db129":"##  So we that, it would have only got an accuracy of 20% and now we have something close to 80% with hyperparameter tuning we can get even better result bit I will leave this kernel at this point, Please fell fee to do hyperparameter tuning and other adjustments like dropping corelated columns and share the result with me.","39c9f2f4":"#1) Use the PCA, and interpret it according to what variables load out on it\n#2) Choose one of the highly correlated variables as identified as those that all load onto the same variable and analyse only it.","3a14b0ba":"##  So we See the problem with most kernels here they achieve an accuracy of 80-88% and are just slightly better or worse than our guess. This is due to the imbalance in data, even if the prediction goes upto 95% its not doing a whole lot of good","84cdc91f":"# Challenge :\n\nI have tried removing outliers and it slightly degrades my results, if anyone can remove the outliers and get a better result let me know. \n\nFind me on linkedin - @justsuyash","ac417322":"### So we see that 8 features explain about 99% of the variablity so we will use 8 features","faf1a28a":"## But we are not dropping them as we will be doing PCA  and If N variables are highly correlated than they will all load out on the SAME Principal Component (Eigenvector) and hence we do not need to remove them","84f25ab8":"## We see that there are a lot of corelated variables like :","c44b1f38":"## Wine Quality Prediction: ( Challenge at the end )\n\n### Date : 14-02-2020\n### @justsuyash (linkedin)","427f5ed8":"#1) 'fixed acidity' is corealted to 'citric acid' \n#2) 'free sulphur' and 'total sulphur dioxide' are corealted\n#3) 'ph' and 'acidity' are negatively corelated(obviously)","1e114966":"For more information, read [Cortez et al., 2009].\nInput variables (based on physicochemical tests):\n1 - fixed acidity\n2 - volatile acidity\n3 - citric acid\n4 - residual sugar\n5 - chlorides\n6 - free sulfur dioxide\n7 - total sulfur dioxide\n8 - density\n9 - pH\n10 - sulphates\n11 - alcohol\nOutput variable (based on sensory data):\n12 - quality (score between 0 and 10)","2ff75972":"### We are looking at a highly imbalanced dataset and hence lets make a guess that all wines are bad and lets see the results","25c1d495":"Think like a Data Scientist and So you become!"}}