{"cell_type":{"ccf908b7":"code","e6f535d4":"code","8a3c8144":"code","77fbef25":"code","5ea9faed":"code","288b140a":"code","33dc1406":"code","5e96d9be":"code","1e328660":"code","74c832fa":"code","01043353":"code","8d887cdc":"code","952ac73a":"code","3fe1a0ca":"code","f3194fab":"code","8ac4d6e9":"code","4cd868a8":"code","66b78a13":"code","b38eddcc":"code","dc3d6abc":"code","a07ae42a":"code","6ed759ad":"code","756988bf":"code","84e7cd3f":"code","539196c3":"code","aceb3652":"code","5a6e6711":"code","e7a85956":"code","3bdf0de6":"code","e58a060c":"markdown","02fc23e7":"markdown","60f38b1a":"markdown","dc96be77":"markdown","8c4a512c":"markdown","df3cf92e":"markdown","d189c47e":"markdown","631ca4e8":"markdown","1b2fa117":"markdown","f4c6d60e":"markdown","6c415093":"markdown","2434c512":"markdown","d1223c22":"markdown","6c1ac4d4":"markdown","c29a0977":"markdown","76fdfc9a":"markdown","570d9ca3":"markdown","02bc22a6":"markdown"},"source":{"ccf908b7":"import numpy as np \nimport pandas as pd\npd.set_option('display.max_columns', None)\n\nimport re\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport matplotlib.pyplot as plt\nplt.rcParams.update({'font.size': 14})\n%matplotlib inline\n\nimport seaborn as sns\n\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom plotly.subplots import make_subplots\n","e6f535d4":"districts = pd.read_csv('..\/input\/learnplatform-covid19-impact-on-digital-learning\/districts_info.csv')\nproducts = pd.read_csv('..\/input\/learnplatform-covid19-impact-on-digital-learning\/products_info.csv')","8a3c8144":"districts.head()","77fbef25":"districts.info()","5ea9faed":"# Getting percentage of null values in the dataset\nprint(f'The district data has {round(sum(districts.isna().sum()) \/ (districts.shape[0] * districts.shape[1]) * 100, 2)} % missing values')","288b140a":"#Dropping Districts with NaN States\ndistricts.dropna(subset=['state'],inplace = True)","33dc1406":"products.head()","5e96d9be":"products.info()","1e328660":"temp_sectors = products['Sector(s)'].str.get_dummies(sep=\"; \")\ntemp_sectors.columns = [f\"sector_{re.sub(' ', '', c)}\" for c in temp_sectors.columns]\nproducts = products.join(temp_sectors)\nproducts.drop(\"Sector(s)\", axis=1, inplace=True)\n\ndel temp_sectors","74c832fa":"products['primary_function_main'] = products['Primary Essential Function'].apply(lambda x: x.split(' - ')[0] if x == x else x)\nproducts['primary_function_sub'] = products['Primary Essential Function'].apply(lambda x: x.split(' - ')[1] if x == x else x)\n\n# Synchronize similar values\nproducts['primary_function_sub'] = products['primary_function_sub'].replace({'Sites, Resources & References' : 'Sites, Resources & Reference'})\nproducts.drop(\"Primary Essential Function\", axis=1, inplace=True)","01043353":"path =  '..\/input\/learnplatform-covid19-impact-on-digital-learning\/engagement_data'\n\nfiles = []\n\nfor district in districts.district_id.unique():\n    df = pd.read_csv(f'{path}\/{district}.csv', index_col=None, header=0)\n    df[\"district_id\"] = district\n    files.append(df)\n    \n# To merge each districts engagement data into one dataframe\nengagement = pd.concat(files, ignore_index=True)\n\ndel files\n# To convert time column type to datetime\nengagement['time'] = pd.to_datetime(engagement['time'])\nengagement.head()","8d887cdc":"fig, ax = plt.subplots(1, 1, figsize=(8,4))\n\nsns.histplot(engagement.groupby('district_id').time.nunique(), bins=30)\nax.set_title('Unique Days of Engagement Data per District')\nplt.show()","952ac73a":"# Delete previously created engagement dataframe and create a new one\ndel engagement\n\nfiles = []\n\nfor district in districts.district_id.unique():\n    df = pd.read_csv(f'{path}\/{district}.csv', index_col=None, header=0)\n    df[\"district_id\"] = district\n    if df.time.nunique() == 366:\n        files.append(df)\n\nengagement = pd.concat(files, ignore_index=True)\n\n# Only consider districts with full 2020 engagement data\ndistricts = districts[districts.district_id.isin(engagement.district_id.unique())].reset_index(drop=True)\nproducts = products[products['LP ID'].isin(engagement.lp_id.unique())].reset_index(drop=True)","3fe1a0ca":"for district in districts.district_id.unique()[:10]:\n    df = pd.read_csv(f'{path}\/{district}.csv', index_col=None, header=0)\n    print(f'District {district} uses {df.lp_id.nunique()} unique products.')\n    \nprint(f'\\nConcatenated engagement data contains {engagement.lp_id.nunique()} unique products.')","f3194fab":"print(len(engagement))\nengagement = engagement[engagement.lp_id.isin(products['LP ID'].unique())]\nprint(len(engagement))","8ac4d6e9":"engagement.time = engagement.time.astype('datetime64[ns]')","4cd868a8":"us_state_abbrev = {\n    'Alabama': 'AL',\n    'Alaska': 'AK',\n    'American Samoa': 'AS',\n    'Arizona': 'AZ',\n    'Arkansas': 'AR',\n    'California': 'CA',\n    'Colorado': 'CO',\n    'Connecticut': 'CT',\n    'Delaware': 'DE',\n    'District Of Columbia': 'DC',\n    'Florida': 'FL',\n    'Georgia': 'GA',\n    'Guam': 'GU',\n    'Hawaii': 'HI',\n    'Idaho': 'ID',\n    'Illinois': 'IL',\n    'Indiana': 'IN',\n    'Iowa': 'IA',\n    'Kansas': 'KS',\n    'Kentucky': 'KY',\n    'Louisiana': 'LA',\n    'Maine': 'ME',\n    'Maryland': 'MD',\n    'Massachusetts': 'MA',\n    'Michigan': 'MI',\n    'Minnesota': 'MN',\n    'Mississippi': 'MS',\n    'Missouri': 'MO',\n    'Montana': 'MT',\n    'Nebraska': 'NE',\n    'Nevada': 'NV',\n    'New Hampshire': 'NH',\n    'New Jersey': 'NJ',\n    'New Mexico': 'NM',\n    'New York': 'NY',\n    'North Carolina': 'NC',\n    'North Dakota': 'ND',\n    'Northern Mariana Islands':'MP',\n    'Ohio': 'OH',\n    'Oklahoma': 'OK',\n    'Oregon': 'OR',\n    'Pennsylvania': 'PA',\n    'Puerto Rico': 'PR',\n    'Rhode Island': 'RI',\n    'South Carolina': 'SC',\n    'South Dakota': 'SD',\n    'Tennessee': 'TN',\n    'Texas': 'TX',\n    'Utah': 'UT',\n    'Vermont': 'VT',\n    'Virgin Islands': 'VI',\n    'Virginia': 'VA',\n    'Washington': 'WA',\n    'West Virginia': 'WV',\n    'Wisconsin': 'WI',\n    'Wyoming': 'WY'\n}\n\ndistricts['state_abbrev'] = districts['state'].replace(us_state_abbrev)\ndistricts_by_state = districts['state_abbrev'].value_counts().to_frame().reset_index(drop=False)\ndistricts_by_state.columns = ['state_abbrev', 'num_districts']\n\nfig = go.Figure()\nlayout = dict(\n    title_text = \"Number of Available School Districts per State\",\n    geo_scope='usa',\n)\n\nfig.add_trace(\n    go.Choropleth(\n        locations=districts_by_state.state_abbrev,\n        zmax=1,\n        z = districts_by_state.num_districts,\n        locationmode = 'USA-states', # set of locations match entries in `locations`\n        marker_line_color='white',\n        geo='geo',\n        colorscale=px.colors.sequential.Teal, \n    )\n)\n            \nfig.update_layout(layout)   \nfig.show()\n\nplt.figure(figsize = (15, 8))\nsns.set_style(\"white\")\na = sns.barplot(data = districts['state'].value_counts().reset_index(), x = 'state', y = 'index', color = '#90afc5')\nplt.xticks([])\nplt.yticks(fontname = 'monospace', fontsize = 14, color = '#283655')\nplt.ylabel('')\nplt.xlabel('')\n\na.spines['left'].set_linewidth(1.5)\nfor w in ['right', 'top', 'bottom']:\n    a.spines[w].set_visible(False)\n    \nfor p in a.patches:\n    width = p.get_width()\n    plt.text(0.5 + width, p.get_y() + 0.55 * p.get_height(), f'{int(width)}',\n             ha = 'center', va = 'center', fontname = 'monospace', fontsize = 15, color = '#283655')\n\nplt.show()","66b78a13":"fig = px.pie(districts['locale'].value_counts().reset_index().rename(columns = {'locale': 'count'}), values = 'count', names = 'index', width = 700, height = 700)\n\nfig.update_traces(textposition = 'inside', \n                  textinfo = 'percent + label', \n                  hole = 0.7, \n                  marker = dict(colors = ['#90afc5','#336b87','#2a3132','#763626'], line = dict(color = 'white', width = 2)))\n\nfig.update_layout(annotations = [dict(text = ' The count of districts <br>in each type <br>of areas', \n                                      x = 0.5, y = 0.5, font_size = 26, showarrow = False, \n                                      font_family = 'monospace',\n                                      font_color = '#283655')],\n                  showlegend = False)\n                  \nfig.show()","b38eddcc":"districts.pp_total_raw.unique()\ntemp = districts.groupby('locale').pp_total_raw.value_counts().to_frame()\ntemp.columns = ['amount']\n\ntemp = temp.reset_index(drop=False)\n\ntemp = temp.pivot(index='locale', columns='pp_total_raw')['amount']\ntemp = temp[['[4000, 6000[', '[6000, 8000[', '[8000, 10000[', '[10000, 12000[',\n       '[12000, 14000[', '[14000, 16000[', '[16000, 18000[', \n       '[18000, 20000[', '[20000, 22000[', '[22000, 24000[', ]]\n\n\nfig, ax = plt.subplots(1, 2, figsize=(24,4))\n\nsns.countplot(data=districts, x='locale', ax=ax[0], palette='GnBu')\nax[0].set_title('Locale Distribution in Districts Data')\nsns.heatmap(temp, annot=True,  cmap='GnBu', ax=ax[1])\nax[1].set_title('Heatmap of Districts According To locale and pp_total_raw')\nplt.show()","dc3d6abc":"fig, ax = plt.subplots(2, 2, figsize=(16,8))\n\nsns.countplot(data=districts, x='pct_black\/hispanic', order=['[0, 0.2[', '[0.2, 0.4[', '[0.4, 0.6[', '[0.6, 0.8[','[0.8, 1[', ], palette='GnBu', ax=ax[0,0])\nax[0,0].set_ylim([0,135])\nsns.countplot(data=districts, x='pct_free\/reduced', order=['[0, 0.2[', '[0.2, 0.4[', '[0.4, 0.6[', '[0.6, 0.8[','[0.8, 1[', ], palette='GnBu', ax=ax[0,1])\nax[0,1].set_ylim([0,135])\n\nsns.countplot(data=districts, x='county_connections_ratio', palette='GnBu', ax=ax[1,0])\nax[1,0].set_ylim([0,135])\nsns.countplot(data=districts, x='pp_total_raw', order=['[4000, 6000[', '[6000, 8000[', '[8000, 10000[', '[10000, 12000[',\n       '[12000, 14000[', '[14000, 16000[', '[16000, 18000[', \n       '[18000, 20000[', '[20000, 22000[', '[22000, 24000[', ], palette='GnBu', ax=ax[1,1])\nax[1,1].set_ylim([0,135])\nax[1,1].set_xticklabels(ax[1,1].get_xticklabels(), rotation=90)\n\nplt.tight_layout()\nplt.show()","a07ae42a":"districts['pct_free\/reduced'] = districts[['pct_free\/reduced']].applymap(lambda x:float(x.split(',')[0][1:]) + 0.1, na_action='ignore')\n\ndistricts['pct_black\/hispanic'] = districts['pct_black\/hispanic'].apply(lambda x:float(x.split(',')[0][1:]) + 0.1)\n\n\ndistricts.head(3)","6ed759ad":"districts['pp_total_raw'] = districts[['pp_total_raw']].applymap(lambda x: int(x.split(',')[0][1:]) + 1000,\n                                                                 na_action='ignore')\n\ndistricts.drop('county_connections_ratio', axis = 1, inplace = True)","756988bf":"districts.head(3)","84e7cd3f":"def plot_state_mean_for_var(col):\n    temp = districts.groupby('state_abbrev')[col].mean().to_frame().reset_index(drop=False)\n\n    fig = go.Figure()\n    layout = dict(\n        title_text = f\"Mean {col} per State\",\n        geo_scope='usa',\n    )\n\n    fig.add_trace(\n        go.Choropleth(\n            locations=temp.state_abbrev,\n            zmax=1,\n            z = temp[col],\n            locationmode = 'USA-states', # set of locations match entries in `locations`\n            marker_line_color='white',\n            geo='geo',\n            colorscale=px.colors.sequential.Teal, \n        )\n    )\n\n    fig.update_layout(layout)   \n    fig.show()\n\nplot_state_mean_for_var('pct_black\/hispanic')\nplot_state_mean_for_var('pct_free\/reduced')\nplot_state_mean_for_var('pp_total_raw')\n","539196c3":"fig, ax = plt.subplots(1, 2, figsize=(16,4))\nsns.countplot(data=products, x='primary_function_main', palette ='GnBu', ax=ax[0])\nax[0].set_title('Main Categories in Primary Functions')\n\nsns.countplot(data=products[products.primary_function_main == 'LC'], x='primary_function_sub', palette ='GnBu', ax=ax[1])\nax[1].set_title('Sub-Categories in Primary Function LC')\nax[1].set_xticklabels(ax[1].get_xticklabels(), rotation=90)\nplt.show()","aceb3652":"virtual_classroom_lp_id = products[products.primary_function_sub == 'Virtual Classroom']['LP ID'].unique()\n\n# Remove weekends from the dataframe\nengagement['weekday'] = pd.DatetimeIndex(engagement['time']).weekday\nengagement_without_weekends = engagement[engagement.weekday < 5]\n\n# Figure 1\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(24, 6))\nfor virtual_classroom_product in virtual_classroom_lp_id:\n    temp = engagement_without_weekends[engagement_without_weekends.lp_id == virtual_classroom_product].groupby('time').pct_access.mean().to_frame().reset_index(drop=False)\n    sns.lineplot(x=temp.time, y=temp.pct_access, label=products[products['LP ID'] == virtual_classroom_product]['Product Name'].values[0])\nplt.legend()\nplt.show()\n\n# Figure 2\nf, ax = plt.subplots(nrows=1, ncols=1, figsize=(24, 6))\nfor virtual_classroom_product in virtual_classroom_lp_id:\n    temp = engagement_without_weekends[engagement_without_weekends.lp_id == virtual_classroom_product].groupby('time').engagement_index.mean().to_frame().reset_index(drop=False)\n    sns.lineplot(x=temp.time, y=temp.engagement_index, label=products[products['LP ID'] == virtual_classroom_product]['Product Name'].values[0])\nplt.title('')\nplt.legend()\nplt.show()","5a6e6711":"products['lp_id'] = products['LP ID'].copy()\n\nf, ax = plt.subplots(nrows=3, ncols=3, figsize=(18, 8))\n\ni = 0\nj = 0\nfor subfunction in products[products.primary_function_main == 'LC'].primary_function_sub.unique():\n    lp_ids = products[products.primary_function_sub == subfunction]['LP ID'].unique()\n\n    temp = engagement_without_weekends[engagement_without_weekends.lp_id.isin(lp_ids)]\n    temp = temp.groupby('lp_id').pct_access.mean().sort_values(ascending=False).to_frame().reset_index(drop=False)\n    temp = temp.merge(products[['lp_id', 'Product Name']], on='lp_id').head()\n    \n    sns.barplot(data=temp, x='pct_access', y='Product Name', palette='GnBu', ax=ax[i, j])\n    \n    ax[i, j].set_title(f'Top 5 in \\n{subfunction}', fontsize=12)\n    ax[i, j].set_xlim([0, 20])\n    j = j + 1\n    if j == 3:\n        i = i + 1\n        j = 0\n        \nf.delaxes(ax[2, 1])\nf.delaxes(ax[2, 2])\n\nplt.tight_layout()\nplt.show()","e7a85956":"engagement['quarter'] = pd.DatetimeIndex(engagement['time']).quarter.astype(str)\n\ntemp = engagement.merge(products[['lp_id', 'Product Name', 'primary_function_main', 'primary_function_sub']], on='lp_id')\ntemp = temp[temp.primary_function_sub.notna()]\ntemp = temp.groupby(['quarter', 'primary_function_sub'])['pct_access', 'engagement_index'].mean().reset_index(drop=False)\n\ntemp = temp.pivot(index='primary_function_sub', columns='quarter')[['pct_access', 'engagement_index']].fillna(0)\n\ntemp.columns = [\"_\".join(a) for a in temp.columns.to_flat_index()]\n\ntemp['pct_access_delta'] = temp['pct_access_4'] - temp['pct_access_1']\ntemp['engagement_index_delta'] = temp['engagement_index_4'] - temp['engagement_index_1']\ntemp=temp.reset_index(drop=False)\n#temp = temp.merge(products_info[['lp_id', 'Product Name', 'primary_function_sub']], on='lp_id')\n\nf, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n\ndf = temp.sort_values(by='pct_access_delta', ascending=False)#.head(5)\n\nsns.barplot(data=df, x='pct_access_delta', y='primary_function_sub', palette='GnBu', ax=ax[0])\n\ndf = temp.sort_values(by='engagement_index_delta', ascending=False)#.head(5)\n\nsns.barplot(data=df, x='engagement_index_delta', y='primary_function_sub', palette='GnBu', ax=ax[1])\nplt.tight_layout()\nplt.show()","3bdf0de6":"temp = engagement.fillna(0).groupby(['quarter', 'lp_id'])['pct_access', 'engagement_index'].mean().reset_index(drop=False)\n\ntemp = temp.pivot(index='lp_id', columns='quarter')[['pct_access', 'engagement_index']].fillna(0)\n\ntemp.columns = [\"_\".join(a) for a in temp.columns.to_flat_index()]\n\ntemp['pct_access_delta'] = temp['pct_access_4'] - temp['pct_access_1']\ntemp['engagement_index_delta'] = temp['engagement_index_4'] - temp['engagement_index_1']\ntemp = temp.merge(products[['lp_id', 'Product Name', 'primary_function_sub']], on='lp_id')\n\nf, ax = plt.subplots(nrows=1, ncols=2, figsize=(14, 5))\n\ndf = temp.sort_values(by='pct_access_delta', ascending=False).head(10)\n\nsns.barplot(data=df, x='pct_access_delta', y='Product Name', palette='GnBu', ax=ax[0])\n\ndf = temp.sort_values(by='engagement_index_delta', ascending=False).head(10)\n\nsns.barplot(data=df, x='engagement_index_delta', y='Product Name', palette='GnBu', ax=ax[1])\nplt.tight_layout()\nplt.show()","e58a060c":"**PRODUCTS**","02fc23e7":"The districts info shows some null values","60f38b1a":"The plots show the column county_connection_ratio is the same throughout the dataset, so it will be dropped","dc96be77":"# Exploratory Data Analysis","8c4a512c":"To make the data easier to analyse, we will only consider distrcits with engagement data for everyday in 2020.","df3cf92e":"\nOver the last quarter of 2020, we can see a pct_access of roughly 15. What does this mean? 15 % of students in the district have at least one page-load event of Zoom or Meet. To be honest, that seems a little bit low from the home schooling point of view given that every student needs to attend classes on a school day. However, this seems to hint at the fact that not all students had to attend classes virtually but were able to physically go to school. Judging from this State-by-State Map of Where School Buildings Are Opened or Closed it seems like a lot of schools offered in-person education in 2020. That means, when looking at digital learning, we should probably focus our analysis on districts where digital learning was actually applied to get some key insights.\n\nWhile for pct_access Zoom and Meet seem to have roughly similiar values, we can see in the lower graph that Meet has more than 4 times the value of Zoom for engagement_index in the last quarter of 2020. What does this mean? If we have 1000 page-load events per 1000 students for Zoom on a given day that means that one student uses Zoom once a day. In contrast, Meet is used 4 or 5 times daily on average per student.\n\nTo make things a little bit clearer: If every students has two applications on their phone, the pct_access indicates how many of the students access this app on a daily basis but the engagement_index tells you how much the students engage with that application on a daily basis.\n","d189c47e":"**DISTRICTS**\n\nThe districts file includes information about the characteristics of school districts, including data from NCES (2018-19), FCC (Dec 2018), and Edunomics Lab:\n1. district_id : Unique Identifier for each district\n2. state : The state the district falls in\n3. locale : NCES locale classification that categorizes U.S. territory into four types of areas: City, Suburban, Town, and Rural.\n4. pct_black\/hispanic - percentage of students in the districts identified as Black or Hispanic based on 2018-19 NCES data.\n5. pct_free\/reduced - percentage of students in the districts eligible for free or reduced-price lunch based on 2018-19 NCES data.\n6. county_connections_ratio - ratio (residential fixed high-speed connections over 200 kbps in at least one direction\/households) based on the county level data from FCC From 477 (December 2018 version).\n7. pp_total_raw - per-pupil total expenditure (sum of local and federal expenditure) from Edunomics Lab's National Education Resource Database on Schools (NERD$) project.\n\n","631ca4e8":"Above we can see the top 5 most accessed products for each LC sub-category sorted by the mean pct_access for 2020 over all districts. We can see that most of the products are on average accessed by less than 5 % of students on a daily basis. Exceptions are YouTube, Google Docs, and Canvas. YouTube in this case is a difficult one to evaluate since it can be used for leisure in addition to education, so we need to be careful here. Google Docs seems to make a lot of sense since students can use Google Docs. \n\nCanvas is a web-based learning management system, or LMS. It is used by learning institutions, educators, and students to access and manage online course learning materials and communicate about skill development and learning achievement, so it also makes sense that this one is quite often accessed.\n\nRegarding the sub-category 'Career Planning & Job Search' the average pct_access is very low. This is probably due top the fact that career planning is only relevant to older students. Therefore, we can probably exclude this subcategory when inspecting the digital learning aspect.","1b2fa117":"Products dataset has some null values","f4c6d60e":"# Libraries Import","6c415093":"Furthermore, if we look at a few sample districts, we can  see that most districts use more than the 369 unique products from products_info. In fact, the concatenated engagement data contains more than 8000 unique products. Since we don't have any additional information the majority of these products, we will remove engagement data for unknown products. This reduces the engagement data roughly by half.","2434c512":"One-Hot Encoding the Product Sectors:","d1223c22":"# Data Import and Preprocessing","6c1ac4d4":"**DISTRICTS**","c29a0977":"Finally, we will convert the time column to the type datetime64[ns] for easier handling.","76fdfc9a":"Splitting the primary essential functions","570d9ca3":"To summarize, we have removed districts without any information on the location and we have removed districts with incomplete data in 2020.","02bc22a6":"The most common category in the column Primary Essential Function is Learning & Curriculum (LC) as shown in the above figure. For the categories Classroom Management (CM) and School & District Operations (SDO) there are far fewer tool options available."}}