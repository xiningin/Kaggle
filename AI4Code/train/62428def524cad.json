{"cell_type":{"198d5fe3":"code","6535d33d":"code","52872d55":"code","8fb8d483":"code","42bdcb70":"code","d1c281b9":"code","7e39d109":"code","67958712":"code","a325a9fd":"code","3456b0ad":"code","27afc18a":"code","f973f880":"code","64053c0e":"code","fd70707b":"code","f8437522":"code","e3b27763":"markdown","18f0a169":"markdown","464482a2":"markdown","47603f2a":"markdown","4a5224c8":"markdown","bfcf5422":"markdown","f6e31603":"markdown","afb34aff":"markdown","c8bf0311":"markdown","17cc3688":"markdown","3b8714f9":"markdown"},"source":{"198d5fe3":"def black_box_function(x,y):\n    return -x**2 - (y-0) **2 +1","6535d33d":"import numpy as np\nx_range = np.linspace(-100, 100, num=1000) # -100 ~ 100 \uc0ac\uc774\uc758 \uc784\uc758\uc758 x\ub97c \ub9cc\ub4e4\uc5b4 \ub0c5\ub2c8\ub2e4.\ny_range = np.linspace(-100, 100, num=1000) # -100 ~ 100 \uc0ac\uc774\uc758 \uc784\uc758\uc758 y\ub97c \ub9cc\ub4e4\uc5b4 \ub0c5\ub2c8\ub2e4.\n\n# \ubbf8\ub9ac \uc9c0\uc815\ud574\ub454 \ud568\uc218\uc5d0 \ucd9c\ub825\uac12\uc744 \ubc1b\uc544 \uadf8\ub9bc\uc73c\ub85c \ud655\uc778\ud558\uaca0\uc2b5\ub2c8\ub2e4 .","52872d55":"import matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\nfig = plt.figure(figsize=(15,5))\nax = fig.add_subplot(111, projection='3d')\n\nX, Y = np.meshgrid(x_range, y_range)\nZ = black_box_function(X,Y)\nax = plt.axes(projection='3d')\nax.contour3D(X, Y, Z, 50, cmap='binary')\nax.set_xlabel('x')\nax.set_ylabel('y')\nax.set_zlabel('z')\nax.view_init(50, 30)","8fb8d483":"# 2. Getting Started\nfrom bayes_opt import BayesianOptimization\n\n# Bounded region of parameter space\npbounds = {'x': (-10, 10), 'y': (-10, 10)}\n\n# \uc138\ubd80 \uc0ac\ud56d \uc124\uc815\noptimizer = BayesianOptimization(\n    f=black_box_function,\n    pbounds=pbounds,\n    verbose=2, # verbose = 1 prints only when a maximum is observed, verbose = 0 is silent\n    random_state=1\n)\n\n# \ucd5c\ub300\ud654!!\noptimizer.maximize(init_points=2, n_iter=30 )\n    # n_iter: \ubc18\ubcf5 \ud69f\uc218 (\ub9ce\uc744 \uc218\ub85d \uc815\ud655\ud55c \uac12\uc744 \uc5bb\uc744 \uc218 \uc788\ub2e4)\n    # init_points: \ucd08\uae30 \ub79c\ub364 \ud3ec\uc778\ud2b8 \uac2f\uc218","42bdcb70":"# \ucd5c\uc801\uc758 (x, y) \uac12 \uacb0\uacfc \ud655\uc778\nprint(optimizer.max)","d1c281b9":"optimizer.res[-5:] # \uc774\uc804 history \ub97c \ud655\uc778 \ud560 \uc218 \uc788\ub2e4.","7e39d109":"# 2.1 Changing bounds\noptimizer.set_bounds(new_bounds={\"x\": (-1, 1)})\n\n# \uc774\ud6c4 \uc808\ucc28\ub294 \ub3d9\uc77c\ud788\ub2e4.\noptimizer.maximize( init_points=0, n_iter=5)","67958712":"# \ucd5c\uc801\uc758 \ud30c\ub77c\ubbf8\ud130 \uac12 \ud655\uc778\nprint(optimizer.max)","a325a9fd":"# \ubaa8\ub4c8 \ubd88\ub7ec\uc624\uae30\nfrom bayes_opt import BayesianOptimization\nfrom sklearn.linear_model import Ridge\nfrom sklearn.model_selection import cross_val_score\nimport numpy as np\n\n# seed \uace0\uc815!\nnp.random.seed(0)\nn_samples, n_features = 100, 1 # 100\uac1c\uc758 \ub370\uc774\ud130\uc640 1\uac1c\uc758 \ubcc0\uc218 \uc0dd\uc131\n\nX = np.random.randn(n_samples, n_features) # shape = (100 , 1)\ny = np.random.randn(n_samples) # shape = (100 , )\n\n\n# CV \ub97c \uc774\uc6a9\ud55c, Ridge \ud30c\ub77c\ubbf8\ud130 \ucc3e\uae30\ndef Ridge_cv(alpha):\n    '''\n    :param alpha: Ridge's \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130\n    :return: -RMSE --> \ucd5c\uc18c\ud654\ub97c \uc704\ud574 \uc74c\uc218 \ubd80\ud638\ub97c \ubd99\ud798\n    '''\n\n    RMSE = cross_val_score(Ridge(alpha=alpha), X, y, scoring='neg_mean_squared_error', cv=5).mean()\n\n    return -RMSE","3456b0ad":"# \ud30c\ub77c\ubbf8\ud130\ub97c \ud0d0\uc0c9\ud560 \uacf5\uac04\n# Ridge\ub294 0 ~ 10 \uc0ac\uc774\uc5d0\uc11c \uc801\uc808\ud55c \uac12\uc744 \ucc3e\ub294\ub2e4.\npbounds = {'alpha': ( 0, 10 )}\n\n# \ubca0\uc774\uc9c0\uc548 \uc635\ud2f0\ub9c8\uc774\uc81c\uc774\uc158 \uac1d\uccb4\ub97c \uc0dd\uc131\nRidge_BO = BayesianOptimization( f = Ridge_cv, pbounds  = pbounds , verbose=2, random_state=1 )\n\n# \ucd5c\ub300\ud654!!!\nRidge_BO.maximize(init_points=2, n_iter = 10)\n\nRidge_BO.max # \ucc3e\uc740 \ud30c\ub77c\ubbf8\ud130 \uac12 \ud655\uc778","27afc18a":"import numpy as np\nimport matplotlib\nfrom matplotlib import pyplot  as plt\nfrom sklearn import svm, datasets\nfrom sklearn.model_selection import cross_val_score\nfrom bayes_opt import BayesianOptimization\n\nmatplotlib.rc('font', family = 'Malgun Gothic')\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\ndef SVM_rbf_cv(gamma, C):\n    model = svm.SVC(kernel = 'rbf', gamma=gamma, C = C)\n    RMSE = cross_val_score(model, X, y, scoring='accuracy', cv=5).mean()\n    return -RMSE","f973f880":"# \uc8fc\uc5b4\uc9c4 \ubc94\uc704 \uc0ac\uc774\uc5d0\uc11c \uc801\uc808\ud55c \uac12\uc744 \ucc3e\ub294\ub2e4.\npbounds = {'gamma': ( 0.001, 1000 ), \"C\": (0.001, 1000)}\n\n# \ubca0\uc774\uc9c0\uc548 \uc635\ud2f0\ub9c8\uc774\uc81c\uc774\uc158 \uac1d\uccb4\ub97c \uc0dd\uc131\nSVM_rbf_BO = BayesianOptimization( f = SVM_rbf_cv, pbounds = pbounds, verbose = 2, random_state = 1 )\n\n# \uba54\uc18c\ub4dc\ub97c \uc774\uc6a9\ud574 \ucd5c\ub300\ud654!\nSVM_rbf_BO.maximize(init_points=2, n_iter = 10)\n\nSVM_rbf_BO.max # \ucc3e\uc740 \ud30c\ub77c\ubbf8\ud130 \uac12 \ud655\uc778","64053c0e":"import numpy as np\nimport matplotlib\nfrom matplotlib import pyplot as plt\nfrom sklearn import datasets\nfrom sklearn.model_selection import cross_val_score\nfrom bayes_opt import BayesianOptimization\nimport xgboost as xgb\n\n\niris = datasets.load_iris()\nX = iris.data\ny = iris.target\n\n\ndef XGB_cv(max_depth,learning_rate, n_estimators, gamma\n           ,min_child_weight, max_delta_step, subsample\n           ,colsample_bytree, silent=True, nthread=-1):\n    model = xgb.XGBClassifier(max_depth=int(max_depth),\n                              learning_rate=learning_rate,\n                              n_estimators=int(n_estimators),\n                              silent=silent,\n                              nthread=nthread,\n                              gamma=gamma,\n                              min_child_weight=min_child_weight,\n                              max_delta_step=max_delta_step,\n                              subsample=subsample,\n                              colsample_bytree=colsample_bytree)\n    RMSE = cross_val_score(model, X, y, scoring='accuracy', cv=5).mean()\n    return -RMSE\n\n# \uc8fc\uc5b4\uc9c4 \ubc94\uc704 \uc0ac\uc774\uc5d0\uc11c \uc801\uc808\ud55c \uac12\uc744 \ucc3e\ub294\ub2e4.\npbounds = {'max_depth': (5, 10),\n          'learning_rate': (0.01, 0.3),\n          'n_estimators': (50, 1000),\n          'gamma': (1., 0.01),\n          'min_child_weight': (2, 10),\n          'max_delta_step': (0, 0.1),\n          'subsample': (0.7, 0.8),\n          'colsample_bytree' :(0.5, 0.99)\n          }\n\nxgboostBO = BayesianOptimization(f = XGB_cv,pbounds = pbounds, verbose = 2, random_state = 1 )\n\n# \uba54\uc18c\ub4dc\ub97c \uc774\uc6a9\ud574 \ucd5c\ub300\ud654!\nxgboostBO.maximize(init_points=2, n_iter = 10)\n\nxgboostBO.max # \ucc3e\uc740 \ud30c\ub77c\ubbf8\ud130 \uac12 \ud655\uc778","fd70707b":"fit_xgb = xgb.XGBClassifier(max_depth= int( xgboostBO.max['params']['max_depth'] ),\n                             learning_rate=xgboostBO.max['params']['learning_rate'],\n                             n_estimators=int(xgboostBO.max['params']['n_estimators']),\n                             gamma= xgboostBO.max['params']['gamma'],\n                             min_child_weight=xgboostBO.max['params']['min_child_weight'],\n                             max_delta_step=xgboostBO.max['params']['max_delta_step'],\n                             subsample=xgboostBO.max['params']['subsample'],\n                             colsample_bytree=xgboostBO.max['params']['colsample_bytree'])\n","f8437522":"model  = fit_xgb.fit(X,y)","e3b27763":"* X = 0, Y = 0 \uc5d0\uc11c black_box_function\uc774 \ucd5c\ub300\ud654 \ucd5c\ub294 \ubaa8\uc2b5\uc744 \ud655\uc778 \ud588\uc2b5\ub2c8\ub2e4. <br>\n\n* \uacfc\uc5f0 \ubca0\uc774\uc9c0\uc548 \uc635\ud2f0\ub9c8\uc774\uc81c\uc774\uc158 \uc5d0\uc11c\ub3c4 \uac19\uc740 \uacb0\uacfc\ub97c \ud655\uc778 \ud560 \uc218 \uc788\uc744\uae4c\uc694?","18f0a169":"\ucd08\uae30 \ud3ec\uc778\ud2b8 2\uac1c\uc640 \uc635\ud2f0\ub9c8\uc774 \uc81c\uc774\uc158 30\ud68c \uc9c4\ud589 \uacb0\uacfc \uc704\uc640 \uac19\uc740 \ucd5c\uc801\uc758 \uac12\uc744 \ucc3e\uc558\uc2b5\ub2c8\ub2e4.<br>\n\uacb0\uacfc\ub97c \ubcf4\ub2c8, \uc815\ud655\ud55c (0,0)\uc744 \ucc3e\uc9c0 \ubabb\ud588\uc9c0\ub9cc, \uaf64 \uadfc\uc811\ud55c \ub2f5\uc744 \ucc3e\uc558\uc2b5\ub2c8\ub2e4.<br>\n\n\ub354 \uc9c4\ud589 \ud560 \uc218 \uc788\uc9c0\ub9cc, \uc7a0\uc2dc \uadf8\ub3d9\uc548 \uc5b4\ub5a4 \uc9c4\ud589\uacfc\uc815\uc774 \uc788\uc5c8\ub294\uc9c0 \ud655\uc778\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4 .","464482a2":"## \uc784\uc758\uc758 \ud568\uc218 \ucd5c\uc801\ud654 \ud558\uae30\n* black_box_function\uc740 \uc784\uc758\uc758 \uc5b4\ub5a4 Cost function\ub77c \uac00\uc815\ud558\uaca0\uc2b5\ub2c8\ub2e4. <br>\n\n    x\uc640 y\ub294 \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130\uc77c \ub54c, \uc5b4\ub5a4 \ucd5c\uc801\uc758 \uac12\uc774 black_box_function\uc744 \ucd5c\ub300\ud654 \ud560 \uc218 \uc788\ub294\uc9c0 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.","47603f2a":"**2019-04-16** <br>\n\ub300\ud68c\uac00 \uc5bc\ub9c8 \ub0a8\uc9c0 \uc54a\uc740 \uc2dc\uc810.<br>\n\ub300\ubd80\ubd84\uc758 \ucc38\uac00\uc790 \ubd84\ub4e4\uc774 \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130\ub97c \uc218\uc815\ud558\uace0 \uc788\uc73c\uc2e4\ud150\ub370\uc694,<br>\n\ubca0\uc774\uc9c0\uc548 \uc635\ud2f0\ub9c8\uc774\uc81c\uc774\uc158\uc744 \uc774\uc6a9\ud574 \uc190\uc27d\uac8c \ud558\uc774\ud37c \ud30c\ub77c\ubbf8\ud130\ub97c \ucc3e\ub294 \ubc29\ubc95\uc744 \uc18c\uac1c\ud574 \ub4dc\ub9bd\ub2c8\ub2e4.","4a5224c8":"## SVC - C, gamma \ub97c \ucd5c\uc801\ud654 \ud558\uae30","bfcf5422":"### \uc774\ud6c4 \uacfc\uc815\uc740 \uc784\uc758\uc758 \ud568\uc218\uac00 \uc544\ub2cc, \uc2e4\uc81c \ubaa8\ub378\uc744 \uc774\uc6a9\ud558\ub294 \uacfc\uc815\uc785\ub2c8\ub2e4.\n\n* CV\ub97c \ucd5c\ub300\ud55c \uc774\uc6a9\ud558\uba70, \ucd5c\uc801\uc758 Hyper Parameter\uac12\uc744 \ucc3e\uc2b5\ub2c8\ub2e4.","f6e31603":"\uc21c\uc11c\ub294 \ub2e4\uc74c\uacfc \uac19\uc2b5\ub2c8\ub2e4. <br>\n* \uc608\uc81c: \uc784\uc758\uc758 \ud568\uc218\ub97c \ucd5c\uc801\ud654 \ud558\uae30 <br>\n* \uc608\uc81c: Ridge - Alpha \ub97c \ucd5c\uc801\ud654 \ud558\uae30 <br>\n* \uc608\uc81c: SVC - C, gamma \ub97c \ucd5c\uc801\ud654 \ud558\uae30 <br>\n* \uc608\uc81c: XGB - learning_rate, max_depth \ub4f1 \ucd5c\uc801\ud654 \ud558\uae30 <br>\n\n\n\uc544\ub798 \uc608\uc2dc\ubcf4\ub2e4 \ub354 \uc790\uc138\ud55c \ud328\ud0a4\uc9c0 \uc0ac\uc6a9\ubc95\uacfc \uc774\ub860\uc801\uc778 \uc124\uba85\uc740 \ub2e4\uc74c \ub9c1\ud06c\ub97c \ucc38\uc870 \ubd80\ud0c1\ub4dc\ub9bd\ub2c8\ub2e4.\n\n\ud328\ud0a4\uc9c0 \uc124\uba85 : https:\/\/github.com\/fmfn\/BayesianOptimization\n\n\uc774\ub860 \uc124\uba85 : http:\/\/research.sualab.com\/introduction\/practice\/2019\/02\/19\/bayesian-optimization-overview-1.html","afb34aff":"## Ridge - Alpha \ucd5c\uc801\ud654","c8bf0311":"## XGB - learning_rate, max_depth \ub4f1 \ucd5c\uc801\ud654 \ud558\uae30","17cc3688":"\uc9c4\ud589 \uacfc\uc815\uc744 \uc0b4\ud3b4\ubcf4\uba74, X, Y \uac00 0 \uadfc\ucc98\uc5d0\uc11c \ub192\uc740 \uac12\uc744 \uac00\uc9c0\ub294 \uac83\uc744 \ud655\uc778 \ud560 \uc218 \uc788\uc2b5\ub2c8\ub2e4. <br>\n\n\uc774\ubc88\uc5d0\ub294 X\uc758 \ubc94\uc704\ub97c -1 ~ 1 \uc0ac\uc774\ub85c \ubcc0\uacbd\ud55c \ub2e4\uc74c, \uc635\ud2f0\ub9c8\uc774 \uc81c\uc774\uc158 \uacfc\uc815\uc744 \uc9c4\ud589\ud574 \ubcf4\uaca0\uc2b5\ub2c8\ub2e4.","3b8714f9":"### \ub9c8\uc9c0\ub9c9\uc73c\ub85c \ucc3e\uc740 \ud30c\ub77c\ubbf8\ud130 \uac12\uc744 \uc801\uc6a9\ud558\ub294 \ubc29\ubc95\uc744 \ubcf4\uc5ec\ub4dc\ub9ac\uace0 \ub05d \ub9c8\uce58\uaca0\uc2b5\ub2c8\ub2e4. \n### \uae34 \uae00 \uc77d\uc5b4 \uc8fc\uc154\uc11c \uac10\uc0ac\ud569\ub2c8\ub2e4."}}