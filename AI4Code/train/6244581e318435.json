{"cell_type":{"626efa32":"code","f49f7599":"code","3f45cb67":"code","e5e62511":"code","577fa6f4":"code","40b31ce4":"code","c28a6bf5":"code","f2312c94":"code","14396c55":"code","a26365c9":"code","c4b77832":"code","aa0ff24c":"code","1dafe046":"code","5ce513ab":"code","b73f0e29":"code","709f6ef7":"code","03f4ff87":"code","2bfd68a8":"code","f02b78d4":"code","edc5895c":"code","7866683d":"code","0f0c893b":"code","4c09fc97":"code","87960a86":"code","505f16b3":"code","e531488b":"code","2674d70e":"code","22c163af":"code","96961f02":"code","73b68175":"code","0cb1c439":"code","6e1d4e92":"code","87d2c8a5":"code","39168424":"code","5a4dfd44":"markdown","0715ce38":"markdown","d36fa394":"markdown","3efa2aa6":"markdown","383010df":"markdown","bc2458b5":"markdown","6f9d3754":"markdown","468dc847":"markdown","6df51146":"markdown","5a0833be":"markdown","6cdcc294":"markdown","310929db":"markdown","82461b08":"markdown","ae5749e0":"markdown","3effc993":"markdown","7b22f929":"markdown","64fe4d9a":"markdown","e1294509":"markdown","9b9511cc":"markdown","08be36c4":"markdown"},"source":{"626efa32":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier \nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import GridSearchCV\n\nfrom sklearn import tree\n","f49f7599":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","3f45cb67":"train.isnull().sum() # To check No missing values ","e5e62511":"sns.boxplot(x='Pclass', y='Age', data = train)\nplt.show()","577fa6f4":"def impute_age(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age):\n\n        if Pclass == 1:\n            return 37\n\n        elif Pclass == 2:\n            return 29\n\n        else:\n            return 24\n\n    else:\n        return Age","40b31ce4":"train['Age'] = train[['Age','Pclass']].apply(impute_age,axis=1)","c28a6bf5":"train.drop('Cabin',axis=1,inplace=True) \ntrain","f2312c94":"train.dropna(inplace = True)\ntrain","14396c55":"train.info()","a26365c9":"\nsex = pd.get_dummies(train['Sex'],drop_first=True)\nembark = pd.get_dummies(train['Embarked'],drop_first=True)\n\ntrain.drop(['Sex','Embarked','Name','Ticket'],axis=1,inplace=True)","c4b77832":"train","aa0ff24c":"train = pd.concat([train,sex,embark],axis=1)\n\ntrain_data = train.drop('Survived', axis=1)\nlabel = train['Survived']","1dafe046":"train_data","5ce513ab":"train_data.info()","b73f0e29":"k_fold = KFold(n_splits=10, shuffle=True, random_state=0)","709f6ef7":"DT_clf = tree.DecisionTreeClassifier(random_state=0)\nclf = DT_clf.fit(train_data, label)\ntree.plot_tree(clf)\n","03f4ff87":"fn=list(train_data.columns)\ncn=[\"0\",\"1\"]\nfig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (2,2), dpi=500)\ntree.plot_tree(clf,\n               feature_names = fn, \n               class_names=cn,\n               filled = True);\nfig.savefig('Titanic_DT.png')","2bfd68a8":"train_mini = train_data.drop('PassengerId', axis=1)\ntrain_mini = train_mini.drop('Age', axis=1)   # hash.remove this if you want to include it\ntrain_mini = train_mini.drop('Fare', axis=1)  # hash.remove this if you want to include it\ntrain_mini = train_mini.drop('SibSp', axis=1) # hash.remove this if you want to include it\ntrain_mini = train_mini.drop('Pclass', axis=1) # hash.remove this if you want to include it\n\nDT_mini_clf = tree.DecisionTreeClassifier(random_state=0)\nclf_mini = DT_mini_clf.fit(train_mini, label)\n\nfn=list(train_mini.columns)\ncn=[\"0\",\"1\"]\nfig, axes = plt.subplots(nrows = 1,ncols = 1,figsize = (2,2), dpi=700)\ntree.plot_tree(clf_mini,\n               feature_names = fn, \n               class_names=cn,\n               filled = True);","f02b78d4":" #Decision Tree Score\nDT_score = cross_val_score(DT_clf, train_data, label, cv=k_fold, n_jobs=1, scoring='accuracy')\nprint(DT_score)\nround(np.mean(DT_score)*100, 2)","edc5895c":"RF_clf = RandomForestClassifier(n_estimators=13)\nscoring = 'accuracy'\nRF_score = cross_val_score(RF_clf, train_data, label, cv=k_fold, n_jobs=1, scoring=scoring)\nprint(RF_score)","7866683d":" #Random Forest Score\nround(np.mean(RF_score)*100, 2)","0f0c893b":"test.isnull().sum()","4c09fc97":"test['Age'] = test[['Age','Pclass']].apply(impute_age,axis=1)\n\ntest","87960a86":"test.isnull().sum()","505f16b3":"test[\"Fare\"] = test.Fare.astype(float)","e531488b":"test.info()","2674d70e":"test.drop('Cabin',axis=1,inplace=True) ","22c163af":"\ntest['Fare'] = test['Fare'].fillna(0)\n\ntest.isnull().sum()","96961f02":"sex = pd.get_dummies(test['Sex'],drop_first=True)\nembark = pd.get_dummies(test['Embarked'],drop_first=True)\n\ntest.drop(['Sex','Embarked','Name','Ticket'],axis=1,inplace=True)\n\ntest = pd.concat([test,sex,embark],axis=1)\n\ntest","73b68175":"test.info()","0cb1c439":"xtrain=train_data\nytrain=label\nxtest=test","6e1d4e92":"RF=RandomForestClassifier(random_state=1)\nPRF=[{'n_estimators':[10,100],'max_depth':[3,6],'criterion':['gini','entropy']}]\nGSRF=GridSearchCV(estimator=RF, param_grid=PRF, scoring='accuracy',cv=2)\nscores_rf=cross_val_score(GSRF,xtrain,ytrain,scoring='accuracy',cv=5)\nnp.mean(scores_rf)","87d2c8a5":"model=GSRF.fit(xtrain, ytrain)\npred=model.predict(xtest)","39168424":"\n\noutput = pd.DataFrame({'PassengerId': xtest.PassengerId, 'Survived': pred})\noutput.to_csv('Kamal_submission.csv', index=False)  #change name to own\nprint(\"Your submission was successfully saved!\")","5a4dfd44":"# Submission","0715ce38":"Why is Random Forest typically better than Decision Tree?\n\n","d36fa394":" Question: Why am I taking these ages?\n \n \n \n Hint: look at the box plot above","3efa2aa6":"# look at the test data","383010df":"# Categorical Values","bc2458b5":"What does this code do?\nWould you also remove Cabin?","6f9d3754":"# Modelling","468dc847":"# Does this DF look identical to the train before modelling?","6df51146":"# Import Libraries","5a0833be":"# What is the Titanic Problem?\n\nPlease refer to the notebook for EDA and Vizualizations: \nhttps:\/\/www.kaggle.com\/kmldas\/titanic-eda-viz-for-beginners\n","6cdcc294":"![Source: https:\/\/i.ytimg.com\/vi\/Wan0IJuXlys\/maxresdefault.jpg](https:\/\/i.ytimg.com\/vi\/Wan0IJuXlys\/maxresdefault.jpg)","310929db":"# Titanic: Decision Trees and Random Forest\n\nThis is a notebook to understand Decision Trees and Random Forest and use it for the Titanic: Machine Learning from Disaster case. \n\nThe aim is to  Predict survival on the Titanic and get familiar with ML basics\n","82461b08":"# chart too crowded?\n\nsee the data with less features or column","ae5749e0":"What does this code do?\nDoes this matter?","3effc993":"# Read the files","7b22f929":"# Can you remove the Fare and Cabin?\nwhy or why not?","64fe4d9a":"# Random Forest","e1294509":"**Good to know**\n\n\nWhen we use \"drop\" only, it drops the columns\/rows you define\n\nWhen we use \"dropna\", it removes all entries with NaN values (or null in general)","9b9511cc":"# Data Cleaning ","08be36c4":"# Decision Tree"}}