{"cell_type":{"475530c8":"code","1adb06f7":"code","14d90e4c":"code","32cf63c7":"code","a4e7818a":"code","706436fe":"code","0ebbe5a5":"code","e3e39973":"code","f425ee63":"code","9547d99f":"code","b87bc402":"code","28b79ca8":"code","e70b730e":"code","df17a632":"code","65edd8b6":"code","fa46a393":"code","e5911f30":"code","a8d621a2":"code","d3483ccc":"code","3d66885c":"code","1a28713d":"code","dc02ac6a":"code","2319137a":"code","92c1dc18":"code","0537a0dc":"code","59d4e653":"code","87831784":"code","e6965966":"code","89fd7e1c":"code","5c1da007":"code","73bded43":"code","a504254b":"code","6745724e":"markdown","e3465106":"markdown","9eaf0041":"markdown","c0fb4837":"markdown","62023f06":"markdown","17801992":"markdown","60ea7ced":"markdown","f42f6776":"markdown","392241fa":"markdown","63dbaa85":"markdown","55899378":"markdown","02f21c95":"markdown"},"source":{"475530c8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1adb06f7":"#Read the dataset\n\ndf_pre = pd.read_csv(\"\/kaggle\/input\/wine-quality\/winequalityN.csv\")\n\nShape=df_pre.shape\n\nprint(\"Rows:\",Shape[0])\nprint(\"Columns:\",Shape[1])\n\ndf_pre.head(10)","14d90e4c":"#Info about the dataset\ndf_pre.info()","32cf63c7":"dff=df_pre","a4e7818a":"dff.describe().T","706436fe":"#Checking or the null values\n\ndff.isna().any()","0ebbe5a5":"#Null value count in the dataset\n\ndff.isna().sum()","e3e39973":"#Replacing the null values with the median\n\ndff = dff.fillna(dff.median())\n","f425ee63":"#Handling the categorical values\ndff = pd.get_dummies(data = dff, columns = ['type'] , prefix = ['type'] , drop_first = True)\n","9547d99f":"# Lets check for highly correlated variables\n\ncor= dff.corr()\ncor.loc[:,:] = np.tril(cor,k=-1)  # reference:https:\/\/www.geeksforgeeks.org\/numpy-tril-python\/\ncor=cor.stack()\ncor[(cor > 0.55) | (cor< -0.55)]","b87bc402":"cor=dff.corr()\ncor","28b79ca8":"#Correlation plot\nplt.figure(figsize=(10,8))\nsns.heatmap(cor,annot=True,linewidths=.5,center=0,cbar=False,cmap=\"YlGnBu\")\nplt.show()","e70b730e":"Target_Imb=dff[\"quality\"].value_counts(normalize=True)\nTarget_Imb","df17a632":"#Combine 7&8 together; combine 3 and 4 with 5 so that we have only 3 levels and a more balanced Y variable\ndff['quality'] = dff['quality'].replace(8,7)\ndff['quality'] = dff['quality'].replace(3,5)\ndff['quality'] = dff['quality'].replace(4,5)\ndff['quality'].value_counts()","65edd8b6":"# splitting data into training and test set for independent attributes\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test =train_test_split(dff.drop('quality',axis=1), dff['quality'], test_size=0.30,\n                                                   random_state=22)\nX_train.shape,X_test.shape","fa46a393":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nfrom sklearn.metrics import accuracy_score,f1_score,recall_score,precision_score, confusion_matrix\nmodel_entropy=DecisionTreeClassifier(criterion='entropy')\nmodel_entropy.fit(X_train, y_train)\nmodel_entropy.score(X_train, y_train)\nmodel_entropy.score(X_test, y_test)","e5911f30":"clf_pruned = DecisionTreeClassifier(criterion = \"entropy\", random_state = 100, max_depth=5, min_samples_leaf=5)\nclf_pruned.fit(X_train, y_train)\npreds_pruned = clf_pruned.predict(X_test)\npreds_pruned_train = clf_pruned.predict(X_train)\nprint(accuracy_score(y_test,preds_pruned))\nprint(accuracy_score(y_train,preds_pruned_train))","a8d621a2":"acc_DT = accuracy_score(y_test, preds_pruned)\n#Store the accuracy results for each model in a dataframe for final comparison\nresultsDf = pd.DataFrame({'Method':['Decision Tree'], 'accuracy': acc_DT})\nresultsDf = resultsDf[['Method', 'accuracy']]\nresultsDf","d3483ccc":"from sklearn.ensemble import RandomForestClassifier\nrfcl = RandomForestClassifier(n_estimators = 50)\nrfcl = rfcl.fit(X_train, y_train)\npred_RF = rfcl.predict(X_test)\nacc_RF = accuracy_score(y_test, pred_RF)\ntempResultsDf = pd.DataFrame({'Method':['Random Forest'], 'accuracy': [acc_RF]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf = resultsDf[['Method', 'accuracy']]\nresultsDf\n\nacc_RF=acc_RF*100","3d66885c":"from sklearn.ensemble import AdaBoostClassifier\nabcl = AdaBoostClassifier( n_estimators= 100, learning_rate=0.1, random_state=22)\nabcl = abcl.fit(X_train, y_train)\npred_AB =abcl.predict(X_test)\nacc_AB = accuracy_score(y_test, pred_AB)\ntempResultsDf = pd.DataFrame({'Method':['Adaboost'], 'accuracy': [acc_AB]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf = resultsDf[['Method', 'accuracy']]\nresultsDf","1a28713d":"from sklearn.ensemble import GradientBoostingClassifier\ngbcl = GradientBoostingClassifier(n_estimators = 50, learning_rate = 0.1, random_state=22)\ngbcl = gbcl.fit(X_train, y_train)\npred_GB =gbcl.predict(X_test)\nacc_GB = accuracy_score(y_test, pred_GB)\ntempResultsDf = pd.DataFrame({'Method':['Gradient Boost'], 'accuracy': [acc_GB]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf = resultsDf[['Method', 'accuracy']]\nresultsDf","dc02ac6a":"from sklearn.ensemble import BaggingClassifier\n\nbgcl = BaggingClassifier(n_estimators=50, max_samples= .7, bootstrap=True, oob_score=True, random_state=22)\nbgcl = bgcl.fit(X_train, y_train)\npred_BG =bgcl.predict(X_test)\nacc_BG = accuracy_score(y_test, pred_BG)\ntempResultsDf = pd.DataFrame({'Method':['Bagging'], 'accuracy': [acc_BG]})\nresultsDf = pd.concat([resultsDf, tempResultsDf])\nresultsDf = resultsDf[['Method', 'accuracy']]\nresultsDf","2319137a":"from sklearn.model_selection import KFold\nfrom sklearn.model_selection import cross_val_score\nnum_folds = 20\nseed = 100\nkfold = KFold(n_splits=num_folds, random_state=seed)\n\ny = dff['quality']\nX = dff.loc[:, dff.columns != 'quality']\n\nresults = cross_val_score(rfcl,X, y, cv=kfold)\n\nKfold_CV=np.around(np.mean(abs(results*100)))\n\nfor i in range(num_folds):\n    print(\"Kfold\",i,\":\",results[i]*100,\"%\\n\")\n\nprint(\"Mean:\",Kfold_CV,\"%\")\n\nprint(\"\\nStandard Deviation:\",results.std())\n\n\nprint(\"\\n\\nRandom Forest Accuracy\",acc_RF,\"%\\n\\n\")\n\nimprovement=Kfold_CV-acc_RF\nprint(\"Accuracy improvement:\",np.around(improvement),\"%\")\n\n","92c1dc18":"from sklearn.model_selection import LeaveOneOut\nfrom sklearn.model_selection import cross_val_score\nscores = cross_val_score(rfcl, X_train, y_train, cv=LeaveOneOut())\nscores\n\nprint(\"Mean accuracy:\",scores.mean()*100,\"%\")\n\nprint(\"\\nRandom Forest Accuracy\",acc_RF,\"%\")\n\nprint(\"\\nStandard Deviation:\",scores.std())\n\nLeave_One_Out=np.mean(abs(scores.mean()*100))\nimprovement=Leave_One_Out-acc_RF\nprint(\"\\nAccuracy improvement:\",np.around(improvement),\"%\")","0537a0dc":"from sklearn.model_selection  import StratifiedKFold, cross_val_score\n\nk = 20\n\nstratified_kfold = StratifiedKFold(n_splits = k, random_state = 55)\nresults = cross_val_score(rfcl, X, y, cv = stratified_kfold)\n\nstrat_CV=np.around(np.mean(abs(results*100)))\n\nfor i in range(k):\n    print(\"Kfold\",i,\":\",results[i]*100,\"%\\n\")\n\nprint(\"\\n\\nMean:\",strat_CV,\"%\")\n\nprint(\"\\nStandard Deviation:\",results.std())\n\nprint(\"\\n\\nRandom Forest Accuracy\",acc_RF,\"%\\n\\n\")\n\nimprovement=strat_CV-acc_RF\nprint(\"Accuracy improvement:\",np.around(improvement),\"%\")","59d4e653":"# Number of iterations for bootstrapping\nbootstrap_iteration = 75\naccuracy = []\n\nfrom sklearn.utils import resample\nfrom sklearn.metrics import accuracy_score\n\nfor i in range(bootstrap_iteration):\n    X_, y_ = resample(X_train, y_train)\n    rfcl.fit(X_, y_)\n    y_pred = rfcl.predict(X_test)\n    \n    acc = accuracy_score(y_pred, y_test)\n    accuracy.append(acc)\n    \naccuracy = np.array(accuracy)\n\nprint('Standard deviation: ', accuracy.std())\n\nBoot=np.around(accuracy.mean()*100)\n\nprint(\"\\n\\nMean:\",Boot,\"%\")\n\nprint(\"\\nStandard Deviation:\",accuracy.std())\n\nprint(\"\\n\\nRandom Forest Accuracy\",acc_RF,\"%\\n\\n\")\n\nimprovement=Boot-acc_RF\nprint(\"Accuracy improvement:\",np.around(improvement),\"%\")","87831784":"#pretty print\n\nfrom pprint import pprint\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor(random_state = 1)\n\nprint('Parameters currently in use:\\n')\npprint(rf.get_params())","e6965966":"import numpy as np\nprint(np.linspace(start = 5, stop = 10, num = 2))","89fd7e1c":"from sklearn.model_selection import RandomizedSearchCV\n\n# Number of trees in random forest\nn_estimators = [int(x) for x in np.linspace(start = 10 , stop = 15, num = 2)]   # returns evenly spaced 10 numbers\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [int(x) for x in np.linspace(5, 10, num = 2)]  # returns evenly spaced numbers can be changed to any\nmax_depth.append(None)\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\n\n# Create the random grid\nrandom_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\npprint(random_grid)\n\nrf_random = RandomizedSearchCV(estimator=rf, param_distributions=random_grid,\n                              n_iter = 5, scoring='neg_mean_absolute_error', \n                              cv = 3, verbose=2, random_state=42, n_jobs=-1,\n                              return_train_score=True)\n\n# Fit the random search model\nrf_random.fit(X_train, y_train);\nrf_random.best_params_","5c1da007":"from sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'bootstrap': [True],\n    'max_depth': [5,6],\n    'max_features': [2, 3],\n    'min_samples_leaf': [3, 4],\n    'min_samples_split': [5,10],\n    'n_estimators': [5,6,7]\n}    \n\nrf = RandomForestRegressor(random_state = 1)\n\ngrid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n                          cv = 3, n_jobs = 1, verbose = 0, return_train_score=True)\n\ngrid_search.fit(X_train, y_train);\n\ngrid_search.best_params_\n","73bded43":"best_grid = grid_search.best_estimator_\n\nGrid_search_cv=np.around(best_grid.score(X_test, y_test)*100)\nprint(\"Grid sarch CV Score:\",Grid_search_cv,\"%\")\n\nprint(\"\\n\\nRandom Forest Accuracy\",acc_RF,\"%\\n\\n\")\n\nimprovement=Grid_search_cv-acc_RF\nprint(\"Accuracy improvement:\",np.around(improvement),\"%\")\n\n","a504254b":"print(\"Randon Forest Accuracy:\", acc_RF,\"%\")\n\nprint(\"\\nK Fold:\", Kfold_CV,\"%\")\n\nprint(\"\\nLeave one Out:\", Leave_One_Out,\"%\")\n\nprint(\"\\nStratified CV:\", strat_CV,\"%\")\n\nprint(\"\\nBootstrapping:\", Boot,\"%\")\n\nprint(\"\\nRandom sarch CV:\",rf_random,\"%\")\n\nprint(\"\\Grid sarch CV Score:\",grid_search,\"%\")\n\n","6745724e":"## Random Forest","e3465106":"## K fold Cross Validation","9eaf0041":"## Random Search CV","c0fb4837":"## Bagging","62023f06":"## Decision Tree","17801992":"## Grid Search CV","60ea7ced":"# Bootstrapping","f42f6776":"## Gradient Boost","392241fa":"# Model Tuning using hyper parameters","63dbaa85":"## ADA Boosting\n","55899378":"## Leave One Out Cross-Validation","02f21c95":"## Stratified Cross Validation"}}