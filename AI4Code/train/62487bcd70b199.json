{"cell_type":{"28a764c1":"code","c894df01":"code","f9f3f064":"code","06bd702d":"code","d8976251":"code","88717d3a":"code","c09a64bb":"code","ebad00ca":"code","ee724d21":"code","478a9120":"code","5d7e7b78":"code","7c75ec8f":"code","7e65729e":"code","2cdb171d":"code","6fa5ef2f":"code","dfc387fc":"code","ce1adcae":"code","d0d555e2":"code","b16a73a2":"code","14c83bd8":"code","a7ebac7a":"code","cd28b2d4":"code","5611ecba":"code","ed5b49a3":"code","6e6acff5":"code","df9c182e":"code","f1de0adb":"code","31e7c887":"code","a868a6b1":"code","40bde6c1":"code","e102ae21":"code","1de67f75":"code","8dcf57aa":"code","c3bd5f75":"code","a98b1e82":"code","e33e2b59":"code","9a94ba3b":"code","1cf28de2":"code","0c0e38ad":"code","3c75921b":"code","3713c751":"code","4913d028":"code","4c893ac1":"code","5228287c":"code","d98e89fb":"code","1d6d3070":"code","93c53961":"code","76badbd9":"code","e437332c":"code","40f0b237":"code","21a67158":"code","5a4c5aac":"code","56a75619":"code","0dadc441":"code","80210a8f":"markdown","b10aa7ee":"markdown","aeb8061f":"markdown","299f8fd2":"markdown","2ab75e5f":"markdown","60f7a5b2":"markdown","1879e4ef":"markdown","1d74ad2c":"markdown","5d0b9698":"markdown","2342aa2d":"markdown","905df325":"markdown","141f7e33":"markdown","efb3eb35":"markdown","ef1e8e0b":"markdown","ee53d4c8":"markdown","02bcb536":"markdown","d6211884":"markdown","4dc812ce":"markdown","7d3f9061":"markdown","9e987765":"markdown","231e332a":"markdown","ad7f3d0b":"markdown","747aa668":"markdown","506d617d":"markdown","2e8d9fd9":"markdown","aa0df33c":"markdown","85ea85bf":"markdown","9964e102":"markdown","ff8d7eda":"markdown","160638a9":"markdown","3f4a9537":"markdown","2635ea8e":"markdown","7d55e0f9":"markdown","27846b39":"markdown","5a9a13b2":"markdown","99b29072":"markdown","ba93f72e":"markdown","051d6173":"markdown","ad342184":"markdown","b6e0a2c6":"markdown","8cde8328":"markdown","6285dccf":"markdown","695c7d90":"markdown","21bb26c8":"markdown","c813450b":"markdown","894a50b9":"markdown","a4af381a":"markdown","df20d17f":"markdown","f851efc7":"markdown","47ec39da":"markdown","01aecfb5":"markdown","eee850cc":"markdown","b3e0cf9d":"markdown","3ddf5923":"markdown","235458ca":"markdown","e1b48e79":"markdown","477343eb":"markdown","bbdcbc6d":"markdown","9b9c3cec":"markdown","8db9901a":"markdown","7837b198":"markdown","2f62f2c0":"markdown","22b13c8f":"markdown","d99c2c9c":"markdown","f341165e":"markdown","e7421843":"markdown"},"source":{"28a764c1":"# import all libraries\n\nimport pandas as pd #basic\nimport numpy as np #basic\nimport pandas_profiling as pp #EDA\n\nfrom scipy.stats import shapiro #Stats\nfrom scipy.stats import pearsonr #Stats\nimport scipy.stats as stats #Stats\n\nimport plotly.express as px #visualization\nimport plotly.graph_objs as go#visualization\nimport plotly.offline as py#visualization\nimport plotly.figure_factory as ff#visualization\n\nfrom sklearn.model_selection import train_test_split #Split data\nfrom sklearn import preprocessing #manipulate data\nfrom sklearn.metrics import confusion_matrix,accuracy_score,classification_report,precision_score,recall_score,f1_score,roc_auc_score\n\n","c894df01":"#load the dataset \nloans = pd.read_csv('..\/input\/Bank_Personal_Loan_Modelling-1.csv')","f9f3f064":"print (\"Rows     : \" ,loans.shape[0])\nprint (\"Columns  : \" ,loans.shape[1])\nprint (\"\\nFeatures : \\n\" ,loans.columns.tolist())\nprint (\"\\nMissing values :  \", loans.isnull().sum().values.sum())\nprint (\"\\nUnique values :  \\n\",loans.nunique())","06bd702d":"loans.describe().transpose()","d8976251":"# Get the target column distribution. Your comments\nloans['Personal Loan'].value_counts()","88717d3a":"df_new=loans.copy()\npp.ProfileReport(df_new)","c09a64bb":"# Experience column has -ve values as seen from decribe output\n# total rows with this issue\n\nprint (loans[loans['Experience'] < 0]['Experience'].value_counts().sum())\n\n# we have 52 record thats less than one percent, so we will take abstract values\n\nloans['Experience'] = loans['Experience'].apply(abs)\n\n# New dataframe to get Vizulizations from data\n\nloans_viz = loans.copy()\n\n# replace values\n\nloans_viz['Securities Account'] = loans_viz['Securities Account'].replace({1: 'Yes', 0: 'No'})\nloans_viz['CD Account'] = loans_viz['CD Account'].replace({1: 'Yes',0: 'No'})\nloans_viz['Online'] = loans_viz['Online'].replace({1: 'Yes', 0: 'No'})\nloans_viz['CreditCard'] = loans_viz['CreditCard'].replace({1: 'Yes',0: 'No'})\n\n# Make Mortgage a Yes or No field just for vizulization purposes\n\ndef mort_lab(loans_viz):\n    if loans_viz['Mortgage'] > 0:\n        return 'Yes'\n    elif loans_viz['Mortgage'] == 0:\n        return 'No'\n\nloans_viz['Mortgage'] = loans_viz.apply(lambda loans_viz:mort_lab(loans_viz), axis=1)\n\n# Separating customers who took loans vs who didnt\n\nloan = loans_viz[loans_viz['Personal Loan'] == 1]\nnot_loan = loans_viz[loans_viz['Personal Loan'] == 0]\n\n# Separating catagorical and numerical columns\n\nId_col = ['ID']\ntarget_col = ['Personal Loan']\nexclude = ['ZIP Code']# as zip code is not having any value in personal loan predictions\ncat_cols = loans_viz.nunique()[loans_viz.nunique() < 6].keys().tolist()\ncat_cols = [x for x in cat_cols if x not in target_col + exclude]\nnum_cols = [x for x in loans_viz.columns if x not in cat_cols\n            + target_col + Id_col + exclude]\n","ebad00ca":"#check distirbution\ndef check_dist(col_name):\n    stat, p = shapiro(loans[col_name])\n#     print('stat=%.3f, p=%.3f' % (stat, p))\n    if p > 0.05:\n        print('Distribution of {} column is Probably Normal'.format(col_name))\n    else:\n        print('Distribution of {} column is Probably Not Normal'.format(col_name))\nfor col in num_cols:\n    check_dist(col)        ","ee724d21":"#check correlation\ndef check_dependancy(col1,col2):\n    stat, p = pearsonr(loans[col1], loans[col2])\n    if p > 0.05:\n        print('{} and {} are Probably independent<----'.format(col1,col2))\n    else:\n        print('{} and {} are Probably dependent'.format(col1,col2))\ncheck_dependancy('Experience','Income')\ncheck_dependancy('Age','Income')\ncheck_dependancy('Education','Income')\ncheck_dependancy('Family','Income')\ncheck_dependancy('Family','Mortgage')\ncheck_dependancy('Family','Personal Loan')\ncheck_dependancy('CCAvg','Personal Loan')\ncheck_dependancy('ZIP Code','Personal Loan')\ncheck_dependancy('Income','Personal Loan')\ncheck_dependancy('Age','Personal Loan')","478a9120":"# T-test to check dependency of smoking on charges\nHo = \"Income of Loan Customer and non-Loan Customer are same\"   # Stating the Null Hypothesis\nHa = \"Income of Loan Customer and non-Loan Customer are not same\"   # Stating the Alternate Hypothesis\n\nx = np.array(loans[loans['Personal Loan']==1].Income)  # Selecting income corresponding to  Loan Customer as an array\ny = np.array(loans[loans['Personal Loan']==0].Income) # Selecting income corresponding to non- Loan Customer as an array\n\nt, p_value  = stats.ttest_ind(x,y, axis = 0)  #Performing an Independent t-test\n\nif p_value < 0.05:  # Setting our significance level at 5%\n    print(f'{Ha} as the p_value ({p_value}) < 0.05')\nelse:\n    print(f'{Ho} as the p_value ({p_value}) > 0.05')","5d7e7b78":"Ho = \"No. of family has no effect on loan\"   # Stating the Null Hypothesis\nHa = \"No. of family has an effect on loan\"   # Stating the Alternate Hypothesis\n\n\none = loans[loans.Family == 1]['Personal Loan']\ntwo = loans[loans.Family == 2]['Personal Loan']\nthree = loans[loans.Family == 3]['Personal Loan']\nfour = loans[loans.Family == 4]['Personal Loan']\n\n\nf_stat, p_value = stats.f_oneway(one,two,three,four)\n\n\nif p_value < 0.05:  # Setting our significance level at 5%\n    print(f'{Ha} as the p_value ({p_value.round(3)}) < 0.05')\nelse:\n    print(f'{Ho} as the p_value ({p_value.round(3)}) > 0.05')","7c75ec8f":"names=loans_viz['Personal Loan'].value_counts().keys()\nvalues=loans_viz['Personal Loan'].value_counts().values\nfig = go.Figure(data=[go.Pie(labels=names, values=values, pull=[0,  0.2])])\nfig.show()","7e65729e":"px.scatter(data_frame=loans_viz,x='Age',y='Income',color='Personal Loan')","2cdb171d":"px.scatter(data_frame=loans_viz,x='Age',y='Experience',color='Personal Loan')","6fa5ef2f":"px.strip(data_frame=loans_viz,x='Family',y='Income',color='Personal Loan')","dfc387fc":"px.strip(data_frame=loans,x='Family',y='Mortgage',color='Personal Loan')","ce1adcae":"px.strip(data_frame=loans,x='Income',y='Mortgage',color='Personal Loan')","d0d555e2":"px.scatter(data_frame=loans_viz,x='CCAvg',y='Family',color='Personal Loan')","b16a73a2":"def plot_pie(column):\n\n    trace1 = go.Pie(\n        values=loans_viz[column].value_counts().values.tolist(),\n        labels=loans_viz[column].value_counts().keys().tolist(),\n        hoverinfo='label+percent+name',\n        domain=dict(x=[0, .48]),\n        name='Personal Loan Customers',\n        marker=dict(line=dict(width=2, color='rgb(243,243,243)')),\n        hole=.6,\n        )\n    trace2 = go.Pie(\n        values=not_loan[column].value_counts().values.tolist(),\n        labels=not_loan[column].value_counts().keys().tolist(),\n        hoverinfo='label+percent+name',\n        marker=dict(line=dict(width=2, color='rgb(243,243,243)')),\n        domain=dict(x=[.52, 1]),\n        hole=.6,\n        name='Non Loan Customers',\n        )\n\n    layout = go.Layout(dict(title=column\n                       + ' Distribution in Personal Loans ',\n                       plot_bgcolor='rgb(243,243,243)',\n                       paper_bgcolor='rgb(243,243,243)',\n                       annotations=[dict(text='Personal Loan Customers'\n                       , font=dict(size=13), showarrow=False, x=.15,\n                       y=.5), dict(text='Non Loan Customers',\n                       font=dict(size=13), showarrow=False, x=.88,\n                       y=.5)]))\n\n    data = [trace1, trace2]\n    fig = go.Figure(data=data, layout=layout)\n    py.iplot(fig)\n\n\n    # for all categorical columns plot pie\n\nfor i in cat_cols:\n    plot_pie(i)","14c83bd8":"def histogram(column):\n    trace1 = go.Histogram(x=loans[column], histnorm='percent',\n                          name='Loan Customers',\n                          marker=dict(line=dict(width=.5, color='black'\n                          )), opacity=.9)\n\n    trace2 = go.Histogram(x=not_loan[column], histnorm='percent',\n                          name='Non Loan Customers',\n                          marker=dict(line=dict(width=.5, color='black'\n                          )), opacity=.9)\n    data = [trace1, trace2]\n    layout = go.Layout(dict(title=column\n                       + ' distribution in Personal Loans ',\n                       plot_bgcolor='rgb(243,243,243)',\n                       paper_bgcolor='rgb(243,243,243)',\n                       xaxis=dict(gridcolor='rgb(255, 255, 255)',\n                       title=column, zerolinewidth=1, ticklen=5,\n                       gridwidth=2),\n                       yaxis=dict(gridcolor='rgb(255, 255, 255)',\n                       title='percent', zerolinewidth=1, ticklen=5,\n                       gridwidth=2)))\n    fig = go.Figure(data=data, layout=layout)\n    py.iplot(fig)\nfor i in num_cols:\n    histogram(i)","a7ebac7a":"##Split the train and test with 30%ratio\n\n(train, test) = train_test_split(loans, test_size=.3, random_state=111)\n\n##seperating dependent and independent variables\n\ncols = [i for i in loans.columns if i not in Id_col + target_col + exclude]\nX_train = train[cols]\ny_train = train[target_col].values.ravel()\nX_test = test[cols]\ny_test = test[target_col].values.ravel()","cd28b2d4":"y_train","5611ecba":"X_train=preprocessing.scale(X_train)\nX_test=preprocessing.scale(X_test)","ed5b49a3":"def print_conf_matrix(conf_mat):\n    import plotly.graph_objects as go\n    fig = go.Figure(data=go.Heatmap(\n        z=conf_mat,\n        x=['Actual: Not Loan', ' Actual: Loan'],\n        y=['Predict: Not Loan', 'Predict: Loan'],\n        showscale=False,\n        colorscale='Rainbow',\n        name='matrix',\n        xaxis='x2',\n        yaxis='y2',\n        ))\n    fig.show()","6e6acff5":"from sklearn.neighbors import KNeighborsClassifier\ndef knn_classifier(k):\n    knn = KNeighborsClassifier(algorithm = 'auto', leaf_size = 30, metric = 'minkowski', metric_params = None, n_jobs = 1, n_neighbors = k, p = 2, weights = 'uniform')\n    knn.fit(X_train, y_train)\n    predictions = knn.predict(X_test)\n    probabilities = knn.predict_proba(X_test)\n    precision = precision_score(y_test, predictions)\n    accuracy = accuracy_score(y_test, predictions)\n    rs = recall_score(y_test, predictions)\n    f1 = f1_score(y_test, predictions)\n    print('f1_score  for k ={}  is  {}'.format(k, f1))\n#     print('Accuracy score for k ={}  is  {}'.format(k, accuracy))\nfor k in range(1, 20, 2):\n    knn_classifier(k)","df9c182e":"    from sklearn.neighbors import KNeighborsClassifier\n    knn = KNeighborsClassifier(algorithm = 'auto', leaf_size = 30, metric = 'minkowski', metric_params = None, n_jobs = 1, n_neighbors = 5, p = 2, weights = 'uniform')\n    knn.fit(X_train, y_train)\n    predictions = knn.predict(X_test)\n    print(classification_report(y_test, predictions))","f1de0adb":"## Logistic regression\nfrom sklearn.linear_model import LogisticRegression\nlog_reg = LogisticRegression(random_state=1,solver='liblinear')\nlog_reg.fit(X_train, y_train)\npredictions = log_reg.predict(X_test)\nprint(classification_report(y_test, predictions))","31e7c887":"from sklearn.naive_bayes import GaussianNB\ngb = GaussianNB()\ngb.fit(X_train, y_train)\npredictions = gb.predict(X_test)\nprint(classification_report(y_test, predictions))","a868a6b1":"from sklearn.tree import DecisionTreeClassifier\ndtc = DecisionTreeClassifier(max_depth=11)\ndtc.fit(X_train, y_train)\npredictions = dtc.predict(X_test)\nprint(classification_report(y_test, predictions))","40bde6c1":"from sklearn.neighbors import RadiusNeighborsClassifier\nrnc = RadiusNeighborsClassifier(radius=1.5,outlier_label =1)\nrnc.fit(X_train, y_train)\npredictions = rnc.predict(X_test)\nprint(classification_report(y_test, predictions))","e102ae21":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import cohen_kappa_score\n\n#gives model report in dataframe\ndef model_report(model,name) :\n    model.fit(X_train,y_train)\n    predictions  = model.predict(X_test)\n    accuracy     = accuracy_score(y_test,predictions)\n    recallscore  = recall_score(y_test,predictions)\n    precision    = precision_score(y_test,predictions)\n    roc_auc      = roc_auc_score(y_test,predictions)\n    f1score      = f1_score(y_test,predictions) \n    kappa_metric = cohen_kappa_score(y_test,predictions)\n    \n    df = pd.DataFrame({\"Model\"           : [name],\n                       \"Accuracy_score\"  : [accuracy],\n                       \"Recall_score\"    : [recallscore],\n                       \"Precision\"       : [precision],\n                       \"f1_score\"        : [f1score],\n                       \"Area_under_curve\": [roc_auc],\n                       \"Kappa_metric\"    : [kappa_metric],\n                      })\n    return df\n\n#outputs for every model\nmodel1 = model_report(knn,\"KNN Classifier\")\nmodel2 = model_report(log_reg,\"LogisticRegr\")\nmodel3 = model_report(gb,\"GaussianNB\")\nmodel4 = model_report(dtc,\"Decision Tree\")\nmodel5 = model_report(rnc,\"RNC\")\n#concat all models\nmodel_performances = pd.concat([model1,model2,model3,\n                                model4,model5],axis = 0).reset_index()\n\nmodel_performances = model_performances.drop(columns = \"index\",axis =1)\n\ntable  = ff.create_table(np.round(model_performances,4))\n\npy.iplot(table)","1de67f75":"model_performances\ndef output_tracer(metric,color) :\n    tracer = go.Bar(y = model_performances[\"Model\"] ,\n                    x = model_performances[metric],\n                    orientation = \"h\",name = metric ,\n                    marker = dict(line = dict(width =.7),\n                                  color = color)\n                   )\n    return tracer\n\nlayout = go.Layout(dict(title = \"Model performances\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"metric\",\n                                     zerolinewidth=1,\n                                     ticklen=5,gridwidth=2),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                        margin = dict(l = 250),\n                        height = 780\n                       )\n                  )\n\n\ntrace1  = output_tracer(\"Accuracy_score\",\"#6699FF\")\ntrace2  = output_tracer('Recall_score',\"red\")\ntrace3  = output_tracer('Precision',\"#33CC99\")\ntrace4  = output_tracer('f1_score',\"lightgrey\")\ntrace5  = output_tracer('Kappa_metric',\"#FFCC99\")\n\ndata = [trace1,trace2,trace3,trace4,trace5]\nfig = go.Figure(data=data,layout=layout)\npy.iplot(fig)\n","8dcf57aa":"from sklearn.ensemble import BaggingClassifier\n","c3bd5f75":"k_bgc = BaggingClassifier(base_estimator=KNeighborsClassifier(),n_estimators=25,max_samples=.7,oob_score=True,random_state=1)\nk_bgc.fit(X_train,y_train)\npredictions=k_bgc.predict(X_test)\nprint(f1_score(y_test,predictions))\nprint(classification_report(y_test, predictions))","a98b1e82":"lr_bgc = BaggingClassifier(base_estimator=LogisticRegression(random_state=1,solver='liblinear'),n_estimators=25,max_samples=.7,oob_score=True,random_state=1)\nlr_bgc.fit(X_train,y_train)\npredictions=lr_bgc.predict(X_test)\nprint(f1_score(y_test,predictions))\nprint(classification_report(y_test, predictions))","e33e2b59":"gb_bgc = BaggingClassifier(base_estimator=GaussianNB(),n_estimators=25,max_samples=.7,oob_score=True,random_state=1)\ngb_bgc.fit(X_train,y_train)\npredictions=gb_bgc.predict(X_test)\nprint(f1_score(y_test,predictions))\nprint(classification_report(y_test, predictions))","9a94ba3b":"dt_bgc = BaggingClassifier(base_estimator=DecisionTreeClassifier(),n_estimators=25,max_samples=.7,oob_score=True,random_state=1)\ndt_bgc.fit(X_train,y_train)\npredictions=dt_bgc.predict(X_test)\nprint(f1_score(y_test,predictions))\nprint(classification_report(y_test, predictions))","1cf28de2":"from sklearn.ensemble import AdaBoostClassifier","0c0e38ad":"lr_abc = AdaBoostClassifier(base_estimator=LogisticRegression(random_state=1,solver='liblinear'),random_state=1,learning_rate=.5,n_estimators=75)\nlr_abc.fit(X_train,y_train)\npredictions = lr_abc.predict(X_test)\nprint(f1_score(y_test,predictions))\nprint(classification_report(y_test, predictions))","3c75921b":"gb_abc = AdaBoostClassifier(base_estimator=GaussianNB(),random_state=1,learning_rate=.5,n_estimators=75)\ngb_abc.fit(X_train,y_train)\npredictions = gb_abc.predict(X_test)\nprint(f1_score(y_test,predictions))\nprint(classification_report(y_test, predictions))","3713c751":"dtc_abc = AdaBoostClassifier(base_estimator=DecisionTreeClassifier(),random_state=1,learning_rate=.5,n_estimators=75)\ndtc_abc.fit(X_train,y_train)\npredictions = dtc_abc.predict(X_test)\nprint(f1_score(y_test,predictions))\nprint(classification_report(y_test, predictions))","4913d028":"abc = AdaBoostClassifier(random_state=1,learning_rate=.5,n_estimators=75)\nabc.fit(X_train,y_train)\npredictions = abc.predict(X_test)\nprint(f1_score(y_test,predictions))\nprint(classification_report(y_test, predictions))","4c893ac1":"from sklearn.ensemble import GradientBoostingClassifier","5228287c":"gbc = GradientBoostingClassifier(n_estimators=50,random_state=50)\ngbc.fit(X_train,y_train)\npredictions = gbc.predict(X_test)\nprint(f1_score(y_test,predictions))\nprint(classification_report(y_test, predictions))","d98e89fb":"from sklearn.ensemble import VotingClassifier","1d6d3070":"clf1= GradientBoostingClassifier(n_estimators=50,random_state=50)\nclf2=AdaBoostClassifier(base_estimator=dtc,random_state=1,learning_rate=.5,n_estimators=75)\nclf3=LogisticRegression(random_state=1,solver='liblinear')\nclf4=DecisionTreeClassifier(max_depth=11)\nclf5=RadiusNeighborsClassifier(radius=1.5,outlier_label =1)\n\nvc = VotingClassifier(estimators=[  ('log_reg', clf3), ('dtc', clf4), ('rnvc', clf5),('gb',clf1),('adab',clf2)],voting='hard')\nvc.fit(X_train,y_train)\npredictions= vc.predict(X_test)\nprint(f1_score(y_test,predictions))\nprint(classification_report(y_test, predictions))","93c53961":"from sklearn.ensemble import RandomForestClassifier\nrc = RandomForestClassifier(n_estimators =75,random_state=1)\nrc.fit(X_train,y_train)\npredictions = rc.predict(X_test)\nprint(f1_score(y_test,predictions))\nprint(classification_report(y_test, predictions))","76badbd9":"import lightgbm as lgb\nlgb_train = lgb.Dataset(X_train, y_train)","e437332c":"params = {\n 'task': 'train'\n , 'boosting_type': 'gbdt'\n , 'objective':  'multiclass'\n , 'num_class': 2\n , 'metric':  'multi_logloss'\n , 'min_data': 1\n , 'verbose': -1\n}\n \ngbm = lgb.train(params, lgb_train, num_boost_round=50)","40f0b237":"predictions = gbm.predict(X_test)\npredictions_classes = []\nfor i in predictions:\n  predictions_classes.append(np.argmax(i))\npredictions_classes = np.array(predictions_classes)\nprint(classification_report(predictions_classes, y_test))","21a67158":"import xgboost  as xgb","5a4c5aac":"D_train = xgb.DMatrix(X_train, label=y_train)\nD_test = xgb.DMatrix(X_test, label=y_test)\n\n\nparam = {\n    'eta': 0.3, \n    'max_depth': 3,  \n    'objective': 'multi:softprob',  \n    'num_class': 3} \n\nsteps = 20  # The number of training iterations\n\n\n\nmodel = xgb.train(param, D_train, steps)\n\n\n\npreds = model.predict(D_test)\nbest_preds = np.asarray([np.argmax(line) for line in preds])\n\n# print(\"Precision = {}\".format(precision_score(y_test, best_preds, average='macro')))\n# print(\"Recall = {}\".format(recall_score(y_test, best_preds, average='macro')))\n# print(\"Accuracy = {}\".format(accuracy_score(y_test, best_preds)))\n# print(\"f1 = {}\".format(f1_score(y_test, best_preds)))\nprint(classification_report(y_test,best_preds))","56a75619":"from sklearn.metrics import f1_score\nfrom sklearn.metrics import cohen_kappa_score\n\n#gives model report in dataframe\ndef model_report(model,name) :\n    model.fit(X_train,y_train)\n    predictions  = model.predict(X_test)\n    accuracy     = accuracy_score(y_test,predictions)\n    recallscore  = recall_score(y_test,predictions)\n    precision    = precision_score(y_test,predictions)\n    roc_auc      = roc_auc_score(y_test,predictions)\n    f1score      = f1_score(y_test,predictions) \n    kappa_metric = cohen_kappa_score(y_test,predictions)\n    \n    df = pd.DataFrame({\"Model\"           : [name],\n                       \"Accuracy_score\"  : [accuracy],\n                       \"Recall_score\"    : [recallscore],\n                       \"Precision\"       : [precision],\n                       \"f1_score\"        : [f1score],\n                       \"Area_under_curve\": [roc_auc],\n                       \"Kappa_metric\"    : [kappa_metric],\n                      })\n    return df\n\n#outputs for every model\nmodel1 = model_report(k_bgc,\"bag_KNN\")\nmodel2 = model_report(lr_bgc,\"bag_LR\")\nmodel3 = model_report(gb_bgc,\"bag_GB\")\nmodel4 = model_report(dt_bgc,\"bag_DT\")\nmodel5 = model_report(lr_abc,\"ab_LR\")\nmodel6 = model_report(gb_abc,\"ab_GB\")\nmodel7 = model_report(dtc_abc,\"ab_DT\")\nmodel8 = model_report(abc,\"ab_default\")\nmodel9 = model_report(gbc,\"GBC\")\nmodel10 = model_report(vc,\"VC\")\nmodel11 = model_report(rc,\"RFC\")\n# model12 = model_report(gbm,\"LGB\")\n# model13 = model_report(model,\"XGB\")\n#concat all models\nens_performances = pd.concat([model1,model2,model3,\n                                model4,model5,model6,\n\t\t\t\t\t\t\t\tmodel7,model8,model9,\n\t\t\t\t\t\t\t\tmodel10,model11],axis = 0).reset_index()\n\nens_performances = ens_performances.drop(columns = \"index\",axis =1)\n\ntable  = ff.create_table(np.round(ens_performances,4))\n\npy.iplot(table)","0dadc441":"model_performances\ndef output_tracer(metric,color) :\n    tracer = go.Bar(y = ens_performances[\"Model\"] ,\n                    x = ens_performances[metric],\n                    orientation = \"h\",name = metric ,\n                    marker = dict(line = dict(width =.7),\n                                  color = color)\n                   )\n    return tracer\n\nlayout = go.Layout(dict(title = \"Model performances\",\n                        plot_bgcolor  = \"rgb(243,243,243)\",\n                        paper_bgcolor = \"rgb(243,243,243)\",\n                        xaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     title = \"metric\",\n                                     zerolinewidth=1,\n                                     ticklen=5,gridwidth=2),\n                        yaxis = dict(gridcolor = 'rgb(255, 255, 255)',\n                                     zerolinewidth=1,ticklen=5,gridwidth=2),\n                        margin = dict(l = 250),\n                        height = 780\n                       )\n                  )\n\n\ntrace1  = output_tracer(\"Accuracy_score\",\"#6699FF\")\ntrace2  = output_tracer('Recall_score',\"red\")\ntrace3  = output_tracer('Precision',\"#33CC99\")\ntrace4  = output_tracer('f1_score',\"lightgrey\")\ntrace5  = output_tracer('Kappa_metric',\"#FFCC99\")\n\ndata = [trace1,trace2,trace3,trace4,trace5]\nfig = go.Figure(data=data,layout=layout)\npy.iplot(fig)\n","80210a8f":"## <a id='8.1.4.'>8.1.4. DecisionTreeClassifier<\/a>","b10aa7ee":"## <a id='8.5'>8.5. lightgbm<\/a>","aeb8061f":"## <a id='8.6'>8.6. xgboost<\/a>","299f8fd2":"## Inference:\n\nThere seems to be a better cutoff of mortgage greater than 300 and income greater than 100, we can target these customers for loans campaign","2ab75e5f":"## <a id='4.2.'>4.2. Age and Income in Personal Loan<\/a>","60f7a5b2":"# <a id='6.'>6. Model Building<\/a>","1879e4ef":"## Inference:\nCCAVG > 3 there is higher chance of personal loan and there are better chances on family sizes 3 & 4.\nThey can be targetted for campain","1d74ad2c":"## <a id='8.2.1.1.'>8.2.1.1. LogisticRegression<\/a>","5d0b9698":"## <a id='8.2.1.2.'>8.2.1.2. GaussianNB<\/a>","2342aa2d":"## Inference:\nits very evident that person with a family greater than 2 and income greater than 100 are very much likey to be taking a personal loan and they can be targeted for campaign","905df325":"## <a id='8.4'>8.4. RandomForestClassifier<\/a>","141f7e33":"## <a id='4.6.'>4.6. Income and Mortgage in Personal Loan<\/a>","efb3eb35":"##  <a id='3.4'>3.4.Performing ANOVA for Family and Loan<\/a>","ef1e8e0b":"## <a id='8.1.1.'>8.1.1. KNeighborsClassifier<\/a>","ee53d4c8":"## <a id='4.5.'>4.5. Family and Mortgage in Personal Loan<\/a>","02bcb536":"## <a id='8.2.1.3.'>8.2.1.3. DecisionTreeClassifier<\/a>","d6211884":"## Inference:\nData distribution of both the classes are similar across all numerical variables","4dc812ce":"## Inference:\nMortgage and family size doesnt really explain much on personal loan","7d3f9061":"## Inference:\n\nBased on the above table, as we can chose the best model based on business scenario.\n\nas a bank, if I have to chose between these models I would go with the one with best f1 score as data is imbalanced. \n### Decision Tree would be good choice followed by KNN","9e987765":"## <a id='6.1.1.'>6.1.1.Finding best K value<\/a>","231e332a":"## <a id='6.2.'>6.2. LogisticRegression Model<\/a>","ad7f3d0b":"# Campaign for selling personal loans.\n\nThis case is about a bank (Thera Bank) which has a growing customer base. Majority of these customers are liability customers (depositors) with varying size of deposits. The number of customers who are also borrowers (asset customers) is quite small, and the bank is interested in expanding this base rapidly to bring in more loan business and in the process, earn more through the interest on loans. In particular, the management wants to explore ways of converting its liability customers to personal loan customers (while retaining them as depositors). A campaign that the bank ran last year for liability customers showed a healthy conversion rate of over 9% success. This has encouraged the retail marketing department to devise campaigns with better target marketing to increase the success ratio with minimal budget.\n\nThe department wants to build a model that will help them identify the potential customers who have higher probability of purchasing the loan. This will increase the success ratio while at the same time reduce the cost of the campaign.\n\nThe file Bank.xls contains data on 5000 customers. The data include customer demographic information (age, income, etc.), the customer's relationship with the bank (mortgage, securities account, etc.), and the customer response to the last personal loan campaign (Personal Loan). Among these 5000 customers, only 480 (= 9.6%) accepted the personal loan that was offered to them in the earlier campaign.","747aa668":"## <a id='6.5.'>6.5. RadiusNeighborsClassifier Model<\/a>","506d617d":"## <a id='4.8.'>4.8. Distribution of Categorical Variables across Loan and Non Loan Cusotmers<\/a>","2e8d9fd9":"## <a id='6.0.'>6.0. Functions to be used across all models<\/a>","aa0df33c":"## Inferences:\n1. The distribution of target column is not uniform. \n   The dataset is imbalanced and we cannot rely only on accuracy score of models to evaluate performance.\n   We will use f1 score to measure different models performance\n2. Experience Column has -ve values and to be corrected\n3. Highly skewed Income and Mortgage columns need to check for outliers\n4. No Missing values in any columns","85ea85bf":"## <a id='1.1'>1.1. Data Overview<\/a>","9964e102":"# <a id='7.'>7. Model Performances <\/a>\n## <a id='7.1'>7.1. Model Performance Metrics<\/a>","ff8d7eda":"## Inference:\nat k=5 we have better score and hence we will use this in our model","160638a9":"##  <a id='3.1'>3.1. Distribution check on Numerical Features<\/a>","3f4a9537":"## <a id='6.4.'>6.4. DecisionTreeClassifier Model<\/a>","2635ea8e":"# <a id='2'>2. Data Manipulation<\/a>","7d55e0f9":"## <a id='8.2.2.'>8.2.2. GradientBoostingClassifier<\/a>","27846b39":"## <a id='4.9.'>4.9. Distribution of Numerical Features across Loan and Non Loan Cusotmers<\/a>","5a9a13b2":"## Inference\nNone of the numerical columns are Normally distributed and for models to perform better we have to perform Normalization","99b29072":"# <a id='1'>1.Data<\/a>","ba93f72e":"# <a id='8.'>8. Ensemble Methods <\/a>\n## <a id='8.1'>8.1. Bagging Classifier<\/a>","051d6173":"##  <a id='3.2'>3.2. Dependancy check across Features<\/a>","ad342184":"## <a id='4.4.'>4.4. Family and Income in Personal Loan<\/a>","b6e0a2c6":"## <a id='8.1.2.'>8.1.2. LogisticRegression<\/a>","8cde8328":"KNeighborsClassifier doesn't support sample_weight, hence cannot be used with adaboost","6285dccf":"## Inference\nAge and Experience are alomost linear and they dont give much info on personal loans","695c7d90":"## <a id='8.2'>8.2. Boosting Methods<\/a>\n## <a id='8.2.1.'>8.2.1. AdaBoostClassifier<\/a>","21bb26c8":"# <a id='3'>3. Statistical Tests<\/a>","c813450b":"# <a id='4'>4. Exploratory Data Analysis<\/a>","894a50b9":"## <a id='6.1.2.'>6.1.2. KNN Model<\/a>","a4af381a":"## Inference:\nthere are many cases where if income is above 100, then personal loans are taken, There could be 2 reasons\n1. people with lesser incomes are not preffering loans \n\nor\n2. people withe lesser incomes were not sanctioned loans\n\nSo for marketing campaign, it would be better to target people with higher incomes are there will be more sucess rate","df20d17f":"## <a id='4.1'>4.1. Personal loans distribution in data<\/a>","f851efc7":"## <a id='5.1.'>5.1. Data Split into Test and Train<\/a>","47ec39da":"# Index:\n\n- <a href='#1'>1. Data<\/a>\n    - <a href='#1.1'>1.1. Data overview<\/a>\n    - <a href='#1.2'>1.2. Profile Report of data for better understanding<\/a>\n- <a href='#2'>2. Data Manipulation<\/a>\n- <a href='#3'>3. Statistical Tests<\/a>\n    - <a href='#3.1'>3.1. Distribution check on Numerical Features<\/a>\n    - <a href='#3.2'>3.2. Dependancy check across Features<\/a>\n    - <a href='#3.3'>3.3.Performing an Independent t-test for Income and Loan<\/a>\n    - <a href='#3.4'>3.4.Performing ANOVA for Family and Loan<\/a>\n- <a href='#4'>4. Exploratory Data Analysis<\/a>    \n    - <a href='#4.1'>4.1. Personal loans distribution in data<\/a>\n    - <a href='#4.2.'>4.2. Age and Income in Personal Loan<\/a>\n    - <a href='#4.3.'>4.3. Age and Experience in Personal Loan<\/a>\n    - <a href='#4.4.'>4.4. Family and Income in Personal Loan<\/a>\n    - <a href='#4.5.'>4.5. Family and Mortgage in Personal Loan<\/a>\n    - <a href='#4.6.'>4.6. Income and Mortgage in Personal Loan<\/a>\n    - <a href='#4.7.'>4.7. CCAvg and Family in Personal Loan<\/a>\n    - <a href='#4.8.'>4.8. Distribution of Categorical Variables across Loan and Non Loan Cusotmers<\/a>\n\t- <a href='#4.9.'>4.9. Distribution of Numerical Features across Loan and Non Loan Cusotmers<\/a>\n- <a href='#5'>5. Data Preprocessing<\/a>\n\t- <a href='#5.1.'>5.1. Data Split into Test and Train<\/a>\n\t- <a href='#5.2.'>5.2. Normalise the data<\/a>\n- <a href='#6.'>6. Model Building<\/a>\n\t- <a href='#6.0.'>6.0. Functions to be used across all models<\/a>\n\t- <a href='#6.1.'>6.1. KNN Classifier<\/a>\n\t\t- <a href='#6.1.1.'>6.1.1.Finding best K value<\/a>\n\t\t- <a href='#66.1.2.'>6.1.2. KNN Model<\/a>\n\t- <a href='#6.2.'>6.2. LogisticRegression Model<\/a>\n\t- <a href='#6.3.'>6.3. GaussianNB Model<\/a>\n\t- <a href='#6.4.'>6.4. DecisionTreeClassifier Model<\/a>\n\t- <a href='#6.5.'>6.5. RadiusNeighborsClassifier Model<\/a>\n- <a href='#7.'>7. Model Performances <\/a>\n\t- <a href='#7.1'>7.1. Model Performance Metrics<\/a>\n\t- <a href='#7.2'>7.2. Compare Model Metrics<\/a>\n- <a href='#8.'>8. Ensemble Methods <\/a>\n\t- <a href='#8.1'>8.1. Bagging Classifier<\/a>\n\t\t- <a href='#8.1.1.'>8.1.1. KNeighborsClassifier<\/a>\n\t\t- <a href='#8.1.12.'>8.1.2. LogisticRegression<\/a>\t\n\t\t- <a href='#8.1.3.'>8.1.3. GaussianNB<\/a>\n\t\t- <a href='#8.1.4.'>8.1.4. DecisionTreeClassifier<\/a>\n\t- <a href='#8.2'>8.2. Boosting Methods<\/a>\n\t\t- <a href='#8.2.1.'>8.2.1. AdaBoostClassifier<\/a>\n\t\t\t- <a href='#8.2.1.1.'>8.2.1.1. LogisticRegression<\/a>\n\t\t\t- <a href='#8.2.1.2.'>8.2.1.2. GaussianNB<\/a>\n\t\t\t- <a href='#8.2.1.3.'>8.2.1.3. DecisionTreeClassifier<\/a>\n\t\t\t- <a href='#8.2.1.4.'>8.2.1.4. default AdaBoostClassifier<\/a>\n\t- <a href='#8.2.2.'>8.2.2. GradientBoostingClassifier<\/a>\n\t- <a href='#8.3'>8.3. VotingClassifier<\/a>\n\t- <a href='#8.4'>8.4. RandomForestClassifier<\/a>\n\t- <a href='#8.5'>8.5. lightgbm<\/a>\n\t- <a href='#8.6'>8.6. xgboost<\/a>\n- <a href='#9.'>9. Ensemble Performances <\/a>\n\t- <a href='#9.1'>9.1. Ensemble Performance Metrics<\/a>\n\t- <a href='#9.2'>9.2. Compare Ensemble Metrics<\/a>","01aecfb5":"# <a id='5'>5. Data Preprocessing<\/a>","eee850cc":"## <a id='9.'>9. Ensemble Performances <\/a>\n## <a id='9.1'>9.1. Ensemble Performance Metrics<\/a>\n","b3e0cf9d":"## Inference:\nall the distributions are identical except for CD Account","3ddf5923":"## <a id='8.2.1.4.'>8.2.1.4. default AdaBoostClassifier<\/a>","235458ca":"## <a id='7.2'>7.2. Compare Model Metrics<\/a>","e1b48e79":"## <a id='6.3.'>6.3. GaussianNB Model<\/a>","477343eb":"## <a id='4.7.'>4.7. CCAvg and Family in Personal Loan<\/a>","bbdcbc6d":"## <a id ='9.2'>9.2. Compare Ensemble Metrics<\/a>","9b9c3cec":"## <a id='8.3'>8.3. VotingClassifier<\/a>","8db9901a":"## <a id='1.2'>1.2. Profile Report of data for better understanding<\/a>","7837b198":"##  <a id='3.3'>3.3.Performing an Independent t-test for Income and Loan<\/a>","2f62f2c0":"## Inferences\n1. ZIP_Code column is not important for our modelling and can be removed from features\n2. Family size is not impacting mortgage\n3. Age is not contributing to Personal Loan\n4. all others combinations we saw are related and we can use this info for modelling purposes","22b13c8f":"## <a id='8.1.3.'>8.1.3. GaussianNB<\/a>","d99c2c9c":"## <a id='4.3.'>4.3. Age and Experience in Personal Loan<\/a>","f341165e":"## <a id='5.2.'>5.2. Normalise the data<\/a>","e7421843":"## <a id='6.1.'>6.1. KNN Classifier<\/a>"}}