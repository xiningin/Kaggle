{"cell_type":{"5788e891":"code","7919dc93":"code","3c8097d8":"code","ef410cdf":"code","e9cd12b0":"code","d4a6e317":"code","481af586":"code","58a78a50":"code","a0efaa35":"code","098388e5":"code","531f8d6c":"code","de3bf1de":"code","b0004326":"code","7ed68807":"code","c759b20f":"code","de285398":"code","711ad403":"code","c9ab9573":"code","e8b00f42":"code","97b77569":"code","2bbb6711":"code","ae1fa640":"code","84e2ea23":"code","37dce139":"code","f694b2be":"code","5ad4b6d5":"code","6ed6d3b4":"code","3e929e13":"code","cd3f25b4":"code","5387bcdd":"code","144e356a":"code","17ce2113":"code","a297959c":"code","d8b67962":"code","3bdfa37c":"code","6307c996":"code","c5391f1e":"code","91824edc":"code","28351225":"code","e49e98df":"markdown","71c3e591":"markdown","fb3e8e95":"markdown","3dc648ce":"markdown","daa4f7ea":"markdown","c0f0e6e9":"markdown","c7dbc958":"markdown","29210166":"markdown","92ad2ad4":"markdown","f7b6837b":"markdown","b8061ef3":"markdown","ebadc6f4":"markdown","43ef6a95":"markdown","956d7ed8":"markdown","e6bd58e8":"markdown","ff328901":"markdown","e2f9eba0":"markdown","87658c1b":"markdown","cdf90637":"markdown","008dc87b":"markdown","0117b167":"markdown","b1a92f28":"markdown","2bd2f54f":"markdown","0142d421":"markdown","15861120":"markdown","33e6ec78":"markdown","db5383ca":"markdown","469ef36b":"markdown","f309eb19":"markdown","5dcfd0e4":"markdown","3f18b1bb":"markdown","e9ae3bfd":"markdown"},"source":{"5788e891":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","7919dc93":"from sklearn import metrics as metrics\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import plot_confusion_matrix\n\ndef metricas(y_train,y_pred_train,y_test,y_pred_test):\n    valores=y.value_counts().index.to_list()\n    \n    # Matriz de confusion: Train\n    cm_train=metrics.confusion_matrix(y_train,y_pred_train,labels=valores)\n    df_cm=pd.DataFrame(cm_train,index=valores,columns=valores)\n    plt.figure(figsize=(8,5))\n    sns.heatmap(df_cm,annot=True,cmap=\"YlGnBu\")\n    plt.title('Matriz de Confusi\u00f3n: Train')\n    plt.xlabel('Predicci\u00f3n')\n    plt.ylabel('Valores Reales')\n    plt.show()\n    \n    # Matriz de confusion: Test\n    cm_test=metrics.confusion_matrix(y_test,y_pred_test,labels=valores)\n    df_cm=pd.DataFrame(cm_test,index=valores,columns=valores)\n    plt.figure(figsize=(8,5))\n    sns.heatmap(df_cm,annot=True,cmap=\"YlGnBu\")\n    plt.title('Matriz de Confusi\u00f3n: Test')\n    plt.xlabel('Predicci\u00f3n')\n    plt.ylabel('Valores Reales')\n    plt.show()\n    \n    accuracy_train=metrics.accuracy_score(y_train,y_pred_train)\n    accuracy_test=metrics.accuracy_score(y_test,y_pred_test)\n    precision_train=metrics.precision_score(y_train,y_pred_train,average='micro')\n    precision_test=metrics.precision_score(y_test,y_pred_test,average='micro')\n    recall_train=metrics.recall_score(y_train,y_pred_train,average='micro')\n    recall_test=metrics.recall_score(y_test,y_pred_test,average='micro')\n    f_score=f1_score(y_test,y_pred_test,average='micro')\n    \n    train = (accuracy_train*100, precision_train*100, recall_train*100)\n    test = (accuracy_test*100, precision_test*100, recall_test*100)\n\n    ind = np.arange(3)  # the x locations for the groups\n    ind_n = np.arange(4)  # the x locations for the groups\n    width = 0.3       # the width of the bars\n    \n    fig = plt.figure(figsize = (8,5))\n    ax = fig.add_subplot(111)\n    \n    rects1 = ax.bar(ind, train, width, color='r')\n    rects2 = ax.bar(ind+width, test, width, color='g')\n    rects3 = ax.bar(3, f_score*100, width, color='b')\n    \n    ax.set_ylabel('Scores')\n    ax.set_xticks(ind_n + width\/2)\n    ax.set_xticklabels( ('Accuracy', 'Precisi\u00f3n', 'Recall', 'F1 Score') )\n    ax.legend( (rects1[0], rects2[0]), ('Train', 'Test') )\n    \n    def autolabel(rects):\n        for rect in rects:\n            h = rect.get_height()\n            ax.text(rect.get_x()+rect.get_width()\/2., 1.00*h, '%.3f'%round(h,3),\n                    ha='center', va='bottom')\n\n    autolabel(rects1)\n    autolabel(rects2)\n    autolabel(rects3)\n    plt.title('Puntajes')\n    plt.ylim(0,120)\n    plt.show()\n    \n    return ","3c8097d8":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px","ef410cdf":"df = pd.read_csv('\/kaggle\/input\/glass\/glass.csv')\ndf","e9cd12b0":"df.shape","d4a6e317":"df.dtypes","481af586":"for i in df.columns:\n    print(\"*\"*20)\n    print(i)\n    print(df[i].sort_values().unique())","58a78a50":"datos_x = df.Type.value_counts().index.to_list()\ndatos_y = df.Type.value_counts().to_list()\nsuma = df.Type.value_counts().sum()\nprint(df.Type.value_counts())","a0efaa35":"list_color=['grey','black','orange','green','blue','red','red']\ngraph = plt.bar(datos_x, datos_y, color=list_color)\nplt.title('Distribuci\u00f3n de datos del Target')\n\ni = 0\n\nfor p in graph:\n    width = p.get_width()\n    height = p.get_height()\n    x, y = p.get_xy()\n    plt.text(x+width\/2,\n             y+height*1.01,\n             str(height)+\" -> \"+str(round(height\/suma,2))+'%',\n             ha='center',\n             weight='bold')\n    i+=1\n\nplt.show()","098388e5":"df.describe().T","531f8d6c":"filas=len(df.columns.to_list())\nc=1\nfig=plt.figure(figsize=(25,7*filas))\n    \nfor i,j in enumerate(df.columns.to_list()):\n    plt.subplot(filas,2, c)\n    sns.distplot(df[j])\n    c = c + 1\n    \n    plt.subplot(filas,2, c)\n    ax1=sns.boxplot(x=df[j],palette=\"Blues\",linewidth=1)\n    c = c + 1\n\nplt.show()","de3bf1de":"for i in df.columns:\n    plt.figure(figsize=(10,4))\n    sns.boxplot(x='Type', y=i, data=df)\n    plt.title(\"Type VS \"+i)\n    plt.show()","b0004326":"plt.figure(figsize=(10,6))\nsns.heatmap(df.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","7ed68807":"fig = px.scatter_3d(df, x='Mg', y='Al', z='Ba',\n              color='Type')\nfig.show()","c759b20f":"df.isnull().sum()","de285398":"df_fi = df.copy()","711ad403":"plt.figure(figsize=(10,6))\nsns.heatmap(df_fi.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","c9ab9573":"df_fi['nuevo'] = df_fi['Mg']*df_fi['Al']+df_fi['Ba']","e8b00f42":"from sklearn.decomposition import PCA\n\npca = PCA()\npca.fit(df.drop('Type',axis=1))\npca.explained_variance_ratio_","97b77569":"for i in range(len(pca.components_)):\n    print('% Var. explicada ('+str(i+1)+' componentes): ', np.cumsum(pca.explained_variance_ratio_)[i]*100)\n    \nplt.bar(range(1,len(pca.components_)+1),pca.explained_variance_ratio_, alpha=.2,color='red')\nplt.plot(range(1,len(pca.components_)+1),np.cumsum(pca.explained_variance_ratio_),alpha=4,color='blue')\nplt.title(\"Varianza explicada y pareto\")\nplt.show()","2bbb6711":"pcaFin = PCA(n_components=3)\npcaFin.fit(df.drop('Type',axis=1))\npd.DataFrame(pcaFin.components_,columns=df.drop('Type',axis=1).columns.to_list())","ae1fa640":"X = pcaFin.transform(df.drop('Type',axis=1))\nX = pd.DataFrame(X)\nX.head()","84e2ea23":"df_pca = X.copy()\ndf_pca['Target'] = df['Type']","37dce139":"sns.pairplot(df_pca, hue='Target', palette='hls')\nplt.show()","f694b2be":"fig = px.scatter_3d(df_pca, x=0, y=1, z=2,\n              color='Target')\nfig.show()","5ad4b6d5":"X = df_pca.drop('Target', axis=1)\ny = df_pca.Target","6ed6d3b4":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size=0.2,\n                                                    stratify=y)","3e929e13":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.model_selection import GridSearchCV\n\nparameters = {'n_estimators':[300,400,500,600,700,800],\n              'max_depth':[5,6,7,8,9,10] ,\n              'n_jobs':[-1],\n              'max_features':[1,2]}\ngrid_search = GridSearchCV(estimator=RandomForestClassifier(),\n             param_grid=parameters)","cd3f25b4":"grid_search.fit(X_train,y_train)","5387bcdd":"best_param= grid_search.best_params_\nbest_param","144e356a":"best_model = grid_search.best_estimator_","17ce2113":"y_pred_train=best_model.predict(X_train)\ny_pred_test= best_model.predict(X_test)\nmetricas(y_train,y_pred_train,y_test,y_pred_test)","a297959c":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression(multi_class='auto')\nlr.fit(X_train,y_train)\ny_pred_train=lr.predict(X_train)\ny_pred_test= lr.predict(X_test)\nmetricas(y_train,y_pred_train,y_test,y_pred_test)","d8b67962":"from sklearn.tree import DecisionTreeClassifier\ntree = DecisionTreeClassifier()\ntree_model = tree.fit(X_train,y_train)\ny_pred_train = tree_model.predict(X_train)\ny_pred_test = tree_model.predict(X_test)\nmetricas(y_train,y_pred_train,y_test,y_pred_test)","3bdfa37c":"from sklearn.ensemble import RandomForestClassifier\n\"\"\"\nrf = RandomForestClassifier(n_estimators=500,     # Numero de arboles\n                            max_features=3,       # Numero de variables por arbol\n                            min_samples_leaf=15,  # Numero de obs por nodo hoja\n                            min_samples_split=40) # Numero de obs por nodo hoja\n\"\"\"\nrf = RandomForestClassifier(n_estimators=100)\nrf.fit(X_train, y_train)\ny_pred_train = tree_model.predict(X_train)\ny_pred_test = tree_model.predict(X_test)\nmetricas(y_train,y_pred_train,y_test,y_pred_test)","6307c996":"from sklearn.ensemble import AdaBoostClassifier\nAdaBoost=AdaBoostClassifier(learning_rate=0.001, n_estimators=250)\nAdaBoost.fit(X_train, y_train)\ny_pred_train = AdaBoost.predict(X_train)\ny_pred_test = AdaBoost.predict(X_test)\nmetricas(y_train,y_pred_train,y_test,y_pred_test)","c5391f1e":"import xgboost\nxgb = xgboost.XGBClassifier()\nxgb.fit(X_train, y_train)\ny_pred_train = xgb.predict(X_train)\ny_pred_test = xgb.predict(X_test)\nmetricas(y_train,y_pred_train,y_test,y_pred_test)","91824edc":"from sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier()\nclassifier.fit(X_train, y_train)\ny_pred_train = classifier.predict(X_train)\ny_pred_test = classifier.predict(X_test)\nmetricas(y_train,y_pred_train,y_test,y_pred_test)","28351225":"from sklearn.ensemble import GradientBoostingClassifier\n\ngbm = GradientBoostingClassifier(learning_rate=0.01, n_estimators=1500,max_depth=4, \n                                    min_samples_split=2, min_samples_leaf=1, subsample=1,\n                                    max_features='sqrt', random_state=10)\ngbm.fit(X_train,y_train)\ny_pred_train=gbm.predict(X_train)\ny_pred_test= gbm.predict(X_test)\nmetricas(y_train,y_pred_train,y_test,y_pred_test)","e49e98df":"# Pre-Funciones","71c3e591":"## 4.2. Algoritmos de Machine Learning","fb3e8e95":"Las pocas caracter\u00edsticas, que tienen poca correlaci\u00f3n y las pocas variables altamente correlacionadas. Nos conviene realizar un Feature Extraction con PCA.","3dc648ce":"## 3.1. Creaci\u00f3n de Caracter\u00edsticas","daa4f7ea":"# 3. Feature Engineering","c0f0e6e9":"# 2. Limpieza de Datos","c7dbc958":"### Tipos de datos de todas las caracter\u00edsticas\n\nTodas las variables son num\u00e9ricas","29210166":"En un pairplot de las 3 variables generadas por el PCA, podemos observar un mejor alejamiento de los tipos de glass. Esto favorece al algoritmo, para que pueda realizar una mejor predicci\u00f3n del modelo.","92ad2ad4":"Aqui observamos que al hacer un gr\u00e1fico de las 3 variables m\u00e1s correlacionadas al target, se forma una especie de alejamiento de los glass de tipo 7, un alejamiento en glass de tipo 6 y 5, y mucha junta de datos los datos en los glass de tipo 1, 2 y 3.","f7b6837b":"### Distribuci\u00f3n de datos del target","b8061ef3":"## 4.1. Partici\u00f3n Muestral","ebadc6f4":"Analizando los valores \u00fanicos podemos observar que las variables todas son de tipo num\u00e9ricas continuas, adem\u00e1s de que el target esta dividido en 7 tipos de glass.","43ef6a95":"Podemos observar que con las 3 variables de PCA, los diferentes tipos de glass se distancian a\u00fan m\u00e1s, lo que favorece a la predicci\u00f3n del algoritmo.","956d7ed8":"No encontramos datos nulos","e6bd58e8":"### Regresi\u00f3n Log\u00edstica","ff328901":"### XGBoost","e2f9eba0":"### GBM","87658c1b":"### AdaBoost","cdf90637":"### K-Nearest Neighbors","008dc87b":"# 1. AED","0117b167":"### \u00c1rboles de Decisi\u00f3n","b1a92f28":"## 3.2. Feature Extraction (PCA)","2bd2f54f":"### Veremos los valores \u00fanicos de cada variables","0142d421":"### Bosques Aleatorios","15861120":"### An\u00e1lisis gr\u00e1fico multivariado","33e6ec78":"Podemos observar solo hay 6 tipos de glass y que el cuarto tipo de glass no se encuentra.","db5383ca":"### Resumen estad\u00edstico de cada caracter\u00edstica","469ef36b":"## 2.1. Completitud de Datos","f309eb19":"# 4. Entrenamiento y Validaci\u00f3n","5dcfd0e4":"# Cargando Datos","3f18b1bb":"## GridSearchCV with RF","e9ae3bfd":"Observamos la importancia de las variables para las nuevas variables generadas por el PCA."}}