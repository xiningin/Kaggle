{"cell_type":{"d02d1468":"code","641511c2":"code","15084639":"code","e6c774b3":"code","41472c4f":"code","73d0f1bc":"code","30564bac":"code","f9829a73":"code","3cfef67d":"code","2dfddbb4":"code","fdde8570":"code","584b6ec9":"code","9c5a95d9":"code","8e153dde":"code","283dd192":"code","ea41c4be":"code","1c545441":"code","ec920706":"code","09ba6c01":"code","5cd79dc7":"code","e7d999e3":"code","659f94fe":"code","db43a6dd":"code","23705fc5":"code","8bbe2cbe":"code","ec752652":"code","dc4eb794":"code","02e2fcf7":"code","5de3a761":"code","30833fe0":"code","c613778a":"code","3c6e8279":"code","1f4632cb":"code","04f246b9":"code","3620261a":"code","3be5c6ab":"code","dbea0c82":"code","1873db9f":"code","5b4d510f":"code","cc58204f":"code","1c300ddc":"code","8dc7298f":"code","9863c5ae":"markdown","a259e896":"markdown","61a7cb7c":"markdown","55e6d29e":"markdown","cbf6ae2c":"markdown","d4082913":"markdown","6e5bb5aa":"markdown","9c5b5704":"markdown","90e12415":"markdown","1fcec615":"markdown","bb0be0b0":"markdown","12e6f5f1":"markdown","ce7072b5":"markdown","a6dd9f9d":"markdown","4b1ca039":"markdown","9ae90596":"markdown","ba40cb3f":"markdown","63960644":"markdown","7e6f36f9":"markdown","9569388b":"markdown","bbee009f":"markdown","2c473ca8":"markdown","1fea4a9d":"markdown","8144763a":"markdown","d8fe749b":"markdown","87d77444":"markdown","6eb81a2d":"markdown","9e61af03":"markdown","4a268867":"markdown","9da9a502":"markdown","a1a18085":"markdown","c990c93c":"markdown","0e946385":"markdown","17463e64":"markdown","97f5c0d7":"markdown","5742a732":"markdown","100b01ac":"markdown","d8639b7a":"markdown","59c3fb19":"markdown"},"source":{"d02d1468":"!pip install torch","641511c2":"import torch","15084639":"c = torch.tensor(1, dtype=torch.float32)\nprint(f\"c: shape {c.shape} - dtype: {c.dtype} - device: {c.device}\")\nprint(c)","e6c774b3":"v = torch.tensor([1.0, 2.0, 3.0], dtype=torch.int32)\nprint(f\"v: shape {v.shape} - dtype: {v.dtype} - device: {v.device}\")\nprint(v)","41472c4f":"A = torch.tensor([[1, 2, 3, 4], [3, 4, 5, 6], [6, 7, 8, 9]])\nA = A.cuda()\nprint(f\"A: shape {A.shape} - dtype: {A.dtype} - device: {A.device}\")\nprint(A)","73d0f1bc":"data = torch.tensor([[1, 2], [4, 5]])\nprint(data)","30564bac":"data = torch.zeros(5)\nprint(data)","f9829a73":"# difference from NumPy:\n#     * NumPy: data = np.ones((3, 3)) (i.e. we pass a tuple as argument)\n#     * PyTorch: data = torch.tensor(3, 3) (i.e. we pass two integers as arguments)\ndata = torch.ones(3, 3)\nprint(data)","3cfef67d":"data = torch.arange(5)\nprint(data)","2dfddbb4":"# CPU tensor initially\ndata = torch.ones(3, 3)\nprint(data.device)","fdde8570":"# GPU tensor\ndata = data.cuda()\nprint(data.device)","584b6ec9":"# GPU tensor\ndata = data.to(\"cuda:0\")\nprint(data.device)","9c5a95d9":"data = data.detach().cpu()\nprint(data.device)","8e153dde":"data = torch.ones(3, 3).cuda(0)\nprint(data.device)","283dd192":"with torch.cuda.device(0):\n    data = torch.ones(3, 3).cuda()\n    print(data.device)","ea41c4be":"arr = torch.tensor([1, 2, 3])\nprint(f\"Original data type: {arr.dtype}\")","1c545441":"arr = arr.type(torch.FloatTensor)\nprint(f\"Converted data type: {arr.dtype}\")","ec920706":"arr = arr.float()\nprint(f\"Converted data type: {arr.dtype}\")","09ba6c01":"import time\nimport numpy as np\nimport torch\n\nx = np.random.randn(3000, 3000)\ny = np.random.randn(3000, 3000)\nstart = time.time()\nfor _ in range(100):\n    z = x + y\nprint(f\"NumPy time: {time.time() - start}\")\n\nx = torch.randn(3000, 3000)\ny = torch.randn(3000, 3000)\nstart = time.time()\nfor _ in range(100):\n    z = x + y\nprint(f\"PyTorch CPU time: {time.time() - start}\")\n\nx = torch.randn(3000, 3000).cuda()\ny = torch.randn(3000, 3000).cuda()\nstart = time.time()\nfor _ in range(100):\n    z = x + y\nprint(f\"PyTorch GPU time: {time.time() - start}\")","5cd79dc7":"import time\nimport numpy as np\nimport torch\n\nx = np.random.randn(3000, 3000)\ny = np.random.randn(3000, 3000)\nstart = time.time()\nfor _ in range(100):\n    z = x * y\nprint(f\"NumPy time: {time.time() - start}\")\n\nx = torch.randn(3000, 3000)\ny = torch.randn(3000, 3000)\nstart = time.time()\nfor _ in range(100):\n    z = x * y\nprint(f\"PyTorch CPU time: {time.time() - start}\")\n\nx = torch.randn(3000, 3000).cuda()\ny = torch.randn(3000, 3000).cuda()\nstart = time.time()\nfor _ in range(100):\n    z = x * y\nprint(f\"PyTorch GPU time: {time.time() - start}\")","e7d999e3":"import time\nimport numpy as np\nimport torch\n\nx = np.random.randn(3000, 3000)\ny = np.random.randn(3000, 3000)\nstart = time.time()\nfor _ in range(100):\n    z = np.exp(x) + np.exp(y)\nprint(f\"NumPy time: {time.time() - start}\")\n\nx = torch.randn(3000, 3000)\ny = torch.randn(3000, 3000)\nstart = time.time()\nfor _ in range(100):\n    z = torch.exp(x) + torch.exp(y)\nprint(f\"PyTorch CPU time: {time.time() - start}\")\n\nx = torch.randn(3000, 3000).cuda()\ny = torch.randn(3000, 3000).cuda()\nstart = time.time()\nfor _ in range(100):\n    z = torch.exp(x) + torch.exp(y)\nprint(f\"PyTorch GPU time: {time.time() - start}\")","659f94fe":"import time\nimport numpy as np\nimport torch\n\nx = np.random.randn(3000, 3000)\ny = np.random.randn(3000, 3000)\nstart = time.time()\nfor _ in range(100):\n    z = np.log(x) + np.log(y)\nprint(f\"NumPy time: {time.time() - start}\")\n\nx = torch.randn(3000, 3000)\ny = torch.randn(3000, 3000)\nstart = time.time()\nfor _ in range(100):\n    z = torch.log(x) + torch.log(y)\nprint(f\"PyTorch CPU time: {time.time() - start}\")\n\nx = torch.randn(3000, 3000).cuda()\ny = torch.randn(3000, 3000).cuda()\nstart = time.time()\nfor _ in range(100):\n    z = torch.log(x) + torch.log(y)\nprint(f\"PyTorch GPU time: {time.time() - start}\")","db43a6dd":"import time\nimport numpy as np\nimport torch\n\nx = np.random.randn(3000, 3000)\ny = np.random.randn(3000, 3000)\np = 10\nstart = time.time()\nfor _ in range(100):\n    z = np.power(x, p) + np.power(y, p)\nprint(f\"NumPy time: {time.time() - start}\")\n\nx = torch.randn(3000, 3000)\ny = torch.randn(3000, 3000)\nstart = time.time()\nfor _ in range(100):\n    z = torch.pow(x, p) + torch.pow(y, p)\nprint(f\"PyTorch CPU time: {time.time() - start}\")\n\nx = torch.randn(3000, 3000).cuda()\ny = torch.randn(3000, 3000).cuda()\nstart = time.time()\nfor _ in range(100):\n    z = torch.pow(x, p) + torch.pow(y, p)\nprint(f\"PyTorch GPU time: {time.time() - start}\")","23705fc5":"x = torch.randn(1, requires_grad=True)\ny = 3 * x + 2\nz = 4 * y + 5","8bbe2cbe":"torch.autograd.backward(z)\nprint(f\"Gradient of z w.r.t x: {x.grad}\")","ec752652":"y_grad = torch.autograd.grad(z, y)\nprint(f\"Gradient of z w.r.t y: {y_grad}\")","dc4eb794":"x = torch.randn(2, requires_grad=True)\ny = 3 * x + 5","02e2fcf7":"torch.autograd.backward(y, grad_tensors=torch.ones_like(y))\nprint(x.grad)","5de3a761":"x_grad = torch.autograd.grad(y, x, grad_outputs=torch.ones_like(y))\nprint(x_grad)","30833fe0":"with torch.autograd.no_grad():\n    ...","c613778a":"with torch.autograd.enable_grad():\n    ...","3c6e8279":"with torch.autograd.set_grad_enabled(True):\n    ...","1f4632cb":"tensor.backward()","04f246b9":"tensor.backward(gradient=)","3620261a":"tensor.retain_grad()","3be5c6ab":"tensor.grad\ntensor.is_leaf\ntensor.requires_grad","dbea0c82":"class Exp(Function):\n    @staticmethod\n    def forward(ctx, i):\n        result = i.exp()\n        ctx.save_for_backward(result)\n        return result\n    \n    @staticmethod\n    def backward(ctx, grad_output):\n        result, = ctx.saved_tensors\n        return grad_output * result","1873db9f":"import torch\nimport numpy as np\n\n# step 1: generate 2000 random values of X from standard Gaussian\nX = np.random.randn(2000)\n# step 2: generate 2000 noises from standard Gaussian\nnoise = np.random.randn(2000)\n# step 3: generate 2000 values of Y\nY = 3*X+2+noise","5b4d510f":"import matplotlib.pyplot as plt\n\nplt.plot(X, Y, \"o\", alpha=0.2)\nplt.title(\"Data points\")\nplt.xlabel(\"X\")\nplt.ylabel(\"Y\")\nplt.show()","cc58204f":"def make_prediction(X, W, b):\n    # write the formula of f(x) in torch\n    Y_preds = W*X+b\n    return Y_preds","1c300ddc":"def compute_loss(Y, Y_preds):\n    # write the formula of the loss L in torch\n    loss = torch.mean((Y - Y_preds)**2)\n    return loss","8dc7298f":"from matplotlib.animation import FuncAnimation\n\n# convert training data to GPU tensors\nX_train = torch.from_numpy(X).cuda()\nY_train = torch.from_numpy(Y).cuda()\n\n# initialization\nW = torch.randn(1).cuda()\nW.requires_grad = True\n\nb = torch.randn(1).cuda()\nb.requires_grad = True\n\nmax_iter = 300\nalpha = 1e-2\nfor i in range(max_iter):\n    # predict and compute loss\n    Y_preds = W*X_train+b\n    loss = torch.mean((Y_train - Y_preds)**2)\n    \n    # compute gradients with option 1 (APPLIED IN SPECIAL CASES ONLY)\n    dL_dW, dL_db = torch.autograd.grad(loss, [W, b])\n    \n    # compute gradients with option 2 (MORE COMMON)\n    # W.grad = torch.zeros_like(W)\n    # b.grad = torch.zeros_like(b)\n    # loss.backward()\n    # dL_dW, dL_db = W.grad, b.grad\n\n    # update\n    with torch.no_grad():\n        W -= alpha * dL_dW\n        b -= alpha * dL_db\n    \n    # visualize result\n    if i % 10 == 0:\n        print(f\"Step {i} - loss: {loss}\")\n        Y_preds = make_prediction(X_train, W, b)\n        plt.plot(X, Y, \"o\", alpha=0.2)\n        plt.plot(X, Y_preds.detach().cpu().numpy())\n        plt.show()\n\nprint(f\"Optimized parameters: W: {W} - b: {b}\")","9863c5ae":"**Tensor autograd**:\n* Functions","a259e896":"# CAUTION\nIf you don't know something, Google it!\n\n# References\n* [PyTorch's Tensor documentation](https:\/\/pytorch.org\/docs\/stable\/tensors.html)\n* [PyTorch's documentation](https:\/\/pytorch.org\/docs\/stable\/torch.html)\n* [PyTorch' autograd documentation](https:\/\/pytorch.org\/docs\/stable\/autograd.html)","61a7cb7c":"* Option 1: use method `cuda()` to convert the tensor from CPU tensor to GPU tensor","55e6d29e":"* Option 2: use `with` keyword\n    * Usage: when we want to re-allocate too many tensors, we should put all of them into a code block","cbf6ae2c":"# Torch tensor\n## Introduction to `torch.tensor`\n**`torch.tensor`**: a N-dimensional array object, just as ndarray (NumPy), which is the key features of PyTorch's tensor computing\n* Advantages: \n    * Fast, flexible container for datasets in Python\n    * Allow us to perform mathematical operations\n    * Allow GPU acceleration on operations **(most crucial)**\n\n\n**Basic attributes of a `torch.tensor`**:\n* `tensor.shape`: return shape of the array\n* `tensor.dtype`: return the data type of the data in the array\n* `tensor.device`: return the device (e.g. CPU, GPU, etc.) where the tensor lies in\n* `tensor.requires_grad`: return whether we can compute the gradient w.r.t the tensor automatically or not\n\n**Example code**:\n* Create one array representing a scalar $c = 1$ with type `float32`","d4082913":"* Create one array representing a matrix $\\textbf{A} = \\begin{bmatrix} 1 & 2 & 3 & 4 \\\\ 3 & 4 & 5 & 6 \\\\ 6 & 7 & 8 & 9 \\end{bmatrix}$ whose type is inferred by PyTorch and is stored in **GPU**","6e5bb5aa":"* Attributes","9c5b5704":"* Method: `torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=False)`\n    * Description: computes and returns the sum of gradients of outputs w.r.t. the inputs\n    * Arguments:\n        * `outputs` (sequence of Tensor): outputs of the differentiated function\n        * `inputs` (sequence of Tensor): inputs w.r.t which the gradient will be returned","90e12415":"Copyright &copy; 2019 COTAI. All rights reserved.","1fcec615":"**Allocate tensors to the desired GPU**:\n* Option 1: use the parameter `device` in `cuda()`\n    * Usage: when we want to re-allocate some tensors only ","bb0be0b0":"**Power**","12e6f5f1":"**Backward pass**:\n* Core formula of gradient descent: $W^{(t)} = W^{(t-1)} - \\alpha \\cdot \\frac{\\partial L}{\\partial W^{(t-1)}}$\n\n**Overall pipeline**:","ce7072b5":"## Creating `torch.tensor`\n### CPU tensors\nThese tensors are created just like NumPy's ndarrays\n\n**Create pre-specified elements**: we can create the matrix $\\begin{bmatrix} 1 & 2 \\\\ 4 & 5 \\end{bmatrix}$ with the code below","a6dd9f9d":"**Jacobian-vector product**:\n* Problem: if any of tensors are non-scalar (i.e. their data has more than one element) and require gradient, then the Jacobian-vector product would be computed\n    * Solution: in this case the function additionally requires specifying `grad_tensors`. \n* Requirements on the `grad_tensors`:\n    * It should be a sequence of matching length, that contains the \u201cvector\u201d in the Jacobian-vector product, usually the gradient of the differentiated function w.r.t. corresponding tensors \n    \n    >**NOTE**: `None` is an acceptable value for all tensors that don\u2019t need gradient tensors","4b1ca039":"## GPU tensors\n**From CPU tensor to GPU tensor**:\nConsider the following tensor `data`","9ae90596":"# Auto-differentiation with PyTorch\n## Computation graph\n\n<image src=\"https:\/\/miro.medium.com\/max\/2994\/1*vPb9E0Yd1QUAD0oFmAgaOw.png\" width=\"500px\">\n\n**Computation graph** is the core concept around the auto-differentiation packages for optimization like TensorFlow or PyTorch.\n* Computation graph: a graph whose:\n    * Nodes: represent operations, variables, or placeholders (i.e. the place where we feed model inputs to the graph)\n    * Edges: represent data, or multi-dimensional arrays (i.e. Tensors) which flow through the different operations\n\n**TensorFlow's idea**:\n* Step 1: to construct a computational graph first and compile the graph\n* Step 2: for each execution, an input is fed into the graph\n* Step 3: the calculations are carried out throughout the graph\n* Step 4: the execution returns an output, which is produced by the computational graph\n\n**Dynamic graph - PyTorch**: for each pass, PyTorch construct the computation graph from scratch.\n* Advantages: easy-to-use, easy-to-debug, and easyto-understand\n* Disadvantages: slower computation since we have to initialize the graph for every single run\n\n## `torch.autograd`\n**`torch.autograd`**\n\n`torch.autograd` provides classes and functions implementing automatic differentiation of arbitrary scalar-valued functions without significantly change the existing code. \n\nAll we have to do is delcare `torch.tensor`, for which gradients should be computed, with `requires_grad=True` keyword, then use the implemented methods of `torch.autograd`\n\n**How PyTorch derivates its variables**: the graph is differentiated using the chain rule. \n\n**Backward passing with `torch.autograd`**: \n\nConsider the function below:\n* $y = 3 x + 2$\n* $z = 4 y + 5$","ba40cb3f":"* Option 2: use method `to(device_id)` to allocate the tensor in the desired device","63960644":"**Custom function with autograd**:","7e6f36f9":"* Create an array with elements ranging from $0$ to $4$ (i.e. $\\begin{bmatrix} 0 & 1 & 2 & 3 & 4 \\end{bmatrix}$)","9569388b":"* Create an array with all ones (e.g. $\\begin{bmatrix} 1 & 1 & 1 \\\\ 1 & 1 & 1 \\\\ 1 & 1 & 1 \\end{bmatrix}$)","bbee009f":"**Logarithm**","2c473ca8":"**Context manager for gradient requirements**:","1fea4a9d":"* Import PyTorch","8144763a":"* Option 2: use `arr.float()` if we want to cast `arr.dtype` to `torch.float32` (or `arr.long()` if we want to cast `arr.dtype` to `torch.int64`)","d8fe749b":"# Basic operations\nAll of the basic operations with `torch.tensor` can be found at the two links given in the *References* part. \n\nThe core ideas of these operations are the same as NumPy's idea, thus no need to go into details here. Please refer to [the NumPy lecture](https:\/\/colab.research.google.com\/drive\/1gLaygnaxiPE5KssySFjvFLUuy8OwGhKm) for details.","87d77444":"**From GPU tensor to CPU tensor**:\n* Step 1: get the detached version (from the corresponding current computation graph) of the tensor\n* Step 2: convert the tensor to CPU tensor","6eb81a2d":"# Introduction to PyTorch\n## PyTorch\n**PyTorch**: short for Python Torch, an open-source machine learning library based on the Torch library, which is developed mainly by Facebook AI research (FAIR)\n\n**Utilities provided by PyTorch**:\n* Tensor computing (like NumPy) with strong acceleration via GPUs\n* Deep neural networks built on tape-based auto-differentiation system\n\n## Why PyTorch\n**TensorFlow**: the most famous deep learning package recently, but people are biased towards using PyTorch. \n\nThis is due to the fact that TF is very hard to use, debug, and understand. Also, TF gives its user ability to contribute to the package with loose approvement, this makes the package much more messy than PyTorch. Even the `BatchNormalization` method has 3 implementations in TensorFlow!\n\nMost of recent advances in deep learning are all implemented in PyTorch. The only reason that TF is still there is that TF supports cross-platform deep learning models.\n\n**PyTorch**: provide an easy-to-use, easy-to-debug, and fast-computing deep learning pakage. PyTorch can be used as GPU-ed NumPy for basic computations, or can be used for optimization (broader than just deep learning) with its built-in auto-differentiation\n\n## Setup\n* Install PyTorch","9e61af03":"* Method: `torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None)`\n    * Description: Computes the sum of gradients of given tensors w.r.t. graph leaves\n    * Arguments:\n        * `tensors` (sequence of Tensors): tensors of which the derivative will be computed\n        * `retain_graph` (bool, optional): if False, the grpah used to compute the grad will be freed (dynamic graph), otherwise the graph will be retained (fixed graph)\n        * `create_graph` (bool optional): if True, graph of the derivative will be constructed, allowing to compute higher-order derivative products\n","4a268867":"* Create one array representing a row vector $\\textbf{v} = \\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix}$ with type `int32`","9da9a502":"## Revisit linear regression with PyTorch\nPlease revisit [this link](https:\/\/colab.research.google.com\/drive\/13b4BaolG7zGcRAsV5BX2tW7Pfz-O-Ilp) to review linear regression to understand the cells below\n\n**Forward pass**:\n* Step 1: generate random data","a1a18085":"# Performance comparison\nIn this section, we compare the performance of various operations between NumPy and PyTorch\n\n**Summation**:","c990c93c":"# Indexing and slicing\nSimilar to NumPy. Please refer to [this link](https:\/\/colab.research.google.com\/drive\/1gLaygnaxiPE5KssySFjvFLUuy8OwGhKm)","0e946385":"If we want to convert `arr.dtype` to `torch.float32`, we have two options:\n* Option 1: use the constructor of `torch.float32` (i.e. `torch.FloatTensor(arr)`) if we want to cast `arr.dtype` to `torch.float32`","17463e64":"**Create some special arrays**:\n* Create an array with all zeros (e.g. $\\begin{bmatrix} 0 & 0 & 0 & 0 & 0 \\end{bmatrix}$)","97f5c0d7":"**Exponentiation**:","5742a732":"* Step 2: formulate $f(x; W, b) = W \\cdot x + b$","100b01ac":"**Multiplication**:","d8639b7a":"**Other options for creating a `torch.tensor`**:\n\n| Function | Description |\n| --- | --- |\n| `tensor` | Convert input data (list, tuple, etc.) to a tensor. The input data is copied by default |\n| `from_numpy` | Convert NumPy array to `torch.tensor` |\n| `as_tensor` | Convert input to `torch.tensor` without copying if the input is an `torch.tensor` already |\n| `full` | Produce a `torch.tensor` whose elements are the same and equal to some given value, given shape and dtype |\n| `ones`, `ones_like` | Produce a `torch.tensor` of ones given shape and dtype |\n| `zeros`, `zeros_like` | Produce a `torch.tensor` of ones given shape and dtype |\n| `empty`, `empty_like` | Create new `torch.tensor` by allocating new memory with random values |\n| `eye` | Create a square $N \\times N$ matrix |\n\n**Creating a tensor with `torch.Tensor`**:\n\n| Function | Description |\n| --- | --- |\n| `new_tensor` | Return a new Tensor with initial tensor data |\n| `new_full` | Return a tensor filled with some value |\n| `new_empty` | Return tensor filled with uninitialized data |\n\n**Tensor attribute**:\n\n| Attribute | Description |\n| --- | --- |\n| `is_cuda` |  |\n| `device` |  |\n| `grad` |  |\n| `ndim` |  |\n| `T` |  |\n\n# Data types for `torch.tensor`\n**Data types**:\n* Float:\n\n| dtype | CPU tensor constructor |  GPU tensor constructor |\n| --- | --- | --- |\n| `torch.float32` or `torch.float` | `torch.FloatTensor` | `torch.cuda.FloatTensor` |\n| `torch.float64` or `torch.double` | `torch.DoubleTensor` | `torch.cuda.DoubleTensor` |\n| `torch.float16` or `torch.half` | `torch.HalfTensor` | `torch.cuda.HalfTensor` |\n\n* Integer:\n\n| dtype | CPU tensor constructor |  GPU tensor constructor |\n| --- | --- | --- |\n| `torch.uint8` | `torch.ByteTensor` | `torch.cuda.ByteTensor` |\n| `torch.int8` | `torch.CharTensor` | `torch.cuda.CharTensor` |\n| `torch.int16` or `torch.short` | `torch.ShortTensor` | `torch.cuda.ShortTensor` |\n| `torch.int32` or `torch.int` | `torch.IntTensor` | `torch.cuda.IntTensor` |\n| `torch.int64` or `torch.long` | `torch.LongTensor` | `torch.cuda.LongTensor` |\n\n* Bool:\n\n| dtype | CPU tensor constructor |  GPU tensor constructor |\n| --- | --- | --- |\n| `torch.bool` | | `torch.BoolTensor` | `torch.cuda.BoolTensor` |\n\n**Data type casting**: consider the array `arr` $= \\begin{bmatrix} 1 & 2 & 3 \\end{bmatrix}$","59c3fb19":"* Step 3: define the loss function $L = \\frac{1}{N} \\sum_{i=1}^N [y_i - f(x_i; W, b)]^2$"}}