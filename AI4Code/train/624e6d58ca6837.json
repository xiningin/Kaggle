{"cell_type":{"b865f5c7":"code","f0d753d0":"code","41bbfbbd":"code","4a285d91":"code","db0fe7d3":"code","b651add5":"code","a6947432":"code","8eb3c55a":"code","3ab2d4ef":"code","8b116dd1":"code","749cffeb":"code","ad0d8be5":"code","e1ea0f4a":"code","e016e79e":"code","008ecb19":"code","9a5ed212":"code","031c5260":"code","e4504982":"code","537bac61":"code","c1152146":"code","634a1dab":"code","7c4b5b41":"code","0ad05c90":"code","22238c40":"code","04026258":"code","e2e85196":"code","57f84dd2":"code","f18940bc":"code","8579cf65":"code","af53b98d":"code","945cd7d7":"code","f27f624f":"code","6be395a5":"code","c28156dd":"code","327dfd9f":"code","b138fef1":"code","64722842":"code","b73496f8":"code","017cb26b":"code","cf988551":"code","f425b6ce":"code","ebfddb23":"code","08dff0e5":"code","a7a0a203":"code","faf55fbd":"code","05b942e4":"code","e858a121":"code","a1c0e2de":"code","a28802ef":"code","68f8b103":"code","9274fd6d":"code","eaf748f7":"code","12133220":"code","8caedee8":"code","c534e897":"code","4e6ee1cd":"code","b4c7876e":"code","176fea1e":"code","f551e9b8":"code","f2f87141":"code","97345ea7":"code","f9e45c4d":"code","59f3bfb8":"markdown","92e4a803":"markdown","7e2a71bc":"markdown","ae9a3880":"markdown","3febdfde":"markdown","f97ec4e5":"markdown","12f18a7b":"markdown","6ad9fffa":"markdown","68e9050f":"markdown","4b0ff354":"markdown","5eb5a74c":"markdown","4e0f3855":"markdown","bf579fa9":"markdown","1153cd54":"markdown","d5c67e17":"markdown","3032f538":"markdown","32690347":"markdown","28e27c2b":"markdown","4defed43":"markdown","100d0168":"markdown","970b0cff":"markdown","6d6921fe":"markdown","b38d04b2":"markdown","9d0e93d1":"markdown","c1449151":"markdown"},"source":{"b865f5c7":"!pip -q install --upgrade pip\n!pip -q install --upgrade seaborn","f0d753d0":"import gc\nimport catboost\nimport lightgbm\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import LabelEncoder\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","41bbfbbd":"##### Functions\n\ndef get_denominator(serie):\n    \n    array_n   = np.array(serie)\n    uniques_n = np.unique(array_n[~np.isnan(array_n)])\n    \n    result  = None\n    for i in range(1,1000):\n        decimals = uniques_n*i - np.floor(uniques_n*i)\n\n        for decimal in decimals:\n            integer = []\n            if ((decimal < 0.001) | (decimal > 0.999)):\n                integer.append(True)\n            else:\n                integer.append(False)\n                break\n\n        if all(integer):\n            result = i\n            break\n\n    print('denominator', serie.name, ':', result)\n    \ndef get_fraud_weights(serie):\n    \n    head        = 10\n    values      = list(serie.value_counts(dropna=False).index[:head])\n    df_wg_fraud = pd.DataFrame(columns=[serie.name, 'count', 'weight fraud'])\n    \n    for v in values:\n        if (v == v): # is not nan\n            n = train.query('{} == \"{}\" & isFraud == 1'.format(serie.name, v)).shape[0]\n            c = train.query('{} == \"{}\"'.format(serie.name, v)).shape[0]\n        else:\n            n = train.query('{} != {} & isFraud == 1'.format(serie.name, serie.name)).shape[0]\n            c = train.query('{} != {}'.format(serie.name, serie.name)).shape[0]\n\n        w = n\/c\n        df_wg_fraud = df_wg_fraud.append({serie.name:v, 'count': c, 'weight fraud':w}, ignore_index = True)\\\n                      .sort_values('weight fraud', ascending=False)\n        \n    return df_wg_fraud.head(head)","4a285d91":"##### Download of files.\n\nprint('Downloading datasets...')\nprint(' ')\ntrain = pd.read_pickle('\/kaggle\/input\/1-ieee-cis-memory-reduction\/train_mred.pkl')\nprint('Train has been downloaded... (1\/2)')\ntest  = pd.read_pickle('\/kaggle\/input\/1-ieee-cis-memory-reduction\/test_mred.pkl')\nprint('Test has been downloaded... (2\/2)')\nprint('Done!')","db0fe7d3":"##### Info for the colums until R_emaildomain\n\ncolumns = train.iloc[:,:17]\ncolumns.info()","b651add5":"##### Main statistics\n\ncolumns.describe()","a6947432":"##### Number of nulls and percentages\n#dist1 greater than 90% of nulls\n\nprint('Number of NaNs in train (amount):\\n', columns.isnull().sum(), sep='')\nprint(' ')\nprint('Number of NaNs in train (% of total):\\n', columns.isnull().sum()\/train.shape[0], sep='')","8eb3c55a":"print('Is transaction unique?:', len(train.TransactionID.unique()) == train.shape[0]) # unique transaction id -> (checking that it is truly unique)","3ab2d4ef":"print('Fraud unbalance:', train.isFraud.value_counts()[0]\/train.shape[0]) # balance legit\/totals\nprint('Fraud correlations: \\n', abs(train.corrwith(train.isFraud)).head(10).sort_values(ascending = False), sep='')","8b116dd1":"print('TransactionDT min:', train.TransactionDT.min()) # From the discussions, 86400 id 60*60*24, a day in seconds\nprint('TransactionDT max:', train.TransactionDT.max()) \nprint('TransactionDT days (train):', round(train.TransactionDT.max()\/train.TransactionDT.min())) # Days in 6 months in train\nprint('TransactionDT days (test):', round((test.TransactionDT.max()-test.TransactionDT.min())\/train.TransactionDT.min())) # Days in 6 months in train","749cffeb":"print('TransactionAmt min:', train.TransactionAmt.min())\nprint('TransactionAmt max:', train.TransactionAmt.max())\nprint('TransactionAmt mean:', train.TransactionAmt.mean())\nprint('TransactionAmt correlations: \\n', abs(train.corrwith(train.TransactionAmt)).sort_values(ascending = False).head(10), sep='')\nprint('TransactionAmt fraud weights: \\n', get_fraud_weights(train.TransactionAmt), sep='')","ad0d8be5":"plt.figure(figsize=(12,6))\nsns.histplot(data=train, x = 'TransactionAmt', hue='isFraud', bins=40)\nplt.show()","e1ea0f4a":"print('ProductCD fraud weights: \\n', get_fraud_weights(train.ProductCD), sep='')","e016e79e":"plt.figure(figsize=(12,6))\nsns.countplot(x = train.ProductCD.values, hue= train.isFraud)\nplt.show()","008ecb19":"print('card1 values counts: \\n', train.card1.value_counts(dropna=False), sep='')\nprint('card1 fraud weights: \\n', get_fraud_weights(train.card1))\nprint('card1 correlations: \\n', abs(train.corrwith(train.card1)).sort_values(ascending = False).head(10))","9a5ed212":"plt.figure(figsize=(12,6))\nsns.histplot(data=train, x = 'card1', hue='isFraud')\nplt.show()","031c5260":"print('card2 value counts: \\n', train.card2.value_counts(dropna=False).head(10), sep='')\nprint('card2 fraud weights: \\n', get_fraud_weights(train.card2), sep='')\nprint('card2 correlations: \\n', abs(train.corrwith(train.card2)).sort_values(ascending = False).head(10), sep='')","e4504982":"plt.figure(figsize=(12,6))\nsns.histplot(data=train, x = 'card2', hue='isFraud')\nplt.show()","537bac61":"print('card3 value counts: \\n', train.card3.value_counts(dropna=False).head(10), sep='')\nprint('card3 fraud weights: \\n', get_fraud_weights(train.card3), sep='')\nprint('card3 correlations: \\n', abs(train.corrwith(train.card3)).sort_values(ascending = False).head(10), sep='')","c1152146":"plt.figure(figsize=(12,6))\nsns.histplot(data=train, x = 'card3', hue='isFraud')\nplt.show()","634a1dab":"print('card4 value counts: \\n', train.card4.value_counts(dropna=False).head(10), sep='')\nprint('card4 fraud weights: \\n', get_fraud_weights(train.card4), sep='')","7c4b5b41":"plt.figure(figsize=(12,6))\nsns.countplot(data=train, x = 'card4', hue='isFraud')\nplt.show()","0ad05c90":"print('card5 value counts: \\n', train.card5.value_counts(dropna=False).head(10), sep='')\nprint('card5 fraud weights: \\n', get_fraud_weights(train.card5), sep='')\nprint('card5 correlations: \\n', abs(train.corrwith(train.card5)).sort_values(ascending = False).head(10), sep='')","22238c40":"plt.figure(figsize=(12,6))\nsns.histplot(data=train, x = 'card5', hue='isFraud')\nplt.show()","04026258":"print('card6 value counts: \\n', train.card6.value_counts(dropna=False).head(10), sep='')\nprint('card6 fraud weights: \\n', get_fraud_weights(train.card6), sep='')","e2e85196":"plt.figure(figsize=(12,6))\nsns.countplot(data=train, x = 'card6', hue='isFraud')\nplt.show()","57f84dd2":"print('addr1 value counts: \\n', train.addr1.value_counts(dropna=False).head(10), sep='')\nprint('addr1 fraud weights: \\n', get_fraud_weights(train.addr1), sep='') # NaN values have 11,7 % fraud vs 3.5%\nprint('addr1 correlations: \\n', abs(train.corrwith(train.addr1).sort_values(ascending = False)).head(15), sep='') # Hight correlation","f18940bc":"plt.figure(figsize=(12,6))\nsns.histplot(data=train, x = 'addr1', hue='isFraud')\nplt.show()","8579cf65":"print('addr2 value counts: \\n', train.addr2.value_counts(dropna=False).head(10), sep='')\nprint('addr2 fraud weights: \\n', get_fraud_weights(train.addr2), sep='') # Country 65 has ~50% of fraud!!\nprint('addr2 correlations: \\n', abs(train.corrwith(train.addr2)).sort_values(ascending = False).head(15), sep='') # High correlation with card3","af53b98d":"plt.figure(figsize=(12,6))\nsns.histplot(data=train, x = 'addr2', hue='isFraud')\nplt.show()","945cd7d7":"##### Inspecting addr1 and addr2 Nans\n# When addr1 is null, then addr2 is null too (and the other way around). Try filling NaNs with -999.\n\nprint('addr1 null if addr not null (train):', train.query('addr1!=addr1 & addr2==addr2').shape)\nprint('addr2 null if add2 not null (train):', train.query('addr2!=addr2 & addr1==addr1').shape)\nprint('addr1 null if addr not null (test):' , test.query('addr1!=addr1 & addr2==addr2').shape)\nprint('addr2 null if add2 not null (test):' , test.query('addr2!=addr2 & addr1==addr1').shape)","f27f624f":"# NaNs to be filled with the mode of the card, the rest with unknown and then to -999\n\nprint('P_emaildomain value counts: \\n', train.P_emaildomain.value_counts(dropna=False).head(10), sep='')\nprint('P_emaildomain fraud weights: \\n:', get_fraud_weights(train.P_emaildomain), sep='')","6be395a5":"plt.figure(figsize=(12,6))\nsns.countplot(data=train, x = 'P_emaildomain', hue='isFraud')\nplt.xticks(rotation=90)\nplt.show()","c28156dd":"# NaNs to be filled with the mode of the card, the rest with unknown and then to -999\n\nprint('R_emaildomain value counts: \\n', train.R_emaildomain.value_counts(dropna=False).head(10), sep='')\nprint('R_emaildomain fraud weights: \\n:', get_fraud_weights(train.R_emaildomain), sep='')","327dfd9f":"plt.figure(figsize=(12,6))\nsns.countplot(data=train, x = 'R_emaildomain', hue='isFraud')\nplt.xticks(rotation=90)\nplt.show()","b138fef1":"print('dist1 value counts: \\n', train.dist1.value_counts(dropna=False).head(10), sep='')\nprint('dist1 fraud weights: \\n', get_fraud_weights(train.dist1), sep='')\nprint('dist1 correlations: \\n', train.corrwith(train.dist1).sort_values(ascending = False).head(15), sep='')","64722842":"plt.figure(figsize=(12,6))\nsns.histplot(data=train, x = 'dist1', hue='isFraud', bins=35)\nplt.show()","b73496f8":"print('dist2 value counts: \\n', train.dist2.value_counts(dropna=False).head(10), sep='')\nprint('dist2 fraud weights: \\n', get_fraud_weights(train.dist2), sep='')\nprint('dist2 correlations: \\n', train.corrwith(train.dist2).sort_values(ascending = False).head(15), sep='')","017cb26b":"plt.figure(figsize=(12,6))\nsns.histplot(data=train, x = 'dist2', hue='isFraud', bins=35)\nplt.show()","cf988551":"# All are in days, except D8 and probably D9. D8, D9 look they are in hours, not in days.\n\nfor col in list(train.iloc[:,31:46]):\n    print(col, ':', len(train[col].value_counts()))\n    \nprint('#' * 50)    \nget_denominator(train['D8'])\nget_denominator(train['D9'])","f425b6ce":"# D6, D7, D8, D9, D12, D13, D14 about 90% nulls\n\nprint(train.iloc[:,31:46].describe())\nprint()\nprint('D null count: \\n', train.iloc[:,31:46].isnull().sum()\/train.shape[0], sep='')","ebfddb23":"plt.figure(figsize=(10,10))\nsns.heatmap(train.iloc[:,31:46].corr(), annot=True, fmt='.2g', cmap='Reds')\nplt.show()","08dff0e5":"#D3 days from the previous card transaction, D5 and D7 looks something similar. D3 and D5 quite similar. Probably we will remove D7, but due to it gets 93% of NaNs.\n\ntrain.iloc[:,31:46].query('D3==D3 & D5==D5 & D7==D7 & D5!=D7')[['D3', 'D5', 'D7']].head(10)","a7a0a203":"##### Adding the Dxachr columns to perform the validation instead of de original Ds. These columns already come from the binaries loaded.\n\ntrain_til_M = train.iloc[:,:55].copy()\ntrain_til_M = train_til_M.drop('isFraud', axis=1)\ntrain_til_M['is_train'] = 1\n\ntrain_til_M = pd.concat([train_til_M, train.iloc[:,-10:]], axis=1)\n\ntest_til_M = test.iloc[:,:54].copy()\ntest_til_M['is_train'] = 0\ntest_til_M = pd.concat([test_til_M, test.iloc[:,-10:]], axis=1)\n\ntrain_test_til_M = pd.concat([train_til_M, test_til_M], axis=0, ignore_index=True)\n\ndel train_til_M\ndel test_til_M\n\ngc.collect()","faf55fbd":"##### Basic Fill NaNs for LGBM: Get categories and preprocess with Label Encoder.\n\ncateg_cols = ['ProductCD', 'card4', 'card6', 'P_emaildomain', 'R_emaildomain']\ncateg_cols += ['M'+str(i) for i in range(1,10)]\n\n# Replace null by not_know\ntrain_test_til_M.loc[:,categ_cols] = train_test_til_M[categ_cols].fillna('not_know')\n\n# Label Encoder\nle = LabelEncoder()\nfor col in categ_cols:\n    le.fit(train_test_til_M[col])\n    train_test_til_M[col] = le.transform(train_test_til_M[col])","05b942e4":"##### Adversarial Validation - sample\n\nadversarial_val   = train_test_til_M.sample(200000, replace=False)\nadversarial_train = train_test_til_M[~train_test_til_M.index.isin(adversarial_val.index)]","e858a121":"##### Remove columns that we know relates to transaction and keep the rest.\n\nfeatures = list(train_test_til_M)\nfeatures.remove('TransactionID') \nfeatures.remove('TransactionDT')\nfeatures.remove('is_train')\n\n# Remove the original Dxs for those we have the engineered Dxahcrs\ncolumns_D = ['D' + str(i) for i in range(1,16) if i not in [3,5,7,8,9]]\nfor col in columns_D: \n    features.remove(col)\n    \n# Target definition\ntarget = 'is_train'\n\ndel train_test_til_M","a1c0e2de":"##### Adversarial validation with LGBM\n\ntrain_data    = lightgbm.Dataset(adversarial_train[features], label=adversarial_train[target], categorical_feature=categ_cols)\ntest_data     = lightgbm.Dataset(adversarial_val[features], label=adversarial_val[target], categorical_feature=categ_cols)\n\nparameters = {\n                    'objective':'binary', # in classification, binary or multiclass\n                    'boosting_type':'gbdt', # boosting type, gradient boosting decission trees, dart, goss. Behind XGBoost\n                    'metric':'auc',\n                    'n_jobs':-1,\n                    'learning_rate':0.05,\n                    'num_leaves': 2**8,\n                    'max_depth':-1,\n                    'tree_learner':'serial',\n                    'colsample_bytree': 0.7,\n                    'subsample_freq':1,\n                    'subsample':0.5,      #each tree will use this percentage of rows (randomly). Smaller, improves generalization and speed. Not too low, as we look for overfiting.\n                    'n_estimators':1000,  #number of trees. This mean that there will be N trees, that they will be executed one after another, unless early_stopping_rounds condition is met.\n                    'max_bin':255,\n                    'verbose':-1,\n                    'early_stopping_rounds':100, \n                } \n\nmodel = lightgbm.train(parameters,\n                       train_data,\n                       valid_sets=test_data, verbose_eval=200)","a28802ef":"##### AUC\n\nprint('AUC:',model.best_score.get('valid_0').get('auc'))","68f8b103":"##### Feature Importance\n\nfeatures_importance = model.feature_importance()\nfeatures_array = np.array(features)\nfeatures_array_ordered_i = features_array[(features_importance).argsort()[::-1]]\n\nplt.figure(figsize=(16,10))\nsns.barplot(y=features, x=features_importance, orient='h', order=features_array_ordered_i)\nplt.show()\n\ndel model","9274fd6d":"train_V = train.iloc[:,55:-11:].copy()\ntrain_V['is_train'] = 1\n\n\ntest_V = test.iloc[:,55:-11:].copy()\ntest_V['is_train'] = 0\n\ntrain_test_V = pd.concat([train_V, test_V], axis=0, ignore_index=True)\n\ndel train_V\ndel test_V\ngc.collect()","eaf748f7":"##### Adversarial Validation - sample\n\nadversarial_val   = train_test_V.sample(200000, replace=False)\nadversarial_train = train_test_V[~train_test_V.index.isin(adversarial_val.index)]\n\ndel train_test_V\n\n# Remove columns that we know relates to transaction and keep the rest\n\nfeatures = list(adversarial_train)\nfeatures.remove('is_train')\n\n# Target definition\n\ntarget = 'is_train'","12133220":"##### Adversarial validation with LGBM\n\ntrain_data    = lightgbm.Dataset(adversarial_train[features], label=adversarial_train[target])\ntest_data     = lightgbm.Dataset(adversarial_val[features],   label=adversarial_val[target])\n\nparameters = {\n                    'objective':'binary', # in classification, binary or multiclass\n                    'boosting_type':'gbdt', # boosting type, gradient boosting decission trees, dart, goss. Behind XGBoost\n                    'metric':'auc',\n                    'n_jobs':-1,\n                    'learning_rate':0.05,\n                    'num_leaves': 2**8,\n                    'max_depth':-1,\n                    'tree_learner':'serial',\n                    'colsample_bytree': 0.7,\n                    'subsample_freq':1,\n                    'subsample':0.5,      #each tree will use this percentage of rows (randomly). Smaller, improves generalization and speed. Not too low, as we look for overfiting.\n                    'n_estimators':1000,  #number of trees. This mean that there will be N trees, that they will be executed one after another, unless early_stopping_rounds condition is met.\n                    'max_bin':255,\n                    'verbose':-1,\n                    'early_stopping_rounds':100, \n                } \n\nmodel = lightgbm.train(parameters,\n                       train_data,\n                       valid_sets=test_data, verbose_eval=200)","8caedee8":"##### AUC\n\nprint('AUC:',model.best_score.get('valid_0').get('auc'))","c534e897":"##### Feature Importance\n\nfeatures_importance = model.feature_importance()\nfeatures_array = np.array(features)\nfeatures_array_ordered_v = features_array[(features_importance).argsort()[::-1]]\n\nplt.figure(figsize=(16,10))\nsns.barplot(y=features, x=features_importance, orient='h', order=features_array_ordered_v[:30])\nplt.show()","4e6ee1cd":"mixed_features_array = np.hstack([\n    features_array_ordered_i[:int(np.floor(len(features_array_ordered_i)\/2))],\n    features_array_ordered_v[:int(np.floor(len(features_array_ordered_v)\/2))]])","b4c7876e":"train_M = train[mixed_features_array].copy()\ntrain_M['is_train'] = 1\n\ntest_M = test[mixed_features_array].copy()\ntest_M['is_train'] = 0\n\ntrain_test_M = pd.concat([train_M, test_M], axis=0, ignore_index=True)\n\ndel train_M\ndel test_M\ndel train\ndel test\ngc.collect()","176fea1e":"categ_cols = list(train_test_M.dtypes[train_test_M.dtypes == 'O'].index)\n\ntrain_test_M.loc[:,categ_cols] = train_test_M[categ_cols].fillna('not_know')\n\nle = LabelEncoder()\nfor col in categ_cols:\n    le.fit(train_test_M[col])\n    train_test_M[col] = le.transform(train_test_M[col])","f551e9b8":"##### Adversarial Validation - sample\n\nadversarial_val   = train_test_M.sample(200000, replace=False)\nadversarial_train = train_test_M[~train_test_M.index.isin(adversarial_val.index)]\n\ndel train_test_M\n\n# Remove columns that we know relates to transaction and keep the rest\nfeatures = list(adversarial_train)\nfeatures.remove('is_train')\n\nfor col in columns_D: # Remove the original Dxs for those we have the engineered Dxahcrs\n    try:\n        features.remove(col)\n    except:\n        pass\n    \n# Target definition\ntarget = 'is_train'","f2f87141":"##### Adversarial validation with LGBM\n\ntrain_data    = lightgbm.Dataset(adversarial_train[features], label=adversarial_train[target], categorical_feature=categ_cols)\ntest_data     = lightgbm.Dataset(adversarial_val[features],   label=adversarial_val[target],   categorical_feature=categ_cols)\n\nparameters = {\n                    'objective':'binary', # in classification, binary or multiclass\n                    'boosting_type':'gbdt', # boosting type, gradient boosting decission trees, dart, goss. Behind XGBoost\n                    'metric':'auc',\n                    'n_jobs':-1,\n                    'learning_rate':0.05,\n                    'num_leaves': 2**8,\n                    'max_depth':-1,\n                    'tree_learner':'serial',\n                    'colsample_bytree': 0.7,\n                    'subsample_freq':1,\n                    'subsample':0.5,      #each tree will use this percentage of rows (randomly). Smaller, improves generalization and speed. Not too low, as we look for overfiting.\n                    'n_estimators':1000,  #number of trees. This mean that there will be N trees, that they will be executed one after another, unless early_stopping_rounds condition is met.\n                    'max_bin':255,\n                    'verbose':-1,\n                    'early_stopping_rounds':100, \n                } \n\nmodel = lightgbm.train(parameters,\n                       train_data,\n                       valid_sets=test_data, verbose_eval=200)","97345ea7":"##### AUC\n\nprint('AUC:',model.best_score.get('valid_0').get('auc'))","f9e45c4d":"##### Feature Importance\n\nfeatures_importance = model.feature_importance()\nfeatures_array = np.array(features)\nfeatures_array_ordered_v = features_array[(features_importance).argsort()[::-1]]\n\nplt.figure(figsize=(16,10))\nsns.barplot(y=features, x=features_importance, orient='h', order=features_array_ordered_v[:30])\nplt.show()","59f3bfb8":"`card4` - Card payments company. Categorical, strings.","92e4a803":"# Customer\/Card Detection techniques\n\nOne of the main challenges of this competition (https:\/\/www.kaggle.com\/c\/ieee-fraud-detection) is **how you obtain the card or the customer whose the transactions, fraudulent or not, belong to.** It has been said that the datasets have all the information to get this, but we faced two problems:\n1. It is **anonymized**, they're collected by Vesta\u2019s fraud protection system and digital security partners. \n2. The true meaning of the columns that seems to compose the card or the customer is somewhat obscure too. **The field names are masked** and pairwise dictionary will not be provided for privacy protection and contract agreement.\n\n> For this reason, the identification of both customer or card is one of the most discussed issues in the competition these datasets come from, and the approachs and solutions are quite different.\n> **In this notebook, we are based partially on the first approach coming from one of the winners of the competition Chris Deotte teaming with Konstantin Yakovlev. We will use the same analysis techniques, but how we implement the model using this information will finally difer.**\n\n\n## Our approaches\n\n### `uid1`\nWe used a simple approach from the more evident columns: \n* `card1`:  Probably, card number given by the issuing company or maybe customer number.\n* `addr1`:  Billing address zip code\n* `D1achr`: `D1` could be or the days passed between the transaction and the card issue date, or between transaction and the first transaction done with it. For us, this does not really matters, as can be used them both to identify the card. We made this attribute substracting `D1` (in days) to the column `TransactionDelt` (in seconds) with the formula:  \n\n**`D1achr` = `TransactionDT` \/(60x60x24) - `D1`**  \n**`uid1` = `card1` + `addr1` + `D1achr`**  \n\n---\n\n### `uid2`\nOur second approach was to add more card and address field to get the unique card or user id, similar to the choice made by Taemyung Heo (https:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/111696). When doing the FE, this uid has been really helpful, and the aggregations created with this uid improves greatly the model.\n\n**`uid2` = `card1` + `card2` ... + `card6` + `addr1` + `addr2` + `D1achr` + `ProductCD`**\n\n---\n\n### `uid3`\n\nApart from these approaches, we wanted to replicate the analysis done by Chris Deotte performing adversarial validation which is based on mixing the data from train and test, removing the business target (`isFraud`) and transaction identification columns and runing a model to determine if we can predict which observations are from train and which from test, and assuming that the most important features are strogly related to the customer identification.\n\nhttps:\/\/www.kaggle.com\/c\/ieee-fraud-detection\/discussion\/111510\n\n**We will use a combination of the three approaches to create new features that identifies the card and new aggregates from them.**","7e2a71bc":"# Conclusions\n\nAttending the adversarial validation, these characteristics can be strongly related to the customer identification:\n\n* `card1` y\/o `card2`\n* `D1achr` y\/o `D11achr`, `D10achr`, `D15achr`, `D4achr`\n* `C13`\n* `addr1`\n* `dist1`\n\nWe tested the model outcomes, engineering the features based on this uid, f.e. creating aggregations, against our original (card columns +addr columns + ProductCD + D1achr) and the results were better with the latter than the former. However, these features are also important for the fraud detection model. We are using a mix between these two and the first uid approach in order to obtain new features that help the model to differenciate between fraudulent and legit transactions.","ae9a3880":"`dist1` - Distance between, but not limited to, billing address, delivery address, telephonic area, etc.","3febdfde":"`isFraud` - As the transactions have been reported as fraudulent, all the following transactions with the same customer account, billing address, or email address will be also labeled as fraudulent.","f97ec4e5":"`Di columns`","12f18a7b":"`card1` - Probably, this is the number or part of the number given by the issuing entity. Categorical. It is not ordinal.","6ad9fffa":"`card3` - Categorical.","68e9050f":"`addr1` - This is the billing region or zip code. It is used to identify unique cards or user ids. In the real world, the zip codes are different between countries, but in the dataset, there are not many different ones: that depends on how the anonymization was done. This information has been analyzed in previous notebook, and it is part of the identification of a card.","4b0ff354":"## Analysis of the first 54 columns","5eb5a74c":"`addr2` - This is the billing country.","4e0f3855":"`P_emaildomain` - This is the mail domain of the purchaser.","bf579fa9":"`dist2` - Distance between, but not limited to, billing address, delivery address, telephonic area, etc.","1153cd54":"`R_emaildomain` - This is the mail domain of the recipient.","d5c67e17":"# Brief Exploratory Data Analysis for the customer related columns\nInfo and description of the columns from `TransactionID` to `R_emaildomain` and `Di` columns. This can be extended after getting the results of the adversarial validation","3032f538":"`card5` - Numeric, categorical.","32690347":"# Adversarial Validation","28e27c2b":"`card2` - Categorical. Numeric, not ordinal.","4defed43":"`TransactionAmt` - Transaction amount.","100d0168":"## Analysis of V columns","970b0cff":"`card6` - Card type. Categorical, strings.","6d6921fe":"`TransactionID` - Transactions identity information.","b38d04b2":"## Finally, we train the model for the adversarial validation with the best 50% of the first batch and the the best 50% of the second","9d0e93d1":"`TransactionDT` - Timedelta from a given reference datetime (not an actual timestamp).Delta between the begining of the dataset (86400 secs) and the transaction 183 days, aprox half a year. The unit is a second. We are only using this variable to create `DeltaDays`.\n\n`DeltaDays` = np.floor(`TransactionDT` \/ 60 \/ 60 \/ 24) -> converts seconds to days.","c1449151":"`ProductCD` - 5 different product code, the product for each transaction."}}