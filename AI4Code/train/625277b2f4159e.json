{"cell_type":{"7ad5d382":"code","a1d5a569":"code","b4c8a99e":"code","c89d3210":"code","977c3935":"code","882d985d":"code","b304d2d3":"code","0eccf10a":"code","dd347089":"code","2b80205e":"code","d9060e07":"code","964d2a88":"code","1b651291":"code","139bdf0b":"code","e7401c8b":"code","7e706c22":"code","64cc3dd5":"code","57d80326":"code","5cdc5b83":"code","3ace6e59":"code","c5c48086":"markdown","e07a3bf7":"markdown","0848bed5":"markdown","a906693c":"markdown","e46c793b":"markdown","0a7c4a69":"markdown","2d6f3801":"markdown","8aa2d290":"markdown","22b20ac3":"markdown","64fe8590":"markdown","83c12f41":"markdown","4d5d3e93":"markdown","5e8218a3":"markdown","52f38d26":"markdown","7f97018b":"markdown","318101f7":"markdown","7503bda9":"markdown","f2c0fc0f":"markdown","8655d95c":"markdown","d8302cf4":"markdown","c922c19d":"markdown","d027466e":"markdown"},"source":{"7ad5d382":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport numpy as np \nimport pandas as pd\nfrom scipy import stats \nfrom statsmodels.stats.proportion import proportions_ztest\nfrom statsmodels.stats.proportion import binom_test","a1d5a569":"#Storing baseline data\nd = {\"Metric Name\": [\"Cookies\", \"Clicks\", \"User-ids\", \"Click-through-probability\", \"Gross conversion\", \"Retention\", \"Net conversion\"], \n     \"Estimator\": [40000, 3200, 660, 0.08, 0.20625, 0.53, 0.109313],\n     \"dmin\": [3000, 240, -50, 0.01, -0.01, 0.01, 0.0075]}\nmd = pd.DataFrame(data=d, index=[\"C\", \"CL\", \"ID\", \"CTP\", \"CG\", \"R\", \"CN\"])\nmd","b4c8a99e":"#create new column to store scaled estimators\nmd.insert (2, \"Scaled_Est\", np.nan)\n\n#scale count estimates\nscaling_factor = 5000\/md.loc[\"C\"][\"Estimator\"]\n\nfor i in [\"C\", \"CL\", \"ID\"]:\n    md.at[i, \"Scaled_Est\"] = md.loc[i][\"Estimator\"] * scaling_factor\nmd","c89d3210":"def checkN (n, p, metric):\n    '''Given sample size n and probability p, return whether n is large enough to pass the 3-standard deviation rule,\n    i.e. whether we can assume that the distribution can be approximated by the normal distribution'''\n    if n > 9*((1-p)\/p) and n > 9*(p\/(1-p)):\n        result = print(metric,\":  n =\", n, \"is large enough to assume normal distribution approximation\")\n    else:\n        result = print(metric,\":  n =\", n, \"is not large enough to assume normal distribution approximation\")\n    return result\n\n#check whether n is large enough to assume normal distribution approximation\nfor i,j in zip([\"CL\", \"ID\", \"CL\"],[\"CG\", \"R\", \"CN\"]):\n    checkN (md.at[i, \"Scaled_Est\"], md.at[j,\"Estimator\"], md.at[j,\"Metric Name\"])","977c3935":"#create new column to store standard errors\nmd[\"SE\"] = np.nan\n\n#formula to calculate standard deviation\ndef standardError (n, p):\n    '''Return the standard deviation for a given probability p and sample size n'''\n    return (p*(1-p)\/n)**0.5\n\n#calculating standard errors for evaluation metrics and store them in md\nfor i in [\"CG\", \"CN\"]:\n    md.at[i, \"SE\"] = standardError(md.loc[\"CL\"][\"Scaled_Est\"], md.loc[i][\"Estimator\"]) \n    \nmd.at[\"R\", \"SE\"] = standardError(md.loc[\"ID\"][\"Scaled_Est\"], md.loc[\"R\"][\"Estimator\"])\nmd\n","882d985d":"#storing alpha and beta in a dictionary\nerror_prob = {\"alpha\": 0.05, \"beta\": 0.20}\nerror_prob","b304d2d3":"#create new column n_c to store sample sizes\nmd[\"n_C\"] = np.nan\n\n#define function for approach B\ndef get_sampleSize (alpha, beta, p, dmin):\n    '''Return sample size given alpha, beta, p and dmin'''\n    return (pow((stats.norm.ppf(1-alpha\/2)*(2*p*(1-p))**0.5+stats.norm.ppf(1-beta)*(p*(1-p)+(p+dmin)*(1-(p+dmin)))**0.5),2))\/(pow(dmin,2))\n\n#calculate sample sizes for evaluation metrics with defined adjustments and store results in md\nfor i in [\"CG\", \"CN\"]:\n    md.at[i, \"n_C\"] = round((get_sampleSize(error_prob[\"alpha\"], error_prob[\"beta\"], md.loc[i][\"Estimator\"], md.loc[i][\"dmin\"])\/md.loc[\"CTP\"][\"Estimator\"])*2)\n\nmd.at[\"R\", \"n_C\"] = round(((get_sampleSize(error_prob[\"alpha\"], error_prob[\"beta\"], md.loc[\"R\"][\"Estimator\"], md.loc[\"R\"][\"dmin\"])\/md.loc[\"CTP\"][\"Estimator\"])\/md.loc[\"CG\"][\"Estimator\"])*2)\nmd\n","0eccf10a":"#traffic diverted to experiment [0:1]\ntraffic_diverted = 1\n\n#Days it would take to run experiment for each case\nfor i, j in zip([\"CG\", \"CN\", \"R\"],[\"CG\", \"CG+CN\", \"CG+CN+R\"]):\n   print(\"Days required for\",j,\":\", round(md.loc[i][\"n_C\"]\/(md.loc[\"C\"][\"Estimator\"]*traffic_diverted),2))\n","dd347089":"#traffic diverted to experiment\ntraffic_diverted = 0.47\n\n#Days it would take to run experiment if we use net conversion and gross coversion as evaluation metrics\nprint(\"Experiment duration in days, CN+CG: \",round(md.loc[\"CN\"][\"n_C\"]\/(md.loc[\"C\"][\"Estimator\"]*traffic_diverted),2))","2b80205e":"#loading experiment data into new dataframes\ncontrol = pd.read_csv(\"..\/input\/Final Project Results - Control.csv\") \nexperiment = pd.read_csv(\"..\/input\/Final Project Results - Experiment.csv\")\n\n#check if loaded correctly\ncontrol.head()","d9060e07":"#check if loaded correctly\nexperiment.head()","964d2a88":"#check number of entries\ncontrol.count()","1b651291":"#check number of entries\nexperiment.count()","139bdf0b":"#check sample size and store it as sample_size\nsample_size_control = control[\"Pageviews\"].sum()\nsample_size_experiment = experiment[\"Pageviews\"].sum()\nsample_size = sample_size_control+sample_size_experiment\nsample_size","e7401c8b":"#create empty dataframe to store sanity check results\nsanity_check = pd.DataFrame(columns=[\"CI_left\", \"CI_right\", \"obs\",\"passed?\"], index=[\"C\", \"CL\", \"CTP\"])\n\n#set alpha and p_hat\np = 0.5\nalpha = 0.05\n\n#fill dataframe with results from binomial test\n#for cookies and clicks do the following\nfor i,j in zip([\"C\", \"CL\"], [\"Pageviews\", \"Clicks\"]):\n    #calculate the number of successes (n_control) and number of observations (n)\n    n = control[j].sum()+experiment[j].sum()\n    n_control = control[j].sum()\n    \n    #compute confidence interval\n    sanity_check.at[i, \"CI_left\"] = p-(stats.norm.ppf(1-alpha\/2)*standardError(n,p))\n    sanity_check.at[i, \"CI_right\"] = p+(stats.norm.ppf(1-alpha\/2)*standardError(n,p))\n    \n    #compute observed fraction of successes\n    sanity_check.at[i, \"obs\"] = round(n_control\/(n),4)\n    \n    #check if the observed fraction of successes lies within the 95% confidence interval\n    if sanity_check.at[i, \"CI_left\"] <= sanity_check.at[i, \"obs\"] <= sanity_check.at[i, \"CI_right\"]:\n        sanity_check.at[i, \"passed?\"] = \"yes\"\n    else:\n        sanity_check.at[i, \"passed?\"] = \"no\"\n\n#return results\nsanity_check","7e706c22":"#calculate the number of observations\nn = control[\"Pageviews\"].sum()+experiment[\"Pageviews\"].sum()\n#calculate the number of successes\nn_control = control[\"Pageviews\"].sum()\n\n#calculate the test-statistic Z and corresponding p_value\nz_statistic, p_value = proportions_ztest(n_control, n, value=0.5, alternative=\"two-sided\", prop_var=False)\n\nprint(\"z-test-statistic: \", z_statistic)\nprint(\"p-value:\" , p_value)\n\n#alternatively compute p-value using the exact binomial test\np_value_binom = binom_test(n_control, n, prop=0.5, alternative='two-sided')\nprint(\"p-value_binomial: \", p_value_binom)\n\n#check whether p_value is smaller than alpha\nalpha = 0.05\n\nif p_value_binom > 0.05:\n    print(\"The null hypothesis cannot be rejected and the sanity check is passed\")\nelse:\n    print(\"The null hypothesis is rejected and the sanity check is not passed\")","64cc3dd5":"#compute CTP for both groups\nCTP_control = control[\"Clicks\"].sum()\/control[\"Pageviews\"].sum()\nCTP_experiment = experiment[\"Clicks\"].sum()\/experiment[\"Pageviews\"].sum()\n\n#compute sample standard deviations for both groups\nS_control = (CTP_control*(1-CTP_control))**0.5\nS_experiment = (CTP_experiment*(1-CTP_experiment))**0.5\n\n#compute SE_pooled\nSE_pooled = (S_control**2\/control[\"Pageviews\"].sum()+S_experiment**2\/experiment[\"Pageviews\"].sum())**0.5\n\n#compute 95% confidence interval and store it in sanity check\nalpha = 0.05\n\nsanity_check.at[\"CTP\", \"CI_left\"] = 0-(stats.norm.ppf(1-alpha\/2)*SE_pooled)\nsanity_check.at[\"CTP\", \"CI_right\"] = 0+(stats.norm.ppf(1-alpha\/2)*SE_pooled)\n\n#compute observed difference d and store it in sanity check\nsanity_check.at[\"CTP\", \"obs\"] = round(CTP_experiment - CTP_control,4)\n\n#check if sanity check is passed\nif sanity_check.at[\"CTP\", \"CI_left\"] <= sanity_check.at[\"CTP\", \"obs\"] <= sanity_check.at[\"CTP\", \"CI_right\"]:\n    sanity_check.at[\"CTP\", \"passed?\"] = \"yes\"\nelse:\n    sanity_check.at[\"CTP\", \"passed?\"] = \"no\"\n\n#return results\nsanity_check\n","57d80326":"#calculate the number of observations for each group and store results in an array\nn = np.array([control[\"Pageviews\"].sum(), experiment[\"Pageviews\"].sum()])\n#calculate the number of successes for each group and store results in an array\nn_clicks = np.array([control[\"Clicks\"].sum(), experiment[\"Clicks\"].sum()])\n\n#calculate the test-statistic Z and corresponding p_value\nz_statistic, p_value = proportions_ztest(n_clicks, n, value=0, alternative=\"two-sided\", prop_var=False)\n\nprint(\"z-test-statistic: \", z_statistic)\nprint(\"p-value:\" , p_value)\n\n#check whether p_value is smaller than alpha\nalpha = 0.05\n\nif p_value > 0.05:\n    print(\"The null hypothesis cannot be rejected and the sanity check is passed\")\nelse:\n    print(\"The null hypothesis is rejected and the sanity check is not passed\")","5cdc5b83":"#compute true sample size\ntrue_sample_size = control.iloc[:23][\"Pageviews\"].sum()+experiment.iloc[:23][\"Pageviews\"].sum()\ntrue_sample_size","3ace6e59":"#create dataframe test_results\ntest_results = pd.DataFrame(columns=[\"CI_left\", \"CI_right\", \"d\",\"stat sig?\", \"dmin\", \"pract rel?\"], index=[\"CG\", \"CN\"])\n\n#set alpha\nalpha = 0.05\n\n\n#run two proportion z test for both metrics\nfor i,j in zip([\"Enrollments\", \"Payments\"],[\"CG\", \"CN\"]):\n    #compute sample conversion rates\n    conv_control = control.iloc[:23][i].sum()\/control.iloc[:23][\"Clicks\"].sum()\n    conv_experiment = experiment.iloc[:23][i].sum()\/experiment.iloc[:23][\"Clicks\"].sum()\n    \n    #compute observed difference between treatment and control conversion d\n    test_results.at[j, \"d\"] = conv_experiment-conv_control\n    \n    #compute sample standard deviations\n    S_control = (conv_control*(1-conv_control))**0.5\n    S_experiment = (conv_experiment*(1-conv_experiment))**0.5\n    \n    #compute SE_pooled\n    SE_pooled = (S_control**2\/control.iloc[:23][\"Clicks\"].sum()+S_experiment**2\/experiment.iloc[:23][\"Clicks\"].sum())**0.5\n    \n    #compute 95% confidence interval around observed difference d\n    test_results.at[j, \"CI_left\"] = test_results.at[j, \"d\"]-(stats.norm.ppf(1-alpha\/2)*SE_pooled)\n    test_results.at[j, \"CI_right\"] = test_results.at[j, \"d\"]+(stats.norm.ppf(1-alpha\/2)*SE_pooled)\n    \n    #check statistical significance\n    if test_results.at[j, \"CI_left\"] <= 0 <= test_results.at[j, \"CI_right\"]:\n        test_results.at[j, \"stat sig?\"] = \"no\"\n    else:\n        test_results.at[j, \"stat sig?\"] = \"yes\"\n    \n    #import dmin\n    test_results.at[j, \"dmin\"] = md.loc[j][\"dmin\"]\n    \n    \n    #check if practical relevant\n    #check if dmin is positive or negative\n    if test_results.at[j, \"dmin\"] >= 0:\n        #check if d is larger than dmin and if dmin lies left of the confidence interval around d\n        if test_results.at[j, \"d\"] > test_results.at[j, \"dmin\"] and test_results.at[j, \"CI_left\"] > test_results.at[j, \"dmin\"]:\n                test_results.at[j, \"pract rel?\"] = \"yes\"\n        else:\n            test_results.at[j, \"pract rel?\"] = \"no\"\n    else:\n        #check if d is smaller than dmin and if dmin lies right of the confidence interval around d\n        if test_results.at[j, \"d\"] < test_results.at[j, \"dmin\"] and test_results.at[j, \"dmin\"] > test_results.at[j, \"CI_right\"]:\n                test_results.at[j, \"pract rel?\"] = \"yes\"\n        else:\n            test_results.at[j, \"pract rel?\"] = \"no\"\n\n#return results\ntest_results","c5c48086":"We see that we would need to run the experiment for about 119 days in order to test all three hypotheses (and this does not even take into account the 14 additional days (free trial period) we have to wait until we can evaluate the experiment). Such a duration (esp. with 100% traffic diverted to it) appears to be very risky. First, we cannot perfom any other experiment during this period (opportunity costs). Secondly, if the treatment harms the user experience (frustrated students, inefficient coaching resources) and decreases conversion rates, we won't notice it (or cannot really say so) for more than four months (business risk). <i>Consequently, it seems more reasonable to only test the first and third hypothesis and to discard retention as an evaluation metric.<\/i> Especially since net conversion is a product of rentention and gross conversion, so that we might be able to draw inferences about the retention rate from the two remaining evaluation metrics.\n\nSo, how much traffic should we divert to the experiment? Given the considerations above, we want the experiment to run relatively fast and for not more than a few weeks. Also, as the nature of the experiment itself does not seem to be very risky (e.g. the treatment doesn't involve a feature that is critical with regards to potential media coverage), we can be confident in diverting a high percentage of traffic to the experiment. Still, since there is always the potential that something goes wrong during implemention, we may not want to divert all of our traffic to it. Hence, 80% (22 days) would seem to be quite reasonable. <i>However, when we look at the data provided by Udacity (see 4.1) we see that it takes 37 days to collect 690,203 pageviews, meaning that they most likely diverted somewhere between 45% and 50% of their traffic to the experiment<\/i>\n","e07a3bf7":"### 5. Interpretation of Results and Recommendations\n<a id=\"section5\"><\/a>\n\nGross conversion: the observed gross conversion in the treatment group is around 2.06% smaller than the gross conversion observed in the control group. Further, we see that also the values within the confidence interval are most compatible with a negative effect. Lastly, this effect appears to be practically relevant as those values are smaller than dmin, the minimum effect size to be considered relevant for the business. \n\nNet conversion: While we cannot reject the null hypothesis for this test, we see that the observed net conversion in the treatment group is around 0.49% smaller than the net conversion observed in the control group. Further, the values that are considered most reasonabily compatible with the data range from -1.16% to 0.19%. \n\nGiven these results, we can assume that the introduction of the \"Free Trial Screener\" may indeed help to set clearer expectations for students upfront. However, the results are less compatible with the assumption that the decrease in gross conversion is entirely absorbed by an improvement in the overall student experience and still less compatible with dmin(net conversion), the minimum effect size to be considered relevant for the business. <i>Consequently, assuming that Udacity has a fair interest in increasing revenues, we would recommend to not roll out the \"Free Trial Screener\" feature.<\/i>\n\nThis being said, as outlined in 3.3.2, the feature may increase the total number of people who opt for the freely available materials. If true and assuming a steady conversion rate from users who first learn with the freely accessible materials and then upgrade, the feature may still help to increase net conversion. However, if at all, this effect is more likely to happen over a longer time period and, hence, would require a test with a longer timeframe.","0848bed5":"### 4. Data Analysis\n<a id=\"section4\"><\/a>\n\n<a id=\"section4_1\"><\/a>\n#### 4.1 Loading experiment data","a906693c":"The alternative approach using normalstats' proportion z-test function would have looked like this:","e46c793b":"### 3. Experiment Setup\n<a id=\"section3\"><\/a>\n\n<a id=\"section3_1\"><\/a>\n#### 3.1 Unit of diversion (provided by Udacity)\n\nThe unit of diversion is a <i>cookie, although if the student enrolls in the free trial, they are tracked by user-id from that point forward<\/i>. The same user-id cannot enroll in the free trial twice. For users that do not enroll, their user-id is not tracked in the experiment, even if they were signed in when they visited the course overview page.\n\n<a id=\"section3_2\"><\/a>\n#### 3.2 Initial hypotheses\n\nGiven the challenge outlined above, we can define null and alternative hypotheses for our experiment. These hypotheses will be revisited and further specified in 3.4.\n\n* $ H_{0}$: The treatment has no effect on the share of people who enroll in the free trial\n* $ H_{1}$: The treatment reduces the share of people who enroll in the free trial\n\n\n* $ H_{0}$: The treatment has no effect on the share of people who leave the free trial\n* $ H_{1}$: The treatment improves the overall student experience thereby reducing the share of people who leave the free trial\n\n\n* $ H_{0}$: The treatment has no effect on the number of people who continue past the free trial\n* $ H_{1}$: The treatment affects the number of people who continue past the free trial\n\n<a id=\"section3_3\"><\/a>\n#### 3.3 Metric choice and practical relevance\n\nTo test these hypotheses, we need to define appropriate evaluation metrics. Evaluation metrics should be sensitive enough that they pick up the changes we care about. At the same time, they should be robust against the changes we do not care about.\n\nFurther, it is suggested to define a set of invariant metrics\/control variables. Later on, these can help to \"sanity check\" the results and experiment setup. Invariant metrics are metrics that we expect not to change between test and control group. \n\nThereby, we care not only about statistical significance but also about whether changes in metrics are practially relevant (dmin) as treatments might be not worth the resources to implement (despite being statistically significant).\n\nFor the project, Udacity provides a pool of seven metrics to choose from. Additionally, the practical relevance level of each metric is given. To actually derive metrics (or in this case: to choose), I find it helpful to first visualize the experiment. A visualization with the provided metric options is shown below:\n\n![A_b_test_overview.png](attachment:A_b_test_overview.png)\n\n&nbsp;\n\n##### 3.3.1 Invariant Metrics\n\nThe metrics we don't expect to change between treatment and control group are the ones on top as they are independent from the treatment. Although it might be sufficient to only use the click-through-probability as an invariant metric (since it is defined by the other two), it is beneficial to track the other two as well to check back on them in case the click-through-probabilities vary unexpectedly.\n\n\n| <p align=\"left\">Metric<\/p>                    | <p align=\"left\">Formula<\/p>                                                             | <p align=\"left\">dmin<\/p> |\n| ------------------------- | ---------------------------------------------------------------------- | --------------------------------------- |\n| <p align=\"left\">Number of cookies<\/p>        | <p align=\"left\">$ \\text{C = } \\text{# of unique daily cookies to view course overview page}$<\/p>          | <p align=\"left\">3000<\/p>                                |\n| <p align=\"left\">Number of clicks<\/p>       | <p align=\"left\">$ \\text{CL = } \\text{# of unique daily cookies to click \"start free trial\" button}$<\/p>   | <p align=\"left\">240<\/p>                                  |\n| <p align=\"left\">Click-through-probability<\/p> | <p align=\"left\">$ \\text{CTP = } \\frac{CL}{C}$<\/p>                                                            | <p align=\"left\">0.01<\/p>                                 |\n\n\n\n&nbsp;\n\n\n##### 3.3.2 Evaluation Metrics\n\nIn contrast to the invariant metrics, we expect the following metrics to be affected by the treatment and vary between control and treatment group.\n\n* Gross conversion: we expect gross conversion in the treatment group to be lower than gross conversion in the control group as, through the treatment, we are trying to divert users that don't seem to be a good fit for the paid option (likely to churn due to limited time resources --> time commitment of less than 5 hours\/week) to the freely accessible course materials.\n\n* Retention: at the same time, we expect the retention rate to increase as the treatment should have filtered out users who are likely to churn.\n\n* Net conversion: this metric is a product of the two former. From a business perspective, we hope to see an increase but it could vary in each direction or stay constant depending on gross conversion and retention effects.\n\nLastly, we would also expect the number of user-ids (i.e. the number of users who enroll in the free trial; dmin=-50) to decrease. However, the metric is not normalized and would not provide any information we are not already capturing with gross conversion (as the number of clicks will be controlled for). Thus, we will not use it as an evaluation metric.\n\nAdditional thoughts:\nThis being said, it would have been also interesting to track whether the treatment increases the total number of people who opt for the freely available course materials as users in this group potentially want to upgrade later on. Based on the reasoning of this experiment, we would expect the total number of people who opt for the freely available materials to increase since we expect that the treatment reduces direct churn from people who are frustrated with the free trial (users who drop out of the free trial are less likely to access the free materials afterwards). However, given the provided data this is not possible. In case such an effect would indeed exist and materialize within the duration of the experiment, impact gross conversion rates would be positively impacted (assuming conversion rates from users who first learn with the freely accessible materials and then upgrade do not decline).\n\n\n\n \n| <p align=\"left\">Metric<\/p>                   | <p align=\"left\">Formula<\/p>                                                               | <p align=\"left\">dmin<\/p> |\n| ------------------------- | ---------------------------------------------------------------------- | --------------------------------------- |\n| <p align=\"left\">Gross Conversion<\/p>        | <p align=\"left\">$ \\text{CG = } \\frac{\\text{# of user-ids that enrolled}}{CL}$<\/p>      | <p align=\"left\">-0.01<\/p>                                    |\n| <p align=\"left\">Retention<\/p>         | <p align=\"left\">$ \\text{R = } \\frac{\\text{# of user-ids that paid}}{\\text{# of user-ids that enrolled}}$<\/p>  | <p align=\"left\">0.01<\/p>                                   |\n| <p align=\"left\">Net Conversion<\/p> | <p align=\"left\">$ \\text{CN = } \\frac{\\text{# of user-ids that paid}}{CL}$<\/p>                                                         | <p align=\"left\">0.0075<\/p>                                    |\n\n&nbsp;\n\n<a id=\"section3_4\"><\/a>\n#### 3.4 Hypotheses revisited\n\nGiven the available and selected metrics, we can now specify our hypotheses. While it could be argued that in some cases a one-sided test is appropriate, we are thereby sticking with a more conservative two-sided test.\n\n\n* $ H_{0}: CG_{treatment} = CG_{control} $\n* $ H_{1}: CG_{treatment} \\neq CG_{control} $\n\n\n* $ H_{0}: R_{treatment} = R_{control} $\n* $ H_{1}: R_{treatment} \\neq R_{control} $\n\n\n* $ H_{0}: CN_{treatment} = CN_{control} $\n* $ H_{1}: CN_{treatment} \\neq CN_{control} $\n\n\nNote: later on, we will drop the second hypothesis as it would demand a sample size that requires the the test to run unreasonably long (see 3.8).\n\n<a id=\"section3_5\"><\/a>\n#### 3.5 Measuring variability in metrics\n\n##### 3.5.1 Collecting baseline data (provided by Udacity)\n\n\n| <p align=\"left\">Metric<\/p>                   | <p align=\"left\">Estimator<\/p> |\n| ------------------------- | ---------------------------------------------------------------------- |\n| <p align=\"left\">Number of cookies<\/p>           | <p align=\"left\">40000<\/p>      |\n| <p align=\"left\">Number of clicks<\/p>            | <p align=\"left\">3200<\/p>       |\n| <p align=\"left\">Number of user-ids<\/p>          | <p align=\"left\">660<\/p>        |\n| <p align=\"left\">CTP<\/p>                         | <p align=\"left\">0.08<\/p>       |\n| <p align=\"left\">Gross conversion<\/p>            | <p align=\"left\">0.20625<\/p>    |\n| <p align=\"left\">Retention<\/p>                   | <p align=\"left\">0.53<\/p>       |\n| <p align=\"left\">Net conversion<\/p>              | <p align=\"left\">0.1093125<\/p>  |\n\n","0a7c4a69":"#### 3.8 Experiment exposure and duration\n<a id=\"section3_8\"><\/a>\n\nNow, for each case, we can calculate how many days we would approximately need to run the experiment in order to reach n_C. According to the challenge description, we are thereby assuming that there are no other experiments we want to run simultaneously. So, theoretically, we could divert 100% of the traffic to our experiment (i.e. about 50% of all visitors would then be in the treatment condition). Given our estimation that there are about 40,000 unique pageviews per day, this would result in:","2d6f3801":"#### 3.6 Setting alpha and beta\n<a id=\"section3_6\"><\/a>\n\nWe set the alpha level to 0.05 and the statistical power to 0.80 (by setting beta to 0.20).","8aa2d290":"#### 3.7 Determining experiment sample size\n<a id=\"section3_7\"><\/a>\n\nApproach A:\n\nIf we assume that the standard deviations of the population proportion and the sample proportion are equal and that also the sample sizes of treatment and control group are the same, then the required experiment sample size per group can be determined through (as outlined [here](http:\/\/vanbelle.org\/chapters%5Cwebchapter2.pdf)):\n\n$ n = 2(z_{1-\\alpha\/2}+z_{1-\\beta})^2*\\frac{\\sigma^2}{dmin^2} $\n\nwith $ \\sigma = SE * \\sqrt{n} = \\sqrt{\\hat{p}*(1-\\hat{p})}$    ,whereby here n is the sample size we used to calculate SE earlier.\n\n&nbsp;\n\nApproach B:\n\nIf we do not assume common standard deviations then a more precise way to determine the required sample size would be (as outlined [here](http:\/\/vanbelle.org\/chapters%5Cwebchapter2.pdf)):\n\n\n$ n = \\frac{(z_{1-\\alpha\/2}*\\sqrt{2*\\hat{p}*(1-\\hat{p})}+z_{1-\\beta}*\\sqrt{\\hat{p}*(1-\\hat{p})+(\\hat{p}+dmin)*(1-(\\hat{p}+dmin))})^2}{dmin^2}$\n\nThis is also the approach used by many online sample size calculators such as the one by [Evan Miller ](https:\/\/www.evanmiller.org\/ab-testing\/sample-size.html) and we will apply it in the following. \n\n&nbsp;\n\nFurther considerations\n\nWhen we calculate the experiment sample size we have to keep in mind that n gives us the sample size per group. For a classical A\/B test, there are two groups. Further, we want to calculate the experiment sample size in terms of cookies that visit the page. Thus, we also need to account for the circumstance that our evaluation metrics' units of analysis are clicks and user-ids, respectively.\n\nThe total experiment sample size per evaluation metric is hence given by:\n\n$ n_{c} = \\frac{n}{CTP}*2 $\n\n\nand\n\n$ n_{c} = \\frac{\\frac{n}{CTP}}{CG}*2 $\n","22b20ac3":"##### 3.5.2 Calculating standard errors\n\nNext, we need to calculate the standard deviation of the sampling distribution of the sample mean (standard error, in short) for each of the evaluation metrics. To be more precise, in this case we calculate the estimated standard errors of the sample proportions as our evaluation metrics are probabilities. The standard error is an estimate of how far the sample proportion is likely to be from the population proportion.","64fe8590":"#### 4.2 Sanity check\n<a id=\"section4_2\"><\/a>\n\nTo ensure that the experiment has been run properly, we first conduct a sanity check using the three invariant metrics outlined above (3.3.1). We have two counts (number of cookies, number of clicks) and one probability. As stated earlier, we would expect that these metrics do not differ significantly between control and treatment group. Otherwise, this would imply that someting is wrong with the experiment setup and that our results are biased. \n\n\n##### 4.2.1 Sanity check: number of cookies + number of clicks\n\nIn the provided data, the column \"pageviews\" represents the number of cookies that browse the course overview page. Given our assumptions, we would expect that the total number of cookies in the treatment group and the total number of cookies in the control group each account for about 50% of the combined number of cookies of both groups (treatment + control) as they should have been assigned randomly. If we now regard being assigned to the control group as a success, we can use the binominal distribution to model the number of successes in the given sample (treatment+control) and perform a binomial test\/one-proportion z-test as a sanity check:\n\na)  Compute confidence interval around the binominal, i.e. the number of success we expect to get out of n (as n is large, we can further assume that the sampling distribution of the sample proportion approaches a normal distribution (Central Limit Theorem))\n\n\n$ CI = [\\hat{p}-Z_{1-\\alpha\/2}*SE; \\hat{p}+Z_{1-\\alpha\/2}*SE] $\n\nwith $ SE = \\sqrt{\\frac{\\hat{p}*(1-\\hat{p})}{n}} $\n\n\n&nbsp;\n\nb)  Check if observed fraction $ \\frac{\\text{Number of successes}}{n} $ is within the interval. If yes, then the sanity checked is passed.\n\n\nWe will conduct the same test also for our second invariant metric \"number of clicks\". Again, as n is large we can assume that the sampling distribution of the sample proportion approximates a normal distribution.\n","83c12f41":"## Udacity A\/B Testing\u2014Final Course Project\n\n1. [Context](#section1)<br\/>\n2. [Project Challenge](#section2)<br\/>\n    2.1 [Status quo](#section2_1)<br\/>\n    2.2 [Treatment](#section2_2)<br\/>\n    2.3 [Reasoning](#section2_3)<br\/>\n3. [Experiment Setup](#section3)<br\/>\n    3.1 [Unit of diversion](#section3_1)<br\/>\n    3.2 [Initial hypotheses](#section3_2)<br\/>\n    3.3 [Metric choice and practical relevance](#section3_3)<br\/>\n    3.4 [Hypotheses revisited](#section3_4)<br\/>\n    3.5 [Measuring variability in metrics](#section3_5)<br\/>\n    3.6 [Setting alpha and beta](#section3_6)<br\/>\n    3.7 [Determining experiment sample size](#section3_7)<br\/>\n    3.8 [Experiment exposure and duration](#section3_8)<br\/>\n    3.9 [Accounting for multiple hypotheses](#section3_9)<br\/>\n4. [Data Analysis](#section4)<br\/>\n    4.1 [Loading experiment data](#section4_1)<br\/>\n    4.2 [Sanity check](#section4_2)<br\/>\n    4.3 [Test analysis](#section4_3)<br\/>\n5. [Interpretation of Results and Recommendations](#section5)<br\/>","4d5d3e93":"###### 3.5.2.1 Scaling\n\nSince the sample size given by Udacity is n = 5000 cookies, we first need to scale the collected count data, i.e. the number of cookies, the number of clicks and the number of user-ids.","5e8218a3":"Alternatively, we could have calculated the test-statistic Z and compared the corresponding p-value against our selected alpha level. Another option would have been an exact binomial test. This would have looked like this for the metric \"number of cookies\":","52f38d26":"#### 3.9 Accounting for multiple hypotheses?\n<a id=\"section3_9\"><\/a>\n\nAs we now have more than one hypothesis, the chance to get false positives increases. However, our metrics are not fully independent which is why the true probability for false positives will still be lower than 9.75% (that's the case for independent metrics). We could then use family-wise error rate such as Bonferroni or false discovery rate methods to account for the multiple hypotheses problem. However, they have flaws as well (e.g. we could easily end up with more false negatives; see [here](https:\/\/multithreaded.stitchfix.com\/blog\/2015\/10\/15\/multiple-hypothesis-testing\/) and [here](https:\/\/www.statisticshowto.datasciencecentral.com\/multiple-testing-problem\/)). Hence, given that the chance to get more false positives is only slightly increased in this case, we won't control for multiple hypothese here.","7f97018b":"### 1. Context\n<a id='section1'><\/a>\n\nIn collaboration with Google, Udacity provides an [introductory course to A\/B testing](https:\/\/www.udacity.com\/course\/ab-testing--ud257). The course covers the design and analysis of A\/B tests using a frequentist approach. This notebook provides a walkthrough of the course's final project.","318101f7":"###### 3.5.2.3 Computing standard errors\n\nGiven above assumptions we can approximate the standard error through:\n\n$ SE = \\sqrt{\\frac{\\hat{p}*(1-\\hat{p})}{n}}$\n\nwith $ \\sqrt{\\hat{p}*(1-\\hat{p})}$ estimating the population standard deviation.\n\n","7503bda9":"While Udacity suggests conducting an additional sign-test to double-check the results, we will forgo this test as the traditional sign-test assumes dependent samples. Instead, we will jump right at the interpretation of our results.","f2c0fc0f":"#### 4.3 Test analysis\n<a id=\"section4_3\"><\/a>\n\nSimilar to the click-through probability, we can test our evaluation metric hypotheses using two proportion z-tests (thereby, the same assumptions as outlined above apply). However, in contrast to the previous implementation, this time we will compute the respective confidence interval around the observed difference between the conversion metrics. Further, we will check if the observed changes also matter to the business (dmin).\n\nRecall our hypotheses:\n\n* $ H_{0}: CG_{treatment} = CG_{control} $\n* $ H_{1}: CG_{treatment} \\neq CG_{control} $\n\n\n* $ H_{0}: CN_{treatment} = CN_{control} $\n* $ H_{1}: CN_{treatment} \\neq CN_{control} $\n\n\n<i>Note: As could be seen in 4.1 when we loaded the data, \"payments\" (and strangely also \"enrollments\") were only tracked for 37 days (23+14 days) and not for 51 days (37+14 days) which would have been necessary in order to fully account for the 14-day trial period. Consequentally, in our actual A\/B test, the true sample size is lower (n_true = 423,525) than we initially aimed for (n = 685,336 (3.7)). However, at this point there is not much we can do about it other than taking it into consideration in our interpretations.<\/i>\n","8655d95c":"##### 4.2.2 Sanity check: click-through probability\n\nTo check whether the click-through probabilites in the control and treatment groups are significantly different from each other, we conduct a two proportion z-test with a click being interpreted as a success. We thereby assume that the two populations have normal distributions but not necessarily equal variances (hence p is not pooled below). To perform the test, we can calculate a confidence interval around the expected difference of the two metrics which is 0. Alternatively, we can calculate the Z-test-statistic and then check the corresponding p-value. The steps for the first approach are the following:\n\na) Compute confidence interval around the expected difference of 0.\n\n$ CI = [0-Z_{1-\\alpha\/2}*SE; 0+Z_{1-\\alpha\/2}*SE] $\n\nwith $ SE_{pooled} = \\sqrt{\\frac{S_{cont}^2}{n_{cont, pageviews}}+\\frac{S_{exp}^2}{n_{exp, pageviews}}} $\n\nwhereby $ S = \\sqrt{p*(1-p)} $\n\nand $ p = CTP = \\frac{n_{clicks}}{n_{pageviews}} $\n\n&nbsp;\n\nb) Compute the observed difference between the two metrics d and check whether d lies within CI\n\n$ d = CTP_{experiment}-CTP_{control} $","d8302cf4":"Given our calculations, we would need around 638,940 pageviews (cookies) to test the first hypothesis (given our assumptions on alpha, beta, baseline conversions and dmin). To additionally test the third hypothesis, we would need a total of 685,336 pageviews. And, in case we would like to also test the second hypothesis, we would need a total of around 4,737,771 pageviews.","c922c19d":"###### 3.5.2.2 Assumptions\n\nSince the unit of diversion is the same as the unit of analysis (denominator of the metric formula) for each evaluation metric (cookie in the case of Gross Conversion and Net Conversion and user-id in the case of Retention) and we can make assumptions about the distributions of the metrics (binominal), we can calculate the standard errors analytically (instead of empirically).\n\nFurther, as n is relatively large in each case, we can assume that the sampling distribution of a sample proportion approaches a normal distribution (due to the Central Limit Theorem). We can also use a rule such as the [3-standard-deviation rule](https:\/\/en.wikipedia.org\/wiki\/Binomial_distribution) to check if n is large enough:","d027466e":"### 2. Project Challenge (provided by Udacity)\n<a id='section2'><\/a>\n\nUdacity's mission is to power careers through tech education. Working towards this mission, the company aims to provide a stimulating learning experience that is tailored to the individual learner and supported by experienced coaches. To improve its services, Udacity tinkered with changing the user flow on its website and set up an A\/B test titled \"Free Trial Screener\" to test its idea.\n\n<a id='section2_1'><\/a>\n#### 2.1 Status quo\n\n* At the time of the experiment, Udacity courses have two options on the course overview page: \"start free trial\", and \"access course materials\".\n\n* If students click \"start free trial\", they will be asked to enter their credit card information, and then they will be enrolled in a free trial for the paid version of the course. After 14 days, they will automatically be charged unless they cancel first. \n\n* If students click \"access course materials\", they will be able to view the videos and take the quizzes for free, but they will not receive coaching support or a verified certificate, and they will not submit their final project for feedback.\n\n\n<a id=\"section2_2\"><\/a>\n#### 2.2 Treatment\n\n* In the experiment, Udacity tests a change where if students click \"start free trial\", they are asked how much time they have available to devote to the course. \n\n* If students indicate 5 or more hours per week, they are taken through the checkout process as usual. \n\n* If they indicate fewer than 5 hours per week, a message appears indicating that Udacity courses usually require a greater time commitment for successful completion, and suggesting that the student might like to access the course materials for free. At this point, students have the option to continue enrolling in the free trial, or access the course materials for free instead. [This screenshot](https:\/\/drive.google.com\/file\/d\/0ByAfiG8HpNUMakVrS0s4cGN2TjQ\/view) shows what the experiment looks like.\n\n<a id=\"section2_3\"><\/a>\n#### 2.3 Reasoning\n\n* The hypothesis is that this might set clearer expectations for students upfront, thus reducing the number of frustrated students who left the free trial because they didn't have enough time\u2014without significantly reducing the number of students to continue past the free trial and eventually complete the course.\n\n* If this hypothesis held true, Udacity could improve the overall student experience and improve coaches' capacity to support students who are likely to complete the course."}}