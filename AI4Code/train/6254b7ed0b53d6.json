{"cell_type":{"a1c8e47a":"code","d5c6abdb":"code","b1955f02":"code","d3e87048":"code","684d333b":"code","6065ee6f":"code","aed76b16":"code","201e4a75":"code","e999927c":"code","3ea54371":"code","463db209":"code","0d4a1d3e":"code","bfd13e95":"code","13a5bdfb":"code","e9d68eee":"code","b9bae26f":"code","43578cbe":"code","f0df28b4":"code","3e4c5af0":"code","4283e436":"code","a094e36c":"code","05520c75":"code","212e9eae":"code","b0f31ab3":"code","68ac079f":"code","aa379d17":"code","cd9ada75":"code","956adc4b":"code","81b05b16":"code","b9cc6c7a":"code","f5b3f90e":"code","0537f202":"code","2724a77b":"code","a6f301ff":"code","438adbf8":"code","36eabfc5":"code","d3ed2721":"code","fa328224":"code","0bc950e0":"code","e21da0d0":"code","95336d32":"code","184e3dd1":"code","98c2948b":"code","110e0220":"code","936bbaba":"code","62e999ab":"code","bcb61d83":"code","5ec45cf2":"code","d7bfe26c":"code","f37dd71d":"code","58d6aba7":"code","7b3f953a":"code","85252e2b":"code","6528efd1":"code","eccc0ca3":"code","0f321749":"code","03faf684":"code","8c81642a":"code","979c1bb7":"code","8dc10b1a":"code","f9c92d6e":"code","dd649f41":"code","90eaa80a":"code","83691df5":"code","f1547b31":"code","e7bb752f":"code","3813b046":"code","8328d122":"code","d7347916":"code","329aaf59":"markdown","c862d2bf":"markdown","63adc791":"markdown","9be522d6":"markdown","3eb9d9f3":"markdown","ca85bf3a":"markdown","cc06d78c":"markdown","a388f103":"markdown","1559492b":"markdown","ac4ed744":"markdown","27ce609c":"markdown","b768ff5b":"markdown","cd329b8c":"markdown","9c0b2b97":"markdown","57f17e8c":"markdown","3968d785":"markdown","2c99a3d8":"markdown","85adb1d5":"markdown","40469089":"markdown","b35e5a46":"markdown","042b1a7a":"markdown","fd049050":"markdown","cf4af5b3":"markdown","464f0a0f":"markdown","dad033ec":"markdown","c43f97ee":"markdown","40eef7b2":"markdown","cdfed213":"markdown","500dfa32":"markdown","61ed1f40":"markdown","ba9602e2":"markdown","ad88f616":"markdown","c0f3f7ea":"markdown","4cf2c9bd":"markdown","97f8e982":"markdown","c3e13522":"markdown"},"source":{"a1c8e47a":"### Standard Imports\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n### Model Building\nfrom sklearn.model_selection import train_test_split, GridSearchCV, KFold, cross_val_score\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\n\n### Model Evaluation\nfrom sklearn.metrics import classification_report\n\n### Saving model for future use\nfrom joblib import dump, load","d5c6abdb":"df = pd.read_csv('..\/input\/digit-recognizer\/train.csv')","b1955f02":"df.head()","d3e87048":"df.info()","684d333b":"### Checking missing values\nfor i in df.columns:\n    if df[i].isna().sum():\n        print(i)","6065ee6f":"df.describe()","aed76b16":"df.label.value_counts()","201e4a75":"sns.countplot(x='label', data=df)\nplt.show()","e999927c":"X = df.drop('label', axis=1)\ny = df['label']","3ea54371":"# since the number of data points is large, we will be using only 10% of data for training\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, random_state=42)","463db209":"X_train.shape, X_test.shape, y_train.shape, y_test.shape","0d4a1d3e":"x_train = X_train.values.reshape(-1,28,28,1)\nplt.figure(figsize=(12,10))\nx, y = 15, 5\nfor i in range(75):  \n    plt.subplot(y, x, i+1)\n    plt.imshow(x_train[i],interpolation='nearest')\nplt.show()","bfd13e95":"scaler = StandardScaler()","13a5bdfb":"X_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","e9d68eee":"model = LogisticRegression(multi_class='multinomial', solver='sag', max_iter=1000, random_state=42)","b9bae26f":"for i in X_train.columns:\n    if not X_train[i].dtype == 'int64':\n        print(X_train[i].dtype)","43578cbe":"model.fit(X_train, y_train)","f0df28b4":"y_train_pred = model.predict(X_train)","3e4c5af0":"y_train_pred","4283e436":"from sklearn.metrics import classification_report","a094e36c":"pd.DataFrame(classification_report(y_train, y_train_pred, output_dict=True))","05520c75":"y_test_pred = model.predict(X_test)","212e9eae":"pd.DataFrame(classification_report(y_test, y_test_pred, output_dict=True))","b0f31ab3":"model.fit(X_train_scaled, y_train)\ny_train_pred = model.predict(X_train_scaled)\ny_test_pred = model.predict(X_test_scaled)","68ac079f":"pd.DataFrame(classification_report(y_train, y_train_pred, output_dict=True))","aa379d17":"pd.DataFrame(classification_report(y_test, y_test_pred, output_dict=True))","cd9ada75":"pca = PCA(0.95, random_state=42)","956adc4b":"X_train_scaled_pca = pca.fit_transform(X_train_scaled)\nX_test_scaled_pca = pca.transform(X_test_scaled)","81b05b16":"X_train.shape","b9cc6c7a":"X_train_scaled_pca.shape","f5b3f90e":"model.fit(X_train_scaled_pca, y_train)\ny_train_pred = model.predict(X_train_scaled_pca)\ny_test_pred = model.predict(X_test_scaled_pca)","0537f202":"pd.DataFrame(classification_report(y_train, y_train_pred, output_dict=True))","2724a77b":"pd.DataFrame(classification_report(y_test, y_test_pred, output_dict=True))","a6f301ff":"svc = SVC(C=1, kernel='linear', random_state=42)","438adbf8":"model = svc.fit(X_train_scaled, y_train)","36eabfc5":"y_train_pred = model.predict(X_train_scaled)\ny_test_pred = model.predict(X_test_scaled)","d3ed2721":"pd.DataFrame(classification_report(y_train, y_train_pred, output_dict=True))","fa328224":"pd.DataFrame(classification_report(y_test, y_test_pred, output_dict=True))","0bc950e0":"svc = SVC(C=1, kernel='rbf', random_state=42)","e21da0d0":"model = svc.fit(X_train_scaled, y_train)","95336d32":"y_train_pred = model.predict(X_train_scaled)\ny_test_pred = model.predict(X_test_scaled)","184e3dd1":"pd.DataFrame(classification_report(y_train, y_train_pred, output_dict=True))","98c2948b":"pd.DataFrame(classification_report(y_test, y_test_pred, output_dict=True))","110e0220":"svc = SVC(C=1, kernel='rbf', random_state=42)","936bbaba":"model = svc.fit(X_train_scaled_pca, y_train)","62e999ab":"y_train_pred = model.predict(X_train_scaled_pca)\ny_test_pred = model.predict(X_test_scaled_pca)","bcb61d83":"pd.DataFrame(classification_report(y_train, y_train_pred, output_dict=True))","5ec45cf2":"pd.DataFrame(classification_report(y_test, y_test_pred, output_dict=True))","d7bfe26c":"dump(model, 'svm_rbf_pca.joblib') ","f37dd71d":"test_df = pd.read_csv('..\/input\/digit-recognizer\/test.csv')","58d6aba7":"test_df.head()","7b3f953a":"X_t = test_df.copy()","85252e2b":"X_t_scaled = scaler.transform(X_t)\nX_t_scaled_pca = pca.transform(X_t_scaled)","6528efd1":"pred = model.predict(X_t_scaled_pca)","eccc0ca3":"submission = pd.read_csv('..\/input\/digit-recognizer\/sample_submission.csv')","0f321749":"submission.head()","03faf684":"pred","8c81642a":"submission['Label'] = pred","979c1bb7":"submission.head()","8dc10b1a":"submission.shape","f9c92d6e":"submission.to_csv('submission.csv', index=False)","dd649f41":"X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.1, random_state=42)","90eaa80a":"X_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)","83691df5":"X_train_scaled_pca = pca.fit_transform(X_train_scaled)\nX_test_scaled_pca = pca.transform(X_test)","f1547b31":"folds = KFold(n_splits = 5, shuffle = True, random_state = 42)\n\n# specify range of hyperparameters\n# Set the parameters by cross-validation\nhyper_params = [ {'gamma': [1e-2, 1e-3, 1e-4],\n                     'C': [1, 10, 100, 1000]}]\n\n\n# specify model\nmodel = SVC(kernel=\"rbf\")\n\n# set up GridSearchCV()\nmodel_cv = GridSearchCV(estimator = model, \n                        param_grid = hyper_params, \n                        scoring= 'accuracy', \n                        cv = folds, \n                        verbose = 1,\n                        return_train_score=True)      \n\n# fit the model\nmodel_cv.fit(X_train_scaled_pca, y_train)","e7bb752f":"model_cv.best_score_","3813b046":"model_cv.best_params_","8328d122":"cv_results = pd.DataFrame(model_cv.cv_results_)\ncv_results","d7347916":"# converting C to numeric type for plotting on x-axis\ncv_results['param_C'] = cv_results['param_C'].astype('int')\n\n# # plotting\nplt.figure(figsize=(16,6))\n\n# subplot 1\/3\nplt.subplot(131)\ngamma_01 = cv_results[cv_results['param_gamma']==0.01]\n\nplt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_test_score\"])\nplt.plot(gamma_01[\"param_C\"], gamma_01[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.01\")\nplt.ylim([0.60, 1])\nplt.legend(['test accuracy', 'train accuracy'], loc='lower right')\nplt.xscale('log')\n\n# subplot 2\/3\nplt.subplot(132)\ngamma_001 = cv_results[cv_results['param_gamma']==0.001]\n\nplt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_test_score\"])\nplt.plot(gamma_001[\"param_C\"], gamma_001[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.001\")\nplt.ylim([0.60, 1])\nplt.legend(['test accuracy', 'train accuracy'], loc='lower right')\nplt.xscale('log')\n\n\n# subplot 3\/3\nplt.subplot(133)\ngamma_0001 = cv_results[cv_results['param_gamma']==0.0001]\n\nplt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_test_score\"])\nplt.plot(gamma_0001[\"param_C\"], gamma_0001[\"mean_train_score\"])\nplt.xlabel('C')\nplt.ylabel('Accuracy')\nplt.title(\"Gamma=0.0001\")\nplt.ylim([0.60, 1])\nplt.legend(['test accuracy', 'train accuracy'], loc='lower right')\nplt.xscale('log')\n\nplt.show()","329aaf59":"##### Overall Accuracy on test set increased by 1% to 91.6%","c862d2bf":"---","63adc791":"### SVM - RBF Kernel","9be522d6":"### Let's see what our model is trying to learn by Visualizing the Data as actual images","3eb9d9f3":"### Logistic Regression - Scaled","ca85bf3a":"##### Overall Accuracy on test set increased by 0.5% to 90.6%","cc06d78c":"# Digit Recognition using Linear Models\n---","a388f103":"### SVM - RBF + Hyper Parameter Tuning + PCA","1559492b":"---","ac4ed744":"##### Overall Accuracy on test set increased by 0.5% to 92%","27ce609c":"## SVM - Linear Kernel","b768ff5b":"##### Observation: there are no missing values in the dataset","cd329b8c":"---","9c0b2b97":"---","57f17e8c":"## Submissions","3968d785":"##### Saving the model for future use due to extremely high test accurary","2c99a3d8":"### Logistic Regression - Scaled + PCA","85adb1d5":"---","40469089":"##### Overall Accuracy on test set is around 90%","b35e5a46":"##### Observation: The data is relatively equally distributed among various labels","042b1a7a":"### Why Linear Models\nAlthough this problem can be solved much more effectively using Deep Learning, I wanted to show that linear models are also peforming at a very good level. As you will see in the results below, iteratively we are able to achieve better and better results by trying different types of linear models. \n\nThe best model we got was by SVM with Radias Bias Kernel whose training input was standardarized and transformed using PCA. The benefit of PCA is that we can reduce the dimensionality while keeping most of the information. This helps in reducing the overall run time. \n\nThe best accuracy score on the test set was around 96%. A similar result is seen in the final submission as well.","fd049050":"### SVM - RBF Kernel + PCA","cf4af5b3":"---","464f0a0f":"### Models Used\n- Logistic Regression with Multi Class solver\n- Support Vector Machine\n\n### Techniques\n- Principal Component Analysis\n- Cross Validation","dad033ec":"---","c43f97ee":"## Reading and Understanding the Data","40eef7b2":"### Train Test Split","cdfed213":"---","500dfa32":"### Scaling","61ed1f40":"#### Observation\n- Hyper Parameter tuning takes a really long time to complete due to which we have taken only 10% of the data for training. This has resulted in poor test score compared to the previous models which had exposure to the full training data. Hence, I won't be using this model for predictions.\n- I actually tried cross_val_score training on the whole data with default parameters as well, but that was also taking an unreasonably long amount of time, so I killed it.","ba9602e2":"## Imports","ad88f616":"## Logistic Regression MultiNomial","c0f3f7ea":"#### Overall Accuracy on test set increased by 0.1% to `95.9`%\n- pca had not much effect on accuracy, but it definitely reduced dimensions and hence the computation time","4cf2c9bd":"---","97f8e982":"##### Overall Accuracy on test set increased by 3.8% to 95.8%","c3e13522":"## Model Building"}}