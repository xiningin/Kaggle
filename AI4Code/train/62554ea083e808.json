{"cell_type":{"a91ae89e":"code","0e27c458":"code","4d52a1b8":"code","5499983f":"code","311cd877":"code","c9ad8d92":"code","2627e167":"code","76a732fd":"code","6bd2f642":"code","f4e8d343":"code","0062160c":"code","44f51fee":"code","e0053db6":"code","e556c824":"code","ee2d03dd":"code","78cd8cc4":"code","5924b01e":"code","dfcd51a1":"code","33d82d11":"code","3aaaef9c":"code","0f4c91ca":"code","8c2b306f":"code","7972968d":"code","6ccb46c0":"code","5437a184":"code","a338df7c":"code","6b065bde":"code","7a212358":"code","78325afe":"code","d5ba596b":"code","169cb666":"code","b1bbd4b5":"code","69fe9a16":"code","60764bad":"code","cae39ae6":"code","cbec6491":"code","b2624ce4":"code","90825e70":"code","b3f4526f":"code","1e61f58a":"code","b0036462":"code","222756b1":"code","c7463c1e":"code","8ebe2829":"code","648a5f36":"code","e2cd4608":"markdown","53e9ddf2":"markdown","a40e0a0d":"markdown","043344ad":"markdown","a64a47ea":"markdown","cbbf8d18":"markdown","09641e21":"markdown","829ae692":"markdown","8cdc0531":"markdown","79983e8d":"markdown","154a9eef":"markdown","da755853":"markdown","c8533e33":"markdown","ceaa1af6":"markdown","f1ba1957":"markdown","77e76183":"markdown","d245c7f7":"markdown","cc82c55c":"markdown","ee3937f5":"markdown","e57e12ef":"markdown"},"source":{"a91ae89e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0e27c458":"# Importando os dados\ntrain = pd.read_csv('\/kaggle\/input\/big-mart-sales-prediction-datasets\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/big-mart-sales-prediction-datasets\/test.csv')\ntest2 = test.copy()\ntrain.shape, test.shape","4d52a1b8":"# Tipos:\ntrain.info()","5499983f":"# Valores \u00fanicos de cada coluna:\ntrain.nunique()","311cd877":"# Quais s\u00e3o os valores nulos:\ntrain.isna().sum()","c9ad8d92":"test.isna().sum()","2627e167":"# Analisando a vari\u00e1vel target:\ntrain['Item_Outlet_Sales'].describe()","76a732fd":"#Imputando valores das notas a partir da m\u00e9dia\nfrom sklearn.impute import SimpleImputer\nimport numpy as np\n\nimputer = SimpleImputer(missing_values=np.nan, strategy=\"mean\")\nimputer.fit(test[['Item_Weight']])\ntest['Item_Weight'] = imputer.transform(test[['Item_Weight']]).ravel()","6bd2f642":"# Criando um dicion\u00e1rio para mapear todos os registros em 'low' ou 'regular':\nitem_fat = {'Low Fat':'low', 'Regular':'regular', 'LF':'low', 'reg':'regular','low fat':'low'}\n\ntrain['Item_Fat_Content'] = train['Item_Fat_Content'].map(item_fat)\ntest['Item_Fat_Content'] = test['Item_Fat_Content'].map(item_fat)\n\n# Verificando\ntrain['Item_Fat_Content'].unique()","f4e8d343":"# Item_Fat_Content:\ntrain['Item_Fat_Content'].unique()","0062160c":"# Verificando Outlet_Size\ntrain['Outlet_Size'].unique()","44f51fee":"# Verificando Outlet_Location_Type\ntrain['Outlet_Location_Type'].unique()","e0053db6":"# Verificando Outlet_Type\ntrain['Outlet_Type'].unique()","e556c824":"# Verificando Item_Type\ntrain['Item_Type'].unique()","ee2d03dd":"# Verificando Outlet_Identifier\ntrain['Outlet_Identifier'].unique()","78cd8cc4":"# Quantos valores \u00fanicos de Item_Identifier\ntrain['Item_Identifier'].nunique()","5924b01e":"# Tratando 'Item_Weight' e 'Outlet_Size':\n\n# Vamos usar a m\u00e9dia para imputar valores em Item_Weight\ntrain['Item_Weight'].fillna(train['Item_Weight'].mean(), inplace=True)\n\n# Vamos usar a moda para imputar valores em Outlet_Size\ntrain['Outlet_Size'].fillna(train['Outlet_Size'].mode()[0], inplace=True)","dfcd51a1":"# Vari\u00e1veis categ\u00f3ricas:\ntrain.dtypes","33d82d11":"test.columns","3aaaef9c":"test.dtypes","0f4c91ca":"# Codificando as colunas categ\u00f3ricas usando one hot encoding:\ncat_cols = ['Item_Fat_Content', 'Item_Type', 'Outlet_Identifier', 'Outlet_Size', 'Outlet_Location_Type', 'Outlet_Type']\n\ntrain = pd.get_dummies(train, columns=cat_cols)\ntest = pd.get_dummies(test, columns=cat_cols)\n\ntrain.shape","8c2b306f":"# Resultado final:\ntrain.head()","7972968d":"# Verificando as colunas\ntrain.info()","6ccb46c0":"# Separando o dataframe\nfrom sklearn.model_selection import train_test_split\n\ntrain, valid = train_test_split(train, random_state=42)\n\ntrain.shape, valid.shape","5437a184":"# Obtendo as colunas para treinamento\nfeatures = [c for c in train.columns if c not in ['Item_Identifier', 'Item_Outlet_Sales']]\n\nfeatures","a338df7c":"# RandomForest:\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf_padrao = RandomForestRegressor(n_estimators=200, n_jobs=-1, random_state=42)\n\nrf_padrao.fit(train[features], train['Item_Outlet_Sales'])","6b065bde":"# Predi\u00e7\u00e3o:\npreds_rf = rf_padrao.predict(valid[features])\npreds_rf","7a212358":"# Calculando a m\u00e9trica:\nfrom sklearn.metrics import mean_squared_error\n\nrmse_rf = mean_squared_error(valid['Item_Outlet_Sales'], preds_rf, squared=False)\n\nrmse_rf","78325afe":"# GBR:\nfrom sklearn.ensemble import GradientBoostingRegressor\n\ngbr_padrao = GradientBoostingRegressor(n_estimators=200, random_state=42)\n\ngbr_padrao.fit(train[features], train['Item_Outlet_Sales'])\n\npreds_gbr = gbr_padrao.predict(valid[features])\n\nrmse_gbr = mean_squared_error(valid['Item_Outlet_Sales'], preds_gbr, squared=False)\n\nrmse_gbr","d5ba596b":"# AdaBoost:\nfrom sklearn.ensemble import AdaBoostRegressor\n\nada_padrao = AdaBoostRegressor(n_estimators=200, random_state=42)\n\nada_padrao.fit(train[features], train['Item_Outlet_Sales'])\n\npreds_ada = ada_padrao.predict(valid[features])\n\nrmse_ada = mean_squared_error(valid['Item_Outlet_Sales'], preds_ada, squared=False)\n\nrmse_ada","169cb666":"# Regress\u00e3o Linear:\n\nfrom sklearn.linear_model import LinearRegression\n\nlr_padrao = LinearRegression()\n\nlr_padrao.fit(train[features], train['Item_Outlet_Sales'])\n\npreds_lr = lr_padrao.predict(valid[features])\n\nrmse_lr = mean_squared_error(valid['Item_Outlet_Sales'], preds_lr, squared=False)\n\nrmse_lr","b1bbd4b5":"# BaggingRegressor:\n\nfrom sklearn.ensemble import  BaggingRegressor \n\nbgr_padrao = BaggingRegressor()\n\nbgr_padrao.fit(train[features], train['Item_Outlet_Sales'])\n\npreds_bgr = bgr_padrao.predict(valid[features])\n\nrmse_bgr = mean_squared_error(valid['Item_Outlet_Sales'], preds_bgr, squared=False)\n\nrmse_bgr","69fe9a16":"# VotingRegressor:\nfrom sklearn.ensemble import VotingRegressor\n\nestimators = [('rf_padrao', rf_padrao),('gbr_padrao',gbr_padrao),('ada_padrao', ada_padrao),('lr_padrao', lr_padrao),('bgr_padrao', bgr_padrao)]\n\nensemble1 = VotingRegressor(estimators=estimators, n_jobs=-1)\n\nensemble1.fit(train[features], train['Item_Outlet_Sales'])\n\npreds_ens = ensemble1.predict(valid[features])\n\nrmse_ens = mean_squared_error(valid['Item_Outlet_Sales'], preds_ens, squared=False)\n\nrmse_ens","60764bad":"# Modelo 1:\npreds_rf = rf_padrao.predict(test[features])\npreds_rf","cae39ae6":"# Modelo 2:\npreds_gbr = gbr_padrao.predict(test[features])\npreds_gbr","cbec6491":"# Modelo 3:\npreds_ada = ada_padrao.predict(test[features])\npreds_ada","b2624ce4":"# Modelo 4:\npreds_lr = lr_padrao.predict(test[features])\npreds_lr","90825e70":"# Modelo 5:\npreds_bgr = bgr_padrao.predict(test[features])\npreds_bgr","b3f4526f":"# Voting Regressor:\npreds_ens = ensemble1.predict(test[features])\npreds_ens","1e61f58a":"# Modelo 1:\ndf_rf = test2[['Item_Identifier','Outlet_Identifier']].copy()\ndf_rf['Item_Outlet_Sales'] = preds_rf\ndf_rf.to_csv('predict_rf.csv')\ndf_rf.head()","b0036462":"# Modelo 2:\ndf_gbr = test2[['Item_Identifier','Outlet_Identifier']].copy()\ndf_gbr['Item_Outlet_Sales'] = preds_gbr\ndf_gbr.to_csv('predict_gbr.csv')\ndf_gbr.head()","222756b1":"# Modelo 3:\ndf_ada = test2[['Item_Identifier','Outlet_Identifier']].copy()\ndf_ada['Item_Outlet_Sales'] = preds_ada\ndf_ada.to_csv('predict_ada.csv')\ndf_ada.head()","c7463c1e":"# Modelo 4:\ndf_lr= test2[['Item_Identifier','Outlet_Identifier']].copy()\ndf_lr['Item_Outlet_Sales'] = preds_lr\ndf_lr.to_csv('predict_lr.csv')\ndf_lr.head()","8ebe2829":"# Modelo 5:\ndf_bgr= test2[['Item_Identifier','Outlet_Identifier']].copy()\ndf_bgr['Item_Outlet_Sales'] = preds_bgr\ndf_bgr.to_csv('predict_bgr.csv')\ndf_bgr.head()","648a5f36":"# Voting Regressor:\n\ndf_ens = test2[['Item_Identifier','Outlet_Identifier']].copy()\ndf_ens['Item_Outlet_Sales'] = preds_ens\ndf_ens = df_ens.set_index('Item_Identifier')\ndf_ens.to_csv('predict_ens.csv')\ndf_ens.head()","e2cd4608":"[AVALIA\u00c7\u00c3O] Trabalho de Machine Learning\n\nUsando como base a Aula 07,que trata de Ensemble de Modelos (https:\/\/www.kaggle.com\/marcosvafg\/iesb-cia035-aula-07-ensemble-de-modelos), vamos fazer diversas previs\u00f5es para os dados da Competi\u00e7\u00e3o Big Mart Sales Prediction.\n\n\nPara participar da competi\u00e7\u00e3o e poder submeter seus dados, voc\u00ea deve se registrar no site - https:\/\/datahack.analyticsvidhya.com\/contest\/practice-problem-big-mart-sales-iii\n\n\nComo entrega para o trabalho voc\u00ea deve:\n - Criar um notebook no Kaggle usando como base a Aula 07\n - Fazer an\u00e1lise completa dos dados da base\n - Criar pelos menos 5 modelos diferentes, fazer as previs\u00f5es e submeter cada uma delas em separado\n - Registrar o resultado de cada submiss\u00e3o no pr\u00f3prio notebook\n - Usar todas as t\u00e9cnicas e mecanismos aprendidos para otimizar cada um dos 5 modelos escolhidos\n - Ao final voc\u00ea deve usar os melhores modelos em um VotingRegressor, submeter as previs\u00f5es e registrar o resultado no notebook\n \nO link para o notebook criado no Kaggle deve ser enviado para meu e-mail (marcosvafg@gmail) at\u00e9 o s\u00e1bado, dia 16\/10.\n\n\nBons estudos e bom trabalho\nProf. Marcos Guimar\u00e3es","53e9ddf2":"# **3 - Escolher e testar 5 modelos:**","a40e0a0d":"**Quais os problemas \u00e1 serem resolvidos?**\n* A vari\u00e1vel Item_Weight tem 1463 valores nulos e a Outlet_Size tem 2410 valores nulos;\n* Item_Fat_Content tem valores em excesso;\n* As vari\u00e1veis categ\u00f3ricas precisam ser codificadas;","043344ad":"# **Modelo 4**","a64a47ea":"O modelo 3 se trata de um Ada Boost, que \u00e9 um metaestimador que come\u00e7a ajustando um regressor no conjunto de dados original e, em seguida, ajusta c\u00f3pias adicionais do regressor no mesmo conjunto de dados, mas onde os pesos das inst\u00e2ncias s\u00e3o ajustados de acordo com o erro da previs\u00e3o atual. Como tal, os regressores subsequentes se concentram mais nos casos dif\u00edceis.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.AdaBoostRegressor.html","cbbf8d18":"Um regressor de vota\u00e7\u00e3o \u00e9 um metaestimador de conjunto que se ajusta a v\u00e1rios regressores de base, cada um no conjunto de dados inteiro. Em seguida, ele calcula a m\u00e9dia das previs\u00f5es individuais para formar uma previs\u00e3o final.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.VotingRegressor.html","09641e21":"# Modelo 3","829ae692":"## Modelo 1","8cdc0531":"O modelo 4 se trata de um LinearRegression, ele ajusta um modelo linear com coeficientes w = (w1, ..., wp) para minimizar a soma residual dos quadrados entre os alvos observados no conjunto de dados e os alvos previstos pela aproxima\u00e7\u00e3o linear.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.linear_model.LinearRegression.html","79983e8d":"O modelo 5 se trata de Bagging regressor, \u00e9 um metaestimador de conjunto que ajusta cada um dos regressores de base em subconjuntos aleat\u00f3rios do conjunto de dados original e, em seguida, agrega suas previs\u00f5es individuais (por vota\u00e7\u00e3o ou por m\u00e9dia) para formar uma previs\u00e3o final. Esse metaestimador pode normalmente ser usado como uma forma de reduzir a vari\u00e2ncia de um estimador de caixa preta (por exemplo, uma \u00e1rvore de decis\u00e3o), introduzindo a aleatoriza\u00e7\u00e3o em seu procedimento de constru\u00e7\u00e3o e, em seguida, fazendo um conjunto a partir dela.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.BaggingRegressor.html","154a9eef":"O modelo 2 se trata de um Gradient Boosting Regressor, que constr\u00f3i um modelo aditivo de forma avan\u00e7ada no est\u00e1gio; permite a otimiza\u00e7\u00e3o de fun\u00e7\u00f5es de perda diferenci\u00e1veis \u200b\u200barbitr\u00e1rias. Em cada est\u00e1gio, uma \u00e1rvore de regress\u00e3o \u00e9 ajustada no gradiente negativo da fun\u00e7\u00e3o de perda fornecida.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.GradientBoostingRegressor.html","da755853":"# **2 - Tratamento dos dados:**","c8533e33":" **Resultado:**\n Os valores nulos foram tratados atrav\u00e9s da m\u00e9dia e da moda, as colunas categ\u00f3ricas foram codificadas com o one hot encoding e o item fat foi ajustado em regular e low.","ceaa1af6":"# **5 - VotingRegressor:**","f1ba1957":"# **6 - Dados de teste:**","77e76183":"# **7 - Resultados:**\nCriando dataframes com os resultados, que seram passados para CSV e depois submetidos","d245c7f7":"# Modelo 2","cc82c55c":"O modelo 1 se trata de um random forest, que \u00e9 um metaestimador que ajusta v\u00e1rias \u00e1rvores de decis\u00e3o de classifica\u00e7\u00e3o em v\u00e1rias subamostras do conjunto de dados e usa a m\u00e9dia para melhorar a precis\u00e3o preditiva e o sobreajuste de controle.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestRegressor.html","ee3937f5":"# **Modelo 5**","e57e12ef":"# **1 - An\u00e1lise explorat\u00f3ria:**"}}