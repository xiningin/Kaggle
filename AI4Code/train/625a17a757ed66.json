{"cell_type":{"99caddff":"code","bd8e1354":"code","1127fd09":"code","91127fb8":"code","7842d087":"code","7530675f":"code","12c76796":"code","265219c8":"code","de709676":"code","0b3c5952":"code","127f9cf8":"code","65630f78":"code","7c2c6ce0":"code","c35b3a24":"code","3d8fb578":"code","fc910147":"code","694cd70a":"code","7a9758af":"code","0f7d2a88":"code","adbe71dd":"code","33501708":"code","d4c567a0":"markdown","bf9228d2":"markdown","4312da53":"markdown","929a0f2e":"markdown","ebaf3b6d":"markdown","52e7b777":"markdown","2a0ec417":"markdown","3b4bbae0":"markdown","b86413f2":"markdown","8744bee0":"markdown","29634e1a":"markdown","9feb7665":"markdown","8a07bc38":"markdown","44deffe6":"markdown","750a846c":"markdown","03e19108":"markdown","ba6662b9":"markdown","626f2883":"markdown","dbc09af3":"markdown","61403ab8":"markdown","218979c2":"markdown","7112293c":"markdown","44f03049":"markdown","df059a48":"markdown","1170c229":"markdown","c4ba9d17":"markdown","f85ebf90":"markdown","d4bd5b60":"markdown","dbfaf7b9":"markdown","82898dfd":"markdown","47f65f12":"markdown","b5453429":"markdown","0f3cde34":"markdown","a667f045":"markdown","a5d64aa1":"markdown","48147593":"markdown","4847b6b2":"markdown","c4c62d5c":"markdown","d44da24a":"markdown","f5b286a6":"markdown","505c4099":"markdown","d8eb4e3d":"markdown","8e33941a":"markdown","6167c026":"markdown","bcad9d8a":"markdown","73581f30":"markdown","7b6d36f3":"markdown","c5b3cdc1":"markdown","3cf0285d":"markdown"},"source":{"99caddff":"!pip install pycaret","bd8e1354":"import numpy as np\nimport pandas as pd","1127fd09":"housing_data = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nhousing_data.head()","91127fb8":"categorical = []\nfor i in housing_data.columns:\n    if (housing_data[i].dtype=='object'):\n        categorical.append(i)\nprint(\"Categorical Attribute : {}\\n \".format(len(categorical)))\ncategorical.append('MSSubClass')\nfor x in range(len(categorical)): \n    print(categorical[x])\n","7842d087":"(housing_data[categorical].nunique()).sort_values(ascending=False)","7530675f":"for i in categorical:\n    print(i)\n    print(housing_data[i].value_counts())\n    print()\n","12c76796":"housing_data.shape","265219c8":"from pycaret.regression import *\nreg_experiment = setup(housing_data, \n                       target = 'SalePrice', \n                       session_id=42, \n                       experiment_name='me_housing',\n                       ignore_features=['Id'],\n                       normalize = True, \n                  transformation = True, \n                  remove_multicollinearity = True, #rop one of the two features that are highly correlated with each other\n                  ignore_low_variance = True,#all categorical features with statistically insignificant variances are removed from the dataset.\n                  combine_rare_levels = True,# all levels in categorical features below the threshold defined in rare_level_threshold param are combined together as a single level\n                    transform_target = True,\n                       categorical_features=categorical,ordinal_features = {\n                         'Utilities' : ['AllPub', 'NoSeWa'],\n                           'LandSlope':['Gtl', 'Mod', 'Sev'],\n                           'OverallQual':['1','2','3','4','5','6','7','8','9','10'],\n                           'MoSold':['1','2','3','4','5','6','7','8','9','10','11','12'],\n                       },\n                      high_cardinality_features =['Neighborhood','Exterior2nd','MSSubClass','Exterior1st']\n                           )","de709676":"best_model = compare_models()","0b3c5952":"catboost = create_model('catboost')","127f9cf8":"tuned_catboost = tune_model(catboost, optimize = 'MSE')","65630f78":"plot_model(tuned_catboost)","7c2c6ce0":"plot_model(tuned_catboost, plot = 'error')","c35b3a24":"plot_model(tuned_catboost, plot = 'feature')","3d8fb578":"print(evaluate_model(tuned_catboost))","fc910147":"interpret_model(tuned_catboost)","694cd70a":"automl_model = automl(optimize = 'MSE')","7a9758af":"automl_model","0f7d2a88":"pred_holdouts = predict_model(automl_model)\npred_holdouts.head()","adbe71dd":"save_model(automl_model, model_name='.\/automl-model')","33501708":"loaded_model = load_model('.\/automl-model')\nprint(loaded_model)","d4c567a0":"PyCaret also allows us to save trained models with the save_model function. This function saves the transformation pipeline for the model to a pickle file","bf9228d2":"# Pros and Cons of Using PyCaret","4312da53":"# AutoML","929a0f2e":"With just one line of code, we can create a SHAP beeswarm plot for the model.","ebaf3b6d":"Based on the plot above, we can see that the GrLivArea field has the greatest impact on the predicted house value.","52e7b777":"There are many plots that we can create with PyCaret to visualize a model\u2019s performance. PyCaret uses another high-level library called Yellowbrick for building these visualizations.","2a0ec417":"In the code below, We simply imported Numpy and Pandas for handling the data for this demonstration.","3b4bbae0":"When we approach supervised machine learning problems, it can be tempting to just see how a random forest or gradient boosting model performs and stop experimenting if we are satisfied with the results. What if you could compare many different models with just one line of code? What if you could reduce each step of the data science process from feature engineering to model deployment to just a few lines of code?\n\nThis is exactly where PyCaret comes into play. PyCaret is a high-level, low-code Python library that makes it easy to compare, train, evaluate, tune, and deploy machine learning models with only a few lines of code. At its core, PyCaret is basically just a large wrapper over many data science libraries such as Scikit-learn, Yellowbrick, SHAP, Optuna, and Spacy. Yes, you could use these libraries for the same tasks, but if you don\u2019t want to write a lot of code, PyCaret could save you a lot of time.","b86413f2":"# Import Libraries","8744bee0":"# Read the Data","29634e1a":"While PyCaret is a great tool, it comes with its own pros and cons that you should be aware of if you plan to use it for your data science projects.\n\n**Pros**\n- Low-code library.\n- Great for simple, standard tasks and general-purpose machine learning.\n- Provides support for regression, classification, natural language processing, clustering, anomaly detection, and association rule mining.\n- Makes it easy to create and save complex transformation pipelines for models.\n- Makes it easy to visualize the performance of your model.\n\n**Cons**\n- As of now, PyCaret is not ideal for text classification because the NLP utilities are limited to topic modeling algorithms.\n- PyCaret is not ideal for deep learning and doesn\u2019t use Keras or PyTorch models.\n- We can\u2019t perform more complex machine learning tasks such as image classification and text generation with PyCaret.\n- By using PyCaret, we are sacrificing a certain degree of control for simple and high-level code.","9feb7665":"PyCaret also has a function for running automated machine learning (AutoML). We can specify the loss function or metric that we want to optimize and then just let the library take over as demonstrated below.","8a07bc38":"Now that we have a trained model, we can optimize it even further with hyperparameter tuning. With just one line of code, we can tune the hyperparameters of this model.","44deffe6":"To install the default, smaller version of PyCaret with only the required dependencies, you can run the following command.","750a846c":"The interpret_model function is a useful tool for explaining the predictions of a model. This function uses a library for explainable machine learning called SHAP ","03e19108":"# Saving the Model","ba6662b9":"AutoML model also happens to be a CatBoost regressor, which we can confirm by printing out the model.","626f2883":"The predict_model function above produces predictions for the holdout datasets used for validating the model during cross-validation. The code also gives us a dataframe with performance statistics for the predictions generated by the AutoML model.","dbc09af3":"The most important results, in this case, the average metrics, are highlighted in yellow.","61403ab8":"Now that we have the data, we can initialize a PyCaret experiment, which will preprocess the data and enable logging for all of the models that we will train on this dataset.","218979c2":"# Creating a Model","7112293c":"The plot above is particularly useful because it gives us a visual representation of the R\u00b2 coefficient for the CatBoost model. In a perfect scenario (R\u00b2 = 1), where the predicted values exactly matched the actual target values, this plot would simply contain points along the dashed identity line.","44f03049":"# PyCaret \u2014 the library for low-code ML","df059a48":"The create_model function produces the dataframe above with cross-validation metrics for the trained CatBoost model.","1170c229":"## Residual Plot","c4ba9d17":"# Initialize Experiment","f85ebf90":"# Interpreting the Model","d4bd5b60":"We can also create multiple plots for evaluating a model with the evaluate_model function.","dbfaf7b9":"The plot_model function will produce a residual plot by default for a regression model as demonstrated below.","82898dfd":"# Feature Importances","47f65f12":"Printing out the loaded model produces the output","b5453429":"The function produces a data frame with the performance statistics for each model and highlights the metrics for the best performing model, which in this case was the CatBoost regressor.","0f3cde34":"The output above gives us an idea of what the data looks like. The data contains mostly numerical features with multiple categorical features. The target column that we are trying to predict is the SalePrice column. The entire dataset contains a total of 1460 observations.","a667f045":"# Evaluating the Model Using All Plots","a5d64aa1":"We can compare different baseline models at once to find the model that achieves the best K-fold cross-validation performance with the compare_models function as shown in the code below. ","48147593":"The predict_model function allows us to generate predictions by either using data from the experiment or new unseen data.","4847b6b2":"We can also visualize the predicted values against the actual target values by creating a prediction error plot.","c4c62d5c":"# Prediction Error","d44da24a":"We can also visualize the feature importances for a model as shown below.","f5b286a6":"Based on the plot above, we can see that the median_income feature is the most important feature when predicting the price of a house. Since this feature corresponds to the median income in the area in which a house was built, this evaluation makes perfect sense. Houses built in higher-income areas are likely more expensive than those in lower-income areas.","505c4099":"For this example, We used the California Housing Prices Dataset available on Kaggle. In the code below, I read this dataset into a dataframe and displayed the first five rows of the dataframe.","d8eb4e3d":"# Hyperparameter Tuning","8e33941a":"Train, visualize, evaluate, interpret, and deploy models with minimal code","6167c026":"# Visualizing the Model\u2019s Performance","bcad9d8a":"We can also train a model in just a single line of code with PyCaret. The create_model function simply requires a string corresponding to the type of model that you want to train. ","73581f30":"# Installing PyCaret","7b6d36f3":"# Generating Predictions","c5b3cdc1":"# Compare Baseline Models","3cf0285d":"We can also load the saved AutoML model with the load_model function."}}