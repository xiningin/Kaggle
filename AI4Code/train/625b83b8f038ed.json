{"cell_type":{"7018165d":"code","3da89315":"code","fd03f96b":"code","0a5376b6":"code","d6fb69d9":"code","a888c09d":"code","eb1188d0":"code","b316e444":"code","c508a6ac":"code","dcce2663":"code","cc1ddba4":"code","614c1da4":"code","642b0a79":"code","16d4233c":"code","906572fa":"code","e7e7186e":"code","424bd3a4":"code","e15b6ad9":"code","eecf3e77":"code","30d9f0a8":"code","c73c3e82":"code","5352283e":"code","c3475828":"code","f2c69686":"code","1ba1a342":"code","b067c4d0":"code","f5648f4e":"markdown","1e5535de":"markdown","d1130a13":"markdown","c82a588a":"markdown","b741df4f":"markdown","b9e28a2b":"markdown","c1b7349f":"markdown","561fe920":"markdown","cd0bc9c1":"markdown"},"source":{"7018165d":"# %%capture\n# !curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n!pip install timm\n# !python pytorch-xla-env-setup.py --version 1.7","3da89315":"# Data preprocessing\n\nimport numpy as np \nimport pandas as pd\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import StratifiedKFold\n\n\n# Plotting\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# ANN + ML\n\nimport torch\nimport torch.nn as nn\nimport lightgbm as lgb\nfrom torch.optim import Adam\nfrom torch.utils.data import DataLoader, Dataset\nfrom torch.cuda.amp import GradScaler, autocast\nfrom torch.optim.lr_scheduler import CosineAnnealingLR,CosineAnnealingWarmRestarts, ReduceLROnPlateau\n\n\n# Image preprocessing\n\nimport cv2\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\n\n# Miscellanous\n\nimport os\nimport gc\nimport sys\nimport math\nimport time\nimport pickle\nimport random\nfrom tqdm.auto import tqdm\nsys.path.append('..\/input\/pytorch-image-models\/pytorch-image-models-master')\nimport timm\nimport warnings\n\n\nwarnings.filterwarnings('ignore')\ndevice = torch.device('cuda' if torch.cuda.is_available() else 'cpu')","fd03f96b":"# general global variables\nDATA_PATH = \"..\/input\/petfinder-pawpularity-score\"\nTRAIN_PATH = \"..\/input\/petfinder-pawpularity-score\/train\"\nTEST_PATH = \"..\/input\/petfinder-pawpularity-score\/test\"\n\nCFG = {\n    'batch_size' : 16,\n    'num_workers' : 4,\n    'n_fold': 5,\n    'epochs' : 4,\n    'lr' : 1e-4,\n    'gamma':0.7,\n    'img_size' : 384,\n    'model_name': 'vit_base_patch32_384',\n    'target_col':'Pawpularity',\n    'train':True,\n    'apex': False,\n    'trn_fold':[0,1,2,3,4],\n    'print_freq' : 100,\n    'gradient_accumulation_steps':1,\n    'device': 'GPU',\n    'freeze_epo' : 0, # GradualWarmupSchedulerV2\n    'warmup_epo' : 1, # GradualWarmupSchedulerV2\n    'cosine_epo' : 19, # GradualWarmupSchedulerV2\n    'epochs' : 20, # [freeze_epo, + warmup_epo + cosine_epo]\n    'scheduler':'CosineAnnealingLR',\n    'T_0' : 10,\n    'T_max': 3,\n    'min_lr':1e-6,\n    'weight_decay':1e-6,\n    'max_grad_norm':1000,\n    'seed' : 42,\n    'nprocs':1\n}","0a5376b6":"if CFG['device'] == 'TPU':\n    import ignite.distributed as idist\n    os.system('curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py')\n    os.system('python pytorch-xla-env-setup.py --version nightly --apt-packages libomp5 libopenblas-dev')\n    os.system('export XLA_USE_BF16=1')\n    os.system('export XLA_TENSOR_ALLOCATOR_MAXSIZE=100000000')\n    import torch_xla.core.xla_model as xm\n    import torch_xla.distributed.parallel_loader as pl\n    import torch_xla.distributed.xla_multiprocessing as xmp\n    import torch_xla\n    import torch_xla.core.xla_model as xm\n    import torch_xla.distributed.parallel_loader as pl\n    import torch_xla.distributed.xla_multiprocessing as xmp\n    \n    CFG['lr'] = CFG['lr'] * CFG['nprocs']\n    CFG['batch_size'] = CFG['batch_size'] \/\/ CFG['nprocs']    \n    \nelif CFG['device'] == 'GPU' and CFG['apex']:\n    from torch.cuda.amp import autocast, GradScaler","d6fb69d9":"train_df = pd.read_csv(DATA_PATH + \"\/train.csv\")\ntest_df = pd.read_csv(DATA_PATH  + \"\/test.csv\")\ntrain_df.head()","a888c09d":"def get_score(y_true, y_pred):\n    score = mean_squared_error(y_true, y_pred, squared=False) # RMSE\n    return score\n\ndef seed_everything(seed):\n    \"\"\"\n    Seeds basic parameters for reproductibility of results\n    \n    Arguments:\n        seed {int} -- Number of the seed\n    \"\"\"\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = False\n\nseed_everything(seed=CFG['seed'])","eb1188d0":"train_df[\"fold\"] = -1\nskf = StratifiedKFold(n_splits=CFG['n_fold'])\ntrain_df[\"bins\"] = pd.cut(train_df[\"Pawpularity\"], bins=10, labels=False)\ntarget = train_df[\"bins\"]\n\nfor fold, (train_idx, val_idx) in enumerate(skf.split(train_df, target)):\n    train_df.loc[val_idx, 'fold'] = fold\n    \n# train_df = train_df.drop([\"bins\"], axis=1)\ntrain_df['fold'] = train_df['fold'].astype(int)\ntrain_df.groupby(['fold','bins']).size()","b316e444":"train_df.to_pickle('.\/train_fold.pkl')","c508a6ac":"class PawpularityDataset(Dataset):\n    def __init__(self, df, augs=None):\n        self.df = df\n        self.augs = augs\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        image = self._load_image(self.df[\"Id\"].iloc[index])\n\n        # Apply image augmentations if available\n        if self.augs:\n            image = self.augs(image=image)[\"image\"]\n\n        return image, self.df[\"Pawpularity\"].iloc[index]\n\n    def _load_image(self, image_id):\n        image = cv2.imread(f\"{TRAIN_PATH}\/{image_id}.jpg\", cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        return image","dcce2663":"def get_train_augs():\n    return A.Compose([A.RandomResizedCrop(CFG['img_size'],CFG['img_size'],scale = (0.85, 1.0)),\n                         A.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]),\n                          ToTensorV2()])\n\n\ndef get_valid_augs():\n    return A.Compose([A.Resize(CFG['img_size'],CFG['img_size']),\n                         A.Normalize(mean = [0.485, 0.456, 0.406], std = [0.229, 0.224, 0.225]),\n                          ToTensorV2()])","cc1ddba4":"train_dataset = PawpularityDataset(train_df, augs=get_train_augs())\n\nfor i in range(5):\n    plt.figure(figsize=(4, 4))\n    image, label = train_dataset[i]\n    plt.imshow(image[0],cmap = 'gray')\n    plt.title(f'label: {label}')\n    plt.show()","614c1da4":"class PawpularityModelCNN(nn.Module):\n    def __init__(self, model_name = CFG['model_name'], pretrained = False):\n    \n        super().__init__()\n        \n        self.model = timm.create_model(model_name, pretrained=pretrained)\n        self.features = self.model.classifier.in_features\n        self.model.classifier = nn.Identity()\n        self.fc = nn.Linear(self.features, 1)\n        \n    def feature(self, image):\n        feature = self.model(image)\n        return feature\n    \n    def forward(self, image):\n        feature = self.feature(image)\n        output = self.fc(feature)\n        return output\n\nclass PawpularityModelViT(nn.Module):\n    def __init__(self, model_name=CFG['model_name'], pretrained=False):\n\n        super().__init__()\n\n        self.model = timm.create_model(model_name, pretrained=pretrained)\n        self.features = self.model.head.in_features\n        self.model.head = nn.Identity()\n        self.fc = nn.Linear(self.features, 1)\n        \n    def feature(self, image):\n        feature = self.model(image)\n        return feature \n    \n\n    def forward(self, image):\n        feature  = self.feature(image)\n        output = self.fc(feature)\n        return output","642b0a79":"class RMSELoss(nn.Module):\n    def __init__(self, eps=1e-6):\n        super().__init__()\n        self.mse = nn.MSELoss()\n        self.eps = eps\n\n    def forward(self, yhat, y):\n        loss = torch.sqrt(self.mse(yhat, y.float()) + self.eps)\n        return loss","16d4233c":"# ====================================================\n# Helper functions\n# ====================================================\nclass AverageMeter(object):\n    \"\"\"Computes and stores the average and current value\"\"\"\n    def __init__(self):\n        self.reset()\n\n    def reset(self):\n        self.val = 0\n        self.avg = 0\n        self.sum = 0\n        self.count = 0\n\n    def update(self, val, n=1):\n        self.val = val\n        self.sum += val * n\n        self.count += n\n        self.avg = self.sum \/ self.count\n\n\ndef asMinutes(s):\n    m = math.floor(s \/ 60)\n    s -= m * 60\n    return '%dm %ds' % (m, s)\n\n\ndef timeSince(since, percent):\n    now = time.time()\n    s = now - since\n    es = s \/ (percent)\n    rs = es - s\n    return '%s (remain %s)' % (asMinutes(s), asMinutes(rs))\n\ndef train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device):\n    if CFG['device'] == 'GPU':\n        scaler = GradScaler()\n        \n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    scores = AverageMeter()\n    # switch to train mode\n    model.train()\n    start = end = time.time()\n    global_step = 0\n    for step, (images, labels) in enumerate(train_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n        images = images.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        if CFG['device'] == 'GPU':\n            with autocast():\n                y_preds = model(images)\n                loss = criterion(y_preds.view(-1), labels)\n                # record loss\n                losses.update(loss.item(), batch_size)\n                if CFG['gradient_accumulation_steps'] > 1:\n                    loss = loss \/ CFG['gradient_accumulation_steps']\n                scaler.scale(loss).backward()\n                grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG['max_grad_norm'])\n                if (step + 1) % CFG['gradient_accumulation_steps'] == 0:\n                    scaler.step(optimizer)\n                    scaler.update()\n                    optimizer.zero_grad()\n                    global_step += 1\n        elif CFG['device'] == 'TPU':\n            y_preds = model(images)\n            loss = criterion(y_preds, labels)\n            # record loss\n            losses.update(loss.item(), batch_size)\n            if CFG['gradient_accumulation_steps'] > 1:\n                loss = loss \/ CFG['gradient_accumulation_steps']\n            loss.backward()\n            grad_norm = torch.nn.utils.clip_grad_norm_(model.parameters(), CFG['max_grad_norm'])\n            if (step + 1) % CFG['gradient_accumulation_steps'] == 0:\n                xm.optimizer_step(optimizer, barrier=True)\n                optimizer.zero_grad()\n                global_step += 1\n                \n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        \n        if CFG['device'] == 'GPU':\n            if (step % CFG['print_freq'] == 0) or (step == (len(train_loader)-1)):\n                print('Epoch: [{0}][{1}\/{2}] '\n                      'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                      'Elapsed {remain:s} '\n                      'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                      'Grad: {grad_norm:.4f}  '\n                      #'LR: {lr:.6f}'\n                      .format(\n                       epoch+1, step, len(train_loader), batch_time=batch_time,\n                       data_time=data_time, loss=losses,\n                       remain=timeSince(start, float(step+1)\/len(train_loader)),\n                       grad_norm=grad_norm,\n                       #lr=scheduler.get_lr()[0],\n                       ))\n        elif CFG['device'] == 'TPU':\n            if (step % CFG['print_freq'] == 0) or (step == (len(train_loader)-1)):\n                xm.master_print('Epoch: [{0}][{1}\/{2}] '\n                                'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                                'Elapsed {remain:s} '\n                                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                                'Grad: {grad_norm:.4f}  '\n                                #'LR: {lr:.6f}  '\n                                .format(\n                                epoch+1, step, len(train_loader), batch_time=batch_time,\n                                data_time=data_time, loss=losses,\n                                remain=timeSince(start, float(step+1)\/len(train_loader)),\n                                grad_norm=grad_norm,\n                                #lr=scheduler.get_lr()[0],\n                                ))\n                \n        \n        print({f\"[fold{fold}] loss\": losses.val,\n                           f\"[fold{fold}] lr\": scheduler.get_lr()[0]})\n        \n    return losses.avg\n\n\ndef valid_fn(valid_loader, model, criterion, device):\n    batch_time = AverageMeter()\n    data_time = AverageMeter()\n    losses = AverageMeter()\n    scores = AverageMeter()\n    # switch to evaluation mode\n    model.eval()\n    trues = []\n    preds = []\n    start = end = time.time()\n    for step, (images, labels) in enumerate(valid_loader):\n        # measure data loading time\n        data_time.update(time.time() - end)\n        images = images.to(device)\n        labels = labels.to(device)\n        batch_size = labels.size(0)\n        # compute loss\n        with torch.no_grad():\n            y_preds = model(images)\n        loss = criterion(y_preds.view(-1), labels)\n        losses.update(loss.item(), batch_size)\n        # record accuracy\n        trues.append(labels.to('cpu').numpy())\n        preds.append(y_preds.to('cpu').numpy())\n        if CFG['gradient_accumulation_steps'] > 1:\n            loss = loss \/ CFG['gradient_accumulation_steps']\n        # measure elapsed time\n        batch_time.update(time.time() - end)\n        end = time.time()\n        if CFG['device'] == 'GPU':\n            if step % CFG['print_freq'] == 0 or step == (len(valid_loader)-1):\n                print('EVAL: [{0}\/{1}] '\n                      'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                      'Elapsed {remain:s} '\n                      'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                      .format(\n                       step, len(valid_loader), batch_time=batch_time,\n                       data_time=data_time, loss=losses,\n                       remain=timeSince(start, float(step+1)\/len(valid_loader)),\n                       ))\n        elif CFG['device'] == 'TPU':\n            if step % CFG['print_freq'] == 0 or step == (len(valid_loader)-1):\n                xm.master_print('EVAL: [{0}\/{1}] '\n                                'Data {data_time.val:.3f} ({data_time.avg:.3f}) '\n                                'Elapsed {remain:s} '\n                                'Loss: {loss.val:.4f}({loss.avg:.4f}) '\n                                .format(\n                                step, len(valid_loader), batch_time=batch_time,\n                                data_time=data_time, loss=losses,\n                                remain=timeSince(start, float(step+1)\/len(valid_loader)),\n                                ))\n\n    predictions = np.concatenate(preds)\n    return losses.avg, predictions","906572fa":"# ====================================================\n# Train loop\n# ====================================================\ndef train_loop(folds, fold):\n\n    print(f\"========== fold: {fold} training ==========\")\n\n    # ====================================================\n    # loader\n    # ====================================================\n    trn_idx = folds[folds['fold'] != fold].index\n    val_idx = folds[folds['fold'] == fold].index\n\n    train_folds = folds.loc[trn_idx].reset_index(drop=True)\n    valid_folds = folds.loc[val_idx].reset_index(drop=True)\n\n    train_dataset = PawpularityDataset(train_folds, \n                                 augs=get_train_augs())\n    valid_dataset = PawpularityDataset(valid_folds, \n                                 augs=get_valid_augs())\n\n    \n    train_loader = DataLoader(train_dataset, \n                              batch_size=CFG['batch_size'], \n                              shuffle=True, \n                              num_workers=CFG['num_workers'], pin_memory=True, drop_last=True)\n    valid_loader = DataLoader(valid_dataset, \n                              batch_size=CFG['batch_size'], \n                              shuffle=False, \n                              num_workers=CFG['num_workers'], pin_memory=True, drop_last=False)\n    \n    valid_labels = valid_folds[CFG['target_col']].values\n    \n    # ====================================================\n    # scheduler \n    # ====================================================\n    def get_scheduler(optimizer):\n        if CFG['scheduler']=='ReduceLROnPlateau':\n            scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=CFG['factor'], patience=CFG['patience'], verbose=True, eps=CFG['eps'])\n        elif CFG['scheduler']=='CosineAnnealingLR':\n            scheduler = CosineAnnealingLR(optimizer, T_max=CFG['T_max'], eta_min=CFG['min_lr'], last_epoch=-1)\n        elif CFG['scheduler']=='CosineAnnealingWarmRestarts':\n            scheduler = CosineAnnealingWarmRestarts(optimizer, T_0=CFG['T_0'], T_mult=1, eta_min=CFG['min_lr'], last_epoch=-1)\n        elif CFG['scheduler']=='GradualWarmupSchedulerV2':\n            scheduler_cosine=torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, CFG['cosine_epo'])\n            scheduler_warmup=GradualWarmupSchedulerV2(optimizer, multiplier=10, total_epoch=CFG['warmup_epo'], after_scheduler=scheduler_cosine)\n            scheduler=scheduler_warmup\n        return scheduler\n\n    # ====================================================\n    # model & optimizer\n    # ====================================================\n    if CFG['device'] == 'TPU':\n        device = xm.xla_device()\n        xm.set_rng_state(CFG['seed'], device)\n    elif CFG['device'] == 'GPU':\n        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    \n    def get_model(pretrained=False):\n        if CFG['model_name'] =='vit_base_patch32_384':\n            model = PawpularityModelViT(model_name=CFG['model_name'], pretrained=pretrained)\n        elif CFG['model_name'] =='tf_efficientnet_b0_ns':\n            model = PawpularityModelCNN(CFG['model_name'], pretrained=pretrained)\n        return model\n    \n\n    model = get_model(pretrained=True)    \n    model.to(device)\n\n    optimizer = Adam(model.parameters(), lr=CFG['lr'], weight_decay=CFG['weight_decay'], amsgrad=False)\n    scheduler = get_scheduler(optimizer) \n\n\n    # ====================================================\n    # loop \n    # ====================================================\n    criterion = RMSELoss()\n    print(f'Criterion: {criterion}')\n\n    best_score = np.inf\n    best_loss = np.inf\n    \n    for epoch in range(CFG['epochs']):\n        \n        start_time = time.time()\n        \n        # train\n        if CFG['device'] == 'TPU':\n            if CFG['nprocs'] == 1:\n                avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n            elif CFG['nprocs'] == 8:\n                para_train_loader = pl.ParallelLoader(train_loader, [device])\n                avg_loss = train_fn(fold, para_train_loader.per_device_loader(device), model, criterion, optimizer, epoch, scheduler, device)\n        elif CFG['device'] == 'GPU':\n            avg_loss = train_fn(fold, train_loader, model, criterion, optimizer, epoch, scheduler, device)\n                \n        # eval\n        if CFG['device'] == 'TPU':\n            if CFG['nprocs'] == 1:\n                avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n            elif CFG['nprocs'] == 8:\n                para_valid_loader = pl.ParallelLoader(valid_loader, [device])\n                avg_val_loss, preds = valid_fn(para_valid_loader.per_device_loader(device), model, criterion, device)\n                preds = idist.all_gather(torch.tensor(preds)).to('cpu').numpy()\n                valid_labels = idist.all_gather(torch.tensor(valid_labels)).to('cpu').numpy()\n        elif CFG['device'] == 'GPU':\n            avg_val_loss, preds = valid_fn(valid_loader, model, criterion, device)\n        \n        if isinstance(scheduler, ReduceLROnPlateau):\n            scheduler.step(avg_val_loss)\n        elif isinstance(scheduler, CosineAnnealingLR):\n            scheduler.step()\n        elif isinstance(scheduler, CosineAnnealingWarmRestarts):\n            scheduler.step()\n        elif isinstance(scheduler, GradualWarmupSchedulerV2):\n            scheduler.step(epoch)\n\n        score = get_score(valid_labels, preds)\n\n        elapsed = time.time() - start_time\n\n        if CFG['device'] == 'GPU':\n            print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n            print(f'Epoch {epoch+1} - Score: {score:.4f}')\n        elif CFG['device'] == 'TPU':\n            if CFG['nprocs'] == 1:\n                print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n                print(f'Epoch {epoch+1} - Score: {score:.4f}')\n            elif CFG['nprocs'] == 8:\n                xm.master_print(f'Epoch {epoch+1} - avg_train_loss: {avg_loss:.4f}  avg_val_loss: {avg_val_loss:.4f}  time: {elapsed:.0f}s')\n                xm.master_print(f'Epoch {epoch+1} - Score: {score:.4f}')\n        \n        if score < best_score:\n            best_score = score\n            if CFG['device'] == 'GPU':\n                print(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n                torch.save({'model': model.state_dict(),'preds': preds},\n                             '.\/{}_fold{}_best_score.pth'.format(CFG['model_name'], fold))\n            elif CFG['device']== 'TPU':\n                if CFG['nprocs'] == 1:\n                    print(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n                elif CFG['nprocs'] == 8:\n                    xm.master_print(f'Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model')\n                torch.save({'model': model.state_dict(),'preds': preds}, \n                           '.\/{}_fold{}_best_score.pth'.format(CFG['model_name'], fold))\n    \n    valid_folds['preds'] = torch.load('.\/{}_fold{}_best_score.pth'.format(CFG['model_name'],fold), \n                                      map_location=torch.device('cpu'))['preds']\n\n    return valid_folds","e7e7186e":"model = PawpularityModelViT(pretrained = True)","424bd3a4":"def count_parameters(model):\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n\nprint(f\"The model has {count_parameters(model):,} trainable parameters\")","e15b6ad9":"# ====================================================\n# main\n# ====================================================\ndef main():\n\n    \"\"\"\n    Prepare: 1.train \n    \"\"\"\n\n    def get_result(result_df):\n        preds = result_df['preds'].values\n        labels = result_df[CFG['target_col']].values\n        score = get_score(labels, preds)\n        print(f'Score: {score:<.4f}')\n    \n    if CFG['train']:\n        # train \n        oof_df = pd.DataFrame()\n        for fold in range(CFG['n_fold']):\n            if fold in CFG['trn_fold']:\n                _oof_df = train_loop(train_df, fold)\n                oof_df = pd.concat([oof_df, _oof_df])\n                print(f\"========== fold: {fold} result ==========\")\n                get_result(_oof_df)\n                \n                \n        if CFG['nprocs'] != 8:\n            # CV result\n            print(f\"========== CV ==========\")\n            get_result(oof_df)\n            # save result\n            oof_df.to_csv('.\/oof_df.csv', index=False)","eecf3e77":"main()","30d9f0a8":"def get_features(test_loader, model, device):\n    model.eval()\n    features = []\n    tk0 = tqdm(enumerate(test_loader), total = len(test_loader))\n    for step, (images) in tk0:\n        images = images.to(device)\n        batch_size = images.size(0)\n        with torch.no_grad():\n            feature = model.feature(images)\n        features.append(feature.to('cpu').numpy())\n    features = np.concatenate(features)\n    return features","c73c3e82":"class PawpularityDataset(Dataset):\n    def __init__(self, df, augs=None):\n        self.df = df\n        self.augs = augs\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, index):\n        image = self._load_image(self.df[\"Id\"].iloc[index])\n\n        # Apply image augmentations if available\n        if self.augs:\n            image = self.augs(image=image)[\"image\"]\n\n        return image\n\n    def _load_image(self, image_id):\n        image = cv2.imread(f\"{TRAIN_PATH}\/{image_id}.jpg\", cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        return image","5352283e":"IMG_FEATURES = []\ntest_dataset = PawpularityDataset(train_df, augs=get_valid_augs())\ntest_loader = DataLoader(test_dataset, \n                         batch_size=CFG['batch_size'] * 2, \n                         shuffle=False, \n                         num_workers=CFG['num_workers'], pin_memory=True, drop_last=False)\nfor fold in range(CFG['n_fold']):\n    model = PawpularityModelViT(pretrained=False)\n    state = torch.load('.\/{}_fold{}_best_score.pth'.format(CFG['model_name'], fold), \n                       map_location=torch.device('cpu'))['model']\n    model.load_state_dict(state)\n    model.to(device)\n    features = get_features(test_loader, model, device)\n    IMG_FEATURES.append(features)\n    del state; gc.collect()\n    torch.cuda.empty_cache()","c3475828":"IMG_FEATURES[0].shape","f2c69686":"def run_single_lightgbm(param, train, features, target, fold=0, categorical=[]):\n    \n    train[[f\"img_{i}\" for i in np.arange(768)]] = IMG_FEATURES[fold]\n    \n    trn_idx = train[train.fold != fold].index\n    val_idx = train[train.fold == fold].index\n    print(f'train size : {len(trn_idx)}  valid size : {len(val_idx)}')\n    \n    if categorical == []:\n        trn_data = lgb.Dataset(train.iloc[trn_idx][features].values, label=target.iloc[trn_idx].values)\n        val_data = lgb.Dataset(train.iloc[val_idx][features].values, label=target.iloc[val_idx].values)\n    else:\n        trn_data = lgb.Dataset(train.iloc[trn_idx][features], label=target.iloc[trn_idx].values, categorical_feature=categorical)\n        val_data = lgb.Dataset(train.iloc[val_idx][features], label=target.iloc[val_idx].values, categorical_feature=categorical)\n        \n    num_round = 10000\n    clf = lgb.train(param, \n                    trn_data,\n                    num_round,\n                    valid_sets=[trn_data, val_data],\n                    verbose_eval=10,\n                    early_stopping_rounds=10)\n    print(f'Dumping model with pickle... lightgbm_fold{fold}.pkl')\n    with open(f'.\/lightgbm_fold{fold}.pkl', 'wb') as fout:\n        pickle.dump(clf, fout)\n    \n    oof = np.zeros(len(train))\n    oof[val_idx] = clf.predict(train.iloc[val_idx][features], num_iteration=clf.best_iteration)\n    score = get_score(target.iloc[val_idx].values, oof[val_idx])\n    print(f\"fold{fold} score: {score:<.5f}\")\n    \n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"Feature\"] = features\n    fold_importance_df[\"importance\"] = clf.feature_importance(importance_type='gain')\n    fold_importance_df[\"fold\"] = fold\n\n    return oof, fold_importance_df, val_idx\n\n\ndef run_kfold_lightgbm(param, train, features, target, n_fold=5, categorical=[]):\n    \n    oof = np.zeros(len(train))\n    feature_importance_df = pd.DataFrame()\n    val_idxes = []\n    \n    for fold in range(n_fold):\n        print(f\"===== Fold {fold} =====\")\n        _oof, fold_importance_df, val_idx = run_single_lightgbm(param, \n                                                                train, features, target, \n                                                                fold=fold, categorical=categorical)\n        oof += _oof\n        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n        val_idxes.append(val_idx)\n    \n    val_idxes = np.concatenate(val_idxes)\n    score = get_score(target.iloc[val_idxes].values, oof[val_idxes])\n    print(f\"CV score: {score:<.5f}\")\n    \n    return oof, feature_importance_df, val_idxes\n\n\ndef show_feature_importance(feature_importance_df):\n    cols = (feature_importance_df[[\"Feature\", \"importance\"]]\n                .groupby(\"Feature\").mean().sort_values(by=\"importance\", ascending=False)[:50].index)\n    best_features = feature_importance_df.loc[feature_importance_df.Feature.isin(cols)]\n    plt.figure(figsize=(8, 16))\n    sns.barplot(x=\"importance\", y=\"Feature\", data=best_features.sort_values(by=\"importance\",ascending=False))\n    plt.title('Features importance (averaged\/folds)')\n    plt.tight_layout()\n    plt.savefig(f'.\/feature_importance_df_lightgbm.png')","1ba1a342":"target = train_df[CFG['target_col']]\nfeatures = ['Subject Focus', 'Eyes', 'Face', 'Near', 'Action', 'Accessory',\n            'Group', 'Collage', 'Human', 'Occlusion', 'Info', 'Blur'] + [f\"img_{i}\" for i in np.arange(768)]\n\nlgb_param = {\n    'objective': 'regression',\n    'metric': 'rmse',\n    'boosting_type': 'gbdt',\n    'learning_rate': 0.01,\n    'seed': 42,\n    'max_depth': -1,\n    'min_data_in_leaf': 10,\n    'verbosity': -1,\n}\n\noof, feature_importance_df, _ = run_kfold_lightgbm(lgb_param, \n                                                   train_df, features, target, \n                                                   n_fold=CFG['n_fold'], categorical=[])\n\nshow_feature_importance(feature_importance_df)\nfeature_importance_df.to_csv(f'.\/feature_importance_df.csv', index=False)","b067c4d0":"train_df['pred'] = oof\nscore = get_score(train_df['Pawpularity'].values, train_df['pred'].values)\nprint(f\"CV: {score:<.5f}\")\ntrain_df[['Id', 'Pawpularity', 'pred']].to_pickle('.\/oof.pkl')\n","f5648f4e":"# Utils","1e5535de":"# Loss","d1130a13":"# CV Split\n","c82a588a":"# CFG","b741df4f":"# Transforms","b9e28a2b":"# Helper Function","c1b7349f":"# Model","561fe920":"# Main\n","cd0bc9c1":"# Dataset"}}