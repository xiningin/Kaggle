{"cell_type":{"d199c9d5":"code","42d372af":"code","e87036f9":"code","360fd8d9":"code","6b330ef9":"code","1b60a6ea":"code","0001e952":"code","69214eba":"code","69f1fd25":"code","d5ef318c":"code","e1249d4d":"code","49abb908":"code","cac43ca1":"code","b5fd68fe":"markdown","5a53394c":"markdown","19870ca4":"markdown","0828550b":"markdown","c4cebc7a":"markdown","3dd70efa":"markdown","30ba339f":"markdown","cdf122b4":"markdown","f74055c0":"markdown","30b192f0":"markdown","1d525b73":"markdown","ac58478d":"markdown","680a36f6":"markdown","3c900366":"markdown","eeb256e0":"markdown","0f4c8607":"markdown"},"source":{"d199c9d5":"from keras.layers import Input, Dense, Reshape, Flatten, Dropout\nfrom keras.layers import BatchNormalization, Activation, ZeroPadding2D\nfrom keras.layers.advanced_activations import LeakyReLU\nfrom keras.layers.convolutional import UpSampling2D, Conv2D\nfrom keras.models import Sequential, Model\nfrom keras.optimizers import Adam,SGD\nimport keras\nimport matplotlib.pyplot as plt\nimport sys\nimport numpy as np\nimport os\nprint(os.listdir(\"..\/input\"))\n\n","42d372af":"from os import listdir, makedirs\nfrom os.path import join, exists, expanduser\n\ncache_dir = expanduser(join('~', '.keras'))\nif not exists(cache_dir):\n    makedirs(cache_dir)\ndatasets_dir = join(cache_dir, 'datasets') # \/cifar-10-batches-py\nif not exists(datasets_dir):\n    makedirs(datasets_dir)\n\n\n!cp ..\/input\/cifar-10-python.tar.gz ~\/.keras\/datasets\/\n!ln -s  ~\/.keras\/datasets\/cifar-10-python.tar.gz ~\/.keras\/datasets\/cifar-10-batches-py.tar.gz\n!tar xzvf ~\/.keras\/datasets\/cifar-10-python.tar.gz -C ~\/.keras\/datasets\/","e87036f9":"# Load CIFAR10 data\n(X_train, y_train), (_, _) = keras.datasets.cifar10.load_data()\n\n# Select a single class images (birds)\nX_train = X_train[y_train.flatten() == 2]","360fd8d9":"# Input shape\nimg_rows = 32\nimg_cols = 32\nchannels = 3\n        \nimg_shape = (img_rows, img_cols, channels)        \nlatent_dim = 100","6b330ef9":"def build_generator():\n\n        model = Sequential()\n\n        model.add(Dense(128 * 8 * 8, activation=\"relu\", input_dim=latent_dim))\n        model.add(Reshape((8, 8, 128)))\n        \n        model.add(UpSampling2D())#upsamples to 16*16*128\n        \n        model.add(Conv2D(128, kernel_size=3, padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Activation(\"relu\"))\n        \n        model.add(UpSampling2D()) #upsamples to 32*32*128\n        \n        model.add(Conv2D(64, kernel_size=3, padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(Activation(\"relu\"))\n        \n        model.add(Conv2D(channels, kernel_size=3, padding=\"same\"))\n        model.add(Activation(\"tanh\"))\n\n        #outputs an image of 32*32*3\n\n        noise = Input(shape=(latent_dim,))\n        img = model(noise)\n\n        return Model(noise, img)","1b60a6ea":"def build_discriminator():\n\n        model = Sequential()\n\n        model.add(Conv2D(32, kernel_size=3, strides=2, input_shape=img_shape, padding=\"same\"))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        #no normalization for the first layer \n        \n        model.add(Conv2D(64, kernel_size=3, strides=2, padding=\"same\"))\n        model.add(ZeroPadding2D(padding=((0,1),(0,1))))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        \n        model.add(Conv2D(128, kernel_size=3, strides=2, padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        \n        model.add(Conv2D(256, kernel_size=3, strides=1, padding=\"same\"))\n        model.add(BatchNormalization(momentum=0.8))\n        model.add(LeakyReLU(alpha=0.2))\n        model.add(Dropout(0.25))\n        \n        model.add(Flatten())\n        model.add(Dense(1, activation='sigmoid'))\n\n        \n\n        img = Input(shape=img_shape)\n        validity = model(img)\n\n        return Model(img, validity)","0001e952":"# Build and compile the discriminator\ndiscriminator = build_discriminator()\ndiscriminator.compile(loss='binary_crossentropy',\n            optimizer=Adam(0.0002,0.5),\n            metrics=['accuracy'])\n\n# Build the generator\ngenerator = build_generator()\n\n# The generator takes noise as input and generates imgs\nz = Input(shape=(latent_dim,))\nimg = generator(z)\n\n# For the combined model we will only train the generator\ndiscriminator.trainable = False\n\n# The discriminator takes generated images as input and determines validity\nvalid = discriminator(img)\n\n# The combined model  (stacked generator and discriminator)\n# Trains the generator to fool the discriminator\ncombined = Model(z, valid)\ncombined.compile(loss='binary_crossentropy', optimizer=Adam(0.0002,0.5))","69214eba":"def show_imgs(epoch):\n        r, c = 4,4\n        noise = np.random.normal(0, 1, (r * c,latent_dim))\n        gen_imgs = generator.predict(noise)\n\n        # Rescale images 0 - 1\n        gen_imgs = 0.5 * gen_imgs + 0.5\n\n        fig, axs = plt.subplots(r, c)\n        cnt = 0\n        for i in range(r):\n            for j in range(c):\n                axs[i,j].imshow(gen_imgs[cnt, :,:,])\n                axs[i,j].axis('off')\n                cnt += 1\n        plt.show()\n        plt.close()","69f1fd25":"def show_losses(losses):\n    losses = np.array(losses)\n    \n    fig, ax = plt.subplots()\n    plt.plot(losses.T[0], label='Discriminator')\n    plt.plot(losses.T[1], label='Generator')\n    plt.title(\"Training Losses\")\n    plt.legend()\n    plt.show()","d5ef318c":"epochs=30000\nbatch_size=32\ndisplay_interval=5000\nlosses=[]\n\n#normalizing the input\nX_train = X_train \/ 127.5 - 1.\n        \n\n        # Adversarial ground truths\nvalid = np.ones((batch_size, 1))\n        #let's add some noise \nvalid += 0.05 * np.random.random(valid.shape)\nfake = np.zeros((batch_size, 1))\nfake += 0.05 * np.random.random(fake.shape)\n\nfor epoch in range(epochs):\n\n            \n            #  Train Discriminator\n            \n\n            # Select a random half of images\n            idx = np.random.randint(0, X_train.shape[0], batch_size)\n            imgs = X_train[idx]\n\n            # Sample noise and generate a batch of new images\n            noise = np.random.normal(0, 1, (batch_size, latent_dim))\n            gen_imgs = generator.predict(noise)\n            \n\n            # Train the discriminator (real classified as ones and generated as zeros)\n            d_loss_real = discriminator.train_on_batch(imgs, valid)\n            d_loss_fake = discriminator.train_on_batch(gen_imgs, fake)\n            d_loss = 0.5 * np.add(d_loss_real, d_loss_fake)\n\n            \n            #  Train Generator\n            \n\n            # Train the generator (wants discriminator to mistake images as real)\n            g_loss = combined.train_on_batch(noise, valid)\n            \n            # Plot the progress\n            if epoch % 5000==0:\n                print (\"%d [D loss: %f] [G loss: %f]\" % (epoch, d_loss[0], g_loss))\n            if epoch % 1000==0:\n                losses.append((d_loss[0],g_loss))\n                \n            if epoch % display_interval == 0:\n                 show_imgs(epoch)","e1249d4d":"show_losses(losses)","49abb908":"s=X_train[:40]\ns = 0.5 * s + 0.5\nf, ax = plt.subplots(5,8, figsize=(16,10))\nfor i, img in enumerate(s):\n        ax[i\/\/8, i%8].imshow(img)\n        ax[i\/\/8, i%8].axis('off')\n        \nplt.show()","cac43ca1":"noise = np.random.normal(size=(40, latent_dim))\ngenerated_images = generator.predict(noise)\ngenerated_images = 0.5 * generated_images + 0.5\nf, ax = plt.subplots(5,8, figsize=(16,10))\nfor i, img in enumerate(generated_images):\n        ax[i\/\/8, i%8].imshow(img)\n        ax[i\/\/8, i%8].axis('off')\n        \nplt.show()","b5fd68fe":"With that said what follows is a loop in which -\n* The generator tries to maximize the probability of fooling the Discriminator by making the images(for example) more close to real in each step thereby making the Discriminator classify them as real.\n* And the discriminator guides the generator to produce more realistic images , by classifying it's images as fake.\n\n![](https:\/\/skymind.ai\/images\/wiki\/GANs.png)\n\n\n\n\n\n\n\nLet's fit this into an analogy \nYou can think of a GAN as a game of cat and mouse between a counterfeiter (Generator) and a cop (Discriminator). The counterfeiter is learning to create fake money, and the cop is learning to detect the fake money. Both of them are learning and improving. The counterfeiter is constantly learning to create better fakes, and the cop is constantly getting better at detecting them. Competition in this game drives both teams to improve their methods until the counterfeits are indistiguishable from the genuine or real money. ","5a53394c":"## **The Discriminator**\n\nThe discriminator is also a  CNN  with leaky ReLU activations. Many activation functions will work fine with this basic GAN architecture. However, leaky ReLUs are very popular because they help the gradients flow easier through the architecture.\n\nA regular ReLU function works by truncating negative values to 0. This has the effect of blocking the gradients to flow through the network. Instead of the function being zero, leaky ReLUs allow a small negative value to pass through. That is, the function computes the greatest value between the features and a small factor.\nLeaky ReLUs represent an attempt to solve the dying ReLU problem. This situation occurs when the neurons get stuck in a state in which ReLU units always output 0s for all inputs. For these cases, the gradients are completely shut to flow back through the network.\n\n> *This is especially important for GANs since the only way the generator has to learn is by receiving the gradients from the discriminator.*\n\nFinally, the discriminator needs to output probabilities. We use a Sigmoid Activation for that.","19870ca4":"Let's have a look at the original images. It will help us to compare how well our generator did. (I have only selected the bird images for training)","0828550b":"GANs are a class of Unsupervised Learning Algorithms that do much more than just recognizing image \/ voice , predicting or translating. They implement  deep neural networks or CNN and are comprised of two parts, pitting one against the other (thus the \u201cadversarial\u201d). These two parts are called the Generator and the Discriminator.","c4cebc7a":"Some images produced by the generator after training for 30000 epochs","3dd70efa":"In this kernel  I'm implementing Deep Convolutional GAN , based on this paper on  [DCGAN](https:\/\/arxiv.org\/pdf\/1511.06434.pdf) which is in contrast to but builds on [Ian GoodFellow's paper](http:\/\/papers.nips.cc\/paper\/5423-generative-adversarial-nets.pdf). GoodFellow's paper is the first paper on GAN and implements a dense network both in the generator and the discriminator rather than a CNN. I am using the images (without class labels) from the **CIFAR_10** Dataset. These images along with the fake ones will be fed in batches to the Discriminator.Let's take a look at the steps our GAN will follow-\n\n1. The Generator takes in random numbers and returns an image.\n2. This generated image is fed into the Discriminator alongside a stream of images taken from the actual dataset.\n3. The Discriminator takes in both real and fake images and returns probabilities, a number between 0 and 1, with 1 representing a prediction of authenticity and 0 representing fake.\n\nThere are 2 feedback loops :- \n\n1. The Discriminator is in a feedback loop with the ground truth of the images (are they real or fake), which we know.\n2. The Generator is in a feedback loop with the Discriminator (did the Discriminator label it real or fake, regardless of the truth).\n\nWithout any further delay let's import the libraries , load the dataset and get going.","30ba339f":"A lot of changes have been made in GAN's Architecture since Goodfeloow's original paper , but some things remain the same :-\n* Normalizing the input\n* The activation function in all except the last layer of the generator must be a relu.\n* The activation in the last layer of the generator which is a Dense Layer is tanh activation.\n* Same goes for the discriminator, all the layers except the last have relu as activtaion and the last Dense layer uses Sigmoid Activation.\n* We use binary_cross_entropy method to calculate loss in both the adversaries.(Though in some papers like [Wasserstein gan](https:\/\/arxiv.org\/pdf\/1701.07875.pdf)  different loss functions is used)","cdf122b4":"These images are far from perfect and can be improvised by more training or some hacks , but I think they are pretty amazing , given the fact that they are generated from nothing(random noise actually). \n\n![](https:\/\/skymind.ai\/images\/wiki\/GANdancers.png)\n\n","f74055c0":"For those of you, who have already heard about GANs and are wondering *\"What's the hype about?* should definitely go through this kernel to see the immense potential these new species of networks have. \n\nAnd for those who have probably heard the name for the first time, you would be all the more amazed when you will learn about these networks.","30b192f0":"Time to train the model! ","1d525b73":"> **Generator** - The generator takes the role of a forger and tries to create music\/image\/speech from random noise. It learns to map from a latent space to a particular data distribution of interest. It generally implements a Deconvolutional Network to do so.\n\n> **Discriminator**- The Discriminator on the other hand takes the role of the evaluater and tries to distinguish the fake data (created by the Generator) from the real one. It is usually implemented as a Convolutional Network. ","ac58478d":"Now some hacks\/tips that have been introduced in papers in the last few years to make GANs better are:-\n* Using BatchNormalization in all layers except the input layer in the generator and the output layer in the discriminator.\n* Using Adam Optimizer for the generator and SGD for the discriminator.\n* Adding some random noise to the labels before feeding them to the discriminator.\n* Sampling from a Gaussian Distribution instead of a Uniform distribution.\n* Construct different mini-batches for real and fake, i.e. each mini-batch needs to contain only all real images or all generated images.\n* Pre-training the discriminator.\n* Adding some noise to the images before feeding them to the discriminator.\n\nIt's not necessary that all of the above tricks will work for your model. You will have to find the ones that do.\n\n","680a36f6":"For those of you , who might be wondering \"why haven't I trained the network on all the class of images ?\". The answer is this network won't do well on multi-class data(you can check it yourself) because of a problem called **mode collapse**.\n\nSo, STAY TUNED!\n\nIt would make me very happy if you upvote this kernel and I'll be glad to hear any suggestions or feedback (leave them in the comments below).\n\n**Reference**:-\n\nhttps:\/\/skymind.ai\/wiki\/generative-adversarial-network-gan","3c900366":"# **First things first (what are they?)**","eeb256e0":"GANs\u2019 have incredible potential, because they can learn to imitate any distribution of data. That is, GANs can learn to create worlds spookily similar to our own in any domain: images, music, speech.\n\n\nGANs have a variety of applications ranging from reconstructing 3D model of objects from images to creating the 2018 painting *Edmond de Belamy* which sold for $432,500 (Woah).\nSome of them are-\n* Image denoising\n* Inpainting\n* Super Resolution\n* Structured Prediction\n* Exploration in Reinforcement Learning \n* Image to Image Translation ","0f4c8607":"## **The Generator** \n\n\nTo learn a generator distribution p<sub>g<\/sub> over data x,the generator builds a mapping function from a prior noise distribution p<sub>z<\/sub>(z) to data space as G(z). The discriminator outputs, a single scalar representing the probability that x came from training data rather than p<sub>g<\/sub>.\n\nG and D are both trained simultaneously: we adjust parameters for G to minimize log(1 - D(G(z)) and adjust parameters for D to minimize logD(x), as if they are following the two-player min-max game with value function V (G;D):\n\n![](https:\/\/github.com\/Ibtastic\/Generative-Adversarial-Networks\/raw\/master\/GAN\/loss.png)\n\n* In the generator we use a method called  [Upsampling](https:\/\/towardsdatascience.com\/up-sampling-with-transposed-convolution-9ae4f2df52d0) to produce images. I have used Upsampling2D but TransposeConv2d + stride or PixelShuffle could be used alternatively. \n\n"}}