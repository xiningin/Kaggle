{"cell_type":{"a148e3c8":"code","7ca00370":"code","0f7347b9":"code","9a8cf4ec":"code","02d46324":"code","1ac93b39":"code","eff21b3a":"code","7880ebfa":"code","0cd88b15":"code","fce012e8":"code","3c2d955c":"code","345ab711":"code","cb7dc42d":"code","74edf7cb":"code","469045e6":"code","6f4cac7d":"code","71d1ae4f":"code","adb5f81b":"code","3942c43d":"code","4b631a1f":"code","985f1b41":"code","864cfb9c":"code","835505d8":"code","b38744c7":"code","e1455871":"code","95f06d92":"code","459e08bd":"code","85453a57":"code","b4de2a27":"code","c74b6378":"code","ff2c4f93":"code","c6a8c98d":"code","78edee95":"code","62f6b4b9":"code","fd704a35":"code","82d0b97f":"code","07c373b4":"code","0c39d851":"code","eb4321f2":"code","8f7a2637":"code","c00bf14d":"code","3fd791f4":"code","dc931aea":"code","0cc93730":"code","4c738539":"code","692b2e35":"code","6447fab0":"code","56beadc7":"code","15ea560c":"code","239470a1":"code","9c2239f9":"code","971a7856":"markdown","ec876df1":"markdown","4613a990":"markdown","776da627":"markdown","1f18fb2a":"markdown","01c4dca5":"markdown","cc709b71":"markdown","a373ebf0":"markdown","cbc79f3b":"markdown","a9ddf97a":"markdown","36256d5f":"markdown","5035a4a1":"markdown","85299d71":"markdown","2f716575":"markdown","4f0cd899":"markdown","467950cb":"markdown","8c06f377":"markdown","2339f53c":"markdown","b920faa8":"markdown","3800f1f0":"markdown","2b0b4020":"markdown","6864cfe3":"markdown","55b6c468":"markdown","017c3b49":"markdown","606622ff":"markdown","1454ecac":"markdown","95a3b7f7":"markdown","17e4e13f":"markdown","0de036da":"markdown","3a51f0e9":"markdown","b064610d":"markdown","12d65cee":"markdown","84606192":"markdown","afd04f39":"markdown","6e34e703":"markdown"},"source":{"a148e3c8":"import pandas as pd \nimport numpy as np\nfrom catboost import CatBoostClassifier\nfrom sklearn.model_selection import GridSearchCV","7ca00370":"def concat_df(train, test):\n    df = pd.concat([train,test],ignore_index=True )\n    return  df ","0f7347b9":"def divide_df(all_data,shape):\n    train,test = all_data.loc[:shape-1], all_data.loc[shape:].drop(['claim'], axis=1) \n    return train,test ","9a8cf4ec":"def load(path) : \n    df = pd.read_csv(path)\n    return df ","02d46324":"train = load('..\/input\/tabular-playground-series-sep-2021\/train.csv')\ntest = load('..\/input\/tabular-playground-series-sep-2021\/test.csv')","1ac93b39":"def reduce_mem_usage(df):\n   \n    start_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage of dataframe is {:.2f} MB'.format(start_mem))\n    \n    for col in df.columns:\n        col_type = df[col].dtype\n        \n        if col_type != object:\n            c_min = df[col].min()\n            c_max = df[col].max()\n            if str(col_type)[:3] == 'int':\n                if c_min > np.iinfo(np.int8).min and c_max < np.iinfo(np.int8).max:\n                    df[col] = df[col].astype(np.int8)\n                elif c_min > np.iinfo(np.int16).min and c_max < np.iinfo(np.int16).max:\n                    df[col] = df[col].astype(np.int16)\n                elif c_min > np.iinfo(np.int32).min and c_max < np.iinfo(np.int32).max:\n                    df[col] = df[col].astype(np.int32)\n                elif c_min > np.iinfo(np.int64).min and c_max < np.iinfo(np.int64).max:\n                    df[col] = df[col].astype(np.int64)  \n            else:\n                if c_min > np.finfo(np.float16).min and c_max < np.finfo(np.float16).max:\n                    df[col] = df[col].astype(np.float16)\n                elif c_min > np.finfo(np.float32).min and c_max < np.finfo(np.float32).max:\n                    df[col] = df[col].astype(np.float32)\n                else:\n                    df[col] = df[col].astype(np.float64)\n        else:\n            df[col] = df[col].astype('category')\n\n    end_mem = df.memory_usage().sum() \/ 1024**2\n    print('Memory usage after optimization is: {:.2f} MB'.format(end_mem))\n    print('Decreased by {:.1f}%'.format(100 * (start_mem - end_mem) \/ start_mem))\n    \n    return df","eff21b3a":"train = reduce_mem_usage(train)\ntest = reduce_mem_usage(test)","7880ebfa":"df_all = concat_df(train, test)","0cd88b15":"import matplotlib.pyplot as plt\nimport seaborn as sns\ndef missing_values(df): \n  total = df.isnull().sum().sort_values(ascending=False)\n  percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n  missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n  f, ax = plt.subplots(figsize=(15, 6))\n  plt.xticks(rotation='90')\n  sns.barplot(x=missing_data.index, y=missing_data['Percent'])\n  plt.xlabel('Features', fontsize=15)\n  plt.ylabel('Percent of Missing Values', fontsize=15)\n  plt.title('Percentage of Missing Data by Feature', fontsize=15)\n  # missing_data.head()\n  return (missing_data)","fce012e8":"missing_data = missing_values(train)","3c2d955c":"# from sklearn.impute import SimpleImputer\n# import numpy as np\n# def Fillna(train,test):\n#     imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n#     train = imp_mean.fit_transform(train.drop('claim',axis=1))\n#     test = imp_mean.transform(test)\n#     return train,test","345ab711":"def Random_sampling(df, variable):\n    # extract the random sample to fill the na\n    random_sample = df[variable].dropna().sample(df[variable].isnull().sum(), random_state=0) \n    # pandas needs to have the same index in order to merge datasets\n    random_sample.index = df[df[variable].isnull()].index\n    df.loc[df[variable].isnull(), variable] = random_sample\n","cb7dc42d":"def missing_search (missing_data) : \n    missing_columns = [idx for idx , row in missing_data.iterrows() if row['Percent'] != 0 ]\n    return missing_columns","74edf7cb":"missing_columns = missing_search (missing_data)","469045e6":"def fillna(df_all,missing_columns):\n    for column in missing_columns: \n        Random_sampling(df_all,column) \n    return df_all","6f4cac7d":"df_all = fillna(df_all,missing_columns)","71d1ae4f":"_ = missing_values(df_all)","adb5f81b":"train, test = divide_df(df_all,train.shape[0])","3942c43d":"def correlation_matrix (df) : \n    corr = df.corr(method='pearson')\n    mask = np.zeros_like(corr, dtype=np.bool)\n    mask[np.triu_indices_from(mask)] = True\n    f, ax = plt.subplots(figsize=(12, 9))\n    sns.heatmap(corr, mask=mask, vmax=1, center=0, annot=True, fmt='.1f',\n                square=True, linewidths=.5, cbar_kws={\"shrink\": .5});\n    return corr\n    ","4b631a1f":"corr  = correlation_matrix (train)\n","985f1b41":"def to_drop_feat_corr (corr,threshold) :\n    # Select upper triangle of correlation matrix\n    upper = corr.where(np.triu(np.ones(corr.shape), k=1).astype(np.bool))\n    # Find index of feature columns with correlation greater than 0.95\n    to_drop = [column for column in upper.columns if any(upper[column] >threshold )]\n    return to_drop ","864cfb9c":"to_drop = to_drop_feat_corr (corr,0.95)","835505d8":"to_drop","b38744c7":"from sklearn.feature_selection import VarianceThreshold\ndef constant_features (df,threshold) :  \n    sel = VarianceThreshold(threshold=threshold)\n    sel.fit(df)  # fit finds the features with zero variance\n    to_drop = [x for x in df.columns if x not in df.columns[sel.get_support()]]\n    return to_drop ","e1455871":"to_drop = constant_features (train,0.01)","95f06d92":"to_drop","459e08bd":"# df_all.drop(to_drop,axis=1,inplace=True) ","85453a57":"import seaborn as sns \nimport matplotlib. pyplot as plt\nfrom sklearn.ensemble import RandomForestClassifier\ndef feature_importance (df) : \n    clf = RandomForestClassifier(n_estimators=1, random_state=0)\n    clf.fit(df.drop('claim',axis=1) ,df['claim']) # 'loss' hia label \n    #visualize feature importance\n    plt.figure(num=None, figsize=(10,8), dpi=80, facecolor='w', edgecolor='k')\n    feat_importances = pd.Series(clf.feature_importances_, index= df.drop('claim',axis=1).columns)\n    feat_importances.nlargest(7).plot(kind='barh')\n    return feat_importances\n","b4de2a27":"feat_importances = feature_importance (train)","c74b6378":"from sklearn.decomposition import PCA\ndef pca_variance_matrix(df) : \n    pca = PCA()\n    _ = pca.fit_transform(df.drop('claim',axis=1))\n    explained_variance=pca.explained_variance_ratio_\n    return explained_variance\n\nexplained_variance = pca_variance_matrix(df_all)","ff2c4f93":"def explained_variance_plot (explained_variance) : \n    with plt.style.context('dark_background'):\n        plt.figure(figsize=(6, 4))\n        plt.bar(range(explained_variance.shape[0]), explained_variance, alpha=0.5, align='center',label='individual explained variance')\n        plt.ylabel('Explained variance ratio')\n        plt.xlabel('Principal components')\n        plt.legend(loc='best')\n        plt.tight_layout()\n\nexplained_variance_plot (explained_variance) ","c6a8c98d":"def pca_fitting (nb , df) : \n    pca=PCA(n_components=nb) # ou nb  le nb de dimensions choisis ....\n    X_new=pca.fit_transform(df.drop('loss',axis=1))\n    return X_new ","78edee95":"# X_new = pca_fitting (df)","62f6b4b9":"train, test = divide_df(df_all,train.shape[0])","fd704a35":"model = CatBoostClassifier() , \nparameters =  {'depth'         : [6,8,10], \n                  'learning_rate' : [0.01, 0.05, 0.1],      \n                  'iterations'    : [1000,5000,10000]\n                 }, \ncv = 2, \nn_jobs=-1","82d0b97f":"import numpy as np\nimport pandas as pd\nfrom tqdm import tqdm\nfrom sklearn.model_selection import GridSearchCV\nclass GridSearch :\n  def __init__(self,train,test) :\n    self.data = train\n    self.X_train = train.drop('claim',axis=1)\n    self.y_train = train['claim']\n    self.X_test = test\n  def model (self,model) :\n    self.model = model\n  def parameters (self, parameters) :\n    self.parameters = parameters\n  def GridSearch (self) :\n    self.grid = GridSearchCV(estimator=self.model, param_grid = self.parameters, cv = 5, n_jobs=-1)\n  def Gridfit (self) :\n    self.grid.fit(np.asarray(self.X_train),np.asarray(self.y_train))\n  def finalfit (self) :\n    self.grid.best_estimator_.fit(np.asarray(self.X_train),np.asarray(self.y_train))\n  def predict(self):\n    self.pred = self.grid.best_estimator_.predict(np.asarray(self.X_test))\n    return (self.pred)\n#   def score(self,label):\n#     from sklearn.metrics import mean_absolute_error\n#     mean_absolute_error(label,self.pred)\n","07c373b4":"# gridsearch = GridSearch(train,test)\n# gridsearch.model(model)\n# gridsearch.parameters(parameters)\n# gridsearch.GridSearch()\n# gridsearch.Gridfit()\n# gridsearch.finalfit()\n# gridsearch.predict()","0c39d851":"def features_type (df) : \n  numerical = df.dtypes[df.dtypes != \"object\"].index\n  non_num = []\n  for x in  list(df.columns) : \n      if x not in numerical : \n          non_num.append(x)\n  return numerical , non_num","eb4321f2":"numerical , non_num = features_type (train)","8f7a2637":"non_num","c00bf14d":"\nclass CFG_Catboost :\n  SEED = 42\n  n_splits = 5\n  catboost_params = {'learning_rate':0.05,'iterations':10000,'eval_metric':'Accuracy',\n                      'use_best_model' :True,'verbose':100,'random_seed': 0,'loss_function':'MultiClass',\n                         'leaf_estimation_method':'Newton'}\n\n  remove_features = ['claim']\n  categ_features = non_num\n  TARGET_COL = 'claim'","3fd791f4":"def features_utils (train):\n  features_columns = [col for col in train.columns if col not in CFG_Catboost.remove_features]\n  return features_columns","dc931aea":"features_columns = features_utils (train)","0cc93730":"def divide_train (train):\n  skf = StratifiedKFold(n_splits=CFG_Catboost.n_splits,shuffle=True, random_state=CFG_Catboost.SEED)\n  X , y   = train[features_columns] , train[CFG_Catboost.TARGET_COL]\n  return X,y,skf","4c738539":"from sklearn.model_selection import StratifiedKFold\nX,y,skf = divide_train (train)","692b2e35":"def StratifiedKFold_Train(X,y):\n  # oof_cat = np.zeros((train.shape[0],))\n  test['target'] = 0\n  cat_preds= []\n  for fold_, (trn_idx, val_idx) in enumerate(skf.split(X,y)):\n      print(50*'-')\n      print('Fold:',fold_+1)\n      X_train, y_train = X.iloc[trn_idx,:], y[trn_idx] \n      X_test, y_test = X.iloc[val_idx,:], y[val_idx] \n        \n      estimator = CatBoostClassifier(**CFG_Catboost.catboost_params)\n      estimator.fit(Pool(X_train,y_train,cat_features = CFG_Catboost.categ_features),\n                    eval_set = Pool(X_test,y_test,cat_features = CFG_Catboost.categ_features),\n                    early_stopping_rounds=200)\n      \n      # y_pred_val = estimator.predict(X_test)\n      # oof_cat[val_idx] = y_pred_val\n      y_pred_test = estimator.predict(test[features_columns])\n      cat_preds.append(y_pred_test)\n      print(50*'-')\n      print()\n  return cat_preds\n\n","6447fab0":"from catboost import CatBoostClassifier ,Pool\n# cat_preds = StratifiedKFold_Train(X,y)","56beadc7":"from sklearn.base import BaseEstimator, TransformerMixin, clone, RegressorMixin\nfrom sklearn.model_selection import KFold\nimport numpy as np\n\n\nclass StackingModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5, task_type='classification', use_features_in_secondary=False):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n        self.task_type = task_type\n        self.use_features_in_secondary = use_features_in_secondary\n\n    def fit(self, X, y):\n        \"\"\"Fit all the models on the given dataset\"\"\"\n        self.base_models_ = [list() for _ in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=42)\n\n        # Train cloned base models and create out-of-fold predictions\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n\n        if self.use_features_in_secondary:\n            self.meta_model_.fit(np.hstack((X, out_of_fold_predictions)), y)\n        else:\n            self.meta_model_.fit(out_of_fold_predictions, y)\n\n        return self\n\n    def predict(self, X):\n        if self.task_type == 'classification':\n            meta_features = np.column_stack([[np.argmax(np.bincount(predictions)) for predictions in\n                                              np.column_stack([model.predict(X) for model in base_models])]\n                                             for base_models in self.base_models_])\n        else:\n            meta_features = np.column_stack([\n                np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n                for base_models in self.base_models_])\n        if self.use_features_in_secondary:\n            return self.meta_model_.predict(np.hstack((X, meta_features)))\n        else:\n            return self.meta_model_.predict(meta_features)\n\n    def predict_proba(self, X):\n        if self.task_type == 'classification':\n            meta_features = np.column_stack([[np.argmax(np.bincount(predictions)) for predictions in\n                                              np.column_stack([model.predict(X) for model in base_models])]\n                                             for base_models in self.base_models_])\n            if self.use_features_in_secondary:\n                return self.meta_model_.predict_proba(np.hstack((X, meta_features)))\n            else:\n                return self.meta_model_.predict_proba(meta_features)","15ea560c":"from sklearn.model_selection import KFold, cross_val_score\nn_folds = 5 # number of folds\ndef get_cv_scores(model, X, y, print_scores=True):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X) # create folds\n    rmse = np.sqrt(-cross_val_score(model, X, y, scoring=\"neg_mean_squared_error\", cv = kf)) # get rmse\n    if print_scores:\n        print(f'Root mean squared error: {rmse.mean():.3f} ({rmse.std():.3f})')\n    return [rmse]","239470a1":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nimport pathlib\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\nfrom sklearn.linear_model import Lasso\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\n\nimport xgboost as xgb\nimport lightgbm as lgb\n\nlasso_model = make_pipeline(RobustScaler(), Lasso(alpha=0.0005, random_state=1))\nrf = RandomForestRegressor()\ngbr = GradientBoostingRegressor()\nxgb_model = xgb.XGBRegressor()\nlgb_model = lgb.LGBMRegressor()\n# for model in [lasso_model, rf, gbr, xgb_model, lgb_model]:\n#     get_cv_scores(model, X, train.claim)","9c2239f9":"stacking_model1 = StackingModels([gbr, lgb_model, xgb_model], lasso_model)\n# get_cv_scores(stacking_model1, X, y_train);\n","971a7856":"5.2 Constant features","ec876df1":"6.1 Gridsearch CV","4613a990":"0. Import ","776da627":"4. fillna","1f18fb2a":"I will demonstrate this process using it as follows\n\n","01c4dca5":"Fasten your belt : The modeling part coded by SBH in an original way ...","cc709b71":"And then you can choose to do the train with the most important features that you choose","a373ebf0":"Here is our pure (clean) data, without any nans to consider ","cbc79f3b":"Stacking was introduced by Wolpert in the paper Stacked Generalization in 1992. It is a method that uses k-fold for training base models which then make predictions on the left out fold. These so-called out of fold predictions are then used to train another model\u200a\u2014\u200athe meta model\u200a\u2014\u200awhich can use the information produced by the base models to make final predictions.\n\nTo implement this functionality we need to first train each base model k times (k\u2026Number of folds) and then use their predictions to train our meta model.\n\nTo even get better results we can not only use the predictions of all the base models for  training the meta model but also the initial features. Because of the added model complexity which is caused when adding the input features we should make a boolean parameter to determine whether we want to use input features.","a9ddf97a":"5.4 PCA","36256d5f":"Here I present some functions that will be useful for this notebook \nconcat_df(train, test) : to concatenate the train and the test \ndivide_df(all_data,shape) : to split the train and the test already concatenated \nload(path) : to load the data","5035a4a1":"A constant (quasi constant) column has absolutely no added value since it takes the same value for any observation it would be better to delete it. sklearn offers here the VarianceThreshold tool which is used to find the columns that have a variance lower than a certain threshold and delete it ","85299d71":"since we can only run a model after checking that our data set does not contain any nan we have to do a quick visualization ","2f716575":"2. Reducing Memory Usage","4f0cd899":"Here we will iterate through all the columns of a dataframe and modify the data typeto reduce memory usage.        \n    ","467950cb":"3. Missing values","8c06f377":"Random forests are one the most popular machine learning algorithms. They are so successful because they provide in general a good predictive performance, low overfitting and easy interpretability. This interpretability is given by the fact that it is straightforward to derive the importance of each variable on the tree decision. In other words, it is easy to compute how much each variable is contributing to the decision.\n\n","2339f53c":"5. Drop ","b920faa8":"The correlation matrix here is used to study the correlation between different columns. Two correlated columns would be better to keep one and delete one since the second one has no meaning it adds no information and it increases the space for nothing ","3800f1f0":"So choose the best model\/combination you want in order to win the competition. \nI am waiting for your upvotes\/comments . Do not hesitate to contact me if you have any question ","2b0b4020":"It consists on an Exhaustive search over specified parameter values for an estimator.\n\nImportant members are fit, predict.\n\nGridSearchCV implements a \u201cfit\u201d and a \u201cscore\u201d method. It also implements \u201cscore_samples\u201d, \u201cpredict\u201d, \u201cpredict_proba\u201d, \u201cdecision_function\u201d, \u201ctransform\u201d and \u201cinverse_transform\u201d if they are implemented in the estimator used.\n\nThe parameters of the estimator used to apply these methods are optimized by cross-validated grid-search over a parameter grid.","6864cfe3":"Welcome to you for the best kernel of this competition. Coded by me S\u00e9mi Ben Hsan (SBH) .A Free lance Machine Learning Engineer (contact me for a work if want :p ) . This kernel includes all the advanced techniques of Regression\n, programmed in a professional way ( The SBH style ) and explained in detail . I am waiting for your votes, opinions and comments ... If you use parts of this notebook in your scripts\/notebooks, giving some kind of credit would be very much appreciated :) You can for instance link back to this notebook. Thanks!","55b6c468":"6.2 StratifiedKFold_Train ","017c3b49":"Stratified K-Folds cross-validator.\n\nProvides train\/test indices to split data in train\/test sets.\n\nThis cross-validation object is a variation of KFold that returns stratified folds. The folds are made by preserving the percentage of samples for each class.","606622ff":"6. Modeling","1454ecac":"0. Import \n1. Utils \n\n2. Reducing Memory Usage\n3. Missing values\n4. fillna\n5. Drop\n\n  5.1 Correlation matrix\n\n  5.2 Constant features\n\n  5.3 Random Forest selector \n\n  5.4 PCA \n\n6. Modeling \n\n 6.1 GridsearchCV\n\n 6.2 StratifiedKfoldCV train\n \n 6.3 Stacking ","95a3b7f7":"Dimensionality reduction is way to reduce the complexity of a model and avoid overfitting. There are two main categories of dimensionality reduction: feature selection and feature extraction. Via feature selection, we select a subset of the original features (the past methods ) , whereas in feature extraction (this method ) , we derive information from the feature set to construct a new feature subspace.","17e4e13f":"6.3 Stacking","0de036da":"5.1 Correlation matrix\n","3a51f0e9":"Since we have a large space here (120 columns) reducing this space would be an extraordinary idea. Several methods proposed here ","b064610d":"As shown in the figure, our data set contains several nans. We have to do a filling nans job","12d65cee":"Two methods proposed here : \n- SimpleImputer of sklearn \n- Random sampling : consists in filling the nans of each column with values already existing in the considered column ","84606192":"1. Utils ","afd04f39":"5.3 Random forest selector ","6e34e703":"To give a better intuition, features that are selected at the top of the trees are in general more important than features that are selected at the end nodes of the trees, as generally the top splits lead to bigger information gains."}}