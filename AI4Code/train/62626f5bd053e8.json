{"cell_type":{"7dd4f4b3":"code","375a6b07":"code","fbe26b15":"code","530f621f":"code","653d3e8e":"code","a7a0997c":"code","c2fa06ae":"code","fc2b5eef":"code","d6033771":"code","d08a5844":"code","0eb22c07":"code","fda54933":"code","aaa5e5e8":"code","5887c561":"code","dde7a5ab":"code","88cb6492":"code","faf2452c":"code","c8970fa2":"code","e74c0e31":"code","7972e880":"code","fffebf64":"code","850ec6fd":"code","5cfe49bc":"code","c185c46d":"code","b2d80914":"code","fbb9c2bc":"markdown","144304a2":"markdown","d9aaa6b7":"markdown","55c4a136":"markdown","9ae124d8":"markdown","e9145233":"markdown","1c33b54c":"markdown","6bff0127":"markdown","4d207021":"markdown","85b273a2":"markdown","f8abe245":"markdown","8c1cb32a":"markdown","948b055b":"markdown","ef5d2d6b":"markdown","2281eb68":"markdown","f591b74f":"markdown","4785a7ce":"markdown","684e4712":"markdown","1f18aa8d":"markdown","0e8848be":"markdown","e3d6eed0":"markdown","f4f0427d":"markdown","8316a01c":"markdown","2723e760":"markdown","6d7e5b39":"markdown","1e937cfe":"markdown","bb42ce3f":"markdown","12d30798":"markdown","7fc2b2a2":"markdown","2de74838":"markdown"},"source":{"7dd4f4b3":"import numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nplt.rcParams[\"figure.figsize\"] = (20,7)\n\ndata_dir = '\/kaggle\/input\/competitive-data-science-predict-future-sales\/sales_train.csv'\n\ndf = pd.read_csv(data_dir)\ndf = df[['date', 'item_cnt_day']]\ndf['date'] = pd.to_datetime(df['date'])\ndf = df.set_index('date')\n\n# group data by weekly sales\ndf_data = df.groupby(pd.Grouper(freq='W')).sum()\ndf_data = df_data.rename(columns = {'item_cnt_day': 'weekly_sales'})\ndf_data.plot()\nplt.grid()","375a6b07":"from statsmodels.graphics.tsaplots import plot_acf\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (20,7)\n\nacf_plot = plot_acf(df_data['weekly_sales'].values, lags = 70)\nplt.xlabel('lag')\nplt.ylabel('correlation')\nplt.show()","fbe26b15":"from statsmodels.graphics.tsaplots import plot_pacf\nimport matplotlib.pyplot as plt\nplt.rcParams[\"figure.figsize\"] = (20,7)\n\nacf_plot = plot_pacf(df_data['weekly_sales'].values, lags = 70)\nplt.xlabel('lag')\nplt.ylabel('pacf coefficient')\nplt.show()","530f621f":"from statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.stattools import kpss\nfrom statsmodels.graphics.tsaplots import plot_acf\n\ndef adf_stat(time_series):\n    \"\"\"\n    Run the augmented Dickey-Fuller test on a time series\n    to determine if it's stationary.\n    Arguments: \n        time_series: series. Time series that we want to test \n    Outputs: \n        Test statistics for the Augmented Dickey Fuller test in \n        the console \n        \n    Null hypothesis: Signal is non-stationary\n    \n    if ADF Statistic > critical values then we cannot reject null hypothesis (signal is non-stationary)\n    if ADF Statistic p < critical values then signal is stationary (based on significance)\n    \"\"\"\n    result = adfuller(time_series.values)\n    s = ''\n    s+= 'ADF Statistic: %.3f\\n' % result[0]\n    s+= 'p-value: %.3f\\n' % result[1]\n    s+= 'Critical Values:\\n'\n    for key, value in result[4].items():\n        s += '%s: %.3f\\n' % (key, value)\n    \n    if result[0] < result[4]['1%']:\n        stationarity = 1\n    elif result[4]['1%'] <= result[0] and result[0] <= result[4]['10%']:\n        stationarity = 2\n    else:\n        stationarity = 0\n    \n    return s, stationarity\ndef kpss_stat(time_series):\n    \"\"\"\n    Kwiatkowski-Phillips-Schmidt-Shin test for stationarity.\n\n    Computes the Kwiatkowski-Phillips-Schmidt-Shin (KPSS) test for the null hypothesis that x is level or trend stationary.\n    \"\"\"\n    import warnings\n    warnings.filterwarnings(\"ignore\")\n    \n    result = kpss(time_series.values)\n    s = ''\n    s+= 'KPSS Statistic: %.3f\\n' % result[0]\n    s+= 'p-value: %.3f\\n' % result[1]\n    s+= 'Critical Values:\\n'\n    for key, value in result[3].items():\n        if key == '2.5%':\n            continue\n        s += '%s: %.3f\\n' % (key, value)\n    \n    if result[0] > result[3]['1%']:\n        stationarity = 0\n    elif result[3]['1%'] >= result[0] and result[0] >= result[3]['10%']:\n        stationarity = 2\n    else:\n        stationarity = 1\n    \n    return s, stationarity\n\ndef plot_acf_stat_test(series, series_title, ax1, ax2):\n\n    series.plot(ax = ax1)\n    ax1.set_title(series_title)\n    acf_plot = plot_acf(series.values, lags = 100, ax = ax2)\n    ax2.set_title(f'{series_title} autocorrelation')\n    \n    text_color_dict = {0:'red', 1: 'green', 2: 'yellow'}\n    s, s_idx = adf_stat(series)\n    s2, s2_idx = kpss_stat(series)\n    ax2.annotate(s, size=11, color='black', xy=(0.8, 0.6),xycoords='axes fraction',\n                   bbox=dict(boxstyle=\"square,pad=0.3\", fc=\"white\", ec=text_color_dict[s_idx], lw=2))\n    ax2.annotate(s2, size=11, color='black', xy=(0.8, 0.0),xycoords='axes fraction',\n                   bbox=dict(boxstyle=\"square,pad=0.3\", fc=\"white\", ec=text_color_dict[s2_idx], lw=2))\n\nplt.rcParams[\"figure.figsize\"] = (20,14)\nfig, axs = plt.subplots(3, 2)\nfig.subplots_adjust(hspace=0.4)\n\nplot_acf_stat_test(df_data['weekly_sales'].dropna(),'Weekly sales', axs[0,0], axs[0,1])\nplot_acf_stat_test(df_data['weekly_sales'].diff().dropna(),\n                   'Weekly sales difference', axs[1,0], axs[1,1])\n# the value 5000 in replace is hardcoded because of division by 0 in pct_change\nplot_acf_stat_test(df_data['weekly_sales'].replace(0, 5000).pct_change().dropna(),\n                   'Weekly sales percent of change', axs[2,0], axs[2,1])","653d3e8e":"from sklearn.metrics import mean_squared_error\nplt.rcParams[\"figure.figsize\"] = (20,7)\n\ndf_pred = df_data.copy()\ndf_pred['weekly_sales_pred'] = df_pred['weekly_sales'].shift()\n\n# calculate rolling standard deviation of using 50 values \ndf_pred['std_50'] = df_pred['weekly_sales_pred'].rolling(50).std()\n\n# get last 50 predictions\ndf_pred = df_pred[-50:]\n# confidence interval\ndf_pred['ci_lower'] = df_pred['weekly_sales_pred']-2*df_pred['std_50']\ndf_pred['ci_upper'] = df_pred['weekly_sales_pred']+2*df_pred['std_50']\n\nplt.plot(df_pred['weekly_sales'], color = 'green')\nplt.plot(df_pred['weekly_sales_pred'], color = 'red')\nplt.legend(['Weekly sales', 'Predicted weekly sales'])\nplt.fill_between(df_pred.index,\n                 df_pred['ci_lower'],\n                 df_pred['ci_upper'], color='lightblue', alpha=0.5)\nplt.title('Naive forecast model, MSE: {:,}'.format(round(\n            mean_squared_error(df_pred['weekly_sales'], df_pred['weekly_sales_pred']),3)))\nplt.grid()\nplt.show()\ndel df_pred","a7a0997c":"from sklearn.metrics import mean_squared_error\n\ndf_pred = df_data.copy()\ndf_pred['weekly_sales_t-1'] = df_pred['weekly_sales'].shift()\n\n# mean of the last 5 values\ndf_pred['weekly_sales_pred'] = df_pred['weekly_sales_t-1'].rolling(5).mean()\ndf_pred['std_50'] = df_pred['weekly_sales_pred'].rolling(50).std()\n\n# get last 50 weekly sales predictions\ndf_pred = df_pred[-50:]\n# confidence interval\ndf_pred['ci_lower'] = df_pred['weekly_sales_pred']-2*df_pred['std_50']\ndf_pred['ci_upper'] = df_pred['weekly_sales_pred']+2*df_pred['std_50']\n\nplt.plot(df_pred['weekly_sales'], color = 'green')\nplt.plot(df_pred['weekly_sales_pred'], color = 'red')\nplt.legend(['Weekly sales', 'Predicted weekly_sales'])\nplt.fill_between(df_pred.index,\n                 df_pred['ci_lower'],\n                 df_pred['ci_upper'], color='lightblue', alpha=0.5)\nplt.title('Simple rolling mean with k=2, MSE: {:,}'.format(round(mean_squared_error(\n    df_pred['weekly_sales'], df_pred['weekly_sales_pred']), 2)))\nplt.grid()\nplt.show()\ndel df_pred","c2fa06ae":"from sklearn.metrics import mean_squared_error\n\n\ndf_pred = df_data.copy()\ndf_pred['weekly_sales_diff'] = df_pred['weekly_sales'].diff()\ndf_pred = df_pred.dropna()\n\ndf_pred['weekly_sales_diff_t-1'] = df_pred['weekly_sales_diff'].shift()\n\n# mean of the last 10 values\nk = 10\ndf_pred['weekly_sales_diff_pred'] = df_pred['weekly_sales_diff_t-1'].rolling(k).mean()\ndf_pred['weekly_sales_pred'] = df_pred['weekly_sales_diff_pred'] + df_pred['weekly_sales'].shift()\ndf_pred['weekly_sales'] = df_pred['weekly_sales_diff'] + df_pred['weekly_sales'].shift()\n\ndf_pred['std_50'] = df_pred['weekly_sales_pred'].rolling(50).std()\n # get last 50 weekly sales predictions\ndf_pred = df_pred[-50:]\n# confidence interval\ndf_pred['ci_lower'] = df_pred['weekly_sales_pred']-2*df_pred['std_50']\ndf_pred['ci_upper'] = df_pred['weekly_sales_pred']+2*df_pred['std_50']\n\nplt.plot(df_pred['weekly_sales'], color = 'green')\nplt.plot(df_pred['weekly_sales_pred'], color = 'red')\nplt.legend(['Weekly sales', 'Predicted weekly sales'])\nplt.fill_between(df_pred.index,\n                 df_pred['ci_lower'],\n                 df_pred['ci_upper'], color='lightblue', alpha=0.5)\nplt.title('Simple rolling mean using weekly_sales difference with k={}, MSE: {:,}'.format(\n    k,round(mean_squared_error(df_pred['weekly_sales'], df_pred['weekly_sales_pred']),2)))\nplt.grid()\nplt.show()\ndel df_pred","fc2b5eef":"from sklearn.metrics import mean_squared_error\n\n\ndf_pred = df_data.copy()\ndf_pred['weekly_sales_diff'] = df_pred['weekly_sales'].diff()\ndf_pred = df_pred.dropna()\n\ndf_pred['weekly_sales_diff_t-1'] = df_pred['weekly_sales_diff'].shift()\n\nk =10\n#normalize linear weights\nwts = np.arange(1,k+1)\nwts = wts\/np.sum(wts)\n\ndef wma(w):                        \n    def g(x):\n        return (w*x).sum()\n    return g\n\ndf_pred['weekly_sales_diff_pred'] = df_pred['weekly_sales_diff_t-1'].rolling(k).apply(wma(wts))\n\ndf_pred['weekly_sales_pred'] = df_pred['weekly_sales_diff_pred'] + df_pred['weekly_sales'].shift()\ndf_pred['weekly_sales'] = df_pred['weekly_sales_diff'] + df_pred['weekly_sales'].shift()\n\n\ndf_pred['std_50'] = df_pred['weekly_sales_pred'].rolling(50).std()\n # get last 50 Open values predictions\ndf_pred = df_pred[-50:]\n# confidence interval\ndf_pred['ci_lower'] = df_pred['weekly_sales_pred']-2*df_pred['std_50']\ndf_pred['ci_upper'] = df_pred['weekly_sales_pred']+2*df_pred['std_50']\n\nplt.plot(df_pred['weekly_sales'], color = 'green')\nplt.plot(df_pred['weekly_sales_pred'], color = 'red')\nplt.legend(['Weekly sales', 'Predicted weekly sales'])\nplt.fill_between(df_pred.index,\n                 df_pred['ci_lower'],\n                 df_pred['ci_upper'], color='lightblue', alpha=0.5)\n\nplt.title('Weighted rolling mean using open percent of change with k={}, MSE: {:,}'.format(\n    k,round(mean_squared_error(df_pred['weekly_sales'], df_pred['weekly_sales_pred']),2)))\nplt.grid()\nplt.show()\ndel df_pred","d6033771":"from statsmodels.tsa.holtwinters import SimpleExpSmoothing\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm.notebook import tqdm\n\ndf_ses = df_data.copy()\ndf_ses['weekly_sales_diff'] = df_ses['weekly_sales'].diff()\ndf_ses = df_ses.dropna()\n\ndf_pred = pd.DataFrame()\n\n# predict last 100 values\nfor i in tqdm(range(100)):\n    \n    training_data = df_ses['weekly_sales_diff'][:(-100+i)]\n    \n    model = SimpleExpSmoothing(training_data)\n    # the alpha value can be set here using smoothing_level parameter\n    # if it is not provided, the model will automatically optimize the value\n    model_fit = model.fit()\n    yhat = model_fit.predict().to_frame()\n    df_pred = df_pred.append(yhat)\n\ndf_pred.columns = ['weekly_sales_diff_pred']\ndf_pred = pd.merge(df_pred, df_ses, how = 'left', left_index = True, right_index = True)\ndf_pred['weekly_sales_pred'] = df_pred['weekly_sales_diff_pred'] + df_ses['weekly_sales'].shift()\ndf_pred['weekly_sales'] = df_pred['weekly_sales_diff'] + df_ses['weekly_sales'].shift()\n\n# create confidence interval\ndf_pred['std_50'] = df_pred['weekly_sales_pred'].rolling(50).std()\n # get last 50 Open values predictions\ndf_pred = df_pred[-50:]\n# confidence interval\ndf_pred['ci_lower'] = df_pred['weekly_sales_pred']-2*df_pred['std_50']\ndf_pred['ci_upper'] = df_pred['weekly_sales_pred']+2*df_pred['std_50']\n\nplt.plot(df_pred['weekly_sales'], color = 'green')\nplt.plot(df_pred['weekly_sales_pred'], color = 'red')\nplt.legend(['Weekly sales', 'Predicted weekly sales'])\nplt.fill_between(df_pred.index,\n                 df_pred['ci_lower'],\n                 df_pred['ci_upper'], color='lightblue', alpha=0.5)\n\nplt.title('Simple exponential smoothing, MSE: {:,}'.format(\n    round(mean_squared_error(df_pred['weekly_sales'], df_pred['weekly_sales_pred']),2)))\nplt.grid()\nplt.show()\n\n#del df_pred, df_ses","d08a5844":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm.notebook import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndf_des = df_data.copy()\n\ndf_pred = pd.DataFrame()\n\n# predict last 100 values\nfor i in tqdm(range(100)):\n    \n    training_data = df_des['weekly_sales'][:(-100+i)]\n    \n    model = ExponentialSmoothing(training_data, trend = 'add')\n    \n    model_fit = model.fit()\n    yhat = model_fit.predict().to_frame()\n    df_pred = df_pred.append(yhat)\n\ndf_pred.columns = ['weekly_sales_pred']\ndf_pred = pd.merge(df_pred, df_des, how = 'left', left_index = True, right_index = True)\n\n# create confidence interval\ndf_pred['std_50'] = df_pred['weekly_sales_pred'].rolling(50).std()\ndf_pred['ci_lower'] = df_pred['weekly_sales_pred']-2*df_pred['std_50']\ndf_pred['ci_upper'] = df_pred['weekly_sales_pred']+2*df_pred['std_50']\n\ndf_pred = df_pred[-50:]\n\nplt.plot(df_pred['weekly_sales'], color = 'green')\nplt.plot(df_pred['weekly_sales_pred'], color = 'red')\nplt.legend(['Weekly sales', 'Predicted weekly sales'])\nplt.fill_between(df_pred.index,\n                 df_pred['ci_lower'],\n                 df_pred['ci_upper'], color='lightblue', alpha=0.5)\nplt.title('Double exponential smoothing, MSE: {:,}'.format(round(\n    mean_squared_error(df_pred['weekly_sales'], df_pred['weekly_sales_pred']),2)))\nplt.grid()\nplt.show()","0eb22c07":"from statsmodels.tsa.holtwinters import ExponentialSmoothing\nfrom sklearn.metrics import mean_squared_error\nfrom tqdm.notebook import tqdm\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndf_tes = df_data.copy()\n\ndf_pred = pd.DataFrame()\n\n# predict last 100 values\nfor i in tqdm(range(100)):\n    \n    training_data = df_tes['weekly_sales'][:(-100+i)]\n    \n    model = ExponentialSmoothing(training_data, trend='add',\n                                 seasonal='add',seasonal_periods=12)\n    \n    model_fit = model.fit()\n    yhat = model_fit.predict().to_frame()\n    df_pred = df_pred.append(yhat)\n\ndf_pred.columns = ['weekly_sales_pred']\ndf_pred = pd.merge(df_pred, df_tes, how = 'left', left_index = True, right_index = True)\n\n# create confidence interval\ndf_pred['std_50'] = df_pred['weekly_sales_pred'].rolling(50).std()\ndf_pred['ci_lower'] = df_pred['weekly_sales_pred']-2*df_pred['std_50']\ndf_pred['ci_upper'] = df_pred['weekly_sales_pred']+2*df_pred['std_50']\n\ndf_pred = df_pred[-50:]\n\nplt.plot(df_pred['weekly_sales'], color = 'green')\nplt.plot(df_pred['weekly_sales_pred'], color = 'red')\nplt.legend(['Weekly sales', 'Predicted weekly sales'])\nplt.fill_between(df_pred.index,\n                 df_pred['ci_lower'],\n                 df_pred['ci_upper'], color='lightblue', alpha=0.5)\nplt.title('Holt Winter\u2019s Exponential Smoothing, MSE: {:,}'.format(round(\n    mean_squared_error(df_pred['weekly_sales'], df_pred['weekly_sales_pred']),2)))\nplt.grid()\nplt.show()\ndel df_tes","fda54933":"from statsmodels.tsa.arima.model import ARIMA\n\ndf_ar = df_data.copy()\ndf_ar['weekly_sales_diff'] = df_ar['weekly_sales'].diff()\ndf_ar = df_ar.dropna()\n\ntraining_data = df_ar['weekly_sales_diff'][:-50]\n\n# order = (p, d, q) where AR(p)\nmodel = ARIMA(training_data, order = (3, 0, 0))\nmodel_fit = model.fit()\nmodel_fit.summary()","aaa5e5e8":"from tqdm.notebook import tqdm\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom sklearn.metrics import mean_squared_error\n\n# predict last 50 Return values one by one\n# also called rolling forecasting origin\n\ndf_ar = df_data.copy()\ndf_ar['weekly_sales_diff'] = df_ar['weekly_sales'].diff()\ndf_ar = df_ar.dropna()\n\ndf_pred = pd.DataFrame()\n\nfor i in tqdm(range(50)):\n    \n    training_data = df_ar['weekly_sales_diff'][:(-50+i)]\n    \n    #order = (p, d, q) where AR(p)\n    model = ARIMA(training_data, order = (3, 0, 0))\n    model_fit = model.fit()\n    pred_temp = model_fit.get_forecast(1).summary_frame()\n    df_pred = df_pred.append(pred_temp)\n\ndf_pred = pd.merge(df_pred, df_ar, how = 'left', left_index = True, right_index = True)\ndf_pred['weekly_sales_pred'] = df_pred['mean'] + df_ar['weekly_sales'].shift()\ndf_pred['weekly_sales'] = df_pred['weekly_sales_diff'] + df_ar['weekly_sales'].shift()\ndf_pred['ci_lower'] = df_pred['mean_ci_lower'] + df_ar['weekly_sales'].shift()\ndf_pred['ci_upper'] = df_pred['mean_ci_upper'] + df_ar['weekly_sales'].shift()\n\nplt.rcParams[\"figure.figsize\"] = (20,7)\nplt.plot(df_pred['weekly_sales'], color = 'green')\nplt.plot(df_pred['weekly_sales_pred'], color = 'red')\nplt.legend(['Weekly sales', 'Predicted weekly sales'])\nplt.fill_between(df_pred.index,\n                 df_pred['ci_lower'],\n                 df_pred['ci_upper'], color='lightblue', alpha=0.5)\nplt.title('AR(3), MSE: {:,}'.format(round(\n                mean_squared_error(df_pred['weekly_sales'], df_pred['weekly_sales_pred']),2)))\nplt.grid()\nplt.show()\ndel df_ar, df_pred","5887c561":"from tqdm.notebook import tqdm\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom sklearn.metrics import mean_squared_error\n\n# predict last 50 Return values one by one\n# also called rolling forecasting origin\n\ndf_ma = df_data.copy()\ndf_ma['weekly_sales_diff'] = df_ma['weekly_sales'].diff()\ndf_ma = df_ma.dropna()\n\ndf_pred = pd.DataFrame()\n\nfor i in tqdm(range(50)):\n    \n    training_data = df_ma['weekly_sales_diff'][:(-50+i)]\n    \n    #order = (p, d, q) where MA(q)\n    model = ARIMA(training_data, order = (0, 0, 4))\n    model_fit = model.fit()\n    pred_temp = model_fit.get_forecast(1).summary_frame()\n    df_pred = df_pred.append(pred_temp)\n\ndf_pred = pd.merge(df_pred, df_ma, how = 'left', left_index = True, right_index = True)\ndf_pred['weekly_sales_pred'] = df_pred['mean'] + df_ma['weekly_sales'].shift()\ndf_pred['weekly_sales'] = df_pred['weekly_sales_diff'] + df_ma['weekly_sales'].shift()\ndf_pred['ci_lower'] = df_pred['mean_ci_lower'] + df_ma['weekly_sales'].shift()\ndf_pred['ci_upper'] = df_pred['mean_ci_upper'] + df_ma['weekly_sales'].shift()\n\nplt.rcParams[\"figure.figsize\"] = (20,7)\nplt.plot(df_pred['weekly_sales'], color = 'green')\nplt.plot(df_pred['weekly_sales_pred'], color = 'red')\nplt.legend(['Weekly sales', 'Predicted weekly sales'])\nplt.fill_between(df_pred.index,\n                 df_pred['ci_lower'],\n                 df_pred['ci_upper'], color='lightblue', alpha=0.5)\nplt.title('MA(4), MSE: {:,}'.format(round(\n                mean_squared_error(df_pred['weekly_sales'], df_pred['weekly_sales_pred']),2)))\nplt.grid()\nplt.show()\ndel df_ma, df_pred","dde7a5ab":"from tqdm.notebook import tqdm\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom sklearn.metrics import mean_squared_error\n\n# predict last 50 Return values one by one\n# also called rolling forecasting origin\n\ndf_arma = df_data.copy()\ndf_arma['weekly_sales_diff'] = df_arma['weekly_sales'].diff()\ndf_arma = df_arma.dropna()\n\ndf_pred = pd.DataFrame()\n\nfor i in tqdm(range(50)):\n    \n    training_data = df_arma['weekly_sales_diff'][:(-50+i)]\n    \n    #order = (p, d, q) where MA(q)\n    model = ARIMA(training_data, order = (3, 0, 3))\n    model_fit = model.fit()\n    pred_temp = model_fit.get_forecast(1).summary_frame()\n    df_pred = df_pred.append(pred_temp)\n\ndf_pred = pd.merge(df_pred, df_arma, how = 'left', left_index = True, right_index = True)\ndf_pred['weekly_sales_pred'] = df_pred['mean'] + df_arma['weekly_sales'].shift()\ndf_pred['weekly_sales'] = df_pred['weekly_sales_diff'] + df_arma['weekly_sales'].shift()\ndf_pred['ci_lower'] = df_pred['mean_ci_lower'] + df_arma['weekly_sales'].shift()\ndf_pred['ci_upper'] = df_pred['mean_ci_upper'] + df_arma['weekly_sales'].shift()\n\nplt.rcParams[\"figure.figsize\"] = (20,7)\nplt.plot(df_pred['weekly_sales'], color = 'green')\nplt.plot(df_pred['weekly_sales_pred'], color = 'red')\nplt.legend(['Weekly sales', 'Predicted weekly sales'])\nplt.fill_between(df_pred.index,\n                 df_pred['ci_lower'],\n                 df_pred['ci_upper'], color='lightblue', alpha=0.5)\nplt.title('ARMA(3,3), MSE: {:,}'.format(round(\n                mean_squared_error(df_pred['weekly_sales'], df_pred['weekly_sales_pred']),2)))\nplt.grid()\nplt.show()\ndel df_arma, df_pred","88cb6492":"from tqdm.notebook import tqdm\nfrom statsmodels.tsa.arima.model import ARIMA\nfrom sklearn.metrics import mean_squared_error\nplt.rcParams[\"figure.figsize\"] = (20,7)\n\n# predict last 50 Open values one by one\n\ndf_pred = pd.DataFrame()\n\nfor i in tqdm(range(50)):\n    \n    # we will use directly open price without transforming\n    training_data = df_data['weekly_sales'][:(-50+i)]\n    \n    #order = (p, d, q)\n    model = ARIMA(training_data, order = (3, 1, 3))\n    model_fit = model.fit()\n    pred_temp = model_fit.get_forecast(1).summary_frame()\n    df_pred = df_pred.append(pred_temp)\n\ndf_pred['weekly_sales'] = df_data['weekly_sales'][-50:]\n\nplt.plot(df_pred['weekly_sales'], color = 'green')\nplt.plot(df_pred['mean'], color = 'red')\nplt.legend(['Weekly sales', 'Predicted weekly sales'])\nplt.fill_between(df_pred.index,\n                 df_pred['mean_ci_lower'],\n                 df_pred['mean_ci_upper'], color='lightblue', alpha=0.5)\nplt.title('ARIMA(3, 1, 3), MSE: {:,}'.format(round(mean_squared_error(\n    df_pred['weekly_sales'], df_pred['mean'])), 2))\nplt.grid()\nplt.show()","faf2452c":"!pip install pmdarima","c8970fa2":"#!pip install pmdarima\n\nfrom pmdarima import auto_arima\ntraining_data = df_data['weekly_sales'][:-50]\n\n# Note, by default m=1 which means that data is non-seasonal.\n# This parameter must be known apriori.\n# For the sake of example m>1 will be used.\nsmodel = auto_arima(training_data, start_p=0, start_q=0,\n                       max_p=3, max_q=3, max_d = 2,\n                       seasonal=True,m=4,\n                       start_P=0, start_Q = 0, max_P = 3, max_Q = 3,\n                       max_D = 2,information_criterion='aic',test='adf',\n                       trace=True)\n\nsmodel.summary()","e74c0e31":"from tqdm.notebook import tqdm\nfrom pmdarima import auto_arima\nfrom sklearn.metrics import mean_squared_error\nfrom dateutil.relativedelta import relativedelta\nplt.rcParams[\"figure.figsize\"] = (20,7)\n\n# predict last 50 weekly sales values one by one\n\npred_dict = {'date':[], 'pred_weekly_sales':[], 'ci_lower':[],'ci_upper':[] }\n\nfor i in tqdm(range(50)):\n    \n    # we will use directly weekly sales without transforming\n    training_data = df_data['weekly_sales'][:(-50+i)]\n    \n    model = auto_arima(training_data)\n    print('best model {}'.format(model.get_params()['order']), end = ' ')\n    pred, confint = model.predict(n_periods=1, return_conf_int=True)\n    pred_date = training_data.index[-1] + relativedelta(weeks = 1)\n    \n    pred_dict['date'].append(pred_date)\n    pred_dict['pred_weekly_sales'].append(pred[0])\n    pred_dict['ci_lower'].append(confint[0][0])\n    pred_dict['ci_upper'].append(confint[0][1])\n    \n\ndf_pred = pd.DataFrame(pred_dict)\ndf_pred = df_pred.set_index('date')\ndf_pred['weekly_sales'] = df_data['weekly_sales'][-50:]\n\nplt.plot(df_pred['weekly_sales'], color = 'green')\nplt.plot(df_pred['pred_weekly_sales'], color = 'red')\nplt.legend(['Weekly sales', 'Predicted weekly sales'])\nplt.fill_between(df_pred.index,\n                 df_pred['ci_lower'],\n                 df_pred['ci_upper'], color='lightblue', alpha=0.5)\nplt.title('Auto SARIMA, MSE: {:,}'.format(round(mean_squared_error(\n    df_pred['weekly_sales'], df_pred['pred_weekly_sales']),2)))\nplt.grid()\nplt.show()","7972e880":"from tqdm.notebook import tqdm\nfrom pmdarima import auto_arima\nfrom sklearn.metrics import mean_squared_error\nfrom dateutil.relativedelta import relativedelta\nplt.rcParams[\"figure.figsize\"] = (20,7)\n\n#prepare exogenous variable\ndf_sarimax = df_data.copy()\ndf_sarimax['weekly_sales_rolling_std_3'] = df_sarimax.rolling(3).std().shift()\ndf_sarimax = df_sarimax.dropna()\n\n# predict last 50 values one by one\npred_dict = {'date':[], 'pred_weekly_sales':[], 'ci_lower':[],'ci_upper':[] }\n\nfor i in tqdm(range(50)):\n    \n    # we will use directly weekly sales price without transforming\n    training_data = df_sarimax['weekly_sales'][:(-50+i)]\n    training_exo = df_sarimax['weekly_sales_rolling_std_3'][:(-50+i)].to_frame()\n    \n    model = auto_arima(training_data, X=training_exo)\n    print('best model {}'.format(model.get_params()['order']), end = ' ')\n    \n    pred_date = training_data.index[-1] + relativedelta(weeks = 1)\n    pred, confint = model.predict(X =df_sarimax[df_sarimax.index == pred_date]['weekly_sales_rolling_std_3'].to_frame(),\n                                  n_periods=1, return_conf_int=True)\n    \n    pred_dict['date'].append(pred_date)\n    pred_dict['pred_weekly_sales'].append(pred[0])\n    pred_dict['ci_lower'].append(confint[0][0])\n    pred_dict['ci_upper'].append(confint[0][1])\n    \n\ndf_pred = pd.DataFrame(pred_dict)\ndf_pred = df_pred.set_index('date')\ndf_pred['weekly_sales'] = df_data['weekly_sales'][-50:]\n\nplt.plot(df_pred['weekly_sales'], color = 'green')\nplt.plot(df_pred['pred_weekly_sales'], color = 'red')\nplt.legend(['Weekly sales', 'Predicted weekly sales'])\nplt.fill_between(df_pred.index,\n                 df_pred['ci_lower'],\n                 df_pred['ci_upper'], color='lightblue', alpha=0.5)\nplt.title('Auto SARIMAX, MSE: {:,}'.format(round(mean_squared_error(\n    df_pred['weekly_sales'], df_pred['pred_weekly_sales']), 2)))\nplt.grid()\nplt.show()","fffebf64":"import statsmodels.tsa.stattools as ts\n\ndef eg_stat(s1, s2):\n    \"\"\"\n    Test for no-cointegration of a univariate equation.\n\n    The null hypothesis is no cointegration. Variables in y0 and y1 are assumed to be integrated \n    of order 1, I(1).\n\n    This uses the augmented Engle-Granger two-step cointegration test.\n    Constant or trend is included in 1st stage regression, i.e. in cointegrating equation. \n    \n    Null hypothesis: Signal is non-stationary\n    \n    if EG Statistic p > critical values then we cannot reject null hypothesis (signals aren't cointegrated)\n    if EG Statistic p < critical values then signals are cointegrated (based on significance)\n    \n    \"\"\"\n    result = ts.coint(s1, s2)\n    s = ''\n    s+= 'EG Statistic: %.3f\\n' % result[0]\n    s+= 'p-value: %.3f\\n' % result[1]\n    s+= 'Critical Values:\\n'\n    for key, value in zip(['1%', '5%', '10%'], result[2]):\n        s += '%s: %.3f\\n' % (key, value)\n    \n    if result[0] < result[2][0]:\n        coint = 1\n    elif result[2][0] <= result[0] and result[0] <= result[2][2]:\n        coint = 2\n    else:\n        coint = 0\n    \n    return s, coint\n\ndef plot_eg_stat_test(s1,s2, ax):\n\n    s1.plot(ax = ax, legend = s1.name)\n    s2.plot(ax = ax, legend = s2.name)\n    \n    text_color_dict = {0:'red', 1: 'green', 2: 'yellow'}\n    s, c_idx = eg_stat(s1, s2)\n    \n    ax.annotate(s, size=14, color='black', xy=(0.0, 0.6),xycoords='axes fraction',\n                   bbox=dict(boxstyle=\"square,pad=0.3\", fc=\"white\", ec=text_color_dict[c_idx], lw=2))\n    if c_idx == 0:\n        ax.set_title('signals are not cointegrated')\n    else:\n        ax.set_title('signals are cointegrated')\n    ax.grid()\n\nplt.rcParams[\"figure.figsize\"] = (20,7)\nfig, axs = plt.subplots(1, 2)\n\ndf_coint = df_data.copy()\ndf_coint['weekly_sales_rolling_std_3'] = df_coint.rolling(3).std().shift()\ndf_coint['noise'] = [5000*np.random.rand() for x in range(len(df_coint))]\ndf_coint['weekly_sales_plus_noise'] = df_coint['weekly_sales']+df_coint['noise']\ndf_coint = df_coint.dropna()\n\nplot_eg_stat_test(df_coint['weekly_sales'],df_coint['weekly_sales_plus_noise'], axs[0])\nplot_eg_stat_test(df_coint['weekly_sales'],df_coint['weekly_sales_rolling_std_3'], axs[1])\nplt.show()","850ec6fd":"from statsmodels.tsa.api import VAR\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom dateutil.relativedelta import relativedelta\nplt.rcParams[\"figure.figsize\"] = (20,7)\n\n# Two stationary signals, weekly sales difference and standard deviation of the last 3 weekly sales, are used in VAR\n\ndf_var = df_data.copy()\ndf_var['weekly_sales_diff'] = df_var.diff()\ndf_var['weekly_sales_rolling_std_3'] = df_var['weekly_sales'].rolling(3).std()\ndf_var = df_var.dropna()\n\npred_dict = {'date':[], 'pred_diff':[], 'ci_lower':[],'ci_upper':[] }\n\nfor i in tqdm(range(50)):\n    \n    training_data = df_var[['weekly_sales_diff', 'weekly_sales_rolling_std_3']][:(-50+i)]\n    \n    # VAR model selects lag p based on AIC\n    \n    model = VAR(training_data)\n    model_fit = model.fit(ic = 'aic')\n    model_results = model_fit.forecast_interval(model_fit.y, steps=1)\n    \n    #model_results has this form \n    #     (array([[4.63470515e-02, 9.85550752e+02]]),\n    #      array([[ -0.19744317, 173.58328614]]),\n    #      array([[2.90137274e-01, 1.79751822e+03]]))\n    \n    pred_date = training_data.index[-1] + relativedelta(weeks = 1)\n    \n    pred_dict['date'].append(pred_date)\n    pred_dict['pred_diff'].append(model_results[0][0][0])\n    pred_dict['ci_lower'].append(model_results[1][0][0])\n    pred_dict['ci_upper'].append(model_results[2][0][0])\n    \n    \n# transform data\ndf_pred = pd.DataFrame(pred_dict)\ndf_pred = df_pred.set_index('date')\ndf_pred['weekly_sales_diff'] = df_var['weekly_sales_diff'][-50:]\n\n\n#df_pred = pd.merge(df_pred, df_var, how = 'left', left_index = True, right_index = True)\ndf_pred['pred_weekly_sales'] = df_pred['pred_diff'] + df_var['weekly_sales'].shift()\ndf_pred['weekly_sales'] = df_pred['weekly_sales_diff'] + df_var['weekly_sales'].shift()\ndf_pred['ci_lower'] = df_pred['ci_lower'] + df_var['weekly_sales'].shift()\ndf_pred['ci_upper'] = df_pred['ci_upper'] + df_var['weekly_sales'].shift()\n\nplt.plot(df_pred['weekly_sales'], color = 'green')\nplt.plot(df_pred['pred_weekly_sales'], color = 'red')\nplt.legend(['Weekly sales', 'Predicted weekly sales'])\nplt.fill_between(df_pred.index,\n                 df_pred['ci_lower'],\n                 df_pred['ci_upper'], color='lightblue', alpha=0.5)\nplt.title('VAR, MSE: {:,}'.format(round(mean_squared_error(\n    df_pred['weekly_sales'], df_pred['pred_weekly_sales']), 2)))\nplt.grid()\nplt.show()","5cfe49bc":"from statsmodels.tsa.api import VECM\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom dateutil.relativedelta import relativedelta\nplt.rcParams[\"figure.figsize\"] = (20,7)\n\n# Cointegrated signals weekly sales and weekly sales plus noise\n\ndf_vecm = df_data.copy()\ndf_vecm['noise'] = [5000*np.random.rand() for x in range(len(df_vecm))]\ndf_vecm['weekly_sales_plus_noise'] = df_vecm['weekly_sales']+df_vecm['noise']\n\npred_dict = {'date':[], 'pred_weekly_sales':[], 'ci_lower':[],'ci_upper':[] }\n\nfor i in tqdm(range(50)):\n    \n    training_data = df_vecm[['weekly_sales', 'weekly_sales_plus_noise']][:(-50+i)]\n    \n    # VECM model, k_ar_diff - number of lagged differences in the model\n    \n    model = VECM(training_data, coint_rank = 1, k_ar_diff=5)\n\n    model_fit = model.fit()\n    pred, lower, upper = model_fit.predict(steps = 1, alpha = 0.05)\n    \n    pred_date = training_data.index[-1] + relativedelta(weeks = 1)\n    \n    pred_dict['date'].append(pred_date)\n    pred_dict['pred_weekly_sales'].append(pred[0][0])\n    pred_dict['ci_lower'].append(lower[0][0])\n    pred_dict['ci_upper'].append(upper[0][0])\n    \ndf_pred = pd.DataFrame(pred_dict)\ndf_pred = df_pred.set_index('date')\ndf_pred['weekly_sales'] = df_vecm['weekly_sales'][-50:]\n\nplt.plot(df_pred['weekly_sales'], color = 'green')\nplt.plot(df_pred['pred_weekly_sales'], color = 'red')\nplt.legend(['Weekly sales', 'Predicted weekly sales'])\nplt.fill_between(df_pred.index,\n                 df_pred['ci_lower'],\n                 df_pred['ci_upper'], color='lightblue', alpha=0.5)\nplt.title('VECM, MSE: {:,}'.format(round(mean_squared_error(\n    df_pred['weekly_sales'], df_pred['pred_weekly_sales']), 2)))\nplt.grid()\nplt.show()","c185c46d":"from statsmodels.tsa.api import VARMAX\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom dateutil.relativedelta import relativedelta\nplt.rcParams[\"figure.figsize\"] = (20,7)\n\n# Two stationary signals, weekly sales difference and standard deviation of the last 3 weekly sales, are used in VARMA\n\ndf_varma = df_data.copy()\ndf_varma['weekly_sales_diff'] = df_varma.diff()\ndf_varma['weekly_sales_rolling_std_3'] = df_varma['weekly_sales'].rolling(3).std()\ndf_varma = df_varma.dropna()\n\n# predict last 50 values one by one\ndf_pred = pd.DataFrame()\n\nfor i in tqdm(range(50)):\n    \n    training_data = df_varma[['weekly_sales_diff', 'weekly_sales_rolling_std_3']][:(-50+i)]\n    \n    # VARMA model selects lag p based on AIC\n    \n    model = VARMAX(training_data)\n    model_fit = model.fit(ic = 'aic')\n    model_forecast = model_fit.get_forecast(steps = 1)\n    \n    df_pred_temp = pd.concat([model_forecast.conf_int(),\n                              model_forecast.predicted_mean], axis = 1)\n    \n    df_pred_temp = df_pred_temp[['lower weekly_sales_diff', 'upper weekly_sales_diff', 'weekly_sales_diff']]\n    \n    df_pred = df_pred.append(df_pred_temp)\n    \ndf_pred = df_pred.rename(columns = {'lower weekly_sales_diff': 'ci_lower', \n                                   'upper weekly_sales_diff': 'ci_upper',\n                                   'weekly_sales_diff':'pred_weekly_sales_diff'})\n\ndf_pred['weekly_sales_diff'] = df_varma['weekly_sales_diff'][-50:]\n\n# transform data\ndf_pred['pred_weekly_sales'] = df_pred['pred_weekly_sales_diff'] + df_varma['weekly_sales'].shift()\ndf_pred['weekly_sales'] = df_pred['weekly_sales_diff'] + df_varma['weekly_sales'].shift()\ndf_pred['ci_lower'] = df_pred['ci_lower'] + df_varma['weekly_sales'].shift()\ndf_pred['ci_upper'] = df_pred['ci_upper'] + df_varma['weekly_sales'].shift()\n\nplt.plot(df_pred['weekly_sales'], color = 'green')\nplt.plot(df_pred['pred_weekly_sales'], color = 'red')\nplt.legend(['Weekly sales', 'Predicted weekly sales'])\nplt.fill_between(df_pred.index,\n                 df_pred['ci_lower'],\n                 df_pred['ci_upper'], color='lightblue', alpha=0.5)\nplt.title('VARMA, MSE: {:,}'.format(round(mean_squared_error(\n    df_pred['weekly_sales'], df_pred['pred_weekly_sales']), 2)))\nplt.grid()\nplt.show()","b2d80914":"from statsmodels.tsa.statespace.varmax import VARMAX\nfrom tqdm.notebook import tqdm\nfrom sklearn.metrics import mean_squared_error\nfrom dateutil.relativedelta import relativedelta\nplt.rcParams[\"figure.figsize\"] = (20,7)\n\n# Construct two stationary signals\ndf_varmax = df_data.copy()\ndf_varmax['weekly_sales_diff'] = df_varmax.diff()\ndf_varmax['weekly_sales_rolling_std_3'] = df_varmax['weekly_sales'].rolling(3).std()\ndf_varmax = df_varmax.dropna()\n#prepare exogenous variable\nexo_df = df_varmax['weekly_sales_diff'].to_frame()\nexo_df['noise'] = [5000*np.random.rand() for x in range(len(exo_df))]\nexo_df['weekly_sales_diff_plus_noise'] = exo_df['weekly_sales_diff']+exo_df['noise']\nexo_df['weekly_sales_diff_plus_noise'] = exo_df['weekly_sales_diff_plus_noise'].shift().fillna(0)\n\n#predict last 50 values one by one\ndf_pred = pd.DataFrame()\n\nfor i in tqdm(range(50)):\n    \n    training_data = df_varmax[['weekly_sales_diff', 'weekly_sales_rolling_std_3']][:(-50+i)]\n    training_exo = exo_df[exo_df.index.isin(training_data.index)]['weekly_sales_diff_plus_noise'].to_frame()\n    \n    # VARMAX model selects lag p based on AIC\n    \n    model = VARMAX(training_data, exog=training_exo)\n    model_fit = model.fit(ic = 'aic')\n    \n    pred_date = training_data.index[-1] + relativedelta(weeks = 1)\n    model_forecast = model_fit.get_forecast(steps = 1, exog =exo_df[exo_df.index == pred_date]['weekly_sales_diff_plus_noise'].to_frame())\n    \n    df_pred_temp = pd.concat([model_forecast.conf_int(),\n                              model_forecast.predicted_mean], axis = 1)\n    \n    df_pred_temp = df_pred_temp[['lower weekly_sales_diff', 'upper weekly_sales_diff', 'weekly_sales_diff']]\n    \n    df_pred = df_pred.append(df_pred_temp)\n    \ndf_pred = df_pred.rename(columns = {'lower weekly_sales_diff': 'ci_lower', \n                                   'upper weekly_sales_diff': 'ci_upper',\n                                   'weekly_sales_diff':'pred_weekly_sales_diff'})\n\ndf_pred['weekly_sales_diff'] = df_varmax['weekly_sales_diff'][-50:]\n\n# transform data\ndf_pred['pred_weekly_sales'] = df_pred['pred_weekly_sales_diff'] + df_varmax['weekly_sales'].shift()\ndf_pred['weekly_sales'] = df_pred['weekly_sales_diff'] + df_varmax['weekly_sales'].shift()\ndf_pred['ci_lower'] = df_pred['ci_lower'] + df_varmax['weekly_sales'].shift()\ndf_pred['ci_upper'] = df_pred['ci_upper'] + df_varmax['weekly_sales'].shift()\n\nplt.plot(df_pred['weekly_sales'], color = 'green')\nplt.plot(df_pred['pred_weekly_sales'], color = 'red')\nplt.legend(['Weekly sales', 'Predicted weekly sales'])\nplt.fill_between(df_pred.index,\n                 df_pred['ci_lower'],\n                 df_pred['ci_upper'], color='lightblue', alpha=0.5)\nplt.title('VARMAX, MSE: {:,}'.format(round(mean_squared_error(\n    df_pred['weekly_sales'], df_pred['pred_weekly_sales']), 2)))\nplt.grid()\nplt.show()","fbb9c2bc":"<a id =topic6> <\/a>\n# Triple exponential smoothing (Holt Winter\u2019s Exponential Smoothing)\n\n<div><span style=\"font-family: Arial;font-size:1.1em\">HWES is extended version of double exponential smoothing method with addition of seasonal component (equation). It means that is applicable for signals <b>with trend and seasonality<\/b>.<br><br>\n\nThere are two types of seasonality in this method, additive and multiplicative. For instance, if every month of July the value of time series $y_{t}$ is higher than it is in June <b>by X<\/b>, then the seasonality is <b>additive<\/b> in nature. Otherwise, if the value in July is higher <b>by X%<\/b> then it means that the seasonality is <b>multiplicative<\/b>. The figure of signals with different type of seasonality is shown in the previous section.<br><br>\n\nAfter all, <b>Holt Winter's<\/b> method with <b>additive trend<\/b> and <b>additive seasonality<\/b> that forecasts <b>one step<\/b> ahead is given by\n\n\\begin{equation}\n\\begin{split}\n\\text{Forecast equation}& \\quad \\hat{y}_{t+1} &= l_{t} + b_{t} + s_{t-m}\\\\\n\\text{Level equation}& \\quad l_{t} &= \\alpha (y_{t}-s_{t-m}) + (1-\\alpha)(l_{t-1} + b_{t-1})\\\\\n\\text{Trend equation}& \\quad b_{t} &= \\beta(l_{t} - l_{t-1}) + (1-\\beta)b_{t-1}\\\\\n\\text{Seasonal equation}& \\quad s_{t} &= \\gamma(y_{t} - l_{t-1} - b_{t-1}) + (1-\\gamma)s_{t-m},\n\\end{split}\n\\end{equation}\n\nwhere $l_{t}$ denotes an estimate of the signal level at time $t$, $b_{t}$ denotes an estimate of the signal trend at time $t$, $s_{t}$ denotes an estimate of the signal seasonality at time $t$, $\\alpha \\in [0, 1]$ is the smoothing parameter for the level, $\\beta \\in [0, 1]$ is the smoothing parameter for the trend and $\\gamma \\in [0, 1]$ is the smoothing parameter for the seasonality. Parameter $m$ denotes the frequency of the seasonality. For example, for quarterly data $m=4$, and for monthly data $m=12.$<br><br>\n\nNotice the differences between double and triple exponential smoothing. The seasonal equation is new and the seasonal estimate $s_{t-m}$ is added in the forecast equation. Also, in the level equation, the only difference is $(y_{t}-s_{t-m})$ which means de-seasonalizing of the value $y_{t}$. Everything else in the formula is the same. The formula which has multiplicative trend or multiplicative seasonality or longer forecasting period is slightly different and will not be mentioned here.<br><br>\n\nThis method is applicable for signals with trend and seasonality.<span><\/div>","144304a2":"<a id =topic4> <\/a>\n# Simple exponential smoothing (SES)\n\n<div><span style=\"font-family: Arial;font-size:1.1em\">This method forecasts a time series as a linear combination between the previous forecast and the previous error of that forecast. \n\n$$\\hat{y}_{t} = \\hat{y}_{t-1} + \\alpha(\\underbrace{y_{t-1} - \\hat{y}_{t-1}}_\\text{previous error}),$$\n$$\\hat{y}_{0} = y_{0},$$\n\nwhere $\\hat{y}_{t}$ is forecasted value at time $t$, $y_{t}$ real value at time $t$ and $\\alpha \\in [0,1]$ smoothing constant. For the easier calculation, this formula can be rewritten using the level updating equation as\n\n$$\\hat{y}_{t} = \\alpha y_{t-1} + (1-\\alpha)\\hat{y}_{t-1}.$$\n\nThe term \"exponential\" stands for the coefficient $(1-\\alpha)$ which becomes exponential through time if we unroll the formula above, ie.\n\n\\begin{equation}\n\\begin{split}\n\\hat{y}_{t} &= \\alpha y_{t-1} + (1-\\alpha)\\hat{y}_{t-1}\\\\\n            &= \\alpha y_{t-1} + (1-\\alpha)(\\alpha y_{t-2} + (1-\\alpha)\\hat{y}_{t-2})\\\\\n            &= \\alpha y_{t-1} + \\alpha (1-\\alpha)y_{t-2} + (1-\\alpha)^{2}\\hat{y}_{t-2}\\\\\n            &=\\alpha(y_{t-1} + (1-\\alpha)y_{t-2} + (1-\\alpha)^{2}y_{t-3} + (1-\\alpha)^{3}y_{t-4}+...+(1-\\alpha)^{t-2}y_{1}) + (1-\\alpha)^{t-1}y_{0}.\n\\end{split}\n\\end{equation}\n\nNotice that from this form that SES forecasts future value using a weighted average of all previous values in the signal and weights decay exponentially into the past. \n\nThis method is used for signals with no trend and no seasonality.<\/div>","d9aaa6b7":"<a id =topic18> <\/a>\n# Vector Autoregression Moving Average Model (VARMA)\n\n<div><span style=\"font-family: Arial;font-size:1.1em\">  This model is a combination of VAR and MA models. The simplest form VARMA(1, 1) is formulated as\n$$y_{t} = \\alpha_{1} + \\beta_{11}y_{t-1} + \\beta_{12}x_{t-1} + \\epsilon_{y, t}+\\gamma_{11} \\epsilon_{y, t-1}$$\n$$x_{t} = \\alpha_{2} + \\beta_{21}y_{t-1} + \\beta_{22}x_{t-1} + \\epsilon_{x, t}+\\gamma_{21} \\epsilon_{x, t-1},$$\nwhere $\\alpha, \\beta$ and $\\gamma$ are coefficients, and $\\epsilon$ error.<br>\nVARMA requires stationarity.<span><\/div>","55c4a136":"<a id =topic16.2> <\/a>\n# VAR example","9ae124d8":"<a id =topic16.3> <\/a>\n# VECM example","e9145233":"<div style=\"text-align:center\"> <span style=\"color:Black; font-size:3.4em;\"> Time series forecasting<span><\/div>\n<h2 align='center'>Part 1: Econometric methods<\/h2>\n<h3 align='center'>Beginner - Intermediate level<\/h3>\n<h4 align='center'>Author: Enes Zvornicanin<\/h4>","1c33b54c":"<div><span style=\"font-family: Arial;font-size:1.1em\">In the figure above, there are plots of signals (left) and their autocorrelation with ADF and KPSS statistics (right). The color of the statistics box border represents whether a signal is stationary or not and what is the confidence of that.<br><br>\n\n<span style=\"color: green\"><b>The green color<\/b><\/span> indicates stationarity with <b>p-value<\/b> $< 0.01$ (ADF) or <b>p-value<\/b> $>0.1$ (KPSS).<br>\n\n<span style=\"color: yellow;background:black\"><b>The yellow color<\/b><\/span> indicates stationarity with less confidence because it means that $0.01 <$ <b>p-value<\/b> $< 0.1$ for both ADF and KPSS.<br>\n\n<span style=\"color: red\"><b>The red color<\/b><\/span> indicates non-stationarity with <b>p-value<\/b> $> 0.1$ (ADF) or <b>p-value<\/b> $<0.01$ (KPSS).<br> \n\nAlso, the stationarity of a signal can be approximately determined from the ACF plot. If autocorrelation slowly decreases through time, it usually indicates that the signal is non-stationary.<\/div>","6bff0127":"<a id =topic18> <\/a>\n# Vector Autoregression Moving Average with Exogenous Regressors Model (VARMAX)\n\n<div><span style=\"font-family: Arial;font-size:1.1em\">VARMAX is VARMA model with exogenous variables, also called covariates. Exogenous variables represent additional factors or input features for the model, which have observations at the same time steps as the forecasted series. It means that for forecasting $y_{t+1}$, the exogenous input should have the same time-step, i.e. $x_{t+1}$. The simplest model VARMAX(1, 1) with one exogenous variable $z_{t}$ can be written as\n\n$$y_{t} = \\alpha_{1} + \\beta_{11}y_{t-1} + \\beta_{12}x_{t-1} +\\delta_{11}z_{t} + \\epsilon_{y, t}+\\gamma_{11} \\epsilon_{y, t-1}$$\n$$x_{t} = \\alpha_{2} + \\beta_{21}y_{t-1} + \\beta_{22}x_{t-1} +\\delta_{21}z_{t} + \\epsilon_{x, t}+\\gamma_{21} \\epsilon_{x, t-1},$$\n\nwhere $\\alpha, \\beta, \\gamma$ and $\\delta$ are coefficients and $\\epsilon$ $\\sim$ $\\mathcal{N}$$(0,\\Omega)$  are errors.\n\nVARMAX requires stationarity.<span><\/div>","4d207021":"# Introduction\n\n<span style=\"font-family: Arial;font-size:1.1em\"> The purpose of this notebook is to provide a simple and clear theoretical explanation and minimal working example of the several models for time series forecasting. <u>Note: The main goal is not to have the best possible prediction on this particular dataset but to list and describe some of the algorithms for times series forecasting.<\/u> Thus, in this notebook, using some of the methods to predict monthly sales may not make much sense and they are provided as an example.\n\n<span style=\"font-family: Arial;font-size:1.1em\">Prior knowledge: Pandas and basics about time series. Also, some great notebooks as [Topic 9. Part 1. Time series analysis in Python](https:\/\/www.kaggle.com\/kashnitsky\/topic-9-part-1-time-series-analysis-in-python) by Dmitriy Sergeyev and [Time Series I || An Introductory Start](https:\/\/www.kaggle.com\/janiobachmann\/time-series-i-an-introductory-start?scriptVersionId=53165252) by Janio Martinez Bachmann may provide additional explanation of some terms.","85b273a2":"<a id =topic15> <\/a> \n# Seasonal Autoregressive Integrated Moving Average Model with Exogenous Regressors(SARIMAX)\n\n<div><span style=\"font-family: Arial;font-size:1.1em\">SARIMAX model is an extension of SARIMA model which simply adds exogenous variables, also called covariates. Exogenous variables represent additional factors or input features for the model, which have observations at the same time steps as the forecasted series. It means that for forecasting $y_{t+1}$, the exogenous input should have the same time-step, i.e. $x_{t+1}$. Formally, SARIMAX(1,1,1)(1,1,1)4 with one exogenous time series $x_{t}$ can be written as\n\n$$y_{t} = \\beta_{0} + \\beta_{1}x_{t} + u_{t}$$\n$$\\underbrace{(1-\\phi_{1}B)}_\\text{p}\\underbrace{(1-\\Phi_{1}B^{4})}_\\text{P}\\underbrace{(1-B)}_\\text{d}\\underbrace{(1-B^{4})}_\\text{D}u_{t}=\\underbrace{(1+\\theta_{1}B)}_\\text{q}\\underbrace{(1+\\Theta_{1}B^{4})}_\\text{Q}\\epsilon_{t}.$$\n\nNotice that the first equation is just a linear regression and the second equation describes SARIMA process. \n\nIn this example, as the exogenous variable will be used standard deviation of the last 3 weekly sales.<span><\/div>","f8abe245":"## References:\n* Rob J Hyndman and George Athanasopoulos, Forecasting: Principles & Practice, https:\/\/otexts.com\/fpp2\/\n* ritvikmath, https:\/\/www.youtube.com\/channel\/UCUcpVoi5KkJmnE3bvEhHR0Q\n* Statistics How To, https:\/\/www.statisticshowto.com\/\n* https:\/\/stats.stackexchange.com\/questions\/281666\/how-does-acf-pacf-identify-the-order-of-ma-and-ar-terms\n* https:\/\/stats.stackexchange.com\/questions\/394796\/should-my-time-series-be-stationary-to-use-arima-model\n* auto_arima documentation, https:\/\/alkaline-ml.com\/pmdarima\/modules\/generated\/pmdarima.arima.auto_arima.html\n* https:\/\/www.statsmodels.org\/dev\/examples\/notebooks\/generated\/statespace_sarimax_stata.html\n* Rob J Hyndman, The ARIMAX model muddle, https:\/\/robjhyndman.com\/hyndsight\/arimax\/\n* Jeffrey A. Parker, Vector Autoregression and Vector Error-Correction Models, https:\/\/www.reed.edu\/economics\/parker\/s14\/312\/tschapters\/S13_Ch_5.pdf\n* Hany Abdel-Latif, Lecture 5: VAR and VEC Models, https:\/\/www.youtube.com\/watch?v=XK3cEJw93jA\n* https:\/\/stats.stackexchange.com\/questions\/148994\/var-or-vecm-for-a-mix-of-stationary-and-nonstationary-variables\n* Econometrics with R, Cointegration, https:\/\/www.econometrics-with-r.org\/16-3-cointegration.html\n* Yury Kashnitsky, Topic 9. Part 1. Time series analysis in Python, https:\/\/www.kaggle.com\/kashnitsky\/topic-9-part-1-time-series-analysis-in-python\n* Wikipedia, Exponential smoothing, https:\/\/en.wikipedia.org\/wiki\/Exponential_smoothing\n* Jason Brownlee, How to Decompose Time Series Data into Trend and Seasonality, https:\/\/machinelearningmastery.com\/decompose-time-series-data-trend-seasonality\/\n* Galit Shmueli, Smoothing 5: Holt's exponential smoothing, https:\/\/www.youtube.com\/watch?v=DUyZl-abnNM\n* Galit Shmueli, Smoothing 6: Winter's exponential smoothing, https:\/\/www.youtube.com\/watch?v=mrLiC1biciY","8c1cb32a":"<a id =topic12> <\/a>\n# Autoregressive Moving Average Model (ARMA)\n\n<div><span style=\"font-family: Arial;font-size:1.1em\">ARMA(p,q) is a combination of AR(p) and MA(q) models. For example, ARMA(3,3) of signal $S_{t}$ can be formulated as<br><br>\n$$S_{t} =\\beta_{0} + \\beta_{1}S_{t-1} + \\beta_{2}S_{t-2} + \\beta_{3}S_{t-3} + \\epsilon_{t} + \\gamma_{1}\\epsilon_{t-1}+\\gamma_{2}\\epsilon_{t-2}+\\gamma_{3}\\epsilon_{t-3},$$<br>\nwhere $\\beta, \\gamma$ are coefficients and $\\epsilon$ error.<br><br>\nARMA model requires stationarity.<span><\/div>","948b055b":"<a id =topic3> <\/a> \n# Weighted average\n\n<div><span style=\"font-family: Arial;font-size:1.1em\">This method is an improvement of the moving average with the addition of weights that multiply lagged values. \n\n$$y_{t} = w_{1}y_{t-1} + w_{2}y_{t-2} +...+w_{k}y_{t-k},\\quad t\\geq k,$$\n$$w_{1} + w_{2} + ... + w_{k} = \\sum_{i=1}^{k}w_{i} = 1,$$\n\nwhere $y_{t}$ is a signal and $w_{i}$ are weights. Generally, the form of weighted average is\n\n$$y_{t} = \\frac{w_{1}y_{t-1} + w_{2}y_{t-2} +...+w_{k}y_{t-k}}{w_{1} + w_{2} + ... + w_{k}}, \\quad t\\geq k.$$\n\nbut with the simple transformation, it can be represented as at the top.<span><\/div>\n<div><span style=\"font-family: Arial;font-size:0.8em\">\n$$y_{t} = \\frac{\\sum_{i=1}^{k}w_{i}(\\frac{\nw_{1}y_{t-1} + w_{2}y_{t-2} +...+w_{k}y_{t-k}}\n{\\sum_{j=1}^{k}w_{j}})\n}{\\sum_{i=1}^{k}w_{i}} = \n\\frac{1(\n    \\frac{w_{1}y_{t-1}}{\\sum_{j=1}^{k}w_{j}} + \n    \\frac{w_{2}y_{t-2}}{\\sum_{j=1}^{k}w_{j}} +...+\n    \\frac{w_{k}y_{t-k}}{\\sum_{j=1}^{k}w_{j}}\n    )\n}{1} = w^{'}_{1}y_{t-1} + w^{'}_{2}y_{t-2} +...+w^{'}_{k}y_{t-k}, \\quad t\\geq k,$$<span><\/div>\n<div><span style=\"font-family: Arial;font-size:1.1em\">where\n$$w^{'}_{i} = \\frac{w_{i}}{\\sum_{j=1}^{k}w_{j}}, \\quad \\overline{i=1, k}$$\nand \n$$w^{'}_{1} + w^{'}_{2} + ... + w^{'}_{k} = \n\\frac{w_{1}}{\\sum_{j=1}^{k}w_{j}}+\\frac{w_{2}}{\\sum_{j=1}^{k}w_{j}}+...+\\frac{w_{k}}{\\sum_{j=1}^{k}w_{j}} =\n\\frac{\\sum_{i=1}^{k}w_{i}}{\\sum_{j=1}^{k}w_{j}} = 1.$$<span><\/div>","ef5d2d6b":"<a id =topic8> <\/a> \n# Partial Autocorrelation (PACF)\n\n<div><span style=\"font-family: Arial;font-size:1.1em\"><b>\"Direct\" correlation<\/b> between time series and a lagged version of itself.<br><br> \n\nFor example, let's consider ACF of a signal S with a lag of 2. When we calculate correlation between $S_{t-2}$ and $S_{t}$, it includes both direct influence of $S_{t-2}$ to $S_{t}$ but also indirect through $S_{t-1}$ (all the arrows from the image). What we really want to calculate with PACF is the only direct effect of $S_{t-2}$ to $S_{t}$ without any undirect components.<br><br>\n\nPACF of a signal $S$ with a lag 2 is calculated from regression model<span><\/div>\n<span style=\"font-family: Arial;font-size:1.1em\">$$S_{t} = \\phi_{21}S_{t-1} + \\phi_{22}S_{t-2} + \\epsilon_{t},$$   \n<span style=\"font-family: Arial;font-size:1.1em\">using the coefficient $\\phi_{22}$ as a value for PACF.\n\n![im1.png](attachment:8dbe97ce-0472-46cd-ad63-f00d0ea07c4b.png)","2281eb68":"<a id =topic7> <\/a>\n# Autocorrelation (ACF)\n\n\n<div><span style=\"font-family: Arial;font-size:1.1em\">Just a <b>simple Pearson correlation<\/b> between time series and a lagged version of itself.<br><br>\nBlue bars on an ACF plot below are the error bands and anything within these bars is not statistically significant. It means that correlation values outside of this area are very likely a correlation and not a statistical fluke. The confidence interval is set to 95% by default.<br><br>\nNotice that for a lag 0, ACF is always 1.<span><\/div>","f591b74f":"<a id =topic14> <\/a>\n# Seasonal Autoregressive Integrated Moving Average Model (SARIMA)\n\n\n<div><span style=\"font-family: Arial;font-size:1.1em\">SARIMA(p, d, q)(P, D, Q)m is an extenson of ARIMA(p, d, q). New parameters P, D and Q stands for seasonal autoregressive, integrated and moving average order respectively. Parameter $m$ means how many time steps are there in a single seasonal period.<br><br>\n\nTo understand the formula behind ARIMA model, we need first to understand the backward shift operator $B$ defined as $$By_{t} = y_{t-1}.$$ Analogously, $$B(By_{t}) = B^{2}y_{t} = y_{t-2}$$\nThis operator is just convenient way of describing the process of differencing because the first difference can be written as $$y^{'}_{t} = y_{t} - y_{t-1} = y_{t} - By_{t} = (1-B)y_{t}$$ and second difference as $$y^{\"}_{t}= ( y_{t} - y_{t-1})^{'} = y^{'}_{t} - y^{'}_{t-1} = y_{t} - y_{t-1} - (y_{t-1} - y_{t-2}) = y_{t} - 2y_{t-1} + y_{t-2} = (1-2B+B^{2})y_{2} = (1-B)^{2}y_{t}.$$\nIn general, d-order difference can be formulated as $$y^{(d)}_{t} = (1-B)^{(d)}y_{t}.$$\nFinally, SARIMA(1,1,1)(1,1,1)4 can be formulated as\n$$\\underbrace{(1-\\phi_{1}B)}_\\text{p}\\underbrace{(1-\\Phi_{1}B^{4})}_\\text{P}\\underbrace{(1-B)}_\\text{d}\\underbrace{(1-B^{4})}_\\text{D}y_{t}=c + \\underbrace{(1+\\theta_{1}B)}_\\text{q}\\underbrace{(1+\\Theta_{1}B^{4})}_\\text{Q}\\epsilon_{t}$$ where $\\phi, \\Phi, \\theta, \\Theta, c$ are coefficients and $B$ is backward shift operator. Notice that all brackets represent a version of operator's B polynomial of first difference $(1-B)$ because $p=d=q=P=D=Q=1$ and $B^{4}$ in some of them is due to $m=4.$ From that, if the one the parameters $p, d, q, P, D, Q$ is different than 1, then we need to use a corresponding polynomial. For example if $p=2$, then from  $(1-B)^{2} = (1-2B+B^{2})$ matching polynomial is $(1-\\phi_{1}B + \\phi_{2}B^{2}$). Similar, for $P=2$, matching polynomial is $(1-\\Phi_{1}B^{4} + \\Phi_{2}B^{8}$) (if $m=4$).<br><br>\n\nAs for AR and MA, seasonal part of SARIMA model can be seen from PACF and ACF plot.\nFor example, SARIMA(0, 0, 0)(0, 0, 1)4 model will show:\n<ul>\n<li>a spike at lag 4 in the ACF but no other significant spikes,<\/li>\n<li>exponential decay in the seasonal lags of the PACF (at lags 4, 8, 12, ...).<\/li>    \n<\/ul>\n\nFor SARIMA(0, 0, 0)(1, 0, 0)4 it will be opposite:\n<ul>\n<li>exponential decay in the seasonal lags of the ACF,<\/li>\n<li>a single significant spike at lag 4 in the PACF.<\/li>\n<\/ul>\n\nHowever, maybe the best option is to apply grid search using predefined set of SARIMA hyperparameters or using <i>auto_arima<\/i> which will select the best model based on AIC.<span><\/div> ","4785a7ce":"<a id =topic2> <\/a> \n# Simple rolling mean (simple moving average)\n\n<span style=\"font-family: Arial;font-size:1.1em\">For a signal $y_{t}$, a simple rolling mean is just a mean over the last $k$ entries.\n\n<span style=\"font-family: Arial;font-size:1.1em\">$$y_{t} = \\frac{y_{t-1} + y_{t-2} +...+y_{t-k}}{k}, t\\geq k.$$\n\n<span style=\"font-family: Arial;font-size:1.1em\">Generally, the assumption of this method is stationarity of the signal but for the sake of example, this method will be applied both on the stationary and non-stationary signal. Also, with the short window (k), this method can work well even if the signal is non-stationary.","684e4712":"# Data preparation","1f18aa8d":"<a id =topic10.1> <\/a> \n<div><span style=\"font-family: Arial;font-size:1.1em\"> From the summary above in a top section can be seen some information about model and training data, as well as measures of a model such as Akaike Information Criteria (AIC) and Bayesian Information Criteria (BIC). The lower the value of these criteria, the better the model.<br><br>\n$$AIC = 2k - 2l$$ and\n$$BIC = k\\log(n) - 2l,$$ where $l$ is a log-likelihood (how strong the model is in fitting the data), $k$ is a number of parameters (more parameters - more complicated model and AR(p) has p+1 parameters) and $n$ is a number of samples used for fitting (size of the training set).<br><br>\n\nIn the middle section, we can see a table with information about coefficients of the AR(3) model. The most important column is a <b>p-value P>|Z|<\/b>. This column shows the importance of each lag or each coefficient in the model and the lower the <b>p-value<\/b>, the more significant is coefficient in our prediction. Ideally, we want that <b>p-value<\/b> is less than 0.05.<span><\/div>","0e8848be":"<a id =topic5> <\/a>\n# Double exponential smoothing (Holt's exponential smoothing)\n\n<div><span style=\"font-family: Arial;font-size:1.1em\">Generally, we can think that time series is comprising of two parts:<br>\n<ul>\n<li><b>Systematic<\/b>- components that can be described and modeled.<\/li>\n<li><b>Non-Systematic<\/b>- components that cannot be directly modeled (noise).<\/li>\n<\/ul>    \nSystematic part consist of three components:\n<ul>\n<li><b>Level<\/b>- the average value in the series.<\/li>\n<li><b>Trend<\/b>- the increasing or decreasing value in the series.<\/li>\n<li><b>Seasonality<\/b>- the repeating short-term cycle in the series.<\/li>\n<\/ul>\n\nAlso, we can have an <b>additive model<\/b> where\n\n$$y_{t} = \\text{level} + \\text{trend} + \\text{seasonality} + \\text{noise}$$\n\nor <b>multiplicative model<\/b> where\n\n$$y_{t} = \\text{level} \\cdot \\text{trend} \\cdot \\text{seasonality} \\cdot \\text{noise}.$$\n\nThe distinction between signals which consisted of different components can be seen from the image below.<\/span><\/div>\n\n![hwes_signal.jpg](attachment:fd75cf27-00eb-49be-b513-29dafaff897e.jpg)\n<div><span style=\"font-family: Arial;font-size:1.1em\">$$\\text{Source: Galit Shmueli, Smoothing 6: Winter's exponential smoothing, youtube.com}$$<br><br>\n\n<b>Holt's exponential smoothing<\/b> is an extended version of simple exponential smoothing which allows the forecasting of data with a trend. This method, with <b>the additive trend<\/b>, estimates two smoothing equations (one for the level and one for the trend)\n\n\\begin{equation}\n\\begin{split}\n\\text{Forecast equation}& \\quad \\hat{y}_{t} &= l_{t-1} + b_{t-1}\\\\\n\\text{Level equation}& \\quad l_{t-1} &= \\alpha y_{t-1} + (1-\\alpha)(l_{t-2} + b_{t-2})\\\\\n\\text{Trend equation}& \\quad b_{t-1} &= \\beta(l_{t-1} - l_{t-2}) + (1-\\beta)b_{t-2},\n\\end{split}\n\\end{equation}\n\nwhere $l_{t-1}$ denotes an estimate of the level of the series at time $t-1$, $b_{t-1}$ denotes an estimate of the trend of the series at time $t$, $\\alpha \\in [0, 1]$ is the smoothing parameter for the level and $\\beta \\in [0, 1]$ is the smoothing parameter for the trend.<br><br>\n\nThe level equation is similar to the SES equation with the difference in adjusting previous level $l_{t-2}$ by adding trend $b_{t-2}$. The trend equation updates previous trend $b_{t-2}$ using the difference between the most recent level values.<br><br>\n\nIn the case of <b>the multiplicative trend<\/b>, the only difference is in the forecasting equation where the multiplication sign stands instead of the plus sign.\n$$\\text{Forecast equation} \\quad \\hat{y}_{t} = l_{t-1} \\cdot b_{t-1}$$\n\nNote: Presented methods forecast only one step ahead. If we need to forecast more than one step, then the forecast equations have a slightly different form\n\\begin{equation}\n\\begin{split}\n\\text{Additive forecast equation}& \\quad \\hat{y}_{t+k} &= l_{t} + kb_{t}\\\\\n\\text{Multiplicative forecast equation}& \\quad \\hat{y}_{t+k} &= l_{t} \\cdot (b_{t})^{k},\n\\end{split}\n\\end{equation}\n\nwhere parameter $k$ denotes the number of forecasting steps.\n<br><br>\nThis method is used for signals with trend and without seasonality. However, even if the signal contains seasonality, SEM can be applied after de-seasonalizing (for example, removing the seasonality by differencing with lag k).<span><\/div>","e3d6eed0":"<div style=\"font-family: Arial;font-size:1.3em;background:rgb(255, 204, 0);color:black\">\nFeel free to contact me by commenting or message me in private if you need a more detailed explanation of some part of this work. <br><b>If you like this notebook and found it useful, please give an upvote.<\/b> \ud83d\ude4f \ud83d\udd1d<br>Also, feel free to share your criticisms or point me for my mistakes. It would help me a lot to improve my knowledge. <br>\nAfter all, I have plans to write more about this theme so feel free to share your ideas in the comment section.\n<\/div>","f4f0427d":"<a id =topic10> <\/a> \n# Autoregressive Model (AR)\n\n<div><span style=\"font-family: Arial;font-size:1.1em\">A model where signal $S_{t}$ depends only on its own past values. For example, AR(3) is a model that depends on 3 of its past values and can be written as $$ S_{t} =\\beta_{0} + \\beta_{1}S_{t-1} + \\beta_{2}S_{t-2} + \\beta_{3}S_{t-3} + \\epsilon_{t},  $$\nwhere $\\beta_{0}, \\beta_{1}, \\beta_{2}, \\beta_{3}$ are coefficients and $\\epsilon_{t}$ is error. \n\nThe order p for AR(p) model can be selected based on significant spikes from the PACF plot. One more indication of AR process is that ACF plot decays more slowly.\n\nAR model requires stationarity.<span><\/div>","8316a01c":"<a id =topic11> <\/a>\n# Moving Average Model (MA)\n\n<div><span style=\"font-family: Arial;font-size:1.1em\"> MA is the model which depends on the past error terms. For example, MA(3) for a signal $S_{t}$ can be formulated as <br><br>\n$$S_{t} = \\mu + \\epsilon_{t} + \\gamma_{1}\\epsilon_{t-1}+\\gamma_{2}\\epsilon_{t-2}+\\gamma_{3}\\epsilon_{t-3}, $$\n<br>\nwhere $\\mu$ is the mean of a series, $\\gamma_{1}, \\gamma_{2}, \\gamma_{3}$ are coefficients and $\\epsilon_{t}, \\epsilon_{t-1}, \\epsilon_{t-2}, \\epsilon_{t-3}$ are errors which have a normal distribution with mean 0 and std 1 (sometimes called white noise).\n\nThe order q for model MA(q) can be selected from ACF if this plot has a sharp cut-off after lag q. One more indication of MA process is that PACF plot decays more slowly.<br><br>  \n\nMA model requires stationarity.<span><\/div>","2723e760":"<a id =topic13> <\/a>\n# Autoregressive Integrated Moving Average Model (ARIMA)\n\n<div><span style=\"font-family: Arial;font-size:1.1em\">ARIMA(p,d,q) model is the ARMA(p,q) model with additional functionality I(d) that makes time series stationary. I(d) defines an order of integration. \nFor example, ARIMA(3,1,3) of signal $S_{t}$ can be formulated as<br><br> \n$$S^{'}_{t} =\\beta_{0} + \\beta_{1}S^{'}_{t-1} + \\beta_{2}S^{'}_{t-2} + \\beta_{3}S^{'}_{t-3} + \\epsilon_{t} + \\gamma_{1}\\epsilon_{t-1}+\\gamma_{2}\\epsilon_{t-2}+\\gamma_{3}\\epsilon_{t-3}, $$ where $$S^{'}_{t} = S_{t} - S_{t-1}$$\nand $\\beta, \\gamma$ are coefficients and $\\epsilon$ error.<br>\nFrom the last equation, $$S_{t} = S^{'}_{t} + S_{t-1} = S^{'}_{t}+S^{'}_{t-1} + S_{t-2}=...=\\sum^{t-1}_{i=1}S^{'}_{t-i}+S_{0}.$$\n\n<b><u>ARIMA can handle 2 types of non-stationarity<\/u><\/b>: hidden trend, such as linear and polynomial, and unit roots. Differencing removes any type of polynomial trend. The higher-degree polynomial is, the more differencing needed.<br><br>\n\nIf the coefficient which multiplies lagged value in AR process is equal to 1, then the time series has <b>a unit root<\/b>. For example, for a given AR(1) signal $$S_{t} =\\beta_{0} + \\beta_{1}S_{t-1}+\\epsilon_{t},$$ if $\\beta_{1}=1$ then the signal has a unit root. This signal is a typical case of <b>random walk<\/b>. In addition, if coefficient $\\beta_{0}$ is different than zero, signal $S_{t}$ is called <b>random walk with drift<\/b>. To find whether time series has a unit root or not, we can use Dickey\u2013Fuller test (not ADF).<br><br> \n\nAlso, differencing removes 1 unit root per application. If there are 2 unit roots, we will need differencing twice. 3 unit roots - three times, etc. <span><\/div>","6d7e5b39":"<a id =topic9> <\/a>\n# Stationarity\n\n<div><span style=\"font-family: Arial;font-size:1.1em\">When it comes to time series forecasting, the stationarity of a time series is one of the most important conditions that the majority of algorithms require. Briefly, time series $S_{t}$ is stationary (weak stationarity) if these conditions are met:<br><br>\n1. $S_{t}$ has a constant mean,<br>\n2. $S_{t}$ has a constant standard deviation,<br>\n3. There is no seasonality in $S_{t}.$ If $S_{t}$ has a repeating pattern within a year, then it has seasonality.<br><br>\n\nThe stationarity of the signal can be checked visually (approximation) or using some statistical hypothesis for a more precise answer. For that purpose, we will use 2 tests:\n<a id =topic9.1> <\/a>\n* **Augmented Dickey-Fuller Test (ADF)**\n\nNull hypothesis: The signal is non-stationary.\n\nIf the p-value of this test is less than a critical value (for example 0.05), then we reject a null hypothesis. It means that signal is stationary\n<a id =topic9.2> <\/a>\n* **Kwiatkowski-Phillips-Schmidt-Shin Test (KPSS)**\n\nNull hypothesis: The signal is stationary.\n\nIf the p-value is less than 0.05, then we reject a null hypothesis. It means that signal is non-stationary<br><br>\n\nIf signal $S_{t}$ is non-stationary, we can <b>convert them into stationary<\/b> signal $T_{t}$ by differencing $$T_{t} = S_{t} - S_{t-1},$$ or calculating percent of change $$T_{t} =\\frac{S_{t} - S_{t-1}}{S_{t-1}}.$$\n\nNote: After these transformations, signal $T_{t}$ won't always be stationary. It is rare but can happen. In that case, if $T_{t}$ stays non-stationary, you can apply the same transformation to signal $T_{t}$. \n\nAlso, it is worth mentioning that the stationary time series has an order of integration 0 (I(0)). If the time series is non-stationary and if after one difference becomes stationary, then it has an order of integration 1 (I(1)). Thus, non-stationary time series that can be transformed into stationary by $d$ successive differences, has an order of integration d (I(d)).","1e937cfe":"# Content\n\n* [Autocorrelation (ACF)](#topic7)\n* [Partial Autocorrelation (PACF)](#topic8)\n* [Stationarity](#topic9)\n  * [Augmented Dickey-Fuller Test (ADF)](#topic9.1)\n  * [Kwiatkowski-Phillips-Schmidt-Shin Test (KPSS)](#topic9.2)\n* [Naive model](#topic1)\n* [Simple rolling mean (simple moving average)](#topic2)\n* [Weighted average](#topic3)\n* [Simple exponential smoothing (SES)](#topic4)\n* [Double exponential smoothing (Holt's exponential smoothing)](#topic5)\n* [Triple exponential smoothing (Holt Winter\u2019s Exponential Smoothing)](#topic6)\n* [Autoregressive Model (AR)](#topic10)\n  * [Akaike Information Critera (AIC) and Bayesian Information Critera (BIC)](#topic10.1)\n* [Moving Average Model (MA)](#topic11)\n* [Autoregressive Moving Average Model (ARMA)](#topic12)\n* [Autoregressive Integrated Moving Average Model (ARIMA)](#topic13)\n* [Seasonal Autoregressive Integrated Moving Average Model (SARIMA)](#topic14)\n* [Seasonal Autoregressive Integrated Moving Average Model with Exogenous Regressors (SARIMAX)](#topic15)\n* [Vector Autoregression Model (VAR)](#topic16)\n  * [Cointegration](#topic16.1)\n  * [Vector Error Correction Model (VECM)](#topic17)\n  * [VAR example](#topic16.2)\n  * [VECM example](#topic16.3)\n* [Vector Autoregression Moving Average Model (VARMA)](#topic18)\n* [Vector Autoregression Moving Average with Exogenous Regressors Model (VARMAX)](#topic19)","bb42ce3f":"<a id =topic1> <\/a>\n# Naive forecast model \n\n<span style=\"font-family: Arial;font-size:1.1em\">The next step value is the same as previous\n\n<span style=\"font-family: Arial;font-size:1.1em\">$$Y_{t} = Y_{t-1}.$$","12d30798":"Rolling mean applied on the difference of weekly sales.","7fc2b2a2":"<a id =topic17> <\/a>\n# Vector Error Correction Model (VECM)\n\n<div><span style=\"font-family: Arial;font-size:1.1em\"> As explained in the previous section, if signals $x_{t}$ and $y_{t}$ are <b>cointegrated<\/b>, instead of VAR model, it is recommended to use VEC model.<br><br> \n\nUsually, using non-stationary time series in regression models can lead to a high $r^{2}$ value and statistically significant regression coefficients. These results are very likely misleading or spurious. It is because probably there is no real relationship between them and the only common thing is that they are growing (declining) over time. Everyone can test that by computing the correlation between two random walks.\n\nThus, from a simple regression $$y_{t} = \\beta_{0} + \\beta_{1}x_{t} + \\epsilon_{t},$$ where both signals $x_{t}$ and $y_{t}$ are non-stationary, the error $\\epsilon_{t}$ is also non-stationary as a linear combination of two non-stationary signals $$\\epsilon_{t} = y_{t}-\\beta_{0}-\\beta_{1}x_{t}.$$\n\nThis violates some assumptions about errors in the model, as well as the assumption that $\\epsilon_{t} \\sim \\mathcal{N}(1,0)$ (errors have a normal distribution with mean 0 and std 1).\n<br><br>\nHowever, if we found that $\\epsilon_{t}$ is a stationary signal (using any stationarity test), then signals $y_{t}$ and $x_{t}$ are cointegrated and the regression mentioned above is not spurious. Also, it means that these signals have a long-run relationship between them.\n<br><br>\nEven if two series are cointegrated, it is still likely that $\\epsilon_{t}$ will be serially correlated. To capture the serial correlation we can think of fitting the following specification $$y_{t} = \\beta_{0} + \\beta_{1}x_{t} + \\alpha_{2}y_{t-1}+ \\beta_{2}x_{t-1} + \\epsilon_{t}$$ or however many lags are required. Also, we can rearrange this model to make sure that we have only stationary variables there. In the previous expression can be added $-y_{t-1}$ on both sides as well as $\\beta_{1}x_{t-1} - \\beta_{1}x_{t-1}$ on the right side. After rearranging, the formula will look like $$y_{t} - y_{t-1} = \\beta_{0} + \\beta_{1}(x_{t}-x_{t-1}) + (\\alpha_{2}-1)y_{t-1}+ x_{t-1}(\\beta_{1}+\\beta_{2}) + \\epsilon_{t}$$ or\n$$\\Delta y_{t} = \\beta_{1}\\Delta x_{t} + \\psi(y_{t-1} - \\pi_{1} -\\pi_{2}x_{t-1}) + \\epsilon_{t},$$\nwhere $\\psi = (\\alpha_{2}-1)$, $\\pi_{1} = \\frac{\\beta_{0}}{1-\\alpha_{2}}$ and $\\pi_{2} = \\frac{\\beta_{1}+\\beta_{2}}{1-\\alpha_{2}}$. This model is known as the error correction model. The term $\\Delta x_{t}$ represents the model's short-run dynamics. The term $\\psi(y_{t-1} - \\pi_{1} -\\pi_{2}x_{t-1})$ is known as the error-correction mechanism and $\\psi$ is the error-correction parameter that measures how $y_{t}$ and $x_{t}$ react to deviations from long-run equilibrium. Also, notice that the expression $y_{t-1} - \\pi_{1} -\\pi_{2}x_{t-1}$ is basically the error from the model $y_{t-1} = \\pi_{1} + \\pi_{2}x_{t-1} + \\epsilon_{t-1}.$\n<br><br>\nAfter all, the VECM from the simple VAR(1) model might look like\n$$\\Delta y_{t} = \\alpha_{1} + \\beta_{11}\\Delta y_{t-1} + \\beta_{12}\\Delta x_{t-1} + \\psi_{y}(y_{t-1} - \\pi_{1} -\\pi_{2}x_{t-1}) + \\epsilon_{y, t}$$\n$$\\Delta x_{t} = \\alpha_{2} + \\beta_{21}\\Delta y_{t-1} + \\beta_{22}\\Delta x_{t-1} + \\psi_{x}(y_{t-1} - \\pi_{1} -\\pi_{2}x_{t-1}) +\\epsilon_{x, t}.$$<span><\/div>","2de74838":"<a id =topic16> <\/a>\n# Vector Autoregression Model (VAR)\n\n<div><span style=\"font-family: Arial;font-size:1.1em\">Similar to the AR model but using multiple time series. As opposite to previous models which are utilizing univariate time series, where the signal has only a single time-dependent variable, the VAR model using <b>multivariate time series<\/b>. Basically, it means that each signal depends not only on its past but also on the previous values of some other signals. The simplest model VAR(1) with two time series $y_{t}$ and $x_{t}$ can be written as\n$$y_{t} = \\alpha_{1} + \\beta_{11}y_{t-1} + \\beta_{12}x_{t-1} + \\epsilon_{y, t}$$\n$$x_{t} = \\alpha_{2} + \\beta_{21}y_{t-1} + \\beta_{22}x_{t-1} + \\epsilon_{x, t}$$\nwhere $\\alpha$ and $\\beta$ are coefficients, and $\\epsilon$ is error.<br>\n<a id =topic16.1> <\/a>\nVAR model requires stationarity of signals. However, when signals $x_{t}$ and $y_{t}$ are I(1) (non-stationary with the order of integration 1) and if there is a $\\theta$ such that $(y_{t} - \\theta x_{t})$ is stationary, $x_{t}$ and $y_{t}$ are <b>cointegrated<\/b> and a VEC model can be used.<br>\n\n<b>Cointegration<\/b> can be tested using augmented Engle-Granger cointegration test where the null hypothesis assumes that there is no cointegration. It means that if the p-value is less than 0.05, then we can reject a null hypothesis and say that signals are cointegrated. The example provided below.<span><\/div>"}}