{"cell_type":{"4f137184":"code","9ac25927":"code","271222ff":"code","8f9706da":"code","156b1fec":"code","c193ee9e":"code","c42fc9b4":"code","7a1c5dc6":"code","464a29cb":"code","219eaef1":"code","350c8c9b":"code","6228b3f5":"code","fd52345e":"code","9957453e":"code","eb02a5b0":"code","f471ba14":"code","0bf36492":"code","5e019305":"code","aba7cd93":"code","415cda98":"code","6d865723":"code","1ec644cc":"code","9dde87b5":"code","1b6e3a7d":"code","3f83b225":"markdown"},"source":{"4f137184":"import pandas as pd\nimport numpy as np\nimport os\nimport re\nfrom tqdm import tqdm\n\nfrom sklearn.preprocessing import MultiLabelBinarizer\nfrom sklearn.preprocessing import OneHotEncoder","9ac25927":"df_anime = pd.read_csv(\"..\/input\/anime-recommendations-database\/anime.csv\")\ndf_rating = pd.read_csv(\"..\/input\/anime-recommendations-database\/rating.csv\")","271222ff":"df_anime.head()","8f9706da":"df_rating.head()","156b1fec":"def get_anime_feature_map(df_anime):\n    ## cleaning names\n    # df_anime['name'] = df_anime['name'].apply(lambda x: re.sub(r'[^A-Za-z0-9 ]+', '', re.sub(r'&#(\\d)+;', '', x)))\n    # df_anime = df_anime[df_anime['name'] != '']\n    \n    # ## Imputing episodes based on type of anime(mean value)\n    tmp = df_anime[df_anime.episodes != 'Unknown'][['type', 'episodes']]\n    tmp['episodes'] = tmp['episodes'].astype(int)\n    tmp = tmp.groupby('type').mean().to_dict()['episodes']\n    df_anime['episodes'] = df_anime.apply(lambda x: tmp.get(x['type'], 1) if (x['episodes'] == 'Unknown') else x['episodes'], axis=1)\n    df_anime['episodes'] = df_anime['episodes'].astype(int)\n    \n    ## Imputing rating with the mean rating\n    df_anime['rating'] = df_anime['rating'].fillna(df_anime['rating'].mean())\n    \n    #Imputing genre with extra '' class\n    df_anime['genre'] = df_anime['genre'].apply(lambda x: [g.strip() for g in (x.split(',') if (type(x) == str) else [''])])\n    mat = df_anime.to_numpy()\n    genres = mat[:,2]\n    \n    mlb = MultiLabelBinarizer()\n    mlb.fit(genres)\n    \n    ## Imputing type column with extra '' class\n    df_anime['type'] = df_anime['type'].fillna('')\n    \n    ohe = OneHotEncoder(sparse=False)\n    ohe.fit(np.array(list(set(df_anime['type']))).reshape(-1, 1))\n    \n    df_anime['genre'] = df_anime['genre'].apply(lambda x: mlb.transform([x])[0])\n    df_anime['type'] = df_anime['type'].apply(lambda x: ohe.transform([[x]])[0])\n    \n    ## normalize ratings and members\n    df_anime['rating'] = (df_anime['rating'] - df_anime['rating'].min())\/(df_anime['rating'].max()-df_anime['rating'].min())\n    df_anime['members'] = (df_anime['members'] - df_anime['members'].min())\/(df_anime['members'].max()-df_anime['members'].min())\n    \n    ## generating feature_map\n    anime_feature_map = {}\n    for idx, row in tqdm(df_anime.iterrows()):\n        anime_feature_map[row[\"anime_id\"]] = list(row[\"genre\"]) + list(row[\"type\"]) + [row[\"rating\"], row[\"members\"]]\n        \n    return anime_feature_map, mlb, ohe","c193ee9e":"anime_feature_map, mlb, ohe = get_anime_feature_map(df_anime)","c42fc9b4":"# df_rating.head()\ndf_rating['anime_features'] = df_rating['anime_id'].apply(lambda x: anime_feature_map.get(x))\ndf_rating = df_rating[~df_rating.anime_features.isna()]\ndf_rating = df_rating[df_rating['rating'] != -1]","7a1c5dc6":"df_rating.head()","464a29cb":"user_count = df_rating.groupby('user_id').count()['rating']\ndf_rating = df_rating[df_rating['user_id'].apply(lambda x: 5 <= user_count[x] <= 100)]","219eaef1":"user_idx_map = {u: e for e, u in enumerate(df_rating.user_id.unique())}\nanime_idx_map = {i: e for e, i in enumerate(df_rating.anime_id.unique())}","350c8c9b":"df_rating[\"user_idx\"] = df_rating[\"user_id\"].apply(lambda x: user_idx_map[x])\ndf_rating[\"anime_idx\"] = df_rating[\"anime_id\"].apply(lambda x: anime_idx_map[x])","6228b3f5":"print(df_rating[\"user_idx\"].max())\nprint(df_rating[\"anime_idx\"].max())","fd52345e":"df_rating.head()","9957453e":"from sklearn.model_selection import train_test_split\n\ndf_rating_train, df_rating_test = train_test_split(df_rating, test_size=0.1, stratify=df_rating.user_id, random_state=93)","eb02a5b0":"X_train = [df_rating_train['user_idx'].values, df_rating_train['anime_idx'].values, np.array([np.array(t) for t in df_rating_train['anime_features']])]\ny_train = df_rating_train['rating'].values\n\nX_test = [df_rating_test['user_idx'].values, df_rating_test['anime_idx'].values, np.array([np.array(t) for t in df_rating_test['anime_features']])]\ny_test = df_rating_test['rating'].values","f471ba14":"import tensorflow as tf\ntf.compat.v1.disable_v2_behavior()\nimport tensorflow.keras as keras\nfrom tensorflow.keras.layers import Input, Embedding, Dot, Concatenate, Add, Flatten, Dense, Dropout, BatchNormalization\nfrom tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, LearningRateScheduler\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.optimizers import Adam","0bf36492":"def create_model(n_users, user_embed_size_dot, user_embed_size_concat, n_items, item_embed_size, item_feature_len, regularization=1e-4):\n     \n    item_features = Input(shape=(item_feature_len, ), name=\"item_features\")\n    user_inp = Input(shape=(1, ), dtype='int32', name=\"user_embed\")\n    user_embed = Embedding(n_users, \n                           user_embed_size_dot, \n                           name='user_embed_mat',\n                           embeddings_initializer=\"glorot_uniform\", \n                           embeddings_regularizer=keras.regularizers.l2(regularization))(user_inp)\n    user_embed_bias = Embedding(n_users, \n                                1, \n                                name='user_embed_bias_mat',\n                                embeddings_initializer=\"glorot_uniform\")(user_inp)\n    user_embed_c = Embedding(n_users, \n                             user_embed_size_concat, \n                             name='user_embed_c_mat',\n                             embeddings_initializer=\"glorot_uniform\", \n                             embeddings_regularizer=keras.regularizers.l2(regularization))(user_inp)\n    \n    item_inp = Input(shape=(1, ), dtype='int32', name=\"item_embed\")\n    item_embed = Embedding(n_items, \n                           item_embed_size, \n                           name='item_embed_mat',\n                           embeddings_initializer=\"glorot_uniform\", \n                           embeddings_regularizer=keras.regularizers.l2(regularization))(item_inp)\n    item_embed_bias = Embedding(n_items, \n                                1, \n                                name='item_embed_bias_mat',\n                                embeddings_initializer=\"glorot_uniform\")(item_inp)\n    \n    user_item_dot = Dot(axes=2, name='user_item_dot')([user_embed, item_embed])\n    \n    user_item_dot = Add()([user_item_dot, user_embed_bias, item_embed_bias])\n    user_item_dot = Flatten()(user_item_dot)\n    user_embed_c = Flatten()(user_embed_c)\n    \n    user_item_concat = Concatenate(axis=1)([user_embed_c, item_features])\n    \n    hidden1 = Dense(8, activation=\"relu\")(user_item_concat)\n    hidden1 = BatchNormalization()(hidden1)\n    hidden1 = Dropout(0.2)(hidden1)\n    \n    dot_hidden1_concat = Concatenate(axis=1)([hidden1, user_item_dot])\n    \n    output = Dense(1, activation=\"relu\")(dot_hidden1_concat)\n    \n    model = Model([user_inp, item_inp, item_features], output)\n    \n    return model\n    ","5e019305":"N_USERS = df_rating.user_idx.max() + 1\nN_ITEMS = df_rating.anime_idx.max() + 1\nUSER_EMBEDDING_SIZE_DOT = 20\nUSER_EMBEDDING_SIZE_CONCAT = 20\nITEM_EMBEDDING_SIZE = 20\nITEM_FEATURE_LEN = 53\n\nmodel = create_model(N_USERS, USER_EMBEDDING_SIZE_DOT, USER_EMBEDDING_SIZE_CONCAT, N_ITEMS, ITEM_EMBEDDING_SIZE, ITEM_FEATURE_LEN)\nmodel.summary()","aba7cd93":"model.compile(Adam(1e-3), loss=\"mse\", metrics=[\"mae\"])","415cda98":"# callbacks defined\n\n# learning rate schedule\ndef step_decay(epoch):\n    initial_lrate = 0.001\n    drop = 0.5\n    epochs_drop = 5\n    lrate = initial_lrate * (drop**((1 + epoch)\/epochs_drop))\n    return lrate\n\nlrate_scheduler = LearningRateScheduler(step_decay)\nearly_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)\nmodel_chkpoint = ModelCheckpoint('best_model.h5', monitor='val_loss', mode='min', verbose=1, save_best_only=True)\n\n# model fitting\nmodel.fit(X_train, y_train, batch_size=32, epochs=50, validation_split=0.1, callbacks=[early_stop, model_chkpoint, lrate_scheduler])","6d865723":"model.predict(X_test)","1ec644cc":"df_rating_test['prediction'] = [t[0] for t in model.predict(X_test)]","9dde87b5":"df_rating_test.head()","1b6e3a7d":"print(\"Test MAE: {}\".format(sum(abs(df_rating_test[\"rating\"] - df_rating_test[\"prediction\"]))\/len(df_rating_test)))","3f83b225":"### Upvote if you liked the approach."}}