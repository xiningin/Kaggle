{"cell_type":{"10a18d5d":"code","23741f8f":"code","8bd544bd":"code","b98b9a51":"code","7741ef8e":"code","8e28b7d1":"code","9db8a70c":"code","6433ad49":"code","82479848":"code","fd297cd9":"code","12c07760":"code","01679af5":"code","642d5b9a":"code","5acaa510":"code","319ee054":"code","f4daa5af":"code","01f03507":"code","8c7dd14a":"code","bd61a404":"code","6423b1ee":"code","f199e140":"code","863b08a2":"code","f81f325e":"code","9544449e":"code","68ad4c25":"code","20af123e":"code","c96b451c":"code","5b621538":"code","3d1d368c":"code","e212ff64":"code","12c046ba":"code","09141c56":"code","c9b3dc94":"code","1628c920":"code","d1517d10":"code","b452f842":"code","177892aa":"code","ba3f8255":"code","69505a14":"code","4fb34bf5":"code","5994293d":"code","38183e5c":"code","ef6272f9":"code","85e02c54":"code","b9b2ed01":"code","b1a46bbb":"code","88db7611":"code","b7b5ad86":"code","33f3d705":"code","1def8fe2":"code","82044415":"code","e98316eb":"code","7ac44a3b":"code","4dc27d30":"code","ae4686a5":"code","927cf0cb":"code","da46133d":"code","ffe53f5f":"code","850766b6":"code","d6343e3e":"code","29ca471e":"code","82b31c6c":"code","48aea285":"code","df1dd90d":"code","c815beee":"code","187eb56d":"code","7eccd898":"code","f93b23e0":"code","ca98edbe":"code","af7e1aa1":"code","e7492f21":"code","d69992b4":"code","39e04408":"code","e961959d":"code","4d60d0e2":"code","808df466":"code","baee3347":"code","78ed2e2b":"code","d925ea04":"code","e139ab16":"code","f72fa8d7":"code","bfed899c":"code","07a687aa":"code","6f9a8fa2":"code","a42a6bfc":"code","d814eef9":"code","f7a2cbdf":"code","f2573970":"code","ca6b4f1d":"code","3e9a4fc8":"markdown","a9c6c74c":"markdown","c9c3555f":"markdown","0b462f92":"markdown","b9db5f2a":"markdown","b13d2436":"markdown","bcadf40f":"markdown","7d73cc46":"markdown","d930a29a":"markdown","f4f0227f":"markdown","c605ebf1":"markdown","2aad5031":"markdown","638cc542":"markdown","5fc18c75":"markdown","c5c8581d":"markdown","9e5dabb2":"markdown","f843f6fd":"markdown","cb79aed2":"markdown","cc7b85d0":"markdown","bfec7fe1":"markdown","6e34cfc6":"markdown","ee7882c3":"markdown","c56ca1ff":"markdown","acc52a95":"markdown","7505c02e":"markdown","7fee8c9c":"markdown","57234568":"markdown","3f0d08fc":"markdown","33764a2e":"markdown","44ca34e8":"markdown","88286415":"markdown","1f6ce1c6":"markdown","5d3efdc6":"markdown","94d827d9":"markdown","3e9b090d":"markdown","219e419b":"markdown","87b61ec2":"markdown","b417430c":"markdown","745e41c5":"markdown","c534c561":"markdown","6e5afd0a":"markdown","03dc0451":"markdown","360e5da9":"markdown","419fc0d1":"markdown","7007a386":"markdown","2cee5dc7":"markdown","be0e1134":"markdown","c0c9fe9e":"markdown","993e4146":"markdown","424b6b33":"markdown","3d56352a":"markdown","3ff42680":"markdown","15851a43":"markdown","d8ba14e8":"markdown","8da8eef4":"markdown","a48b6303":"markdown","878debca":"markdown","e30d2c1b":"markdown"},"source":{"10a18d5d":"import os\nprint(os.listdir(\"..\/input\/\"))","23741f8f":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport re\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, StratifiedKFold\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.svm import LinearSVC, SVC\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier","8bd544bd":"train = pd.read_csv(\"..\/input\/train.csv\")\ntrain['label'] = 'train'\ntest = pd.read_csv(\"..\/input\/test.csv\")\ntest['label'] = 'test'\ntest_passengerId = test.PassengerId  #Save test passengerId. It will be required at the end\ndf = train.append(test)\ndf.sample(2)","b98b9a51":"df.info()","7741ef8e":"df.isnull().sum()","8e28b7d1":"df.describe(include = 'all')","9db8a70c":"#Fill missing value\ndf['Embarked'].fillna('S', inplace = True)    #top value with freq 914","6433ad49":"df[df.Fare.isnull()]","82479848":"df.corr().Fare","fd297cd9":"print(df[df.Pclass == 1].Fare.quantile([0.25, 0.50, 0.75]))\nprint(df[df.Pclass == 2].Fare.quantile([0.25, 0.50, 0.75]))\nprint(df[df.Pclass == 3].Fare.quantile([0.25, 0.50, 0.75]))","12c07760":"sns.factorplot(x = 'Pclass', y = 'Fare', data = df)","01679af5":"df['Fare'].fillna(df[df.Pclass == 3].Fare.median(), inplace = True)   #Fare is dependent on Pclass","642d5b9a":"print(\"Age column has\", df.Age.isnull().sum(), \"missing values out of\", len(df), \". Missing value percentage =\", df.Age.isnull().sum()\/len(df)*100)","5acaa510":"df.corr().Age","319ee054":"df.pivot_table(values = 'Age', index = 'Pclass').Age.plot.bar()","f4daa5af":"df.pivot_table(values = 'Age', index = ['Pclass', 'SibSp'], aggfunc = 'median').Age.plot.bar()","01f03507":"df.pivot_table(values = 'Age', index = ['Pclass', 'SibSp', 'Parch'], aggfunc = 'median')","8c7dd14a":"df.Age.isnull().sum()","bd61a404":"age_null = df.Age.isnull()\ngroup_med_age = df.pivot_table(values = 'Age', index = ['Pclass', 'SibSp'], aggfunc = 'median')\ndf.loc[age_null, 'Age'] = df.loc[age_null, ['Pclass', 'SibSp']].apply(lambda x: group_med_age.loc[(group_med_age.index.get_level_values('Pclass') == x.Pclass) & (group_med_age.index.get_level_values('SibSp') == x.SibSp)].Age.values[0], axis = 1)","6423b1ee":"df.Age.isnull().sum()","f199e140":"print(\"Cabin has\", df.Cabin.isnull().sum(), \"missing values out of\", len(df))","863b08a2":"df['Cabin'] = df.Cabin.str[0]\ndf.Cabin.unique()","f81f325e":"df.Cabin.fillna('O', inplace = True)","9544449e":"df.isnull().sum()","68ad4c25":"df.sample(2)","20af123e":"sns.factorplot(data = df, x = 'Sex', hue = 'Survived', kind = 'count')","c96b451c":"df.pivot_table(values = 'Survived', index = 'Sex').Survived.plot.bar()\nplt.ylabel('Survival Probability')","5b621538":"q = sns.kdeplot(df.Age[df.Survived == 1], shade = True, color = 'red')\nq = sns.kdeplot(df.Age[df.Survived == 0], shade = True, color = 'blue')\nq.set_xlabel(\"Age\")\nq.set_ylabel(\"Frequency\")\nq = q.legend(['Survived', 'Not Survived'])","3d1d368c":"q = sns.FacetGrid(df, col = 'Survived')\nq.map(sns.distplot, 'Age')","e212ff64":"sns.factorplot(data = df, x = 'Embarked', hue = 'Survived', kind = 'count')","12c046ba":"df.pivot_table(values = 'Survived', index = 'Embarked').Survived.plot.bar()\nplt.ylabel('Survival Probability')","09141c56":"df.pivot_table(values = 'Survived', index = ['Sex','Embarked']).Survived.plot.bar()\nplt.ylabel('Survival Probability')","c9b3dc94":"fig, ax =plt.subplots(1,2)\nsns.countplot(data = df[df.Sex == 'female'], x = 'Embarked', hue = 'Survived', ax = ax[0])\nsns.countplot(data = df[df.Sex == 'male'], x = 'Embarked', hue = 'Survived', ax = ax[1])\nfig.show()","1628c920":"sns.factorplot(data = df, x = 'Parch', hue = 'Survived', kind = 'count')","d1517d10":"df.pivot_table(values = 'Survived', index = 'Parch').Survived.plot.bar()\nplt.ylabel('Survival Probability')","b452f842":"sns.factorplot(data = df, x = 'Pclass', hue = 'Survived', kind = 'count')","177892aa":"df.pivot_table(values = 'Survived', index = 'Pclass').Survived.plot.bar()\nplt.ylabel('Survival Probability')","ba3f8255":"df.pivot_table(values = 'Survived', index = ['Sex', 'Pclass']).Survived.plot.bar()\nplt.ylabel('Survival Probability')","69505a14":"sns.factorplot(data = df, x = 'Cabin', hue = 'Survived', kind = 'count')","4fb34bf5":"df.pivot_table(values = 'Survived', index = 'Cabin').Survived.plot.bar()\nplt.ylabel('Survival Probability')","5994293d":"plt.boxplot(train.Fare, showmeans = True)\nplt.title('Fare Boxplot')\nplt.ylabel('Fares')","38183e5c":"sns.distplot(df.Fare)","ef6272f9":"df.Fare.skew()    #Measure of skewness level","85e02c54":"df['Fare_log'] = df.Fare.map(lambda i: np.log(i) if i > 0 else 0)","b9b2ed01":"sns.distplot(df.Fare_log)","b1a46bbb":"df.Fare_log.skew()","88db7611":"df['Family_size'] = 1 + df.Parch + df.SibSp\ndf['Alone'] = np.where(df.Family_size == 1, 1, 0)","b7b5ad86":"print(df.Family_size.value_counts())\nprint(df.Alone.value_counts())","33f3d705":"sns.factorplot(data = df, x = 'Family_size', hue = 'Survived', kind = 'count')","1def8fe2":"df.pivot_table(values = 'Survived', index = 'Family_size').Survived.plot.bar()\nplt.ylabel('Survival Probability')","82044415":"df.loc[df['Family_size'] == 1, 'Family_size_bin'] = 0\ndf.loc[(df['Family_size'] >= 2) & (df['Family_size'] <= 4), 'Family_size_bin'] = 1\ndf.loc[df['Family_size'] >=5, 'Family_size_bin'] = 2","e98316eb":"sns.factorplot(data = df, x = 'Alone', hue = 'Survived', kind = 'count')","7ac44a3b":"df.pivot_table(values = 'Survived', index = 'Alone').Survived.plot.bar()\nplt.ylabel('Survival Probability')","4dc27d30":"df['Title'] = df.Name.str.split(\", \", expand = True)[1].str.split(\".\", expand = True)[0]\ndf.Title.value_counts()","ae4686a5":"minor_titles = df.Title.value_counts() <= 4\ndf['Title'] = df.Title.apply(lambda x: 'Others' if minor_titles.loc[x] == True else x)\ndf.Title.value_counts()","927cf0cb":"sns.factorplot(data = df, x = 'Title', hue = 'Survived', kind = 'count')","da46133d":"df.pivot_table(values = 'Survived', index = 'Title').Survived.plot.bar()\nplt.ylabel('Survival Probability')","ffe53f5f":"df['Fare_bin'] = pd.qcut(df.Fare, 4, labels = [0,1,2,3]).astype(int)\ndf['Age_bin'] = pd.cut(df.Age.astype(int), 5, labels = [0,1,2,3,4]).astype(int)","850766b6":"sns.factorplot(data = df, x = 'Age_bin', hue = 'Survived', kind = 'count')","d6343e3e":"sns.factorplot(data = df, x = 'Fare_bin', hue = 'Survived', kind = 'count')","29ca471e":"fig, axs = plt.subplots(1, 3,figsize=(15,5))\n\nsns.pointplot(x = 'Fare_bin', y = 'Survived',  data=df, ax = axs[0])\nsns.pointplot(x = 'Age_bin', y = 'Survived',  data=df, ax = axs[1])\nsns.pointplot(x = 'Family_size', y = 'Survived', data=df, ax = axs[2])","82b31c6c":"label = LabelEncoder()\ndf['Title'] = label.fit_transform(df.Title)\ndf['Sex'] = label.fit_transform(df.Sex)\ndf['Embarked'] = label.fit_transform(df.Embarked)\ndf['Cabin'] = label.fit_transform(df.Cabin)","48aea285":"df.sample(2)","df1dd90d":"#We will look at correlation between variables. So before working with ticket column, save all variables we worked on yet.\n#This id because we will use get_dummies on ticket and not label encoding.\ncorr_columns = list(df.drop(['Name', 'PassengerId', 'Ticket', 'label'], axis = 1).columns)","c815beee":"df['Ticket'] = df.Ticket.map(lambda x: re.sub(r'\\W+', '', x))   #Remove special characters","187eb56d":"#If ticket is of digit value, make them a character X\nTicket = []\nfor i in list(df.Ticket):\n    if not i.isdigit():\n        Ticket.append(i[:2])\n    else:\n        Ticket.append(\"X\")\ndf['Ticket'] = Ticket","7eccd898":"df.Ticket.unique()","f93b23e0":"df = pd.get_dummies(df, columns = ['Ticket'], prefix = 'T')","ca98edbe":"cat_variables = [x for x in df.columns if df.dtypes[x] == 'object']\ncat_variables","af7e1aa1":"df.drop(['Name', 'PassengerId'], axis = 1, inplace = True)","e7492f21":"df.sample(2)","d69992b4":"train = df.loc[df.label == 'train'].drop('label', axis = 1)\ntest = df.loc[df.label == 'test'].drop(['label', 'Survived'], axis = 1)","39e04408":"plt.figure(figsize = [14,10])\nsns.heatmap(train[corr_columns].corr(), cmap = 'RdBu', annot = True)","e961959d":"plt.figure(figsize = [14,10])\nsns.heatmap(train[corr_columns].corr(method = 'spearman'), cmap = 'RdBu', annot = True)","4d60d0e2":"X_train = train.drop(['Survived'], axis = 1)\ny_train = train['Survived'].astype(int)\nX_test = test","808df466":"# Cross validate model with Kfold stratified cross val\nkfold = StratifiedKFold(n_splits = 5)","baee3347":"classifiers = []\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LinearDiscriminantAnalysis())\nclassifiers.append(LogisticRegression(random_state = 0))\nclassifiers.append(LinearSVC(random_state = 0))\nclassifiers.append(SVC(random_state = 0))\nclassifiers.append(RandomForestClassifier(random_state = 0))\nclassifiers.append(ExtraTreesClassifier(random_state = 0))\nclassifiers.append(XGBClassifier(random_state = 0))\nclassifiers.append(LGBMClassifier(random_state = 0))\nclassifiers.append(MLPClassifier())\n\ncv_results = []\nfor classifier in classifiers:\n    cv_results.append(cross_val_score(classifier, X_train, y_train, scoring = 'accuracy', cv = kfold, n_jobs = -1))\n    \ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\n    \ncv_res = pd.DataFrame({'CV_means':cv_means, 'CV_std':cv_std, 'Algorithm':['KNN', 'LinearDiscriminantAnalysis', 'LogisticRegression', 'LinearSVC', 'SVC', 'RandomForest', 'ExtraTrees', 'XGB', 'LGB', 'MLP']})\n","78ed2e2b":"g = sns.barplot(\"CV_means\", \"Algorithm\", data = cv_res, **{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","d925ea04":"LDA_best = LinearDiscriminantAnalysis().fit(X_train, y_train)","e139ab16":"RF = RandomForestClassifier(random_state = 0)\nRF_params = {'n_estimators' : [10,50,100],\n             'criterion' : ['gini', 'entropy'],\n             'max_depth' : [5,8,None],\n             'min_samples_split' : [2,5,8],\n             'min_samples_leaf' : [1,3,5],\n             'max_features' : ['auto', 'log2', None]}\nGS_RF = GridSearchCV(RF, param_grid = RF_params, cv = kfold, scoring = 'accuracy', n_jobs = -1, verbose = 1)\nGS_RF.fit(X_train, y_train)\nRF_best = GS_RF.best_estimator_\nprint(\"Best parameters :\", RF_best)\nprint(\"Best score :\", GS_RF.best_score_)","f72fa8d7":"ET = ExtraTreesClassifier(random_state = 0)\nET_params = {'n_estimators' : [10,50,100],\n             'criterion' : ['gini', 'entropy'],\n             'max_depth' : [5,8,None],\n             'min_samples_split' : [2,5,8],\n             'min_samples_leaf' : [1,3,5],\n             'max_features' : ['auto', 'log2', None]}\nGS_ET = GridSearchCV(ET, param_grid = ET_params, cv= kfold, scoring = 'accuracy', n_jobs = -1, verbose = 1)\nGS_ET.fit(X_train, y_train)\nET_best = GS_ET.best_estimator_\nprint('Best parameters :', ET_best)\nprint('Best score :', GS_ET.best_score_)","bfed899c":"import warnings\nwarnings.filterwarnings('ignore')","07a687aa":"XGB = XGBClassifier(random_state = 0)\nXGB_params = {'n_estimators' : [100,200,500],\n              'max_depth' : [3,4,5],\n              'learning_rate' : [0.01,0.05,0.1,0.2],\n              'booster' : ['gbtree', 'gblinear', 'dart']}\nGS_XGB = GridSearchCV(XGB, param_grid = XGB_params, cv= kfold, scoring = 'accuracy', n_jobs = -1, verbose = 1)\nGS_XGB.fit(X_train, y_train)\nXGB_best = GS_XGB.best_estimator_\nprint('Best parameters :', XGB_best)\nprint('Best score :', GS_XGB.best_score_)","6f9a8fa2":"LGB = LGBMClassifier(random_state = 0)\nLGB_params = {'n_estimators' : [100,200,500],\n              'max_depth' : [5,8,-1],\n              'learning_rate' : [0.01,0.05,0.1,0.2],\n              'boosting_type' : ['gbdt', 'goss', 'dart']}\nGS_LGB = GridSearchCV(LGB, param_grid = LGB_params, cv= kfold, scoring = 'accuracy', n_jobs = -1, verbose = 1)\nGS_LGB.fit(X_train, y_train)\nLGB_best = GS_LGB.best_estimator_\nprint('Best parameters :', LGB_best)\nprint('Best score :', GS_LGB.best_score_)","a42a6bfc":"MLP = MLPClassifier(random_state = 0)\nMLP_params = {'hidden_layer_sizes' : [[10], [10,10], [10,100], [100,100]],\n              'activation' : ['relu', 'tanh', 'logistic'],\n              'alpha' : [0.0001,0.001,0.01]}\nGS_MLP = GridSearchCV(MLP, param_grid = MLP_params, cv= kfold, scoring = 'accuracy', n_jobs = -1, verbose = 1)\nGS_MLP.fit(X_train, y_train)\nMLP_best = GS_MLP.best_estimator_\nprint('Best parameters :', MLP_best)\nprint('Best score :', GS_MLP.best_score_)","d814eef9":"fig, axes = plt.subplots(2, 2, figsize = [20,10])\nfig.subplots_adjust(hspace = 0.7)\nclassifiers_list = [[\"RandomForest\", RF_best], [\"ExtraTrees\", ET_best],\n                    [\"XGBoost\", XGB_best], [\"LGBoost\", LGB_best]]\n\nnClassifier = 0\nfor row in range(2):\n    for col in range(2):\n        name = classifiers_list[nClassifier][0]\n        classifier = classifiers_list[nClassifier][1]\n        feature = pd.Series(classifier.feature_importances_, X_train.columns).sort_values(ascending = False)\n        feature.plot.bar(ax = axes[row,col])\n        axes[row,col].set_xlabel(\"Features\")\n        axes[row,col].set_ylabel(\"Relative Importance\")\n        axes[row,col].set_title(name + \" Feature Importance\")\n        nClassifier +=1","f7a2cbdf":"LDA_pred = pd.Series(LDA_best.predict(X_test), name = 'LDA')\nMLP_pred = pd.Series(MLP_best.predict(X_test), name = \"MLP\")\nRF_pred = pd.Series(RF_best.predict(X_test), name = \"RFC\")\nET_pred = pd.Series(ET_best.predict(X_test), name = 'ETC')\nXGB_pred = pd.Series(XGB_best.predict(X_test), name = \"XGB\")\nLGB_pred = pd.Series(LGB_best.predict(X_test), name = \"LGB\")\n\nensemble_results = pd.concat([LDA_pred, MLP_pred, RF_pred, ET_pred, XGB_pred, LGB_pred], axis = 1)\nplt.figure(figsize = [8,5])\nsns.heatmap(ensemble_results.corr(), annot = True)","f2573970":"voting = VotingClassifier(estimators = [['LDA', LDA_best], [\"MLP\", MLP_best],\n                                        ['RFC', RF_best], ['ETC', ET_best],\n                                        ['XGB', XGB_best], ['LGB', LGB_best]], voting = 'soft', n_jobs = -1)\nvoting = voting.fit(X_train, y_train)","ca6b4f1d":"results = pd.DataFrame(test_passengerId, columns = ['PassengerId']).assign(Survived = pd.Series(voting.predict(X_test)))\nresults.to_csv('models_voting.csv', index = None)","3e9a4fc8":"Its high! Thus any value derived statistically (mean or median) based on only Age column can mislead the dataset for the classifier. We will fill them based on the relations with other variables.","a9c6c74c":"Surprised! Port of embarkation is very safe for females  but more dangerous form males. Similarly other two ports also contradict for males and females.","c9c3555f":"We will take log transform. This might help classifier in preditions. Also it will help us to find correlation between variables.","0b462f92":"So, we are done with data cleaning part. Missing Survived are from test set.","b9db5f2a":"As much you pay, that much you will get security!","b13d2436":"Females tend to survive more than males.","bcadf40f":"As expected! Correlation are somewhat more stronger than pearson's (looking at more blue blocks in spearan's). Also, Fare_log and Fare have correlation 1 because one is just the log transformation of another. Thus, rank will be same.[](http:\/\/)","7d73cc46":"We found something interesting! Cherbourg port is very safe for females and Qweenstone and Southampton ports are very dangerous for males.","d930a29a":"So we have to handle missing values of age, cabin, embarked and fare. Survived has missing values of test set.","f4f0227f":"#### Cabin","c605ebf1":"### Handling categorical variables","2aad5031":"I've kept voting = 'soft' because for each case, I want the result of that classifier that makes more confident prediction i.e., prediction with more probability.","638cc542":"## Modeling","5fc18c75":"#### Cabin","c5c8581d":"### Import libraries","9e5dabb2":"#### Hyperparamter Tuning","f843f6fd":"#### Parch","cb79aed2":"#### Fare","cc7b85d0":"Cherbourg port is more save as compared to others. Lets look more into this.","bfec7fe1":"This was my first kernel. Any kind of suggestion or appreciation is heartily welcomed. Also please UPVOTE it if you liked my work and found helpful to you.\n\nThank you!","6e34cfc6":"A basic trend can be found from the graph. Thus, we are on right path!","ee7882c3":"#### Age","c56ca1ff":"#### Embarked","acc52a95":"### Ensemble Modelling","7505c02e":"We will plot feature importances of 4 classifiers excluding LDA and MLP. LDA and MLP doesnot have attribute feature_importances_. For linear models, finding feature importance or parameter influence depends on various techniques i.e., p-values, bootstrap scores, various discriminative indices, etc.","7fee8c9c":"### Feature Engineering","57234568":"### Plot Feature Importance","3f0d08fc":"## EDA","33764a2e":"So instead of filling those values, form their cluster. We will assume that those people don't have cabin.","44ca34e8":"#### Fare","88286415":"Lets make bins for age and fare also and visualize them.","1f6ce1c6":"So, we can see that the predictions made by linear models, trees and boosters are quite different. Now, we will make our final output based on VotingClassifier.","5d3efdc6":"### Handling Missing Values","94d827d9":"### Sex","3e9b090d":"Highly right skewed!","219e419b":"We will fill missing values based on Pclass and SibSp.","87b61ec2":"People travelling alone are likely to less survive.","b417430c":"1. Pclass and Cabin has some relations.\n2. Offcourse Family_size will have relation with Parch and SibSp as it is derived from these two.\n\nLets look at spearman's correlation also. Note I'm looking this only for Fare variable as it is not skewed.","745e41c5":"From dataset description, it was clear that Fare values are not skewed. Lets visualize it.","c534c561":"#### Pclass","6e5afd0a":"#### Age","03dc0451":"###### Spearman's","360e5da9":"Now we will select features from the dataset for modelling purpose.","419fc0d1":"Here I'm not using HPT for LDA. Reason behind this is there is no combination which makes some difference. Thus, best parameter will be the default parameters.","7007a386":"Youngters are likely to survive more.","2cee5dc7":"1. RF and ET used more features for prediction whereas boosters gave Fare and Age variable higher weightage.\n2. Bins formed were used by trees whereas boosters did not use bins at all inspite of using Fare and Age variable for predictions.\n\n###### Reasons to keep linear model and MLP:\nWe have created features of variables by forming bins i.e., Fare_bin, Age_bin, etc. Now, linear models, SVC and MLP have tendency to predict better when bins are formed from a feature whereas boosters don't give importance to bins and they use other parameters for classification. Also we saw that trees also used bins features for their prediction. Thus, combining models which classifies the data based on different features is want we want. Thus, I've kept two linear models with trees.","be0e1134":"Split the data into train and test sets.","c0c9fe9e":"#### Embarked","993e4146":"Looks like Pclass can help to fill the missing value.","424b6b33":"### Feature Selection","3d56352a":"This kernel is based on the learnings of others kernels and offcourse my own intuitions and methods too. This notebook will be helpful for beginners.\n\nThis is my first kernel so any kind of suggestion or appreaciation is heartly welcomed.","3ff42680":"Now, family size with 2 to 4 members are more likely to survive. Thus, we will form bins for this groups.","15851a43":"We will use Linear Discriminant Analysis, Random Forest, Extra Trees, XGBoost, Light Gradient Boosting and Multi Layer Perceptron. This is because these all performed well and while using voting classifier, I don't want to bias the classifier. So I used two bagging models, two boosting models, linear model and neural network.[](http:\/\/)","d8ba14e8":"#### Ticket","8da8eef4":"Yup! Values differ totally according to Pclass. Let's look it through visualization.","a48b6303":"###### Pearson's","878debca":"Qualty of tickets class assures more safety!","e30d2c1b":"Lets look correlation between variables."}}