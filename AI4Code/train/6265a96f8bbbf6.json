{"cell_type":{"304b3ea9":"code","c75ef380":"code","4e503fe2":"code","83d7e259":"code","38c971a7":"code","33fec110":"code","31c3c136":"code","8e14d8a3":"code","3f4551c3":"code","ef490825":"code","c897a50d":"code","cfd79640":"code","fcc8b80d":"markdown","14f5818a":"markdown","ff16598e":"markdown","42cd7c77":"markdown","b96aec2c":"markdown","ed31599d":"markdown","0c633dfd":"markdown","9ec5effc":"markdown","1897c900":"markdown","975e6bbf":"markdown","3018d3e6":"markdown","4c97558e":"markdown"},"source":{"304b3ea9":"import pandas as pd\nimport tensorflow as tf\n\nfrom tensorflow import keras\nlayers = keras.layers\n\n# This code was tested with TensorFlow v1.7\nprint(\"You have TensorFlow version\", tf.__version__)","c75ef380":"# Convert the data to a Pandas data frame\ncomments = pd.read_csv('..\/input\/metacritic_game_user_comments.csv')\n# Shuffle with a fixed random seed\n# This will help us to have the same training and test set every time\ncomments = comments.sample(frac=1, random_state=387)\ncomments = comments[pd.notnull(comments['Comment'])]\ncomments.drop(['Unnamed: 0','Username'], axis=1, inplace=True)\n\n# Drop comments with less than 200 characters\n# Modify this parameter to obtain different results\ncomments = comments[comments['Comment'].str.len() > 200]\n# Print the first 5 rows\nprint(len(comments))\ncomments.head()","4e503fe2":"# Split data into train and test\ntrain_size = int(len(comments) * .8)\nprint (\"Train size: %d\" % train_size)\nprint (\"Test size: %d\" % (len(comments) - train_size))\n\n# Train features\ncomments_train = comments['Comment'][:train_size]\n# Train labels\nlabels_train = comments['Userscore'][:train_size]\n# Test features\ncomments_test = comments['Comment'][train_size:]\n# Test labels\nlabels_test = comments['Userscore'][train_size:]","83d7e259":"# Create a tokenizer to preprocess our text descriptions\nvocab_size = 12000 # This is a hyperparameter, experiment with different values for your dataset\ntokenize = keras.preprocessing.text.Tokenizer(num_words=vocab_size, char_level=False)\ntokenize.fit_on_texts(comments_train) # only fit on train","38c971a7":"# Define our wide model with the functional API\nbow_inputs = layers.Input(shape=(vocab_size,))\ninter = layers.Dense(256, activation='relu')(bow_inputs)\ninter = layers.Dropout(0.3)(inter)\npredictions = layers.Dense(1, activation='linear')(inter)\nwide_model = keras.Model(inputs=bow_inputs, outputs=predictions)\nwide_model.compile(loss='mse', optimizer='adam', metrics=['accuracy'])\nprint(wide_model.summary())","33fec110":"max_seq_length = 200\n\n# Define our deep model with the Functional API\ndeep_inputs = layers.Input(shape=(max_seq_length,))\nembedding = layers.Embedding(vocab_size, 16, input_length=max_seq_length)(deep_inputs)\nembedding = layers.Flatten()(embedding)\nembedding = layers.Dense(64, activation='relu')(embedding)\nembedding = layers.Dropout(0.3)(embedding)\nembed_out = layers.Dense(1, activation='linear')(embedding)\ndeep_model = keras.Model(inputs=deep_inputs, outputs=embed_out)\ndeep_model.compile(loss='mse',\n                   optimizer='adam',\n                   metrics=['accuracy'])\nprint(deep_model.summary())","31c3c136":"# Combine wide and deep into one model\nmerged_out = layers.concatenate([wide_model.output, deep_model.output])\nmerged_out = layers.Dense(64, activation='relu')(merged_out)\nmerged_out = layers.Dropout(0.3)(merged_out)\nmerged_out = layers.Dense(1)(merged_out)\ncombined_model = keras.Model([wide_model.input, deep_model.input], merged_out)\ncombined_model.compile(loss='mse',\n                       optimizer='adam',\n                       metrics=['accuracy'])\nprint(combined_model.summary())","8e14d8a3":"def process_comments(comments, tokenize, max_seq_length):\n    # Create the Bag Of Words and the embed version of only this\n    # batch of examples. \n    # This is to avoid using all the memory at the same time\n    bow = tokenize.texts_to_matrix(comments)\n    embed = tokenize.texts_to_sequences(comments)\n    embed = keras.preprocessing.sequence.pad_sequences(\n        embed, maxlen=max_seq_length, padding=\"post\"\n    )\n    return [bow, embed]\n  \n# Create the generator for fit and evaluate\ndef generator(comments_list, labels_list, batch_size, tokenize, max_seq_length):\n    batch_number = 0\n    data_set_len = len(comments_list)\n    batches_per_epoch = int(data_set_len\/batch_size)\n\n    while True:\n        initial = (batch_number*batch_size) % data_set_len\n        final = initial + batch_size\n        comments_to_send = comments_list[initial:final]\n\n        x = process_comments(comments_to_send, tokenize, max_seq_length) \n        y = labels_list[initial:final]\n\n        batch_number = (batch_number+1) % batches_per_epoch\n        yield x, y","3f4551c3":"def on_epoch_end(epoch, logs, print_preditions=0):\n    # Generate predictions\n    predictions = combined_model.predict_generator(\n        generator(comments_test, labels_test, 128, tokenize, max_seq_length),\n        steps=int(len(comments_test)\/128)\n    )\n\n    # Compare predictions with actual values for the first few items in our test dataset\n    diff = 0\n    printed = 0\n    for i in range(len(predictions)):\n        val = predictions[i]\n        if print_preditions and printed < print_preditions:\n            print(comments_test.iloc[i])\n            print('Predicted: ', val[0], 'Actual: ', labels_test.iloc[i], '\\n')\n            printed += 1\n        diff += abs(val[0] - labels_test.iloc[i])\n\n    # Compare the average difference between actual price and the model's predicted price\n    print('\\nEpoch: %d. Average prediction difference: %0.4f\\n' %\n            (epoch+1, diff\/len(predictions)))\n    \n\nprint_callback = keras.callbacks.LambdaCallback(on_epoch_end=on_epoch_end)","ef490825":"# Run training\n# It is a fairly deep network, it will take around 2 minutes per epoch\ncombined_model.fit_generator(\n    generator(comments_train, labels_train, 128, tokenize, max_seq_length),\n    steps_per_epoch=int(len(comments_train)\/128),\n    epochs=7,\n    validation_data=generator(comments_test, labels_test, 128, tokenize, max_seq_length),\n    validation_steps=int(len(comments_test)\/128),\n    callbacks=[print_callback]\n)","c897a50d":"# We manually call on_epoch_end with the trained model, but this time with \n# print_preditions=20\n# It will print 20 examples of the training set, with its predicted and actual \n# value\non_epoch_end(7, {}, print_preditions=20)","cfd79640":"# Let's try some user review of World War Z for Playstatin 4\n# https:\/\/www.metacritic.com\/game\/playstation-4\/world-war-z\/user-reviews\ntest_comments = [\n    \"This game is fun. No microtransactions i can find. Buy it on sale unless you have friends then its a no brainer. It looks good and runs well on ultra 1060 60fps. Its not deep or big or original or creative but its solid which few zombie games are. They have kept it small and as a result its incompetent and capable. The single player works with decent ai which is a first for me. Your ai team will have your back though youll have to do the heavy lifting. There are no bosses which is a real missed opportunity. The progression system is very average but theres plenty to do. Sound and music is average. Gameplay is good. Lastability is what ever you make it but id like to see more free content within a month. The game can be scary and thrilling. The sequel could be great. If you love zombies its a no brainer (lol) and maybe a good fix while waiting on dayz gone.\",\n    \"It's one of the worst games I've played in my life, the AI is extremely stupid, the enemies are completely idiots, there is no challenge in the game, the targets are very bad, there is no sense in anything that is ago, many missions end in a very foolish way. The bots are useless, they do not know how to heal themselves, they do not defend or help. It is a copy of very poor quality of l4d2 in the third person, l4d2, a game of the year 2009 has better graphics, artificial intelligence, objectives, history and a great etc. compared to wwz, if the game was on Steam it would be a complete failure, OTWD was bad, but WWZ is not much better.\",\n    \"I would have given this game an 8-9, but because of the clear review bombing below I am upping the score to a 10. This game is super fun. It is mindless zombie action best played with friends. I agree with the guy below - there is now way this is worth less than a 7. The game is beyond insane in certain areas. It isn't Left4Dead because the action was nowhere near as intense in that game. Streamers are loving it and the one guy who gave a critic review was clearly doing it as clickbait. I would say a solid 8 is much more accurate. I will say it is much more fun online and Sony PSN was down yesterday so I haven't played as much as I would like.\",\n    \"The game has a fun progression system, but the constant buggyness holds it back. I have ran into a bug daily since release, and it has really killed my enjoyment for the game. If you are hoping to play with friends, it's a gamble whether or not you can invite them. Some people can, others can't. There are no check points, so if you bug out (which happens often) you have to restart with 0 progress gained. The past days since release there has been constant connection errors online. And when you do join, half of the people I have seen are either hackers or extremely toxic. (I was doing a higher difficulty with a low level, and someone just kept shooting me every time I respawned claiming we were going to lose anyways.) And even if you do play offline, despite what people say, the bots are garbage. But they're probably designed that way so they don't nick things important to you. They don't pick up any new items, and when you set up defenses you have to do it all by yourself. Not to mention any mission where you have to haul multiple of the objective back to a point you have to do that solo too. With all that negativity, what does this game offer over left 4 dead? The progression system lets you upgrade guns you use frequently, and you get experience for classes which have special abilities (some let you hit more than one target with a melee ability, others let you rez people from a distance, etc.) and get different special equipment, like grenades or stun guns. This alone is what got me into the game, but since I'm usually playing solo and my team comp is just AI with aimbots with no abilities I never get to see classes shine. Sound design and enemy, design compared to L4D is also lackluster. Bombs feel like they're smothered by wet paper towels, along with grenade launchers. You go to different locations but the Bull (charger) special infected always has POLICE on it. One enemy spawns more zombies but it usually is too far away to shoot, and happens during defenses which is where you're already shooting a large amount of zombies so you can't kill it and get easily overrun. If the devs ever get around to fixing the numerous bugs and getting online stable, it'll already be out on steam and they'll be doing the same **** over again. Wait for left 4 dead 3, if it even exists.\"\n]\n# The scores are \n# 7\n# 0\n# 10\n# 3\n\ncombined_model.predict(process_comments(test_comments, tokenize, max_seq_length))","fcc8b80d":"Then we use Pandas to read the csv and transform it into a dataframe. After that we do some processing.","14f5818a":"Now we are going to create the Keras models.\n\nFirst we define the \"***wide***\" model. That will take a bag of words. ","ff16598e":"Then we split the dataset in train and test sets. Since we used a constant random seed, this will return the same result every time.","42cd7c77":"Generators are used in cases where your whole training set would not fit into memory, or when you want to apply some kind of data augmentation on training time.\n\n\nIn this case we are going to use it because the \"***wide***\" representation (bags of words) of all the training examples at the same time would take all of the memory allocated for this Kaggle kernel (?).\n\nAlso note that at training time we will all *process_comments*, which will create the Bag Of Words and sequences of words to send to the Embedding layer. Naturally, this will slow down the training a little bit (we could actually pre-calculate the sequences to speed up).","b96aec2c":"Then we combine both models using Keras functional API","ed31599d":"At the end, you should get an average difference of around 1.25 points per user review.\n\nFinally, we evaluate again the final, trained model, printing some of the results.","0c633dfd":"Another thing we will do before starting the training, we will define a callback function, which means, a function that will be called after each epoch ends. \n\nWhat we want is to test the partially trained model to check how it is predicting the score of the test set.\n\n**NOTE:** Experienced users may notice that we are basically rewriting the formula for \"mean_absolute_error\". We left this code to illustrate the use of callbacks for custom operations every epoch end.","9ec5effc":"We define a keras Tokenizer, and we fit it with the train set.","1897c900":"Finally, let's try to predict the score of freshly retrieved user reviews.","975e6bbf":"Next we define the second model, which is the \"***deep***\" model, that will take the sequences of words and pass them to a Embedding layer","3018d3e6":"# Predicting the user score of Metacritic user reviews of Video Games using Keras functional API and Tensorflow.\n\n\nCan we predict the score given to a Video Game based on the user review posted in Metacritic? In this kernel we are going to use Kera's functional API and Tensorflow backend to try to achieve this task.\n\nWe are going to be using this dataset which includes the metascore (the one derived from professional reviews) and the user comments (or reviews) for the top 5000 games.","4c97558e":"Since we are going to use generators, we use Model.fit_generator instead of Model.fit.\n\nPlease note that the generator is responsible to yield both the inputs and the expected labels of each batch. \n\nWe are also sending validation_data, so the model will get evaluated after each epoch, printing the loss and accuracy of the test set (val_loss, val_acc).\n\nFinally, we send the callback we created a couple of steps before, so it will execute *on_epoch_end* every epoch."}}