{"cell_type":{"3dca0af3":"code","2ce00896":"code","0bc47fb8":"code","088a7898":"code","d777d877":"code","09692af5":"code","4889d08a":"code","5c31219f":"code","adaad680":"code","1d6d678e":"code","acfec5d9":"code","ff0e9487":"code","06f4523d":"code","633e2aad":"code","6f33fdeb":"code","9bab90d1":"code","cc7559dc":"code","483c3bbf":"code","eb2f98f7":"code","bfd3835e":"code","2da8f625":"code","beba1561":"markdown","56845f2a":"markdown","c422ff76":"markdown","5d57c585":"markdown","7f08394c":"markdown","898a980d":"markdown"},"source":{"3dca0af3":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport tensorflow as tf\nimport keras\nimport keras.layers as layers\nfrom keras.models import Sequential\nfrom keras.preprocessing.image import ImageDataGenerator","2ce00896":"file_path = '..\/input\/digit-recognizer\/'\n\ntrain_data = pd.read_csv(file_path + 'train.csv')","0bc47fb8":"X = np.array(train_data.iloc[:,1:])\ny = train_data.iloc[:,0]\nlabels = pd.get_dummies(y)\nlabels = np.array(labels)","088a7898":"train_data = []\nfor i in X:\n    train_data.append(i.reshape(28,28,1))\ntrain_data = np.array(train_data)","d777d877":"val_data = train_data[:2000]\nval_labels = labels[:2000]\n\ntest_data = train_data[2000:2*2000]\ntest_labels = labels[2000:2*2000]\n\ntrain_data = train_data[4000:]\ntrain_labels = labels[4000:]","09692af5":"train_labels.shape","4889d08a":"import random\nrandom_indices = random.sample(range(0, len(train_data)), 9)\n\nfor i, idx in enumerate(random_indices):\n    img = train_data[idx]\n    label = y[4000+idx]\n    \n    plt.subplot(330 + 1 + i)    \n    plt.tick_params(left=False,\n            bottom=False,\n            labelleft=False,\n            labelbottom=False)\n    \n    plt.title('Label is {label}'.format(label=label))\n    plt.tight_layout()\n    plt.imshow(img, cmap='gray') \n    \nplt.show()","5c31219f":"plt.figure(figsize=(6, 6))\nfor i in range(25):\n    plt.subplot(5, 5, i + 1)\n    plt.xticks([])\n    plt.yticks([])\n    plt.imshow(train_data[i])\n    plt.tight_layout()\nplt.show()","adaad680":"# Pad images with 0s\ntrain_data = np.pad(train_data, ((0,0),(2,2),(2,2),(0,0)), 'constant')\nval_data = np.pad(val_data, ((0,0),(2,2),(2,2),(0,0)), 'constant')\ntest_data = np.pad(test_data, ((0,0),(2,2),(2,2),(0,0)), 'constant')","1d6d678e":"train_data.shape\n# The data is now ready","acfec5d9":"model = keras.Sequential()\n\nmodel.add(layers.Conv2D(filters=6, kernel_size=(3, 3), activation='relu', input_shape=(32,32,1)))\nmodel.add(layers.AveragePooling2D())\n\nmodel.add(layers.Conv2D(filters=16, kernel_size=(3, 3), activation='relu'))\nmodel.add(layers.AveragePooling2D())\n\nmodel.add(layers.Flatten())\n\nmodel.add(layers.Dense(units=120, activation='relu'))\n\nmodel.add(layers.Dense(units=84, activation='relu'))\n\nmodel.add(layers.Dense(units=10, activation = 'softmax'))","ff0e9487":"model.summary()","06f4523d":"from keras.optimizers import Adam\nmodel.compile(loss=keras.losses.categorical_crossentropy, optimizer=Adam(), metrics=['accuracy'])","633e2aad":"# define the type of augmentation techniques we will apply.\ntrain_datagen = ImageDataGenerator(\n    rescale =1\/255,\n    shear_range=10,\n    zoom_range = 0.2,\n    width_shift_range=0.2,\n    height_shift_range=0.2,\n    rotation_range=20,\n    fill_mode = 'nearest',\n)\nval_datagen = ImageDataGenerator(\n    rescale =1\/255,\n#     shear_range=10,\n#     zoom_range = 0.2,\n#     width_shift_range=0.2,\n#     height_shift_range=0.2,\n#     rotation_range=20,\n#     fill_mode = 'nearest',\n)\ntest_datagen = ImageDataGenerator(\n    rescale =1\/255,\n)\n","6f33fdeb":"train_generator=train_datagen.flow(\n    train_data,\n    train_labels,\n    batch_size= 128,\n)\nvalidation_generator = val_datagen.flow(\n        val_data,\n        val_labels,\n        batch_size=128,\n)\ntest_generator = test_datagen.flow(\n    test_data,\n    test_labels,\n    batch_size=128,\n)","9bab90d1":"history = model.fit(train_generator,\n                    epochs=30,\n                    steps_per_epoch = 38000 \/\/ 128,\n                    validation_data = validation_generator,\n                    validation_steps = 2000 \/\/ 128)","cc7559dc":"model.evaluate(validation_generator)[1]","483c3bbf":"model.evaluate(test_generator)[1]","eb2f98f7":"sub_data = pd.read_csv(file_path + 'test.csv')\ntest = np.reshape(np.array(sub_data), (sub_data.shape[0], 28, 28, 1))\ntest = np.pad(test, ((0,0),(2,2),(2,2),(0,0)), 'constant')","bfd3835e":"prediction = model.predict_classes(test).reshape(-1,1)\nout = [{'ImageId': i+1, 'Label': prediction[i][0]} for i in range(len(prediction))]","2da8f625":"pd.DataFrame(out).to_csv('submission2.csv', index=False)","beba1561":"# Implementing the Yann LeCun's Le-Net architecture\n> \"The implementation is not about getting the best accuracy\"","56845f2a":"# Splitting Data","c422ff76":"## Getting out Features ready\n\nThe LeNet architecture accepts a 32x32 pixel images as input, mnist data is 28x28 pixels. We simply pad the images with zeros to overcome that.","5d57c585":"![](https:\/\/drek4537l1klr.cloudfront.net\/elgendy\/v-3\/Figures\/05_01.png)\n\n#### Input\n    32x32x1 pixels image\n\n#### Architecture\n* **Convolutional #1** outputs 28x28x6\n    * **Activation** any activation function, we will `relu`\n\n* **Pooling #1** The output shape should be 14x14x6.\n\n                        \n* **Convolutional #2** outputs 10x10x16.\n     * **Activation** any activation function, we will `relu`\n    \n\n* **Pooling #2** outputs 5x5x16.\n    * **Flatten** Flatten the output shape of the final pooling layer\n\n* **Fully Connected #1** outputs 120\n    * **Activation** any activation function, we will `relu`\n\n* **Fully Connected #2** outputs 84\n    * **Activation** any activation function, we will `relu`\n\n* **Fully Connected (Logits) #3** outpute 10\n","7f08394c":"Original Le-Net architecture","898a980d":"## Load the data\n\nLoad train and test data in memory"}}