{"cell_type":{"bdffbdfc":"code","9569d5e3":"code","699c6bfe":"code","f9441e26":"code","4236227b":"code","a40fee6a":"code","6c6d4fac":"code","57a33fc3":"code","c4adc9dc":"code","62b2e199":"code","1c1fb435":"code","387fafec":"code","afb883d8":"code","3ec86d39":"code","9cbd8bf1":"code","a0fdac42":"code","e3975627":"code","6b2236de":"code","b104b0ed":"code","a03e7906":"code","a9e90687":"code","76da6b2e":"markdown","c8d3585b":"markdown","793f215f":"markdown","ffff7add":"markdown"},"source":{"bdffbdfc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n# import numpy as np # linear algebra\n# import pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","9569d5e3":"import numpy as np     # working with arrays\nimport pandas as pd     # data preprocesssing\nimport matplotlib.pyplot as plt   # visualization\n\nfrom sklearn.preprocessing   import StandardScaler , MinMaxScaler   # data normalization\nfrom sklearn.model_selection import train_test_split     # data split\n\nfrom sklearn.tree          import  DecisionTreeClassifier   # Decision tree algorithm\nfrom sklearn.neighbors     import  KNeighborsClassifier    # KNN algorithm\nfrom sklearn.linear_model  import  LogisticRegression    # Logistic regression algorithm\nfrom sklearn.svm           import  SVC    # SVM algorithm\nfrom sklearn.ensemble      import  RandomForestClassifier   # Random forest tree algorithm\nfrom xgboost               import  XGBClassifier    # XGBoost algorithm\n\nfrom sklearn.metrics import confusion_matrix , accuracy_score , f1_score    # evaluation metric\n","699c6bfe":"# IMPORTING DATA\nurl = os.path.join(dirname, filename)\ndf = pd.read_csv(url)","f9441e26":"df.head(10)","4236227b":"df.dtypes\ndf.columns","a40fee6a":"df.Class.value_counts().to_frame()","6c6d4fac":"df.shape  # (284807, 31)","57a33fc3":"df.describe()","c4adc9dc":"# df.corr()","62b2e199":"#df.isnull().sum()\ndf.drop('Time' , axis=1 , inplace=True)","1c1fb435":"cases_count = len(df)","387fafec":"non_fraud_count,fraud_count = df.Class.value_counts()\nfraud_percentage = round(fraud\/non_fraud*100 , 2)\n\nprint('Total number of cases = {}'.format(cases_count) )\nprint('Number of Non-fraud cases = {}'.format(non_fraud_count))\nprint('Number of fraud cases = {}'.format(fraud_count))\nprint(f'Percentage of fraud cases = {fraud_percentage}')","afb883d8":"# or\n# cases = len(df)\n# nonfraud_count = len(df[df.Class == 0])\n# fraud_count = len(df[df.Class == 1])\n# fraud_percentage = round(fraud_count\/nonfraud_count*100, 2)\n# fraud_percentage","3ec86d39":"#  CASE AMOUNT STATISTICS\n\nnonfraud_cases = df[df.Class == 0]\nfraud_cases = df[df.Class == 1]\n\nprint('NON-FRAUD CASE AMOUNT STATS')\nprint(nonfraud_cases.Amount.describe())\n# print(nonfraud_cases['Amount'].describe())\nprint('--------------------------------------------')\nprint('FRAUD CASE AMOUNT STATS')\nprint(fraud_cases.Amount.describe())","9cbd8bf1":"df.hist()\nplt.show()","a0fdac42":"sc = StandardScaler()\ndf['Amount'] = sc.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['Amount'].head()","e3975627":"# DATA SPLIT\n\nX= df.iloc[:,:len(df.columns)-1]\ny= df['Class']\n\n# X = df.drop('Class', axis = 1).values\n# y = df['Class'].values\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)\n\nprint('X samples:' , X.shape)\nprint('y samples:' , y.shape)\n\nprint('X_train samples:' , X_train.shape)\nprint('X_test samples :' , X_test.shape)\nprint('y_train samples:' , y_train.shape)\nprint('y_test samples :' , y_test.shape)\n","6b2236de":"# 1. Decision Tree\n\ntree_model = DecisionTreeClassifier(max_depth = 4, criterion = 'entropy')\ntree_model.fit(X_train, y_train)\ntree_yhat = tree_model.predict(X_test)\n\n# ----------------------------------------------------------------------\n\n# 2. K-Nearest Neighbors\nn = 5\nknn = KNeighborsClassifier(n_neighbors = n)\nknn.fit(X_train, y_train)\nknn_yhat = knn.predict(X_test)\n\n# ----------------------------------------------------------------------\n\n# 3. Logistic Regression\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nlr_yhat = lr.predict(X_test)\n\n# ----------------------------------------------------------------------\n\n# 4. SVM \nsvm = SVC()\nsvm.fit(X_train, y_train)\nsvm_yhat = svm.predict(X_test)\n\n# ----------------------------------------------------------------------\n\n# 5. Random Forest Tree\nrf = RandomForestClassifier(max_depth = 4)\nrf.fit(X_train, y_train)\nrf_yhat = rf.predict(X_test)\n\n# ----------------------------------------------------------------------\n\n# 6. XGBoost\nxgb = XGBClassifier(max_depth = 4)\nxgb.fit(X_train, y_train)\nxgb_yhat = xgb.predict(X_test)","b104b0ed":"#  Accuracy score\n#  Accuracy score = No.of correct predictions \/ Total no.of predictions\n\nprint('Accuracy score of the Decision Tree model = {}'.format(accuracy_score(y_test, tree_yhat)))\n\nprint('Accuracy score of the KNN model = {}'.format(accuracy_score(y_test, knn_yhat)))\n\nprint('Accuracy score of the Logistic Regression model = {}'.format(accuracy_score(y_test, lr_yhat)))\n\nprint('Accuracy score of the SVM model = {}'.format(accuracy_score(y_test, svm_yhat)))\n\nprint('Accuracy score of the Random Forest Tree model = {}'.format(accuracy_score(y_test, rf_yhat)))\n\nprint('Accuracy score of the XGBoost model = {}'.format(accuracy_score(y_test, xgb_yhat)))\n","a03e7906":"# F1 Score\n# F1 score = 2( (precision * recall) \/ (precision + recall) )\n\nprint(f'F1 score of the Decision Tree model = {f1_score(y_test, tree_yhat)}')\n\nprint(f'F1 score of the KNN model = {f1_score(y_test, knn_yhat)}')\n\nprint(f'F1 score of the Logistic Regression model = {f1_score(y_test, lr_yhat)}')\n\nprint(f'F1 score of the SVM model = {f1_score(y_test, svm_yhat)}')\n\nprint(f'F1 score of the Random Forest Tree model = {f1_score(y_test, rf_yhat)}')\n\nprint(f'F1 score of the XGBoost model = {f1_score(y_test, xgb_yhat)}')\n","a9e90687":"import itertools # advanced tools\n#  Confusion Matrix\n\n# defining the plot function\n\ndef plot_confusion_matrix(cm, classes, title, normalize = False, cmap = plt.cm.Blues):\n    title = 'Confusion Matrix of {}'.format(title)\n    if normalize:\n        cm = cm.astype(float) \/ cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation = 'nearest', cmap = cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation = 45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment = 'center',\n                 color = 'white' if cm[i, j] > thresh else 'black')\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\n# ***********************************************************************************\n\n# Compute confusion matrix for the models\n\ntree_matrix = confusion_matrix(y_test, tree_yhat, labels = [0, 1])   # Decision Tree\nknn_matrix  = confusion_matrix(y_test, knn_yhat , labels = [0, 1])     # K-Nearest Neighbors\nlr_matrix   = confusion_matrix(y_test, lr_yhat  , labels = [0, 1])       # Logistic Regression\nsvm_matrix  = confusion_matrix(y_test, svm_yhat , labels = [0, 1])     # Support Vector Machine\nrf_matrix   = confusion_matrix(y_test, rf_yhat  , labels = [0, 1])       # Random Forest Tree\nxgb_matrix  = confusion_matrix(y_test, xgb_yhat , labels = [0, 1])     # XGBoost\n\n# ***********************************************************************************\n\n# Plot the confusion matrix\n\nplt.rcParams['figure.figsize'] = (6, 6)\n\n\n# 1. Decision tree\ntree_cm_plot = plot_confusion_matrix(tree_matrix, \n                                classes = ['Non-Default(0)','Default(1)'], \n                                normalize = False, title = 'Decision Tree')\nplt.savefig('tree_cm_plot.png')\nplt.show()\n\n\n# 2. K-Nearest Neighbors\nknn_cm_plot = plot_confusion_matrix(knn_matrix, \n                                classes = ['Non-Default(0)','Default(1)'], \n                                normalize = False, title = 'KNN')\nplt.savefig('knn_cm_plot.png')\nplt.show()\n\n\n# 3. Logistic regression\nlr_cm_plot = plot_confusion_matrix(lr_matrix, \n                                classes = ['Non-Default(0)','Default(1)'], \n                                normalize = False, title = 'Logistic Regression')\nplt.savefig('lr_cm_plot.png')\nplt.show()\n\n\n# 4. Support Vector Machine\nsvm_cm_plot = plot_confusion_matrix(svm_matrix, \n                                classes = ['Non-Default(0)','Default(1)'], \n                                normalize = False, title = 'SVM')\nplt.savefig('svm_cm_plot.png')\nplt.show()\n\n\n# 5. Random forest tree\nrf_cm_plot = plot_confusion_matrix(rf_matrix, \n                                classes = ['Non-Default(0)','Default(1)'], \n                                normalize = False, title = 'Random Forest Tree')\nplt.savefig('rf_cm_plot.png')\nplt.show()\n\n\n# 6. XGBoost\nxgb_cm_plot = plot_confusion_matrix(xgb_matrix, \n                                classes = ['Non-Default(0)','Default(1)'], \n                                normalize = False, title = 'XGBoost')\nplt.savefig('xgb_cm_plot.png')\nplt.show()","76da6b2e":"Evaluation","c8d3585b":"Feature Selection & Data Split","793f215f":"Credit Card Fraud Detection With Machine Learning in Python\n==> Using XGBoost, Random forest, KNN, Logistic regression, SVM, and Decision tree to solve classification problems","ffff7add":"Modeling :"}}