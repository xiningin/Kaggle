{"cell_type":{"dd6be3c4":"code","2bdeccb2":"code","78eee539":"code","3499ba34":"code","784c0c31":"code","f329525f":"code","a4578a81":"code","e0e89b70":"code","bf373b4a":"code","e25dfef3":"code","487c3e89":"code","336511f3":"code","82e3e2e5":"code","69f0c35d":"code","0853a8d1":"code","4c47915d":"code","10e94272":"code","a945e8d6":"code","8abc32c0":"code","5e664581":"code","7ab0bd7f":"code","688fa2b6":"code","fdaad18c":"code","c837fa10":"code","67977c60":"code","abe5a761":"code","08d5be03":"code","8e552088":"code","7543ac96":"code","744b1617":"code","d65ca57b":"code","c96c7915":"code","8e4e182d":"code","3a2877be":"code","a058d3bd":"code","34c06e5e":"code","08546209":"code","792ba43d":"code","8d394d19":"code","ad25cf9d":"code","b7c3e91e":"code","d13a0bf5":"code","9e47241f":"code","983ce330":"code","23b6e2ab":"code","4cff6cc4":"code","610cc969":"code","1145e243":"code","de04ccce":"markdown","28893eaf":"markdown","c63096b8":"markdown","502ae889":"markdown","20f47911":"markdown","a09ec988":"markdown","28ad8a06":"markdown","0d37b3b0":"markdown","8611fd9c":"markdown","31ffeef7":"markdown"},"source":{"dd6be3c4":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport os\nimport glob","2bdeccb2":"data_dir = '..\/input\/optiver-realized-volatility-prediction\/'","78eee539":"def calc_wap(df):\n    wap = (df['bid_price1'] * df['ask_size1'] + df['ask_price1'] * df['bid_size1'])\/(df['bid_size1'] + df['ask_size1'])\n    return wap\ndef calc_wap2(df):\n    wap = (df['bid_price2'] * df['ask_size2'] + df['ask_price2'] * df['bid_size2'])\/(df['bid_size2'] + df['ask_size2'])\n    return wap\ndef calc_wap3(df):\n    wap = (df['bid_price2'] * df['bid_size2'] + df['ask_price2'] * df['ask_size2']) \/ (df['bid_size2']+ df['ask_size2'])\n    return wap\n\n\ndef log_return(list_stock_prices):\n    return np.log(list_stock_prices).diff() \n\ndef realized_volatility(series):\n    return np.sqrt(np.sum(series**2))\n\ndef count_unique(series):\n    return len(np.unique(series))","3499ba34":"book_train = pd.read_parquet(data_dir + \"book_train.parquet\/stock_id=15\")","784c0c31":"def preprocessor_book(file_path):\n    df = pd.read_parquet(file_path)\n    #calculate return etc\n    df['wap'] = calc_wap(df)\n    df['log_return'] = df.groupby('time_id')['wap'].apply(log_return)\n    \n    df['wap2'] = calc_wap2(df)\n    df['log_return2'] = df.groupby('time_id')['wap2'].apply(log_return)\n    \n    df['wap3'] = calc_wap3(df)\n    df['log_return3'] = df.groupby('time_id')['wap3'].apply(log_return)\n    \n    df['wap_balance'] = abs(df['wap'] - df['wap2'])\n    \n    df['price_spread'] = (df['ask_price1'] - df['bid_price1']) \/ ((df['ask_price1'] + df['bid_price1'])\/2)\n    df['bid_spread'] = df['bid_price1'] - df['bid_price2']\n    df['ask_spread'] = df['ask_price1'] - df['ask_price2']\n    df['total_volume'] = (df['ask_size1'] + df['ask_size2']) + (df['bid_size1'] + df['bid_size2'])\n    df['volume_imbalance'] = abs((df['ask_size1'] + df['ask_size2']) - (df['bid_size1'] + df['bid_size2']))\n\n    #dict for aggregate\n    create_feature_dict = {\n        'log_return':[realized_volatility],\n        'log_return2':[realized_volatility],\n        'log_return3':[realized_volatility],\n        'wap_balance':[np.mean],\n        'price_spread':[np.mean],\n        'bid_spread':[np.mean],\n        'ask_spread':[np.mean],\n        'volume_imbalance':[np.mean],\n        'total_volume':[np.mean],\n        'wap':[np.mean],\n            }\n\n    #####groupby \/ all seconds\n    df_feature = pd.DataFrame(df.groupby(['time_id']).agg(create_feature_dict)).reset_index()\n    \n    df_feature.columns = ['_'.join(col) for col in df_feature.columns] #time_id is changed to time_id_\n        \n    ######groupby \/ last XX seconds\n    last_seconds = [300]\n    \n    for second in last_seconds:\n        second = 600 - second \n    \n        df_feature_sec = pd.DataFrame(df.query(f'seconds_in_bucket >= {second}').groupby(['time_id']).agg(create_feature_dict)).reset_index()\n\n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns] #time_id is changed to time_id_\n     \n        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n\n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n    \n    #create row_id\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['time_id_'],axis=1)\n    \n    return df_feature","f329525f":"%%time\nfile_path = data_dir + \"book_train.parquet\/stock_id=0\"\npreprocessor_book(file_path)","a4578a81":"trade_train = pd.read_parquet(data_dir + \"trade_train.parquet\/stock_id=0\")","e0e89b70":"def preprocessor_trade(file_path):\n    df = pd.read_parquet(file_path)\n    df['log_return'] = df.groupby('time_id')['price'].apply(log_return)\n    \n    \n    aggregate_dictionary = {\n        'log_return':[realized_volatility],\n        'seconds_in_bucket':[count_unique],\n        'size':[np.sum],\n        'order_count':[np.mean],\n    }\n    \n    df_feature = df.groupby('time_id').agg(aggregate_dictionary)\n    \n    df_feature = df_feature.reset_index()\n    df_feature.columns = ['_'.join(col) for col in df_feature.columns]\n\n    \n    ######groupby \/ last XX seconds\n    last_seconds = [300]\n    \n    for second in last_seconds:\n        second = 600 - second\n    \n        df_feature_sec = df.query(f'seconds_in_bucket >= {second}').groupby('time_id').agg(aggregate_dictionary)\n        df_feature_sec = df_feature_sec.reset_index()\n        \n        df_feature_sec.columns = ['_'.join(col) for col in df_feature_sec.columns]\n        df_feature_sec = df_feature_sec.add_suffix('_' + str(second))\n        \n        df_feature = pd.merge(df_feature,df_feature_sec,how='left',left_on='time_id_',right_on=f'time_id__{second}')\n        df_feature = df_feature.drop([f'time_id__{second}'],axis=1)\n    \n    df_feature = df_feature.add_prefix('trade_')\n    stock_id = file_path.split('=')[1]\n    df_feature['row_id'] = df_feature['trade_time_id_'].apply(lambda x:f'{stock_id}-{x}')\n    df_feature = df_feature.drop(['trade_time_id_'],axis=1)\n    \n    return df_feature","bf373b4a":"%%time\nfile_path = data_dir + \"trade_train.parquet\/stock_id=0\"\npreprocessor_trade(file_path)","e25dfef3":"def preprocessor(list_stock_ids, is_train = True):\n    from joblib import Parallel, delayed # parallel computing to save time\n    df = pd.DataFrame()\n    \n    def for_joblib(stock_id):\n        if is_train:\n            file_path_book = data_dir + \"book_train.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_train.parquet\/stock_id=\" + str(stock_id)\n        else:\n            file_path_book = data_dir + \"book_test.parquet\/stock_id=\" + str(stock_id)\n            file_path_trade = data_dir + \"trade_test.parquet\/stock_id=\" + str(stock_id)\n            \n        df_tmp = pd.merge(preprocessor_book(file_path_book),preprocessor_trade(file_path_trade),on='row_id',how='left')\n     \n        return pd.concat([df,df_tmp])\n    \n    df = Parallel(n_jobs=-1, verbose=1)(\n        delayed(for_joblib)(stock_id) for stock_id in list_stock_ids\n        )\n\n    df =  pd.concat(df,ignore_index = True)\n    return df\n","487c3e89":"list_stock_ids = [0,1]\npreprocessor(list_stock_ids, is_train = True)","336511f3":"train = pd.read_csv(data_dir + 'train.csv')","82e3e2e5":"train_ids = train.stock_id.unique()","69f0c35d":"%%time\ndf_train = preprocessor(list_stock_ids= train_ids, is_train = True)","0853a8d1":"train['row_id'] = train['stock_id'].astype(str) + '-' + train['time_id'].astype(str)\ntrain = train[['row_id','target']]\ndf_train = train.merge(df_train, on = ['row_id'], how = 'left')","4c47915d":"df_train.head()","10e94272":"df_train.isnull().sum()","a945e8d6":"df_train.fillna(0, inplace=True)","8abc32c0":"df_train.isna().sum()","5e664581":"test = pd.read_csv(data_dir + 'test.csv')","7ab0bd7f":"test_ids = test.stock_id.unique()","688fa2b6":"%%time\ndf_test = preprocessor(list_stock_ids= test_ids, is_train = False)","fdaad18c":"df_test = test.merge(df_test, on = ['row_id'], how = 'left')","c837fa10":"from sklearn.model_selection import KFold\n#stock_id target encoding\ndf_train['stock_id'] = df_train['row_id'].apply(lambda x:x.split('-')[0])\ndf_test['stock_id'] = df_test['row_id'].apply(lambda x:x.split('-')[0])\n\nstock_id_target_mean = df_train.groupby('stock_id')['target'].mean() \ndf_test['stock_id_target_enc'] = df_test['stock_id'].map(stock_id_target_mean) # test_set\n\n#training\ntmp = np.repeat(np.nan, df_train.shape[0])\nkf = KFold(n_splits = 20, shuffle=True,random_state = 19911109)\nfor idx_1, idx_2 in kf.split(df_train):\n    target_mean = df_train.iloc[idx_1].groupby('stock_id')['target'].mean()\n\n    tmp[idx_2] = df_train['stock_id'].iloc[idx_2].map(target_mean)\ndf_train['stock_id_target_enc'] = tmp","67977c60":"from tensorflow import keras\nfrom keras.models import Sequential\nfrom keras.layers import LSTM, Dense","abe5a761":"df_train['stock_id'] = df_train['stock_id'].astype(int)\ndf_test['stock_id'] = df_test['stock_id'].astype(int)","08d5be03":"X= df_train.drop(['row_id','target'],axis=1)\ny= df_train['target']","8e552088":"from sklearn.model_selection import KFold\nkf = KFold(n_splits=25, random_state=19901028, shuffle=True)\noof = pd.DataFrame()                 # out-of-fold result\nmodels = []                          # models\nscores = 0.0   ","7543ac96":"%%time\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n    \n    # create dataset\n    X_train, y_train = X.loc[trn_idx], y[trn_idx]\n    X_valid, y_valid = X.loc[val_idx], y[val_idx]\n    ","744b1617":"%%time\nfor fold, (trn_idx, val_idx) in enumerate(kf.split(X, y)):\n    \n    # create dataset\n    X_train, y_train = X.loc[trn_idx], y[trn_idx]\n    X_valid, y_valid = X.loc[val_idx], y[val_idx]\n    \n","d65ca57b":"from sklearn.preprocessing import StandardScaler","c96c7915":"scaler = StandardScaler()\nX_train_scaled = scaler.fit_transform(X_train)\nX_valid_scaled = scaler.transform(X_valid)","8e4e182d":"n = 30  \nXtrain = []\nytrain = []\n\nfor i in range(n, len(X_train_scaled)): \n    Xtrain.append(X_train_scaled[(i - n):i, :X_train_scaled.shape[1]])\n    ytrain.append(y_train[i-1:i]) # predict the next record\n","3a2877be":"Xtrain, ytrain = (np.array(Xtrain), np.array(ytrain))\n#Xtrain = np.reshape(Xtrain, (Xtrain.shape[0], Xtrain.shape[1], Xtrain.shape[2]))","a058d3bd":"Xtrain[0].shape","34c06e5e":"Xvalid = []\nyvalid = []","08546209":"m = 30\nfor i in range(m, len(X_valid)): #use different numbers\n    Xvalid.append(X_valid_scaled[(i - n):i, :X_valid_scaled.shape[1]])\n    yvalid.append(y_valid[i-1:i]) # predict the next record","792ba43d":"Xvalid, yvalid = (np.array(Xvalid), np.array(yvalid))\n#Xvalid = np.reshape(Xvalid, (Xvalid.shape[0], Xvalid.shape[1], Xvalid.shape[2]))","8d394d19":"yvalid.shape","ad25cf9d":"def layer_maker(n_layers, n_nodes, activation, drop=None, d_rate=.5):\n    \"\"\"\n    Creates a specified number of hidden layers for an RNN\n    Optional: Adds regularization option - the dropout layer to prevent potential overfitting (if necessary)\n    \"\"\"\n    \n    # Creating the specified number of hidden layers with the specified number of nodes\n    for x in range(1,n_layers+1):\n        model.add(LSTM(n_nodes, activation=activation, return_sequences=True))\n\n        # Adds a Dropout layer after every Nth hidden layer (the 'drop' variable)\n        try:\n            if x % drop == 0:\n                model.add(Dropout(d_rate))\n        except:\n            pass","b7c3e91e":"# Instatiating the model\nmodel = Sequential()\n\n# Activation\nactiv = \"tanh\"\n\n# Input layer\nmodel.add(LSTM(50, \n               activation=activ, \n               return_sequences=True, \n               input_shape=(30, Xtrain.shape[1])))\n\n# Hidden layers\nlayer_maker(n_layers=2, \n        n_nodes=30, \n            activation=activ,\n            drop=1,\n            d_rate=.1)\n\n# Final Hidden layer\nmodel.add(LSTM(30, activation=activ))\n\n# Output layer\nmodel.add(Dense(1))\n\n# Model summary\nmodel.summary()","d13a0bf5":"model.compile(optimizer='adam', loss='mse', metrics=['mean_squared_error'])","9e47241f":"LSTM_Model = model.fit(Xtrain, ytrain, epochs = 20, batch_size =32, validation_split=0.1)","983ce330":"#validation\ny_pred = model.predict(Xvalid)","23b6e2ab":"y_pred.shape","4cff6cc4":"def rmspe(y_true, y_pred):\n    return  (np.sqrt(np.mean(np.square((y_true - y_pred) \/ y_true))))","610cc969":"RMSPE = round(rmspe(y_true = yvalid, y_pred = y_pred),3)\nprint(f'Performance of the\u3000prediction: , RMSPE: {RMSPE}')\n    \n ","1145e243":" \n\n    #keep scores and models\n    scores += RMSPE \/ 25\n    models.append(model)\n    print(\"*\" * 100)","de04ccce":"## LSTM-Regression","28893eaf":"## Model Building","c63096b8":"## Training set","502ae889":"## Target encoding by stock_id","20f47911":"## Main function for preprocessing book data","a09ec988":"## Combined preprocessor function","28ad8a06":"## Functions for preprocess","0d37b3b0":"## Main function for preprocessing trade data","8611fd9c":"**Labeling","31ffeef7":"## Test set"}}