{"cell_type":{"e0397c0c":"code","56e6fb8f":"code","c3ccf6c6":"code","cc35257f":"code","f7563aa0":"code","8146109c":"code","95f29253":"code","c80ef0bd":"code","86830b48":"code","f87b5f2a":"code","2cf24d6e":"code","8467743b":"code","e0bf08ec":"code","26a8a151":"code","ca39817c":"code","2136d5b3":"code","19c62bad":"code","bedb8b9f":"code","0519c5a7":"code","28c632aa":"code","79e80498":"code","bcfc895c":"code","f81d5da9":"code","a566c10b":"code","2e55ddb7":"code","644b2d6e":"code","2211ece2":"code","9587a007":"code","435676ed":"code","e88ccd01":"code","b4b0fdbf":"markdown","1d47e539":"markdown","767b28e4":"markdown","34ee3191":"markdown","7d9292a4":"markdown","f139a368":"markdown","cc80d826":"markdown"},"source":{"e0397c0c":"# import libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport re\nimport seaborn as sn\nimport xgboost as xgb\nfrom statistics import mode\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC \nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\n","56e6fb8f":"# load data set\ndf_train = pd.read_csv('..\/input\/titanic\/train.csv')\ndf_train.head()","c3ccf6c6":"df_test = pd.read_csv('..\/input\/titanic\/test.csv')\ndf_test.head()","cc35257f":"# check missing value in df_train\ndf_train.isna().sum()","f7563aa0":"# replace NA in Age with the median\ndf_train.Age = df_train.Age.fillna(df_train.Age.median())\n\n# replace NA in Cabin with 0\ndf_train.Cabin = df_train.Cabin.fillna(0)\n\n# replace NA in Embarked with the mode\ndf_train.Embarked = df_train.Embarked.fillna(mode(df_train.Embarked))","8146109c":"# check missing value again\ndf_train.isna().sum()","95f29253":"# check missing value in df_test\ndf_test.isna().sum()","c80ef0bd":"# replace NA in Age with the median\ndf_test.Age = df_test.Age.fillna(df_test.Age.median())\n\n# replace NA in Cabin with 0\ndf_test.Cabin = df_test.Cabin.fillna(0)\n\n# replace NA in Fare with the mean\ndf_test.Fare = df_test.Fare.fillna(np.mean(df_test.Fare))","86830b48":"# check missing value in df_test agian\ndf_test.isna().sum()","f87b5f2a":"# Pipeline\nclass data(object):\n    def __init__(self,dataset = df_train):\n        self.dataset = dataset.copy()\n    def generate_features1(self):\n        self.dataset['Sex'] = self.dataset.Sex.replace(\"female\", 0)\n        self.dataset['Sex'] = self.dataset.Sex.replace(\"male\", 1)\n        self.dataset['name_length'] = self.dataset.Name.str.len()\n        self.dataset['title'] = self.dataset.Name.str.extract(pat='([a-zA-Z]+\\.)')\n        self.dataset['family'] = self.dataset.SibSp +self.dataset.Parch\n        self.dataset['ticket_length'] = self.dataset.Ticket.str.len()\n        self.dataset['ticket_letter'] = self.dataset.Ticket.apply(lambda x : 1 if bool(re.search('[A-Za-z]+',x)) else 0)\n        self.dataset['cabin_letter'] = self.dataset.Cabin.str.replace('[0-9]+','').str.replace(' ','')\n        self.dataset['cabin_number'] = self.dataset.cabin_letter.str.len()\n        self.dataset['cabin_number'] = self.dataset.cabin_number.apply(lambda x: 1 if x != x else x)\n        self.dataset['ave_fare'] = self.dataset.Fare \/ self.dataset.cabin_number\n        self.dataset = self.dataset.drop(columns = ['PassengerId', 'Name', 'Ticket', 'Cabin'])\n        return self.dataset\n    def generate_features2(self):\n        self.dataset['Sex'] = self.dataset.Sex.replace(\"female\", 0)\n        self.dataset['Sex'] = self.dataset.Sex.replace(\"male\", 1)\n        self.dataset['Age'] = pd.cut(self.dataset['Age'], [0,18,30,60,100], labels = ['bin1', 'bin2', 'bin3', 'bin4'])\n        self.dataset['family'] = self.dataset.SibSp +self.dataset.Parch\n        self.dataset['family'] = pd.cut(self.dataset['family'], [-0.5,0.5,3.5,np.Inf], labels = ['b1','b2','b3'])\n        self.dataset['title'] = self.dataset.Name.str.extract(pat='([a-zA-Z]+\\.)')\n        for titl in ['Master.','Mrs.','Miss.','Mr.']:\n            self.dataset[titl] = self.dataset.title.apply(lambda x: int(titl in x if isinstance(x, str) else False))\n        self.dataset['title_Other'] = self.dataset.loc[:,['Master.','Mrs.','Miss.','Mr.']].sum(1)\n        self.dataset['title_Other'] = self.dataset.title_Other.apply(lambda x : 1 if x == 0 else 0)\n        self.dataset['name_length'] = self.dataset.Name.str.len()\n        self.dataset['cabin_letter'] = self.dataset.Cabin.str.replace('[0-9]+','').str.replace(' ','')\n        self.dataset['cabin_letter'] = self.dataset.cabin_letter.apply(lambda x : 'other' if not(isinstance(x, str)) else \\\n                                                                       'b1' if any(['A'in x, 'G' in x]) else \\\n                                                                      'b2' if any(['B' in x, 'C' in x, 'D' in x, 'E' in x, 'F' in x]) else 'other')\n        self.dataset = pd.get_dummies(self.dataset, columns = ['Embarked', 'Pclass', 'Age', 'family', 'cabin_letter'])\n        self.dataset = self.dataset.drop(columns = ['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin','title'])\n        return self.dataset\n    def generate_features3(self):\n        self.dataset['Sex'] = self.dataset.Sex.replace(\"female\", 0)\n        self.dataset['Sex'] = self.dataset.Sex.replace(\"male\", 1)\n        self.dataset['Age'] = self.dataset.Age - self.dataset.Age.mean()\n        self.dataset['Fare'] = self.dataset.Fare - self.dataset.Fare.mean()\n        self.dataset['family'] = self.dataset.SibSp +self.dataset.Parch\n        self.dataset['family'] = pd.cut(self.dataset['family'], [-0.5,0.5,3.5,np.Inf], labels = ['b1','b2','b3'])\n        self.dataset['title'] = self.dataset.Name.str.extract(pat='([a-zA-Z]+\\.)')\n        for titl in ['Mrs.','Miss.','Mr.']:\n            self.dataset[titl] = self.dataset.title.apply(lambda x: int(titl in x if isinstance(x, str) else False))\n        self.dataset['title_Other'] = self.dataset.loc[:,['Mrs.','Miss.','Mr.']].sum(1)\n        self.dataset['title_Other'] = self.dataset.title_Other.apply(lambda x : 1 if x == 0 else 0)\n        self.dataset['name_length'] = self.dataset.Name.str.len()\n        self.dataset['cabin_letter'] = self.dataset.Cabin.str.replace('[0-9]+','').str.replace(' ','')\n        self.dataset['cabin_letter'] = self.dataset.cabin_letter.apply(lambda x : 'other' if not(isinstance(x, str)) else \\\n                                                                       'b1' if any(['A'in x, 'G' in x]) else \\\n                                                                      'b2' if any(['B' in x, 'C' in x, 'D' in x, 'E' in x, 'F' in x]) else 'other')\n        self.dataset = pd.get_dummies(self.dataset, columns = ['Embarked', 'Pclass', 'Age', 'family', 'cabin_letter'])\n        self.dataset = self.dataset.drop(columns = ['PassengerId', 'Name', 'SibSp', 'Parch', 'Ticket', 'Cabin','title'])\n        return self.dataset\n    def grid_search(self,x, y, model_info, parameters, cv):\n        x = np.array(x)\n        y = np.array(y)\n        grid_result = []\n        for model in model_info.keys():\n            search_rf = GridSearchCV(estimator = model_info[model],\n                                     param_grid = parameters[model], \n                                     cv = cv,\n                                     n_jobs = -1,\n                                     verbose = 2)\n            search_rf.fit(x, y)\n            grid_result.append(search_rf.best_params_)\n        return  grid_result\n    def scores(self, model_list,train_x, train_y):\n        model_names = []\n        model_score = []\n        for model in model_list:\n            model.fit(train_x, train_y)\n            model_names.append(model)\n            model_score.append(model.score(train_x, train_y))\n        result = pd.DataFrame({'models':model_names, 'socres':model_score})\n        return result\n    def predict(self, cv, base_models, target_model,train_x, train_y, test_x):\n        train_x = np.array(train_x)\n        train_y = np.array(train_y)\n        test_x = np.array(test_x)\n        kf =KFold(n_splits = cv, shuffle=True, random_state=2021)\n        result1 = np.zeros((train_x.shape[0],len(base_models)))\n        result2 = np.zeros((test_x.shape[0],len(base_models)))\n        for ind1, model in enumerate(base_models):\n            result3 = np.zeros((test_x.shape[0], cv))\n            for ind2, (train_index, test_index) in enumerate(kf.split(train_x)):\n                f_x = train_x[train_index,:]\n                f_y = train_y[train_index]\n                s_x = train_x[test_index,:]\n                model.fit(f_x,f_y)\n                pred1 = model.predict(s_x)[:]\n                pred2 = model.predict(test_x)[:]\n                result1[test_index,ind1] = pred1\n                result3[:,ind2] = pred2\n            result2[:,ind1] = result3.mean(1)\n        target_model.fit(result1,train_y)\n        y_pred = target_model.predict(result2)[:]\n        return y_pred\n  ","2cf24d6e":"new_train = data(df_train).generate_features1()\nnew_train","8467743b":"# violinplot of Pclass\nsn.violinplot(x=\"Survived\", y=\"Pclass\", data=new_train, size=6)","e0bf08ec":"# violinplot of Sex\nsn.violinplot(x=\"Survived\", y=\"Sex\", data=new_train, size=6)","26a8a151":"# kdeplot of Age\nsn.FacetGrid(new_train, hue=\"Survived\", height=6) \\\n   .map(sn.kdeplot, \"Age\") \\\n   .add_legend()\n\n# I plan to create severals bins for Age: 0-18, 18-30, 30-60, 60+","ca39817c":"# SibSp, Parch, and  family\nsn.jointplot(x=\"SibSp\", y=\"Parch\", data=new_train, height=5)\nplt.show()\n\nsn.FacetGrid(new_train, hue=\"Survived\", size=6) \\\n   .map(sn.kdeplot, \"family\") \\\n   .add_legend()\n\n# bins for family: 0, 1-3, 4+","2136d5b3":"# kdeplot of Fare, ave_fare\nsn.histplot(new_train, x ='Fare')\nplt.show()\n\nsn.histplot(new_train, x = 'ave_fare')\nplt.show()\n\nsn.FacetGrid(new_train, hue=\"Survived\", height=6) \\\n   .map(sn.kdeplot, \"Fare\") \\\n   .add_legend()\nplt.show()\n\nsn.FacetGrid(new_train, hue=\"Survived\", height=6) \\\n   .map(sn.kdeplot, \"ave_fare\") \\\n   .add_legend()\nplt.show()\n\n# higher fare can bring higher survivied possibility","19c62bad":"sn.violinplot(x=\"Embarked\", y=\"Survived\", data=new_train, height=6)","bedb8b9f":"sn.violinplot(x = 'Survived', y = 'title', data = new_train, height = 6)\n\n# five groups: Mr., Mrs., Miss., Master., other","0519c5a7":"sn.violinplot(x = 'Survived', y = 'name_length', data=new_train, height=6)","28c632aa":"sn.violinplot(x = 'Survived', y = 'ticket_letter', data=new_train, height=6)\n\n#there is no siginificant difference, so the feature is not good","79e80498":"sn.violinplot(x = 'Survived', y = 'ticket_length', data=new_train, height=6)\n\n#there is no siginificant difference, so the feature is not good","bcfc895c":"sn.violinplot(x = 'Survived', y = 'cabin_letter', data = new_train, height=6)\n\n# people in B C D E F cabin has a higher survived rate,in A ,G and other are not","f81d5da9":"# Grid Search\nmodel_info = {'rf':RandomForestClassifier(), 'xgb':xgb.XGBClassifier(), 'ert':ExtraTreesClassifier(),\n              'lr':LogisticRegression(), 'knn':KNeighborsClassifier(), 'svc':SVC()}\n\n# Random Forset\ngrid_rf = {\n    \"n_estimators\": np.linspace(100,1000,5, dtype = int),\n    \"max_depth\": [3,5,7],\n    \"max_features\": [3,5,7,9,11],\n    \"min_samples_leaf\": [3,5,7],\n    \"min_samples_split\":[3,5,7],\n    \"random_state\": [2020,2021]\n}\n\ngrid_xgb = {\n    'booster': ['gbtree'],\n    'objective': ['binary:logistic'],\n    'subsample': [0.6,0.7,0.8],\n    'colsample_bytree': [0.6,0.7,0.8],\n    'eta': [0.05,0.1,0.2],\n    'max_depth': [3,5],\n    'seed': [2020, 2021],\n    'eval_metric': ['logloss']\n}\n\ngrid_ert = {\n    \"n_estimators\": np.linspace(100,900,5, dtype = int),\n    \"max_depth\": [3,5,7],\n    \"max_features\": [3,5,6,7],\n    \"min_samples_leaf\": [3,5,7],\n    \"min_samples_split\":[3,5,7],\n    \"random_state\": [2020, 2021]\n}\n\ngrid_lr = {\n    \"penalty\": ['l1', 'l2'],\n    \"C\":[0.01, 0.05, 0.1, 0.25, 0.5],\n    \"random_state\": [2020, 2021]\n}\n\ngrid_knn = {\n    \"n_neighbors\": [5,7,10,15,20]\n}\n\ngrid_svc = {\n    \"C\":[0.001, 0.005, 0.01, 0.05, 0.1, 0.25, 0.5],\n    \"gamma\":[0.001, 0.005, 0.01, 0.05, 0.1, 0.25, 0.5],\n    \"random_state\": [2020, 2021]\n}\n\nparameters = {'rf':grid_rf, 'xgb':grid_xgb, 'ert':grid_ert,\n              'lr':grid_lr, 'knn':grid_knn, 'svc':grid_svc}","a566c10b":"new_train = data(df_train).generate_features2()\nnew_test = data(df_test).generate_features2()\n\ntrain_x = new_train.drop(columns = 'Survived')\ntrain_y = new_train.Survived\ntest_x = new_test","2e55ddb7":"(best_rf_1, best_xgb_1, best_ert_1, best_lr_1, best_knn_1, best_svc_1) = data().grid_search(train_x, train_y,model_info, parameters,3)","644b2d6e":"# create models\nmodel_rf = RandomForestClassifier(**best_rf_1)\n\nmodel_xgb = xgb.XGBClassifier(**best_xgb_1)\n\nmodel_ert = ExtraTreesClassifier(**best_ert_1)\n\nmodel_lr = LogisticRegression(**best_lr_1)\n\nmodel_knn = KNeighborsClassifier(**best_knn_1)\n\nmodel_svc = SVC(**best_svc_1)","2211ece2":"model_list = [model_rf, model_xgb, model_ert, model_lr, model_knn, model_svc]\ndata().scores(model_list,train_x, train_y)","9587a007":"base_models = [model_rf, model_ert, model_lr]\ntarget_ert = xgb.XGBClassifier()\n\ntarget_model = target_ert","435676ed":"pred = data().predict(5, base_models, target_model,train_x, train_y, test_x)\n","e88ccd01":"res = pd.DataFrame({'PassengerId':df_test.PassengerId, 'Survived': pred})\nres.to_csv('submission.csv', index=False)","b4b0fdbf":"# Data Cleansing","1d47e539":"**Conclusion:**<br>\n[1] Pclass:keep<br>\n[2] Sex: keep<br>\n[3] Age: bins(0-18,18-30,30-60,60+)<br>\n[4] SibSp: remove<br>\n[5] parch: remove<br>\n[6] family: bins(0,1-3,4+)<br>\n[7] Fare, ave_fare: keep one<br>\n[8] Embarked: keep, dummy<br>\n[9] ttile: bins(Mr., Mrs., Miss., Master., other)<br>\n[10] name_length: keep<br>\n[11] ticket (letter and length): remove<br>\n[12] Cabin_letter: bins (BCDEF, AG, other)","767b28e4":"Now, the task is generate as many features as possible. here is my plan:<br>\n[1] Name. generate two features: name length and title, and title will be transform into dummy variables<br>\n[2] SibSp and Parch. For me, they are very similar, so I can create a new feature called family which equals SibSp + Parch, or I can set several bins for the two columns (0, 1, or 2+)<br>\n[3] Ticket. generate two features: ticket length and letter (the ticket number contains letter or not)<br>\n[4] Cabin. two features: Cabin letter (dummy variable) and Cabin number<br>\n[5] Fare. based on Cabin number, generate ave_fare = Fare \/ Cabin number <br>\n[6] Embarked. Transform into dummy variables <br>","34ee3191":"# Ensemble Generation","7d9292a4":"# Model Selection","f139a368":"# Feature Engineering","cc80d826":"EDA"}}