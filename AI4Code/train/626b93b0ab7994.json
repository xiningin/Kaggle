{"cell_type":{"849842e0":"code","0c8aedcf":"code","3b29677a":"code","57d5f212":"code","12f59c10":"code","9359f4a8":"code","1d8a671c":"code","ae8f0337":"code","e9ca8338":"code","6e4c8597":"code","e207a5aa":"code","62c426c0":"code","d57530d6":"code","37bcd20f":"code","88e153dd":"code","926f4ef8":"code","a847897f":"code","a19637ae":"code","94141b85":"code","1ebe179d":"code","ae9c0343":"code","17c9fa6f":"code","ed9059da":"code","5f16d568":"code","96890f62":"code","f14115e8":"code","343a7d81":"code","2d46a82f":"code","2fa31bc2":"code","f72abbb7":"code","5407c523":"code","cb8b7ffd":"code","37aa999f":"code","4605701a":"code","2a4bd8f6":"code","ac25556e":"code","7041814b":"code","9ac50bbd":"code","ec0affb8":"code","322b860a":"code","abd0f2e6":"markdown","90fd2bea":"markdown"},"source":{"849842e0":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","0c8aedcf":"# Import libraries\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew\n\nfrom sklearn.preprocessing import RobustScaler, StandardScaler\nfrom sklearn.linear_model import LinearRegression, Ridge, Lasso, ElasticNet\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.base import clone\nfrom sklearn.neighbors import KNeighborsRegressor","3b29677a":"# Warning ignore\nimport warnings\n\nwarnings.filterwarnings('ignore')","57d5f212":"# XGBoost\n\nimport xgboost as xgb","12f59c10":"data = pd.read_csv(\"\/kaggle\/input\/autompg-dataset\/auto-mpg.csv\",na_values = \"?\", comment = \"\\t\", skipinitialspace = True)","9359f4a8":"# Return the first 5 rows\n\ndata.head()","1d8a671c":"# Drop one feature 'car name'\n\ndata.drop(['car name'], inplace = True, axis = 1)\ndata.head()","ae8f0337":"# Return a tuple representing the dimensionality of the DataFrame\n\ndata.shape","e9ca8338":"# Print a concise summary of a DataFrame\n\ndata.info()","6e4c8597":"data.columns","e207a5aa":"#Generate descriptive statistics\n\ndata.describe()","62c426c0":"# Missing values\n\nprint(data.isna().sum())\n\n","d57530d6":"data[\"horsepower\"].unique()","37bcd20f":"data[\"horsepower\"] = data[\"horsepower\"].fillna(data[\"horsepower\"].mean())\n\nprint(data.isna().sum())\n\n","88e153dd":"plt.figure(figsize=(15,10))\nsns.distplot(data.horsepower)","926f4ef8":"# EDA\n\ncorr_matrix = data.corr()\nsns.clustermap(corr_matrix, annot = True, fmt = \".2f\")\nplt.title(\"Correlation btw features\")\nplt.show()","a847897f":"threshold = 0.75\nfiltre = np.abs(corr_matrix[\"mpg\"])>threshold\ncorr_features = corr_matrix.columns[filtre].tolist()\nsns.clustermap(data[corr_features].corr(), annot = True, fmt = \".2f\")\nplt.title(\"Correlation btw features\")\nplt.show()","a19637ae":"# Pair plot\n\nsns.pairplot(data, diag_kind = \"kde\", markers = \"+\")\nplt.show()","94141b85":"plt.figure()\nsns.countplot(data[\"cylinders\"])\nprint(data[\"cylinders\"].value_counts())\n\nplt.figure()\nsns.countplot(data[\"origin\"])\nprint(data[\"origin\"].value_counts())","1ebe179d":"# box plot\n\nfor i in data.columns:\n    plt.figure(figsize=(15,10))\n    sns.boxplot(x = i, data = data, orient = \"v\")","ae9c0343":"# outlier: horsepower and acceleration\n\nquartile1 = data[\"horsepower\"].quantile(0.25)\nquartile3 = data[\"horsepower\"].quantile(0.75)\ninterquantile_range = quartile3 - quartile1\nup_limit = quartile3 + 1.5 * interquantile_range\nlow_limit = quartile1 - 1.5 * interquantile_range\nfilter_hp_low = low_limit < data[\"horsepower\"]\nfilter_hp_up = data[\"horsepower\"] < up_limit\nfilter_hp = filter_hp_low & filter_hp_up\ndata = data[filter_hp]\ndata\n\n\nquartile1 = data[\"acceleration\"].quantile(0.25)\nquartile3 = data[\"acceleration\"].quantile(0.75)\ninterquantile_range = quartile3 - quartile1\nup_limit = quartile3 + 1.5 * interquantile_range\nlow_limit = quartile1 - 1.5 * interquantile_range\nfilter_acc_low = low_limit < data[\"acceleration\"]\nfilter_acc_up = data[\"acceleration\"] < up_limit\nfilter_acc = filter_hp_low & filter_hp_up\ndata = data[filter_acc]\ndata\n    \n\n\n\n","17c9fa6f":"# Feature Engineering\n\nplt.figure(figsize=(15,10))\nsns.distplot(data.mpg, fit = norm, color=\"b\")\n\n(mu, sigma) = norm.fit(data[\"mpg\"])\nprint(\"mu: {}, sigma = {}\".format(mu, sigma))","ed9059da":"fig = plt.figure(figsize = (15,10))\nstats.probplot(data[\"mpg\"], plot = plt)\nplt.show()","5f16d568":"# log1p transformation\n\ndata[\"mpg\"] = np.log1p(data[\"mpg\"])\n\n","96890f62":"plt.figure(figsize = (15,10))\nsns.distplot(data.mpg, fit= norm)\nplt.show()","f14115e8":"fig = plt.figure(figsize = (15,10))\nstats.probplot(data[\"mpg\"], plot = plt)\nplt.show()","343a7d81":"skewed_feats = data.apply(lambda x: skew(x.dropna())).sort_values(ascending = False)\nskewness = pd.DataFrame(skewed_feats, columns = [\"skewed\"])\nskewness","2d46a82f":"# One hot encoding\ndata[\"cylinders\"] = data[\"cylinders\"].astype(str)  \ndata[\"origin\"] = data[\"origin\"].astype(str) \n\ndata = pd.get_dummies(data)","2fa31bc2":"data","f72abbb7":"# Split\n\nX = data.drop([\"mpg\"], axis = 1)\ny = data.mpg\n\ntest_size = 0.2\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = test_size, random_state = 42)\nX","5407c523":"# Standardization\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\n","cb8b7ffd":"# KNN Regressor\n\nknn_reg = KNeighborsRegressor(n_neighbors = 5,\n                              weights ='uniform',\n                              metric = 'minkowski',\n                              algorithm = 'auto')\n\nknn_reg.fit(X_train,y_train)\ny_pred = knn_reg.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\n\nprint(\"KNN Regression MSE:\", mse)","37aa999f":"# Linear Regression\n\nlr = LinearRegression()\nlr.fit(X_train, y_train)\nprint(\"LR Coef: \",lr.coef_)\ny_predicted = lr.predict(X_test)\nmse = mean_squared_error(y_test, y_predicted)\nprint(\"Linear Regression MSE: \",mse)","4605701a":"# Ridge Regression\n\nridge = Ridge(random_state = 42, max_iter = 10000)\nalphas = np.logspace(-4,-0.5,30)\n\ntuned_parameters = [{'alpha':alphas}]\n\nclf = GridSearchCV(ridge, tuned_parameters, cv = 5, scoring = \"neg_mean_squared_error\", refit = True)\nclf.fit(X_train, y_train)\nscores = clf.cv_results_[\"mean_test_score\"]\nscores_std = clf.cv_results_[\"std_test_score\"]\n\nprint(\"Ridge Coef: \",clf.best_estimator_.coef_)\nridge = clf.best_estimator_\nprint(\"Ridge Best Estimator: \", ridge)\n\ny_pred = clf.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Ridge MSE: \",mse)","2a4bd8f6":"plt.figure(figsize = (15,10))\nplt.semilogx(alphas, scores)\nplt.xlabel(\"alpha\")\nplt.ylabel(\"score\")\nplt.title(\"Ridge Regressor\")","ac25556e":"# Lasso Regression\n\nlasso = Lasso(random_state = 42, max_iter = 10000)\nalphas = np.logspace(-4,-0.5,30)\n\ntuned_parameters = [{'alpha':alphas}]\n\nclf = GridSearchCV(lasso, tuned_parameters, cv = 5, scoring = \"neg_mean_squared_error\", refit = True)\nclf.fit(X_train, y_train)\nscores = clf.cv_results_[\"mean_test_score\"]\nscores_std = clf.cv_results_[\"std_test_score\"]\n\nprint(\"Lasso Coef: \",clf.best_estimator_.coef_)\nlasso = clf.best_estimator_\nprint(\"Lasso Best Estimator: \", ridge)\n\ny_pred = clf.predict(X_test)\nmse = mean_squared_error(y_test, y_pred)\nprint(\"Lasso MSE: \",mse)","7041814b":"plt.figure(figsize = (15,10))\nplt.semilogx(alphas, scores)\nplt.xlabel(\"alpha\")\nplt.ylabel(\"score\")\nplt.title(\"Lasso Regressor\")","9ac50bbd":"# ElasticNet\n\nparametersGrid = {\"alpha\": alphas,\n                  \"l1_ratio\": np.arange(0.0, 1.0, 0.05)}\n\neNet = ElasticNet(random_state=42, max_iter=10000)\nclf = GridSearchCV(eNet, parametersGrid, cv=5, scoring='neg_mean_squared_error', refit=True)\nclf.fit(X_train, y_train)\n\n\nprint(\"ElasticNet Coef: \",clf.best_estimator_.coef_)\nprint(\"ElasticNet Best Estimator: \",clf.best_estimator_)\n\n\ny_pred = clf.predict(X_test)\nmse = mean_squared_error(y_test,y_pred)\nprint(\"ElasticNet MSE: \",mse)\n","ec0affb8":"plt.figure(figsize = (15,10))\nplt.semilogx(alphas, scores)\nplt.xlabel(\"alpha\")\nplt.ylabel(\"score\")\nplt.title(\"ElasticNet\")","322b860a":"# XGBoost\n\nparametersGrid  = {'nthread': [5],\n                  'objective': ['reg:linear'],\n                  'learning_rate':[0.03, 0.05, 0.07, 0.1],\n                  'max_depth': [5, 6, 7],\n                  'min_child_weight': [4],\n                  'silent': [1],\n                  'subsample': [0.7],\n                  'colsample_bytree': [0.7],\n                  'n_estimators':[500, 800, 1000]}\n\nmodel_xgb = xgb.XGBRegressor()\n\nclf = GridSearchCV(model_xgb,\n                   parametersGrid,\n                   cv = 10,\n                   scoring= \"neg_mean_squared_error\",\n                   refit = True)\n\n\nclf.fit(X_train, y_train)\nmodel_xgb = clf.best_estimator_\n\ny_pred = clf.predict(X_test)\nmse = mean_squared_error(y_test,y_pred)\nprint(\"XGBRegressor MSE:\", mse)","abd0f2e6":"# Data Set Information:\n\n### Context\nThe data is technical spec of cars. The dataset is downloaded from UCI Machine Learning Repository\n\n\n### Title: \nAuto-Mpg Data\n\n### Sources:\n\nOrigin: This dataset was taken from the StatLib library which is\nmaintained at Carnegie Mellon University. The dataset was\nused in the 1983 American Statistical Association Exposition.\n\nDate: July 7, 1993\n\n### Past Usage:\n\nSee 2b (above)\nQuinlan,R. (1993). Combining Instance-Based and Model-Based Learning.\nIn Proceedings on the Tenth International Conference of Machine\nLearning, 236-243, University of Massachusetts, Amherst. Morgan\nKaufmann.\n\n### Relevant Information:\n\nThis dataset is a slightly modified version of the dataset provided in\nthe StatLib library. In line with the use by Ross Quinlan (1993) in\npredicting the attribute \"mpg\", 8 of the original instances were removed\nbecause they had unknown values for the \"mpg\" attribute. The original\ndataset is available in the file \"auto-mpg.data-original\".\n\n\"The data concerns city-cycle fuel consumption in miles per gallon,\nto be predicted in terms of 3 multivalued discrete and 5 continuous\nattributes.\" (Quinlan, 1993)\n\n### Number of Instances: \n398\n\n### Number of Attributes: \n9 including the class attribute\n\n### Attribute Information:\n\nmpg: continuous\ncylinders: multi-valued discrete\ndisplacement: continuous\nhorsepower: continuous\nweight: continuous\nacceleration: continuous\nmodel year: multi-valued discrete\norigin: multi-valued discrete\ncar name: string (unique for each instance)\n\n### Missing Attribute Values: \nhorsepower has 6 missing values\n\n","90fd2bea":"cylinders and origin can be categorical \n"}}