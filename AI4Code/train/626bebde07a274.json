{"cell_type":{"298d73cf":"code","f73926b4":"code","551d51cf":"code","89b8137e":"code","f6342830":"code","e4ebfaf3":"code","7398feaa":"code","2ded842b":"code","e3ac3709":"code","5bddc634":"code","b831692b":"code","551b563b":"code","d85b42ee":"code","922ce9a7":"code","dd8c73ca":"code","747b4218":"code","3f2797ac":"code","7ba48712":"code","6fc4f49d":"code","cb166d66":"code","1424232b":"code","6217112e":"code","b874bd05":"code","55729913":"code","c30aee92":"code","02881558":"code","2b3306d6":"code","e65a6b5b":"code","049f672a":"code","f3b05be9":"code","acc13cb5":"code","ba2199b9":"code","a9b29880":"code","07d93e39":"code","bb388ad7":"code","1855f029":"code","4f6f7331":"code","4a27483a":"code","59b7d30e":"code","786a8c83":"code","87973fdd":"code","4acd99bd":"code","8c806037":"code","38846eae":"code","7a564fec":"code","e20eaa52":"code","c8a4edef":"code","b96c846d":"code","726e3b00":"code","2bb9ebf8":"code","9c352d27":"code","48628741":"code","e7fbed9f":"code","32dd2234":"code","8caede04":"code","88cbafbd":"code","7954baf6":"code","e46310c3":"code","2bbc8ad9":"code","fc057fff":"code","f6323adf":"code","1c7f5b86":"code","c95d4376":"code","594b137d":"code","a1e3aa27":"code","8201168d":"code","64d509fb":"code","cbe79a15":"code","6da4fe36":"code","3de474ab":"code","e321161b":"code","ae97af9a":"code","4dcd0cb1":"code","dcaee259":"code","e2dcf8f7":"code","7f708849":"code","0f1019ce":"code","23e66284":"code","1e07f1f8":"code","eec779c0":"code","3c99d556":"code","3f583f77":"code","607514de":"code","2432f15b":"code","a6816d1c":"code","c059ac65":"code","4ebaff4f":"code","d690ba8d":"code","331ea3ec":"code","77342c1b":"code","44eeeeeb":"code","58a2b3ad":"code","c99c8e22":"code","c2ba8bfa":"code","ef3d4e32":"code","ce2e12f7":"code","fcb0a3c0":"code","25e01925":"code","07d556ae":"code","22bb9e2f":"code","bfb251a3":"code","631c95ff":"code","322f649f":"code","cd38eb9c":"code","f8db4fc7":"code","58e79742":"code","397563d1":"code","3d1cbaab":"code","31d0e79b":"code","2980340a":"markdown","45ecc541":"markdown","1d7f2e4b":"markdown","92292bef":"markdown","59f93a9b":"markdown","fb0e975e":"markdown","b1098845":"markdown","aa764a28":"markdown","3d3b5170":"markdown","94d30e5c":"markdown","63bb3703":"markdown","60a6cae7":"markdown","12150a85":"markdown","52249976":"markdown","538550b9":"markdown","7c1a97cf":"markdown","89637518":"markdown","0a49bac2":"markdown","118d61b4":"markdown","dc4cd9f6":"markdown","604a33f6":"markdown","fc716164":"markdown","cb4bf9c8":"markdown","e09de0cc":"markdown","f18a573a":"markdown","6996cc50":"markdown","5a6a1573":"markdown","2740e24c":"markdown","a549269c":"markdown","c52b886a":"markdown","388e6bb8":"markdown","df080677":"markdown","4f7406c3":"markdown","ee44f1de":"markdown","82b319e4":"markdown","356b2015":"markdown","464df066":"markdown","b247097c":"markdown","106d055e":"markdown","3ff5f6d8":"markdown","36525fdd":"markdown","3d5acb2d":"markdown"},"source":{"298d73cf":"import pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport numpy as np","f73926b4":"!ls ..\/input\/bod-results\/","551d51cf":"results = pd.read_csv('..\/input\/bod-results\/BOD Kaggle_results.csv',usecols=['Methodology','RMSE','Change log'])","89b8137e":"pd.set_option('display.max_colwidth', -1)","f6342830":"results['Methodology'] = results['Methodology'].str.replace('\\n','')","e4ebfaf3":"results","7398feaa":"df = pd.read_csv('..\/input\/prediction-bod-in-river-water\/train.csv')","2ded842b":"df.head()","e3ac3709":"test_data = pd.read_csv('..\/input\/prediction-bod-in-river-water\/test.csv')","5bddc634":"test_data.head()","b831692b":"test_data['Id'].unique()","551b563b":"df['Id'].unique()","d85b42ee":"df.describe()","922ce9a7":"corr =df[df.columns.to_list()].corr()","dd8c73ca":"corr","747b4218":"plt.figure(figsize=(10,8))\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns)","3f2797ac":"len(df)","7ba48712":"df.drop(columns='Id', inplace=True)","6fc4f49d":"sns.set()\nplt.figure(figsize=(14,10))\nsns.distplot(df['target'])\nplt.title('Distribution of data points in the dataset');","cb166d66":"lower,upper = np.percentile(df['target'],[1,99])\ndf['target'] = np.clip(df['target'],lower,upper)","1424232b":"sns.set()\nplt.figure(figsize=(14,10))\nsns.distplot(df['target'])\nplt.title('Distribution of data points in the dataset');","6217112e":"df.isna().sum()","b874bd05":"df.columns.to_list()[:3]","55729913":"df = df[df.columns.to_list()[:3]]","c30aee92":"df.isna().sum()","02881558":"nan_cols = df.isna().sum()[df.isna().sum()>0].index.to_list()","2b3306d6":"from sklearn.impute import KNNImputer","e65a6b5b":"imputer = KNNImputer(n_neighbors=5)","049f672a":"nan_cols","f3b05be9":"df[nan_cols] = imputer.fit_transform(df[nan_cols])","acc13cb5":"df.head()","ba2199b9":"df.max()","a9b29880":"df.min()","07d93e39":"corr = df[df.columns.to_list()].corr()","bb388ad7":"corr","1855f029":"df.columns.to_list()[1:3]","4f6f7331":"df['combined'] = df[df.columns.to_list()[1:]].mean(axis=1)","4a27483a":"df = df[['target','1','2','combined']]","59b7d30e":"corr = df[df.columns.to_list()].corr()\ndisplay(corr)\nplt.figure(figsize=(10,8))\nsns.heatmap(corr, \n        xticklabels=corr.columns,\n        yticklabels=corr.columns)","786a8c83":"from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor","87973fdd":"from sklearn.metrics import mean_squared_error,make_scorer","4acd99bd":"from sklearn.model_selection import cross_val_score, train_test_split","8c806037":"from skopt import forest_minimize","38846eae":"def rmse(y,y_pred):\n    return np.sqrt(mean_squared_error(y_true=y,y_pred=y_pred))","7a564fec":"scorer = make_scorer(rmse,greater_is_better=False)","e20eaa52":"feature_columns = df.columns.to_list()\ntarget_column = feature_columns.pop(0)\n(feature_columns,target_column)","c8a4edef":"y = df[target_column].values","b96c846d":"X = df[feature_columns].values","726e3b00":"def optimize_gbd(space):\n    alpha,learning_rate, max_depth,max_features,max_leaf_nodes,n_estimators= space\n    gbd = GradientBoostingRegressor(alpha=alpha,learning_rate=learning_rate, max_depth=max_depth,\n                                    max_features=max_features,max_leaf_nodes=max_leaf_nodes,\n                                    n_estimators=n_estimators,random_state=5, criterion='mse')\n    score =  -1*cross_val_score(gbd,X,y,cv=5,scoring=scorer).mean()\n    print('Error : {}'.format(score))\n    return score","2bb9ebf8":"def train_best_gbd(space):\n    alpha,learning_rate, max_depth,max_features,max_leaf_nodes,n_estimators= space\n    max_depth,max_features,max_leaf_nodes,n_estimators = int(max_depth), int(max_features), int(max_leaf_nodes), int(n_estimators)\n    gbd = GradientBoostingRegressor(alpha=alpha,learning_rate=learning_rate, max_depth=max_depth,\n                                    max_features=max_features,max_leaf_nodes=max_leaf_nodes,\n                                    n_estimators=n_estimators,random_state=5,\n                                   criterion='mse')\n    gbd.fit(X=X,y=y)\n    error = np.sqrt(mean_squared_error(y_pred=gbd.predict(X),y_true=y))\n    print('Overall error : {}'.format(error))\n    return gbd","9c352d27":"space = [(0.1,0.9),#alpha\n         (1e-3,0.8),#learning_rate\n         (2,20),#max_depth\n         (1,3),#max_features\n         (2,100),#max_leaf_nodes\n         (100,1000)#n_estimators\n    \n]","48628741":"best_params = forest_minimize(optimize_gbd,dimensions=space,n_calls=40,n_jobs=6,random_state=5)","e7fbed9f":"def to_df(best_params,cols=['alpha','learning_rate','max_depth',\n                            'max_features','max_leaf_nodes','n_estimators']):\n    params =  np.array(best_params['x_iters'])\n    df = pd.DataFrame(columns=cols,data=params)\n    df['scores'] = best_params['func_vals']\n    return df\n    ","32dd2234":"df_scores = to_df(best_params)","8caede04":"df_scores.head()","88cbafbd":"df_scores.loc[df_scores['scores'].argmin()]","7954baf6":"df_scores['scores'].median()","e46310c3":"df_scores['scores'].quantile(0.25)","2bbc8ad9":"by_percentile = df_scores[(df_scores['scores']>df_scores['scores'].quantile(0.45)) & (df_scores['scores']<df_scores['scores'].quantile(0.55))]","fc057fff":"by_percentile","f6323adf":"np.random.seed(14)\nchoice = np.random.choice(len(by_percentile))","1c7f5b86":"params = by_percentile.iloc[choice][['alpha','learning_rate','max_depth',\n                            'max_features','max_leaf_nodes','n_estimators']].values","c95d4376":"params","594b137d":"params = df_scores.loc[df_scores['scores'].argmin()].drop('scores').values","a1e3aa27":"gbd_trained = train_best_gbd(params)","8201168d":"ensemble_df = pd.DataFrame()\nensemble_df['gbd'] = gbd_trained.predict(X)","64d509fb":"ensemble_df.head()","cbe79a15":"sns.set()\nplt.figure(figsize=(14,10))\nplt.scatter(range(len(df)),df['target'],color='black')\nplt.plot(range(len(df)),ensemble_df['gbd'],color='blue')\nplt.legend(['Values predicted with GBT','Real values'])\nplt.xlabel('Timestamps')\nplt.ylabel('Target values')\nplt.title('Results of GBT model');","6da4fe36":"from sklearn.ensemble import ExtraTreesRegressor","3de474ab":"def optimize_extratree(space):\n    n_estimators,max_depth,min_samples_split,min_samples_leaf,max_features = space\n    extra_tree = ExtraTreesRegressor(min_samples_split=min_samples_split,max_features=max_features,random_state=5,\n                            min_samples_leaf=min_samples_leaf,max_depth=max_depth,n_estimators=n_estimators)\n    score =  -1*cross_val_score(extra_tree,X,y,cv=5,scoring=scorer).mean()\n    print('Error : {}'.format(score))\n    return score","e321161b":"def train_best_extratree(space):\n    n_estimators,max_depth,min_samples_split,min_samples_leaf,max_features = list(map(int,space))\n    extra_tree = ExtraTreesRegressor(min_samples_split=min_samples_split,max_features=max_features,random_state=5,\n                            min_samples_leaf=min_samples_leaf,max_depth=max_depth,n_estimators=n_estimators)\n    extra_tree.fit(X,y)\n    error = np.sqrt(mean_squared_error(y_true=extra_tree.predict(X),y_pred=y))\n    print('Overall error : {}'.format(error))\n    return extra_tree","ae97af9a":"space = [(100,1000),#n_estimators\n        (8,20),#max_depth\n        (2,4),#min_samples_split\n         (2,5),#min_samples_leaf\n        (1,3)#max_features\n        ]","4dcd0cb1":"best_params= forest_minimize(optimize_extratree,random_state=5,dimensions=space,n_calls=30)","dcaee259":"df_scores = to_df(best_params,cols=[\"n_estimators\",\"max_depth\",\"min_samples_split\",\n                                    \"min_samples_leaf\",\n                                    \"max_features\"])","e2dcf8f7":"df_scores.head()","7f708849":"df_scores['scores'].mean()","0f1019ce":"params = df_scores.iloc[2][[\"n_estimators\",\"max_depth\",\"min_samples_split\",\n                                    \"min_samples_leaf\",\n                                    \"max_features\"]].values","23e66284":"ex_tree_reg = train_best_extratree(params)","1e07f1f8":"ensemble_df['extra_tree'] = ex_tree_reg.predict(X)","eec779c0":"sns.set()\nplt.figure(figsize=(14,10))\nplt.scatter(range(len(df)),df['target'],color='black')\nplt.plot(range(len(df)),ensemble_df['extra_tree'],color='green')\nplt.legend(['Values predicted with ETR','Real values'])\nplt.xlabel('Timestamps')\nplt.ylabel('Target values')\nplt.title('Results of ETR model');","3c99d556":"from sklearn.linear_model import LinearRegression","3f583f77":"ensemble_df['y'] = y","607514de":"ensemble_df.head()","2432f15b":"ensemble_df.corr()","a6816d1c":"first_lvl_features = ensemble_df[['gbd','extra_tree']].values","c059ac65":"labels = ensemble_df['y'].values","4ebaff4f":"lr_second_lvl = LinearRegression()","d690ba8d":"-1*cross_val_score(lr_second_lvl,first_lvl_features,labels,scoring=scorer).mean()","331ea3ec":"lr_second_lvl.fit(first_lvl_features,labels)","77342c1b":"np.sqrt(mean_squared_error(y_pred=lr_second_lvl.predict(first_lvl_features),y_true=labels))","44eeeeeb":"sns.set()\nplt.figure(figsize=(14,10))\nplt.scatter(range(len(df)),df['target'],color='black')\nplt.plot(range(len(df)),lr_second_lvl.predict(first_lvl_features),color='red')\nplt.legend(['Final predictions with second level model','Real values'])\nplt.xlabel('Timestamps')\nplt.ylabel('Target values')\nplt.title('Results of predictions using second level model');","58a2b3ad":"sns.set()\nplt.figure(figsize=(14,10))\nplt.scatter(range(len(df)),df['target'],color='black')\nplt.plot(range(len(df)),ensemble_df['extra_tree'],color='green')\nplt.plot(range(len(df)),ensemble_df['gbd'],color='blue')\nplt.plot(range(len(df)),lr_second_lvl.predict(first_lvl_features),color='red')\nplt.legend(['Values predicted with ETR','Values predicted with GBT','Final predictions with second level model','Real values'])\nplt.xlabel('Timestamps')\nplt.ylabel('Target values')\nplt.title('Results of predictions using second level model');","c99c8e22":"def detect_outliers(df,constant=2):\n    outliers = df[(df['target']<=df['target'].mean()-constant*df['target'].std()) | (df['target']>=df['target'].mean()+constant*df['target'].std())]\n    return outliers","c2ba8bfa":"outliers = detect_outliers(df,1)","ef3d4e32":"sns.set()\nfrom matplotlib.lines import Line2D\nlegend_elements = [Line2D([0], [0], color='red', lw=4, label='Final predictions with second level model'),\n                   Line2D([0], [0], marker='o', label='Outliers in real values',\n                          markerfacecolor='orange',color='#999999', markersize=15),\n                   Line2D([0], [0], marker='o',color='#999999', label='Real values',\n                          markerfacecolor='black', markersize=15),\n                   ]\nplt.figure(figsize=(14,10))\nfor c,i in enumerate(df['target'].values):\n    if i in outliers['target'].values:\n        plt.scatter(c,i,color='orange',label='Outliers in real values')\n    else:\n        plt.scatter(c,i,color='black',label='Real values')\nplt.plot(range(len(df)),lr_second_lvl.predict(first_lvl_features),color='red')\nplt.legend(handles=legend_elements)\nplt.xlabel('Timestamps')\nplt.ylabel('Target values')\nplt.title('Results of predictions using second level model');","ce2e12f7":"sns.set()\nplt.figure(figsize=(14,10))\nplt.scatter(range(len(df))[-24:],df['target'][-24:],color='black')\nplt.plot(range(len(df))[-24:],lr_second_lvl.predict(first_lvl_features)[-24:],color='red')\nplt.legend(['Final predictions with second level model','Real values'])\nplt.xlabel('Monthes')\nplt.ylabel('Target values')\nplt.title('Results of predictions using second level model (showing data only for last two years)')\nplt.yticks(np.arange(min(df['target'][-24:]),max(df['target'][-24:])+0.5,0.5))\nplt.xticks(range(len(df))[-24:],range(1,25));","fcb0a3c0":"sub_df = pd.read_csv(\"..\/input\/prediction-bod-in-river-water\/test.csv\",usecols=['Id','1','2'])","25e01925":"sub_df.head()","07d556ae":"sub_df.isna().sum()","22bb9e2f":"sub_df.head()","bfb251a3":"sub_df['combined'] = sub_df[sub_df.columns.to_list()[1:]].mean(axis=1)","631c95ff":"sub_df.head()","322f649f":"feature_columns = sub_df.columns.to_list()[1:]","cd38eb9c":"ensemble_df_test = pd.DataFrame()\nensemble_df_test['gbd'] = gbd_trained.predict(sub_df[feature_columns])\nensemble_df_test['extra_tree'] = ex_tree_reg.predict(sub_df[feature_columns])","f8db4fc7":"ensemble_df_test.head()","58e79742":"sub_df['Predicted'] = lr_second_lvl.predict(ensemble_df_test.values)","397563d1":"sub_df = sub_df[['Id','Predicted']]","3d1cbaab":"sub_df.to_csv('submission.csv',index=False)","31d0e79b":"sub_df['Predicted'].values","2980340a":"# Feature engineering","45ecc541":"Let's check our dataset for missing values. It's obvious that the feature columns from 3 to 7 \nconsist of mostly NaN values. Thus it's better to drop them.","1d7f2e4b":"Finally, we can show the linear regression predictions over \"stacked\" features.  ","92292bef":"We won't need an Id column for our predictions.","59f93a9b":"We hope, that this notebook was useful for you.","fb0e975e":"From the plot below it's obvious that the biggest correlation is between first two features and our target value.","b1098845":"Let's also visualize the predictions of our model and real y values with respect to timestamp.","aa764a28":"We see that the correlation between new feature and target value is also big enough, thus we will use it for models training.","3d3b5170":"# Part 0 : Description of the used approaches and corresponding scores.","94d30e5c":"Now we are ready to train our first model in an ensemble.","63bb3703":"We are now ready to make the final submission using our model stacking technique.","60a6cae7":"We then can choose the random experiment. We will set a constant seed for reproducability.","12150a85":"Let's show the outliers which weren't covered by my model.","52249976":"# Making gbd model and training ","538550b9":"We still have some NaN values in first two features, thus we will impute them using KNN.","7c1a97cf":"We will store the predictions of the gbd in ensemble_df as the first feature for the second lvl model.","89637518":"Function <b>optimize_gbd<\/b> - finds the best hyperparameters for GradientBoostingRegressor in a defined space with respect to averaged by cross validation with kfold of 5 rmse score. ","0a49bac2":"We can already choose the middle accurate model, that has 1.458386 cross val score.","118d61b4":"The best model is not the right choice as it's too complex. The better choice is to use the middle accurate model.\nFor this purprose we will choose a the subarray of experiments, which consists of middle accurate models.","dc4cd9f6":"As the score is validated using rmse, we will implement it and use as a scorer for cross validation.","604a33f6":"# Gathering everything togather","fc716164":"# Part 2 : first level models training","cb4bf9c8":"Let's also create a new feature which is just an average of the availbale ones.","e09de0cc":"Finnaly we can predict the final value.","f18a573a":"# ExtraTree train","6996cc50":"# Reading of data and basic statistics","5a6a1573":"Given best parameters, <b>function train_best_gbd<\/b> trains the best model.","2740e24c":"The second and last model we will use in ensemble is ExtraTreesRegressor. We will also tune this model with respect to average cross val score with kfold of 5 using rmse as the scoring function.","a549269c":"Function <b>to_df<\/b> casts our experiments to data frame instance.","c52b886a":"# Part 1 : EDA","388e6bb8":"# Making submission","df080677":"We tend to have a different correlation between gbd and extra_tree predictions with target value, which is  good for overall generalization.","4f7406c3":"We will also analyse the distribution of our dataset w.r.t target value.","ee44f1de":"As there are lot's of parameters that we tune, we will make 200 calls to forest minimize optimizer to find the best ones.","82b319e4":"We still need to create the additional feature.","356b2015":"# Part 3 : Second level model training","464df066":"space - the space of parameters we will seek for best ones in","b247097c":"First of all we will train a gbd model.","106d055e":"We will also visualize results of Extra Trees.","3ff5f6d8":"# Imputation of missing values","36525fdd":"As we now have two features from both gbd and extra tree, we can train a linear regression using y as target. We use a linear regession here because the relationship is simple enough, thus we don't need to use a more complex model.","3d5acb2d":"We now will choose the best hyperparameters with respect to our current choice."}}