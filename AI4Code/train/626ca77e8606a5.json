{"cell_type":{"06dd0bf0":"code","68751a88":"code","682ae08c":"code","e748388c":"code","757b3032":"code","bbf83369":"code","f14340b8":"code","17105267":"code","ab6d4862":"code","b929ad12":"code","334ef40f":"code","7c618ebe":"code","8ab9c836":"code","6c6c6402":"code","082f9e67":"code","16ee4de1":"code","df3bbf5e":"code","7e0eb50b":"code","806004f5":"code","57afebc2":"code","94a1ea3e":"code","600473b6":"code","1d92bf64":"code","a6bd09d6":"code","5c50373d":"code","5e5d8d93":"code","1042f204":"code","c3e448e1":"code","95141c4a":"code","2d200980":"code","d0f1a627":"code","2a2d3fe4":"code","a9fab24e":"code","a277a460":"code","e0c8d4b6":"code","4ad7c867":"code","8510f6ea":"code","f0a37cf2":"code","8174566a":"code","125e2ec9":"code","10d0d52c":"code","e2d3fb68":"code","2e390ac7":"code","9dc9018c":"code","f86afe48":"code","0d364b33":"code","5e6df770":"code","5dd18a94":"code","954706fa":"code","eba652f3":"code","1b50837e":"code","c4891c9c":"code","91991189":"code","43bff32f":"code","fcb4ffce":"code","2449e523":"code","300132fa":"code","391c82e6":"code","01429961":"code","13e836f7":"code","80cdbb2c":"code","6e318d67":"code","78064133":"code","def031f0":"code","704e9fab":"code","8488eca3":"code","4681e2fe":"code","296d044f":"code","2d6ba29d":"code","718b8381":"markdown","a2ea9e8c":"markdown","bb789563":"markdown","8f6e1ddc":"markdown","f79bf66c":"markdown","160b8f34":"markdown","6cd7f508":"markdown","ffc45bb1":"markdown","a0b91925":"markdown","7c78eddf":"markdown","0a0dd48b":"markdown","7805d822":"markdown","e4882b2c":"markdown","2681f792":"markdown","e41e875c":"markdown","bcfe9a69":"markdown","6402079f":"markdown","ec184726":"markdown","e2fde767":"markdown","6956c043":"markdown","61690052":"markdown","8bd799ae":"markdown","403fa927":"markdown","74c32b8e":"markdown","ac35c552":"markdown","d3049b5d":"markdown","19869722":"markdown","9bbc7d4a":"markdown","4d71a395":"markdown","3d54eca8":"markdown","126afaec":"markdown","1bd3c32d":"markdown","a775f63d":"markdown","81536345":"markdown","29e67601":"markdown","750265e0":"markdown","c7b1192c":"markdown","e2e34ef4":"markdown","a407c344":"markdown","77320975":"markdown","50d51f93":"markdown","96baef24":"markdown","46841331":"markdown","eb6e977e":"markdown","99974bf5":"markdown","2d1eb61b":"markdown","25102281":"markdown","85efd376":"markdown","bca70190":"markdown","1f1cb8b4":"markdown","f3e78e1e":"markdown","28fd3566":"markdown","806efc24":"markdown"},"source":{"06dd0bf0":"#Load Packages\nimport numpy as np # linear algebra\nimport matplotlib as mpl\nimport pandas as pd\nimport seaborn as sns\nimport re \nfrom IPython.display import display_html\nimport itertools\nimport math\nimport random\nfrom tqdm import tqdm_notebook as tqdm\nfrom sklearn.metrics import *\nimport matplotlib.pyplot as plt\nimport matplotlib\n\n#Load sklearn\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import preprocessing\nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import linear_model\n\n# Import Classifiers\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Perceptron\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC, LinearSVC\nfrom sklearn.naive_bayes import GaussianNB\n\n#Learning curve\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import f1_score\nfrom sklearn.model_selection import learning_curve\nfrom sklearn.model_selection import ShuffleSplit\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import validation_curve\n\nfrom IPython.display import display_html\nimport warnings\n\n# for inline plots\n%matplotlib inline\nwarnings.filterwarnings('ignore')\n\nmpl.rcParams['figure.figsize'] = (8, 6)\nplt.rcParams[\"legend.fontsize\"] = 15\nplt.rcParams[\"axes.labelsize\"] = 15\nmpl.rc('xtick', labelsize = 15) \nmpl.rc('ytick', labelsize = 15)\nsns.set(style = 'whitegrid', palette = 'muted', font_scale = 2)\n    \nprint('Libraries Imported')","68751a88":"# get data from csv files\ntest  = pd.read_csv('..\/input\/test.csv')\ntrain = pd.read_csv('..\/input\/train.csv')\n\n#determine sizes of datasets\nn_train, m_train = train.shape\nn_test, m_test = test.shape\n\n\n# divide into X and y data\nX_train = pd.DataFrame(train.iloc[:,1: m_train])\ny_train = pd.DataFrame(train.iloc[0:, 1])\n\nX_test_original  = test\nX_test = test\n\nprint('Data Imported')","682ae08c":"# determint the size of the data sets\n\n# print a summary of loaded results\nprint('FULL DATA')\nprint('Number of features (m): %.0f'%(m_train))\nprint('Number of traing samples (n): %.0f'%(n_train))\n\nprint('\\n\\nTest DATA')\nprint('Number of features (m): %.0f'%(m_test))\nprint('Number of traing samples (n): %.0f'%(n_test))\n\ncnt = 0\n# print out the features\nprint('\\n\\nFeatures: ')\nfor feature in X_train.columns:\n    cnt += 1\n    print('%d. '%(cnt), feature,'\\t\\t')\n","e748388c":"# take a sample of what the data looks like\nX_train.head(10)","757b3032":"#sets up the parametes for plotting.. size and font\ndef PlotParams(Font, sizex, sizey):\n    mpl.rcParams['figure.figsize'] = (sizex,sizey)\n    plt.rcParams[\"legend.fontsize\"] = Font\n    plt.rcParams[\"axes.labelsize\"] = Font\n    mpl.rc('xtick', labelsize = Font) \n    mpl.rc('ytick', labelsize = Font)\n\n#sets up Seaborn parametes for plotting\ndef snsParams(font, colour_scheme):\n    #eaborn.set(context='notebook', style='darkgrid', palette='deep', font='sans-serif', font_scale=1, color_codes=True, rc=None)\n    sns.set(style = 'whitegrid', palette = colour_scheme, font_scale = font)\n\n#determined ht emissing data\ndef Missing (X):\n    total = X.isnull().sum().sort_values(ascending = False)\n    percent = round(X.isnull().sum().sort_values(ascending = False)\/len(X)*100, 2)\n    missing = pd.concat([total, percent], axis = 1,keys= ['Total', 'Percent'])\n    return(missing) \n\n#plots number of dataframes side by side\ndef SideSide(*args):\n    html_str=''\n    for df in args:\n        html_str+=df.to_html()\n    display_html(html_str.replace('table','table style=\"display:inline\"'),raw = True)\n\n#makes heat map of correllations\ndef PlotCorr(X):\n    corr = X.corr()\n    #fig , ax = plt.figure( figsize = (6,6 ) )\n    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n    sns.heatmap(\n        corr, cmap = cmap, square = True, cbar = False, cbar_kws = { 'shrink' : 1 }, \n     annot = True, annot_kws = { 'fontsize' : 14 }\n    )\n    plt.yticks(rotation = 0)\n    plt.xticks(rotation = 90) \n    \n#plot top correlatins in a heat map\ndef TopCorr(X, lim):\n    corr = X.corr()\n    cmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\n    #fig , ax = plt.subplots( figsize = (6,6 ) )\n    sns.heatmap(corr[(corr >= lim) | (corr <= -lim)], \n         vmax = 1.0,  cmap = cmap, vmin = -1.0, square = True, cbar = False, linewidths = 0.2, annot = True, \n                annot_kws = {\"size\": 14})\n    plt.yticks(rotation = 0)\n    plt.xticks(rotation = 90)","bbf83369":"# provide information about the types of data we are dealing with\nprint('ORIGINAL TRAINING DATA:')\nX_train.info()\n\nprint('\\n\\n\\nORIGINGAL TEST DATA:')\nX_test.info()\n\n#summarise the types of data\nprint('\\ndata types of features:')\n\ncnt = 0\nd_type = ['float64', 'int64','object','dtype']\nprint('\\n\\tTRAIN \\t\\t TEST')\nfor c1, c2 in zip(X_train.get_dtype_counts(), X_test.get_dtype_counts()):\n    cnt += 1\n    print(\"%s:\\t%-9s \\t%s\"%(d_type[cnt],c1, c2))\n    ","f14340b8":"X_train.describe(include = \"all\")","17105267":"# Fill empty values with NaN\nX_train = X_train.fillna(np.nan)\nX_test = X_test.fillna(np.nan)\n\n#finds missing values\nmissing_train = Missing(X_train)\nmissing_test = Missing(X_test)\n    \nprint('TRAIN DATA','\\t\\t','TEST DATA')\nSideSide(missing_train, missing_test)\n\n#plot missing data in heatmap for visualisation\nprint('\\n\\n  MISSING TRAINING DATA \\t\\t\\t MISSING TEST DATA')\ncmap = sns.diverging_palette( 220 , 10 , as_cmap = True )\nplt.figure(figsize = (10,5));\nplt.subplot(1, 2, 1)\nsns.heatmap(X_train.isnull(), yticklabels = False, cbar = False, cmap = cmap)\nplt.subplot(1, 2, 2)\nsns.heatmap(X_test.isnull(), yticklabels = False, cbar = False,cmap = cmap);","ab6d4862":"#show the correlations between all the featured in a heatmap\nplt.figure(figsize = (20,6))\nplt.subplot(1,2,1)\nPlotCorr(X_train);\nplt.subplot(1,2,2)\nTopCorr(X_train, 0.2)","b929ad12":"# highest correlated with correlation of features with 'Survived'\nprint('Featured hights correlation with survival')\nprint('Feature\\tCorrelation')\nSurvive_Corr = X_train.corr()[\"Survived\"]\nSurvive_Corr = Survive_Corr[1:9] # remove the 'Survived'\nSurvive_Corr= Survive_Corr[np.argsort(Survive_Corr, axis = 0)[::-1]] #sort in descending order\nprint(Survive_Corr)","334ef40f":"# Plot the top correlationin a bar chart for east visualisation.\nwidth = 0\nfig, ax = plt.subplots(figsize = (10,6))\nrects = ax.barh(np.arange(len(Survive_Corr)), np.array(Survive_Corr.values), color = 'red')\nax.set_yticks(np.arange(len(Survive_Corr)) + ((width)\/1))\nax.set_yticklabels(Survive_Corr.index, rotation ='horizontal')\nax.set_xlabel(\"Correlation coefficient\")\nax.set_title(\"Correlation Coefficients w.r.t Survival\",fontsize = 14);\nax.grid(True)","7c618ebe":"snsParams(2, 'muted')\n# plot survival count for male and female\nplt.figure(figsize = (20,5))\nplt.subplot(1, 3, 1)\nb = sns.countplot(x = 'Survived',hue = 'Sex', data = X_train);\nb.set_xlabel(\"Survived\",fontsize = 15)\nb.set_ylabel(\"Count\",fontsize = 15)\nb.legend(fontsize = 14)\nsnsParams(1.5, 'muted')\n\n#survival probability of males and females\nplt.subplot(1, 3, 2)\ng = sns.barplot(x = \"Sex\", y = \"Survived\",data = X_train)\ng = g.set_ylabel(\"Survival Probability\")\n\nplt.subplot(1, 3, 3)\nsns.violinplot(y = 'Survived', x = 'Sex', data = X_train, inner = 'quartile')\n","8ab9c836":"# plot survival number for age dependandcy\nfig, axes = plt.subplots(figsize = (20,6), nrows = 1, ncols = 3)\n\ng = sns.distplot(X_train[X_train['Survived'] == 1].Age.dropna(), bins=20, label = 'Survived')\ng = sns.distplot(X_train[X_train['Survived'] == 0].Age.dropna(), bins=20, label = 'Not Survived')\n\ng = sns.kdeplot(X_train[\"Age\"][(X_train[\"Survived\"] == 0) & (X_train[\"Age\"].notnull())], color = \"Green\", shade = False)\ng = sns.kdeplot(X_train[\"Age\"][(X_train[\"Survived\"] == 1) & (X_train[\"Age\"].notnull())], ax = g, color = \"Blue\", shade= False)\n\ng.set_xlabel(\"Age\",fontsize = 15)\ng.set_ylabel(\"Frequency\",fontsize = 15)\ng = g.legend([\"Not Survived\",\"Survived\"],fontsize = 15)\nplt.xlim(0,80)\nplt.ylim(0,0.04)\nplt.grid(True)\n\nwomen = X_train[X_train['Sex'] == 'female']\nmen = X_train[X_train['Sex'] == 'male']\n\n#For womwn\nax = sns.distplot(women[women['Survived'] == 1].Age.dropna(), bins = 20, label = 'survived', ax = axes[0], kde = False)\nax = sns.distplot(women[women['Survived'] == 0].Age.dropna(), bins = 20, label = 'not survived', ax = axes[0], kde = False)\nax.set_xlabel(\"Age\",fontsize = 15)\nax.set_ylabel(\"Count\",fontsize = 15)\nax.legend(fontsize = 15)\nax.set_title('Female', fontsize = 15)\nax.set(xlim = (0, X_train['Age'].max()));\nax.set(ylim = (0, 50));\n    \n    \n#For men\nax = sns.distplot(men[men['Survived'] == 1].Age.dropna(), bins = 20, label = 'survived', ax = axes[1], kde = False)\nax = sns.distplot(men[men['Survived'] == 0].Age.dropna(), bins = 20, label = 'not survived', ax = axes[1], kde = False)\nax.set_xlabel(\"Age\",fontsize = 15)\nax.set_ylabel(\"Count\",fontsize = 15)\nax.legend(fontsize = 15)\nax.set_title('Male', fontsize = 15)\nax.set(xlim = (0, X_train['Age'].max()))\nax.set(ylim = (0, 50));\n\ng = sns.factorplot(x = \"Survived\", y = \"Age\",data = X_train, kind=\"box\")\ng = sns.factorplot(x = \"Survived\", y = \"Age\",data = X_train, kind=\"violin\")\n\n","6c6c6402":"plt.figure(figsize = (16,6))\nplt.subplot(1, 3, 1)\nsns.barplot(x = 'Pclass', y = 'Survived', data = X_train)\n\n# Explore Pclass vs Survived by Sex\nplt.subplot(1, 3, 2)\ng = sns.barplot(x = \"Pclass\", y = \"Survived\", hue = \"Sex\", data = X_train)\n#g = g.set_ylabels(\"survival probability\")\n\nplt.subplot(1, 3, 3)\nsns.countplot(x = 'Survived',hue = 'Pclass',data = X_train);\n\nplt.figure(figsize = (16,6))\nplt.subplot(1, 2, 1)\nsns.violinplot(y = 'Survived', x = 'Pclass', data = X_train, inner = 'quartile')\nplt.subplot(1, 2, 2)\nsns.violinplot(x='Pclass', y = 'Age', hue = 'Survived', data = X_train, split = True)\n\n\nax = sns.factorplot(y = \"Age\", x = \"Pclass\", hue = \"Sex\", data = X_train, kind = \"box\")\nsns.factorplot(y = \"Age\", x = \"Sex\", hue = \"Pclass\", data = X_train, kind = \"box\")\n\n","082f9e67":"# grid = sns.FacetGrid(train_df, col='Pclass', hue='Survived')\ngrid = sns.FacetGrid(X_train, col = 'Survived', row = 'Pclass', size = 3.2, aspect = 1.2)\ngrid.map(plt.hist, 'Age', alpha = 0.8, bins=20)\ngrid.add_legend();","16ee4de1":"PlotParams(15,6,6)\nX_train.Age[X_train.Pclass == 1].plot(kind = 'kde')    \nX_train.Age[X_train.Pclass == 2].plot(kind = 'kde')\nX_train.Age[X_train.Pclass == 3].plot(kind = 'kde')\n # plots an axis lable\nplt.xlabel(\"Age\")    \nplt.title(\"Age Distribution within classes\", fontsize = 15)\n# sets our legend for our graph.\nplt.legend(('1st Class', '2nd Class','3rd Class'), loc = 'best') ;\nplt.xlim(0,80)\nplt.ylim(0,0.04)","df3bbf5e":"# Explore Embarked vs Survived \nplt.figure(figsize = (16,6))\nplt.subplot(1, 3, 1)\ng = sns.barplot(x = \"Embarked\", y = \"Survived\",  data = X_train)\n\n# Explore Pclass vs Survived by Sex\nplt.subplot(1, 3, 2)\ng = sns.barplot(x = \"Embarked\", y = \"Survived\", hue = \"Sex\", data = X_train)\n#g = g.set_ylabels(\"survival probability\")\nplt.subplot(1, 3, 3)\nsns.countplot(x = 'Survived',hue = 'Embarked',data = X_train);","7e0eb50b":"sns.factorplot(y = \"Age\", x = \"Embarked\", hue = \"Pclass\", data = X_train, kind = \"box\")","806004f5":"# Explore Pclass vs Embarked \nPlotParams(15, 8, 6)\nsnsParams(2,'muted')\n\ng = sns.factorplot(\"Pclass\", col = \"Embarked\",  data = X_train, size = 8, \n                   kind = \"count\", palette = \"muted\")\ng = g.set_ylabels(\"Count\")\ng = sns.factorplot(\"Pclass\", col = \"Embarked\",  data = X_train,\n                   hue = \"Sex\", size = 8, kind = \"count\", palette = \"muted\")\n\ng = g.set_ylabels(\"Count\")\n","57afebc2":"PlotParams(15, 10, 6)\nplt.figure(figsize = (16,5))\nplt.subplot(1, 2, 1)\ng = sns.barplot(x = \"Parch\", y = \"Survived\",  data = X_train, palette = \"muted\")\nplt.subplot(1, 2, 2)\ng = sns.barplot(x = \"SibSp\", y = \"Survived\",  data = X_train, palette = \"muted\")\n\nplt.figure(figsize=(20,5))\nplt.subplot(1, 2, 1)\nsns.violinplot(y = 'Survived', x = 'Parch', data = X_train, inner = 'quartile')\nplt.subplot(1, 2, 2)\nsns.violinplot(y = 'Survived', x = 'SibSp', data = X_train, inner = 'quartile')","94a1ea3e":"PlotParams(15, 8, 6)\nplt.figure()\nsns.kdeplot(X_train[\"Fare\"][X_train.Survived == 1])\nsns.kdeplot(X_train[\"Fare\"][X_train.Survived == 0])\nplt.legend(['Survived', 'Died'])\nplt.xlabel('Fare')\nplt.ylabel('Survival Probability')\n# limit x axis to zoom on most information. there are a few outliers in fare. \nplt.xlim(0,200)\nplt.ylim(0,.060)\nplt.show()\n\n\nfig, ax = plt.subplots(figsize=(16,4),ncols=2)\nax1 = sns.boxplot(x = \"Embarked\", y = \"Fare\", hue = \"Pclass\", data = X_train, ax = ax[0]);\nax2 = sns.boxplot(x = \"Embarked\", y = \"Fare\", hue = \"Pclass\", data = X_test, ax = ax[1]);\nax1.set_title(\"Training Set\", fontsize = 15)\nax2.set_title('Test Set',  fontsize = 15)\nfig.show()","600473b6":"#combine the tets and training data so that operations can be performed together\nfull_data = [X_train, X_test] ","1d92bf64":"#fill in Embarked datta with S as it is the most common\nfor X in full_data:\n    X['Embarked'] = X['Embarked'].fillna(\"S\")","a6bd09d6":"X_train.head()","5c50373d":"# cabine Vrs no cabine survival rates\nfor X in full_data:\n    X[\"CabinBool\"] = (X[\"Cabin\"].notnull().astype('int'))\n    \n#draw a bar plot of CabinBool vs. survival\nsns.barplot(x = \"CabinBool\", y = \"Survived\", data = X_train)\nplt.show()","5e5d8d93":"# Extract deck \ndef extract_cabin(x):\n    return x != x and 'Other' or x[0]\n\nfor X in full_data:\n    X['Cabin'] = X['Cabin'].apply(extract_cabin)\n    X['Deck'] = X['Cabin']\n\ntrain_deck = pd.DataFrame(X_train.groupby('Deck').size())\ntest_deck = pd.DataFrame(X_test.groupby('Deck').size())\n\nprint('TRAIN \\t\\t TEST')\nSideSide(train_deck,test_deck )","1042f204":"snsParams(1.2, 'muted')\nplt.figure(figsize = (16,5))\n\nplt.subplot(1, 3, 1)\ng = sns.countplot(X_train[\"Cabin\"], palette = \"muted\")\nplt.subplot(1, 3, 2)\ng = sns.barplot(x = \"Deck\", y = \"Survived\",  data = X_train, palette = \"muted\")\n\nplt.subplot(1, 3, 3)\nsns.countplot(x = 'Survived',hue = 'Deck',data = X_train, palette = \"muted\");\n\nsnsParams(2, 'muted')\nplt.figure(figsize = (16,5))\ng = sns.factorplot(\"Deck\", col = \"Pclass\",  data = X_train, size = 8, \n                   kind = \"count\", palette = \"muted\")\ng = g.set_ylabels(\"Count\")\ng = sns.factorplot(\"Deck\", col = \"Embarked\",  data = X_train,\n                   hue = \"Sex\", size = 8, kind = \"count\", palette = \"muted\")\ng = g.set_ylabels(\"Count\")","c3e448e1":"# To get the full family size of a person, added siblings and parch.\n\nPlotParams(15, 8, 6)\n# determine size of family on board\nfor X in full_data:\n    X['Family'] = X['SibSp'] + X['Parch'] + 1 \n    \naxes = sns.factorplot('Family','Survived', hue = 'Sex', data = X_train, aspect = 2)\nplt.grid(True)\naxes = sns.factorplot('Family','Survived',  data = X_train, aspect = 2)\nplt.grid(True)\n\nfor X in full_data:\n    X['Alone'] = [1 if i<2 else 0 for i in X['Family']]\n    \n\nfig, (axis1,axis2,axis3) = plt.subplots(1,3,figsize=(18,6))\nsns.barplot(x = \"Family\", y = \"Survived\", hue = \"Sex\", data = X_train, ax = axis1);\nsns.barplot(x = \"Alone\", y = \"Survived\", hue = \"Sex\", data = X_train, ax = axis2);\nsns.barplot(x = \"Alone\", y = \"Survived\", data = X_train)\nplt.show()\n   ","95141c4a":"# Explore Age vs Sex, Parch , Pclass and SibSP\nsns.factorplot(y = \"Age\", x = \"Sex\", data = X_train, kind = \"box\")\nsns.factorplot(y = \"Age\", x = \"Sex\", hue = \"Pclass\", data = X_train, kind = \"box\")\nsns.factorplot(y = \"Age\", x = \"Parch\", data = X_train, kind = \"box\", palette = \"muted\")\nsns.factorplot(y = \"Age\", x = \"SibSp\", data = X_train, kind = \"box\", palette = \"muted\")","2d200980":"\nPlotCorr(X_train[[\"Age\",\"Sex\",\"SibSp\",\"Parch\",\"Pclass\",'Family','Alone', 'Fare']])\n\n#correlation of features with target variable\nAge_Corr = X_train.corr()[\"Age\"]\n#Age_Corr= Age_Corr[np.argsort(Age_Corr, axis = 0)[::-1]] #sort in descending order\nAge_Corr = Age_Corr[1:10] # remove the 'Survived'\nprint(Age_Corr)\n","d0f1a627":"#Filling missing value of Age \n\n## Fill Age with the median age of similar rows according to Pclass, Parch and SibSp\n# Index of NaN age rows\n\ndef ReplaceAge(X):\n    index_NaN_age = list(X[\"Age\"][X[\"Age\"].isnull()].index)\n\n    for i in index_NaN_age :\n        age_med = X[\"Age\"].median()\n        \n        age_pred = X[\"Age\"][((X['SibSp'] == X.iloc[i][\"SibSp\"]) & \n                                        (X['Parch'] == X.iloc[i][\"Parch\"]) &\n                                        (X['Pclass'] == X.iloc[i][\"Pclass\"]) &\n                                        (X['Family'] == X.iloc[i][\"Family\"]) &\n                                        (X['Alone'] == X.iloc[i][\"Alone\"]) &\n                                         (X['Alone'] == X.iloc[i][\"Alone\"])\n                                        )].median()\n        if not np.isnan(age_pred) :\n            X['Age'].iloc[i] = age_pred\n        else :\n            X['Age'].iloc[i] = age_med\n    return (X)\n\nfor X in tqdm(full_data):\n     X = ReplaceAge(X)\n    \nprint('Done')","2a2d3fe4":"#sort the ages into logical categories\n## create bins for age\ndef AgeCategory(age):\n    a = ''\n    if age <= 3:\n        a = 'Baby'\n    elif age <= 12: \n        a = 'Child'\n    elif age <= 18:\n        a = 'Teenager'\n    elif age <= 35:\n        a = 'Young Adult'\n    elif age <= 65:\n        a = 'Adult'\n    elif age == 'NaN':\n        a = 'NaN'\n    else:\n        a = 'Senior'\n    return a\n        \nfor X in full_data:\n    X['Age Group'] = X['Age'].map(AgeCategory)\n\n\nplt.figure(figsize = (16,6))\nplt.subplot(1, 3, 1)\ng = sns.barplot(x = \"Age Group\", y = \"Survived\",  data = X_train)\nplt.xticks(rotation = 90)\n\nplt.subplot(1, 3, 2)\nsns.countplot(x = 'Survived', hue = 'Age Group',data = X_train)\n\nplt.subplot(1, 3, 3)\nsns.boxplot(data = X_train, x = \"Age Group\", y = \"Age\");\nplt.xticks(rotation = 90)","a9fab24e":"# fill missing Fare with median fare for each Pclass\nfor X in full_data:\n    X[\"Fare\"].fillna(X.groupby(\"Pclass\")[\"Fare\"].transform(\"median\"), inplace = True)\n    \nfor X in full_data:\n    X.loc[ X['Fare'] <= 7.91, 'Fare'] = 0\n    X.loc[(X['Fare'] > 7.91) & (X['Fare'] <= 14.454), 'Fare'] = 1\n    X.loc[(X['Fare'] > 14.454) & (X['Fare'] <= 31), 'Fare']   = 2\n    X.loc[(X['Fare'] > 31) & (X['Fare'] <= 99), 'Fare']   = 3\n    X.loc[(X['Fare'] > 99) & (X['Fare'] <= 250), 'Fare']   = 4\n    X.loc[X['Fare'] > 250, 'Fare'] = 5\n    X['Fare'] = X['Fare'].astype(int)","a277a460":"def get_title(name):\n    title_search = re.search(' ([A-Za-z]+)\\.', name)\n    # If the title exists, extract and return it.\n    if title_search:\n        return title_search.group(1)\n    return \"\"\n\n# Create a new feature Title, containing the titles of passenger names\nfor X in full_data:\n    X['Title'] = X['Name'].apply(get_title)\n    \n# Group all non-common titles into one single grouping \"Rare\"\nfor X in full_data:\n    X['Title'] = X['Title'].replace(['Lady', 'Countess', 'Don', 'Sir', 'Jonkheer', 'Dona'], 'Noble')\n    X['Title'] = X['Title'].replace(['Capt', 'Col', 'Dr', 'Major', 'Rev'], 'Officer')\n    X['Title'] = X['Title'].replace('Mlle', 'Miss')\n    X['Title'] = X['Title'].replace('Ms', 'Miss')\n    X['Title'] = X['Title'].replace('Mme', 'Mrs')\n\n    \nprint('TRAIN TITLE \\t TEST TITLES')\ntrain_titles = pd.DataFrame(X_train.Title.value_counts())\ntest_titles = pd.DataFrame(X_test.Title.value_counts())\n\nSideSide(train_titles,test_titles)\n\nplt.figure(figsize = (16,6))\nplt.subplot(1, 3, 1)\ng = sns.barplot(x = \"Title\", y = \"Survived\",  data = X_train)\nplt.xticks(rotation = 90)\n\nplt.subplot(1, 3, 2)\nsns.countplot(x = 'Survived', hue = 'Title',data = X_train);\nplt.xticks(rotation = 90)\n\nplt.subplot(1, 3, 3)\nsns.boxplot(data = X_train, x = \"Title\", y = \"Age\");\nplt.xticks(rotation = 90)\n\ntab = pd.crosstab(X_train['Title'], X_train['Pclass'])\ntab_prop = tab.div(tab.sum(1).astype(float), axis=0)\n\ntab_prop.plot(kind = \"bar\", stacked = True)\nplt.xticks(rotation = 90)","e0c8d4b6":"#map each Sex value to a numerical value\nsex_map = {\"male\": 0, \"female\": 1}\nEmbark_map = {\"C\": 1,\"S\": 2, \"Q\": 3}\ndeck_map = {\"A\": 1, \"B\": 2, \"C\": 3, \"D\": 4, \"E\": 5, \"F\": 6, \"G\": 7, \"T\": 8, \"Other\": 9}\nage_map = {\"Baby\": 1, \"Child\": 2, \"Teenager\": 3, \"Young Adult\": 4, \"Adult\": 5, \"Senior\": 6}\ntitle_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Officer\": 5, \"Noble\": 5}\n\nfor X in full_data:\n    X[\"Sex\"] = X[\"Sex\"].map(sex_map)\n    X[\"Embarked\"] = X[\"Embarked\"].map(Embark_map)\n    X[\"Deck\"] = X[\"Deck\"].map(deck_map)\n    X[\"Age Group\"] = X[\"Age Group\"].map(age_map)\n    X[\"Title\"] = X[\"Title\"].map(title_mapping)","4ad7c867":"X_train = X_train.drop(\"Name\", axis = 1) \nX_test = X_test.drop(\"Name\", axis = 1) \nX_train = X_train.drop(\"Ticket\", axis = 1) \nX_test = X_test.drop(\"Ticket\", axis = 1) \nX_train = X_train.drop(\"Cabin\", axis = 1) \nX_test = X_test.drop(\"Cabin\", axis = 1) \nX_train = X_train.drop(\"Age\", axis = 1) \nX_test = X_test.drop(\"Age\", axis = 1) \nX_test = X_test.drop(\"PassengerId\", axis = 1)  ","8510f6ea":"X_test.head(10)","f0a37cf2":"plt.figure(figsize = (20,12))\nPlotCorr(X_train);","8174566a":"#correlation of features with target variable\nSurvive_Corr = X_train.corr()[\"Survived\"]\nSurvive_Corr = Survive_Corr[np.argsort(Survive_Corr, axis = 0)[::-1]] #sort in descending order\nSurvive_Corr = Survive_Corr[1:15] # remove the 'Survived'\nprint(Survive_Corr)","125e2ec9":"X_train = X_train.drop(\"Survived\", axis = 1)","10d0d52c":"missing_train = Missing(X_train)\nmissing_test = Missing(X_test)\n\n\nprint('TRAIN DATA','\\t\\t','TEST DATA')\nSideSide(missing_train, missing_test)\n\nprint('\\n\\nMISSING TRAINING DATA \\t\\t\\t MISSING TEST DATA')\nplt.figure(figsize = (10,5));\nplt.subplot(1, 2, 1)\nsns.heatmap(X_train.isnull(), yticklabels = False, cbar = False, cmap = 'viridis')\nplt.subplot(1, 2, 2)\nsns.heatmap(X_test.isnull(), yticklabels = False, cbar = False,cmap = 'viridis');","e2d3fb68":"print('TRAINING')\nprint(X_train.info())\nprint('\\n\\nTEST')\nprint(X_train.info())\n\nX_train.head(0)\nX_test.head(0)\n\ncnt = 0\nd_type = ['float64', 'int64','object','dtype']\nprint('\\n\\tTRAIN \\t\\t TEST')\nfor c1, c2 in zip(X_train.get_dtype_counts(), X_test.get_dtype_counts()):\n    cnt += 1\n    print(\"%s:\\t%-9s \\t%s\"%(d_type[cnt],c1, c2))\n    ","2e390ac7":"\n# grid search\ndef GridSearchModel(X, Y, model, parameters, cv):\n    CV_model = GridSearchCV(estimator = model, param_grid = parameters, cv = cv)\n    CV_model.fit(X, Y)\n    CV_model.cv_results_\n    print(\"Best Score:\", CV_model.best_score_,\" \/ Best parameters:\", CV_model.best_params_)\n    \n# Learning curve\ndef LearningCurve(X, y, model, cv, train_sizes):\n\n    train_sizes, train_scores, test_scores = learning_curve(model, X, y, cv = cv, n_jobs = 4, \n                                                            train_sizes = train_sizes)\n\n    train_scores_mean = np.mean(train_scores, axis = 1)\n    train_scores_std  = np.std(train_scores, axis = 1)\n    test_scores_mean  = np.mean(test_scores, axis = 1)\n    test_scores_std   = np.std(test_scores, axis = 1)\n    \n    train_Error_mean = np.mean(1- train_scores, axis = 1)\n    train_Error_std  = np.std(1 - train_scores, axis = 1)\n    test_Error_mean  = np.mean(1 - test_scores, axis = 1)\n    test_Error_std   = np.std(1 - test_scores, axis = 1)\n\n    Scores_mean = np.mean(train_scores_mean)\n    Scores_std = np.mean(train_scores_std)\n    \n    _, y_pred, Accuracy, Error, precision, recall, f1score = ApplyModel(X, y, model)\n    \n    plt.figure(figsize = (16,4))\n    plt.subplot(1,2,1)\n    ax1 = Confuse(y, y_pred, classes)\n    plt.subplot(1,2,2)\n    plt.fill_between(train_sizes, train_Error_mean - train_Error_std,train_Error_mean + train_Error_std, alpha = 0.1,\n                     color = \"r\")\n    plt.fill_between(train_sizes, test_Error_mean - test_Error_std, test_Error_mean + test_Error_std, alpha = 0.1, color = \"g\")\n    plt.plot(train_sizes, train_Error_mean, 'o-', color = \"r\",label = \"Training Error\")\n    plt.plot(train_sizes, test_Error_mean, 'o-', color = \"g\",label = \"Cross-validation Error\")\n    plt.legend(loc = \"best\")\n    plt.grid(True)\n     \n    return (model, Scores_mean, Scores_std )\n\ndef ApplyModel(X, y, model):\n    \n    model.fit(X, y)\n    y_pred  = model.predict(X)\n\n    Accuracy = round(np.median(cross_val_score(model, X, y, cv = cv)),2)*100\n \n    Error   = 1 - Accuracy\n    \n    precision = precision_score(y_train, y_pred) * 100\n    recall = recall_score(y_train, y_pred) * 100\n    f1score = f1_score(y_train, y_pred) * 100\n    \n    return (model, y_pred, Accuracy, Error, precision, recall, f1score)  \n    \ndef Confuse(y, y_pred, classes):\n    cnf_matrix = confusion_matrix(y, y_pred)\n    \n    cnf_matrix = cnf_matrix.astype('float') \/ cnf_matrix.sum(axis = 1)[:, np.newaxis]\n    c_train = pd.DataFrame(cnf_matrix, index = classes, columns = classes)  \n\n    ax = sns.heatmap(c_train, annot = True, cmap = cmap, square = True, cbar = False, \n                          fmt = '.2f', annot_kws = {\"size\": 20})\n    return(ax, c_train)\n\ndef PrintResults(model, X, y, title):\n    \n    model, y_pred, Accuracy, Error, precision, recall, f1score = ApplyModel(X, y, model)\n    \n    _, Score_mean, Score_std = LearningCurve(X, y, model, cv, train_size)\n    Score_mean, Score_std = Score_mean*100, Score_std*100\n    \n    \n    print('Scoring Accuracy: %.2f %%'%(Accuracy))\n    print('Scoring Mean: %.2f %%'%(Score_mean))\n    print('Scoring Standard Deviation: %.4f %%'%(Score_std))\n    print(\"Precision: %.2f %%\"%(precision))\n    print(\"Recall: %.2f %%\"%(recall))\n    print('f1-score: %.2f %%'%(f1score))\n    \n    Summary = pd.DataFrame({'Model': title,\n                       'Accuracy': Accuracy, \n                       'Score Mean': Score_mean, \n                       'Score St Dv': Score_std, \n                       'Precision': precision, \n                       'Recall': recall, \n                       'F1-Score': f1score}, index = [0])\n    return (model, Summary)","9dc9018c":"classes = ['Dead','Survived']\ncv = ShuffleSplit(n_splits = 100, test_size = 0.25, random_state = 0)\ntrain_size = np.linspace(.1, 1.0, 15)","f86afe48":"#Logistic Regresion\nmodel = LogisticRegression()\nmodel, Summary_LR = PrintResults(model, X_train, y_train, 'Logistic Regression')\n\ny_train_LR = pd.Series(model.predict(X_train), name = \"LR\")\ny_test_LR = pd.Series(model.predict(X_test), name = \"LR\")","0d364b33":"# stochastic gradient descent (SGD) learning\nmodel = linear_model.SGDClassifier(max_iter = 200, tol = None)\nmodel,Summary_SGD = PrintResults(model, X_train, y_train, 'SGD')\ny_train_SGD = pd.Series(model.predict(X_train), name = \"SGD\")\ny_test_SGD = pd.Series(model.predict(X_test), name = \"SGD\")","5e6df770":"# Random Forest\nmodel = RandomForestClassifier(n_estimators = 10)\nmodel,Summary_RF = PrintResults(model, X_train,y_train, 'Random Forest')\ny_train_RF = pd.Series(model.predict(X_train), name = \"RF\")\ny_test_RF = pd.Series(model.predict(X_test), name = \"RF\")","5dd18a94":"#SVM\nmodel = SVC()\nmodel,Summary_SVM = PrintResults(model, X_train, y_train, 'SVM')\ny_train_SVM = pd.Series(model.predict(X_train), name = \"SVM\")\ny_test_SVM = pd.Series(model.predict(X_test), name = \"SVM\")","954706fa":"# KNN\nmodel = KNeighborsClassifier(n_neighbors = 3)\nmodel,Summary_KNN = PrintResults(model, X_train, y_train,'KNN')\ny_train_KNN = pd.Series(model.predict(X_train), name = \"KNN\")\ny_test_KNN = pd.Series(model.predict(X_test), name = \"KNN\")","eba652f3":"# Gaussian Naive Bayes\nmodel = GaussianNB()\nmodel,Summary_GNB = PrintResults(model, X_train, y_train, \"GNB\")\ny_train_GNB = pd.Series(model.predict(X_train), name = \"GNB\")\ny_test_GNB = pd.Series(model.predict(X_test), name = \"GNB\")","1b50837e":"# Perceptron\nmodel = Perceptron(max_iter = 5)\nmodel,Summary_MLP = PrintResults(model, X_train, y_train, 'MLP')\ny_train_MLP = pd.Series(model.predict(X_train), name = \"MLP\")\ny_test_MLP = pd.Series(model.predict(X_test), name = \"MLP\")","c4891c9c":"# Linear SVC\nmodel = LinearSVC()\nmodel,Summary_LSVM = PrintResults(model, X_train, y_train,\"LSVM\")\ny_train_LSVM = pd.Series(model.predict(X_train), name = \"LSVM\")\ny_test_LSVM = pd.Series(model.predict(X_test), name = \"LSVM\")","91991189":"# Decision Tree\nmodel = DecisionTreeClassifier()\nmodel,Summary_DT = PrintResults(model, X_train, y_train, 'DT')\ny_train_DT = pd.Series(model.predict(X_train), name = \"DT\")\ny_test_DT = pd.Series(model.predict(X_test), name = \"DT\")","43bff32f":"#Which is the best Model ?\n\nClass_Results = pd.concat([Summary_LR, Summary_SGD, Summary_RF, \n                           Summary_SVM, Summary_KNN, Summary_GNB,\n                           Summary_MLP, Summary_LSVM, Summary_DT], ignore_index = True)\n    \n\nClass_Results = Class_Results.sort_values(by = 'Accuracy', ascending=False)\nClass_Results = Class_Results.set_index('Accuracy')\nClass_Results.head(10)\n","fcb4ffce":"# Concatenate all classifier results\ny_test_Results = pd.concat([y_test_LR, y_test_SGD, y_test_RF, y_test_SVM, y_test_KNN,y_test_GNB,\n                              y_test_MLP, y_test_LSVM, y_test_DT], axis=1)\n\ny_train_Results = pd.concat([y_train_LR, y_train_SGD, y_train_RF, y_train_SVM, y_train_KNN, y_train_GNB,\n                              y_train_MLP, y_train_LSVM, y_train_DT], axis=1)\n\nplt.figure(figsize = (14, 7))\nplt.subplot(1,2,1)\nPlotCorr(y_train_Results)\nplt.title('Training data')\nplt.subplot(1,2,2)\nPlotCorr(y_test_Results)\nplt.title('Test data')\n\n","2449e523":"# Random Forest\nmodel = RandomForestClassifier(n_estimators = 10, oob_score = True)\nmodel, y_pred, Accuracy, Error, precision, recall, f1score = ApplyModel(X_train, y_train, model)\nPriority = pd.DataFrame({'Feature': X_train.columns,'Importance':np.round(model.feature_importances_,3)})\nPriority  = Priority .sort_values('Importance',ascending = False).set_index('Feature')","300132fa":"Priority.head(15)","391c82e6":"\nwidth = 0\nfig, ax = plt.subplots(figsize = (10,6))\nrects = ax.barh(np.arange(len(Priority)), np.array(Priority.values), color = 'red')\nax.set_yticks(np.arange(len(Priority)) + ((width)\/1))\nax.set_yticklabels(Priority.index, rotation ='horizontal')\nax.set_xlabel(\"Importance\")\nax.set_title(\"Feature Importance for Random Forrest w.r.t Survival\",fontsize = 14);\nax.grid(True)","01429961":"X_train = X_train.drop(\"Alone\", axis = 1) \nX_train = X_train.drop(\"CabinBool\", axis = 1)\nX_train = X_train.drop(\"Parch\", axis = 1) \nX_train = X_train.drop(\"SibSp\", axis = 1) \n\nX_test = X_test.drop(\"Alone\", axis = 1) \nX_test = X_test.drop(\"CabinBool\", axis = 1)\nX_test = X_test.drop(\"Parch\", axis = 1) \nX_test = X_test.drop(\"SibSp\", axis = 1) ","13e836f7":"# Random Forest again after droppingn parameters\n\nmodel = RandomForestClassifier(n_estimators = 200, oob_score = True)\n_, Summary_RF = PrintResults(model, X_train,y_train,'Random Forest')\ny_test_RF = pd.Series(model.predict(X_test), name = \"Survived\")\nprint(\"oob score:\", round(model.oob_score_, 4) * 100, \"%\")","80cdbb2c":"def plot_precision_and_recall(precision, recall, threshold):\n    plt.plot(threshold, precision[:-1], \"r-\", label = \"precision\", linewidth = 5)\n    plt.plot(threshold, recall[:-1], \"b\", label = \"recall\", linewidth = 5)\n    plt.xlabel(\"threshold\", fontsize = 19)\n    plt.legend(loc = \"upper right\", fontsize = 19)\n    plt.ylim([0, 1])\n    \n    \n# getting the probabilities of our predictions\ny_scores = model.predict_proba(X_train)\ny_scores = y_scores[:,1]\n\nprecision, recall, threshold = precision_recall_curve(y_train, y_scores)\nplt.figure(figsize = (10, 6))\nplot_precision_and_recall(precision, recall, threshold)\nplt.show()\n","6e318d67":"#this section is commente out as it takes too long to run. the results are shown at the bottom\n\n#from sklearn.ensemble import RandomForestClassifier\n#from sklearn.metrics import make_scorer, accuracy_score\n#from sklearn.model_selection import GridSearchCV\n\n## Choose the type of classifier. \n#model = RandomForestClassifier()\n\n## Choose some parameter combinations to try\n#parameters = {'n_estimators': [10,100,200,400,600], \n#              'max_features': ['log2', 'sqrt','auto'], \n#              'criterion': ['entropy', 'gini'],\n#              'max_depth': [2, 3, 5, 10, 20], \n#              'min_samples_split': [2, 3, 5, 10, 20, 30],\n#              'min_samples_leaf': [1,5,10,20,30,50]\n#             }\n\n## Type of scoring used to compare parameter combinations\n#acc_scorer = make_scorer(accuracy_score)\n\n## Run the grid search\n#grid_obj = GridSearchCV(model, parameters, scoring = acc_scorer, n_jobs = 4, verbose = 1)\n#grid_obj = grid_obj.fit(X_train, y_train.values.ravel())\n\n#3 Set the clf to the best combination of parameters\n#model_rf_final = grid_obj.best_estimator_\n\n## Fit the best algorithm to the data. \n#model_rf_final.fit(X_train, y_train)\n\n\n##The Following areh the results.... took several hourse to run\n#RandomForestClassifier(bootstrap = True, class_weight = None, criterion = 'entropy',\n#            max_depth = 10, max_features = 'log2', max_leaf_nodes = None,\n#            min_impurity_decrease = 0.0, min_impurity_split = None,\n#            min_samples_leaf = 1, min_samples_split = 30,\n#            min_weight_fraction_leaf = 0.0, n_estimators = 100, n_jobs = 1,\n#            oob_score = False, random_state = None, verbose = 0, warm_start = False)","78064133":"model_rf_final = RandomForestClassifier(bootstrap = True, class_weight = None, criterion = 'entropy',\n            max_depth = 10, max_features = 'log2', max_leaf_nodes = None,\n            min_impurity_decrease = 0.0, min_impurity_split = None,\n            min_samples_leaf = 1, min_samples_split = 30,\n            min_weight_fraction_leaf = 0.0, n_estimators = 100, n_jobs = 1,\n            oob_score = False, random_state = None, verbose = 0, warm_start = False)","def031f0":"_, Summary_LR = PrintResults(model_rf_final, X_train, y_train, 'Logistic Regression')\ny_train_pred = pd.Series(model_rf_final.predict(X_train), name = 'Survived')\ny_test_pred = pd.Series(model_rf_final.predict(X_test), name = \"Survived\")","704e9fab":"y_test_pred = pd.Series(model_rf_final.predict(X_test), name = \"Survived\")","8488eca3":"y_test_pred_final = pd.DataFrame(y_test_pred)","4681e2fe":"y_test_pred_final.head()","296d044f":"submission = pd.DataFrame({\n        \"PassengerId\": X_test_original[\"PassengerId\"],\n        \"Survived\": y_test_pred_final['Survived']\n    })\nsubmission.to_csv('Titanic Submission.csv', index = False)\n\nprint('Done')","2d6ba29d":"submission.head()","718b8381":"## Closer Look at Random Forest","a2ea9e8c":"Below you can see the code of the hyperparamter tuning for the parameters criterion, min_samples_leaf, min_samples_split and n_estimators.\n\nI put this code into a comments because it takes a long time to run it. ","bb789563":"It seems that passenger coming from Cherbourg (C) have more chance to survive. Below we can see that this is not related to class. perhaps it is related to Derck level... see later","8f6e1ddc":"VARIABLE DESCRIPTIONS:\n\n    Survived: Survived (1) or died (0)\n    Pclass: Passenger's class\n    Name: Passenger's name\n    Sex: Passenger's sex\n    Age: Passenger's age\n    SibSp: Number of siblings\/spouses aboard\n    Parch: Number of parents\/children aboard\n    Ticket: Ticket number\n    Fare: Fare\n    Cabin: Cabin\n    Embarked: Port of embarkation\n","f79bf66c":"### Feature Correlation\n","160b8f34":"# Feature Engineering","6cd7f508":"### Missing data and Combine","ffc45bb1":"The plot above confirms our assumption about pclass 1, but we can also spot a high probability that a person in pclass 3 will not survive.\n","a0b91925":"Age, embarked and cabin have the most missing values. Both of these terms can be used to determin the survival (see later). thus these missing values will need to be filled in based on their relationship with other featured\n\nThe Embarked (test) feature has only 2 missing values, which can easily be filled. \nThe 'Age' feature, which has 177 missing values, will be filled with values based on its relationship with other features. ","7c78eddf":"\n\nAs the distributions are clearly different for the fares of survivors vs. deceased, it's likely that this would be a significant predictor in our final model. Passengers who paid lower fare appear to have been less likely to survive. This is probably strongly correlated with Passenger Class, which we'll look at next.\n","0a0dd48b":"\n\nSmall families have more chance to survive, more than single (Parch 0), medium (Parch 3,4) and large families (Parch 5,6 ).\n\nBe carefull there is an important standard deviation in the survival of passengers with 3 parents\/children\n","7805d822":"### Families or Alone","e4882b2c":"### SibSP & Parch","2681f792":"### Fare Feature","e41e875c":"the median age increases with decreasing class the third class will have a a large number of infants","bcfe9a69":"the floats will need to be converted into int64 values, some of the objects (e.g. sex) will ned to be converted into numberics, all NaN and null values will need to be filled, ","6402079f":"### Hyperparameter Tuning","ec184726":"\n\nAssumption: the less people was in your family the faster you were to get to the boat. The more people they are the more managment is required. However, if you had no family members you might wanted to help others and therefore sacrifice.\n\nThe females traveling with up to 2 more family members had a higher chance to survive. However, a high variation of survival rate appears once family size exceeds 4 as mothers\/daughters would search longer for the members and therefore the chanes for survival decrease.\n\nAlone men might want to sacrifice and help other people to survive.\n\n","e2fde767":"### Look at hte prepared Data","6956c043":"it can be seen that tha Pclass and the Fare have the stongest correlation with the survival rate. these parameters themselves are also highly correlated with eachther. ","61690052":"### AGE Categories","8bd799ae":"in the following section several classification methods with be tested to determine which is the best, logistical Regression, Stochastic Gradient Descent, Random Forest, Support Vector Machine, K-Nearest Neighbour, Gausssian Naive Bayes, Multi-Layer Perceptron, Linear Support Vector Machine, Decisison Tree.","403fa927":"Results will be printed and sumarised using the follow parameters\n\nConfusion Matrix: \n    -  True Negatived: passengers were correctly classified as not survived \n    -  False positives: where wrongly classified as not survived \n    -  True postivies: correctly classified as survived \n    -  False negative: passengers where wrongly classified as survived \n\nPrecision: a passengers survival correctly  \n\nRecall: The recall tells us that it predicted the survival \n\nF-Score: The F-score is computed with the harmonic mean of precision and recall. \n\nOOB: out-of-bag samples to estimate the generalization accuracy.\n\n\n Learning curves allow us to diagnose if the is overfitting or underfitting.\n\n\nOVERFITTING:\nWhen the model overfits, it means that it performs well on the training set, but not not on the validation set. Accordingly, the model is not able to generalize to unseen data. If the model is overfitting, the learning curve will present a gap between the training and validation scores. Two common solutions for overfitting are reducing the complexity of the model and\/or collect more data.\n    \n    \nUNDERFITTING: \nunderfitting means that the model is not able to perform well in either training or validations sets. In those cases, the learning curves will converge to a low score value. When the model underfits, gathering more data is not helpful because the model is already not being able to learn the training data. Therefore, the best approaches for these cases are to improve the model (e.g., tuning the hyperparameters) or to improve the quality of the data (e.g., collecting a different set of features).\n","74c32b8e":"# Fare","ac35c552":"we can see that passengers with a cabin have generally more chance to survive than passengers without (X).\n\nIt is particularly true for cabin B, C, D, E and F.","d3049b5d":"\n\nAge distribution seems to be the same in Male and Female subpopulations, so Sex is not informative to predict Age.\n\nHowever, 1rst class passengers are older than 2nd class passengers who are also older than 3rd class passengers.\n\nMoreover, the more a passenger has parents\/children the older he is and the more a passenger has siblings\/spouses the younger he is.\n\nLets take a closer look at the correlations\n","19869722":"### Helper Functions","9bbc7d4a":"A general rule is that, the more features you have, the more likely your model will suffer from overfitting and vice versa.'Alone', 'Parch', 'CabinBool', 'SibSp' don't play a significant role in our random forest classifiers prediction process. Because of that I will drop them from the dataset and train the classifier again. ","4d71a395":"### Survival by sex","3d54eca8":"Here we see clearly, that Pclass is contributing to a persons chance of survival, especially if this person is in class 1.\n\nThis trend is conserved when we look at both male and female passengers. it can also be seen that a far large percentage of women survive compared to men\n","126afaec":"as can be seen from the above data, there are several featured with NaN missing values. Lets take a look at the features that have missing values missing","1bd3c32d":"\nWe want to analyze if Name feature can be engineered to extract titles and test correlation between titles and survival, before dropping Name and PassengerId features.","a775f63d":"### PClass","81536345":"### AGE","29e67601":"Above you can clearly see that the recall is falling of rapidly at a precision of around 85%. Because of that you may want to select the precision\/recall tradeoff before that - maybe at around 75 %.\n\nYou are now able to choose a threshold, that gives you the best precision\/recall tradeoff for your current machine learning problem. If you want for example a precision of 80%, you can easily look at the plots and see that you would need a threshold of around 0.4. Then you could train a model with exactly that threshold and would get the desired accuracy.\n\n","750265e0":"This first bar plot above shows the distribution of female and male survived and died.\nThis count plot shows the actual distribution of male and female passengers that survived and did not survive. It shows that among all the females ~ 230 survived and ~ 70 did not survive. While among male passengers ~110 survived and ~480 did not survive. \n\n\nsecond plot. The second plot reinforces this idea where, it shows that ~74% female passenger survived while only ~19% male passenger survived\n\n\nthe violin plot also reinforces the fact that more males die and more women survive\n\n\n\nfrom this is is evident that  Males have less chance to survive than Female. this is probably due to the \"Women and children first\" mentality\n\n","c7b1192c":"The correlation map confirms the factorplots observations except for Parch. Age is not correlated with Sex, but is negatively correlated with Pclass, Parch and SibSp.\n\nIn the plot of Age in function of Parch, Age is growing with the number of parents \/ children. But the general correlation is negative.\n\nSo, i decided to use SibSP, Parch and Pclass in order to impute the missing ages.\n\nThe strategy is to fill Age with the median age of similar rows according to Pclass, Parch and SibSp.\n","e2e34ef4":"Average Age is 29 years and ticket price is 32. As there are 681 unique tickets and there is no way to extract less detailed information we exclude this variable. There are 891 unique names but we could take a look on the title of each person to understand if the survival rate of people from high society was higher","a407c344":"## DATA EXPLORATION","77320975":"### Embarked","50d51f93":"From the above analysis the Random Forest algorithm provides results (Score, precision, recall, f-score). so lets take a closer look at htis to see if even furth improvements can be made\n\nSklearn measure a features importance by looking at how much the treee nodes, that use that feature, reduce impurity on average (across all trees in the forest).","96baef24":"# Prepare Submission Data","46841331":"HOw do we fill in the missing age??? lets investigate\n","eb6e977e":"### TESTING FINAL MODEL","99974bf5":"### Titles","2d1eb61b":"FEMALES: have a much higher survival count than men. there are 2 intervals with a high survival count: infants 0 - 5 year, and adults 16 - 38 years old\n\nMEN: have a much lower survival count than women. again there are 2 intervals with relatively high survival counts, infants 0 - 5 year, and adults 20 - 32 years old\n\n\nWhen we superimpose the two densities , we cleary see a peak correponsing (between 0 and 5) for babies and very young childrens.\n\n\nThe age distribution for survivors and non-survivors are very similar. One notable difference is that, of the survivors, a larger proportion were children. ","25102281":"## PREDICTION MODELS","85efd376":"### Cabin number to Deck level","bca70190":"Our random forest model predicts the same as before. Dropping the features did not make much of a difference except to decrease the stdv\n\nCnnfusion Matrix: \n95% passengers were correctly classified as not survived (called true negatives),\n5% where wrongly classified as not survived (false positives).\n85% 249 where correctly classified as survived (true positives).\n15% were passengers where wrongly classified as survived (false negatives)\n\nPrecision:\nOur model predicts 91% of the time, a passengers survival correctly  \n\nRecall: The recall tells us that it predicted the survival of 85 % of the people who actually survived.\n\nF-Score: The F-score is computed with the harmonic mean of precision and recall. \n\nOOB: out-of-bag samples to estimate the generalization accuracy.","1f1cb8b4":"### Missing Data","f3e78e1e":"ater looking closely at the cabin number, it can be seen that it is an alpha-numeric identity. The letter indicates the deck and the number represents the cabin number on this deck. We will therefore subsitute this 'Cabin' category for a 'Deck\" categor and simply extract the deck letter\n\nrecall most cabin number are missing so lets see how may people have cabine and if it is related to surviving","28fd3566":"As we see, Age column contains 256 missing values in the whole dataset.\n\nSince there is subpopulations that have more chance to survive (children for example), it is preferable to keep the age feature and to impute the missing values.\n\nTo adress this problem, i looked at the most correlated features with Age (Sex, Parch , Pclass and SibSP).","806efc24":"### Mapping and removal of features"}}