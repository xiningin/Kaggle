{"cell_type":{"aea4cebd":"code","edcf4be0":"code","50acaba0":"code","67c3ba68":"code","f2097f7d":"code","82235d29":"code","ed09b6d3":"code","a6182164":"code","995f38f4":"markdown","e955cf7d":"markdown"},"source":{"aea4cebd":"import pandas as pd # Data processing, CSV file I\/O \nimport numpy as np # Linear Algebra\nimport matplotlib.pyplot as plt # Plotting graphs\n\nimport tensorflow as tf\n\n# Since I have a GPU & I've GPU enabled, I am going to use the GPU version of keras \n# (NOTE: Ignore if you do not have GPU enabled)\nfrom keras import backend as K\nsess = tf.Session(config=tf.ConfigProto(log_device_placement=True))\nK.tensorflow_backend._get_available_gpus()\n\nprint(f'Tensorflow {tf.__version__}')","edcf4be0":"# Reading the training and testing datasets\ntrain_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')","50acaba0":"# Let's explore the datasets by plotting a few images (from their pixel values)\n# The number of rows & cols to plot\nrows, cols = 4, 4\n\n# Creating a matplotlib figure that holds 16 subplots (for 16 digit images)\nplt.figure(figsize=(8,7))\nplt.suptitle('Training Data')\n\n# Plotting the first 16 datapoints from the training data\nfor i in range(4):\n    for j in range(4):\n        index = (i * cols) + j # Row major ordering\n        plt.subplot(rows, cols, index + 1)\n        plt.xticks([])\n        plt.yticks([])\n        \n        label = train_data['label'].values[index]\n        image = train_data.drop('label', axis=1).iloc[index].values.reshape(28, 28) # Reshaping the 1D array into a 2D array of pixel values\n        \n        plt.title(label)\n        \n        # Using a binary color map\n        plt.imshow(image, cmap='binary')","67c3ba68":"# A function to scale the values and return the data as numpy arrays\ndef preprocess_data(df):\n    # Training Data\n    if 'label' in df.columns: \n        df_x = df.drop('label', axis=1) \/ 255.0\n        df_y = df['label']\n        \n        df_x = df_x.values.reshape(df.shape[0], 28, 28, 1)        \n        return (df_x, df_y)\n    \n    # Testing Data\n    else: \n        df_x = df.div(255.0)\n        \n        df_x = df_x.values.reshape(df.shape[0], 28, 28, 1)\n        return df_x\n\n# Applying the preprocess_data function to both the train & test data\ntrain_X, train_y = preprocess_data(train_data)\ntest_X = preprocess_data(test_data)\n\nprint(f'Training data shape: X-{train_X.shape} & y-{train_y.shape}')\nprint(f'Testing data shape: X-{test_X.shape}')","f2097f7d":"# Initializing some constants for the Neural Network Architecture\nINPUT_SHAPE = train_X.shape[1:] # (28, 28, 1)\n\n# The number of training examples per batch of training\nBATCH_SIZE = 128\n\n# The number of epochs or iterations of the training loop\nEPOCHS = 10","82235d29":"# The various layers used for this Neural Network Model.\n# Dense - A layer that is fully connected (densely-connected.)\n# Conv2D - A 2-dimensional convolutional layer.\n# Dropout - A layer that helps prevent overfitting.\n# Flatten - A layer that flattens the input.\n# MaxPooling2D - A layer that performs Max Pooling of the input.\n# BatchNormalization - A layer that normalizes the values of each batch.\n\n# Building the model\nmodel = tf.keras.models.Sequential()\n\nmodel.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same', input_shape=INPUT_SHAPE))\nmodel.add(tf.keras.layers.Conv2D(filters=32, kernel_size=(3,3), activation='relu', padding='same'))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2)))\nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.BatchNormalization())\n\nmodel.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\nmodel.add(tf.keras.layers.Conv2D(filters=64, kernel_size=(3,3), activation='relu', padding='same'))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2))) \nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.BatchNormalization())\n\nmodel.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same'))\nmodel.add(tf.keras.layers.Conv2D(filters=128, kernel_size=(3,3), activation='relu', padding='same'))\nmodel.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2))) \nmodel.add(tf.keras.layers.Dropout(0.2))\nmodel.add(tf.keras.layers.BatchNormalization())\n\nmodel.add(tf.keras.layers.Flatten())\nmodel.add(tf.keras.layers.Dense(512, activation='relu'))\nmodel.add(tf.keras.layers.Dense(10, activation='softmax'))\n\nmodel.summary()","ed09b6d3":"# 'adam' is the most used optimizer\n# The loss function used is SCC because it's a multi-class classification problem with integer value classes\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['acc'])\n\n# Fitting the model and using 10% of the data for validation\nhistory = model.fit(x=train_X, y=train_y, epochs=EPOCHS, batch_size=BATCH_SIZE, validation_split=0.2)","a6182164":"# Creating the predictions file to submit\nsubmission_df = pd.read_csv('..\/input\/sample_submission.csv')\npredictions = model.predict(test_X)\nsubmission_vals = []\n\nfor i in range(len(predictions)):\n    submission_vals.append(np.argmax(predictions[i]))\n\nsubmission_df['Label'] = submission_vals\n\n# Saving the submission file\nsubmission_df.to_csv('submission.csv', index=False)","995f38f4":"The shape of each training example is set to (28, 28, 1).\nHere, the 1 represents the number of **channels** the image uses, i.e., basically the number of bytes used to represent each pixel value.\n\n> - 1 - Greyscale\n> - 3 - RGB (Red, Green, Blue)\n> - 4 - RGBA (Red, Green, Blue, Alpha)\n\n- [Images and Channels](http:\/\/www.georeference.org\/doc\/images_and_channels.htm)","e955cf7d":"__As we can see from the above images, the color of the image doesn't play a factor in the value represented by the image, and if we feed such pixel values that represent colors to our Neural Network, it would learn features that don't affect the classification problem and could take a long time to converge.\nHence, we can scale the pixel values to be from 0-1 instead of 0-255. __\n\nWe can either do this by dividing each pixel value by 255. This is a direct approach to this problem.\nOr we can normalize the data either by Mean Normalization or Min-Max Normalization.\nSince this application is a fairly simple application, I've deciced to just divide the pixel values by 255.\n\n- [Normalization](https:\/\/en.wikipedia.org\/wiki\/Normalization_(statistics)\n- [Feature Scaling](https:\/\/en.wikipedia.org\/wiki\/Feature_scaling)"}}