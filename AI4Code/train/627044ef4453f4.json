{"cell_type":{"aa368490":"code","1cded7f1":"code","d9579a8a":"code","df2161d1":"code","e7fd19bb":"code","ba6ef7b0":"code","40bd37e2":"code","30975878":"code","9deb24d4":"code","0802c6de":"code","a8a8c050":"code","863f2b41":"code","f6b35ed6":"code","58e55d17":"code","f476a574":"code","a1509c93":"code","3d63ebb8":"code","51025f44":"code","7e95a070":"code","bee5a021":"code","adb40151":"code","2da92ec2":"code","b82abaec":"code","ba7ba065":"code","61c7b8b4":"code","0e053ba6":"code","a586aba8":"code","8d74960a":"code","b0cc592b":"code","702b53e0":"code","74c99e6f":"code","3253f7af":"code","046ce0ac":"code","3e5d998b":"code","350192f3":"code","986b5e6f":"markdown","057bf668":"markdown","c593183d":"markdown","c2f581a4":"markdown","abcc7e79":"markdown","b059e16a":"markdown","78bf239c":"markdown","a35925db":"markdown","caf98b25":"markdown","ac312348":"markdown","30e25c3a":"markdown","46ab18dc":"markdown","ac70a627":"markdown","08043d09":"markdown","99584d98":"markdown","6736c9ee":"markdown","4f2ccd51":"markdown","a8cec582":"markdown","f657eca8":"markdown","b8e9b75f":"markdown","1be51308":"markdown","8221725a":"markdown","aa3cd49b":"markdown"},"source":{"aa368490":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport re\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n%matplotlib inline\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","1cded7f1":"train = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\nsub = pd.read_csv('..\/input\/gender_submission.csv')\ntrain.head()","d9579a8a":"test.head()","df2161d1":"sub.head()","e7fd19bb":"# combine train and test dataset\ndf = train.append(test, ignore_index=True, sort=True)\nprint(df.info())\ndf.head(30)","ba6ef7b0":"# seperate cabin, SibSp, Parch, and Survived feature as another dataframe\n_ = df[['Cabin', 'SibSp', 'Parch', 'Survived']]\n\n# sum SibSp dan Parch as famsize then drop those two features\n_['Famsize'] = _['SibSp'] + _['Parch']\n_ = _.drop(['SibSp', \"Parch\"], axis=1)\n\n# drop the missing value in cabin\n_ = _[~_['Cabin'].isnull()]\n\n# count how many unique cabin in it\nprint(\"There are total :\", _['Cabin'].nunique(), \"cabins in the data\")\n\n# make new columns to count if there any data that\n_['Cabin Count'] = _['Cabin'].apply(lambda x: x.split(\" \"))\n_['Cabin Count'] = _['Cabin Count'].apply(lambda x: len(x))\n\n# print Cabin Count\nprint(\"Cabin Counts\")\nprint(_['Cabin Count'].value_counts())","40bd37e2":"# show cabin different condition\nprint(\"Cabin with 4 data\")\nprint(_[_['Cabin Count'] == 4])\n\nprint(\"Cabin with 3 data\")\nprint(_[_['Cabin Count'] == 3])\n\nprint(\"Cabin with 2 data\")\nprint(_[_['Cabin Count'] == 2])\n\nprint(\"Cabin with 1 data\")\nprint(_[_['Cabin Count'] == 1][:20])","30975878":"# make new feature as cabin code\n_['Cabin Code'] = _['Cabin'].str[0]\n_.head()","9deb24d4":"fig, ax = plt.subplots(1,2, figsize=(18,6))\nsns.boxplot(x='Cabin Count', y='Famsize', data=_.sort_values(\"Cabin Count\"), ax=ax[0])\nsns.countplot(y='Cabin Code', hue='Survived', data=_[~_['Survived'].isnull()].sort_values('Cabin Code'), ax=ax[1])","0802c6de":"# age categorization\ndef age_cat(x):\n    if x<1:\n        return 'Infant'\n    elif x>=1 and x<12:\n        return \"Children\"\n    elif x>=12 and x<18:\n        return \"Teen\"\n    elif x>=18 and x<65:\n        return \"Adult\"\n    elif x>= 65:\n        return \"Senior Adult\"\n\ndf['AgeCat'] = df['Age'].apply(lambda x: age_cat(x))\n\n# Family Size\ndf['FamSize'] = df['SibSp'] + df['Parch'] + 1\n\n# Honorific title\ndf['HonTit'] = df['Name'].apply(lambda x: re.findall('(\\w+)\\.',x)[0])\n\ndf.head()","a8a8c050":"# inspect Honorific Title that age are missing\nfig, ax = plt.subplots(figsize=(18,6))\nsns.countplot(x='HonTit', data=df[df['Age'].isnull()], ax=ax)\nax.set_title(\"Title Missing Age Count\")","863f2b41":"# plot age range and honorific title\nfig, ax = plt.subplots(figsize=(18,6))\nsns.boxplot(y='Age', x='HonTit', data=df, ax=ax)","f6b35ed6":"# check the most Age Category for each Honorific Title\ndf.groupby(['HonTit','AgeCat'])['PassengerId'].count()","58e55d17":"# fill the age category\ndef age_fill(x):\n    if x == \"Mr\" or x == \"Mrs\" or x == \"Miss\" or x == \"Dr\" or x == \"Ms\":\n        return \"Adult\"\n    elif x == \"Master\":\n        return \"Children\"\n\n# make data slicing condition\nadult = (df['HonTit'].str.contains('Mr|Mrs|Miss|Dr|Ms'))\nchild = (df['HonTit'] == 'Master')\n\n# fill missing age category value\ndf.loc[adult,'AgeCat'] = df.loc[adult,'AgeCat'].fillna('Adult')\ndf.loc[child,'AgeCat'] = df.loc[child,'AgeCat'].fillna('Children')","f476a574":"# fill fare missing value with fare median\ndf['Fare'] = df['Fare'].fillna(df['Fare'].median())\n\n# fill Embarked missing value with the most point of embarkment\ndf['Embarked'] = df['Embarked'].fillna(df['Embarked'].value_counts().index[0])\n\n# check data condition\ndf.info()","a1509c93":"# drop un-used features\n_ = ['Age', 'Cabin', 'Name', 'SibSp', 'Parch', 'Ticket']\ndf = df.drop(_, axis=1)\ndf.info()","3d63ebb8":"print(df['HonTit'].unique())","51025f44":"# change honorific title \ndef hon_cat(x):\n    if x in ['Lady', 'Sir', 'Countess', 'Jonkheer', 'Don', 'Dona']:\n        return \"Noble\"\n    elif x in ['Rev', 'Dr']:\n        return \"Professional\"\n    elif x in ['Major', 'Col', 'Capt']:\n        return \"Officer\"\n    else:\n        return \"Common\"\n    \ndf['HonTit'] = df['HonTit'].map(hon_cat)\ndf['HonTit'].value_counts()","7e95a070":"# make some type changes to several features\ndf.loc[:,'Sex'] = df['Sex'].astype('category')\ndf.loc[:,'Embarked'] = df['Embarked'].astype('category')\ndf.loc[:,'Survived'] = df['Survived'].astype('category')\ndf.loc[:,'Pclass'] = df['Pclass'].astype('category')\ndf.loc[:,'FamSize'] = df['FamSize'].astype('category')\ndf.loc[:,'AgeCat'] = df['AgeCat'].astype('category')\ndf.loc[:,'HonTit'] = df['HonTit'].astype('category')\ndf.info()","bee5a021":"# show numeric code representation from Sex and Embarked Feature Value\nprint(dict(zip(df['Sex'].cat.codes, df['Sex'])))\nprint(dict(zip(df['Embarked'].cat.codes, df['Embarked'])))\nprint(dict(zip(df['AgeCat'].cat.codes, df['AgeCat'])))\nprint(dict(zip(df['HonTit'].cat.codes, df['HonTit'])))\n\n# make copy of dataframe to store the converted\ndf2 = df.copy()\n\n# convert to numerical value\ncat_col = df2.drop('Survived', axis=1).select_dtypes(['category']).columns\ndf2[cat_col] = df2[cat_col].apply(lambda x: x.cat.codes)\ndf2.head()","adb40151":"# calculate the correlation to data that survived value are exist\n_ = df2[~df2['Survived'].isnull()]\n_['Survived'] = _['Survived'].astype('int64')\n_ = _.corr()\n\n# plot the corrleation heatmap\nfig, ax = plt.subplots(figsize=(18,5))\nsns.heatmap(_, xticklabels=_.columns, yticklabels=_.columns, annot=True, fmt=\".2f\", ax=ax)","2da92ec2":"# plot distribution from features\nfig, ax = plt.subplots(4, 2, figsize=(18,16))\nsns.countplot(y='Pclass', data=df, ax=ax[0][0])\nsns.violinplot(y='Fare',data=df, ax=ax[0][1])\nsns.countplot(y='AgeCat', data=df, ax=ax[1][0])\nsns.countplot(y='Sex', data=df, ax=ax[1][1])\nsns.countplot(y='Embarked', data=df, ax=ax[2][0])\nsns.countplot(y='FamSize', data=df, ax=ax[2][1])\nsns.countplot(y='HonTit', data=df, ax=ax[3][0])\nsns.countplot(y='Survived', data=df, ax=ax[3][1])","b82abaec":"sns.catplot(x=\"Pclass\", hue=\"Sex\", col=\"Survived\", data=df, kind=\"count\");","ba7ba065":"fig, ax = plt.subplots(figsize=(18,4))\n_ = df.groupby(['AgeCat'])['Survived'].value_counts(normalize=True).rename('percentage').mul(100).reset_index()\nsns.barplot(x=\"AgeCat\", y='percentage', hue=\"Survived\", data=_, ax=ax)\nax.set_title(\"Surival Percentage per Age Category\")\nax.grid(True, axis='y')","61c7b8b4":"fig, ax = plt.subplots(1,2, figsize=(18,4))\nsns.countplot(x=\"Pclass\", hue=\"Survived\", data=df[df['AgeCat'] == 'Children'], ax=ax[0])\nsns.countplot(x=\"Pclass\", hue=\"Survived\", data=df[df['AgeCat'] == 'Teen'], ax=ax[1])\n\n# give title\nax[0].set_title(\"Children Surival\")\nax[1].set_title(\"Teen Surival\")","0e053ba6":"fig, ax = plt.subplots(figsize=(18,4))\n_ = df.groupby(['FamSize'])['Survived'].value_counts(normalize=True).rename('percentage').mul(100).reset_index()\nsns.barplot(x=\"FamSize\", y='percentage', hue=\"Survived\", data=_, ax=ax)\nax.set_title(\"Surival Percentage per Passenger Family Size\")\nax.grid(True, axis='y')","a586aba8":"fig, ax = plt.subplots(1,2, figsize=(18,4))\n_ = df.groupby(['HonTit'])['Survived'].value_counts(normalize=True).rename('percentage').mul(100).reset_index()\nsns.barplot(x=\"HonTit\", y='percentage', hue=\"Survived\", data=_, ax=ax[0])\nax[0].set_title(\"Surival Percentage per Passenger Title\")\nax[0].grid(True, axis='y')\n\n_ = df.groupby(['HonTit'])['Pclass'].value_counts(normalize=True).rename('percentage').mul(100).reset_index()\nsns.barplot(x=\"HonTit\", y=\"percentage\", hue='Pclass', data=_, ax=ax[1])\nax[1].set_title(\"Passenger Class Distribution per Title\")\nax[1].grid(True, axis='y')","8d74960a":"# import some algorithm\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n# separete data from the dataframe that has missing value in survived feature as test dataset, and other as training dataset\nX = df2[~df2['Survived'].isnull()].reset_index(drop=True)\ny = X['Survived'].astype('int64')\ntest = df2[df2['Survived'].isnull()].reset_index(drop=True)\n\n# drop survived column and passenger id columns\nX = X.drop(['PassengerId','Survived', 'Embarked'], axis=1)\ntest = test.drop(['PassengerId', 'Survived', 'Embarked'], axis=1)\n\nprint(\"There are :\", len(X), \"data in training dataset\")\nprint(\"There are :\", len(test), \"data in test dataset\")","b0cc592b":"# split data into train and test split\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)\nprint(X_train.shape, X_test.shape, y_train.shape, y_test.shape)","702b53e0":"# prediction using LinReg Classifier\nlogistic_regression = LogisticRegression()\nlogistic_regression.fit(X_train, y_train)\ny_pred_logreg = logistic_regression.predict(X_test)\n\n# prediction using KNN\nknn = KNeighborsClassifier(n_neighbors=3)\nknn.fit(X_train, y_train)\ny_pred_knn = knn.predict(X_test)\n\n# prediction using random forest\nrandom_forest = RandomForestClassifier(n_estimators=100, random_state=1)\nrandom_forest.fit(X_train, y_train)\ny_pred_ranfor = random_forest.predict(X_test)","74c99e6f":"# Validation with mean absolute error\nfrom sklearn.metrics import mean_absolute_error\nprint(\"MAE with Logistic Regression are: \", mean_absolute_error(y_test, y_pred_logreg))\nprint(\"MAE with KNN are: \", mean_absolute_error(y_test, y_pred_knn))\nprint(\"MAE with Random Forest are: \", mean_absolute_error(y_test, y_pred_ranfor))\n\n# validation with sklearn accuracy score\nfrom sklearn.metrics import accuracy_score\nprint(\"Accuracy score with Logistic Regression are: \", accuracy_score(y_test, y_pred_logreg))\nprint(\"Accuracy score with KNN are: \", accuracy_score(y_test, y_pred_knn))\nprint(\"Accuracy score with Random Forest are: \", accuracy_score(y_test, y_pred_ranfor))","3253f7af":"# prediction using decision tree regressor\nfrom sklearn.tree import DecisionTreeClassifier\n\n# create function to create MAE from different max leaf to control underfitting \/ overfitting\ndef check_mae(max_leaf, X_train, X_test, y_train, y_test):\n    model = DecisionTreeClassifier(max_leaf_nodes=max_leaf, random_state=1)\n    model.fit(X_train,y_train)\n    y_pred_tree = model.predict(X_test)\n    mae = mean_absolute_error(y_test, y_pred_tree)\n    return mae\n\n# loop over different max leaf nodes to get best MAE\nfor max_leaf in [5, 50, 500, 5000, 50000, 500000]:\n    print(\"Max Leaf Nodes :\", max_leaf, \"\\t\\t Mean Absolute Error :\", check_mae(max_leaf, X_train, X_test, y_train, y_test))","046ce0ac":"# set the max leaf nodes as 5000\ntree_classifier = DecisionTreeClassifier(max_leaf_nodes=5000, random_state=1)\ntree_classifier.fit(X_train, y_train)\ny_pred_tree = tree_classifier.predict(X_test)\nprint(\"MAE with Decision Tree Regressor are: \", mean_absolute_error(y_test, y_pred_tree))","3e5d998b":"# try using logistic regression CV\nfrom sklearn.linear_model import LogisticRegressionCV\n\n# create function to create MAE from different CV to control underfitting \/ overfitting\ndef check_mae(num_CV, X_train, X_test, y_train, y_test):\n    model = LogisticRegressionCV(cv=num_CV, random_state=1, multi_class='multinomial')\n    model.fit(X_train,y_train)\n    y_pred_tree = model.predict(X_test)\n    mae = mean_absolute_error(y_test, y_pred_tree)\n    return mae\n\n# loop over different max leaf nodes to get best MAE\nfor num_cv in [5, 10, 15, 20]:\n    print(\"Max Leaf Nodes :\", num_cv, \"\\t\\t Mean Absolute Error :\", check_mae(num_cv, X_train, X_test, y_train, y_test))","350192f3":"# store prediction from logreg model\nlogistic_regression = LogisticRegressionCV(cv=5, random_state=1, multi_class='multinomial')\nlogistic_regression.fit(X,y)\ny_pred = logistic_regression.predict(test)\n\n# make new dataframe with predictions\nsubmission = pd.DataFrame({'PassengerId': sub['PassengerId'], 'Survived': y_pred})\n\n# save to csv\nsubmission.to_csv('submission.csv', index=False)","986b5e6f":"The correlation of each features to *Survived* looks good to me, with Passenger Class, fare, and sex are have high score correlation to passenger survival.\n\n### 2. Data Exploration","057bf668":"Now i have the cabin code, i want to make some analysis with this data:\n1. which cabin that much more saver\n2. how the family size distribution for row with multiple cabin","c593183d":"Appereantly, most who don't survive are male, with class 3 are having the most casualies. It is logical since only economic class 1 and 2 passengers were have access to saveboat\n\n#### b. Are most childrens and teen saved?","c2f581a4":"Some title are much more an ships official titles, and there are also noble titles. I think will arrange the title into smaller category like this :\n1. Lady, Sir, Countess, Jonkheer, Don, Dona = Noble\n2. Rev, Dr = Professional\n2. Mr, Mrs, Miss, Master, Mme, Ms, Mlle = Common\n3. Major, Col, Capt = Officer\n\nI got reference after reading this [article](https:\/\/medium.com\/i-like-big-data-and-i-cannot-lie\/how-i-scored-in-the-top-9-of-kaggles-titanic-machine-learning-challenge-243b5f45c8e9)<br>\nFor Honorific Title reference, i look at this [article](https:\/\/en.wikipedia.org\/wiki\/English_honorifics) and other i found by googling","abcc7e79":"### Another Data Preparation\nThe data is much simplier now. But for the last touch, i still concern with the Honorific Title Category. I think this titles can be categorized even more to much more hierarchical category.","b059e16a":"1. ### Submission","78bf239c":"#### d. Are noble were much saved?","a35925db":"Much teen and children in class 3 are not survived then in class 1 and 2.\n\n#### c. Are passenger who bring less family member are saver?","caf98b25":"Result above show that if passenger bring 3 or more family member, they most likely have more cabin code in their data. From perspective of survival, Cabin B, C, D, and E are much more saver because it has more survived people. But this analysis actually would not be relevant because of manny missing value from the cabin and so this found can't directly be accepted\n\n### Data Customization\nIn this section, i will make new feature : Age Category, Honorific Title, and Family Size. For the age category, i use age categorization by WHO that i found in this [Link](https:\/\/help.healthycities.org\/hc\/en-us\/articles\/219556208-How-are-the-different-age-groups-defined-?mobile_site=true)\n. The configuration are:\n* Infant : < 1\n* Child : 1 - 11\n* Teen : 12 - 17\n* Adult : 18 - 64\n* Senior : 64 >","ac312348":"## Data Preparation\n***\nFor the analysis, i will combine the train and test dataset so i can see the whole condition and story from the data.","30e25c3a":"By looking at the result above, i will fill the missing value for each honorific title with the most age category, and so:\n1. Mr : Adult\n2. Mrs : Adult\n3. Miss : Adult\n4. Master : Children\n5. Dr : Adult\n6. Ms : Adult\n\n### Fill the Missing Value\n#### 1. Age Category\nFor age category, i will use the result that i have decided above","46ab18dc":"Most noble are saved from the titanic, while most professional are not. For noble, this is logical since 100% noble are in first class and so they have access to lifeboats. But even no proffesionals are in third class, much of proffesional are not survive.\n\n## Prediction\n***\nAfter doing some analysis, and according to correlation matrix result that quite good enough, i now proceed to prediction using some machine learning algorithm","ac70a627":"From result above, adult and most of senior adult were not survived. Most of childrens are survived, but most of teens are not, but i'm curious about the children in different passenger class","08043d09":"## Exploratory Data Analysis\n***\n\nNow the data are clean and i can proceed to data analysis. There are some analysis that i want to make from the data:\n1. **How each feature correlated to the passengers survival**\n2. **Data Exploration :**\n    1. Which economic class are the most in the titanic?\n    2. How is the fare distribution in the titanic?\n    3. How the age distribituion in the titanic?\n    4. How is the most gender in the ship?\n    5. Where is the most embarkment point?\n    6. How many family passenger bring to the ship?\n    7. Is there are many noble in the ship?\n    8. How many who did survive in the data?\n\n### 1. Feature Correlation\nBefore i calculate each features correlation, i should change some categorical features with string type into numerical type representation. To do that, i should change those categorical type to \"category\" and convert it to numerical","99584d98":"The cabin data consist of some alfabethical code followed by number. There are 147 unique cabin. There are some cabin that have one or more code seperated by space.","6736c9ee":"The data contain **1309 rows and 12 features**. Looking at the data, i conclude some condition:\n\n1. **Interesting Pattern:**\n    1. **Age** value are numerical. I think i could categorize the age to make the data more simple\n    2. **Cabin** data have some alfabethical code in front of the it numbers. But i found in 27th row the cabin data have multiple code\n    3. The **Name** value has honorific \/ title in it. And it seems related to the passenger's age (female children have title \"Miss\",          while female adult have title \"Mrs\", Male children have title \"Master\" While male adult have title \"Mr\").\n    4. **Parch and SibSp** give use how many family they have in the ship. This data could be combined to give use total family of each        passengers in the ship\n    5. There are a ticket with some code before its numerical value.\n    \n\n2. **Missing Value:**\n    1. There are several missing value found in **age, embarked, and fare** and a lot of missing value in **cabin** data.\n    2. i will fill missing value in **fare** with median value\n    3. to fill the age value, i think it would be simple if i categorize the age range, then use the pattern from the name to determine        passenger's age category\n    4. given so many missing value in **Cabin** feature, i don't think i can use the data as training dataset, altough cabin will give          us image where is passenger room are in the ship. But i still can use it for analysis. But before that, i should inspect this            feature more because of the 27th row that make this feature seems unclean\n    5. to fill the missing value in **embarked**, i will use the most place passenger embarked place.\n\n3. **Unused Feature:**\n    1. Name, Ticket, and passenger id will be dropped since these feature are an identitiy for the passenger thus it will not have some        pattern that will help the prediction\n    2. Since there were so many missing value in Cabin, i will drop this from training dataset. Although cabin position in the ship in          my opinion are important for suvivability, this data still are also represented by Pclass since in Titanic, cabin position have          been ordered with Passenger Class. (See this data on **internet**)\n    \n### Cabin Inspection","4f2ccd51":"There are some pattern from this data:\n1. There is some pattern in which passengers with many cabin code bring many family (except in 872th and 1263th row)\n2. Cabin with 4 code and 3 code all have the same first alphabet code\n3. Cabin with 2 code have some row with different alphabet code. There are F cabin combined with E and G Cabin\n\nI want to make new feature that store cabin first alphabet code. Since row with multiple cabin have same alphabetical code, and there are cabin with F followed by E and G code, i will mark this cabin still as the first alphabet, so for F followed with G or E cabin, they will have value as F","a8cec582":"Those plot give me clear picture about passenger aboard the titanic:\n1. Most passengers are the lower class (class 3). This is correlated to the fare of passengers that are mostly betweet 1 - 100 \n2. Most passengers are male and most age of passengers are categorized as adult\n3. Southampton are the most embarkment point\n4. Most passengers not bring any family member, or only bring 1 or 2 family member with them\n5. There are small amount of noble and known proffesional in the ship\n6. Most of passengers are not survived\n\n### 3. Surival Analysis\nNow i want to look to some factor of passenger survival\n\n#### a. Which economic class that are not survived what gender?","f657eca8":"Result above show that those with *master* title are most likely children. But i'm surprised that age range for title *miss* mostly in between 30 - 10, that is either adult or teen. Mr, and Mrs to have age ranging to 10.\n\nIn this condition, i decided to fill the age category that still missing with the most age category in each missing category. ","b8e9b75f":"We see age that are missing are **Mr, Mrs, Miss, Master, Dr, and Ms**. To determine wich age category they fall into, i will plot age range for each honorif title","1be51308":"#### 2. Fare and Embarked\nFor Fare, i will fill the missing value with median of fare <br>\nFor embarked, i will fill the missing value with the most point of embarkment","8221725a":"## Look at the data","aa3cd49b":"### Dropping Features\nNow the missing values has filled. I can now drop the other missing value and further unused data that i have been dicided before. Since i have sum the SibSp and Parch feature, i will drop this two features too"}}