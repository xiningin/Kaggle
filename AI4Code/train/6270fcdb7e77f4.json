{"cell_type":{"814e623c":"code","2d3c2f8a":"code","b588d4f6":"code","d86370cb":"code","b195b6c4":"code","823b78fa":"code","047a84bb":"code","0ac698eb":"code","72028968":"code","9e7fcae2":"code","5be84e39":"code","9d0afd9f":"code","2e489b06":"code","aab17699":"code","22a79bd4":"code","e4389c4d":"code","a0da184a":"code","9f2e1c3e":"code","a4912490":"markdown","23237075":"markdown","a2d2b325":"markdown","95791983":"markdown","6b49da9d":"markdown","9d922d7a":"markdown"},"source":{"814e623c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom sklearn.preprocessing import Normalizer\nfrom sklearn.cluster import KMeans, DBSCAN\nfrom sklearn import metrics","2d3c2f8a":"data = pd.read_csv('..\/input\/ccdata\/CC GENERAL.csv')\ndata.head()","b588d4f6":"print('Qtd of rows and columns is: ',data.shape)","d86370cb":"print(data.isna().sum())","b195b6c4":"# The features 'MINIMUN_PAYMENTS' is NaN.\n# I will replace NaN for the median\n\ndata.fillna(data.median(), inplace = True)\n\n# I will drop some columns that i will not use, like a \"CUST_ID\" and \"TENURE\"\n\ndata.drop(columns = ['CUST_ID', 'TENURE'], inplace = True)","823b78fa":"data.head()\n\n# the dataset its like i want to work...","047a84bb":"print(data.isna().sum())","0ac698eb":"# Normalizing the dataset values\n\nvalues = Normalizer().fit_transform(data.values)","72028968":"# KMeans\n\nkmeans = KMeans(n_clusters = 5, n_init=10, max_iter=300)\ny_pred = kmeans.fit_predict(values)\nlabels_km = kmeans.labels_\nprint('qtd of clustrs of k-means is: ',len(set(labels_km)))\n\n\n# DBSCAN\n\ndbscan = DBSCAN(eps=1.31, min_samples=15, metric ='manhattan').fit(values)\nlabels_db = dbscan.labels_\nprint('qtd of clustrs of dbscan is: ',len(set(labels_db)))\n\n# We can see that the dbscan is't work with this that set\n# nor will I test Meanshift because its simple see that this dataset is simple.","9e7fcae2":"def validation(data):\n    kmeans = KMeans(n_clusters = 5, n_init=10, max_iter=300).fit(data)\n    labels = kmeans.labels_\n    s = metrics.silhouette_score(data, labels, metric='euclidean')\n    dbs = metrics.davies_bouldin_score(data, labels)\n    calisnki = metrics.calinski_harabasz_score(data, labels)\n    return print(s, dbs, calisnki)","5be84e39":"# K-means results with the normal dataset\n\nvalidation(data = values)","9d0afd9f":"# results with a random dataset\n\nrandom_data = np.random.rand(8950,16)\n\nvalidation(random_data)\n\n# its normal that have varaivel the numbers. This results means that this cluster are adjusting well to the data and the amount of classes.","2e489b06":"# seeing the clustering stability\n# i will split the data for training\n\nset1, set2, set3 = np.array_split(values, 3)\n\nprint(len(set1))\nprint(len(set2))\nprint(len(set3))","aab17699":"# Here for validation this data set i need that the results are similars\n\nprint(f'para o 1 dataset:{validation(data = set1)}')\nprint('')\nprint(f'para o 2 dataset:{validation(data = set2)}')\nprint('')\nprint(f'para o 3 dataset:{validation(data = set2)}')\n\n# yes sir, they have similars results. Means that the clustering is OK...","22a79bd4":"# Picking the features with more variance.....\n# because are this that have more impact at the time to creting cluster\n\ncentroids = kmeans.cluster_centers_\nmax = len(centroids[0])\nfor i in range(max):\n    print(data.columns.values[i],'\\n{:.4f}'.format(centroids[:,i].var()))\n\n# On start of this notebook the feature \"MINIMAL_PAYMENTS\" is filled with value \"NaN\" and i fill that with median of the values... because of this the values is very high","e4389c4d":"# add clusters on dataset and number of clients \n\ndata['clusters'] = labels_km\n\ndata.head()","a0da184a":"# Make by the tecaher on alura, was confused me\n\ndescription = data.groupby(\"clusters\")[\"BALANCE\", \"PURCHASES\", \"CASH_ADVANCE\", \"CREDIT_LIMIT\", \"PAYMENTS\"]\nn_clients = description.size()\ndescription = description.mean()\ndescription['n_clients'] = n_clients\ndescription","9f2e1c3e":"# Only values that have more than 0.01 (variance) to creating a new dataframe.\n\nanalisys = data.groupby('clusters')['BALANCE','PURCHASES','CASH_ADVANCE','CREDIT_LIMIT','PAYMENTS']\nn_clients = analisys.size()\nanalisys = analisys.mean()\nanalisys['n_clients'] = n_clients\nanalisys\n\n# Now its OK to make a Analysis, here i have the features that are most important for clustering... So... what is the question?","a4912490":"# Conclusion of this cluster","23237075":"# Analisys of data","a2d2b325":"# Validation","95791983":"# Modifying data","6b49da9d":"# Cluestering","9d922d7a":"# Normalizing values"}}