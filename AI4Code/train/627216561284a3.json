{"cell_type":{"501efec2":"code","06d2b900":"markdown"},"source":{"501efec2":"import numpy as np\nimport pickle\n\nBOARD_ROWS = 3\nBOARD_COLS = 3\nBOARD_SIZE = BOARD_ROWS * BOARD_COLS\n\n\nclass State:\n    def __init__(self):\n        # the board is represented by an n * n array,\n        # 1 represents a chessman of the player who moves first,\n        # -1 represents a chessman of another player\n        # 0 represents an empty position\n        self.data = np.zeros((BOARD_ROWS, BOARD_COLS))\n        self.winner = None\n        self.hash_val = None\n        self.end = None\n\n    # compute the hash value for one state, it's unique\n    def hash(self):\n        if self.hash_val is None:\n            self.hash_val = 0\n            for i in np.nditer(self.data):\n                self.hash_val = self.hash_val * 3 + i + 1\n        return self.hash_val\n\n    # check whether a player has won the game, or it's a tie\n    def is_end(self):\n        if self.end is not None:\n            return self.end\n        results = []\n        # check row\n        for i in range(BOARD_ROWS):\n            results.append(np.sum(self.data[i, :]))\n        # check columns\n        for i in range(BOARD_COLS):\n            results.append(np.sum(self.data[:, i]))\n\n        # check diagonals\n        trace = 0\n        reverse_trace = 0\n        for i in range(BOARD_ROWS):\n            trace += self.data[i, i]\n            reverse_trace += self.data[i, BOARD_ROWS - 1 - i]\n        results.append(trace)\n        results.append(reverse_trace)\n\n        for result in results:\n            if result == 3:\n                self.winner = 1\n                self.end = True\n                return self.end\n            if result == -3:\n                self.winner = -1\n                self.end = True\n                return self.end\n\n        # whether it's a tie\n        sum_values = np.sum(np.abs(self.data))\n        if sum_values == BOARD_SIZE:\n            self.winner = 0\n            self.end = True\n            return self.end\n\n        # game is still going on\n        self.end = False\n        return self.end\n\n    # @symbol: 1 or -1\n    # put chessman symbol in position (i, j)\n    def next_state(self, i, j, symbol):\n        new_state = State()\n        new_state.data = np.copy(self.data)\n        new_state.data[i, j] = symbol\n        return new_state\n\n    # print the board\n    def print_state(self):\n        for i in range(BOARD_ROWS):\n            print('------------')\n            out = '| '\n            for j in range(BOARD_COLS):\n                if self.data[i, j] == 1:\n                    token = '*'\n                elif self.data[i, j] == -1:\n                    token = 'x'\n                else:\n                    token = '0'\n                out += token + ' | '\n            print(out)\n        print('------------')\n\n\ndef get_all_states_impl(current_state, current_symbol, all_states):\n    for i in range(BOARD_ROWS):\n        for j in range(BOARD_COLS):\n            if current_state.data[i][j] == 0:\n                new_state = current_state.next_state(i, j, current_symbol)\n                new_hash = new_state.hash()\n                if new_hash not in all_states:\n                    is_end = new_state.is_end()\n                    all_states[new_hash] = (new_state, is_end)\n                    if not is_end:\n                        get_all_states_impl(new_state, -current_symbol, all_states)\n\n\ndef get_all_states():\n    current_symbol = 1\n    current_state = State()\n    all_states = dict()\n    all_states[current_state.hash()] = (current_state, current_state.is_end())\n    get_all_states_impl(current_state, current_symbol, all_states)\n    return all_states\n\n\n# all possible board configurations\nall_states = get_all_states()\n\n\nclass Judger:\n    # @player1: the player who will move first, its chessman will be 1\n    # @player2: another player with a chessman -1\n    def __init__(self, player1, player2):\n        self.p1 = player1\n        self.p2 = player2\n        self.current_player = None\n        self.p1_symbol = 1\n        self.p2_symbol = -1\n        self.p1.set_symbol(self.p1_symbol)\n        self.p2.set_symbol(self.p2_symbol)\n        self.current_state = State()\n\n    def reset(self):\n        self.p1.reset()\n        self.p2.reset()\n\n    def alternate(self):\n        while True:\n            yield self.p1\n            yield self.p2\n\n    # @print_state: if True, print each board during the game\n    def play(self, print_state=False):\n        alternator = self.alternate()\n        self.reset()\n        current_state = State()\n        self.p1.set_state(current_state)\n        self.p2.set_state(current_state)\n        if print_state:\n            current_state.print_state()\n        while True:\n            player = next(alternator)\n            i, j, symbol = player.act()\n            next_state_hash = current_state.next_state(i, j, symbol).hash()\n            current_state, is_end = all_states[next_state_hash]\n            self.p1.set_state(current_state)\n            self.p2.set_state(current_state)\n            if print_state:\n                current_state.print_state()\n            if is_end:\n                return current_state.winner\n\n\n# AI player\nclass Player:\n    # @step_size: the step size to update estimations\n    # @epsilon: the probability to explore\n    def __init__(self, step_size=0.1, epsilon=0.1):\n        self.estimations = dict()\n        self.step_size = step_size\n        self.epsilon = epsilon\n        self.states = []\n        self.greedy = []\n        self.symbol = 0\n\n    def reset(self):\n        self.states = []\n        self.greedy = []\n\n    def set_state(self, state):\n        self.states.append(state)\n        self.greedy.append(True)\n\n    def set_symbol(self, symbol):\n        self.symbol = symbol\n        for hash_val in all_states:\n            state, is_end = all_states[hash_val]\n            if is_end:\n                if state.winner == self.symbol:\n                    self.estimations[hash_val] = 1.0\n                elif state.winner == 0:\n                    # we need to distinguish between a tie and a lose\n                    self.estimations[hash_val] = 0.5\n                else:\n                    self.estimations[hash_val] = 0\n            else:\n                self.estimations[hash_val] = 0.5\n\n    # update value estimation\n    def backup(self):\n        states = [state.hash() for state in self.states]\n\n        for i in reversed(range(len(states) - 1)):\n            state = states[i]\n            td_error = self.greedy[i] * (\n                self.estimations[states[i + 1]] - self.estimations[state]\n            )\n            self.estimations[state] += self.step_size * td_error\n\n    # choose an action based on the state\n    def act(self):\n        state = self.states[-1]\n        next_states = []\n        next_positions = []\n        for i in range(BOARD_ROWS):\n            for j in range(BOARD_COLS):\n                if state.data[i, j] == 0:\n                    next_positions.append([i, j])\n                    next_states.append(state.next_state(\n                        i, j, self.symbol).hash())\n\n        if np.random.rand() < self.epsilon:\n            action = next_positions[np.random.randint(len(next_positions))]\n            action.append(self.symbol)\n            self.greedy[-1] = False\n            return action\n\n        values = []\n        for hash_val, pos in zip(next_states, next_positions):\n            values.append((self.estimations[hash_val], pos))\n        # to select one of the actions of equal value at random due to Python's sort is stable\n        np.random.shuffle(values)\n        values.sort(key=lambda x: x[0], reverse=True)\n        action = values[0][1]\n        action.append(self.symbol)\n        return action\n\n    def save_policy(self):\n        with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'wb') as f:\n            pickle.dump(self.estimations, f)\n\n    def load_policy(self):\n        with open('policy_%s.bin' % ('first' if self.symbol == 1 else 'second'), 'rb') as f:\n            self.estimations = pickle.load(f)\n\n\n# human interface\n# input a number to put a chessman\n# | q | w | e |\n# | a | s | d |\n# | z | x | c |\nclass HumanPlayer:\n    def __init__(self, **kwargs):\n        self.symbol = None\n        self.keys = [\n                     'q', 'w', 'e',\n                     'a', 's', 'd',\n                     'z', 'x', 'c'\n                     ]\n        self.state = None\n\n    def reset(self):\n        pass\n\n    def set_state(self, state):\n        self.state = state\n\n    def set_symbol(self, symbol):\n        self.symbol = symbol\n\n    def act(self):\n        self.state.print_state()\n        key = input(\"Input your position:\")\n        data = self.keys.index(key)\n        i = data \/\/ BOARD_COLS\n        j = data % BOARD_COLS\n        return i, j, self.symbol\n\n\ndef train(epochs, print_every_n=500):\n    player1 = Player(epsilon=0.01)\n    player2 = Player(epsilon=0.01)\n    judger = Judger(player1, player2)\n    player1_win = 0.0\n    player2_win = 0.0\n    for i in range(1, epochs + 1):\n        winner = judger.play(print_state=False)\n        if winner == 1:\n            player1_win += 1\n        if winner == -1:\n            player2_win += 1\n        if i % print_every_n == 0:\n            print('Epoch %d, player 1 winrate: %.02f, player 2 winrate: %.02f' % (i, player1_win \/ i, player2_win \/ i))\n        player1.backup()\n        player2.backup()\n        judger.reset()\n    player1.save_policy()\n    player2.save_policy()\n\n\ndef compete(turns):\n    player1 = Player(epsilon=0)\n    player2 = Player(epsilon=0)\n    judger = Judger(player1, player2)\n    player1.load_policy()\n    player2.load_policy()\n    player1_win = 0.0\n    player2_win = 0.0\n    for _ in range(turns):\n        winner = judger.play()\n        if winner == 1:\n            player1_win += 1\n        if winner == -1:\n            player2_win += 1\n        judger.reset()\n    print('%d turns, player 1 win %.02f, player 2 win %.02f' % (turns, player1_win \/ turns, player2_win \/ turns))\n\n\n# The game is a zero sum game. If both players are playing with an optimal strategy, every game will end in a tie.\n# So we test whether the AI can guarantee at least a tie if it goes second.\ndef play():\n    while True:\n        player1 = HumanPlayer()\n        player2 = Player(epsilon=0)\n        judger = Judger(player1, player2)\n        player2.load_policy()\n        winner = judger.play()\n        if winner == player2.symbol:\n            print(\"You lose!\")\n        elif winner == player1.symbol:\n            print(\"You win!\")\n        else:\n            print(\"It is a tie!\")\n\n\nif __name__ == '__main__':\n    print('Begin training...')\n    train(int(1e5))\n    print('Begin compete...')\n    compete(int(1e3))\n    print('Play agaist the player...')\n    play()","06d2b900":"# 3x3 tic-tac-toe AI\n## TODO: COMMENT THE CODE\n\n"}}