{"cell_type":{"b57c5917":"code","714d6260":"code","d466c7e7":"code","61db9c51":"code","5f5ce44b":"code","2b8a83a9":"code","d4801bf6":"code","059999ec":"code","f254b0a2":"code","996f9f58":"code","511f0de1":"code","3741396d":"code","b2fdf34c":"code","e4c2efbf":"code","f060702d":"code","8aed0bb0":"code","c861840c":"code","2dd95c2d":"code","7c3dddfa":"code","934cb354":"code","87a69c5f":"code","383c486f":"code","312c257d":"code","9f787c1c":"code","3cd6e794":"code","2c0013cd":"code","dc35f300":"code","111dbf7a":"code","04d1f542":"code","912b1dd5":"code","b2a54432":"code","86cd865c":"code","d0c19765":"code","abbe6fe7":"code","29742e7f":"code","b2bcd64c":"code","1ab98930":"code","20ba921e":"code","85b13404":"code","4265f7f0":"code","4fdc9b3c":"code","fc30beea":"code","bca3b1a4":"code","2b461df7":"markdown","b14c2558":"markdown","085aee32":"markdown","f19a076b":"markdown","84ca577b":"markdown","1f18900b":"markdown","4c1b10c0":"markdown","f185b4f3":"markdown","20ecf8c5":"markdown","2d14af44":"markdown","538d25f2":"markdown","8ef16736":"markdown","8f9e9a90":"markdown","e22aaec2":"markdown","3fa01a36":"markdown","f85fe8ae":"markdown","c4872230":"markdown","5c03d106":"markdown","26b35e81":"markdown","27fbabc3":"markdown","7578dfe0":"markdown","dd08400f":"markdown","853a96c8":"markdown","9db790c5":"markdown","e168598c":"markdown","31e80d46":"markdown","0c1b28b8":"markdown","56bc2279":"markdown"},"source":{"b57c5917":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","714d6260":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom numpy import random","d466c7e7":"df_Train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ndf_Test = pd.read_csv(\"..\/input\/titanic\/test.csv\")","61db9c51":"df_Train.head()","5f5ce44b":"print(\"Shape\")\nprint(\"Training data : \", df_Train.shape)\nprint(\"Test data     : \", df_Test.shape)","2b8a83a9":"df_Train.describe(include='all')","d4801bf6":"df_Test.describe(include='all')","059999ec":"print(df_Train.columns)\nprint()\nprint(df_Train.dtypes)\nprint()\n\ns = (df_Train.dtypes == 'object')\nobject_cols = list(s[s].index)\nprint(\"Categorical variables:\")\nprint(object_cols)\n\ns = (df_Train.dtypes == 'int')\nnum_cols = list(s[s].index)\nprint(\"Integer variables:\")\nprint(num_cols)\n\ns = (df_Train.dtypes == 'float')\nnum_cols = list(s[s].index)\nprint(\"Real variables:\")\nprint(num_cols)","f254b0a2":"plt.figure(figsize=(10,12))\nplt.title('Frequencies')\n\nplt.subplot(3,2,1)\nsns.countplot(x = df_Train['Sex'])\n\nplt.subplot(3,2,2)\nsns.countplot(x = df_Train['Pclass'])\n\nplt.subplot(3,2,3)\nsns.countplot(x = df_Train['SibSp'])\n\nplt.subplot(3,2,4)\nsns.countplot(x = df_Train['Parch'])\n\nplt.subplot(3,2,5)\nsns.countplot(x = df_Train['Embarked'])\n\nplt.subplot(3,2,6)\nsns.countplot(x = df_Train['Fare'])","996f9f58":"plt.figure(figsize=(10,12))\nplt.title('Survival Fractions')\n\nplt.subplot(3,2,1)\nsns.barplot(x = df_Train['Sex'],y = df_Train['Survived'])\n\nplt.subplot(3,2,2)\nsns.barplot(x = df_Train['Pclass'],y = df_Train['Survived'])\n\nplt.subplot(3,2,3)\nsns.barplot(x = df_Train['SibSp'],y = df_Train['Survived'])\n\nplt.subplot(3,2,4)\nsns.barplot(x = df_Train['Parch'],y = df_Train['Survived'])\n\nplt.subplot(3,2,5)\nsns.barplot(x = df_Train['Embarked'],y = df_Train['Survived'])\n\nprint('Survival Fraction')\n\nfor i in df_Train['Sex'].unique():\n    print('Sex = ', i, ': ', df_Train['Survived'][df_Train['Sex'] == i].value_counts(normalize=True)[1])\nprint()\n\nfor i in df_Train['Pclass'].sort_values().unique():\n  print('Pclass = ', i, ': ', df_Train['Survived'][df_Train['Pclass'] == i].value_counts(normalize=True)[1])\nprint()\n\nfor i in df_Train['SibSp'].sort_values().unique():\n  if (df_Train['Survived'][df_Train['SibSp']==i].sum() > 0):\n    print('SibSp = ', i, ': ', df_Train['Survived'][df_Train['SibSp'] == i].value_counts(normalize=True)[1])\n  else:\n    print('SibSp = ', i, ': ', 0)\n\nprint()\n\nfor i in df_Train['Parch'].sort_values().unique():\n  if (df_Train['Survived'][df_Train['Parch']==i].sum() > 0):\n    print('Parch = ', i, ': ', df_Train['Survived'][df_Train['Parch'] == i].value_counts(normalize=True)[1])\n  else:\n    print('Parch = ', i, ': ', 0)\nprint()\n\nfor i in df_Train['Embarked'].dropna().unique():\n  print('Embarked = ', i, ': ', df_Train['Survived'][df_Train['Embarked'] == i].value_counts(normalize=True)[1])\nprint()","511f0de1":"#sns.distplot(X_Train['Age'],kde=False, label='Total')\nsns.histplot(df_Train['Age'][df_Train['Survived']==1],kde=False,label='Survived')\nsns.histplot(df_Train['Age'][df_Train['Survived']==0],kde=False,label='Not Survived')\nplt.legend()","3741396d":"#sns.distplot(X_Train['Fare'],kde=False, label='Total')\nsns.histplot(df_Train['Fare'][df_Train['Survived']==1],kde=False,label='Survived')\nsns.histplot(df_Train['Fare'][df_Train['Survived']==0],kde=False,label='Not Survived')\nplt.legend()","b2fdf34c":"df_combined = pd.concat([df_Train,df_Test])\nX_Train = df_Train.copy()\nX_Test = df_Test.copy()","e4c2efbf":"print(\"Missing Values : Training Data\")\nprint(df_Train.isnull().sum())\nprint()\nprint(\"Missing Values : Test Data\")\nprint(df_Test.isnull().sum())","f060702d":"X_Train[\"CabinBool\"] = (X_Train[\"Cabin\"].notnull().astype('int'))\nX_Test[\"CabinBool\"] = (X_Test[\"Cabin\"].notnull().astype('int'))","8aed0bb0":"sns.barplot(x = X_Train['CabinBool'],y = X_Train['Survived'])","c861840c":"df_Train.fillna({'Embarked' : 'S'},inplace=True)","2dd95c2d":"Age_mean = df_combined['Age'].mean()\nAge_sigma  = df_combined['Age'].std()\nX_Train['Age'][X_Train['Age'].isnull()] = random.randint(Age_mean - Age_sigma, Age_mean + Age_sigma, int(X_Train['Age'].isnull().sum()))\nX_Test['Age'][X_Test['Age'].isnull()]   = random.randint(Age_mean - Age_sigma, Age_mean + Age_sigma, int(X_Test['Age'].isnull().sum()))","7c3dddfa":"#sns.distplot(X_Train['Age'],kde=False, label='Total')\nsns.histplot(X_Train['Age'][X_Train['Survived']==1],kde=False,label='Survived')\nsns.histplot(X_Train['Age'][X_Train['Survived']==0],kde=False,label='Not Survived')\nplt.legend()","934cb354":"df_Test[df_Test['Fare'].isnull()]","87a69c5f":"X_Test.loc[152, 'Fare'] = df_combined['Fare'][df_combined['Pclass']==3].mean()","383c486f":"X_Train.head()","312c257d":"X_Train.drop(columns='PassengerId',inplace=True)\nX_Test.drop(columns='PassengerId',inplace=True)","9f787c1c":"X_Train.drop(columns='Name',inplace=True)\nX_Test.drop(columns='Name',inplace=True)","3cd6e794":"sex_mapping = {'male': 0, 'female': 1}\nX_Train['Sex'] = df_Train['Sex'].map(sex_mapping)\nX_Test['Sex'] = df_Test['Sex'].map(sex_mapping)","2c0013cd":"X_Train.drop(columns='Ticket',inplace=True)\nX_Test.drop(columns='Ticket',inplace=True)","dc35f300":"X_Train.drop(columns='Cabin',inplace=True)\nX_Test.drop(columns='Cabin',inplace=True)","111dbf7a":"embarked_mapping = {'S' : 0, 'Q' : 1, 'C' : 2}\nX_Train['Embarked'] = df_Train['Embarked'].map(embarked_mapping)\nX_Test['Embarked']  = df_Test['Embarked'].map(embarked_mapping)","04d1f542":"X_Train['Family_Size'] = X_Train['SibSp'] + X_Train['Parch'] + 1\nsns.barplot(x = X_Train['Family_Size'],y = X_Train['Survived'])","912b1dd5":"X_Train['Small_Family'] = 0\nX_Train.loc[(X_Train['Family_Size'] > 1) & (X_Train['Family_Size'] < 5) , 'Small_Family'] = 1\nX_Train.drop(columns = ['SibSp','Parch','Family_Size'], inplace = True)","b2a54432":"X_Test['Family_Size'] = X_Test['SibSp'] + X_Test['Parch'] + 1\nX_Test['Small_Family'] = 0\nX_Test.loc[(X_Test['Family_Size'] > 1) & (X_Test['Family_Size'] < 5) , 'Small_Family'] = 1\nX_Test.drop(columns = ['SibSp','Parch','Family_Size'], inplace = True)","86cd865c":"X_Train['Fare_band'] = pd.qcut(X_Train['Fare'],q=4,labels=[1,2,3,4])\nX_Test['Fare_band']  = pd.qcut(X_Test['Fare'],q=4,labels=[1,2,3,4])\nX_Train.drop(columns ='Fare', inplace = True)\nX_Test.drop(columns = 'Fare', inplace = True)\nsns.barplot(x = X_Train['Fare_band'] ,y = X_Train['Survived'])","d0c19765":"X_Train['AgeBand'] = pd.cut(X_Train['Age'], 5)\nX_Train[['AgeBand', 'Survived']].groupby(['AgeBand'], as_index=False).mean().sort_values(by='AgeBand', ascending=True)","abbe6fe7":"Age_bins = [0,16,32,48,64,80]\nX_Train['AgeBand'] = pd.cut(X_Train['Age'], bins = Age_bins, labels = [0,1,2,3,4])\nX_Test['AgeBand']  = pd.cut(X_Test['Age'], bins = Age_bins, labels = [0,1,2,3,4])","29742e7f":"X_Train.drop(columns = 'Age', inplace = True)\nX_Test.drop(columns = 'Age', inplace = True)  ","b2bcd64c":"X_Train.head()","1ab98930":"X_Test.head()","20ba921e":"Y_Train = X_Train['Survived']\nX_Train.drop(columns = 'Survived', inplace = True)","85b13404":"# Logistic Regression\nfrom sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(X_Train, Y_Train)\nY_Pred_logreg = logreg.predict(X_Test)\nacc_logreg = round(logreg.score(X_Train, Y_Train) * 100, 2)\nprint(acc_logreg)","4265f7f0":"# Decision Tree Classifier\nfrom sklearn.tree import DecisionTreeClassifier\n\ndectree = DecisionTreeClassifier()\ndectree.fit(X_Train, Y_Train)\nY_Pred_dectree = dectree.predict(X_Test)\nacc_dectree = round(dectree.score(X_Train, Y_Train) * 100, 2)\nprint(acc_dectree)","4fdc9b3c":"# Random Forest Classifier\nfrom sklearn.ensemble import RandomForestClassifier \n\nranfor = RandomForestClassifier()\nranfor.fit(X_Train, Y_Train)\nY_Pred_ranfor = ranfor.predict(X_Test)\nacc_ranfor = round(ranfor.score(X_Train, Y_Train) * 100, 2)\nprint(acc_ranfor)","fc30beea":"# Stochastic Gradient Descent\nfrom sklearn.linear_model import SGDClassifier\n\nsgd = SGDClassifier()\nsgd.fit(X_Train,Y_Train)\nY_Pred_sgd = sgd.predict(X_Test)\nacc_sgd = round(sgd.score(X_Train,Y_Train) * 100, 2)\nprint(acc_sgd)","bca3b1a4":"submission = pd.DataFrame({\"PassengerId\": df_Test[\"PassengerId\"], \"Survived\": Y_Pred_ranfor })\nsubmission.to_csv('Titanic.csv', index=False)","2b461df7":"### Age :\nWe can either leave the column 'Age' as it is and use it as a feature vector. In this case it might be better to shift and rescale the values using the mean and standard deviation.\nAn alternate approach would be to band different age groups into a number of bins (5 or 6) and label the passenger age depending on the age group into which it falls.","b14c2558":"### Cabin :\nDrop this column as it contains a large number of missing values. Any useful information has been saved in the column CabinBool.","085aee32":"Do the same for the Test Dataset.","f19a076b":"### Embarked :\nMap the 'Embarked' column into numerical feature vector such that S : 0, Q : 1 and C : 2.\nAlternatively one can perform a onehot coding of the three labels for better results (maybe).","84ca577b":"## Import Dataset","1f18900b":"### Embarked:\nThere are two missing values in the training set. We will fill up these values with the mode of the mode of the distribution i.e. with 'S'","4c1b10c0":"Both Random Forest Classifier and Decision Tree Classifier have better accuracies compared to the other methods.","f185b4f3":"## Explore the Dataset","20ecf8c5":"### Ticket :\nDrop this column as it is unlikely to yield any useful information.","2d14af44":"Family sizes with No. of Members = 2,3, and 4 are more likely to survive (probability ~ 60-70%) wheras persons travellig alone (Family size = 1) or with a large family (Family size >= 5) have a low survival probability (~15-25%). We can create a feature vector 'Small_Family' and set it to 1 for Family Sizes 2,3, and 4. Use this feature vector and drop 'SibSp', 'Parch' and 'Family_Size'","538d25f2":"### Cabin:\n\nThe majority of the entries in the feature 'Cabin' is missing. So we might as well drop it. However it possible that the values were more likely to be available for those that survived the sinking. Conversely, the passenger with a cabin information is more likely to survive.","8ef16736":"### PassengerId :\nDrop this column as survival is unlikely to depend on these values.","8f9e9a90":"Thus the Training and Test datasets contains records of 891 and 418 passengers, respectively. In the Training dateset, there is column 'Survived' that specifies whether the passenger did or did not survive. ","e22aaec2":"## Preparing submission File\nI would be using the results obtained from the RandomForestClassifier to submit to Kaggle.","3fa01a36":"### Fare :\nWe will be dividing the column 'Fare' into 4 quantiles with labels : 1,2,3,4. This will be saved as a new feature 'Fare_band'. Subsequently drop the 'Fare' column.","f85fe8ae":"## Data Cleaning\nIn this section, we will go over the columns one by one and modify them as features if necessary.","c4872230":"The datasets have the following columns : \n* Categorical:\n  * Name : Passenger name\n  * Sex : female or male\n  * Ticket : Ticket number, an alphnumeric value\n  * Cabin : Cabin number, an alphanumeric value\n  * Embarked : Where the passenger embarked on the ship ( S : Southampton, C : Cherbourg or Q : Queenstown)\n* Integer:\n  * PassengerId\n  * Survived : 0 (did not survive) or 1 (survived)\n  * Pclass: Passenger class ( 1, 2 or 3)\n  * SibSp : Number of siblings or spouse\n  * Parch : Number of parents or children\n* Floating:\n  * Age : Age of the passenger\n  * Fare : Ticket fare","5c03d106":"Finally prepare for model fitting. Copy the column 'Survived' into Y_Train and delete the column from the training set.","26b35e81":"## Missing Values","27fbabc3":"## Model Fitting\nThe problem is a classification problem. I would be using the following classifiers and compare the results.\n* Logistic Regression\n* Decision Tree Classifier\n* Random Forest Classifier\n* Stochastic Gradient Descent\n","7578dfe0":"### Age:\nThe training and test datasets have 177 and 86 missing values for Age, respectively. There are a large number of missing values, and they can be dealt with the following approaches:\n* Fill the missing values with a random distribution with mean and standard deviation of the training and test sets respectively. However, this can give rise to unphysical Age values < 0.\n* Fill the missing values with random integer values between [Mean - Std. Dev, Mean +  Std. Dev]. I will be using this approach as a first approximation.\n* A more elegant approach might be to infer the age of the passenger based on the title (i.e. Mr. Mrs. Master etc.) on the passenger's name.","dd08400f":"### Pclass :\nThis column can be directly used as a feature vector.\n\n### Name :\nAs a first approximation, let us drop this column. A more detailed approach might be to extract a feature depending on the title on the passenger name. However this is likely to have a correspondence with the Age of the passengers.","853a96c8":"### Fare:\nThe Test set has 1 missing fare value. We can set it to be the mean of the fares of the corresponding Pclass.","9db790c5":"### Observations\n* The passengers in the training set were predominantly male ( almost twice as many as the number of females). However the chances of survival were much higher for female passengers. \n* The number of passengers travelling in 3rd class was much larger compared with the other two. However the probability of survival decreases with the passenger class with PC1 > PC2 > PC3. This might be expected.\n* Most of the passengers embarked at Southampton. For some reason the chance of survival is maximum for Cherbourg, followed by Queenstown and Southampton.\n* The number of passengers travelling alone outnumbers the others (i.e. those with children, parents of spouses). It is difficult to see direclty from the probabilities any correlation between SibSp and Parch. All we can say is that people who were travelling alone or with large families were less likely to survive. People who were accompanied by 1-2 spouse\/siblings or 2-3 parents\/children were more likely to survive.\n* Children were more likely to survive.\n* Fares has an extremmely skewed distribution. We need to either (a) plot it on a logithmic scale and see if it has a more uniform  distribution, or (b) divide it into suitable bins.","e168598c":"### SibSp and Parch :\nSince both these columns have information regarding the family sizes and co-passengers, we can combine both of them to create a new feature called 'Family_Size' = No. of Parents\/Children + No. of Siblings\/Spouses + 1 (the passenger himself\/herself). Plot and see how survival probability depends on this feature.","31e80d46":"## Data Analysis and Visualization\n","0c1b28b8":"# Titanic : Machine Learning from Disaster\nThis is a beginners attempt to work with the Titanic: ML from disaster dataset available from [Kaggle](https:\/\/www.kaggle.com\/c\/titanic\/data).\nThe data has been already separated into two groups - the training set and the test set.\n\n### Contents:\n* Import dataset from Kaggle\/local storage.\n* Import necessary libraries.\n* Explore and visualize the dataset. Identify numerical and categorical features.\n* Fnd outif any of the features has missing data. If yes, fill up missing data.\n* Feature Engineering : Drop unneccesary columns. Adjust other columns into usable features.\n* Model fitting : Choose best fit model.\n\n## Import necessary Libraries","56bc2279":"### Sex :\nWe can map this column to numerical values such that male : 0 and female : 1."}}