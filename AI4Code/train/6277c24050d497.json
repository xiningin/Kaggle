{"cell_type":{"fc64e416":"code","6bd6839d":"code","eb10c799":"code","14aa7deb":"code","ed9dd7e7":"code","56819ddd":"code","ffd1fbcf":"code","ad43a2c6":"code","2218c975":"code","a52dff74":"code","2e11cf35":"code","d332473f":"code","bba3cc2f":"code","b2e5f2ba":"code","972ef3e3":"code","9aee9241":"code","4997a1c0":"code","f9be2c11":"code","57a0b234":"code","15055322":"code","9759ccaa":"code","76d9bff4":"code","a6fd940c":"code","c5e14090":"code","9f11278d":"code","c316f5ef":"code","a7adc67f":"code","0e0a7819":"code","dae263ff":"code","0d125f3c":"markdown","34cb91f8":"markdown","74a172a7":"markdown","c95e815c":"markdown","2d6d4924":"markdown","ae4155a7":"markdown","1ac9163e":"markdown","65f2c914":"markdown","238fa161":"markdown"},"source":{"fc64e416":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt \nimport seaborn as sns\n%matplotlib inline\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.offline as py\nimport plotly.express as px\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","6bd6839d":"df = pd.read_csv('..\/input\/lexical-relations-from-the-wisdom-of-the-crowd\/toloka-isa-50-skip-300-train-hit.tsv', sep='\\t', error_bad_lines=False)\ndf.head()","eb10c799":"df1 = pd.read_csv('..\/input\/lexical-relations-from-the-wisdom-of-the-crowd\/lrwc-1.1-assignments.tsv', sep='\\t', error_bad_lines=False)\ndf1.head()","14aa7deb":"df2 = pd.read_csv('..\/input\/lexical-relations-from-the-wisdom-of-the-crowd\/lrwc-1.1-aggregated.tsv', sep='\\t', error_bad_lines=False)\ndf2.head()","ed9dd7e7":"df.isnull().sum()","56819ddd":"# keras\nimport keras\n\n# matplotlib\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nnp.random.seed(42)","ffd1fbcf":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom sklearn.model_selection import train_test_split\n\ndef tokenizer_sequences(num_words, X):\n    \n    # when calling the texts_to_sequences method, only the top num_words are considered while\n    # the Tokenizer stores everything in the word_index during fit_on_texts\n    tokenizer = Tokenizer(num_words=num_words)\n    # From doc: By default, all punctuation is removed, turning the texts into space-separated sequences of words\n    tokenizer.fit_on_texts(X)\n    sequences = tokenizer.texts_to_sequences(X)\n    \n    return tokenizer, sequences","ad43a2c6":"max_words = 10000 \n# for the tokenizer, we configure it to only take into account the 1000 most common words when calling the texts_to_sequences method.\n\nmaxlen = 300\n# maxlen is the dimension that each email will have a fixed word sequence, in this case each email will be of a 1-d tensor (300,).","2218c975":"!pip install dataprep","a52dff74":"#Code by https:\/\/docs.dataprep.ai\/user_guide\/clean\/clean_headers.html\n\nfrom dataprep.clean import clean_headers\nclean_headers(df)","2e11cf35":"df=clean_headers(df)","d332473f":"#Use a Tokenizer class of Keras to convert Messages text to sequences consistently\n\ntokenizer, sequences = tokenizer_sequences(max_words, df.hint_text.copy())","bba3cc2f":"word_index = tokenizer.word_index\nprint('Found %s unique tokens.' % len(word_index))\n\n# # We will pad all input sequences to have the length of 300. Each email will be the same length of sequence.\nX = pad_sequences(sequences, maxlen=maxlen)\n\ny = df.golden_judgement.copy()\n\nprint('Shape of data tensor:', X.shape)\nprint('Shape of label tensor:', y.shape)","b2e5f2ba":"max_words = len(tokenizer.word_index) + 1 # 33672 + 1\n# 0 is reserved for padding \/no data. The word indexes (i.e. tokenizer.word_index) are 1-offset.\n# max_words is the size of the vocabulary, you can think of a book is of max_words pages\n\nembedding_dim = 100 # the dimension of the word dictinory, i.e. this will be 100-dimensional word vector\n# you can think of a book that each page has embedding_dim words.","972ef3e3":"from keras.models import Sequential\nfrom keras.layers import Flatten, Dense, Embedding\n\nmodel = Sequential()\n\n# embedding dictionary = 33673 * 100 = 3_367_300 parameters\n# we have a 33673 x 100 word vector, Embedding accepts 2D input and returns 3D output as shown in the summary\n# input_length = the length of input sequences (i.e. e-mails)\nmodel.add(Embedding(max_words, embedding_dim, input_length=maxlen)) # 33673, 100, input_length=300 = (None, 300,100)\n# the activations have shape of (33673, 300, embedding_dim=100)\n\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.summary()","9aee9241":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)","4997a1c0":"X_train.shape, y_train.shape","f9be2c11":"model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n\nhistory = model.fit(X_train, y_train, epochs=3, batch_size=32, validation_split=0.2)","57a0b234":"acc = history.history['acc']\nval_acc = history.history['val_acc']\nloss = history.history['loss']\nval_loss = history.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', color='red', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend();","15055322":"model.evaluate(X_test, y_test)\n# loss value & acc metrics","9759ccaa":"#save the trained word vector and use it later.  I didn't get this part\n\nmodel.save_weights('pre_trained_model_100D.h5')","76d9bff4":"# just curiosity, let's explore the shape of the trained word embedding\n\nmodel.layers[0].get_weights()[0].shape\n\n# (vocabulary len x dimension of word vector) = word vector!","a6fd940c":"model2 = Sequential()\n\nmodel2.add(Embedding(max_words, embedding_dim, input_length=maxlen))\nmodel2.add(Flatten())\nmodel2.add(Dense(32, activation='relu'))\nmodel2.add(Dense(1, activation='sigmoid'))\nmodel2.summary()","c5e14090":"model2.layers[0].set_weights(model.layers[0].get_weights()) # load\nmodel2.layers[0].trainable = False # freeze","9f11278d":"model2.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['acc'])\n\nhistory2 = model2.fit(X_train, y_train, epochs=2, batch_size=32, validation_split=0.2)","c316f5ef":"from sklearn.metrics import precision_score, recall_score\n\ny_test_pred = np.where(model2.predict(X_test) > .5, 1, 0).reshape(1, -1)[0]\nprint(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_test_pred)))\nprint(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_test_pred)))","a7adc67f":"acc = history2.history['acc']\nval_acc = history2.history['val_acc']\nloss = history2.history['loss']\nval_loss = history2.history['val_loss']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, acc, 'bo', color='red', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.legend();","0e0a7819":"# predict the test score\nmodel2.evaluate(X_test, y_test)\n# loss value & acc metrics","dae263ff":"model2.save_weights('pre_trained_model2100D_dense.h5')","0d125f3c":"#Instead of renaming columns I applied Dataprep Clean Headers.","34cb91f8":"#My accuracy is dead again","74a172a7":"#Code by Hakan Ozler https:\/\/www.kaggle.com\/ozlerhakan\/neural-network-word-embedding-using-keras","c95e815c":"\"In linguistics, hyponymy is a semantic relation between a hyponym denoting a subtype and a hypernym or hyperonym denoting a supertype. In other words, the semantic field of the hyponym is included within that of the hypernym.\"\n\n\"In simpler terms, a hyponym is in a type-of relationship with its hypernym. For example: pigeon, crow, eagle, and seagull are all hyponyms of bird, their hypernym; which itself is a hyponym of animal, its hypernym.\"\n\nhttps:\/\/en.wikipedia.org\/wiki\/Hyponymy_and_hypernymy","2d6d4924":"#Save the model2 along with the fully connected layers and word vector.","ae4155a7":"#Create a new model and add the pre-trained word vector is of shape (140, 100).\n\nThose numbers are on the snippet below.","1ac9163e":"![](https:\/\/www.researchgate.net\/profile\/Andrew-Krizhanovsky\/publication\/257858125\/figure\/fig2\/AS:640263219318786@1529662055212\/Semantic-relations-section-of-the-Russian-adjective-neznyj.png)researchgate.net","65f2c914":"#Another \"dead\" chart","238fa161":"#The Keras Embedding Layer\n\nIn the first example we use the embedding layer to train the word embedding alone so that we can save and use it in another model later."}}