{"cell_type":{"1a14b0e5":"code","2d2b4d79":"code","6bedbb2c":"code","a956d07b":"code","be517016":"code","bf711033":"code","b6e3cad1":"code","1c998b21":"code","ae2a1791":"code","2f8f5a9b":"code","180f66b5":"code","862f414f":"code","ccb0dd7b":"code","c5f1a004":"code","f9c4df2c":"code","94605b18":"code","be9bb3f5":"markdown","5f3801c4":"markdown","15e90d0d":"markdown","3b2de9e8":"markdown","255eb310":"markdown","76d02c32":"markdown","e67320d3":"markdown","29f5bdb8":"markdown","40836bf1":"markdown","7e4b9710":"markdown","5ce9e64e":"markdown","34eb872b":"markdown","ad168354":"markdown","834134d3":"markdown","495da0ec":"markdown","4c98c306":"markdown","cab4b15d":"markdown","ba991b4d":"markdown","6000d603":"markdown","5e27ce3c":"markdown"},"source":{"1a14b0e5":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","2d2b4d79":"# We will be reading train dataset to perform PCA.\n# For reading datframe 'train' , we will be using pandas library and function 'read_csv' as our file is in csv format\ntrain = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\nprint('data train is available')","6bedbb2c":"# shape of the data - this give us number of rows and columns in the dataframe\nprint('shape of the data is {} rows and {} columns'.format(train.shape[0],train.shape[1]))","a956d07b":"# we should take a look at complete dataset... so as our dataset have large number of rows and columns. we will be looking at only top 5 rows\n# for looking only top 5 rows, we will be using train.head()\nprint(train.head())","be517016":"# getting more info about dataframe..\nprint(train['label'].value_counts()) # value_counts give us count of different labels in particular column, here we use it on our 'label' column of train dataset\n# it always better to see things in graphs and plot...\n# for plot, we will be using seaborn package\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nlabel_count = train['label'].value_counts()\nplt.figure(figsize=(10,5))\nsns.barplot(label_count.index, label_count.values, alpha=0.8)\nplt.title('Count of different labels')\nplt.ylabel('Number of Occurrences', fontsize=12)\nplt.xlabel('Digits', fontsize=12)\nplt.show()","bf711033":"# 1. droping labels from the train dataset and but we need to save them to use later\nlabel = train['label']\ntrain.drop('label',axis=1,inplace=True)\nprint(train.shape) ","b6e3cad1":"# 2. We should standardize the datset 'train', we will use StandardScaler from sklearn.preprocessing \n\nfrom sklearn.preprocessing import StandardScaler # So what standardscaler do? Ans - Details given below\nstandardized_train =  StandardScaler().fit_transform(train)\nprint(standardized_train.shape)","1c998b21":"# Co-variance matrix of A is A*A^T i.e A is multiplied by transpose of A\nsample_data = standardized_train\n\n# matrix multiplication using numpy\ncovar_matrix = np.matmul(sample_data.T,sample_data) \n\nprint('the shape of co-variance matrix is', covar_matrix.shape)","ae2a1791":"from scipy.linalg import eigh\n\n# the parameter 'eighvals' is defined (low to high value)\n# eigh function will return the eigen value in the ascending order\n# this code will generate only top two (782,783) eigen value\n\nvalues,vectors = eigh(covar_matrix,eigvals= (782,783))\n\nprint('shape of eigen vector', vectors.shape)\n\n# coverting eigen vector into (2,d) shape\n\nvectors = vectors.T\n\nprint('shape of updated eigen vector', vectors.shape)","2f8f5a9b":"# projecting the original data frame on the plane formed by two principal eigen vectors by vector-vector multiplication\\\n\nnew_coordinates = np.matmul(vectors,sample_data.T)\nprint('resultant matrix by multiplication of matrix vector having shape of ',vectors.shape,' and co-variance matrix having shape ', sample_data.shape ,' is new_coordinates matrix having shape ',new_coordinates.shape )","180f66b5":"# appending labels to 2D projected data\nnew_coordinates = np.vstack((new_coordinates,label)).T\n\n# creating the new dataframe for ploting the labeled points.\ndataframe = pd.DataFrame(data=new_coordinates,columns = ('1stPrincipal','2ndPrincipal','label'))\nprint(dataframe.head())\n","862f414f":"# Ploting the 2D data point with seaborn\n\nsns.FacetGrid(dataframe, hue=\"label\",height=6).map(plt.scatter,'1stPrincipal','2ndPrincipal').add_legend()\nplt.show()","ccb0dd7b":"# initailization of PCA\nfrom sklearn import decomposition\npca = decomposition.PCA()","c5f1a004":"# configuring the parameters\npca.n_components = 2\npca_data = pca.fit_transform(sample_data)\n\n# pca_data will contain th 2-d projection of sample_data\nprint('shape of pca_data is ', pca_data.shape)","f9c4df2c":"# attaching the labels with pca_data\n\npca_data = np.vstack((pca_data.T,label)).T\n\n# creating the new dataframe for ploting the labeled points.\ndataframe2 = pd.DataFrame(data=pca_data,columns = ('1stPrincipal','2ndPrincipal','label'))\n","94605b18":"# Ploting the 2D data point with seaborn\n\nsns.FacetGrid(dataframe2, hue=\"label\",height=6).map(plt.scatter,'1stPrincipal','2ndPrincipal').add_legend()\nplt.show()","be9bb3f5":"We can see that first column of dataframe is label. and all other column are corresponding pixel values in 784 dimension\/columns... we should investigate more about columns...","5f3801c4":"# Step 6 : Ploting resultant dataframe containing projected points on axis (with highest and second highest eigen values)","15e90d0d":"> # Step 4 : Findind top (2) eigen value and corresponding eigen vectors for projection in 2-D","3b2de9e8":"# Step 2 : Prepare the dataset****","255eb310":"# Step 1 : Capture the Correlated data ","76d02c32":"Already Done, MNIST data availbale","e67320d3":"# PCA using Scikit-Learn","29f5bdb8":"It may look different due to different color used for label from first plot and axis is also rotated","40836bf1":"# Step 3 : Co-variance Matrix of standarized data","7e4b9710":"# Step 4:   Multiplying transpose of matrix generated from step 4 with Co-variance matrix from step 2****","5ce9e64e":"![hl_pca_matmult.png](attachment:hl_pca_matmult.png)","34eb872b":"As we can see from above plot, that our labels are fairly distributed... It is indeed nice thing...  ","ad168354":"# Really looking forward for your suggestions in comment section... If you like my notebook, then give it a 'LIKE' ","834134d3":"![pca.jpg](attachment:pca.jpg)","495da0ec":"# So How to do principal component analysis on the dataset?\n\nWe are going to take step by step approach, which will give us insights in how PCA works... \n\n# *Using Sklearn.decomposition() for PCA is a shortcut, We will use it to do PCA in the end part of this tutorial. \n\nSo have patience and have a nice ride through this tutorial...\n\nSo steps involved are as follows :\n\nStep 2 : Prepare the dataset - It involves standardization of the dataset\n\nStep 3 : Create Co-Variance Matrix of standardised data\n\nStep 4 : (1) Find the top eigen values and corresponding eigen vectors...for projection in 2-D, find top two eigen value              and corresponding vector \n         (2) Multiplying transpose of matrix generated from step 4 with Co-variance matrix from step 2\n\nStep 5 : Combine resultant matrix of step 4 with saved \n\nStep 6 : Plotting the resultant matrix which have two top features\n","4c98c306":"# Step 5 : Combine resultant matrix of step 4 with saved labels and transponsing it","cab4b15d":"# PCA to Speed-up Machine Learning Algorithms\n\nOne of the most important applications of PCA is for speeding up machine learning algorithms. The MNIST database of handwritten digits is more suitable as it has 784 feature columns (784 dimensions), a training set of 60,000 examples, and a test set of 10,000 examples. PCA help us in reducing dimensionality of the dataset and help us in retaining most of the information availble in all features\/dimension of the dataset","ba991b4d":"# PCA for Data Visualization\n# \nFor a lot of machine learning applications it helps to be able to visualize your data. Visualizing 2 or 3 dimensional data is not that challenging. However, our MNIST dataset used in this part of the tutorial is 784 dimensional. You can use *PCA to reduce that 784 dimensional data into 2 or 3 dimensions so that you can plot and hopefully understand the data better*.","6000d603":"# What is PCA ? Why we bother about it ?\n\n**Principal Component Analysis** (PCA) is an *unsupervised, non-parametric statistical technique* primarily used for dimensionality reduction in machine learning. Models also become more efficient as the reduced feature set *boosts learning rates and diminishes computation costs* by removing redundant features.\n\nor in more simple words :\n\nPCA is a method used to *reduce number of variables in your data by extracting important one from a large pool*. It reduces the dimension of your data with the aim of retaining as much information as possible.","5e27ce3c":"# StandardScaler : \nIt transforms the data in such a manner that it has mean as 0 and standard deviation as 1. In short, it standardizes the data. Standardization is useful for data which has negative values. It arranges the data in a standard normal distribution."}}