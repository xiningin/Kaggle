{"cell_type":{"5b4585f2":"code","5a16cc21":"code","7fdae651":"code","a2d2046c":"code","04027e1c":"code","ce050081":"code","ea8a45f2":"code","a044fafe":"code","ff121cf5":"code","1e03100f":"code","41b9b1af":"code","718a1e83":"code","05c852f7":"code","45723584":"code","5b81e2ac":"code","1d66925b":"code","7a6c9871":"code","169b86ee":"code","8f1ad4cc":"code","8ba373f7":"code","f7a789d3":"code","dd470670":"code","28e87ffe":"code","b68b6531":"code","4301ead9":"code","68561e15":"code","58a1cc35":"code","a6207b71":"code","da6cc567":"code","d94439f3":"code","23f8b2e9":"code","197db5e9":"code","020a3740":"code","a4468d85":"code","8971fb10":"code","69243aa3":"code","a4f0b116":"code","b74ab903":"code","2a1b1068":"code","327d8120":"code","3512cd84":"code","eabd9289":"code","c603f1c0":"code","d057ec39":"code","e2116309":"code","82d72bd2":"code","a828573a":"code","5a368c09":"code","0f8f90b5":"code","c6019199":"code","a45e4fd7":"code","f7ee6fea":"code","b50bb012":"code","23989079":"code","e3fb7b2e":"code","ea50e254":"code","02ac2db0":"code","a0a20211":"code","01fe67ea":"code","830de2a9":"code","bff043f8":"code","ff249c14":"code","e478dc9b":"code","e74725cf":"code","40bfc437":"code","b82f4060":"markdown","ec97c58a":"markdown","f2b6e0eb":"markdown","02eeb5e4":"markdown","a732d4f3":"markdown","d5cd67df":"markdown","055abb0a":"markdown","c7cbfdd4":"markdown","0092b157":"markdown","2c3e27b0":"markdown","1d9860f7":"markdown","1cde94bb":"markdown","636f3d73":"markdown","5227f06d":"markdown","ba44787d":"markdown","6a7433bf":"markdown","9f308a18":"markdown","67333c24":"markdown"},"source":{"5b4585f2":"import numpy as np \nimport pandas as pd\nimport warnings\nwarnings.filterwarnings('ignore')\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline","5a16cc21":"url = 'https:\/\/raw.githubusercontent.com\/LuYuChen03\/Complex-system\/main\/garments_worker_productivity.csv'\ndf = pd.read_csv(url)","7fdae651":"df","a2d2046c":"df.info()","04027e1c":"df['team'] = df['team'].apply(lambda x:str(x)) # Making team number an object type\ncategory = df.select_dtypes(include='object')\nnumerical = df.select_dtypes(exclude='object')","ce050081":"for c in category.columns:\n    print(f\"{c}\")\n    print(category[c].unique())\n    print()","ea8a45f2":"df.department.value_counts() # needs to be fixed","a044fafe":"category.loc[:,'department'] = category.loc[:,'department'].str.strip() # removing error in 'finishing' \ncategory['department']=category['department'].replace(['sweing'],['sewing']) # fixing the error in the name","ff121cf5":"category.department.value_counts()","1e03100f":"sns.catplot(kind='box', data=numerical, orient='h', height=10, aspect=1.5)","41b9b1af":"numerical.boxplot(column=['wip', 'over_time', 'incentive'],figsize=(10,10))","718a1e83":"numerical['idle_time'].plot(kind='hist')\nplt.show()","05c852f7":"numerical['idle_men'].plot(kind='hist')\nplt.show()","45723584":"numerical['no_of_style_change'].plot(kind='hist')\nplt.show()","5b81e2ac":"numerical['no_of_workers'].plot(kind='hist')\nplt.show()","1d66925b":"category['department'].value_counts().plot(kind='pie', autopct=\"%.2f\")\nplt.show()","7a6c9871":"plt.title(\"Quarters\")\ncategory['quarter'].value_counts().plot(kind='pie')\nplt.show()","169b86ee":"category['day'].value_counts().plot(kind='barh')\nplt.title(\"Total working days\")\nplt.xlabel('Frequency')\nplt.show()","8f1ad4cc":"sns.histplot(data=numerical[['targeted_productivity', 'actual_productivity']], element='poly')\nplt.show()","8ba373f7":"numerical['incentive'].plot(kind='hist')\nplt.show()","f7a789d3":"numerical.isna().sum()","dd470670":"numerical[numerical.isnull().any(axis=1)] # these entries all have NAs ","28e87ffe":"# Reassemble the df\ndf2 = pd.concat([category, numerical], axis=1)","b68b6531":"df2[df2.isnull().any(axis=1)]['department'].unique() # when we have NaN values, it always labeled as 'finishing'","4301ead9":"df2['wip'] = df2['wip'].fillna(value=0.0) # We then fill NaNs with a float64 version of 0, to fix them\nnumerical['wip'] = numerical['wip'].fillna(value=0.0)","68561e15":"x_axis = df2['no_of_workers'].unique()\ny_axis=[]\nfor val in x_axis:\n    y_axis.append(df2['over_time'][df2['no_of_workers'] == val].mean())\n\nsns.lineplot(x=x_axis, y=y_axis)\nplt.title(\"Team and Overtime\")\nplt.xlabel('Team Size')\nplt.ylabel('Average Overtime')\nplt.show()","58a1cc35":"corrMatrix = numerical.corr()\n\nfig, ax = plt.subplots(figsize=(15,15)) # Sample figsize in inches\nsns.heatmap(corrMatrix, annot=True, linewidths=.5, ax=ax)\nplt.show()","a6207b71":"df2.drop(columns='year', inplace=True) \ndf2 = pd.get_dummies(df2,drop_first=True, columns=['day', 'department', 'quarter','team'])","da6cc567":"fig, ax = plt.subplots(figsize=(20,20)) # Sample figsize in inches\nsns.heatmap(df2.corr(), annot=True, linewidths=.5, ax=ax)\nplt.show()","d94439f3":"corrMatrix = df2.corr()\n\nplt.figure(figsize=(5, 10))\nheatmap = sns.heatmap(corrMatrix[['actual_productivity']].sort_values(by='actual_productivity', ascending=False), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Features Correlating with actual_productivity', fontdict={'fontsize':18}, pad=16);","23f8b2e9":"actual_productivity_corr = pd.DataFrame(corrMatrix[['actual_productivity']])\nsignificant = actual_productivity_corr[abs(actual_productivity_corr) >= 0.05]\nsignificant['Use'] = significant['actual_productivity'].notna()\nnon_significant = significant[ significant['Use'] == False]\nsignificant_col = significant.index[significant['Use']].tolist()\nnon_significant_col = non_significant.index[non_significant['Use'] == False].tolist()\nsignificant_col.remove('actual_productivity')\nnon_significant_col.append('no_of_workers') # Due to coliniearity between smv and no_of_workers \nnon_significant_col.append('department_sewing') # Collinear with smv\n#non_significant_col.append('department_finishing') # Collinear with department_finishing\nnon_significant_col.append('idle_time') # Due to high frequency of 0 values\nnon_significant_col.append('idle_men') # Due to high frequency of 0 values\ndf2.drop(columns=non_significant_col, inplace=True)\nX = np.array(df2.drop(columns='actual_productivity'))","197db5e9":"non_significant_col","020a3740":"from sklearn.cluster import KMeans\nfrom matplotlib.pyplot import figure\nfrom sklearn.decomposition import PCA","a4468d85":"# calculate distortion for a range of number of cluster\ndistortions = []\nfigure(figsize=(16, 8), dpi=80)\n\nfor i in range(1, 16):\n    km = KMeans(\n        n_clusters=i, init='random',\n        n_init=10, max_iter=300,\n        tol=1e-04, random_state=0\n    )\n    km.fit(X)\n    distortions.append(km.inertia_)\n\n# plot\nplt.plot(range(1, 16), distortions, marker='o')\nplt.title('The Elbow Method')\nplt.xlabel('Number of clusters')\nplt.ylabel('Distortion')\nplt.show()","8971fb10":"kmeans = KMeans(n_clusters=4, random_state=0).fit(X) # By using an Elbow Method for K-means clustering we find the optimal number of clusters to be = 5","69243aa3":"df2['cluster'] = kmeans.labels_\nX = df2.drop(columns='actual_productivity') # X to be used with the model\n","a4f0b116":"# Run PCA on the data and reduce the dimensions in pca_num_components dimensions\nreduced_data = PCA(n_components=2).fit_transform(X)\nresults = pd.DataFrame(reduced_data,columns=['pca1','pca2'])\ncentroids = np.array(kmeans.cluster_centers_)\nsns.set(rc={\"figure.figsize\":(24, 12)})\nsns.scatterplot(x=\"pca1\", y=\"pca2\", hue=X['cluster'], data=results, palette=\"deep\")\nplt.title('K-means Clustering with 2 dimensions')\nplt.show()","b74ab903":"from sklearn.linear_model import Ridge, Lasso\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn import metrics\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn import linear_model\nfrom sklearn.model_selection import train_test_split,cross_val_score, cross_val_predict\nimport xgboost as xgb\nfrom sklearn.model_selection import train_test_split","2a1b1068":"X0 = df2[df2['cluster'] == 0 ].drop(columns='actual_productivity')\ny0 = df2[df2['cluster'] == 0 ].actual_productivity\nX0_train, X0_test, y0_train, y0_test = train_test_split(X0, y0, test_size=0.15, random_state= 0)\n\nX1 = df2[df2['cluster'] == 1 ].drop(columns='actual_productivity')\ny1 = df2[df2['cluster'] == 1 ].actual_productivity\nX1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.15, random_state= 0)\n\nX2 = df2[df2['cluster'] == 2 ].drop(columns='actual_productivity')\ny2 = df2[df2['cluster'] == 2 ].actual_productivity\nX2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.15, random_state= 0)\n\n\nX3 = df2[df2['cluster'] == 3 ].drop(columns='actual_productivity')\ny3 = df2[df2['cluster'] == 3 ].actual_productivity\nX3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.15, random_state= 0)\n","327d8120":"cluster0 = []\ncluster1 = []\ncluster2 = []\ncluster3 = []","3512cd84":"# Cluster #0\nlin_reg = LinearRegression()\nlin_reg.fit(X0_train,y0_train)\n#Prediction using test set \ny0_pred = lin_reg.predict(X0_test)\nmae=metrics.mean_absolute_error(y0_test, y0_pred)\nmse=metrics.mean_squared_error(y0_test, y0_pred)\n# Printing the metrics\nprint('Linear Regression for Cluster #0:')\nprint('R2 square:',metrics.r2_score(y0_test, y0_pred))\ncluster0.append(metrics.r2_score(y0_test, y0_pred))\nprint('MAE: ', mae)\nprint('MSE: ', mse)\nprint('')\n\n# Cluster #1\nlin_reg = LinearRegression()\nlin_reg.fit(X1_train,y1_train)\n#Prediction using test set \ny1_pred = lin_reg.predict(X1_test)\nmae=metrics.mean_absolute_error(y1_test, y1_pred)\nmse=metrics.mean_squared_error(y1_test, y1_pred)\n# Printing the metrics\nprint('Linear Regression for Cluster #1:')\nprint('R2 square:',metrics.r2_score(y1_test, y1_pred))\ncluster1.append(metrics.r2_score(y1_test, y1_pred))\nprint('MAE: ', mae)\nprint('MSE: ', mse)\nprint('')\n\n# Cluster #2\nlin_reg = LinearRegression()\nlin_reg.fit(X2_train,y2_train)\n#Prediction using test set \ny2_pred = lin_reg.predict(X2_test)\nmae=metrics.mean_absolute_error(y2_test, y2_pred)\nmse=metrics.mean_squared_error(y2_test, y2_pred)\n# Printing the metrics\nprint('Linear Regression for Cluster #2:')\nprint('R2 square:',metrics.r2_score(y2_test, y2_pred))\ncluster2.append(metrics.r2_score(y2_test, y2_pred))\nprint('MAE: ', mae)\nprint('MSE: ', mse)\nprint('')\n\n# Cluster #3\nlin_reg = LinearRegression()\nlin_reg.fit(X3_train,y3_train)\n#Prediction using test set \ny3_pred = lin_reg.predict(X3_test)\nmae=metrics.mean_absolute_error(y3_test, y3_pred)\nmse=metrics.mean_squared_error(y3_test, y3_pred)\n# Printing the metrics\nprint('Linear Regression for Cluster #3:')\nprint('R2 square:',metrics.r2_score(y3_test, y3_pred))\ncluster3.append(metrics.r2_score(y3_test, y3_pred))\nprint('MAE: ', mae)\nprint('MSE: ', mse)\nprint('')","eabd9289":"# Cluster #0\nlas_reg = Lasso()\nlas_reg.fit(X0_train, y0_train)\n#Prediction using test set \ny0_pred = las_reg.predict(X0_test)\nmae=metrics.mean_absolute_error(y0_test, y0_pred)\nmse=metrics.mean_squared_error(y0_test, y0_pred)\n# Printing the metrics\nprint('Lasso Regression for Cluster #0:')\nprint('R2 square:',metrics.r2_score(y0_test, y0_pred))\ncluster0.append(metrics.r2_score(y0_test, y0_pred))\nprint('MAE: ', mae)\nprint('MSE: ', mse)\nprint('')\n\n# Cluster #1\nlas_reg = Lasso()\nlas_reg.fit(X1_train, y1_train)\n#Prediction using test set \ny1_pred = las_reg.predict(X1_test)\nmae=metrics.mean_absolute_error(y1_test, y1_pred)\nmse=metrics.mean_squared_error(y1_test, y1_pred)\n# Printing the metrics\nprint('Lasso Regression for Cluster #1:')\nprint('R2 square:',metrics.r2_score(y1_test, y1_pred))\ncluster1.append(metrics.r2_score(y1_test, y1_pred))\nprint('MAE: ', mae)\nprint('MSE: ', mse)\nprint('')\n\n# Cluster #2\nlas_reg = Lasso()\nlas_reg.fit(X2_train, y2_train)\n#Prediction using test set \ny2_pred = las_reg.predict(X2_test)\nmae=metrics.mean_absolute_error(y2_test, y2_pred)\nmse=metrics.mean_squared_error(y2_test, y2_pred)\n# Printing the metrics\nprint('Lasso Regression for Cluster #2:')\nprint('R2 square:',metrics.r2_score(y2_test, y2_pred))\ncluster2.append(metrics.r2_score(y2_test, y2_pred))\nprint('MAE: ', mae)\nprint('MSE: ', mse)\nprint('')\n\n# Cluster #3\nlas_reg = Lasso()\nlas_reg.fit(X3_train, y3_train)\n#Prediction using test set \ny3_pred = las_reg.predict(X3_test)\nmae=metrics.mean_absolute_error(y3_test, y3_pred)\nmse=metrics.mean_squared_error(y3_test, y3_pred)\n# Printing the metrics\nprint('Lasso Regression for Cluster #3:')\nprint('R2 square:',metrics.r2_score(y3_test, y3_pred))\ncluster3.append(metrics.r2_score(y3_test, y3_pred))\nprint('MAE: ', mae)\nprint('MSE: ', mse)\nprint('')","c603f1c0":"# Cluster #0\nrid_reg = Ridge()\nrid_reg.fit(X0_train, y0_train)\n#Prediction using test set \ny0_pred = rid_reg.predict(X0_test)\nmae=metrics.mean_absolute_error(y0_test, y0_pred)\nmse=metrics.mean_squared_error(y0_test, y0_pred)\n# Printing the metrics\nprint('Ridge Regression for Cluster #0:')\nprint('R2 square:',metrics.r2_score(y0_test, y0_pred))\nprint('MAE: ', mae)\nprint('MSE: ', mse)\nprint('')\n\n# Cluster #1\nrid_reg = Ridge()\nrid_reg.fit(X1_train, y1_train)\n#Prediction using test set \ny1_pred = rid_reg.predict(X1_test)\nmae=metrics.mean_absolute_error(y1_test, y1_pred)\nmse=metrics.mean_squared_error(y1_test, y1_pred)\n# Printing the metrics\nprint('Ridge Regression for Cluster #1:')\nprint('R2 square:',metrics.r2_score(y1_test, y1_pred))\nprint('MAE: ', mae)\nprint('MSE: ', mse)\nprint('')\n\n# Cluster #2\nrid_reg = Ridge()\nrid_reg.fit(X2_train, y2_train)\n#Prediction using test set \ny2_pred = rid_reg.predict(X2_test)\nmae=metrics.mean_absolute_error(y2_test, y2_pred)\nmse=metrics.mean_squared_error(y2_test, y2_pred)\n# Printing the metrics\nprint('Ridge Regression for Cluster #2:')\nprint('R2 square:',metrics.r2_score(y2_test, y2_pred))\nprint('MAE: ', mae)\nprint('MSE: ', mse)\nprint('')\n\n# Cluster #3\nrid_reg = Ridge()\nrid_reg.fit(X3_train, y3_train)\n#Prediction using test set \ny3_pred = rid_reg.predict(X3_test)\nmae=metrics.mean_absolute_error(y3_test, y3_pred)\nmse=metrics.mean_squared_error(y3_test, y3_pred)\n# Printing the metrics\nprint('Ridge Regression for Cluster #3:')\nprint('R2 square:',metrics.r2_score(y3_test, y3_pred))\nprint('MAE: ', mae)\nprint('MSE: ', mse)\nprint('')\n\n\ncluster0.append(metrics.r2_score(y0_test, y0_pred))\ncluster1.append(metrics.r2_score(y1_test, y1_pred))\ncluster2.append(metrics.r2_score(y2_test, y2_pred))\ncluster3.append(metrics.r2_score(y3_test, y3_pred))","d057ec39":"# Cluster #0\ndt_regressor = DecisionTreeRegressor(random_state = 0)\ndt_regressor.fit(X0_train,y0_train)\n#Predicting using test set \ny0_pred = dt_regressor.predict(X0_test)\nmae=metrics.mean_absolute_error(y0_test, y0_pred)\nmse=metrics.mean_squared_error(y0_test, y0_pred)\n# Printing the metrics\nprint('Decision Tree Regressor for Cluster #0:')\nprint('Suppport Vector Regression Accuracy: ', dt_regressor.score(X0_test,y0_test))\nprint('R2 square:',metrics.r2_score(y0_test, y0_pred))\nprint('MAE: ', mae)\nprint('MSE: ', mse)\nprint('')\n\n# Cluster #1\ndt_regressor = DecisionTreeRegressor(random_state = 0)\ndt_regressor.fit(X1_train,y1_train)\n#Predicting using test set \ny1_pred = dt_regressor.predict(X1_test)\nmae=metrics.mean_absolute_error(y1_test, y1_pred)\nmse=metrics.mean_squared_error(y1_test, y1_pred)\n# Printing the metrics\nprint('Decision Tree Regressor for Cluster #1:')\nprint('Suppport Vector Regression Accuracy: ', dt_regressor.score(X1_test,y1_test))\nprint('R2 square:',metrics.r2_score(y1_test, y1_pred))\nprint('MAE: ', mae)\nprint('MSE: ', mse)\nprint('')\n\n# Cluster #2\ndt_regressor = DecisionTreeRegressor(random_state = 0)\ndt_regressor.fit(X2_train,y2_train)\n#Predicting using test set \ny2_pred = dt_regressor.predict(X2_test)\nmae=metrics.mean_absolute_error(y2_test, y2_pred)\nmse=metrics.mean_squared_error(y2_test, y2_pred)\n# Printing the metrics\nprint('Decision Tree Regressor for Cluster #2:')\nprint('Suppport Vector Regression Accuracy: ', dt_regressor.score(X2_test,y2_test))\nprint('R2 square:',metrics.r2_score(y2_test, y2_pred))\nprint('MAE: ', mae)\nprint('MSE: ', mse)\nprint('')\n\n# Cluster #3\ndt_regressor = DecisionTreeRegressor(random_state = 0)\ndt_regressor.fit(X3_train,y3_train)\n#Predicting using test set \ny3_pred = dt_regressor.predict(X3_test)\nmae=metrics.mean_absolute_error(y3_test, y3_pred)\nmse=metrics.mean_squared_error(y3_test, y3_pred)\n# Printing the metrics\nprint('Decision Tree Regressor for Cluster #3:')\nprint('Suppport Vector Regression Accuracy: ', dt_regressor.score(X3_test,y3_test))\nprint('R2 square:',metrics.r2_score(y3_test, y3_pred))\nprint('MAE: ', mae)\nprint('MSE: ', mse)\nprint('')\n\ncluster0.append(metrics.r2_score(y0_test, y0_pred))\ncluster1.append(metrics.r2_score(y1_test, y1_pred))\ncluster2.append(metrics.r2_score(y2_test, y2_pred))\ncluster3.append(metrics.r2_score(y3_test, y3_pred))","e2116309":"# Cluster #0\nrf_regressor = RandomForestRegressor(n_estimators = 300 ,  random_state = 0)\nrf_regressor.fit(X0_train,y0_train)\n#Predicting the SalePrices using test set \ny0_pred = rf_regressor.predict(X0_test)\nmae=metrics.mean_absolute_error(y0_test, y0_pred)\nmse=metrics.mean_squared_error(y0_test, y0_pred)\n# Printing the metrics\nprint('Random Forest Regression for Cluster #0:')\nprint('Suppport Vector Regression Accuracy: ', rf_regressor.score(X0_test,y0_test))\nprint('R2 square:',metrics.r2_score(y0_test, y0_pred))\nprint('MAE: ', mae)\nprint('MSE: ', mse)\nprint('')\n\n# Cluster #1\nrf_regressor = RandomForestRegressor(n_estimators = 300 ,  random_state = 0)\nrf_regressor.fit(X1_train,y1_train)\n#Predicting the SalePrices using test set \ny1_pred = rf_regressor.predict(X1_test)\nmae=metrics.mean_absolute_error(y1_test, y1_pred)\nmse=metrics.mean_squared_error(y1_test, y1_pred)\n# Printing the metrics\nprint('Random Forest Regression for Cluster #1:')\nprint('Suppport Vector Regression Accuracy: ', rf_regressor.score(X1_test,y1_test))\nprint('R2 square:',metrics.r2_score(y1_test, y1_pred))\nprint('MAE: ', mae)\nprint('MSE: ', mse)\nprint('')\n\n# Cluster #2\nrf_regressor = RandomForestRegressor(n_estimators = 300 ,  random_state = 0)\nrf_regressor.fit(X2_train,y2_train)\n#Predicting the SalePrices using test set \ny2_pred = rf_regressor.predict(X2_test)\nmae=metrics.mean_absolute_error(y2_test, y2_pred)\nmse=metrics.mean_squared_error(y2_test, y2_pred)\n# Printing the metrics\nprint('Random Forest Regression for Cluster #2:')\nprint('Suppport Vector Regression Accuracy: ', rf_regressor.score(X2_test,y2_test))\nprint('R2 square:',metrics.r2_score(y2_test, y2_pred))\nprint('MAE: ', mae)\nprint('MSE: ', mse)\nprint('')\n\n# Cluster #3\nrf_regressor = RandomForestRegressor(n_estimators = 300 ,  random_state = 0)\nrf_regressor.fit(X3_train,y3_train)\n#Predicting the SalePrices using test set \ny3_pred = rf_regressor.predict(X3_test)\nmae=metrics.mean_absolute_error(y3_test, y3_pred)\nmse=metrics.mean_squared_error(y3_test, y3_pred)\n# Printing the metrics\nprint('Random Forest Regression for Cluster #3:')\nprint('Suppport Vector Regression Accuracy: ', rf_regressor.score(X3_test,y3_test))\nprint('R2 square:',metrics.r2_score(y3_test, y3_pred))\nprint('MAE: ', mae)\nprint('MSE: ', mse)\nprint('')\n\ncluster0.append(metrics.r2_score(y0_test, y0_pred))\ncluster1.append(metrics.r2_score(y1_test, y1_pred))\ncluster2.append(metrics.r2_score(y2_test, y2_pred))\ncluster3.append(metrics.r2_score(y3_test, y3_pred))","82d72bd2":"all_clusters = [cluster0,cluster1, cluster2, cluster3]\nr2_result = pd.DataFrame(all_clusters, columns=['Linear Reg', 'Lasso Reg', \n                                                'Ridge Reg', 'Decision Tree Regressor', \n                                                'Random Forest Regression'], \n                         index=['Cluster0', 'Cluster1', 'Cluster2', 'Cluster3'])\nr2_result","a828573a":"df_cl1 = df2[df2['cluster'] == 1]","5a368c09":"sns.displot(df_cl1['actual_productivity'] )","0f8f90b5":"plt.figure(figsize=(10,10))\ndf_cl1.actual_productivity.hist()","c6019199":"plt.figure(figsize=(10,10))\nsns.boxplot(data=df_cl1,y='actual_productivity')","a45e4fd7":"df_cl1.skew().sort_values(ascending=True)","f7ee6fea":"plt.figure(figsize=(10,10))\nlog_price = np.log(df_cl1.actual_productivity)\nlog_price.hist()","b50bb012":"sns.heatmap(df_cl1.corr(),annot=True)","23989079":"xgbr_cl1 = xgb.XGBRegressor(verbosity = 0)\nxgbr_cl1.fit(X1_train, y1_train)","e3fb7b2e":"score = xgbr_cl1.score(X1_train, y1_train)  \nprint(\"Training score: \", score)","ea50e254":"scores = cross_val_score(xgbr_cl1, X1_train, y1_train,cv=10)\nprint(\"Mean cross-validation score: %.2f\" % scores.mean())","02ac2db0":"y1_pred = xgbr_cl1.predict(X1_test)\nmae=metrics.mean_absolute_error(y1_test, y1_pred)\nmse=metrics.mean_squared_error(y1_test, y1_pred)\nprint('Suppport Vector Regression Accuracy: ', xgbr_cl1.score(X1_test,y1_test))\nprint('R2 square:', metrics.r2_score(y1_test, y1_pred))\nprint('MAE: ', mae)\nprint('MSE: ', mse)","a0a20211":"x_ax = range(len(y1_test))\nplt.plot(x_ax, y1_test, label=\"original\")\nplt.plot(x_ax, y1_pred, label=\"predicted\")\nplt.title(\"Cluster 1 XGBoost Regressor test and predicted data\")\nplt.legend()\nplt.show()","01fe67ea":"# Run PCA on the data and reduce the dimensions in pca_num_components dimensions\nreduced_data_cl1 = PCA(n_components=2).fit_transform(X1)\nresults_cl1 = pd.DataFrame(reduced_data_cl1,columns=['pca1','pca2'])\nsns.set(rc={\"figure.figsize\":(15, 10)})\nsns.scatterplot(x=\"pca1\", y=\"pca2\", data=results, palette=\"deep\")\nplt.title('PCA Cluster 1')\nplt.show()","830de2a9":"xgbr_res = []\n\nxgbr = xgb.XGBRegressor(verbosity = 0)\nxgbr.fit(X0_train, y0_train)\ny0_pred = xgbr.predict(X0_test)\n\nxgbr_res.append(metrics.r2_score(y0_test, y0_pred))\nxgbr_res.append(metrics.r2_score(y1_test, y1_pred))\n\nxgbr = xgb.XGBRegressor(verbosity = 0)\nxgbr.fit(X2_train, y2_train)\ny2_pred = xgbr.predict(X2_test)\nxgbr_res.append(metrics.r2_score(y2_test, y2_pred))\n\nxgbr = xgb.XGBRegressor(verbosity = 0)\nxgbr.fit(X3_train, y3_train)\ny2_pred = xgbr.predict(X3_test)\nxgbr_res.append(metrics.r2_score(y3_test, y3_pred))","bff043f8":"r2_result['XGBRegressor'] = xgbr_res\nr2_result","ff249c14":"X = df2.drop(columns='actual_productivity')\ny = df2.actual_productivity\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.15, random_state= 0)","e478dc9b":"xgbr = xgb.XGBRegressor(verbosity = 0)\nxgbr.fit(X_train, y_train)\ny_pred = xgbr.predict(X_test)\nmae=metrics.mean_absolute_error(y_test, y_pred)\nmse=metrics.mean_squared_error(y_test, y_pred)\nprint('Suppport Vector Regression Accuracy: ', xgbr.score(X_test,y_test))\nprint('R2 square:', metrics.r2_score(y_test, y_pred))\nprint('MAE: ', mae)\nprint('MSE: ', mse)","e74725cf":"x_ax = range(len(y_test))\nplt.plot(x_ax, y_test, label=\"original\")\nplt.plot(x_ax, y_pred, label=\"predicted\")\nplt.title(\"XGBoost Regressor for all 4 Clusters test and predicted data\")\nplt.legend()\nplt.show()","40bfc437":"corrMatrix = df2.corr()\n\nplt.figure(figsize=(5, 10))\nheatmap = sns.heatmap(corrMatrix[['actual_productivity']].sort_values(by='actual_productivity', ascending=False), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Features Correlating with actual_productivity', fontdict={'fontsize':18}, pad=16);","b82f4060":"# EDA","ec97c58a":"# Random Forest Regression","f2b6e0eb":"1 date : Date in MM-DD-YYYY\n\n02 day : Day of the Week\n\n03 quarter : A portion of the month. A month was divided into four quarters\n\n04 department : Associated department with the instance\n\n05 team_no : Associated team number with the instance\n\n06 no_of_workers : Number of workers in each team\n\n07 no_of_style_change : Number of changes in the style of a particular product\n\n08 targeted_productivity : Targeted productivity set by the Authority for each team for each day.\n\n09 smv : Standard Minute Value, it is the allocated time for a task\n\n10 wip : Work in progress. Includes the number of unfinished items for products\n\n11 over_time : Represents the amount of overtime by each team in minutes\n\n12 incentive : Represents the amount of financial incentive (in BDT) that enables or motivates a particular course of action.\n\n13 idle_time : The amount of time when the production was interrupted due to several reasons\n\n14 idle_men : The number of workers who were idle due to production interruption\n\n15 actual_productivity : The actual % of productivity that was delivered by the workers. It ranges from 0-1.\n","02eeb5e4":"# PCA\n*Principal Component Analysis*, or PCA, is a dimensionality-reduction method that is often used to reduce the dimensionality of large data sets, by transforming a large set of variables into a smaller one that still contains most of the information in the large set.","a732d4f3":"# Decision Tree Regressor","d5cd67df":"# Building Prediction Models","055abb0a":"# Ridge Regression","c7cbfdd4":"# Results ","0092b157":"# Linear Regression","2c3e27b0":"XGBoost showed an improvemnt in model accuracy, however it is still far from being acceptable.","1d9860f7":"# The Elbow Method (KMeans)","1cde94bb":"We see that `no_of_workers` & `smv` are collinear, having a high correlation coefficient between themselves. ","636f3d73":"# Exploring Cluster 1","5227f06d":"# Lasso Regression","ba44787d":"# Exploring Correlation & Selecting Significant Features","6a7433bf":"# XGBoost with Cluster 1","9f308a18":"# General Model\n\nWill use all of the data, disregarding the cluster breakdown.","67333c24":"# XGBoost with other clusters"}}