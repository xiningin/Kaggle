{"cell_type":{"d6e610c7":"code","92e31b44":"code","6658ca02":"code","0752d83a":"code","1fbb5c17":"code","08e612c9":"code","785e7383":"code","2d600fa3":"code","d6dba9a1":"code","7485c50e":"code","35832f0b":"code","aeb342a9":"code","62584c7f":"code","4cf04cce":"code","c53cea44":"code","6711d4b8":"code","c7760799":"code","affdd9d6":"code","d059d3d6":"code","8e398b96":"code","299aa64b":"code","c808ecbd":"code","ddc3968e":"code","a5aae269":"code","2275e6b8":"code","b1cbdd54":"code","eb052773":"code","9e5a30ce":"code","849c2307":"code","766c1396":"markdown","f410e0fb":"markdown","082f6ee6":"markdown","5739360a":"markdown"},"source":{"d6e610c7":"!pip install ..\/input\/pretrainedmodels\/pretrainedmodels-0.7.4\/pretrainedmodels-0.7.4\/ > \/dev\/null\n#package_path = '..\/input\/unet-model'\npackage_path = '..\/input\/unetmodelscript' # add unet script dataset\nimport sys\nsys.path.append(package_path)","92e31b44":"# Get necessary Imports\nimport pdb\nimport os\nimport cv2\nimport torch\nimport pandas as pd\nimport numpy as np\nfrom tqdm import tqdm\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data import DataLoader, Dataset\nfrom albumentations import (Normalize, Compose)\nfrom albumentations.torch import ToTensor\nimport torch.utils.data as data\nimport torchvision.models as models\nimport torch.nn as nn\nfrom torch.nn import functional as F\nfrom model import Unet","6658ca02":"# Codes from Heng's baseline\n# This code is for classifcation model\n\nBatchNorm2d = nn.BatchNorm2d\n\nIMAGE_RGB_MEAN = [0.485, 0.456, 0.406]\nIMAGE_RGB_STD  = [0.229, 0.224, 0.225]\n\n\n###############################################################################\nCONVERSION=[\n 'block0.0.weight',\t(64, 3, 7, 7),\t 'conv1.weight',\t(64, 3, 7, 7),\n 'block0.1.weight',\t(64,),\t 'bn1.weight',\t(64,),\n 'block0.1.bias',\t(64,),\t 'bn1.bias',\t(64,),\n 'block0.1.running_mean',\t(64,),\t 'bn1.running_mean',\t(64,),\n 'block0.1.running_var',\t(64,),\t 'bn1.running_var',\t(64,),\n 'block1.1.conv_bn1.conv.weight',\t(64, 64, 3, 3),\t 'layer1.0.conv1.weight',\t(64, 64, 3, 3),\n 'block1.1.conv_bn1.bn.weight',\t(64,),\t 'layer1.0.bn1.weight',\t(64,),\n 'block1.1.conv_bn1.bn.bias',\t(64,),\t 'layer1.0.bn1.bias',\t(64,),\n 'block1.1.conv_bn1.bn.running_mean',\t(64,),\t 'layer1.0.bn1.running_mean',\t(64,),\n 'block1.1.conv_bn1.bn.running_var',\t(64,),\t 'layer1.0.bn1.running_var',\t(64,),\n 'block1.1.conv_bn2.conv.weight',\t(64, 64, 3, 3),\t 'layer1.0.conv2.weight',\t(64, 64, 3, 3),\n 'block1.1.conv_bn2.bn.weight',\t(64,),\t 'layer1.0.bn2.weight',\t(64,),\n 'block1.1.conv_bn2.bn.bias',\t(64,),\t 'layer1.0.bn2.bias',\t(64,),\n 'block1.1.conv_bn2.bn.running_mean',\t(64,),\t 'layer1.0.bn2.running_mean',\t(64,),\n 'block1.1.conv_bn2.bn.running_var',\t(64,),\t 'layer1.0.bn2.running_var',\t(64,),\n 'block1.2.conv_bn1.conv.weight',\t(64, 64, 3, 3),\t 'layer1.1.conv1.weight',\t(64, 64, 3, 3),\n 'block1.2.conv_bn1.bn.weight',\t(64,),\t 'layer1.1.bn1.weight',\t(64,),\n 'block1.2.conv_bn1.bn.bias',\t(64,),\t 'layer1.1.bn1.bias',\t(64,),\n 'block1.2.conv_bn1.bn.running_mean',\t(64,),\t 'layer1.1.bn1.running_mean',\t(64,),\n 'block1.2.conv_bn1.bn.running_var',\t(64,),\t 'layer1.1.bn1.running_var',\t(64,),\n 'block1.2.conv_bn2.conv.weight',\t(64, 64, 3, 3),\t 'layer1.1.conv2.weight',\t(64, 64, 3, 3),\n 'block1.2.conv_bn2.bn.weight',\t(64,),\t 'layer1.1.bn2.weight',\t(64,),\n 'block1.2.conv_bn2.bn.bias',\t(64,),\t 'layer1.1.bn2.bias',\t(64,),\n 'block1.2.conv_bn2.bn.running_mean',\t(64,),\t 'layer1.1.bn2.running_mean',\t(64,),\n 'block1.2.conv_bn2.bn.running_var',\t(64,),\t 'layer1.1.bn2.running_var',\t(64,),\n 'block1.3.conv_bn1.conv.weight',\t(64, 64, 3, 3),\t 'layer1.2.conv1.weight',\t(64, 64, 3, 3),\n 'block1.3.conv_bn1.bn.weight',\t(64,),\t 'layer1.2.bn1.weight',\t(64,),\n 'block1.3.conv_bn1.bn.bias',\t(64,),\t 'layer1.2.bn1.bias',\t(64,),\n 'block1.3.conv_bn1.bn.running_mean',\t(64,),\t 'layer1.2.bn1.running_mean',\t(64,),\n 'block1.3.conv_bn1.bn.running_var',\t(64,),\t 'layer1.2.bn1.running_var',\t(64,),\n 'block1.3.conv_bn2.conv.weight',\t(64, 64, 3, 3),\t 'layer1.2.conv2.weight',\t(64, 64, 3, 3),\n 'block1.3.conv_bn2.bn.weight',\t(64,),\t 'layer1.2.bn2.weight',\t(64,),\n 'block1.3.conv_bn2.bn.bias',\t(64,),\t 'layer1.2.bn2.bias',\t(64,),\n 'block1.3.conv_bn2.bn.running_mean',\t(64,),\t 'layer1.2.bn2.running_mean',\t(64,),\n 'block1.3.conv_bn2.bn.running_var',\t(64,),\t 'layer1.2.bn2.running_var',\t(64,),\n 'block2.0.conv_bn1.conv.weight',\t(128, 64, 3, 3),\t 'layer2.0.conv1.weight',\t(128, 64, 3, 3),\n 'block2.0.conv_bn1.bn.weight',\t(128,),\t 'layer2.0.bn1.weight',\t(128,),\n 'block2.0.conv_bn1.bn.bias',\t(128,),\t 'layer2.0.bn1.bias',\t(128,),\n 'block2.0.conv_bn1.bn.running_mean',\t(128,),\t 'layer2.0.bn1.running_mean',\t(128,),\n 'block2.0.conv_bn1.bn.running_var',\t(128,),\t 'layer2.0.bn1.running_var',\t(128,),\n 'block2.0.conv_bn2.conv.weight',\t(128, 128, 3, 3),\t 'layer2.0.conv2.weight',\t(128, 128, 3, 3),\n 'block2.0.conv_bn2.bn.weight',\t(128,),\t 'layer2.0.bn2.weight',\t(128,),\n 'block2.0.conv_bn2.bn.bias',\t(128,),\t 'layer2.0.bn2.bias',\t(128,),\n 'block2.0.conv_bn2.bn.running_mean',\t(128,),\t 'layer2.0.bn2.running_mean',\t(128,),\n 'block2.0.conv_bn2.bn.running_var',\t(128,),\t 'layer2.0.bn2.running_var',\t(128,),\n 'block2.0.shortcut.conv.weight',\t(128, 64, 1, 1),\t 'layer2.0.downsample.0.weight',\t(128, 64, 1, 1),\n 'block2.0.shortcut.bn.weight',\t(128,),\t 'layer2.0.downsample.1.weight',\t(128,),\n 'block2.0.shortcut.bn.bias',\t(128,),\t 'layer2.0.downsample.1.bias',\t(128,),\n 'block2.0.shortcut.bn.running_mean',\t(128,),\t 'layer2.0.downsample.1.running_mean',\t(128,),\n 'block2.0.shortcut.bn.running_var',\t(128,),\t 'layer2.0.downsample.1.running_var',\t(128,),\n 'block2.1.conv_bn1.conv.weight',\t(128, 128, 3, 3),\t 'layer2.1.conv1.weight',\t(128, 128, 3, 3),\n 'block2.1.conv_bn1.bn.weight',\t(128,),\t 'layer2.1.bn1.weight',\t(128,),\n 'block2.1.conv_bn1.bn.bias',\t(128,),\t 'layer2.1.bn1.bias',\t(128,),\n 'block2.1.conv_bn1.bn.running_mean',\t(128,),\t 'layer2.1.bn1.running_mean',\t(128,),\n 'block2.1.conv_bn1.bn.running_var',\t(128,),\t 'layer2.1.bn1.running_var',\t(128,),\n 'block2.1.conv_bn2.conv.weight',\t(128, 128, 3, 3),\t 'layer2.1.conv2.weight',\t(128, 128, 3, 3),\n 'block2.1.conv_bn2.bn.weight',\t(128,),\t 'layer2.1.bn2.weight',\t(128,),\n 'block2.1.conv_bn2.bn.bias',\t(128,),\t 'layer2.1.bn2.bias',\t(128,),\n 'block2.1.conv_bn2.bn.running_mean',\t(128,),\t 'layer2.1.bn2.running_mean',\t(128,),\n 'block2.1.conv_bn2.bn.running_var',\t(128,),\t 'layer2.1.bn2.running_var',\t(128,),\n 'block2.2.conv_bn1.conv.weight',\t(128, 128, 3, 3),\t 'layer2.2.conv1.weight',\t(128, 128, 3, 3),\n 'block2.2.conv_bn1.bn.weight',\t(128,),\t 'layer2.2.bn1.weight',\t(128,),\n 'block2.2.conv_bn1.bn.bias',\t(128,),\t 'layer2.2.bn1.bias',\t(128,),\n 'block2.2.conv_bn1.bn.running_mean',\t(128,),\t 'layer2.2.bn1.running_mean',\t(128,),\n 'block2.2.conv_bn1.bn.running_var',\t(128,),\t 'layer2.2.bn1.running_var',\t(128,),\n 'block2.2.conv_bn2.conv.weight',\t(128, 128, 3, 3),\t 'layer2.2.conv2.weight',\t(128, 128, 3, 3),\n 'block2.2.conv_bn2.bn.weight',\t(128,),\t 'layer2.2.bn2.weight',\t(128,),\n 'block2.2.conv_bn2.bn.bias',\t(128,),\t 'layer2.2.bn2.bias',\t(128,),\n 'block2.2.conv_bn2.bn.running_mean',\t(128,),\t 'layer2.2.bn2.running_mean',\t(128,),\n 'block2.2.conv_bn2.bn.running_var',\t(128,),\t 'layer2.2.bn2.running_var',\t(128,),\n 'block2.3.conv_bn1.conv.weight',\t(128, 128, 3, 3),\t 'layer2.3.conv1.weight',\t(128, 128, 3, 3),\n 'block2.3.conv_bn1.bn.weight',\t(128,),\t 'layer2.3.bn1.weight',\t(128,),\n 'block2.3.conv_bn1.bn.bias',\t(128,),\t 'layer2.3.bn1.bias',\t(128,),\n 'block2.3.conv_bn1.bn.running_mean',\t(128,),\t 'layer2.3.bn1.running_mean',\t(128,),\n 'block2.3.conv_bn1.bn.running_var',\t(128,),\t 'layer2.3.bn1.running_var',\t(128,),\n 'block2.3.conv_bn2.conv.weight',\t(128, 128, 3, 3),\t 'layer2.3.conv2.weight',\t(128, 128, 3, 3),\n 'block2.3.conv_bn2.bn.weight',\t(128,),\t 'layer2.3.bn2.weight',\t(128,),\n 'block2.3.conv_bn2.bn.bias',\t(128,),\t 'layer2.3.bn2.bias',\t(128,),\n 'block2.3.conv_bn2.bn.running_mean',\t(128,),\t 'layer2.3.bn2.running_mean',\t(128,),\n 'block2.3.conv_bn2.bn.running_var',\t(128,),\t 'layer2.3.bn2.running_var',\t(128,),\n 'block3.0.conv_bn1.conv.weight',\t(256, 128, 3, 3),\t 'layer3.0.conv1.weight',\t(256, 128, 3, 3),\n 'block3.0.conv_bn1.bn.weight',\t(256,),\t 'layer3.0.bn1.weight',\t(256,),\n 'block3.0.conv_bn1.bn.bias',\t(256,),\t 'layer3.0.bn1.bias',\t(256,),\n 'block3.0.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.0.bn1.running_mean',\t(256,),\n 'block3.0.conv_bn1.bn.running_var',\t(256,),\t 'layer3.0.bn1.running_var',\t(256,),\n 'block3.0.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.0.conv2.weight',\t(256, 256, 3, 3),\n 'block3.0.conv_bn2.bn.weight',\t(256,),\t 'layer3.0.bn2.weight',\t(256,),\n 'block3.0.conv_bn2.bn.bias',\t(256,),\t 'layer3.0.bn2.bias',\t(256,),\n 'block3.0.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.0.bn2.running_mean',\t(256,),\n 'block3.0.conv_bn2.bn.running_var',\t(256,),\t 'layer3.0.bn2.running_var',\t(256,),\n 'block3.0.shortcut.conv.weight',\t(256, 128, 1, 1),\t 'layer3.0.downsample.0.weight',\t(256, 128, 1, 1),\n 'block3.0.shortcut.bn.weight',\t(256,),\t 'layer3.0.downsample.1.weight',\t(256,),\n 'block3.0.shortcut.bn.bias',\t(256,),\t 'layer3.0.downsample.1.bias',\t(256,),\n 'block3.0.shortcut.bn.running_mean',\t(256,),\t 'layer3.0.downsample.1.running_mean',\t(256,),\n 'block3.0.shortcut.bn.running_var',\t(256,),\t 'layer3.0.downsample.1.running_var',\t(256,),\n 'block3.1.conv_bn1.conv.weight',\t(256, 256, 3, 3),\t 'layer3.1.conv1.weight',\t(256, 256, 3, 3),\n 'block3.1.conv_bn1.bn.weight',\t(256,),\t 'layer3.1.bn1.weight',\t(256,),\n 'block3.1.conv_bn1.bn.bias',\t(256,),\t 'layer3.1.bn1.bias',\t(256,),\n 'block3.1.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.1.bn1.running_mean',\t(256,),\n 'block3.1.conv_bn1.bn.running_var',\t(256,),\t 'layer3.1.bn1.running_var',\t(256,),\n 'block3.1.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.1.conv2.weight',\t(256, 256, 3, 3),\n 'block3.1.conv_bn2.bn.weight',\t(256,),\t 'layer3.1.bn2.weight',\t(256,),\n 'block3.1.conv_bn2.bn.bias',\t(256,),\t 'layer3.1.bn2.bias',\t(256,),\n 'block3.1.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.1.bn2.running_mean',\t(256,),\n 'block3.1.conv_bn2.bn.running_var',\t(256,),\t 'layer3.1.bn2.running_var',\t(256,),\n 'block3.2.conv_bn1.conv.weight',\t(256, 256, 3, 3),\t 'layer3.2.conv1.weight',\t(256, 256, 3, 3),\n 'block3.2.conv_bn1.bn.weight',\t(256,),\t 'layer3.2.bn1.weight',\t(256,),\n 'block3.2.conv_bn1.bn.bias',\t(256,),\t 'layer3.2.bn1.bias',\t(256,),\n 'block3.2.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.2.bn1.running_mean',\t(256,),\n 'block3.2.conv_bn1.bn.running_var',\t(256,),\t 'layer3.2.bn1.running_var',\t(256,),\n 'block3.2.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.2.conv2.weight',\t(256, 256, 3, 3),\n 'block3.2.conv_bn2.bn.weight',\t(256,),\t 'layer3.2.bn2.weight',\t(256,),\n 'block3.2.conv_bn2.bn.bias',\t(256,),\t 'layer3.2.bn2.bias',\t(256,),\n 'block3.2.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.2.bn2.running_mean',\t(256,),\n 'block3.2.conv_bn2.bn.running_var',\t(256,),\t 'layer3.2.bn2.running_var',\t(256,),\n 'block3.3.conv_bn1.conv.weight',\t(256, 256, 3, 3),\t 'layer3.3.conv1.weight',\t(256, 256, 3, 3),\n 'block3.3.conv_bn1.bn.weight',\t(256,),\t 'layer3.3.bn1.weight',\t(256,),\n 'block3.3.conv_bn1.bn.bias',\t(256,),\t 'layer3.3.bn1.bias',\t(256,),\n 'block3.3.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.3.bn1.running_mean',\t(256,),\n 'block3.3.conv_bn1.bn.running_var',\t(256,),\t 'layer3.3.bn1.running_var',\t(256,),\n 'block3.3.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.3.conv2.weight',\t(256, 256, 3, 3),\n 'block3.3.conv_bn2.bn.weight',\t(256,),\t 'layer3.3.bn2.weight',\t(256,),\n 'block3.3.conv_bn2.bn.bias',\t(256,),\t 'layer3.3.bn2.bias',\t(256,),\n 'block3.3.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.3.bn2.running_mean',\t(256,),\n 'block3.3.conv_bn2.bn.running_var',\t(256,),\t 'layer3.3.bn2.running_var',\t(256,),\n 'block3.4.conv_bn1.conv.weight',\t(256, 256, 3, 3),\t 'layer3.4.conv1.weight',\t(256, 256, 3, 3),\n 'block3.4.conv_bn1.bn.weight',\t(256,),\t 'layer3.4.bn1.weight',\t(256,),\n 'block3.4.conv_bn1.bn.bias',\t(256,),\t 'layer3.4.bn1.bias',\t(256,),\n 'block3.4.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.4.bn1.running_mean',\t(256,),\n 'block3.4.conv_bn1.bn.running_var',\t(256,),\t 'layer3.4.bn1.running_var',\t(256,),\n 'block3.4.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.4.conv2.weight',\t(256, 256, 3, 3),\n 'block3.4.conv_bn2.bn.weight',\t(256,),\t 'layer3.4.bn2.weight',\t(256,),\n 'block3.4.conv_bn2.bn.bias',\t(256,),\t 'layer3.4.bn2.bias',\t(256,),\n 'block3.4.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.4.bn2.running_mean',\t(256,),\n 'block3.4.conv_bn2.bn.running_var',\t(256,),\t 'layer3.4.bn2.running_var',\t(256,),\n 'block3.5.conv_bn1.conv.weight',\t(256, 256, 3, 3),\t 'layer3.5.conv1.weight',\t(256, 256, 3, 3),\n 'block3.5.conv_bn1.bn.weight',\t(256,),\t 'layer3.5.bn1.weight',\t(256,),\n 'block3.5.conv_bn1.bn.bias',\t(256,),\t 'layer3.5.bn1.bias',\t(256,),\n 'block3.5.conv_bn1.bn.running_mean',\t(256,),\t 'layer3.5.bn1.running_mean',\t(256,),\n 'block3.5.conv_bn1.bn.running_var',\t(256,),\t 'layer3.5.bn1.running_var',\t(256,),\n 'block3.5.conv_bn2.conv.weight',\t(256, 256, 3, 3),\t 'layer3.5.conv2.weight',\t(256, 256, 3, 3),\n 'block3.5.conv_bn2.bn.weight',\t(256,),\t 'layer3.5.bn2.weight',\t(256,),\n 'block3.5.conv_bn2.bn.bias',\t(256,),\t 'layer3.5.bn2.bias',\t(256,),\n 'block3.5.conv_bn2.bn.running_mean',\t(256,),\t 'layer3.5.bn2.running_mean',\t(256,),\n 'block3.5.conv_bn2.bn.running_var',\t(256,),\t 'layer3.5.bn2.running_var',\t(256,),\n 'block4.0.conv_bn1.conv.weight',\t(512, 256, 3, 3),\t 'layer4.0.conv1.weight',\t(512, 256, 3, 3),\n 'block4.0.conv_bn1.bn.weight',\t(512,),\t 'layer4.0.bn1.weight',\t(512,),\n 'block4.0.conv_bn1.bn.bias',\t(512,),\t 'layer4.0.bn1.bias',\t(512,),\n 'block4.0.conv_bn1.bn.running_mean',\t(512,),\t 'layer4.0.bn1.running_mean',\t(512,),\n 'block4.0.conv_bn1.bn.running_var',\t(512,),\t 'layer4.0.bn1.running_var',\t(512,),\n 'block4.0.conv_bn2.conv.weight',\t(512, 512, 3, 3),\t 'layer4.0.conv2.weight',\t(512, 512, 3, 3),\n 'block4.0.conv_bn2.bn.weight',\t(512,),\t 'layer4.0.bn2.weight',\t(512,),\n 'block4.0.conv_bn2.bn.bias',\t(512,),\t 'layer4.0.bn2.bias',\t(512,),\n 'block4.0.conv_bn2.bn.running_mean',\t(512,),\t 'layer4.0.bn2.running_mean',\t(512,),\n 'block4.0.conv_bn2.bn.running_var',\t(512,),\t 'layer4.0.bn2.running_var',\t(512,),\n 'block4.0.shortcut.conv.weight',\t(512, 256, 1, 1),\t 'layer4.0.downsample.0.weight',\t(512, 256, 1, 1),\n 'block4.0.shortcut.bn.weight',\t(512,),\t 'layer4.0.downsample.1.weight',\t(512,),\n 'block4.0.shortcut.bn.bias',\t(512,),\t 'layer4.0.downsample.1.bias',\t(512,),\n 'block4.0.shortcut.bn.running_mean',\t(512,),\t 'layer4.0.downsample.1.running_mean',\t(512,),\n 'block4.0.shortcut.bn.running_var',\t(512,),\t 'layer4.0.downsample.1.running_var',\t(512,),\n 'block4.1.conv_bn1.conv.weight',\t(512, 512, 3, 3),\t 'layer4.1.conv1.weight',\t(512, 512, 3, 3),\n 'block4.1.conv_bn1.bn.weight',\t(512,),\t 'layer4.1.bn1.weight',\t(512,),\n 'block4.1.conv_bn1.bn.bias',\t(512,),\t 'layer4.1.bn1.bias',\t(512,),\n 'block4.1.conv_bn1.bn.running_mean',\t(512,),\t 'layer4.1.bn1.running_mean',\t(512,),\n 'block4.1.conv_bn1.bn.running_var',\t(512,),\t 'layer4.1.bn1.running_var',\t(512,),\n 'block4.1.conv_bn2.conv.weight',\t(512, 512, 3, 3),\t 'layer4.1.conv2.weight',\t(512, 512, 3, 3),\n 'block4.1.conv_bn2.bn.weight',\t(512,),\t 'layer4.1.bn2.weight',\t(512,),\n 'block4.1.conv_bn2.bn.bias',\t(512,),\t 'layer4.1.bn2.bias',\t(512,),\n 'block4.1.conv_bn2.bn.running_mean',\t(512,),\t 'layer4.1.bn2.running_mean',\t(512,),\n 'block4.1.conv_bn2.bn.running_var',\t(512,),\t 'layer4.1.bn2.running_var',\t(512,),\n 'block4.2.conv_bn1.conv.weight',\t(512, 512, 3, 3),\t 'layer4.2.conv1.weight',\t(512, 512, 3, 3),\n 'block4.2.conv_bn1.bn.weight',\t(512,),\t 'layer4.2.bn1.weight',\t(512,),\n 'block4.2.conv_bn1.bn.bias',\t(512,),\t 'layer4.2.bn1.bias',\t(512,),\n 'block4.2.conv_bn1.bn.running_mean',\t(512,),\t 'layer4.2.bn1.running_mean',\t(512,),\n 'block4.2.conv_bn1.bn.running_var',\t(512,),\t 'layer4.2.bn1.running_var',\t(512,),\n 'block4.2.conv_bn2.conv.weight',\t(512, 512, 3, 3),\t 'layer4.2.conv2.weight',\t(512, 512, 3, 3),\n 'block4.2.conv_bn2.bn.weight',\t(512,),\t 'layer4.2.bn2.weight',\t(512,),\n 'block4.2.conv_bn2.bn.bias',\t(512,),\t 'layer4.2.bn2.bias',\t(512,),\n 'block4.2.conv_bn2.bn.running_mean',\t(512,),\t 'layer4.2.bn2.running_mean',\t(512,),\n 'block4.2.conv_bn2.bn.running_var',\t(512,),\t 'layer4.2.bn2.running_var',\t(512,),\n 'logit.weight',\t(1000, 512),\t 'fc.weight',\t(1000, 512),\n 'logit.bias',\t(1000,),\t 'fc.bias',\t(1000,),\n\n]\n\n###############################################################################\nclass ConvBn2d(nn.Module):\n\n    def __init__(self, in_channel, out_channel, kernel_size=3, padding=1, stride=1):\n        super(ConvBn2d, self).__init__()\n        self.conv = nn.Conv2d(in_channel, out_channel, kernel_size=kernel_size, padding=padding, stride=stride, bias=False)\n        self.bn   = nn.BatchNorm2d(out_channel, eps=1e-5)\n\n    def forward(self,x):\n        x = self.conv(x)\n        x = self.bn(x)\n        return x\n\n\n\n\n#############  resnext50 pyramid feature net #######################################\n# https:\/\/github.com\/Hsuxu\/ResNeXt\/blob\/master\/models.py\n# https:\/\/github.com\/D-X-Y\/ResNeXt-DenseNet\/blob\/master\/models\/resnext.py\n# https:\/\/github.com\/miraclewkf\/ResNeXt-PyTorch\/blob\/master\/resnext.py\n\n\n# bottleneck type C\nclass BasicBlock(nn.Module):\n    def __init__(self, in_channel, channel, out_channel, stride=1, is_shortcut=False):\n        super(BasicBlock, self).__init__()\n        self.is_shortcut = is_shortcut\n\n        self.conv_bn1 = ConvBn2d(in_channel,    channel, kernel_size=3, padding=1, stride=stride)\n        self.conv_bn2 = ConvBn2d(   channel,out_channel, kernel_size=3, padding=1, stride=1)\n\n        if is_shortcut:\n            self.shortcut = ConvBn2d(in_channel, out_channel, kernel_size=1, padding=0, stride=stride)\n\n\n    def forward(self, x):\n        z = F.relu(self.conv_bn1(x),inplace=True)\n        z = self.conv_bn2(z)\n\n        if self.is_shortcut:\n            x = self.shortcut(x)\n\n        z += x\n        z = F.relu(z,inplace=True)\n        return z\n\n\n\n\nclass ResNet34(nn.Module):\n\n    def __init__(self, num_class=1000 ):\n        super(ResNet34, self).__init__()\n\n\n        self.block0  = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=7, padding=3, stride=2, bias=False),\n            BatchNorm2d(64),\n            nn.ReLU(inplace=True),\n        )\n        self.block1  = nn.Sequential(\n             nn.MaxPool2d(kernel_size=3, padding=1, stride=2),\n             BasicBlock( 64, 64, 64, stride=1, is_shortcut=False,),\n          * [BasicBlock( 64, 64, 64, stride=1, is_shortcut=False,) for i in range(1,3)],\n        )\n        self.block2  = nn.Sequential(\n             BasicBlock( 64,128,128, stride=2, is_shortcut=True, ),\n          * [BasicBlock(128,128,128, stride=1, is_shortcut=False,) for i in range(1,4)],\n        )\n        self.block3  = nn.Sequential(\n             BasicBlock(128,256,256, stride=2, is_shortcut=True, ),\n          * [BasicBlock(256,256,256, stride=1, is_shortcut=False,) for i in range(1,6)],\n        )\n        self.block4 = nn.Sequential(\n             BasicBlock(256,512,512, stride=2, is_shortcut=True, ),\n          * [BasicBlock(512,512,512, stride=1, is_shortcut=False,) for i in range(1,3)],\n        )\n        self.logit = nn.Linear(512,num_class)\n\n\n\n    def forward(self, x):\n        batch_size = len(x)\n\n        x = self.block0(x)\n        x = self.block1(x)\n        x = self.block2(x)\n        x = self.block3(x)\n        x = self.block4(x)\n        x = F.adaptive_avg_pool2d(x,1).reshape(batch_size,-1)\n        logit = self.logit(x)\n        return logit","0752d83a":"class Resnet34_classification(nn.Module):\n    def __init__(self,num_class=4):\n        super(Resnet34_classification, self).__init__()\n        e = ResNet34()\n        self.block = nn.ModuleList([\n            e.block0,\n            e.block1,\n            e.block2,\n            e.block3,\n            e.block4,\n        ])\n        e = None  #dropped\n        self.feature = nn.Conv2d(512,32, kernel_size=1) #dummy conv for dim reduction\n        self.logit = nn.Conv2d(32,num_class, kernel_size=1)\n\n    def forward(self, x):\n        batch_size,C,H,W = x.shape\n\n        for i in range( len(self.block)):\n            x = self.block[i](x)\n            #print(i, x.shape)\n\n        x = F.dropout(x,0.5,training=self.training)\n        x = F.adaptive_avg_pool2d(x, 1)\n        x = self.feature(x)\n        logit = self.logit(x)\n        return logit\n","1fbb5c17":"model_classification = Resnet34_classification()\nmodel_classification.load_state_dict(torch.load('..\/input\/clsification\/00007500_model.pth', map_location=lambda storage, loc: storage), strict=True)","08e612c9":"# Dataset setup\nclass TestDataset(Dataset):\n    '''Dataset for test prediction'''\n    def __init__(self, root, df, mean, std):\n        self.root = root\n        df['ImageId'] = df['ImageId_ClassId'].apply(lambda x: x.split('_')[0])\n        self.fnames = df['ImageId'].unique().tolist()\n        self.num_samples = len(self.fnames)\n        self.transform = Compose(\n            [\n                Normalize(mean=mean, std=std, p=1),\n                ToTensor(),\n            ]\n        )\n\n    def __getitem__(self, idx):\n        fname = self.fnames[idx]\n        path = os.path.join(self.root, fname)\n        image = cv2.imread(path)\n        images = self.transform(image=image)[\"image\"]\n        return fname, images\n\n    def __len__(self):\n        return self.num_samples","785e7383":"sample_submission_path = '..\/input\/severstal-steel-defect-detection\/sample_submission.csv'\ntest_data_folder = \"..\/input\/severstal-steel-defect-detection\/test_images\"","2d600fa3":"# hyperparameters\nbatch_size = 1\n\n# mean and std\nmean = (0.485, 0.456, 0.406)\nstd = (0.229, 0.224, 0.225)","d6dba9a1":"df = pd.read_csv(sample_submission_path)","7485c50e":"# dataloader\ntestset = DataLoader(\n    TestDataset(test_data_folder, df, mean, std),\n    batch_size=batch_size,\n    shuffle=False,\n    num_workers=0,\n    pin_memory=True\n)","35832f0b":"# useful functions for setting up inference\n\ndef sharpen(p,t=0.5):\n        if t!=0:\n            return p**t\n        else:\n            return p\n\ndef get_classification_preds(net,test_loader):\n    test_probability_label = []\n    test_id   = []\n    \n    net = net.cuda()\n    for t, (fnames, images) in enumerate(tqdm(test_loader)):\n        batch_size,C,H,W = images.shape\n        images = images.cuda()\n\n        with torch.no_grad():\n            net.eval()\n\n            num_augment = 0\n            if 1: #  null\n                logit =  net(images)\n                probability = torch.sigmoid(logit)\n\n                probability_label = sharpen(probability,0)\n                num_augment+=1\n\n            if 'flip_lr' in augment:\n                logit = net(torch.flip(images,dims=[3]))\n                probability  = torch.sigmoid(logit)\n\n                probability_label += sharpen(probability)\n                num_augment+=1\n\n            if 'flip_ud' in augment:\n                logit = net(torch.flip(images,dims=[2]))\n                probability = torch.sigmoid(logit)\n\n                probability_label += sharpen(probability)\n                num_augment+=1\n\n            probability_label = probability_label\/num_augment\n\n        probability_label = probability_label.data.cpu().numpy()\n        \n        test_probability_label.append(probability_label)\n        test_id.extend([i for i in fnames])\n\n    \n    test_probability_label = np.concatenate(test_probability_label)\n    return test_probability_label, test_id","aeb342a9":"# threshold for classification\nthreshold_label = [0.50,0.50,0.50,0.50,]","62584c7f":"augment = ['null'] #['null', 'flip_lr','flip_ud'] #['null, 'flip_lr','flip_ud','5crop']","4cf04cce":"# Get prediction for classification model\n\nprobability_label, image_id = get_classification_preds(model_classification, testset)\npredict_label = probability_label>np.array(threshold_label).reshape(1,4,1,1)\n\nimage_id_class_id = []\nencoded_pixel = []\nfor b in range(len(image_id)):\n    for c in range(4):\n        image_id_class_id.append(image_id[b]+'_%d'%(c+1))\n        if predict_label[b,c]==0:\n            rle=''\n        else:\n            rle ='1 1'\n        encoded_pixel.append(rle)\n\ndf_classification = pd.DataFrame(zip(image_id_class_id, encoded_pixel), columns=['ImageId_ClassId', 'EncodedPixels'])","c53cea44":"df_classification.head()","6711d4b8":"# get segmentation model and load weights\n\n# Initialize mode and load trained weights\nckpt_path = \"..\/input\/res18fullsize\/model_swa.pth\"\ndevice = torch.device(\"cuda\")\nmodel_segmentation = Unet(\"resnet18\", encoder_weights=None, classes=4, activation=None)\nmodel_segmentation.to(device)\nmodel_segmentation.eval()\nstate = torch.load(ckpt_path, map_location=lambda storage, loc: storage)\nmodel_segmentation.load_state_dict(state[\"state_dict\"])","c7760799":"models = [model_segmentation] # add other models for ensemble","affdd9d6":"#https:\/\/www.kaggle.com\/paulorzp\/rle-functions-run-lenght-encode-decode\ndef mask2rle(img):\n    '''\n    img: numpy array, 1 - mask, 0 - background\n    Returns run length as string formated\n    '''\n    pixels= img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)","d059d3d6":"def post_process(probability, threshold, min_size):\n    '''Post processing of each predicted mask, components with lesser number of pixels\n    than `min_size` are ignored'''\n    mask = cv2.threshold(probability, threshold, 1, cv2.THRESH_BINARY)[1]\n    num_component, component = cv2.connectedComponents(mask.astype(np.uint8))\n    predictions = np.zeros((256, 1600), np.float32)\n    num = 0\n    for c in range(1, num_component):\n        p = (component == c)\n        if p.sum() > min_size:\n            predictions[p] = 1\n            num += 1\n    return predictions, num","8e398b96":"# thresholds and min_size for segmentation predictions\n# play with them and see how LB changes\nthreshold_pixel = [0.5,0.5,0.5,0.5,] \nmin_size = [200,1500,1500,2000]   ","299aa64b":"#test time augmentation  -----------------------\ndef null_augment   (input): return input\ndef flip_lr_augment(input): return torch.flip(input, dims=[2])\ndef flip_ud_augment(input): return torch.flip(input, dims=[3])\n\ndef null_inverse_augment   (logit): return logit\ndef flip_lr_inverse_augment(logit): return torch.flip(logit, dims=[2])\ndef flip_ud_inverse_augment(logit): return torch.flip(logit, dims=[3])\n\naugment = (\n        (null_augment,   null_inverse_augment   ),\n        (flip_lr_augment,flip_lr_inverse_augment),\n        (flip_ud_augment,flip_ud_inverse_augment),\n    )","c808ecbd":"# make preidctions for segmentation model\n\npredictions = []\nfor i, batch in enumerate(tqdm(testset)):\n    fnames, images = batch\n    #print('images', images.shape)\n    images = images.cuda()\n    batch_preds = 0\n    probabilities = []\n    for model in models:\n        model = model.cuda()\n        for k, (a, inv_a) in enumerate(augment):\n                logit = model(a(images))\n                p = inv_a(torch.sigmoid(logit))\n\n                if k ==0:\n                    probability  = p**0.5\n                else:\n                    probability += p**0.5\n        probability = probability\/len(augment)\n        probabilities.append(probability)\n        \n        batch_preds+=probability\n    \n    batch_preds = batch_preds.data.cpu().numpy()\n    #print(batch_preds.shape)\n    for fname, preds in zip(fnames, batch_preds):\n        for cls, pred in enumerate(preds):\n            #print(cls)\n            pred, num = post_process(pred, threshold_pixel[cls], min_size[cls])\n            rle = mask2rle(pred)\n            name = fname + f\"_{cls+1}\"\n            predictions.append([name, rle])\n\n\ndf_segmentation = pd.DataFrame(predictions, columns=['ImageId_ClassId', 'EncodedPixels'])","ddc3968e":"df_segmentation.head()","a5aae269":"df= df_segmentation.copy()","2275e6b8":"# stats for predictions from segmentation model\nif 1:\n        df['Class'] = df['ImageId_ClassId'].str[-1].astype(np.int32)\n        df['Label'] = (df['EncodedPixels']!='').astype(np.int32)\n        pos1 = ((df['Class']==1) & (df['Label']==1)).sum()\n        pos2 = ((df['Class']==2) & (df['Label']==1)).sum()\n        pos3 = ((df['Class']==3) & (df['Label']==1)).sum()\n        pos4 = ((df['Class']==4) & (df['Label']==1)).sum()\n\n        num_image = len(df)\/\/4\n        num = len(df)\n        pos = (df['Label']==1).sum()\n        neg = num-pos\n\n        print('')\n        print('\\t\\tnum_image = %5d(1801)'%num_image)\n        print('\\t\\tnum  = %5d(7204)'%num)\n        print('\\t\\tneg  = %5d(6172)  %0.3f'%(neg,neg\/num))\n        print('\\t\\tpos  = %5d(1032)  %0.3f'%(pos,pos\/num))\n        print('\\t\\tpos1 = %5d( 128)  %0.3f  %0.3f'%(pos1,pos1\/num_image,pos1\/pos))\n        print('\\t\\tpos2 = %5d(  43)  %0.3f  %0.3f'%(pos2,pos2\/num_image,pos2\/pos))\n        print('\\t\\tpos3 = %5d( 741)  %0.3f  %0.3f'%(pos3,pos3\/num_image,pos3\/pos))\n        print('\\t\\tpos4 = %5d( 120)  %0.3f  %0.3f'%(pos4,pos4\/num_image,pos4\/pos))","b1cbdd54":"df_mask = df_segmentation.copy()\ndf_label = df_classification.copy()","eb052773":"# do filtering using predictions from classification and segmentation models\nassert(np.all(df_mask['ImageId_ClassId'].values == df_label['ImageId_ClassId'].values))\nprint((df_mask.loc[df_label['EncodedPixels']=='','EncodedPixels'] != '').sum() ) #202\ndf_mask.loc[df_label['EncodedPixels']=='','EncodedPixels']=''","9e5a30ce":"df_mask.to_csv(\"submission.csv\", index=False)","849c2307":"# stats for final submission\nif 1:\n        df_mask['Class'] = df_mask['ImageId_ClassId'].str[-1].astype(np.int32)\n        df_mask['Label'] = (df_mask['EncodedPixels']!='').astype(np.int32)\n        pos1 = ((df_mask['Class']==1) & (df_mask['Label']==1)).sum()\n        pos2 = ((df_mask['Class']==2) & (df_mask['Label']==1)).sum()\n        pos3 = ((df_mask['Class']==3) & (df_mask['Label']==1)).sum()\n        pos4 = ((df_mask['Class']==4) & (df_mask['Label']==1)).sum()\n\n        num_image = len(df_mask)\/\/4\n        num = len(df_mask)\n        pos = (df_mask['Label']==1).sum()\n        neg = num-pos\n\n        print('')\n        print('\\t\\tnum_image = %5d(1801)'%num_image)\n        print('\\t\\tnum  = %5d(7204)'%num)\n        print('\\t\\tneg  = %5d(6172)  %0.3f'%(neg,neg\/num))\n        print('\\t\\tpos  = %5d(1032)  %0.3f'%(pos,pos\/num))\n        print('\\t\\tpos1 = %5d( 128)  %0.3f  %0.3f'%(pos1,pos1\/num_image,pos1\/pos))\n        print('\\t\\tpos2 = %5d(  43)  %0.3f  %0.3f'%(pos2,pos2\/num_image,pos2\/pos))\n        print('\\t\\tpos3 = %5d( 741)  %0.3f  %0.3f'%(pos3,pos3\/num_image,pos3\/pos))\n        print('\\t\\tpos4 = %5d( 120)  %0.3f  %0.3f'%(pos4,pos4\/num_image,pos4\/pos))","766c1396":"So we will first make prediction for classification models and save the predictions, then use segmentation models to make predictions; the predictions from classification models will be used to filer(clean) the predictions from segmentation model to get final prediction for submission which scores `0.89306`. See [version2](https:\/\/www.kaggle.com\/bibek777\/heng-s-model-inference-kernel?scriptVersionId=20535985) for this!!","f410e0fb":"If this kernel was helpful, upvote every topic that heng has posted as this is purely based on his code\/ideas. Thank you again Heng!!!! ","082f6ee6":"This kernel uses [Heng](https:\/\/www.kaggle.com\/hengck23)'s (latest) baseline code and trained models to make inference. Thank you soooooooooooo much Heng for your generous contributions to this competition. Your code and insights have been a great learning resource for beginners like me and others.\n\nAs Heng mentioned [here](https:\/\/www.kaggle.com\/c\/severstal-steel-defect-detection\/discussion\/106462#621101), his latest experiments uses Classification + Segmentation pipeline, shown in the figure below:\n![Alt text](https:\/\/www.dropbox.com\/s\/zzdy1h6fzt5tz7b\/heng.png?dl=1)","5739360a":"The segmentation model used in this version of kernel is different from what Heng has uploaded in his starter-kit. I used the same architecture(ResNet18) but using [segmentation_models.pytorch](https:\/\/github.com\/qubvel\/segmentation_models.pytorch) but you can use his configuration as well. Actually, I used his configuration in [version1](https:\/\/www.kaggle.com\/bibek777\/heng-s-model-inference-kernel\/output?scriptVersionId=20501547) of this kenel but failed to get 0.90161 as claimed by Heng. Maybe I'm missing something.....\nI will leave that for you to figure out. "}}