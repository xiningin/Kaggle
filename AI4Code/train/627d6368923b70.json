{"cell_type":{"2201abc7":"code","60af79d3":"code","6f4d2acb":"code","c73f13ef":"code","706337d7":"code","a7d162ac":"code","1915c13f":"code","a2a931aa":"code","ea22998e":"code","caf385cb":"code","9e5da42e":"code","8031028f":"code","a364c6c3":"code","723faef1":"code","ca73979d":"code","b96c73d8":"code","797cfc0c":"code","dfa150fd":"code","53d000a9":"code","32e3e149":"code","f881a6e5":"code","d645b143":"code","acafcfc7":"code","0ad2c4df":"markdown","e90ff22d":"markdown","d6ca6914":"markdown","26d46fef":"markdown","8ec283f4":"markdown","e7a60ea3":"markdown","005f8925":"markdown","7e08f15c":"markdown","08e87ebb":"markdown","2929db72":"markdown"},"source":{"2201abc7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","60af79d3":"from sklearn.model_selection import StratifiedKFold\n","6f4d2acb":"# Reading the Data\ntrain = pd.read_csv('\/kaggle\/input\/widsdatathon2021\/TrainingWiDS2021.csv')\ntest = pd.read_csv('\/kaggle\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv')","c73f13ef":"# checking the data head\ntrain.head()","706337d7":"train.isnull().sum()","a7d162ac":"test.isnull().sum()","1915c13f":"# selecting our categorical features \ncat_feats = train.dtypes[train.dtypes == 'object'].index","a2a931aa":"# fuctions to filll the categorical features\nfor cols in cat_feats:\n    train[cols] = train[cols].fillna('missing')\n    test[cols] = test[cols].fillna('missing')","ea22998e":"# filling the remaining missing value with 0\ntrain = train.fillna(value=0)\ntest = test.fillna(value=0)","caf385cb":"# checking if there is still missing value\nprint(train.columns[train.isnull().any()])\nprint(test.columns[test.isnull().any()])","9e5da42e":"# checking our cat features number of unique\ncat_feats","8031028f":"for cols in cat_feats:\n    print('___________')\n    print(train[cols].value_counts())","a364c6c3":"for cols in cat_feats:\n    print('___________')\n    print(test[cols].value_counts())","723faef1":"# storing my test_id \ntest_id = test['encounter_id']","ca73979d":"# drop the insignificant features\ntrain.drop(['Unnamed: 0','encounter_id', 'hospital_id'], axis=1, inplace=True)\ntest.drop(['Unnamed: 0','encounter_id', 'hospital_id'], axis=1, inplace=True)","b96c73d8":"categorical_feats = [\n    f for f in train.columns if train[f].dtype == 'object'\n]\n\ncategorical_feats\nfor f_ in categorical_feats:\n    train[f_] = train[f_].astype('category')","797cfc0c":"categorical_feats = [\n    f for f in test.columns if test[f].dtype == 'object'\n]\n\ncategorical_feats\nfor f_ in categorical_feats:\n    test[f_] = test[f_].astype('category')","dfa150fd":"X = train.drop(\"diabetes_mellitus\", axis=1)\ny = train[\"diabetes_mellitus\"]","53d000a9":"    params = {}\n    params[\"objective\"] = \"binary\"\n    params['metric'] = 'auc'\n    params[\"max_depth\"] = -1\n    params[\"num_leaves\"] = 31\n    params[\"min_data_per_group\"] = 10 # this paramter can change the modell performance if tune well\n    params[\"learning_rate\"] = 0.1","32e3e149":"from lightgbm import LGBMClassifier\nfrom sklearn.metrics import accuracy_score,confusion_matrix,roc_auc_score\nerrcb=[]\ny_pred_totcb=[]\nfold=StratifiedKFold(n_splits=3, shuffle=True, random_state=2021)\ni=1\nfor train_index, test_index in fold.split(X,y):\n    X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n    y_train, y_test = y[train_index], y[test_index]\n    m = LGBMClassifier(**params)\n    m.fit(X_train,y_train,categorical_feature=categorical_feats,eval_set=[(X_train,y_train),(X_test, y_test)],verbose=-1, early_stopping_rounds=100)#,verbose=100)\n    preds=m.predict_proba(X_test)[:, 1]\n    print(\"err: \",roc_auc_score(y_test,preds))\n    errcb.append(roc_auc_score(y_test,preds))\n    p = m.predict_proba(test)[:, 1]\n    y_pred_totcb.append(p)","f881a6e5":"np.mean(errcb)","d645b143":"d = {\"encounter_id\": test_id, 'diabetes_mellitus': np.mean(y_pred_totcb, 0)}\npred = pd.DataFrame(data=d)\nsub = pred[[\"encounter_id\", 'diabetes_mellitus']]","acafcfc7":"sub.to_csv('lgb_encode.csv', index=False)","0ad2c4df":"I will be filling the missing categorical value using **Missing** and contineous missing value using **0**. it works best for me now.","e90ff22d":"### Modelling\n\ni will be using 3 stratified Kfold you can increase it ","d6ca6914":"Modlling part and encoding the categorical features","26d46fef":"**LightGbm Encoding.\nIn this notebook i will introduce us to one of the GBDT'S power encoding catgorical variable using LGBM**\n\n**Categorical Feature Support\nLightGBM offers good accuracy with integer-encoded categorical features. LightGBM applies Fisher (1958) to find the optimal split over categories as described here. This often performs better than one-hot encoding.**\n\n**Use categorical_feature to specify the categorical features. Refer to the parameter categorical_feature in Parameters.**\n\n**Categorical features must be encoded as non-negative integers (int) less than Int32.MaxValue (2147483647). It is best to use a contiguous range of integers started from zero.\n**\nUse min_data_per_group, cat_smooth to deal with over-fitting (when #data is small or #category is large).**\n\n**For a categorical feature with high cardinality (#category is large), it often works best to treat the feature as numeric, either by simply ignoring the categorical interpretation of the integers or by embedding the categories in a low-dimensional numeric space.****\n\nread more through this link - https:\/\/lightgbm.readthedocs.io\/en\/latest\/Advanced-Topics.html","8ec283f4":"Progress","e7a60ea3":"In this Noteboook i will not be doing more of visualization as there are many good note you can follow\n\nhttps:\/\/www.kaggle.com\/reverie5\/statistical-analysis-univariate-multivariate\n\nThis notebook will help","005f8925":"The Hospital is overfitting the leaderboard so we have to drop","7e08f15c":"A problem with hospital admit source as the train and test nunique are not equal.\nlet leave it for now, since we are going to be using Lightgbm encoding power","08e87ebb":"#### Checking and dealing with missing value","2929db72":"Thanks. More to come\n\nHow to improve your model\n\n1. Try filling the missing value with more detailed analysis\n2. tune the lightgbm parameters\n3. increase the Number of splits"}}