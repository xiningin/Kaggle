{"cell_type":{"8e925837":"code","283478e1":"code","ae354be5":"code","eb675250":"code","78801413":"code","50688e92":"code","4c879765":"code","4d2dfb3e":"code","3adc2caf":"code","28ac8244":"code","702e1d66":"code","8b6e70c4":"code","7de65f3b":"code","ae036875":"code","05271456":"code","3570aaf9":"code","309fadee":"code","d64ec1e8":"code","da910f45":"code","91582165":"code","d228259d":"code","7dff10a9":"code","982a64e4":"markdown","4b1433f7":"markdown","e6d4fb6b":"markdown","e176b4f7":"markdown","f97c6dd0":"markdown","f29e4029":"markdown","8ffe7297":"markdown","0e22759d":"markdown","4f825ed2":"markdown","e97ea7bb":"markdown","47ebfa66":"markdown","00b22269":"markdown","5f21e3e5":"markdown","2d4a9eae":"markdown","cf712b70":"markdown","0443259d":"markdown","5782247d":"markdown","28e56e58":"markdown","e6d2b31e":"markdown","5d9df668":"markdown","c000770a":"markdown"},"source":{"8e925837":"import numpy as np # linear algebra\nfrom tqdm import tqdm_notebook\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport gc\nimport pdb","283478e1":"!pip install transformers","ae354be5":"import plotly.graph_objs as go\nfrom matplotlib import pyplot as plt\nimport plotly.offline as py\nimport regex as re\nfrom bs4 import BeautifulSoup\nimport string\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\nfrom nltk.tokenize import word_tokenize\nfrom gensim.models import word2vec","eb675250":"\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim","78801413":"import nltk\nnltk.download('punkt')","50688e92":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import cross_val_score\nimport torch\nimport transformers as ppb\nimport warnings\nwarnings.filterwarnings('ignore')","4c879765":"device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")","4d2dfb3e":"train = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/test.csv\")\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")","3adc2caf":"def get_embedding(df,col):\n    \n    corpus = create_corpus(df,col)\n    w2v_model = word2vec.Word2Vec([corpus], size=50, window=10, min_count=1, workers=4)\n    embedding_dict={}\n    for word in corpus:\n        embedding_dict[word] = w2v_model.wv[word]\n\n    num_words=len(corpus)\n    embedding_matrix=np.zeros((num_words,50))\n\n    for i,word in tqdm_notebook(enumerate(df[col].unique())):\n        if i > num_words:\n            continue\n        emb_vec=embedding_dict.get(word)\n        if emb_vec is not None:\n            embedding_matrix[i]=emb_vec\n    return embedding_matrix\n\ndef create_corpus(df,col):\n    corpus=[]\n    for keyword in tqdm_notebook(df[col].unique()):\n        keyword=keyword.lower()\n        corpus.append(keyword)\n    return corpus","28ac8244":"def combine_features(df,features,cat_cols):\n    \n    df_copy=df.copy()\n\n    for col in cat_cols:\n        embedding = get_embedding(df_copy,col)\n        vec = {val:embedding[i]  for i, val in enumerate(df_copy[col].unique())}\n        for key,value in vec.items():\n            df_copy[col] = df_copy[col].map(lambda x: value if x == key else x)\n        \n        embed_array = np.stack(df_copy[col].values,axis=0)\n        features = np.concatenate((features,embed_array),axis=1)\n    return features","702e1d66":"train.head()","8b6e70c4":"# lets understand the target distribution\ntarget_cnt=train.target.value_counts()\n\nlabels = (np.array(target_cnt.index))\nsizes = (np.array((target_cnt \/ target_cnt.sum())*100))\n\ntrace = go.Pie(labels=labels, values=sizes)\nlayout = go.Layout(\n    title='Target distribution',\n    font=dict(size=10),\n    width=300,\n    height=300,\n)\ndata = [trace]\nfig = go.Figure(data=data, layout=layout)\npy.iplot(fig, filename=\"usertype\")","7de65f3b":"# fill NaN with empty string\ntrain['keyword'] = train.keyword.fillna(value='None')\ntrain['location'] = train.location.fillna(value='None')\n\ntest['keyword'] = test.keyword.fillna(value='None')\ntest['location'] = test.location.fillna(value='None')","ae036875":"train.head()","05271456":"\ndef get_features(data, batch_size=2500):\n    # Use DistilBERT as feature extractor:\n    model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n    # Load pretrained model\/tokenizer\n    tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n    model = model_class.from_pretrained(pretrained_weights)\n    model.to(device)\n    model = nn.DataParallel(model)\n    \n    # tokenize,padding and masking\n    tokenized = data[\"text\"].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n    max_len = 0\n    for i in tokenized.values:\n        if len(i) > max_len:\n            max_len = len(i)\n    padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n\n    attention_mask = np.where(padded != 0, 1, 0)\n    \n    last_hidden_states=[]\n    no_batch = data.shape[0]\/\/batch_size\n    start_index=0\n    end_index=1\n    for i in tqdm_notebook(range(1,no_batch+2)):\n\n        if  data.shape[0]>batch_size*i:\n                end_index=batch_size*i\n        else:\n            end_index=train.shape[0]\n\n        input_ids = torch.tensor(padded[start_index:end_index])  \n        batch_attention_mask = torch.tensor(attention_mask[start_index:end_index])\n\n        input_ids = input_ids.to(device)\n        batch_attention_mask = batch_attention_mask.to(device)\n\n        with torch.no_grad():\n            batch_hidden_state = model(input_ids, attention_mask=batch_attention_mask)\n            print(\"Batch {} is completed sucessfully\".format(i))\n            last_hidden_states.append(batch_hidden_state[0])\n\n        start_index=batch_size*i\n        end_index=batch_size*i\n    fin_features = torch.cat(last_hidden_states,0)\n    clf_features = fin_features[:,0,:].cpu().numpy()\n    return clf_features","3570aaf9":"gc.collect()\nfeatures = get_features(train,batch_size=2500)\ntest_distil_features = get_features(test,batch_size=2500)","309fadee":"cat_cols = ['keyword']","d64ec1e8":"train_features = combine_features(train,features,cat_cols)\ntest_features = combine_features(test,test_distil_features,cat_cols)","da910f45":"labels = train[\"target\"]","91582165":"## Use features from previous modle and train a Logistic regression model\n# labels = train[\"target\"]\n# train model\nlr_clf = LogisticRegression()\nlr_clf.fit(train_features, labels)","d228259d":"test_pred = lr_clf.predict(test_features)","7dff10a9":"submission['target'] = test_pred\nsubmission.to_csv('submission.csv', index=False)","982a64e4":"## Acknowledgement\n\nSome code block are used from\n* https:\/\/www.kaggle.com\/nandhuelan\/tweets-models-v3\n* http:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/\n\n![](http:\/\/)Refer a detailed and wonderful blog at http:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/ to understand how DistilBERT is used for feature extraction.","4b1433f7":"### Data cleaning ","e6d4fb6b":"## About Data","e176b4f7":"## Feature Extraction using DistilBERT","f97c6dd0":"![image.png](attachment:image.png)\n\nSource: http:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/","f29e4029":"## Functions","8ffe7297":"The model() function runs our sentences through BERT. The results of the processing will be returned into last_hidden_states.","0e22759d":"This notebook uses DistilBERT for feature extraction.\nIt combines extracted features and word2Vec embedding for a categorical columns and train  a classifier for tweet classification.\n\n","4f825ed2":"Columns\n\n* id - a unique identifier for each tweet\n* text - the text of the tweet\n* location - the location the tweet was sent from (may be blank)\n* keyword - a pmarticular keyword from the tweet (may be blank)\n* target - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n","e97ea7bb":"## Settings","47ebfa66":"##  Create Word embedding from the keywords add it to the DistilBERT feature","00b22269":"### embedding","5f21e3e5":"## Problem Introduction","2d4a9eae":"This notebook approach is derived and inspired from http:\/\/jalammar.github.io\/a-visual-guide-to-using-bert-for-the-first-time\/","cf712b70":"Twitter has become an important communication channel in times of emergency.\nThe ubiquitousness of smartphones enables people to announce an emergency they\u2019re observing in real-time. Because of this, more agencies are interested in programatically monitoring Twitter (i.e. disaster relief organizations and news agencies).\n\nBut, it\u2019s not always clear whether a person\u2019s words are actually announcing a disaster.\n\n**We have to build a machine learning model that predicts which Tweets are about real disasters and which one\u2019s aren\u2019t. We have a dataset of 10,000 tweets that were had classified.**","0443259d":"## Training a Logistic Regression model using features from DistilBERT","5782247d":"Let's slice only the part of the output that we need. That is the output corresponding the first token of each sentence. The way BERT does sentence classification, is that it adds a token called [CLS] (for classification) at the beginning of every sentence. The output corresponding to that token can be thought of as an embedding for the entire sentence.","28e56e58":"### data understanding","e6d2b31e":"After tokenization, tokenized is a list of sentences -- each sentences is represented as a list of tokens. We want BERT to process our examples all at once (as one batch). It's just faster that way. For that reason, we need to pad all lists to the same size, so we can represent the input as one 2-d array, rather than a list of lists (of different lengths).","5d9df668":"If we directly send padded to BERT, that would slightly confuse it. We need to create another variable to tell it to ignore (mask) the padding we've added when it's processing its input. That's what attention_mask is:","c000770a":"## Create a submission file\n"}}