{"cell_type":{"be357547":"code","cc66cf5c":"code","d596cd0a":"code","145f25cd":"code","a24fcee5":"code","87e8ffcb":"code","8e971983":"code","70ea5fbc":"code","801dd88c":"code","d5cc3010":"code","cfc0e807":"code","3200a8da":"code","778f4156":"code","98d62be1":"code","a0696a4d":"code","9562620e":"code","4d52ac4c":"code","ab2e53af":"code","cef4eb40":"code","0fb2a426":"code","2826d3e6":"code","34973a6a":"code","1242b2cb":"markdown","64b1668b":"markdown","a0916464":"markdown","229c002d":"markdown","50794800":"markdown","eeed1fdf":"markdown","2c164c9e":"markdown","33bc69f8":"markdown","e033333e":"markdown"},"source":{"be357547":"# Check if TPU\/GPU is available\nimport tensorflow as tf\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n    DEVICE = \"tpu\"\nexcept ValueError:\n    if tf.test.is_gpu_available():\n        DEVICE = \"gpu\"\n    else:\n        DEVICE = \"cpu\"\n\nprint(\"Accelerator: {}\".format(DEVICE))","cc66cf5c":"# Set up an environment for accessing TPU\nif DEVICE == \"tpu\":\n    !curl https:\/\/raw.githubusercontent.com\/pytorch\/xla\/master\/contrib\/scripts\/env-setup.py -o pytorch-xla-env-setup.py\n    !python pytorch-xla-env-setup.py --apt-packages libomp5 libopenblas-dev\n    !pip install pytorch-lightning\n    import torch_xla\n    import torch_xla.core.xla_model as xm","d596cd0a":"import os\nos.environ[\"WANDB_API_KEY\"] = \"0\"  # to silence warning","145f25cd":"import gc\nimport glob\nimport numpy as np\nimport pandas as pd\nimport torch\nimport torch.nn.functional as F\nfrom sklearn.model_selection import train_test_split\nfrom transformers import BertTokenizer, BertForSequenceClassification\nfrom torch.utils.data import Dataset, DataLoader\ntry:\n    from pytorch_lightning import LightningDataModule, LightningModule, Trainer, loggers, seed_everything\n    from pytorch_lightning.metrics import functional as FM\nexcept OSError:  # Reloading pytorch_lightning again to resolve OSError issues with TPU\n    from pytorch_lightning import LightningDataModule, LightningModule, Trainer, loggers, seed_everything\n    from pytorch_lightning.metrics import functional as FM","a24fcee5":"# Remove temp installation files to release space\ngc.collect()\npaths = glob.glob(\"\/kaggle\/working\/*\")\nfor path in paths:\n    try:\n        if os.path.isfile(path):\n            os.remove(path)\n        elif os.path.isdir(path):\n            shutil.rmtree(path)\n    except:\n        print(\"Not removable: {}\".format(path))","87e8ffcb":"# Global Variables\nSEED = 2020\nPRETRAINED_MODEL = \"bert-base-multilingual-cased\"\nMAX_EPOCHS = 2\nif DEVICE == \"tpu\":\n    BATCH_SIZE = 8\n    MAX_TOKEN_LEN = 50\n    TPU_CORES = 1\n    GPUS = 1\n    NUM_WORKERS = 4\nelse:\n    BATCH_SIZE = 64\n    MAX_TOKEN_LEN = 50\n    TPU_CORES = 1\n    GPUS = 1\n    NUM_WORKERS = 4","8e971983":"# Dataset with labels\ndataset_df = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/train.csv\")\ndataset_df.head()","70ea5fbc":"len(dataset_df)","801dd88c":"dataset_df[\"language\"].value_counts()","d5cc3010":"dataset_df[\"label\"].value_counts()","cfc0e807":"dataset_df.groupby([\"language\", \"label\"]).size()","3200a8da":"# Production dataset for inference and submission\nprod_df = pd.read_csv(\"..\/input\/contradictory-my-dear-watson\/test.csv\")\nprod_df.head()","778f4156":"len(prod_df)","98d62be1":"prod_df[\"language\"].value_counts()","a0696a4d":"# # Figure out how transformers tokenizer encodes sentences\n# tokenizer = BertTokenizer.from_pretrained(PRETRAINED_MODEL)\n# premise_example = dataset_df.premise.values[1]\n# hypothesis_example = dataset_df.hypothesis.values[1]\n# print(\"Original premise: {}\".format(premise_example))\n# print(\"Original hypothesis: {}\".format(hypothesis_example))\n# print()\n# encoded_sents = tokenizer.encode_plus(premise_example, \n#                                       hypothesis_example,\n#                                       add_special_tokens=True, \n#                                       pad_to_max_length=True,\n#                                       max_length=MAX_TOKEN_LEN, \n#                                       truncation=True, \n#                                       return_attention_mask=True, \n#                                       return_token_type_ids=True,\n#                                       return_tensors=\"pt\")\n# print(encoded_sents)\n# print()\n# decoded_sents = tokenizer.decode(encoded_sents[\"input_ids\"][0])\n# print(decoded_sents)","9562620e":"class NLIDataset(Dataset):\n    def __init__(self, \n                 dataset: pd.DataFrame, \n                 model: str = PRETRAINED_MODEL,\n                 max_token_len: int = MAX_TOKEN_LEN,\n                 production: bool = False\n                ):\n        self.dataset = dataset\n        self.model = model\n        self.max_token_len = max_token_len\n        self.production = production\n    \n    def __len__(self):\n        return len(self.dataset)\n    \n    def __getitem__(self, index: int):\n        premise = self.dataset.premise.values[index]\n        hypothesis = self.dataset.hypothesis.values[index]\n        tokenizer = BertTokenizer.from_pretrained(self.model)\n        encoded_sents = tokenizer.encode_plus(premise, \n                                              hypothesis,\n                                              add_special_tokens=True, \n                                              pad_to_max_length=True, \n                                              max_length=self.max_token_len, \n                                              truncation=True, \n                                              return_attention_mask=True, \n                                              return_token_type_ids=True,\n                                              return_tensors=\"pt\")\n        \n        inputs = {\n            \"input_ids\": encoded_sents[\"input_ids\"][0],\n            \"token_type_ids\": encoded_sents[\"token_type_ids\"][0],\n            \"attention_mask\": encoded_sents[\"attention_mask\"][0]\n        }\n        \n        if self.production:\n            row_id = self.dataset.id.values[index]\n            return inputs, row_id\n        else:\n            label = self.dataset.label.values[index]\n            return inputs, label","4d52ac4c":"class NLIDataModule(LightningDataModule):\n    def __init__(self, \n                 dataset: pd.DataFrame, \n                 model: str = PRETRAINED_MODEL,\n                 batch_size: int = BATCH_SIZE,\n                 random_state: int = SEED,\n                 num_workers: int = NUM_WORKERS\n                ):\n        super().__init__()\n        self.dataset = dataset\n        self.model = model\n        self.batch_size = batch_size\n        self.random_state = random_state\n        self.num_workers = num_workers\n        self.train_df = None\n        self.val_df = None\n        self.test_df = None\n        self.train_set = None\n        self.val_set = None\n        self.test_set = None\n\n    def prepare_data(self): \n        tmp, self.test_df = train_test_split(self.dataset, \n                                             test_size=0.15, \n                                             random_state=self.random_state, \n                                             stratify=self.dataset[['label', 'language']])\n        self.train_df, self.val_df = train_test_split(tmp,\n                                                      test_size=0.15, \n                                                      random_state=self.random_state, \n                                                      stratify=tmp[['label', 'language']])\n        \n    def setup(self, stage: str = None):\n        if stage == \"fit\" or stage is None:\n            self.train_set = NLIDataset(self.train_df)\n            self.val_set = NLIDataset(self.val_df)\n        elif stage == \"test\":\n            self.test_set = NLIDataset(self.test_df)\n\n    def train_dataloader(self):\n        return DataLoader(self.train_set, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=True)\n    \n    def val_dataloader(self):\n        return DataLoader(self.val_set, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False)\n\n    def test_dataloader(self):\n        return DataLoader(self.test_set, batch_size=self.batch_size, num_workers=self.num_workers, shuffle=False)","ab2e53af":"# # Test NLIDataset & NLIDataModule\n# dm = NLIDataModule(dataset_df)\n# dm.prepare_data()\n# dm.setup('fit')\n# next(iter(dm.train_dataloader()))","cef4eb40":"class NLIMultilingualBERT(LightningModule):\n    def __init__(self, model: BertForSequenceClassification):\n        super().__init__()\n        self.model = model\n\n    def forward(self, inputs):\n        predictions = self.model(input_ids=inputs[\"input_ids\"], \n                                 attention_mask=inputs[\"attention_mask\"],\n                                 token_type_ids=inputs[\"token_type_ids\"])\n        return predictions\n\n    def training_step(self, batch, batch_idx):\n        inputs, labels = batch\n        _, predictions = self.model(input_ids=inputs[\"input_ids\"],\n                                    attention_mask=inputs[\"attention_mask\"],\n                                    token_type_ids=inputs[\"token_type_ids\"],\n                                    labels=labels)\n        train_loss = F.cross_entropy(predictions, labels)\n        self.log('train_loss', train_loss, prog_bar=True)\n        return train_loss\n    \n    def validation_step(self, batch, batch_idx):\n        inputs, labels = batch\n        _, predictions = self.model(input_ids=inputs[\"input_ids\"],\n                                    attention_mask=inputs[\"attention_mask\"],\n                                    token_type_ids=inputs[\"token_type_ids\"],\n                                    labels=labels)\n        \n        val_loss = F.cross_entropy(predictions, labels)\n        val_acc = FM.accuracy(predictions, labels)\n        metrics = {'val_acc': val_acc, 'val_loss': val_loss}\n        self.log_dict(metrics, prog_bar=True)\n        return metrics\n\n    def test_step(self, batch, batch_idx):\n        metrics = self.validation_step(batch, batch_idx)\n        metrics = {'test_acc': metrics['val_acc'], 'test_loss': metrics['val_loss']}\n        self.log_dict(metrics, prog_bar=True)\n    \n    def configure_optimizers(self):\n        optimizer = torch.optim.Adam(self.parameters(), lr=2e-5)\n        return optimizer","0fb2a426":"# General settings\nseed_everything(SEED)\nlogger = loggers.TensorBoardLogger('logs')\n\n# Prepare DataModule\ndata_module = NLIDataModule(dataset_df)\ndata_module.prepare_data()\ndata_module.setup('fit')\n\n# Prepare Model\nmodel = BertForSequenceClassification.from_pretrained(PRETRAINED_MODEL, \n                                                      num_labels=3,\n                                                      output_attentions=False,\n                                                      output_hidden_states=False)\nprint(model)\nmodel_module = NLIMultilingualBERT(model)\n\n# Set up Trainer\ntrainer = None\nif DEVICE == \"tpu\":\n    trainer = Trainer(tpu_cores=[TPU_CORES], logger=logger, max_epochs=MAX_EPOCHS, accelerator=\"dp\")\nelif DEVICE == \"gpu\":\n    trainer = Trainer(gpus=GPUS, logger=logger, max_epochs=MAX_EPOCHS)\nelse:\n    trainer = Trainer(logger=logger, max_epochs=MAX_EPOCHS)\n\ntrainer.fit(model_module, data_module.train_dataloader(), data_module.val_dataloader())","2826d3e6":"data_module.setup('test')\ntrainer.test(model_module, test_dataloaders=data_module.test_dataloader())","34973a6a":"prod_set = NLIDataset(prod_df, production=True)\nprod_dataloader = DataLoader(prod_set, batch_size=1, shuffle=False)\nmodel_module.freeze()  # eval\npred_lists = []\n\nfor inputs, row_id in prod_dataloader:\n    if DEVICE == \"tpu\":\n        device = xm.xla_device()\n    elif DEVICE == \"gpu\":\n        device = torch.device(\"cuda\") \n    else:\n        device = torch.device(\"cpu\") \n    \n    inputs[\"input_ids\"] = inputs[\"input_ids\"].to(device)\n    inputs[\"token_type_ids\"] = inputs[\"token_type_ids\"].to(device)\n    inputs[\"attention_mask\"] = inputs[\"attention_mask\"].to(device)\n    \n    predictions = model_module.forward(inputs)\n    pred_label = torch.argmax(predictions[0], dim=1).cpu().numpy()\n    pred_lists.append([row_id[0], pred_label[0]])\n\npred_pd = pd.DataFrame(pred_lists, columns=[\"id\", \"prediction\"])\npred_pd.to_csv('submission.csv', index=False)","1242b2cb":"# Define Global Variables\nNote: For some reason, the draft session with TPU can crash easily because of CPU memory allocation issues. In the situation, you may want to use different configurations when editing and testing your scripts in a draft session; for example, NUM_WORKERS could be 1, instead of 4.","64b1668b":"# PyTorch Lightning Resources\n* [PyTorch Lightning Github](https:\/\/github.com\/PyTorchLightning\/pytorch-lightning)\n* [PyTorch Lightning Documentation](https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/) \n* [PyTorch Lightning BERT Tutorial](https:\/\/colab.research.google.com\/github\/PytorchLightning\/pytorch-lightning\/blob\/master\/notebooks\/04-transformers-text-classification.ipynb)","a0916464":"# Exploratory Data Analysis","229c002d":"# Dataset Preparation\n* [Transformer Tokenizer Documentation](https:\/\/huggingface.co\/transformers\/main_classes\/tokenizer.html)\n* [PyTorch DataLoader and Dataset Documentation](https:\/\/pytorch.org\/docs\/stable\/data.html)\n* [LightningDataModule Documentation](https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/datamodules.html)","50794800":"# Environment Setup\n* [Kaggle TPU Support Documentation](https:\/\/www.kaggle.com\/docs\/tpu)\n* [PyTorch Lightning TPU Support Documentation](https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/tpu.html)","eeed1fdf":"# Model Training\n* [PyTorch Lightning Style Guide](https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/style_guide.html)\n* [LightningModule Documentation](https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/lightning_module.html)\n* [PyTorch Lightning Trainer Documentation](https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/trainer.html)","2c164c9e":"# Project Goals\n* Establish a basic pipeline to complete a modelling iteration, including dataset preparation, model training, model evaluation, and inference for submission\n* Use PyTorch Lightning to organize PyTorch code and access TPU\/GPU resources\n* Fine-tune a pre-trained bert-base-multilingual-cased model to be a baseline model","33bc69f8":"# Inference for Submission\n* [PyTorch Lightning Trainer Documentation - Deployment \/ Prediction](https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/trainer.html#deployment-prediction)","e033333e":"# Model Evaluation\n* [PyTorch Lightning Trainer Documentation - Testing](https:\/\/pytorch-lightning.readthedocs.io\/en\/latest\/trainer.html#testing)"}}