{"cell_type":{"96933704":"code","58f51016":"code","b3650ef5":"code","efb1e1e1":"code","aa186fd7":"code","95c2438d":"code","bf497d2d":"code","e1929ac1":"code","8c18d1ac":"code","95960812":"code","dc91e19e":"code","4f5eb550":"code","673f8ce9":"code","ef569c5f":"code","0afa9adc":"code","5a68c792":"code","5aa17f9a":"code","3b716243":"code","0638b09b":"code","6016e413":"code","d277164d":"code","c369d3b2":"code","9445398a":"code","7cd9237e":"code","fdfe7f9a":"code","4e3cc625":"code","ac4447ed":"code","92b68e72":"code","20c4723a":"code","1e32778a":"code","05168a6a":"code","01e629de":"code","af5c813f":"code","bfffea11":"code","45a3ded0":"code","98cbcbc5":"code","64a95e37":"markdown","2cb2c78e":"markdown","abb8e5a1":"markdown","2815a656":"markdown","a1deeafe":"markdown","f0e5a067":"markdown","88aa3305":"markdown","8d2b023c":"markdown","12276f88":"markdown","b20cbd9e":"markdown","9b81fc0c":"markdown","bae628b7":"markdown","66aecd7a":"markdown","c9fa3c85":"markdown","b2bebaa6":"markdown","027bbf49":"markdown","e4e1a808":"markdown","5ec74743":"markdown","1c8f59e2":"markdown","702efda3":"markdown"},"source":{"96933704":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","58f51016":"import pandas as pd\n%matplotlib inline\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport datetime\nfrom scipy import stats\nfrom scipy.stats import skew\nfrom scipy.stats import mode\nfrom scipy.optimize import minimize\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import svm\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier,GradientBoostingClassifier, VotingClassifier\nfrom sklearn.naive_bayes import GaussianNB","b3650ef5":"train = pd.read_csv(\"..\/input\/carinsurance\/carInsurance_train.csv\")\ntest = pd.read_csv(\"..\/input\/carinsurance\/carInsurance_test.csv\")","efb1e1e1":"train.describe()","aa186fd7":"train.shape","95c2438d":"train.head()","bf497d2d":"import matplotlib.pyplot as plt\nimport seaborn as sns","e1929ac1":"# First check out correlations among numeric features\n# Heatmap is a useful tool to get a quick understanding of which variables are important\ncolormap = plt.cm.viridis\ncor = train.corr()\ncor = cor.drop(['Id'],axis=1).drop(['Id'],axis=0)\nplt.figure(figsize=(12,12))\nsns.heatmap(cor,vmax=0.8,cmap=colormap,annot=True,fmt='.2f',square=True,annot_kws={'size':10},linecolor='black',linewidths=0.1)","8c18d1ac":"train.dtypes","95960812":"# Next, pair plot some important features\nimp_feats = ['CarInsurance','Age','Balance','HHInsurance', 'CarLoan','NoOfContacts','DaysPassed','PrevAttempts']\nsns.pairplot(train[imp_feats], palette = \"viridis\", size=2.5)\nplt.show()","dc91e19e":"# Take a further look at Age\nfacet = sns.FacetGrid(train, hue='CarInsurance',size=5,aspect=3,palette='seismic')\nfacet.map(plt.hist,'Age',bins=30,alpha=0.5,normed=True)\nfacet.set(xlim=(0,train.Age.max()+10))\nfacet.add_legend()","4f5eb550":"# Next check out categorical features\ncat_feats = train.select_dtypes(include=['object']).columns\nplt_feats = cat_feats[(cat_feats!= 'CallStart') & (cat_feats!='CallEnd')]\n\nfor feature in plt_feats:\n    plt.figure(figsize=(10,6))\n    sns.barplot(feature,'CarInsurance', data=train,palette='Set2')","673f8ce9":"# Check outliers\n# From the pairplot, we can see there is an outlier with extreme high balance. Drop that obs here.\ntrain[train['Balance']>80000]\ntrain = train.drop(train[train.index==1742].index)","ef569c5f":"# merge train and test data here in order to impute missing values all at once\nall=pd.concat([train,test],keys=('train','test'))\nall.drop(['CarInsurance','Id'],axis=1,inplace=True)\nprint(all.shape)","0afa9adc":"total = all.isnull().sum()\npct = total\/all.isnull().count()\nNAs = pd.concat([total,pct],axis=1,keys=('Total','Pct'))\nNAs[NAs.Total>0].sort_values(by='Total',ascending=False)","5a68c792":"all_df = all.copy()\n\n# Fill missing outcome as not in previous campaign\nall_df[all_df['DaysPassed']==-1].count()\nall_df.loc[all_df['DaysPassed']==-1,'Outcome']='NoPrev'\n\n# Fill missing communication with none \nall_df['Communication'].value_counts()\nall_df['Communication'].fillna('None',inplace=True)\n\n# Fill missing education with the most common education level by job type\nall_df['Education'].value_counts()\n\n# Create job-education level mode mapping\nedu_mode=[]\njob_types = all_df.Job.value_counts().index\nfor job in job_types:\n    mode = all_df[all_df.Job==job]['Education'].value_counts().nlargest(1).index\n    edu_mode = np.append(edu_mode,mode)\nedu_map=pd.Series(edu_mode,index=all_df.Job.value_counts().index)\n\n# Apply the mapping to missing eductaion obs\nfor j in job_types:\n    all_df.loc[(all_df['Education'].isnull()) & (all_df['Job']==j),'Education'] = edu_map.loc[edu_map.index==j][0]\nall_df['Education'].fillna('None',inplace=True)\n\n# Fill missing job with none\nall_df['Job'].fillna('None',inplace=True)\n\n# Double check if there is still any missing value\nall_df.isnull().sum().sum()","5aa17f9a":"# First simplify some client features\n\n# Create age group based on age bands\nall_df['AgeBand']=pd.cut(all_df['Age'],5)\nprint(all_df['AgeBand'].value_counts())\n\nall_df.loc[(all_df['Age']>=17) & (all_df['Age']<34),'AgeBin'] = 1\nall_df.loc[(all_df['Age']>=34) & (all_df['Age']<49),'AgeBin'] = 2\nall_df.loc[(all_df['Age']>=49) & (all_df['Age']<65),'AgeBin'] = 3\nall_df.loc[(all_df['Age']>=65) & (all_df['Age']<80),'AgeBin'] = 4\nall_df.loc[(all_df['Age']>=80) & (all_df['Age']<96),'AgeBin'] = 5\nall_df['AgeBin'] = all_df['AgeBin'].astype(int)\n\n# Create balance groups\nall_df['BalanceBand']=pd.cut(all_df['Balance'],5)\nprint(all_df['BalanceBand'].value_counts())\nall_df.loc[(all_df['Balance']>=-3200) & (all_df['Balance']<17237),'BalanceBin'] = 1\nall_df.loc[(all_df['Balance']>=17237) & (all_df['Balance']<37532),'BalanceBin'] = 2\nall_df.loc[(all_df['Balance']>=37532) & (all_df['Balance']<57827),'BalanceBin'] = 3\nall_df.loc[(all_df['Balance']>=57827) & (all_df['Balance']<78122),'BalanceBin'] = 4\nall_df.loc[(all_df['Balance']>=78122) & (all_df['Balance']<98418),'BalanceBin'] = 5\nall_df['BalanceBin'] = all_df['BalanceBin'].astype(int)\n\nall_df = all_df.drop(['AgeBand','BalanceBand','Age','Balance'],axis=1)\n\n# Convert education level to numeric \nall_df['Education'] = all_df['Education'].replace({'None':0,'primary':1,'secondary':2,'tertiary':3})","3b716243":"# Next create some new communication Features. This is the place feature engineering coming into play\n\n# Get call length\nall_df['CallEnd'] = pd.to_datetime(all_df['CallEnd'])\nall_df['CallStart'] = pd.to_datetime(all_df['CallStart'])\nall_df['CallLength'] = ((all_df['CallEnd'] - all_df['CallStart'])\/np.timedelta64(1,'m')).astype(float)\nall_df['CallLenBand']=pd.cut(all_df['CallLength'],5)\nprint(all_df['CallLenBand'].value_counts())\n\n# Create call length bins\nall_df.loc[(all_df['CallLength']>= 0) & (all_df['CallLength']<11),'CallLengthBin'] = 1\nall_df.loc[(all_df['CallLength']>=11) & (all_df['CallLength']<22),'CallLengthBin'] = 2\nall_df.loc[(all_df['CallLength']>=22) & (all_df['CallLength']<33),'CallLengthBin'] = 3\nall_df.loc[(all_df['CallLength']>=33) & (all_df['CallLength']<44),'CallLengthBin'] = 4\nall_df.loc[(all_df['CallLength']>=44) & (all_df['CallLength']<55),'CallLengthBin'] = 5\nall_df['CallLengthBin'] = all_df['CallLengthBin'].astype(int)\nall_df = all_df.drop('CallLenBand',axis=1)\n\n# Get call start hour\nall_df['CallStartHour'] = all_df['CallStart'].dt.hour\nprint(all_df[['CallStart','CallEnd','CallLength','CallStartHour']].head())\n\n# Get workday of last contact based on call day and month, assuming the year is 2016\nall_df['LastContactDate'] = all_df.apply(lambda x:datetime.datetime.strptime(\"%s %s %s\" %(2016,x['LastContactMonth'],x['LastContactDay']),\"%Y %b %d\"),axis=1)\nall_df['LastContactWkd'] = all_df['LastContactDate'].dt.weekday\nall_df['LastContactWkd'].value_counts()\nall_df['LastContactMon'] = all_df['LastContactDate'].dt.month\nall_df = all_df.drop('LastContactMonth',axis=1)\n\n# Get week of last contact\nall_df['LastContactWk'] = all_df['LastContactDate'].dt.week\n\n# Get num of week in a month. There might be easier ways to do this, I will keep exploring. \nMonWk = all_df.groupby(['LastContactWk','LastContactMon'])['Education'].count().reset_index()\nMonWk = MonWk.drop('Education',axis=1)\nMonWk['LastContactWkNum']=0\nfor m in range(1,13):\n    k=0\n    for i,row in MonWk.iterrows():\n        if row['LastContactMon']== m:\n            k=k+1\n            row['LastContactWkNum']=k\n            \ndef get_num_of_week(df):\n    for i,row in MonWk.iterrows():\n        if (df['LastContactWk']== row['LastContactWk']) & (df['LastContactMon']== row['LastContactMon']):\n            return row['LastContactWkNum']\n\nall_df['LastContactWkNum'] = all_df.apply(lambda x: get_num_of_week(x),axis=1)\nprint(all_df[['LastContactWkNum','LastContactWk','LastContactMon']].head(10))","0638b09b":"# Spilt numeric and categorical features\ncat_feats = all_df.select_dtypes(include=['object']).columns\nnum_feats = all_df.select_dtypes(include=['float64','int64']).columns\nnum_df = all_df[num_feats]\ncat_df = all_df[cat_feats]\nprint('There are %d numeric features and %d categorical features\\n' %(len(num_feats),len(cat_feats)))\nprint('Numeric features:\\n',num_feats.values)\nprint('Categorical features:\\n',cat_feats.values)","6016e413":"# One hot encoding\ncat_df = pd.get_dummies(cat_df)","d277164d":"# Merge all features\nall_data = pd.concat([num_df,cat_df],axis=1)","c369d3b2":"# Split train and test\nidx=pd.IndexSlice\ntrain_df=all_data.loc[idx[['train',],:]]\ntest_df=all_data.loc[idx[['test',],:]]\ntrain_label=train['CarInsurance']\nprint(train_df.shape)\nprint(len(train_label))\nprint(test_df.shape)","9445398a":"# Train test split\nx_train, x_test, y_train, y_test = train_test_split(train_df,train_label,test_size = 0.3,random_state=3)","7cd9237e":"# Create a cross validation function \ndef get_best_model(estimator, params_grid={}):\n    \n    model = GridSearchCV(estimator = estimator,param_grid = params_grid,cv=3, scoring=\"accuracy\", n_jobs= -1)\n    model.fit(x_train,y_train)\n    print('\\n--- Best Parameters -----------------------------')\n    print(model.best_params_)\n    print('\\n--- Best Model -----------------------------')\n    best_model = model.best_estimator_\n    print(best_model)\n    return best_model","fdfe7f9a":"# Create a model fitting function\ndef model_fit(model,feature_imp=True,cv=5):\n\n    # model fit   \n    clf = model.fit(x_train,y_train)\n    \n    # model prediction     \n    y_pred = clf.predict(x_test)\n    \n    # model report     \n    cm = confusion_matrix(y_test,y_pred)\n    plot_confusion_matrix(cm, classes=class_names, title='Confusion matrix')\n\n    print('\\n--- Train Set -----------------------------')\n    print('Accuracy: %.5f +\/- %.4f' % (np.mean(cross_val_score(clf,x_train,y_train,cv=cv)),np.std(cross_val_score(clf,x_train,y_train,cv=cv))))\n    print('AUC: %.5f +\/- %.4f' % (np.mean(cross_val_score(clf,x_train,y_train,cv=cv,scoring='roc_auc')),np.std(cross_val_score(clf,x_train,y_train,cv=cv,scoring='roc_auc'))))\n    print('\\n--- Validation Set -----------------------------')    \n    print('Accuracy: %.5f +\/- %.4f' % (np.mean(cross_val_score(clf,x_test,y_test,cv=cv)),np.std(cross_val_score(clf,x_test,y_test,cv=cv))))\n    print('AUC: %.5f +\/- %.4f' % (np.mean(cross_val_score(clf,x_test,y_test,cv=cv,scoring='roc_auc')),np.std(cross_val_score(clf,x_test,y_test,cv=cv,scoring='roc_auc'))))\n    print('-----------------------------------------------') \n\n    # feature importance \n    if feature_imp:\n        feat_imp = pd.Series(clf.feature_importances_,index=all_data.columns)\n        feat_imp = feat_imp.nlargest(15).sort_values()\n        plt.figure()\n        feat_imp.plot(kind=\"barh\",figsize=(6,8),title=\"Most Important Features\")","4e3cc625":"# The confusion matrix plotting function is from the sklearn documentation below:\n# http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\nimport itertools\ndef plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, cm[i, j],\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')\n\nclass_names = ['Success','Failure']","ac4447ed":"from sklearn.model_selection import GridSearchCV","92b68e72":"# Let's start with KNN. An accuracy of 0.76 is not very impressive. I will just take this as the model benchmark. \nknn = KNeighborsClassifier()\nparameters = {'n_neighbors':[5,6,7], \n              'p':[1,2],\n              'weights':['uniform','distance']}\nclf_knn = get_best_model(knn,parameters)\nmodel_fit(model=clf_knn, feature_imp=False)","20c4723a":"# As expected, Naive Bayes classifier doesn't perform well here. \n# There are multiple reasons. Some of the numeric features are not normally distributed, which is a strong assemption hold by Naive Bayes. \n# Also, features are definitely not independent.  \nclf_nb = GaussianNB()\nmodel_fit(model=clf_nb,feature_imp=False)","1e32778a":"# We're making progress here. Logistic regression performs better than KNN. \nlg = LogisticRegression(random_state=3)\nparameters = {'C':[0.8,0.9,1], \n              'penalty':['l1','l2']}\nclf_lg = get_best_model(lg,parameters)\nmodel_fit(model=clf_lg, feature_imp=False)","05168a6a":"# I did some manual parameter tuning here. This is the best model so far. \n# Based on the feature importance report, call length, last contact week, and previous success are strong predictors of cold call success\nrf = RandomForestClassifier(random_state=3)\nparameters={'n_estimators':[100],\n            'max_depth':[10],\n            'max_features':[13,14],\n            'min_samples_split':[11]}\nclf_rf= get_best_model(rf,parameters)\nmodel_fit(model=clf_rf, feature_imp=True)","01e629de":"# try a SVM RBF model \nsvc = svm.SVC(kernel='rbf', probability=True, random_state=3)\nparameters = {'gamma': [0.005,0.01,0.02],\n              'C': [0.5,1,5]}\nclf_svc = get_best_model(svc, parameters)\nmodel_fit(model=clf_svc,feature_imp=False)","af5c813f":"# Finally let's try out XBGoost. As expected, it outperforms all other algorithms. \n# Also, based on feature importances, some of the newly created features such as call start hour, last contact week and weekday \n# have been picked as top features. \n\nimport xgboost as xgb\nxgb = xgb.XGBClassifier()\nparameters={'n_estimators':[900,1000,1100],\n            'learning_rate':[0.01],\n            'max_depth':[8],\n            'min_child_weight':[1],\n            'subsample':[0.8],\n            'colsample_bytree':[0.3,0.4,0.5]}\nclf_xgb= get_best_model(xgb,parameters)\nmodel_fit(model=clf_xgb, feature_imp=True)","bfffea11":"# Compare model performance\nclfs= [clf_knn, clf_nb, clf_lg, clf_rf, clf_svc, clf_xgb]\nindex =['K-Nearest Neighbors','Naive Bayes','Logistic Regression','Random Forest','Support Vector Machines','XGBoost']\nscores=[]\nfor clf in clfs:\n    score = np.mean(cross_val_score(clf,x_test,y_test,cv=5,scoring = 'accuracy'))\n    scores = np.append(scores,score)\nmodels = pd.Series(scores,index=index)\nmodels.sort_values(ascending=False)","45a3ded0":"# XGBoost and Random Forest show different important features, implying that those models are capturing different aspects of the data\n# To get the final model, I ensembled different classifiers based on majority voting.\n# XGBoost and Random Forest are given larger weights due to their better performance. \n\nclf_vc = VotingClassifier(estimators=[('xgb', clf_xgb),                                       \n                                      ('rf', clf_rf),\n                                      ('lg', clf_lg), \n                                      ('svc', clf_svc)], \n                          voting='hard',\n                          weights=[4,4,1,1])\nclf_vc = clf_vc.fit(x_train, y_train)","98cbcbc5":"print('Final Model Accuracy: %.5f'%(accuracy_score(y_test, clf_vc.predict(x_test))))","64a95e37":"## Next Steps\n\nHere are some thoughts on steps for further improvements:\n* Do more feature engineering, including exploring interaction and polynomial terms\n* Visualize decision boundaries for some classifiers\n* Introduce more base models for learning\n* Try different ensembling approaches\n","2cb2c78e":"* Age: It's interesting to see that seniors are more likely to buy car insurance.\n* Balance: For balance, the data point at the upper right corner might be an outlier\n* HHInsurance: Households insured are less likely to buy car insurance\n* CarLoan: People with car loan are less likely to buy\n* NoOfContacts: Too many contacts causes customer attrition\n* DaysPassed: It looks like the more day passed since the last contact, the better\n* PrevAttempts: Also, more previous attempts, less likely to buy. There is a potential outlier here","abb8e5a1":"## Assembling Final Datasets","2815a656":"## Handling Missing Data","a1deeafe":"\n\nIt looks like young people(<=30 years) and seniors are more likely to buy car insurance from this bank\n","f0e5a067":"## Naive Bayes Classifier","88aa3305":"## Ensemble Voting","8d2b023c":"* Job: Student are most likely to buy insurance, followed by retired and unemployed folks.This is aligned with the age distribution. There might be some promotion targeting students?\n* Marital status: Married people are least likely to buy car insurance. Opportunities for developing family insurance business\n* Education: People with higher education are more likely to buy\n* Communication: No big difference between cellular and telephone\n* Outcome in previous campaign: Success in previous marketing campaign is largely associated with success in this campaign\n* Contact Month: Mar, Sep, Oct, and Dec are the hot months. It might be associated with school season?","12276f88":"\n\nThe two previous campaign features are good to go, no cleaning needed. I also tried to add some interactions and polynomial features, but none of them seems helpful. I am planning to explore more on this.\n","b20cbd9e":"## Random Forest","9b81fc0c":"## k-Nearest Neighbors (KNN)","bae628b7":"* There are three types of features:\n* Client features: Age, Job, Marital, Education, Default, Balance, HHInsurance, CarLoan\n* Communication features: LastContactDay, LastContactMonth, CallStart, CallEnd, Communication, NoOfContacts, DaysPassed\n* Previous campaign features: PrevAttempts, Outcome","66aecd7a":"## Modeling","c9fa3c85":"## Logistic Regression","b2bebaa6":"## XGBoost","027bbf49":"## Model Evaluation","e4e1a808":"## Data Exploration & Visualization","5ec74743":"Features are fairly independent, except DaysPassed and PreAttempts. Cold call success is positively correlated with PreAttemps,DaysPassed,Age and Balance, and negatively correlated with default, HHInsurance, CarLoan, LastContactDay and NoOfContacts.","1c8f59e2":"## Feature Engineering","702efda3":"## Support Vector Machines"}}