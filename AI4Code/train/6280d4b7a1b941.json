{"cell_type":{"fe09945c":"code","b4e923ca":"code","90bcc477":"code","29863f67":"code","ebdb5c74":"code","0e895d8b":"code","8c278370":"code","4ad84209":"code","d5b44821":"code","599da8ff":"code","1ad2acc3":"code","7950dea8":"code","443c6dda":"code","deefd479":"code","8a6b488d":"code","c7e2a013":"markdown"},"source":{"fe09945c":"# !pip install torchvision\nimport torch\nimport torchvision\nfrom torch import nn\nimport torch.nn.functional as func\nfrom torch.utils.data import DataLoader\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nimport matplotlib.pyplot as plt\nimport numpy as np","b4e923ca":"# Train transformation\ntrain_transform = transforms.Compose([\n    transforms.RandomHorizontalFlip(p=0.5),\n    transforms.ToTensor(),\n    transforms.Normalize((0.5), (0.5)),\n    transforms.RandomCrop(32, padding=4)  \n])\n# Test transformation\ntest_transform = transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Normalize((0.5), (0.5))\n])\n\n# Download training data from open datasets.\ntraining_data = datasets.CIFAR10(\n    root=\"data\",\n    train=True,\n    download=True,\n    transform=train_transform,\n)\n\n# Download test data from open datasets.\ntest_data = datasets.CIFAR10(\n    root=\"data\",\n    train=False,\n    download=True,\n    transform=test_transform,\n)","90bcc477":"# Training batch size\nbatch_size = 256\n\n# Create data loaders.\ntrain_dataloader = DataLoader(training_data, batch_size=batch_size)\ntest_dataloader = DataLoader(test_data, batch_size=batch_size)\n\ndef imshow(img):\n    img = img \/ 2 + 0.5     # unnormalize\n    npimg = img.numpy()\n    plt.figure(figsize=(15,15))\n    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n    plt.show()\n\n# Each batch tensor shape\nfor X, y in test_dataloader:\n    print(\"Shape of X [N, C, H, W]: \", X.shape)\n    print(\"Shape of y: \", y.shape, y.dtype)\n    \n    imshow(torchvision.utils.make_grid(X, nrow=20))\n    \n    break","29863f67":"# Get cpu or gpu device for training.\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(\"Using {} device\".format(device))\n\n# Define model\nclass NeuralNetwork(nn.Module):\n    def __init__(self):\n        \n        super(NeuralNetwork, self).__init__()\n        # self.flatten = nn.Flatten()\n        self.linear_relu_stack = nn.Sequential(\n            nn.Conv2d(3, 64, kernel_size=3, padding=1), #32\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(64, 64, kernel_size=3, padding=1), #32\n            nn.ReLU(),\n            nn.BatchNorm2d(64),\n            nn.Conv2d(64, 128, kernel_size=3, padding=1), #32\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.MaxPool2d(2, 2),#16\n            nn.Dropout(0.2),\n            \n            nn.Conv2d(128, 128, kernel_size=3, padding=1), #16\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.Conv2d(128, 128, kernel_size=3, padding=1), #16\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.Conv2d(128, 256, kernel_size=3, padding=1), #8\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.MaxPool2d(2, 2),#8\n            nn.Dropout(0.3),\n            \n            nn.Conv2d(256, 256, kernel_size=3, padding=1), #8\n            nn.ReLU(),\n            nn.BatchNorm2d(128),\n            nn.Conv2d(256, 512, kernel_size=3, padding=1), #8\n            nn.ReLU(),\n            nn.BatchNorm2d(512),\n            nn.MaxPool2d(2, 2),#4\n            nn.Dropout(0.4),\n\n            \n            nn.Conv2d(512, 512, kernel_size=3, padding=1), #16\n            nn.ReLU(),\n            nn.BatchNorm2d(512),\n            nn.Conv2d(512, 512, kernel_size=3, padding=1), #16\n            nn.ReLU(),\n            nn.BatchNorm2d(512),\n            nn.MaxPool2d(2, 2),#2\n                \n            nn.Flatten(),#128 x 4 x 4\n            nn.Linear(512 * 2 * 2, 10)\n        )\n\n    def forward(self, x):\n        # print(x.shape)\n        # x = self.flatten(x)\n        # print(x.shape)\n        logits = self.linear_relu_stack(x)\n        return logits\n\ndef conv_block(in_dim, out_dim, pool=False, drop = 0):\n    block = [nn.Conv2d(in_dim, out_dim, kernel_size=3, padding=1), #32\n            nn.ReLU(),\n            nn.BatchNorm2d(out_dim),\n    ]\n    if pool: block.append(nn.MaxPool2d(2,2))\n    if drop > 0: block.append(nn.Dropout(drop))\n    return nn.Sequential(*block)\n    \n# Define model\nclass ResNet(nn.Module):\n    def __init__(self):\n        \n        super(ResNet, self).__init__()\n        \n        self.conv1 = conv_block(3, 64)\n        self.conv2 = conv_block(64,64)\n        self.conv3 = conv_block(64,128, True, 0.2)\n        \n        self.conv4 = conv_block(128,128)\n        self.conv5 = conv_block(128,128)\n        self.conv6 = conv_block(128,256, True, 0.3)\n        \n        self.conv7 = conv_block(256,256)\n        self.conv8 = conv_block(256,256)\n        self.conv9 = conv_block(256,512, True, 0.4)\n        \n        self.conv10 = conv_block(512,512)\n        self.conv11 = conv_block(512,512)\n        self.conv12 = conv_block(512,512, True, 0.4)\n        \n        self.final = nn.Sequential(nn.Flatten(),\n                                  nn.Linear(512 * 2 * 2, 10))\n\n    def forward(self, x):\n        out_1 = self.conv1(x)\n        out_2 = self.conv2(out_1)\n        out_3 = self.conv3(out_2)\n        \n        out_4 = self.conv4(out_3)\n        out_5 = self.conv5(out_4)+out_3\n        out_6 = self.conv6(out_5)\n        \n        out_7 = self.conv7(out_6)\n        out_8 = self.conv8(out_7)+out_6\n        out_9 = self.conv9(out_8)\n        \n        out_10 = self.conv10(out_9)\n        out_11 = self.conv11(out_10)+out_9\n        out_12 = self.conv12(out_11)\n        \n        out_13 = self.final(out_12)\n        return out_13\n    \n    \n    \nmodel = ResNet().to(device)\n# for X, y in test_dataloader:\n#     output = model(X)\n#     break\nprint(model)","ebdb5c74":"# Loss function\nloss_fn = nn.CrossEntropyLoss()\n\n# SGD Optimizer\noptimizer = torch.optim.SGD(model.parameters(), lr=0.01, weight_decay=1e-4, momentum = 0.9)\n\n# adam optimizer\noptimizer_adam = torch.optim.Adam","0e895d8b":"\n# Training function\ndef train(dataloader, model, loss_fn, optimizer):\n    size = len(dataloader.dataset)\n    \n    # Turn on training mode\n    model.train()\n    train_loss, correct = 0, 0\n    for batch, (X, y) in enumerate(dataloader):\n        X, y = X.to(device), y.to(device)\n\n        # Compute prediction error\n        pred = model(X)\n        loss = loss_fn(pred, y)\n\n        # Backpropagation\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # record loss\n        train_loss += loss.item()\n        correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n    \n    train_loss \/= len(dataloader)\n    correct \/= size\n    \n    print(f\" Train accuracy: {(100*correct):>0.1f}%, Avg loss: {train_loss:>8f}\")\n    return train_loss,correct","8c278370":"# Test function\ndef test(dataloader, model, loss_fn):\n    size = len(dataloader.dataset)\n    num_batches = len(dataloader)\n    \n    # Turn on evalution mode\n    model.eval()\n    test_loss, correct = 0, 0\n    \n    # Turn off gradient descent\n    with torch.no_grad():\n        for X, y in dataloader:\n            X, y = X.to(device), y.to(device)\n            pred = model(X)\n            \n            # record loss\n            test_loss += loss_fn(pred, y).item()\n            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n            \n    test_loss \/= num_batches\n    correct \/= size\n    \n    print(f\" Test accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f}\")\n    return test_loss,correct","4ad84209":"# Total training epochs\nepochs = 100\ntest_losses = []\ntrain_losses = []\ntest_accuracy = []\ntrain_accuracy = []\nfor t in range(epochs):\n    print('\\n', \"=\" * 15, \"Epoch\", t + 1, \"=\" * 15)\n    train_loss, train_acc = train(train_dataloader, model, loss_fn, optimizer)\n    test_loss, test_acc = test(test_dataloader, model, loss_fn)\n    test_losses.append(test_loss)\n    test_accuracy.append(test_acc)\n    train_losses.append(train_loss)\n    train_accuracy.append(train_acc)\nprint(\" Done!\")","d5b44821":"plt.figure(figsize=(10,5))\nplt.title(\"Training and Testing Loss\")\nplt.plot(test_losses,label=\"test\")\nplt.plot(train_losses,label=\"train\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","599da8ff":"\nplt.figure(figsize=(10,5))\nplt.title(\"Training and Testing accuracy\")\nplt.plot(test_accuracy[:],label=\"test\")\nplt.plot(train_accuracy[:],label=\"train\")\nplt.xlabel(\"iterations\")\nplt.ylabel(\"Loss\")\nplt.legend()\nplt.show()","1ad2acc3":"max(test_accuracy)","7950dea8":"test_accuracy.index(max(test_accuracy))","443c6dda":"# Saving model weights\n# torch.save(model)\ntorch.save(model.state_dict(), \"resnet.pth\")\nprint(\" Saved PyTorch Model State to model.pth\")","deefd479":"# Build the network\nmodel = NeuralNetwork()\n\n# Load trained weights\n# .pth pt pkl pth.tar\nmodel.load_state_dict(torch.load(\"model.pth\"))","8a6b488d":"# 10 Classes\nclasses = [\n    \"Airplane\",\n    \"Automobile\",\n    \"Bird\",\n    \"Cat\",\n    \"Deer\",\n    \"Dog\",\n    \"Frog\",\n    \"Horse\",\n    \"Ship\",\n    \"Truck\",\n]\n\n# Evaluation mode\nmodel.eval()\n\n# Get one sample\nx, y = torch.tensor(test_data.data[0]).float().unsqueeze(0), test_data.targets[0]\nprint(x.shape)\n# x, y = test_data.data[0], test_data.targets[1]\n\n# Turn off gradient descent\nwith torch.no_grad():\n    pred = model(x)\n    predicted, actual = classes[pred[0].argmax(0)], classes[y]\n    print(f' Predicted: \"{predicted}\", Actual: \"{actual}\"')","c7e2a013":"![avatar](pytorch.png)\n\nOfficial website: [https:\/\/pytorch.org\/tutorials\/](https:\/\/pytorch.org\/tutorials\/)"}}