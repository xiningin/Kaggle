{"cell_type":{"59269e77":"code","8170dbcd":"code","b1ca4cf1":"code","e14a5a6a":"code","619eafb8":"code","4537114a":"code","69c0e958":"code","053160d1":"code","0b9e9f23":"code","a5b827c1":"code","294ab2bb":"code","1e2cb06a":"code","9db4118c":"code","727e5aa8":"code","6afe6715":"code","955b8f6d":"code","dd4ea931":"code","cb7f5d35":"code","c125c4f7":"code","9eb4d76c":"code","ac66ce13":"code","cc47cc5a":"code","40dd196b":"code","dc95f553":"code","b7e99c99":"code","e10f3deb":"code","02f72b21":"code","c8b2803e":"code","0b429a07":"code","cccebb5f":"code","a61cef1d":"code","7ce9069e":"code","876d882f":"code","1c500efe":"code","10dad935":"code","d1535309":"code","ec091c3c":"code","1ab2b954":"code","ea75cc96":"code","230648e1":"code","ced89037":"code","9f68324e":"markdown","6443f9a4":"markdown","4dfc4082":"markdown","a10b9d2f":"markdown","5905674a":"markdown","127a1703":"markdown","66a24d53":"markdown","6c4e678c":"markdown","7de0c900":"markdown","3ae6a604":"markdown","cc5c0061":"markdown","4e57006a":"markdown"},"source":{"59269e77":"import numpy as np\nimport os\nimport pandas as pd\nimport random\nimport torch\nimport torch.optim as optim\n\nfrom torch import nn\nfrom torch.utils.data import Dataset, DataLoader\nfrom tqdm import tqdm\nfrom transformers import BertTokenizer, BertModel\n\nSEED = 1234\n\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)","8170dbcd":"INPUT_FOLDER = \"\/kaggle\/input\/scl-2021-ds\"\ndf_train = pd.read_csv(os.path.join(INPUT_FOLDER, \"train.csv\"))\ndf_test = pd.read_csv(os.path.join(INPUT_FOLDER, \"test.csv\"))","b1ca4cf1":"df_train[['POI', 'street']] = df_train['POI\/street'].str.split('\/', expand=True)\ndf_train['POI'] = df_train['POI'].str.lower()\ndf_train['street'] = df_train['street'].str.lower()\ndf_train","e14a5a6a":"check_POI = df_train.apply(lambda row: row[\"POI\"] in row[\"raw_address\"], axis=1)\ncheck_street = df_train.apply(lambda row: row[\"street\"] in row[\"raw_address\"], axis=1)","619eafb8":"df_train[~check_POI].head(100)","4537114a":"df_train[~check_street]","69c0e958":"df_train_clean = df_train[check_POI & check_street]\ndf_train_clean.replace('', np.nan, inplace=True)\ndf_train_clean.dropna(inplace=True)\ndf_train_clean","053160d1":"print(len(df_train_clean.POI.unique()))\nprint(len(df_train_clean.street.unique()))","0b9e9f23":"model_name='cahya\/bert-base-indonesian-522M'\ntokenizer = BertTokenizer.from_pretrained(model_name, do_lower_case=True)\nmodel = BertModel.from_pretrained(model_name)\ntext = \"Silakan diganti dengan text apa saja.\"\nencoded_input = tokenizer(text, return_tensors='pt')","a5b827c1":"tokenizer.convert_tokens_to_ids([\"[UNK]\", \"[CLS]\", \"ksduhvigawrhu\", \"[SEP]\", \".\"])","294ab2bb":"encoded_input","1e2cb06a":"model","9db4118c":"# https:\/\/colab.research.google.com\/github\/bentrevett\/pytorch-pos-tagging\/blob\/master\/2%20-%20Fine-tuning%20Pretrained%20Transformers%20for%20PoS%20Tagging.ipynb#scrollTo=CJjgWsdol8fI\nclass BERTTagger(nn.Module):\n    def __init__(self,\n                 bert,\n                 output_dim, \n                 dropout):\n        \n        super().__init__()\n        \n        self.bert = bert\n        \n        embedding_dim = bert.config.to_dict()['hidden_size']\n        \n        self.fc = nn.Linear(embedding_dim, output_dim)\n        \n        self.dropout = nn.Dropout(dropout)\n        \n    def forward(self, text):\n  \n        #text = [sent len, batch size]\n    \n        text = text.permute(1, 0)\n        \n        #text = [batch size, sent len]\n        \n        embedded = self.dropout(self.bert(text)[0])\n        \n        #embedded = [batch size, seq len, emb dim]\n                \n        embedded = embedded.permute(1, 0, 2)\n                    \n        #embedded = [sent len, batch size, emb dim]\n        \n        predictions = self.fc(self.dropout(embedded))\n        \n        #predictions = [sent len, batch size, output dim]\n        \n        return predictions","727e5aa8":"bert_poi = BERTTagger(model, 2, 0.2)\nbert_street = BERTTagger(model, 2, 0.2)\nbert_poi","6afe6715":"for param in bert_poi.bert.parameters():\n    param.requires_grad = False\n    \nfor param in bert_street.bert.parameters():\n    param.requires_grad = False","955b8f6d":"encoded_input[\"input_ids\"]","dd4ea931":"poi_output = bert_poi(encoded_input[\"input_ids\"]).max(axis=2)[1][:, 1:-1]\nstreet_output = bert_street(encoded_input[\"input_ids\"]).max(axis=2)[1][:, 1:-1]\npoi_output","cb7f5d35":"token_ids = torch.masked_select(encoded_input[\"input_ids\"][:, 1:-1], poi_output.bool())\ntokens = tokenizer.convert_ids_to_tokens(token_ids)\npoi_sentence = tokenizer.convert_tokens_to_string(tokens)\n\ntoken_ids = torch.masked_select(encoded_input[\"input_ids\"][:, 1:-1], street_output.bool())\ntokens = tokenizer.convert_ids_to_tokens(token_ids)\nstreet_sentence = tokenizer.convert_tokens_to_string(tokens)\n\nprint(poi_sentence + \"\/\" + street_sentence)","c125c4f7":"text = [\"aaa bbb\", \"cccaaa asdas ddd\"]\nencoded_input = tokenizer(text, return_tensors='pt', padding=True)\nencoded_input","9eb4d76c":"text = [\"[UNK] [SEP] [MASK] [PAD] [CLS]\", \"cccaaa asdas ddd\"]\nencoded_input = tokenizer(text, return_tensors='pt', padding=True)\nencoded_input","ac66ce13":"bert_street(encoded_input[\"input_ids\"]).max(dim=2)[1][:, 1:-1]","cc47cc5a":"df_train_clean","40dd196b":"def subfinder(mylist, pattern):\n    \n    if len(mylist) == 0 or len(pattern) == 0:\n        return -1\n    \n    start_idx = 0\n    j = 0\n    \n    for i, _ in enumerate(mylist):\n\n        if mylist[i] == pattern[j]:\n            \n            if j == 0:\n                start_idx = i\n            \n            j += 1\n            \n        else:\n            \n            j = 0\n        \n        if j == len(pattern):\n            return start_idx\n        \n    return -1\n    ","dc95f553":"print(tokenizer([\"siung\"]))\nprint(tokenizer.convert_tokens_to_ids([\"siung\"]))\nprint(tokenizer.convert_ids_to_tokens([   3, 2139, 1563,    1]))","b7e99c99":"def get_start_positions(row):\n    \n    raw_address_token_ids = tokenizer(row.raw_address, return_tensors='pt')['input_ids'][:, 1:-1].tolist()[0]\n    poi_token_ids = tokenizer(row.POI, return_tensors='pt')['input_ids'][:, 1:-1].tolist()[0]\n    street_token_ids = tokenizer(row.street, return_tensors='pt')['input_ids'][:, 1:-1].tolist()[0]\n    poi_len = len(poi_token_ids)\n    street_len = len(street_token_ids)\n    start_poi_position = subfinder(raw_address_token_ids, poi_token_ids)\n    start_street_position = subfinder(raw_address_token_ids, street_token_ids)\n    \n    return start_poi_position, poi_len, start_street_position, street_len\n    \ndf_train_clean['start_POI_position'], df_train_clean['poi_len'], df_train_clean['start_street_position'], df_train_clean['street_len'] = zip(*df_train_clean.apply(get_start_positions, axis=1))\ndf_train_clean","e10f3deb":"train_encodings = tokenizer(list(df_train_clean.raw_address.values), return_tensors='pt', padding=True)","02f72b21":"class AddressExtractionDataset(Dataset):\n    \"\"\"Face Landmarks dataset.\"\"\"\n\n    def __init__(self, encoding, df, target_type):\n        \n        self.encoding = encoding\n        self.df = df\n        self.target_type = target_type\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        \n        \n        input_ids = self.encoding['input_ids'][idx]\n        if self.target_type == \"POI\":\n            target = \"POI\"\n            start_position = self.df['start_POI_position'].iloc[idx]\n            target_len = self.df['poi_len'].iloc[idx]\n            \n            #target_text = self.df['POI']\n        elif self.target_type == \"street\":\n            target = \"street\"\n            start_position = self.df['start_street_position'].iloc[idx]\n            target_len = self.df['street_len'].iloc[idx]\n            \n            #target_text = self.df['street']\n        elif self.target_type == \"test\":\n            return input_ids\n        else:\n            raise NotImplementedError\n        \n        labels = torch.zeros_like(input_ids)\n        labels[start_position + 1:start_position + 1 + target_len] = 1\n\n        return input_ids, labels","c8b2803e":"aed_poi = AddressExtractionDataset(train_encodings, df_train_clean, \"POI\")\naed_street = AddressExtractionDataset(train_encodings, df_train_clean, \"street\")\nnum_example = 50\n\ncount = 0\nfor input_ids, labels in aed_poi:\n    count += 1\n\n    #print(input_ids, labels)\n    print(tokenizer.decode(input_ids.masked_select((labels > 0).bool())))\n    if count > num_example:\n        break\n        \nprint(\"----------------\")        \n        \ncount = 0\nfor input_ids, labels in aed_street:\n    count += 1\n\n    #print(input_ids, labels)\n    print(tokenizer.decode(input_ids.masked_select((labels > 0).bool())))\n    if count > num_example:\n        break","0b429a07":"df_train_clean.head(num_example)","cccebb5f":"dataloader_poi = DataLoader(aed_poi, batch_size=16,\n                        shuffle=True, num_workers=0)\ndataloader_street = DataLoader(aed_street, batch_size=16,\n                        shuffle=True, num_workers=0)","a61cef1d":"for inputs, labels in dataloader_poi:\n    print(inputs, labels)\n    break","7ce9069e":"LEARNING_RATE = 5e-5\noptimizer_poi = optim.Adam(bert_poi.parameters(), lr = LEARNING_RATE)\noptimizer_street = optim.Adam(bert_street.parameters(), lr = LEARNING_RATE)\ncriterion = nn.CrossEntropyLoss(ignore_index = 2)","876d882f":"def train(model, dataloader, optimizer, criterion):\n    \n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n    runnning_loss = 0\n    interval = 100\n    epoch_loss = 0\n    #epoch_acc = 0\n    \n    model.train()\n    \n    for idx, (input_ids, labels) in enumerate(tqdm(dataloader, position=0, leave=True)):\n                  \n        input_ids = input_ids.to(device)\n        labels = labels.to(device)\n            \n        optimizer.zero_grad()   \n        \n        predictions = model(input_ids)\n        predictions = predictions.view(-1, predictions.shape[-1])\n        labels = labels.view(-1)\n        \n        loss = criterion(predictions, labels)\n        loss.backward()\n        \n        optimizer.step()\n        \n        runnning_loss += loss.item()\n        \n        if idx % interval == interval - 1:\n            print(\"running_loss: {}\".format(runnning_loss \/ interval))\n            runnning_loss = 0\n        \n        epoch_loss += loss.item()\n        #epoch_acc += acc.item()\n        \n    return epoch_loss \/ len(dataloader)#, epoch_acc \/ len(dataloader)","1c500efe":"NUM_EPOCHS = 3\n\npoi_losses = []\nstreet_losses = []\n\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nbert_poi = bert_poi.to(device)\nbert_street = bert_street.to(device)\n\nfor epoch in range(NUM_EPOCHS):\n    poi_loss = train(bert_poi, dataloader_poi, optimizer_poi, criterion)\n    street_loss = train(bert_street, dataloader_street, optimizer_street, criterion)\n    \n    poi_losses.append(poi_loss)\n    street_losses.append(street_loss)\n    \n    torch.save(bert_poi.state_dict(), \"bert_poi_{}.pth\".format(str(epoch)))\n    torch.save(bert_street.state_dict(), \"bert_street_{}.pth\".format(str(epoch)))","10dad935":"df_test","d1535309":"train_encodings['input_ids'].shape[1]","ec091c3c":"test_encodings = tokenizer(list(df_test.raw_address.values), return_tensors='pt', padding=True, truncation=True, max_length=train_encodings['input_ids'].shape[1])","1ab2b954":"test_encodings","ea75cc96":"aed_test = AddressExtractionDataset(test_encodings, df_test, 'test')\ndataloader_test = DataLoader(aed_test, batch_size=1, num_workers=4)","230648e1":"def clean_answer(answer):\n    answer = answer.replace(\"[SEP]\", \"\")\n    answer = answer.replace(\"[UNK]\", \"\")\n    answer = answer.replace(\"[PAD]\", \"\")\n    answer = answer.replace(\"  \", \" \")\n    return answer.strip()","ced89037":"\n\nwith open(\"submission.csv\", \"w\") as f_csv:\n    \n    row_format = '{},\"{}\"\\n'\n    header = row_format.format(\"id\", \"POI\/street\")\n    f_csv.write(header)\n    \n    count = 0\n    for idx, input_ids in enumerate(tqdm(dataloader_test, leave=True, position=0)):\n\n        input_ids = input_ids.to(device)\n        \n        count += 1\n\n        poi_mask = bert_poi(input_ids).max(dim=2)[1][:, 1:-1][0]\n        poi_ids = input_ids[:,1:-1].masked_select(poi_mask.bool())\n\n        street_mask = bert_street(input_ids).max(dim=2)[1][:, 1:-1][0]\n        street_ids = input_ids[:,1:-1].masked_select(street_mask.bool())\n\n        poi = clean_answer(tokenizer.decode(poi_ids))\n        street = clean_answer(tokenizer.decode(street_ids))\n\n        row = row_format.format(idx, poi + \"\/\" + street)\n        f_csv.write(row)\n        \n        #if count >= 3:\n        #    break","9f68324e":"# Bert Model\n* load pre-trained model\n* modify model for this task\n* input\n* label\n* output","6443f9a4":"## load pre-trained model","4dfc4082":"# Goal \nFrom raw address ->\n* POI name \n* street name","a10b9d2f":"## modify bert model for this task","5905674a":"# Load csv","127a1703":"# prediction","66a24d53":"# Training","6c4e678c":"# Import modules","7de0c900":"## input","3ae6a604":"# Pre-processing\n* \u7cfe\u932f\n    * \u62fc\u5b57\u932f\u8aa4\n    * street name\u662f\u5426\u5b58\u5728\n    * POI \u662f\u5426\u5b58\u5728\n    * POI\/steet\u662f\u5426\u6709\u5728raw_address\u88e1\u9762","cc5c0061":"## output","4e57006a":"## label"}}