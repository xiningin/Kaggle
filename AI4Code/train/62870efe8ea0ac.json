{"cell_type":{"6e0919f3":"code","bc87bc43":"code","dce095e6":"code","aaf82906":"code","916b11ae":"code","4782c200":"code","a0fb27dd":"code","4171ca23":"code","f1d4d79c":"code","88c9d7aa":"code","a8b36f61":"code","8d32b862":"code","3df3308b":"code","04b9b0a9":"code","2770c983":"code","633cad86":"code","0d030008":"code","eb31210d":"code","fbc2ba08":"markdown","567f591b":"markdown","b6d3dba3":"markdown","07998096":"markdown","ee31a92f":"markdown","48f3ba4e":"markdown","9804a947":"markdown","e866416c":"markdown","58386a2a":"markdown","5d02238b":"markdown"},"source":{"6e0919f3":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\n\nfrom imblearn.under_sampling import RandomUnderSampler\n\nimport tensorflow as tf\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")","bc87bc43":"train = pd.read_csv(\"..\/input\/quora-insincere-questions-classification\/train.csv\")\ntest = pd.read_csv(\"..\/input\/quora-insincere-questions-classification\/test.csv\")\nss = pd.read_csv(\"..\/input\/quora-insincere-questions-classification\/sample_submission.csv\")\ntrain.question_text = train.question_text.astype(\"str\")\ntrain.info()","dce095e6":"train.head()","aaf82906":"train.target.value_counts().plot(kind='bar', title=\"Count of target label\")","916b11ae":"print(f\"Maximum sequence length: {train.question_text.apply(len).max()}\")\nprint(f\"Most frequent sequence length: {train.question_text.apply(len).mode()[0]}\")\nprint(f\"Mean sequence length: {train.question_text.apply(len).mean()}\")","4782c200":"train.question_text.apply(len).plot(kind='hist', bins=50, title=\"Histogram of question length\")","a0fb27dd":"#set maxlen based on right edge of question length histogram \nmaxlen = 250\n\n#arbitrary choice of top 25000 words\nmax_features=25000\n\ntokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=max_features, \n                                                  oov_token=\"<oov>\", \n                                                  filters='\"!\"#$%&()*+,-.\/:;<=>?@[\\\\]^_`{|}~\\t\\n\"',\n                                                  split=\" \")\n\ntokenizer.fit_on_texts(train.question_text)\ntrain_df = tokenizer.texts_to_sequences(train.question_text)\ntrain_df = tf.keras.preprocessing.sequence.pad_sequences(train_df, maxlen=maxlen, padding=\"post\", truncating='post')","4171ca23":"#create a small 100k training sample and 5k validation sample\ntrain_len = int(train_df.shape[0] * 0.1)\n\nX_train = train_df[:train_len]\nY_train = train.target[:train_len]\n\nX_val = train_df[train_len:train_len+5000]\nY_val = train.target[train_len:train_len+5000]","f1d4d79c":"#balance data on train set\nrus = RandomUnderSampler(random_state=42)\nX_balance, Y_balance = rus.fit_resample(X_train, Y_train.values)","88c9d7aa":"EMBEDDING_FILE = '..\/input\/glove-global-vectors-for-word-representation\/glove.6B.50d.txt'\n\nembeddings_index = dict()\n\n#open the embedding file\nf = open(EMBEDDING_FILE)\n\n#load values into the embeddings_index dictionary filtering for words not in corpus\nfor line in f:\n    values=line.split()\n    word = values[0]\n    coefs = np.asarray(values[1:], dtype='float')\n    embeddings_index[word] = coefs\nf.close()\n\n# get the mean and standard deviation of the embeddings weights\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\n#add the missing words to the embeddings and generate the random values\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix[i] = embedding_vector","a8b36f61":"print(f\"Maximum sequence length: {maxlen}\")\nprint(f\"Number of words in the embedding: {max_features}\")\nprint(f\"Number of words in the vocabulary: {len(tokenizer.word_index)}\")\nprint(f\"Number of features per embedding: {embed_size}\")","8d32b862":"def sequence_model(maxlen,\n                   max_features,\n                   embed_size,\n                   embedding_matrix,\n                   metrics):\n    \n    tf.keras.backend.clear_session()\n    \n    if embedding_matrix is not None:\n        embeddings = [embedding_matrix]\n        output_dim = embedding_matrix.shape[1]\n    else:\n        embeddings = None\n    \n    model = tf.keras.models.Sequential([\n        tf.keras.layers.Input(shape=maxlen),\n        tf.keras.layers.Embedding(max_features, \n                                  embed_size, \n                                  weights=[embedding_matrix], \n                                  trainable=True),\n        tf.keras.layers.Bidirectional(\n            tf.keras.layers.LSTM(27, activation='relu', return_sequences = True)),\n        tf.keras.layers.GlobalMaxPooling1D(),\n        tf.keras.layers.Dense(16, activation='relu'),\n        tf.keras.layers.Dense(1, activation='sigmoid')\n    ])\n    \n    optimizer = tf.keras.optimizers.Adam(lr=0.00008, epsilon=0.01)\n    \n    model.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=metrics)\n    \n    return model","3df3308b":"METRICS = [\n    tf.keras.metrics.AUC(name='roc-auc'),\n    tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n    tf.keras.metrics.Precision(name='precision'),\n    tf.keras.metrics.Recall(name=\"recall\")\n          ]\nmodel = sequence_model(maxlen, max_features, embed_size, embedding_matrix, METRICS)\nhist = model.fit(X_balance, Y_balance, epochs=50, batch_size=64, validation_data=(X_val, Y_val))","04b9b0a9":"def plot(history, *metrics):\n    \n    n_plots = len(metrics)\n    \n    fig, axs = plt.subplots(1,n_plots, figsize=(18, 5))\n\n    hist = history.history\n\n    for ax, metric in zip(axs, metrics):\n        ax.plot(np.clip(hist[metric], 0, 1))\n        ax.plot(np.clip(hist[\"val_\"+metric], 0, 1))\n        ax.legend([metric, \"val_\" + metric])\n        ax.set_title(metric)","2770c983":"plot(hist, 'loss', 'roc-auc', 'accuracy', 'precision', \"recall\")","633cad86":"test.head()","0d030008":"X_test = tokenizer.texts_to_sequences(test.question_text)\nX_test = tf.keras.preprocessing.sequence.pad_sequences(X_test, maxlen=maxlen, padding='post', truncating='post')\n\npreds = model.predict_classes(X_test)\n\nss_predictions = ss.copy()\nss_predictions['prediction'] = preds","eb31210d":"ss_predictions.to_csv(\"submission.csv\", index=False)","fbc2ba08":"# Summary\n\nThis notebook developed a simple model for classifying sentences as genuine or insincere. To accomplish this task it looked at creating a numerical representation of the text using the TensorFlow `Tokenizer` object. To enhance the model we looked at how to add a pretrained Embedding layer to the model. FInally we trained and evaluated a Bidrectional LSTM model.","567f591b":"<a id=\"NLP-Preprocessing\"><\/a>\n# NLP Preprocessing\nUsing text data with neural networks requires special preprocessing in order to prepare the data for modelling. Specifically we'll need to convert the text to a numerical representation, and to standardize this representation. We can do this by building a word vocabulary with a tokenizer, and by padding the sequences to standardize their length.\n\n<a id=\"Tokenizer\"><\/a>\n## Tokenizer\nTokenization is the process ot converting sentences, phrases, words, or characters into unique ids. The unique ids are then used to represent the text in the model. The process of tokenizing is two steps. First we create the token dictionary of unique entries. And second, we create a text representation where the uniqe ids are assigned based on the frequency of occurance.\n\nWe can tokenize a word corpus with `tf.keras.preprocessing.text.Tokenizer` and create a vocbulary using the the `fit_on_texts` method. When initalizing the `Tokenizer` there are arguments that help shape the vocabulary.\n- `num_words`: Will set the total number of words in the voaculary based on the most frequent. I.e. if set to 5000 the vocabulary will contain the 5000 most frequent words.\n- `oov_token`: This is the text representation given to words that are outside the vocabulary. \n- `filters`: These can be punctuation, special characters, and anything the vocabulary should ignore.\n\nBy default the vocublary will return out of vocabulary words at index 0, followed by the most frequently occuring words.\n\nWe can view the vocabulary dictionary with the `word_index` method and also the word counts with the `word_counts`. The article [Hands-on NLP Deep Learning Model Preparation in TensorFlow 2.X](https:\/\/towardsdatascience.com\/hands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633) has a detailed discussion on Tokenization and how its used in TensorFlow 2.0.\n\n<a id=\"Padding\"><\/a>\n## Padding\nWhen talking about sentences, not all sentences are the same length, and yet when we pass the vector to the neural network they will have to all match a certain input shape.In natural language processing like computer vision, padding is the process of filling a vector with zero values to manipulate it's shape. By padding we add zeros \"pre\" to the beginning of the sentence or \"post\" to the end of the sentence to fill the missing values that are needed to make the sentence the same lenght.\n\n![image.png](attachment:image.png)\n[Source](https:\/\/arxiv.org\/pdf\/1903.07288.pdf)\n\n\nWe can pad our sequences with `tf.keras.preprocessing.sequence.pad_sequnces`. By default it will set the maximum sequence length as the longest sequence in the set. This value can also be modified using the `maxlen` argument. ","b6d3dba3":"<a id=\"sequential-layers\"><\/a>\n# Sequential Layers\n\nRecrusive Neural Network (RNN) are good for sequental data like that seen in NLP. RNNs can be considered as a typical feed forward network that takes the output of a neuron and loops it back as input. It does this through the hidden state output.\n\n![image.png](attachment:image.png)\n[Source](https:\/\/towardsdatascience.com\/recurrent-neural-networks-rnn-explained-the-eli5-way-3956887e8b75)\n\nRNNs can be implented in the TensorFlow API using the `tf.keras.layers.RNN` layer. However they have a primary  shortcoming, namely:\n- On long input sequences weights might not update because of the vanishing gradient problem.\n- As a result of vanishing gradients, RNNs perform well on short inputs, but fail to learn effectively on long inputs i.e. they only have \"short-memory\"\n\nFor more information about RNNs and how they work the article [Illustradted Guide to Recurrent Nerual Networks](https:\/\/towardsdatascience.com\/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9) is a great reference.","07998096":"\n\n\n![image.png](attachment:image.png)\n[Source](https:\/\/ccri.com\/deep-learning-ontology-development\/)\n\n\n\nThere are three main algorithms used to generate word embeddings and you can learn more about this in the article [What are word Embeddings for text?](https:\/\/machinelearningmastery.com\/what-are-word-embeddings\/).\n1. Using The Embedding Layer\n2. Word 2 Vect\n3. GloVe\n\nWhen using an embedding layer in TensorFlow the weights can be either learned or reused. Learning the weights requires a large amount of text to training and so typically the option of reusing weights from one of the above algorithms is used. \n\nReuising weights is similar to the idea of transfer learning because we learn the weights once and can reuse them in any model. When we reuse weights we already have a vector representation for many of the words in our vocabulary, and when training will only need to learn representations of a small number of words.\n\n<a id=\"pretrained-embeddings\"><\/a>\n## Using pretrained embeddings\nIn order to use a pretrained embedding we will need to adapt the embedding for the vocabulary of the task. The general process is as follows:\n1. Download the embeddings text file and convert it into a dictionary wher the key is a word, and the value is the vector representation of the word.\n2. Not all words in our voacbulary are found in the embeddings file. We add them to the embeddings dictionary with random values centred around the mean and standard deviation of all the embeddings weights.\n3. Point the `tf.keras.layers.Embeddings` layer in our model to the newly generated embeddings matrix. We set `trainable=True` to allow the model to learn embeddings for words outside the pretrained embeddings, and to refine pretrained embeddings to our problem set.","ee31a92f":"# Tensorflow NLP\n\nThis notebook is part of a series of notebooks in this [github repo](https:\/\/github.com\/nicholasjhana\/tensorflow-certification-study-guide) that help prepare for the Tensorflow Certification. Feel free to check out the other notebooks too!\n\n1. [Tensorflow: Guide to Getting Started](https:\/\/www.kaggle.com\/nicholasjhana\/tensorflow-getting-started\/edit\/run\/46189040)\n2. [TensorFlow: Multi-Class Image Classification With Transfer Learning](https:\/\/www.kaggle.com\/nicholasjhana\/tensorflow-multi-classification-transfer-learning)\n3. [Tensorflow: NLP] (https:\/\/www.kaggle.com\/nicholasjhana\/tensorflow-nlp)\n4. [Multi-Variate Time Series Forecasting Tensorflow](https:\/\/www.kaggle.com\/nicholasjhana\/multi-variate-time-series-forecasting-tensorflow)\n\n## TensorFlow Concepts\nThis notebook covers the following topics from the tensorflow certification handbook:\n- Prepare text for use in a TensorFlow model\n- Use TensorFlow to identify text in binary categorization.\n- Use RNNs, LSTMs, and GRUs\n- Train word embeddings and import word embedding weights\n\n## Contents\n1. [Introduction](#introduction)\n2. [NLP Preprocessing](#NLP-Preprocessing)\n    - [Tokenizer](#Tokenizer)\n    - [Padding](#Padding)\n    - [Truncating](#Truncating)\n3. [Embeddings](#embeddings)\n    - [Using Pretrained Embeddings](#pretrained-embeddings)\n4. [Sequential Layers](#sequential-layers)\n    - [GRU & LSTM](#gru-lstm)\n    - [Bidirectional](#bidirectional)\n5. [Summary](#summary)\n\n\n\n## References\n\n1. [Do pretrained embeddings give you the extra edge?](https:\/\/www.kaggle.com\/sbongo\/do-pretrained-embeddings-give-you-the-extra-edge)\n2. [How to use word embedding layers - Machine Learning Mastery](https:\/\/machinelearningmastery.com\/use-word-embedding-layers-deep-learning-keras\/)\n3. [TensorFlow Tutorial: Word2Vec](https:\/\/www.tensorflow.org\/tutorials\/text\/word2vec)\n4. [TensorFlow Tutorial: Using the embedding layer](https:\/\/www.tensorflow.org\/tutorials\/text\/word_embeddings#using_the_embedding_layer)\n5. [Illustrated Guide to LSTMs and GRUs](https:\/\/towardsdatascience.com\/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n6. [Illustradted Guide to Recurrent Nerual Networks](https:\/\/towardsdatascience.com\/illustrated-guide-to-recurrent-neural-networks-79e5eb8049c9)\n7. [Birectional LSTM](http:\/\/ling.snu.ac.kr\/class\/cl_under1801\/BidirectionalLSTM.pdf)","48f3ba4e":"<a id=\"Truncating\"><\/a>\n### Truncating\nTruncation occurs when a sequence is longer than `maxlen` and part of the sequence must be dropped. By default the `truncating` argument is set to \"pre\" meaning it will drop the part of the sequence at the beginning that is longer than `maxlen`. Depending on the lanauge and use case this might be desireable, however in english the standard practice is to set `truncating` \"post\".\n\n![image.png](attachment:image.png)\n[Source](https:\/\/towardsdatascience.com\/hands-on-nlp-deep-learning-model-preparation-in-tensorflow-2-x-2e8c9f3c7633)","9804a947":"# Make Predictions","e866416c":"\n<a id=\"gru-lstm\"><\/a>\n## GRU & LSTM\n\nGated recursive Units (GRU) and Long-Short Term Memory (LSTM) are two solutions the help learn form both short- and long-term inputs. They are a variation on the RNN cell architecture that use internal gates within the cell to control the flow of information. The gates learn to distinguish relevant information from non relevant and in this way maintain state information of the input vector even when the sequence is long. The article [Illustrated Guide to LSTM\u2019s and GRU\u2019s: A step by step explanation] does a great job describing how input, forget, output gates (in the case of LSTM), reset, and update gates (GRU).\n\n![image.png](attachment:image.png)\n[Source](https:\/\/towardsdatascience.com\/illustrated-guide-to-lstms-and-gru-s-a-step-by-step-explanation-44e9eb85bf21)\n\nThese layers can be implemented with the [`tf.keras.layers.LSTM`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/LSTM) and [`tf.keras.layers.GRU`](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/layers\/GRU) layers. They have several parameters of interest, the most relevant being the `return_sequences` parameter. This allows us to chain several LSTM\/GRU layers together by returning the embedding from the layer as inputs to the next.\n\nThe layers also have several regularization parameters that allow us to apply dropout or weight decay at specific points within the cell.\n\n|Regularization Parameter| Applies To |\n|--|--|\n|`dropout`|units in input|\n|`recurrent_dropout`|units in recurrent state|\n| `kernel_regularizer` | weights matrix |\n| `recurrent_regularizer` | recurrent kernel weigths |\n| `bias_regularizer` | bias vector |\n| `activity_regularizer` | output layer |\n\n<a id=\"bidirectional\"><\/a>\n## Bidirectional\nBidirectional layers allow the model to run a recurrent layer forward and backward. The idea is that by learning from sequences both forward and backwards the layer maximizes information gain from the sample. Using this layer assumes that the whole sequence is available to training, and should be used according to the usecase.\n\nThe `tf.keras.layers.Bidirectional` layer is a wrapper for the `go_backwards = True` parmeter available in RNN\/GRU\/LSTM layers. The wrapper creates two layers that are side by side. The two layers run in parallel with one for the layer going forward, a second with the layer going backward and then a merge step that combines the two results.\n\nThe `merge_mode` allows us to configure how we combine the forward and backward passes on the sequence layer with several options.\n\n- \u2018sum\u2019: Outputs are added together.\n- \u2018mul\u2019: Outputs are multiplied together.\n- \u2018concat\u2019: Outputs are concatenated together (the default). This provids double the number of outputs to the next layer.\n- \u2018ave\u2019: Averages outputs. ","58386a2a":"<a id=\"introduction\"><\/a>\n# Introduction\n\nThis notebook uses the Quora Insincere Question dataset where the task is to detect insincere questions from sincere questions. The dataset contains 1.3M question and target pairs that form the classification problem.\n\nTaking a quick inital look at the data we notice it is not balanced. There are many more sincere questions to insincere. We also notice that the average and most frequent question length is significantly shorter than the longest. So we have a highly skewed distribution and this will influence how we choose the maximum length in our preprocessing.","5d02238b":"<a id=\"embeddings\"><\/a>\n# Embeddings\n\nEmbeddings are a way of representing words as a vector in such a way that similar words will have similar vector representations. The embedding is created in a many dimensional space (GloVe for examle is available in 50, 100, and 200 dimensions). To conceptualize the idea of an embedding we can think of word pairs we see relationships like that in the image below.\n\n![image.png](attachment:image.png)\n[Source](https:\/\/towardsdatascience.com\/word-embeddings-for-nlp-5b72991e01d4)\n\nIntuitively words that have similar meanings will be grouped together. As the embedding vectors are learned clusters will form within the vector space. As a result of the embedding space, the network will later be able to measure the 'similarity' or 'disimilarity' of words and use this information to train the model. In the image below we see what an embedding might start to look like when we have many different words forming into clusters within the vector space."}}