{"cell_type":{"660b7947":"code","33524ddb":"code","a1056cac":"code","e4bd71fa":"code","fa45afa2":"code","bacf8698":"code","d82b2601":"markdown","0e565391":"markdown","9d2a85d8":"markdown"},"source":{"660b7947":"from tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras import regularizers\nimport tensorflow.keras.utils as ku \nimport numpy as np ","33524ddb":"tokenizer = Tokenizer()\n!wget --no-check-certificate \\\n    https:\/\/storage.googleapis.com\/laurencemoroney-blog.appspot.com\/sonnets.txt \\\n    -O \/tmp\/sonnets.txt\ndata = open('\/tmp\/sonnets.txt').read()\n\ncorpus = data.lower().split(\"\\n\")\n\n\ntokenizer.fit_on_texts(corpus)\ntotal_words = len(tokenizer.word_index) + 1\n\n# create input sequences using list of tokens\ninput_sequences = []\nfor line in corpus:\n\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n\tfor i in range(1, len(token_list)):\n\t\tn_gram_sequence = token_list[:i+1]\n\t\tinput_sequences.append(n_gram_sequence)\n\n\n# pad sequences \nmax_sequence_len = max([len(x) for x in input_sequences])\ninput_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n\n# create predictors and label\npredictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n\nlabel = ku.to_categorical(label, num_classes=total_words)","a1056cac":"model = Sequential()\nmodel.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\nmodel.add(Bidirectional(LSTM(150, return_sequences = True)))\nmodel.add(Dropout(0.2))\nmodel.add(LSTM(100))\nmodel.add(Dense(total_words\/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\nmodel.add(Dense(total_words, activation='softmax'))\nmodel.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())\n","e4bd71fa":" history = model.fit(predictors,\n                     label,\n                     epochs=100,\n                     verbose=1\n                     )","fa45afa2":"import matplotlib.pyplot as plt\nacc = history.history['accuracy']\nloss = history.history['loss']\n\nepochs = range(len(acc))\n\nplt.plot(epochs, acc, 'b', label='Training accuracy')\nplt.title('Training accuracy')\n\nplt.legend()\nplt.figure()\n\nplt.plot(epochs, loss, 'b', label='Training Loss')\nplt.title('Training loss')\n\nplt.legend()\nplt.rc('font', size = 15)\nplt.rc('figure', figsize=[10,10])\nplt.show()","bacf8698":"seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\nnext_words = 100\n  \nfor _ in range(next_words):\n\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n\tpredicted = model.predict_classes(token_list, verbose=0)\n\toutput_word = \"\"\n\tfor word, index in tokenizer.word_index.items():\n\t\tif index == predicted:\n\t\t\toutput_word = word\n\t\t\tbreak\n\tseed_text += \" \" + output_word\nprint(seed_text)","d82b2601":"<a href=\"https:\/\/colab.research.google.com\/github\/mohnabil2020\/machine_learning\/blob\/master\/Shakespeare_words_predictor.ipynb\" target=\"_parent\"><img src=\"https:\/\/colab.research.google.com\/assets\/colab-badge.svg\" alt=\"Open In Colab\"\/><\/a>","0e565391":"**This to make 100 words prediction after this sentence:**\n\n\"Help me Obi Wan Kenobi, you're my only hope\"","9d2a85d8":"\n**This notebook is for predicting words from Shakespeare  using natural language processing**\n\n\n---\n\n\n\n---\n***My target is*** :\n\nIncreasing accuracy rate and decreasing loss rate as \n\n1.   Increasing accuracy rate and decreasing loss rate as possible\n2.   prediciting words correctly as possible\n\n3.   Preventing overfitting\n\n\n**hint**\n\nI used dataset from [HERE](https:\/\/storage.googleapis.com\/laurencemoroney-blog.appspot.com\/sonnets.txt)\nTo view the notebook better you can view it from my github [HERE](https:\/\/github.com\/mohnabil2020\/machine_learning\/blob\/master\/Shakespeare_words_predictor.ipynb)\n\n"}}