{"cell_type":{"b9c3577c":"code","760585d0":"code","60cd9442":"code","b5019adc":"code","a5d3c0ce":"code","c440ca56":"code","f71de6d7":"code","4bc18496":"code","3e1e9a8d":"code","e441ea19":"code","2e0fcbf3":"code","3a05fb17":"code","7d5948a0":"code","3d7cc371":"code","7341cd19":"code","4987965e":"code","e523cdcc":"code","851c8cfb":"code","32e9dc53":"code","9c01c858":"code","41c27fca":"code","c17d0d04":"code","99de9e6d":"code","dae790ba":"code","c7a02f54":"code","9580b6ce":"code","4b34fc64":"code","364432f7":"code","5e0da232":"code","e74234ef":"code","2dc66c86":"code","744c6e14":"code","6a15e2f4":"code","c694cb55":"code","3bd031bd":"markdown","e638cc5a":"markdown","93b2d798":"markdown","1708e2f9":"markdown","bc85d703":"markdown","fe02a849":"markdown","94f867a1":"markdown","321245a2":"markdown","aae0b287":"markdown","7611f6f0":"markdown","80a6bdca":"markdown","d5b28e9a":"markdown","3559977d":"markdown","44412f0b":"markdown","2e2264ff":"markdown","2d148b80":"markdown","9e8b9c53":"markdown","70acc45e":"markdown","0156add8":"markdown","7e1c6b3f":"markdown"},"source":{"b9c3577c":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn\nfrom pandas import Series, DataFrame\nfrom pylab import rcParams\nfrom sklearn import preprocessing\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics \nfrom sklearn.metrics import classification_report\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.svm import SVC\nfrom sklearn.metrics import confusion_matrix","760585d0":"import os\n####*IMPORANT*: Have to do this line *before* importing tensorflow\nos.environ['PYTHONHASHSEED']=str(1)\nimport tensorflow as tf\nimport tensorflow.keras as keras\nimport tensorflow.keras.layers \nimport random\nimport pandas as pd\nimport numpy as np\n\ndef reset_random_seeds():\n   os.environ['PYTHONHASHSEED']=str(1)\n   tf.random.set_seed(1)\n   np.random.seed(1)\n   random.seed(1)\n\n#make some random data\nreset_random_seeds()","60cd9442":"%matplotlib inline\nrcParams['figure.figsize'] = 10, 8\nsns.set_style('whitegrid')","b5019adc":"titanic_train = pd.read_csv('..\/input\/titanic-dataset\/train.csv')\ntitanic_train.columns = ['PassengerId','Survived','Pclass','Name','Sex','Age','SibSp','Parch','Ticket','Fare','Cabin','Embarked']\ntitanic_test = pd.read_csv('..\/input\/titanic-dataset\/test.csv')\ntitanic = titanic_train.append(titanic_test, ignore_index=True)\n\ntitanic.head()","a5d3c0ce":"sns.countplot(x='Survived',data=titanic, palette='hls')","c440ca56":"titanic.isnull().sum()","f71de6d7":"titanic.info()","4bc18496":"titanic.describe()","3e1e9a8d":"titanic[['Pclass', 'Survived']].groupby(['Pclass'], as_index=False).mean().sort_values(by='Survived', ascending=False)","e441ea19":"titanic[[\"Sex\", \"Survived\"]].groupby(['Sex'], as_index=False).mean().sort_values(by='Survived', ascending=False)","2e0fcbf3":"titanic_data = titanic.drop(['PassengerId','Name','Ticket','Cabin'], 1)\ntitanic_data.head()","3a05fb17":"sns.boxplot(x='Pclass', y='Age', data=titanic_data, palette='hls')","7d5948a0":"titanic_data.head()","3d7cc371":"def age_approx(cols):\n    Age = cols[0]\n    Pclass = cols[1]\n    \n    if pd.isnull(Age):\n        if Pclass == 1:\n            return 37\n        elif Pclass == 2:\n            return 29\n        else:\n            return 24\n    else:\n        return Age","7341cd19":"titanic_data['Age'] = titanic_data[['Age', 'Pclass']].apply(age_approx, axis=1)\ntitanic_data.isnull().sum()","4987965e":"titanic_data.dropna(inplace=True)\ntitanic_data.isnull().sum()","e523cdcc":"titanic_data['Family_Size'] = titanic_data['SibSp'] + titanic_data['Parch'] + 1\n\ntitanic_data[['Family_Size', 'Survived']].groupby(['Family_Size'], as_index=False).mean().sort_values(by='Survived', ascending=False)","851c8cfb":"titanic_data['IsAlone'] = 0\ntitanic_data.loc[titanic_data['Family_Size'] == 1, 'IsAlone'] = 1\ntitanic_data[['IsAlone', 'Survived']].groupby(['IsAlone'], as_index=False).mean()","32e9dc53":"titanic_data = titanic_data.drop(['Parch', 'SibSp', 'Family_Size'], axis=1)","9c01c858":"titanic_data.head()","41c27fca":"gender = pd.get_dummies(titanic_data['Sex'],drop_first=True)\ngender.head()","c17d0d04":"embark_location = pd.get_dummies(titanic_data['Embarked'],drop_first=True)\nembark_location.head()","99de9e6d":"titanic_data.head()","dae790ba":"titanic_data.drop(['Sex', 'Embarked'],axis=1,inplace=True)\ntitanic_data.head()","c7a02f54":"titanic_dmy = pd.concat([titanic_data,gender,embark_location],axis=1)\ntitanic_dmy.head()","9580b6ce":"sns.heatmap(titanic_dmy.corr())  ","4b34fc64":"#Fare and Pclass are dependent of each other.\ntitanic_dmy.drop(['Fare', 'Pclass'],axis=1,inplace=True)\ntitanic_dmy.head()","364432f7":"titanic_dmy.info()","5e0da232":"X = titanic_dmy.iloc[:,1:].values\ny = titanic_dmy.iloc[:,0].values","e74234ef":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size =0.20, random_state=25)","2dc66c86":"reset_random_seeds()\n# instantiate the model\nsvm = SVC()\n# fit the model\nfit3=svm.fit(X_train, y_train)\ny_pred = fit3.predict(X_test)\n\nsvm_conf = confusion_matrix(y_test, y_pred)\nprint(svm_conf)\nprint(classification_report(y_test, y_pred))","744c6e14":"# Do feature scaling\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\nX_scaled = sc.fit_transform(X)\n\n# Apply PCA\nfrom sklearn.decomposition import PCA\npca = PCA()\nX_pca = pca.fit_transform(X_scaled)\n\n# Get the transformed dataset\nX_pca = pd.DataFrame(X_pca)\nprint(X_pca.head())\nprint(\"\\nSize: \")\nprint(X_pca.shape)","6a15e2f4":"X_train_pca, X_test_pca, y_train, y_test = train_test_split(X_pca, y, test_size =0.20, random_state=25)","c694cb55":"reset_random_seeds()\n# instantiate the model\nsvm = SVC()\n# fit the model\nsvm.fit(X_train_pca, y_train)\ny_pred = svm.predict(X_test_pca)\n\nsvm_conf = confusion_matrix(y_test, y_pred)\nprint(svm_conf)\nprint(classification_report(y_test, y_pred))","3bd031bd":"### Taking care of missing values\n##### Dropping missing values\nDrop all the variables that aren't relevant for predicting survival. We should at least keep the following:\n- Survived - This variable is obviously relevant.\n- Pclass - Does a passenger's class on the boat affect their survivability?\n- Sex - Could a passenger's gender impact their survival rate?\n- Age - Does a person's age impact their survival rate?\n- SibSp - Does the number of relatives on the boat (that are siblings or a spouse) affect a person survivability? Probability\n- Parch - Does the number of relatives on the boat (that are children or parents) affect a person survivability? Probability\n- Fare - Does the fare a person paid effect his survivability? Maybe \n- Embarked - Does a person's point of embarkation matter? It depends on how the boat was filled","e638cc5a":"### Combine both train and test dataset in one large dataset.  And splitting the data with 30% for the test set support vector machine achieve accuracy score :62%. \n\n### On the other hand, second experiment using (PCA) support vector machine achieve:76%.","93b2d798":"### Support Vector Machine","1708e2f9":"#### Imputing missing values\nhow passenger age is related to their class as a passenger on the boat.","bc85d703":"# Data analysis of titanic dataset using Principal Component Analysis (PCA)","fe02a849":"### reading dataset","94f867a1":"### Converting categorical variables to a dummy indicators","321245a2":"Younger a passenger is, the more likely it is for them to be in 3rd class. The older a passenger is, the more likely it is for them to be in 1st class. So there is a loose relationship between these variables. Write a function that approximates a passengers age, based on their class. From the box plot, it looks like the average age of 1st class passengers is about 37, 2nd class passengers is 29, and 3rd class pasengers is 24.","aae0b287":"There are 4 null values in the embarked variable. We can drop those 4 records without loosing too much important information from our dataset, so we will do that.","7611f6f0":"### Checking for missing values","80a6bdca":"### Checking dataset size\nThere are 6 predictive features that remain. The rule of thumb is 50 records per feature, so we need to have at least 300 records in this dataset.","d5b28e9a":"### Support Vector Machine","3559977d":"records are there in the data frame","44412f0b":"## Deploying and evaluating the model using two experiments","2e2264ff":"##### Second experiment with using Principal Component Analysis (PCA)","2d148b80":"Apply the function and check again for null values.","9e8b9c53":"### Checking that your target variable is binary\nThe target is going to be \"Survived\" variable from the titanic dataframe. To make sure that it's a binary variable by using Seaborn's countplot() function.","70acc45e":"Just a quick fyi (we will examine these variables more closely in a minute):\n\n##### VARIABLE DESCRIPTIONS\n\nSurvived - Survival (0 = No; 1 = Yes)<br>\nPclass - Passenger Class (1 = 1st; 2 = 2nd; 3 = 3rd)<br>\nName - Name<br>\nSex - Sex<br>\nAge - Age<br>\nSibSp - Number of Siblings\/Spouses Aboard<br>\nParch - Number of Parents\/Children Aboard<br>\nTicket - Ticket Number<br>\nFare - Passenger Fare (British pound)<br>\nCabin - Cabin<br>\nEmbarked - Port of Embarkation (C = Cherbourg; Q = Queenstown; S = Southampton)","0156add8":"### Checking for independence between features","7e1c6b3f":"##### First experiment without using Principal Component Analysis (PCA)"}}