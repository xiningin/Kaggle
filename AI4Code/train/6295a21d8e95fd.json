{"cell_type":{"46026bce":"code","13db60d3":"code","cbce6969":"code","da9f214f":"code","12eb5c1f":"code","6e7079d0":"code","790cf88a":"code","59769436":"code","7edd0a2c":"code","67463dc4":"code","41cf05b0":"code","c4d6aff6":"markdown","ca596aee":"markdown","f60eb111":"markdown","e76cb04d":"markdown","4b4e26c5":"markdown","0a9860c0":"markdown"},"source":{"46026bce":"''' Imports '''\nimport os\nimport numpy as np\nimport pandas as pd\nimport scipy as sp\nimport sys\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom collections import defaultdict, Counter\nfrom functools import partial\nfrom sklearn import model_selection\nfrom sklearn.metrics import cohen_kappa_score,confusion_matrix\n\nimport albumentations\nimport cv2\nimport openslide\nimport PIL\nimport skimage.io\nfrom PIL import Image\nfrom tqdm.notebook import tqdm","13db60d3":"TRAIN = '..\/input\/prostate-cancer-grade-assessment\/train_images\/'\nMASKS = '..\/input\/prostate-cancer-grade-assessment\/train_label_masks\/'\nTILES = '..\/input\/tiles\/'\nMODEL = '..\/input\/modelweights\/se_resnext50_32x4d-a260b3a4.pth'\nTEST_PATH = '..\/input\/prostate-cancer-grade-assessment\/test_images\/'\n\nTRAIN_BAT_SIZE = 16\nVALID_BAT_SIZE = 32\nDEVICE = 'cuda'\nEPOCHS = 10\nFOLDS = 5\nTRAIN_FOLDS = {0 : [1,2,3,4], 1: [0,2,3,4], 2: [0,1,3,4], 3: [0,1,2,4], 4: [0,1,2,3]}\nMODEL_MEAN=(0.485, 0.456, 0.406)\nMODEL_STD=(0.229, 0.224, 0.225)","cbce6969":"train_csv = pd.read_csv('..\/input\/prostate-cancer-grade-assessment\/train.csv')\n\n# Create 5 stratified folds for the data. Stratification helps distribute all classes equally across folds.\ntrain_csv['kfolds'] = -1\nkf = model_selection.StratifiedKFold(n_splits = FOLDS, shuffle = False, random_state = 10)\nfor fold, (train_idx, val_idx) in enumerate(kf.split(X = train_csv, y=train_csv.isup_grade.values)):\n    print(len(train_idx), len(val_idx))\n    train_csv.loc[val_idx, 'kfolds'] = fold\n    \ntrain_csv.to_csv('\/kaggle\/train_folds.csv', index = False)\ntrain_csv = pd.read_csv('\/kaggle\/train_folds.csv')\ntrain_csv","da9f214f":"class PANDADataset:\n    def __init__(self, folds, mean, std):\n        df = pd.read_csv('\/kaggle\/train_folds.csv')\n        df = df[df.kfolds.isin(folds)].reset_index()\n        df = df[['image_id', 'isup_grade']]\n        self.image_ids = df.image_id.values\n        self.isup_grade = df.isup_grade\n        \n        '''Normalize the images, Apply Shift\/Scale\/Rotate operations to training data with 0.8 probability.'''\n        if len(folds) == 1:\n            self.aug = albumentations.Compose([\n                albumentations.Normalize(mean, std, always_apply = True)\n            ])\n        else:\n            self.aug = albumentations.Compose([\n                albumentations.ShiftScaleRotate(scale_limit = (1, 1.2), \n                                                rotate_limit = 180,\n                                                p = 0.8),\n                albumentations.Normalize(mean, std, always_apply = True)\n            ])\n\n    def __len__(self):\n        return len(self.image_ids)\n\n    def __getitem__(self, item):\n        tiles = Image.open(TILES + self.image_ids[item] + '.png').convert('RGB')\n        tiles = np.transpose(tiles, (0, 1, 2)).astype(np.float32)\n        tiles = self.aug(image = np.array(tiles))['image']\n        tiles = torch.Tensor(tiles).permute(2, 0, 1)\n        \n        return {\n            'image': tiles,\n            'isup_grade': torch.tensor(self.isup_grade[item], dtype = torch.float),\n        }","12eb5c1f":"\"\"\"\nCode borrowed from\nhttps:\/\/github.com\/Cadene\/pretrained-models.pytorch\/blob\/master\/pretrainedmodels\/models\/senet.py\n\"\"\"\nfrom __future__ import print_function, division, absolute_import\nfrom collections import OrderedDict\nimport math\n\nimport torch.nn as nn\nfrom torch.utils import model_zoo\n\n__all__ = ['SENet', 'senet154', 'se_resnet50', 'se_resnet101', 'se_resnet152',\n           'se_resnext50_32x4d', 'se_resnext101_32x4d']\n\npretrained_settings = {\n    'senet154': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/senet154-c7b49a05.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnet50': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnet50-ce0d4300.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnet101': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnet101-7e38fcc6.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnet152': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnet152-d17c99b7.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnext50_32x4d': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnext50_32x4d-a260b3a4.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n    'se_resnext101_32x4d': {\n        'imagenet': {\n            'url': 'http:\/\/data.lip6.fr\/cadene\/pretrainedmodels\/se_resnext101_32x4d-3b2fe3d8.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n}\n\n\nclass SEModule(nn.Module):\n\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels \/\/ reduction, kernel_size=1,\n                             padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels \/\/ reduction, channels, kernel_size=1,\n                             padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\n\nclass Bottleneck(nn.Module):\n    \"\"\"\n    Base class for bottlenecks that implements `forward()` method.\n    \"\"\"\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.se_module(out) + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(Bottleneck):\n    \"\"\"\n    Bottleneck for SENet154.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n                               stride=stride, padding=1, groups=groups,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * 4)\n        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBottleneck(Bottleneck):\n    \"\"\"\n    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n    (the latter is used in the torchvision implementation of ResNet).\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n                               stride=stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n                               groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNeXtBottleneck(Bottleneck):\n    \"\"\"\n    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None, base_width=4):\n        super(SEResNeXtBottleneck, self).__init__()\n        width = math.floor(planes * (base_width \/ 64)) * groups\n        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n                               stride=1)\n        self.bn1 = nn.BatchNorm2d(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n                               padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(width)\n        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SENet(nn.Module):\n\n    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n                 downsample_padding=1, num_classes=1000):\n        \"\"\"\n        Parameters\n        ----------\n        block (nn.Module): Bottleneck class.\n            - For SENet154: SEBottleneck\n            - For SE-ResNet models: SEResNetBottleneck\n            - For SE-ResNeXt models:  SEResNeXtBottleneck\n        layers (list of ints): Number of residual blocks for 4 layers of the\n            network (layer1...layer4).\n        groups (int): Number of groups for the 3x3 convolution in each\n            bottleneck block.\n            - For SENet154: 64\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models:  32\n        reduction (int): Reduction ratio for Squeeze-and-Excitation modules.\n            - For all models: 16\n        dropout_p (float or None): Drop probability for the Dropout layer.\n            If `None` the Dropout layer is not used.\n            - For SENet154: 0.2\n            - For SE-ResNet models: None\n            - For SE-ResNeXt models: None\n        inplanes (int):  Number of input channels for layer1.\n            - For SENet154: 128\n            - For SE-ResNet models: 64\n            - For SE-ResNeXt models: 64\n        input_3x3 (bool): If `True`, use three 3x3 convolutions instead of\n            a single 7x7 convolution in layer0.\n            - For SENet154: True\n            - For SE-ResNet models: False\n            - For SE-ResNeXt models: False\n        downsample_kernel_size (int): Kernel size for downsampling convolutions\n            in layer2, layer3 and layer4.\n            - For SENet154: 3\n            - For SE-ResNet models: 1\n            - For SE-ResNeXt models: 1\n        downsample_padding (int): Padding for downsampling convolutions in\n            layer2, layer3 and layer4.\n            - For SENet154: 1\n            - For SE-ResNet models: 0\n            - For SE-ResNeXt models: 0\n        num_classes (int): Number of outputs in `last_linear` layer.\n            - For all models: 1000\n        \"\"\"\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        if input_3x3:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n                                    bias=False)),\n                ('bn1', nn.BatchNorm2d(64)),\n                ('relu1', nn.ReLU(inplace=True)),\n                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn2', nn.BatchNorm2d(64)),\n                ('relu2', nn.ReLU(inplace=True)),\n                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn3', nn.BatchNorm2d(inplanes)),\n                ('relu3', nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n                                    padding=3, bias=False)),\n                ('bn1', nn.BatchNorm2d(inplanes)),\n                ('relu1', nn.ReLU(inplace=True)),\n            ]\n        # To preserve compatibility with Caffe weights `ceil_mode=True`\n        # is used instead of `padding=1`.\n        layer0_modules.append(('pool', nn.MaxPool2d(3, stride=2,\n                                                    ceil_mode=True)))\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0\n        )\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        # =====================> Changing this from AvgPool to AdaptiveAvgPool, so that all-dimensional inputs can be used. <=================================\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n                    downsample_kernel_size=1, downsample_padding=0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=downsample_kernel_size, stride=stride,\n                          padding=downsample_padding, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n                            downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups, reduction))\n\n        return nn.Sequential(*layers)\n\n    def features(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, x):\n        x = self.avg_pool(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.logits(x)\n        return x\n\n\ndef initialize_pretrained_model(model, num_classes, settings):\n    assert num_classes == settings['num_classes'], \\\n        'num_classes should be {}, but is {}'.format(\n            settings['num_classes'], num_classes)\n    model.load_state_dict(model_zoo.load_url(settings['url']))\n    model.input_space = settings['input_space']\n    model.input_size = settings['input_size']\n    model.input_range = settings['input_range']\n    model.mean = settings['mean']\n    model.std = settings['std']\n    \ndef initialize_pretrained_model(model, num_classes, settings):\n    assert num_classes == settings['num_classes'], \\\n        'num_classes should be {}, but is {}'.format(\n            settings['num_classes'], num_classes)\n\n#========================================== to load the model from inputs instead of downloading from GitHub===============================================\n#     model.load_state_dict(model_zoo.load_url(settings['url']))\n    model.load_state_dict(torch.load(MODEL))\n    model.input_space = settings['input_space']\n    model.input_size = settings['input_size']\n    model.input_range = settings['input_range']\n    model.mean = settings['mean']\n    model.std = settings['std']\n\n\ndef senet154(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEBottleneck, [3, 8, 36, 3], groups=64, reduction=16,\n                  dropout_p=0.2, num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['senet154'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet50(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNetBottleneck, [3, 4, 6, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnet50'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet101(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNetBottleneck, [3, 4, 23, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnet101'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnet152(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNetBottleneck, [3, 8, 36, 3], groups=1, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnet152'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext50_32x4d(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 6, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnext50_32x4d'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model\n\n\ndef se_resnext101_32x4d(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnext101_32x4d'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model","6e7079d0":"class Flatten(nn.Module):\n    def forward(self, input):\n        return input.view(input.size(0), -1)\n\nclass SEResNext50(nn.Module):\n    def __init__(self, h1=512, out = 1, d=0.5, freeze = False, pretrained = True):\n        super().__init__()\n        self.model = se_resnext50_32x4d()\n        self.model = nn.Sequential(*list(self.model.children()))[:-2]\n        inp = 2048\n        if freeze:\n            for param in self.model.parameters():\n                param.requires_grad = False\n        self.ap = nn.AdaptiveAvgPool2d((1,1))\n        self.mp = nn.AdaptiveMaxPool2d((1,1))\n        self.fla = Flatten()\n        self.bn0 = nn.BatchNorm1d(2 * inp,eps=1e-05, momentum=0.1, affine=True)\n        self.dropout0 = nn.Dropout(d)\n        self.fc1 = nn.Linear(inp*2, h1)\n        self.bn1 = nn.BatchNorm1d(h1,eps=1e-05, momentum=0.1, affine=True)\n        self.dropout1 = nn.Dropout(d)\n        self.fc2 = nn.Linear(h1, out)\n        \n    def forward(self, x):\n        shape = x[0].shape\n        x = self.model(x)\n        ap = self.ap(x)\n        mp = self.mp(x)\n        x = torch.cat((ap,mp),dim=1)\n        x = self.fla(x)\n        x = self.bn0(x)\n        x = self.dropout0(x)\n        x = F.relu(self.fc1(x))\n        x = self.bn1(x)\n        x = self.dropout1(x)         \n        x = self.fc2(x)\n        return x","790cf88a":"def quadratic_weighted_kappa(y_hat, y):\n    return cohen_kappa_score(y_hat, y, weights='quadratic')\n\n\nclass OptimizedRounder():\n    def __init__(self):\n        self.coef_ = 0\n\n    def _kappa_loss(self, coef, X, y):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            elif pred >= coef[3] and pred < coef[4]:\n                X_p[i] = 4\n            else:\n                X_p[i] = 5\n\n        ll = quadratic_weighted_kappa(y, X_p)\n        return -ll\n\n    def fit(self, X, y):\n        loss_partial = partial(self._kappa_loss, X=X, y=y)\n        initial_coef = [0.5, 1.5, 2.5, 3.5, 4.5]\n        self.coef_ = sp.optimize.minimize(loss_partial, initial_coef, method='nelder-mead')\n\n    def predict(self, X, coef):\n        X_p = np.copy(X)\n        for i, pred in enumerate(X_p):\n            if pred < coef[0]:\n                X_p[i] = 0\n            elif pred >= coef[0] and pred < coef[1]:\n                X_p[i] = 1\n            elif pred >= coef[1] and pred < coef[2]:\n                X_p[i] = 2\n            elif pred >= coef[2] and pred < coef[3]:\n                X_p[i] = 3\n            elif pred >= coef[3] and pred < coef[4]:\n                X_p[i] = 4\n            else:\n                X_p[i] = 5\n        return X_p\n\n    def coefficients(self):\n        return self.coef_['x']","59769436":"def train(dataset, data_loader, model, optimizer):\n    model.train()\n    avg_loss = 0\n    for bi, d in tqdm(enumerate(data_loader), total = int(len(dataset) \/ data_loader.batch_size)):\n        image = d['image']\n        isup_grade = d['isup_grade']\n        image = image.to(DEVICE, dtype = torch.float)\n        isup_grade = isup_grade.to(DEVICE, dtype = torch.float)\n        \n        ''' Resets gradients to zero for at the start of every mini-batch, so as not mix up the gradients.'''\n        optimizer.zero_grad()\n        outputs = model(image)\n        targets = isup_grade\n        loss = loss_function(outputs.view(-1), targets)\n        ''' Calling .backward() mutiple times accumulates the gradient (by addition) for each parameter.\n            This is why you should call optimizer.zero_grad() after each .step() call. \n            Note that following the first .backward call,\n            a second call is only possible after you have performed another forward pass\n        '''\n        loss.backward()\n\n        '''optimizer.step is performs a parameter update based on the current gradient \n           (stored in .grad attribute of a parameter) and the update rule\n        '''\n        optimizer.step()\n        avg_loss += loss.item() \/ len(data_loader)\n    return avg_loss\n\ndef eval(dataset, data_loader, model):\n    model.eval()\n    final_loss = 0\n    final_score = 0\n    counter = 0\n    all_targets = []\n    all_outputs = []\n    for bi, d in tqdm(enumerate(data_loader), total = int(len(dataset) \/ data_loader.batch_size)):\n        counter += 1\n        image = d['image']\n        isup_grade = d['isup_grade']\n        \n        image = image.to(DEVICE, dtype = torch.float)\n        isup_grade = isup_grade.to(DEVICE, dtype = torch.float)\n        \n        outputs = model(image)\n        targets = isup_grade\n        \n        all_outputs.append(outputs.view(-1))\n        all_targets.append(targets)\n        del image\n        del outputs\n        del targets\n    all_targets = torch.cat(all_targets, 0)\n    all_outputs = torch.cat(all_outputs, 0)\n    loss = loss_function(all_outputs, all_targets) \/ len(data_loader)\n    return loss, all_outputs, all_targets\n\ndef loss_function(outputs, targets):\n    o = outputs\n    t = targets\n    \n    l = nn.MSELoss()(o, t)\n    \n    return l","7edd0a2c":"def main(folds):\n    model = SEResNext50()\n    model.to(DEVICE)\n    \n    optimized_rounder = OptimizedRounder()\n    \n    train_dataset = PANDADataset(\n        folds = TRAIN_FOLDS[folds],\n        mean = MODEL_MEAN,\n        std = MODEL_STD\n    )\n    \n    train_loader = torch.utils.data.DataLoader(\n        dataset = train_dataset,\n        batch_size = TRAIN_BAT_SIZE,\n        shuffle = True,\n        num_workers = 4\n    )\n    \n    valid_dataset = PANDADataset(\n        folds = [folds],\n        mean = MODEL_MEAN,\n        std = MODEL_STD\n    )\n    \n    valid_loader = torch.utils.data.DataLoader(\n        dataset = valid_dataset,\n        batch_size = VALID_BAT_SIZE,\n        shuffle = True,\n        num_workers = 4\n    )\n    \n    optimizer = torch.optim.Adam(model.parameters(), lr = 1e-4)\n    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode = 'min',\n                                                          patience = 2, factor = 0.5, verbose = True)\n    \n    best_score = -100\n    for epoch in tqdm(range(EPOCHS)):\n        train_loss = train(train_dataset, train_loader, model, optimizer)\n        with torch.no_grad():\n            loss, all_outputs, all_targets = eval(valid_dataset, valid_loader, model)\n        scheduler.step(loss)\n        \n        all_outputs = all_outputs.to('cpu').numpy()\n        all_targets = all_targets.to('cpu').numpy()\n        \n        optimized_rounder.fit(all_outputs, all_targets)\n        coefficients = optimized_rounder.coefficients()\n        final_preds = optimized_rounder.predict(all_outputs, coefficients)\n        print(f'Counter preds: {Counter(final_preds)}')\n        print(f'coefficients: {coefficients}')\n        score = quadratic_weighted_kappa(all_targets, final_preds)\n        \n        print(f'  Epoch {epoch+1} - avg_train_loss: {train_loss:.4f}  avg_val_loss: {loss:.4f}')\n        print(f'  Epoch {epoch+1} - QWK: {score}  coefficients: {coefficients}')\n        \n        if score>best_score:\n            best_score = score\n            Predictions = all_outputs\n            Targets = all_targets\n            print(f'  Epoch {epoch+1} - Save Best Score: {best_score:.4f} Model  coefficients: {coefficients}')\n            torch.save(model.state_dict(), f'fold{folds}_se_resnext50.pth')\n    return all_outputs, all_targets","67463dc4":"Predictions, Targets = [], []\nfor i in range(5):\n    _outs, _labels = main(i)\n    Predictions.append(_outs)\n    Targets.append(_labels)","41cf05b0":"preds = np.concatenate(Predictions)\nvalid_labels = np.concatenate(Targets)\n\noptimized_rounder = OptimizedRounder()\noptimized_rounder.fit(preds, valid_labels)\ncoefficients = optimized_rounder.coefficients()\nfinal_preds = optimized_rounder.predict(preds, coefficients)\nprint(f'Counter preds: {Counter(final_preds)}')\nprint(f'coefficients: {coefficients}')\n\nscore = quadratic_weighted_kappa(valid_labels, final_preds)\nprint(f'CV QWK: {score}')","c4d6aff6":"### Working of Optimized Rounder\nWe'll be giving equal weightage to the outputs of all the 5 models we have trained. We'll take the mean of all the predictions and then output the ISUP_Score using the result.<br>\n\nThe outputs of our models are continuous values and we need to convert them into one of the 6 ISUP Scores (ie. 0,1,2,3,4,5). For this conversion, we'll use optimized rounder. The optimized rounder works by creating ranges for each class and all the values lying in the certain range are given corresponding values.<br>\nComponents of Optimized Rounder:\n**coefficients**: These are the coefficients that help define the value range for each class.<br>\nfor eg: we might set initial coefficients as \\[0.5, 1.5, 2.5, 3.5, 4.5\\], what this means is:<br>\nRange     => Class<br>\n-inf - 0.5 => 0<br>\n0.5  - 1.5 => 1<br>\n1.5  - 2.5 => 2<br>\n... and so on.<br>\n\n**fit**: We use the fit() to optimize our coefficients, so that the ranges are better selected according to our data.","ca596aee":"## My Approach\nInstead of using the biopsy images directly as inputs to the model, I'll be using Iafoss's **Concatenate Tile Pooling** approach. I have explained this approach in [this](https:\/\/www.kaggle.com\/norrsken\/tiles) notebook. The approach basically breaks the biopsy images which contain a lot of empty space into 16 tiles of 128*128 blocks that contain the actual biopsies.<br><br>\n\nI'll be using SE-ResNeXt-50 model with pretrained weights on 5 folds of input data provided. You can read more about Squeeze-And_Excitation Networks in this [paper](https:\/\/arxiv.org\/pdf\/1709.01507.pdf). I have replaced the last two FC layers with custom FC network that will be explained later.\n\nIn this notebook, I'll explain all the steps performed and help beginnners like myself understand how we can create a model for classifying the dataset into the 6 ISUP Scores.","f60eb111":"# Inference\nInference Notebook can be found [here](https:\/\/www.kaggle.com\/norrsken\/panda-prediction\/edit\/run\/35532892).","e76cb04d":"## Model Architecture\n \nI'll Fine Tune the model by removing last Two layers of the SE-ResNeXt50 model and replace them with the Fully Connected architecture as shown in the image below:\n![image.png](attachment:image.png)","4b4e26c5":"## Training Results","0a9860c0":"# Prostate cANcer graDe Assessment (PANDA) Challenge\n### About The Challenge\nThe training set consists of around 11,000 whole-slide images of digitized H&E-stained biopsies originating from two centers. This is the largest public whole-slide image dataset available, one of the largest digital pathology datasets and best known challenges in the field.<br>\nDiagnosis of PCa is based on the grading of prostate tissue biopsies. These tissue samples are examined by a pathologist and scored according to the Gleason grading system. In this challenge, we will develop models for detecting PCa on images of prostate tissue samples, and estimate severity of the disease using the most extensive multi-center dataset on Gleason grading yet available. <br>\nThe grading process consists of finding and classifying cancer tissue into so-called Gleason patterns (3, 4, or 5) based on the architectural growth patterns of the tumor (Fig. 1). After the biopsy is assigned a Gleason score, it is converted into an ISUP grade on a 1-5 scale. The Gleason grading system is the most important prognostic marker for PCa, and the ISUP grade has a crucial role when deciding how a patient should be treated. There is both a risk of missing cancers and a large risk of overgrading resulting in unnecessary treatment. However, the system suffers from significant inter-observer variability between pathologists, limiting its usefulness for individual patients. This variability in ratings could lead to unnecessary treatment, or worse, missing a severe diagnosis.<br>\n![image.png](attachment:image.png)"}}