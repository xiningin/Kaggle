{"cell_type":{"c2c84979":"code","d67dea9e":"code","aef9a84c":"code","279b884b":"code","eb176562":"code","9f7ae400":"code","39c05153":"code","7986b6ee":"code","b4e503f0":"code","b90d97a9":"code","93be3f28":"code","f1a5568e":"code","09cc5628":"code","10cb4e05":"code","55ac7f92":"code","85ce41e7":"code","e8e7d591":"code","e7ac2552":"code","80dc4845":"code","73d07a45":"code","a0ee403c":"code","e2705405":"code","95eea8b2":"code","cd4636b5":"code","69a31805":"code","bc96aa91":"code","21761afe":"code","004d1cda":"code","e49a5f14":"code","4fc00adb":"code","7370f862":"code","9057eda4":"code","8a3f70b6":"code","5b4b6008":"code","c4147b88":"code","84511636":"code","4d4ab9ba":"code","12643d73":"code","5a1c552d":"code","6708e56c":"code","1214a6c7":"code","bb50015e":"code","ed432665":"code","d9356a9d":"code","467f90fe":"code","5c58d6ba":"code","62b02019":"code","c68fa63b":"code","ddfa4de3":"code","38fc8874":"code","93653fe8":"code","7c75a2b8":"code","8f5a10e0":"code","e415512e":"code","24bac149":"code","470bf965":"code","d77136ff":"code","282ac27c":"code","5f9c689b":"code","4a34ddb5":"code","e289de78":"code","5447ca4e":"code","988ee7bb":"code","2e631008":"code","32feafd6":"code","2cefebf9":"code","97f193be":"code","56520bb9":"code","d4de7d7b":"code","bddccd17":"code","917a60c3":"code","d5b14da1":"code","6bd1da91":"code","9d2bd2d3":"code","9245bff0":"code","dd05506b":"code","36404cf3":"code","ce0e2298":"code","d29ab138":"code","267a3959":"code","1aad6d6c":"code","b1bdab95":"code","6f2194ef":"code","81c115e8":"code","42e638a4":"code","9933dcf1":"code","dd9f7822":"code","f7389292":"code","65830d83":"code","0e5c7e81":"code","f99e91c3":"code","a6538e37":"code","d5631205":"code","05065012":"code","f56b7233":"code","ec9b32d8":"code","4ab66fde":"code","35a1162a":"code","bdfa14fb":"code","6e4d6398":"code","69d45cf5":"code","571af164":"code","7648ef29":"code","1ba3d3df":"code","cdf5a102":"code","baef34dd":"code","23ec25a0":"code","2ff22926":"code","a1137055":"code","33ac63f5":"code","afb2af02":"code","cee7b44f":"code","a53aa222":"code","5bb988c5":"code","208e0c88":"code","50ed3605":"code","9cca3cfa":"code","aa8389de":"code","4bc0222c":"code","1e82c26b":"code","7d23ce1d":"code","7e849c60":"code","27131598":"code","851f4b7d":"code","a8961a3a":"code","980ceba5":"code","b557ef43":"code","51f2fa48":"code","ec7dac1b":"code","89aaa9b7":"code","507a7fb8":"code","cdb744c8":"code","ad4fa1f5":"code","ad257bd5":"code","192ad20f":"code","b23f89c7":"code","57bf79ad":"code","f57a3ccd":"code","360952b8":"code","fb95d193":"code","aa9139ad":"code","11fb8d82":"code","270a5b81":"code","a9fce0f2":"code","ddf30db8":"code","ace59a57":"code","455ea332":"code","f12ba614":"code","cb30c941":"code","e34a9c04":"code","081bb93f":"code","b0e4c607":"code","76440f08":"code","22550afb":"code","cfb1b817":"code","ffa7f83f":"code","7a1eb3ee":"code","8337b568":"code","db722000":"code","4922306b":"code","fe920813":"code","2054a9a2":"code","67b8aed9":"code","fd4bc45f":"code","28031cee":"code","9ad68f8c":"code","dc3a5164":"code","dbc4be71":"code","5f939371":"code","b38e007c":"markdown","66b516db":"markdown","6602cd82":"markdown","df5b91c1":"markdown","77d66603":"markdown","1b729e68":"markdown","62f29fb4":"markdown","62619d69":"markdown","ed04da84":"markdown","f61d0b63":"markdown","89f0bad0":"markdown","daa80d62":"markdown","eca47dc2":"markdown","ccc2d7f0":"markdown","c2214531":"markdown","80bfae5e":"markdown","16144127":"markdown","b929786a":"markdown","6456be79":"markdown","b39551b5":"markdown","fdd60f15":"markdown","de15f661":"markdown","7654a18a":"markdown","95e4f33f":"markdown","5c0d2524":"markdown","d58398ac":"markdown","fca90570":"markdown","4af707b0":"markdown","76fcd7b6":"markdown","447c7c29":"markdown","259f5103":"markdown","a239d949":"markdown","2c55ad11":"markdown","c9796440":"markdown","ee1a99a8":"markdown","e1e5a8b4":"markdown","43f9954d":"markdown","228a3100":"markdown","0d11f324":"markdown","8520c964":"markdown","045b8e20":"markdown","9c38bc0b":"markdown","2d17f6b4":"markdown","2511abc7":"markdown","5fa55ceb":"markdown","90790129":"markdown","5b092b6f":"markdown","64490c08":"markdown","7662a5f4":"markdown","a325c1a8":"markdown","c68ea316":"markdown","beaf0e34":"markdown","6cb882b8":"markdown","25a512e4":"markdown","eee08187":"markdown","edc6e17e":"markdown","9dc45ce0":"markdown","cdd78482":"markdown","6a9d6147":"markdown","ea62241e":"markdown","9c3e2e25":"markdown","f83264d6":"markdown","d58f2232":"markdown","272a5efa":"markdown","a8e76557":"markdown","b4208304":"markdown","ded11da6":"markdown","aefccb48":"markdown","1c3470af":"markdown","dc903b4b":"markdown","6e23e30a":"markdown","632fedad":"markdown","9361e0d2":"markdown","6ca25ee8":"markdown","27c124dd":"markdown","1cf396d2":"markdown","cc33ab13":"markdown","764c8019":"markdown","2df82d88":"markdown","b1f7212c":"markdown","23e5fcc9":"markdown","8f7c1974":"markdown","ca6a162a":"markdown","e3977b17":"markdown","0835d12e":"markdown","81f51a4b":"markdown","d27e96f2":"markdown","3e743a99":"markdown","5ed7a36c":"markdown","319220f4":"markdown","e25700ab":"markdown","367d4c97":"markdown","6d15ed0a":"markdown","9c1dbca0":"markdown","363d2539":"markdown","49a9f88e":"markdown","a9c637e1":"markdown","5085bc50":"markdown","6ebe6910":"markdown","82976b97":"markdown","7926816e":"markdown","94a6f5f6":"markdown","bdc54e5c":"markdown","0beb1d4f":"markdown","e33f8d8c":"markdown","eaf3b131":"markdown","c58c3e99":"markdown","5ec9e291":"markdown","c4e636e9":"markdown","87674201":"markdown","2b9009b7":"markdown","acd865f6":"markdown","2735e9fd":"markdown","19918ae9":"markdown","93419491":"markdown","1f0d79a5":"markdown","7e2f6c84":"markdown","3fe8cd85":"markdown","0613952a":"markdown","8d931e0b":"markdown","740ce633":"markdown","46404699":"markdown","16e4a810":"markdown","c0a10ddf":"markdown","d8d42a24":"markdown","3f19b25e":"markdown","6d11fbc6":"markdown","426d5f1d":"markdown","d947e0d3":"markdown","078607d4":"markdown","186c8ff5":"markdown","aa364085":"markdown","a02a2a64":"markdown","3d601f88":"markdown","cf48a3d7":"markdown","6f4b73a6":"markdown","60d6de29":"markdown","611af7bd":"markdown","0ffb701b":"markdown","91fdc16e":"markdown","9c02dfff":"markdown","2b63d115":"markdown","354d9386":"markdown","eeb1c212":"markdown","392ee74d":"markdown","330bd8a1":"markdown","fd41641e":"markdown","1ba1ba43":"markdown","d8b67585":"markdown","2938e5cd":"markdown"},"source":{"c2c84979":"# data processing tools\nimport pandas as pd\nimport numpy as np\nfrom numpy import mean\nfrom numpy import std\nfrom numpy import argmax\nfrom math import sqrt\nimport time\nfrom collections import Counter\nimport itertools\n\n# preprocessing\nfrom statsmodels.tsa.stattools import adfuller\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\n#from sklearn.experimental import enable_halving_search_cv #doesn't work on Kaggle\nfrom sklearn.model_selection import train_test_split, cross_validate, validation_curve, cross_val_score, GridSearchCV, KFold, RepeatedKFold #HalvingGridSearchCV, \n\n# scoring and algorithm selection packages\nfrom sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score \nfrom sklearn.inspection import permutation_importance\n\n# model tools\nimport statsmodels.api as sm\nfrom statsmodels.formula.api import ols\n\nimport scipy.stats as stats\nfrom scipy.stats import norm\n\nfrom sklearn.linear_model import LinearRegression, BayesianRidge, ElasticNet, GammaRegressor, HuberRegressor,  Lars, Lasso, SGDRegressor\nfrom sklearn.linear_model import LassoLars, OrthogonalMatchingPursuit, PassiveAggressiveRegressor, PoissonRegressor, RANSACRegressor, Ridge\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.neural_network import MLPRegressor\nfrom sklearn.svm import SVR, LinearSVR, NuSVR\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor, ExtraTreesRegressor, AdaBoostRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nimport xgboost as xgb\n\n\n# visualization packages\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# ignore warnings (gets rid of Pandas copy warnings)\nimport warnings\nwarnings.filterwarnings('ignore')\npd.options.display.max_columns = None\n","d67dea9e":"def iqr_outliers(column, iqr_level):\n    \"\"\"return the lower range and upper range for the data based on IQR\n    Arguments: \n    column - column to be evaluated\n    iqr_level - iqr range to be evaluated\n    \"\"\"\n    Q1,Q3 = np.percentile(column , [25,75])\n    iqr = Q3 - Q1\n    lower_range = Q1 - (iqr_level * iqr)\n    upper_range = Q3 + (iqr_level * iqr)\n    return lower_range,upper_range  ","aef9a84c":"# this function is by Max Halford at the address noted above\ndef calc_smooth_mean(df, by, on, m, target_df):\n    '''Function returns a weighted mean value for the each member of a column.\n    Arguments:\n    df: The df being used to calculate the means\n    by: the column being target encoded\n    on: the thing to be encoded; almost always price in this circumstance\n    m: weight before moving toward global mean; usually a min # samples\n    target_df: the target df for the mean encoding. Could be same as df or different.'''\n    # Compute the global mean\n    mean = df[on].mean() \n\n    # Compute the number of values and the mean of each group\n    agg = df.groupby(by)[on].agg(['count', 'mean'])  \n    counts = agg['count']\n    means = agg['mean']\n\n    # Compute the \"smoothed\" means\n    smooth = (counts * means + m * mean) \/ (counts + m)\n\n    # Replace each value by the according smoothed mean\n    return round(target_df[by].map(smooth), 0) ","279b884b":"def test_feature_combinations(price, variables):\n    \n    \"\"\"Function takes in target price and a dataframe of independent variables, and \n    tests model improvement for each combination of variables\n    ARGUMENTS:\n    Y of target values\n    X-dataframe of continuous features\n    Returns dataframe of score improvements over base score for each interaction combination\"\"\"\n    \n    # select our estimator and our cross validation plan\n    regression = LinearRegression()\n    cv = RepeatedKFold(n_splits=5, n_repeats=2, random_state=1)\n    \n    # prepare our scoring dataframe\n    scoring_df = pd.DataFrame()\n    \n    # prepare our lists to store our features and scores as we iterate\n    scores = []\n    feature1 = []\n    feature2 = []\n    \n    # Get a list of all of our features, and remove our target variable 'price' from the list\n    features = list(variables.columns)\n\n    # make a list of all of our possible feature combinations\n    feature_combos = itertools.combinations(features, 2)\n    feature_combos = list(feature_combos)\n    \n    # set our y-value as our target variable\n    y = price\n    \n    # prepare our x-value with our independent variables. We do an initial split here in order to run a \n    # linear regression to get a base r^2 on our basic model without interactions\n    X = variables\n    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=randomstate)\n    base_score = round(np.mean(cross_val_score(regression, X_train, y_train, scoring='r2', cv=cv)), 4)   \n    print(\"Model base score is \",base_score)\n    \n    # now we run the regression on each feature combo\n    for feature in feature_combos:\n        feat1, feat2 = feature[0], feature[1]\n        \n        # create the test interaction on our data set\n        variables['test_interaction'] = variables[feat1] * variables[feat2]\n        # create a new X which includes the test interaction and drops our target value\n        X = variables\n        # make a new split so that our x-splits include the test interaction\n        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=randomstate)\n        \n        # Run a linear regression with cross-val just like our base model, and append the score to our scores list\n        new_score = round(np.mean(cross_val_score(regression, X_train, y_train, scoring='r2', cv=cv)), 4)\n        scores.append(new_score)\n        # put feature 1 on a list\n        feature1.append(feat1)\n        # put feature 2 on a list\n        feature2.append(feat2)\n        print(feat1, feat2, new_score)\n        \n        \n    \n    # load all of our lists into the scoring dataframe\n    scoring_df['feature1'] = feature1\n    scoring_df['feature2'] = feature2\n    scoring_df['scores'] = scores\n    scoring_df['improvement'] = scoring_df['scores'] - base_score\n    variables.drop('test_interaction', axis=1, inplace=True)\n    \n    # return our scoring dataframe to the function\n    return scoring_df","eb176562":"def stepwise_selection(X, y, \n                       initial_list=[], \n                       threshold_in=0.01, \n                       threshold_out = 0.05, \n                       verbose=True):\n    \"\"\" \n    Perform a forward-backward feature selection \n    based on p-value from statsmodels.api.OLS\n    Arguments:\n        X - pandas.DataFrame with candidate features\n        y - list-like with the target\n        initial_list - list of features to start with (column names of X)\n        threshold_in - include a feature if its p-value < threshold_in\n        threshold_out - exclude a feature if its p-value > threshold_out\n        verbose - whether to print the sequence of inclusions and exclusions\n    Returns: list of selected features \n    Always set threshold_in < threshold_out to avoid infinite looping.\n    See https:\/\/en.wikipedia.org\/wiki\/Stepwise_regression for the details\n    \"\"\"\n    included = list(initial_list)\n    while True:\n        changed=False\n        # forward step\n        excluded = list(set(X.columns)-set(included))\n        new_pval = pd.Series(index=excluded)\n        for new_column in excluded:\n            modelols = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included+[new_column]]))).fit()\n            new_pval[new_column] = modelols.pvalues[new_column]\n        best_pval = new_pval.min()\n        if best_pval < threshold_in:\n            best_feature = new_pval.idxmin()\n            included.append(best_feature)\n            changed=True\n            if verbose:\n                print('Add  {:30} with p-value {:.6}'.format(best_feature, best_pval))\n\n        # backward step\n        modelols = sm.OLS(y, sm.add_constant(pd.DataFrame(X[included]))).fit()\n        # use all coefs except intercept\n        pvalues = modelols.pvalues.iloc[1:]\n        worst_pval = pvalues.max() # null if pvalues is empty\n        if worst_pval > threshold_out:\n            changed=True\n            worst_feature = included[pvalues.argmax()]\n            included.remove(worst_feature)\n            if verbose:\n                print('Drop {:30} with p-value {:.6}'.format(worst_feature, worst_pval))\n        if not changed:\n            break\n    return included\n\n","9f7ae400":"def grid_optimizer(model, grid, x, y):\n    \n    '''Takes in a model and a grid of hyperparameters, and runs a HalvingGridSearch\n    arguments: \n    model - the model to be grid searched\n    parameter grid - all parameters to grid search\n    x, y - train features and target to be tested in search\n    returns: best parameters'''\n    \n    start=time.time()\n    \n    print(\"Making Search\")\n    grid_search = HalvingGridSearchCV(model, grid, verbose=10, scoring='neg_mean_absolute_error', cv=5, min_resources='exhaust')\n\n    print(\"Running Grid\")\n    grid_search.fit(x, y)\n\n    grid_search.best_estimator_\n    \n    # Best f1\n    print('Best mae: %.3f' % grid_search.best_score_)\n\n    print(\"Best parameters set found on train set: \\n\")\n    print(grid_search.best_params_)\n    print(\"\\nGrid scores on train set:\\n\")\n    means = grid_search.cv_results_['mean_test_score']\n    stds = grid_search.cv_results_['std_test_score']\n    for mean, std, params in zip(means, stds, grid_search.cv_results_['params']):\n        print(\"%0.3f (+\/-%0.03f) for %r\" % (mean, std * 2, params))\n    \n    print(f'Elapsed Time: {time.time() - start}')\n    \n    return grid_search.best_params_","39c05153":"def score_model(model, x, y, model_type, score_list):\n    '''Takes in an instantiated model along with x, y and scores it, then appends to list\n    Arguments:\n    model: tuned\/instantiated model\n    x, y: train features and targets to be scored with CV\n    model_type: label with name of model\n    score_list: score list which is then returned back\n    Returns: List with scores appended\n    '''\n    # get accuracy cross val score for cv 5\n    scores = cross_validate(model, x, y, cv=5, n_jobs=-1,\n        scoring=('r2', 'neg_mean_absolute_error', 'neg_root_mean_squared_error'),\n        return_train_score=True)\n    r2 = round(scores['test_r2'].mean()*100,2)\n    mae = round(scores['test_neg_mean_absolute_error'].mean(), 4)\n    mrse = round(scores['test_neg_root_mean_squared_error'].mean(), 4)\n    \n    print(\"\\n\\n\",model_type,\" scores\")\n    print(\"CV 5 R2 Train Score: {}\".format(r2))\n    print(\"CV 5 MAE Train Score: {}\".format(mae))\n    print(\"CV 5 RMSE Train Score: {}\".format(mrse))\n\n    # append our scores to our lists\n    score_list['model'].append(model_type)\n    score_list['r2'].append(r2)\n    score_list['mae'].append(mae)\n    score_list['rmse'].append(mrse)\n    \n    return score_list","7986b6ee":"def plot_polys(y, xlabel, title):\n    '''Takes in a y-axis, x-axis label, and title and plots with various polynomial levels\n    ARGUMENTS:\n    y axis variable values\n    x-axis label\n    visualization title'''\n    x = y.index\n    \n    # express numbers as arrays and reshape\n    y = np.array(y)\n    x = np.array(x)\n    x = x.reshape(-1, 1)\n    \n    # plot figure\n    plt.figure(figsize=(16, 8))\n\n    # standard linear regression\n    linreg = LinearRegression()\n    linreg.fit(x, y)\n\n    # 2nd degree polynomial regression\n    poly2 = PolynomialFeatures(degree=2)\n    x_poly2 = poly2.fit_transform(x)\n    poly_reg2 = LinearRegression()\n    poly_reg2.fit(x_poly2, y)\n\n    # third degree polynomial regression \n    poly3 = PolynomialFeatures(degree=3)\n    x_poly3 = poly3.fit_transform(x)\n    poly_reg3 = LinearRegression()\n    poly_reg3.fit(x_poly3, y)\n\n    # predict on x values\n    pred = linreg.predict(x)\n    pred2 = poly_reg2.predict(x_poly2)\n    pred3 = poly_reg3.predict(x_poly3)\n\n    # plot regression lines\n    plt.scatter(x, y)\n    plt.yscale('log')\n    plt.title(title)\n    plt.xlabel(xlabel)\n    plt.ylabel('Average')\n    plt.plot(x, pred, c='red', label='Linear regression line')\n    plt.plot(x, pred2, c='yellow', label='Polynomial regression line 2')\n    plt.plot(x, pred3, c='#a3cfa3', label='Polynomial regression line 3');","b4e503f0":"# adding our chosen polynomial features\n\ndef create_polynomial_array(data, column, num_features):\n    '''takes a dataframe, a target column, and number of polynomial features\n    returns a small dataframe of polynomial features\n    '''\n    values = data[column]\n    poly_array = np.array(values)\n    poly_array = poly_array.reshape(-1,1)\n    poly_fit = PolynomialFeatures(degree=num_features, include_bias=False)\n    fit_features = poly_fit.fit_transform(poly_array)\n    poly_df = pd.DataFrame(fit_features)\n    return poly_df","b90d97a9":"def train_oof_predictions(x, y, models, verbose=True):\n    '''Function to perform Out-Of-Fold predictions on train data\n    returns re-ordered predictors x, re-ordered target y, and model dictionary with filled predictors\n    Parameters:\n    x: training predictors\n    y: training targets\n    models: dictionary of models in form of 'model name' : [instantiated model, predictors list]\n    verbose: whether to print the sequence of inclusions(True recommended)\n    '''\n    \n    # instantiate a KFold with 10 splits\n    kfold = KFold(n_splits=10, shuffle=True, random_state=randomstate)\n    \n    # prepare lists to hold the re-ordered x and y values\n    data_x, data_y  = [], []\n    \n    # run the following block for each of the 10 kfold splits\n    for train_ix, test_ix in kfold.split(x, y):\n    \n        if verbose: print(\"\\nStarting a new fold\\n\")\n    \n        if verbose: print(\"Creating splits\")\n        #create this fold's training and test sets\n        train_X, test_X = x[train_ix], x[test_ix] \n        train_y, test_y = y[train_ix], y[test_ix]\n    \n        if verbose: print(\"Adding x and y to lists\\n\")\n        # add the data that is used in this fold to the re-ordered lists\n        data_x.extend(test_X)\n        data_y.extend(test_y)\n    \n        # run each model on this kfold and add the predictors to the model's running predictors list\n        for item in models:\n            \n            label = item # get label for reporting purposes\n            model = models[item][0] # get the model to use on the kfold\n        \n            # fit and make predictions \n            if verbose: print(\"Running\",label,\"on this fold\")\n            model.fit(train_X, train_y) # fit to the train set for the kfold\n            predictions = model.predict(test_X) # fit on the out-of-fold set\n            models[item][1].extend(predictions) # add predictions to the model's running predictors list\n    \n    return data_x, data_y, models","93be3f28":"def model_selector(X, y, meta_model, models_dict, model_label, verbose=True):\n    \n    \"\"\" \n    Perform a forward model selection based on MAE improvement\n    Parameters:\n        X - baseline X_train with all features\n        y - baseline y_train with all targets\n        meta_model - meta_model to be trained\n        models_dict - dictionary of models in format of 'model name' : [model object, out-of-fold predictions]\n        label - the label for the current meta model\n        verbose - whether to print the sequence of inclusions(True recommended)\n    Returns: list of selected models, best MAE \n    \"\"\"\n\n    print(\"\\n\\nRunning model selector for \", model_label)\n    included_models = []\n     \n    while True:\n        changed=False\n        \n        # forward step\n        \n        if verbose: print(\"\\nNEW ROUND - Setting up score charts\")\n        excluded_models = list(set(models_dict.keys())-set(included_models)) # make a list of the current excluded_models\n        if verbose: print(\"Included models: {}\".format(included_models))\n        if verbose: print(\"Exluded models: {}\".format(excluded_models))\n        new_mae = pd.Series(index=excluded_models) # make a series where the index is the current excluded_models\n        \n        current_meta_x = np.array(X)\n        \n        if len(included_models) > 0:\n            for included in included_models:\n                included = np.array(models_dict[included][1]).reshape((len(models_dict[included][1]), 1))\n                current_meta_x = np.hstack((current_meta_x, included))\n\n        # score the current model\n        scores = cross_validate(meta_model, current_meta_x, y, cv=5, n_jobs=-1, scoring=('neg_mean_absolute_error'))\n        starting_mae = round(scores['test_score'].mean(),4)\n        if verbose: print(\"Starting mae: {}\\n\".format(starting_mae))\n        \n       \n        for excluded in excluded_models:  # for each item in the excluded_models list:\n            \n            new_yhat = np.array(models_dict[excluded][1]).reshape(-1, 1) # get the current item's predictions\n            meta_x = np.hstack((current_meta_x, new_yhat)) # add the predictions to the meta set\n            \n            # score the current item\n            scores = cross_validate(meta_model, meta_x, y, cv=5, n_jobs=-1, scoring=('neg_mean_absolute_error'))\n            mae = round(scores['test_score'].mean(),4)\n            if verbose: print(\"{} score: {}\".format(excluded, mae))\n            \n            new_mae[excluded] = mae # append the mae to the series field\n        \n        best_mae = new_mae.max() # evaluate best mae of the excluded_models in this round\n        if verbose: print(\"Best mae: {}\\n\".format(best_mae))\n        \n        if best_mae > starting_mae:  # if the best mae is better than the initial mae\n            best_feature = new_mae.idxmax()  # define this as the new best feature\n            included_models.append(str(best_feature)) # append this model name to the included list\n            changed=True # flag that we changed it\n            if verbose: print('Add  {} with mae {}\\n'.format(best_feature, best_mae))\n        else: changed = False\n        \n        if not changed:\n            break\n            \n    print(model_label, \"model optimized\")\n    print('resulting models:', included_models)\n    print('MAE:', starting_mae)\n    \n    return included_models, starting_mae","f1a5568e":"def create_meta_dataset(data_x, items):\n    '''Function that takes in a data set and list of predictions, and forges into one dataset\n    parameters:\n    data_x - original data set\n    items - list of predictions\n    returns: stacked data set\n    '''\n    \n    meta_x = data_x\n    \n    for z in items:\n        z = np.array(z).reshape((len(z), 1))\n        meta_x = np.hstack((meta_x, z))\n        \n    return meta_x","09cc5628":"def stack_prediction(X_test, final_models): \n    '''takes in a test set and a list of fitted models.\n    Fits each model in the list on the test set and stores it in a predictions list\n    Then sends the test set and the predictions to the create_meta_dataset to be combined\n    Returns: combined meta test set\n    Parameters:\n    X_test - testing dataset\n    final_dict - list of fitted models\n    '''\n    predictions = []\n    \n    for item in final_models:\n        print(item)\n        preds = item.predict(X_test).reshape(-1,1)\n        predictions.append(preds)\n    \n    meta_X = create_meta_dataset(X_test, predictions)\n        \n    return meta_X","10cb4e05":"# load and look at our king county housing data\ndf = pd.read_csv('..\/input\/housesalesprediction\/kc_house_data.csv')\ndf","55ac7f92":"# What are the data types?\ndf.dtypes","85ce41e7":"# Get the basic statistics\ndf.describe()","e8e7d591":"# Missing data?\ndf.isna().sum()","e7ac2552":"# make a new df with just the date and price\ntemp_df = df[['date', 'price']]\n\n# convert our date field to a proper datetime\ntemp_df['date'] = pd.to_datetime(temp_df['date'])\n\n# drop outliers (somewhat arbitrary)\ntemp_df.drop(temp_df.loc[(temp_df['price'] > 1000000) | (temp_df['price'] < 100000)].index, axis=0, inplace=True)\n\n# set the date as our index\ntemp_df.set_index('date', inplace=True)\n\n# group our data by day\ntemp = temp_df.groupby(pd.Grouper(freq='D')).mean()\n\n# backfill any empty days by getting previous day's mean\ntemp.bfill(inplace=True)\n\n# find the rolling mean and rolling standard deviation\nroll_mean = temp.rolling(window=10, center=False).mean()\nroll_std = temp.rolling(window=10, center=False).std()\n\n# plot the figure with rolling mean and standard deviation\nfig = plt.figure(figsize=(12,7))\nplt.plot(temp, color='blue', label='Original')\nplt.plot(roll_mean, color='red', label='Rolling Mean')\nplt.plot(roll_std, color='black', label = 'Rolling Std')\nplt.legend(loc='best')\nplt.title('Rolling Mean & Standard Deviation')\nplt.show(block=False)","80dc4845":"dftest = adfuller(temp)\n\n# Extract and display test results in a user friendly manner\ndfoutput = pd.Series(dftest[0:4], index=['Test Statistic', 'p-value', '#Lags Used', 'Number of Observations Used'])\nfor key,value in dftest[4].items():\n    dfoutput['Critical Value (%s)'%key] = value\n\nprint ('Results of Dickey-Fuller test: \\n')\nprint(dfoutput)","73d07a45":"# on our original df, convert the date to a proper date-time, and then drop the day so it's only year-month\ndf['date'] = pd.to_datetime(df['date'])\ndf['date'] = df['date'].dt.strftime('%Y-%m')","a0ee403c":"# Look for duplicate data using lat\/long\n\ndf[df.duplicated(subset=['lat','long'], keep=False)].sort_values('lat')","e2705405":"# We have a lot of duplicate entries. We're going to keep the later of these entries and hope that if it's an outlier,\n# it's caught in our outlier processing later.\n\ndf.drop_duplicates(['lat','long'], keep='last', inplace=True)","95eea8b2":"# plotting latitude and longitude as a visual scatter plot to look for location outliers\n\nplt.figure(figsize=(25,25))\n\nsns.scatterplot(data=df, x=\"long\", y=\"lat\", hue=\"price\", palette=\"magma_r\");","cd4636b5":"# drop the properties east of the metropolitan area\ndf.drop(df[df['long'] > -121.7].index, inplace=True)","69a31805":"# looking for outliers in the percentiles\ndf.describe()","bc96aa91":"# check how our histograms are looking\n\ndf.hist(figsize=(18,15), bins='auto');","21761afe":"# determing our IQR ranges for lot size, sq footage by calling our iqr_outliers function\nlotlower,lotupper = iqr_outliers(df.sqft_lot, 1.6)\nsqftlower, sqftupper = iqr_outliers(df.sqft_living, 1.6)\n\n# dropping the things outside of our lower and upper range\ndf.drop(df[ (df.sqft_lot > lotupper) | (df.sqft_lot < lotlower) ].index , inplace=True)\ndf.drop(df[ (df.sqft_living > sqftupper) | (df.sqft_living < sqftlower) ].index , inplace=True)","004d1cda":"# We're going to visualize a zip code's importance, so we're making a temporary feature\n# we're using the median house value for a zip code to determine the zip code's sort \n\n# group our dataframe by zipcode on median home price, sorted ascending. \nzipsorted = pd.DataFrame(df.groupby('zipcode')['price'].median().sort_values(ascending=True))\n\n# rank each zip code and assign rank to new column\nzipsorted['rank'] = np.divmod(np.arange(len(zipsorted)), 1)[0]+1\n\n# function that looks up a segment that a data entry belongs to\ndef make_group(x, frame, column):\n    '''Takes in a line, a lookup table, and a target column\n    returns value of target column\n    ARGUMENTS:\n    line from dataframe x\n    lookup table frame\n    column to return rank'''\n    y = frame.loc[(frame.index == x)][column]\n    z = np.array(y)\n    z[0]\n    return z[0]\n\n# make a new column on our dataframe. Look up each zip entry's group, and append to the column.\ndf['zip_rank'] = df['zipcode'].apply(lambda x: make_group(x, zipsorted, 'rank'))\n\n# apply the median home price per zip code to the data frame\ndf['median_zip'] = df['zipcode'].apply(lambda x: round(df.loc[df['zipcode']==x]['price'].median(), 0))","e49a5f14":"# visualize zip code as a color function, on a plot of price per square footage\n\nfig, ax = plt.subplots(figsize=(20, 15))\n\nax.scatter(df['sqft_living'], df['price'] \/100000, c=df['zip_rank'], cmap='magma_r')\n\nax.set_xlabel('Square Feet of Living Space', fontsize=12)\nax.set_ylabel('Price in $100,000', fontsize=12)\nax.set_title('Price per Total Square Feet, by Zip Code Median Rank', fontsize=20)\n;","4fc00adb":"# visualize zip code as a color function\n\nfig, ax = plt.subplots(figsize=(20, 15))\n\nax.scatter(df['median_zip'], df['price'] \/100000, c=df['zip_rank'], cmap='magma_r')\n\nax.set_xlabel('Median Home Price per Zip', fontsize=12)\nax.set_ylabel('Price in $100,000', fontsize=12)\nax.set_title('Price per Zip Code Median, by Zip Code Median Rank', fontsize=20)\n;","7370f862":"# Eliminating outliers on a per-zipcode basis\n\nzipcodes = df['zipcode'].unique()\n\nfor i in zipcodes:\n    lower, upper = iqr_outliers(df[df['zipcode'] == i]['price'], 1.6)\n    df.drop(df[ ( (df.price > upper) & (df['zipcode'] == i) ) | ( (df.price < lower)  & (df['zipcode'] == i) ) ].index , inplace=True)\n","9057eda4":"# check price stats by zip code and displaying top 30 zip codes by mean\n# I'm looking for suspiciously low \"min\" values\n\nfind_zip_outliers = df.groupby('zipcode')['price'].describe()\nfind_zip_outliers.sort_values('mean', ascending=False).head(35)\n\n# very suspicious values in many zip codes for min\n# 98112, 98109, 98033, 98115, 98027, 98116, 98122, 98117, 98136, 98065, 98144, 98072, 98028","8a3f70b6":"df.loc[(df['price'] <= 250000) & (df['zipcode'] == 98109)]\n# There are two listings in this zip code selling for under 250k. We're going to consider this an outlier sale.\n# worth noting these are both the exact same property. These are both bad listings.\n\n# It looks like we missed a dupe due to a slight mismatch in lat. Since this is a dupe AND an anomalous price, we are going to dump it.","5b4b6008":"df.loc[(df['price'] <= 250000) & (df['zipcode'] == 98033)]\n#We're going to consider these outlier sales.","c4147b88":"df.loc[(df['price'] <= 250000) & (df['zipcode'] == 98115)]\n#We're going to consider these outlier sales.","84511636":"df.loc[(df['price'] <= 250000) & (df['zipcode'] == 98027)]\n# listings under 250k in this zip. We'll drop them.","4d4ab9ba":"df.loc[(df['price'] <= 250000) & (df['zipcode'] == 98116)]\n# suspicious but in bad condition. We will drop though.","12643d73":"df.loc[(df['price'] <= 250000) & (df['zipcode'] == 98122)]\n# suspicious but VERY tiny. We will leave.","5a1c552d":"df.loc[(df['price'] <= 250000) & (df['zipcode'] == 98136)]\n# Drop","6708e56c":"df.loc[(df['price'] <= 250000) & (df['zipcode'] == 98065)]\n# Drop","1214a6c7":"df.loc[(df['price'] <= 250000) & (df['zipcode'] == 98144)]\n# Dropping most","bb50015e":"df.loc[(df['price'] <= 250000) & (df['zipcode'] == 98072)]\n# Dropping","ed432665":"df.loc[(df['price'] <= 250000) & (df['zipcode'] == 98028)]\n# Dropping","d9356a9d":"df.drop(index=[3844, 11436, 1222, 12823, 1975, 3885, 14001, 18897, 3975, 18332, 3987, 881, 11500, 17578, 8133, 9854, 18338, 5573, 5885, 8911, 11298, 2, 1922, 9231, 12306], inplace=True) #16879, 7088,  ","467f90fe":"# looking again at our percentile outliers\n\ndf.describe()","5c58d6ba":"#check what is going on with the sqft_lot15 outliers by sorting descending\ndf.sort_values('sqft_lot15', ascending=False).head(5)","62b02019":"# there is something off about these two large and nearly identical entries at the top of the list. We are going to drop these two rows.\n\ndf.drop(index=[4611, 9445], inplace=True)","c68fa63b":"#check what is going on with the weird bedroom value by sorting descending\n\ndf.loc[df['floors']==3.5]","ddfa4de3":"#check what is going on with the weird bedroom value by sorting descending\n\ndf.sort_values('bedrooms', ascending=False).head(5)","38fc8874":"# this value of 33 in 1620 square feet is obviously a mistake. We're going to impute the mean into this field.\n\n# 11 bedrooms in 3000sf, 10 bedrooms in 2920sf, 10 bedrooms in 3610sf are also obviously mistakes\n# We're going to impute the mean into the fields for 10 and 11 bedrooms as well.\n\n# we'll also impute the mean into the few bedroom listings with 0 bedrooms\n\nbedroom_mean = round(df['bedrooms'].mean(), 0)\nbedroom_mean\ndf.loc[df['bedrooms'] == 33.0, 'bedrooms'] = bedroom_mean\ndf.loc[df['bedrooms'] == 11.0, 'bedrooms'] = bedroom_mean\ndf.loc[df['bedrooms'] == 10.0, 'bedrooms'] = bedroom_mean\ndf.loc[df['bedrooms'] == 0, 'bedrooms'] = bedroom_mean\ndf.loc[df['bathrooms'] == 0, 'bathrooms'] = 2\n\n# I remain suspicious of the 9-bedroom entries, but we'll decline from manipulating them further","93653fe8":"df['yr_renovated'].value_counts(normalize=True).head(10)\n# 96% of houses have not been renovated, so we will turn this into a binary flag","7c75a2b8":"#For renovated, will change all values > 0 in those columns to 1\n# this will turn renovated into dichotomous choice flags\ndf.loc[df['yr_renovated'] > 0, 'yr_renovated'] = 1\n\n# now anything that is not a 1 becomes a 0, just in case we missed something weird\ndf.loc[df['yr_renovated'] != 1, 'yr_renovated'] = 0\n\n# since we're making it a binary flag, we'll rename yr_renovated to renovated\ndf.rename(columns={'yr_renovated' : 'renovated'}, inplace=True)","8f5a10e0":"df['waterfront'].value_counts(normalize=True).head(10)\n# Hardly any homes on the waterfront, and it's already a binary flag","e415512e":"df['view'].value_counts(normalize=True).head(10)\n# most houses don't have a view, but we have a few distinct choices, so we will KEEP this categorical ","24bac149":"df['sqft_basement'].value_counts(normalize=True).head(10)\n#Many properties don't have basements, but enough do to keep this as a continuous","470bf965":"# We're trying out engineering a feature that penalizes or rewards being the smallest or biggest\n# property in the neighborhood, as generally you don't want to be either. This is attempting \n# to impose a categorical relationship on how the property relates to its neighbors\n\ndf['comparative_sf'] = 0\ndf.loc[df['sqft_living'] <= (df['sqft_living15']*.75), 'comparative_sf'] = '1'\ndf.loc[(df['sqft_living'] > (df['sqft_living15']*.75)) & (df['sqft_living'] < (df['sqft_living15']*1.25)), 'comparative_sf'] = '2'\ndf.loc[df['sqft_living'] >= (df['sqft_living15']*1.25), 'comparative_sf'] = '3'\ndf['comparative_sf'] = df['comparative_sf'].astype('int8')","d77136ff":"# We're making all 0 basement values 1, and 0 view values .01, so that we can log transform this column ( cannot log 0)\ndf.loc[df['sqft_basement'] == 0, 'sqft_basement'] = 1\ndf.loc[df['view'] == 0, 'view'] = .01","282ac27c":"# Now that we are done cleaning, we will reset our indices\ndf.reset_index(inplace=True, drop=True)","5f9c689b":"# redo our zip code medians and rankings after data cleaning for visualizations\n# this is a repeat of the task we did further up\n\n# apply the median home price per zip code to the data frame again after outlier removal\ndf['median_zip'] = df['zipcode'].apply(lambda x: round(df.loc[df['zipcode']==x]['price'].median(), 0))\n\n# group our dataframe by zipcode on median home price, sorted ascending. We want to bin like-medians together.\nzipsorted = pd.DataFrame(df.groupby('zipcode')['price'].median().sort_values(ascending=True))\n\n# divide our dataframe into groups with entries per group as specified above,\n# and assign this number to a new column\nzipsorted['rank'] = np.divmod(np.arange(len(zipsorted)), 1)[0]+1\n\n# make a new column on our dataframe. Look up each zip entry's group, and append to the column.\ndf['zip_rank'] = df['zipcode'].apply(lambda x: make_group(x, zipsorted, 'rank'))","4a34ddb5":"# re-visualize zip code as a color function, using the median zip after outlier removal. \n\nfig, ax = plt.subplots(figsize=(20, 15))\n\nax.scatter(df['median_zip'], df['price'] \/100000, c=df['zip_rank'], cmap='magma_r')\n\nax.set_xlabel('Zip Code by Median Rank', fontsize=12)\nax.set_ylabel('Price in $100,000', fontsize=12)\nax.set_title('Price per Zip Code Median, by Zip Code Median Rank', fontsize=20);\n\n# save visualization to png\n#plt.savefig('images\/zip_prices.png')","e289de78":"# visualize zip code as a color function, on a plot of price per square footage\n\nfig, ax = plt.subplots(figsize=(20, 15))\n\nax.scatter(df['sqft_living'], df['price'] \/100000, c=df['zip_rank'], cmap='magma_r')\n\nax.set_xlabel('Square Feet of Living Space', fontsize=12)\nax.set_ylabel('Price in $100,000', fontsize=12)\nax.set_title('Price per Total Square Footage, by Zip Code Median Rank', fontsize=20)\n;","5447ca4e":"# plotting latitude and longitude as a visual scatter plot to look for location outliers\n\nplt.figure(figsize=(25,25))\n\nax = sns.scatterplot(data=df, x=\"long\", y=\"lat\", hue='price', palette=\"magma_r\");\n\n# save visualization to png\nplt.show()","988ee7bb":"# We can also use our scatter to see the zip codes by rank! Pretty cool.\n\nplt.figure(figsize=(25,25))\n\nsns.scatterplot(data=df, x=\"long\", y=\"lat\", hue=\"zip_rank\", palette=\"magma_r\");","2e631008":"#histogram and normal probability plot\nsns.distplot(df['price'], fit=norm);\nfig = plt.figure()\n\nres = stats.probplot(df['price'], plot=plt)\n\n# our sales price histogram is positively skewed and has a high peak\n# Our QQ-plot shows that we have heavy tails with right skew","32feafd6":"#skewness and kurtosis\nprint(\"Skewness: %f\" % df['price'].skew())\nprint(\"Kurtosis: %f\" % df['price'].kurt())\n\n# price is highly right skewed\n# very positive kurtosis, indicating lots in the tails. We can see those tails in the right skew.","2cefebf9":"# log transform our target price to improve normality of distribution\ndf_target_log = np.log(df['price'])\n\n#histogram and normal probability plot\nsns.distplot(df_target_log, fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_target_log, plot=plt)\n\n# Our target price is quite normally distributed when log transformed, so we'll be doing that\n","97f193be":"y = pd.DataFrame(np.log(df['price']))\n# get min and max for binning\n\ny.describe()","56520bb9":"# I enter the min price, max price, and how many bins I want\nbins = np.linspace(y['price'].min(), y['price'].max()+.01, 5)\n\n# numpy sorts the indices into the proper bins based on value of y\ny_binned = np.digitize(y, bins)","d4de7d7b":"# set our random seed for the notebook. We could randomize this each time the notebook is run,\n# but ultimately we want all of our train\/test splits to use the same data\nrandomstate = 30785\n\n# creating our train\/validation sets and our test sets\ntrain_data, holdout, y_train, y_test = train_test_split(df, y, test_size=0.15, random_state=randomstate, stratify=y_binned)\n\n# reset indices to prevent any index mismatches\ntrain_data.reset_index(inplace=True, drop=True)\nholdout.reset_index(inplace=True, drop=True)\ny_train.reset_index(inplace=True, drop=True)\ny_test.reset_index(inplace=True, drop=True)","bddccd17":"# Adding target encoding\n\n# get size of training data\nnum_of_samples = train_data.shape[0]\n\n# determining minimum number of samples for zip and month to use their\n# own mean rather than expanding into the full data set mean \nzip_samples = num_of_samples\/train_data['zipcode'].unique().shape[0]\nmonth_samples = num_of_samples\/train_data['date'].unique().shape[0]\n\n# create smooth additive encoded variables for zipcode, year built, and monthsold\ntrain_data['zip_smooth'] = calc_smooth_mean(train_data, 'zipcode', 'price', zip_samples, train_data)\ntrain_data['year_smooth'] = calc_smooth_mean(train_data, 'yr_built', 'price', 300, train_data)\ntrain_data['month_smooth'] = calc_smooth_mean(train_data, 'date', 'price', month_samples, train_data)\n\n# Create a wider lat and long zone to calculate an area mean\ntrain_data['lat_zone'] = round(train_data['lat'], 2)\ntrain_data['long_zone'] = round(train_data['long'], 2)\n\n# determing min number samples for lat and long mean\nlat_samples = num_of_samples\/train_data['lat_zone'].unique().shape[0]\nlong_samples = num_of_samples\/train_data['long_zone'].unique().shape[0]\n\n# calculate smooth mean variables for lat and long, then create an interactive variable describing both together\ntrain_data['lat_smooth'] = calc_smooth_mean(train_data, 'lat_zone', 'price', lat_samples, train_data)\ntrain_data['long_smooth'] = calc_smooth_mean(train_data, 'long_zone', 'price', long_samples, train_data)\ntrain_data['lat_long'] = (train_data['lat_smooth'] + train_data['long_smooth'])\/2","917a60c3":"# look for multicollinearity of features\nfig, ax = plt.subplots(figsize=(20, 20))\n\n# get the correlations for our train data\ntrain_data_c = train_data.corr()\n\n# we want our heatmap to not show the upper triangle, which is redundant data\n# get a mask for the upper diagonal\ntrain_data_c_mask = np.triu(np.ones_like(train_data_c, dtype=np.bool))\n\n# adjust mask and df to hide center diagonal\ntrain_data_c_mask = train_data_c_mask[1:, :-1]\ncorr = train_data_c.iloc[1:,:-1].copy()\n\n# color map\ncmap = sns.diverging_palette(220, 20, as_cmap=True)\n\n# plot heatmap\nsns.heatmap(corr, mask=train_data_c_mask, annot=True, fmt=\".2f\", cmap=cmap,\n           vmin=-1, vmax=1, cbar_kws={\"shrink\": .8}, square=True)\n\n# title\nplt.title('PEARSON CORRELATION MATRIX', fontsize=18)\n\nplt.show()","d5b14da1":"train_data.corr()","6bd1da91":"#Get our list of highly correlated feature pairs with following steps:\n\n# save correlation matrix as a new data frame\n# converts all values to absolute value\n# stacks the row:column pairs into a multindex\n# reset the index to set the multindex to seperate columns\n# sort values. 0 is the column automatically generated by the stacking\ndf_correlations = train_data.corr().abs().stack().reset_index().sort_values(0, ascending=False)\n\n# zip the variable name columns in a new column named \"pairs\"\ndf_correlations['pairs'] = list(zip(df_correlations.level_0, df_correlations.level_1))\n\n# set index to pairs\ndf_correlations.set_index(['pairs'], inplace = True)\n\n# rename our results column to correlation\ndf_correlations.rename(columns={0: \"correlation\"}, inplace=True)\n\n# Drop 1:1 correlations to get rid of self pairs\ndf_correlations.drop(df_correlations[df_correlations['correlation'] == 1.000000].index, inplace=True)\n\n# view pairs above 70% correlation and below 90% correlation (engineered features will correlate with each other above 95%)\ndf_correlations[(df_correlations.correlation>.75) & (df_correlations.correlation<.95)]\n","9d2bd2d3":"# drop multicollinear features and unneeded features\ntrain_data.drop(['id', 'zip_rank', 'median_zip', 'sqft_above', 'sqft_lot15'], axis=1, inplace=True)","9245bff0":"# Check out our variables correlationg with price\ndf_correlations = train_data.corr().abs().stack().reset_index().sort_values(0, ascending=False)\ndf_correlations.loc[df_correlations['level_0'] == 'price'].sort_values(0, ascending=False)","dd05506b":"# Checking out our mean sales price for year built scattered versus price shows a polynomial relationship\n\nyearly_prices = train_data.groupby('yr_built')['price'].mean()\n\nplt.scatter(yearly_prices.index, yearly_prices)\nplt.title(\"Linearity check\")\nplt.xlabel('year built')\nplt.ylabel('sales price')\nplt.show()","36404cf3":"# Checking out our mean sales price for month sold scattered versus price shows a polynomial relationship\nmonthly_prices = train_data.groupby('date')['price'].mean()\n\nplt.scatter(monthly_prices.index, monthly_prices)\nplt.title(\"Linearity check\")\nplt.xlabel('date sold')\nplt.ylabel('sales price')\nplt.show()","ce0e2298":"# Checking out our mean sales price for longitude does not show a polynomial relationship\nlong_prices = train_data.groupby('lat_long')['price'].mean()\n\nplt.scatter(long_prices.index, long_prices)\nplt.title(\"Linearity check\")\nplt.xlabel('long')\nplt.ylabel('sales price')\nplt.show()","d29ab138":"# Checking out our mean sales price for grade shows a polynomial relationship\ngrade_prices = train_data.groupby('grade')['price'].mean()\n\nplt.scatter(grade_prices.index, grade_prices)\nplt.title(\"Linearity check\")\nplt.xlabel('long')\nplt.ylabel('sales price')\nplt.show()","267a3959":"# Checking out our mean sales price for grade shows a polynomial relationship\ngrade_prices = train_data.groupby('sqft_living')['price'].mean()\n\nplt.scatter(grade_prices.index, grade_prices)\nplt.title(\"Linearity check\")\nplt.xlabel('square footage')\nplt.ylabel('sales price')\nplt.show()","1aad6d6c":"categoricals = ['floors', 'waterfront', 'renovated', 'comparative_sf', 'date', 'yr_built', 'zipcode', 'lat_long'] #\n\n# make our categorical data frame to work with\ndf_categoricals = train_data[categoricals]\n\n# binning our year built bins\ndf_categoricals[\"year_block\"] = pd.qcut(df_categoricals['yr_built'], q=30, labels=np.array(range(1,30+1)))\n\n# binning our latitude bins\ndf_categoricals[\"loc_block\"] = pd.qcut(df_categoricals['lat_long'], q=50, labels=np.array(range(1,50+1)))\n\n# dropping the original categories we just binned\ndf_categoricals.drop(['yr_built', 'lat_long'], axis=1, inplace=True)\n\n# telling Pandas that these columns are categoricals\nfor item in df_categoricals.columns:\n    df_categoricals[item] = df_categoricals[item].astype('category')\n\n# temporarily adding price to our dataframe so that we can do some visualizations    \ndf_categoricals['price'] = train_data['price']\n\n# plot our categoricals as box plots vs price\ndef boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\n\n#visualization categories\ncategorical=['floors', 'waterfront', 'renovated', 'comparative_sf', 'date', 'year_block', 'zipcode', 'loc_block']\nf = pd.melt(df_categoricals, id_vars=['price'], value_vars=categorical)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, size=5)\ng = g.map(boxplot, \"value\", \"price\")\n\ndf_categoricals.drop('price', axis=1, inplace=True)","b1bdab95":"# make a processed one hot file \n\nfinal_cats =  ['floors', 'waterfront', 'renovated','comparative_sf', 'zipcode', 'date', 'year_block', 'loc_block']\n\ndf_cats_train = pd.get_dummies(df_categoricals[final_cats], prefix=final_cats, drop_first=True)","6f2194ef":"continuous = ['sqft_living', 'sqft_living15', 'sqft_lot', 'sqft_basement', 'view', 'bedrooms', 'bathrooms', 'condition', 'grade', 'zip_smooth', 'year_smooth', 'month_smooth', 'lat_long'] #'floors',  \n\n# make our continuous frame to work with\nx_continuous = train_data[continuous]\nx_continuous['price'] = train_data['price']\n\n# plot our smaller choice continuous as box plots vs price\nsmall_cont = ['bedrooms', 'bathrooms', 'condition', 'grade', 'view', 'month_smooth'] \n\ndef boxplot(x, y, **kwargs):\n    sns.boxplot(x=x, y=y)\n    x=plt.xticks(rotation=90)\n\nf = pd.melt(x_continuous, id_vars=['price'], value_vars=small_cont)\ng = sns.FacetGrid(f, col=\"variable\",  col_wrap=2, sharex=False, sharey=False, size=5)\ng = g.map(boxplot, \"value\", \"price\")","81c115e8":"# plot our larger continuous as scatter plots vs price\nlarge_cont = ['sqft_living', 'sqft_living15', 'sqft_lot', 'sqft_basement', 'zip_smooth', 'year_smooth', 'month_smooth', 'lat_long']\n\nfig, axes = plt.subplots(nrows=4, ncols=2, figsize=(15,25), sharey=True)\n\nfor ax, column in zip(axes.flatten(), large_cont):\n    ax.scatter(x_continuous[column], x_continuous['price']\/100000, label=column, alpha=.1)\n    ax.set_title(f'Sale Price vs {column}')\n    ax.set_xlabel(column)\n    ax.set_ylabel('Sale Price in $100,000')\n\nfig.tight_layout()\n\nx_continuous.drop('price', axis=1, inplace=True)","42e638a4":"# running our function on our continuous variables to look for improvement\n# our R2 is much lower for model base score because we aren't including our categorical variables in this improvement assessment\n\nscoring_df = test_feature_combinations(y_train, x_continuous)","9933dcf1":"# showing our improvement scores for our interactions\n\nscoring_df.sort_values('improvement', ascending=False)","dd9f7822":"# check out our histograms to see if we should transform our data before scaling\n\nx_continuous.hist(figsize=(18,15), bins='auto');","f7389292":"# log transform\nlog_continuous = np.log(x_continuous)\n\n# standardize all of our values with scikit-learn StandardScaler\nscaler = StandardScaler()\n\nscaled_continuous = pd.DataFrame(scaler.fit_transform(log_continuous),columns = log_continuous.columns)\nscaled_continuous.head(5)\n\n# Our scaled and transformed continuous features\n\nscaled_continuous.hist(figsize=(18,15), bins='auto');","65830d83":"# make a processed continuous frame\ncontinuous =  ['sqft_living', 'sqft_living15', 'sqft_lot', 'sqft_basement', 'view', 'bedrooms', 'bathrooms', 'condition', 'grade',  'zip_smooth', 'year_smooth', 'month_smooth', 'lat_long']  \n\ndf_cont_train = scaled_continuous[continuous]","0e5c7e81":"# group by average year built mean to see relationship\ny = train_data.groupby('yr_built')['price'].mean()\nplot_polys(y, \"Year Sold\", \"Year Sold Mean\")","f99e91c3":"# group by grade mean to see relationship\ny = train_data.groupby('grade')['price'].mean()\nplot_polys(y,'Grade', \"Grade Mean\")","a6538e37":"# group by grade mean to see relationship\ny = train_data.groupby('sqft_living')['price'].mean()\nplot_polys(y,'Square Footage', \"Square Footage Mean\")","d5631205":"# group by grade mean to see relationship\ny = train_data.groupby('sqft_lot')['price'].mean()\nplot_polys(y,'Lot Square Footage', \"Lot Square Footage Mean\")","05065012":"year_poly = create_polynomial_array(df_cont_train, 'year_smooth', 2)\ngrade_poly = create_polynomial_array(df_cont_train, 'grade', 2)\nsq_ft_poly = create_polynomial_array(df_cont_train, 'sqft_living', 3)\n\ndf_cont_train['grade1'] = grade_poly[1]\ndf_cont_train['year1'] = year_poly[1]\ndf_cont_train['sqft_living3'] = sq_ft_poly[2]\ndf_cont_train['sqft_living2'] = sq_ft_poly[1]\n","f56b7233":"# apply target encoding to test data, using train data to map\n\n# create smooth additive encoded variables for zipcode, year built, and monthsold\nholdout['zip_smooth'] = calc_smooth_mean(train_data, 'zipcode', 'price', zip_samples, holdout)\nholdout['year_smooth'] = calc_smooth_mean(train_data, 'yr_built', 'price', 300, holdout)\nholdout['month_smooth'] = calc_smooth_mean(train_data, 'date', 'price', month_samples, holdout)\n\n\n# Create a wider lat and long zone to calculate an area mean\nholdout['lat_zone'] = round(holdout['lat'], 2)\nholdout['long_zone'] = round(holdout['long'], 2)\n\n# calculate smooth mean variables for lat and long, then create an interactive variable describing both together\nholdout['lat_smooth'] = calc_smooth_mean(train_data, 'lat_zone', 'price', lat_samples, holdout)\nholdout['long_smooth'] = calc_smooth_mean(train_data, 'long_zone', 'price', long_samples, holdout)\nholdout['lat_long'] = (holdout['lat_smooth'] + holdout['long_smooth'])\/2\nholdout.fillna(holdout.mean(), inplace=True)","ec9b32d8":"# Process Holdout Categoricals\n\nholdout_categoricals = holdout[categoricals]\n\n# binning our year built bins\nholdout_categoricals[\"year_block\"] = pd.qcut(holdout_categoricals['yr_built'], q=30, labels=np.array(range(1,30+1)))\n\n# binning our latitude bins\nholdout_categoricals[\"loc_block\"] = pd.qcut(holdout_categoricals['lat_long'], q=50, labels=np.array(range(1,50+1)))\n\nholdout_categoricals.drop(['yr_built'], axis=1, inplace=True)\n\n# telling Pandas that these columns are categoricals\nfor item in holdout_categoricals.columns:\n    holdout_categoricals[item] = holdout_categoricals[item].astype('category')\n\n# make a processed bins file for use with linear regression\ndf_cats_test = pd.get_dummies(holdout_categoricals[final_cats], prefix=final_cats, drop_first=True)","4ab66fde":"# Process Holdout Continuous\n\nholdout_continuous = holdout[continuous]\n\n# log transform\nlog_holdout = np.log(holdout_continuous)\n\n# standard scaler\nscaled_holdout_continuous = pd.DataFrame(scaler.transform(log_holdout),columns = log_holdout.columns)\n\n# adding polynomial features\n\nyear_poly = create_polynomial_array(scaled_holdout_continuous, 'year_smooth', 2)\ngrade_poly = create_polynomial_array(scaled_holdout_continuous, 'grade', 2)\nsq_ft_poly = create_polynomial_array(scaled_holdout_continuous, 'sqft_living', 3)\n\nscaled_holdout_continuous['grade1'] = grade_poly[1]\nscaled_holdout_continuous['year1'] = year_poly[1]\nscaled_holdout_continuous['sqft_living3'] = sq_ft_poly[2]\nscaled_holdout_continuous['sqft_living2'] = sq_ft_poly[1]","35a1162a":"X_train = pd.concat([df_cont_train, df_cats_train], axis=1)\nX_test = pd.concat([scaled_holdout_continuous, df_cats_test], axis=1)\n\ntest_actual = np.array(np.exp(y_test))","bdfa14fb":"y_train = np.array(y_train).ravel()\ny_test = np.array(y_test).ravel()","6e4d6398":"# prepare dictionary to store results\nmodels = {}\nmodels['model'] = []\nmodels['r2'] = []\nmodels['mae'] = []\nmodels['rmse'] = [] ","69d45cf5":"break","571af164":"# Set upline models\nbaseline_models = {\n    \"LR\": LinearRegression(),\n    \"Decision Tree\": DecisionTreeRegressor(random_state = randomstate),\n    \"Elastic Net\": ElasticNet(random_state = randomstate, normalize=True, copy_X=True, tol=1e-3),\n    \"Extra Trees\": ExtraTreesRegressor(random_state = randomstate),\n    \"Gaussian\": GaussianProcessRegressor(random_state = randomstate, normalize_y=True),\n    \"Gradient Boosted\" : GradientBoostingRegressor(random_state = randomstate),\n    \"KNN\" : KNeighborsRegressor(),\n    \"Lars\" : Lars(random_state = randomstate, copy_X=True, normalize=False),\n    \"Lasso\" : Lasso(random_state = randomstate, copy_X=True, tol=1e-3),\n    \"LinearSVR\" : LinearSVR(random_state = randomstate),\n    \"MLPRegressor\" : MLPRegressor(random_state = randomstate),\n    \"Bayesian Ridge\" : BayesianRidge(),\n    \"Gamma Regressor\" : GammaRegressor(),\n    \"Lasso Lars\" : LassoLars(random_state = randomstate, normalize=False, copy_X=True),\n    \"Nu SVR\": NuSVR(),\n    \"Orthogonal Matching Pursuit\" : OrthogonalMatchingPursuit(),\n    \"Passive Aggressive\" : PassiveAggressiveRegressor(random_state = randomstate),\n    \"RANSAC\" : RANSACRegressor(random_state = randomstate),\n    \"Ridge\" : Ridge(random_state = randomstate, tol=1e-3, normalize=False),\n    \"SVR\" : SVR(),\n    \"XGB Regressor\" : xgb.XGBRegressor(random_state = randomstate),\n    \"Ada Boost\" : AdaBoostRegressor(random_state=randomstate),\n    \"SGD Regressor\" : SGDRegressor(random_state=randomstate),\n    \"Random Forest Regressor\" : RandomForestRegressor(random_state=randomstate)\n}\n\n# run spot check on each model inline\nfor model in baseline_models:\n    this_model = baseline_models[model]\n    label = model\n    spot_check = score_model(this_model, X_train, y_train, label, models)","7648ef29":"# make data frame from our models dictionary\ntarget = pd.DataFrame(models).reset_index(drop=True)\n\n# sort data frame by mae and reset index\ntarget.sort_values('mae', ascending=False).head(20)","1ba3d3df":"# Unless we specify a different model, this is our baseline model\nmodel = LinearRegression()","cdf5a102":"predictors_train = sm.add_constant(X_train)\nmodelols = sm.OLS(y_train, predictors_train).fit()\nmodelols.summary()","baef34dd":"#result = stepwise_selection(X_train, y_train, verbose=True)\n#print('resulting features:', result)\n\n#We run this and get the following results:\nresult = ['grade', 'zip_smooth', 'sqft_living', 'lat_long', 'view', 'zipcode_98074', 'condition', 'year_smooth', 'zipcode_98133', 'zipcode_98002', 'zipcode_98075', 'zipcode_98022', 'sqft_living2', 'zipcode_98006', 'zipcode_98052', 'zipcode_98032', 'zipcode_98039', 'zipcode_98092', 'date_2015-04', 'zipcode_98155', 'zipcode_98125', 'renovated_1', 'sqft_living15', 'zipcode_98040', 'zipcode_98199', 'zipcode_98112', 'zipcode_98065', 'zipcode_98004', 'zipcode_98126', 'zipcode_98033', 'sqft_lot', 'zipcode_98106', 'zipcode_98118', 'zipcode_98146', 'zipcode_98034', 'date_2015-05', 'zipcode_98103', 'zipcode_98107', 'zipcode_98117', 'date_2015-03', 'zipcode_98188', 'zipcode_98029', 'year_block_6', 'loc_block_19', 'zipcode_98010', 'zipcode_98102', 'zipcode_98122', 'zipcode_98109', 'zipcode_98115', 'year_block_29', 'waterfront_1', 'zipcode_98031', 'zipcode_98148', 'year_block_16', 'loc_block_23', 'year_block_13', 'zipcode_98136', 'zipcode_98030', 'zipcode_98119', 'floors_3.0', 'sqft_basement', 'zipcode_98003', 'zipcode_98144', 'zipcode_98116', 'zipcode_98105', 'bathrooms', 'zipcode_98056', 'zipcode_98014', 'zipcode_98019', 'year_block_5', 'grade1', 'zipcode_98168', 'year_block_30', 'year_block_17', 'zipcode_98177', 'date_2015-02', 'year_block_3', 'loc_block_5', 'sqft_living3', 'loc_block_24', 'zipcode_98072', 'loc_block_17', 'loc_block_50', 'loc_block_25', 'zipcode_98070', 'zipcode_98024', 'year_block_25', 'year_block_28', 'loc_block_28', 'zipcode_98005', 'loc_block_26', 'zipcode_98198', 'loc_block_44', 'loc_block_22', 'year_block_4', 'year_block_2', 'date_2014-06', 'loc_block_11', 'loc_block_21']","23ec25a0":"# Run our ols regression again, using only the features recommended by our feature selector\n\nX_train_refined = X_train[result]\n\npredictors_int = sm.add_constant(X_train_refined)\nmodelols = sm.OLS(y_train, predictors_int).fit()\nmodelols.summary()","2ff22926":"print(\"{} predictors used\".format(len(result)))","a1137055":"# Score this feature set with NuSVR\nmodels = score_model(model, X_train_refined, y_train, \"LR Forward Backward\", models)","33ac63f5":"# We're doing the feature selection using a basic Linear Regression\nperm_model = LinearRegression()\nperm_model.fit(X_train, y_train)\n\n\n# Call the premutation_importance method\nr = permutation_importance(perm_model, X_train, y_train,\n                           n_repeats=15,\n                            random_state=0,\n                          n_jobs=-1)\n\nimportances = {}\n\nfor i in r.importances_mean.argsort()[::-1]:\n    if r.importances_mean[i] >= 0.001:\n        importances[X_train.columns[i]] = r.importances_mean[i]\n    else: continue\n        \nimportances\n\nimportant_features = list(importances.keys())\nprint(important_features)","afb2af02":"# set the features recommended by our feature selector\n\nX_train_perm = X_train[important_features]\n\npredictors_int = sm.add_constant(X_train_perm)\nmodelols = sm.OLS(y_train, predictors_int).fit()\nmodelols.summary()","cee7b44f":"print(\"{} predictors used\".format(len(important_features)))","a53aa222":"# Score this method with NuSVR\nmodels = score_model(model, X_train_perm, y_train, \"LR Perm Importance\", models)","5bb988c5":"# Using sklearn RFECV to perform integrated CV while picking the number of features\n# picks the number of features itself\n# Trying it with linear regression. This method takes too long with SVR.\n\nrfecv = RFECV(estimator=LinearRegression(), step=1, cv=5, scoring='neg_mean_absolute_error')\n\n# fit model to train set\nrfecv.fit(X_train, y_train)\n\n# print optimal number of features\nprint('Optimal number of features: {}'.format(rfecv.n_features_))","208e0c88":"dset = pd.DataFrame()\ndset['attr'] = X_train.columns\ndset['used'] = rfecv.support_\n\n# make a list of the features used in the rfecv\nrfecv_result = list(dset[(dset['used'] == True)]['attr'])\n\n# Show the features that RFECV did not use\ndset[dset['used']==False]","50ed3605":"plt.figure(figsize=(16, 9))\nplt.title('Recursive Feature Elimination with Cross-Validation', fontsize=18, fontweight='bold', pad=20)\nplt.xlabel('Number of features selected', fontsize=14, labelpad=20)\nplt.ylabel('R2', fontsize=14, labelpad=20)\nplt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_, color='#303F9F', linewidth=3)\n\nplt.show()","9cca3cfa":"# Score this method with LR\nmodels = score_model(rfecv, X_train, y_train, \"LR RFECV\", models)","aa8389de":"# Exploring the explained variance based on the various components\n\nfrom sklearn.decomposition import PCA\n\npca_all = PCA(n_components=X_train.shape[1], random_state=randomstate)\nx_pca = pca_all.fit_transform(X_train)\n\nprint(\"Explained variance with all components is \", sum(pca_all.explained_variance_ratio_ * 100))\nprint(\"1 component explains \", np.cumsum(pca_all.explained_variance_ratio_ * 100)[0])\nprint(\"1st and 2nd components explains \", np.cumsum(pca_all.explained_variance_ratio_ * 100)[1])\nprint(\"1st to 3rd components explains \", np.cumsum(pca_all.explained_variance_ratio_ * 100)[2])\nprint(\"1st to 4th components explains \", np.cumsum(pca_all.explained_variance_ratio_ * 100)[3])\n\nplt.plot(np.cumsum(pca_all.explained_variance_ratio_))\nplt.xlabel(\"Number of Components\")\nplt.ylabel(\"Explained Variance\")","4bc0222c":"# Reducing the dataset into principal components that explain 95% of the variance\n\npca_95th = PCA(n_components=.95, random_state=randomstate)\nx_pca_95 = pca_95th.fit_transform(X_train)\n\nprint(x_pca_95.shape[1],\"components, starting from all features\")","1e82c26b":"# Score PCA model\n\nmodels = score_model(model, x_pca_95, y_train, \"LR PCA\", models)","7d23ce1d":"# make data frame from our models dictionary\ntarget = pd.DataFrame(models).reset_index(drop=True)\n\n# sort data frame by mae and reset index\ntarget.sort_values('mae', ascending=False).head(20)","7e849c60":"# convert the train\/test data frames into arrays\nX_train = np.array(X_train)\nX_test = np.array(X_test)","27131598":"# initialize empty lists for the storage of out-of-fold predictions\n\nxgbr_yhat, gradient_boost_yhat, random_forest_yhat, neighbor_yhat, svr_yhat, nu_svr_yhat = [], [], [], [], [], []\north_yhat, linreg_yhat, ridge_yhat, bay_ridge_yhat, mlp_yhat = [], [], [], [], []","851f4b7d":"# instantiate tuned models\n\nxgbr = xgb.XGBRegressor(\n                n_estimators=500,\n                seed=randomstate,\n                missing=0,\n                eval_metric='mae',\n                verbosity=1,\n                max_depth = 7,\n                eta = .1,\n                min_child_weight = 5,\n                subsample=.6,\n                colsample_bytree = .4,\n                reg_alpha = 1e-5,\n                reg_lambda = 50)\n\ngradient_boost = GradientBoostingRegressor(\n                n_estimators=1000,\n                random_state=randomstate,\n                learning_rate = .05,\n                max_depth = 5,\n                max_features = 'auto',\n                loss = 'ls',\n                subsample = .7)\n\nrandom_forest = RandomForestRegressor(\n                n_estimators=1000,\n                random_state=randomstate,\n                n_jobs=-1,\n                max_depth = 20,\n                )\n\nneighbor = KNeighborsRegressor(n_neighbors = 11)\nsvr = SVR(gamma = 'scale', kernel = 'linear', C=10, epsilon=.05)\nnu_svr = NuSVR(kernel = 'rbf', gamma = 'scale')\north = OrthogonalMatchingPursuit()\nlinreg = LinearRegression()\nridge = Ridge(random_state = randomstate, tol=1e-3, normalize=False, solver='auto')\nbay_ridge = BayesianRidge(alpha_init=50, normalize=False)\nmlp = MLPRegressor(random_state = randomstate, solver='lbfgs', activation='logistic', alpha=1, hidden_layer_sizes=100)","a8961a3a":"# create the model dictionary to do out-of-fold predictions and model stack testing\n\nmodels_dict = {'XGB' : [xgbr, xgbr_yhat],\n            'Gradient Boosted' : [gradient_boost, gradient_boost_yhat],\n            'Random Forest' : [random_forest, random_forest_yhat],\n            'KNN' : [neighbor, neighbor_yhat],\n            'SVR' : [svr, svr_yhat],\n            'Nu SVR': [nu_svr, nu_svr_yhat],\n            'Orthogonal': [orth, orth_yhat],\n            'Linear Regression' : [linreg, linreg_yhat],\n            'Ridge' : [ridge, ridge_yhat],\n            'Bayesian Ridge': [bay_ridge, bay_ridge_yhat],\n            'MLP Perceptron': [mlp, mlp_yhat]\n              }","980ceba5":"# run the out-of-fold predictions\n# This takes a long time!!\n\ndata_x, data_y, models_dict = train_oof_predictions(X_train, y_train, models_dict)","b557ef43":"# Set up a scoring dictionary to hold the model stack selector results\nscores = {}\nscores['Model'] = []\nscores['MAE'] = []\nscores['Included'] = []\n\n# Run the model stack selector for each model in our models_dict\n\nfor model in models_dict:\n    \n    label = model\n    meta_model = models_dict[model][0]\n\n    resulting_models, best_mae = model_selector(data_x, data_y, meta_model, models_dict, label, verbose=True)\n    \n    scores['Model'].append(model)\n    scores['MAE'].append(best_mae)\n    scores['Included'].append(resulting_models)","51f2fa48":"# Look at the scores of our model combinations\n\nbest_model = pd.DataFrame(scores).reset_index(drop=True)\nbest_model.sort_values('MAE', ascending=False)","ec7dac1b":"# Fit the models to be used in the stack on the base Train datasets\nprint(\"Fitting Models\")\n\nprint(\"Fitting SVR\")\nsvr.fit(data_x, data_y)\n\nprint(\"Fitting XGB\")\nxgbr.fit(data_x, data_y)","89aaa9b7":"print(\"Fitting Bay Ridge\")\nbay_ridge.fit(data_x, data_y)\n\nprint(\"Fitting Gradient Boost\")\ngradient_boost.fit(data_x, data_y)\n\nprint(\"Fitting Nu SVR\")\nnu_svr.fit(data_x, data_y)","507a7fb8":"# make a list with the oof predictions from the stack models that we plan to use in our stack\nyhat_predics = [models_dict['XGB'][1],  models_dict['Bayesian Ridge'][1], models_dict['Gradient Boosted'][1], \n                models_dict['SVR'][1],  models_dict['Nu SVR'][1]]#, models_dict['Random Forest'][1]]\n\n# create the meta data set using the oof predictions\nmeta_X_train = create_meta_dataset(data_x, yhat_predics)","cdb744c8":"# Make a list holding the fitted models to be used in the stack\nfinal_models = [xgbr, bay_ridge, gradient_boost, svr, nu_svr]#, random_forest]\n\n# Create the test set, including the predictions of the stack models\nmeta_X_test = stack_prediction(X_test, final_models)\n\nmeta_X_test.shape","ad4fa1f5":"# Instantiate the chosen meta model\nmeta_model =  SVR(gamma = 'scale', kernel = 'linear', C=10, epsilon=.05)","ad257bd5":"# Check our meta model score on the non-stacked train\/test\n\nmeta_model.fit(X_train, y_train)\npredictions = meta_model.predict(X_test)\n\npred_exp = np.exp(predictions)\nactual = np.exp(y_test)\n\nprint(\"MAE: \",int(mean_absolute_error(pred_exp, actual)))\nprint(\"RMSE:\",int(np.sqrt(mean_squared_error(pred_exp, actual))))\nprint(\"R2:\", r2_score(pred_exp, actual)*100)","192ad20f":"# Instantiate the chosen meta model\n#meta_model =  SVR(gamma = 'scale', kernel = 'linear', C=10, epsilon=.05)\n\n# fit the meta model to the Train meta dataset\n# There is no data leakage in the meta dataset since we did all of our predictions out-of-sample\nmeta_model.fit(meta_X_train, data_y)\npredictions = meta_model.predict(meta_X_test)\n\npred_exp = np.exp(predictions)\nactual = np.exp(y_test)\n\nprint(\"MAE: \",int(mean_absolute_error(pred_exp, actual)))\nprint(\"RMSE:\",int(np.sqrt(mean_squared_error(pred_exp, actual))))\nprint(\"R2:\", r2_score(pred_exp, actual)*100)","b23f89c7":"param_grid = {\"max_depth\": [5, 10],\n              \"min_child_weight\" : [2, 7],\n              'eta': [.05, .1],\n              }\n\nxgbr = xgb.XGBRegressor(\n                n_estimators=500,\n                seed=randomstate,\n                missing=0,\n                eval_metric='mae',\n                verbosity=1)\n\nbest_params = grid_optimizer(xgbr, param_grid, X_train, y_train)","57bf79ad":"param_grid = {\"max_depth\": [3, 7],\n              \"min_child_weight\" : [2, 5],\n              'eta': [.1, .3],\n              }\n\nxgbr = xgb.XGBRegressor(\n                n_estimators=500,\n                seed=randomstate,\n                missing=0,\n                eval_metric='mae',\n                verbosity=1)\n\nbest_params = grid_optimizer(xgbr, param_grid, X_train, y_train)","f57a3ccd":"param_grid = {'gamma': [0, .6],\n              'subsample':[.6, 1],\n\n              }\n\nxgbr = xgb.XGBRegressor(\n                n_estimators=500,\n                seed=randomstate,\n                missing=0,\n                eval_metric='mae',\n                verbosity=1,\n                max_depth = 7,\n                eta = .1,\n                min_child_weight = 5)\n\nbest_params = grid_optimizer(xgbr, param_grid, X_train, y_train)","360952b8":"param_grid = {'colsample_bytree':[.6, 1],\n              'subsample':[.3, .6],\n              }\n\nxgbr = xgb.XGBRegressor(\n                n_estimators=500,\n                seed=randomstate,\n                missing=0,\n                eval_metric='mae',\n                verbosity=1,\n                max_depth = 7,\n                eta = .1,\n                min_child_weight = 5,\n\n                )\n\nbest_params = grid_optimizer(xgbr, param_grid, X_train, y_train)","fb95d193":"param_grid = {'colsample_bytree':[.2, .4, .6],\n              }\n\nxgbr = xgb.XGBRegressor(\n                n_estimators=500,\n                seed=randomstate,\n                missing=0,\n                eval_metric='mae',\n                verbosity=1,\n                max_depth = 7,\n                eta = .1,\n                min_child_weight = 5,\n                subsample = .6,\n                )\n\nbest_params = grid_optimizer(xgbr, param_grid, X_train, y_train)","aa9139ad":"param_grid = {'reg_alpha':[1e-5, 1, 100],\n              'reg_lambda':[1e-5, 1, 100],\n\n              }\n\nxgbr = xgb.XGBRegressor(\n                n_estimators=500,\n                seed=randomstate,\n                missing=0,\n                eval_metric='mae',\n                verbosity=1,\n                max_depth = 7,\n                eta = .1,\n                min_child_weight = 5,\n                subsample = .6,\n                colsample_bytree = .4,\n)\n\nbest_params = grid_optimizer(xgbr, param_grid, X_train, y_train)","11fb8d82":"param_grid = {'reg_lambda':[25, 50, 100],\n\n              }\n\nxgbr = xgb.XGBRegressor(\n                n_estimators=500,\n                seed=randomstate,\n                missing=0,\n                eval_metric='mae',\n                verbosity=1,\n                max_depth = 7,\n                eta = .1,\n                min_child_weight = 5,\n                subsample=.6,\n                colsample_bytree = .4,\n                reg_alpha = 1e-5,\n)\n\nbest_params = grid_optimizer(xgbr, param_grid, X_train, y_train)","270a5b81":"param_grid = {'n_estimators' : [500, 750, 1000],\n              }\n\nxgbr = xgb.XGBRegressor(\n                #n_estimators=500,\n                seed=randomstate,\n                missing=0,\n                eval_metric='mae',\n                verbosity=1,\n                max_depth = 7,\n                eta = .1,\n                min_child_weight = 5,\n                subsample = .6,\n                colsample_bytree = .4,\n                reg_alpha = 1e-5,\n                reg_lambda = 50)\n\nbest_params = grid_optimizer(xgbr, param_grid, X_train, y_train)","a9fce0f2":"xgbr = xgb.XGBRegressor(\n                n_estimators=500,\n                seed=randomstate,\n                missing=0,\n                eval_metric='mae',\n                verbosity=1,\n                max_depth = 7,\n                eta = .1,\n                min_child_weight = 5,\n                subsample=.6,\n                colsample_bytree = .4,\n                reg_alpha = 1e-5,\n                reg_lambda = 50)","ddf30db8":"param_grid = {\"max_depth\": [5, 10, 15],\n              'learning_rate': [.1, .3],\n              }\n\ngradient_boost = GradientBoostingRegressor(\n                n_estimators=500,\n                random_state=randomstate,\n                )\n\nbest_params = grid_optimizer(gradient_boost, param_grid, X_train, y_train)","ace59a57":"param_grid = {\"max_depth\": [3, 5, 7],\n              'learning_rate': [.05, .1],\n              }\n\ngradient_boost = GradientBoostingRegressor(\n                n_estimators=500,\n                random_state=randomstate,\n                )\n\nbest_params = grid_optimizer(gradient_boost, param_grid, X_train, y_train)","455ea332":"param_grid = {\"min_impurity_decrease\" : [0, .5],\n              'max_features': ['auto', 'sqrt', 'log2'],\n              }\n\ngradient_boost = GradientBoostingRegressor(\n                n_estimators=500,\n                random_state=randomstate,\n                learning_rate = .05,\n                max_depth = 5,\n                )\n\nbest_params = grid_optimizer(gradient_boost, param_grid, X_train, y_train)","f12ba614":"param_grid = {'subsample' : [.3, .7, 1],\n              'loss': ['lad', 'ls']\n              }\n\ngradient_boost = GradientBoostingRegressor(\n                n_estimators=500,\n                random_state=randomstate,\n                learning_rate = .05,\n                max_depth = 5,\n                max_features = 'auto',               \n                )\n\nbest_params = grid_optimizer(gradient_boost, param_grid, X_train, y_train)","cb30c941":"param_grid = {'n_estimators' : [500,1000,5000]\n              }\n\ngradient_boost = GradientBoostingRegressor(\n                n_estimators=500,\n                random_state=randomstate,\n                learning_rate = .05,\n                max_depth = 5,\n                max_features = 'auto',\n                loss = 'ls',\n                subsample = .7,\n                )\n\nbest_params = grid_optimizer(gradient_boost, param_grid, X_train, y_train)","e34a9c04":"gradient_boost = GradientBoostingRegressor(\n                n_estimators=1000,\n                random_state=randomstate,\n                learning_rate = .05,\n                max_depth = 5,\n                max_features = 'auto',\n                loss = 'ls',\n                subsample = .7,\n                )","081bb93f":"param_grid = {\"max_depth\": [5, 10, 15],\n              \"criterion\" : ['squared_error', 'absolute_error']\n              }\n\nrandom_forest = RandomForestRegressor(\n                n_estimators=500,\n                random_state=randomstate,\n                n_jobs=-1)\n\nbest_params = grid_optimizer(random_forest, param_grid, X_train, y_train)","b0e4c607":"param_grid = {\"max_depth\": [15, 20],\n              \"max_features\" : ['auto', 'sqrt', 'log2']\n              }\n\nrandom_forest = RandomForestRegressor(\n                n_estimators=500,\n                random_state=randomstate,\n                n_jobs=-1,\n                )\n\nbest_params = grid_optimizer(random_forest, param_grid, X_train, y_train)","76440f08":"param_grid = {'bootstrap': [True, False],\n              'min_samples_leaf' : [1, 10]\n              }\n\nrandom_forest = RandomForestRegressor(\n                n_estimators=500,\n                random_state=randomstate,\n                n_jobs=-1,\n                max_depth = 20,\n                )\n\nbest_params = grid_optimizer(random_forest, param_grid, X_train, y_train)","22550afb":"param_grid = {'n_estimators' : [500,1000]\n              }\n\nrandom_forest = RandomForestRegressor(\n                #n_estimators=500,\n                random_state=randomstate,\n                n_jobs=-1,\n                max_depth = 20,\n                )\n\nbest_params = grid_optimizer(random_forest, param_grid, X_train, y_train)","cfb1b817":"random_forest = RandomForestRegressor(\n                n_estimators=1000,\n                random_state=randomstate,\n                n_jobs=-1,\n                max_depth = 20,\n                )","ffa7f83f":"mae_val = [] #to store mae values for different k\n\n# checks mean absolute error scores on k from 1 to 25\nfor K in range(1, 20):\n    K = K+1 \n    # set up the KNN regressor\n    model = KNeighborsRegressor(n_neighbors = K)\n    # get accuracy cross val score for cv 5\n    scores = cross_validate(model, X_train, y_train, cv=5,\n        scoring=('neg_mean_absolute_error'),\n        return_train_score=True)\n    mae = round(scores['test_score'].mean(), 4) \n    mae_val.append(mae) #store mae values\n    print('MAE value for k= ' , K , 'is:', mae)\n    \n# gets optimal k-value based on score minimum\nindex_max = np.argmax(mae_val) + 1\nprint(index_max)","7a1eb3ee":"# Parameter Tuning\n\nparam_grid = {'kernel' : ['linear', 'rbf', 'poly'],\n              'gamma' : ['scale', 'auto']            \n              }\n\nsvr = SVR()\n\nbest_params = grid_optimizer(svr, param_grid, X_train, y_train)","8337b568":"# Parameter Tuning\n\nparam_grid = {'C' : [1, 10],\n              'epsilon' : [.05, .5]  \n              }\n\nsvr = SVR(gamma = 'scale', kernel = 'linear')\n\nbest_params = grid_optimizer(svr, param_grid, X_train, y_train)","db722000":"# Parameter Tuning\n\nparam_grid = {'C' : [5, 10, 15],\n              'epsilon' : [.05, .1]  \n              }\n\nsvr = SVR(gamma = 'scale', kernel = 'linear')\n\nbest_params = grid_optimizer(svr, param_grid, X_train, y_train)","4922306b":"param_grid = {'kernel' : ['linear', 'rbf', 'poly', 'sigmoid'],\n              'gamma' : ['scale', 'auto']            \n              }\n\nnu_svr = NuSVR()\n\nbest_params = grid_optimizer(nu_svr, param_grid, X_train_refined, y_train)","fe920813":"param_grid = {'nu' : [.25, .5, .75],\n              'C' : [1, 10]            \n              }\n\nnu_svr = NuSVR(kernel = 'rbf', gamma = 'scale')\n\nbest_params = grid_optimizer(nu_svr, param_grid, X_train_refined, y_train)","2054a9a2":"# Parameter Tuning\n\nparam_grid = {'solver' : ['auto', 'saga', 'cholesky', 'lsqr'],\n              }\n\nridge = Ridge(random_state = randomstate, tol=1e-3, normalize=False)\n\nbest_params = grid_optimizer(ridge, param_grid, X_train_onehot, y_train)","67b8aed9":"param_grid = {'alpha' : [1, 5, 50],\n              'normalize' : [True, False]            \n              }\n\nridge = Ridge(random_state = randomstate, tol=1e-3, normalize=False, solver='auto')\n\nbest_params = grid_optimizer(ridge, param_grid, X_train_onehot, y_train)","fd4bc45f":"ridge = Ridge(random_state = randomstate, tol=1e-3, normalize=False, solver='auto')","28031cee":"param_grid = {'alpha_init' : [1, 5, 50],\n              'normalize' : [True, False]            \n              }\n\nbay_ridge = BayesianRidge()\n\nbest_params = grid_optimizer(bay_ridge, param_grid, X_train_onehot, y_train)","9ad68f8c":"bay_ridge = BayesianRidge(alpha_init=50, normalize=False)","dc3a5164":"param_grid = {'solver' : ['lbfgs', 'sgd', 'adam']            \n              }\n\nmlp = MLPRegressor(random_state = randomstate)\n\nbest_params = grid_optimizer(mlp, param_grid, X_train_onehot, y_train)","dbc4be71":"param_grid = {'hidden_layer_sizes' : [100, 500],\n              'activation' : ['identity', 'logistic'],\n              'alpha':[.0001, 1]\n              }\n\nmlp = MLPRegressor(random_state = randomstate, solver='lbfgs')\n\nbest_params = grid_optimizer(mlp, param_grid, X_train_onehot, y_train)","5f939371":"mlp = MLPRegressor(random_state = randomstate, solver='lbfgs', activation='logistic')","b38e007c":"Time to check and remove any features with high collinearity.","66b516db":"## Visualize Categoricals","6602cd82":"* Perform exhaustive and accurate data cleaning\n* Explore different categorical handling methods\n* Build a stacked model that accurately predicts house prices in King County","df5b91c1":"Best parameters set found on train set: \n\n{'activation': 'logistic', 'alpha': 0.0001, 'hidden_layer_sizes': 100}","77d66603":"##### Manually locating zip\/price outliers","1b729e68":"![Fold example](https:\/\/i.imgur.com\/2dYvF53.png)","62f29fb4":"## Study Target Variable","62619d69":"# Obtaining Our Data","ed04da84":"Best mae: -0.107\nBest parameters set found on train set: \n\n{'colsample_bytree': 0.6, 'subsample': 0.6}","f61d0b63":"We're ready to make our final continuous data set.","89f0bad0":"Here's a fun way to see the improvements to our data quality after we cleaned outliers! A much deeper color map.","daa80d62":"This was a significant decrease in MAE from 53136 to 47535. Our model stack worked!","eca47dc2":"### Gradient Boosted Trees","ccc2d7f0":"![Spot Selector Results](https:\/\/i.imgur.com\/rrLVJmy.png)","c2214531":"We will express this as a second degree polynomial.","80bfae5e":"## Finding Interactions","16144127":"The test_feature_combinations function finds all of the feature combinations possible in our dataset. Then for each combination, the function runs a linear regression with cross validation on 5 folds and gets the r^2 score for the regression including that feature combination. All scores are recorded and r^2 score improvement is assessed, with the resulting table giving the increase in model improvement from a feature combo. ","b929786a":"### Feature Selectors","6456be79":"K-Nearest Neighbors is more commonly used for classification. Its basic premise is to determine \"what is this like\" in making a prediction, by looking at other things that are close in value\/type. We can pick how many neighbors it assesses to make a classification. As we will see, it doesn't work very well for this type of application (or, I've not tuned the hyperparameters properly and\/or don't know how to use it well).","b39551b5":"We're going to artificially stratify our train\/test sets so that we get samples for all price ranges in both train and test sets. To stratify a continuous target we're using the strategy outlined by Michael J Sanders, located here: https:\/\/michaeljsanders.com\/2017\/03\/24\/stratify-continuous-variable.html","fdd60f15":"Best mae: -0.108\nBest parameters set found on train set: \n\n{'eta': 0.1, 'max_depth': 5, 'min_child_weight': 2}","de15f661":"### K-Nearest Neighbors Model","7654a18a":"Best mae: -0.114\nBest parameters set found on train set: \n\n{'gamma': 'scale', 'kernel': 'linear'}","95e4f33f":"## Add Polynomial Features","5c0d2524":"Best mae: -0.109\nBest parameters set found on train set: \n\n{'learning_rate': 0.05, 'max_depth': 5}","d58398ac":"### Random Forest","fca90570":"## Correlations\/Multicollinearity","4af707b0":"For our model stack we will explore:\n\n* NuSVR\n* XGB\n* SVR\n* Ridge\n* Linear Regression\n* Bayesian Ridge\n* Random Forest\n* Gradient Boosted\n* Orthogonal Matching Pursuit\n* MLP\n* KNN\n\nExtra trees performed okay but takes far too long.","76fcd7b6":"Best mae: -0.107\nBest parameters set found on train set: \n\n{'n_estimators': 1000}","447c7c29":"![Stack Selector in action](https:\/\/i.imgur.com\/iDsehRJ.png)","259f5103":"In order to keep track of our results, we'll be making a dictionary to store our model scores.","a239d949":"# Exploring\/Visualizing Data","2c55ad11":"### Get OOF Predictions","c9796440":"We see no real improvements based on feature combinations, and opt to add none.","ee1a99a8":"Before we make a stacked model, we try the meta model on the test set so we can get a comparison for the stacked model\u2019s improvements! Fit and predict the meta model on the train\/test set. This is the baseline we\u2019re expecting to beat with a stacked model.","e1e5a8b4":"## Checking for Time Series Trends","43f9954d":"We run a Dickey-Fuller hypothesis test on our time series to test for stationarity. The null hypothesis is that the data is not stationary. If we have a p-value below .05, we will reject the null hypothesis.","228a3100":"Looking at our price per zip code in a different way, we can see a potential issue with our data. Most of our data set is fairly normal and expected. Then we have three zip codes giving us very strong right tails. Then we have ONE zip code way off yonder which may cause problems with our model. ","0d11f324":"Best mae: -0.114\nBest parameters set found on train set: \n\n{'C': 10, 'epsilon': 0.05}","8520c964":"Next we're going to select features using a method called permutation importance. This is a great model-agnostic method that you can use with any model type, and the way it works is very easy to understand. After fitting the model, it calculates a baseline R^2 score. Then for each feature, it scrambles the inputs of that feature, turnings its contribution into noise. The model is evaluated again with the feature scrambled, and the change in overall R^2 is logged as the importance for that feature. After scrambling all features, each feature has been assigned an importance based on the R^2 reduction. You can then select the features that had an effect on R^2 based on your own threshold (I kept anything >= .001) and throw out the remaining features.\n\nYou can learn more about this underrated feature selection method here: https:\/\/explained.ai\/rf-importance\/\nThe article focuses on Random Forest, but discusses permutation importance as an excellent feature selection method for any model type.","045b8e20":"### Prepare Testing Assets","9c38bc0b":"## Objective","2d17f6b4":"Best parameters set found on train set: \n\n{'alpha_init': 50, 'normalize': False}","2511abc7":"Best mae: -0.113\nBest parameters set found on train set: \n\n{'C': 1, 'nu': 0.5}","5fa55ceb":"Gradient Boosting performs best with optimal parameter tuning. We're going to use sklearn's GridSearchCV to find the optimal hyperparameters to use with our gradient booster! Here are the parameters we are trying out:\n\n* n_estimators: Number of boosts to perform. Gradient boosting is pretty robust to over-fitting so more is usually better\n* max_depth: This determines how many tree nodes the estimator looks at before making a prediction. We don't know what is best here, so we are trying things from 2-4 to see what works the best\n* min_child_weight: Min sum of instance weight needed in a child\n* gamma: Minimum loss reduction to make another partition on a leaf node. Higher results in more conservative algorithm.\n* subsample: Ratio of training sets. .5 means that it will sample half the training data before making trees. Occurs with each boosting iteration.\n* colsample_by_tree: ratio of columns when making a tree\n* alpha: L1 regularization. Higher will make model more conservative.\n* learning_rate: Tuning this setting alters how much the model corrects after it runs a boost. .1 is a common rate and we will test a lower and higher rate as well.","90790129":"Let's look closer at the feature importances","5b092b6f":"Best parameters set found on train set: \n\n{'solver': 'lbfgs'}","64490c08":"## Final Model Setup","7662a5f4":"### Spot Check Evaluation","a325c1a8":"Best parameters set found on train set: \n\n{'solver': 'auto'}","c68ea316":"## Spot Check Models","beaf0e34":"# Scrubbing\/Cleaning Data","6cb882b8":"We now need to process our holdout in the same way we processed our training set.","25a512e4":"## Package Imports","eee08187":"### Cleaning Functions","edc6e17e":"Best mae: -0.107\nBest parameters set found on train set: \n\n{'reg_lambda': 50}","9dc45ce0":"This plot of price per total square feet of living space, colored by the zip code median rank, visually shows the importance of location. Let's look at that closer.","cdd78482":"### Ridge","6a9d6147":"Using latitude and longitude, we make a visual map of the King County area that lets us see the map outliers. We can see that there are a few sales in locations that are outliers in some way - still in King County but very far away from the metro area on which we are focusing. Sales that are too far outside of our comparison area geographically may add noise to our model. So I'm going to drop the longitude outliers as well. \n\nThis visualization suggests that location is very important to home price. We're going to check that out more directly.","ea62241e":"PCA is a dimensionality reduction method. It seeks to reduce the size of the feature space into a more manageable feature set by mathematically combining like-features while retaining a specified amount of the variance explainability.","9c3e2e25":"Best mae: -0.109\nBest parameters set found on train set: \n\n{'max_features': 'auto', 'min_impurity_decrease': 0}","f83264d6":"In order to find our best parameters, we use GridSearchCV. GridSearch takes quite a while, so these sections are commented out as we iterate through different search sets, tuning one parameter at a time. My results are copied in markdown after each section.\n\nWe COULD tune all of these at once with one huge grid search. I try to avoid this because it takes several hours, and we can get a similar result with careful iterative tuning.\n\nThe testing iterations are found in the appendix.","d58f2232":"Best mae: -0.108\nBest parameters set found on train set: \n\n{'eta': 0.1, 'max_depth': 7, 'min_child_weight': 5}","272a5efa":"## Visualize Continuous","a8e76557":"The following cell runs out of memory on Kaggle, so several of our outputs\/results will be presented as images from my local notebook.","b4208304":"# Cleaning Final Data","ded11da6":"Best parameters set found on train set: \n\n{'alpha': 1, 'normalize': False}","aefccb48":"## Duplicate Data","1c3470af":"### Prepare Final Assets","dc903b4b":"RFECV is a reverse forward-backward selector. It starts the model with all features in use then removes the weakest one, and iterates until the best feature set is found. It uses integrated cross-validation to determine the optimal set of features in the model with the best cross-validated score. We score on mean absolute error.","6e23e30a":"We'll add year built as a second degree polynomial.","632fedad":"Month sold has no polynomial relationship","9361e0d2":"##### Process Continuous","6ca25ee8":"## Feature Engineering","27c124dd":"Fun fact - \"grade\" was not well defined in the features list. I looked it up and found that grade is \"\tClassification by construction quality which refers to the types of materials used and the quality of workmanship. Buildings of better quality (higher grade) cost more to build per unit of measure and command higher value.\" So, it's the quality of building materials, and DEFINITELY worth keeping in our model.","1cf396d2":"# APPENDIX - PARAMETER TUNING","cc33ab13":"### Tuning Functions","764c8019":"Now we'll look for bad data in other categories. This is easy to do using describe.","2df82d88":"The p-value of 3.23e-16 on the Dickey-Fuller test is FAR under .05, so we reject the null hypothesis that there is a trend in the data. \nOur data set is not exhibiting time series trends, so we don't need to account for this (surprisingly!!)","b1f7212c":"## Binary data","23e5fcc9":"### Preprocessing Functions","8f7c1974":"Best mae: -0.108\nBest parameters set found on train set: \n\n{'reg_alpha': 1e-05, 'reg_lambda': 100}\n\nNOT an improvement over no alpha\/lambda","ca6a162a":"Seems like year built might have a polynomial relationship with price","e3977b17":"Feature selectors are different methods to help us pick which features we want to use in our model. In our example above where we used ALL predictors in our linear regression, several of our features had a p-value over .05, which indicates that there is more than a 5% chance that the changes attributed to that feature were actually by random chance. We want features where our p-value is below a threshold that we specify where we are reasonably confident that the feature is contributing to the model and not by random chance.","0835d12e":"Best mae: -0.108\nBest parameters set found on train set: \n\n{'loss': 'ls', 'subsample': 0.7}","81f51a4b":"Best mae: -0.109\nBest parameters set found on train set: \n\n{'learning_rate': 0.1, 'max_depth': 5}","d27e96f2":"### Run Stack Selector","3e743a99":"Support vector regression is a form of regression that allows us to define the acceptable error in our model and then finds the line that best fits the data, according to our specifications. This is really useful with something like housing price predictions, where we are ok with our prediction being within a certain dollar amount. SVR will attempt to get all of the predictions within that dollar amount when possible. This will result in a fit line that is different than a linear regression would have produced, but should result in a lower absolute error, which is a reasonable scoring metric for housing price predictions.","5ed7a36c":"## Create Holdout Set","319220f4":"## Final Model Evaluation","e25700ab":"##### Target Encoding","367d4c97":"##### Process Categoricals","6d15ed0a":"## Create Train\/Test Final Set","9c1dbca0":"## Standardize and Transform","363d2539":"### Scoring Functions","49a9f88e":"Why should we spend the time excising just a few outliers from thousands of rows of data? Because it's the right thing to do to clean well and improve our data quality.","a9c637e1":"## Outlier Detection","5085bc50":"This looks better expressed with a third degree polynomial","6ebe6910":"One thing this data set lacks is *type of sale*. We want a model that properly describes the open market, but this dataset definitely includes sales that are outside the market, such as inter-family sales or other sales that don't properly reflect market value. The best way I can think of to find these outliers is to find suspiciously low sales based on location, because we can see in the lat\/long scatter that location makes a difference. We will hunt these down manually even though that takes some time.","82976b97":"![Model Stacks](https:\/\/i.imgur.com\/W1q3Dpq.png)","7926816e":"##### Other outlier detection","94a6f5f6":"Best mae: -0.114\nBest parameters set found on train set: \n\n{'C': 1, 'epsilon': 0.05}","bdc54e5c":"Our first step is to check if our data has a time series trend. This data covers home prices for a period of a year, and a lot can change in a year for real estate. If we find a trend in our home prices, we'll want to correct and remove this trend before continuing.","0beb1d4f":"### Stacking Functions","e33f8d8c":"No clear relationship","eaf3b131":"![Spot Checks Completed](https:\/\/i.imgur.com\/QRz4ewJ.png)","c58c3e99":"Best mae: -0.119\nBest parameters set found on train set: \n\n{'criterion': 'squared_error', 'max_depth': 15}","5ec9e291":"### Support Vector Regression","c4e636e9":"## Polynomial Relationships","87674201":"We're going to do some target-encoded feature engineering on our data set, using additive smoothing based on this article by Max Halford https:\/\/maxhalford.github.io\/blog\/target-encoding\/\n\nUsing this method we'll add features for:\n   * smoothed zip code\n   * smoothed year\n   * smoothed month sold\n   * smoothed latitude\n   * smoothed longitude\n   * lat\/long zone as function of smoothed lat and smoothed long\n\nThis feature engineering is the primary reason that we chose to separate our holdout set before any other processing. We cannot, under any circumstances, use the test data to inform our train set features!","2b9009b7":"There does not appear to be a polynomial relationship","acd865f6":"##### Reset Indices","2735e9fd":"I'll be eliminating price outliers on a per-zipcode basis using the same IQR range. My goal here is not to strip usable data, but to identify and excise any rogue non-standard sales. If I were to do this dataset-wide, many zipcodes would retain their own outliers. We can see in the visualizations just above that zipcodes each have their own price distribution. So we keep our price outlier detection within zipcode.","19918ae9":"### MLP Regressor","93419491":"**Recursive Feature Elimination with Cross Validation - Linear Regression**","1f0d79a5":"Best mae: -0.117\nBest parameters set found on train set: \n\n{'bootstrap': True, 'min_samples_leaf': 1}","7e2f6c84":"Best parameters set found on train set: \n\n{'gamma': 'scale', 'kernel': 'rbf'}","3fe8cd85":"We will log transform our data, and standardize our continuous inputs.","0613952a":"First we'll try a simple forward-backward feature selection model based on p-value, using a statsmodel OLS linear regression model. This selector starts with zero features, internally runs the model for each feature individually, and adds the lowest p-value feature to its list to include. It then runs the model again with the original feature included and tries adding each other feature individually. It will either add the next best feature under the threshold or remove an existing feature if it is no longer within the threshold. This process iterates until all features in the model are under the p-value threshold.\n\nThe selector was written by David Dale: https:\/\/datascience.stackexchange.com\/questions\/937\/does-scikit-learn-have-forward-selection-stepwise-regression-algorithm","8d931e0b":"median_zip and zip_rank are dummy variables that we won't be using, so we can ignore correlations using those for now.\n\nWe also drop sqft_above and sqft_lot15","740ce633":"We need to separate our holdout data before any further processing.\n\nThe reasons for this are:\n\n    * We will standardize our continuous variables, and you should standardize only on your train set and apply that to your test set.\n    * We will be feature engineering on our train set, and applying that later to our test set. We cannot have our test set data leak into our engineered features.","46404699":"Same score, but no features above our threshold.","16e4a810":"Best mae: -0.107\nBest parameters set found on train set: \n\n{'colsample_bytree': 0.4}","c0a10ddf":"Best mae: -0.108\nBest parameters set found on train set: \n\n{'gamma': 0, 'subsample': 0.6}","d8d42a24":"## Table of Contents\n\n* **Project Overview**\n    * Table of Contents\n    * Objective\n    * Package Imports\n    * Notebook Functions\n\n* **Obtaining our Data**\n\n* **Scrubbing\/Cleaning Data**\n    * Checking Time Series\n    * Duplicate Data\n    * Outlier Detection\n    * Binary Data\n\n* **Exploring\/Visualizing Data**\n    * Study Target Variable\n    * Create Holdout Set\n    * Feature Engineering\n    * Correlations and Multicollinearity\n    * Polynomial Relationships\n    * Visualize Categoricals\n    * Visualize Continuous\n    * Feature Interactions\n\n* **Cleaning Final Data**\n    * Standardize and Transform\n    * Add Polynomial Features\n    * Process Test Set\n    * Create Train\/Test Final Set\n\n* **Modeling**\n    * Spot Check Models\n        * Feature Selectors\n        * Spot Check Evaluation\n    * Final Model Setup\n        * Prepare Testing Assets\n        * Get OOF Predictions\n        * Run Stack Selector\n        * Prepare Final Assets\n    * Final Model Evaluation\n\n* **Analysis**\n\n* **APPENDIX**","3f19b25e":"**Forward-Backward Selector**","6d11fbc6":"Now we'll run our model stack selector! This definitely doesn't run on Kaggle, but I've included an example output of some of the selector in action.","426d5f1d":"We should never remove outliers indiscriminately, especially if they contain real data. Ultimately I opt to remove via IQR on sqft_living and sqft_lot, with reservations. A lot of our variables are not normally distributed, so we can't reliably remove outliers via standard deviation.\n\nFor the square footage variables, I ultimately concluded that extremely large houses and lots are so seriously under-represented in the dataset that we won't be able to reliably predict on them anyway and they are better left off.\n\nIn order to prevent a lot of data loss in this way, I kept IQR range of 1.6 instead of the standard 1.5","d947e0d3":"### Bayesian Ridge","078607d4":"## Process Test Set","186c8ff5":"# Project Overview\n\n> This is an in-depth notebook which explores the King County Housing Dataset through several different models. The notebook includes a thorough EDA and cleaning section, numerous visualizations, exploration of different models, feature selection methods, and a model stack to create the final model.\n\n> You can read my tutorial about simple model stacking on Medium: https:\/\/towardsdatascience.com\/simple-model-stacking-explained-and-automated-1b54e4357916","aa364085":"    * sqft_living, sqft_living15, zip_smooth, lat_long, sqft_basement(when present) have a very strong visual relationship with price\n    * sqft_lot do not appear to have a strong relationship with price\n    * year_smooth and month_smooth are unclear, we will investigate further","a02a2a64":"First we'll glance at feature importances using OLS regression in Statsmodels","3d601f88":"# Modeling","cf48a3d7":"### Visualization Functions","6f4b73a6":"We're going to use sklearn's GridSearchCV to find the optimal hyperparameters to use with our SVM! Here are the parameters we are trying out:\n\n* kernel: linear is parametric, and rbf is non-parametric. One of these should perform better. Our data is not totally normal, so it might be rbf.\n* epsilon: This value is how much error we're ok with accepting without assigning a penalty to the model\n* C: The error that we will accept from a point outside our epsilon\n\nOur C and epsilon need to be in scale with our output variable, which is our log-transformed price.\n","60d6de29":"# Analysis\n\n> Our final model utilizes a combination of continuous variables and one-hot-encoded categoricals to produce a stacked model with R^2 of 89.78%, a mean absolute error of 47.2k, and a root mean squared error of 74k. I tried several different zip code transformations including polynomial features, mean target encoding, lower-granularity binning, and median rank as a continuous, and ALL of these efforts resulted in a lower R^2 and higher mean absolute error, leading to a final decision to one-hot encode all 70 zip codes individually. Similar efforts on other categoricals such as age and month sold also did not improve the model over one-hot encoding. This resulted in the greatest accuracy despite a model that is more \"messy\" with a large number of features.\n\n### What are the primary factors influencing housing prices in the King County metro area?\n> As square footage increases so does quality of materials. Most importantly you can see the upward price trend with both increased square footage and materials grade. I was intrigued that our lower bound of data points is very linear, but as our square footage increases, the upper bound gradually breaks away with higher variance. \n\n>I ranked the 70 zip codes in King County by median home value, and used those ranks to color our data points.  Our low median zip codes have a low price per square footage, and price per square foot increases with zip code median, which makes sense, but also shows the importance of zip code to pricing. I found it interesting that while most zip codes exhibit a clear trend of price per square foot decreasing with increased total square footage, which is entirely normal, certain very high value zip codes seem to retain their high price per square foot regardless of total square footage. Certain zip codes seem immune to the usual price per square foot decay. \n\n> As they say, location is everything, and it is the primary influencing factor for a home price in the King County metro area. Our darkest areas, and therefore highest value sales, are clustered in and around Seattle to the west of Lake Washington and into the eastern lake cities of Bellevue and Redmond which are the technical employer hubs of the region. As we move away from Seattle and the tech hubs into the suburbs, our prices clearly go down.\n\n> These three features alone explain 85% of the price variance.\n\n### Can we effectively use a regression model based system for realtors to determine a proper list price?\n> Our model, while explaining over 89.78% of the price variance with our features, was nonetheless far from accurate in absolute terms. A mean average error of 47.2k in either direction is a huge variance to a home price - one that is so large that it renders the prediction much less meaningful and useful. Other models need to be explored, better data needs to be sourced, or easy-to-use features that an average realtor is capable of evaluating\/acquiring should be added to the model to improve its predictive quality. The model is providing at best a baseline starting point.\n","611af7bd":"**Principal Component Analysis**","0ffb701b":"**All model tuning is located in the APPENDIX**","91fdc16e":"Best mae: -0.117\nBest parameters set found on train set: \n\n{'max_depth': 20, 'max_features': 'auto'}","9c02dfff":"## Notebook Functions","2b63d115":"Time to run our OOF predictions function. We're sending in our train set, our train targets, and the model dictionary that we just set up. We'll have returned to us a re-ordered train and target set, as well as a model dictionary with full yhat prediction lists.\n\nThis won't run on Kaggle, but I've included an example of the results.","354d9386":"### Nu SVR","eeb1c212":"There are a few columns that I might want to be binary flags. I need to see what value choices are in these columns.","392ee74d":"### XGBoost","330bd8a1":"**Permutation Importance**","fd41641e":"Our Permuted Features gave a clear reduction to our MAE in the spot checks, as well as reduced the dimensionality of our data, so we will use this reduced feature space for our final models.","1ba1ba43":"It looks like zip code (target encoded) works better than the lat\/long zones I made. We should also note here that zip and lat\/long descriptors are all highly correlated, so we should consider if we need to include all of them in our model or just the best one (zipcode)\n\nWe can get a sense of the most important features to our price from our correlation table. Zipcode as a plain variable does not correlate, which makes sense, because without some sort of transformation it is an arbitrary unordered number. We can see how transformed as zip_smooth, median_zip, or zip_rank it becomes the MOST important contributor to price. We can see here that big contributors to price include:\n    \n    * zip code (in some altered form, not as arbitrary number)\n    * grade\n    * sqft_living\n    * bathrooms","d8b67585":"We see potential outliers in price, sqft_lot, sqft_living, and bedrooms.","2938e5cd":"Grade appears to have a polynomial relationship"}}