{"cell_type":{"e7f9fbe9":"code","fc0aee15":"code","244b8881":"code","06f12b19":"code","fca3ff9a":"code","ee296461":"code","15a6d5a8":"code","547f5b65":"code","6db14414":"code","27a429bc":"code","721dd5d4":"code","f1b525ec":"code","1937c51e":"code","38afe6be":"code","c420b241":"code","f54f6bb2":"code","ad103b62":"code","425dd679":"markdown","34fcf1ad":"markdown","509b57fb":"markdown","9b9ddde3":"markdown"},"source":{"e7f9fbe9":"import pandas as pd\nimport numpy as np\n\ntrain = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/train.csv\")\ntest = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/test.csv\")\nsub = pd.read_csv(\"..\/input\/tabular-playground-series-jan-2021\/sample_submission.csv\")\ntrain","fc0aee15":"train = train.drop([\"id\"], axis=1)\nfeatures = [c for c in train.columns if \"cont\" in c]\ntest = test.drop(\"id\", axis=1)\ntest","244b8881":"fe = dict(\n    rankgauss = True,\n    stats = True,\n    gaussmix = False,\n    pca = True,\n    tsne = True,\n    umap = True,\n    drop_original = False,\n)","06f12b19":"all_data = pd.concat([train, test], axis=0, ignore_index=True)\ntargets = all_data.target[:300000]\nall_data = all_data.drop(\"target\", axis=1)\nCOLS = [c for c in all_data.columns if \"cont\" in c]\nall_data","fca3ff9a":"import tqdm\n\nif fe[\"stats\"]:\n    for stats in tqdm.tqdm([\"sum\", \"var\", \"mean\", \"median\", \"std\", \"kurt\", \"skew\"]):\n        all_data[\"cont_\" + stats] = getattr(all_data[COLS], stats)(axis = 1)\n        \nall_data","ee296461":"import sys\nsys.path.append(\"..\/input\/rank-gauss\")\nfrom gauss_rank_scaler import GaussRankScaler\n\nif fe[\"rankgauss\"]:\n    scaler = GaussRankScaler()\n    rankgauss_feat = scaler.fit_transform(all_data[COLS])\n    rankgauss_df = pd.DataFrame(rankgauss_feat, columns=[f\"rankgauss_{i}\" for i in range(rankgauss_feat.shape[1])])\n    all_data = pd.concat([all_data, rankgauss_df], axis=1)\nall_data","15a6d5a8":"from sklearn.mixture import GaussianMixture\n\nif fe[\"gaussmix\"]:\n    def get_gmm_class_feature(feat, n):\n        gmm = GaussianMixture(n_components=n, random_state=42)\n\n        gmm.fit(all_data[feat].values.reshape(-1, 1))\n\n        all_data[f'{feat}_class'] = gmm.predict(all_data[feat].values.reshape(-1, 1))\n\n    get_gmm_class_feature('cont1', 4)\n    get_gmm_class_feature('cont2', 10)\n    get_gmm_class_feature('cont3', 6)\n    get_gmm_class_feature('cont4', 4)\n    get_gmm_class_feature('cont5', 3)\n    get_gmm_class_feature('cont6', 2)\n    get_gmm_class_feature('cont7', 3)\n    get_gmm_class_feature('cont8', 4)\n    get_gmm_class_feature('cont9', 4)\n    get_gmm_class_feature('cont10', 8)\n    get_gmm_class_feature('cont11', 5)\n    get_gmm_class_feature('cont12', 4)\n    get_gmm_class_feature('cont13', 6)\n    get_gmm_class_feature('cont14', 6)\n    CLASS_COLS = [c for c in all_data.columns if \"_class\" in c]\n    CLASS_COLS_IDX = []\n    for c in CLASS_COLS:\n        CLASS_COLS_IDX.append(all_data.columns.get_loc(c))\n    assert len(CLASS_COLS) > 0\nall_data","547f5b65":"from sklearn.decomposition import PCA\n\nif fe[\"pca\"]:\n    pca = PCA(n_components = 0.9, random_state = 42).fit(all_data[COLS])\n    pca_feat = pca.transform(all_data[COLS])\n    pca_df = pd.DataFrame(pca_feat, columns = [f\"pca_cont{i}\" for i in range(pca.n_components_)])\n    all_data = pd.concat([all_data, pca_df], axis=1)\n    PCA_COLS = [c for c in all_data.columns if \"pca\" in c]\n    assert len(PCA_COLS) > 0\n\nall_data","6db14414":"from cuml import TSNE\n\nif fe[\"tsne\"]:\n    tsne_components = 2\n    \n    perplexity = [5, 10, 15, 20, 25]\n    for per in perplexity:\n        tsne = TSNE(n_components = tsne_components, perplexity = per, n_neighbors = 3.5 * per)\n        tsne_feat = tsne.fit_transform(all_data[COLS])\n        tsne_df = pd.DataFrame(tsne_feat, columns=[f\"tsne_{per}_{i}\" for i in range(tsne_components)])\n        all_data = pd.concat([all_data, tsne_df], axis = 1)\n    TSNE_COLS = [c for c in all_data.columns if \"tsne\" in c]\nall_data","27a429bc":"from cuml import UMAP\n\nif fe[\"umap\"]:\n    umap_components = 10\n    umap = UMAP(n_components = umap_components)\n    umap_feat = umap.fit_transform(all_data[COLS])\n    umap_df = pd.DataFrame(umap_feat, columns=[f\"umap{i}\" for i in range(umap_components)])\n    all_data = pd.concat([all_data, umap_df], axis=1)\n    UMAP_COLS = [c for c in all_data.columns if \"umap\" in c]\n    assert len(UMAP_COLS) > 0\nall_data","721dd5d4":"if fe[\"drop_original\"]:\n    all_data = all_data.drop(COLS, axis=1)","f1b525ec":"train = all_data[:300000]\ntest = all_data[300000:]\nfeatures = list(all_data.columns)","1937c51e":"all_data","38afe6be":"from sklearn.model_selection import train_test_split\nfrom catboost import Pool, CatBoostRegressor\n\ndtypes = {c: \"int8\" for c in train.columns if \"_class\" in c}\ntrain = train.astype(dtypes)\nX_train, X_val, y_train, y_val = train_test_split(train, targets, test_size=0.1, random_state=42)\ntrain_pool = Pool(X_train, y_train, cat_features = [c for c in train.columns if \"_class\" in c])\nval_pool = Pool(X_val, y_val, cat_features = [c for c in train.columns if \"_class\" in c])","c420b241":"from hyperopt import fmin, tpe, hp, STATUS_OK\nfrom functools import partial\n\nITERATIONS = 5000\nMAX_EVALS = 150\ndef objective_func(params, train_pool, val_pool):\n    model = CatBoostRegressor(iterations = ITERATIONS,task_type=\"GPU\", devices='0:1', grow_policy=\"Lossguide\",\n                              loss_function = \"RMSE\", custom_metric = \"RMSE\", eval_metric=\"RMSE\", verbose = 1000, **params)\n    model.fit(train_pool, eval_set = val_pool, early_stopping_rounds = 200, plot=False)\n    loss = model.get_best_score()\n    return {\"loss\": loss[\"validation\"][\"RMSE\"], \"status\": STATUS_OK}\n        \nspace = {\n    \"learning_rate\": hp.loguniform(\"learning_rate\", np.log(0.01), np.log(1)),\n    \"max_depth\": hp.quniform(\"max_depth\", 6, 11, 1),\n    \"l2_leaf_reg\": hp.quniform(\"l2_leaf_reg\", 1, 5, 0.9),\n    \"bagging_temperature\": hp.quniform(\"bagging_temperature\", 0, 4, 0.9),\n    \"min_data_in_leaf\": hp.quniform(\"min_data_in_leaf\", 1, 42, 1),\n    \"max_leaves\": hp.quniform(\"max_leaves\", 2**4-1, 2**6-1, 1),\n}\n\n\nfn = partial(objective_func, train_pool=train_pool, val_pool=val_pool)\nbest_params = fmin(fn = fn, space=space, algo=tpe.suggest, max_evals = MAX_EVALS)\n\n\nmodel = CatBoostRegressor(iterations = ITERATIONS,task_type=\"GPU\", devices='0:1', grow_policy=\"Lossguide\",\n                              loss_function = \"RMSE\", custom_metric = \"RMSE\", eval_metric=\"RMSE\", verbose = 200, **best_params)\nmodel.fit(train_pool, eval_set = val_pool, early_stopping_rounds = 200, plot=True)","f54f6bb2":"sub[\"target\"] = model.predict(test)\nsub","ad103b62":"sub.to_csv(\"submission.csv\", index=False)","425dd679":"# Train the model\nWe will not use any Feature Engineering technique to test the model's power!","34fcf1ad":"# Feature Engineering","509b57fb":"# Drop the ID column","9b9ddde3":"# Install TabNet and load data"}}