{"cell_type":{"af273dea":"code","7c5f8c1b":"code","2be1480b":"code","59c32d20":"code","f16ef3b4":"code","4d6c06ea":"code","c654ff5b":"code","d345e441":"code","b7dd7595":"code","f96401cd":"code","04000c0c":"code","1549e942":"code","d9ed158d":"code","c44e4092":"code","2f276482":"code","5ff14f11":"code","0dce0fec":"code","8fa5db61":"code","480e90bb":"code","b75b3abf":"code","31577743":"code","43e72b1f":"code","25e06885":"code","a3943fe7":"code","8383ea59":"code","d7035d6c":"code","9327e71b":"code","d903507f":"code","bdf34ee2":"code","444c1ef6":"code","b13ef5f5":"code","7fcaa764":"code","b93805a9":"code","54ba810d":"code","d49f88e3":"code","8f8390fb":"code","3b081542":"code","27d0e2f1":"code","90950d5b":"code","1cd10d02":"code","2f9a59e1":"markdown","a922d024":"markdown","ddcde712":"markdown","7335f26b":"markdown","ce17db99":"markdown","50bd8f44":"markdown","0c5179d4":"markdown","0e6241c5":"markdown","03f38609":"markdown","73748c88":"markdown","32e10aa4":"markdown","c2270e8f":"markdown","361e90ef":"markdown","edeb02c4":"markdown","643e2034":"markdown","f4ff3194":"markdown","f174c73c":"markdown","68864f52":"markdown","d2bddd44":"markdown"},"source":{"af273dea":"# import pandas as pd  # data processing , csv file I\/O (pd.read_csv)\n# import numpy as np #linear algebra \n\n# import matplotlib.pyplot as plt #plotting \n# import seaborn as sns # statical plotting \n# %matplotlib inline \n\n# # for normalization\n# from sklearn.preprocessing import StandardScaler\n\n# # for encoding \n# from sklearn.preprocessing import LabelEncoder\n\n# #for feature selection \n# from sklearn.ensemble import RandomForestClassifier\n# from sklearn.feature_selection import RFE\n\n# # for model selection and training \n# from sklearn.svm import SVC\n# from sklearn.linear_model import LogisticRegression\n# from sklearn.naive_bayes import BernoulliNB\n# from lightgbm import LGBMClassifier \n# from xgboost import XGBClassifier \n\n# from sklearn.model_selection import cross_val_score","7c5f8c1b":"import numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd \nimport seaborn as sns\n%matplotlib inline","2be1480b":"dataset = pd.read_csv(\"..\/input\/network-intrusion-detection\/Train_data.csv\")\nprint(\"Training data has {} rows & {} columns\".format(dataset.shape[0],dataset.shape[1]))\ndataset.head()","59c32d20":"X = dataset.drop(\"class\", axis=1)\nY = dataset[\"class\"]\n# print(X)\nX.head()","f16ef3b4":"# print(Y)\n# head(): To print the first n rows.(Default Value = 5)\nX.head()","4d6c06ea":"print(X['is_host_login'].value_counts())","c654ff5b":"print(X['num_outbound_cmds'].value_counts())","d345e441":"X.drop(['num_outbound_cmds','is_host_login'], axis=1, inplace=True)\n# print(X)\nX.head()","b7dd7595":"from sklearn.impute import SimpleImputer \nimputer = SimpleImputer(missing_values = np.nan, strategy = 'mean')\n\nX_object = X.select_dtypes(\"object\")\nX_int = X.select_dtypes(exclude = \"object\")\n\n# To save the columns name for later use \nX_columns = X_int.columns\n\n# print (X_columns)\n\nimputer.fit(X_int)\nX_int = imputer.transform(X_int)\nX_int = pd.DataFrame(X_int)\nX_int.columns = X_columns\n\n# print(X_int.columns)\n\nX_object = X_object.fillna(\"unknown\")\n# X_object = np.array(X_object)\n# print(X_object)\n\nX = X_object.join(X_int)\n# X = pd.concat([X_object, X_int], axis=1)\n\n# print(X_int)\n# X_int = np.array(X_int)\n# # imputer.fit(X_int[0:5, 0:36])\n# # X_int[0:5, 0:36] = imputer.transform(X_int[0:5, 0:36])\n# X_int = pd.DataFrame(X_int)\n","f96401cd":"# to print the full summary\nX.info()","04000c0c":"# Descriptive analysis of the data\nX.describe()","1549e942":"X.head()","d9ed158d":"import warnings\nwarnings.filterwarnings('ignore')\n# Target Class Distribution\nsns.countplot(Y)\n","c44e4092":"sns.countplot(X['protocol_type'], hue=Y)","2f276482":"plt.figure(figsize=(12,6))\nsns.countplot(X['flag'], hue=Y)\n","5ff14f11":"plt.figure(figsize=(16,6))\nsns.distplot(X['count'], kde=False)","0dce0fec":"sns.distplot(X.dst_host_srv_count)","8fa5db61":"from sklearn.preprocessing import LabelEncoder\nLE = LabelEncoder()\nY = LE.fit_transform(Y)\nY = pd.Series(Y)\nY\n\n\n# Y = Y.apply(lambda x: 1 if x==\"anomaly\" else 0)\n","480e90bb":"# Correlation Heatmap\nplt.figure(figsize=(16,10))\nsns.heatmap(X.corr().apply(abs))","b75b3abf":"corr_with_target = X.corrwith(Y).apply(abs)\ncorr_with_target[corr_with_target>0.7]","31577743":"# Custom Label Encoder for handling unknown values\nclass LabelEncoderExt(object):\n    def __init__(self):\n        self.label_encoder = LabelEncoder()\n\n    def fit(self, data):\n        self.label_encoder = self.label_encoder.fit(list(data) + ['Unknown'])\n        self.classes_ = self.label_encoder.classes_\n        return self\n\n    def transform(self, data):\n        new_data = list(data)\n        for unique_item in np.unique(data):\n            if unique_item not in self.label_encoder.classes_:\n                new_data = ['Unknown' if x==unique_item else x for x in new_data]\n        return self.label_encoder.transform(new_data)","43e72b1f":"le = LabelEncoderExt()\n\n# encode the selected columns\nfor col in X.select_dtypes(\"object\"):\n  le.fit(X[col])\n  X[col] = le.transform(X[col])","25e06885":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.3, random_state=42)","a3943fe7":"X_train","8383ea59":"X_test","d7035d6c":"y_train","9327e71b":"y_test","d903507f":"from sklearn.preprocessing import StandardScaler\nscaler = StandardScaler()\n# store the columns\ncols = X_train.columns\n\n# transform the data\nX_train_scaled = scaler.fit_transform(X_train)\nX_test_scaled = scaler.transform(X_test)\n\nX_train_scaled = pd.DataFrame(X_train_scaled, columns = cols)\nX_test_scaled = pd.DataFrame(X_test_scaled, columns = cols)","bdf34ee2":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier();\n\nrfc.fit(X_train_scaled, y_train)\n\nfeature_imp = pd.DataFrame({'feature':X_train.columns,'importance':rfc.feature_importances_})\nfeature_imp = feature_imp.sort_values('importance',ascending=False).set_index('feature')","444c1ef6":"# plot feat_imp\nplt.figure(figsize = (10, 5))\nplt.title(\"Feature Importance\")\nplt.ylabel(\"Importances\")\nplt.xlabel(\"Features\")\nplt.xticks(rotation=90)\nplt.plot(feature_imp)","b13ef5f5":"from sklearn.feature_selection import RFE\nestimator = RandomForestClassifier()\nselector = RFE(estimator)\nselector.fit(X_train_scaled, y_train)\n\nX_train_scaled = selector.transform(X_train_scaled)\nX_test_scaled = selector.transform(X_test_scaled)","7fcaa764":"#  SEED FOR RANDOM STATE\nSEED = 42\n\n\n# SVC MODEL\nfrom sklearn.svm import SVC\nsvc = SVC(random_state = SEED)\n\n# LOGISTIC REGRESSION MODEL\nfrom sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\n\n# GAUSSIAN NAIVE BAYES\nfrom sklearn.naive_bayes import BernoulliNB\nbnb = BernoulliNB()\n\n# Train XGBOOST Classifier\nfrom xgboost import XGBClassifier\nxgbc = XGBClassifier(eval_metric=\"logloss\", random_state=SEED)\n\n# Train LightGBM Classifier\nfrom lightgbm import LGBMClassifier\nlgbmc = LGBMClassifier(random_state=SEED)\n","b93805a9":"from sklearn.model_selection import cross_val_score\nmodels = {}\nmodels['SVC']= svc\nmodels['LogisticRegression']= lr\nmodels['Naive Bayes Classifier']= bnb\nmodels['XGBoost Classifier']= xgbc\nmodels['LightGBM Classifier']= lgbmc\nscores = {}\nfor name in models:\n  scores[name]={}\n  for scorer in ['precision','recall']:\n    scores[name][scorer] = cross_val_score(models[name], X_train_scaled, y_train, cv=10, scoring=scorer)","54ba810d":"def line(name):\n  return '*'*(25-len(name)\/\/2)\n\nfor name in models:\n  print(line(name), name, 'Model Validation', line(name))\n\n  for scorer in ['precision','recall']:\n    mean = round(np.mean(scores[name][scorer])*100,2)\n    stdev = round(np.std(scores[name][scorer])*100,2)\n    print (\"Mean {}:\".format(scorer),\"\\n\", mean,\"%\", \"+-\",stdev)\n    print()","d49f88e3":"for name in models:\n    for scorer in ['precision','recall']:\n        scores[name][scorer] = scores[name][scorer].mean()\nscores=pd.DataFrame(scores).swapaxes(\"index\", \"columns\")*100","8f8390fb":"scores.plot(kind = \"bar\",  ylim=[80,100], figsize=(10,6), rot=0)","3b081542":"models = {}\nmodels['SVC']= svc\nmodels['LogisticRegression']= lr\nmodels['Naive Bayes Classifier']= bnb\nmodels['XGBoost Classifier']= xgbc\nmodels['LightGBM Classifier']= lgbmc\npreds={}\nfor name in models:\n    models[name].fit(X_train_scaled, y_train)\n    preds[name] = models[name].predict(X_test_scaled)\nprint(\"Predictions complete.\")","27d0e2f1":"from sklearn.metrics import confusion_matrix, classification_report, f1_score\ndef line(name,sym=\"*\"):\n    return sym*(25-len(name)\/\/2)\ntarget_names=[\"normal\",\"anamoly\"]\nfor name in models:\n    print(line(name), name, 'Model Testing', line(name))\n    print(confusion_matrix(y_test, preds[name]))\n    print(line(name,'-'))\n    print(classification_report(y_test, preds[name], target_names=target_names))","90950d5b":"f1s = {}\nfor name in models:\n    f1s[name]=f1_score(y_test, preds[name])\nf1s=pd.DataFrame(f1s.values(),index=f1s.keys(),columns=[\"F1-score\"])*100","1cd10d02":"f1s.plot(kind = \"bar\",  ylim=[80,100], figsize=(10,6), rot=0)","2f9a59e1":"* We plot various graphs to identify distributions, relationships or any pattern that is not visible by seeing raw data.\n\n","a922d024":"# **Observations from Data Analysis**\n* We identified a slight imbalance in the target column \"class\" of our dataset. But it is not significant, otherwise we could go for oversampling.\n* 80% of traffic belongs TCP while 12% belongs to UDP and rest to ICMP.\n* Most of the ICMP traffic had anomaly; most of the UDP traffic was normal; while the distribution was almost equal in case of TCP.\n* The traffic distribution on the basis of flags was also uneven where most of it had SF(Sign Flag).\n* Most of the traffic with SF was normal, while that had S0 flag had anomaly.\n* Most of the traffic recorded was unique.\n* Count of most of the connections having the same destination host and using the same service was either very low or very high.","ddcde712":"# Importing the Dataset","7335f26b":"**Feature Selection**","ce17db99":"# Exploratory Data Analysis","50bd8f44":"**Encoding X_object **","0c5179d4":"* Here, we found that 'is_host_login' and 'num_outbound_cmds' have only one unique value i.e., 0. This introduces redundancy, as a feature with only 1 value won't affect our model. We can remove it and reduce the size of the data and hence improve the training process.","0e6241c5":"**Using recursive feature elimination for Feature Selection**","03f38609":"**Visualization of the Feature Importance**","73748c88":"**MODEL TESTING ON VALIDATION DATA**","32e10aa4":"# Importing the Libraries","c2270e8f":"*** We are encoding the target class to 0s and 1s, so that it can be used for further analysis and training.**","361e90ef":"**Taking Care of Missing Values**","edeb02c4":"Though SVC classifier was close, but from the above results, we can observe that our model XGBoost Classifier and LightGBM Classifier perform the best on the validation data.\n\nThe evaluation metrics used are:\n\n* Precision: also called positive predictive value, is the fraction of correct positive predictions among all the positive predictions.\n\n* Recall: also known as sensitivity, is the fraction of correct positive predictions that were correct positives.\n\nPrecision and Recall can be calculated by:\n\n\n   ![image.png](attachment:4bc3b697-b56c-425b-b38f-4a5b1762c03b.png)","643e2034":"* We identified various features with binary values and a few with redundancy.\n* A few features have object data type that need to be encoded into numerical values.\n* Also, few features have high scale difference and need to normalised.","f4ff3194":"# **Machine Learning based Intrusion Detection System.**\n\nPerformed the Machine Learning workflow which included the following steps:\n\n1. **Data Collection:**\n\n    * This included finding and selecting data that was processed and used to train our machine learning model.\n    * The source of the dataset used is from Kaggle. It is a huge repository of community published data.\n    * Short description: The dataset consists of a wide variety of intrusions simulated in a military network environment. It has 25192 rows & 42 columns.\n    * Note: Just after loading the data, the data was split into 7:3 training and testing data to prevent data leakage.\n    \n2.  **Data Analysis and Preparation:**\n    * Here, we analysed the data to find any discrepancies, interesting patterns, coorrelation in data, etc. This step is also popularly known as Exploratory Data Analysis.\n    * After analysing, we performed some standard data preprocessing techniques. It was done wherever we felt that it would affect our process.\n    * Most of our time was consumed during this process.\n    * Some methods used are:\n        * Data cleaning - handling missing values by mean imputation, etc.\n        * Data Scaling and Normalisation - Scaling or Normalisation is common preprocessing technique used in machine learning where the data is ususally normalised to a scale of 0 to 1.\n        * Data Encoding - Most of the models cannot process strings\/objects. So the data needs to be transformed to numerical data. This process is known as data encoding(also data transformation).\n        * Feature Selection - Removing redundant features or selecting the most \"useful\" features. We used recursive feature elimination for feature selection.      \n\n3. **Model Selection\/Building:**\n\n    * Here, we choose the right models that can be used for the required task.\n    * Our task required us to use a Classification model.\n    * We selected 2 State of the art models - LightGBM and XGBoost.\n    * We also selected a few standard models to compare the results namely Logistic Regression, SVC, Naive Bayes.\n\n4. **Model Training:**\n\n    * The model was trained on the training data which took a few minutes for each model.\n\n5. **Model Evaluation**\n\n    * After the model is trained, we evaluate the performance of the model.\n    * The evaluation metrics used are: Precision and Recall.\n\n6. **Parameter Tuning**\n\n    * A model needs to be \"tuned\" for each particular scenario\/ usecase based on the dataset.\n    * This includes changing various parameters and evaluating the results simulaneously.\n\n7. **Making Predictions**\n\n    * After the model parameters are finalized and it is trained, it can be saved and used for making predictions.","f174c73c":"**Feature scaling**","68864f52":"# MODEL SELECTION","d2bddd44":"**Splitting Data**"}}