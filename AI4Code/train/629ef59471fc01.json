{"cell_type":{"4af87de0":"code","df20ccb8":"code","d53d858b":"code","9334a11a":"code","dc77af4e":"code","ea71c438":"code","c8faeb56":"code","6237e92e":"code","cfa8e828":"code","e16c1f6d":"code","925ff975":"code","bfb18c23":"code","3b88395e":"code","780d2c37":"code","fc6e4e7c":"code","22d7b8f1":"code","d4c0a01b":"code","0e5c95a7":"code","a60ea22a":"code","d939e076":"code","c7d6e80a":"code","01f173bf":"code","b60636f3":"code","6c9b3bf1":"code","5cb6482e":"code","12d69cca":"code","f7f0d06b":"code","26f9048e":"code","ea944e03":"code","4108824a":"code","faa9d2ff":"code","e58e55e0":"code","e1526b38":"code","2eb7e514":"code","303df293":"code","69d339e7":"code","6578e301":"code","f551d840":"code","1eb99e7c":"code","d54fda71":"code","98b3f568":"code","eed88386":"code","1c3d6d00":"code","5b8f111c":"code","3db497d7":"code","37b24617":"code","d4440792":"code","694a13dc":"code","eb3f3932":"code","d49ae7eb":"code","f18698e2":"code","7895cdf1":"code","efe51dac":"code","6e59e5a0":"code","656e6dd4":"code","22a423eb":"code","bccd8056":"code","c1a454b0":"code","a9f74fcd":"code","17d7ebcf":"code","3d87df6c":"code","c2fc8170":"code","d6206da8":"code","a0968c5c":"code","5e5243ce":"code","ca6434ce":"code","5702e85e":"code","d712221d":"code","65ecb1e2":"code","c0a53f5e":"markdown","3db62e65":"markdown","7fbc6b46":"markdown","da9546c5":"markdown","a71f1e00":"markdown","d703a503":"markdown","95b0a049":"markdown","4f93cac4":"markdown","4138a268":"markdown","b4030a9c":"markdown","3423b0bb":"markdown","03224633":"markdown","90e5a76d":"markdown","b013ce6e":"markdown","009967fb":"markdown","e7b84d08":"markdown","0eb3b1fc":"markdown"},"source":{"4af87de0":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import ensemble, tree, linear_model\nfrom sklearn.model_selection import train_test_split, cross_val_score\nfrom sklearn.metrics import r2_score, mean_squared_error\nfrom sklearn.utils import shuffle\n\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","df20ccb8":"train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ntest = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","d53d858b":"train.describe()","9334a11a":"train.head(3)","dc77af4e":"test.head(3)","ea71c438":"train.shape,test.shape","c8faeb56":"#drop id col as it has unique values\ntrain.drop(['Id'],axis =1,inplace=True)","6237e92e":"#correlation matrix\nnum=train.select_dtypes(exclude='object')\nnumcorr=num.corr()\nf,ax=plt.subplots(figsize=(40,1))\nsns.heatmap(numcorr.sort_values(by=['SalePrice'], ascending=False).head(1), cmap='PuRd_r', annot=True)\nplt.title(\" Numerical features correlation with the sale price\", weight='bold', fontsize=20)\nplt.xticks(weight='bold')\nplt.yticks(weight='bold', color='dodgerblue', rotation=0)\n\n\nplt.show()","cfa8e828":"Num=numcorr['SalePrice'].sort_values(ascending=False).head(10).to_frame()\n\ncm = sns.light_palette(\"cyan\", as_cmap=True)\ns = Num.style.background_gradient(cmap=cm)\ns","e16c1f6d":"plt.figure(figsize=(10,6))\nplt.scatter(x=train['GrLivArea'], y=train['SalePrice'], color='crimson', alpha=0.4)\nplt.title('Ground living area\/ Sale price', weight='bold', fontsize=16)\nplt.xlabel('Ground living area', weight='bold', fontsize=12)\nplt.ylabel('Sale price', weight='bold', fontsize=12)\nplt.xticks(weight='bold')\nplt.yticks(weight='bold')\nplt.show()","925ff975":"fig, ax = plt.subplots(figsize=(9,6))\n\n# Horizontal Bar Plot\ntitle_cnt=train.Neighborhood.value_counts().sort_values(ascending=False).reset_index()\nmn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1],edgecolor='black', \n            color=sns.color_palette('Reds',len(title_cnt)))\n\n# Show top values \nax.invert_yaxis()\n\n# Add Plot Title\nax.set_title('Most frequent neighborhoods',weight='bold',\n             loc='center', pad=10, fontsize=16)\nax.set_xlabel('Count', weight='bold')\n\n# Add annotation to bars\nfor i in ax.patches:\n    ax.text(i.get_width()+1, i.get_y()+0.5, str(round((i.get_width()), 2)),\n            fontsize=10, fontweight='bold', color='green')\n\nplt.show()","bfb18c23":"fig, ax = plt.subplots(figsize=(9,6))\n\n# Horizontal Bar Plot\ntitle_cnt=train.BldgType.value_counts().sort_values(ascending=False).reset_index()\nmn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1],edgecolor='black', \n            color=sns.color_palette('Greens',len(title_cnt)))\n\n# Show top values \nax.invert_yaxis()\n\n# Add Plot Title\nax.set_title('Building type: Type of dwelling',weight='bold',\n             loc='center', pad=10, fontsize=16)\nax.set_xlabel('Count', weight='bold')\n\n# Add annotation to bars\nfor i in ax.patches:\n    ax.text(i.get_width()+1, i.get_y()+0.5, str(round((i.get_width()), 2)),\n            fontsize=10, fontweight='bold', color='green')\n\nplt.show()","3b88395e":"fig, ax = plt.subplots(figsize=(9,6))\n\n# Horizontal Bar Plot\ntitle_cnt=train.GarageCond.value_counts().sort_values(ascending=False).reset_index()\nmn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1],edgecolor='black', \n            color=sns.color_palette('Blues',len(title_cnt)))\n\n# Show top values \nax.invert_yaxis()\n\n# Add Plot Title\nax.set_title('Garage condition',weight='bold',\n             loc='center', pad=10, fontsize=16)\nax.set_xlabel('Count', weight='bold')\n\n# Add annotation to bars\nfor i in ax.patches:\n    ax.text(i.get_width()+1, i.get_y()+0.5, str(round((i.get_width()), 2)),\n            fontsize=10, fontweight='bold', color='green')\n\nplt.show()","780d2c37":"fig, ax = plt.subplots(figsize=(9,6))\n\n# Horizontal Bar Plot\ntitle_cnt=train.GarageType.value_counts().sort_values(ascending=False).reset_index()\nmn= ax.barh(title_cnt.iloc[:,0], title_cnt.iloc[:,1],edgecolor='black', \n            color=sns.color_palette('Blues',len(title_cnt)))\n\n# Show top values \nax.invert_yaxis()\n\n# Add Plot Title\nax.set_title('Garage Type',weight='bold',\n             loc='center', pad=10, fontsize=16)\nax.set_xlabel('Count', weight='bold')\n\n# Add annotation to bars\nfor i in ax.patches:\n    ax.text(i.get_width()+1, i.get_y()+0.5, str(round((i.get_width()), 2)),\n            fontsize=10, fontweight='bold', color='green')\n\nplt.show()","fc6e4e7c":"def plot_cat(categories=['1Story', '2Story', '1.5Fin', 'SLvl', 'SFoyer','1.5Unf', '2.5Unf', '2.5Fin'],\n             data=train, column='HouseStyle', title='House Style frequency'):\n    \n    HouseStyles = categories \n    h_s = data[column].value_counts()[HouseStyles]\n\n    fig, ax = plt.subplots(figsize=(12, 6))\n    ax.bar(h_s.index, h_s, width=0.55, edgecolor='black', \n           color=sns.color_palette(\"Purples\", len(HouseStyles)))\n\n    for i in h_s.index:\n        ax.annotate(f\"{h_s[i]}\", \n                   xy=(i, h_s[i] + 20),\n                   va = 'center', ha='center',fontweight='light',\n                   color='purple', weight='bold')\n    return plt.show()\n\nplot_cat()","22d7b8f1":"y_train = train['SalePrice'].to_frame()\n\n#Combine train and test sets\nc1 = pd.concat((train, test), sort=False).reset_index(drop=True)\n\n#Drop the target \"SalePrice\" and Id columns\nc1.drop(['SalePrice','Id'], axis=1, inplace=True)\nprint(f\"Total size is {c1.shape}\")","d4c0a01b":"c1.isna().sum().sort_values(ascending=False).head(10)","0e5c95a7":"c=c1.dropna(thresh=len(c1)*0.8, axis=1)\nprint(f\"We dropped {c1.shape[1]-c.shape[1]} features in the combined set\")","a60ea22a":"c.isnull().sum().sort_values(ascending=False).head(10)","d939e076":"(c.isnull().sum())*100","c7d6e80a":"allna = (c.isnull().sum() \/ len(c))*100\nallna = allna.drop(allna[allna == 0].index).sort_values()\nallna.head(3)","01f173bf":"NA=c[allna.index.to_list()]\nNA.head(3)","b60636f3":"NAcat=NA.select_dtypes(include='object')\nNAnum=NA.select_dtypes(exclude='object')\nprint(f'We have :{NAcat.shape[1]} categorical features with missing values')\nprint(f'We have :{NAnum.shape[1]} numerical features with missing values')","6c9b3bf1":"c['MasVnrArea']=c.MasVnrArea.fillna(0)\n#LotFrontage has 16% missing values. We fill with the median\nc['LotFrontage']=c.LotFrontage.fillna(c.LotFrontage.median())\n#GarageYrBlt:  Year garage was built, we fill the gaps with the median: 1980\nc['GarageYrBlt']=c[\"GarageYrBlt\"].fillna(1980)","5cb6482e":"NAcat1= NAcat.isnull().sum().to_frame().sort_values(by=[0]).T\ncm = sns.light_palette(\"lime\", as_cmap=True)\n\nNAcat1 = NAcat1.style.background_gradient(cmap=cm)\nNAcat1","12d69cca":"fill_cols = ['Electrical', 'SaleType', 'KitchenQual', 'Exterior1st',\n             'Exterior2nd', 'Functional', 'Utilities', 'MSZoning']\n\nfor col in c[fill_cols]:\n    c[col] = c[col].fillna(method='ffill')","f7f0d06b":"#Categorical missing values\nNAcols=c.columns\nfor col in NAcols:\n    if c[col].dtype == \"object\":\n        c[col] = c[col].fillna(\"None\")","26f9048e":"#Numerical missing values\nfor col in NAcols:\n    if c[col].dtype != \"object\":\n        c[col]= c[col].fillna(0)","ea944e03":"c.isnull().sum().sort_values(ascending=False).head()","4108824a":"# Changing Numeric to String Type\nc['MSSubClass'] = c['MSSubClass'].apply(str)\nc['YrSold'] = c['YrSold'].astype(str)","faa9d2ff":"# One Hot Encoding\ncb=pd.get_dummies(c)\nprint(f\"the shape of the original dataset {c.shape}\")\nprint(f\"the shape of the encoded dataset {cb.shape}\")\nprint(f\"We have {cb.shape[1]- c.shape[1]} new encoded features\")","e58e55e0":"na = train.shape[0]\nnb = test.shape[0]\nTrain1 = cb[:na]  #na is the number of rows of the original training set\nTest1 = cb[na:] ","e1526b38":"fig = plt.figure(figsize=(15,15))\nax1 = plt.subplot2grid((3,2),(0,0))\nplt.scatter(x=train['GrLivArea'], y=train['SalePrice'], color=('yellowgreen'), alpha=0.5)\nplt.axvline(x=4600, color='r', linestyle='-')\nplt.title('Ground living Area- Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(0,1))\nplt.scatter(x=train['TotalBsmtSF'], y=train['SalePrice'], color=('red'),alpha=0.5)\nplt.axvline(x=5900, color='r', linestyle='-')\nplt.title('Basement Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(1,0))\nplt.scatter(x=train['1stFlrSF'], y=train['SalePrice'], color=('deepskyblue'),alpha=0.5)\nplt.axvline(x=4000, color='r', linestyle='-')\nplt.title('First floor Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(1,1))\nplt.scatter(x=train['MasVnrArea'], y=train['SalePrice'], color=('gold'),alpha=0.9)\nplt.axvline(x=1500, color='r', linestyle='-')\nplt.title('Masonry veneer Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(2,0))\nplt.scatter(x=train['GarageArea'], y=train['SalePrice'], color=('orchid'),alpha=0.5)\nplt.axvline(x=1230, color='r', linestyle='-')\nplt.title('Garage Area - Price scatter plot', fontsize=15, weight='bold' )\n\nax1 = plt.subplot2grid((3,2),(2,1))\nplt.scatter(x=train['TotRmsAbvGrd'], y=train['SalePrice'], color=('tan'),alpha=0.9)\nplt.axvline(x=13, color='r', linestyle='-')\nplt.title('TotRmsAbvGrd - Price scatter plot', fontsize=15, weight='bold' )\nplt.show()","2eb7e514":"train['GrLivArea'].sort_values(ascending=False).head(2)","303df293":"train['TotalBsmtSF'].sort_values(ascending=False).head(1)","69d339e7":"train['MasVnrArea'].sort_values(ascending=False).head(1)","6578e301":"train['1stFlrSF'].sort_values(ascending=False).head(1)","f551d840":"train['GarageArea'].sort_values(ascending=False).head(4)","1eb99e7c":"train['TotRmsAbvGrd'].sort_values(ascending=False).head(1)","d54fda71":"# Remove the outliers\ntarget=train[['SalePrice']]\npos = [1298,523, 297]\ntarget.drop(target.index[pos], inplace=True)","98b3f568":"Train2=Train1[(Train1['GrLivArea'] < 4600) & (Train1['MasVnrArea'] < 1500)]\n\nprint(f'We removed {Train1.shape[0]- Train2.shape[0]} outliers')","eed88386":"print('We make sure that both train and target sets have the same row number after removing the outliers:')\nprint( 'Train: ',Train2.shape[0], 'rows')\nprint('Target:', target.shape[0],'rows')","1c3d6d00":"plt.style.use('seaborn')\nsns.set_style('whitegrid')\nfig = plt.figure(figsize=(15,5))\n#1 rows 2 cols\n#first row, first col\nax1 = plt.subplot2grid((1,2),(0,0))\nplt.scatter(x=train['GrLivArea'], y=train['SalePrice'], color=('orchid'), alpha=0.5)\nplt.title('Area-Price plot with outliers',weight='bold', fontsize=18)\nplt.axvline(x=4600, color='r', linestyle='-')\n#first row sec col\nax1 = plt.subplot2grid((1,2),(0,1))\nplt.scatter(x=Train2['GrLivArea'], y=target['SalePrice'], color='navy', alpha=0.5)\nplt.axvline(x=4600, color='r', linestyle='-')\nplt.title('Area-Price plot without outliers',weight='bold', fontsize=18)\nplt.show()","5b8f111c":"print(\"Skewness before log transform: \", train['GrLivArea'].skew())\nprint(\"Kurtosis before log transform: \", train['GrLivArea'].kurt())","3db497d7":"print(f\"Skewness after log transform: {Train2['GrLivArea'].skew()}\")\nprint(f\"Kurtosis after log transform: {Train2['GrLivArea'].kurt()}\")","37b24617":"plt.style.use('seaborn')\nsns.set_style('whitegrid')\nfig = plt.figure(figsize=(15,10))\n#1 rows 2 cols\n#first row, first col\nax1 = plt.subplot2grid((2,2),(0,0))\nsns.distplot(train.GrLivArea, color='plum')\nplt.title('Before: Distribution of GrLivArea',weight='bold', fontsize=18)\n#first row sec col\nax1 = plt.subplot2grid((2,2),(0,1))\nsns.distplot(train['1stFlrSF'], color='tan')\nplt.title('Before: Distribution of 1stFlrSF',weight='bold', fontsize=18)\n\n\nax1 = plt.subplot2grid((2,2),(1,0))\nsns.distplot(Train2.GrLivArea, color='plum')\nplt.title('After: Distribution of GrLivArea',weight='bold', fontsize=18)\n#first row sec col\nax1 = plt.subplot2grid((2,2),(1,1))\nsns.distplot(Train2['1stFlrSF'], color='tan')\nplt.title('After: Distribution of 1stFlrSF',weight='bold', fontsize=18)\nplt.show()","d4440792":"print(f\"Skewness before log transform: {target['SalePrice'].skew()}\")\nprint(f\"Kurtosis before log transform: {target['SalePrice'].kurt()}\")","694a13dc":"#log transform the target:\ntarget[\"SalePrice\"] = np.log1p(target[\"SalePrice\"])","eb3f3932":"plt.style.use('seaborn')\nsns.set_style('whitegrid')\nfig = plt.figure(figsize=(15,5))\n#1 rows 2 cols\n#first row, first col\nax1 = plt.subplot2grid((1,2),(0,0))\nplt.hist(train.SalePrice, bins=10, color='mediumpurple',alpha=0.5)\nplt.title('Sale price distribution before normalization',weight='bold', fontsize=18)\n#first row sec col\nax1 = plt.subplot2grid((1,2),(0,1))\nplt.hist(target.SalePrice, bins=10, color='darkcyan',alpha=0.5)\nplt.title('Sale price distribution after normalization',weight='bold', fontsize=18)\nplt.show()","d49ae7eb":"print(f\"Skewness after log transform: {target['SalePrice'].skew()}\")\nprint(f\"Kurtosis after log transform: {target['SalePrice'].kurt()}\")","f18698e2":"x=Train2\ny=target.SalePrice.values","7895cdf1":"from sklearn.model_selection import train_test_split\nx_train, x_test, y_train, y_test = train_test_split(x, y,test_size = .3, random_state=42)","efe51dac":"from sklearn.preprocessing import RobustScaler\nscaler= RobustScaler()\n\nx_train = scaler.fit_transform(x_train)\nx_test  = scaler.transform(x_test)","6e59e5a0":"import sklearn.model_selection as GridSearchCV\nfrom sklearn.linear_model import Ridge\nimport sklearn.model_selection as ms\nimport math\n\nridge=Ridge()\nparameters= {'alpha':[x for x in range(1,101)]}\n\nridge_reg=ms.GridSearchCV(ridge, param_grid=parameters, scoring='neg_mean_squared_error', cv=15)\nridge_reg.fit(x_train,y_train)\n\nprint(f\"The best value of Alpha is: {ridge_reg.best_params_}\")\nprint(f\"The best score achieved with Alpha=11 is: {math.sqrt(-ridge_reg.best_score_)}\")\n\nridge_pred=math.sqrt(-ridge_reg.best_score_)","656e6dd4":"def score(y_pred):\n    return str(math.sqrt(sklm.mean_squared_error(y_test, y_pred)))","22a423eb":"import sklearn.metrics as sklm\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import cross_val_score, GridSearchCV, KFold, RandomizedSearchCV, train_test_split\n\nridge_mod = Ridge(alpha=18)\nridge_mod.fit(x_train,y_train)\ny_pred_train = ridge_mod.predict(x_train)\ny_pred_test  = ridge_mod.predict(x_test)\n\nprint(f'Root Mean Square Error train =  {str(math.sqrt(sklm.mean_squared_error(y_train, y_pred_train)))}')\nprint(f'Root Mean Square Error test =  {score(y_pred_test)}') ","bccd8056":"# RIDGE\nRidge_CV=Ridge(alpha=18)\nMSEs=ms.cross_val_score(Ridge_CV, x, y, scoring='neg_mean_squared_error', cv=5)\n\n#Final RMSE score with Lasso\nprint(f'Mean RMSE with Ridge: {round(math.sqrt(np.mean(-MSEs)),4)}')","c1a454b0":"# LASSO\nfrom sklearn.linear_model import Lasso\n\nparameters= {'alpha':[0.0001,0.0009,0.001,0.002,0.003,0.01,0.1,1,10,100]}\n\n\nlasso=Lasso()\nlasso_reg=ms.GridSearchCV(lasso, param_grid=parameters, scoring='neg_mean_squared_error', cv=15)\nlasso_reg.fit(x_train,y_train)\n\nprint(f'The best value of Alpha is: {lasso_reg.best_params_}')","a9f74fcd":"lasso_mod=Lasso(alpha=0.0009)\nlasso_mod.fit(x_train,y_train)\ny_lasso_train=lasso_mod.predict(x_train)\ny_lasso_test=lasso_mod.predict(x_test)\n\nprint(f'Root Mean Square Error train  {str(math.sqrt(sklm.mean_squared_error(y_train, y_lasso_train)))}')\nprint(f'Root Mean Square Error test  {score(y_lasso_test)}')","17d7ebcf":"# Finding the Coefficients\ncoefs = pd.Series(lasso_mod.coef_, index = x.columns)\n\nimp_coefs = pd.concat([coefs.sort_values().head(10),\n                     coefs.sort_values().tail(10)])\nimp_coefs.plot(kind = \"barh\", color='yellowgreen')\nplt.xlabel(\"Lasso coefficient\", weight='bold')\nplt.title(\"Feature importance in the Lasso Model\", weight='bold')\nplt.show()","3d87df6c":"print(f\"Lasso kept {sum(coefs != 0)} important features and dropped the other {sum(coefs == 0)} features\")","c2fc8170":"#Lasso regression\nLasso_CV=Lasso(alpha=0.0009)\nMSEs=ms.cross_val_score(Lasso_CV, x, y, scoring='neg_mean_squared_error', cv=5)\n\n#Final RMSE score with Lasso\nprint(f'Mean RMSE with Lasso: {round(math.sqrt(np.mean(-MSEs)),4)}')","d6206da8":"from sklearn.linear_model import ElasticNetCV\n\nalphas = [0.000542555]\nl1ratio = [0.1, 0.3,0.5, 0.9, 0.95, 0.99, 1]\n\nelastic_cv = ElasticNetCV(cv=5, max_iter=1e7, alphas=alphas,  l1_ratio=l1ratio)\n\nelasticmod = elastic_cv.fit(x_train, y_train)\nela_pred=elasticmod.predict(x_test)\nprint(f'Root Mean Square Error test = {score(ela_pred)}')\nprint(elastic_cv.alpha_)\nprint(elastic_cv.l1_ratio_)","a0968c5c":"from xgboost.sklearn import XGBRegressor\n\nxgb= XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=0.5, gamma=0,\n             importance_type='gain', learning_rate=0.01, max_delta_step=0,\n             max_depth=3, min_child_weight=0, missing=None, n_estimators=4000,\n             n_jobs=1, nthread=None, objective='reg:squarederror', random_state=0,\n             reg_alpha=0.0001, reg_lambda=0.01, scale_pos_weight=1, seed=None,\n             silent=None, subsample=1, verbosity=1)\nxgmod=xgb.fit(x_train,y_train)\nxg_pred=xgmod.predict(x_test)\nprint(f'Root Mean Square Error test = {score(xg_pred)}')","5e5243ce":"from sklearn.ensemble import VotingRegressor\n\nvote_mod = VotingRegressor([('Ridge', ridge_mod), ('Lasso', lasso_mod), ('Elastic', elastic_cv), \n                            ('XGBRegressor', xgb)])\n\nvote= vote_mod.fit(x_train, y_train.ravel())\nvote_pred=vote.predict(x_test)\nprint(f'Root Mean Square Error test = {score(vote_pred)}')","ca6434ce":"from mlxtend.regressor import StackingRegressor\n\n\nstregr = StackingRegressor(regressors=[elastic_cv,ridge_mod, lasso_mod, vote_mod], \n                           meta_regressor=xgb, use_features_in_secondary=True)\n\nstack_mod=stregr.fit(x_train, y_train.ravel())\nstacking_pred=stack_mod.predict(x_test)\nprint(f'Root Mean Square Error test = {score(stacking_pred)}')","5702e85e":"# coefficients were assigned manually\nfinal_test=(0.25*vote_pred+0.35*stacking_pred+ 0.2*y_lasso_test)\nprint(f'Root Mean Square Error test=  {score(final_test)}')","d712221d":"X_test= scaler.transform(Test1)\n#VotingRegressor to predict the final Test\nvote_test = vote_mod.predict(X_test)\nfinal1=np.expm1(vote_test)\n\n#StackingRegressor to predict the final Test\nstack_test = stregr.predict(X_test)\nfinal2=np.expm1(stack_test)\n\n#LassoRegressor to predict the final Test\nlasso_test = lasso_mod.predict(X_test)\nfinal3=np.expm1(lasso_test)","65ecb1e2":"# Submission of the results predicted by the average of Voting\/Stacking\/Lasso\nfinal=(0.2*final1+0.6*final2+0.2*final3)\n\nfinal_submission = pd.DataFrame({\n        \"Id\": test[\"Id\"],\n        \"SalePrice\": final\n    })\nfinal_submission.to_csv(\"final_submission.csv\", index=False)\nfinal_submission.head()","c0a53f5e":"### Log transform skewed numeric features\nSkewness is the degree of distortion from the symmetrical bell curve or the normal distribution.\nKurtosis is actually the measure of outliers present in the distribution.\nFor more info, check this [link](https:\/\/codeburst.io\/2-important-statistics-terms-you-need-to-know-in-data-science-skewness-and-kurtosis-388fef94eeaa)","3db62e65":"### VOTING REGRESSOR:\nA voting regressor is an ensemble meta-estimator that fits base regressors each on the whole dataset. It, then, averages the individual predictions to form a final prediction.\nAfter running the regressors, we combine them first with voting regressor in order to get a better model","7fbc6b46":"#### you can see there are so many NAN values in both train and test sets which needs to treated well.","da9546c5":"#### We apply np.expm1 to cancel the np.logp1 (we did previously in data processing) and convert the numbers to their original form","a71f1e00":"### Most occuring Neighborhood, building type and house styles","d703a503":"#### Next, we try ElasticNet. A regressor that combines both ridge and Lasso. We use cross validation to find:\n\nAlpha, and  \nRatio between Ridge and Lasso, for a better combination of both","95b0a049":"### Cross Validation\nWe are using cv=5 where we split the dataset to 5 equal chunks and run the model 5 times, each time one fold is hold as a validation set","4f93cac4":"### Handling Missing Data","4138a268":"### STACKING REGRESSOR:\nWe stack all the previous models, including the votingregressor with XGBoost as the meta regressor","b4030a9c":"### Building the model","3423b0bb":"### Averaging Regressors\nLast thing to do is average our regressors and fit them on the testing dataset","03224633":"#### Example of a strong correlation between 2 numerical features: Sale price and ground living area","90e5a76d":"#### The overall quality, the living area, basement area, garage cars and garage area have the highest correlation values with the sale price, which is logical, better quality and bigger area = Higher price.\n\n1. Also some features such as, full bath or 1st floor surface have a higher correlation, those are luxury features, more luxury = Higher price.\n2. and Year built, the newer buildings seem to have higher sale prices.","b013ce6e":"### Visualizing the Outliers in dataset","009967fb":"#### With Ridge regularization, we need to find the alpha parameter that penalizes the error","e7b84d08":"### Regularization","0eb3b1fc":"### Outlier Detection"}}