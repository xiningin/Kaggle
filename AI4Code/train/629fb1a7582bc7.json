{"cell_type":{"c96338c1":"code","87e04475":"code","6cd307c0":"code","d0d6cc65":"code","0419e630":"code","a3edb30b":"code","3461203e":"code","f68da50c":"code","5c7d3744":"code","67f2ef63":"code","d61ff55f":"code","4be51fb2":"code","24a6d9ce":"code","269fc40d":"code","27ac57c6":"code","4ca7adea":"code","044ee287":"code","0b7d1fcc":"code","b429f5b7":"code","bee8890d":"code","6c6f560a":"code","4ae87161":"code","07710100":"code","05abf761":"code","736d89af":"code","30aeb53e":"code","52fa90a9":"code","c18160ba":"code","ecf9019d":"code","c4071f17":"code","e15c0ee0":"code","6a14c092":"code","ff6f0ed3":"code","cfe86182":"code","903f8067":"code","2d4b706d":"code","a0d611e2":"code","3accde88":"code","cbc9167c":"markdown","e62061e4":"markdown","f54bc797":"markdown","78010e50":"markdown","fe3b25ab":"markdown","90ab5020":"markdown","77107b38":"markdown","fc00a515":"markdown","df74f909":"markdown","277decce":"markdown","1976efb8":"markdown","4f1f0f10":"markdown","a5667294":"markdown","83cd9238":"markdown","f9fb64ba":"markdown","95a4a9b2":"markdown","d48206eb":"markdown","4b30b83c":"markdown","96d4451e":"markdown","e566dd62":"markdown","c9c68e35":"markdown","b173556b":"markdown","5daaf196":"markdown","25ada401":"markdown","0615eb21":"markdown","13214ad9":"markdown","5279fe9d":"markdown","deb32f49":"markdown","c5b655b5":"markdown","bae894bc":"markdown","55131367":"markdown","9b44c644":"markdown","93494a05":"markdown","f9ed6cfa":"markdown","0f8e3b06":"markdown"},"source":{"c96338c1":"import sys\n!cp ..\/input\/rapids\/rapids.0.13.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.6\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.6\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!cp \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/\n\nprint('rapids ai installed')","87e04475":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport matplotlib.patches as patches\nimport matplotlib.dates as mdates\nimport matplotlib.ticker as tic\nimport matplotlib\nimport datetime\n\nimport re\nfrom itertools import product\nfrom catboost import Pool, CatBoostRegressor,CatBoostClassifier\n\nfrom sklearn.metrics               import mean_squared_error\nfrom sklearn.preprocessing         import StandardScaler\nfrom sklearn.ensemble              import RandomForestRegressor\nfrom sklearn.linear_model          import LinearRegression\n\nimport cudf, cuml, cupy\nfrom cuml.neighbors import KNeighborsRegressor \n\npd.plotting.register_matplotlib_converters()\npd.options.display.max_rows = 100\npd.options.display.max_columns = 100\nmax_width = 12\nimport warnings\nwarnings.filterwarnings('ignore')\n\nprint('all libraries imported')","6cd307c0":"path = '..\/input\/competitive-data-science-predict-future-sales\/'\n\nf_categories      = pd.read_csv(path + 'item_categories.csv')\nf_items           = pd.read_csv(path + 'items.csv')\nf_transactions    = pd.read_csv(path + 'sales_train.csv')\nf_sample          = pd.read_csv(path + 'sample_submission.csv')\nf_shops           = pd.read_csv(path + 'shops.csv')\nf_test            = pd.read_csv(path + 'test.csv')","d0d6cc65":"f_transactions.date    = pd.to_datetime(f_transactions.date, format = '%d.%m.%Y')\n\nf_sales = f_transactions\\\n.join(f_items.set_index('item_id'), on='item_id')\\\n.join(f_categories.set_index('item_category_id'), on='item_category_id')\\\n.join(f_shops.set_index('shop_id'), on='shop_id')\n\nf_sales.head(2).T","0419e630":"f_test\\\n.join(f_sample.set_index('ID'), on='ID').head()","a3edb30b":"pd.pivot_table(f_sales.loc[f_sales.date_block_num==33], index='item_id', columns='shop_id', values='item_cnt_day', aggfunc='sum')\\\n.clip(0,20).fillna(0)\\\n.unstack().to_frame('item_cnt_month').reset_index().head()","3461203e":"#data\npiv = np.log(pd.pivot_table(f_sales,\n                            index='date',\n                            columns=['shop_id','shop_name'],\n                            values='item_cnt_day',\n                            aggfunc='sum').fillna(0).clip(0,)+1).T\n\n#plotting\n\nfig, ax = plt.subplots(figsize=(max_width,10))\n\n_y = piv.index.get_level_values(0)[::-1]\n_x = mdates.date2num(piv.columns)\n    \nax.imshow(piv, aspect='auto', extent = [ _x[0],  _x[-1]+1,  _y[0]+1, _y[-1]], interpolation='none')\n\nax.xaxis_date()\nax.xaxis.set_major_locator(mdates.YearLocator())\nax.xaxis.set_major_formatter(mdates.DateFormatter('%Y - %m'))\nax.xaxis.set_minor_locator(mdates.MonthLocator())\nax.xaxis.set_minor_formatter(mdates.DateFormatter('%m'))\n\nax.yaxis.set_ticks(_y)\nax.set_yticklabels(piv.index.get_level_values(0)[::-1].astype('str') + ' ' + piv.index.get_level_values(1)[::-1],\n                  fontsize=8, va='top')\n\nfig.autofmt_xdate(which='both', rotation=90, ha='left')\n\n\nplt.tight_layout()\nplt.show()","f68da50c":"test = f_test.copy()","5c7d3744":"#prepare transactions\n\nf_transactions.date    = pd.to_datetime(f_transactions.date, format = '%d.%m.%Y')\ntransactions = f_transactions.copy()\n\n#---------------------------------------------------------------------------\n#some handpicked outliers:\ntransactions = transactions.loc[~transactions.index.isin([1573253,1573252,1618930,1571406,2909818,1829149,1088966])]\n\n#---------------------------------------------------------------------------\n# filtering for clipping to speed up process and not to distort items with small ammount of sales\n#a = transactions.groupby('item_id').agg(sales_sum=('item_cnt_day','sum'),\n#                                        sales_mean=('item_cnt_day','mean'),\n#                                        sales_max=('item_cnt_day','max'),)\\\n#.assign(max_to_mean=(lambda x: x.sales_max\/x.sales_mean))\\\n#.loc[lambda x: (x.sales_sum>20)&(x.sales_max>10)&(x.max_to_mean>2)]\\\n#.index\n\n#for i in a:\n#    _ = transactions.loc[transactions.item_id==i,'item_cnt_day']\n#    transactions.loc[transactions.item_id==i,'item_cnt_day'] = _.clip(upper=_[_>0].quantile(0.98))\n    \n#---------------------------------------------------------------------------   \n#clip more and  fix shop id's\n\n#transactions.item_cnt_day = transactions.item_cnt_day.clip(upper=100)\n#transactions.shop_id = transactions.shop_id.replace([0,1,11],[57,58,10])\n\n#---------------------------------------------------------------------------   \n\ntransactions['year']            = transactions.date.dt.year\ntransactions['month']           = transactions.date.dt.month\ntransactions['day']             = transactions.date.dt.day\ntransactions['day_of_year']     = transactions.date.dt.dayofyear\ntransactions['week_day']        = transactions.date.dt.weekday\ntransactions['activity_cnt']    = transactions.item_cnt_day.abs()\ntransactions['activity_rev']    = transactions.activity_cnt*transactions.item_price\ntransactions['revenue']         = transactions.item_cnt_day*transactions.item_price","67f2ef63":"#prepare items\n\nitems = f_items.copy()\n\n#---------------------------------------------------------------------------   \n#separating plastic bag item from accessories to a separate category\nitems.at[20949,'item_category_id'] = 84\n\n#only item from PC category, move to ps accessories\nitems.at[5441,'item_category_id'] = 3\n\n#---------------------------------------------------------------------------   \n\nitems = items\\\n.join(transactions.groupby('item_id').agg(\n    sale_start=('date','min'),\n    sale_end=('date','max'),\n    price_mean=('item_price','mean'),\n    #sales_cnt=('item_cnt_day','sum'),\n    item_total_revenue=('revenue','sum'),\n))","d61ff55f":"#prepare categories\n\ncategories = f_categories.copy()\n\n#---------------------------------------------------------------------------   \n#separating plastic bag item from accessories to a separate category\n\ncategories = categories.append({'item_category_name': '\u0424\u0438\u0440\u043c\u0435\u043d\u043d\u044b\u0439 \u043f\u0430\u043a\u0435\u0442','item_category_id': 84,}, ignore_index=True)\n\n# global category level\n\ncategories['cat_global'] = np.select(\n    [categories.item_category_id.isin(range(0,8)),\n    categories.item_category_id.isin([8,80]),\n    categories.item_category_id==9,\n    categories.item_category_id.isin(range(10,18)),\n    categories.item_category_id.isin(range(18,32)),\n    categories.item_category_id.isin([32,33,34,35,36,37,79]),\n    categories.item_category_id.isin(range(37,42)),\n    categories.item_category_id.isin(range(42,55)),\n    categories.item_category_id.isin(range(55,61)),\n    categories.item_category_id.isin(range(61,73)),\n    categories.item_category_id.isin(range(73,79)),\n    categories.item_category_id.isin([81,82]),\n    categories.item_category_id==83,\n    categories.item_category_id==84],    \n    ['accessories','tickets','delivery','consoles','games',\n    'payment_cards','movies','books','music','gifts','programs',\n     'discs','batteries','plastic_bags'])\n\n# subcategories by platform\n\ncategories['cat_platform'] = np.select(\n    [categories.item_category_name.str.contains('PS2'),\n    categories.item_category_name.str.contains('PS3'),\n    categories.item_category_name.str.contains('PS4'),\n    categories.item_category_name.str.contains('PSP'),\n    categories.item_category_name.str.contains('PSVita'),\n    categories.item_category_name.str.contains('XBOX 360'),\n    categories.item_category_name.str.contains('XBOX ONE'),\n    categories.item_category_name.str.contains('PC'),\n    categories.item_category_name.str.contains('MAC'),\n    categories.item_category_name.str.contains('Android')],\n    ['PS2','PS3','PS4','PSP','PSVita','XBOX_360','XBOX_ONE','PC','MAC','Android'],\n    default='other')\n\n# digital subcategory\n\ncategories['cat_digital'] = categories.item_category_name\\\n.str.contains('\u0426\u0438\u0444\u0440\u0430')*1\n\n# marking categories that are in test set\n\ncategories['cat_in_test'] = categories.item_category_id\\\n.isin(test.set_index('item_id').join(items.item_category_id).item_category_id)*1\n\n# that are too scattered or not standard for training\n\ncategories['cat_to_drop'] = categories.item_category_id\\\n.isin([ 1,8,10,13,17,18,39,46,48,50,51,52,53,66,68,80,81,82])*1","4be51fb2":"#prepare shops\n\nshops = f_shops.copy()\n\n#---------------------------------------------------------------------------   \n#droppin dublicates, marking shops in test, marking shops that are unsuitable to train\nshops = shops.loc[~shops.shop_id.isin([0,1,11])]\nshops['shop_in_test'] = shops.shop_id.isin(test.shop_id)*1\nshops['shop_to_drop'] = shops.shop_id.isin([8,9,20,23,32,33,40])*1\n\n#---------------------------------------------------------------------------   \n# querying city name from shop name\n\nsplit = shops['shop_name'].\\\nstr.replace('\u041d.\u041d\u043e\u0432\u0433\u043e\u0440\u043e\u0434','\u041d\u041d\u043e\u0432\u0433\u043e\u0440\u043e\u0434').\\\nstr.replace('\u0421\u0435\u0440\u0433\u0438\u0435\u0432 \u041f\u043e\u0441\u0430\u0434','\u0421\u0435\u0440\u0433\u0438\u0435\u0432\u041f\u043e\u0441\u0430\u0434').\\\nstr.replace('\\W+',' ').\\\nstr.strip().\\\nstr.split(' ', 1).\\\nstr\n    \nshops['shop_city'] = split[0]\nshops['shop_name_part'] = split[1]\n\n#---------------------------------------------------------------------------   \n# calculating relative shop average sales, for further analysis\n\nsold_items_cnt_per_work_day = transactions.groupby(['shop_id']).item_cnt_day.sum()\/transactions.groupby(['shop_id']).date.nunique()\nshops = shops.join((sold_items_cnt_per_work_day\/sold_items_cnt_per_work_day.mean()).to_frame('shop_size'))","24a6d9ce":"sales = transactions\\\n.join(items.set_index('item_id'), on='item_id')\\\n.join(categories.set_index('item_category_id'), on='item_category_id')\\\n.join(shops.set_index('shop_id'), on='shop_id')\\\n.loc[lambda x: (x.cat_to_drop==0)&(x.shop_to_drop==0)]","269fc40d":"_ = sales.groupby('date').shop_id.nunique()\n\nsales = sales\\\n.set_index('date')\\\n.join(_.to_frame('shops_opened'))\\\n.reset_index()\\\n.assign(shop_compensation = lambda x: (x.shop_size*x.shops_opened)\/(_.mean()))\\\n.assign(item_cnt_day_sw = lambda x: x.item_cnt_day\/x.shop_compensation)","27ac57c6":"fig, ax = plt.subplots(sales.cat_global.nunique(),sharex=True,figsize=(max_width,sales.cat_global.nunique()*2))\nstart = datetime.date(2013,1,1)\nend = datetime.date(2015,10,31)\n\nfor i,j in enumerate(sales.cat_global.unique()):\n    \n    plt.subplots_adjust(hspace=0)\n    \n    view = sales.loc[(sales.cat_global==j)].groupby('date').item_cnt_day.sum().clip(0,)\n\n    ax[i].plot(view, linewidth=1.1)  \n    ax[i].set_title(' '+j, loc='center', fontsize=10, y=0.8, fontweight='bold')\n    ax[i].set_xlim([start,end])\n    \n    ax[i].set_ylim(0,)\n    ax[i].yaxis.set_major_locator(tic.FixedLocator([0,view.max()\/2]))\n    \n    ax[i].xaxis.set_major_locator(mdates.YearLocator())\n    ax[i].xaxis.set_major_formatter(mdates.DateFormatter('%Y   -   '))\n    \n    ax[i].xaxis.set_minor_locator(mdates.MonthLocator())\n    ax[i].xaxis.set_minor_formatter(mdates.DateFormatter('%m'))\n\nfig.autofmt_xdate(which='both',rotation=90)","4ca7adea":"m = sales.loc[(~sales.month.isin([12,1,2]))&(sales.day!=31)]\\\n.groupby('day').item_cnt_day_sw.sum()\nm = m\/m.mean()\n\n_ = sales.loc[~sales.month.isin([12,1])]\\\n.groupby(['cat_global','week_day'])\\\n.item_cnt_day_sw.sum().unstack().T\n\nweekly_cat = _.div(_.mean())\n\nsales = sales\\\n.set_index(['cat_global','week_day'])\\\n.join(weekly_cat.loc[:,weekly_cat.columns!='delivery'].unstack().to_frame('week_day_effect_cat'), how='left')\\\n.fillna(value={'week_day_effect_cat':1})\\\n.reset_index()\\\n.assign(item_cnt_day_sw=lambda x: x.item_cnt_day_sw\/x.week_day_effect_cat)","044ee287":"fig, ax = plt.subplots(1,3, sharey=True, figsize=(max_width-1,3))\n\nax[0].plot(m)\nax[1].plot(_.T.mean().div(_.T.mean().mean()))\nax[2].plot(_.div(_.mean()))\n\nax[0].axhline(1, linestyle='-', color='gray', linewidth=0.5)\nax[1].axhline(1, linestyle='-', color='gray', linewidth=0.5)\nax[2].axhline(1, linestyle='-', color='gray', linewidth=0.5)\n\nax[0].set_title('monthly')\nax[1].set_title('weekly global')\nax[2].set_title('weekly per category')\n\nax[1].tick_params(labelleft=True)\nax[2].tick_params(labelleft=True)\n\nax[0].set_ylim(0,2)\n\nax[0].yaxis.set_major_locator(tic.LinearLocator(numticks=5))\n\nax[0].set_xlim(1,30)\nax[1].set_xlim(0,6)\nax[2].set_xlim(0,6)\n\nax[2].legend(_.columns,\n             loc='center left', \n             bbox_to_anchor=(1.05, 0.5), \n             frameon=False)\n\nplt.show()","0b7d1fcc":"sales['days_from_sale_start'] = (sales.date-sales.sale_start).dt.days\n\n_ = sales.loc[(sales.sale_start>=datetime.datetime(2013, 6, 1))\n             &(~sales.cat_global.isin(['delivery']))\n             &(sales.days_from_sale_start<=60)] \n\n_ = pd.pivot_table(_,\n               index=['cat_global','item_id'],\n               columns='days_from_sale_start',\n               values='item_cnt_day_sw',\n               aggfunc='sum').fillna(0).reset_index()\\\n.drop('item_id',axis=1) \n\n\nnew_item_effect_cat = (_.groupby('cat_global').mean()).div((_.groupby('cat_global').mean()).mean(axis=1),axis=0).T\nnew_item_effect_cat = new_item_effect_cat.div(new_item_effect_cat.tail(20).mean())\n\nnew_item_effect_glob  = (_.mean()\/(_.mean().mean())).T\nnew_item_effect_glob = new_item_effect_glob.div(new_item_effect_glob.tail(20).mean())\n\nsales = sales\\\n.set_index(['cat_global','days_from_sale_start'])\\\n.join(new_item_effect_cat.unstack().to_frame('new_item_effect_cat'), how='left')\\\n.fillna(value={'new_item_effect_cat':1})\\\n.reset_index()\\\n.assign(item_cnt_day_sw=lambda x: x.item_cnt_day_sw\/x.new_item_effect_cat)","b429f5b7":"fig, ax = plt.subplots(1,2, sharey=True, sharex=True, figsize=(max_width-1,3))\n\nax[0].plot(new_item_effect_glob)\nax[1].plot(new_item_effect_cat)\n\nax[1].tick_params(labelleft=True)\n\n\nax[0].set_title('global')\nax[1].set_title('per category')\n\n\nax[0].set_ylim(0,20)\nax[0].set_xlim(0,60)\n\nax[1].legend(new_item_effect_cat.columns,\n             loc='center left', \n             bbox_to_anchor=(1.05, 0.5), \n             frameon=False)\n\nplt.show()","bee8890d":"year_size = sales.loc[~sales.month.isin([1,2,11,12])].groupby('year').item_cnt_day_sw.sum()\\\n.div(sales.loc[~sales.month.isin([1,2,11,12]),'item_cnt_day_sw'].sum()).multiply(3).to_frame('year_size')\n\nsales = sales.set_index('year')\\\n.join(year_size).reset_index()\\\n.assign(item_cnt_day_sw=lambda x: x.item_cnt_day_sw\/x.year_size)","6c6f560a":"ax = year_size.plot(kind='bar', color=\"coral\", fontsize=10, legend=False, figsize=(4,3));\nax.set_alpha(0.8)\nax.set_ylim(0,1.5)\nax.set_xlabel(None)\nax.set_title('relative year size by sales')\n\n\nax.axes.get_yaxis().set_visible(False)\n\n\nfor i in ax.patches:\n    ax.text(i.get_x()+.11, i.get_height()+.05, \\\n            str(round((i.get_height())*100, 2))+'%')","4ae87161":"_ = sales.loc[(sales.cat_global!='delivery')&(sales.date>=datetime.datetime(2013, 3, 1))]\\\n.groupby(['cat_global','year','day_of_year']).item_cnt_day_sw.sum()\\\n.groupby(['cat_global','day_of_year']).mean()\n\nsales = sales\\\n.set_index(['cat_global','day_of_year'])\\\n.join(_.unstack().T.div(_.unstack().T.mean()).unstack().to_frame('seasonality_annual_cat'), how='left')\\\n.fillna(value={'seasonality_annual_cat':1})\\\n.reset_index()","07710100":"fig, ax = plt.subplots(figsize=(max_width,4))\n\n\n_ = sales.loc[sales.cat_global!='delivery']\\\n.groupby(['year','day_of_year']).item_cnt_day_sw.sum()\\\n.groupby(['day_of_year']).mean()\n\n_=_\/_.mean()\n\nax.plot(_, linewidth=2)\n\nax.set_xticks([1,32,60,91,121,152,182,213,244,274,305,335])\nax.set_xticklabels(['Jan','Feb','Mar','Apr','May','Jun','Jul','Aug','Sep','Oct','Nov','Dec'],rotation=90)\n\nax2 = ax.twiny()\nax2.set_xlim(1,365)\nax2.set_xticks([7,54,67,121,129,163,308,364.5])\nax2.set_xticklabels(['Christmas','Fatherland Defender',\n                     'Womens Day','Labour Day',\n                     'Victory Day','Russia Day',\n                     'Unity Day','New Year'],rotation=90,color='red')\n\nax2.xaxis.grid(True, which='major')\n\nax.set_xlim(1,365)\nax.set_ylim(0,5)\n\nax.axes.get_yaxis().set_visible(False)\nax.set_title('annual seasonality in relative values')\n\nplt.show()","05abf761":"sales['item_cnt_day_deseasoned'] = sales.item_cnt_day\\\n                                .div(sales.week_day_effect_cat)\\\n                                .div(sales.seasonality_annual_cat)","736d89af":"fig, (ax1,ax2) = plt.subplots(2,figsize=(max_width,5))\n\ncond1 = (sales.date>=datetime.datetime(2013, 11, 1))&(sales.cat_global=='games')\ncond2 = (sales.date>=datetime.datetime(2013, 11, 1))&(sales.cat_global!='games')\n\nsales.loc[cond1].assign(price_to_mean=sales.item_price\/sales.price_mean)\\\n.groupby('date').price_to_mean.mean()\\\n.div(sales.loc[cond2].assign(price_to_mean=sales.item_price\/sales.price_mean)\\\n.groupby('date').price_to_mean.mean())\\\n.plot(ax=ax1,color='coral')\n\nplt.subplots_adjust(hspace=0)\n\nsales.loc[cond1]\\\n.groupby('date').item_cnt_day_sw.sum()\\\n.plot(ax=ax2)   \n\nax1.set_ylim(0.64,1.16)\nax2.set_ylim(0,5500)\n\nax1.set_ylabel('price to price mean')\nax2.set_ylabel('daily sale count')\n\n\nfig.patches.extend([plt.Rectangle((0.82,0.125),0.029,0.757,\n                                  fill=True, color='gray', alpha=0.2,\n                                  transform=fig.transFigure, figure=fig)])\n\nfig.patches.extend([plt.Rectangle((0.626,0.125),0.029,0.757,\n                                  fill=True, color='gray', alpha=0.2,\n                                  transform=fig.transFigure, figure=fig)])\n\nfig.patches.extend([plt.Rectangle((0.217,0.125),0.029,0.757,\n                                  fill=True, color='gray', alpha=0.2,\n                                  transform=fig.transFigure, figure=fig)])\n\n\nax1.set_title('discount effect in gaming categories')\n\nplt.show()","30aeb53e":"######\nsales = sales.loc[(sales.shop_in_test==1)] #(sales.shop_id!=36)&","52fa90a9":"#data\npiv = np.log(pd.pivot_table(sales,\n                            index='date',\n                            columns=['shop_id','shop_name'],\n                            values='item_cnt_day',\n                            aggfunc='sum').fillna(0).clip(0,)+1).T\n\n#plotting\n\nfig, ax = plt.subplots(figsize=(max_width-1,9))\n\n_y = list(range(piv.shape[0]))[::-1]\n_x = mdates.date2num(piv.columns)\n    \nax.imshow(piv, aspect='auto', extent = [ _x[0],  _x[-1]+1,  _y[0]+1, _y[-1]], interpolation='none')\n\nax.xaxis_date()\nax.xaxis.set_major_locator(mdates.YearLocator())\nax.xaxis.set_major_formatter(mdates.DateFormatter('%Y - %m'))\nax.xaxis.set_minor_locator(mdates.MonthLocator())\nax.xaxis.set_minor_formatter(mdates.DateFormatter('%m'))\n\nax.yaxis.set_ticks(_y)\nax.set_yticklabels(piv.index.get_level_values(0)[::-1].astype('str') + ' ' + piv.index.get_level_values(1)[::-1],\n                  fontsize=8, va='top')\n\nfig.autofmt_xdate(which='both', rotation=90, ha='left')\n\n\nplt.tight_layout()\nplt.show()","c18160ba":"del f_sales,transactions,f_transactions","ecf9019d":"def get_monthly_list(n):  \n    #generate n blocks of days, n0 is test november, n+ are preceding month\n    \n    L=[0]*n\n    for i in range(n):\n        L[i]=pd.date_range(start=pd.Timestamp(2015, 11-i, 1), end=pd.Timestamp(2015, 12-i, 1),closed='left').values\n    return L\n\ndef get_30d_list(n):\n    #generate n blocks of days, n0 is test november, n+ are preceding 30 day intervals from 27th of september, \n    #each shifting 7 days back \n    \n    L=[0]*n\n    L[0]=pd.date_range(pd.Timestamp(2015, 11, 1), freq='D', periods=30).values\n    for i in range(n-1):\n        L[i+1]=pd.date_range(pd.Timestamp(2015, 9, 27), freq='D', periods=30).values-np.timedelta64(14*i, 'D')\n    return L\n\ndef get_grid(sales,L):\n    #generate n blocks of days, n0 is test november, n+ are preceding 30 day intervals from 27th of september, \n    #each shifting 7 days back \n    \n    index_cols = ['shop_id', 'item_id', 'date_block']\n    grid = [] \n    for i in range(len(L)-1):\n        cur_shops  = sales[sales.date.isin(L[i+1])]['shop_id'].unique()\n        cur_items  = sales[sales.date.isin(L[i+1])]['item_id'].unique()\n        grid.append(np.array(list(product(*[cur_shops, cur_items, [i+1]])),dtype='int32'))\n\n\n    grid = pd.DataFrame(np.vstack(grid), columns = index_cols,dtype=np.int32)\\\n            .append(test.loc[:,['shop_id','item_id']].assign(date_block=0))\n    return grid","c4071f17":"def get_features_normal(grid,L):\n    date_length_max = (np.datetime64(L[-1][0],'D')-np.datetime64('2013-01-01','D'))\n\n    X=[0]*len(L)\n    y=[0]*len(L)\n\n    for block in range(len(L)):\n        date_start = L[block][0]\n        \n        col_drop = ['shop_in_test',  \n                    'item_category_name',\n                    'cat_to_drop',\n                    'shop_name',\n                    'shop_name_part',\n                    'cat_in_test',\n                    'shop_to_drop'\n                    ] #,\n\n        data = sales.loc[(sales.date<date_start) & (sales.date>=date_start-date_length_max),\n                         ~sales.columns.isin(col_drop)]\n\n        X_temp = grid.loc[grid.date_block==block]\\\n                .join(items.loc[:,items.columns.isin(['item_id','item_category_id'])].set_index('item_id'), on='item_id')\\\n                .join(categories.set_index('item_category_id'), on='item_category_id')\\\n                .join(shops.set_index('shop_id'), on='shop_id')\\\n                .loc[:,lambda x : ~x.columns.isin(col_drop)]\n        \n\n#--------------------------------------------------\n        \n\n# date features, first\/last item sale in shop and global: \n        \n        lt = len(X_temp.columns)\n        X_temp = X_temp\\\n                .join(data.groupby('item_id')\\\n                      .agg(days_first_item_sold_glob=('date','min'),\n                           days_last_item_sold_glob=('date','max')),\n                      on='item_id')\\\n                .join(data.groupby(['item_id','shop_id'])\\\n                      .agg(days_first_item_sold_shop=('date','min'),\n                           days_last_item_sold_shop=('date','max')),\n                      on=['item_id','shop_id'])\\\n        \n        \n        \n        X_temp.iloc[:,lt:] = ((date_start-X_temp.iloc[:,lt:])\/np.timedelta64(1, 'D')).fillna(-1)\n        \n        \n#--------------------------------------------------        \n# global features: \n\n        X_temp = X_temp\\\n                .join(data.groupby('item_id')\\\n                       .agg(item_price_mean=('item_price','mean')),\n                      on='item_id')\\\n                .join(data.groupby('item_category_id')\\\n                       .agg(cat_price_mean=('item_price','mean')),\n                      on='item_category_id')\\\n        \n        X_temp = X_temp.assign(new_item = lambda x: x.item_price_mean.isna()*1)\n        \n        X_temp.loc[X_temp.cat_price_mean.isna(),['cat_price_mean']] = data.item_price.mean()        \n        X_temp.loc[X_temp.item_price_mean.isna(),['item_price_mean']] = X_temp['cat_price_mean']  \n\n        \n        lt = len(X_temp.columns)\n        \n        for i in ['item_category_id']:\n       \n            X_temp = X_temp.join(sales.groupby(['shop_id',i]).item_cnt_day.sum().to_frame('s1')\\\n                    .join(sales.groupby('shop_id').item_cnt_day.sum().to_frame('s2'))\\\n                    .join(sales.groupby(i).item_cnt_day.sum().to_frame('g1'))\\\n                    .assign(g2=sales.item_cnt_day.sum())\\\n                    .assign(**{i+'_shop_to_cat_glob': lambda x: (x.s1\/x.s2)\/(x.g1\/x.g2)})\\\n                    .drop(['s1','s2','g1','g2'],axis=1),\n                                on=['shop_id',i])\n        \n            X_temp.iloc[:,-1].fillna(0,inplace=True)\n#--------------------------------------------------        \n# target:     \n        X_temp = X_temp.join(sales.loc[sales.date.isin(L[block])].groupby(['item_id','shop_id'])\\\n                      .agg(target=('item_cnt_day','sum')),\n                      on=['item_id','shop_id'])\n\n    \n#--------------------------------------------------\n# lag features, aggregations in past periods:    \n\n        \n        j0 = 0\n        \n        for j in [30,120]:\n            ll = len(X_temp.columns)\n            dd = data.loc[(data.date>=date_start-np.timedelta64(j, 'D'))&\n                          (data.date<date_start-np.timedelta64(j0, 'D'))]\n            \n                \n            X_temp = X_temp\\\n                .join(dd.groupby('item_id')\\\n                      .agg(item_glob_sales=('item_cnt_day','sum')),on='item_id')\\\n                .join(dd.groupby(['shop_id','item_id'])\\\n                       .agg(item_shop_sales=('item_cnt_day','sum')),on=['shop_id','item_id'])\\\n                .join(dd.groupby('item_category_id')\\\n                      .agg(cat_glob_sales=('item_cnt_day','sum')),on='item_category_id')\\\n                .join(dd.groupby(['shop_id','item_category_id'])\\\n                       .agg(cat_shop_sales=('item_cnt_day','sum')),on=['shop_id','item_category_id'])\\\n                .join(dd.groupby('item_id')\\\n                      .agg(item_glob_days=('date','nunique')),on='item_id')\\\n                .join(dd.groupby(['item_id','shop_id'])\\\n                      .agg(item_shop_days=('date','nunique')),on=['item_id','shop_id'])\\\n\n                \n                \n\n            X_temp.rename(columns=dict(zip(X_temp.iloc[:,ll:].columns,\n                                           X_temp.iloc[:,ll:].columns+'_'+str(j0)+'-'+str(j))),\n                          inplace=True)\n            \n            j0 = j\n            \n        X_temp.iloc[:,lt:] = X_temp.iloc[:,lt:].fillna(0)\n#--------------------------------------------------           \n        X_temp.loc[:,['shop_id','item_category_id']] = X_temp.loc[:,['shop_id','item_category_id']].astype('str')\n\n        X[block] = X_temp.loc[:,~X_temp.columns.isin(['target','cat_digital','shop_city'])] #,'item_id'\n        y[block] = X_temp.target.fillna(0).clip(0,20)\n    \n    \n    X = pd.concat(X)\n    \n    print('features - done')\n    \n    return X,y","e15c0ee0":"def data_for_cb(X,y,tb,bn,cat_features):\n    \n    tune_set_cb  = Pool(data=X.loc[X.date_block.isin(range(tb+1,bn))],\n                        label=pd.concat(y[tb+1:]),\n                        cat_features=cat_features)   \n\n    eval_set_cb  = Pool(data=X.loc[X.date_block.isin([1])],\n                        label=y[1],\n                        cat_features=cat_features)\n\n    train_set_cb = Pool(data=X.loc[X.date_block.isin(range(1,bn-tb))],\n                        label=pd.concat(y[1:-tb]),\n                        cat_features=cat_features)    \n\n    test_set_cb  = Pool(data=X.loc[X.date_block.isin([0])],\n                        label=y[0],\n                        cat_features=cat_features)\n    \n    print('cb data - done')\n    \n    return tune_set_cb, eval_set_cb, train_set_cb, test_set_cb","6a14c092":"def encode_X(X,cat_features):\n    \n    X = pd.concat([X.loc[:,~X.columns.isin(cat_features)],\n                        pd.get_dummies(X.loc[:,cat_features])], axis=1) \n\n    _ = X.date_block.values\n    X = pd.DataFrame(data=StandardScaler().fit_transform(X), columns=X.columns)\n    X.iloc[:,23:] = X.iloc[:,23:]*0.1\n    X.date_block = _\n    \n    return X","ff6f0ed3":"def data_for_rf_knn(X,y,tb,bn):\n    \n    X_tune_enc  = X.loc[X.date_block.isin(range(tb+1,bn))].astype(np.float32)\n    y_tune_enc  = pd.concat(y[tb+1:]).astype(np.float32)\n\n    X_eval_enc  = X.loc[X.date_block.isin([1])].astype(np.float32)\n    y_eval_enc  = y[1].astype(np.float32)\n\n    X_train_enc = X.loc[X.date_block.isin(range(1,bn-tb))].astype(np.float32)\n    y_train_enc = pd.concat(y[1:-tb]).astype(np.float32)\n\n    X_test_enc  = X.loc[X.date_block.isin([0])].astype(np.float32)\n    y_test_enc  = y[0].astype(np.float32)\n    \n    print('rf and knn data - done')\n    \n    return X_tune_enc,y_tune_enc,X_eval_enc,y_eval_enc,X_train_enc,y_train_enc,X_test_enc,y_test_enc\n","cfe86182":"cb  = CatBoostRegressor(task_type='GPU',\n                        iterations = 1500, \n                        boosting_type = 'Ordered',\n                        depth = 4)\n\nrf  = RandomForestRegressor(min_samples_leaf=15,\n                           n_jobs=-1,\n                           random_state=0,\n                           n_estimators=60)\n\nknn = KNeighborsRegressor(n_neighbors=30)\n\nlr  = LinearRegression(n_jobs=-1)","903f8067":"def run_models(tune_set_cb, eval_set_cb, train_set_cb, test_set_cb,X_tune_enc,y_tune_enc,X_eval_enc,y_eval_enc,X_train_enc,y_train_enc,X_test_enc,y_test_enc):\n    \n    eval_df = pd.DataFrame(np.zeros((len(y_eval_enc),5)),columns=['rf','cb','knn','lr','y'])\n    test_df = pd.DataFrame(np.zeros((len(y_test_enc),5)),columns=['rf','cb','knn','lr','y'])\n    \n    print(' ')\n    print('tuning...')\n\n    rf.fit(X_tune_enc,y_tune_enc.values)\n    print('rf - done')\n\n    cb.fit(tune_set_cb, silent=True)\n    print('cb - done')\n\n    knn.fit(cudf.DataFrame.from_pandas(X_tune_enc), y_tune_enc.values)\n    print('knn - done')\n\n    eval_df.cb  = cb.predict(eval_set_cb).clip(0,20)\n    eval_df.rf  = rf.predict(X_eval_enc).clip(0,20)\n    eval_df.knn = knn.predict(cudf.DataFrame.from_pandas(X_eval_enc)).to_pandas().clip(0,20)\n    eval_df.y   = y_eval_enc.values\n\n    print('rf','{:.4}'.format(mean_squared_error(eval_df.y, eval_df.rf,squared=False)))\n    print('cb','{:.4}'.format(mean_squared_error(eval_df.y, eval_df.cb,squared=False)))\n    print('knn','{:.4}'.format(mean_squared_error(eval_df.y, eval_df.knn,squared=False)))\n\n    lr.fit(eval_df.iloc[:,:-2],eval_df.y)\n    print('lr - done')\n\n    eval_df.lr = lr.predict(eval_df.iloc[:,:-2]).clip(0,20)\n\n    print('lr','{:.4}'.format(mean_squared_error(eval_df.y, eval_df.lr,squared=False)))\n    print('model weights',\n          'rf',\n          '{:.1%}'.format(lr.coef_[0]),\n          'cb',\n          '{:.1%}'.format(lr.coef_[1]),\n          'knn',\n          '{:.1%}'.format(lr.coef_[2]))\n\n    print(' ')\n    print('training...')\n\n    rf.fit(X_train_enc,y_train_enc.values)\n    print('rf - done')\n    \n    cb.fit(train_set_cb, silent=True)\n    print('cb - done')\n\n    knn.fit(cudf.DataFrame.from_pandas(X_train_enc), y_train_enc.values)\n    print('knn - done')\n\n    test_df.rf  = rf.predict(X_test_enc).clip(0,20)\n    test_df.cb  = cb.predict(test_set_cb).clip(0,20)\n    test_df.knn = knn.predict(cudf.DataFrame.from_pandas(X_test_enc)).to_pandas().clip(0,20)\n    test_df.lr  = lr.predict(test_df.iloc[:,:-2]).clip(0,20)\n\n    print('all done')\n\n    return test_df.lr","2d4b706d":"print('start monthly models...')\nprint(' ')\n\nbn = 4\ntb = 1\ncat_features = ['cat_global','cat_platform','shop_id','item_category_id']\n\nL = get_monthly_list(bn)\ngrid = get_grid(sales,L)\nX,y = get_features_normal(grid,L)\n\ntune_set_cb, eval_set_cb, train_set_cb, test_set_cb = data_for_cb(X,y,tb,bn,cat_features)\n\nX = encode_X(X,cat_features)\n\nX_tune_enc,y_tune_enc,X_eval_enc,y_eval_enc,X_train_enc,y_train_enc,X_test_enc,y_test_enc = data_for_rf_knn(X,y,tb,bn)\n\ndel X,y\n\npred_monthly = run_models(tune_set_cb, eval_set_cb, train_set_cb, test_set_cb,X_tune_enc,y_tune_enc,X_eval_enc,y_eval_enc,X_train_enc,y_train_enc,X_test_enc,y_test_enc)","a0d611e2":"print(' ')\nprint('start 30d models...')\nprint(' ')\n\nbn = 7\ntb = 2\ncat_features = ['cat_global','cat_platform','shop_id','item_category_id']\n\nL = get_30d_list(bn)\ngrid = get_grid(sales,L)\nX,y = get_features_normal(grid,L)\n\ntune_set_cb, eval_set_cb, train_set_cb, test_set_cb = data_for_cb(X,y,tb,bn,cat_features)\n\nX = encode_X(X,cat_features)\n\nX_tune_enc,y_tune_enc,X_eval_enc,y_eval_enc,X_train_enc,y_train_enc,X_test_enc,y_test_enc = data_for_rf_knn(X,y,tb,bn)\n\ndel X,y\n\npred_30d = run_models(tune_set_cb, eval_set_cb, train_set_cb, test_set_cb,X_tune_enc,y_tune_enc,X_eval_enc,y_eval_enc,X_train_enc,y_train_enc,X_test_enc,y_test_enc)","3accde88":"f_sample.item_cnt_month = (pred_monthly*0.5+pred_30d*0.5).clip(0,20)\nf_sample.to_csv(r'submission.csv',index=False)","cbc9167c":"Visualizing the data that will be used for training:","e62061e4":"After adjusting sales to all the effects above we finally may calculate annual seasonality, global and for each category.\nSince we had only 3 years, weekly fluctuations are slightly visible.\nNational holidays are clearly visible aswell.","f54bc797":"It is also interesting how price affect sales. For example temporary actions are launched for games and usually are a \"plain discount\" or \"buy one get other for free\" type.\nList of all actions may be found here: https:\/\/www.1c-interes.ru\/special_actions ","78010e50":"We have almost 3 years of transactions from several 1C shops, aggregated daily:","fe3b25ab":"![30d](https:\/\/tdkgva.am.files.1drv.com\/y4m7Qc5M5fdQTawYZmR-9Pe9Blw6LeNhR0NFrusFBEiTEh45zoVPIGaKyHvnlB2bEkesPYyOHHWuIFdkFMbFJQQwYE3hB5mZ1rorBATomBnLqk_eOAQXg7awmc1W5DDbeSVcN65qfyY6vZlnV1jxVcQEZr3ply6eBaCYuNYXwor-gND28L8Y5ZnGcgPvwFIJaADaeQ1OSr3Wyp0IObQZV91Pw?width=709&height=299&cropmode=none)","90ab5020":"#### Available data:","77107b38":"In the cells below I form time blocks and generate relateg features. The code is more or less readablee: ","fc00a515":"One more thing that we may notice from spikes is that sales count depends on days passed after release date. Separating this effect will make seasonality less noisy.  ","df74f909":"### Import libraries","277decce":"Forming splits for random forest and knn:","1976efb8":"After preprocessing is done I combine everything into one dataframe.","4f1f0f10":"## Submission","a5667294":"![abc](https:\/\/gb7esq.am.files.1drv.com\/y4mnloWp3MGoGY05jQxYinL30uVQRpkv1X7iE9bIFG0L9pn0OSxDsSfXtc5bPszg6-DAwJLiwQuBqsss-hCdmvQK84vIuingocy11JNY3YUOi6XhLyGcfIVmJXD8KVattepX_FA8REqTHImwWuBvLVp8_rF7EgnqP5XK96F2qZPwcERgFjCxQCaGfUaCuKeu_HBQWeGDEJvYydFF_d8y3q_XA?width=709&height=119&cropmode=none)","83cd9238":"30 day split:","f9fb64ba":"Since the ammount of opened and closed shops is different all the time, to separate seasonal effects I adjust item sales by the ammount and size of shops that are opened at any given point:","95a4a9b2":"Overall sales trend looks to be declining, so I calculate relative year size to adjust sales count. ","d48206eb":"It is also oblious that different catgories and shops will have different seasonalities, lets look at it first: ","4b30b83c":"Now it is possible to calculate deaseasonalized item sales count:","96d4451e":"### tune - evaluate - train - test splitting\n\nDue to strong weekly seasonality - I think that it is better to split data not only by monthly chunks, but also try 30 day splits that have same ammount of weekdays.","e566dd62":"#### How to form evaluation sample:","c9c68e35":"Basic preprocessing for future EDA needs, with comments in code:","b173556b":"monthly split:","5daaf196":"#### First look - item sales per shop per day in relative numbers as a heatmap","25ada401":"Some conclusions that we may make from the plot below:\n* shops open and close, may be closed for a day or a month\n* some shops may have wrong id's (id 0,1 vs 57,58, and 10 vs 11)\n* shops differ in type, some are seasonal sales (id 9 and 20)\n* not all shops are in test, but some are stable and may give extra info (for example id 29,30,54)\n* when looking at the demand over time, closed shops may distort the picture","0615eb21":"I see clear weekly trends, but monthly seasonality is also possible. From the charts below we can make an important note - for future cross validation:\nsince weekly seasonality is stronger it is better to use 30 day training splits with the same ammount of weekdays, instead of using default monthly split.","13214ad9":"Please be aware that this notebook is a work in progress, features and model parameters are not yet perfectly tuned.","5279fe9d":"## Preprocessing","deb32f49":"I use 3 first level models - catboost, random forest from sklearn, knn from rapids ai. Linear model is used on meta features.","c5b655b5":"#### What is in test sample:","bae894bc":"# Overview","55131367":"# Model training","9b44c644":"Encoding data for random forest and knn:","93494a05":"## Seasonality","f9ed6cfa":"# Feature engineering","0f8e3b06":"Forming splits for catboost:"}}