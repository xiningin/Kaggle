{"cell_type":{"6b970989":"code","04705996":"code","2d12b306":"code","d08f710b":"code","c6a97642":"code","9d268921":"code","cfd0c353":"code","b7c1ed7a":"code","acc78594":"code","aec340d4":"code","089b7306":"code","27749d06":"code","06d40a51":"code","968cab03":"code","e9a04842":"code","afae89eb":"code","8e6546b8":"code","d24cbf3a":"code","492aa0c6":"code","5862d78b":"code","aca08285":"code","3a5290ce":"code","93de1a3b":"code","c633ce3b":"code","cb666c6a":"code","da0e20a6":"code","e1f09d83":"code","5895b287":"code","ce466d01":"code","11a2f83c":"markdown","372e44a6":"markdown","3321e6e9":"markdown","69ac1c4c":"markdown","520d8a9c":"markdown","d37022b7":"markdown","e500a3f4":"markdown","83f1fb79":"markdown","a533e4bd":"markdown","76d3980f":"markdown","114c6884":"markdown"},"source":{"6b970989":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nimport operator\n#import gc\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy.stats import describe\n%matplotlib inline\n\nfrom sklearn.model_selection import KFold\nfrom sklearn.preprocessing import LabelEncoder\n\nfrom sklearn.metrics import mean_squared_error\nimport lightgbm as lgb\nimport xgboost as xgb","04705996":"print(os.listdir(\"..\/input\"))","2d12b306":"train_df = pd.read_csv('..\/input\/train.csv')\ntrain_df.head()","d08f710b":"train_df.shape","c6a97642":"train_df.info()","9d268921":"train_df.isnull().values.sum(axis=0)","cfd0c353":"train_df_describe = train_df.describe()\ntrain_df_describe","b7c1ed7a":"test_df = pd.read_csv('..\/input\/test.csv')\ntest_df.head()","acc78594":"test_df.shape","aec340d4":"test_df.info()","089b7306":"test_df.isnull().values.sum(axis=0)","27749d06":"test_df_describe = test_df.describe()\ntest_df_describe","06d40a51":"plt.figure(figsize=(12, 5))\nplt.hist(train_df.Target.values, bins=4)\nplt.title('Histogram target counts')\nplt.xlabel('Count')\nplt.ylabel('Target')\nplt.show()","968cab03":"np.unique(train_df.Target.values)","e9a04842":"pd.value_counts(train_df.Target)","afae89eb":"columns_to_use = train_df.columns[1:-1]","8e6546b8":"columns_to_use","d24cbf3a":"y = train_df['Target'].values-1","492aa0c6":"train_test_df = pd.concat([train_df[columns_to_use], test_df[columns_to_use]], axis=0)\ncols = [f_ for f_ in train_test_df.columns if train_test_df[f_].dtype == 'object']\n\nfor col in cols:\n    le = LabelEncoder()\n    le.fit(train_test_df[col].astype(str))\n    train_df[col] = le.transform(train_df[col].astype(str))\n    test_df[col] = le.transform(test_df[col].astype(str))\ndel le","5862d78b":"train = lgb.Dataset(train_df[columns_to_use].astype('float'),y ,feature_name = \"auto\")","aca08285":"params = {\n    'boosting_type': 'gbdt',\n    'objective': 'multiclass',\n    'metric': 'multi_logloss',\n    'max_depth': 5,\n    'num_leaves': 31,\n    'learning_rate': 0.05,\n    'feature_fraction': 0.9,\n    'bagging_fraction': 0.6,\n    'bagging_freq': 5,\n    'verbose': -1,\n    'num_threads': 6,\n    'lambda_l2': 1.0,\n    'min_gain_to_split': 0,\n    'num_class': len(np.unique(y)),\n}","3a5290ce":"clf = lgb.train(params,\n        train,\n        num_boost_round = 500,\n        verbose_eval=True)","93de1a3b":"preds1 = clf.predict(test_df[columns_to_use])\n","c633ce3b":"xgb_params = {\n        'learning_rate': 0.1,\n        'n_estimators': 1000,\n        'max_depth': 5,\n        'min_child_weight': 1,\n        'gamma': 0,\n        'subsample': 0.9,\n        'colsample_bytree': 0.84,\n        'objective': 'multi:softprob',\n        'scale_pos_weight': 1,\n        'eval_metric': 'merror',\n        'silent': 1,\n        'verbose': False,\n        'num_class': 4,\n        'seed': 44}\n    \nd_train = xgb.DMatrix(train_df[columns_to_use].values.astype('float'), y)\nd_test = xgb.DMatrix(test_df[columns_to_use].values.astype('float'))\n    \nmodel = xgb.train(xgb_params, d_train, num_boost_round = 500, verbose_eval=100)\n                        \nxgb_pred = model.predict(d_test)","cb666c6a":"xgb_pred.shape","da0e20a6":"preds = 0.5*preds1 + 0.5*xgb_pred\n\npreds = np.argmax(preds, axis = 1) +1\npreds","e1f09d83":"sample_submission = pd.read_csv(\"..\/input\/sample_submission.csv\")\nsample_submission.head()","5895b287":"sample_submission['Target'] = preds\nsample_submission.to_csv('simple_lgbm_xgb_1.csv', index=False)\nsample_submission.head()","ce466d01":"np.mean(preds)","11a2f83c":"Now let us look at the input folder. Here we find all the relevant files for this competition.","372e44a6":"To be continued ...","3321e6e9":"Now we want to subset the features, so that they don't include Id and target.","69ac1c4c":"Now is the time to build our first model. We'll use make a simple LGBM model.","520d8a9c":"We see that tehre are only 4 numerical values in for the target. Value 4 seems to dominate, with about 60% of all values. ","d37022b7":"Next, we'll combine the train and test sets, so we can consistenly label encode all the categorical features.","e500a3f4":"We'll set up the new variable ```y``` that will be our target variable for training.","83f1fb79":"## Overview\n\nThe purpose of this kernel is to take a look at the data, come up with some insights, and attempt to create a predictive model or two. This notebook is still **very** raw - the first few EDA sections have been decently explored\/formatted, but the latter predictive sections are completely uncommented. I hope to work on those as my, very limited, time permits.\n\n## Packages\n\nFirst, let's load a few useful Python packages. This section will keep growing in subsequent versions of this EDA.","a533e4bd":"We see that the input folder only contains three files ```train.csv```, ```test.csv```, and ```sample_submission.csv```. It seems that for this competition we don't have to do any complicated combination and mergers of files.\n\nNow let's import and take a glimpse at these files.","76d3980f":"The test set is almost 2.5 times larger than the train set. It also has 5 features with missing data.\n\nNow, let's take a look at the target. We want to see the distribution of target values in the train set.","114c6884":"We see that the train dataset is fairly small - just under 10,000 rows. It has 143 features, including the Id and Target. Total number of features that can be used for training is 141, which includes 8 float-valued, 130 integer-valued, and 4 object valued, which need to be converted to numerical values. We also see that there are 5 features with missing data, including 3 that are dominated by missing values."}}