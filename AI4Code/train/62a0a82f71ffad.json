{"cell_type":{"67cbbf42":"code","6dcb82a4":"code","14add038":"code","6858ef70":"code","9714c32e":"code","cfb7b3c5":"code","c2ff7b05":"code","5b282ca9":"code","320d0bf8":"code","1522446e":"code","cbda611b":"code","cc2d08c8":"code","d3bbec57":"code","82d8204f":"code","319ee1ce":"code","3b5504a6":"code","60adfa45":"code","04f54339":"code","7f898035":"code","116789c9":"code","774b66d8":"code","3979d2e2":"code","d49165bd":"code","f126645e":"code","b2de8273":"code","d0905afb":"code","b238616b":"code","bcfe99d3":"code","7c44e7e4":"code","e916facc":"code","f84b5f09":"code","60e80ca9":"markdown","398b937a":"markdown","51164ac5":"markdown","259a9b45":"markdown","7d9bdf3c":"markdown","45fb2886":"markdown","2ef1b7ec":"markdown","524b370f":"markdown","5a31756b":"markdown","f9f506dc":"markdown","a0a7710d":"markdown","0909c30c":"markdown","b5e6d6ce":"markdown","e2d17de3":"markdown","1383bc7b":"markdown","4f9c31b2":"markdown","7a3eab8a":"markdown"},"source":{"67cbbf42":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os","6dcb82a4":"import pandas as pd\ntraining = pd.read_csv(\"..\/input\/train.csv\");\n\nx_test = pd.read_csv(\"..\/input\/test.csv\");","14add038":"training.head(10)","6858ef70":"training['n_new']= training['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())","9714c32e":"training['n_new'].value_counts()","cfb7b3c5":"training['Title'] = 0\n","c2ff7b05":"training.loc[training[\"n_new\"] == 'Mr', 'Title'] = 1\ntraining.loc[training[\"n_new\"] == 'Miss', 'Title'] = 4\ntraining.loc[training[\"n_new\"] == 'Mrs', 'Title'] = 5\ntraining.loc[training[\"n_new\"] == 'Master', 'Title'] = 3\ntraining.loc[training[\"n_new\"] == 'Dr', 'Title'] = 2","5b282ca9":"import seaborn as sns\n\n\nimport matplotlib.pyplot as plt\n\n\ncorr = training.corr()\nf, ax = plt.subplots(figsize=(25, 25))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nsns.heatmap(corr, cmap=cmap, vmax=.1, center=0,\n            square=True, linewidths=.5)\n","320d0bf8":"corr","1522446e":"print(\"The number of traning examples(data points) = %i \" % training.shape[0])\nprint(\"The number of features we have = %i \" % training.shape[1])","cbda611b":"unique, count= np.unique(training[\"Survived\"], return_counts=True)\nprint(\"The number of occurances of each class in the dataset = %s \" % dict (zip(unique, count) ), \"\\n\" )","cc2d08c8":"training.isna().sum()","d3bbec57":"training.columns","82d8204f":"C = training.Cabin[training.Cabin.isna()]\nC_not = training.Cabin[training.Cabin.notna()]\nC.values[:] = 0\nC_not.values[:] = 1","319ee1ce":"cabine_not = pd.concat([C, C_not]).sort_index()","3b5504a6":"np.random.seed(0)\ntraining['sp'] = training.SibSp + training['Parch']\ntraining['cabine_n'] = cabine_not\ntraining.cabine_n = training.cabine_n.astype(int)\ntraining.drop([\"n_new\", \"Name\" ,\"Embarked\", \"PassengerId\",\"Ticket\",\"Cabin\"], inplace = True, axis = 1 )","60adfa45":"x_train = training\nrepCol9 = {1 : 3 ,   2 : 2 , 3 : 1  }\n\n\nx_train['IsAlone'] = 1\nx_train['IsAlone'].loc[x_train['sp'] > 0] = 0\n\nx_train.replace({'Pclass': repCol9} , inplace = True )\nx_train = pd.get_dummies(x_train)\n\ny_train = x_train[\"Survived\"]\nx_train = x_train.drop(['Survived'], axis = 1)\n\nprint(y_train.shape )\ny_train.head(20)\nx_train.head()","04f54339":"import numpy as np\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer\n\nimp = IterativeImputer(max_iter=10, random_state=0)\nx = imp.fit_transform(x_train)\nx_train = pd.DataFrame(x, columns = x_train.columns)\nx_train[[ 'Pclass', 'SibSp', 'Parch', 'Title', 'sp',\n       'cabine_n', 'IsAlone', 'Sex_female', 'Sex_male']] = x_train[[ 'Pclass', 'SibSp', 'Parch', 'Title', 'sp',\n       'cabine_n', 'IsAlone', 'Sex_female', 'Sex_male']].astype(int)","7f898035":"from scipy import stats\nimport numpy as np\n\nz = np.abs(stats.zscore(x_train))\nzee = (np.where(z > 3))[1]\n\nprint(\"number of data examples greater than 3 standard deviations = %i \" % len(zee))\n# x_train = x_train[(z < 2.5).all(axis=1)]","116789c9":"import matplotlib.pyplot as plt\n\nimport seaborn as sns\nsns.pairplot(training)","774b66d8":"x_test.head()","3979d2e2":"x_test['n_new']= x_test['Name'].apply(lambda x: x.split(',')[1].split('.')[0].strip())","d49165bd":"x_test['Title'] = 0\n\nx_test.loc[x_test[\"n_new\"] == 'Mr', 'Title'] = 1\nx_test.loc[x_test[\"n_new\"] == 'Miss', 'Title'] = 4\nx_test.loc[x_test[\"n_new\"] == 'Mrs', 'Title'] = 5\nx_test.loc[x_test[\"n_new\"] == 'Master', \"Title\"] = 3\nx_test.loc[x_test[\"n_new\"] == 'Dr', 'Title'] = 2","f126645e":"C = x_test.Cabin[x_test.Cabin.isna()]\nC_not = x_test.Cabin[x_test.Cabin.notna()]\nC.values[:] = 0\nC_not.values[:] = 1\n\nx_test['sp'] = x_test.SibSp + x_test['Parch']\nx_test['cabine_n'] = cabine_not\nx_test.cabine_n = x_test.cabine_n.astype(int)\n\nx_test['IsAlone'] = 1\nx_test['IsAlone'].loc[x_test['sp'] > 0] = 0\n\n\nx_test.drop([\"Name\", \"Embarked\", \"Ticket\", \"n_new\", \"Cabin\"], inplace = True, axis = 1 )\nx_test.replace({'Pclass': repCol9} , inplace = True )\nx_test = pd.get_dummies(x_test)\nx_test.head()","b2de8273":"fact = y_train[y_train == 0].count() \/ y_train[y_train == 1].count()\n\nclass_weight = {1: fact, 0: 1.}","d0905afb":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.utils import np_utils\nfrom keras import callbacks\nfrom keras import optimizers\nfrom keras.layers import BatchNormalization\n\n#y_train = np_utils.to_categorical(y_train)\n\nInputDimension = 11\nprint(y_train.shape )\n\nmodel = Sequential()\nmodel.add(Dense(25,input_dim=InputDimension, activation='relu'))\nmodel.add(Dense(50, activation='relu'))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(100, activation='relu'))\nmodel.add(Dense(100, activation='relu'))\n\n\n\nmodel.add(Dense(2, activation='softmax'))\n\n\nearlystopping = callbacks.EarlyStopping(monitor='val_loss', min_delta=0, patience=30, verbose=0, mode='min')\noptimizer = optimizers.Adam(lr=0.001, decay=0.0001)\nmodel.compile(loss='categorical_crossentropy', optimizer=optimizer, metrics=['acc'])\nhistory = model.fit(x_train, pd.get_dummies(y_train), epochs=2000, batch_size=60, validation_split=0.2, verbose=1, callbacks=[earlystopping], class_weight = class_weight)","b238616b":"import matplotlib.pyplot as plt\n\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])","bcfe99d3":"id = x_test['PassengerId']\n\nx_test.drop(['PassengerId'], inplace = True, axis = 1)\n\nimp = IterativeImputer(max_iter=10, random_state=0)\nx = imp.fit_transform(x_test)\nx_test = pd.DataFrame(x, columns = x_train.columns)\nx_test[[ 'Pclass', 'SibSp', 'Parch', 'Title', 'sp',\n       'cabine_n', 'IsAlone', 'Sex_female', 'Sex_male']] = x_test[['Pclass', 'SibSp', 'Parch', 'Title', 'sp',\n       'cabine_n', 'IsAlone', 'Sex_female', 'Sex_male']].astype(int)","7c44e7e4":"predictions = model.predict(x_test)","e916facc":"predictions = np.argmax(predictions, axis = 1)","f84b5f09":"id.reset_index(drop=True, inplace=True)\nout = pd.DataFrame({'PassengerId': id, \"Survived\": predictions})\nout.to_csv('titanic-predictions.csv', index = False)\nout.head(100)","60e80ca9":"Predictions will return probability between 0 and 1 for survived or non survived so i will take the argmax() of the array to get the max index for each test example","398b937a":"# Simple Deep Neural Networks with Keras\nIn this tutorial i am going to show how to implement Deep Neural Networks in keras and Also we will have a look on simple feature engineering to be able to classify the dataset efficiently","51164ac5":"The numbers doesn't seem to be very far from each other, So i will use accuracy for performance eval..","259a9b45":"From the results of correlation matrix and the Nan number and manual checking of dataset, I will drop \"Name\" ,\"Ticket\",\"Cabin\" and \"PassengerId\".\n<lb>I will also store the label column and drop it from training.\n<lb>Then i will engineer the catagorical features and map the strings to integers to be able to use it in the model later.","7d9bdf3c":"Now it is time to design the ML Pipeline. I will use Deep Neural Networks in Keras to classify the dataset. The number of layers i am using is optmized using some error analys of the results.\n<lb> I will use early stopping to stop if the error is not decreasing.","45fb2886":"First let's start by importing our dataset into dataframes using pandas. Dataframes enables us to work easily with data usign its built in functions.","2ef1b7ec":"Now every thing is okay with the dataset so i will predict the output values for submission","524b370f":"Since i dropped some features from the training set I have to drop the same featurres from the test set and do the same steps of feature eng.","5a31756b":"I will count the number of data examples in each class in the target to determine which metric to use while evaluationg performance.","f9f506dc":"I will explore the size of the dataset to compare it with the number of NANs in the dataset","a0a7710d":"Now I will explore the correlations of the featues relative to the target variable.\n#### Note: not all features are listed but only numeric features.","0909c30c":"I will Plot some features to see if there is any pattern in the data.","b5e6d6ce":"Now let's have a look on the dataset and search for important information manually.","e2d17de3":"Now let's see how good is my training with respect to validation accuracies.","1383bc7b":"Now i will check the number of Null values. If most of a column's values are Nulls or NaNs i will drop it because filling it will not be accurate but if the number is small then i will fill it with the mean values.","4f9c31b2":"It is time to make submission.","7a3eab8a":"Now i will normalize the test set as i did before and will fill null values in the dataset as well."}}