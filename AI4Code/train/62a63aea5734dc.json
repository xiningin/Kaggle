{"cell_type":{"9d3ebf10":"code","6c490d4e":"code","d9a55003":"code","6c36499e":"code","59612903":"code","47c6b088":"code","94ecaf9c":"code","005c7098":"code","5a9d3d89":"code","9e59119f":"code","45b4cf30":"code","8a0269ab":"code","f533502d":"code","5855151d":"code","af9a3fe8":"code","490f449c":"code","4a102d1a":"code","ce4ee953":"code","0f45da51":"code","876ac1e5":"code","f1889cdc":"code","75045031":"code","51cd98ef":"code","9d33516c":"code","2cc1ffa0":"code","94e547fa":"code","f8ec72ba":"code","f304ac17":"code","6cdda42d":"code","90c4b699":"code","9d6203da":"code","758a7003":"code","a119110a":"code","9fdd98f2":"code","98af19ad":"code","c8cab4e9":"markdown","ae97a040":"markdown","3e8664bb":"markdown","e2cb2b2c":"markdown","0672edcb":"markdown","d7176ded":"markdown","b2033828":"markdown","4839c171":"markdown","988ea68e":"markdown","190ed875":"markdown","961847f3":"markdown","018e7a90":"markdown","942e82c7":"markdown","a8dd5772":"markdown","eeba5eba":"markdown","c3c10e9f":"markdown","fdb017d6":"markdown","341d1a33":"markdown","844978bc":"markdown","1d384be5":"markdown","e31d8d4f":"markdown","1c5e6eca":"markdown","2f307cf7":"markdown","07b6c1b7":"markdown","3c3dfd94":"markdown","a5130f5d":"markdown","c65d9080":"markdown","6b0ba6ce":"markdown","d85402f8":"markdown","194a3730":"markdown","5ef7a2f7":"markdown","eaf14eef":"markdown","b6da3ddd":"markdown","87ba6e5f":"markdown","65b9e1d3":"markdown","4b59896f":"markdown","500862ff":"markdown","63cb71d3":"markdown"},"source":{"9d3ebf10":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport os\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt, matplotlib.image as mpimg\nimport time \nimport warnings\nfrom sklearn import svm\nfrom sklearn.model_selection import GridSearchCV\n\n%matplotlib inline\nwarnings.filterwarnings('ignore')\n\nprint(os.listdir(\"..\/input\"))","6c490d4e":"data = pd.read_csv('..\/input\/train.csv')\nprint(\"Train Data Shape is: \",data.shape)\ndata.head()","d9a55003":"label = data.label\ndata=data.drop('label',axis=1)\nprint(\"Data Shape: \",data.shape)\nprint(\"Label Shape: \",label.shape)","6c36499e":"data.columns","59612903":"for x in range(0,4):\n    train_0=data[label==x]\n    data_new=[]\n    for idx in train_0.index:\n        val=train_0.loc[idx].values.reshape(28,28)\n        data_new.append(val)\n    plt.figure(figsize=(25,25))   \n    for x in range(1,5):\n        ax1=plt.subplot(1, 20, x)\n        ax1.imshow(data_new[x],cmap='gray')","47c6b088":"y = pd.value_counts(data.values.ravel()).sort_index()\nN = len(y)\nx = range(N)\nwidth =0.9\nplt.figure(figsize=[20,8])\nax1=plt.subplot(1, 2,1)\nax1.bar(x, y, width, color=\"blue\")\nplt.title('Pixel Value Frequency For Gray Scale Images')\nplt.xlabel('Pixel Value (0-255)')\nplt.ylabel('Frequency')\n#ax1.imshow(data_new[x],cmap='gray')\nax2=plt.subplot(1, 2,2)\nax2.bar(x, y, width, color=\"blue\")\nplt.title('Pixel Value Frequency (Log Scale)')\nplt.yscale('log')\nplt.xlabel('Pixel Value (0-255)')\nplt.ylabel('Frequency')\n#plt.yscale('')","94ecaf9c":"train, test,train_labels, test_labels = train_test_split(data, label, train_size=0.8, random_state=42)\nprint(\"Train Data Shape: \",train.shape)\nprint(\"Train Label Shape: \",train_labels.shape)\nprint(\"Test Data Shape: \",test.shape)\nprint(\"Test Label Shape: \",test_labels.shape)","005c7098":"i=5000;\nscore=[]\nfittime=[]\nscoretime=[]\nclf = svm.SVC(random_state=42)","5a9d3d89":"print(\"Default Parameters are: \\n\",clf.get_params)","9e59119f":"start_time = time.time()\nclf.fit(train[:i], train_labels[:i].values.ravel())\nfittime = time.time() - start_time\nprint(\"Time consumed to fit model: \",time.strftime(\"%H:%M:%S\", time.gmtime(fittime)))\nstart_time = time.time()\nscore=clf.score(test,test_labels)\nprint(\"Accuracy for grayscale: \",score)\nscoretime = time.time() - start_time\nprint(\"Time consumed to score: \",time.strftime(\"%H:%M:%S\", time.gmtime(scoretime)))\ncase1=[score,fittime,scoretime]","45b4cf30":"test_b=test\ntrain_b=train\ntest_b[test_b>0]=1\ntrain_b[train_b>0]=1\nfor x in range(0,4):\n    train_0=train_b[train_labels==x]\n    data_new=[]\n    for idx in train_0.index:\n        val=train_0.loc[idx].values.reshape(28,28)\n        data_new.append(val)\n    plt.figure(figsize=(25,25))   \n    for x in range(1,5):\n        ax1=plt.subplot(1, 20, x)\n        ax1.imshow(data_new[x],cmap='binary')","8a0269ab":"start_time = time.time()\nclf.fit(train_b[:i], train_labels[:i].values.ravel())\nfititme = time.time() - start_time\nprint(\"Time consumed to fit model: \",time.strftime(\"%H:%M:%S\", time.gmtime(fittime)))\nscore=clf.score(test_b,test_labels)\nstart_time = time.time()\nclf.fit(train_b[:i], train_labels[:i].values.ravel())\nprint(\"Accuracy for binary: \",score)\nscoretime = time.time() - start_time\nprint(\"Time consumed to score: \",time.strftime(\"%H:%M:%S\", time.gmtime(scoretime)))\ncase2=[score,fittime,scoretime]","f533502d":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA as sklearnPCA\n\n#standardized data\nsc = StandardScaler().fit(train)\nX_std_train = sc.transform(train)\nX_std_test = sc.transform(test)\n\n#If n_components is not set then all components are stored \nsklearn_pca = sklearnPCA().fit(X_std_train)\ntrain_pca = sklearn_pca.transform(X_std_train)\ntest_pca = sklearn_pca.transform(X_std_test)\n\n#Percentage of variance explained by each of the selected components.\n#If n_components is not set then all components are stored and the sum of the ratios is equal to 1.0.\nvar_per = sklearn_pca.explained_variance_ratio_\ncum_var_per = sklearn_pca.explained_variance_ratio_.cumsum()\n\nplt.figure(figsize=(30,10))\nind = np.arange(len(var_per)) \nplt.bar(ind,var_per)\nplt.xlabel('n_components')\nplt.ylabel('Variance')","5855151d":"n_comp=len(cum_var_per[cum_var_per <= 0.90])\nprint(\"Keeping 90% Info with \",n_comp,\" components\")\nsklearn_pca = sklearnPCA(n_components=n_comp)\ntrain_pca = sklearn_pca.fit_transform(X_std_train)\ntest_pca = sklearn_pca.transform(X_std_test)\nprint(\"Shape before PCA for Train: \",X_std_train.shape)\nprint(\"Shape after PCA for Train: \",train_pca.shape)\nprint(\"Shape before PCA for Test: \",X_std_test.shape)\nprint(\"Shape after PCA for Test: \",test_pca.shape)\n","af9a3fe8":"start_time = time.time()\nclf.fit(train_pca[:i], train_labels[:i].values.ravel())\nfittime = time.time() - start_time\nprint(\"Time consumed to fit model: \",time.strftime(\"%H:%M:%S\", time.gmtime(fittime)))\nstart_time = time.time()\nscore=clf.score(test_pca,test_labels)\nprint(\"Accuracy for grayscale: \",score)\nscoretime = time.time() - start_time\nprint(\"Time consumed to score model: \",time.strftime(\"%H:%M:%S\", time.gmtime(scoretime)))\ncase3=[score,fittime,scoretime]","490f449c":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA as sklearnPCA\n\n#standardized data\nsc = StandardScaler().fit(train_b)\nX_std_train = sc.transform(train_b)\nX_std_test = sc.transform(test_b)\n\n#If n_components is not set then all components are stored \nsklearn_pca = sklearnPCA().fit(X_std_train)\n#train_pca_b = sklearn_pca.transform(X_std_train)\n#test_pca_b = sklearn_pca.transform(X_std_test)\n\n#Percentage of variance explained by each of the selected components.\n#If n_components is not set then all components are stored and the sum of the ratios is equal to 1.0.\nvar_per = sklearn_pca.explained_variance_ratio_\ncum_var_per = sklearn_pca.explained_variance_ratio_.cumsum()\n\nplt.figure(figsize=(30,10))\nind = np.arange(len(var_per)) \nplt.bar(ind,var_per)\nplt.xlabel('n_components')\nplt.ylabel('Variance')","4a102d1a":"n_comp=len(cum_var_per[cum_var_per <= 0.90])\nprint(\"Keeping 90% Info with \",n_comp,\" components\")\nsklearn_pca = sklearnPCA(n_components=n_comp)\ntrain_pca_b = sklearn_pca.fit_transform(X_std_train)\ntest_pca_b = sklearn_pca.transform(X_std_test)\nprint(\"Shape before PCA for Train: \",X_std_train.shape)\nprint(\"Shape after PCA for Train: \",train_pca_b.shape)\nprint(\"Shape before PCA for Test: \",X_std_test.shape)\nprint(\"Shape after PCA for Test: \",test_pca_b.shape)","ce4ee953":"start_time = time.time()\nclf.fit(train_pca_b[:i], train_labels[:i].values.ravel())\nfittime = time.time() - start_time\nprint(\"Time consumed to fit model: \",time.strftime(\"%H:%M:%S\", time.gmtime(fittime)))\nstart_time = time.time()\nscore=clf.score(test_pca_b,test_labels)\nprint(\"Accuracy for grayscale: \",score)\nscoretime = time.time() - start_time\nprint(\"Time consumed to score model: \",time.strftime(\"%H:%M:%S\", time.gmtime(scoretime)))\ncase4=[score,fittime,scoretime]","0f45da51":"head =[\"Accuracy\",\"FittingTime\",\"ScoringTime\"]\nprint(\"\\t\\t case1 \\t\\t\\t case2 \\t\\t\\t case3 \\t\\t\\t case4\")\nfor h, c1, c2, c3, c4 in zip(head, case1, case2, case3, case4):\n    print(\"{}\\t{}\\t{}\\t{}\\t{}\".format(h, c1, c2, c3, c4))","876ac1e5":"from tqdm import tqdm\n\nfit_time=[]\nscore=[]\nscore_time=[]\nfor j in tqdm(range(1000,31000,5000)):\n    start_time = time.time()\n    clf.fit(train_pca_b[:j], train_labels[:j].values.ravel())\n    fit_time.append(time.time() - start_time)\n    start_time = time.time()\n    score.append(clf.score(test_pca_b,test_labels))\n    score_time.append(time.time() - start_time)","f1889cdc":"x=list(range(1000,31000,5000))\nplt.figure(figsize=[20,5]);\nax1=plt.subplot(1, 2,1)\nax1.plot(x,score,'-o');\nplt.xlabel('Number of Training Samples')\nplt.ylabel('Accuray')\nax2=plt.subplot(1, 2,2)\nax2.plot(x,score_time,'-o');\nax2.plot(x,fit_time,'-o');\nplt.xlabel('Number of Training Samples')\nplt.ylabel('Time to Compute Score\/Fit (sec)')\nplt.legend(['score_time','fitting_time'])","75045031":"clf.get_params","51cd98ef":"parameters = {'gamma': [1, 0.1, 0.01, 0.001],\n             'C': [1000, 100, 10, 1]} \n\np = GridSearchCV(clf , param_grid=parameters, cv=3)","9d33516c":"X=train_pca_b[:i]\ny=train_labels[:i].values.ravel()\nstart_time = time.time()\np.fit(X,y)\nelapsed_time = time.time() - start_time\nprint(\"Time consumed to fit model: \",time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))","2cc1ffa0":"print(\"Scores for all Parameter Combination: \\n\",p.cv_results_['mean_test_score'])\nprint(\"\\nOptimal C and Gamma Combination: \",p.best_params_)\nprint(\"\\nMaximum Accuracy acheieved on LeftOut Data: \",p.best_score_)","94e547fa":"C=p.best_params_['C']\ngamma=p.best_params_['gamma']\nclf=svm.SVC(C=C,gamma=gamma, random_state=42)","f8ec72ba":"start_time = time.time()\nclf.fit(train_pca_b[:i], train_labels[:i].values.ravel())\nelapsed_time = time.time() - start_time\nprint(\"Time consumed to fit model: \",time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\nprint(\"Accuracy for binary: \",clf.score(test_pca_b,test_labels))","f304ac17":"start_time = time.time()\nclf.fit(train_pca_b, train_labels.values.ravel())\nelapsed_time = time.time() - start_time\nprint(\"Time consumed to fit model: \",time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))\nprint(\"Accuracy for binary: \",clf.score(test_pca_b,test_labels))","6cdda42d":"train_data = data #read and label removed in initial steps\ntrain_label=label\ntest_data = pd.read_csv('..\/input\/test.csv')","90c4b699":"train_data[train_data>0]=1\ntest_data[test_data>0]=1","9d6203da":"#standardize data\nsc = StandardScaler().fit(train_data)\nX_std_train = sc.transform(train_data)\nX_std_test = sc.transform(test_data)\n\n#If n_components is not set then all components are stored \nsklearn_pca = sklearnPCA().fit(X_std_train)\n\n#Percentage of variance explained by each of the selected components.\n#If n_components is not set then all components are stored and the sum of the ratios is equal to 1.0.\nvar_per = sklearn_pca.explained_variance_ratio_\ncum_var_per = sklearn_pca.explained_variance_ratio_.cumsum()\n\nplt.figure(figsize=(30,10))\nind = np.arange(len(var_per)) \nplt.bar(ind,var_per)\nplt.xlabel('n_components')\nplt.ylabel('Variance')","758a7003":"n_comp=len(cum_var_per[cum_var_per <= 0.90])\nprint(\"Keeping 90% Info with \",n_comp,\" components\")\nsklearn_pca = sklearnPCA(n_components=n_comp)\ntrain_pca_b = sklearn_pca.fit_transform(X_std_train)\ntest_pca_b = sklearn_pca.transform(X_std_test)\nprint(\"Shape before PCA for Train: \",X_std_train.shape)\nprint(\"Shape after PCA for Train: \",train_pca_b.shape)\nprint(\"Shape before PCA for Test: \",X_std_test.shape)\nprint(\"Shape after PCA for Test: \",test_pca_b.shape)","a119110a":"start_time = time.time()\nclf.fit(train_pca_b, label.values.ravel())\nfittime = time.time() - start_time\nprint(\"Time consumed to fit model: \",time.strftime(\"%H:%M:%S\", time.gmtime(fittime)))","9fdd98f2":"start_time = time.time()\nresult=clf.predict(test_pca_b)\nprint(\"Accuracy for Binary(PCA): \",result)\nelapsed_time = time.time() - start_time\nprint(\"Time consumed to predict: \",time.strftime(\"%H:%M:%S\", time.gmtime(elapsed_time)))","98af19ad":"data_to_submit = pd.DataFrame({\n    'ImageId':test_data.index.values+1,\n    'Label':result\n})\ndata_to_submit.index=data_to_submit['ImageId'].values\n\ndata_to_submit.to_csv('result.csv', index = False)\n","c8cab4e9":"## <a id=\"svm\"> 2. Support Vector Machine (SVM)<a>\n\n> *<br>Support Vector Machine (SVM) is a supervised machine learning algorithm which can be used for both classification or regression problems.\n> It performs classification by finding the hyperplane that maximizes the margin between the two classes.*\n\nTo understand better, lets look at the following cases:<br>\n\n**CASE 1**: <br> \nHere, we have three hyper-planes (A, B and C). Now, identify the right hyper-plane to classify star and circle<br>\nAs per SVM: \u201cSelect the hyper-plane which segregates the two classes better\u201d.<br>\nHere better means which keeps almost equal margin\/distance from both classes. <br>\n**Answer : B**\n\n![scenario1](https:\/\/github.com\/SabaSiddiqi\/Backup\/blob\/master\/svm_scenario1.png?raw=true)\n\n\n**CASE 2: **<br>\nHere, we have three hyper-planes (A, B and C). To identify the right hyper-plane to classify star and circle,\nMaximizing the distances between nearest data point (either class) and hyper-plane will\u00a0help us to decide the right hyper-plane. \nThis distance is called as\u00a0Margin\n**Answer : C**\n![margin](https:\/\/github.com\/SabaSiddiqi\/Backup\/blob\/master\/margin.png?raw=true)\n\n","ae97a040":"Fitting train data and finding a score for test data to check model performance","3e8664bb":"## <a id=\"clf_svm\">3. Classification using SVM<a>","e2cb2b2c":"Saving result to Output File for submission","0672edcb":"# Classification (MultiClass) using SVM & Optimal Parameter Selection","d7176ded":"Using all data from train and test files for Submission.\nFollowing Case4 for submission, which involves conversion of images to binary followed by dimensionality reduction.","b2033828":"It can be seen that for the chosen training samples, accuracy of Case 2 (~90%) is way higher than Case 1 (10%).","4839c171":"### <a id=\"case2\">Case 2 - Binary Images<\/a>\n<br>\nTo simply the problem, converting images to black and white from gray scale by replacing all values > 0 to 1. \n<br>And Converting 1D array to 2D 28x28 array using [**reshape**](https:\/\/docs.scipy.org\/doc\/numpy-1.13.0\/reference\/generated\/numpy.reshape.html) , to plot and view **binary** images. ","988ea68e":"For Case 4 (Binary Images and PCA Reduction):","190ed875":"## <a id=\"param\"> 5. Parameter Selection for SVM using GridSearchCV<\/a>","961847f3":"### <a id=\"case4\">Case 4 - Binary + Dimensionality Reduction - PCA<\/a> <br> \n<br>\nUsing the steps steps we used in Case 3.","018e7a90":"Importing required libraries","942e82c7":"### <a id=\"case1\">Case 1 - Gray Scale Images<\/a>\n","a8dd5772":"### Splitting data into Train and Test Data and Labels\n<br>\n\nDividing Data randomly into **train** and **test**. Out of total **train.csv** Data, 80% is kept as train for training the model and 20% is kept as test to score the model. ","eeba5eba":"However, the high dimensionaly of data is making computational time high. Lets reduce the dimensions using PCA (Principal Component Analysis)\n\n### <a id=\"case3\">Case 3 - GrayScale + Dimensionality Reduction - PCA<\/a> <br>\n\n It is a linear transformation technique used to identify strong patterns in data by finding out variable correlation. It maps the data to a lower dimensional subspace in a way that data variance is maximized while retaining most of the information.\n \n To understand how PCA works, this tutorial may help - [Principal Component Analysis Explained](https:\/\/www.kaggle.com\/sabasiddiqi\/principal-component-analysis-explained)\n\nWe are using [sklearnPCA](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.decomposition.PCA.html) library here to perform PCA Dimensionality Reduction.\n\nHere, Data is standardized and PCA is performed on data with all the components. Then variance is plotted for all components to decide which components to remove. ","c3c10e9f":"Converting Images to binary","fdb017d6":"I'm writng these notebooks as a part of my learning and to give back to community, would love to hear if I have missed something, or if you have any suggestions. Thanks. ","341d1a33":"Having a look at pixel values frequency (0 to 255)<br>\nTo get a better idea, lets convert the plot to Log Scale.<br>\nBased on leading 0s and 255s we can try converting it to binary in the later steps to simplify the problem.","844978bc":"Out of parameters below, we will be playing with **Gamma** and **C**, where<br> \n    <br>[Gamma](https:\/\/www.quora.com\/What-are-C-and-gamma-with-regards-to-a-support-vector-machine) is the parameter of a Gaussian Kernel (to handle non-linear classification)<br>\n    and **C** the parameter for the soft margin cost function, also known as cost of misclassification. A large C gives you low bias and high variance and vice versa.","1d384be5":" ## <a id=\"sizevsacc\">4. Training Data Size Vs Accuracy , Fitting & Score Times <\/a>\n\n\n<br>Understanding how training data size affects accuracy,","e31d8d4f":"We are not passing parameters in this step to keep it simple and will be using the default ones.","1c5e6eca":"It can be seen that for the chosen training samples, accuracy of Case1 after PCA is (~91%) which was previously (10%).","2f307cf7":"  ## <a id=\"explore\">1. Exploring and Preparing Data<\/a>\n<br>\nReading data from \"train.csv\", which will later be divided into train(to train model) and test(to check accuracy).  ","07b6c1b7":"It can be seen that for the chosen training samples, accuracy of Case 2 has increased from (~91%) to (94%) for the optimal parameters.<br>\nNow using all training samples","3c3dfd94":"## <a id=\"sub\"> 6. Submission<\/a>","a5130f5d":"Lets find the score using reduced dimensions keeping the same amount of samples, to compare accuracy.","c65d9080":"Predicting Test Data","6b0ba6ce":"Converting 1D array to 2D 28x28 array using [**reshape**](https:\/\/docs.scipy.org\/doc\/numpy-1.13.0\/reference\/generated\/numpy.reshape.html) , to plot and view grayscale images. ","d85402f8":"Using [SVM Classifier](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.svm.SVC.html) from sklearn library.<br>\nWe have 33600 training samples, fitting them is going to take alot of time. To keep it simple for now, lets select 5000 out of them.<br>\nYou can change the value i to use the desired number of samples.","194a3730":"To find optimal combination of parameters to achieve maximum accuracy ,using **GridSearchCV** from **sklearn** library. [GridSearchCV](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.GridSearchCV.html) does exhaustive search over specified parameter values for an estimator. <br>\nStoring values of parameters to be passed to GridSearch in **parameters**, keeping cross-validation folds as **3** and passing SVM as estimator. ","5ef7a2f7":"Fitting Training Data","eaf14eef":"To verify, lets pass the optimal parameters to Classifier and check the score.","b6da3ddd":"**Observations:**\n*  By simplifying the problem in Case 2 (by converting images to binary), accuracy increases from ~10% to ~91% for the chosen number of samples.\n* By reducing dimensions in Case3 and Case4, Fitting Times Reduces Drastically from ~43sec to ~6sec for the chosen number of samples.","87ba6e5f":"### <a id=\"cases\">Comparison of 4 Cases<\/a>","65b9e1d3":"Keeping 90% of information by choosing components falling within 0.90 cumulative.","4b59896f":"This notebook is intended for beginners to provide them a guideline for where to start using Simple Classifier  **SVM**. \n\n**Level :** Beginner <br>\n\n**Task :** To classify digits from 0 to 9 using [MNIST Handdrawn Digits Image Datatset](https:\/\/www.kaggle.com\/c\/digit-recognizer).\n\n**About Data: **\nData contains images of hand-drawn digits, from zero through nine. \n\nImage Details : Gray Scale , 28x28 pixel ,pixel value - 0(lightest) to 255(darkest)\n\n### Notebook Content \n1. [Exploring and Preparing Data](#explore)\n2. [Support Vector Machine (SVM)](#svm)\n3. [Classifcation Using SVM](#clf_svm) \n *    [ Case1 - GrayScale Images](#case1)\n *    [Case2 - Binary Images](#case2)\n *    [Case3 - GrayScale + Reduced Dimensions(using PCA)](#case3)\n *    [Case4 - Binary + Reduced Dimensions(using PCA)](#case4)\n *    [Comparison of Four Cases](#cases) \n4.  [Training Data Size Vs Accuracy, Score & Fitting Time](#compare)\n5. [Parameter Selection for SVM Using GridSearchCV](#param)\n6. [Submission](#sub)\n","500862ff":"Extracting label from data","63cb71d3":"Reducing Dimensions using PCA"}}