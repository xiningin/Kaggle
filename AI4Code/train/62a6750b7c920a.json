{"cell_type":{"2b50f1a3":"code","e13b4479":"code","da6b14cd":"code","56203afc":"code","121d3e4b":"code","6165c032":"code","86fc1edb":"code","63deb93d":"code","dd5d9bdc":"code","36d21203":"code","f79ef1de":"code","adaa5620":"code","9f0a2b01":"code","5e3f6be5":"code","5e2190f1":"code","1434c0ca":"code","2586bf7b":"code","fb1fe166":"code","deeecd49":"code","15020d16":"code","33509501":"code","8adfa122":"code","a76ee823":"code","081c0b46":"code","08d81b98":"code","bbb4a818":"code","610fa310":"code","3f64f627":"code","3a996935":"code","2417d7b3":"code","a770f746":"code","99ff7915":"code","910e12b0":"code","5013a633":"code","112334dc":"code","e7b61c7b":"code","3176e7f4":"code","331f1b2d":"code","381c58dc":"code","897fa28a":"code","c602a3e2":"code","9848c810":"code","a7fdeb23":"code","66ecba0b":"code","b9fae76c":"code","bb74e134":"code","eed08bf3":"code","6db6ec2d":"code","30573c43":"code","344b48ca":"code","26f32b09":"code","269a12b7":"code","95e8e334":"code","8004f510":"code","72c6dc03":"code","34ed246a":"code","75dd4f0d":"code","37d87775":"code","b3771b93":"code","31241dc3":"code","52f68741":"markdown","09f22e18":"markdown","3597cc47":"markdown","9715057c":"markdown","8df812c7":"markdown","e315a075":"markdown","6c5ecaed":"markdown","2496b0e6":"markdown","4474ad10":"markdown","ad324e34":"markdown","62e2ab01":"markdown","dd6c887c":"markdown","71233b06":"markdown","1b6593aa":"markdown","07c8ef32":"markdown","5008aa6a":"markdown","7c149f05":"markdown","85aa7eb4":"markdown","b5166da8":"markdown","5d4cd3cb":"markdown","d2828550":"markdown","cb88296b":"markdown","a58f7b96":"markdown"},"source":{"2b50f1a3":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\nimport plotly.offline as px\n\nimport datetime\nimport re   # Regular expressions\n\nimport nltk   # natural language tool kit\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.decomposition import LatentDirichletAllocation\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n%matplotlib inline\nprint(\"Pandas version: \", pd.__version__)\n\n# To export plotly figures in an HTML version of the jupyter notebook\npx.init_notebook_mode()","e13b4479":"# Read the dataset for Banking account\ndf_banking = pd.read_csv('..\/input\/bankingtransactions\/banking.csv', header=None, usecols=[0, 1, 3, 4], names = ['Transaction Date', 'Quantity', 'Transaction description', 'Transaction Type'], parse_dates=['Transaction Date'])\ndf_banking = df_banking.set_index('Transaction Date')\nprint(f'Shape of dataframe: {df_banking.shape}')\ndf_banking = df_banking.sort_index()\ndf_banking.head()","da6b14cd":"# Read the dataset for credit card bank 1\ndf_creditcard_bank01 = pd.read_csv('..\/input\/bankingtransactions\/bank01_creditcard.csv', header=None, names = ['Transaction Date', 'Transaction description', 'Quantity'],  parse_dates=['Transaction Date'])\ndf_creditcard_bank01 = df_creditcard_bank01.set_index('Transaction Date')\nprint(f'Shape of dataframe: {df_creditcard_bank01.shape}')\ndf_creditcard_bank01 = df_creditcard_bank01.sort_index()\ndf_creditcard_bank01.head()","56203afc":"# Read the dataset for credit card bank 2 and join them (In this case the transactions available are only last 25 months and you can download 12 months at a time)\ndf_creditcard_bank02a = pd.read_csv('..\/input\/bankingtransactions\/bank02_creditcard_01.csv', usecols=[0, 3, 4, 5, 6], parse_dates=['Transaction Date'])\ndf_creditcard_bank02b = pd.read_csv('..\/input\/bankingtransactions\/bank02_creditcard_02.csv', usecols=[0, 3, 4, 5, 6], parse_dates=['Transaction Date'])\ndf_creditcard_bank02 = pd.concat([df_creditcard_bank02a, df_creditcard_bank02b ])\ndf_creditcard_bank02 = df_creditcard_bank02.set_index('Transaction Date')\n\n# Debit column should be negative\ndf_creditcard_bank02['Debit'] = -df_creditcard_bank02['Debit']\n# Null values should be zero in Debit and credit columns. A new column 'Quantity' is created from Debit and Credit\ndf_creditcard_bank02.loc[df_creditcard_bank02['Debit'].isnull(), 'Debit'] = 0\ndf_creditcard_bank02.loc[df_creditcard_bank02['Credit'].isnull(), 'Credit'] = 0\ndf_creditcard_bank02['Quantity'] = df_creditcard_bank02['Debit'] + df_creditcard_bank02['Credit']\n\nprint(f'Shape of dataframe: {df_creditcard_bank02.shape}')\n \n# df_creditcard_bank02 = df_creditcard_bank02.rename(columns={'Debit': 'Quantity'})\ndf_creditcard_bank02 = df_creditcard_bank02.sort_index()\ndf_creditcard_bank02.head()","121d3e4b":"df_banking.info()","6165c032":"# Fill NULL transaction type with \"Transaction description\"\ndf_banking['Transaction Type'] = df_banking['Transaction Type'].fillna(df_banking['Transaction description'])","86fc1edb":"df_creditcard_bank01.info()","63deb93d":"df_creditcard_bank02.info()","dd5d9bdc":"df_banking['Year'] = df_banking.index.year\ndf_banking['Month'] = df_banking.index.month\ndf_banking['Day'] = df_banking.index.day\ndf_banking['Week'] = df_banking.index.isocalendar().week  \ndf_banking['Weekday'] = df_banking.index.weekday\n\ndf_creditcard_bank01['Year'] = df_creditcard_bank01.index.year\ndf_creditcard_bank01['Month'] = df_creditcard_bank01.index.month\ndf_creditcard_bank01['Day'] = df_creditcard_bank01.index.day\ndf_creditcard_bank01['Week'] = df_creditcard_bank01.index.isocalendar().week  \ndf_creditcard_bank01['Weekday'] = df_creditcard_bank01.index.weekday\n\ndf_creditcard_bank02['Year'] = df_creditcard_bank02.index.year\ndf_creditcard_bank02['Month'] = df_creditcard_bank02.index.month\ndf_creditcard_bank02['Day'] = df_creditcard_bank02.index.day\ndf_creditcard_bank02['Week'] = df_creditcard_bank02.index.isocalendar().week \ndf_creditcard_bank02['Weekday'] = df_creditcard_bank02.index.weekday","36d21203":"min_Banking = df_banking[\"Quantity\"].min()\nmin_Banking_date = df_banking[\"Quantity\"].idxmin() \nmax_Banking = df_banking[\"Quantity\"].max()\nmax_Banking_date = df_banking[\"Quantity\"].idxmax()\n\nprint(f\"The minimum value transaction for banking account was: {min_Banking} CAD on {min_Banking_date.date()}\")\nprint(f\"The maximum value transaction for banking account was: {max_Banking} CAD on {max_Banking_date.date()}\")\n","f79ef1de":"min_Credit_bank01 = df_creditcard_bank01[\"Quantity\"].min()\nmin_Credit_bank01_date = df_creditcard_bank01[\"Quantity\"].idxmin() \nmax_Credit_bank01 = df_creditcard_bank01[\"Quantity\"].max()\nmax_Credit_bank01_date = df_creditcard_bank01[\"Quantity\"].idxmax()\n\nprint(f\"The minimum value transaction for banking account was: {min_Credit_bank01} CAD on {min_Credit_bank01_date.date()}\")\nprint(f\"The maximum value transaction for banking account was: {max_Credit_bank01} CAD on {max_Credit_bank01_date.date()}\")\n","adaa5620":"min_Credit_bank02 = df_creditcard_bank02[\"Quantity\"].min()\nmin_Credit_bank02_date = df_creditcard_bank02[\"Quantity\"].idxmin() \nmax_Credit_bank02 = df_creditcard_bank02[\"Quantity\"].max()\nmax_Credit_bank02_date = df_creditcard_bank02[\"Quantity\"].idxmax()\n\nprint(f\"The minimum value transaction for banking account was: {min_Credit_bank02} CAD on {min_Credit_bank02_date.date()}\")\nprint(f\"The maximum value transaction for banking account was: {max_Credit_bank02} CAD on {max_Credit_bank02_date.date()}\")","9f0a2b01":"print(f\"First available date of banking data is {df_banking.index[0].date()}\")\nprint(f\"First available date of credit card bank 1 data is {df_creditcard_bank01.index[0].date()}\")\nprint(f\"First available date of credit card bank 2 data is {df_creditcard_bank02.index[0].date()}\")","5e3f6be5":"fig = make_subplots(rows=1, cols=3, subplot_titles=(\"Banking\", \"Credit Card 1\", \"Credit Card 2\"))\n\nfig.add_trace(\n    go.Histogram(x=df_banking[\"Quantity\"].values, name=\"Banking\"), \n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=df_creditcard_bank01[\"Quantity\"].values, name=\"Credit Card 1\"),\n    row=1, col=2\n)\n\nfig.add_trace(\n    go.Histogram(x=df_creditcard_bank02[\"Quantity\"].values, name=\"Credit Card 2\"),\n    row=1, col=3\n)\n\nfig.update_layout(height=400, width=800, title_text=\"Histograms for different financial products\")\nfig.update_xaxes(title_text=\"Quantity\", row=1, col=1)\nfig.update_xaxes(title_text=\"Quantity\", row=1, col=2)\nfig.update_xaxes(title_text=\"Quantity\", row=1, col=3)\nfig.update_yaxes(title_text=\"Count\", row=1, col=1)\n\nfig.update_traces(marker_line_width=1.5, opacity=0.6)\n\nfig.show()","5e2190f1":"# Create figure and plot space\nfig, axes = plt.subplots(3, 1, figsize=(12, 15))\nfig.subplots_adjust(hspace=0.5)\n\n# For banking\naxes[0].bar(df_banking.index.values, df_banking['Quantity'])\naxes[1].bar(df_creditcard_bank01.index.values, df_creditcard_bank01['Quantity'])\naxes[2].bar(df_creditcard_bank02.index.values, df_creditcard_bank02['Quantity']) \n\n# Set title and labels for axes\naxes[0].set(xlabel=\"Date\", ylabel=\"Quantity (CAD)\", title=\"Banking transactions\")\naxes[1].set(xlabel=\"Date\", ylabel=\"Quantity (CAD)\", title=\"Credit card transactions from bank 1\")\naxes[2].set(xlabel=\"Date\", ylabel=\"Quantity (CAD)\", title=\"Credit card transactions from bank 2\")\n\n","1434c0ca":"fig = make_subplots(rows=3, cols=1, subplot_titles=(\"Banking\", \"Credit Card 1\", \"Credit Card 2\"))\n\nfig.add_trace(\n    go.Histogram(x=df_banking[\"Month\"].values, name=\"Banking\"), \n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=df_creditcard_bank01[\"Month\"].values, name=\"Credit Card 1\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=df_creditcard_bank02[\"Month\"].values, name=\"Credit Card 2\"),\n    row=3, col=1\n)\n\nfig.update_layout(height=800, width=800, title_text=\"Plot for different financial products in time\")\nfig.update_xaxes(title_text=\"Month\", row=3, col=1)\nfig.update_yaxes(title_text=\"Count\", row=1, col=1)\nfig.update_yaxes(title_text=\"Count\", row=2, col=1)\nfig.update_yaxes(title_text=\"Count\", row=3, col=1)\n\nfig.update_traces(marker_line_width=1.5, opacity=0.6)\n\nfig.show()","2586bf7b":"fig = make_subplots(rows=3, cols=1, subplot_titles=(\"Banking\", \"Credit Card 1\", \"Credit Card 2\"))\n\nfig.add_trace(\n    go.Histogram(x=df_banking[\"Day\"].values, name=\"Banking\"), \n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=df_creditcard_bank01[\"Day\"].values, name=\"Credit Card 1\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=df_creditcard_bank02[\"Day\"].values, name=\"Credit Card 2\"),\n    row=3, col=1\n)\n\nfig.update_layout(height=800, width=800, title_text=\"Plot for different financial products in time\")\nfig.update_xaxes(title_text=\"Day\", row=3, col=1)\nfig.update_yaxes(title_text=\"Count\", row=1, col=1)\nfig.update_yaxes(title_text=\"Count\", row=2, col=1)\nfig.update_yaxes(title_text=\"Count\", row=3, col=1)\n\nfig.update_traces(marker_line_width=1.5, opacity=0.6)\nfig.show()","fb1fe166":"fig = make_subplots(rows=3, cols=1, subplot_titles=(\"Banking\", \"Credit Card 1\", \"Credit Card 2\"))\n\nfig.add_trace(\n    go.Histogram(x=df_banking[\"Week\"].values, name=\"Banking\"), \n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=df_creditcard_bank01[\"Week\"].values, name=\"Credit Card 1\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=df_creditcard_bank02[\"Week\"].values, name=\"Credit Card 2\"),\n    row=3, col=1\n)\n\nfig.update_layout(height=800, width=800, title_text=\"Plot for different financial products in time\")\nfig.update_xaxes(title_text=\"Week\", row=3, col=1)\nfig.update_yaxes(title_text=\"Count\", row=1, col=1)\nfig.update_yaxes(title_text=\"Count\", row=2, col=1)\nfig.update_yaxes(title_text=\"Count\", row=3, col=1)\n\nfig.update_traces(marker_line_width=1.5, opacity=0.6)\nfig.show()","deeecd49":"fig = make_subplots(rows=3, cols=1, subplot_titles=(\"Banking\", \"Credit Card 1\", \"Credit Card 2\"))\n\nfig.add_trace(\n    go.Histogram(x=df_banking[\"Weekday\"].values, name=\"Banking\"), \n    row=1, col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=df_creditcard_bank01[\"Weekday\"].values, name=\"Credit Card 1\"),\n    row=2, col=1\n)\n\nfig.add_trace(\n    go.Histogram(x=df_creditcard_bank02[\"Weekday\"].values, name=\"Credit Card 2\"),\n    row=3, col=1\n)\n\nfig.update_layout(height=800, width=800, title_text=\"Plot for different financial products in time\")\nfig.update_xaxes(title_text=\"Weekday\", row=3, col=1)\nfig.update_yaxes(title_text=\"Count\", row=1, col=1)\nfig.update_yaxes(title_text=\"Count\", row=2, col=1)\nfig.update_yaxes(title_text=\"Count\", row=3, col=1)\n\nfig.update_traces(marker_line_width=1.5, opacity=0.6)\nfig.show()","15020d16":"df_banking.head()","33509501":"# Transactions in 2019\ndf_2019_credit02_IN = df_creditcard_bank01[ (df_creditcard_bank01['Year'] == 2019) & (df_creditcard_bank01['Quantity'] > 0 )]\ndf_2019_credit02_OUT = df_creditcard_bank01[ (df_creditcard_bank01['Year'] == 2019) & (df_creditcard_bank01['Quantity'] < 0 )]\n\ndf_2019_credit02_IN","8adfa122":"data = [\n    [2019, df_banking[ (df_banking['Year'] == 2019) & (df_banking['Quantity'] > 0 )]['Quantity'].sum(),  -df_banking[ (df_banking['Year'] == 2019) & (df_banking['Quantity'] < 0 )]['Quantity'].sum(), 'Banking'],\n    [2019, df_creditcard_bank01[ (df_creditcard_bank01['Year'] == 2019) & (df_creditcard_bank01['Quantity'] > 0 )]['Quantity'].sum(),  -df_creditcard_bank01[ (df_creditcard_bank01['Year'] == 2019) & (df_creditcard_bank01['Quantity'] < 0 )]['Quantity'].sum(), 'Credit Card Bank 1'],\n    [2019, df_creditcard_bank02[ (df_creditcard_bank02['Year'] == 2019) & (df_creditcard_bank02['Quantity'] > 0 )]['Quantity'].sum(),  -df_creditcard_bank02[ (df_creditcard_bank02['Year'] == 2019) & (df_creditcard_bank02['Quantity'] < 0 )]['Quantity'].sum(), 'Credit Card Bank 2'],\n    [2020, df_banking[ (df_banking['Year'] == 2020) & (df_banking['Quantity'] > 0 )]['Quantity'].sum(),  -df_banking[ (df_banking['Year'] == 2020) & (df_banking['Quantity'] < 0 )]['Quantity'].sum(), 'Banking'],\n    [2020, df_creditcard_bank01[ (df_creditcard_bank01['Year'] == 2020) & (df_creditcard_bank01['Quantity'] > 0 )]['Quantity'].sum(),  -df_creditcard_bank01[ (df_creditcard_bank01['Year'] == 2020) & (df_creditcard_bank01['Quantity'] < 0 )]['Quantity'].sum(), 'Credit Card Bank 1'],\n    [2020, df_creditcard_bank02[ (df_creditcard_bank02['Year'] == 2020) & (df_creditcard_bank02['Quantity'] > 0 )]['Quantity'].sum(),  -df_creditcard_bank02[ (df_creditcard_bank02['Year'] == 2020) & (df_creditcard_bank02['Quantity'] < 0 )]['Quantity'].sum(), 'Credit Card Bank 2']    \n ]\n\ndf_transactionsSummary = pd.DataFrame(data = data, columns=['Year', 'IN', 'OUT', 'Origin'])\ndf_transactionsSummary['Difference'] = df_transactionsSummary['IN'] - df_transactionsSummary['OUT']\ndf_transactionsSummary","a76ee823":"change_bank = df_transactionsSummary[df_transactionsSummary['Origin'] == \"Banking\"]['IN'].reset_index(drop=True).values\nchange_CC1 = df_transactionsSummary[df_transactionsSummary['Origin'] == \"Credit Card Bank 1\"]['OUT'].reset_index(drop=True).values\nchange_CC2 = df_transactionsSummary[df_transactionsSummary['Origin'] == \"Credit Card Bank 2\"]['OUT'].reset_index(drop=True).values\n\nprint('Debt (IN):', \"{:.2f}\".format(100*(change_bank[1] - change_bank[0])\/change_bank[0]) + '%' )\nprint('Credit Card 1 (OUT):', \"{:.2f}\".format(100*(change_CC1[1] - change_CC1[0])\/change_CC1[0]) + '%')\nprint('Credit Card 2 (OUT):', \"{:.2f}\".format(100*(change_CC2[1] - change_CC2[0])\/change_CC2[0]) + '%')\n","081c0b46":"df_transactionsSummary_banking = df_transactionsSummary[df_transactionsSummary['Origin'] == 'Banking']\nimport plotly.express as px\nfig = px.bar(df_transactionsSummary_banking, x=\"Year\", y=[\"IN\", \"OUT\"],\n             labels={                     \n                     \"value\": \"Quantity (CAD)\",\n                     \"variable\": \"Type\"\n                 },\n              barmode='group',\n             height=400, width=600)\n\nfig.update_traces(marker_line_width=1.5, opacity=0.6)\nfig.update_layout(title_text='Quantity of Debit Transactions - 2019 vs 2020')\n\nfig.show()","08d81b98":"df_transactionsSummary_CC1 = df_transactionsSummary[df_transactionsSummary['Origin'] == 'Credit Card Bank 1']\nimport plotly.express as px\nfig = px.bar(df_transactionsSummary_CC1, x=\"Year\", y=[\"IN\", \"OUT\"],\n              labels={                     \n                     \"value\": \"Quantity (CAD)\",\n                     \"variable\": \"Type\"\n                 },\n              barmode='group',\n             height=400, width=600)\n\nfig.update_traces(marker_line_width=1.5, opacity=0.6)\nfig.update_layout(title_text='Quantity of Credit Card Transactions (Bank 1) - 2019 vs 2020')\n\nfig.show()","bbb4a818":"df_transactionsSummary_CC2 = df_transactionsSummary[df_transactionsSummary['Origin'] == 'Credit Card Bank 2']\n\nfig = px.bar(df_transactionsSummary_CC2, x=\"Year\", y=[\"IN\", \"OUT\"],\n              labels={                     \n                     \"value\": \"Quantity (CAD)\",\n                     \"variable\": \"Type\"\n                 },\n              barmode='group',\n             height=400, width=600)\n\nfig.update_traces(marker_line_width=1.5, opacity=0.6)\nfig.update_layout(title_text='Quantity of Credit Card Transactions (Bank 2) - 2019 vs 2020')\n\nfig.show()","610fa310":"date01 = datetime.date(2018, 12, 1)\ndate02 = datetime.date(2020, 9, 30)\ndf_banking = df_banking[(df_banking.index.date >= date01) & (df_banking.index.date <= date02)]\ndf_creditcard_bank01 = df_creditcard_bank01[(df_creditcard_bank01.index.date >= date01) & (df_creditcard_bank01.index.date <= date02)]\ndf_creditcard_bank02 = df_creditcard_bank02[(df_creditcard_bank02.index.date >= date01) & (df_creditcard_bank02.index.date <= date02)]\n","3f64f627":"df_banking['Transaction description'].unique()[:5]","3a996935":"df_banking['Transaction Type'].unique()[:5]","2417d7b3":"df_creditcard_bank01['Transaction description'].unique()[:5]","a770f746":"df_creditcard_bank02['Description'].unique()[:5]","99ff7915":"# Category field for second credit card wont be used\ncategories_CC2 = df_creditcard_bank02['Category'].unique()\nprint(categories_CC2.shape)\ncategories_CC2","910e12b0":"df_banking[['Transaction description', 'Transaction Type', 'Year', 'Quantity']].groupby(['Transaction description', 'Transaction Type', 'Year']).sum().head()","5013a633":"df_creditcard_bank01[['Transaction description', 'Year', 'Quantity']].groupby(['Transaction description', 'Year']).sum().head()","112334dc":"df_creditcard_bank02[['Description', 'Year', 'Quantity']].groupby(['Description', 'Year']).sum().head()","e7b61c7b":"df_banking['Original DF'] = 'Banking'\ndf_creditcard_bank01['Original DF'] = 'Credit Card - Bank 1'\ndf_creditcard_bank02['Original DF'] = 'Credit Card - Bank 2'","3176e7f4":"df_transactions_01a = df_banking.copy()\ndf_transactions_01a = df_transactions_01a.drop(columns=['Transaction Type'])\ndf_transactions_01a = df_transactions_01a.reset_index()\n\ndf_transactions_01b = df_creditcard_bank01.copy()\ndf_transactions_01b = df_transactions_01b.reset_index()\n\ndf_transactions_01c = df_creditcard_bank02.copy()\ndf_transactions_01c = df_transactions_01c.drop(columns=['Category', 'Debit', 'Credit'])\ndf_transactions_01c = df_transactions_01c.rename(columns={'Description': 'Transaction description'})\ndf_transactions_01c = df_transactions_01c.reset_index()\n\ndf_transactions_01b","331f1b2d":"frames = [df_transactions_01a, df_transactions_01b, df_transactions_01c]\ndf_transactions = pd.concat(frames)\ndf_transactions = df_transactions.reset_index().drop(columns='index').sort_values(['Transaction Date']).reset_index().drop(columns='index')","381c58dc":"df_transactions.head()","897fa28a":"df_transactions[df_transactions['Transaction description'].str.contains('DOLLARAMA')].head()","c602a3e2":"# lowercase\ndf_transactions['Transaction description'] = df_transactions['Transaction description'].str.lower()\n\n# Erasing special characters\ndf_transactions['Transaction description'] = df_transactions['Transaction description'].str.replace('*', ' ')\ndf_transactions['Transaction description'] = df_transactions['Transaction description'].str.replace('.', ' ')\ndf_transactions['Transaction description'] = df_transactions['Transaction description'].apply(lambda x: re.sub(r'[^a-zA-Z\\s\\']+', ' ', x))","9848c810":"df_transactions[df_transactions['Transaction description'].str.contains('dollarama')].head()","a7fdeb23":"# Removing stop words\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop_words = set(stopwords.words('english'))\n\ndf_transactions['Transaction description'] = df_transactions['Transaction description'].apply(lambda x: [word for word in x.split() if word not in stop_words])\ndf_transactions['Transaction description'] = [' '.join(map(str, l)) for l in df_transactions['Transaction description']]\n\ndf_transactions.head()","66ecba0b":"df_transactions.shape","b9fae76c":"# One way to count words\nword_count = df_transactions['Transaction description'].str.split(expand=True).stack().value_counts()\nword_count = pd.DataFrame(data=word_count, columns=['Count'])\nword_count = word_count.sort_values(['Count'], ascending=False)\nprint('- The 10 most used words are:')\n\nmost_used = word_count[:10]\n\nsns.barplot(x=most_used['Count'], y=most_used.index)\n","bb74e134":"# The transaction description field \ndocuments = df_transactions['Transaction description'].values\nno_features = 1500\n\n# LDA can only use raw term counts for LDA because it is a probabilistic graphical model\ntf_vectorizer = CountVectorizer(ngram_range=(1, 3), max_df=0.95, min_df=2, max_features=no_features) # Using count vectorized with 1 to 3 ngrams to count the words\ntf = tf_vectorizer.fit_transform(documents)\ntf_feature_names = tf_vectorizer.get_feature_names()","eed08bf3":"topics = [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20]\nperplexityResult = []\nscoreResult = []\n\nfor no_topics in topics:\n    # Run LDA\n    lda = LatentDirichletAllocation(n_components=no_topics, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n    perplexityResult.append(lda.perplexity(tf))\n    scoreResult.append(lda.score(tf))\n    \n","6db6ec2d":"fig, axes = plt.subplots(1, 2, figsize=(10, 5))\nfig.subplots_adjust(hspace=0.5) \n\nsns.despine(left=True)\n\nsns.lineplot(topics, perplexityResult, ax=axes[0])\nsns.lineplot(topics, scoreResult, ax=axes[1])\n\naxes[0].set_title('Perplexity')\naxes[0].set_xlabel('Topics')\naxes[0].set_ylabel('Perplexity')\n\naxes[1].set_title('Score')\naxes[1].set_xlabel('Topics')\naxes[1].set_ylabel('Score')\n","30573c43":"# Define Search Param\nsearch_params = {'n_components': [10, 12, 15, 20], 'learning_decay': [.5, .7, .9]}\n# Init the Model\nlda = LatentDirichletAllocation(max_iter=5, learning_method='online', learning_offset=50., random_state=0)\n# Init Grid Search Class\nmodel = GridSearchCV(lda, param_grid=search_params)\n# Do the Grid Search\nmodel.fit(tf)\n","344b48ca":"# Best Model\nbest_lda_model = model.best_estimator_\n# Model Parameters\nprint(\"Best Model's Params: \", model.best_params_)\n# Log Likelihood Score\nprint(\"Best Log Likelihood Score: \", model.best_score_)\n# Perplexity\nprint(\"Model Perplexity: \", best_lda_model.perplexity(tf)) \n","26f32b09":"def display_topics(model, feature_names, no_top_words):\n    for topic_idx, topic in enumerate(model.components_):\n        print(\"Topic %d:\" % (topic_idx))\n        print(\" - \".join([feature_names[i]\n                        for i in topic.argsort()[:-no_top_words - 1:-1]]))","269a12b7":"selectedTopics = 10\nlearning_decay = 0.5\n\nlda = LatentDirichletAllocation(n_components= selectedTopics, learning_decay = learning_decay, max_iter=5, learning_method='online', learning_offset=50.,random_state=0).fit(tf)\n\nno_top_words = 10\ndisplay_topics(lda, tf_feature_names, no_top_words)","95e8e334":"import pyLDAvis.sklearn\npanel = pyLDAvis.sklearn.prepare(lda, tf, tf_vectorizer, mds='tsne')\npyLDAvis.display(panel)","8004f510":"df_lda = pd.DataFrame(data=lda.transform(tf))\ndominant_topic = np.argmax(df_lda.values, axis=1)\ndf_lda['Dominant topic'] = dominant_topic\n\ndf_result = pd.concat([df_transactions, df_lda[['Dominant topic']]], axis=1, sort=False)\ndf_result.head()","72c6dc03":"fig, axes = plt.subplots(nrows=1, ncols=3, figsize=(20, 6))\n\ndataPlot_bank = df_result.copy()\ndataPlot_bank = dataPlot_bank[ (dataPlot_bank['Original DF'] == 'Banking') & (dataPlot_bank['Year'] != 2018)]\ndataPlot_bank = dataPlot_bank.drop(columns=['Month', 'Week', 'Weekday', 'Day'])\ndataPlot_bank = dataPlot_bank.groupby(['Year', 'Dominant topic']).sum()\ndataPlot_bank['Quantity'].unstack().plot(kind='barh', stacked=False, ax=axes[0])\n\ndataPlot_CC1 = df_result.copy()\ndataPlot_CC1 = dataPlot_CC1[ (dataPlot_CC1['Original DF'] == 'Credit Card - Bank 1') & (dataPlot_CC1['Year'] != 2018)]\ndataPlot_CC1 = dataPlot_CC1.drop(columns=['Month', 'Week', 'Weekday', 'Day'])\ndataPlot_CC1 = dataPlot_CC1.groupby(['Year', 'Dominant topic']).sum()\ndataPlot_CC1['Quantity'].unstack().plot(kind='barh', stacked=False, ax=axes[1])\n\ndataPlot_CC2 = df_result.copy()\ndataPlot_CC2 = dataPlot_CC2[ (dataPlot_CC2['Original DF'] == 'Credit Card - Bank 2') & (dataPlot_CC2['Year'] != 2018)]\ndataPlot_CC2 = dataPlot_CC2.drop(columns=['Month', 'Week', 'Weekday', 'Day'])\ndataPlot_CC2 = dataPlot_CC2.groupby(['Year', 'Dominant topic']).sum()\ndataPlot_CC2['Quantity'].unstack().plot(kind='barh', stacked=False, ax=axes[2] )\n\naxes[0].set_xlabel('Quantity (CAD)')\naxes[1].set_xlabel('Quantity (CAD)')\naxes[2].set_xlabel('Quantity (CAD)')\n\naxes[0].set_title('Banking')\naxes[1].set_title('Credit Card 1')\naxes[2].set_title('Credit Card 2')","34ed246a":"# Year 2019\ndataPlot02_2019 = df_result.copy()\ndataPlot02_2019 = dataPlot02_2019[ (dataPlot02_2019['Quantity'] < 0) & (dataPlot02_2019['Year'] == 2019)]\ndataPlot02_2019 = dataPlot02_2019.drop(columns=['Month', 'Week', 'Weekday', 'Day', 'Transaction description', 'Transaction Date', 'Original DF', 'Year'])\n\n# Set expenses as positive values\ndataPlot02_2019['Quantity'] = - dataPlot02_2019['Quantity']\ndataPlot02_2019 = dataPlot02_2019.groupby(['Dominant topic']).sum()\n\n\n\n\n# Year 2020\ndataPlot02_2020 = df_result.copy()\ndataPlot02_2020 = dataPlot02_2020[ (dataPlot02_2020['Quantity'] < 0) & (dataPlot02_2020['Year'] == 2020)]\ndataPlot02_2020 = dataPlot02_2020.drop(columns=['Month', 'Week', 'Weekday', 'Day', 'Transaction description', 'Transaction Date', 'Original DF', 'Year'])\n\n# Set expenses as positive values\ndataPlot02_2020['Quantity'] = - dataPlot02_2020['Quantity']\ndataPlot02_2020 = dataPlot02_2020.groupby(['Dominant topic']).sum()\n","75dd4f0d":"fig = make_subplots(rows=1, cols=2, specs=[[{'type':'domain'}, {'type':'domain'}]])\n\nlabels=dataPlot02_2019['Quantity'].index.values\n\nfig.add_trace(go.Pie(labels=labels, values=dataPlot02_2019['Quantity'].values, name=\"2019 Expenses\"),\n              1, 1)\nfig.add_trace(go.Pie(labels=labels, values=dataPlot02_2020['Quantity'].values, name=\"2020 Expenses\"),\n              1, 2)\n\nfig.update_traces(hole=.4, hoverinfo=\"label+percent+name\")\n\nfig.update_layout(\n    title_text=\"Expenses by Topic (2019 vs. 2020)\",\n    # Add annotations in the center of the donut pies.\n    annotations=[dict(text='2019', x=0.20, y=0.5, font_size=20, showarrow=False),\n                 dict(text='2020', x=0.80, y=0.5, font_size=20, showarrow=False)])","37d87775":"dataPlot02_2019['Quantity'].index.values","b3771b93":"categories_CC2","31241dc3":"coffee01 = df_result[df_result['Transaction description'].str.contains(\"tim's\")]['Quantity'].sum()\ncoffee02 = df_result[df_result['Transaction description'].str.contains(\"coffee\")]['Quantity'].sum()\ncoffee03 = df_result[df_result['Transaction description'].str.contains(\"starbucks\")]['Quantity'].sum()\ncoffee04 = df_result[df_result['Transaction description'].str.contains(\"juan\")]['Quantity'].sum()\n\nTotal_coffee = coffee01 + coffee02 + coffee03 + coffee04\nTotal_coffee","52f68741":"[GitHub repository](https:\/\/github.com\/smlopezza\/BankTransactions)\n\n\n# Topic Modelling of Banking Transactions using Latent Dirichlet Allocation (LDA)\n\n\n## Objective of the project\n\nThis is a personal project to get insights related to my personal expenses. In this notebook I analyze bank statements from my everyday banking account and my two credit cards using Topic Modelling with the idea to classify transactions. I also take a look on how my transactions have been affected by COVID 19.\n\nIn this notebook I am practicing my skills in:\n* __Pandas__ for working with files and datasets\n* __Seaborn__ and __Ploty__ to plot the different figures\n* __Scikit-Learn__ - I use Latent Dirichlet Allocation (__LDA__) for analysis of the transactions description field \n\n\n### Considerations\n- Data is available from Nov 2018. I will use data between Dec. 1st, 2018 and Sep. 30, 2020\n- Transactions are made in Canadian dollars (CAD)\n- If you would like to do a similar analysis, your personal bank has the option to export your transactions to a CSV file.\n\n### Funfact\nI spent 172.89 CAD in coffee since Dec. 1, 2018. I love coffee!\n\n### Graphical summary of results\n* The transactions from the 3 financial products were classified in 10 topics.\n![TopicResults.png](attachment:TopicResults.png)\n![PieChart.png](attachment:PieChart.png)\n\n\n## Contents\n1. <a href='#pre_processing'> Pre-processing  <\/a>\n2. <a href='#eda'> Exploratory Data Analysis (EDA) <\/a> \n3. <a href='#TopicModelling'> Topic Modelling <\/a>\n4. <a href='#Visualization'> Visualization <\/a>\n5. <a href='#Conclusions'> Conclusions <\/a>\n\n\n## Initial setup\n* Import the needed packages: pandas, numpy and seaborn, matplotlib and plotly","09f22e18":"<a id='eda_conclusions'><\/a>\n### Some conclusion from EDA\n- The months where I made more transactions were January and December. A peak in June\/July can also be observed. This is consistent with Summer and Christmas seasons\n\nComparing 2019 vs. 2020 (up to Sep. 30)\n- Quantity of IN debit transactions in 2020 had increased in 39.99%\n- Quantity of Output credit card transactions from bank 1 (limit 1000 CAD) had increased in 13.11%\n- Quantity of Output credit card transactions from bank 2 (limit 5000 CAD) had decreased in 48.28% ","3597cc47":"### Transactions in time","9715057c":"* Joining topics results to initial dataframe","8df812c7":"### Comparision 2019 vs 2020\n","e315a075":"<a id='Visualization'><\/a>\n## Visualization \n\n* This auxiliary function helps to check the top words in each topic as proposed by [Aneesha Bakharia\n](https:\/\/medium.com\/mlreview\/topic-modeling-with-scikit-learn-e80d33668730#.vivglhmhv)","6c5ecaed":"### Check if there are null values","2496b0e6":"<a id='eda'><\/a>\n## Exploratory Data Analysis (EDA)\n\nGetting to know the dataset is very important. As initial steps I looked for the maximum and minimum transactions of each financial product. Then I looked for the available dates and general distribution of the transactions. I also checked how the transaction are distributed in time, month and day of the week. Finally, I compared IN\/OUT transactions in 2019 period with the 2020 period as Sep. 30.\n\n### Maximum and minimum transactions\n\n","4474ad10":"### Distribution plots","ad324e34":"#### Joining the data\nI will join the three data sets. A new column corresponding to the original dataset will be created\n","62e2ab01":"### Transactions by week and weekday\nI am interested to see if the month, the day of the week, the day of month or the week of the year will make a difference","dd6c887c":"#### Latent Dirichlet allocation (LDA) \nA first approach varying the number of topics to see how the perplexity and score changes was done. Then a GridSearchCV was applied to select the best conditions for the model\n\n","71233b06":"### Minimum date available\n","1b6593aa":"* Visualization for banking transactions","07c8ef32":"<a id='pre_processing'><\/a>\n## Pre-processing\n\n* For the pre-processing of the data, the CSV files generated by the two different banks are read. Then, null values corresponding to \"Transaction Type\" are filled with value from \"Transaction description\". Finally, some features are added: year, month, day, week of year, weekday.\n\n### Read Dataset","5008aa6a":"* To visualize better the words for each of the Topics I am using [pyLDAvis](https:\/\/pyldavis.readthedocs.io\/en\/latest\/readme.html#:~:text=pyLDAvis%20is%20designed%20to%20help,an%20interactive%20web%2Dbased%20visualization). This is a Python library for interactive topic model visualization. The package extracts information from a fitted LDA topic model to inform an interactive web-based visualization. ","7c149f05":"#### Bag of Words\n\nFirst I am counting the words by splitting them to have an initial idea of the most used words. Finally, I use CountVectorizer with 1 to 3 ngrams to vectorize the 'Transaction description' field","85aa7eb4":"* The plot shows that the perplexity is lower when the number of topics increases while score increases with the number of topics. The optimum value is around 10 topics. Grid Search shows that the best model has 10 topics and a learning rate of 0.5 considering the provided parameters.","b5166da8":"### Text Analysis for Transaction description field\n#### Pre-processing\nWithin the transaction description, we can see items with numbers, for example for transactions related with Dollarama have associated a number corresponding to the store. However, for this analysis what I want to know is the category of the expense and not the place. In that sense, the 'Transaction description' is converted to lower case and the numbers and symbols are eliminated.\n\nIt is important to 'clean' the transaction description field as follows:\n* Making the transaction description lower case\n* Erasing the special characters\n* Remove stop words","5d4cd3cb":"### Addition of features for year, month , day","d2828550":"#### Quick view on description fields\nA quick look on the description field show us that we can use it to help the classification of transactions. It is interesting that Bank 2 provides a category field. For this study that column won't be used. ","cb88296b":"<a id='Conclusions'><\/a>\n## Conclusions\n\n* The main transactions for each topic can be summarized as follows below. A better classification is needed to be able to recognize the different categories as proposed by Bank 2, for example.\n    * __Topic 0:__ Income to my banking account \n    * __Topic 1:__ Related to bill payments from the banking account, and payments to credit card 2\n    * __Topic 2:__ Monthly subscriptions\n    * __Topic 3:__ Phone plan and Amazon purchases\n    * __Topic 4:__ Coffee + Groceries + Pets\n    * __Topic 5:__ Payments to credit card 1\n    * __Topic 6:__ miscellaneous payments (Purchases in convenience stores, Canadian tire and staples --> the word 'store' is common in these transactions), and transactions from eTransfer\n    * __Topic 7:__ POS purchases from credit card + groceries Foodland\n    * __Topic 8:__ Interest charges, dollarama, transactions using google pay, customer transfers out of banking account\n    * __Topic 9:__ Car loan, car insurance, transactions made in Colombia with credit card 1, costco and marshalls\n\n\n* Topics 0, 6 and 9 present the higher reduction from 2019 to 2020. This can be explained as my miscellaneous payments and purchases at Costco or Marshalls have decreased due to COVID 19.\n\n* Topics 4 and 8 have increased from 2019 to 2020. This can be explained as I have spend more money in groceries and now I do more transfers via eTransfer\n\n* I love coffee, so I was interested to know how much I have spent in coffee since Dec. 1, 2018 --> the answer is 172.89 CAD","a58f7b96":"<a id='TopicModelling'><\/a>\n## Topic Modelling\n\nThe idea of Topic Modelling in Natural Language Processing is to discover abstract topics in documents. The Latent Dirichlet Allocation [(LDA)](https:\/\/en.wikipedia.org\/wiki\/Latent_Dirichlet_allocation) is a generative statistical model that allows sets of observations to be explained by unobserved groups. This is within the *Unsupervised  Learning* category. Some examples can be found here: [Example 1](https:\/\/blog.usejournal.com\/nlp-for-topic-modeling-summarization-of-financial-documents-10-k-q-93070db96c1d), [Example 2](https:\/\/medium.com\/@yanlinc\/how-to-build-a-lda-topic-model-using-from-text-601cdcbfd3a6), [Example 3](https:\/\/towardsdatascience.com\/end-to-end-topic-modeling-in-python-latent-dirichlet-allocation-lda-35ce4ed6b3e0)\n\nSome preparation is needed first.\n\n### Pre preposing\n#### Selection of dates of interest\n* Data is available from different dates according to the financial product. The selected dates are from Dec 1, 2018 to Sep 30, 2020"}}