{"cell_type":{"7b370100":"code","52b6937a":"code","d381de2d":"code","3251a56c":"code","56497766":"code","66cee7d9":"code","4b54550f":"code","d82faff4":"code","71b60fad":"code","2e0ca808":"code","1d602e10":"code","09e5579a":"code","a346b42e":"markdown","3a087263":"markdown"},"source":{"7b370100":"import numpy as np \nimport pandas as pd \nimport os, random, sys, time, re, copy\n\nimport torch\nimport torch.nn as nn\nimport torch.optim as optim\nimport torch.nn.functional as F\nimport torch.utils.data as D\nfrom torch.nn.utils.rnn import pad_sequence\n\n# K-Fold spliter\nfrom sklearn.model_selection import StratifiedKFold, KFold\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Transformers library\nfrom transformers import *","52b6937a":"# Path to train csv file\nDATA_PATH = \"..\/input\/commonlitreadabilityprize\/\"\n\n# Path to model weights and vocab (select desirable): \n\n# MODEL_PATH = '..\/input\/distilbertbaseuncased'\n# MODEL_PATH = '..\/input\/pretrained-albert-pytorch\/albert-base-v1'\n# MODEL_PATH = '..\/input\/pretrained-albert-pytorch\/albert-xlarge-v1'\n# MODEL_PATH = '..\/input\/camembertbasesquadfrfquadpiaf\/camembert-base-squadFR-fquad-piaf'\n# MODEL_PATH = '..\/input\/roberta-transformers-pytorch\/distilroberta-base'\nMODEL_PATH = '..\/input\/roberta-transformers-pytorch\/roberta-base'\n# MODEL_PATH = '..\/input\/roberta-transformers-pytorch\/roberta-large'\n# MODEL_PATH = '..\/input\/bart-models-hugging-face-model-repository\/bart-base'\n# MODEL_PATH = '..\/input\/electra-base'\n# MODEL_PATH = '..\/input\/deberta\/base'\n\nMODEL_NAME = 'optimus_prime'\n# VOCAB_PATH = '..\/input\/roberta-transformers-pytorch\/roberta-base' \nVOCAB_PATH = MODEL_PATH\n\nN_FOLDS = 5\nEPOCHES = 5\nBATCH_SIZE = 24\nDEVICE = 'cuda' if torch.cuda.is_available() else 'cpu'\nMAX_SEQUENCE_LENGTH = 320\nLR = 2e-5\n\n# error log\nsys.stderr = open('err.txt', 'w')","d381de2d":"SEED = 7117\nrandom.seed(SEED)\nos.environ['PYTHONHASHSEED'] = str(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\ntorch.backends.cudnn.deterministic = True","3251a56c":"train_csv = pd.read_csv(os.path.join(DATA_PATH, 'train.csv'), index_col='id')\ntest_csv = pd.read_csv(os.path.join(DATA_PATH, 'test.csv'), index_col='id')\n\nsubm = pd.read_csv(os.path.join(DATA_PATH, 'sample_submission.csv'), index_col='id')\n\n# https:\/\/www.kaggle.com\/abhishek\/step-1-create-folds\ndf_size = train_csv.shape[0]\nnum_bins = int(np.floor(1 + np.log2(df_size)))\n# bin targets\ny = pd.cut(train_csv[\"target\"], bins=num_bins, labels=False)\n\ncv = StratifiedKFold(n_splits=N_FOLDS, shuffle=True, random_state=SEED)","56497766":"tokenizer = AutoTokenizer.from_pretrained(VOCAB_PATH,\n                   model_max_length=MAX_SEQUENCE_LENGTH)\ntrain_csv['token'] = train_csv.excerpt.apply(tokenizer)\ntest_csv['token'] = test_csv.excerpt.apply(tokenizer)","66cee7d9":"class LitDataset(D.Dataset):\n    \n    def __init__(self, token, target):\n        self.token = token\n        self.target = target\n        \n    def __len__(self):\n        return self.token.shape[0]\n\n    def __getitem__(self, idx):\n        return torch.tensor(self.token[idx].input_ids), \\\n                torch.tensor(self.token[idx].attention_mask), self.target[idx]\n    \ndef collate_fn(batch):\n    ids, attns, targets = zip(*batch)\n    ids = pad_sequence(ids, batch_first=True, padding_value=tokenizer.pad_token_id).to(DEVICE)\n    attns = pad_sequence(attns, batch_first=True, padding_value=tokenizer.pad_token_id).to(DEVICE)\n    targets = torch.tensor(targets).float().to(DEVICE)\n    return ids, attns, targets\n\ndef collate_fn_test(batch):\n    ids, attns, idxs = zip(*batch)\n    ids = pad_sequence(ids, batch_first=True, padding_value=tokenizer.pad_token_id).to(DEVICE)\n    attns = pad_sequence(attns, batch_first=True, padding_value=tokenizer.pad_token_id).to(DEVICE)\n    return idxs, ids, attns","4b54550f":"ds = LitDataset(train_csv.token, train_csv.target)\ntest_ds = LitDataset(test_csv.token, test_csv.index)\n\ntloader = D.DataLoader(test_ds, batch_size=BATCH_SIZE,\n                       shuffle=False, collate_fn = collate_fn_test, num_workers=0)","d82faff4":"### Table for results\nheader = r'''\n            Train         Validation\nEpoch |  MSE  |  RMSE |  MSE  |  RMSE | Time, m\n'''\n#          Epoch         metrics            time\nraw_line = '{:6d}' + '\\u2502{:7.3f}'*4 + '\\u2502{:6.2f}'","71b60fad":"@torch.no_grad()\ndef validation_fn(model, loader, loss_fn):\n    tloss = []\n    model.eval();\n    for texts, attns, target in loader:\n        outputs = model(texts, attention_mask=attns)\n        loss = loss_fn(outputs.logits.squeeze(-1), target)\n        tloss.append(loss.item())\n    tloss = np.array(tloss).mean()\n    return tloss\n\ndef oof_train(ds, cv, y, epochs = EPOCHES):\n    \n    loss_fn = torch.nn.MSELoss()\n    \n    for fold, (train_idx, valid_idx) in enumerate(cv.split(range(len(ds)), y)):\n        \n        train_ds = D.Subset(ds, train_idx)\n        loader = D.DataLoader(train_ds, batch_size=BATCH_SIZE,\n                              shuffle=True, collate_fn = collate_fn,num_workers=0)\n        \n        valid_ds = D.Subset(ds, valid_idx)\n        vloader = D.DataLoader(valid_ds, batch_size=BATCH_SIZE,\n                      shuffle=False, collate_fn = collate_fn,num_workers=0)\n        \n        model = AutoModelForSequenceClassification.from_pretrained( \n                          MODEL_PATH, num_labels=1).to(DEVICE);\n        \n        optimizer = optim.AdamW(model.parameters(), LR,\n                                betas=(0.9, 0.999), weight_decay=1e-1)\n        scheduler = get_constant_schedule_with_warmup(optimizer, 35)\n        print(header)\n        \n        # init state\n        best_metric = np.inf\n        best_model = model.state_dict()\n        \n        for epoch in range(1, epochs+1):      \n            start_time = time.time()\n            tloss = []          \n            model.train()\n            \n            for texts, attns, target in loader:\n                optimizer.zero_grad()\n                outputs = model(texts, attention_mask=attns)\n                loss = loss_fn(outputs.logits.squeeze(-1), target)\n                tloss.append(loss.item())\n                loss.backward()\n                optimizer.step()\n                scheduler.step()\n            tloss = np.array(tloss).mean()\n            vloss = validation_fn(model, vloader, loss_fn)\n            tmetric = tloss**.5\n            vmetric = vloss**.5\n            print(raw_line.format(epoch,tloss,tmetric,vloss,vmetric,(time.time()-start_time)\/60**1))\n            del loss, outputs\n            \n            if best_metric > vmetric:\n                with torch.no_grad():\n                    best_metric = vmetric\n                    best_model = copy.deepcopy(model.state_dict())\n            \n        # Save final state to the checkpoint\n        filename = f'{MODEL_NAME}_fold_{fold:02}.pt'\n        checkpoint = {\n            'model' : model.state_dict(),\n            'best_model' : best_model,\n            'best_metric' : best_metric,\n        }\n        torch.save(checkpoint,  filename)\n    \n        del model, vloader, loader, train_ds, valid_ds\n        torch.cuda.empty_cache()","2e0ca808":"oof_train(ds, cv, y, epochs = EPOCHES)","1d602e10":"model = AutoModelForSequenceClassification.from_pretrained( \n                  MODEL_PATH, num_labels=1).to(DEVICE);\n\nfor fold in range(N_FOLDS):\n    \n    filename = f'{MODEL_NAME}_fold_{fold:02}.pt'\n    weights = torch.load(filename)['model']\n    model.load_state_dict(weights);\n    model.eval();\n    del weights\n    # Get prediction for test set\n    ids, preds = [], [] \n    with torch.no_grad():\n        for batch_ids, texts, attn in tloader:\n            outputs = model(texts, attention_mask=attn)\n            ids += batch_ids\n            preds.append(outputs.logits.detach().squeeze(-1).cpu().numpy())\n\n    # Save prediction of test set\n    preds = np.concatenate(preds)\n    subm.loc[ids, 'target']  =  subm.loc[ids, 'target'].values + preds \/ N_FOLDS\n\n# Save to the file\nsubm.to_csv('submission.csv')","09e5579a":"# clean saves\n!rm -r *.pt\n!rm err.txt","a346b42e":"## Inference","3a087263":"## K-fold Cross-Validation"}}