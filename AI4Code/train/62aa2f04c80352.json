{"cell_type":{"4775352e":"code","6a05b5ac":"code","ab135bcb":"code","a73858f8":"code","8903dc13":"code","83d4a4c6":"code","e4defd68":"code","7ddf89ce":"code","38cbc7bd":"code","27ce8e21":"code","102f240e":"code","66aa8913":"code","b388dcd3":"code","a1a6c349":"code","3dcf4443":"code","eb5ea99a":"code","2c6d327c":"code","80412f0f":"code","860f7a48":"code","22f54dc9":"code","a07d128a":"code","a0e04fc1":"code","986d6059":"code","d4caaf95":"code","bef132b2":"code","7a02949e":"code","eb12cad2":"code","587084d9":"code","9dfa30cc":"code","35db2912":"markdown","fcb71910":"markdown","1a18345c":"markdown","0cbc8bf4":"markdown","61eb729b":"markdown","706a3fc0":"markdown","1c5a4c3f":"markdown","7fdf5ec5":"markdown","541e054f":"markdown","5bf94207":"markdown","1078eb65":"markdown","9040e007":"markdown","1165a733":"markdown","07fde05d":"markdown","69e324a9":"markdown","6435d562":"markdown","30777742":"markdown","87ce6df2":"markdown"},"source":{"4775352e":"import numpy as np \nimport pandas as pd \nimport tensorflow as tf\nimport string\nimport re\nimport os\nimport nltk\nfrom nltk.corpus import stopwords, twitter_samples\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer\nimport matplotlib.pyplot as plt ","6a05b5ac":"imdb_data_path = \"..\/input\/imdb-dataset-of-50k-movie-reviews\/IMDB Dataset.csv\"","ab135bcb":"imdb_data = pd.read_csv(imdb_data_path)","a73858f8":"# Converting the positive labels to 1 and the negative labels to 0\n\nimdb_data['sentiment'].mask(imdb_data['sentiment'] == 'positive', 1, inplace=True)\nimdb_data['sentiment'].mask(imdb_data['sentiment'] == 'negative', 0, inplace=True)","8903dc13":"# Get the reviews and the labels\n\nall_reviews = list(imdb_data['review'])\nlabels = np.asarray(imdb_data['sentiment'])","83d4a4c6":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\n\n# cutoff reviews after 200 words\nmaxlen = 200\ntraining_samples = 40000\nvalidation_samples = 5000\ntesting_samples = 5000\n\n# consider the top 100000 words in the dataset\nmax_words = 100000\n\n# tokenize each review in the dataset\ntokenizer = Tokenizer(num_words=max_words)\ntokenizer.fit_on_texts(all_reviews)\nsequences = tokenizer.texts_to_sequences(all_reviews)","e4defd68":"word_index = tokenizer.word_index\nprint(\"Found {} unique tokens.\".format(len(word_index)))\nind2word = dict([(value, key) for (key, value) in word_index.items()])","7ddf89ce":"# pad the sequences so that all sequences are of the same size\ndata = pad_sequences(sequences, maxlen=maxlen)","38cbc7bd":"# shuffling the data and labels\n\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\n\ndata = data[indices]\nlabels = labels[indices]\n\n# Splitting the data set to training and validation datasets \n\nx_train = data[: training_samples]\ny_train = labels[: training_samples]\n\nx_val = data[training_samples : training_samples + validation_samples]\ny_val = labels[training_samples : training_samples + validation_samples]\n\nx_test = data[training_samples + validation_samples: training_samples + validation_samples + testing_samples]\ny_test = labels[training_samples + validation_samples: training_samples + validation_samples + testing_samples]\n\nx_train = np.asarray(x_train).astype(np.int)\ny_train = np.asarray(y_train).astype(np.int)\nx_val = np.asarray(x_val).astype(np.int)\ny_val = np.asarray(y_val).astype(np.int)\nx_test = np.asarray(x_test).astype(np.int)\ny_test = np.asarray(y_test).astype(np.int)","27ce8e21":"x_train.shape","102f240e":"x_val.shape","66aa8913":"x_test.shape","b388dcd3":"embedding_dim = 300\nsimple_model = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(max_words, embedding_dim, input_length=maxlen),\n    #tf.keras.layers.Flatten(),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(256, activation='relu'),\n    tf.keras.layers.Dense(128, activation='relu'),\n    tf.keras.layers.Dense(64, activation='relu'),\n\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nsimple_model.compile(loss='binary_crossentropy',\n                     optimizer='adam',\n                     metrics=['accuracy'])\n\nsimple_model_history = simple_model.fit(x_train,y_train,\n                                        validation_data=(x_val,y_val),\n                                        epochs=10)","a1a6c349":"# embedding_dim = 300\n# simple_rnn_model = tf.keras.models.Sequential([\n#     tf.keras.layers.Embedding(max_words, embedding_dim, input_length=maxlen),\n#     tf.keras.layers.SimpleRNN(units=64, activation='tanh', return_sequences=True),\n#     tf.keras.layers.SimpleRNN(units=32, activation='tanh'),\n\n#     tf.keras.layers.Dense(1, activation='sigmoid')\n# ])\n\n# simple_rnn_model.compile(loss='binary_crossentropy',\n#                          optimizer='adam',\n#                          metrics=['accuracy'])\n\n# simple_rnn_model_history = simple_rnn_model.fit(x_train,y_train,\n#                                                 validation_data=(x_val,y_val),\n#                                                 epochs=5)","3dcf4443":"embedding_dim = 300\ngru_model = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(max_words, embedding_dim, input_length=maxlen),\n    tf.keras.layers.GRU(units=64, activation='tanh', return_sequences=True),\n    tf.keras.layers.GRU(units=32, activation='tanh'),\n\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\ngru_model.compile(loss='binary_crossentropy',\n                  optimizer='adam',\n                  metrics=['accuracy'])\n\ngru_model_history = gru_model.fit(x_train,y_train,\n                                  validation_data=(x_val,y_val),\n                                  epochs=7)","eb5ea99a":"embedding_dim = 300\nlstm_model = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(max_words, embedding_dim, input_length=maxlen),\n    tf.keras.layers.LSTM(units=64, activation='tanh', return_sequences=True),\n    tf.keras.layers.LSTM(units=32, activation='tanh'),\n\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nlstm_model.compile(loss='binary_crossentropy',\n                   optimizer='adam',\n                   metrics=['accuracy'])\n\nlstm_model_history = lstm_model.fit(x_train,y_train,\n                                    validation_data=(x_val,y_val),\n                                    epochs=7)","2c6d327c":"embedding_dim = 300\nbidirectional_lstm_model = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(max_words, embedding_dim, input_length=maxlen),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=64, activation='tanh', return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=32, activation='tanh', return_sequences=True)),\n\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nbidirectional_lstm_model.compile(loss='binary_crossentropy',\n                   optimizer='adam',\n                   metrics=['accuracy'])\n\nbidirectional_lstm_model_history = bidirectional_lstm_model.fit(x_train,y_train,\n                                                                validation_data=(x_val,y_val),\n                                                                epochs=7)","80412f0f":"embedding_dim = 100\nbidirectional_gru_model = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(max_words, embedding_dim, input_length=maxlen),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=64, activation='tanh', return_sequences=True)),\n    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(units=32, activation='tanh', return_sequences=True)),\n\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nbidirectional_gru_model.compile(loss='binary_crossentropy',\n                   optimizer='adam',\n                   metrics=['accuracy'])\n\nbidirectional_gru_model_history = bidirectional_gru_model.fit(x_train,y_train,\n                                                              validation_data=(x_val,y_val),\n                                                              epochs=7)","860f7a48":"embedding_dim = 300\nconv1d_model = tf.keras.models.Sequential([\n    tf.keras.layers.Embedding(max_words, embedding_dim, input_length=maxlen),\n    \n    tf.keras.layers.Conv1D(64, 7, activation='relu'),\n    tf.keras.layers.MaxPool1D(5),\n    tf.keras.layers.Conv1D(32, 7, activation='relu'),\n    tf.keras.layers.GlobalMaxPool1D(),\n\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nconv1d_model.compile(loss='binary_crossentropy',\n                     optimizer='adam',\n                     metrics=['accuracy'])\n\nconv1d_model_history = conv1d_model.fit(x_train,y_train,\n                                        validation_data=(x_val,y_val),\n                                        epochs=7)","22f54dc9":"def plot_acc_and_loss(model_histpry):\n\n    acc = model_histpry.history['accuracy']\n    val_acc = model_histpry.history['val_accuracy']\n\n    loss = model_histpry.history['loss']\n    val_loss = model_histpry.history['val_loss']\n\n    epochs = range(1, len(acc) + 1)\n\n    fig, ax = plt.subplots(1, 2, constrained_layout=True, figsize=(6, 4), dpi=80)\n\n    ax[0].plot(epochs, acc, label = \"Training Accuracy\", color='darkblue')\n    ax[0].plot(epochs, val_acc, label = \"Validation Accuracy\", color='darkgreen')\n    ax[0].grid(alpha=0.3)\n    ax[0].title.set_text('Training Vs Validation Accuracy')\n    ax[0].fill_between(epochs, acc, val_acc, color='crimson', alpha=0.3)\n    plt.setp(ax[0], xlabel='Epochs')\n    plt.setp(ax[0], ylabel='Accuracy')\n\n\n    ax[1].plot(epochs, loss, label = \"Training Loss\", color='darkblue')\n    ax[1].plot(epochs, val_loss, label = \"Validation Loss\", color='darkgreen')\n    ax[1].grid(alpha=0.3)\n    ax[1].title.set_text('Training Vs Validation Loss')\n    ax[1].fill_between(epochs,loss, val_loss, color='crimson', alpha=0.3)\n    plt.setp(ax[1], xlabel='Epochs')\n    plt.setp(ax[1], ylabel='Loss')","a07d128a":"plot_acc_and_loss(simple_model_history)","a0e04fc1":"# plot_acc_and_loss(simple_rnn_model_history)","986d6059":"plot_acc_and_loss(gru_model_history)","d4caaf95":"plot_acc_and_loss(lstm_model_history)","bef132b2":"plot_acc_and_loss(bidirectional_lstm_model_history)","7a02949e":"plot_acc_and_loss(bidirectional_gru_model_history)","eb12cad2":"plot_acc_and_loss(conv1d_model_history)","587084d9":"def eval_model(model):\n    \n    model_acc_train_dataset = model.evaluate(x_train, y_train)\n    model_acc_val_dataset = model.evaluate(x_val, y_val)\n    model_acc_test_dataset = model.evaluate(x_test, y_test)\n    \n    return model_acc_train_dataset, model_acc_val_dataset, model_acc_test_dataset\n\nsimple_model_acc_train_dataset, simple_model_acc_val_dataset, simple_model_acc_test_dataset = eval_model(simple_model)\n#simple_rnn_model_acc_train_dataset, simple_rnn_model_acc_val_dataset, simple_rnn_model_acc_test_dataset = eval_model(simple_rnn_model)\ngru_model_acc_train_dataset, gru_model_acc_val_dataset, gru_model_acc_test_dataset = eval_model(gru_model)\nlstm_model_acc_train_dataset, lstm_model_acc_val_dataset, lstm_model_acc_test_dataset = eval_model(lstm_model)\nbidirectional_lstm_model_acc_train_dataset, bidirectional_lstm_model_acc_val_dataset, bidirectional_lstm_model_acc_test_dataset = eval_model(bidirectional_lstm_model)\nbidirectional_gru_model_acc_train_dataset, bidirectional_gru_model_acc_val_dataset, bidirectional_gru_model_acc_test_dataset = eval_model(bidirectional_gru_model)\nconv1d_model_acc_train_dataset, conv1d_model_acc_val_dataset, conv1d_model_acc_test_dataset = eval_model(conv1d_model)\n\n\ntrain_accs = [simple_model_acc_train_dataset[1], gru_model_acc_train_dataset[1],\n              lstm_model_acc_train_dataset[1], bidirectional_lstm_model_acc_train_dataset[1], bidirectional_gru_model_acc_train_dataset[1],\n              conv1d_model_acc_train_dataset[1]]\n\nval_accs = [simple_model_acc_val_dataset[1], gru_model_acc_val_dataset[1],\n            lstm_model_acc_val_dataset[1], bidirectional_lstm_model_acc_val_dataset[1],bidirectional_gru_model_acc_val_dataset[1],\n            conv1d_model_acc_val_dataset[1]]\n\ntest_accs = [simple_model_acc_test_dataset[1], gru_model_acc_test_dataset[1],\n             lstm_model_acc_test_dataset[1], bidirectional_lstm_model_acc_test_dataset[1],bidirectional_gru_model_acc_val_dataset[1],\n             conv1d_model_acc_test_dataset[1]]\n\n\nmodels_eval_df = pd.DataFrame({\"Training Accuracy\":train_accs, \"Validation Accuracy\":val_accs, \"Testing Accuracy\":test_accs},\n                              index=['simple_model', 'gru_model','lstm_model',\n                                     'bidirectional_lstm_model', 'bidirectional_gru_model','conv1d_model'])","9dfa30cc":"models_eval_df","35db2912":"**Thank you for reading, I hope you enjoyed and benefited from it.**\n\n\n**If you have any questions or notes please leave it in the comment section.**\n\n\n**If you like this notebook please press upvote and thanks again.**","fcb71910":"In the GRU layer below the parameter dropout is a float specifying the dropot rate for input units of the layer, and the parameter recurrent_dropout specifying the dropout rate of the recurrent units. ","1a18345c":"## 3.1- Simple Model ","0cbc8bf4":"# 5- Testing and Prediction","61eb729b":"## 3.2- Simple Recurrent Neural Network ","706a3fc0":"# 1- Importing libraries","1c5a4c3f":"## 3.4- Using  Long Short-Tearm Memory (LSTM) ","7fdf5ec5":"## 3.3- Using Gated Recurrent Unit (GRU)  ","541e054f":"## 3.6- Using 1D ConvNets","5bf94207":"## 3.5- Using Bidirectional RNN","1078eb65":"In this notebook different Deep Learning models will be tested. The task is to classify reviews into two categories (binary classification) to positive or negative review. \n\nIt's very simple task where have different kinds of layers in the model that process the reviews and then the model ends with a dense layer with one neuron with sigmoid activation function for binary classification. \n\n\nDifferent types on NN will be tried Simple Recurrent Neural Networks (RNN), Long Short-Tearm Memory (LSTM), Gated Recurrent Neural Unit (GRU), Bidirectional RNN with different kinds of units, and 1D ConvNets. ","9040e007":"# 5- Thank you","1165a733":"# 3- Deep Learning Models","07fde05d":"# 2- Loading and Preprocessing data","69e324a9":"Simple RNN will take alot of time to train  after trying it on my machine even with  a GPU and will not perform well so it will not be trained ","6435d562":"# 4- Models Performance visualization","30777742":"We will see a warning:\n    \nWARNING:tensorflow:Layer gru_2 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n\n\nWARNING:tensorflow:Layer gru_3 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n\n\nTF will not the GPU for training and it will takes a lot of time. ","87ce6df2":"All models are overfitting which mean that we need for more tuning for the hyperparameters and adding some Regularization for the layers. \n\nDropout layers will be added for the GRU model and we will see if that has any impact on the model performance. "}}