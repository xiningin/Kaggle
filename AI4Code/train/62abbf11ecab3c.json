{"cell_type":{"032d5913":"code","30321e7e":"code","e75490a0":"code","e8c8fc7a":"code","cf7d14a2":"code","fd0241d0":"code","74e941bb":"code","01963926":"code","e80383bb":"code","fe4721a4":"code","8c827573":"code","f6d72fdd":"code","e65a63ad":"code","15ed42ff":"code","2158a9b6":"code","0ccef1b4":"code","9726ae67":"code","29b61244":"code","e9aeb20f":"code","295b1f2e":"code","ec4df21d":"markdown","7de515e4":"markdown","5756a106":"markdown","4af6c9b6":"markdown","15fea627":"markdown","ea5813d0":"markdown","a33731f6":"markdown","2dd230d0":"markdown"},"source":{"032d5913":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","30321e7e":"# Different Regressor libraries \nfrom lightgbm import LGBMRegressor # LGBM Regressor\nimport xgboost as xgb # XGB Regressor\nfrom catboost import CatBoostRegressor # CatBoost Resgressor\nimport gresearch_crypto\nimport traceback","e75490a0":"# Read the Crypto dataset files\ndf_train = pd.read_csv('\/kaggle\/input\/g-research-crypto-forecasting\/train.csv')\ndf_asset = pd.read_csv('\/kaggle\/input\/g-research-crypto-forecasting\/asset_details.csv').sort_values(\"Asset_ID\")","e8c8fc7a":"# Modify the training dataset\ndf_train.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf_train = df_train.dropna()\n\n# Modify the Assest Dataset\ndf_asset.replace([np.inf, -np.inf], np.nan, inplace=True)\ndf_asset = df_asset.dropna()","cf7d14a2":"# Asset dataset details are following.\ndf_asset","fd0241d0":"# Two new features from the competition tutorial\ndef upper_shadow(df):\n    return df['High'] - np.maximum(df['Close'], df['Open'])\n\ndef lower_shadow(df):\n    return np.minimum(df['Close'], df['Open']) - df['Low']\n\n# Extract Asset features\ndef get_features(df):\n    df_feat = df[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']].copy()\n    df_feat['Upper_Shadow'] = upper_shadow(df_feat)\n    df_feat['Lower_Shadow'] = lower_shadow(df_feat)\n    return df_feat\n\n# Extract features from dataset\ndef get_Xy_features(df_train, df_asset_id):\n    # Modfify with respect to 'asset ID'\n    df = df_train[df_train[\"Asset_ID\"] == df_asset_id]\n    df_proc = get_features(df)\n    df_proc['y'] = df['Target']\n    df_proc = df_proc.dropna(how=\"any\")\n    \n    # X, y features from the dataset.\n    X = df_proc.drop(\"y\", axis=1)\n    y = df_proc[\"y\"]\n    \n    return X, y","74e941bb":"# LGBM Regressor Model Function\ndef LGBMRegressor_model(X, y):\n        \n    # LGBM Regresor Model \n    model = LGBMRegressor(n_estimators=40) # 40000\n    model.fit(X, y)\n    \n    return model","01963926":"# Calling the implemenation of LGBM Regressor\nmodel_lgb = {}\n\nfor asset_id, asset_name in zip(df_asset['Asset_ID'], df_asset['Asset_Name']):\n    print(f\"Training model for {asset_name:<16} (ID={asset_id:<2})\")\n    X, y = get_Xy_features(df_train, asset_id)\n    model = LGBMRegressor_model(X, y)\n    \n    model_lgb[asset_id] = model","e80383bb":"# Checking LGBM Regressor Model interface\nx = get_features(df_train.iloc[1])\ny_pred_lgb = model_lgb[0].predict([x])\ny_pred_lgb[0]","fe4721a4":"# Delete unused dataset\ndel x\ndel y_pred_lgb","8c827573":"# LGBM Regressor Model Function\ndef XGBRegressor_model(X, y):\n    \n    model = xgb.XGBRegressor(\n    n_estimators=50, #500\n    learning_rate=0.05,\n    max_depth=12,\n    subsample=0.9,\n    colsample_bytree=0.7,\n    missing=-999,\n    random_state=1111,\n    tree_method='hist'  \n    )\n    \n    model.fit(X, y)\n    \n    return model","f6d72fdd":"# Calling the implemenation of XGB Regressor\nmodel_xgb = {}\n\nfor asset_id, asset_name in zip(df_asset['Asset_ID'], df_asset['Asset_Name']):\n    print(f\"Training model for {asset_name:<16} (ID={asset_id:<2})\")\n    X, y = get_Xy_features(df_train, asset_id)\n    model = XGBRegressor_model(X, y)\n    \n    model_xgb[asset_id] = model","e65a63ad":"# Checking XGB Regressor Model interface\nx = get_features(df_train.iloc[1])\ny_pred_xgb = model_xgb[0].predict(pd.DataFrame([x]))\ny_pred_xgb[0]","15ed42ff":"# Delete unused dataset\ndel x\ndel y_pred_xgb","2158a9b6":"# Features for CatBoost Regressor\ndef get_features_catboost(df,row=False):\n    df_feat = df[['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP', \"timestamp\"]].copy()\n    df_feat['Upper_Shadow'] = upper_shadow(df_feat)\n    df_feat['Lower_Shadow'] = lower_shadow(df_feat)\n    \n    ## Add some more feats\n    df_feat[\"high_div_low\"] = df_feat[\"High\"]\/df_feat[\"Low\"]\n    df_feat[\"open_sub_close\"] = df_feat[\"Open\"]-df_feat[\"Close\"]\n\n    ## possible seasonality, datetime  features (unlikely to me meaningful, given very short time-frames)\n    times = pd.to_datetime(df_feat[\"timestamp\"],unit=\"s\",infer_datetime_format=True)\n    if row:\n        df_feat[\"hour\"] = times.hour\n        df_feat[\"dayofweek\"] = times.dayofweek \n        df_feat[\"day\"] = times.day \n    else:\n        df_feat[\"hour\"] = times.dt.hour\n        df_feat[\"dayofweek\"] = times.dt.dayofweek \n        df_feat[\"day\"] = times.dt.day \n    df_feat = df_feat.drop(columns = \"timestamp\")\n    return df_feat\n\n# X, y Features from dataset\ndef get_Xy_and_Catboost(df_train, asset_id):\n    df = df_train[df_train[\"Asset_ID\"] == asset_id]\n    \n    df_proc = get_features_catboost(df)\n    df_proc['y'] = df['Target']\n    df_proc = df_proc.dropna(how=\"any\")\n    \n    X = df_proc.drop(\"y\", axis=1)\n    y = df_proc[\"y\"]\n\n    return X, y","0ccef1b4":"#Implementation of CatBoost Regressor\ndef CatBoostRegressor_model(X, y):\n    model = CatBoostRegressor(iterations=20, learning_rate=0.05, depth=10, random_seed=42) #1000\n    model.fit(X, y)\n\n    return model","9726ae67":"# Calling of CatBoost Regressor Model\nmodel_cat = {}\n\nfor asset_id, asset_name in zip(df_asset['Asset_ID'], df_asset['Asset_Name']):\n    print(f\"Training model for {asset_name:<16} (ID={asset_id:<2})\")\n    X, y = get_Xy_and_Catboost(df_train, asset_id)\n    model = CatBoostRegressor_model(X, y)\n    \n    model_cat[asset_id] = model","29b61244":"# Check the CatBoost model interface\nx = get_features_catboost(df_train.iloc[1],row=True)\ny_pred_cat = model_cat[0].predict([x])\ny_pred_cat[0]","e9aeb20f":"# Delete unused dataset\ndel x\ndel y_pred_cat","295b1f2e":"env = gresearch_crypto.make_env()\niter_test = env.iter_test()\n\nfor i, (df_test, df_pred) in enumerate(iter_test):\n    df_pred_lgbr = df_pred\n    df_pred_xgbr = df_pred\n    df_pred_cbtr = df_pred\n    for j , row in df_test.iterrows():\n        \n        # LGBM Regressor\n        model_lgbr = model_lgb[row['Asset_ID']]\n        x_test_lgbr = get_features(row)\n        y_pred_lgbr = model_lgbr.predict([x_test_lgbr])[0]\n        \n        df_pred_lgbr.loc[df_pred_lgbr['row_id'] == row['row_id'], 'Target'] = y_pred_lgbr\n        \n        # XGB Regressor\n        model_xgbr = model_xgb[row['Asset_ID']]\n        x_test_xgbr = get_features(row)\n        y_pred_xgbr = model_xgbr.predict(pd.DataFrame([x_test_xgbr]))[0]\n        \n        df_pred_xgbr.loc[df_pred_xgbr['row_id'] == row['row_id'], 'Target'] = y_pred_xgbr\n        \n        # CatBoost Regressor\n        model_cbtr = model_cat[row['Asset_ID']]\n        x_test_cbtr = get_features_catboost(row, row=True)\n        y_pred_cbtr = model_cbtr.predict([x_test_cbtr])[0]\n        \n        df_pred_cbtr.loc[df_pred_cbtr['row_id'] == row['row_id'], 'Target'] = y_pred_cbtr\n\n    # Display the first prediction dataframe\n    if i == 0:\n        pred_final = 0.5 * df_pred_lgbr + 0.2 * df_pred_xgbr +  0.3 * df_pred_cbtr\n        display(pred_final)\n\n    # Send submissions\n    env.predict(pred_final)","ec4df21d":"**In the Final prediction there are some computational error occurs due to whcih submission is not happening but still if you like the effort please upvote.!!!**","7de515e4":"# G-Research Crypto - Beginners Pipeline \n\nWe train regressor from different algorithms, one is `LGBMRegressor`, `XGBRegressor` & last `CatBoostRegressor`. Dataset of features from input frame are (`['Count', 'Open', 'High', 'Low', 'Close', 'Volume', 'VWAP']`), we get the predictions correctly using the iterator and we submit.\n\n## References:\n* [G-Research Crypto - Starter XGB Pipeline](https:\/\/www.kaggle.com\/tarlannazarov\/g-research-crypto-starter-xgb-pipeline)\n* [Basic Submission Template](https:\/\/www.kaggle.com\/sohier\/basic-submission-template)\n* [CatBoost Regressor](https:\/\/www.kaggle.com\/yamqwe\/crypto-prediction-catboost-regressor#Predict-&-submit)","5756a106":"# Reading & Feature Modification","4af6c9b6":"# Final Prediction: LGBM + XGB + CatBoost","15fea627":"# Implementation: LGBM Regressor","ea5813d0":"# Implementation: XGB Regressor","a33731f6":"# Implementation: CatBoost Regressor","2dd230d0":"# Utility Function\nThese function are used to create the some new features which are useful for the model.\nSpecial thanks to @DATAISTA0 (JULI\u00c1N PELLER) [notebook](https:\/\/www.kaggle.com\/julian3833\/g-research-starter-lgbm-pipeline) for functionality."}}