{"cell_type":{"009b2475":"code","b38d63bb":"code","69faed4d":"code","cd062117":"code","2deee65f":"code","dd8f9dca":"code","e4c720b2":"code","038b13c1":"code","2d659df2":"code","1af62fde":"code","8f6fda35":"code","70ecfb6a":"code","d6b56ee3":"code","3c225848":"code","c061e2cf":"code","92e79e16":"code","c71de5f9":"code","31f9f9d2":"code","a6440c89":"code","91cc4142":"code","c54cf748":"code","cd76daca":"code","18416f9a":"code","623336e5":"code","c7242a42":"code","089ef09a":"code","e6ae72cc":"code","f3d1ff38":"code","3645b8f2":"code","fb0fc3bd":"code","c6b7ba25":"code","17ba255d":"code","d323d21d":"code","8f2f98cf":"code","d75f7c25":"code","7a10a4de":"code","4c0248b5":"code","a5b7b321":"code","7a4091aa":"code","7a21a325":"code","e882aff9":"code","1949348f":"code","82c6ff32":"code","87200f7a":"code","8ff8201b":"code","a34763f4":"code","221c8f4a":"markdown","8978d558":"markdown","8f598171":"markdown","3065faed":"markdown","f75317ae":"markdown","3c25345c":"markdown","9ef6650d":"markdown","f196fdc0":"markdown","1a3c390b":"markdown","4010ec47":"markdown","d40c5538":"markdown","727b6ef3":"markdown","3ba0c42c":"markdown","0311c4e2":"markdown","137d9baa":"markdown","e3a8c104":"markdown","502ee71d":"markdown","5cc56212":"markdown","432af612":"markdown","eeffc23b":"markdown","596017f6":"markdown","911ebbb4":"markdown","16abf6a8":"markdown","864ed3f9":"markdown","b17bbd41":"markdown","e5934e62":"markdown","717b37ac":"markdown","3052ae83":"markdown","eb136bf2":"markdown","8922c629":"markdown","61d3568d":"markdown","78e8caaf":"markdown","b2e5bdb2":"markdown","a4738598":"markdown"},"source":{"009b2475":"#Libraries\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport math\nfrom math import *\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom plotly.offline import iplot\nimport plotly.graph_objects as go\nfrom collections import Counter\n# from ipywidgets import widgets\nfrom ipywidgets import *\nimport missingno as msno","b38d63bb":"train_df=pd.read_csv('..\/input\/titanic\/train.csv')\ntest_df=pd.read_csv('..\/input\/titanic\/test.csv')\nsub_df=pd.read_csv('..\/input\/titanic\/gender_submission.csv')\nIDtest = test_df[\"PassengerId\"]","69faed4d":"train_df.head()","cd062117":"train_df.info()","2deee65f":"def detect_outliers(df,n,features):\n    outlier_indices = []\n    for col in features:\n        # 1st quartile (25%)\n        Q1 = np.percentile(df[col], 25)\n        # 3rd quartile (75%)\n        Q3 = np.percentile(df[col],75)\n        # Interquartile range (IQR)\n        IQR = Q3 - Q1\n        # outlier step\n        outlier_step = 1.5 * IQR\n        # Determine a list of indices of outliers for feature col\n        outlier_list_col = df[(df[col] < Q1 - outlier_step) | (df[col] > Q3 + outlier_step )].index\n        # append the found outlier indices for col to the list of outlier indices \n        outlier_indices.extend(outlier_list_col)\n    # select observations containing more than 2 outliers\n    outlier_indices = Counter(outlier_indices)        \n    multiple_outliers = list( k for k, v in outlier_indices.items() if v > n )\n    return multiple_outliers   \n# detect outliers from Age, SibSp , Parch and Fare\nOutliers_to_drop = detect_outliers(train_df,2,[\"Age\",\"SibSp\",\"Parch\",\"Fare\"])","dd8f9dca":"# Drop outliers\ntrain_df = train_df.drop(Outliers_to_drop, axis = 0).reset_index(drop=True)","e4c720b2":"## Join train and test datasets \ntrain_len = len(train_df)\ntitanic_data =  pd.concat(objs=[train_df, test_df], axis=0).reset_index(drop=True)","038b13c1":"# Fill empty and NaNs values with NaN\ntitanic_data = titanic_data.fillna(np.nan)\n# Check for Null values\ntitanic_data.isnull().sum()","2d659df2":"# Correlation matrix between numerical values (SibSp Parch Age and Fare values) and Survived \nc = sns.heatmap(titanic_data[[\"Survived\",\"SibSp\",\"Parch\",\"Age\",\"Fare\"]].corr(),annot=True, fmt = \".2f\", cmap = \"coolwarm\")","1af62fde":"values=titanic_data[\"Survived\"].value_counts().values\nfig = go.Figure(data=[go.Pie(title='Titanic Survived\/Not Survived Data',labels=['Not Survived','Survived'], values=values, textinfo='label+percent',\n                             insidetextorientation='radial'\n                            )])\nfig.show()","8f6fda35":"sns.factorplot('Survived',data=titanic_data,hue='Sex',kind='count')","70ecfb6a":"fig,ax=plt.subplots(ncols=2,nrows=5,figsize=(15,20))\n\ntitanic_data['Sex'].value_counts().plot.pie(ax=ax[0,0])\nsns.countplot(x='Sex', hue=\"Survived\", data=titanic_data,ax=ax[0,1])\n\ntitanic_data['Embarked'].value_counts().plot.pie(ax=ax[1,0])\nsns.countplot(x='Embarked', hue=\"Survived\", data=titanic_data,ax=ax[1,1])\n\ntitanic_data['Pclass'].value_counts().plot.pie(ax=ax[2,0])\nsns.countplot(x='Pclass', hue=\"Survived\", data=titanic_data,ax=ax[2,1])\n\ntitanic_data['SibSp'].value_counts().plot.pie(ax=ax[3,0])\nsns.countplot(x='SibSp', hue=\"Survived\", data=titanic_data,ax=ax[3,1])\n\ntitanic_data['Parch'].value_counts().plot.pie(ax=ax[4,0])\nsns.countplot(x='Parch', hue=\"Survived\", data=titanic_data,ax=ax[4,1])\n\nplt.legend()","d6b56ee3":"msno.bar(titanic_data)","3c225848":"# Explore Fare distribution \ng = sns.distplot(train_df[\"Fare\"], color=\"m\", label=\"Skewness : %.2f\"%(titanic_data[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")","c061e2cf":"# Apply log to Fare to reduce skewness distribution\ntitanic_data[\"Fare\"] = train_df[\"Fare\"].map(lambda i: np.log(i) if i > 0 else 0)\ng = sns.distplot(train_df[\"Fare\"], color=\"b\", label=\"Skewness : %.2f\"%(titanic_data[\"Fare\"].skew()))\ng = g.legend(loc=\"best\")","92e79e16":"# Embarked has 2 missing values\n#Fill Embarked nan values of dataset set with 'S' most frequent value\ntitanic_data[\"Embarked\"] = titanic_data[\"Embarked\"].fillna(\"S\")","c71de5f9":"#Fill Fare missing values with the median value\ntitanic_data[\"Fare\"] = titanic_data[\"Fare\"].fillna(titanic_data[\"Fare\"].median())","31f9f9d2":"# convert Sex into categorical value 0 for male and 1 for female\ntitanic_data[\"Sex\"] = titanic_data[\"Sex\"].map({\"male\": 0, \"female\":1})","a6440c89":"# Filling missing value of Age \n## Fill Age with the median age of similar rows according to Pclass, Parch and SibSp\n# Index of NaN age rows\nindex_NaN_age = list(titanic_data[\"Age\"][titanic_data[\"Age\"].isnull()].index)\nfor i in index_NaN_age :\n    age_med = titanic_data[\"Age\"].median()\n    age_pred = titanic_data[\"Age\"][((titanic_data['SibSp'] == titanic_data.iloc[i][\"SibSp\"]) & (titanic_data['Parch'] == titanic_data.iloc[i][\"Parch\"]) & (titanic_data['Pclass'] == titanic_data.iloc[i][\"Pclass\"]))].median()\n    if not np.isnan(age_pred) :\n        titanic_data['Age'].iloc[i] = age_pred\n    else :\n        titanic_data['Age'].iloc[i] = age_med","91cc4142":"# Get Title from Name\ntitanic_data_title = [i.split(\",\")[1].split(\".\")[0].strip() for i in titanic_data[\"Name\"]]\ntitanic_data[\"Title\"] = pd.Series(titanic_data_title)\n# train_df[\"Title\"].head()\ntitanic_data[\"Title\"].unique()","c54cf748":"# Convert to categorical values Title \ntitanic_data[\"Title\"] = titanic_data[\"Title\"].replace(['Lady', 'the Countess','Countess','Capt', 'Col','Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'Rare')\ntitanic_data[\"Title\"] = titanic_data[\"Title\"].map({\"Master\":0, \"Miss\":1, \"Ms\" : 1 , \"Mme\":1, \"Mlle\":1, \"Mrs\":1, \"Mr\":2, \"Rare\":3})\ntitanic_data[\"Title\"] = titanic_data[\"Title\"].astype(int)","cd76daca":"# Drop Name variable\ntitanic_data.drop(labels = [\"Name\"], axis = 1, inplace = True)","18416f9a":"# Create a family size descriptor from SibSp and Parch\ntitanic_data['Family_Size']=titanic_data[\"SibSp\"] + titanic_data[\"Parch\"] + 1","623336e5":"#The family size seems to play an important role, survival probability is worst for large families.\n#Additionally, i decided to created 4 categories of family size.\ntitanic_data['Single'] = titanic_data['Family_Size'].map(lambda s: 1 if s == 1 else 0)\ntitanic_data['SmallF'] = titanic_data['Family_Size'].map(lambda s: 1 if  s == 2  else 0)\ntitanic_data['MedF'] = titanic_data['Family_Size'].map(lambda s: 1 if 3 <= s <= 4 else 0)\ntitanic_data['LargeF'] = titanic_data['Family_Size'].map(lambda s: 1 if s >= 5 else 0)","c7242a42":"titanic_data.head()","089ef09a":"titanic_data.info()","e6ae72cc":"# convert to indicator values Title and Embarked \ntitanic_data = pd.get_dummies(titanic_data, columns = [\"Title\"])\ntitanic_data = pd.get_dummies(titanic_data, columns = [\"Embarked\"], prefix=\"Em\")","f3d1ff38":"titanic_data.head()","3645b8f2":"# Replace the Cabin number by the type of cabin 'X' if not\ntitanic_data[\"Cabin\"] = pd.Series([i[0] if not pd.isnull(i) else 'X' for i in titanic_data['Cabin'] ])\ntitanic_data = pd.get_dummies(titanic_data, columns = [\"Cabin\"],prefix=\"Cabin\")","fb0fc3bd":"#extract the ticket prefix. When there is no prefix it returns X. \nTicket = []\nfor i in list(titanic_data.Ticket):\n    if not i.isdigit() :\n        Ticket.append(i.replace(\".\",\"\").replace(\"\/\",\"\").strip().split(' ')[0]) #Take prefix\n    else:\n        Ticket.append(\"X\")\n        \ntitanic_data[\"Ticket\"] = Ticket\ntitanic_data = pd.get_dummies(titanic_data, columns = [\"Ticket\"], prefix=\"T\")","c6b7ba25":"# Create categorical values for Pclass\ntitanic_data[\"Pclass\"] = titanic_data[\"Pclass\"].astype(\"category\")\ntitanic_data = pd.get_dummies(titanic_data, columns = [\"Pclass\"],prefix=\"Pc\")","17ba255d":"# Drop useless variables \ntitanic_data.drop(labels = [\"PassengerId\"], axis = 1, inplace = True)","d323d21d":"titanic_data.head()","8f2f98cf":"# Transform dataset back to train and test dataset\ntrain_df = titanic_data[:train_len]\ntest_df = titanic_data[train_len:]\ntest_df.drop(labels=[\"Survived\"],axis = 1,inplace=True)","d75f7c25":"# Separate train features and label \ntrain_df[\"Survived\"] = train_df[\"Survived\"].astype(int)\nY_train = train_df[\"Survived\"]\nX_train = train_df.drop(labels = [\"Survived\"],axis = 1)","7a10a4de":"# Libraries\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, GradientBoostingClassifier, ExtraTreesClassifier, VotingClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import GridSearchCV, cross_val_score, StratifiedKFold, learning_curve","4c0248b5":"# Cross validate model with Kfold stratified cross validation\nkfold = StratifiedKFold(n_splits=10)","a5b7b321":"# Test all the above stated algorithms\nrandom_state = 2\nclassifiers = []\nclassifiers.append(SVC(random_state=random_state))\nclassifiers.append(DecisionTreeClassifier(random_state=random_state))\nclassifiers.append(AdaBoostClassifier(DecisionTreeClassifier(random_state=random_state),random_state=random_state,learning_rate=0.1))\nclassifiers.append(RandomForestClassifier(random_state=random_state))\nclassifiers.append(ExtraTreesClassifier(random_state=random_state))\nclassifiers.append(GradientBoostingClassifier(random_state=random_state))\nclassifiers.append(MLPClassifier(random_state=random_state))\nclassifiers.append(KNeighborsClassifier())\nclassifiers.append(LogisticRegression(random_state = random_state))\nclassifiers.append(LinearDiscriminantAnalysis())\ncv_results = []\nfor classifier in classifiers :\n    cv_results.append(cross_val_score(classifier, X_train, y = Y_train, scoring = \"accuracy\", cv = kfold, n_jobs=4))\ncv_means = []\ncv_std = []\nfor cv_result in cv_results:\n    cv_means.append(cv_result.mean())\n    cv_std.append(cv_result.std())\ncv_res = pd.DataFrame({\"CrossValMeans\":cv_means,\"CrossValerrors\": cv_std,\"Algorithm\":[\"SVC\",\"DecisionTree\",\"AdaBoost\",\n\"RandomForest\",\"ExtraTrees\",\"GradientBoosting\",\"MultipleLayerPerceptron\",\"KNeighboors\",\"LogisticRegression\",\"LinearDiscriminantAnalysis\"]})\n\ng = sns.barplot(\"CrossValMeans\",\"Algorithm\",data = cv_res, palette=\"Set3\",orient = \"h\",**{'xerr':cv_std})\ng.set_xlabel(\"Mean Accuracy\")\ng = g.set_title(\"Cross validation scores\")","7a4091aa":"# I chose the SVC, AdaBoost, RandomForest , ExtraTrees and the GradientBoosting classifiers for the ensemble modeling.\n\n# Adaboost\nDTC = DecisionTreeClassifier()\n\nadaDTC = AdaBoostClassifier(DTC, random_state=7)\n\nada_param_grid = {\"base_estimator__criterion\" : [\"gini\", \"entropy\"],\n              \"base_estimator__splitter\" :   [\"best\", \"random\"],\n              \"algorithm\" : [\"SAMME\",\"SAMME.R\"],\n              \"n_estimators\" :[1,2],\n              \"learning_rate\":  [0.0001, 0.001, 0.01, 0.1, 0.2, 0.3,1.5]}\n\ngsadaDTC = GridSearchCV(adaDTC,param_grid = ada_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsadaDTC.fit(X_train,Y_train)\n\nada_best = gsadaDTC.best_estimator_\ngsadaDTC.best_score_","7a21a325":"#ExtraTrees \nExtC = ExtraTreesClassifier()\n\n\n## Search grid for optimal parameters\nex_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsExtC = GridSearchCV(ExtC,param_grid = ex_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsExtC.fit(X_train,Y_train)\n\nExtC_best = gsExtC.best_estimator_\n\n# Best score\ngsExtC.best_score_\n","e882aff9":"# RFC Parameters tunning \nRFC = RandomForestClassifier()\n\n\n## Search grid for optimal parameters\nrf_param_grid = {\"max_depth\": [None],\n              \"max_features\": [1, 3, 10],\n              \"min_samples_split\": [2, 3, 10],\n              \"min_samples_leaf\": [1, 3, 10],\n              \"bootstrap\": [False],\n              \"n_estimators\" :[100,300],\n              \"criterion\": [\"gini\"]}\n\n\ngsRFC = GridSearchCV(RFC,param_grid = rf_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsRFC.fit(X_train,Y_train)\n\nRFC_best = gsRFC.best_estimator_\n\n# Best score\ngsRFC.best_score_","1949348f":"# Gradient boosting tunning\n\nGBC = GradientBoostingClassifier()\ngb_param_grid = {'loss' : [\"deviance\"],\n              'n_estimators' : [100,200,300],\n              'learning_rate': [0.1, 0.05, 0.01],\n              'max_depth': [4, 8],\n              'min_samples_leaf': [100,150],\n              'max_features': [0.3, 0.1] \n              }\n\ngsGBC = GridSearchCV(GBC,param_grid = gb_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsGBC.fit(X_train,Y_train)\n\nGBC_best = gsGBC.best_estimator_\n\n# Best score\ngsGBC.best_score_\n","82c6ff32":"### SVC classifier\nSVMC = SVC(probability=True)\nsvc_param_grid = {'kernel': ['rbf'], \n                  'gamma': [ 0.001, 0.01, 0.1, 1],\n                  'C': [1, 10, 50, 100,200,300, 1000]}\n\ngsSVMC = GridSearchCV(SVMC,param_grid = svc_param_grid, cv=kfold, scoring=\"accuracy\", n_jobs= 4, verbose = 1)\n\ngsSVMC.fit(X_train,Y_train)\n\nSVMC_best = gsSVMC.best_estimator_\n\n# Best score\ngsSVMC.best_score_","87200f7a":"def plot_learning_curve(estimator, title, X, y, ylim=None, cv=None,\n                        n_jobs=-1, train_sizes=np.linspace(.1, 1.0, 5)):\n    \"\"\"Generate a simple plot of the test and training learning curve\"\"\"\n    plt.figure()\n    plt.title(title)\n    if ylim is not None:\n        plt.ylim(*ylim)\n    plt.xlabel(\"Training examples\")\n    plt.ylabel(\"Score\")\n    train_sizes, train_scores, test_scores = learning_curve(\n        estimator, X, y, cv=cv, n_jobs=n_jobs, train_sizes=train_sizes)\n    train_scores_mean = np.mean(train_scores, axis=1)\n    train_scores_std = np.std(train_scores, axis=1)\n    test_scores_mean = np.mean(test_scores, axis=1)\n    test_scores_std = np.std(test_scores, axis=1)\n    plt.grid()\n\n    plt.fill_between(train_sizes, train_scores_mean - train_scores_std,\n                     train_scores_mean + train_scores_std, alpha=0.1,\n                     color=\"r\")\n    plt.fill_between(train_sizes, test_scores_mean - test_scores_std,\n                     test_scores_mean + test_scores_std, alpha=0.1, color=\"g\")\n    plt.plot(train_sizes, train_scores_mean, 'o-', color=\"r\",\n             label=\"Training score\")\n    plt.plot(train_sizes, test_scores_mean, 'o-', color=\"g\",\n             label=\"Cross-validation score\")\n\n    plt.legend(loc=\"best\")\n    return plt\n\ng = plot_learning_curve(gsRFC.best_estimator_,\"RF mearning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsExtC.best_estimator_,\"ExtraTrees learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsSVMC.best_estimator_,\"SVC learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsadaDTC.best_estimator_,\"AdaBoost learning curves\",X_train,Y_train,cv=kfold)\ng = plot_learning_curve(gsGBC.best_estimator_,\"GradientBoosting learning curves\",X_train,Y_train,cv=kfold)","8ff8201b":"votingC = VotingClassifier(estimators=[('rfc', RFC_best), ('extc', ExtC_best),\n('svc', SVMC_best), ('adac',ada_best),('gbc',GBC_best)], voting='soft', n_jobs=4)\n\nvotingC = votingC.fit(X_train, Y_train)","a34763f4":"# Predict and Submit results\ntest_Survived = pd.Series(votingC.predict(test_df), name=\"Survived\")\nresults = pd.concat([IDtest,test_Survived],axis=1)\nresults.to_csv(\"ensemble_python_voting.csv\",index=False)","221c8f4a":"##### Cabin","8978d558":"##### Age","8f598171":"<h3>Feature Engineering<\/h3>","3065faed":"![18686404_101.jpg](attachment:9bf003f8-e0b6-461c-8168-065d2f4fcfbc.jpg)","f75317ae":"<b> We have 22 features now.....<\/b>","3c25345c":"#### Columns Description\n\n- PassengerId: Passenger ID number\n- Survived: Passegner survived or not (0= Not survived, 1=Survived)\n- pclass: A proxy for socio-economic status (1st = Upper, 2nd = Middle, 3rd = Lower)\n- age: Passenger Age\n- sibsp:  siblings \/ spouses aboard the Titani\n- parch:  of parents \/ children aboard the Titanic\n- fare:\tPassenger fare\n- sex :\tmale\/female\n- Embarked : Port of Embarkation (C = Cherbourg, Q = Queenstown, S = Southampton)","9ef6650d":"#### \nOnly Fare feature seems to have a significative correlation with the survival probability.\n\nIt doesn't mean that the other features are not usefull. Subpopulations in these features can be correlated with the survival. To determine this, we need to explore in detail these features","f196fdc0":"##### Sex","1a3c390b":"For more information about the passengers let's replace the Ticket feature column by the ticket prefix. ","4010ec47":"##### Ticket","d40c5538":"<h3>Data Description<\/h3>","727b6ef3":"The Cabin feature column contains 292 values and 1007 missing values. Let's assume that passengers without a cabin have a missing value displayed instead of the cabin number.","3ba0c42c":"### Let's compare 10 popular classifiers and evaluate the mean accuracy of each of them by a stratified kfold cross validation procedure.\n<ol>\n  <li>Logistic regression<\/li>\n  <li>SVC<\/li>\n  <li>KNN<\/li>\n  <li>Decision Tree<\/li>\n  <li>Random Forest<\/li>\n  <li>Gradient Boosting<\/li>\n  <li>AdaBoost<\/li>\n  <li>Extra Trees<\/li>\n  <li>Multiple layer perceprton (neural network)<\/li>\n  <li>Linear Discriminant Analysis<\/li>\n<\/ol>","0311c4e2":"##### Inference\nAs we see, Age column contains 256 missing values in the whole dataset. Since there is subpopulations that have more chance to survive (children for example), it is preferable to keep the age feature and to impute the missing values.","137d9baa":"##### Embarked","e3a8c104":"##### Inference\nAs we can see, Fare distribution is very skewed. This can lead to overweigth very high values in the model, even if it is scaled. In this case, it is better to transform it with the log function to reduce this skew.","502ee71d":"<h4>Missing Values<\/h4>","5cc56212":"<h4>Survived Column Analysis<\/h4>","432af612":"##### Name","eeffc23b":"##### Inference\nGradientBoosting and Adaboost classifiers tend to overfit the training set. According to the growing cross-validation curves GradientBoosting and Adaboost could perform better with more training examples.\n\nSVC and ExtraTrees classifiers seem to better generalize the prediction since the training and cross-validation curves are close together.","596017f6":"##### Pclass","911ebbb4":"##### PassengerId","16abf6a8":"#### Inference\n\n##### 1. Sex\nIt is clearly obvious that Male have less chance to survive than Female.So Sex, might play an important role in the prediction of the survival.\n##### 2. Pclass\nThe passenger survival is not the same in the 3 classes. First class passengers have more chance to survive than second class and third class passengers.\n##### 3. Embarked\nIt seems that passenger coming from Cherbourg (C) have more chance to survive. My hypothesis is that the proportion of first class passengers is higher for those who came from Cherbourg than Queenstown (Q), Southampton (S)\n##### 4. SibSp \nIt seems that passengers having a lot of siblings\/spouses have less chance to survive.\nSingle passengers (0 SibSP) or with two other persons (SibSP 1 or 2) have more chance to survive\n##### 5. Parch\nSmall families have more chance to survive, more than single (Parch 0), medium (Parch 3,4) and large families (Parch 5,6 ).","864ed3f9":"#### Ensemble modeling\n##### Combining models\nLet's use voting classifier to combine the predictions coming from the 5 classifiers. I preferred to pass the argument \"soft\" to the voting parameter to take into account the probability of each vote.","b17bbd41":"<h5> From above plots we can analyze that more male died and more female survived.<\/h5>","e5934e62":"##### Family","717b37ac":"<h1><center>Titanic - Machine Learning from Disaster<\/center><\/h1>","3052ae83":"<h4> Outlier Detection<\/h4>","eb136bf2":"<h4> Let's Explore more 'Columns' quickly!!! <\/h4>","8922c629":"##### Fare","61d3568d":"##### Fare","78e8caaf":"<h4> First, find the Correlation<\/h4>","b2e5bdb2":"<h2><center>Modelling<\/center><\/h2>","a4738598":"<h2><center> Exploratory Data Analysis<\/center><\/h2>"}}