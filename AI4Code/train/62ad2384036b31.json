{"cell_type":{"d34454ee":"code","35f37a12":"code","b840e4e1":"code","0ae6b162":"code","bede574e":"code","48f19e5c":"code","518df535":"code","e0465160":"code","22438a84":"code","781b18e1":"code","fb81c207":"code","16e3f419":"code","d7b7caf8":"code","b689a555":"code","0d58adfd":"code","8d7ede08":"code","b97bf9fb":"code","1f0d961b":"code","c8892f35":"code","93896d64":"code","34843094":"code","54da6f7f":"code","d529d33e":"code","2e3a63ff":"code","ef6705d2":"code","4112b42b":"code","00f729c1":"code","a6bbcd0b":"code","e5351ad4":"code","1daaf8d6":"code","550fdad9":"code","4d08a0d2":"code","bbb0d28b":"code","7e890f83":"code","54a5b707":"code","84303517":"code","ed8e0150":"code","97f85694":"code","7f17ac1f":"code","26a6bc11":"code","f646f992":"code","defd7b7f":"code","65c17269":"code","b94d573d":"code","3e7422e1":"code","28151396":"code","3869dfed":"code","2d494e8a":"code","4eeed3e4":"code","b01206c7":"code","2ff32b6d":"code","41f6d11a":"code","6dea4438":"code","af0b5de3":"code","de7a4793":"code","35a2252f":"code","841848c1":"code","2dc2a150":"code","57b025aa":"code","a78a8899":"code","c167e1f0":"code","26c5529d":"code","83e0c3e3":"code","ce626f52":"code","51053b29":"code","6f477c48":"code","c5742617":"code","e89eab82":"code","c024b75c":"code","043026b4":"code","2a66ab3d":"code","893fce7a":"markdown","a28dc9db":"markdown","d2040a19":"markdown","574d5bbe":"markdown","43080b5d":"markdown","b0309512":"markdown","785228c9":"markdown","5996a6a8":"markdown","cc62adef":"markdown","f86b602a":"markdown","51edc172":"markdown","a62ee33c":"markdown","b71ca6b0":"markdown","9093b171":"markdown","8ce59a24":"markdown","e1d42e8c":"markdown","baf3c39e":"markdown","f699bb62":"markdown","7116fab2":"markdown","a76e620e":"markdown","ae4eed9b":"markdown","5d50aa76":"markdown","ceb74290":"markdown","496e4092":"markdown","8834b9f2":"markdown","98f76314":"markdown","d8885296":"markdown","0eaad862":"markdown","ff666b73":"markdown","ea6a6d70":"markdown","3cd4254b":"markdown","ef0a5fda":"markdown","c1792547":"markdown","df1a02e5":"markdown","28708c90":"markdown","fe5e0e76":"markdown","df62c377":"markdown","5fb26435":"markdown","dd06b0f9":"markdown","3fe5a4d1":"markdown","8f74e6ac":"markdown","7aefd432":"markdown","91892fdd":"markdown","1a6bed69":"markdown","a7d9bd5b":"markdown"},"source":{"d34454ee":"#Modules\nimport os\nimport time\nimport glob\nimport numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport scipy.stats\n%matplotlib inline","35f37a12":"# Grab all .dat files in input folder\npath = '..\/input\/' # use your path\nall_files = glob.glob(os.path.join(path, \"*.dat\"))     # advisable to use os.path.join as this makes concatenation OS independent\n\n# Remove batch 7 as there is an experimental error which caused mismatches in concentration levels. \n    # Refer to (https:\/\/www.sciencedirect.com\/science\/article\/pii\/S2352340915000050) for more info.\n\n# Read each .dat file and concat all\ndf_from_each_file = (pd.read_csv(f, sep=\"\\s+\",index_col=0, header=None) for f in all_files)\ndf = pd.concat(df_from_each_file, sort=True)\n\n# Seperate feature and value in each cell eg. 1:15596.16 --> 15596.16 \nfor col in df.columns.values:\n    df[col] = df[col].apply(lambda x: float(str(x).split(':')[1]))\n\n# Make Index(Gas type) a column and reset index to original\ndf = df.rename_axis('Gas').reset_index()\n\n# Sort by Gas and reindex\ndf.sort_values(by=['Gas'],inplace=True)\ndf.reset_index(drop=True,inplace=True)","b840e4e1":"# Check total number of gases\ndf.Gas.nunique()","0ae6b162":"df.head()","bede574e":"# Check no values were lost in concatenation\ndf.shape","48f19e5c":"pd.unique(df.dtypes),len(df.select_dtypes(exclude='object').columns) - 1","518df535":"df.describe()","e0465160":"sns.countplot(df.Gas)\nsns.set(style=\"darkgrid\")\nplt.title('Gas Count')\nplt.show()","22438a84":"sns.distplot(df.Gas)\nplt.xlim(1, 6)\nplt.title('Distribution of Gas')\nplt.show()","781b18e1":"conc = df.iloc[:,1]","fb81c207":"# Divide concentrations for readability in plot\nconc_red = conc.apply(lambda x: x\/10000)\n\nfig = plt.figure(figsize=(22, 5))\nfig.add_subplot(121)\nsns.distplot(conc_red)\nplt.title('Distribution of Concentrations')\nplt.xlabel('Gas concentration Levels (x10000)')\n\nfig.add_subplot(122)\nsns.boxplot(conc_red)\nplt.title('Concentration')\nplt.xlabel('Gas concentration Levels (x10000)')\n\nplt.show()","16e3f419":"print(\"Skew of Gas concentration is: {}\".format(conc.skew().round(decimals=2)))","d7b7caf8":"plt.figure(figsize=(22, 5))\nsns.distplot(np.log(conc + 1 - min(conc)))\nplt.title('Distribution of Log-transformed Concentration')\nplt.xlabel('log(Concentration)')\nplt.show()","b689a555":"attr = df.iloc[:,2:].copy()\nattr.head()","0d58adfd":"fig = plt.figure(figsize=(20,200))\nfor i in range(len(attr.columns)):\n    fig.add_subplot(64,2,i+1)\n    sns.scatterplot(attr.iloc[:,i],conc_red, hue=\"Gas\", data=df, legend=\"full\")\n    plt.xlabel(attr.columns[i-1])\n    plt.ylabel(\"Gas Concentration(x10000)\")\n    \nfig.tight_layout()    \nplt.show()","8d7ede08":"correlation = df.corr()\n\nf, ax = plt.subplots(figsize=(20,10))\nplt.title('Correlations in dataset', size=20)\nsns.heatmap(correlation)\nplt.show()","b97bf9fb":"# Sort correletions of the concentration column\nconc_corr = correlation.iloc[:,1].sort_values(ascending=False)\n\n# Show all but with itself (correlation with self = 1)\nconc_corr[1:].head(20)","1f0d961b":"# The bottom of the list \nconc_corr[1:].tail(20)","c8892f35":"fig = plt.figure(figsize=(20,50))\nfor i in range(0,20):\n    fig.add_subplot(10,2,i+1)\n    sns.scatterplot(attr.iloc[:,conc_corr.index[i]],conc_red, hue=\"Gas\", palette= \"Set1\", data=df, legend=\"full\")\n    plt.xlabel(conc_corr.index[i])\n    plt.ylabel(\"Gas Concentration(x10000)\")\n    \nfig.tight_layout()    \nplt.show()","93896d64":"# Make a copy of the data\ndf_copy = df.copy() \n\n# Assign features and target\nX = df_copy.iloc[:,1:]\ny = df_copy.iloc[:,0]","34843094":"y.head()","54da6f7f":"X.head()","d529d33e":"from sklearn.preprocessing import StandardScaler\nX_scaled = X.copy()\nX_scaled = StandardScaler().fit(X_scaled).transform(X_scaled)","2e3a63ff":"cov_matrix = np.cov(X_scaled.T)","ef6705d2":"eig_val, eig_vec = np.linalg.eig(cov_matrix)\nprint('Eigenvectors \\n%s' %eig_vec)\nprint('\\nEigenvalues \\n%s' %eig_val)","4112b42b":"# Make a list of (eigenvalue, eigenvector) tuples\neig_pairs = [(np.abs(eig_val[i]), eig_vec[:,i]) for i in range(len(eig_val))]\n\n# Sort the (eigenvalue, eigenvector) tuples from high to low\neig_pairs.sort(key=lambda x: x[0], reverse=True)\n\n# Visually confirm that the list is correctly sorted by decreasing eigenvalues\nprint('Eigenvalues in descending order:')\nfor i in eig_pairs:\n    print(i[0])","00f729c1":"tot = sum(eig_val)\nvar_exp = [(i \/ tot)*100 for i in sorted(eig_val, reverse=True)]","a6bbcd0b":"plt.figure(figsize=(20, 4))\nplt.bar(range(128), var_exp)\nplt.ylabel('Explained variance ratio')\nplt.xlabel('Principal components')\nplt.xlim(0,20)\nplt.xticks(range(-1, 20))\nplt.tight_layout()","e5351ad4":"from sklearn.decomposition import PCA\npca = PCA()\nX_scaled = pca.fit_transform(X_scaled)","1daaf8d6":"plt.figure(figsize=(10, 4))\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlim(0,27)\nplt.xticks(range(0,27))\nplt.title('Cumulative variance of principle components')\nplt.xlabel('Number of components')\nplt.ylabel('Cumulative explained variance')\nplt.tight_layout()","550fdad9":"print(pca.explained_variance_ratio_)","4d08a0d2":"pca = PCA(n_components=2)\npca = pca.fit(X_scaled)\nX_PCA = pca.transform(X_scaled)","bbb0d28b":"from sklearn.preprocessing import label_binarize\n\n# Binarize classes into one hot columns.\ny_ohe = label_binarize(y, classes=[1,2,3,4,5,6])\n\n# Store the amount of classes (We know there are 6 but this is good practice)\nn_classes = y_ohe.shape[1]","7e890f83":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.2,random_state=4)\n\ny_train_nobinary = y_train.copy()\ny_test_nobinary = y_test.copy()\n\nscaler = StandardScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.transform(X_test)\ny_train = label_binarize(y_train, classes=[1,2,3,4,5,6])\ny_test = label_binarize(y_test, classes=[1,2,3,4,5,6])","54a5b707":"def plot_roc(y_test,y_pred,title):\n    fpr = dict()\n    tpr = dict()\n    roc_auc = dict()\n    for i in range(n_classes):\n        fpr[i], tpr[i], _ = roc_curve(y_test[:, i], y_pred[:, i])\n        roc_auc[i] = auc(fpr[i], tpr[i])\n\n    # Compute micro-average ROC curve and ROC area\n    fpr[\"micro\"], tpr[\"micro\"], _ = roc_curve(y_test.ravel(), y_pred.ravel())\n    roc_auc[\"micro\"] = auc(fpr[\"micro\"], tpr[\"micro\"])\n\n    # Compute macro-average ROC curve and ROC area\n    all_fpr = np.unique(np.concatenate([fpr[i] for i in range(n_classes)]))\n\n    # Then interpolate all ROC curves at this points\n    mean_tpr = np.zeros_like(all_fpr)\n    for i in range(n_classes):\n        mean_tpr += interp(all_fpr, fpr[i], tpr[i])\n\n    # Finally average it and compute AUC\n    mean_tpr \/= n_classes\n    lw=2\n    fpr[\"macro\"] = all_fpr\n    tpr[\"macro\"] = mean_tpr\n    roc_auc[\"macro\"] = auc(fpr[\"macro\"], tpr[\"macro\"])\n\n    # Plot all ROC curves\n    plt.figure()\n    plt.plot(fpr[\"micro\"], tpr[\"micro\"],\n             label='micro-average ROC curve (area = {0:0.3f})'\n                   ''.format(roc_auc[\"micro\"]),\n             color='deeppink', linestyle=':', linewidth=4)\n\n    plt.plot(fpr[\"macro\"], tpr[\"macro\"],\n             label='macro-average ROC curve (area = {0:0.3f})'\n                   ''.format(roc_auc[\"macro\"]),\n             color='navy', linestyle=':', linewidth=4)\n\n    plt.plot([0, 1], [0, 1], 'k--', lw=lw)\n    plt.xlim([-0.01, 1.0])\n    plt.ylim([0.0, 1.05])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title(title)\n    plt.legend(loc=\"lower right\")\n    plt.show()","84303517":"def permutation_test_between_clfs(y_test, pred_proba_1, pred_proba_2, nsamples=1000):\n    auc_differences = []\n    auc1 = roc_auc_score(y_test.ravel(), pred_proba_1.ravel())\n    auc2 = roc_auc_score(y_test.ravel(), pred_proba_2.ravel())\n    observed_difference = auc1 - auc2\n    for _ in range(nsamples):\n        mask = np.random.randint(2, size=len(pred_proba_1.ravel()))\n        p1 = np.where(mask, pred_proba_1.ravel(), pred_proba_2.ravel())\n        p2 = np.where(mask, pred_proba_2.ravel(), pred_proba_1.ravel())\n        auc1 = roc_auc_score(y_test.ravel(), p1)\n        auc2 = roc_auc_score(y_test.ravel(), p2)\n        auc_differences.append(auc1 - auc2)\n    return print(\"difference in roc curves: {0:.4f} \\nprobability to observe a larger difference on a shuffled data set: {1}\".format(observed_difference, np.mean(auc_differences >= observed_difference)))\n","ed8e0150":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import roc_curve, auc, roc_auc_score\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn import metrics\nfrom scipy import interp\nfrom itertools import cycle\n\nstart = time.time()\n\n# Learn to predict each class against the other\nclassifier = OneVsRestClassifier(LogisticRegression(solver='sag',n_jobs=-1))\nclassifier.fit(X_train, y_train)\ny_pred1 = classifier.predict_proba(X_test)\n\nend = time.time()\nprint(\"\\nTime taken: {:.2f} seconds\".format(end-start))","97f85694":"confusion_matrix = metrics.confusion_matrix(y_test.argmax(axis=1),y_pred1.argmax(axis=1))\nconfusion_matrix","7f17ac1f":"auc_roc = metrics.classification_report(np.argmax(y_test, axis=1),np.argmax(y_pred1, axis=1))\nprint('Logistic Regression Classification Report:\\n {}'.format(auc_roc))","26a6bc11":"plot_roc(y_test,y_pred1,\"ROC Logistic Regression\")","f646f992":"from sklearn.svm import SVC\n\nstart = time.time()\n\nclassifier = OneVsRestClassifier(SVC(kernel=\"linear\",verbose=1, decision_function_shape='ovr', probability=True))\nclassifier.fit(X_train, y_train)\ny_pred2 = classifier.predict_proba(X_test)\n\nend = time.time()\nprint(\"\\nTime taken: {:.2f} seconds\".format(end-start))","defd7b7f":"confusion_matrix = metrics.confusion_matrix(y_test.argmax(axis=1),y_pred2.argmax(axis=1))\nconfusion_matrix","65c17269":"auc_roc = metrics.classification_report(np.argmax(y_test, axis=1),np.argmax(y_pred2, axis=1))\nprint('SVC Classification Report:\\n {}'.format(auc_roc))","b94d573d":"plot_roc(y_test,y_pred2,\"ROC SVC\")","3e7422e1":"permutation_test_between_clfs(y_test, y_pred1, y_pred2, nsamples=1000)","28151396":"from sklearn.feature_selection import RFE\n\nstart = time.time()\n\nclassifier = OneVsRestClassifier(LogisticRegression(solver='sag',n_jobs=-1))\nrfe = RFE(classifier, n_features_to_select=64,verbose=1,step=1)\nrfe = rfe.fit(X_train, y_train_nobinary)\n\nend = time.time()\nprint(\"\\nTime taken: {:.2f} seconds\".format(end-start))","3869dfed":"# List of best features ranked by RFE algorithm\nfeatures = X.columns[rfe.support_]\nprint(features)\nX_train_rfe = pd.DataFrame(X_train)[features]\nX_test_rfe = pd.DataFrame(X_test)[features]","2d494e8a":"classifier = OneVsRestClassifier(LogisticRegression(solver='sag',n_jobs=-1))\nclassifier.fit(X_train_rfe, y_train)\ny_pred11 = classifier.predict_proba(X_test_rfe)","4eeed3e4":"confusion_matrix = metrics.confusion_matrix(y_test.argmax(axis=1),y_pred11.argmax(axis=1))\nconfusion_matrix","b01206c7":"auc_roc = metrics.classification_report(np.argmax(y_test, axis=1),np.argmax(y_pred11, axis=1))\nprint('Logistic regression with Recursive Feature Elimination:\\n {}'.format(auc_roc))","2ff32b6d":"plot_roc(y_test,y_pred11,'ROC for Logistic regression with Recursive Feature Elimination')","41f6d11a":"permutation_test_between_clfs(y_test, y_pred1, y_pred11, nsamples=1000)","6dea4438":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.feature_selection import chi2 ,SelectKBest\nnorm = MinMaxScaler()\n# Normalise training data \nX_train_norm = norm.fit_transform(X_train)","af0b5de3":"selector = SelectKBest(chi2, k=64)\nselector.fit(X_train_norm, y_train)\nX_train_kbest = selector.transform(X_train)\nX_test_kbest = selector.transform(X_test)","de7a4793":"classifier = OneVsRestClassifier(LogisticRegression(solver='sag',n_jobs=-1))\nclassifier.fit(X_train_kbest, y_train)\ny_pred12 = classifier.predict_proba(X_test_kbest)","35a2252f":"confusion_matrix = metrics.confusion_matrix(y_test.argmax(axis=1),y_pred12.argmax(axis=1))\nconfusion_matrix","841848c1":"auc_roc = metrics.classification_report(np.argmax(y_test, axis=1),np.argmax(y_pred12, axis=1))\nprint('Logistic regression with chi2 test feature selection:\\n {}'.format(auc_roc))","2dc2a150":"plot_roc(y_test,y_pred12,'ROC Logistic regression with chi2 test')","57b025aa":"permutation_test_between_clfs(y_test, y_pred1, y_pred12, nsamples=1000)","a78a8899":"classifier = OneVsRestClassifier(SVC(kernel=\"linear\", decision_function_shape='ovr'))\nrfe = RFE(classifier, n_features_to_select=64,verbose=1,step=1)\nrfe = rfe.fit(X_train, y_train_nobinary)","c167e1f0":"# List of best features ranked by RFE algorithm\nfeatures = pd.DataFrame(X_train).columns[rfe.support_]\nprint(features)\nX_train_rfe = pd.DataFrame(X_train)[features]\nX_test_rfe = pd.DataFrame(X_test)[features]","26c5529d":"classifier = OneVsRestClassifier(SVC(kernel=\"linear\",probability=True, verbose=1, decision_function_shape='ovr'))\nclassifier.fit(X_train_rfe, y_train)\ny_pred21 = classifier.predict_proba(X_test_rfe)","83e0c3e3":"confusion_matrix = metrics.confusion_matrix(np.argmax(y_test, axis=1),np.argmax(y_pred21, axis=1))\nconfusion_matrix","ce626f52":"auc_roc = metrics.classification_report(np.argmax(y_test, axis=1),np.argmax(y_pred21, axis=1))\nprint('SVC with Recursive Feature Elimination:\\n {}'.format(auc_roc))","51053b29":"plot_roc(y_test,y_pred21,'ROC for SVC with Recursive Feature Elimination')","6f477c48":"permutation_test_between_clfs(y_test, y_pred2, y_pred21, nsamples=1000)","c5742617":"classifier = OneVsRestClassifier(SVC(kernel=\"linear\",probability=True , verbose=1, decision_function_shape='ovr'))\nclassifier.fit(X_train_kbest, y_train)\ny_pred22 = classifier.predict_proba(X_test_kbest)","e89eab82":"confusion_matrix = metrics.confusion_matrix(np.argmax(y_test, axis=1),np.argmax(y_pred22, axis=1))\nconfusion_matrix","c024b75c":"auc_roc = metrics.classification_report(np.argmax(y_test, axis=1),np.argmax(y_pred22, axis=1))\nprint('SVC with chi2 test feature selection:\\n {}'.format(auc_roc))","043026b4":"plot_roc(y_test,y_pred22,'ROC for SVC with feature selection based on chi2 test')","2a66ab3d":"permutation_test_between_clfs(y_test, y_pred2, y_pred22, nsamples=1000)","893fce7a":"Gases in the dataset by number:\n\n1. Ethanol\n2. Ethylene\n3. Ammonia\n4. Acetaldehyde\n5. Acetone \n6. Toluene","a28dc9db":"Despite the highly saturated graphic, we can clearly see which attributes have little to no correlation with any other attributes (notice dark red vertical and horizontal lines). <br><br>\nWe can check which attributes have the highest correlation with the concentration level of the gas:","d2040a19":"### 4.5 Apply feature selection technique to algorithm 1 - Logistic regression with Chi Square test","574d5bbe":"# SVC vs SVC w\/RFE permutation test","43080b5d":"## 4. Principal Component Analysis","b0309512":"### 2.2 Data types","785228c9":"# Log Reg vs Log Reg w\/chi2 permutation test","5996a6a8":"### **Correlation analysis**","cc62adef":"### 4.2 Import and fit algorithm 1 - Logistic Regression","f86b602a":"ROC curve function","51edc172":"### **Bivariate analysis 2 - plotting gas concentration against only highly correlated attributes **","a62ee33c":"We see only numerical data in the dataset (comprised of 64 bit float and int dtypes). <br> \nAlso we see 128 non-categorical columns which validated the above, with the following charecteristics: <br>","b71ca6b0":"### 2.32 Concentrations","9093b171":"### 4.1 Scale data","8ce59a24":"#### Handling skew\nThe negative values in cocentration mean a log transformation of the concentration column is not possible, even when values are added to it. See below the graph after concentration is log transformed.","e1d42e8c":"### **Bivariate analysis - plotting gas concentration against attribute**","baf3c39e":"### 2.33 Other attributes","f699bb62":"# Big Data and Cloud Computing assignment 1\n\nThe features \nbefore applying the feature selection algorithm are indicated as FDj , where Di is the ith-dataset (D1 or D2) where it is applied. The features after applying a feature selection algorithm to the 1st or the 2nd\ndataset are indicated as follows: FSi,Dj where Si the feature selection algorithm and Di the dataset where it is applied.\n\n\n- [A1 with FD1] vs. [A2 with FD1] \n- [A1 with FD1] vs. [A1 with FS1,D1] \n- [A1 with FD1] vs. [A1 with FS2,D1]\n- [A2 with FD1] vs. [A2 with FS1,D1]\n- [A2 with FD1] vs. [A2 with FS2,D1]\n\n","7116fab2":"#### Some observations on concentrations:\n1. There is a positive skew in the concentration.\n1. There are negative values","a76e620e":"In order to decide which eigenvectors can dropped without losing too much information for the construction of lower-dimensional subspace, we need to inspect the corresponding eigenvalues: The eigenvectors with the lowest eigenvalues bear the least information about the distribution of the data; those are the ones can be dropped.","ae4eed9b":"##  4. Assessing Machine learning algorithms 1 and 2","5d50aa76":"### 4.3 Import and fit algorithm 2 - SVC","ceb74290":"# Log Reg vs Log Reg w\/RFE permutation test","496e4092":"# Log Reg vs SVC permutation test","8834b9f2":"### 4.4 Apply feature selection technique to algorithm 1 - Logistic regression with Recursive Feature Elimination","98f76314":"### 4.1 Splitting data into training and test sets","d8885296":"## 2. Exploratory data analysis\nWe will check data for: \n* Information on data types\n* Missing values\n* Correlations\n* Other observations (outliers etc.)\n\n### 2.1 Initial observartions","0eaad862":"### Scikit learn PCA","ff666b73":"## 3. Feature selection basic","ea6a6d70":"#### 4.21 Covariance Matrix","3cd4254b":"### 2.31 Gases","ef0a5fda":"We can see that the first 11 components consititute around 95% of the variance, therefore we can drop the remaining components. Literature varies when it comes to the maximum components to keep after PCA, some say keep each one until the variance ratio is 1, some say until 0.95.\n\nTaking 0.95 as the threshhold, we can reduce the dimensionality of the input by a factor of 12  while compromising only 5% of potential information.","c1792547":"Permutation function","df1a02e5":"We will use the macro average method to evaluate the algorithm.","28708c90":"### 4.5 Apply feature selection technique to algorithm 2 - SVC with Recursive Feature Elimination","fe5e0e76":"We see 13910 total instances with data from 129 attributes. <br>\nOne of the 129 attributes is our target that we want to predict. <br>\nTherefore there are 128 features that ay be used for feature engineering. <br>\nWe can also see that we kept our data integrity after concatenation of all datasets.","df62c377":"# SVC vs SVC w\/chi2 permutation test","5fb26435":"## 2.3 Exploring the numbers\n","dd06b0f9":"The maximum variance of about 53%  can be explained by the first principal component.","3fe5a4d1":"### 4.3 Selecting Principal Components","8f74e6ac":"### 4.5 Apply feature selection technique to algorithm 2 - SVC with chi2 test","7aefd432":"#### 4.22 Eigenvector and eigenvalue calulations\n","91892fdd":"https:\/\/towardsdatascience.com\/journey-to-the-center-of-multi-label-classification-384c40229bff <br>\nhttps:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_roc.html","1a6bed69":"### 4.2 Eigenvectors and Eigenvalues","a7d9bd5b":"##  1. Import modules and data\nWe combine all individual datasets into one, and further format the data so we can explore efficiently."}}