{"cell_type":{"29fc9156":"code","861a6574":"code","58f01689":"code","b6816f72":"code","aa19cff3":"code","a2c211ca":"code","9a122689":"code","eb79bc00":"code","981daf93":"code","4ef660be":"code","9a4fbd85":"code","540f5a46":"code","554bcc78":"code","6807d754":"code","6b647b83":"code","ff40ef3e":"code","17067f2e":"code","b4d1112c":"code","17257c90":"code","ab22d635":"code","2428b5e0":"code","293aae33":"code","0327f516":"code","202bcb49":"code","40358462":"code","ab1144d0":"code","9f259a18":"code","1e271a94":"code","87f66900":"code","d5cd859d":"code","3484928b":"code","65eba872":"code","48d58684":"code","a1343d4e":"code","db4de988":"code","0d650698":"code","c2e50edf":"code","cc06ab80":"code","f9520438":"code","25d83bc1":"code","5a896210":"code","387eccae":"code","3ce71b48":"code","287cf540":"code","c8adce2a":"code","360b5abd":"code","d701ea50":"code","a179b0f4":"code","a31c10b6":"code","2d9b4657":"code","c32d8626":"code","e1359d4a":"code","5f930bf9":"code","15ca6b04":"code","0ecd1db3":"code","c770a10f":"code","c985dfde":"code","c9a084bf":"code","2ed651f1":"code","827a3b5f":"code","f436513e":"code","67c1ef7a":"code","4a150331":"code","e6c03605":"code","b051b560":"code","ef1ff9ad":"code","12be08d8":"code","aefb366d":"code","f8fce537":"code","ef9da4b4":"code","e44d4ac4":"code","691757db":"code","c9f1f6d5":"code","a54b6c18":"code","4005ca13":"code","219f05e5":"code","eb04e66f":"code","9f39a327":"code","6a4270cf":"code","67830983":"code","c12afe3f":"code","8e16d30c":"code","5dbc29b0":"code","4eb11ea6":"code","e5dfdb85":"code","f52fea0b":"code","6e94c7d5":"code","7e67aafc":"code","3efde632":"code","e861dbfb":"markdown","da3c4da7":"markdown"},"source":{"29fc9156":"# \u30a4\u30f3\u30bf\u30cd\u30c3\u30c8\u3092On\u306b\u306a\u308b\n!pip install py7zr","861a6574":"import os\nfrom datetime import date, timedelta\nimport gc\nimport pandas as pd\nimport numpy as np\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\n\nprint(os.listdir(\"..\/input\"))","58f01689":"print(pd.__version__)\nprint(np.__version__)","b6816f72":"import py7zr\n\nfor f in ['test.csv.7z', 'items.csv.7z', 'stores.csv.7z', 'train.csv.7z']:\n    zf = py7zr.SevenZipFile(os.path.join('..', 'input', f), mode='r')\n    zf.extractall('..\/working')","aa19cff3":"print(os.listdir('..\/working\/'))","a2c211ca":"df_train = pd.read_csv(\n    '..\/working\/train.csv', usecols=[1, 2, 3, 4, 5],\n    dtype={'onpromotion': bool},\n    converters={'unit_sales': lambda u: np.log1p(\n        float(u)) if float(u) > 0 else 0},\n    parse_dates=[\"date\"],\n    skiprows=range(1, 66458909)  # from 2016-01-01\n)\n\ndf_test = pd.read_csv(\n    \"..\/working\/test.csv\", usecols=[0, 1, 2, 3, 4],\n    dtype={'onpromotion': bool},\n    parse_dates=[\"date\"]\n).set_index(\n    ['store_nbr', 'item_nbr', 'date']\n)\n\nitems = pd.read_csv(\n    \"..\/working\/items.csv\",\n).set_index(\"item_nbr\")\n\nstores = pd.read_csv(\n    \"..\/working\/stores.csv\",\n).set_index(\"store_nbr\")","9a122689":"df_train.head()","eb79bc00":"df_test.head()","981daf93":"items.head()","4ef660be":"stores.head()","9a4fbd85":"le = LabelEncoder()\nitems['family'] = le.fit_transform(items['family'].values)\nstores['city'] = le.fit_transform(stores['city'].values)\nstores['state'] = le.fit_transform(stores['state'].values)\nstores['type'] = le.fit_transform(stores['type'].values)","540f5a46":"items.head()","554bcc78":"stores.head()","6807d754":"df_2017 = df_train.loc[df_train.date>=pd.datetime(2017,1,1)]\ndel df_train","6b647b83":"df_2017.head()","ff40ef3e":"df_2017.shape","17067f2e":"promo_2017_train = df_2017.set_index([\"store_nbr\", \"item_nbr\", \"date\"])[[\"onpromotion\"]].unstack(level=-1).fillna(False)\npromo_2017_train.shape","b4d1112c":"promo_2017_train.head()","17257c90":"promo_2017_train.columns.get_level_values(1)","ab22d635":"promo_2017_train.columns = promo_2017_train.columns.get_level_values(1)\npromo_2017_train.head()","2428b5e0":"promo_2017_test = df_test[[\"onpromotion\"]].unstack(level=-1).fillna(False)\npromo_2017_test.columns = promo_2017_test.columns.get_level_values(1)\npromo_2017_test.shape","293aae33":"promo_2017_test.head()","0327f516":"# align test (store, item) index as same as train index.\n# some (store, item) will be missing.\npromo_2017_test = promo_2017_test.reindex(promo_2017_train.index).fillna(False)\npromo_2017_test.shape","202bcb49":"promo_2017_test.head()","40358462":"promo_2017 = pd.concat([promo_2017_train, promo_2017_test], axis=1)\ndel promo_2017_test, promo_2017_train\npromo_2017.shape","ab1144d0":"promo_2017.head()","9f259a18":"df_2017 = df_2017.set_index([\"store_nbr\", \"item_nbr\", \"date\"])[[\"unit_sales\"]].unstack(level=-1).fillna(0)\ndf_2017.shape","1e271a94":"df_2017.head()","87f66900":"df_2017.columns = df_2017.columns.get_level_values(1)\ndf_2017.shape","d5cd859d":"df_2017.head()","3484928b":"items.shape, stores.shape","65eba872":"items.shape, stores.shape","48d58684":"# align items\nitems = items.reindex(df_2017.index.get_level_values(1))\nstores = stores.reindex(df_2017.index.get_level_values(0))\nitems.shape, stores.shape","a1343d4e":"items.head()","db4de988":"stores.head()","0d650698":"df_2017_item = df_2017.groupby('item_nbr')[df_2017.columns].sum()\ndf_2017_item.shape","c2e50edf":"df_2017_item.head()","cc06ab80":"promo_2017_item = promo_2017.groupby('item_nbr')[promo_2017.columns].sum()\npromo_2017_item.shape","f9520438":"promo_2017_item.head()","25d83bc1":"df_2017_store_class = df_2017.reset_index()\ndf_2017_store_class.shape","5a896210":"df_2017_store_class.head()","387eccae":"# df and items now have same row index.\ndf_2017_store_class['class'] = items['class'].values\ndf_2017_store_class.head()","3ce71b48":"df_2017_store_class_index = df_2017_store_class[['class', 'store_nbr']]\ndf_2017_store_class_index.shape","287cf540":"df_2017_store_class_index.head()","c8adce2a":"df_2017_store_class = df_2017_store_class.groupby(['class', 'store_nbr'])[df_2017.columns].sum()\ndf_2017_store_class.shape","360b5abd":"df_2017_store_class.head()","d701ea50":"df_2017_promo_store_class = promo_2017.reset_index()\ndf_2017_promo_store_class.shape","a179b0f4":"df_2017_promo_store_class.head()","a31c10b6":"df_2017_promo_store_class['class'] = items['class'].values\ndf_2017_promo_store_class.head()","2d9b4657":"df_2017_promo_store_class_index = df_2017_promo_store_class[['class', 'store_nbr']]\ndf_2017_promo_store_class = df_2017_promo_store_class.groupby(['class', 'store_nbr'])[promo_2017.columns].sum()\ndf_2017_promo_store_class.shape","c32d8626":"df_2017_promo_store_class.head()","e1359d4a":"def get_timespan(df, dt, minus, periods, freq='D'):\n    \"\"\" Back minus days, get n==periods dates, each period is freq (D) away.\n        if dt=6\/16, minus=5, period=3, fred=D\n        then return 6\/11, 6\/12, 6\/13.\n    \"\"\"\n    return df[pd.date_range(dt - timedelta(days=minus), periods=periods, freq=freq)]","5f930bf9":"t2017 = date(2017, 6, 14)\npd.date_range(t2017, periods=16)","15ca6b04":"dt = t2017 = date(2017, 6, 14)\nminus = 5\nperiods = 3\nfreq = \"D\"\npd.date_range(dt - timedelta(days=minus), periods=periods, freq=freq)","0ecd1db3":"print(\"Preparing dataset...\")\nt2017 = date(2017, 6, 14)\nnum_days = 6\nX_l, y_l = [], []\nfor i in range(num_days):\n    delta = timedelta(days=7 * i)\n    print(t2017, delta, t2017 + delta)","c770a10f":"def prepare_dataset(df, promo_df, t2017, is_train=True, name_prefix=None):\n    \"\"\"\n    args:\n    ----\n        df: sale data\n        promo_df: promo data\n        t2017: pivot date\n    \"\"\"\n    \n    # Promotion counts.\n    # How many promotions in last 14 days.\n    # How many promotions in next 3 days. \n    X = {\n        \"promo_14_2017\": get_timespan(promo_df, t2017, 14, 14).sum(axis=1).values,\n        \"promo_3_2017_aft\": get_timespan(promo_df, t2017 + timedelta(days=16), 15, 3).sum(axis=1).values, \n        \"promo_7_2017_aft\": get_timespan(promo_df, t2017 + timedelta(days=16), 15, 7).sum(axis=1).values,\n        \"promo_14_2017_aft\": get_timespan(promo_df, t2017 + timedelta(days=16), 15, 14).sum(axis=1).values,\n    }\n\n    # Sale on promotion and Non-promotion days.\n    for i in [3, 7, 14]:\n        # get sale in last i days.\n        # get promo flag in last i days. if date has promo, value is 1. else, value is 0.\n        tmp1 = get_timespan(df, t2017, i, i)\n        tmp2 = (get_timespan(promo_df, t2017, i, i) > 0) * 1\n\n        # average sale on promo dates in last i=3, 7, ... days. if last 3 days, 2 has promo, then average sale of these 2 promo dates.\n        X['has_promo_mean_%s' % i] = (tmp1 * tmp2.replace(0, np.nan)).mean(axis=1).values\n        # most recent dates has more influences on average sale.\n        # if i == 3, np.power(0.9, np.arange(i)[::-1]) == [0.81, 0.9 , 1.  ]\n        X['has_promo_mean_%s_decay' % i] = (tmp1 * tmp2.replace(0, np.nan) * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\n        # average sale on NON-promo dates in last i=3, 7, ... days. if last 3 days, 2 has NO-promo, then average sale of these 2 dates.\n        X['no_promo_mean_%s' % i] = (tmp1 * (1 - tmp2).replace(0, np.nan)).mean(axis=1).values\n        # most recent dates has more influences on average sale.\n        # if i == 3, np.power(0.9, np.arange(i)[::-1]) == [0.81, 0.9 , 1.  ]\n        X['no_promo_mean_%s_decay' % i] = (tmp1 * (1 - tmp2).replace(0, np.nan) * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\n\n        \n    # Stats on sale values.\n    for i in [3, 7, 14]:\n        # Get sale in last i days.\n        tmp = get_timespan(df, t2017, i, i)\n        # descritive stats on sale.\n        X['mean_%s' % i] = tmp.mean(axis=1).values\n        X['median_%s' % i] = tmp.median(axis=1).values\n        X['min_%s' % i] = tmp.min(axis=1).values\n        X['max_%s' % i] = tmp.max(axis=1).values\n        X['std_%s' % i] = tmp.std(axis=1).values\n        # weighted mean, most recent contribute more on average.\n        X['mean_%s_decay' % i] = (tmp * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\n        # get diff of day_T vs. day_T-1 > get mean.\n        X['diff_%s_mean' % i] = tmp.diff(axis=1).mean(axis=1).values\n\n\n    # using same stats on sale, but now shift back 1 more week.\n    for i in [3, 7, 14]:\n        tmp = get_timespan(df, t2017 + timedelta(days=-7), i, i)\n        X['diff_%s_mean_2' % i] = tmp.diff(axis=1).mean(axis=1).values\n        X['mean_%s_decay_2' % i] = (tmp * np.power(0.9, np.arange(i)[::-1])).sum(axis=1).values\n        X['mean_%s_2' % i] = tmp.mean(axis=1).values\n        X['median_%s_2' % i] = tmp.median(axis=1).values\n        X['min_%s_2' % i] = tmp.min(axis=1).values\n        X['max_%s_2' % i] = tmp.max(axis=1).values\n        X['std_%s_2' % i] = tmp.std(axis=1).values\n\n\n    for i in [7, 14]:\n        # sale in last 3 days.\n        tmp = get_timespan(df, t2017, i, i)\n        # how many days has sales in last 3 day\n        X['has_sales_days_in_last_%s' % i] = (tmp > 0).sum(axis=1).values\n        # distance to last day has sale. if i == 3, sales = [0, 4, 3], then distance = 1\n        # if sales = [0, 4, 0], then distance = 2, etc...\n        X['last_has_sales_day_in_last_%s' % i] = i - ((tmp > 0) * np.arange(i)).max(axis=1).values\n        # distance to first day has sale.\n        X['first_has_sales_day_in_last_%s' % i] = ((tmp > 0) * np.arange(i, 0, -1)).max(axis=1).values\n\n        # promo in last 3 days\n        tmp = get_timespan(promo_df, t2017, i, i)\n        # how many days has promo\n        X['has_promo_days_in_last_%s' % i] = (tmp > 0).sum(axis=1).values\n        # distance to last day has promo\n        X['last_has_promo_day_in_last_%s' % i] = i - ((tmp > 0) * np.arange(i)).max(axis=1).values\n        # distance to first day has promo\n        X['first_has_promo_day_in_last_%s' % i] = ((tmp > 0) * np.arange(i, 0, -1)).max(axis=1).values\n\n\n    # promo in next 16 days\n    tmp = get_timespan(promo_df, t2017 + timedelta(days=16), 15, 15)\n    # how many promo in next 16 days\n    X['has_promo_days_in_after_15_days'] = (tmp > 0).sum(axis=1).values\n    # distance to last day has promo\n    X['last_has_promo_day_in_after_15_days'] = i - ((tmp > 0) * np.arange(15)).max(axis=1).values\n    # distance to first day has promo\n    X['first_has_promo_day_in_after_15_days'] = ((tmp > 0) * np.arange(15, 0, -1)).max(axis=1).values\n\n    # get sale in day t-1, t-2, t-3, ...\n    for i in range(1, 7):\n        X['day_%s_2017' % i] = get_timespan(df, t2017, i, 1).values.ravel()\n\n    # get promo in date ... t-3, t-2, t-1, t, t1, t2, t3...\n    for i in range(-7, 7):\n        X[\"promo_{}\".format(i)] = promo_df[t2017 + timedelta(days=i)].values.astype(np.uint8)\n\n    # for each day of week, for example wednesday\n    # get previous 4\/20 wednesdays sales, then take average\n    for i in range(7):\n        X['mean_4_dow{}_2017'.format(i)] = get_timespan(df, t2017, 28-i, 4, freq='7D').mean(axis=1).values\n        X['mean_20_dow{}_2017'.format(i)] = get_timespan(df, t2017, 140-i, 20, freq='7D').mean(axis=1).values\n\n    X = pd.DataFrame(X)\n\n    if is_train:\n        y = df[\n            pd.date_range(t2017, periods=16)\n        ].values\n        return X, y\n    if name_prefix is not None:\n        X.columns = ['%s_%s' % (name_prefix, c) for c in X.columns]\n    return X","c985dfde":"tmp1 = get_timespan(df_2017, t2017, 3, 3).head()\ntmp1.head()","c9a084bf":"tmp2 = (get_timespan(promo_2017, t2017, 3, 3) > 0) * 1\ntmp2.head(50)","2ed651f1":"(tmp1 * tmp2.replace(0, np.nan)).mean(axis=1).head()","827a3b5f":"np.power(0.9, np.arange(3))","f436513e":"np.power(0.9, np.arange(3)[::-1])","67c1ef7a":"tmp = get_timespan(df_2017, t2017, 3, 3)\ntmp.head()","4a150331":"tmp.head().diff(axis=1)#.mean(axis=1)","e6c03605":"tmp.head().diff(axis=1).mean(axis=1)","b051b560":"tmp = get_timespan(df_2017, t2017, 3, 3)\ntmp.head()","ef1ff9ad":"(tmp > 0).sum(axis=1).head()","12be08d8":"((tmp > 0) * np.arange(3)).head()","aefb366d":"((tmp > 0) * np.arange(3)).head().max(axis=1)","f8fce537":"3 - ((tmp > 0) * np.arange(3)).head().max(axis=1)","ef9da4b4":"np.arange(3, 0, -1)","e44d4ac4":"((tmp > 0) * np.arange(3, 0, -1)).head()","691757db":"((tmp > 0) * np.arange(3, 0, -1)).head().max(axis=1)","c9f1f6d5":"t2017","a54b6c18":"get_timespan(df_2017, t2017, 28-0, 4, freq='7D').head()","4005ca13":"t2017 = date(2017, 6, 14)\nminus = 28\nperiods = 4\npd.date_range(t2017 - timedelta(days=minus), periods=periods, freq=\"7D\")","219f05e5":"df_2017.head()","eb04e66f":"promo_2017.head()","9f39a327":"print(\"Preparing dataset...\")\nt2017 = date(2017, 6, 14)\nnum_days = 5\nX_l, y_l = [], []\nfor i in range(num_days):\n    delta = timedelta(days=7 * i)\n    print(t2017, delta, t2017 + delta)\n    print(\"process sales...\")\n    X_tmp, y_tmp = prepare_dataset(df_2017, promo_2017, t2017 + delta)\n    print(X_tmp.shape, y_tmp.shape)\n\n    print(\"process items...\")\n    X_tmp2 = prepare_dataset(df_2017_item, promo_2017_item, t2017 + delta, is_train=False, name_prefix='item')\n    print(X_tmp2.shape)\n    X_tmp2.index = df_2017_item.index\n    print(X_tmp2.shape)\n    X_tmp2 = X_tmp2.reindex(df_2017.index.get_level_values(1)).reset_index(drop=True)\n    print(X_tmp2.shape)\n\n    print(\"process store...\")\n    X_tmp3 = prepare_dataset(df_2017_store_class, df_2017_promo_store_class, t2017 + delta, is_train=False, name_prefix='store_class')\n    print(X_tmp3.shape)\n    X_tmp3.index = df_2017_store_class.index\n    print(X_tmp3.shape)\n    X_tmp3 = X_tmp3.reindex(df_2017_store_class_index).reset_index(drop=True)\n    print(X_tmp3.shape)\n\n    print(\"append sale, item, store, item-context, store-context data.\")\n    X_tmp = pd.concat([X_tmp, X_tmp2, X_tmp3, items.reset_index(), stores.reset_index()], axis=1)\n    X_tmp['date'] = t2017 + delta\n    print(X_tmp.shape)\n          \n    X_l.append(X_tmp)\n    y_l.append(y_tmp)\n\n    del X_tmp2\n    gc.collect()\n    print(\"-\" * 30)","6a4270cf":"X_train = pd.concat(X_l, axis=0)\ny_train = np.concatenate(y_l, axis=0)\ndel X_l, y_l\nX_train.shape, y_train.shape","67830983":"X_train.head()","c12afe3f":"y_train[:3]","8e16d30c":"os.remove('..\/working\/stores.csv')","5dbc29b0":"import pickle\nwith open(os.path.join('..', 'working', 'X_train.pkl'), 'wb') as f:\n    pickle.dump(X_train, f)\n    ","4eb11ea6":"import pickle\nwith open(os.path.join('..', 'working', 'y_train.pkl'), 'wb') as f:\n    pickle.dump(y_train, f)\n    ","e5dfdb85":"X_val, y_val = prepare_dataset(df_2017, promo_2017, date(2017, 7, 26))\n\nX_val2 = prepare_dataset(df_2017_item, promo_2017_item, date(2017, 7, 26), is_train=False, name_prefix='item')\nX_val2.index = df_2017_item.index\nX_val2 = X_val2.reindex(df_2017.index.get_level_values(1)).reset_index(drop=True)\n\nX_val3 = prepare_dataset(df_2017_store_class, df_2017_promo_store_class, date(2017, 7, 26), is_train=False, name_prefix='store_class')\nX_val3.index = df_2017_store_class.index\nX_val3 = X_val3.reindex(df_2017_store_class_index).reset_index(drop=True)\n\nX_val = pd.concat([X_val, X_val2, X_val3, items.reset_index(), stores.reset_index()], axis=1)\nX_val['date'] = date(2017, 7, 26)\nX_val.shape, y_val.shape","f52fea0b":"with open(os.path.join('..', 'working', 'X_val.pkl'), 'wb') as f:\n    pickle.dump(X_val, f)","6e94c7d5":"with open(os.path.join('..', 'working', 'y_val.pkl'), 'wb') as f:\n    pickle.dump(y_val, f)","7e67aafc":"X_test = prepare_dataset(df_2017, promo_2017, date(2017, 8, 16), is_train=False)\n\nX_test2 = prepare_dataset(df_2017_item, promo_2017_item, date(2017, 8, 16), is_train=False, name_prefix='item')\nX_test2.index = df_2017_item.index\nX_test2 = X_test2.reindex(df_2017.index.get_level_values(1)).reset_index(drop=True)\n\nX_test3 = prepare_dataset(df_2017_store_class, df_2017_promo_store_class, date(2017, 8, 16), is_train=False, name_prefix='store_class')\nX_test3.index = df_2017_store_class.index\nX_test3 = X_test3.reindex(df_2017_store_class_index).reset_index(drop=True)\n\nX_test = pd.concat([X_test, X_test2, X_test3, items.reset_index(), stores.reset_index()], axis=1)\nX_test.shape","3efde632":"del X_test2, X_val2, df_2017_item, promo_2017_item, df_2017_store_class, df_2017_promo_store_class, df_2017_store_class_index\ngc.collect()","e861dbfb":"# Import","da3c4da7":"# Interesting points\n\n* We have to predict the sale of an Item in a Store in 16 days windows. Last day in train data is 8\/15, using data to this cutoff date, we have to predict sale of Milk in Store A in each of next 16 dates (8\/16, 8\/17, ..., 8\/30, 8\/31). One can do 1-day-rolling-training (using data up to 8\/29 as train to predict 8\/30, using data up to 8\/30 as train to predict 8\/31), but we don't have real label in 16 dates window.\n* As with many Tabular data, there are many many features and feature engineering is the hardest part. And this is a TimeSeries feature which makes problem even harder.\n* Training data is really big (~125 million rows in almost 4 years). So the question is: do we really need to use all data? If not, how much?\n* I'm not familiar with TimeSeries data, so this is a huge opportunity to learn TimeSeries feature engineering.\n\n# Winning Solution\n\nThis is the simplified solution of [1st place solution](https:\/\/www.kaggle.com\/shixw125\/1st-place-lgb-model-public-0-506-private-0-511). Lets dive in.\n\nValidation strategy:\n\n> train data\uff1a20170531 - 20170719 or 20170614 - 20170719, different models are trained with different data set. validition: 20170726 - 20170810\n\nIdea:\n\nBuild 16 models, each using same training data but predict for different day in future. For example: model 1 trained on X then predicts for t+1. Model 2 trained on X then predicts for t+2, etc...\n\nFeature Engineering: The most cool part in winning solution. And this notebook tries to break down the feature engineering part."}}