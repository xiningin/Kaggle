{"cell_type":{"9c5d99e0":"code","2a7d5640":"code","efe0b90e":"code","1122a9bc":"code","c2f36b67":"code","14b9927d":"code","f838c86c":"code","0484c283":"code","03aab123":"code","f47bb06a":"code","2f89472d":"code","1a30c800":"code","e57d5fd6":"code","d932ff91":"code","6834fffd":"code","f30dd731":"code","23929513":"code","6abddc6f":"code","398e3ba6":"code","fa0a372f":"code","b61233dd":"code","8a99940a":"code","df4fc239":"markdown","08436e24":"markdown","ab237ae9":"markdown","def8ba70":"markdown","cbb7f179":"markdown","04ed2625":"markdown","27882adf":"markdown","793a18fa":"markdown","c9021887":"markdown","b5dcf4f8":"markdown","ac17dafb":"markdown","dc630f44":"markdown","0fc14d51":"markdown","98ba1bd1":"markdown"},"source":{"9c5d99e0":"# import libraries that we  might need\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom nltk.corpus import stopwords\nimport nltk\nimport re\n\nfrom keras.preprocessing.text import one_hot\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nfrom keras.layers.core import Activation, Dropout, Dense\nfrom keras.layers import Flatten\nfrom keras import layers\nfrom keras.layers import GlobalMaxPooling1D\nfrom keras.layers.embeddings import Embedding\nfrom sklearn.model_selection import train_test_split\nfrom keras.preprocessing.text import Tokenizer","2a7d5640":"# Load the data set from csv file\ndf = pd.read_csv('..\/input\/IMDB Dataset.csv')\nprint('Shape of  Data is {} rows  and {} columns'.format(df.shape[0],df.shape[1]))","efe0b90e":"print('A Sample Input & Output')\nprint('-----------------------------------------------------------')\nprint(df['review'][5])\nprint('-----------------------------------------------------------')\nprint(df['sentiment'][5])","1122a9bc":"# We need to preprocess the text to remove the punctuations and tags and any other junk characters.\ndef preprocessing(sen):\n    #Remove Tags\n    # Remove any html tags that might have\n    sen = remove_tags(sen)\n    \n    sen = sen.lower()\n    \n    #  we only want to keep text and not words or punctuations.\n    sen = re.sub(r'[^a-zA-Z]',' ',sen)\n    \n    # We will remove any single characters\n    sen = re.sub(r'\\s+',' ',sen)\n    \n    #  We will remove any extra spaces we have added\n    sen = re.sub(r'\\s+[a-zA-Z]\\s+',' ',sen)\n    \n    return sen\n\ntags = re.compile(r'<[^>]+>')\ndef remove_tags(sen):\n    return tags.sub('',sen)","c2f36b67":"#  lets  pre-process the the data and put it  into our Input variables\nX = df['review'].apply(preprocessing)\ny = df['sentiment'].map({'positive':1,'negative':0})","14b9927d":"#  Lets look at the processed data\nprint('--------------------------------------------')\nX[5]","f838c86c":"# We will split the data into two sets, train data set would later be split to be used for validation as well.\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)","0484c283":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(X_train)\n\nX_train = tokenizer.texts_to_sequences(X_train)\nX_test = tokenizer.texts_to_sequences(X_test)","03aab123":"vocab_size = len(tokenizer.word_index) + 1\nmaxlen = 100\n\nX_train = pad_sequences(X_train,padding='post',maxlen=maxlen)\nX_test = pad_sequences(X_test,padding='post',maxlen=maxlen)","f47bb06a":"vocab_size","2f89472d":"glove_file = open('..\/input\/glove.6B.100d.txt',encoding='utf-8')\nword_embedding = dict()\n\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    word_embedding[word] = np.asarray(records[1:],dtype='float32')","1a30c800":"len(word_embedding)","e57d5fd6":"glove_file.close()","d932ff91":"word_matrix = np.zeros((vocab_size,100))\nfor word,index in tokenizer.word_index.items():\n    embed_vector = word_embedding.get(word)\n    if  embed_vector is not None:\n        word_matrix[index] = embed_vector","6834fffd":"import matplotlib.pyplot as plt\nimport seaborn as sns\n\ndef performance_plot(data):\n    fig,ax = plt.subplots(figsize=(16,5),ncols=2,nrows=1)\n    val_loss = data['val_loss']\n    val_acc = data['val_acc']\n    loss = data['loss']\n    acc = data['acc']\n    epochs = range(1,len(acc)+1)\n\n    sns.lineplot(epochs,val_loss,ax=ax[0],label='Validation Loss')\n    sns.lineplot(epochs,loss,ax=ax[0],label='Training Loss')\n\n    sns.lineplot(epochs,val_acc,ax=ax[1],label='Validation acc')\n    sns.lineplot(epochs,acc,ax=ax[1],label='Training acc')","f30dd731":"model = Sequential()\nembedding_layer = Embedding(vocab_size, 100, weights=[word_matrix], input_length=maxlen , trainable=False)\nmodel.add(embedding_layer)\nmodel.add(layers.LSTM(128))\nmodel.add(layers.Dense(1,activation='sigmoid'))\nmodel.compile(optimizer='adam', loss='binary_crossentropy', metrics=['acc'])\nprint(model.summary())","23929513":"history = model.fit(X_train, y_train, batch_size=128, epochs=6, verbose=1, validation_split=0.2)","6abddc6f":"performance_plot(history.history)","398e3ba6":"model.evaluate(X_test,y_test)","fa0a372f":"def sentiment_prediction(instance):\n    instance = tokenizer.texts_to_sequences(instance)\n    instance = pad_sequences(instance, padding='post', maxlen=maxlen)\n    prob = model.predict(instance)\n    if prob < 0.5: return 'Negative sentiment'\n    else: return 'Positive sentiment'","b61233dd":"review = \"You probably already know the answer to the above question, but writer-director Sujeeth takes you through a maze of twists and turns before he gets there \u2013 none of them engaging. Pegged as India\u2019s biggest action thriller, Saaho gets into the action mode pretty early on. The film begins with signature wide angles of massive structures and grim looking men who mean serious business. All through the first half, the film travels through cities trying to connect high stake robberies in Mumbai and the search for a missing black box that holds the key to a fortune. But by the time the \u2018interval bang\u2019 rolls around, you kind of already know where this is heading, thanks to on-the-nose dialogues. Then there\u2019s Prabhas, with an entry so subtle it quickly takes a turn, leading to a loud, high-octane fight scene that sets the stage for many more such confrontations. While he does fit the bill perfectly for the larger-than-life role, his dialogue delivery is deliberately slow, almost like a drawl, and doesn\u2019t always work. His one-liners and humour falls flat, none of the jokes somehow land. However, the way his character unfolds does keep the viewer guessing. Shraddha Kapoor looks glamorous but delivers a lifeless and expressionless performance for a character that\u2019s poorly sketched to begin with. Introduced as a tough-talking cop, it doesn\u2019t take long before she\u2019s turned into a damsel-in-distress who often needs saving. She always seems to be the last one to know what\u2019s up. Even the chemistry between the lead pair is a touch and go with even the hyped up \u2018romantic fight scene\u2019 not working completely. Among the many villains, Chunkey Pandey as Devraj stands out with a very convincing portrayal of his character. He oozes menace and seethes with anger, if only he had gotten better lines to match that acting prowess. The rest, despite being stupendous actors, somehow come off as mere caricatures who fail to make an impact. The way Mandira Bedi\u2019s character develops is laughable. The songs are so oddly placed in the narrative; they only manage to add on to an already long runtime and add to a choppy narrative. Saaho surely delivers well as an action extravaganza with a climax that attempts to compensate for its many flaws. The film\u2019s second half picks up pace, but is marred by a weak narrative that needs constant suspension of disbelief. The hyped up special effects and CGI too lack finesse for a film mounted on such a grand scale. Sujeeth attempts at a potboiler that fires in any and all directions to entertain the audience. You can also see how the story, despite being predictable, might have sounded good on paper, but the many twists and turns just leave you exhausted than excited. Saaho is an attempt at reinventing a story as old as time, if only the numerous \u2018bangs\u2019 managed to land.\"\nreview","8a99940a":"review = preprocessing(review)\nreview = pd.Series(review)\nsentiment_prediction(review)","df4fc239":"### Word Embedding\n\n#### Its a entire topic on its own,\n\nIn simple words, these are pre trained vectors, which define the relationship betwween words.\nFor more, please use google, or there is a excellent lecture series by Standford on youtube you can checkout.","08436e24":"## Awesome!!!","ab237ae9":"# Sentiment Analysis - IMDB Dataset","def8ba70":"# Results\n\nWe get a accuracy of 85% (approx), which is really good, considering how difficult it is to understand human langauge model, even for us humans.","cbb7f179":"Through this notebook, we  will try to implement and understand basics of a sequential deep learning model and the data prepration involved in it.\n\nWe will be using a kaggle data set of imdb movie reviews and build a model which can learn to predict the sentiment of the review.","04ed2625":"## Deep Learning with text data","27882adf":"The notebook is in parts insipired by <b>Fran\u00e7ois Chollet's<\/b> book on deep learning with python and from below article,\nhttps:\/\/stackabuse.com\/python-for-nlp-movie-sentiment-analysis-using-deep-learning-in-keras\/\n","793a18fa":"### Tokenization\n-----------------------------------------------------------------\nTokenization is a NLP based technique to convert the text data to numerical format inorder to be processed by mathematical models that we will be building.\n\nIn the process that we will be using below, we would use the count vectorization approch, where every word is give a index value based on it frequency of occuernece,threfor lower the value, higher the frequency of the word in the text.\n\nWe could also use one-hot encoding apprroch, but that would create a huge matrix.","c9021887":"### Network Modelling\n\nHere we are using a simple Sequential LSTM network for our training,\n\nFirst Layer : Embedding layer is most used for NLP modelling, as you can see, we have provided pre trained weights and have assigned that this layer does not need training, which reduces our work, as in first layer, we will need to learn relations between words, which we can get from glove embedding which we have downloaded.\n\nSecond Layer : Is a Long Short Term Memory layer, which is very useful in learning time series data, but also has proven useful over text data.\n\nThird Layer : Is the output layer, with a sigmoid activation function which gives a probability value for result, if probability is greater than 50% we predict 1 (Positive) or else 0 (Negative)\n\nCompilation : We need to define parameters for our model training, we will be using binary crossentropy as loss function which will be minimized and accuracy will be maximized during training.","b5dcf4f8":"## Hope you enjoy reading, as much as I have enjoyed writing this","ac17dafb":"## Upvote's and Comments are always welcome","dc630f44":"## The Fun\n\nThe entiire exercise would be useless if we can not run the model on our own reviews and see the results.","0fc14d51":"## So this is it, do leave a like, or comment if you enjoyed the work, thanks for your time.","98ba1bd1":"### Padding\n\nNow that we have completed the tokenization process, we will have lists of all the sentences, but as as all have different length, our data has varying number of lengths, we will now define the maximum length we want our each review to have and if needed will perform padding at the end of each list."}}