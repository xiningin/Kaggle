{"cell_type":{"f9a9c4f2":"code","8fee68f5":"code","42713473":"code","fc424797":"code","7b68097e":"code","c5232e1f":"code","977268ea":"code","dea22fbe":"code","42e9c809":"code","76f82110":"code","8e3df993":"code","1dd970d7":"code","6ddf8ed8":"code","7d3ab55a":"code","9d5f6966":"code","4370fae4":"code","54af0193":"code","84bdc59c":"code","a6738f49":"code","c0c85edc":"code","7b56807a":"code","a09d84e0":"code","45f93d4e":"code","9ac4c09e":"code","3fdfed9c":"code","594ac8b2":"code","15aa9e1b":"code","6e68dd6f":"code","ae1eea05":"code","7c9ecb20":"code","5d1230e9":"code","ad614941":"code","41757395":"code","ca243ca2":"code","8b147bc4":"code","4f39c22d":"code","dc1051e4":"code","8e85aa89":"code","c44063e9":"code","e27722a6":"code","18820ff4":"code","f5f36bcd":"code","e7bd99f5":"code","41acaf5b":"code","b0e980f8":"code","faecdd89":"code","640f3e38":"code","6c08e72b":"code","bee732e1":"code","765fe5fc":"code","ec81ba87":"code","ed2198a5":"code","16fa910c":"code","f58122d5":"code","82267be5":"code","6b7c1f8f":"code","26b7fcf6":"code","bf520afd":"code","a14216e6":"code","80a4d90c":"code","b7aad5d5":"code","6491e3a3":"code","85af20f1":"code","f0ccc523":"code","50e32a32":"code","1898dc11":"code","b1a858a5":"code","0ed3e75f":"code","6b11bee8":"code","3c476d9c":"code","23daffd7":"code","30c2b1e0":"code","74d57c72":"code","4b286770":"code","cd2bfc78":"code","8a97fc8b":"code","7b29fc60":"code","54d88269":"code","fb91ebf5":"code","790b449f":"code","c01a5522":"code","d9d7ad65":"code","a8513a25":"code","691f837e":"code","696d1155":"code","fa758dd0":"code","9b0e507b":"code","eede03d1":"code","c2a25aea":"code","4d7c9228":"code","3f00ac77":"code","3d57a1d8":"code","f7e4c3a0":"code","16905daf":"code","b7fc213d":"code","98e546eb":"code","ab1405d3":"markdown","fe4f84f1":"markdown","07356a88":"markdown","d7e25552":"markdown","265ed3f8":"markdown","3464ee9e":"markdown","d63c9e98":"markdown","ed262bec":"markdown","ef643bea":"markdown","b063e9de":"markdown","b5f12a03":"markdown","15aeaf3a":"markdown","23900cf5":"markdown","6723dd55":"markdown","05adb38b":"markdown","f72aadf8":"markdown","11b95816":"markdown","22fbb9b9":"markdown","1f43a195":"markdown","044265d9":"markdown","78f4c369":"markdown","fca5addb":"markdown","7acafff3":"markdown","69f903f4":"markdown","b558dded":"markdown","5ee90f0d":"markdown","6a12f2ac":"markdown","3052aca0":"markdown","9f78e5d6":"markdown","d393ca6c":"markdown","107f30eb":"markdown","abbcf5b4":"markdown","94c8997f":"markdown","4d44a1c4":"markdown","92d6e3b8":"markdown","bc97514f":"markdown","4c98aebe":"markdown","b674035e":"markdown","4f86eafb":"markdown","4f9143ff":"markdown","4a65f3e6":"markdown","89342c9d":"markdown"},"source":{"f9a9c4f2":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom tqdm import tqdm\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport random","8fee68f5":"def fix_seed(seed):\n    # random\n    random.seed(seed)\n    # Numpy\n    np.random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n\nSEED = 42\nfix_seed(SEED)","42713473":"df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndf","fc424797":"test = pd.read_csv(\"..\/input\/titanic\/test.csv\")\ntest","7b68097e":"df.info()","c5232e1f":"df.columns","977268ea":"df.nunique()","dea22fbe":"from sklearn.preprocessing import LabelEncoder","42e9c809":"le=LabelEncoder()\n\nle.fit(df[\"Sex\"])\ndf[\"Sex\"] = le.transform(df[\"Sex\"])\ntest[\"Sex\"] = le.transform(test[\"Sex\"])","76f82110":"df[\"Embarked\"] = df[\"Embarked\"].fillna(\"NoData\")","8e3df993":"le2=LabelEncoder()\nle2.fit(df[\"Embarked\"])\ndf[\"Embarked\"] = le2.transform(df[\"Embarked\"])\ntest[\"Embarked\"] = le2.transform(test[\"Embarked\"])","1dd970d7":"train = df.copy()","6ddf8ed8":"from sklearn import preprocessing\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.model_selection import StratifiedKFold","7d3ab55a":"folds = train.copy()\nFold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\nfor n, (train_index, val_index) in enumerate(Fold.split(folds, folds[\"Survived\"])):\n    folds.loc[val_index, 'fold'] = int(n)\nfolds['fold'] = folds['fold'].astype(int)\nprint(folds.groupby(['fold', \"Survived\"]).size())","9d5f6966":"folds","4370fae4":"p_train = folds[folds[\"fold\"] != 0]\np_val = folds[folds[\"fold\"] == 0]","54af0193":"p_train","84bdc59c":"# An error will occur later, so reassign the index.\n# \u5f8c\u307b\u3069\u30a8\u30e9\u30fc\u304c\u51fa\u308b\u306e\u3067\u3001index\u3092\u632f\u308a\u306a\u304a\u3059\u3002\n\np_train = p_train.reset_index(drop=True)\np_val = p_val.reset_index(drop=True)","a6738f49":"p_train","c0c85edc":"import lightgbm as lgb","7b56807a":"fix_seed(SEED) # for repetability","a09d84e0":"# defining the feature columns and the target\n\nFEATURES = [\"Pclass\",\"Sex\",\"Age\",\"SibSp\",\"Parch\",\"Embarked\"]\nTARGET = \"Survived\"","45f93d4e":"p_train[FEATURES]","9ac4c09e":"lgb_train = lgb.Dataset(p_train[FEATURES], p_train[TARGET])\nlgb_eval = lgb.Dataset(p_val[FEATURES], p_val[TARGET])","3fdfed9c":"# example of parameters\nlgbm_params = {\n    'objective': 'binary', # Binary classification : 2\u5024\u5206\u985e\u3067\u306f\u3053\u308c\u3092\u4f7f\u3046\n    'seed': 42, # random seed : \u3053\u308c\u3092\u56fa\u5b9a\u3059\u308b\u3068\u3001\u518d\u73fe\u6027\u304c\u51fa\u308b\n    'metric': 'auc', \n#    'learning_rate': 0.01,\n#    'max_bin': 800, # depth\n#    'num_leaves': 80, # leaves,\n    \"verbose\":-1,\n    \"deterministic\":True\n}","594ac8b2":"train[FEATURES].head(3)","15aa9e1b":"cat_list = ['Embarked']\n\n\nmodel = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval,\n                  verbose_eval=50,  # Learning result output every 50 iterations : 50\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u6bce\u306b\u5b66\u7fd2\u7d50\u679c\u51fa\u529b\n                  num_boost_round=1000,  # Specify the maximum number of iterations : \u6700\u5927\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u56de\u6570\u6307\u5b9a\n                  early_stopping_rounds=100, # Early stopping number : early stopping\u3092\u63a1\u7528\u3059\u308biteration\u56de\u6570\n                  categorical_feature = cat_list # manual categorical feature setting\n                 \n                 )","6e68dd6f":"import pickle\n\nmodel_name = \"LGBMmodel.bin\"\n\n# saving model\npickle.dump(model, open(model_name, 'wb'))\n\n# loading model\nmodel = pickle.load(open(model_name, 'rb'))\n\n","ae1eea05":"import matplotlib.pyplot as plt","7c9ecb20":"# model.save_model(f'model.txt')\nlgb.plot_importance(model, importance_type='gain')\nplt.show()","5d1230e9":"# predicting validation value\noof_pred = model.predict(p_val[FEATURES], num_iteration=model.best_iteration)","ad614941":"oof_pred[:3]","41757395":"# Calculating AUC (Area Under the Curve)\nfrom sklearn import metrics\nfpr, tpr, thresholds = metrics.roc_curve(p_val[TARGET], oof_pred)\nauc = metrics.auc(fpr, tpr)\nprint(auc)\n\n# Ploting the ROC curve\nplt.plot(fpr, tpr, label='ROC curve (area = %.2f)'%auc)\nplt.legend()\nplt.title('ROC curve')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.grid(True)","ca243ca2":"oof_pred","8b147bc4":"oof_pred2 = np.where(oof_pred>=0.5,1,0)","4f39c22d":"accuracy_score(p_val[TARGET], oof_pred2)","dc1051e4":"model = pickle.load(open(model_name, 'rb'))","8e85aa89":"# confirming submission file\n\nsubmission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\nsubmission","c44063e9":"test","e27722a6":"test_X = test[FEATURES]\ntest_X ","18820ff4":"# predicting for test_X\npreds = model.predict(test_X[FEATURES])","f5f36bcd":"preds2 = np.where(preds>=0.5,1,0)","e7bd99f5":"preds2[:3]","41acaf5b":"submission","b0e980f8":"submission[\"Survived\"] = preds2","faecdd89":"submission.to_csv(\"submission1.csv\",index = False)","640f3e38":"fix_seed(SEED) # for repetability\n\np_train = folds[folds[\"fold\"] != 0]\np_val = folds[folds[\"fold\"] == 0]\n\np_train = p_train.reset_index(drop=True)\np_val = p_val.reset_index(drop=True)\n\nlgb_train = lgb.Dataset(p_train[FEATURES], p_train[TARGET])\nlgb_eval = lgb.Dataset(p_val[FEATURES], p_val[TARGET])\n\ncat_list = ['Embarked']\n\n\nmodel = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval,\n                  verbose_eval=50,  # Learning result output every 50 iterations : 50\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u6bce\u306b\u5b66\u7fd2\u7d50\u679c\u51fa\u529b\n                  num_boost_round=1000,  # Specify the maximum number of iterations : \u6700\u5927\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u56de\u6570\u6307\u5b9a\n                  early_stopping_rounds=100, # Early stopping number : early stopping\u3092\u63a1\u7528\u3059\u308biteration\u56de\u6570\n                  categorical_feature = cat_list, # manual categorical feature setting\n                 \n                 )\n\nimport pickle\n\nmodel_name = \"LGBMmodel.bin\"\n\n# saving model\npickle.dump(model, open(model_name, 'wb'))\n\n# loading model\nmodel = pickle.load(open(model_name, 'rb'))\n\n# predicting validation value\noof_pred = model.predict(p_val[FEATURES], num_iteration=model.best_iteration)\n\noof_pred2 = np.where(oof_pred>=0.5,1,0)\n\naccuracy_score(p_val[TARGET], oof_pred2)\n\n# predicting for test_X\npreds = model.predict(test_X[FEATURES])\n\npreds2 = np.where(preds>=0.5,1,0)","6c08e72b":"scores = []\nallpreds = []\n\nallvaliddf = pd.DataFrame()\n\n\nfor fold in range(5):\n    \n    fix_seed(SEED) # for repetability\n\n    p_train = folds[folds[\"fold\"] != fold]\n    p_val = folds[folds[\"fold\"] == fold]\n\n    p_train = p_train.reset_index(drop=True)\n    p_val = p_val.reset_index(drop=True)\n\n    lgb_train = lgb.Dataset(p_train[FEATURES], p_train[TARGET])\n    lgb_eval = lgb.Dataset(p_val[FEATURES], p_val[TARGET])\n\n    cat_list = ['Embarked']\n\n\n    model = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval,\n                      verbose_eval=50,  # Learning result output every 50 iterations : 50\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u6bce\u306b\u5b66\u7fd2\u7d50\u679c\u51fa\u529b\n                      num_boost_round=1000,  # Specify the maximum number of iterations : \u6700\u5927\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u56de\u6570\u6307\u5b9a\n                      early_stopping_rounds=100, # Early stopping number : early stopping\u3092\u63a1\u7528\u3059\u308biteration\u56de\u6570\n                      categorical_feature = cat_list, # manual categorical feature setting\n                      \n                     )\n\n    import pickle\n\n    model_name = f\"LGBMmodel{fold}.bin\"\n\n    # saving model\n    pickle.dump(model, open(model_name, 'wb'))\n\n    # loading model\n    model = pickle.load(open(model_name, 'rb'))\n\n    # predicting validation value\n    oof_pred = model.predict(p_val[FEATURES], num_iteration=model.best_iteration)\n\n    oof_pred2 = np.where(oof_pred>=0.5,1,0)\n\n    scores.append(accuracy_score(p_val[TARGET], oof_pred2))\n\n    # predicting for test_X\n    preds = model.predict(test_X[FEATURES])\n\n    #preds2 = np.where(preds>=0.5,1,0)\n    \n    allpreds.append(preds)\n    \n    # out of fold : oof\n    p_val[\"preds\"] = oof_pred2\n    \n    allvaliddf = pd.concat([allvaliddf,p_val])\n    \n    ","bee732e1":"scores","765fe5fc":"np.mean(scores)","ec81ba87":"allvaliddf","ed2198a5":"accuracy_score(allvaliddf[TARGET],allvaliddf[\"preds\"])","16fa910c":"allpreds","f58122d5":"allpreds = np.mean(allpreds,axis=0)","82267be5":"allpreds","6b7c1f8f":"allpreds2 = np.where(allpreds>=0.5,1,0)","26b7fcf6":"submission.head(3)","bf520afd":"submission[\"Survived\"] = allpreds2","a14216e6":"submission.to_csv(\"submission2.csv\",index=False)","80a4d90c":"!pip install -U optuna","b7aad5d5":"import optuna.integration.lightgbm as lgbo","6491e3a3":"lgbm_params = {\n    'objective': 'binary', # Binary classification : 2\u5024\u5206\u985e\u3067\u306f\u3053\u308c\u3092\u4f7f\u3046\n    \"seed\":42,\n    'metric': \"auc\",\n    \"verbose\":-1,\n\"deterministic\":True}\n\n    ","85af20f1":"fix_seed(SEED) # for repetability\n\np_train = folds[folds[\"fold\"] != 0]\np_val = folds[folds[\"fold\"] == 0]\n\np_train = p_train.reset_index(drop=True)\np_val = p_val.reset_index(drop=True)\n\nlgbo_train = lgbo.Dataset(p_train[FEATURES], p_train[TARGET])\nlgbo_eval = lgbo.Dataset(p_val[FEATURES], p_val[TARGET])\n\ncat_list = ['Embarked']\n\n# For repeatability, change this part\nbooster = lgbo.LightGBMTuner(\n    params = lgbm_params, \n    train_set = lgbo_train,\n    valid_sets=lgbo_eval,\n    optuna_seed=42, # new\n    verbose_eval=50,  # Learning result output every 50 iterations : 50\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u6bce\u306b\u5b66\u7fd2\u7d50\u679c\u51fa\u529b\n    num_boost_round=1000,  # Specify the maximum number of iterations : \u6700\u5927\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u56de\u6570\u6307\u5b9a\n    early_stopping_rounds=100, # Early stopping number : early stopping\u3092\u63a1\u7528\u3059\u308biteration\u56de\u6570\n    categorical_feature = cat_list, # manual categorical feature setting\n    )\n\nbooster.run()\n\n# get best model\nmodel = booster.get_best_booster()\n\nimport pickle\n\nmodel_name = \"lgboMmodel.bin\"\n\n# saving model\npickle.dump(model, open(model_name, 'wb'))\n\n# loading model\nmodel = pickle.load(open(model_name, 'rb'))\n\n# predicting validation value\noof_pred = model.predict(p_val[FEATURES], num_iteration=model.best_iteration)\n\noof_pred2 = np.where(oof_pred>=0.5,1,0)\n\naccuracy_score(p_val[TARGET], oof_pred2)\n\n# predicting for test_X\npreds = model.predict(test_X[FEATURES])\n\npreds2 = np.where(preds>=0.5,1,0)","f0ccc523":"accuracy_score(p_val[TARGET], oof_pred2)","50e32a32":"submission[\"Survived\"] = preds2","1898dc11":"submission.to_csv(\"submission3.csv\",index=False)","b1a858a5":"scores = []\nallpreds = []\n\nmetric_scores = []\n\nallvaliddf = pd.DataFrame()\n\n\nfor fold in range(5):\n    \n    fix_seed(SEED) # for repetability\n\n    p_train = folds[folds[\"fold\"] != fold]\n    p_val = folds[folds[\"fold\"] == fold]\n\n    p_train = p_train.reset_index(drop=True)\n    p_val = p_val.reset_index(drop=True)\n\n    lgbo_train = lgbo.Dataset(p_train[FEATURES], p_train[TARGET])\n    lgbo_eval = lgbo.Dataset(p_val[FEATURES], p_val[TARGET])\n\n    cat_list = ['Embarked']\n\n    \"\"\"\n    model = lgbo.train(lgbm_params, lgbo_train, valid_sets=lgbo_eval,\n                      verbose_eval=50,  # Learning result output every 50 iterations : 50\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u6bce\u306b\u5b66\u7fd2\u7d50\u679c\u51fa\u529b\n                      num_boost_round=1000,  # Specify the maximum number of iterations : \u6700\u5927\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u56de\u6570\u6307\u5b9a\n                      early_stopping_rounds=100, # Early stopping number : early stopping\u3092\u63a1\u7528\u3059\u308biteration\u56de\u6570\n                      categorical_feature = cat_list,# manual categorical feature setting\n                      \n                     )\n    \"\"\"\n    \n    # For repeatability, change this part\n    booster = lgbo.LightGBMTuner(\n        params = lgbm_params, \n        train_set = lgbo_train,\n        valid_sets=lgbo_eval,\n        optuna_seed=42, # new\n        verbose_eval=50,  # Learning result output every 50 iterations : 50\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u6bce\u306b\u5b66\u7fd2\u7d50\u679c\u51fa\u529b\n        num_boost_round=1000,  # Specify the maximum number of iterations : \u6700\u5927\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u56de\u6570\u6307\u5b9a\n        early_stopping_rounds=100, # Early stopping number : early stopping\u3092\u63a1\u7528\u3059\u308biteration\u56de\u6570\n        categorical_feature = cat_list, # manual categorical feature setting\n    \n    )\n\n    booster.run()\n\n    metric_scores.append(booster.best_score)\n\n\n    # get best model\n    model = booster.get_best_booster()\n    \n\n    import pickle\n\n    model_name = \"lgboMmodel.bin\"\n\n    # saving model\n    pickle.dump(model, open(model_name, 'wb'))\n\n    # loading model\n    model = pickle.load(open(model_name, 'rb'))\n\n    # predicting validation value\n    oof_pred = model.predict(p_val[FEATURES], num_iteration=model.best_iteration)\n\n    oof_pred2 = np.where(oof_pred>=0.5,1,0)\n\n    scores.append(accuracy_score(p_val[TARGET], oof_pred2))\n\n    # predicting for test_X\n    preds = model.predict(test_X[FEATURES])\n\n    #preds2 = np.where(preds>=0.5,1,0)\n    \n    allpreds.append(preds)\n    \n    # out of fold : oof\n    p_val[\"preds\"] = oof_pred2\n    \n    allvaliddf = pd.concat([allvaliddf,p_val])\n    \n    ","0ed3e75f":"metric_scores","6b11bee8":"np.mean(metric_scores)","3c476d9c":"scores","23daffd7":"np.mean(scores)","30c2b1e0":"accuracy_score(allvaliddf[TARGET],allvaliddf[\"preds\"])","74d57c72":"allpreds = np.mean(allpreds,axis=0)","4b286770":"allpreds2 = np.where(allpreds>0.5,1,0)","cd2bfc78":"submission[\"Survived\"] = allpreds2","8a97fc8b":"submission","7b29fc60":"submission.to_csv(\"submission4.csv\",index=False)","54d88269":"def feval_accuracy(y_pred, lgb_train):\n    \n    y_true = lgb_train.get_label()\n    \n    y_pred = np.where(y_pred>=0.5,1,0)\n    score = accuracy_score(y_true, y_pred)\n    \n    \n    return 'Accuracy', score, True","fb91ebf5":"lgbm_params = {\n    'objective': 'binary', \n    'seed': 42, \n    'metric': 'None', # Change here \n    \"verbose\":-1,\n    \"deterministic\":True\n}","790b449f":"fix_seed(SEED) # for repetability\n\np_train = folds[folds[\"fold\"] != 0]\np_val = folds[folds[\"fold\"] == 0]\n\np_train = p_train.reset_index(drop=True)\np_val = p_val.reset_index(drop=True)\n\nlgb_train = lgb.Dataset(p_train[FEATURES], p_train[TARGET])\nlgb_eval = lgb.Dataset(p_val[FEATURES], p_val[TARGET])\n\ncat_list = ['Embarked']\n\n\nmodel = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval,\n                  verbose_eval=50,  \n                  num_boost_round=1000,  \n                  early_stopping_rounds=100, \n                  categorical_feature = cat_list, \n                 feval = feval_accuracy # add here\n                 )\n\nimport pickle\n\nmodel_name = \"LGBMmodel.bin\"\n\n# saving model\npickle.dump(model, open(model_name, 'wb'))\n\n# loading model\nmodel = pickle.load(open(model_name, 'rb'))\n\n# predicting validation value\noof_pred = model.predict(p_val[FEATURES], num_iteration=model.best_iteration)\n\noof_pred2 = np.where(oof_pred>=0.5,1,0)\n\naccuracy_score(p_val[TARGET], oof_pred2)\n\n# predicting for test_X\npreds = model.predict(test_X[FEATURES])\n\npreds2 = np.where(preds>=0.5,1,0)","c01a5522":"submission[\"Survived\"] = preds2\nsubmission.to_csv(\"submission5.csv\",index=False)","d9d7ad65":"scores = []\nallpreds = []\n\nallvaliddf = pd.DataFrame()\n\n\nfor fold in range(5):\n    \n    fix_seed(SEED) # for repetability\n\n    p_train = folds[folds[\"fold\"] != fold]\n    p_val = folds[folds[\"fold\"] == fold]\n\n    p_train = p_train.reset_index(drop=True)\n    p_val = p_val.reset_index(drop=True)\n\n    lgb_train = lgb.Dataset(p_train[FEATURES], p_train[TARGET])\n    lgb_eval = lgb.Dataset(p_val[FEATURES], p_val[TARGET])\n\n    cat_list = ['Embarked']\n\n\n    model = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval,\n                      verbose_eval=50,  # Learning result output every 50 iterations : 50\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u6bce\u306b\u5b66\u7fd2\u7d50\u679c\u51fa\u529b\n                      num_boost_round=1000,  # Specify the maximum number of iterations : \u6700\u5927\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u56de\u6570\u6307\u5b9a\n                      early_stopping_rounds=100, # Early stopping number : early stopping\u3092\u63a1\u7528\u3059\u308biteration\u56de\u6570\n                      categorical_feature = cat_list, # manual categorical feature setting\n                      feval = feval_accuracy # add here\n                     )\n\n    import pickle\n\n    model_name = f\"LGBMmodel{fold}.bin\"\n\n    # saving model\n    pickle.dump(model, open(model_name, 'wb'))\n\n    # loading model\n    model = pickle.load(open(model_name, 'rb'))\n\n    # predicting validation value\n    oof_pred = model.predict(p_val[FEATURES], num_iteration=model.best_iteration)\n\n    oof_pred2 = np.where(oof_pred>=0.5,1,0)\n\n    scores.append(accuracy_score(p_val[TARGET], oof_pred2))\n\n    # predicting for test_X\n    preds = model.predict(test_X[FEATURES])\n\n    #preds2 = np.where(preds>=0.5,1,0)\n    \n    allpreds.append(preds)\n    \n    # out of fold : oof\n    p_val[\"preds\"] = oof_pred2\n    \n    allvaliddf = pd.concat([allvaliddf,p_val])\n    \n    ","a8513a25":"scores","691f837e":"np.mean(scores)","696d1155":"allpreds = np.mean(allpreds,axis=0)\nallpreds2 = np.where(allpreds>=0.5,1,0)\nsubmission[\"Survived\"] = allpreds2\nsubmission.to_csv(\"submission6.csv\",index=False)","fa758dd0":"import optuna","9b0e507b":"def objective_fold(num):\n    \n    \n    def objective(trial):\n\n\n        for fold in range(5):\n            \n            if fold != num:\n                continue\n                \n\n            fix_seed(SEED) # for repetability\n\n            p_train = folds[folds[\"fold\"] != fold]\n            p_val = folds[folds[\"fold\"] == fold]\n\n            p_train = p_train.reset_index(drop=True)\n            p_val = p_val.reset_index(drop=True)\n\n            lgb_train = lgb.Dataset(p_train[FEATURES], p_train[TARGET])\n            lgb_eval = lgb.Dataset(p_val[FEATURES], p_val[TARGET])\n\n            cat_list = ['Embarked']\n\n\n\n\n        #==== \u6700\u9069\u5316\u3057\u305f\u3044\u30d1\u30e9\u30e1\u30fc\u30bf ====\n        lambda_l1 = trial.suggest_loguniform('lambda_l1', 1e-8, 10.0)\n        lambda_l2 = trial.suggest_loguniform('lambda_l2', 1e-8, 10.0)\n\n        learning_rate = trial.suggest_uniform('learning_rate', 0, 1.0)\n\n        feature_fraction = trial.suggest_uniform('feature_fraction', 0, 1.0)\n\n        bagging_fraction = trial.suggest_uniform('bagging_fraction', 0, 1.0)\n        bagging_freq = trial.suggest_int('bagging_freq', 5, 500)\n\n        num_leaves = trial.suggest_int('num_leaves', 5, 1000)\n        num_iterations = trial.suggest_int('num_iterations', 5, 1000)\n\n        min_child_samples = trial.suggest_int('min_child_samples', 5, 500)\n        min_child_weight = trial.suggest_int('min_child_weight', 5, 500)\n\n        max_depth = trial.suggest_int('max_depth', 5, 100)\n\n        #==== \u5b9a\u7fa9\u3057\u305f\u30d1\u30e9\u30e1\u30fc\u30bf ====\n\n        lgbm_params = {\n        'objective': 'binary', \n        'seed': 42, \n        'metric': 'None', # Change here \n        \"lambda_l1\": lambda_l1,\n                  \"lambda_l2\": lambda_l2,\n                  \"learning_rate\": learning_rate,\n                  \"feature_fraction\": feature_fraction,\n                  \"bagging_fraction\": bagging_fraction,\n                  \"bagging_freq\": bagging_freq,\n                  \"num_leaves\": num_leaves,\n                  \"num_iterations\": num_iterations,\n                  \"min_child_samples\": min_child_samples,\n                  \"min_child_weight\": min_child_weight,\n                  \"max_depth\": max_depth,\n                  \"verbosity\": -1,\n            \"deterministic\":True\n        }\n\n\n\n        model = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval,\n                              verbose_eval=50,  # Learning result output every 50 iterations : 50\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u6bce\u306b\u5b66\u7fd2\u7d50\u679c\u51fa\u529b\n                              num_boost_round=1000,  # Specify the maximum number of iterations : \u6700\u5927\u30a4\u30c6\u30ec\u30fc\u30b7\u30e7\u30f3\u56de\u6570\u6307\u5b9a\n                              early_stopping_rounds=100, # Early stopping number : early stopping\u3092\u63a1\u7528\u3059\u308biteration\u56de\u6570\n                              categorical_feature = cat_list, # manual categorical feature setting\n                              feval = feval_accuracy # add here\n                             )\n\n\n\n        oof_pred = model.predict(p_val[FEATURES])\n        oof_pred2 = np.where(oof_pred>=0.5,1,0)\n\n\n        score = accuracy_score(p_val[TARGET], oof_pred2)\n        return score\n    \n    return objective","eede03d1":"study = optuna.create_study(direction=\"maximize\",sampler=optuna.samplers.TPESampler(seed=42))\nstudy.optimize(objective_fold(0), n_trials=100)","c2a25aea":"trial = study.best_trial\nprint(trial.params)\n\nlgbm_params = {\n    'objective': 'binary', \n    'seed': 42, \n    'metric': 'None', # Change here \n    \"verbose\":-1,\n    \"deterministic\":True\n}\nlgbm_params.update(**lgbm_params,**trial.params)\nlgbm_params","4d7c9228":"fix_seed(SEED) # for repetability\n\np_train = folds[folds[\"fold\"] != 0]\np_val = folds[folds[\"fold\"] == 0]\n\np_train = p_train.reset_index(drop=True)\np_val = p_val.reset_index(drop=True)\n\nlgb_train = lgb.Dataset(p_train[FEATURES], p_train[TARGET])\nlgb_eval = lgb.Dataset(p_val[FEATURES], p_val[TARGET])\n\ncat_list = ['Embarked']\n\n\nmodel = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval,\n                  verbose_eval=50,  \n                  num_boost_round=1000,  \n                  early_stopping_rounds=100, \n                  categorical_feature = cat_list, \n                 feval = feval_accuracy # add here\n                 )\n\nimport pickle\n\nmodel_name = \"LGBMmodel.bin\"\n\n# saving model\npickle.dump(model, open(model_name, 'wb'))\n\n# loading model\nmodel = pickle.load(open(model_name, 'rb'))\n\n# predicting validation value\noof_pred = model.predict(p_val[FEATURES], num_iteration=model.best_iteration)\n\noof_pred2 = np.where(oof_pred>=0.5,1,0)\n\naccuracy_score(p_val[TARGET], oof_pred2)\n\n# predicting for test_X\npreds = model.predict(test_X[FEATURES])\n\npreds2 = np.where(preds>=0.5,1,0)","3f00ac77":"#allpreds = np.mean(allpreds,axis=0)\nsubmission[\"Survived\"] = preds2\nsubmission.to_csv(\"submission7.csv\",index=False)","3d57a1d8":"scores = []\nallpreds = []\n\nallvaliddf = pd.DataFrame()\n\nfor fold in range(5):\n    \n\n\n    study = optuna.create_study(direction=\"maximize\",sampler=optuna.samplers.TPESampler(seed=42))\n    study.optimize(objective_fold(fold), n_trials=100)\n\n    trial = study.best_trial\n    print(trial.params)\n\n    lgbm_params = {\n        'objective': 'binary', \n        'seed': 42, \n        'metric': 'None', # Change here \n        \"verbose\":-1,\n        \"deterministic\":True\n    }\n    lgbm_params.update(**lgbm_params,**trial.params)\n    lgbm_params\n\n    fix_seed(SEED) # for repetability\n\n    p_train = folds[folds[\"fold\"] != 0]\n    p_val = folds[folds[\"fold\"] == 0]\n\n    p_train = p_train.reset_index(drop=True)\n    p_val = p_val.reset_index(drop=True)\n\n    lgb_train = lgb.Dataset(p_train[FEATURES], p_train[TARGET])\n    lgb_eval = lgb.Dataset(p_val[FEATURES], p_val[TARGET])\n\n    cat_list = ['Embarked']\n\n\n    model = lgb.train(lgbm_params, lgb_train, valid_sets=lgb_eval,\n                      verbose_eval=50,  \n                      num_boost_round=1000,  \n                      early_stopping_rounds=100, \n                      categorical_feature = cat_list, \n                     feval = feval_accuracy # add here\n                     )\n\n    import pickle\n\n    model_name = \"LGBMmodel.bin\"\n\n    # saving model\n    pickle.dump(model, open(model_name, 'wb'))\n\n    # loading model\n    model = pickle.load(open(model_name, 'rb'))\n\n    # predicting validation value\n    oof_pred = model.predict(p_val[FEATURES], num_iteration=model.best_iteration)\n\n    oof_pred2 = np.where(oof_pred>=0.5,1,0)\n\n    scores.append(accuracy_score(p_val[TARGET], oof_pred2))\n\n    # predicting for test_X\n    preds = model.predict(test_X[FEATURES])\n    \n    #preds2 = np.where(preds>=0.5,1,0)\n    \n    allpreds.append(preds)\n    \n    # out of fold : oof\n    p_val[\"preds\"] = oof_pred2\n    \n    allvaliddf = pd.concat([allvaliddf,p_val])\n    \n","f7e4c3a0":"scores","16905daf":"np.mean(scores)","b7fc213d":"accuracy_score(allvaliddf[TARGET],allvaliddf[\"preds\"])","98e546eb":"allpreds = np.mean(allpreds,axis=0)\nallpreds2 = np.where(allpreds>=0.5,1,0)\nsubmission[\"Survived\"] = allpreds2\nsubmission.to_csv(\"submission8.csv\",index=False)","ab1405d3":"### 0.835 \u2192 0.849 \u306b\u4e0a\u6607","fe4f84f1":"## 4.1 Defining features and target\n##     \u7279\u5fb4\u91cf\u3068\u30e9\u30d9\u30eb(\u30bf\u30fc\u30b2\u30c3\u30c8)\u3092\u5b9a\u7fa9\u3057\u307e\u3059","07356a88":"## 6.2 Step2 : Turn around 5 times\n\nfor\u6587\u30675\u500b\u5206\u56de\u3059","d7e25552":"# 1. Confirming the train\/test data : \u30c7\u30fc\u30bf\u306e\u78ba\u8a8d\n","265ed3f8":"# About this notebook\n\nThis is an introduction to how to build Light GBM, optuna for beginners using Titanic data.\n\n\u203b\u3000I have devised a way to improve the reproducibility of LGBM optuna.\n\n\u30bf\u30a4\u30bf\u30cb\u30c3\u30af\u30c7\u30fc\u30bf\u3092\u7528\u3044\u305f\u521d\u5fc3\u8005\u306e\u305f\u3081\u306eLight GBM, optuna\u306e\u7d44\u307f\u65b9\u5165\u9580\u3067\u3059\u3002\n\n\n\u203b LGBM optuna\u306e\u518d\u73fe\u6027\u3082\u51fa\u3059\u3088\u3046\u306b\u5de5\u592b\u3057\u307e\u3057\u305f\u3002\n","3464ee9e":"## 6.1 Step1 : Combine from the chapter3.1 to 5 in one cell\n\n3.1\u304b\u30895\u30921\u3064\u306e\u30bb\u30eb\u306b\u307e\u3068\u3081\u308b","d63c9e98":"# 7. inference","ed262bec":"## 4.6 Show importance features","ef643bea":"# 2 label encoding\n#### Automatically convert strings to numbers. In the case of neural networks, you shouldn't do it without significant differences, but LGBM is irrelevant.\n#### \u6587\u5b57\u5217\u3092\u6570\u5b57\u306b\u81ea\u52d5\u5909\u63db. \n#### \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u5834\u5408\u306f\u3001\u6709\u610f\u5dee\u304c\u306a\u3044\u3068\u3084\u3063\u3066\u306f\u3044\u3051\u306a\u3044\u304c\u3001LGBM\u306f\u95a2\u4fc2\u306a\u3057(Categorical feature\u3067\u53d6\u308a\u6271\u3046)","b063e9de":"## 8.1 Let's do only fold. We just changed the model in Chapter 6.1 to the optuna version.\n\n* You can do it just by changing lgb to lgbo, but it is not reproducible, so it seems to be done by this method now.\n\n\u307e\u305a\u306f1\u3064\u3060\u3051\u3084\u308b. 6.1\u7ae0\u306emodel\u3092optuna\u7248\u306b\u5909\u3048\u305f\u3060\u3051\u3067\u3059\u3002\n\n* lgb\u3092lgbo\u306b\u5909\u3048\u308b\u3060\u3051\u3067\u3082\u3067\u304d\u307e\u3059\u304c\u3001\u518d\u73fe\u6027\u304c\u3042\u308a\u307e\u305b\u3093\u306e\u3067\u3001\u3053\u306e\u65b9\u6cd5\u3067\u4eca\u306f\u3084\u308b\u307f\u305f\u3044\u3067\u3059\u3002","b5f12a03":"## About data\nsurvival\tSurvival\t0 = No, 1 = Yes\npclass\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\nsex\tSex\t\nAge\tAge in years\t\nsibsp\t# of siblings \/ spouses aboard the Titanic\t\nparch\t# of parents \/ children aboard the Titanic\t\nticket\tTicket number\t\nfare\tPassenger fare\t\ncabin\tCabin number\t\nembarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton\n\n#########\u65e5\u672c\u8a9e#################\n\nsurvival\t\u751f\u6b7b\t0 = \u6b7b\u4ea1, 1 = \u751f\u5b58\npclass\tTicket class\t1 = 1st, 2 = 2nd, 3 = 3rd\nsex\t\u6027\u5225\t\nAge\t\u5e74\u9f62\t\nsibsp\t# of siblings \/ \u89aa\u65cf\u306e\u6570\t\nparch\t# of parents \/ \u5b50\u4f9b\u306e\u6570\t\nticket\tTicket number\u3000\u30c1\u30b1\u30c3\u30c8\u30ca\u30f3\u30d0\u30fc\t\nfare\tPassenger fare\t\u904b\u8cc3\ncabin\tCabin number\t\u90e8\u5c4b\u306e\u756a\u53f7\nembarked\tPort of Embarkation\tC = Cherbourg, Q = Queenstown, S = Southampton\u3000\u4e57\u8239\u3057\u305f\u5834\u6240\n","15aeaf3a":"### 0.826 \u2192 0.849\u306b\u4e0a\u6607\u3002","23900cf5":"## 9.2 Custom Metric LGBM without optuna : 5 kfold , Submission 6","6723dd55":"#### There are 891 rows. There are NaN data in Age, Cabin, Embarked.\n#### \u5168\u90e8\u3067891\u884c\u3042\u3063\u3066\u3001Age,Cabin,Embarked\u306b\u306fnull\u30c7\u30fc\u30bf\u306f\u306a\u3044\u304c\u3001NaN\u30c7\u30fc\u30bf\u304c\u3042\u308a\u305d\u3046\u3002","05adb38b":"## 4.4 Modeling","f72aadf8":"# 6. Application : Kfold","11b95816":"## 4.2 Making Dataset","22fbb9b9":"## 4.8 Scoring : Accuracy","1f43a195":"## After setting parameters, change lgb.train to optuna version\n\nparameter\u3092\u8a2d\u5b9a\u3057\u3066\u3001model\u8a2d\u5b9a\u306e\u3068\u3053\u308d\u3092\u5909\u3048\u308b\u3060\u3051\u3067\u3059\u3002","044265d9":"### cv\u306e\u8868\u73fe\u306e\u4ed5\u65b9\u306f2\u3064\u3042\u308b : score\u306e\u5e73\u5747\u3001out of fold : oof \u306e\u30b9\u30b3\u30a2","78f4c369":"## 4.7 Show ROC curve and calculating accuracy","fca5addb":"### 9.2.1 Inference","7acafff3":"## 4.5 Saving model and loading model method","69f903f4":"Meaningful as continuous data: Pclass, Age, SibSp, Parch\nCategorical feature: Embarked\n\nSince it is 0,1, it doesn't matter which one Sex (In the case of LGBM, the result does not change regardless of which one you put in experience)\n\nThe categorical feature is automatically determined by the basic default, but if you do not want it, you define it yourself.\n\n\n\u4e00\u5ea6\u6574\u7406\u3002\n\n\u9023\u7d9a\u30c7\u30fc\u30bf\u3068\u3057\u3066\u610f\u5473\u304c\u3042\u308b\u3082\u306e : Pclass, Age, SibSp, Parch\n\u610f\u5473\u304c\u3042\u308b\u304b\u308f\u304b\u3089\u306a\u3044\u3082\u306e(categorical feature) : Embarked\n\n0,1 \u306a\u306e\u3067\u3001\u3069\u3061\u3089\u3067\u3082\u3088\u3044\u3082\u306e Sex ( LGBM\u306e\u5834\u5408\u3001\u7d4c\u9a13\u4e0a\u3069\u3061\u3089\u306b\u5165\u308c\u3066\u3082\u7d50\u679c\u5909\u308f\u3089\u306a\u3044)\n\ncategorical feature\u306f\u57fa\u672cdefault\u3067\u81ea\u52d5\u5224\u5b9a\u3057\u3066\u304f\u308c\u308b\u304c\u3001\u3055\u308c\u305f\u304f\u306a\u3044\u5834\u5408\u306f\u81ea\u5206\u3067\u5b9a\u7fa9\u3059\u308b\u3002\n\n\n\u307e\u305f\u3001categorical feature\u306f0\u304b\u3089\u306e\u9023\u7d9a\u3059\u308b\u6574\u6570\u304c\u597d\u307e\u3057\u3044\u3002\u3068LGBM\u306e\u8aac\u660e\u30da\u30fc\u30b8\u306b\u66f8\u3044\u3066\u3042\u308b\n","b558dded":"## 5 kfold without optuna is the best LB score. optuna may overfit.\n\n5 kfold\u306eoptuna\u304c\u4e00\u756a\u826f\u3044\u30b9\u30b3\u30a2\u306b\u306a\u308a\u307e\u3057\u305f\u3002optuna\u306f\u904e\u5b66\u7fd2\u3057\u3066\u3044\u308b\u306e\u304b\u3082\u3057\u308c\u307e\u305b\u3093\u3002","5ee90f0d":"## 9.3 Kfold with custom metric LGBM each fold optuna","6a12f2ac":"## 9.2 Custom Metric Optuna with LGBM : fold = 0 , Submission 7\n\n\u203b Optunalightgbmtuner\u3057\u304boptuna\u306eseed\u56fa\u5b9a\u3067\u304d\u307e\u305b\u3093\u304c\u3001custom metric\u304c\u4f7f\u3048\u307e\u305b\u3093\u3067\u3057\u305f\u3002\u305d\u306e\u305f\u3081\u3001\u901a\u5e38\u7248\u3067\u3059\u3002","3052aca0":"# 4.Light GBM","9f78e5d6":"## In order to get repeatablity, we must use optuna_seed. It can be optuna version > 2.8. So update.","d393ca6c":"# 5.inference for test data","107f30eb":"## 9.1 Custom Metric LGBM without optuna : fold 0, Submission 5","abbcf5b4":"# 0.824 \u2192 0.835\u306b\u4e0a\u6607","94c8997f":"# 9. Appendix : LGBM using custom metric","4d44a1c4":"# 8. Optuna : Optimization of hyper parameters of Light GBM","92d6e3b8":"## 4.3 Setting parameter","bc97514f":"## 3.1 : 1-fold separation for example of modeling & inference","4c98aebe":"*  Nan data can be used with LGBM, but it must be float type.\n*  Embarked contains a character string and nan, so when label encoding, first replace na with fillna and then label encode.\n\n##### \u65e5\u672c\u8a9e\n*  Nan\u30c7\u30fc\u30bf\u306f\u3001LGBM\u3067\u4f7f\u7528\u3067\u304d\u308b\u304c\u3001float\u578b\u306b\u3057\u306a\u3044\u3068\u3044\u3051\u306a\u3044\u3002\n*  Embarked\u306f\u6587\u5b57\u5217\u3068\u3001nan\u304c\u5165\u3063\u3066\u3044\u308b\u305f\u3081\u3001label encode\u3059\u308b\u3068\u304d\u306f\u3001fillna\u3067\u307e\u305a\u306f\u3001na\u3092\u7f6e\u63db\u3057\u305f\u5f8c\u3001label encoding\u3059\u308b ","b674035e":"## 8.2 Kfold","4f86eafb":"## for practice, fold0 is defined as validation, fold1-4 are defined as train\n## \u7df4\u7fd2\u306e\u305f\u3081\u306b\u3001\u307e\u305a\u3001fold0\u3092\u691c\u8a3c\u30c7\u30fc\u30bf\u3001fold1-4\u3092\u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u3057\u307e\u3059\u3002","4f9143ff":"# 3. Kfold\n#### Prepare training data and verification data in 5 combinations.\n#### \u8a13\u7df4\u30c7\u30fc\u30bf\u3068\u691c\u8a3c\u30c7\u30fc\u30bf\u30925\u3064\u306e\u7d44\u307f\u5408\u308f\u305b\u3067\u6e96\u5099\u3059\u308b\u3002","4a65f3e6":"![image.png](attachment:2d070816-bb47-4d80-afd3-3309b9de97e2.png)","89342c9d":"## 5.1 loading model"}}