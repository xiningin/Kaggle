{"cell_type":{"dbe31b6d":"code","f05d2e30":"code","33f0bb32":"code","5abdccaf":"code","53b0eded":"code","da4306c6":"code","9c778135":"code","56eb353d":"code","d9fffe2a":"code","f77a444e":"code","cc855aa6":"code","1afdc399":"code","65099fa5":"code","555c8d50":"code","0a6b22a6":"code","9c41e5ce":"code","67581131":"code","9294a380":"code","d14ad399":"code","c3fb278e":"code","9afcd0eb":"code","9317e79b":"code","e6a069b5":"code","8510c656":"code","f108a105":"code","65b1a6a9":"code","f70e6c90":"code","36b49c5c":"code","6645ee83":"code","8a6cfa3c":"code","c435f6d4":"code","a53efacd":"code","effd0f8d":"code","1e016606":"code","98aa7eef":"code","e209db8b":"code","cc67a522":"code","a282cbfa":"code","fc90324d":"code","52eda3a1":"code","19a78420":"code","19c53a46":"code","7a98fe97":"code","59bacaa2":"code","e88d9083":"code","c4293baf":"code","a0f347d3":"code","91e170d2":"code","7ede01b4":"code","48c48c70":"code","5d881b5c":"code","4c7e8a99":"code","fafd6c01":"code","86c693fb":"code","c51adde4":"code","cab7e47c":"markdown","1a27a92c":"markdown","c027ad42":"markdown","36d2e46f":"markdown","531d2283":"markdown","f41b6e7d":"markdown","967f53c1":"markdown","a8c555ba":"markdown","e0aa2f56":"markdown","7ed8406f":"markdown","d42fe8d5":"markdown","9d902efb":"markdown","ea95f463":"markdown","bf537c82":"markdown","8dcc0740":"markdown","6e5ea6db":"markdown","2db2fdaa":"markdown","40b73a8f":"markdown","bc4e4b46":"markdown","7f44f1fc":"markdown","737ad174":"markdown","10b6a0b6":"markdown","9292b4b8":"markdown","282ba57f":"markdown","2be38482":"markdown","c11c68bd":"markdown","69ef1467":"markdown","2536665c":"markdown","a4c67046":"markdown","f6f2350f":"markdown","d29bb890":"markdown","8abe6c21":"markdown","df734e31":"markdown","45004263":"markdown","fe082702":"markdown","8d521359":"markdown","762de5f1":"markdown","b8fe1ef2":"markdown","18fd2479":"markdown","88fe43ce":"markdown","f345ab21":"markdown","8bad4ace":"markdown","3f19c8cd":"markdown","3686c5b3":"markdown","02be0695":"markdown","3850c49f":"markdown","dd00d80f":"markdown","6764c10e":"markdown","14f343c2":"markdown","f8b9c9b5":"markdown","a01a1ba3":"markdown","9e0d67ff":"markdown","4ddf41e4":"markdown","e91be05e":"markdown","891eade4":"markdown"},"source":{"dbe31b6d":"#import some necessary librairies\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns\ncolor = sns.color_palette()\nsns.set_style('darkgrid')\nimport warnings\ndef ignore_warn(*args, **kwargs):\n    pass\nwarnings.warn = ignore_warn #ignore annoying warning (from sklearn and seaborn)\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics","f05d2e30":"#Now let's import and put the train and test datasets in  pandas dataframe\n\ntrain = pd.read_csv('..\/input\/train.csv')\ntest = pd.read_csv('..\/input\/test.csv')\n\ntrain.describe()","33f0bb32":"print (\"Size of train data : {}\" .format(train.shape))\n\nprint (\"Size of test data : {}\" .format(test.shape))","5abdccaf":"#Save the 'Id' column\ntrain_ID = train['Id']\ntest_ID = test['Id']\n\n#Now drop the  'Id' colum since it's unnecessary for  the prediction process.\ntrain.drop(\"Id\", axis = 1, inplace = True)\ntest.drop(\"Id\", axis = 1, inplace = True)","53b0eded":"print (\"Size of train data after dropping Id: {}\" .format(train.shape))\nprint (\"Size of test data after dropping Id: {}\" .format(test.shape))","da4306c6":"fig, ax = plt.subplots()\nax.scatter(x = train['GrLivArea'], y = train['SalePrice'])\nplt.ylabel('SalePrice', fontsize=13)\nplt.xlabel('GrLivArea', fontsize=13)\nplt.show()","9c778135":"#Deleting outliers\ntrain = train.drop(train[(train['GrLivArea']>4000) & (train['SalePrice']<300000)].index)","56eb353d":"# most correlated features\ncorrmat = train.corr()\ntop_corr_features = corrmat.index[abs(corrmat[\"SalePrice\"])>0.5]\nplt.figure(figsize=(10,10))\ng = sns.heatmap(train[top_corr_features].corr(),annot=True,cmap=\"RdYlGn\")","d9fffe2a":"sns.barplot(train.OverallQual,train.SalePrice)","f77a444e":"sns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train[cols], size = 2.5)\nplt.show();","cc855aa6":"sns.scatterplot(train.GrLivArea,train.TotalBsmtSF)","1afdc399":"def check_skewness(col):\n    sns.distplot(train[col] , fit=norm);\n    fig = plt.figure()\n    res = stats.probplot(train[col], plot=plt)\n    # Get the fitted parameters used by the function\n    (mu, sigma) = norm.fit(train[col])\n    print( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))\n    \ncheck_skewness('SalePrice')","65099fa5":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ntrain[\"SalePrice\"] = np.log1p(train[\"SalePrice\"])\n\ncheck_skewness('SalePrice')","555c8d50":"ntrain = train.shape[0]\nntest = test.shape[0]\ny_train = train.SalePrice.values\nall_data = pd.concat((train, test)).reset_index(drop=True)\nall_data.drop(['SalePrice'], axis=1, inplace=True)\nprint(\"all_data size is : {}\".format(all_data.shape))","0a6b22a6":"all_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)[:30]\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})","9c41e5ce":"f, ax = plt.subplots(figsize=(15, 12))\nplt.xticks(rotation='90')\nsns.barplot(x=all_data_na.index, y=all_data_na)\nplt.xlabel('Features', fontsize=15)\nplt.ylabel('Percent of missing values', fontsize=15)\nplt.title('Percent missing data by feature', fontsize=15)","67581131":"all_data.PoolQC.loc[all_data.PoolQC.notnull()]","9294a380":"all_data[\"PoolQC\"] = all_data[\"PoolQC\"].fillna(\"None\")","d14ad399":"all_data[\"MiscFeature\"] = all_data[\"MiscFeature\"].fillna(\"None\")","c3fb278e":"all_data[\"Alley\"] = all_data[\"Alley\"].fillna(\"None\")","9afcd0eb":"all_data[\"Fence\"] = all_data[\"Fence\"].fillna(\"None\")","9317e79b":"all_data[\"FireplaceQu\"] = all_data[\"FireplaceQu\"].fillna(\"None\")","e6a069b5":"# Grouping by Neighborhood and Check the LotFrontage. Most of the grouping has similar areas\ngrouped_df = all_data.groupby('Neighborhood')['LotFrontage']\n\nfor key, item in grouped_df:\n    print(key,\"\\n\")\n    print(grouped_df.get_group(key))\n    break","8510c656":"#Group by neighborhood and fill in missing value by the median LotFrontage of all the neighborhood\nall_data[\"LotFrontage\"] = all_data.groupby(\"Neighborhood\")[\"LotFrontage\"].transform(\n    lambda x: x.fillna(x.median()))","f108a105":"for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n    all_data[col] = all_data[col].fillna('None')","65b1a6a9":"abc = ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond','GarageYrBlt', 'GarageArea', 'GarageCars']\nall_data.groupby('GarageType')[abc].count()","f70e6c90":"for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n    all_data[col] = all_data[col].fillna(0)","36b49c5c":"for col in ('BsmtFinSF1', 'BsmtFinSF2', 'BsmtUnfSF','TotalBsmtSF', 'BsmtFullBath', 'BsmtHalfBath'):\n    all_data[col] = all_data[col].fillna(0)","6645ee83":"for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n    all_data[col] = all_data[col].fillna('None')","8a6cfa3c":"all_data[\"MasVnrType\"] = all_data[\"MasVnrType\"].fillna(\"None\")\nall_data[\"MasVnrArea\"] = all_data[\"MasVnrArea\"].fillna(0)","c435f6d4":"all_data['MSZoning'].value_counts()","a53efacd":"all_data['MSZoning'] = all_data['MSZoning'].fillna(all_data['MSZoning'].mode()[0])","effd0f8d":"all_data['Utilities'].value_counts()","1e016606":"all_data = all_data.drop(['Utilities'], axis=1)","98aa7eef":"all_data[\"Functional\"] = all_data[\"Functional\"].fillna(\"Typ\")","e209db8b":"mode_col = ['Electrical','KitchenQual', 'Exterior1st', 'Exterior2nd', 'SaleType']\nfor col in mode_col:\n    all_data[col] = all_data[col].fillna(all_data[col].mode()[0])","cc67a522":"all_data['MSSubClass'] = all_data['MSSubClass'].fillna(\"None\")","a282cbfa":"#Check remaining missing values if any \nall_data_na = (all_data.isnull().sum() \/ len(all_data)) * 100\nall_data_na = all_data_na.drop(all_data_na[all_data_na == 0].index).sort_values(ascending=False)\nmissing_data = pd.DataFrame({'Missing Ratio' :all_data_na})\nmissing_data.head()","fc90324d":"all_data['OverallCond'].value_counts()","52eda3a1":"#MSSubClass=The building class\nall_data['MSSubClass'] = all_data['MSSubClass'].apply(str)\n\n\n#Changing OverallCond into a categorical variable\nall_data['OverallCond'] = all_data['OverallCond'].astype(str)\n\n\n#Year and month sold are transformed into categorical features.\nall_data['YrSold'] = all_data['YrSold'].astype(str)\nall_data['MoSold'] = all_data['MoSold'].astype(str)","19a78420":"from sklearn.preprocessing import LabelEncoder\ncols = ('FireplaceQu', 'BsmtQual', 'BsmtCond', 'GarageQual', 'GarageCond', \n        'ExterQual', 'ExterCond','HeatingQC', 'PoolQC', 'KitchenQual', 'BsmtFinType1', \n        'BsmtFinType2', 'Functional', 'Fence', 'BsmtExposure', 'GarageFinish', 'LandSlope',\n        'LotShape', 'PavedDrive', 'Street', 'Alley', 'CentralAir', 'MSSubClass', 'OverallCond', \n        'YrSold', 'MoSold')\n# process columns, apply LabelEncoder to categorical features\nfor c in cols:\n    lbl = LabelEncoder() \n    lbl.fit(list(all_data[c].values)) \n    all_data[c] = lbl.transform(list(all_data[c].values))\n\n# shape        \nprint('Shape all_data: {}'.format(all_data.shape))","19c53a46":"# Adding total sqfootage feature \nall_data['TotalSF'] = all_data['TotalBsmtSF'] + all_data['1stFlrSF'] + all_data['2ndFlrSF']","7a98fe97":"numeric_feats = all_data.dtypes[all_data.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = all_data[numeric_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint(\"\\nSkew in numerical features: \\n\")\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(15)","59bacaa2":"skewness = skewness[abs(skewness) > 0.75]\nprint(\"There are {} skewed numerical features to Box Cox transform\".format(skewness.shape[0]))\n\nfrom scipy.special import boxcox1p\nskewed_features = skewness.index\nlam = 0.15\nfor feat in skewed_features:\n    #all_data[feat] += 1\n    all_data[feat] = boxcox1p(all_data[feat], lam)","e88d9083":"all_data = pd.get_dummies(all_data)\nall_data.shape","c4293baf":"train = all_data[:ntrain]\ntest = all_data[ntrain:]\ntrain.shape","a0f347d3":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","91e170d2":"#Validation function\nn_folds = 5\n\ndef rmsle_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(train.values)\n    rmse= np.sqrt(-cross_val_score(model, train.values, y_train, scoring=\"neg_mean_squared_error\", cv = kf))\n    return(rmse)","7ede01b4":"KRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nscore = rmsle_cv(KRR)\nprint(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","48c48c70":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nscore = rmsle_cv(lasso)\nprint(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","5d881b5c":"ENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nscore = rmsle_cv(ENet)\nprint(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","4c7e8a99":"GBoost = GradientBoostingRegressor(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='huber', random_state =5)\nscore = rmsle_cv(GBoost)\nprint(\"Gradient Boosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","fafd6c01":"LassoMd = lasso.fit(train.values,y_train)\nENetMd = ENet.fit(train.values,y_train)\nKRRMd = KRR.fit(train.values,y_train)\nGBoostMd = GBoost.fit(train.values,y_train)","86c693fb":"finalMd = (np.expm1(LassoMd.predict(test.values)) + np.expm1(ENetMd.predict(test.values)) + np.expm1(KRRMd.predict(test.values)) + np.expm1(GBoostMd.predict(test.values)) ) \/ 4\nfinalMd","c51adde4":"sub = pd.DataFrame()\nsub['Id'] = test_ID\nsub['SalePrice'] = finalMd\nsub.to_csv('submission.csv',index=False)","cab7e47c":"**Lets see the highly skewed features we have**","1a27a92c":"## Cross Validation\nIt's simple way to calculate error for evaluation. \n\n**KFold( )** splits the train\/test data into k consecutive folds, we also have made shuffle attrib to True.\n\n**cross_val_score ( )** evaluate a score by cross-validation.","c027ad42":"* **MSSubClass** : Na most likely means No building class. We can replace missing values with None\n","36d2e46f":"## Gradient Boosting Regression\nRefer [here](https:\/\/medium.com\/mlreview\/gradient-boosting-from-scratch-1e317ae4587d)","531d2283":"## Lasso Regression\nLASSO (Least Absolute Shrinkage Selector Operator), is quite similar to ridge.\n\nIn case of lasso, even at smaller alpha\u2019s, our coefficients are reducing to absolute zeroes.\n Therefore, lasso selects the only some feature while reduces the coefficients of others to zero. This property is known as feature selection and which is absent in case of ridge.\n \n- Lasso uses L1 regularization technique.\n- Lasso is generally used when we have more number of features, because it automatically does feature selection.\n","f41b6e7d":"* **FireplaceQu** : data description says NA means \"no fireplace\"","967f53c1":"Since area related features are very important to determine house prices, we add one more feature which is the total area of basement, first and second floor areas of each house","a8c555ba":"# Importing packages\nWe have **numpy** and **pandas** to work with numbers and data, and we have **seaborn** and **matplotlib** to visualize data. We would also like to filter out unnecessary warnings. **Scipy** for normalization and skewing of data.","e0aa2f56":"One of the figures we may find interesting is the one between ** 'TotalBsmtSF' and 'GrLiveArea'. **\n\nWe can see the dots drawing a linear line, which almost acts like a border. It totally makes sense that the majority of the dots stay below that line. Basement areas can be equal to the above ground living area, but it is not expected a basement area bigger than the above ground living area","7ed8406f":"* **MasVnrArea and MasVnrType** : NA most likely means no masonry veneer for these houses. We can fill 0 for the area and None for the type.","d42fe8d5":"**This distribution is positively skewed.** Notice that the black curve is more deviated towards the right. If you encounter that your predictive (response) variable is skewed, it is **recommended to fix the skewness** to make good decisions by the model.\n\n## Okay, So how do I fix the skewness?\nThe best way to fix it is to perform a **log transform** of the same data, with the intent to reduce the skewness.","9d902efb":"* **MiscFeature** : Data documentation says NA means \"no misc feature\"","ea95f463":"# Step By Step Procedure To Predict House Price","bf537c82":"* **MSZoning (The general zoning classification)** : 'RL' is by far the most common value. So we can fill in missing values with 'RL'","8dcc0740":"## Correlation Analysis\n\nLet see the most correlated features.","6e5ea6db":"* **BsmtFinSF1, BsmtFinSF2, BsmtUnfSF, TotalBsmtSF, BsmtFullBath and BsmtHalfBath** : missing values are likely zero for having no basement","2db2fdaa":"## Lets apply Modelling\n\n1. Importing Libraries\n\n2. We will use models\n - Lasso\n - Ridge\n - ElasticNet\n - Gradient Boosting\n \n3. Find the Cross Validation Score.\n4. Calculate the mean of all model's prediction.\n5. Submit the CSV file.\n ","40b73a8f":"**Now there any many features that are numerical but categorical.**","bc4e4b46":"After taking logarithm of the same data the curve seems to be normally distributed, although not perfectly normal, this is sufficient to fix the issues from a skewed dataset as we saw before.\n\n**Important : If you log transform the response variable, it is required to also log transform feature variables that are skewed.**","7f44f1fc":"Since PoolQC has the highest null values according to the data documentation says **null values means 'No Pool.**\nSince majority of houses has no pool.\nSo we will replace those null values with 'None'.","737ad174":"# Feature Engineering","10b6a0b6":"# Loading and Inspecting data\nWith various Pandas functions, we load our training and test data set as well as inspect it to get an idea of the data we're working with. This is a large dataset we will be working on.\n","9292b4b8":"**Getting dummy categorical features**","282ba57f":"* **LotFrontage** : Since the area of each street connected to the house property most likely have a similar area to other houses in its neighborhood , we can fill in missing values by the median LotFrontage of the neighborhood.","2be38482":"## Submission","c11c68bd":"## Label Encoding \nAs you might know by now, we can\u2019t have text in our data if we\u2019re going to run any kind of model on it. So before we can run a model, we need to make this data ready for the model.\n\nAnd to convert this kind of categorical text data into model-understandable numerical data, we use the Label Encoder class.\n\nSuppose, we have a feature State which has 3 category i.e India , France, China . So, Label Encoder will categorize them as 0, 1, 2.","69ef1467":"* **Functional** : data description says NA means typical","2536665c":"* **Utilities** : Since this is a categorical data and most of the data are of same category, Its not gonna effect on model. So we choose to drop it.","a4c67046":"* **Fence** : data description says NA means \"no fence\"\n","f6f2350f":"## Dealing with outliers\n\nOutlinear in the GrLivArea is recommended by the author of the data  to remove it. The author says in documentation \u201cI would recommend removing any houses with more than 4000 square feet from the data set (which eliminates these five unusual observations) before assigning it to students.\u201d\n","d29bb890":"* **GarageType, GarageFinish, GarageQual and GarageCond** : Replacing missing data with None as per documentation. ","8abe6c21":"# Modelling\nSince in this dataset we have a large set of features. So to make our model avoid Overfitting and noisy we will use Regularization.\nThese model have Regularization parameter.\n\nRegularization will reduce the magnitude of the coefficients.","df734e31":"**If you found this notebook helpful or you just liked it , some upvotes would be very much appreciated.**\n\n**I'll be glad to hear suggestions on improving my models**","45004263":"# Handle Missing Data","fe082702":"# Within Top 10% with Simple Regression Model.","8d521359":"**Scatter plots between 'SalePrice' and correlated variables**","762de5f1":"Creating train and test data.","b8fe1ef2":"## Target Variable Transform\nDifferent features in the data set may have values in different ranges. For example, in this data set, the range of SalePrice feature may lie from thousands to lakhs but the range of values of YearBlt feature will be in thousands. That means a column is more weighted compared to other.\n\n**Lets check the skewness of data**\n![Skew](https:\/\/cdn-images-1.medium.com\/max\/800\/1*hxVvqttoCSkUT2_R1zA0Tg.gif)","18fd2479":"## Lets check for any missing values","88fe43ce":"## Ridge Regression\n- It shrinks the parameters, therefore it is mostly used to prevent multicollinearity.\n- It reduces the model complexity by coefficient shrinkage.\n- It uses L2 regularization technique.","f345ab21":"We can see that there are outlinear with low SalePrice and high GrLivArea. This looks odd.\nWe need to remove it.","8bad4ace":"> That is a very large data set! We are going to have to do a lot of work to clean it up\n\n**Drop the Id column because we dont need it currently.**","3f19c8cd":"## Box Cox Transformation of (highly) skewed features\n\nWhen you are dealing with real-world data, you are going to deal with features that are heavily skewed. Transformation technique is useful to **stabilize variance**, make the data **more normal distribution-like**, improve the validity of measures of association.\n\nThe problem with the Box-Cox Transformation is **estimating lambda**. This value will depend on the existing data, and should be considered when performing cross validation on out of sample datasets.","3686c5b3":"* **GarageYrBlt, GarageArea and GarageCars** : Replacing missing data with 0 (Since No garage = no cars in such garage.)","02be0695":"- From this we can tell which features **(OverallQual, GrLivArea and TotalBsmtSF )** are highly positively correlated with the SalePrice. \n- **GarageCars and GarageArea ** also seems correlated with other, Since the no. of car that will fit into the garage will depend on GarageArea. ","3850c49f":"**Fit the training dataset on every model**","dd00d80f":"## Elastic Net Regression\n\nElastic net is basically a combination of both L1 and L2 regularization. So if you know elastic net, you can implement both Ridge and Lasso by tuning the parameters.","6764c10e":"# Missing Data","14f343c2":"* **Electrical,KitchenQual, Exterior1st, Exterior2nd, SaleType** : Since this all are categorical values so its better to replace nan values with the most used keyword.","f8b9c9b5":"* **BsmtQual, BsmtCond, BsmtExposure, BsmtFinType1 and BsmtFinType2** : For all these categorical basement-related features, NaN means that there is no basement.","a01a1ba3":"* **Alley** : data description says NA means \"no alley access\"\n","9e0d67ff":"**Converting some numerical variables that are really categorical type.**\n\nAs you can see the category range from 1 to 9 which are numerical (**not ordinal type**). Since its categorical we need to change it to String type.\n\nIf we do not convert these to categorical, some model may get affect by this as model will compare the value 1<5<10 . We dont need that to happen with our model.","4ddf41e4":"**GarageType,  GarageFinish, GarageQual,  GarageCond, GarageYrBlt,  GarageArea,  GarageCars  these all features have same percentage of null values.**","e91be05e":"## Mean of all model's prediction.\nnp.expm1 ( ) is used to calculate exp(x) - 1 for all elements in the array. ","891eade4":"Here is the [Documentation](http:\/\/ww2.amstat.org\/publications\/jse\/v19n3\/Decock\/DataDocumentation.txt) you can refer , to know more about the dataset.\n\n**Concatenate both train and test values.**"}}