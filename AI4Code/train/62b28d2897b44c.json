{"cell_type":{"9bfa86aa":"code","b20968db":"code","a8055b6d":"code","c169a432":"code","9c51fe6f":"code","80da62c7":"code","0c863b7b":"code","da1cafd8":"code","a4e19e1e":"code","de52e7b9":"code","03828e67":"code","499442fc":"code","3bc91a2f":"code","fd466c9c":"code","48fc97b7":"code","4cd45516":"code","fb33d6bf":"code","d44cd524":"code","17eee85a":"code","8db2ffc9":"code","155f5a78":"code","411bfd6a":"code","74593a7a":"code","bcd9be81":"code","652e2cf4":"code","2251a980":"code","3d207100":"code","010e1afb":"code","c046e446":"code","b022c229":"code","f5550c27":"code","04e13106":"code","6d57f532":"code","08f2d7ef":"code","4a676f54":"markdown","38f71e7b":"markdown","65ba9d10":"markdown","79336bd3":"markdown","3b867049":"markdown","19208691":"markdown","59da4a4c":"markdown","fcfa41a1":"markdown","ac8ad869":"markdown","8626e852":"markdown","0a0954c8":"markdown","4bbd09e2":"markdown","6f92b55f":"markdown","a9efc265":"markdown","4db93fd0":"markdown","eb544022":"markdown","f92088e9":"markdown","5b454c09":"markdown","547ffcaf":"markdown","33922809":"markdown","8a623f8b":"markdown","af065d09":"markdown","0db3b0c4":"markdown","13c78a93":"markdown","447d7e54":"markdown","51a52f4b":"markdown","1ed28630":"markdown"},"source":{"9bfa86aa":"import numpy as np \nimport pandas as pd\nimport matplotlib.pyplot as plt\nplt.style.use(\"seaborn-whitegrid\")\nimport seaborn as sns\nfrom collections import Counter\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\ndata = pd.read_csv(\"\/kaggle\/input\/spam-text-message-classification\/SPAM text message 20170820 - Data.csv\")\ndata.head(10)","b20968db":"data.info()","a8055b6d":"data.describe().T","c169a432":"def bar_plot(variable):\n    var = data[variable]\n    varValue = var.value_counts()\n    \n    # visualize\n    plt.figure(figsize=(9,3))\n    plt.bar(varValue.index, varValue)\n    plt.xticks(varValue.index,varValue.index.values)\n    plt.ylabel(\"Count\")\n    plt.title(variable)\n    plt.show()\n    print(\"{}: \\n{}\".format(variable,varValue))","9c51fe6f":"bar_plot(\"Category\")","80da62c7":"ham_message_length = []\nspam_message_length = []\nfor i in data.values:\n    if(i[0] == \"ham\"):\n        ham_message_length.append(len(i[1]))\n    else:\n        spam_message_length.append(len(i[1]))\n        \n# average\nham_average = sum(ham_message_length)\/len(ham_message_length)\nspam_average = sum(spam_message_length)\/len(spam_message_length)\nprint(\"ham_average: \", ham_average)\nprint(\"spam_average: \", spam_average)","0c863b7b":"plt.figure(figsize=(9,3))\nplt.bar([\"ham_average\",\"spam_average\"], [ham_average,spam_average])\nplt.show()","da1cafd8":"data.isnull().sum()","a4e19e1e":"from sklearn.model_selection import train_test_split\nimport re\nfrom nltk.corpus import stopwords\nimport nltk as nlp\nfrom sklearn.feature_extraction.text import CountVectorizer ","de52e7b9":"data.Category = [1 if each == \"ham\" else 0 for each in data.Category]\ndf = data\ndata","03828e67":"# for example\n\nexample = data.Message[0]\nprint(\"before: \", example)\n\nexample = re.sub(\"[^a-zA-Z]\",\" \",example)\nprint(\"after:\", example)","499442fc":"regularExpressionMessages = []\nfor message in data[\"Message\"]:\n    message = re.sub(\"[^a-zA-Z]\",\" \",message)\n    regularExpressionMessages.append(message)\n\ndata[\"Message\"] = regularExpressionMessages\ndata","3bc91a2f":"# for example\n\nexample = data.Message[0]\nprint(\"before: \", example)\n\nexample = example.lower()\nprint(\"after:\", example)","fd466c9c":"lowercaseMessages = []\nfor message in data[\"Message\"]:\n    message = message.lower()\n    lowercaseMessages.append(message)\n\ndata[\"Message\"] = lowercaseMessages\ndata","48fc97b7":"# for example\n\nexample = data.Message[0]\nprint(\"before: \", example)\n\nexample = nlp.word_tokenize(example)\nprint(\"after:\", example)","4cd45516":"splitMessages = []\nfor message in data[\"Message\"]:\n    message = nlp.word_tokenize(message)\n    splitMessages.append(message)\n\ndata[\"Message\"] = splitMessages\ndata","fb33d6bf":"# for example\n\nprint(\"before: \", data[\"Message\"][0] )\n\nmessage1 = [message for message in data[\"Message\"][0] if not message in set(stopwords.words(\"english\"))]\nprint(\"after: \", message1)","d44cd524":"stopwordsMessages = []\nfor i in data[\"Message\"]:\n    i = [message for message in i if not message in set(stopwords.words(\"english\"))]\n    stopwordsMessages.append(i)\n\ndata[\"Message\"] = stopwordsMessages\ndata","17eee85a":"# for example\n\nexample = data.Message[6]\nprint(\"before: \", example)\n\nlemma = nlp.WordNetLemmatizer()\nexample = [ lemma.lemmatize(word) for word in example]\nprint(\"after:\", example)","8db2ffc9":"example = \" \".join(example)\nprint(\"example: \", example)","155f5a78":"joinMessages = []\nfor message in data[\"Message\"]:\n    message = \" \".join(message)\n    joinMessages.append(message)\n\ndata[\"Message\"] = joinMessages\ndata","411bfd6a":"# for example\nsentence1 = \"I am coming from school\"\nsentence2 = \"I am coming from Istanbul today\"\n\nfrom sklearn.feature_extraction.text import CountVectorizer \ncount_vectorizer = CountVectorizer(max_features=5, stop_words=\"english\")\nsparce_matrix = count_vectorizer.fit_transform([sentence1,sentence2]).toarray()\nsparce_matrix","74593a7a":"count_vectorizer = CountVectorizer(max_features=10000, stop_words=\"english\")\nsparce_matrix = count_vectorizer.fit_transform(np.array(data[\"Message\"])).toarray()\nsparce_matrix","bcd9be81":"sparce_matrix.shape","652e2cf4":"# text classification\n\ny = data.iloc[:,0].values\nx = sparce_matrix","2251a980":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x,y,test_size=0.2,random_state=42)\n\nprint(\"x_train\",x_train.shape)\nprint(\"x_test\",x_test.shape)\nprint(\"y_train\",y_train.shape)\nprint(\"y_test\",y_test.shape)","3d207100":"from sklearn.naive_bayes import GaussianNB\n\nnb = GaussianNB()\nnb.fit(x_train,y_train)\n\nprint(\"Accuracy: \", nb.score(x_test,y_test)*100)","010e1afb":"from sklearn.model_selection import train_test_split, StratifiedKFold, GridSearchCV\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score","c046e446":"logreg = LogisticRegression()\nlogreg.fit(x_train,y_train)\nacc_log_train = round(logreg.score(x_train,y_train)*100,2)\nacc_log_test = round(logreg.score(x_test,y_test)*100,2)\nprint(\"Training Accuracy: %{}\".format(acc_log_train))\nprint(\"Testing Accuracy: %{}\".format(acc_log_test))","b022c229":"from sklearn.neighbors import KNeighborsClassifier\nknn = KNeighborsClassifier(n_neighbors = 3)\n","f5550c27":"knn.fit(x_train,y_train)","04e13106":"print('With KNN (K=3) accuracy is: ',knn.score(x_test,y_test))","6d57f532":"from sklearn.svm import SVC\n\nsvm = SVC(random_state=1)\nsvm.fit(x_train,y_train)\n\nprint(\"accuracy of svm algo: \", svm.score(x_test,y_test)*100)","08f2d7ef":"from sklearn.tree import DecisionTreeClassifier\n\ndt = DecisionTreeClassifier(random_state=42)\ndt.fit(x_train,y_train)\n\nprint(\"Decision Tree Score: \", dt.score(x_test,y_test)*100)","4a676f54":"# Introduction \nWe will apply machine learning to this dataset with a total of 5572 messages. We will guess that the message received as a result of this application came from a man or a woman.\n\n\nContent:\n1. [Load and Check Data](#1)\n1. [Veriable Description](#2)\n1. [Categorical Variable](#3)\n1. [Missing Value](#4)\n1. [Cleaning](#5)\n    - [Regular Expression](#6)\n    - [Convert to lowercase](#7)\n    - [Split](#8)\n    - [Stopwords](#9)\n    - [Lemmatization](#10)\n1. [Bag Of Words](#11)\n1. [Modelling](#12)\n    - [Train Test Split](#13)\n    - [Naive Bayes](#14)\n    - [Simple Logistic Regression](#15)\n    - [KNN](#16)","38f71e7b":"## Lemmatization <a id=\"10\"><\/a>\nNow we need to find the roots of the separated words.","65ba9d10":"# SVM <a id=\"17\"><\/a>","79336bd3":"Let's look at the average length of normal and spam messages.","3b867049":"# Bag Of Words <a id=\"11\"><\/a>","19208691":"# Load and Check Data <a id=\"1\"><\/a>","59da4a4c":"## Cleaning <a id=\"5\"><\/a>\nFirst, to apply machine learning to the dataset, we equal normal messages to 1 and spam messages to 0.","fcfa41a1":"# Missing Value <a id=\"4\"><\/a>\nFirst of all, let's check if there is a missing value.","ac8ad869":"# Veriable Description <a id=\"2\"><\/a>\nThe data set consists of 2 columns.\n- Category: Indicates whether there is spam.\n- Message: Message post.","8626e852":"As can be seen from the above values, there is no missing data in this data set.","0a0954c8":"Let's apply the above example to all data.","4bbd09e2":"Like the example above, we can combine all the words and make them a sentence. In this way, we can now start machine learning.","6f92b55f":"## Split <a id=\"8\"><\/a>","a9efc265":"Let's apply the above example to all data.","4db93fd0":"Let's apply the above example to all data.","eb544022":"# Decision Tree Regression <a id=\"18\"><\/a>","f92088e9":"# KNN <a id=\"16\"><\/a>","5b454c09":"## Regular Expression <a id=\"6\"><\/a>","547ffcaf":"# Simple Logistic Regression <a id=\"15\"><\/a>","33922809":"# Modelling <a id=\"12\"><\/a>\n## Train Test Split <a id=\"13\"><\/a>","8a623f8b":"As seen above, there are 4825 normal messages and 747 spam messages.","af065d09":"## Stopwords <a id=\"9\"><\/a>\nHere it is:\n- It is to delete words that will not be useful for us while machine learning .\n- \"the\",\"in\", \"at\" like words can be given as examples.","0db3b0c4":"Let's apply the above example to all data.","13c78a93":"Let's apply the above example to all data.","447d7e54":"# Naive Bayes <a id=\"14\"><\/a>","51a52f4b":"# Categorical Variable <a id=\"3\"><\/a>","1ed28630":"## Convert to lowercase <a id=\"7\"><\/a>"}}