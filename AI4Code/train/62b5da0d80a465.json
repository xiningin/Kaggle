{"cell_type":{"92dd41c3":"code","afe65835":"code","b6c8ae56":"code","4ec55a68":"code","6bcc9e49":"code","344e99db":"code","d10a1386":"code","cf0fa3d6":"code","e7a3362f":"code","298f059f":"code","629249ca":"code","f3175f65":"code","f64d1018":"code","b10d4369":"code","82d19536":"code","af957fb8":"code","98899590":"code","61fbbaae":"code","ae5090ad":"code","0fcae583":"code","2fd49b46":"code","f2b9df19":"code","f7d76f95":"code","c824b629":"code","32a32d14":"code","b5c2e249":"markdown","30b44803":"markdown","25cab1a6":"markdown","fd5c9318":"markdown","203a5aae":"markdown","67940cf7":"markdown","8a71152c":"markdown","639be8d1":"markdown","7c52537e":"markdown"},"source":{"92dd41c3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","afe65835":"import pandas as pd\nimport numpy as np\nimport datetime\nfrom sklearn.model_selection import train_test_split\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport tensorflow as tf\nfrom tensorflow import keras","b6c8ae56":"\ndf1 = pd.read_csv('\/kaggle\/input\/jena-climate-2009-2016\/jena_climate_2009_2016.csv',index_col = None)\ndf1","4ec55a68":"df1.shape","6bcc9e49":"df1['wv (m\/s)']=df1['wv (m\/s)'].replace(-9999.00, 0)\ndf1['max. wv (m\/s)']=df1['max. wv (m\/s)'].replace(-9999.00, 0)","344e99db":"df1.describe().transpose()","d10a1386":"corr = df1.corr()\nax = sns.heatmap(\n    corr, \n    vmin=-1, vmax=1, center=0,\n    cmap=sns.diverging_palette(20, 220, n=200),\n    square=True\n)\nax.set_xticklabels(\n    ax.get_xticklabels(),\n    rotation=45,\n    horizontalalignment='right'\n);","cf0fa3d6":"print(df1[df1[\"Date Time\"]=='31.12.2014 23:50:00'].index.values)\nprint(df1[df1[\"Date Time\"]=='31.12.2015 23:50:00'].index.values)","e7a3362f":"df=df1","298f059f":"df=df.iloc[:, [0, 1,2,6,8,9,11,12]]\ndf","629249ca":"date_time = pd.to_datetime(df.pop('Date Time'), format='%d.%m.%Y %H:%M:%S')\ndf","f3175f65":"timestamp_s = date_time.map(datetime.datetime.timestamp)\nday = 24*60*60\nyear = (365.2425)*day\n\ndf['Day sin'] = np.sin(timestamp_s * (2 * np.pi \/ day))\ndf['Day cos'] = np.cos(timestamp_s * (2 * np.pi \/ day))\ndf['Year sin'] = np.sin(timestamp_s * (2 * np.pi \/ year))\ndf['Year cos'] = np.cos(timestamp_s * (2 * np.pi \/ year))","f64d1018":"df.head()","b10d4369":"split_fraction = 0.75075\ntrain_split = int(split_fraction * int(df.shape[0]))\n\nstep = 6\npast = 720\nfuture = 72\nbatch_size = 256\nepochs = 8\n\ndata_mean = df[:train_split].mean(axis=0)\ndata_std = df[:train_split].std(axis=0)\n\ndf=(df-data_mean)\/data_std\n\n","82d19536":"#df = normalize(df.values, train_split)\ndf=df.values\ndf = pd.DataFrame(df)\ndf.head()\n\ntrain_data = df.loc[0 : train_split - 1]\nval_data = df.loc[train_split:]","af957fb8":"train_data.head()","98899590":"start = past + future\nend = start + train_split\n\nx_train = train_data[[i for i in range(11)]].values\ny_train = df.iloc[start:end][[1]]\n\nsequence_length = int(past \/ step)","61fbbaae":"print('X_train shape == {}.'.format(x_train.shape))\nprint('y_train shape == {}.'.format(y_train.shape))","ae5090ad":"dataset_train = keras.preprocessing.timeseries_dataset_from_array(\n    x_train,\n    y_train,\n    sequence_length=sequence_length,\n    sampling_rate=step,\n    batch_size=batch_size,\n)","0fcae583":"x_end = len(val_data) - past - future\n\nlabel_start = train_split + past + future\n\nx_val = val_data.iloc[:x_end][[i for i in range(11)]].values\ny_val = df.iloc[label_start:][[1]]\n\ndataset_val = keras.preprocessing.timeseries_dataset_from_array(\n    x_val,\n    y_val,\n    sequence_length=sequence_length,\n    sampling_rate=step,\n    batch_size=batch_size,\n)\n\n\nfor batch in dataset_train.take(1):\n    inputs, targets = batch\n\nprint(\"Input shape:\", inputs.numpy().shape)\nprint(\"Target shape:\", targets.numpy().shape)","2fd49b46":"from keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import LSTM\nfrom keras.layers import Dropout\nfrom keras.optimizers import Adam\nfrom keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint, TensorBoard\n","f2b9df19":"learning_rate = 0.001\ninputs = keras.layers.Input(shape=(inputs.shape[1], inputs.shape[2]))\nlstm_out = keras.layers.LSTM(48)(inputs)\noutputs = keras.layers.Dense(1)(lstm_out)\n\nmodel = keras.Model(inputs=inputs, outputs=outputs)\nmodel.compile(optimizer=keras.optimizers.Adam(learning_rate=learning_rate), loss=\"mse\")\nmodel.summary()","f7d76f95":"%%time\npath_checkpoint = \"model_checkpoint.h5\"\nes_callback = keras.callbacks.EarlyStopping(monitor=\"val_loss\", min_delta=0, patience=5)\n\nmodelckpt_callback = keras.callbacks.ModelCheckpoint(\n    monitor=\"val_loss\",\n    filepath=path_checkpoint,\n    verbose=1,\n    save_weights_only=True,\n    save_best_only=True,\n)\n\nhistory = model.fit(\n    dataset_train,\n    epochs=epochs,\n    validation_data=dataset_val,\n    callbacks=[es_callback, modelckpt_callback],\n)","c824b629":"def visualize_loss(history, title):\n    loss = history.history[\"loss\"]\n    val_loss = history.history[\"val_loss\"]\n    epochs = range(len(loss))\n    plt.figure()\n    \n    plt.plot(epochs, loss, \"b\", label=\"Training loss\")\n    plt.plot(epochs, val_loss, \"r\", label=\"Validation loss\")\n    plt.title(title)\n    plt.xlabel(\"Epochs\")\n    plt.ylabel(\"Loss\")\n    plt.legend()\n    plt.show()\n\n\nvisualize_loss(history, \"Training and Validation Loss\")","32a32d14":"std=data_std[1]\navg=data_mean[1]\n   \ndef show_plot(plot_data, delta, title):\n    labels = [\"History\", \"True Future\", \"Model Prediction\"]\n    marker = [\".-\", \"rx\", \"go\"]\n    time_steps = list(range(-(plot_data[0].shape[0]), 0))\n    if delta:\n        future = delta\n    else:\n        future = 0\n\n    plt.title(title)\n    for i, val in enumerate(plot_data):\n        if i:\n            plt.plot(future, plot_data[i], marker[i], markersize=10, label=labels[i])\n        else:\n            plt.plot(time_steps, plot_data[i].flatten(), marker[i], label=labels[i])\n    plt.legend()\n    plt.xlim([time_steps[0], (future + 5) * 2])\n    plt.xlabel(\"Time-Step\")\n    plt.show()\n    return\n\n\nfor x, y in dataset_val.take(20):\n    p=x[0][:, 1].numpy()\n    q=y[0].numpy()\n    p=p*std+avg\n    q=q*std+avg\n    pred=model.predict(x)[0]\n    pred=(pred*std + avg)\n    \n    show_plot(\n        [p, q, pred],\n        12,\n        \"Single Step Prediction\",\n    )","b5c2e249":"**To know the indices of point of split**","30b44803":"**I am suing ModelCheckpoint to save checkpoints, and the EarlyStopping to stop training when the validation loss is not longer improving.**","25cab1a6":"**Split fraction to use 6 yrs of training data**","fd5c9318":"**Replace -9999 with 0**","203a5aae":"**Sin-Cos Extraction**","67940cf7":"**Normalising the Data**","8a71152c":"**Extract Date-Time column**","639be8d1":"**Removing the redundant features**","7c52537e":"**Tarining is lower then validation Loass, it means the model is slightly over fittting**"}}