{"cell_type":{"401c8d23":"code","d23bea23":"code","1a1565af":"code","75006d79":"code","3617492c":"code","3df74475":"code","a8e07257":"code","560966e5":"code","42cea0b5":"code","d3e8c36c":"code","f486da81":"code","7d8a7828":"code","8e2c04de":"code","9dcf42a2":"code","0d5ba0bc":"code","307abf61":"markdown","47f69edc":"markdown","f5085c45":"markdown","febe0e7a":"markdown","87344ee4":"markdown","c563ee90":"markdown","059c2e70":"markdown","f8d38226":"markdown","8599d198":"markdown","7c93ca12":"markdown","41706a2a":"markdown","1eff50b4":"markdown","a5ca8135":"markdown","ae921916":"markdown","0bd1958f":"markdown","9f74c845":"markdown","3a2ffc71":"markdown","ebca1c39":"markdown"},"source":{"401c8d23":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d23bea23":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler","1a1565af":"train = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/train.csv\")\ntest = pd.read_csv(\"\/kaggle\/input\/tabular-playground-series-sep-2021\/test.csv\")","75006d79":"train.head()","3617492c":"test.head()","3df74475":"print(train.shape)\nprint(test.shape)","a8e07257":"train = train.fillna(train.mean())\ntest = test.fillna(train.mean())","560966e5":"train.drop(\"id\",axis=1,inplace=True)\ntest_id = test['id']\ntest.drop(\"id\",axis=1,inplace=True)","42cea0b5":"y = train['claim']\ntrain.drop(\"claim\",axis=1,inplace=True)\nfrom sklearn.preprocessing import StandardScaler\nss = StandardScaler()","d3e8c36c":"scaled_train = ss.fit_transform(train)\nscaled_test = ss.transform(test)","f486da81":"import seaborn as sns\ntrain.corr()","7d8a7828":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(scaled_train, y, test_size=0.3)","8e2c04de":"import catboost as cb\ncb_model = cb.CatBoostClassifier(learning_rate = 0.19, iterations=1000)\ncb_model.fit(scaled_train,y)\npred = cb_model.predict_proba(scaled_test)","9dcf42a2":"pred = cb_model.predict_proba(scaled_test)\nnew = pred[:,1]\noutput = pd.DataFrame({'id': test_id, 'claim': new})\noutput.to_csv('submission_tp5.csv', index=False)","0d5ba0bc":"#from catboost import CatBoostRegressor\n#from xgboost import XGBRegressor\n#from lightgbm import LGBMRegressor\n#from sklearn.ensemble import VotingRegressor\n\n#estimator = []\n#estimator.append(('cb', CatBoostRegressor()))\n#estimator.append(('XGB', XGBRegressor()))\n#estimator.append(('lgb', LGBMRegressor()))\n  \n# Voting Classifier with hard voting\n#vot_hard = VotingRegressor(estimators = estimator)\n#vot_hard.fit(scaled_train, y)\n#y_pred = vot_hard.predict(scaled_test)","307abf61":"## Train Test Splitting","47f69edc":"## Importing all the required libraries","f5085c45":"Make sure to use the `predict_proba()` function, if you want to find out the probabaility of it being 1.\n\nIf you use the `predict() ` function, then the model will be doing binary classification.\n\nThe `predict()` function gave a score of around 0.75, whereas `predict_proba()` gave a score of around 0.79545.","febe0e7a":"## Handling Null Values\n\n![](https:\/\/miro.medium.com\/max\/1400\/1*2L2lSHCYCYQTDoHTy2o9Kw.png)","87344ee4":"## ROC-AUC Score\n\nROC and Precision Recall curves are tools that helps in the interpretation of how our classfier model is working. ROC stands for `Reciever Operating Characterestic` curves. Generally ROC is used for datasets with balanced data and precision recall curve when we have imbalanced dataset.\n\nFor understanding how ROC and Precision Recall Curves work, first you need to understand about `Confusion Matrix`.","c563ee90":"### Confusion Matrix\n\nConfusion matrix also refrerred to as error matrix, allows visualization of the performance of algorithm. It shows the way in which our model is actually confused. It segregates the error into 4 classifications - True Positive, True Negative, False Positive, False Negative.\n\nThis is how a confusion matrix generally looks like:\n\n![](https:\/\/miro.medium.com\/max\/1400\/1*85t6zbUiQA0fotnhDaJLaA.png)","059c2e70":"\nSo basically what ROC curve will contain is \n\n1. `False Positive Rate ` on x-axis\n2. `True Positive Rate ` on y-axis\n3. Variation of Probability Threshold(0-1)\n\n`True Positive Rate ` gives us information about how good the model is predicting when the actual value is positive.\n\n`False Positive Rate ` gives us information about how often a class is predicted as positive when it is actually negative. `FPR ` is referred to as ` 1 - specificity ` .","f8d38226":"# STEPS FOLLOWED\n\n* Importing the Required libraries\n* Importing the Datasets\n* Learning more about the Dataset\n* Exploratory Data Analysis\n* Handling Missing Values\n* Feature Scaling of Data\n* Understanding Evaluation Metric\n* Model Validation and Prediction\n* Making the final submission","8599d198":"#### This submission gave a score of 0.79545","7c93ca12":"## Feature Scaling of Data\n\nMost of the values seem to be scaled already. But some columns like f9 do seem to have very high values.\n\n![](https:\/\/www.gstatic.com\/education\/formulas2\/355397047\/en\/z_score.svg)","41706a2a":"## Getting info about the Training and Test Data","1eff50b4":"## Exploratory Data Analysis","a5ca8135":"## Intepretations from ROC curve\n\n1. ROC curve shows us the tradeoff between sensitivity(TPR) and specificity(1-FPR). \n\n2. Classifiers that give curves closer to the top-left corner indicate a better performance.\n\n3. Smaller x-axis on the ROC plot means we have low false positive values and high true negative values.\n\n4. Higher values on y-axis means we have high true positive values and lower false negative values.\n\n5. AUC(Area under ROC curve) is used to summarize the performance of the classification model. It is equivalent to the probability that a randomly chosen positive instance is ranked higher than a randomly chosen negative instance.","ae921916":"## Next Step:\n\nParameter tuning of Catboost\n\nFeature Selection Techniques","0bd1958f":"![](https:\/\/www.researchgate.net\/profile\/Md-Ashraful-Amin\/publication\/220176738\/figure\/fig4\/AS:669969142534168@1536744499664\/The-confusion-matrix-left-and-the-calculation-of-true-positive-rate-false-positive.png)","9f74c845":"# Extras\n\n\n## Trying out with Voting Classifier \n\nUsed 3 different models and stacked using Voting Classifier.\n\nGot an accuracy around 0.78.","3a2ffc71":"## Importing the Training and Test dataset","ebca1c39":"There are 957,919 rows and 120 columns in the training data.\n\nThere are 493,474 rows and 119 columns in the test data.\n\nUsing the features ranging from f1-f118, we were asked to predict the claim(0\/1) - **CLASSIFICATION PROBLEM**\n\nAll the features are anonymized, so we wont be able to get much insights based on the features."}}