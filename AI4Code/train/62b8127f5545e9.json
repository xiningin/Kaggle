{"cell_type":{"f1d17111":"code","eaf11a13":"code","1629844a":"code","566d2699":"code","c61cc973":"code","7667c2d6":"code","56782652":"code","14aca4ec":"code","5cdc7d92":"code","09a725a0":"code","3b240851":"code","9d60b244":"code","c5907559":"code","77c00d99":"code","88e11eb5":"code","2a1e4706":"code","fc39bc06":"code","33a092d1":"code","5fc9d896":"code","86638192":"code","bb612513":"code","0df1b6e7":"code","bd3a3238":"code","e0667d42":"code","dfae13c2":"code","0dc69bb3":"code","5e3b1c2a":"code","9f61729e":"code","b77526ad":"code","8b241eae":"code","2734d8b8":"code","228ec50d":"code","2f99483b":"code","6b38e280":"code","e2d3720f":"code","02b23dd3":"code","c6b9c5a9":"code","42ad7d13":"code","d9698ab8":"code","ae4fb7ac":"code","efd7cbb2":"code","903d12e6":"code","f57760b6":"code","c4e11e31":"code","603d6b56":"code","78920f1e":"code","a4964418":"code","0e1041a8":"code","e5f6931f":"code","63b4728c":"code","041a2fc8":"code","57d8ee7e":"code","c2c0eabc":"code","97d15d7b":"code","38383d4a":"code","4fd23b5e":"markdown"},"source":{"f1d17111":"\nimport pandas as pd\nimport io\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\ntrain = pd.read_csv('..\/input\/basic-needs-basic-rights-kenya\/data.csv', delimiter=',')\ntest = pd.read_csv('..\/input\/basic-needs-basic-rights-kenya\/data', delimiter=',' )\nSample_submission = pd.read_csv('\/kaggle\/input\/Sample_submission.csv', delimiter=',', nrows = nRowsRead)\n\nSample_submission.dataframeName = 'Sample_submission.csv'\ntrain.dataframeName = 'Trainhot.csv'\ntest.dataframeName = 'Test.csv'\n\n      \n\n\n","eaf11a13":"!pip install tensorflow==1.15.0 ","1629844a":"\nimport pandas as pd\nimport tensorflow as tf\nimport tensorflow_hub as hub\nfrom datetime import datetime\nfrom sklearn.model_selection import train_test_split\nimport os\n\nprint(\"tensorflow version : \", tf.__version__)\nprint(\"tensorflow_hub version : \", hub.__version__)","566d2699":"\n!pip install bert-tensorflow ","c61cc973":"#Importing BERT modules\nimport bert\nfrom bert import run_classifier\nfrom bert import optimization\nfrom bert import tokenization","7667c2d6":"from sklearn.model_selection import train_test_split\nfrom sklearn import preprocessing ","56782652":"train","14aca4ec":"test.head() ","5cdc7d92":"train.isnull().values.any()\nprint(train['text'].isnull().sum())\n","09a725a0":"print(train['text'].isnull().sum())\nprint(train['label'].isnull().sum())\n","3b240851":"train.shape\n","9d60b244":"le = preprocessing.LabelEncoder()\ntrain['label'] = train['label'].astype(str)\ntrain['label']= le.fit_transform(train.label.values)\n\n","c5907559":"integer_mapping = {l: i for i, l in enumerate(le.classes_)}\ninteger_mapping","77c00d99":"train, val = train_test_split(train, test_size = 0.2, random_state = 100)\ntrain.head() ","88e11eb5":"print(train['text'].apply(str).map(len).max()) \nprint(test['text'].apply(str).map(len).max())","2a1e4706":"#Test set sample\ntest.head()","fc39bc06":"print(\"Training Set Shape :\", train.shape)\nprint(\"Validation Set Shape :\", val.shape)\nprint(\"Test Set Shape :\", test.shape)","33a092d1":"#Features in the dataset\ntrain.columns","5fc9d896":"#unique classes\ntrain['label'].unique()","86638192":"#Distribution of classes\ntrain['label'].value_counts().plot(kind = 'bar')","bb612513":"\nDATA_COLUMN = 'text'\nLABEL_COLUMN = 'label'\n# The list containing all the classes (train['SECTION'].unique())\nlabel_list = [0, 1, 2, 3]","0df1b6e7":"type(DATA_COLUMN)","bd3a3238":"train_InputExamples = train.apply(lambda x: bert.run_classifier.InputExample(guid=None,\n                                                                   text_a = x[DATA_COLUMN], \n                                                                   text_b = None, \n                                                                   label = x[LABEL_COLUMN]), axis = 1)\n\nval_InputExamples = val.apply(lambda x: bert.run_classifier.InputExample(guid=None, \n                                                                   text_a = x[DATA_COLUMN], \n                                                                   text_b = None, \n                                                                   label = x[LABEL_COLUMN]), axis = 1)","e0667d42":"train_InputExamples","dfae13c2":"val_InputExamples","0dc69bb3":"print(\"Row 0 - guid of training set : \", train_InputExamples.iloc[0].guid)\nprint(\"\\n__________\\nRow 0 - text_a of training set : \", train_InputExamples.iloc[0].text_a)\nprint(\"\\n__________\\nRow 0 - text_b of training set : \", train_InputExamples.iloc[0].text_b)\nprint(\"\\n__________\\nRow 0 - label of training set : \", train_InputExamples.iloc[0].label)","5e3b1c2a":"BERT = \"https:\/\/tfhub.dev\/google\/bert_uncased_L-12_H-768_A-12\/1\"\nbert_module = hub.Module(BERT)\n\nprint('Download complete')\n#model = BERT_CLASS.from_pretrain(PRE_TRAINED_MODEL_NAME_OR_PATH, cache_dir=None)","9f61729e":"def create_tokenizer_from_hub_module():\n  \"\"\"Get the vocab file and casing info from the Hub module.\"\"\"\n  with tf.Graph().as_default():\n    bert_module = hub.Module(BERT)\n    tokenization_info = bert_module(signature=\"tokenization_info\", as_dict=True)\n    with tf.Session() as sess:\n      vocab_file, do_lower_case = sess.run([tokenization_info[\"vocab_file\"],\n                                            tokenization_info[\"do_lower_case\"]])\n      \n  return bert.tokenization.FullTokenizer(\n      vocab_file=vocab_file, do_lower_case=do_lower_case)\n\ntokenizer = create_tokenizer_from_hub_module()","b77526ad":"#Here is what the tokenised sample of the first training set observation looks like\nprint(tokenizer.tokenize(train_InputExamples.iloc[0].text_a))","8b241eae":"train.columns","2734d8b8":"label_list","228ec50d":"tokenizer","2f99483b":"train_InputExamples","6b38e280":"# We'll set sequences to be at most 128 tokens long.\nMAX_SEQ_LENGTH = 196","e2d3720f":"type(MAX_SEQ_LENGTH)","02b23dd3":"train","c6b9c5a9":"# Convert our train and validation features to InputFeatures that BERT understands.\ntrain_features = bert.run_classifier.convert_examples_to_features(train_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)\n\nval_features = bert.run_classifier.convert_examples_to_features(val_InputExamples, label_list, MAX_SEQ_LENGTH, tokenizer)","42ad7d13":"#Example on first observation in the training set\nprint(\"Sentence : \", train_InputExamples.iloc[0].text_a)\nprint(\"-\"*30)\nprint(\"Tokens : \", tokenizer.tokenize(train_InputExamples.iloc[0].text_a))\nprint(\"-\"*30)\nprint(\"Input IDs : \", train_features[0].input_ids)\nprint(\"-\"*30)\nprint(\"Input Masks : \", train_features[0].input_mask)\nprint(\"-\"*30)\nprint(\"Segment IDs : \", train_features[0].segment_ids)","d9698ab8":"def create_model(is_predicting, input_ids, input_mask, segment_ids, labels,\n                 num_labels):\n  \n  bert_module = hub.Module(\n      BERT,\n      trainable=True)\n  bert_inputs = dict(\n      input_ids=input_ids,\n      input_mask=input_mask,\n      segment_ids=segment_ids)\n  bert_outputs = bert_module(\n      inputs=bert_inputs,\n      signature=\"tokens\",\n      as_dict=True)\n\n  # Use \"pooled_output\" for classification tasks on an entire sentence.\n  # Use \"sequence_outputs\" for token-level output.\n  output_layer = bert_outputs[\"pooled_output\"]\n\n  hidden_size = output_layer.shape[-1].value\n\n  # Create our own layer to tune for politeness data.\n  output_weights = tf.get_variable(\n      \"output_weights\", [num_labels, hidden_size],\n      initializer=tf.truncated_normal_initializer(stddev=0.02))\n\n  output_bias = tf.get_variable(\n      \"output_bias\", [num_labels], initializer=tf.zeros_initializer())\n  with tf.variable_scope(\"loss\"):\n\n    # Dropout helps prevent overfitting\n    output_layer = tf.nn.dropout(output_layer, keep_prob=0.9)\n\n    logits = tf.matmul(output_layer, output_weights, transpose_b=True)\n    logits = tf.nn.bias_add(logits, output_bias)\n    log_probs = tf.nn.log_softmax(logits, axis=-1)\n\n    # Convert labels into one-hot encoding\n    one_hot_labels = tf.one_hot(labels, depth=num_labels, dtype=tf.float32)\n\n    predicted_labels = tf.squeeze(tf.argmax(log_probs, axis=-1, output_type=tf.int32))\n    # If we're predicting, we want predicted labels and the probabiltiies.\n    if is_predicting:\n        \n        return (predicted_labels, log_probs)\n\n    # If we're train\/eval, compute loss between predicted and actual label\n    per_example_loss = -tf.reduce_sum(one_hot_labels * log_probs, axis=-1)\n    loss = tf.reduce_mean(per_example_loss)\n    return (loss, predicted_labels, log_probs)","ae4fb7ac":"#A function that adapts our model to work for training, evaluation, and prediction.\n\n# model_fn_builder actually creates our model function\n# using the passed parameters for num_labels, learning_rate, etc.\ndef model_fn_builder(num_labels, learning_rate, num_train_steps,\n                     num_warmup_steps):\n  \"\"\"Returns `model_fn` closure for TPUEstimator.\"\"\"\n  def model_fn(features, labels, mode, params):  # pylint: disable=unused-argument\n    \"\"\"The `model_fn` for TPUEstimator.\"\"\"\n\n    input_ids = features[\"input_ids\"]\n    input_mask = features[\"input_mask\"]\n    segment_ids = features[\"segment_ids\"]\n    label_ids = features[\"label_ids\"]\n\n    is_predicting = (mode == tf.estimator.ModeKeys.PREDICT)\n    \n    # TRAIN and EVAL\n    if not is_predicting:\n\n      (loss, predicted_labels, log_probs) = create_model(\n        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n\n      train_op = bert.optimization.create_optimizer(\n          loss, learning_rate, num_train_steps, num_warmup_steps, use_tpu=False)\n\n      # Calculate evaluation metrics. \n      def metric_fn(label_ids, predicted_labels):\n        accuracy = tf.metrics.accuracy(label_ids, predicted_labels)\n        true_pos = tf.metrics.true_positives(\n            label_ids,\n            predicted_labels)\n        true_neg = tf.metrics.true_negatives(\n            label_ids,\n            predicted_labels)   \n        false_pos = tf.metrics.false_positives(\n            label_ids,\n            predicted_labels)  \n        false_neg = tf.metrics.false_negatives(\n            label_ids,\n            predicted_labels)\n        \n        return {\n            \"eval_accuracy\": accuracy,\n            \"true_positives\": true_pos,\n            \"true_negatives\": true_neg,\n            \"false_positives\": false_pos,\n            \"false_negatives\": false_neg\n            }\n\n      eval_metrics = metric_fn(label_ids, predicted_labels)\n\n      if mode == tf.estimator.ModeKeys.TRAIN:\n        return tf.estimator.EstimatorSpec(mode=mode,\n          loss=loss,\n          train_op=train_op)\n      else:\n          return tf.estimator.EstimatorSpec(mode=mode,\n            loss=loss,\n            eval_metric_ops=eval_metrics)\n    else:\n      (predicted_labels, log_probs) = create_model(\n        is_predicting, input_ids, input_mask, segment_ids, label_ids, num_labels)\n\n      predictions = {\n          'probabilities': log_probs,\n          'labels': predicted_labels\n      }\n      return tf.estimator.EstimatorSpec(mode, predictions=predictions)\n\n  # Return the actual model function in the closure\n  return model_fn","efd7cbb2":"# Compute train and warmup steps from batch size\n# These hyperparameters are copied from this colab notebook (https:\/\/colab.sandbox.google.com\/github\/tensorflow\/tpu\/blob\/master\/tools\/colab\/bert_finetuning_with_cloud_tpus.ipynb)\nBATCH_SIZE = 32\nLEARNING_RATE = 2e-5\nNUM_TRAIN_EPOCHS = 5.0\n# Warmup is a period of time where the learning rate is small and gradually increases--usually helps training.\nWARMUP_PROPORTION = 0.1\n# Model configs\nSAVE_CHECKPOINTS_STEPS = 300\nSAVE_SUMMARY_STEPS = 100\n\n# Compute train and warmup steps from batch size\nnum_train_steps = int(len(train_features) \/ BATCH_SIZE * NUM_TRAIN_EPOCHS)\nnum_warmup_steps = int(num_train_steps * WARMUP_PROPORTION)\n\n# Specify output directory and number of checkpoint steps to save\nrun_config = tf.estimator.RunConfig(\n    model_dir= \"C:\/Users\/121\/Documents\/zindi_\",\n    save_summary_steps=SAVE_SUMMARY_STEPS,\n    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)\n\n# Specify output directory and number of checkpoint steps to save\nrun_config = tf.estimator.RunConfig(\n    model_dir= \"C:\/Users\/121\/Documents\/zindi_\" ,\n    save_summary_steps=SAVE_SUMMARY_STEPS,\n    save_checkpoints_steps=SAVE_CHECKPOINTS_STEPS)","903d12e6":"#Initializing the model and the estimator\nmodel_fn = model_fn_builder(\n  num_labels=len(label_list),\n  learning_rate=LEARNING_RATE,\n  num_train_steps=num_train_steps,\n  num_warmup_steps=num_warmup_steps)\n\nestimator = tf.estimator.Estimator(\n  model_fn=model_fn,\n  config=run_config,\n  params={\"batch_size\": BATCH_SIZE})","f57760b6":"# Create an input function for training. drop_remainder = True for using TPUs.\ntrain_input_fn = bert.run_classifier.input_fn_builder(\n    features=train_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=True,\n    drop_remainder=False)\n\n# Create an input function for validating. drop_remainder = True for using TPUs.\nval_input_fn = run_classifier.input_fn_builder(\n    features=val_features,\n    seq_length=MAX_SEQ_LENGTH,\n    is_training=False,\n    drop_remainder=False)","c4e11e31":"#Training the model\nfrom datetime import datetime\nprint(f'Beginning Training!')\ncurrent_time = datetime.now()\nestimator.train(input_fn=train_input_fn, max_steps=num_train_steps)\nprint(\"Training took time \", datetime.now() - current_time)","603d6b56":"#Evaluating the model with Validation set\nestimator.evaluate(input_fn=val_input_fn, steps=None)","78920f1e":"# A method to get predictions\ndef getPrediction(in_sentences):\n  #A list to map the actual labels to the predictions\n  labels = [\"Alcohol\",\"Depression\", \"Drugs\",\"Suicide\"]\n  \n\n  #Transforming the test data into BERT accepted form\n  input_examples = [run_classifier.InputExample(guid=\"\", text_a = x, text_b = None, label = 0) for x in in_sentences] \n  \n  #Creating input features for Test data\n  input_features = run_classifier.convert_examples_to_features(input_examples, label_list, MAX_SEQ_LENGTH, tokenizer)\n\n  #Predicting the classes \n  predict_input_fn = run_classifier.input_fn_builder(features=input_features, seq_length=MAX_SEQ_LENGTH, is_training=False, drop_remainder=False)\n  predictions = estimator.predict(predict_input_fn)\n  return [(sentence, prediction['probabilities'],prediction['labels'], labels[prediction['labels']]) for sentence, prediction in zip(in_sentences, predictions)]","a4964418":"pred_sentences = list(test['text'])","0e1041a8":"predictions = getPrediction(pred_sentences)","e5f6931f":"predictions[1]","63b4728c":"predictions[1][1][1]","041a2fc8":"Depression = list()\nAlcohol = list()\nSuicide = list()\nDrugs = list()\nfor i in range(len(predictions)):\n  Depression.append(predictions[i][1][1])\n  Alcohol.append(predictions[i][1][0])\n  Suicide.append(predictions[i][1][3])\n  Drugs.append(predictions[i][1][2]) \n\n\nsample_submission['Depression']= Depression\nsample_submission['Alcohol'] = Alcohol\nsample_submission['Suicide']= Suicide\nsample_submission['Drugs'] = Drugs \n","57d8ee7e":"Depression","c2c0eabc":"sample_submission","97d15d7b":"#sample_submission.reset_index(drop=True, inplace=True)","38383d4a":"\nfrom google.colab import files\n\n\nsample_submission.to_csv('SampleSubmission.csv' , index=False)\nfiles.download('SAmpleSubmission.csv')","4fd23b5e":"# Run the code in google colab ****"}}