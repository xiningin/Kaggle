{"cell_type":{"1fec3f2e":"code","a3f7a4b6":"code","864d1b6a":"code","1dcc8fcd":"code","e754066d":"code","17d592a1":"code","0b7bd6af":"code","bd2fd5fb":"code","0e717c8d":"code","de703c40":"code","d9a3e129":"code","9e2f3dd5":"code","7065f934":"code","13601128":"code","66fe1f43":"code","aefa56d0":"code","bc6fcbed":"markdown","f6654538":"markdown","42bf4f69":"markdown"},"source":{"1fec3f2e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a3f7a4b6":"df=pd.read_json(\"\/kaggle\/input\/news-headlines-dataset-for-sarcasm-detection\/Sarcasm_Headlines_Dataset.json\",lines=True)\ndf.head()","864d1b6a":"del df['article_link'] \ndf.head()","1dcc8fcd":"import missingno as msno\nmsno.matrix(df)","e754066d":"import seaborn as sns\nsns.countplot(df['is_sarcastic'])","17d592a1":"import re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\nnltk.download('stopwords')\ncorpus = []\nfor i in range(0, len(df)):\n    text = re.sub('[^a-zA-Z]', ' ', df['headline'][i])\n    text = text.lower()\n    text = text.split()\n    \n    text = [ps.stem(word) for word in text if not word in stopwords.words('english')]\n    text = ' '.join(text)\n    corpus.append(text)","0b7bd6af":"corpus","bd2fd5fb":"words=[]\nfor word in corpus:\n    words.append(word.split())\nwords\n    ","0e717c8d":"leng=[]\nfor word in words:\n    leng.append(len(word))\nprint(max(leng)) ","de703c40":"from tensorflow.keras.preprocessing.text import one_hot","d9a3e129":"sent=[]\nfor i in range(len(df)):\n    sent.append(df['headline'][i])\nvoc_size=10000\nonehot_repr=[one_hot(words,voc_size)for words in sent] \nprint(onehot_repr)","9e2f3dd5":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.layers import Dropout","7065f934":"sent_len=40\nembedded_sent=pad_sequences(onehot_repr,padding='pre',maxlen=sent_len)\nprint(embedded_sent)","13601128":"from tensorflow.keras import layers\nfrom keras.layers import Bidirectional\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Flatten, Bidirectional, GlobalMaxPool1D\nfrom keras.models import Model, Sequential\ndim=20\nmodel=Sequential()\nmodel.add(Embedding(voc_size,10,input_length=sent_len))\nmodel.add(Bidirectional(LSTM(128, return_sequences = True)))\nmodel.add(GlobalMaxPool1D())\nmodel.add(Dense(40, activation=\"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(20, activation=\"relu\"))\nmodel.add(Dropout(0.5))\nmodel.add(Dense(1, activation=\"sigmoid\"))\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n\nbatch_size = 100\nepochs = 5\nhistory = model.fit(X, Y, batch_size=batch_size, epochs=epochs, validation_split=0.2)","66fe1f43":"Y=df['is_sarcastic']\nX=pd.DataFrame(embedded_sent)","aefa56d0":"Y.head()","bc6fcbed":"# Data Cleaning","f6654538":"As we can see there are no missing values to handle so we will go ahead with EDA.","42bf4f69":"Here we can see dataset is almost balanced as both values have equal no of datasets."}}