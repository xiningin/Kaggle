{"cell_type":{"b83f9be4":"code","f4808f17":"code","f95fb928":"code","89c763b8":"code","a193f81f":"code","6673281b":"code","fe46f8ce":"code","1dcbd9d8":"code","63890c19":"code","df17f362":"code","cab2d656":"code","7d67273e":"code","1c614956":"code","2f748bb4":"code","e87ed8ce":"code","3c9e33f6":"code","c3d994f2":"code","10906696":"code","a42e3ade":"code","a3afea0a":"code","fa72f938":"code","dca215d4":"code","69c26185":"code","d163bfab":"code","79032270":"code","7cfc27f1":"code","6ff70ed2":"code","c037c071":"code","5f528762":"code","79be5978":"code","e1062b02":"code","1e8cd4ef":"code","ab3d4473":"code","275ee410":"code","9e942d8d":"code","0c6b3a5d":"code","dc918b5d":"code","b1b69086":"code","70df51f4":"code","ed76e475":"code","9329e07d":"code","f7fd8006":"code","ab7e4a28":"code","2352fbb6":"code","93e05cda":"code","25c1d032":"code","be17178b":"code","ee412c86":"code","ed130a9b":"code","9c60bf0b":"code","a5c505ed":"code","91adc8f0":"code","4c68f449":"code","7c8ad36f":"code","88348621":"code","0908683b":"code","477f1c5f":"code","411e0725":"code","7d6ba948":"code","15602688":"code","5fb33f97":"code","7148d49c":"code","d71cf1f7":"code","b9b501de":"code","2d682a95":"code","1bc20dda":"code","ad7b1b94":"code","e4d9c512":"code","673bc653":"code","d07750ed":"code","8cb56a2b":"code","712b8686":"code","e1039951":"code","d0d0749d":"code","21ab838b":"code","41272ec1":"code","dd45bb1d":"code","52cb1d20":"code","25f629d9":"code","3100c502":"code","421727b0":"code","0f8b83f6":"code","265094f6":"code","c950c9e3":"code","f3debc80":"code","3a44a1ca":"code","f81d7649":"code","1bdd968d":"code","3c6c23c7":"code","6ca57738":"code","d7b5f770":"code","f868dfa1":"code","dd8daab0":"code","7016fc52":"code","c6496985":"code","c6150f96":"code","fa39e5dd":"markdown","83b89781":"markdown","2ade4ddd":"markdown","a4e529c7":"markdown","68ad2f0a":"markdown","94d4a727":"markdown","980270d0":"markdown","02cff4ef":"markdown","1fb1e0ff":"markdown","e40695f0":"markdown","7ee97eed":"markdown","2dbca561":"markdown","31a47ad3":"markdown","a85d8ce9":"markdown","0018cdb0":"markdown","676b1a9c":"markdown","2b3bd333":"markdown","fa578709":"markdown","99b5df84":"markdown","2b1dbb15":"markdown","95e07bd6":"markdown","fa2a34c7":"markdown","b578a527":"markdown","01f1b2f2":"markdown","37b474ba":"markdown","c7ef5a53":"markdown","596a0fbb":"markdown","bae01b9b":"markdown","dcee483a":"markdown","64dcdeaf":"markdown","e8d5aedb":"markdown","cc19a549":"markdown","ac132892":"markdown","9e2eb8af":"markdown","8dfb3931":"markdown","8af106cb":"markdown","984a6307":"markdown","d9fdedf6":"markdown","2f0c97cf":"markdown","1a804e81":"markdown","e46b97a3":"markdown","4d2ed7d3":"markdown","dffe0542":"markdown","fdc9d4f0":"markdown","f2696483":"markdown","6f1b3fe2":"markdown","1afdc3eb":"markdown","f80d58a3":"markdown","096e84ee":"markdown"},"source":{"b83f9be4":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport copy\nimport random\nimport itertools\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\n%matplotlib inline","f4808f17":"shoppers = pd.read_csv(\"..\/input\/online-shoppers-intention\/online_shoppers_intention.csv\")","f95fb928":"type(shoppers)","89c763b8":"shoppers.head()","a193f81f":"shoppers.shape ## 12330 rows, 18 columns","6673281b":"shoppers.describe()","fe46f8ce":"shoppers.columns ## just the column names","1dcbd9d8":"shoppers.isnull().sum()","63890c19":"shoppers = shoppers.dropna(axis = 0)","df17f362":"shoppers.isnull().sum() ## no remaining null values","cab2d656":"shoppers.shape ## should be 14 rows less","7d67273e":"shoppers.info()","1c614956":"cat_cols = ['Month', 'OperatingSystems', 'Browser', 'Region',\n           'TrafficType', 'VisitorType', 'Weekend', 'Revenue']\nfor col in cat_cols:\n    shoppers[col] = shoppers[col].astype('category')","2f748bb4":"shoppers.dtypes ","e87ed8ce":"for col in cat_cols:\n    print(shoppers[col].unique())","3c9e33f6":"sum(shoppers['VisitorType'] == 'Other') ## 85 have this other type","c3d994f2":"len(shoppers.loc[shoppers['ProductRelated_Duration'] == -1, 'ProductRelated_Duration'])","10906696":"len(shoppers.loc[shoppers['Informational_Duration'] == -1, 'Informational_Duration'])","a42e3ade":"len(shoppers.loc[shoppers['Administrative_Duration'] == -1, 'Administrative_Duration'])","a3afea0a":"## okay so looks like the -1 are all in the same row so let's drop those rows\nshoppers = shoppers.loc[shoppers['Administrative_Duration'] != -1, ]","fa72f938":"## how many shoppers spent zero amount of time on the website - these ones maybe are\n## rounded down because they spent so little time on the site (<0)\nsum(shoppers[['ProductRelated_Duration',\n              'Informational_Duration',\n              'Administrative_Duration']].sum(axis = 1) == 0)","dca215d4":"plt.figure(figsize = (16,8))\nsns.countplot(x = \"Region\", data = shoppers, \n              order = shoppers['Region'].value_counts().index)\nplt.title(\"Site Visitors by Region\", fontsize = 16)\nplt.xlabel(\"Region\", fontsize = 16)\nplt.ylabel(\"Number of Visitors\", fontsize = 16)","69c26185":"shoppers.columns","d163bfab":"dur = shoppers[['Administrative_Duration', \n          'Informational_Duration', \n          'ProductRelated_Duration']].sum(axis = 1)","79032270":"site_duration = pd.DataFrame({\"VisitorType\": shoppers['VisitorType'],\n                           \"TotalDuration\": dur\n                           })","7cfc27f1":"plt.figure(figsize = (16,8))\nsns.boxplot('VisitorType', 'TotalDuration', data = site_duration)\nplt.title(\"Time on Site by Visitor Type\", fontsize = 16)\nplt.xlabel(\"Visitor Type\", fontsize = 16)\nplt.ylabel(\"Total Time on Site (seconds)\", fontsize = 16)\nplt.ylim(10, 10000)","6ff70ed2":"ret_duration = list(site_duration.loc[\n    site_duration['VisitorType'] == 'Returning_Visitor', 'TotalDuration'])","c037c071":"new_duration = list(site_duration.loc[\n    site_duration['VisitorType'] == 'New_Visitor', 'TotalDuration'])","5f528762":"import copy\nimport random","79be5978":"## write a permutation function for mean\n\ndef perm_mean(group_1, group_2, p): ## two lists and a numeric value for the number of permutations\n    \"\"\"Returns the p-value for a permutation test of difference in means between\n    two groups\"\"\"\n    \n    ## observed difference in means\n    obs_mean = np.abs(np.average(group_1) - np.average(group_2))\n    \n    ## pool the observations into a single list\n    pooled_groups = list(group_1 + group_2)\n    \n    ## make a copy that can be randomly shuffled for the permutations\n    pooled_copy = copy.copy(pooled_groups)\n    \n    ## a space to save permutation output\n    perm_means = []\n    \n    ## permutations\n    for i in range(0, p):\n        ## randomly shuffle the pooled observations\n        random.shuffle(pooled_copy)\n        \n        ## calculate differences in mean for each permutation\n        perm_means.append(\n            np.abs(np.average(\n                pooled_copy[0:len(group_1)]) - np.average(pooled_copy[len(group_1):])))\n\n    ## calculate the p-value as proportion of the permuted means that had a larger\n    ## difference in means than the observed difference in means\n    p_value = sum(perm_means >= obs_mean)\/p\n    \n    return p_value\n","e1062b02":"perm_mean(ret_duration, new_duration, 1000)","1e8cd4ef":"shoppers.columns","ab3d4473":"duration = list(shoppers['Administrative_Duration']) + list(\n    shoppers['ProductRelated_Duration']) + list(\n    shoppers['Informational_Duration'])","275ee410":"import itertools","9e942d8d":"duration_type = list(\n    itertools.repeat('Administrative', len(shoppers['Administrative_Duration']))) + list(\n    itertools.repeat('ProductRelated', len(shoppers['ProductRelated_Duration']))) + list(\n    itertools.repeat('Informational', len(shoppers['Informational_Duration'])))","0c6b3a5d":"visitor_type = list(shoppers['VisitorType'])*3","dc918b5d":"duration_info = pd.DataFrame({\"Visitor Type\": visitor_type,\n                           \"Duration Type\": duration_type,\n                           \"Duration\": duration})","b1b69086":"duration_info","70df51f4":"plt.figure(figsize = (16,8))\nsns.boxplot('Visitor Type', 'Duration', data = duration_info, hue = 'Duration Type')\nplt.title(\"Duration on Each Page Type by Visitor Type\", fontsize = 16)\nplt.xlabel(\"Visitor Type\", fontsize = 16)\nplt.ylabel(\"Duration on Page (seconds)\", fontsize = 16)\nplt.show()","ed76e475":"plt.figure(figsize = (16,8))\nsns.boxplot('Visitor Type', 'Duration', data = duration_info, hue = 'Duration Type')\nplt.title(\"Duration on Each Page Type by Visitor Type\", fontsize = 16)\nplt.xlabel(\"Visitor Type\", fontsize = 16)\nplt.ylabel(\"Duration on Page (seconds)\", fontsize = 16)\nplt.ylim((-100, 3000))\nplt.show()","9329e07d":"duration_info ## ok something got fucked up","f7fd8006":"shoppers","ab7e4a28":"duration_info['Total Duration'] = list(site_duration['TotalDuration'])*3","2352fbb6":"duration_info","93e05cda":"## percentages\nduration_info['Duration Percent'] = duration_info['Duration']\/duration_info['Total Duration']","25c1d032":"duration_info","be17178b":"min(duration_info['Duration Percent'])","ee412c86":"duration_info = duration_info.dropna()","ed130a9b":"plt.figure(figsize = (16,8))\nsns.boxplot('Visitor Type', 'Duration Percent', data = duration_info, hue = 'Duration Type',\n              palette = \"colorblind\")\nplt.title(\"Duration on Each Page Type by Visitor Type\", fontsize = 16)\nplt.xlabel(\"Visitor Type\", fontsize = 16)\nplt.ylabel(\"Duration on Page (seconds)\", fontsize = 16)\nplt.show()","9c60bf0b":"percent_ret = round(shoppers['VisitorType'].value_counts()['Returning_Visitor']\/len(shoppers['VisitorType'])*100, 2)\npercent_new = round(shoppers['VisitorType'].value_counts()['New_Visitor']\/len(shoppers['VisitorType'])*100, 2)\n\nprint(f\"{percent_ret} % visitors were returning\")\nprint(f\"{percent_new} % visitors were new\")","a5c505ed":"ordered_months = ['Jan', 'Feb', 'Mar', 'Apr', 'May', 'June', 'Jul', 'Aug', 'Sep', 'Oct', \n                  'Nov', 'Dec']","91adc8f0":"## this bit might not be the cleanest and I wish I could figure out a better way\n## dataframe that counts the entries in each group (visitor type & month)\nmonth_info = shoppers.groupby(['VisitorType', 'Month']).count()","4c68f449":"## only need one of those columns\nmonth_info = pd.DataFrame(month_info.iloc[:, 1])","7c8ad36f":"## turns the index into columns\nmonth_info.reset_index(inplace = True)","88348621":"## change column names\nmonth_info.columns = ['Visitor Type', 'Month', 'Num. Visitors']","0908683b":"month_info['Num. Visitors'].fillna(value = 0, inplace = True)","477f1c5f":"month_info['Month']","411e0725":"month_info['Visitor Type'] = month_info['Visitor Type'].astype(str)","7d6ba948":"month_info['Month'] = month_info['Month'].astype(str)","15602688":"to_add = pd.DataFrame([], columns = month_info.columns)","5fb33f97":"to_add['Visitor Type'] = ['New_Visitor', 'New_Visitor', \n                          'Other', 'Other', \n                          'Returning_Visitor', 'Returning_Visitor']","7148d49c":"to_add['Month'] = ['Jan', 'Apr']*3","d71cf1f7":"to_add['Num. Visitors'] = [0]*6","b9b501de":"to_add","2d682a95":"month_info = month_info.append(to_add)","1bc20dda":"month_info['Month'] = pd.Categorical(month_info['Month'], categories = ordered_months,\n                                    ordered = True)","ad7b1b94":"month_info","e4d9c512":"plt.figure(figsize=(12, 8))\nsns.lineplot(x = 'Month', y = \"Num. Visitors\", hue = \"Visitor Type\", data = month_info, \n            hue_order = [\"Returning_Visitor\", \"New_Visitor\", \"Other\"], sizes=(2.5, 2.5))\nplt.title(\"Number of Monthly Visitors by Visitor Type\", fontsize=16)\nplt.xlabel(\"Month\", fontsize=16)\nplt.xticks(rotation=45)\nplt.ylabel(\"Number of Visitors\", fontsize=16)\nplt.show()","673bc653":"shoppers.columns","d07750ed":"## closeness to special days in may? \nnp.average(shoppers.loc[shoppers['Month'] == 'May', 'SpecialDay'])","8cb56a2b":"np.average(shoppers.loc[shoppers['Month'] == 'Mar', 'SpecialDay'])","712b8686":"np.average(shoppers.loc[shoppers['Month'] == 'Nov', 'SpecialDay'])","e1039951":"np.average(shoppers.loc[shoppers['Month'] == 'Dec', 'SpecialDay'])","d0d0749d":"shoppers['SpecialDay'].unique()","21ab838b":"shoppers.loc[shoppers['SpecialDay'] > 0, 'Month'].unique()","41272ec1":"plt.figure(figsize=(12, 8))\nsns.countplot(x = 'Revenue', data = shoppers)\nplt.title(\"How many visitors generated revenue overall?\", fontsize=16)\nplt.xlabel(\"Revenue Generated?\", fontsize=16)\nplt.xticks(rotation=45)\nplt.ylabel(\"Number of Visitors\", fontsize=16)\nplt.show()","dd45bb1d":"print(round(sum(shoppers['Revenue'])\/len(shoppers['Revenue']), 2)*100, \"% of visitors generate revenue\")","52cb1d20":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics","25f629d9":"shoppers.columns","3100c502":"shoppers.dtypes","421727b0":"cat_vars = list(shoppers.select_dtypes('category').columns)","0f8b83f6":"num_vars = list(shoppers.select_dtypes('float').columns)","265094f6":"cat_vars = cat_vars[:-1] ## drop the revenue label","c950c9e3":"shoppers_dummies = pd.get_dummies(shoppers, columns = cat_vars)","f3debc80":"shoppers_dummies.head()","3a44a1ca":"shoppers_dummies.columns ## no revenue column because we already dropped it so no need to dop it now","f81d7649":"X = shoppers_dummies ## independent variables","1bdd968d":"y = shoppers['Revenue'] ## dependent variable","3c6c23c7":"y = y.astype(int) ## transform boolean to 0s and 1s","6ca57738":"## use the stratify argument here because of the uneven distribution of true and false values\n## in the target variable (revenue)\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                test_size = 0.3, random_state = 42,\n                                                stratify = y)","d7b5f770":"from sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn import svm\nfrom sklearn import tree\nfrom sklearn.metrics import classification_report\nfrom sklearn import svm\nfrom xgboost import XGBClassifier\nfrom itertools import compress","f868dfa1":"# define models to test:\nbase_models = [(\"clf\", LogisticRegression(random_state=42)),\n               (\"clf\", DecisionTreeClassifier(random_state=42)),\n               (\"clf\", svm.SVC(random_state=42))]","dd8daab0":"svm.SVC().get_params().keys()","7016fc52":"check_params_lr = {'clf__C': [0.1, 1, 10, 100],\n                  'pca__n_components':[2, 3, 4, 5, 6]}\ncheck_params_dt = {'pca__n_components':[2, 3, 4, 5, 6],\n               'clf__criterion':['gini', 'entropy'],\n               'clf__min_samples_split': [2,3,4],\n               'clf__max_depth': np.arange(3,15)}\ncheck_params_svc = {'pca__n_components': [2, 3, 4, 5, 6],\n                   'clf__C': [0.1, 1, 10, 100]}\n\ncheck_params = [check_params_lr, check_params_dt, check_params_svc]","c6496985":"def model_fit(model, params, X_train, y_train, X_test, y_test):\n    \n    pipe = Pipeline([('sc1', StandardScaler()),\n                     ('pca', PCA()),\n                    model])\n    \n    gs = GridSearchCV(estimator = pipe,\n                     param_grid = params,\n                     scoring = 'accuracy',\n                     cv = 5)\n    \n    gs.fit(X_train, y_train)\n    \n    # evaluate the model on the test set\n    y_true, y_pred = y_test, gs.predict(X_test)\n\n    # get classification report for the gs model\n    print(classification_report(y_true, y_pred))\n    ","c6150f96":"for mod, param in zip(base_models, check_params):\n    model_fit(mod, param, X_train, y_train, X_test, y_test)","fa39e5dd":"### What region do shoppers come from?\n\nTo answer this we have to count the number of values of each region and then plot the values in an ordered histogram -- can do this all at once using `sns.countplot`","83b89781":"It's evident that the majority of visitors to the site do not generate revenue (as to be expected)\n\n<br>\n\nThis also means that our data are imbalanced","2ade4ddd":"This is showing that ONLY February and May are \"close to special days\" which seems wrong since we know there are special days (e.g. Christmas, Father's Day) that occur on other days\nHmmm... \nThis indicates to me that in this case the \"Special Day\" data that this company is collecting could be more informative if you now what they consider to be a \"special day\", but as is, it's not particularly useful","a4e529c7":"Columns you want are \"Administrative_Duration\", \"Informational_Duration\", 'ProductRelated_Duration\"","68ad2f0a":"That's all for now!","94d4a727":"Based on our exploratory data analysis if you asked me to guess what would be the best predictor of whether a visitor generated revenue or not is whether that visitor is a returning visitor (returning visitor = higher probability of revenue), followed by the month that visitor accessed the site (if it's May, Mar, Nov, Dec = higher probability of revenue)","980270d0":"### Did visitation change across the year? \n\nNeed to count the number of visitors per month\n<br>\n\nBut first need to fix the probelem of the missing two months and make sure that the months will plot in chronological order (by ordering them)","02cff4ef":"To check whether returning visitors spend a significantly longer time on the site than new visitors we can do a permutation test","1fb1e0ff":"None of the numerical features appear as though they have \"incorrect\" values (e.g. negative values in a feature that could only have positive values).\n\nOne thing that's noticeable is that the mean value in most categories is a lot smaller than the max value, suggesting strongly right skewed distributions of values in each category. \n\nNotice that \"Region\", \"Browser\", \"OperatingSystems\", and \"TrafficType\" are being categorized as a numeric features, but they're actually numbers representing categories.","e40695f0":"Look at the shape of the dataset to understand how much data you're working with","7ee97eed":"Describe the numerical features","2dbca561":"So only 16% of visitors generate revenue - low! But i'm not sure how that is in comparison to other sites - perhaps it's actually relatively high!\n\nSince there is an imblanaced data in our label values (more falses than trues) we will use a stratified sampling approach when splitting the data into training and test data and use F1 score to assess model performance.","31a47ad3":"we do still have the problem of missing months - Jan and Apr","a85d8ce9":"### Feature Selection\nTarget = \"Revenue\"\n\nFeatures = All the other columns","0018cdb0":"### Load the dataset","676b1a9c":"### Hyperparameters and GridSearch CV\n\nIn order to get the best performing model we can also tune the hyperparameters\n\nDefine the parameters to check for each of the different models","2b3bd333":"Not many null values, but suspicially 14 samples have null values in the first 8 features, and these might be important features for our model. But 14 samples represents a small portion of the total dataset, to the easiest way to fix those missing values is to drop those samples from the dataset rather than finding a way to impute them. ","fa578709":"## Preparing the data for modeling","99b5df84":"# Online Shopper's Intention\n\n<br>\n\nGoal: build a classification model that will predict whether a shopper is a paying customer based on the available features (10 numerical and 7 categorical features, not including the \"revenue\" category). \n\n<br>\n\nBasic steps will involve: data cleaning -> data exploration -> model pipeline -> model examination\n\n<br>\n\nThe data were downloaded from Kaggle [here](https:\/\/www.kaggle.com\/roshansharma\/online-shoppers-intention) and the explanation of the features can be found on there.","2b1dbb15":"Looking for null values in the columns","95e07bd6":"The vast majority of users time is spent on product related pages, with more time spent on administrative pages by new visitors than returning visitors. ","fa2a34c7":"## Data Cleaning\n\n\n.head(), .shape, .describe(), .info(), .isnull().sum(), .columns, .dtypes(), .fillna(value = ), .drop(..., axis = 1) - to drop columns, pd.read_csv()\n\ncheck out numerical and categorical values - does everything look like it makes sense?\n\n### Load the libraries","b578a527":"The region with the most visitors is region 1 - but we don't know where exactly that is. But maybe if you were in charge of these data you would. ","01f1b2f2":"### Do new or returning customers spend longer on the website?\n\nGroup the data by `visitortype` and then add up their total time spent on the site (across informational, product, administrative types) and then plot a boxplot comparing the returning versus new customer. Then perform a permutation test to see if there is a statistical difference between new and returning customers time on the site (if it looks like there might be one).","37b474ba":"We have 7 categorical features, 10 numeric features, and 1 target variable (aka label or independent variable)\n\nWe must encode the categorical features so that they can be given to the model, since the model does not take python objects (strings)","c7ef5a53":"Look at the categorical values of the category features and see if there are any weird values that might be data errors","596a0fbb":"Moving on then to looking at the distribution of our target variable and the thing that we are trying to predict: whether or not a site visitor generated revenue for the company","bae01b9b":"Appears as though returning visitors spend longer on the site. I would say we could take the log to make the spread of the durations easier to see but the values include zeros and negatives, so it wouldn't work. ","dcee483a":"### Preprocess the data and build the model\n\nBuild a pipeline that will standardize the numeric values, tune the hyperparameters, and run classification model","64dcdeaf":"12330 rows = 12330 samples","e8d5aedb":"Things to notice:\n\n1) the month column only has 10 months represented, January and April are missing.\n\n2) there is an \"other\" value in the returning visitor category, let's see how many samples have \"other\" as the return visitor value","cc19a549":"### Which visitors generated revenue? ","ac132892":"Again, the distributions are so right skewed but you can see from the graph when it's zoomed in that returning customers spend longer on the product related portion of the website, and it appears as though new visitors spend a little longer than returning visitors on the administrative parts of the website. ","9e2eb8af":"Many choices, but I will use the classifiers Logisitic Regression, Decision Tree, and SVC","8dfb3931":"A value of 0 means that none of the permutations had a difference in means greater than the observed difference in means. In addition, the mean duration on the site of the returning customers is larger than the mean duration on the site of new customers. This leads us to determine that returning customers spend significantly more time on the site than new customers, which makes sense!","8af106cb":"Define the types of features in the data - categorical or numeric","984a6307":"## Data Exploration\n\nNow that data are clean and there are no missing values we can begin the exploratory data analysis\n\n### EDA Questions\n\n* Where do shoppers come from (region)?\n* Do new or returning customers spend longer on the website?\n* Which part of the website (admin\/product\/information) are new vs returning customers more likely to spend their time? \n* Does site visitation increase around special days? Which special days? Does this change based on the type of customer (new vs. returning)? \n* What special day attracts more new users? \n* What month of the year are there more users? Is this related to special days? \n","d9fdedf6":"### Which part of the website (admin\/product\/information) are new vs returning customers more likely to spend their time? \n\nWill want to do as similar thing as above except we break up the box plots into the page type ","2f0c97cf":"### Examine the data","1a804e81":"### Special days\nAre any of these spikes near \"special days\"?","e46b97a3":"### Fit the models\n\nRun the pipeline for each model","4d2ed7d3":"The best performing model (marginally) is the SVC. Since we have imbalanced data, we can look at the accuracy but it is more useful to look at the F1 score since that also takes into account incorrect predictions (which we may have in the minority class). The model performancees are good not great, so there is room for improvement in these models. Future directions would be to include a Random Forest classifier or an XGBoost classifier. Other options for improving the models would be dealing with the imbalanced data beyond stratifying the train-test datasplit. Dealing with imbalanced data includes weighting the data or over-sampling techniques (SMOTE and ADASYN).","dffe0542":"### What percent of visitors to the site were new vs. returning?","fdc9d4f0":"### Split the dataset into train\/test\n\n70-30 train-test split -- because the data are imbalanced we stratify the train test split","f2696483":"### Build the model pipeline","6f1b3fe2":"Because some people spent \"no time\" on the website (their duration was zero - is this truly zero or is it just a very very small amount of time that got rounded to zero? Not sure). We can remove the rows that have zero because we can't get a percentage out of them","1afdc3eb":"What percent of their visit time does each customer spends in each section","f80d58a3":"What the graph is showing is spikes in the number of visitors around Mar, May, and November\/December. It also shows that the number of new visitors and the number of returning visitors are fairly in sync - there isn't a particular time during the year where more new visitors seem to be attracted and returning visitors aren't. This indicates that in terms of attracting visitors overall, the influence of time of the year is about the same for new and returning visitors. In terms of attracting new visitors overall, it appears as though March, May, and November attract a lot of new visitors (and December to and extent). Spikes in November and December could be related to the holiday season, but spikes in March and May are more questionable - why? ","096e84ee":"85 samples have the visitor type listed as \"Other\" - that's not insignificant I suppose and \"Other\" might have some actually important value, so let's leave it. It shouldn't interfere too much. \n\nThere are also duration values that are negative - what would that even represent in the real world? Not sure, could be an error. There are only 33 of those entries, so we can remove. "}}