{"cell_type":{"ff7a33c6":"code","ebbf8ba9":"code","7693c99b":"code","c5776d50":"code","9958118e":"code","32ed8898":"code","9b53045f":"code","0f3c3523":"code","263e7596":"code","69cf2ea3":"code","d460002a":"code","a4146edd":"code","57f49fd2":"code","b20050e0":"code","44dac372":"code","65c82571":"code","f4b1cd1e":"code","022f8c0d":"markdown","ee8afc56":"markdown","3dc8c0e6":"markdown","f5a6b07b":"markdown","496aacb9":"markdown","7b5cccbe":"markdown"},"source":{"ff7a33c6":"%config Completer.use_jedi = False","ebbf8ba9":"import numpy as np\nimport pandas as pd\nimport seaborn as sns\n\nimport matplotlib.pyplot as plt\nfrom matplotlib.pyplot import imshow, imread\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.model_selection import KFold\nfrom sklearn.decomposition import PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cluster import KMeans\nfrom sklearn.mixture import GaussianMixture\n\nimport scipy.stats as stats\n\nimport lightgbm as lgb\nimport warnings\n\nimport optuna","7693c99b":"R_SEED = 37","c5776d50":"submit = True # for some testing","9958118e":"submission_ex = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')\ntrain_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv')\ntest_df = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv')","32ed8898":"targets_df = train_df[['loss']].copy()\ntrain_df.drop(['id', 'loss'], axis=1, inplace=True) \ntest_df.drop(['id'], axis=1, inplace=True) ","9b53045f":"def plot_fea_hist(df, fea_name, bins):\n    fig = plt.figure(figsize = (10, 10))\n    ax = fig.gca()\n    hist = df[fea_name].hist(bins = bins, color = 'k', alpha = 0.5, ax = ax)\n    ax.set_title(fea_name)","0f3c3523":"plot_fea_hist(targets_df, 'loss', 42)\nprint('skew: ', targets_df['loss'].skew())","263e7596":"arr = np.ones(250000)\narr[:125000]  = -1\nnp.random.shuffle(arr)\ntargets_df['loss_real'] = np.multiply(targets_df['loss'], arr)\nplot_fea_hist(targets_df, 'loss_real', 84)\nprint('skew: ', targets_df['loss_real'].skew())\nprint('just an illustration!')","69cf2ea3":"del targets_df['loss_real']","d460002a":"if submit:\n    X = train_df.copy()\n    y = targets_df[['loss']].copy()\n# else:\n#     np.random.seed(R_SEED)\n#     msk = np.random.rand(len(train_df)) < 0.8\n#     X = train_df[msk].copy()\n#     my_X = train_df[~msk].copy()\n#     y = targets_df[msk].copy()\n#     my_y = targets_df[~msk].copy()","a4146edd":"kfolds = KFold(n_splits = 3, shuffle = True, random_state = R_SEED)","57f49fd2":"def tune(objective):\n    study = optuna.create_study(direction = \"minimize\")\n    study.optimize(objective, n_trials = 1000, timeout = 3*60*60)\n    optuna.visualization.plot_optimization_history(study)\n    params = study.best_params\n    best_score = study.best_value\n    print(f\"Best score: {best_score} \\nOptimized parameters: params = {params}\")\n    return params","b20050e0":"def lightgbm_objective(trial):\n    \n    params = {\n        \"objective\": \"rmse\",\n        \"metric\": \"rmse\",\n        \"boosting_type\": \"gbdt\",\n        'n_estimators': 5000, #trial.suggest_int(\"n_estimators\", 2000, 5000),\n        \"learning_rate\": 0.005,\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 256),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 1, 3000),\n        'feature_fraction': trial.suggest_uniform('feature_fraction', 0.25, 0.7),\n        'bagging_fraction': trial.suggest_uniform('bagging_fraction', 0.7, 1.0),\n        'bagging_freq': trial.suggest_int('bagging_freq', 0, 5),\n#         'device': 'gpu',\n#         'gpu_platform_id': 0,\n#         'gpu_device_id': 0\n    }\n    \n    pruning_callback = optuna.integration.LightGBMPruningCallback(trial, 'rmse', valid_name='valid_0')\n    \n    model = lgb.LGBMRegressor(**params,\n                              n_jobs=-1,\n                              random_state = R_SEED) # ,device_type=\"gpu\"\n    \n    val_rmse = []\n    # !!!!!!!!!!!!!!!! intentionally !!!!!!!!!!!!!!!!\n    for test_index, train_index in kfolds.split(X): # train_index, test_index\n \n        X_train, X_val = X.iloc[train_index], X.iloc[test_index]\n        y_train, y_val = y.iloc[train_index], y.iloc[test_index]\n    \n        model.fit(\n            X_train, \n            y_train, \n            eval_metric = \"rmse\", \n            eval_set = [(X_val, y_val)],\n            verbose = 100,\n            early_stopping_rounds = 300,\n            callbacks = [pruning_callback])\n        oof_pred1 = model.predict(X_val)\n        oof_pred1 = np.clip(oof_pred1, y['loss'].min(), y['loss'].max())\n        val_rmse.append(mean_squared_error(y_val, oof_pred1, squared = False))\n    \n   \n    score = sum(val_rmse) \/ len(val_rmse)\n    \n    return score","44dac372":"# lightgbm_params = tune(lightgbm_objective)","65c82571":"params_loss = {\n                'n_estimators': 30000,\n                'learning_rate': 0.001,\n                'min_child_samples': 295,\n                'feature_fraction': 0.2915087392510538,\n                'bagging_fraction': 0.8549961258824171,\n                'bagging_freq': 0,\n                'num_leaves': 105, \n                }\n\nlgbm_reg = lgb.LGBMRegressor(\n                            **params_loss, \n                            objective='rmse',\n                            metric='rmse',\n                            n_jobs=-1\n                            )","f4b1cd1e":"_target = 'loss'\n\nprint(X.shape)\n\nlgbm_reg.fit(\n                X, \n                y,\n                callbacks = [lgb.reset_parameter(learning_rate = [0.001] * 20000 + [0.0005] * 10000)]\n                )\n\np_s = lgbm_reg.predict(test_df)\n\nsubmission_s = submission_ex[['id']].copy()\nsubmission_s[_target] = p_s\nsubmission_s.to_csv('submission_s.csv', index=False)\n\nsubmission_s.head()","022f8c0d":"While having more train-data than test-data is desirable, I swapped places for them here. In that case we have more than 150000 samples for testing in cross-validation. So, if we get big differences for submitted data compared to these here ...","ee8afc56":"Only now we can see how big problem this zero value make.","3dc8c0e6":"#### Optuna","f5a6b07b":"#### Submission","496aacb9":"If we put aside **rmse**, distribution will get negative side.<br\/>\nSomething like this:","7b5cccbe":"#### Interesting about target value\nThis is distribution of loss value. All positive values!"}}