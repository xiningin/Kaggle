{"cell_type":{"83b6c82d":"code","2494e8a7":"code","84d60e1c":"code","2e1c56f5":"code","e827b59e":"code","62d16ef7":"code","de588ea8":"code","91a0a5b7":"code","552b291d":"code","362f5828":"code","62cca8c4":"code","5ec431d4":"code","be56e262":"code","4f51d956":"code","3a26044b":"code","4c42aa9c":"code","1ead4708":"code","c9190e8b":"code","aed5e07f":"code","3d7733fa":"code","f72d7da9":"code","4fd9bfb7":"code","bfe238d7":"markdown","18e7a62c":"markdown","64017fc7":"markdown","6e2b657a":"markdown","1243e3bc":"markdown","0937b729":"markdown","53425ce3":"markdown","bd112673":"markdown","13ffe718":"markdown","63075ad6":"markdown","c04f3c82":"markdown","085e2525":"markdown","6b617c0b":"markdown","4f3defd9":"markdown","b78443b3":"markdown","41730c31":"markdown","a977ea67":"markdown","0bc3c81b":"markdown","2ccb386c":"markdown","689f4bb7":"markdown","ef65375b":"markdown","f7835245":"markdown","73ff3393":"markdown","f1ecd029":"markdown","b60144f3":"markdown","e7a89121":"markdown","7ffaaf70":"markdown","a01b318b":"markdown","c7eaecad":"markdown","d25762cf":"markdown","0693ad76":"markdown"},"source":{"83b6c82d":"from sklearn.model_selection import KFold,StratifiedKFold,RepeatedStratifiedKFold\nfrom sklearn.metrics import fbeta_score\nfrom sklearn.feature_selection import SelectKBest,chi2\nfrom imblearn.pipeline import Pipeline\nfrom sklearn.metrics import classification_report, confusion_matrix\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.impute import KNNImputer\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom sklearn.preprocessing import MinMaxScaler,KBinsDiscretizer\nfrom sklearn.experimental import enable_iterative_imputer\nfrom sklearn.impute import IterativeImputer,SimpleImputer\nfrom sklearn.linear_model import BayesianRidge,LogisticRegression,SGDClassifier\nfrom sklearn.svm import LinearSVC\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport warnings\nwarnings.filterwarnings('ignore')","2494e8a7":"MAX_UNIQUE_VALUES_FOR_CATEGORICAL=7\ndef is_categorical(data, columnName):\n  return len(data[columnName].unique()) < MAX_UNIQUE_VALUES_FOR_CATEGORICAL","84d60e1c":"def get_percentage(data,featureName,featureVal,targetName,targetVal):\n  needed_data=data.loc[(data[featureName]==featureVal)&(data[targetName]==targetVal)]\n  records_in_target_class=len(data.loc[data[targetName]==targetVal])\n  perc=round(100*len(needed_data)\/(1.0*records_in_target_class),2)\n  return perc","2e1c56f5":"def compare_features(new_data,selected_features,target_label):\n    report={}\n    report[\"features\"]=[]\n    for featureName in selected_features:\n        featureReport={}\n        featureReport[\"name\"]=featureName\n        featureReport[\"type\"]=\"categorical\"\n        if not is_categorical(new_data,featureName):\n            featureReport[\"type\"]=\"continuous\"\n            discretizer=KBinsDiscretizer(n_bins=5,encode='ordinal',strategy='uniform')\n            transformed_distance=pd.DataFrame(discretizer.fit_transform(new_data[[featureName]]))\n            transformed_distance.columns=[featureName]\n            # print('Inverse_values: {}'.format(discretizer.inverse_transform(pd.DataFrame({'distance':transformed_distance[featureName].unique()}))))\n            new_data[featureName]=transformed_distance[featureName]\n        featureReport[\"classDescription\"]=[]\n        for targetVal in new_data[target_label].unique():\n            classReport={} \n            classReport[\"targetValue\"]=targetVal\n            classReport[\"description\"]=[]\n            for featureVal in new_data[featureName].unique():\n                percentage=0\n                try:\n                    percentage=get_percentage(new_data,featureName,featureVal,target_label,targetVal)\n                except ZeroDivisionError:\n                    percentage=0\n                classReport[\"description\"].append({'featureVal':featureVal, 'percentage':percentage})\n            featureReport[\"classDescription\"].append(classReport)  \n        report[\"features\"].append(featureReport)\n    return report","e827b59e":"def evaluate_model(data,model):\n    report = {}\n    report[\"stats_by_iteration\"]=[]\n    iteration=1\n    pipeline = Pipeline(steps=[('scaler',MinMaxScaler(feature_range=(0,1))),('o',SMOTE(sampling_strategy=0.2)),('u',RandomUnderSampler(sampling_strategy=0.5)),('selector',SelectKBest(chi2, k=6)), ('m', model)])\n    y=data['outcome']\n    X=data.drop(['outcome'],axis=1)\n    num_splits=10\n    num_repeats=5\n    skf = RepeatedStratifiedKFold(n_splits=num_splits, n_repeats=num_repeats, random_state=42)\n    \n    for train_index, test_index in skf.split(X, y): \n        run_stats={}\n        x_train_fold, x_test_fold = X.iloc[train_index], X.iloc[test_index] \n        y_train_fold, y_test_fold = y.iloc[train_index], y.iloc[test_index] \n        run_stats[\"iteration\"]=iteration\n        run_stats[\"repeat_number\"]=iteration\/\/num_splits\n        run_stats[\"num_train_data\"]=len(x_train_fold)\n        run_stats[\"num_test_data\"]=len(x_test_fold)\n        pipeline.fit(x_train_fold, y_train_fold) \n        predictions=pipeline.predict(x_test_fold)\n        run_stats[\"selected_features\"]=X.columns[pipeline['selector'].get_support(indices=True)].tolist()\n        f0_5_score = round(fbeta_score(y_test_fold,predictions,average='weighted',beta=0.5)*100,2)\n        run_stats[\"f0.5\"]=f0_5_score\n        iteration+=1\n        report[\"stats_by_iteration\"].append(run_stats)\n    return report","62d16ef7":"def show_heat_map(data):\n  correlation_matrix = data.corr()\n  #To mask out the upper triangle\n  plt.figure(figsize=(20,10))\n  mask = np.zeros_like(correlation_matrix, dtype=np.bool)\n  mask[np.triu_indices_from(mask)] = True\n  sns.heatmap(data.corr(),mask=mask,annot=True)","de588ea8":"def identify_outliers(data, featureName):\n  mean = np.mean(data[featureName])\n  stddev = np.std(data[featureName])\n  indices = []\n  for index,value in data[featureName].iteritems():\n    z_score = (value-mean)\/stddev\n    if abs(z_score) > 3:\n      indices.append(index)\n  return indices","91a0a5b7":"def plot_data_from_report(report):\n    index=0\n    features_length=len(report[\"features\"])\n    fig,axj=plt.subplots(nrows=features_length\/\/2,ncols=2,figsize=(8,8),dpi=200) #Create pie chart pits\n    axes = axj.flatten() #Subgraph flattening\n    for feature in report[\"features\"]:\n        for classDesc in feature[\"classDescription\"]:\n            if classDesc[\"targetValue\"] != 1:\n                continue\n            actual_labels=[]\n            percentages=[]\n            for featureDescription in classDesc[\"description\"]:\n                percentages.append(featureDescription[\"percentage\"])\n                actual_labels.append('{}={}'.format(feature[\"name\"],featureDescription[\"featureVal\"]))\n            patches, text, autotext=axes[index].pie(percentages,labels=[['']*len(actual_labels)][0],explode=[0.08]*len(actual_labels),autopct='%1.2f%%',textprops={'fontsize': 5},radius=0.9)\n            axes[index].legend( actual_labels, loc='upper left', bbox_to_anchor=(-0.1, 1.),fontsize=5)\n            plt.tight_layout()\n            axes[index].set_title('Feature Description {} for Target {}'.format(feature[\"name\"],\"YES\" if classDesc[\"targetValue\"]==1 else \"NO\"),fontsize=5)\n            plt.subplots_adjust(left=None, bottom=None, right=None, top=None, wspace=0.5, hspace=0.2) \n        index+=1","552b291d":"data=pd.read_csv('..\/input\/change-of-primary-care-physician\/DataSet_PCP_Change.csv',index_col='testindex')\nprint(data.shape)","362f5828":"print(data.isna().sum())","62cca8c4":"data['tier'].fillna(data['tier'].mode()[0],inplace=True)\ndata.drop(['claims_daysaway'],axis=1,inplace=True)","5ec431d4":"data.hist(figsize=(18,14))","be56e262":"print(data.skew())","4f51d956":"data.distance=data.distance.map(lambda x: np.log(x+1))\ndata.visit_count=data.visit_count.map(lambda x: np.log(x+1))\nprint(data.skew())","3a26044b":"data.boxplot(figsize=(18,10))","4c42aa9c":"distance_outliers=identify_outliers(data,'distance')\nvisit_count_outliers=identify_outliers(data,'visit_count')\ndata.drop(distance_outliers,inplace=True)\n#Drop the indices that are common for both the outliers\nvisit_count_outliers=[index for index in visit_count_outliers if index not in distance_outliers] \ndata.drop(visit_count_outliers,inplace=True)\nprint('Removed {} outliers from the dataset'.format(len(distance_outliers)+len(visit_count_outliers)))\nprint(data.shape)","1ead4708":"show_heat_map(data)","c9190e8b":"data.drop(['is_ped'],axis=1,inplace=True)","aed5e07f":"models=[{'name':'RandomForest', 'model': RandomForestClassifier()},{'name':'GaussianNB','model':GaussianNB()},{'name':'LogisticRegression','model':LogisticRegression()},{'name':'LinearSVC','model':LinearSVC()},{'name':'kNN','model':KNeighborsClassifier(n_neighbors=10,weights='distance')},{'name':'DecisionTree','model':DecisionTreeClassifier()}]\nanalysis=[]\nfor model in models:\n  model_training_report=evaluate_model(data,model[\"model\"])\n  model_training_report[\"name\"]=model[\"name\"]\n  condensed_data=[(item[\"iteration\"],item[\"f0.5\"])for item in model_training_report[\"stats_by_iteration\"]]\n  model_training_report[\"analysis_result\"]=condensed_data\n  analysis.append(model_training_report)","3d7733fa":"plt.figure(figsize=(18,10))\nplt.xlabel('Iterations')\nplt.ylabel('F0.5 Score(%)')\n\ncolors=[\"red\",'green','blue','yellow','black','darkorchid','lime']\nindex=0\nmodel_names=[]\nvariances=[]\nfor analysis_res in analysis:\n  condensed_data = analysis_res[\"analysis_result\"]\n  model_names.append(analysis_res['name'])\n  percentages=[item[1] for item in condensed_data]\n  variances.append({'name':analysis_res['name'],'variance':np.var(percentages),'max_score':max(percentages)})\n  plt.plot([item[0] for item in condensed_data],percentages,color=colors[index])\n  index+=1\nplt.legend(model_names)\nvariances=sorted(variances,key=lambda x: x['variance'])\nprint('Least variance for model {}'.format(variances[0]))","f72d7da9":"chosenModel=\"LinearSVC\"\nfeatureVotes={}\nfor model in analysis:\n  if model[\"name\"]==chosenModel:\n    for iteration_stats in model[\"stats_by_iteration\"]:\n      for feature in iteration_stats[\"selected_features\"]:\n        if feature not in featureVotes:\n          featureVotes[feature]=0\n        featureVotes[feature]+=1\n# print(featureVotes)\nselected_features=sorted(featureVotes, key=featureVotes.get,reverse=True)[:6]\nprint(\"Top 6 important features are {}\".format(selected_features))\n","4fd9bfb7":"target_label='outcome'\nreport=compare_features(data,selected_features, target_label)\n# print(report)\nplot_data_from_report(report)\nplt.show()","bfe238d7":"### show_heat_map\nThis function is used to generate the heat map for the data\n```\nMethod: show_heat_map\nParams:\n  data- input in the form of a DataFrame\nReturns:\n  None\ngenerates a correlation matrix from the data, and plots the heat map.\n```","18e7a62c":"Observations:\n*   `outcome`:Data is highly imbalanced i.e data points with class label 0 exceeds the number of data points with label 1.\n*   `distance` - the distribution is skewed \n*   `visit_count` - the distribution is skewed\n*   Other binary attributes seem to be fairly distributed or slightly skewed\n\n\n\n","64017fc7":"Columns claims_daysaway and tier have null values.<br>\ntier is a categorical variable and can be replaced with the mode.<br>\nclaims_days_away has significantly large null values and so we will drop the column for the analysis. ","6e2b657a":"### Evaluating Performance of the models","1243e3bc":"### Outlier Analysis - Detection and removal\nWe use box plot to check for outliers","0937b729":"### Check the data for null values","53425ce3":"#**Assignment 4 - Change of Primary Care Physician**\n\n**Problem statement:** <br>\nAn insurance provider (US based) offers health insurance to customers. The provider assigns a PCP(primary care physician) to each customer. The PCP addresses most health concerns of the customers assigned to them.  For various reasons, customers want change of PCP. It involves significant effort for the provider whenever the customer makes a change of PCP. \nYou will find a subset of the insurance provider data along with PCP changes. The provider likes to understand why are members likely to leave the recommended provider. Further, they like to recommend a provider to them that they are less likely to leave.\n\n**Data Description**\n<table>\n  <tr><th>Variable<\/th><th>Description<\/th><\/tr>\n  <tr>\n      <td>Outcome<br> [0|1]<\/td>\n      <td>Member changed to his\/her preferred primary care provider instead of auto assigned to<\/td>\n  <\/tr>\n  <tr>\n    <td>distance<\/td>\n    <td>Distance between member and provider in miles.<\/td>\n  <\/tr>\n  <tr>\n    <td>visit_count<\/td>\n    <td>Number of claims between member and provider.<\/td>\n  <\/tr>\n  <tr>\n    <td>claims_days_away<\/td>\n    <td>Days between member changed to \/ assigned to the provider and latest claim between member and provider.<\/td>\n  <\/tr>\n  <tr>\n    <td>tier<br>[1|2|3|4]<\/td>\n    <td>Provider Tier from service. Tier 1 is highest benefit level and most cost-effective level.<\/td>\n  <\/tr>\n  <tr>\n    <td>fqhc<br>[0|1]<\/td>\n    <td>Provider is a certified Federally Qualified Health Center or not<\/td>\n  <\/tr>\n  <tr>\n    <td>pcp_lookback<br>[0|1]<\/td>\n    <td>he provider was the member primary care provider before or not<\/td>\n  <\/tr>\n  <tr>\n    <td>family_Assignment<br>[0|1]<\/td>\n    <td>The provider is the pcp of the member in the same family or not<\/td>\n  <\/tr>\n  <tr>\n    <td>kid<br>[0|1]<\/td>\n    <td>Member is a kid. (under 18 for state of New York)<\/td>\n  <\/tr>\n  <tr>\n    <td>is_Ped<br>[0|1]<\/td>\n    <td>Provider is a pediatrician or not<\/td>\n  <\/tr>\n  <tr>\n    <td>same_gender<\/td>\n    <td>provider and member are the same gender or not<\/td>\n  <\/tr>\n  <tr>\n    <td>same_language<\/td>\n    <td>Provider and member speak the same language or not<\/td>\n  <\/tr>\n  <tr>\n    <td>same_address<\/td>\n    <td>The re-assigned provider has the same address as the provider pre-assigned.<\/td>\n  <\/tr>\n<\/table>","bd112673":"Linear Suport Vector Classifier is the model with the least variance across the iterations, meaning that the performance of the model is consistent","13ffe718":"We have reduced the skewness to a significant extent.","63075ad6":"### Check the data distribution","c04f3c82":"The above charts represent information regarding why members might have decided to opt for a change.<br>\n**Observations:**\n*   `pcp_lookback`- If the provider was not the member primary care provider before, it is very likely that the member would opt for a change\n*   `same_address`- Out of people who opted for a change, most of them prefer the re-assigned provider to not have  same address as the provider pre-assigned.\n*   `visit_count`- If the member visit count is less, then it is very likely that the member would request for a change\n*   `same_language`- If the Provider and member do not speak the same language, it is very likely that the member would request for a change\n*   `fqhc`- Members prefer a Provider who is a certified Federally Qualified Health Center, else they'd request to change to one.\n*   `distance`- This is an intruiging observation, members who stay closer to the provider opt for a change.\nThe encoded data can be interpreted as:\n    *   Very Near\n    *   Near\n    *   Reachable\n    *   Far\n    *   Very Far\n  <br>People who are VeryNear and Reachable tend to opt for a change than the members belonging to the other group\n\n\n\n\n\n\n\n\n\n","085e2525":"## Utility methods","6b617c0b":"## Exploratory Data Analysis","4f3defd9":"We check the performance of the model using the RepeatedStratifiedKFold cross validation technique and record the f0.5 score for each iteration.<br>\nWe use this data to compare the models.<br>\nThe reason we use f0.5 score instead of the usual metrics is because the data is imbalanced i.e. class to be learned is the minority class and as the false positives are costly.<br>\n","b78443b3":"As observed from the histogram, `distance` and `visit_count` are highly skewed(right-skewed or positively skewed).<br>\nWe will apply a logarithm transformation to reduce the skewness.<br>\n`log (x+1)` - +1 is included to prevent values from going to infinity","41730c31":"## Model Training and testing","a977ea67":"Include all the imports.\n\nLibraries used: *sklearn,imblearn,pandas,numpy,seaborn,matplotlib*","0bc3c81b":"From the above heatmap, there is a very obvious strong positive correlation.<br>\nFeatures `kid` and `is_ped` are highly correlated, this is due to the fact that if the person is a kid, then the assigned doctor is a pediatrician, hence one of the features can be dropped","2ccb386c":"Based on the features that was selected by the model to be most important to label the data, we will inspect the dataset to get some insights.","689f4bb7":"### evaluate model\nThis function is used to evaluate the performance of a given model against the provided data\nA pipeline is created with the following steps:\n*   Scaler - MinMaxScaler(0,1) - to scale the values to the range 0-1\n*   Oversampler - SMOTE(Synthetic Minority Oversampling Technique) - Used to oversample the minority class\n*   Undersampler - RandomUnderSampler - Used to undersample the majority class\n*   Feature Selector - SelectKBest - Used to select k best features\n*   Model - The provided model to train\n\nRepeatedStratifiedKFold approach is used for cross validation, stratified model is used since the class is imbalanced\n```javascript\nmethod: evaluate_model\nparams:\n  data - input data in the form of a DataFrame\n  model - model for which the data needs to be trained\nreturns:\n  report - an object with details of f0.5 score for every iteration\n  report is of form\n  {\n    \"stats_by_iteration\":\n    [\n        {\n            \"iteration\": 1,\n            \"repeat_number\": 0, \/* repetition number *\/\n            \"num_train_data\": 2706, \/* number of samples in train data*\/\n            \"num_test_data\": 301, \/* number of samples in test data*\/\n            \"selected_features\": [\"feature1\",\"feature4\",...],  \/*features selected by the selector *\/\n            \"f0.5\": 90.74 \/*f0.5 score for the test data *\/\n        },..\n    ]\n  }\n```\n\n\n\n\n","ef65375b":"### Model Selection\n\nThe following models are selected to be evaluated\n1.   RandomForestClassifier\n2.   Gaussian Naive Bayes Classifier\n3.   Logistic Regression\n4.   Linear Suport Vector Classification\n5.   k Nearest Neighbors\n6.   Decision Tree Classifier\n\n\n\n\n","f7835245":"## Deduction from the data and model","73ff3393":"### Correlation analysis","f1ecd029":"### Read data from csv file","b60144f3":"### identify_outliers\nThis function is used to identify the outliers for feature - `featureName` in dataset and return the indices\n```\nMethod: identify_outliers\n  Uses z score to identify outliers i.e if z_score \u2265 \u00b13\nParams: \n  data - input data in the form of a DataFrame\n  featureName - feature for which outliers have to be detected\nReturns:\n  indices - list of indexes of the data points that are the outliers\n\n```","e7a89121":"### is_categorical \nused to check if a variable is categorical or not<br>\nSignature:<br>\n```\nMethod name: is_categorical\nParams: \n  data - DataFrame generated from the csv file\n  columnName - featureName\/ columnName which needs to be checked\nReturns:\n  bool - true if the column\/feature is categorical else false\n\n```","7ffaaf70":"### compare_features\nThis function returns a comparison by percentage of the data distribution for a target variable for all possible values of a feature.\nIf the feature is continuous, then it is discretized into certain number of classes, which is then encoded.<br>\n```javascript\nMethod name: compare_features\nParams:\n  new_data: input data in the form of DataFrame\n  selected_features : features which is selected by the model\n  target_label: target column name\nReturns:\n  report - which contains description by percentage for all the features\n  report is of the form\n  {\n    \"features: [\n          {\n            \"name\": \"featureName\",\n            \"type\" : \"categorical|continuous\",\n            \"classDescription\" : [ \/*For each value that the target class takes *\/\n              {\n                \"targetValue\" : \"0\",\n                \"description\":[ \/* For each value the feature takes, provided the target class is given *\/\n                  {\"featureVal\": 0, \"percentage\": 56.1},...\n                ]\n              },\n              {\n                \"targetValue\" : \"1\",\n                \"description\":[\n                  {\"featureVal\": 0, \"percentage\": 72.8},...\n                ]\n              }\n            ]\n          },...\n    ]\n  }\n\n```","a01b318b":"# Proposed Methodology and Solution","c7eaecad":"### Plot Report from evaluation report\n```\nMethod: plot_data_from_report\nParams: \n  report - report generated from the function - compare_features\nReturns:\n  None\nPlots pie charts showing the percentage distribution of feature values for a given target class\n```","d25762cf":"### get_precentage\nThis method returns the percent of datapoints that a feature `featureName` takes a value `featureVal`, for a `targetName` taking a value `targetVal` <br>\nEx: say for column is_kid and target outcome = 1<br>\n48% of members(kid) request for a change\n```\nMethod name: get_percentage\nParams:\n  data : inputData in the form of DataFrame\n  featureName: name of the feature \n  featureVal: value that a feature takes\n  targetName: name of the target column\n  targetVal: value that target column takes\nReturns:\n  percentage rounded of to 2 decimals\n\n```","0693ad76":"### Check data for skewness"}}