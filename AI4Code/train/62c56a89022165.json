{"cell_type":{"27dbb7cc":"code","5dcd9a5b":"code","d1c919e9":"code","72793ab2":"code","1436bab0":"code","63dd50e4":"code","99278bca":"code","ce221074":"code","d7d7e3e5":"code","47f90bd6":"code","1267fbe7":"code","4f1fa20e":"code","5460579d":"code","f111d0db":"code","5828c71a":"code","939c03dd":"code","52fd672a":"code","4c31a894":"code","cffa02a4":"code","b0f21e61":"code","d83e083a":"code","cdba0b6d":"code","93b2a8c8":"code","b88f7e21":"code","fbba339a":"code","91fe135a":"code","4a7b92b6":"code","fbb628a9":"code","103c5a4d":"code","f5dbbf64":"code","b05b1676":"code","6c73e163":"code","e014eb4c":"code","dd4cb093":"code","711f1056":"code","901fd5ca":"code","d2cda643":"code","80304da3":"markdown","4d81a1b0":"markdown","bbd6aea3":"markdown","88a3c10e":"markdown","12789e56":"markdown","7333dbaf":"markdown","52afda0f":"markdown","b6aaa144":"markdown","ab2384b1":"markdown","835f5547":"markdown","8861538b":"markdown","32b21b0f":"markdown","bfb2a070":"markdown","ad2ff9db":"markdown","72b68493":"markdown","e64cc657":"markdown","7f2cb6c6":"markdown","ed408136":"markdown","44b94751":"markdown","afc87f61":"markdown","bb5e5de9":"markdown","0c245d44":"markdown","b538d457":"markdown","de4b1a15":"markdown","3e4e95c3":"markdown","70c3efb5":"markdown","19e35a88":"markdown","a701e541":"markdown","85fbe83f":"markdown","87f595fb":"markdown","e15e29bc":"markdown","c9c28d82":"markdown","311685c7":"markdown","1ad2b674":"markdown","21c68e7e":"markdown","95c3f5cd":"markdown","d4252f95":"markdown","5de71350":"markdown"},"source":{"27dbb7cc":"import pandas as pd\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\nimport numpy as np","5dcd9a5b":"# Standard ML Models for comparison\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import ExtraTreesRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\n\n# Splitting data into training\/testing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\n\n# Metrics\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error, median_absolute_error\n\n# Distributions\nimport scipy","d1c919e9":"student = pd.read_csv('..\/input\/student-mat.csv')\nstudent.head()","72793ab2":"print('Total number of students:',len(student))","1436bab0":"student['G3'].describe()","63dd50e4":"plt.subplots(figsize=(8,12))\ngrade_counts = student['G3'].value_counts().sort_values().plot.barh(width=.9,color=sns.color_palette('inferno',40))\ngrade_counts.axes.set_title('Number of students who scored a particular grade',fontsize=30)\ngrade_counts.set_xlabel('Number of students', fontsize=30)\ngrade_counts.set_ylabel('Final Grade', fontsize=30)\nplt.show()","99278bca":"b = sns.countplot(student['G3'])\nb.axes.set_title('Distribution of Final grade of students', fontsize = 30)\nb.set_xlabel('Final Grade', fontsize = 20)\nb.set_ylabel('Count', fontsize = 20)\nplt.show()","ce221074":"student.isnull().any()","d7d7e3e5":"male_studs = len(student[student['sex'] == 'M'])\nfemale_studs = len(student[student['sex'] == 'F'])\nprint('Number of male students:',male_studs)\nprint('Number of female students:',female_studs)","47f90bd6":"b = sns.kdeplot(student['age'], shade=True)\nb.axes.set_title('Ages of students', fontsize = 30)\nb.set_xlabel('Age', fontsize = 20)\nb.set_ylabel('Count', fontsize = 20)\nplt.show()","1267fbe7":"b = sns.countplot('age',hue='sex', data=student)\nb.axes.set_title('Number of students in different age groups',fontsize=30)\nb.set_xlabel(\"Age\",fontsize=30)\nb.set_ylabel(\"Count\",fontsize=20)\nplt.show()","4f1fa20e":"b = sns.boxplot(x='age', y='G3', data=student)\nb.axes.set_title('Age vs Final', fontsize = 30)\nb.set_xlabel('Age', fontsize = 20)\nb.set_ylabel('Final Grade', fontsize = 20)\nplt.show()","5460579d":"b = sns.swarmplot(x='age', y='G3',hue='sex', data=student)\nb.axes.set_title('Does age affect final grade?', fontsize = 30)\nb.set_xlabel('Age', fontsize = 20)\nb.set_ylabel('Final Grade', fontsize = 20)\nplt.show()","f111d0db":"b = sns.countplot(student['address'])\nb.axes.set_title('Urban and rural students', fontsize = 30)\nb.set_xlabel('Address', fontsize = 20)\nb.set_ylabel('Count', fontsize = 20)\nplt.show()","5828c71a":"# Grade distribution by address\nsns.kdeplot(student.loc[student['address'] == 'U', 'G3'], label='Urban', shade = True)\nsns.kdeplot(student.loc[student['address'] == 'R', 'G3'], label='Rural', shade = True)\nplt.title('Do urban students score higher than rural students?', fontsize = 20)\nplt.xlabel('Grade', fontsize = 20);\nplt.ylabel('Density', fontsize = 20)\nplt.show()","939c03dd":"b = sns.swarmplot(x='reason', y='G3', data=student)\nb.axes.set_title('Reason vs Final grade', fontsize = 30)\nb.set_xlabel('Reason', fontsize = 20)\nb.set_ylabel('Final Grade', fontsize = 20)\nplt.show()","52fd672a":"student.corr()['G3'].sort_values()","4c31a894":"# Select only categorical variables\ncategory_df = student.select_dtypes(include=['object'])\n\n# One hot encode the variables\ndummy_df = pd.get_dummies(category_df)\n\n# Put the grade back in the dataframe\ndummy_df['G3'] = student['G3']\n\n# Find correlations with grade\ndummy_df.corr()['G3'].sort_values()","cffa02a4":"# selecting the most correlated values and dropping the others\nlabels = student['G3']\n\n# drop the school and grade columns\nstudent = student.drop(['school', 'G1', 'G2'], axis='columns')\n    \n# One-Hot Encoding of Categorical Variables\nstudent = pd.get_dummies(student)","b0f21e61":"# Find correlations with the Grade\nmost_correlated = student.corr().abs()['G3'].sort_values(ascending=False)\n\n# Maintain the top 8 most correlation features with Grade\nmost_correlated = most_correlated[:9]\nmost_correlated","d83e083a":"student = student.loc[:, most_correlated.index]\nstudent.head()","cdba0b6d":"b = sns.swarmplot(x=student['failures'],y=student['G3'])\nb.axes.set_title('Students with less failures score higher', fontsize = 30)\nb.set_xlabel('Number of failures', fontsize = 20)\nb.set_ylabel('Final Grade', fontsize = 20)\nplt.show()","93b2a8c8":"family_ed = student['Fedu'] + student['Medu'] \nb = sns.boxplot(x=family_ed,y=student['G3'])\nb.axes.set_title('Educated families result in higher grades', fontsize = 30)\nb.set_xlabel('Family education (Mother + Father)', fontsize = 20)\nb.set_ylabel('Final Grade', fontsize = 20)\nplt.show()","b88f7e21":"b = sns.swarmplot(x=family_ed,y=student['G3'])\nb.axes.set_title('Educated families result in higher grades', fontsize = 30)\nb.set_xlabel('Family education (Mother + Father)', fontsize = 20)\nb.set_ylabel('Final Grade', fontsize = 20)\nplt.show()","fbba339a":"student = student.drop('higher_no', axis='columns')\nstudent.head()","91fe135a":"b = sns.boxplot(x = student['higher_yes'], y=student['G3'])\nb.axes.set_title('Students who wish to go for higher studies score more', fontsize = 30)\nb.set_xlabel('Higher education (1 = Yes)', fontsize = 20)\nb.set_ylabel('Final Grade', fontsize = 20)\nplt.show()","4a7b92b6":"b = sns.countplot(student['goout'])\nb.axes.set_title('How often do students go out with friends', fontsize = 30)\nb.set_xlabel('Go out', fontsize = 20)\nb.set_ylabel('Count', fontsize = 20)\nplt.show()","fbb628a9":"b = sns.swarmplot(x=student['goout'],y=student['G3'])\nb.axes.set_title('Students who go out a lot score less', fontsize = 30)\nb.set_xlabel('Going out', fontsize = 20)\nb.set_ylabel('Final Grade', fontsize = 20)\nplt.show()","103c5a4d":"b = sns.swarmplot(x=student['romantic_no'],y=student['G3'])\nb.axes.set_title('Students with no romantic relationship score higher', fontsize = 30)\nb.set_xlabel('Romantic relationship (1 = None)', fontsize = 20)\nb.set_ylabel('Final Grade', fontsize = 20)\nplt.show()","f5dbbf64":"# splitting the data into training and testing data (75% and 25%)\n# we mention the random state to achieve the same split everytime we run the code\nX_train, X_test, y_train, y_test = train_test_split(student, labels, test_size = 0.25, random_state=42)","b05b1676":"X_train.head()","6c73e163":"# Calculate mae and rmse\ndef evaluate_predictions(predictions, true):\n    mae = np.mean(abs(predictions - true))\n    rmse = np.sqrt(np.mean((predictions - true) ** 2))\n    \n    return mae, rmse","e014eb4c":"# find the median\nmedian_pred = X_train['G3'].median()\n\n# create a list with all values as median\nmedian_preds = [median_pred for _ in range(len(X_test))]\n\n# store the true G3 values for passing into the function\ntrue = X_test['G3']","dd4cb093":"# Display the naive baseline metrics\nmb_mae, mb_rmse = evaluate_predictions(median_preds, true)\nprint('Median Baseline  MAE: {:.4f}'.format(mb_mae))\nprint('Median Baseline RMSE: {:.4f}'.format(mb_rmse))","711f1056":"# Evaluate several ml models by training on training set and testing on testing set\ndef evaluate(X_train, X_test, y_train, y_test):\n    # Names of models\n    model_name_list = ['Linear Regression', 'ElasticNet Regression',\n                      'Random Forest', 'Extra Trees', 'SVM',\n                       'Gradient Boosted', 'Baseline']\n    X_train = X_train.drop('G3', axis='columns')\n    X_test = X_test.drop('G3', axis='columns')\n    \n    # Instantiate the models\n    model1 = LinearRegression()\n    model2 = ElasticNet(alpha=1.0, l1_ratio=0.5)\n    model3 = RandomForestRegressor(n_estimators=100)\n    model4 = ExtraTreesRegressor(n_estimators=100)\n    model5 = SVR(kernel='rbf', degree=3, C=1.0, gamma='auto')\n    model6 = GradientBoostingRegressor(n_estimators=50)\n    \n    # Dataframe for results\n    results = pd.DataFrame(columns=['mae', 'rmse'], index = model_name_list)\n    \n    # Train and predict with each model\n    for i, model in enumerate([model1, model2, model3, model4, model5, model6]):\n        model.fit(X_train, y_train)\n        predictions = model.predict(X_test)\n        \n        # Metrics\n        mae = np.mean(abs(predictions - y_test))\n        rmse = np.sqrt(np.mean((predictions - y_test) ** 2))\n        \n        # Insert results into the dataframe\n        model_name = model_name_list[i]\n        results.loc[model_name, :] = [mae, rmse]\n    \n    # Median Value Baseline Metrics\n    baseline = np.median(y_train)\n    baseline_mae = np.mean(abs(baseline - y_test))\n    baseline_rmse = np.sqrt(np.mean((baseline - y_test) ** 2))\n    \n    results.loc['Baseline', :] = [baseline_mae, baseline_rmse]\n    \n    return results","901fd5ca":"results = evaluate(X_train, X_test, y_train, y_test)\nresults","d2cda643":"plt.figure(figsize=(12, 8))\n\n# Root mean squared error\nax =  plt.subplot(1, 2, 1)\nresults.sort_values('mae', ascending = True).plot.bar(y = 'mae', color = 'b', ax = ax, fontsize=20)\nplt.title('Model Mean Absolute Error', fontsize=20) \nplt.ylabel('MAE', fontsize=20)\n\n# Median absolute percentage error\nax = plt.subplot(1, 2, 2)\nresults.sort_values('rmse', ascending = True).plot.bar(y = 'rmse', color = 'r', ax = ax, fontsize=20)\nplt.title('Model Root Mean Squared Error', fontsize=20) \nplt.ylabel('RMSE',fontsize=20)\n\nplt.show()","80304da3":"There seems to be a slight trend that with the increase in family education the grade moves up (apart from the unusual high value at family_ed = 1 (maybe students whose parents did not get to study have more motivation)\n\n### Note:\n\nI prefer swarm plots over box plots because it is much more useful to see the distribution of data (and also to spot outliers)","4d81a1b0":"Most students have an average score when it comes to going out with friends. (normal distribution)","bbd6aea3":"## Most students are from urban ares, but do urban students perform better than rurual students?","88a3c10e":"# Some basic analysis","12789e56":"## Next let us take a look at the gender variable","7333dbaf":"The graph shows that on there is not much difference between the scores based on location.","52afda0f":"### Student with less previous failures usually score higher","b6aaa144":"## Does having a romantic relationship affect grade?\n\nAgain because of one hot encoding we have our variable called romantic_no which is slightly less intuitive but I am going to stick with it. Keep in mind that:\n\n- romantic_no = 1 means NO romantic relationship\n- romantic_no = 0 means romantic relationship","ab2384b1":"# Final grade distribution","835f5547":"### We see that linear regression is performing the best in both cases","8861538b":"The ages seem to be ranging from 15 - 19. The students above that age may not necessarily be outliers but students with year drops. Also the gender distribution is pretty even.","32b21b0f":"### Going out with friends","bfb2a070":"The graph shows a slight downward trend","ad2ff9db":"### Histogram might be more useful to compare different ages","72b68493":"## Hmmmmm!\n\nSomething seems off here. Apart from the high number of students scoring 0, the distribution is normal as expected.\nMaybe the value 0 is used in place of null. Or maybe the students who did not appear for the exam, or were not allowed to sit for the exam due to some reason are marked as 0. We cannot be sure. Let us check the table for null values","e64cc657":"## Count of students from urban and rural areas","7f2cb6c6":"### Plotting the distribution rather than statistics would help us better understand the data","ed408136":"# Now we will analyse these variables and then train a model","44b94751":"## Correlation\n\nNext we find the correlation between various features and the final grade.\n \n### Note: This correlation is only between numeric values","afc87f61":"## Checking the final grade","bb5e5de9":"As we can see there are only 2 points in family_ed = 1 hence our conclusion was faulty.","0c245d44":"### Grades according to the number of students who scored them","b538d457":"### Import the relevant modules","de4b1a15":"## Applying one hot encoding to our data and finding correlation again!\n\n\n### Note: \nAlthough G1 and G2 which are period grades of a student and are highly correlated to the final grade G3, we drop them. It is more difficult to predict G3 without G2 and G1, but such prediction is much more useful because we want to find other factors affect the grade.","3e4e95c3":"### None of the variables has null values so maybe grade 0 does not mean null after all","70c3efb5":"We see that age 20 has only 3 data points hence the inconsistency in statistics. Otherwise there seems to be no clear relation of age or gender with final grade","19e35a88":"# Modeling\n\n### We can create a model in 3 ways\n\n1. Binary classification\n    - G3 > 10: pass\n    - G3 < 10: fail\n2. 5-level classification based on Erasmus grade conversion system\n    - 16-20: very good\n    - 14-15: good\n    - 12-13: satisfactory\n    - 10-11: sufficient\n    -  0-9 : fail\n3. Regression (Predicting G3)\n\n### We will be using the 3rd type","a701e541":"\n\nThis plot does not tell us much. What we should really plot is the distribution of grade.\n\n","85fbe83f":"## Higher education\n\nHigher education was a categorical variable with values yes and no. Since we used one hot encoding it has been converted to 2 variables. So we can safely eliminate one of them (since the values are compliments of each other). We will eliminate higher_no, since higher_yes is more intuitive.","87f595fb":"# Predicting the final grade of a student\n\nThe data used is from a Portuguese secondary school. The data includes academic and personal characteristics of the students as well as final grades. The task is to predict the final grade from the student information. (Regression)\n\n### [Link to dataset](https:\/\/archive.ics.uci.edu\/ml\/datasets\/student+performance)\n\n### Citation:\n\nP. Cortez and A. Silva. Using Data Mining to Predict Secondary School Student Performance. In A. Brito and J. Teixeira Eds., Proceedings of 5th FUture BUsiness TEChnology Conference (FUBUTEC 2008) pp. 5-12, Porto, Portugal, April, 2008, EUROSIS, ISBN 978-9077381-39-7.\n[Web Link](http:\/\/www3.dsi.uminho.pt\/pcortez\/student.pdf)\n\n### Reference [article](\/home\/dipamvasani7\/Desktop\/Ubuntu\/jupyter_notebooks\/data)","e15e29bc":"## Checking the distribution of Age along with gender","c9c28d82":"# Encoding categorical variables\n\nA machine learning model cannot deal with categorical variables (except for some models). Therefore we need to find a way to encode them (represent as numbers) before handing them to the model.\n\n## Label encoding\n\nThis method involves assigning one label for each category\n\n| Occupation    | Label         |\n| ------------- |:-------------:|\n| programmer    | 0             |\n| data scientist| 1             |\n| Engineer      | 2             |\n\n\n\nThe problem with label encoding is that the assignment of integers is random and changes every time we run the function. Also the model might give higher priority to larger labels. Label encoding can be used when we have only 2 unique values.\n\n## One hot encoding\n\nThe problem with label encoding is solved by one hot encoding. It creates a new column for each category and uses only binary values. The downside of one hot encoding is that the number of features can explode if the categorical variables have many categories. To deal with this we can perform PCA (or other dimensionality reduction methods) followed by one hot encoding.\n\n| Occupation    | Occupation_prog| Occupation_ds | Occupation_eng |\n| ------------- |:-------------: |:-------------:|:-------------: |\n| programmer    | 1              | 0             | 0              |\n| data scientist| 0              | 1             | 0              |\n| Engineer      | 0              | 0             | 1              |","311685c7":"## Other features\n\nIt might not be wise to analyse every feature so I will find the features most correlated to the final grade and spend more time on them.","1ad2b674":"## Reason to choose this school","21c68e7e":"### Naive baseline is the median prediction","95c3f5cd":"### MAE - Mean Absolute Error\n### RMSE - Root Mean Square Error","d4252f95":"### Example of one hot encoding","5de71350":"## Does age have anything to do with the final grade?"}}