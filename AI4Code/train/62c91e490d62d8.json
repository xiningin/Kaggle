{"cell_type":{"827fad65":"code","5d389e6f":"code","add62df1":"code","3a2b19c1":"code","416ed9ab":"code","f6c25579":"code","8c906c55":"code","e56ccbee":"code","475b215c":"code","df522795":"code","92ad4a67":"code","23c69cc3":"code","eee036b6":"code","47a16b45":"code","92c68e06":"markdown","7a008eee":"markdown","4cafc5f5":"markdown","309631a1":"markdown","ccdbfe57":"markdown","9a3d4efd":"markdown","7ac9c044":"markdown","e18721a3":"markdown","56bf8d08":"markdown","94f703c8":"markdown","135487fe":"markdown","8ebda46c":"markdown","fd84421f":"markdown"},"source":{"827fad65":"# Importing libraries\nimport random\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import auc, accuracy_score, confusion_matrix\nimport pandas as pd\nimport category_encoders as ce\n\n# set a seed for reproducability\nrandom.seed(42)\n\n# read in our data\ndf_2018 = pd.read_csv(\"..\/input\/data-prep-for-job-title-classification\/data_jobs_info_2018.csv\")\ndf_2019 = pd.read_csv(\"..\/input\/data-prep-for-job-title-classification\/data_jobs_info_2019.csv\")","5d389e6f":"# split into predictor & target variables\nX = df_2018.drop(\"job_title\", axis=1)\ny = df_2018[\"job_title\"]\n\n# Splitting data into training and test set\nX_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.80, test_size=0.20)\n\n# save out the split training data to use with Cloud AutoML\nwith open(\"train_data_2018.csv\", \"+w\") as file:\n    pd.concat([X_train, y_train], axis=1).to_csv(file, index=False)\nwith open(\"test_data_2018.csv\", \"+w\") as file:\n    pd.concat([X_test, y_test], axis=1).to_csv(file, index=False)","add62df1":"# encode all features using ordinal encoding\nencoder_x = ce.OrdinalEncoder()\nX_encoded = encoder_x.fit_transform(X)\n\n# you'll need to use a different encoder for each dataframe\nencoder_y = ce.OrdinalEncoder()\ny_encoded = encoder_y.fit_transform(y)\n\n# split encoded dataset\nX_train_encoded, X_test_encoded, y_train_encoded, y_test_encoded = train_test_split(X_encoded, y_encoded,\n                                                    train_size=0.80, test_size=0.20)","3a2b19c1":"from xgboost import XGBClassifier\n\n# train XGBoost model with default parameters\nmy_model = XGBClassifier()\nmy_model.fit(X_train_encoded, y_train_encoded, verbose=False)\n\n# and save our model\nmy_model.save_model(\"xgboost_baseline.model\")","416ed9ab":"from google.cloud import automl_v1beta1 as automl\nfrom kaggle.gcp import KaggleKernelCredentials\nfrom kaggle_secrets import GcpTarget\nfrom google.cloud import storage\n\n# don't change this value!\nREGION = 'us-central1' # don't change: this is the only region that works currently\n\n# these you'll change based on your GCP project\/data\nPROJECT_ID = 'kaggle-automl-example' # this will come from your specific GCP project\nDATASET_DISPLAY_NAME = 'data_jobs_info_2018' # name of your uploaded dataset (from GCP console)\nTARGET_COLUMN = 'job_title' # column with feature you're trying to predict\n\n# these can be whatever you like\nMODEL_DISPLAY_NAME = 'kaggle_automl_example_model' # what you want to call your model\nTRAIN_BUDGET = 1000 # max time to train model in milli-hours, from 1000-72000\n\nstorage_client = storage.Client(project=PROJECT_ID, credentials=KaggleKernelCredentials(GcpTarget.GCS)) \ntables_gcs_client = automl.GcsClient(client=storage_client, credentials=KaggleKernelCredentials(GcpTarget.GCS)) \ntables_client = automl.TablesClient(project=PROJECT_ID, region=REGION, gcs_client=tables_gcs_client, credentials=KaggleKernelCredentials(GcpTarget.AUTOML))","f6c25579":"# first you'll need to make sure your model is predicting the right column\ntables_client.set_target_column(\n    dataset_display_name=DATASET_DISPLAY_NAME,\n    column_spec_display_name=TARGET_COLUMN,\n)","8c906c55":"# let our model know that input columns may have missing values\nfor col in tables_client.list_column_specs(project=PROJECT_ID,\n                                           dataset_display_name=DATASET_DISPLAY_NAME):\n    if TARGET_COLUMN in col.display_name:\n        continue\n    tables_client.update_column_spec(project=PROJECT_ID,\n                                     dataset_display_name=DATASET_DISPLAY_NAME,\n                                     column_spec_display_name=col.display_name,\n                                     nullable=True)","e56ccbee":"# and then you'll need to kick off your model training\nresponse = tables_client.create_model(MODEL_DISPLAY_NAME, dataset_display_name=DATASET_DISPLAY_NAME, \n                                      train_budget_milli_node_hours=TRAIN_BUDGET, \n                                      exclude_column_spec_names=[TARGET_COLUMN])\n\n# check if it's done yet (it won't be)\nresponse.done()","475b215c":"from tpot import TPOTClassifier\n\n# create & fit TPOT classifier with \ntpot = TPOTClassifier(generations=8, population_size=20, \n                      verbosity=2, early_stop=2)\ntpot.fit(X_train_encoded, y_train_encoded)\n\n# save our model code\ntpot.export('tpot_pipeline.py')\n\n# print the model code to see what it says\n!cat tpot_pipeline.py","df522795":"import h2o\nfrom h2o.automl import H2OAutoML\n\n# initilaize an H20 instance running locally\nh2o.init()","92ad4a67":"# convert our data to h20Frame, an alternative to pandas datatables\ntrain_data = h2o.H2OFrame(X_train)\ntest_data = h2o.H2OFrame(list(y_train))\n\ntrain_data = train_data.cbind(test_data)\n\n# Run AutoML for 20 base models (limited to 1 hour max runtime by default)\naml = H2OAutoML(max_models=20, seed=1)\naml.train(y=\"C1\", training_frame=train_data)","23c69cc3":"# View the top five models from the AutoML Leaderboard\nlb = aml.leaderboard\nlb.head(rows=5)\n\n# The leader model can be access with `aml.leader`","eee036b6":"# save the model out (we'll need to for tomorrow!)\nh2o.save_model(aml.leader)","47a16b45":"# check to see that we've saved all of our models\n! ls","92c68e06":"Once our model starts training, we don't need to do anything else: it's already saved in our GCP account and good to go for tomorrow.","7a008eee":"# Exercise\n\nNow it's your turn! Following the steps above, use the `df_2019` dataframe and: \n\n* Prepare your data (split into testing and training, encode variables)\n* Train your models: XGBoost, Cloud AutoML, TPOT and H20 AutoML\n\n> Note: if you can't or would prefer not to set up billing on order to use Cloud AutoML, feel free to skip training that model.\n\n* Remember to save your models! You'll need them tomorrow and, since it takes a while to run AutoML code, you don't want to have to run it multiple times.\n\nHave fun training your models and I'll see you all tomorrow for our final model evaluation!","4cafc5f5":"These notebooks are part of Kaggle\u2019s [Practical Model Evaluation](https:\/\/www.kaggle.com\/practical-model-evaluation) event, which ran from December 3-5 2019. You can find the [livestreams for this event here](https:\/\/youtu.be\/7RdKnACscjA?list=PLqFaTIg4myu-HA1VGJi_7IGFkKRoZeOFt).\n\n* Day 1 Notebook: [Figuring out what matters for you](https:\/\/www.kaggle.com\/rtatman\/practical-model-evaluation-day-1)\n* Day 2 Notebook: [Training models with automated machine learning](https:\/\/www.kaggle.com\/rtatman\/practical-model-evaluation-day-2)\n* Day 3 Notebook: [Evaluating our models](https:\/\/www.kaggle.com\/rtatman\/practical-model-evaluation-day-3)\n\n***","309631a1":"For today's exercise, we're going to be working on classifying roles into job titles based on information about the role. The data will be from the [2018](https:\/\/www.kaggle.com\/kaggle\/kaggle-survey-2018) and [2019](https:\/\/www.kaggle.com\/c\/kaggle-survey-2019) Kaggle data science survey. \n\nI've already [done some data cleaning](https:\/\/www.kaggle.com\/rebeccaturner\/data-prep-for-job-title-classification) but if you'd like to do your own or do some additional feature engineering, feel free!\n\nToday we'll be building four different models using four different libraries, including some automated machine learning libraries. \n\n> Automated machine learning (or AutoML for short) is the task of removing human labor from the process of training machine learning models. Currently most AutoML research is focused on automating model selection and hyperparameter tuning. [This video](https:\/\/www.youtube.com\/watch?v=Rsg_XzgGqZw&utm_medium=notebook&utm_source=kaggle&utm_campaign=automl-event) goes into more details.\n\nThe libraries we'll be using are:\n\n* [XGBoost](https:\/\/xgboost.readthedocs.io\/en\/latest\/) (not automated machine learning: we'll be using this as a baseline)\n* [Cloud AutoML](https:\/\/cloud.google.com\/automl\/?utm_medium=notebook&utm_source=kaggle&utm_campaign=automl-event), an enterprise-focused automated machine learning product\n* [TPOT](https:\/\/epistasislab.github.io\/tpot\/), an open source automated machine learning library developed at the University of Pennsylvania\n* [H20.ai AutoML](http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html), a second open source automated machine learning library developed by researchers at H20.ai\n\nLet's get started!","ccdbfe57":"# XGBoost Baseline\n\nFirst, we're going to train a basic XGBoost model using the default arguments. We cover XGBoost in more detail in the [Intro To Machine Learning course](https:\/\/www.kaggle.com\/learn\/intro-to-machine-learning), so I'm not going to talk about it here.","9a3d4efd":"## Data preparation\n\nWe do have an additional step of preperation. First, we'll split into training and testing sets:","7ac9c044":"## Load in data\n\nFirst let's load in our pre-cleaned data. I'll be using the 2018 data as an example and then have you work through the 2019 data as your exercise.","e18721a3":"# H20.ai AutoML\n\nFor our final model we'll be using [H20.ai's open source AutoML library](http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html). One thing that I like about this library is that, as each model is trained, its evaluated both on its own and as part of a stacked ensemble. Kaggle competitors are very fond of stacking (and H20 is known for hiring a lot of top Kagglers) so it's nice to have that automated for us.","56bf8d08":"# Check that we've saved each of our models\n\nBefore we wrap up for the day, we want to make sure we've saved all of our models for tomorrow! The Cloud AutoML model is saved automatically on GCP, but we've saved each of the other models in our current working directory. Let's just double check that that's the case:","94f703c8":"Alright, we've got three models and the code for the notebook. We're all set!","135487fe":"# TPOT\n\nAlright, now we'll move onto [TPOT](https:\/\/epistasislab.github.io\/tpot\/). This is an academic library built on top of scikit-learn, and my favorite thing about it is that when you export a model you're actually exporting all the Python code you need to train that model.","8ebda46c":"For H20 AutoML and Cloud AutoML we don't need to do anything else. (Actually for Cloud AutoML we don't even need to split our data, but we'll look at that in a minute.) \n\nFor TPOT and XGBoost, however, we need to make sure that all our input data is numeric. We'll be using [ordinal label encoding](https:\/\/contrib.scikit-learn.org\/categorical-encoding\/ordinal.html) for this.","fd84421f":"# Cloud AutoML\n\nNow let's train our Cloud AutoML model! We'll be using both the GCP console and notebook code here, so you'll probably want to open those in separate tabs or windows.\n\n### Prepare your account and project\n\n\n* First you\u2019ll need to [create a GCP account](https:\/\/accounts.google.com\/signup\/v2\/webcreateaccount?service=cloudconsole&continue=https%3A%2F%2Fconsole.cloud.google.com%2F&dsh=S-385463669%3A1575309184770524&gmb=exp&biz=false&flowName=GlifWebSignIn&flowEntry=SignUp&nogm=true&utm_medium=notebook&utm_source=kaggle&utm_campaign=automl-event) (if you already have a Google account you can use that one) and [enable billing](https:\/\/www.youtube.com\/watch?v=uINleRduCWM&utm_medium=notebook&utm_source=kaggle&utm_campaign=automl-event).\n\n\n> For now, you do need to have a credit card in order to enable billing and you need billing enabled to use Cloud AutoML. If you\u2019re not able to enable billing you can still follow along with the rest of the workshop, just skip the Cloud AutoML parts. \n\n \n\n* From there, [create a new project](https:\/\/cloud.google.com\/appengine\/docs\/standard\/nodejs\/building-app\/creating-project?utm_medium=notebook&utm_source=kaggle&utm_campaign=automl-event). You should [set the region of your project](https:\/\/cloud.google.com\/compute\/docs\/regions-zones\/?utm_medium=notebook&utm_source=kaggle&utm_campaign=automl-event) to \u201cus-central1\u201d.\n\n* Go to the [AutoML Tables](https:\/\/console.cloud.google.com\/automl-tables?utm_medium=notebook&utm_source=kaggle&utm_campaign=automl-event) page in the Google Cloud Console and click *enable API*. This will let you train an AutoML Tables model in your current project. \n\n\n### Creating your dataset\n\n\nFor this workshop, we\u2019re going to create our AutoML datasets using the GCP console. The reason for this is that importing datasets can take a while. If you have the code in your notebook to import your dataset right before the code to create your model, when you run your notebook top to bottom it\u2019ll give you an error because the modelling code was run before the dataset was done importing.\n\n\n* Click on \u201cDatasets\u201d in the list on the left hand side of your screen and then click on the blue **[+] New Dataset** text near the top of your screen.\n\n* Give your dataset a name and make sure the region is **US-CENTRAL1**.  \n\n* Select \u201cUpload files from your computer\u201d and select the file with the dataset you want. \n\n* Click on **BROWSE** under the \u201cSelect Files\u201d button and a side panel will pop up. \n\n    * If you haven\u2019t created any buckets you\u2019ll see the text \u201cNo buckets found\u201d. To create a new bucket, click on the icon that looks like a shopping basket with a plus sign in it.\n\n    * Follow the prompts to create your bucket. **Important:** Make sure in the \u201cChoose where to store your data\u201d step, you pick \u201cRegion\u201d and set the location as \u201cus-central1 (Iowa). \n\n* Select the bucket where you\u2019d like to store your data.\n\n* Import your dataset. (This may take a while.)\n\n* Once your dataset is done importing, take a close look at your imported data and make sure it looks the way you\u2019d expect.\n\n\n### Training our model\n\n\nIn order to train an AutoML model from inside Kaggle Notebooks, you\u2019ll need to attach a notebook to your Google Cloud Account. [This video goes into more detail](https:\/\/youtu.be\/xP99eh6nQN0?utm_medium=notebook&utm_source=kaggle&utm_campaign=automl-event). \n\n\nThen you can modify the following code to start your AutoML model training:"}}