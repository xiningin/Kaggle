{"cell_type":{"0aba5ceb":"code","92158a7b":"code","3b1d29dc":"code","04ab9815":"code","7c6fa6c9":"code","2344d7e2":"code","464c5432":"code","7111f2ca":"code","5455102e":"code","34ec6832":"code","69c385c8":"code","6dd69b6a":"code","c8fab1dc":"code","01ac09e1":"code","44130d53":"code","5a7bc5db":"code","0bed4b50":"code","abe20425":"code","16c9da37":"code","83762def":"code","7cdfddc4":"code","ad26f2b7":"code","20e41d81":"code","36f19ab0":"code","ea59210d":"code","a67dea94":"code","20dd1c7d":"code","b45d6f31":"code","7fd035e2":"code","46c8ee47":"code","3c786b4d":"code","0ed8f69c":"code","f47d0dca":"code","dede3251":"code","e18146f7":"code","b1cc02eb":"code","caa38d85":"code","a0b5e6fc":"code","2e2cf2c5":"code","ae28c2fa":"code","0e29bf25":"code","4001cef4":"code","b1104a7d":"code","eb8474a6":"code","386b94e8":"code","2f0b8f06":"code","a36098ad":"code","22a7c08d":"code","e911c341":"code","85a5201d":"code","33bb1e5f":"code","c6cc4e91":"code","049e4250":"code","ef20c2e6":"code","a3a66afa":"code","77e2a04b":"code","5bd7fa3e":"code","a1801c9f":"code","57a79c09":"code","1c931385":"code","6939a63a":"code","7ae0f17a":"code","f9b84282":"code","0a2ac92e":"code","f60abf8a":"code","c352145d":"code","a104d745":"code","6d5e987e":"code","985913dc":"code","7545a2da":"code","3a9d5e87":"code","d09666a8":"code","83b41761":"code","fe04cfd0":"code","bd0934a8":"markdown","9591cdf5":"markdown","a181f4db":"markdown","bb474d5c":"markdown","e756b90a":"markdown","f3cded4e":"markdown","379003c8":"markdown","60bc1aaa":"markdown","1c5eb67d":"markdown","bb414772":"markdown","654073f0":"markdown","49953d6e":"markdown","b1572aee":"markdown","ffc40a3e":"markdown","77ad9b37":"markdown","ba95a626":"markdown","d54b1dbb":"markdown","3d624a4a":"markdown","147ca04c":"markdown","aeeb6a56":"markdown","f2f91287":"markdown","975cee8f":"markdown","0558d946":"markdown","b22d8566":"markdown","30505e74":"markdown","d3ffeedf":"markdown","ab4c9005":"markdown","07e2d274":"markdown","69ddda3c":"markdown","b1464745":"markdown","7bd523ba":"markdown","4705958d":"markdown","0b0fe58d":"markdown","d7ed11d2":"markdown","13a08025":"markdown","5a38ef9f":"markdown","0b5fdab5":"markdown","4ed2c389":"markdown","08653d8a":"markdown","a0e70451":"markdown","28533e2c":"markdown","0ed7ffee":"markdown","18d86a6f":"markdown","32eed36a":"markdown","36af7324":"markdown","3286b582":"markdown","74255f7b":"markdown","8eb28ad4":"markdown","90780930":"markdown","e1eb106f":"markdown","d58c2484":"markdown","a28b26c3":"markdown","9efadf6a":"markdown","a230cab5":"markdown","67ffb385":"markdown","7c96a801":"markdown","0ef77559":"markdown","c49d837d":"markdown","18518dcd":"markdown","28efe74e":"markdown","84778441":"markdown","a3001dd0":"markdown","1726a17c":"markdown"},"source":{"0aba5ceb":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport warnings\nimport sklearn\nimport gensim\nimport scipy\nimport numpy\nimport json\nimport nltk\nimport sys\nimport csv\nimport os","92158a7b":"print('matplotlib: {}'.format(matplotlib.__version__))\nprint('scipy: {}'.format(scipy.__version__))\nprint('seaborn: {}'.format(sns.__version__))\nprint('pandas: {}'.format(pd.__version__))\nprint('numpy: {}'.format(np.__version__))\nprint('Python: {}'.format(sys.version))","3b1d29dc":"sns.set(style='white', context='notebook', palette='deep')\nwarnings.filterwarnings('ignore')\nsns.set_style('white')\n%matplotlib inline","04ab9815":"print(os.listdir(\"..\/input\/\"))","7c6fa6c9":"gendered_pronoun_df = pd.read_csv('..\/input\/test_stage_1.tsv', delimiter='\\t')","2344d7e2":"submission = pd.read_csv('..\/input\/sample_submission_stage_1.csv')","464c5432":"gendered_pronoun_df.shape","7111f2ca":"submission.shape","5455102e":"gendered_pronoun_df.head()","34ec6832":"gendered_pronoun_df.info()","69c385c8":"print(gendered_pronoun_df.Text.head())","6dd69b6a":"print(\"Shape of train set : \",gendered_pronoun_df.shape)","c8fab1dc":"gendered_pronoun_df.columns","01ac09e1":"def check_missing_data(df):\n    flag=df.isna().sum().any()\n    if flag==True:\n        total = df.isnull().sum()\n        percent = (df.isnull().sum())\/(df.isnull().count()*100)\n        output = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n        data_type = []\n        # written by MJ Bahmani\n        for col in df.columns:\n            dtype = str(df[col].dtype)\n            data_type.append(dtype)\n        output['Types'] = data_type\n        return(np.transpose(output))\n    else:\n        return(False)","44130d53":"check_missing_data(gendered_pronoun_df)","5a7bc5db":"gendered_pronoun_df[\"num_words\"] = gendered_pronoun_df[\"Text\"].apply(lambda x: len(str(x).split()))","0bed4b50":"#MJ Bahmani\nprint('maximum of num_words in data_df',gendered_pronoun_df[\"num_words\"].max())\nprint('min of num_words in data_df',gendered_pronoun_df[\"num_words\"].min())","abe20425":"gendered_pronoun_df[\"num_unique_words\"] = gendered_pronoun_df[\"Text\"].apply(lambda x: len(set(str(x).split())))\nprint('maximum of num_unique_words in train',gendered_pronoun_df[\"num_unique_words\"].max())\nprint('mean of num_unique_words in data_df',gendered_pronoun_df[\"num_unique_words\"].mean())","16c9da37":"gendered_pronoun_df[\"num_chars\"] = gendered_pronoun_df[\"Text\"].apply(lambda x: len(str(x)))\nprint('maximum of num_chars in data_df',gendered_pronoun_df[\"num_chars\"].max())","83762def":"from nltk.corpus import stopwords\neng_stopwords = set(stopwords.words(\"english\"))","7cdfddc4":"gendered_pronoun_df[\"num_stopwords\"] = gendered_pronoun_df[\"Text\"].apply(lambda x: len([w for w in str(x).lower().split() if w in eng_stopwords]))\n\nprint('maximum of num_stopwords in data_df',gendered_pronoun_df[\"num_stopwords\"].max())","ad26f2b7":"import string\ngendered_pronoun_df[\"num_punctuations\"] =gendered_pronoun_df['Text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]) )\nprint('maximum of num_punctuations in data_df',gendered_pronoun_df[\"num_punctuations\"].max())","20e41d81":"gendered_pronoun_df[\"num_words_upper\"] = gendered_pronoun_df[\"Text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\nprint('maximum of num_words_upper in data_df',gendered_pronoun_df[\"num_words_upper\"].max())","36f19ab0":"print(gendered_pronoun_df.columns)\ngendered_pronoun_df.head(1)","ea59210d":"pronoun=gendered_pronoun_df[\"Pronoun\"]","a67dea94":"np.unique(pronoun)","20dd1c7d":"## is suggested by  https:\/\/www.kaggle.com\/aavella77\nbinary = {\n    \"He\": 0,\n    \"he\": 0,\n    \"She\": 1,\n    \"she\": 1,\n    \"His\": 2,\n    \"his\": 2,\n    \"Him\": 3,\n    \"him\": 3,\n    \"Her\": 4,\n    \"her\": 4\n}\nfor index in range(len(gendered_pronoun_df)):\n    key = gendered_pronoun_df.iloc[index]['Pronoun']\n    gendered_pronoun_df.at[index, 'Pronoun_binary'] = binary[key]\ngendered_pronoun_df.head(30)","b45d6f31":"from wordcloud import WordCloud as wc\nfrom nltk.corpus import stopwords\ndef generate_wordcloud(text): \n    wordcloud = wc(relative_scaling = 1.0,stopwords = eng_stopwords).generate(text)\n    fig,ax = plt.subplots(1,1,figsize=(10,10))\n    ax.imshow(wordcloud, interpolation='bilinear')\n    ax.axis(\"off\")\n    ax.margins(x=0, y=0)\n    plt.show()","7fd035e2":"from nltk.corpus import stopwords\neng_stopwords = set(stopwords.words(\"english\"))","46c8ee47":"text =\" \".join(gendered_pronoun_df.Text)\ngenerate_wordcloud(text)","3c786b4d":"gendered_pronoun_df.hist();","0ed8f69c":"pd.plotting.scatter_matrix(gendered_pronoun_df,figsize=(10,10))\nplt.figure();","f47d0dca":"sns.jointplot(x='Pronoun-offset',y='A-offset' ,data=gendered_pronoun_df, kind='reg')","dede3251":"sns.swarmplot(x='Pronoun-offset',y='B-offset',data=gendered_pronoun_df);","e18146f7":"sns.distplot(gendered_pronoun_df[\"Pronoun-offset\"])","b1cc02eb":"sns.violinplot(data=gendered_pronoun_df,x=\"Pronoun_binary\", y=\"num_words\")","caa38d85":"from nltk.tokenize import sent_tokenize, word_tokenize","a0b5e6fc":"gendered_pronoun_df.Text[0]","2e2cf2c5":"our_text=gendered_pronoun_df.Text[0]","ae28c2fa":"print(word_tokenize(our_text))","0e29bf25":"from nltk.tokenize import sent_tokenize, word_tokenize\nprint(sent_tokenize(our_text))","4001cef4":"from nltk.tokenize import sent_tokenize, word_tokenize\n \nphrases = sent_tokenize(our_text)\nwords = word_tokenize(our_text)\nprint(phrases)","b1104a7d":"print(words)","eb8474a6":"type(words)","386b94e8":"from nltk.tokenize import sent_tokenize, word_tokenize\nfrom nltk.corpus import stopwords","2f0b8f06":" \n\nstopWords = set(stopwords.words('english'))\nwords = word_tokenize(our_text)\nwordsFiltered = []\n \nfor w in words:\n    if w not in stopWords:\n        wordsFiltered.append(w)\n \nprint(wordsFiltered)","a36098ad":"from nltk.corpus import stopwords\n","22a7c08d":"stopWords = set(stopwords.words('english'))\n","e911c341":"print(len(stopWords))\nprint(stopWords)","85a5201d":"for w in words:\n    if w not in stopWords:\n        wordsFiltered.append(w)","33bb1e5f":"our_text=gendered_pronoun_df.Text[0]\n","c6cc4e91":"from nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize","049e4250":"from nltk.stem import PorterStemmer\nfrom nltk.tokenize import sent_tokenize, word_tokenize\n\n\nps = PorterStemmer()\n \nfor word in word_tokenize(our_text):\n    print(ps.stem(word))","ef20c2e6":"import nltk\nfrom nltk.tokenize import PunktSentenceTokenizer\n \n\nsentences = nltk.sent_tokenize(our_text)   \nfor sent in sentences:\n    print(nltk.pos_tag(nltk.word_tokenize(sent)))","a3a66afa":"import nltk\nfrom nltk.corpus import state_union\nfrom nltk.tokenize import PunktSentenceTokenizer\n \n\nsentences = nltk.sent_tokenize(our_text)   \n \ndata = []\nfor sent in sentences:\n    data = data + nltk.pos_tag(nltk.word_tokenize(sent))\n \nfor word in data: \n    if 'NNP' in word[1]: \n        print(word)","77e2a04b":"#https:\/\/pythonspot.com\/natural-language-processing-prediction\/\nfrom nltk.corpus import names\n \n# Load data and training \nnames = ([(name, 'male') for name in names.words('male.txt')] + [(name, 'female') for name in names.words('female.txt')])","5bd7fa3e":"[(u'Aaron', 'male'), (u'Abbey', 'male'), (u'Abbie', 'male')]\n[(u'Zorana', 'female'), (u'Zorina', 'female'), (u'Zorine', 'female')]","a1801c9f":"def gender_features(word): \n    return {'last_letter': word[-1]}","57a79c09":"#Based on https:\/\/pythonspot.com\/category\/nltk\/\nimport nltk.classify.util\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.corpus import names\n \ndef gender_features(word): \n    return {'last_letter': word[-1]} \n \n# Load data and training \nnames = ([(name, 'male') for name in names.words('male.txt')] + \n\t [(name, 'female') for name in names.words('female.txt')])\n \nfeaturesets = [(gender_features(n), g) for (n,g) in names] \ntrain_set = featuresets\nclassifier = nltk.NaiveBayesClassifier.train(train_set) \n \n# Predict\nprint(classifier.classify(gender_features('Frank')))","1c931385":"# Predict, you can change name\nname = 'Sarah'\nprint(classifier.classify(gender_features(name)))","6939a63a":"positive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]\nnegative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]\nneutral_vocab = [ 'movie','the','sound','was','is','actors','did','know','words','not' ]","7ae0f17a":"def word_feats(words):\n    return dict([(word, True) for word in words])\n \npositive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\nnegative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\nneutral_features = [(word_feats(neu), 'neu') for neu in neutral_vocab]","f9b84282":"train_set = negative_features + positive_features + neutral_features","0a2ac92e":"#Based on https:\/\/pythonspot.com\/category\/nltk\/\nimport nltk.classify.util\nfrom nltk.classify import NaiveBayesClassifier\nfrom nltk.corpus import names\n \ndef word_feats(words):\n    return dict([(word, True) for word in words])\n \npositive_vocab = [ 'awesome', 'outstanding', 'fantastic', 'terrific', 'good', 'nice', 'great', ':)' ]\nnegative_vocab = [ 'bad', 'terrible','useless', 'hate', ':(' ]\nneutral_vocab = [ 'movie','the','sound','was','is','actors','did','know','words','not' ]\n \npositive_features = [(word_feats(pos), 'pos') for pos in positive_vocab]\nnegative_features = [(word_feats(neg), 'neg') for neg in negative_vocab]\nneutral_features = [(word_feats(neu), 'neu') for neu in neutral_vocab]\n \ntrain_set = negative_features + positive_features + neutral_features\n \nclassifier = NaiveBayesClassifier.train(train_set) \n \n# Predict\nneg = 0\npos = 0\n##sentence = \"Awesome movie, I liked it\"\nour_text = our_text.lower()\nwords = our_text.split(' ')\nfor word in words:\n    classResult = classifier.classify( word_feats(word))\n    if classResult == 'neg':\n        neg = neg + 1\n    if classResult == 'pos':\n        pos = pos + 1\n \nprint('Positive: ' + str(float(pos)\/len(words)))\nprint('Negative: ' + str(float(neg)\/len(words)))","f60abf8a":"import spacy","c352145d":"nlp = spacy.load('en')\ndoc = nlp(our_text)\ni=0\nfor token in doc:\n    i=i+1;\n    if i<20:\n        print('\"' + token.text + '\"')","a104d745":"nlp = spacy.load('en')\ndoc=nlp(our_text)\ni=0\nfor sent in doc.sents:\n    i=i+1\n    print(i,' - ',sent)","6d5e987e":"doc = nlp( our_text)\nprint([(token.text, token.tag_) for token in doc])","985913dc":"doc = nlp(our_text)\nfor ent in doc.ents:\n    print(ent.text, ent.label_)","7545a2da":"from spacy import displacy\n \ndoc = nlp(our_text )\ndisplacy.render(doc, style='ent', jupyter=True)","3a9d5e87":"from spacy import displacy\n \ndoc = nlp(our_text)\ndisplacy.render(doc, style='dep', jupyter=True, options={'distance': 90})","d09666a8":"import gensim\nfrom gensim import corpora\nfrom pprint import pprint\n# How to create a dictionary from a list of sentences?\ndocuments = [\" Zoe Telford  played the police officer girlfriend of Simon, Maggie.\",\n             \"Dumped by Simon in the final episode of series 1, after he slept with Jenny, and is not seen again.\",\n             \"Phoebe Thomas played Cheryl Cassidy, Paulines friend and also a year 11 pupil in Simons class.\", \n             \"Dumped her boyfriend following Simons advice after he wouldnt \",\n             \"have sex with her but later realised this was due to him catching crabs off her friend Pauline.\"]\n\ndocuments_2 = [\"One source says the report will likely conclude that\", \n                \"the operation was carried out without clearance and\", \n                \"transparency and that those involved will be held\", \n                \"responsible. One of the sources acknowledged that the\", \n                \"report is still being prepared and cautioned that\", \n                \"things could change.\"]","83b41761":"\ntexts = [[text for text in doc.split()] for doc in documents]\n\n# Create dictionary\ndictionary = corpora.Dictionary(texts)\n\n# Get information about the dictionary\nprint(dictionary)","fe04cfd0":"# \nprint(dictionary.token2id)","bd0934a8":"<a id=\"1\"><\/a> <br>\n# 1-Introduction\nThis Kernel is mostly for **beginners**, and of course, all **professionals** who think they need to review  their  knowledge.\nAlso, we introduce and teach three known libraries ( NLTK+spaCy+Gensim) for text processing And we will introduce for each of them some examples based on [gendered-pronoun-resolution](https:\/\/www.kaggle.com\/c\/gendered-pronoun-resolution).","9591cdf5":"If you want to give the name during runtime, change the last line to:","a181f4db":"Training and prediction\nWe train and predict using:","bb474d5c":"<a id=\"6\"><\/a> <br>\n# 6- References & Credits\n1. [https:\/\/www.coursera.org\/specializations\/data-science-python](https:\/\/www.coursera.org\/specializations\/data-science-python)\n1. [https:\/\/github.com\/chirayukong\/gensim](https:\/\/github.com\/chirayukong\/gensim)\n1. [https:\/\/pythonspot.com\/category\/nltk\/](https:\/\/pythonspot.com\/category\/nltk\/)\n1. [sunscrapers](https:\/\/sunscrapers.com\/blog\/6-best-python-natural-language-processing-nlp-libraries\/)\n1. [spacy](https:\/\/spacy.io\/)\n1. [gensim](https:\/\/pypi.org\/project\/gensim\/)\n1. [nlpforhackers](https:\/\/nlpforhackers.io\/complete-guide-to-spacy\/)\n1. [a-sentiment-analysis-approach-to-predicting-stock-returns](https:\/\/medium.com\/@tomyuz\/a-sentiment-analysis-approach-to-predicting-stock-returns-d5ca8b75a42)\n1. [machinelearningplus](https:\/\/www.machinelearningplus.com\/nlp\/gensim-tutorial\/)\n1. [https:\/\/towardsdatascience.com\/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21](https:\/\/towardsdatascience.com\/topic-modelling-in-python-with-nltk-and-gensim-4ef03213cd21)\n###### [Go to top](#top)","e756b90a":"You can define your own set of tuples if you wish, its simply a list containing many tuples.\n\nFeature extraction\nBased on the dataset, we prepare our feature. The feature we will use is the last letter of a name:\nWe define a featureset using:","f3cded4e":"### 1-5-4-1 WordCloud","379003c8":"<a id=\"261\"><\/a> <br>\n## 2-6-1 NLP Prediction Example Based on pythonspot\nGiven a name, the classifier will predict if it\u2019s a male or female.\n\nTo create our analysis program, we have several steps:\n\n1. Data preparation\n1. Feature extraction\n1. Training\n1. Prediction\n1. Data preparation\nThe first step is to prepare data. We use the names set included with nltk.[https:\/\/pythonspot.com\/natural-language-processing-prediction\/](https:\/\/pythonspot.com\/natural-language-processing-prediction\/)","60bc1aaa":"This dataset is simply a collection of tuples. To give you an idea of what the dataset looks like:","1c5eb67d":"<a id=\"24\"><\/a> <br>\n## 2-4 NLTK \u2013 Stemming\nStemming is the process of producing morphological variants of a root\/base word. Stemming programs are commonly referred to as stemming algorithms or stemmers. A stemming algorithm reduces the words \u201cchocolates\u201d, \u201cchocolatey\u201d, \u201cchoco\u201d to the root word, \u201cchocolate\u201d and \u201cretrieval\u201d, \u201cretrieved\u201d, \u201cretrieves\u201d reduce to the stem \u201cretrieve\u201d.[https:\/\/www.geeksforgeeks.org\/python-stemming-words-with-nltk\/](https:\/\/www.geeksforgeeks.org\/python-stemming-words-with-nltk\/).\n<img src='https:\/\/pythonspot-9329.kxcdn.com\/wp-content\/uploads\/2016\/08\/word-stem.png.webp'>\n[Image-credit](https:\/\/pythonspot.com\/nltk-stemming\/)\n\nStart by defining some words:","bb414772":"classifier = NaiveBayesClassifier.train(train_set)","654073f0":"<a id=\"154\"><\/a> <br>\n## 1-5-4 Some New Features\nIn this section, I will extract a few new statistical features from the text field","49953d6e":"<a id=\"23\"><\/a> <br>\n## 2-3 NLTK Stop Words\nNatural language processing (nlp) is a research field that presents many challenges such as natural language understanding.\nText may contain stop words like \u2018the\u2019, \u2018is\u2019, \u2018are\u2019. Stop words can be filtered from the text to be processed. There is no universal list of stop words in nlp research, however the nltk module contains a list of stop words.\n\nIn this article you will learn how to remove stop words with the nltk module.[https:\/\/pythonspot.com\/nltk-stop-words\/](https:\/\/pythonspot.com\/nltk-stop-words\/)","b1572aee":"## 3-1 Sentence detection\n","ffc40a3e":"We start by defining 3 classes: positive, negative and neutral.\nEach of these is defined by a vocabulary:","77ad9b37":"<a id=\"14\"><\/a> <br>\n## 1-4 Data set","ba95a626":"Our training set is then the sum of these three feature sets:","d54b1dbb":"<a id=\"27\"><\/a> <br>\n## 2-7 Python Sentiment Analysis\nIn Natural Language Processing there is a concept known as **Sentiment Analysis**. in this section we use this great [**course**](https:\/\/pythonspot.com\/category\/nltk\/) to explain Sentiment Analysis\n\n<img src='https:\/\/s3.amazonaws.com\/com.twilio.prod.twilio-docs\/images\/SentimentAnalysis.width-800.png'>\n[image-credit](https:\/\/www.twilio.com\/docs\/glossary\/what-is-sentiment-analysis)\n\n1. Given a movie review or a tweet, it can be automatically classified in categories.\n1. These categories can be user defined (positive, negative) or whichever classes you want.\n1. Classification is done using several steps: training and prediction.\n1. The training phase needs to have training data, this is example data in which we define examples. \n1. The classifier will use the training data to make predictions.","3d624a4a":"<a id=\"11\"><\/a> <br>\n##   1-1 Import","147ca04c":"We get a set of English stop words using the line:\n\n","aeeb6a56":"### Number of stopwords in the text","f2f91287":"The returned list stopWords contains 153 stop words on my computer.\nYou can view the length or contents of this array with the lines:","975cee8f":"## 3-2 Part Of Speech Tagging","0558d946":"Every word is converted into a feature using a simplified bag of words model:","b22d8566":"### Number of punctuations in the text\n","30505e74":" <a id=\"top\"><\/a> <br>\n## Notebook  Content\n1. [Introduction](#1)\n    1. [Import](#11)\n    1. [Version](#12)\n    1. [Setup](#13)\n    1. [Data set](#14)\n    1. [Gendered Pronoun Analysis](#15)\n        1. [Problem Feature](#151)\n        1. [Variables](#152)\n1. [NLTK](#2)\n    1. [Tokenizing sentences](#21)\n    1. [NLTK and arrays](#22)\n    1. [NLTK stop words](#23)\n    1. [NLTK \u2013 stemming](#24)\n    1. [NLTK speech tagging](#25)\n    1. [Natural Language Processing \u2013 prediction](#26)\n        1. [nlp prediction example](#261)\n    1. [nlp prediction example](#27)\n1. [spaCy](#3)\n    1. [Sentence detection](#31)\n    1. [Part Of Speech Tagging](#32)\n    1. [spaCy](#33)\n    1. [displaCy](#34)\n1. [Gensim](#4)\n1. [Comparison of Python NLP libraries by Activewizards](#5)\n1. [References](#6)","d3ffeedf":"<a id=\"5\"><\/a> <br>\n## 5- Comparison of Python NLP libraries by Activewizards","ab4c9005":"<a id=\"21\"><\/a> <br>\n## 2-1 Tokenizing sentences\n**What is Tokenizer?**\nTokenizing raw text data is an important pre-processing step for many NLP methods. As explained on wikipedia, tokenization is \u201cthe process of breaking a stream of text up into words, phrases, symbols, or other meaningful elements called tokens.\u201d In the context of actually working through an NLP analysis, this usually translates to converting a string like \"My favorite color is blue\" to a list or array like [\"My\", \"favorite\", \"color\", \"is\", \"blue\"].[**http:\/\/tint.fbk.eu\/tokenization.html**](http:\/\/tint.fbk.eu\/tokenization.html)","07e2d274":"### Number of characters in the text","69ddda3c":"<a id=\"3\"><\/a> <br>\n# 3- spaCy\n<img src='https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/8\/88\/SpaCy_logo.svg\/1920px-SpaCy_logo.svg.png' width=400 height=400>\nspaCy is an Industrial-Strength Natural Language Processing in python. [**spacy**](https:\/\/spacy.io\/)","b1464745":"## 3-3 Named Entity Recognition\n","7bd523ba":"## 3-4 displaCy ","4705958d":"<a id=\"22\"><\/a> <br>\n## 2-2 NLTK and Arrays\nIf you wish to you can store the words and sentences in arrays.","0b0fe58d":"To get started, we first select a few sentences from the data set.","d7ed11d2":"visualizing the dependency tree!","13a08025":"<img src='http:\/\/s9.picofile.com\/file\/8351628176\/nlp.png' width=600 height=600 >\n<div style=\"text-align:center\">last update: <b>12\/02\/2019<\/b><\/div>\n\n\n>You are reading **10 Steps to Become a Data Scientist** and are now in the 8th step : \n\n1. [Leren Python](https:\/\/www.kaggle.com\/mjbahmani\/the-data-scientist-s-toolbox-tutorial-1)\n2. [Python Packages](https:\/\/www.kaggle.com\/mjbahmani\/the-data-scientist-s-toolbox-tutorial-2)\n3. [Mathematics and Linear Algebra](https:\/\/www.kaggle.com\/mjbahmani\/linear-algebra-for-data-scientists)\n4. <font color=\"red\">You are in the 4th step<\/font>\n5. [Big Data](https:\/\/www.kaggle.com\/mjbahmani\/a-data-science-framework-for-quora)\n6. [Data visualization](https:\/\/www.kaggle.com\/mjbahmani\/top-5-data-visualization-libraries-tutorial)\n7. [Data Cleaning](https:\/\/www.kaggle.com\/mjbahmani\/machine-learning-workflow-for-house-prices)\n8. [Tutorial-on-ensemble-learning](https:\/\/www.kaggle.com\/mjbahmani\/tutorial-on-ensemble-learning)\n9. [A Comprehensive ML  Workflow with Python](https:\/\/www.kaggle.com\/mjbahmani\/a-comprehensive-ml-workflow-with-python)\n10. [Deep Learning](https:\/\/www.kaggle.com\/mjbahmani\/top-5-deep-learning-frameworks-tutorial)\n\n\n\n---------------------------------------------------------------------\nYou can Fork and Run this kernel on Github:\n> ###### [ GitHub](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist)\n\n-------------------------------------------------------------------------------------------------------------\n\n **I hope you find this kernel helpful and some <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated**\n \n -----------","5a38ef9f":"We train the classifier:","0b5fdab5":"We can filter this data based on the type of word:","4ed2c389":"<a id=\"26\"><\/a> <br>\n## 2-6 Natural Language Processing \u2013 prediction\nWe can use natural language processing to make predictions. Example: Given a product review, a computer can predict if its positive or negative based on the text. In this article you will learn how to make a prediction program based on natural language processing.","08653d8a":"We create a new list called wordsFiltered which contains all words which are not stop words.\nTo create it we iterate over the list of words and only add it if its not in the stopWords list.","a0e70451":"Go to first step: [**Course Home Page**](https:\/\/www.kaggle.com\/mjbahmani\/10-steps-to-become-a-data-scientist)\n\nGo to next step : [**Mathematics and Linear Algebra**](https:\/\/www.kaggle.com\/mjbahmani\/linear-algebra-for-data-scientists)","28533e2c":"<a id=\"25\"><\/a> <br>\n## 2-5 NLTK speech tagging\nThe **module NLTK** can automatically **tag speech**.\nGiven a sentence or paragraph, It can label words such as verbs, nouns and so on.\n\nThe example below automatically tags words with a corresponding class.[https:\/\/www.nltk.org\/book\/ch05.html](https:\/\/www.nltk.org\/book\/ch05.html)","0ed7ffee":"If you are using Windows or Linux or Mac, you can install NLTK using pip:\n>**$ pip install nltk**\n\nYou can use NLTK on Python 2.7, 3.4, and 3.6.","18d86a6f":"### Number of title case words in the text","32eed36a":"<a id=\"12\"><\/a> <br>\n## 1-2 Version","36af7324":"A module has been imported:\n\n","3286b582":"<a id=\"152\"><\/a> <br>\n### 1-5-2  Variables\n\n1. ID - Unique identifier for an example (Matches to Id in output file format)\n1. Text - Text containing the ambiguous pronoun and two candidate names (about a paragraph in length)\n1. Pronoun - The target pronoun (text)\n1. Pronoun-offset The character offset of Pronoun in Text\n1. A - The first name candidate (text)\n1. A-offset - The character offset of name A in Text\n1. B - The second name candidate\n1. B-offset - The character offset of name B in Text\n1. URL - The URL of the source Wikipedia page for the example","74255f7b":"## 1-5-4 Visualization","8eb28ad4":"### Tokenize(split) the sentences into words","90780930":"This example classifies sentences according to the training set.","e1eb106f":"<a id=\"14\"><\/a> <br>\n# Top 3 NLP Libraries Tutorial\n1. NLTK\n1. spaCy\n1. Gensim","d58c2484":"### Number of words in the text","a28b26c3":"<a id=\"4\"><\/a> <br>\n# 4- Gensim\nGensim is a Python library for topic modelling, document indexing and similarity retrieval with large corpora. Target audience is the natural language processing (NLP) and information retrieval (IR) community.[https:\/\/github.com\/chirayukong\/gensim](https:\/\/github.com\/chirayukong\/gensim)\n1. Gensim is a FREE Python library\n1. Scalable statistical semantics\n1. Analyze plain-text documents for semantic structure\n1. Retrieve semantically similar documents. [https:\/\/radimrehurek.com\/gensim\/](https:\/\/radimrehurek.com\/gensim\/)","9efadf6a":"### Number of unique words in the text","a230cab5":"<a id=\"153\"><\/a> <br>\n### 1-5-3  Evaluation\nSubmissions are evaluated using the multi-class logarithmic loss. Each pronoun has been labeled with whether it refers to A, B, or NEITHER. For each pronoun, you must submit a set of predicted probabilities (one for each class). The formula is :\n<img src='http:\/\/s8.picofile.com\/file\/8351608076\/1.png'>","67ffb385":"<img src='https:\/\/activewizards.com\/content\/blog\/Comparison_of_Python_NLP_libraries\/nlp-librares-python-prs-and-cons01.png'>","7c96a801":"<a id=\"2\"><\/a> <br>\n# 2- NLTK\nThe Natural Language Toolkit (NLTK) is one of the leading platforms for working with human language data and Python, the module NLTK is used for natural language processing. NLTK is literally an acronym for Natural Language Toolkit. with it you can tokenizing words and sentences.[https:\/\/www.nltk.org\/](https:\/\/www.nltk.org\/)\n<br>\nNLTK is a library of Python that can mine (scrap and upload data) and analyse very large amounts of textual data using computational methods.this tutorial is based on **this great course** [**https:\/\/pythonspot.com\/category\/nltk\/**](https:\/\/pythonspot.com\/category\/nltk\/)\n<img src='https:\/\/arts.unimelb.edu.au\/__data\/assets\/image\/0005\/2735348\/nltk.jpg' width=400 height=400>","0ef77559":"featuresets = [(gender_features(n), g) for (n,g) in names]\nand the features (last letters) are extracted using:","c49d837d":"## Check Missing Data","18518dcd":"### Show the word to id map","28efe74e":"And stem the words in the list using:","84778441":">###### you may  be interested have a look at it: [**10-steps-to-become-a-data-scientist**](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist)\n\n\n---------------------------------------------------------------------\nYou can Fork and Run this kernel on Github:\n> ###### [ GitHub](https:\/\/github.com\/mjbahmani\/10-steps-to-become-a-data-scientist)\n\n-------------------------------------------------------------------------------------------------------------\n\n **I hope you find this kernel helpful and some <font color=\"red\"><b>UPVOTES<\/b><\/font> would be very much appreciated**\n \n -----------","a3001dd0":"<a id=\"15\"><\/a> <br>\n## 1-5 Gendered Pronoun Data set Analysis\n<img src='https:\/\/storage.googleapis.com\/kaggle-media\/competitions\/GoogleAI-GenderedPronoun\/PronounResolution.png' width=600 height=600>\n**Pronoun resolution** is part of coreference resolution, the task of pairing an expression to its referring entity. This is an important task for natural language understanding, and the resolution of ambiguous pronouns is a longstanding challenge. for more information you can check this [link](https:\/\/www.kaggle.com\/c\/gendered-pronoun-resolution)\n<a id=\"151\"><\/a> <br>\n### 1-5-1 Problem Feature\nIn this competition, you must identify the target of a pronoun within a text passage. The source text is taken from Wikipedia articles. You are provided with the pronoun and two candidate names to which the pronoun could refer. You must create an algorithm capable of deciding whether the pronoun refers to name A, name B, or neither.","1726a17c":"<a id=\"13\"><\/a> <br>\n## 1-3 Setup\n\nA few tiny adjustments for better **code readability**"}}