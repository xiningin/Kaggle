{"cell_type":{"ff1a7a0a":"code","e807d90a":"code","78fec960":"code","ef1923ea":"code","4e2cb323":"code","fda67287":"markdown","add0edac":"markdown","3559008d":"markdown","29e1568a":"markdown","a17514e3":"markdown","96fef6b6":"markdown","3022789b":"markdown","27e79619":"markdown","aa5ff1c4":"markdown","1c693480":"markdown","3db95c95":"markdown"},"source":{"ff1a7a0a":"df.isnull()  \n# Returns a boolean matrix, if the value is NaN then True otherwise,\n\nFalsedf.isnull().sum() \n# Returns the column names along with the number of NaN values in that particular column\n \n# The easiest way to solve this problem is by dropping the rows or columnsthat contain null values.\ndf.dropna()","e807d90a":"from sklearn.impute import SimpleImputer\nimputer = SimpleImputer(missing_values=np.nan, strategy='mean')\nimputer = imputer.fit(df[['Weight']])\ndf['Weight'] = imputer.transform(df[['Weight']])\n\n# .values used here return a numpy representation of the data frame.\n#Only the values in the data frame will be returned, the axes labels will be removed.","78fec960":"from sklearn.preprocessing import StandardScaler\nstd = StandardScaler()\nX = std.fit_transform(df[['Age','Weight']])","ef1923ea":"# Handling Ordinal Categorical Variables \u2014\n# First of all, we need to create a dataframe.\ndf_cat = pd.DataFrame(data = \n                     [['green','M',10.1,'class1'],\n                      ['blue','L',20.1,'class2'],\n                      ['white','M',30.1,'class1']])\ndf_cat.columns = ['color','size','price','classlabel']","4e2cb323":"# 1.Using map() function \u2014\nsize_mapping = {'M':1,'L':2}\ndf_cat['size'] = df_cat['size'].map(size_mapping)\n\nHere M will be replaced with 1 and L with 2.\n\n# 2.Using Label Encoder \u2014\nfrom sklearn.preprocessing import LabelEncoder\nclass_le = LabelEncoder()\ndf_cat['classlabel'] =\nclass_le.fit_transform(df_cat['classlabel'].values)\n\nHere class1 will be represented with 0 and class2 with 1 .","fda67287":"## Standardization\nIt is another integral preprocessing step. In Standardization, we transform our values such that the mean of the values is 0 and the standard deviation is 1.\n> ![image.png](attachment:image.png)\n\nConsider the above data frame, here we have 2 numerical values: Age and Weight. They are not on the same scale as Age is in years and Weight is in Kg and since Weight is more likely to greater than Age ; therefore, our model will give more weightage to Weight, which is not the ideal scenario as Age is also an integral factor here. In order to avoid this issue, we perform **Standardization**.So in simple terms, we just calculate the mean and standard deviation of the values and then for each data point we just subtract the mean and divide it by standard deviation.\n\n**Example \u2014**\nConsider the column Age from Dataframe 1. In order to standardize this column, we need to calculate the mean and standard deviation and then we will transform each value of age using the above formula.\nWe don\u2019t need to do this process manually as sklearn provides a function called **StandardScaler**.","add0edac":"## Feature Scaling\nThe final step of data preprocessing is to apply the very important feature scaling.The formula and graphical representation of **Euclidean distance**\n>![image.png](attachment:image.png)\n\n* **But what is it?**\nIt is a method used to standardize the range of independent variables or features of data.\n* **But why is it necessary?** \nA lot of machine learning models are based on Euclidean distance. If, for example, the values in one column (x) is much higher than the value in another column (y), (x2-x1) squared will give a far greater value than (y2-y1) squared. So clearly, one square difference dominates over the other square difference. In the machine learning equations, the square difference with the lower value in comparison to the far greater value will almost be treated as if it does not exist. We do not want that to happen. That is why it is necessary to transform all our variables into the same scale. There are several ways of scaling the data. One way is called Standardization which may be used. For every observation of the selected column, our program will apply the formula of standardization and fit it to a scale.\n \nTo accomplish the job, we will import the class StandardScaler from the sckit preprocessing library and as usual create an object of that class.\n>from sklearn.preprocessing import StandardScaler\nsc_X = StandardScaler()\n\nNow we will fit and transform our X_train set (It is important to note that when applying the Standard Scalar object on our training and test sets, we can simply transform our test set but for our training set we have to at first fit it and then transform the set). That will transform all the data to a same standardized scale.\n\n>X_train = sc_X.fit_transform(X_train)\nX_test = sc_X.transform(X_test)\n\nThese are the general 6 steps of preprocessing the data before using it for machine learning. Depending on the condition of your dataset, you may or may not have to go through all these steps.","3559008d":"## Handling Categorical Variables\nHandling categorical variables is another integral aspect of Machine Learning. Categorical variables are basically the variables that are discrete and not continuous. *Ex \u2014 color of an item is a discrete variable whereas its price is a continuous variable.\nCategorical variables are further divided into 2 types \u2014\n* Ordinal categorical variables \u2014 These variables can be ordered. *Ex \u2014 Size of a T-shirt. We can say that M<L<XL.\n* Nominal categorical variables \u2014 These variables can\u2019t be ordered. Ex \u2014 Color of a T-shirt. We can\u2019t say that Blue<Green as it doesn\u2019t make any sense to compare the colors as they don\u2019t have any relationship.\n\n> The important thing to note here is that we need to preprocess ordinal and nominal categorical variables differently.","29e1568a":"## DATA PREPROCESSING\n\nData preprocessing is an integral step in Machine Learning as the quality of data and the useful information that can be derived from it directly affects the ability of our model to learn; therefore, it is extremely important that we preprocess our data before feeding it into our model.\n* Handling Null Values\n* Standardization\n* Handling Categorical Variables\n* One-Hot Encoding\n* Multicollinearity","a17514e3":"**Incorrect way of handling Nominal Categorical Variables \u2014**\nThe *biggest mistake* that most people make is that they are not able to differentiate between ordinal and nominal CVs.So if you use the same map() function or LabelEncoder with nominal variables then the model will think that there is some sort of relationship between the nominal CVs.\n\nSo if we use map() to map the colors like -\ncol_mapping = {'Blue':1,'Green':2}\n\nThen according to the model, Green > Blue, which is a senseless assumption and the model will give you results considering this relationship. So, although you will get the results using this method they won\u2019t be optimal.\n\n**Correct way of handling Nominal Categorical Variables \u2014**\nThe correct way of handling nominal CVs is to use One-Hot Encoding. The easiest way to use One-Hot Encoding is to use the get_dummies() function.\n\n>df_cat=pd.get_dummies\n          (df_cat[['color','size','price']])\n          \nHere we have passed \u2018size\u2019 and \u2018price\u2019 along with \u2018color\u2019 but the get_dummies() function is pretty smart and will consider only the string variables. So it will just transform the \u2018color\u2019 variable.\nNow, you must be wondering what the hell is this One-Hot Encoding.","96fef6b6":"The important thing to note here is that we need to standardize both training and testing data.\n* fit_transform is equivalent to using fit and then transform.\n* fit function calculates the mean and standard deviation and the transform function actually standardizes the dataset and we can do this process in a single line of code using the fit_transform function.\n\n> Another important thing to note here is that we will use only the transform method when dealing with the test data.","3022789b":"## Multicollinearity and its impact\nMulticollinearity occurs in our dataset when we have features which are strongly dependent on each other. Ex- In this case we have features -\ncolor_blue,color_green and color_white which are all dependent on each other and it can impact our model.\n>If we have multicollinearity in our dataset then we won\u2019t be able to use our weight vector to calculate the feature importance.\n\n>Multicollinearity impacts the interpretability of our model.\n\nI think this much of information is enough in the context of Machine Learning however if you are still not convinced, then you can visit the below link to understand the maths and logic associated with Multicollinearity.\nNow that we have understood what Multicollinearity is, let\u2019s now try to understand how to identify it.\n* The easiest method to identify Multicollinearity is to just plot a pairplot and you can observe the relationships between different features. If you get a linear relationship between 2 features then they are strongly correlated with each other and there is multicollinearity in your dataset.\n> ![image.png](attachment:image.png)\n\nHere (Weight,BP) and (BSA,BP) are closely related. You can also use the correlation matrix to check how closely related the features are.\n \ncorrelation Matrix\nWe can observe that there is a strong co-relation (0.950) between Weight and BP and also between BSA and BP (0.875).\nSimple hack to avoid Multicollinearity-\nWe can use drop_first=True in order to avoid the problem of Multicollinearity.\n\n>df_cat = pd.get_dummies(df_cat[['color','size','price']],drop_first=True)\n\nHere drop_first will drop the first column of color. So here color_blue will be dropped and we will only have color_green and color_white.\nThe important thing to note here is that we don\u2019t lose any information because if color_green and color_white are both 0 then it implies that the color must have been blue. So we can infer the whole information with the help of only these 2 columns, hence the strong correlation between these three columns is broken.","27e79619":"dropna() takes various parameters like :\n* axis \u2014 We can specify axis=0 if we want to remove the rows and axis=1 if we want to remove the columns.\n* how \u2014 If we specify how = \u2018all\u2019 then the rows and columns will only be dropped if all the values are NaN.By default how is set to \u2018any\u2019.\n* thresh \u2014 It determines the threshold value so if we specify thresh=5 then the rows having less than 5 real values will be dropped.\n* subset \u2014If we have 4 columns A, B, C and D then if we specify subset=[\u2018C\u2019] then only the rows that have their C value as NaN will be removed.\n* inplace \u2014 By default, no changes will be made to your dataframe. So if you want these changes to reflect onto your dataframe then you need to use inplace = True.\n\nHowever, it is not the best option to remove the rows and columns from our dataset as it can lead to loss of valuable information. If you have 300K data points then removing 2\u20133 rows won\u2019t affect your dataset much but if you only have 100 data points and out of which 20 have NaN values for a particular field then you can\u2019t simply drop those rows. In real-world datasets it can happen quite often that you have a large number of NaN values for a particular field.\n\nEx \u2014 Suppose we are collecting the data from a survey, then it is possible that there could be an optional field which let\u2019s say 20% of people left blank. So when we get the dataset then we need to understand that the remaining 80% data is still useful, so rather than dropping these values we need to somehow substitute the missing 20% values. We can do this with the help of Imputation.\n\n**Imputation-**\nImputation is simply the process of substituting the missing values of our dataset. We can do this by defining our own customised function or we can simply perform imputation by using the SimpleImputer class provided by sklearn.","aa5ff1c4":"## Handling Null Values\nIn any real-world dataset there are always few null values. It doesn\u2019t really matter whether it is a regression, classification or any other kind of problem, no model can handle these NULL or NaN values on its own so we need to intervene.\n>In python NULL is reprsented with NaN. So don\u2019t get confused between these two,they can be used interchangably.\n\nFirst of all, we need to check whether we have null values in our dataset or not. We can do that using the isnull() method.    ","1c693480":"Here the columns \u2018size\u2019 and \u2018classlabel\u2019 are ordinal categorical variables whereas \u2018color\u2019 is a nominal categorical variable.\nThere are 2 pretty simple and neat techniques to transform ordinal CVs.","3db95c95":"## One-Hot Encoding\nSo in One-Hot Encoding what we essentially do is that we create \u2019n\u2019 columns where n is the number of unique values that the nominal variable can take.\n\nEx \u2014 Here if color can take Blue,Green and White then we will just create three new columns namely \u2014 color_blue,color_green and color_white and if the color is green then the values of color_blue and color_white column will be 0 and value of color_green column will be 1 .\n\nSo out of the n columns, only one column can have value = 1 and the rest all will have value = 0.\nOne-Hot Encoding is a pretty cool and neat hack but there is only one problem associated with it and that is Multicollinearity. As you all must have assumed that it is a pretty heavy word so it must be difficult to understand, so let me just validate your newly formed belief.**Multicollinearity** is indeed a slightly tricky but extremely important concept of Statistics. The good thing here is that we don\u2019t really need to understand all the nitty-gritty details of multicollinearity, rather we just need to focus on how it will impact our model. So let\u2019s dive into this concept of Multicollinearity and how it will impact our model.\n"}}