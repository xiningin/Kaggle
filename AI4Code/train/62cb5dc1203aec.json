{"cell_type":{"5b5a432b":"code","4343409a":"code","26269bfe":"code","19aa67be":"code","62fffed5":"code","d96af1b4":"code","951edb80":"code","9bdf16f0":"code","26c81f13":"code","4f15b4d6":"code","e08b3f7f":"code","b7992877":"code","c0e4da8d":"code","138901e7":"code","1d367d23":"code","01dd7bb7":"code","ff202435":"code","e3b81d73":"code","3b8f40b1":"code","ddc222e2":"code","289398fb":"code","6495bbc7":"code","3278cfd3":"code","322ee0d8":"code","5ef32c45":"code","02547348":"code","ca16c521":"code","71791197":"code","ffc6c12e":"markdown","14112244":"markdown","faf8bf6d":"markdown","4e758656":"markdown","cfa50d47":"markdown","da6574e1":"markdown","c6822341":"markdown","2629195b":"markdown","31fd931c":"markdown","ef036924":"markdown","da64f897":"markdown","c90fafe7":"markdown","f79b65f4":"markdown","1849dd09":"markdown","b2b28730":"markdown","4382fbc7":"markdown","798a530b":"markdown","e5f1667c":"markdown","b2da3288":"markdown","65efa87d":"markdown","fbbc24ec":"markdown","42fbcd6f":"markdown","ace9f4c0":"markdown","c700ae20":"markdown","db547ad7":"markdown","7d13c74e":"markdown","db9d932a":"markdown","4c74d727":"markdown","6dcd8fda":"markdown","e99d56d8":"markdown","3e2b27d8":"markdown","3630ebec":"markdown","229010e1":"markdown","dc44878a":"markdown","57a217a6":"markdown","08138e0a":"markdown","50d569dc":"markdown","426125fb":"markdown","66118f76":"markdown","884fbfe0":"markdown"},"source":{"5b5a432b":"#standard import\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport pandas as pd\n%matplotlib inline","4343409a":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df    = pd.read_csv(\"..\/input\/test.csv\")\n\n# preview the data\ntrain_df.head()","26269bfe":"#train_df.info()\n#print(\"----------------------------\")\n#test_df.info()","19aa67be":"#drop unnecessary data\ntrain_df=train_df.drop(['Id'],axis=1)\ntrain_df.head()","62fffed5":"train_df['SalePrice'].describe()","d96af1b4":"sns.distplot(train_df['SalePrice'])","951edb80":"# The intuition here is that the larger the area is, the higher the price should be.\n# Therefore, we are examing this proportionality in this regards.\ndata = pd.concat([train_df['SalePrice'], train_df['LotArea']], axis=1)\ndata.plot.scatter(x='LotArea', y='SalePrice', ylim=(0, 800000))\n\n#Well, not really see a proportionality in the chart, probably drop it.","9bdf16f0":"# Similar intuition as LotArea\ndata = pd.concat([train_df['SalePrice'], train_df['GrLivArea']], axis=1)\ndata.plot.scatter(x='GrLivArea', y='SalePrice', ylim=(0, 800000))\n#Yeah, in this case, the linear proportionality is quite obvious. Keep it.\n#However, we can further clean the data by removing outliers.\ntrain_df = train_df.drop(train_df[(train_df['GrLivArea']>4000) & (train_df['SalePrice']<300000)].index)","26c81f13":"data = pd.concat([train_df['SalePrice'], train_df['OverallQual']], axis=1)\n#data.plot.scatter(x='OverallQual', y='SalePrice', ylim=(0, 800000))\n#better version of visualization\nfig = sns.boxplot(x='OverallQual', y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\n#It is a quite good index. Definitely keep it.","4f15b4d6":"#dropped. All the same\npass","e08b3f7f":"data = pd.concat([train_df['SalePrice'], train_df['Neighborhood']], axis=1)\nf, ax = plt.subplots(figsize=(26, 12))\nfig = sns.boxplot(x='Neighborhood', y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000)\n# Different Neighborhoods have different range of price. Keep it.","b7992877":"#Neighborhood is proven to be less effective, ignore.\n\n# Using Dummies to extract this feature\n#neighborhood_dummies_train  = pd.get_dummies(train_df['Neighborhood'])\n#neighborhood_dummies_test  = pd.get_dummies(test_df['Neighborhood'])\n#train_df = train_df.join(neighborhood_dummies_train)\n#test_df    = test_df.join(neighborhood_dummies_test)\n\n#train_df.drop(['Neighborhood'], axis=1,inplace=True)\n#test_df.drop(['Neighborhood'], axis=1,inplace=True)\n\n#train_df.head()","c0e4da8d":"data = pd.concat([train_df['SalePrice'], train_df['CentralAir']], axis=1)\nf, ax = plt.subplots()\nfig = sns.boxplot(x='CentralAir', y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\n#With CentralAir, the sale price is higher. Keep it for now","138901e7":"#Another way to handle non-numerical data.","1d367d23":"#convert Y,N into 1,0\ntrain_df['CentralAir'].replace(to_replace=['N', 'Y'], value=[0, 1])\ntrain_df.head()","01dd7bb7":"# This can be seen as a factor to represent the number of cars owning.\n# Typically, more cars, the house tends to be more expensive\n# We select GarageCars due to personal preference only\ndata = pd.concat([train_df['SalePrice'], train_df['GarageCars']], axis=1)\ndata.plot.scatter(x='GarageCars', y='SalePrice', ylim=(0, 800000))\ndata = pd.concat([train_df['SalePrice'], train_df['GarageArea']], axis=1)\ndata.plot.scatter(x='GarageArea', y='SalePrice', ylim=(0, 800000))\n# Fairly representative, keeping it","ff202435":"# This may be a tricky one to see the correlation, since time series is involvd.\ndata = pd.concat([train_df['SalePrice'], train_df['YearBuilt']], axis=1)\ndata.plot.scatter(x='YearBuilt', y='SalePrice', ylim=(0, 800000))\n#for better visualization\nf, ax = plt.subplots(figsize=(26, 12))\nfig = sns.boxplot(x='YearBuilt', y=\"SalePrice\", data=data)\nfig.axis(ymin=0, ymax=800000);\n#index is okay, but too many outliers, think about it later.","e3b81d73":"#This sk method can process non-value datat= like Neighborhood\nfrom sklearn import preprocessing\nf_names = ['CentralAir', 'Neighborhood']\nfor x in f_names:\n    label = preprocessing.LabelEncoder()\n    train_df[x] = label.fit_transform(train_df[x])\n","3b8f40b1":"corrmat =train_df.corr()\nf, ax = plt.subplots(figsize=(20, 9))\nsns.heatmap(corrmat, vmax=0.8, square=True)","ddc222e2":"k  = 10 \ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(train_df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, \\\n                 square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","289398fb":"from sklearn import preprocessing\nfrom sklearn import linear_model, svm, gaussian_process\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.model_selection import train_test_split\nimport numpy as np\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestRegressor,  GradientBoostingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","6495bbc7":"cols = ['OverallQual','GrLivArea', 'GarageArea','1stFlrSF', 'FullBath', 'YearBuilt']\nx = train_df[cols].values\ny = train_df['SalePrice'].values\n#Normalization\nx_scaled = preprocessing.StandardScaler().fit_transform(x)\ny_scaled = preprocessing.StandardScaler().fit_transform(y.reshape(-1,1))\n#Train and validation\nX_train,X_vali, y_train, y_vali = train_test_split(x_scaled, y_scaled, test_size=0.3, random_state=42)","3278cfd3":"cols = ['OverallQual','GrLivArea', 'GarageArea','1stFlrSF', 'FullBath', 'YearBuilt']\nX_train = train_df[cols].values\ny_train = train_df['SalePrice'].values\n\nclf_1 = RandomForestRegressor(n_estimators=400)\nclf_1.fit(X_train, y_train)\ny_pred = clf_1.predict(X_vali)\nprint(np.sum(abs(y_pred - y_vali))\/len(y_pred))\n\nclf_2 = KNeighborsRegressor(n_neighbors=7)\nclf_2.fit(X_train, y_train)\ny_pred = clf_2.predict(X_vali)\nprint(np.sum(abs(y_pred - y_vali))\/len(y_pred))\n\nclf_3 = lgb.LGBMRegressor(objective='regression',num_leaves=5,\n                              learning_rate=0.05, n_estimators=720,\n                              max_bin = 55, bagging_fraction = 0.8,\n                              bagging_freq = 5, feature_fraction = 0.2319,\n                              feature_fraction_seed=9, bagging_seed=9,\n                              min_data_in_leaf =6, min_sum_hessian_in_leaf = 11)\n\nclf_3.fit(X_train, y_train)\ny_pred = clf_3.predict(X_vali)\nprint(np.sum(abs(y_pred - y_vali))\/len(y_pred))","322ee0d8":"cols = ['OverallQual','GrLivArea', 'GarageArea','1stFlrSF', 'FullBath', 'YearBuilt']\ntest_df[cols].isnull().sum()","5ef32c45":"#Handling GarageArea Missing data\ntest_df['GarageArea'].describe()","02547348":"test_df['GarageArea']=test_df['GarageArea'].fillna(472.768861)\ntest_df['GarageArea'].isnull().sum()","ca16c521":"cols = ['OverallQual','GrLivArea', 'GarageArea','1stFlrSF', 'FullBath', 'YearBuilt']\ntest_x = pd.concat( [test_df[cols]] ,axis=1)\n\nx = test_x.values\n\ny_pred_1 = clf_1.predict(x)\ny_pred_2 =clf_2.predict(x)\ny_pred_3 =clf_3.predict(x)\n\ny_pred=y_pred_1*0.5+y_pred_2*0.5","71791197":"prediction = pd.DataFrame(y_pred, columns=['SalePrice'])\nprint(prediction)\nresult = pd.concat([test_df['Id'], prediction], axis=1)\nresult.to_csv('.\/Predictions.csv', index=False)","ffc6c12e":"Standard Import  \nWe always start a notebook with import. These are libraries that people usually use for data science.","14112244":"**1.Target Analysis**","faf8bf6d":"Neighborhood is proven to be less relevent. However, for learning purposes, I will demonstrate how to deal with non-numerical data.","4e758656":"Until this point, we have almost the right dataset to sail. Then, in the following section, different methods are taken to further analyze the data.","cfa50d47":"2.2.GrLivArea ","da6574e1":"From this matrix, 1stFlrSF and TotalBsmtSF are similar. We take 1stFlrSF.\nGarageCars and GarageArea are similar. We takeGarageCars.\nTotRmsAbvGrd and GrLivArea are similar. We take GrLivArea.","c6822341":"To drwa heatmap for correlation. The fainter the colour, the higher the correlation. Now, by focusing the bottom row named SalePrice, we can find the correlation between our targets and features.","2629195b":"**4. ML to generate Models**","31fd931c":"After training our model, we have to think about our input. Is the dataset complete?","ef036924":"No, there is one missing data in GarageArea we have to handle by filling with mean value.","da64f897":"ML import with different models.","c90fafe7":"**3. Overall Analysis**     \n","f79b65f4":"From the heat map, we extract missing potential useful index:     \n1.FullBath.    \n2.TotRmsAbvGrd.   \n3.TotalBsmtSF.   \n4.1stFlrSF.   \nAlso, Neighborhood and CentralAir seem to be less relevant. So drop them.","1849dd09":"2.8.YearBuilt  ","b2b28730":"2.7.GarageCars&GarageArea","4382fbc7":"In almost any situation, id is an irrelevant feature to prediction. Therefore, we drop it.","798a530b":"**What you will learn from here: **   \n1. General Kaggle Data Science workflow\n2. Numerical and Non-numerical data analysis and handling\n3. Feature Extraction and engineering\n4. Heat map, correlation matrix for systematic analysis.\n5. Machine learning techniques and how to validate models.\n6. Missing data handling.\n","e5f1667c":"Therefore, the model now only consists the following index, which we believed to be most correlated from the map: \n1. OveralQual\n2. GrLivArea\n3. GarageArea\n4. 1stFlrSF\n5. FullBath\n6. YearBuilt\n\n","b2da3288":"For the first step, it is always a good idea to start with analyzing target, in this case, the SalePrice.","65efa87d":"After process 2, we gain an general idea about how indeces can affect our prediction. However, the above features are selected neither scientific nor systematic. Then, in this section, I will introduce heatmap, a systematic approach in finding right features.","fbbc24ec":"Then, we are ready to go for submitting this vanila model!\nI will in the following days to summerize techniques of optimising this model, and I will upload a link here. If you like my idea, please stay with me. Thanks!!!","42fbcd6f":"2.5.Neighborhood ","ace9f4c0":"2.1 LotArea","c700ae20":"**5.Missing Data Handling**","db547ad7":"In this sections, the 8 features extracted are:       \n1.LotArea   \n2.GrLivArea    \n3.OverallQual   \n4.Utilities   \n5.Neighborhood     \n6.CentralAir    \n7.GarageCars&GarageArea    \n8.YearBuilt    ","7d13c74e":"**6. Submission**","db9d932a":"2.6.CentralAir ","4c74d727":"In this section, I mannually pick 8 features which I think is critical to this analysis. Then, I will examine all of them to see if they are actually usefull and relevant for prediction. It is a very good exercise to build data scientist intuition.","6dcd8fda":"Process the data with our 6 top chosen features.  \nThen using normalization to do feature scaling.\nAlso, we split data into training and validation set.","e99d56d8":"2.4.Utilities  ","3e2b27d8":"We inspect 3 example ML models: RandomForest, KNN and LGBoost (for the idea of dropping poorly performed one).  \nA very naive method of validation is used to test models.\n","3630ebec":"Read the dataset, and use head( ) to preview, this is a good way to understand data generally.","229010e1":"The target here will not assist a lot to increase accuracy in this model. However, for advanced settings, skewness can be used to adjust outliers. Please refer to https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard.","dc44878a":"For regression problems, we usually take staking approch (a very vanila one here, just to introduce the idea). ","57a217a6":"**If you are as new as me, why don't you start your House Pricing Model with me.  **  \n**Created by Raymond Wang **\n\n**If you are as new as me Series **     \nTitanic https:\/\/www.kaggle.com\/yw6916\/if-you-are-as-new-as-me-why-don-t-you-start-here1?scriptVersionId=15917018    \nHouse Pricing Advanced https:\/\/www.kaggle.com\/yw6916\/house-pricing-advance-if-you-are-as-new-as-me-3  (Part 2 of this model, please go and check it out if you want something to improve yours' performance)\n\nIn this Kernal, I will lead you through the journey of making a vanilla model with a straight and simple approach. I hope you will enjoy this very intuitive method.","08138e0a":"Then, use correlation matrix to examin the top 10 features to eliminate similar ones.","50d569dc":"**2. Features Extraction**","426125fb":"In here, I introduce a new and general method to handle non-numerical data with sklearn.","66118f76":"2.3.OverallQual ","884fbfe0":"We can see, the off-set of LGBoost is highest. Therefore, we discard LGBoost."}}