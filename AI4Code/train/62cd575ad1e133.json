{"cell_type":{"2f322d62":"code","e3e22a48":"code","ae31c492":"code","fea59c47":"code","67b5aee8":"code","29205580":"code","b68e55cc":"code","f95c83ff":"code","f4a3ea38":"code","c5b8a3f3":"code","80accbe7":"code","f0154362":"code","61c5b449":"code","32841a84":"code","a94178ec":"code","0e842dd0":"code","733e8bf8":"code","09cf769a":"code","09ef0652":"code","ae1064d7":"code","f8c054dd":"code","56bc7c58":"code","b1208503":"code","3c420b12":"code","0f3b3892":"code","571aafd4":"code","ddf4346a":"code","31964945":"code","39d392af":"code","aec2f163":"code","3535a40f":"code","f6428c80":"code","e423abc2":"code","4dcb29d6":"code","0f01dbec":"code","1a3d58c3":"code","05fbf678":"code","1155ff7b":"code","18797036":"code","2210df2c":"code","93918596":"code","8d67fdd0":"markdown","d6295197":"markdown","2310d8b4":"markdown","d64be351":"markdown","2f167c60":"markdown","6ad0e913":"markdown","c4b9fa82":"markdown","480789f0":"markdown","0c7d85db":"markdown","c0c63147":"markdown","60419a42":"markdown","8ec369a7":"markdown","19dba049":"markdown","84d316c7":"markdown","533baff9":"markdown","a8f7e163":"markdown","c39ef311":"markdown","6d4b43cf":"markdown","3df879fc":"markdown","f2e0835e":"markdown","bb8b3c18":"markdown","4c815b52":"markdown","dbca0f69":"markdown","2b386740":"markdown","c2d135c3":"markdown","9ec0e730":"markdown","1430adad":"markdown","424eae05":"markdown","edc95da2":"markdown","21f67d38":"markdown","21b819a2":"markdown","ea333960":"markdown","98b3f805":"markdown","98124d8e":"markdown","6388ebb3":"markdown","512cc64d":"markdown","c5ca01eb":"markdown","1ccd5cbf":"markdown","999112c5":"markdown","2d48d793":"markdown","fdf1e774":"markdown","134ae73d":"markdown","dc7c42d8":"markdown","08800ca4":"markdown","917d3f3d":"markdown","d0ec0dbb":"markdown"},"source":{"2f322d62":"!wget http:\/\/archive.ics.uci.edu\/ml\/machine-learning-databases\/00332\/OnlineNewsPopularity.zip -O OnlineNewsPopularity.zip\n!yes | unzip OnlineNewsPopularity.zip","e3e22a48":"!cat OnlineNewsPopularity\/OnlineNewsPopularity.names","ae31c492":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n##  SKlearn libs for regressions\nfrom sklearn import metrics, linear_model\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV, GridSearchCV\nfrom sklearn.feature_selection import SelectKBest, chi2, f_classif\nfrom sklearn.preprocessing import PolynomialFeatures\nfrom sklearn.ensemble import RandomForestRegressor\n\n##Importing Libraries for Neural Nets\nfrom keras.models import Sequential, model_from_json\nfrom keras.layers import Dense, Dropout, Activation\n\nimport xgboost\n\n","fea59c47":"df = pd.read_csv('.\/OnlineNewsPopularity\/OnlineNewsPopularity.csv')\ndf.shape","67b5aee8":"df.head()","29205580":"# gives some infos on columns types and number of null values\ntab_info=pd.DataFrame(df.dtypes).T.rename(index={0:'column type'})\ntab_info=tab_info.append(pd.DataFrame(df.isnull().sum()).T.rename(index={0:'null values (nb)'}))\ntab_info=tab_info.append(pd.DataFrame(df.isnull().sum()\/df.shape[0]*100)\n                         .T.rename(index={0:'null values (%)'}))\ntab_info","b68e55cc":"cols_to_remove = ['url']\ndf = df.drop(['url'], axis=1)\ndf.head()","f95c83ff":"plt.subplots(3,1,figsize=(20,16))\nplt.subplot(3,1,1)\nsns.distplot(df[' shares'], hist=True, kde=False)\nplt.subplot(3,1,2)\nsns.violinplot(df[' shares'])\nplt.subplot(3,1,3)\nsns.scatterplot(data=df, x=' timedelta', y=' shares')","f4a3ea38":"Q1 = df[' shares'].quantile(0.25)\nQ3 = df[' shares'].quantile(0.75)\nIQR = Q3 - Q1\nLTV= Q1 - (1.5 * IQR)\nUTV= Q3 + (1.5 * IQR)\ndf = df.drop(df[df[' shares'] > UTV].index)\ndf.shape","c5b8a3f3":"plt.subplots(3,1,figsize=(20,16))\nplt.subplot(3,1,1)\nsns.distplot(df[' shares'], hist=True, kde=False)\nplt.subplot(3,1,2)\nsns.violinplot(df[' shares'])\nplt.subplot(3,1,3)\nsns.scatterplot(data=df, x=' timedelta', y=' shares')","80accbe7":"df_corr = abs(df.corr())\ndf_corr = df_corr[' shares']\ndf_corr = pd.DataFrame(df_corr.values, df.columns).reset_index()\n# print(df_corr[0:20],df_corr[21:40], df_corr[41:]  )\ndf_corr.columns = ['Feature', 'Corr']\ndf_corr = df_corr[df_corr['Corr'] > 0.06]\ndf_corr = df_corr.sort_values(by='Corr', ascending=False)[1:]\ndf_corr = df_corr.head(20)\ndf_corr","f0154362":"y = df[' shares']\nX = df.drop([' shares'], axis=1)\n\nbestfeatures = SelectKBest(score_func=f_classif, k=10)\nfit = bestfeatures.fit(X,y)\ndfscores = pd.DataFrame(fit.scores_)\ndfcolumns = pd.DataFrame(X.columns)\nfeatureScores = pd.concat([dfcolumns,dfscores],axis=1)\nfeatureScores.columns = ['Specs','Score']  #naming the dataframe columns\nfeatureScores = featureScores.sort_values(by='Score', ascending=False).head(20)\nfeatureScores","61c5b449":"idx1 = pd.Index(df_corr['Feature'])\nidx2 = pd.Index(featureScores['Specs'])\nfeatures_list = idx1.intersection(idx2)\nfeatures_list","32841a84":"X1 = X[features_list]\nplt.subplots(1,1,figsize=(18,8))\nsns.heatmap(X1.corr(),annot=True,cmap=\"RdYlGn\")","a94178ec":"features_to_remove = [' kw_max_avg',' kw_min_avg',' weekday_is_saturday',' LDA_02',' LDA_01',' LDA_04']\nlst = list(features_list.values)\nselected_features = [e for e in lst if e not in features_to_remove]\n\n## Let us take top 6\n# selected_features = selected_features[:6]\n\nselected_features ","0e842dd0":"plt.subplots(1,1,figsize=(10,8))\n\nplt.subplot(1,1,1)\n\nplt.title('Scatter shares vs kw_avg_avg')\nsns.scatterplot(data=df, x=' kw_avg_avg', y=' shares', ci=None)\n\n","733e8bf8":"plt.subplots(2,1,figsize=(10, 8))\n\nplt.subplot(2,1,1)\nplt.title('No. of articles shares during weekend')\nsns.barplot(data=df, x=' is_weekend', y=' shares', ci=False)\n\nplt.subplot(2,1,2)\nplt.title('No. of articles during weekend')\nsns.distplot(df[' is_weekend'], hist=True, kde=False)","09cf769a":"index = 100\ndf['weekday'] = df[' weekday_is_monday'] * 2 ** 1 + \\\n      df[' weekday_is_tuesday'] * 2 ** 2 + \\\n      df[' weekday_is_wednesday'] * 2 ** 3 + \\\n      df[' weekday_is_thursday'] * 2 ** 4 + \\\n      df[' weekday_is_friday'] * 2 ** 5 + \\\n      df[' weekday_is_saturday'] * 2 ** 6 + \\\n      df[' weekday_is_sunday'] * 2 ** 7 \n    \n# np.log2(val)\ndf.head()","09ef0652":"plt.subplots(1,1,figsize=(10, 6))\n\nplt.subplot(1,1,1)\nplt.title('No. of articles shares by no. of keywords')\nsns.barplot(data=df, x=' num_keywords', y=' shares', ci=False)","ae1064d7":"fig, ax = plt.subplots(1,1,figsize=(10,8))\nax = fig.add_subplot(1, 1, 1)\n# ax.set_yscale('log')\nplt.title('Feature: rate_negative_words')\nsns.scatterplot(data=df, x=' rate_negative_words', y=' shares', ci=None)    ","f8c054dd":"def token_group(x):\n    if x < 100:\n        return 1\n    elif x >=100 and x<500:\n        return 2\n    elif x >=500 and x<1000:\n        return 3\n    elif x >=1000 and x<2000:\n        return 4\n    else: \n        return 5\n    \ndf['token_group'] =  df[' n_tokens_content'].apply(token_group)","56bc7c58":"plt.subplots(2,1,figsize=(10,15))\n\nplt.subplot(2,1,1)\nplt.title('No. of articles in token group')\nsns.distplot(df.token_group, hist=True, kde=False)\nplt.subplot(2,1,2)\nplt.title('Avg shares in each group')\nsns.barplot(data=df, x='token_group', y=' shares', ci=None)","b1208503":"X = X[selected_features]","3c420b12":"# print(X[selected_features].shape)\n# print(y.shape)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3)","0f3b3892":"model = linear_model.LinearRegression()\nmodel.fit(X_train, y_train)","571aafd4":"results = model.predict(X_test)\nscore1 = metrics.mean_squared_error(y_test,results )\nscore2 = metrics.r2_score(y_test,results )\nprint('MSE: ',score1, '  R2 Score: ', score2)","ddf4346a":"poly = PolynomialFeatures(degree = 2)\nregr = linear_model.LinearRegression()\nX_ = poly.fit_transform(X_train)\nregr.fit(X_, y_train)\nX_ = poly.fit_transform(X_test)\nresults = regr.predict(X_)\nscore1 = metrics.mean_squared_error(y_test,results )\nscore2 = metrics.r2_score(y_test,results )\nprint('MSE: ',score1, '  R2 Score: ', score2)","31964945":"\n# # model2 = RandomForestRegressor(n_estimators=5, max_depth=1000 )\n# # model2.fit(X_train, y_train)\n\n# # Number of trees in random forest\n# n_estimators = [int(x) for x in np.linspace(start = 100, stop = 2000, num = 5)]\n# # Number of features to consider at every split\n# max_features = ['auto', 'sqrt']\n# # Maximum number of levels in tree\n# max_depth = [int(x) for x in np.linspace(10, 110, num = 5)]\n# max_depth.append(None)\n# # Minimum number of samples required to split a node\n# min_samples_split = [2, 5 ]\n# # Minimum number of samples required at each leaf node\n# min_samples_leaf = [1, 2, 4]\n# # Method of selecting samples for training each tree\n# bootstrap = [True]\n\n# param_grid = {\n#     'bootstrap': bootstrap,\n#     'max_depth': max_depth,\n#     'max_features': max_features,\n#     'min_samples_leaf': min_samples_leaf,\n#     'min_samples_split': min_samples_split,\n#     'n_estimators': n_estimators\n# }\n# # Create a based model\n# rf = RandomForestRegressor()\n# # Instantiate the grid search model\n# grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n#                           cv = 3, n_jobs = -1, verbose = 2)\n\n","39d392af":"# # Fit the grid search to the data\n# grid_search.fit(X_train, y_train)","aec2f163":"# grid_search.best_params_","3535a40f":"# model2 = grid_search.best_estimator_\n# results2 = model2.predict(X_test)\n# score1 = metrics.mean_squared_error(y_test,results2 )\n# score2 = metrics.r2_score(y_test,results2 )\n# print('MSE: ',score1, '  R2 Score: ', score2)","f6428c80":"model2 = RandomForestRegressor(bootstrap= True, max_depth = 10, \n max_features = 'sqrt',\n min_samples_leaf =  10,\n min_samples_split = 8,\n n_estimators = 1550)\n\nmodel2.fit(X_train, y_train)\nresults2 = model2.predict(X_test)\nscore1 = metrics.mean_squared_error(y_test,results2 )\nscore2 = metrics.r2_score(y_test,results2 )\nprint('MSE: ',score1, '  R2 Score: ', score2)","e423abc2":"import lightgbm as lgb\nd_train = lgb.Dataset(X_train, label=y_train)\nparams = {}\nparams['learning_rate'] = 0.015\nparams['boosting_type'] = 'gbdt'\n# params['boosting_type'] = 'dart'\nparams['objective'] = 'regression'\nparams['metric'] = 'mse'\nparams['sub_feature'] = 0.99\nparams['num_leaves'] = 10\nparams['min_data'] = 100\nparams['max_depth'] = 10000\ny_train=y_train.ravel()\nreg= lgb.train(params, d_train, 100)\nresults=reg.predict(X_test)\nscore1 = metrics.mean_squared_error(y_test,results )\nscore2 = metrics.r2_score(y_test,results )\nprint('MSE: ',score1, '  R2 Score: ', score2)","4dcb29d6":"# uniform(loc=0.2, scale=0.8)\nestimator = lgb.LGBMRegressor(boosting_type= 'gbdt', metric='mse',objective='regression')","0f01dbec":"param_grid = {\n    'learning_rate': [0.005, 0.01, 0.1, 0.5],\n    'n_estimators': [int(x) for x in np.linspace(start = 20, stop = 2000, num = 7)],\n    'num_leaves' : [int(x) for x in np.linspace(start = 10, stop = 100, num = 5)],\n    'sub_feature' : [float(x) for x in np.linspace(start = 0.1, stop = 1, num = 3)]\n}\ngbm = GridSearchCV(estimator, param_grid, cv=3)","1a3d58c3":"gbm.fit(X_train, y_train)\nprint('Best parameters found by grid search are:', gbm.best_params_)\n\ngbm_best = gbm.best_estimator_\nresults = gbm_best.predict(X_test)\nscore1 = metrics.mean_squared_error(y_test,results )\nscore2 = metrics.r2_score(y_test,results )\nprint('MSE: ',score1, '  R2 Score: ', score2)\n","05fbf678":"model = Sequential()\nmodel.add(Dense(32, input_dim=X_train.shape[1], init='uniform', activation='relu'))\nmodel.add(Dense(64, init='uniform', activation='relu'))\nmodel.add(Dense(64, init='uniform', activation='relu'))\nmodel.add(Dense(1, activation='linear'))\nmodel.summary()","1155ff7b":"model.compile(loss='mse', optimizer='Adamax', metrics=['mse'])\nhistory = model.fit(X_train, y_train, epochs=5000, batch_size=50,  verbose=0, validation_split=0.2)","18797036":"results=model.predict(X_test)\nscore1 = metrics.mean_squared_error(y_test,results )\nscore2 = metrics.r2_score(y_test,results )\nprint('MSE: ',score1, '  R2 Score: ', score2)","2210df2c":"print(history.history.keys())\n# \"Loss\"\nplt.subplots(1,1,figsize=(12,10))\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper right')\nplt.show()","93918596":"xgb = xgboost.XGBRegressor(n_estimators=120, learning_rate=0.08, gamma=0, subsample=0.75,\n                           colsample_bytree=1, max_depth=2)\nxgb.fit(X_train,y_train)\nresults=xgb.predict(X_test)\nscore1 = metrics.mean_squared_error(y_test,results )\nscore2 = metrics.r2_score(y_test,results )\nprint('MSE: ',score1, '  R2 Score: ', score2)","8d67fdd0":"Load the data and print shape of the data.","d6295197":"### Features shortlisting ","2310d8b4":"### XGBoost\nLet us now try to implement XGBoost Algorithm\n","d64be351":"Let us try to analyse null values in the input","2f167c60":"We can observe that, there are no null values in any of the columns, which is great news.. Let us now try to remove unwanted and non-value adding columns","6ad0e913":"The details of dataset is provided below","c4b9fa82":"### Correlation Analysis","480789f0":"The above Gridsearch Algorthm has been executed in version 6 with following output. It took 281 min to get the best hyperparameters for RandomSearchRegression. \n```\nFitting 3 folds for each of 360 candidates, totalling 1080 fits\n[Parallel(n_jobs=-1)]: Using backend LokyBackend with 4 concurrent workers.\n[Parallel(n_jobs=-1)]: Done  33 tasks      | elapsed:  7.2min\n\/opt\/conda\/lib\/python3.6\/site-packages\/joblib\/externals\/loky\/process_executor.py:706: UserWarning: A worker stopped while some jobs were given to the executor. This can be caused by a too short worker timeout or by a memory leak.\n  \"timeout or by a memory leak.\", UserWarning\n[Parallel(n_jobs=-1)]: Done 154 tasks      | elapsed: 26.6min\n[Parallel(n_jobs=-1)]: Done 357 tasks      | elapsed: 79.6min\n[Parallel(n_jobs=-1)]: Done 640 tasks      | elapsed: 168.1min\n[Parallel(n_jobs=-1)]: Done 1005 tasks      | elapsed: 268.8min\n[Parallel(n_jobs=-1)]: Done 1080 out of 1080 | elapsed: 281.4min finished\nOut[31]:\nGridSearchCV(cv=3, error_score='raise-deprecating',\n             estimator=RandomForestRegressor(bootstrap=True, criterion='mse',\n                                             max_depth=None,\n                                             max_features='auto',\n                                             max_leaf_nodes=None,\n                                             min_impurity_decrease=0.0,\n                                             min_impurity_split=None,\n                                             min_samples_leaf=1,\n                                             min_samples_split=2,\n                                             min_weight_fraction_leaf=0.0,\n                                             n_estimators='warn', n_jobs=None,\n                                             oob_score=False, random_state=None,\n                                             verbose=0, warm_start=False),\n             iid='warn', n_jobs=-1,\n             param_grid={'bootstrap': [True],\n                         'max_depth': [10, 35, 60, 85, 110, None],\n                         'max_features': ['auto', 'sqrt'],\n                         'min_samples_leaf': [1, 2, 4],\n                         'min_samples_split': [2, 5],\n                         'n_estimators': [100, 575, 1050, 1525, 2000]},\n             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n             scoring=None, verbose=2)\n```\n\nBest parameters are given below:\n```\n{'bootstrap': True,\n 'max_depth': 10,\n 'max_features': 'sqrt',\n 'min_samples_leaf': 4,\n 'min_samples_split': 5,\n 'n_estimators': 1525}\n```\n\nWith above setup, the score was :\n```\nMSE:  1104065.159504597   R2 Score:  0.0931234303865699\n```","0c7d85db":"We can see there are 39.6K data records with 61 columns. 1 target variable\n\nLet us see how the data look like","c0c63147":"Green patches other than diagonal are the once to check and consider one of them. Following pairs are correlated features. The highlighted ones are features whose scores are higher in either **Correlation** and\/or **ANOVA**\n\n- **kw_avg_avg** - kw_max_avg\n- **kw_avg_avg** - kw_min_avg\n- **is_weekend** - weekday_is_saturday\n- **data_channel_is_world** - LDA_02\n- **data_channel_is_entertainment** - LDA_01\n- **data_channel_is_tech** - LDA_04 \n\nFollowing features should be remvoed from above list: kw_max_avg, kw_min_avg, weekday_is_saturday, LDA_02, LDA_01, LDA_04\n\n","60419a42":"Up next, let us try to create a GridSearchCV for LightGBM","8ec369a7":"### Feature selection using *Univariate Selection*\n","19dba049":"Now the data looks really good for next steps.. ","84d316c7":"### No. of shares vs weekday of the article\nLet us get a new column with weekday to see how the shares are varying with that column","533baff9":"## Data Cleaning, Outlier treatment\n\n### No. of Shares\nLet us analyse how the sharing data is spread","a8f7e163":"As expected the R2 score is pretty bad. \n","c39ef311":"We can see that articles published on weekend is getting shared more than weekday. The same is explained below","6d4b43cf":"# Summary\nWith all the above we can summarize our findings as below\n\n|Model|Parameters|MSE |R2 Score|\n|-| -|-|-|\n|Linear Model (**Baseline**) |-|1157909|0.071 |\n|Polynomial Model| Degree = 2|1096834|0.092 | \n|Random Forest|max_depth=10,max_features='sqrt',min_samples_leaf=10,min_samples_split=8, n_estimators=1550|1150355|0.077|\n|LightGBM|learning_rate= 0.15,boosting_type= 'gbdt,objective = 'regression',metric= 'mse',sub_feature= 0.3,num_leaves= 10,min_data = 1200,max_depth= 100|1094217|0.094|\n|Neural Net|4 layers= 9,64,32,32|1114359|0.078|\n|XGBoost| | | 0.077|","3df879fc":"We use MSE and R2 as Metrics","f2e0835e":"# Exploratory Data Analysis\nLet us now perform some deep exploratory data analysis","bb8b3c18":"Wow! Light GBM without much tuning we got much better results. \n\n**We can find best hyperparameters for LightGBM as improvement**","4c815b52":"### LightGBM algorithm\n**LightGBM** is a gradient boosting framework that uses tree based learning algorithms. It is designed to be distributed and efficient with many advantages","dbca0f69":"Polynominal algorithm could not finish as 9 features, its too much to calculate the multiple possible poly degrees\nHowever, in one of the previous versions, we have run and following is the output \n\n```\nModel with Polynominal Degree 1 MSE:  1138058.1341509074   R2 Score:  0.08366289295516138\nModel with Polynominal Degree 2 MSE:  1128850.3833187486   R2 Score:  0.09107675302675189\nModel with Polynominal Degree 3 MSE:  1131198.7487819889   R2 Score:  0.08918590549419136\nModel with Polynominal Degree 4 MSE:  1133594.5157797942   R2 Score:  0.08725689138318526\nModel with Polynominal Degree 5 MSE:  1140495.5739477822   R2 Score:  0.08170032490609536\nModel with Polynominal Degree 6 MSE:  2199982.8449245617   R2 Score:  -0.7713734080645245\n```\n\n","2b386740":"Next up we will see how *no. of shares* and *rate_negative_words* featrues are correlated","c2d135c3":"We can see that no. of shares increase with key words in the meta data","9ec0e730":"We can see that the data is very skewed. We can also observe that there are very low no. of articles with very large number of shares. The 3rd scatter plot confirms the same. \n\nSo, next let us try to find and remove the **outliers**","1430adad":"After removing the outliers, let us try to plot the same graphs","424eae05":"Above are common features in both Corr and ANOVA. We can use these features for building the model\n\nNext step, let us analyse if any of there features are highly correlated to each other using a heatmap. We can keep one of highly correlated features.","edc95da2":"We can see best parameters have  greatly improved the model. ","21f67d38":"### Shares vs kw_avg_avg\nThis is feature which is most impacting, as per statistics. Let us see how it looks","21b819a2":"Let us now split the data into X and Y, where Xs are all the features and Y is the *no. of shares*","ea333960":"Above list shows the list of features which has more than 6% correlation (P Value) with respect to no. of shares","98b3f805":"The above scatterplot is very sparse, but we can observe that there is a correlation","98124d8e":"### Random Forest Regressor\nWe will use Gridsearch with cross validation to get best parameter for RandomForest Algo","6388ebb3":"### Neural Networks\nLet us build a deep NN model for this regression","512cc64d":"# Data Load and Understanding","c5ca01eb":"# Build Model\n\n### Baseline Model\nWe have ```selected_features``` as part of feature selection section. Let us try to build naive (Baseline) model based on simple linear model\n\nFirst of all, let us split the train and test sets for the model","1ccd5cbf":"We have most of the articles in group 2, which is with token length in the range of 100 to 500. As the articles size is increasing, we see avg no. shares has also increased","999112c5":"The model summary is shown above. Let us compile now","2d48d793":"We see that the R2 score is around 0.77. ","fdf1e774":"With above parameters, let us try to build RandomForest model","134ae73d":"# Online News Popularity Data Set \nThis dataset is provided by [UCI ML Dataset](http:\/\/archive.ics.uci.edu\/ml\/datasets\/Online+News+Popularity).  It has got details about 39K articles published in Mashables and number of shares each article has got. The idea is to build a model to predict the number of shares. Before doing so, we will also try to perform certan feature selection and also perform EDA\n\nFirst of all, let's get the data from UCI dataset. \n","dc7c42d8":"# Feature selection\nWe have seen that there are around 60 features. In this section, we analyse statistically and find which are all the important features to consider","08800ca4":"By using two features selection techniques we found 20 top features and then we selected common features among them and then removed correlated features. We get the list of features as shown above. They are in the order of importance ","917d3f3d":"Now we have found top 20 features based on *correlation* and *ANOVA*. Let us compare and see which are common in both","d0ec0dbb":"The above graph shows the improvement of loss with respect to epochs"}}