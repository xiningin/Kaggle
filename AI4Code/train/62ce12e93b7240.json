{"cell_type":{"42700e64":"code","ed160934":"code","dd6beb10":"code","4c5d3ef5":"code","4a2f50f5":"code","699bc694":"code","ae1c73e4":"code","ad64de80":"code","69dc7760":"code","f7df5bc2":"code","730bc038":"code","ed21b146":"code","0b34f1b8":"code","dffb76d3":"code","601171c5":"code","9f564e99":"code","21d1c051":"code","20154ead":"code","0d8e838e":"code","940f1757":"code","4f7c979f":"code","ebcf1e07":"code","cbaff1d1":"code","86e7fc0d":"code","b2022c3a":"code","38a70d2b":"code","5b17d936":"code","b5ba5981":"code","b12f7127":"code","af72583c":"code","f556eebb":"code","0917273f":"code","865016c9":"code","fcf31d4a":"code","05fe46eb":"code","c4c5ec73":"code","d1c54d5a":"code","1d4cb384":"code","19e2de42":"code","4e38a717":"code","0a630f49":"code","5c8dc965":"code","eb80e965":"code","583455bd":"markdown","ac00188f":"markdown","4f34fdda":"markdown"},"source":{"42700e64":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split","ed160934":"train_data_identity = pd.read_csv('..\/input\/ieee-fraud-detection\/train_identity.csv')\ntrain_data_transaction = pd.read_csv('..\/input\/ieee-fraud-detection\/train_transaction.csv')\ntest_data_identity = pd.read_csv('..\/input\/ieee-fraud-detection\/test_identity.csv')\ntest_data_transaction = pd.read_csv('..\/input\/ieee-fraud-detection\/test_transaction.csv')","dd6beb10":"del train_data_identity\ndel train_data_transaction\ndel test_data_identity\ndel test_data_transaction\nimport gc\ngc.collect()","4c5d3ef5":"train_data_identity.head()","4a2f50f5":"pd.set_option('display.max_columns', None)\ntrain_data_transaction.head()","699bc694":"# combining the datasets\ncombined_train_dataset = train_data_transaction.merge(train_data_identity, how='inner',on='TransactionID') # I know this drops like 400000 rows...","ae1c73e4":"len(combined_train_dataset['TransactionID'])","ad64de80":"combined_train_dataset.head()","69dc7760":"columnsToDelete = []\nfor col in combined_train_dataset.columns:\n    if(combined_train_dataset[col].isnull().sum()\/len(combined_train_dataset[col]) >= 0.8):\n        print(col, \"% NaN:\", combined_train_dataset[col].isnull().sum()\/len(combined_train_dataset[col]))\n        columnsToDelete.append(col)","f7df5bc2":"combinedToDelete = list(set(columnsToDelete + cols_not_in_test))","730bc038":"combined_train_dataset = combined_train_dataset.drop(columns=combinedToDelete)","ed21b146":"len(combined_train_dataset.columns)","0b34f1b8":"import missingno as msno\nmsno.matrix(combined_train_dataset.iloc[:,:20],labels=True,fontsize=10)","dffb76d3":"combined_train_dataset.iloc[:,:2]","601171c5":"# Already removed all columns with 80% or more NaN values, now I guess I'll just chuck vals into an imputer and see where it goes...\nnumericalCols = []\ncategoricalCols = []\n\nfor col in combined_train_dataset.columns:\n    if(combined_train_dataset[col].dtype == 'object'):\n        categoricalCols.append(col)\n    else:\n        numericalCols.append(col)\nnumericalCols.remove('isFraud')","9f564e99":"X = combined_train_dataset.drop(columns=['isFraud'])\ny = combined_train_dataset['isFraud']","21d1c051":"from sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.preprocessing import OneHotEncoder\n\n# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')\n\n# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])\n\n# Bundle preprocessing for numerical and categorical data\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numericalCols),\n        ('cat', categorical_transformer, categoricalCols)\n    ])","20154ead":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.25)","0d8e838e":"import matplotlib.pyplot as plt\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.gaussian_process import GaussianProcessClassifier\nfrom sklearn.gaussian_process.kernels import RBF\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.ensemble import GradientBoostingClassifier","940f1757":"'''\nKNeighborsClassifier(3),\n    DecisionTreeClassifier(max_depth=5),\n    RandomForestClassifier(max_depth=5, n_estimators=10, max_features=1),\n    MLPClassifier(alpha=1, max_iter=1000),\n    AdaBoostClassifier(),\n    GaussianNB()'''\nclassifiers = [\n    GradientBoostingClassifier(random_state=0)\n    ]","4f7c979f":"for cls in classifiers:\n    # Bundle preprocessing and modeling code in a pipeline\n    my_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                                  ('model', cls)\n                                 ])\n\n    # Preprocessing of training data, fit model \n    print(\"Training\" , cls)\n    my_pipeline.fit(X_train, y_train)\n\n    # Preprocessing of validation data, get predictions\n    preds = my_pipeline.predict(X_test)\n\n    # Evaluate the model\n    score = accuracy_score(y_test, preds)\n    print('Accuracy of',cls,\":\", score)","ebcf1e07":"# KNeighborsClassifier(3) : 0.9064865\n# DecisionTreeClassifier(max_depth=5) : 0.953\n# RandomForestClassifier(max_depth=5, max_features=1, n_estimators=10) : 0.9202695582240218\n# MLPClassifier(alpha=1, max_iter=1000) : 0.7159655009844976\n# AdaBoostClassifier() : 0.9548240383815414\n# GaussianNB() : 0.1614298788097285\n# GradientBoostingClassifier(random_state=0) : 0.9610915444133226","cbaff1d1":"len(test_data_identity['TransactionID'])","86e7fc0d":"len(test_data_transaction['TransactionID'])","b2022c3a":"test_data_identity_cpy = test_data_identity\ntest_data_transaction_cpy = test_data_transaction","38a70d2b":"test_data_transaction_cpy.merge(test_data_identity_cpy, how='outer',on='TransactionID')","5b17d936":"missingCols = ['id_07','id_08', 'id_21' ,'id_22', 'id_23' ,'id_24' ,'id_25' ,'id_26', 'id_27']\ncolumnsToDelete_cpy = columnsToDelete\nfor i in missingCols:\n    try:\n        columnsToDelete_cpy.remove(i)\n    except:\n        print(i, \"not in list\")","b5ba5981":"test_data_transaction_cpy = test_data_transaction_cpy.drop(columns=columnsToDelete_cpy)","b12f7127":"len(test_data_transaction_cpy.columns)","af72583c":"len(X_test.columns)","f556eebb":"cols_not_in_test = []\nfor i in train_data_identity.columns:\n    if i not in test_data_transaction_cpy.columns:\n        cols_not_in_test.append(i)","0917273f":"cols_not_in_test","865016c9":"len(combined_train_dataset.columns)","fcf31d4a":"len(test_data_transaction_cpy.columns)","05fe46eb":"# What cols are in test that are not in train and vice versa?\ncolumnsToDelete = []\nfor col in combined_train_dataset:\n    if col not in test_data_transaction_cpy.columns:\n        columnsToDelete.append(col)\nfor col in test_data_transaction_cpy:\n    if col not in combined_train_dataset.columns:\n        columnsToDelete.append(col)\ncolumnsToDelete.remove('isFraud')","c4c5ec73":"combined_train_dataset = combined_train_dataset.drop(columns=columnsToDelete,errors='ignore')\ntest_data_transaction_cpy = test_data_transaction_cpy.drop(columns=columnsToDelete,errors='ignore')","d1c54d5a":"len(combined_train_dataset.columns)","1d4cb384":"len(test_data_transaction_cpy.columns)","19e2de42":"predictions = my_pipeline.predict(test_data_transaction_cpy)","4e38a717":"transactionIDs = test_data_transaction_cpy['TransactionID'].values","0a630f49":"submit_df = pd.DataFrame(data=transactionIDs,columns=['TransactionID'])\nsubmit_df['isFraud'] = predictions\nsubmit_df = submit_df.set_index('TransactionID')","5c8dc965":"submit_df['isFraud'].value_counts()","eb80e965":"submit_df.to_csv('submission_v1.csv')","583455bd":"Had to run this multiple times each time with different classifiers because there were some memory issues...\n\nHere are the results","ac00188f":"# Train\/Test, Pipeline, and Models","4f34fdda":"# Visualizing and Dealing with NaN"}}