{"cell_type":{"255496d1":"code","7fc4bb7f":"code","fefdc598":"code","615355a4":"code","3e52ee5c":"code","f764b476":"code","b620f354":"code","34547149":"code","a72973a4":"code","9e1f95e5":"code","94b902a3":"code","39d3b512":"code","3911e81e":"code","6bc4a955":"code","2afe3067":"code","7e8c2d78":"code","1b59f97d":"code","d8932efe":"code","5c6d09b7":"code","bdaeb2d1":"code","e2d684a8":"code","8c3e56eb":"code","95b3985a":"code","8d9c2a81":"code","2dbd81b4":"code","afdb5655":"code","38c22a08":"code","fb5a1a11":"code","de1c54ea":"code","43f7165b":"code","5276e3f1":"code","801710e9":"code","b138a246":"code","aad9d9a8":"code","26ae7b16":"code","245c589d":"code","048d8d2d":"code","abeaf6ec":"code","0a4ba441":"code","8473b3b4":"code","6f118c69":"code","e8485aee":"code","a6a70706":"code","3d0e61d4":"code","0df0163e":"code","c01ce621":"code","7c76420d":"markdown","0d6ff8b0":"markdown","af7a829f":"markdown","f133f9f4":"markdown","fdde4891":"markdown","c9a2c5ab":"markdown","583ffea5":"markdown","ae2feeb7":"markdown","2706a198":"markdown","4221059e":"markdown","d322bb04":"markdown","d2b087a7":"markdown","b8bfe9b2":"markdown","42b25b79":"markdown","ef3d24b7":"markdown","0ae9f18d":"markdown","e31975f0":"markdown","515df8f7":"markdown","174cdd1a":"markdown","92ffd64a":"markdown","8936b0fe":"markdown","324d8c20":"markdown","4b57dbda":"markdown","1c0e4248":"markdown","54892a93":"markdown","8bf19ab9":"markdown","be98e4fe":"markdown","36d20667":"markdown","53af48cc":"markdown","02428373":"markdown","4a5d00f1":"markdown","16ff09dd":"markdown","3a19254e":"markdown","29d0c87b":"markdown","1208d962":"markdown","10d19cab":"markdown","a7464e31":"markdown","ebaa9b40":"markdown","c21c016f":"markdown","a5d19cf2":"markdown","43749aac":"markdown"},"source":{"255496d1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\n\nimport collections\nimport itertools\n\nimport scipy.stats as stats\nfrom scipy.stats import norm\nfrom scipy.special import boxcox1p\n\nimport statsmodels\nimport statsmodels.api as sm\n#print(statsmodels.__version__)\n\nfrom sklearn.preprocessing import scale, StandardScaler, RobustScaler, OneHotEncoder\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, KFold, StratifiedKFold, RandomizedSearchCV\nfrom sklearn.linear_model import Ridge, RidgeCV, Lasso, LassoCV, LinearRegression, ElasticNet,  HuberRegressor\nfrom sklearn.metrics import mean_squared_error, balanced_accuracy_score\nfrom xgboost import XGBRegressor\nfrom sklearn.gaussian_process import GaussianProcessRegressor\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.utils import resample\n\nfrom xgboost import XGBRegressor\nimport lightgbm as lgb\n\nimport warnings\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","7fc4bb7f":"Combined_data = pd.read_csv('..\/input\/new-york-city-airbnb-open-data\/AB_NYC_2019.csv')\nCombined_data.head()","fefdc598":"print('Number of features: {}'.format(Combined_data.shape[1]))\nprint('Number of examples: {}'.format(Combined_data.shape[0]))","615355a4":"#for c in df.columns:\n#    print(c, dtype(df_train[c]))\nCombined_data.dtypes","3e52ee5c":"Combined_data['last_review'] = pd.to_datetime(Combined_data['last_review'],infer_datetime_format=True) ","f764b476":"total = Combined_data.isnull().sum().sort_values(ascending=False)\npercent = (Combined_data.isnull().sum())\/Combined_data.isnull().count().sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total','Percent'], sort=False).sort_values('Total', ascending=False)\nmissing_data.head(40)","b620f354":"Combined_data.drop(['host_name','name'], axis=1, inplace=True)","34547149":"Combined_data[Combined_data['number_of_reviews']== 0.0].shape","a72973a4":"Combined_data['reviews_per_month'] = Combined_data['reviews_per_month'].fillna(0)","9e1f95e5":"earliest = min(Combined_data['last_review'])\nCombined_data['last_review'] = Combined_data['last_review'].fillna(earliest)\nCombined_data['last_review'] = Combined_data['last_review'].apply(lambda x: x.toordinal() - earliest.toordinal())","94b902a3":"total = Combined_data.isnull().sum().sort_values(ascending=False)\npercent = (Combined_data.isnull().sum())\/Combined_data.isnull().count().sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total','Percent'], sort=False).sort_values('Total', ascending=False)\nmissing_data.head(40)","39d3b512":"fig, axes = plt.subplots(1,3, figsize=(21,6))\nsns.distplot(Combined_data['price'], ax=axes[0])\nsns.distplot(np.log1p(Combined_data['price']), ax=axes[1])\naxes[1].set_xlabel('log(1+price)')\nsm.qqplot(np.log1p(Combined_data['price']), stats.norm, fit=True, line='45', ax=axes[2]);","3911e81e":"Combined_data = Combined_data[np.log1p(Combined_data['price']) < 8]\nCombined_data = Combined_data[np.log1p(Combined_data['price']) > 3]","6bc4a955":"fig, axes = plt.subplots(1,3, figsize=(21,6))\nsns.distplot(Combined_data['price'], ax=axes[0])\nsns.distplot(np.log1p(Combined_data['price']), ax=axes[1])\naxes[1].set_xlabel('log(1+price)')\nsm.qqplot(np.log1p(Combined_data['price']), stats.norm, fit=True, line='45', ax=axes[2]);","2afe3067":"Combined_data['price'] = np.log1p(Combined_data['price'])","7e8c2d78":"print(Combined_data.columns)","1b59f97d":"print('In this dataset there are {} unique hosts renting out  a total number of {} properties.'.format(len(Combined_data['host_id'].unique()), Combined_data.shape[0]))","d8932efe":"Combined_data = Combined_data.drop(['host_id', 'id'], axis=1)","5c6d09b7":"sns.catplot(x='neighbourhood_group', kind='count' ,data=Combined_data)\nfig = plt.gcf()\nfig.set_size_inches(12, 6)","bdaeb2d1":"fig, axes = plt.subplots(1,3, figsize=(21,6))\nsns.distplot(Combined_data['latitude'], ax=axes[0])\nsns.distplot(Combined_data['longitude'], ax=axes[1])\nsns.scatterplot(x= Combined_data['latitude'], y=Combined_data['longitude'])","e2d684a8":"sns.catplot(x='room_type', kind='count' ,data=Combined_data)\nfig = plt.gcf()\nfig.set_size_inches(8, 6)","8c3e56eb":"fig, axes = plt.subplots(1,2, figsize=(21, 6))\n\nsns.distplot(Combined_data['minimum_nights'], rug=False, kde=False, color=\"green\", ax = axes[0])\naxes[0].set_yscale('log')\naxes[0].set_xlabel('minimum stay [nights]')\naxes[0].set_ylabel('count')\n\nsns.distplot(np.log1p(Combined_data['minimum_nights']), rug=False, kde=False, color=\"green\", ax = axes[1])\naxes[1].set_yscale('log')\naxes[1].set_xlabel('minimum stay [nights]')\naxes[1].set_ylabel('count')","95b3985a":"Combined_data['minimum_nights'] = np.log1p(Combined_data['minimum_nights'])","8d9c2a81":"fig, axes = plt.subplots(1,2,figsize=(18.5, 6))\nsns.distplot(Combined_data[Combined_data['reviews_per_month'] < 17.5]['reviews_per_month'], rug=True, kde=False, color=\"green\", ax=axes[0])\nsns.distplot(np.sqrt(Combined_data[Combined_data['reviews_per_month'] < 17.5]['reviews_per_month']), rug=True, kde=False, color=\"green\", ax=axes[1])\naxes[1].set_xlabel('ln(reviews_per_month)')","2dbd81b4":"fig, axes = plt.subplots(1,1, figsize=(21,6))\nsns.scatterplot(x= Combined_data['availability_365'], y=Combined_data['reviews_per_month'])","afdb5655":"Combined_data['reviews_per_month'] = Combined_data[Combined_data['reviews_per_month'] < 17.5]['reviews_per_month']","38c22a08":"fig, axes = plt.subplots(1,1,figsize=(18.5, 6))\nsns.distplot(Combined_data['availability_365'], rug=False, kde=False, color=\"blue\", ax=axes)\naxes.set_xlabel('availability_365')\naxes.set_xlim(0, 365)","fb5a1a11":"corrmatrix = Combined_data.corr()\nf, ax = plt.subplots(figsize=(15,12))\nsns.heatmap(corrmatrix, vmax=0.8, square=True)","de1c54ea":"#sns.pairplot(Combined_data.select_dtypes(exclude=['object']))","43f7165b":"categorical_features = Combined_data.select_dtypes(include=['object'])\nprint('Categorical features: {}'.format(categorical_features.shape))","5276e3f1":"categorical_features_one_hot = pd.get_dummies(categorical_features)\ncategorical_features_one_hot.head()","801710e9":"Combined_data['reviews_per_month'] = Combined_data['reviews_per_month'].fillna(0)","b138a246":"numerical_features =  Combined_data.select_dtypes(exclude=['object'])\ny = numerical_features.price\nnumerical_features = numerical_features.drop(['price'], axis=1)\nprint('Numerical features: {}'.format(numerical_features.shape))","aad9d9a8":"X = np.concatenate((numerical_features, categorical_features_one_hot), axis=1)\nX_df = pd.concat([numerical_features, categorical_features_one_hot], axis=1)\n#print('Dimensions of the design matrix: {}'.format(X.shape))\n#print('Dimension of the target vector: {}'.format(y.shape))","26ae7b16":"Processed_data = pd.concat([X_df, y], axis = 1)\nProcessed_data.to_csv('NYC_Airbnb_Processed.dat')","245c589d":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","048d8d2d":"print('Dimensions of the training feature matrix: {}'.format(X_train.shape))\nprint('Dimensions of the training target vector: {}'.format(y_train.shape))\nprint('Dimensions of the test feature matrix: {}'.format(X_test.shape))\nprint('Dimensions of the test target vector: {}'.format(y_test.shape))","abeaf6ec":"scaler = RobustScaler()\nX_train = scaler.fit_transform(X_train)\nX_test = scaler.fit_transform(X_test)","0a4ba441":"n_folds = 5\n\n# squared_loss\ndef rmse_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state = 91).get_n_splits(numerical_features)\n    return cross_val_score(model, X_train, y_train, scoring='neg_mean_squared_error', cv=kf)\n\ndef rmse_lv_cv(model):\n    kf = KFold(n_folds, shuffle=True, random_state = 91).get_n_splits(numerical_features)\n    return cross_val_score(model, Xlv_train, y_train, scoring='neg_mean_squared_error', cv=kf)","8473b3b4":"xgb_baseline = XGBRegressor(n_estimators=1000, learning_rate=0.05, early_stopping=5)\nkf = KFold(n_folds, shuffle=True, random_state = 91).get_n_splits(numerical_features)\ncv_res = cross_val_score(xgb_baseline, X_train, y_train, scoring='neg_mean_squared_error', cv=kf)\nxgb_baseline.fit(X_train, y_train)\ny_train_xgb_base = xgb_baseline.predict(X_train)\ny_test_xgb_base = xgb_baseline.predict(X_test)\nxgb_baseline_results = pd.DataFrame({'algorithm':['XGBRegressor[baseline]'],\n            'CV error': cv_res.mean(), \n            'CV std': cv_res.std(),\n            'training error': [mean_squared_error(y_train_xgb_base, y_train)]})","6f118c69":"d = {'Learning Rate':[],\n            'Mean CV Error': [],\n            'CV Error Std': [],\n            'Training Error': []}\nfor lr in [0.01, 0.05, 0.1, 0.5]:\n    continue\n    xgb_model = XGBRegressor(n_estimators=1000, learning_rate=lr, early_stopping=5)\n    cv_res = -cross_val_score(xgb_model, X_train, y_train, scoring='neg_mean_squared_error', cv=kf)\n    xgb_model.fit(X_train, y_train)\n    y_train_xgb = xgb_model.predict(X_train)\n    d['Learning Rate'].append(lr)\n    d['Mean CV Error'].append(cv_res.mean())\n    d['CV Error Std'].append(cv_res.std())\n    # makes no sense to look at max\/min when we only have 3 CV folds\n    #d['Max CV Error'].append(max(cv_res)\n    #d['Min CV Error'].append(max(cv_res)\n    d['Training Error'].append(mean_squared_error(y_train_xgb, y_train))\n\n#without early stopping\nd = {'Learning Rate':[0.01, 0.05, 0.1, 0.5],\n        'Mean CV Error': [0.184223, 0.177748, 0.175002, 0.188239],\n        'CV Error Std': [0.00626211, 0.00575213, 0.00544426, 0.00525595],\n        'Training Error': [0.179093, 0.164874, 0.154238, 0.109885]}\n\nxgb_tuning_1 = pd.DataFrame(d)\nxgb_tuning_1","e8485aee":"fig, ax = plt.subplots(1, 1, figsize=(20,6))\n\nax.plot(xgb_tuning_1['Learning Rate'], xgb_tuning_1['Mean CV Error'], color='red')\nax.plot(xgb_tuning_1['Learning Rate'], xgb_tuning_1['Mean CV Error'], 'o', color='black')\nax.fill_between(xgb_tuning_1['Learning Rate'], xgb_tuning_1['Mean CV Error'] - xgb_tuning_1['CV Error Std'], xgb_tuning_1['Mean CV Error'] + xgb_tuning_1['CV Error Std'], color='r', alpha=.1)\nax.plot(xgb_tuning_1['Learning Rate'], xgb_tuning_1['Training Error'], color='blue')\nax.plot(xgb_tuning_1['Learning Rate'], xgb_tuning_1['Training Error'], 'o', color='black')\nax.legend(fontsize=12, loc = 'center right');\nax.set_ylim(0.1, 0.2)\nax.set_xlabel('Learning Rate')\nax.set_ylabel('Mean Squared Error')\n#ax.set_title('')","a6a70706":"d = {'Max_depth':[],\n             'Min_child_weight': [],\n            'Mean CV Error': [],\n            'CV Error Std': [],\n            'Training Error': []}\nxgbreg = XGBRegressor(n_estimators=2, learning_rate=0.05, early_stopping=5)\nparams2 = {'max_depth': list(range(3,10,2)), 'min_child_weight': list(range(1,6,2))}\n#print(params2)\n#xgb_random.fit(X_train, y_train)\nkf = KFold(n_folds, shuffle=True, random_state = 91).get_n_splits(X_train)\nfor md in params2['max_depth']:\n    for mcw in params2['min_child_weight']:\n        continue\n        xgb_model = XGBRegressor(n_estimators=1000, learning_rate=0.1, early_stopping=5, max_depth=md, min_child_weight=mcw )\n        cv_res = -cross_val_score(xgb_model, X_train, y_train, scoring='neg_mean_squared_error', cv=kf)\n        xgb_model.fit(X_train, y_train)\n        y_train_xgb = xgb_model.predict(X_train)\n        d['Max_depth'].append(md)\n        d['Min_child_weight'].append(mcw)\n        d['Mean CV Error'].append(cv_res.mean())\n        d['CV Error Std'].append(cv_res.std())\n        # makes no sense to look at max\/min when we only have 3 CV folds\n        #d['Max CV Error'].append(max(cv_res)\n        #d['Min CV Error'].append(max(cv_res)\n        d['Training Error'].append(mean_squared_error(y_train_xgb, y_train))\n\n#print(d)\n\nd = {'Max_depth': [3, 3, 3, 5, 5, 5, 7, 7, 7, 9, 9, 9], 'Min_child_weight': [1, 3, 5, 1, 3, 5, 1, 3, 5, 1, 3, 5], \n 'Mean CV Error': [0.1750024956601357, 0.17483011840929769, 0.17493846554576997, 0.17309889297300166, 0.17316622731288867, \n        0.17351576928079232, 0.17662213266155447, 0.17623539711716868, 0.17586167155362295, 0.18027062402369495, 0.1795815552171006, 0.1794402792605232], \n 'CV Error Std': [0.0054442612607845196, 0.005346726848155686, 0.005781224325978589, 0.0047992091315554805, 0.005078460548746871, 0.0055470435006580825, \n                  0.004522282538112627, 0.005521088520254507, 0.005182127039391581, 0.00548502303198156, 0.0056636180606624885, 0.005837983614899652],\n 'Training Error': [0.15423828100740364, 0.1548338435116449, 0.15489721899341147, 0.1174713383813709, 0.11768836644071619, 0.11962286723882598, \n                    0.07157996439924702, 0.07249081997317249, 0.0809473890478948, 0.03364907441870936, 0.03787025803370217, 0.045449523400453724]}\n        \nxgb_tuning_2 = pd.DataFrame(d)\nxgb_tuning_2","3d0e61d4":"fig, ax = plt.subplots(1, 1, figsize=(20,6))\n\ncolors = ['green','blue','red']\nfor i, mcw in enumerate(params2['min_child_weight']):\n    color = colors[i]\n    xgb_tuning_3 = xgb_tuning_2[xgb_tuning_2['Min_child_weight']==mcw]\n    ax.plot(xgb_tuning_3['Max_depth'], xgb_tuning_3['Mean CV Error'], color=color, label= 'mean_child_weight='+str(mcw)+', CV')\n    ax.plot(xgb_tuning_3['Max_depth'], xgb_tuning_3['Mean CV Error'], 'o', color='black', label='_nolegend_')\n    #ax.fill_between(xgb_tuning_3['Max_depth'], xgb_tuning_3['Mean CV Error'] - xgb_tuning_3['CV Error Std'], \n                    #xgb_tuning_3['Mean CV Error'] + xgb_tuning_3['CV Error Std'], color='r', alpha=.1, label='_nolegend_')\n    ax.plot(xgb_tuning_3['Max_depth'], xgb_tuning_3['Training Error'], color=color, label='mean_child_weight='+str(mcw)+', Training')\n    ax.plot(xgb_tuning_3['Max_depth'], xgb_tuning_3['Training Error'], 'o', color='black', label='_nolegend_')\n\nax.legend(fontsize=12, loc = 'center right');\nax.set_ylim(0, 0.25)\nax.set_xlabel('Max_depth')\nax.set_ylabel('Mean Squared Error')\n#ax.set_title('')","0df0163e":"print('Optimal values of max_depth and mean_child_weight are: \\n')\nprint(xgb_tuning_2.iloc[xgb_tuning_2.idxmin()['Mean CV Error']])","c01ce621":"d = {'gamma':[],\n            'Mean CV Error': [],\n            'CV Error Std': [],\n            'Training Error': []}\nparams3 = {'gamma': [i\/10.0 for i in range(0,12, 2)]}\n#print(params2)\n#xgb_random.fit(X_train, y_train)\nkf = KFold(n_folds, shuffle=True, random_state = 91).get_n_splits(X_train)\nfor g in params3['gamma']:\n        #print(g)\n        #continue\n        xgb_model = XGBRegressor(n_estimators=1000, learning_rate=lr, early_stopping=5, max_depth=5, min_child_weight=1, gamma = g)\n        cv_res = -cross_val_score(xgb_model, X_train, y_train, scoring='neg_mean_squared_error', cv=kf)\n        xgb_model.fit(X_train, y_train)\n        y_train_xgb = xgb_model.predict(X_train)\n        d['gamma'].append(g)\n        d['Mean CV Error'].append(cv_res.mean())\n        d['CV Error Std'].append(cv_res.std())\n        # makes no sense to look at max\/min when we only have 3 CV folds\n        #d['Max CV Error'].append(max(cv_res)\n        #d['Min CV Error'].append(max(cv_res)\n        d['Training Error'].append(mean_squared_error(y_train_xgb, y_train))\n        \nprint(d)\n\nxgb_tuning_4 = pd.DataFrame(d)\nxgb_tuning_4","7c76420d":"There don't appear to exist obvious, strong correlations between these variables. \n\nHowever, the number of reviews per month is fairly (40%) correlated with the total number of reviews and the the total number of reviews is correlated (at 30%) with the availability of the property. Both of these correlations make sense.\n\nIt's also interesting that the longitude is anticorrelated (at 20%) with the price. That also makes sense - property in the Bronx and in Queens is cheaper than Manhattan and Brooklyn.","0d6ff8b0":"### Reviews per month","af7a829f":"1. Penalized regression: https:\/\/www.kaggle.com\/aaron7sun\/you-got-this-feature-engineering-and-lasso\n2. Hyperparameter tuning for random forests: https:\/\/towardsdatascience.com\/hyperparameter-tuning-the-random-forest-in-python-using-scikit-learn-28d2aa77dd74","f133f9f4":"# MODEL BUILDING","fdde4891":"### Room type","c9a2c5ab":"This distribution is highly skewed towards the low and high end. The dataset contains a hiuge number of properties that are available only for a couple of days each year, and a decent number that are available for > 300 days. ","583ffea5":"I now scale the design matrix with sklearn's RobustScaler() so that each predictor has zero mean and unit variance. This helps the convergence of machine learning algorithms such as linear regression.\n\nI avoid data snooping by defining the scaleing transformation based on the training data not the test data.","ae2feeb7":"### Availability_365","2706a198":"## Train-test split","4221059e":"Since the number of unique hosts is close to the total number of examples, we're not going to use hostname in our regression analysis since it would cause the number of parameters in our model to baloon! \n\nIn real-life, when there is more data and perhaps some feature data on hosts, I expect past history of a host and of a property to be a strong predictor of price!","d322bb04":"## Missing data","d2b087a7":"# Input preprocessing","b8bfe9b2":"## Cross-validation routine","42b25b79":"As far as room types, this dataset is balanced away from 'Shared room' properties. The proportions of private room and entire home\/apt rentals are close, with entire home\/apt dominating prive room by <10%.","ef3d24b7":"### Pearson correlation matrix","0ae9f18d":"### Learning Rate","e31975f0":"Longitude and latitude are somewhat correlated with each other. This is because the locations of properties tend to come from clusters.","515df8f7":"### Neighbourhood group","174cdd1a":"I'm going to split the data into a test set and a training set. I will hold out the test set until the very end and use the error on those data as an unbiased estimate of how my models did. \n\nI might perform a further split later on the training set into training set proper and a validation set or I might cross-validate.","92ffd64a":"## Distributions of predictors","8936b0fe":"I will score models based on K-fold cross-validation with 5 folds.","324d8c20":"# Acknolwedgements","4b57dbda":"### Categorical features","1c0e4248":"## Distribution of the target (price)","54892a93":"### Baseline","8bf19ab9":"## Choose modelling task","be98e4fe":"### Longitude and latitude","36d20667":"I notice that the target has a highly skewed distribution. This can cause problems for machine learning algorithms such as linear regression. A log transformation and removal of outliers makes the distribution look much closer to normal.","53af48cc":"## Load data","02428373":"Since this is not a competition, the next step is to decide on a task. The obvious thing to look at is price and that's what I'll do. That makes this a regression problem.","4a5d00f1":"The NaN values in the last_review and reviews_per_month columns all occur for examples where no reviews were given in the first place. \n\nFor reviews_per_month, I will fill those values with 0's.","16ff09dd":"The distribution of the number of reviews per month is highly skewed however way we cut it. This is because there is a large weight on small numbers: there are a lot of properties which only get a few reviews and a rather fat tail of properties which get a lot of reviews. \n\nOne explanation would be that the properties which are available a larger fraction of the year get more reviews. However, a scatter plot of reviews_per_month and availability_365 variables shows no evidence of a relationship so that explanation would appear to not be valid.","3a19254e":"### Minimum nights","29d0c87b":"## More Data Preprocessing: Rescale the design matrix","1208d962":"## XGBoost Regressor","10d19cab":"### Host_id","a7464e31":"### Numerical features","ebaa9b40":"# EDA","c21c016f":"# SUMMARY","a5d19cf2":"## Bivariate correlations","43749aac":"I notice that Statten Island and the Bronx are highly underrepresented in this dataset. For Statten Island, the reason is that the population of the island is small. However, this can't be the case for the Bronx which has a population comparable (~1.4mln) to Manhattan (~1.6mln) or for for Brooklyn \/Queens with their populations of ~2.5mln and ~2.4mln, respectively. \n\nThis makes sense: Queens, the Bronx  and, to a fair extent Brooklyn, are residential neighborhoods unlike Manhattan which is a business centre as well as a tourist destination."}}