{"cell_type":{"0a064d8a":"code","a593c807":"code","8b51509e":"code","c07ac738":"code","8dc9e1de":"code","5386f090":"code","fdd580ef":"code","373ffc60":"code","6ed35ada":"code","49d92025":"code","3b0ed011":"code","bc9a333c":"code","27a43e66":"code","bf5877ca":"code","4b5bd89e":"code","7b381461":"code","b6fcf50d":"code","437d67d2":"code","e8743962":"code","912373e3":"code","61b8cdf3":"code","1cec75dc":"code","5d9f13b4":"code","e04438ae":"code","ab141fa9":"code","ce27b28a":"code","56550572":"code","09a7c490":"code","eb9557d4":"code","daf73cf8":"code","8bfedc21":"code","236eced0":"code","b2efb489":"code","ba279ff3":"code","63570dd8":"code","2ae85012":"code","7484e57b":"code","7c840179":"code","36b17853":"code","94542b25":"code","f49af2e4":"code","151ea5ca":"code","94fac3ff":"code","ca1b5132":"code","c9fb3a09":"code","43ade351":"code","08de1968":"code","f2c8983f":"code","c178bcfa":"code","ad46090a":"code","1c52dfdc":"code","6cc84569":"code","29a053d0":"code","047ce873":"code","713c6a05":"code","03d3b24b":"code","2ecb05ac":"code","3098765c":"code","78b1b6bc":"code","51ccd33f":"code","2901366e":"code","7a573ce2":"code","4aa29015":"code","7fa12336":"code","312d0b55":"code","8389a341":"code","3978295b":"code","7453e772":"code","f507da1e":"code","f8616317":"code","6af842e1":"code","722f4a41":"code","a8318fb4":"code","c9f7a533":"code","40f85447":"code","ad60b8ae":"code","e79d601a":"code","7ea03fcf":"code","8b73e940":"code","b5e3671a":"code","bebdb0a6":"code","c951edcd":"code","659a1be5":"code","7f5bd065":"code","569b50ff":"code","f897c650":"code","d627abdd":"code","098c8bef":"code","c9547cd3":"code","77180857":"code","3d9f2ca0":"code","e91cd825":"code","0bd382f1":"code","66e70f7e":"code","a6794663":"code","412fab52":"code","9a3cc033":"code","54b61747":"code","58994995":"code","ca587322":"code","445ae523":"code","99e17d32":"code","9710384e":"code","7a60044a":"code","6ad41c4d":"code","150c8976":"code","da2d0e43":"code","c20a54f0":"code","6b5276c6":"code","45b3b399":"code","3cc732fd":"code","0998a5b5":"code","4ab67e0f":"code","d33fb218":"code","3a386507":"code","b606db19":"code","2123ba44":"code","0ff5c096":"code","15520ee2":"code","4bb38e6f":"code","ef279a51":"code","55110acc":"code","ca2c48be":"code","b54c0999":"code","ba92f830":"code","6e492cc3":"code","fd229d55":"code","15bcc340":"code","bf8ff9c9":"code","803a8407":"code","8b92c74e":"markdown","a2f1d1b4":"markdown","3890f531":"markdown","118115e8":"markdown","3b80fa21":"markdown","6044c504":"markdown","38a0c86e":"markdown","042d9830":"markdown","f30a4c63":"markdown","86f57002":"markdown","8db277c2":"markdown","526fcf35":"markdown","83c9f973":"markdown","92a9b261":"markdown","d3d9b6b6":"markdown","e879fb45":"markdown","0060c61f":"markdown","d68303f7":"markdown","86ac63c1":"markdown","498c1a92":"markdown","b8cddc90":"markdown","a322b164":"markdown","6b0473e9":"markdown","44476746":"markdown","49140328":"markdown","1ab23d70":"markdown"},"source":{"0a064d8a":"import numpy as np\nimport pandas as pd\n%matplotlib inline\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom matplotlib import colors","a593c807":"print ('numpy   ver: ', np.__version__)\nprint ('pandas  ver: ', pd.__version__)\nprint ('seaborn ver: ', sns.__version__)","8b51509e":"from sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, cross_validate\nfrom sklearn import metrics\nfrom sklearn.metrics import f1_score, confusion_matrix","c07ac738":"from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier, VotingClassifier, BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier, GradientBoostingClassifier, StackingClassifier\nfrom sklearn.experimental import enable_hist_gradient_boosting\nfrom sklearn.ensemble import HistGradientBoostingClassifier\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.neighbors import KNeighborsClassifier, RadiusNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB, CategoricalNB, ComplementNB\nfrom sklearn.tree import DecisionTreeClassifier, ExtraTreeClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.linear_model import PassiveAggressiveClassifier, LogisticRegressionCV, LarsCV, LassoCV, LassoLarsCV\nfrom sklearn.linear_model import LogisticRegression, Perceptron, SGDClassifier, RidgeClassifierCV\nfrom sklearn.linear_model import RidgeClassifier, ElasticNetCV, OrthogonalMatchingPursuit \nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis, QuadraticDiscriminantAnalysis\nfrom sklearn.semi_supervised import LabelPropagation\n\nfrom sklearn.cluster import KMeans, AgglomerativeClustering\nfrom sklearn.preprocessing import LabelEncoder, OneHotEncoder\nfrom sklearn import preprocessing\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.base import is_classifier, is_regressor","8dc9e1de":"from xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier","5386f090":"import warnings\nwarnings.filterwarnings('ignore')\nwarnings.filterwarnings('ignore', category=DeprecationWarning)","fdd580ef":"# Loading data files into pandas DataFrames\ntrain = pd.read_csv('\/kaggle\/input\/titanic\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/titanic\/test.csv')","373ffc60":"# lets merge dataframes before cleaning data\nfull = pd.merge(train, test, how = 'outer')","6ed35ada":"# lets see on our data\nplt.figure(figsize=(16,7))\nsns.heatmap(full.isnull())","49d92025":"full.isnull().sum()","3b0ed011":"# we can use median to fill missed data on 'Age'\nfull.groupby(['Pclass', 'Sex'])['Age'].median()","bc9a333c":"def set_null_age(cols):\n    Age,Pclass,Sex = cols\n    if pd.isnull(Age):\n        if Pclass == 1: \n            if Sex == 'female':\n                return 36\n            else:\n                return 42\n        elif Pclass == 2: \n            if Sex == 'female':\n                return 28\n            else:\n                return 29.5          \n        elif Pclass == 3: \n            if Sex == 'female':\n                return 22\n            else:\n                return 25  \n    else:\n        return Age","27a43e66":"full['Age']=full[['Age', 'Pclass', 'Sex']].apply(set_null_age, axis = 1)","bf5877ca":"#lets round all ages\nfull.Age = full.Age.apply('ceil').astype(int)","4b5bd89e":"# im goint ot use ML to split ages for some clusters:)\n\nN = 6\nage = full.pivot_table(values = 'Survived', index = 'Age').sort_values('Age').reset_index()\nX = age[['Age', 'Survived']]\n\nclust = KMeans(n_clusters=N).fit(X)\nc = clust.cluster_centers_\n\n# we can take colors fro mathplotlib (10 colors - max 10 clusters)\nclrs = list(colors.TABLEAU_COLORS.keys())\n\nfig = plt.figure(figsize=(15, 3))\nfor x, y in zip(age.Age, age.Survived):\n    cl = clust.predict(np.array([x,y]).reshape(1, -1))\n    plt.scatter(x, y, s=75, c = clrs[cl[0]])\n    for i in range(len(c)):\n        plt.scatter(c[i][0], c[i][1], s=200, marker=\"x\", c=\"black\")\nplt.show()","7b381461":"# Another algorithm for clustering\n# N = 12\n# age = full.pivot_table(values = 'Survived', index = 'Age').sort_values('Age').reset_index()\n# X = age[['Age', 'Survived']]\n\n# clust = AgglomerativeClustering(n_clusters=N).fit(X)\n\n# lab =  clust.labels_\n# colors = ['g', 'y', 'm', 'r', 'c', 'b', 'g', 'y', 'm', 'r', 'c', 'b']\n\n# fig = plt.figure(figsize=(15, 3))\n# for x, y, z in zip(age.Age, age.Survived, lab):\n#     plt.scatter(x, y, s=75, c = colors[z])\n# plt.show()","b6fcf50d":"age['age_grouped'] = clust.labels_\nage = age.drop('Survived', axis = 1)","437d67d2":"full = pd.merge(full, age, on = 'Age', how = 'left')","e8743962":"full[pd.isnull(full['age_grouped'])]","912373e3":"# we fill set nearest clusters numbers for those rows\nfull.loc[972, 'age_grouped'] = full['age_grouped'][full.Age == 66].min()\nfull.loc[987, 'age_grouped'] = full['age_grouped'][full.Age == 74].min()","61b8cdf3":"col = 'age_grouped'\ntarget = 'Survived'\nsort = target\nprint(full[:891].pivot_table(values = target, index = col).sort_values(sort))\nfull[:891].pivot_table(values = target, index = col).sort_values(sort).plot(kind = 'bar', figsize=(5,3))","1cec75dc":"# we have some duplicates on 'Name'\nfull.Name.nunique()","5d9f13b4":"full[full.duplicated('Name')]","e04438ae":"# its ok - they are different ppl\nfull[(full.Name == 'Kelly, Mr. James') | (full.Name == 'Connolly, Miss. Kate')].sort_values('Name')","ab141fa9":"# we dont really need names, but we can try to use Titles","ce27b28a":"full['Title'] = full['Name'].str.extract(' ([A-Za-z]+)\\.', expand=False)","56550572":"full['Title'] = full['Title'].replace(['Lady', 'Countess','Capt', 'Col',\\\n'Don', 'Dr', 'Major', 'Rev', 'Sir', 'Jonkheer', 'Dona'], 'other')\n\nfull['Title'] = full['Title'].replace(['Mlle', 'Ms'], 'Miss')\nfull['Title'] = full['Title'].replace('Mme', 'Mrs')","09a7c490":"# seems like a good feature\ncol = 'Title'\ntarget = 'Survived'\nsort = target\nprint(full[:891].pivot_table(values = target, index = col).sort_values(sort))\nfull[:891].pivot_table(values = target, index = col).sort_values(sort).plot(kind = 'bar', figsize=(5,3))","eb9557d4":"full['Title'] = full['Title'].map({\"Mr\": 1, \"other\": 2, \"Master\": 3, \"Miss\": 4, \"Mrs\": 5})","daf73cf8":"# some tickets have diplicates in number.. we can try to use it\nfull['ticket_double'] = 0","8bfedc21":"full['ticket_double'][full.duplicated('Ticket')] = 1","236eced0":"# almost 50% survuve rate against 36% basic.. it can be the feature\ncol = 'ticket_double'\ntarget = 'Survived'\nsort = col\nprint(full[:891].pivot_table(values = target, index = col).sort_values(sort))\nfull[:891].pivot_table(values = target, index = col).sort_values(sort).plot(kind = 'bar', figsize=(5,3))","b2efb489":"# we can try to extract cabin name and use it later as feature\nfull.Cabin.unique()","ba279ff3":"full['Cabin'] = full['Cabin'].str.extract('([A-Za-z]+)', expand = False)","63570dd8":"full['Cabin'] = full['Cabin'].map({'A':1, 'G':2, 'C':3, 'F':4, 'B':5, 'E':6, 'D':7, 'T':0})","2ae85012":"full['Cabin'] = full['Cabin'].fillna(0)","7484e57b":"# also seems good for feature\ncol = 'Cabin'\ntarget = 'Survived'\nsort = col\nprint(full[:891].pivot_table(values = target, index = col).sort_values(sort))\nfull[:891].pivot_table(values = target, index = col).sort_values(sort).plot(kind = 'bar', figsize=(5,3))","7c840179":"full[pd.isnull(full.Fare) | (full.Fare == 0)].Fare.count()","36b17853":"full.groupby(['Pclass'])['Fare'].median().round(2)","94542b25":"def set_null_fare(cols):\n    Fare, Pclass = cols\n    if Fare == 0 or pd.isnull(Fare):\n        if Pclass == 1:\n            return 63.36\n        elif Pclass == 2:\n            return 15.75\n        elif Pclass == 3:\n            return 8.05\n    else:\n        return Fare","f49af2e4":"full['Fare']=full[['Fare', 'Pclass']].apply(set_null_fare, axis = 1).round(2)","151ea5ca":"# we can try different classes for 'Fare'\ndef set_gr_fare(col):\n    Fare = col\n    if Fare < 8.05:\n        return 1\n    elif Fare < 15.75:\n        return 2\n    elif Fare < 32:\n        return 3\n    else:\n        return 4","94fac3ff":"full['fare_grouped']=full['Fare'].apply(set_gr_fare)","ca1b5132":"# also seems good for feature\ncol = 'fare_grouped'\ntarget = 'Survived'\nsort = col\nprint(full[:891].pivot_table(values = target, index = col).sort_values(sort))\nfull[:891].pivot_table(values = target, index = col).sort_values(sort).plot(kind = 'bar', figsize=(5,3))","c9fb3a09":"#very good feature\ncol = 'Sex'\ntarget = 'Survived'\nsort = target\nprint(full[:891].pivot_table(values = target, index = col).sort_values(sort))\nfull[:891].pivot_table(values = target, index = col).sort_values(sort).plot(kind = 'bar', figsize=(5,3))","43ade351":"full['Sex'] = full['Sex'].map({'male':1, 'female':2})","08de1968":"full[pd.isnull(full.Embarked)]","f2c8983f":"full.loc[(61,829), 'Embarked'] = 'S'","c178bcfa":"col = 'Embarked'\ntarget = 'Survived'\nsort = target\nprint(full[:891].pivot_table(values = target, index = col).sort_values(sort))\nfull[:891].pivot_table(values = target, index = col).sort_values(sort).plot(kind = 'bar', figsize=(5,3))","ad46090a":"# we will use 1, 2, 3 insteed of S, Q, C\nfull['Embarked'] = full['Embarked'].map({\"S\": 1, \"Q\": 2, \"C\": 3})","1c52dfdc":"# we can count family size\nfull['f_size'] = full.SibSp + full.Parch + 1\nfull['age_class'] = full.age_grouped * full.Pclass\nfull['is_alone'] = full['f_size'].apply(lambda x: 0 if x == 1 else 1)","6cc84569":"col = 'f_size'\ntarget = 'Survived'\nsort = col\nprint(full[:891].pivot_table(values = target, index = col).sort_values(sort))\nfull[:891].pivot_table(values = target, index = col).sort_values(sort).plot(kind = 'bar', figsize=(5,3))","29a053d0":"# we dony anymore need those columns\nfull = full.drop(['PassengerId', 'Name', 'Ticket'], axis = 1)","047ce873":"# We can use features as is.. but i want ot test dummies on all columns.. sometimes it gives better results. \n# But we can try both ways later\n\nbfull = full.drop(['Age', 'Fare'], axis = 1)\nbfull = bfull.fillna(0).astype(int)\n\ndummy_col=[ 'Pclass',\n            'Sex',\n            'Cabin',\n            'Embarked',\n            'age_grouped',\n            'Title',\n            'ticket_double',\n            'fare_grouped',\n            'f_size',\n            'is_alone']\ndummy = pd.get_dummies(bfull[dummy_col], columns=dummy_col)\nbfull = pd.concat([dummy, bfull], axis = 1)\nbfull.drop([\n            'Pclass',\n            'Sex',\n            'SibSp',\n            'Parch',\n            'Cabin',\n            'Embarked',\n            'age_grouped',\n            'Title',\n            'ticket_double',\n            'fare_grouped',\n            'f_size',\n            'age_class',\n            'is_alone'], inplace = True, axis = 1)\n\n\n# X_train = bfull[:891].drop('Survived', axis = 1).astype(int)\n# y_train = bfull[:891].Survived.astype(int)\n# X_test = bfull[891:].drop('Survived', axis = 1).astype(int)","713c6a05":"# X_train = full[:891].drop('Survived', axis = 1).astype(int)\n# y_train = full[:891].Survived.astype(int)\n# X_test = full[891:].drop('Survived', axis = 1).astype(int)","03d3b24b":"# list of all features we have.. we will reduce that number later\nlist(bfull.columns)","2ecb05ac":"# let see correlation rates\ndf = bfull[:891].astype(int)\ndf_all_corr = df.corr().abs().unstack().sort_values(kind=\"quicksort\", ascending=False).reset_index()\ndf_all_corr.rename(columns={\"level_0\": \"F1\", \"level_1\": \"feature\", 0: 'Corr'}, inplace=True)\nfinal_corr = df_all_corr[df_all_corr['F1'] == 'Survived']\nfinal_corr.Corr = final_corr.Corr.apply('abs').round(3)\nfinal_corr = final_corr.sort_values('Corr', ascending=False)\n\ncol = 'feature'\ntarget = 'Corr'\nsort = target\nprint(final_corr[1:].pivot_table(values = target, index = col).sort_values(sort, ascending=False))\nfinal_corr[1:].pivot_table(values = target, index = col).sort_values(sort).plot(kind = 'barh', figsize=(10,7), fontsize = 9)","3098765c":"# lets have dataset with features rates\nliverate = pd.DataFrame(list(bfull.columns)[:-1], columns = ['feature'])","78b1b6bc":"liverate[[ 'nSurv', 'Surv', 'count']] = 0","51ccd33f":"for i, feat in enumerate(liverate.feature):\n    col = feat\n    target = 'Survived'\n    sort = col\n    r = bfull[:891].pivot_table(values = target, index = col).sort_values(sort)\n    q = bfull[:891].groupby(feat).count()\n    liverate.loc[i, 'count'] = q.iloc[1,0]\n    liverate.loc[i, 'nSurv'] = r.loc[0,'Survived'].round(3)\n    liverate.loc[i, 'Surv'] = r.loc[1,'Survived'].round(3)","2901366e":"liverate = pd.merge(liverate, final_corr[['feature', 'Corr']], on = 'feature', \n                    how = 'outer').sort_values('feature').reset_index().drop('index', axis=1)","7a573ce2":"liverate.sort_values('feature')","4aa29015":"# we have different ways to chose starting fetures from whole fetures list to test the difference \n# (there is almost no difference what features you start with, because we will reduce the number of them later)\ndf_nSurv = sorted(list(liverate.sort_values(['nSurv'], \n                                            ascending = False).head(20).feature) + ['Survived'])\ndf_Surv = sorted(list(liverate.sort_values(['Surv'], \n                                           ascending = False).head(20).feature) + ['Survived'])\ndf_count = sorted(list(liverate.sort_values(['count'], \n                                            ascending = False).head(20).feature) + ['Survived'])\ndf_corr = sorted(list(liverate.sort_values(['Corr'], ascending = False).head(21).feature))\n\ndf_mix = sorted(list(set(pd.concat([liverate.sort_values(['nSurv'], ascending = False).head(10).feature, \n                    liverate.sort_values(['Surv'], ascending = False).head(25).feature,\n                        liverate.sort_values(['count'], ascending = False).head(7).feature,                                                \n                        liverate.sort_values(['Corr'], ascending = False).head(8).feature]))))\ndf_max = list(liverate.feature)\ndf_mix","7fa12336":"start = df_mix","312d0b55":"# we will put here best features after first models testing\n\ndf_next_try = ['Cabin_0',\n 'Embarked_3',\n 'Pclass_1',\n 'Pclass_2',\n 'Pclass_3',\n 'Sex_1',\n 'Sex_2',\n 'Title_1',\n 'Title_4',\n 'Title_5',\n 'age_grouped_2',\n 'age_grouped_4',\n 'f_size_1',\n 'f_size_3',\n 'is_alone_0'] + ['Survived']","8389a341":"start = df_next_try","3978295b":"bfull_test = bfull[start]","7453e772":"X_train0 = X_train = bfull_test[:891].drop('Survived', axis = 1).astype(int)\nX_test0 = bfull_test[891:].drop('Survived', axis = 1).astype(int)\ny_train0 = bfull_test[:891].Survived.astype(int)\n# y_test0 = pd.read_csv('test.csv').drop('PassengerId', axis  = 1)","f507da1e":"X_train, X_test, y_train, y_test = train_test_split(X_train0, y_train0, test_size=0.25, random_state=0)","f8616317":"X_train.describe()","6af842e1":"# sometimes we need to make some preprocessing on our data\n# some algorithms show better results\n\ncolumns = list(X_train.columns)\n\nNZ = preprocessing.Normalizer()\nX_NZ = pd.DataFrame(NZ.fit_transform(X_train), columns = columns)\nRS = preprocessing.RobustScaler()\nX_RS = pd.DataFrame(RS.fit_transform(X_train), columns = columns)\nMM = preprocessing.MinMaxScaler()\nX_MM = pd.DataFrame(MM.fit_transform(X_train), columns = columns)\nSS = preprocessing.StandardScaler()\nX_SS = pd.DataFrame(SS.fit_transform(X_train), columns = columns)","722f4a41":"plots = 5\nfig, ax = plt.subplots(ncols=plots, figsize=(20, 4))\n\ntitles  = ['Original', 'Normalizer', 'RobustScaler', 'MinMaxScaler', 'StandardScaler']\ndfs = [X_train, X_NZ, X_RS, X_MM, X_SS]\n\nfor i in range(plots):\n    ax[i].set_title(titles[i])\n    for col in columns:\n        sns.kdeplot(dfs[i][col], ax=ax[i], bw=0.15, legend = None)\n        ax[i].set_xlabel(None)","a8318fb4":"# we can use 2 steps pretraing for models\n# 1st step - random test for some model parameters, its good, because RandomSearch works musch faster then GridSearch\n\nrfc = RandomForestClassifier()\nn_estimators = [int(x) for x in np.linspace(start = 100, stop = 800, num = 10)]\nmax_features = ['log2', 'sqrt']\nmax_depth = [int(x) for x in np.linspace(start = 3, stop = 15, num = 15)]\nmin_samples_split = [int(x) for x in np.linspace(start = 2, stop = 50, num = 15)]\nmin_samples_leaf = [int(x) for x in np.linspace(start = 2, stop = 50, num = 15)]\nbootstrap = [True, False]\nparam_dist = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\nrs = RandomizedSearchCV(rfc, \n                        param_dist, \n                        n_iter = 200, \n                        cv = 3, \n                        verbose = 1, \n                        n_jobs=-1, \n                        random_state=0)\nrs.fit(X_train, y_train)","c9f7a533":"rs_df = pd.DataFrame(rs.cv_results_).sort_values('rank_test_score').reset_index(drop=True)\nrs_df = rs_df.drop([\n            'mean_fit_time', \n            'std_fit_time', \n            'mean_score_time',\n            'std_score_time', \n            'params', \n            'split0_test_score', \n            'split1_test_score', \n            'split2_test_score', \n            'std_test_score'],\n            axis=1)\nrs_df.sort_values('mean_test_score', ascending = False)","40f85447":"colors = ['g', 'c', 'y',  'm', 'r', 'b']\ncolumns = list(rs_df.columns)\ny_min = round(rs_df['mean_test_score'].min(), 3)\ny_max = round(rs_df['mean_test_score'].max(), 3)\nfig, axs = plt.subplots(ncols=3, nrows=2, figsize=(25,15), )\nsns.set(style=\"whitegrid\", font_scale = 2)\n# fig.set_size_inches(25,18)\n\nfor i,k in enumerate(columns[:6]):\n    sns.barplot(x=k, y=columns[6], data=rs_df, color=colors[i], ax = axs[i\/\/3,i%3])\n    axs[i\/\/3,i%3].set_ylim((y_min, y_max))\n    axs[i\/\/3,i%3].set_title(label = k[6:], weight='bold', fontsize = 20)\n    axs[i\/\/3,i%3].set_xlabel(None,fontsize = 18)\n    axs[i\/\/3,1].set_ylabel(None)\n    axs[i\/\/3,2].set_ylabel(None)\nplt.show()","ad60b8ae":"# now we can use some of the nest params in GridSearch to tune the model\n\nrfc_2 = RandomForestClassifier()\nn_estimators = [722]\nmax_features = ['log2']\nmax_depth = [6, 7, 12, 15]\nmin_samples_split = [8, 32]\nmin_samples_leaf = [2,3,4,5,6,7,8]\nbootstrap = [False]\nparam_grid = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\ngs = GridSearchCV(rfc_2, param_grid, cv = 3, verbose = 1, n_jobs=-1)\ngs.fit(X_train, y_train)\nrfc_3 = gs.best_estimator_\nbest_params = gs.best_params_\nbest_params","e79d601a":"model_rfc = RandomForestClassifier(**(best_params)).fit(X_train, y_train)\nY_pred_RF = model_rfc.predict(X_test)\nRF = round(metrics.accuracy_score(y_test, Y_pred_RF), 5)\nprint(RF)","7ea03fcf":"# im going to start algorithm factory:) want it try all algorithms and compair it\n# but im too lazy to do it, so this ive automatised it:)\ns_names = ['RFC', 'LR', 'ENCV', 'OMP', 'LDA', 'QDA', 'RC', 'KNC', 'SVC', 'GNB', 'PCT', 'LSVC', 'SGDC', 'DTC', \n           'MLPC', 'LGBM', 'CBC', 'RCCV', 'ABC', 'PAC', 'LRCV', 'ETC', 'LLCV', 'ETsC', 'GBC', 'HGBC', 'NSVC', \n           'LPG', 'BC', 'CNB', 'LrCV', 'LsCV', 'XGBC'] \n\n\nalg = [RandomForestClassifier(), LogisticRegression(), ElasticNetCV(), OrthogonalMatchingPursuit(), \n       LinearDiscriminantAnalysis(), QuadraticDiscriminantAnalysis(), RidgeClassifier(), KNeighborsClassifier(),\n       SVC(), GaussianNB(), Perceptron(), LinearSVC(), SGDClassifier(), DecisionTreeClassifier(), MLPClassifier(),\n       LGBMClassifier(), CatBoostClassifier(verbose=False), RidgeClassifierCV(), AdaBoostClassifier(),\n       PassiveAggressiveClassifier(), LogisticRegressionCV(), ExtraTreeClassifier(), LassoLarsCV(), \n       ExtraTreesClassifier(), GradientBoostingClassifier(), HistGradientBoostingClassifier(), NuSVC(), \n       LabelPropagation(), BaggingClassifier(), ComplementNB(), LarsCV(), LassoCV(), XGBClassifier()]\n\nalgs = pd.DataFrame(columns = ['name', 'algorithm', 's_name', \n                               'acc', 'pres_1', 'pres_2', 'rec_1', 'rec_2', 'fsc_1', 'fsc_2', 'matrix', 'nS_S', 'model'])\n\nalgs['algorithm'] = alg\nalgs['s_name'] = s_names\n\n\ndef add_column(col_name):\n    list_temp = []\n    for i in range(len(alg)):\n        list_temp.append({})\n    algs[col_name] = list_temp\n    \n    \n# we can use those params for Random or Grid testing in aotmat mode\nparam_grid =  {\n                'AdaBoostClassifier': {},\n                'BaggingClassifier': {},\n                'CatBoostClassifier': {},\n                'ComplementNB': {},\n                'DecisionTreeClassifier': {},\n                'ElasticNetCV': {},\n                'ExtraTreeClassifier': {},\n                'ExtraTreesClassifier': {},\n                'GaussianNB': {},\n                'GradientBoostingClassifier': {},\n                'HistGradientBoostingClassifier': {},\n                'KNeighborsClassifier': {},\n                'LGBMClassifier': \n                    {\n                        'iterations': [10, 20, 50, 100, 300, 500],\n                        'learning_rate': [0.01, 0.05, 0.1],\n                        'depth': [5, 7, 9, 11],\n                        'l2_leaf_reg': [1, 3, 5, 7, 9]\n                    },\n                'LabelPropagation': {},\n                'LarsCV': {},\n                'LassoCV': {},\n                'LassoLarsCV': {},\n                'LinearDiscriminantAnalysis': {},\n                'LinearSVC': {},\n                'LogisticRegression': {},\n                'LogisticRegressionCV': {},\n                'MLPClassifier': \n                    {\n                        'n_estimators': [25, 50, 100],\n                        'learning_rate': [0.01, 0.05, 0.1],\n                        'num_leaves': [7, 15, 31]\n                    },\n                'NuSVC': {},\n                'OrthogonalMatchingPursuit': {},\n                'PassiveAggressiveClassifier': {},\n                'Perceptron': {},\n                'QuadraticDiscriminantAnalysis': {},\n                'RandomForestClassifier': \n                    {\n                        'n_estimators': [20, 50, 200, 500, 800],\n                        'max_features': ['log2', 'sqrt'],\n                        'max_depth': [5, 7, 10],\n                        'min_samples_split': [1, 5, 10, 20],\n                        'min_samples_leaf': [2,3,4,5],\n                        'bootstrap': [False, True]\n                    },\n                'RidgeClassifier': {},\n                'RidgeClassifierCV': {},\n                'SGDClassifier': {},\n                'SVC': {},\n                'XGBClassifier': {}\n            }\n\n# we can use those params for algorithms in automode\nbest_params =  {\n                'AdaBoostClassifier': {},\n                'BaggingClassifier': {},\n                'CatBoostClassifier': {},\n                'ComplementNB': {},\n                'DecisionTreeClassifier': {},\n                'ElasticNetCV': {},\n                'ExtraTreeClassifier': {},\n                'ExtraTreesClassifier': {},\n                'GaussianNB': {},\n                'GradientBoostingClassifier': {},\n                'HistGradientBoostingClassifier': {},\n                'KNeighborsClassifier': \n                    {\n                        'n_neighbors': 4, \n                        'weights': 'uniform', \n                        'algorithm': 'brute'\n                    },\n                'LGBMClassifier': \n                    {\n                        'random_state': 0,\n                        'learning_rate': 0.01,\n                        'num_leaves': 3,\n                        'n_estimators': 100,\n                        'class_weight': 'balanced',\n                        'n_jobs': -1\n                    },\n                'LabelPropagation': {},\n                'LarsCV': {},\n                'LassoCV': {},\n                'LassoLarsCV': {},\n                'LinearDiscriminantAnalysis': {},\n                'LinearSVC': {},\n                'LogisticRegression': \n                    {\n                        'random_state': 1, \n                        'solver': 'liblinear', \n                        'penalty': 'l1'\n                    },\n                'LogisticRegressionCV': \n                    {'cv': 5,\n                        'random_state': 0,\n                        'dual': False,\n                        'penalty': 'l2',\n                        'solver': 'lbfgs',\n                        'max_iter': 100,\n                        'n_jobs': -1\n                    },\n                'MLPClassifier': \n                    {\n                        'solver': 'lbfgs',\n                        'alpha': 1e-05,\n                        'hidden_layer_sizes': (4, 3),\n                        'random_state': 0,\n                        'max_iter': 10000,\n                        'learning_rate': 'adaptive'\n                    },\n                'NuSVC': {},\n                'OrthogonalMatchingPursuit': {},\n                'PassiveAggressiveClassifier': {},\n                'Perceptron': \n                    {\n                        'random_state': 0, \n                        'penalty': 'l2', \n                        'max_iter': 5000\n                    },\n                'QuadraticDiscriminantAnalysis': {},\n                'RandomForestClassifier': \n                    {\n                        'n_estimators': 20,\n                        'criterion': 'gini',\n                        'max_features': 'log2',\n                        'max_depth': 8,\n                        'min_samples_split': 3,\n                        'min_samples_leaf': 13,\n                        'bootstrap': True,\n                        'random_state': 0\n                    },\n                'RidgeClassifier': {},\n                'RidgeClassifierCV': {},\n                'SGDClassifier': {},\n                'SVC': {},\n                'XGBClassifier': \n                    {\n                        'random_state': 0,\n                        'learning_rate': 0.01,\n                        'max_depth': 2,\n                        'n_estimators': 20,\n                        'n_jobs': -1\n                    }\n                }\n  \nnames = []\nfor i in alg:\n    names.append(str(i))\n\nalgs['names'] = names\n# algs['names'] = algs['algorithm'].apply('str')\nalgs['name'] = algs['names'].str.extract('([A-Za-z]+)', expand = False)\nalgs[algs.name == 'catboost']\nalgs.loc[16, 'name'] = 'CatBoostClassifier'\nalgs = algs.sort_values('name').reset_index().drop(['names', 'index'], axis  =1)\nalgs['param_grid'] = dict.fromkeys(algs['name'], {})\nadd_column('best_params')\nadd_column('best_model')\nalgs['type'] = algs['algorithm'].apply(lambda x: 'classifier'if is_classifier(x) else 'regressor'if is_regressor(x) else 'NA')\nalgs['param_grid'] = algs['name'].apply(lambda x: param_grid[x])\nalgs['best_params'] = algs['name'].apply(lambda x: best_params[x])\n# algs.to_csv ('algorithms_sklearn.csv', index = False, encoding = 'utf-8')","8b73e940":"# rfc_2 = RandomForestClassifier()\n# n_estimators = [720,800]\n# max_features = ['log2']\n# max_depth = [5, 7, 10]\n# min_samples_split = [8, 22]\n# min_samples_leaf = [2,3,4,5]\n# bootstrap = [False]\n# param_grid = {'n_estimators': n_estimators,\n#                'max_features': max_features,\n#                'max_depth': max_depth,\n#                'min_samples_split': min_samples_split,\n#                'min_samples_leaf': min_samples_leaf,\n#                'bootstrap': bootstrap}\n# gs = GridSearchCV(rfc_2, param_grid, cv = 3, verbose = 1, n_jobs=-1)\n# gs.fit(X_train, y_train)\n# rfc_3 = gs.best_estimator_\n# gs.best_params_","b5e3671a":"# Algorithm factory here. All target scores saved to algs DataFrame\nbest_score = 0\nbest_model = ''\nfor i, alg in enumerate(algs.algorithm):\n    print(alg)\n#     param_grid = algs.loc[i, 'param_grid']\n#     param_grid = {}\n    best_params  = algs.loc[i, 'best_params']\n    model = alg.set_params(**best_params)\n#     try:\n#         model = GridSearchCV(model, param_grid, cv = 5, scoring='accuracy', verbose = 0, n_jobs=-1).fit(X_train, y_train)\n#     except:\n#         continue\n    try:\n        model.fit(X_train, y_train)\n    except:\n        continue\n    Y_pred = model.predict(X_test).round()\n    accuracy_score = metrics.accuracy_score(y_test, Y_pred).round(5)\n    f1_score = metrics.f1_score(y_test, Y_pred, average=None).round(5)\n    pr_re_fscore  = metrics.precision_recall_fscore_support(y_test, Y_pred, average=None)\n    confusion_matrix = metrics.confusion_matrix(y_test, Y_pred)\n    if i == 27:\n        RFC_score = accuracy_score\n        RFC_model = model\n    algs.at[i, 'acc'] = accuracy_score\n    algs.at[i, 'pres_1'] = pr_re_fscore[0][0].round(5)\n    algs.at[i, 'pres_2'] = pr_re_fscore[0][1].round(5)\n    algs.at[i, 'rec_1'] = pr_re_fscore[1][0].round(5)\n    algs.at[i, 'rec_2'] = pr_re_fscore[1][1].round(5)\n    algs.at[i, 'fsc_1'] = pr_re_fscore[2][0].round(5)\n    algs.at[i, 'fsc_2'] = pr_re_fscore[2][1].round(5)\n    algs.at[i, 'matrix'] = confusion_matrix.round(5)\n    algs.at[i, 'nS_S'] = pr_re_fscore[3].round(5)\n    algs.at[i, 'model'] = model\n\n    \n#     algs.loc[i, 'best_params'] = model.best_params_\n#     algs.loc[i, 'best_model'] = model.best_estimator_\n# algs.to_csv ('class.csv', columns =['name', 'algorithm', 's_name', 'score'], index=False, encoding = 'utf-8')    ","bebdb0a6":"# dataset with various csores from all algoriths finished\nalgs[['name', 's_name', 'acc', 'pres_1', 'pres_2', 'rec_1', 'rec_2', \n      'fsc_1', 'fsc_2', 'matrix', 'nS_S']].sort_values('acc', ascending = False)","c951edcd":"# now ill try to chose better combination of algorithm results to reach max accuracy","659a1be5":"# those give max acuracy (top4)\nt1 = algs[['name', 'algorithm', 'acc', 'rec_1', 'rec_2','fsc_1', 'fsc_2']][algs.type == 'classifier']\\\n                    .sort_values('acc', ascending = False)[0:4].reset_index().drop('index', axis  =1)\nt1","7f5bd065":"#max pressision not Survived (top1)\nt2 = algs[['name', 'algorithm', 'acc', 'pres_1', 'pres_2', 'rec_1', 'rec_2','fsc_1', 'fsc_2']][algs.type == 'classifier']\\\n                    .sort_values('pres_1', ascending = False)[0:1].reset_index().drop('index', axis  =1)\nt2","569b50ff":"#max pressision Survived (top1)\nt3 = algs[['name', 'algorithm', 'acc', 'pres_1', 'pres_2', 'rec_1', 'rec_2','fsc_1', 'fsc_2']][algs.type == 'classifier']\\\n                    .sort_values('pres_2', ascending = False)[0:1].reset_index().drop('index', axis  =1)\nt3","f897c650":"# max recall for not Suvived here (top1)\nt4 = algs[['name', 'algorithm', 'acc', 'pres_1', 'pres_2', 'rec_1', 'rec_2','fsc_1', 'fsc_2']][algs.type == 'classifier']\\\n                    .sort_values('rec_1', ascending = False)[0:1].reset_index().drop('index', axis  =1)\nt4","d627abdd":"# max recall for 'Suvived' here (top1)\nt5 = algs[['name', 'algorithm', 'acc', 'pres_1', 'pres_2', 'rec_1', 'rec_2','fsc_1', 'fsc_2']][algs.type == 'classifier']\\\n                    .sort_values('rec_2', ascending = False)[0:1].reset_index().drop('index', axis  =1)\nt5","098c8bef":"#F-score for not Survived\/Survived (top2 each)\ntf1  = algs[['name', 'algorithm', 'acc', 'pres_1', 'pres_2', 'rec_1', 'rec_2','fsc_1', 'fsc_2']][algs.type == 'classifier']\\\n                    .sort_values('fsc_1', ascending = False)[0:2].reset_index().drop('index', axis  =1)\ntf2  = algs[['name', 'algorithm', 'acc', 'pres_1', 'pres_2', 'rec_1', 'rec_2','fsc_1', 'fsc_2']][algs.type == 'classifier']\\\n                    .sort_values('fsc_2', ascending = False)[0:2].reset_index().drop('index', axis  =1)\n\ntf = pd.concat([tf1, tf2], axis=0).drop_duplicates('name')\ntf","c9547cd3":"#top TN\/TP scores (top1)\ntm1 = algs[['name', 'algorithm', 'acc', 'pres_1', 'pres_2', 'rec_1', 'rec_2','fsc_1', 'fsc_2', 'matrix']][algs.type == 'classifier']\\\n                    .sort_values('matrix', key = lambda x: pd.Series(y[0][0] for y in x), \n                                 ascending = False)[0:1].reset_index().drop('index', axis  =1)\ntm2 = algs[['name', 'algorithm', 'acc', 'pres_1', 'pres_2', 'rec_1', 'rec_2','fsc_1', 'fsc_2', 'matrix']][algs.type == 'classifier']\\\n                    .sort_values('matrix', key = lambda x: pd.Series(y[0][1] for y in x), \n                                 ascending = False)[0:1].reset_index().drop('index', axis  =1)\n\ntm = pd.concat([tm1, tm2], axis=0).drop_duplicates('name')\ntm","77180857":"t = pd.concat([t1, t2, t3, t4, t5], axis=0).drop_duplicates('name')\nt","3d9f2ca0":"# we can play with different algorithms we want to use them at finishing Voting algorithm\nt_full=pd.concat([t, tf, tm], axis=0).drop_duplicates('name').reset_index().drop('index', axis=1)\nt_full","e91cd825":"# t.drop([6], inplace=True)\n# t.drop([8], inplace=True)","0bd382f1":"# So finishing Voting algorithm\nestimators  = list(zip(t_full.name, t_full.algorithm))\neclf1 = VotingClassifier(estimators=estimators, \n                     voting='hard').fit(X_train0, y_train0)\nY_pred_VT1 = eclf1.predict(X_test0).round()\n# accuracy_score = metrics.accuracy_score(y_test, Y_pred_VT1).round(5)\n# f1_score = metrics.f1_score(y_test, Y_pred_VT1, average=None).round(5)\n# pr_re_fscore  = metrics.precision_recall_fscore_support(y_test, Y_pred_VT1, average=None)\n# confusion_matrix = metrics.confusion_matrix(y_test, Y_pred_VT1)\n# print('acc   : ' , accuracy_score)\n# print('pres_1: ' , pr_re_fscore[0][0].round(5))\n# print('pres_2: ' , pr_re_fscore[0][1].round(5))\n# print('rec_1 : ' , pr_re_fscore[1][0].round(5))\n# print('rec_2 : ' , pr_re_fscore[1][1].round(5))\n# print('fsc_1 : ' , pr_re_fscore[2][0].round(5))\n# print('fsc_1 : ' , pr_re_fscore[2][1].round(5))\n# print('matrix: ' , confusion_matrix[0])\n# print('        ' , confusion_matrix[1])\n# metrics.plot_confusion_matrix(eclf1, X_test0, y_test0)\n# test0 = eclf1.predict(X_test0).round()\n# print(metrics.accuracy_score(y_test0, test0).round(5))","66e70f7e":"# example of cross-validation\ncross_validate(eclf1, X_test, y_test, return_train_score=True, return_estimator=True, cv=2, n_jobs=-1)","a6794663":"imp = pd.DataFrame(RFC_model.feature_importances_.round(3), index=X_train0.columns, columns=['importance'])\nprint(imp.sort_values('importance', ascending = False).to_markdown())\nimp.sort_values('importance').plot(kind='barh', figsize=(10, 6), fontsize = 12)","412fab52":"best_feachers = sorted(list(imp.sort_values('importance', ascending = False)[0:15].index))","9a3cc033":"#lets take best 15 features for next try\nbest_feachers","54b61747":"submission = pd.DataFrame(columns = ['PassengerId', 'Survived'])","58994995":"for i in range(len(Y_pred_VT1)):\n    submission.loc[i, 'PassengerId'] = i+892\n    submission.loc[i, 'Survived'] = int(round(Y_pred_VT1[i]))\nsubmission = submission.astype(int)","ca587322":"submission[submission.Survived == 1].count()","445ae523":"submission.to_csv ('\/kaggle\/working\/submission.csv', columns =['PassengerId', 'Survived'], index=False, encoding = 'utf-8')","99e17d32":"# we can train different models separatelly to take a look how its work","9710384e":"model = LogisticRegression(random_state=1, solver='liblinear', penalty = 'l1').fit(X_train, y_train)\nY_pred_LR = model.predict(X_test).round()\nLR = round(metrics.accuracy_score(y_test, Y_pred_LR), 5)\nprint(LR)","7a60044a":"model = ElasticNetCV(cv=5, random_state=0).fit(X_train, y_train)\nY_pred_EN = model.predict(X_test).round(0)\nEN = round(metrics.accuracy_score(y_test, Y_pred_EN), 5)\nprint(EN)","6ad41c4d":"model = OrthogonalMatchingPursuit().fit(X_train, y_train)\nY_pred_OMP = model.predict(X_test).round(0)\nOMP = round(metrics.accuracy_score(y_test, Y_pred_OMP), 5)\nprint(OMP)","150c8976":"model = LinearDiscriminantAnalysis().fit(X_train, y_train)\nY_pred_LDA = model.predict(X_test)\nLDA = round(metrics.accuracy_score(y_test, Y_pred_LDA), 5)\nprint(LDA)","da2d0e43":"model = QuadraticDiscriminantAnalysis().fit(X_train, y_train)\nY_pred_QDA = model.predict(X_test)\nQDA = round(metrics.accuracy_score(y_test, Y_pred_QDA), 5)\nprint(QDA)","c20a54f0":"model = RidgeClassifier(random_state=0).fit(X_train, y_train)\nY_pred_RC = model.predict(X_test)\nRC = round(metrics.accuracy_score(y_test, Y_pred_RC), 5)\nprint(RC)","6b5276c6":"model = KNeighborsClassifier(n_neighbors = 4, weights = 'uniform', algorithm = 'brute').fit(X_train, y_train)\nY_pred_KNN = model.predict(X_test)\nKNN = round(metrics.accuracy_score(y_test, Y_pred_KNN), 5)\nprint(KNN)","45b3b399":"model = GaussianNB().fit(X_train, y_train)\nY_pred_GNB = model.predict(X_test)\nGNB = round(metrics.accuracy_score(y_test, Y_pred_GNB), 5)\nprint(GNB)","3cc732fd":"model = Perceptron(random_state=0, penalty = 'l2', max_iter = 5000).fit(X_train, y_train)\nY_pred_PCT = model.predict(X_test)\nPCT = round(metrics.accuracy_score(y_test, Y_pred_PCT), 5)\nprint(PCT)","0998a5b5":"model = LinearSVC(random_state=0, max_iter = 10000).fit(X_train, y_train)\nY_pred_LSVC = model.predict(X_test)\nLSVC = round(metrics.accuracy_score(y_test, Y_pred_LSVC), 5)\nprint(LSVC)","4ab67e0f":"model = SGDClassifier().fit(X_train, y_train)\nY_pred_SGD = model.predict(X_test)\nSGD = round(metrics.accuracy_score(y_test, Y_pred_SGD), 5)\nprint(SGD)","d33fb218":"model = DecisionTreeClassifier(max_depth = 3, min_samples_leaf = 1, min_samples_split = 2).fit(X_train, y_train)\nY_pred_DTC = model.predict(X_test)\nDTC = round(metrics.accuracy_score(y_test, Y_pred_DTC), 5)\nprint(DTC)","3a386507":"model = MLPClassifier(solver='lbfgs', alpha=1e-5, hidden_layer_sizes=(4, 3), \n                   random_state=0, max_iter = 10000, learning_rate = 'adaptive').fit(X_train, y_train)\nY_pred_MLPC = model.predict(X_test)\nMLPC = round(metrics.accuracy_score(y_test, Y_pred_MLPC), 5)\nprint(MLPC)","b606db19":"model = XGBClassifier(random_state=0, learning_rate = 0.01, \n                              max_depth = 2, n_estimators = 20, n_jobs=-1).fit(X_train, y_train)\nY_pred_XGBC = model.predict(X_test)\nXGBC = round(metrics.accuracy_score(y_test, Y_pred_XGBC), 5)\nprint(XGBC)","2123ba44":"model = LGBMClassifier(random_state=0, learning_rate = 0.01, num_leaves = 3, n_estimators = 100, \n                                 class_weight='balanced').fit(X_train, y_train)\nY_pred_LGBM = model.predict(X_test)\nLGBM = round(metrics.accuracy_score(y_test, Y_pred_LGBM), 5)\nprint(LGBM)","0ff5c096":"model = CatBoostClassifier(random_state=0, learning_rate = 0.01, l2_leaf_reg = 5,\n                                       depth = 5, iterations = 10, verbose=False).fit(X_train, y_train)\nY_pred_CBC = model.predict(X_test)\nCBC = round(metrics.accuracy_score(y_test, Y_pred_CBC), 5)\nprint(CBC)","15520ee2":"model = RidgeClassifierCV(cv=5).fit(X_train, y_train)\nY_pred_RCCV = model.predict(X_test)\nRCCV = round(metrics.accuracy_score(y_test, Y_pred_RCCV), 5)\nprint(RCCV)","4bb38e6f":"model = AdaBoostClassifier().fit(X_train, y_train)\nY_pred_ABC = model.predict(X_test)\nABC = round(metrics.accuracy_score(y_test, Y_pred_ABC), 5)\nprint(ABC)","ef279a51":"model = PassiveAggressiveClassifier().fit(X_train, y_train)\nY_pred_PAC = model.predict(X_test)\nPAC = round(metrics.accuracy_score(y_test, Y_pred_PAC), 5)\nprint(PAC)","55110acc":"model = LogisticRegressionCV().fit(X_train, y_train)\nY_pred_LRCV = model.predict(X_test)\nLRCV = round(metrics.accuracy_score(y_test, Y_pred_LRCV), 5)\nprint(LRCV)","ca2c48be":"model = ExtraTreeClassifier().fit(X_train, y_train)\nY_pred_ETC = model.predict(X_test)\nETC = round(metrics.accuracy_score(y_test, Y_pred_ETC), 5)\nprint(ETC)","b54c0999":"model = LogisticRegressionCV(cv=5, random_state=0, dual=False, \n                             penalty='l2', solver='lbfgs', max_iter=100, \n                             n_jobs=-1).fit(X_train, y_train)\nY_pred_LRCV = model.predict(X_test)\nLRCV = round(metrics.accuracy_score(y_test, Y_pred_LRCV), 5)\nprint(LRCV)","ba92f830":"model = LassoLarsCV().fit(X_train, y_train)\nY_pred_LLCV = model.predict(X_test).round()\nLLCV = round(metrics.accuracy_score(y_test, Y_pred_LLCV), 5)\nprint(LLCV)","6e492cc3":"model = HistGradientBoostingClassifier().fit(X_train, y_train)\nY_pred_HGBC = model.predict(X_test)\nHGBC = round(metrics.accuracy_score(y_test, Y_pred_HGBC), 5)\nprint(HGBC)","fd229d55":"model = ExtraTreesClassifier().fit(X_train, y_train)\nY_pred_ETC = model.predict(X_test)\nETC = round(metrics.accuracy_score(y_test, Y_pred_ETC), 5)\nprint(ETC)","15bcc340":"model = GradientBoostingClassifier().fit(X_train, y_train)\nY_pred_GBC = model.predict(X_test)\nGBC = round(metrics.accuracy_score(y_test, Y_pred_GBC), 5)\nprint(GBC)","bf8ff9c9":"model = NuSVC().fit(X_train, y_train)\nY_pred_NSVC = model.predict(X_test)\nNSVC = round(metrics.accuracy_score(y_test, Y_pred_NSVC), 5)\nprint(NSVC)","803a8407":"model = LabelPropagation().fit(X_train, y_train)\nY_pred_LP = model.predict(X_test)\nLP = round(metrics.accuracy_score(y_test, Y_pred_LP), 5)\nprint(LP)","8b92c74e":"<a id=\"9\">Fare<\/a>","a2f1d1b4":"<a id=\"15\">RandomizedSearch<\/a>","3890f531":"<a id=\"12\">SibSp Parch<\/a>","118115e8":"<a id=\"1\">Libraries import<\/a>","3b80fa21":"<a id=\"6\">Name<\/a>","6044c504":"<a id=\"8\">Cabin<\/a>","38a0c86e":"[set Next_try](#60)","042d9830":"# \u0425\u043e\u0447\u0443 \u043d\u0430\u043f\u0438\u0441\u0430\u0442\u044c \u043d\u0435\u0441\u043e\u043a\u043b\u044c\u043a\u043e \u0441\u043b\u043e\u0432 \u043e \u0446\u0435\u043b\u044f\u0445 \u0438 \u0437\u0430\u0434\u0430\u0447\u0430\u0445 \u044d\u0442\u043e\u0433\u043e \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0430..\n\n\u041a\u043e\u043d\u0435\u0447\u043d\u043e, \u043e\u0441\u043d\u043e\u0432\u043d\u0430\u044f \u0446\u0435\u043b\u044c \u0434\u043e\u0431\u0438\u0442\u044c\u0441\u044f \u043c\u0430\u043a\u0441\u0438\u043c\u0430\u043b\u044c\u043d\u043e\u0433\u043e \u0440\u0435\u0437\u0443\u043b\u044c\u0442\u0430\u0442\u0435 \u0432 \u044d\u0442\u043e\u043c \u0441\u043e\u0440\u0435\u0432\u043d\u043e\u0432\u0430\u043d\u0438\u0438, \u043d\u043e \u043d\u0435 \u0442\u043e\u043b\u044c\u043a\u043e.. \u044f \u0441\u0430\u043c \u043d\u0435 \u0442\u0430\u043a \u0434\u0430\u0432\u043d\u043e \u043d\u0430\u0447\u0430\u043b \u043a\u0430\u0440\u044c\u0435\u0440\u0443 DS\/ML \u0438 \u043f\u043e \u0445\u043e\u0434\u0443 \u0440\u0430\u0431\u043e\u0442\u044b \u0432\u043e\u0437\u043d\u0438\u043a\u043b\u043e \u043c\u043d\u043e\u0433\u043e \u0432\u043e\u043f\u0440\u043e\u0441\u043e\u0432. \u0412 \u044d\u0442\u043e\u043c \u043d\u043e\u0443\u0442\u0431\u0443\u043a\u0435 \u044f \u043f\u043e\u043f\u044b\u0442\u0430\u044e\u0441\u044c \u0434\u0430\u0442\u044c \u043e\u0442\u0432\u0435\u0442\u044b \u043d\u0430 \u0432\u043e\u043f\u0440\u043e\u0441\u044b \u043d\u043e\u0432\u0438\u0447\u043a\u043e\u0432, \u043a\u043e\u0442\u043e\u0440\u044b\u0435, \u043a\u0430\u043a \u043c\u043d\u0435 \u043a\u0430\u0436\u0435\u0442\u0441\u044f \u043e\u043d\u0438 \u043c\u043e\u0433\u0443\u0442 \u0437\u0430\u0434\u0430\u0442\u044c, \u043d\u0443 \u0438\u043b\u0438 \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u044f \u0441\u0430\u043c \u0431\u044b \u0437\u0430\u0434\u0430\u043b \u0432 \u0441\u0430\u043c\u043e\u043c \u043d\u0430\u0447\u0430\u043b\u0435..\n\u0418\u0442\u0430\u043a.. \u0438\u0442\u043e\u0433\u043e\u043c \u043f\u0440\u043e\u0432\u0435\u0434\u0435\u043d\u043d\u043e\u0439 \u0440\u0430\u0431\u043e\u0442\u044b \u0431\u0443\u0434\u0435\u0442 submission. \u043d\u043e \u0434\u0430\u0432\u0430\u0439\u0442\u0435 \u043f\u043e-\u043f\u043e\u0440\u044f\u0434\u043a\u0443 \u0440\u0430\u0437\u0431\u0438\u0440\u0430\u0442\u044c\u0441\u044f  , \u0447\u0442\u043e \u0434\u043b\u044f \u044d\u0442\u043e\u0433\u043e \u043d\u0443\u0436\u043d\u043e\n\u044f \u0431\u044b \u0443\u0441\u043b\u043e\u0432\u043d\u043e \u0440\u0430\u0437\u0431\u0438\u043b \u0432\u0435\u0441\u044c \u043f\u0440\u043e\u0446\u0435\u0441\u0441 \u043d\u0430 6 \u0447\u0430\u0441\u0442\u0435\u0439:\n1. \u0412\u043d\u0438\u043c\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u043f\u0440\u043e\u0447\u0438\u0442\u0430\u0442\u044c \u0443\u0441\u043b\u043e\u0432\u0438\u044f \u043a\u043e\u043d\u043a\u0443\u0440\u0441\u0430!!! \n\u0412\u0430\u0436\u043d\u043e \u043f\u043e\u043d\u044f\u0442\u044c, \u0447\u0442\u043e \u043d\u0443\u0436\u043d\u043e \u0441\u0434\u0435\u043b\u0430\u0442\u044c, \u0438 \u0441\u0430\u043c\u043e\u0435 \u0433\u043b\u0430\u0432\u043d\u043e\u0435 \u043a\u0430\u043a \u044d\u0442\u043e \u0431\u0443\u0434\u0435\u0442 \u043e\u0446\u0435\u043d\u0438\u0432\u0430\u0442\u044c\u0441\u044f!\n2. \u0417\u0430\u0433\u0440\u0443\u0437\u0438\u0442\u044c \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435: \n\u0412\u043d\u0438\u043c\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u0438\u0437\u0443\u0447\u0438\u0442\u044c \u0438\u0445, \u043f\u0440\u043e\u0432\u0435\u0440\u0438\u0442\u044c \u043d\u0430 \u043f\u043e\u043b\u043d\u043e\u0442\u0443, \u0432\u0430\u043b\u0438\u0434\u043d\u043e\u0441\u0442\u044c. \u0412\u0430\u043c \u0434\u043e\u043b\u0436\u0435\u043d \u0431\u044b\u0442\u044c \u043f\u043e\u043d\u044f\u0442\u0435\u043d \u0441\u043c\u044b\u0441\u043b \u044d\u0442\u0438\u0445 \u0434\u0430\u043d\u043d\u044b\u0445. \u0415\u0441\u043b\u0438 \u043d\u0435 \u043f\u043e\u043d\u044f\u0442\u0435\u043d, \u0440\u0435\u0448\u0430\u0439\u0442\u0435 \u044d\u0442\u0443 \u043f\u0440\u043e\u0431\u043b\u0435\u043c\u0443 - \u0438\u0449\u0438\u0442\u0435 \u0432 \u0438\u043d\u0442\u0435\u0440\u0435\u043d\u0435\u0442\u0435, \u0441\u043f\u0440\u0430\u0448\u0438\u0432\u0430\u0439\u0442\u0435 \u043e\u0440\u0433\u0430\u043d\u0438\u0437\u0430\u0442\u043e\u0440\u043e\u0432, \u0441\u043f\u0440\u0430\u0448\u0438\u0432\u0430\u0439\u0442\u0435 \u0443 \u043a\u043e\u043b\u043b\u0435\u0433 \u043d\u0430 kaggle. \u0421\u043c\u044b\u0441\u043b \u0432 \u0442\u043e\u043c, \u0447\u0442\u043e \u043d\u0435 \u0432\u043e\u0437\u043c\u043e\u0436\u043d\u043e \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u0445\u043e\u0440\u043e\u0448\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c, \u0435\u0441\u043b\u0438 \u0432\u044b \u043d\u0435 \u043f\u043e\u043d\u0438\u043c\u0430\u0435\u0442\u0435 \u043b\u043e\u0433\u0438\u043a\u0443 \u043f\u0440\u043e\u0446\u0435\u0441\u0441\u0430 \u0438\u043b\u0438 \u043d\u0430\u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 \u0442\u0435\u0445 \u0438\u043b\u0438 \u0438\u043d\u044b\u0445 \u0434\u0430\u043d\u043d\u044b\u0445.\n3. \u041e\u0431\u0440\u0430\u0431\u043e\u0442\u043a\u0430 \u0438 \u043f\u043e\u0434\u0433\u043e\u0442\u043e\u0432\u043a\u0430 \u0434\u0430\u043d\u043d\u044b\u0445:\n\u041d\u0443\u0436\u043d\u043e \u0443\u0436\u0435 \u043e\u0447\u0435\u043d\u044c \u0432\u043d\u0438\u043c\u0430\u0442\u0435\u043b\u044c\u043d\u043e \u0438\u0437\u0443\u0447\u0438\u0442\u044c \u0434\u0430\u043d\u043d\u044b\u0435, \u0432\u044b\u0431\u0440\u0430\u0442\u044c \u0442\u0435 \u043f\u043e\u043b\u044f, \u043a\u043e\u0442\u043e\u0440\u044b\u0435 \u0432\u044b \u043f\u0440\u0435\u0434\u043f\u043e\u043b\u0430\u0433\u0430\u0435\u0442\u0435 \u0438\u0441\u043f\u043e\u043b\u044c\u0437\u043e\u0432\u0430\u0442\u044c \u0432 \u043c\u043e\u0434\u0435\u043b\u0438, \u0438 \u0438\u0437\u0431\u0430\u0432\u0438\u0442\u044c\u0441\u044f \u043e\u0442 \u0442\u0435\u0445, \u043a\u043e\u0442\u043e\u0440\u044b\u0435, \u043a\u0430\u043a \u0432\u0430\u043c \u043a\u0430\u0436\u0435\u0442\u0441\u044f, \u043d\u0435 \u043d\u0443\u0436\u043d\u044b (\u0432\u0441\u0435\u0433\u0434\u0430 \u043c\u043e\u0436\u043d\u043e \u0431\u0443\u0434\u0435\u0442 \u044d\u0442\u043e \u043e\u0442\u044b\u0433\u0440\u0430\u0442\u044c \u043d\u0430\u0437\u0430\u0434).\n\u041f\u0440\u0438\u0432\u0435\u0434\u0438\u0442\u0435 \u0441\u0430\u043c\u0438 \u0434\u0430\u043d\u043d\u044b\u0435 \u0432 \u043f\u043e\u0440\u044f\u0434\u043e\u043a  - \u0443\u0434\u0430\u043b\u0438\u0442\u0435 \u0438\u043b\u0438 \u0437\u0430\u043f\u043e\u043b\u043d\u0438\u0442\u0435 \u043f\u0440\u043e\u043f\u0443\u0449\u0435\u043d\u043d\u044b\u0435 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u044f, \u043f\u0440\u0438\u0432\u0435\u0434\u0438\u0442\u0435 \u0432\u0441\u0435 \u043a \u043e\u0434\u043d\u043e\u043c\u0443 \u0432\u0438\u0434\u0443. \u0412\u0441\u0435 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0430\u043b\u044c\u043d\u044b\u0435 \u0434\u0430\u043d\u043d\u044b\u0435 \u043d\u0443\u0436\u043d\u043e \u043f\u0440\u0438\u0432\u0435\u0441\u0442\u0438 \u043a \u0447\u0438\u0441\u043b\u043e\u0432\u043e\u043c\u0443 \u0432\u0438\u0434\u0443 - \u043d\u0430\u043f\u0440\u0438\u043c\u0435\u0440 \u0438\u043c\u0435\u0435\u043c A, B, C - \u0434\u043e\u043b\u0436\u043d\u043e \u0431\u044b\u0442\u044c 0,1,2 \u0438\u043b\u0438 1,2,3 \u0438\u043b\u0438 male\/female \u043f\u0440\u0435\u0432\u0440\u0430\u0449\u0430\u0435\u0442\u0441\u044f \u0432 1\/2.. \u043d\u0443 \u0438 \u0442\u0430\u043a \u0434\u0430\u043b\u0435\u0435.\n4. \u0412\u044b\u044f\u0432\u043b\u0435\u043d\u0438\u0435 \u0444\u0438\u0447\u0435\u0439:\n\u0434\u0430\u043b\u044c\u0448\u0435 \u043d\u0430\u0447\u0438\u043d\u0435\u0442\u0441\u044f \u0442\u0432\u043e\u0440\u0447\u0441\u0435\u043a\u0438\u0439 \u043f\u0440\u043e\u0446\u0435\u0441\u0441:)\n\u0434\u0430\u043b\u044c\u0448\u0435 \u0431\u0443\u0434\u0435\u043c \u0440\u0430\u0441\u0441\u043c\u0430\u0442\u0440\u0438\u0432\u0430\u0442\u044c \u043a\u0430\u043a \u0438\u0437, \u043a\u0430\u0437\u0430\u043b\u043e\u0441\u044c \u0431\u044b \u0441\u043e\u0432\u0435\u0440\u0448\u0435\u043d\u043d\u043e \u0431\u0435\u0441\u043f\u043e\u043b\u0435\u0437\u043d\u043e\u0439 \u043d\u0430 \u043f\u0435\u0440\u0432\u044b\u0439 \u0432\u0437\u0433\u043b\u044f\u0434 \u0438\u043d\u0444\u043e\u0440\u043c\u0430\u0446\u0438\u0438, \u043c\u043e\u0436\u043d\u043e \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c \u0446\u0435\u043d\u043d\u0443\u044e \u0444\u0438\u0447\u0443\n5. \u0415\u0441\u0442\u044c \u043d\u0430\u0431\u043e\u0440 \u0444\u0438\u0447\u0435\u0439 \u0434\u043b\u044f \u043f\u0435\u0440\u0432\u043e\u0433\u043e \u043f\u0440\u043e\u0433\u043e\u043d\u0430, \u043c\u043e\u0436\u043d\u043e \u0441\u0442\u0430\u0440\u0442\u043e\u0432\u0430\u0442\u044c..\n6. \u041e\u0442\u043b\u0430\u0434\u043a\u0430 \u0438 \u043a\u043e\u043b\u0438\u0431\u0440\u043e\u0432\u043a\u0430 \u043c\u043e\u0434\u0435\u043b\u0435\u0439 - \u0435\u0449\u0435 \u043e\u0434\u0438\u043d \u0442\u0432\u043e\u0440\u0447\u0435\u0441\u043a\u0438\u0439 \u043f\u0440\u043e\u0446\u0435\u0441\u0441:)) \u0445\u043e\u0442\u044f \u0438 \u0432 \u0431\u043e\u043b\u044c\u0448\u0435\u0439 \u0441\u0442\u0435\u043f\u0435\u043d\u0438 \u0444\u043e\u0440\u043c\u0430\u043b\u0438\u0437\u043e\u0432\u0430\u043d \u0438 \u0430\u0432\u0442\u043e\u043c\u0430\u0442\u0438\u0437\u0438\u0440\u043e\u0432\u0430\u043d, \u0447\u0435\u043c \u0432\u044b\u0431\u043e\u0440 \u0444\u0438\u0447\u0435\u0439:)\n\n\u0422\u0430\u043a \u0436\u0435 \u0441\u0440\u0430\u0437\u0443 \u0445\u043e\u0447\u0443 \u0441\u043a\u0430\u0437\u0430\u0442\u044c, \u0447\u0442\u043e \u0432\u044b \u043d\u0435 \u043d\u0430\u0439\u0434\u0435\u0442\u0435 \u0432 \u044d\u0442\u043e\u0439 \u0440\u0430\u0431\u043e\u0442\u0435 \u043c\u0435\u0433\u0430 \u0441\u0443\u043f\u0435\u0440 \u043a\u0440\u0443\u0442\u043e\u0439 \u043c\u043e\u0434\u0435\u043b\u0438, \u043a\u043e\u0442\u043e\u0440\u0430\u044f \u0432\u044b\u0434\u0430\u0435\u0442 100500 % accuracy.\n\u041c\u0430\u043a\u0441\u0438\u043c\u0443\u043c, \u043c\u043d\u0435 \u0443\u0434\u0430\u043b\u043e\u0441\u044c \u043f\u043e\u043b\u0443\u0447\u0438\u0442\u044c 0.80861, \u0438 \u0442\u043e 1 \u0440\u0430\u0437. \u0410 \u0442\u0430\u043a \u0432\u044b\u0434\u0430\u0435\u0442 \u0432 \u0441\u0440\u0435\u0434\u043d\u0435\u043c 79.5 - 80.5 \u0435\u0441\u043b\u0438 \u043f\u043e\u0438\u0433\u0440\u0430\u0442\u044c\u0441\u044f \u043d\u0430\u0441\u0442\u0440\u043e\u0439\u043a\u0430\u043c\u0438.","f30a4c63":"\"..Mrs Stone boarded the Titanic in Southampton on 10 April 1912 and was travelling in first class with her maid Amelie Icard. She occupied cabin B-28..\"\n\nhttps:\/\/www.encyclopedia-titanica.org\/titanic-survivor\/martha-evelyn-stone.html","86f57002":"<a id=\"13\">Features choosing import<\/a>","8db277c2":"<a id=\"60\">Next try array<\/a>","526fcf35":"# I want to write a few words about the goals and objectives of this notebook..\n\nof Course, the main goal is to achieve the maximum result in this competition, but not only.. I started my DS \/ ML career not so long ago and a lot of questions arise in the course of my work. In this notebook, I will try to give answers to the questions of beginners that I think they can ask, or that I myself would have asked at the very beginning..\nSo.. the result of this work will be submission. but let's understand in order what is needed for this\nI would conditionally split the whole process into 6 parts:\n1. Carefully read the terms of the contest!!!\nIt is important to understand what needs to be done, and most importantly how it will be evaluated!\n\n2. Upload the submitted data:\ncarefully study it, check for completeness and validity. You should understand the meaning of this data. If it is not clear, then look for an opportunity to solve this problem-search on the Internet, ask the organizers, ask your colleagues on kaggle. The point is that it is not possible to build a good model if you do not understand the logic of the process or the purpose of certain data.\n\n3. Processing and preparing data:\nYou need to study the data very carefully, select the fields that you intend to use in the model, and get rid of those that you don't think you need (you can always play this back).\nPut the data itself in order-delete or fill in the missing values, bring everything to the same view. All categorical data must be converted to a numeric form - for example, we have A, B, C-it must be 0,1,2 or 1,2,3, or male\/female turns into 1\/2.. and so on.\n\n4. Identifying features:\nthen the creative process begins:)\nnext, we will consider how you can get a valuable feature from information that would seem completely useless at first glance.\n\n5. there Is a set of features for the first run, you can start..\n\n6. Debugging and calibrating models - another one creative process:)) although it is more formalized and automated than the selection of features:)\n\nI also want to say right away that you will not find in this work a mega super cool model that gives 100500% accuracy.\nMaximum, I managed to get 0.80861, and it was just once. And so it gives an average of 0.795-0.805 if you play around with the settings.","83c9f973":"<a id=\"17\">Algorithms combine<\/a>","92a9b261":"<a id=\"19\">Features importance<\/a>","d3d9b6b6":"<a id=\"0\">Navigation<\/a>\n\n* [1. Libraries import](#1)\n* [2. Data loading](#2)\n* [3. Vizualization](#3)\n* [4. Features preprocessing](#4)\n    * [Age](#5)\n    * [Name](#6)\n    * [Ticket](#7)\n    * [Cabin](#8)\n    * [Fare](#9)\n    * [Sex](#10)\n    * [Embarked](#11)\n    * [SibSp, Parch](#12)\n* [5. Features choosing](#13)\n* [6. Normalization](#14)\n* [7. RandomizedSearch](#15)\n* [8. GridSearch](#16)\n* [9. Algorithms combine](#17)\n* [10. Final result voting](#18)\n* [11. Features importance](#19)","e879fb45":"<a id=\"2\">Data loading<\/a>","0060c61f":"<a id=\"3\">Vizualization<\/a>","d68303f7":"<a id=\"5\">Age<\/a>","86ac63c1":"[Begin](#0)","498c1a92":"[Voting](#18)","b8cddc90":"<a id=\"7\">Ticket<\/a>","a322b164":"<a id=\"11\">Embarked<\/a>","6b0473e9":"<a id=\"18\">Final result voting<\/a>","44476746":"<a id=\"16\">GridSearch<\/a>","49140328":"<a id=\"14\">Normalization<\/a>","1ab23d70":"<a id=\"10\">Sex<\/a>"}}