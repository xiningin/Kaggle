{"cell_type":{"7a412389":"code","5bcc5ed8":"code","30d44073":"code","9d8dd148":"code","1872cc9a":"code","15842578":"code","e879af69":"code","d2e01418":"code","3bf033fe":"code","066e46ac":"code","512c940a":"code","b4243ad0":"code","ea98c92e":"code","cbb88f2c":"code","4647cda7":"code","859c3ee1":"code","8fd0ec01":"code","82e04b8a":"code","c13f1fac":"code","b2f85b60":"code","2c94cf69":"code","a894c097":"markdown","c1709e61":"markdown","f3e641ff":"markdown","9a56cba9":"markdown","61e0d3fa":"markdown","a0edd39a":"markdown"},"source":{"7a412389":"import numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom matplotlib import pyplot as plt\nimport os\nimport math","5bcc5ed8":"'''Pulling the photos from folders with their paths'''\n\npath_cats = []\ntrain_path_cats = \"..\/input\/cat-and-dog\/training_set\/training_set\/cats\/\"\nfor path in os.listdir(train_path_cats):\n    if '.jpg' in path:\n        path_cats.append(os.path.join(train_path_cats, path))\npath_dogs = []\ntrain_path_dogs = \"..\/input\/cat-and-dog\/training_set\/training_set\/dogs\/\"\nfor path in os.listdir(train_path_dogs):\n    if '.jpg' in path:\n        path_dogs.append(os.path.join(train_path_dogs, path))\nlen(path_dogs), len(path_cats)","30d44073":"'''Load training set'''\n\n'''total pics in training set =  8005\n    training_set = 6000 --- first 3000 of dogs and rest of cats\n    validation_set = 2000 --- first 1000 of dogs and rest of cats'''\n\ntrain_set_orig = np.zeros((6000, 64, 64, 3), dtype='float32')\nfor i in range(6000):\n    if i < 3000:\n        image = Image.open(path_dogs[i])\n        img_resized = image.resize((64,64))\n        train_set_orig[i] = np.asarray(img_resized)\n    else:\n        image = Image.open(path_cats[i - 3000])\n        img_resized = image.resize((64,64))\n        train_set_orig[i] = np.asarray(img_resized)","9d8dd148":"'''Load validation set'''\n\nval_set_orig = np.zeros((2000, 64, 64, 3), dtype='float32')\nfor i in range(2000):\n    if i < 1000:\n        image = Image.open(path_dogs[i + 3000])\n        img_resized = image.resize((64,64))\n        val_set_orig[i] = np.asarray(img_resized)\n    else:\n        image = Image.open(path_cats[i + 2000])\n        img_resized = image.resize((64,64))\n        val_set_orig[i] = np.asarray(img_resized)","1872cc9a":"'''Labelling the training and validation set'''\n\ny_train_ = np.zeros((3000, 1))\ny_train_ = np.concatenate((y_train_, np.ones((3000, 1))))\ny_val = np.zeros((1000, 1))\ny_val = np.concatenate((y_val, np.ones((1000, 1))))\nprint(\"Training set labels\" +str(y_train_.shape)+ \"  validation set labels\" + str(y_val.shape))","15842578":"'''Suffling training set pics'''\n\nr = np.arange(train_set_orig.shape[0])\nnp.random.shuffle(r)\ntrain_set_x_orig = train_set_orig[r, :]\ny_train = y_train_[r, :]","e879af69":"'''Example of a image'''\n\nindex = 0\nplt.imshow(np.uint8(train_set_x_orig[index]), interpolation='nearest')\nplt.show()\nprint(y_train[index])","d2e01418":"m_train = train_set_x_orig.shape[0]\nm_val = val_set_orig.shape[0]\nnum_px = train_set_x_orig.shape[1]\n\nprint (\"Number of training examples: m_train = \" + str(m_train))\nprint (\"Number of validation examples: m_val = \" + str(m_val))\nprint (\"Height\/Width of each image: num_px = \" + str(num_px))\nprint (\"Each image is of size: (\" + str(num_px) + \", \" + str(num_px) + \", 3)\")\nprint (\"train_set_x shape: \" + str(train_set_x_orig.shape))\nprint (\"train_set_y shape: \" + str(y_train.shape))\nprint (\"val_set_x shape: \" + str(val_set_orig.shape))\nprint (\"val_set_y shape: \" + str(y_val.shape))","3bf033fe":"'''Reshape the training and validation examples'''\n\nx_train = train_set_x_orig.reshape(6000,-1)\nx_val = val_set_orig.reshape(2000,-1)\n\nprint (\"train_set_x_flatten shape: \" + str(x_train.shape))\nprint (\"train_set_y shape: \" + str(y_train.shape))\nprint (\"val_set_x_flatten shape: \" + str(x_val.shape))\nprint (\"val_set_y shape: \" + str(y_val.shape))","066e46ac":"'''Standardizing dataset'''\n\nx_train = x_train \/255\nx_val = x_val\/255","512c940a":"def sigmoid(z):\n    \n    '''Compute the sigmoid of z\n    Arguments:\n    z -- A scalar or numpy array of any size.\n    Return:\n    s -- sigmoid(z)'''\n    \n    s = 1\/(1 + np.exp(-z))\n    return s","b4243ad0":"def initialize_with_zeros(dim):\n    \n    '''This function creates a vector of zeros of shape (dim, 1) for w and initializes b t\n    o 0.\n    Argument:\n    dim -- size of the w vector we want (or number of parameters in this case)\n    Returns:\n    w -- initialized vector of shape (dim, 1)\n    b -- initialized scalar (corresponds to the bias)'''\n    \n    w = np.zeros((dim, 1))\n    b = 0\n    return w, b","ea98c92e":"def random_mini_batches(X, Y, mini_batch_size, seed = 0):\n    \n    '''Creates a list of random minibatches from (X, Y)\n    Arguments:\n    X -- input data, of shape (number of examples, input size)\n    Y -- true \"label\" vector (1 for cat \/ 0 for dog), of shape (number of examples, 1)\n    mini_batch_size -- size of the mini-batches, integer\n    Returns:\n    mini_batches -- list of synchronous (mini_batch_X, mini_batch_Y)'''\n    \n    np.random.seed(seed)            \n    m = X.shape[0]                  # number of training examples\n    mini_batches = []\n        \n    # Shuffle (X, Y)\n    \n    permutation = list(np.random.permutation(m))\n    shuffled_X = X[permutation, :]\n    shuffled_Y = Y[permutation, :]\n\n    # Partition (shuffled_X, shuffled_Y). Minus the end case.\n    \n    num_complete_minibatches = math.floor(m\/mini_batch_size) # number of mini batches of size mini_batch_size in your partitionning\n    for k in range(0, num_complete_minibatches):\n        mini_batch_X = shuffled_X[k*mini_batch_size:(k+1)*mini_batch_size, :]\n        mini_batch_Y = shuffled_Y[k*mini_batch_size:(k+1)*mini_batch_size, :]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    # Handling the end case (last mini-batch < mini_batch_size)\n    if m % mini_batch_size != 0:\n        mini_batch_X = shuffled_X[mini_batch_size * num_complete_minibatches::, :]\n        mini_batch_Y = shuffled_Y[mini_batch_size * num_complete_minibatches::, :]\n        mini_batch = (mini_batch_X, mini_batch_Y)\n        mini_batches.append(mini_batch)\n    \n    return mini_batches","cbb88f2c":"def propagate(w, b, X, Y):\n    \n    '''Implement the cost function and its gradient for the propagation explained above\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (number of examples, num_px * num_px * 3)\n    Y -- true \"label\" vector (containing 0 if dog, 1 if cat) of size (number of examples, 1)\n    Return:\n    cost -- negative log-likelihood cost for logistic regression\n    dw -- gradient of the loss with respect to w, thus same shape as w\n    db -- gradient of the loss with respect to b, thus same shape as b'''\n    \n    m = X.shape[0]\n    \n    '''FORWARD PROPAGATION (FROM X TO COST)'''\n\n    A = (sigmoid(np.dot(w.T, X.T) + b)).T\n    cost = -(np.sum(Y*np.log(A) + (1 - Y)*np.log(1 - A)))\/m\n    \n    '''BACKWARD PROPAGATION (TO FIND GRAD)'''\n\n    dw = np.dot(X.T, (A-Y))\/m\n    db = np.sum(A-Y)\/m\n    \n    cost = np.squeeze(cost)\n    grads = {\"dw\": dw,\n             \"db\": db}\n    return grads, cost","4647cda7":"def optimize(w, b, X, Y, num_epochs, learning_rate, mini_batch_size, print_cost = False):\n    \n    '''This function optimizes w and b by running a mini-batch gradient descent algorithm\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of shape (number of examples, num_px * num_px * 3)\n    Y -- true \"label\" vector (containing 0 if dog, 1 if cat), of shape (number of examples, 1)\n    num_epochs -- number of epochs of the optimization loop\n    mini_batch_size --- number of training examples in one batch\n    learning_rate -- learning rate of the gradient descent update rule\n    print_cost -- True to print the loss every 10 steps\n    Returns:\n    params -- dictionary containing the weights w and bias b\n    grads -- dictionary containing the gradients of the weights and bias with respect to the cost function\n    costs -- list of the costs computed during the optimization, this will be used to plot the learning curve.'''\n    seed = 0\n    costs = []\n    for i in range(num_epochs):\n        \n        # Define the random minibatches. We increment the seed to reshuffle differently the dataset after each epoch\n        \n        seed = seed + 1\n        minibatches = random_mini_batches(X, Y, mini_batch_size, seed)\n        m = len(minibatches)\n        cost_total = 0                                                # total cost for one epoch\n        \n        for minibatch in minibatches:\n            (minibatch_X, minibatch_Y) = minibatch\n            grads, cost = propagate(w, b, minibatch_X, minibatch_Y)\n            cost_total = cost_total + cost\n            dw = grads[\"dw\"]\n            db = grads[\"db\"]\n            w = w - learning_rate*dw\n            b = b - learning_rate*db\n        cost_avg = cost_total \/ m                                    # average cost for one epoch\n        if i % 10 == 0:\n            costs.append(cost_avg)\n        if print_cost and i % 10 == 0:\n            print (\"Cost after epochs %i: %f\" %(i, cost_avg))\n    params = {\"w\": w,\n              \"b\": b}\n    grads = {\"dw\": dw,\n             \"db\": db}\n    return params,grads, costs","859c3ee1":"def predict(w, b, X):\n    \n    '''Predict whether the label is 0 or 1 using learned logistic regression parameters (w, b)\n    Arguments:\n    w -- weights, a numpy array of size (num_px * num_px * 3, 1)\n    b -- bias, a scalar\n    X -- data of size (number of examples, num_px * num_px * 3)\n    Returns:\n    Y_prediction -- a numpy array (vector) containing all predictions (0\/1) for the examples in X'''\n    \n    m = X.shape[0]\n    Y_prediction = np.zeros((m,1))\n    w = w.reshape(X.shape[1], 1)\n    A = sigmoid(np.dot(w.T,X.T) + b)\n    for i in range(A.shape[1]):\n        if A[0, i]>=0.5:\n            Y_prediction[i, 0] = 1\n        else:\n            Y_prediction[i, 0] = 0\n    return Y_prediction","8fd0ec01":"def model(X_train, Y_train, X_val, Y_val, num_epochs = 100, learning_rate = 0.0006, mini_batch_size = 32, print_cost = False ):\n    \n    '''Builds the logistic regression model by calling the function you've implemented previously\n    Arguments:\n    X_train -- training set represented by a numpy array of shape ( m_train, num_px * num_px * 3)\n    Y_train -- training labels represented by a numpy array (vector) of shape (m_train, 1)\n    X_val -- test set represented by a numpy array of shape (m_test, num_px * num_px * 3)\n    Y_val -- test labels represented by a numpy array (vector) of shape (m_test, 1)\n    num_epochs -- number of epochs of the optimization loop\n    learning_rate -- hyperparameter representing the learning rate used in the update rule of optimize()\n    mini_batch_size --- number of training examples in one batch\n    print_cost -- Set to true to print the cost every 10 iterations\n    Returns:\n    d -- dictionary containing information about the model.'''\n    \n    w, b = initialize_with_zeros(np.size(X_train,1))\n    \n    parameters, grads, costs = optimize(w, b, X_train, Y_train, num_epochs, learning_rate, mini_batch_size, print_cost)\n    \n    w = parameters[\"w\"]\n    b = parameters[\"b\"]\n    \n    Y_prediction_val = predict(w, b, X_val)\n    Y_prediction_train = predict(w, b, X_train)\n    \n    print(\"train set accuracy for model: {} %\".format(100 - np.mean(np.abs(Y_prediction_train - Y_train)) * 100))\n    print(\"validation set accuracy for model: {} %\".format(100 - np.mean(np.abs(Y_prediction_val - Y_val)) * 100))\n    d = {\"costs\": costs,\n         \"Y_prediction_val\": Y_prediction_val,\n         \"Y_prediction_train\" : Y_prediction_train,\n         \"w\" : w,\n         \"b\" : b,\n         \"learning_rate\" : learning_rate,\n         \"num_epochs\": num_epochs}\n    return d","82e04b8a":"d = model(x_train, y_train, x_val, y_val, num_epochs = 200, learning_rate = 0.0006, mini_batch_size = 32, print_cost = True)","c13f1fac":"plt.plot(range(0, 200, 10), d[\"costs\"], c=\"r\", linewidth=1)\nplt.xlabel(\"number of epochs\", fontsize=14)\nplt.ylabel(\"cost\", fontsize=14)\nplt.title(\"Learning rate =\" + str(0.0006))\nplt.grid()","b2f85b60":"'''Reshaping the label vectors, to be used for sklearn logistic regression library'''\n\ny_val_ = np.reshape(y_val, (y_val.shape[0], )).reshape(-1) \ny_train_ = np.reshape(y_train, (y_train.shape[0], )).reshape(-1)","2c94cf69":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import confusion_matrix\nLR = LogisticRegression(C=0.01, solver='liblinear')\nLR.fit(x_train,y_train_)\nprint(\"train set accuracy using Sklearn: {} %\".format(LR.score(x_train, y_train_.T)*100))\nprint(\"validation set accuracy using Sklearn: {} %\".format(LR.score(x_val, y_val_.T)*100))","a894c097":"***Pre-Processing the dataset***","c1709e61":"***Sklearn Logistic Regression Model***","f3e641ff":"***Flattening the training and validation arrays***","9a56cba9":"***Model for Cat\/Dog Classification with neural network mindset***","61e0d3fa":"***Learning Curve***","a0edd39a":"***Summary of Processed data***"}}