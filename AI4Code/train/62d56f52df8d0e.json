{"cell_type":{"5ffa4986":"code","f3da5e7b":"code","9d6843bb":"code","9d170c2f":"code","69089385":"code","095de29d":"code","548e3739":"code","bd5fab22":"code","d17f5569":"code","03b5031f":"code","1daf3c38":"code","19e2f8a1":"code","577b5bf2":"code","b33b1c2c":"code","95033f26":"code","3890c384":"code","988a398c":"code","c8366667":"code","1d445eff":"code","baecf378":"code","4f3c0936":"code","940c29f3":"code","169a8ffb":"code","354d25fa":"code","6de24a8e":"code","ae5dd795":"code","74a9d1a3":"code","94eeda2c":"code","e53fcd54":"code","3d53d20d":"code","7cd9bd97":"code","ccc28b7d":"code","f5f58b4f":"code","82af5939":"code","eb4e3e17":"code","0598a44f":"code","6750ae7c":"code","1e7f6a67":"code","35f97f57":"code","8846b37d":"code","c5049cfa":"code","c3b51ab1":"code","de274838":"code","22182b1d":"code","83ab4bf2":"code","a4bfa35d":"code","09a5ea41":"code","2e3419d2":"code","6ff0d7d7":"code","978d4b24":"code","811078ec":"code","a0980dde":"code","1b8f45b4":"code","a7fbc909":"code","89fbd35a":"code","0e1fd9d8":"code","a19a3c9d":"code","788b6427":"code","90d50f07":"code","45ff8398":"code","45d0d700":"code","495f9573":"code","09b9f7c0":"code","8a1db070":"code","3626c0f0":"code","fa0ea587":"code","f09ae61d":"code","50dc6286":"code","2ff05ba5":"code","9cc7903d":"code","366de25d":"code","f40d2db4":"code","6d94b89b":"code","87884379":"code","f8f16c12":"code","3a6a74bd":"markdown","bcaf340b":"markdown","9cb02c6b":"markdown","4aabf721":"markdown","66498d9c":"markdown","bf0a874e":"markdown","c1d31cbd":"markdown","0e5eb839":"markdown","71ad4f08":"markdown","19d698f0":"markdown","b78c7909":"markdown","d16c48ee":"markdown","a91a4b29":"markdown","5d5763e9":"markdown","2f677253":"markdown","1132cfc6":"markdown","6e5f6beb":"markdown","3f51e79a":"markdown","9e798c59":"markdown","8f80575d":"markdown","189c7e8c":"markdown","5ce47192":"markdown","46c911ed":"markdown","7b5a52e4":"markdown","639511ed":"markdown","3b02ca17":"markdown","0f72539c":"markdown","cb8362fa":"markdown","89803271":"markdown","1d73524f":"markdown","eadc5da6":"markdown","b0b6601f":"markdown","b52e8919":"markdown","a9b27719":"markdown","a316720b":"markdown","47a8f6d0":"markdown","76fa0474":"markdown","66b9d0c2":"markdown","467e9cfe":"markdown","aa2a681b":"markdown","907c9789":"markdown","4896c973":"markdown","e1f7bb4f":"markdown"},"source":{"5ffa4986":"import numpy as np\nimport pandas as pd \nimport IPython\nfrom IPython.display import display\nimport re\nimport random\nimport warnings\nwarnings.simplefilter('ignore')\n\nimport matplotlib.pyplot as plt\n\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, cross_validate, cross_val_score, GridSearchCV, RandomizedSearchCV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.neighbors import DistanceMetric\nfrom sklearn.pipeline import Pipeline, make_pipeline\n\nfrom sklearn.metrics import SCORERS\nfrom sklearn.metrics import accuracy_score, f1_score, recall_score, precision_score, confusion_matrix\nfrom sklearn.metrics import roc_auc_score, roc_curve, classification_report, precision_recall_curve\n\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\nfrom sklearn.svm import SVC, LinearSVC, NuSVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.linear_model import SGDClassifier \n\nimport lightgbm as lgb\nimport catboost as ctb\nfrom catboost import CatBoostClassifier\n\nfrom tensorflow import keras\nfrom tensorflow.keras import Sequential\nfrom tensorflow.keras.layers import Conv2D, SeparableConv2D, Flatten, Dense, MaxPooling2D, AvgPool2D, GlobalAveragePooling2D\nfrom tensorflow.keras.layers import Dropout, BatchNormalization\nfrom tensorflow.keras.optimizers import Adam \nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications.resnet import ResNet50\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, Callback, EarlyStopping\nfrom keras.regularizers import L1L2\ndense_regularizer = L1L2(l2=0.0001)\n\nimport hyperopt\nfrom hyperopt import fmin, tpe, STATUS_OK, STATUS_FAIL, Trials, hp","f3da5e7b":"train_set = pd.read_csv('\/kaggle\/input\/digit-recognizer\/train.csv')\ntest_set = pd.read_csv('\/kaggle\/input\/digit-recognizer\/test.csv')\nsample = pd.read_csv('\/kaggle\/input\/digit-recognizer\/sample_submission.csv')","9d6843bb":"RND_ST = 42","9d170c2f":"train_set","69089385":"train_set.info()","095de29d":"test_set.info()","548e3739":"X_train = train_set.drop('label', axis=1)\ny_train = train_set['label']\n\nX_test = test_set","bd5fab22":"digit_example = X_train.iloc[85]\ndigit_example = digit_example.values.reshape(28, 28)","d17f5569":"print(digit_example[5:-5,5:-5])","03b5031f":"plt.imshow(digit_example, cmap='gray')","1daf3c38":"X_train_sub, X_valid, y_train_sub, y_valid = train_test_split(X_train, y_train, test_size=0.1, random_state=RND_ST)","19e2f8a1":"temp = y_train.value_counts(normalize=True).reset_index()\ntemp.rename(columns={'index':'label', 'label':'percentage'}, inplace=True)\ntemp.sort_values(by='label')","577b5bf2":"lvc_1 = LinearSVC(penalty='l2', loss='squared_hinge', tol=0.0001, \n                C=1.0, multi_class='ovr', random_state=RND_ST, max_iter=1000)","b33b1c2c":"def valid_prediction(model, X_train, X_valid, y_train, y_valid):\n    \n    model.fit(X_train, y_train)\n    pred = model.predict(X_valid)\n    \n    print(f1_score(y_valid, pred, average='macro'))","95033f26":"#valid_prediction(lvc_1, X_train_sub, X_valid, y_train_sub, y_valid)","3890c384":"mm = MinMaxScaler()\n\nX_train_sub_mm = mm.fit_transform(X_train_sub)\nX_valid_mm = mm.fit_transform(X_valid)\n\nX_train_mm = mm.fit_transform(X_train)\nX_test_mm = mm.fit_transform(X_test)","988a398c":"raw_exam = X_train_sub.iloc[0]\nraw_exam = raw_exam.values.reshape(28, 28)\nplt.imshow(raw_exam, cmap='gray')\nplt.colorbar();","c8366667":"scaled_exam = X_train_sub_mm[0]\nscaled_exam = scaled_exam.reshape(28,28)\nplt.imshow(scaled_exam, cmap='gray')\nplt.colorbar();","1d445eff":"#valid_prediction(lvc_1, X_train_sub_mm, X_valid_mm, y_train_sub, y_valid)","baecf378":"sgd = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001,\n                    max_iter=100, tol=0.00001, shuffle=True, \n                    verbose=0, epsilon=0.1, n_jobs=-1, random_state=RND_ST, \n                    learning_rate='adaptive', \n                    eta0=0.1)","4f3c0936":"#valid_prediction(sgd, X_train_sub_mm, X_valid_mm, y_train_sub, y_valid)","940c29f3":"#### best 0.9145478702430342","169a8ffb":"train_sub_data = lgb.Dataset(X_train_sub_mm, label=y_train_sub) \n\ntrain_data = lgb.Dataset(data=X_train_mm, label=y_train)","354d25fa":"lgb_params = {'learning_rate': 0.1,\n              'metric': 'multi_logloss',\n              'num_leaves': 31,               \n              'objective': 'multiclass',\n              'num_class': 10, \n              'verbose':1}\n\nnum_round = 100","6de24a8e":"#model = lgb.train(lgb_params, train_sub_data, num_round)","ae5dd795":"#pred = model.predict(X_valid_mm)\n#pred_1 = [np.argmax(line) for line in pred]","74a9d1a3":"#f1_score(y_valid, pred_1, average='macro')","94eeda2c":"### best metric 0.9771673277812697","e53fcd54":"#lgbm = lgb.train(lgb_params, train_data, num_round)","3d53d20d":"def submission_lgbm(model, X_test):\n    \n    final_pred = model.predict(X_test)\n    final_pred_norm = [np.argmax(line) for line in final_pred]\n    \n    submission = sample.copy()\n    submission['Label'] = final_pred_norm\n    \n    submission.to_csv('\/kaggle\/working\/lgbm_2.csv', index=False)","7cd9bd97":"#submission_lgbm(lgbm, X_test_mm)","ccc28b7d":"OPTIMIZER = Adam(lr=0.0001)\nNUM_EPOCH = 10\nBATCH_SIZE = 32","f5f58b4f":"model = Sequential()\n\nmodel.add(Dense(units=10, activation='softmax'))\n\nmodel.compile(\n    optimizer=OPTIMIZER,\n    loss='sparse_categorical_crossentropy', \n    metrics=['acc']\n    )","82af5939":"#temp = X_train_sub \/ 255.","eb4e3e17":"model.fit(X_train_sub_mm, \n          y_train_sub, \n          epochs=NUM_EPOCH, \n          batch_size=BATCH_SIZE,\n          steps_per_epoch=None, \n          validation_steps=None,\n          verbose=2, shuffle=True,\n          validation_data=(X_valid, y_valid))","0598a44f":"### best f1 value = 0.9055","6750ae7c":"del model","1e7f6a67":"OPTIMIZER = Adam(lr=0.0001)\nNUM_EPOCH = 10\nBATCH_SIZE = 64","35f97f57":"model = Sequential()\n\nmodel.add(Dense(units=1024, activation='relu', input_dim=28 * 28))\nmodel.add(Dense(units=512, activation='relu'))\nmodel.add(Dense(units=128, activation='relu'))\n#model.add(Dense(units=32, activation='relu'))\nmodel.add(Dense(units=10, activation='softmax'))\n\nmodel.compile(\n    optimizer=OPTIMIZER,\n    loss='sparse_categorical_crossentropy', \n    metrics=['acc']\n    )","8846b37d":"model.fit(X_train_mm, \n          y_train, \n          epochs=NUM_EPOCH, \n          batch_size=BATCH_SIZE,\n          steps_per_epoch=None, \n          validation_steps=None,\n          verbose=2,\n          validation_split=0.2,\n          shuffle=True)\n          #validation_data=(X_valid, y_valid))","c5049cfa":"X_train_ks = X_train_mm.reshape(-1, 28, 28, 1)\nX_test_ks = X_test_mm.reshape(-1, 28, 28, 1)\n\nX_train_sub_ks = X_train_sub_mm.reshape(-1, 28, 28, 1)\nX_valid_ks = X_valid_mm.reshape(-1, 28, 28, 1)\n\nX_train_ks.shape, X_test_ks.shape","c3b51ab1":"del model","de274838":"OPTIMIZER = Adam(lr=0.0001)\nNUM_EPOCH = 20\nBATCH_SIZE = 32","22182b1d":"model = Sequential()\n\nmodel.add(Conv2D(filters=4, kernel_size=(3, 3), activation='relu', padding='same', input_shape=(28,28,1)))\nmodel.add(Conv2D(filters=4, kernel_size=(3, 3), activation='relu', strides=2, input_shape=(28,28,1)))\nmodel.add(Flatten())\nmodel.add(Dense(units=10, activation='softmax'))\n\nmodel.compile(\n    optimizer=OPTIMIZER,\n    loss='sparse_categorical_crossentropy', \n    metrics=['acc']\n    )","83ab4bf2":"X_train_ks.shape","a4bfa35d":"model.fit(X_train_ks, \n          y_train, \n          epochs=NUM_EPOCH, \n          batch_size=BATCH_SIZE,\n          steps_per_epoch=None, \n          validation_steps=None,\n          verbose=2,\n          validation_split=0.2,\n          shuffle=True)","09a5ea41":"del model","2e3419d2":"OPTIMIZER = Adam(lr=0.0001)\nNUM_EPOCH = 25\nBATCH_SIZE = 32\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=6, kernel_size=(5, 5), \n                 padding='same', activation='relu', input_shape=(28,28,1)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(filters=32, kernel_size=(5, 5),\n                 padding='valid',activation='relu', input_shape=(28,28,1)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(units=120, activation='relu'))\n#model.add(Dense(units=512, activation='relu'))\nmodel.add(Dense(units=84, activation='relu'))\nmodel.add(Dense(units=10, activation='softmax'))\n\nmodel.compile(\n    optimizer=OPTIMIZER,\n    loss='sparse_categorical_crossentropy', \n    metrics=['acc']\n    )","6ff0d7d7":"model.fit(X_train_ks, \n          y_train, \n          epochs=NUM_EPOCH, \n          batch_size=BATCH_SIZE,\n          steps_per_epoch=None, \n          validation_steps=None,\n          verbose=2,\n          validation_split=0.2,\n          shuffle=True)","978d4b24":"### best loss: 0.0030 - acc: 0.9991 - val_loss: 0.0388 - val_acc: 0.9902","811078ec":"del model","a0980dde":"OPTIMIZER = Adam(lr=0.0001)\nNUM_EPOCH = 30\nBATCH_SIZE = 100\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu', input_shape=(28,28,1)))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Conv2D(filters=265, kernel_size=(3,3),padding='valid',activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(3,3)))\n\nmodel.add(Conv2D(filters=1024, kernel_size=(3,3),padding='valid',activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2,2)))\n\nmodel.add(Flatten())\nmodel.add(Dense(units=2048, activation='relu'))\nmodel.add(Dropout(0.2,seed=RND_ST))\n\nmodel.add(Dense(units=2048, activation='relu'))\nmodel.add(Dropout(0.2,seed=RND_ST))\n\nmodel.add(Dense(units=1000, activation='relu'))\nmodel.add(Dropout(0.2,seed=RND_ST))\n\nmodel.add(Dense(units=10, activation='softmax'))\n\nmodel.compile(\n    optimizer=OPTIMIZER,\n    loss='sparse_categorical_crossentropy', \n    metrics=['acc']\n    )\n\nmodel.summary()","1b8f45b4":"model.fit(X_train_sub_ks, \n          y_train_sub, \n          epochs=NUM_EPOCH, \n          batch_size=BATCH_SIZE,\n          steps_per_epoch=None, \n          validation_steps=None,\n          verbose=2,\n          validation_split=0.1,\n          shuffle=True)","a7fbc909":"pred = model.predict(X_valid_ks)\npred_norm = [np.argmax(line) for line in pred]\n\nf1_score(y_valid, pred_norm, average='macro')","89fbd35a":"### Epoch 16\/20\n### 84\/84 - 5s - loss: 0.0106 - acc: 0.9967 - val_loss: 0.0285 - val_acc: 0.9921\n\n### 168\/168 - 5s - loss: 0.0054 - acc: 0.9982 - val_loss: 0.0414 - val_acc: 0.9911\n\n### best values  \n### 525\/525 - 15s - loss: 0.0039 - acc: 0.9988 - val_loss: 0.0376 - val_acc: 0.9930","0e1fd9d8":"learning_rate_reduction = ReduceLROnPlateau(monitor='val_loss', \n                                            patience=5, \n                                            verbose=1, \n                                            factor=0.5, \n                                            min_lr=0.00001)","a19a3c9d":"datagen = ImageDataGenerator(\n        rotation_range= 25,  \n        zoom_range = 0.18, \n        width_shift_range=0.1,  \n        height_shift_range=0.1)  \n\n\ndatagen.fit(X_train_sub_ks)","788b6427":"EPOCH = 20\nBATCH_SIZE = 400\n\nhistory = model.fit_generator(\n    datagen.flow(X_train_sub_ks, \n                 y_train_sub, \n                 batch_size=BATCH_SIZE), \n                 epochs = EPOCH,\n                 validation_data = (X_valid_ks,y_valid),           \n                 verbose = 2, \n                 steps_per_epoch=X_train_sub_ks.shape[0] \/\/ BATCH_SIZE, \n                 callbacks=[learning_rate_reduction])","90d50f07":"### best value  \n### 378\/378 - 15s - loss: 0.0101 - acc: 0.9973 - val_loss: 0.0183 - val_acc: 0.9943 - lr: 1.0000e-05","45ff8398":"def submission_keras(model, X_test):\n    \n    final_pred = model.predict(X_test)\n    final_pred_norm = [np.argmax(line) for line in final_pred]\n    \n    submission = sample.copy()\n    submission['Label'] = final_pred_norm\n    \n    submission.to_csv('\/kaggle\/working\/lr_upd_02.csv', index=False)","45d0d700":"submission_keras(model, X_test_ks)","495f9573":"del model_02","09b9f7c0":"OPTIMIZER = Adam(lr=0.0001)\n\nmodel_02 = Sequential()\n\nmodel_02.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu', input_shape=(28,28,1)))\nmodel_02.add(BatchNormalization())\nmodel_02.add(Conv2D(filters=128, kernel_size=(3,3), padding='same', activation='relu'))\nmodel_02.add(BatchNormalization())\nmodel_02.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\nmodel_02.add(Dropout(0.3,seed=RND_ST))\n\nmodel_02.add(Conv2D(filters=264, kernel_size=(3,3),padding='same',activation='relu'))\nmodel_02.add(BatchNormalization())\nmodel_02.add(Conv2D(filters=264, kernel_size=(3,3),padding='same',activation='relu'))\nmodel_02.add(BatchNormalization())\nmodel_02.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\nmodel_02.add(Dropout(0.3,seed=RND_ST))\n\nmodel_02.add(Conv2D(filters=512, kernel_size=(5,3),padding='same',activation='relu'))\nmodel_02.add(BatchNormalization())\nmodel_02.add(Conv2D(filters=512, kernel_size=(5,5),padding='same',activation='relu'))\nmodel_02.add(BatchNormalization())\nmodel_02.add(Conv2D(filters=512, kernel_size=(5,5),padding='same',activation='relu'))\nmodel_02.add(BatchNormalization())\nmodel_02.add(MaxPooling2D(pool_size=(2,2), strides=(2, 2)))\nmodel_02.add(Dropout(0.3,seed=RND_ST))\n\nmodel_02.add(Flatten())\nmodel_02.add(Dense(units=2048, activation='relu'))\nmodel_02.add(Dropout(0.2,seed=RND_ST))\n\nmodel_02.add(Dense(units=2048, activation='relu'))\nmodel_02.add(Dropout(0.2,seed=RND_ST))\n\nmodel_02.add(Dense(units=1000, activation='relu'))\nmodel_02.add(Dropout(0.2,seed=RND_ST))\n\nmodel_02.add(Dense(units=10, activation='softmax'))\n\nmodel_02.compile(\n    optimizer=OPTIMIZER,\n    loss='sparse_categorical_crossentropy', \n    metrics=['acc']\n    )\n\nmodel_02.summary()","8a1db070":"EPOCH = 30\nBATCH_SIZE = 200\n\nmodel_02.fit_generator(\n                datagen.flow(X_train_sub_ks, y_train_sub, batch_size=BATCH_SIZE), \n                epochs = EPOCH,\n                validation_data = (X_valid_ks,y_valid),           \n                verbose = 2, \n                steps_per_epoch=X_train_sub_ks.shape[0] \/\/ BATCH_SIZE, \n                callbacks=[learning_rate_reduction])","3626c0f0":"### best\n### 126\/126 - 18s - loss: 0.0061 - acc: 0.9984 - val_loss: 0.0285 - val_acc: 0.9948 - lr: 1.0000e-05","fa0ea587":"OPTIMIZER = Adam(lr=0.00005, decay=1e-5)","f09ae61d":"del model_03","50dc6286":"model_03 = Sequential()\n\nmodel_03.add(Conv2D(64, (3,3), activation='relu', padding='same', input_shape=(28,28,1)))\nmodel_03.add(Conv2D(64, (3,3), activation='relu', padding='same'))\nmodel_03.add(MaxPooling2D((2,2)))\n    \nmodel_03.add(Conv2D(128, (3,3), activation='relu', padding='same'))\nmodel_03.add(Conv2D(128, (3,3), activation='relu', padding='same'))\nmodel_03.add(MaxPooling2D((2,2)))\n    \nmodel_03.add(Conv2D(256, (3,3), activation='relu', padding='same'))\nmodel_03.add(BatchNormalization())\nmodel_03.add(Conv2D(256, (3,3), activation='relu', padding='same'))\nmodel_03.add(BatchNormalization())\nmodel_03.add(Conv2D(256, (3,3), activation='relu', padding='same'))\nmodel_03.add(MaxPooling2D((2,2)))\n    \nmodel_03.add(Conv2D(512, (3,3), activation='relu', padding='same'))\nmodel_03.add(BatchNormalization())\nmodel_03.add(Conv2D(512, (3,3), activation='relu', padding='same'))\nmodel_03.add(BatchNormalization())\nmodel_03.add(Conv2D(512, (3,3), activation='relu', padding='same'))\nmodel_03.add(MaxPooling2D((2,2)))\n    \nmodel_03.add(Flatten())\nmodel_03.add(Dense(1024, activation='relu'))\nmodel_03.add(Dropout(0.5))\nmodel_03.add(Dense(1024, activation='relu'))\nmodel_03.add(Dropout(0.5))\nmodel_03.add(Dense(512, activation='relu'))\nmodel_03.add(Dropout(0.5))\nmodel_03.add(Dense(10, activation='softmax'))\n\nmodel_03.compile(\n    optimizer=OPTIMIZER,\n    loss='sparse_categorical_crossentropy', \n    metrics=['acc']\n    )\n\nmodel_03.summary()","2ff05ba5":"lr_red = ReduceLROnPlateau(monitor='val_loss', \n                           patience=4, \n                           verbose=1, \n                           factor=0.5, \n                           min_lr=0.000009)\n\ncheckpoint = ModelCheckpoint(filepath='best_model_todate', save_best_only=True, save_weights_only=True)","9cc7903d":"EPOCH = 30\nBATCH_SIZE = 32\n\nhistory = model_03.fit_generator(\n                datagen.flow(X_train_sub_ks, y_train_sub, batch_size=BATCH_SIZE), \n                epochs = EPOCH,\n                validation_data = (X_valid_ks,y_valid),           \n                verbose = 2, \n                steps_per_epoch=X_train_sub_ks.shape[0] \/\/ BATCH_SIZE, \n                callbacks=[lr_red, checkpoint])","366de25d":"### best \n### Epoch 30\/30\n### 1181\/1181 - 20s - loss: 0.0127 - acc: 0.9965 - val_loss: 0.0237 - val_acc: 0.9955 - lr: 9.0000e-06","f40d2db4":"def fitting(model, feat, targ):\n    \n    model.fit(feat, targ)","6d94b89b":"def submission(model, X_test):\n    \n    final_pred = model.predict(X_test)\n    \n    submission = sample.copy()\n    submission['Label'] = final_pred\n    \n    submission.to_csv('\/kaggle\/working\/lgbm.csv', index=False)","87884379":"def submission_keras(model, X_test):\n    \n    final_pred = model.predict(X_test)\n    final_pred_norm = [np.argmax(line) for line in final_pred]\n    \n    submission = sample.copy()\n    submission['Label'] = final_pred_norm\n    \n    submission.to_csv('\/kaggle\/working\/m0301.csv', index=False)","f8f16c12":"submission_keras(model_03, X_test_ks)","3a6a74bd":"No missing values, great!","bcaf340b":"lgb_2_params = {'learning_rate': 0.1,\n              'metric': 'multi_logloss',\n              'num_leaves': 31,               \n              'objective': 'multiclass',\n              'num_class': 10, \n              'verbose':1} num_round = 200  \n\nlgb_params = {'learning_rate': 0.1,\n              'metric': 'multi_logloss',\n              'num_leaves': 31,               \n              'objective': 'multiclass',\n              'num_class': 10, \n              'verbose':1} num_round = 100\n\nsgd = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001,\n                    max_iter=100, tol=0.00001, shuffle=True, \n                    verbose=0, epsilon=0.1, n_jobs=-1, random_state=RND_ST, \n                    learning_rate='adaptive', \n                    eta0=0.1,\n                    validation_fraction=0.1)\n\nlvc_2 = LinearSVC(penalty='l2', loss='hinge', tol=0.0001, \n                C=0.2, multi_class='ovr', random_state=RND_ST, max_iter=100)","9cb02c6b":"Just in case, check the datasets info.","4aabf721":"*I uptade this work daily. I commented some code to increase re-run time. Don't hesitate to uncomment and run all cells.*","66498d9c":"Value labels are balanced. Not perfectly, indeed.","bf0a874e":"Better, but we have to raise the acuracy score to at least 0.99.  \n\n**Convolutional networks**  \n\nWe change the network architecture by adding some convolutional layers.  \n\nThis type of layers requires data with a n-dimension shape. This shape depends on the number of colours in pictures. We have only black and white digits. So we need to set 1 colour layer.   \nWhen you work with colourful pics, set this parameter on 3. Cause there will be three channels: Red, Green and Blue (RGB scheme).","c1d31cbd":"Cheking the results.","0e5eb839":"#### LightGBM","71ad4f08":"Train and valid accuracy are roughly at the same low point. How can we improve the model?  \nRight, just add more layers. Or more epoches. And don't forget to change the optimizer learning rate.","19d698f0":"### Data augmentation \n\nLet's apply some data augmentation and learning rate reduction  \n\nSpecial thanks to https:\/\/www.kaggle.com\/gordotron85\/introduction-to-cnn-with-mnist","b78c7909":"f1_macro = 0.909  \nBetter than yesterday!","d16c48ee":"Score 0.99553 \/ Rank 371 \/ new_02  \nScore 0.99503 \/ Rank 418 \/ lr_upd_02  \nScore 0.99439 \/ Rank 510 \/ lr_upd  \nScore 0.99250 \/ Rank 796 \/ test_06  \nScore 0.99050 \/ Rank 1086 \/ test_05  \nScore 0.99025 \/ Rank 1117 \/ test_04  \nScore 0.98878 \/ Rank 1276 \/ test_01  \nScore 0.98871 \/ Rank 1265 \/ LeNet_4  \nScore 0.98696 \/ Rank 1403 \/ LeNet_3  \nScore 0.98139 \/ Rank 1741 \/ keras LeNet  \nScore 0.97796 \/ Rank 1892 \/ keras model  \nScore 0.97457 \/ Rank 2035 \/ lgbm_2 \/ X_train_mm  \nScore 0.96800 \/ Rank 2254 \/ lgbm \/ X_train_mm  \nScore 0.91564 \/ Rank 2665 \/ sgd \/ X_train_mm  \nScore 0.91553 \/ Rank 2665 \/ lvc_2 \/ X_train_mm  \nScore 0.91260 \/ Rank 2678 \/ Linear SVC \/ X_train_mm   \nScore 0.87350 \/ Rank 2707 \/ Simple Linear SVC model without any tuning and preprocessing.\n","a91a4b29":"You need to scale a data before train the model. We have done this operation before using MisMaxScaling. But you may just apply the method bellow.","5d5763e9":"Note, that you may split data to train valid subset right into fit() function.","2f677253":"f1_macro = 0.872 \n\nNot great, not terrible. But we need to improve the result.","1132cfc6":"Seems like it's NINE","6e5f6beb":"Don't forget to change the model prediction from a class probability array to class value.","3f51e79a":"Are boosting machines better than simple ones?  \n\nMaking the trainung and valid datasets for the gradiend boosting machine.","9e798c59":"## Hello, my beautiful Kagglers!  \n\nToday I start a new competition.   \n\nI will learn a computer vision from the scratch. I'd like to explore and apply some basic and modern ways of image classification. First I will use classic models, like SVM and k-neighbours. After that, I'm planning to use Keras model to improve the quality of prediction.\n\nJoin me on this beautiful journey. I will update this notebook according to my new knowledge. It would be a pleasure for me to read your comments and suggestions.\n\n**Let's jump into it!**\n","8f80575d":"Using sub and validation sets from prevoius steps.","189c7e8c":"### Model selection\n\n#### Starting with a simple Linear Support Vector Machine.","5ce47192":"What if we scale the brigthness of each pixel from [0:255] to [0:1].  \nApplying MinMaxScaler for this model.","46c911ed":"### Submission  \nFit the final model with the whole train dataset.","7b5a52e4":"For this classifier, we will use a special submission algorithm.","639511ed":"### Improving the CNN architecture, vol. 2","3b02ca17":"## CNN (Keras) \n\n#### Fully connected networks\n\nIt's neural networks time!  \n\nIf you've just started to learn Computer vision (just like me), please find some useful info below. \n\nFirst, we try to build simple Neural network with one layer. Sounds amazing, but this is a classic logistic regression.  \nTo build a successful model we have to set at least three parameters. Optimizer: instead of default 'sgd' - stochastic gradient boosting, we apply the Adam algorithm.  \nWe also starts with 10 epochs (how many times the model iterates the data) and a batch size = 32 (how many examples (rows, pictures) the model uses for learning)","0f72539c":"The architecture of a logistic regression is shown below. The fully connected layer contains 10 neurons: according to the number of total different classes (digits from 0 to 9).","cb8362fa":"Yup, it works.  \nLet's fit the model with the new data.","89803271":"#### Improving network architecture","1d73524f":"Set the random seed ","eadc5da6":"**test_06**\n\nOPTIMIZER = Adam(lr=0.0001)\nNUM_EPOCH = 15\nBATCH_SIZE = 64\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=512, kernel_size=(3, 3), \n                 padding='same', activation='relu', input_shape=(28,28,1)))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=2))\nmodel.add(Conv2D(filters=1024, kernel_size=(3, 3),\n                 padding='valid',activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(filters=512, kernel_size=(3, 3),\n                 padding='valid',activation='relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2), strides=2))\n\nmodel.add(Flatten())\nmodel.add(Dense(units=2048, activation='relu'))\nmodel.add(Dense(units=2048, activation='relu'))\nmodel.add(Dense(units=512, activation='relu'))\nmodel.add(Dense(units=10, activation='softmax'))\n\nmodel.compile(\n    optimizer=OPTIMIZER,\n    loss='sparse_categorical_crossentropy', \n    metrics=['acc']\n    )","b0b6601f":"#### SGDClassifier","b52e8919":"## Sklearn models\n\nFor the 'classic way' I apply LinearSVC.\nSplit train set into sub-train and valid get the prediction power of a model.","a9b27719":"Start with libs and data import as usual","a316720b":"#### Brightness scaling","47a8f6d0":"### Moving forward","76fa0474":"### Improving the CNN architecture, vol. 3","66b9d0c2":"Here we have a multiclass classification problem. Let's check the amount af percentage of the target values.\n","467e9cfe":"## Scoreboard","aa2a681b":"Yeah, we guessed!  \n\n","907c9789":"**LeNet_4** \n\nOPTIMIZER = Adam(lr=0.0001)\nNUM_EPOCH = 20\nBATCH_SIZE = 32\n\nmodel = Sequential()\n\nmodel.add(Conv2D(filters=6, kernel_size=(5, 5), \n                 padding='same', activation='relu', input_shape=(28,28,1)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Conv2D(filters=32, kernel_size=(5, 5),\n                 padding='valid',activation='relu', input_shape=(28,28,1)))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Flatten())\nmodel.add(Dense(units=1024, activation='relu'))\nmodel.add(Dense(units=512, activation='relu'))\nmodel.add(Dense(units=128, activation='relu'))\nmodel.add(Dense(units=10, activation='softmax'))\n\nmodel.compile(\n    optimizer=OPTIMIZER,\n    loss='sparse_categorical_crossentropy', \n    metrics=['acc']\n    )","4896c973":"It works!\n\nLet's build LeNet architecture.","e1f7bb4f":"#### Look at the digits  \n\nPick up a random row of the training set and transform the row walues into a picture."}}