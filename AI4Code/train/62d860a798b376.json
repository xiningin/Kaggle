{"cell_type":{"e05555b6":"code","68d46d76":"code","eb1fc977":"code","8ccf9ad5":"code","0cde6ac5":"code","8f715fd4":"code","9a7dd717":"code","6710d51c":"code","780202da":"code","297e66f5":"code","996e12a4":"code","7b724fa4":"code","4e70782d":"code","909feef2":"code","4634c1c1":"code","cf281ae0":"code","01a62cd3":"code","bd579ed4":"code","2a7bee9e":"code","fab3304d":"code","ee7f1ee8":"code","a96b8cfe":"code","2c927c97":"code","13ddaf00":"code","96756903":"code","40486793":"code","e249ab43":"code","dc5a8002":"code","e27e60d2":"code","bc502efa":"code","be3a9e70":"code","950bdcc0":"code","e456fc9d":"code","e8e4e6d1":"code","7cb02dbe":"code","98c7306f":"code","e72b0e45":"code","59183ab9":"code","31da7dcb":"code","3ae5bc73":"code","bef45f45":"code","5c955dc0":"code","62c50173":"code","08c0e1fa":"code","314bddcd":"code","1529bf11":"code","0a9e0c8a":"code","aa1c7f1a":"code","d5e49b09":"code","16ee7e3a":"code","79cd6fa1":"code","e6650308":"code","196202d8":"code","c020e221":"markdown","8576abdf":"markdown","32703c3a":"markdown","7ef605a6":"markdown","94d80cac":"markdown","0a520d89":"markdown","79b0821b":"markdown","19cdba0c":"markdown","1d91ae79":"markdown","0300b5b1":"markdown","da81b431":"markdown","cfa917f7":"markdown","90687414":"markdown","97451d5a":"markdown","e71f1bb9":"markdown","4239968b":"markdown","38553749":"markdown","a55826d6":"markdown","09882f34":"markdown","b8a1a5a7":"markdown","5a8f28c8":"markdown","56b582f5":"markdown","a6aeae63":"markdown","df380db2":"markdown","5ce9ba68":"markdown","77273c15":"markdown","1e607841":"markdown","e1002c09":"markdown","76dcf078":"markdown","ae16fa64":"markdown","2d5c96ef":"markdown","29514d47":"markdown","6aa706e8":"markdown","c59db5e3":"markdown","ef91fbf9":"markdown","f2f241c7":"markdown","56b4c70e":"markdown","760c42d9":"markdown","94ca8cda":"markdown"},"source":{"e05555b6":"# Not sure why the output isnt visble had made this project seperately on jypyter notebook,Feel free to check out with outputs on my github : https:\/\/bit.ly\/3jbLDaw\n\n\n#import libraries\n\nimport numpy as np\nimport pandas as pd\nfrom matplotlib import pyplot\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom scipy import stats\nfrom scipy.stats import zscore\nfrom sklearn import preprocessing\nfrom sklearn import neighbors, linear_model\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import classification_report\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import f1_score\nfrom imblearn.over_sampling import SMOTE\nfrom collections import Counter\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings(\"ignore\")","68d46d76":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","eb1fc977":"n= pd.read_csv('\/kaggle\/input\/healthcare-insurance\/Part1 - Normal.csv')\nh= pd.read_csv('\/kaggle\/input\/healthcare-insurance\/Part1 - Type_H.csv')\ns= pd.read_csv('\/kaggle\/input\/healthcare-insurance\/Part1 - Type_S.csv')","8ccf9ad5":"print(\"Duplicate labels found in dataset:\")\nprint(n.Class.unique(),h.Class.unique(),s.Class.unique())","0cde6ac5":"h['Class'] =h['Class'].str.replace('type_h','Type_H')\nn['Class'] =n['Class'].str.replace('Nrmal','Normal')\ns['Class'] =s['Class'].str.replace('tp_s','Type_S')\nprint(\"Adjusting dataset to have appropriate class:\")\nprint(n.Class.unique(),h.Class.unique(),s.Class.unique())","8f715fd4":"#reformatting for visualisation\nh['Class'] =h['Class'].str.replace('Type_H','Type H')\ns['Class'] =s['Class'].str.replace('Type_S','Type S')\nprint(n.Class.unique(),h.Class.unique(),s.Class.unique())","9a7dd717":"def cleansing (x):\n    \n    print(\"\\033[1m HEAD\\n\\033[0m\",x.head())\n    print(\"\\033[1m\\nTAIL\\n\\033[0m\",x.tail())\n    print(\"\\033[1m\\nDATA TYPES\\n\\033[0m\",s.dtypes)\n    print(\"\\033[1m\\nDESCRIBE\\n\\033[0m\",x.describe())\n    print(\"\\033[1m\\nMissing value is:\\033[0m\",x.isnull().values.any())\n    print(\"\\033[1m\\nDupicate value is:\\033[0m\",x.duplicated().values.any())\n    print(\"\\033[1m\\nShape of Dataset:\\033[0m\",x.shape)\n    print(\"\\033[1m\\nSize of Dataset:\\033[0m\",x.size)\n    print(\"\\033[1m\\nDimension of Dataset:\\033[0m\",x.ndim)  ","6710d51c":"def correlate(x):\n    cl=x.Class.nunique()\n    if cl>2:\n        cls=\"Mixed\"\n    else:\n        cls=x.Class[0] \n    print(f\"\\033[1m\\nDiagonal Correlation Heat Map Type: {cls}\\033[0m\\n\")\n    corr = x.corr()\n    mask = np.triu(np.ones_like(corr, dtype=bool))\n    \n    ax = sns.heatmap(\n        corr,mask=mask, \n        vmin=-1,vmax=1, center=0,\n        cmap=sns.diverging_palette(20, 220, n=200),\n        square=True,linewidths=.5, cbar_kws={\"shrink\": .5})\n   \n    ax.set_xticklabels(\n        ax.get_xticklabels(),\n        rotation=45,\n        horizontalalignment='right');","780202da":"def distribution(x):\n    cl=x.Class.nunique()\n    if cl>2:\n        cls=\"Mixed\"\n    else:\n        cls=x.Class[0]  \n    \n    print(f\"\\033[1m\\nDistribution Observed In Dataset Type: {cls}\\033[0m\\n\")\n    fig, ((ax1, ax2, ax3), (ax4, ax5, ax6)) = plt.subplots(nrows=2, ncols=3, sharex=True, sharey=True, figsize=(12,6));\n    sns.histplot( x[\"P_incidence\"] , color=\"skyblue\", ax=ax1)\n    ax1.set_title(\"Incidence\");\n    sns.histplot( x[\"P_tilt\"] , color=\"dimgray\", ax=ax2);\n    ax2.set_title(\"Tilt\");\n    sns.histplot( x[\"L_angle\"] , color=\"olive\", ax=ax3);\n    ax3.set_title(\"Angle\");\n    sns.histplot( x[\"S_slope\"] , color=\"gold\", ax=ax4);\n    ax4.set_title(\"Slope\");\n    sns.histplot( x[\"P_radius\"] , color=\"teal\", ax=ax5);\n    ax5.set_title(\"Radius\");\n    sns.histplot( x[\"S_Degree\"] , color=\"seagreen\", ax=ax6);\n    ax6.set_title(\"Degree\");\n    \n    plt.tight_layout();","297e66f5":"cleansing(n)","996e12a4":"distribution(n)","7b724fa4":"correlate(n)","4e70782d":"cleansing(h)","909feef2":"distribution(h)","4634c1c1":"correlate(h)","cf281ae0":"cleansing(s)","01a62cd3":"distribution(s)","bd579ed4":"correlate(s)","2a7bee9e":"c = pd.concat(([n,h,s]), ignore_index=True)\nc","fab3304d":"c.Class.value_counts().plot(kind = 'bar');\nplt.axhline(y=(n.Class.count()), color='r', linestyle='dotted');\nplt.axhline(y=(h.Class.count()), color='r', linestyle='dotted');\nplt.axhline(y=(s.Class.count()), color='r', linestyle='dotted');","ee7f1ee8":"cleansing(c)","a96b8cfe":"distribution(c)","2c927c97":"correlate(c)","13ddaf00":"sns.pairplot(c,hue=\"Class\")\nplt.tight_layout();","96756903":"hs = pd.concat(([h,s]), ignore_index=True)\nsns.pairplot(hs,hue=\"Class\");","40486793":"sns.displot(data=c,x=\"S_slope\", hue=\"Class\",kind=\"kde\", height=6,multiple=\"fill\", clip=(0, None),\n            palette=\"ch:rot=-.25,hue=1,light=.75\");","e249ab43":"sns.displot(data=c,x=\"P_radius\", hue=\"Class\",kind=\"kde\", height=6,multiple=\"fill\", clip=(0, None),\n            palette=\"ch:rot=-.25,hue=1,light=.75\");","dc5a8002":"sns.displot(data=c,x=\"S_Degree\", hue=\"Class\",kind=\"kde\", height=6,multiple=\"fill\", clip=(0, None),\n            palette=\"ch:rot=-.25,hue=1,light=.75\");","e27e60d2":"font = {'family': 'serif',\n        'color':  'darkred',\n        'weight': 'normal',\n        'size': 16}\nsns.boxplot(c[\"S_Degree\"]);\nout=int(c[\"S_Degree\"].max())\nplt.axvline(x=([c[\"S_Degree\"].max()]), color='r', linestyle='dotted');\nplt.text(out,.6, out,fontdict=font );","bc502efa":"q = c[\"S_Degree\"].max()\nc1=c[c[\"S_Degree\"] < q]\nsns.boxplot(c1[\"S_Degree\"]);","be3a9e70":"sns.displot(data=c1,x=\"S_Degree\", hue=\"Class\",kind=\"kde\", height=6,multiple=\"fill\", clip=(0, None),\n            palette=\"ch:rot=-.25,hue=1,light=.75\");","950bdcc0":"sns.barplot(x='Class', y='S_Degree', data=c1,palette ='magma');\nplt.title('Type S patients highly correlated to S_Degree values');","e456fc9d":"distribution(c1)","e8e4e6d1":"sns.set_theme(style=\"ticks\")\nsns.jointplot(data=c,x=\"S_slope\", y=\"P_radius\", hue=\"Class\",kind=\"kde\");","7cb02dbe":"sns.set_theme(style=\"ticks\")\nplot=sns.jointplot(data=hs,x=\"S_slope\", y=\"P_radius\", hue=\"Class\",kind=\"kde\");\n#setting equal magnitude of x and y to get a 1:1 scaled visualisation\nplot.ax_marg_x.set_xlim(0, 120)\nplot.ax_marg_y.set_ylim(60, 180);","98c7306f":"sns.jointplot(data=hs,x=\"P_incidence\", y=\"L_angle\", hue=\"Class\",kind=\"kde\");","e72b0e45":"plot=sns.jointplot(data=hs,x=\"P_incidence\", y=\"P_tilt\", hue=\"Class\",kind=\"kde\");\n#setting equal magnitude of x and y to get a 1:1 scaled visualisation\nplot.ax_marg_x.set_xlim(0, 140);\nplot.ax_marg_y.set_ylim(-40,100);","59183ab9":"plot=sns.jointplot(data=hs,x=\"P_radius\", y=\"L_angle\", hue=\"Class\",kind=\"kde\");\n#setting equal magnitude of x and y to get a 1:1 scaled visualisation\nplot.ax_marg_x.set_xlim(40,180 )\nplot.ax_marg_y.set_ylim(0, 140);","31da7dcb":"c1.corr()","3ae5bc73":"plt.figure(figsize=(6,3))\nc1.Class.hist();","bef45f45":"sc_X = StandardScaler()\nX =  pd.DataFrame(sc_X.fit_transform(c1.drop([\"Class\"],axis = 1),),\n        columns=['P_incidence', 'P_tilt', 'L_angle', 'S_slope', 'P_radius','S_Degree'])\ny = c1.Class\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1,stratify=y)","5c955dc0":"knn = KNeighborsClassifier(n_neighbors=3)\nknn_model = knn.fit(X_train, y_train,)","62c50173":"y_true, y_pred = y_test, knn_model.predict(X_test)\nprint('k-NN score for training set: %f' % knn_model.score(X_train, y_train))\nprint('k-NN score for test set: %f' % knn_model.score(X_test, y_test))\nprint(classification_report(y_true, y_pred))","08c0e1fa":"# creating odd list of K for KNN\nmyList = list(range(3,20))\n\n# subsetting just the odd ones\nneighbors = list(filter(lambda x: x % 2 != 0, myList))\n\n# empty list that will hold accuracy scores\nac_scores = []\n\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(X_train, y_train)\n    # predict the response\n    y_pred = knn.predict(X_test)\n    # evaluate accuracy\n    scores = accuracy_score(y_test, y_pred)\n    ac_scores.append(scores)\n              \n# changing to misclassification error\nMSE = [1 - x for x in ac_scores]\n\n# determining best k\noptimal_k = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_k)","314bddcd":"## plot misclassification error vs k\nplt.figure(figsize=(6,3))\nplt.plot(neighbors, MSE)\nplt.xlabel('Number of Neighbors K')\nplt.ylabel('Misclassification Error')\nplt.axvline(x=(optimal_k), color='r', linestyle='dotted');\nplt.text(optimal_k,.25, optimal_k,fontdict=font );","1529bf11":"knn = KNeighborsClassifier(n_neighbors=optimal_k)\nknn_model = knn.fit(X_train, y_train,)\nprint('k-NN score for training set: %f' % knn_model.score(X_train, y_train))\nprint('k-NN score for test set: %f' % knn_model.score(X_test, y_test))\nprint(classification_report(y_true, y_pred))","0a9e0c8a":"oversample = SMOTE()\nXs_train,ys_train = oversample.fit_resample(X_train, y_train)","aa1c7f1a":"Xs_test,ys_test = oversample.fit_resample(X_test, y_test)","d5e49b09":"counter = Counter(ys_train)\nprint(counter)","16ee7e3a":"# creating odd list of K for KNN\nmyList = list(range(3,25))\n\n# subsetting just the odd ones\nneighbors = list(filter(lambda x: x % 2 != 0, myList))\n\n# empty list that will hold accuracy scores\nac_scores = []\n\nfor k in neighbors:\n    knn = KNeighborsClassifier(n_neighbors=k)\n    knn.fit(Xs_train, ys_train)\n    # predict the response\n    ys_pred = knn.predict(Xs_test)\n    # evaluate accuracy\n    scores = accuracy_score(ys_test, ys_pred)\n    ac_scores.append(scores)\n              \n# changing to misclassification error\nMSE = [1 - x for x in ac_scores]\n\n# determining best k\noptimal_ks = neighbors[MSE.index(min(MSE))]\nprint(\"The optimal number of neighbors is %d\" % optimal_ks)","79cd6fa1":"knn = KNeighborsClassifier(n_neighbors=optimal_ks)\nsmote_model=knn.fit(Xs_train, ys_train)\n\nys_true, ys_pred = ys_test, knn_model.predict(Xs_test)\nprint('k-NN score for training set: %f' % smote_model.score(Xs_train, ys_train))\nprint('k-NN score for test set: %f' % smote_model.score(Xs_test, ys_test))\nprint(classification_report(ys_true, ys_pred))","e6650308":"#import GridSearchCV\nfrom sklearn.model_selection import GridSearchCV\n#In case of classifier like knn the parameter to be tuned is n_neighbors\nparam_grid = {'n_neighbors':np.arange(1,50)}\nknn = KNeighborsClassifier()\nknn_cv= GridSearchCV(knn,param_grid,cv=5)\nknn_cv.fit(X,y)\n\nprint(\"Best Score:\" + str(knn_cv.best_score_))\nprint(\"Best Parameters: \" + str(knn_cv.best_params_))\n","196202d8":"# Display confusion matrix\nfrom sklearn.metrics import confusion_matrix\nconf_mat = confusion_matrix(y_test, y_pred)\ndf_conf_mat = pd.DataFrame(conf_mat)\nplt.figure(figsize = (10,7))\nsns.heatmap(df_conf_mat, annot=True,cmap='Blues', fmt='g')","c020e221":"**High values of S_slope (Above 80) gets classified as Type S**","8576abdf":"## EDA ","32703c3a":"## Merging All Datasets ","7ef605a6":"While the true positive rate and true negative rate is good there are quite a few misclassification for minority data.<br>\nAs we saw from bivariate and multivariate analysis differentiating between 'Type H' and 'Type S' is easier from the data availabe.<br>\nTo Classify Type N accurately in between the other 2 conditions we will need better features which associate with the class.<br>\nKnn might not be the best approach maybe taking data to a different dimension like svm will be useful to cluster better.\n","94d80cac":"## Find optimal value of k","0a520d89":"**Using best k value has made a significant change.<br>\nModel is predicting better, it is no longer overfitting.Infact test scores are now higher that training scores.<br>\nWeighted average scores for precision and recall has increased to almost 80 percent.<br>\nBut,our individual classes are imbalanced, lets check if blancing input deatures will help in getting higher accuracy**","79b0821b":"**INFERENCE<br>\nP_ Radius : Beyond a threshold of deviation will get patient classifeid from Type H to Type S<br>\nS_Slope is directly prroportional i.e a higher value of S_slope will tend to be classified as Type S**","19cdba0c":"**INFERENCE<br>\n-No coorelation between S_Degree and P_radius<br>\n-Strong coorealtion between S_slope and P_incidence ~80%<br>\n-Similiar strong relation was showed visually earlier between P_incidence and L_angle of ~74%**","1d91ae79":"## Overall co-relation of new combined dataset ","0300b5b1":"**from pairplot it seems like type h and type s patients are opposite to each other\nfor example:type h maybe healthy patients and type s maybe sick.....checking hypothesis by comparing distribution of type h and type s**","da81b431":"The number of type s is more than double of type h","cfa917f7":"**INFERENCE<br>\nHigher L_angle values tend to be of Type S Classification**","90687414":"**Same jointplot without the set of people from normal classification**","97451d5a":"**Remove duplicates**","e71f1bb9":"**INFERENCE<br>\nP_incidence and L_angle have a positive correlation w.r.t each other<br>\nIf both are low : Type H<br>\nIf both are high: Type S**","4239968b":"**Outlier visualised in S_Degree**","38553749":"## Analysing Dataset :Type N","a55826d6":"Gridsearch also does not help refining result, so we will follow the model using best n value","09882f34":"**INFERENCE<br>\nEven after the very high outlier value a direct correlation can be seen between high values of S_Degree and Type S Classification<br>\nConfirming the same with bar plot**","b8a1a5a7":"## DOMAIN: Healthcare","5a8f28c8":"## Creating a function to understand each dataset independently","56b582f5":"## Analysing Dataset :Type H","a6aeae63":"**Precision for classifying minor classes have slightly increased but the F1 scores have reduced.<br>\nWhile a higher precision\/recall is assumed to be a better way to judge this model, this cannot necessarily come at the expense of lower accuracy.**","df380db2":"## Model training, testing and tuning","5ce9ba68":"**Comparing Type H and Type S as they are visually easier to decipher without the normal classfication<br>\nwhich is always overlapping between these two classifications<br>\nFor example consider joint plot with combined dataset**","77273c15":"**INFERENCE<br>\n-No empty row in dataset from count<br>\n-Very high standard deviation fir column S_Degree<br>\n-Max value of S_Degree column confirms something unusual which needs to be further checked<br>\n-The huge gap between 75% and max value of S_Degree indicates some outliers with unusally high value**","1e607841":"**Conclusion**","e1002c09":"## Identifying outlier","76dcf078":"**There is a slight class imbalance which we may need to resolve if it affects our prediction.<br>\nFor now have made stratify=y in train_test_split so that the model can try to accurately map the classification**","ae16fa64":"## Using the same defined function to analyse and label the new combined data.","2d5c96ef":"**Visually confirming successful combination of all 3 datasets into 1**","29514d47":"**Outliers might be present here but a direct correlation can be assumed between S_Degree and Type S Classification**","6aa706e8":"## Using SMOTE to adjust the class imbalance","c59db5e3":"**Import and merge data:**","ef91fbf9":"## Using best value for k in model","f2f241c7":"**As we can see most data would be easily classified using this dataset**","56b4c70e":"**Dropping outlier row with very high max value**","760c42d9":"## Analysing Dataset :Type S","94ca8cda":"Precision - Precision is the ratio of correctly predicted positive observations to the total predicted positive observations. High precision relates to the low false positive rate. Our precision per class is varying due to imbalance in dataset.A weighted average gives a better overall reading, BUt since it is a medical dataset we would like this to go up.\n\nRecall is the ratio of correctly predicted positive observations to the all observations in actual class - yes. For a healthcare dataset its is essential that recall is prioritised and we can get as high a value for recall as possible.\n\nF1 Score is the weighted average of Precision and Recall. Therefore, this score takes both false positives and false negatives into account. F1 is usually more useful than accuracy, especially in an uneven class distribution. Accuracy works best if false positives and false negatives have similar cost. If the cost of false positives and false negatives are very different, it\u2019s better to look at both Precision and Recall\n"}}