{"cell_type":{"134a1f52":"code","f5ae3988":"code","dfd75dfb":"code","31b0402e":"code","8d8ddf0a":"code","ee479846":"code","401bac23":"code","1c8a53f7":"code","a00f2428":"code","1d0ca1cb":"code","4274bca3":"code","19fbf673":"code","82a52ac8":"code","c88004f6":"code","a3b0656d":"code","58a8b982":"code","6e9147b5":"code","2398e078":"code","5fb8da37":"code","17d365b7":"code","503c4ec1":"code","7f08e13d":"code","25eeddb7":"code","3fc0fb39":"code","ea1e8655":"code","05b922c9":"code","df858ae5":"code","51d05975":"code","e7411859":"code","802bd64a":"code","9b88a11d":"code","102113e4":"code","b1506f83":"code","b2b1f365":"code","8044ddef":"code","4c223c41":"code","48416496":"code","fa87de55":"code","85cdca8f":"code","01ea5be4":"markdown","7369c0a8":"markdown"},"source":{"134a1f52":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f5ae3988":"from sklearn.model_selection import train_test_split,cross_val_predict\nfrom sklearn.metrics import log_loss\n\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import AdaBoostClassifier,RandomForestClassifier,GradientBoostingClassifier,BaggingClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\n\nimport xgboost as xgb\nimport lightgbm as lgb\n","dfd75dfb":"train=pd.read_csv(\"\/kaggle\/input\/Train.csv\")\ntest=pd.read_csv(\"\/kaggle\/input\/Test.csv\")","31b0402e":"train.head()","8d8ddf0a":"train.isnull().sum()","ee479846":"train.describe()","401bac23":"train[\"labels\"].value_counts().plot.bar()","1c8a53f7":"del train[\"feature_12\"]\ndel train[\"feature_10\"]\n","a00f2428":"del train[\"feature_5\"]\ndel train[\"feature_6\"]","1d0ca1cb":"import seaborn as sns\nimport matplotlib.pyplot as plt\nplt.figure(figsize=(20,20))\ncorr = train.corr()\nsns.heatmap(corr,annot=True)","4274bca3":"features = list(set(train.columns)-set(['labels']))\ntarget = 'labels'\nlen(features)","19fbf673":"from sklearn.metrics import f1_score ","82a52ac8":"def metric(y,y0):\n    return f1_score(y,y0)\n","c88004f6":"def cross_valid(model,train,features,target,cv=3):\n    results = cross_val_predict(model, train[features], train[target], method=\"predict\",cv=cv)\n    return metric(train[target],results)\n\n","a3b0656d":"models = [lgb.LGBMClassifier(), xgb.XGBClassifier(), GradientBoostingClassifier()\n             ]\n\nfor i in models:\n    model = i\n    error = cross_valid(model,train,features,target,cv=10)\n    print(str(model).split(\"(\")[0], error)","58a8b982":"train.head()","6e9147b5":"y=train[\"labels\"]\ndel train[\"labels\"]\nx=train","2398e078":"def xgb_model(x,y, plot=True):\n    evals_result = {}\n    trainX, validX, trainY, validY = train_test_split(x,y, test_size=0.2, random_state=13)\n    print(\"XGB Model\")\n    \n    dtrain = xgb.DMatrix(trainX, label=trainY)\n    dvalid = xgb.DMatrix(validX, label=validY)\n    watchlist = [(dtrain, 'train'), (dvalid, 'valid')]\n    \n    MAX_ROUNDS=2000\n    early_stopping_rounds=100\n    params = {\n        'booster': 'gbtree',\n        'objective': 'multi:softprob',\n        'eval_metric': 'mlogloss',\n        'learning_rate': 0.01,\n        'num_round': MAX_ROUNDS,\n        'max_depth': 8,\n        'seed': 25,\n        'nthread': -1,\n        'num_class':5\n    }\n    \n    model = xgb.train(\n        params,\n        dtrain,\n        evals=watchlist,\n        num_boost_round=MAX_ROUNDS,\n        early_stopping_rounds=early_stopping_rounds,\n        verbose_eval=50\n        #feval=metric_xgb\n    \n    )\n    \n    print(\"Best Iteration :: {} \\n\".format(model.best_iteration))\n    \n    \n    if plot:\n        # Plotting Importances\n        fig, ax = plt.subplots(figsize=(24, 24))\n        xgb.plot_importance(model, height=0.4, ax=ax)\n","5fb8da37":"xgb_model(x,y,plot=True)","17d365b7":"xgb1 = xgb.XGBClassifier(\n    booster='gbtree',\n    objective='multi:softprob',\n    learning_rate= 0.1,\n    num_round= 1149,\n    max_depth=8,\n    seed=25,\n    nthread=-1,\n    eval_metric='mlogloss',\n    num_class=5\n\n)","503c4ec1":"from imblearn.over_sampling import SMOTE\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.3, random_state=0)\n\nprint(\"X_train dataset: \", X_train.shape)\nprint(\"y_train dataset: \", y_train.shape)\nprint(\"X_test dataset: \", X_test.shape)\nprint(\"y_test dataset: \", y_test.shape)","7f08e13d":"print(\"Before OverSampling, counts of label '1': {}\".format(sum(y_train==1)))\nprint(\"Before OverSampling, counts of label '0': {} \\n\".format(sum(y_train==0)))\n\nsm = SMOTE(random_state=2)\nX_train_res, y_train_res = sm.fit_sample(X_train, y_train.ravel())\n\nprint('After OverSampling, the shape of train_X: {}'.format(X_train_res.shape))\nprint('After OverSampling, the shape of train_y: {} \\n'.format(y_train_res.shape))\n\nprint(\"After OverSampling, counts of label '1': {}\".format(sum(y_train_res==1)))\nprint(\"After OverSampling, counts of label '0': {}\".format(sum(y_train_res==0)))\n","25eeddb7":"xgb1.fit(X_train_res,y_train_res)\npred=xgb1.predict(X_test)\nprint(f1_score(pred,y_test))","3fc0fb39":"def learning_rate_010_decay_power_099(current_iter):\n    base_learning_rate = 0.1\n    lr = base_learning_rate  * np.power(.99, current_iter)\n    return lr if lr > 1e-3 else 1e-3\n\ndef learning_rate_010_decay_power_0995(current_iter):\n    base_learning_rate = 0.1\n    lr = base_learning_rate  * np.power(.995, current_iter)\n    return lr if lr > 1e-3 else 1e-3\n\ndef learning_rate_005_decay_power_099(current_iter):\n    base_learning_rate = 0.05\n    lr = base_learning_rate  * np.power(.99, current_iter)\n    return lr if lr > 1e-3 else 1e-3\n","ea1e8655":"import lightgbm as lgb\nfit_params={\"early_stopping_rounds\":30, \n            \"eval_metric\" : 'auc', \n            \"eval_set\" : [(X_test,y_test)],\n            'eval_names': ['valid'],\n            #'callbacks': [lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_099)],\n            'verbose': 100,\n            'categorical_feature': 'auto'}\n","05b922c9":"from scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nparam_test ={'num_leaves': sp_randint(6, 50), \n             'min_child_samples': sp_randint(100, 500), \n             'min_child_weight': [1e-5, 1e-3, 1e-2, 1e-1, 1, 1e1, 1e2, 1e3, 1e4],\n             'subsample': sp_uniform(loc=0.2, scale=0.8), \n             'colsample_bytree': sp_uniform(loc=0.4, scale=0.6),\n             'reg_alpha': [0, 1e-1, 1, 2, 5, 7, 10, 50, 100],\n             'reg_lambda': [0, 1e-1, 1, 5, 10, 20, 50, 100]}","df858ae5":"n_HP_points_to_test = 100\n\nimport lightgbm as lgb\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\n#n_estimators is set to a \"large value\". The actual number of trees build will depend on early stopping and 5000 define only the absolute maximum\nclf = lgb.LGBMClassifier(max_depth=-1, random_state=314, silent=True, metric='None', n_jobs=4, n_estimators=5000)\ngs = RandomizedSearchCV(\n    estimator=clf, \n    param_distributions=param_test, \n    n_iter=n_HP_points_to_test,\n    scoring='accuracy',\n    cv=3,\n    refit=True,\n    random_state=314,\n    verbose=True)","51d05975":"gs.fit(X_train_res, y_train_res, **fit_params)\nprint('Best score reached: {} with params: {} '.format(gs.best_score_, gs.best_params_))","e7411859":"opt_parameters =  {'colsample_bytree': 0.404828563763895, 'min_child_samples': 350, 'min_child_weight': 0.001, 'num_leaves': 36, 'reg_alpha': 1, 'reg_lambda': 5, 'subsample': 0.9274016358891894} \n","802bd64a":"clf_sw = lgb.LGBMClassifier(**clf.get_params())\n#set optimal parameters\nclf_sw.set_params(**opt_parameters)","9b88a11d":"gs_sample_weight = GridSearchCV(estimator=clf_sw, \n                                param_grid={'scale_pos_weight':[1,2,6,12]},\n                                scoring='roc_auc',\n                                cv=5,\n                                refit=True,\n                                verbose=True)\n\n","102113e4":"gs_sample_weight.fit(X_train_res, y_train_res, **fit_params)\nprint('Best score reached: {} with params: {} '.format(gs_sample_weight.best_score_, \n                                                       gs_sample_weight.best_params_))","b1506f83":"\nclf_final = lgb.LGBMClassifier(**clf.get_params())\n#set optimal parameters\nclf_final.set_params(**opt_parameters)\n#Train the final model with learning rate decay\nclf_final.fit(X_train_res, y_train_res, **fit_params, callbacks=[lgb.reset_parameter(learning_rate=learning_rate_010_decay_power_0995)])\n\n","b2b1f365":"pred=clf_final.predict(X_test)\nprint(f1_score(pred,y_test))","8044ddef":"del test[\"feature_5\"]\ndel test[\"feature_6\"]\ndel test[\"feature_12\"]\ndel test[\"feature_10\"]\n","4c223c41":"predf=xgb1.predict(test)","48416496":"sub=pd.read_excel(\"\/kaggle\/input\/sample_submission.xlsx\")\nsub[\"labels\"]=predf","fa87de55":"sub.to_excel(\"s1.xlsx\",index=False)","85cdca8f":"from IPython.display import FileLink\nFileLink(r's1.xlsx')","01ea5be4":"**LIGHT GBM**","7369c0a8":"**Testing**"}}