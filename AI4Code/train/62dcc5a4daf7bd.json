{"cell_type":{"f178df02":"code","df3276dd":"code","ee281e0a":"code","7305a33b":"code","b2f4dfae":"code","1e7b87f6":"code","8d0be1ac":"code","1e4040e0":"code","2d42198e":"code","482eb918":"code","8d45f424":"code","cdb52a73":"code","dbb31b83":"code","9260b0c5":"code","d32e4cfa":"code","0a2526fa":"code","f19baa2f":"code","648b2b39":"code","45966422":"code","61e971e8":"code","7187b0e2":"code","687b285b":"code","326c1e0f":"code","edd469ac":"code","bb45b6ee":"code","e04fbd50":"code","1aad0029":"code","5200bd97":"code","61a94a81":"code","80dd9c3f":"code","a00d4340":"code","904b944d":"code","225d4e68":"code","6fcd32ae":"code","532ed949":"code","2190cade":"code","f82560b3":"code","abe8c6a5":"code","fe7f9504":"code","1e880202":"code","01d11cee":"code","76950718":"code","b6b2152d":"code","331f0b26":"code","c16b5f1a":"code","3eeed364":"code","f3f0cb22":"code","d21ad241":"code","079fa493":"code","56693e05":"code","91c1e277":"code","55dbc949":"code","5ad7d6c7":"code","b36ce97c":"code","3f5beb9c":"code","993b3c10":"code","521f735c":"code","c056db84":"code","6d3f5028":"code","fa7025be":"code","5a97f99c":"code","4d577e07":"code","e2a2f158":"code","639bad98":"code","31e1f9d2":"code","ea148e90":"code","3cddaef2":"code","2b0874ff":"code","3d25b696":"markdown","f9576e66":"markdown","e652b120":"markdown","295a5af7":"markdown","a02d538a":"markdown","0182e8d2":"markdown","2fdf0272":"markdown","7b7d3ec0":"markdown","e9795c92":"markdown","d18067ae":"markdown","4eca5783":"markdown","1d95cc89":"markdown","2040b538":"markdown","2cb19f86":"markdown","5612187f":"markdown","43939b4a":"markdown","d46487d3":"markdown","46b0da00":"markdown","9091b274":"markdown","55374db9":"markdown","a232e19c":"markdown","4a0ff6a7":"markdown","741e4ef7":"markdown","fcb70b2b":"markdown","46495018":"markdown","9dd477ee":"markdown","e961b1c7":"markdown","2348a129":"markdown","893b7582":"markdown"},"source":{"f178df02":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport scipy.stats as stats\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom IPython.display import Image\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","df3276dd":"# df = pd.read_csv('\/kaggle\/input\/sf-salaries\/Salaries.csv')\ndf = pd.read_csv('\/kaggle\/input\/sf-salaries\/Salaries.csv\/Salaries.csv')\ndf.head()","ee281e0a":"df.columns","7305a33b":"df.dtypes","b2f4dfae":"df.info()","1e7b87f6":"df.describe(include='all')","8d0be1ac":"df.head()","1e4040e0":"from fuzzywuzzy import process\nstr2Match = \"police\"\nstrOptions = \"CAPTAIN III (POLICE DEPARTMENT)\".split()\nRatios = process.extract(str2Match,strOptions)\n# print(Ratios)\n# You can also select the string with the highest matching percentage\nhighest = process.extractOne(str2Match,strOptions)\nprint(highest[1])","2d42198e":"from fuzzywuzzy import process\ndef fuzzy_job_field(row):\n    strOptions = row.lower().split()\n    for field, field_key in all_career.items():\n        for key in field_key:\n            highest= process.extractOne(key,strOptions)\n            if highest[1] > 90:\n                return field\n    return \"Other\"\n\ndef fuzzy_job_level(row):\n    strOptions = row.lower().split()\n    for field, field_key in all_career.items():\n        for key in field_key:\n            highest= process.extractOne(key,strOptions)\n            if highest[1] > 90:\n                return field\n    return \"Staff\"\n\n\n#Map \u0111\u1ec3 Extract\n# df['Career'] = df['JobTitle'].map(fuzzy_job_field)\n# df['Level'] = df['JobTitle'].map(fuzzy_job_level)\ndf.head()","482eb918":"all_career = dict({\n    'Fire': ['fire'],\n    'Police': ['police', 'sherif', 'probation', 'sergeant', 'officer', 'lieutenant'],\n    'Transit': ['mta', 'transit', 'truck'],\n    'Medical': ['anesth', 'medical', 'nurs', 'health', 'physician',\n             'orthopedic', 'health', 'pharm', 'care'],\n    'Airport': ['airport'],\n    'Animal': ['animal'],\n    'Architectural': ['architect'],\n    'Court': ['court', 'legal'],\n    'Mayor': ['mayor'],\n    'Library': ['librar'],\n    'Parking': ['parking'],\n    'Public Works': ['public works'],\n    'Attorney': ['attorney'],\n    'Mechanic': ['mechanic', 'automotive'],\n    'Custodian': ['custodian'],\n    'Engineering': ['engineer', 'engr', 'eng', 'program'],\n    'Accounting': ['account'],\n    'Gardening': ['gardener'],\n    'General Laborer': ['general laborer', 'painter', 'inspector',\n                     'carpenter', 'electrician', 'plumber', 'maintenance',\n                        'custodian', 'garden', 'guard', 'clerk', 'porter'],\n    'Food Service': ['food serv'],\n    'Clerk': ['clerk'],\n    'Porter': ['porter'],\n    'Aide': ['aide', 'assistant', 'secretary', 'attendant'],\n    'Data': ['analyst', 'data'],\n    'Airport': ['airport'],\n    'Architect': ['architect'],\n    'Accountant': ['Accountant'],\n    'Mayoral': ['mayoral'],\n    'Recreation': ['recreation'], \n    'Admin': ['Admin', 'account'], \n    'Lawyer': ['attorney', 'lawyer'],\n    'Public Service': ['public service', 'Social Worker'],\n    'Food Service': ['food serv'],\n    'Not provided':['not provide']\n})\n\nall_level = dict({\n    'Manager': ['manager', 'chief'],\n    'Senior': ['senior'],\n    'Junior': ['Junior'],\n    'Trainee': ['trainee'],\n    'Not provided':['not provide']\n})\n\ndef find_job_field(row):\n    for field, field_key in all_career.items():\n        for key in field_key:\n            if key in row.lower():\n                return field\n    return \"Other\"\n\ndef find_job_level(row):\n    for field, field_key in all_level.items():\n        for key in field_key:\n            if key in row.lower():\n                return field\n    return \"Staff\"\n\ndef fuzzy_job_field(row):\n    strOptions = row.lower().split()\n    for field, field_key in all_career.items():\n        for key in field_key:\n            highest= process.extractOne(key,strOptions)\n            if highest[1] > 90:\n                return field\n    return \"Other\"\n\ndef fuzzy_job_level(row):\n    strOptions = row.lower().split()\n    for field, field_key in all_career.items():\n        for key in field_key:\n            highest= process.extractOne(key,strOptions)\n            if highest[1] > 90:\n                return field\n    return \"Staff\"\n\n#Map \u0111\u1ec3 Extract\ndf['Career'] = df['JobTitle'].map(find_job_field)\ndf['Level'] = df['JobTitle'].map(find_job_level)\n# df['Career'] = df['JobTitle'].map(fuzzy_job_field)\n# df['Level'] = df['JobTitle'].map(fuzzy_job_level)\n\n# df[df['JobTitle'].str.lower().str.contains('food serv')].JobTitle\n\ndf.head()","8d45f424":"all_pay_columns = ['BasePay', 'OvertimePay', 'OtherPay', 'Benefits',\n                   'TotalPay', 'TotalPayBenefits']\n\npay_columns = ['BasePay', 'OvertimePay', 'OtherPay', 'Benefits']","cdb52a73":"#Lo\u1ea1i b\u1ecf nh\u1eefng c\u1ed9t m\u00e0 \u0111\u01b0\u1ee3c khai l\u00e0 \"Not provide\" (kh\u00f4ng c\u00f3 gi\u00e1 tr\u1ecb g\u00ec)\nprint('Number of Not Provided ', df[(df=='Not provided').any(axis=1)].shape[0])\ndf.drop(df[(df=='Not provided').any(axis=1)].index, inplace=True)","dbb31b83":"# convert the pay columns to numeric\nfor col in all_pay_columns:\n    df[col] = pd.to_numeric(df[col], errors='coerce')","9260b0c5":"df.describe()","d32e4cfa":"print('Total sample with Negative value is ', df[(df[pay_columns] < 0).any(axis=1)].shape[0])\ndf[(df[all_pay_columns] < 0).any(axis=1)].head()\n","0a2526fa":"print('Data shape before remove Negative sample ', df.shape)\ndf.drop(df[(df[all_pay_columns] < 0).any(axis=1)].index, inplace=True)\nprint('Data shape after remove Negative sample ', df.shape)\n    \n","f19baa2f":"df.describe()","648b2b39":"df.isnull().sum()","45966422":"is_null = df.isnull().sum()\nis_null = is_null[is_null>0]\nis_null.sort_values(inplace=True, ascending=False)\n\n#missing data\ntotal = is_null\npercent = is_null\/len(df) * 100\n\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\n# plot missing data percent again\n\nprint(missing_data.index)\nplt.figure(figsize=(13, 5))\nsns.set(style='whitegrid')\ng = sns.barplot(x=missing_data.index, y='Percent', data=missing_data)\n\nplt.xticks(rotation = 90)\nplt.title(\"Actual Percentage of missing values.\")\nplt.xticks(rotation=45)\n\n#plot value on top of bar\nfor p in range(len(missing_data)):\n  value = missing_data.iloc[p, 1]\n  g.text(p, value, f'{value:1.2f}%', color='black', ha=\"center\")\n\nplt.show()","61e971e8":"df[\"BasePay\"].fillna(df.groupby(\"Career\")[\"BasePay\"].transform(\"median\"), inplace=True)\ndf[\"Benefits\"].fillna(df.groupby(\"Career\")[\"Benefits\"].transform(\"median\"), inplace=True)","7187b0e2":"df['Career'].isnull().sum()","687b285b":"is_null = df.isnull().sum()\nis_null = is_null[is_null>0]\nis_null.sort_values(inplace=True, ascending=False)\n\n#missing data\ntotal = is_null\npercent = is_null\/len(df) * 100\n\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n\n# plot missing data percent again\n\nprint(missing_data.index)\nplt.figure(figsize=(13, 5))\nsns.set(style='whitegrid')\ng = sns.barplot(x=missing_data.index, y='Percent', data=missing_data)\n\nplt.xticks(rotation = 90)\nplt.title(\"Actual Percentage of missing values.\")\nplt.xticks(rotation=45)\n\n#plot value on top of bar\nfor p in range(len(missing_data)):\n  value = missing_data.iloc[p, 1]\n  g.text(p, value, f'{value:1.2f}%', color='black', ha=\"center\")\n\nplt.show()","326c1e0f":"all_pay_columns","edd469ac":"# fig, axes = plt.subplots(6, 1, figsize=(8,24))\n\n# for i in range(6):\n#     df[all_pay_columns[i]].hist(bins=100, ax = axes[i])\n#     axes[i].set_title(all_pay_columns[i])\n\n# plt.show()","bb45b6ee":"sns.pairplot(df[pay_columns])\n# sns.pairplot(df[pay_columns], kind='reg')","e04fbd50":"df_sal = df[all_pay_columns]\n# df_sal = df[pay_columns]\n\n\n# Scale the data using the natural logarithm\ndf_log_sal = np.log1p(df_sal)\n\n# Produce a scatter matrix for each pair of newly-transformed features\n# sns.pairplot(df[pay_columns], kind='reg')\n# sns.pairplot(df_log_sal, diag_kind = 'kde', kind='reg')\n\nfig, axes = plt.subplots(6, 2, figsize=(12,24))\nfor i in range(6):\n    df[all_pay_columns[i]].hist(bins=100, ax = axes[i, 0])\n    df_log_sal[all_pay_columns[i]].hist(bins=100, ax = axes[i, 1])\n#     axes[i, 0].set_title(all_pay_columns[i])\n#     axes[i, 1].set_title(all_pay_columns[i])\n    axes[i, 0].set_xlabel(f'Original {all_pay_columns[i]}')\n    axes[i, 1].set_xlabel(f'Log {all_pay_columns[i]}')\n\nplt.show()","1aad0029":"# For each feature find the data points with extreme high or low values\nimport collections \n\noutliers_index_all = []\ndf_find_outliers = df_log_sal\nfor feature in df_find_outliers.keys():\n    \n    # TODO: CalAculate Q1 (25th percentile of the data) for the given feature\n    Q1 = df_find_outliers[feature].quantile(0.25)\n    \n    # TODO: Calculate Q3 (75th percentile of the data) for the given feature\n    Q3 = df_find_outliers[feature].quantile(0.75)\n    \n    # TODO: Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n    step = (Q3 - Q1) * 1.5\n    \n    # Display the outliers\n    feature_outliers = df_find_outliers[~((df_find_outliers[feature] >= Q1 - step) & (df_find_outliers[feature] <= Q3 + step))]\n    print(f\"{feature_outliers.shape[0]} Data points considered outliers for the feature '{feature}':\")\n#     display(feature_outliers)\n   \n# OPTIONAL: Select the indices for data points you wish to remove\n    outliers_index_all  += feature_outliers.index.tolist()\n\nprint(\"\\nTotal outliers are\" , len(set(outliers_index_all)))\n# print(\"\\nFollowing index were found as outliers in more than one features\")\n# print([(item,count) for item, count in collections.Counter(outliers_index_all).items() if count > 1])\n\noutliers_index = list(set(outliers_index_all))\n# Remove the outliers, if any were specified\ndf_log_sal_no_outlier = df_find_outliers.drop(outliers_index) #.reset_index(drop = True)","5200bd97":"# For each feature find the data points with extreme high or low values\nimport collections \n\noutliers_index_all = []\ndf_find_outliers = df_sal\nfor feature in df_find_outliers.keys():\n    \n    # TODO: CalAculate Q1 (25th percentile of the data) for the given feature\n    Q1 = df_find_outliers[feature].quantile(0.25)\n    \n    # TODO: Calculate Q3 (75th percentile of the data) for the given feature\n    Q3 = df_find_outliers[feature].quantile(0.75)\n    \n    # TODO: Use the interquartile range to calculate an outlier step (1.5 times the interquartile range)\n    step = (Q3 - Q1) * 1.5\n    \n    # Display the outliers\n    feature_outliers = df_find_outliers[~((df_find_outliers[feature] >= Q1 - step) & (df_find_outliers[feature] <= Q3 + step))]\n    print(f\"{feature_outliers.shape[0]} Data points considered outliers for the feature '{feature}':\")\n#     display(feature_outliers)\n   \n# OPTIONAL: Select the indices for data points you wish to remove\n    outliers_index_all  += feature_outliers.index.tolist()\n\nprint(\"\\nTotal outliers are\" , len(set(outliers_index_all)))\n# print(\"\\nFollowing index were found as outliers in more than one features\")\n# print([(item,count) for item, count in collections.Counter(outliers_index_all).items() if count > 1])\n\noutliers_index = list(set(outliers_index_all))\n# Remove the outliers, if any were specified\ndf_sal_no_outlier = df_find_outliers.drop(outliers_index) #.reset_index(drop = True)","61a94a81":"fig, axes = plt.subplots(6, 2, figsize=(12,24))\nfor i in range(6):\n    df_sal[all_pay_columns[i]].hist(bins=100, ax = axes[i, 0])\n    df_sal_no_outlier[all_pay_columns[i]].hist(bins=100, ax = axes[i, 1])\n#     axes[i, 0].set_title(all_pay_columns[i])\n#     axes[i, 1].set_title(all_pay_columns[i])\n    axes[i, 0].set_xlabel(f'Original {all_pay_columns[i]}')\n    axes[i, 1].set_xlabel(f'Removed outlier {all_pay_columns[i]}')\n\nplt.show()","80dd9c3f":"fig, axes = plt.subplots(6, 2, figsize=(12,24))\nfor i in range(6):\n    df_log_sal[all_pay_columns[i]].hist(bins=100, ax = axes[i, 0])\n    df_log_sal_no_outlier[all_pay_columns[i]].hist(bins=100, ax = axes[i, 1])\n#     axes[i, 0].set_title(all_pay_columns[i])\n#     axes[i, 1].set_title(all_pay_columns[i])\n    axes[i, 0].set_xlabel(f'Log of {all_pay_columns[i]}')\n    axes[i, 1].set_xlabel(f'Removed outlier Log of {all_pay_columns[i]}')\n\nplt.show()","a00d4340":"from sklearn.decomposition import PCA","904b944d":"df_clean = df.loc[df_log_sal_no_outlier.index]\ndf_model = df_clean[pay_columns]","225d4e68":"# Apply PCA by fitting the good data with only two dimensions\ndf_pca = df_model\npca = PCA(n_components=2).fit(df_pca)\n\n# Transform the good data using the PCA fit above\nreduced_data = pca.transform(df_pca)\n\n# Create a DataFrame for the reduced data\nreduced_data = pd.DataFrame(reduced_data, columns = ['Dimension 1', 'Dimension 2'], index=df_pca.index)\n\n# Generate PCA results plot\npca_table = pd.DataFrame(np.round(pca.components_, 4), columns = list(df_pca.keys()))\nprint('% Variance ', pca.explained_variance_ratio_.cumsum())\npca_table.plot.bar(figsize=(15,8))","6fcd32ae":"def biplot(good_data, reduced_data, pca, feature_scale_ratio = 1):\n    '''\n    Produce a biplot that shows a scatterplot of the reduced\n    data and the projections of the original features.\n    \n    good_data: original data, before transformation.\n               Needs to be a pandas dataframe with valid column names\n    reduced_data: the reduced data (the first two dimensions are plotted)\n    pca: pca object that contains the components_ attribute\n\n    return: a matplotlib AxesSubplot object (for any additional customization)\n    \n    This procedure is inspired by the script:\n    https:\/\/github.com\/teddyroland\/python-biplot\n    '''\n\n    fig, ax = plt.subplots(figsize = (14,8))\n    # scatterplot of the reduced data    \n    ax.scatter(x=reduced_data.loc[:, 'Dimension 1'], y=reduced_data.loc[:, 'Dimension 2'], \n        facecolors='b', edgecolors='b', s=70, alpha=0.5)\n    \n    feature_vectors = pca.components_.T * feature_scale_ratio\n\n    # we use scaling factors to make the arrows easier to see\n    arrow_size, text_pos = 7.0, 8.0,\n\n    # projections of the original features\n    for i, v in enumerate(feature_vectors):\n        ax.arrow(0, 0, arrow_size*v[0], arrow_size*v[1], \n                  head_width=0.2, head_length=0.2, linewidth=2, color='red')\n        ax.text(v[0]*text_pos, v[1]*text_pos, good_data.columns[i], color='black', \n                 ha='center', va='center', fontsize=18)\n\n    ax.set_xlabel(\"Dimension 1\", fontsize=14)\n    ax.set_ylabel(\"Dimension 2\", fontsize=14)\n    ax.set_title(\"PC plane with original feature projections.\", fontsize=16);\n    return ax","532ed949":"biplot(df_pca, reduced_data, pca, 10000)","2190cade":"plt.figure(figsize=(16,5))\nsns.countplot('Career', data = df, order = df['Career'].value_counts().index)\nplt.xticks(rotation = 45)\nplt.tight_layout()","f82560b3":"plt.figure(figsize=(16,5))\nsns.countplot('Career', data = df, order = df['Career'].value_counts().index, hue = 'Year')\nplt.xticks(rotation = 45)\nplt.tight_layout()","abe8c6a5":"plt.figure(figsize=(12,5))\nsns.barplot(data= df.groupby('Career')['TotalPayBenefits'].agg('median').reset_index(), x ='Career', y = 'TotalPayBenefits')\nplt.xticks(rotation=85)\nplt.title('Mean Pay')","fe7f9504":"df_input = df.loc[df_log_sal_no_outlier.index]\ntop_ten_occupations = df_input['Career'].value_counts().sort_values(ascending=False)[:10].index\ntop_ten_occupations","1e880202":"salaries_averages_by_occupation = (df_input[df_input.Career.isin(top_ten_occupations)]\n                                   .groupby('Career')[pay_columns]\n                                   .aggregate('mean')).sort_values('BasePay', 0)\n\nax = salaries_averages_by_occupation.plot(kind='barh', figsize=(8,8))\n\nax.set_xlabel('Mean Pay of Top 10 Polular Career ')","01d11cee":"salaries_averages_by_occupation = (df_input[df_input.Career.isin(top_ten_occupations)]\n                                   .groupby('Career')[pay_columns]\n                                   .aggregate('median')).sort_values('BasePay', 0)\n\nax = salaries_averages_by_occupation.plot(kind='barh', figsize=(8,8))\n\nax.set_xlabel('Median Pay of Top 10 Polular Career ')","76950718":"salaries_averages_by_occupation = (df_input[df_input.Career.isin(top_ten_occupations)]\n                                   .groupby('Career')[pay_columns]\n                                   .aggregate('min')).sort_values('BasePay', 0)\n\nax = salaries_averages_by_occupation.plot(kind='barh', figsize=(8,8))\n\nax.set_xlabel('Min Pay of Top 10 Polular Career ')","b6b2152d":"salaries_averages_by_occupation = (df_input[df_input.Career.isin(top_ten_occupations)]\n                                   .groupby('Career')[pay_columns]\n                                   .aggregate('max')).sort_values('BasePay', 0)\n\nax = salaries_averages_by_occupation.plot(kind='barh', figsize=(8,8))\n\nax.set_xlabel('Max Pay of Top 10 Polular Career ')","331f0b26":"salaries_averages_by_occupation = (df_input[df_input.Career.isin(top_ten_occupations)]\n                                   .groupby('Career')[pay_columns]\n                                   .aggregate('var')).sort_values('BasePay', 0)\n\nax = salaries_averages_by_occupation.plot(kind='barh', figsize=(8,8))\n\nax.set_xlabel('Var Pay of Top 10 Polular Career ')","c16b5f1a":"salaries_averages_by_occupation = (df_input[df_input.Career.isin(top_ten_occupations)]\n                                   .groupby('Career')[pay_columns]\n                                   .aggregate('mean')).sort_values('BasePay', 0)\n\nax.set_xlabel('Mean Pay of Top 10 Polular Career ')\n# the above graph can be transformed into a proportions stacked bar graph\n\n# use the dataframe method div to proportionalize the values by axis=0(row)\nsalary_percents = salaries_averages_by_occupation.div(salaries_averages_by_occupation.sum(1), \n                                                      axis=0)\n\n# and plot the bar graph with a stacked argument.  \nax = salary_percents.plot(kind='bar', stacked=True, rot=90)","3eeed364":"salaries_averages_by_occupation = (df_input[df_input.Career.isin(top_ten_occupations)]\n                                   .groupby('Career')[pay_columns]\n                                   .aggregate('median')).sort_values('BasePay', 0)\n\nax.set_xlabel('Mean Pay of Top 10 Polular Career ')\n# the above graph can be transformed into a proportions stacked bar graph\n\n# use the dataframe method div to proportionalize the values by axis=0(row)\nsalary_percents = salaries_averages_by_occupation.div(salaries_averages_by_occupation.sum(1), \n                                                      axis=0)\n\n# and plot the bar graph with a stacked argument.  \nax = salary_percents.plot(kind='bar', stacked=True, rot=90)","f3f0cb22":"salaries_averages_by_occupation = (df_input[df_input.Career.isin(top_ten_occupations)]\n                                   .groupby('Career')[pay_columns]\n                                   .aggregate('mean')).sort_values('BasePay', 0)\n\nax = salaries_averages_by_occupation.plot(kind='barh', figsize=(8,8))\n\nax.set_xlabel('Mean Pay of Top 10 Polular Career ')","d21ad241":"sal_feauture = 'BasePay'\ngroup_data = df[df['Career'] == 'Data'][sal_feauture]\ngroup_2 = df[df['Career'] == 'Engineering'][sal_feauture]\n\nprint(\"Data group vs Engineering group\")\nstats.ttest_ind(group_data, group_2)","079fa493":"sal_feauture = 'TotalPay'\ngroup_data = df[df['Career'] == 'Data'][sal_feauture]\ngroup_medical = df[df['Career'] == 'Medical'][sal_feauture]\n\nprint(\"Data group vs Medical group\")\n\nstats.ttest_ind(group_data, group_medical)","56693e05":"sal_feauture = 'TotalPay'\ngroup_1 = df[df['Career'] == 'Police'][sal_feauture]\ngroup_2 = df[df['Career'] == 'Fire'][sal_feauture]\n\n#Null\nprint(\"Police group vs Fire group\")\nstats.ttest_ind(group_1, group_2)","91c1e277":"sal_feauture = 'BasePay'\ngroup_1 = df[df['Career'] == 'Data'][sal_feauture]\ngroup_2 = df[df['Career'] == 'Aide'][sal_feauture]\n\nprint(\"Data group vs Aide group\")\nstats.ttest_ind(group_1, group_2)","55dbc949":"df.columns","5ad7d6c7":"career_group = []\ntemp = top_ten_occupations\n# temp = df['Career'].unique()\nsal_feauture = 'TotalPay'\nfor i in temp:\n    df_temp = df[df['Career']== i]\n#     df_temp['Overtime\/Total'] = df_temp['OvertimePay'] \/ df_temp['TotalPay']\n    result = df_temp['TotalPay'].dropna()\n    career_group.append(result)","b36ce97c":"stats.levene(*career_group)","3f5beb9c":"stats.f_oneway(*career_group)","993b3c10":"!pip install kneed","521f735c":"from sklearn.cluster import KMeans\nfrom scipy.spatial.distance import cdist\nfrom kneed import KneeLocator\n\n# k means determine k\n# distortions = []\n# inertia = []\n# K = range(1,10)\n# X = df_model\n# for k in K:\n#     kmeanModel = KMeans(n_clusters=k).fit(X)\n#     kmeanModel.fit(X)\n#     #append error\n#     distortions.append(sum(np.min(cdist(X, kmeanModel.cluster_centers_, 'euclidean'), axis=1)) \/ X.shape[0])\n#     inertia.append(kmeanModel.inertia_)\n\n# kn = KneeLocator(list(K), distortions, S=1.0, curve='convex', direction='decreasing')\n# kn2 = KneeLocator(list(K), inertia, S=1.0, curve='convex', direction='decreasing')\n\n# # Plot the elbow\n# fig, (ax1, ax2) = plt.subplots(1,2, figsize=(16,4))\n# fig.suptitle('The Elbow Method showing the optimal k')\n\n# ax1.plot(K, distortions, 'bx-')\n# ax1.set_xlabel('k')\n# ax1.set_ylabel('Distortion')\n# ax1.vlines(kn.knee, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')\n\n# ax2.plot(K, distortions, 'bx-')\n# ax2.set_xlabel('k')\n# ax2.set_ylabel('Distortion')\n# ax2.vlines(kn2.knee, plt.ylim()[0], plt.ylim()[1], linestyles='dashed')\n\n# print(\"Number of clusters by distortions\", kn.knee)\n# print(\"Number of clusters by inertia\", kn2.knee)","c056db84":"url = '\/kaggle\/input\/sf-kmean\/kmean1_9.png'\nImage(url)","6d3f5028":"url = '\/kaggle\/input\/sf-kmean\/kmean10_15.png'\nImage(url)","fa7025be":"kmeans_centers","5a97f99c":"df_input = df_model\n\nkmeans = KMeans(n_clusters=4)\nkmeans.fit(df_input)\n\nkmeans_centers = pd.DataFrame(kmeans.cluster_centers_, columns=df_input.columns)\nkmeans_centers_reduced = pd.DataFrame(pca.transform(kmeans_centers), columns = ['Dimension 1', 'Dimension 2'])\nkmeans_centers = pd.concat([kmeans_centers, kmeans_centers_reduced], axis=1)\n\nlabels = pd.DataFrame(kmeans.labels_, columns=['Label'], index = df_input.index)\ncareer_labels = df.loc[df_input.index]['Career']\n\nsal_centers = kmeans_centers\nclustered_data = pd.concat([df_input, reduced_data, career_labels, labels], axis=1)\n\ndisplay(sal_centers.head())\nclustered_data.head()","4d577e07":"f, (ax1, ax2) = plt.subplots(1, 2, sharey=True,figsize=(10,6))\n\nax1.set_title('K Means')\nsns.scatterplot(x='Dimension 1', y='Dimension 2',hue = 'Label',data=clustered_data, ax =ax1)\nax1.scatter(x='Dimension 1', y='Dimension 2', marker=\"X\", c='r', s = 100, data=kmeans_centers)\n\nax2.set_title(\"Original\")\nsns.scatterplot(x='Dimension 1', y='Dimension 2',hue = 'Career',data=clustered_data, ax =ax2)","e2a2f158":"sal_centers","639bad98":"display(sal_centers.head())\n#BasePay very High, OT Average\n#BasePay High, OT High\n#BasePay Average, OT Average\n#BasePay Low, OT Low","31e1f9d2":"fig, axes = plt.subplots(2, 2, figsize=(12,8))\nfor i in range(2):\n    df[pay_columns[i]].hist(bins=100, ax = axes[i, 0])\n    df_input[pay_columns[i]].hist(bins=100, ax = axes[i, 1])\n#     axes[i, 0].set_title(all_pay_columns[i])\n#     axes[i, 1].set_title(all_pay_columns[i])\n    axes[i, 0].set_xlabel(f'Original {all_pay_columns[i]}')\n    axes[i, 1].set_xlabel(f'Removed outlier Log of {all_pay_columns[i]}')\n\naxes[1,0].set_xlim([0, 40000])\naxes[1,1].set_xlim([0, 40000])\n\nplt.show()","ea148e90":"from sklearn.mixture import GaussianMixture\nfrom sklearn.metrics import silhouette_score\n\ndf_input = df_model\nGM_clusterer = GaussianMixture(4, random_state=0).fit(df_input)\n# GM_clusterer.fit(df_input)\n\nGM_centers = pd.DataFrame(GM_clusterer.means_, columns=df_input.columns)\nGM_centers_reduced = pd.DataFrame(pca.transform(GM_centers), columns = ['Dimension 1', 'Dimension 2'])\nGM_centers = pd.concat([GM_centers, GM_centers_reduced], axis=1)\n\nGM_labels = pd.DataFrame(GM_clusterer.predict(df_input), columns=['GM_labels'], index = df_input.index)\n\nclustered_data = pd.concat([clustered_data, GM_labels], axis=1)\n\ndisplay(GM_centers.head())\nclustered_data.head()","3cddaef2":"f, (ax1, ax2) = plt.subplots(1, 2, sharey=True,figsize=(10,6))\n\nax1.set_title('GM')\nsns.scatterplot(x='Dimension 1', y='Dimension 2',hue = 'GM_labels',data=clustered_data, ax =ax1)\nax1.scatter(x='Dimension 1', y='Dimension 2', marker=\"X\", c='r', s = 100, data=GM_centers)\n\nax2.set_title(\"Original\")\nsns.scatterplot(x='Dimension 1', y='Dimension 2',hue = 'Career',data=clustered_data, ax =ax2)","2b0874ff":"# from sklearn.mixture import GaussianMixture\n# from sklearn.metrics import silhouette_score\n\n# def clusterGMM(input_df, k):\n#     global clusterer, preds, centers, sample_preds\n#     # Apply your GMM algorithm to the reduced data \n#     clusterer = GaussianMixture(k, random_state=0).fit(input_df)\n\n#     # Predict the cluster for each data point\n#     preds = clusterer.predict(input_df)\n\n#     # Find the cluster centers\n#     centers = clusterer.means_\n\n#     # Predict the cluster for each transformed sample data point\n# #     sample_preds = clusterer.predict(pca_samples)\n\n#     # Calculate the mean silhouette coefficient for the number of clusters chosen\n#     score = silhouette_score(input_df, preds)\n#     return score\n\n# results = pd.DataFrame(columns=['Silhouette Score'])\n# results.columns.name = 'Number of Clusters'\n\n# for k in range(2,16):\n#     score = clusterGMM(reduced_data, k)\n#     results = results.append(pd.DataFrame([score], columns=['Silhouette Score'], index=[k]))\n# results\n","3d25b696":"This data contains the names, job title, and compensation for San Francisco city employees on an annual basis from 2011 to 2014.","f9576e66":"# Pre Process data","e652b120":"# Remove Outlier","295a5af7":"EDA\n#Job Title c\u00f3 2159 Unique value\nIdea: ph\u00e2n th\u00e0nh c\u00e1c nh\u00f3m ng\u00e0nh c\u01a1 b\u1ea3n : Finance, Police, Engineer, IT, ... 10 ng\u00e0nh\n\n#T\u00ecm dictionary c\u00e1c ng\u00e0nh c\u01a1 b\u1ea3n \u0111\u1ec3 x\u1eed l\u00fd string c\u1ed9t Jobtitle (Th\u1ea7y khuy\u00ean d\u00f9ng Fuzzy text search)\n\n1. S\u1ef1 kh\u00e1c bi\u1ec7t v\u1ec1 l\u01b0\u01a1ng gi\u1eefa c\u00e1c ng\u00e0nh => C\u00f3 th\u1eadt l\u00e0 c\u00f3 ng\u00e0nh hot h\u01a1n\n2. Ph\u00e2n b\u1ed1 l\u01b0\u01a1ng c\u1ee7a t\u1eebng ng\u00e0nh (Min, max, median, distribution) => ng\u00e0nh n\u00e0o d\u1ec5 l\u01b0\u01a1ng cao. Level c\u00f3 quy\u1ebft \u0111\u1ecbnh\n3. C\u01a1 c\u1ea5u l\u01b0\u01a1ng c\u1ee7a ng\u00e0nh. => bi\u1ebft \u0111\u1ec3 n\u00e9\n4. Model -> cluster\n\nFuzzy text searching","a02d538a":"# Missing data","0182e8d2":"# There are many non-number value in Pay columns => Convert them to numberic","2fdf0272":"# Distribution before & after removing outlier Of Original Data & Log Data","7b7d3ec0":"# Because there are only 21 sample with negative value -> remove them all.","e9795c92":"# GaussianMixture","d18067ae":"# Cluster by Salary","4eca5783":"# Fillna by Group by median of each Career Group","1d95cc89":"This is a two-sided test for the null hypothesis that 2 independent samples have identical average (expected) values. This test assumes that the populations have identical variances by default.","2040b538":"# Check NA value","2cb19f86":"# T Test","5612187f":"# Extract Jobtitle to Career and Level to get more features","43939b4a":"# top_ten_occupations & df_log_sal_no_outlier","d46487d3":"# Read the data and first looking","46b0da00":"# We see that Log data is closer to Normal Distribution => use Log Data for DA","9091b274":"# We will not use Notes and Status for EDA. => Finished remove NA","55374db9":"df\n\ndf_sal & df_sal_no_outlier\n\ndf_log_sal & df_log_sal_no_outlier","a232e19c":"# PCA by Salary\n","4a0ff6a7":"# Insight","741e4ef7":"# K mean","fcb70b2b":"# BasePay va Benefit c\u00f3 high correlation","46495018":"# Anova","9dd477ee":"# We see that some Pay value is negative -> let's check them","e961b1c7":"# Use Log Data without Outlier for DA","2348a129":"# There 4 sample which are \"Not Provided\" => remove them","893b7582":"# Histogram of all pay"}}