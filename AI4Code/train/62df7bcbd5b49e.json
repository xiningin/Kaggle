{"cell_type":{"8cf25b0f":"code","bc47e8fd":"code","358b28bf":"code","96c32658":"code","285838cc":"code","7c3bd616":"code","a981da80":"code","dd3d68d6":"code","6f4b9e4d":"code","3e77b94a":"code","8f84e115":"code","48300c2f":"code","70718f8e":"code","96cfc0e3":"code","c5163cab":"code","8966341b":"code","c0168cf4":"code","a242b8b2":"code","cda7843c":"code","dae53abd":"code","6c657fab":"code","867362fa":"code","333eaab2":"code","6df2b862":"code","70d3f963":"code","71e16edc":"code","00f2e254":"code","a651faab":"code","f290d390":"code","cc3c3c09":"code","791be16c":"code","49b92cfa":"code","fec4cdf0":"code","1e21fc32":"code","194108c7":"code","5a2e2afe":"code","85c775eb":"code","3ef47004":"code","2c231b7c":"code","624201ba":"code","c287b119":"code","b81e7e60":"code","11321bb2":"code","ba8c6b24":"code","ba37c3ea":"code","cfa92369":"code","67259a7c":"code","baae2dbb":"code","eeb9f356":"code","4ca65a4b":"code","91b667ed":"code","d88d8653":"code","0eace84c":"code","6e43a65e":"code","679607c5":"code","2f68bcdc":"code","bfe03d64":"code","e2e9075e":"code","89261677":"code","1cd500f5":"code","3c509394":"code","5e3a7c28":"code","fe38cf3d":"code","beb2556b":"code","46579288":"code","2b7b1b4a":"code","22608e40":"code","0264f998":"code","f76bf9c2":"code","a44b99cd":"code","353cc822":"code","61bcb760":"code","38295396":"code","e72a091a":"code","0bc8f85b":"code","05573f18":"markdown","516cf125":"markdown","9b87add7":"markdown","bbcfb06b":"markdown","d2cacfdb":"markdown","a252b31d":"markdown","6b0ff173":"markdown","f17a4d09":"markdown","df7789d8":"markdown","30e49dd2":"markdown","44bf163a":"markdown","0598ec35":"markdown","76fc2f8c":"markdown","1724c216":"markdown","24ce5877":"markdown","423ad105":"markdown","210412f2":"markdown","a1dd7f4a":"markdown","80f6a1d6":"markdown","27774020":"markdown","6931e01e":"markdown","e022f0f5":"markdown","7e7738bb":"markdown","1e586ee4":"markdown","33ec0c39":"markdown","b5983836":"markdown"},"source":{"8cf25b0f":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","bc47e8fd":"data = pd.read_csv('\/kaggle\/input\/womens-ecommerce-clothing-reviews\/Womens Clothing E-Commerce Reviews.csv',index_col =[0])","358b28bf":"data.head(2)","96c32658":"data.shape","285838cc":"data.isnull().sum()\/len(data)*100","7c3bd616":"data.info()","a981da80":"data.drop(labels =['Clothing ID','Title'],axis = 1,inplace = True) #Dropping unwanted columns","dd3d68d6":"data[data['Review Text'].isnull()]","6f4b9e4d":"data = data[~data['Review Text'].isnull()]  #Dropping columns which don't have any review","3e77b94a":"data.shape","8f84e115":"import plotly.express as px","48300c2f":"px.histogram(data, x = 'Age')","70718f8e":"px.histogram(data, x = data['Rating'])","96cfc0e3":"px.histogram(data, x = data['Class Name'])","c5163cab":"px.scatter(data, x=\"Age\", y=\"Positive Feedback Count\", facet_row=\"Recommended IND\", facet_col=\"Rating\",trendline=\"ols\",category_orders={\"Rating\": [1,2,3,4,5],'Recommended IND':[0,1]})","8966341b":"px.violin(data, x=\"Age\", y=\"Department Name\", orientation=\"h\", color=\"Recommended IND\")","c0168cf4":"px.box(data, x=\"Age\", y=\"Division Name\", orientation=\"h\",color = 'Recommended IND')","a242b8b2":"err1 = data['Review Text'].str.extractall(\"(&amp)\")\nerr2 = data['Review Text'].str.extractall(\"(\\xa0)\")","cda7843c":"print('with &amp',len(err1[~err1.isna()]))\nprint('with (\\xa0)',len(err2[~err2.isna()]))","dae53abd":"data['Review Text'] = data['Review Text'].str.replace('(&amp)','')\ndata['Review Text'] = data['Review Text'].str.replace('(\\xa0)','')","6c657fab":"err1 = data['Review Text'].str.extractall(\"(&amp)\")\nprint('with &amp',len(err1[~err1.isna()]))\nerr2 = data['Review Text'].str.extractall(\"(\\xa0)\")\nprint('with (\\xa0)',len(err2[~err2.isna()]))","867362fa":"!pip install TextBlob\nfrom textblob import *","333eaab2":"data['polarity'] = data['Review Text'].map(lambda text: TextBlob(text).sentiment.polarity)","6df2b862":"data['polarity']","70d3f963":"px.histogram(data, x = 'polarity')","71e16edc":"px.box(data, y=\"polarity\", x=\"Department Name\", orientation=\"v\",color = 'Recommended IND')","00f2e254":"data['review_len'] = data['Review Text'].astype(str).apply(len)","a651faab":"px.histogram(data, x = 'review_len')","f290d390":"data['token_count'] = data['Review Text'].apply(lambda x: len(str(x).split()))","cc3c3c09":"px.histogram(data, x = 'token_count')","791be16c":"sam = data.loc[data.polarity == 1,['Review Text']].sample(3).values","49b92cfa":"for i in sam:\n    print(i[0])","fec4cdf0":"sam = data.loc[data.polarity == 0.5,['Review Text']].sample(3).values","1e21fc32":"for i in sam:\n    print(i[0])","194108c7":"sam = data.loc[data.polarity < 0,['Review Text']].sample(3).values","5a2e2afe":"for i in sam:\n    print(i[0])","85c775eb":"negative = (len(data.loc[data.polarity <0,['Review Text']].values)\/len(data))*100\npositive = (len(data.loc[data.polarity >0.5,['Review Text']].values)\/len(data))*100\nneutral  = len(data.loc[data.polarity >0 ,['Review Text']].values) - len(data.loc[data.polarity >0.5 ,['Review Text']].values)\nneutral = neutral\/len(data)*100","3ef47004":"from matplotlib import pyplot as plt \nplt.figure(figsize =(10, 7)) \nplt.pie([positive,negative,neutral], labels = ['Positive','Negative','Neutral']) ","2c231b7c":"from sklearn.feature_extraction.text import CountVectorizer","624201ba":"def top_n_ngram(corpus,n = None,ngram = 1):\n    vec = CountVectorizer(stop_words = 'english',ngram_range=(ngram,ngram)).fit(corpus)\n    bag_of_words = vec.transform(corpus) #Have the count of  all the words for each review\n    sum_words = bag_of_words.sum(axis =0) #Calculates the count of all the word in the whole review\n    words_freq = [(word,sum_words[0,idx]) for word,idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq,key = lambda x:x[1],reverse = True)\n    return words_freq[:n]","c287b119":"common_words = top_n_ngram(data['Review Text'], 20,1)\ndf = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\nplt.figure(figsize =(10,5))\ndf.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\nkind='bar', title='Top 20 unigrams in review after removing stop words')","b81e7e60":"common_words = top_n_ngram(data['Review Text'], 20,2)\ndf = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\nplt.figure(figsize =(10,5))\ndf.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\nkind='bar', title='Top 20 bigrams in review after removing stop words')","11321bb2":"common_words = top_n_ngram(data['Review Text'], 20,3)\ndf = pd.DataFrame(common_words, columns = ['ReviewText' , 'count'])\nplt.figure(figsize =(10,5))\ndf.groupby('ReviewText').sum()['count'].sort_values(ascending=False).plot(\nkind='bar', title='Top 20 trigrams in review after removing stop words')","ba8c6b24":"blob= TextBlob(str(data['Review Text']))\npos = pd.DataFrame(blob.tags,columns =['word','pos'])\npos1 = pos.pos.value_counts()[:20]\nplt.figure(figsize = (10,5))\npos1.plot(kind='bar',title ='Top 20 Part-of-speech taggings')","ba37c3ea":"y = data['Recommended IND']","cfa92369":"X = data.drop(columns = 'Recommended IND')","67259a7c":"import seaborn as sns\nsns.heatmap(X.corr(),annot =True)","baae2dbb":"set1 =set()\ncor = X.corr()\nfor i in cor.columns:\n    for j in cor.columns:\n        if cor[i][j]>0.8 and i!=j:\n            set1.add(i)\nprint(set1)","eeb9f356":"X = X.drop(labels = ['token_count'],axis = 1)","4ca65a4b":"X.corr()","91b667ed":"class1 =[]\nfor i in X.polarity:\n    if float(i)>=0.0:\n        class1.append(1)\n    elif float(i)<0.0:\n        class1.append(0)\nX['sentiment'] = class1","d88d8653":"X.groupby(X['sentiment']).describe().T","0eace84c":"import nltk\nimport re\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer","6e43a65e":"corpus =[]","679607c5":"X.index = np.arange(len(X))","2f68bcdc":"for i in range(len(X)):\n    review = re.sub('[^a-zA-z]',' ',X['Review Text'][i])\n    review = review.lower()\n    review = review.split()\n    ps = PorterStemmer()\n    review =[ps.stem(i) for i in review if not i in set(stopwords.words('english'))]\n    review =' '.join(review)\n    corpus.append(review)","bfe03d64":"from sklearn.feature_extraction.text import CountVectorizer as CV\ncv  = CV(max_features = 3000,ngram_range=(1,1))\nX_cv = cv.fit_transform(corpus).toarray()\ny = y.values","e2e9075e":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_cv, y, test_size = 0.20, random_state = 0)","89261677":"from sklearn.naive_bayes import BernoulliNB\nclassifier = BernoulliNB()\nclassifier.fit(X_train, y_train)","1cd500f5":"y_pred = classifier.predict(X_test)\nfrom sklearn.metrics import accuracy_score\nacc = accuracy_score(y_test, y_pred)","3c509394":"acc","5e3a7c28":"from sklearn.feature_extraction.text import TfidfVectorizer as TV\ntv  = TV(ngram_range =(1,1),max_features = 3000)\nX_tv = tv.fit_transform(corpus).toarray()","fe38cf3d":"X_train, X_test, y_train, y_test = train_test_split(X_tv, y, test_size = 0.20, random_state = 0)\nfrom sklearn.naive_bayes import MultinomialNB\nclassifier = MultinomialNB()\nclassifier.fit(X_train, y_train)","beb2556b":"y_pred = classifier.predict(X_test)\nacc = accuracy_score(y_test, y_pred)","46579288":"acc","2b7b1b4a":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences","22608e40":"tokenizer = Tokenizer(num_words = 3000)\ntokenizer.fit_on_texts(corpus)","0264f998":"sequences = tokenizer.texts_to_sequences(corpus)\npadded = pad_sequences(sequences, padding='post')","f76bf9c2":"word_index = tokenizer.word_index\ncount = 0\nfor i,j in word_index.items():\n    if count == 11:\n        break\n    print(i,j)\n    count = count+1","a44b99cd":"embedding_dim = 64\nmodel = tf.keras.Sequential([\n    tf.keras.layers.Embedding(3000, embedding_dim),\n    tf.keras.layers.GlobalAveragePooling1D(),\n    tf.keras.layers.Dense(6, activation='relu'),\n    tf.keras.layers.Dense(1, activation='sigmoid')\n])\n\nmodel.summary()","353cc822":"num_epochs = 10\n\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","61bcb760":"model.fit(padded,y,epochs= num_epochs)","38295396":"sample_string = \"I Will tell my friends for sure\"\nsample = tokenizer.texts_to_sequences(sample_string)\npadded_sample = pad_sequences(sample, padding='post')","e72a091a":"padded_sample.T","0bc8f85b":"model.predict(padded_sample.T)","05573f18":"I am very glad to welcome you all to my notebook, In this notebook I'm going to work on a text classification problem. The problem is described as **'Given Review about clothing on E-commerce predict whether the custoomer will recommed it to her friends or not'...**<br\/><br\/>\nI need to mention one thing I'm new to Machine Learning with text and I have used very easy and effective approaches to solve this problem. I have done some data analysis, data visualizations then finally build both machine learning and deep learning models.\nIt's time to jump on the process but before that I will mention the Workflow:<br\/><br\/>\n**1)Loading the data<br\/>\n2)Handling Missing Values<br\/>\n3)Cleaning the data<br\/>\n4)Data Analysis and Visualization<br\/>\n5)Handling MultiColinearity<br\/>\n6)Tokenisation+stemming+corpus creation<br\/>\n7)Buidling ML model using Bag of words<br\/>\n8)Building ML model using Tf-Idf Vectoriztion<br\/>\n9)Deep Learning Model with Embeddings<br\/>\n10)Checking the model with new example**","516cf125":"# Visualizing Top 20 Part-of-Speech","9b87add7":"# Data Analysis and Visualization","bbcfb06b":"# Visualizing Top 20 Bigrams","d2cacfdb":"# <center> Text Classification with womens-ecommerce-clothing-reviews data set<\/center>","a252b31d":"**That's all for this notebook....See you soon!!!**<br\/>\n*If you like my work upvote it!!!!!*","6b0ff173":"# Loading the Data","f17a4d09":"# Checking For Missing Values and Handling it","df7789d8":"# RE + Tokenizing + Stemming + Corpus Creation","30e49dd2":"# Bag of Words Technique","44bf163a":"# Visualizing Top 20 Trigrams","0598ec35":"# Cleaning the Text Data","76fc2f8c":"# Term Frequency- Inverse Document Frequency Technique","1724c216":"# Handling Multi-Colinearity","24ce5877":"# Statistical Description of Data","423ad105":"CHECKING NEW EXAMPLE","210412f2":"# Deep Learning Model","a1dd7f4a":"# Visualizing Top 20 Unigrams","80f6a1d6":"![image.png](attachment:image.png)","27774020":"# Pie-Chart about Polarity","6931e01e":"# Model Building","e022f0f5":"# Reviews with Neutral Polarity","7e7738bb":"# Correlation HeatMap","1e586ee4":"# Reviews with Positive Polarity","33ec0c39":"# Reviews with Negative Polarity","b5983836":"![image.png](attachment:image.png)"}}