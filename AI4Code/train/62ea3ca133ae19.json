{"cell_type":{"d3faa28c":"code","e9212e01":"code","d1c5f785":"code","4169fd78":"code","83580f80":"code","3f64bc01":"code","e1f49fa7":"code","3ffee0fc":"code","83521b11":"code","4a389665":"code","2299a7ee":"markdown","a088eeca":"markdown"},"source":{"d3faa28c":"# LOAD PACKAGES\nimport os\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nimport tensorflow as tf\nfrom tensorflow.keras import activations, regularizers, Sequential, utils, callbacks, optimizers\nfrom tensorflow.keras.layers import Flatten, Dense, BatchNormalization, Activation, Dropout","e9212e01":"# LOAD DATA\nBASEPATH = '..\/input\/digit-recognizer\/'\ndef reload(df='train.csv', msg=True, path=BASEPATH):\n    o = pd.read_csv(BASEPATH + df)\n    print(f'{df} loaded!')\n    return o\ntrain = reload()\ny_train = train.label.copy()\ny_train = utils.to_categorical(y_train, 10)\nX_train = train.drop(columns='label')\ndef preprocess(df):\n    df = df \/ 256.0\n    df = df.values.reshape(-1, 28, 28)\n    return df\nX_train = preprocess(X_train)\ntest = reload('test.csv')\nX_test = preprocess(test)","d1c5f785":"# PREPROCESS DATA\nVAL_SIZE = 0.2\nBATCH = 30\nX_tr, X_val, y_tr, y_val = train_test_split(X_train, y_train, test_size=VAL_SIZE, random_state=713)\n# Transform data to tf.data format in case we decide to ues TPU later\ntrain_set = tf.data.Dataset.from_tensor_slices((X_tr, y_tr)).batch(BATCH)\nval_set = tf.data.Dataset.from_tensor_slices((X_val, y_val)).batch(BATCH)\ntest_set = tf.data.Dataset.from_tensor_slices((X_test,))","4169fd78":"# MODEL SETUP\n# I've tried my best to labeled parts where I implemented the tricks mentioned\n# above, but feel free to drop any questions you may have\nL1 = 0\nL2 = 0.01\nINI_LR = 0.001\nEPOCHS = 100\nTRAIN_SIZE = 42000\nsched_lr_val_acc = False # Because SELU activation doesn't accept Exponential Scheduling\n\n# Regularizers\ndef get_regularizer(l1=L1, l2=L2):\n    return regularizers.l1_l2(l1=l1, l2=l2)\n\n# Learning rate scheduling and Adam optimization technique\ndef get_optimizer(lr=INI_LR, beta_1=0.9, beta_2=0.999):\n    # Learning rate scheduling\n    if sched_lr_val_acc == False:\n        lr = optimizers.schedules.ExponentialDecay(\n            lr\n            , decay_steps=500\n            , decay_rate=0.96\n        )\n    # Adam optimizer\n    return optimizers.Adam(lr, beta_1=beta_1, beta_2=beta_2)\n\ndef get_callbacks(monitor='val_accuracy', min_delta=0.001\n                  , patience_es=10, patience_lr=5):\n    cb = []\n    # Early stopping\n    cb.append(callbacks.EarlyStopping(\n        monitor=monitor, min_delta=min_delta, patience=patience_es\n        , restore_best_weights=True\n    ))\n    # Another learning rate schedule that lowers learning rate\n    # using validation set's accuracy instead of automatically\n    # reducing learning rate after each step. Try searching\n    # ``ReduceLROnPlateau`` for more information\n    if sched_lr_val_acc:\n        cb.append(callbacks.ReduceLROnPlateau(\n            monitor=monitor, factor=0.5, patience=patience_lr\n            , min_delta=min_delta, min_lr=INI_LR*0.1**5\n        ))\n    # This is to shut down model in case of gradient explosion\n    cb.append(callbacks.TerminateOnNaN())\n    return cb    \n\ndef get_model(compiling=True, activation='selu', layers=[300, 100, 50], p=0.2):\n    model = Sequential([Flatten(input_shape=(28, 28))\n                       , BatchNormalization()\n                       , Dropout(rate=p)])\n    regu = get_regularizer()\n    # Non-saturating activation function: SELU\n    if activation == 'selu':\n        initializer = 'lecun_normal' # SELU works best with Lecun's normal initialization\n    elif activation in ['relu', 'elu']:\n        initializer = 'he_normal' # while RELU and ELU works best with He's normal initialization\n    else:\n        initializer = 'glorot_normal' # Other functions (Tanh, Sigmoid, etc.) works best\n                                      # with the default Glorot's normal initialization\n    for u in layers:\n        model.add(Dense(units = u\n                        , use_bias = False\n                        , kernel_initializer = initializer\n                        , kernel_regularizer = regu\n                       ))\n        # Batch Normalization\n        model.add(BatchNormalization())\n        model.add(Activation(activation))\n        # Dropout\n        model.add(Dropout(rate=p))\n    model.add(Dense(10, activation='softmax'))\n    if compiling:\n        opt = get_optimizer()\n        model.compile(optimizer=opt, loss='categorical_crossentropy', metrics=['accuracy'])\n    return model","83580f80":"# tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n# tf.config.experimental_connect_to_cluster(tpu)\n# tf.tpu.experimental.initialize_tpu_system(tpu)\n# tpu_strategy = tf.distribute.experimental.TPUStrategy(tpu)\n# VERBOSE = 1\n# with tpu_strategy.scope():\n#     NN = get_model(False)\n\n# opt = get_optimizer()\n# NN.compile(optimizer=opt, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])","3f64bc01":"VERBOSE = 1\nmodel = get_model()\ncb = get_callbacks()\nhistory = model.fit(train_set, epochs=EPOCHS\n                    , validation_data=(X_val, y_val), verbose=VERBOSE\n                    , callbacks=cb)","e1f49fa7":"perf = pd.DataFrame(history.history)\nplt.style.use('seaborn')\nperf.plot(y=['accuracy', 'val_accuracy'], figsize=(10,7))\nplt.xlabel('Epoch')\nplt.ylabel('Accuracy')\nplt.title('Training and validation accuracy')\nplt.show()","3ffee0fc":"y_val_pred = model.predict(X_val)\ny_val_pred = np.argmax(y_val_pred, axis=1)\ny_val_act = np.argmax(y_val, axis=1)\nprediction = np.equal(y_val_act, y_val_pred)\n\ndef plot_predictions(correct_pred=True, ncol=6, nrow=3):\n    plt.figure(figsize=(2*ncol,2.5*nrow))\n    index = np.where(prediction == correct_pred)\n    for i in np.arange(ncol*nrow):\n        plt.subplot(nrow, ncol, i+1)\n        plt.imshow(X_val[index][i], interpolation='nearest')\n        plt.axis('off')\n        title = ''\n        if correct_pred == False:\n            title =  f'- Actual {y_val_act[index][i]}'\n        plt.title(f'Pred {y_val_pred[index][i]}'+title)\n    suptitle = 'Accurate ' if correct_pred else 'Inaccurate '\n    plt.suptitle(suptitle + 'predictions', size=17)\n\nplot_predictions()","83521b11":"plot_predictions(False)","4a389665":"y_pred = model.predict(X_test)\ny_pred = np.argmax(y_pred, axis=1)\nsubmission = reload('sample_submission.csv')\nsubmission['Label'] = y_pred\nsubmission.to_csv('submission.csv', index=False)","2299a7ee":"This is a short notebook where I use simple-to-implement tricks that can greatly improve your current model. These tricks have been widely used in both state-of-the-art architectures and basic \"hello world\" tutorials. They are below (together with resources\/original paper that will provide you further information on the topic):\n- L1 and L2 regularizations\n- Learning rate scheduling ([exponential scheduling](https:\/\/static.googleusercontent.com\/media\/research.google.com\/vi\/\/pubs\/archive\/40808.pdf), to be specific)\n- More efficient (than SGD) optimizer ([Adam optimizer](https:\/\/arxiv.org\/abs\/1412.6980))\n- Early stopping\n- [Batch Normalization](https:\/\/arxiv.org\/abs\/1502.03167)\n- [Dropout](https:\/\/arxiv.org\/abs\/1207.0580)\n- Non-saturating activation functions ([SELU](https:\/\/arxiv.org\/abs\/1706.02515)) and their appropriate initialization methods (`lecun_normal` for `SELU`)","a088eeca":"Please drop any comment you may have! Cheers everyone!"}}