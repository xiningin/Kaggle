{"cell_type":{"c35cbcce":"code","86a1f161":"code","5d880241":"code","c6f12aeb":"code","0955a9c0":"code","dc42536b":"code","b3ab65d7":"code","cfeed664":"code","aaab6842":"code","8898e2ca":"code","e6723e8b":"code","3df040aa":"code","71a9d961":"code","9b35ce79":"code","b3861405":"code","5a8ac065":"code","7d3627e4":"code","0a8171b3":"code","2ebd39e6":"code","db6ca5da":"code","4f42c1db":"code","baf53dbe":"code","2327ce4e":"code","28b379c4":"code","d04bd521":"code","cef9ab3a":"code","e69e39d7":"code","08998557":"code","a65ed82d":"code","de8ce7e1":"code","8d6cb0b7":"code","563188b3":"code","daab2c08":"code","7e3af6bc":"code","18c11a5c":"code","c5c412cb":"code","ba51de53":"code","adeca5f2":"code","ac766025":"code","f6b96ac4":"code","787b0ff1":"code","e9ba5fbd":"code","81c32639":"code","2badbda3":"code","1c6762aa":"code","a21c4eef":"code","4be3f2ad":"code","8ec99a86":"code","ee3a9472":"code","3035fa97":"code","34200458":"code","e73a5d4c":"code","34627b91":"markdown","fca18e4c":"markdown","ac7d9676":"markdown"},"source":{"c35cbcce":"from tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nimport pandas as pd\nimport numpy as np\nimport tensorflow.keras.utils as ku\nfrom transformers import TFAutoModel, AutoTokenizer\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, LSTM, Bidirectional, Embedding\nfrom keras.utils import np_utils\nfrom tensorflow.keras.utils import to_categorical\nfrom keras.callbacks import ModelCheckpoint\nimport re\nimport tensorflow as tf\nimport h5py","86a1f161":"df = pd.read_csv('..\/input\/all-trumps-twitter-insults-20152021\/trump_insult_tweets_2014_to_2021.csv',index_col='Unnamed: 0')\ndf.describe()","5d880241":"df['clean_tweets'] = df['tweet'].apply(lambda x: re.sub(r'http\\S+', '', str(x)))","c6f12aeb":"clean_tweets = np.array(df['clean_tweets'][df['clean_tweets'] != ''].unique())","0955a9c0":"np.array([len(tweet) for tweet in clean_tweets]).argmin()","dc42536b":"clean_tweets = [x.replace('\"','') for x in clean_tweets]","b3ab65d7":"bert = TFAutoModel.from_pretrained(\"bert-base-cased\")\ntokenizer = AutoTokenizer.from_pretrained(\"bert-base-cased\")","cfeed664":"SEQ_LEN = 60","aaab6842":"def tokenize(sentence):\n    tokens = tokenizer.encode_plus(sentence, max_length=SEQ_LEN,\n                                   truncation=True, padding='max_length',\n                                   add_special_tokens=True, return_attention_mask=True,\n                                   return_token_type_ids=False, return_tensors='tf')\n    return tokens['input_ids'], tokens['attention_mask']","8898e2ca":"# tokenizer = Tokenizer()\n# corpus = clean_tweets\n# tokenizer.fit_on_texts(corpus)\n# total_words = len(tokenizer.word_index) + 1\n# print(total_words)","e6723e8b":"token_list = tokenize(clean_tweets[100])\nprint(token_list)\nprint([clean_tweets[100]])","3df040aa":"# initialize two arrays for input tensors\nXids = np.zeros((len(clean_tweets), SEQ_LEN))\nXmask = np.zeros((len(clean_tweets), SEQ_LEN))\n\n# loop through data and tokenize everything\nfor i, tweet in enumerate(clean_tweets):\n    Xids[i, :], Xmask[i, :] = tokenize(tweet)\n\nprint(Xids, Xmask)\n","71a9d961":"\n# seq_len = 5\nX_data = []\ny_data = []\ninput_sequences = []\ninput_masks = []\n\n# initialize two arrays for input tensors\n# Xids = np.zeros((len(clean_tweets), SEQ_LEN))\n# Xmask = np.zeros((len(clean_tweets), SEQ_LEN))\n\n# loop through data and tokenize everything\n# for i, tweet in enumerate(clean_tweets):\n#     Xids[i, :], Xmask[i, :] = tokenize(tweet)\n\nfor tweet in clean_tweets:\n    token_list, mask_list = tokenize(tweet)\n    for i in range(1, SEQ_LEN):\n        n_gram_sequence = token_list[0][:i+1]\n        input_sequences.append(n_gram_sequence)\n        input_masks.append(mask_list[0][:i+1])\n","9b35ce79":"input_masks","b3861405":"# pad sequences \ninput_sequences = np.array(pad_sequences(input_sequences, maxlen=SEQ_LEN, padding='pre'))\ninput_masks = np.array(pad_sequences(input_masks, maxlen=SEQ_LEN, padding='pre'))","5a8ac065":"predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\npred_masks = input_masks[:,:-1]","7d3627e4":"drop_list = []\n\nfor idx, val in enumerate(label):\n    if val == 0:\n        drop_list.append(idx)\n        \nprint(len(drop_list))","0a8171b3":"# keep_list = [i for i in range(label.shape[0]) if i not in drop_list]","2ebd39e6":"# hf = h5py.File('keeplist.h5', 'w')\n# hf.create_dataset('keeplist', data=keep_list)\n# hf.close()\n# hf.create_dataset('dataset_2', data=d2)\nhf = h5py.File('.\/keeplist.h5', 'r')\nkeep_list = np.array(hf.get('keeplist'))\nhf.close()","db6ca5da":"predictors = predictors[keep_list]\nlabel = label[keep_list]\npred_masks = pred_masks[keep_list]\nprint(predictors.shape, label.shape)","4f42c1db":"#del keep_list\ndel input_sequences\ndel input_masks\ndel clean_tweets\ndel Xids","baf53dbe":"label = ku.to_categorical(label)\nlabel.shape","2327ce4e":"# create tensorflow dataset object\ndataset = tf.data.Dataset.from_tensor_slices((predictors[:50000, :], pred_masks[:50000, :], label[:50000, :]))","28b379c4":"# restructure dataset format for BERT\ndef map_func(input_ids, masks, labels):\n    return {'input_ids': input_ids, 'attention_mask': masks}, labels\n","d04bd521":"dataset = dataset.map(map_func)  # apply the mapping function\n\n# shuffle and batch the dataset\ndataset = dataset.shuffle(100).batch(32)","cef9ab3a":"DS_LEN = len(list(dataset))  # get dataset length\n\nSPLIT = 0.9  # we will create a 90-10 split\n\n# create training-validation sets\ntrain = dataset.take(round(DS_LEN*SPLIT))\nval = dataset.skip(round(DS_LEN*SPLIT))","e69e39d7":"# build the model\ninput_ids = tf.keras.layers.Input(shape=(59,), name='input_ids', dtype='int32')\nmask = tf.keras.layers.Input(shape=(59,), name='attention_mask', dtype='int32')","08998557":"embeddings = bert(input_ids, attention_mask=mask)[0]  # we only keep tensor 0 (last_hidden_state)","a65ed82d":"#n_patterns = len(X_data) #157566 seq_length = 5\n#pd.DataFrame(y_data).iloc[:,0].sort_values().unique()\n","de8ce7e1":"# X = np.reshape(X_data, (n_patterns, seq_len, 1))\n# X = X\/float(len(tokenizer.word_index.items()))\n# y = np.asarray(pd.get_dummies(y_data))\n","8d6cb0b7":"#X.shape, y.shape, len(tokenizer.word_index.items())","563188b3":"# print(y[0][1529])\n# len(X_data[0])","daab2c08":"# X.shape\n# embeddings.shape","7e3af6bc":"# X = tf.keras.layers.GlobalMaxPool1D()(embeddings)  # reduce tensor dimensionality\nX = tf.keras.layers.BatchNormalization()(embeddings)\nprint(X.shape)\nX = tf.keras.layers.Bidirectional(LSTM(128))(X)\nX = tf.keras.layers.Dropout(0.1)(X)\ny = tf.keras.layers.Dense(label.shape[1], activation='softmax', name='outputs')(X)  # adjust based on number of sentiment classes\n\nmodel = tf.keras.Model(inputs=[input_ids, mask], outputs=y)","18c11a5c":"model.summary()","c5c412cb":"label.shape","ba51de53":"# model = Sequential()\n\n# model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n\n# model.add(Bidirectional(LSTM(256, return_sequences=True)))\n# model.add(Dropout(0.2))\n# model.add(LSTM(256))\n# model.add(Dropout(0.2))\n# #model.add(LSTM(128))\n# #model.add(Dropout(0.2))\n# model.add(Dense(total_words, activation='softmax')) #outputs a on hot encoded row \n# model.summary()","adeca5f2":"filename = \"model_weights_saved.hdf5\"\n#model.load_weights(filename)\n#model.compile(loss='categorical_crossentropy', optimizer='adam')","ac766025":"\ntf.keras.utils.plot_model(model)","f6b96ac4":"acc = tf.keras.metrics.CategoricalAccuracy('accuracy')","787b0ff1":"model.compile(loss='categorical_crossentropy', optimizer='adam',metrics=[acc])\nfilepath = \"model_weights_saved_long.hdf5\"\ncheckpoint = ModelCheckpoint(filepath, monitor='loss', verbose=1, save_best_only=True, mode='min')\ndesired_callbacks = [checkpoint]\n","e9ba5fbd":"print(train)","81c32639":"model.fit(dataset, epochs=100, batch_size=50)","2badbda3":"model.save('trump_model.h5')","1c6762aa":"pattern = [6,\n  4304,\n  36,\n  706,\n  380,]\nprint(pattern)\n#model.summary()\nx = np.reshape(pattern, (1,len(pattern), 1))\nprint(x.shape)","a21c4eef":"reverse_word_map = dict(map(reversed, tokenizer.word_index.items()))\n\n# Function takes a tokenized sentence and returns the words\ndef sequence_to_text(list_of_indices):\n    # Looking up words in dictionary\n    words = [reverse_word_map.get(word) for word in list_of_indices]\n    return(words)\n\n# Creating texts \nmy_texts = sequence_to_text(pattern)\nmy_texts\n#tokenizer.sequences_to_texts_generator([pattern])","4be3f2ad":"seed_text = \"This was the best\"\nnext_words = 100","8ec99a86":"for _ in range(next_words):\n    token_list = tokenizer.texts_to_sequences([seed_text])[0]\n    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n    predicted = model.predict_classes(token_list, verbose=0)\n    output_word = \"\"\n    \n    for word, index in tokenizer.word_index.items():\n        if index == predicted:\n            output_word = word\n            break\n    seed_text += \" \" + output_word\n    \nprint(seed_text)","ee3a9472":"pattern = [6,1,4304,36,706,]\n#pattern = [random.randint(1,total_words) for x in range(5)]\nout = [sequence_to_text([value]) for value in pattern]\nfor i in range(50):\n    x = np.reshape(pattern, (1,len(pattern), 1))\n    x = x\/float(len(tokenizer.word_index.items())) #cast as float so it doesn't do int divid \n#    print(x.shape)\n    prediction = model.predict(x, verbose=0)\n    #print(prediction[0][:100])\n    index = np.argmax(prediction[0]) + 1 #it goes zero to 10383 so add 1 for offset\n    #print(index)\n    print(sequence_to_text([index]))\n    \n  \n\n    pattern.append(index)\n    pattern = pattern[1:len(pattern)]\n    seq_in = [sequence_to_text([value]) for value in pattern]\n    #print(seq_in)\n    out.append(sequence_to_text([index]))\nprint(out)\n","3035fa97":"print(prediction)","34200458":"sequence_to_text([2])","e73a5d4c":"#sequence_to_text(y_data)\n#y_data","34627b91":"Hello this note book is our first attempt at text generation.\nTHhs is an intro to nlp and the dangers of ai and ethics all in one","fca18e4c":"Reading and cleanning data so that we take only the unique tweets and remove all links in them. we also make sure that after we do that we remove any that are empty strings. additonally remove all double quotes","ac7d9676":"Tokenizing the tweets. our model will take an input of (5=seq_len) word hashes and predict the next word we will try to do a different n-grams with padding and see if its better in the next model. "}}