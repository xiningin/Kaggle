{"cell_type":{"5ebe8744":"code","690883d8":"code","8af8767a":"code","5d4409e9":"code","39568d8c":"code","e1e4c564":"code","0bf633c8":"code","5143b68b":"code","154da40c":"code","7b814710":"code","b2b6c2c5":"code","6ec6f0ed":"code","23449f69":"code","89193d80":"code","5ad33968":"code","7656e39a":"code","5bf17765":"code","8923f9ef":"code","6f660daa":"code","76538ef5":"code","fc8046f6":"code","33d500f7":"code","8df0a2cd":"code","64d1e831":"code","652b25a0":"code","1efe8bed":"code","e739a38a":"code","00bc0968":"code","857c7c72":"code","1af1e6e3":"code","05b120eb":"code","3b3d8600":"code","ab54f81f":"code","63227180":"code","bc42f0cc":"code","ef6c3907":"code","2d6ea2dd":"code","4a990920":"code","62e1cc59":"code","b6a82731":"code","e39baf57":"code","c349e536":"code","d643fd1b":"code","618cd12c":"code","d0ae05ee":"code","38e0e0f6":"code","263f991b":"code","cfc760e4":"code","82cb0379":"code","84ba797e":"code","d8300286":"code","89a5f7ef":"code","536e99fd":"code","e9e7e1cd":"code","a7997ebc":"code","b487685b":"code","076d1e7b":"code","5378c1eb":"code","eeb135e9":"code","4b903e19":"code","519ab6ac":"code","c9350f7b":"code","043e7778":"code","03c28344":"code","6d2f210b":"code","d4941bd2":"code","52a9f193":"code","5cda3dcf":"code","7d30f4d8":"code","215b4bdb":"code","5d4414b2":"code","f0b66005":"code","09509e64":"code","29c69d05":"code","6d5f942a":"code","acc1ce56":"code","3a3783d3":"code","545b788e":"code","d69957f6":"code","99dbcced":"code","9bf10f90":"code","b926e24b":"code","351c039f":"code","60b156b0":"code","6e3d1ed7":"code","7e941912":"code","802ee24f":"code","cb3374a0":"code","7a0643cf":"code","6056237e":"code","5da9952c":"code","80607f48":"code","71dcff22":"markdown","7f8fb89a":"markdown","a556895e":"markdown","98a5a7ae":"markdown","c5e90697":"markdown","6374e7c6":"markdown","9d944d6e":"markdown","5b91fe20":"markdown","e9fa2b0d":"markdown","9b766b5e":"markdown","52b1b4d4":"markdown","94abf058":"markdown","1199e298":"markdown","720204cb":"markdown","cc041bdd":"markdown"},"source":{"5ebe8744":"!pip install change_finder -q","690883d8":"import numpy as np \nimport pandas as pd \nfrom math import sqrt\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nfrom plotly.subplots import make_subplots\nimport plotly.figure_factory as ff\nfrom datetime import datetime\n\nfrom sklearn.model_selection import StratifiedShuffleSplit, KFold, GridSearchCV, RandomizedSearchCV\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.metrics import mean_squared_error, mean_absolute_error,make_scorer,r2_score\nfrom sklearn.inspection import plot_partial_dependence\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport statsmodels.api  as sm\nfrom scipy import stats\nfrom sklearn.model_selection import train_test_split,cross_val_score\n\nfrom sklearn.linear_model import Lasso, Ridge, SGDRegressor,LinearRegression,RidgeCV\nfrom sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder,StandardScaler,RobustScaler\nfrom sklearn.svm import SVR\nfrom sklearn.pipeline import Pipeline\nfrom xgboost import XGBRegressor\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.tree import ExtraTreeRegressor,DecisionTreeRegressor\nfrom sklearn.neural_network import MLPRegressor\nimport xgboost as xgb\nimport lightgbm as lgb\n\nfrom fbprophet import Prophet\n\nimport warnings\nwarnings.filterwarnings('ignore')\nimport pprint\n\npd.set_option('max_columns', 200)","8af8767a":"df = pd.read_csv('..\/input\/salesdata\/10000 Sales Records.csv')","5d4409e9":"print('Shape of Dataset {}'.format(df.shape))","39568d8c":"df.columns = df.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('(', '').str.replace(')', '')","e1e4c564":"df.head()","0bf633c8":"df['ship_date'] = pd.to_datetime(df['ship_date'])\ndf['order_date'] = pd.to_datetime(df['order_date'])\ndf[\"Quarter\"] = df.order_date.dt.quarter\ndf['quarter'] = pd.PeriodIndex(df.order_date, freq='Q')","5143b68b":"print(list(df.columns))","154da40c":"null_feat = pd.DataFrame(len(df['order_id']) - df.isnull().sum(), columns = ['Count'])\n\ntrace = go.Bar(x = null_feat.index, y = null_feat['Count'] ,opacity = 0.8, marker=dict(color = 'red',\n        line=dict(color='#000000',width=1.5)))\n\nlayout = dict(title =  \"Missing Values\")\n                    \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)","7b814710":"def missing_data(data):\n    total = data.isnull().sum()\n    percent = (data.isnull().sum()\/data.isnull().count()*100)\n    tt = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n    types = []\n    for col in data.columns:\n        dtype = str(data[col].dtype)\n        types.append(dtype)\n    tt['Types'] = types\n    return(np.transpose(tt))","b2b6c2c5":"missing_data(df)","6ec6f0ed":"def unique_values(data):\n    total = data.count()\n    tt = pd.DataFrame(total)\n    tt.columns = ['Total']\n    uniques = []\n    for col in data.columns:\n        unique = data[col].nunique()\n        uniques.append(unique)\n    tt['Uniques'] = uniques\n    return(np.transpose(tt))","23449f69":"unique_values(df)","89193d80":"for column in df.columns.values:\n    print(f\" Unique values of {column} : {df[column].nunique()}\")","5ad33968":"df.dtypes","7656e39a":"df.apply(lambda x :sum(x.isnull()))","5bf17765":"data = df.copy()\ndata_for_yr = df.copy()\ndata_for_outlier = df.copy()","8923f9ef":"data.head()","6f660daa":"M = data[(data['order_priority'] == 'C')]\nB = data[(data['order_priority'] == 'H')]\nS = data[(data['order_priority'] == 'L')]\nK = data[(data['order_priority'] == 'M')]\n\ntrace = go.Bar(y = (len(M), len(B),len(S),len(K)), x = ['C','H','L','M'], orientation = 'v',opacity = 0.8, marker=dict(\n        color=['green','red','blue','grey'],\n        line=dict(color='#000000',width=1.5)))\n\nlayout = dict(title =  'Count of Order Priority')\n                    \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)","76538ef5":"F=data['item_type'].value_counts().sort_values(ascending=False)\nlabel=F.index\nsize=F.values\ncolors = ['skyblue', '#FEBFB3', '#96D38C', '#D0F9B1', 'gold', 'orange', 'lightgrey', \n          'lightblue','lightgreen','aqua','yellow','#D4E157','#D1C4E9','#1A237E','#64B5F6','#009688',\n          '#1DE9B6','#66BB6A','#689F38','#FFB300']\ntrace =go.Pie(labels=label, values=size, marker=dict(colors=colors), hole=.1)\ndata_trace = [trace]\nlayout = go.Layout(title='Distribution of Item Type')\nfig=go.Figure(data=data_trace,layout=layout)\npy.iplot(fig)","fc8046f6":"F=data['country'].value_counts().sort_values(ascending=False)[:8]\ntrace1 = go.Scatter(x=F.index, y=F.values,name='Top Countries',marker=dict(color='green'))\n\nF=data['country'].value_counts().sort_values(ascending=True)[:7].iloc[::-1]\ntrace2 = go.Scatter(x=F.index, y=F.values,name='Leasr Countries ',marker=dict(color='red'))\n\ndata_trace=[trace1,trace2]\nlayout = dict(title = 'Country Distribution')\nfig = dict(data=data_trace,layout = layout)\npy.iplot(fig)","33d500f7":"#TotalPrice = data.groupby(['Country'])['Pack Price'].sum().nlargest(15)\nF=data.groupby('country').total_profit.sum().to_frame().sort_values(by='total_profit', ascending=False).round(3)\ntrace=go.Bar(x=F.index, y=F.total_profit, marker=dict(color = F.total_profit, colorscale='Cividis',showscale=True))\nddata=[trace]\nlayout = go.Layout(xaxis=dict(tickangle=45),title='Total Profit Country wise', yaxis = dict(title = 'Count'))\nfig = go.Figure(data=ddata, layout=layout)\npy.iplot(fig)","8df0a2cd":"cnt_srs = data['order_date'].dt.year.value_counts()\ncnt_srs = cnt_srs.sort_index()\nplt.figure(figsize=(14,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='red')\nplt.xticks(rotation='vertical')\nplt.xlabel('year', fontsize=12)\nplt.ylabel('', fontsize=12)\nplt.title(\"Yearly orders\")\nplt.show()","64d1e831":"cnt_srs = data['ship_date'].dt.year.value_counts()\ncnt_srs = cnt_srs.sort_index()\nplt.figure(figsize=(14,6))\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='green')\nplt.xticks(rotation='vertical')\nplt.xlabel('year', fontsize=12)\nplt.ylabel('', fontsize=12)\nplt.title(\"Years Shipments\")\nplt.show()","652b25a0":"cnt_srs = data['order_date'].dt.day.value_counts()\ncnt_srs_ = data['order_date'].dt.month.value_counts()\n\ncnt_srs = cnt_srs.sort_index()\nfig,ax1 = plt.subplots(1,2)\nsns.barplot(cnt_srs.index, cnt_srs.values, alpha=0.8, color='y',ax = ax1[0])\nsns.barplot(cnt_srs_.index, cnt_srs_.values, alpha=0.8, color='b',ax = ax1[1])\nplt.xticks(rotation='vertical')\nax1[0].set_title(\"Daily Orders \", fontsize=18)\nax1[1].set_title(\"Monthly Orders \", fontsize=18)\nplt.xlabel('year', fontsize=12)\nplt.ylabel('count', fontsize=12)\nplt.figure(figsize=(14,6))\nplt.show()","1efe8bed":"F=data.item_type.value_counts().sort_values(ascending=False)[:15]\n\ntrace1 = go.Bar(x=F.index,y=F.values,marker=dict(color = '#009688'))\n                                                      \nF = data.sales_channel.value_counts().sort_values(ascending=False)[:15]\n\ntrace2 = go.Bar(x=F.index, y=F.values, marker=dict( color ='#689F38' ))\n \nfig = make_subplots(rows=1, cols=2, subplot_titles=('Item Type','Sales Channel'))                                                        \n \nfig.add_trace(trace1, 1,1) \nfig.add_trace(trace2, 1,2)  \nfig['layout'].update(yaxis = dict(title = 'Values'), height=500, width=900, showlegend=False)\npy.iplot(fig)","e739a38a":"F=data.sales_channel.value_counts().sort_values(ascending=False)[:15]\n\ntrace1 = go.Bar(x=F.index,y=F.values,marker=dict(color = 'mediumvioletred'))\n                                                      \nF = data.order_priority.value_counts().sort_values(ascending=False)[:15]\n\ntrace2 = go.Bar(x=F.index, y=F.values, marker=dict( color ='palegreen' ))\n \nfig = make_subplots(rows=1, cols=2, subplot_titles=('Sales Channel','Order Priority'))                                                        \n \nfig.add_trace(trace1, 1,1) \nfig.add_trace(trace2, 1,2)  \nfig['layout'].update(yaxis = dict(title = 'Values'), height=500, width=900, showlegend=False)\npy.iplot(fig)","00bc0968":"print(\"Unique Values in `region` => {}\".format(np.sort(data.region.unique())))\nprint(\"Unique Values in `item_type` => {}\".format(np.sort(data.item_type.unique())))\nprint(\"Unique Values in `country` => {}\".format(np.sort(data.country.unique())))\nprint(\"Unique Values in `sales_channel` => {}\".format(np.sort(data.sales_channel.unique())))\nprint(\"Unique Values in `order_priority` => {}\".format(np.sort(data.order_priority.unique())))","857c7c72":"sns.distplot(data['total_profit'],bins=10)","1af1e6e3":"data.head()","05b120eb":"df_pivot = pd.pivot_table(data, values ='total_profit', \n                         columns =['quarter'], aggfunc = np.average) ","3b3d8600":"df_pivot","ab54f81f":"df_pivot = data[['quarter','region','item_type','units_sold','total_profit']]\ndf_pivot = df_pivot.sort_values('quarter').reset_index(drop=True)","63227180":"df_pivot","bc42f0cc":"dataset_ts = data.groupby(['quarter'])['total_profit'].mean().reset_index()\ndataset_ts.shape","ef6c3907":"dataset_ts['quarter'] = dataset_ts['quarter'].astype('str')","2d6ea2dd":"dataset_ts_cpy = dataset_ts.copy()","4a990920":"dataset_ts.head()","62e1cc59":"for f in dataset_ts.columns:\n    if dataset_ts[f].dtype=='object': \n        lbl = LabelEncoder()\n        lbl.fit(list(dataset_ts[f].values))\n        dataset_ts[f] = lbl.transform(list(dataset_ts[f].values))","b6a82731":"dataset_ts","e39baf57":"# Def X and Y\ny = np.array(dataset_ts.total_profit)\ndataset_ts = dataset_ts.drop('total_profit', 1)\nX = np.array(dataset_ts)","c349e536":"scaler = RobustScaler()\nX = scaler.fit_transform(X)","d643fd1b":"X_sm = sm.add_constant(X)\n\n# Fit the resgression line using 'OLS'\nlr = sm.OLS(y, X_sm).fit()\nprint(lr.summary())\n","618cd12c":"model_lr = LinearRegression()\nmodel_lr.fit(X, y)\n\nlrpred = model_lr.predict(X)\n\nprint(\"Training Accuracy: \", model_lr.score(X, y))","d0ae05ee":"# Define error measure for official scoring : RMSE\nscorer = make_scorer(mean_squared_error, greater_is_better = False)\n\ndef rmse_cv_train(model):\n    rmse= np.sqrt(-cross_val_score(model, X_train, y_train, scoring = scorer, cv = 10))\n    return(rmse)\n\ndef rmse_cv_test(model):\n    rmse= np.sqrt(-cross_val_score(model, X_test, y_test, scoring = scorer, cv = 10))\n    return(rmse)","38e0e0f6":"data_for_yr.info()","263f991b":"data_for_yr = data_for_yr.drop(['order_id','ship_date','Quarter','quarter'],1)\ndata_for_yr['Year'] = data_for_yr['order_date'].dt.year\ndata_for_yr = data_for_yr.drop(['order_date'],1)","cfc760e4":"data_for_yr.columns","82cb0379":"data_for_model = data_for_yr[['region', 'country', 'item_type', 'sales_channel', 'order_priority', 'units_sold', 'unit_price', 'unit_cost', 'total_profit', 'Year']]","84ba797e":"plt.rcParams['figure.figsize'] = (15, 10)\nsns.heatmap(data_for_model.corr(), cmap = 'copper',annot = True)\nplt.title('Heat Map for Correlations', fontsize = 20)\nplt.show()","d8300286":"data_for_model = pd.get_dummies(data_for_model,drop_first=True)","89a5f7ef":"data_for_model","536e99fd":"df_ohe = data_for_model.copy()\nx = df_ohe.drop(['total_profit'], axis = 1)\ny = df_ohe['total_profit']\n\nprint(\"Shape of x :\", x.shape)\nprint(\"Shape of y :\", y.shape)","e9e7e1cd":"y_trans,fitted_lambda = stats.boxcox(y)\nfig, ax=plt.subplots(1,2)\nsns.distplot(y, ax=ax[0])\nsns.distplot(y_trans, ax=ax[1])","a7997ebc":"X_train, X_test, y_train, y_test = train_test_split(x, y, test_size = 0.3, random_state = 42)\n\nprint(\"Shape of X_train :\", X_train.shape)\nprint(\"Shape of X_test :\", X_test.shape)\nprint(\"Shape of y_train :\", y_train.shape)\nprint(\"Shape of y_test :\", y_test.shape)","b487685b":"scr = RobustScaler()\nX_train = scr.fit_transform(X_train)\nX_test = scr.transform(X_test)","076d1e7b":"%%time\nclfs = []\nseed = 3\n\nclfs.append((\"LinearRegression\", \n             Pipeline([(\"Scaler\", RobustScaler()),\n                       (\"LogReg\", LinearRegression())])))\n\nclfs.append((\"XGB\",\n             Pipeline([(\"Scaler\", RobustScaler()),\n                       (\"XGB\", XGBRegressor())]))) \nclfs.append((\"KNN\", \n             Pipeline([(\"Scaler\", RobustScaler()),\n                       (\"KNN\", KNeighborsRegressor())]))) \n\nclfs.append((\"DTR\", \n             Pipeline([(\"Scaler\", RobustScaler()),\n                       (\"DecisionTrees\", DecisionTreeRegressor())]))) \n\nclfs.append((\"RFRegressor\", \n             Pipeline([(\"Scaler\", RobustScaler()),\n                       (\"RandomForest\", RandomForestRegressor())]))) \n\nclfs.append((\"GBRegressor\", \n             Pipeline([(\"Scaler\", RobustScaler()),\n                       (\"GradientBoosting\", GradientBoostingRegressor())]))) \n\nclfs.append((\"MLP\", \n             Pipeline([(\"Scaler\", RobustScaler()),\n                       (\"MLP Regressor\", MLPRegressor())])))\n\n\nclfs.append((\"EXT Regressor\",\n             Pipeline([(\"Scaler\", RobustScaler()),\n                       (\"ExtraTrees\", ExtraTreeRegressor())])))\nclfs.append((\"SV Regressor\",\n             Pipeline([(\"Scaler\", RobustScaler()),\n                       (\"ExtraTrees\", SVR())])))\n\nscoring = 'r2'\nn_folds = 10\nmsgs = []\nresults, names  = [], [] \n\nfor name, model  in clfs:\n    kfold = KFold(n_splits=n_folds, random_state=None)\n    cv_results = cross_val_score(model, x, y, \n                                 cv=kfold, scoring=scoring, n_jobs=-1)    \n    names.append(name)\n    results.append(cv_results)    \n    msg = \"%s: %f (+\/- %f)\" % (name, cv_results.mean(),  \n                               cv_results.std())\n    msgs.append(msg)\n    print(msg)","5378c1eb":"rfr = RandomForestRegressor()\nrfr.fit(X_train, y_train)\n\n# Look at predictions on training and validation set\nprint(\"RMSE on Training set :\", rmse_cv_train(rfr).mean())\nprint(\"RMSE on Test set :\", rmse_cv_test(rfr).mean())\ny_train_pred = rfr.predict(X_train)\ny_test_pred = rfr.predict(X_test)\n\n# Plot residuals\nplt.scatter(y_train_pred, y_train_pred - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_pred, y_test_pred - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"RF Regressor\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_pred, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_pred, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"RF Regressor\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()","eeb135e9":"def timer(start_time=None):\n    if not start_time:\n        start_time = datetime.now()\n        return start_time\n    elif start_time:\n        thour, temp_sec = divmod((datetime.now() - start_time).total_seconds(), 3600)\n        tmin, tsec = divmod(temp_sec, 60)\n        print('\\n Time taken: %i hours %i minutes and %s seconds.' % (thour, tmin, round(tsec, 2)))\n        \n        \nrf_cfl = RandomForestRegressor(n_jobs = -1)\n\n# Number of trees in random forest\nn_estimators = [100,200,300,400]\n# Number of features to consider at every split\nmax_features = ['auto', 'sqrt']\n# Maximum number of levels in tree\nmax_depth = [10,20,30,40,50]\n# Minimum number of samples required to split a node\nmin_samples_split = [2, 5, 10]\n# Minimum number of samples required at each leaf node\nmin_samples_leaf = [1, 2, 4]\n# Method of selecting samples for training each tree\nbootstrap = [True, False]\nparams = {'n_estimators': n_estimators,\n               'max_features': max_features,\n               'max_depth': max_depth,\n               'min_samples_split': min_samples_split,\n               'min_samples_leaf': min_samples_leaf,\n               'bootstrap': bootstrap}\n\nfolds = 5\nparam_comb = 10\n\nrandom_search = RandomizedSearchCV(rf_cfl,param_distributions=params, n_iter=param_comb, scoring='neg_root_mean_squared_error', n_jobs=-1, cv=5, verbose=3, random_state=42)\n\nstart_time = timer(None) \nrandom_search.fit(x, y)\nprint(random_search.best_params_)\ntimer(start_time)","4b903e19":"random_clf = RandomForestRegressor(n_estimators = 200,min_samples_split=2,min_samples_leaf=1,max_features='auto',max_depth=30,bootstrap=True)\n\nrandom_clf.fit(X_train,y_train)\ny_pred = random_clf.predict(X_test)\ntrain_pred = random_clf.predict(X_train)\n\nprint(\"RMSE on Training set :\", rmse_cv_train(random_clf).mean())\nprint(\"RMSE on Test set :\", rmse_cv_test(random_clf).mean())\nprint('MSE train data: %.3f, MSE test data: %.3f' % (mean_squared_error(y_train,train_pred),mean_squared_error(y_test,y_pred)))\nprint('R2 train data: %.3f, R2 test data: %.3f' % (r2_score(y_train,train_pred),r2_score(y_test,y_pred)))","519ab6ac":"ridge = RidgeCV(alphas = [0.01, 0.03, 0.06, 0.1, 0.3, 0.6, 1, 3, 6, 10, 30, 60])\nridge.fit(X_train, y_train)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Try again for more precision with alphas centered around \" + str(alpha))\nridge = RidgeCV(alphas = [alpha * .6, alpha * .65, alpha * .7, alpha * .75, alpha * .8, alpha * .85, \n                          alpha * .9, alpha * .95, alpha, alpha * 1.05, alpha * 1.1, alpha * 1.15,\n                          alpha * 1.25, alpha * 1.3, alpha * 1.35, alpha * 1.4], \n                cv = 10)\nridge.fit(X_train, y_train)\nalpha = ridge.alpha_\nprint(\"Best alpha :\", alpha)\n\nprint(\"Ridge RMSE on Training set :\", rmse_cv_train(ridge).mean())\nprint(\"Ridge RMSE on Test set :\", rmse_cv_test(ridge).mean())\ny_train_rdg = ridge.predict(X_train)\ny_test_rdg = ridge.predict(X_test)\n\n# Plot residuals\nplt.scatter(y_train_rdg, y_train_rdg - y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_rdg, y_test_rdg - y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Ridge regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Residuals\")\nplt.legend(loc = \"upper left\")\nplt.hlines(y = 0, xmin = 10.5, xmax = 13.5, color = \"red\")\nplt.show()\n\n# Plot predictions\nplt.scatter(y_train_rdg, y_train, c = \"blue\", marker = \"s\", label = \"Training data\")\nplt.scatter(y_test_rdg, y_test, c = \"lightgreen\", marker = \"s\", label = \"Validation data\")\nplt.title(\"Linear regression with Ridge regularization\")\nplt.xlabel(\"Predicted values\")\nplt.ylabel(\"Real values\")\nplt.legend(loc = \"upper left\")\nplt.plot([10.5, 13.5], [10.5, 13.5], c = \"red\")\nplt.show()","c9350f7b":"data_for_outlier.sample(5)","043e7778":"data_for_outlier= data_for_outlier.sort_values('ship_date').reset_index(drop=True)\ndata_for_outlier_ts = data_for_outlier[['ship_date','unit_price']]","03c28344":"data_for_outlier['ship_date_y'] = data_for_outlier['ship_date'].dt.year\ndata_for_outlier['ship_date_m'] = data_for_outlier['ship_date'].dt.month\ndata_for_outlier['ship_date_d'] = data_for_outlier['ship_date'].dt.day","6d2f210b":"data_for_outlier.head()","d4941bd2":"def box_plot_dist_var(var):\n    data_ = pd.concat([data_for_outlier['unit_price'], data_for_outlier[var]], axis=1)\n    f, ax = plt.subplots(figsize=(16, 8))\n    fig = sns.boxplot(x=var, y=\"unit_price\", data=data_)\n    plt.xticks(rotation=90);\n\nbox_plot_dist_var(\"ship_date_y\")        \nbox_plot_dist_var(\"ship_date_d\")  ","52a9f193":"def scatter_plot_var(var):\n    data = pd.concat([data_for_outlier['unit_price'],data_for_outlier[var] ], axis=1)\n    data.plot.scatter(x=var, y='unit_price')\nscatter_plot_var(\"units_sold\")\nscatter_plot_var(\"unit_cost\")\nscatter_plot_var(\"total_revenue\")\nscatter_plot_var(\"total_profit\")","5cda3dcf":"import change_finder \n\ndef changeFinder(data, r, smooth):\n    #CHANGEFINDER PACKAGE\n    f, (ax1, ax2) = plt.subplots(2, 1, figsize=(13,8))\n    f.subplots_adjust(hspace=0.4)\n    ax1.plot(data)\n    ax1.set_title(\"data count\")\n    #Initiate changefinder function\n    cf = change_finder.ChangeFinder(r=r, k=2, T=smooth, R=2)\n    scores = [cf.update(p) for p in data]\n    ax2.plot(scores)\n    ax2.set_title(\"Anomaly Score\")\n    plt.show()\n    return scores","7d30f4d8":"datafinder = data_for_outlier_ts.unit_price.values\npltdata = changeFinder(datafinder, r=0.02, smooth=10)    ","215b4bdb":"datades = np.array(pltdata)\nfrom scipy import stats\ndescribedatades = stats.describe(datades)\ndescribedatades","5d4414b2":"qt25 = np.percentile(datades, 25)  # Q1\nqt50 = np.percentile(datades, 50)  # Q2\nqt60 = np.percentile(datades, 60)  # Q31\nqt75 = np.percentile(datades, 75)  # Q3\nqt95 = np.percentile(datades, 95)","f0b66005":"qt75_ws = np.percentile(datades, 75)\nqt95_ws = np.percentile(datades, 95)","09509e64":"dfscore= pd.DataFrame(datades, columns=['Score_CF'])\ndfscore.head()","29c69d05":"plt.figure(figsize=(16,6))\ndf_high_data_ = dfscore[dfscore <= qt75]\ndf_high_score_ = dfscore[dfscore > qt75]\nplt.plot(dfscore.index, dfscore.Score_CF.fillna(1), c='gray', alpha=0.4)\nplt.scatter(df_high_data_.index, df_high_data_.values, label='LOW', s=10)\nplt.scatter(df_high_score_.index, df_high_score_.values, label='HIGH', c='red', s=10)\nplt.margins(x=0.01,y=0.2)\nplt.title('Anomaly Score ')\nplt.ylabel('Score')\nplt.xlabel('Data Count')\nplt.legend()\nplt.show()","6d5f942a":"plt.figure(figsize=(16,6))\ndf_high_data = data_for_outlier_ts[data_for_outlier_ts['unit_price'] < qt75_ws]['unit_price']\ndf_high_score = data_for_outlier_ts[data_for_outlier_ts['unit_price'] >= qt75_ws]['unit_price']\nplt.plot(data_for_outlier_ts['unit_price'].index, data_for_outlier_ts['unit_price'].fillna(1), c='g', alpha=0.4)\nplt.scatter(df_high_data.index, df_high_data.values, label='LOW', s=10)\nplt.scatter(df_high_score.index, df_high_score.values, label='HIGH', c='red', s=10)\nplt.margins(x=0.01,y=0.2)\nplt.title('Price Time Series ')\nplt.xlabel('Date')\nplt.legend()\nplt.show()","acc1ce56":"data_for_outlier_ts = data_for_outlier_ts.groupby(['ship_date'])['unit_price'].sum().reset_index()\ndata_for_outlier_ts.shape","3a3783d3":"data_for_outlier_ts","545b788e":"from statsmodels.tsa.seasonal import seasonal_decompose\nts=data_for_outlier_ts.groupby([\"ship_date\"])[\"unit_price\"].sum()\nts.astype('float')\nplt.figure(figsize=(16,8))\nplt.title('Unit Price')\nplt.xlabel('Ship Date')\nplt.ylabel('Unit Price')\nplt.plot(ts);","d69957f6":"plt.figure(figsize=(16,6))\nplt.plot(ts.rolling(window=12,center=False).mean(),label='Rolling Mean');\nplt.plot(ts.rolling(window=12,center=False).std(),label='Rolling sd');\nplt.legend();","99dbcced":"import statsmodels.api as sm\nplt.rcParams[\"figure.figsize\"] = (15,9)\nres = sm.tsa.seasonal_decompose(ts.values,period=12,model=\"multiplicative\")\nfig = res.plot()","9bf10f90":"data_for_outlier_plot = data_for_outlier_ts.set_index(\"ship_date\")\ndata_for_outlier_plot.index = pd.to_datetime(data_for_outlier_plot.index)\ndata_for_outlier_plot = data_for_outlier_plot[['unit_price']].resample('M').mean()\ndata_for_outlier_plot.head()","b926e24b":"trace3 = go.Scatter(\n    x = data_for_outlier_plot.index[:-1],\n    y = data_for_outlier_plot.unit_price[:-1]\n)\nlayout3 = go.Layout(\n    title = \"Average Prices by Month\",\n    xaxis = dict(title = 'Month'),\n    yaxis = dict(title = 'Price')\n)\ndata3 = [trace3]\nfigure3 = go.Figure(data = data3, layout = layout3)\npy.iplot(figure3)","351c039f":"data_for_outlier_ts","60b156b0":"data_for_outlier_ts_cpy = data_for_outlier_ts.copy()\ndata_for_outlier_ts_cpy = data_for_outlier_ts_cpy[['ship_date', 'unit_price']]\ndata_for_outlier_ts_cpy.columns = ['ds', 'y']\ndata_for_outlier_ts_cpy.head()","6e3d1ed7":"data_for_outlier_ts_cpy['y_original'] = data_for_outlier_ts_cpy['y']\n\ndata_for_outlier_ts_cpy['y'] = np.log(data_for_outlier_ts_cpy['y'])\n\ndata_for_outlier_ts_cpy['ds'] =  pd.to_datetime(data_for_outlier_ts_cpy['ds'])\ndata_for_outlier_ts_cpy.head()","7e941912":"mean = data_for_outlier_ts_cpy['y'].mean()\nstdev = data_for_outlier_ts_cpy['y'].std()\n\nq1 = data_for_outlier_ts_cpy['y'].quantile(0.25)\nq3 = data_for_outlier_ts_cpy['y'].quantile(0.75)\niqr = q3 - q1\nhigh = mean + stdev\nlow = mean - stdev","802ee24f":"filtered_iqr = data_for_outlier_ts_cpy[(data_for_outlier_ts_cpy['y'] < q1 - (1.5 * iqr)) | (data_for_outlier_ts_cpy['y'] < q3 + (1.5 * iqr)) ]","cb3374a0":"trace = go.Scatter(\n    x = data_for_outlier_ts_cpy['ds'],\n    y = data_for_outlier_ts_cpy['y'],\n    mode = 'lines',\n    name = 'actual data'\n)\ntrace_cp = go.Scatter(\n    x = filtered_iqr['ds'],\n    y = filtered_iqr['y'],\n    mode = 'markers',\n    name = 'changepoint'\n)\n\ndata = [trace,trace_cp]\nfig = go.Figure(data=data)\npy.offline.iplot(fig)","7a0643cf":"prophet = Prophet(yearly_seasonality = True)\nprophet.fit(data_for_outlier_ts_cpy)","6056237e":"future = prophet.make_future_dataframe(periods = 365)\nfuture['cap'] = 7.05\nforecast = prophet.predict(future)\n","5da9952c":"py.iplot([\n    go.Scatter(x=data_for_outlier_ts_cpy['ds'], y=data_for_outlier_ts_cpy['y'], name='y'),\n    go.Scatter(x=forecast['ds'], y=forecast['yhat'], name='yhat'),\n    go.Scatter(x=forecast['ds'], y=forecast['yhat_upper'], fill='tonexty', mode='none', name='upper'),\n    go.Scatter(x=forecast['ds'], y=forecast['yhat_lower'], fill='tonexty', mode='none', name='lower')\n])","80607f48":"prophet.plot_components(forecast)","71dcff22":"### 1.2 Exploratory Data Analysis","7f8fb89a":"### 1.1 Data Munging","a556895e":" #### -->Gross profit can be calculated by subtracting the cost of goods sold (COGS) from revenue (sales).Since we have gross profit in our train data so dropping total revenue and total cost due to high correlation. \n #### -->Region,Country,Item_type,Sales_channel and Order_priority plays direct impact in total profit.","98a5a7ae":"#### --> Using Box cox transformation since data is skewed.","c5e90697":"### 2.2 Prediction Model-Linear Regression","6374e7c6":"### Applying L2 Regularization ","9d944d6e":"## 4. Develop an anomalies detection model \n\n### 4.1 Anomaly Detection using Change Finder","5b91fe20":"### Dataset Description\n - Region\n - country\n - item type\n - Sales Channel\n - Order Priority\n - Order Date\n - Order Id\n - Ship Date\n - Unit Sold\n - Unit Price\n - Unit Cost\n - Total Revenue\n - Total Cost\n - Total Profit","e9fa2b0d":"## 2. Identify the **optimal** linear regression model \n\n### 2.1 Feature Engineering","9b766b5e":"### Import Neccessary Packages and Libraries","52b1b4d4":"### Model Selection - Cross validated","94abf058":"# END","1199e298":"### Hyper Parameter Tunning using Randomized Search CV[](http:\/\/)","720204cb":"## 3. Build a non-linear model \n### 3.1 Feature Engineering","cc041bdd":"### 4.2 Time Series Forecasting using FBProphet"}}