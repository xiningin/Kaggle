{"cell_type":{"ff8fd8e4":"code","581d5675":"code","299d07cc":"code","f8e5168e":"code","e1e66ed6":"code","15307a07":"code","24bb1253":"code","4fc7bcd3":"code","8f6b687e":"code","04020b5b":"code","0c62bd75":"code","0663fe58":"code","76f1dbd4":"code","dffc9e29":"code","19e182b5":"code","c26e3ab1":"code","3d594fa1":"code","caba3b51":"code","91d45d38":"code","200d142a":"code","eb99453f":"code","c299a061":"code","4bb9c290":"code","3236e665":"code","7ac8b329":"code","ab2e0665":"code","8cc84b5a":"markdown","e86f8217":"markdown","db78cb0c":"markdown","93f175db":"markdown","5483b293":"markdown","3083a9ce":"markdown","8fc778cc":"markdown","7a5007f5":"markdown","7cda8b6a":"markdown","07ab2738":"markdown","155d78fe":"markdown","8e1699d5":"markdown","2ffab211":"markdown","92b88e30":"markdown","f138eb2f":"markdown","963fb492":"markdown","3525b519":"markdown","5d7bb898":"markdown","14b1f737":"markdown","5eb5b23f":"markdown","488cf861":"markdown","e3cc8e5f":"markdown","4ecb0fec":"markdown","ccbe6775":"markdown","fb0ec091":"markdown","0933a3a9":"markdown","3ee0ed88":"markdown","97c1110d":"markdown","7b6b3e7a":"markdown","950fd276":"markdown","a219a2ea":"markdown","3bf6bab2":"markdown","5dd3139d":"markdown","214b98f9":"markdown","146135d9":"markdown","57d0e00b":"markdown","a6739617":"markdown","44f2c954":"markdown","bae61412":"markdown","74d669d4":"markdown","9741536d":"markdown","6c335099":"markdown"},"source":{"ff8fd8e4":"%cd '..\/input\/older-dataset-for-dont-overfit-ii-challenge\/'\n!ls","581d5675":"import warnings\nwarnings.filterwarnings('ignore')\nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport missingno as msno\nfrom sklearn.manifold import TSNE\nfrom sklearn.decomposition import PCA","299d07cc":"data = pd.read_csv('train.csv')\ndata.head()","f8e5168e":"data.shape","e1e66ed6":"data.describe()","15307a07":"nul = 0\nfor col in data.columns:\n    if(data[col].isnull().any()):\n        print(col,'has null values')\n        nul = 1\nif(nul==0):\n    print('There is no Null value present')","24bb1253":"msno.matrix(data)","4fc7bcd3":"sns.set(rc={'figure.figsize':(30,10)})\nplt.subplot(1,2,1)\nax = sns.countplot(data['target'])\nfor p in ax.patches:\n    ax.annotate('{:.1f}%'.format(100*p.get_height()\/len(data)), (p.get_x()+0.35, p.get_height()+1))\nax.set_title('count plot of Target Distribution')\nax.set_xlabel('Target')\nax.set_ylabel('Count')\n\nplt.subplot(1,2,2)\nax = data['target'].value_counts().plot(kind='pie', colormap='coolwarm')\nax.set_title('Pic chart of Target Distribution')","8f6b687e":"# https:\/\/stackoverflow.com\/questions\/50940283\/show-metrics-like-kurtosis-skewness-on-distribution-plot-using-seaborn-in-pytho\nsns.set(rc={'figure.figsize':(30,90)})\nfig, ax = plt.subplots(30, 10,sharex='col', sharey='row')\nax = ax.reshape(-1)\nfor i, col in enumerate(data.columns[2:]):\n    g0 = data[data['target']==0.0][col]\n    g1 = data[data['target']==1.0][col]\n    sns.distplot(g0, label = 'target 0', ax=ax[i], color='b')\n    sns.distplot(g1, label = 'target 1', ax=ax[i], color='r')\n\nmin_skew, max_skew = 100, -100\nmin_kurt, max_kurt = 100, -100\n\nfor i, x in enumerate(ax):\n    skew = data.iloc[:,i+2].skew()\n    min_skew = min(min_skew, skew)\n    max_skew = max(max_skew, skew)\n    x.text(x=0.97, y=0.97, transform=x.transAxes, s=\"Skewness: %f\" % skew,\\\n        fontweight='demibold', fontsize=10, verticalalignment='top', horizontalalignment='right',\\\n        backgroundcolor='white', color='xkcd:poo brown')\n    kurt = data.iloc[:,i+2].kurt()\n    min_kurt = min(min_kurt, kurt)\n    max_kurt = max(max_kurt, kurt)\n    x.text(x=0.97, y=0.87, transform=x.transAxes, s=\"Kurtosis: %f\" % kurt,\\\n        fontweight='demibold', fontsize=10, verticalalignment='top', horizontalalignment='right',\\\n        backgroundcolor='white', color='xkcd:dried blood')\nplt.tight_layout()\nplt.show()","04020b5b":"print('Skewness Range:[{},{}] '.format(min_skew,max_skew))\nprint('Kurtosis Range:[{},{}] '.format(min_kurt,max_kurt))","0c62bd75":"sns.set(rc={'figure.figsize':(30,90)})\nfig, ax = plt.subplots(30, 10,sharex='col', sharey='row')\nfig.text(0.5, 0.12, s='Target', ha='center',)\nfig.text(0.1, 0.5, s='Distribution', va='center', rotation='vertical')\nax = ax.reshape(-1)\nfor i, col in enumerate(data.columns[2:]):\n    axe= sns.boxenplot(np.array(data['target']),np.array(data[col]), ax=ax[i])\n    ax[i].set_title('Feature '+col)\nplt.show()","0663fe58":"sns.set(rc={'figure.figsize':(30,15)})\nmatrix = np.tril(data.drop(['id'], axis=1).corr())\nax = sns.heatmap(data.drop(['id'], axis=1).corr(), cbar_kws= {'orientation': 'horizontal'}, mask=matrix)","76f1dbd4":"correlations = data.drop(['id'], axis=1).corr().unstack().drop_duplicates()\nprint('Top 10 positive correlated features with target:')\nprint(correlations[\"target\"].sort_values(ascending=False)[:10])\nprint('-'*100)\nprint('Top 10 negative correlated features with target:',)\nprint(correlations[\"target\"].sort_values(ascending=True)[:10])","dffc9e29":"sns.set(rc={'figure.figsize':(30,10)})\nplt.subplot(1,2,1)\nx = correlations[\"target\"].sort_values(ascending=False)[1:10].plot(kind='bar', title='Top 10 Positively correlated features with target')\nx.set_xlabel('Features')\nx.set_ylabel('Correlation values')\n\nplt.subplot(1,2,2)\nx = correlations[\"target\"].sort_values(ascending=True)[:10].plot(kind='bar', title='Top 10 Negatively correlated features with target', color='r')\nx.set_xlabel('Features')\nx.set_ylabel('Correlation values')","19e182b5":"features = list(correlations[\"target\"].sort_values(ascending=False)[:5].index)\nax = sns.pairplot(data[features], hue='target', height=3)","c26e3ab1":"features = list(correlations[\"target\"].sort_values(ascending=True)[:5].index)\nax = sns.pairplot(data[features+['target']], hue='target', height=2.5)","3d594fa1":"sns.set(rc={'figure.figsize':(20,10)})\npca = PCA(n_components=3, svd_solver='full')\npca_result = pca.fit_transform(data.drop(['id','target'], axis=1).values)\nx = pca_result[:,0]\ny = pca_result[:,1]\nax = sns.scatterplot(x, y, hue = data['target'], palette=sns.color_palette(\"hls\", 2), legend=\"full\", alpha=1)\nax.set_title('2D visualization')\nax.set_xlabel('pca-one')\nax.set_ylabel('pca-two')","caba3b51":"ax = plt.figure(figsize=(20,10)).gca(projection='3d')\nz = pca_result[:,2]\nax.scatter(xs=x, ys=y, zs=z, c=data['target'], cmap='Accent')\nax.set_title('3D visualization')\nax.set_xlabel('pca-one')\nax.set_ylabel('pca-two')\nax.set_zlabel('pca-three')\nplt.show()","91d45d38":"sns.set(rc={'figure.figsize':(30,15)})\ng0 = data[data['target']==0.0][data.columns[2:]]\ng1 = data[data['target']==1.0][data.columns[2:]]\n\n\nfor i, fun in enumerate(['mean','std','skew','kurt']):\n    plt.subplot(2,2,i+1)\n    if(fun=='mean'):\n        f0, f1, p0, p1 = g0.mean().mean(), g1.mean().mean(), g0.mean(), g1.mean()\n    elif(fun=='std'):\n        f0, f1, p0, p1 = g0.std().std(), g1.std().std(), g0.std(), g1.std()\n    elif(fun=='skew'):\n        f0, f1, p0, p1 = g0.skew().skew(), g1.skew().skew(), g0.skew(), g1.skew()\n    elif(fun=='kurt'):\n        f0, f1, p0, p1 = g0.kurt().kurt(), g1.kurt().kurt(), g0.kurt(), g1.kurt()\n\n    x = p0.plot(kind = 'hist', alpha=0.5, label='0', color='b')\n    x.text(x=0.90, y=0.98, transform=x.transAxes, s=\"Target 0 {}: {:.4f}\".format(fun,f0),\\\n            fontweight='demibold', fontsize=10, verticalalignment='top', horizontalalignment='right',\\\n            backgroundcolor='white', color='b')\n    x = p1.plot(kind = 'hist', alpha=0.5, label='1', color='r')\n    x.text(x=0.90, y=0.92, transform=x.transAxes, s=\"Target 1 {}: {:.4f}\".format(fun,f1),\\\n            fontweight='demibold', fontsize=10, verticalalignment='top', horizontalalignment='right',\\\n            backgroundcolor='white', color='r')\n    plt.title(fun+' Frequency')\n    plt.legend()\nplt.show()","200d142a":"test = pd.read_csv('test.csv')\ntest.shape","eb99453f":"bf = list()\nfor i in data.columns[2:]:\n    skew = data[i].skew()\n    if(skew>0.3 or skew<-0.3):\n        bf.append((i,skew))\nprint('Number of Features for binning: ', len(bf))\nprint('Features for with their skewness: ', bf)\nbf = dict(bf)","c299a061":"# https:\/\/towardsdatascience.com\/understanding-feature-engineering-part-1-continuous-numeric-data-da4e47099a7b\nfor col in bf.keys():\n    quantile_list = [0, .25, .5, .75, 1.]\n    quantiles = data[col].quantile(quantile_list)\n    quantile_labels = [0, 1, 2, 3]\n    data['Quantile_binning_'+col] = pd.qcut(data[col], q=quantile_list, labels=quantile_labels)\n    test['Quantile_binning_'+col] = pd.qcut(test[col], q=quantile_list, labels=quantile_labels)","4bb9c290":"sns.set(rc={'figure.figsize':(30,35)})\n\ncnt = 1;\nfor col in bf.keys():\n    plt.subplot(len(bf.keys()),2,cnt)\n    g0 = data[data['target']==0.0][col]\n    g1 = data[data['target']==1.0][col]\n    sns.distplot(g0, label = 'target 0', color='b')\n    sns.distplot(g1, label = 'target 1', color='r')\n    plt.legend()\n\n    plt.subplot(len(bf.keys()),2,cnt+1)\n    g0 = data[data['target']==0.0]['Quantile_binning_'+col]\n    g1 = data[data['target']==1.0]['Quantile_binning_'+col]\n    sns.distplot(g0, label = 'target 0', color='b')\n    sns.distplot(g1, label = 'target 1', color='r')\n    plt.legend()\n    cnt += 2\n\nplt.show()","3236e665":"data.head()","7ac8b329":"test.head()","ab2e0665":"fe = ['Quantile_binning_'+f for f in bf.keys()]\nfe","8cc84b5a":"<p style=\"text-indent: 40px\"> One of the main objectives of predictive modelling is to build a model that will give accurate predictions on unseen data which will only be possible when we make sure that they have not overfitted the training data. Before going to understand how to ensure that our model hasn\u2019t overfitted training data, let\u2019s first get aware of the reasons which lead the model to get overfitted.<\/p> <\/br>\n<p style=\"text-indent: 40px\"> There are many reasons for the same, yet we would like to point out some major reasons. First, being of fewer data points in training samples, second is the dataset being imbalanced, and last but not the least, complex nature of model.<\/p><\/br>\n<p style=\"text-indent: 40px\"> In this case study, we are going to handle the same problem of overfitting and the challenging part is to make a model with 250 data points in the train set and predict the binary target accurately for 19750 unseen data points in the test set. The dataset has 300 features of a continuous variable. <\/br>\n<li>The dataset is obtained from the link: https:\/\/www.kaggle.com\/c\/dont-overfit-ii\/data<\/li>\n\n\n\n\n\n\n\n","e86f8217":"# Import Libraries","db78cb0c":"* All the features, based on mean and standard deviation fequency graph seem to be normalized and normally distrubuted with mean = 0 (approx) and std=0.1 (approx). \n* If we looked at skew and kurtosis frequency graph, skewnness = 0.15 and kurtosis > 1.5 (for target 1 data points) which shows that it has heavier tail than a normal distribution.\n* Distribution of target 0 and target 1 data point are overlapped.\n* Thus we can conclude tha it is not randomly distributed.","93f175db":"There is no null value present and all values are real","5483b293":"## Problem Statements","3083a9ce":"# Exploratory Data Analysis(EDA)","8fc778cc":"## Describtion of train Dataset","7a5007f5":"### 2D visualization of 300D data point","7cda8b6a":"# Mount Drive","07ab2738":"* All the point are scattered in haphazard fashion thus it is not linearly seprable","155d78fe":"# Feature Engineering(FE) ","8e1699d5":"### Distribution plot of all 300 features in single frame","2ffab211":"* Using quartile based adaptive binning we converted 7 skewed features into a categorical features\n* engineered features also shows the overlap betwenn target 0 and target 1 points but above certain limit of frequencies target 0 points are dominenet which is the evidence of useful festure","92b88e30":"## Univariate Analysis","f138eb2f":"### Mean, Standard deviation, Skewness and Kurtosis for all the features","963fb492":"* Above 4 features [33, 65, 24, 183] are going to be the most important towards the classification task, As it is seen clearly that they are not fully overlaped but quite seperated","3525b519":"\n\n*   https:\/\/towardsdatascience.com\/underfitting-and-overfitting-in-machine-learning-and-how-to-deal-with-it-6fe4a8a49dbf\n*   https:\/\/iopscience.iop.org\/article\/10.1088\/1742-6596\/1168\/2\/022022\/pdf\n*   http:\/\/www.jmlr.org\/papers\/volume3\/reunanen03a\/reunanen03a.pdf\n*   https:\/\/static.aminer.org\/pdf\/PDF\/000\/265\/793\/a_noise_metric_on_binary_training_inputs_and_a_framework.pdf\n*   https:\/\/towardsdatascience.com\/noise-its-not-always-annoying-1bd5f0f240f\n*   https:\/\/medium.com\/analytics-vidhya\/just-dont-overfit-e2fddd28eb29\n*   https:\/\/towardsdatascience.com\/dont-overfit-how-to-prevent-overfitting-in-your-deep-learning-models-63274e552323\n\n\n\n\n\n\n\n","5d7bb898":"* From above two plots it is clearly perceived that the distribution of all the 300 features are almost identicle, all have almost same means and standard deviations. \n* If you look into the each graph of feature, you would found that even in signle feature distribution of target 0 and target 1 points are overlaped. skew values of all the features distribution are in range of [-0.37 , 0.43] and kurtosis values are in range [-0.78, 0.98] thus we can say all distributions are approximately symmetric but not perfecltly noraml distributed","14b1f737":"### Box Plot of all 300 features in single frame","5eb5b23f":"## Source\/Useful Links","488cf861":"### Columns\n\n\n\n*   id- sample id\n*   target- a binary target of mysterious origin.\n*   0-299- continuous variables.","e3cc8e5f":"* It is clearly visible that dataset is imbalanced where data points belonging to target 0 is 36%% but target 1 to 64%%. target 1 is roughly 2 times target 0","4ecb0fec":"* target 0 and rarget 1 points are getting more separated as the dimension increased thus in high dimension, it will be more distinguishable","ccbe6775":"### Quartile based adaptive binning of features 94, and 270","fb0ec091":"### Files\n\n\n*   train.csv - the training set. 250 rows.\n*   test.csv - the test set. 19,750 rows.\n*   sample_submission.csv - a sample submission file in the correct format","0933a3a9":"* From the above heatmap it is really difficult to get exact value of correlation between the features but from colorbar it can say that features are not highly correlated as the range fo correlation is in range of [-0.2 to 0.2]","3ee0ed88":"## Binning of Features which skewness>0.4 OR skewness<-0.4","97c1110d":"### Pair Plot of top 5 positively correlated features with target","7b6b3e7a":"# Don't Overfit! II","950fd276":"## Multivariate Analysis","a219a2ea":"Pair Plot of top 5 netatively correlated features with target","3bf6bab2":"* Here also features [217, 117, 91, 295, 73] would be the important features to make classification, as it is seen clearly that they are not fully overlaped but quite seperated","5dd3139d":"### Target","214b98f9":"### Top 10 Correlated features with target","146135d9":"## Performance Metric\n\n\n\n*   AUROC\n\n","57d0e00b":"* Feature 33 and 217 are most correlated with the target","a6739617":"## Bivariate Analysis","44f2c954":"### 3D visualization of 300D data point","bae61412":"## Data","74d669d4":"### Correlation","9741536d":"## Null Values present?","6c335099":"# Import Dataset"}}