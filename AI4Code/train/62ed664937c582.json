{"cell_type":{"795ae21f":"code","4a765189":"code","1e341d12":"code","0935f25f":"code","175e0ff5":"code","44cd8059":"code","05bc222d":"code","ac54902b":"code","41f48bbb":"code","0ca40f8f":"code","4334e195":"code","a1ae8e9e":"code","94b2ed56":"code","5471cbdb":"code","5f8033c9":"code","2fdf6c27":"code","74edc86d":"code","8585de71":"code","ee4d819f":"code","d6c86857":"code","79549154":"code","b6e40257":"markdown","c7a6d8f2":"markdown","489bcd3b":"markdown","fb586056":"markdown","e59aa17d":"markdown","9ce505f7":"markdown","d45f8b01":"markdown","bde4b6cc":"markdown","9524bb53":"markdown","d51edf0a":"markdown"},"source":{"795ae21f":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n\nimport os\n\n'''\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n'''\n ","4a765189":"import l5kit \n\nfrom l5kit.data import ChunkedDataset, LocalDataManager\nfrom l5kit.dataset import EgoDataset, AgentDataset\n\nfrom l5kit.rasterization import build_rasterizer\nfrom l5kit.configs import load_config_data\nfrom l5kit.visualization import draw_trajectory, TARGET_POINTS_COLOR\nfrom l5kit.geometry import transform_points\n\nfrom l5kit.data import PERCEPTION_LABELS\n\nimport zarr\nimport os\n\nl5kit.__version__","1e341d12":"os.environ[\"L5KIT_DATA_FOLDER\"] = \"\/kaggle\/input\/lyft-motion-prediction-autonomous-vehicles\"\n# get config\ncfg = load_config_data(\"\/kaggle\/input\/lyft-config-files\/visualisation_config.yaml\")\ncfg1 = load_config_data(\"\/kaggle\/input\/lyft-config-files\/agent_motion_config.yaml\")\nprint(cfg1)","0935f25f":"data = zarr.open(\"\/kaggle\/input\/lyft-motion-prediction-autonomous-vehicles\/scenes\/train.zarr\/\")\nprint(data.info)","175e0ff5":"dm = LocalDataManager()\n#dataset_path = dm.require(cfg[\"val_data_loader\"][\"key\"])\ndataset_path = '\/kaggle\/input\/lyft-motion-prediction-autonomous-vehicles\/scenes\/sample.zarr'\nzarr_dataset = ChunkedDataset(dataset_path)\nzarr_dataset.open()\nprint(zarr_dataset)","44cd8059":"train_cfg = cfg1[\"train_data_loader\"]\nrasterizer = build_rasterizer(cfg, dm)\nagent_zarr = ChunkedDataset(dm.require(train_cfg[\"key\"])).open()\nagent_dataset = AgentDataset(cfg, agent_zarr, rasterizer)\nprint(agent_dataset)","05bc222d":"agents = pd.DataFrame.from_records(zarr_dataset.agents, columns = ['centroid', 'extent', 'yaw', 'velocity', 'track_id', 'label_probabilities'])\nframes = pd.DataFrame.from_records(zarr_dataset.frames, columns = ['timestamp','agent_index_interval','traffic_light_faces_index_interval','ego_translation','ego_rotation'])\nscenes = pd.DataFrame.from_records(zarr_dataset.scenes, columns = ['frame_index_interval', 'host', 'start_time','end_time'])\ntraffic_light = pd.DataFrame.from_records(zarr_dataset.tl_faces, columns = ['face_id','traffic_light_id','traffic_light_face_status'])","ac54902b":"agents[['centroid_x','centroid_y']] = agents.centroid.tolist()\nagents[['extent_x','extent_y','extent_z']] = agents.extent.tolist()\nagents[['velocity_x', 'velocity_y']] =  agents.velocity.tolist()\nagents.drop(['centroid','extent','velocity'],axis = 'columns',inplace = True)\nPERCEPTION_LABELS = [\n    \"PERCEPTION_LABEL_NOT_SET\",\n    \"PERCEPTION_LABEL_UNKNOWN\",\n    \"PERCEPTION_LABEL_DONTCARE\",\n    \"PERCEPTION_LABEL_CAR\",\n    \"PERCEPTION_LABEL_VAN\",\n    \"PERCEPTION_LABEL_TRAM\",\n    \"PERCEPTION_LABEL_BUS\",\n    \"PERCEPTION_LABEL_TRUCK\",\n    \"PERCEPTION_LABEL_EMERGENCY_VEHICLE\",\n    \"PERCEPTION_LABEL_OTHER_VEHICLE\",\n    \"PERCEPTION_LABEL_BICYCLE\",\n    \"PERCEPTION_LABEL_MOTORCYCLE\",\n    \"PERCEPTION_LABEL_CYCLIST\",\n    \"PERCEPTION_LABEL_MOTORCYCLIST\",\n    \"PERCEPTION_LABEL_PEDESTRIAN\",\n    \"PERCEPTION_LABEL_ANIMAL\",\n    \"AVRESEARCH_LABEL_DONTCARE\"]   #only these perception labels were true hence using only these values\nagents[PERCEPTION_LABELS]= agents['label_probabilities'].tolist()","41f48bbb":"agents.drop(columns = ['PERCEPTION_LABEL_NOT_SET', 'AVRESEARCH_LABEL_DONTCARE', 'PERCEPTION_LABEL_ANIMAL', 'PERCEPTION_LABEL_MOTORCYCLE', 'PERCEPTION_LABEL_MOTORCYCLIST', 'PERCEPTION_LABEL_BICYCLE',  'PERCEPTION_LABEL_OTHER_VEHICLE', 'PERCEPTION_LABEL_EMERGENCY_VEHICLE', 'PERCEPTION_LABEL_DONTCARE', 'PERCEPTION_LABEL_VAN', 'PERCEPTION_LABEL_TRAM', 'PERCEPTION_LABEL_BUS', 'PERCEPTION_LABEL_TRUCK'], inplace = True)\nagents.drop(columns = ['label_probabilities'], inplace = True)\nagents.head()","0ca40f8f":"frames[['frame_id']] = frames.index\nscenes['frame_index_interval'] = scenes['frame_index_interval'].tolist()","4334e195":"df = pd.DataFrame( columns = ['host','start_time','end_time'])\nfor i in range(0,100):\n    for j in range(scenes['frame_index_interval'][i][0], scenes['frame_index_interval'][i][1]):\n        df = df.append(scenes.iloc[i,1:4], ignore_index=True)\ndf[['frame_id']] = df.index        \n        \n         ","a1ae8e9e":"df = df.merge(frames, on = df.frame_id, how = 'left')\ndf[['ego_translation_x', 'ego_translation_y', 'ego_translation_z']] = df['ego_translation'].tolist()\ndf['ego_rotation'] = df['ego_rotation'].tolist()\ndf.drop(['frame_id_x','frame_id_y','ego_translation'], axis = 'columns', inplace = True)\ndf.head()","94b2ed56":"\n#sns.scatterplot(df['ego_translation_x'], df['ego_rotation_y'], df['ego_translation_z'])\nfig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(df['ego_translation_x'], df['ego_translation_y'], df['ego_translation_z'], c='skyblue', s=60)\nax.view_init(10, 50)\nplt.show()\n","5471cbdb":"fig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(agents['extent_x'], agents['extent_y'], agents['extent_z'], c='green', s=60)\nax.view_init(10, 50)\nplt.show()","5f8033c9":"fig = plt.figure()\nax = fig.add_subplot(111, projection='3d')\nax.scatter(agents.iloc[38:86, 5], agents.iloc[38:86, 6], agents.iloc[38:86, 7], c='green', s=60)\nax.view_init(10, 50)\nplt.show()","2fdf6c27":"sns.scatterplot(x = agents.centroid_x, y = agents.centroid_y, color = 'blue')","74edc86d":"sns.scatterplot(x = agents[agents.velocity_x != 0].velocity_x, y = agents[agents.velocity_y != 0].velocity_y, color = 'blue')","8585de71":"#agents.head()\nsns.distplot(agents.yaw)","ee4d819f":"subsample = agents.iloc[38:86]\n\n#Create combo chart\nfig, ax1 = plt.subplots(figsize=(10,6))\n \nax1 = sns.scatterplot(x = subsample.centroid_x, y = subsample.centroid_y, color = 'blue')\n\n#specify we want to share the same x-axis\nax2 = ax1.twinx()\ncolor = 'tab:red'\n#line plot creation\n\nax2 = sns.scatterplot([df.iloc[1,8]], [df.iloc[1,9]], color=color)\nax2.tick_params(axis='y', color=color)\n#show plot\nplt.show()","d6c86857":"dataset_path = '\/kaggle\/input\/lyft-motion-prediction-autonomous-vehicles\/scenes\/train.zarr'\nzarr_dataset = ChunkedDataset(dataset_path)\nzarr_dataset.open()\nrast = build_rasterizer(cfg, dm)\ntrain_agent_dataset = AgentDataset(cfg, zarr_dataset, rast)","79549154":"data = train_agent_dataset[1]\ncfg['raster_params']['map_type'] = \"py_satellite\"\n\nim = data[\"image\"].transpose(1, 2, 0)\nim = train_agent_dataset.rasterizer.to_rgb(im)\ntarget_positions_pixels = transform_points(data[\"target_positions\"] + data[\"centroid\"][:2], data[\"world_to_image\"])\n#draw_trajectory(im, target_positions_pixels, data[\"target_yaws\"], TARGET_POINTS_COLOR)\n\nplt.imshow(im[::-1])\nplt.show()","b6e40257":"Reading agents, frames, scenes and traffic light dataset into csv files","c7a6d8f2":"This is the ego motion for the whole scenario.","489bcd3b":"Agent representation for the first scene.","fb586056":"Agent Representation in 2-D graph","e59aa17d":"In this kernel, I am working on Lyft sample dataset. I have tried merging frame dataset with scene dataset, so that I could replicate the scenes both with and without using Rastiser library provided by L5Kit. Also using the curated dataset, I will try to do some feature engineering and train models on it. ","9ce505f7":"Here the red dot represents the AV, while blue dots depicts the agents with which it is surrounded.","d45f8b01":"All the agents, encountered by the car during its motion.","bde4b6cc":"Merging scene dataset and frame dataset using frame_id key","9524bb53":"There is alot of a work yet to be done wrt to feature engineering and model training.","d51edf0a":"Using the L5Kit's Rastesizer library."}}