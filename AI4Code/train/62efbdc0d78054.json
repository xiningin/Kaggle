{"cell_type":{"d5eb1d51":"code","3315f6b2":"code","716d7ffd":"code","9165ca04":"code","113e9231":"code","117b92d7":"code","30b4aba0":"code","7845a71d":"code","8b5ff8bc":"code","fab30892":"code","c8f50f6b":"code","9b95056d":"code","41539873":"code","a1d90daf":"code","65623237":"code","d9ee2e5f":"code","21e043fa":"code","c0bfbdbd":"code","9788ba7b":"code","cb0e7dd2":"code","9214f65e":"code","d20d94c1":"code","e24bbdaa":"markdown","21c3833b":"markdown","983245bc":"markdown","434fafed":"markdown","bcacdbce":"markdown","bd2cfdc9":"markdown","c3b3b862":"markdown","4db7d347":"markdown","87b451e6":"markdown","e1105cee":"markdown","9c008f2c":"markdown"},"source":{"d5eb1d51":"!pip install -U layoutparser\n!pip install layoutparser[ocr]     \n!pip install 'git+https:\/\/github.com\/facebookresearch\/detectron2.git@v0.4#egg=detectron2' \n!git clone https:\/\/github.com\/Layout-Parser\/layout-parser.git\n%cd layout-parser\/","3315f6b2":"import cv2\nimage = cv2.imread(\"\/kaggle\/working\/layout-parser\/examples\/data\/paper-image.jpg\")\nimage = image[..., ::-1] ","716d7ffd":"import layoutparser as lp\nmodel = lp.Detectron2LayoutModel('lp:\/\/PubLayNet\/faster_rcnn_R_50_FPN_3x\/config',\n                                 extra_config=[\"MODEL.ROI_HEADS.SCORE_THRESH_TEST\", 0.8],\n                                 label_map={0: \"Text\", 1: \"Title\", 2: \"List\", 3:\"Table\", 4:\"Figure\"})\nlayout = model.detect(image)\nlp.draw_box(image, layout, box_width=3)","9165ca04":"text_blocks = lp.Layout([b for b in layout if b.type==\"Table\"])\nfigure_blocks = lp.Layout([b for b in layout if b.type=='Figure'])","113e9231":"text_blocks_1 = lp.Layout([b for b in layout if b.type=='Text'])\nfigure_blocks_1 = lp.Layout([b for b in layout if b.type=='Figure'])","117b92d7":"text_blocks_2 = lp.Layout([b for b in layout if b.type==\"Title\"])\nfigure_blocks_2 = lp.Layout([b for b in layout if b.type=='Figure'])","30b4aba0":"text_blocks = lp.Layout([b for b in text_blocks \\\n                   if not any(b.is_in(b_fig) for b_fig in figure_blocks)])","7845a71d":"text_blocks_1 = lp.Layout([b for b in text_blocks_1 \\\n                   if not any(b.is_in(b_fig) for b_fig in figure_blocks_1)])","8b5ff8bc":"text_blocks_2 = lp.Layout([b for b in text_blocks_2 \\\n                   if not any(b.is_in(b_fig) for b_fig in figure_blocks_2)])","fab30892":"h, w = image.shape[:2]\nleft_interval = lp.Interval(0, w\/2*1.05, axis='x').put_on_canvas(image)\nleft_blocks = text_blocks.filter_by(left_interval, center=True)\nleft_blocks.sort(key = lambda b:b.coordinates[1])\nright_blocks = [b for b in text_blocks if b not in left_blocks]\nright_blocks.sort(key = lambda b:b.coordinates[1])\ntext_blocks = lp.Layout([b.set(id = idx) for idx, b in enumerate(left_blocks + right_blocks)])","c8f50f6b":"h, w = image.shape[:2]\nleft_interval = lp.Interval(0, w\/2*1.05, axis='x').put_on_canvas(image)\nleft_blocks = text_blocks_1.filter_by(left_interval, center=True)\nleft_blocks.sort(key = lambda b:b.coordinates[1])\nright_blocks = [b for b in text_blocks_1 if b not in left_blocks]\nright_blocks.sort(key = lambda b:b.coordinates[1])\ntext_blocks_1 = lp.Layout([b.set(id = idx) for idx, b in enumerate(left_blocks + right_blocks)])","9b95056d":"h, w = image.shape[:2]\nleft_interval_2 = lp.Interval(0, w\/2*1.05, axis='x').put_on_canvas(image)\nleft_blocks_2 = text_blocks_2.filter_by(left_interval_2, center=True)\nleft_blocks_2.sort(key = lambda b:b.coordinates[1])\nright_blocks_2 = [b for b in text_blocks_2 if b not in left_blocks_2]\nright_blocks_2.sort(key = lambda b:b.coordinates[1])\ntext_blocks_2 = lp.Layout([b.set(id = idx) for idx, b in enumerate(left_blocks_2 + right_blocks_2)])","41539873":"lp.draw_box(image, text_blocks_1,\n            box_width=3, \n            show_element_id=True)","a1d90daf":"lp.draw_box(image, text_blocks,\n            box_width=3, \n            show_element_id=True)","65623237":"lp.draw_box(image, text_blocks_2,\n            box_width=3, \n            show_element_id=True)","d9ee2e5f":"ocr_agent = lp.TesseractAgent(languages='eng') ","21e043fa":"for block in text_blocks:\n    segment_image = (block\n                       .pad(left=5, right=5, top=5, bottom=5)\n                       .crop_image(image))\n        \n    text = ocr_agent.detect(segment_image)\n    block.set(text=text, inplace=True)","c0bfbdbd":"for block in text_blocks_1:\n    segment_image_1 = (block\n                       .pad(left=5, right=5, top=5, bottom=5)\n                       .crop_image(image))\n        \n    text_1 = ocr_agent.detect(segment_image_1)\n    block.set(text=text_1, inplace=True)","9788ba7b":"for block in text_blocks_2:\n    segment_image_2 = (block\n                       .pad(left=5, right=5, top=5, bottom=5)\n                       .crop_image(image))\n        \n    text_2 = ocr_agent.detect(segment_image_2)\n    block.set(text=text_2, inplace=True)","cb0e7dd2":"for txt in text_blocks.get_texts():\n    print(txt, end='\\n---\\n')","9214f65e":"for txt in text_blocks_1.get_texts():\n    print(txt, end='\\n---\\n')","d20d94c1":"for txt in text_blocks_2.get_texts():\n    print(txt, end='\\n---\\n')","e24bbdaa":"**In this we are separating the detected Text part of our image**I","21c3833b":"**Here is our extracted text part**","983245bc":"**In this we are separating the detected Table part of our image**I","434fafed":"**To load an image we have used openCV and this cell is basically loading the image**","bcacdbce":"**In this we are detecting the Table part of our image**I","bd2cfdc9":"**In this cell we are loading our layout parser and the asking the \nparser to parse our loaded image and draw a boundary after each cell it has detected**","c3b3b862":"**Here is our extracted Title part**","4db7d347":"**In this we are detecting the text part of our image**I","87b451e6":"**Here is our extracted Table part**","e1105cee":"**In this we are detecting the title part of our image**I","9c008f2c":"**In this we are separating the detected Title part of our image**I"}}