{"cell_type":{"24ca73bf":"code","a97f6e59":"code","2bcee7b2":"code","1989ac29":"code","211c8552":"code","b7a70ad7":"code","0c991c5f":"code","780f89ce":"code","a06b61db":"code","b0c95ac8":"code","d579eda7":"code","cc5ac9fc":"code","92ab9bd4":"code","2521039f":"code","11e992a6":"code","5ce95508":"code","300b004a":"code","3282741e":"code","ece8bcff":"code","a96c2392":"code","f508de6a":"code","e63ba9aa":"code","0203fdaa":"code","0699cc42":"code","915876d9":"code","01f6d748":"code","b55fc777":"code","1dc155b3":"code","58a7c814":"code","7bd57c16":"code","5036ceae":"code","cf5005fa":"code","7699813b":"markdown","fde834a7":"markdown","2032a8cb":"markdown","4e0d0aa5":"markdown","818c01f6":"markdown","796e6d30":"markdown","58f1cd79":"markdown"},"source":{"24ca73bf":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nimport seaborn as sns\nplt.style.use('ggplot')\nimport re\nimport nltk\nfrom nltk.util import ngrams\nfrom nltk.corpus import stopwords\nstop=set(stopwords.words('english'))\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom collections import defaultdict\nfrom collections import  Counter\nfrom sklearn.model_selection import train_test_split\nimport keras\nfrom keras.models import Sequential\nfrom keras.initializers import Constant\nfrom keras.layers import (LSTM, \n                          Embedding, \n                          BatchNormalization,\n                          Dense, \n                          TimeDistributed, \n                          Dropout, \n                          Bidirectional,\n                          Flatten, \n                          GlobalMaxPool1D)\nfrom nltk.tokenize import word_tokenize\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers.embeddings import Embedding\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau\nfrom keras.optimizers import Adam\nfrom sklearn.metrics import (\n    precision_score, \n    recall_score, \n    f1_score, \n    classification_report,\n    accuracy_score\n)","a97f6e59":"tweet = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\nsubmission = pd.read_csv('..\/input\/nlp-getting-started\/sample_submission.csv')","2bcee7b2":"#Checking the class distribution\nx = tweet.target.value_counts()\nsns.barplot(x.index, x, palette='cool')\nplt.gca().set_ylabel('tweets')","1989ac29":"#Number of characters in tweets\nfig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\ntweet_len = tweet[tweet['target']==1]['text'].str.len()\nax1.hist(tweet_len,color='crimson')\nax1.set_title('Disaster tweets')\ntweet_len = tweet[tweet['target']==0]['text'].str.len()\nax2.hist(tweet_len,color='skyblue')\nax2.set_title('Non disaster tweets')\nfig.suptitle('Characters in tweets')","211c8552":"#Number of words in a tweet\nfig,(ax1,ax2) = plt.subplots(1,2,figsize=(10,5))\ntweet_len = tweet[tweet['target']==1]['text'].str.split().map(lambda x: len(x))\nax1.hist(tweet_len, color='black')\nax1.set_title('Disaster tweets')\ntweet_len = tweet[tweet['target']==0]['text'].str.split().map(lambda x: len(x))\nax2.hist(tweet_len,color='purple')\nax2.set_title('Non disaster tweets')\nfig.suptitle('Words in a tweet')","b7a70ad7":"fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\nword = tweet[tweet['target']==1]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax1,color='darkblue')\nax1.set_title('Disaster')\nword = tweet[tweet['target']==0]['text'].str.split().apply(lambda x : [len(i) for i in x])\nsns.distplot(word.map(lambda x: np.mean(x)),ax=ax2,color='magenta')\nax2.set_title('Non disaster')\nfig.suptitle('Average word length in each tweet')","0c991c5f":"def create_corpus(target):\n    corpus=[]\n    for x in tweet[tweet['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","780f89ce":"def create_corpus_df(tweet, target):\n    corpus=[]\n    for x in tweet[tweet['target']==target]['text'].str.split():\n        for i in x:\n            corpus.append(i)\n    return corpus","a06b61db":"#Punctuations in non-disaster class\nplt.figure(figsize=(10,5))\ncorpus = create_corpus(0)\ndic = defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\nx,y=zip(*dic.items())\nplt.bar(x, y,color='cyan')","b0c95ac8":"#Punctuations in disaster class\nplt.figure(figsize=(10,5))\ncorpus = create_corpus(1)\ndic = defaultdict(int)\nimport string\nspecial = string.punctuation\nfor i in (corpus):\n    if i in special:\n        dic[i]+=1\nx,y = zip(*dic.items())\nplt.bar(x, y, color='red')","d579eda7":"#Common words\ncounter = Counter(corpus)\nmost = counter.most_common()\nx=[]\ny=[]\nfor word, count in most[:40]:\n    if (word not in stop) :\n        x.append(word)\n        y.append(count)\nsns.barplot(x=y, y=x, palette='Greens_r')","cc5ac9fc":"#Bigram analysis\ndef get_top_tweet_bigrams(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq = sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\nplt.figure(figsize=(10,5))\ntop_tweet_bigrams = get_top_tweet_bigrams(tweet['text'])[:10]\nx,y = map(list,zip(*top_tweet_bigrams))\nsns.barplot(x=y,y=x, palette='Reds_r')","92ab9bd4":"df = pd.concat([tweet,test])\ndf.shape","2521039f":"#Renaming location names\ndf['location'].replace({'United States':'USA',\n                           'New York':'USA',\n                            \"London\":'UK',\n                            \"Los Angeles, CA\":'USA',\n                            \"Washington, D.C.\":'USA',\n                            \"California\":'USA',\n                             \"Chicago, IL\":'USA',\n                             \"Chicago\":'USA',\n                            \"New York, NY\":'USA',\n                            \"California, USA\":'USA',\n                            \"FLorida\":'USA',\n                            \"Nigeria\":'Africa',\n                            \"Kenya\":'Africa',\n                            \"Everywhere\":'Worldwide',\n                            \"San Francisco\":'USA',\n                            \"Florida\":'USA',\n                            \"United Kingdom\":'UK',\n                            \"Los Angeles\":'USA',\n                            \"Toronto\":'Canada',\n                            \"San Francisco, CA\":'USA',\n                            \"NYC\":'USA',\n                            \"Seattle\":'USA',\n                            \"Earth\":'Worldwide',\n                            \"Ireland\":'UK',\n                            \"London, England\":'UK',\n                            \"New York City\":'USA',\n                            \"Texas\":'USA',\n                            \"London, UK\":'UK',\n                            \"Atlanta, GA\":'USA',\n                            \"Mumbai\":\"India\"},inplace=True)\nsns.barplot(y = df['location'].value_counts()[:5].index, x = df['location'].value_counts()[:5],\n            palette='autumn', orient='h')","11e992a6":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text\n# Applying the cleaning function to both test and training datasets\ndf['text'] = df['text'].apply(lambda x: clean_text(x))\n# Let's take a look at the updated text\ndf['text'].head()","5ce95508":"#Removing Emojis\ndef remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)\ndf['text']=df['text'].apply(lambda x: remove_emoji(x))","300b004a":"tweet_1 = tweet.text.values\ntest_1 = test.text.values\nsentiments = tweet.target.values","3282741e":"word_tokenizer = Tokenizer()\nword_tokenizer.fit_on_texts(tweet_1)\nvocab_length = len(word_tokenizer.word_index) + 1","ece8bcff":"def metrics(pred_tag, y_test):\n    print(\"F1-score: \", f1_score(pred_tag, y_test))\n    print(\"Precision: \", precision_score(pred_tag, y_test))\n    print(\"Recall: \", recall_score(pred_tag, y_test))\n    print(\"Acuracy: \", accuracy_score(pred_tag, y_test))\n    print(\"-\"*50)\n    print(classification_report(pred_tag, y_test))\ndef embed(corpus): \n    return word_tokenizer.texts_to_sequences(corpus)","a96c2392":"def plot(history, arr):\n    fig, ax = plt.subplots(1, 2, figsize=(20, 5))\n    for idx in range(2):\n        ax[idx].plot(history.history[arr[idx][0]])\n        ax[idx].plot(history.history[arr[idx][1]])\n        ax[idx].legend([arr[idx][0], arr[idx][1]],fontsize=18)\n        ax[idx].set_xlabel('A ',fontsize=16)\n        ax[idx].set_ylabel('B',fontsize=16)\n        ax[idx].set_title(arr[idx][0] + ' X ' + arr[idx][1],fontsize=16)","f508de6a":"longest_train = max(tweet_1, key=lambda sentence: len(word_tokenize(sentence)))\nlength_long_sentence = len(word_tokenize(longest_train))\npadded_sentences = pad_sequences(embed(tweet_1), length_long_sentence, padding='post')\ntest_sentences = pad_sequences(\n    embed(test_1), \n    length_long_sentence,\n    padding='post'\n)","e63ba9aa":"embeddings_dictionary = dict()\nembedding_dim = 100\nglove_file = open('..\/input\/glove-file\/glove.6B.100d.txt')\nfor line in glove_file:\n    records = line.split()\n    word = records[0]\n    vector_dimensions = np.asarray(records[1:], dtype='float32')\n    embeddings_dictionary [word] = vector_dimensions\nglove_file.close()","0203fdaa":"embedding_matrix = np.zeros((vocab_length, embedding_dim))\nfor word, index in word_tokenizer.word_index.items():\n    embedding_vector = embeddings_dictionary.get(word)\n    if embedding_vector is not None:\n        embedding_matrix[index] = embedding_vector","0699cc42":"X_train, X_test, y_train, y_test = train_test_split(\n    padded_sentences, \n    sentiments, \n    test_size=0.25\n)","915876d9":"def BLSTM():\n    model = Sequential()\n    model.add(Embedding(input_dim=embedding_matrix.shape[0], \n                        output_dim=embedding_matrix.shape[1], \n                        weights = [embedding_matrix], \n                        input_length=length_long_sentence))\n    model.add(Bidirectional(LSTM(length_long_sentence, return_sequences = True, recurrent_dropout=0.2)))\n    model.add(GlobalMaxPool1D())\n    model.add(BatchNormalization())\n    model.add(Dropout(0.5))\n    model.add(Dense(length_long_sentence, activation = \"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(length_long_sentence, activation = \"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(1, activation = 'sigmoid'))\n    model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n    return model","01f6d748":"model = BLSTM()\ncheckpoint = ModelCheckpoint(\n    'model.h5', \n    monitor = 'val_loss', \n    verbose = 1, \n    save_best_only = True\n)\nreduce_lr = ReduceLROnPlateau(\n    monitor = 'val_loss', \n    factor = 0.2, \n    verbose = 1, \n    patience = 5,                        \n    min_lr = 0.001\n)\nhistory = model.fit(\n    X_train, \n    y_train, \n    epochs = 7,\n    batch_size = 32,\n    validation_data = [X_test, y_test],\n    verbose = 1,\n    callbacks = [reduce_lr, checkpoint]\n)","b55fc777":"plot(history, [['loss', 'val_loss'],['accuracy', 'val_accuracy']])","1dc155b3":"loss, accuracy = model.evaluate(X_test, y_test)\nprint('Loss:', loss)\nprint('Accuracy:', accuracy)","58a7c814":"preds = model.predict_classes(X_test)\nmetrics(preds, y_test)","7bd57c16":"model.load_weights('model.h5')\npreds = model.predict_classes(X_test)\nmetrics(preds, y_test)","5036ceae":"submission.target = model.predict_classes(test_sentences)\nsubmission.to_csv(\"submission.csv\", index=False)\nsubmission.target.value_counts().plot.bar();","cf5005fa":"submission","7699813b":"# Project Goal\nTo analyze tweets classifying them into **disaster** and **non-disaster** ones to extract useful information during crises","fde834a7":"To obtain a vector representation for words we can use an unsupervised learning algorithm called **GloVe (Global Vectors for Word Representation)**, which focuses on words co-occurrences over the whole corpus. Its embeddings relate to the probabilities that two words appear together","2032a8cb":"We are going to use **LSTM (long short-term memory)** model because it solves a vanishing gradient problem","4e0d0aa5":"# EDA and Preprocessing","818c01f6":"**References**\n\n* https:\/\/www.kaggle.com\/shahules\/basic-eda-cleaning-and-glove\n* https:\/\/www.kaggle.com\/xwalker\/glove-bilstm\n* http:\/\/nltk.org\/.\n* Deep Learning. Ian Goodfellow, Yoshua Bengio, Aaron Courville","796e6d30":"We need to perform **tokenization** - the processing of segmenting text into sentences of words. In the process we throw away punctuation and extra symbols too. The benefit of tokenization is that it gets the text into a format that is easier to convert to raw numbers, which can actually be used for processing ","58f1cd79":"# Model"}}