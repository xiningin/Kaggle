{"cell_type":{"9e9ad1fe":"code","7f10afa3":"code","d8ae637a":"code","44c6b3c6":"code","62c30243":"code","4dffb447":"code","d0b4229a":"code","174ea680":"code","44370eef":"code","8e368240":"code","8adb5891":"code","68d109b9":"code","bce48a3a":"code","94e2a46a":"code","61b89098":"code","135b3b75":"code","0477dd6a":"code","f113bf24":"code","f4b1ed1c":"code","54564d0a":"code","4f31f933":"code","aa4bcd05":"code","a7e45001":"code","8fb137d0":"code","e4c25e0b":"code","652e6c48":"code","9f4336e1":"code","f959a0c0":"code","0b722fa7":"code","b54c8db7":"code","a29f9668":"code","34a1b670":"code","ecdbb1d6":"code","b7f03fda":"code","45ee6db5":"code","5ed43cf5":"code","098cccc8":"code","60bdc504":"markdown","d4beea2c":"markdown","1a9b2be8":"markdown","cd10eaf2":"markdown","75dc0fcf":"markdown","ac7624cc":"markdown","f89ac910":"markdown","3bb3ce73":"markdown","01577726":"markdown","ba41f6c5":"markdown","40895722":"markdown","12a5cc7d":"markdown","ee853036":"markdown","acc6c508":"markdown","81f163ba":"markdown","fa5312d4":"markdown","ef6dd932":"markdown","c24349af":"markdown","502c9b47":"markdown","f0971ea3":"markdown","7e3ad478":"markdown","36636457":"markdown"},"source":{"9e9ad1fe":"!pip -q install sacrebleu\n!pip -q install googletrans\n!pip -q install tensorflow-addons --upgrade","7f10afa3":"import random, re, string, itertools, timeit, sacrebleu\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm.notebook import tqdm\nfrom IPython.display import display, clear_output\nfrom sklearn.model_selection import train_test_split\n\n# Tensorflow & Keras\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import Input, Dense, LSTM, LSTMCell\nfrom tensorflow.keras.layers import Embedding, Bidirectional\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.preprocessing.text import Tokenizer\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.losses import SparseCategoricalCrossentropy\nfrom tensorflow.keras.optimizers import Adam\n\n# Japanese Word Tokenizer\nfrom janome.tokenizer import Tokenizer as janome_tokenizer\n\nplt.style.use('seaborn-pastel')","d8ae637a":"# Download Data & Unzip Data\n!wget http:\/\/www.manythings.org\/anki\/jpn-eng.zip\n!unzip jpn-eng.zip","44c6b3c6":"data = []\n\nf1 = open('.\/jpn.txt', 'r')\ndata += [x.rstrip().lower().split('\\t')[:2] for x in tqdm(f1.readlines())]\nf1.close()\n\nprint(f'Loaded {len(data)} Sentences')","62c30243":"mispell_dict = {\n    \"aren't\" : \"are not\",\n    \"can't\" : \"cannot\",\n    \"couldn't\" : \"could not\",\n    \"didn't\" : \"did not\",\n    \"doesn't\" : \"does not\",\n    \"don't\" : \"do not\",\n    \"hadn't\" : \"had not\",\n    \"hasn't\" : \"has not\",\n    \"haven't\" : \"have not\",\n    \"he'd\" : \"he would\",\n    \"he'll\" : \"he will\",\n    \"he's\" : \"he is\",\n    \"i'd\" : \"i would\",\n    \"i'd\" : \"i had\",\n    \"i'll\" : \"i will\",\n    \"i'm\" : \"i am\",\n    \"isn't\" : \"is not\",\n    \"it's\" : \"it is\",\n    \"it'll\":\"it will\",\n    \"i've\" : \"i have\",\n    \"let's\" : \"let us\",\n    \"mightn't\" : \"might not\",\n    \"mustn't\" : \"must not\",\n    \"shan't\" : \"shall not\",\n    \"she'd\" : \"she would\",\n    \"she'll\" : \"she will\",\n    \"she's\" : \"she is\",\n    \"shouldn't\" : \"should not\",\n    \"that's\" : \"that is\",\n    \"there's\" : \"there is\",\n    \"they'd\" : \"they would\",\n    \"they'll\" : \"they will\",\n    \"they're\" : \"they are\",\n    \"they've\" : \"they have\",\n    \"we'd\" : \"we would\",\n    \"we're\" : \"we are\",\n    \"weren't\" : \"were not\",\n    \"we've\" : \"we have\",\n    \"what'll\" : \"what will\",\n    \"what're\" : \"what are\",\n    \"what's\" : \"what is\",\n    \"what've\" : \"what have\",\n    \"where's\" : \"where is\",\n    \"who'd\" : \"who would\",\n    \"who'll\" : \"who will\",\n    \"who're\" : \"who are\",\n    \"who's\" : \"who is\",\n    \"who've\" : \"who have\",\n    \"won't\" : \"will not\",\n    \"wouldn't\" : \"would not\",\n    \"you'd\" : \"you would\",\n    \"you'll\" : \"you will\",\n    \"you're\" : \"you are\",\n    \"you've\" : \"you have\",\n    \"'re\": \" are\",\n    \"wasn't\": \"was not\",\n    \"we'll\":\" will\",\n    \"didn't\": \"did not\",\n    \"tryin'\":\"trying\"\n}\n\nmispell_re = re.compile('(%s)' % '|'.join(mispell_dict.keys()))\n\ndef preprocess(text) -> str:\n    def replace(match):\n        return mispell_dict[match.group(0)]\n    \n    text = mispell_re.sub(replace, text)\n    return text","4dffb447":"# Adding Japanese Punctuation\nstring.punctuation += '\u3001\u3002\u3010\u3011\u300c\u300d\u300e\u300f\u2026\u30fb\u303d\uff08\uff09\u301c\uff1f\uff01\uff61\uff1a\uff64\uff1b\uff65'\n\nCP = lambda x : x.translate(str.maketrans('', '', string.punctuation))","d0b4229a":"data = [x for x in data if len(x) == 2]\n\neng_data = [CP(preprocess(x[0])) for x in data]\njpn_data = [CP(x[1]) for x in data]","174ea680":"# Initialize Janome Tokenizer\ntoken_jp = janome_tokenizer()","44370eef":"sample_text = '\u3053\u3053\u3067\u79c1\u306f\u82f1\u8a9e\u3067\u8a71\u3057\u3066\u3044\u308b'\n' '.join([word for word in token_jp.tokenize(sample_text, wakati=True) \\\n          if word != ' '])","8e368240":"# Apply to Japanese Sentences\njpn_data = [' '.join([word for word in token_jp.tokenize(x, wakati=True) \\\n                      if word != ' ']) for x in tqdm(jpn_data)]","8adb5891":"eng_train, eng_test, jpn_train, jpn_test = \\\ntrain_test_split(eng_data, jpn_data, test_size = 0.04, random_state = 42)\n\nprint(f\"Splitting to {len(eng_train)} Train data and \\\n{len(eng_test)} Test data\")","68d109b9":"eng_train = ['bos '+ x + ' eos' for x in eng_train + ['unk unk unk']]\njpn_train = ['bos '+ x + ' eos' for x in jpn_train + ['unk unk unk']]\n\neng_val = ['bos '+ x + ' eos' for x in eng_test]\njpn_val = ['bos '+ x + ' eos' for x in jpn_test]","bce48a3a":"# English Tokenizer\nen_tokenizer = Tokenizer(filters='')\nen_tokenizer.fit_on_texts(eng_train)\n\n# Japannese Tokenizer\njp_tokenizer = Tokenizer(filters='')\njp_tokenizer.fit_on_texts(jpn_train)","94e2a46a":"print(f'English vocab size   :', len(en_tokenizer.word_index) - 3)\nprint(f'Japanese vocab size  :', len(jp_tokenizer.word_index) - 3)","61b89098":"!wget https:\/\/noto-website-2.storage.googleapis.com\/pkgs\/NotoSansCJKjp-hinted.zip\n!wget https:\/\/raw.githubusercontent.com\/Hyuto\/NMT-TF-Seq2seq-EN-JP\/master\/Japan.jpg\n!wget https:\/\/raw.githubusercontent.com\/Hyuto\/NMT-TF-Seq2seq-EN-JP\/master\/English.png\n!mkdir font\n!unzip NotoSansCJKjp-hinted.zip -d .\/font","135b3b75":"from wordcloud import WordCloud, ImageColorGenerator\nfrom PIL import Image\n\ndef get_words(arr):\n    keys = list(arr.keys())\n    count = list(arr.values())\n    return ' '.join([x for _,x in sorted(zip(count, keys), reverse = True)][2:])\n\ndef transform(arr):\n    for i in range(len(arr)):\n        for j in range(len(arr[i])):\n            if not any(arr[i][j]):\n                arr[i][j] = np.array([225, 225, 225, 225])\n    return arr\n\nfont_path = '.\/font\/NotoSansCJKjp-Light.otf'\n\n\nmask = '.\/English.png'\nmask = np.array(Image.open(mask))\nmask = transform(mask)\nimage_colors = ImageColorGenerator(mask)\nwords = get_words(en_tokenizer.word_counts).title()\nwc = WordCloud(background_color=\"white\", max_words=2000, random_state=42,\n               width=mask.shape[1], height=mask.shape[0])\nwc = wc.generate(words)\nfig1, ax1 = plt.subplots(figsize=(20,15))\nax1.imshow(wc.recolor(color_func=image_colors), interpolation='bilinear')\nax1.axis(\"off\")\n\nmask = '.\/Japan.jpg'\nmask = np.array(Image.open(mask))\nimage_colors = ImageColorGenerator(mask)\nwords = get_words(jp_tokenizer.word_counts).title()\nwc = WordCloud(collocations=False, background_color=\"white\", mode=\"RGBA\", \n               max_words=6000, font_path=font_path, contour_width=1, \n               scale=5, max_font_size = 50, relative_scaling=0.5, \n               random_state=42, width=mask.shape[1], height=mask.shape[0])\nwc = wc.generate(words)\nfig2, ax2 = plt.subplots(figsize=(20,15))\nax2.imshow(wc.recolor(color_func=image_colors), interpolation='bilinear')\nax2.axis(\"off\")\n\nfig1.savefig('WC_English.png')\nfig2.savefig('WC_Japanese.png')\nplt.close(fig1)\nplt.close(fig2)\n\n!rm -rf .\/font","0477dd6a":"%%HTML\n\n<head>\n    <meta name=\"viewport\" content=\"width=device-width, initial-scale=1\">\n    <style>\n        body {font-family: Arial;}\n        img {max-width:100%; height:auto}\n        .tab {\n          overflow: hidden;\n          border: 1px solid #ccc;\n          background-color: #f1f1f1;\n        }\n        .tab button {\n          background-color: #5ECF57;\n          float: left;\n          border: none;\n          outline: none;\n          cursor: pointer;\n          padding: 14px 16px;\n          transition: 0.3s;\n          font-size: 17px;\n        }\n        .tab button:hover {\n          background-color: #48C768;\n        }\n        .tab button.active {\n          background-color: #48C768;\n        }\n        .tabcontent {\n          display: none;\n          padding: 6px 12px;\n          border: 1px solid #ccc;\n          border-top: none;\n        }\n    <\/style>\n<\/head>\n<body>\n    <h2>Word Cloud<\/h2>\n    <p>Click on the buttons inside the tabbed menu<\/p>\n\n    <div class=\"tab\">\n        <button class=\"tablinks\" onclick=\"openCity(event, 'English')\">English<\/button>\n        <button class=\"tablinks\" onclick=\"openCity(event, 'Japanese')\">Japanese<\/button>\n    <\/div>\n\n    <div id=\"English\" class=\"tabcontent\">\n        <h3>English<\/h3>\n        <p>English Vocabulary WordCloud<\/p>\n        <img src=\".\/WC_English.png\" alt=\"WC_English.png\">\n    <\/div>\n\n    <div id=\"Japanese\" class=\"tabcontent\">\n        <h3>Japanese<\/h3>\n        <p>Japanese Vocabulary WordCloud<\/p>\n        <img src=\".\/WC_Japanese.png\" alt=\"WC_Japanese.png\">\n    <\/div>\n\n    <script>\n    function openCity(evt, cityName) {\n      var i, tabcontent, tablinks;\n      tabcontent = document.getElementsByClassName(\"tabcontent\");\n      for (i = 0; i < tabcontent.length; i++) {\n        tabcontent[i].style.display = \"none\";\n      }\n      tablinks = document.getElementsByClassName(\"tablinks\");\n      for (i = 0; i < tablinks.length; i++) {\n        tablinks[i].className = tablinks[i].className.replace(\" active\", \"\");\n      }\n      document.getElementById(cityName).style.display = \"block\";\n      evt.currentTarget.className += \" active\";\n    }\n    <\/script>\n   \n<\/body>","f113bf24":"def Sequences(texts, tokenizer):\n    res = []\n    for text in texts:\n        seq = []\n        for w in text.split():\n            try:\n                seq.append(tokenizer.word_index[w])\n            except:\n                seq.append(tokenizer.word_index['unk'])\n        res.append(seq)\n    return res","f4b1ed1c":"# Transform Sentences to Sequences\ndata_en = en_tokenizer.texts_to_sequences(eng_train)\ndata_jp = jp_tokenizer.texts_to_sequences(jpn_train)\n\nval_en = Sequences(eng_val, en_tokenizer)\nval_jp = Sequences(jpn_val, jp_tokenizer)","54564d0a":"plt.figure(figsize = (8,8))\nsns.distplot([len(x) for x in data_en], label='English')\nsns.distplot([len(x) for x in data_jp], label='Japanese')\nplt.title('Distribution of Sentences Length')\nplt.legend()\nplt.show()","4f31f933":"max_en = max([len(x) for x in data_en] + [len(x) for x in val_en])\nmax_jp = max([len(x) for x in data_jp] + [len(x) for x in val_jp])\n\nprint(f'Maximum length of English sequences is  {max_en}')\nprint(f'Maximum length of Japanese sequences is {max_jp}')","aa4bcd05":"# Padding Sequences\ndata_en = pad_sequences(data_en, padding='post', maxlen = max_en)\ndata_jp = pad_sequences(data_jp, padding='post', maxlen = max_jp)\n\nval_en = pad_sequences(val_en, padding='post', maxlen = max_en)\nval_jp = pad_sequences(val_jp, padding='post', maxlen = max_jp)","a7e45001":"# Config\nepochs = 7\nBATCH_SIZE = 64\nBUFFER_SIZE = len(data_jp)\nsteps_per_epoch = BUFFER_SIZE\/\/BATCH_SIZE\nval_steps_per_epoch = len(val_jp) \/\/ BATCH_SIZE\nembedding_dims = 256\nrnn_units = 1024\ndense_units = 1024\nDtype = tf.float32","8fb137d0":"def max_len(tensor):\n    \"\"\"\n    Get max len in Sequences\n    \"\"\"\n    return max( len(t) for t in tensor)","e4c25e0b":"# Max Len\nTx = max_len(data_en)\nTy = max_len(data_jp)\n\n# Vocab\ninput_vocab_size = len(en_tokenizer.word_index) + 1   # English\noutput_vocab_size = len(jp_tokenizer.word_index) + 1  # Japanese\n\n# Changging to TF data\ndataset = (tf.data.Dataset.from_tensor_slices((data_en, data_jp))\n           .shuffle(BUFFER_SIZE)\n           .batch(BATCH_SIZE, drop_remainder=True)\n          )\n\nval_dataset = (tf.data.Dataset.from_tensor_slices((val_en, val_jp))\n               .batch(BATCH_SIZE)\n              )","652e6c48":"#ENCODER\nclass EncoderNetwork(tf.keras.Model):\n    def __init__(self,input_vocab_size,embedding_dims, rnn_units ):\n        super().__init__()\n        self.encoder_embedding = Embedding(input_dim=input_vocab_size,\n                                           output_dim=embedding_dims)\n        self.encoder_rnnlayer = LSTM(rnn_units,return_sequences=True,\n                                     return_state=True )\n    \n#DECODER\nclass DecoderNetwork(tf.keras.Model):\n    def __init__(self,output_vocab_size, embedding_dims, rnn_units):\n        super().__init__()\n        self.decoder_embedding = Embedding(input_dim=output_vocab_size,\n                                           output_dim=embedding_dims) \n        self.dense_layer = Dense(output_vocab_size)\n        self.decoder_rnncell = LSTMCell(rnn_units)\n        # Sampler\n        self.sampler = tfa.seq2seq.sampler.TrainingSampler()\n        # Create attention mechanism with memory = None\n        self.attention_mechanism = \\\n            self.build_attention_mechanism(dense_units,None,BATCH_SIZE*[Tx])\n        self.rnn_cell = self.build_rnn_cell(BATCH_SIZE)\n        self.decoder = tfa.seq2seq.BasicDecoder(self.rnn_cell,\n                                                sampler= self.sampler,\n                                                output_layer = self.dense_layer\n                                               )\n\n    def build_attention_mechanism(self, units, memory, MSL):\n        \"\"\"\n        MSL : Memory Sequence Length\n        \"\"\"\n        #return tfa.seq2seq.LuongAttention(units, memory = memory, \n        #                                  memory_sequence_length = MSL)\n        return tfa.seq2seq.BahdanauAttention(units, memory = memory, \n                                             memory_sequence_length = MSL)\n\n    # wrap decodernn cell  \n    def build_rnn_cell(self, batch_size):\n        return tfa.seq2seq.AttentionWrapper(self.decoder_rnncell, \n                                            self.attention_mechanism,\n                                            attention_layer_size=dense_units)\n    \n    def build_decoder_initial_state(self, batch_size, encoder_state, Dtype):\n        decoder_initial_state = self.rnn_cell.get_initial_state(batch_size = batch_size, \n                                                                dtype = Dtype)\n        decoder_initial_state = decoder_initial_state.clone(cell_state = encoder_state) \n        return decoder_initial_state","9f4336e1":"# Build Model\nencoderNetwork = EncoderNetwork(input_vocab_size, embedding_dims, rnn_units)\ndecoderNetwork = DecoderNetwork(output_vocab_size, embedding_dims, rnn_units)\n\n# Optimizer\noptimizer = tf.keras.optimizers.Adam()","f959a0c0":"def loss_function(y_pred, y):\n    #shape of y [batch_size, ty]\n    #shape of y_pred [batch_size, Ty, output_vocab_size] \n    sparsecategoricalcrossentropy = SparseCategoricalCrossentropy(from_logits=True,\n                                                                  reduction='none')\n    loss = sparsecategoricalcrossentropy(y_true=y, y_pred=y_pred)\n    mask = tf.logical_not(tf.math.equal(y,0))   #output 0 for y=0 else output 1\n    mask = tf.cast(mask, dtype=loss.dtype)\n    loss = mask * loss\n    loss = tf.reduce_mean(loss)\n    return loss\n\n@tf.function\ndef train_step(input_batch, output_batch, encoder_initial_cell_state):\n    # initialize loss = 0\n    loss = 0\n    with tf.GradientTape() as tape:\n        encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n        a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp, \n                                                        initial_state = encoder_initial_cell_state)\n\n        # [last step activations,last memory_state] of \n        # encoder passed as input to decoder Network\n         \n        # Prepare correct Decoder input & output sequence data\n        decoder_input = output_batch[:,:-1] # ignore eos\n        # compare logits with timestepped +1 version of decoder_input\n        decoder_output = output_batch[:,1:] #ignore bos\n\n        # Decoder Embeddings\n        decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n\n        # Setting up decoder memory from encoder output \n        # and Zero State for AttentionWrapperState\n        decoderNetwork.attention_mechanism.setup_memory(a)\n        decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\n                                                                           encoder_state=[a_tx, c_tx],\n                                                                           Dtype=tf.float32)\n        \n        # BasicDecoderOutput        \n        outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,\n                                               sequence_length=BATCH_SIZE*[Ty-1])\n\n        logits = outputs.rnn_output\n        \n        # Calculate loss\n        loss = loss_function(logits, decoder_output)\n\n    # Returns the list of all layer variables \/ weights.\n    variables = encoderNetwork.trainable_variables + decoderNetwork.trainable_variables  \n    # differentiate loss wrt variables\n    gradients = tape.gradient(loss, variables)\n\n    # grads_and_vars \u2013 List of(gradient, variable) pairs.\n    grads_and_vars = zip(gradients,variables)\n    optimizer.apply_gradients(grads_and_vars)\n    return loss\n\n@tf.function\ndef evaluate(input_batch, output_batch, encoder_initial_cell_state):\n    loss = 0\n    encoder_emb_inp = encoderNetwork.encoder_embedding(input_batch)\n    a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp, \n                                                    initial_state =encoder_initial_cell_state)\n    decoder_input = output_batch[:,:-1]\n    decoder_output = output_batch[:,1:]\n    decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n    decoderNetwork.attention_mechanism.setup_memory(a)\n    decoder_initial_state = decoderNetwork.build_decoder_initial_state(BATCH_SIZE,\n                                                                       encoder_state=[a_tx, c_tx],\n                                                                       Dtype=tf.float32)\n    outputs, _, _ = decoderNetwork.decoder(decoder_emb_inp,initial_state=decoder_initial_state,\n                                           sequence_length=BATCH_SIZE*[Ty-1])\n    logits = outputs.rnn_output\n    loss = loss_function(logits, decoder_output)\n    return loss","0b722fa7":"# RNN LSTM hidden and memory state initializer\ndef initialize_initial_state():\n    return [tf.zeros((BATCH_SIZE, rnn_units)), tf.zeros((BATCH_SIZE, rnn_units))]","b54c8db7":"# Translate\ndef Translate(input_raw):\n    input_raw = CP(preprocess(input_raw))\n    input_lines = ['bos '+ input_raw + '']\n    \n    input_sequences, unique = [], []\n    for line in input_lines:\n        temp = []\n        for w in line.split(' '):\n            try:\n                temp.append(en_tokenizer.word_index[w])\n            except: # Avoid Error\n                unique.append(w)\n                temp.append(en_tokenizer.word_index['unk'])\n        input_sequences.append(temp)\n    \n    input_sequences = pad_sequences(input_sequences, maxlen=Tx, padding='post')\n    inp = tf.convert_to_tensor(input_sequences)\n    inference_batch_size = input_sequences.shape[0]\n    encoder_initial_cell_state = [tf.zeros((inference_batch_size, rnn_units)),\n                                  tf.zeros((inference_batch_size, rnn_units))]\n    encoder_emb_inp = encoderNetwork.encoder_embedding(inp)\n    a, a_tx, c_tx = encoderNetwork.encoder_rnnlayer(encoder_emb_inp,\n                                                    initial_state = encoder_initial_cell_state)\n\n    start_tokens = tf.fill([inference_batch_size], jp_tokenizer.word_index['bos'])\n\n    end_token = jp_tokenizer.word_index['eos']\n\n    greedy_sampler = tfa.seq2seq.GreedyEmbeddingSampler()\n\n    decoder_input = tf.expand_dims([jp_tokenizer.word_index['bos']] * inference_batch_size,1)\n    decoder_emb_inp = decoderNetwork.decoder_embedding(decoder_input)\n\n    decoder_instance = tfa.seq2seq.BasicDecoder(cell = decoderNetwork.rnn_cell, \n                                                sampler = greedy_sampler,\n                                                output_layer = decoderNetwork.dense_layer)\n    decoderNetwork.attention_mechanism.setup_memory(a)\n\n    decoder_initial_state = decoderNetwork.build_decoder_initial_state(\n        inference_batch_size, encoder_state=[a_tx, c_tx], Dtype=tf.float32)\n\n    maximum_iterations = tf.round(tf.reduce_max(Tx) * 2)\n\n    decoder_embedding_matrix = decoderNetwork.decoder_embedding.variables[0] \n    (first_finished, first_inputs,first_state) = decoder_instance.initialize(\n        decoder_embedding_matrix, start_tokens = start_tokens,\n        end_token = end_token, initial_state = decoder_initial_state)\n\n    inputs = first_inputs\n    state = first_state  \n    predictions = np.empty((inference_batch_size,0), dtype = np.int32)                                                              \n    for j in range(maximum_iterations):\n        outputs, next_state, next_inputs, finished = decoder_instance.step(j, inputs,state)\n        inputs = next_inputs\n        state = next_state\n        outputs = np.expand_dims(outputs.sample_id,axis = -1)\n        predictions = np.append(predictions, outputs, axis = -1)\n        \n    res = ''\n    for i in range(len(predictions)):\n        line = predictions[i,:]\n        seq = list(itertools.takewhile(lambda index: index !=2, line))\n        res += \" \".join( [jp_tokenizer.index_word[w] for w in seq])\n    res = res.split()\n    \n    # Return back Unique words\n    for i in range(len(res)):\n        if res[i] == 'unk' and unique != []:\n            res[i] = unique.pop(0)\n    \n    return ' '.join(res)\n\n# Calculate BLEU\ndef BLEU(X, y):\n    # Prediction\n    pred = [Translate(w) for w in tqdm(X)]\n    # Calculate BLEU \n    score = sacrebleu.corpus_bleu(pred, [y]).score \/ 100\n    return score, pred","a29f9668":"# Custom Train Progress\nclass Progress:\n    def __init__(self):\n        self.fig = plt.figure(figsize = (8,6))\n        self.ax = self.fig.add_subplot(1, 1, 1)\n        self.loss, self.val_loss, self.BLEU = [], [], []\n        self.epoch_loss = 0\n        \n    def get_val_loss(self):\n        return [x[1] for x in self.val_loss]\n        \n    # Plot\n    def dynamic_plot(self):\n        self.ax.cla()\n        self.ax.plot(range(len(self.loss)), self.loss, label='loss')\n        if len(self.val_loss) >= 1:\n            x = [l[0] for l in self.val_loss]\n            y = [l[1] for l in self.val_loss]\n            self.ax.plot(x, y, color = 'r', label='val_loss')\n            self.ax.plot(x, self.BLEU, color = 'purple', label='BLEU')\n        self.ax.set_ylim(0,)\n        self.ax.legend(loc = 1)\n        display(self.fig)\n    \n    # Train step progress\n    def train_progress(self, epoch, step, steps_per_epoch, start):\n        self.dynamic_plot()\n        print(f'Working on Epoch {epoch}')\n        print('[' + ('=' * int((step + 1) \/ steps_per_epoch * 60)).ljust(61, ' ') \n              + f']  {step + 1}\/{steps_per_epoch} - loss : {round(self.epoch_loss \/ step, 4)}')\n        print(f'Time per Step {round(timeit.default_timer() - start, 2)} s')\n        \n    def summary(self):\n        loss = np.array_split(np.array(self.loss), len(self.val_loss))\n        loss = [np.mean(x) for x in loss]\n        val_loss = [x[1] for x in self.val_loss]\n        df = pd.DataFrame({'Epochs' : range(1, len(val_loss) + 1), 'loss' : loss,\n                           'val loss' : val_loss, 'BLEU' : self.BLEU})\n        \n        self.dynamic_plot()\n        clear_output(wait = True)\n        display(df)","34a1b670":"# Initialize Train Progress\nTP = Progress()\nbest_prediction = []\n\nfor i in range(1, epochs + 1):\n\n    encoder_initial_cell_state = initialize_initial_state()\n    total_loss = 0.0\n    # Train Loss\n    TP.epoch_loss = 0\n\n    # Train\n    for (batch , (input_batch, output_batch)) in enumerate(dataset.take(steps_per_epoch)):\n        start = timeit.default_timer()\n        batch_loss = train_step(input_batch, output_batch, encoder_initial_cell_state)\n        total_loss += batch_loss\n        TP.loss.append(batch_loss.numpy())\n        TP.epoch_loss += batch_loss.numpy()\n            \n        if (batch+1) % 30 == 0:\n            TP.train_progress(i, batch, steps_per_epoch, start)\n            clear_output(wait = True)\n    \n    # Validitate\n    encoderNetwork.trainable = False  # Freeze our model layer to make sure\n    decoderNetwork.trainable = False  # it didn't learn anything from val_data\n    \n    # Valid loss\n    val_loss = 0\n    for (batch, (input_batch, output_batch)) in enumerate(val_dataset.take(val_steps_per_epoch)):\n        batch_loss = evaluate(input_batch, output_batch, encoder_initial_cell_state)\n        val_loss += batch_loss.numpy()\n    val_loss \/= val_steps_per_epoch\n    \n    TP.val_loss.append((i * steps_per_epoch - 1, val_loss))\n    \n    # Bleu Score\n    bleu_score, pred = BLEU(eng_test, jpn_test)\n    TP.BLEU.append(bleu_score)\n    \n    encoderNetwork.trainable = True  # Unfreeze layer for next epoch\n    decoderNetwork.trainable = True\n    \n    # Save best model\n    if bleu_score == max(TP.BLEU) and val_loss == min(TP.get_val_loss()):\n        best_prediction = pred\n        encoderNetwork.save_weights('encoderNetwork')\n        decoderNetwork.save_weights('decoderNetwork')\n\nTP.summary()","ecdbb1d6":"# Load best weights\nencoderNetwork.load_weights('encoderNetwork')\ndecoderNetwork.load_weights('decoderNetwork')","b7f03fda":"for i in range(7,16):\n    print(\"English Sentence:\")\n    print(eng_test[i])\n    print(\"\\nJapanese Translation:\")\n    print(best_prediction[i])\n    print(\"\\nJapanese Reference:\")\n    print(jpn_test[i])\n    print(''.ljust(60, '-'))","45ee6db5":"from googletrans import Translator\n# Google Translate\ntranslator = Translator()","5ed43cf5":"raw_input = ['i love you', 'i am sorry', 'hello', 'thank you',\n             'is there something i can help?']\n\nfor i in range(len(raw_input)):\n    prediction = Translate(raw_input[i])\n    print(\"English Sentence:\")\n    print(raw_input[i])\n    print(\"\\nJapanese Translation:\")\n    print(prediction)\n    print(\"\\nEnglish Translation from prediction [GoogleTranslate]:\")\n    print(translator.translate(prediction).text)\n    print(''.ljust(60, '-'))","098cccc8":"import pickle\n\nwith open('en_tokenizer.pickle', 'wb') as handle:\n    pickle.dump(en_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\nhandle.close()\n\nwith open('jp_tokenizer.pickle', 'wb') as handle:\n    pickle.dump(jp_tokenizer, handle, protocol=pickle.HIGHEST_PROTOCOL)\nhandle.close()","60bdc504":"Hello guys, lately i've been studying about machine translation and give it a try. Most of code in this notebook is based on tensorflow tutorial on their website, you can find it [here](https:\/\/www.tensorflow.org\/addons\/tutorials\/networks_seq2seq_nmt). \n# English - Japanese Machine Translation\nSo in this notebook we're going to build English to Japanese machine translation, Japanese text contains lots of unique words because they have 3 type of it:\n1. Kanji\n1. Katakana\n1. Hiragana\n\nthats the insteristing part of it and so it'll be little complicated to process. So let's get started.","d4beea2c":"Let's check the best prediction of our model.","1a9b2be8":"# Dataset\nHere we use 55463 en-jp corpus from multiple source\n\n1. ManyThings.org Bilingual Sentence Pairs<br>\n-> Official Site : [Here](http:\/\/www.manythings.org\/bilingual\/)<-","cd10eaf2":"Load data to memory.","75dc0fcf":"# Table Of Content\n1. Load Dataset\n1. Text Preprocessing\n    1. English missplell handling\n    1. Segmentating Japanese words\n    1. Add BOS and EOS to train sentences\n1. Word Tokenizing\n    1. Word Cloud\n1. Build & Train Model\n1. Scoring Bleu\n1. Test with Some Raw Input","ac7624cc":"## Word Cloud\nWhat comes when doing NLP? It's Word Cloud. Let's do it for our vocab.\n\nFont : [Google Noto Fonts](https:\/\/www.google.com\/get\/noto\/) -> Noto Sans CJK JP","f89ac910":"# Text Preprocessing\n### Handling misspell words & Clearing Punctuation\nwe're gonna change the misspell words in english sentences and clearing punctuation from text.\n\n\"aren't my english bad?\" -> \"are not my english bad\"","3bb3ce73":"now let's transform our train sentences to sequences.","01577726":"### Add BOS and EOS\nWe put BOS \"Begin of Sequence\" and EOS \u201cEnd of Sequence\" to help our decoder recognize begin and end of a sequance.","ba41f6c5":"# Test with Some Raw Input\nYeay now let's play with **our** Machine Translation with some raw input. We'll cross check the prediction from MT with Google Translate API to translate it back to english and see how bad **our** MT is :).","40895722":"Japanese words have their own punctuation like \u3010this\u3011","12a5cc7d":"## Thank you very much for reading my post\nPlease tell me when I make mistakes in program and English.\nI hope this kernel will help and if you think this kernel is useful, please upvote.","ee853036":"# Build & Train Model\nNow it's the time brace yourself.\n\nWe'll build model based on Seq2seq approaches with Attention optimization.\n> Seq2Seq is a method of encoder-decoder based machine translation that maps an input of sequence to an output of sequence with a tag and attention value. The idea is to use 2 RNN that will work together with a special token and trying to predict the next state sequence from the previous sequence.\n\n![Seq2Seq](https:\/\/raw.githubusercontent.com\/Hyuto\/NMT-TF-Seq2seq-EN-JP\/master\/Seq2Seq.png)","acc6c508":"## Install some tools\n1. Sacreblue for calculate BLEU score\n1. Googletrans => Google Translate for testing some sentences later\n\nNote : You can use NLTK for calculating BLEU score [documentation](https:\/\/www.nltk.org\/_modules\/nltk\/translate\/bleu_score.html)","81f163ba":"### Segmentating Japanese Sentences\nUnlike english sentence we can tokenize it by splitting words with space just like this,\n```\n'This is english or i think so'.split()\n\nOutput:\n['This', 'is', 'english', 'or', 'i', 'think', 'so']\n```\nbut in Japanese we can't do it that way. Here we gonna use Janome Tokennizer to segmentating Japanese sentence and adding space to it so Keras Tokenizer can handle it.","fa5312d4":"# Calculating BLEU Score\n> BLEU (bilingual evaluation understudy) is an algorithm for evaluating the quality of text which has been machine-translated from one natural language to another. Quality is considered to be the correspondence between a machine's output and that of a human: \"the closer a machine translation is to a professional human translation, the better it is\". - Wikipedia\n\nBLEU is a metric for evaluating a generated sentence to a reference sentence.\nA perfect match results in a score of 1.0, whereas a perfect mismatch results in a score of 0.0.\n\n\nSo now we're going to define our translation function & calculate the BLEU score for test data at the end of every epoch while training. Note that test data is sentences that our tokenizer didn't train with, so there must be some words that our tokenizer didn't know. \nI'm currently working to fix this issue. Based on keras Tokenizer API it have `oov_token` for handling this but i'm not sure.\n\nFor now i'm handling this by adding `unk` in train dataset so the tokenizer can read it, and then when coming to translation if there is word that our tokenizer don't know i'll set it by index of `unk` not very eficient but it works.","ef6dd932":"Make custom training loop","c24349af":"# Word Tokenizing\nHere we use Tokenizer API from Keras to make vocabulary and tokenizing our data","502c9b47":"# Reference\n1. TensorFlow Addons Networks : Sequence-to-Sequence NMT with Attention Mechanism [Link](https:\/\/www.tensorflow.org\/addons\/tutorials\/networks_seq2seq_nmt)\n2. seq2seq (Sequence to Sequence) Model for Deep Learning with PyTorch [Link](https:\/\/www.guru99.com\/seq2seq-model.html)","f0971ea3":"based on the distplot English sentences contains about 20 - 40 words while Japanese have more wider range. \n\nLet's check their max lenght","7e3ad478":"For evaluating our model let's split our data.","36636457":"Let's define our based Seq2Seq Model"}}