{"cell_type":{"00c9a7c4":"code","8c768964":"code","4aeca570":"code","90a7a6f3":"code","e6b8da8b":"code","f8a3bd42":"code","a24c8845":"code","377cab9e":"code","4bc225ef":"code","56e3a7d1":"code","c4d909ce":"code","7904f42c":"code","f8ac2b36":"code","a71b2a53":"code","860c9b30":"code","c8edc882":"code","ba08fafb":"code","c46539c8":"code","b72d00b8":"code","d123b646":"code","9cf1f973":"code","8a34cf55":"code","cd6a384a":"code","fc382af8":"code","17f3a41a":"code","b766d153":"code","f3b932a6":"code","0a498fdd":"code","38850e40":"code","c14b98c6":"code","69bd081d":"code","fb9c0551":"code","031922d0":"code","c58a271e":"code","568bb244":"code","496d1133":"code","8ed4707b":"code","a2a7f3a7":"code","0ee1f32a":"code","36b8adbb":"code","ba330328":"code","171196b2":"code","f2254425":"code","90153764":"code","70c89794":"code","43ff14d0":"code","c8cb7291":"code","e5da0c6e":"code","fb66ca86":"code","7ebf9626":"code","17a4cf41":"code","7afceaee":"code","37115d17":"code","cecb6e13":"markdown","52bdec13":"markdown","2f03254e":"markdown","bbf900c7":"markdown","1538bb76":"markdown","d47fcdc5":"markdown","82ef1575":"markdown","ae74e0ee":"markdown","b9b6e385":"markdown","2b590b67":"markdown","9c6daa0d":"markdown","1f2b9a29":"markdown","df34353c":"markdown","58a99055":"markdown","a786ee46":"markdown","f51246db":"markdown","92fd03a7":"markdown","70f340ba":"markdown","9ed36590":"markdown","61cc8012":"markdown","e5f3cf27":"markdown","b063e5f9":"markdown","c1133aed":"markdown","c028bba2":"markdown","14156aaf":"markdown","6470a68d":"markdown","a67d1992":"markdown","71725ecd":"markdown","668140f6":"markdown","40e448a3":"markdown","4c568294":"markdown","6526a204":"markdown","c9e434e9":"markdown","1e228976":"markdown","03a61e7d":"markdown","55666c1f":"markdown","ccaa7d00":"markdown","03536d82":"markdown","b2f0dbba":"markdown","08616720":"markdown","beff3113":"markdown","284d1e83":"markdown","7b2a54dc":"markdown","5ff610d8":"markdown","23391e0c":"markdown","8e704a5d":"markdown","f1cd9fc4":"markdown","959d3fbe":"markdown","cb642f75":"markdown","6dc9130e":"markdown","868c0aa9":"markdown","029ff4a0":"markdown"},"source":{"00c9a7c4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","8c768964":"!pip install pycaret\n!pip install missingno","4aeca570":"import pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib import rcParams\n\nfrom sklearn import model_selection\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, roc_auc_score\nfrom sklearn.metrics import f1_score, confusion_matrix, precision_recall_curve, roc_curve\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom sklearn.preprocessing import StandardScaler\n\n\nimport plotly.express as px\nfrom plotly.subplots import make_subplots\nimport plotly.graph_objects as go\n\nimport warnings\nwarnings.filterwarnings(action='ignore')","90a7a6f3":"def get_clf_eval(y_test, pred=None, pred_proba=None):\n    confusion = confusion_matrix( y_test, pred)\n    accuracy = accuracy_score(y_test , pred)\n    precision = precision_score(y_test , pred)\n    recall = recall_score(y_test , pred)\n    f1 = f1_score(y_test,pred)   \n     \n    roc_auc = roc_auc_score(y_test, pred_proba)\n\n    # ROC-AUC print \n    print('accuracy: {0:.4f}, precision: {1:.4f}, recall: {2:.4f},\\\n    F1: {3:.4f}, AUC:{4:.4f}'.format(accuracy, precision, recall, f1, roc_auc))\n    return confusion","e6b8da8b":"diabetes_df = pd.read_csv(\"..\/input\/pima-indians-diabetes-database\/diabetes.csv\")\ndiabetes_df.head().T.style.set_properties(**{'background-color': 'grey',\n                           'color': 'white',\n                           'border-color': 'white'})","f8a3bd42":"diabetes_df.rename(columns ={\"DiabetesPedigreeFunction\":\"DPF\"},inplace=True)","a24c8845":"import missingno as msno\nmsno.matrix(diabetes_df)","377cab9e":"colors = ['gold', 'mediumturquoise']\nlabels = ['0','1']\nvalues = diabetes_df['Outcome'].value_counts()\/diabetes_df['Outcome'].shape[0]\n\n# Use `hole` to create a donut-like pie chart\nfig = go.Figure(data=[go.Pie(labels=labels, values=values, hole=.3)])\nfig.update_traces(hoverinfo='label+percent', textinfo='percent', textfont_size=20,\n                  marker=dict(colors=colors, line=dict(color='#000000', width=2)))\nfig.update_layout(\n    title_text=\"Outcome\")\nfig.show()","4bc225ef":"def highlight_min(s, props=''):\n    return np.where(s == np.nanmin(s.values), props, '')\n\ndiabetes_df.describe().style.apply(highlight_min, props='color:Black;background-color:Grey', axis=0)","56e3a7d1":"feature_names = [cname for cname in diabetes_df.loc[:,:'Age'].columns]","c4d909ce":"rcParams['figure.figsize'] = 40,60\nsns.set(font_scale = 3)\nsns.set_style(\"white\")\nsns.set_palette(\"bright\")\nplt.subplots_adjust(hspace=0.5)\ni = 1;\nfor name in feature_names:\n    plt.subplot(5,2,i)\n    sns.histplot(data=diabetes_df, x=name, hue=\"Outcome\",kde=True,palette=\"YlGnBu\")\n    i = i + 1","7904f42c":"zero_features = ['Pregnancies','Glucose','BloodPressure','SkinThickness',\"Insulin\",'BMI']\ntotal_count = diabetes_df['Glucose'].count()\n\nfor feature in zero_features:\n    zero_count = diabetes_df[diabetes_df[feature]==0][feature].count()\n    print('{0} 0 number of cases {1}, percent is {2:.2f} %'.format(feature, zero_count, 100*zero_count\/total_count))","f8ac2b36":"diabetes_mean = diabetes_df[zero_features].mean()\ndiabetes_df[zero_features]=diabetes_df[zero_features].replace(0, diabetes_mean)","a71b2a53":"X = diabetes_df.iloc[:,:-1]\ny = diabetes_df.iloc[:,-1]","860c9b30":"from sklearn.preprocessing import QuantileTransformer\nscaler = QuantileTransformer(n_quantiles=100, random_state=0, output_distribution='normal')\nX_scaled = scaler.fit_transform(X)","c8edc882":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","ba08fafb":"def highlight_min(s, props=''):\n    return np.where(s == np.nanmin(s.values), props, '')\n\nX_train.describe().style.apply(highlight_min, props='color:Black;background-color:Grey', axis=0)","c46539c8":"rcParams['figure.figsize'] = 40,60\nsns.set(font_scale = 3)\nsns.set_style(\"white\")\nsns.set_palette(\"bright\")\nplt.subplots_adjust(hspace=0.5)\ni = 1;\nfor name in feature_names:\n    plt.subplot(5,2,i)\n    sns.histplot(data=diabetes_df, x=name, hue=\"Outcome\",kde=True,palette=\"YlGnBu\")\n    i = i + 1","b72d00b8":"corr=diabetes_df.corr().round(2)\n\nsns.set(font_scale=1.15)\nplt.figure(figsize=(14, 10))\nsns.set_palette(\"bright\")\nsns.set_style(\"white\")\nmask = np.zeros_like(corr)\nmask[np.triu_indices_from(mask)] = True\nsns.heatmap(corr,annot=True,cmap='gist_yarg_r',mask=mask,cbar=True)\nplt.title('Correlation Plot')","d123b646":"sns.set(font_scale=2)\nplt.figure(figsize=(10, 8))\nsns.set_style(\"white\")\nsns.set_palette(\"bright\")\nsns.pairplot(diabetes_df,kind = 'reg',corner = True,palette ='YlGnBu' )","9cf1f973":"fig = px.histogram(diabetes_df, x=\"Glucose\", \n                   color=\"Outcome\", \n                   marginal=\"box\",\n                   barmode =\"overlay\",\n                   histnorm ='density'\n                  )  \nfig.update_layout(\n    title_font_color=\"black\",\n    legend_title_font_color=\"green\",\n    title={\n        'text': \"Glucose Histogram per Outcome\",\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n)\nfig.show()","8a34cf55":"import plotly.express as px\nfig = px.histogram(diabetes_df, x=\"BMI\", \n                   color=\"Outcome\", \n                   marginal=\"box\",\n                   barmode =\"overlay\",\n                   histnorm ='density'\n                  )  \nfig.update_layout(\n    title_font_color=\"black\",\n    legend_title_font_color=\"green\",\n    title={\n        'text': \"BMI Histogram per Outcome\",\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n)\nfig.show()","cd6a384a":"import plotly.express as px\nfig = px.histogram(diabetes_df, x=\"Age\", \n                   color=\"Outcome\", \n                   marginal=\"box\",\n                   barmode =\"overlay\",\n                   histnorm ='density'\n                  )  \nfig.update_layout(\n    title_font_color=\"black\",\n    legend_title_font_color=\"green\",\n    title={\n        'text': \"Age Histogram per Outcome\",\n        'x':0.5,\n        'xanchor': 'center',\n        'yanchor': 'top'},\n)\nfig.show()","fc382af8":"X_train = diabetes_df.drop('Outcome',axis=1)\ny_train = diabetes_df['Outcome']","17f3a41a":"import umap\nimport umap.plot\n\nmapper = umap.UMAP().fit(X_train) \numap.plot.points(mapper, labels=y_train, theme='fire')","b766d153":"from umap import UMAP\n\numap_3d = UMAP(n_components=3, init='random', random_state=0)\nx_umap = umap_3d.fit_transform(X_train)\numap_df = pd.DataFrame(x_umap)\ntrain_y_sr = pd.Series(y_train,name='label').astype(str)\nprint(type(x_umap))\nnew_df = pd.concat([umap_df,train_y_sr],axis=1)\nfig = px.scatter_3d(\n    new_df, x=0, y=1, z=2,\n    color='label', labels={'color': 'number'}\n)\nfig.update_traces(marker_size=1.5)\nfig.show()","f3b932a6":"from pycaret.classification import *","0a498fdd":"clf1 = setup(data = diabetes_df, \n             target = 'Outcome',\n             preprocess = False,\n             silent = True)","38850e40":"top5 = compare_models(sort='AUC',\n                      n_select = 5,\n                      exclude=['lightgbm','xgboost','dummy','svm','ridge','knn','dt','nb','qda']\n                     )","c14b98c6":"catboost = create_model('catboost')\nrf = create_model('rf')\nlr = create_model('lr')\nlda = create_model('lda')\ngbc = create_model('gbc')","69bd081d":"interpret_model(catboost)","fb9c0551":"interpret_model(rf)","031922d0":"tuned_catboost = tune_model(catboost, optimize = 'AUC')\ntuned_rf = tune_model(rf, optimize = 'AUC')\ntuned_lr = tune_model(lr, optimize = 'AUC')\ntuned_lda = tune_model(lda, optimize = 'AUC')\ntuned_gbc = tune_model(gbc, optimize = 'AUC')","c58a271e":"stack_model = stack_models(estimator_list = top5, meta_model = top5[0],optimize = 'AUC')","568bb244":"plt.figure(figsize=(8, 8))\nplot_model(stack_model, plot='boundary')","496d1133":"plt.figure(figsize=(8, 8))\nplot_model(stack_model, plot = 'auc')","8ed4707b":"#prediction\npred = stack_model.predict(X_test)\npred_proba = stack_model.predict_proba(X_test)[:,1]\n#Accuracy\nconfusion_stack = get_clf_eval(y_test,pred,pred_proba)","a2a7f3a7":"plt.figure(figsize=(8, 6))\nax = sns.heatmap(confusion_stack, cmap = 'YlGnBu',annot = True, fmt='d')\nax.set_title('Confusion Matrix (Stacking)')","0ee1f32a":"blend_soft = blend_models(estimator_list = top5, optimize = 'AUC',method = 'soft')","36b8adbb":"plt.figure(figsize=(8, 8))\nplot_model(blend_soft, plot='boundary')","ba330328":"plt.figure(figsize=(8, 8))\nplot_model(blend_soft, plot = 'auc')","171196b2":"#prediction\npred = blend_soft.predict(X_test)\npred_proba = blend_soft.predict_proba(X_test)[:,1]\n#Accuracy\nconfusion_soft = get_clf_eval(y_test,pred,pred_proba)","f2254425":"plt.figure(figsize=(8, 6))\nax = sns.heatmap(confusion_soft, cmap = 'YlGnBu',annot = True, fmt='d')\nax.set_title('Confusion Matrix (Soft Blending)')","90153764":"blend_hard = blend_models(estimator_list = top5, optimize = 'AUC',method = 'hard')","70c89794":"plt.figure(figsize=(8, 8))\nplot_model(blend_hard, plot='boundary')","43ff14d0":"#prediction\npred = blend_hard.predict(X_test)\n#Accuracy\nconfusion_hard = confusion_matrix( y_test, pred)\naccuracy = accuracy_score(y_test , pred)\nprecision = precision_score(y_test , pred)\nrecall = recall_score(y_test , pred)\nf1 = f1_score(y_test,pred) \nprint('accuracy: {0:.4f}, precision: {1:.4f}, recall: {2:.4f},\\\nF1: {3:.4f}'.format(accuracy, precision, recall, f1))","c8cb7291":"plt.figure(figsize=(8, 6))\nax = sns.heatmap(confusion_hard, cmap = 'YlGnBu',annot = True, fmt='d')\nax.set_title('Confusion Matrix (Hard Blending)')","e5da0c6e":"cali_model = calibrate_model(blend_soft)","fb66ca86":"final_model = finalize_model(cali_model)","7ebf9626":"plt.figure(figsize=(8, 8))\nplot_model(final_model, plot='threshold')","17a4cf41":"plt.figure(figsize=(8, 8))\nplot_model(final_model, plot='boundary')","7afceaee":"#prediction\npred = final_model.predict(X_test)\n#Accuracy\nfinal_model = confusion_matrix( y_test, pred)\naccuracy = accuracy_score(y_test , pred)\nprecision = precision_score(y_test , pred)\nrecall = recall_score(y_test , pred)\nf1 = f1_score(y_test,pred) \nprint('accuracy: {0:.4f}, precision: {1:.4f}, recall: {2:.4f},\\\nF1: {3:.4f}'.format(accuracy, precision, recall, f1))","37115d17":"plt.figure(figsize=(8, 6))\nax = sns.heatmap(confusion_hard, cmap = 'YlGnBu',annot = True, fmt='d')\nax.set_title('Confusion Matrix (final_model)')","cecb6e13":"## Checking Target Imbalance","52bdec13":"<hr style=\"border: solid 3px blue;\">\n\n# Conclusion\n\n**After EDA and preprocessing, three ensamble models were run and the performance was verified with the validation dataset. Ensamble using soft and hard voting gave the best result in solving this problem, but different results may come out depending on preprocessing and selection of base models and hyperparameter settings.**","2f03254e":"OK! The target is well balanced.","bbf900c7":"## Checking Statistics","1538bb76":"Before we get started, let's take a quick look at datasets.\n\n**The objective of the dataset**\n\n> This dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective of the dataset is to diagnostically predict whether or not a patient has diabetes, based on certain diagnostic measurements included in the dataset. Several constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n**The Pima Indian Diabetes data set consists of:**\n\n* Pregnancies: Number of times pregnant\n* Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n* BloodPressure: Diastolic blood pressure (mm Hg)\n* SkinThickness: Triceps skin fold thickness (mm)\n* Insulin: 2-Hour serum insulin (mu U\/ml)\n* BMI: Body mass index (weight in kg\/(height in m)^2)\n* DiabetesPedigreeFunction: Diabetes pedigree function\n* Age: Age (years)\n* Outcome: Class variable (0 or 1) 268 of 768 are 1, the others are 0","d47fcdc5":"--------------------------------------------------------------------------------------------------------------------------\n# Load Libraries","82ef1575":"-----------------------------------------------------------------------------------------------------------------------------------------\n# Stacking\n\n![](https:\/\/miro.medium.com\/max\/1000\/1*CoauXirckomVXxw2Id2w_Q.jpeg)\n\nPicture Credit: https:\/\/miro.medium.com\n\n","ae74e0ee":"The confusion matrix is \u200b\u200balso well balanced and the results are good.","b9b6e385":"## Creating Models\n\n> This function trains and evaluates the performance of a given estimator using cross validation. The output of this function is a score grid with CV scores by fold.\n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/classification.html","2b590b67":"# Defining Utility functions","9c6daa0d":"## Comparing Models\n\n> This function trains and evaluates performance of all estimators available in the model library using cross validation. The output of this function is a score grid with average cross validated scores. \n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/classification.html","1f2b9a29":"From the above figures, you can see what distribution each feature has for each output. In the case of glucose with high correlation, it can be seen that the distribution of outcomes 1 and 0 has a more distant shape than other features. In this case, it is expected to have a better effect on classification.","df34353c":"<hr style=\"border: solid 3px blue;\">\n\n# Introduction\n\n![](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/6\/6f\/Pima.jpg) \n\nPicture Credit: https:\/\/upload.wikimedia.org\n\n**In this notebook, I would like to organize the following three ensemble models.**\n* Stacking Model\n* Soft Voting Model\n* Hard Voting Model\n\nAfter constructing and evaluating each model, we will select the optimal ensemble model to solve this problem.","58a99055":"# Reading and Checking data","a786ee46":"Looking at the figure above, the correlation coefficient between Outcome and Glucose is the highest at 0.49. Let's examine this in more depth.","f51246db":"DiabetesPedigreeFunction has a long name. Change to DPF.","92fd03a7":"-----------------------------------------------\n# Calibrating the final model\n\n> This function calibrates the probability of a given estimator using isotonic or logistic regression.\n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/classification.html","70f340ba":"It seems to have been well learned.","9ed36590":"As shown above, the ratio of the value of zero in the SkinThickness and Insulin features seems to be high. Let's change the corresponding values to the mean value of each feature. \n\n> However, a zero value may be meaningful to the corresponding feature. If you have an expert with expertise in diabetes, you will be able to confirm that your decision is correct.\nHowever, since there is no such domain knowledge or friends, we will first replace the corresponding value with the mean value.","61cc8012":"# Tuning Hyperparameters\n\n> This function tunes the hyperparameters of a given estimator. The output of this function is a score grid with CV scores by fold of the best selected model based on optimize parameter.","e5f3cf27":"Precision and recall have a trade-off relationship. The picture above seems to be well balanced because it has been studied well.","b063e5f9":"The 8-dimensional training dataset shown in the figure above is drawn by reducing the dimensions to 2D. As you can see in the figure, positive and negative are overlapped at the bottom of the figure. Since our models are mainly tree-based models, we will mainly work on determining the boundary, but it seems to be a difficult task in 2D.\n\nHowever, our training dataset is 8-dimensional, just that we can't visualize it. Therefore, it will not be a very difficult task for our models to determine the boundary as shown above.","c1133aed":"If we remove the zero value of each feature, we have a distribution similar to the normal distribution. Therefore, perform linear scaling and standard scaling.","c028bba2":"# Predicting with the test dateset","14156aaf":"## Interpreting Models","6470a68d":"--------------------------------------------\n# EDA","a67d1992":"## 3D plot","71725ecd":"The result is not bad. However, it is worse than the soft blending result.","668140f6":"## Checking Missing Values and Data type\nLet's check if there are missing values.","40e448a3":"Let's compare the feature importance of the above two models. A comparison was made with SHAP values, and the feature importance of each model is slightly different.\nThe diversity of each model seems to be stable and improve performance while compensating for each other's weaknesses.\n\nIf you are more interested in feature importance, please refer to the notebook below.\n\n[Notebook](https:\/\/www.kaggle.com\/ohseokkim\/interpreting-models-by-feature-importnace)","4c568294":"## Setup\n\n> This function initializes the training environment and creates the transformation pipeline. Setup function must be called before executing any other function. It takes two mandatory parameters: data and target. All the other parameters are optional.\n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/classification.html","6526a204":"--------------------------------------------------\n# Checking and Removing Outliers","c9e434e9":"**Good!. There is no missing value and all features type are number. Therefore, there is no need to preprocess for missing values.**","1e228976":"# Checking correlation between features\n\nLet's check the correlations between each variable.","03a61e7d":"----------------------------------------------\n# Finalizing the last model\n\n> This function trains a given estimator on the entire dataset including the holdout set.\n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/classification.html","55666c1f":"Even if you increase the dimension to 3D, you can see overlapping points. We want to see it from a higher dimension, but we can't draw.\n\n**Now, all we have to do is do some good modeling and fine tune the hyperparameters.**","ccaa7d00":"<hr style=\"border: solid 3px blue;\">\n\n# Ensemble\n\n![](https:\/\/miro.medium.com\/max\/637\/1*3GIDYOn2GNcv9bq4bQk5YA.jpeg)\n\nPicture Credit: https:\/\/miro.medium.com\n\n> Supervised learning algorithms perform the task of searching through a hypothesis space to find a suitable hypothesis that will make good predictions with a particular problem.Even if the hypothesis space contains hypotheses that are very well-suited for a particular problem, it may be very difficult to find a good one. Ensembles combine multiple hypotheses to form a (hopefully) better hypothesis. The term ensemble is usually reserved for methods that generate multiple hypotheses using the same base learner. The broader term of multiple classifier systems also covers hybridization of hypotheses that are not induced by the same base learner.\n\nRef: https:\/\/en.wikipedia.org\/wiki\/Ensemble_learning\n","03536d82":"\nTop 1, Top 2, Top 3 ,Top 4, and Top 5 models were tuned with different feature importance and decision boundary. Also, there is a big difference in feature importance from the catboost classifier.","b2f0dbba":"-----------------------------------------------\n# Checking features before modeling","08616720":"<span style=\"color:Blue\"> **Observation:**\n* The correlation between Outcome and Glucose is high. Glucoe seems to be the most important feature in model training.\n* BMI, Pregnancys, and Age are also expected to be used as important features in model training.\n* High correlation coefficient with SkinThickness and BMI. There may be a multicollinearity problem, but if there is a performance problem after checking the result, check it again.","beff3113":"-------------------------------------------\n# Scaling\n\nAlthough the zero values of each feature are converted to mean values, some features have a one-sided shape. Therefore, we decided to perform nonlinear scaling, and decided to use the QuantileTransformer that changes the distribution closest to the normal distribution by referring to the notebook referenced below.\n\n> The quantile function ranks or smooths out the relationship between observations and can be mapped onto other distributions, such as the uniform or normal distribution.\n\nIf you want to know more about Scaling, please refer to the notebook below.\n\n[Notebook](https:\/\/www.kaggle.com\/ohseokkim\/preprocessing-linear-nonlinear-scaling)","284d1e83":"---------------------------------------------------------------------------------------------------------------------------------------\n# Hard Voting\n\n![](https:\/\/miro.medium.com\/max\/428\/1*XnZwlg7Th3nga25sSlanJQ.jpeg)\n\nPicture Credit: https:\/\/vitalflux.com\n\n\n> This function trains a **Majority Rule classifier** for select models passed in the estimator_list param. The output of this function is a score grid with CV scores by fold.\n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/classification.html","7b2a54dc":"----------------------------------------------------------------------------\n# Visualizing Training Dataset after Dimension Reduction","5ff610d8":"**OK! Let's make models and train it.**","23391e0c":"## 2D plot","8e704a5d":"**Among the features, there are many features whose min() value is 0. Let's check out more of these features.**","f1cd9fc4":"---------------------------------------------------------------------------------------------------------------------------------------\n# Soft Voting\n\n![](https:\/\/miro.medium.com\/max\/806\/1*bliKQZGPccS7ho9Zo6uC7A.jpeg)\n\nPicture Credit: https:\/\/miro.medium.com\n\n> This function trains a Soft Voting classifier for select models passed in the estimator_list param. The output of this function is a score grid with CV scores by fold.\n\nRef: https:\/\/pycaret.readthedocs.io\/en\/latest\/api\/classification.html","959d3fbe":"It looks like the model has been properly trained. ","cb642f75":"<span style=\"color:Blue\"> **Observation:**\n* As expected, glucose is used as the most important feature.\n* SkinTickness and BloodPressure have the low importance.","6dc9130e":"Looking at the pictures above, it would be good to change the zero value of each feature to another value.\nFirst, let's calculate the proportion of zero values in each feature.","868c0aa9":"Looking at the picture above, you can see that the zero value is removed and scaling is done. However, the shape of the distribution of Pregnancies does not look good.","029ff4a0":"In the case of a stacking model, in some cases overfitting and in some cases underfitting.\n\nIf you are interested in under\/overfitting, please refer to the notebook below.\n\n[Notebook](https:\/\/www.kaggle.com\/ohseokkim\/overfitting-and-underfitting-eda)\n\nLet's compare the other model's boundary."}}