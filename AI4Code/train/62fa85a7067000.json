{"cell_type":{"671b9a23":"code","1963f47d":"code","5dc9212e":"code","b71fee79":"code","bd9e4a76":"code","fcd3f1e5":"code","bafd5c29":"code","f712409d":"code","29e3d06a":"code","0466108e":"code","05c00ade":"code","a9738fc2":"code","a0239e2d":"code","d43aad6c":"code","e085552d":"code","e7a78df3":"code","bb7f0e3b":"code","518ed0a2":"markdown","9ac1855e":"markdown","b262062b":"markdown","0e508342":"markdown","8c4cedcb":"markdown","ccd4d349":"markdown","b281b59f":"markdown","215b0ebe":"markdown"},"source":{"671b9a23":"import os\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom matplotlib.font_manager import FontProperties\n\nfrom wordcloud import WordCloud\nfrom collections import Counter\nfrom spacy.lang.hi import Hindi\nfrom spacy.lang.ta import Tamil\nfrom spacy.lang.hi import STOP_WORDS as hindi_stopwords\nfrom spacy.lang.ta import STOP_WORDS as tamil_stopwords\n\nfrom sklearn.feature_extraction.text import CountVectorizer\n\nfrom typing import Counter\nfrom pandas._typing import FrameOrSeries","1963f47d":"!wget -q https:\/\/www.wfonts.com\/download\/data\/2016\/04\/29\/nirmala-ui\/nirmala-ui.zip\n!unzip -q nirmala-ui.zip\n!ls -lrt *.ttf","5dc9212e":"# configure the Tamil font\ntamil_font = FontProperties(fname='.\/Nirmala.ttf')","b71fee79":"train_df = pd.read_csv(\"..\/input\/chaii-hindi-and-tamil-question-answering\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/chaii-hindi-and-tamil-question-answering\/test.csv\")\nprint(f\"Train Shape: {train_df.shape}, Test Shape: {test_df.shape}\")","bd9e4a76":"train_df.head()","fcd3f1e5":"test_df.head()","bafd5c29":"def target_distribution(df: FrameOrSeries, target_column: str) -> None:\n    \"\"\"\n    Target Variable Distribution\n    Args:\n        df (FrameOrSeries): DataFrame\n        target_column (str): Target column name\n    \"\"\"    \n    vc = df[target_column].value_counts()\n    print(f'Distribution: \\n\\n{vc} \\n')\n    \n    colors = ['#66b3ff', '#ff9999']\n    plt.pie(vc.values, labels=vc.keys(), colors=colors, shadow=True, startangle=90, autopct='%1.1f%%')\n    \n    #draw circle\n    centre_circle = plt.Circle((0,0), 0.80, fc='white')\n    fig = plt.gcf()\n    fig.gca().add_artist(centre_circle)\n    \n    plt.title(f'\"{target_column}\" Distribution')\n    plt.show()","f712409d":"# Language Distribution across training dataset\n\ntarget_distribution(df=train_df, target_column=\"language\")","29e3d06a":"# Language Distribution across test dataset\n\ntarget_distribution(df=test_df, target_column=\"language\")","0466108e":"def plot_missing_values(df: FrameOrSeries) -> None:\n    \"\"\"Plot HeatMap of missing values\n    Args:\n        df (FrameOrSeries): DataFrame\n    \"\"\"\n    plt.figure(figsize=(10, 8))\n    sns.heatmap(df.isnull().T, cbar=False)\n    plt.yticks(rotation=45)","05c00ade":"plot_missing_values(train_df)","a9738fc2":"train_corpus = train_df['context']\ntest_corpus = test_df['context']\n\ndef get_top_n_anigram(corpus, n=None):\n    vec = CountVectorizer().fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef get_top_n_bigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef get_top_n_trigram(corpus, n=None):\n    vec = CountVectorizer(ngram_range=(3, 3)).fit(corpus)\n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq[:n]\n\ndef plot_top_n(corpus: list, title: str = None):\n    df = pd.DataFrame(corpus, columns=['word','freq'])\n    plt.figure(figsize=(16, 8))\n    ax = sns.barplot(x='freq', y='word', data=df, facecolor=(0, 0, 0, 0), linewidth=2, edgecolor=sns.color_palette(\"ch:start=3, rot=.1\",20))\n    ax.bar_label(ax.containers[0], padding=5)\n    \n    plt.title(title)\n    plt.xlabel(\"Frequency\")\n    plt.ylabel(\"Count\")\n    plt.yticks(fontproperties=tamil_font)\n    plt.show()","a0239e2d":"# \"context\" corpus of Training data\n\nplot_top_n(get_top_n_anigram(train_corpus, 20), title=\"Train Top 20 Unigrams\")\nplot_top_n(get_top_n_bigram(train_corpus, 20), title=\"Train Top 20 Bigrams\")\nplot_top_n(get_top_n_trigram(train_corpus, 20), title=\"Train Top 20 Trigrams\")","d43aad6c":"# \"context\" corpus of Test data\n\nplot_top_n(get_top_n_anigram(test_corpus, 20), title=\"Test Top 20 Unigrams\")\nplot_top_n(get_top_n_bigram(test_corpus, 20), title=\"Test Top 20 Bigrams\")\nplot_top_n(get_top_n_trigram(test_corpus, 20), title=\"Test Top 20 Trigrams\")","e085552d":"## WordCloud on \"question\" variable\n\ntamil_text = \" \".join(train_df[train_df[\"language\"]==\"tamil\"][\"question\"])\nhindi_text = \" \".join(train_df[train_df[\"language\"]==\"hindi\"][\"question\"])\n\n# Get the tokens and frequencies for Hindi language\nhindi_nlp = Hindi()\nhindi_doc = hindi_nlp(hindi_text)\nhindi_tokens = set([token.text for token in hindi_doc])\nhindi_tokens_counter = Counter(hindi_tokens)\n\n# Get the tokens and frequencies for Tamil language\ntamil_nlp = Tamil()\ntamil_doc = hindi_nlp(tamil_text)\ntamil_tokens = set([token.text for token in tamil_doc])\ntamil_tokens_counter = Counter(tamil_tokens)\n\ndef plot_wordcloud(frequencies: Counter, stopwords: set, title: str = None):\n    wordcloud = WordCloud(font_path=\".\/Nirmala.ttf\",\n                      width=400,\n                      height=400,\n                      background_color=\"white\",\n                      stopwords=stopwords,\n                      collocations=True,\n                      min_font_size=7).generate_from_frequencies(frequencies)\n    \n    plt.figure(figsize=(10, 10))\n    plt.imshow(wordcloud)\n    plt.axis(\"off\")\n    plt.title(title)\n    plt.show()","e7a78df3":"plot_wordcloud(frequencies=hindi_tokens_counter, stopwords=hindi_stopwords, title=\"Hindi WordCloud\")","bb7f0e3b":"plot_wordcloud(frequencies=tamil_tokens_counter, stopwords=tamil_stopwords, title=\"Tamil WordCloud\")","518ed0a2":"# Introduction \ud83d\udcdd\n\ud83c\udfaf **Goal:** To predict answers to real questions about Wikipedia articles. You will use `chaii-1`, a new question answering dataset with question-answer pairs. The dataset covers Hindi and Tamil, collected without the use of translation. It provides a realistic information-seeking task with questions written by native-speaking expert data annotators.\n\n\ud83d\udcd6 **Data:** \n> **train.csv** - the training dataset\n> - ```id``` - a unique identifier\n> - ```context```- the text of the Hindi\/Tamil sample from which answers should be derived\n> - ```question``` - the question, in Hindi\/Tamil\n> - ```answer_text``` - the answer to the question (note: for test, this is what you are attempting to predict)\n> - ```answer_start``` - the starting character in context for the answer\n> - ```language``` - whether the text in question is in Tamil or Hindi\n\n> **test.csv** - the test dataset\n> - ```id``` - a unique identifier\n> - ```context```- the text of the Hindi\/Tamil sample from which answers should be derived\n> - ```question``` - the question, in Hindi\/Tamil\n> - ```language``` - whether the text in question is in Tamil or Hindi\n\n> **sample_submission.csv** - the submission format\n> - ```id``` - a unique identifier\n> - ```PredictionString```- string that best answers the provided question based on the context.\n\n\n\ud83e\uddea **Evaluation metric:** [Jaccard Score](https:\/\/en.wikipedia.org\/wiki\/Jaccard_index)\n> $$Score = \\frac{1}{n} \\sum_{i=1}^n jaccard( gt_i, dt_i )$$\n> where \n> * $n$ = $\\textrm{number of documents}$\n> * $jaccard$ = $J(y_i, \\hat{y}_i) = \\frac{|y_i \\cap \\hat{y}_i|}{|y_i \\cup \\hat{y}_i|}$\n> * $gt_i$ = $\\textrm{the ith ground truth}$\n> * $dt_i$ = $\\textrm{the ith prediction}$","9ac1855e":"# EDA \ud83d\udcca","b262062b":"<center> <h4> Please <b><span style=\"color:red\">LIKE<\/span><\/b> the Notebook if you like it !! <\/h4><\/center>","0e508342":"### **More EDA and Model Coming Soon**","8c4cedcb":"# Import libraries \ud83d\udcda","ccd4d349":"> ***As the plot is black, there are no Missing Values in the Train dataset***","b281b59f":"# Missing values \u274c","215b0ebe":"# Language Distribution \ud83c\udccf"}}