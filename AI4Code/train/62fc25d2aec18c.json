{"cell_type":{"ec21aa70":"code","8d246535":"code","c7e116a5":"code","68743704":"code","08815e1d":"code","c703e27f":"code","36b449fd":"code","40db930d":"code","3fdad7cb":"code","490e8a25":"code","bec301d6":"code","90b76723":"code","6051c0d7":"code","b16841cb":"code","3c28dfc2":"code","dece4d3f":"code","c73b7a58":"code","01c58070":"code","22286f7e":"code","38f2eebb":"code","cf8bda4d":"code","39587182":"code","9908bade":"code","acbb491a":"code","f21bd06b":"code","bb791393":"code","fe178016":"code","2ebddc93":"code","a1fa0c19":"code","a0a62b2b":"code","27729ff6":"code","d1f115f0":"code","c5a299ba":"code","4bdf2a7d":"code","45d7e624":"code","c987893a":"code","0cbedbfc":"code","aac226b4":"code","14c0c623":"code","23c8fc7c":"code","f53a8c0b":"code","c79b2cd3":"code","b64d6433":"code","b1ce3701":"code","bb03eab7":"code","6e14d4c1":"code","1fd9f303":"code","c11eb339":"code","4308fac2":"code","46830a0b":"code","1f480fc7":"code","2462afe4":"code","1dd91b08":"code","2c067fc3":"code","7aed16c1":"code","7e7f909c":"code","631c2699":"code","6eb9cf08":"code","c8528c0f":"code","8012d3e9":"code","c2b58848":"code","07994cb6":"code","390f0256":"code","66b1545f":"code","cb43a1e4":"code","6bd6ac8b":"code","b993d8c4":"code","dc459ad9":"code","9d51bad7":"code","467e5486":"markdown","ffb4a62f":"markdown","3e6f0aec":"markdown","313716e7":"markdown","b61f5949":"markdown","1b20b93b":"markdown","950cbecd":"markdown","704db0f1":"markdown","40691d51":"markdown","c8fef2e4":"markdown","113f5846":"markdown","6ecbe12f":"markdown","c4d04ce8":"markdown","defe04e3":"markdown","ab6b1de0":"markdown","7d20b0d6":"markdown","dc75ed10":"markdown","e79a1897":"markdown","2462b838":"markdown","2dd99a6a":"markdown","31264d9a":"markdown","7013def3":"markdown","d96d2638":"markdown","26c5f098":"markdown","f790559d":"markdown","6ce27e56":"markdown","fdb41752":"markdown","2c451133":"markdown","fa8ec3d3":"markdown","43934e41":"markdown","92925873":"markdown","ad2ff79e":"markdown","58d7c7a6":"markdown","c55b92d7":"markdown","7f934970":"markdown","9d135fad":"markdown","80d7a79c":"markdown","1485cf05":"markdown","ef75f32c":"markdown","daac5786":"markdown","875a78e6":"markdown","3e2da6a9":"markdown","369402cf":"markdown","48487199":"markdown","3a552012":"markdown","981f766a":"markdown"},"source":{"ec21aa70":"import re\nimport string\nimport numpy as np\nimport random\nimport pandas as pd\n\n#For data visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport plotly.offline\nimport plotly.graph_objects as go\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\n\nfrom collections import Counter \n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n#For NLP\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch","8d246535":"#helper Function to generate random colors used to give different colors to plots.\ndef random_colours(number_of_colors):\n  colors=[]\n  for i in range(number_of_colors):\n    colors.append(\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]))\n  return colors\n","c7e116a5":"train=pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ntest=pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\nss=pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv')","68743704":"print(train.shape)\nprint(test.shape)","08815e1d":"train.info()","c703e27f":"#dropping row with null value\ntrain.dropna(inplace=True)","36b449fd":"train.info()","40db930d":"test.info()","3fdad7cb":"train.head()","490e8a25":"train.describe()","bec301d6":"temp=train.groupby('sentiment').count()['text'].reset_index().sort_values(by='text',ascending=False)\ntemp.style.background_gradient(cmap='Purples')","90b76723":"#Funnel Chart for sentiments\nfig=go.Figure(go.Funnelarea(\n    text=temp.sentiment,\n    values=temp.text,\n    title={\"position\":\"top center\",\"text\":\"Funnel=Chart of Sentiment Distribution\"}\n))\nfig.show()","6051c0d7":"def jaccard(str1,str2):\n  a=set(str1.lower().split())\n  b=set(str2.lower().split())\n  c=a.intersection(b)\n  return(float(len(c))\/(len(a) + len(b) - len(c)))","b16841cb":"results_jaccard=[]\nfor ind,row in train.iterrows():\n  sentence1=row.text\n  sentence2=row.selected_text\n  jaccard_score=jaccard(sentence1, sentence2)\n  results_jaccard.append([sentence1,sentence2,jaccard_score])","3c28dfc2":"jaccard=pd.DataFrame(results_jaccard,columns=[\"text\",\"selected_text\",\"jaccard_score\"])\ntrain=train.merge(jaccard,how='outer')","dece4d3f":"train['Num_words_ST']=train['selected_text'].apply(lambda x:len(str(x).split()))\ntrain['Num_words_text']=train['text'].apply(lambda x:len(str(x).split()))\ntrain['difference_in_words']=train['Num_words_text']-train['Num_words_ST']","c73b7a58":"train.head()","01c58070":"hist_data = [train['Num_words_ST'],train['Num_words_text']]\n\ngroup_labels = ['Selected_Text', 'Text']\n\n# Create distplot with custom bin_size\nfig = ff.create_distplot(hist_data, group_labels,show_curve=False)\nfig.update_layout(title_text='Distribution of Number Of words')\nfig.update_layout(\n    autosize=False,\n    width=900,\n    height=700,\n    paper_bgcolor=\"LightSteelBlue\",\n)\nfig.show()","22286f7e":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train['Num_words_ST'], shade=True, color=\"r\").set_title('Kernel Distributuion of Number of words')\np1=sns.kdeplot(train['Num_words_text'], shade=True, color=\"b\")","38f2eebb":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train[train['sentiment']=='positive']['difference_in_words'], shade=True, color=\"b\").set_title('Kernel Distribution of Difference in no of words for positive and negative sentiments')\np2=sns.kdeplot(train[train['sentiment']=='negative']['difference_in_words'], shade=True, color=\"r\")","cf8bda4d":"plt.figure(figsize=(12,6))\nsns.distplot(train[train['sentiment']=='neutral']['difference_in_words'],kde=False).set_title('Kernel Distribution of Difference in no of words for neutral sentiments')","39587182":"plt.figure(figsize=(12,6))\np1=sns.kdeplot(train[train['sentiment']=='positive']['jaccard_score'], shade=True, color=\"b\").set_title('KDE of jaccard Scores across different Sentiments')\np2=sns.kdeplot(train[train['sentiment']=='negative']['jaccard_score'],shade=True, color=\"r\")\nplt.legend(labels=['positive','negative'])\n","9908bade":"plt.figure(figsize=(8,5))\nsns.distplot(train[train['sentiment']=='neutral']['jaccard_score'],kde=False)","acbb491a":"#Kurtosis is the measure of how peaked a distribution is and how much spread it is around that peak\n#Skewness measures how much a curve deviates from a normal distribution","f21bd06b":"k=train[train['Num_words_text']<=2]\nk.groupby('sentiment').mean()['jaccard_score']","bb791393":"k[k['sentiment']=='positive']","fe178016":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return text","2ebddc93":"train['text']=train['text'].apply(lambda x:clean_text(x))\ntrain['selected_text']=train['selected_text'].apply(lambda x:clean_text(x))\n","a1fa0c19":"train.head()","a0a62b2b":"train['temp_list']=train['selected_text'].apply(lambda x:str(x).split())\ntop=Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp=pd.DataFrame(top.most_common(20))\ntemp.columns=['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","27729ff6":"fig=px.bar(temp, x=\"count\", y=\"Common_words\",title='Most Common Words in Selected Text', orientation='h',width=700, height=700, color='Common_words')\nfig.show()","d1f115f0":"nltk.download('stopwords')","c5a299ba":"def remove_stopword(x):\n  return [y for y in x if y not in stopwords.words('english')]\ntrain['temp_list']=train['temp_list'].apply(lambda x:remove_stopword(x))","4bdf2a7d":"top=Counter([item for sublist in train['temp_list'] for item in sublist])\ntemp=pd.DataFrame(top.most_common(20))\ntemp=temp.iloc[1:,:]\ntemp.columns=['Common_words','count']\ntemp.style.background_gradient(cmap='Purples')","45d7e624":"fig = px.treemap(temp, path=['Common_words'], values='count',title='Tree of Most Common Words')\nfig.show()","c987893a":"train['temp_list1'] = train['text'].apply(lambda x:str(x).split()) #List of words in every row for text\ntrain['temp_list1'] = train['temp_list1'].apply(lambda x:remove_stopword(x)) #Removing Stopwords","0cbedbfc":"top = Counter([item for sublist in train['temp_list1'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(25))\ntemp = temp.iloc[1:,:]\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","aac226b4":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Text', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","14c0c623":"Positive_sent=train[train['sentiment']=='positive']\nNegative_sent=train[train['sentiment']=='negative']\nNeutral_sent=train[train['sentiment']=='neutral']","23c8fc7c":"#MosT common positive words\ntop = Counter([item for sublist in Positive_sent['temp_list'] for item in sublist])\ntemp_positive = pd.DataFrame(top.most_common(20))\ntemp_positive.columns = ['Common_words','count']\ntemp_positive.style.background_gradient(cmap='Greens')","f53a8c0b":"fig = px.bar(temp_positive, x=\"count\", y=\"Common_words\", title='Most Commmon Positive Words', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","c79b2cd3":"#MosT common negative words\ntop = Counter([item for sublist in Negative_sent['temp_list'] for item in sublist])\ntemp_negative = pd.DataFrame(top.most_common(20))\ntemp_negative = temp_negative.iloc[1:,:]\ntemp_negative.columns = ['Common_words','count']\ntemp_negative.style.background_gradient(cmap='Reds')","b64d6433":"fig = px.treemap(temp_negative, path=['Common_words'], values='count',title='Tree Of Most Common Negative Words')\nfig.show()","b1ce3701":"#MosT common Neutral words\ntop = Counter([item for sublist in Neutral_sent['temp_list'] for item in sublist])\ntemp_neutral = pd.DataFrame(top.most_common(20))\ntemp_neutral = temp_neutral.loc[1:,:]\ntemp_neutral.columns = ['Common_words','count']\ntemp_neutral.style.background_gradient(cmap='Reds')","bb03eab7":"fig = px.bar(temp_neutral, x=\"count\", y=\"Common_words\", title='Most Commmon Neutral Words', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","6e14d4c1":"fig = px.treemap(temp_neutral, path=['Common_words'], values='count',title='Tree Of Most Common Neutral Words')\nfig.show()","1fd9f303":"raw_text=[word for word_list in train['temp_list1'] for word in word_list]","c11eb339":"def words_unique(sentiment,numwords,raw_words):\n    '''\n    Input:\n        segment - Segment category (ex. 'Neutral');\n        numwords - how many specific words do you want to see in the final result; \n        raw_words - list  for item in train_data[train_data.segments == segments]['temp_list1']:\n    Output: \n        dataframe giving information about the name of the specific ingredient and how many times it occurs in the chosen cuisine (in descending order based on their counts)..\n\n    '''\n    allother = []\n    for item in train[train.sentiment != sentiment]['temp_list1']:\n        for word in item:\n            allother .append(word)\n    allother  = list(set(allother ))\n    \n    specificnonly = [x for x in raw_text if x not in allother]\n    \n    mycounter = Counter()\n    \n    for item in train[train.sentiment == sentiment]['temp_list1']:\n        for word in item:\n            mycounter[word] += 1\n    keep = list(specificnonly)\n    \n    for word in list(mycounter):\n        if word not in keep:\n            del mycounter[word]\n    \n    Unique_words = pd.DataFrame(mycounter.most_common(numwords), columns = ['words','count'])\n    \n    return Unique_words","4308fac2":"Unique_Positive=words_unique('positive',20,raw_text)\nprint(\"The top 20 unique words in Positive Tweets are:\")\nUnique_Positive.style.background_gradient(cmap='Greens')","46830a0b":"fig=px.treemap(Unique_Positive,path=['words'],values='count',title='Tree Of Unique Positive Words')\nfig.show()","1f480fc7":"Unique_Negative= words_unique('negative', 10, raw_text)\nprint(\"The top 10 unique words in Negative Tweets are:\")\nUnique_Negative.style.background_gradient(cmap='Reds')","2462afe4":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.rcParams['text.color'] = 'black'\nplt.pie(Unique_Negative['count'], labels=Unique_Negative.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique Negative Words')\nplt.show()","1dd91b08":"Unique_Neutral= words_unique('neutral', 10, raw_text)\nprint(\"The top 10 unique words in Neutral Tweets are:\")\nUnique_Neutral.style.background_gradient(cmap='Oranges')","2c067fc3":"def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), color = 'white',\n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'u', \"im\"}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color=color,\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=400, \n                    height=200,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \nd = '\/kaggle\/input\/masks\/masks-wordclouds\/'","7aed16c1":"pos_mask = np.array(Image.open(d+ 'star.png'))\nplot_wordcloud(Neutral_sent.text,mask=pos_mask,color='white',max_font_size=100,title_size=30,title=\"WordCloud of Neutral Tweets\")","7e7f909c":"plot_wordcloud(Positive_sent.text,mask=pos_mask,title=\"Word Cloud Of Positive tweets\",title_size=30)","631c2699":"plot_wordcloud(Negative_sent.text,mask=pos_mask,title=\"Word Cloud of Negative Tweets\",color='white',title_size=30)","6eb9cf08":"df_train=pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/train.csv')\ndf_test=pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/test.csv')\ndf_submission=pd.read_csv('\/kaggle\/input\/tweet-sentiment-extraction\/sample_submission.csv')","c8528c0f":"#Number Of words in main Text in train set\ndf_train['Num_words_text']=df_train['text'].apply(lambda x:len(str(x).split()))","8012d3e9":"df_train=df_train[df_train['Num_words_text']>=3]","c2b58848":"def save_model(output_dir, nlp, new_model_name):\n\n\n  ''' This Function Saves model to \n    given output directory'''\n\n  output_dir=f'.\/'\n  if output_dir is not None:\n    if not os.path.exists(output_dir):\n      os.makedirs(output_dir)\n    nlp.meta[\"name\"]=new_model_name\n    nlp.to_disk(output_dir)\n    print(\"Saved model to\",output_dir)  \n","07994cb6":"# pass model = nlp if you want to train on top of existing model \n\ndef train(train_data, output_dir, n_iter=20, model=None):\n    \"\"\"Load the model, set up the pipeline and train the entity recognizer.\"\"\"\n    \"\"\n    if model is not None:\n        nlp = spacy.load(output_dir)  # load existing spaCy model\n        print(\"Loaded model '%s'\" % model)\n    else:\n        nlp = spacy.blank(\"en\")  # create blank Language class\n        print(\"Created blank 'en' model\")\n    \n    # create the built-in pipeline components and add them to the pipeline\n    # nlp.create_pipe works for built-ins that are registered with spaCy\n    if \"ner\" not in nlp.pipe_names:\n        ner = nlp.create_pipe(\"ner\")\n        nlp.add_pipe(ner, last=True)\n    # otherwise, get it so we can add labels\n    else:\n        ner = nlp.get_pipe(\"ner\")\n    \n    # add labels\n    for _, annotations in train_data:\n        for ent in annotations.get(\"entities\"):\n            ner.add_label(ent[2])\n\n    # get names of other pipes to disable them during training\n    other_pipes = [pipe for pipe in nlp.pipe_names if pipe != \"ner\"]\n    with nlp.disable_pipes(*other_pipes):  # only train NER\n        # sizes = compounding(1.0, 4.0, 1.001)\n        # batch up the examples using spaCy's minibatch\n        if model is None:\n            nlp.begin_training()\n        else:\n            nlp.resume_training()\n\n\n        for itn in tqdm(range(n_iter)):\n            random.shuffle(train_data)\n            batches = minibatch(train_data, size=compounding(4.0, 500.0, 1.001))    \n            losses = {}\n            for batch in batches:\n                texts, annotations = zip(*batch)\n                nlp.update(texts,  # batch of texts\n                            annotations,  # batch of annotations\n                            drop=0.5,   # dropout - make it harder to memorise data\n                            losses=losses, \n                            )\n            print(\"Losses\", losses)\n    save_model(output_dir, nlp, 'st_ner')\n","390f0256":"def get_model_out_path(sentiment):\n    '''\n    Returns Model output path\n    '''\n    model_out_path = None\n    if sentiment == 'positive':\n        model_out_path = '.\/'\n    elif sentiment == 'negative':\n        model_out_path = '.\/'\n    return model_out_path","66b1545f":"def get_training_data(sentiment):\n    '''\n    Returns Trainong data in the format needed to train spacy NER\n    '''\n    train_data = []\n    for index, row in df_train.iterrows():\n        if row.sentiment == sentiment:\n            selected_text = row.selected_text\n            text = row.text\n            start = text.find(selected_text)\n            end = start + len(selected_text)\n            train_data.append((text, {\"entities\": [[start, end, 'selected_text']]}))\n    return train_data","cb43a1e4":"sentiment='positive'\ntrain_data=get_training_data(sentiment)\nmodel_path=get_model_out_path(sentiment)\ntrain(train_data, model_path, n_iter=3, model=None)","6bd6ac8b":"sentiment='negative'\ntrain_data=get_training_data(sentiment)\nmodel_path=get_model_out_path(sentiment)\ntrain(train_data, model_path, n_iter=3, model=None)","b993d8c4":"def predict_entities(text, model):\n    doc = model(text)\n    ent_array = []\n    for ent in doc.ents:\n        start = text.find(ent.text)\n        end = start + len(ent.text)\n        new_int = [start, end, ent.label_]\n        if new_int not in ent_array:\n            ent_array.append([start, end, ent.label_])\n    selected_text = text[ent_array[0][0]: ent_array[0][1]] if len(ent_array) > 0 else text\n    return selected_text","dc459ad9":"selected_texts = []\nMODELS_BASE_PATH = '..\/input\/tse-spacy-model\/models\/'\n\nif MODELS_BASE_PATH is not None:\n    print(\"Loading Models  from \", MODELS_BASE_PATH)\n    model_pos = spacy.load(MODELS_BASE_PATH + 'model_pos')\n    model_neg = spacy.load(MODELS_BASE_PATH + 'model_neg')\n        \n    for index, row in df_test.iterrows():\n        text = row.text\n        output_str = \"\"\n        if row.sentiment == 'neutral' or len(text.split()) <= 2:\n            selected_texts.append(text)\n        elif row.sentiment == 'positive':\n            selected_texts.append(predict_entities(text, model_pos))\n        else:\n            selected_texts.append(predict_entities(text, model_neg))\n        \ndf_test['selected_text'] = selected_texts","9d51bad7":"df_submission['selected_text']=df_test['selected_text']\ndf_submission.to_csv(\"submission.csv\", index=False)\ndisplay(df_submission.head(10))","467e5486":"Here, it's clear that we have more neutral tweets with almost equal no of negative and positive tweets","ffb4a62f":"# **Exploratory Data Analysis**","3e6f0aec":"**Tree of Most Common Words**","313716e7":"**Most Common words in our Target-Selected Text**","b61f5949":"**Most Common words in Text**","1b20b93b":"**Approach:**\nExploratory Data Analysis by Generating Meta Features and calculating Jaccard Similarity scores. Modeling using Named Entity Recognition(NER)\n\n","950cbecd":"# **Twitter Sentiment Analysis**\n\nWith all of the tweets circulating every second it is hard to tell whether the sentiment behind a specific tweet will impact a company, or a person's, brand for being viral (positive), or devastate profit because it strikes a negative tone. Capturing sentiment in language is important in these times where decisions and reactions are created and updated in seconds.In this project I'll predict the words which actually lead to the sentiment description. ","704db0f1":"# **Modelling**","40691d51":"Here we have the predicted word or phrase from the tweet that exemplifies the provided sentiment. The word or phrase includes all characters within that span (i.e. including commas, spaces, etc.). ","c8fef2e4":"**Jaccered Scores across different Sentiments**","113f5846":"**Neutral Tweets**","6ecbe12f":"# **Distribution of Meta Features**","c4d04ce8":"**Training models for Positive and Negative tweets**","defe04e3":"approach:\n1. using text as selected_text fro all neutral tweets due to their high jaccared similarity.\n2. using text as selected_text for all tweets having number of words less than 3 in text.\n3. training two different models for Positive and Negtive tweets.\n4. no preprocessing of data because the selected text contains raw text","ab6b1de0":"**Target:**\n\nTo predict the word or phrase from the tweet that exemplifies the provided sentiment. The word or phrase should include all characters within that span (i.e. including commas, spaces, etc.)","7d20b0d6":"# **Cleaning Data**","dc75ed10":"**Creating helper Function**","e79a1897":"We can see some interesting trends here:\n\n*   Positive and negative tweets have high kurtosis and thus values are concentrated in two regions narrow and high density\n*   Neutral tweets have a low kurtosis value and their is bump in density near values of 1\n\n","2462b838":"# **Loading the Data**","2dd99a6a":"**Difference in number of words and jaccard_scores across different Sentiments**","31264d9a":"**WordClouds**","7013def3":"**Jaccard Similarity Scores between text and Selected_text**","d96d2638":"# **Importing Necessary Libraries and Modules**","26c5f098":"**Negative Tweets**","f790559d":"We will be using spacy for creating our own customised NER model or models (seperate for each Sentiment).","6ce27e56":"**creating necessary functions**","fdb41752":"**removing stopwords**","2c451133":"we have one null value in the train, as the text field for value is NAN.","fa8ec3d3":"**Dataset:**\n\nEach row contains the text of a tweet and a sentiment label. In the training set you are provided with a word or phrase drawn from the tweet (selected_text) that encapsulates the provided sentiment.","43934e41":"**Predicting with the trained Model**","92925873":"# **Generating Meta-Features**","ad2ff79e":"**Filling missing values**","58d7c7a6":"**Negative Tweets**","c55b92d7":"**Difference In Number of words in Selected_text and Text**","7f934970":"# **Please upvote my work if it could help! Thank you!**","9d135fad":"**Most common words SentimentsWise**","80d7a79c":"Named Entity Recognition (NER) is a standard NLP problem which involves spotting named entities (people, places, organizations etc.) from a chunk of text, and classifying them into a predefined set of categories.","1485cf05":"test set contains no null value","ef75f32c":"**Distribution of Number of words**","daac5786":"1. We can see words like get,go,dont,got,u,cant,lol,like are common in all three segments . That's interesting because words like dont and cant are more of negative nature and words like lol are more of positive nature.Does this mean our data is incorrectly labelled , we will have more insights on this after N-gram analysis\n\n2. It will be interesting to see the word unique to different sentiments\n\n\n","875a78e6":"**Modelling the problem as NER**","3e2da6a9":"**Positive Tweets**","369402cf":"**Analysing distribution of tweets**","48487199":"# **Conclusion Of EDA**","3a552012":"**Positive Tweets**","981f766a":"**Unique words in each Segment**"}}