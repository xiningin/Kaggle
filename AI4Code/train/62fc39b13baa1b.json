{"cell_type":{"4c432485":"code","4c85d31d":"code","be33b1db":"code","f9c012a0":"code","952ede99":"code","38149d70":"code","3a4563dd":"code","9aed89a6":"code","b69bbb18":"code","918b3db3":"code","82948c21":"code","57cbe8d8":"code","bebd2743":"code","c2553343":"code","0811ae75":"code","29ecbf4c":"code","a154f293":"markdown","b7198ee6":"markdown","85df216d":"markdown","49d15b49":"markdown"},"source":{"4c432485":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport seaborn as sns  # visualization tool\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.utils.np_utils import to_categorical\nimport matplotlib.pyplot as plt\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","4c85d31d":"data_train = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_train.csv')\ndata_test = pd.read_csv('..\/input\/fashionmnist\/fashion-mnist_test.csv')","be33b1db":"X_train = data_train.iloc[:,1:785].values\nX_test = data_test.iloc[:,1:785].values","f9c012a0":"print('X_train', X_train.shape)\nprint('X_test', X_test.shape)","952ede99":"y_train = data_train.iloc[:,0].values\ny_test = data_test.iloc[:,0].values","38149d70":"y_train = to_categorical(y_train)\ny_test = to_categorical(y_test)","3a4563dd":"print('y_train', y_train.shape)\nprint('y_test', y_test.shape)","9aed89a6":"x_train = X_train.T\nx_test = X_test.T\nY_train = y_train.T\nY_test = y_test.T\nprint(\"x train: \",x_train.shape)\nprint(\"x test: \",x_test.shape)\nprint(\"y train: \",Y_train.shape)\nprint(\"y test: \",Y_test.shape)","b69bbb18":"#normalization \nx_train = (x_train-np.min(x_train))\/(np.max(x_train)-np.min(x_train))\nx_test = (x_test-np.min(x_test))\/(np.max(x_test)-np.min(x_test))","918b3db3":"#print(Y_train[:,:])","82948c21":"# short description and example of definition (def)\ndef dummy(parameter):\n    dummy_parameter = parameter + 5\n    return dummy_parameter\nresult = dummy(3)     # result = 8\n\n# lets initialize parameters\n# So what we need is dimension 4096 that is number of pixels as a parameter for our initialize method(def)\ndef initialize_weights_and_bias(dimension):\n    w = np.full((dimension,1),0.01)\n    b = 0.0\n    return w, b","57cbe8d8":"# calculation of z\n#z = np.dot(w.T,x_train)+b\ndef sigmoid(z):\n    y_head = 1\/(1+np.exp(-z))\n    return y_head","bebd2743":"y_head = sigmoid(0)\ny_head","c2553343":"# Forward propagation steps:\n# find z = w.T*x+b\n# y_head = sigmoid(z)\n# loss(error) = loss(y,y_head)\n# cost = sum(loss)\ndef forward_propagation(w,b,x_train,y_train):\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z) # probabilistic 0-1\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    return cost \n\n# In backward propagation we will use y_head that found in forward progation\n# Therefore instead of writing backward propagation method, lets combine forward propagation and backward propagation\ndef forward_backward_propagation(w,b,x_train,y_train):\n    # forward propagation\n    z = np.dot(w.T,x_train) + b\n    y_head = sigmoid(z)\n    loss = -y_train*np.log(y_head)-(1-y_train)*np.log(1-y_head)\n    cost = (np.sum(loss))\/x_train.shape[1]      # x_train.shape[1]  is for scaling\n    # backward propagation\n    derivative_weight = (np.dot(x_train,((y_head-y_train).T)))\/x_train.shape[1] # x_train.shape[1]  is for scaling\n    derivative_bias = np.sum(y_head-y_train)\/x_train.shape[1]                 # x_train.shape[1]  is for scaling\n    gradients = {\"derivative_weight\": derivative_weight,\"derivative_bias\": derivative_bias}\n    return cost,gradients","0811ae75":"# Updating(learning) parameters\ndef update(w, b, x_train, y_train, learning_rate,number_of_iterarion):\n    cost_list = []\n    cost_list2 = []\n    index = []\n    # updating(learning) parameters is number_of_iterarion times\n    for i in range(number_of_iterarion):\n        # make forward and backward propagation and find cost and gradients\n        cost,gradients = forward_backward_propagation(w,b,x_train,y_train)\n        cost_list.append(cost)\n        # lets update\n        w = w - learning_rate * gradients[\"derivative_weight\"]\n        b = b - learning_rate * gradients[\"derivative_bias\"]\n        if i % 10 == 0:\n            cost_list2.append(cost)\n            index.append(i)\n            print (\"Cost after iteration %i: %f\" %(i, cost))\n    # we update(learn) parameters weights and bias\n    parameters = {\"weight\": w,\"bias\": b}\n    plt.plot(index,cost_list2)\n    plt.xticks(index,rotation='vertical')\n    plt.xlabel(\"Number of Iterarion\")\n    plt.ylabel(\"Cost\")\n    plt.show()\n    return parameters, gradients, cost_list\n#parameters, gradients, cost_list = update(w, b, x_train, y_train, learning_rate = 0.009,number_of_iterarion = 200)\n\n # prediction\ndef predict(w,b,x_test):\n    # x_test is a input for forward propagation\n    z = sigmoid(np.dot(w.T,x_test)+b)\n    Y_prediction = np.zeros((1,x_test.shape[1]))\n    # if z is bigger than 0.5, our prediction is sign one (y_head=1),\n    # if z is smaller than 0.5, our prediction is sign zero (y_head=0),\n    for i in range(z.shape[1]):\n        if z[0,i]<= 0.5:\n            Y_prediction[0,i] = 0\n        else:\n            Y_prediction[0,i] = 1\n\n    return Y_prediction\n# predict(parameters[\"weight\"],parameters[\"bias\"],x_test)","29ecbf4c":"def logistic_regression(x_train, Y_train, x_test, Y_test, learning_rate ,  num_iterations):\n    # initialize\n    dimension =  x_train.shape[0]  # that is 4096\n    w,b = initialize_weights_and_bias(dimension)\n    # do not change learning rate\n    parameters, gradients, cost_list = update(w, b, x_train, Y_train, learning_rate,num_iterations)\n    \n    y_prediction_test = predict(parameters[\"weight\"],parameters[\"bias\"],x_test)\n    y_prediction_train = predict(parameters[\"weight\"],parameters[\"bias\"],x_train)\n\n    # Print train\/test Errors\n    print(\"train accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_train - Y_train)) * 100))\n    print(\"test accuracy: {} %\".format(100 - np.mean(np.abs(y_prediction_test - Y_test)) * 100))\n    \nlogistic_regression(x_train, Y_train, x_test, Y_test,learning_rate = 0.0005, num_iterations = 300)","a154f293":"* Now lets look at computation graph of logistic regression\n<a href=\"http:\/\/ibb.co\/c574qx\"><img src=\"http:\/\/preview.ibb.co\/cxP63H\/5.jpg\" alt=\"5\" border=\"0\"><\/a>\n    * Parameters are weight and bias.\n    * Weights: coefficients of each pixels\n    * Bias: intercept\n    * z = (w.t)x + b  => z equals to (transpose of weights times input x) + bias \n    * In an other saying => z = b + px1*w1 + px2*w2 + ... + px4096*w4096\n    * y_head = sigmoid(z)\n    * Sigmoid function makes z between zero and one so that is probability. You can see sigmoid function in computation graph.\n* Why we use sigmoid function?\n    * It gives probabilistic result\n    * It is derivative so we can use it in gradient descent algorithm (we will see as soon.)\n* Lets make example:\n    * Lets say we find z = 4 and put z into sigmoid function. The result(y_head) is almost 0.9. It means that our classification result is 1 with 90% probability.\n* Now lets start with from beginning and examine each component of computation graph more detailed.","b7198ee6":"<font color='purple'>\nWhat we did up to this point:\n* Choose our labels (classes) that are sign zero and sign one\n* Create and flatten train and test sets\n* Our final inputs(images) and outputs(labels or classes) looks like this:\n<a href=\"http:\/\/ibb.co\/bWMK7c\"><img src=\"http:\/\/image.ibb.co\/fOqCSc\/3.png\" alt=\"3\" border=\"0\"><\/a>","85df216d":"**# LOGISTIC REGRESSION ----- START ------ **","49d15b49":"\nLogistic Regression is tried with Fashion Dataset of MNIST\n\n2019 - November"}}