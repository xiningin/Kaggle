{"cell_type":{"ee80c7ee":"code","f7991474":"code","2b7316d7":"code","f66336b0":"code","c7f5a1da":"code","aa34279a":"code","b4218ad8":"code","b93254bb":"code","4f6596e6":"code","83a13835":"code","7aacb010":"code","8525d58c":"code","bb3c31d8":"code","250bb2e0":"code","90a44769":"code","20d8c1c6":"code","78ac791a":"code","e3b6fd14":"code","566849c8":"code","996209ae":"code","120d4626":"code","9deabdad":"code","d8337242":"code","eba754f5":"code","578164ef":"code","2b12397f":"code","514be139":"code","f0eb2f46":"code","9de43148":"code","411a94d8":"code","d713010e":"code","8ee61bf5":"code","adb87b55":"code","40e002f4":"code","232d7c08":"code","b1c2b3dc":"code","42dc9228":"code","07169bdb":"code","4e6996da":"code","cd4d7788":"code","4b86f6c3":"code","2c4adb35":"markdown","d40ba40c":"markdown","16bb6b5d":"markdown","f14c3669":"markdown","fa74b867":"markdown","0733703b":"markdown","695660f3":"markdown","38849064":"markdown","26402167":"markdown","09a8eb47":"markdown","737fad28":"markdown","668b6d16":"markdown","20f817bf":"markdown","77acbda2":"markdown","6d421c88":"markdown","1ee5d970":"markdown","bde13f4d":"markdown","ff9021cf":"markdown","6565425b":"markdown","b3a25f10":"markdown","a1c26efd":"markdown","9d4dc925":"markdown","09ceee0e":"markdown","78913f6e":"markdown","a16eb9f0":"markdown","50b02fd7":"markdown","543ae7b2":"markdown","b7590936":"markdown"},"source":{"ee80c7ee":"#Libraries we will need.\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport librosa\nimport librosa.display\nfrom tqdm import tqdm\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport tensorflow as tf\nimport tensorflow.keras.models as models\nimport tensorflow.keras.layers as layers\nimport IPython.display as ipd\nfrom IPython.core.interactiveshell import InteractiveShell\nInteractiveShell.ast_node_interactivity = \"all\"\n\n%matplotlib inline\n%load_ext tensorboard","f7991474":"sample = r\"..\/input\/environmental-sound-classification-50\/audio\/audio\/1-100032-A-0.wav\" # single bark\nx,freq = librosa.load(sample)\nsr=freq","2b7316d7":"print(x.shape)\nprint(type(x))\nprint(freq)\nprint(type(freq))","f66336b0":"import IPython.display as ipd\nipd.Audio(sample)","c7f5a1da":"import matplotlib.pyplot as plt\nimport librosa.display\nplt.figure(figsize=(10,3))\nplt.title(\"Single Bark Wave Plot\")\nlibrosa.display.waveplot(x,sr=freq)","aa34279a":"sample2 = \"..\/input\/urbansound8k\/fold10\/100795-3-1-1.wav\"\nx,sr = librosa.load(sample2)\nipd.Audio(x,rate=sr) ","b4218ad8":"plt.figure(figsize=(10,3))\nplt.title(\"Multi Bark Wave Plot\")\nlibrosa.display.waveplot(x,sr=freq)","b93254bb":"X=librosa.stft(x) #stft -> Short-time Fourier transform\nX_db=librosa.amplitude_to_db(abs(X)) #Translation from amplitude to desibel(db) value\nplt.figure(figsize=(20,8))\nlibrosa.display.specshow(X_db, sr=sr,x_axis=\"time\",y_axis=\"hz\")\nplt.title(\"Multi Bark Sound Spectogram\")\nplt.colorbar()","4f6596e6":"sample3 = r\"..\/input\/gtzan-dataset-music-genre-classification\/Data\/genres_original\/jazz\/jazz.00005.wav\"\nx,sr = librosa.load(sample3)\nipd.Audio(x,rate=sr) ","83a13835":"data_h, data_p = librosa.effects.hpss(x)\nspec_h = librosa.feature.melspectrogram(data_h, sr=sr)\nspec_p = librosa.feature.melspectrogram(data_p, sr=sr)\ndb_spec_h = librosa.power_to_db(spec_h,ref=np.max)\ndb_spec_p = librosa.power_to_db(spec_p,ref=np.max)","7aacb010":"ipd.Audio(data_h,rate=sr) ","8525d58c":"librosa.display.specshow(db_spec_h,y_axis='mel', x_axis='s', sr=sr)\nplt.title(\"Harmonic Mel Spectogram\")\nplt.colorbar()","bb3c31d8":"ipd.Audio(data_p,rate=sr) ","250bb2e0":"librosa.display.specshow(db_spec_p,y_axis='mel', x_axis='s', sr=sr)\nplt.title(\"Percuisive Mel Spectogram\")\nplt.colorbar();","90a44769":"mfcc=librosa.feature.mfcc(x,sr=sr)\nprint(\"shape of mfcc:\" ,mfcc.shape)\n\n\nplt.figure(figsize=(15,6))\nlibrosa.display.specshow(mfcc,x_axis=\"s\")\nplt.title(\"Mel-Frequency Cepstral Coefficients\")\nplt.colorbar()","20d8c1c6":"zero_crossing=librosa.zero_crossings(x)\nprint(\"Type of Zero Crossing Rate\",type(zero_crossing))\nprint(zero_crossing, \" --> See it contains booleans\")\nprint(\"Total Number of Zero Crossing is: \",sum(zero_crossing))","78ac791a":"plt.figure(figsize=(15,5))\nplt.title(\"Zero Crossing Rate\")\nplt.plot(x[4000:5100])\nplt.grid()","e3b6fd14":"spec_cent=librosa.feature.spectral_centroid(x)\nprint(spec_cent.shape)\n\nplt.figure(figsize=(15,5))\nplt.title(\"Spectral Centroid\")\nplt.semilogy(spec_cent.T, \"r\")\nplt.ylabel(\"Hz\")","566849c8":"spec_roll=librosa.feature.spectral_rolloff(x,sr=sr)\nprint(spec_roll.shape)\n\nplt.figure(figsize=(15,5))\nplt.title(\"Spectral Roll off\")\nplt.semilogy(spec_roll.T,\"r\")\nplt.ylabel(\"Hz\")","996209ae":"y, sr = librosa.load(sample3)\nchroma=librosa.feature.chroma_stft(y=y, sr=sr)\nplt.figure(figsize=(10, 4))\nlibrosa.display.specshow(chroma, y_axis='chroma', x_axis='time')\nplt.colorbar()\nplt.title('Chromagram')\nplt.tight_layout()","120d4626":"spec_band=librosa.feature.spectral_bandwidth(x,sr=sr)\nprint(\"Spectral Bandwidth Shape: \",spec_band.shape)\nprint(\"Spectral Bandwidth: \", spec_band)","9deabdad":"x,sr = librosa.load(sample3)\n","d8337242":"S = librosa.magphase(librosa.stft(x, window=np.ones, center=False))[0]\nRMSEn= librosa.feature.rms(S=S)\nprint(RMSEn.shape)\n\n\nfig, ax = plt.subplots(nrows=2, sharex=True)\ntimes = librosa.times_like(RMSEn)\nplt.title(\"Root Mean Squared Energy & log Pawer Spectogram\")\nax[0].semilogy(times, RMSEn[0], label='RMS Energy')\nax[0].set(xticks=[])\nax[0].legend()\nax[0].label_outer()\nlibrosa.display.specshow(librosa.amplitude_to_db(S, ref=np.max), y_axis='log', x_axis='time', ax=ax[1])\nax[1].set(title='log Power spectrogram')\nplt.show()","eba754f5":"CSV_FILE_PATH = \"..\/input\/environmental-sound-classification-50\/esc50.csv\"  # path of csv file\nDATA_PATH = \"..\/input\/environmental-sound-classification-50\/audio\/audio\/44100\/\" # path to folder containing audio files","578164ef":"df = pd.read_csv(CSV_FILE_PATH)\ndf.head()\nprint(\"shape of df: \", df.shape)","2b12397f":"df=df.drop(['fold','esc10','src_file','take'], axis=1)","514be139":"classes = df['category'].unique()\nprint(\"Classes are: \",classes)\nprint(\"# of Classes are: \",classes.shape[0])","f0eb2f46":"class_dict = {i:x for x,i in enumerate(classes)}\n","9de43148":"#drop duplicates if there are any in filename...\ndf = df.drop_duplicates(subset=['filename'])","411a94d8":"df['target'] = df['category'].map(class_dict)\ndf.head()\nprint(\"df shape: \", df.shape)","d713010e":"X = []\ny = []","8ee61bf5":"for data in tqdm(df.iterrows(),  desc='Progress'):\n    sig , sr = librosa.load(DATA_PATH+data[1][0])\n    mfcc_ = librosa.feature.mfcc(sig , sr=sr, n_mfcc=40)\n    X.append(mfcc_)\n    y.append(data[1][1])\n","adb87b55":"X = np.array(X) \ny = np.array(y)","40e002f4":"X.shape","232d7c08":"y = tf.keras.utils.to_categorical(y , num_classes=50)\nX = X.reshape(X.shape[0], X.shape[1], X.shape[2], 1)\n","b1c2b3dc":"print(\"X Shape is: \", X.shape)\nprint(\"y Shape is: \", y.shape)","42dc9228":"X_train , X_test , y_train , y_test = train_test_split(X , y ,test_size=0.2, random_state=42)","07169bdb":"INPUTSHAPE = (40,216,1)","4e6996da":"model =  models.Sequential([\n    \n                          layers.Conv2D(32 , (3,3),activation = 'relu',padding='valid', input_shape = INPUTSHAPE),  \n                          layers.MaxPooling2D(2, padding='same'),\n                          layers.Conv2D(128, (3,3), activation='relu',padding='valid'),\n                          layers.MaxPooling2D(2, padding='same'),\n                          layers.Dropout(0.3),\n                          layers.Conv2D(128, (3,3), activation='relu',padding='valid'),\n                          layers.MaxPooling2D(2, padding='same'),\n                          layers.Dropout(0.3),\n                          layers.GlobalAveragePooling2D(),\n                          layers.Dense(512 , activation = 'relu'),\n                          layers.Dense(50 , activation = 'softmax')\n])\n\nmodel.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = 'acc')\nmodel.summary()","cd4d7788":"batch_size = 8\ncallback = tf.keras.callbacks.EarlyStopping(\n    monitor='val_loss', min_delta=0, patience=8, verbose=0, mode='auto',\n    baseline=None, restore_best_weights=False)\n\nhistory = model.fit(X_train,y_train ,\n            validation_data=(X_test,y_test),\n            epochs=40,\n            callbacks = [callback],batch_size=batch_size)","4b86f6c3":"import matplotlib.pyplot as plt\nplt.plot(history.history['acc'])\nplt.plot(history.history['val_acc'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()\n\n\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'validation'], loc='upper left')\nplt.show()","2c4adb35":"Print the names of the classes in the dataset.","d40ba40c":"#### 3.5 Spectral Centroid\n\nThe spectral centroid is a measure used in digital signal processing to characterise a spectrum. It indicates where the center of mass of the spectrum is located. Perceptually, it has a robust connection with the impression of brightness of a sound.","16bb6b5d":"##### Harmonics Only:","f14c3669":"#### 3.4 Zero Crossing Rate (ZCR)\n\n\nThe zero-crossing rate (ZCR) is the rate at which a signal changes from positive to zero to negative or from negative to zero to positive. Its value has been used in both speech recognition and music information retrieval, being a key feature to classify percussive sounds.\n\n![zcr](https:\/\/upload.wikimedia.org\/wikipedia\/commons\/0\/03\/Zero_crossing.svg)\n\n*Image Taken: Wikipedia*","fa74b867":"To install the librosa follow these steps:\n> !pip install librosa\n\n> import librosa","0733703b":"Sound can be played with ipd library as follows.","695660f3":"<div style=\"background-color:#B4DBE9; color:#636363;\">\n    <h1><right>Table of Contents<\/center><\/h1>\n    <a class=\"anchor\" id=\"chapter0\"><\/a>    \n<\/div>\n\n\n\n\n* [Chapter 1. Introduction](#chapter1)     \n* [Chapter 2. librosa Library Basics](#chapter2)\n* [Chapter 3. Audio Processing Basics & Features](#chapter3)\n* [Chapter 4. Environmental Audio Classification with Neural Networks](#chapter4)\n\n\n ##### ****$\\color{pink}{\\text{If You like my work, Please upvote!}}$****\n\n","38849064":"#### 3.7 Spectral Bandwidth\n\nBandwidth is the difference between the upper and lower frequencies in a continuous band of frequencies. It is typically measured in hertz, and depending on context, may specifically refer to passband bandwidth or baseband bandwidth.","26402167":"* We use Mel-Frequency Cepstral Coefficients (mfcc=40) as sampling and prepering the input.\n","09a8eb47":"#### 3.1 Spectogram\n\nA spectrogram is a visual way of representing the signal strength, or \u201cloudness\u201d, of a signal over time at various frequencies present in a particular waveform. Spectrograms are basically two-dimensional graphs, with a third dimension represented by colors. Time runs from left (oldest) to right (youngest) along the horizontal axis. More, the vertical axis represents frequency, which can also be thought of as pitch or tone, with the lowest frequencies at the bottom and the highest frequencies at the top.  The amplitude (or energy or \u201cloudness\u201d) of a particular frequency at a particular time is represented by the third dimension, color, with dark blues corresponding to low amplitudes and brighter colors up through red corresponding to progressively stronger (or louder) amplitudes.\n\nThe Short-time Fourier transform (STFT), is a Fourier-related transform used to determine the sinusoidal frequency and phase content of local sections of a signal as it changes over time.\n\nThe Fourier transform (a one-dimensional function) of the resulting signal is taken **as the window is slide along the time axis**, resulting in a two-dimensional representation of the signal. Mathematically, this is written as [2]:\n\n![stft.PNG](attachment:03f25b5a-dd0c-42dd-9a29-67402e2c839d.PNG)\n\nThe STFT is invertible, that is, the original signal can be recovered from the transform by the Inverse STFT. \n\n> librosa.stfx(x)\n\n*[1] Ref= https:\/\/pnsn.org\/spectrograms\/what-is-a-spectrogram , Retrieved = Oct 2021*\n\n*[2] Ref= https:\/\/en.wikipedia.org\/wiki\/Short-time_Fourier_transform , Retrieved = Oct 2021*","737fad28":"##### Percussion Only:","668b6d16":"## $\\color{Pink}{\\text{Chapter 3. Sound Processing Basics & Features}}$ <a class=\"anchor\" id=\"chapter3\"><\/a>\n\n\n* [Click to Turn Back to Table of Contents](#chapter0)  \n\nThe main features of audio can be categories as follows:\n\n* Time domain features\n\n* Frequency domain features\n\n* Time-Frequancy domain features","20f817bf":"![audio_kaggle.jpg](attachment:ab1d029d-3568-40db-9af0-0e4bec98fadf.jpg)","77acbda2":"#### 3.8 Root Mean Squared Energy","6d421c88":"#### 3.2 Harmonic-Percussive Separation (HPS)\n\nThe goal of harmonic\u2013percussive separation (HPS) is to decompose a given audio signal into two parts: one consisting of the harmonic and another of the percussive events. Librosa can also separate the initial audio series into harmonic and percussive components.\n\n> librosa.effects.hpss(x)","1ee5d970":"## $\\color{Pink}{\\text{Chapter 2. librosa Library Basics}}$ <a class=\"anchor\" id=\"chapter2\"><\/a>\n\n* [Click to Turn Back to Table of Contents](#chapter0)  ","bde13f4d":"#### 3.3 Mel-Frequency Cepstral Coefficients (MFCC)\n\nIn sound processing, the mel-frequency cepstrum (MFC) is a representation of the short-term power spectrum of a sound, based on a linear cosine transform of a log power spectrum on a nonlinear mel scale of frequency. Mel-frequency cepstral coefficients (MFCCs) are coefficients that collectively make up an MFC. They are derived from a type of cepstral representation of the audio clip (a nonlinear \"spectrum-of-a-spectrum\") [3]. \n\nThe Mel Frequency Cepstral Coefficients (MFCC) are commonly used in order to extract essential information from a voice signal and became a popular feature extractor used in audio processing.\n\n*[3] Ref:  Min Xu; et al. (2004). \"HMM-based audio keyword generation\" (PDF). In Kiyoharu Aizawa; Yuichi Nakamura; Shin'ichi Satoh (eds.). Advances in Multimedia Information Processing \u2013 PCM 2004: 5th Pacific Rim Conference on Multimedia. Springer. ISBN 978-3-540-23985-7. Archived from the original (PDF) on 2007-05-10*","ff9021cf":"For 2nd sample .wav we will use multiple bark of a dog.","6565425b":"* As seen model overfits and stoped by early stopping.","b3a25f10":"#### 3.6 Spectral Roll off\n\nSpectral rolloff is the frequency below which a specified percentage of the total spectral energy, e.g. 85%, lies.","a1c26efd":"## $\\color{Pink}{\\text{Chapter 4. Environmental Audio Classification with CNN}}$ <a class=\"anchor\" id=\"chapter4\"><\/a>\n\n\n* [Click to Turn Back to Table of Contents](#chapter0)  ","9d4dc925":"CNN takes input as (Number of Images , Height , Width , Channel). Therefore,we will reshape the input as Number of Images , Height , Width , 1. ","09ceee0e":"## $\\color{Pink}{\\text{Chapter 1. Introduction}}$ <a class=\"anchor\" id=\"chapter1\"><\/a>\n\n","78913f6e":"We drop the un releated columns in the dataframe.","a16eb9f0":"#### 3.7 Chroma Feature\n\nThe main idea of chroma features is to aggregate all spectral information that relates to a given pitch class into a single coefficient. It is a powerful representation for sound in which 12 parts representing the 12 different halftones (chroma) of the spectrum musical octave are specified: A, A#, B, C, C#, D, D#, E, F, F#, G, G#.","50b02fd7":"librosa.load() method loads an audio file as a floating point time series. Audio will be automatically resampled to the given rate (default sr=22050). It can be stated as librosa.load(sample, sr=11025) \n\n* x  \u2192 sound time series\n\n* sr \u2192 sound frequency (Hz)\n\nWe will use an .wav input to analysis. It contains single bark of a dog.","543ae7b2":"Welcome, Kagglers!!\n\nI decided to dig dive to audio processing and classification with Neural Networks. Thus, i wrote that notebook to state the audio basics, audio features and neural network training. So thumbs up if you like and enjoy please.\n\nThe dataset **\"Environmental Sound Classification 50\"** consists in 50 WAV files sampled at 16KHz for 50 different classes.To each one of the classes, corresponds 40 audio sample of 5 seconds each. All of these audio files have been concatenated by class in order to have 50 wave files of 3 min. 20sec.\n","b7590936":"librosa is a python package for music and audio analysis. It provides the building blocks necessary to create music information retrieval systems.\n\n\n![lr](https:\/\/librosa.org\/images\/librosa_logo_text.png)\n\nYou can access the paper of librosa from Brian McFee et al. from here -> [Librosa: Audio and Music Signal Analysis in Python](http:\/\/conference.scipy.org\/proceedings\/scipy2015\/pdfs\/brian_mcfee.pdf)\n\n*Ref: https:\/\/librosa.org\/*"}}