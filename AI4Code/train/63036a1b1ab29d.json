{"cell_type":{"398c4903":"code","cdc8a627":"code","d943a2ea":"code","d36556e5":"code","a857fcbd":"code","2d1a8e0d":"code","14c3ae2e":"code","cea3a304":"code","f361c6e0":"code","0c4f02ee":"code","3722da1e":"code","5baba7e2":"code","3ea56208":"code","708e593c":"code","91a8313b":"code","c9bc1848":"code","40202408":"code","c386ed4c":"code","65f4f9ba":"code","8e1a56d1":"code","c96e2c13":"code","7c7d2fb4":"code","80755d42":"code","322cc1ed":"code","19463ff5":"code","41478ece":"code","0f1ae47d":"code","ee5601ae":"code","57f02089":"code","1907434f":"code","af2741b1":"code","11bf064e":"code","592ab5de":"markdown","c28bbc36":"markdown"},"source":{"398c4903":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","cdc8a627":"np.random.seed(42)","d943a2ea":"# !pip install catboost featuretools hyperopt colorama","d36556e5":"df = pd.read_csv(\"..\/input\/train.csv\", index_col=\"ID_code\")","a857fcbd":"df.info()","2d1a8e0d":"df.sample(5)","14c3ae2e":"df.target.value_counts()","cea3a304":"from sklearn.model_selection import train_test_split","f361c6e0":"X = df.drop(\"target\", axis=\"columns\")\ny = df['target']","0c4f02ee":"from scipy.stats import norm, rankdata\ndef add_features(df):  \n    columns = df.columns\n    \n    df['sum']  = df[columns].sum(axis=1)  \n    df['min']  = df[columns].min(axis=1)\n    df['max']  = df[columns].max(axis=1)\n    df['mean'] = df[columns].mean(axis=1)\n    df['std']  = df[columns].std(axis=1)\n    df['skew'] = df[columns].skew(axis=1)\n    df['kurt'] = df[columns].kurtosis(axis=1)\n    df['med']  = df[columns].median(axis=1)\n    df['pos']  = df[columns].aggregate(lambda x: np.sum(x >= 0), axis=1)\n    df['neg']  = df[columns].aggregate(lambda x: np.sum(x < 0), axis=1)\n    \n    # Add more features\n    for col in df.columns:\n        # Normalize the data, so that it can be used in norm.cdf(), as though it is a standard normal variable\n        df[col] = ((df[col] - df[col].mean()) \/ df[col].std()).astype('float32')\n\n        # Square root\n        df[col+'_r1'] = np.round(df[col], 1)\n        \n        # Square root\n        df[col+'_r2'] = np.round(df[col], 2)\n\n        # Square\n        df[col+'_s'] = np.power(df[col], 2)\n\n        # Cube\n        df[col+'_c'] = np.power(df[col], 3)\n\n        # 4th power\n        df[col+'_q'] = np.power(df[col], 4)\n        \n        # Normalize the data, so that it can be used in norm.cdf(), as though it is a standard normal variable\n        # df[col] = ((df[col] - df[col].mean()) \/ df[col].std()).astype('float32')\n\n        # Cumulative percentile (not normalized)\n        # df[col+'_r'] = rankdata(df[col]).astype('float32')\n\n        # Cumulative normal percentile\n        # df[col+'_n'] = norm.cdf(df[col]).astype('float32')\n\n    return df","3722da1e":"X = add_features(X)","5baba7e2":"X.columns","3ea56208":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y)\nX_train.head()","708e593c":"from sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler, MaxAbsScaler, QuantileTransformer\nfrom sklearn.linear_model import LogisticRegression","91a8313b":"preprocessor = Pipeline([\n    ('scaler', MaxAbsScaler())\n])","c9bc1848":"import sklearn\nimport catboost\nimport hyperopt\nfrom hyperopt import hp, fmin, tpe, STATUS_OK, Trials\nimport colorama\n\nN_HYPEROPT_PROBES = 60\nHYPEROPT_ALGO = tpe.suggest\ncolorama.init()","40202408":"#preprocessor.fit(X_train)","c386ed4c":"#X_train = preprocessor.transform(X_train)\n#X_test = preprocessor.transform(X_test)","65f4f9ba":"D_train = catboost.Pool(X_train, y_train)\nD_test = catboost.Pool(X_test, y_test)","8e1a56d1":"def get_catboost_params(space):\n    params = dict()\n    params['learning_rate'] = space['learning_rate']\n    params['depth'] = int(space['depth'])\n    params['l2_leaf_reg'] = space['l2_leaf_reg']\n    params['border_count'] = space['border_count']\n    #params['rsm'] = space['rsm']\n    return params","c96e2c13":"obj_call_count = 0\ncur_best_loss = np.inf\nlog_writer = open( 'catboost-hyperopt-log.txt', 'w' )","7c7d2fb4":"def objective(space):\n    global obj_call_count, cur_best_loss\n\n    obj_call_count += 1\n\n    print('\\nCatBoost objective call #{} cur_best_loss={:7.5f}'.format(obj_call_count,cur_best_loss) )\n\n    params = get_catboost_params(space)\n\n    sorted_params = sorted(space.items(), key=lambda z: z[0])\n    params_str = str.join(' ', ['{}={}'.format(k, v) for k, v in sorted_params])\n    print('Params: {}'.format(params_str) )\n\n    model = catboost.CatBoostClassifier(iterations=100000,\n                                        learning_rate=params['learning_rate'],\n                                        depth=int(params['depth']),\n                                        loss_function='Logloss',\n                                        use_best_model=True,\n                                        task_type=\"GPU\",\n                                        eval_metric='AUC',\n                                        l2_leaf_reg=params['l2_leaf_reg'],\n                                        early_stopping_rounds=3000,\n                                        od_type=\"Iter\",\n                                        border_count=int(params['border_count']),\n                                        verbose=False\n                                        )\n    \n    model.fit(D_train, eval_set=D_test, verbose=False)\n    nb_trees = model.tree_count_\n\n    print('nb_trees={}'.format(nb_trees))\n\n    y_pred = model.predict_proba(D_test.get_features())\n    test_loss = sklearn.metrics.log_loss(D_test.get_label(), y_pred, labels=[0, 1])\n    acc = sklearn.metrics.accuracy_score(D_test.get_label(), np.argmax(y_pred, axis=1))\n    auc = sklearn.metrics.roc_auc_score(D_test.get_label(), y_pred[:,1])\n\n    log_writer.write('loss={:<7.5f} acc={} auc={} Params:{} nb_trees={}\\n'.format(test_loss, acc, auc, params_str, nb_trees ))\n    log_writer.flush()\n\n    if test_loss<cur_best_loss:\n        cur_best_loss = test_loss\n        print(colorama.Fore.GREEN + 'NEW BEST LOSS={}'.format(cur_best_loss) + colorama.Fore.RESET)\n\n\n    return{'loss':test_loss, 'status': STATUS_OK }","80755d42":"space = {\n        'depth': hp.quniform(\"depth\", 1, 6, 1),\n        'border_count': hp.uniform ('border_count', 32, 255),\n        'learning_rate': hp.loguniform('learning_rate', -5.0, -2),\n        'l2_leaf_reg': hp.uniform('l2_leaf_reg', 3, 8),\n       }\n\ntrials = Trials()\nbest = hyperopt.fmin(fn=objective,\n                     space=space,\n                     algo=HYPEROPT_ALGO,\n                     max_evals=N_HYPEROPT_PROBES,\n                     trials=trials,\n                     verbose=True)\n\nprint('-'*50)\nprint('The best params:')\nprint( best )\nprint('\\n\\n')","322cc1ed":"print(best)","19463ff5":"best.update({'border_count': int(best['border_count'])})","41478ece":"model = catboost.CatBoostClassifier(iterations=20000,\n                                    loss_function='Logloss',\n                                    use_best_model=True,\n                                    task_type=\"GPU\",\n                                    eval_metric='AUC',\n                                    early_stopping_rounds=500,\n                                    od_type=\"Iter\",\n                                    verbose=2000,\n                                    **best\n                                    )\n\nmodel.fit(D_train, eval_set=D_test, verbose=2000)","0f1ae47d":"pred = model.predict_proba(D_test.get_features())\nprint(\"auc = \", sklearn.metrics.roc_auc_score(D_test.get_label(), pred[:,1]))\nprint(\"acc = \", sklearn.metrics.accuracy_score(D_test.get_label(), np.argmax(pred, axis=1)))\nprint(\"loss = \", sklearn.metrics.log_loss(D_test.get_label(), pred, labels=[0, 1]))","ee5601ae":"df_test = pd.read_csv(\"..\/input\/test.csv\", index_col=\"ID_code\")\ndf_test.info()","57f02089":"df_test = add_features(df_test)\narr_test = df_test.values","1907434f":"D_train = catboost.Pool(arr_test)\ntest_preds = model.predict_proba(arr_test)[:,1]","af2741b1":"test_preds[:10]","11bf064e":"print(\"Saving submission file\")\nsample = pd.read_csv('..\/input\/sample_submission.csv')\nsample.target = test_preds.astype(float)\nsample.ID_code = df_test.index\nsample.to_csv('submission.csv', index=False)","592ab5de":"# submission","c28bbc36":"# catboost + hyperopt"}}