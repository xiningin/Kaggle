{"cell_type":{"567e3d0f":"code","3afd77d7":"code","8a080ef1":"code","f3259a9e":"code","d00ab71a":"code","13bc481a":"code","5b026ace":"code","0615cb90":"code","0fd67cf9":"code","7d7d02f1":"code","b60f246e":"code","232eac92":"code","e9264245":"code","da830afd":"code","f5c2ab2f":"code","0fa73fe3":"code","929fd54c":"code","7ff743aa":"code","88553f3f":"code","e3d23e55":"code","bafb76b2":"code","d8dc0414":"code","72a92f5a":"code","8f805ce6":"code","30e6e3f6":"code","75c56644":"code","5d4ae056":"code","ac441474":"code","979234b3":"code","7676f83f":"code","fc3293d8":"code","91ac7a56":"code","4e96386f":"code","a05b4641":"code","c11ea861":"code","76d327e2":"code","f5d85ff0":"code","3b298f9c":"code","5c6327c7":"code","9fed8920":"code","31ea0e85":"code","ed474d53":"code","cd70796b":"code","0c38f9aa":"code","40185ff5":"code","47bff369":"code","7cdef148":"code","b649a201":"code","a389e668":"code","d49d4ed7":"code","1e465d5b":"code","2472352b":"code","2eb998be":"code","f24559d0":"code","9fe9f06e":"code","2b3252e9":"code","fb371b5d":"code","4978e0d0":"code","f5071927":"code","1349bcb3":"markdown","73cd7004":"markdown","b6d8d96e":"markdown","56863bc4":"markdown","dd319281":"markdown","1ac02514":"markdown","77fc3245":"markdown","2392d0d5":"markdown","4c219fb6":"markdown","74c9122d":"markdown","24c8c45a":"markdown","de12f866":"markdown","a50b1420":"markdown","6454c643":"markdown","4ce9aa23":"markdown","be9b2ab3":"markdown","bc5865a4":"markdown","382b9009":"markdown","152fdac0":"markdown","6a4d98b7":"markdown","08591482":"markdown","50d0f354":"markdown","d20e5adb":"markdown","c4e421b9":"markdown","6fabad4d":"markdown","d55ff919":"markdown","611476e4":"markdown","89cc1494":"markdown","954fac6d":"markdown","09605881":"markdown","ec6edcc9":"markdown","e733a8ed":"markdown","b7eaba2c":"markdown","a6003aa9":"markdown","b955d885":"markdown","41251120":"markdown","e94618b5":"markdown","727211ca":"markdown","9c10c8aa":"markdown","606097d4":"markdown","817aeba1":"markdown","f6ba2c09":"markdown","109c4a61":"markdown","a7640f1f":"markdown","bec946bc":"markdown","fd10a912":"markdown","ef8368e2":"markdown","afc179bf":"markdown","cb3e9dce":"markdown","702a0be2":"markdown","bb418ad5":"markdown","bd0dfd91":"markdown","9fad34b5":"markdown","83cff1c0":"markdown"},"source":{"567e3d0f":"# Supress Warnings\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# import all important libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import r2_score\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import LinearRegression","3afd77d7":"df_car = pd.read_csv(\"..\/input\/CarPrice_Assignment.csv\")\ndf_car.head()","8a080ef1":"df_car.shape","f3259a9e":"df_car.head()","d00ab71a":"df_car.info()","13bc481a":"df_car.describe()","5b026ace":"df_car['carcompany'] = df_car.CarName.str.split().str[0]\ndf_car['carmodel'] = df_car.CarName.str.split().str[1]\ndf_car.carcompany = df_car.carcompany.replace({'maxda' : 'mazda' , 'vw':'volkswagen','vokswagen':'volkswagen',\n                                                       'Nissan':'nissan', 'porcshce':'porsche','toyouta':'toyota'})","0615cb90":"# drop car id , car name and carmodel as we dont need them for further analysis OR model building\ndf_car.drop(['car_ID'],axis=1,inplace=True)\ndf_car.drop(['CarName'],axis=1,inplace=True)\ndf_car.drop(['carmodel'],axis=1,inplace=True)","0fd67cf9":"plt.figure(figsize=(20, 20))\nplt.subplot(2,1,1)\nax1 = df_car.carcompany.value_counts().plot('bar')\nplt.title('Car Companies')\nplt.xticks(rotation=45,fontsize=12)\nax1.set(xlabel = 'Car Companies', ylabel='Favorite Car Company')\n\nplt.subplot(2,1,2)\nplt.title('Average car price of each company')\nplt.xticks(rotation=45,fontsize=12)\nax2=sns.barplot(x='carcompany', y='price', data=df_car, estimator=np.mean)\nax2.set_xlabel(\"Car Companies\")\nax2.set_ylabel(\"Avg Price (Dollars)\")\n\nplt.show()","7d7d02f1":"plt.figure(figsize=(16, 10))\n\nplt.subplot(3,2,1)\nax1 = sns.countplot(df_car['symboling'])\nax1.set(xlabel = 'Risk', ylabel= 'Count of Cars')\n\nplt.subplot(3,2,2)\nax2=sns.barplot(x='symboling', y='price', data=df_car, estimator=np.mean)\nax2.set_xlabel(\"Risk\")\nax2.set_ylabel(\"Avg Price (Dollars)\")\n\nplt.subplot(3,2,3)\nax1 = sns.countplot(df_car['carbody'])\nax1.set(xlabel = 'Car Body', ylabel= 'Count of Cars')\n\nplt.subplot(3,2,4)\nax1 = sns.countplot(df_car['doornumber'])\nax1.set(xlabel = 'No Of Doors', ylabel= 'Count of Cars')\n\nplt.subplot(3,2,5)\nax1 = sns.countplot(df_car['drivewheel'])\nax1.set(xlabel = 'Wheel Drive Type', ylabel= 'Count of Cars')\n\nplt.subplot(3,2,6)\nax1 = sns.countplot(df_car['enginelocation'])\nax1.set(xlabel = 'Engine Location', ylabel= 'Count of Cars')\n\nplt.show()\n# plt.tight_layout()","b60f246e":"plt.figure(figsize=(20, 14))\n\nplt.subplot(3,1,1)\nplt.xticks(rotation=45,fontsize=12)\nax1=sns.barplot(x='cylindernumber', y='price', data=df_car, estimator=np.mean)\nax1.set(xlabel = '# Cylinders', ylabel= 'Count of Cars')\n\nplt.subplot(3,1,2)\nplt.xticks(rotation=90,fontsize=12)\nax2=sns.barplot(x='horsepower', y='price', data=df_car, estimator=np.mean)\nax2.set_xlabel(\"Horsepower\")\nax2.set_ylabel(\"Avg Price (Dollars)\")\n\nplt.subplot(3,1,3)\nplt.xticks(rotation=45,fontsize=12)\nax2=sns.barplot(x='enginetype', y='price', data=df_car, estimator=np.mean)\nax2.set_xlabel(\"Engine Type\")\nax2.set_ylabel(\"Avg Price (Dollars)\")\n\nplt.show()","232eac92":"plt.figure(figsize=(20, 20))\nplt.subplot(3,3,1)\nsns.boxplot(x = 'fueltype', y = 'price', data = df_car)\nplt.subplot(3,3,2)\nsns.boxplot(x = 'aspiration', y = 'price', data = df_car)\nplt.subplot(3,3,3)\nsns.boxplot(x = 'carbody', y = 'price', data = df_car)\nplt.subplot(3,3,4)\nsns.boxplot(x = 'doornumber', y = 'price', data = df_car)\nplt.subplot(3,3,5)\nsns.boxplot(x = 'drivewheel', y = 'price', data = df_car)\nplt.subplot(3,3,6)\nsns.boxplot(x = 'enginelocation', y = 'price', data = df_car)\nplt.subplot(3,3,7)\nsns.boxplot(x = 'enginetype', y = 'price', data = df_car)\nplt.subplot(3,3,8)\nsns.boxplot(x = 'cylindernumber', y = 'price', data = df_car)\nplt.subplot(3,3,9)\nsns.boxplot(x = 'fuelsystem', y = 'price', data = df_car)\nplt.show()","e9264245":"plt.figure(figsize = (20, 20))\nsns.pairplot(df_car)\nplt.show()","da830afd":"plt.figure(figsize = (16, 10))\nsns.heatmap(df_car.corr(), annot = True, cmap=\"YlGnBu\")\nplt.show()","f5c2ab2f":"df_car['fuel-economy'] = df_car.highwaympg \/ df_car.citympg\ndf_car['car-dimension'] = df_car.carwidth \/ df_car.carheight\ndf_car['car-perf'] = df_car.horsepower \/ df_car.enginesize\ndf_car['car-load'] = df_car.curbweight \/ df_car.carlength\n\n# drop original variables  as we dont need it anymore\ndf_car.drop(['highwaympg'],axis=1,inplace=True)\ndf_car.drop(['citympg'],axis=1,inplace=True)\ndf_car.drop(['carwidth'],axis=1,inplace=True)\ndf_car.drop(['carheight'],axis=1,inplace=True)\ndf_car.drop(['carlength'],axis=1,inplace=True)\ndf_car.drop(['horsepower'],axis=1,inplace=True)\ndf_car.drop(['curbweight'],axis=1,inplace=True)\ndf_car.drop(['enginesize'],axis=1,inplace=True)","0fa73fe3":"plt.figure(figsize=(20,8))\n\nplt.subplot(1,2,1)\nplt.title('Car Price Distribution Plot')\nsns.distplot(df_car.price)\n\nplt.subplot(1,2,2)\nplt.title('Car Price Spread')\nsns.boxplot(y=df_car.price)\n\nplt.show()","929fd54c":"print(df_car.price.describe(percentiles = [0.25,0.50,0.75,1]))","7ff743aa":"categorical_cols=['fueltype','aspiration','enginelocation','carbody','drivewheel','enginetype','fuelsystem',\n                  'carcompany']\n\n# num_cols = ['wheelbase','boreratio','stroke','compressionratio','peakrpm','highwaympg','citympg','cylindernumber','doornumber',\n#             'symboling','carheight','carlength','carwidth','curbweight','enginesize','horsepower','price']\n\nnum_cols = ['wheelbase','boreratio','stroke','compressionratio','peakrpm','cylindernumber','doornumber',\n            'symboling','price','fuel-economy','car-dimension','car-perf','car-load']\n\ncnt = len (categorical_cols) + len(num_cols)\nprint(cnt)","88553f3f":"# Encoding : Convert binary categorical variables to 1's and 0's\ndf_car.doornumber = df_car.doornumber.replace({'two' : 2,'four' : 4})\ndf_car.cylindernumber = df_car.cylindernumber.replace({'two' : 2,'three': 3,'four' : 4,'five' : 5, 'six': 6, \n                                                       'eight' : 8, 'twelve' : 12})","e3d23e55":"df_car.head(10)","bafb76b2":"# Dummies : Let's convert categorical variables into numerical variables\ndf_ft = pd.get_dummies(df_car['fueltype'], drop_first = True)\ndf_car = pd.concat([df_car, df_ft], axis = 1)\n\ndf_asp = pd.get_dummies(df_car['aspiration'], drop_first = True)\ndf_car = pd.concat([df_car, df_asp], axis = 1)\n\ndf_el = pd.get_dummies(df_car['enginelocation'], drop_first = True)\ndf_car = pd.concat([df_car, df_el], axis = 1)\n\ndf_cb = pd.get_dummies(df_car['carbody'], drop_first = True)\ndf_car = pd.concat([df_car, df_cb], axis = 1)\n\ndf_dwhl = pd.get_dummies(df_car['drivewheel'], drop_first = True)\ndf_car = pd.concat([df_car, df_dwhl], axis = 1)\n\ndf_engtp = pd.get_dummies(df_car['enginetype'], drop_first = True)\ndf_car = pd.concat([df_car, df_engtp], axis = 1)\n\ndf_fs = pd.get_dummies(df_car['fuelsystem'], drop_first = True)\ndf_car = pd.concat([df_car, df_fs], axis = 1)\n\ndf_cc = pd.get_dummies(df_car['carcompany'], drop_first = True)\ndf_car = pd.concat([df_car, df_cc], axis = 1)","d8dc0414":"df_car.drop(columns=categorical_cols,axis=1,inplace=True)","72a92f5a":"df_car.shape","8f805ce6":"df_train, df_test = train_test_split(df_car, train_size = 0.7, test_size = 0.3, random_state = 100)","30e6e3f6":"df_train.shape","75c56644":"df_test.shape","5d4ae056":"scaler = MinMaxScaler()\ndf_train[num_cols] = scaler.fit_transform(df_train[num_cols])\ndf_train.head()","ac441474":"df_train.describe()","979234b3":"y_train = df_train.pop('price')\nX_train = df_train","7676f83f":"# Running RFE with the output number of the variable equal to 10\nlm = LinearRegression()\nlm.fit(X_train, y_train)\n\nrfe = RFE(lm, 20)             # running RFE\nrfe = rfe.fit(X_train, y_train)","fc3293d8":"list(zip(X_train.columns,rfe.support_,rfe.ranking_))","91ac7a56":"col_sel = X_train.columns[rfe.support_]\ncol_sel","4e96386f":"col_not_sel = X_train.columns[~rfe.support_]\ncol_not_sel","a05b4641":"X_train_rfe = X_train[col_sel]\nX_train_rfe = sm.add_constant(X_train_rfe)\nlm = sm.OLS(y_train,X_train_rfe).fit()   # Running the linear model\nprint(lm.summary())","c11ea861":"# Calculate the VIFs for the new model\nvif = pd.DataFrame()\nX = X_train_rfe\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","76d327e2":"# High VIF and High pVal\nX_train_new1 = X_train_rfe.drop([\"idi\"], axis = 1)","f5d85ff0":"X_train_lm1 = sm.add_constant(X_train_new1)\nlm = sm.OLS(y_train,X_train_lm1).fit()   # Running the linear model\nprint(lm.summary())","3b298f9c":"# Calculate the VIFs for the new model\nvif = pd.DataFrame()\nX = X_train_new1\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","5c6327c7":"X_train_new2 = X_train_new1.drop([\"chevrolet\"], axis = 1)","9fed8920":"X_train_lm2 = sm.add_constant(X_train_new2)\nlm = sm.OLS(y_train,X_train_lm2).fit()   # Running the linear model\nprint(lm.summary())","31ea0e85":"# Calculate the VIFs for the new model\nvif = pd.DataFrame()\nX = X_train_new2\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","ed474d53":"X_train_new3 = X_train_new2.drop([\"compressionratio\"], axis = 1)","cd70796b":"X_train_lm3 = sm.add_constant(X_train_new3)\nlm = sm.OLS(y_train,X_train_lm3).fit()   # Running the linear model\nprint(lm.summary())","0c38f9aa":"# Calculate the VIFs for the new model\nvif = pd.DataFrame()\nX = X_train_new3\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","40185ff5":"X_train_new4 = X_train_new3.drop([\"gas\"], axis = 1)","47bff369":"X_train_lm4 = sm.add_constant(X_train_new4)\nlm = sm.OLS(y_train,X_train_lm4).fit()   # Running the linear model\nprint(lm.summary())","7cdef148":"# Calculate the VIFs for the new model\nvif = pd.DataFrame()\nX = X_train_new4\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","b649a201":"X_train_new5 = X_train_new4.drop([\"l\"], axis = 1)","a389e668":"X_train_lm5 = sm.add_constant(X_train_new5)\nlm = sm.OLS(y_train,X_train_lm5).fit()   # Running the linear model\nprint(lm.summary())","d49d4ed7":"# Calculate the VIFs for the new model\nvif = pd.DataFrame()\nX = X_train_new5\nvif['Features'] = X.columns\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif['VIF'] = round(vif['VIF'], 2)\nvif = vif.sort_values(by = \"VIF\", ascending = False)\nvif","1e465d5b":"X_train_new5 = X_train_new5.drop([\"const\"], axis = 1)","2472352b":"y_train_price = lm.predict(X_train_lm5)","2eb998be":"# Plot the histogram of the error terms\nfig = plt.figure()\nsns.distplot((y_train - y_train_price), bins = 20)\nfig.suptitle('Error Terms', fontsize = 20)                  # Plot heading \nplt.xlabel('Errors', fontsize = 18)                         # X-label","f24559d0":"df_test[num_cols] = scaler.transform(df_test[num_cols])","9fe9f06e":"y_test = df_test.pop('price')\nX_test = df_test","2b3252e9":"# # Now let's use our model to make predictions.\n\n# # Creating X_test_new dataframe by dropping variables from X_test\nX_test_new = X_test[X_train_new5.columns]\n\n# # Adding a constant variable \nX_test_new = sm.add_constant(X_test_new)","fb371b5d":"# Making predictions with the final model (model-6)\ny_pred = lm.predict(X_test_new)","4978e0d0":"# Plotting y_test and y_pred to understand the spread\n\nfig = plt.figure()\nplt.scatter(y_test, y_pred)\nfig.suptitle('y_test vs y_pred', fontsize = 20)              # Plot heading \nplt.xlabel('y_test', fontsize = 18)                          # X-label\nplt.ylabel('y_pred', fontsize = 16)   ","f5071927":"r2_score(y_test, y_pred)","1349bcb3":"### Data Preparation\n- Encoding Variables","73cd7004":"### Dividing into X and Y sets for the model building","b6d8d96e":"Let's now analyze numerical variables and check their correlation","56863bc4":"Lets now look at our target\/dependent variable 'price'","dd319281":"Inference :\n- It seems that the cars with symboling 0 and 1 values are most sold (as compared to others)\n- The cars with -1 symboling value(i.e insurance risk rating)   seems to be high priced. But it seems that symboling with 3 value has the price range similar to -2 value. There is a dip in price at symboling 1.\n- Sedan cars are most sold\n- 4 door cars are most sold\n- Front wheel drive(fwd) type cars are more sold than rear wheel and four wheel drive cars\n- Front engine cars are more favored","1ac02514":"###### MODEL-6","77fc3245":"Let's check which all features are selected","2392d0d5":"Inference :\n- Toyota seems to be the most selling car\n- Jaguar(which is obvious) seems to be the costliest car based on average car prices and Buick,Porsche follows as 2nd and 3rd respectively.","4c219fb6":"### Model Evaluation","74c9122d":"The error terms are also normally distributed (which is infact, one of the major assumptions of linear regression).","24c8c45a":"###### MODEL-5","de12f866":"#### Applying the scaling on the test sets","a50b1420":"### Residual Analysis of the train data","6454c643":"##### Re-checking VIF","4ce9aa23":"##### Re-checking VIF","be9b2ab3":"We can derive from above the equation of the best fitted line and that is as given below :\n\n$ price = (0.273 \\times wheelbase) + (0.188 \\times cylindernumber) + (0.274 \\times car-dimension) + (0.460 \\times car-load)  + (0.493 \\times rear( + (0.215 \\times bmw) + (-0.093 \\times dodge ) \n+ ( -0.055 \\times honda ) + (-0.130 \\times mitsubishi) + (-0.044 \\times nissan) + (-0.159 \\times peugeot) + (-0.087 \\times plymouth) + (-0.104 \\times renault) + ( -0.084 \\times subaru ) + ( -0.092 \\times toyota ) $","bc5865a4":"Inference\n- Cars with 8 & 12 cylinders are more used\/sold\n- Cars with more horsepower are high priced(very obvious). Horsepower and price are highly correlated variables.\n- Cars with engine type dohcv & ohcv are more costlier than others","382b9009":"Drop the categorical variables from the main data set as we have created dummies","152fdac0":"### Build a Linear Model\nHere we will start with building the mode using \n##### RFE : Recursive feature elimination","6a4d98b7":"Load the dataset of car prices. (Source : https:\/\/archive.ics.uci.edu\/ml\/datasets\/Automobile )","08591482":"From above it now is clear that that all the features are good for making predictions as the `pValues are below 0.05(pValue < 0.05)` and `VIF is less than 5(VIF < 5)`\n\nThe VIFs and p-values both are within an acceptable range. So we go ahead and make our predictions using this model on the training data.","50d0f354":"#### Dropping the variable(s) and Updating the Model\n\nAs you can notice some of the variable have low VIF values but high p-values. Such variables are insignificant and should be dropped.\n\nAs you might have noticed, the variable `chevrolet` has a low VIF (`1.62`) but a high p-value (`0.236`). Hence, this variable isn't of much use and should be dropped.","d20e5adb":"##### Re-checking VIF","c4e421b9":"Lets group the variables\/ columns in the datset ","6fabad4d":"#### Dropping the variable(s) and Updating the Model\nAs you might have noticed, the variable `gas` has a low VIF (`1.31`) but a high p-value (`0.467`). Hence, this variable isn't of much use and should be dropped.","d55ff919":"### Data Preparation\n- Dummy Variables (conversion to indicator variables)","611476e4":"Lets check the train data","89cc1494":"So there are 205 rows and 26 columns. Lets look at the data now.","954fac6d":"Inference\n- Most prices in the dataset are low(around 15,000). The price seems to be right-skewed\n- Significant difference between the mean and the median of the price\n- There is high variance in the car prices.(75% of the prices are below 16,500, whereas the remaining 25% are between 16,500 and 45,400.)\n- The data points are far spread out from the mean","09605881":"###### MODEL-3","ec6edcc9":"#### Dropping the variable(s) and Updating the Model\n\nAs you can notice some of the variable have high VIF values as well as high p-values. Such variables are insignificant and should be dropped.\n\nAs you might have noticed, the variable `idi` has a significantly high VIF (`inf`) and a high p-value (`0.922`) as well. Hence, this variable isn't of much use and should be dropped.","e733a8ed":"##### Re-checking VIF","b7eaba2c":"#### Dropping the variable(s) and Updating the Model\nAs you might have noticed, the variable `l` has a high VIF (`9.2`) but a high p-value (`0.054`). Hence, this variable isn't of much use and should be dropped.","a6003aa9":"###### MODEL-4","b955d885":"### Splitting the Data into Training and Testing Sets\n\nThe first basic step for regression is performing a train-test split.","41251120":"#### Checking VIF","e94618b5":"##### Re-checking VIF","727211ca":"#### Building model using statsmodel, for the detailed statistics\n\n###### MODEL-1","9c10c8aa":"### Multicollinearity check and handling\nMulticollinearity refers to the phenomenon of having related predictor (independent)  variables in the input data set. In simple terms, in a model that has been built using several independent variables, some of these variables might be interrelated, due to which the presence of that variable in the model is redundant. You drop some of these related independent variables OR create a new variable OR perform variable transformations as a way of handling\/dealing with multicollinearity.\n\nMulticollinearity has an impact on the following:\n\n- Interpretation\n        - Does \u201cchange in Y when all others are held constant\u201d apply?\n- Inference \n        - Coefficients swing wildly, signs can invert.Therefore, p-values are not reliable.\n\nFrom above heatmap and pairplots, we have seen that few of the indepdent variables are highly correlated and hence we need to deal with them","606097d4":"### Business Case\nA Chinese automobile company Geely Auto aspires to enter the US market by setting up their manufacturing unit there and producing cars locally to give competition to their US and European counterparts. \nThey have contracted an automobile consulting company to understand the factors on which the pricing of cars depends. Specifically, they want to understand the factors affecting the pricing of cars in the American market, since those may be very different from the Chinese market. The company wants to know:\n\n- Which variables are significant in predicting the price of a car\n- How well those variables describe the price of a car\n\nBased on various market surveys, the consulting firm has gathered a large dataset of different types of cars across the Americal market. \n\n### Business Goal \n\nYou are required to model the price of cars with the available independent variables. It will be used by the management to understand how exactly the prices vary with the independent variables. They can accordingly manipulate the design of the cars, the business strategy etc. to meet certain price levels. Further, the model will be a good way for management to understand the pricing dynamics of a new market.","817aeba1":"Inference\n- Horspower and price are highly correlated\n- Its interesting to also note that in this dataset there are high degree of correlation between independent \/ predictor variables for example as listed below\n    - High way mileage and City mileage (highwampg, citympg - 0.97)\n    - More the car length more the weight of a car without occupants or baggage.(carlenght,curbweght - 0.88)\n    - Horsepwer direcly correlated with engine size(horsepower,enginesize - 0.81) like wise.","f6ba2c09":"#### Dividing into X_test and y_test","109c4a61":"Inference:\n- Cars with diesel as a fuel type are more costlier OR Cars with gas as aa fuel type are cheaper then diesel cars.\n- Cars with turbo engines are costlier\n- Hatchback(small) cars are the cheapest out of others cars\n- 4 door cars are costlier than 2 door\n- Rear wheel drive(rwd) cars are the costliest amongst Front wheel drive and 4 wheel drive. This is interesting !\n- Cars with rear engine are costlier ! This is again very interesting.\n- Cars with mpfi and idi fuel system are the costlier amongst other fuel system cars","a7640f1f":"From above it is seen that there are no columns with null values","bec946bc":"Lets analyze th categorical variables and its relation with the target variable i.e price","fd10a912":"To work on the problem statement above and the business requirement we need to build a multiple linear regression model for the prediction of car prices.\n\nLets begin by loading all the required important libraries that we need","ef8368e2":"In the data set there is a variable named CarName which is comprised of two parts - the first word is the name of 'car company' and the second is the 'car model'. \n### Data Preparation \nLet's split the CarName column and derive 2 car company and car model.","afc179bf":"#### Dropping the variable(s) and Updating the Model\nAs you might have noticed, the variable `compressionratio` has a high VIF (`54.81`) and a high p-value (`0.121`). Hence, this variable isn't of much use and should be dropped.","cb3e9dce":"### Making Predictions Using the Final Model (MODEL-6)","702a0be2":"So now that we have the train data set and test data split correct as per 70-30 ratio(0.7\/0.3) lets scale the variables as the next step\n\n### Rescaling the Features \n\nIt is extremely important to rescale the variables so that they have a comparable scale. If we don't have comparable scales, then some of the coefficients as obtained by fitting the regression model might be very large or very small as compared to the other coefficients. This might become very annoying at the time of model evaluation. So it is advised to use standardization or normalization so that the units of the coefficients obtained are all on the same scale. There are two common ways of rescaling:\n\n1. Min-Max scaling \n2. Standardisation (mean-0, sigma-1) \n\nWe will use MinMax scaling.","bb418ad5":"Overall we have a decent model","bd0dfd91":"###### MODEL-2","9fad34b5":"### Data Analysis","83cff1c0":"### Data Visualization"}}