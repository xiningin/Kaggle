{"cell_type":{"540181d9":"code","0e08a9ce":"code","81365988":"code","464b5bd4":"code","94bfe660":"code","1ccbb784":"code","3cc48cdf":"code","3a789b76":"code","daf3e632":"code","77c2f474":"code","295cd521":"code","edea05b5":"code","2722ba5a":"code","74a7c3e9":"code","404f3e83":"code","cf9d87b8":"code","cdf0dcbd":"code","ad00aa83":"code","a82f4099":"code","0953ca66":"code","b1306de4":"code","2eca745d":"code","168f8f47":"code","19c62b9d":"code","021cac15":"code","1d10fd6e":"code","bf4314d0":"code","f92e9d53":"code","833b4366":"code","fdf8a621":"code","f8601b2b":"code","8b094d12":"code","815054c6":"code","98fbb9e9":"code","91ae07cc":"code","8dbfeca1":"code","ff06e434":"code","086b534b":"code","c481ae3b":"code","1d70b7bd":"code","3129f2cc":"code","e1ed4168":"code","da91efda":"code","bf912a8c":"code","f8841724":"code","5e06c664":"code","f8eacf86":"code","9d29d8e3":"code","2f9a3c32":"code","a2d86906":"code","cd2f67f3":"code","8ebbe72c":"code","89e480af":"markdown","5ea87428":"markdown","0ed7f4cb":"markdown","eb508bf8":"markdown","5d0c8bd7":"markdown","ada86759":"markdown","5599d213":"markdown","d56fd529":"markdown","662bd474":"markdown","2374549d":"markdown","ce1704c2":"markdown","e6c69a42":"markdown","05b4d7a9":"markdown","bec763eb":"markdown","94974af8":"markdown","b2a74c82":"markdown","7c11b4b6":"markdown","b7b3ee5a":"markdown","6c35088c":"markdown","53544f95":"markdown","c16d2444":"markdown","3c284af6":"markdown","1a486560":"markdown","55df7c7d":"markdown","bcf09f74":"markdown","580bea74":"markdown","66c70cbf":"markdown","08a38b2c":"markdown","1bc3d85f":"markdown","ed0dd3ec":"markdown","7d73b587":"markdown","b183907c":"markdown","28db49f4":"markdown","f43e19c8":"markdown","7f889c9f":"markdown","83e2c756":"markdown","cbea1a26":"markdown","feed995a":"markdown","05e6de3e":"markdown","c30451b2":"markdown"},"source":{"540181d9":"#%% Importing Libraries\n\n# Basic Imports \nimport numpy as np\nimport pandas as pd\nfrom IPython.display import display, HTML\n\n# Plotting \nfrom matplotlib import pyplot\nimport matplotlib.pyplot as plt\nimport plotly.express as px\nimport seaborn as sns\n%matplotlib inline\n\n# Preprocessing\nfrom sklearn.model_selection import train_test_split, StratifiedKFold,cross_val_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.impute import SimpleImputer\n# explicitly require this experimental feature\nfrom sklearn.experimental import enable_iterative_imputer  # noqa\n# now you can import normally from sklearn.impute\nfrom sklearn.impute import IterativeImputer\n\n# Metrics \nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import plot_confusion_matrix\nfrom sklearn.metrics import precision_recall_fscore_support\nfrom sklearn.metrics import classification_report,accuracy_score, recall_score, roc_auc_score, precision_score\nfrom sklearn.metrics import roc_curve, auc\n\n\n# ML Models\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nimport xgboost as xgb\nimport lightgbm as lgb\nfrom lightgbm import LGBMClassifier \n\n# Model Tuning \nfrom bayes_opt import BayesianOptimization\n\n# Feature Importance \nimport shap \n\n# Ignore Warnings \nimport warnings\nwarnings.filterwarnings('ignore')","0e08a9ce":"#%% Read train.csv\ntrain_csv = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/train.csv')\n\n# Initial glance at train.csv\nprint(train_csv.info(verbose = True,show_counts=True))","81365988":"#%% Read train.csv\ntest_csv = pd.read_csv('..\/input\/tabular-playground-series-apr-2021\/test.csv')\n\n# Initial glance at train.csv\nprint(test_csv.info(verbose = True,show_counts=True))","464b5bd4":"#%% PlotMultiplePie \n# Input: df = Pandas dataframe, categorical_features = list of features , dropna = boolean variable to use NaN or not\n# Output: prints multiple px.pie() \n\ndef PlotMultiplePie(df,categorical_features = None,dropna = False):\n    # set a threshold of 30 unique variables, more than 50 can lead to ugly pie charts \n    threshold = 30\n    \n    # if user did not set categorical_features \n    if categorical_features == None: \n        categorical_features = df.select_dtypes(['object','category']).columns.to_list()\n        \n    print(\"The Categorical Features are:\",categorical_features)\n    \n    # loop through the list of categorical_features \n    for cat_feature in categorical_features: \n        num_unique = df[cat_feature].nunique(dropna = dropna)\n        num_missing = df[cat_feature].isna().sum()\n        # prints pie chart and info if unique values below threshold \n        if num_unique <= threshold:\n            print('Pie Chart for: ', cat_feature)\n            print('Number of Unique Values: ', num_unique)\n            print('Number of Missing Values: ', num_missing)\n            fig = px.pie(df[cat_feature].value_counts(dropna = dropna), values=cat_feature, \n                 names = df[cat_feature].value_counts(dropna = dropna).index,title = cat_feature,template='ggplot2')\n            fig.show()\n        else: \n            print('Pie Chart for ',cat_feature,' is unavailable due high number of Unique Values ')\n            print('Number of Unique Values: ', num_unique)\n            print('Number of Missing Values: ', num_missing)\n            print('\\n')","94bfe660":"#%% Use PlotMultiplePie to see the distribution of the categorical variables for train_csv\nPlotMultiplePie(train_csv)","1ccbb784":"#%% Use PlotMultiplePie to see the distribution of the categorical variables for test_csv\nPlotMultiplePie(test_csv)","3cc48cdf":"# Cabin Feature for train.csv\n# https:\/\/stackoverflow.com\/questions\/35552874\/get-first-letter-of-a-string-from-column\ntrain_csv_cabin_letter = pd.DataFrame(train_csv['Cabin'],columns = ['Cabin'])\ntrain_csv_cabin_letter['Cabin'] = [x[0] if isinstance(x, str) else np.nan for x in train_csv_cabin_letter['Cabin']]\nPlotMultiplePie(train_csv_cabin_letter)","3a789b76":"# Cabin Feature for test.csv\n# https:\/\/stackoverflow.com\/questions\/35552874\/get-first-letter-of-a-string-from-column\ntest_csv_cabin_letter = pd.DataFrame(test_csv['Cabin'],columns = ['Cabin'])\ntest_csv_cabin_letter['Cabin'] = [x[0] if isinstance(x, str) else np.nan for x in test_csv_cabin_letter['Cabin']]\nPlotMultiplePie(test_csv_cabin_letter)","daf3e632":"#%% Print the continous features in train_csv \ncontinous_features = train_csv.drop([\"Survived\",\"PassengerId\"],axis = \"columns\").select_dtypes(['float64']).columns.to_list()\n\nfor cont_feature in continous_features: \n    plt.figure()\n    plt.title(cont_feature)\n    ax = sns.histplot(train_csv[cont_feature])","77c2f474":"#%% Print the continous features in test_csv \ncontinous_features = test_csv.drop([\"PassengerId\"],axis = \"columns\").select_dtypes(['float64']).columns.to_list()\n\nfor cont_feature in continous_features: \n    plt.figure()\n    plt.title(cont_feature)\n    ax = sns.histplot(train_csv[cont_feature])","295cd521":"PlotMultiplePie(train_csv,['Survived'])","edea05b5":"# Save 'PassengerId' for Train and Test \ntrain_csv_id = train_csv['PassengerId'].to_list()\ntest_csv_id = test_csv['PassengerId'].to_list()\n\n# Seperate train_csv into target and features \ny_train_csv = train_csv['Survived']\nX_train_csv = train_csv.drop(['Survived','PassengerId'],axis = 'columns')\n\n# Create a copy of test_csv \nX_test_csv = test_csv.copy(deep=True).drop('PassengerId',axis = 'columns')\n\n# Features to drop\ndrop_features = ['Name','Ticket']\nX_train_csv = X_train_csv.drop(drop_features,axis = 'columns')\nX_test_csv = X_test_csv.drop(drop_features,axis = 'columns')\n\n# Use list comprehension to extract letter from Cabin, if NaN replace with string 'Null'\nX_train_csv['Cabin'] = [x[0] if isinstance(x, str) else 'Null' for x in X_train_csv['Cabin']]\nX_test_csv['Cabin'] = [x[0] if isinstance(x, str) else 'Null' for x in X_test_csv['Cabin']]\n\ndisplay(X_train_csv.head())\ndisplay(X_test_csv.head())","2722ba5a":"# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.impute.SimpleImputer.html#sklearn.impute.SimpleImputer\n# from sklearn.impute import SimpleImputer\n\n# use most frequent simple imputer which is median and mode imputation for numeric and categorical features respectively  \nX_train_most_frequent_imputer = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\",\n                            add_indicator=False).fit(X_train_csv)\nX_train_simple_imputed = pd.DataFrame(X_train_most_frequent_imputer.transform(X_train_csv), columns = X_train_csv.columns.to_list()).convert_dtypes()\n\nX_train_simple_imputed[X_train_simple_imputed.select_dtypes(['string']).columns.to_list()] =  X_train_simple_imputed.select_dtypes(['string']).astype('object')\nX_train_simple_imputed[X_train_simple_imputed.select_dtypes(['Int64']).columns.to_list()] =  X_train_simple_imputed.select_dtypes(['Int64']).astype('int64')\nX_train_simple_imputed[X_train_simple_imputed.select_dtypes(['Float64']).columns.to_list()] =  X_train_simple_imputed.select_dtypes(['Float64']).astype('float64')\n\n# SimpleImputer summary statistics for X_train_csv\nX_train_simple_imputed.describe(include='all')","74a7c3e9":"# use most frequent simple imputer which is median and mode imputation for numeric and categorical features respectively  \nX_test_most_frequent_imputer = SimpleImputer(missing_values=np.nan, strategy=\"most_frequent\",\n                            add_indicator=False).fit(X_test_csv)\nX_test_simple_imputed = pd.DataFrame(X_test_most_frequent_imputer.transform(X_test_csv), columns = X_test_csv.columns.to_list()).convert_dtypes()\n\nX_test_simple_imputed[X_test_simple_imputed.select_dtypes(['string']).columns.to_list()] =  X_test_simple_imputed.select_dtypes(['string']).astype('object')\nX_test_simple_imputed[X_test_simple_imputed.select_dtypes(['Int64']).columns.to_list()] =  X_test_simple_imputed.select_dtypes(['Int64']).astype('int64')\nX_test_simple_imputed[X_test_simple_imputed.select_dtypes(['Float64']).columns.to_list()] =  X_test_simple_imputed.select_dtypes(['Float64']).astype('float64')\n\n# SimpleImputer summary statistics for X_test_csv\nX_test_simple_imputed.describe(include='all')","404f3e83":"X_test_simple_imputed.head()","cf9d87b8":"# Save the index for X_train_csv \nX_train_csv_index = X_train_simple_imputed.index.to_list()\n\n# Row bind train.csv features with test.csv features \n# this makes it easier to apply label encoding or one hot encoding onto the entire dataset \nX_train_test = X_train_simple_imputed.append(X_test_simple_imputed,ignore_index = True)\n\n# save the index for test.csv \nX_test_csv_index = np.setdiff1d(X_train_test.index.to_list() ,X_train_csv_index) ","cdf0dcbd":"#%% MultiColumnLabelEncoder\n# Code snipet found on Stack Exchange \n# https:\/\/stackoverflow.com\/questions\/24458645\/label-encoding-across-multiple-columns-in-scikit-learn\n# from sklearn.preprocessing import LabelEncoder\n\nclass MultiColumnLabelEncoder:\n    def __init__(self,columns = None):\n        self.columns = columns # array of column names to encode\n\n    def fit(self,X,y=None):\n        return self # not relevant here\n\n    def transform(self,X):\n        '''\n        Transforms columns of X specified in self.columns using\n        LabelEncoder(). If no columns specified, transforms all\n        columns in X.\n        '''\n        output = X.copy()\n        if self.columns is not None:\n            for col in self.columns:\n                # convert float NaN --> string NaN\n                output[col] = output[col].fillna('NaN')\n                output[col] = LabelEncoder().fit_transform(output[col])\n        else:\n            for colname,col in output.iteritems():\n                output[colname] = LabelEncoder().fit_transform(col)\n        return output\n\n    def fit_transform(self,X,y=None):\n        return self.fit(X,y).transform(X)\n\n# store the catagorical features names as a list      \ncat_features = X_train_test.select_dtypes(['object']).columns.to_list()\n\n# use MultiColumnLabelEncoder to apply LabelEncoding on cat_features \n# uses NaN as a value , no imputation will be used for missing data\n\n# using One Hot Encoding for this version \n# X_train_test_encoded = MultiColumnLabelEncoder(columns = cat_features).fit_transform(X_train_test)","ad00aa83":"# cast categorical features to object dtype\ncat_features = ['Pclass','Sex','Cabin','Embarked']\nX_train_test[cat_features] = X_train_test[cat_features].astype(\"object\")\n# get_dummies to apply one-hot encoding to X_train_test,drop_first = True to avoid dummy varibale trap\nX_train_test_encoded = pd.get_dummies(X_train_test,drop_first=True)","a82f4099":"##% Split X_train_clean_encoded \nX_train_csv_encoded = X_train_test_encoded.iloc[X_train_csv_index, :]\nX_test_csv_encoded = X_train_test_encoded.iloc[X_test_csv_index, :].reset_index(drop = True) ","0953ca66":"##% Before and After LabelEncoding for train.csv \ndisplay(X_train_csv.head())\ndisplay(X_train_csv_encoded.head())","b1306de4":"##% Before and After LabelEncoding for test.csv \ndisplay(X_test_csv.head())\ndisplay(X_test_csv_encoded.head())\nX_test_csv_encoded.info()","2eca745d":"# Create test and train set 80-20\n#%%  train-test stratified split using a 80-20 split\nX_train, X_test, y_train, y_test = train_test_split(X_train_csv_encoded, y_train_csv, test_size=0.2, shuffle = True, stratify = y_train_csv, random_state=0)\n\nfor df in [X_train, X_test, y_train, y_test]:\n    df.reset_index(drop = True,inplace = True)\n    \nprint(\" Training Target\")\nprint(y_train.value_counts())\nprint(\"\\n\")\nprint(\" Test Target\")\nprint(y_test.value_counts())","168f8f47":"display(X_train)\ndisplay(X_test)","19c62b9d":"# https:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_roc.html#sphx-glr-auto-examples-model-selection-plot-roc-py\n\ndef plot_roc_curve(y_true,y_probas, title = 'ROC Curve for training data'):\n    # calculate roc curves\n    fpr, tpr, thresholds = roc_curve(y_true, y_probas)\n    # calculate the g-mean for each threshold\n    gmeans = np.sqrt(tpr * (1-fpr))\n    # locate the index of the largest g-mean\n    ix = np.argmax(gmeans)\n    print('Best Threshold=%f, G-Mean=%.3f' % (thresholds[ix], gmeans[ix]))\n    # plot the roc curve for the model\n    pyplot.figure(num=0, figsize=[6.4, 4.8])\n    pyplot.plot([0,1], [0,1], linestyle='--', label='No Skill')\n    pyplot.plot(fpr, tpr, marker='.', label='LightGBM')\n    pyplot.scatter(fpr[ix], tpr[ix], marker='o', color='black', label='Best')\n    # axis labels\n    pyplot.xlabel('False Positive Rate')\n    pyplot.ylabel('True Positive Rate')\n    plt.title(title)\n    pyplot.legend()\n    # show the plot\n    pyplot.show()\n    \n    return thresholds[ix]","021cac15":"# Great Function found on Kaggle for plotting a Confusion Matrix\n# https:\/\/www.kaggle.com\/grfiv4\/plot-a-confusion-matrix\ndef plot_confusion_matrix_kaggle(cm,\n                          target_names,\n                          title='Confusion matrix',\n                          cmap=None,\n                          normalize=True):\n    \"\"\"\n    given a sklearn confusion matrix (cm), make a nice plot\n\n    Arguments\n    ---------\n    cm:           confusion matrix from sklearn.metrics.confusion_matrix\n\n    target_names: given classification classes such as [0, 1, 2]\n                  the class names, for example: ['high', 'medium', 'low']\n\n    title:        the text to display at the top of the matrix\n\n    cmap:         the gradient of the values displayed from matplotlib.pyplot.cm\n                  see http:\/\/matplotlib.org\/examples\/color\/colormaps_reference.html\n                  plt.get_cmap('jet') or plt.cm.Blues\n\n    normalize:    If False, plot the raw numbers\n                  If True, plot the proportions\n\n    Usage\n    -----\n    plot_confusion_matrix(cm           = cm,                  # confusion matrix created by\n                                                              # sklearn.metrics.confusion_matrix\n                          normalize    = True,                # show proportions\n                          target_names = y_labels_vals,       # list of names of the classes\n                          title        = best_estimator_name) # title of graph\n\n    Citiation\n    ---------\n    http:\/\/scikit-learn.org\/stable\/auto_examples\/model_selection\/plot_confusion_matrix.html\n\n    \"\"\"\n    import matplotlib.pyplot as plt\n    import numpy as np\n    import itertools\n\n    accuracy = np.trace(cm) \/ float(np.sum(cm))\n    misclass = 1 - accuracy\n\n    if cmap is None:\n        cmap = plt.get_cmap('Blues')\n\n    plt.figure(figsize=(8, 6))\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n\n    if target_names is not None:\n        tick_marks = np.arange(len(target_names))\n        plt.xticks(tick_marks, target_names, rotation=45)\n        plt.yticks(tick_marks, target_names)\n\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n\n    thresh = cm.max() \/ 1.5 if normalize else cm.max() \/ 2\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        if normalize:\n            plt.text(j, i, \"{:0.4f}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n        else:\n            plt.text(j, i, \"{:,}\".format(cm[i, j]),\n                     horizontalalignment=\"center\",\n                     color=\"white\" if cm[i, j] > thresh else \"black\")\n\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label\\naccuracy={:0.4f}; misclass={:0.4f}'.format(accuracy, misclass))\n    plt.show()\n\n# binarize an array based of a threshold \ndef binarizeArray(array,threshold = 0.5):\n    return [0 if num < threshold else 1 for num in array]","1d10fd6e":"#% Initial Models\n# Create initial models\nLogReg = LogisticRegression(random_state=0).fit(X_train, y_train)\n\nXGBClass = xgb.XGBClassifier(eval_metric  = \"logloss\", max_depth=5, learning_rate=0.01, n_estimators=100, gamma=0, \n                        min_child_weight=1, subsample=0.8, colsample_bytree=0.8, reg_alpha=0.005,seed = 0).fit(X_train,y_train)\n\nRFClass = RandomForestClassifier(n_estimators = 50, max_depth = 50,n_jobs = -1, random_state = 0).fit(X_train,y_train)\n\nLGBMClass = LGBMClassifier(random_state=0).fit(X_train, y_train)","bf4314d0":"# initial Model Peformance on Training Data\nprint(\"             Model Peformance on Training Data\\n\")\n\npred_y = LogReg.predict(X_train)\nprint(\"                    Logistic Regression\")\nprint(classification_report(y_train,pred_y,digits=3))\n\npred_y = XGBClass.predict(X_train)\nprint(\"                    XGBoost Classifier\")\nprint(classification_report(y_train,pred_y,digits=3))\n\npred_y = RFClass.predict(X_train)\nprint(\"                    Random Forest Classifier\")\nprint(classification_report(y_train,pred_y,digits=3))\n\npred_y = LGBMClass.predict(X_train)\nprint(\"                    LightGBM Classifier\")\nprint(classification_report(y_train,pred_y,digits=3))","f92e9d53":"# initial Model Peformance on Testing Data\nprint(\"             Model Peformance on Testing Data\\n\")\n\npred_y = LogReg.predict(X_test)\nprint(\"                    Logistic Regression\")\nprint(classification_report(y_test,pred_y,digits=3))\n\npred_y = XGBClass.predict(X_test)\nprint(\"                    XGBoost Classifier\")\nprint(classification_report(y_test,pred_y,digits=3))\n\npred_y = RFClass.predict(X_test)\nprint(\"                    Random Forest Classifier\")\nprint(classification_report(y_test,pred_y,digits=3))\n\npred_y = LGBMClass.predict(X_test)\nprint(\"                    LightGBM Classifier\")\nprint(classification_report(y_test,pred_y,digits=3))","833b4366":"# https:\/\/github.com\/fmfn\/BayesianOptimization\n# https:\/\/github.com\/fmfn\/BayesianOptimization\/blob\/master\/bayes_opt\/bayesian_optimization.py\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.ensemble.RandomForestClassifier.html\n# https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.model_selection.cross_val_score.html\n# https:\/\/scikit-learn.org\/stable\/modules\/model_evaluation.html#scoring-parameter\n# https:\/\/tech.ovoenergy.com\/bayesian-optimisation\/\n# https:\/\/www.kdnuggets.com\/2019\/07\/xgboost-random-forest-bayesian-optimisation.html\n#crash\ndef search_best_param_rf(X, y):\n    def rf_cv(X, y, **kwargs):\n        estimator = RandomForestClassifier(**kwargs)\n        cval = cross_val_score(\n            estimator,\n            X,\n            y,\n            scoring=\"roc_auc\",\n            cv=5,\n            verbose=0,\n            n_jobs=-1,\n            error_score=0,\n        )\n        return cval.mean() # maximize roc_auc\n\n    def rf_crossval(n_estimators, max_depth, min_samples_split, min_samples_leaf):\n        return rf_cv(\n            X=X,\n            y=y,\n            n_estimators=int(n_estimators),\n            max_depth=int(max(max_depth, 1)),\n            min_samples_split=int(max(min_samples_split, 2)),\n            min_samples_leaf=int(max(min_samples_leaf, 1)),\n        )\n    \n    RFC_BO_params = {\n        \"n_estimators\": (10, 100),\n        \"max_depth\": (1, 100),\n        \"min_samples_split\": (2, 10),\n        \"min_samples_leaf\": (1, 5),\n    }\n\n    RFC_Bo = BayesianOptimization(rf_crossval, \n                                  RFC_BO_params, \n                                  random_state=0, \n                                  verbose=2\n                                 )\n    np.random.seed(1)\n    \n    RFC_Bo.maximize(init_points=4, n_iter=4) # 5 + 5 = 10 iterations \n    # n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n    # init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n    # more iterations more time spent searching \n    \n    params_set = RFC_Bo.max['params']\n    \n    params_set['n_estimators'] = int(round(params_set['n_estimators']))\n    params_set['max_depth'] = int(round(params_set['max_depth']))\n    params_set['min_samples_split'] = int(round(params_set['min_samples_split']))\n    params_set['min_samples_leaf'] = int(round(params_set['min_samples_leaf']))\n    \n    params_set.update({'n_jobs': -1})\n    params_set.update({'random_state': 0})\n    \n    return params_set","fdf8a621":"# Random Forest Cross Validation\n\ndef K_Fold_RandomForest(X_train,y_train, params_set = [], num_folds = 5):\n    model_num = 0 # model number \n    models = [] # model list\n    folds = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=0) # create folds\n\n        # num_folds times ; default is 5\n    for n_fold, (train_idx, valid_idx) in enumerate (folds.split(X_train, y_train)):\n        \n        print(f\"     model{model_num}\")\n        \n        train_X, train_y = X_train.iloc[train_idx], y_train.iloc[train_idx]\n        valid_X, valid_y = X_train.iloc[valid_idx], y_train.iloc[valid_idx]\n        \n        if (params_set == []): # if param_set is empty\n            # find best param_set in each fold, can lead to overfitting\n            params_set = search_best_param(train_X,train_y,cat_features) \n        \n        # fit RFC based of param_set and current fold\n        CV_RF = RandomForestClassifier(**params_set).fit(train_X, train_y)\n        \n        # append RF model to model list \n        models.append(CV_RF)\n        \n        # model metrics for current fold \n        print(\"Training Dataset\")\n        print(\"ROC_AUC_SCORE: \",roc_auc_score(y_train,models[model_num].predict_proba(X_train)[:,1]))\n        print(\"Test Dataset\")\n        print(\"ROC_AUC_SCORE: \",roc_auc_score(y_test,models[model_num].predict_proba(X_test)[:,1]))\n        print(\"\\n\")\n        \n        model_num = model_num + 1\n        \n    return models","f8601b2b":"best_params_rf_cv = search_best_param_rf(X_train_csv_encoded,y_train_csv)","8b094d12":"# Print best_params_rf_cv\nfor key, value in best_params_rf_cv.items():\n    print(key, ' : ', value)","815054c6":"rf_models = K_Fold_RandomForest(X_train_csv_encoded,y_train_csv,params_set = best_params_rf_cv,num_folds = 10)","98fbb9e9":"# Predict y_preds using models from RFC cross validation \ndef predict_models_RFC(models_cv,X):\n    y_preds = np.zeros(shape = X.shape[0])\n    for model in models_cv:\n        y_preds += model.predict_proba(X)[:,1]\n        \n    return y_preds\/len(models_cv)","91ae07cc":"# RFC ROC Curve\nRFC_pred_y = predict_models_RFC(rf_models,X_train_csv_encoded)\nRFC_threshold = plot_roc_curve(y_train_csv,RFC_pred_y)","8dbfeca1":"# RFC Confusion Matrix\nRFC_pred_y_bin = binarizeArray(RFC_pred_y,RFC_threshold)\n\ncm = confusion_matrix(y_train_csv,RFC_pred_y_bin)\nplot_confusion_matrix_kaggle(cm =cm, \n                      normalize    = False,\n                      target_names = ['Did Not Survived(0)', 'Survived(1)'],\n                      title        = \"Confusion Matrix\")\nprint(classification_report(y_train_csv,RFC_pred_y_bin))\nprint(\"Accuracy: %.2f%%\" % (accuracy_score(y_train_csv, RFC_pred_y_bin)*100.0))","ff06e434":"##% parameter tuning for lightgbm \n\n# store the catagorical features names as a list      \n# cat_features = X_train_test.select_dtypes(['object']).columns.to_list() # for label encoding\ncat_features = X_train_test.select_dtypes(['uint8']).columns.to_list() # for one hot encoding \n# print(cat_features)\n\n# Create the LightGBM data containers\n# Make sure that cat_features are used\ntrain_lgbdata=lgb.Dataset(X_train,label=y_train, categorical_feature = cat_features,free_raw_data=False)\ntest_lgbdata=lgb.Dataset(X_test,label=y_test, categorical_feature = cat_features,free_raw_data=False)","086b534b":"# https:\/\/github.com\/fmfn\/BayesianOptimization\n# https:\/\/testlightgbm.readthedocs.io\/en\/latest\/Parameters.html\ndef search_best_param(X,y,cat_features):\n    \n    trainXY = lgb.Dataset(data=X, label=y,categorical_feature = cat_features,free_raw_data=False)\n    # define the lightGBM cross validation\n    def lightGBM_CV(max_depth, num_leaves, n_estimators, learning_rate, subsample, colsample_bytree, \n                lambda_l1, lambda_l2, min_child_weight):\n    \n        params = {'boosting_type': 'gbdt', 'objective': 'binary', 'metric':'auc', 'verbose': -1,\n                  'early_stopping_round':100}\n        \n        params['max_depth'] = int(round(max_depth))\n        params[\"num_leaves\"] = int(round(num_leaves))\n        params[\"n_estimators\"] = int(round(n_estimators))\n        params['learning_rate'] = learning_rate\n        params['subsample'] = subsample\n        params['colsample_bytree'] = colsample_bytree\n        params['lambda_l1'] = max(lambda_l1, 0)\n        params['lambda_l2'] = max(lambda_l2, 0)\n        params['min_child_weight'] = min_child_weight\n    \n        score = lgb.cv(params, trainXY, nfold=5, seed=1, stratified=True, verbose_eval =False, metrics=['auc'])\n        return np.mean(score['auc-mean']) # maximize auc-mean\n\n    # use bayesian optimization to search for the best hyper-parameter combination\n    lightGBM_Bo = BayesianOptimization(lightGBM_CV, \n                                       {\n                                          'max_depth': (5, 50),\n                                          'num_leaves': (20, 100),\n                                          'n_estimators': (50, 500),\n                                          'learning_rate': (0.01, 0.3),\n                                          'subsample': (0.7, 0.8),\n                                          'colsample_bytree' :(0.5, 0.99),\n                                          'lambda_l1': (0, 5),\n                                          'lambda_l2': (0, 3),\n                                          'min_child_weight': (2, 50) \n                                      },\n                                       random_state = 1,\n                                       verbose = 2\n                                      )\n    np.random.seed(1)\n    \n    lightGBM_Bo.maximize(init_points= 5, n_iter=5) # 5 + 5, 10 iterations \n    # n_iter: How many steps of bayesian optimization you want to perform. The more steps the more likely to find a good maximum you are.\n    # init_points: How many steps of random exploration you want to perform. Random exploration can help by diversifying the exploration space.\n    # more iterations more time spent searching \n    \n    params_set = lightGBM_Bo.max['params']\n    \n    # get the params of the maximum target     \n    max_target = -np.inf\n    for i in lightGBM_Bo.res: # loop thru all the residuals \n        if i['target'] > max_target:\n            params_set = i['params']\n            max_target = i['target']\n    \n    params_set.update({'verbose': -1})\n    params_set.update({'metric': 'auc'})\n    params_set.update({'boosting_type': 'gbdt'})\n    params_set.update({'objective': 'binary'})\n    \n    params_set['max_depth'] = int(round(params_set['max_depth']))\n    params_set['num_leaves'] = int(round(params_set['num_leaves']))\n    params_set['n_estimators'] = int(round(params_set['n_estimators']))\n    params_set['seed'] = 1 #set seed\n    \n    return params_set","c481ae3b":"best_params = search_best_param(X_train,y_train,cat_features)","1d70b7bd":"# Print best_params\nfor key, value in best_params.items():\n    print(key, ' : ', value)","3129f2cc":"# Train lgbm_best using the best params found from Bayesian Optimization\nlgbm_best = lgb.train(best_params,\n                 train_lgbdata,\n                 num_boost_round = 100,\n                 valid_sets = test_lgbdata,\n                 early_stopping_rounds = 100,\n                 verbose_eval = 50\n                 )","e1ed4168":"print(\"     LGBM Tuned\")\nprint(\"Training Dataset\")\nprint(\"ROC_AUC_SCORE: \",roc_auc_score(y_train,lgbm_best.predict(X_train)))\nprint(\"Test Dataset\")\nprint(\"ROC_AUC_SCORE: \",roc_auc_score(y_test,lgbm_best.predict(X_test)))","da91efda":"##% Feature Importance \n# https:\/\/scikit-learn.org\/stable\/auto_examples\/ensemble\/plot_forest_importances.html\nlgb.plot_importance(lgbm_best,figsize=(25,20),max_num_features = 10)","bf912a8c":"##% Feature Importance using shap package \n# sample 10000 data points from X_train_test\nshap_values = shap.TreeExplainer(lgbm_best).shap_values(X_train_test_encoded.sample(n=10000))\nshap.summary_plot(shap_values, X_train_test_encoded.sample(n=10000))","f8841724":"# Cross Validation with LightGBM\n\ndef K_Fold_LightGBM(X_train, y_train , cat_features, num_folds = 5, params_set = []):\n    num = 0 # model number\n    models = [] # list of models \n    folds = StratifiedKFold(n_splits=num_folds, shuffle=True, random_state=0) # create folds \n\n        # num_folds times \n    for n_fold, (train_idx, valid_idx) in enumerate (folds.split(X_train, y_train)):\n        \n        print(f\"     model{num}\")\n        train_X, train_y = X_train.iloc[train_idx], y_train.iloc[train_idx]\n        valid_X, valid_y = X_train.iloc[valid_idx], y_train.iloc[valid_idx]\n        \n        train_data=lgb.Dataset(train_X,label=train_y, categorical_feature = cat_features,free_raw_data=False)\n        valid_data=lgb.Dataset(valid_X,label=valid_y, categorical_feature = cat_features,free_raw_data=False)\n        \n        \n        # params_set = search_best_param(train_X,train_y,cat_features) # find best param_set in each fold\n        \n        CV_LGBM = lgb.train(params_set,\n                            train_data,\n                            num_boost_round = 100,\n                            valid_sets = valid_data,\n                            early_stopping_rounds = 100,\n                            verbose_eval = 50\n                           )\n        # increase early_stopping_rounds can lead to overfitting \n        \n        # append LGBM model to models list \n        models.append(CV_LGBM)\n        \n        # model metrics for each fold \n        print(\"Training Dataset\")\n        print(\"ROC_AUC_SCORE: \",roc_auc_score(y_train,models[num].predict(X_train)))\n        print(\"Test Dataset\")\n        print(\"ROC_AUC_SCORE: \",roc_auc_score(y_test,models[num].predict(X_test)))\n        print(\"\\n\")\n        \n        num = num + 1\n        \n    return models","5e06c664":"best_params_cv = search_best_param(X_train_csv_encoded,y_train_csv,cat_features)","f8eacf86":"lgbm_models = K_Fold_LightGBM(X_train_csv_encoded,y_train_csv,cat_features,10,params_set = best_params_cv)","9d29d8e3":"# Predict y_prds using models from cross validation \ndef predict_models_LGBM(models_cv,X):\n    y_preds = np.zeros(shape = X.shape[0])\n    for model in models_cv:\n        y_preds += model.predict(X)\n        \n    return y_preds\/len(models_cv)","2f9a3c32":"# LightGBM ROC Curve\nLGBM_pred_y = predict_models_LGBM(lgbm_models,X_train_csv_encoded)\nLGBM_threshold = plot_roc_curve(y_train_csv,LGBM_pred_y)","a2d86906":"# LightGBM Confusion Matrix\nLGBM_pred_y_bin = binarizeArray(LGBM_pred_y,LGBM_threshold)\n\ncm = confusion_matrix(y_train_csv,LGBM_pred_y_bin)\nplot_confusion_matrix_kaggle(cm =cm, \n                      normalize    = False,\n                      target_names = ['Did Not Survived(0)', 'Survived(1)'],\n                      title        = \"Confusion Matrix\")\nprint(classification_report(y_train_csv,LGBM_pred_y_bin))\nprint(\"Accuracy: %.2f%%\" % (accuracy_score(y_train_csv, LGBM_pred_y_bin)*100.0))","cd2f67f3":"# Prediction for Test.csv using LightGBM CV \npredictLGBM = predict_models_LGBM(lgbm_models,X_test_csv_encoded) \npredictLGBM_bin = binarizeArray(predictLGBM,LGBM_threshold)\n\nsubmissionLGBM = pd.DataFrame({'PassengerId':test_csv_id,'Survived':predictLGBM_bin})\n\ndisplay(submissionLGBM.head())\n\n# Prediction for Test.csv using RFC CV \npredictRFC = predict_models_RFC(rf_models,X_test_csv_encoded) \npredictRFC_bin = binarizeArray(predictRFC,RFC_threshold)\n\nsubmissionRFC = pd.DataFrame({'PassengerId':test_csv_id,'Survived':predictRFC_bin})\n\ndisplay(submissionRFC.head())","8ebbe72c":"#% Submit Predictions \nsubmissionLGBM.to_csv('submissionCV_LGBM3.csv',index=False)\nsubmissionRFC.to_csv('submissionCV_RFC3.csv',index=False)","89e480af":"<a id=\"Imputation\"><\/a>\n## Imputation\n\nThe features Age, Fare, and Embarked need to be imputed before bulding a model. I did not want to use NaN as a value as there weren't that many missing values. On the other hand, Cabin has many missing values so we should treat NaN as a categorical value. I used median and mode imputation for numeric and categorical features respectively. In addition, imputation must be done seperately btwn train.csv and test.csv as model building is strictly on train.csv and model performance is on test.csv ","5ea87428":"Categorical features Name, Cabin, and Ticket have a high number of unique value which makes it difficult to extract meaningful information from these features. Although the Cabin feature has high missing values and unique values, a closer look can be seen that the feature can be split up into the letter category (A - G). The letter represents the floor the passenger cabin was on. A diagram of the actual titanic is shown below to visualize this. ","0ed7f4cb":"<a id=\"Target\"><\/a>\n## Target","eb508bf8":"**Conclusion**\n* LightGBM is a great ML algorithm that handles categorical features and missing values \n* Cross Validation is useful to combat overfitting \n* Bayesian Optimization is necessary to get hyper parameters when building an initial model\n* This is a great dataset to work on and lots of knowledge can be gain from withing with this dataset \n* Researching and reading other Kaggle notebooks is essential for becoming a better data scientist\n\n**Challenges**\n* Difficult to utilize all features, feature engineering might be useful to research\n* Overfitting might have occurred which reduces the model performance on the test set\n\n**More Notebooks** \n\n**Regression Notebooks**   \n[https:\/\/www.kaggle.com\/josephchan524\/housepricesregressor-using-lightgbm](https:\/\/www.kaggle.com\/josephchan524\/housepricesregressor-using-lightgbm)\n\n[https:\/\/www.kaggle.com\/josephchan524\/tabularplaygroundregressor-using-lightgbm-feb2021](https:\/\/www.kaggle.com\/josephchan524\/tabularplaygroundregressor-using-lightgbm-feb2021)\n\n[https:\/\/www.kaggle.com\/josephchan524\/studentperformanceregressor-rmse-12-26-r2-0-26](https:\/\/www.kaggle.com\/josephchan524\/studentperformanceregressor-rmse-12-26-r2-0-26)\n\n\n**Classification Notebooks**  \n[https:\/\/www.kaggle.com\/josephchan524\/bankruptcyclassifier-using-lightgbm-91-recall](https:\/\/www.kaggle.com\/josephchan524\/bankruptcyclassifier-using-lightgbm-91-recall)  \n\n[https:\/\/www.kaggle.com\/josephchan524\/hranalytics-lightgbm-classifier-auc-80](https:\/\/www.kaggle.com\/josephchan524\/hranalytics-lightgbm-classifier-auc-80)\n\n[https:\/\/www.kaggle.com\/josephchan524\/bankchurnersclassifier-recall-97-accuracy-95](https:\/\/www.kaggle.com\/josephchan524\/bankchurnersclassifier-recall-97-accuracy-95)\n\n[https:\/\/www.kaggle.com\/josephchan524\/tabularplaygroundclassifier-using-lightgbm-mar2021](https:\/\/www.kaggle.com\/josephchan524\/tabularplaygroundclassifier-using-lightgbm-mar2021)  \n\n\n04-06-2020\nJoseph Chan ","5d0c8bd7":"<a id=\"One-Hot-Encoding\"><\/a>\n## One Hot Encoding\n\nSince we only have 8 features, 4 which are categorical: Pclass, Sex, Cabin, Embarked; I believe using one hot encoding might improve the model.","ada86759":"<a id=\"Random-Forest-Cross-Validation\"><\/a>\n## Random Forest Cross Validation ","5599d213":"<a id=\"Bayesian-Optimization\"><\/a>\n## Bayesian Optimization","d56fd529":"<a id=\"Feature-Importance \"><\/a>\n## Feature Importance ","662bd474":"<a id=\"LightGBM-CV-Model-Peformance \"><\/a>\n## LightGBM CV Model Peformance ","2374549d":"<a id=\"Random-Forest-CV-Model-Peformance\"><\/a>\n## Random Forest CV Model Peformance ","ce1704c2":"<a id=\"Cross-Validation \"><\/a>\n## Cross Validation ","e6c69a42":"<img src= \"https:\/\/upload.wikimedia.org\/wikipedia\/commons\/thumb\/0\/0d\/Olympic_%26_Titanic_cutaway_diagram.png\/1200px-Olympic_%26_Titanic_cutaway_diagram.png\" height=\"200\" align=\"center\"\/>\n","05b4d7a9":"<a id=\"Notes\"><\/a>\n## Notes\n\nTrain.csv and Test.csv have  missing values so imputation might be needed. Since there aren't many features in this dataset a quick explanatory data analysis can be done on the features and target.","bec763eb":"<a id=\"Continuous-Features\"><\/a>\n## Continuous Features","94974af8":"<a id=\"Task-Details\"><\/a>\n# Task Detail \n\n## Goal\nYour task is to predict whether or not a passenger survived the sinking of the Synthanic (a synthetic, much larger dataset based on the actual Titanic dataset). For each PasengerId row in the test set, you must predict a 0 or 1 value for the Survived target.\n\n## Metric\nYour score is the percentage of passengers you correctly predict. This is known as **[accuracy](http:\/\/https:\/\/en.wikipedia.org\/wiki\/Accuracy_and_precision#In_binary_classification)**.  \n\nAccuracy = (TP + TN)\/(TP + TN + FP + FN)","b2a74c82":"<a id=\"Read-in-Data\"><\/a>\n# Read in Data","7c11b4b6":"The target variable, **Survived** has about a 40 - 60 split which means imbalanced classfication techniques are not needed. ","b7b3ee5a":"<a id=\"ROC-Curve\"><\/a>\n## ROC Curve","6c35088c":"<a id=\"Tuning-LightGBM\"><\/a>\n## Tuning LightGBM","53544f95":"<a id=\"Test.csv\"><\/a>\n## Test.csv","c16d2444":"<a id=\"Confusion-Matrix\"><\/a>\n## Confusion Matrix","3c284af6":"<a id=\"Model-Performance-Metrics\"><\/a>\n# Model Performance Metrics\n\nI created several functions to help evaluate the performance of a model. Some functions help binarize an array, plot the confusion matrix, and plot the ROC curve.","1a486560":"<a id=\"LightGBM-Model-Peformance \"><\/a>\n## LightGBM Model Peformance ","55df7c7d":"<a id=\"Prediction-for-Test.csv\"><\/a>\n# Prediction for Test.csv","bcf09f74":"<a id=\"Introduction\"><\/a>\n# Introduction\nThis is my fourth competition notebook on Kaggle. I hope to learn more about working with tabular data and I hope anyone who reads this learns more as well! This notebook will be working with a classfication task. If you have any questions or comments please leave below! ","580bea74":"<a id=\"Initial Models\"><\/a>\n# Initial Models\nI applied different machine learning algorthims to test which model perform better on this dataset. I've listed below various machine learning techniques applied in this section.\n\n1. Logistic Regression\n2. XGBoost Classifier\n3. Random Forest Classifier\n5. LightGBM Classifier","66c70cbf":"<a id=\"LightGBM-Classifier\"><\/a>\n# LightGBM Classifier","08a38b2c":"<a id=\"Table-Of-Contents\"><\/a>\n# Table Of Contents\n* [Table Of Contents](#Table-Of-Contents)\n* [Introduction](#Introduction)\n* [Importing Libraries](#Importing-Libraries)\n* [Task Details](#Task-Details)\n* [Read in Data](#Read-in-Data)\n    - [Train.csv](#Train.csv)\n    - [Test.csv](#Test.csv)\n    - [Notes](#Notes)\n* [Data Visualization](#Data-Visualization)\n    - [Data Dictionary](#Data-Dictionary)\n    - [Variable Notes](#Variable-Notes)\n    - [Categorical Features](#Categorical-Features)\n    - [Continuous Features](#Continuous-Features)\n    - [Target](#Target)\n* [Preprocessing Data](#Preprocessing-Data)\n    - [Label Encoding](#Label-Encoding)\n    - [One Hot Encoding](#One-Hot-Encoding)\n    - [Imputation](#Imputation)\n    - [Train-Test Stratified Split](#Train-Test-Stratified-Split)\n* [Model Performance Metrics](#Model-Performance-Metrics)\n    - [ROC Curve](#ROC-Curve)\n    - [Confusion Matrix](#Confusion-Matrix)\n* [Initial Models](#Initial-Models)\n* [Random Forest Classifier](#Random-Forest-Classifier)\n    - [Random Forest Bayesian Optimization](#Random-Forest-Bayesian-Optimization)\n    - [Random Forest Cross Validation](#Random-Forest-Cross-Validation)\n    - [Random Forest CV Model Peformance](#Random-Forest-CV-Model-Peformance)\n* [LightGBM Classifier](#LightGBM-Classifier)\n    - [Bayesian Optimization](#Bayesian-Optimization)\n    - [Tuning LightGBM](#Tuning-LightGBM)\n    - [Feature Importance](#Feature-Importance)\n    - [Cross Validation](#Cross-Validation)\n    - [LightGBM CV Model Peformance](#LightGBM-CV-Model-Peformance)\n* [Prediction for Test.csv](#Prediction-for-Test.csv)\n* [Conclusion](#Conclusion)","1bc3d85f":"<a id=\"Train.csv\"><\/a>\n## Train.csv","ed0dd3ec":"# <center>TabularPlaygroundClassifier April2021<\/center>\n<img src= \"https:\/\/www.pixelstalk.net\/wp-content\/uploads\/images1\/Titanic-Wallpapers-HD-Free-download.jpg\" height=\"200\" align=\"center\"\/>\n","7d73b587":"<a id=\"Importing-Libraries\"><\/a>\n# Importing Libraries","b183907c":"<a id=\"Random-Forest-Classifier\"><\/a>\n# Random Forest Classifier","28db49f4":"<a id=\"Label-Encoding\"><\/a>\n## Label Encoding","f43e19c8":"## Data Dictionary\n| Variable | Definition                                 | Key                                            |\n|----------|--------------------------------------------|------------------------------------------------|\n| Survived | Survival                                   | 0 = No, 1 = Yes                                |\n| Pclass   | Ticket class                               | 1 = 1st, 2 = 2nd, 3 = 3rd                      |\n| Sex      | Sex                                        |                                                |\n| Age      | Age in years                               |                                                |\n| SibSp    | # of siblings \/ spouses aboard the Titanic |                                                |\n| Parch    | # of parents \/ children aboard the Titanic |                                                |\n| Ticket   | Ticket number                              |                                                |\n| Fare     | Passenger fare                             |                                                |\n| Cabin    | Cabin number                               |                                                |\n| Embarked | Port of Embarkation                        | C = Cherbourg, Q = Queenstown, S = Southampton |","7f889c9f":"<a id=\"Conclusion\"><\/a>\n# Conclusion","83e2c756":"<a id=\"Preprocessing-Data\"><\/a>\n# Preprocessing Data\nBecause Train.csv and Test.csv have missing data imputation is needed.  \n\nLabel encoding is also still require as this dataset has categorical features. ","cbea1a26":"<a id=\"Random-Forest-Bayesian-Optimization\"><\/a>\n## Random Forest Bayesian Optimization","feed995a":"<a id=\"#Train-Test-Stratified-Split\"><\/a>\n## Train-Test Stratified Split","05e6de3e":"## Variable Notes\npclass: A proxy for socio-economic status (SES)\n1st = Upper\n2nd = Middle\n3rd = Lower\n\nage: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\nsibsp: The dataset defines family relations in this way...\nSibling = brother, sister, stepbrother, stepsister\nSpouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\nparch: The dataset defines family relations in this way...\nParent = mother, father\nChild = daughter, son, stepdaughter, stepson\nSome children travelled only with a nanny, therefore parch=0 for them.","c30451b2":"<a id=\"Data-Visualization\"><\/a>\n# Data Visualization "}}