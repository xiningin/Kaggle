{"cell_type":{"fc8e7145":"code","d114b7a4":"code","8267a64f":"code","119aacb1":"code","e30ee1de":"code","5d2d80f9":"code","f1fe728c":"code","ee595ae1":"code","418001a1":"code","c122ca6e":"code","6c246fdf":"code","74e4cee7":"code","8f376518":"code","97481de5":"code","52e751c3":"code","2fa49ba8":"code","1cb047f9":"code","0098fb98":"code","dc242c68":"code","142e4674":"code","ffd6c8d8":"code","c4c91cd5":"code","4166d8f9":"code","a1c05f8b":"code","53071240":"code","87e63985":"code","94fe2416":"code","8cc987eb":"code","92f9a801":"code","d4361f97":"code","724266cb":"code","c4e642b5":"code","4f9d3307":"code","e4a83f24":"code","1d0e4c79":"code","cdb705b7":"code","32f4300f":"code","5e72d07a":"code","4b24131f":"code","79a4f660":"code","399a0bab":"code","a6aee57e":"code","86b22232":"code","735c5650":"code","277f9c6e":"code","cfe2e669":"code","218e2800":"code","eab152b2":"code","d4f86da7":"code","e3b25613":"code","72bd657b":"markdown","3d74921a":"markdown","8cb7e33e":"markdown","8791b0d2":"markdown"},"source":{"fc8e7145":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d114b7a4":"import matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as sc\nfig,ax = plt.figsize=(12,8)","8267a64f":"train_df=pd.read_csv('..\/input\/train_V2.csv')\nprint('training data is loaded...')\ntest_df=pd.read_csv('..\/input\/test_V2.csv')\nprint('testing data is loaded...')","119aacb1":"train_df.head()","e30ee1de":"train_df.dtypes","5d2d80f9":"null_cnt = train_df.isnull().sum().sort_values()\nprint('null count:', null_cnt[null_cnt>0])\n# dropna\ntrain_df.dropna(inplace=True)","f1fe728c":"def reduce_mem_usage(props):\n    start_mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage of properties dataframe is :\",start_mem_usg,\" MB\")\n    NAlist = [] # Keeps track of columns that have missing values filled in. \n    for col in props.columns:\n        if props[col].dtype != object:  # Exclude strings\n            \n            # Print current column type\n            print(\"******************************\")\n            print(\"Column: \",col)\n            print(\"dtype before: \",props[col].dtype)\n            \n            # make variables for Int, max and min\n            IsInt = False\n            mx = props[col].max()\n            mn = props[col].min()\n            \n            # Integer does not support NA, therefore, NA needs to be filled\n            if not np.isfinite(props[col]).all(): \n                NAlist.append(col)\n                props[col].fillna(mn-1,inplace=True)  \n                   \n            # test if column can be converted to an integer\n            asint = props[col].fillna(0).astype(np.int64)\n            result = (props[col] - asint)\n            result = result.sum()\n            if result > -0.01 and result < 0.01:\n                IsInt = True\n\n            \n            # Make Integer\/unsigned Integer datatypes\n            if IsInt:\n                if mn >= 0:\n                    if mx < 255:\n                        props[col] = props[col].astype(np.uint8)\n                    elif mx < 65535:\n                        props[col] = props[col].astype(np.uint16)\n                    elif mx < 4294967295:\n                        props[col] = props[col].astype(np.uint32)\n                    else:\n                        props[col] = props[col].astype(np.uint64)\n                else:\n                    if mn > np.iinfo(np.int8).min and mx < np.iinfo(np.int8).max:\n                        props[col] = props[col].astype(np.int8)\n                    elif mn > np.iinfo(np.int16).min and mx < np.iinfo(np.int16).max:\n                        props[col] = props[col].astype(np.int16)\n                    elif mn > np.iinfo(np.int32).min and mx < np.iinfo(np.int32).max:\n                        props[col] = props[col].astype(np.int32)\n                    elif mn > np.iinfo(np.int64).min and mx < np.iinfo(np.int64).max:\n                        props[col] = props[col].astype(np.int64)    \n            \n            # Make float datatypes 32 bit\n            else:\n                props[col] = props[col].astype(np.float32)\n            \n            # Print new column type\n            print(\"dtype after: \",props[col].dtype)\n            print(\"******************************\")\n    \n    # Print final result\n    print(\"___MEMORY USAGE AFTER COMPLETION:___\")\n    mem_usg = props.memory_usage().sum() \/ 1024**2 \n    print(\"Memory usage is: \",mem_usg,\" MB\")\n    print(\"This is \",100*mem_usg\/start_mem_usg,\"% of the initial size\")\n    return props, NAlist","ee595ae1":"props, NAlist = reduce_mem_usage(train_df)\nprint(\"_________________\")\nprint(\"\")\nprint(\"Warning: the following columns have missing values filled with 'df['column_name'].min() -1': \")\nprint(\"_________________\")\nprint(\"\")\nprint(NAlist)","418001a1":"train_df.nunique()","c122ca6e":"sns.set(style=\"darkgrid\")\nax = sns.barplot(x=\"assists\", y=\"assists\", data=train_df, estimator=lambda x: len(x) \/ len(train_df) * 100)\nax.set(ylabel=\"Percent\")","6c246fdf":"#assists vs win percentage\nsns.jointplot(x=\"winPlacePerc\", y=\"kills\", data=train_df, height=10, ratio=3, color=\"r\")\nplt.show()","74e4cee7":"#damage delt vs winning percentage\nsns.scatterplot(x=\"winPlacePerc\", y=\"kills\", data=train_df)\nplt.show()","8f376518":"#Damage dealt by zero kill players\ndata = train_df.copy()\ndata = data[data['kills']==0]\nplt.figure(figsize=(15,10))\nplt.title(\"Damage Dealt by 0 killers\",fontsize=15)\nsns.distplot(data['damageDealt'])\nplt.show()","97481de5":"#let us investigate further \nprint(\"There are {} players won the match without single win\".format(len(data[data['winPlacePerc']==0])))\nprint(\"There are {} players won the match without single Damage\".format(len(data[data['damageDealt']==0])))","52e751c3":"#Ride Distance \nplt.figure(figsize=(15,10))\nsns.distplot(data['rideDistance'])\nplt.show()","2fa49ba8":"print(\"On average the persons runs {}m, while the Maxium distance rided is {}m and 90% of the people rided distance is {}m\".format(data['rideDistance'].mean(),data['rideDistance'].max(),data['rideDistance'].quantile(0.9)))","1cb047f9":"plt.figure(figsize=(15,10))\nsns.distplot(data['walkDistance'])\nplt.show()","0098fb98":"print(\"On average the persons walks {}m, while the Maxium distance walked is {}m and 90% of the people walks distance is {}m\".format(data['walkDistance'].mean(),data['walkDistance'].max(),data['walkDistance'].quantile(0.9)))","dc242c68":"#vehicle destoys vs winnnig percentage\nplt.figure(figsize=(15,10))\nsns.pointplot(x='vehicleDestroys',y='winPlacePerc',data=train_df)\nplt.title('vehicleDestroys Vs Winning %')\nplt.show()","142e4674":"data = data[data['heals'] < data['heals'].quantile(0.99)]\ndata","ffd6c8d8":"#match duration vs winning percentage\nplt.figure(figsize=(20,10))\nsns.distplot(train_df['matchDuration'])\nplt.title('Duration vs Winnign percentage')\nplt.show()","c4c91cd5":"#Match Type vs winning percentage\n# plt.figure(figsize=(20,10))\n# sns.catplot(x='matchType',y='winPlacePerc',kind='bar',data=train_df)\n# plt.title('Match Type vs Winning perc')\n# plt.show()","4166d8f9":"#Swimming vs the win distribution\nprint(\"The average person swims for {:.1f}m, 99% of people have swimemd {}m or less, while the olympic champion swimmed for {}m.\".format(train_df['swimDistance'].mean(), train_df['swimDistance'].quantile(0.99), train_df['swimDistance'].max()))","a1c05f8b":"data = train_df[train_df['swimDistance'] < train_df['swimDistance'].quantile(0.95)]\nplt.figure(figsize=(15,10))\nplt.title(\"Swim Distance Distribution\",fontsize=15)\nsns.distplot(data['swimDistance'])\nplt.show()","53071240":"swim=train_df.copy()\nswim['swimDistance']=pd.cut(swim['swimDistance'],[-1, 0, 5, 20, 5286], labels=['0m','1-5m', '6-20m', '20m+'])\nplt.figure(figsize=(15,8))\nsns.boxplot(x=\"swimDistance\", y=\"winPlacePerc\", data=swim)\nplt.show()","87e63985":"print(\"The average person uses {:.1f} heal items, 99% of people use {} or less, while the maximum % used is {}.\".format(train_df['heals'].mean(), train_df['heals'].quantile(0.99), train_df['heals'].max()))\nprint(\"The average person uses {:.1f} boost items, 99% of people use {} or less, while the maximum % used is {}.\".format(train_df['boosts'].mean(), train_df['boosts'].quantile(0.99), train_df['boosts'].max()))","94fe2416":"#pointplot to determin heals vs boosts vs winplacePerc\ndata = train_df.copy()\ndata = data[data['heals'] < data['heals'].quantile(0.99)]\ndata = data[data['boosts'] < data['boosts'].quantile(0.99)]\n\nf,ax1 = plt.subplots(figsize =(20,10))\nsns.pointplot(x='heals',y='winPlacePerc',data=data,color='red',alpha=0.8)\nsns.pointplot(x='boosts',y='winPlacePerc',data=data,color='green',alpha=0.8)\nplt.text(4,0.6,'Heals',color='red',fontsize = 17,style = 'italic')\nplt.text(4,0.55,'Boosts',color='green',fontsize = 17,style = 'italic')\nplt.xlabel('Number of heal\/boost items',fontsize = 15,color='blue')\nplt.ylabel('Win Percentage',fontsize = 15,color='blue')\nplt.title('Heals vs Boosts',fontsize = 20,color='blue')\nplt.legend(loc='best')\nplt.grid()\nplt.show()","8cc987eb":"k = 5 #number of variables for heatmap\nf,ax = plt.subplots(figsize=(11, 11))\ncols = train_df.corr().nlargest(k, 'winPlacePerc')['winPlacePerc'].index\ncm = np.corrcoef(train_df[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","92f9a801":"# Create headshot_rate feature\ntrain_df['headshot_rate'] = train_df['headshotKills'] \/ train_df['kills']\ntrain_df['headshot_rate'] = train_df['headshot_rate'].fillna(0)","d4361f97":"#Total Distance covered\ntrain_df['totalDistance'] = train_df['rideDistance'] + train_df['walkDistance'] + train_df['swimDistance']","724266cb":"# Create feature killsWithoutMoving\ntrain_df['killsWithoutMoving'] = ((train_df['kills'] > 0) & (train_df['totalDistance'] == 0))","c4e642b5":"#Encoding matchType to categorical data type\n# One hot encode matchType\ntrain_df = pd.get_dummies(train_df, columns=['matchType'])\n\n# Take a look at the encoding\nmatchType_encoding = train_df.filter(regex='matchType')\nmatchType_encoding.head()","4f9d3307":"# Turn groupId and match Id into categorical types\ntrain_df['groupId'] = train_df['groupId'].astype('category')\ntrain_df['matchId'] = train_df['matchId'].astype('category')\n\n# Get category coding for groupId and matchID\ntrain_df['groupId_cat'] = train_df['groupId'].cat.codes\ntrain_df['matchId_cat'] = train_df['matchId'].cat.codes\n\n# Get rid of old columns\ntrain_df.drop(columns=['groupId', 'matchId'], inplace=True)\n\n# Lets take a look at our newly created features\ntrain_df[['groupId_cat', 'matchId_cat']].head()","e4a83f24":"train_df.head()","1d0e4c79":"train_df.drop(columns = ['Id'], inplace=True)","cdb705b7":"#Using the subset of data for the splitting purpose\nsample = 500000\ndf_sample = train_df.sample(sample)","32f4300f":"# Split sample into training data and target variable\ndf = df_sample.drop(columns = ['winPlacePerc']) #all columns except target\ny = df_sample['winPlacePerc'] # Only target variable","5e72d07a":"# Function for splitting training and validation data\ndef split_vals(a, n : int): \n    return a[:n].copy(), a[n:].copy()\nval_perc = 0.12 # % to use for validation set\nn_valid = int(val_perc * sample) \nn_trn = len(df)-n_valid\n# Split data\nraw_train, raw_valid = split_vals(df_sample, n_trn)\nX_train, X_valid = split_vals(df, n_trn)\ny_train, y_valid = split_vals(y, n_trn)\n\n# Check dimensions of samples\nprint('Sample train shape: ', X_train.shape, \n      'Sample target shape: ', y_train.shape, \n      'Sample validation shape: ', X_valid.shape)","4b24131f":"# Metric used for the PUBG competition (Mean Absolute Error (MAE))\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\n# Function to print the MAE (Mean Absolute Error) score\n# This is the metric used by Kaggle in this competition\ndef print_score(m : RandomForestRegressor):\n    res = ['mae train: ', mean_absolute_error(m.predict(X_train), y_train), \n           'mae val: ', mean_absolute_error(m.predict(X_valid), y_valid)]\n    if hasattr(m, 'oob_score_'): res.append(m.oob_score_)\n    print(res)","79a4f660":"m1 = RandomForestRegressor(n_estimators=40, min_samples_leaf=3, max_features='sqrt',\n                          n_jobs=-1)\nm1.fit(X_train, y_train)\nprint_score(m1)","399a0bab":"def rf_feat_importance(m, df):\n    return pd.DataFrame({'cols':df.columns, 'imp':m.feature_importances_}\n                       ).sort_values('imp', ascending=False)","a6aee57e":"fi = rf_feat_importance(m1, df); fi[:10]","86b22232":"# Plot a feature importance graph for the 20 most important features\nplot1 = fi[:20].plot('cols', 'imp', figsize=(14,6), legend=False, kind = 'barh')\nplot1","735c5650":"to_keep = fi[fi.imp>0.005].cols\nprint('Significant features: ', len(to_keep))\nto_keep","277f9c6e":"# Make a DataFrame with only significant features\ndf_keep = df[to_keep].copy()\nX_train, X_valid = split_vals(df_keep, n_trn)","cfe2e669":"m2 = RandomForestRegressor(n_estimators=80, min_samples_leaf=3, max_features='sqrt',\n                          n_jobs=-1)\nm2.fit(X_train, y_train)\nprint_score(m2)","218e2800":"fi_to_keep = rf_feat_importance(m2, df_keep)\nplot2 = fi_to_keep.plot('cols', 'imp', figsize=(14,6), legend=False, kind = 'barh')\nplot2","eab152b2":"# Correlation heatmap\ncorr = df_keep.corr()\n\n# Set up the matplotlib figure\nf, ax = plt.subplots(figsize=(15, 11))\n\n# Create heatmap\nheatmap = sns.heatmap(corr,annot=True)","d4f86da7":"# Add engineered features to the test_df set\ntest_df['headshot_rate'] = test_df['headshotKills'] \/ test_df['kills']\ntest_df['headshot_rate'] = test_df['headshot_rate'].fillna(0)\ntest_df['totalDistance'] = test_df['rideDistance'] + test_df['walkDistance'] + test_df['swimDistance']\ntest_df['playersJoined'] = test_df.groupby('matchId')['matchId'].transform('count')\ntest_df['healsandboosts'] = test_df['heals'] + test_df['boosts']\ntest_df['killsWithoutMoving'] = ((test_df['kills'] > 0) & (test_df['totalDistance'] == 0))\n\n# Turn groupId and match Id into categorical types\ntest_df['groupId'] = test_df['groupId'].astype('category')\ntest_df['matchId'] = test_df['matchId'].astype('category')\n\n# Get category coding for groupId and matchID\ntest_df['groupId_cat'] = test_df['groupId'].cat.codes\ntest_df['matchId_cat'] = test_df['matchId'].cat.codes\n\n# Remove irrelevant features from the test_df set\ntest_df_pred = test_df[to_keep].copy()\n\n# Fill NaN with 0 (temporary)\ntest_df_pred.fillna(0, inplace=True)\ntest_df_pred.head()","e3b25613":"# Make submission ready for Kaggle\n# We use our final Random Forest model (m3) to get the predictions\npredictions = np.clip(a = m2.predict(test_df_pred), a_min = 0.0, a_max = 1.0)\npred_df = pd.DataFrame({'Id' : test_df['Id'], 'winPlacePerc' : predictions})\n\n# Create submission file\npred_df.to_csv(\"submission.csv\", index=False)","72bd657b":"**assists - Number of enemy players this player damaged that were killed by teammates.**","3d74921a":"**Feature Engineering**\n","8cb7e33e":"* **groupId -** Integer ID to identify a group within a match. If the same group of players plays in different matches, they will have a different groupId each time.\n* **matchId** - Integer ID to identify match. There are no matches that are in both the training and testing set.\n* **assists** - Number of enemy players this player damaged that were killed by teammates.\n* **boosts** - Number of boost items used.\n* **damageDealt** - Total damage dealt. Note: Self inflicted damage is subtracted.\n* **DBNOs** - Number of enemy players knocked.\n* **headshotKills** - Number of enemy players killed with headshots.\n* **heals** - Number of healing items used.\n* **killPlace** - Ranking in match of number of enemy players killed.\n* **killPoints** - Kills-based external ranking of player. (Think of this as an Elo ranking where only kills matter.)\n* **kills** - Number of enemy players killed.\n* **killStreaks** - Max number of enemy players killed in a short amount of time.\n* **longestKill** - Longest distance between player and player killed at time of death. This may be misleading, as downing a - player and driving away may lead to a large longestKill stat.\n* **maxPlace** - Worst placement we have data for in the match. This may not match with numGroups, as sometimes the data skips over placements.\n* **numGroups** - Number of groups we have data for in the match.\n* **revives** - Number of times this player revived teammates.\n* **rideDistance** - Total distance traveled in vehicles measured in meters.\n* **roadKills** - Number of kills while in a vehicle.\n* **swimDistance** - Total distance traveled by swimming measured in meters.\n* **teamKills** - Number of times this player killed a teammate.\n* **vehicleDestroys** - Number of vehicles destroyed.\n* **walkDistance** - Total distance traveled on foot measured in meters.\n* **weaponsAcquired** - Number of weapons picked up.\n* **winPoints** - Win-based external ranking of player. (Think of this as an Elo ranking where only winning matters.)\n* **winPlacePerc** - The target of prediction. This is a percentile winning placement, where 1 corresponds to 1st place, and 0 corresponds to last place in the match. It is calculated off of maxPlace, not numGroups, so it is possible to have missing chunks in a match.****","8791b0d2":"we can observe that most 80% of the teammates hasn't killed even single shot and 10% had single shot and 3% had 2 shots"}}