{"cell_type":{"9d2332a3":"code","485d5e9d":"code","e2e33659":"code","e410f91a":"code","9b65b865":"code","0ae29e04":"code","31faf3cb":"code","ec6403ee":"code","3f740eb4":"code","db820bf5":"code","09d52cc2":"code","2009debb":"code","f422baf8":"code","3e1d576c":"code","f8e797fc":"code","8a3c6d7e":"code","2421382a":"code","4bccb3ca":"code","ee34c832":"code","ee02d5fc":"code","b20ce90d":"code","3e6080aa":"code","f25aa9d2":"code","fc5dccc0":"code","4a193142":"markdown","31a7f95d":"markdown","a36bcce5":"markdown","bb0c6d6c":"markdown","99e268af":"markdown","ade39542":"markdown","2b31fcc8":"markdown","234a54ce":"markdown","86f1fc5a":"markdown","66cf653a":"markdown","c554d9cb":"markdown"},"source":{"9d2332a3":"\n\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n\n\n\n\n","485d5e9d":"wine=pd.read_csv('..\/input\/red-wine-quality-cortez-et-al-2009\/winequality-red.csv')","e2e33659":"wine.head(20)","e410f91a":"wine.tail(20)","9b65b865":"wine.info()","0ae29e04":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'alcohol', data = wine)","31faf3cb":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'free sulfur dioxide', data = wine)","ec6403ee":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'pH', data = wine)","3f740eb4":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'total sulfur dioxide', data = wine)","db820bf5":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'volatile acidity', data = wine)","09d52cc2":"fig = plt.figure(figsize = (10,6))\nsns.barplot(x = 'quality', y = 'fixed acidity', data = wine)","2009debb":"wine.quality = [1 if each >= 7 else 0 for each in wine.quality]\nwine.quality","f422baf8":"wine.quality.value_counts()","3e1d576c":"sns.countplot(wine.quality)","f8e797fc":"y = wine.quality.values\ny","8a3c6d7e":"x_data = wine.drop([\"quality\"],axis=1)\nx_data","2421382a":"x = (x_data - np.min(x_data))\/(np.max(x_data)-np.min(x_data))\nx","4bccb3ca":"from sklearn.model_selection import train_test_split\nx_train,x_test,y_train,y_test = train_test_split(x,y,test_size=0.2,random_state=1)\n","ee34c832":"from sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import accuracy_score \nRF = RandomForestClassifier(n_estimators=200, random_state=1)\nRF.fit(x_train,y_train)\npredictions = RF.predict(x_test)\nscore = round(accuracy_score(y_test, predictions), 5)\nprint(\"Random Forest Score {}\".format(score))\n","ee02d5fc":"from sklearn.model_selection import  cross_val_score\nRF_CrossValidation = cross_val_score(estimator=RF, X=x, y=y, cv = 40)\nprint(\"Random Forest Cross Validation Score \",RF_CrossValidation.max())","b20ce90d":"from sklearn.linear_model import SGDClassifier\nSGD = SGDClassifier(penalty=None)\nSGD.fit(x_train,y_train)\npredictions = SGD.predict(x_test)\nscore = round(accuracy_score(y_test,predictions),5)\nprint(\"Stochastic Gradient Decent Score {}\".format(score))","3e6080aa":"from sklearn.svm import SVC\nSVM = SVC(random_state=1)\nSVM.fit(x_train,y_train)\npredictions = SVM.predict(x_test)\nscore = round(accuracy_score(y_test,predictions),5)\nprint(\"Support Vector Machine Score {}\".format(score))","f25aa9d2":"from sklearn.model_selection import GridSearchCV\nparam = {\n    'C'     :[0.1,0.5,0.9,1,1.5,1.2,1.3,1.4],\n    'kernel':['linear', 'rbf'],\n    'gamma' :[0.1,0.8,0.9,1,1.1,1.2,1.3,1.4]\n}\nSVM_GridSearchCV = GridSearchCV(SVM, param_grid=param, scoring='accuracy', cv=40)\nSVM_GridSearchCV.fit(x_train,y_train)\nSVM_GridSearchCV.best_params_","fc5dccc0":"SVM_BestGridSearchCV1 = SVC(C = 1.5, gamma = 1.3, kernel = 'rbf')\nSVM_BestGridSearchCV1.fit(x_train,y_train)\npredictions = SVM_BestGridSearchCV1.predict(x_test)\nscore = round(accuracy_score(y_test,predictions),5)\nprint(\"Support Vector Machine Grid Search CV Score {}\".format(score))","4a193142":"# Create Test and Train Arrays","31a7f95d":"# Support Vector Machine Classifier\n\nSupport Vector Machines; Let's assume that we place our data with 2 classes on the coordinate system. If our properties (feature, axis) are suitable, we should be able to see our data in 2 groups. Let's divide it with such a line that we can classify this data set. Learning methods that enable us to find this line do not have to be a single line, are called Support Vector Machines (SVM). With SVM, not only classification, but also regression can be done.\nIt is an algorithm that we need to label our data, which is supervised. Because while doing the separation process I mentioned above, our algorithm will make the error calculation according to the labels of the data points.\n\n[![image.png](attachment:image.png)](http:\/\/)","a36bcce5":"# Random Forest Classifier\n\n Random forest classifier creates a set of decision trees from randomly selected subset of training set. It then aggregates the votes from different decision trees to decide the final class of the test object.\n \n First, the Random Forest algorithm is a controlled classification algorithm. (Supervised classification algorithm). As we can see from the name, the algorithm simply creates a forest randomly. There is a direct relationship between the number of trees in the algorithm and the results they can achieve. As the number of trees increases, we get a definite result.\n[![image.png](attachment:image.png)](http:\/\/)","bb0c6d6c":"# Data Visualization","99e268af":"# Loading Dataset and Data Information","ade39542":"# Stochastic Gradient Decent Classifier\n\nStochastic Gradient Descent (SGD) is a simple yet very efficient approach to discriminative learning of linear classifiers under convex loss functions such as (linear) Support Vector Machines and Logistic Regression. Even though SGD has been around in the machine learning community for a long time, it has received a considerable amount of attention just recently in the context of large-scale learning.\nSGD has been successfully applied to large-scale and sparse machine learning problems often encountered in text classification and natural language processing. Given that the data is sparse, the classifiers in this module easily scale to problems with more than 10^5 training examples and more than 10^5 features.","2b31fcc8":"# Grid Search CV\n\nGrid search is the process of performing hyper parameter tuning in order to determine the optimal values for a given model. This is significant as the performance of the entire model is based on the hyper parameter values specified.\n If you work with ML, you know what a nightmare it is to stipulate values for hyper parameters. There are libraries that have been implemented, such as GridSearchCV of the sklearn library, in order to automate this process and make life a little bit easier for ML enthusiasts.","234a54ce":"# Cross Validation\n\nMachine learning is an iterative process.\nYou will face choices about predictive variables to use, what types of models to use,what arguments to supply those models, etc. We make these choices in a data-driven way by measuring model quality of various alternatives.\nYou've already known to use train_test_split to split the data, so you can measure model quality on the test data. Cross-validation extends this approach to model scoring (or \"model validation.\") Compared to train_test_split, cross-validation gives you a more reliable measure of your model's quality, though it takes longer to run.\n\n[![image.png](attachment:image.png)](http:\/\/)","86f1fc5a":"# Categorizing Data","66cf653a":"# **CLASSIFICATION**\n\nMachine learning and statistics; classification is a controlled learning approach that the computer program learns from the given data entry and then uses this learning to classify new observations. These datasets are either bi-class (such as determining whether a person is male or female, an e-mail is spam or spam) or multi some examples of classification problems are: voice recognition, handwriting recognition, biometric identification, document classification, etc.\nHere we have types of classification algorithms in machine learning:\n1. Linear Classifiers: Logistic Regression, Naive Bayes Classifier\n2. Support Vector Machines\n3. Decision Trees\n4. Increased Trees\n5. Random Forests\n6. Neural Networks\n7. Nearest Neighbor","c554d9cb":"# Data Normalization"}}