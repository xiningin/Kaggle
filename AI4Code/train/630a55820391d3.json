{"cell_type":{"2b688687":"code","837ab3a3":"code","3ec4ec36":"code","cd0699b3":"code","364fbdd1":"code","3d4dc6f4":"code","be003f15":"code","eaa7f44d":"code","d10c7ac1":"code","0609c8d1":"code","c70ff5a5":"code","b531114d":"code","ebc03460":"code","d18945a9":"code","59ba7611":"code","272f9cce":"code","5f37093e":"code","f238c1ed":"code","91f5c3c6":"code","c65db62e":"markdown","882f4f65":"markdown","9c070dfc":"markdown","8a5fdc32":"markdown","2a4abcf1":"markdown","5f8875b0":"markdown","10ad2b34":"markdown"},"source":{"2b688687":"# LIBRARIES\n# General\nimport pandas as pd\nimport numpy as np\nimport os\nimport tqdm\nimport gc\nfrom itertools import chain\nfrom glob import glob\nimport warnings\nwarnings.filterwarnings('ignore')\n# Preprocessing\nfrom sklearn.preprocessing import QuantileTransformer, StandardScaler, PolynomialFeatures, LabelEncoder\nfrom sklearn.neighbors import LocalOutlierFactor\n# Visualization\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n# Hyperpameter Tuning\n!pip install optuna\nimport optuna\nfrom optuna.samplers import TPESampler\n# Modeling\nimport lightgbm as lgb\nimport catboost as cb\nimport xgboost as xgb\nfrom sklearn.linear_model import RidgeClassifier,RidgeClassifierCV,Ridge\nfrom sklearn.calibration import CalibratedClassifierCV\nfrom sklearn.tree import DecisionTreeRegressor\n#Inference\nfrom sklearn.model_selection import StratifiedKFold,train_test_split\nfrom catboost import CatBoostClassifier, Pool\n# Metric\nfrom sklearn.metrics import roc_auc_score, log_loss, accuracy_score,confusion_matrix,classification_report\n# Set constants\nNUM_FOLDS=10\nN_TRIALS = 100\nS_TRIALS = 7000\nTIME = 3600*6\nRANDOM_STATE=60\nEARLY_STOPPING_ROUNDS=1000\nsub_columns=['Class_1','Class_2','Class_3','Class_4']","837ab3a3":"# Get data\ntrain=pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/train.csv\",index_col = 0)\ntest=pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/test.csv\",index_col = 0)\nsample=pd.read_csv(\"..\/input\/tabular-playground-series-may-2021\/sample_submission.csv\")\n# Preprocessing. Take only this observations\nfeatures=[f'feature_{i}' for i in range(50)]\nidr=train[features].drop_duplicates().index #Duplicated values\n# inr=np.load() # https:\/\/www.kaggle.com\/fusioncenter\/keras-embeddings-with-resnet-architecture. This idea is logic\ntrain=train.iloc[idr]\ntrain.reset_index(inplace=True,drop=True)\n# Encoding Target\ny_train= LabelEncoder().fit_transform(train['target'])","3ec4ec36":"# Get models\ntrain_files=sorted(glob(os.path.join('..\/input\/tps-may-1st-stage-of-modeling\/train_*')))\ntest_files=sorted(glob(os.path.join('..\/input\/tps-may-1st-stage-of-modeling\/test_*')))\n# del train_files[??] ; del test_files[??] # In a previous analysis, i checked some models were a leakage (the predictions in the execution are worse). I'm not sure why (Now those models were removed)\nnames=[file[file.find('train_')+6:-3] for file in train_files]\ntrain_predictions=pd.DataFrame()\ntest_predictions=pd.DataFrame()\noof=[]\npreds=[]\nfor n_file,file in enumerate(train_files):\n    train_tmp=pd.DataFrame(np.load(file),columns = [f'{names[n_file]}_Class_1',f'{names[n_file]}_Class_2',f'{names[n_file]}_Class_3',f'{names[n_file]}_Class_4'])\n    test_tmp=pd.DataFrame(np.load(test_files[n_file]),columns =[f'{names[n_file]}_Class_1',f'{names[n_file]}_Class_2',f'{names[n_file]}_Class_3',f'{names[n_file]}_Class_4'])\n    # Prepare the files_1: One dataframe with all classes in each file\n    train_predictions=pd.concat([train_predictions,train_tmp],axis=1)\n    test_predictions=pd.concat([test_predictions,test_tmp],axis=1)\n    # Prepare the files_2: A list of dataframes\n    train_tmp.columns=sub_columns\n    test_tmp.columns=sub_columns\n    oof.append(train_tmp)\n    preds.append(test_tmp)","cd0699b3":"#Let's see the dataset containing all models\ntrain_predictions #dataframe with all fields\n# oof # a list of dataframes","364fbdd1":"# Analyze the models\ndef see_model(file,yreal=y_train):\n    df=pd.DataFrame(np.load(file,allow_pickle=False),columns=sub_columns)\n    print(f'Description of the classes:\\n {df.describe()}') # To do a Table Img\n    print(f'Most possible class poportion:\\n {dict(df.idxmax(axis=1).value_counts())}') #To do a Pie Chart\n    print(f'Distribution of the classes:')\n    fig,ax=plt.subplots(figsize=(8.5,6.5),nrows=2,ncols=2)\n    sns.histplot(df['Class_1'],ax=ax[0,0],color='khaki')\n    sns.histplot(df['Class_2'],ax=ax[0,1],color='steelblue')\n    sns.histplot(df['Class_3'],ax=ax[1,0],color='lightcoral')\n    sns.histplot(df['Class_4'],ax=ax[1,1],color='palegreen')\n    ax[0,0].set_ylabel(' ')\n    ax[0,1].set_ylabel(' ')\n    ax[1,0].set_ylabel(' ')\n    ax[1,1].set_ylabel(' ')\n    plt.savefig(f\"model_{file[file.find('modeling\/')+9:-3]}.png\")","3d4dc6f4":"see_model('..\/input\/tps-may-1st-stage-of-modeling\/test_lgb_1.0914046652034706.npy') # A dashboard is useful here if you have a lot of models\n# for file in glob('..\/input\/tps-may-1st-stage-of-modeling'): see_model(file) # An alternative","be003f15":"def stacking(X_train,y_train,X_test):\n    skf=StratifiedKFold(n_splits = NUM_FOLDS,shuffle = True,random_state = RANDOM_STATE)\n    yv=np.zeros((len(X_train),4))\n    yt=np.zeros((len(X_test),4))\n    for fold,(idx_tr,idx_vl) in enumerate(skf.split(X_train,y_train)):\n        X_tr,y_tr=pd.DataFrame(X_train.iloc[idx_tr]),pd.Series(y_train).iloc[idx_tr]\n        X_vl,y_vl=pd.DataFrame(X_train.iloc[idx_vl]),pd.Series(y_train).iloc[idx_vl]\n        model = CalibratedClassifierCV(RidgeClassifier(random_state=RANDOM_STATE),cv=10).fit(X_tr,y_tr)\n        # See the pattern inference, forecast and evaluation\n        yv[idx_vl]=model.predict_proba(X_vl)\n        yt+=model.predict_proba(X_test)\/NUM_FOLDS\n        print(f\"Found Metric in {fold}:{log_loss(y_vl,yv[idx_vl])}\")\n    metric=log_loss(y_train,yv)\n    print(f'Results of the training: {metric}')\n    # Create submission\n    sub=pd.DataFrame(yt,columns=sub_columns)\n    sub['id']=sample.id\n    sub=sub[['id','Class_1','Class_2','Class_3','Class_4']]\n    sub.to_csv(f'stacking_{metric}.csv',index=False)\n    return yv,sub\ndef evaluation(y,yreal=y_train):\n    metric=log_loss(yreal,y)\n    print(f'Results of the training: {metric}')\n    print(classification_report(yreal, np.argmax(y, axis = 1), target_names=sub_columns))\n    sns.heatmap(pd.DataFrame(confusion_matrix(yreal, np.argmax(y, axis = 1))), annot=True, linewidths=.5, fmt=\"d\")\n    plt.savefig(f'evaluation_{metric}.png')","eaa7f44d":"yst,ysf=stacking(train_predictions,y_train,test_predictions) # lb: 1.08563","d10c7ac1":"evaluation(yst)","0609c8d1":"class optimizer_1():    \n    def __init__(self, metric='mse', n_weights=len(oof),trials=S_TRIALS):\n        self.metric = metric\n        self.trials = trials\n        self.sampler = TPESampler(seed=RANDOM_STATE)\n        self.n_weights=n_weights\n    def objective(self,trial):\n        weigths=[]\n        sum_weights=0\n        for i in range(self.n_weights):\n            weigths.append({f\"w_{i}\": trial.suggest_uniform(f\"w_{i}\",0,3)})\n            sum_weights+=weigths[i][f'w_{i}']\n        y_pred=0\n        for i in range(self.n_weights): y_pred+=oof[i]*weigths[i][f'w_{i}']\/sum_weights\n        return log_loss(y_train,np.array(y_pred))\n    def optimize(self):\n        study = optuna.create_study(direction=\"minimize\", sampler=self.sampler)\n        study.optimize(self.objective, n_trials=self.trials)\n        # Best weights\n        sum_weights_test=0\n        for i in range(self.n_weights): sum_weights_test+=study.best_params[f'w_{i}']\n        # Get predictions\n        yv,yt=0,0\n        for i in range(self.n_weights): yv+=oof[i]*study.best_params[f'w_{i}']\/sum_weights_test\n        for i in range(self.n_weights): yt+=preds[i]*study.best_params[f'w_{i}']\/sum_weights_test\n        # Save submission and send it \n        yt['id']=sample.id\n        yt=yt[['id','Class_1','Class_2','Class_3','Class_4']]\n        yt.to_csv(f'blending_{study.best_value}.csv',index=False)\n        return yv,yt,study.best_params","c70ff5a5":"ybt1,ybf1,wb1=optimizer_1(trials=1200).optimize()","b531114d":"evaluation(np.array(ybt1))","ebc03460":"class optimizer_2():    \n    def __init__(self, metric='mse',n_weights=train_predictions.shape[1],trials=S_TRIALS):\n        self.metric = metric\n        self.trials = trials\n        self.sampler = TPESampler(seed=RANDOM_STATE)\n        self.n_weights=n_weights\n    def objective(self,trial):\n        weigths=[]; w=[]\n        sum_weights=0\n        for i in range(self.n_weights):\n            weigths.append({f\"w_{i}\": trial.suggest_uniform(f\"w_{i}\",0,3)})\n            w.append(weigths[i][f'w_{i}'])\n            sum_weights+=weigths[i][f'w_{i}']\n        y_pred1,y_pred2,y_pred3,y_pred4=0,0,0,0\n        for i in range(0,self.n_weights,4): y_pred1+=train_predictions.iloc[:,i]*weigths[i][f'w_{i}']\/sum_weights\n        for i in range(1,self.n_weights,4): y_pred2+=train_predictions.iloc[:,i]*weigths[i][f'w_{i}']\/sum_weights\n        for i in range(2,self.n_weights,4): y_pred3+=train_predictions.iloc[:,i]*weigths[i][f'w_{i}']\/sum_weights\n        for i in range(3,self.n_weights,4): y_pred4+=train_predictions.iloc[:,i]*weigths[i][f'w_{i}']\/sum_weights\n        y_pred=pd.concat([y_pred1,y_pred2,y_pred3,y_pred4],axis=1)\n        y_pred.columns=sub_columns\n        return log_loss(y_train,np.array(y_pred))\n    def optimize(self):\n        study = optuna.create_study(direction=\"minimize\", sampler=self.sampler)\n        study.optimize(self.objective, n_trials=self.trials)\n        # Get predictions\n        yv=self.get_matrix(train_predictions,study.best_params)\n        yt=self.get_matrix(test_predictions,study.best_params)\n        # Save submission and send it \n        yt['id']=sample.id\n        yt=yt[['id','Class_1','Class_2','Class_3','Class_4']]\n        yt.to_csv(f'blending_{study.best_value}.csv',index=False)\n        return yv,yt,study.best_params\n    def get_matrix(self,df,mat):\n        sum_weights=0\n        for i in range(self.n_weights): sum_weights+=mat[f'w_{i}']\n        yv1,yv2,yv3,yv4=0,0,0,0\n        for i in range(0,self.n_weights,4): yv1+=df.iloc[:,i]*mat[f'w_{i}']*4\/sum_weights\n        for i in range(1,self.n_weights,4): yv2+=df.iloc[:,i]*mat[f'w_{i}']*4\/sum_weights\n        for i in range(2,self.n_weights,4): yv3+=df.iloc[:,i]*mat[f'w_{i}']*4\/sum_weights\n        for i in range(3,self.n_weights,4): yv4+=df.iloc[:,i]*mat[f'w_{i}']*4\/sum_weights\n        yv=pd.concat([yv1,yv2,yv3,yv4],axis=1)\n        yv.columns=sub_columns\n        return yv","d18945a9":"ybt2,ybf2,wb2=optimizer_2(trials=1200).optimize()","59ba7611":"evaluation(np.array(ybt2))","272f9cce":"def new_data(df_train,df_test,labels,p=0.7):\n    idx=labels.apply(lambda x: any(x>p), axis=1)\n    # New labels\n    y_test=labels[idx].idxmax(axis=1)\n    y=pd.concat([train['target'],y_test],axis=0)\n    y.reset_index(inplace = True,drop=True)\n    y=LabelEncoder().fit_transform(y)\n    # New observations\n    new_obs=df_test.iloc[labels[idx].index]\n    X=pd.concat([df_train,new_obs],axis=0)\n    X.reset_index(inplace = True,drop=True)\n    return X,y\n# p should be a higher value. Honestly i'm not sure if pseudolabelling works. It's very risky. However a notebook of a gm shows the similiarity between train and test split","5f37093e":"X,y=new_data(train_predictions,test_predictions,ysf[sub_columns],0.7)\nyspt,yspf=stacking(X,y,test_predictions) # lb: 1.08563","f238c1ed":"evaluation(yspt,y)","91f5c3c6":"def avg(models,weights):\n    sub_lb=0\n    for n_model,model in enumerate(models): sub_lb+=model*weights[n_model]\n    sub_lb.id=sample.id\n    sub_lb.to_csv(f'sub_lb_{sub_lb.iloc[0,1]}_{weights[0]}.csv',index=False) #sub_lb.iloc[0,1] is and index\n    return sub_lb\nbest_lb=pd.read_csv('..\/input\/tps-may2021-stacking\/sub.csv') #2nd best lb. You can try the other\nyvg_1=avg([best_lb,ysf],[0.7,0.3]) #lb:1.08510\nyvg_2=avg([best_lb,yspf],[0.7,0.3])\nyvg_3=avg([best_lb,yspf,ybf2],[0.7,0.25,0.05])","c65db62e":"# [TPS - May] Stacking+Blending+Pseudolabelling+Averaging \ud83e\udd47\n![tab](https:\/\/i.imgur.com\/uHVJtv0.png\")\n\n**Description:** The dataset in the competition is synthetic but based on a real dataset and generated using a CTGAN. The original dataset deals with predicting the category on an eCommerce product given various attributes about the listing. Although the features are anonymized, they have properties relating to real-world features.\n\n**Solution:** Two levels of modeling. (1) Define single models -include some of that competition- and obtain its predictions. (2) Then define a  metamodel with the information of the previous step.\n\n<div class=\"alert alert-block alert-success\"> Hello everybody \ud83d\udc4b This is my first notebook in TPS community, i hope you'll like it. \ud83d\ude0a<\/div>\n\n\nI present a possible solution for this competition. I'm relatively new in Kaggle, if you find any improvement in the proposal or the code, please let me know.\n\n![stackedml](https:\/\/storage.googleapis.com\/kagglesdsdata\/datasets\/1251020\/2086737\/stacked.PNG?X-Goog-Algorithm=GOOG4-RSA-SHA256&X-Goog-Credential=databundle-worker-v2%40kaggle-161607.iam.gserviceaccount.com%2F20210529%2Fauto%2Fstorage%2Fgoog4_request&X-Goog-Date=20210529T233928Z&X-Goog-Expires=345599&X-Goog-SignedHeaders=host&X-Goog-Signature=5fb5391b275b6879052666f8134f5d83ed8a3d359cdf89c23675c2300fdf20fcd7b0c1e52731aca74c09cf884a81338e768f8b2104f00c2ed519b5b45b70377fd1fe4f883e4f3f5cee125b982f376c68e8a57a9ab15786d44f3feb566dfde02d32210889fe17c08ee4d6aff60364c401dfd10d4c26d923736adface3063026e4b6eec0db5feb27cea05e778428232e97e830f57c8abe2e025c50d28665962f8bf9852754091f90844ab40ce708131729093110969f6a812a5be0e9c0f56f04465c5f3e2db951b1aa229204066c6da3783dfbdfa592efe5a0bd3fb2798f3f39acfeb88d75f426bee0008631359ad73865aafbba32e3a37f0cb1aa65c7a794adb4)\n\nI did this diagram in **TPS - March Competition [Top 1%]** ([link](https:\/\/www.kaggle.com\/c\/tabular-playground-series-mar-2021\/leaderboard)).  Generally, this is a good workflow i think. See the information that you should save and how you arrive to 2nd stage. You can add more ideas - Ex: Pseudolabelling, Leakage, Selection of best models and PostProcessing - to get a better score. More information [here](https:\/\/mlwave.com\/kaggle-ensembling-guide\/) and in the additional resources.\n\nSo, in this opportunity, I propose that. In the 1st stage, the models that i got are the following: 11 lightgbm, 4 xgboost, 7 catboost, 1 keras, 1 deebtable, 2 logistic regressions, 5 autolightml.\nSome models i got from my own ideas and others, from the notebooks of the competition (not all, some are a leakage and my stacking - or blending - ensemble model showed weird results). The 2nd stage is presented in this notebook.\n\nPd: Now you can see the respective dataset!","882f4f65":"# Analyze the 1st stage of models","9c070dfc":"# Averaging","8a5fdc32":"# Additional resources\n- https:\/\/www.kaggle.com\/davidedwards1\/tabmar21-tabular-blend-final-sub\n- https:\/\/www.kaggle.com\/hiro5299834\/3rd-tps-mar-2021-stacking\n- https:\/\/www.kaggle.com\/cdeotte\/pseudo-labeling-qda-0-969\n- https:\/\/www.kaggle.com\/gomes555\/tps-may2021-stacking\n- https:\/\/www.kaggle.com\/tunguz\/adversarial-tps-may-2021\n- https:\/\/www.kaggle.com\/c\/tabular-playground-series-may-2021\/discussion\/236561\n\n## Last thoughts\n- I'm not sure if this workflow is good, anyway i share the resources that i used.. i'd like to hear your comments. How do you improve this? Any fix in the code?\n- On the other hand, i'm not sure what's the reason of avg (cv+lb) is much better. The lb is very weird. See this results: only best cv -> 1.08563 and best cv + best lb -> 1.08510.\n- Hmm.. do you think are there exist possibilities of a shakeup? What is the good side?\n- Things to do: add deep learning models (dae+model) and consider other possibilites of preprocessing.\n<div class=\"alert alert-block alert-success\">\nDon't forget to upvote if you think this notebook is useful (or interesting), i apreciate that.. Well, That's all, thanks for reading and good luck in your projects!\n<\/div>\n","2a4abcf1":"# Stacking","5f8875b0":"# Blending","10ad2b34":"# Pseudolabel"}}