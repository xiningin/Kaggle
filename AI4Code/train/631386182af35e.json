{"cell_type":{"70ce60b8":"code","e18752c5":"code","81d81d17":"code","470ec2b2":"code","9c92a19f":"code","02a5cde2":"code","ef7738d8":"code","4feb306a":"code","e2e7df83":"code","8dfc0e1c":"code","127904fa":"code","a4b67c00":"code","279056f4":"code","9942c716":"code","7214f757":"code","6f20608e":"code","889d9feb":"code","f718e5a8":"code","b813b9a2":"code","9bd6947a":"code","c078ceb3":"code","c8026bc1":"code","1ff43203":"code","fc018a8a":"code","58123262":"code","e414ab25":"code","1a606ee2":"code","b41a6141":"code","2b174dd4":"code","7fe7e75c":"code","c7c161a4":"code","ec44beaa":"code","d891519b":"code","2501f159":"code","cabe505c":"code","00370066":"code","51b15d8b":"code","3962c346":"code","2ef9d9c1":"code","b5eaf316":"code","4b566e70":"code","27d7b7e2":"code","f1394c46":"code","99886cef":"code","0c8c7aa2":"code","1c141020":"markdown","15e864db":"markdown","bcabdbae":"markdown","01b45638":"markdown","4733544d":"markdown","d80c99a5":"markdown","ab70c734":"markdown","314ebbc5":"markdown","3a23e782":"markdown","930b80fa":"markdown","2ef36a4c":"markdown","7a9fecb9":"markdown","663b1890":"markdown","7a515fb9":"markdown","39632d87":"markdown","f5e2d43c":"markdown","dc33c77f":"markdown","acdb0463":"markdown","cb1d1290":"markdown","8286f35f":"markdown","0d8d1e7d":"markdown","d05c3774":"markdown","06bcf026":"markdown","37f3ae39":"markdown","7c5d6e5d":"markdown","529c7a26":"markdown"},"source":{"70ce60b8":"# Base\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Model\nimport lightgbm as lgb\nimport shap\nfrom sklearn.model_selection import TimeSeriesSplit, GridSearchCV, RandomizedSearchCV\nfrom sklearn.metrics import make_scorer\n\n# Configuration\nimport warnings\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_columns', None)","e18752c5":"train = pd.read_csv('..\/input\/demand-forecasting-kernels-only\/train.csv', parse_dates=['date'])\ntest = pd.read_csv('..\/input\/demand-forecasting-kernels-only\/test.csv', parse_dates=['date'])\n# sample_sub = pd.read_csv('.\/input\/demand-forecasting-kernels-only\/sample_submission.csv')\ndf = pd.concat([train, test], sort=False)\n\nprint(train.shape, test.shape, df.shape, \"\\n\")\ntrain.head()","81d81d17":"# How many stores and items are there?\ntrain.store.nunique(), test.store.nunique(), train.item.nunique(), test.item.nunique()","470ec2b2":"# Time Range\ntrain[\"date\"].min(), train[\"date\"].max(), test[\"date\"].min(), test[\"date\"].max()","9c92a19f":"# How many items are in the store?\ndf.groupby([\"store\"])[\"item\"].nunique()","02a5cde2":"# Summary Stats for each store\ndf.groupby([\"store\"]).agg({\"sales\": [\"count\",\"sum\", \"mean\", \"median\", \"std\", \"min\", \"max\"]})","ef7738d8":"# Summary Stats for each item\ndf.groupby([\"item\"]).agg({\"sales\": [\"count\",\"sum\", \"mean\", \"median\", \"std\", \"min\", \"max\"]})","4feb306a":"fig, axes = plt.subplots(2, 5, figsize=(20, 10))\nfor i in range(1,11):\n    if i < 6:\n        train[train.store == i].sales.hist(ax=axes[0, i-1])\n        axes[0,i-1].set_title(\"Store \" + str(i), fontsize = 15)\n        \n    else:\n        train[train.store == i].sales.hist(ax=axes[1, i - 6])\n        axes[1,i-6].set_title(\"Store \" + str(i), fontsize = 15)\nplt.tight_layout(pad=4.5)\nplt.suptitle(\"Histogram: Sales\");","e2e7df83":"store = 1\nsub = train[train.store == store].set_index(\"date\")\n\nfig, axes = plt.subplots(10, 5, figsize=(20, 35))\nfor i in range(1,51):\n    if i < 6:\n        sub[sub.item == i].sales.plot(ax=axes[0, i-1], legend=True, label = \"Item \"+str(i)+\" Sales\")\n    if i >= 6 and i<11:\n        sub[sub.item == i].sales.plot(ax=axes[1, i - 6], legend=True, label = \"Item \"+str(i)+\" Sales\")\n    if i >= 11 and i<16:\n        sub[sub.item == i].sales.plot(ax=axes[2, i - 11], legend=True, label = \"Item \"+str(i)+\" Sales\")    \n    if i >= 16 and i<21:\n        sub[sub.item == i].sales.plot(ax=axes[3, i - 16], legend=True, label = \"Item \"+str(i)+\" Sales\")    \n    if i >= 21 and i<26:\n        sub[sub.item == i].sales.plot(ax=axes[4, i - 21], legend=True, label = \"Item \"+str(i)+\" Sales\")  \n    if i >= 26 and i<31:\n        sub[sub.item == i].sales.plot(ax=axes[5, i - 26], legend=True, label = \"Item \"+str(i)+\" Sales\")    \n    if i >= 31 and i<36:\n        sub[sub.item == i].sales.plot(ax=axes[6, i - 31], legend=True, label = \"Item \"+str(i)+\" Sales\")    \n    if i >= 36 and i<41:\n        sub[sub.item == i].sales.plot(ax=axes[7, i - 36], legend=True, label = \"Item \"+str(i)+\" Sales\")    \n    if i >= 41 and i<46:\n        sub[sub.item == i].sales.plot(ax=axes[8, i - 41], legend=True, label = \"Item \"+str(i)+\" Sales\") \n    if i >= 46 and i<51:\n        sub[sub.item == i].sales.plot(ax=axes[9, i - 46], legend=True, label = \"Item \"+str(i)+\" Sales\") \nplt.tight_layout(pad=4.5)\nplt.suptitle(\"Store 1 Item Sat\u0131\u015f Da\u011f\u0131l\u0131m\u0131\");","8dfc0e1c":"storesales = train.groupby([\"date\", \"store\"]).sales.sum().reset_index().set_index(\"date\")\ncorr =  pd.pivot_table(storesales, values = \"sales\", columns=\"store\", index=\"date\").corr(method = \"spearman\")\nplt.figure(figsize = (7,7))\nsns.heatmap(corr[(corr >= 0.5) | (corr <= -0.5)], \n            cmap='viridis', vmax=1.0, vmin=-1.0, linewidths=0.1,\n            annot=True, annot_kws={\"size\": 9}, square=True);","127904fa":"# T Test\ndef CompareTwoGroups(dataframe, group, target):\n    \n    import itertools\n    from scipy.stats import shapiro\n    import scipy.stats as stats\n    \n    # 1. Normality Test: Shapiro Test\n    # 2. Homogeneity Test: Levene Test\n    # 3. Parametric or Non-Parametric T Test: T-Test, Welch Test, Mann Whitney U\n    \n    # Create Combinations\n    item_comb = list(itertools.combinations(dataframe[group].unique(), 2))\n    \n    AB = pd.DataFrame()\n    for i in range(0, len(item_comb)):\n        # Define Groups\n        groupA = dataframe[dataframe[group] == item_comb[i][0]][target]\n        groupB = dataframe[dataframe[group] == item_comb[i][1]][target]\n        \n        # Assumption: Normality\n        ntA = shapiro(groupA)[1] < 0.05\n        ntB = shapiro(groupB)[1] < 0.05\n        # H0: Distribution is Normal! - False\n        # H1: Distribution is not Normal! - True\n        \n        if (ntA == False) & (ntB == False): # \"H0: Normal Distribution\"\n            # Parametric Test\n            # Assumption: Homogeneity of variances\n            leveneTest = stats.levene(groupA, groupB)[1] < 0.05\n            # H0: Homogeneity: False\n            # H1: Heterogeneous: True\n            if leveneTest == False:\n                # Homogeneity\n                ttest = stats.ttest_ind(groupA, groupB, equal_var=True)[1]\n                # H0: M1 = M2 - False\n                # H1: M1 != M2 - True\n            else:\n                # Heterogeneous\n                ttest = stats.ttest_ind(groupA, groupB, equal_var=False)[1]\n                # H0: M1 = M2 - False\n                # H1: M1 != M2 - True\n        else:\n            # Non-Parametric Test\n            ttest = stats.mannwhitneyu(groupA, groupB)[1] \n            # H0: M1 = M2 - False\n            # H1: M1 != M2 - True\n            \n        temp = pd.DataFrame({\"Compare Two Groups\":[ttest < 0.05], \n                             \"p-value\":[ttest],\n                             \"GroupA_Mean\":[groupA.mean()], \"GroupB_Mean\":[groupB.mean()],\n                             \"GroupA_Median\":[groupA.median()], \"GroupB_Median\":[groupB.median()],\n                             \"GroupA_Count\":[groupA.count()], \"GroupB_Count\":[groupB.count()]\n                            }, index = [item_comb[i]])\n        temp[\"Compare Two Groups\"] = np.where(temp[\"Compare Two Groups\"] == True, \"Different Groups\", \"Similar Groups\")\n        temp[\"TestType\"] = np.where((ntA == False) & (ntB == False), \"Parametric\", \"Non-Parametric\")\n        \n        AB = pd.concat([AB, temp[[\"TestType\", \"Compare Two Groups\", \"p-value\",\"GroupA_Median\", \"GroupB_Median\",\"GroupA_Mean\", \"GroupB_Mean\",\n                                 \"GroupA_Count\", \"GroupB_Count\"]]])\n        \n    return AB\n    \n    \nCompareTwoGroups(storesales, group = \"store\", target = \"sales\")","a4b67c00":"itemsales = train.groupby([\"date\", \"item\"]).sales.sum().reset_index().set_index(\"date\")\nctg_is = CompareTwoGroups(itemsales, group = \"item\", target = \"sales\")\nctg_is[ctg_is[\"Compare Two Groups\"] == \"Similar Groups\"]","279056f4":"# 1. Time Related Features\n#####################################################\ndef create_date_features(df):\n    df['month'] = df.date.dt.month\n    df['day_of_month'] = df.date.dt.day\n    df['day_of_year'] = df.date.dt.dayofyear\n    df['week_of_year'] = df.date.dt.weekofyear\n    df['day_of_week'] = df.date.dt.dayofweek + 1\n    df['year'] = df.date.dt.year\n    df[\"is_wknd\"] = df.date.dt.weekday \/\/ 4\n    df[\"quarter\"] = df.date.dt.quarter\n    df['is_month_start'] = df.date.dt.is_month_start.astype(int)\n    df['is_month_end'] = df.date.dt.is_month_end.astype(int)\n    df['is_quarter_start'] = df.date.dt.is_quarter_start.astype(int)\n    df['is_quarter_end'] = df.date.dt.is_quarter_end.astype(int)\n    df['is_year_start'] = df.date.dt.is_year_start.astype(int)\n    df['is_year_end'] = df.date.dt.is_year_end.astype(int)\n    # 0: Winter - 1: Spring - 2: Summer - 3: Fall\n    df[\"season\"] = np.where(df.month.isin([12,1,2]), 0, 1)\n    df[\"season\"] = np.where(df.month.isin([6,7,8]), 2, df[\"season\"])\n    df[\"season\"] = np.where(df.month.isin([9, 10, 11]), 3, df[\"season\"])\n    return df\ndf = create_date_features(df)\n\n\n# Rolling Summary Stats Features\n#####################################################\nfor i in [91, 98, 105, 112, 119, 126, 186, 200, 210, 250, 300, 365, 546, 700]:\n    df[\"sales_roll_mean_\"+str(i)]=df.groupby([\"store\", \"item\"]).sales.rolling(i).mean().shift(1).values\n    #df[\"sales_roll_std_\"+str(i)]= df.groupby([\"store\", \"item\"]).sales.rolling(i).std().shift(1).values\n    #df[\"sales_roll_max_\"+str(i)]= df.groupby([\"store\", \"item\"]).sales.rolling(i).max().shift(1).values\n    #df[\"sales_roll_min_\"+str(i)]= df.groupby([\"store\", \"item\"]).sales.rolling(i).min().shift(1).values\n\n\n# 2. Hypothesis Testing: Similarity\n#####################################################\n\n# Store Based\nstoresales = train.groupby([\"date\", \"store\"]).sales.sum().reset_index()\nctg_ss = CompareTwoGroups(storesales, group=\"store\", target=\"sales\")\ndel storesales\n\ndf[\"StoreSalesSimilarity\"] = np.where(df.store.isin([3,10]), 1, 0)\ndf[\"StoreSalesSimilarity\"] = np.where(df.store.isin([4,9]), 2, df[\"StoreSalesSimilarity\"])\ndf[\"StoreSalesSimilarity\"] = np.where(df.store.isin([5,6]), 3, df[\"StoreSalesSimilarity\"])\n\n# Item Based\n\nitemsales = train.groupby([\"date\", \"item\"]).sales.sum().reset_index()\nctg_is = CompareTwoGroups(itemsales, group = \"item\", target = \"sales\")\ndel itemsales\n\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([1,4,27,41,47]), 1, 0)\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([2,6,7,14,31,46]), 2, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([3,42]), 3, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([8,36]), 4, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([9,43,48]), 5, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([11,12,29,33]), 6, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([13,18]), 7, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([15,28]), 8, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([16,34]), 9, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([19,21,30]), 10, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([20,26]), 11, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([22,25,38,45]), 12, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([23,37,40,44,49]), 13, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([24,35,50]), 14, df[\"ItemSalesSimilarity\"])\ndf[\"ItemSalesSimilarity\"] = np.where(df.item.isin([32,39]), 15, df[\"ItemSalesSimilarity\"])\n\n# 3. Lag\/Shifted Features\n#####################################################\n\n# test.groupby([\"store\", \"item\"]).date.count()\n# Test verisinde +90 g\u00fcn tahmin edilmesi isteniyor bu y\u00fczden\n# Lag featurelar\u0131 en az 91 olmal\u0131!\n\ndf.sort_values(by=['store', 'item', 'date'], axis=0, inplace=True)\n\ndef lag_features(dataframe, lags, groups = [\"store\", \"item\"], target = \"sales\", prefix = ''):\n    dataframe = dataframe.copy()\n    for lag in lags:\n        dataframe[prefix + str(lag)] = dataframe.groupby(groups)[target].transform(\n            lambda x: x.shift(lag))\n    return dataframe\n\ndf = lag_features(df, lags = [91, 92,93,94,95,96, 97, 98, 100, 105, 112, 119, 126, 150,\n                              182,200,220, 250, 300, 350, 355, 360,361,362,363, 364,\n                              365, 370, 375,380, 546, 600, 650, 680, 690, 700, 710, 728,\n                              730, 800, 900, 950, 990, 1000, 1050, 1090, 1095],\n                  groups = [\"store\", \"item\"], target = 'sales', prefix = 'sales_lag_')\n\ndef drop_cor(dataframe, name, index):\n    ind = dataframe[dataframe.columns[dataframe.columns.str.contains(name)].tolist()+[\"sales\"]].corr().sales.sort_values(ascending = False).index[1:index]\n    ind = dataframe.drop(ind, axis = 1).columns[dataframe.drop(ind, axis = 1).columns.str.contains(name)]\n    dataframe.drop(ind, axis = 1, inplace = True)\n\ndrop_cor(df, \"sales_lag\", 16)\n\n\n# 4. Last i. Months\n#####################################################\ndf[\"monthyear\"] = df.date.dt.to_period('M')\n\n# Store-Item Based\nfor i in [3, 6, 9, 12, 15, 18, 21, 24, 27, 30, 33, 36]:\n    last_months = df.groupby([\"store\", \"item\", \"monthyear\"]).sales.agg([\n        \"sum\", \"mean\", \"std\", \"min\", \"max\"]).shift(i).reset_index()\n    last_months.columns = ['store', 'item', 'monthyear', 'last_'+str(i)+'months_sales_sum',\n                           'last_'+str(i)+'months_sales_mean', 'last_'+str(i)+'months_sales_std',\n                           'last_'+str(i)+'months_sales_min', 'last_'+str(i)+'months_sales_max']\n    df = pd.merge(df, last_months, how   = \"left\", on = [\"store\", \"item\", \"monthyear\"])\ndel last_months, i\n\ndrop_cor(df, \"last_\", 15)\n\n# Store Based\n\n\nfor i in [3, 6, 9, 12]:\n    last_months = df.groupby([\"store\", \"monthyear\"]).sales.agg([\n        \"sum\", \"mean\", \"std\", \"min\", \"max\"]).shift(i).reset_index()\n    last_months.columns = ['store', 'monthyear', 'store_last_'+str(i)+'months_sales_sum',\n                           'store_last_'+str(i)+'months_sales_mean', 'store_last_'+str(i)+'months_sales_std',\n                           'store_last_'+str(i)+'months_sales_min', 'store_last_'+str(i)+'months_sales_max']\n    df = pd.merge(df, last_months, how = \"left\", on = [\"store\", \"monthyear\"])\ndel last_months, i\n\n# Item Based\nfor i in [3, 6, 9, 12]:\n    last_months = df.groupby([\"item\", \"monthyear\"]).sales.agg([\n        \"sum\", \"mean\", \"std\", \"min\", \"max\"]).shift(i).reset_index()\n    last_months.columns = ['item', 'monthyear', 'item_last_'+str(i)+'months_sales_sum',\n                           'item_last_'+str(i)+'months_sales_mean', 'item_last_'+str(i)+'months_sales_std',\n                           'item_last_'+str(i)+'months_sales_min', 'item_last_'+str(i)+'months_sales_max']\n    df = pd.merge(df, last_months, how = \"left\", on = [\"item\", \"monthyear\"])\ndel last_months, i\n\n# Similarity Based\n\n\nfor i in [3, 6, 9, 12]:\n    last_months = df.groupby([\"StoreSalesSimilarity\", \"monthyear\"]).sales.agg([\n        \"sum\", \"mean\", \"std\", \"min\", \"max\"]).shift(i).reset_index()\n    last_months.columns = ['StoreSalesSimilarity', 'monthyear', 'storesim_last_'+str(i)+'months_sales_sum',\n                           'storesim_last_'+str(i)+'months_sales_mean', 'storesim_last_'+str(i)+'months_sales_std',\n                           'storesim_last_'+str(i)+'months_sales_min', 'storesim_last_'+str(i)+'months_sales_max']\n    df = pd.merge(df, last_months, how = \"left\", on = [\"StoreSalesSimilarity\", \"monthyear\"])\ndel last_months, i\n\n\nfor i in [3, 6, 9, 12]:\n    last_months = df.groupby([\"ItemSalesSimilarity\", \"monthyear\"]).sales.agg([\n        \"sum\", \"mean\", \"std\", \"min\", \"max\"]).shift(i).reset_index()\n    last_months.columns = ['ItemSalesSimilarity', 'monthyear', 'itemsim_last_'+str(i)+'months_sales_sum',\n                           'itemsim_last_'+str(i)+'months_sales_mean', 'itemsim_last_'+str(i)+'months_sales_std',\n                           'itemsim_last_'+str(i)+'months_sales_min', 'itemsim_last_'+str(i)+'months_sales_max']\n    df = pd.merge(df, last_months, how = \"left\", on = [\"ItemSalesSimilarity\", \"monthyear\"])\ndel last_months, i\n\ndf.drop(\"monthyear\", axis = 1, inplace = True)\n\n\n# 5. Last i. day of week\n#####################################################\ndf.sort_values([\"store\", \"item\", \"day_of_week\", \"date\"], inplace = True)\n\ndf = lag_features(df, lags = np.arange(12,41, 1).tolist()+[91, 92, 95, 98, 99, 100, 105, 112, 119, 126, 133, 140, 200, 205, 210, 215, 220, 250],\n                  groups = [\"store\", \"item\", \"day_of_week\"], target = 'sales', prefix = 'dayofweek_sales_lag_')\n\ndf[df.columns[df.columns.str.contains(\"dayofweek_sales_lag_\")].tolist()+[\"sales\"]].corr().sales.sort_values(ascending = False)\n\ndrop_cor(df, \"dayofweek_sales_lag_\", 16)\n\ndf.sort_values([\"store\", \"item\", \"date\"], inplace = True)\n\n\n#####################################################\n# Exponentially Weighted Mean Features\n#####################################################\ndef ewm_features(dataframe, alphas, lags):\n    dataframe = dataframe.copy()\n    for alpha in alphas:\n        for lag in lags:\n            dataframe['sales_ewm_alpha_' + str(alpha).replace(\".\", \"\") + \"_lag_\" + str(lag)] = \\\n                dataframe.groupby([\"store\", \"item\"])['sales']. \\\n                    transform(lambda x: x.shift(lag).ewm(alpha=alpha).mean())\n    return dataframe\n\nalphas = [0.95, 0.9, 0.8, 0.7, 0.5]\nlags = [91, 98, 105, 112, 180, 270, 365, 546, 728]\n\ndf = ewm_features(df, alphas, lags)\n\n# Day of year \ndf.sort_values([\"day_of_year\", \"store\", \"item\"], inplace = True)\ndf = lag_features(df, lags = [1,2,3,4],\n                  groups = [\"day_of_year\", \"store\", \"item\"], target = 'sales', prefix = 'dayofyear_sales_lag_')\n\n\n# pd.cut\nclus = df.groupby([\"store\"]).sales.mean().reset_index()\nclus[\"store_cluster\"] =  pd.cut(clus.sales, bins = 4, labels = range(1,5))\nclus.drop(\"sales\", axis = 1, inplace = True)\ndf = pd.merge(df, clus, how = \"left\")\nclus = df.groupby([\"item\"]).sales.mean().reset_index()\nclus[\"item_cluster\"] =  pd.cut(clus.sales, bins = 5, labels = range(1,6))\nclus.drop(\"sales\", axis = 1, inplace = True)\ndf = pd.merge(df, clus, how = \"left\")\ndel clus\n\ndf.shape","9942c716":"# Dataframe must be sorted by date because of Time Series Split \ndf = df.sort_values(\"date\").reset_index(drop = True)\n\n# Train Validation Split\n# Validation set includes 3 months (Oct. Nov. Dec. 2017)\ntrain = df.loc[(df[\"date\"] < \"2017-10-01\"), :]\nval = df.loc[(df[\"date\"] >= \"2017-10-01\") & (df[\"date\"] < \"2018-01-01\"), :]\n\n\ncols = [col for col in train.columns if col not in ['date', 'id', \"sales\", \"year\"]]\n\nY_train = train['sales']\nX_train = train[cols]\n\nY_val = val['sales']\nX_val = val[cols]\n\nY_train.shape, X_train.shape, Y_val.shape, X_val.shape","7214f757":"# SMAPE: Symmetric mean absolute percentage error (adjusted MAPE)\ndef smape(preds, target):\n    n = len(preds)\n    masked_arr = ~((preds == 0) & (target == 0))\n    preds, target = preds[masked_arr], target[masked_arr]\n    num = np.abs(preds-target)\n    denom = np.abs(preds)+np.abs(target)\n    smape_val = (200*np.sum(num\/denom))\/n\n    return smape_val\n\ndef lgbm_smape(y_true, y_pred):\n    smape_val = smape(y_true, y_pred)\n    return 'SMAPE', smape_val, False\n","6f20608e":"first_model = lgb.LGBMRegressor(random_state=384).fit(X_train, Y_train, \n                                                      eval_metric= lambda y_true, y_pred: [lgbm_smape(y_true, y_pred)])\n\nprint(\"TRAIN SMAPE:\", smape(Y_train, first_model.predict(X_train)))\nprint(\"VALID SMAPE:\", smape(Y_val, first_model.predict(X_val)))","889d9feb":"\ndef plot_lgb_importances(model, plot=False, num=10):\n    from matplotlib import pyplot as plt\n    import seaborn as sns\n    \n    # LGBM API\n    #gain = model.feature_importance('gain')\n    #feat_imp = pd.DataFrame({'feature': model.feature_name(),\n    #                         'split': model.feature_importance('split'),\n    #                         'gain': 100 * gain \/ gain.sum()}).sort_values('gain', ascending=False)\n    \n    # SKLEARN API\n    gain = model.booster_.feature_importance(importance_type='gain')\n    feat_imp = pd.DataFrame({'feature': model.feature_name_,\n                             'split': model.booster_.feature_importance(importance_type='split'),\n                             'gain': 100 * gain \/ gain.sum()}).sort_values('gain', ascending=False)\n    if plot:\n        plt.figure(figsize=(10, 10))\n        sns.set(font_scale=1)\n        sns.barplot(x=\"gain\", y=\"feature\", data=feat_imp[0:25])\n        plt.title('feature')\n        plt.tight_layout()\n        plt.show()\n    else:\n        print(feat_imp.head(num))\n        return feat_imp\n\nfeature_imp_df = plot_lgb_importances(first_model, num=50)","f718e5a8":"feature_imp_df.shape, feature_imp_df[feature_imp_df.gain > 0].shape, feature_imp_df[feature_imp_df.gain > 0.57].shape","b813b9a2":"plot_lgb_importances(first_model, plot=True, num=30)","9bd6947a":"explainer = shap.Explainer(first_model)\nshap_values_train = explainer(X_train)\nshap_values_valid = explainer(X_val)\n\nlen(shap_values_train), len(shap_values_valid)","c078ceb3":"# summarize the effects of all the features\nshap.plots.beeswarm(shap_values_train, max_display=30)","c8026bc1":"# summarize the effects of all the features\nshap.plots.beeswarm(shap_values_valid, max_display=30)","1ff43203":"shap.plots.bar(shap_values_train, max_display=30)","fc018a8a":"error = pd.DataFrame({\n    \"date\":val.date,\n    \"store\":X_val.store,\n    \"item\":X_val.item,\n    \"actual\":Y_val,\n    \"pred\":first_model.predict(X_val)\n}).reset_index(drop = True)\n\nerror[\"error\"] = np.abs(error.actual-error.pred)\n\nerror.sort_values(\"error\", ascending=False).head(20)","58123262":"error[[\"actual\", \"pred\", \"error\"]].describe([0.7, 0.8, 0.9, 0.95, 0.99]).T","e414ab25":"shap.plots.waterfall(shap_values_valid[30125])","1a606ee2":"# visualize the first prediction's explanation with a force plot\nshap.initjs()\nshap.plots.force(shap_values_valid[30125])","b41a6141":"shap.plots.waterfall(shap_values_valid[20669])","2b174dd4":"# visualize the first prediction's explanation with a force plot\nshap.initjs()\nshap.plots.force(shap_values_valid[20669])","7fe7e75c":"shap.plots.waterfall(shap_values_valid[9009])","c7c161a4":"# visualize the first prediction's explanation with a force plot\nshap.initjs()\nshap.plots.force(shap_values_valid[9009])","ec44beaa":"# Mean Absolute Error\nerror.groupby([\"store\", \"item\"]).error.mean().sort_values(ascending = False)","d891519b":"# Mean Absolute Error\nerror.groupby([\"store\"]).error.mean().sort_values(ascending = False)","2501f159":"# Mean Absolute Error\nerror.groupby([\"item\"]).error.mean().sort_values(ascending = False)","cabe505c":"# Store 1 Actual - Pred\nsub = error[error.store == 1].set_index(\"date\")\nfig, axes = plt.subplots(10, 5, figsize=(20, 35))\nfor i in range(1,51):\n    if i < 6:\n        sub[sub.item == i].actual.plot(ax=axes[0, i-1], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        sub[sub.item == i].pred.plot(ax=axes[0, i - 1], legend=True, label=\"Item \" + str(i) + \" Pred\", linestyle = \"dashed\")\n    if i >= 6 and i<11:\n        sub[sub.item == i].actual.plot(ax=axes[1, i - 6], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        sub[sub.item == i].pred.plot(ax=axes[1, i - 6], legend=True, label=\"Item \" + str(i) + \" Pred\",  linestyle=\"dashed\")\n    if i >= 11 and i<16:\n        sub[sub.item == i].actual.plot(ax=axes[2, i - 11], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        sub[sub.item == i].pred.plot(ax=axes[2, i - 11], legend=True, label=\"Item \" + str(i) + \" Pred\", linestyle=\"dashed\")\n    if i >= 16 and i<21:\n        sub[sub.item == i].actual.plot(ax=axes[3, i - 16], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        sub[sub.item == i].pred.plot(ax=axes[3, i - 16], legend=True, label=\"Item \" + str(i) + \" Pred\", linestyle=\"dashed\")\n    if i >= 21 and i<26:\n        sub[sub.item == i].actual.plot(ax=axes[4, i - 21], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        sub[sub.item == i].pred.plot(ax=axes[4, i - 21], legend=True, label=\"Item \" + str(i) + \" Pred\", linestyle=\"dashed\")\n    if i >= 26 and i<31:\n        sub[sub.item == i].actual.plot(ax=axes[5, i - 26], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        sub[sub.item == i].pred.plot(ax=axes[5, i - 26], legend=True, label=\"Item \" + str(i) + \" Pred\", linestyle=\"dashed\")\n    if i >= 31 and i<36:\n        sub[sub.item == i].actual.plot(ax=axes[6, i - 31], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        sub[sub.item == i].pred.plot(ax=axes[6, i - 31], legend=True, label=\"Item \" + str(i) + \" Pred\", linestyle=\"dashed\")\n    if i >= 36 and i<41:\n        sub[sub.item == i].actual.plot(ax=axes[7, i - 36], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        sub[sub.item == i].pred.plot(ax=axes[7, i - 36], legend=True, label=\"Item \" + str(i) + \" Pred\", linestyle=\"dashed\")\n    if i >= 41 and i<46:\n        sub[sub.item == i].actual.plot(ax=axes[8, i - 41], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        sub[sub.item == i].pred.plot(ax=axes[8, i - 41], legend=True, label=\"Item \" + str(i) + \" Pred\",linestyle=\"dashed\")\n    if i >= 46 and i<51:\n        sub[sub.item == i].actual.plot(ax=axes[9, i - 46], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        sub[sub.item == i].pred.plot(ax=axes[9, i - 46], legend=True, label=\"Item \" + str(i) + \" Pred\",linestyle=\"dashed\")\nplt.tight_layout(pad=4.5)\nplt.suptitle(\"Store 1 Item Sat\u0131\u015f Da\u011f\u0131l\u0131m\u0131\");\nplt.show()","00370066":"fig, axes = plt.subplots(4, 2, figsize = (20,20))\nfor axi in axes.flat:\n    axi.ticklabel_format(style=\"sci\", axis=\"y\", scilimits=(0,10))\n    axi.ticklabel_format(style=\"sci\", axis=\"x\", scilimits=(0,10))\n    axi.get_yaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n    axi.get_xaxis().set_major_formatter(plt.FuncFormatter(lambda x, loc: \"{:,}\".format(int(x))))\n    \n(error.actual-error.pred).hist(ax = axes[0, 0], color = \"steelblue\", bins = 20)\nerror.error.hist(ax = axes[0,1], color = \"steelblue\", bins = 20)\nsr = error.copy()\nsr[\"StandardizedR\"] = (sr.error \/ (sr.actual-sr.pred).std())\nsr[\"StandardizedR2\"] = ((sr.error \/ (sr.actual-sr.pred).std())**2)\nsr.plot.scatter(x = \"pred\",y = \"StandardizedR\", color = \"red\", ax = axes[1,0])\nsr.plot.scatter(x = \"pred\",y = \"StandardizedR2\", color = \"red\", ax = axes[1,1])\nerror.actual.hist(ax = axes[2, 0], color = \"purple\", bins = 20)\nerror.pred.hist(ax = axes[2, 1], color = \"purple\", bins = 20)\nerror.plot.scatter(x = \"actual\",y = \"pred\", color = \"seagreen\", ax = axes[3,0]);\n# QQ Plot\nimport statsmodels.api as sm\nimport pylab\nsm.qqplot(sr.pred, ax = axes[3,1], c = \"seagreen\")\nplt.suptitle(\"ERROR ANALYSIS\", fontsize = 20)\naxes[0,0].set_title(\"Error Histogram\", fontsize = 15)\naxes[0,1].set_title(\"Absolute Error Histogram\", fontsize = 15)\naxes[1,0].set_title(\"Standardized Residuals & Fitted Values\", fontsize = 15)\naxes[1,1].set_title(\"Standardized Residuals^2 & Fitted Values\", fontsize = 15)\naxes[2,0].set_title(\"Actual Histogram\", fontsize = 15)\naxes[2,1].set_title(\"Pred Histogram\", fontsize = 15);\naxes[3,0].set_title(\"Actual Pred Relationship\", fontsize = 15);\naxes[3,1].set_title(\"QQ Plot\", fontsize = 15);\naxes[1,0].set_xlabel(\"Fitted Values (Pred)\", fontsize = 12)\naxes[1,1].set_xlabel(\"Fitted Values (Pred)\", fontsize = 12)\naxes[3,0].set_xlabel(\"Actual\", fontsize = 12)\naxes[1,0].set_ylabel(\"Standardized Residuals\", fontsize = 12)\naxes[1,1].set_ylabel(\"Standardized Residuals^2\", fontsize = 12)\naxes[3,0].set_ylabel(\"Pred\", fontsize = 12)\nfig.tight_layout(pad=3.0)\nplt.savefig(\"errors.png\")\nplt.show()","51b15d8b":"# First model feature importance\ncols = feature_imp_df[feature_imp_df.gain > 0.015].feature.tolist()\nprint(\"Independent Variables:\", len(cols))\n\nsecond_model = lgb.LGBMRegressor(random_state=384).fit(\n    X_train[cols], Y_train, \n    eval_metric= lambda y_true, y_pred: [lgbm_smape(y_true, y_pred)])\n\nprint(\"TRAIN SMAPE:\", smape(Y_train, second_model.predict(X_train[cols])))\nprint(\"VALID SMAPE:\", smape(Y_val, second_model.predict(X_val[cols])))","3962c346":"# Best Params: {'num_leaves': 31, 'n_estimators': 15000, 'max_depth': 20}\n# model_tuned2 = lgb.LGBMRegressor(**rsearch.best_params_, random_state=384, metric = \"custom\")\n\nmodel_tuned2 = lgb.LGBMRegressor(num_leaves=31, n_estimators=15000, max_depth=20, random_state=384, metric = \"custom\")\n              \nmodel_tuned2.fit(\n    X_train[cols], Y_train,\n    eval_metric= lambda y_true, y_pred: [lgbm_smape(y_true, y_pred)],\n    eval_set = [(X_train[cols], Y_train), (X_val[cols], Y_val)],\n    eval_names = [\"Train\", \"Valid\"],\n    early_stopping_rounds= 1000, verbose = 500\n)\nprint(\"Best Iteration:\", model_tuned2.booster_.best_iteration)","2ef9d9c1":"df.sort_values([\"store\", \"item\", \"date\"], inplace = True)\n\ntrain_final = df.loc[(df[\"date\"] < \"2018-01-01\"), :]\ntest_final = df.loc[(df[\"date\"] >= \"2018-01-01\"), :]\n\nX_train_final = train_final[cols]\nY_train_final = train_final.sales\nX_test_final = test_final[cols]\n\n\n#final_model = lgb.LGBMRegressor(**rsearch.best_params_, random_state=384, metric = \"custom\") # Tuned parameters\n# Best Params: {'num_leaves': 31, 'n_estimators': 15000, 'max_depth': 20}\nfinal_model = lgb.LGBMRegressor(num_leaves=31, n_estimators=15000, max_depth=20, random_state=384, metric = \"custom\")\nfinal_model.set_params(n_estimators=model_tuned2.booster_.best_iteration) # Best Iteration: 983\nfinal_model.fit(X_train_final[cols], Y_train_final,\n                eval_metric= lambda y_true, y_pred: [lgbm_smape(y_true, y_pred)])","b5eaf316":"submission = pd.DataFrame({\n    \"id\":test_final.id.astype(int),\n    \"sales\":final_model.predict(X_test_final)\n})\nsubmission.to_csv(\"submission.csv\", index = None)","4b566e70":"submission[[\"sales\"]].describe([0.1, 0.75, 0.8, 0.9, 0.95, 0.99]).T","27d7b7e2":"submission.sales.hist(color = \"g\");","f1394c46":"forecast = pd.DataFrame({\n    \"date\":test_final.date,\n    \"store\":test_final.store,\n    \"item\":test_final.item,\n    \"sales\":final_model.predict(X_test_final)\n})\n\nforecast[(forecast.store == 1) & (forecast.item == 1)].set_index(\"date\").sales.plot(color = \"orange\", figsize = (20,9),legend=True, label = \"Store 1 Item 1 Forecast\");","99886cef":"train_final[(train_final.store == 1) & (train_final.item == 17)].set_index(\"date\").sales.plot(figsize = (20,9),legend=True, label = \"Store 1 Item 1 Sales\")\nforecast[(forecast.store == 1) & (forecast.item == 17)].set_index(\"date\").sales.plot(legend=True, label = \"Store 1 Item 1 Forecast\");","0c8c7aa2":"store = 1\nsub = train[train.store == store].set_index(\"date\")\nforc = forecast[forecast.store == store].set_index(\"date\")\n\n\nfig, axes = plt.subplots(10, 5, figsize=(20, 35))\nfor i in range(1,51):\n    if i < 6:\n        sub[sub.item == i].sales.plot(ax=axes[0, i-1], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        forc[forc.item == i].sales.plot(ax=axes[0, i-1], legend=True, label = \"Forecast\")\n    if i >= 6 and i<11:\n        sub[sub.item == i].sales.plot(ax=axes[1, i - 6], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        forc[forc.item == i].sales.plot(ax=axes[1, i-6], legend=True, label = \"Forecast\")\n    if i >= 11 and i<16:\n        sub[sub.item == i].sales.plot(ax=axes[2, i - 11], legend=True, label = \"Item \"+str(i)+\" Sales\") \n        forc[forc.item == i].sales.plot(ax=axes[2, i-11], legend=True, label = \"Forecast\")\n    if i >= 16 and i<21:\n        sub[sub.item == i].sales.plot(ax=axes[3, i - 16], legend=True, label = \"Item \"+str(i)+\" Sales\")    \n        forc[forc.item == i].sales.plot(ax=axes[3, i-16], legend=True, label = \"Forecast\")\n    if i >= 21 and i<26:\n        sub[sub.item == i].sales.plot(ax=axes[4, i - 21], legend=True, label = \"Item \"+str(i)+\" Sales\") \n        forc[forc.item == i].sales.plot(ax=axes[4, i-21], legend=True, label = \"Forecast\")\n    if i >= 26 and i<31:\n        sub[sub.item == i].sales.plot(ax=axes[5, i - 26], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        forc[forc.item == i].sales.plot(ax=axes[5, i-26], legend=True, label = \"Forecast\")\n    if i >= 31 and i<36:\n        sub[sub.item == i].sales.plot(ax=axes[6, i - 31], legend=True, label = \"Item \"+str(i)+\" Sales\")  \n        forc[forc.item == i].sales.plot(ax=axes[6, i-31], legend=True, label = \"Forecast\")\n    if i >= 36 and i<41:\n        sub[sub.item == i].sales.plot(ax=axes[7, i - 36], legend=True, label = \"Item \"+str(i)+\" Sales\")\n        forc[forc.item == i].sales.plot(ax=axes[7, i-36], legend=True, label = \"Forecast\")\n    if i >= 41 and i<46:\n        sub[sub.item == i].sales.plot(ax=axes[8, i - 41], legend=True, label = \"Item \"+str(i)+\" Sales\") \n        forc[forc.item == i].sales.plot(ax=axes[8, i-41], legend=True, label = \"Forecast\")\n    if i >= 46 and i<51:\n        sub[sub.item == i].sales.plot(ax=axes[9, i - 46], legend=True, label = \"Item \"+str(i)+\" Sales\") \n        forc[forc.item == i].sales.plot(ax=axes[9, i-46], legend=True, label = \"Forecast\")\nplt.tight_layout(pad=6.5)\nplt.suptitle(\"Store 1 Items Actual & Forecast\");","1c141020":"#### Correlation between total sales of stores","15e864db":"# Custom Cost Function\n\nIf you want to see the SMAPE formula, click [here](https:\/\/en.wikipedia.org\/wiki\/Symmetric_mean_absolute_percentage_error).","bcabdbae":"**First Model Scores**\n\n- TRAIN SMAPE: 13.1559\n- VALID SMAPE: 12.7387\n- 200 Features\n- Default hyperparameters\n\n**Second Model Scores**\n\n- TRAIN SMAPE: 13.1598\n- VALID SMAPE: 12.7291\n- 89 Features\n- Default hyperparameters\n\n**Third Model Scores**\n\n- TRAIN SMAPE: 9.4247\n- VALID SMAPE: 12.7840\n- 89 Features\n- Hyperparameter Tuning: Random Searched CV \n- {'num_leaves': 31, 'n_estimators': 15000, 'max_depth': 20}\n- Overfitting\n\n**Fourth Model Scores**\n\n- TRAIN SMAPE: 12.5183\n- VALID SMAPE: 12.4974\n- 89 Features\n- Hyperparameter Tuning: Random Searched CV \n- Best iteration number: 983\n\n","01b45638":"#### Store 1 Validation Set: Actual & Pred","4733544d":"# Next Model\n\n### Default Parameters & Feature Selection with LGBM Feature Importance","d80c99a5":"#### Second Optimization: Find best iteration number","ab70c734":"# Submission","314ebbc5":"# Feature Importance","3a23e782":"\n\n**First Model Scores**\n\n- TRAIN SMAPE: 13.1559\n- VALID SMAPE: 12.7387\n- 200 Features\n- Default hyperparameters\n\n**Second Model Scores**\n\n- TRAIN SMAPE: 13.1598\n- VALID SMAPE: 12.7291\n- 89 Features\n- Default hyperparameters\n\n**Third Model Scores**\n\n- TRAIN SMAPE: 9.4247\n- VALID SMAPE: 12.7840\n- 89 Features\n- Hyperparameter Tuning: Random Searched CV \n- {'num_leaves': 31, 'n_estimators': 15000, 'max_depth': 20}\n- Overfitting","930b80fa":"# Feature Engineering\n\n- Time Related Features\n- Lagged Features\n- Moving Average Features\n- Hypothesis Testing: Similarity Features\n- Exponentially Weighted Mean Features","2ef36a4c":"# Hyperparameter Tuning\n\nThere are two steps to tune LGBM models!\n\n- **1st Optimization:** Finding other parameters when the number of iterations is constant (GridSearchedCV, RandomSearchedCV etc.)\n- **2nd Optimization:** Finding best iteration number by using early stopping round\n\n**Hyperparameter tuning takes too long because of high iteration number and data dimension, that's why I add the Random Search CV algorithm below as text.**","7a9fecb9":"##### Items","663b1890":"# Shap","7a515fb9":"# Data","39632d87":"**First Model Scores**\n\n- TRAIN SMAPE: 13.1559\n- VALID SMAPE: 12.7387\n- 200 Features\n\n**Second Model Scores**\n- TRAIN SMAPE: 13.1598\n- VALID SMAPE: 12.7291\n- 89 Features","f5e2d43c":"# First Model\n\n### Default Parameters","dc33c77f":"#### Hypothesis Testing\n\n##### Stores","acdb0463":"# Packages","cb1d1290":"#### Sales distribution for each item in the 1st store","8286f35f":"# Error Analysis","0d8d1e7d":"#### Histogram: Store Sales","d05c3774":"<img src='https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/9999\/logos\/header.png?t=2018-06-28-21-19-41' \/>\n\n# Store Item Demand Forecasting\n\n**Description**\n\nThis competition is provided as a way to explore different time series techniques on a relatively simple and clean dataset.\n\nYou are given 5 years of store-item sales data, and asked to predict 3 months of sales for 50 different items at 10 different stores.\n\nWhat's the best way to deal with seasonality? Should stores be modeled separately, or can you pool them together? Does deep learning work better than ARIMA? Can either beat xgboost?\n\n**Evaluation**\n\nSubmissions are evaluated on [SMAPE](https:\/\/en.wikipedia.org\/wiki\/Symmetric_mean_absolute_percentage_error) between forecasts and actual values. We define [SMAPE](https:\/\/en.wikipedia.org\/wiki\/Symmetric_mean_absolute_percentage_error) = 0 when the actual and predicted values are both 0.\n\n**Variables:**\n- date\n- store\n- item\n- sales","06bcf026":"# Exploratory Data Analysis","37f3ae39":"# Train-Validation Split","7c5d6e5d":"# Final Model","529c7a26":"#### First Optimization: Hyperparameter Tuning with Random Searched\nlgbm_params = {\n    \n    \"num_leaves\":[20,31], # Default 31\n    \"max_depth\":[-1, 20, 30], # Default -1\n    \"learning_rate\":[0.1, 0.05], # Default 0.1\n    \"n_estimators\":[10000,15000], # Default 100\n    \"min_split_gain\":[0.0, 2,5], # Default 0\n    \"min_child_samples\":[10, 20, 30], # Default 20\n    \"colsample_bytree\":[0.5, 0.8, 1.0], # Default 1\n    \"reg_alpha\":[0.0, 0.5, 1], # Default 0\n    \"reg_lambda\":[0.0, 0.5, 1] # Default 0\n}\n\nmodel = lgb.LGBMRegressor(random_state=384)\n\n- from sklearn.model_selection import TimeSeriesSplit, GridSearchCV, RandomizedSearchCV\n- from sklearn.metrics import make_scorer\n\ntscv = TimeSeriesSplit(n_splits=3)\n\nrsearch = RandomizedSearchCV(model, lgbm_params, random_state=384, \n                             cv=tscv, scoring=make_scorer(smape),\n                             verbose = True, n_jobs = -1).fit(\n    X_train[cols], Y_train\n)\n\nprint(rsearch.best_params_)\n\n{'num_leaves': 31, 'n_estimators': 15000, 'max_depth': 20}\n\n**When Random Searched CV finished running, it gives us best parameters in sample paramater space. Then, we should train a new model with best parameters and evaluate model performance.**\n\nmodel_tuned = lgb.LGBMRegressor(**rsearch.best_params_, random_state=384).fit(X_train[cols], Y_train)\n\nprint(\"TRAIN SMAPE:\", smape(Y_train, model_tuned.predict(X_train[cols])))\n\nprint(\"VALID SMAPE:\", smape(Y_val, model_tuned.predict(X_val[cols])))\n\n- TRAIN SMAPE: 9.424761658139554\n- VALID SMAPE: 12.784089823495902"}}