{"cell_type":{"88c3d502":"code","f57c42f2":"code","6f1052b3":"code","2df7154a":"code","4ddc9b12":"code","bd00b70a":"code","00332344":"code","b4fcd851":"code","f7902129":"code","418acd69":"code","1aec486f":"code","fdeebfe7":"code","df641f36":"code","3423a25b":"code","578ab6ea":"code","3b564859":"code","16a0c5aa":"code","e1956b1e":"code","9d73d752":"code","fa80243f":"code","8ea4b5f0":"code","78b7ddcf":"code","8b689a8a":"code","661241b3":"code","4760083a":"code","4baae88a":"code","38bde717":"code","ba240157":"code","451a96d7":"code","507f4332":"code","eeaf28f8":"code","dbb44fbb":"code","e2ee6cba":"code","da52856e":"code","c2c38ee0":"code","f75d4879":"code","6c84d0db":"code","5516fa42":"code","eabaf13f":"code","b3a6cde5":"code","50c778bf":"code","478d2ed2":"code","8188db79":"code","05291ccc":"code","8840933d":"code","e200717a":"code","9ef3e9a3":"code","b70004fc":"code","45471f88":"code","064eaef2":"code","09632968":"code","f7be82bc":"markdown","7eb9c090":"markdown","ff0dde59":"markdown","6db9cc8f":"markdown","6293cb7e":"markdown","8e3319a9":"markdown","fa1f1b87":"markdown","28e66cd8":"markdown","6b4ddd24":"markdown","2860cfb7":"markdown","d5cf71c2":"markdown","8dee246c":"markdown","4b605fc9":"markdown","626d8fa3":"markdown","96e60091":"markdown","2197b3fb":"markdown","1f0d897d":"markdown","991d4d9c":"markdown","1c436f8e":"markdown","52924df3":"markdown","494de70e":"markdown","c6849c49":"markdown","0f7b3a24":"markdown","91ad08c3":"markdown","5933d74c":"markdown","d99ea53e":"markdown","aa780505":"markdown","58d0f49c":"markdown","40f6d2fb":"markdown","db97dcd6":"markdown","9ae7882d":"markdown","76822933":"markdown","392a964e":"markdown","4b6b2c8d":"markdown","cc7866bc":"markdown"},"source":{"88c3d502":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt # visualization\n!pip install seaborn as sns -q # visualization with seaborn v0.11.1\nimport seaborn as sns # visualization\nimport missingno as msno # missing values pattern visualization\n\n!pip install dabl -q\nimport dabl # quick exploration and model assessment\n\nimport warnings # supress warnings\nwarnings.filterwarnings('ignore')\n\n# set pandas display option\npd.set_option('display.max_columns', None)\npd.set_option('display.max_rows', None)\n\n# Load the data \ntrain_df = pd.read_csv('..\/input\/widsdatathon2021\/TrainingWiDS2021.csv')\ntest_df = pd.read_csv('..\/input\/widsdatathon2021\/UnlabeledWiDS2021.csv')\ndata_dictionary_df = pd.read_csv(\"..\/input\/widsdatathon2021\/DataDictionaryWiDS2021.csv\")\n\n# Drop first column because it is identical to index\ntrain_df.drop(\"Unnamed: 0\", axis = 1, inplace = True)\ntest_df.drop(\"Unnamed: 0\", axis = 1, inplace = True)\n\n# display the dataset\ntrain_df.head().style.set_caption('Sample of training data')\n\n# import os\n# for dirname, _, filenames in os.walk('\/kaggle\/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f57c42f2":"sns.__version__","6f1052b3":"# dimension of the dataset\nprint(f'Train_df shape is: {train_df.shape}')","2df7154a":"# dimension of the dataset\nprint(f'Test_df shape is: {test_df.shape}')","4ddc9b12":"# peek at the dataset\n# percentile list \nperc =[.20, .40, .60, .80] \n  \n# list of dtypes to include \ninclude =['object', 'float', 'int'] \ntrain_df.describe(percentiles = perc, include = include)\n# print(f'Train_df statistical summary: \\n{train_df.describe(percentiles = perc, include = include)}')","bd00b70a":"print({train_df.info(verbose = True, null_counts = True)})","00332344":"# let's check the class distribution\nprint(f'Class distribution in train_df: \\n{train_df.groupby(\"diabetes_mellitus\").size()}')","b4fcd851":"# investigating missing values per column sorted in Desc order in %\nround(train_df.isnull().mean().mul(100).sort_values(ascending = False), 2)","f7902129":"# set a treshhold - only trues should be either imputed or dropped -> will investigate more in EDA\ntreshold = 30\n(train_df.isnull().sum().sort_values()\/len(train_df) * 100) < treshold","418acd69":"\ndef missing_zero_values_table(df):\n        mis_val = df.isnull().sum()\n        mis_val_percent = round(df.isnull().mean().mul(100), 2)\n        mz_table = pd.concat([mis_val, mis_val_percent], axis=1)\n        mz_table = mz_table.rename(\n        columns = {df.index.name:'col_name', 0 : 'Missing Values', 1 : '% of Total Values'})\n        mz_table['Data_type'] = df.dtypes\n        mz_table = mz_table[\n            mz_table.iloc[:,1] != 0 ].sort_values(\n        '% of Total Values', ascending=False)\n        print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns and \" + str(df.shape[0]) + \" Rows.\\n\"      \n            \"There are \" + str(mz_table.shape[0]) +\n              \" columns that have missing values.\")\n        return mz_table.reset_index()","1aec486f":"missing = missing_zero_values_table(train_df)\nmissing[:20].style.background_gradient(cmap='Reds')","fdeebfe7":"missing_test = missing_zero_values_table(test_df)\n\nmissing_test[:20].style.background_gradient(cmap='Greens')","df641f36":"# Investigate categorical variables and how many levels each one has in one bug dictionary\ntemp = {str(k): list(v) for k, v in train_df.groupby(train_df.dtypes, axis=1)}\n","3423a25b":"for c in train_df:\n    if train_df[c].dtypes == 'object':\n        print(f'{c} has {train_df[c].unique()} \\n')\n        print(f'{train_df[c].value_counts()} \\n')\n        print(f'Missing count: {train_df[c].isnull().sum()}')\n        print('================')","578ab6ea":"def skew_test(df):\n    col = df.skew(axis = 0, skipna = True)\n    val = df.skew(axis = 0, skipna = True) \n    sk_table = pd.concat([col, val], axis = 1)\n    sk_table = sk_table.rename(\n    columns = {0 : 'skewness'})\n    print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns and \" + str(df.shape[0]) + \" Rows.\\n\"      \n        \"There are \" + str(sk_table.shape[0]) +\n          \" columns that have skewed values - Non Gaussian distribution.\")\n    return sk_table.drop([1], axis = 1).sort_values('skewness',ascending = False).reset_index()\n\n","3b564859":"skk = skew_test(train_df)\nskk[:20].style.background_gradient(cmap='Blues')","16a0c5aa":"# Use plotly to plot interactive bar chart\nimport plotly.graph_objects as go\n\nfig = go.Figure([go.Bar(x = train_df['diabetes_mellitus'].value_counts().index, y = train_df['diabetes_mellitus'].value_counts())])\nfig.update_traces(marker_color = 'rgb(0,200,0)', marker_line_color = 'rgb(0,255,0)',\n                  marker_line_width = 7, opacity = 0.6)\nfig.show()","e1956b1e":"target_df = train_df[['diabetes_mellitus']]\ntarget_df.shape","9d73d752":"cat_pred_df = train_df.select_dtypes('object')\ncat_train = pd.concat([cat_pred_df, target_df], axis = 1)\ncat_train.head().style.set_caption('Sample of training data for only categorical data')","fa80243f":"# Number of unique classes in each object column\ncat_train.select_dtypes('object').apply(pd.Series.nunique, axis = 0)","8ea4b5f0":"# function to plot the count of each variable\n\ndef count_plot(df, x_lab, hu):\n    sns.set_style('white')\n    sns.set_palette('Set1', 8, .75)\n    g = sns.factorplot(x = x_lab, hue = hu, data = df[df[x_lab].notnull()], kind = 'count')\n    g.set_xticklabels(rotation=90)\n    g.fig.suptitle(f'{x_lab} count plot')\n    g.fig.set_size_inches(10, 7)\n    ","78b7ddcf":"count_plot(df = cat_train, x_lab = 'ethnicity', hu = 'diabetes_mellitus')","8b689a8a":"count_plot(df = cat_train, x_lab = 'icu_stay_type', hu = 'diabetes_mellitus')","661241b3":"count_plot(df = cat_train, x_lab = 'icu_type', hu = 'diabetes_mellitus')","4760083a":"# function to plot the count of each category subcategories \ndef cat_plot(df, x_lab, hu, col, asp):\n    sns.set_style('white')\n    sns.set_palette('Set1', 8, .75)\n    t = sns.catplot(x = x_lab, hue = hu, col = col, \n                data = df, kind = 'count', height = 6, aspect = asp)\n    t.set_xticklabels(rotation=90)\n","4baae88a":"cat_plot(df = cat_train, x_lab = 'gender', hu = 'diabetes_mellitus', col = 'ethnicity', asp = 0.4)","38bde717":"cat_plot(df = cat_train, x_lab = 'hospital_admit_source', hu = 'diabetes_mellitus', col = 'gender', asp = 1.4)","ba240157":"cat_plot(df = cat_train, x_lab = 'icu_admit_source', hu = 'diabetes_mellitus', col = 'gender', asp = 1.4)","451a96d7":"# let's check the missing patterns\ncat_missing = missing_zero_values_table(cat_train)\ncat_missing.head()\nplt.figure(figsize=(10, 8))\nplt.title('Missing value percentage per each category')\nsns.diverging_palette(240, 10, n=5)\ns = sns.barplot(x = '% of Total Values', y = 'index', data = cat_missing)","507f4332":"plt.figure(figsize=(20, 10))\n\n# cubehelix palette is a part of seaborn that produces a colormap\ncmap = sns.cubehelix_palette(light=1, as_cmap=True, reverse=True)\nsns.heatmap(cat_train.isnull(), cmap=cmap)","eeaf28f8":"msno.dendrogram(cat_train)","dbb44fbb":"# take a subset of patient characteristic \nnum_pred_df = train_df.iloc[:, 0:45]\n# concat the target variable target_df\nnum_train = pd.concat([num_pred_df, target_df], axis = 1)\n# we may drop the unnecessary column which is encounter_id, hospital_id, \nnum_train.head()","e2ee6cba":"num_train.shape","da52856e":"# filter out constants and quazi constants\nfrom sklearn.feature_selection import VarianceThreshold\n# treshold = 0.1 -> constant\n\ndef variance_threshold_selector(df, threshold=0):\n    data = df.select_dtypes([np.int, np.float])\n    selector = VarianceThreshold(threshold)\n    selector.fit(data)\n    return data[data.columns[selector.get_support(indices=True)]]","c2c38ee0":"# df = num_train.select_dtypes([np.int, np.float])\n# df.head()\nsan_const = variance_threshold_selector(num_train, 0.1)\nsan_const.columns","f75d4879":"\n#  drop ids columns \ndf1 = san_const.drop(columns = ['encounter_id', 'hospital_id'])\n\n#  plot Numerical Data\na = 15  # number of rows\nb = 3  # number of columns\n\nfig = plt.figure(figsize=(15,30))\n\nfor i , col in enumerate(df1.columns, 1):\n    plt.subplot(a, b, i)\n    plt.title(f'{col} histplot')\n    sns.histplot(x = df1[col])\n\nplt.tight_layout()\nplt.show()\n    ","6c84d0db":"#  drop ids columns \ndf1 = san_const.drop(columns = ['encounter_id', 'hospital_id'])\n\n#  plot Numerical Data\na = 15  # number of rows\nb = 3  # number of columns\n\nfig = plt.figure(figsize=(15,30))\n\nfor i , col in enumerate(df1.columns, 1):\n    plt.subplot(a, b, i)\n    plt.title(f'{col} boxplot')\n    sns.boxplot(x = df1[col])\n\nplt.tight_layout()\nplt.show()","5516fa42":"df1.head()","eabaf13f":"import dabl\ntemp = dabl.clean(df1)\ndabl.plot(temp, 'diabetes_mellitus')","b3a6cde5":"cor_matrix = temp.corr()\ncor_matrix.style.background_gradient(cmap='Reds')","50c778bf":"# build correlation tree bwteen variables\nimport networkx as nx\n\n# Transform it in a links data frame (3 columns only):\nlinks = cor_matrix.stack().reset_index()\nlinks.columns = ['predictor_1', 'predictor_2', 'value']\nlinks\n\n# Keep only correlation over a threshold and remove self correlation (cor(A,A)=1)\nlinks_filtered=links.loc[ (links['value'] > 0.12) & (links['predictor_1'] != links['predictor_2']) ]\nlinks_filtered\n\n\n# Build your graph\n# plt.figure(figsize = (20, 20))\nG = nx.from_pandas_edgelist(links_filtered, 'predictor_1', 'predictor_2')\nplt.figure(3,figsize=(15,15)) \n# Plot the network:\nnx.draw(G, with_labels=True, node_color='red', node_size=200, edge_color='skyblue', node_shape = 's',alpha = 0.5, linewidths=1, font_size=12, pos = nx.circular_layout(G))\nplt.show()","478d2ed2":"#  drop ids columns \ndf1 = san_const.drop(columns = ['encounter_id', 'hospital_id'])\nsns.displot(x = 'age', hue = 'diabetes_mellitus', data= temp, kde = True)\n\nplt.show()","8188db79":"sns.displot(x = 'weight', hue = 'diabetes_mellitus', data= temp, kde = True)\n\nplt.show()","05291ccc":"sns.displot(x = 'height', hue = 'diabetes_mellitus', data= temp, kde = True)\n\nplt.show()","8840933d":"temp.shape","e200717a":"variables = [\"bmi\", \"weight\", \"height\", \"bun_apache\"]\nplt.figure(3,figsize=(10,10))\ng = sns.PairGrid(temp, hue=\"diabetes_mellitus\", vars=variables)\ng.map_diag(sns.histplot, hue=None, color=\".3\")\ng.map_offdiag(sns.scatterplot)\nplt.tight_layout()\nplt.show()","9ef3e9a3":"# take a subset of patient characteristic \nres_train = train_df.iloc[:, 46:]\n# we may drop the unnecessary column which is encounter_id, hospital_id, \nres_train.shape","b70004fc":"rest = dabl.clean(res_train)\ndabl.plot(rest, 'diabetes_mellitus')","45471f88":"rest.shape","064eaef2":"rest.head()","09632968":"cor_matrix = rest.corr()\ncor_matrix.style.background_gradient(cmap='Blues')","f7be82bc":"Exploring each category distribution with regards to the target variable","7eb9c090":"Majority of cases came from ER department","ff0dde59":"Getting more insights about each category sub categories","6db9cc8f":"- **Age**: It is a little skewed and biased within age group (60 - 80) -> we can consider binning or discretization for this variable to make it perform better on the final model. \n\n- **Elective surgery**, **ventilated_apache**, **intubated_apache**, **gcs_motor_pache**, **gcs_verbal_apache**, **gcs_eye_apache**, and **apache_post_operative** should considered a categorical variable instead of int -> change the type! also has imbalance - > may consider resampling for better performance incase considered to consume by the final model. \n\n- **Billirubin_apache** have less data and severly skewed! we may consider drop during the data preprocessing step. \n\n- Although **temp_apache** and **sodium_apache** has a significant number of missing data, it has a nice bell shape. We may consider an advance imputation technique. \n\n- **Apache_3j_dignosis** cannot decide the distribution!!\n\n- The rest of the numeric predictors are either has a normal distribution or bimodel distribution with two peaks! We need further visualization to investigate why that behaviour.  ","6293cb7e":"# Summarize the dataset","8e3319a9":"After removing the constant variables, I will drop the columns with id as is doesn't infer any insights out of the data. ","fa1f1b87":"Caucasians seems to have the majority amonge other ethnicities who admit to the hospital. On the other hand, Native American and Asians who admitted to the hospital less than 10,000. ","28e66cd8":"A scree plot displays how much variation each PCA captures from the data. As we can see from the Scree plot, the eigenvalue more than 8 explains about 60% of the cumulative variance. ","6b4ddd24":"# Numeric variables\n","2860cfb7":"It is obvious that there is an imbalance in the target class with about 78.3% biased to people who are free of diabetes ","d5cf71c2":"Let's get the correlation between variables","8dee246c":"# Work in progress....","4b605fc9":"There are colinearties in this part of the dataset. However, the majority of the variables didn't have a great effect of the target variable, only the glucose_apache of 0.354. Bmi has 0.169 but has a strong relationship with height and weight as expected. ","626d8fa3":"# Univariate Analysis\nWe will start with some univariate plots, that is, plots of each individual variable. Given that\nthe input variables are numeric, we can create box and whisker plots of each.","96e60091":"Check the variables skewness and sort them in a Desc order. ","2197b3fb":"Let's do some Bi-variate, multi-variate visualizations with the target variable","1f0d897d":"Sure we need to normalize height, weight, ... rest of the variables to better visualize the data and get sense of the real correlations!","991d4d9c":"Define a function to remove constants from the `num_train` with a threshold of 0.1 removes constants. ","1c436f8e":"We can see here that the target variable is presented as an `int64` not object representation!!","52924df3":"Check for rare labels, how many subcategories per each categorical variable. This would help when feature engineering to reduce the feature space to include important features only that would make the final model more robust. Eliminating rare labels, if doesn't have effect on the target variable, would ultimately enhance the predictive power. ","494de70e":"It seems that both icu_stay_type and transfer may be considered as rare labels when we come to the data processing step. ","c6849c49":"Investigating what type of missing values are \n\n* Missing Values\n* % of Total Values\n* Data Type","0f7b3a24":"Now, let's check the predictors statistics with the boxplot\n","91ad08c3":"We have 130,157 observations with 181 features including target variable `diabetes_mellitus`","5933d74c":"Check the test data missing values\n","d99ea53e":"It seems that `hospital_admit_source` would require attention to either impute or drop, will findout later!","aa780505":"# Introduction\nIn this notebook, we will try to comprehend the business problem to better understand the data - getting know each other \ud83d\ude0f\n\n# Goals\n\n* Explore the data, figure out if there is a pattern, bias, noise, imbalance, etc.\n* Do some exploratory summary statistics \n* Figure out a way to deal with such a huge feature space to be able to talk to the data.\n* Do exhaustive exploratory data analysis using communacative visualization\n* Create a to-do list of the mandatory preprocessing steps for data cleaning\n* Start preprocessing \n* Feature engineering\n* Model building\n* gauge performance\n\n# Updates\n\n> **01\/28\/2021**: \n    - Update seaborn to v0.11 \n    - Restructure the Notebook, hide code, ignore warnings\n    - Drop unnecessay columns in train dataset\n    - Numerical data visualization including uni-variate, bi-variate, multi-variate\n    \n<hr> \n\n> **01\/29\/2021**\n    - Use dabl for visualization - Thanks to [TensorGirl](https:\/\/www.kaggle.com\/usharengaraju\/widsdatathon2021-catboost-starter)\n    - Investigate correlation matrix for numerical variables\n    - start investigating the rest of the vars\n    - Build network correlation visuals\n\n<hr>\n\n# To do list\n\n- [X] Partion the data into subdata-frame.\n- [X] Summary statistics\n- [X] Categorical data visualization\n- [X] Numerical data visualization\n- [ ] Preprocessing categorical data, visualization\n- [ ] Preprocess numerical data\n- [ ] Feature selection\n- [ ] Split the data model preparation steps\n- [ ] Model evaluation\n- [ ] Model selection \n- [ ] Submission \n- [ ] re-iterate for enhancements.","58d0f49c":"We got some interesting insights \ud83e\udd14 \n\nLet's compare with the histplot. \n\n- **Age**: As we expected, the age within the senior group between (65 - 70). Also, has some outliers -> need treatment. \n\n- **Elective surgery**, **ventilated_apache**, **intubated_apache**, **gcs_motor_pache**, **gcs_verbal_apache**, **gcs_eye_apache**, and **apache_post_operative** more confirmation to consider as categorical variables. \n\n- **Billirubin_apache** most of the data seems to be outiers. \n\n- Although **temp_apache** and **sodium_apache** The majority of the points are outliers \n\n- **Apache_3j_dignosis** has outliers, skewness!\n\n- **Bmi**, **height** have outliers, and would consider standardization technique. \n ","40f6d2fb":"## Missing values patterns\n\nLet's find out if there is a pattern in the missing datapoints within the categorical variables","db97dcd6":"The target variable seems to have imbalance class! we may consider resampling procedure later\ud83e\udd14","9ae7882d":"# Visualize categorical variable\nFor simplicity, I will partitioning the train dataset into sub dataframes. \n* target_df: a dataframe that contains only target variable\n* cat_train: which contains the categorical data with the target_df\n* num_train: contains numerical variables that related to patient characteristics\n* prexist_train: contains variables related to if the patient has pre-existing conditions\n* measure_train: contains the rest of the variables in the dataset.\n","76822933":"## Results - Categorical variables\n\nIt seems that `hospital_admit_source` could be dropped in the preprocess step as from the dendrogram, there is a big difference in the cluster of missing values between target variable `diabetes_mellitus` and the `hospital_admit_source` variable. However, the `icu_admit_source` seems much closer to the target variable - has a shorter tree. \n\nFor the rest of the variables, we need to include into the model. However, we need to impute some of the variables. Also, we may consider to remove rare-labels like the `icu_admit_source` to reduce noise and feature space. \n\n<hr>","392a964e":"<hr>\n\n# Visualization\nFor this section, we would infer more insights from the data to get a more clear idea of the business problem. We will start by univariate visualation per feature - starting with categorical variables. After that, will investigate more using bivariate analysis, finally multivariate analysis to figure out the relations between features and target variable. ","4b6b2c8d":"It seems that we can safely drop some of the variables that doesn't have any effect on the target variable. `gcs_eyes`, `gcs_motor`, ` gcs_verbal`, and `temp_apache` have no direct or indirect effect on `diabetes_millitus`","cc7866bc":"Let's explore the distribution per variable.\n\nWe need first to drop constants from the dataset to reduce complexity space. "}}