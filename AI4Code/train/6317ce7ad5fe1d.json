{"cell_type":{"19fd877a":"code","5828d287":"code","73a02b22":"code","44c8f0e8":"code","a433a322":"code","95daf1e5":"code","bb6da06f":"code","3dc6df04":"code","567b04f9":"code","e49034fd":"code","ea6cef0d":"code","226e45e3":"code","c4c1c24c":"markdown","62623fc9":"markdown","e613fb41":"markdown","871bdd46":"markdown","037507b2":"markdown","ba3ba75c":"markdown","68273863":"markdown","ebb0d06d":"markdown"},"source":{"19fd877a":"\n\nimport pandas as pd\nimport numpy as np\nimport os\nimport re\nimport gensim\nimport spacy\nimport string\nfrom nltk.tokenize import word_tokenize\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import SnowballStemmer\nstemmer = SnowballStemmer(\"german\")\n\n\n\ndata_test = pd.read_csv('..\/input\/10k-german-news-articles\/Articles.csv')\ndata_test.head()","5828d287":"\nbody_new = [re.sub('<[^<]+?>|&amp', '', word) for word in data_test['Body'].values]\ndata_test['Body'] = body_new\n#! python -m spacy download de_core_news_sm\n","73a02b22":"! python -m spacy download de_core_news_sm\nimport spacy\nimport de_core_news_sm\nnlp = de_core_news_sm.load()\n\n\n#nlp = spacy.load('de_core_news_sm')\n\n","44c8f0e8":"## Drop Duplicates and NA values\n\ndata_test_clean = data_test.dropna()\ndata_test_clean = data_test_clean.drop_duplicates()","a433a322":"article_data = data_test_clean[['ID_Article','Title','Body']].drop_duplicates()\narticle_data.shape","95daf1e5":"\n## Convert the body text into a series of words to be fed into the model\n\ndef text_clean_tokenize(article_data):\n    \n    review_lines = list()\n\n    lines = article_data['Body'].values.astype(str).tolist()\n\n    for line in lines:\n        tokens = word_tokenize(line)\n        tokens = [w.lower() for w in tokens]\n        table = str.maketrans('','',string.punctuation)\n        stripped = [w.translate(table) for w in tokens]\n        # remove remaining tokens that are not alphabetic\n        words = [word for word in stripped if word.isalpha()]\n        stop_words = set(stopwords.words('german'))\n        words = [w for w in words if not w in stop_words]\n        words = [stemmer.stem(w) for w in words]\n\n        review_lines.append(words)\n    return(review_lines)\n    \n    \nreview_lines = text_clean_tokenize(article_data)","bb6da06f":"## Building the word2vec model\n\nmodel =  gensim.models.Word2Vec(sentences = review_lines,\n                               size=100,\n                               window=2,\n                               workers=4,\n                               min_count=2,\n                               seed=42,\n                               iter= 50)\n\nmodel.save(\"word2vec.model\")\n","3dc6df04":"import spacy\n\ndef tokenize(sent):\n    doc = nlp.tokenizer(sent)\n    return [token.lower_ for token in doc if not token.is_punct]\n\nnew_df = (article_data['Body'].apply(tokenize).apply(pd.Series))\n\nnew_df = new_df.stack()\nnew_df = (new_df.reset_index(level=0)\n                .set_index('level_0')\n                .rename(columns={0: 'word'}))\n\nnew_df = new_df.join(article_data.drop('Body', 1), how='left')\n\nnew_df = new_df[['word','ID_Article','Title']]\nword_list = list(model.wv.vocab)\nvectors = model.wv[word_list]\nvectors_df = pd.DataFrame(vectors)\nvectors_df['word'] = word_list\nmerged_frame = pd.merge(vectors_df, new_df, on='word')\nmerged_frame_rolled_up = merged_frame.drop('word',axis=1).groupby(['ID_Article','Title']).mean().reset_index()\ndel merged_frame\ndel new_df\ndel vectors","567b04f9":"merged_frame_rolled_up.columns","e49034fd":"\nfrom sklearn.decomposition import PCA\nfrom sklearn.manifold import TSNE\ndata_subset = merged_frame_rolled_up.drop(['ID_Article','Title'],axis=1).values\n\nfrom sklearn.cluster import KMeans\nnumber_of_clusters = range(1,20,5)\nkmeans = [KMeans(n_clusters=i,max_iter=5,init='k-means++') for i in number_of_clusters]\nscore = [kmeans[i].fit(data_subset).inertia_ for i in range(len(kmeans))]\ntmp =0\nbest_k = 0\nvalue_all = 0\ndiff1 = []\nfor i in range(len(score)-1):\n    \n    scores = (score[i] - score[i+1])\n    diff1.append(scores)\n    \ndiff2=[]\nfor i in range(len(diff1)-1):\n    difference = diff1[i] - diff1[i+1]\n    diff2.append(difference)\ndiff2.insert(0, 0) \ndiff3 = [i-j for i,j in zip(diff2,diff1)]\n\nm = max(i for i in diff3)\nbest_k = number_of_clusters[diff3.index(m)]\nprint(best_k)\n\nkmeans = KMeans(n_clusters=best_k,init='k-means++',max_iter=5).fit(data_subset)\nlabels = kmeans.labels_\nmerged_frame_rolled_up['labels'] = labels\n\n\n\n","ea6cef0d":"tsne = TSNE(n_components=2, verbose=1, perplexity=10, n_iter=3000,init='pca',random_state=42)\ntsne_results = tsne.fit_transform(data_subset)\n%matplotlib inline\nimport matplotlib.pyplot as plt\nfrom mpl_toolkits.mplot3d import Axes3D\nimport seaborn as sns\n\nmerged_frame_rolled_up['tsne-2d-one'] = tsne_results[:,0]\nmerged_frame_rolled_up['tsne-2d-two'] = tsne_results[:,1]\nplt.figure(figsize=(16,10))\nsns.scatterplot(\n    x=\"tsne-2d-one\", y=\"tsne-2d-two\",\n    hue = 'labels',\n    palette=sns.color_palette(\"hls\", len(merged_frame_rolled_up['labels'].unique())),\n    data=merged_frame_rolled_up,\n    legend=\"full\",\n    alpha=0.3\n)\n","226e45e3":"## Interactive t-SNE mapping using https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering\n\nfrom bokeh.models import ColumnDataSource, HoverTool, LinearColorMapper\nfrom bokeh.palettes import Category20\nfrom bokeh.transform import linear_cmap\nfrom bokeh.io import output_file, show\nfrom bokeh.transform import transform\nfrom bokeh.io import output_notebook\nfrom bokeh.plotting import figure\n\noutput_notebook()\n\n# data sources\nsource = ColumnDataSource(data=dict(\n    x= merged_frame_rolled_up['tsne-2d-one'].values, \n    y= merged_frame_rolled_up['tsne-2d-two'].values, \n    desc= merged_frame_rolled_up['labels'].values, \n    titles= merged_frame_rolled_up['Title'].values\n    ))\n\n# hover over information\nhover = HoverTool(tooltips=[\n    (\"Title\", \"@titles\")\n])\n# map colors\nmapper = linear_cmap(field_name='desc', \n                     palette=Category20[len(merged_frame_rolled_up['labels'].unique())],\n                     low=min(merged_frame_rolled_up['labels'].values) ,high=max(merged_frame_rolled_up['labels'].values))\n#prepare the figure\np = figure(plot_width=1000, plot_height=1000, \n           tools=[hover, 'pan', 'wheel_zoom', 'box_zoom', 'reset'], \n           title=\"Clustering German Articles Based on K Means and Word2Vec\", \n           toolbar_location=\"right\")\n# plot\np.scatter('x', 'y', size=5, \n          source=source,\n          fill_color=mapper,\n          line_alpha=0.3,\n          line_color=\"black\")\n\n# output_file('plots\/t-sne_covid-19_interactive.html')\nshow(p)\n","c4c1c24c":"After dropping rows that contain NA values and rows that have duplicated values, redundant words such as punctuation marks and stopwords are removed. As the texts are in German, the stopword corpus for German is used from the `spacy` library. After this preprocessing, the words are converted to a list of words to be fed into the model.","62623fc9":"## Interactive Visualization using Bokeh","e613fb41":"## K- Means Clustering\n\nAs the data set is un-labelled, its our job to find structure and label it accordingly. For this purpose, the K - Means clustering algorithm is used. 'K' or the number of clusters is chosen dynamically by looking K for which the difference between the first and second differential of change in distortion is maximum. This idea of dynamically choosing K can be found in this article. https:\/\/www.datasciencecentral.com\/profiles\/blogs\/how-to-automatically-determine-the-number-of-clusters-in-your-dat\n","871bdd46":"The word2vec representation of each word is used to build the representations for articles. The constitutent words's word2vec representations are rolled up and averaged. This averaged vector would be the numerical vector representation of the article in question.","037507b2":"To tackle stopwords in German, the spaCy module is used. The spaCy module consists of information regarding stopwords for different languages. The command `! python -m spacy download de_core_news_sm` enables the download of German- related module","ba3ba75c":"# Clustering German Articles using Word2Vec\n\n## Word2Vec\n\nWord2Vec is a method to represent words in a numerical - vector format such that words that are closely related to each other are close to each other in numeric vector space. This method was developed by Thomas Mikolov in 2013 at Google.\n\nEach word in the corpus is modeled against surrounding words, in such a way that the surrounding words get maximum probabilities. The mapping that allows this to happen , becomes the word2vec representation of the word. The number of surrounding words can be chosen through a model parameter called \"window size\". The length of the vector representation is chosen using the parameter 'size'.\n\nIn this notebook, the library gensim is used to construct the word2vec models\n\n## Loading the library and getting the data\n","68273863":"## Dropping NA values and Duplicate Articles\n\nFrom the pulled data, rows that contain NA values are dropped. Duplicate rows are also dropped. After this, the columns \"Headline\" & \"Body\" are chosen. Each article is indexed with the row number it belongs in.","ebb0d06d":"## Visualizing data\n\nAfter each data point is assigned a cluster, the next natural step is to visualize it. But there is a problem. The data we have has over 100 columns. We will need to reduce the number of columns to at most 3 to visualize it. Here to do that, we use t-SNE, which helps keep closer points together and farther points far from each other in lower dimensions. The visualization was made possible by using this kernel. https:\/\/www.kaggle.com\/maksimeren\/covid-19-literature-clustering#Dimensionality-Reduction-with-t-SNE\n"}}