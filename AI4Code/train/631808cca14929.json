{"cell_type":{"adb02653":"code","a3508d2f":"code","0f5cc5e5":"code","6dcb2e10":"code","81d2ec35":"code","b4b3bc5c":"code","529cefa2":"code","0f87fed5":"code","ace0cd2c":"code","59986873":"code","8d9b7ec5":"code","32ba50a0":"code","d4af8d09":"code","300f1a72":"code","9fc279d9":"code","0a5f8f8b":"code","f52d8bfd":"code","ef0502cb":"markdown","b52ffae1":"markdown","b55377a5":"markdown","a54bbc6f":"markdown","781b0935":"markdown","a28fe31a":"markdown","4a61d5e6":"markdown","6dff3001":"markdown","45f6b90c":"markdown","517c7b85":"markdown","eabc1269":"markdown"},"source":{"adb02653":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","a3508d2f":"column_names= ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']","0f5cc5e5":"df=pd.read_csv('\/kaggle\/input\/boston-house-prices\/housing.csv',delim_whitespace=True,names=column_names)\ndf.head()","6dcb2e10":"df.info()","81d2ec35":"df.info()","b4b3bc5c":"#checking for missing data\ndf.isnull().sum()","529cefa2":"plt.scatter(df.RM,df.MEDV)\nplt.xlabel(\"RM\")\nplt.ylabel(\"MEDV\")","0f87fed5":"from sklearn.linear_model import LinearRegression\nlinear_regression=LinearRegression()\nx=df.RM.values.reshape(-1,1)\ny=df.MEDV.values.reshape(-1,1)\nlinear_regression.fit(x,y)\n","ace0cd2c":"y_head=linear_regression.predict(x)\nplt.scatter(df.RM,df.MEDV)\nplt.xlabel(\"RM\")\nplt.ylabel(\"MEDV\")\nplt.plot(x,y_head,color=\"red\")\n","59986873":"#%% prediction\nb0=linear_regression.predict([[0]])\nb1=linear_regression.coef_\nprint(\"b0 is: \",b0)\nprint(\"b1 is: \",b1)\n\nprint(\"Predict: \",b1*80+b0)","8d9b7ec5":"from sklearn.metrics import r2_score\nprint(\"r2_score\",r2_score(y,y_head))","32ba50a0":"from sklearn.linear_model import LinearRegression\nx=df.iloc[:,[1,2,3,4,5]].values\ny=df.RM.values.reshape(-1,1)\nmultiple_linear_reg=LinearRegression()\nmultiple_linear_reg.fit(x,y)\nprint(\"b0 is:\",multiple_linear_reg.intercept_)\nprint(\"b1, b2,b3,b4,b5 are:\",multiple_linear_reg.coef_)","d4af8d09":"from sklearn.preprocessing import PolynomialFeatures\npolynomial_reg=PolynomialFeatures(degree=4)\nx_polynomial=polynomial_reg.fit_transform(x)\n","300f1a72":"from sklearn.linear_model import LinearRegression\npoly_reg=LinearRegression()\npoly_reg.fit(x_polynomial,y)\ny_head=poly_reg.predict(x_polynomial)\nplt.plot(x,y_head,color=\"olive\",label=\"polynomial\")\nplt.show()","9fc279d9":"from sklearn.metrics import r2_score\nprint(\"r2_score\",r2_score(y,y_head))","0a5f8f8b":"from sklearn.tree import DecisionTreeRegressor\nx=df.RM.values.reshape(-1,1)\ny=df.MEDV.values.reshape(-1,1)\nDtree=DecisionTreeRegressor()\nDtree.fit(x,y)\nx_=np.arange(min(x),max(x),0.01).reshape(-1,1)\ny_head=Dtree.predict(x_)\nplt.plot(x_,y_head,color=\"orange\")\nplt.show()","f52d8bfd":"from sklearn.ensemble import RandomForestRegressor\nrf=RandomForestRegressor(n_estimators=150,random_state=42)\nrf.fit(x,y)\nx_=np.arange(min(x),max(x),0.01).reshape(-1,1)\ny_head=rf.predict(x_)\nplt.plot(x_,y_head,color=\"purple\")\nplt.show()","ef0502cb":"MULTIPLE LINEAR REGRESSION","b52ffae1":"The variable we need predict is the **MEDV ** variable","b55377a5":"Regression, using the relationship between variables to find the best fit line or the regression equation that can be used to make preedictions.\nThe core idea is to obtain a line that best fits the data.\n\ny=b0+b1*x\nb0=coefficient\nb1=constant(bias)\n\nSimple linear regression is useful for finding relationship between two continuous variables. One is independent(x) variable and other is dependent(y) variable.\n\nSo i used CRIM column as x.\n ","a54bbc6f":"A decision tree is a supervised ml model used to predict a target by learning decision rules from features","781b0935":"There is no missing values in the data set.","a28fe31a":"Decision Tree Regression","4a61d5e6":"df.shape()","6dff3001":"Simple Linear Regression ","45f6b90c":"Polynomial Linear Regression","517c7b85":"Random Forest Regression","eabc1269":"the closer the result is to one, the more accurate the guess will be"}}