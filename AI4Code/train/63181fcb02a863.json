{"cell_type":{"61b5df4c":"code","73f29d46":"code","214705f5":"code","d7629c48":"code","dcaef7cf":"code","7789026e":"code","6cd7ba2b":"code","d21467f1":"code","27abb6ce":"code","a3caac9c":"code","39e00b5e":"code","dcbc889c":"code","f9f4de97":"code","f817dd18":"code","0332837c":"code","8477a917":"code","08e2d2fb":"code","150a92f5":"code","ee201c98":"code","26c008eb":"code","48cbeb47":"code","c4abe4e5":"code","bd96a344":"code","9224c479":"code","736773ad":"code","25a13cad":"code","84c1d864":"markdown","336bfb1a":"markdown","eb1e0f88":"markdown","3f75af57":"markdown","5699c58d":"markdown","fabd95e7":"markdown","323ad3c4":"markdown"},"source":{"61b5df4c":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nnp.set_printoptions(precision=4)\nfrom sklearn.metrics import accuracy_score, recall_score, precision_score , confusion_matrix, f1_score, roc_auc_score\nfrom sklearn.inspection import permutation_importance\nfrom sklearn.metrics import classification_report\nfrom sklearn.model_selection import GridSearchCV\nfrom IPython.display import display,Markdown,HTML\nimport warnings\nwarnings.filterwarnings('ignore')","73f29d46":"df = pd.read_csv('\/kaggle\/input\/hr-employee-attrition\/HR-Employee-Attrition.csv')\ndf.head(5)","214705f5":"df.info()","d7629c48":"df.duplicated().sum()","dcaef7cf":"df.isna().sum()","7789026e":"df['Attrition'] = df['Attrition'].map({\"No\":0, \"Yes\":1})","6cd7ba2b":"text_negative = \"Negative\"\ntext_positive = \"Positive\"\ntarget_column = \"Attrition\"\n\ndf_all = df.copy()\n\ndf_positive = df[df[target_column]==1]\n\ndf_negative = df[df[target_column]==0]","d21467f1":"def plot_pie(column, title=\"All Group\/Class\"):\n    fig,axs = plt.subplots(1,1)\n    data = df_all[column].value_counts()\n    plt.pie(data,autopct='%1.2f%%',labels=data.index)\n    plt.title(title)\n    plt.show()\n    \ndef plot_hist(column, title=\"All Group\/Class\"):\n    plt.hist(df_all[column],density=True)\n    plt.title(title)\n    plt.show()\n\ndef plot_bar(column, sort=False, title=\"All Group\/Class\"):\n    if sort:\n        data_all = df_all[column].value_counts().sort_index()\n    else:\n        data_all = df_all[column].value_counts()\n    plt.bar(data_all.index.astype(str),data_all)\n    plt.title(title)\n    plt.show()\n    \ndef plot_bar_compare(column, sort=False):\n    if sort:\n        data_positive = df_positive[column].value_counts().sort_index()\n        data_negative = df_negative[column].value_counts().sort_index()\n    else:\n        data_positive = df_positive[column].value_counts()\n        data_negative = df_negative[column].value_counts()\n    \n    fig,axs = plt.subplots(2,1)\n    plt.subplots_adjust(left=0, bottom=0, right=1, top=2, wspace=0, hspace=0.2)\n    axs[0].bar(data_negative.index.astype(str),data_negative)\n    axs[0].title.set_text(text_negative)\n    axs[1].bar(data_positive.index.astype(str),data_positive)\n    axs[1].title.set_text(text_positive)\n    plt.show()\n\ndef plot_hist_compare(column, bins=5):\n    plt.hist([df_negative[column], df_positive[column]] , color=['c','r'])\n    plt.legend((text_negative, text_positive))\n    plt.show()\n    \ndef plot_pie_compare(column):\n    data_positive = df_positive[column].value_counts()\n    data_negative = df_negative[column].value_counts()\n    \n    fig,axs = plt.subplots(2,1)\n    plt.subplots_adjust(left=0, bottom=0, right=1, top=2, wspace=0, hspace=0.2)\n    axs[0].pie(data_negative,autopct='%1.2f%%',labels=data_negative.index)\n    axs[0].title.set_text(text_negative)\n    axs[1].pie(data_positive,autopct='%1.2f%%',labels=data_positive.index)\n    axs[1].title.set_text(text_positive)\n    plt.show()\n\ndef plot_boxplot(column, title=\"\"):\n    ax = sns.boxplot(x=target_column, y=column, palette=[\"c\", \"r\"],\n            hue=target_column,  data=df_all).set_title(title, fontsize=15)\n    plt.show()\n\ndef check_median(column):\n    data_negative = df_negative[column].describe()\n    data_positive = df_positive[column].describe()\n    print(\"Median:\")\n    print('{}: {}'.format(text_negative,data_negative['50%']))\n    print('{}: {}'.format(text_positive,data_positive['50%']))\n\ndef check_most(column):\n    data_negative = df_negative[column].value_counts()\n    data_positive = df_positive[column].value_counts()\n    print(\"Most:\")\n    print('{}: {}'.format(text_negative,data_negative.index[0]))\n    print('{}: {}'.format(text_positive,data_positive.index[0]))","27abb6ce":"def eda(df_all):\n    display(HTML('<h1>Exploratory Data Analysis<h1>'))\n    \n    for column in df_all.columns:\n        if column == target_column:\n            continue\n        display(HTML('<h2>{}<h2>'.format(column)))\n        if df[column].dtype == 'int64' or df[column].dtype == 'float64':\n            if len(df[column].unique())>10 :\n                plot_boxplot(column)\n                check_median(column)\n            else:\n                plot_bar(column)\n                plot_pie(column)\n                plot_pie_compare(column)\n                check_most(column)\n        elif df[column].dtype == 'object':\n            if len(df[column].unique())>10 :\n                df[column].value_counts().head(5)\n                df_negative[column].value_counts().head(5)\n                df_positive[column].value_counts().head(5)\n            else:\n                plot_bar(column)\n                plot_pie(column)\n                plot_pie_compare(column)\n                check_most(column)\n        else:\n            None","a3caac9c":"df['Attrition'].value_counts()","39e00b5e":"plot_pie('Attrition')","dcbc889c":"eda(df_all)","f9f4de97":"X = df.copy()\n\ny = X[target_column]\n\nX = X.drop([target_column,'Over18',\"EmployeeCount\",\"EmployeeNumber\",\"HourlyRate\",\"MonthlyRate\",\"PercentSalaryHike\",\"StandardHours\",\"YearsSinceLastPromotion\"], axis=1)","f817dd18":"X = pd.get_dummies(X, columns=[\"BusinessTravel\",\"Department\",\"EducationField\",\"Gender\",\"JobRole\",\"MaritalStatus\",\"OverTime\"],drop_first=True)","0332837c":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 1234)","8477a917":"from imblearn.over_sampling import SMOTE, RandomOverSampler\nfrom imblearn.under_sampling import RandomUnderSampler\n\n#sm = SMOTE(random_state=1234)\nros = RandomOverSampler(sampling_strategy='minority',random_state=1234)\n#rus = RandomUnderSampler(sampling_strategy='majority', random_state=1234)\n\n#X_balance, y_balance  = sm.fit_resample(X_train, y_train)\nX_balance, y_balance = ros.fit_resample(X_train, y_train)\n#X_balance, y_balance = rus.fit_resample(X_train, y_train)\n\nprint(f'''Shape of X before Balancing: {X.shape}\nShape of X after Balancing: {X_balance.shape}''')\n\nprint('\\nBalance of positive and negative classes (%):')\ny_balance.value_counts(normalize=True) * 100","08e2d2fb":"from sklearn.preprocessing import StandardScaler\nsc = StandardScaler()\n\nX_balance = sc.fit_transform(X_balance)\nX_test = sc.transform(X_test)","150a92f5":"# Import ML Libraries\nfrom xgboost import XGBClassifier\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\nfrom sklearn.svm import SVC\nfrom catboost import CatBoostClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.linear_model import LogisticRegression\n\nclassifiers = [[CatBoostClassifier(verbose=0),'CatBoost Classifier'],[XGBClassifier(eval_metric='error'),'XGB Classifier'], [RandomForestClassifier(),'Random Forest'], \n    [KNeighborsClassifier(), 'K-Nearest Neighbours'], [SGDClassifier(),'SGD Classifier'], [SVC(),'SVC'],[LGBMClassifier(),'LGBM Classifier'],\n              [GaussianNB(),'GaussianNB'],[DecisionTreeClassifier(),'Decision Tree Classifier'],[LogisticRegression(),'Logistic Regression'],[AdaBoostClassifier(),\"AdaBoostClassifier\"]]","ee201c98":"for cls in classifiers:\n    model = cls[0]\n    model.fit(X_balance, y_balance)\n    \n    y_pred = model.predict(X_test)\n    print(cls[1])\n    print ('Confusion Matrix:')\n    print(confusion_matrix(y_test, y_pred))\n    print(\"Accuracy : \", accuracy_score(y_test, y_pred) *  100)\n    print(\"Recall : \", recall_score(y_test, y_pred) *  100)\n    print(\"Precision : \", precision_score(y_test, y_pred) *  100)\n    print(\"F1 : \", f1_score(y_test, y_pred) *  100)\n    print(\"ROC AUC : \", roc_auc_score(y_test, y_pred) *  100)\n    print(\"\\n\")","26c008eb":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom keras.layers import Dropout\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.optimizers import Adam\nfrom keras.losses import BinaryCrossentropy\nfrom numpy.random import seed\n\nseed(1234)\ntf.random.set_seed(1234)","48cbeb47":"#train the model\nmodel = Sequential()\nmodel.add(Dense(32, input_shape=(X_train.shape[1],), activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(32, activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(16, activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(8, activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(4, activation='relu')),\nmodel.add(Dropout(0.2)),\nmodel.add(Dense(1, activation='sigmoid'))","c4abe4e5":"opt = Adam(learning_rate=0.001)\nearlystopper = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy',mode='max',patience=15, verbose=1,restore_best_weights=True)\nmodel.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\nhistory = model.fit(X_balance, y_balance, batch_size=32, epochs=200,validation_split = 0.15, callbacks = [earlystopper],verbose = 1)\nhistory_dict = history.history","bd96a344":"loss_values = history_dict['loss']\nval_loss_values=history_dict['val_loss']\nplt.plot(loss_values,'b',label='training loss')\nplt.plot(val_loss_values,'r',label='val training loss')\nplt.legend()\nplt.xlabel(\"Epochs\")","9224c479":"accuracy_values = history_dict['accuracy']\nval_accuracy_values=history_dict['val_accuracy']\nplt.plot(val_accuracy_values,'-r',label='val_accuracy')\nplt.plot(accuracy_values,'-b',label='accuracy')\nplt.legend()\nplt.xlabel(\"Epochs\")","736773ad":"y_pred = model.predict(X_test)\ny_pred = (y_pred > 0.5)\ny_pred = [1 if x == True else 0 for x in y_pred]","25a13cad":"print(confusion_matrix(y_test, y_pred))\nprint(\"Accuracy : \", accuracy_score(y_test, y_pred) *  100)\nprint(\"Recall : \", recall_score(y_test, y_pred) *  100)\nprint(\"Precision : \", precision_score(y_test, y_pred) *  100)\nprint(\"F1 : \", f1_score(y_test, y_pred) *  100)\nprint(\"ROC AUC : \", roc_auc_score(y_test, y_pred) *  100)","84c1d864":"# Attrition","336bfb1a":"The data is imbalanced","eb1e0f88":"# Check for duplicated data ","3f75af57":"# Data Preprocessing","5699c58d":"# Artificial Neural Network","fabd95e7":"# Check for missing data","323ad3c4":"The best algorithm is **XGB Classifier**\n\n* Accuracy :  88.09523809523809\n* Recall :  44.680851063829785\n* Precision :  70.0\n* F1 :  54.54545454545453\n* ROC AUC :  70.51856318373675"}}