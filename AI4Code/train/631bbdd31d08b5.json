{"cell_type":{"768c0f62":"code","3b483f40":"code","d84bab87":"code","b774d521":"code","f5261bb6":"code","6adbf327":"code","ef160e58":"code","cd043863":"code","2e90f08a":"code","91d85bb0":"code","902864d1":"code","0f85ba04":"code","c14c182b":"code","368dff53":"code","c6ae9d18":"code","901e03ff":"code","70d9e4ad":"code","0d19ce30":"code","bdafc26c":"code","9d60ce83":"code","af5afc45":"code","45db2ce8":"code","bb010d15":"code","74eb7786":"code","03d3b9a1":"code","4e69bebf":"code","8dea344a":"code","4d301286":"code","66864ea8":"code","427c612f":"code","bf4dedd4":"code","709f1965":"code","3bbe7f36":"code","c249308f":"code","a5113cde":"code","6dd20b8b":"markdown","7f3ad995":"markdown","0c74dd40":"markdown","d73cb579":"markdown","24a561cd":"markdown","ff03a4e1":"markdown","c4f66091":"markdown","f4d37b38":"markdown","ad36450a":"markdown","988d6f5d":"markdown","d1c8b70f":"markdown"},"source":{"768c0f62":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\n # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport statsmodels.formula.api as sm\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import linear_model\nimport random\nfrom sklearn.metrics import mean_squared_error\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nimport pylab \nimport scipy.stats as stats\nimport math\nimport statsmodels.api as sm\nfrom statsmodels.sandbox.regression.predstd import wls_prediction_std\nfrom statsmodels.stats.stattools import durbin_watson\nfrom statsmodels.compat import lzip\nimport statsmodels.stats.api as sms\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3b483f40":"df1 = pd.read_csv(r'\/kaggle\/input\/used-car-data\/data_casestudy.csv')","d84bab87":"df1.head()","b774d521":"# Find % of MV in each variable\np_mv_df=pd.DataFrame(df1.isnull().sum()*100\/df1.shape[0]).rename(columns={0:'p_mv'})\np_mv_df.head()","f5261bb6":"#remove cols with>15% MV\nrem_val=list(filter(lambda x: x>15,p_mv_df['p_mv']))\nrem_cols=list(pd.DataFrame(p_mv_df.loc[p_mv_df[\"p_mv\"].isin(rem_val)]).index)\nl1=list(df1.columns)\nl2=rem_cols\nkeep_cols=[x for x in l1 if x not in l2]","6adbf327":"df2=df1[keep_cols]\np_mv=df2.isnull().sum()*100\/df1.shape[0]\np_mv\n# good to go! no more columns with>15% MV","ef160e58":"# find cols with MV for imputation\nmv_vals=list(filter(lambda x: x>0,p_mv))\nmv_cols=list(pd.DataFrame(p_mv_df.loc[p_mv_df[\"p_mv\"].isin(mv_vals)]).index)\ndf_temp=df2[mv_cols]\ndf_temp.head()","cd043863":"# since all MV cols are categorical, impute them with their modes\ndf3=df2.copy()\ndf3.apply(lambda x:x.fillna(x.value_counts().index[0]))\ndf3.head()","2e90f08a":"#calculating a decent size for bins \n(df1[\"HVB\"].max()-df1[\"HVB\"].min())\/50\nplt.hist(df1['HVB'], 50, facecolor='blue', alpha=0.5)\nplt.show()","91d85bb0":"# doesn't look normal, so try log transformation\nlog_HVB=np.log(df1['HVB'])\nplt.hist(log_HVB, 50, facecolor='blue', alpha=0.5)\nplt.show()\n# looks pretty normal, good to go!","902864d1":"# correlation matrix for numerical variables\ncorr_mat=df3.corr()\ncorr_mat","0f85ba04":"for i in corr_mat:\n    col=corr_mat[i]\n    print('\\033[1m' + i.center(50) +'\\033[0m')\n    for j in col:\n        if abs(j)>0.8:\n            print(corr_mat.loc[corr_mat[i]==j,i],'\\n')\n# since none of the variables are highly correlated with each other, we can't remove any on the basis of multicollinearity","c14c182b":"corr_mat1=corr_mat.reset_index().rename(columns={'index':'var'})\nlow_corr_num=[]\nfor i in corr_mat1['HVB']:\n    df5=pd.DataFrame(corr_mat1.loc[corr_mat1['HVB']==i,('var')])\n    if abs(i)<0.2:\n        low_corr_num.append(df5['var'].values[0])\nprint(low_corr_num)","368dff53":"num_df=df3._get_numeric_data()\nhigh_corr_num=num_df.columns.drop(low_corr_num)\nhigh_corr_num=list(set(high_corr_num))\nhigh_corr_num","c6ae9d18":"low_var_cat=[]\nfor i in cat_df:\n    col=i\n    df4=pd.DataFrame(cat_df[i].value_counts()\/len(cat_df[i])).reset_index().rename(columns={'index':i,i:'freq'})\n    for j in df4['freq']:\n        if j>=0.7:\n            low_var_cat.append(i)\nprint(low_var_cat)","901e03ff":"num_df=df3._get_numeric_data()\ncat_df=df3.drop(num_df.columns,axis=1)\nhigh_var_cat=cat_df.columns.drop(low_var_cat)\nhigh_var_cat=list(set(high_var_cat))\n# logic dictates that inspection_date won't have an effect on HVB, so we can discard that as well\nhigh_var_cat.remove('inspection_date')\nhigh_var_cat","70d9e4ad":"final=[]\nfinal=high_corr_num\nfinal.extend(high_var_cat)\nfinal=list(set(final))\nfinal","0d19ce30":"df6=df3.loc[:,final]\ndf6.head()","bdafc26c":"df7=pd.concat([df6,log_HVB],axis=1)\ndf7.columns.values[-1]='log_HVB'\ndf7.head()","9d60ce83":"dep_df=df6.iloc[:,1:]\ndep_cols=dep_df.columns\ndep_df.head()","af5afc45":"dumm_df=pd.get_dummies(dep_df,drop_first=True)\ndumm_df.head()","45db2ce8":"num_df=df7._get_numeric_data()\nnum_cols=list(num_df.columns)\nnum_cols","bb010d15":"for i in num_cols:\n    num_df.boxplot(column=[i])\n    plt.show()\n    plt.close()\n    q1=np.percentile(num_df[i],25)\n    q3=np.percentile(num_df[i],75)\n    iqr=q3-q1\n    x=[]\n    y=[]\n    for j in num_df[i]:\n        if j>=q1-1.5*iqr:\n            x.append(j)\n    print(min(x))\n    for j in num_df[i]:\n        if j<=q3+1.5*iqr:\n            y.append(j)\n    print(max(y))","74eb7786":"num_df1=num_df.copy(deep=True)\n# num_df1['lower_whisker']=np.nan\n# num_df1['upper_whisker']=np.nan\n\n# ind=0\nfor i in num_cols:\n#     num_df.boxplot(column=[i])\n#     plt.show()\n#     plt.close()\n    q1=np.percentile(num_df1[i],25)\n    q3=np.percentile(num_df1[i],75)\n    iqr=q3-q1\n    x=[]\n    y=[]\n    for j in num_df1[i]:\n        if j>=q1-1.5*iqr:\n            x.append(j)\n#     print(min(x))\n#     print(ind)-\n    \n#     print(min(x))\n#     for j in num_df1[i]:\n        if j<=q1+1.5*iqr:\n            y.append(j)\n    num_df1.loc[:,'lower_whisker'+'_'+i]=min(x)\n    num_df1.loc[:,'upper_whisker'+'_'+i]=max(y)\n#     print(max(x))\n#     ind+=1","03d3b9a1":"df8=df7.drop(['HVB'],axis=1)\nnum_df1=df8._get_numeric_data()\ncat_df1=df8.drop(num_df1.columns,axis=1)\ndumm_df1=pd.get_dummies(cat_df1,drop_first=True)\ndf9=pd.concat([num_df1,dumm_df1],axis=1)\ndf9.head()\n# df3._get_numeric_data()","4e69bebf":"# dividing into test and train data\nrandom.seed(7)\n# X_train,X_test,y_train,y_test=train_test_split(df7,log_HVB,test_size=0.30)\ndivide=train_test_split(df9,test_size=0.30)\ndf_train=divide[0]\ndf_test=divide[1]\n\nX_test=df_test.drop(['log_HVB'],axis=1)\ny_test=df_test['log_HVB'].reset_index()['log_HVB']\ny_train=df_train['log_HVB']\nX_train=df_train.drop(['log_HVB'],axis=1)","8dea344a":"# fitting into model\nlm=linear_model.LinearRegression()\nmodel=lm.fit(X_train,y_train)\npredicted=lm.predict(X_test)\n\ny_pred=pd.Series(predicted)\ntable=pd.concat([y_pred, y_test],axis=1)\ntable.rename(columns={0:\"y_pred\",\"log_HVB\":\"y_test\"},inplace=True)\ntable['error']=abs(table['y_test']-table[\"y_pred\"])","4d301286":"# calculating summary statistics\nroot_mse = mean_squared_error(table['y_pred'],table['y_test'])**0.5\nMAPE = 100*(abs(table['y_pred']-table[\"y_test\"])\/table[\"y_test\"]).mean()\nr_squared = lm.score(X_test,y_test)\nadjusted_r_squared = 1 - (1-r_squared)*(len(y_test)-1)\/(len(y_test)-X_test.shape[1]-1)\n\nprint ('Adjusted r squared:%.2f' % adjusted_r_squared)\nprint(\"Mean squared error: %.2f\" % root_mse)\nprint('MAPE: %.2f' % MAPE)\n\n# high adjusted r^2 and low MSE and MAPE is good news!","66864ea8":"X = num_df1\n# X.drop(['owner_number','HVB','rating_accessoriesFeatures','rating_airConditioning','rating_electricalsInterior','rating_engineTransmission','rating_exteriorTyres','rating_steeringSuspensionBrakes'],axis=1,inplace=True)\nvif = pd.DataFrame()\nvif[\"VIF Factor\"] = [variance_inflation_factor(X.values, i) for i in range(X.shape[1])]\nvif[\"features\"] = X.columns\nvif\n\n# VIF is high but this shouldn't be a problem as long as only prediction is our objective","427c612f":"#QQ-plot\nsm.qqplot(y_test, loc = 20, scale = 5,  line='q')\npylab.show()\n\n# looks fairly normal","bf4dedd4":"model1=sm.OLS(y_train,X_train)\nresult=model1.fit()","709f1965":"durbin_watson(result.resid)\n\n# Durbin-Watson statistic will always have a value between 0 and 4\n# a value of 2.0 means that there is no autocorrelation detected in the sample -> great! means we haven't omitted any \n# impt vars","3bbe7f36":"import statsmodels as sms\nname=['Lagrange multiplier statistic', 'p-value','f-value', 'f p-value']\ntest=sms.stats.diagnostic.het_breuschpagan(result.resid, result.model.exog)\nlzip(name, test)","c249308f":"plt.scatter(np.exp(table['y_pred']),np.exp(table['error']))\nplt.show()\n\n# no fanning as X increases => no cause for concern","a5113cde":"# Please comment if you have any doubts\n\n#Thank you","6dd20b8b":"**Autocorrelation**","7f3ad995":"**Heteroscedasticity**","0c74dd40":"# Removing Numeric Variables that don't Have a High Correlation with HVB","d73cb579":"Below is an example of an simple Linear Regression model with all the assumptions check. \nIn the script you will find the following:\n\n* Data Understanding and Cleaning\n* Check if dependent variable is normally distributed\n* Removing Numeric Variables that don't Have a High Correlation with dependent variable\n* Removing Categorical Variables that have low variation\n* Check for Outlier\n* Modelling\n* Assumption Check (Multicollinearity,Autocorrelation,Normality and Heteroscedasticity","24a561cd":"**Normality**","ff03a4e1":"# Checking Assumptions\n\n**Multicollinearity**","c4f66091":"# Check for Outliers","f4d37b38":"# Removing Categorical Variables that Don't Vary Much","ad36450a":"# DATA UNDERSTANDING AND CLEANING\n\n**Treating Missing Values**","988d6f5d":"\n# Removing Highly Correlated Variables","d1c8b70f":"# Check if Dependent Variable is Normally Distributed"}}