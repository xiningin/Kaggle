{"cell_type":{"defa0bc0":"code","f99c04b0":"code","a0364461":"code","e1300016":"code","714e27ba":"code","6951ec24":"code","0e6c4b3d":"code","8a9c0b0c":"code","e5210dde":"code","b360a411":"code","5df208f5":"code","9bedf90b":"code","013c8a25":"code","4b3063ce":"code","e1d86034":"code","1edd7979":"code","1671c869":"code","e50f3173":"code","fdf1d3dc":"code","983a15b7":"code","0875061c":"code","609022bf":"code","54f43efd":"code","bd855f41":"code","6ff3faf1":"code","e53be25a":"code","bac81928":"code","bad20f8a":"code","84e445e8":"markdown","20455b2c":"markdown","3f5bcb07":"markdown","451e6681":"markdown","3849e6d9":"markdown","74300a03":"markdown","13407ce4":"markdown","ef0eb96e":"markdown","dcd51f38":"markdown","38d38f88":"markdown","05617b76":"markdown","05e3d27e":"markdown","cc9c2ead":"markdown","88b11f15":"markdown"},"source":{"defa0bc0":"# !pip install ..\/input\/my-wheels\/scikit_learn-0.21.0-cp37-cp37m-manylinux1_x86_64.whl\n\n! pip install scikit_learn==0.21.0","f99c04b0":"import numpy as np\nimport pandas as pd \nfrom tqdm.notebook import tqdm\nimport os\nimport gc\nimport skopt\nimport random\n\nfrom skopt import gp_minimize, forest_minimize\nfrom skopt.space import Real, Categorical, Integer\nfrom skopt.plots import *\nfrom skopt.utils import use_named_args\n\nimport tensorflow as tf\nimport tensorflow.keras.backend as K\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nimport tensorflow_addons as tfa\n\nimport sklearn\nfrom sklearn.model_selection import train_test_split, KFold, GroupKFold\nfrom sklearn.metrics import mean_squared_error\n\nprint(skopt.__version__, sklearn.__version__)","a0364461":"def seed_everything(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    tf.random.set_seed(seed)\n\nseed_everything(seed=42)","e1300016":"token2int = {x:i for i, x in enumerate('().ACGUBEHIMSX')}\n\ndef preprocess_char_inputs(df, cols=['sequence', 'structure', 'predicted_loop_type']):\n    return np.transpose(\n        np.array(\n            df[cols]\n            .applymap(lambda seq: [token2int[x] for x in seq])\n            .values\n            .tolist()\n        ),\n        (0, 2, 1)\n    )","714e27ba":"def rmse(y_true, y_pred):\n        return K.sqrt(K.mean(K.square(y_pred - y_true), axis=-1)) \n\ndef rmse_2(y_true, y_pred):\n    return np.sqrt(mean_squared_error(y_true, y_pred))\n    \n    \ndef rmse_metric(y_true, y_pred):\n    metrics = []\n    \n    y_pred_flat = y_pred.reshape(-1, 5)\n    y_true_flat = y_true.reshape(-1, 5)\n        \n    for _target in range(len(target_cols)):\n        metrics.append(rmse_2(y_true_flat[:, _target], y_pred_flat[:, _target]))\n    \n    return np.mean(metrics)\n\n\ndef MCRMSE(y_true, y_pred):\n    colwise_mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=1)\n    return tf.reduce_mean(tf.sqrt(colwise_mse), axis=1)\n\n\n# label_encoding\ndef label_encode(train_data, test_data):\n    \n    sequence_encmap = {'A': 0, 'G' : 1, 'C' : 2, 'U' : 3}\n    structure_encmap = {'.' : 0, '(' : 1, ')' : 2}\n    looptype_encmap = {'S':0, 'E':1, 'H':2, 'I':3, 'X':4, 'M':5, 'B':6}\n\n    enc_targets = ['sequence', 'structure', 'predicted_loop_type']\n    enc_maps = [sequence_encmap, structure_encmap, looptype_encmap]\n\n    for t,m in zip(enc_targets, enc_maps):\n        for c in [c for c in train_data.columns if t in c]:\n            train_data[c] = train_data[c].replace(m)\n            test_data[c] = test_data[c].replace(m)\n    return train_data, test_data","6951ec24":"target_cols = ['reactivity', 'deg_Mg_pH10', 'deg_pH10', 'deg_Mg_50C', 'deg_50C']\ntarget_scored = ['reactivity', 'deg_Mg_pH10', 'deg_Mg_50C']","0e6c4b3d":"FILTER_SNR = True\nN_TRIALS = 15\nSEED = 34          # 2020","8a9c0b0c":"train = pd.read_json('\/kaggle\/input\/stanford-covid-vaccine\/train.json', lines=True)\ntest = pd.read_json('\/kaggle\/input\/stanford-covid-vaccine\/test.json', lines=True)\nsample_df = pd.read_csv('\/kaggle\/input\/stanford-covid-vaccine\/sample_submission.csv')","e5210dde":"if FILTER_SNR:\n    train = train[train.signal_to_noise > 1]\n\n\ntrain_inputs = preprocess_char_inputs(train)\ntrain_labels = np.array(train[target_cols].values.tolist()).transpose((0, 2, 1))\n\nprint(train_inputs.shape, train_labels.shape)","b360a411":"train_inputs, val_inputs, train_labels, val_labels = train_test_split(train_inputs, train_labels, test_size=0.1, random_state=34)\n\nprint('training\/validation shapes:', train_inputs.shape, val_inputs.shape)","5df208f5":"dim_learning_rate = Real(low=1e-4, high=1e-1, prior='uniform', name='learning_rate')\n# dim_num_dense_nodes_3 = Integer(low=256, high=512, name='num_dense_nodes_3')\n# dim_activation = Categorical(categories=['relu', 'elu'], name='activation')\n\n\ndim_spatial_dp = Integer(low=1, high=6, name='spatial_dp')\ndim_dropout1 = Integer(low=1, high=6, name='dp1')\ndim_dropout2 = Integer(low=1, high=6, name='dp2')\ndim_dropout3 = Integer(low=1, high=6, name='dp3')\ndim_embed_dim = Integer(low=50, high=120, name='embed_dim')\n# dim_look_ahead = Integer(low=5, high=15, name='look_ahead')\n\n\ndimensions = [dim_learning_rate, dim_spatial_dp, dim_dropout1, dim_dropout2, dim_dropout3, dim_embed_dim]\n\n\n# set default params - make sure are within the search space\ndefault_params = [0.001, 2, 5, 5, 5, 75] \n\nassert len(default_params)==len(dimensions), 'Error: check shapes!'","9bedf90b":"def gru_layer(hidden_dim, dropout):\n    return tf.keras.layers.Bidirectional(\n                                tf.keras.layers.GRU(hidden_dim,\n                                dropout=dropout,\n                                return_sequences=True,\n                                kernel_initializer = 'orthogonal'))\n\ndef lstm_layer(hidden_dim, dropout):\n    return tf.keras.layers.Bidirectional(\n                                tf.keras.layers.LSTM(hidden_dim,\n                                dropout=dropout,\n                                return_sequences=True,\n                                kernel_initializer = 'orthogonal'))\n\n\n\ndef build_model(learning_rate, spatial_dp, dp1, dp2, dp3, embed_dim, gru=0, seq_len=107, pred_len=68, hidden_dim=128):\n    \n    inputs = tf.keras.layers.Input(shape=(seq_len, 3))\n\n    embed = tf.keras.layers.Embedding(input_dim=len(token2int), output_dim=embed_dim)(inputs)\n    reshaped = tf.reshape(embed, shape=(-1, embed.shape[1],  embed.shape[2] * embed.shape[3]))\n    \n    reshaped = tf.keras.layers.SpatialDropout1D(spatial_dp*0.1)(reshaped)\n    \n    if gru==1:\n        hidden = gru_layer(hidden_dim, dp1*0.1)(reshaped)\n        hidden = gru_layer(hidden_dim, dp2*0.1)(hidden)\n        hidden = gru_layer(hidden_dim, dp3*0.1)(hidden)\n        \n    elif gru==0:\n        hidden = lstm_layer(hidden_dim, dp1*0.1)(reshaped)\n        hidden = lstm_layer(hidden_dim, dp2*0.1)(hidden)\n        hidden = lstm_layer(hidden_dim, dp3*0.1)(hidden)\n        \n    elif gru==3:\n        hidden = gru_layer(hidden_dim, dropout)(reshaped)\n        hidden = lstm_layer(hidden_dim, dropout)(hidden)\n        hidden = lstm_layer(hidden_dim, dropout)(hidden)\n        \n    elif gru==4:\n        hidden = lstm_layer(hidden_dim, dropout)(reshaped)\n        hidden = gru_layer(hidden_dim, dropout)(hidden)\n        hidden = gru_layer(hidden_dim, dropout)(hidden)\n    elif gru==5:\n        hidden = lstm_layer(hidden_dim, dropout)(reshaped)\n        hidden = gru_layer(hidden_dim, dropout)(hidden)\n        hidden = lstm_layer(hidden_dim, dropout)(reshaped)\n    \n    # only making predictions on the first part of each sequence\n    truncated = hidden[:, :pred_len]\n    \n    out = tf.keras.layers.Dense(5, activation='linear')(truncated)\n    model = tf.keras.Model(inputs=inputs, outputs=out)\n\n    # optimizers\n    opt = tf.keras.optimizers.Adam(learning_rate=learning_rate)\n    #     opt = tfa.optimizers.RectifiedAdam()\n    #     opt = tfa.optimizers.Lookahead(opt, sync_period=6)\n    \n    model.compile(optimizer=opt, loss=MCRMSE, metrics=[tf.keras.metrics.RootMeanSquaredError(name='rmse')])\n    \n    return model","013c8a25":"@use_named_args(dimensions=dimensions)\ndef fitness1(learning_rate, spatial_dp, dp1, dp2, dp3, embed_dim):   \n    \n    \"\"\"\n    Hyper-parameters:\n    learning_rate: lr\n    spatial_dp:   Spatial dropout\n    dp1:          Dropout rates layer-1\n    dp2:          Dropout rates layer-2\n    dp3:          Dropout rates layer-3\n    embed_dim:    embed dim\n    \"\"\"\n\n    # Print the hyper-parameters.\n    print('LR:', learning_rate)\n    print('spatial dropout:', spatial_dp*0.1)\n    print('dropout 1:', dp1*0.1)\n    print('dropout 2:', dp2*0.1)\n    print('dropout 3:', dp3*0.1)\n    print('embed_dim:', embed_dim)\n    print()\n    \n    \n    \n    # GRU \n    callbacks=[\n    tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss'),   #,patience=15, factor=0.8\n    #     get_lr_schedule(),\n    #     tf.keras.callbacks.ModelCheckpoint('model_gru.h5'),\n    #     tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=30)\n    ]\n\n    \n    # Create the neural network with these hyper-parameters.\n    model = build_model(learning_rate, spatial_dp, dp1, dp2, dp3, embed_dim, gru=1)\n\n   \n    # train the model    \n    hist = model.fit(\n        train_inputs, train_labels, \n        validation_data=(val_inputs,val_labels),\n        batch_size=64,\n        epochs=70, \n        callbacks=callbacks,\n        verbose=0)\n\n    print()\n    print(f\"Min training loss={min(hist.history['loss'])}, min validation loss={min(hist.history['val_loss'])}\")\n    #     print(f\"Min training rmse={min(hist.history['rmse'])}, min validation rmse={min(hist.history['val_rmse'])}\")\n\n    # Get the error on the validation-set\n    score = min(hist.history['val_loss'])   \n\n    # Print the classification accuracy.\n    print('-'*20)\n    print(f\"> Val loss (mse): {score}\")\n    print('-'*20)\n\n    # Save the model if it improves on the best-found performance.\n    global best_loss\n\n    # If the classification accuracy of the saved model is improved ...\n    if score < best_loss:\n        \n        # Save the new model & Update the error\n        model.save(path_best_model)\n        best_loss = score\n\n    # Delete the Keras model with these hyper-parameters from memory.\n    del model\n    gc.collect()\n    \n    # Clear the Keras session, to empty the TensorFlow graph \n    K.clear_session()\n    \n    return score","4b3063ce":"# # check objective function (uncomment bellow if you like to test)\n\n# fitness1(default_params)","e1d86034":"path_best_model = 'model_gru.h5'\nbest_loss = 0","1edd7979":"search_result_gru = skopt.gp_minimize(func=fitness1,   \n                            dimensions=dimensions,\n                            acq_func='EI',    #  'gp_hedge'       \n                            n_calls=N_TRIALS,\n                            random_state=SEED,\n                            x0=default_params)","1671c869":"print('optimal hyper-parameters') \nprint()\nprint(f'LR: {search_result_gru.x[0]}')\nprint(f'spatial dropout: {search_result_gru.x[1]}')\nprint(f'dropout 1: {search_result_gru.x[2]}')\nprint(f'dropout 2: {search_result_gru.x[3]}')\nprint(f'dropout 3: {search_result_gru.x[4]}')\nprint(f'embed dim: {search_result_gru.x[5]}')","e50f3173":"pd.DataFrame(sorted(zip(search_result_gru.func_vals, search_result_gru.x_iters)), index=np.arange(N_TRIALS), columns=['score', 'params'])","fdf1d3dc":"%matplotlib inline\nplot_convergence(search_result_gru)","983a15b7":"# create a list for plotting\ndim_names = ['LR', 'spatial_dropout', 'dropout_1', 'dropout_2', 'dropout_3', 'embed_dim']\n\n# %matplotlib inline\nplot_objective(result=search_result_gru, dimensions=dim_names);","0875061c":"path_best_model = 'model_lstm.h5'\nbest_loss = 0","609022bf":"@use_named_args(dimensions=dimensions)\ndef fitness2(learning_rate, spatial_dp, dp1, dp2, dp3, embed_dim):   \n    \n    \"\"\"\n    Hyper-parameters:\n    learning_rate: LR\n    spatial_dp:   Spatial dropout\n    dp1:          Dropout rates layer-1\n    dp2:          Dropout rates layer-2\n    dp3:          Dropout rates layer-3\n    embed_dim:    embed dim\n    \"\"\"\n\n    # Print the hyper-parameters.\n    print('learning_rate', learning_rate)\n    print('spatial dropout:', spatial_dp*0.1)\n    print('dropout 1:', dp1*0.1)\n    print('dropout 2:', dp2*0.1)\n    print('dropout 3:', dp3*0.1)\n    print('embed_dim:', embed_dim)\n    print()\n    \n    \n    # GRU \n    callbacks=[tf.keras.callbacks.ReduceLROnPlateau(monitor='val_loss')]\n\n    \n    # Create the neural network with these hyper-parameters.\n    model = build_model(learning_rate, spatial_dp, dp1, dp2, dp3, embed_dim, gru=0)\n\n   \n    # train the model    \n    hist = model.fit(\n        train_inputs, train_labels, \n        validation_data=(val_inputs,val_labels),\n        batch_size=64,\n        epochs=70, \n        callbacks=callbacks,\n        verbose=0)\n\n    print()\n    print(f\"Min training loss={min(hist.history['loss'])}, min validation loss={min(hist.history['val_loss'])}\")\n    #     print(f\"Min training rmse={min(hist.history['rmse'])}, min validation rmse={min(hist.history['val_rmse'])}\")\n    \n\n    # Get the error on the validation-set\n    score = min(hist.history['val_loss'])   \n\n    # Print the classification accuracy.\n    print('-'*20)\n    print(f\"> Val loss (mse): {score}\")\n    print('-'*20)\n\n    # Save the model if it improves on the best-found performance.\n    global best_loss\n\n    # If the classification accuracy of the saved model is improved ...\n    if score < best_loss:\n        \n        # Save the new model & Update the error\n        model.save(path_best_model)\n        best_loss = score\n\n    # Delete the Keras model with these hyper-parameters from memory.\n    del model\n    gc.collect()\n    \n    # Clear the Keras session, to empty the TensorFlow graph \n    K.clear_session()\n    \n    return score","54f43efd":"search_result_lstm = skopt.gp_minimize(func=fitness2,   \n                            dimensions=dimensions,\n                            acq_func='EI',    #  'gp_hedge'       \n                            n_calls=N_TRIALS,\n                            random_state=SEED,\n                            x0=default_params)","bd855f41":"print('optimal hyper-parameters') \nprint()\nprint(f'LR: {search_result_lstm.x[0]}')\nprint(f'spatial dropout: {search_result_lstm.x[1]}')\nprint(f'dropout 1: {search_result_lstm.x[2]}')\nprint(f'dropout 2: {search_result_lstm.x[3]}')\nprint(f'dropout 3: {search_result_lstm.x[4]}')\nprint(f'embed dim: {search_result_lstm.x[5]}')","6ff3faf1":"pd.DataFrame(sorted(zip(search_result_lstm.func_vals, search_result_lstm.x_iters)), index=np.arange(N_TRIALS), columns=['score', 'params'])","e53be25a":"%matplotlib inline\nplot_convergence(search_result_lstm)","bac81928":"# %matplotlib inline\nplot_objective(result=search_result_lstm, dimensions=dim_names);","bad20f8a":"# create models with best hyperparams\n\nbest_model_gru = build_model(*search_result_gru.x, gru=1)\nbest_model_gru.summary()\n\nbest_model_lstm = build_model(*search_result_lstm.x, gru=0)\nbest_model_lstm.summary()","84e445e8":"# Fittness Function to Optimize\n\nThis is the function that creates and trains a neural network with the given hyper-parameters, and then evaluates its performance on the validation-set. The function then returns the so-called fitness value (aka. objective value), which is the negative classification accuracy on the validation-set. It is negative because skopt performs minimization instead of maximization.\n\nThe main steps that we perform are:\n\nbuild and train a network with given hyper-parameters\nevaluate the model performance with the validation dataset\nIt returns the fitness value, in our case the logloss error.\nNote the function decorator @use_named_args which wraps the fitness function so that it can be called with all the parameters as a single list, for example: fitness(x=[1e-4, 3, 256, 'relu']). This is the calling-style skopt uses internally.","20455b2c":"# Work in progress! \n","3f5bcb07":"![NN_architectures.png](attachment:NN_architectures.png)","451e6681":"# Use Bayesian Optimization for Hyper-parameter search\n\nThe idea with Bayesian optimization is to construct another model of the search-space for hyper-parameters. One kind of model is known as a Gaussian Process. This gives us an estimate of how the performance varies with changes to the hyper-parameters. Whenever we evaluate the actual performance for a set of hyper-parameters, we know for a fact what the performance is - except perhaps for some noise. We can then ask the Bayesian optimizer to give us a new suggestion for hyper-parameters in a region of the search-space that we haven't explored yet, or hyper-parameters that the Bayesian optimizer thinks will bring us most improvement. We then repeat this process a number of times until the Bayesian optimizer has built a good model of how the performance varies with different hyper-parameters, so we can choose the best parameters.\n\nThe flowchart of the algorithm is roughly:","3849e6d9":"# Load data","74300a03":"# Reproduce models with best hyper-parameters","13407ce4":"# Imports & Helpers\n\nNOTE: make sure you downgrade sklearn (due to compatibility issues)","ef0eb96e":"# GRU - Hyper-parameter search","dcd51f38":"# Config","38d38f88":"# LSTM - Hyper-parameter search","05617b76":"![image.png](attachment:image.png)","05e3d27e":"# Hyper-parameter Search with Bayesian Optimisation (skopt)\n\nThe purpose of the kernel is to demonstrate how to find an optimal set of hyper-parameters for the RNN models that suit to our problem. \n\n\n## UPDATE:\n\n- v.1-6: Model architecture: Bi-GRU\/Bi-LSTM --> hyperparams: spatial dropout, GRU\/LSTM dropouts, embedding dim\n\n- v.7: Add learning rate as hyperparameter, change loss to custom","cc9c2ead":"# Credits\n\n- towards-data-science article\n- skopt documentation and examples","88b11f15":"NN hyper-parameters\n\n- spatial dropout\n- dropouts \n- looak ahead steps (for optimizer)\n- embedding dim"}}