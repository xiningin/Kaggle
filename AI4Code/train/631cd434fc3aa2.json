{"cell_type":{"85aa25d3":"code","effbc587":"code","f8b0690b":"code","3b7fb8bb":"code","4969718b":"code","f5865af7":"code","c518fbfe":"code","133d8622":"code","51e0b1dd":"code","ea42169a":"code","54f025f1":"code","d9c928b6":"code","6754766b":"code","55ebe6b5":"code","6c145559":"code","728e687a":"code","67127469":"code","ff10b083":"code","16cfcdc5":"code","fd241457":"code","b33fb6c2":"code","09811e32":"code","004e4523":"code","685c090b":"code","69becc0f":"code","12a87716":"code","55258c03":"code","80c37bbb":"code","c54432b6":"code","8707ecd0":"code","9e83d488":"code","3dbad18b":"code","9a02f33d":"code","f48765f6":"code","2c89c607":"code","ff645e70":"code","8cb49963":"markdown","c6363921":"markdown","6949f6ae":"markdown","2cd24191":"markdown","e5bff94d":"markdown","200132b5":"markdown","27f8565d":"markdown","78f46bf1":"markdown","1145293d":"markdown","7849a436":"markdown","4747d716":"markdown","ed2f85f3":"markdown","7335b1d8":"markdown","c6624461":"markdown","4b665d95":"markdown","a3886469":"markdown","29b005b8":"markdown","133eb0d0":"markdown","9a5c4e0b":"markdown","a9226ff1":"markdown","a9efa2d0":"markdown","dfff173f":"markdown","616660cf":"markdown","b2e25313":"markdown","5cea9504":"markdown","0f145277":"markdown","f2147433":"markdown","d07230d1":"markdown","1e470dea":"markdown","e6b2aca5":"markdown","a5236d91":"markdown","fa9687c4":"markdown","a31fc5e2":"markdown","654c2d5e":"markdown","fb462133":"markdown"},"source":{"85aa25d3":"# Load all necessary modules.\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n%matplotlib inline\nimport matplotlib.pyplot as plt  # Matlab-style plotting\nimport seaborn as sns # data visualization\n\nfrom scipy import stats\nfrom scipy.stats import norm, skew #for some statistics\n\npd.set_option('display.float_format', lambda x: '{:.3f}'.format(x)) #Limiting floats output to 3 decimal points\n\n# ignore warnings (NB: it's not a good practise to ignore warnings)\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# check if the dataset has been loaded\nimport os\nassert 'house-prices-advanced-regression-techniques' in os.listdir('..\/input\/'), 'House Prices dataset not loaded!'","effbc587":"# Base folder\nHOME_FOLDER = '..\/input\/house-prices-advanced-regression-techniques\/'\n\n# Read the data\ndf_train = pd.read_csv(HOME_FOLDER + 'train.csv', index_col='Id')\ndf_test = pd.read_csv(HOME_FOLDER + 'test.csv', index_col='Id')","f8b0690b":"# Print some useful info for the train set\nprint(f'Train test size: {df_train.shape}')\nprint(f'Test test size: {df_test.shape}')","3b7fb8bb":"# Descriptive statistics summary\nprint(df_train['SalePrice'].describe())\n\n# We'll need again to plot the distribution and the qqplot, so let's make a function\ndef plot_distribution_and_qqplot(data):\n    # Get the fitted parameters used by the function\n    (mu, sigma) = norm.fit(data)\n    print('mu = {:.2f} and sigma = {:.2f}'.format(mu, sigma))\n\n    # Plot the distribution\n    sns.distplot(data, fit=norm)\n    plt.legend(['Normal dist. ($\\mu=$ {:.2f} and $\\sigma=$ {:.2f} )'.format(mu, sigma)], loc='best')\n    plt.ylabel('Frequency')\n    plt.title(f'{data.name} distribution')\n\n    # Get also the QQ-plot\n    fig = plt.figure()\n    res = stats.probplot(data, plot=plt)\n    plt.show()\n    \nplot_distribution_and_qqplot(df_train['SalePrice'])","4969718b":"# We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\ndf_train[\"SalePrice\"] = np.log1p(df_train[\"SalePrice\"])\n\n# Check the new distribution\nplot_distribution_and_qqplot(df_train['SalePrice'])","f5865af7":"# As before, before plotting let's create a useful function to use it again later\ndef plot_scatter(x, y):\n    fig, ax = plt.subplots()\n    ax.scatter(x=x, y=y)\n    plt.ylabel(y.name, fontsize=13)\n    plt.xlabel(x.name, fontsize=13)\n    plt.show()\n    \nplot_scatter(df_train['GrLivArea'], df_train['SalePrice'])","c518fbfe":"# Deleting outliers\noutliers_idx = df_train['GrLivArea'].sort_values(ascending=False)[:2].index\ndf_train.drop(outliers_idx, inplace=True)\n\n# Check the graphic again\nplot_scatter(df_train['GrLivArea'], df_train['SalePrice'])","133d8622":"# We are gonna need this later for split back the fill df into train and test\nn_train, n_test = df_train.shape[0], df_test.shape[0]\n\n# Create one, whole dataframe\ndf_full = pd.concat((df_train, df_test), sort=False).reset_index(drop=True)\n\n# Store the target variable for later use\ny_train = df_train['SalePrice']\n\n# There shouldn't be missing data in the target variable\ndf_full.drop(['SalePrice'], axis=1, inplace=True)","51e0b1dd":"# Compute the percentage of missing values for each column and sort the result\nmissing_ratio = (df_full.isnull().sum() \/ df_full.shape[0] * 100).sort_values(ascending=False)\n\n# Drop all columns which have no missing data and sort them\nmissing_ratio.drop(missing_ratio[missing_ratio == 0].index, inplace=True)\n\n# Create a simple table for better visualization (and understanding)\nmissing_values = pd.DataFrame(missing_ratio, columns=['Missing Ratio'])\n\nprint(missing_values)","ea42169a":"df_full['PoolQC'].fillna('None', inplace=True)","54f025f1":"df_full['MiscFeature'].fillna('None', inplace=True)","d9c928b6":"cols = ['Alley', 'Fence', 'FireplaceQu']\ndf_full[cols] = df_full[cols].fillna('None', inplace=False)","6754766b":"# Group by neighborhood\ndf_full['LotFrontage'] = df_full.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))","55ebe6b5":"cols = ['GarageCond', 'GarageQual', 'GarageFinish', 'GarageType']\ndf_full[cols] = df_full[cols].fillna('None')","6c145559":"cols = ['GarageYrBlt', 'GarageArea', 'GarageCars']\ndf_full[cols] = df_full[cols].fillna(0)","728e687a":"cols = ['BsmtCond', 'BsmtExposure', 'BsmtQual', 'BsmtFinType2', 'BsmtFinType1']\ndf_full[cols] = df_full[cols].fillna('None')","67127469":"cols = ['BsmtHalfBath', 'BsmtFullBath', 'BsmtFinSF2', 'BsmtFinSF1', 'BsmtUnfSF', 'TotalBsmtSF']\ndf_full[cols] = df_full[cols].fillna(0)","ff10b083":"# House with a value for 'MasVnrArea' but a missing value in 'MasVnrType'\nprint(df_full[['MasVnrType', 'MasVnrArea']].loc[df_full['MasVnrType'].isnull() & df_full['MasVnrArea'].notnull()])\n\n# In respect to the house of observation above, what 'MasVnrType' do the similar houses have?\nprint('\\nHouses with price between 196.000 and 200.000:')\nprint(df_full[['MasVnrType']].loc[(df_full['MasVnrArea'] <= 200.000) & (df_full['MasVnrArea'] >= 196.000)].squeeze().value_counts())","16cfcdc5":"# Replace the single value\nmask = (df_full['MasVnrType'].isnull()) & (df_full['MasVnrArea'].notnull())\ndf_full.loc[mask, 'MasVnrType'] = df_full.loc[mask, 'MasVnrType'].fillna('None')\n\n# The others\ndf_full['MasVnrType'].fillna('None', inplace=True)\ndf_full['MasVnrArea'].fillna(0, inplace=True)","fd241457":"df_full['MSZoning'].fillna(df_full['MSZoning'].mode()[0], inplace=True)","b33fb6c2":"df_full.drop(['Utilities'], axis=1, inplace=True)","09811e32":"df_full['Functional'].fillna('Typ', inplace=True)","004e4523":"df_full['Exterior2nd'].fillna(df_full['Exterior2nd'].mode()[0], inplace=True)\ndf_full['Exterior1st'].fillna(df_full['Exterior1st'].mode()[0], inplace=True)","685c090b":"df_full['SaleType'].fillna(df_full['SaleType'].mode()[0], inplace=True)\ndf_full['Electrical'].fillna(df_full['Electrical'].mode()[0], inplace=True)\ndf_full['KitchenQual'].fillna(df_full['KitchenQual'].mode()[0], inplace=True)","69becc0f":"assert df_full.isnull().sum().sort_values(ascending=False)[0] == 0, 'Mmm, there are still missing values! Check again.'","12a87716":"# Change column tpye to set the feature as categorical\ndf_full['MSSubClass'] = df_full['MSSubClass'].apply(str)\ndf_full['OverallCond'] = df_full['OverallCond'].apply(str)\n\nimport datetime\ndef mapper(month):\n    date = datetime.datetime(2000, month, 1)  # You need a dateobject with the proper month\n    return date.strftime('%b')  # %b returns the months abbreviation, other options [here][1]\n\ndf_full['MoSold'] = df_full['MoSold'].apply(mapper)","55258c03":"from sklearn.preprocessing import LabelEncoder\n\n# Write down explicitly all the feature names\ncols = ['MSSubClass', 'Street', 'Alley','LandSlope', 'OverallQual', 'OverallCond', \n        'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1',\n        'BsmtFinType2', 'HeatingQC', 'CentralAir', 'KitchenQual', 'Functional',\n        'FireplaceQu', 'GarageFinish', 'GarageQual', 'GarageCond', 'PavedDrive',\n        'PoolQC', 'Fence',  'YrSold', 'MoSold']\n\n# For each column create and apply the encoder\nfor c in cols:   \n    le = LabelEncoder()\n    \n    le.fit(list(df_full[c].values))\n    \n    df_full[c] = le.transform(list(df_full[c].values))","80c37bbb":"# Get the non categorical features\nnumerical_feats = df_full.dtypes[df_full.dtypes != \"object\"].index\n\n# Check the skew of all numerical features\nskewed_feats = df_full[numerical_feats].apply(lambda x: skew(x.dropna())).sort_values(ascending=False)\nprint('Skew in numerical features:')\nskewness = pd.DataFrame({'Skew' :skewed_feats})\nskewness.head(10)","c54432b6":"skewness = skewness[abs(skewness['Skew']) > 0.75]\nprint(f'There are {skewness.shape[0]} skewed numerical features to Box Cox transform.')\n\nfrom scipy.special import boxcox1p\n\nlambda_param = 0.15\nfor feature in skewness.index:\n    df_full[feature] = boxcox1p(df_full[feature], lambda_param)","8707ecd0":"df_full = pd.get_dummies(df_full)\nprint(f'Now there are {df_full.shape[1]} features.')","9e83d488":"X_train = df_full[:n_train]\nX_test = df_full[n_train:]","3dbad18b":"from sklearn.model_selection import KFold, cross_val_score\n#from sklearn.preprocessing import RobustScaler\nfrom xgboost import XGBRegressor","9a02f33d":"# Define the validation function\ndef model_validation(model, X, y, n_folds=5):\n    kf = KFold(n_folds, shuffle=True, random_state=42).get_n_splits(X.values)\n    \n    score = -cross_val_score(model, X.values, y, scoring='neg_mean_squared_error', cv=kf)\n    \n    return np.sqrt(score)","f48765f6":"# Create the model\nmodel_xgb = XGBRegressor(colsample_bytree=0.8, subsample=0.5,\n                         learning_rate=0.05, max_depth=3,\n                         min_child_weight=1.8, n_estimators=2000,\n                         reg_alpha=0.1, reg_lambda=0.8, gamma=0.01,\n                         silent=1, random_state=7, n_jobs=-1,\n                         early_stopping_rounds=10)\n\n# Check the performance using the defined strategy\nscore = model_validation(model_xgb, X_train, y_train)\nprint(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()))","2c89c607":"# Fit using the whole train data\nmodel_xgb.fit(X_train, y_train)\n\n# Predict using the test set\n# NOTE: remember that we log-transformed the target variable, so now we need to apply the inverse process to get the actual predictions\npredictions = np.expm1(model_xgb.predict(X_test))","ff645e70":"# Save predictions to file\noutput = pd.DataFrame({'Id': df_test.index,\n                       'SalePrice': predictions})\n\noutput.to_csv('submission.csv', index=False)","8cb49963":"The skew seems now corrected and the data appears more normally distributed, good.","c6363921":"### Missing data\nTo check for missing data in both train and test data and keep the preprocess pipeline consistent between both, let's concatenate them and consider, for the moment, just one big dataset.","6949f6ae":"* _Functional_: documentation says \"_Assume typical unless deductions are warranted_\", so we fill the NaN with typical (_Typ_).","2cd24191":"#### Read dataset","e5bff94d":"#### Create dummy variables","200132b5":"#### Skewed features","27f8565d":"* It's the same also for _Alley_, _Fence_, _FireplaceQu_.","78f46bf1":"Ok, let's go through them one by one:\n* _PoolQC_: documentation (see above) state that NA means there's no pool (make sense, right?) so we can be pretty confident by replacing those with a None.**","1145293d":"### Feature engineering\nIt seems that some feature listed as numerical are actually categorical.\n* _MSSubClass_: Identifies the type of dwelling involved in the sale (e.g. 020, 050, 160).\n* _OverallCond_: Rates the overall condition of the house (ordinal).\n* _MoSold_: month sold (we transform the month from numbers to names just for easier visualization).","7849a436":"* _GarageYrBlt_, _GarageArea_ and _GarageCars_: since a missing value means no garage, we can raplace them with zeros.\n**NOTE**: Issues with setting _GarageYrBlt_ to 0? Another solution could be imputing by mode groupinb by neighboor (read [here](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard314950) for more info). ","4747d716":"* _BsmtCond_, _BsmtExposure_, _BsmtQual_, _BsmtFinType2_, _BsmtFinType1_: Na means no basement. ","ed2f85f3":"* _SaleType_, _Electrical_ and _KitchenQual_: same as before (since the missing values are only two and one value respectively)","7335b1d8":"## Data preprocessing\n### Target Variable\nFirst, let's start by doing some analysis on the target variable _SalesPrice_.","c6624461":"We can see that the target variable:\n* deviate from the normal distribution\n* have appreciable positive skewness\n* show peakedness\n\nAs (linear) models love normally distributed data , we need to transform this variable and make it more normally distributed.","4b665d95":"## Final training and submission\nWe've almost reached the end of this kernel.\nThe lasts steps we have to perform is to apply our pipeline are: \n* retrain our model using the whole train set\n* predict value of _SalePrice_ using the test set\n* create the submission file.","a3886469":"* _LotFrontage_: means \"Linear feet of street connected to property\". Since the missing ratio is not too high (so better not drop the entire column) we can fill the missing values as the median for the neighborhood: houses in the neighborhood are likely to have the area of each street connected to the house property.**","29b005b8":"## Modelling","133eb0d0":"* _BsmtHalfBath_, _BsmtFullBath_, _BsmtFinSF2_, _BsmtFinSF1_, _BsmtUnfSF_, _TotalBsmtSF_: likely a missing values is due to no basement.","9a5c4e0b":"We'll start with XGBoost since gradient boosting dominates many Kaggle competitions and achieves state-of-the-art results on a variety of datasets. Let's see the parameters:\n\n* _colsample bytree_: Percentage of features used per tree. High value can lead to overfitting (default=1).\n* _subsample_: Percentage of samples used per tree. Low value can lead to underfitting (default=1).\n* _learning rate:_ Step size shrinkage used to prevent overfitting. Range is (0,1).\n* _max depth_: Determines how deeply each tree is allowed to grow during any boosting round (default=6).\n* _min child weight_: Minimum sum of instance weight (hessian) needed in a child. The larger min child weight: is, the more conservative the algorithm will be (default=1).\n* _n estimators_: Number of trees you want to build.\n* _reg alpha_: L1 regularization term on weights. Increasing this value will make model more conservative.(default=0).\n* _reg lambda_: L2 regularization term on weights. Increasing this value will make model more conservative (default=1).\n* _gamma_: Controls whether a given node will split based on the expected reduction in loss after the split. A higher value leads to fewer splits.\n* _silent_: 0 means printing running messages, 1 means silent mode (default=0).\n* _n_jobs: Number of parallel threads used to run xgboost. (replaces nthread).\n* _random state_: Random number seed (replaces seed).\n\nWe'll use Cross Validation as model valition technique and Root Mean Square Error (RMSE) as evaluation metric.\n\n**NOTE**: We specify negative MAE since scikit-learn has a convention where all metrics are defined so a high number is better. Using negatives here allows them to be consistent with that convention, though negative MAE is almost unheard of elsewhere (you can read a bit more of this [here](https:\/\/www.kaggle.com\/alexisbcook\/cross-validation)).","a9226ff1":"We can see that there are two point with (very) large value of _GrLivArea_ and with (very) low price. These are outliers and we can safely remove them.","a9efa2d0":"## Setup","dfff173f":"### Outliers\nThe dataset [documentation](http:\/\/jse.amstat.org\/v19n3\/decock\/DataDocumentation.txt) mentioned that there were outliers and that we should probably get rid of them:\n\n> There are 5 observations that an instructor may wish to remove from the data set before giving it to students (a plot of SALE PRICE versus GR LIV AREA will indicate them quickly). Three of them are true outliers (Partial Sales that likely don\u2019t represent actual market values) and two of them are simply unusual sales (very large houses priced relatively appropriately). I would recommend removing any houses with more than 4000 square feet from the data set (which eliminates these 5 unusual observations) before assigning it to students.\n> Let's check it out for ourselves.\n","616660cf":"* _Exterior2nd_ and _Exterior1nd_: we'll impute with the most frequent value.","b2e25313":"## Intro\nTo help me build my first Kernel here are some greate notebooks that I've read:\n* [Stacked Regressions : Top 4% on LeaderBoard](https:\/\/www.kaggle.com\/serigne\/stacked-regressions-top-4-on-leaderboard) by Serigne.\n* [Comprehensive data exploration with Python](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) by Pedro Marcelino.","5cea9504":"* _Utilities_: three unique values where one is NaN, two observations with NaN are in the test set, almost all values are 'AllPub' except one 'NoSeWa' which is in the train set and hence this feature will not help in the predictive modelling. We can safely remove it.","0f145277":"Before ending this section, we have to split again the data into train and test.","f2147433":"* _MSZoning_: replacing this with the most frequent value.","d07230d1":"* _MasVnrType_ and _MasVnrArea_: almost all the missing values in the first column correspond to a missing value for the second, so it should be plausible to assume that a NA measn no Masonry veneer area\/type. Therefore, for almost all the missing values we'll set them as zeros and Nones. However, there's an house with a missing value for _MasVnrType_ but a specified value in _MasVnrArea_ (check in the code below). Imputing this observation with a None in _MasVnrType_ could be misleading and we see that the majority of houses with that price (between 196.000 and 200.000) have 'Brick Face' as Masonry veneer type (see code below), so we could impute it with this value.\n\n**NOTE**: I'm feeling like there could some issue with this, but I'm not really sure. I've compute the 'majority' of _MasVnrType_ using also the test data, maybe it would be better to use only the train data to impute this one? Could be data leakage?","1e470dea":"Checking the other features indicated by Pedro Marcelino in his [Kernel](https:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) as important:\n* _OverallQual_: Using a boxplot we see some values (actually two) that could be candidates for outliers; nevertheless, these are the only observations for very pool quality houses, maybe we should keep them.\n* _YearBuilt_ and _TotalBsmtSF_: some observations could be outliers, but since they're not so extreme let's keep them.\n","e6b2aca5":"Here we'll apply Box-Cox tranformation for highly skewed features.\nWe use the scipy function _boxcox1p_ which computes the Box-Cox transformation of _1+x_.","a5236d91":"* _MiscFeature_: NA means \"No misc feature\". Same as before.","fa9687c4":"#### Label encoding\nNow it's time to transform the categorical feature that have ordinal relationship.\n\n**NOTE**: I'm not so sure about _MSSubClass_. Maybe we should specify the ordering?","a31fc5e2":" Which feature has missing data? In what percentage?","654c2d5e":"* _GarageCond_, _GarageQual_, _GarageFinish_, _GarageType_: replace NA with None.","fb462133":"Ok, it should be done by now. Let's check if we have still some missing value."}}