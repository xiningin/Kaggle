{"cell_type":{"c63990f6":"code","d3fa7cca":"code","1a4abf74":"code","2fc773b6":"code","79644400":"code","86e3c70a":"code","0cb06bee":"code","6cc78422":"code","dfdf7614":"code","daf0af4b":"code","f8a0c753":"code","01ac683d":"code","d2159003":"code","d636577e":"markdown","632eebb3":"markdown","ad79f00c":"markdown","c11cd16f":"markdown","6e29b330":"markdown","97fa0926":"markdown","91e20f8d":"markdown","a90080d7":"markdown","a8062653":"markdown","3bace342":"markdown","dea728c6":"markdown","236908ca":"markdown","390ffdf7":"markdown","911bbba3":"markdown","922b537c":"markdown","d500a267":"markdown","ebcaf8a7":"markdown","facb8208":"markdown","ab814793":"markdown","85bdc020":"markdown"},"source":{"c63990f6":"import tensorflow as tf\nfrom tensorflow.keras.datasets import cifar10\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Dense, Flatten, Conv2D, MaxPooling2D\nfrom tensorflow.keras.losses import sparse_categorical_crossentropy\nfrom tensorflow.keras.optimizers import Adam\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.layers import Dropout, Flatten, Dense, Input\nfrom keras import applications, optimizers\nfrom keras.models import Model\nfrom keras.applications.vgg16 import VGG16\nfrom keras.callbacks import ModelCheckpoint, EarlyStopping\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report, confusion_matrix\nimport numpy as np\n","d3fa7cca":"train_data_dir = \"..\/input\/myautoge-cars-data\/training_set\/training_set\"\ninput_shape = (128,128)\nbatch_size = 128\ntrain_datagen = ImageDataGenerator(rescale=1.\/255,\n    shear_range=0.2,\n    zoom_range=0.2,\n    vertical_flip=True,\n    validation_split=0.2) # set validation split\n\ntrain_generator = train_datagen.flow_from_directory(\n    train_data_dir,\n    target_size=input_shape,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='training',\n) # set as training data\n\nvalidation_generator = train_datagen.flow_from_directory(\n    train_data_dir, # same directory as training data\n    target_size=input_shape,\n    batch_size=batch_size,\n    class_mode='categorical',\n    subset='validation',\n    shuffle=False,\n) # set as validation data\n\n","1a4abf74":"from keras import backend as K\nimport keras\n\ndef recall_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n    recall = true_positives \/ (possible_positives + K.epsilon())\n    return recall\n\ndef precision_m(y_true, y_pred):\n    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n    precision = true_positives \/ (predicted_positives + K.epsilon())\n    return precision\n\ndef f1_m(y_true, y_pred):\n    precision = precision_m(y_true, y_pred)\n    recall = recall_m(y_true, y_pred)\n    return 2*((precision*recall)\/(precision+recall+K.epsilon()))\n\nMETRICS = [\n      precision_m,\n      recall_m,\n      f1_m,\n      keras.metrics.AUC(name='auc', num_thresholds=200, curve='ROC', summation_method='interpolation',\n            dtype=None, thresholds=None, multi_label=False, label_weights=None),\n]\n\n","2fc773b6":"cnn = Sequential()\n\ncnn.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(128,128, 3)))\ncnn.add(MaxPooling2D(pool_size=(2, 2)))\ncnn.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\ncnn.add(MaxPooling2D(pool_size=(2, 2)))\ncnn.add(Flatten())\ncnn.add(Dense(256, activation='relu'))\ncnn.add(Dense(256, activation='relu'))\ncnn.add(Dense(5, activation='softmax'))\ncnn.summary()","79644400":"cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=METRICS)","86e3c70a":"# # Generate a model with all layers (with top)\n# vgg16 = VGG16(weights=None,include_top=True, input_shape=(64, 64, 3))\n\n# #Add a layer where input is the output of the  second last layer \n# x = Dense(5, activation='softmax', name='predictions')(vgg16.layers[-2].output)\n\n# #Then create the corresponding model \n# cnn = Model(vgg16.input, x)\n# cnn.summary()","0cb06bee":"pat = 5 #this is the number of epochs with no improvment after which the training will stop\nearly_stopping = EarlyStopping(monitor='val_loss', patience=pat, verbose=1)\n\n#define the model checkpoint callback -> this will keep on saving the model as a physical file\nmodel_checkpoint = ModelCheckpoint('result_model.h5', verbose=1, save_best_only=True)\n\nhistory = cnn.fit(\n            train_generator,\n            steps_per_epoch = train_generator.samples \/\/ batch_size,\n            validation_data = validation_generator, \n            validation_steps = validation_generator.samples \/\/ batch_size,\n            epochs = 200,\n            callbacks=[model_checkpoint]\n)","6cc78422":"validation_generator.samples","dfdf7614":"Y_pred = cnn.predict_generator(validation_generator, validation_generator.samples \/\/ batch_size+1, verbose=1)\ny_pred = np.argmax(Y_pred, axis=1)\nprint('Confusion Matrix')\ncm = confusion_matrix(validation_generator.classes, y_pred)\nprint(cm)","daf0af4b":"print('Classification Report')\ntarget_names = [\"Ford\", \"Hyundai\", \"Lexus\", \"Mercedes-benz\", \"Toyota\"]\nprint(classification_report(validation_generator.classes, y_pred, target_names=target_names))","f8a0c753":"def plot_history(history):\n    loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' not in s]\n    val_loss_list = [s for s in history.history.keys() if 'loss' in s and 'val' in s]\n    acc_list = [s for s in history.history.keys() if 'f1' in s and 'val' not in s]\n    val_acc_list = [s for s in history.history.keys() if 'f1' in s and 'val' in s]\n    \n    if len(loss_list) == 0:\n        print('Loss is missing in history')\n        return \n    \n    ## As loss always exists\n    epochs = range(1,len(history.history[loss_list[0]]) + 1)\n    \n    ## Loss\n    plt.figure(1)\n    for l in loss_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n    for l in val_loss_list:\n        plt.plot(epochs, history.history[l], 'g', label='Validation loss (' + str(str(format(history.history[l][-1],'.5f'))+')'))\n    \n    plt.title('Loss')\n    plt.xlabel('Epochs')\n    plt.ylabel('Loss')\n    plt.legend()\n    \n    ## Accuracy\n    plt.figure(2)\n    for l in acc_list:\n        plt.plot(epochs, history.history[l], 'b', label='Training f1-score (' + str(format(history.history[l][-1],'.5f'))+')')\n    for l in val_acc_list:    \n        plt.plot(epochs, history.history[l], 'g', label='Validation f1-score (' + str(format(history.history[l][-1],'.5f'))+')')\n\n    plt.title('F1 score')\n    plt.xlabel('Epochs')\n    plt.ylabel('F1 score')\n    plt.legend()\n    plt.show()\n    \nplot_history(history)","01ac683d":"def get_model():\n    cnn = Sequential()\n\n    cnn.add(Conv2D(32, kernel_size=(3, 3), activation='relu', input_shape=(128,128, 3)))\n    cnn.add(MaxPooling2D(pool_size=(2, 2)))\n    cnn.add(Conv2D(64, kernel_size=(3, 3), activation='relu'))\n    cnn.add(MaxPooling2D(pool_size=(2, 2)))\n    cnn.add(Flatten())\n    cnn.add(Dense(256, activation='relu'))\n    cnn.add(Dense(256, activation='relu'))\n    cnn.add(Dense(5, activation='softmax'))\n    \n    cnn.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics=['accuracy'])\n    \n    return cnn","d2159003":"# from sklearn.model_selection import RandomizedSearchCV, GridSearchCV, train_test_split\n# from keras.wrappers.scikit_learn import KerasClassifier\n# from sklearn.externals.joblib import parallel_backend\n# from time import time\n\n\n# start=time()\n\n# optimizer = ['rmsprop', 'adam']\n# epochs = [50, 100, 200, 300]\n# batches = [64, 128, 256]\n\n\n# model = KerasClassifier(build_fn=get_model, verbose=0)\n# param_grid = dict(nb_epoch=epochs, batch_size=batches)\n\n# grid = GridSearchCV(estimator=model, \n#                     param_grid=param_grid,\n#                     scoring = 'accuracy',\n#                     cv = 10,\n#                     verbose=10)\n\n# X, y = train_generator.next()\n# y = np.argmax(y, axis=1)\n\n# grid_result = grid.fit(X, y)\n\n# print(\"----------------Done---------------\")\n\n# print(\"Best: %f using %s\" % (grid_result.best_score_, grid_result.best_params_))\n\n# print(\"total time:\",time()-start)","d636577e":"\n### Optimize and tune the hyperparameters by Grid Search.","632eebb3":"### Using many metrics: recall, precision, f1 score, and auc roc","ad79f00c":"### Importing the libraries","c11cd16f":"### Confusion Matrix","6e29b330":"## Fitting our model","97fa0926":"### Classification Report","91e20f8d":"## Cars Classification problem","a90080d7":"# Part 2 - Building the CNN","a8062653":"### * Unzip data first","3bace342":"# Part 1 - Data Preprocessing","dea728c6":"### Aim is to classify 5 car manufacturers, which are: Ford, Hyundai, Lexus, Mercedes-benz, Toyota","236908ca":"## Metrics","390ffdf7":"### Commented part, using VGG16 model, last layer changed corresponding to our classification problem","911bbba3":"# Part 4 - Analysis","922b537c":"# Part 3 - Training the CNN","d500a267":"### Firstly, I use CNN Net, where the performance is easy to see. Secondly, I use VGG16 which is an innovative object-recognition model that supports up to 16 layers. Built as a deep CNN, VGG also outperforms baselines on many tasks and datasets outside of ImageNet. VGG is now still one of the most used image-recognition architectures. ","ebcaf8a7":"###  <font color='red'>* Initial part is data preprocessing, we have to delete outlier images and leave clean data. As soon as the data is clean, we can work on it<\/font>","facb8208":"## Plot History","ab814793":"# Part 5 - Model Tuning","85bdc020":"# Convolutional Neural Network"}}