{"cell_type":{"ef764f61":"code","7956183c":"code","80fee716":"code","cb2fd42c":"code","d4f19385":"code","3d0dd1a7":"code","eecb967a":"code","6fadc8d3":"code","12aee60e":"code","d2fca2de":"code","db9d7830":"code","e5fa2ddd":"code","d6479d6f":"code","ff73f1ad":"code","0eef9399":"code","cba94c19":"code","551b08f4":"code","28ad42a2":"code","365fc919":"code","35ef2dc9":"code","9c6182ac":"code","2416ed11":"code","041958c4":"code","0264d6de":"code","3c51adfe":"code","2642a582":"code","1cea2fcd":"code","9e2e19c2":"code","a78e2311":"code","737c1064":"code","404db4f1":"code","581b3f5b":"code","db888dad":"code","c96c5cb8":"code","bf0d50c5":"code","d6b984a1":"code","afe8ad46":"code","812a563a":"code","ed70d86e":"markdown","792dd9bf":"markdown","b32cbef0":"markdown","65073822":"markdown","2f73529f":"markdown","a0e4a196":"markdown","4024fd86":"markdown","284dc8f9":"markdown","c4213a5c":"markdown","5912e61f":"markdown","e2c55726":"markdown","0c342b70":"markdown","f0a4ce1b":"markdown","64eb0492":"markdown","7b406f7a":"markdown","20557e02":"markdown","158b9d11":"markdown","f8c0da09":"markdown","7a6c22b4":"markdown","5fcbda57":"markdown","82c204fc":"markdown","a58e4b49":"markdown","bec52d06":"markdown","0b194441":"markdown","8ed08800":"markdown","4496c34d":"markdown","1c067178":"markdown","440f18df":"markdown","73f903a6":"markdown","17ce2ce2":"markdown"},"source":{"ef764f61":"import warnings\nimport numpy as np\nimport pandas as pd\nimport lightgbm\nimport datetime\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn import datasets\nfrom sklearn import model_selection\nfrom sklearn.metrics import mean_squared_error, roc_auc_score\nfrom sklearn.preprocessing import LabelEncoder\nfrom scipy.stats import boxcox\nfrom tqdm.notebook import tqdm\n\nwarnings.filterwarnings('ignore')\npd.set_option('display.max_colwidth', 0)","7956183c":"def create_stratified_folds_for_regression(data_df, n_splits=5):\n    \"\"\"\n    @param data_df: training data to split in Stratified K Folds for a continous target value\n    @param n_splits: number of splits\n    @return: the training data with a column with kfold id\n    \"\"\"\n    data_df['kfold'] = -1\n    # randomize the data\n    data_df = data_df.sample(frac=1, random_state=42).reset_index(drop=True)\n    # calculate the optimal number of bins based on log2(data_df.shape[0])\n    num_bins = np.int(np.floor(1 + np.log2(len(data_df))))\n    # bins value will be the equivalent of class value of target feature used by StratifiedKFold to \n    # distribute evenly the classed over each fold\n    data_df.loc[:, \"bins\"] = pd.cut(pd.to_numeric(data_df['target'], downcast=\"signed\"), bins=num_bins, labels=False)\n    kf = model_selection.StratifiedKFold(n_splits=n_splits, random_state=42)\n    \n    # set the fold id as a new column in the train data\n    for f, (t_, v_) in enumerate(kf.split(X=data_df, y=data_df.bins.values)):\n        data_df.loc[v_, 'kfold'] = f\n    \n    # drop the bins column (no longer needed)\n    data_df = data_df.drop(\"bins\", axis=1)\n    \n    return data_df","80fee716":"def feature_importance_graph(model, X_train, fold, mse):\n    feature_importance = pd.DataFrame(zip(model.feature_importances_,X_train.columns), columns=['Value','Feature'])\n    feature_importance = feature_importance.sort_values('Value', ascending=False)\n    plt.rcParams['figure.dpi'] = 600\n    fig = plt.figure(figsize=(5, 5), facecolor='#f6f5f5')\n    gs = fig.add_gridspec(1, 1)\n    gs.update(wspace=0.4, hspace=0.1)\n\n    background_color = \"#f6f5f5\"\n    sns.set_palette(['#ffd514']*100)\n\n    ax0 = fig.add_subplot(gs[0, 0])\n    for s in [\"right\", \"top\"]:\n        ax0.spines[s].set_visible(False)\n    ax0.set_facecolor(background_color)\n    ax0_sns = sns.barplot(ax=ax0, y=feature_importance['Feature'], x=feature_importance['Value'], \n                          zorder=2, linewidth=0, orient='h', saturation=1, alpha=1)\n    ax0_sns.set_xlabel(\"Feature Importance\",fontsize=4, weight='bold')\n    ax0_sns.set_ylabel(\"Features\",fontsize=4, weight='bold')\n    ax0_sns.tick_params(labelsize=4, width=0.5, length=1.5)\n    ax0_sns.grid(which='major', axis='x', zorder=0, color='#EEEEEE', linewidth=0.4)\n    ax0_sns.grid(which='major', axis='y', zorder=0, color='#EEEEEE', linewidth=0.4)\n    ax0.text(0, -1.7, 'Features Importance', fontsize=6, ha='left', va='top', weight='bold')\n    ax0.text(0, -1, f'LGBM feature importance for fold {fold} with RMSE validation of {np.sqrt(mse)}', fontsize=4, ha='left', va='top')\n    # data label\n    for p in ax0.patches:\n        value = f'{p.get_width():.0f}'\n        x = p.get_x() + p.get_width() + 10\n        y = p.get_y() + p.get_height() \/ 2 \n        ax0.text(x, y, value, ha='center', va='center', fontsize=4, \n                bbox=dict(facecolor='none', edgecolor='black', boxstyle='round', linewidth=0.3))\n    plt.show()","cb2fd42c":"def cv_evaluation_regression(df, n_fold):\n    oof = np.zeros((300000,))\n    for fold in tqdm(range(n_fold)):\n        val_ind = df[df.kfold == fold].index\n        train = df[df.kfold != fold].reset_index(drop=True)\n        valid = df[df.kfold == fold].reset_index(drop=True)\n\n        features = [feature for feature in df.columns if feature not in ['id', 'target', 'kfold']]\n\n        X_train = train[features]\n        y_train = train['target']\n        X_valid = valid[features]\n        y_valid = valid['target']\n\n        model = lightgbm.LGBMRegressor(random_state=42, objective='regression', metric='rmse')\n        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n        preds = model.predict(valid[features])\n        mse = mean_squared_error(y_valid, preds)\n        \n        oof[val_ind] = preds\n       \n        feature_importance_graph(model, X_train, fold, mse)\n    \n    mse_oof = mean_squared_error(oof, train_df['target'])\n    \n    print(f'LGBM Overall RMSE:{np.sqrt(mse_oof)}')\n    return np.sqrt(mse_oof)","d4f19385":"n_fold = 5\nresult = pd.DataFrame(columns=['Model', 'RMSE'])","3d0dd1a7":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntrain_df = create_stratified_folds_for_regression(train_df)\ncat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train_df[feature])\n    train_df[feature] = le.transform(train_df[feature])\nnew_row = {'Model':'4. Base Regression Model', 'RMSE': cv_evaluation_regression(train_df, 5)}\nresult = result.append(new_row, ignore_index=True)","eecb967a":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntrain_df = create_stratified_folds_for_regression(train_df)\ncat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train_df[feature])\n    train_df[feature] = le.transform(train_df[feature])\ntrain_df[cont_features] = np.log(train_df[cont_features])\nnew_row = {'Model':'5.1. Log Continuous Features and Label Encoding', 'RMSE': cv_evaluation_regression(train_df, 5)}\nresult = result.append(new_row, ignore_index=True)","6fadc8d3":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntrain_df = create_stratified_folds_for_regression(train_df)\ncat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train_df[feature])\n    train_df[feature] = le.transform(train_df[feature])\ntrain_df[cont_features] = np.log(train_df[cont_features])\ntrain_df[cat_features] = np.log(train_df[cat_features])\nnew_row = {'Model':'5.2. Log Continuous Features and Log Label Encoding', 'RMSE': cv_evaluation_regression(train_df, 5)}\nresult = result.append(new_row, ignore_index=True)","12aee60e":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntrain_df = create_stratified_folds_for_regression(train_df)\ncat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train_df[feature])\n    train_df[feature] = le.transform(train_df[feature])\ntrain_df['cont_min'] = train_df[cont_features].min(axis=1)\nnew_row = {'Model':'5.3. Continuous Features, Label Encoding and Min on Continuous Features', 'RMSE': cv_evaluation_regression(train_df, 5)}\nresult = result.append(new_row, ignore_index=True)","d2fca2de":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntrain_df = create_stratified_folds_for_regression(train_df)\ncat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train_df[feature])\n    train_df[feature] = le.transform(train_df[feature])\ntrain_df['cont_min'] = train_df[cont_features].min(axis=1)\ntrain_df['cont_min'] = np.log(train_df['cont_min'])\nnew_row = {'Model':'5.4. Continuous Features, Label Encoding and Log Min on Continuous Features', 'RMSE': cv_evaluation_regression(train_df, 5)}\nresult = result.append(new_row, ignore_index=True)","db9d7830":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntrain_df = create_stratified_folds_for_regression(train_df)\ncat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train_df[feature])\n    train_df[feature] = le.transform(train_df[feature])\ntrain_df['cont_max'] = train_df[cont_features].max(axis=1)\nnew_row = {'Model':'5.5. Continuous Features, Label Encoding and Max on Continuous Features', 'RMSE': cv_evaluation_regression(train_df, 5)}\nresult = result.append(new_row, ignore_index=True)","e5fa2ddd":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntrain_df = create_stratified_folds_for_regression(train_df)\ncat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train_df[feature])\n    train_df[feature] = le.transform(train_df[feature])\ntrain_df['cont_max'] = train_df[cont_features].max(axis=1)\ntrain_df['cont_max'] = np.log(train_df['cont_max'])\nnew_row = {'Model':'5.6. Continuous Features, Label Encoding and Log Max on Continuous Features', 'RMSE': cv_evaluation_regression(train_df, 5)}\nresult = result.append(new_row, ignore_index=True)","d6479d6f":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntrain_df = create_stratified_folds_for_regression(train_df)\ncat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train_df[feature])\n    train_df[feature] = le.transform(train_df[feature])\ntrain_df['cont_sum'] = train_df[cont_features].sum(axis=1)\nnew_row = {'Model':'5.7. Continuous Features, Label Encoding and Sum on Continuous Features', 'RMSE': cv_evaluation_regression(train_df, 5)}\nresult = result.append(new_row, ignore_index=True)","ff73f1ad":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntrain_df = create_stratified_folds_for_regression(train_df)\ncat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train_df[feature])\n    train_df[feature] = le.transform(train_df[feature])\ntrain_df['cont_sum'] = train_df[cont_features].sum(axis=1)\ntrain_df['cont_sum'] = np.log(train_df['cont_sum'])\nnew_row = {'Model':'5.8. Continuous Features, Label Encoding and Log Sum on Continuous Features', 'RMSE': cv_evaluation_regression(train_df, 5)}\nresult = result.append(new_row, ignore_index=True)","0eef9399":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntrain_df = create_stratified_folds_for_regression(train_df)\ncat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train_df[feature])\n    train_df[feature] = le.transform(train_df[feature])\ntrain_df['cont_multiply'] = 1\nfor col in cont_features:\n    train_df['cont_multiply'] = train_df[col] * train_df['cont_multiply']\nnew_row = {'Model':'5.9. Continuous Features, Label Encoding and Multiplication on Continuous Features', 'RMSE': cv_evaluation_regression(train_df, 5)}\nresult = result.append(new_row, ignore_index=True)","cba94c19":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntrain_df = create_stratified_folds_for_regression(train_df)\ncat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train_df[feature])\n    train_df[feature] = le.transform(train_df[feature])\ntrain_df['cont_multiply'] = 1\nfor col in cont_features:\n    train_df['cont_multiply'] = train_df[col] * train_df['cont_multiply']\ntrain_df['cont_multiply'] = np.log(train_df['cont_multiply'])\nnew_row = {'Model':'5.10. Continuous Features, Label Encoding and Log Multiplication on Continuous Features', 'RMSE': cv_evaluation_regression(train_df, 5)}\nresult = result.append(new_row, ignore_index=True)","551b08f4":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntrain_df = create_stratified_folds_for_regression(train_df)\ncat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train_df[feature])\n    train_df[feature] = le.transform(train_df[feature])\ntrain_df['cont_sum'] = train_df[cont_features].sum(axis=1)\nfor col in cont_features:\n    train_df[col+'new'] = train_df[col] \/ train_df['cont_sum']\ntrain_df = train_df.drop('cont_sum', axis=1)\nnew_row = {'Model':'5.11. Continuous Features, Label Encoding and Prorate on Continuous Features', 'RMSE': cv_evaluation_regression(train_df, 5)}\nresult = result.append(new_row, ignore_index=True)","28ad42a2":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntrain_df = create_stratified_folds_for_regression(train_df)\ncat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train_df[feature])\n    train_df[feature] = le.transform(train_df[feature])\ntrain_df['cont_sum'] = train_df[cont_features].sum(axis=1)\nfor col in cont_features:\n    train_df[col+'new'] = np.log(train_df[col] \/ train_df['cont_sum'])\ntrain_df = train_df.drop('cont_sum', axis=1)\nnew_row = {'Model':'5.12. Continuous Features, Label Encoding and Log Prorate on Continuous Features', 'RMSE': cv_evaluation_regression(train_df, 5)}\nresult = result.append(new_row, ignore_index=True)","365fc919":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntrain_df = create_stratified_folds_for_regression(train_df)\ncat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train_df[feature])\n    train_df[feature] = le.transform(train_df[feature])\ntrain_df[cont_features] = np.exp(train_df[cont_features])\nnew_row = {'Model':'5.13. Exponential Continuous and Label Encoding', 'RMSE': cv_evaluation_regression(train_df, 5)}\nresult = result.append(new_row, ignore_index=True)","35ef2dc9":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntrain_df = create_stratified_folds_for_regression(train_df)\ncat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train_df[feature])\n    train_df[feature] = le.transform(train_df[feature])\nfor col in cont_features:\n    train_df[col] = boxcox(train_df[col]+1, 0)\nnew_row = {'Model':'5.14. Boxcox Continuous and Label Encoding', 'RMSE': cv_evaluation_regression(train_df, 5)}\nresult = result.append(new_row, ignore_index=True)","9c6182ac":"def cv_evaluation_regression_mod(df, n_fold):\n    oof = np.zeros((300000,))\n    for fold in tqdm(range(n_fold)):\n        val_ind = df[df.kfold == fold].index\n        train = df[df.kfold != fold].reset_index(drop=True)\n        valid = df[df.kfold == fold].reset_index(drop=True)\n\n        features = [feature for feature in df.columns if feature not in ['id', 'target', 'kfold']]\n        \n        for col in cat_features:\n            encode = train.groupby(col)['target'].mean()\n            train[col] = train[col].map(encode)\n            valid[col] = valid[col].map(encode)\n\n        X_train = train[features]\n        y_train = train['target']\n        X_valid = valid[features]\n        y_valid = valid['target']\n\n        model = lightgbm.LGBMRegressor(random_state=42, objective='regression', metric='rmse')\n        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n        preds = model.predict(valid[features])\n        mse = mean_squared_error(y_valid, preds)\n        \n        oof[val_ind] = preds\n       \n        feature_importance_graph(model, X_train, fold, mse)\n    \n    mse_oof = mean_squared_error(oof, train_df['target'])\n    \n    print(f'LGBM Overall RMSE:{np.sqrt(mse_oof)}')\n    return np.sqrt(mse_oof)","2416ed11":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntrain_df = create_stratified_folds_for_regression(train_df)\ncat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train_df[feature])\n    train_df[feature] = le.transform(train_df[feature])\nnew_row = {'Model':'6.1. Continuous Features and Mean Encoding', 'RMSE': cv_evaluation_regression_mod(train_df, 5)}\nresult = result.append(new_row, ignore_index=True)","041958c4":"def cv_evaluation_regression_mod(df, n_fold):\n    oof = np.zeros((300000,))\n    for fold in tqdm(range(n_fold)):\n        val_ind = df[df.kfold == fold].index\n        train = df[df.kfold != fold].reset_index(drop=True)\n        valid = df[df.kfold == fold].reset_index(drop=True)\n\n        features = [feature for feature in df.columns if feature not in ['id', 'target', 'kfold']]\n        \n        for col in cat_features:\n            encode = train.groupby(col)['target'].min()\n            train[col] = train[col].map(encode)\n            valid[col] = valid[col].map(encode)\n\n        X_train = train[features]\n        y_train = train['target']\n        X_valid = valid[features]\n        y_valid = valid['target']\n\n        model = lightgbm.LGBMRegressor(random_state=42, objective='regression', metric='rmse')\n        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n        preds = model.predict(valid[features])\n        mse = mean_squared_error(y_valid, preds)\n        \n        oof[val_ind] = preds\n       \n        feature_importance_graph(model, X_train, fold, mse)\n    \n    mse_oof = mean_squared_error(oof, train_df['target'])\n    \n    print(f'LGBM Overall RMSE:{np.sqrt(mse_oof)}')\n    return np.sqrt(mse_oof)","0264d6de":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntrain_df = create_stratified_folds_for_regression(train_df)\ncat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train_df[feature])\n    train_df[feature] = le.transform(train_df[feature])\nnew_row = {'Model':'6.2. Continuous Features and Min Encoding', 'RMSE': cv_evaluation_regression_mod(train_df, 5)}\nresult = result.append(new_row, ignore_index=True)","3c51adfe":"def cv_evaluation_regression_mod(df, n_fold):\n    oof = np.zeros((300000,))\n    for fold in tqdm(range(n_fold)):\n        val_ind = df[df.kfold == fold].index\n        train = df[df.kfold != fold].reset_index(drop=True)\n        valid = df[df.kfold == fold].reset_index(drop=True)\n\n        features = [feature for feature in df.columns if feature not in ['id', 'target', 'kfold']]\n        \n        for col in cat_features:\n            encode = train.groupby(col)['target'].max()\n            train[col] = train[col].map(encode)\n            valid[col] = valid[col].map(encode)\n\n        X_train = train[features]\n        y_train = train['target']\n        X_valid = valid[features]\n        y_valid = valid['target']\n\n        model = lightgbm.LGBMRegressor(random_state=42, objective='regression', metric='rmse')\n        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n        preds = model.predict(valid[features])\n        mse = mean_squared_error(y_valid, preds)\n        \n        oof[val_ind] = preds\n       \n        feature_importance_graph(model, X_train, fold, mse)\n    \n    mse_oof = mean_squared_error(oof, train_df['target'])\n    \n    print(f'LGBM Overall RMSE:{np.sqrt(mse_oof)}')\n    return np.sqrt(mse_oof)","2642a582":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntrain_df = create_stratified_folds_for_regression(train_df)\ncat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train_df[feature])\n    train_df[feature] = le.transform(train_df[feature])\nnew_row = {'Model':'6.3. Continuous Features and Max Encoding', 'RMSE': cv_evaluation_regression_mod(train_df, 5)}\nresult = result.append(new_row, ignore_index=True)","1cea2fcd":"def cv_evaluation_regression_mod(df, n_fold):\n    oof = np.zeros((300000,))\n    for fold in tqdm(range(n_fold)):\n        val_ind = df[df.kfold == fold].index\n        train = df[df.kfold != fold].reset_index(drop=True)\n        valid = df[df.kfold == fold].reset_index(drop=True)\n\n        features = [feature for feature in df.columns if feature not in ['id', 'target', 'kfold']]\n        \n        for col in cat_features:\n            encode = train.groupby(col)['target'].median()\n            train[col] = train[col].map(encode)\n            valid[col] = valid[col].map(encode)\n\n        X_train = train[features]\n        y_train = train['target']\n        X_valid = valid[features]\n        y_valid = valid['target']\n\n        model = lightgbm.LGBMRegressor(random_state=42, objective='regression', metric='rmse')\n        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n        preds = model.predict(valid[features])\n        mse = mean_squared_error(y_valid, preds)\n        \n        oof[val_ind] = preds\n       \n        feature_importance_graph(model, X_train, fold, mse)\n    \n    mse_oof = mean_squared_error(oof, train_df['target'])\n    \n    print(f'LGBM Overall RMSE:{np.sqrt(mse_oof)}')\n    return np.sqrt(mse_oof)","9e2e19c2":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntrain_df = create_stratified_folds_for_regression(train_df)\ncat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train_df[feature])\n    train_df[feature] = le.transform(train_df[feature])\nnew_row = {'Model':'6.4. Continuous Features and Median Encoding', 'RMSE': cv_evaluation_regression_mod(train_df, 5)}\nresult = result.append(new_row, ignore_index=True)","a78e2311":"def cv_evaluation_regression_mod(df, n_fold):\n    oof = np.zeros((300000,))\n    for fold in tqdm(range(n_fold)):\n        val_ind = df[df.kfold == fold].index\n        train = df[df.kfold != fold].reset_index(drop=True)\n        valid = df[df.kfold == fold].reset_index(drop=True)\n\n        features = [feature for feature in df.columns if feature not in ['id', 'target', 'kfold']]\n        \n        for col in cat_features:\n            encode = train.groupby(col)['target'].std()\n            train[col] = train[col].map(encode)\n            valid[col] = valid[col].map(encode)\n\n        X_train = train[features]\n        y_train = train['target']\n        X_valid = valid[features]\n        y_valid = valid['target']\n\n        model = lightgbm.LGBMRegressor(random_state=42, objective='regression', metric='rmse')\n        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n        preds = model.predict(valid[features])\n        mse = mean_squared_error(y_valid, preds)\n        \n        oof[val_ind] = preds\n       \n        feature_importance_graph(model, X_train, fold, mse)\n    \n    mse_oof = mean_squared_error(oof, train_df['target'])\n    \n    print(f'LGBM Overall RMSE:{np.sqrt(mse_oof)}')\n    return np.sqrt(mse_oof)","737c1064":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntrain_df = create_stratified_folds_for_regression(train_df)\ncat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train_df[feature])\n    train_df[feature] = le.transform(train_df[feature])\nnew_row = {'Model':'6.5. Continuous Features and Standard Deviation Encoding', 'RMSE': cv_evaluation_regression_mod(train_df, 5)}\nresult = result.append(new_row, ignore_index=True)","404db4f1":"def cv_evaluation_regression_mod(df, n_fold):\n    oof = np.zeros((300000,))\n    for fold in tqdm(range(n_fold)):\n        val_ind = df[df.kfold == fold].index\n        train = df[df.kfold != fold].reset_index(drop=True)\n        valid = df[df.kfold == fold].reset_index(drop=True)\n\n        features = [feature for feature in df.columns if feature not in ['id', 'target', 'kfold']]\n        \n        for col in cat_features:\n            encode = train.groupby(col)['target'].skew()\n            train[col] = train[col].map(encode)\n            valid[col] = valid[col].map(encode)\n\n        X_train = train[features]\n        y_train = train['target']\n        X_valid = valid[features]\n        y_valid = valid['target']\n\n        model = lightgbm.LGBMRegressor(random_state=42, objective='regression', metric='rmse')\n        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n        preds = model.predict(valid[features])\n        mse = mean_squared_error(y_valid, preds)\n        \n        oof[val_ind] = preds\n       \n        feature_importance_graph(model, X_train, fold, mse)\n    \n    mse_oof = mean_squared_error(oof, train_df['target'])\n    \n    print(f'LGBM Overall RMSE:{np.sqrt(mse_oof)}')\n    return np.sqrt(mse_oof)","581b3f5b":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntrain_df = create_stratified_folds_for_regression(train_df)\ncat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train_df[feature])\n    train_df[feature] = le.transform(train_df[feature])\nnew_row = {'Model':'6.6. Continuous Features and Skewness Encoding', 'RMSE': cv_evaluation_regression_mod(train_df, 5)}\nresult = result.append(new_row, ignore_index=True)","db888dad":"def cv_evaluation_regression_mod(df, n_fold):\n    oof = np.zeros((300000,))\n    for fold in tqdm(range(n_fold)):\n        val_ind = df[df.kfold == fold].index\n        train = df[df.kfold != fold].reset_index(drop=True)\n        valid = df[df.kfold == fold].reset_index(drop=True)\n\n        features = [feature for feature in df.columns if feature not in ['id', 'target', 'kfold']]\n\n        for col in cat_features:\n            encode = train.groupby(col)['target'].min()\n            train[col+'min'] = train[col].map(encode)\n            valid[col+'min'] = valid[col].map(encode)\n            \n        for col in cat_features:\n            encode = train.groupby(col)['target'].max()\n            train[col+'max'] = train[col].map(encode)\n            valid[col+'max'] = valid[col].map(encode)\n            \n        for col in cat_features:\n            encode = train.groupby(col)['target'].std()\n            train[col+'med'] = train[col].map(encode)\n            valid[col+'med'] = valid[col].map(encode)\n            \n        for col in cat_features:\n            encode = train.groupby(col)['target'].median()\n            train[col+'std_dev'] = train[col].map(encode)\n            valid[col+'std_dev'] = valid[col].map(encode)\n\n        for col in cat_features:\n            encode = train.groupby(col)['target'].mean()\n            train[col] = train[col].map(encode)\n            valid[col] = valid[col].map(encode)\n\n        X_train = train[features]\n        y_train = train['target']\n        X_valid = valid[features]\n        y_valid = valid['target']\n\n        model = lightgbm.LGBMRegressor(random_state=42, objective='regression', metric='rmse')\n        model.fit(X_train, y_train, eval_set=[(X_valid, y_valid)], verbose=False)\n        preds = model.predict(valid[features])\n        mse = mean_squared_error(y_valid, preds)\n        \n        oof[val_ind] = preds\n       \n        feature_importance_graph(model, X_train, fold, mse)\n    \n    mse_oof = mean_squared_error(oof, train_df['target'])\n    \n    print(f'LGBM Overall RMSE:{np.sqrt(mse_oof)}')\n    return np.sqrt(mse_oof)","c96c5cb8":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntrain_df = create_stratified_folds_for_regression(train_df)\ncat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train_df[feature])\n    train_df[feature] = le.transform(train_df[feature])\nnew_row = {'Model':'6.7. Continuous Features, Mean, Median, Min, Max and Std Dev Encoding', 'RMSE': cv_evaluation_regression_mod(train_df, 5)}\nresult = result.append(new_row, ignore_index=True)","bf0d50c5":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntrain_df = create_stratified_folds_for_regression(train_df)\ncat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train_df[feature])\n    train_df[feature] = le.transform(train_df[feature])\nfor col in cat_features:\n    rate_df = train_df[col].value_counts() \/ len(train_df)\n    train_df[col] = train_df[col].map(rate_df)\nnew_row = {'Model':'7.1. Continuous Features and Percentage Categorical Encoding', 'RMSE': cv_evaluation_regression(train_df, 5)}\nresult = result.append(new_row, ignore_index=True)","d6b984a1":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntrain_df = create_stratified_folds_for_regression(train_df)\ncat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train_df[feature])\n    train_df[feature] = le.transform(train_df[feature])\nfor col in cat_features:\n    rate_df = train_df[col].value_counts() \/ len(train_df)\n    train_df[col] = train_df[col].map(rate_df)\ntrain_df['sum_cat'] = train_df[cat_features].sum(axis=1)\nnew_row = {'Model':'7.2. Continuous Features and Sum Categorical Encoding', 'RMSE': cv_evaluation_regression(train_df, 5)}\nresult = result.append(new_row, ignore_index=True)","afe8ad46":"train_df = pd.read_csv('\/kaggle\/input\/30-days-of-ml\/train.csv')\ntrain_df = create_stratified_folds_for_regression(train_df)\ncat_features = [feature for feature in train_df.columns if 'cat' in feature]\ncont_features = [feature for feature in train_df.columns if 'cont' in feature]\nfor feature in cat_features:\n    le = LabelEncoder()\n    le.fit(train_df[feature])\n    train_df[feature] = le.transform(train_df[feature])\nfor col in cat_features:\n    rate_df = train_df[col].value_counts() \/ len(train_df)\n    train_df[col] = train_df[col].map(rate_df)\ntrain_df['cont_multiply'] = 1\nfor col in cat_features:\n    train_df['cont_multiply'] = train_df[col] * train_df['cont_multiply']\nnew_row = {'Model':'7.3. Continuous Features and Multiplication Categorical Encoding', 'RMSE': cv_evaluation_regression(train_df, 5)}\nresult = result.append(new_row, ignore_index=True)","812a563a":"result.sort_values('RMSE')","ed70d86e":"[back to top](#table-of-contents)\n<a id=\"base_label_encode\"><\/a>\n# 5. Base Regression Model and Label Encoding\n<a id=\"modified_model_1\"><\/a>\n## 5.1. Log Continuous Features and Label Encoding\nFeatures that are used:\n* Log on all continuous features\n* Label encoding on all categorical features","792dd9bf":"[back to top](#table-of-contents)\n<a id=\"modified_model_23\"><\/a>\n## 5.13. Exponential Continuous Features and Label Encoding\nFeatures that are used:\n* Exponential on all continuous features\n* Label encoding on all categorical features","b32cbef0":"[back to top](#table-of-contents)\n<a id=\"modified_model_9\"><\/a>\n## 5.9. Continuous Features, Label Encoding and Multiplication on Continuous Features\nFeatures that are used:\n* Continuous features\n* Label encoding on categorical features\n* New Features:\n    * Multiplication on all continuous features","65073822":"[back to top](#table-of-contents)\n<a id=\"modified_model_11\"><\/a>\n## 5.11. Continuous Features, Label Encoding and Prorate on Continuous Features\nFeatures that are used:\n* Continuous features\n* Label encoding on categorical features\n* New Features:\n    * Prorate on all continuous features","2f73529f":"[back to top](#table-of-contents)\n<a id=\"modified_model_18\"><\/a>\n## 7.2. Continuous Features and Sum Categorical Encoding\nFeatures that are used:\n* Continuous features\n* Categorical encoding base on the percentage of categorical items\n* New Features:\n    * Sum of encoded categorical features","a0e4a196":"[back to top](#table-of-contents)\n<a id=\"modified_model_12\"><\/a>\n## 5.12. Continuous Features, Label Encoding and Log Prorate on Continuous Features\nFeatures that are used:\n* Continuous features\n* Label encoding on categorical features\n* New Features:\n    * Log prorate on all continuous features","4024fd86":"[back to top](#table-of-contents)\n<a id=\"modified_model_8\"><\/a>\n## 5.8. Continuous Features, Label Encoding and Log Sum on Continuous Features\nFeatures that are used:\n* Continuous features \n* Label encoding on categorical features\n* New Features:\n    * Log sum of all continuous features","284dc8f9":"[back to top](#table-of-contents)\n<a id=\"modified_model_14\"><\/a>\n## 6.2. Continuous Features and Min Encoding\nFeatures that are used:\n* Continuous features\n* Minimum encoding on categorical features","c4213a5c":"[back to top](#table-of-contents)\n<a id=\"modified_model_3\"><\/a>\n## 5.3. Continuous Features, Label Encoding and Min on Continuous Features\nFeatures that are used:\n* Continuous features \n* Label encoding on categorical features\n* New Features:\n    * Min of all continuous features","5912e61f":"[back to top](#table-of-contents)\n<a id=\"modified_model_5\"><\/a>\n## 5.5. Continuous Features, Label Encoding and Max on Continuous Features\nFeatures that are used:\n* Continuous features \n* Label encoding on categorical features\n* New Features:\n    * Max of all continuous features","e2c55726":"[back to top](#table-of-contents)\n<a id=\"base_cat_encode\"><\/a>\n# 7. Base Regression Model and Categorical Encoding\n<a id=\"modified_model_17\"><\/a>\n## 7.1. Continuous Features and Percentage Categorical Encoding\nFeatures that are used:\n* Continuous features\n* Categorical encoding base on the percentage of categorical items","0c342b70":"[back to top](#table-of-contents)\n<a id=\"summary\"><\/a>\n## 8. Summary\nBelow are the temporary results of OOF validation:","f0a4ce1b":"[back to top](#table-of-contents)\n<a id=\"modified_model_19\"><\/a>\n## 6.4. Continuous Features and Median Encoding\nFeatures that are used:\n* Continuous features\n* Median encoding on categorical features","64eb0492":"[back to top](#table-of-contents)\n<a id=\"modified_model_10\"><\/a>\n## 5.10. Continuous Features, Label Encoding and Log Multiplication on Continuous Features\nFeatures that are used:\n* Continuous features\n* Label encoding on categorical features\n* New Features:\n    * Log multiplication on all continuous features","7b406f7a":"[back to top](#table-of-contents)\n<a id=\"base_target_encode\"><\/a>\n# 6. Base Regression Model and Target Encoding\n<a id=\"modified_model_13\"><\/a>\n## 6.1. Continuous Features and Mean Encoding\nFeatures that are used:\n* Continuous features\n* Mean encoding on categorical features","20557e02":"[back to top](#table-of-contents)\n<a id=\"modified_model_20\"><\/a>\n## 6.5. Continuous Features and Standard Deviation Encoding\nFeatures that are used:\n* Continuous features\n* Standard Deviation encoding on categorical features","158b9d11":"[back to top](#table-of-contents)\n<a id=\"modified_model_7\"><\/a>\n## 5.7. Continuous Features, Label Encoding and Sum on Continuous Features\nFeatures that are used:\n* Continuous features \n* Label encoding on categorical features\n* New Features:\n    * Sum of all continuous features","f8c0da09":"[back to top](#table-of-contents)\n<a id=\"reference\"><\/a>\n# 2. Reference\n\nThis is a copy of previous notebook: [TPS Feb 2021 Base Model & Features Engineering](https:\/\/www.kaggle.com\/dwin183287\/tps-feb-2021-base-model-features-engineering\/edit\/run\/72556895)\n\nPlease find great notebooks that are used in this notebook:\n* [February Solution - Stratified KFolds](https:\/\/www.kaggle.com\/gpreda\/february-solution-stratified-kfolds) by [Gabriel Preda](https:\/\/www.kaggle.com\/gpreda)\n* [TPS Feb 2021 with LGBMRegressor](https:\/\/www.kaggle.com\/tunguz\/tps-feb-2021-with-lgbmregressor) by [Bojan Tunguz](https:\/\/www.kaggle.com\/tunguz)","7a6c22b4":"[back to top](#table-of-contents)\n<a id=\"modified_model_16\"><\/a>\n## 6.7. Continuous Features, Mean, Median, Min, Max and Std Dev Encoding\nFeatures that are used:\n* Continuous features\n* Mean encoding on categorical features\n* New Features:\n    * Minimum encoding on categorical features\n    * Maximum encoding on categorical features","5fcbda57":"[back to top](#table-of-contents)\n<a id=\"base_model\"><\/a>\n# 4. Base Regression Model\nFeatures that are used:\n* Continuous features\n* Label encoding on categorical features","82c204fc":"[back to top](#table-of-contents)\n<a id=\"modified_model_24\"><\/a>\n## 5.14. Boxcox Continuous Features and Label Encoding\nFeatures that are used:\n* Boxcox 0 on all continuous features\n* Label encoding on all categorical features","a58e4b49":"[back to top](#table-of-contents)\n<a id=\"modified_model_6\"><\/a>\n## 5.6. Continuous Features, Label Encoding and Log Max on Continuous Features\nFeatures that are used:\n* Continuous features \n* Label encoding on categorical features\n* New Features:\n    * Log max of all continuous features","bec52d06":"[back to top](#table-of-contents)\n<a id=\"modified_model_4\"><\/a>\n## 5.4. Continuous Features, Label Encoding and Log Min on Continuous Features\nFeatures that are used:\n* Continuous features \n* Label encoding on categorical features\n* New Features:\n    * Log min of all continuous features","0b194441":"[back to top](#table-of-contents)\n<a id=\"modified_model_15\"><\/a>\n## 6.3. Continuous Features and Max Encoding\nFeatures that are used:\n* Continuous features\n* Maximum encoding on categorical features","8ed08800":"[back to top](#table-of-contents)\n<a id=\"introduction\"><\/a>\n# 1. Introduction\n\nThe purpose of the notebook is to try as many feature engineering without any parameters tuning to see the impact of the new features. The base model is Lightgbm. There is a little summary on each section on features that are used on the model.","4496c34d":"[back to top](#table-of-contents)\n<a id=\"modified_model_2\"><\/a>\n## 5.2. Log Continuous Features and Log Label Encoding\nFeatures that are used:\n* Log on all continuous features\n* Log label encoding on all categorical features","1c067178":"[back to top](#table-of-contents)\n<a id=\"modified_model_21\"><\/a>\n## 6.6. Continuous Features and Skewness Encoding\nFeatures that are used:\n* Continuous features\n* Skewness encoding on categorical features","440f18df":"[back to top](#table-of-contents)\n<a id=\"modified_model_22\"><\/a>\n## 7.3. Continuous Features and Multiplication Categorical Encoding\nFeatures that are used:\n* Continuous features\n* Categorical encoding base on the percentage of categorical items\n* New Features:\n    * Multiplication of encoded categorical features","73f903a6":"# Table of Contents\n\n<a id=\"table-of-contents\"><\/a>\n1. [Introduction](#introduction)\n2. [Reference](#reference)\n3. [Preparation](#preparation)\n4. [Base Regression Model](#base_model)\n5. [Base Regression Model and Label Encoding](#base_label_encode)\n    * 5.1. [Log Continuous Features and Label Encoding](#modified_model_1)\n    * 5.2. [Log Continuous Features and Log Label Encoding](#modified_model_2)\n    * 5.3. [Continuous Features, Label Encoding and Min on Continuous Features](#modified_model_3)\n    * 5.4. [Continuous Features, Label Encoding and Log Min on Continuous Features](#modified_model_4)\n    * 5.5. [Continuous Features, Label Encoding and Max on Continuous Features](#modified_model_5)\n    * 5.6. [Continuous Features, Label Encoding and Log Max on Continuous Features](#modified_model_6)\n    * 5.7. [Continuous Features, Label Encoding and Sum on Continuous Features](#modified_model_7)\n    * 5.8. [Continuous Features, Label Encoding and Log Sum on Continuous Features](#modified_model_8)\n    * 5.9. [Continuous Features, Label Encoding and Multiplication on Continuous Features](#modified_model_9)\n    * 5.10. [Continuous Features, Label Encoding and Log Multiplication on Continuous Features](#modified_model_10)\n    * 5.11. [Continuous Features, Label Encoding and Prorate on Continuous Features](#modified_model_11)\n    * 5.12. [Continuous Features, Label Encoding and Log Prorate on Continuous Features](#modified_model_12)\n    * 5.13. [Exponential Continuous Features and Label Encoding](#modified_model_23)\n    * 5.14. [Boxcox Continuous Features and Label Encoding](#modified_model_23)\n6. [Base Regression Model and Target Encoding](#base_target_encode)\n    * 6.1. [Continuous Features and Mean Encoding](#modified_model_13)\n    * 6.2. [Continuous Features and Min Encoding](#modified_model_14)\n    * 6.3. [Continuous Features and Max Encoding](#modified_model_15)\n    * 6.4. [Continuous Features and Median Encoding](#modified_model_19)\n    * 6.5. [Continuous Features and Standar Deviation Encoding](#modified_model_20)\n    * 6.6. [Continuous Features and Skewness Encoding](#modified_model_21)\n    * 6.7. [Continuous Features, Mean, Median, Min, Max and Std Dev Encoding](#modified_model_16)\n7. [Base Regression Model and Categorical Encoding](#base_cat_encode)\n    * 7.1. [Continuous Features and Percentage Encoding](#modified_model_17)\n    * 7.2. [Continuous Features and Sum Categorical Encoding](#modified_model_18)\n    * 7.3. [Continuous Features and Multiplication Categorical Encoding](#modiefied_model_22)\n8. [Summary](#summary)","17ce2ce2":"[back to top](#table-of-contents)\n<a id=\"preparation\"><\/a>\n# 3. Preparation\nLoading packages, setup some function to cross validate the training dataset and set cross validation to 5."}}