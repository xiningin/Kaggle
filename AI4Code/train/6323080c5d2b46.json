{"cell_type":{"50075534":"code","75bd0e2c":"code","d2539394":"code","5fd9b047":"code","2784b34f":"code","1cebdb99":"code","6e5e1821":"code","2f117d1c":"code","5999fd97":"code","ea8452ea":"code","2c3046fe":"code","502b6ad4":"code","85eb2dfd":"code","220cb12b":"code","c80fca0b":"code","7241fc2b":"code","dfe45937":"code","da584b12":"code","1804e1ad":"code","f24ebd69":"code","906ae6b8":"code","3c9aea39":"code","8567e1f7":"code","42b836ca":"code","bdc65d81":"code","735ae108":"code","d8d40cf7":"code","10a36bb5":"code","e3ac7664":"code","d247b855":"code","de1c7a04":"code","b53b4fe2":"code","c4e33188":"code","6bcf6a6b":"code","39b14663":"code","d6223057":"code","b7a4f274":"code","713866df":"code","a8fb86c8":"code","262c9b90":"code","3d06adc6":"code","fe843bfc":"code","b5cb6492":"code","cc73f238":"code","d850e0bb":"code","3602a54e":"code","9ca12607":"code","40447b7f":"code","3d3e0cea":"code","2c5897a5":"code","52a8d48a":"code","0bd2afe2":"code","681dbfae":"code","57771fdc":"code","348fca56":"code","e29dab40":"code","97467830":"code","a355e105":"code","83d1a3e1":"code","8932ea1b":"code","d64f631f":"code","501ba544":"code","f689535a":"code","170f1db3":"code","1c02adb9":"code","1f03d7c3":"code","19e4d2f0":"code","4e89ded4":"code","b9e9c438":"code","42997d00":"code","81964ead":"code","5c10a701":"code","cf327ccd":"code","33cb36dc":"code","54d03352":"code","59a28dcc":"code","d355c0a6":"code","7ae14c7d":"code","dee6ccea":"code","4f04c26c":"code","f6c14ea4":"code","744ed0e9":"code","a85ee5e3":"code","b503262d":"code","6d992ab0":"code","4fdd90d0":"code","f0805fc8":"code","61f6f49f":"code","4133d79c":"code","1e4bfb53":"code","8b468d6d":"code","50b61859":"code","890a64c7":"code","9d5b828c":"code","7dfbeaac":"code","38275c08":"code","f5c8a0aa":"code","c39e0e18":"code","4f8bc2f4":"code","924cc717":"code","2948a6d1":"code","570013cc":"code","199be41c":"code","5f2f3ef6":"code","a771532e":"code","553ec53e":"code","08606a5d":"code","f8c33513":"code","b2d109b7":"code","e066162f":"code","e6454fee":"code","313a62d3":"code","c0cbc824":"code","5ce6b40f":"code","7b158ef3":"code","d21901b8":"code","630e0596":"code","58712e66":"code","8e17258c":"code","d1412756":"code","c4e6ef09":"code","775a7c38":"code","1cbab21f":"code","46ba71df":"code","b6fdcadd":"code","ca291728":"code","cd2011d1":"code","c0684413":"code","6f18143a":"code","a52713c5":"code","d8ef1d38":"code","ac43876c":"code","2df3aac6":"code","2af55eb1":"code","7321e7a0":"code","3b69e92f":"code","3c9fbe25":"code","3003d651":"code","419b2be1":"code","d0f19e31":"code","df99f7e0":"code","a7c2c7b1":"code","4b29c339":"code","1da2669d":"code","5ea345ff":"code","4dc5ef10":"code","32352061":"code","4f57204a":"code","474ec6d3":"code","d9c6e352":"code","3c6996e1":"code","eb6ec200":"code","5e15964b":"code","bc0e5c1e":"code","5ecb1807":"code","d496e845":"code","30a3874e":"code","4c002c88":"code","772bd229":"code","6a98f1a6":"code","d7824f4e":"code","42793851":"code","bea090fc":"code","ef13aecc":"code","683fd40d":"code","add81eb3":"code","2f02fd40":"code","6a7b49cf":"code","d541b22d":"code","dd4753d2":"markdown","b6ea5f56":"markdown","4dcdf9fd":"markdown","9476956b":"markdown","e2b158aa":"markdown","b2cddc09":"markdown","c43f42ec":"markdown","910fa206":"markdown","0ecdd603":"markdown","43cc27b3":"markdown","3821702b":"markdown","2eca633d":"markdown","299d4ee5":"markdown","fa930422":"markdown","c184042b":"markdown","a2d8be84":"markdown","9a2f446a":"markdown","63cff94d":"markdown","fa96d1bb":"markdown","27265925":"markdown","6ee93b48":"markdown","b014fec6":"markdown","31ab0c60":"markdown","d3645287":"markdown","43c18f20":"markdown","cf6de06d":"markdown","32b3b96b":"markdown","c527563b":"markdown","f401bcae":"markdown","bd5d00ad":"markdown","b7180716":"markdown","ff5eede5":"markdown","ba2f9098":"markdown","a5d3bd08":"markdown","97e998a3":"markdown","e01b4d9f":"markdown","feba1492":"markdown","ee7ccfa2":"markdown","45f4ef85":"markdown","8cbb5b9b":"markdown","ff220698":"markdown","f59ac50d":"markdown"},"source":{"50075534":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.impute import KNNImputer\nfrom sklearn.model_selection import cross_val_score,KFold,train_test_split\nfrom sklearn.linear_model import LinearRegression\nimport statsmodels.api as sm\nfrom statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.ensemble import BaggingRegressor,AdaBoostRegressor,GradientBoostingRegressor,RandomForestRegressor,VotingRegressor\nfrom sklearn.model_selection import cross_val_score,GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.feature_selection import RFE\nfrom sklearn.linear_model import RidgeCV, LassoCV, Ridge, Lasso, ElasticNet\nfrom sklearn.preprocessing import PolynomialFeatures\nimport statsmodels.formula.api as smf","75bd0e2c":"import os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\ndf=pd.read_csv('..\/input\/autompg-dataset\/auto-mpg.csv')\ndf.head()","d2539394":"df['origin'].replace({1:'American',2:'European',3:'Japanese'},inplace=True)","5fd9b047":"df.info()","2784b34f":"df.describe()","1cebdb99":"df['horsepower']=pd.to_numeric(df['horsepower'],errors='coerce')","6e5e1821":"df['mpg'].plot(kind='kde')","2f117d1c":"df['cylinders'].plot(kind='kde')","5999fd97":"df['displacement'].plot(kind='kde')","ea8452ea":"df['horsepower'].plot(kind='kde')","2c3046fe":"df['weight'].plot(kind='kde')","502b6ad4":"df['acceleration'].plot(kind='kde')","85eb2dfd":"plt.figure(figsize=(8,8))\nax=sns.countplot(df['origin'])\nfor i in ax.patches:\n    ax.annotate('{}'.format(i.get_height()),(i.get_x()+0.3,i.get_height()))","220cb12b":"plt.figure(figsize=(8,8))\nax=sns.barplot(x=df['origin'],y=df['weight'].median())","c80fca0b":"acc=(df.groupby('origin')['acceleration'].median())\nprint(acc)\nacc.plot(kind='bar')\nplt.ylabel('Avg Acceleration')","7241fc2b":"hp=(df.groupby('origin')['horsepower'].median())\nprint(hp)\nhp.plot(kind='bar')\nplt.ylabel('Avg. HP')","dfe45937":"mpg=(df.groupby('origin')['mpg'].median())\nprint(mpg)\nmpg.plot(kind='bar')\nplt.ylabel('Avg Mpg')","da584b12":"plt.figure(figsize=(8,8))\nax=sns.countplot(df['model year'])\nfor i in ax.patches:\n    ax.annotate('{}'.format(i.get_height()),(i.get_x()+0.3,i.get_height()))","1804e1ad":"sns.scatterplot(x=df['weight'],y=df['mpg'])","f24ebd69":"sns.scatterplot(x=df['weight'],y=df['horsepower'])","906ae6b8":"sns.scatterplot(x=df['horsepower'],y=df['mpg'])","3c9aea39":"sns.scatterplot(x=df['acceleration'],y=df['horsepower'])","8567e1f7":"cor_mat=df.corr()\nsns.heatmap(cor_mat,annot=True)","42b836ca":"sns.pairplot(df,vars=['mpg','cylinders','displacement','horsepower','weight','acceleration'])","bdc65d81":"col=['origin','model year']\ndf=pd.get_dummies(data=df,drop_first=True,columns=col)","735ae108":"df.head()","d8d40cf7":"df.drop('car name',axis=1,inplace=True)","10a36bb5":"imp=KNNImputer(missing_values=np.nan,n_neighbors=4)\ndf1=imp.fit_transform(df)","e3ac7664":"df=pd.DataFrame(df1,columns=df.columns)","d247b855":"df['horsepower'].unique()","de1c7a04":"# Base Model\nx=df.drop('mpg',axis=1)\ny=df['mpg']\nx_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.30,random_state=123)","b53b4fe2":"x_const=sm.add_constant(x_train)\nmodel=sm.OLS(y_train,x_const).fit()\nmodel.summary()","c4e33188":"vif = [variance_inflation_factor(x_const.values, i) for i in range(x_const.shape[1])]\npd.DataFrame({'vif': vif[1:]}, index=x_train.columns).T","6bcf6a6b":"x1=x.drop('horsepower',axis=1)","39b14663":"x_train,x_test,y_train,y_test=train_test_split(x1,y,test_size=0.30,random_state=123)","d6223057":"x_const=sm.add_constant(x_train)\nmodel=sm.OLS(y_train,x_const).fit()\nmodel.summary()","b7a4f274":"vif = [variance_inflation_factor(x_const.values, i) for i in range(x_const.shape[1])]\npd.DataFrame({'vif': vif[1:]}, index=x_train.columns).T","713866df":"x1=x1.drop('cylinders',axis=1)","a8fb86c8":"x_train,x_test,y_train,y_test=train_test_split(x1,y,test_size=0.30,random_state=123)","262c9b90":"x_const=sm.add_constant(x_train)\nmodel=sm.OLS(y_train,x_const).fit()\nmodel.summary()","3d06adc6":"vif = [variance_inflation_factor(x_const.values, i) for i in range(x_const.shape[1])]\npd.DataFrame({'vif': vif[1:]}, index=x_train.columns).T","fe843bfc":"x1=x1.drop('displacement',axis=1)","b5cb6492":"x_train,x_test,y_train,y_test=train_test_split(x1,y,test_size=0.30,random_state=123)","cc73f238":"x_const=sm.add_constant(x_train)\nmodel=sm.OLS(y_train,x_const).fit()\nmodel.summary()","d850e0bb":"vif = [variance_inflation_factor(x_const.values, i) for i in range(x_const.shape[1])]\npd.DataFrame({'vif': vif[1:]}, index=x_train.columns).T","3602a54e":"lr=LinearRegression()\nmodel=lr.fit(x_train,y_train)","9ca12607":"print(f'R^2 score for train: {lr.score(x_train, y_train)}')\nprint(f'R^2 score for test: {lr.score(x_test, y_test)}')","40447b7f":"y_pred=lr.predict(x_test)","3d3e0cea":"cv_results = cross_val_score(lr, x_train, y_train,cv=5, scoring='neg_mean_squared_error')\nprint(np.mean(np.sqrt(np.abs(cv_results))))\nprint(np.std(np.sqrt(np.abs(cv_results)),ddof=1))","2c5897a5":"mse=mean_squared_error(y_test,y_pred)\nrmse=np.sqrt(mse)\nprint(rmse)","52a8d48a":"GB_bias=[]\nGB_var=[]\nfor n in np.arange(1,150):\n    GB=GradientBoostingRegressor(n_estimators=n,random_state=0)\n    scores=cross_val_score(GB,x_train,y_train,cv=5,scoring='neg_mean_squared_error')\n    rmse=np.sqrt(np.abs(scores))\n    GB_bias.append(np.mean(rmse))\n    GB_var.append(np.std(rmse,ddof=1))","0bd2afe2":"x_axis=np.arange(len(GB_bias))\nplt.plot(x_axis,GB_bias)","681dbfae":"np.argmin(GB_var),GB_var[np.argmin(GB_var)],GB_bias[np.argmin(GB_var)]","57771fdc":"np.argmin(GB_bias),GB_bias[np.argmin(GB_bias)],GB_var[np.argmin(GB_bias)]","348fca56":"ABLR_bias=[]\nABLR_var=[]\nfor n in np.arange(1,150):\n    ABLR=AdaBoostRegressor(base_estimator=lr,n_estimators=n,random_state=0)\n    scores=cross_val_score(ABLR,x_train,y_train,cv=5,scoring='neg_mean_squared_error')\n    rmse=np.sqrt(np.abs(scores))\n    ABLR_bias.append(np.mean(rmse))\n    ABLR_var.append(np.std(rmse,ddof=1))","e29dab40":"x_axis=np.arange(len(ABLR_bias))\nplt.plot(x_axis,ABLR_bias)","97467830":"np.argmin(ABLR_bias), ABLR_bias[np.argmin(ABLR_bias)],ABLR_var[np.argmin(ABLR_bias)]","a355e105":"np.argmin(ABLR_var), ABLR_var[np.argmin(ABLR_var)],ABLR_bias[np.argmin(ABLR_var)]","83d1a3e1":"Bag_bias=[]\nBag_var=[]\nfor n in np.arange(1,150):\n    Bag=BaggingRegressor(base_estimator=lr,n_estimators=n,random_state=0)\n    scores=cross_val_score(Bag,x_train,y_train,cv=5,scoring='neg_mean_squared_error')\n    rmse=np.sqrt(np.abs(scores))\n    Bag_bias.append(np.mean(rmse))\n    Bag_var.append(np.std(rmse,ddof=1))","8932ea1b":"np.argmin(Bag_var),Bag_var[np.argmin(Bag_var)],Bag_bias[np.argmin(Bag_var)]","d64f631f":"np.argmin(Bag_bias),Bag_bias[np.argmin(Bag_bias)],Bag_var[np.argmin(Bag_bias)]","501ba544":"RF_bias=[]\nRF_var=[]\nfor n in np.arange(1,150):\n    RF=RandomForestRegressor(criterion='mse',n_estimators=n,random_state=0)\n    scores=cross_val_score(RF,x_train,y_train,cv=5,scoring='neg_mean_squared_error')\n    rmse=np.sqrt(np.abs(scores))\n    RF_bias.append(np.mean(rmse))\n    RF_var.append(np.std(rmse,ddof=1))","f689535a":"np.argmin(RF_bias),RF_bias[np.argmin(RF_bias)],RF_var[np.argmin(RF_bias)]","170f1db3":"np.argmin(RF_var),RF_var[np.argmin(RF_var)],RF_bias[np.argmin(RF_var)]","1c02adb9":"x_train,x_test,y_train,y_test=train_test_split(x,y,test_size=0.30,random_state=1234)\nss=StandardScaler()\nx_s=ss.fit_transform(x)\nx_trains=ss.fit_transform(x_train)\nx_tests=ss.transform(x_test)","1f03d7c3":"lr=LinearRegression()\nmodel=lr.fit(x_trains,y_train)","19e4d2f0":"print(f'R^2 score for train: {lr.score(x_trains, y_train)}')\nprint(f'R^2 score for test: {lr.score(x_tests, y_test)}')","4e89ded4":"cv_results = cross_val_score(lr, x_trains, y_train,cv=5, scoring='neg_mean_squared_error')\nprint(np.mean(np.sqrt(np.abs(cv_results))))\nprint(np.std(np.sqrt(np.abs(cv_results)),ddof=1))","b9e9c438":"y_pred=lr.predict(x_tests)","42997d00":"mse=mean_squared_error(y_test,y_pred)\nrmse=np.sqrt(mse)\nprint(rmse)","81964ead":"GB_bias=[]\nGB_var=[]\nfor n in np.arange(1,150):\n    GB=GradientBoostingRegressor(n_estimators=n,random_state=0)\n    scores=cross_val_score(GB,x_trains,y_train,cv=5,scoring='neg_mean_squared_error')\n    rmse=np.sqrt(np.abs(scores))\n    GB_bias.append(np.mean(rmse))\n    GB_var.append(np.std(rmse,ddof=1))","5c10a701":"np.argmin(GB_var),GB_var[np.argmin(GB_var)],GB_bias[np.argmin(GB_var)]","cf327ccd":"np.argmin(GB_bias),GB_bias[np.argmin(GB_bias)],GB_var[np.argmin(GB_bias)]","33cb36dc":"ABLR_bias=[]\nABLR_var=[]\nfor n in np.arange(1,150):\n    ABLR=AdaBoostRegressor(base_estimator=lr,n_estimators=n,random_state=0)\n    scores=cross_val_score(ABLR,x_trains,y_train,cv=5,scoring='neg_mean_squared_error')\n    rmse=np.sqrt(np.abs(scores))\n    ABLR_bias.append(np.mean(rmse))\n    ABLR_var.append(np.std(rmse,ddof=1))","54d03352":"np.argmin(ABLR_bias), ABLR_bias[np.argmin(ABLR_bias)],ABLR_var[np.argmin(ABLR_bias)]","59a28dcc":"np.argmin(ABLR_var), ABLR_var[np.argmin(ABLR_var)],ABLR_bias[np.argmin(ABLR_var)]","d355c0a6":"Bag_bias=[]\nBag_var=[]\nfor n in np.arange(1,150):\n    Bag=BaggingRegressor(base_estimator=lr,n_estimators=n,random_state=0)\n    scores=cross_val_score(Bag,x_train,y_train,cv=5,scoring='neg_mean_squared_error')\n    rmse=np.sqrt(np.abs(scores))\n    Bag_bias.append(np.mean(rmse))\n    Bag_var.append(np.std(rmse,ddof=1))","7ae14c7d":"np.argmin(Bag_var),Bag_var[np.argmin(Bag_var)],Bag_bias[np.argmin(Bag_var)]","dee6ccea":"np.argmin(Bag_bias),Bag_bias[np.argmin(Bag_bias)],Bag_var[np.argmin(Bag_bias)]","4f04c26c":"RF_bias=[]\nRF_var=[]\nfor n in np.arange(1,150):\n    RF=RandomForestRegressor(criterion='mse',n_estimators=n,random_state=0)\n    scores=cross_val_score(RF,x_trains,y_train,cv=5,scoring='neg_mean_squared_error')\n    rmse=np.sqrt(np.abs(scores))\n    RF_bias.append(np.mean(rmse))\n    RF_var.append(np.std(rmse,ddof=1))","f6c14ea4":"np.argmin(RF_bias),RF_bias[np.argmin(RF_bias)],RF_var[np.argmin(RF_bias)]","744ed0e9":"np.argmin(RF_var),RF_var[np.argmin(RF_var)],RF_bias[np.argmin(RF_var)]","a85ee5e3":"cols = list(x.columns)\npmax = 1\nwhile (len(cols)>0):\n    p= []\n    x = x[cols]\n    Xc = sm.add_constant(x)\n    model = sm.OLS(y,Xc).fit()\n    p = pd.Series(model.pvalues.values[1:],index = cols)      \n    pmax = max(p)\n    feature_with_p_max = p.idxmax()\n    if(pmax>0.05):\n        cols.remove(feature_with_p_max)\n    else:\n        break\nselected_features = cols\nprint(selected_features)","b503262d":"X_new=x[selected_features]\nX_new.head()","6d992ab0":"x_train1,x_test1,y_train1,y_test1=train_test_split(X_new,y,test_size=0.30,random_state=1234)","4fdd90d0":"lr=LinearRegression()\nmodel=lr.fit(x_train1,y_train1)","f0805fc8":"print(f'R^2 score for train: {lr.score(x_train1, y_train1)}')\nprint(f'R^2 score for test: {lr.score(x_test1, y_test1)}')","61f6f49f":"cv_results = cross_val_score(lr, x_train1, y_train1,cv=5, scoring='neg_mean_squared_error')\nprint(np.mean(np.sqrt(np.abs(cv_results))))\nprint(np.std(np.sqrt(np.abs(cv_results)),ddof=1))","4133d79c":"y_pred=lr.predict(x_test1)","1e4bfb53":"mse=mean_squared_error(y_test1,y_pred)\nrmse=np.sqrt(mse)\nprint(rmse)","8b468d6d":"GB_bias=[]\nGB_var=[]\nfor n in np.arange(1,150):\n    GB=GradientBoostingRegressor(n_estimators=n,random_state=0)\n    scores=cross_val_score(GB,x_train1,y_train1,cv=5,scoring='neg_mean_squared_error')\n    rmse=np.sqrt(np.abs(scores))\n    GB_bias.append(np.mean(rmse))\n    GB_var.append(np.std(rmse,ddof=1))","50b61859":"np.argmin(GB_var),GB_var[np.argmin(GB_var)],GB_bias[np.argmin(GB_var)]","890a64c7":"np.argmin(GB_bias),GB_bias[np.argmin(GB_bias)],GB_var[np.argmin(GB_bias)]","9d5b828c":"ABLR_bias=[]\nABLR_var=[]\nfor n in np.arange(1,150):\n    ABLR=AdaBoostRegressor(base_estimator=lr,n_estimators=n,random_state=0)\n    scores=cross_val_score(ABLR,x_train1,y_train1,cv=5,scoring='neg_mean_squared_error')\n    rmse=np.sqrt(np.abs(scores))\n    ABLR_bias.append(np.mean(rmse))\n    ABLR_var.append(np.std(rmse,ddof=1))","7dfbeaac":"np.argmin(ABLR_bias), ABLR_bias[np.argmin(ABLR_bias)],ABLR_var[np.argmin(ABLR_bias)]","38275c08":"np.argmin(ABLR_var), ABLR_var[np.argmin(ABLR_var)],ABLR_bias[np.argmin(ABLR_var)]","f5c8a0aa":"Bag_bias=[]\nBag_var=[]\nfor n in np.arange(1,150):\n    Bag=BaggingRegressor(base_estimator=lr,n_estimators=n,random_state=0)\n    scores=cross_val_score(Bag,x_train1,y_train1,cv=5,scoring='neg_mean_squared_error')\n    rmse=np.sqrt(np.abs(scores))\n    Bag_bias.append(np.mean(rmse))\n    Bag_var.append(np.std(rmse,ddof=1))","c39e0e18":"np.argmin(Bag_var),Bag_var[np.argmin(Bag_var)],Bag_bias[np.argmin(Bag_var)]","4f8bc2f4":"np.argmin(Bag_bias),Bag_bias[np.argmin(Bag_bias)],Bag_var[np.argmin(Bag_bias)]","924cc717":"RF_bias=[]\nRF_var=[]\nfor n in np.arange(1,150):\n    RF=RandomForestRegressor(criterion='mse',n_estimators=n,random_state=0)\n    scores=cross_val_score(RF,x_train1,y_train1,cv=5,scoring='neg_mean_squared_error')\n    rmse=np.sqrt(np.abs(scores))\n    RF_bias.append(np.mean(rmse))\n    RF_var.append(np.std(rmse,ddof=1))","2948a6d1":"np.argmin(RF_bias),RF_bias[np.argmin(RF_bias)],RF_var[np.argmin(RF_bias)]","570013cc":"np.argmin(RF_var),RF_var[np.argmin(RF_var)],RF_bias[np.argmin(RF_var)]","199be41c":"x_news=ss.fit_transform(X_new)\nx_train1s=ss.fit_transform(x_train1)\nx_test1s=ss.transform(x_test1)","5f2f3ef6":"lr=LinearRegression()\nmodel=lr.fit(x_train1s,y_train1)","a771532e":"print(f'R^2 score for train: {lr.score(x_train1s, y_train1)}')\nprint(f'R^2 score for test: {lr.score(x_test1s, y_test1)}')","553ec53e":"cv_results = cross_val_score(lr, x_train1s, y_train1,cv=5, scoring='neg_mean_squared_error')\nprint(np.mean(np.sqrt(np.abs(cv_results))))\nprint(np.std(np.sqrt(np.abs(cv_results)),ddof=1))","08606a5d":"y_pred=lr.predict(x_test1s)","f8c33513":"mse=mean_squared_error(y_test1,y_pred)\nrmse=np.sqrt(mse)\nprint(rmse)","b2d109b7":"GB_bias=[]\nGB_var=[]\nfor n in np.arange(1,150):\n    GB=GradientBoostingRegressor(n_estimators=n,random_state=0)\n    scores=cross_val_score(GB,x_train1s,y_train1,cv=5,scoring='neg_mean_squared_error')\n    rmse=np.sqrt(np.abs(scores))\n    GB_bias.append(np.mean(rmse))\n    GB_var.append(np.std(rmse,ddof=1))","e066162f":"np.argmin(GB_bias),GB_bias[np.argmin(GB_bias)],GB_var[np.argmin(GB_bias)]","e6454fee":"np.argmin(GB_var),GB_var[np.argmin(GB_var)],GB_bias[np.argmin(GB_var)]","313a62d3":"ABLR_bias=[]\nABLR_var=[]\nfor n in np.arange(1,150):\n    ABLR=AdaBoostRegressor(base_estimator=lr,n_estimators=n,random_state=0)\n    scores=cross_val_score(ABLR,x_train1s,y_train1,cv=5,scoring='neg_mean_squared_error')\n    rmse=np.sqrt(np.abs(scores))\n    ABLR_bias.append(np.mean(rmse))\n    ABLR_var.append(np.std(rmse,ddof=1))","c0cbc824":"np.argmin(ABLR_bias), ABLR_bias[np.argmin(ABLR_bias)],ABLR_var[np.argmin(ABLR_bias)]","5ce6b40f":"np.argmin(ABLR_var), ABLR_var[np.argmin(ABLR_var)],ABLR_bias[np.argmin(ABLR_var)]","7b158ef3":"Bag_bias=[]\nBag_var=[]\nfor n in np.arange(1,150):\n    Bag=BaggingRegressor(base_estimator=lr,n_estimators=n,random_state=0)\n    scores=cross_val_score(Bag,x_train1s,y_train1,cv=5,scoring='neg_mean_squared_error')\n    rmse=np.sqrt(np.abs(scores))\n    Bag_bias.append(np.mean(rmse))\n    Bag_var.append(np.std(rmse,ddof=1))","d21901b8":"np.argmin(Bag_bias),Bag_bias[np.argmin(Bag_bias)],Bag_var[np.argmin(Bag_bias)]","630e0596":"np.argmin(Bag_var),Bag_var[np.argmin(Bag_var)],Bag_bias[np.argmin(Bag_var)]","58712e66":"RF_bias=[]\nRF_var=[]\nfor n in np.arange(1,150):\n    RF=RandomForestRegressor(criterion='mse',n_estimators=n,random_state=0)\n    scores=cross_val_score(RF,x_train1s,y_train1,cv=5,scoring='neg_mean_squared_error')\n    rmse=np.sqrt(np.abs(scores))\n    RF_bias.append(np.mean(rmse))\n    RF_var.append(np.std(rmse,ddof=1))","8e17258c":"np.argmin(RF_bias),RF_bias[np.argmin(RF_bias)],RF_var[np.argmin(RF_bias)]","d1412756":"np.argmin(RF_var),RF_var[np.argmin(RF_var)],RF_bias[np.argmin(RF_var)]","c4e6ef09":"Rd=Ridge(alpha=0.5,normalize=True)\nLs=Lasso(alpha=0.1,normalize=True)\nEn=ElasticNet(alpha=0.01,l1_ratio=0.919,normalize=True)\nmodels = []\nmodels.append(('Ridge',Rd))\nmodels.append(('Lasso',Ls))\nmodels.append(('Elastic',En))","775a7c38":"results = []\nnames = []\nfor name, model in models:\n    kfold = KFold(shuffle=True,n_splits=5,random_state=0)\n    cv_results = cross_val_score(model, x, y,cv=kfold, scoring='neg_mean_squared_error')\n    results.append(np.sqrt(np.abs(cv_results)))\n    names.append(name)\n    print(\"%s: %f (%f)\" % (name, np.mean(np.sqrt(np.abs(cv_results))),np.std(np.sqrt(np.abs(cv_results)),ddof=1)))\n   # boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","1cbab21f":"results = []\nnames = []\nfor name, model in models:\n    kfold = KFold(shuffle=True,n_splits=5,random_state=0)\n    cv_results = cross_val_score(model, X_new, y,cv=kfold, scoring='neg_mean_squared_error')\n    results.append(np.sqrt(np.abs(cv_results)))\n    names.append(name)\n    print(\"%s: %f (%f)\" % (name, np.mean(np.sqrt(np.abs(cv_results))),np.std(np.sqrt(np.abs(cv_results)),ddof=1)))\n   # boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","46ba71df":"df","b6fdcadd":"x_final=df.drop('mpg',axis=1)","ca291728":"x_qr=x_final[['displacement','horsepower','weight','acceleration']]","cd2011d1":"qr=PolynomialFeatures(degree=2)\nx_qr=qr.fit_transform(x_qr)","c0684413":"x_qr_df=pd.DataFrame(x_qr)\nx_qr_df.head()","6f18143a":"x_qr_df=x_qr_df.drop(0,axis=1)","a52713c5":"idx=np.arange(x_final.shape[0])","d8ef1d38":"y.index=idx","ac43876c":"x_final.index=idx","2df3aac6":"x_qr_df=pd.concat([x_final,x_qr_df,y],axis=1)","2af55eb1":"x_qr_df.head()","7321e7a0":"x_qr_df.drop(['displacement','horsepower','weight','acceleration'],axis=1,inplace=True)","3b69e92f":"x_qr_df.columns","3c9fbe25":"x_qr_df.columns=['cylinders', 'origin_European', 'origin_Japanese','model year_71',   'model year_72',   'model year_73',\n                 'model year_74',   'model year_75',   'model year_76','model year_77',   'model year_78',   'model year_79',\n                 'model year_80',   'model year_81',   'model year_82','f1','f2','f3','f4','f5','f6','f7','f8','f9','f10',\n                 'f11','f12','f13','f14','mpg']","3003d651":"x_qr=x_qr_df.drop('mpg',axis=1)\ny_qr=x_qr_df['mpg']","419b2be1":"qr=LinearRegression()\nmodels = []\nmodels.append(('Ridge',Rd))\nmodels.append(('Lasso',Ls))\nmodels.append(('Elastic',En))\nmodels.append(('Quadratic',qr))","d0f19e31":"results = []\nnames = []\nfor name, model in models:\n    kfold = KFold(shuffle=True,n_splits=5,random_state=0)\n    cv_results = cross_val_score(model, x_qr, y_qr,cv=kfold, scoring='neg_mean_squared_error')\n    results.append(np.sqrt(np.abs(cv_results)))\n    names.append(name)\n    print(\"%s: %f (%f)\" % (name, np.mean(np.sqrt(np.abs(cv_results))),np.std(np.sqrt(np.abs(cv_results)),ddof=1)))\n   # boxplot algorithm comparison\nfig = plt.figure()\nfig.suptitle('Algorithm Comparison')\nax = fig.add_subplot(111)\nplt.boxplot(results)\nax.set_xticklabels(names)\nplt.show()","df99f7e0":"xqr_train,xqr_test,yqr_train,yqr_test=train_test_split(x_qr,y_qr,test_size=0.30,random_state=1234)\ncv_results = cross_val_score(qr, xqr_train, yqr_train,cv=5, scoring='neg_mean_squared_error')\nprint(np.mean(np.sqrt(np.abs(cv_results))))\nprint(np.std(np.sqrt(np.abs(cv_results)),ddof=1))","a7c2c7b1":"y_pred=lr.predict(xqr_test)\nmse=mean_squared_error(yqr_test,y_pred)\nrmse=np.sqrt(mse)\nprint(rmse)","4b29c339":"GB_bias=[]\nGB_var=[]\nfor n in np.arange(1,150):\n    GB=GradientBoostingRegressor(n_estimators=n,random_state=0)\n    scores=cross_val_score(GB,xqr_train,yqr_train,cv=5,scoring='neg_mean_squared_error')\n    rmse=np.sqrt(np.abs(scores))\n    GB_bias.append(np.mean(rmse))\n    GB_var.append(np.std(rmse,ddof=1))","1da2669d":"np.argmin(GB_bias),GB_bias[np.argmin(GB_bias)],GB_var[np.argmin(GB_bias)]","5ea345ff":"np.argmin(GB_var),GB_var[np.argmin(GB_var)],GB_bias[np.argmin(GB_var)]","4dc5ef10":"ABLR_bias=[]\nABLR_var=[]\nfor n in np.arange(1,150):\n    ABLR=AdaBoostRegressor(base_estimator=lr,n_estimators=n,random_state=0)\n    scores=cross_val_score(ABLR,xqr_train,yqr_train,cv=5,scoring='neg_mean_squared_error')\n    rmse=np.sqrt(np.abs(scores))\n    ABLR_bias.append(np.mean(rmse))\n    ABLR_var.append(np.std(rmse,ddof=1))","32352061":"np.argmin(ABLR_bias), ABLR_bias[np.argmin(ABLR_bias)],ABLR_var[np.argmin(ABLR_bias)]","4f57204a":"np.argmin(ABLR_var), ABLR_var[np.argmin(ABLR_var)],ABLR_bias[np.argmin(ABLR_var)]","474ec6d3":"Bag_bias=[]\nBag_var=[]\nfor n in np.arange(1,150):\n    Bag=BaggingRegressor(base_estimator=lr,n_estimators=n,random_state=0)\n    scores=cross_val_score(Bag,xqr_train,yqr_train,cv=5,scoring='neg_mean_squared_error')\n    rmse=np.sqrt(np.abs(scores))\n    Bag_bias.append(np.mean(rmse))\n    Bag_var.append(np.std(rmse,ddof=1))","d9c6e352":"np.argmin(Bag_bias),Bag_bias[np.argmin(Bag_bias)],Bag_var[np.argmin(Bag_bias)]","3c6996e1":"np.argmin(Bag_var),Bag_var[np.argmin(Bag_var)],Bag_bias[np.argmin(Bag_var)]","eb6ec200":"RF_bias=[]\nRF_var=[]\nfor n in np.arange(1,150):\n    RF=RandomForestRegressor(criterion='mse',n_estimators=n,random_state=0)\n    scores=cross_val_score(RF,xqr_train,yqr_train,cv=5,scoring='neg_mean_squared_error')\n    rmse=np.sqrt(np.abs(scores))\n    RF_bias.append(np.mean(rmse))\n    RF_var.append(np.std(rmse,ddof=1))","5e15964b":"np.argmin(RF_bias),RF_bias[np.argmin(RF_bias)],RF_var[np.argmin(RF_bias)]","bc0e5c1e":"np.argmin(RF_var),RF_var[np.argmin(RF_var)],RF_bias[np.argmin(RF_var)]","5ecb1807":"xqr_s=ss.fit_transform(x_qr)\nxqr_trains=ss.fit_transform(xqr_train)\nxqr_tests=ss.transform(xqr_test)","d496e845":"cv_results = cross_val_score(qr, xqr_trains, yqr_train,cv=5, scoring='neg_mean_squared_error')\nprint(np.mean(np.sqrt(np.abs(cv_results))))\nprint(np.std(np.sqrt(np.abs(cv_results)),ddof=1))","30a3874e":"y_pred=qr.predict(xqr_tests)\nmse=mean_squared_error(yqr_test,y_pred)\nrmse=np.sqrt(mse)\nprint(rmse)","4c002c88":"GB_bias=[]\nGB_var=[]\nfor n in np.arange(1,150):\n    GB=GradientBoostingRegressor(n_estimators=n,random_state=0)\n    scores=cross_val_score(GB,xqr_trains,yqr_train,cv=5,scoring='neg_mean_squared_error')\n    rmse=np.sqrt(np.abs(scores))\n    GB_bias.append(np.mean(rmse))\n    GB_var.append(np.std(rmse,ddof=1))","772bd229":"np.argmin(GB_bias),GB_bias[np.argmin(GB_bias)],GB_var[np.argmin(GB_bias)]","6a98f1a6":"np.argmin(GB_var),GB_var[np.argmin(GB_var)],GB_bias[np.argmin(GB_var)]","d7824f4e":"ABLR_bias=[]\nABLR_var=[]\nfor n in np.arange(1,150):\n    ABLR=AdaBoostRegressor(base_estimator=lr,n_estimators=n,random_state=0)\n    scores=cross_val_score(ABLR,xqr_trains,yqr_train,cv=5,scoring='neg_mean_squared_error')\n    rmse=np.sqrt(np.abs(scores))\n    ABLR_bias.append(np.mean(rmse))\n    ABLR_var.append(np.std(rmse,ddof=1))","42793851":"np.argmin(ABLR_bias), ABLR_bias[np.argmin(ABLR_bias)],ABLR_var[np.argmin(ABLR_bias)]","bea090fc":"np.argmin(ABLR_var), ABLR_var[np.argmin(ABLR_var)],ABLR_bias[np.argmin(ABLR_var)]","ef13aecc":"Bag_bias=[]\nBag_var=[]\nfor n in np.arange(1,150):\n    Bag=BaggingRegressor(base_estimator=lr,n_estimators=n,random_state=0)\n    scores=cross_val_score(Bag,xqr_trains,yqr_train,cv=5,scoring='neg_mean_squared_error')\n    rmse=np.sqrt(np.abs(scores))\n    Bag_bias.append(np.mean(rmse))\n    Bag_var.append(np.std(rmse,ddof=1))","683fd40d":"np.argmin(Bag_bias),Bag_bias[np.argmin(Bag_bias)],Bag_var[np.argmin(Bag_bias)]","add81eb3":"np.argmin(Bag_var),Bag_var[np.argmin(Bag_var)],Bag_bias[np.argmin(Bag_var)]","2f02fd40":"RF_bias=[]\nRF_var=[]\nfor n in np.arange(1,150):\n    RF=RandomForestRegressor(criterion='mse',n_estimators=n,random_state=0)\n    scores=cross_val_score(RF,xqr_trains,yqr_train,cv=5,scoring='neg_mean_squared_error')\n    rmse=np.sqrt(np.abs(scores))\n    RF_bias.append(np.mean(rmse))\n    RF_var.append(np.std(rmse,ddof=1))","6a7b49cf":"np.argmin(RF_bias),RF_bias[np.argmin(RF_bias)],RF_var[np.argmin(RF_bias)]","d541b22d":"np.argmin(RF_var),RF_var[np.argmin(RF_var)],RF_bias[np.argmin(RF_var)]","dd4753d2":"# **Bagging Regressor**","b6ea5f56":"1. We can observe that there is good positive correlation between displacement and number of cylinders. As number of cylinders increases displacement increases.\n2. Also there is good positive correlation between weight and number of cylinders, since increase in no. of cylinders increases dead weight of car which also increases displacement of car. Thus we can say that weight and displacement are directly correlated which is evident from above heatmap","4dcdf9fd":"American origin cars have lowet mpg among 3 origin cars. Japanese cars have highest mpg.","9476956b":"Most of  cars are heavy, there are very few cars which are light.","e2b158aa":"Multiple peaks indicates, cylinders is discrete feature","b2cddc09":"Increase in horsepower increases weight which reduces mpg","c43f42ec":"# **Bagging Regressor**","910fa206":"# **Importing Data**","0ecdd603":"Most of cars have low to average displacements, few have very high displacements","43cc27b3":"# **Missing Value Imputation**\nBefore using KNN imputer, let us create dummy columns for origin, model year and we will drop 'car name'","3821702b":"Most of the cars were introduced in the year 1973","2eca633d":"# Data Information\n\nData contains technical information about different cars from which we need to predict mpg of car.\nFollowing are the information about the features:\n\n1. mpg: Miles per gallon run by car\n2. cylinders: No. of cylinders in engine\n3. displacement: engine displacement in cubic inches\n4. horsepower: Horse power of particular car\n5. weight: Dead weight of car in lbs\n6. acceleration: Time taken for car to reach from 0 mph to 60 mph\n7. model year: Year in which car was released\n8. origin: Country of origin 1 - American, 2 - European, 3 - Japenese\n9. car name: Name of car","299d4ee5":"Distribution is almost normally distributed","fa930422":"Japanese origin cars have highest acceleration whereas American origin cars have least acceleration among 3 origin","c184042b":"Higher the horsepower, more no. of cylinders, hence more displacement. Higher the displacement, lesser is the time to accelerate","a2d8be84":"# **Standardising the polynomial features**","9a2f446a":"In, the above cell, even though horsepower is numerical feature, it is shown as object","63cff94d":"There are 249 cars of american origin, 79 of Japenese origin and 70 cars of European origin","fa96d1bb":"1. Cars of American origin have highest horsepower. European and Japanese origin are no where near American cars in terms of horse power.\n2. Even though Japanese origin cars have highest acceleration, they have least average horsepower\n3. American origin cars have highest horse power. They outperform Japanese origin but they have lesser acceleration  ","27265925":"# **Model Building**","6ee93b48":"# **Boosting Regressors**","b014fec6":"# Below are the results of all combinations:\n\n1. Removing features which are multicollinear:\n    1. Linear Regression - rmse = 3.18 (0.25)\n    2. GB Regressor - n_estimator = 21, rmse = 3.76 (0.15)\n    3. AdaBoost Regressor - n_estimator = 2, rmse = 3.26 (0.16)\n    4. Bagging LR - n_estimator = 1, rmse = 3.25 (0.16)\n    5. Random Forest - n_estimator = 93, rmse = 3.52 (0.15)\n    \n2. Removing features which are multicollinear and standardising the data:\n    1. Linear Regression - rmse = 3.19 (0.19)\n    2. GB Regressor - n_estimator = 9, rmse = 4.62 (0.22)\n    3. AdaBoost Regressor - n_estimator = 3, rmse = 3.16 (0.14)\n    4. Bagging LR - n_estimator = 135, rmse = 3.19 (0.18)\n    5. Random Forest - n_estimator = 8, rmse = 3.58 (0.42)\n    \n3. Removing features with P-value greater than 0.05:\n    1. Linear Regression - rmse = 3.198 (0.17)\n    2. GB Regressor - n_estimator = 130, rmse = 3.195 (0.58)\n    3. AdaBoost Regressor - n_estimator = 3, rmse = 3.155 (0.14)\n    4. Bagging LR - n_estimator = 62, rmse = 3.199 (0.168)\n    5. Random Forest - n_estimator = 135, rmse = 3.39 (0.46)\n    \n4. Removing Features with P-Value greater than 0.05 and standardising the data:\n    1. Linear Regression - rmse = 3.198 (0.17)\n    2. GB Regressor - n_estimator = 6, rmse = 5.176 (0.26)\n    3. AdaBoost Regressor - n_estimator = 3, rmse = 3.155 (0.14)\n    4. Bagging LR - n_estimator = 62, rmse = 3.199 (0.168)\n    5. Random Forest - n_estimator = 135, rmse = 3.395 (0.457)\n    \n5. Polynomial Features\n    1. Linear Regression - rmse = 2.84 (0.34)\n    2. GB Regressor - n_estimator = 100, rmse = 3.32 (0.30)\n    3. AdaBoost Regressor - n_estimator = 1, rmse = 3.11 (0.28)\n    4. Bagging LR - n_estimator = 116, rmse = 2.83 (0.33)\n    5. Random Forest - n_estimator = 135, rmse = 3.395 (0.457)\n    \n**It can be seen that Ada Boost regressor with features less than 0.05 has lowest RMSE, hence this can be used for our predictions**","31ab0c60":"Replacing the values of origin column to get better results\n\n1 to be replaced with American, 2 with European and 3 with Japan","d3645287":"Most of cars low to average horse powers. Few cars have high horse power","43c18f20":"# **Importing Necessary Libraries**","cf6de06d":" # **Exploratory Data Analysis**","32b3b96b":"# Ridge and Lasso Regression","c527563b":"# **Bagging Regressor**","f401bcae":"It can be seen that as weight increases, it requires higher amount of fuel to move as a result mpg reduces","bd5d00ad":"**In this kernel, I have used 4 different cases with 5 algorithms to predict best possible RMSE value along with varience error**\nFollowing are the four different cases that are tried:\n1. Removing features which are multicollinear\n2. Standardising the data after removing features which are collinear\n3. Removing features with P-value greater than 0.05\n4. Removing features with P-value greater than 0.05 and standardising the data\n\nBelow are the five algorithms that are used in our anaysis:\n\n1. Linear Regression\n2. Gradient Boost Regressor\n3. AdaBoost Regressor\n4. Linear Regressor Bagging\n5. Random Forest regressor","b7180716":"# **Bagging Regressors**","ff5eede5":"If we need higher horsepower, we need more number of cylinders and increase in number of increases weight of car","ba2f9098":"# **Feature Elimination**","a5d3bd08":"acceleration values are almostnormally distributed","97e998a3":"# **Boosting Regressor**","e01b4d9f":"# **Standardising the data**","feba1492":"# **Boosting Regressor**","ee7ccfa2":"# **Bagging Regressor**","45f4ef85":"Weight of cars of all origins are almost same","8cbb5b9b":"# **Standardising the selected features**","ff220698":"# **Boosting Regressor**","f59ac50d":"# **Polynomial Regression**"}}