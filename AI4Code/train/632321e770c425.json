{"cell_type":{"77a6140f":"code","f62bad16":"code","9722112c":"code","ce04f522":"code","2e7983c2":"code","c2642b56":"code","423ffdbc":"code","7e604ca8":"code","736c1ebe":"code","2434a14f":"code","8c824584":"code","3a362119":"code","2a0b9938":"code","2e4d2e4a":"code","3ea749de":"code","723c42b1":"code","977053c4":"code","3804aef3":"code","bcf2db6c":"code","8a1d4321":"code","32a4a7a7":"code","5d937c41":"code","e357e7a6":"code","02d85797":"code","3e7263de":"code","f9ddf64e":"code","4a810b43":"code","2d110f35":"code","550b03d4":"code","078ad874":"code","ffd01825":"code","f8bbb909":"code","44fdea58":"code","cb5f36f8":"code","77dbac4e":"markdown","cfe4f524":"markdown","d44aef7b":"markdown","0d147725":"markdown","c703903c":"markdown","d08441ca":"markdown","d3f2568b":"markdown","522d2d3c":"markdown","e44c8fc3":"markdown","c350543e":"markdown","a8d9e31b":"markdown","403b7ccb":"markdown"},"source":{"77a6140f":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport re, pickle, random, os\nfrom sklearn.preprocessing import LabelEncoder\nimport lightgbm as lgb\nimport xgboost as xgb\nfrom catboost import Pool\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.metrics import accuracy_score\nimport optuna\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torchvision import datasets\nimport torchvision.transforms as transforms\nfrom torch.autograd import Variable\n\n%matplotlib inline\n\n# suppress lengthy outputs\nimport warnings\nwarnings.simplefilter('ignore')\n\noptuna.logging.disable_default_handler()","f62bad16":"# ensure consistent results\ndef seed_everything(seed=1234):\n    random.seed(seed)\n    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nRANDOM_STATE = 42\nseed_everything(seed=RANDOM_STATE)","9722112c":"train = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest = pd.read_csv(\"..\/input\/titanic\/test.csv\")","ce04f522":"train.head()","2e7983c2":"def preprocess(train, test):\n    # concatinate\n    train[\"IsTrain\"] = 1\n    test[\"IsTrain\"] = 0\n    full = pd.concat([train, test])\n\n    # remove unnecessary columns\n    full = full.drop([\"PassengerId\"], axis=1)\n\n    full[\"Name_length\"] = full[\"Name\"].apply(len)\n    full[\"Has_Cabin\"] = full[\"Cabin\"].apply(lambda x: 0 if type(x) == float else 1)\n    full[\"FamilySize\"] = full[\"SibSp\"] + full[\"Parch\"] + 1\n    full[\"IsAlone\"] = full[\"FamilySize\"].apply(lambda x: 1 if x == 1 else 0)\n    full[\"CategoricalFare\"] = pd.qcut(full[\"Fare\"].fillna(full[\"Fare\"].median()), 4)\n    full[\"CategoricalFare\"] = pd.factorize(full[\"CategoricalFare\"])[0]\n    full[\"CategoricalAge\"] = pd.cut(full[\"Age\"].fillna(full[\"Age\"].median()), 5)\n    full[\"CategoricalAge\"] = pd.factorize(full[\"CategoricalAge\"])[0]\n    # Define function to extract titles from passenger names\n    def get_title(name):\n        title_search = re.search(r\" ([A-Za-z]+)\\.\", name)\n        # If the title exists, extract and return it.\n        if title_search:\n            return title_search.group(1)\n        return \"\"\n\n    # Create a new feature Title, containing the titles of passenger names\n    full[\"Title\"] = full[\"Name\"].apply(get_title)\n\n    # Group all non-common titles into one single grouping \"Rare\"\n    full[\"Title\"] = full[\"Title\"].replace(\n        [\n            \"Lady\",\n            \"Countess\",\n            \"Capt\",\n            \"Col\",\n            \"Don\",\n            \"Dr\",\n            \"Major\",\n            \"Rev\",\n            \"Sir\",\n            \"Jonkheer\",\n            \"Dona\",\n        ],\n        \"Rare\",\n    )\n\n    # Replace alias titles\n    full[\"Title\"] = full[\"Title\"].replace(\"Mlle\", \"Miss\")\n    full[\"Title\"] = full[\"Title\"].replace(\"Ms\", \"Miss\")\n    full[\"Title\"] = full[\"Title\"].replace(\"Mme\", \"Mrs\")\n\n    # parse ticket info\n    def parse_ticket(x):\n        s = re.search(r\"(.+ )?(\\d+)\", x)\n        if s is None:\n            return (np.nan, np.nan)\n        prefix = s.group(1).strip().upper() if s.group(1) is not None else np.nan\n        number = s.group(2).strip().upper() if s.group(2) is not None else np.nan\n        return (prefix, number)\n\n    # parse ticket info\n    ticket_prefix_number = full[\"Ticket\"].apply(lambda x: parse_ticket(x))\n    full[\"TicketPrefix\"] = ticket_prefix_number.apply(lambda x: x[0])\n    full[\"TicketNumber\"] = ticket_prefix_number.apply(lambda x: x[1]).astype(float)\n\n    # deal with rare ticket prefixes\n    # if the frequency of a prefix is less than 10, it's categorized as 'RARE'\n    rare_ticket_prefix = (\n        full[\"TicketPrefix\"]\n        .value_counts()[full[\"TicketPrefix\"].value_counts() < 10]\n        .index\n    )\n    full[\"TicketPrefix\"] = full[\"TicketPrefix\"].replace(rare_ticket_prefix, \"RARE\")\n\n    # to categorical\n    full[\"TicketPrefix\"] = pd.factorize(full[\"TicketPrefix\"])[0]\n\n    # parse cabin column\n    def parse_cabin_info(s):\n        if s is np.nan:\n            return np.nan\n        cabins = [c.strip() for c in s.strip().split(\" \")]\n        result = []\n        for c in cabins:\n            m = re.match(r\"([A-Z])(\\d+)?\", c)\n            result.append((m.group(1), m.group(2)))\n        return result\n\n    # parse cabin letter\n    def get_cabin_letter(cabins, index):\n        if cabins is np.nan:\n            return np.nan\n        if len(cabins) <= index:\n            return np.nan\n        return cabins[index][0]\n\n    # parse cabin number\n    def get_cabin_number(cabins, index):\n        if cabins is np.nan:\n            return np.nan\n        if len(cabins) <= index:\n            return np.nan\n        return cabins[index][1]\n\n    # feature engineering with Cabin\n    cabin_parse = full[\"Cabin\"].apply(parse_cabin_info)\n    cabin_letters = {c[0] for cc in cabin_parse.dropna() for c in cc}\n    cabin_letter_dict = {k: v for k, v in zip(cabin_letters, range(len(cabin_letters)))}\n\n    full[\"CabinCount\"] = cabin_parse.apply(\n        lambda x: len(x) if x is not np.nan else np.nan\n    )\n    full[\"CabinLetter1\"] = cabin_parse.apply(lambda x: get_cabin_letter(x, 0)).map(\n        cabin_letter_dict\n    )\n    full[\"CabinNumber1\"] = cabin_parse.apply(lambda x: get_cabin_number(x, 0)).astype(\n        float\n    )\n    full[\"CabinLetter2\"] = cabin_parse.apply(lambda x: get_cabin_letter(x, 1)).map(\n        cabin_letter_dict\n    )\n    full[\"CabinNumber2\"] = cabin_parse.apply(lambda x: get_cabin_number(x, 1)).astype(\n        float\n    )\n    full[\"CabinLetter3\"] = cabin_parse.apply(lambda x: get_cabin_letter(x, 2)).map(\n        cabin_letter_dict\n    )\n    full[\"CabinNumber3\"] = cabin_parse.apply(lambda x: get_cabin_number(x, 2)).astype(\n        float\n    )\n\n    # Mapping Sex\n    full[\"Sex\"] = full[\"Sex\"].map({\"female\": 0, \"male\": 1}).astype(int)\n\n    # Mapping titles\n    title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Rare\": 5}\n    full[\"Title\"] = full[\"Title\"].map(title_mapping)\n    full[\"Title\"] = full[\"Title\"].fillna(0)\n\n    # Mapping Embarked\n    full[\"Embarked\"] = full[\"Embarked\"].map({\"S\": 0, \"C\": 1, \"Q\": 2})\n\n    # back to train and test data\n    full = full.drop([\"Name\", \"Ticket\", \"Cabin\"], axis=1)\n    train = full[full[\"IsTrain\"] == 1].drop([\"IsTrain\"], axis=1)\n    test = full[full[\"IsTrain\"] == 0].drop([\"IsTrain\", \"Survived\"], axis=1)\n    return train, test","c2642b56":"def preprocess_linear(train, test):\n    # concatinate\n    train[\"IsTrain\"] = 1\n    test[\"IsTrain\"] = 0\n    full = pd.concat([train, test])\n\n    # separate features\n    numerical_features = [\n        \"Age\",\n        \"Fare\",\n        \"Parch\",\n        \"SibSp\",\n        \"Name_length\",\n        \"FamilySize\",\n        \"TicketNumber\",\n        \"CabinCount\",\n        \"CabinNumber1\",\n        \"CabinNumber2\",\n        \"CabinNumber3\",\n    ]\n    other_columns = [\"IsTrain\", \"Survived\"]\n    # columns not in numerical features\n    categorical_features = [\n        col for col in full.columns if col not in numerical_features + other_columns\n    ]\n\n    full[\"TicketNumberTooBig\"] = 0\n    full[\"TicketNumberTooBig\"][full[\"TicketNumber\"] > 2500000] = 1\n\n    for f in numerical_features:\n        # clip outliers\n        # !!! clips the entire data\n        # too large frequency in one value\n        #         no_nan = full[f].dropna()\n        #         q1 = np.percentile(no_nan, 25)\n        #         q3 = np.percentile(no_nan, 75)\n        #         iqr = q3 - q1\n        #         clip_min = q1 - 1.5 * iqr\n        #         clip_max = q3 + 1.5 * iqr\n        #         print(f, clip_min, clip_max) # debug\n        #         full[f][full[f].notnull()] = np.clip(no_nan, clip_min, clip_max)\n\n        # impute numerical features by their medians\n        if full[f].isnull().sum() != 0:\n            full[f + \"IsNan\"] = 0\n            full[f + \"IsNan\"][full[f].isnull()] = 1\n        full[f] = full[f].fillna(full[f].median())\n        # scaling\n        full[f] = MinMaxScaler().fit_transform(full[f].values.reshape(-1, 1))\n\n    # one hot encoding for categorical features\n    ohe_list = [full]\n    for f in categorical_features:\n        # treat nan as a category\n        full[f] = full[f].fillna(-1)\n        dummy = pd.get_dummies(full[f])\n        dummy.columns = [f + \"_\" + str(col) for col in dummy.columns]\n        ohe_list.append(dummy)\n    full = pd.concat(ohe_list, axis=1)\n    full = full.drop(categorical_features, axis=1)\n\n    train = full[full[\"IsTrain\"] == 1].drop([\"IsTrain\"], axis=1)\n    test = full[full[\"IsTrain\"] == 0].drop([\"IsTrain\", \"Survived\"], axis=1)\n    return train, test","423ffdbc":"trainp, testp = preprocess(train.copy(), test.copy())","7e604ca8":"X = trainp.drop([\"Survived\"], axis=1)\ny = trainp[\"Survived\"]","736c1ebe":"trainpl, testpl = preprocess_linear(trainp.copy(), testp.copy())\nX_lin = trainpl.drop([\"Survived\"], axis=1)","2434a14f":"class Model:\n    def __init__(self, method):\n        self.method = method\n        self.model = None\n        self.categorical_features_indices = None\n\n    def train(\n        self,\n        params,\n        X,\n        y,\n        X_test,\n        y_test=None,\n        categorical_features_indices=None,\n        nn_params=None,\n    ):\n        self.categorical_features_indices = categorical_features_indices\n\n        # LightGBM\n        if self.method == \"lgb\":\n            train_data = lgb.Dataset(X, label=y)\n            eval_data = lgb.Dataset(X_test, label=y_test, reference=train_data)\n\n            model = lgb.train(\n                params,\n                train_data,\n                valid_sets=[train_data, eval_data],\n                valid_names=[\"train\", \"eval\"],\n                early_stopping_rounds=10,\n            )\n            self.model = model\n\n        # XGBoost\n        elif self.method == \"xgb\":\n            dtrain = xgb.DMatrix(X, label=y)\n            model = xgb.train(params, dtrain)\n            self.model = model\n\n        # CatBoost\n        elif self.method == \"cat\":\n            train_pool = Pool(X, y, cat_features=categorical_features_indices)\n            test_pool = Pool(X_test, y_test, cat_features=categorical_features_indices)\n            model = CatBoostClassifier(**params)\n            model.fit(train_pool)\n            self.model = model\n\n        # Random Forest\n        elif self.method == \"rf\":\n            model = RandomForestClassifier(**params)\n            model.fit(X, y)\n            self.model = model\n\n        # svm\n        elif self.method == \"svm\":\n            model = SVC(**params)\n            model.fit(X, y)\n            self.model = model\n\n        # logistic regression\n        elif self.method == \"log\":\n            model = LogisticRegression(**params)\n            model.fit(X, y)\n            self.model = model\n\n        # neural network\n        elif self.method == \"nn\":\n            model = nn_params.model\n            torch_train = torch.utils.data.TensorDataset(\n                torch.tensor(X, dtype=torch.float32),\n                torch.tensor(y, dtype=torch.float32),\n            )\n            train_loader = torch.utils.data.DataLoader(\n                torch_train, batch_size=nn_params.batch_size, shuffle=True\n            )\n\n            # \u30e2\u30c7\u30eb\u306e\u30c8\u30ec\u30fc\u30cb\u30f3\u30b0\n            for epoch in range(nn_params.num_epochs):\n                model.train()\n                for i, (X_batch, y_batch) in enumerate(train_loader):\n                    y_pred = model(X_batch).view(-1)\n                    loss = nn_params.criterion(y_pred, y_batch)\n                    nn_params.optimizer.zero_grad()\n                    loss.backward()\n                    nn_params.optimizer.step()\n\n                    print(\n                        \"Epoch [%d\/%d], Step [%d\/%d], Loss: %.4f\"\n                        % (\n                            epoch + 1,\n                            nn_params.num_epochs,\n                            i + 1,\n                            len(X_batch) \/\/ nn_params.batch_size,\n                            loss.data.item(),\n                        )\n                    )\n\n        else:\n            raise Exception(\"method not found\")\n\n    def predict(self, X_test, predict_proba=False):\n        # LightGBM\n        if self.method == \"lgb\":\n            pred = self.model.predict(X_test)\n            if predict_proba:\n                return pred\n            else:\n                return (pred > 0.5).astype(int)\n\n        # XGBoost\n        elif self.method == \"xgb\":\n            dtest = xgb.DMatrix(X_test)\n            pred = self.model.predict(dtest)\n            if predict_proba:\n                return pred\n            else:\n                return (pred > 0.5).astype(int)\n\n        # CatBoost\n        elif self.method == \"cat\":\n            test_pool = Pool(X_test, cat_features=self.categorical_features_indices)\n            pred = self.model.predict(test_pool)\n            if predict_proba:\n                return self.model.predict_proba(test_pool)[:, 1]\n            else:\n                return self.model.predict(test_pool)\n\n        # Random Forest\n        elif self.method == \"rf\":\n            if predict_proba:\n                return self.model.predict_proba(X_test)[:, 1]\n            else:\n                return self.model.predict(X_test)\n\n        # svm\n        elif self.method == \"svm\":\n            if predict_proba:\n                self.model.probability = True\n                return self.model.predict_proba(X_test)[:, 1]\n            else:\n                return self.model.predict(X_test)\n\n        # logistic regression\n        elif self.method == \"log\":\n            if predict_proba:\n                return self.model.predict_proba(X_test)[:, 1]\n            else:\n                return self.model.predict(X_test)\n\n        else:\n            raise Exception(\"method not found\")","8c824584":"def asbinary(fp):\n    return (fp > 0.5).astype(int)\n\n\ndef cross_validate(\n    method,\n    params,\n    X=X,\n    X_test=None,\n    categorical_features_indices=None,\n    predict_proba=False,\n    model=None,\n):\n    n_splits = 5\n    scores = np.zeros(n_splits)\n    oof = np.zeros(len(y))\n\n    if X_test is not None:\n        pred_tests = np.zeros((len(X_test), n_splits))\n\n    skf = StratifiedKFold(n_splits=n_splits)\n    for i, (train_index, valid_index) in enumerate(skf.split(X, y)):\n        print(\"#\" * 40)\n        print(\"fold %d\" % (i + 1))\n\n        X_train, y_train = X.loc[train_index], y.loc[train_index]\n        X_valid, y_valid = X.loc[valid_index], y.loc[valid_index]\n\n        model = model or Model(method)\n        model.train(\n            params,\n            X_train,\n            y_train,\n            X_test=X_valid,\n            y_test=y_valid,\n            categorical_features_indices=categorical_features_indices,\n        )\n        pred_valid = model.predict(X_valid, predict_proba=predict_proba)\n        scores[i] = accuracy_score(y_valid, asbinary(pred_valid))\n        oof[valid_index] = pred_valid\n\n        if X_test is not None:\n            pred_test = model.predict(X_test, predict_proba=predict_proba)\n            pred_tests[:, i] = pred_test\n\n    print(\"#\" * 40)\n    print(\"The scores mean : %f, std : %f\" % (scores.mean(), scores.std()))\n    print(\"Oof score : %f\" % accuracy_score(y, asbinary(oof)))\n\n    if X_test is None:\n        return scores, oof\n    else:\n        return scores, oof, pred_tests.mean(axis=1)","3a362119":"%%capture\n%%time\n\n\n# light gbm hyper parameter tuning\n\nlgb_trials = 100\n\n\ndef optuna_lightgbm(trial):\n    param = {\n        \"objective\": \"binary\",\n        \"metric\": \"binary_logloss\",\n        \"random_state\": RANDOM_STATE,\n        \"verbosity\": -1,\n        \"boosting_type\": trial.suggest_categorical(\n            \"boosting\", [\"gbdt\", \"dart\", \"goss\"]\n        ),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 4, 10),\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-8, 1.0)\n    }\n\n    if param[\"boosting_type\"] == \"dart\":\n        param[\"drop_rate\"] = trial.suggest_loguniform(\"drop_rate\", 1e-8, 1.0)\n        param[\"skip_drop\"] = trial.suggest_loguniform(\"skip_drop\", 1e-8, 1.0)\n    if param[\"boosting_type\"] == \"goss\":\n        param[\"top_rate\"] = trial.suggest_uniform(\"top_rate\", 0.0, 1.0)\n        param[\"other_rate\"] = trial.suggest_uniform(\n            \"other_rate\", 0.0, 1.0 - param[\"top_rate\"]\n        )\n\n    min_data_in_leaf_min = 3\n    # num_leaves * min_data_in_leaf <= nrows\n    num_leaves_max = min(2 ** param[\"max_depth\"], int(len(X) \/ min_data_in_leaf_min))\n    param[\"num_leaves\"] = trial.suggest_int(\"num_leaves\", 10, num_leaves_max)\n    # num_leaves * min_data_in_leaf <= nrows\n    min_data_in_leaf_max = int(len(X) \/ param[\"num_leaves\"])\n    param[\"min_data_in_leaf\"] = trial.suggest_int(\n        \"min_data_in_leaf\", min_data_in_leaf_min, min_data_in_leaf_max\n    )\n\n    _, oof = cross_validate(\"lgb\", param)\n\n    return accuracy_score(y, oof)\n\n\nstudy_lgb = optuna.create_study(direction=\"maximize\")\nstudy_lgb.optimize(optuna_lightgbm, n_trials=lgb_trials)\nprint(\"Number of finished trials: {}\".format(len(study_lgb.trials)))\nprint(\"Best trial:\")\ntrial = study_lgb.best_trial\nprint(\"  Value: {}\".format(trial.value))\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n\n# for local execution\n# with open('study_lgb.pkl', 'wb') as f:\n#     pickle.dump(study_lgb, f)","2a0b9938":"# with open('study_lgb.pkl', 'rb') as f:\n#     study_lgb = pickle.load(f)\n\nparams_lgb = {\n    \"objective\": \"binary\",\n    \"metric\": \"binary_logloss\",\n    \"random_state\": RANDOM_STATE,\n}\nparams_lgb.update(study_lgb.best_trial.params)\n\n# final score\n# _, _, _ = cross_validate(\"lgb\", params_lgb, X_test=testp)","2e4d2e4a":"%%capture\n%%time\n# xgboost hyper parameter tuning\n\nxgb_trial = 1000\n\n\ndef optuna_xgb(trial):\n    param = {\n        \"objective\": \"binary:logistic\",\n        \"random_state\": RANDOM_STATE,\n        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 30),\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 1e-9, 1),\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 1, 1000),\n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 20),\n        \"scale_pos_weight\": trial.suggest_int(\"scale_pos_weight\", 1, 100),\n        \"subsample\": trial.suggest_uniform(\"subsample\", 0, 1),\n        \"colsample_bytree\": trial.suggest_uniform(\"colsample_bytree\", 0, 1),\n    }\n\n    _, oof = cross_validate(\"xgb\", param)\n\n    return accuracy_score(y, oof)\n\n\nstudy_xgb = optuna.create_study(direction=\"maximize\")\nstudy_xgb.optimize(optuna_xgb, n_trials=xgb_trial)\n\nprint(\"Number of finished trials: {}\".format(len(study_xgb.trials)))\nprint(\"Best trial:\")\ntrial = study_xgb.best_trial\nprint(\"  Value: {}\".format(trial.value))\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n\n# with open('study_xgb.pkl', 'wb') as f:\n#     pickle.dump(study_xgb, f)","3ea749de":"# with open('study_xgb.pkl', 'rb') as f:\n#     study_xgb = pickle.load(f)\n\nparams_xgb = {\"objective\": \"binary:logistic\", \"random_state\": RANDOM_STATE}\n\nparams_xgb.update(study_xgb.best_trial.params)\n\n# _, _, _ = cross_validate(\"xgb\", params_xgb, X_test=testp)","723c42b1":"%%capture\n%%time\n# catboost hyper parameter tuning\n\ncat_trial = 100\n\ncategorical_features = [\n    \"Embarked\",\n    \"Pclass\",\n    \"Sex\",\n    \"Has_Cabin\",\n    \"IsAlone\",\n    \"CategoricalFare\",\n    \"CategoricalAge\",\n    \"Title\",\n    \"TicketPrefix\",\n    \"CabinLetter1\",\n    \"CabinLetter2\",\n    \"CabinLetter3\",\n]\ncategorical_features_indices = np.where(\n    [col in categorical_features for col in X.columns]\n)[0]\ncategory_dtype_dict = {feat: \"int\" for feat in categorical_features}\nX_cat = X.fillna(-1).astype(category_dtype_dict)\n\n\ndef optuna_cat(trial):\n    param = {\n        \"random_state\": RANDOM_STATE,\n        \"iterations\": trial.suggest_int(\"iterations\", 50, 300),\n        \"depth\": trial.suggest_int(\"depth\", 4, 10),\n        \"learning_rate\": trial.suggest_loguniform(\"learning_rate\", 0.001, 0.5),\n        \"random_strength\": trial.suggest_int(\"random_strength\", 0, 100),\n        \"bagging_temperature\": trial.suggest_loguniform(\n            \"bagging_temperature\", 0.01, 100.00\n        ),\n        \"od_type\": trial.suggest_categorical(\"od_type\", [\"IncToDec\", \"Iter\"]),\n        \"od_wait\": trial.suggest_int(\"od_wait\", 10, 50),\n    }\n\n    _, oof = cross_validate(\n        \"cat\", param, X=X_cat, categorical_features_indices=categorical_features_indices\n    )\n\n    return accuracy_score(y, oof)\n\n\nstudy_cat = optuna.create_study(direction=\"maximize\")\nstudy_cat.optimize(optuna_cat, n_trials=cat_trial)\nprint(\"Number of finished trials: {}\".format(len(study_cat.trials)))\nprint(\"Best trial:\")\ntrial = study_cat.best_trial\nprint(\"  Value: {}\".format(trial.value))\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n\n# with open('study_cat.pkl', 'wb') as f:\n#     pickle.dump(study_cat, f)","977053c4":"# with open('study_cat.pkl', 'rb') as f:\n#     study_cat = pickle.load(f)\n\ncategorical_features = [\n    \"Embarked\",\n    \"Pclass\",\n    \"Sex\",\n    \"Has_Cabin\",\n    \"IsAlone\",\n    \"CategoricalFare\",\n    \"CategoricalAge\",\n    \"Title\",\n    \"TicketPrefix\",\n    \"CabinLetter1\",\n    \"CabinLetter2\",\n    \"CabinLetter3\",\n]\ncategorical_features_indices = np.where(\n    [col in categorical_features for col in X.columns]\n)[0]\n\nX_cat_test = testp.fillna(-1).astype(category_dtype_dict)\n\nparams_cat = {\"random_state\": RANDOM_STATE}\nparams_cat.update(study_cat.best_trial.params)\n\n# _, _, _ = cross_validate(\n#     \"cat\",\n#     params_cat,\n#     X=X_cat,\n#     X_test=X_cat_test,\n#     categorical_features_indices=categorical_features_indices,\n# )","3804aef3":"%%capture\n%%time\n# random forest hyper parameter tuning\n# score of tuned params was the same as default one\n\nrf_trial = 60\n\n\ndef optuna_rf(trial):\n    param = {\n        \"n_estimators\": trial.suggest_int(\"n_estimators\", 200, 2000),\n        \"max_features\": trial.suggest_categorical(\"max_features\", [\"auto\", \"sqrt\"]),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 10, 100),\n        \"min_samples_split\": trial.suggest_int(\"min_samples_split\", 2, 10),\n        \"min_samples_leaf\": trial.suggest_int(\"min_samples_leaf\", 1, 5),\n        \"bootstrap\": trial.suggest_categorical(\"bootstrap\", [True, False]),\n    }\n\n    _, oof = cross_validate(\"rf\", param, X=X_rf)\n\n    return 1 - accuracy_score(y, oof)\n\n\nX_rf = X.fillna(-1)\n\nstudy_rf = optuna.create_study()\nstudy_rf.optimize(optuna_rf, n_trials=rf_trial)\nprint(\"Number of finished trials: {}\".format(len(study_rf.trials)))\nprint(\"Best trial:\")\ntrial = study_rf.best_trial\nprint(\"  Value: {}\".format(trial.value))\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n\n# with open('study_rf.pkl', 'wb') as f:\n#     pickle.dump(study_rf, f)","bcf2db6c":"# with open('study_rf.pkl', 'rb') as f:\n#     study_rf = pickle.load(f)\n\nparams_rf = {\"random_state\": RANDOM_STATE}\nparams_rf.update(study_rf.best_trial.params)\n\n# _, _, _ = cross_validate(\"rf\", params_rf, X=X.fillna(-1), X_test=testp.fillna(-1))","8a1d4321":"%%capture\n%%time\n# svm hyper parameter tuning\n\nsvm_trial = 150\n\n\ndef optuna_svm(trial):\n    param = {\n        \"random_state\": RANDOM_STATE,\n        \"C\": trial.suggest_loguniform(\"C\", 2 ** -10, 2 ** 15),\n        \"gamma\": trial.suggest_loguniform(\"gamma\", 2 ** -20, 2 ** 10),\n    }\n\n    _, oof = cross_validate(\"svm\", param, X=X_lin)\n\n    return accuracy_score(y, oof)\n\n\nstudy_svm = optuna.create_study(direction=\"maximize\")\nstudy_svm.optimize(optuna_svm, n_trials=svm_trial)  # 1000\nprint(\"Number of finished trials: {}\".format(len(study_svm.trials)))\nprint(\"Best trial:\")\ntrial = study_svm.best_trial\nprint(\"  Value: {}\".format(trial.value))\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n\n# with open('study_svm.pkl', 'wb') as f:\n#     pickle.dump(study_svm, f)","32a4a7a7":"# with open('study_svm.pkl', 'rb') as f:\n#     study_svm = pickle.load(f)\n\nparams_svm = {\"random_state\": RANDOM_STATE, \"probability\": True}\nparams_svm.update(study_svm.best_trial.params)\n\n# _, _, _ = cross_validate(\"svm\", params_svm, X=X_lin, X_test=testpl)","5d937c41":"%%capture\n%%time\n# logistic regression hyper parameter tuning\n\nlog_trial = 3000\n\ndef optuna_log(trial):\n    param = {\n        \"random_state\": RANDOM_STATE,\n        \"C\": trial.suggest_loguniform(\"C\", 2 ** -10, 2 ** 15),\n    }\n\n    _, oof = cross_validate(\"log\", param, X=X_lin)\n\n    return accuracy_score(y, oof)\n\n\nstudy_log = optuna.create_study(direction=\"maximize\")\nstudy_log.optimize(optuna_log, n_trials=log_trial)\nprint(\"Number of finished trials: {}\".format(len(study_log.trials)))\nprint(\"Best trial:\")\ntrial = study_log.best_trial\nprint(\"  Value: {}\".format(trial.value))\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n\n# with open('study_log.pkl', 'wb') as f:\n#     pickle.dump(study_log, f)","e357e7a6":"# with open('study_log.pkl', 'rb') as f:\n#     study_log = pickle.load(f)\n\nparams_log = {\"random_state\": RANDOM_STATE}\nparams_log.update(study_log.best_trial.params)\n\n# _, _, _ = cross_validate(\"log\", params_log, X=X_lin, X_test=testpl)","02d85797":"DEVICE = torch.device(\"cpu\")\nBATCHSIZE = 128\nCLASSES = 2\nEPOCHS = 10\nLOG_INTERVAL = 10\nN_TRAIN_EXAMPLES = BATCHSIZE * 30\nN_TEST_EXAMPLES = BATCHSIZE * 10","3e7263de":"class NetOptuna(nn.Module):\n    # Constructor for trial network.\n    def __init__(self, num_inputs, trial):\n        super(NetOptuna, self).__init__()\n        self.layers = []\n        self.dropouts = []\n\n        CLASSES = 2\n\n        # We optimize the number of layers, hidden untis in each layer and drouputs.\n        n_layers = trial.suggest_int(\"n_layers\", 1, 3)\n        dropout = trial.suggest_uniform(\"dropout\", 0.2, 0.5)\n        input_dim = num_inputs\n        for i in range(n_layers):\n            output_dim = int(trial.suggest_loguniform(\"n_units_l{}\".format(i), 4, 128))\n            self.layers.append(nn.Linear(input_dim, output_dim))\n            self.dropouts.append(nn.Dropout(dropout))\n            input_dim = output_dim\n\n        self.layers.append(nn.Linear(input_dim, CLASSES))\n\n        # Assigning the layers as class variables (PyTorch requirement).\n        for idx, layer in enumerate(self.layers):\n            setattr(self, \"fc{}\".format(idx), layer)\n\n        # Assigning the dropouts as class variables (PyTorch requirement).\n        for idx, dropout in enumerate(self.dropouts):\n            setattr(self, \"drop{}\".format(idx), dropout)\n\n    # Forward pass computation function.\n    def forward(self, data):\n        for layer, dropout in zip(self.layers, self.dropouts):\n            data = F.relu(layer(data))\n            data = dropout(data)\n        return F.log_softmax(self.layers[-1](data), dim=1)\n\n\nclass Net(nn.Module):\n    # Constructor for trial network.\n    def __init__(self, num_inputs, params):\n        super(Net, self).__init__()\n        self.layers = []\n        self.dropouts = []\n\n        CLASSES = 2\n\n        n_layers = params[\"n_layers\"]\n        dropout = params[\"dropout\"]\n        input_dim = num_inputs\n        for i in range(n_layers):\n            output_dim = int(params[\"n_units_l{}\".format(i)])\n            self.layers.append(nn.Linear(input_dim, output_dim))\n            self.dropouts.append(nn.Dropout(dropout))\n            input_dim = output_dim\n\n        self.layers.append(nn.Linear(input_dim, CLASSES))\n\n        # Assigning the layers as class variables (PyTorch requirement).\n        for idx, layer in enumerate(self.layers):\n            setattr(self, \"fc{}\".format(idx), layer)\n\n        # Assigning the dropouts as class variables (PyTorch requirement).\n        for idx, dropout in enumerate(self.dropouts):\n            setattr(self, \"drop{}\".format(idx), dropout)\n\n    # Forward pass computation function.\n    def forward(self, data):\n        for layer, dropout in zip(self.layers, self.dropouts):\n            data = F.relu(layer(data))\n            data = dropout(data)\n        return F.log_softmax(self.layers[-1](data), dim=1)","f9ddf64e":"def train_nn(model, optimizer, X, y):\n    # Get the dataset.\n    torch_train = torch.utils.data.TensorDataset(\n        torch.tensor(X, dtype=torch.float32), torch.tensor(y, dtype=torch.long)\n    )\n    train_loader = torch.utils.data.DataLoader(\n        torch_train, batch_size=BATCHSIZE, shuffle=True\n    )\n\n    # Training of the model.\n    model.train()\n    for epoch in range(EPOCHS):\n        for batch_idx, (data, target) in enumerate(train_loader):\n            # Limiting training data for faster epochs.\n            if batch_idx * BATCHSIZE >= BATCHSIZE * 30:\n                break\n\n            data, target = data.to(DEVICE), target.to(DEVICE)\n\n            # Zeroing out gradient buffers.\n            optimizer.zero_grad()\n            # Performing a forward pass.\n            output = model(data)\n            # Computing negative Log Likelihood loss.\n            loss = F.nll_loss(output, target)\n            # Performing a backward pass.\n            loss.backward()\n            # Updating the weights.\n            optimizer.step()\n\n            if (batch_idx + 1) % 2 == 0:\n                print(\n                    \"Epoch [%d\/%d], Step [%d\/%d], Loss: %.4f\"\n                    % (\n                        epoch + 1,\n                        EPOCHS,\n                        batch_idx + 1,\n                        len(data) \/\/ BATCHSIZE,\n                        loss.data.item(),\n                    )\n                )\n\n    return model\n\n\ndef test_nn(model, X_test):\n    torch_test = torch.utils.data.TensorDataset(\n        torch.tensor(X_test, dtype=torch.float32)\n    )\n    test_loader = torch.utils.data.DataLoader(\n        torch_test, batch_size=BATCHSIZE, shuffle=False\n    )\n\n    # Validation of the model.\n    model.eval()\n    correct = 0\n    y_preds = np.zeros(len(X_test))\n    with torch.no_grad():\n        for batch_idx, data in enumerate(test_loader):\n            # Limiting testing data.\n            if batch_idx * BATCHSIZE >= BATCHSIZE * 10:\n                break\n\n            data = data[0].to(DEVICE)\n            output = model(data)\n            pred = output.argmax(\n                dim=1, keepdim=False\n            )  # Get the index of the max log-probability.\n            y_preds[batch_idx * BATCHSIZE : (batch_idx + 1) * BATCHSIZE] = pred\n\n    return y_preds","4a810b43":"def cross_validate_nn(model, optimizer, X, y, X_test):\n    n_splits = 5\n    scores = np.zeros(n_splits)\n    oof = np.zeros(len(y))\n    pred_tests = np.zeros((len(X_test), n_splits))\n    skf = StratifiedKFold(n_splits=n_splits)\n    for i, (train_index, valid_index) in enumerate(skf.split(X, y)):\n        print(\"#\" * 40)\n        print(\"fold %d\" % (i + 1))\n\n        X_train, y_train = X.loc[train_index].values, y.loc[train_index].values\n        X_valid, y_valid = X.loc[valid_index].values, y.loc[valid_index].values\n\n        model = train_nn(model, optimizer, X_train, y_train)\n        pred_valid = test_nn(model, X_valid)\n        pred_test = test_nn(model, X_test.values)\n\n        scores[i] = accuracy_score(y_valid, asbinary(pred_valid))\n        oof[valid_index] = pred_valid\n        pred_tests[:, i] = pred_test\n\n    print(\"#\" * 40)\n    print(\"The scores mean : %f, std : %f\" % (scores.mean(), scores.std()))\n    print(\"Oof score : %f\" % accuracy_score(y, asbinary(oof)))\n    return scores, oof, pred_tests.mean(axis=1)","2d110f35":"def optuna_nn(trial):\n\n    # Generate the model.\n    model = NetOptuna(len(X_lin.columns), trial).to(DEVICE)\n\n    # Generate the optimizers.\n    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"RMSprop\", \"SGD\"])\n    lr = trial.suggest_uniform(\"lr\", 1e-5, 1e-1)\n    optimizer = getattr(torch.optim, optimizer_name)(model.parameters(), lr=lr)\n\n    _, oof, _ = cross_validate_nn(model, optimizer, X_lin, y, testpl)\n\n    return accuracy_score(y, oof)","550b03d4":"%%time\n%%capture\n\nnn_trial = 400\n\nstudy_nn = optuna.create_study(direction=\"maximize\")\nstudy_nn.optimize(optuna_nn, n_trials=nn_trial)\n\nprint(\"Number of finished trials: \", len(study_nn.trials))\n\nprint(\"Best trial:\")\ntrial = study_nn.best_trial\n\nprint(\"  Accuracy: \", trial.value)\n\nprint(\"  Params: \")\nfor key, value in trial.params.items():\n    print(\"    {}: {}\".format(key, value))\n\n# with open('study_nn.pkl', 'wb') as f:\n#     pickle.dump(study_nn, f)","078ad874":"# with open('study_nn.pkl', 'rb') as f:\n#     study_nn = pickle.load(f)\n\nseed_everything(seed=RANDOM_STATE)\n\nparams_nn = study_nn.best_trial.params\nmodel_nn = Net(len(X_lin.columns), params_nn).to(DEVICE)\n\n# torch.save(model_nn.state_dict(), 'model_nn.pt')\n\noptimizer_name = params_nn[\"optimizer\"]\nlr = params_nn[\"lr\"]\noptimizer = getattr(torch.optim, optimizer_name)(model_nn.parameters(), lr=lr)\n\n# _, oof_nn, pred_nn = cross_validate_nn(model_nn, optimizer, X_lin, y, testpl)","ffd01825":"%%capture\n# predict\n_, oof_lgb, pred_lgb = cross_validate(\n    \"lgb\", params_lgb, X_test=testp, predict_proba=True\n)\n_, oof_xgb, pred_xgb = cross_validate(\n    \"xgb\", params_xgb, X_test=testp, predict_proba=True\n)\n_, oof_cat, pred_cat = cross_validate(\n    \"cat\",\n    params_cat,\n    X=X_cat,\n    X_test=X_cat_test,\n    categorical_features_indices=categorical_features_indices,\n    predict_proba=True,\n)\n_, oof_rf, pred_rf = cross_validate(\n    \"rf\", params_rf, X=X.fillna(-1), X_test=testp.fillna(-1), predict_proba=True\n)\n_, oof_svm, pred_svm = cross_validate(\n    \"svm\", params_svm, X=X_lin, X_test=testpl, predict_proba=True\n)\n_, oof_log, pred_log = cross_validate(\n    \"log\", params_log, X=X_lin, X_test=testpl, predict_proba=True\n)\n_, oof_nn, pred_nn = cross_validate_nn(model_nn, optimizer, X_lin, y, testpl)","f8bbb909":"cols = [\"lgb\", \"xgb\", \"cat\", \"rf\", \"svm\", \"log\", \"nn\"]\ndf_oof = pd.DataFrame(\n    dict(zip(cols, [oof_lgb, oof_xgb, oof_cat, oof_rf, oof_svm, oof_log, oof_nn]))\n)\ndf_pred = pd.DataFrame(\n    dict(\n        zip(cols, [pred_lgb, pred_xgb, pred_cat, pred_rf, pred_svm, pred_log, pred_nn])\n    )\n)","44fdea58":"%%capture\n# stacking\n_, oof_lgb2, pred_lgb2 = cross_validate(\n    \"lgb\",\n    {\"objective\": \"binary\", \"metric\": \"binary_logloss\", \"random_state\": RANDOM_STATE},\n    X=df_oof,\n    X_test=df_pred,\n    predict_proba=True\n)\n_, oof_xgb2, pred_xgb2 = cross_validate(\n    \"xgb\",\n    {\"objective\": \"binary:logistic\", \"random_state\": RANDOM_STATE},\n    X=df_oof,\n    X_test=df_pred,\n    predict_proba=True\n)\n_, oof_cat2, pred_cat2 = cross_validate(\n    \"cat\", {\"random_state\": RANDOM_STATE}, X=df_oof, X_test=df_pred, predict_proba=True\n)\n_, oof_rf2, pred_rf2 = cross_validate(\n    \"rf\", {\"random_state\": RANDOM_STATE}, X=df_oof, X_test=df_pred, predict_proba=True\n)\n_, oof_svm2, pred_svm2 = cross_validate(\n    \"svm\", {\"random_state\": RANDOM_STATE, \"probability\": True}, X=df_oof, X_test=df_pred, predict_proba=True\n)\n_, oof_log2, pred_log2 = cross_validate(\n    \"log\", {\"random_state\": RANDOM_STATE}, X=df_oof, X_test=df_pred, predict_proba=True\n)","cb5f36f8":"# output results\nfinal_preds = np.array([pred_lgb2, pred_xgb2, pred_cat2, pred_rf2, pred_svm2, pred_log2]).mean(axis=0)\nsubmission = pd.DataFrame({ 'PassengerId': test.PassengerId,\n                            'Survived': (final_preds > 0.5) * 1 })\nsubmission.to_csv(\"submit.csv\", index=False)","77dbac4e":"## CatBoost","cfe4f524":"# Stacking + Ensembling","d44aef7b":"# Helper functions","0d147725":"# Various Models + Hyper Parameter Optimization\n## Light GBM","c703903c":"# Random forest","d08441ca":"# Logistic Regression","d3f2568b":"## Neural network","522d2d3c":"# SVM","e44c8fc3":"# Introduction\n\n**Bayesian optimization with Optuna + Stacking**\n\nBasic strategy:  \n1. Data cleaning + Feature engineering\n2. Various models (e.g. neural network, lightGBM) + hyper parameter optimization with Optuna\n3. Stacking + ensembling","c350543e":"# Data Cleaning + Feature Engineering","a8d9e31b":"## XGBoost","403b7ccb":"# Load Data"}}