{"cell_type":{"9e6d98ab":"code","06576adb":"code","6e75638a":"code","79caadc0":"code","7423e444":"code","053d9364":"code","b8273d90":"code","af755777":"code","6741fb7a":"code","dfdab59c":"code","aabe9cfd":"code","6ce1d5de":"code","90fb282e":"code","ae2a14c3":"code","906a645d":"code","005cd991":"code","41285574":"code","6929da6c":"code","9f9e57ac":"code","a3963af5":"code","f066dfda":"code","a317f192":"code","a66f9106":"code","22a2cb2f":"code","246b0459":"code","f9e3a395":"code","140b3c83":"code","201008bb":"code","67e5e7cd":"code","787136be":"code","cb401651":"code","7e81a053":"code","017fd0d9":"code","d574de0c":"code","87fbcff1":"code","940421e1":"code","1e03068e":"code","d65ab1d8":"code","a3e91ae0":"code","ad0ea9cf":"code","4e041fee":"code","d34443be":"code","162c5307":"code","5b13a13e":"code","ac030ccf":"code","874dd44a":"code","be4f2f66":"code","d3f4dedb":"code","c5bbc9d5":"code","a8c58a53":"code","ac65d642":"code","d349a2e2":"code","729a489e":"code","a283d65f":"code","77bf9e47":"code","cada9cc2":"code","8ace0c53":"code","c6b54071":"code","d26e022f":"code","28f52e64":"code","6d7e3a60":"code","ff490cd6":"code","15115079":"code","1da7b94d":"code","07599e65":"code","05b01cad":"code","8ec4c965":"code","2da5cd00":"code","6ea393a9":"code","da146010":"code","7ea9802f":"code","8deb16ed":"code","6001e48f":"code","3c760cd2":"code","3b8a0273":"code","655bcb6d":"code","8b230ace":"code","0dbc12ce":"code","ce73713f":"code","4a846546":"code","53ba6f91":"code","a4c0e775":"code","2f95f7e7":"code","bb287787":"code","f0acd222":"code","da15bda0":"code","75056c38":"code","3359e063":"code","3e762496":"code","a1e3268d":"code","7e58a322":"code","34624027":"code","2f197326":"code","8c21f167":"code","dc0fa982":"code","ab13ea96":"code","0790180f":"code","60434d5f":"code","ab1ea901":"code","f53e9df5":"code","896022b0":"code","3dcacfb5":"code","ef55244a":"code","f4b49740":"code","b244f7c8":"code","c0fd1e25":"code","094db7a8":"code","51739a44":"code","411b2c57":"code","0481a50b":"code","8e18ee20":"code","46fc5ca2":"code","4417743e":"code","e1f6f26e":"markdown","391b4ee8":"markdown","52eab04d":"markdown","ec43101e":"markdown","fdc1ea1c":"markdown","013ce718":"markdown","840bcbf0":"markdown","1be1af2f":"markdown","6ad99f6e":"markdown","640c14eb":"markdown","a2c21af9":"markdown","d91cce70":"markdown","3ae67813":"markdown","460b53c2":"markdown","aa481c69":"markdown","8e3b97fa":"markdown","0e2d9dd3":"markdown","b7b08c06":"markdown","fd80def7":"markdown","15239d49":"markdown","b0275c8c":"markdown","5287eb61":"markdown","f0939ccc":"markdown","b33a6202":"markdown","1b415628":"markdown","2d7e6560":"markdown","5a35a653":"markdown","a9e3f463":"markdown","d25da0be":"markdown","cdc488de":"markdown","b6e92534":"markdown","74fb4777":"markdown","1bf8e679":"markdown","a7e76a1c":"markdown","ce87b228":"markdown","ea4e71a4":"markdown","8b666890":"markdown","3ea4ee8b":"markdown","2d4166e4":"markdown","8e040ca5":"markdown","46baaf64":"markdown","0596da4a":"markdown","cc1bb090":"markdown","5d843a3f":"markdown","ac68acfe":"markdown","6220065b":"markdown","0bbb4833":"markdown","eb9a70d8":"markdown","61e7a807":"markdown","5fb23047":"markdown","63b96d23":"markdown","79b626d8":"markdown"},"source":{"9e6d98ab":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport geopy.distance as geo\n\nimport datetime\nimport time\nimport calendar\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix\n\nfrom sklearn import preprocessing\n\nfrom sklearn import metrics","06576adb":"%matplotlib inline","6e75638a":"#df = pd.read_csv('..\/input\/train.csv').sample(250000)\ndf =  pd.read_csv('..\/input\/train.csv', nrows = 2000000)","79caadc0":"df.head()","7423e444":"df.info()","053d9364":"unwanted_indices = df[ (abs(df['pickup_latitude']) > 90) | \n                       (abs(df['dropoff_latitude']) > 90) \n                     ].index\ndf.drop(list(unwanted_indices), inplace=True)\n\ndel unwanted_indices\nunwanted_indices = df[ (abs(df['pickup_longitude']) > 180) | \n                       (abs(df['dropoff_longitude']) > 180) \n                     ].index\ndf.drop(list(unwanted_indices), inplace=True)","b8273d90":"df[ (abs(df['pickup_longitude']) > 180) | \n                       (abs(df['dropoff_longitude']) > 180) \n                     ].index","af755777":"df[ (abs(df['pickup_latitude']) > 90) | \n                       (abs(df['dropoff_latitude']) > 90) \n                     ].index","6741fb7a":"unwanted_indices = df[df['dropoff_latitude'].isnull()].index\ndf.drop(list(unwanted_indices), inplace=True)\n\nunwanted_indices = df[df['dropoff_longitude'].isnull()].index\ndf.drop(list(unwanted_indices), inplace=True)","dfdab59c":"#\n#      Passanger number\n#\nif 'unwanted_indices' in globals(): del unwanted_indices\n\nunwanted_indices = df[ (df['passenger_count'] > 6) | (df['passenger_count'] == 0) ].index\ndf.drop(list(unwanted_indices), inplace=True)","aabe9cfd":"df.head()","6ce1d5de":"unwanted_indices = df[df['fare_amount']<=0].index\ndf.drop(list(unwanted_indices), inplace=True)","90fb282e":"df.head(2)","ae2a14c3":"#\n#      distance\n#      USA:  horizontal - 2,680 miles ; vertical : 1,582 miles.\n#\n# if 'unwanted_indices' in globals(): del unwanted_indices\n# unwanted_indices = df[ df['Travel distance'] > 500 ].index\n# df.drop(list(unwanted_indices), inplace=True)","906a645d":"df['Travel distance'] = list(  map( lambda x1,x2,x3,x4: \n                               geo.distance( (x3,x1), (x4,x2) ).miles,\n                               df['pickup_longitude'], df['dropoff_longitude'],\n                               df['pickup_latitude'],  df['dropoff_latitude'] ) )","005cd991":"newtimeStamp = pd.to_datetime( df['pickup_datetime'].apply( lambda x: x.split(' UTC')[0]) )\n#pd.to_datetime(df['pickup_datetime'])","41285574":"df['Hour']  = newtimeStamp.apply(lambda x : x.hour)\ndf['Month'] = newtimeStamp.apply(lambda x : x.month)\ndf['Date']  = newtimeStamp.apply(lambda x : x.date())","6929da6c":"for x in df['pickup_datetime'] :\n    y = x.split(' ')[2]\n    if( y!='UTC' ) :\n        print( y )","9f9e57ac":"Day_of_Week=newtimeStamp.apply(\n    lambda x : \n    calendar.day_name[datetime.date(x.year,x.month,x.day)\n                      .weekday()]\n)\n\ndmap = {0:'Mon',1:'Tue',2:'Wed',3:'Thu',4:'Fri',5:'Sat',6:'Sun'}\nDay_of_Week = newtimeStamp.apply(\n    lambda x : \n    dmap[datetime.date(x.year,x.month,x.day)\n            .weekday()]\n)\n\ndf['Day of Week'] = Day_of_Week","a3963af5":"df['qty'] = df['pickup_datetime'].apply( lambda x: 1)","f066dfda":"df['timestamp'] = newtimeStamp.apply(lambda x : time.mktime(\n                             ( x.year,x.month,x.day,\n                               x.hour,x.minute,x.second,\n                               1,x.day,-1) )\n                  )","a317f192":"df.head()","a66f9106":"sns.countplot( x='passenger_count',data=df )\n# plt.yscale('log')\n# plt.ylim(1,10**5)\n# plt.show()","22a2cb2f":"sns.lmplot(\"Travel distance\", \"fare_amount\", data=df, fit_reg=False, hue='passenger_count',aspect=1.2)\nplt.xscale('log')\nplt.xlim(0.01,10**4)\nplt.yscale('log')\nplt.ylim(1,10**3)\nplt.show()","246b0459":"sns.kdeplot( df['fare_amount'] )","f9e3a395":"sns.kdeplot(df['Travel distance'])\n#plt.xscale('log')\n#plt.yscale('log')\n#plt.xlim(0.5,100)\n#plt.ylim(0.000001,1)\n#plt.show()","140b3c83":"sns.countplot(x='Day of Week', data=df)","201008bb":"sns.factorplot(x='Day of Week',data=df,hue='passenger_count',\n               kind='count', log=True, size=5, aspect=1.8)","67e5e7cd":"dfm=df.groupby(['Day of Week','Hour']).count()\nfp=dfm['qty'].unstack()","787136be":"plt.figure(figsize=(12,6))\nsns.heatmap(fp,cmap='viridis')","cb401651":"plt.figure(figsize=(12,6))\nsns.clustermap(fp,cmap='viridis')","7e81a053":"dfm=df.groupby(['Day of Week','Month']).count()\nfp=dfm['qty'].unstack()\nplt.figure(figsize=(12,6))\nsns.heatmap(fp,cmap='viridis')","017fd0d9":"plt.figure(figsize=(12,6))\nsns.clustermap(fp,cmap='viridis')","d574de0c":"dfm=df.groupby(['Day of Week','Hour']).mean()\nfp=dfm['Travel distance'].unstack()\nplt.figure(figsize=(12,6))\nsns.heatmap(fp,cmap='viridis')","87fbcff1":"dfm=df.groupby(['Day of Week','Month']).mean()\nfp=dfm['Travel distance'].unstack()\nplt.figure(figsize=(12,6))\nsns.heatmap(fp,cmap='viridis',robust=True,annot=True)","940421e1":"sns.distplot(df['Travel distance'], hist=True, kde=False, \n             bins=40, color = 'darkblue', \n             hist_kws={'edgecolor':'black'},\n             kde_kws={'linewidth': 4})\n# plt.yscale('log')\n# plt.ylim(10**-1,10**2)\n# plt.show()","1e03068e":"sns.kdeplot(df['Travel distance'])\n# plt.xscale('log')\n# plt.yscale('log')\n# plt.xlim(10,1000)\n# plt.ylim(0.0000001,0.0001)\n# plt.show()","d65ab1d8":"dfm=df.groupby('Date').sum()\ndfm['qty'].plot(figsize=(10,5),grid=True,style='k.')","a3e91ae0":"pd.plotting.lag_plot(dfm['qty'],alpha=0.5)","ad0ea9cf":"dfm=df.groupby('Date').mean()\npd.plotting.autocorrelation_plot(dfm['Travel distance'],alpha=0.75)\n#plt.xlim(0,30)\nplt.ylim(-0.2,0.2)\nplt.show()","4e041fee":"dfm=df.groupby('Month').count()\ndfm['qty'].plot.line(figsize=(10,5),grid=True)","d34443be":"sns.lmplot(\"Travel distance\", \"fare_amount\", data=df, fit_reg=False, hue='Hour',aspect=1.5)\nplt.xscale('log')\nplt.yscale('log')\nplt.xlim(0.001,1000)\nplt.ylim(1,300)\nplt.show()","162c5307":"col=list(df.columns)\ncol","5b13a13e":"dftrain = df.drop([\n 'key',\n 'pickup_datetime',\n 'Hour',\n 'Month',\n 'Date',\n 'Day of Week',\n 'qty'\n                   ],axis=1).dropna()\ndftrain.head(2)","ac030ccf":"dftrain.info()","874dd44a":"X_train = dftrain.drop('fare_amount',axis=1)\ny_train = dftrain['fare_amount']","be4f2f66":"XX_train, XX_test, yy_train, yy_test = train_test_split(X_train, y_train, test_size=0.30)","d3f4dedb":"# import sklearn.pipeline\n# from sklearn.linear_model import LinearRegression\n\n# scaler = sklearn.preprocessing.StandardScaler()\n\n# lm = LinearRegression(fit_intercept=True)\n# steps = [('feature_selection', scaler),\n#         ('regression', lm)]\n\n# pipeline = sklearn.pipeline.Pipeline(steps)\n\n# # fit pipeline on X_train and y_train\n# pipeline.fit( XX_train, np.log(yy_train) )\n# #pipeline.fit( XX_train, yy_train )\n\n# # call pipeline.predict() on X_test data to make a set of test predictions\n# yy_predictions = pipeline.predict( XX_test )\n# predictions = np.exp( yy_predictions )\n# #predictions = yy_predictions\n\n# sns.distplot(yy_predictions,bins=1000)\n# # plt.yscale('log')\n# # plt.ylim(0.00001,1)\n# plt.xlabel(\"Predicted Fare\")\n# plt.ylabel(\"Distribution of observations\")\n# # plt.show()","c5bbc9d5":"# for yy in predictions:\n#     if yy<0 :\n#         print(yy)","a8c58a53":"# coeff_df = pd.DataFrame(lm.coef_,X_train.columns,columns=['Coefficient'])\n# coeff_df","ac65d642":"# print('MAE:', metrics.mean_absolute_error(yy_test, predictions))\n# print('MSE:', metrics.mean_squared_error(yy_test, predictions))\n# print('RMSE:', np.sqrt(metrics.mean_squared_error(yy_test, predictions)))","d349a2e2":"# plt.scatter(yy_test,predictions)\n# plt.xscale('log')\n# plt.yscale('log')\n# plt.xlim(1,1000)\n# plt.ylim(1,1000)\n# plt.xlabel(\"Actual Fare\")\n# plt.ylabel(\"Predicted Fare\")\n# plt.show()","729a489e":"# import sklearn.pipeline\n# from sklearn.linear_model import TheilSenRegressor\n\n# scaler = sklearn.preprocessing.StandardScaler()\n\n# lm = TheilSenRegressor()\n# steps = [('feature_selection', scaler),\n#         ('regression', lm)]\n\n# pipeline = sklearn.pipeline.Pipeline(steps)\n\n# # fit pipeline on X_train and y_train\n# #pipeline.fit( XX_train, np.log(yy_train) )\n# pipeline.fit( XX_train, yy_train )\n\n# # call pipeline.predict() on X_test data to make a set of test predictions\n# yy_predictions = pipeline.predict( XX_test )\n# #predictions = np.exp( yy_predictions )\n# predictions = yy_predictions\n\n# for yy in predictions: \n#     if yy!=yy :\n#         print(yy)","a283d65f":"# plt.scatter(yy_test,predictions)\n# plt.xscale('log')\n# plt.yscale('log')\n# plt.xlim(1,1000)\n# plt.ylim(1,1000)\n# plt.xlabel(\"Actual Fare\")\n# plt.ylabel(\"Predicted Fare\")\n# plt.show()","77bf9e47":"# print('MAE:', metrics.mean_absolute_error(yy_test, predictions))\n# print('MSE:', metrics.mean_squared_error(yy_test, predictions))\n# print('RMSE:', np.sqrt(metrics.mean_squared_error(yy_test, predictions)))","cada9cc2":"# import sklearn.pipeline\n# from sklearn import svm\n\n# scaler = sklearn.preprocessing.StandardScaler()\n\n# from sklearn.model_selection import GridSearchCV\n# param_grid = {\n#     'C': [50,75,100],#[0.1,1, 10, 100], \n#     'gamma': [40,20,1],#[1,0.1,0.01,0.001,0.0001], \n#     'kernel': ['rbf'],\n#     'tol':[0.001] \n# } \n# grid = GridSearchCV( svm.SVR(), param_grid,refit=True, verbose=3, n_jobs=4 )\n\n\n# steps = [('feature_selection', scaler),\n#         ('regression', grid)]\n\n# pipeline = sklearn.pipeline.Pipeline(steps)\n\n# # May take awhile!\n# # fit pipeline on X_train and y_train\n# #pipeline.fit( XX_train, np.log(yy_train) )\n# pipeline.fit( XX_train, yy_train )\n\n# # May take awhile!\n# yy_predictions = pipeline.predict(XX_test)\n\n# predictions = yy_predictions #np.exp( yy_predictions )\n\n# print(grid.best_params_)\n# print(grid.best_estimator_)\n\n# for yy in predictions:\n#     if yy<0 :\n#         print(yy)\n        \n# sns.distplot(predictions,bins=1000)\n# plt.xlabel(\"Predicted Fare\")\n# plt.ylabel(\"Distribution of observations\")\n# plt.show()","8ace0c53":"# plt.scatter(yy_test,predictions)\n# plt.xscale('log')\n# plt.yscale('log')\n# plt.xlim(1,1000)\n# plt.ylim(1,1000)\n# plt.xlabel(\"Actual Fare\")\n# plt.ylabel(\"Predicted Fare\")\n# plt.show()","c6b54071":"# eps = (yy_test-predictions)\n# # Density Plot and Histogram of travel distances\n# sns.distplot(eps, hist=True, kde=True, \n#              bins=100, color = 'darkblue', \n#              hist_kws={'edgecolor':'black'},\n#              kde_kws={'linewidth': 2})\n# plt.xlabel('error - difference between fares')\n# plt.show()","d26e022f":"# print('MAE:', metrics.mean_absolute_error(yy_test, predictions))\n# print('MSE:', metrics.mean_squared_error(yy_test, predictions))\n# print('RMSE:', np.sqrt(metrics.mean_squared_error(yy_test, predictions)))","28f52e64":"# import sklearn.pipeline\n# scaler = sklearn.preprocessing.StandardScaler()\n\n# import tensorflow as tf\n# feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(X_train)\n\n# from tensorflow.contrib import learn\n\n# classifier = learn.DNNRegressor(\n#     feature_columns=feature_columns,\n#     hidden_units=[20,40,20],\n#     optimizer=tf.train.ProximalAdagradOptimizer(\n#       learning_rate=0.1,\n#       l1_regularization_strength=0.01\n#     ))\n\n# steps = [('scaler', scaler),\n#         ('DNNclassifier', classifier)]\n# pipeline = sklearn.pipeline.Pipeline(steps)\n\n# ### fit pipeline on X_train and y_train\n# pipeline.fit( XX_train, yy_train, DNNclassifier__steps=800)\n\n# ### call pipeline.predict() on X_test data to make a set of test predictions\n# predictions = pipeline.predict( XX_test )\n# predictions = list(pd.DataFrame(predictions)[0])\n\n# print('MAE:', metrics.mean_absolute_error(yy_test, predictions))\n# print('MSE:', metrics.mean_squared_error(yy_test, predictions))\n# print('RMSE:', np.sqrt(metrics.mean_squared_error(yy_test, predictions)))","6d7e3a60":"# plt.scatter(yy_test,predictions)\n# plt.xscale('log')\n# plt.yscale('log')\n# plt.xlim(1,100)\n# plt.ylim(1,100)\n# plt.xlabel(\"Actual Fare\")\n# plt.ylabel(\"Predicted Fare\")\n# plt.show()","ff490cd6":"# import sklearn.pipeline\n# scaler = sklearn.preprocessing.StandardScaler()\n\n# import lightgbm as lgb\n# # train\n# gbm = lgb.LGBMRegressor(objective='regression',\n#                         boosting_type='gbdt',\n#                         num_leaves=100,#1001,\n#                         learning_rate=0.003,\n#                         n_estimators=1000)\n\n# steps = [('scaler', scaler),\n#         ('GBM', gbm)]\n\n# pipeline = sklearn.pipeline.Pipeline(steps)\n\n# ### fit pipeline on X_train and y_train\n# pipeline.fit( XX_train, yy_train)\n\n# ### call pipeline.predict() on X_test data to make a set of test predictions\n# predictions = pipeline.predict( XX_test )\n\n# print('MAE:', metrics.mean_absolute_error(yy_test, predictions))\n# print('MSE:', metrics.mean_squared_error(yy_test, predictions))\n# print('RMSE:', np.sqrt(metrics.mean_squared_error(yy_test, predictions)))\n\n# plt.scatter(yy_test,predictions)\n# plt.xscale('log')\n# plt.yscale('log')\n# plt.xlim(1,100)\n# plt.ylim(1,100)\n# plt.xlabel(\"Actual Fare\")\n# plt.ylabel(\"Predicted Fare\")\n# plt.show()","15115079":"dftest = pd.read_csv('..\/input\/test.csv')","1da7b94d":"dftest.info()","07599e65":"dftest.head()","05b01cad":"newtimeStamp=pd.to_datetime(dftest['pickup_datetime'])\ndftest['Hour']  = newtimeStamp.apply(lambda x : x.hour)\ndftest['Month'] = newtimeStamp.apply(lambda x : x.month)\ndftest['Date']  = newtimeStamp.apply(lambda x : x.date())","8ec4c965":"#\n# If other than UTC time zone\n#\nfor x in dftest['pickup_datetime'] :\n    y = x.split(' ')[2]\n    if( y!='UTC' ) :\n        print( y )","2da5cd00":"newtimeStamp[0]","6ea393a9":"newtimeStamp[0].date()","da146010":"import datetime\nimport calendar\nDay_of_Week=newtimeStamp.apply(\n    lambda x : \n    calendar.day_name[datetime.date(x.year,x.month,x.day)\n                      .weekday()]\n)\n\ndmap = {0:'Mon',1:'Tue',2:'Wed',3:'Thu',4:'Fri',5:'Sat',6:'Sun'}\nDay_of_Week = newtimeStamp.apply(\n    lambda x : \n    dmap[datetime.date(x.year,x.month,x.day)\n            .weekday()]\n)\n\ndftest['Day of Week'] = Day_of_Week","7ea9802f":"dftest['qty'] = dftest['pickup_datetime'].apply( lambda x: 1)","8deb16ed":"newtimeStamp.head(2)","6001e48f":"dftest['timestamp'] = newtimeStamp.apply(lambda x : time.mktime(\n                              (x.year,x.month,x.day,\n                               x.hour,x.minute,x.second,\n                               1,x.day,-1) ))","3c760cd2":"import geopy.distance as geo\n\ndftest['Travel distance'] = list(map( lambda x1,x2,x3,x4: \n                   geo.distance( (x3,x1), (x4,x2) ).miles,\n                   dftest['pickup_longitude'], dftest['dropoff_longitude'],\n                   dftest['pickup_latitude'],  dftest['dropoff_latitude'] ))","3b8a0273":"sns.countplot( x='passenger_count',data=dftest )\n# plt.yscale('log')\n# plt.ylim(100,10**4)\n# plt.show()","655bcb6d":"sns.kdeplot(dftest['Travel distance'])\nplt.xscale('log')\nplt.yscale('log')\nplt.xlim(10**-1,10**2)\nplt.ylim(10**-5,1)\nplt.show()","8b230ace":"dftest.head()","0dbc12ce":"sns.countplot(x='Day of Week', data=dftest)","ce73713f":"sns.factorplot(x='Day of Week',data=dftest,hue='passenger_count',\n               kind='count', log=True, size=5, aspect=1.8)\n#\n#sns.countplot(x='Day of Week',data=dftest,hue='passenger_count',palette='viridis')\n#plt.legend(loc=2, bbox_to_anchor=(1.05,1),borderaxespad=0.)","4a846546":"# list(dfm.columns)","53ba6f91":"dfm=dftest.groupby(['Day of Week','Hour']).count()\nfp=dfm['qty'].unstack()\nfp","a4c0e775":"plt.figure(figsize=(12,6))\nsns.heatmap(fp,cmap='viridis')","2f95f7e7":"plt.figure(figsize=(12,6))\nsns.clustermap(fp,cmap='viridis')","bb287787":"dfm=dftest.groupby(['Day of Week','Month']).count()\nfp=dfm['qty'].unstack()\nplt.figure(figsize=(12,6))\nsns.heatmap(fp,cmap='viridis')","f0acd222":"plt.figure(figsize=(12,6))\nsns.clustermap(fp,cmap='viridis')","da15bda0":"dfm=dftest.groupby(['Day of Week','Hour']).mean()\nfp=dfm['Travel distance'].unstack()\n#fp\nplt.figure(figsize=(12,6))\nsns.heatmap(fp,cmap='viridis')","75056c38":"dfm=dftest.groupby(['Day of Week','Month']).mean()\nfp=dfm['Travel distance'].unstack()\nplt.figure(figsize=(12,6))\nsns.heatmap(fp,cmap='viridis',robust=True,annot=True)","3359e063":"dfm=dftest.groupby('Date').sum()\ndfm['qty'].plot(figsize=(10,5),grid=True,style='k.')\nplt.yscale('log')\nplt.ylim(0.1,1000)\nplt.show()","3e762496":"pd.plotting.lag_plot(dfm['qty'],alpha=0.5)\nplt.xlim(0,30)\nplt.ylim(0,30)\nplt.show()","a1e3268d":"dfm=dftest.groupby('Date').mean()\n\npd.plotting.autocorrelation_plot(dfm['Travel distance'],alpha=0.75)\n#plt.xlim(0,30)\nplt.ylim(-0.12,0.12)\nplt.show()","7e58a322":"dfm=dftest.groupby('Month').count()\ndfm['qty'].plot.line(figsize=(10,5),grid=True)","34624027":"from sklearn import preprocessing","2f197326":"list(X_train.columns)","8c21f167":"df.head(2)","dc0fa982":"dftest.head(2)","ab13ea96":"coltest=list(dftest.columns)\nidy=coltest[2:7]\nidy.append(coltest[12])\nidy.append(coltest[13])\nidy","0790180f":"X_train = dftrain[idy]\ny_train = dftrain['fare_amount']\n\nX_test  = dftest[idy]","60434d5f":"len(y_train)","ab1ea901":"X_test.head(2)","f53e9df5":"X_train.head(2)","896022b0":"# X_test.head(2)","3dcacfb5":"# X_train.head(2)","ef55244a":"# from sklearn.model_selection import train_test_split\n# from sklearn.metrics import classification_report,confusion_matrix","f4b49740":"# import sklearn.pipeline\n# from sklearn.linear_model import TheilSenRegressor\n\n# scaler = sklearn.preprocessing.StandardScaler()\n# lm = TheilSenRegressor()\n# steps = [('feature_selection', scaler),\n#         ('regression', lm)]\n\n# pipeline = sklearn.pipeline.Pipeline(steps)\n\n# # fit pipeline on X_train and y_train\n# pipeline.fit( X_train, np.log(y_train) )\n\n# # call pipeline.predict() on X_test data to make a set of test predictions\n# y_predictions = pipeline.predict( X_test )\n# predictions = np.exp( y_predictions )\n\n# #print the intercept\n# print(lm.intercept_)\n\n# coeff_df = pd.DataFrame(lm.coef_,X_train_scaled.columns,columns=['Coefficient'])\n# coeff_df","b244f7c8":"# import sklearn.pipeline\n# from sklearn import svm\n\n# scaler = sklearn.preprocessing.StandardScaler()\n\n# from sklearn.model_selection import GridSearchCV\n# param_grid = {\n#     'C': [300,350,400],#[10,50,100,200,300,400], \n#     'gamma': [125,100,75],#[100,75,50,25,1],\n#     'kernel': ['rbf'],\n#     'tol':[0.001] \n# } \n# grid = GridSearchCV( svm.SVR(), param_grid,refit=True, verbose=3, n_jobs=8 )\n\n\n# steps = [('feature_selection', scaler),\n#         ('regression', grid)]\n\n# pipeline = sklearn.pipeline.Pipeline(steps)\n\n# # May take awhile!\n# # fit pipeline on X_train and y_train\n# #pipeline.fit( X_train, np.log(y_train) )\n# pipeline.fit( X_train, y_train )\n\n# # May take awhile!\n# # y_grdsvc = pipeline.predict(X_test)\n# # predictions = np.exp( y_grdsvc )\n# predictions = pipeline.predict(X_test)\n\n# print(grid.best_params_)\n# print(grid.best_estimator_)","c0fd1e25":"# import sklearn.pipeline\n# scaler = sklearn.preprocessing.StandardScaler()\n\n# import tensorflow as tf\n# feature_columns = tf.contrib.learn.infer_real_valued_columns_from_input(X_train)\n\n# from tensorflow.contrib import learn\n\n# classifier = learn.DNNRegressor(\n#     feature_columns=feature_columns,\n#     hidden_units=[20,40,20],\n#     optimizer=tf.train.ProximalAdagradOptimizer(\n#       learning_rate=0.1,\n#       l1_regularization_strength=0.01\n#     ))\n\n# steps = [('scaler', scaler),\n#         ('DNNclassifier', classifier)]\n# pipeline = sklearn.pipeline.Pipeline(steps)\n\n# ### fit pipeline on X_train and y_train\n# pipeline.fit( X_train, y_train, DNNclassifier__steps=1000)\n\n# ### call pipeline.predict() on X_test data to make a set of test predictions\n# predictions = pipeline.predict( X_test )\n# predictions = list(pd.DataFrame(predictions)[0])","094db7a8":"import sklearn.pipeline\nscaler = sklearn.preprocessing.StandardScaler()\n\nimport lightgbm as lgb\n# train\ngbm = lgb.LGBMRegressor(objective='regression',\n                        boosting_type='gbdt',\n                        num_leaves=1001,\n                        learning_rate=0.01,\n                        n_estimators=2500)\n\nsteps = [('scaler', scaler),\n        ('GBM', gbm)]\n\npipeline = sklearn.pipeline.Pipeline(steps)\n\n### fit pipeline on X_train and y_train\npipeline.fit( X_train, y_train)\n\n### call pipeline.predict() on X_test data to make a set of test predictions\npredictions = pipeline.predict( X_test )","51739a44":"for yy in predictions:\n    if yy<0 :\n        print(yy)","411b2c57":"temp = pd.DataFrame(predictions,columns=['Predicted Fare'])\ndf_pred = dftest[['Travel distance','Hour']].join(temp)\n#\n#print( df_pred['Travel distance'].max(), df_pred['Predicted Fare'].max() )\n#\nsns.lmplot(\"Travel distance\", \"Predicted Fare\", data=df_pred, \n           fit_reg=False, hue='Hour', scatter_kws={'alpha':0.5}, aspect=1.8)\nplt.xscale('log')\nplt.yscale('log')\nplt.xlim(0.001,500)\nplt.ylim(1,300)\nplt.show()","0481a50b":"sns.lmplot(\"Travel distance\", \"fare_amount\", data=df, fit_reg=False, \n            hue='Hour', scatter_kws={'alpha':0.5}, aspect=1.8)\nplt.xscale('log')\nplt.yscale('log')\nplt.xlim(0.001,500)\nplt.ylim(1,300)\nplt.show()","8e18ee20":"fare = []\nfor y_pred in predictions:\n    fare.append( '{:.{prec}f}'.format(y_pred, prec=2) ) ","46fc5ca2":"print(len(fare),len(predictions))","4417743e":"pd.DataFrame( { 'key':list(dftest['key']),\n                'fare_amount':fare } ).set_index('key').to_csv('sample_submission.csv', sep=',')","e1f6f26e":"Cluster map:\n- color clustering for hour and day of the week","391b4ee8":"# DATA MODELING\n\n**Method1 : Pipeline - Regression**","52eab04d":"**Descriptive statistics**","ec43101e":"## TEST DATA SET - Visualization","fdc1ea1c":"## TRAINING DATA SET - add data fields\n- split the time in date, hour, month\n- create a unix timestamp for further computation","013ce718":"# DATA ANALYSIS - Hypothesis Testing","840bcbf0":"Heat map and cluster map for Day of Week and Month","1be1af2f":"Fare amount is proportional to travel distance","6ad99f6e":"**Plot**\n- observation at time t on the x-axis and the lag1 observation (t-1) on the y-axis\n- relatively strong positive correlation between observations and their lag1 values.","640c14eb":"**Descriptive stats about the data set**","a2c21af9":"**Plot**\n- correlation calculated against lag values in time series ( autocorrelation \/ self-correlation)\n- value close to zero suggests a weak correlation\n- value closer to -1 or 1 indicates a strong correlation\n- cycles of strong negative and positive correlation\n- dotted lines indicate that any correlation values above those lines are statistically significant ","d91cce70":"**Plot**\n- observation at time t on the x-axis and the lag1 observation (t-1) on the y-axis\n- relatively strong positive correlation between observations and their lag1 values.","3ae67813":"Comparitively higher traffic from Thursday to Saturday","460b53c2":"# Main Work Flow\n- **Import libraries**\n- **Data visualization**\n        - Histograms, number of trips over month, year\n        - Hypothesis : relation between travel distance and fare\n- **Cleaning of training dataset**\n        - Latitude ~[-90,90]\n        - Longitude ~[-180,180]\n        - Passanger count ~ [1,6]\n        - fare amount > 0\n- **Choice of method**\n        - Split the training data between a training and a testing data subset\n        - Apply various supervised machine learning methods\n        - Determine the error and plot the distribution\n- **Preprocessing of testing dataset**\n        - Follow the aforementioned preprocessing steps for the training dataset\n        - Avoid 'dropna' type functions to preserve predictions for all the keys\n- **Transformation and post-processing**\n        - Use of pipeline for processing\n        - Comparison of predictions with y_train dataset to gain insight about the distribution\n        - Testing of hypothesis based on distance and fare ","aa481c69":"Higher passanger counts for 1,2,5","8e3b97fa":"Higher number of rides for May","0e2d9dd3":"Passanger counts varies over an order of magnitude","b7b08c06":"## TRAINING DATA SET - visualization","fd80def7":"Density plot and histogram of travel distance\n","15239d49":"**Based on NaN values**","b0275c8c":"## TRAINING DATA SET : Hypothesis\n- Proportionality : fare and travel distance ","5287eb61":"**Let's remove the rows with -ve fares**","f0939ccc":"**Let's split up the data into a training set and a test set!**","b33a6202":"# TEST DATA\n**Importing data**","1b415628":"**Predicted fare is +ve**","2d7e6560":"# Methods \/ Approaches\n**Method 5 : LightGBM - pipeline**","5a35a653":"Let's test the hypothesis.","a9e3f463":"**Based on distance**","d25da0be":"Let's draw the same for training data.","cdc488de":"Distribution of fare amount","b6e92534":"## TRAINING DATA SET - time serise analysis","74fb4777":"## TEST DATA SET - time serise analysis","1bf8e679":"Distribution of travel distance","a7e76a1c":"## TEST DATA SET - Preprocessing","ce87b228":"## TEST DATA SET - Preprocessing\n**(a) Train and Test Datasets**","ea4e71a4":"Heat map and cluster map of travel distance for Day of Week and Hour","8b666890":"**Method 2 : Machine Learning - Theil-Sen Estimator**","3ea4ee8b":"## Saving predicted data to a file","2d4166e4":"**Let's clean the traning data based on latitude and longitude**","8e040ca5":"# Methods \/ Approaches\n**Method 3 : Machine Learning - SVM Estimator pipeline**","46baaf64":"# TRAINING DATA SET","0596da4a":"# Data Import","cc1bb090":"**Plot**\n- correlation is calculated against lag values in time series ( autocorrelation \/ self-correlation)\n- value close to zero suggests a weak correlation\n- value close to -1 or 1 indicates a strong correlation\n- dotted lines indicate any correlation values above those lines are statistically significant ","5d843a3f":"# Methods \/ Approaches\n**Method 4 : Deep Learning - DNN Estimator pipeline**","ac68acfe":"## TRAINING DATA SET - Cleaning\n","6220065b":"## TRAINING DATA SET : ML method -Train Test Split","0bbb4833":"**Cross-check**","eb9a70d8":"**Method 1 : Machine Learning - LinearRegression Estimator pipeline**","61e7a807":"**Method2 : Pipeline - SVM**","5fb23047":"\n**Based on passenger counts**","63b96d23":"## TRAINING DATA SET : Methods \/ Approaches\n- The test data doesn't include y_train values \n- Test few methodologies to model the system \n- Test the accuracy of the methods","79b626d8":"Heat map: \n- Color is proportional to the number of rides\n- Higher numbers for friday and saturday evenings"}}