{"cell_type":{"009e7691":"code","2bc8d523":"code","37375497":"code","5d79da53":"code","5f10245d":"code","0ed705bd":"code","42e295a9":"code","9cdba52f":"code","9f5a7691":"code","53d26b8a":"code","fc44fb5e":"code","5e3fceaa":"code","032f3e8d":"code","91198ac0":"code","5ddb3960":"code","ba089298":"code","962f31e7":"code","178da211":"code","534938a8":"code","f2a85f50":"code","6fcafaca":"code","2bd2d071":"code","07f7a0ef":"code","0c98658b":"code","d2b4d0fa":"code","aa056762":"code","f53df075":"code","3fd0c781":"code","fb2f471a":"markdown","3d216c58":"markdown","d3a1ff3b":"markdown","ee3f6acc":"markdown","005ed31d":"markdown","b5344901":"markdown","3c4eb03b":"markdown","c63f8f38":"markdown"},"source":{"009e7691":"from tqdm.autonotebook import tqdm\nimport numpy as np # linear algebra\nimport pandas as pd","2bc8d523":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/train.csv')\ntrain.head()","37375497":"test = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/test.csv')\ncat_test = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/test.csv')\nsub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-mar-2021\/sample_submission.csv')","5d79da53":"cols = [i for i in train.columns if train[i].dtypes=='object']\ncols","5f10245d":"for i in cols:\n    print(train[i].unique())\n    \n# we will lable them manualy like a =1 , b =2 so ab = 26+(1*2) = 28,  why we add so that they are above 26 as z ==26  so like that we will do it for all, yes it is to be done manually","0ed705bd":"labels = {}\nimport string\ntest_list = list(string.ascii_uppercase) \nprint(test_list)\nfor i,j in zip(range(1,27),test_list):\n    labels[j] = i\n    ","42e295a9":"for p,q in zip(range(1,27),test_list):\n    for i,j in zip(range(1,27),test_list):\n        labels[q+j] = 26+(p*i)\n\n# we are realy with out lables , lets change our value","9cdba52f":"for i in tqdm(cols):\n    for p,q in labels.items():\n        train[i] = train[i].replace(p,q)","9f5a7691":"for i in tqdm(cols):\n    for p,q in labels.items():\n        test[i] = test[i].replace(p,q)","53d26b8a":"train = train.drop(columns='id')\ntest = test.drop(columns='id')","fc44fb5e":"train.groupby('target').nunique()","5e3fceaa":"from imblearn.under_sampling import RandomUnderSampler\nfrom imblearn.over_sampling import RandomOverSampler\n\nX = train.drop(columns='target')\nY = train['target']\n\n# special case\nover = RandomOverSampler(sampling_strategy=0.5)\nunder = RandomOverSampler(sampling_strategy=0.8)\n\nx1, y1 = over.fit_resample(X,Y)\nc_x, c_y = under.fit_resample(x1, y1)\nprint(len(c_y))\n# # we will create four models ","032f3e8d":"import optuna.integration.lightgbm as lgb\nimport optuna\nfrom sklearn.model_selection import train_test_split\nimport lightgbm\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom catboost import CatBoostClassifier\nfrom xgboost import XGBClassifier\nimport xgboost as xgb","91198ac0":"def objective(trial, data = X, target = Y):\n    X_train, X_val, y_train, y_val = train_test_split(X, Y,stratify=Y, test_size = 0.2, random_state = 0)\n\n    params = {\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'learning_rate': trial.suggest_categorical('learning_rate', [0.005, 0.02, 0.05, 0.08, 0.1]),\n        'n_estimators': trial.suggest_int('n_estimators', 50, 3000),\n        'max_bin': trial.suggest_int('max_bin', 200, 400),\n        'min_data_in_leaf': trial.suggest_int('min_data_in_leaf', 1, 300),\n        'l2_leaf_reg': trial.suggest_float('l2_leaf_reg', 0.0001, 1.0, log = True),\n        'subsample': trial.suggest_float('subsample', 0.1, 0.8),\n        'random_seed': 42,\n        'task_type': 'GPU',\n        'loss_function': 'Logloss',\n        'eval_metric': 'AUC',\n        'bootstrap_type': 'Poisson'\n    }\n    \n    model = CatBoostClassifier(**params)  \n    model.fit(X_train, y_train, eval_set = [(X_val,y_val)], early_stopping_rounds = 222, verbose = False)\n    y_pred = model.predict_proba(X_val)[:,1]\n    roc_auc = roc_auc_score(y_val, y_pred)\n\n    return roc_auc","5ddb3960":"study = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, n_trials = 1)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)","ba089298":"def objective(trial):\n    X_train, X_val, y_train, y_val = train_test_split(X, Y,stratify=Y, test_size = 0.2, random_state = 0)\n    params = {\n        \"verbosity\": 0,\n        \"objective\": \"binary:logistic\",\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        \"eval_metric\": \"auc\",\n        \"eta\" : trial.suggest_float(\"eta\", 1e-8, 1.0, log=True),\n        \"booster\": trial.suggest_categorical(\"booster\", [\"dart\"]),\n        \"lambda\": trial.suggest_float(\"lambda\", 1e-8, 1.0, log=True),\n        \"alpha\": trial.suggest_float(\"alpha\", 1e-8, 1.0, log=True),\n        'tree_method' : 'gpu_hist',\n        'eval_metric': 'auc',\n        'random_seed': 42,\n    }\n\n    model = XGBClassifier(**params) \n    model.fit(X_train, y_train, eval_set = [(X_val,y_val)], early_stopping_rounds = 222, verbose = False)\n    y_pred = model.predict_proba(X_val)[:,1]\n    roc_auc = roc_auc_score(y_val, y_pred)\n    return roc_auc","962f31e7":"study = optuna.create_study(direction = 'maximize')\nstudy.optimize(objective, n_trials = 1)\nprint('Number of finished trials:', len(study.trials))\nprint('Best trial:', study.best_trial.params)\nprint('Best value:', study.best_value)","178da211":"# optuna.visualization.plot_param_importances(study)","534938a8":"from sklearn.preprocessing import StandardScaler\nfrom catboost import CatBoostClassifier\ndef scaling(X_train, X_test, trial):\n    sc = StandardScaler()\n    X_train = sc.fit_transform(X_train)\n    X_test = sc.transform(X_test)\n    test_data = sc.transform(trial)\n    return X_train, X_test, test_data\n\n\ndef lgbm_model(X_train, X_test, Y_train,Y_test,test_data):\n    \n    params =  {'reg_alpha': 9.883045220708368,\n         'reg_lambda': 1.8406970907815618e-05, \n         'num_leaves': 105, 'learning_rate': 0.0363323342120501,\n         'max_depth': 9, 'n_estimators': 7645, 'min_child_samples': 30, \n         'min_child_weight': 0.6126344551951962, 'subsample': 0.305413058674825,\n         'colsample_bytree': 0.9557961982690266, 'random_state': 2021,\n        'metric' : 'auc',\n        'device_type' : 'gpu'}\n    \n    model = lightgbm.LGBMClassifier(**params)\n    model.fit(X_train , Y_train , eval_set = [(X_test , Y_test)] , early_stopping_rounds = 200 , \\\n             verbose = False)\n    preds = model.predict_proba(X_test,num_iteration=model.best_iteration_)[:,1]\n    test_preds = model.predict_proba(test_data,num_iteration=model.best_iteration_)[:,1]\n    return preds,test_preds\n\n\n\n\ndef xgb_model(X_train, X_test, Y_train,Y_test,test_data):\n    \n    params =  {'max_depth': 8, 'eta': 0.1917070083625873, 'booster': 'dart', \n               'lambda': 0.003158888201683581, 'alpha': 0.16427957283436528,\n            'tree_method' : 'gpu_hist',\n        'eval_metric': 'auc',\n        'random_seed': 42 }\n    \n    model = XGBClassifier(**params)  \n    model.fit(X_train, Y_train, eval_set = [(X_test,Y_test)], early_stopping_rounds = 222, verbose = False)\n    preds = model.predict_proba(X_test)[:,1]\n    test_preds = model.predict_proba(test_data)[:,1]\n    return preds,test_preds","f2a85f50":"score = []\nfinal = []\nkfold = StratifiedKFold(n_splits=2, random_state=24, shuffle=True)\n\nfor Train, Test in kfold.split(X,Y):\n    X_train, X_test = X.iloc[Train], X.iloc[Test]\n    Y_train, Y_test = Y.iloc[Train], Y.iloc[Test]\n#     X_train, X_test, test_data = scaling(X_train, X_test,test)\n    #====== MODEL =======================\n    params = {'max_depth': 6, 'learning_rate': 0.05, 'n_estimators': 2997, \n              'max_bin': 328, 'min_data_in_leaf': 28,\n              'l2_leaf_reg': 0.003018532125919831, \n              'subsample': 0.6347767905576908,\n             'random_seed': 42,\n        'task_type': 'GPU',\n             'eval_metric': 'AUC',\n             'bootstrap_type': 'Poisson'}\n    \n    model = CatBoostClassifier(**params)  \n    model.fit(X_train, Y_train, eval_set = [(X_test,Y_test)], early_stopping_rounds = 222, verbose = False)\n    preds = model.predict_proba(X_test)[:,1]\n    test_preds = model.predict_proba(test)[:,1]\n    \n    #====================================\n    roc = roc_auc_score(Y_test, preds)\n    print(roc)\n    score.append(roc)\n    final.append(test_preds)\naverage_score = np.mean(score)\nprint('The average roc is ', average_score)\n    \n","6fcafaca":"score = []\nu_final = []\nkfold = StratifiedKFold(n_splits=2, random_state=24, shuffle=True)\n\nfor Train, Test in kfold.split(X , Y):\n    X_train, X_test = X.iloc[Train], X.iloc[Test]\n    Y_train, Y_test = Y.iloc[Train], Y.iloc[Test]\n#     X_train, X_test, test_data = scaling(X_train, X_test,test)\n    #====== MODEL =======================\n    preds,test_preds = lgbm_model(X_train, X_test, Y_train, Y_test,test)\n    #====================================\n    roc = roc_auc_score(Y_test, preds)\n    print(roc)\n    score.append(roc)\n    u_final.append(test_preds)\naverage_score = np.mean(score)\nprint('The average roc is ', average_score)\n    \n","2bd2d071":"score = []\no_final = []\nkfold = StratifiedKFold(n_splits=2, random_state=24, shuffle=True)\n\nfor Train, Test in kfold.split(X , Y):\n    X_train, X_test = X.iloc[Train], X.iloc[Test]\n    Y_train, Y_test = Y.iloc[Train], Y.iloc[Test]\n#     X_train, X_test, test_data = scaling(X_train, X_test,test)\n    #====== MODEL =======================\n    preds,test_preds = xgb_model(X_train, X_test, Y_train, Y_test,test)\n    #====================================\n    roc = roc_auc_score(Y_test, preds)\n    print(roc)\n    score.append(roc)\n    o_final.append(test_preds)\naverage_score = np.mean(score)\nprint('The average roc is ', average_score)\n    \n","07f7a0ef":"r1= np.mean(final,0)  # cat\nr2 = np.mean(o_final,0) #xgb\nr3 = np.mean(u_final,0)  # lgbm","0c98658b":"r_9 = r1*0.2+r2*0.2+r3*0.6\nr_9","d2b4d0fa":"r_8 = r1*0.3+r2*0.1+r3*0.6\nr_8","aa056762":"r = r_9*0.4+r_8*0.6\nr","f53df075":"sub['target'] = r","3fd0c781":"sub.to_csv('sub.csv',index=False)","fb2f471a":"# **Here i have run it only for one trial, but actually for better score, i run it for 30 trials, you can tune it**","3d216c58":"Special thanks to [https:\/\/www.kaggle.com\/haichaoshang\/tps-mar2021-lgbm-optuna](http:\/\/)","d3a1ff3b":"# **Data Reading**","ee3f6acc":"# Data Balancing","005ed31d":"# Hypermeter Optimization","b5344901":"# Complete Code, it helped me to get score 0.8897 on leaderboard","3c4eb03b":"# Data encoding","c63f8f38":"**Again runing for only 2 splits**"}}