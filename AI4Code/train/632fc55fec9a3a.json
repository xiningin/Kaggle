{"cell_type":{"a30ed1a9":"code","1114b275":"code","a9fddbcc":"code","dedc9c4c":"code","e6c4e70b":"code","0e8cdb48":"code","390526de":"code","92f29c2f":"code","462895dd":"code","1c837875":"code","54c9b52c":"code","96a3db93":"code","4bd05313":"code","176ba1f7":"code","8bd1ae01":"code","f3cfee0c":"markdown","57c795a6":"markdown","5e189b9d":"markdown","e79031aa":"markdown","f263d77a":"markdown","0c3a1157":"markdown","48cf610d":"markdown"},"source":{"a30ed1a9":"# set up environment\nimport math, re, os\n\nimport numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport matplotlib.pyplot as plt  \n\nfrom IPython import display\nfrom kaggle_datasets import KaggleDatasets","1114b275":"AUTOTUNE = tf.data.experimental.AUTOTUNE\nGCS_PATH = KaggleDatasets().get_gcs_path()\nBATCH_SIZE = 16\n\nTRAINING_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/tfrecords\/train\/*.tfrec')\nTEST_FILENAMES = tf.io.gfile.glob(GCS_PATH + '\/tfrecords\/test\/*.tfrec')","a9fddbcc":"def decode_audio(audio_binary):\n    audio, _ = tf.audio.decode_wav(audio_binary)\n    return tf.squeeze(audio, axis=-1)\n\ndef string_split_semicolon(column):\n    split_labels_sc = tf.strings.split(column, sep=';')\n    return split_labels_sc\n\ndef string_split_comma(column):\n    split_labels_c = tf.strings.split(column, sep=',')\n    return split_labels_c\n\ndef read_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"audio_wav\"    : tf.io.FixedLenFeature([], tf.string), \n        \"label_info\"   : tf.io.FixedLenFeature([], tf.string) \n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    audio = decode_audio(example['audio_wav'])    \n    first_split = string_split_semicolon(example['label_info'])\n    remove_quotes = tf.strings.regex_replace(first_split, '\"', \"\") \n    second_split = string_split_comma(remove_quotes)  \n    species_id = tf.gather_nd(second_split, [0, 0])  \n    return audio, species_id \n\ndef read_unlabeled_tfrecord(example):\n    UNLABELED_TFREC_FORMAT = {\n        \"recording_id\" : tf.io.FixedLenFeature([], tf.string),      \n        \"audio_wav\"    : tf.io.FixedLenFeature([], tf.string) \n    }\n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    audio = decode_audio(example['audio_wav'])\n    idnum = example['recording_id']\n    return audio, idnum \n\ndef load_dataset(filenames, labeled=True, ordered=False):\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False  \n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTOTUNE) \n    dataset = dataset.with_options(ignore_order)  \n    dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTOTUNE)\n    return dataset \n\ndef get_training_dataset():\n    dataset = load_dataset(TRAINING_FILENAMES, labeled=True)\n    dataset = dataset.repeat()  \n    dataset = dataset.shuffle(128)\n    dataset = dataset.batch(BATCH_SIZE)\n    dataset = dataset.prefetch(AUTOTUNE) \n    return dataset\n\ndef count_data_items(filenames):\n    n = [int(re.compile(r\"-([0-9]*)\\.\").search(filename).group(1)) for filename in filenames]\n    return np.sum(n)","dedc9c4c":"NUM_TRAINING_FILES = count_data_items(TRAINING_FILENAMES)\nNUM_TEST_FILES = count_data_items(TEST_FILENAMES)\n\nprint('Dataset: {} training files, {} unlabeled test files'.format(NUM_TRAINING_FILES, NUM_TEST_FILES))","e6c4e70b":"for audio, species_id in get_training_dataset().take(3):\n    print(audio.numpy().shape, species_id.numpy().shape)\n\nfor audio, species_id in get_training_dataset().take(1):\n    print(\"\\naudio examples:\", audio.numpy())\n    print(\"species_id examples:\", species_id.numpy())","0e8cdb48":"training_ds = load_dataset(TRAINING_FILENAMES, labeled=True)\ntest_ds = load_dataset(TEST_FILENAMES, labeled=False)","390526de":"# training dataset\n\nrows = 3\ncols = 3\nn = rows*cols\nfig, axes = plt.subplots(rows, cols, figsize=(10, 12))\n\nfor i, (audio, species_id) in enumerate(training_ds.take(n)):\n    r = i \/\/ cols\n    c = i % cols\n    ax = axes[r][c]\n    ax.plot(audio.numpy())\n    ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n    label = species_id.numpy().decode('utf-8')\n    ax.set_title(label)\n\nplt.show()","92f29c2f":"# test dataset\n\nrows = 3\ncols = 3\nn = rows*cols\nfig, axes = plt.subplots(rows, cols, figsize=(10, 12))\n\nfor i, (audio, recording_id) in enumerate(test_ds.take(n)):\n    r = i \/\/ cols\n    c = i % cols\n    ax = axes[r][c]\n    ax.plot(audio.numpy())\n    ax.set_yticks(np.arange(-1.2, 1.2, 0.2))\n    label = recording_id.numpy().decode('utf-8')\n    ax.set_title(label)\n\nplt.show()","462895dd":"def get_spectrogram(audio):\n    waveform = tf.cast(audio, tf.float32)\n    spectrogram = tf.signal.stft(waveform, frame_length=255, frame_step=128)\n    spectrogram = tf.abs(spectrogram)\n    return spectrogram","1c837875":"# training data \n\nfor waveform, label in training_ds.take(1):\n    label = label.numpy().decode('utf-8')\n    spectrogram = get_spectrogram(waveform)\n\nprint('Label:', label)\nprint('Waveform shape:', waveform.shape)\nprint('Spectrogram shape:', spectrogram.shape)\nprint('Audio playback')\ndisplay.display(display.Audio(waveform, rate=16000))","54c9b52c":"# test data\n\nfor waveform, recording_id in test_ds.take(1):\n    recording_id = recording_id.numpy().decode('utf-8')\n    spectrogram = get_spectrogram(waveform)\n\nprint('Recording ID:', recording_id)\nprint('Waveform shape:', waveform.shape)\nprint('Spectrogram shape:', spectrogram.shape)\nprint('Audio playback')\ndisplay.display(display.Audio(waveform, rate=16000))","96a3db93":"def plot_spectrogram(spectrogram, ax):\n    log_spec = np.log(spectrogram.T)\n    height = log_spec.shape[0]\n    X = np.arange(2880000, step=128)  \n    Y = range(height)  \n    ax.pcolormesh(X, Y, log_spec)","4bd05313":"# training data\n\nfig, axes = plt.subplots(2, figsize=(12, 8))\ntimescale = np.arange(waveform.shape[0])\naxes[0].plot(timescale, waveform.numpy())\naxes[0].set_title('Waveform')\naxes[0].set_xlim([0, 2880000])\nplot_spectrogram(spectrogram.numpy(), axes[1])  \naxes[1].set_title('Spectrogram')\nplt.show()","176ba1f7":"def get_spectrogram_and_label_id(audio, label):\n    spectrogram = get_spectrogram(audio)\n    label_id = label\n    return spectrogram, label_id\n\nspectrogram_ds = training_ds.map(get_spectrogram_and_label_id)","8bd1ae01":"rows = 3\ncols = 3\nn = rows*cols\nfig, axes = plt.subplots(rows, cols, figsize=(10, 10))\nfor i, (spectrogram, label_id) in enumerate(spectrogram_ds.take(n)):\n    r = i \/\/ cols\n    c = i % cols\n    ax = axes[r][c]\n    plot_spectrogram(np.squeeze(spectrogram.numpy()), ax)\n    label = label_id.numpy().decode('utf-8')\n    ax.set_title(label)\n\nplt.show()","f3cfee0c":"# Data structures","57c795a6":"# Visualizing spectrograms","5e189b9d":"# Introduction\n\nI wanted to take some of the exploratory data analysis (EDA) done in the TensorFlow **[Simple audio recognition: Recognizing keywords](https:\/\/www.tensorflow.org\/tutorials\/audio\/simple_audio#top_of_page)** tutorial and apply it to the **[TFRecords](https:\/\/www.kaggle.com\/ryanholbrook\/tfrecords-basics)** that are available in this competition.\n\nPlease feel free to copy and edit this notebook and use it in your own analyses. And be sure to tag me if you extend this notebook and build your model using **[Tensor Processing Units (TPUs)](https:\/\/www.kaggle.com\/docs\/tpu)** - I'd love to see what you create!","e79031aa":"# Graphing waveforms","f263d77a":"# Waveform shape, spectogram shape, and audio playback","0c3a1157":"# Set up environment","48cf610d":"# Dataset functions"}}