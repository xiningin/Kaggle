{"cell_type":{"7eddeb8d":"code","01451064":"code","953f74e5":"code","0717fdb9":"code","9aa0bf9d":"code","c8096d21":"code","3bc11a0e":"code","a9f78bfe":"code","60a969e6":"code","18df7a46":"code","62de3b73":"code","41f81f7b":"code","7ed1351c":"code","6993b495":"code","2e35c71d":"code","dbb72323":"code","ee0c0047":"code","3f47f967":"code","6f1a5e96":"code","857f9902":"code","2d1f436d":"code","498b5e90":"code","2870a735":"markdown","5f731f5a":"markdown","b61b572c":"markdown","cb8355ef":"markdown","c7be2aa4":"markdown","51132fa9":"markdown","f6eeef71":"markdown","0c2c36a1":"markdown","164fad95":"markdown","5e62d7b8":"markdown","2d4f6b9a":"markdown","624a781b":"markdown","3cd9882c":"markdown","4cac7263":"markdown","18a44b86":"markdown","a5a76c18":"markdown","41883724":"markdown","583fc90a":"markdown","176f1eac":"markdown","d0d17dff":"markdown","aecc44cc":"markdown","88d447f7":"markdown","07be7dc0":"markdown","024889d8":"markdown"},"source":{"7eddeb8d":"import h2o\nimport time\nimport seaborn\nimport itertools\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom h2o.estimators.glm import H2OGeneralizedLinearEstimator\nfrom h2o.estimators.gbm import H2OGradientBoostingEstimator\nfrom h2o.estimators.random_forest import H2ORandomForestEstimator\n\n%matplotlib inline","01451064":"h2o.init()","953f74e5":"diabetes_df = h2o.import_file(\"..\/input\/diabetes.csv\", destination_frame=\"diabetes_df\")","0717fdb9":"diabetes_df.describe()","9aa0bf9d":"for col in diabetes_df.columns:\n    diabetes_df[col].hist()","c8096d21":"plt.figure(figsize=(10,10))\ncorr = diabetes_df.cor().as_data_frame()\ncorr.index = diabetes_df.columns\nsns.heatmap(corr, annot = True, cmap='RdYlGn', vmin=-1, vmax=1)\nplt.title(\"Correlation Heatmap\", fontsize=16)\nplt.show()","3bc11a0e":"train, valid, test = diabetes_df.split_frame(ratios=[0.6,0.2], seed=1234)\nresponse = \"Outcome\"\ntrain[response] = train[response].asfactor()\nvalid[response] = valid[response].asfactor()\ntest[response] = test[response].asfactor()\nprint(\"Number of rows in train, valid and test set : \", train.shape[0], valid.shape[0], test.shape[0])","a9f78bfe":"predictors = diabetes_df.columns[:-1]\ngbm = H2OGradientBoostingEstimator()\ngbm.train(x=predictors, y=response, training_frame=train)","60a969e6":"print(gbm)","18df7a46":"perf = gbm.model_performance(valid)\nprint(perf)","62de3b73":"gbm_tune = H2OGradientBoostingEstimator(\n    ntrees = 3000,\n    learn_rate = 0.01,\n    stopping_rounds = 20,\n    stopping_metric = \"AUC\",\n    col_sample_rate = 0.7,\n    sample_rate = 0.7,\n    seed = 1234\n)      \ngbm_tune.train(x=predictors, y=response, training_frame=train, validation_frame=valid)","41f81f7b":"gbm_tune.model_performance(valid).auc()","7ed1351c":"from h2o.grid.grid_search import H2OGridSearch\n\ngbm_grid = H2OGradientBoostingEstimator(\n    ntrees = 3000,\n    learn_rate = 0.01,\n    stopping_rounds = 20,\n    stopping_metric = \"AUC\",\n    col_sample_rate = 0.7,\n    sample_rate = 0.7,\n    seed = 1234\n) \n\nhyper_params = {'max_depth':[4,6,8,10,12]}\ngrid = H2OGridSearch(gbm_grid, hyper_params,\n                         grid_id='depth_grid',\n                         search_criteria={'strategy': \"Cartesian\"})\n#Train grid search\ngrid.train(x=predictors, \n           y=response,\n           training_frame=train,\n           validation_frame=valid)","6993b495":"print(grid)","2e35c71d":"sorted_grid = grid.get_grid(sort_by='auc',decreasing=True)\nprint(sorted_grid)","dbb72323":"cv_gbm = H2OGradientBoostingEstimator(\n    ntrees = 3000,\n    learn_rate = 0.05,\n    stopping_rounds = 20,\n    stopping_metric = \"AUC\",\n    nfolds=4, \n    seed=2018)\ncv_gbm.train(x = predictors, y = response, training_frame = train, validation_frame=valid)\ncv_summary = cv_gbm.cross_validation_metrics_summary().as_data_frame()\ncv_summary","ee0c0047":"cv_gbm.model_performance(valid).auc()","3f47f967":"from h2o.estimators import H2OXGBoostEstimator\n\ncv_xgb = H2OXGBoostEstimator(\n    ntrees = 3000,\n    learn_rate = 0.05,\n    stopping_rounds = 20,\n    stopping_metric = \"AUC\",\n    nfolds=4, \n    seed=2018)\ncv_xgb.train(x = predictors, y = response, training_frame = train, validation_frame=valid)\ncv_xgb.model_performance(valid).auc()","6f1a5e96":"cv_xgb.varimp_plot()","857f9902":"from h2o.automl import H2OAutoML\n\naml = H2OAutoML(max_models = 10, max_runtime_secs=100, seed = 1)\naml.train(x=predictors, y=response, training_frame=train, validation_frame=valid)","2d1f436d":"lb = aml.leaderboard\nlb","498b5e90":"metalearner = h2o.get_model(aml.leader.metalearner()['name'])\nmetalearner.std_coef_plot()","2870a735":"**Modeling and Model Tuning : **\n\nNow, let us build a baseline model using these splits. There are [multiple algorithms](http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/data-science.html) available in the H2O module. We can start with the Kaggler's favorite - GBM.\n\nPlease note that the target variable is \"Outcome\" and rest of the features are as input features. Since this is a baseline model, let us use the default parameters.","5f731f5a":"Let us now split the data into three parts - train, valid and test datasets - at a ratio of 60%, 20% and 20% respectively. We could use *split_frame()* function for the same.","b61b572c":"Not much improvement in the performance. Probably we need to tune the parameters more. \n\nGetting the variable importances plot from a model is simple too. *varimp_plot()* will get us that. Now let us check the variable importance of the XGBoost model.","cb8355ef":"Again, it is good to see the model building progress.\n\nNow let us do a *print(model_name)* to understand more about the model.","c7be2aa4":"Interestingly, there is not much change in the AUC for the top two results. Since we train on a very small sample, we might be getting this.\n\nAlso please note that, we just searched for the *max_depth* parameter. Please do a more comprehensive search for better results. Please refer to this [notebook](https:\/\/github.com\/h2oai\/h2o-3\/blob\/master\/h2o-docs\/src\/product\/tutorials\/gbm\/gbmTuning.ipynb) for more comprehensive details on finetuning. \n\n**K-Fold cross validation:**\n\nMost of the times, we will just do K-fold cross valdiation. So now let us do the same using H2O. Just setting the *nfolds* parameter in the model will do the k-fold cross validation.","51132fa9":"**Objective:**\n\nThis is an introductory notebook for people wanting to get started with [H2O](https:\/\/www.h2o.ai\/products\/h2o\/) (the open source machine learning package by H2O.ai) \n\n**What is H2O?:**\n\nH2O is a Java-based software for data modeling and general computing. There are many different perceptions of the H2O software, but the primary purpose of H2O is as a distributed (many machines), parallel (many CPUs), in memory (several hundred GBs Xmx) processing engine. \n\nWait, we as Data Scientists do not need to know Java for using H2O to build models. We can use our favorite language (Python or R) :) \n\n** H2O - Key Features:**\n\nSome of the key features \n1. Access from both R and Python\n2. Access from  web-based interface named Flow. By means of Flow, data scientists are able to import, explore, and modify datasets, play with models, verify models performances, and much more. (This is not accessible here in Kaggle Kernels)\n3. AutoML : automatic training and tuning of many models within a user-specified time-limit. \n4. Distributed, In-memory processing : In-memory processing with fast serialization between nodes and clusters to support massive datasets. \n5. Simple Deployment : Easy to deploy POJOs and MOJOs to deploy models for fast and accurate scoring in any environment, including with very large models.\n\nLet us first import the necessary modules.","f6eeef71":"**More to come. Stay tuned.!**\n\nDisclaimer : I am currently working as a Data Scientist at H2O.ai","0c2c36a1":"**Data Exploration:**\n\nNow that the initialization is done, let us first import the dataset. The command is very similar to *pandas.read_csv* and the data is stored in memory as [H2OFrame](http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-py\/docs\/frame.html)\n\nH2O supports various file formats and data sources. More detailed information can be seen [here](http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/getting-data-into-h2o.html).","164fad95":"Now let us look at the automl leaderboard.","5e62d7b8":"*describe()* gives out a lot of information. \n1. Number of rows and columns in the dataset\n2. A number of summary statistics about the dataset such as \n    * Data type of the column such as integer, categorical etc\n    * Minimum value\n    * Mean value\n    * Maximum value\n    * Standard deviation value\n    * Number of zeros in the column\n    * Number of missing values in the column\n3. A look at the top few rows\n\nNow let us look at the distribution of the individual features using *hist()* command ","2d4f6b9a":"**AutoML : Automatic Machine Learning:**\n\nFrom the [H2O AutoML page](http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html),\n\n*H2O\u2019s AutoML can be used for automating the machine learning workflow, which includes automatic training and tuning of many models within a user-specified time-limit. Stacked Ensembles will be automatically trained on collections of individual models to produce highly predictive ensemble models which, in most cases, will be the top performing models in the AutoML Leaderboard.*\n\nSo let us use the *H2OAutoML* function to do automatic machine learning. We can specify the *max_models* parameter which indicates the number of individual (or \"base\") models, and does not include the two ensemble models that are trained at the end.","624a781b":"Now let us also look at the correlation of the individual features. We can use the *cor()* function in H2OFrame for the same.","3cd9882c":"AutoML has built variety of models inlcuding GBM, GLM, Deep Learning and XRT (Extremely Randomized Trees) and then build two stacked ensemble models (the first two in the leaderboard) on top of them and the best model is a stacked ensemble. \n\nNow let us look at the contribution of the individual models for this meta learner. ","4cac7263":"So this has printed the log loss performance at various depths. If we want to look at the validation AUC, then we can use the following.","18a44b86":"We can also see the progress when the loading happens. This will be very helpful while dealing with larger datasets.\n\nAs a first step, let us have a look at the dataset. ","a5a76c18":"**References:**\n\n1. [GBM tuning tutorial for python](https:\/\/github.com\/h2oai\/h2o-3\/blob\/master\/h2o-docs\/src\/product\/tutorials\/gbm\/gbmTuning.ipynb)\n2. [Machine Learning with H2O](https:\/\/dzone.com\/articles\/machine-learning-with-h2o-hands-on-guide-for-data)\n3. [XGBoost in H2O platform](https:\/\/blog.h2o.ai\/2017\/06\/xgboost-in-h2o-machine-learning-platform\/)\n4. [AutoML H2O Demo](https:\/\/github.com\/h2oai\/h2o-tutorials\/blob\/master\/h2o-world-2017\/automl\/Python\/automl_binary_classification_product_backorders.ipynb)\n5. [AutoML Docs](http:\/\/docs.h2o.ai\/h2o\/latest-stable\/h2o-docs\/automl.html)\n","41883724":"So GBM is the topmost contributor to the ensemble followed by GLM and DL. ","583fc90a":"Now let us check the validation auc to check the performance.","176f1eac":"We are getting similar performance (0.8 valid AUC) using this new model with early stopping too. \n\n**Grid Search:**\n\nNow let us do grid search to find the best paramters for GBM model. ","d0d17dff":"Now that is quite a bit of information. We can look at them individually.\n1. First, we get the name of the model and a key to acces the model ( key is not much useful for us I guess )\n2. Error metrics on the train data like log-loss, mean per class error, AUC, Gini, MSE, RMSE\n3. Confusion matrix for max F1 threshold\n4. Threshold values for different metrics\n5. Gains \/ Lift table \n6. Scoring history - information on how the metrics changed in each of the epochs\n7. Feature importance\n\nOkay. I heard you. How can we use the metrics of train set (as we actually trained on this dataset). We need to evaluate them from the valid set. We can use the *model_performance()* function for the same. We can then print the metrics. ","aecc44cc":"Once the module in imported, the first step is to initialize the h2o module. \n\nThe *h2o.init()* command is pretty smart and does a lot of things. First, an attempt is made to search for an existing H2O instance being started already, before starting a new one. When none is found automatically or specified manually with argument available, a new instance of H2O is started. \n\nDuring startup, H2O is going to print some useful information. Version of the Python it is running on, H2O\u2019s version, how to connect to H2O\u2019s Flow interface or where error logs reside, just to name a few.","88d447f7":"**XGBoost:**\n\nRecently H2O has also added the XGBoost version of GBM into its kitty. Now let us see how to use the XGBoost model in H2O. We follow the same code convention as that of GBM except that we will use *H2OXGBoostEstimator* function. ","07be7dc0":"So using our baseline model, we are getting about 0.8 auc in valid set and 0.98 auc in train set. Similarly, log loss is 0.53 in valid set and 0.21 in train set. \n\nNow we can use the validation set to tune our parameters. We can use the early stopping to find the number of iterations to train similar to other GBM implementations. We can set some random values for the parameters to start with. Please note that, we have added a new *validation_frame* parameter in this one compared to the previous one while training. ","024889d8":"Now let us test the performance on the valid set just like before. "}}