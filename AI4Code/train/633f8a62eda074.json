{"cell_type":{"1e8773ae":"code","a7d9a106":"code","e69557b4":"code","45c2d916":"code","683186fd":"code","be78beeb":"code","10f92d5f":"code","bbc0da21":"code","6343d683":"code","09c307bd":"code","6947ac3b":"code","9bf5f3f0":"code","4aeeea2c":"code","5af08df6":"code","f2f50e23":"code","ab5aa762":"code","194b6adc":"code","68c2140a":"code","2014b170":"code","d7fd4cc8":"code","c30eab9b":"markdown","e03d1e95":"markdown","04dc2d8f":"markdown","7bfe14ea":"markdown","8ad6c901":"markdown","b65b147f":"markdown","ac3b0310":"markdown","f305b762":"markdown","a7a053f6":"markdown","f09eabc6":"markdown","ba686782":"markdown","09dd08ab":"markdown","47b49d22":"markdown","be9c25e8":"markdown","e0f54a10":"markdown","68c15f82":"markdown","20f02f4a":"markdown","5fe590e8":"markdown","7c6fd305":"markdown","1fcd04f8":"markdown","344c815c":"markdown","bd820cad":"markdown"},"source":{"1e8773ae":"# Supressing the warning messages\nimport warnings\nwarnings.filterwarnings('ignore')","a7d9a106":"# for basic mathematics operation \nimport numpy as np\nimport pandas as pd\nfrom pandas import plotting\nfrom sklearn.metrics import confusion_matrix\n\n# for visualizations\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nplt.style.use('fivethirtyeight')\nfrom sklearn.metrics import ConfusionMatrixDisplay\nfrom mlxtend.plotting import plot_confusion_matrix\nimport matplotlib.pyplot as plt\n\n\n# for path\nimport os","e69557b4":"dataset_path = '\/kaggle\/input\/dry-beans-classification-iti-ai-pro-intake01\/'\ndf = pd.read_csv(os.path.join(dataset_path, 'train.csv'))\nprint(\"The shape of the dataset is {}.\\n\\n\".format(df.shape))","45c2d916":"# Shape or Size\ndf.shape","683186fd":"#Dataset information\ndf.info()","be78beeb":"df['y'].value_counts()","10f92d5f":"corr_matrix = df.corr()\n\nplt.figure(figsize=(15,15))\nplt.title('Correlation Heatmap of Beans Dataset')\na = sns.heatmap(corr_matrix, square=True, annot=True, fmt='.2f', linecolor='black')\na.set_xticklabels(a.get_xticklabels(), rotation=30)\na.set_yticklabels(a.get_yticklabels(), rotation=30)\nplt.show()","bbc0da21":"Strongly_corr_features = df[[\"Area\",\"Perimeter\",\"AspectRation\",\"Eccentricity\",\"roundness\",\"Compactness\",\"y\"]]\nStrongly_corr_features.head()\nsns.set_theme(style=\"whitegrid\")\nsns.pairplot(Strongly_corr_features, hue=\"y\")","6343d683":"sns.boxplot(x=\"y\", y=\"MajorAxisLength\", data=df)","09c307bd":"sns.boxplot(x=\"y\", y=\"Perimeter\", data=df)","6947ac3b":"from sklearn.model_selection import cross_val_score,train_test_split\n\ndf_train, df_val = train_test_split(df, test_size=0.2, random_state=42)\n\nX_train = df_train.drop(columns=['ID', 'y' , 'ShapeFactor3','Compactness','AspectRation','Area','MajorAxisLength','MinorAxisLength','ConvexArea','EquivDiameter','ShapeFactor1' ])\ny_train = df_train['y']\n\nX_val = df_val.drop(columns=['ID', 'y', 'ShapeFactor3','Compactness','AspectRation','Area','MajorAxisLength','MinorAxisLength','ConvexArea','EquivDiameter','ShapeFactor1' ])\ny_val = df_val['y']\n\n\nprint(X_train.shape,X_val.shape,y_train.shape,y_val.shape)","9bf5f3f0":"df.describe().T","4aeeea2c":"from sklearn.preprocessing import MinMaxScaler\nscaler = MinMaxScaler()\ndf_scaled = scaler.fit_transform(df.drop(columns = ['ID','y']))\ndf_scaled = pd.DataFrame(df_scaled , columns= df.columns.difference(['ID','y']))\ndf_scaled.describe().T","5af08df6":"from sklearn.tree import DecisionTreeClassifier\n\n# Create an instance of the classifier\nclassifier = DecisionTreeClassifier(random_state=42)\n\n# Train the classifier\nclassifier = classifier.fit(X_train, y_train)\ny_pred = classifier.predict(X_val)","f2f50e23":"# 1st way to calculate Accuracy \n\nfrom sklearn.metrics import accuracy_score\n\naccuracy = accuracy_score(y_val, y_pred)\n\nprint ( 'Accuracy = ', accuracy)","ab5aa762":"# 2nd way to calculate Accuracy \n# calc Accuracy using confusion_matrix paramaters\n\ncm = confusion_matrix(y_val, y_pred)\n\ndef accuracy(confusion_matrix):\n    diagonal_sum = confusion_matrix.trace()\n    sum_of_all_elements = confusion_matrix.sum()\n    return diagonal_sum \/ sum_of_all_elements \n\naccuracy(cm)","194b6adc":"# Classes\nclasses  = np.array([\"SEKER\",\"BARBUNYA\",\"BOMBAY\",\"CALI\",\"DERMASON\",\"HOROZ\",\"SIRA\"])\n\nfigure, ax = plot_confusion_matrix(conf_mat = cm,\n                                   class_names = classes,\n                                   show_absolute = False,\n                                   show_normed = True,\n                                   colorbar = True)\n\nplt.show()","68c2140a":"dataset_path = '\/kaggle\/input\/dry-beans-classification-iti-ai-pro-intake01\/'\ndf_test = pd.read_csv(os.path.join(dataset_path, 'test.csv'))\ndf_test.head()","2014b170":"# Predicting y of Test data\n\n# Step 1 - applying scalling\nX_test_scaled = scaler.fit_transform(df_test.drop(columns = ['ID']))\nX_test_scaled = pd.DataFrame(X_test_scaled , columns= df_test.columns.difference(['ID']))\n\n# Step 2- removing unimportant features\nX_test_scaled = X_test_scaled.drop(columns=['ShapeFactor3','Compactness','AspectRation','Area','MajorAxisLength','MinorAxisLength','ConvexArea','EquivDiameter','ShapeFactor1'])\n\n\ny_test_predicted = classifier.predict(X_test_scaled)\n\n# add y column to the test data\ndf_test['y'] = y_test_predicted\n\ndf_test.head()","d7fd4cc8":"df_test[['ID', 'y']].to_csv('\/kaggle\/working\/submission.csv', index=False)","c30eab9b":"**Number of instancs for each class , Dermason has the highest number.**","e03d1e95":"# 4- Feature Engineering","04dc2d8f":"## Model Prediction \n\nWe have built a model and we'd like to submit our predictions on the test set! In order to do that, we'll load the test set, predict the class and save the submission file. \n","7bfe14ea":"# 3- Explainatry Data Analysis - EDA","8ad6c901":"**Next step will be Detecting how Beans classes can be effected by many features ..**","b65b147f":"# Data Visualization\n**Heatmap**","ac3b0310":"\n**Accuracy : is one of the simplest form of evaluation metrics , it means that how many data points are predicted correctly**\n\n![image-2.png](attachment:image-2.png)\n\n**The accuracy can be defined as the percentage of correctly classified instances (TP + TN)\/(TP + TN + FP + FN). where TP, FN, FP and TN represent the number of true positives, false negatives, false positives and true negatives, respectively.**\n","f305b762":"# 1: Import Libraries","a7a053f6":"**The shape of the dataset is (10834, 18) , containing 17 Features beside (Y \/ Bean Class)**","f09eabc6":"# Dataset Information\n\nGiven a set of features extracted from the shape of the beans in images and  it's required to predict the class of a bean given some features about its shape.\nThere are 7 bean types in this dataset.\n\n**Data fields**\n- ID - an ID for this instance\n- Area - (A), The area of a bean zone and the number of pixels within its boundaries.\n- Perimeter - (P), Bean circumference is defined as the length of its border.\n- MajorAxisLength - (L), The distance between the ends of the longest line that can be drawn from a bean.\n- MinorAxisLength - (l), The longest line that can be drawn from the bean while standing perpendicular to the main axis.\n- AspectRatio - (K), Defines the relationship between L and l.\n- Eccentricity - (Ec), Eccentricity of the ellipse having the same moments as the region.\n- ConvexArea - (C), Number of pixels in the smallest convex polygon that can contain the area of a bean seed.\n- EquivDiameter - (Ed), The diameter of a circle having the same area as a bean seed area.\n- Extent - (Ex), The ratio of the pixels in the bounding box to the bean area.\n- Solidity - (S), Also known as convexity. The ratio of the pixels in the convex shell to those found in beans.\n- Roundness - (R), Calculated with the following formula: (4piA)\/(P^2)\n- Compactness - (CO), Measures the roundness of an object: Ed\/L\n- ShapeFactor1 - (SF1)\n- ShapeFactor2 - (SF2)\n- ShapeFactor3 - (SF3)\n- ShapeFactor4 - (SF4)\n- y - the class of the bean. It can be any of BARBUNYA, SIRA, HOROZ, DERMASON, CALI, BOMBAY, and SEKER.\n","ba686782":"<img src= \"https:\/\/www.thespruceeats.com\/thmb\/eeIti36pfkoNBaipXrTHLjIv5YA=\/1888x1416\/smart\/filters:no_upscale()\/DriedBeans-56f6c2c43df78c78418c3b46.jpg\" alt =\"Titanic\" style='width: 800px;height:400px'>","09dd08ab":"- A perimeter is  a path that encompasses\/surrounds\/outlines a shape or its length. 'Wikipedia'\n- The above graph shows that (BOMBAY) has the highest perimeter","47b49d22":"From this correlation matrix we can exctract features that are strongly correlated like : \n- Area\n- Perimeter\n- MajorAxisLength\n- MinorAxisLength\n- ConvexArea\n- EquivDiameter\n- ShapeFactor1\n\nFeatures to be drobbed : \n\n- ShapeFactor3\n- Compactness\n- AspectRation\n- Area\n- MajorAxisLength\n- MinorAxisLength\n- ConvexArea\n- EquivDiameter\n- ShapeFactor1","be9c25e8":"\nLet's train a model with the data!","e0f54a10":"**Features like:** (Eccentricity , Extent ,Solidity ,roundness ,Compactness ,and shapeFactor1,2,3,4 ) **ranges between (0 and 1)**\n\n**On the other side , there are other features like:**\n- (Area) ranges between (20420 and 254616 )\n- (ConvexArea) ranges between (20684 and 263261 )\n\nWhen a dataset has values of different columns at different scales, it gets tough to analyze the trends and patterns , so we need to make sure that all the columns have a significant difference in their scales, and they can be modified in such a way that all those values fall into the same scale. This process is called Scaling.","68c15f82":"# 2: Reading the Dataset","20f02f4a":"### Data scaling using MinMaxScaler","5fe590e8":"**The features are all numerical but (Y \/ Bean Class)**\n<br>\n**No Nullable Data**","7c6fd305":"## Model Training","1fcd04f8":"## Data Splitting\n\nNow it's time to split the dataset for the training step. Typically the dataset is split into 3 subsets, namely, the training, validation and test sets. In our case, the test set is already predefined. So we'll split the \"training\" set into training and validation sets with 0.8:0.2 ratio. \n","344c815c":"**From the graph above, Linear and log relations can be detected.**","bd820cad":"# Submission File Generation"}}