{"cell_type":{"c57bf562":"code","bb1df051":"code","bbf853bf":"code","48839c0b":"code","759c3898":"code","619bec60":"code","9f65cf4b":"code","fa166a56":"code","1c6a3b9d":"code","bf6dbbbb":"code","82054f0d":"code","695bdca0":"code","eba7d018":"code","13c83281":"code","a71db605":"code","705acd9c":"code","24465037":"code","6b82f9a1":"code","93de4f89":"code","abf0337f":"code","624e1b5d":"code","2e0835c9":"code","7e43f140":"code","cc293b32":"code","dd434e76":"code","410e60d0":"code","b19db342":"code","3eb10254":"code","c07958bb":"code","c103849e":"code","e20cc5a0":"code","f3da7b21":"code","b5be37c1":"code","d218c44a":"code","abfc63f0":"code","be62d8bc":"code","e5afbb79":"code","674cfbc6":"code","df4c993a":"code","cdd5bf8b":"code","20d9811e":"code","533f17e8":"code","42543466":"code","05af6cb4":"code","140813b9":"code","5a9034e2":"code","9bf5593d":"code","92c0eba9":"code","05be64c2":"code","b333eb97":"code","613179db":"code","2cf57324":"code","65c6f33e":"code","37a004ad":"code","cefffbd7":"code","0765dda1":"code","9c7119d5":"code","068ac5ba":"code","b0e63155":"code","757a9a82":"code","f88aaa14":"code","99d4e874":"code","62bb4268":"code","9eb20147":"code","2511da5e":"code","f68c0f23":"code","2b85bffc":"code","62f39d5d":"code","a5896847":"code","c0683274":"code","46a58f2f":"code","f7318373":"code","67c509d3":"code","07cab463":"code","e6dbdda0":"code","026e6595":"code","1dba371b":"code","84209cb4":"code","146fdafb":"code","4dfe889e":"code","8a3ebde8":"code","055c2f2e":"markdown","6a86fb52":"markdown","6b75ee9a":"markdown","353c7d61":"markdown","835ade70":"markdown","f1066bab":"markdown","8f46f820":"markdown","851a3d23":"markdown","aaa8c8b3":"markdown","56bde18d":"markdown","8547d35d":"markdown","70661909":"markdown","77e5879f":"markdown","80cb242a":"markdown","c5852e94":"markdown","a5313f6f":"markdown","e574dd74":"markdown","a239275b":"markdown","3f1e8ea3":"markdown","0523bf34":"markdown","ecb1ec5a":"markdown","5df7a221":"markdown","c98f7ca0":"markdown","eb8747d1":"markdown","21406c54":"markdown","5a728f11":"markdown","6795a922":"markdown","c25998b5":"markdown","ab48b780":"markdown","e3c429fb":"markdown","f1fcf5ef":"markdown","54bfbc4d":"markdown","601f4918":"markdown","7ba01fe0":"markdown","073570d9":"markdown","7025e9fb":"markdown","34aafb8e":"markdown","6a09fa3b":"markdown","440a684a":"markdown","8ec13553":"markdown","8168550f":"markdown","730e8e63":"markdown","674197d0":"markdown","dd032700":"markdown","91a0f958":"markdown","3d14d87a":"markdown","212dd673":"markdown","c87c5667":"markdown","8ec5c6ad":"markdown","d3dc83b1":"markdown","be405a4b":"markdown","eeb729ec":"markdown","1ddb73af":"markdown","c43ccb7e":"markdown"},"source":{"c57bf562":"import torch\nimport torch.nn.functional as F\nimport torchvision\nimport torchvision.transforms as transforms\n\nfrom PIL import Image\n\nimport numpy as np\n\nimport copy\nimport pickle\nimport os\n\nimport matplotlib.pyplot as plt\n\nfrom tqdm.notebook import tqdm","bb1df051":"!git clone https:\/\/github.com\/NVlabs\/stylegan2-ada-pytorch.git","bbf853bf":"%cd stylegan2-ada-pytorch","48839c0b":"!ls","759c3898":"!ls ..\/..\/input","619bec60":"with open('..\/..\/input\/afhq-dog-pretrained\/afhqdog.pkl', 'rb') as f:\n    G = pickle.load(f)['G_ema'].cuda()","9f65cf4b":"z = torch.randn([1, G.z_dim]).cuda()\nc = None\nimg = G(z,c)","fa166a56":"img.size()","1c6a3b9d":"torch.min(img)","bf6dbbbb":"img.squeeze(0).size()","82054f0d":"plt.imshow((img+0.5).cpu().squeeze(0).permute(1,2,0))\nplt.axis('off')\nplt.show();","695bdca0":"g_mapping = G.mapping\ng_synthesis = G.synthesis","eba7d018":"g_mapping","13c83281":"g_synthesis","a71db605":"z = torch.randn([1, G.z_dim]).cuda()\nz.size()","705acd9c":"w = g_mapping(z, None)\nw.size()","24465037":"img = g_synthesis(w)\nimg = img.cpu()\nimg = img.squeeze(0)\nimg.size()","6b82f9a1":"plt.imshow((img+0.3).permute(1,2,0))\nplt.axis('off')\nplt.show();","93de4f89":"z1 = torch.randn([1, G.z_dim]).cuda()\nz2 = torch.randn([1, G.z_dim]).cuda()\n\nw1 = g_mapping(z1, None)\nw2 = g_mapping(z2, None)","abf0337f":"img1 = g_synthesis(w1)\nimg2 = g_synthesis(w2)\n\nimg1 = img1\nimg1 = img1.squeeze().permute(1,2,0)\n\nimg2 = img2\nimg2 = img2.squeeze().permute(1,2,0)","624e1b5d":"imgs = torch.cat([img1,img2], axis=1).cpu()\nimgs.size()","2e0835c9":"plt.imshow(imgs+0.85) # Increase the brightness by adding 0.85 to the pixel values\nplt.axis('off')\nplt.show()","7e43f140":"img_group = []\nlin = np.linspace(0, 1, 10)\nprint(*lin) # unpack the list","cc293b32":"with torch.no_grad():\n    for i in lin:\n        w = ((1-i) * w1) + (i * w2) # style transfrom in for-loop : w1 --> w2\n        result = g_synthesis(w)\n        result = result.squeeze()\n        img_group.append(result)","dd434e76":"img_group = torch.cat(img_group).cpu()\nimg_group.size()","410e60d0":"img_group = img_group.view(10,3,512,512)\nimg_group.size()","b19db342":"grid_img = torchvision.utils.make_grid(img_group, nrow=5)\ngrid_img.size()","3eb10254":"plt.figure(figsize=(16,6))\nplt.imshow(grid_img.permute(1,2,0) +0.85) # The brightness value was also given by 0.85\nplt.axis('off')\nplt.show()","c07958bb":"path = '..\/..\/input\/animal-faces\/afhq\/train\/dog\/'","c103849e":"os.listdir(path)[:10]","e20cc5a0":"sample_flickr_dog_path = path+os.listdir(path)[2]\ntarget_img = Image.open(sample_flickr_dog_path)\ntarget_img","f3da7b21":"target_img.size","b5be37c1":"target_uint8 = np.array(target_img, dtype=np.uint8)\ntarget_uint8.shape","d218c44a":"target_uint8[0]","abfc63f0":"device = torch.device('cuda')\nG_eval = copy.deepcopy(G).eval().requires_grad_(False).to(device) # use g as the evaluation mode (do not track gradients)","be62d8bc":"z_samples = np.random.randn(10000, G_eval.z_dim) # G_eval.z_dim == 512, (10000,512)\nw_samples = G_eval.mapping(torch.from_numpy(z_samples).to(device), None)\nw_samples.size()","e5afbb79":"w_samples = w_samples[:,:1,:].cpu().numpy().astype(np.float32)\nw_samples","674cfbc6":"w_samples.shape","df4c993a":"w_avg = np.mean(w_samples, axis=0, keepdims=True) \nw_avg.shape # mean of w_samples","cdd5bf8b":"w_std = (np.sum((w_samples - w_avg)**2)\/10000)**0.5\nw_std","20d9811e":"noise_bufs = { name: buf for (name, buf) in G.synthesis.named_buffers() if 'noise_const' in name }","533f17e8":"import dnnlib","42543466":"url = 'https:\/\/nvlabs-fi-cdn.nvidia.com\/stylegan2-ada-pytorch\/pretrained\/metrics\/vgg16.pt'\nwith dnnlib.util.open_url(url) as f:\n    vgg16 = torch.jit.load(f).eval().to(device)","05af6cb4":"target = torch.tensor(target_uint8.transpose([2,0,1]), device=device)\ntarget.size()","140813b9":"target = target.unsqueeze(0).to(device).to(torch.float32)\ntarget.size()","5a9034e2":"target = F.interpolate(target, size=(256,256), mode='area') # Resize to pass through the vgg16 network.\ntarget.size()","9bf5593d":"target_features = vgg16(target, resize_images=False, return_lpips=True)\ntarget_features","92c0eba9":"target_features.size()","05be64c2":"# suggested default hyper-parameter (official)\nnum_steps = 1000\ninitial_learning_rate = 0.1\n# ========================================= #\n\nw_opt = torch.tensor(w_avg, dtype=torch.float32, device=device, requires_grad=True) # pylint: disable=not-callable\nw_out = torch.zeros([num_steps] + list(w_opt.shape[1:]), dtype=torch.float32, device=device)\noptimizer = torch.optim.Adam([w_opt] + list(noise_bufs.values()), betas=(0.9, 0.999), lr=initial_learning_rate)\n\n# Init noise.\nfor buf in noise_bufs.values():\n    buf[:] = torch.randn_like(buf)\n    buf.requires_grad = True","b333eb97":"num_steps = 1000\n# suggested default hyper-parameter (official)\nlr_rampdown_length = 0.25\nlr_rampup_length = 0.05\ninitial_noise_factor = 0.05\nnoise_ramp_length = 0.75\nregularize_noise_weight = 1e5\n# ========================================= #\n\nfor step in tqdm(range(num_steps)):\n    # Learning rate schedule.\n    t = step \/ num_steps\n    w_noise_scale = w_std * initial_noise_factor * max(0.0, 1.0 - t \/ noise_ramp_length) ** 2\n    lr_ramp = min(1.0, (1.0 - t) \/ lr_rampdown_length)\n    lr_ramp = 0.5 - 0.5 * np.cos(lr_ramp * np.pi)\n    lr_ramp = lr_ramp * min(1.0, t \/ lr_rampup_length)\n    lr = initial_learning_rate * lr_ramp\n    for param_group in optimizer.param_groups:\n            param_group['lr'] = lr\n            \n    # Synthesize image from opt_w\n    w_noise = torch.randn_like(w_opt) * w_noise_scale\n    ws = (w_opt + w_noise).repeat([1, G.mapping.num_ws, 1])\n    synth_images = G.synthesis(ws, noise_mode='const')\n    \n    # Downsample image to 256x256 if it's larger than that. VGG was built for 224x224 images.\n    synth_images = (synth_images + 1) * (255\/2)\n    if synth_images.shape[2] > 256:\n        synth_images = F.interpolate(synth_images, size=(256, 256), mode='area')\n        \n    # Features for synth images.\n    synth_features = vgg16(synth_images, resize_images=False, return_lpips=True)\n    dist = (target_features - synth_features).square().sum() # Calculate the difference between two feature maps (target vs synth) generated through vgg. \n                                                             # This is the point of projection.\n    # Noise regularization.\n    reg_loss = 0.0\n    for v in noise_bufs.values():\n        noise = v[None,None,:,:] # must be [1,1,H,W] for F.avg_pool2d()\n        while True:\n            reg_loss += (noise*torch.roll(noise, shifts=1, dims=3)).mean()**2\n            reg_loss += (noise*torch.roll(noise, shifts=1, dims=2)).mean()**2\n            if noise.shape[2] <= 8:\n                break\n            noise = F.avg_pool2d(noise, kernel_size=2)\n    loss = dist + reg_loss * regularize_noise_weight\n    \n    # Step\n    optimizer.zero_grad(set_to_none=True)\n    loss.backward()\n    optimizer.step()\n    print(f'step {step+1:>4d}\/{num_steps}: dist {dist:<4.2f} loss {float(loss):<5.2f}')\n    \n    # Save projected W for each optimization step.\n    w_out[step] = w_opt.detach()[0]\n\n    # Normalize noise.\n    with torch.no_grad():\n        for buf in noise_bufs.values():\n            buf -= buf.mean()\n            buf *= buf.square().mean().rsqrt()","613179db":"w_out.size()","2cf57324":"w_out.repeat([1, G.mapping.num_ws, 1]).size()","65c6f33e":"projected_w_steps = w_out.repeat([1, G.mapping.num_ws, 1])\nprojected_w = projected_w_steps[-1]\nprint(projected_w.size())\nprint(projected_w)","37a004ad":"np_w = projected_w.cpu().numpy()\nnp.save('.\/projected_w', np_w)","cefffbd7":"synth_image = G.synthesis(projected_w.unsqueeze(0), noise_mode='const')\nsynth_image = (synth_image + 1) * (255\/2)\nsynth_image = synth_image.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()","0765dda1":"synth_image.shape","9c7119d5":"plt.imshow(np.concatenate((synth_image, target_uint8), axis=1))\nplt.axis('off')\nplt.title('synth <--  vs  -->  target')\nplt.show()","068ac5ba":"target1 = Image.open(path+'flickr_dog_000052.jpg')\ntarget1","b0e63155":"target2 = Image.open(path+'flickr_dog_000064.jpg')\ntarget2","757a9a82":"projected_w1 = np.load('..\/..\/input\/afhq-dog-pretrained\/projected_w1.npy')\nprojected_w1 = torch.from_numpy(projected_w1).to(device)\nprojected_w1.size()","f88aaa14":"projected_w2 = np.load('..\/..\/input\/afhq-dog-pretrained\/projected_w2.npy')\nprojected_w2 = torch.from_numpy(projected_w2).to(device)\nprojected_w2.size()","99d4e874":"synth1 = G.synthesis(projected_w1.unsqueeze(0), noise_mode='const')\nsynth1 = (synth1 + 1) * (255\/2)\nsynth1 = synth1.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()","62bb4268":"plt.imshow(synth1)\nplt.axis('off')\nplt.show()","9eb20147":"synth2 = G.synthesis(projected_w2.unsqueeze(0), noise_mode='const')\nsynth2 = (synth2 + 1) * (255\/2)\nsynth2 = synth2.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()","2511da5e":"plt.imshow(synth2)\nplt.axis('off')\nplt.show()","f68c0f23":"to_smile_w = projected_w1 - projected_w2","2b85bffc":"smile = G.synthesis(to_smile_w.unsqueeze(0), noise_mode='const')\nsmile = (smile + 1) * (255\/2)\nsmile = smile.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()","62f39d5d":"plt.imshow(smile)","a5896847":"projected_test_w = np.load('..\/..\/input\/afhq-dog-pretrained\/projected_test_w.npy')\nprojected_test_w = torch.from_numpy(projected_test_w).to(device)\nprojected_test_w.size()","c0683274":"synth_test = G.synthesis(projected_test_w.unsqueeze(0), noise_mode='const')\nsynth_test = (synth_test + 1) * (255\/2)\nsynth_test = synth_test.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()","46a58f2f":"plt.imshow(synth_test)\nplt.axis('off')\nplt.show()","f7318373":"synth_test_smile = G.synthesis((projected_test_w+to_smile_w*0.38).unsqueeze(0), noise_mode='const')\nsynth_test_smile = (synth_test_smile + 1) * (255\/2)\nsynth_test_smile = synth_test_smile.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n\nplt.imshow(synth_test_smile)\nplt.axis('off')\nplt.show()","67c509d3":"projected_test_w2 = np.load('..\/..\/input\/afhq-dog-pretrained\/projected_test_w-2.npy')\nprojected_test_w2 = torch.from_numpy(projected_test_w2).to(device)\nprojected_test_w2.size()","07cab463":"synth_test2 = G.synthesis(projected_test_w2.unsqueeze(0), noise_mode='const')\nsynth_test2 = (synth_test2 + 1) * (255\/2)\nsynth_test2 = synth_test2.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()","e6dbdda0":"plt.imshow(synth_test2)\nplt.axis('off')\nplt.show()","026e6595":"synth_test_smile2 = G.synthesis((projected_test_w2+to_smile_w*0.38).unsqueeze(0), noise_mode='const')\nsynth_test_smile2 = (synth_test_smile2 + 1) * (255\/2)\nsynth_test_smile2 = synth_test_smile2.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n\nplt.imshow(synth_test_smile2)\nplt.axis('off')\nplt.show()","1dba371b":"projected_test_w3 = np.load('..\/..\/input\/afhq-dog-pretrained\/projected_test_w-3.npy')\nprojected_test_w3 = torch.from_numpy(projected_test_w3).to(device)\nprojected_test_w3.size()","84209cb4":"synth_test3 = G.synthesis(projected_test_w3.unsqueeze(0), noise_mode='const')\nsynth_test3 = (synth_test3 + 1) * (255\/2)\nsynth_test3 = synth_test3.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()","146fdafb":"plt.imshow(synth_test3)\nplt.axis('off')\nplt.show()","4dfe889e":"synth_test_smile3 = G.synthesis((projected_test_w3+to_smile_w*0.38).unsqueeze(0), noise_mode='const')\nsynth_test_smile3 = (synth_test_smile3 + 1) * (255\/2)\nsynth_test_smile3 = synth_test_smile3.permute(0, 2, 3, 1).clamp(0, 255).to(torch.uint8)[0].cpu().numpy()\n\nplt.imshow(synth_test_smile3)\nplt.axis('off')\nplt.show()","8a3ebde8":"group0 = np.concatenate((synth2, synth1))\ngroup1 = np.concatenate((synth_test, synth_test_smile))\ngroup2 = np.concatenate((synth_test2, synth_test_smile2))\ngroup3 = np.concatenate((synth_test3, synth_test_smile3))\n\nplt.figure(figsize=(16,6))\nplt.imshow(np.concatenate((group0,group1,group2,group3),axis=1))\nplt.axis('off')\nplt.show()","055c2f2e":"**5-b. Import w to insert facial expressions**","6a86fb52":"**3-b. generate 2 target fake-dog images**","6b75ee9a":"### Step 1. Initial Setting and load pre-trained model","353c7d61":"It is also possible to divide the G model into submodels.\n- we will use this method in more detail, since we plan to adjust the latent vector later","835ade70":"**1-b. clone the `stylegan2-ada`'s git repository**","f1066bab":"\u270bThank you so much for reading the really long code up to here. \n\nIf it was helpful, please upvote it so more people can see it, and leave a comment if you have any questions.\n\n\nSee you again in another kernel!","8f46f820":"**4-c. load the pre-trained Generator**","851a3d23":"**1-c. load pre-trained model that trained on `afhqdog` dataset**","aaa8c8b3":"**4-b. transform image file to numpy array**","56bde18d":"**4-h. Set optimizer and Initiate noise**","8547d35d":"The image generator `G` can be used like this :","70661909":"### Reference\n- [Paper | Training Generative Adversarial Networks with Limited Data](https:\/\/papers.nips.cc\/paper\/2020\/file\/8d30aa96e72440759f74bd2306c1fa3d-Paper.pdf)\n- [Official PyTorch implementation | NVlabs | stylegan2-ada-pytorch | github](https:\/\/github.com\/NVlabs\/stylegan2-ada-pytorch)\n- [NVlabs | pre-trained model](https:\/\/nvlabs-fi-cdn.nvidia.com\/stylegan2-ada-pytorch\/pretrained\/)\n\n![image](https:\/\/github.com\/NVlabs\/stylegan2-ada-pytorch\/raw\/main\/docs\/stylegan2-ada-teaser-1024x252.png)","77e5879f":"### Step 3. image morphing","80cb242a":"# StyleGAN2-ADA : Style Conversion - Changing A Dog's Facial Expression","c5852e94":"**4-g. Extract features for target image**","a5313f6f":"**5-a. Extract information about smiling expressions**","e574dd74":"make latent vector(space) `z` from random gaussian distribution","a239275b":"This form is not an image. Let's change the shape to `NxCxHxW` and make it an image","3f1e8ea3":"Load two images of the same breed. One should be expressionless and the other should be an image with a smiling expression.\n\nIn fact, for an accurate test (extracting the correct smiling expression), all things other than facial expressions, such as the exact same pose and background, as well as the same breed must be the same. However, in reality, it is difficult to obtain such data, so I will lightly test it with a similar image here.","0523bf34":"---\n- load second image","ecb1ec5a":"**4-i. projection(training)**","5df7a221":"### index\n\n```\nStep 1. Initial Setting and load pre-trained model\n     1-a. import libraries\n     1-b. clone the stylegan2-ada's git repository\n     1-c. load pre-trained model that trained on afhqdog dataset\nStep 2. generate sample fake-dog images\n     2-a. a quick look at the model\n     2-b. try submodels : g_mapping, g_synthesis\nStep 3. image morphing\n     3-a. create 2 random vector z and 2 intermediate latent space w\n     3-b. generate 2 target fake-dog images\n     3-c. control `z` and try image interpolation\nStep 4. Project sample image to the latent space of pretrained network\n     4-a. load a target image\n     4-b. transform image file to numpy array\n     4-c. load the pre-trained Generator\n     4-d. Compute w stats\n     4-e. Setup noise inputs\n     4-f. Load VGG16 feature detector\n     4-g. Extract features for target image\n     4-h. Set optimizer and Initiate noise\n     4-i. projection(training)\n     4-j. Compare the target image with the generated image\nStep 5. Style Conversion\n     5-a. Extract information about smiling expressions\n     5-b. Import w to insert facial expressions\n```","c98f7ca0":"- insert `smile` into first image","eb8747d1":"Let's just have fun with what the smiley expression space looks like","21406c54":"**4-d. Compute `w` stats**","5a728f11":"### Step 5. Style Conversion","6795a922":"We will use this method to get the w of the image we want inversely.","c25998b5":"- Put them all together and compare:","ab48b780":"- insert `smile` into third image","e3c429fb":"Let's take a look at the model structure to see how it is implemented","f1fcf5ef":"**4-e. Setup noise inputs**","54bfbc4d":"**2-a. A quick look at the model**","601f4918":"Image Morphing | meaning\n- (computing) The smooth transformation of one image into another\n","7ba01fe0":"**1-a. Import Libraries**","073570d9":"Now pass `w`(transformed `z`) through the synthesis network to create the `fake image`","7025e9fb":"transforms `z` into the intermediate latent space `w` (disentangled space)","34aafb8e":"I will load `w` obtained by projecting the above image in advance in my personal local environment. You can use the same code learned above. It's exactily same. (but this is too slow in kaggle..)","6a09fa3b":"**3-c. control `z` and try image interpolation**","440a684a":"we extracted the vector for the smiling expression from the first column and applied it to the images of dogs of different breeds in the second, third, and fourth columns.\n\nAs a result of the test, it was applied with a very accurate point to an image with a similar shape, and it can be expected that it will create a fairly accurate style change for the same breed.","8ec13553":"Looks good. Now, let's extract information about the smiling expression. This is really easy. just subtract","8168550f":"**4-j. Compare the target image with the generated image**","730e8e63":"---\n- and third image","674197d0":"### Step 2. generate sample fake-dog images","dd032700":"**2-b. Try submodels : g_mapping, g_synthesis**","91a0f958":"### Step 4. Project sample image to the latent space of pretrained network","3d14d87a":"---\n\nWe will implement the newest and most popular Stylegan2-ada among gans for style conversion, and practice adding expressions to animal photos.\n\nAll the codes are based on the [official pytorch implementation](https:\/\/github.com\/NVlabs\/stylegan2-ada-pytorch) and reorganized in the form of a Jupyter notebook for easy reading and practice.\n\nSo, let's start step by step by reading the table of contents","212dd673":"**4-a. load a target image**","c87c5667":"- insert `smile` into second image","8ec5c6ad":"**4-f. Load VGG16 feature detector**","d3dc83b1":"Now that `w` is loaded, let's check whether it is well expressed similarly to the original image.","be405a4b":"And since we will represent the entire image with one image, let's represent 10(`N`) images as a grid.","eeb729ec":"**3-a. create 2 random vector `z` and 2 intermediate latent space `w`**","1ddb73af":"- reducing the loss to create an image that mimics the target well.\n- Finding the input vector(w) while generating a well-replicated image.","c43ccb7e":"- load first image"}}