{"cell_type":{"940c31d8":"code","2f8dc7b0":"code","412df5bb":"code","9da0bcec":"code","6d398b82":"code","9aec8e7d":"code","79c887e5":"code","21e6d363":"code","e3672a78":"code","bd5a0dc7":"code","010350d3":"code","bac94df1":"code","9bf508f0":"code","f643aed3":"code","77b031aa":"code","ef27cca1":"code","bd51da7d":"code","4f3c8534":"code","b0da62ba":"code","26cf9416":"code","171edca9":"code","7eacc474":"code","7b92ed62":"code","ffe0bbf0":"code","446944aa":"code","a0f54ab6":"code","d6a58d6b":"code","56a94bea":"code","64ba3df7":"code","24c7cc4d":"code","4e8d00a3":"code","18d7b468":"code","a78dc30a":"code","584eb3f4":"code","0709db98":"code","697afeb2":"code","107278ae":"code","68bede98":"code","ac8ea6fe":"code","25253141":"code","5c836f8a":"code","1e4bb32b":"code","6556e013":"code","5bccc6ee":"code","33b6147a":"code","b3715a3d":"code","b3c66074":"code","f763b85f":"code","d83a760a":"code","fb36116f":"code","6d5ad166":"code","47c01bea":"code","7fd0542d":"code","8d849764":"code","b93603b8":"code","0637fae9":"code","1fc0aa5b":"code","6211a340":"code","df535b33":"code","245b9546":"code","9af95fe5":"code","a11eac2d":"code","a35b1a49":"code","208baa5b":"code","5660a6b6":"code","298eaa93":"code","0c87c37c":"code","3bf127b7":"code","e4e002fc":"code","a3032c8c":"code","e2157a61":"code","670f496d":"code","ed4b1da9":"code","b70178cd":"code","a3718c7f":"code","13b8e6f0":"code","9b56a35f":"code","5c74eee7":"code","f2bc5745":"code","34c9816e":"code","09aeee07":"markdown","f959e40c":"markdown","df68dd49":"markdown","9ad204e3":"markdown","ae55f3a5":"markdown","e9d591b5":"markdown","d8d4e7d5":"markdown","535cf271":"markdown","d3f67d44":"markdown","936d29af":"markdown","0341de9c":"markdown","71fae783":"markdown","fb991305":"markdown","3d8c14fb":"markdown","95f269f3":"markdown","fc2c3188":"markdown","e2fd246e":"markdown"},"source":{"940c31d8":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nfrom sklearn import preprocessing \nfrom category_encoders import *\nfrom sklearn.preprocessing import LabelEncoder\n%matplotlib inline\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import datasets, linear_model, metrics\nfrom sklearn.metrics import  confusion_matrix\nfrom sklearn.metrics import accuracy_score, confusion_matrix, classification_report\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import classification_report,confusion_matrix","2f8dc7b0":"df = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\ndf","412df5bb":"\n# Exploratory Data Analysis\ndef libraries():\n    global pd,np\n    import pandas as pd\n    import numpy as np\ndef load():\n    global df\n    df=pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\n    \ndef top_rows(value):\n    print('\\033[1m'+ 'displaying the', value, 'rows from top'+'\\033[0m')\n    a=df.head(value)\n    print(a,'\\n')\n    \ndef bottom_rows(value):\n    print('\\033[1m'+'displaying the', value, 'rows from bottom'+'\\033[0m')\n    b=df.tail(value)\n    print(b,'\\n')\n    \ndef rows_columns():\n    print('\\033[1m'+'Shape of the Data set'+'\\033[0m')\n    c=df.shape\n    print(c,'\\n')\n    \ndef col_names():\n    print('\\033[1m'+'Column Names in the Data set'+'\\033[0m')\n    d=df.columns\n    print(d,'\\n')\n    \ndef information():\n    print('\\033[1m'+'Quick Overview of DataSet(info)'+'\\033[0m')\n    e = df.info()\n    print(e,'\\n')\n\ndef sizee():\n    print('\\033[1m'+'No.of Elements in the DataSet'+'\\033[0m')\n    f = df.size\n    print(f,'\\n')\n\ndef ndimension():\n    print('\\033[1m'+'Dimensions in your dataframe'+'\\033[0m')\n    g = df.ndim\n    print(g,'\\n')\n    \ndef stats_summary():\n    print('\\033[1m'+'Staistical Summary of DataSet'+'\\033[0m')\n    h = df.describe()\n    print(h,'\\n')\n    \ndef null_values():\n    print('\\033[1m'+'Number of Missing values in each column'+'\\033[0m')\n    i = df.isnull().sum()\n    print(i,'\\n')\n    \ndef n_unique():\n    print('\\033[1m'+'Number of unique elements'+'\\033[0m')\n    j = df.nunique()\n    print(j,'\\n')\n    \ndef memory_use():\n    print('\\033[1m'+'Memory used by all colomns in bytes'+'\\033[0m')\n    k = df.memory_usage()\n    print(k,'\\n')\n    \ndef is_na(value):\n    print('\\033[1m'+'Dataframe filled with boolean values with true indicating missing values'+'\\033[0m')\n    l = df.isna().head(value)\n    print(l,'\\n')\n    \ndef duplicate():\n    print('\\033[1m'+'Boolean Series denoting duplicate rows'+'\\033[0m')\n    m = df.duplicated().sum()\n    print(m,'\\n')\n    \ndef valuecounts():\n    print('\\033[1m'+'Series containing count of unique values'+'\\033[0m')\n    n = df.value_counts()\n    print(n,'\\n')\n\ndef datatypes():\n    print('\\033[1m'+'Datatype of each column'+'\\033[0m')\n    o = df.dtypes\n    print(o,'\\n')\n    \ndef correlation():\n    print('\\033[1m'+'Correalation between all columns in DataFrame'+'\\033[0m')\n    p = df.corr()\n    print(p,'\\n')\n    \ndef nonnull_count():\n    print('\\033[1m'+'Count of non-null values'+'\\033[0m')\n    q = df.count()\n    print(q,'\\n')\n    \ndef eda():\n    load()\n    value= 5 \n    datatypes()\n    top_rows(value)\n    bottom_rows(value)\n    rows_columns()\n    col_names()\n    information()\n    sizee()\n    ndimension()\n    stats_summary()\n    null_values()\n    n_unique()\n    memory_use()\n    is_na(value)\n    nonnull_count()\n    duplicate()\n    valuecounts()\n    correlation()\n    \n    \n    \n        \ndef stats_u(data,col):\n    if data[col].dtype == \"float64\":\n        print(col,\"has Quantitative data\")\n        mean_value=data[col].mean()\n        print('mean of',col,'column',mean_value)\n        max_value = data[col].max()\n        print('Maximum value of',col,'column',max_value)\n        min_value = data[col].min()\n        print('Minimum value of',col,'column',min_value)\n        median_value = data[col].median(skipna = True)\n        print('median of',col,'column',median_value)\n        std_value = data[col].std()\n        print('standard deviation of',col,'column',std_value)\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        print('quartile 1 of',col,'column is',q1)\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        print('quartile 2 of',col,'column is',q2)\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        print('quartile 3 of',col,'column is',q3)\n        q4 = data[col].quantile(1,interpolation='nearest')\n        print('quartile 4 of',col,'column is',q4)\n        IQR = q3 -q1\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        print('Lower Limit Point:',LLP)\n        print('Upper Limit Point:',ULP)\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers\")\n        else:\n            print(\"There are outliers\")\n            print(data[data[col]<LLP][col])\n            print(data[data[col]>ULP][col])\n            \n    elif data[col].dtype == \"int64\":\n        print(col,\"has Quantitative data\")\n        mean_value=data[col].mean()\n        print('mean of',col,'column',mean_value)\n        median_value = data[col].median(skipna = True)\n        print('median of',col,'column',median_value)\n        std_value = data[col].std()\n        print('standard deviation of',col,'column',std_value)\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        print('quartile 1 of',col,'column is',q1)\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        print('quartile 2 of',col,'column is',q2)\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        print('quartile 3 of',col,'column is',q3)\n        q4 = data[col].quantile(1,interpolation='nearest')\n        print('quartile 4 of',col,'column is',q4)\n        IQR = q3 -q1\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        print('Lower Limit Point:',LLP)\n        print('Upper Limit Point:',ULP)\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers\")\n        else:\n            print(\"There are outliers\")\n            print(\"Outliers are:\")\n            print(data[data[col]<LLP][col])\n            print(data[data[col]>ULP][col])\n    else:\n        print(col,'has Qualitative Data')\n        z = df[col].mode()\n        print('mode of',col,'column:\\n',z)\n        print('Count of mode is:\\n',df[col].value_counts())\n        print('Unique strings in',col,'are',data[col].nunique())\n        if(data[col].nunique() == 1):\n            print(col,'has same string')\n        elif(data[col].nunique() == 2):\n            print(col,'has binary strings')\n        else:\n            print(col,'has multi stings')\n\n\nlibraries()\neda()\n\nprint(\"----------------------------------------------------------------------------------------------------------------------\")\nprint('\\033[1m'+'Summary Of DataSet'+'\\033[0m')\nprint('\\033[1m'+'DataTypes in the DataSet:\\n'+'\\033[0m',df.dtypes)\nprint('\\033[1m'+'Columns in DataSet:'+'\\033[0m',df.columns)\nprint('\\033[1m'+'Shape of DataSet:'+'\\033[0m',df.shape)\nprint('\\033[1m'+'Size of DataSet:'+'\\033[0m',df.size)\nprint('\\033[1m'+'Dimension of DataSet:'+'\\033[0m',df.ndim)\nprint('\\033[1m'+'Total Memory used in DataSet:'+'\\033[0m',df.memory_usage().sum())\nprint('\\033[1m'+'Total Number of missing values in DataSet:'+'\\033[0m',df.isnull().sum().sum())\nprint('\\033[1m'+'Total Number of Unique values in DataSet:'+'\\033[0m',df.nunique().sum())\nprint('\\033[1m'+'Total Number of non null values in DataSet:'+'\\033[0m',df.count().sum())\nprint('\\033[1m'+'Total Number of duplicate rows in DataSet:'+'\\033[0m',df.duplicated().sum())\nprint(\"----------------------------------------------------------------------------------------------------------------------\")\nprint('\\033[1m'+'Summary Of Each Colomn'+'\\033[0m')\nprint(\"\\n\")\ncols=df.columns\ncols\nfor i in cols:\n    print('\\033[1m'+i+'\\033[0m')\n    stats_u(df,i)\n    print(\"\\n\")\n            ","9da0bcec":"df.head()","6d398b82":"df.tail()","9aec8e7d":"df.dtypes","79c887e5":"df.columns","21e6d363":"df.shape","e3672a78":"df.size","bd5a0dc7":"df.info()","010350d3":"df.describe()","bac94df1":"df.isnull().sum()","9bf508f0":"df.duplicated().sum()","f643aed3":"df.skew()","77b031aa":"df.corr()","ef27cca1":"! pip install Autoviz","bd51da7d":"! pip install xlrd","4f3c8534":"from autoviz.AutoViz_Class import AutoViz_Class\nAV = AutoViz_Class()\ndf_av = AV.AutoViz('..\/input\/heart-failure-prediction\/heart.csv')","b0da62ba":"plt.figure(figsize=(16,9))\nx = df.drop(['Sex','ChestPainType','RestingECG','ExerciseAngina','ST_Slope'],axis = 1)\nax = sns.heatmap(x.corr(),annot = True,cmap = 'viridis')\nplt.show()","26cf9416":"''' Plot a Shifted Correlation Matrix '''\n# Diagonal correlation is always unity & less relevant, shifted variant shows only relevant cases\ndef corrMat(df,id=False):\n    \n    corr_mat = df.corr().round(2)\n    f, ax = plt.subplots(figsize=(12,7))\n    mask = np.triu(np.ones_like(corr_mat, dtype=bool))\n    mask = mask[1:,:-1]\n    corr = corr_mat.iloc[1:,:-1].copy()\n    sns.heatmap(corr,mask=mask,vmin=-0.3,vmax=0.3,center=0, \n                cmap='RdPu_r',square=False,lw=2,annot=True,cbar=False)\n#     bottom, top = ax.get_ylim() \n#     ax.set_ylim(bottom + 0.5, top - 0.5) \n    ax.set_title('Shifted Linear Correlation Matrix')\n    \ncorrMat(x)","171edca9":"'''Plot Correlation to Target Variable only'''\ndef corrMat2(df,target='HeartDisease',figsize=(9,0.5),ret_id=False):\n    \n    corr_mat = df.corr().round(2);shape = corr_mat.shape[0]\n    corr_mat = corr_mat.transpose()\n    corr = corr_mat.loc[:, df.columns == target].transpose().copy()\n    if(ret_id is False):\n        f, ax = plt.subplots(figsize=figsize)\n        sns.heatmap(corr,vmin=-0.3,vmax=0.3,center=0, \n                     cmap='RdPu_r',square=False,lw=2,annot=True,cbar=False)\n        plt.title(f'Feature Correlation to {target}')\n    \n    if(ret_id):\n        return corr\ncorrMat2(x)","7eacc474":"sns.pairplot(x)","7b92ed62":"''' Draw a Bivariate Seaborn Pairgrid \/w KDE density w\/ '''\ndef snsPairGrid(df):\n\n    ''' Plots a Seaborn Pairgrid w\/ KDE & scatter plot of df features'''\n    g = sns.PairGrid(df,diag_sharey=False,hue='HeartDisease',palette='Purples')\n    g.fig.set_size_inches(13,13)\n    g.map_upper(sns.kdeplot,n_levels=5)\n    g.map_diag(sns.kdeplot, lw=2)\n    g.map_lower(sns.scatterplot,s=20,edgecolor=\"k\",linewidth=1,alpha=0.6)\n    g.add_legend()\n    plt.tight_layout()\nnumvars_targ = ['Age', 'RestingBP', 'Cholesterol', 'FastingBS', 'MaxHR', 'Oldpeak','HeartDisease']\nsnsPairGrid(df[numvars_targ])","ffe0bbf0":"obj = ['Sex','ChestPainType','RestingECG','ExerciseAngina','ST_Slope','HeartDisease','FastingBS']\nobj","446944aa":"num = []\nfor i in df.columns:\n    if i not in obj:\n        num.append(i)\nnum","a0f54ab6":"df['Sex'].value_counts()","d6a58d6b":"sns.countplot(x = 'Sex',data = df)\nplt.show()","56a94bea":"df['ChestPainType'].value_counts()","64ba3df7":"sns.countplot(x = 'ChestPainType',data = df)\nplt.show()","24c7cc4d":"df['RestingECG'].value_counts()","4e8d00a3":"sns.countplot(x = 'RestingECG',data = df)\nplt.show()","18d7b468":"df['ExerciseAngina'].value_counts()","a78dc30a":"sns.countplot(x = 'ExerciseAngina',data = df)\nplt.show()","584eb3f4":"df['ST_Slope'].value_counts()","0709db98":"sns.countplot(x = 'ST_Slope',data = df)\nplt.show()","697afeb2":"df['HeartDisease'].value_counts()","107278ae":"sns.countplot(x = 'HeartDisease',data = df)\nplt.show()","68bede98":"for i in obj:\n    data=df.copy()\n    data.groupby(i)['Age'].median().plot.bar()\n    plt.xlabel(i)\n    plt.ylabel('Age')\n    plt.title(i)\n    plt.show()","ac8ea6fe":"for i in obj:\n    data=df.copy()\n    data.groupby(i)['RestingBP'].median().plot.bar()\n    plt.xlabel(i)\n    plt.ylabel('RestingBP')\n    plt.title(i)\n    plt.show()","25253141":"for i in obj:\n    data=df.copy()\n    data.groupby(i)['Cholesterol'].median().plot.bar()\n    plt.xlabel(i)\n    plt.ylabel('Cholesterol')\n    plt.title(i)\n    plt.show()","5c836f8a":"for i in obj:\n    data=df.copy()\n    data.groupby(i)['MaxHR'].median().plot.bar()\n    plt.xlabel(i)\n    plt.ylabel('MaxHR')\n    plt.title(i)\n    plt.show()","1e4bb32b":"for i in obj:\n    data=df.copy()\n    data.groupby(i)['Oldpeak'].median().plot.bar()\n    plt.xlabel(i)\n    plt.ylabel('Oldpeak')\n    plt.title(i)\n    plt.show()","6556e013":"for i in range(len(obj)):\n    x='Sex'\n    for j in range(len(num)):\n        if obj[i] != x:\n            sns.barplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","5bccc6ee":"for i in range(len(obj)):\n    x='ChestPainType'\n    for j in range(len(num)):\n        if obj[i] != x:\n            sns.barplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","33b6147a":"for i in range(len(obj)):\n    x='RestingECG'\n    for j in range(len(num)):\n        if obj[i] != x:\n            sns.barplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","b3715a3d":"for i in range(len(obj)):\n    x='ExerciseAngina'\n    for j in range(len(num)):\n        if obj[i] != x:\n            sns.barplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","b3c66074":"for i in range(len(obj)):\n    x='ST_Slope'\n    for j in range(len(num)):\n        if obj[i] != x:\n            sns.barplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","f763b85f":"for i in range(len(obj)):\n    x='HeartDisease'\n    for j in range(len(num)):\n        if obj[i] != x:\n            sns.barplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","d83a760a":"for i in range(len(obj)):\n    x='FastingBS'\n    for j in range(len(num)):\n        if obj[i] != x:\n            sns.barplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","fb36116f":"for i in range(len(obj)):\n    x='Age'\n    for j in range(len(num)):\n        if num[j] != x:\n            sns.scatterplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","6d5ad166":"for i in range(len(obj)):\n    x='RestingBP'\n    for j in range(len(num)):\n        if num[j] != x:\n            sns.scatterplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","47c01bea":"for i in range(len(obj)):\n    x='Cholesterol'\n    for j in range(len(num)):\n        if num[j] != x:\n            sns.scatterplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","7fd0542d":"for i in range(len(obj)):\n    x='MaxHR'\n    for j in range(len(num)):\n        if num[j] != x:\n            sns.scatterplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","8d849764":"for i in range(len(obj)):\n    x='Oldpeak'\n    for j in range(len(num)):\n        if num[j] != x:\n            sns.scatterplot(x= x,y=num[j],hue=obj[i],data=df)\n            plt.show()","b93603b8":"num","0637fae9":"plt.figure(figsize=(6,8))\nx = df.drop(obj,axis = 1)\nfor i in x.columns:\n    sns.histplot(x[i],kde = True)\n    plt.show()","1fc0aa5b":"x = df.drop(obj,axis = 1)\nfor i in x.columns:\n    sns.boxplot(x = i, data = x,color = 'yellowgreen')   \n    plt.xlabel(i)\n    plt.show()","6211a340":"x = df.drop(obj,axis = 1)\nfor i in x.columns:\n    sns.violinplot(x = i, data = x,color = 'yellowgreen')   \n    plt.xlabel(i)\n    plt.show()","df535b33":"df1 = df.groupby('Age').agg({'Sex' : 'count', \"ChestPainType\":'count','RestingBP':'mean','Cholesterol':'mean',\n                            'FastingBS':'sum','RestingECG':'count','MaxHR':'mean','ExerciseAngina':'count','Oldpeak':'mean',\n                            'ST_Slope':'count','HeartDisease':'sum'})\ndf1\n\n# we can do group wise analysis","245b9546":"df2 = df.groupby('Sex').agg({'Age' : 'mean', \"ChestPainType\":'count','RestingBP':'mean','Cholesterol':'mean',\n                            'FastingBS':'sum','RestingECG':'count','MaxHR':'mean','ExerciseAngina':'count','Oldpeak':'mean',\n                            'ST_Slope':'count','HeartDisease':'sum'})\ndf2\n# average age is same for both male and female","9af95fe5":"px.bar(data_frame=df1, barmode='group',\n       title = \"<b>Age wise Analyzing<\/b>\",template=\"plotly_dark\")","a11eac2d":"px.bar(data_frame=df2, barmode='group',\n       title = \"<b>Gender wise Analyzing<\/b>\",template=\"plotly_dark\")","a35b1a49":"def count_outliers(data,col):\n        q1 = data[col].quantile(0.25,interpolation='nearest')\n        q2 = data[col].quantile(0.5,interpolation='nearest')\n        q3 = data[col].quantile(0.75,interpolation='nearest')\n        q4 = data[col].quantile(1,interpolation='nearest')\n        IQR = q3 -q1\n        global LLP\n        global ULP\n        LLP = q1 - 1.5*IQR\n        ULP = q3 + 1.5*IQR\n        if data[col].min() > LLP and data[col].max() < ULP:\n            print(\"No outliers in\",i)\n        else:\n            print(\"There are outliers in\",i)\n            x = data[data[col]<LLP][col].size\n            y = data[data[col]>ULP][col].size\n            a.append(i)\n            print('Count of outliers are:',x+y)\nglobal a\na = []\nfor i in x.columns:\n    count_outliers(df,i)","208baa5b":"df.isnull().sum() # no null value treatment","5660a6b6":"df=pd.get_dummies(data=df,columns=['Sex','ChestPainType','RestingECG','ExerciseAngina','ST_Slope'],drop_first=True)\ndf","298eaa93":"scaler = StandardScaler()\nscaler.fit(df.drop('HeartDisease',axis = 1))","0c87c37c":"scaled_features = scaler.transform(df.drop('HeartDisease',axis = 1))\ndf_feat = pd.DataFrame(scaled_features,columns = df.columns[:-1])\ndf_feat.head()","3bf127b7":"X = df_feat\ny = df['HeartDisease']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","e4e002fc":"knn = KNeighborsClassifier(n_neighbors = 14)\nknn.fit(X_train,y_train)","a3032c8c":"pred = knn.predict(X_test)\npred","e2157a61":"print(confusion_matrix(y_test,pred))","670f496d":"print(classification_report(y_test,pred))","ed4b1da9":"error_rate= []\nfor i in range(1,40):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","b70178cd":"plt.figure(figsize = (10,6))\nplt.plot(range(1,40),error_rate,color = 'blue',linestyle = '--',marker = 'o',markerfacecolor='red',markersize = 10)\nplt.title('Error Rate vs K')\nplt.xlabel('K')\nplt.ylabel('Error Rate')","a3718c7f":"print(metrics.accuracy_score(y_test, pred))","13b8e6f0":"forest= RandomForestClassifier(n_estimators =40, random_state = 0)\nforest.fit(X_train,y_train)  \ny_pred = forest.predict(X_test)\nforest.score(X_test,y_test)","9b56a35f":"logmodel = LogisticRegression()\nlogmodel.fit(X_train,y_train)","5c74eee7":"pred = logmodel.predict(X_test)","f2bc5745":"print(classification_report(y_test,pred))\nprint(confusion_matrix(y_test,pred))","34c9816e":"logmodel.score(X_test,y_test)","09aeee07":"## Diving numerical and object columns into seperate lists","f959e40c":"## no need of outlier treatment","df68dd49":"# Feature Selection","9ad204e3":"# prediction of heart disease using randomforest classifier","ae55f3a5":"# Data Visualisation","e9d591b5":"# Feature Scaling","d8d4e7d5":"# count of outliers","535cf271":"# Importing Libraries","d3f67d44":"# Prediction of heart disease using Logistic regression","936d29af":"## Encoding","0341de9c":"# Data Preprocessing","71fae783":"# Prediction of heart disease using KNN","fb991305":"# Loading DataSet","3d8c14fb":"# Data visualisation using Autoviz","95f269f3":"## UPVOTE IF U LIKE","fc2c3188":"# Exploratory Data Analysis using userdefined Function","e2fd246e":"# Exploratory Data Analysis"}}