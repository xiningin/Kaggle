{"cell_type":{"4c375c4c":"code","3fe90ba3":"code","7fe50503":"code","881e089a":"code","deb6902c":"code","fc7b1338":"code","3728c270":"code","f76902b2":"code","19d0e042":"code","07bdd618":"code","2562f8db":"code","f9f007ac":"markdown","00508f82":"markdown","e9844065":"markdown","854f850e":"markdown","7c44b52f":"markdown","433ad2b8":"markdown","67d8c20d":"markdown","b68a8880":"markdown"},"source":{"4c375c4c":"print(\"Project Nightfire.\") ","3fe90ba3":"# Install and import nlpaug package.\n!cp -r ..\/input\/nlpaug-from-github\/nlpaug-master .\/\n!pip install nlpaug-master\/\n!rm -r nlpaug-master\n\nimport nlpaug.augmenter.char as nac\nimport nlpaug.augmenter.word as naw\nimport nlpaug.augmenter.word.context_word_embs as nawcwe\nimport nlpaug.augmenter.word.word_embs as nawwe\nimport nlpaug.augmenter.word.spelling as naws","7fe50503":"from colorama import Fore\nfrom pathlib import Path\n\n# Set some seeds. We want randomness for real usage, but for this tutorial, determinism helps explain some examples.\nimport numpy as np\nnp.random.seed(1000)\nimport random\nrandom.seed(1000)\n\nbase_path = Path(\"..\/input\/feedback-prize-2021\/train\")\n\nwith open(base_path \/ \"3FF2F530D590.txt\") as f:\n    sample = f.read()\n    \nprint(f\"Original: {len(sample)}\\n{sample}\")\n\naug = nac.KeyboardAug()\naugmented_text = aug.augment(sample)\nprint(f\"\\nKeyboard augmentation: {len(augmented_text)}\\n{augmented_text}\")","881e089a":"# Replace orig text, with a version that has extra whitespace removed.\nsample = \" \".join([x.strip() for x in sample.split()])","deb6902c":"def print_and_highlight_diff(orig_text, new_texts):\n    \"\"\" A simple diff viewer for augmented texts. \"\"\"\n    orig_split = orig_text.split()\n    print(f\"Original: {len(orig_split)}\\n{orig_text}\\n\")\n    for new_text in new_texts:\n        print(f\"Augmented: {len(new_text.split())}\")\n        for i, word in enumerate(new_text.split()):\n            if i < len(orig_split) and word == orig_split[i]:\n                print(word, end=\" \")\n            else:\n                print(Fore.RED + word + Fore.RESET, end=\" \")\n        print()","fc7b1338":"aug = nac.KeyboardAug(include_numeric=False, include_special_char=False, aug_char_max=1, aug_word_p=0.05)\naugmented_texts = aug.augment(sample, n=3)\naugmented_texts = [x.replace(\" ' \", \"'\") for x in augmented_texts]\nprint_and_highlight_diff(sample, augmented_texts)","3728c270":"aug = naw.SpellingAug()\naugmented_texts = aug.augment(sample, n=3)\naugmented_texts = [x.replace(\" ' \", \"'\") for x in augmented_texts]\nprint_and_highlight_diff(sample, augmented_texts)","f76902b2":"aug = naw.SynonymAug()\naugmented_texts = aug.augment(sample, n=3)\naugmented_texts = [x.replace(\" ' \", \"'\") for x in augmented_texts]\nprint_and_highlight_diff(sample, augmented_texts)","19d0e042":"aug = nawwe.WordEmbsAug(model_type='glove', model_path='..\/input\/glove-embeddings\/glove.6B.300d.txt')\naugmented_texts = aug.augment(sample, n=3)\naugmented_texts = [x.replace(\" ' \", \"'\") for x in augmented_texts]\nprint_and_highlight_diff(sample, augmented_texts)","07bdd618":"\"File \/tmp\/ipykernel_54\/3407753390.py in <module>aug =\"\nnawwe.ContextualWordEmbsAug(model_path=\/input\/huggingface-bert-variants\/bert-base\ncased\/bert-base-cased')'augmented_texts = aug.augment(sample, n=3)'\naugmented_texts = [x.replace(\" ' \", \"'\") for x in augmented_texts]'\nprint_and_highlight_diff(sample, augmented_texts)","2562f8db":"# Conclusions and Next Steps\n\nSpellingAug and ContextualWordEmbsAug look to produce good ouptut, that can be used without adjusting the discourse annotations given for training.\n\nNext steps:\n1. Try applying SpellingAug and ContextualWordEmbsAug to all training texts. How long does it take? How many augmented texts have the same split length?\n2. Apply to training, does CV improve?\n3. Apply to inference, does TTA help?","f9f007ac":"Oh dear, the synonym augmentation is not a 1-to-1 swap at the word level. E.g. small -> pocket size. Therefore this cannot be used while maintaining the discourse annotations.\n\n# WordEmbsAug\n\nUses word embeddings to find a similar word for augmentation, used here with a local copy of the GloVe model.","00508f82":"That looks usable.\n\n# SynonymAug\n\nNow SynonymAug, that replaces some words with synonyms.","e9844065":"Not great, several issues are obvious:\n1. The length of the split text has changed as the original had additional white space for formatting.\n2. It is hard to see all the differences without highlighting.\n3. The augmentation adds digits and special characters, which are unlikely to have been present.\n4. The augmentation can change many characters in one word, making the new word too far from the original.\n5. The augmented text adds spaces around apostrophes, increasing the split length.\n\nLet's solve these by:\n1. Stripping the original text.\n2. Adding a diff viewer to highlight the differences.\n3. Setting arguments to the augmentation method.\n4. Setting arguments to the augmentation method.\n5. Post-processing the augmented text with: replace(\" ' \", \"'\")","854f850e":"That looks usable, but some of the misspellings are fairly severe.\n\n# SpellingAug\n\nIt KeyboardAug creates typos that are too unnatural, let's try SpellingAug which uses a DB of common misspellings.","7c44b52f":"Some interesting swaps there! For example mubarak == president only in a very specific place and time!\n\n# ContextualWordEmbsAug\n\nThis is similar to WordEmbsAug, but uses more powerful contextual word embeddings. Here used with BERT.","433ad2b8":"# Introduction\n\nMore data can help train models to be more general, with less overfitting. One convenient way to generate additional data is simply to transform the given data into something slightly different, such that it still represents the assigned labels. As well as helping during training, augmentation can also be used when running inference (Test Time Augmentation or TTA).\n\nFor an image based challenge, flips, rotations, etc. can be used to generate a new image that still presents the same class of object as the original. But how do we do this for a NLP challenge?\n\nThere are a wide variety of approaches, from character and word substitutions, all the way to translating the text from the source lanaguage to another and then back to get a setence with (hopefully) the same meaning, but a different structure.\n\nFor more info, vad13irt has already posted a great survey here: https:\/\/www.kaggle.com\/c\/feedback-prize-2021\/discussion\/295277\n\nFor this notebook, I want to focus on what the application of some of these methods actually looks like for the feedback-prize-2021 dataset. In particular, I'll be looking at methods that can be used while preserving the discourse_start and discourse_end annotations given in train.csv.","67d8c20d":"# First Attempt\n\nLet's start by taking a arbitrarily selected training text and augmenting it. For this I've picked a very simple augmetation, KeyboardAug, that uses adjacency of keys on the keyboard to simulate typos. Alongside the original and annotated text, I'm printing the length of the text run through split() - if that changes the discourse_start and discourse_end annotations given in train.csv will no longer be valid.","b68a8880":"# KeyboardAug\n\nLet's try again with no digits, no special characters and only one augmented character per word."}}