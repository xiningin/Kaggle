{"cell_type":{"bea31d1a":"code","8b2665b3":"code","7b287190":"code","b58fd2b1":"code","ac295256":"code","01e95476":"code","39f80d8f":"code","b80d4eea":"code","536c0c80":"code","c106ac18":"code","9b3aa745":"markdown","500538f1":"markdown","df9bda37":"markdown","816f5b41":"markdown","db074139":"markdown"},"source":{"bea31d1a":"%matplotlib inline\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torchvision.transforms as transforms\nfrom torch.utils.data import DataLoader\nfrom torchvision.datasets import MNIST","8b2665b3":"mnist_pwd = \"data\"\nbatch_size= 256","7b287190":"transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.1307,), (0.3081,))])\n\ntrainset = MNIST(mnist_pwd, train=True, download=True, transform=transform)\ntrainloader = DataLoader(trainset, batch_size=batch_size, shuffle=True, num_workers=4)\n\ntestset = MNIST(mnist_pwd, train=False, download=True, transform=transform)\ntestloader = DataLoader(testset, batch_size=batch_size * 2, shuffle=False, num_workers=0)","b58fd2b1":"class Net(nn.Module):\n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n        self.conv2_drop = nn.Dropout2d()\n        self.fc1 = nn.Linear(320, 50)\n        self.fc2 = nn.Linear(50, 10)\n\n    def forward(self, x):\n        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n        x = x.view(-1, 320)\n        x = F.relu(self.fc1(x))\n        x = F.dropout(x, training=self.training)\n        x = self.fc2(x)\n        return F.log_softmax(x, dim=1)","ac295256":"from ignite.engine import create_supervised_trainer, create_supervised_evaluator\nfrom ignite.metrics import Loss, Accuracy\nfrom ignite.contrib.handlers import FastaiLRFinder, ProgressBar","01e95476":"device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\ncriterion = nn.NLLLoss()\nmodel = Net()\nmodel.to(device)  # Move model before creating optimizer\noptimizer = optim.SGD(model.parameters(), lr=3e-4, momentum=0.9)","39f80d8f":"trainer = create_supervised_trainer(model, optimizer, criterion, device=device)\nProgressBar(persist=True).attach(trainer, output_transform=lambda x: {\"batch loss\": x})\n\nlr_finder = FastaiLRFinder()\nto_save={'model': model, 'optimizer': optimizer}\nwith lr_finder.attach(trainer, to_save, diverge_th=1.5) as trainer_with_lr_finder:\n    trainer_with_lr_finder.run(trainloader)\n    \ntrainer.run(trainloader, max_epochs=10)\n\nevaluator = create_supervised_evaluator(model, metrics={\"acc\": Accuracy(), \"loss\": Loss(nn.NLLLoss())}, device=device)\nevaluator.run(testloader)\n\nprint(evaluator.state.metrics)","b80d4eea":"lr_finder.plot()","536c0c80":"lr_finder.lr_suggestion()","c106ac18":"optimizer.param_groups[0]['lr'] = lr_finder.lr_suggestion()\n\ntrainer.run(trainloader, max_epochs=10)\nevaluator.run(testloader)\nprint(evaluator.state.metrics)","9b3aa745":"## Loading MNIST","500538f1":"# MNIST example with 3-conv. layer network\n\nThis example demonstrates the usage of `FastaiLRFinder` with a 3-conv. layer network on the MNIST dataset.","df9bda37":"## Training loss (fastai)\n\nThis learning rate test range follows the same procedure used by fastai. The model is trained for `num_iter` iterations while the learning rate is increased from its initial value specified by the optimizer algorithm to `end_lr`. The increase can be linear (`step_mode=\"linear\"`) or exponential (`step_mode=\"exp\"`); linear provides good results for small ranges while exponential is recommended for larger ranges.","816f5b41":"Let's now setup suggested learning rate to the optimizer and we can train the model with optimal learning rate.\n\n*Note, that model and optimizer were restored to their initial states when `FastaiLRFinder` finished.*","db074139":"## Model"}}