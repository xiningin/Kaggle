{"cell_type":{"0fb86388":"code","b2703380":"code","36be78c2":"code","7ea93de4":"code","37c1d3e7":"code","d00d566e":"code","34dbe317":"code","1c97ef6e":"code","8a9005a9":"code","4b1a80fe":"code","8eba12e8":"code","e8d4e47f":"code","9cb0787f":"code","35ca4b0f":"code","741ca272":"code","427fef12":"code","cd174f17":"code","ed3b6281":"code","e9a35144":"code","4094c1c6":"code","4ee545ad":"code","ab331bbd":"code","253d5dda":"code","60cf5771":"code","fd3884b1":"code","731a9ba7":"code","127596ed":"code","9e710600":"code","ff4a682f":"code","49cfc8c9":"code","6d8743b3":"code","ad704516":"code","e2950b9a":"code","f55ae98e":"code","234f870d":"code","7f10ef7e":"markdown","f9cde4e8":"markdown","ef70b026":"markdown","997fd80f":"markdown","80371b97":"markdown","fbbf3df1":"markdown","2d636187":"markdown","1cf4526f":"markdown","92f1389b":"markdown","e6a6ae99":"markdown","19286091":"markdown","05631bd2":"markdown","5254101c":"markdown","52314443":"markdown","fb794430":"markdown","d5027aa9":"markdown","b5b49f73":"markdown","1d3b1c5b":"markdown","cc458dd3":"markdown","5e3306e0":"markdown","66150d69":"markdown","3d204a57":"markdown","633cdffc":"markdown","cb970208":"markdown","4bacab31":"markdown","0432cb1b":"markdown"},"source":{"0fb86388":"import numpy as np # linear algebra\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\nheart = pd.read_csv('..\/input\/heart-failure-prediction\/heart.csv')\nheart.head()","b2703380":"#Shape of the dataset\nprint(\"Shape of the dataset is: \"+str(heart.shape))\n#Is there any null values in the data?\nprint(\"Amount of null values in data: \"+ str(heart.isnull().sum().sum()))","36be78c2":"#Check balance of the output variables\nheart.groupby(['HeartDisease'])['HeartDisease'].count()","7ea93de4":"fig, axes = plt.subplots(2, 3, figsize=(15, 10))\nfig.patch.set_facecolor('lightblue')\nfig.patch.set_alpha(0.3)\n\nsns.countplot(x = heart['RestingECG'], data = heart, palette='colorblind', ax=axes[0,0],hue = \"HeartDisease\")\nsns.countplot(x = heart['ChestPainType'], data = heart, palette='colorblind', ax=axes[0,1],hue = \"HeartDisease\")\n\nsns.countplot(x = heart['ExerciseAngina'], data = heart, palette='colorblind', ax=axes[0,2],hue = \"HeartDisease\")\nsns.countplot(x = heart['FastingBS'], data = heart, palette='colorblind', ax=axes[1,0],hue = \"HeartDisease\")\nsns.countplot(x = heart['Sex'], data = heart, palette='colorblind', ax=axes[1,1],hue = \"HeartDisease\")\nsns.countplot(x = heart['ST_Slope'], data = heart, palette='colorblind', ax=axes[1,2],hue = \"HeartDisease\")\nplt.suptitle('Categorical Variables EDA', fontsize=25)\nplt.show()","37c1d3e7":"fig, axes = plt.subplots(2, 3, figsize=(17, 10))\nfig.patch.set_facecolor('lightgreen')\nfig.patch.set_alpha(0.2)\n\nsns.histplot(data = heart,x = heart['Age'],hue = \"HeartDisease\",element=\"poly\",ax=axes[0,0])\nsns.histplot(data = heart,x = heart['RestingBP'],hue = \"HeartDisease\",element=\"poly\",ax=axes[0,1])\nsns.histplot(data = heart,x = heart['Cholesterol'],hue = \"HeartDisease\",element=\"poly\",ax=axes[0,2])\nsns.histplot(data = heart,x = heart['MaxHR'],hue = \"HeartDisease\",element=\"poly\",ax=axes[1,0])\nsns.histplot(data = heart,x = heart['Oldpeak'],hue = \"HeartDisease\",element=\"poly\",ax=axes[1,1])\nfig.delaxes(axes[1][2])\nplt.suptitle('Numerical Variables EDA', fontsize=25)\nplt.show()","d00d566e":"# Correlation heatmap for Numerical and Binary Variables\nsns.heatmap(heart.corr(), annot=True, cmap='mako')\nplt.show()","34dbe317":"fig, axes = plt.subplots(2, 2, figsize=(15, 10))\nsns.boxplot(x=heart['RestingBP'],ax=axes[0,0])\nsns.boxplot(x=heart['Cholesterol'],ax=axes[0,1])\nsns.boxplot(x=heart['MaxHR'],ax=axes[1,0])\nsns.boxplot(x=heart['Oldpeak'],ax=axes[1,1])\nplt.suptitle(\"Detecting Outliers\",fontsize = 15)\nplt.show()","1c97ef6e":"#It is impossible to have a resting heart bloop pressure of zero.\nprint((heart['RestingBP']==0).sum())\n\n#We need to impute this datapoint (not the whole row). I will do this with the median Resting blood pressure\nfor i in heart['RestingBP']:\n    if i == 0:\n        heart['RestingBP'] = heart['RestingBP'].replace(i, heart['RestingBP'].median())\n\n#Checking if high BP has a correlation with the outcome\nqHigh = heart['RestingBP'].quantile(0.80)\nprint(\"High Resting BP on Heart Disease: \")\nprint(heart[(heart['RestingBP'] >= qHigh)].value_counts(heart['HeartDisease']))\n\n#We can see that there is a increase in heart disease with high RestingBP. So we will not change this outliers.","8a9005a9":"#Low cholesterol levels seems suspicious. But there are many datapoints.\nprint((heart['Cholesterol']==0).sum())\n#Check correlation of heart disease with zero cholesterol level.\nprint(\"Zero Cholesterol on Heart Disease: \")\nprint(heart[(heart['Cholesterol'] == 0)].value_counts(heart['HeartDisease']))\n\n#BUTTT, WE CAN SEE THAT IT HEART DISEASE IS COMMON WITH VERY LOW CHOLESTEROL LEVELS SO WE will keep the data.\n","4b1a80fe":"#There are some rows with very low maximum heart rates. Lets see how it affects the Heart disease\nprint(\"Low Maximum Heart Rate on Heart Disease: \")\nprint(heart[(heart['MaxHR'] <= 75)].value_counts(heart['HeartDisease']))\n#WE CAN SEE THAT LOW MaxHR is ASSOCIATED WITH Heart Disease. So we will not touch this datapoints","8eba12e8":"#There are some rows with very low and very high Old peaks. Lets see how it affects the Heart diseas\nprint(\"Low Oldpeak on Heart Disease: \")\nprint(heart[(heart['Oldpeak'] <= -1)].value_counts(heart['HeartDisease']))\n\nprint(\"High Oldpeak on Heart Disease: \")\nprint(heart[(heart['Oldpeak'] >= 3.5)].value_counts(heart['HeartDisease']))\n\n# We can see that very low and very high old peaks are associated with Heart Disease. ","e8d4e47f":"#Setting up X and y Variables\nX = heart.iloc[:,0:-1] # All columns except HeartDisease Columnn\ny = heart.iloc[:,-1] # HeartDisease Column","9cb0787f":"#One Hot Encoding using Pandas Get Dummies function\nX = pd.get_dummies(X, columns=['Sex','ChestPainType','RestingECG','ExerciseAngina','ST_Slope'], \n                   drop_first=True) #We dont need to encode Fasting BS because it's already in a 0 or 1 format\n#drop_first = true to avoid dummy variable trap. We are removing one encoded column from every categorical variable\nX.head()","35ca4b0f":"#Splitting data into training and test set\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=0)","741ca272":"#For Logistics Regression we dont need to scale data manually\nfrom sklearn.linear_model import LogisticRegression\nLR = LogisticRegression(solver='liblinear',random_state =0)\n#If you dont include solver='liblinear', you'll get an error for some reason (This doesnt happen when i do it offline)\nLR.fit(X_train,y_train)\ny_pred = LR.predict(X_test)","427fef12":"#Confusion Matrix and Accuracy\nfrom sklearn.metrics import accuracy_score, f1_score,classification_report\nfrom sklearn.metrics import confusion_matrix\ncm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm, annot=True)\naccuracy = round(accuracy_score(y_test,y_pred),4)\nprint(\"Accuracy: \",accuracy)\nf1 = f1_score(y_test,y_pred)\nprint(\"F1 Score:\\n \",f1)\nreport = classification_report(y_test,y_pred)\nprint(\"Classification Report:\\n \",report)","cd174f17":"#Using KFold Cross Validation\nfrom sklearn.model_selection import KFold\ncv = KFold(n_splits=5, random_state=1, shuffle=True)\nfrom sklearn.model_selection import cross_validate\nscores = cross_validate(LogisticRegression(solver='liblinear',random_state =0), \n                        X, y, cv=cv,scoring = ['accuracy','f1_macro','precision','recall'])\n\nprint('Accuracy: %.4f (std: %.3f)' % (np.mean(scores['test_accuracy']), np.std(scores['test_accuracy'])))\nprint('F1 Score : %.4f' % (np.mean(scores['test_f1_macro'])))\nprint('Precision : %.4f' % (np.mean(scores['test_precision'])))\nprint('Recall : %.4f' % (np.mean(scores['test_recall'])))","ed3b6281":"# Feature Scaling is needed\nfrom sklearn.preprocessing import StandardScaler\nsc = StandardScaler() \n'''\nLOG - #We will only scale numerical X values and concat that with categorical part (Excluding FastingBS im too lazy lol)\nTurns out this don't work very well..., Scaling all variables equaly performs better (Done below)\nThis vs Equal Scaling Results are as below(F1 Score),\nSVC (grid search) - 0.8424 vs 0.8533\nRandomForest (grid_search) - 0.8501 vs 0.8501\nNN (grid_search) - It changes a lot LMAO\nX_scaled = pd.concat([pd.DataFrame(sc.fit_transform(X.iloc[:,0:6]),columns = X.columns[0:6]),X.iloc[:,6:]],axis = 1)\n'''\nX_scaled = pd.DataFrame(sc.fit_transform(X),columns = X.columns)\nX_train_scaled, X_test_scaled, y_train, y_test = train_test_split(X_scaled, y, test_size=0.20, random_state=0)\nX_scaled.head()","e9a35144":"#Simple Model\nfrom sklearn.svm import SVC\nsvm = SVC(random_state=1)\nsvm.fit(X_train_scaled,y_train)\ny_pred = svm.predict(X_test_scaled)\n\ncm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm, annot=True)\naccuracy = round(accuracy_score(y_test,y_pred),4)\nprint(\"Accuracy: \",accuracy)\nf1 = f1_score(y_test,y_pred)\nprint(\"F1 Score:\\n \",f1)\nreport = classification_report(y_test,y_pred)\nprint(\"Classification Report:\\n \",report)","4094c1c6":"#It's already better than  Logistic Regression even with stock parameters (C=1,rbf,degree = 3,gamma = scale)\n\n#Now I will Optimize Hyperparameters\nfrom sklearn.model_selection import GridSearchCV\n'''\nparams = [ {'C':[1, 10, 100,1000], 'kernel':['linear']},\n               {'C':[1, 10, 100,1000], 'kernel':['rbf'], 'gamma':[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9,'scale']},\n               {'C':[1, 10, 100,1000], 'kernel':['poly'], 'degree': [2,3,4,5] ,'gamma':[0.01,0.02,0.03,0.04,0.05,'scale']},\n                {'C':[1, 10, 100,1000], 'kernel':['sigmoid']}\n              ]\ngrid = GridSearchCV(estimator = svm,  \n                           param_grid = params,\n                           scoring = 'f1', #F1 because false positives and false negatives are crucial\n                           cv = 5,\n                           verbose=0)\ngrid.fit(X_scaled,y)\n'''","4ee545ad":"#y_pred = grid.predict(X_test_scaled)\n#cm = confusion_matrix(y_test,y_pred)\n#accuracy = round(accuracy_score(y_test,y_pred),4)\n#print(\"Accuracy: \",accuracy)# 0.8913\n#sns.heatmap(cm,annot=True,fmt = 'g')\n#Getting Best Scores and Best Parameters\n#print(\"Best Score: \", grid.best_score_) #F1 - 0.8529593322446811 , Accuracy  - 0.8913\n#print(\"Best Parameters : \", grid.best_params_) #  {'C': 1, 'degree': 5, 'gamma': 'scale', 'kernel': 'poly'}","ab331bbd":"#Creating a model according to best parameters \nsvm = SVC(C = 1,degree = 5,kernel = 'poly',gamma = 'scale',random_state=1)\nsvm.fit(X_train_scaled,y_train)\ny_pred = svm.predict(X_test_scaled)\n\ncm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm, annot=True)\naccuracy = round(accuracy_score(y_test,y_pred),4)\nprint(\"Accuracy: \",accuracy) # Accuracy & F1 we get here is lower than grid search model probably due to less avalable data\nf1 = f1_score(y_test,y_pred)\nprint(\"F1 Score:\\n \",f1)\nreport = classification_report(y_test,y_pred)\nprint(\"Classification Report:\\n \",report)","253d5dda":"#I will start with Grid Search (Takes so long to run)\nfrom sklearn.ensemble import RandomForestClassifier \n'''\nRF = RandomForestClassifier(random_state = 0)\nparams = { \n    'n_estimators': [10,20,30,50,100,200,300,400,500,750,900],\n    'max_features': ['auto', 'sqrt', 'log2'],\n    'max_depth' : [4,5,6,7,8,None],\n    'criterion' :['gini', 'entropy']\n}\ngrid_RF = GridSearchCV(estimator = RF,  \n                           param_grid = params,\n                           scoring = 'f1', #F1 because false positives and false negatives are crucial\n                           cv = 5,\n                           verbose=0)\ngrid_RF.fit(X_scaled,y)\n\ny_pred = grid_RF.predict(X_test_scaled)\ncm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm,annot=True)\n'''","60cf5771":"#Getting Best Scores and Best Parameters\n#print(\"Best Score: \", grid_RF.best_score_) #0.8500657209209763\n#accuracy = round(accuracy_score(y_test,y_pred),4) #0.8478\n#print(\"Accuracy: \",accuracy)# 0.8913\n#print(\"Best Parameters : \", grid_RF.best_params_) #  {'criterion': 'entropy', 'max_depth': 4, 'max_features': 'auto', 'n_estimators': 20","fd3884b1":"\n#Creating a model according to best parameters\nRF = RandomForestClassifier(criterion = 'entropy',max_depth = 4,max_features = 'auto',n_estimators = 20,random_state=0)\nRF.fit(X_train_scaled,y_train)\ny_pred = RF.predict(X_test_scaled)\n\ncm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm, annot=True)\naccuracy = round(accuracy_score(y_test,y_pred),4)\nprint(\"Accuracy: \",accuracy)\nf1 = f1_score(y_test,y_pred)\nprint(\"F1 Score:\\n \",f1)\nreport = classification_report(y_test,y_pred)\nprint(\"Classification Report:\\n \",report)","731a9ba7":"#I will start with Grid Search (Takes so long to run)\n\nfrom xgboost import XGBClassifier\n'''\nxgb = XGBClassifier(random_state = 0,verbosity = 0,use_label_encoder = False)\n#{'booster':['gblinear'],'lambda':[0.01, 0.1, 0.5],'updater':['shotgun','coord_descent'],\n               # 'feature_selector':['cyclic','shuffle'],'top_k':[0,1,2,3,4,5]},\nparams = [ {'learning_rate':[0.01, 0.1, 0.2,0.3,0.4], 'booster':['gbtree'], 'min_split_loss':[0,0.01,0.1],\n                   'max_depth':[3,4,5,6,7,8,9]},\n               {'learning_rate':[0.01, 0.1, 0.2,0.3,0.4], 'booster':['dart'], 'min_split_loss':[0,0.01,0.1],\n                   'max_depth':[3,4,5,6,7,8,9]}\n              ]\ngrid_xg = GridSearchCV(estimator = xgb,  \n                           param_grid = params,\n                           scoring = 'f1', #F1 because false positives and false negatives are crucial\n                           cv = 5,\n                           verbose=0)\ngrid_xg.fit(X_scaled,y)\n\ny_pred = grid_xg.predict(X_test_scaled)\ncm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm,annot=True,fmt = 'g')\n'''","127596ed":"#Getting Best Scores and Best Parameters\n#print(\"Best Score: \", grid_xg.best_score_) #0.839683048346517\n#accuracy = round(accuracy_score(y_test,y_pred),4) #0.9076\n#print(\"Accuracy: \",accuracy)# 0.9076\n#print(\"Best Parameters : \", grid_xg.best_params_) #  {'booster': 'gbtree', 'learning_rate': 0.1, 'max_depth': 3, 'min_split_loss': 0}","9e710600":"#Creating a model according to best parameters\nxgb = XGBClassifier(booster = 'gbtree',learning_rate = 0.1,max_depth = 3,\n                             min_split_loss = 0,random_state=0,verbosity = 0,use_label_encoder = False)\nxgb.fit(X_train_scaled,y_train)\ny_pred = xgb.predict(X_test_scaled)\n\ncm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm, annot=True)\naccuracy = round(accuracy_score(y_test,y_pred),4)\nprint(\"Accuracy: \",accuracy)\nf1 = f1_score(y_test,y_pred)\nprint(\"F1 Score:\\n \",f1)\nreport = classification_report(y_test,y_pred)\nprint(\"Classification Report:\\n \",report)","ff4a682f":"import tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Activation, Dropout, BatchNormalization\nfrom keras import metrics\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom tensorflow.keras import callbacks","49cfc8c9":"#Simple Model\nsimpleModel = Sequential(\n    [\n        Dense(units = 15, activation=\"relu\",input_dim = 15),\n        Dense(7, activation=\"relu\"),\n        Dense(4,activation = 'relu'),\n        Dense(1,activation = 'sigmoid') #output layer\n    ]\n)\nsimpleModel.compile(loss = 'binary_crossentropy',optimizer = 'adam', metrics = ['binary_accuracy'])\n\nsimpleHistory = simpleModel.fit(X_train_scaled,y_train,epochs = 1000 ,batch_size = 100,verbose = 0)","6d8743b3":"# summarize history for accuracy\nplt.plot(simpleHistory.history['binary_accuracy'])\n#plt.plot(history.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\n#plt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(simpleHistory.history['loss'])\n#plt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\n#plt.legend(['train', 'test'], loc='upper left')\nplt.show()","ad704516":"y_pred = simpleModel.predict(X_test_scaled)\n#print(y_pred)\ny_pred = y_pred>0.1\n#print(y_pred)\ncm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm, annot=True)\naccuracy = round(accuracy_score(y_test,y_pred),4)\nprint(\"Accuracy: \",accuracy)\nf1 = f1_score(y_test,y_pred)\nprint(\"F1 Score:\\n \",f1)\nreport = classification_report(y_test,y_pred)\nprint(\"Classification Report:\\n \",report)","e2950b9a":"def baseline_model():\n    model = Sequential(\n    [\n        Dense(units = 15, activation='swish',input_dim = 15),\n        Dense(10, activation=\"swish\"),\n        Dropout(0.1),\n        Dense(7,activation = 'swish'),\n        Dropout(0.2),\n        Dense(1,activation = 'sigmoid') #output layer\n    ]\n    )\n    model.compile(loss = 'binary_crossentropy',optimizer = 'adam', metrics = metrics.BinaryAccuracy(threshold = 0.6))\n    return model\n\nmodel  = baseline_model()\n\n#Using early stopping to counter overfitting\nearlystopping = callbacks.EarlyStopping(monitor='val_loss',\n                                        mode='min',\n                                        verbose=0,\n                                        patience=30) #patience is number of epochs to wait after min monitoring variable\n\nhistory = model.fit(X_train_scaled,y_train,validation_data = (X_test_scaled,y_test),\n                    epochs = 500 ,batch_size = 200,verbose = 0,callbacks =[earlystopping])\n","f55ae98e":"# summarize history for accuracy\nplt.plot(history.history['binary_accuracy'])\nplt.plot(history.history['val_binary_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n# summarize history for loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\ny_pred = model.predict(X_test_scaled)\n#print(y_pred)\n\ni = 0\nmaxacc = 0;\nbestthres = 0;\nwhile i <1:\n    y_pred_thres = y_pred>i\n    accuracy  = accuracy_score(y_test, y_pred_thres)\n    if maxacc< accuracy:\n        maxacc = accuracy\n        bestthres = i;\n    i=i+0.05\nprint(\"Best Accuracy: \",maxacc,\"at threshold: \",bestthres)\n\ny_pred = y_pred>bestthres\n\ncm = confusion_matrix(y_test,y_pred)\nsns.heatmap(cm, annot=True,fmt='g')\naccuracy = round(accuracy_score(y_test,y_pred),4)\nprint(\"Accuracy: \",accuracy)\nf1 = f1_score(y_test,y_pred)\nprint(\"F1 Score:\\n \",f1)\nreport = classification_report(y_test,y_pred)\nprint(\"Classification Report:\\n \",report)","234f870d":"#K-Fold Cross Validation\nestimator = KerasClassifier(build_fn=baseline_model, epochs=500, batch_size=200,verbose = 0)\n\nkfold = KFold(n_splits=10, shuffle=True)\nresults = cross_validate(estimator, X_scaled, y, cv=kfold,scoring = ['accuracy','f1_macro','precision','recall'],\n                         fit_params={'callbacks':earlystopping})\nprint('Accuracy: %.4f (std: %.3f)' % (np.mean(results['test_accuracy']), np.std(results['test_accuracy'])))\nprint('F1 Score : %.4f (std: %.3f)' % (np.mean(results['test_f1_macro']) ,np.std(results['test_f1_macro'])))\nprint('Precision : %.4f' % (np.mean(results['test_precision'])))\nprint('Recall : %.4f' % (np.mean(results['test_recall'])))","7f10ef7e":"<a id=\"EDA\"><\/a>  \n### Exploratory Data Analysis\n<a id=\"categorical\"><\/a>  \n#### Categorical Variables","f9cde4e8":"<a id=\"lr\"><\/a>  \n#### Logistic Regression","ef70b026":"<a id=\"understand\"><\/a>  \n#### Understanding what variables in the dataset means and their types\n  1. Age - Age of the Patient - **Numerical**\n  2. Sex - Gender of the Patient - **Categorical**\n        * M - Male\n        * F - Female\n  3. ChestPainType - Floor is made of floor - **Categorical**\n        * TA - [Typical Angina](https:\/\/www.ncbi.nlm.nih.gov\/pmc\/articles\/PMC5680106) - Substernal chest pain precipitated by physical exertion or emotional stress\n        * ATA - [ATypical Angina](https:\/\/www.ncbi.nlm.nih.gov\/medgen\/149267) Angina pectoris which does not have associated classical symptoms of chest pain. Symptoms - weakness, nausea, or sweating\n        * NAP - [Non-Anginal Chest Pain](https:\/\/my.clevelandclinic.org\/health\/diseases\/15851-gerd-non-cardiac-chest-pain) - Pain in the chest that is NOT caused by Heart Disease or Heart Attack\n        * ASY - [Asymptomatic](https:\/\/www.mayoclinic.org\/diseases-conditions\/heart-attack\/expert-answers\/silent-heart-attack\/faq-20057777) - No symptoms\n  4. RestingBP - [Resting Blood Pressure (mm\/Hg)](https:\/\/www.medicinenet.com\/blood_pressure_chart_reading_by_age\/article.htm) - **Numerical**\n  5. Cholesterol - [Serum Cholesterol (mm\/dl)](https:\/\/www.medicalnewstoday.com\/articles\/321519) - **Numerical**\n  6. FastingBS - [Fasting Blood Sugar](https:\/\/www.mayoclinic.org\/diseases-conditions\/diabetes\/diagnosis-treatment\/drc-20371451) - **Categorical (1: if FastingBS > 120 mg\/dl, 0: otherwise)**\n  7. RestingECG - Resting ElectroCardiogram Results - **Categorical**\n        * Normal - [Normal ECG Reading](https:\/\/ecgwaves.com\/topic\/ecg-normal-p-wave-qrs-complex-st-segment-t-wave-j-point\/) \n        * ST - [Abnormality in ST-T Wave Part of ECG](https:\/\/www.healio.com\/cardiology\/learn-the-heart\/ecg-review\/ecg-interpretation-tutorial\/68-causes-of-t-wave-st-segment-abnormalities) \n        * LVH - [Probable or definite Left Ventricular hypertrophy](https:\/\/www.healio.com\/cardiology\/learn-the-heart\/ecg-review\/ecg-interpretation-tutorial\/68-causes-of-t-wave-st-segment-abnormalities) \n  8. MaxHR - Maximum Heart Rate Achieved (60-202) - **Numeric**\n  9. ExerciseAngina - [Exercise Induced Angina](https:\/\/www.mayoclinic.org\/diseases-conditions\/angina\/symptoms-causes\/syc-20369373) - When your Heart wants more blood,but narrowed arteries slow down the blood flow - **Categorical (Yes\/No)**\n  10. Oldpeak - [ST Depression](https:\/\/en.wikipedia.org\/wiki\/ST_depression) - **Numerical**\n  11. ST_Slope - [Slope](https:\/\/pubmed.ncbi.nlm.nih.gov\/3739881\/) of the peak exercise ST Segment - **Categorical**\n        * Up - Upward Slope\n        * Flat - Slope is zero\n        * Down - Downward Slope\n  12. HeartDisease - Output Class - **Categorical (1: Heart Disease,0: Normal)**","997fd80f":"<a id=\"nn\"><\/a>  \n### Feedforward Artificial Neural Networks","80371b97":"**Maximum Heart Rate**","fbbf3df1":"<a id=\"predictive\"><\/a>  \n### Predictive Analysis","2d636187":"As we can see, there are 918 observations in the dataset. We don't have any missing data.","1cf4526f":"<a id=\"numerical\"><\/a>  \n#### Numerical Variables","92f1389b":"<a id=\"svm\"><\/a>  \n#### Support Vector Machines (Classification)","e6a6ae99":"Looks like we need more regularization.","19286091":"##### Simple Model","05631bd2":"<a id=\"xgb\"><\/a>  \n#### XGBoost Classifier","5254101c":"We can get following insights from Numerical variables,\n* People tend to get more susceptible to heart diseases as they get older.\n* Resting blood pressures of people with and without heart diseases are kinda similar. Maybe Resting BP is slightly higher for people with heart disease.\n* **There are unusually high amount of people with zero cholesterol level. This might be a error in data. We have to see about that while considering variables for Machine Learning.**\n* Maximum heart rate is lower (Highest count is around 120) for most people with heart diseases. Healthy (No heart Disease) people tend to have higher Maximum Heart Rate.\n* ST wave depression peak value is generally higher for people with heart disease.","52314443":"**Resting Blood Pressure**","fb794430":"##### Tuned Model","d5027aa9":"# Heart Failure EDA & Prediction using Logistic Regression, SVM, RandomForest, XGBoost, and Neural Networks with Python \n  \n  ## Table of Contents  \n* [Introduction](#introduction)\n* [Understanding the Dataset](#understand)\n* [Exploratory Data Analysis](#EDA)\n    - [Categorical Data](#categorical)\n    - [Numerical Data](#numerical)\n* [Outlier Detection](#outlier)\n* [Predictive Analysis](#predictive)\n    - [Data Preprocessing](#preprocessing)\n    - [Logistic Regression](#lr)\n    - [Support Vector Machines](#svm)\n    - [RandomForest Classifier](#rf)\n    - [XGBoost](#xgb)\n    - [FeedForward Neural Networks](#nn)\n* [Verdict](#verdict)\n\n\n**IMPORTANT**  - Some of the code (Gridsearch) have been commented out because it takes a long time to run. You can try those parts by uncommenting the codeblocks.\n\n<a id=\"introduction\"><\/a>  \n## Introduction","b5b49f73":"<a id=\"outlier\"><\/a>  \n#### Outlier Detection\nNow, We can try to find Outliers from Numerical Variables and determine whether they should be removed or not.","1d3b1c5b":"<a id=\"rf\"><\/a>  \n#### Random Forest Classifier","cc458dd3":"<a id=\"verdict\"><\/a>  \n### Verdict   \n  \n  With these results, It's clear that using neural networks might not be the best idea for this dataset. This is probably due to the limited amount of data we have. Most of the models performs pretty similar to each other. XGBoost (Grid Search) acheives the best accuracy but f1 score is not optimal. Feedforward Neural Networks model acheives better F1 Scores.","5e3306e0":"[Swish](https:\/\/medium.com\/@neuralnets\/swish-activation-function-by-google-53e1ea86f820) Activation function performs slightly better than ReLU.","66150d69":"##### Importing the dataset","3d204a57":"We can see that there is a slight imbalance in our dataset. We'll use methods such as balancing the dataset by removing excess observations (Results in loss of data, Using performance metrics such as precision, Recall, F1-Score other than accuracy, etc..","633cdffc":"**Cholesterol**","cb970208":"##### Decoding these plots can be somewhat tricky.  \nWe had more data for people with heart disease than not. So, small differences between two hues are ignored. After carefully analysing these plots, we can get following insights,\n* People with ST-T wave abnormalities are more likely to have a heart disease.\n* There is an alarmingly high amount of patients with Asymptomatic heart diseases.\n* People who get Exercise induced Angina are more likely to have a heart disease.\n* People with high fasting blood sugar levels are more likely to have a heart disease.\n* Men are more likely to get heart diseases than women.\n* People with flat or down ST_Slope are more likely to have a heart disease.","4bacab31":"<a id=\"preprocessing\"><\/a>  \n#### Data Preprocessing","0432cb1b":"**OldPeak**"}}