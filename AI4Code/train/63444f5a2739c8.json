{"cell_type":{"f4280692":"code","680e3b71":"code","e36ce6ed":"code","3a8d6b71":"code","a6fbaaf8":"code","c1f68a1b":"code","11087689":"code","aff7e324":"code","0f6f5355":"code","2b9e940f":"code","87184fa2":"code","835d380b":"code","eaa551e0":"code","0f75689a":"code","0ba7ebb6":"code","66ca4ba8":"code","8ec4822a":"code","9e12f7f2":"code","75671563":"code","7665e7e8":"code","e95f9ac1":"code","4bb4c6bf":"code","ae2f680c":"code","db9cc6c6":"code","5af52f64":"code","184e67f7":"code","18d4d9c1":"code","06a0404d":"code","0648cf33":"code","7ddc64ca":"code","8011bfdc":"code","9f02ec96":"code","3910ac7e":"code","0df8997e":"code","aebe650a":"code","a6f0ef35":"code","9f822a31":"code","ca463dae":"code","f90c9935":"code","36ddfe16":"code","f841d19a":"code","a891117f":"code","33d502dd":"markdown","4bfa36f0":"markdown","c38f6a9f":"markdown","802b6b15":"markdown","8a1008de":"markdown","31224125":"markdown","ebd5d327":"markdown","10fc0e12":"markdown","f1d7f282":"markdown","7e50662a":"markdown","3474b66f":"markdown","185fa4d7":"markdown","36418904":"markdown","2c3df294":"markdown","906ca2c2":"markdown","8a96829d":"markdown","33035bd8":"markdown","a306611a":"markdown","4a2edc29":"markdown","fa8c6ae6":"markdown","f486e4bd":"markdown","a9f88e8c":"markdown","3fb66a05":"markdown","867bfa0e":"markdown","9cd8b41c":"markdown","bcf33e9f":"markdown","77981c61":"markdown","3b89fda1":"markdown","bd037035":"markdown","1ea63c4a":"markdown","76d0d1f2":"markdown","c91cefb1":"markdown","91567cad":"markdown","92c59cb6":"markdown","3bdbcb6d":"markdown","0240783f":"markdown","3f976b39":"markdown","2a0234eb":"markdown","348d2b0c":"markdown","0bda9e99":"markdown","d4503582":"markdown"},"source":{"f4280692":"!nvidia-smi","680e3b71":"!nvcc --version","e36ce6ed":"import sys\n!rsync -ah --progress ..\/input\/rapids\/rapids.0.14.0 \/opt\/conda\/envs\/rapids.tar.gz\n!cd \/opt\/conda\/envs\/ && tar -xzvf rapids.tar.gz > \/dev\/null\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\/site-packages\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\/python3.7\"] + sys.path\nsys.path = [\"\/opt\/conda\/envs\/rapids\/lib\"] + sys.path \n!rsync -ah --progress \/opt\/conda\/envs\/rapids\/lib\/libxgboost.so \/opt\/conda\/lib\/","3a8d6b71":"import cudf","a6fbaaf8":"import os\ntry:\n    import graphviz\nexcept ModuleNotFoundError:\n    os.system('conda install -c conda-forge graphviz -y')\n    os.system('conda install -c conda-forge python-graphviz -y')","c1f68a1b":"import dask; print('Dask Version:', dask.__version__)\nfrom dask.distributed import Client, LocalCluster\n\n\n# create a local cluster with 4 workers\nn_workers = 4\ncluster = LocalCluster(n_workers=n_workers)\nclient = Client(cluster)","11087689":"# show current Dask status\nclient","aff7e324":"def add_5_to_x(x):\n    return x + 5","0f6f5355":"from dask import delayed\n\n\naddition_operations = [delayed(add_5_to_x)(i) for i in range(n_workers)]\naddition_operations","2b9e940f":"total = delayed(sum)(addition_operations)\ntotal","87184fa2":"total.visualize()","835d380b":"from dask.distributed import wait\nimport time\n\n\naddition_futures = client.compute(addition_operations, optimize_graph=False, fifo_timeout=\"0ms\")\ntotal_future = client.compute(total, optimize_graph=False, fifo_timeout=\"0ms\")\nwait(total_future)  # this will give Dask time to execute the work","eaa551e0":"addition_futures","0f75689a":"print(total_future)\nprint(type(total_future))","0ba7ebb6":"addition_results = [future.result() for future in addition_futures]\nprint('Addition Results:', addition_results)","66ca4ba8":"addition_results = client.gather(addition_futures)\ntotal_result = client.gather(total_future)\nprint('Addition Results:', addition_results)\nprint('Total Result:', total_result)","8ec4822a":"def sleep_1():\n    time.sleep(1)\n    return 'Success!'","9e12f7f2":"%%time\n\nfor _ in range(n_workers):\n    sleep_1()","75671563":"%%time\n\n# define delayed execution graph\nsleep_operations = [delayed(sleep_1)() for _ in range(n_workers)]\n\n# use client to perform computations using execution graph\nsleep_futures = client.compute(sleep_operations, optimize_graph=False, fifo_timeout=\"0ms\")\n\n# collect and print results\nsleep_results = client.gather(sleep_futures)\nprint(sleep_results)","7665e7e8":"from dask.distributed import Client\nfrom dask_cuda import LocalCUDACluster\n\n\n# create a local CUDA cluster\ncluster = LocalCUDACluster()\nclient = Client(cluster)\nclient","e95f9ac1":"import cudf; print('cuDF Version:', cudf.__version__)\nimport numpy as np; print('NumPy Version:', np.__version__)\n\n\ndef load_data(n_rows):\n    df = cudf.DataFrame()\n    random_state = np.random.RandomState(43210)\n    df['key'] = random_state.binomial(n=1, p=0.5, size=(n_rows,))\n    df['value'] = random_state.normal(size=(n_rows,))\n    return df","4bb4c6bf":"def head(dataframe):\n    return dataframe.head()","ae2f680c":"# define the number of workers\nn_workers = 1  # feel free to change this depending on how many GPUs you have\n\n# define the number of rows each dataframe will have\nn_rows = 125000000  # we'll use 125 million rows in each dataframe","db9cc6c6":"from dask.delayed import delayed\n\n\n# create each dataframe using a delayed operation\ndfs = [delayed(load_data)(n_rows) for i in range(n_workers)]\ndfs","5af52f64":"head_dfs = [delayed(head)(df) for df in dfs]\nhead_dfs","184e67f7":"from dask.distributed import wait\n\n\n# use the client to compute - this means create each dataframe and take the head\nfutures = client.compute(head_dfs)\nwait(futures)  # this will give Dask time to execute the work before moving to any subsequently defined operations\nfutures","18d4d9c1":"# collect the results\nresults = client.gather(futures)\nresults","06a0404d":"# let's inspect the head of the first dataframe\nprint(results[0])","0648cf33":"def length(dataframe):\n    return dataframe.shape[0]","7ddc64ca":"lengths = [delayed(length)(df) for df in dfs]\nlengths","8011bfdc":"total_number_of_rows = delayed(sum)(lengths)","9f02ec96":"total_number_of_rows.visualize()","3910ac7e":"# use the client to compute the result and wait for it to finish\nfuture = client.compute(total_number_of_rows)\nwait(future)\nfuture","0df8997e":"# collect result\nresult = client.gather(future)\nresult","aebe650a":"def groupby(dataframe):\n    return dataframe.groupby('key')['value'].mean()","a6f0ef35":"groupbys = [delayed(groupby)(df) for df in dfs]","9f822a31":"# use the client to compute the result and wait for it to finish\ngroupby_dfs = client.compute(groupbys)\nwait(groupby_dfs)\ngroupby_dfs","ca463dae":"results = client.gather(groupby_dfs)\nresults","f90c9935":"for i, result in enumerate(results):\n    print('cuDF DataFrame:', i)\n    print(result)","36ddfe16":"import dask_cudf; print('Dask cuDF Version:', dask_cudf.__version__)\n\n\n# create a distributed cuDF DataFrame using Dask\ndistributed_df = dask_cudf.from_delayed(dfs)\nprint('Type:', type(distributed_df))\ndistributed_df","f841d19a":"result = distributed_df.groupby('key')['value'].mean().compute()\nresult","a891117f":"print(result)","33d502dd":"The above output shows a list of several `Delayed` objects. An important thing to note is that the workers aren't actually executing these results - we're just defining the execution graph for our client to execute later. The `delayed` function wraps our function `add_5_to_x` and returns a `Delayed` object. This ensures that this computation is in fact \"delayed\" - or lazily evaluated - and not executed on the spot i.e. when we define it.\n\nNext, let's sum each one of these intermediate results. We can accomplish this by wrapping Python's built-in `sum` function using our `delayed` function and storing this in a variable called `total`.","4bfa36f0":"Using the `graphviz` library, we can use the `visualize` method of a `Delayed` object to visualize our current graph.","c38f6a9f":"<a id=\"dask\"><\/a>\n## Introduction to Dask\n\nDask is a library the allows for parallelized computing. Written in Python, it allows one to compose complex workflows using large data structures like those found in NumPy, Pandas, and cuDF. In the following examples and notebooks, we'll show how to use Dask with cuDF to accelerate common ETL tasks as well as build and train machine learning models like Linear Regression and XGBoost.\n\nTo learn more about Dask, check out the documentation here: http:\/\/docs.dask.org\/en\/latest\/\n\n#### Client\/Workers\n\nDask operates by creating a cluster composed of a \"client\" and multiple \"workers\". The client is responsible for scheduling work; the workers are responsible for actually executing that work. \n\nTypically, we set the number of workers to be equal to the number of computing resources we have available to us. For CPU based workflows, this might be the number of cores or threads on that particlular machine. For example, we might set `n_workers = 8` if we have 8 CPU cores or threads on our machine that can each operate in parallel. This allows us to take advantage of all of our computing resources and enjoy the most benefits from parallelization.\n\nOn a system with one or more GPUs, we usually set the number of workers equal to the number of GPUs available to us. Dask is a first class citizen in the world of General Purpose GPU computing and the RAPIDS ecosystem makes it very easy to use Dask with cuDF and XGBoost. \n\nBefore we get started with Dask, we need to setup a Local Cluster of workers to execute our work and a Client to coordinate and schedule work for that cluster. As we see below, we can inititate a `cluster` and `client` using only few lines of code.","802b6b15":"Next, let's see what CUDA version we have:","8a1008de":"You can also see the status and more information at the Dashboard, found at `http:\/\/<ip_address>\/status`. This can be ignored now since this is pointing to local machine.\n\nWith our client and workers setup, it's time to execute our first program in parallel. We'll define a function called `add_5_to_x` that takes some value `x` and adds 5 to it.","31224125":"We'll define our operation on the dataframes we've created:","ebd5d327":"Using Dask, we see that this whole process takes a little over a second - each worker is executing in parallel!","10fc0e12":"The dask_cudf API closely mirrors the cuDF API. We can use a groupby similar to how we would with cuDF - but this time, our operation is distributed across multiple GPUs!","f1d7f282":"And then use Python's built-in sum function to sum all of these lengths.","7e50662a":"We'll define the number of workers as well as the number of rows each dataframe will have.","3474b66f":"Awesome! We just wrote our first distributed workflow.\n\nTo confirm that Dask is truly executing in parallel, let's define a function that sleeps for 1 second and returns the string \"Success!\". In serial, this function should take our 4 workers around 4 seconds to execute.","185fa4d7":"We'll also define a function head that takes a cudf.DataFrame and returns the first 5 rows.","36418904":"We'll create each dataframe using the delayed operator.","2c3df294":"We see that our results are a list of cuDF DataFrames, each having 2 columns and 5 rows. Let's inspect the first dataframe:","906ca2c2":"\n\nAs before, we see that the result is a list of Delayed objects - an important thing to note is that our \"key\", or unique identifier for each operation, has changed. You should see the name of the function head followed by a hash sign. For example, one might see:\n\n [Delayed('head-8e946db2-feaf-4e79-99ab-f732b6e28461')\n Delayed('head-eb06bc77-9d5c-4a47-8c01-b5b36710b727')]\n\nAgain, nothing has been computed - let's compute the results and execute the workflow using the client.compute() method.\n","8a96829d":"That's all there is to it! We can define even more complex operations and workflows using cuDF DataFrames by using the delayed, wait, client.submit(), and client.gather() workflow.\n\nHowever, there can sometimes be a drawback from using this pattern. For example, consider a common operation such as a groupby - we might want to group on certain keys and aggregate the values to compute a mean, variance, or even more complex aggregations. Each dataframe is located on a different GPU - and we're not guaranteed that all of the keys necessary for that groupby operation are located on a single GPU i.e. keys may be scattered across multiple GPUs.\n\nTo make our problem even more concrete, let's consider the simple operation of grouping on our key column and calculating the mean of the value column. To sovle this problem, we'd have to sort the data and transfer keys and their associated values from one GPU to another - a tricky thing to do using the delayed pattern. In the example below, we'll show an example of this issue with the delayed pattern and motivate why one might consider using the dask_cudf API.\n\nFirst, let's define a function groupby that takes a cudf.DataFrame, groups by the key column, and calculates the mean of the value column.","33035bd8":"<a id=\"setup\"><\/a>\n## Setup and install RAPIDs\n","a306611a":"We see that our computation has finished - our result is of type int. We can collect our result using the client.gather() method.","4a2edc29":"We'll then execute that operation:","fa8c6ae6":"We'll apply the function groupby to each dataframe using the delayed operation.","f486e4bd":"We see the result of this operation is a list of Delayed objects. It's important to note that these operations are \"delayed\" - nothing has been computed yet, meaning our data has not yet been created!\n\nWe can apply the head function to each of our \"delayed\" dataframes.","a9f88e8c":"The graph can be read from bottom to top. We see that for each worker, we will first execute the load_data function to create each dataframe. Then the function length will be applied to each dataframe; the results from these operations on each worker will then be combined into a single result via the sum function.\n\nLet's now execute our workflow and compute a value for the total_number_of_rows variable.","3fb66a05":"We can see from the above output that our `addition_futures` variable is a list of `Future` objects - not the \"actual results\" of adding 5 to each of `[0, 1, 2, 3]`. These `Future` objects are a promise that at one point a computation will take place and we will be left with a result. Dask is responsible for fulfilling that promise by delegating that task to the appropriate Dask worker and collecting the result.\n\nLet's take a look at our `total_future` object:","867bfa0e":"As expected, our process takes about 4 seconds to run. Now let's execute this same workflow in parallel using Dask.","9cd8b41c":"<a id=\"conclusion\"><\/a>\n## Conclusion\n\nIn this tutorial, we learned how to use Dask with basic Python primitives like integers and strings.\n\nTo learn more about RAPIDS, be sure to check out: \n\n* [Open Source Website](http:\/\/rapids.ai)\n* [GitHub](https:\/\/github.com\/rapidsai\/)\n* [Press Release](https:\/\/nvidianews.nvidia.com\/news\/nvidia-introduces-rapids-open-source-gpu-acceleration-platform-for-large-scale-data-analytics-and-machine-learning)\n* [NVIDIA Blog](https:\/\/blogs.nvidia.com\/blog\/2018\/10\/10\/rapids-data-science-open-source-community\/)\n* [Developer Blog](https:\/\/devblogs.nvidia.com\/gpu-accelerated-analytics-rapids\/)\n* [NVIDIA Data Science Webpage](https:\/\/www.nvidia.com\/en-us\/deep-learning-ai\/solutions\/data-science\/)","bcf33e9f":"Again, we see that this is an object of type `Future` as well as metadata about the status of the request (i.e. whether it has finished or not), the type of the result, and a key associated with that operation. To collect and print the result of each of these `Future` objects, we can call the `result()` method.","77981c61":"This isn't exactly what we wanted though - ideally, we'd get one dataframe where for each unique key (0 and 1), we get the mean of the value column.\n\nWe can use the dask_cudf API to help up solve this problem. First we'll import the dask_cudf library and then use the dask_cudf.from_delayed function to convert our list of delayed dataframes to an object of type dask_cudf.core.DataFrame. We'll use this object - distributed_df - along with the dask_cudf API to perform that \"tricky\" groupby operation.","3b89fda1":"Lastly, let's examine our result!","bd037035":"## Install graphviz\nThe visualizations in this notebook require graphviz.  Your environment may not have it installed, but don't worry! If you don't, we're going to install it now.  This can take a little while, so sit tight.","1ea63c4a":"Next, we'll iterate through our `n_workers` and create an execution graph, where each worker is responsible for taking its ID and passing it to the function `add_5_to_x`. For example, the worker with ID 2 will take its ID and pass it to the function `add_5_to_x`, resulting in the value 7.","76d0d1f2":"## Dask Cudf","c91cefb1":"Let's inspect the `client` object to view our current Dask status. We should see the IP Address for our Scheduler as well as the the number of workers in our Cluster. ","91567cad":"\n\nWe see that our results are a list of futures. Each object in this list tells us a bit information about itself: the status (pending, error, finished), the type of the object, and the key (unique identifief).\n\nWe can use the client.gather method to collect the results of each of these futures.\n","92c59cb6":"At this point, total_number_of_rows hasn't been computed yet. But we can still visualize the graph of operations we've defined using the visualize() method.","3bdbcb6d":"Let's inspect the output of each call to `client.compute`:","0240783f":"<a id=\"introduction\"><\/a>\n## Introduction to Dask\n#### by Paul Hendricks\n#### modified by Beniel Thileepan \n-------\n\nThis work is modified inorder to run in Kaggle with additional rapids dataset.integers and strings.\n\nIn this notebook, we will show how to work with cuDF DataFrames in RAPIDS (https:\/\/www.kaggle.com\/cdeotte\/rapids)\n\n**Table of Contents**\n\n* [Introduction to Dask](#introduction)\n* [Setup](#setup)\n* [Introduction to Dask](#dask)\n* [Conclusion](#conclusion)","3f976b39":"As we mentioned before, none of these results - intermediate or final - have actually been compute. We can compute them using the `compute` method of our `client`.","2a0234eb":"Now we see the results that we want from our addition operations. We can also use the simpler syntax of the `client.gather` method to collect our results.","348d2b0c":"That was a pretty simple example. Let's see how we can use this perform a more complex operation like figuring how many total rows we have across all of our dataframes. We'll define a function called length that will take a cudf.DataFrame and return the first value of the shape attribute i.e. the number of rows for that particular dataframe.","0bda9e99":"Let's start by creating a local cluster of workers and a client to interact with that cluster.","d4503582":"We'll define a function called load_data that will create a cudf.DataFrame with two columns, key and value. The column key will be randomly filled with either a 0 or a 1, with 50% probability of either number being selected. The column value will be randomly filled with numbers sampled from a normal distribution."}}