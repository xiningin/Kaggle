{"cell_type":{"14f8f115":"code","15cd076e":"code","76e309f9":"code","747c1acd":"code","f6a79889":"code","336cd88d":"code","1f61b48f":"code","cc8327a7":"code","8db3c116":"code","3e0389ba":"code","8fb77a53":"code","9b27e8fa":"code","303c2e82":"code","4db29c0e":"code","10e49faf":"code","94ebca55":"code","10a78db8":"code","35e02eab":"code","61dc15e7":"code","9360b09f":"code","d988b209":"code","fac45e97":"code","3c1d4520":"code","c21fd005":"code","5882ec88":"code","e600edc4":"code","54ee5d99":"code","4d13adf4":"code","fce91517":"code","eb4a978c":"code","b22c1844":"code","e32545e3":"code","b031d83c":"code","636415ad":"code","3b3e623b":"code","9a7aead8":"code","73306b04":"code","7e9595f2":"code","40f4b2a5":"markdown","d1e22633":"markdown","7eb32bbd":"markdown","47a99fe9":"markdown","9208028d":"markdown","b65cd51c":"markdown","9d1d40bd":"markdown","c400d571":"markdown","3712d087":"markdown","fdcfebfd":"markdown","970555cd":"markdown","344e6145":"markdown","446d70d2":"markdown","e9b90af8":"markdown","00f826ff":"markdown"},"source":{"14f8f115":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\n#for dirname, _, filenames in os.walk('\/kaggle\/input'):#\n#for filename in filenames:\n#        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","15cd076e":"! pip install ..\/input\/pytorch16gpu\/torch-1.6.0cu101-cp37-cp37m-linux_x86_64.whl","76e309f9":"!pip install ..\/input\/iterative-stratification\/iterative_stratification-0.1.6-py3-none-any.whl\n","747c1acd":"import torch\nfrom torch import nn\nfrom torch.utils.data import DataLoader, Dataset\nimport torch.optim as optim\nimport torch.nn.functional as F\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import StratifiedKFold\nimport numpy as np\nimport os\nimport random\nimport sys\nos.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\nfrom tqdm import tqdm\nfrom sklearn.metrics import log_loss","f6a79889":"def seed_everything(seed_value):\n    random.seed(seed_value)\n    np.random.seed(seed_value)\n    torch.manual_seed(seed_value)\n    os.environ['PYTHONHASHSEED'] = str(seed_value)\n    \n    if torch.cuda.is_available(): \n        torch.cuda.manual_seed(seed_value)\n        torch.cuda.manual_seed_all(seed_value)\n        torch.backends.cudnn.deterministic = True\n        torch.backends.cudnn.benchmark = False\n        \nseed_everything(42)","336cd88d":"\n\ntrain = pd.read_csv('..\/input\/lish-moa\/train_features.csv')\ntrain_targets_scored = pd.read_csv('..\/input\/lish-moa\/train_targets_scored.csv')\ntrain_targets_nonscored = pd.read_csv('..\/input\/lish-moa\/train_targets_nonscored.csv')\ntest_features = pd.read_csv('..\/input\/lish-moa\/test_features.csv')\nsubmission = pd.read_csv('..\/input\/lish-moa\/sample_submission.csv')\n\nremove_vehicle = True\n\nif remove_vehicle:\n    train_features = train.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\n    train_targets_scored = train_targets_scored.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\n    train_targets_nonscored = train_targets_nonscored.loc[train['cp_type']=='trt_cp'].reset_index(drop=True)\nelse:\n    train_features = train\n\n","1f61b48f":"train_features","cc8327a7":"# check the number of categorical features for train\/test\ncol_features = list(train_features.columns)[1:]\nprint(train_features[col_features[0]].value_counts())\nprint(test_features[col_features[0]].value_counts())\nprint(train_features[col_features[1]].value_counts())\nprint(test_features[col_features[1]].value_counts())\nprint(train_features[col_features[2]].value_counts())\nprint(test_features[col_features[2]].value_counts())\n","8db3c116":"# check nan value and inf value ?\nprint(test_features[col_features].isna().sum().values.sum())\nprint(np.isinf(test_features[col_features[3:]].values).sum()) # only for numerical value\nprint(train_features[col_features].isna().values.sum())\nprint(np.isinf(train_features[col_features[3:]].values).sum()) # only for numerical value","3e0389ba":"train_targets_scored","8fb77a53":"# ratio for each label\n\ndef get_ratio_labels(df):\n    columns = list(df.columns)\n    columns.pop(0)\n    ratios = []\n    toremove = []\n    for c in columns:\n        counts = df[c].value_counts()\n        if len(counts) != 1:\n            ratios.append(counts[0]\/counts[1])\n        else:\n            toremove.append(c)\n    print(f\"remove {len(toremove)} columns\")\n    \n    for t in toremove:\n        columns.remove(t)\n    return columns, np.array(ratios).astype(np.int32)\n\ncolumns, ratios = get_ratio_labels(train_targets_scored)\nprint(ratios)\n","9b27e8fa":"train_targets_nonscored","303c2e82":"columns_nonscored, ratios_nonscored = get_ratio_labels(train_targets_nonscored)\nprint(ratios_nonscored)\nprint(len(columns_nonscored), len(ratios_nonscored))","4db29c0e":"print(train_features[col_features[3:]].max().values.max())\nprint(train_features[col_features[3:]].min().values.min())\nprint(test_features[col_features[3:]].min().values.min())\nprint(test_features[col_features[3:]].max().values.max())","10e49faf":"len(col_features[3:])","94ebca55":"\n\ndef transform_data(train, test, col, normalize=True, removed_vehicle=False):\n    \"\"\"\n        the first 3 columns represents categories, the others numericals features\n    \"\"\"\n    mapping = {\"cp_type\":{\"trt_cp\": 0, \"ctl_vehicle\":1},\n               \"cp_time\":{48:0, 72:1, 24:2},\n               \"cp_dose\":{\"D1\":0, \"D2\":1}}\n    \n    if removed_vehicle:\n        categories_tr = np.stack([ train[c].apply(lambda x: mapping[c][x]).values for c in col[1:3]], axis=1)\n        categories_test = np.stack([ test[c].apply(lambda x: mapping[c][x]).values for c in col[1:3]], axis=1)\n    else:\n        categories_tr = np.stack([ train[c].apply(lambda x: mapping[c][x]).values for c in col[:3]], axis=1)\n        categories_test = np.stack([ test[c].apply(lambda x: mapping[c][x]).values for c in col[:3]], axis=1)\n    \n    max_ = 10.\n    min_ = -10.\n   \n    if removed_vehicle:\n        numerical_tr = train[col[3:]].values\n        numerical_test = test[col[3:]].values\n    else:\n        numerical_tr = train[col[3:]].values\n        numerical_test = test[col[3:]].values\n    if normalize:\n        numerical_tr = (numerical_tr-min_)\/(max_ - min_)\n        numerical_test = (numerical_test-min_)\/(max_ - min_)\n    return categories_tr, categories_test, numerical_tr, numerical_test\n\ncol_features = list(train_features.columns)[1:]\ncat_tr, cat_test, numerical_tr, numerical_test = transform_data(train_features, test_features, col_features, normalize=False, removed_vehicle=remove_vehicle)\ntargets_tr = train_targets_scored[columns].values.astype(np.float32)\ntargets2_tr = train_targets_nonscored[columns_nonscored].values.astype(np.float32)\n\n","10a78db8":"class MOADataset(Dataset):\n    def __init__(self, x_cats, x_nums, y=None, y2=None):\n        self.cats = x_cats\n        self.nums = x_nums\n        self.y = y\n        self.y2 = y2\n        \n    def __len__(self):\n        return len(self.cats)\n\n    def __getitem__(self, index):\n        x1 = torch.as_tensor(self.cats[index], dtype=torch.long)\n        x2 = torch.as_tensor(self.nums[index], dtype=torch.float)\n        \n        if self.y is not None:\n            label = torch.as_tensor(self.y[index], dtype=torch.float)\n            if self.y2 is not None:\n                label2 = torch.as_tensor(self.y2[index], dtype=torch.float)\n                return x1, x2, label, label2\n            return  x1, x2, label\n        return  x1, x2","35e02eab":"class MOA_MLP(nn.Module):\n    def __init__(self, num_cats=[2,3,2] , cats_emb_size=[2,2,2], num_numericals=872, hidden_size_numericals=2048,\n                num_class=206, aux=None):\n        super().__init__()\n        self.cat_emb1 = nn.Embedding(num_cats[0], cats_emb_size[0], padding_idx=0)\n        self.cat_emb2 = nn.Embedding(num_cats[1], cats_emb_size[1], padding_idx=0)\n        #self.cat_emb3 = nn.Embedding(num_cats[2], cats_emb_size[2], padding_idx=0)\n\n        self.norms = nn.BatchNorm1d(sum(cats_emb_size) +num_numericals)\n        self.dropout = nn.Dropout(0.2)\n        \n        self.proj = nn.utils.weight_norm(nn.Linear(sum(cats_emb_size) + num_numericals, hidden_size_numericals))\n        self.norm_proj = nn.BatchNorm1d(hidden_size_numericals)\n        self.dropout2 = nn.Dropout(0.5)\n        \n        hd_1 = hidden_size_numericals\/\/2\n        hd_2 = hd_1\/\/2\n        self.extractor = nn.Sequential(nn.utils.weight_norm(nn.Linear(hidden_size_numericals, hd_1)),\n                                        nn.PReLU(),\n                                        nn.BatchNorm1d(hd_1),\n                                        nn.Dropout(0.5),\n                                        #nn.utils.weight_norm(nn.Linear(hd_1, hd_2)),\n                                        #nn.PReLU(),\n                                        #nn.BatchNorm1d(hd_2),\n                                        #nn.Dropout(0.5)\n        )\n        self.cls = nn.utils.weight_norm(nn.Linear(hd_1, num_class))\n        self.cls_aux=None\n        if aux is not None:\n            self.cls_aux = nn.utils.weight_norm(nn.Linear(hd_1, aux))\n    def forward(self, x_cat, x_num):\n        cat_features = torch.cat([self.cat_emb1(x_cat[:,0]), self.cat_emb2(x_cat[:,1])], dim=1)\n        all_features = torch.cat([cat_features, x_num], dim=1)\n        all_features = self.norms(all_features)\n        all_features = self.dropout(all_features)\n        \n        proj_features = self.proj(all_features)\n        proj_features = self.norm_proj(F.relu(proj_features))\n        proj_features = self.dropout2(proj_features )\n        \n        \n        \n        features_reduced = self.extractor(proj_features)\n        \n        outputs = self.cls(features_reduced)\n        if self.cls_aux is not None:\n            outputs2 = self.cls_aux(features_reduced)\n            return outputs, outputs2\n        return outputs","61dc15e7":"from torch.cuda.amp import GradScaler, autocast\ndef train_one_epoch(model, dataloader, cfg, optimizer, loss_fn, loss_fn_aux=None, accumulation=1, with_aux_class=False, verbose=True):\n    model.train()\n    scaler = GradScaler()\n    optimizer.zero_grad()\n    N = 0.\n    total_loss = 0.\n    t=tqdm(dataloader, disable=~verbose)\n    for i, batch in enumerate(t):\n        \n        x1 = batch[0]\n        x2 = batch[1]\n        labels = batch[2]\n        \n        x1 = x1.to(cfg.device)\n        x2 = x2.to(cfg.device)\n        labels = labels.to(cfg.device)\n        \n        if with_aux_class:\n            labels2 = batch[3]\n            labels2 = labels2.to(cfg.device)\n            \n        with autocast(cfg.use_apex):\n            if with_aux_class:\n                outputs, outputs2 = model(x1, x2)\n                loss1 = loss_fn(outputs, labels).mean(0).mean()\n                loss2 = loss_fn_aux(outputs2, labels2).mean(0).mean()\n                loss = loss1 + 0.5*loss2\n\n            else:\n                outputs = model(x1, x2)\n                loss = loss_fn(outputs, labels).mean(0).mean()\n        \n        N += len(x1)\n        total_loss += (loss.item() * len(x1))  \n        \n        if cfg.use_apex:\n            loss = loss\/accumulation\n            scaler.scale(loss).backward()\n        else:\n            loss = loss\/accumulation\n            loss.backward()\n\n\n\n        \n        if (i+1)%accumulation == 0 or i-1 == len(dataloader):\n            if cfg.use_apex:\n                scaler.step(optimizer)\n\n                # Updates the scale for next iteration.\n                scaler.update()\n                optimizer.zero_grad()\n            else:                \n                optimizer.step()\n                optimizer.zero_grad()\n\n\n            t.set_description(\"Loss : {0}\".format(total_loss\/N))\n            t.refresh()\n            \n            \ndef evals(model, dataloader, cfg, loss_fn, loss_fn_aux=None, verbose=True):\n    model.eval()\n    N = 0.\n    total_loss = 0.\n\n    y_preds = []\n    y_targets = []\n    t=tqdm(dataloader, disable=~verbose)\n    with torch.no_grad():\n        for i, batch in enumerate(t):\n\n            x1 = batch[0]\n            x2 = batch[1]\n            labels = batch[2]\n\n            x1 = x1.to(cfg.device)\n            x2 = x2.to(cfg.device)\n            labels = labels.to(cfg.device)\n\n            if cfg.with_aux_class:\n                labels2 = batch[3]\n                labels2 = labels2.to(cfg.device)\n\n            with autocast(cfg.use_apex):\n                if cfg.with_aux_class:\n                    outputs, outputs2 = model(x1, x2)\n                    loss1 = loss_fn(outputs, labels).mean(0).mean()\n                    loss2 = loss_fn_aux(outputs2, labels2).mean(0).mean()\n                    loss = loss1 + 0.5*loss2\n\n                else:\n                    outputs = model(x1, x2)\n                    loss = loss_fn(outputs, labels).mean(0).mean()\n\n            N += len(x1)\n            total_loss += (loss.item() * len(x1))  \n\n            t.set_description(\"Loss : {0}\".format(total_loss\/N))\n            t.refresh()\n            \n            y_preds.append(torch.sigmoid(outputs).detach().cpu().numpy())\n            y_targets.append(labels.detach().cpu().numpy())\n    y_preds = np.concatenate(y_preds, axis=0)\n    y_targets = np.concatenate(y_targets, axis=0)\n    score = log_loss_multi(y_targets, y_preds)\n    #print(\"Logloss = \", score)\n    return y_preds, y_targets, score\n\n\n\ndef inference_fn(model, dataloader, cfg,verbose=True):\n    model.eval()\n    N = 0.\n    \n    y_preds = []\n    with torch.no_grad():\n        for i, batch in enumerate(tqdm(dataloader, disable=~verbose)):\n\n            x1 = batch[0]\n            x2 = batch[1]\n            \n\n            x1 = x1.to(cfg.device)\n            x2 = x2.to(cfg.device)\n\n            \n\n            with autocast(cfg.use_apex):\n                if cfg.with_aux_class:\n                    outputs, outputs2 = model(x1, x2)\n                    \n                else:\n                    outputs = model(x1, x2)\n\n            \n            y_preds.append(torch.sigmoid(outputs).detach().cpu().numpy())\n    y_preds = np.concatenate(y_preds, axis=0)\n    return y_preds","9360b09f":"def log_loss_score(actual, predicted,  eps=1e-15):\n\n        \"\"\"\n        :param predicted:   The predicted probabilities as floats between 0-1\n        :param actual:      The binary labels. Either 0 or 1.\n        :param eps:         Log(0) is equal to infinity, so we need to offset our predicted values slightly by eps from 0 or 1\n        :return:            The logarithmic loss between between the predicted probability assigned to the possible outcomes for item i, and the actual outcome.\n        \"\"\"\n\n        \n        p1 = actual * np.log(predicted+eps)\n        p0 = (1-actual) * np.log(1-predicted+eps)\n        loss = p0 + p1\n\n        return -loss.mean()","d988b209":"np.log(0)*0","fac45e97":"def log_loss_multi(y_true, y_pred):\n    M = y_true.shape[1]\n    results = np.zeros(M)\n    for i in range(M):\n        results[i] = log_loss_score(y_true[:,i], y_pred[:,i])\n        \n    #print(results.min(), results.max())\n    #print(results)\n    return results.mean()\n        ","3c1d4520":"def train_fold(fold, model, tr_dataloader, val_dataloader, cfg, optimizer, reducer, loss_fn, loss_fn_aux=None, accumulation=1, with_aux_class=False):\n    best_score = np.inf\n    best_preds = None\n    best_targets = None\n    for e in range(cfg.EPOCHS):\n        train_one_epoch(model, tr_dataloader, cfg, optimizer, loss_fn, loss_fn_aux=loss_fn_aux, accumulation=accumulation, with_aux_class=with_aux_class, verbose=cfg.verbose)\n        preds, targets, score = evals(model, val_dataloader, cfg, loss_fn, loss_fn_aux=loss_fn_aux, with_aux_class=with_aux_class, verbose=cfg.verbose)\n        reducer.step(score)\n        if score < best_score:\n            print(\"## Epochs {0} : Improvement from {1} to {2}\".format(e, best_score, score))\n            best_score = score\n            best_preds = preds\n            best_targets= targets\n            torch.save(model.state_dict(), cfg.save_name + f\"_{fold}.pth\")\n    print(\"## FOLD {0} : best results : {1}\".format(fold, best_score))\n    return best_preds, best_targets, score","c21fd005":"def inference_fold(folds, model, test_loader,cfg):\n    preds = []\n    for fold in range(folds):\n        name = cfg.save_name + f\"_{fold}.pth\"\n        model.load_state_dict(torch.load(name))\n        p = inference_fn(model, test_loader, cfg, cfg.verbose,cfg.with_aux_class)\n        preds.append(p)\n    \n    return preds","5882ec88":"def check_targets(targets):\n    ### check if targets are all binary in training set\n    \n    for i in range(targets.shape[1]):\n        if len(np.unique(targets[:,i])) != 2:\n            return False\n    return True\n\ndef check_labels(A):\n    w = np.zeros(A.shape[1])\n    for i in range(A.shape[1]):\n        if len(np.unique(A[:,i])) == 2:\n            w[i] = 1\n    return w.reshape(1, -1)\n\n\nfrom sklearn.metrics import roc_auc_score\ndef auc_multi(y_true, y_pred):\n    M = y_true.shape[1]\n    results = np.zeros(M)\n    for i in range(M):\n        try:\n            results[i] = roc_auc_score(y_true[:,i], y_pred[:,i])\n        except:\n            pass\n    return results.mean()\n\n","e600edc4":"from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n","54ee5d99":"class Config(object):\n    def __init__(self):\n        self.num_class = targets_tr.shape[1]\n        self.aux_class = targets2_tr.shape[1]\n        self.use_apex=False\n        self.verbose=False\n        #\n        self.batch_size = 128\n        self.device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n        self.SPLITS = 10\n        # Parameters model\n        self.num_cats=[3+1,2+1] if remove_vehicle else [2+1,3+1,2+1] \n        self.cats_emb_size=[1]* cat_tr.shape[1] #to choose\n        self.num_numericals= numerical_tr.shape[1]\n        self.hidden_size_numericals=2048 # to choose\n        self.num_ensembling = 1\n        self.strategy = \"KFOLD\"\n        # save\n        self.save_name = \"..\/input\/moannweights\/MOAMLP-aux-raw\/MOA_mlp-KFOLD10_\"#\"..\/input\/moabaselinemlp\/MOA_mlp\"\ncfg = Config()\ncfg.with_aux_class = True if cfg.aux_class is not None else False\nprint(cfg.num_class, cfg.aux_class,cfg.with_aux_class)","4d13adf4":"loss_fn = nn.BCEWithLogitsLoss(reduction=\"none\")#, pos_weight=torch.as_tensor(ratios))\nloss_fn_aux = nn.BCEWithLogitsLoss(reduction=\"none\")#, pos_weight=torch.as_tensor(ratios))","fce91517":"test_dataset = MOADataset(cat_test, numerical_test)\ntest_dataloader =    DataLoader(test_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=2)\n","eb4a978c":"## model\nmodel = MOA_MLP(num_cats=cfg.num_cats , cats_emb_size=cfg.cats_emb_size, num_numericals=cfg.num_numericals, hidden_size_numericals=cfg.hidden_size_numericals,\n                num_class=cfg.num_class, aux=cfg.aux_class)\nprint(model)","b22c1844":"#p = inference_fold(cfg.SPLITS, model, test_dataloader,cfg)","e32545e3":"\n\nif cfg.strategy == \"KFOLD\":\n    oof_preds = []\n    oof_targets = []\n    scores = []\n    scores_auc = []\n    preds_test = []\n    masks = []\n    for seed in range(cfg.num_ensembling):\n        mskf = MultilabelStratifiedKFold(n_splits=cfg.SPLITS, random_state=42+seed, shuffle=True)\n        p = []\n        temp_mask = []\n        for j, (train_idx, val_idx) in enumerate(mskf.split(np.zeros(len(cat_tr)), targets_tr)):\n            print(\"FOLDS : \", j)\n\n            ## model\n\n            model = MOA_MLP(num_cats=cfg.num_cats , cats_emb_size=cfg.cats_emb_size, num_numericals=cfg.num_numericals, hidden_size_numericals=cfg.hidden_size_numericals,\n                        num_class=cfg.num_class, aux=cfg.aux_class).to(cfg.device)\n            ## Create dataset then dataloader\n            if cfg.with_aux_class:\n                val_dataset = MOADataset(cat_tr[val_idx], numerical_tr[val_idx], y=targets_tr[val_idx], y2=targets2_tr[val_idx])\n            else:\n                val_dataset = MOADataset(cat_tr[val_idx], numerical_tr[val_idx], y=targets_tr[val_idx] )\n            val_dataloader =    DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=2)\n            temp_mask.append(check_labels(targets_tr[train_idx]))\n            name = cfg.save_name + f\"_{j}_{seed}.pth\"\n            model.load_state_dict(torch.load(name, map_location=cfg.device))\n            ## train fold\n            preds , targets, score = evals(model, val_dataloader, cfg, loss_fn, loss_fn_aux=loss_fn_aux, verbose=True)\n            p.append(inference_fn(model, test_dataloader, cfg,verbose=False))\n            ## save oof to compute the CV later\n            oof_preds.append(preds)\n            oof_targets.append(targets)\n            scores.append(score)\n            scores_auc.append(auc_multi(targets,preds))\n        preds_test.append(np.array(p))\n        masks.append(np.stack(temp_mask))","b031d83c":"if cfg.strategy == \"KFOLD\":\n    oof_preds = np.concatenate(oof_preds).reshape(cfg.num_ensembling, -1, 206)\n    oof_targets = np.concatenate(oof_targets).reshape(cfg.num_ensembling, -1, 206)\n    scores_auc = np.array(scores_auc).reshape(cfg.num_ensembling, -1)\n\n\n\n    for i in range(oof_preds.shape[0]):\n        print(\"CV score : \", log_loss_multi(oof_targets[i,:], oof_preds[i,:]))\n        print(\"auc mean : \", sum(scores_auc[i,:])\/len(scores_auc[i,:]))\n\n    preds_test = np.stack(preds_test)\n    masks = np.stack(masks)","636415ad":"preds_test2 = preds_test*masks\npreds_test2 = preds_test2.sum(1).sum(0)\/masks.sum(1).sum(0).sum(0)","3b3e623b":"submission[columns] = preds_test2\nsubmission.loc[test_features['cp_type']=='ctl_vehicle', submission.columns[1:]] = 0","9a7aead8":"submission.to_csv(\"submission.csv\", index=False)","73306b04":"submission","7e9595f2":"\"\"\"\noof_preds = []\noof_targets = []\nscores = []\nmskf = MultilabelStratifiedKFold(n_splits=cfg.SPLITS, random_state=0)\n\nfor j, (train_idx, val_idx) in enumerate(mskf.split(np.zeros(len(cat_tr)), targets_tr)):\n    print(\"FOLDS : \", j)\n    \n    ## model\n    \n    model = MOA_MLP(num_cats=cfg.num_cats , cats_emb_size=cfg.cats_emb_size, num_numericals=cfg.num_numericals, hidden_size_numericals=cfg.hidden_size_numericals,\n                num_class=cfg.num_class, aux=cfg.aux_class)\n    optimizer = optim.Adam(model.parameters(), lr = 1e-3, amsgrad=True)\n    reducer = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3,  min_lr=1e-5, eps=1e-08, verbose=True)\n    ## Create dataset then dataloader\n    if cfg.with_aux_class:\n        train_dataset = MOADataset(cat_tr[train_idx], numerical_tr[train_idx], y=targets_tr[train_idx], y2=targets2_tr[train_idx])\n        val_dataset = MOADataset(cat_tr[val_idx], numerical_tr[val_idx], y=targets_tr[val_idx], y2=targets2_tr[val_idx])\n    else:\n        train_dataset = MOADataset(cat_tr[train_idx], numerical_tr[train_idx], y=targets_tr[train_idx])\n        val_dataset = MOADataset(cat_tr[val_idx], numerical_tr[val_idx], y=targets_tr[val_idx] )\n    train_dataloader =    DataLoader(train_dataset, batch_size=cfg.batch_size, shuffle=True, num_workers=2)\n    val_dataloader =    DataLoader(val_dataset, batch_size=cfg.batch_size, shuffle=False, num_workers=2)\n    \n    name = cfg.save_name + f\"_{j}.pth\"\n    model.load_state_dict(torch.load(name))\n    ## train fold\n    preds , targets, score = evals(model,  val_dataloader, cfg,  loss_fn, loss_fn_aux=loss_fn_aux,  with_aux_class=cfg.with_aux_class)\n    \n    ## save oof to compute the CV later\n    oof_preds.append(preds)\n    oof_targets.append(targets)\n    scores.append(score)\noof_preds = np.concatenate(oof_preds)\noof_targets = np.concatenate(oof_targets)\nprint(\"CV score : \", log_loss_multi(oof_targets, oof_preds))\nprint(scores)\n\"\"\"","40f4b2a5":"Check the different values than each category can take and if there are nan\/inf values","d1e22633":"# Source \n\n- Pytorch 1.6 : https:\/\/pytorch.org\/docs\/stable\/\n- iterative-stratification : https:\/\/github.com\/trent-b\/iterative-stratification for stratified K fold multilabel","7eb32bbd":"# Targets data","47a99fe9":"# Dataloader","9208028d":"# Data\n","b65cd51c":"we could later normalize our numerical value but we will see that for another version","9d1d40bd":"### seems that some optional labels have only one labels, so we discard them","c400d571":"# script","3712d087":"# Approach :\ntraining script : https:\/\/www.kaggle.com\/ludovick\/training-moa-baseline-mlp-kfold-10\nNeural Network to classify a multi labels tasks with pytorch\n- Stratified K Fold (10 folds)\n- BCE Loss\n- optional labels are used for the training, not for inference though (after filtering)\n- gradient accumulation (not tested yet)","fdcfebfd":"# save CSV","970555cd":"# Model","344e6145":"# Utils","446d70d2":"# Check validation","e9b90af8":"As we can see, the labels are hightly imbalanced, it may be necessary to use a weighted loss function to help the model ?","00f826ff":"# Dataset"}}