{"cell_type":{"e31b9796":"code","c8ad0913":"code","a8896030":"code","e2630975":"code","a86dd9f2":"code","705185ec":"code","d462fb99":"code","16bba066":"code","a70e9f3b":"code","d385dbef":"code","a9d36868":"code","41edc506":"code","1b6b054d":"code","b22aaef9":"code","9575491b":"code","b1cd4b94":"code","951542fc":"code","0a0148ca":"code","24f100fb":"code","349b4e6a":"code","c3195a22":"code","f19885c0":"code","1741796f":"code","d610790f":"code","db051f05":"code","282887f2":"code","4641818a":"code","e36d50a7":"code","98c49f24":"code","7835abf4":"code","0c76a969":"code","4aedc48a":"code","bb38a4e9":"code","cec251c3":"code","6e0e6507":"code","00321ab9":"code","1b64ceab":"code","6d6eeb4b":"code","5316f4b0":"code","1cac3bcf":"code","c6122c1b":"code","89d2345d":"code","0effb555":"code","d557177e":"code","14fbccab":"markdown","b5354a89":"markdown","9f60cdfa":"markdown","62a0d727":"markdown","6326348b":"markdown","3fdc2d21":"markdown","ab507854":"markdown","b1990db5":"markdown","f50ac1f9":"markdown","53291714":"markdown","e9b4b71c":"markdown","38eb6c6b":"markdown","7a51bfa2":"markdown","f3b6b822":"markdown","4d6d15ef":"markdown","097f64df":"markdown","486c1fe3":"markdown","d10e3663":"markdown","3326dedb":"markdown","72d3492b":"markdown","76a5368f":"markdown","f95306df":"markdown","f449bbb2":"markdown","cc5b3859":"markdown","5b3dc305":"markdown","4a5ea010":"markdown","492dc27d":"markdown","31f4c8e2":"markdown","3684c31f":"markdown","a30540dd":"markdown","0ddd37b2":"markdown","1c00eeda":"markdown","2d93d86d":"markdown","2a3c9ab9":"markdown","35df67e6":"markdown","797de337":"markdown","772eab83":"markdown","2fdc0e15":"markdown","af50ba2a":"markdown","b9642818":"markdown","c30bbe94":"markdown","d7bc69db":"markdown","0fc8fda9":"markdown","66b2ec8d":"markdown","8da8d620":"markdown","a9990bdc":"markdown","8c081cfc":"markdown","9e9305ed":"markdown","51e2f896":"markdown","80d47cb1":"markdown","d12adec0":"markdown","0ed0dbc4":"markdown"},"source":{"e31b9796":"#Standard libs\nimport numpy as np \nimport pandas as pd \n\n#Data Visualisation libs\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport scipy.stats as st\n\n#Feature engineering, metrics and modeling libs\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.preprocessing.imputation import Imputer\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor, GradientBoostingClassifier\nfrom sklearn.ensemble.partial_dependence import partial_dependence, plot_partial_dependence\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import accuracy_score\nfrom sklearn import metrics\n\n#Detect Missing values\nimport missingno as msno\n\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n","c8ad0913":"abalone = pd.read_csv('..\/input\/abalone.csv')","a8896030":"abalone.head()","e2630975":"abalone.info()","a86dd9f2":"abalone.describe()","705185ec":"abalone.shape","d462fb99":"len(abalone.isnull())","16bba066":"rings = abalone['Rings']\nplt.figure(1);plt.title('Normal')\nsns.distplot(rings,kde=False,fit=st.norm)\nplt.figure(2);plt.title('Johnson SU')\nsns.distplot(rings,kde=False,fit=st.johnsonsu)\nplt.figure(3);plt.title('Log Normal')\nsns.distplot(rings,kde=False,fit=st.lognorm)","a70e9f3b":"plt.hist(rings,color='green')\nplt.title('Histogram of rings')\nplt.xlabel('Number of Rings')\nplt.ylabel('count')","d385dbef":"numeric_features = abalone.select_dtypes(include=[np.number])\ncorrelation = numeric_features.corr()","a9d36868":"print(correlation['Rings'].sort_values(ascending=False))","41edc506":"plt.title('Correlation of numeric features', y=1,size=16)\nsns.heatmap(correlation,square=True,vmax=0.8)","1b6b054d":"cols = ['Rings','Shell weight','Diameter','Height','Length']\nsns.pairplot(abalone[cols],size=2,kind='scatter')\nplt.show()","b22aaef9":"sns.countplot(abalone['Sex'], palette=\"Set3\")\nplt.title('Count of the Gender of Abalone')","9575491b":"sns.countplot(abalone['Rings'])\nplt.title('Distribution of Rings')","b1cd4b94":"p = sns.FacetGrid(abalone, col=\"Sex\", hue=\"Sex\")\np=p.map(plt.scatter,\"Rings\", \"Shell weight\")\n","951542fc":"x = sns.FacetGrid(abalone,col=\"Sex\",hue=\"Sex\")\nx.map(plt.scatter, \"Rings\", \"Diameter\")","0a0148ca":"f = (abalone.loc[abalone['Sex'].isin(['M','F'])]\n      .loc[:,['Shell weight','Rings','Sex']])\n\nf = f[f[\"Rings\"] >= 8]\nf = f[f[\"Rings\"] < 23]\nsns.boxplot(x=\"Rings\",y=\"Shell weight\", hue='Sex',data=f)","24f100fb":"w = (abalone.loc[abalone['Sex'].isin(['I'])]\n    .loc[:,['Shell weight','Rings','Sex']])\nw = w[w[\"Rings\"] >= 8]\nw = w[w[\"Rings\"] < 23]\nsns.boxplot(x=\"Rings\",y=\"Shell weight\", hue='Sex',data=w)","349b4e6a":"fig, (ax1,ax2) = plt.subplots(nrows=1,ncols=2,figsize=(14,10))\n\nShellWeight_plot = pd.concat([abalone['Rings'],abalone['Shell weight']],axis=1)\nsns.regplot(x='Rings',y='Shell weight',data=ShellWeight_plot,scatter=True,fit_reg=True,ax=ax1)\n\nDiameter_plot = pd.concat([abalone['Rings'],abalone['Diameter']],axis=1)\nsns.regplot(x='Rings',y='Diameter',data=Diameter_plot,scatter=True,fit_reg=True,ax=ax2)\n","c3195a22":"from scipy import stats\nz= np.abs(stats.zscore(abalone.select_dtypes(include=[np.number])))\nprint(z)","f19885c0":"abalone_o = abalone[(z < 3).all(axis=1)]","1741796f":"print(\"Shape of Abalones with outliers: \"+ str(abalone.shape) , \n      \"Shape of Abalones without outliers: \" + str(abalone_o.shape))","d610790f":"low_cardinality_cols = [cname for cname in abalone_o.columns if\n                        abalone_o[cname].nunique() < 10 and \n                       abalone_o[cname].dtype == \"object\"]\nnumeric_cols = [cname for cname in abalone_o.columns if\n                                 abalone_o[cname].dtype in ['int64','float64']]\n\nmy_cols = low_cardinality_cols + numeric_cols\nabalone_predictors = abalone_o[my_cols]","db051f05":"abalone_predictors.dtypes.sample(7)","282887f2":"abalone_encoded_predictors = pd.get_dummies(abalone_predictors)","4641818a":"abalone_encoded_predictors.head(5)","e36d50a7":"abalone_encoded_predictors.shape","98c49f24":"cross_cols = ['Length','Diameter','Height','Whole weight','Shucked weight','Viscera weight','Shell weight','Sex_F','Sex_I','Sex_M']\nX = abalone_encoded_predictors[cross_cols]\ny = abalone_encoded_predictors.Rings\n\ndecision_pipeline = make_pipeline(DecisionTreeRegressor())\ndecision_scores = cross_val_score(decision_pipeline, X,y,scoring='neg_mean_absolute_error')\n\nprint('MAE %2f' %(-1 * decision_scores.mean()))","7835abf4":"dt_train_X,dt_test_X,dt_train_y,dt_test_y = train_test_split(X,y)","0c76a969":"def get_mae(max_leaf_nodes,dt_train_X,dt_test_X,dt_train_y,dt_test_y ):\n    model_pipeline = make_pipeline(DecisionTreeRegressor(max_leaf_nodes=max_leaf_nodes,random_state=0))\n    model_pipeline.fit(dt_train_X,dt_train_y)\n    preds_val = model_pipeline.predict(dt_test_X)\n    mae = mean_absolute_error(dt_test_y,preds_val)\n    return(mae)","4aedc48a":"for max_leaf_nodes in [5,50,500,5000]:\n    my_mae = get_mae(max_leaf_nodes,dt_train_X,dt_test_X,dt_train_y,dt_test_y)\n    print(\"Max leaf nodes: %d \\t\\t MAE: %d\" %(max_leaf_nodes,my_mae))","bb38a4e9":"decision_split_pipeline = make_pipeline(DecisionTreeRegressor(max_leaf_nodes=5))\ndecision_split_pipeline.fit(dt_train_X,dt_train_y)\ndecision_tree_prediction = decision_split_pipeline.predict(dt_test_X)\nprint(\"MAE: \" + str(mean_absolute_error(decision_tree_prediction,dt_test_y)))","cec251c3":"acc_decision = decision_split_pipeline.score(dt_test_X,dt_test_y)\nprint(\"Acc:\", acc_decision )","6e0e6507":"plt.scatter(dt_test_y,decision_tree_prediction,color='green')\nplt.xlabel('Actuals')\nplt.ylabel('Predictions')\nplt.title('Decision Tree: Actuals vs Predictions')\nplt.show()","00321ab9":"forest_pipeline = make_pipeline(RandomForestRegressor(random_state=1))\nforest_scores = cross_val_score(forest_pipeline, X,y,scoring=\"neg_mean_absolute_error\")\nprint('MAE %2f' %(-1 * forest_scores.mean()))","1b64ceab":"f_train_X,f_test_X,f_train_y,f_test_y = train_test_split(X,y)\nforest_split_pipeline = make_pipeline(RandomForestRegressor(random_state=1))\nforest_split_pipeline.fit(f_train_X,f_train_y)\nforest_predictions = forest_split_pipeline.predict(f_test_X)\nprint(\"Accuracy:\",forest_split_pipeline.score(f_test_X,f_test_y))\nprint(\"MAE:\",str(mean_absolute_error(forest_predictions,f_test_y)))\n","6d6eeb4b":"plt.scatter(f_test_y,forest_predictions,color='red')\nplt.xlabel('Actuals')\nplt.ylabel('Predictions')\nplt.title('Actuals vs Predictions')\nplt.show()","5316f4b0":"xgb_pipeline = make_pipeline(XGBRegressor())\nxgb_scores = cross_val_score(xgb_pipeline,X.as_matrix(),y.as_matrix(),scoring=\"neg_mean_absolute_error\")\nprint(\"MAE %2f\" %(-1 * xgb_scores.mean()) )","1cac3bcf":"train_X,test_X,train_y,test_y = train_test_split(X.as_matrix(),y.as_matrix(),test_size=0.25)","c6122c1b":"xgb_model = XGBRegressor()\nxgb_model.fit(train_X,train_y,verbose=False)\nxgb_preds = xgb_model.predict(test_X)\nprint(\"MAE: \" + str(mean_absolute_error(xgb_preds,test_y)))\nprint(\"Accuracy:\",xgb_model.score(test_X,test_y))","89d2345d":"xgb_model_II = XGBRegressor(n_estimators=1000,learning_rat=0.05)\nxgb_model_II.fit(train_X,train_y,early_stopping_rounds=5,\n             eval_set=[(test_X,test_y)],verbose=False)\nxgb_preds = xgb_model_II.predict(test_X)\nprint(\"MAE: \" + str(mean_absolute_error(xgb_preds,test_y)))\nprint(\"Accuracy:\",xgb_model_II.score(test_X,test_y))","0effb555":"plt.scatter(test_y,xgb_preds,color='blue')\nplt.xlabel('Actuals')\nplt.ylabel('Predictions')\nplt.title('Actuals vs Predictions')\nplt.show()","d557177e":"cols = ['Length','Diameter','Height']\nab_par_model = GradientBoostingRegressor()\nab_par_model.fit(X,y)\nmy_plots = plot_partial_dependence(ab_par_model,\n                                  features=[0,2],\n                                  X=X,\n                                  feature_names=cols,\n                                   grid_resolution=10)","14fbccab":"### Decision Trees\n\nDecision Trees allow us to use certain criteria which would be the models features to predict\/classifiy a target. The data is split into smaller sets based on a feature then it will run a prediction on that feature. \n\nYou can read more about them [here](https:\/\/towardsdatascience.com\/decision-trees-in-machine-learning-641b9c4e8052) and [here](https:\/\/medium.com\/@SeattleDataGuy\/what-is-a-decision-tree-algorithm-4531749d2a17).\n\nSkLearn offers two types of Decision Trees: a classifer and regressor. Because we are not trying to determine an event we will use the regressor. ","b5354a89":"This plots clearly shows where the model is lacking, if you look at the actuals the model does poorly in some cases with its predictions. ","9f60cdfa":"# Import packages & Data","62a0d727":"Now we can encode them with the `get_dummies()` method available to a Pandas Dataframe:","6326348b":"#  Train Models\n\nAs mentioned earlier we will be exploring a few models: \n\n* Decision Trees, \n* Random Forest \n* XGBoost\n\nEach one will be trained with a SkLearn Pipeline and with Cross Validation to compare which method gives us greater accuracy. Key thing to note is that we will be performing Regression with these models, Regression is the study of a depanant variable on one or more explanatory variables. \n","3fdc2d21":"#### Train Test Split","ab507854":"## Partial Dependence Plots ","b1990db5":"I was unable to apply the pipeline on this model as it keep printing out the following error: ` Last step of Pipeline should implement fit.` so I opted to just fit it without a pipeline.","f50ac1f9":"## Table of Contents\n\n* Import packages & Data\n* Exploratory Data Analysis\n* Data Cleaning\n* Feature Engineering \n* Train Models\n* Evaluation of Models\n* Conclusion\n* Resources Used","53291714":"#### Train Test Split","e9b4b71c":"### Random Forests\n\nRandom Forests randomly chooses observations and features to generate many decision trees to average the result. While a Decision tree generates rules based on the features in the data. \n\nThe main issue with Random Forests is that they are slow to gather real-time predictions on, but are extremly fast to train.  However, RF prevents overfitting by generating trees on random subsets. ","38eb6c6b":"#### Cross Validation","7a51bfa2":"None of theses distributions fit our data perfectly. So we need to find another one to match our data.\n\n","f3b6b822":"As there is only one signifincant outlier while it is clear that infants have `Rings` in the range of 8 -21.","4d6d15ef":"Now we can loop over the different leaf nodes to determine which one will give us the lowest MAE.","097f64df":"It appears that Abalone with `Rings` between 8-10 have the most observations.","486c1fe3":"We only have one categorical column while the other eight are numerical. Lets go ahead and see if there are any null values:","d10e3663":"It appears that Shell weight is the only feature that has a correlation above 0.50. All the other features linger around 0.42 - 0.51, I feel this will be a problem when making predictions. ","3326dedb":"# Evaluation of Models\n\n| Models        | MAE: Cross-val | MAE:Train-test  | Accuracy| Rank|\n| ------------- |:-------------:| -----:| ----:|-----:|\n| Decision Tree     | 1.959743 |1.597018 |0.44 |3rd|\n| Random Forests   | 1.511744   | 1.521251 |0.48|2nd|\n| XGBoost |1.424527    |    1.402607 |0.56|1st|\n\n**These results are from v20 of the notebook, all the other MAE & accuracy scores are from v20 of the notebook.**","72d3492b":"### XGBoost\n\nTo understand eXtreme Gradient Boosting, we need to understand the logic behind boosting algorithims. They build various weak models to make conclusions about the various importance of features to reduce them. Ultimately, XGBoost is an implementaion of Gradient Boosted Decision Trees. There are three different forms of gradient boosting : \n* Gradient Boosting algorithim\n* Stochastic Gradient Boosting\n* Regularized Gradient Boosting \n\n\nXGBoost gives us great speed in running our models and good model performance. \n","76a5368f":"# Exploratory Data Analysis\n","f95306df":"In our case it seems that Height has more of an effect of determining the number of rings on Abalone than length. ","f449bbb2":"From this it is clear that any number of leaf nodes will result in the same value for the MAE. I suspect due to the size of the dataset this might be the cause. ","cc5b3859":"Males are the most frequent `Sex` of Abalone in the dataset, while Infants just edge out Females. ","5b3dc305":"With an accuracy score of 44% our model is performing very poorly.  The MAE for Cross-val is :`1.959743` while for Train-test it is: `1.597016`. In this case the using train-test split for the Decision Tree is best.","4a5ea010":"We can gauge that the improvement is slightly marginal.  The difference between the XGBoost and Forests is also slightly marginal. ","492dc27d":"## Overview\n\nThe main goal of this notebook is to explore three different Supervised Learning Algorithms: Decision Trees, Random Forest and XGBoost.  I will explore handling missing values, Creating Pipelines, Partial Dependance Plots and One-hot Encoding. \n\nThe focus is on predicting the age of Abalone. These are sea snails that are quite in dangered in South Africa at least, but can be found in Australia, Great Britian and New Zealand. \n\n![abalone shell](https:\/\/cdn.shopify.com\/s\/files\/1\/2086\/1263\/products\/1d89434927bffb6fd1786c19c2d921fb_2000x.jpg?v=1522240385)\n\n*An Abalone shell*\n\nWe will use various measurements to predict the number of rings the abalone have, which determines it's age once 1.5 is added to the total rings.  ","31f4c8e2":"#  Data Cleaning\n\nFrom the EDA it is clear that we will need to do something to remove outliers. To achieve this we can use the Scipy stats module to convert the observations into Zscores. Zscores give us a distribution that illustrate how many standard deviations away from the mean the data is. \n\n","3684c31f":"#  Feature Engineering \n","a30540dd":"These scatter plots give us a different overview of the correlated features. This is what I've picked up:\n\n* `Height` and `Length` have an interesting relationship in that Abalone's with a height of 0.0 mm - 0.5 mm have varing degrees of length. `Length` and `Diameter` seem to have a nice linear relationship with few outliers. \n* It appears that the the number of `Rings` of an Abalone are all concentrated around a `Height` below 0.5 mm.\n* It seems that many Abalone have `Rings` between 5-20 that have`Shell weight`  distributed above 0.00 grams to 0.75 grams. Also the larger in `Diameter` the Abalone is the more `Shell weight` it has too. ","0ddd37b2":"We can see that the mean length from the observations in the dataset is 0.52 mm, while the longest is 0.82 mm and the shortest is 0.075 which is tiny! With the height we get a mininum of 0.0000 which is suspect because that must be a bogus observation. Shell weight seems to not be more than 1 gram, while the oldest Abalone is 30.5 years old and the youngest 2.5 years. The heaviest abalone is 2.83 grams and the lightest 0.001. ","1c00eeda":"# Conclusion\n\nThe XGBoost model is clearly the superior model however, it's accuracy is only just above 50% which is pretty average at best. Also, it seems that the train-test split might be the best path  to train the model. However, with the Decision Tree the cross-validation has the highest MAE. So overall using that techinque does not improve our model has I would've hoped at first.\n\nThe next steps would be to do more feature engineering and some hyperparameter tuning to improve the model. With respects to improvements, having a pipeline that runs all three models at the same time and prints the results together could be positive step. \n\n\nFinally to recap, we have covered:\n\n* How to implement Sklearn pipelines.\n* How to create Decision Tree, Random Forests and XGBoost regression models.\n* How to detect outliers and missing values.\n* How to use cross validation.\n* How to implement One-Hot Encoding.\n* How to create Partial Dependance Plots.","2d93d86d":"This box plot allows us to clearly visualize the presence of outliers, meaning we will need to manage them before training on the data.","2a3c9ab9":"What has happened above is that we are removing all the data points that are below three standard deviations away from the mean.","35df67e6":"Partial Dependence Plots display the effect (marginally) of a feature on the predicted outcome of a model that has just been fitted.","797de337":"I would like observe which variables are strongly correlated to one another:\n\n","772eab83":"###  One Hot encoding\n\nWe will use a technique called one-hot encoding to create binary columns that will show the presence of each possible value from the original dataset. This is because the model will not know how to handle 'M', 'F' or 'I' when it processes it.\n\nHowever first I will need to determine the cardinality of columns:\n\n","2fdc0e15":"#### Cross Validation","af50ba2a":"The `isnull()` method returns a boolean of true or false for the detection of null values.  In our case it has returned `false` for 4177 entries in the dataset, this matches the same amount of entries in the entire dataset. So we can be safe knowing we have no null values.","b9642818":"Lets read the data into a Pandas dataframe:\n\n","c30bbe94":"With this scatter plot I took the `Shell weight` because it has the highest correlation with `Rings` and tried to see how they compare against the different sexes. \n\nWhat first catches the eye is that we can finally see a cut off age for `Rings` in infants which in the region of 20. While Male and Female have a similar distribution, with the Males having more visible outliers. ","d7bc69db":"Yet again the train-test split has out performed the cross-validation, with a MAE of `1.511744` vs a MAE of `1.521251`. The Random Forest model also has an accuracy of 48%. These are marginal improvements!","0fc8fda9":"#### Train Test Split","66b2ec8d":"In this Case it is the second highest correlated variable `Diameter`. Females only have observations starting from 0.2 mm in diameter and with clear outliers. Otherwise Males and Infants have rough a similar spread of the data.","8da8d620":"#                    Predicting The Age of Abalone \ud83d\udc33\ud83d\udc20\n\n![coffee](https:\/\/images.unsplash.com\/photo-1495004984586-0dc339ad4b2c?ixlib=rb-0.3.5&ixid=eyJhcHBfaWQiOjEyMDd9&s=3a6bedd3ca3b1ced9521b663a8f4ccfd&auto=format&fit=crop&w=667&q=80)\n\n*courtesy of Sasha \u2022 Stories from Unsplash*","a9990bdc":"With Decision Trees you are able to run a variation of them to determine how many leaf nodes produces the best set of results. Below we create the pipeline in the `get_mae()` function to return the mean absolute error.","8c081cfc":"# Resources Used Manufacture This Notebook:\n\n* [Detecting_Removing_Outliers]( https:\/\/github.com\/SharmaNatasha\/Machine-Learning-using-Python\/blob\/master\/Simple%20projects\/Detecting_Removing_Outliers.ipynb)\n\n* [Data Modeling](https:\/\/github.com\/AlexIoannides\/ml-project-workflow\/blob\/master\/Data%20Modelling.ipynb)\n\n* [Random Forest Algorithm]( https:\/\/towardsdatascience.com\/the-random-forest-algorithm-d457d499ffcd)\n\n* [random-forest-simple-explanation](https:\/\/medium.com\/@williamkoehrsen\/random-forest-simple-explanation-377895a60d2d)\n* [random-forests-classifier-python](https:\/\/www.datacamp.com\/community\/tutorials\/random-forests-classifier-python)\n* [gentle-introduction-xgboost]( https:\/\/machinelearningmastery.com\/gentle-introduction-xgboost-applied-machine-learning\/)\n* [Partial Dependance Plots]( https:\/\/christophm.github.io\/interpretable-ml-book\/pdp.html)\n* [A Simple Tutorial on Exploratory Data Analysis](https:\/\/www.kaggle.com\/pavansanagapati\/a-simple-tutorial-on-exploratory-data-analysis)\n* [Churn Prediction of Telco Customers](https:\/\/www.kaggle.com\/meldadede\/churn-prediction-of-telco-customers)\n* [Data Camp](https:\/\/github.com\/AmoDinho\/datacamp-python-data-science-track)\n","9e9305ed":"Lets go ahead and see the skewness of the target variable  which is `Rings`","51e2f896":"Before I give my inital reaction to the data please note the following units of measurement for the various variables:\n\n- `Length` - mm\n- `Diameter` - mm\n- `Height` - mm\n- `Whole weight` - grams\n- `Shucked weight` - grams\t\n- `Viscera weight` -grams\t\n- `Shell weight` -grams\n\nFor the rings you need add 1.5 to get the age in years.","80d47cb1":"#### Cross Validation","d12adec0":"Below I opted to train the model again by explictly stating the number of estimators and learning rate.","0ed0dbc4":"Our abalone_encoded_predictors dataframe now has three new columns `Sex_F`,  `Sex_I` and  `Sex_M`."}}