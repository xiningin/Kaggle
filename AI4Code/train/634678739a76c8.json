{"cell_type":{"a95f0b4e":"code","5249e781":"code","74cb66a2":"code","9fec1ed0":"code","5c273c92":"code","e46b9a3b":"code","07e62c6a":"code","588bae8a":"code","82d71604":"code","826878b4":"code","6c660fa0":"code","535b9b33":"code","81553e6d":"code","60c3bf1d":"code","8ccea87f":"code","6d006d34":"code","c5a07aa1":"code","4e1a6cd4":"code","32f83c40":"code","142269a7":"code","62f1ef87":"code","b8fb3047":"code","f9b212a3":"code","c98fdbf0":"code","8079118e":"code","36d10741":"code","d0795339":"code","df9c478c":"code","47928348":"code","fe26df00":"code","6e9ae9eb":"code","3cb2f25e":"code","70d68d71":"code","bf340fd5":"code","a8c85b9d":"code","90dad9b6":"code","462250d3":"code","ac865f1a":"code","21215b07":"code","d9823203":"code","fbb43c61":"code","e9d4b539":"code","17861c97":"code","32527f9e":"code","4e3febe1":"code","5c52d0f7":"code","4661bf48":"code","eebd262c":"code","238f1f0f":"code","57c6feb7":"code","17cb509a":"code","79079421":"code","0444b9e1":"code","f6797d35":"code","a3e7b782":"code","215fe777":"code","016aa42d":"code","7894a1c1":"code","71a921f4":"markdown","f3c973df":"markdown","94ef33e8":"markdown","289aa43a":"markdown","7f79718e":"markdown","ff774a46":"markdown","7fa5a2a3":"markdown","23e76e65":"markdown","202f687c":"markdown","6b9da477":"markdown","897fad16":"markdown","dd7339f3":"markdown","11516fbf":"markdown","d1d50dc4":"markdown","6c0d4377":"markdown"},"source":{"a95f0b4e":"import json\nimport random\nrandom.seed(27)\nfrom functools import partial\nfrom collections import defaultdict\nfrom multiprocessing import Pool\nfrom tqdm import tqdm","5249e781":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.manifold import TSNE","74cb66a2":"import cv2\nimport albumentations as A","9fec1ed0":"import tensorflow as tf\n# import tensorflow_addons as tfa\nfrom tensorflow import keras\nfrom tensorflow.keras import layers","5c273c92":"from transformers import BertTokenizer","e46b9a3b":"path = '\/kaggle\/input\/shopee-product-matching\/'","07e62c6a":"train_csv = pd.read_csv(path+'train.csv')\ntrain_csv.head()","588bae8a":"train_img_name = train_csv['image']\ntrain_img_path = path + 'train_images\/' + train_csv['image']\ntrain_title = train_csv['title']\ntrain_label = train_csv['label_group']\ntrain_hash = train_csv['image_phash']","82d71604":"text_token_dims = 300\n\ntokenizer = BertTokenizer.from_pretrained(\"bert-base-multilingual-cased\")\ntrain_title_token = np.array(tokenizer(train_title.tolist(),\n                                       padding='max_length',\n                                       truncation=True , \n                                       max_length=text_token_dims)['input_ids'],\n                             dtype = np.uint32)","826878b4":"def test_set(imagePath, label_group, test_size = 0.1):\n  n_test = int(len(imagePath) * test_size)\n\n  unique_label, count_label = np.unique(label_group, return_counts=True)\n  labelWith2Samples = unique_label[count_label==2]\n  idx = np.arange(len(labelWith2Samples))\n\n  test_idx = idx[:n_test\/\/2]## floor divide by 2, since each label has two image\n\n  test_filter = np.zeros((len(imagePath), ), dtype=bool)\n    \n  path_index = np.arange(len(imagePath))\n\n  gallery = []\n\n  probe = []\n  \n  for label in labelWith2Samples[test_idx]:\n    labelFilter = label_group == label\n    test_filter = test_filter | labelFilter\n\n    this_label_images = path_index[labelFilter]\n\n    gallery.append(this_label_images[0])\n    probe.append(this_label_images[1])\n  return test_filter, gallery, probe\n\n\ntest_filter, test_gallery, test_probe = test_set(train_img_path, train_label)","6c660fa0":"Gallery_label = train_label[test_gallery]\nProbe_label = train_label[test_probe]","535b9b33":"Gallery_idx_dataset = tf.data.Dataset.from_tensor_slices(test_gallery)\nProbe_idx_dataset = tf.data.Dataset.from_tensor_slices(test_probe)","81553e6d":"len(train_img_path)","60c3bf1d":"image_size = 224\n\ndef fetch(idx, image_size = image_size):\n    image = cv2.resize(cv2.imread(train_img_path.iloc[idx])[...,[2,1,0]], dsize = (image_size,image_size), interpolation = cv2.INTER_AREA)\n    title = train_title_token[idx]\n    return image, title\n\ndef GetData(idx, size):\n    image, title = tf.numpy_function(func=fetch, inp=[idx], Tout=[tf.uint8, tf.uint32])\n    return {\"image\": image, \"title\": title}\n\nGallery = Gallery_idx_dataset.map(partial(GetData, size=image_size), num_parallel_calls=tf.data.AUTOTUNE).batch(500).prefetch(tf.data.AUTOTUNE)\nProbe = Probe_idx_dataset.map(partial(GetData, size=image_size), num_parallel_calls=tf.data.AUTOTUNE).batch(500).prefetch(tf.data.AUTOTUNE)","8ccea87f":"test_batch_g = next(iter(Gallery))","6d006d34":"test_batch_p = next(iter(Probe))","c5a07aa1":"def show_batch():\n    num = 5\n    plt.figure(figsize=(20,8))\n    for i in range(num):\n        plt.subplot(2,num,i+1)\n        plt.imshow(test_batch_g['image'][i])\n        plt.title('Gallery: ')\n        plt.subplot(2,num,i+num+1)\n        plt.imshow(test_batch_p['image'][i])\n        plt.title('Probe: ')\n    plt.show()\nshow_batch()","4e1a6cd4":"Gallery_hash = np.array([*map(lambda x: list(x), train_hash.iloc[test_gallery].to_numpy())])\nProbe_hash = np.array([*map(lambda x: list(x), train_hash.iloc[test_probe].to_numpy())])","32f83c40":"image_size = 224","142269a7":"class TokenAndPositionEmbedding(layers.Layer):\n    def __init__(self, maxlen, vocab_size, embed_dim):\n        super(TokenAndPositionEmbedding, self).__init__()\n        self.token_emb = layers.Embedding(input_dim=vocab_size, output_dim=embed_dim)\n        self.pos_emb = layers.Embedding(input_dim=maxlen, output_dim=embed_dim)\n\n    def call(self, x):\n        maxlen = tf.shape(x)[-1]\n        positions = tf.range(start=0, limit=maxlen, delta=1)\n        positions = self.pos_emb(positions)\n        x = self.token_emb(x)\n        return x + positions","62f1ef87":"class TransformerBlock(layers.Layer):\n    def __init__(self, embed_dim, num_heads, ff_dim, rate=0.1):\n        super(TransformerBlock, self).__init__()\n        self.att = layers.MultiHeadAttention(num_heads=num_heads, key_dim=embed_dim)\n        self.ffn = keras.Sequential(\n            [layers.Dense(ff_dim, activation=\"relu\"), layers.Dense(embed_dim),]\n        )\n        self.layernorm1 = layers.LayerNormalization(epsilon=1e-6)\n        self.layernorm2 = layers.LayerNormalization(epsilon=1e-6)\n        self.dropout1 = layers.Dropout(rate)\n        self.dropout2 = layers.Dropout(rate)\n\n    def call(self, inputs, training):\n        attn_output = self.att(inputs, inputs)\n        attn_output = self.dropout1(attn_output, training=training)\n        out1 = self.layernorm1(inputs + attn_output)\n        ffn_output = self.ffn(out1)\n        ffn_output = self.dropout2(ffn_output, training=training)\n        return self.layernorm2(out1 + ffn_output)","b8fb3047":"class TripletLossLayer(layers.Layer):\n    def __init__(self, alpha, **kwargs):\n        self.alpha = alpha\n        super(TripletLossLayer, self).__init__(**kwargs)\n    \n    def triplet_loss(self, inputs):\n        anchor, positive, negative = inputs\n        \n        anchor = tf.math.l2_normalize(anchor, axis=1)\n        positive = tf.math.l2_normalize(positive, axis=1)\n        negative = tf.math.l2_normalize(negative, axis=1)\n\n        p_dist = tf.math.reduce_sum(tf.math.square(anchor-positive), axis=-1)\n        n_dist = tf.math.reduce_sum(tf.math.square(anchor-negative), axis=-1)\n        return tf.math.reduce_sum(tf.math.maximum(p_dist - n_dist + self.alpha, 0), axis=0)\n    def call(self, inputs):\n        loss = self.triplet_loss(inputs)\n        self.add_loss(loss)\n        return loss","f9b212a3":"tokenizer.vocab_size","c98fdbf0":"def plot_loss(loss, val_loss, t):\n    plt.figure(figsize = (10,5), facecolor = 'white')\n    plt.plot(np.arange(1,len(loss)+1), loss, label = 'Train loss' )\n    plt.plot(np.arange(1,len(val_loss)+1), val_loss, label = 'validation loss')\n    plt.xlabel('epoch')\n    plt.ylabel('loss')\n    plt.title(t+': Training Loss')\n    plt.legend()\n    plt.grid(True)\n    plt.show()","8079118e":"loss = [7.67,6.23,5.76,5.39,5.02,4.75,4.52,4.38,\n        3.82,3.76,3.64,3.57,3.45,3.32,3.21,3.12,\n        3.13,3.07,2.98,2.95,2.93,2.92,2.86,2.81]\nval_loss = [6.90,6.17,4.85,4.98,4.09,4.76,4.83,4.01,\n            4.16,3.99,3.85,3.22,3.91,3.62,3.39,3.25,\n            3.77,3.50,3.27,3.50,3.67,3.46,3.13,3.45]\nplot_loss(loss, val_loss, 'Model3 (Image model)')","36d10741":"def model_builder_Image(resolution, text_token_dims = text_token_dims, vocab_size = tokenizer.vocab_size, embedding_size = 256):\n    ##IMAGE\n    BackBone_img = keras.applications.EfficientNetB0(\n        include_top=False, weights='imagenet',\n        pooling = 'avg',\n        input_tensor=layers.Input((resolution,resolution,3), name = 'image')\n    )\n    net_image_flatten = layers.Flatten()(BackBone_img.layers[-1].output)\n\n    output = layers.Dense(embedding_size)(net_image_flatten)\n\n    BackBone = keras.Model(inputs=[BackBone_img.layers[0].output], outputs=output, name = 'Image_Model')\n\n    img_input_anchor = layers.Input((resolution,resolution,3), name='Anchor_img')\n    img_input_positive = layers.Input((resolution,resolution,3), name='Positive_img')\n    img_input_negative = layers.Input((resolution,resolution,3), name='Negative_img')\n\n    anchor_embedding= BackBone([img_input_anchor])\n    positive_embedding = BackBone([img_input_positive])\n    negative_embedding = BackBone([img_input_negative])\n\n    margin = 1\n    loss_layer = TripletLossLayer(alpha=margin, name='triplet_loss_layer')([anchor_embedding, positive_embedding, negative_embedding])\n    Triplet_Net = keras.Model(inputs=[img_input_anchor, img_input_positive, img_input_negative], outputs=loss_layer)\n    Triplet_Net.compile(optimizer=keras.optimizers.RMSprop())\n    return BackBone, Triplet_Net\n\nEmbedding_Net_Image, Triplet_Net_Image = model_builder_Image(image_size)","d0795339":"Triplet_Net_Image.load_weights('..\/input\/shopee-420-a2\/Image-24-2.81-3.45.hdf5')","df9c478c":"Gallery_embedding_matrix_Image = Embedding_Net_Image.predict(Gallery)\nProbe_embedding_matrix_Image = Embedding_Net_Image.predict(Probe)","47928348":"tf.keras.backend.clear_session()","fe26df00":"loss = [30.45,20.99,17.62,14.79,12.48,11.28,10.00,9.47,8.18,7.82,\n        7.40,6.70,6.27,6.07,5.66,5.35,5.05,4.89,4.70,4.36,\n        4.38,4.24,4.02,3.95,3.68,3.66,3.52,3.37,3.35,3.20,\n        3.14,3.00,2.89,2.88,2.84,2.79,2.65,2.67,2.53,2.47,\n        2.48,2.40,2.39,2.31,2.33,2.24,2.23,2.17,2.10,2.11]\nval_loss = [19.74,15.46,12.88,11.92,9.21,8.80,8.25,6.29,6.72,6.65,\n            6.13,6.08,5.36,5.42,5.32,4.94,5.24,4.93,4.72,4.29,\n            3.97,3.86,4.14,3.89,3.77,3.57,3.82,3.10,3.24,3.37,\n            3.18,3.04,2.85,2.95,2.70,2.62,2.40,2.52,2.26,2.40,\n            2.30,2.60,2.38,2.61,2.19,2.04,2.02,2.28,2.09,1.95\n           ]\nplot_loss(loss, val_loss, 'Model2 (Text model)')","6e9ae9eb":"def model_builder_Text(resolution, text_token_dims = text_token_dims, vocab_size = tokenizer.vocab_size, embedding_size = 256):\n    ##TEXT\n    input_text_embed_dim = 70 # Embedding size for each token\n    num_heads = 15 # Number of attention heads\n    out_dim = 100 # Hidden layer size in feed forward network inside transformer\n\n    inputs = layers.Input(shape=(text_token_dims,), name = 'title')\n    embedding_layer = TokenAndPositionEmbedding(text_token_dims, vocab_size, input_text_embed_dim)\n    x = embedding_layer(inputs)\n    transformer_block = TransformerBlock(input_text_embed_dim, num_heads, out_dim)\n    x = transformer_block(x)\n    x = layers.GlobalAveragePooling1D()(x)\n\n    #Connect two network\n    output = layers.Dense(embedding_size)(x)\n\n    BackBone = keras.Model(inputs=[inputs], outputs=output, name = 'SimpleTransformer')\n\n    title_input_anchor = layers.Input((text_token_dims,), name='Anchor_title')\n    title_input_positive = layers.Input((text_token_dims,), name='Positive_title')\n    title_input_negative = layers.Input((text_token_dims,), name='Negative_title')\n\n    anchor_embedding= BackBone([title_input_anchor])\n    positive_embedding = BackBone([title_input_positive])\n    negative_embedding = BackBone([title_input_negative])\n\n    margin = 1\n    loss_layer = TripletLossLayer(alpha=margin, name='triplet_loss_layer')([anchor_embedding, positive_embedding, negative_embedding])\n    Triplet_Net = keras.Model(inputs=[title_input_anchor, title_input_positive,  title_input_negative], outputs=loss_layer)\n    Triplet_Net.compile(optimizer=keras.optimizers.RMSprop())\n    return BackBone, Triplet_Net\n\nEmbedding_Net_Text, Triplet_Net_Text = model_builder_Text(image_size)\n\nTriplet_Net_Text.load_weights('..\/input\/shopee-420-a2\/text-50-2.11-1.95.hdf5')","3cb2f25e":"Gallery_embedding_matrix_Text = Embedding_Net_Text.predict(Gallery)\nProbe_embedding_matrix_Text = Embedding_Net_Text.predict(Probe)","70d68d71":"tf.keras.backend.clear_session()","bf340fd5":"loss = [7.38, 5.92, 5.20, 4.58, 4.02, 3.62, 3.18, 3.04, \n        2.79, 2.77, 2.54, 2.42, 2.34, 2.28, 2.23, 2.14, \n       2.0, 2.0, 1.94, 1.87, 1.78, 1.72, 1.77, 1.68]\nval_loss = [7.48, 4.92, 4.02, 3.72, 3.73, 2.93, 2.64, 3.10, \n            3.01, 2.51, 2.45, 2.56, 2.58, 2.01, 2.04, 2.19,\n           2.09, 1.84, 2.01, 1.7, 1.51, 1.48, 1.91, 1.63]\nplot_loss(loss, val_loss, 'Model3 (Image + Text model)')","a8c85b9d":"def model_builder_Full(resolution, text_token_dims = text_token_dims, vocab_size = tokenizer.vocab_size, embedding_size = 256):\n    ##IMAGE\n    BackBone_img = keras.applications.EfficientNetB0(\n        include_top=False, weights='imagenet',\n        pooling = 'avg',\n        input_tensor=layers.Input((resolution,resolution,3), name = 'image')\n    )\n    net_image_flatten = layers.Flatten()(BackBone_img.layers[-1].output)\n\n    ##TEXT\n    input_text_embed_dim = 70 # Embedding size for each token\n    num_heads = 15 # Number of attention heads\n    out_dim = 100 # Hidden layer size in feed forward network inside transformer\n\n    inputs = layers.Input(shape=(text_token_dims,), name = 'title')\n    embedding_layer = TokenAndPositionEmbedding(text_token_dims, vocab_size, input_text_embed_dim)\n    x = embedding_layer(inputs)\n    transformer_block = TransformerBlock(input_text_embed_dim, num_heads, out_dim)\n    x = transformer_block(x)\n    x = layers.GlobalAveragePooling1D()(x)\n\n    #Connect two network\n    net_concate = layers.Concatenate()([net_image_flatten, x])\n\n    output = layers.Dense(embedding_size)(net_concate)\n\n    BackBone = keras.Model(inputs=[BackBone_img.layers[0].output, inputs], outputs=output, name = 'EfficientNetB0_SimpleTransformer')\n\n\n    img_input_anchor = layers.Input((resolution,resolution,3), name='Anchor_img')\n    img_input_positive = layers.Input((resolution,resolution,3), name='Positive_img')\n    img_input_negative = layers.Input((resolution,resolution,3), name='Negative_img')\n\n    title_input_anchor = layers.Input((text_token_dims,), name='Anchor_title')\n    title_input_positive = layers.Input((text_token_dims,), name='Positive_title')\n    title_input_negative = layers.Input((text_token_dims,), name='Negative_title')\n\n\n    anchor_embedding= BackBone([img_input_anchor, title_input_anchor])\n    positive_embedding = BackBone([img_input_positive, title_input_positive])\n    negative_embedding = BackBone([img_input_negative, title_input_negative])\n\n    margin = 1\n    loss_layer = TripletLossLayer(alpha=margin, name='triplet_loss_layer')([anchor_embedding, positive_embedding, negative_embedding])\n    Triplet_Net = keras.Model(inputs=[img_input_anchor, title_input_anchor, img_input_positive, title_input_positive, img_input_negative, title_input_negative], outputs=loss_layer)\n    Triplet_Net.compile(optimizer=keras.optimizers.RMSprop())\n    return BackBone, Triplet_Net\n\nEmbedding_Net_Full, Triplet_Net_Full = model_builder_Full(image_size)\n\n\nTriplet_Net_Full.load_weights('..\/input\/shopee-420-a2\/FULL-24-1.68-1.63.hdf5')","90dad9b6":"Gallery_embedding_matrix_Full = Embedding_Net_Full.predict(Gallery)\nProbe_embedding_matrix_Full = Embedding_Net_Full.predict(Probe)","462250d3":"tf.keras.backend.clear_session()","ac865f1a":"def L2_v2m(v, m):\n    return np.sqrt(np.sum(np.square(v - m),axis = 1))\n\ndef cosine_similarity_v2m(v, m):\n    return np.sum(v*m, axis = 1)\/(np.linalg.norm(v) * np.linalg.norm(m, axis = 1))\n\n\ndef hammingDist_v2m(s, sm):\n    return np.sum(s != sm, axis = 1)","21215b07":"def ranked_histogram(Gallery_embedding_matrix, Probe_embedding_matrix, dist_measure = L2_v2m):\n    ranked_histogram_ = np.zeros(len(Probe_label))\n    \n    top5 = []\n\n    resuld_dict = {'posting_id': [], 'matches': []}\n    for i in tqdm(range(len(Gallery_label))):\n        dist_matrix = dist_measure(Gallery_embedding_matrix[i], Probe_embedding_matrix)\n\n        sorted_dist = np.argsort(dist_matrix)\n\n        ranked = Probe_label.iloc[sorted_dist]\n        \n        \n        find_result = ranked == Gallery_label.iloc[i]\n        \n        \n        if (np.any(find_result.iloc[:5])):\n            top5.append(dist_matrix[sorted_dist[find_result][0]])\n            \n        match = np.where(ranked == Gallery_label.iloc[i])[0][0]\n\n        ranked_histogram_[match] += 1\n    return ranked_histogram_, np.array(top5)\n\ndef cmc(ranked_histogram):\n    cmc = np.zeros(len(Probe_label))\n    for i in range(len(Probe_label)):\n        cmc[i] = np.sum(ranked_histogram[:(i + 1)])\n    print(cmc)\n    return cmc","d9823203":"ranked_histogram_phash, top5_phash = ranked_histogram(Gallery_hash, Probe_hash, dist_measure = hammingDist_v2m)\nranked_histogram_text, top5_text = ranked_histogram(Gallery_embedding_matrix_Text, Probe_embedding_matrix_Text)\nranked_histogram_image, top5_image = ranked_histogram(Gallery_embedding_matrix_Image, Probe_embedding_matrix_Image)\nranked_histogram_full, top5_full = ranked_histogram(Gallery_embedding_matrix_Full, Probe_embedding_matrix_Full)","fbb43c61":"cmc_phash = cmc(ranked_histogram_phash)\ncmc_text = cmc(ranked_histogram_text)\ncmc_image = cmc(ranked_histogram_image)\ncmc_full = cmc(ranked_histogram_full)","e9d4b539":"fig = plt.figure(figsize=[20, 8])\nax = fig.add_subplot(1, 2, 1)\nax.plot(cmc_phash, label = 'phash')\nax.plot(cmc_image, label = 'Image')\nax.plot(cmc_text, label = 'Text')\nax.plot(cmc_full, label = 'Full')\nax.set_xlabel('Rank')\nax.set_ylabel('Count')\nax.set_title('CMC Curve')\nax.legend(loc = 'lower right')\nax = fig.add_subplot(1, 2, 2)\nax.plot(cmc_phash\/len(Gallery_label), label = 'phash')\nax.plot(cmc_image\/len(Gallery_label), label = 'Image')\nax.plot(cmc_text\/len(Gallery_label), label = 'Text')\nax.plot(cmc_full\/len(Gallery_label), label = 'Full')\nax.set_xlabel('Rank')\nax.set_ylabel('Count')\nax.set_title('CMC Curve scaled')\nax.legend(loc = 'lower right')\n\nplt.show()","17861c97":"def accuracy(cmc):\n    print('Top 1 accuracy: {:1f}'.format((cmc\/len(Gallery_label))[0]))\n    print('Top 5 accuracy: {:1f}'.format((cmc\/len(Gallery_label))[4]))\n    \nprint('Phash: ')\naccuracy(cmc_phash)\nprint('Image: ')\naccuracy(cmc_image)\nprint('Text: ')\naccuracy(cmc_text)\nprint('Full: ')\naccuracy(cmc_full)","32527f9e":"def pos_neg_dist(Gallery_embedding_matrix, Probe_embedding_matrix, dist_measure = L2_v2m):\n    ranked_histogram_ = np.zeros(len(Probe_label))\n    positive_pair = []\n    negative_pair = []\n    \n    \n\n    for i in tqdm(range(len(Gallery_label))):\n        dist_matrix = dist_measure(Gallery_embedding_matrix[i], Probe_embedding_matrix)\n\n        positive_pair.append(dist_matrix[i])\n        \n        \n        mask = np.ones(dist_matrix.shape, dtype=bool)\n        mask[i] = False\n        \n        negative_pair += dist_matrix[mask].tolist()\n        \n    return positive_pair, negative_pair\n\n","4e3febe1":"positive_pair, negative_pair = pos_neg_dist(Gallery_hash, Probe_hash, dist_measure = hammingDist_v2m)\nsns.histplot(positive_pair, kde=True, stat = 'density', color = 'red')\nsns.histplot(negative_pair, kde=True, stat = 'density', color = 'blue')","5c52d0f7":"positive_pair, negative_pair = pos_neg_dist(Gallery_embedding_matrix_Text, Probe_embedding_matrix_Text)\nsns.histplot(positive_pair, kde=True, stat = 'density', color = 'red')\nsns.histplot(negative_pair, kde=True, stat = 'density', color = 'blue')","4661bf48":"positive_pair, negative_pair = pos_neg_dist(Gallery_embedding_matrix_Image, Probe_embedding_matrix_Image)\nsns.histplot(positive_pair, kde=True, stat = 'density', color = 'red')\nsns.histplot(negative_pair, kde=True, stat = 'density', color = 'blue')","eebd262c":"positive_pair, negative_pair = pos_neg_dist(Gallery_embedding_matrix_Full, Probe_embedding_matrix_Full)\nsns.histplot(positive_pair, kde=True, stat = 'density', color = 'red')\nsns.histplot(negative_pair, kde=True, stat = 'density', color = 'blue')","238f1f0f":"sns.displot(top5_phash, kde=True)","57c6feb7":"sns.displot(top5_text, kde=True)","17cb509a":"sns.displot(top5_image, kde=True)","79079421":"sns.displot(top5_full, kde=True)","0444b9e1":"# tsne_embeddings = TSNE(random_state=4).fit_transform(np.concatenate((Gallery_embedding_matrix_Full, Probe_embedding_matrix_Full), axis = 0))\n# fig = plt.figure(figsize=[12, 12])\n# ax = fig.add_subplot(1, 1, 1)\n# ax.scatter(tsne_embeddings[:,0], tsne_embeddings[:,1], c = np.concatenate((Gallery_label, Probe_label), axis = 0));\n# ax.set_title('Full model')","f6797d35":"tsne_embeddings = TSNE(random_state=4).fit_transform(np.concatenate((Gallery_embedding_matrix_Text, Probe_embedding_matrix_Text), axis = 0))\nfig = plt.figure(figsize=[12, 12])\nax = fig.add_subplot(1, 1, 1)\nfig = plt.figure(figsize=[12, 12])\nax.scatter(tsne_embeddings[:,0], tsne_embeddings[:,1], c = np.concatenate((Gallery_label, Probe_label), axis = 0));\nax.set_title('Text model')","a3e7b782":"tsne_embeddings = TSNE(random_state=4).fit_transform(np.concatenate((Gallery_embedding_matrix_Image, Probe_embedding_matrix_Image), axis = 0))\nfig = plt.figure(figsize=[12, 12])\nax = fig.add_subplot(1, 1, 1)\nax.scatter(tsne_embeddings[:,0], tsne_embeddings[:,1], c = np.concatenate((Gallery_label, Probe_label), axis = 0));\nax.set_title('Image model')","215fe777":"tsne_embeddings = TSNE(random_state=4).fit_transform(np.concatenate((Gallery_embedding_matrix_Full, Probe_embedding_matrix_Full), axis = 0))\nfig = plt.figure(figsize=[12, 12])\nax = fig.add_subplot(1, 1, 1)\nax.scatter(tsne_embeddings[:,0], tsne_embeddings[:,1], c = np.concatenate((Gallery_label, Probe_label), axis = 0));\nax.set_title('Full model')","016aa42d":"def ranked_histogram(idx, Gallery_embedding_matrix, Probe_embedding_matrix, dist_measure = L2_v2m):\n    target_idx = idx\n    \n    plt.figure(figsize=(35,5), facecolor = 'white')\n    plt.subplot(1,7,1)\n    plt.imshow(cv2.imread(train_img_path.iloc[test_gallery[idx]])[...,[2,1,0]])\n    plt.title('ancher')\n    \n    distance = L2_v2m(Gallery_embedding_matrix[idx], Probe_embedding_matrix)\n    \n    sorted_dist = np.argsort(distance)\n    \n    ranked = Probe_label.iloc[sorted_dist]\n    \n    match = np.where(ranked == Gallery_label.iloc[idx])[0][0]\n    \n    plt.subplot(1,7,2)\n    plt.imshow(cv2.imread(train_img_path.iloc[test_probe[idx]])[...,[2,1,0]])\n    plt.title('excepted: {:2f}, found at {}'.format(distance[idx], match))\n    \n    if match > 5:\n        match = 5\n    \n    plot_idx = 3\n    for i in range(match):\n        plt.subplot(1,7,plot_idx)\n        plt.imshow(cv2.imread(train_img_path.iloc[test_probe[sorted_dist[i]]])[...,[2,1,0]])\n        plt.title('unmatch: {:2f}'.format(distance[sorted_dist[i]]))\n        plot_idx += 1\n    plt.plot()\n\n    ","7894a1c1":"for i in range(5):\n    ranked_histogram(i, Gallery_embedding_matrix_Full, Probe_embedding_matrix_Full)","71a921f4":"## Load csv","f3c973df":"## possitive, negative distance","94ef33e8":"# Test set","289aa43a":"# Model 3: Image + Text","7f79718e":"# Model 2: Text","ff774a46":"# Deep Model","7fa5a2a3":"## Error analysis\nif there is image that closer than similar image, plot those image. if more than 5, plot first 5 only","23e76e65":"## CMC","202f687c":"# Non Deep Model","6b9da477":"## phash","897fad16":"# Model 1: Image","dd7339f3":"# Data","11516fbf":"## Top5 distance","d1d50dc4":"# Result analysis","6c0d4377":"## TSNE"}}