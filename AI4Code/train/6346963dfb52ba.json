{"cell_type":{"79b61923":"code","f9b013df":"code","60a06d13":"code","08d257b9":"code","7b2ef33f":"code","41173716":"code","bdae84c1":"code","dfed2424":"code","eb898774":"code","6839eb46":"code","3714711c":"code","308b8933":"code","7549f994":"code","dfe2f9a3":"code","af9b3964":"code","3c65a5cb":"code","3c064438":"code","8c8d8441":"code","dc0ee271":"code","d9f25594":"code","8d0f9452":"code","eb9681ad":"code","7040d297":"code","b7017729":"code","9c841f08":"code","dc2920a5":"code","694d93b1":"code","9e371268":"code","50e01471":"code","6602a6d1":"code","894ce499":"code","7ae9eece":"code","ea0dbd83":"code","fe47d6ae":"code","4984c970":"code","1742d709":"code","94958127":"code","26444f2e":"code","3ac733c6":"code","4d99cd03":"code","b142cca6":"code","a3ed60e1":"markdown","e4be60a7":"markdown","d439e225":"markdown","a01ceb17":"markdown","8ad97587":"markdown","42d70d2b":"markdown","3d5b6fce":"markdown","21ba8f3a":"markdown","7bf64a8d":"markdown","ef24f09f":"markdown","fbdbff86":"markdown","f82a3b3f":"markdown","41b6f30a":"markdown","740f8715":"markdown","57dcf99e":"markdown","983ac09b":"markdown","83779a27":"markdown","a07de334":"markdown","8fcb7c7e":"markdown","de0cc9a8":"markdown","e08fbcc8":"markdown","9bff376b":"markdown","345f1b1a":"markdown","0d5e6d7f":"markdown","8da57aac":"markdown","7cac3bd2":"markdown","5fc856db":"markdown","11605f1c":"markdown"},"source":{"79b61923":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f9b013df":"import string\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.naive_bayes import MultinomialNB\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.feature_extraction.text import TfidfTransformer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import SGDClassifier\nfrom nltk.corpus import stopwords\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom scipy import sparse","60a06d13":"train_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')","08d257b9":"del train_data['id']\ntest_ids = test_data['id']\ndel test_data['id']\n\ny = train_data['target']\ndel train_data['target']\ntrain_data.head()","7b2ef33f":"def clean_sentence(sentence):\n    sentence = sentence.lower()\n    words = sentence.split()\n    table = str.maketrans('', '', string.punctuation)\n    words = [w.translate(table) for w in words]\n    stop_words = stopwords.words('english')\n    words = [w for w in words if w not in stop_words]\n    porter = PorterStemmer()\n    stemmed = [porter.stem(word) for word in words]\n    return ' '.join(stemmed)","41173716":"locs = []\nfor loc in train_data[train_data['location'].notna()]['location']:\n    locs.append(clean_sentence(loc))\ntrain_data.loc[train_data['location'].notna(),'location'] = locs\n\nkeys = []\nfor keyword in train_data[train_data['keyword'].notna()]['keyword']:\n    keys.append(clean_sentence(keyword))\ntrain_data.loc[train_data['keyword'].notna(),'keyword'] = keys\n\nlocs = []\nfor loc in test_data[test_data['location'].notna()]['location']:\n    locs.append(clean_sentence(loc))\ntest_data.loc[test_data['location'].notna(),'location'] = locs\n\nkeys = []\nfor keyword in test_data[test_data['keyword'].notna()]['keyword']:\n    keys.append(clean_sentence(keyword))\ntest_data.loc[test_data['keyword'].notna(),'keyword'] = keys","bdae84c1":"train_data['location'] = train_data['location'].fillna(train_data['location'].mode()[0])\ntrain_data['keyword'] = train_data['keyword'].fillna(train_data['keyword'].mode()[0])\n\ntest_data['location'] = test_data['location'].fillna(test_data['location'].mode()[0])\ntest_data['keyword'] = test_data['keyword'].fillna(test_data['keyword'].mode()[0])","dfed2424":"test_data['location'] = test_data['location'].astype('category')\ntest_data['location'] = test_data['location'].cat.codes\n\ntrain_data['location'] = train_data['location'].astype('category')\ntrain_data['location'] = train_data['location'].cat.codes\n\ntest_data['keyword'] = test_data['keyword'].astype('category')\ntest_data['keyword'] = test_data['keyword'].cat.codes\n\ntrain_data['keyword'] = train_data['keyword'].astype('category')\ntrain_data['keyword'] = train_data['keyword'].cat.codes","eb898774":"train_data.head()","6839eb46":"test_data.head()","3714711c":"max_words = 0\nclean_text = []\nfor text in train_data['text']:\n    clean = clean_sentence(text)\n    clean_text.append(clean)\n    if len(clean) > max_words:\n        max_words = len(clean)\ntrain_data['text'] = clean_text\ntrain_data.head()\n\nclean_text = []\nfor text in test_data['text']:\n    clean = clean_sentence(text)\n    clean_text.append(clean)\n    if len(clean) > max_words:\n        max_words = len(clean)\ntest_data['text'] = clean_text\nprint(\"Max words: \", max_words)","308b8933":"train_data.head()","7549f994":"count_vect = CountVectorizer(ngram_range=(1,2))\nX = count_vect.fit_transform(train_data['text'])\nX_test = count_vect.transform(test_data['text'])\nvocab_size = len(count_vect.vocabulary_)\n\nprint(\"Vocabulary size: \", vocab_size)","dfe2f9a3":"tfidf_transformer = TfidfTransformer()\nX_tfidf = tfidf_transformer.fit_transform(X)\nX_test_tfidf = tfidf_transformer.transform(X_test)","af9b3964":"cv = StratifiedKFold(n_splits=10, shuffle=True, random_state=42)","3c65a5cb":"model = MultinomialNB(alpha=0.6)","3c064438":"scores = cross_val_score(model, X, y, cv=cv)\nprint(\"10-fold accuracy: \", np.mean(scores)*100, \"\\nstandard deviation: \", np.std(scores)*100)","8c8d8441":"scores = cross_val_score(model, X_tfidf, y, cv=cv)\nprint(\"10-fold accuracy: \", np.mean(scores)*100, \"\\nstandard deviation: \", np.std(scores)*100)","dc0ee271":"sgd_model = SGDClassifier(loss='hinge', penalty='l2', alpha=0.0001,random_state=42)","d9f25594":"scores = cross_val_score(sgd_model, X_tfidf, y, cv=cv)\nprint(\"10-fold accuracy: \", np.mean(scores)*100, \"\\nstandard deviation: \", np.std(scores)*100)","8d0f9452":"scores = cross_val_score(sgd_model, X, y, cv=cv)\nprint(\"10-fold accuracy: \", np.mean(scores)*100, \"\\nstandard deviation: \", np.std(scores)*100)","eb9681ad":"model = model.fit(X_tfidf, y)\npredictions = model.predict(X_test_tfidf)","7040d297":"\nprint(len(predictions), len(test_ids), X_test_tfidf.shape)","b7017729":"sub = pd.DataFrame({'id':test_ids, 'target':predictions})\nsub.head()","9c841f08":"sub.to_csv('submission.csv', index=False)","dc2920a5":"import tensorflow as tf\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Activation, Dense, Dropout, Flatten, SpatialDropout1D, LSTM\nfrom keras.layers.convolutional import Conv1D\nfrom keras.layers.convolutional import MaxPooling1D\nfrom keras.layers.embeddings import Embedding\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.metrics import categorical_crossentropy\nfrom keras.preprocessing import sequence\nfrom keras.preprocessing.text import Tokenizer\ntf.random.set_seed(42)","694d93b1":"physical_devices = tf.config.experimental.list_physical_devices(\"GPU\")\nprint(\"Num GPUs available: \", len(physical_devices))\nif len(physical_devices) > 0:\n    tf.config.experimental.set_memory_growth(physical_devices[0], True)","9e371268":"nn_model = Sequential([\n    Dense(units = 16, activation='relu', input_shape=(vocab_size,)),\n    Dropout(0.5),\n    Dense(units = 32, activation='relu'),\n    Dense(units=2, activation='softmax')\n])","50e01471":"nn_model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","6602a6d1":"early_stop = tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', mode='max', patience=3)\nhistory = nn_model.fit(x=X_tfidf.toarray(), y=y, validation_split=0.1, shuffle=True, batch_size=64, epochs=20, verbose=2, callbacks=[early_stop])","894ce499":"import matplotlib.pyplot as plt","7ae9eece":"plt.subplot(211)\nplt.title('Loss')\nplt.plot(history.history['loss'], label='train')\nplt.plot(history.history['val_loss'], label='test')\nplt.legend()\n# plot accuracy during training\nplt.subplot(212)\nplt.title('Accuracy')\nplt.plot(history.history['accuracy'], label='train')\nplt.plot(history.history['val_accuracy'], label='test')\nplt.legend()\nplt.show()","ea0dbd83":"predictions = np.argmax(nn_model.predict(X_test_tfidf.toarray()), axis=-1)\npredictions[:20]","fe47d6ae":"sub = pd.DataFrame({'id':test_ids, 'target':predictions})\nsub.to_csv('submission.csv', index=False)\nsub.head()","4984c970":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts(train_data['text'])","1742d709":"max_length = max([len(s.split()) for s in train_data['text']])\nvocab_size = len(tokenizer.word_index) + 1\nprint('Max document length: %d' % max_length)\nprint('Vocabulary size: %d' % vocab_size)","94958127":"encoded = tokenizer.texts_to_sequences(train_data['text'])\n# pad encoded sequences\npadded = sequence.pad_sequences(encoded, maxlen=max_length, padding='post')","26444f2e":"padded","3ac733c6":"cnn_model = Sequential([\n    Embedding(vocab_size, 64, input_length=max_length),\n    SpatialDropout1D(0.2),\n    LSTM(100, dropout=0.2, recurrent_dropout=0.2),\n    Dense(2, activation='softmax')\n])","4d99cd03":"cnn_model.compile(optimizer=Adam(learning_rate=0.0001), loss='sparse_categorical_crossentropy', metrics=['accuracy'])","b142cca6":"cnn_model.fit(x=padded, y=y , validation_split=0.1, shuffle=True, batch_size=64, epochs=10, verbose=2)","a3ed60e1":"## Training","e4be60a7":"Encode location and keyword features with discretization.","d439e225":"## Model training","a01ceb17":"## Model building","8ad97587":"Clean the sentences on text feature and get the max number of words in a sentence. ","42d70d2b":"# Libraries importing and configuration","3d5b6fce":"# Models training and validation","21ba8f3a":"### with unigram and bigram model","7bf64a8d":"### with tf-idf","ef24f09f":"## Saving results","fbdbff86":"## Multinomial Naive Bayes","f82a3b3f":"The clean_sentence function remove the stop words, punctuations, transform sentence to lower case and transform the words in stems.","41b6f30a":"### with tf-idf","740f8715":"Fill the missing data from locations and keywords with the mode.","57dcf99e":"# Multilayer perceptron","983ac09b":"# Saving results for the best model","83779a27":"<center> <h1><b>Keras<\/b><\/h1> <\/center>","a07de334":"All models will be evaluated by using the stratified 10-fold cross validation with data shuffle. ","8fcb7c7e":"# Preprocessing","de0cc9a8":"Clean all the locations and keywords from the train and test data.","e08fbcc8":"# Libraries importing and configuration","9bff376b":"## Data preparation","345f1b1a":"# Convolutional Neural Networks","0d5e6d7f":"## Model building","8da57aac":"## SGD classifier model","7cac3bd2":"Create a n-gram model for sentences vectorization and compute the vocabulary size.\nThis model will be one of the training sets.","5fc856db":"Also create another training set with the term frequency\u2013inverse document frequency.","11605f1c":"### with unigram and bigram model"}}