{"cell_type":{"e1992e32":"code","1ab15d38":"code","a97fe5b0":"code","8ef7ea95":"code","617017c6":"code","cfc9a1d7":"code","12767fb1":"code","70a52dc3":"code","6b8fd6f4":"code","cf825202":"code","eadf03b0":"code","59d46b41":"markdown","3ece1e05":"markdown","43ebc794":"markdown","3287c850":"markdown","6dbf1c27":"markdown","0ae874f3":"markdown","fdc4ed73":"markdown","21ae4bdb":"markdown"},"source":{"e1992e32":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","1ab15d38":"from sklearn.model_selection import train_test_split\n\ndata_file_path = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/train.csv'\ndata = pd.read_csv(data_file_path, index_col='Id')\n\n#features=['OverallQual','YearBuilt','YearRemodAdd','TotalBsmtSF','1stFlrSF','GrLivArea','GarageYrBlt','GarageArea']\n#X = data[features]\ny = data.SalePrice\ndata.drop(['SalePrice'], axis=1 , inplace = True)\nX = data.select_dtypes(exclude = ['object'])\n\ntrain_X, val_X, train_y, val_y = train_test_split(X,y,train_size=0.8, test_size = 0.2)","a97fe5b0":"cols_with_missing = [col for col in train_X.columns if train_X[col].isnull().any()]\n\ndropped_train_X = train_X.drop(cols_with_missing, axis=1)\ndropped_val_X = val_X.drop(cols_with_missing, axis=1)\n                          \nprint(dropped_train_X.shape)\nprint(train_X.shape)\n","8ef7ea95":"from sklearn.impute import SimpleImputer\n\nmy_imputer = SimpleImputer()\n\nimputed_train_X= pd.DataFrame(my_imputer.fit_transform(train_X))\nimputed_val_X = pd.DataFrame(my_imputer.transform(val_X))\n\nimputed_train_X.columns = train_X.columns\nimputed_val_X.columns = val_X.columns\n\nprint(imputed_train_X.shape)","617017c6":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.ensemble import RandomForestRegressor\n\ndef get_mae(train_X,train_y,val_X,val_y,n_estimators=10):\n    model = RandomForestRegressor(random_state=0, n_estimators = n_estimators)\n    model.fit(train_X,train_y)\n    pred = model.predict(val_X)\n    mae = mean_absolute_error(val_y, pred)\n    return mae","cfc9a1d7":"import matplotlib.pyplot as plt\n\ndef get_best_size(train_X,train_y,val_X,val_y):\n    mae_list = []\n    scales = np.linspace(10,100,91,dtype=\"int16\")\n    for i in scales:\n        mae = get_mae(train_X,train_y,val_X,val_y,n_estimators=i)\n        mae_list.append(mae)\n    min_mae = min(mae_list)\n    best_size = scales[mae_list.index(min_mae)]\n    print(best_size)\n    plt.plot(mae_list)\n    plt.show()\n    return best_size\n    ","12767fb1":"train_X_plus = train_X.copy()\nval_X_plus = val_X.copy()\n\nfor col in cols_with_missing:\n    train_X_plus[col+\"was_missing\"] = train_X_plus[col].isnull()\n    val_X_plus[col+\"was_missing\"] = val_X_plus[col].isnull()\n    \nimputed_train_X_plus = pd.DataFrame(my_imputer.fit_transform(train_X_plus))\nimputed_val_X_plus = pd.DataFrame(my_imputer.transform(val_X_plus))\n\nprint(imputed_train_X_plus.shape)","70a52dc3":"best_size = get_best_size(dropped_train_X, train_y , dropped_val_X , val_y)","6b8fd6f4":"mae_droped = get_mae(dropped_train_X, train_y , dropped_val_X , val_y ,best_size)\nmae_imputed = get_mae(imputed_train_X , train_y , imputed_val_X , val_y,best_size)\nmae_extension = get_mae(imputed_train_X_plus , train_y , imputed_val_X_plus , val_y,best_size)\n\nprint(f\"drop :{mae_droped} \\nimputation :{mae_imputed} \\nextension : {mae_extension}\")","cf825202":"test_file_path = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/test.csv'\ntest_data = pd.read_csv(test_file_path,index_col = 'Id').select_dtypes(exclude = ['object'])\ntest_data.drop(cols_with_missing, axis = 1 , inplace = True)\nimputed_test_data = pd.DataFrame(my_imputer.fit_transform(test_data))\n\ndropped_X = X.drop(cols_with_missing, axis = 1)\n\n\nmy_model = RandomForestRegressor(random_state = 1, n_estimators = 100)\nmy_model.fit(dropped_X, y)\npred = my_model.predict(imputed_test_data)\n","eadf03b0":"sample_file_path = '\/kaggle\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv'\nsample = pd.read_csv(sample_file_path, index_col=\"Id\")\n\noutput = pd.DataFrame({'Id': test_data.index,\n                       'SalePrice': pred})\noutput.to_csv('submission.csv', index=False)\n\n","59d46b41":"# train using the method has minimum MAE.\nit would be just drop-method\nFirst of all, You need to make shape of test data of shape of train data.\nSo you need to drop some columns that was dropped from train data. It is cols_with_missing.\nAnd then you need to check if there are missing data in test data and impute it to reasonable value such as mean.\n","3ece1e05":"it is to optimize a size of n_estimators for RandomForest","43ebc794":"# dropped data with missing","3287c850":"Set up train data and test data. It might have a missing data in it.","6dbf1c27":"# Extension to imputation\nFirt of all, you need to make a list consists of columns that have a null and then make new datas. ","0ae874f3":"# **Compare**\n","fdc4ed73":"# ***Imputation***\nLet's impute missing date to some reasonable value.","21ae4bdb":"# Make a submission file"}}