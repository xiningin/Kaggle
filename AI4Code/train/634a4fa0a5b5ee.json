{"cell_type":{"8fe53a99":"code","e9144113":"code","ae0fd77e":"code","8054ea89":"code","9fa7e2e1":"code","37714af5":"code","1eba70cb":"code","779ff706":"code","f783c55e":"code","86d3fe4a":"code","881a2b07":"code","edf2cb7a":"code","e5c7c39f":"code","2f0f0c88":"code","693587d1":"code","05088e1b":"code","74611ad5":"code","33e04337":"code","f05763ab":"code","9c5a9ff9":"code","8c1b08fd":"code","808d289d":"code","99aae49b":"code","9e9f1f18":"code","98615d8a":"code","10ff3d75":"code","6ea2cd95":"code","9317fdf4":"code","f2d325a6":"code","ead01a4b":"code","782fe8ee":"code","70642620":"code","d778ea64":"code","23943d8a":"code","5345aed5":"code","68c025b9":"code","54609384":"code","e8c8865a":"code","23de296f":"code","387cae25":"markdown","dcdf4c72":"markdown","b92c45df":"markdown","85460c04":"markdown","72c230df":"markdown","844e37ef":"markdown","71a0afb6":"markdown","51939d70":"markdown","cf632dd6":"markdown","2dd8b593":"markdown","ea1b2073":"markdown","42007a62":"markdown"},"source":{"8fe53a99":"import numpy as np \nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt","e9144113":"df=pd.read_csv('..\/input\/pokemon\/Pokemon.csv')","ae0fd77e":"df.head()","8054ea89":"# NUMERICAL FEATURE\n# CATEGORICAL FEATURE\n\ndf.info()","9fa7e2e1":"print(len(df['Name'].unique()))\nprint('We can see here, coulmn Name has all unique features')\nprint('='*50)\n\nprint(df['Type 1'].unique())\nprint(len(df['Type 1'].unique()))\nprint('='*50)\n\nprint(df['Type 2'].unique())\nprint(len(df['Type 2'].unique()))\nprint('='*50)\n\nprint(df['Generation'].unique())\nprint(len(df['Generation'].unique()))\nprint('this is a discrete feature BTW')\nprint('='*50)\n\n# we have:\n# 3 categorical feature\n# 1 binary (target feature) {we will convert it into discrete afterwards}\n# 1 discrete feature\n# 7 continous feature","37714af5":"df['Legendary'].replace(True,1,inplace=True)\ndf['Legendary'].replace(False,0,inplace=True)","1eba70cb":"categorical = [feature for feature in df.columns if df[feature].dtype == 'O' and feature not in 'Name']\ncategorical","779ff706":"continous = [feature for feature in df.columns if df[feature].dtype != 'O' and feature not in 'Generation'+'Legendary']\nprint(continous)\ndiscrete = ['Generation','Legendary']\nprint(discrete)","f783c55e":"df.Legendary.unique()","86d3fe4a":"# Graphs for continous numerical features\n# we make this graph for finding if any type of distribution is here in the feature\n\nfor feature in continous:\n  plt.figure(figsize=(7,5))\n  plt.hist(df[feature],bins=40)\n  plt.xlabel(feature)\n  plt.ylabel('count')\n  plt.title(feature)\n  plt.show()","881a2b07":"# ralation of continous with target with help of scatter plot\n\nfor feature in continous:\n    plt.scatter(df[feature],df['Legendary'])\n    plt.title('correlation between ' +feature+ ' and Legendary')\n    plt.xlabel(feature)\n    plt.ylabel('Legendary')\n    plt.show()","edf2cb7a":"gb=df.groupby('Type 1')\nprint(gb['Legendary'].value_counts())\nprint('='*65)\ngb=df.groupby('Type 2')\ngb['Legendary'].value_counts()","e5c7c39f":"# Graph for categorical features\n\nplt.figure(figsize=(13,7))\nsns.countplot(data=df,x='Type 1',hue='Legendary')\nplt.show()\nplt.figure(figsize=(13,7))\nsns.countplot(data=df,x='Type 2',hue='Legendary')\nplt.show()","2f0f0c88":"gb = df.groupby('Generation')\ngb['Legendary'].value_counts()","693587d1":"# Graphs for discrete numerical features\n\nplt.figure(figsize=(13,7))\nsns.countplot(data=df,x='Type 1',hue='Legendary')\nplt.show()","05088e1b":"# this graph lets us see corelation of features, between features and with the target feature.\n\nplt.figure(figsize=(13,7))\nsns.heatmap(df.corr(),annot=True)","74611ad5":"# checking outliers with help of box plot \n\nfor feature in continous:\n    sns.boxplot(x=feature,data=df)\n    plt.title(feature)\n    plt.show()","33e04337":"df=pd.read_csv('..\/input\/pokemon\/Pokemon.csv')\n\ndf.head(10)","f05763ab":"df.describe()\n\ndf['Type 2']\n# this is a mathematical way to see find outliers, and many other things we can infer from it!","9c5a9ff9":"# importing all the necessary libraries for of feature engineering\n\nfrom sklearn.preprocessing import OneHotEncoder\nencoder = OneHotEncoder(sparse=False,handle_unknown='error')\n\nfrom sklearn.preprocessing import StandardScaler\nscaler = StandardScaler(with_mean=False) #setting with_mean False, is for a reason !\n","8c1b08fd":"# here, we have missing values in Type 2, what we will do here is:\n# fill it with 'No Type' and not with mode. As there are many pokemons with no Type 2 abilty.\n\ndf.iloc[:,3].fillna(value='No Type', inplace=True)\nprint(df.head(10))","808d289d":"# Here, I dropped all unique features as they are not going get us prediction.\n\ndf.drop(columns=['Name','#'],inplace=True)","99aae49b":"# Here, I encoded all the values of categorical feature to numerical feature\n\ntemp = encoder.fit_transform(df.iloc[:,0:2])\ntemp = pd.DataFrame(temp)\ndf = pd.concat([df,temp],axis=1)","9e9f1f18":"# Let's see if everything is A-OK\n\ndf ","98615d8a":"# Now Split X and y\n\ny = df['Legendary']\nX = df.drop(columns=['Type 1','Type 2','Legendary'])","10ff3d75":"# Let's Scale our X for better model ! I have used StandardScaler, you can use any other here and see interesting effects !\n\ntemp=scaler.fit_transform(X)\nX = pd.DataFrame(temp)","6ea2cd95":"# Splitting our data for into train and test, for testing our accuracy\n\nfrom sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)","9317fdf4":"X_train","f2d325a6":"# As our target is to classify, if the pokemon is legendary or not, We are going to use LogisticRegression !\n\nfrom sklearn.linear_model import LogisticRegression\nlg = LogisticRegression()\n\nlg.fit(X_train,y_train)","ead01a4b":"y_pred = lg.predict(X_test)","782fe8ee":"# Let's see our accuracy on train data !\n\nlg.score(X_train,y_train)","70642620":"# Now, Let's check our accuracy on test data !\n\nlg.score(X_test,y_test)","d778ea64":"# Checking our Model by \n\nfrom sklearn.metrics import confusion_matrix\n\nconfusion_matrix(y_test, y_pred)","23943d8a":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(lg, X_test, y_test)","5345aed5":"from sklearn.ensemble import RandomForestClassifier\nrfc = RandomForestClassifier()\n\nrfc.fit(X_train,y_train)","68c025b9":"y_pred = rfc.predict(X_test)","54609384":"# Let's see our accuracy on train data !\n\nrfc.score(X_train,y_train)","e8c8865a":"# Now, Let's check our accuracy on test data !\n\nrfc.score(X_test,y_test)","23de296f":"from sklearn.metrics import plot_confusion_matrix\n\nplot_confusion_matrix(rfc, X_test, y_test)","387cae25":"# Model","dcdf4c72":"#### importing libraries","b92c45df":"# Random Forest Classifier","85460c04":"# Logistic Regression","72c230df":"#### no gaussion or log normal distribution found in above graphs...","844e37ef":"# Feature Engineering","71a0afb6":"# Pokemon dataset of seaborn\n#### In this Notebook, I have performed Exploratory Data Analysis(EDA), Feature Engineering and Model Fitting\n\n> I have used numpy, pandas, seaborn and matplotlib libraries for visulization in EDA\n\n> In Feature Engineering, I imputed missing values, removed outliers and Labeled categorical features and scaled all values.\n\n> In Model Fitting Section, I used most basic classification algorithm i.e. Logistic Regression and one of the most complex classification algorithm i.e. Random Forrest Classifier","51939d70":"#### Visualizing the above data below","cf632dd6":"# Exploratory Data Analysis","2dd8b593":"##### here we can see we have:\n> Total 800 Rows and 12 columns ","ea1b2073":"#### loading data-set","42007a62":"#### visualizing the above data below"}}