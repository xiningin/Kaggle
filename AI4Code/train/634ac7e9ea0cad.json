{"cell_type":{"e73c767d":"code","4385b0d7":"code","65551c80":"code","9839cb75":"code","2e67fb25":"code","6b608cdc":"code","72aebb39":"code","03324fb0":"code","f8e8d608":"code","788f3f26":"code","ca94d96c":"code","370367d1":"code","e6de86cf":"code","e0dde231":"code","1da1182e":"code","d91f24d8":"code","47cb7747":"code","963188ab":"code","d8e69144":"code","77d2bee9":"code","89c4808f":"code","23c1d27d":"code","cfc92413":"code","4c1e1979":"code","aca608ba":"code","1106afa8":"code","f7ac72e0":"code","bba6a72c":"code","20d45875":"code","e1db65c6":"code","2c8b8544":"code","42ec2a23":"code","3f8df20c":"code","9ba296d8":"code","5387d73b":"code","121a05f0":"code","7a0c013f":"code","74818675":"markdown","7ad16396":"markdown","d4435c51":"markdown","916bd1aa":"markdown","21e4af89":"markdown","94a233f0":"markdown","9ac31446":"markdown","cd3eeeff":"markdown","4027e27f":"markdown","f77e4120":"markdown","d811fce1":"markdown","ec061a8b":"markdown","3a03fd4e":"markdown","65031546":"markdown","b20b08d5":"markdown","c7a0cab5":"markdown","7bef109c":"markdown"},"source":{"e73c767d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4385b0d7":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport numpy as np","65551c80":"df_train = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/train.csv\")\ndf_test = pd.read_csv(\"..\/input\/house-prices-advanced-regression-techniques\/test.csv\")","9839cb75":"df_train.columns","2e67fb25":"df_train.SalePrice.describe()","6b608cdc":"# Histogram\nsns.distplot(df_train.SalePrice);","72aebb39":"#skewness and kurtosis\nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())","03324fb0":"sns.scatterplot(df_train.SalePrice, df_train.GrLivArea);","f8e8d608":"sns.scatterplot(df_train.SalePrice, df_train.TotalBsmtSF);","788f3f26":"sns.boxplot(df_train.OverallQual, df_train.SalePrice);","ca94d96c":"f, ax = plt.subplots(figsize = (16,8))\nsns.boxplot(df_train.YearBuilt, df_train.SalePrice);","370367d1":"corrmat = df_train.corr()\nf, ax = plt.subplots(figsize = (12, 9))\nsns.heatmap(corrmat, vmax=.8);","e6de86cf":"k = 10 # number of vars in the heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","e0dde231":"cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']","1da1182e":"#missing data\ntotal = df_train[cols].isnull().sum().sort_values(ascending=False)\npercent = (df_train[cols].isnull().sum()\/df_train[cols].isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","d91f24d8":"# missing data on test set\nif 'SalePrice' in cols:\n  cols.remove('SalePrice')\n\ntotal = df_test[cols].isnull().sum().sort_values(ascending=False)\npercent = (df_test[cols].isnull().sum()\/df_test[cols].isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","47cb7747":"df_test.TotalBsmtSF = df_test.TotalBsmtSF.fillna(0)\ndf_test.GarageCars = df_test.GarageCars.fillna(0)","963188ab":"# missing data on test set\ntotal = df_test[cols].isnull().sum().sort_values(ascending=False)\npercent = (df_test[cols].isnull().sum()\/df_test[cols].isnull().count()).sort_values(ascending=False)\nmissing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data","d8e69144":"from scipy import stats\nfrom scipy.stats import norm\n\n# histogram and normal probability plot\nsns.distplot(df_train.SalePrice, fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train.SalePrice, plot=plt)","77d2bee9":"df_train.SalePrice = np.log(df_train.SalePrice)","89c4808f":"# histogram and normal probability plot\nsns.distplot(df_train.SalePrice, fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train.SalePrice, plot=plt)","23c1d27d":"#histogram and normal probability plot\nsns.distplot(df_train['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['GrLivArea'], plot=plt)","cfc92413":"df_train['GrLivArea'] = np.log(df_train['GrLivArea'])","4c1e1979":"#transformed histogram and normal probability plot\nsns.distplot(df_train['GrLivArea'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['GrLivArea'], plot=plt)","aca608ba":"#histogram and normal probability plot\nsns.distplot(df_train['TotalBsmtSF'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train['TotalBsmtSF'], plot=plt)","1106afa8":"#if area>0 it gets 1, for area==0 it gets 0\ndf_train['HasBsmt'] = pd.Series(len(df_train['TotalBsmtSF']), index=df_train.index)\ndf_train['HasBsmt'] = 0 \ndf_train.loc[df_train['TotalBsmtSF']>0,'HasBsmt'] = 1","f7ac72e0":"#transform data\ndf_train.loc[df_train['HasBsmt']==1,'TotalBsmtSF'] = np.log(df_train['TotalBsmtSF']);","bba6a72c":"#histogram and normal probability plot\nsns.distplot(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], fit=norm);\nfig = plt.figure()\nres = stats.probplot(df_train[df_train['TotalBsmtSF']>0]['TotalBsmtSF'], plot=plt)","20d45875":"#convert categorical variable into dummy\ndf_train = pd.get_dummies(df_train)","e1db65c6":"df_train.head()","2c8b8544":"from sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nif 'SalePrice' in cols:\n  cols.remove('SalePrice')\nX = df_train[cols]\ny = df_train.SalePrice\n\nX_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.2, random_state=39)\n\nX_test = df_test[cols]\n\n# sc = StandardScaler()\n# X_train = sc.fit_transform(X_train)\n# X_val = sc.transform(X_val)\n","42ec2a23":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.svm import SVR\n\nregressor = RandomForestRegressor(n_estimators=200, random_state=0)\nregressor.fit(X_train, y_train)\ny_pred = np.exp(regressor.predict(X_val))\ny_val = np.exp(y_val)","3f8df20c":"from sklearn import metrics\n\nprint('Mean Absolute Error:', metrics.mean_absolute_error(y_val, y_pred))\nprint('Mean Squared Error:', metrics.mean_squared_error(y_val, y_pred))\nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_val, y_pred)))\nprint('RMSE\/Mean (Lower is Better):', np.sqrt(metrics.mean_squared_error(y_val, y_pred)) \/ y_val.mean() )","9ba296d8":"y_test_pred = np.exp(regressor.predict(X_test))","5387d73b":"sub = pd.DataFrame()\nsub['Id'] = df_test.Id\nsub['SalePrice'] = y_test_pred","121a05f0":"sub","7a0c013f":"sub.to_csv('submission.csv',index=False)","74818675":"Here we have a lot of houses that does not have a basement, log(0) is undefined. That's why we would only apply log to houses with basements.","7ad16396":"What we see is:\n* SalePrice is not normal distributed\n* Have positive skewness","d4435c51":"## Splitting and scaling the data","916bd1aa":"## Let's check if there is linearity between the chosen features and SalePrice","21e4af89":"Now lets check GrLivArea","94a233f0":"## Normality ","9ac31446":"What we see is that we have a lot of features we need to consider. So to simplify the problem we are going to analyse it and extract the most important ones. First of all there is a data description file in the dataset, which can help us.\n\nWhat I found is that the most important features are: 'OverallQuall', 'YearBuilt', 'Neighborhood', 'TotalBsmtSF' and 'GrLivArea'","cd3eeeff":"## Now let's see all the correlations between all features in a correlation matrix","4027e27f":"## What about 'SalePrice' ?","f77e4120":"Base on the heatmap above we can extract these most correlated features and remove the features that are correlated with one another like 'TotalBsmtSF' and '1stFlrSF'\n\nWe come up with the following features:","d811fce1":"## Dummy variables","ec061a8b":"As discussed above, 'SalePrice' is not normally distributed. We can solve this with simple log transformation.","3a03fd4e":"What we see is that:\n* GrLivArea, TotalBsmtSF and OverallQual are linearly correlated with SalePrice.\n* YearBuild is not as high correlated as I expexted.","65031546":"# House Prices Prediction - [Kaggle Competition](https:\/\/www.kaggle.com\/c\/house-prices-advanced-regression-techniques)\n\nby Hristo Dinkov\n\n## Competition Description\nAsk a home buyer to describe their dream house, and they probably won't begin with the height of the basement ceiling or the proximity to an east-west railroad. But this playground competition's dataset proves that much more influences price negotiations than the number of bedrooms or a white-picket fence.\n\nWith 79 explanatory variables describing (almost) every aspect of residential homes in Ames, Iowa, this competition challenges you to predict the final price of each home.\n\n## Practise skills:\n* Creative feature engineering \n* Practise regression techniques like random forest and gradient boosting","b20b08d5":"## Missing data","c7a0cab5":"Now we can see the big picture and we notice that there are a lot of uncorrelated features to SalePrice.\n\nNow let's zoom a little bit further","7bef109c":"No missing data in the columns we are interested in"}}