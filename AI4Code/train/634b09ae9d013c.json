{"cell_type":{"40351f78":"code","b8f186ac":"code","5882103f":"code","b0c7a4c8":"code","5c9685ac":"code","df5c02b0":"code","065b7a00":"code","1b07c937":"code","24dc6a54":"code","d92b7828":"code","f370398f":"code","5f40f8f4":"code","3eca58cf":"code","1615dca3":"code","9dd8078d":"code","5a8f7c99":"code","e36381b2":"code","42b56a7f":"code","dc74722f":"markdown","4e09d7e5":"markdown","c8697580":"markdown","723eb500":"markdown","17a9ffc3":"markdown"},"source":{"40351f78":"!pip install --no-deps '..\/input\/timm-package\/timm-0.1.26-py3-none-any.whl' > \/dev\/null\n!pip install --no-deps '..\/input\/pycocotools\/pycocotools-2.0-cp37-cp37m-linux_x86_64.whl' > \/dev\/null","b8f186ac":"import warnings\n\nwarnings.filterwarnings(\"ignore\")\n\nimport sys\nsys.path.insert(0, \"..\/input\/timm-efficientdet-pytorch\/timm-efficientdet-pytorch\/\")\nsys.path.insert(0, \"..\/input\/omegaconf\")\n\nimport torch\nimport os\nfrom datetime import datetime\nfrom tqdm import tqdm\nimport time\nimport random\nimport cv2\nimport pandas as pd\nimport numpy as np\nimport albumentations as A\nimport matplotlib.pyplot as plt\nfrom albumentations.pytorch.transforms import ToTensorV2\nfrom sklearn.model_selection import StratifiedKFold\nfrom torch.utils.data import Dataset,DataLoader\nfrom torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom glob import glob\nfrom effdet import get_efficientdet_config, EfficientDet, DetBenchTrain,DetBenchEval\nfrom effdet.efficientdet import HeadNet","5882103f":"DATA_ROOT_PATH = 'test_images'\nSEED = 42\nFOLDS = 5\nFOLD = 0\nEFF_DET = 5 \nThreshold = 0.3\nLOOK_BACKWARD_LIMIT = 10\nIOU_THRESHOLD = 0.5\nIMAGE_SIZE = 512\nNUM_CLASSES = 2\nBATCH_SIZE = 16\nFRAME_THRESHOLD = 100\nCHECKPOINT = '..\/input\/nfl-512-detection-classification\/effdet5-models-fold-4\/effdet5-models-fold-4\/checkpoint_epoch9.bin'","b0c7a4c8":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n\nseed_everything(SEED)","5c9685ac":"# df = pd.read_json('..\/input\/nfl-2020-video-frames\/meta_data_5.json')\n# test_df = df.loc[df.fold==FOLD,['video','image','frame','image_path']].reset_index(drop=True)\n# print(test_df.shape)\n# test_df.sample(2)","df5c02b0":"def mk_images(video_name, video_labels, video_dir, out_dir):\n    video_path=f\"{video_dir}\/{video_name}\"\n    video_name = os.path.basename(video_path)\n    vidcap = cv2.VideoCapture(video_path)\n    frame = 0\n    while True:\n        it_worked, img = vidcap.read()\n        if not it_worked:\n            break\n        frame += 1\n        img_name = f\"{video_name}_frame{frame}\"\n        image_path = f'{out_dir}\/{video_name}'.replace('.mp4',f'_{frame}.png')\n        _ = cv2.imwrite(image_path, img)\n        if frame==FRAME_THRESHOLD:\n            break\n        \n        \nout_dir = DATA_ROOT_PATH\nif not os.path.exists(out_dir):\n    !mkdir -p $out_dir\n    video_dir = '\/kaggle\/input\/nfl-impact-detection\/test'\n    uniq_video = [path.split('\/')[-1] for path in glob(f'{video_dir}\/*.mp4')]\n    for video_name in tqdm(uniq_video):\n        mk_images(video_name, pd.DataFrame(), video_dir, out_dir)\n        \nimage_root = DATA_ROOT_PATH + '\/'\ndf = pd.DataFrame(os.listdir(image_root),columns=['image'])\ndf['image_path'] = image_root+df['image']\ndf['video'] = df.image.apply(lambda x:'_'.join(x.split('_')[:-1])+'.mp4')\ndf['frame'] = df.image.apply(lambda x:x.split('_')[-1].split('.')[0]).astype(int)\ntest_df = df\nprint(test_df.shape)\ntest_df.sample(2)","065b7a00":"class DatasetRetriever(Dataset):\n    def __init__(self, df, transforms=None):\n        super().__init__()\n        self.df = df\n        self.transforms = transforms\n\n    def __getitem__(self, index: int):\n        row = self.df.loc[index]\n        image_id = row.image\n        image_path = row.image_path\n        image = cv2.imread(image_path, cv2.IMREAD_COLOR).copy().astype(np.float32)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        if self.transforms:\n            sample = {'image': image}\n            sample = self.transforms(**sample)\n            image = sample['image']\n        return image, image_id\n\n    def __len__(self) -> int:\n        return self.df.shape[0]\n\ndef get_test_transforms():\n    return A.Compose([\n            A.Resize(height=512, width=512, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.0)\n\ntest_dataset = DatasetRetriever(\n    df=test_df,\n    transforms=get_test_transforms(),\n)\nlen(test_dataset)","1b07c937":"def load_net(checkpoint_path):\n    config = get_efficientdet_config('tf_efficientdet_d{}'.format(EFF_DET))\n    net = EfficientDet(config, pretrained_backbone=False)\n    config.num_classes = 2\n    config.image_size=512\n    net.class_net = HeadNet(config, num_outputs=config.num_classes, norm_kwargs=dict(eps=.001, momentum=.01))\n    checkpoint = torch.load(checkpoint_path)\n    net.load_state_dict(checkpoint['model_state_dict'])\n    net = DetBenchEval(net, config)\n    net.eval();\n    return net.cuda()\n\nnet = load_net(CHECKPOINT)","24dc6a54":"def collate_fn(batch):\n    return tuple(zip(*batch))\n\ndata_loader = DataLoader(\n    test_dataset,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=4,\n    drop_last=False,\n    collate_fn=collate_fn\n)\nlen(data_loader)","d92b7828":"def iou(bbox1, bbox2):\n    bbox1 = [float(x) for x in bbox1]\n    bbox2 = [float(x) for x in bbox2]\n\n    (x0_1, y0_1, x1_1, y1_1) = bbox1\n    (x0_2, y0_2, x1_2, y1_2) = bbox2\n\n    # get the overlap rectangle\n    overlap_x0 = max(x0_1, x0_2)\n    overlap_y0 = max(y0_1, y0_2)\n    overlap_x1 = min(x1_1, x1_2)\n    overlap_y1 = min(y1_1, y1_2)\n\n    # check if there is an overlap\n    if overlap_x1 - overlap_x0 <= 0 or overlap_y1 - overlap_y0 <= 0:\n            return 0\n\n    # if yes, calculate the ratio of the overlap to each ROI size and the unified size\n    size_1 = (x1_1 - x0_1) * (y1_1 - y0_1)\n    size_2 = (x1_2 - x0_2) * (y1_2 - y0_2)\n    size_intersection = (overlap_x1 - overlap_x0) * (overlap_y1 - overlap_y0)\n    size_union = size_1 + size_2 - size_intersection\n\n    return size_intersection \/ size_union\n\ndef filter_duplicates(row):\n    # Compute intersection over union\n    boxes,scores,labels = row.box_predictions,row.score_list,row.label_list\n    filtered_boxes = []\n    filtered_scores = []\n    filtered_labels = []\n    for box,score,label in zip(boxes,scores,labels):\n        # Assume score_list is sorted\n        add_box = True\n        for i,ref_box in enumerate(filtered_boxes):\n            if iou(box,ref_box)>0.5:\n                add_box = False\n                if label==2:filtered_labels[i]=2\n                \n        if add_box: \n            filtered_boxes.append(box)\n            filtered_scores.append(score)\n            filtered_labels.append(label)\n    row.box_predictions,row.score_list,row.label_list = filtered_boxes,filtered_scores,filtered_labels\n    return row","f370398f":"def make_predictions(images, score_threshold=0.5):\n    images = torch.stack(images).cuda().float()\n    box_list = []\n    score_list = []\n    label_list = []\n    with torch.no_grad():\n        det = net(images, torch.tensor([1]*images.shape[0]).float().cuda())\n        for i in range(images.shape[0]):\n            boxes = det[i].detach().cpu().numpy()[:,:4]    \n            scores = det[i].detach().cpu().numpy()[:,4]   \n            label = det[i].detach().cpu().numpy()[:,5]\n            boxes[:, 2] = boxes[:, 2] + boxes[:, 0]\n            boxes[:, 3] = boxes[:, 3] + boxes[:, 1]\n            boxes[:,0] = boxes[:,0] * 1280 \/ 512\n            boxes[:,1] = boxes[:,1] * 720 \/ 512\n            boxes[:,2] = boxes[:,2] * 1280 \/ 512\n            boxes[:,3] = boxes[:,3] * 720 \/ 512\n            indexes = np.where((scores > score_threshold) )\n            box_list.append(boxes[indexes])\n            score_list.append(scores[indexes])\n            label_list.append(label[indexes])\n            \n    return box_list, score_list, label_list","5f40f8f4":"predictions = pd.DataFrame(index=range(len(test_df)),\n                      columns = [ 'image', 'box_predictions', 'score_list','label_list'])\n\ntest_index = 0\ntest_batch = BATCH_SIZE\nfor step, (images, image_ids) in tqdm(enumerate(data_loader)):\n    start_index = test_index\n    end_index = min(len(test_df)-1,test_index+test_batch-1)\n    n_samples = end_index-start_index+1\n    box_list, score_list, label_list = make_predictions(images, score_threshold=Threshold)\n    predictions.loc[start_index:end_index,'image'] = image_ids[:n_samples]\n    predictions.loc[start_index:end_index,'box_predictions'] = box_list[:n_samples]\n    predictions.loc[start_index:end_index,'score_list'] = score_list[:n_samples]\n    predictions.loc[start_index:end_index,'label_list'] = label_list[:n_samples]\n    test_index += test_batch\n    if end_index==(len(test_df)-1): break\npredictions = predictions.apply(filter_duplicates,axis=1)\npredictions = pd.merge(predictions,test_df,on=['image'])\npredictions.to_json('predictions.csv',orient='records')\npredictions['n_impact'] = predictions.label_list.apply(lambda x:(np.array(x)==2).sum())\npredictions.groupby('video').n_impact.sum()","3eca58cf":"image = cv2.imread(predictions.loc[0,'image_path'])\npred_boxes = predictions.loc[0,'box_predictions']\nimage_id = predictions.loc[0,'image']\nprint(image_id,image.shape)\n\nfig, ax = plt.subplots(1, 1, figsize=(16, 8))\n\nfor box in pred_boxes:\n    cv2.rectangle(image, (box[0], box[1]), (box[2],  box[3]), (1, 0, 0), 3)\n    \nax.set_axis_off()\nax.imshow(image);","1615dca3":"import math\ndef iou(bbox1, bbox2):\n    bbox1 = [float(x) for x in bbox1]\n    bbox2 = [float(x) for x in bbox2]\n\n    (x0_1, y0_1, x1_1, y1_1) = bbox1\n    (x0_2, y0_2, x1_2, y1_2) = bbox2\n\n    # get the overlap rectangle\n    overlap_x0 = max(x0_1, x0_2)\n    overlap_y0 = max(y0_1, y0_2)\n    overlap_x1 = min(x1_1, x1_2)\n    overlap_y1 = min(y1_1, y1_2)\n\n    # check if there is an overlap\n    if overlap_x1 - overlap_x0 <= 0 or overlap_y1 - overlap_y0 <= 0:\n            return 0\n\n    # if yes, calculate the ratio of the overlap to each ROI size and the unified size\n    size_1 = (x1_1 - x0_1) * (y1_1 - y0_1)\n    size_2 = (x1_2 - x0_2) * (y1_2 - y0_2)\n    size_intersection = (overlap_x1 - overlap_x0) * (overlap_y1 - overlap_y0)\n    size_union = size_1 + size_2 - size_intersection\n\n    return size_intersection \/ size_union\n\ndef match_boxes(bbox_set1,bbox_set2,diagonal=True,iou_threshold=0.35):\n    # Compute intersection over union\n    all_pairs = {}\n    relation = [None for x in bbox_set2]\n    done1 = {}\n    done2 = {}\n    for i,bbox1 in enumerate(bbox_set1):\n        for j,bbox2 in enumerate(bbox_set2):\n            if i!=j or diagonal:\n                all_pairs[(i,j)] = iou(bbox1,bbox2)\n    for (index1,index2),iou_score in sorted(all_pairs.items(), key=lambda item: item[1],reverse=True):\n        if iou_score<iou_threshold: return relation\n        if (index1 not in done1) and (index2 not in done2):\n            relation[index2] = index1\n            done1[index1] = 1\n            done2[index2] = 1\n    return relation\n\ndef map_classes(row):\n    mapping = {}\n    for box,pred in zip(row.box_predictions,row.label_list):\n        mapping[tuple(box)] = pred\n    return mapping\n\ndef assign_players(df,video,LOOK_BACKWARD_LIMIT=LOOK_BACKWARD_LIMIT,IOU_THRESHOLD=IOU_THRESHOLD):\n    video_df = df[df.video==video].sort_values(by='frame').set_index('frame')\n    row_to_class_map = video_df['row_to_class_map']\n    row_to_class_map.apply(lambda x:x.update({():-1}))\n    player_to_bbox = pd.DataFrame()\n    bbox_to_player = {k+1:{} for k in range(video_df.shape[0])}\n    N_PLAYERS = 0\n    # Initial Player Prediction\n    bboxes = video_df.loc[1,'box_predictions']\n    for player_id,box in enumerate(bboxes):\n        player_to_bbox[player_id] = None\n        player_to_bbox.at[1,player_id] = box\n        bbox_to_player[1][tuple(box)] = player_id\n        N_PLAYERS += 1\n        \n\n    # Subsequent Player Prediction\n    for frame in range(2,video_df.shape[0]+1):\n        player_to_bbox.loc[frame] = None\n        bboxes = video_df.loc[frame,'box_predictions']\n        assigned = {}\n        look_backward = 1\n        while(look_backward<min(frame,LOOK_BACKWARD_LIMIT+1) and len(bboxes)>0):\n            ref_bbox = []\n            for box in video_df.loc[frame-look_backward,'box_predictions']:\n                if bbox_to_player[frame-look_backward][tuple(box)] not in assigned:\n                    ref_bbox.append(box)\n            relation = match_boxes(ref_bbox,bboxes,iou_threshold=IOU_THRESHOLD)\n            left_out_bboxes = []\n            # Assign Players\n            for index,ref_index in enumerate(relation):\n                if ref_index is not None:\n                    # Read Player ID and box\n                    player_id = bbox_to_player[frame-look_backward][tuple(ref_bbox[ref_index])]\n                    box = bboxes[index]\n                    # Write Player ID\n                    player_to_bbox.at[frame,player_id] = box\n                    bbox_to_player[frame][tuple(box)] = player_id\n                    assigned[player_id] = True\n                else:\n                    left_out_bboxes.append(bboxes[index])\n\n            bboxes = left_out_bboxes\n            look_backward +=1\n        # New Player Addition\n        for box in bboxes:\n            player_id = N_PLAYERS\n            player_to_bbox[player_id] = None\n            player_to_bbox.at[frame,player_id] = box\n            bbox_to_player[frame][tuple(box)] = player_id\n            N_PLAYERS += 1\n            \n    # Map Classes to Players\n    player_to_class_map = player_to_bbox.apply(lambda row: row.fillna('').apply(tuple).apply(\n            lambda x:row_to_class_map.loc[row.name][x]),axis=1)\n    \n    return player_to_bbox,player_to_class_map\n\ndef filter_impacts(player_series):\n    impacted_player_series = pd.DataFrame(player_series.loc[player_series==2])\n    impacted_player_series['indices'] = impacted_player_series.index\n    start_indices = impacted_player_series[impacted_player_series.indices.diff(1).fillna(99).abs()>9].indices.tolist()\n    end_indices = impacted_player_series[impacted_player_series.indices.diff(-1).fillna(99).abs()>9].indices.tolist()\n    output_series = player_series.copy(deep=True)\n    output_series[player_series>0] = 1\n    output_series[player_series<0] = -1\n    for start_index,end_index in zip(start_indices,end_indices):\n        difference = end_index-start_index+1\n        n_medians = math.ceil(difference\/9)\n        difference_split = difference\/(n_medians+1)\n        centres = [int(start_index+(i+1)*difference_split) for i in range(n_medians)]\n        output_series.loc[centres] = 2\n    return output_series\n","9dd8078d":"submission_df = pd.DataFrame(columns=['gameKey','playID','view','video','frame','left','width','top','height'])\npredictions['row_to_class_map'] = predictions.apply(map_classes,axis=1)\nentries = 0\nfor video in predictions.video.unique():\n    # Assign Players\n    assigned_players_boxes,assigned_players_classes = assign_players(predictions,video)\n    # Filter interframe bbox\n    assigned_players_classes_filtered = assigned_players_classes.apply(filter_impacts)\n    n_filtered = (assigned_players_classes_filtered==2).sum().sum()\n    n_classes = (assigned_players_classes==2).sum().sum()\n    print(video,n_filtered,n_classes,(~assigned_players_boxes.isna()).sum().sum())\n    \n    # Only fill if frame gap between labelled boxes<10\n    all_bboxes = assigned_players_boxes.fillna(method='ffill').values.reshape(-1)\n    \n    \n    all_classes = assigned_players_classes_filtered.values.reshape(-1)\n    all_frames = assigned_players_boxes.apply(lambda x: pd.DataFrame(x).apply(\n        lambda y: y.name, axis=1)).values.reshape(-1)\n    bboxes_with_impact = all_bboxes[np.where(all_classes==2)]\n    frames_with_impact = all_frames[np.where(all_classes==2)]\n    \n    # Write to dataframe\n    gameKey,playID,view,_ = video.replace('.','_').split('_')\n    for frame,box in zip(frames_with_impact,bboxes_with_impact):\n        submission_df.loc[entries,'gameKey'] = gameKey\n        submission_df.loc[entries,'playID'] = playID\n        submission_df.loc[entries,'view'] = view\n        submission_df.loc[entries,'video'] = video\n        submission_df.loc[entries,'frame'] = frame\n        submission_df.loc[entries,'left'] = int(box[0])\n        submission_df.loc[entries,'top'] = int(box[1])\n        submission_df.loc[entries,'width'] = int(box[2]-box[0])\n        submission_df.loc[entries,'height'] = int(box[3]-box[1])\n        entries += 1","5a8f7c99":"submission_df = submission_df[submission_df.frame>20]\nsubmission_df","e36381b2":"# clearing working dir\n# be careful when running this code on local environment!\n# !rm -rf *\n!mv * \/tmp\/","42b56a7f":"import nflimpact\nenv = nflimpact.make_env()\nenv.predict(submission_df) ","dc74722f":"# Postprocessing","4e09d7e5":"### Some parts of my work is taken from: https:\/\/www.kaggle.com\/its7171\/2class-object-detection-inference\n","c8697580":"# Preparing Submission File","723eb500":"# Visualisation","17a9ffc3":"# Submission"}}