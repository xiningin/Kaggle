{"cell_type":{"6ca05e23":"code","7082ada1":"code","9241a008":"code","d816df14":"code","b03aa888":"code","57c535ff":"code","0dba51e1":"code","8e297843":"code","b3aeede5":"code","1bca535f":"code","6f6e28a2":"code","303a6cd7":"code","03eccef5":"code","5eb95e26":"code","bad010f9":"code","d8faa65c":"code","de45494d":"code","947a4e8a":"code","9665dab7":"code","49e64738":"code","9dc0df4e":"code","43f94283":"code","926d0e2b":"code","343f7e02":"code","d7087675":"code","17c17b7a":"code","471433f7":"code","554798b7":"code","43937ea7":"markdown","672f2135":"markdown","24482947":"markdown","4be77fc9":"markdown","080dd5ca":"markdown","e891cdee":"markdown","3d77f936":"markdown","76291b34":"markdown","cf14a44d":"markdown","d3da9e6b":"markdown","0c083aac":"markdown","37ca7d4f":"markdown","eba32da8":"markdown","947b8681":"markdown","5f845884":"markdown","62fbbe87":"markdown","5732a725":"markdown","a5eb59fc":"markdown","b8c0628e":"markdown","ab9f8023":"markdown","f0a6f670":"markdown","82c72739":"markdown","64c0627f":"markdown","b202bf6e":"markdown"},"source":{"6ca05e23":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport os\nimport sys\nimport json\nimport tensorflow as tf # Yes, we are going to play with Tensorflow 2!\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport absl # For using flags without tf.compat.v1.flags.Flag\nimport datetime","7082ada1":"# Input data files are available in the \"..\/input\/\" directory.\nIS_KAGGLE = True\nINPUT_DIR = \"\/kaggle\/input\/\"\n\n# The original Bert Joint Baseline data.\nBERT_JOINT_BASE_DIR = os.path.join(INPUT_DIR, \"bertjointbaseline\")\n\n# This nq dir contains all files for publicly use.\nNQ_DIR = os.path.join(INPUT_DIR, \"nq-competition\")\n\n# If you want to use your own .tfrecord or new trained checkpoints, you can put them under you own nq dir (`MY_OWN_NQ_DIR`)\n# Default to NQ_DIR. You have to change it to the dir containing your own working files.\nMY_OWN_NQ_DIR = NQ_DIR\n\n# For local usage.\nif not os.path.isdir(INPUT_DIR):\n    IS_KAGGLE = False\n    INPUT_DIR = \".\/\"\n    NQ_DIR = \".\/\"\n    MY_OWN_NQ_DIR = \".\/\"\n\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\nfor dirname, _, filenames in os.walk(INPUT_DIR):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","9241a008":"# NQ_DIR contains some packages \/ modules\nsys.path.append(NQ_DIR)\nsys.path.append(os.path.join(NQ_DIR, \"transformers\"))\n\nfrom nq_flags import DEFAULT_FLAGS as FLAGS\nfrom nq_flags import del_all_flags\n\nimport sacremoses as sm\nimport transformers\nfrom adamw_optimizer import AdamW","d816df14":"PRETRAINED_MODELS = {\n    \"BERT\": [\n        'bert-base-uncased',\n        'bert-large-uncased',\n        'bert-base-cased',\n        'bert-large-cased',\n        'bert-base-multilingual-uncased',\n        'bert-base-multilingual-cased',\n        'bert-base-chinese',\n        'bert-base-german-cased',\n        'bert-large-uncased-whole-word-masking',\n        'bert-large-cased-whole-word-masking',\n        'bert-large-uncased-whole-word-masking-finetuned-squad',\n        'bert-large-cased-whole-word-masking-finetuned-squad',\n        'bert-base-cased-finetuned-mrpc'\n    ],\n    \"DISTILBERT\": [\n        'distilbert-base-uncased',\n        'distilbert-base-uncased-distilled-squad'\n    ]\n}","b03aa888":"flags = absl.flags\ndel_all_flags(flags.FLAGS)\n\nflags.DEFINE_bool(\n    \"do_lower_case\", True,\n    \"Whether to lower case the input text. Should be True for uncased \"\n    \"models and False for cased models.\")\n\nflags.DEFINE_string(\"vocab_file\", \"vocab-nq.txt\",\n                    \"The vocabulary file that the BERT model was trained on.\")\n\nflags.DEFINE_integer(\n    \"max_seq_length_for_training\", 512,\n    \"The maximum total input sequence length after WordPiece tokenization for training examples. \"\n    \"Sequences longer than this will be truncated, and sequences shorter \"\n    \"than this will be padded.\")\n\nflags.DEFINE_integer(\n    \"max_seq_length\", 384,\n    \"The maximum total input sequence length after WordPiece tokenization. \"\n    \"Sequences longer than this will be truncated, and sequences shorter \"\n    \"than this will be padded.\")\n\nflags.DEFINE_integer(\n    \"doc_stride\", 128,\n    \"When splitting up a long document into chunks, how much stride to \"\n    \"take between chunks.\")\n\nflags.DEFINE_float(\n    \"include_unknowns_for_training\", 0.02,\n    \"If positive, for converting training dataset, probability of including answers of type `UNKNOWN`.\")\n\nflags.DEFINE_float(\n    \"include_unknowns\", -1.0,\n    \"If positive, probability of including answers of type `UNKNOWN`.\")\n\nflags.DEFINE_boolean(\n    \"skip_nested_contexts\", True,\n    \"Completely ignore context that are not top level nodes in the page.\")\n\nflags.DEFINE_integer(\"max_contexts\", 48,\n                     \"Maximum number of contexts to output for an example.\")\n\nflags.DEFINE_integer(\n    \"max_position\", 50,\n    \"Maximum context position for which to generate special tokens.\")\n\nflags.DEFINE_integer(\n    \"max_query_length\", 64,\n    \"The maximum number of tokens for the question. Questions longer than \"\n    \"this will be truncated to this length.\")","57c535ff":"if os.path.isfile(os.path.join(MY_OWN_NQ_DIR, \"nq_train.tfrecord\")):\n    TRAIN_TF_RECORD = os.path.join(MY_OWN_NQ_DIR, \"nq_train.tfrecord\")\nelif os.path.isfile(os.path.join(MY_OWN_NQ_DIR, \"nq-train.tfrecords-00000-of-00001\")):\n    TRAIN_TF_RECORD = os.path.join(MY_OWN_NQ_DIR, \"nq-train.tfrecords-00000-of-00001\")\nelse:\n    TRAIN_TF_RECORD = os.path.join(BERT_JOINT_BASE_DIR, \"nq-train.tfrecords-00000-of-00001\")\n    \nflags.DEFINE_string(\"train_tf_record\", TRAIN_TF_RECORD,\n                    \"Precomputed tf records for training dataset.\")\n\nflags.DEFINE_string(\"valid_tf_record\", os.path.join(NQ_DIR, \"nq_valid.tfrecord\"),\n                    \"Precomputed tf records for validation dataset.\")\n\nflags.DEFINE_string(\"valid_small_tf_record\", os.path.join(NQ_DIR, \"nq_valid_small.tfrecord\"),\n                    \"Precomputed tf records for a smaller validation dataset.\")\n\n# This file should be generated when the kernel is running using the provided test dataset!\n# flags.DEFINE_string(\"test_tf_record\", \"nq_test.tfrecord\",\n#                     \"Precomputed tf records for test dataset.\")\n\nflags.DEFINE_bool(\"do_train\", True, \"Whether to run training dataset.\")\n\nflags.DEFINE_bool(\"do_valid\", True, \"Whether to run validation dataset.\")\n\nflags.DEFINE_bool(\"smaller_valid_dataset\", True, \"Whether to use the smaller validation dataset\")\n\nflags.DEFINE_bool(\"do_predict\", True, \"Whether to run test dataset.\")\n\nflags.DEFINE_string(\n    \"validation_output_file\", \"validation.json\",\n    \"Where to print predictions for validation dataset in NQ prediction format, to be passed to natural_questions.nq_eval.\")\n\nflags.DEFINE_string(\n    \"prediction_output_file\", \"predictions.json\",\n    \"Where to print predictions for test dataset in NQ prediction format, to be passed to natural_questions.nq_eval.\")\n\nflags.DEFINE_string(\n    \"input_checkpoint_dir\", os.path.join(MY_OWN_NQ_DIR, \"checkpoints\"),\n    \"The root directory that contains checkpoints to be loaded of all trained models.\")\n\nflags.DEFINE_string(\n    \"output_checkpoint_dir\", \"checkpoints\",\n    \"The output directory where the model checkpoints will be written to.\")\n\n# If you want to use other Hugging Face's models, change this to `MY_OWN_NQ_DIR` and put the downloaded models at the right place.\nflags.DEFINE_string(\"model_dir\", NQ_DIR, \"Root dir of all Hugging Face's models\")\n\nflags.DEFINE_string(\"model_name\", \"distilbert-base-uncased-distilled-squad\", \"Name of Hugging Face's model to use.\")\n# flags.DEFINE_string(\"model_name\", \"bert-base-uncased\", \"Name of Hugging Face's model to use.\")\n# flags.DEFINE_string(\"model_name\", \"bert-large-uncased-whole-word-masking-finetuned-squad\", \"Name of Hugging Face's model to use.\")\n\nflags.DEFINE_integer(\"epochs\", 1, \"Total epochs for training.\")\n\nflags.DEFINE_integer(\"train_batch_size\", 10, \"Batch size for training.\")\n\nflags.DEFINE_integer(\"shuffle_buffer_size\", 10000, \"Shuffle buffer size for training.\")\n\nflags.DEFINE_integer(\"batch_accumulation_size\", 10, \"Number of batches to accumulate gradient before applying optimization.\")\n\nflags.DEFINE_float(\"init_learning_rate\", 1e-4, \"The initial learning rate for AdamW optimizer.\")\n\nflags.DEFINE_bool(\"cyclic_learning_rate\", False, \"If to use cyclic learning rate.\")\n\nflags.DEFINE_float(\"init_weight_decay_rate\", 0.01, \"The initial weight decay rate for AdamW optimizer.\")\n\nflags.DEFINE_integer(\"num_warmup_steps\", 500, \"Number of training steps to perform linear learning rate warmup.\")\n\nflags.DEFINE_integer(\"num_train_examples\", None, \"Number of precomputed training steps in 1 epoch.\")\n\nflags.DEFINE_integer(\"predict_batch_size\", 8, \"Batch size for predictions.\")\n\n# ----------------------------------------------------------------------------------------\nflags.DEFINE_integer(\n    \"n_best_size\", 20,\n    \"The total number of n-best predictions to generate in the \"\n    \"nbest_predictions.json output file.\")\n\nflags.DEFINE_integer(\n    \"max_answer_length\", 30,\n    \"The maximum length of an answer that can be generated. This is needed \"\n    \"because the start and end predictions are not conditioned on one another.\")\n\n# ----------------------------------------------------------------------------------------\n## Special flags - do not change\n\nflags.DEFINE_string(\n    \"predict_file\", \"\/kaggle\/input\/tensorflow2-question-answering\/simplified-nq-test.jsonl\",\n    \"NQ json for predictions. E.g., dev-v1.1.jsonl.gz or test-v1.1.jsonl.gz\")\nflags.DEFINE_boolean(\"logtostderr\", True, \"Logs to stderr\")\nflags.DEFINE_boolean(\"undefok\", True, \"it's okay to be undefined\")\nflags.DEFINE_string('f', '', 'kernel')\nflags.DEFINE_string('HistoryManager.hist_file', '', 'kernel')\n\n# Make the default flags as parsed flags\nFLAGS.mark_as_parsed()","0dba51e1":"print(FLAGS.do_train)\nprint(FLAGS.do_predict)","8e297843":"NB_ANSWER_TYPES = 5","b3aeede5":"def get_dataset(tf_record_file, seq_length, batch_size=1, shuffle_buffer_size=0, is_training=False):\n\n    if is_training:\n        features = {\n            \"unique_ids\": tf.io.FixedLenFeature([], tf.int64),\n            \"input_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n            \"input_mask\": tf.io.FixedLenFeature([seq_length], tf.int64),\n            \"segment_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n            \"start_positions\": tf.io.FixedLenFeature([], tf.int64),\n            \"end_positions\": tf.io.FixedLenFeature([], tf.int64),\n            \"answer_types\": tf.io.FixedLenFeature([], tf.int64)\n        }\n    else:\n        features = {\n            \"unique_ids\": tf.io.FixedLenFeature([], tf.int64),\n            \"input_ids\": tf.io.FixedLenFeature([seq_length], tf.int64),\n            \"input_mask\": tf.io.FixedLenFeature([seq_length], tf.int64),\n            \"segment_ids\": tf.io.FixedLenFeature([seq_length], tf.int64)\n        }        \n\n    # Taken from the TensorFlow models repository: https:\/\/github.com\/tensorflow\/models\/blob\/befbe0f9fe02d6bc1efb1c462689d069dae23af1\/official\/nlp\/bert\/input_pipeline.py#L24\n    def decode_record(record, features):\n        \"\"\"Decodes a record to a TensorFlow example.\"\"\"\n        example = tf.io.parse_single_example(record, features)\n\n        # tf.Example only supports tf.int64, but the TPU only supports tf.int32.\n        # So cast all int64 to int32.\n        for name in list(example.keys()):\n            t = example[name]\n            if t.dtype == tf.int64:\n                t = tf.cast(t, tf.int32)\n            example[name] = t\n        return example\n\n    def select_data_from_record(record):\n        \n        x = {\n            'input_ids': record['input_ids'],\n            'input_mask': record['input_mask'],\n            'segment_ids': record['segment_ids']\n        }\n\n        if is_training:\n            y = {\n                'start_positions': record['start_positions'],\n                'end_positions': record['end_positions'],\n                'answer_types': record['answer_types']\n            }\n\n            return (x, y)\n        \n        return x\n\n    dataset = tf.data.TFRecordDataset(tf_record_file)\n    \n    dataset = dataset.map(lambda record: decode_record(record, features))\n    dataset = dataset.map(select_data_from_record)\n    \n    if shuffle_buffer_size > 0:\n        dataset = dataset.shuffle(shuffle_buffer_size)\n    \n    dataset = dataset.batch(batch_size)\n    \n    return dataset","1bca535f":"if FLAGS.num_train_examples is None:\n    FLAGS.num_train_examples = 494670","6f6e28a2":"train_dataset = get_dataset(FLAGS.train_tf_record,\n                    seq_length=FLAGS.max_seq_length_for_training,\n                    batch_size=2,\n                    shuffle_buffer_size=FLAGS.shuffle_buffer_size,\n                    is_training=True\n                )\n\nvalid_tf_record = FLAGS.valid_tf_record\nif FLAGS.smaller_valid_dataset:\n    valid_tf_record = FLAGS.valid_small_tf_record\n    \nvalidation_dataset = get_dataset(valid_tf_record,\n                         seq_length=FLAGS.max_seq_length,\n                         batch_size=2,\n                         is_training=False\n                     )\n\n\n# Can't use next(train_dataset)!\nfeatures, targets = next(iter(train_dataset))\nprint(features)\nprint(targets)\n\nfeatures = next(iter(validation_dataset))\nprint(features)","303a6cd7":"from transformers import TFBertModel\nfrom transformers import TFBertMainLayer, TFBertPreTrainedModel\nfrom transformers.modeling_tf_utils import get_initializer\n\nclass TFBertForNQDemo(TFBertPreTrainedModel):\n    \n    def __init__(self, config, *inputs, **kwargs):\n        \n        super(TFBertForNQDemo, self).__init__(config, *inputs, **kwargs)\n\n        self.bert = TFBertMainLayer(config, name='bert')\n        self.dropout = tf.keras.layers.Dropout(config.hidden_dropout_prob)\n        \n        self.start_pos_classifier = tf.keras.layers.Dense(FLAGS.max_seq_length_for_training,\n                                        kernel_initializer=get_initializer(config.initializer_range),\n                                        name='start_pos_classifier')\n\n        self.end_pos_classifier = tf.keras.layers.Dense(FLAGS.max_seq_length_for_training,\n                                        kernel_initializer=get_initializer(config.initializer_range),\n                                        name='end_pos_classifier')        \n\n        self.answer_type_classifier = tf.keras.layers.Dense(NB_ANSWER_TYPES,\n                                        kernel_initializer=get_initializer(config.initializer_range),\n                                        name='answer_type_classifier')         \n        \n    def call(self, inputs, **kwargs):\n        \n        outputs = self.bert(inputs, **kwargs)\n\n        # Output for [CLS] token from original bert\n        pooled_output = outputs[1]\n\n        pooled_output = self.dropout(pooled_output, training=kwargs.get('training', False))\n        \n        start_pos_logits = self.start_pos_classifier(pooled_output)\n        end_pos_logits = self.end_pos_classifier(pooled_output)\n        answer_type_logits = self.answer_type_classifier(pooled_output)\n\n        outputs = (start_pos_logits, end_pos_logits, answer_type_logits)\n\n        return outputs  # logits","03eccef5":"from transformers import BertTokenizer\n\npretrained_weights = os.path.join(NQ_DIR, \"bert-base-uncased\")\n\nbert_tokenizer = BertTokenizer.from_pretrained(pretrained_weights)\n\nbert_barebone = TFBertModel.from_pretrained(pretrained_weights)\nbert_for_nq = TFBertForNQDemo.from_pretrained(pretrained_weights)\n\ninput_ids = tf.constant(bert_tokenizer.encode(\"My cat is so cute!\"))[None, :]  # Batch size 1\ninput_masks = tf.constant(0, shape=input_ids.shape)\nsegment_ids = tf.constant(0, shape=input_ids.shape)\n\n# Actual inputs to model's `call()` method.\ninputs = (input_ids, input_masks, segment_ids)\n\n# Outputs from `bert_barebone`\noutputs = bert_barebone(inputs)\nlast_hidden_states = outputs[0]\nprint(last_hidden_states.shape)\n\n# Outputs from `bert_for_nq`\noutputs = bert_for_nq(inputs)\n(start_pos_logits, end_pos_logits, answer_type_logits) = outputs\nprint(start_pos_logits.shape)\nprint(end_pos_logits.shape)\nprint(answer_type_logits.shape)\n\nlen(bert_for_nq.trainable_variables)","5eb95e26":"from transformers import BertTokenizer\nfrom transformers import TFBertModel, TFDistilBertModel\nfrom transformers import TFBertMainLayer, TFDistilBertMainLayer, TFBertPreTrainedModel, TFDistilBertPreTrainedModel\nfrom transformers.modeling_tf_utils import get_initializer\n\nclass TFNQModel:\n    \n    def __init__(self, config):\n        \"\"\"\n        \n        Subclasses of this class are different in self.backend,\n        which should be a model that outputs a tensor of shape (batch_size, hidden_dim), and the\n        `backend_call()` method.\n        \n        We will use Hugging Face Bert\/DistilBert as backend in this notebook.\n        \"\"\"\n\n        self.backend = None\n        self.dropout = tf.keras.layers.Dropout(0.1)\n        \n        self.start_pos_classifier = tf.keras.layers.Dense(FLAGS.max_seq_length_for_training,\n                                        kernel_initializer=get_initializer(config.initializer_range),\n                                        name='start_pos_classifier')\n\n        self.end_pos_classifier = tf.keras.layers.Dense(FLAGS.max_seq_length_for_training,\n                                        kernel_initializer=get_initializer(config.initializer_range),\n                                        name='end_pos_classifier')        \n\n        self.answer_type_classifier = tf.keras.layers.Dense(NB_ANSWER_TYPES,\n                                        kernel_initializer=get_initializer(config.initializer_range),\n                                        name='answer_type_classifier')         \n        \n    def backend_call(self, inputs, **kwargs):\n        \"\"\"This method should be implemented by subclasses.\n           \n           The implementation should take into account the (somehow) different input formats of Hugging Face's\n           models.\n           \n           For example, the `TFDistilBert` model, unlike `Bert` model, doesn't have segment_id as input.\n           \n           Then it calls `self.backend_call()` to get the outputs from Bert's model, which is used in self.call().\n        \"\"\"\n        \n        raise NotImplementedError\n\n    \n    def call(self, inputs, **kwargs):\n        \n        pooled_output = self.backend_call(inputs, **kwargs)\n        pooled_output = self.dropout(pooled_output, training=kwargs.get('training', False))\n        \n        start_pos_logits = self.start_pos_classifier(pooled_output)\n        end_pos_logits = self.end_pos_classifier(pooled_output)\n        answer_type_logits = self.answer_type_classifier(pooled_output)\n\n        outputs = (start_pos_logits, end_pos_logits, answer_type_logits)\n\n        return outputs  # logits\n    \n    \nclass TFBertForNQ(TFNQModel, TFBertPreTrainedModel):\n    \n    def __init__(self, config, *inputs, **kwargs):\n        \n        TFBertPreTrainedModel.__init__(self, config, *inputs, **kwargs)  # explicit calls without super\n        TFNQModel.__init__(self, config)\n\n        self.bert = TFBertMainLayer(config, name='bert')\n        \n    def backend_call(self, inputs, **kwargs):\n        \n        outputs = self.bert(inputs, **kwargs)\n        pooled_output = outputs[1]\n        \n        return pooled_output\n        \nclass TFDistilBertForNQ(TFNQModel, TFDistilBertPreTrainedModel):\n    \n    def __init__(self, config, *inputs, **kwargs):\n        \n        TFDistilBertPreTrainedModel.__init__(self, config, *inputs, **kwargs)  # explicit calls without super\n        TFNQModel.__init__(self, config)\n\n        self.backend = TFDistilBertMainLayer(config, name=\"distilbert\")\n        \n    def backend_call(self, inputs, **kwargs):\n        \n        if isinstance(inputs, tuple):\n            # Distil bert has no segment_id (i.e. `token_type_ids`)\n            inputs = inputs[:2]\n        else:\n            inputs = inputs\n        \n        outputs = self.backend(inputs, **kwargs)\n        \n        # TFDistilBertModel's output[0] is of shape (batch_size, sequence_length, hidden_size)\n        # We take only for the [CLS].\n        pooled_output = outputs[0][:, 0, :]\n        \n        return pooled_output\n    \n    \nmodel_mapping = {\n    \"bert\": TFBertForNQ,\n    \"distilbert\": TFDistilBertForNQ\n}\n\n\ndef get_pretrained_model(model_name):\n    \n    pretrained_path = os.path.join(FLAGS.model_dir, model_name)\n    \n    tokenizer = BertTokenizer.from_pretrained(pretrained_path)\n    \n    model_type = model_name.split(\"-\")[0]\n    if model_type not in model_mapping:\n        raise ValueError(\"Model definition not found.\")\n    \n    model_class = model_mapping[model_type]\n    model = model_class.from_pretrained(pretrained_path)\n    \n    return tokenizer, model","bad010f9":"bert_tokenizer, bert_for_nq = get_pretrained_model('bert-base-uncased')\n_, distil_bert_for_nq = get_pretrained_model('distilbert-base-uncased-distilled-squad')\n\ninput_ids = tf.constant(bert_tokenizer.encode(\"Hello, my dog is cute\"))[None, :]  # Batch size 1\ninput_masks = tf.constant(0, shape=input_ids.shape)\nsegment_ids = tf.constant(0, shape=input_ids.shape)\n\n# Actual inputs to model\ninputs = (input_ids, input_masks, segment_ids)\n\n# Outputs from bert_for_nq using backend_call()\noutputs = bert_for_nq(inputs)\n(start_pos_logits, end_pos_logits, answer_type_logits) = outputs\nprint(start_pos_logits.shape)\nprint(end_pos_logits.shape)\nprint(answer_type_logits.shape)\n\nlen(bert_for_nq.trainable_variables)\n\n# Outputs from distil_bert_for_nq using backend_call()\noutputs = distil_bert_for_nq(inputs)\n(start_pos_logits, end_pos_logits, answer_type_logits) = outputs\nprint(start_pos_logits.shape)\nprint(end_pos_logits.shape)\nprint(answer_type_logits.shape)\n\nlen(distil_bert_for_nq.trainable_variables)","d8faa65c":"bert_tokenizer, bert_nq = get_pretrained_model(FLAGS.model_name)\n\nif not IS_KAGGLE:\n    bert_nq.trainable_variables","de45494d":"def get_metrics(name):\n\n    loss = tf.keras.metrics.Mean(name=f'{name}_loss')\n    loss_start_pos = tf.keras.metrics.Mean(name=f'{name}_loss_start_pos')\n    loss_end_pos = tf.keras.metrics.Mean(name=f'{name}_loss_end_pos')\n    loss_ans_type = tf.keras.metrics.Mean(name=f'{name}_loss_ans_type')\n    \n    acc = tf.keras.metrics.SparseCategoricalAccuracy(name=f'{name}_acc')\n    acc_start_pos = tf.keras.metrics.SparseCategoricalAccuracy(name=f'{name}_acc_start_pos')\n    acc_end_pos = tf.keras.metrics.SparseCategoricalAccuracy(name=f'{name}_acc_end_pos')\n    acc_ans_type = tf.keras.metrics.SparseCategoricalAccuracy(name=f'{name}_acc_ans_type')\n    \n    return loss, loss_start_pos, loss_end_pos, loss_ans_type, acc, acc_start_pos, acc_end_pos, acc_ans_type\n\ntrain_loss, train_loss_start_pos, train_loss_end_pos, train_loss_ans_type, train_acc, train_acc_start_pos, train_acc_end_pos, train_acc_ans_type = get_metrics(\"train\")\nvalid_loss, valid_loss_start_pos, valid_loss_end_pos, valid_loss_ans_type, valid_acc, valid_acc_start_pos, valid_acc_end_pos, valid_acc_ans_type = get_metrics(\"valid\")","947a4e8a":"loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n\ndef loss_function(nq_labels, nq_logits):\n    \n    (start_pos_labels, end_pos_labels, answer_type_labels) = nq_labels\n    (start_pos_logits, end_pos_logits, answer_type_logits) = nq_logits\n    \n    loss_start_pos = loss_object(start_pos_labels, start_pos_logits)\n    loss_end_pos = loss_object(end_pos_labels, end_pos_logits)\n    loss_ans_type = loss_object(answer_type_labels, answer_type_logits)\n    \n    loss_start_pos = tf.math.reduce_sum(loss_start_pos)\n    loss_end_pos = tf.math.reduce_sum(loss_end_pos)\n    loss_ans_type = tf.math.reduce_sum(loss_ans_type)\n    \n    loss = (loss_start_pos + loss_end_pos + loss_ans_type) \/ 3.0\n    \n    return loss, loss_start_pos, loss_end_pos, loss_ans_type","9665dab7":"class CustomSchedule(tf.keras.optimizers.schedules.PolynomialDecay):\n    \n    def __init__(self,\n      initial_learning_rate,\n      decay_steps,\n      end_learning_rate=0.0001,\n      power=1.0,\n      cycle=False,\n      name=None,\n      num_warmup_steps=1000):\n        \n        # Since we have a custom __call__() method, we pass cycle=False when calling `super().__init__()` and\n        # in self.__call__(), we simply do `step = step % self.decay_steps` to have cyclic behavior.\n        super(CustomSchedule, self).__init__(initial_learning_rate, decay_steps, end_learning_rate, power, cycle=False, name=name)\n        \n        self.num_warmup_steps = num_warmup_steps\n        \n        self.cycle = tf.constant(self.cycle, dtype=tf.bool)\n        \n    def __call__(self, step):\n        \"\"\" `step` is actually the step index, starting at 0.\n        \"\"\"\n        \n        # For cyclic behavior\n        step = tf.cond(self.cycle and step >= self.decay_steps, lambda: step % self.decay_steps, lambda: step)\n        \n        learning_rate = super(CustomSchedule, self).__call__(step)\n\n        # Copy (including the comments) from original bert optimizer with minor change.\n        # Ref: https:\/\/github.com\/google-research\/bert\/blob\/master\/optimization.py#L25\n        \n        # Implements linear warmup: if global_step < num_warmup_steps, the\n        # learning rate will be `global_step \/ num_warmup_steps * init_lr`.\n        if self.num_warmup_steps > 0:\n            \n            steps_int = tf.cast(step, tf.int32)\n            warmup_steps_int = tf.constant(self.num_warmup_steps, dtype=tf.int32)\n\n            steps_float = tf.cast(steps_int, tf.float32)\n            warmup_steps_float = tf.cast(warmup_steps_int, tf.float32)\n\n            # The first training step has index (`step`) 0.\n            # The original code use `steps_float \/ warmup_steps_float`, which gives `warmup_percent_done` being 0,\n            # and causing `learning_rate` = 0, which is undesired.\n            # For this reason, we use `(steps_float + 1) \/ warmup_steps_float`.\n            # At `step = warmup_steps_float - 1`, i.e , at the `warmup_steps_float`-th step, \n            #`learning_rate` is `self.initial_learning_rate`.\n            warmup_percent_done = (steps_float + 1) \/ warmup_steps_float\n            \n            warmup_learning_rate = self.initial_learning_rate * warmup_percent_done\n\n            is_warmup = tf.cast(steps_int < warmup_steps_int, tf.float32)\n            learning_rate = ((1.0 - is_warmup) * learning_rate + is_warmup * warmup_learning_rate)\n                        \n        return learning_rate\n    \nnum_train_steps = int(FLAGS.epochs * FLAGS.num_train_examples \/ FLAGS.train_batch_size \/ FLAGS.batch_accumulation_size)\n\nlearning_rate = CustomSchedule(\n    initial_learning_rate=FLAGS.init_learning_rate,\n    decay_steps=num_train_steps,\n    end_learning_rate=0.0,\n    power=1.0,\n    cycle=FLAGS.cyclic_learning_rate,    \n    num_warmup_steps=FLAGS.num_warmup_steps\n)\n\n# Normally we want to use this way to plot since it's much fast.\n# However, we get the error`The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()`.\n# This is due to some bool operations on `step`. So we use the second way (which is slow) to plot.\n# (This error also occurs even we use `tf.keras.optimizers.schedules.PolynomialDecay` with `cycle=True`.)\n\n# Get error here.\n# plt.plot(learning_rate(tf.range(num_train_steps)))\n\n# This is a workaround to plot, but it's very slow. So we plot the learning rate only when `num_train_steps` is small.\nif num_train_steps <= 5000:\n    xs = tf.range(2 * num_train_steps)\n    ys = [learning_rate(x) for x in xs]\n    plt.plot(xs, ys)\n    \n    plt.ylabel(\"Learning Rate\")\n    plt.xlabel(\"Train Step\")\n    plt.show()\n\nprint(f'num_train_steps: {num_train_steps}')","49e64738":"decay_var_list = []\n\nfor i in range(len(bert_nq.trainable_variables)):\n    name = bert_nq.trainable_variables[i].name\n    if any(x in name for x in [\"LayerNorm\", \"layer_norm\", \"bias\"]):\n        decay_var_list.append(name)\n        \ndecay_var_list","9dc0df4e":"from tensorflow.keras.optimizers import Adam\nfrom adamw_optimizer import AdamW\n\n# The hyperparameters are copied from AdamWeightDecayOptimizer in original bert code.\n# (https:\/\/github.com\/google-research\/bert\/blob\/master\/optimization.py#L25)\noptimizer = AdamW(weight_decay=FLAGS.init_weight_decay_rate, learning_rate=learning_rate, beta_1=0.9, beta_2=0.999, epsilon=1e-6, decay_var_list=decay_var_list)","43f94283":"checkpoint_path = os.path.join(FLAGS.input_checkpoint_dir, FLAGS.model_name)\nckpt = tf.train.Checkpoint(model=bert_nq, optimizer=optimizer)\nckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=5)\n\n# if a checkpoint exists, restore the latest checkpoint.\nif ckpt_manager.latest_checkpoint:\n    ckpt.restore(ckpt_manager.latest_checkpoint)\n    last_epoch = int(ckpt_manager.latest_checkpoint.split(\"-\")[-1])\n    print (f'Latest BertNQ checkpoint restored -- Model trained for {last_epoch} epochs')\nelse:\n    print('Checkpoint not found. Train BertNQ from scratch')\n    last_epoch = 0\n    \n    \n# Reset saving path, because the FLAGS.input_checkpoint_dir is not writable on Kaggle\nprint(ckpt_manager._directory)\nckpt_manager._directory = os.path.join(FLAGS.output_checkpoint_dir, FLAGS.model_name)\nckpt_manager._checkpoint_prefix = os.path.join(ckpt_manager._directory, \"ckpt\")\nprint(ckpt_manager._directory)\n\nfrom tensorflow.python.lib.io.file_io import recursive_create_dir\nrecursive_create_dir(ckpt_manager._directory)","926d0e2b":"# The @tf.function trace-compiles train_step into a TF graph for faster\n# execution. The function specializes to the precise shape of the argument\n# tensors. To avoid re-tracing due to the variable sequence lengths or variable\n# batch sizes (the last batch is smaller), use input_signature to specify\n# more generic shapes.\n\ninput_signature = [\n        tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n        tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n        tf.TensorSpec(shape=(None, None), dtype=tf.int32),\n        tf.TensorSpec(shape=(None,), dtype=tf.int32),\n        tf.TensorSpec(shape=(None,), dtype=tf.int32),\n        tf.TensorSpec(shape=(None,), dtype=tf.int32)\n]","343f7e02":"def get_loss_and_gradients(input_ids, input_masks, segment_ids, start_pos_labels, end_pos_labels, answer_type_labels):\n    \n    nq_inputs = (input_ids, input_masks, segment_ids)\n    nq_labels = (start_pos_labels, end_pos_labels, answer_type_labels)\n\n    with tf.GradientTape() as tape:\n\n        nq_logits = bert_nq(nq_inputs, training=True)\n        loss, loss_start_pos, loss_end_pos, loss_ans_type = loss_function(nq_labels, nq_logits)\n                \n    gradients = tape.gradient(loss, bert_nq.trainable_variables)        \n        \n    (start_pos_logits, end_pos_logits, answer_type_logits) = nq_logits\n        \n    train_acc.update_state(start_pos_labels, start_pos_logits)\n    train_acc.update_state(end_pos_labels, end_pos_logits)\n    train_acc.update_state(answer_type_labels, answer_type_logits)\n\n    train_acc_start_pos.update_state(start_pos_labels, start_pos_logits)\n    train_acc_end_pos.update_state(end_pos_labels, end_pos_logits)\n    train_acc_ans_type.update_state(answer_type_labels, answer_type_logits)\n    \n    return loss, gradients, loss_start_pos, loss_end_pos, loss_ans_type\n\n\n@tf.function(input_signature=input_signature)\ndef train_step_simple(input_ids, input_masks, segment_ids, start_pos_labels, end_pos_labels, answer_type_labels):\n\n    nb_examples = tf.math.reduce_sum(tf.cast(tf.math.not_equal(start_pos_labels, -2), tf.int32))\n    \n    loss, gradients, loss_start_pos, loss_end_pos, loss_ans_type = get_loss_and_gradients(input_ids, input_masks, segment_ids, start_pos_labels, end_pos_labels, answer_type_labels)\n    \n    average_loss = tf.math.divide(loss, tf.cast(nb_examples, tf.float32))\n    \n    # For this simple training step, it's better to use tf.math.reduce_mean() in loss_function() instead of tf.math.reduce_sum(), and not using the following line\n    # to average gradients manually.\n    \n    # Using this line causing `UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape.`.\n    average_gradients = [tf.divide(x, tf.cast(nb_examples, tf.float32)) for x in gradients]\n    \n    optimizer.apply_gradients(zip(gradients, bert_nq.trainable_variables))\n\n    average_loss_start_pos = tf.math.divide(loss_start_pos, tf.cast(nb_examples, tf.float32))\n    average_loss_end_pos = tf.math.divide(loss_end_pos, tf.cast(nb_examples, tf.float32))\n    average_loss_ans_type = tf.math.divide(loss_ans_type, tf.cast(nb_examples, tf.float32))\n    \n    train_loss(average_loss)\n    train_loss_start_pos(average_loss_start_pos)\n    train_loss_end_pos(average_loss_end_pos)\n    train_loss_ans_type(average_loss_ans_type)","d7087675":"@tf.function(input_signature=input_signature)\ndef train_step_with_batch_accumulation(input_ids, input_masks, segment_ids, start_pos_labels, end_pos_labels, answer_type_labels):\n\n    # This gets None! (probably due to input_signature)\n    # batch_size = input_ids.shape[0]\n    \n    # Try this.\n    nb_examples = tf.math.reduce_sum(tf.cast(tf.math.not_equal(start_pos_labels, -2), tf.int32))\n\n    total_loss = 0.0\n    total_loss_start_pos = 0.0\n    total_loss_end_pos = 0.0\n    total_loss_ans_type = 0.0\n    \n    total_gradients = [tf.constant(0, shape=x.shape, dtype=tf.float32) for x in bert_nq.trainable_variables]        \n    ### total_gradients_sparse = [tf.IndexedSlices(values=tf.constant(0.0, shape=[1] + x.shape.as_list()[1:]), indices=tf.constant([0], dtype=tf.int32), dense_shape=x.shape.as_list()) for x in bert_nq.trainable_variables]        \n\n    for idx in tf.range(FLAGS.batch_accumulation_size):    \n                \n        start_idx = FLAGS.train_batch_size * idx\n        end_idx = FLAGS.train_batch_size * (idx + 1)\n        \n        if start_idx >= nb_examples:\n            break\n\n        (input_ids_mini, input_masks_mini, segment_ids_mini) = (input_ids[start_idx:end_idx], input_masks[start_idx:end_idx], segment_ids[start_idx:end_idx])\n        (start_pos_labels_mini, end_pos_labels_mini, answer_type_labels_mini) = (start_pos_labels[start_idx:end_idx], end_pos_labels[start_idx:end_idx], answer_type_labels[start_idx:end_idx])\n        \n        loss, gradients, loss_start_pos, loss_end_pos, loss_ans_type = get_loss_and_gradients(input_ids_mini, input_masks_mini, segment_ids_mini, start_pos_labels_mini, end_pos_labels_mini, answer_type_labels_mini)\n        \n        total_loss += loss\n        total_loss_start_pos += loss_start_pos\n        total_loss_end_pos += loss_end_pos\n        total_loss_ans_type += loss_ans_type\n        \n        total_gradients = [x + y for x, y in zip(total_gradients, gradients)]        \n        ### total_gradients_sparse = [_add_grads_for_var(x, y) for x, y in zip(total_gradients_sparse, gradients)]\n\n    average_loss = tf.math.divide(total_loss, tf.cast(nb_examples, tf.float32))        \n    average_gradients = [tf.divide(x, tf.cast(nb_examples, tf.float32)) for x in total_gradients]\n    ### average_gradients_sparse = [tf.scalar_mul(tf.divide(1.0, tf.cast(nb_examples, tf.float32)), x) for x in total_gradients_sparse]\n    \n    optimizer.apply_gradients(zip(average_gradients, bert_nq.trainable_variables))\n    ### optimizer.apply_gradients(zip(average_gradients_sparse, bert_nq.trainable_variables))\n\n    average_loss_start_pos = tf.math.divide(total_loss_start_pos, tf.cast(nb_examples, tf.float32))\n    average_loss_end_pos = tf.math.divide(total_loss_end_pos, tf.cast(nb_examples, tf.float32))\n    average_loss_ans_type = tf.math.divide(total_loss_ans_type, tf.cast(nb_examples, tf.float32))    \n    \n    train_loss(average_loss)\n    train_loss_start_pos(average_loss_start_pos)\n    train_loss_end_pos(average_loss_end_pos)\n    train_loss_ans_type(average_loss_ans_type)","17c17b7a":"train_step = train_step_simple\nif FLAGS.batch_accumulation_size > 1:\n    train_step = train_step_with_batch_accumulation\n\ntrain_dataset = get_dataset(\n    FLAGS.train_tf_record,\n    FLAGS.max_seq_length_for_training,\n    FLAGS.batch_accumulation_size * FLAGS.train_batch_size,\n    FLAGS.shuffle_buffer_size,\n    is_training=True\n)\n\n# Because the previous checkpoint manager (for loading model) is created with a path which is read-only, ckpt_manager.save() fails here.\n# So we create another checkpoint manager for saving checkpoints.\n# checkpoint_path = os.path.join(FLAGS.output_checkpoint_dir, FLAGS.model_name)\n# ckpt = tf.train.Checkpoint(model=bert_nq, optimizer=optimizer)\n# ckpt_manager = tf.train.CheckpointManager(ckpt, checkpoint_path, max_to_keep=3)\n\ntrain_start_time = datetime.datetime.now()\n\nfor epoch in range(FLAGS.epochs):\n\n    train_loss.reset_states()\n    train_loss_start_pos.reset_states()\n    train_loss_end_pos.reset_states()\n    train_loss_ans_type.reset_states()    \n    \n    train_acc.reset_states()\n    train_acc_start_pos.reset_states()\n    train_acc_end_pos.reset_states()\n    train_acc_ans_type.reset_states()\n    \n    epoch_start_time = datetime.datetime.now()\n    \n    for (batch_idx, (features, targets)) in enumerate(train_dataset):\n\n        # If the training is on kaggle, we stop the training after 8h45m, so we can get the checkpoints!\n        if IS_KAGGLE and (datetime.datetime.now() - train_start_time).total_seconds() > 31500:\n            break\n        \n        (input_ids, input_masks, segment_ids) = (features['input_ids'], features['input_mask'], features['segment_ids'])\n        (start_pos_labels, end_pos_labels, answer_type_labels) = (targets['start_positions'], targets['end_positions'], targets['answer_types'])\n    \n        batch_start_time = datetime.datetime.now()\n        \n        train_step(input_ids, input_masks, segment_ids, start_pos_labels, end_pos_labels, answer_type_labels)\n\n        batch_end_time = datetime.datetime.now()\n        batch_elapsed_time = (batch_end_time - batch_start_time).total_seconds()\n        \n        if (batch_idx + 1) % 100 == 0:\n            print('Epoch {} | Batch {} | Elapsed Time {}'.format(\n                epoch + 1,\n                batch_idx + 1,\n                batch_elapsed_time\n            ))\n            print('Loss {:.6f} | Loss_S {:.6f} | Loss_E {:.6f} | Loss_T {:.6f}'.format(\n                train_loss.result(),\n                train_loss_start_pos.result(),\n                train_loss_end_pos.result(),\n                train_loss_ans_type.result()\n            ))\n            print(' Acc {:.6f} |  Acc_S {:.6f} |  Acc_E {:.6f} |  Acc_T {:.6f}'.format(\n                train_acc.result(),\n                train_acc_start_pos.result(),\n                train_acc_end_pos.result(),\n                train_acc_ans_type.result()\n            ))\n            print(\"-\" * 100)\n       \n    epoch_end_time = datetime.datetime.now()\n    epoch_elapsed_time = (epoch_end_time - epoch_start_time).total_seconds()\n            \n    if (epoch + 1) % 1 == 0:\n        \n        ckpt_save_path = ckpt_manager.save()\n        print ('\\nSaving checkpoint for epoch {} at {}'.format(epoch+1, ckpt_save_path))\n        \n        print('\\nEpoch {}'.format(epoch + 1))\n        print('Loss {:.6f} | Loss_S {:.6f} | Loss_E {:.6f} | Loss_T {:.6f}'.format(\n            train_loss.result(),\n            train_loss_start_pos.result(),\n            train_loss_end_pos.result(),\n            train_loss_ans_type.result()\n        ))\n        print(' Acc {:.6f} |  Acc_S {:.6f} |  Acc_E {:.6f} |  Acc_T {:.6f}'.format(\n            train_acc.result(),\n            train_acc_start_pos.result(),\n            train_acc_end_pos.result(),\n            train_acc_ans_type.result()\n        ))\n\n    print('\\nTime taken for 1 epoch: {} secs\\n'.format(epoch_elapsed_time))\n    print(\"-\" * 80 + \"\\n\")","471433f7":"print(f\"{ckpt_manager._directory}\")\nos.system(f\"ls -l {ckpt_manager._directory} > results.txt\")\nwith open(\"results.txt\", \"r\", encoding=\"UTF-8\") as fp:\n    for line in fp:\n        print(line.strip())","554798b7":"with open(f\"{ckpt_manager._directory}\/checkpoint\", \"r\", encoding=\"UTF-8\") as fp:\n    for line in fp:\n        print(line.strip())","43937ea7":"## Training\n\nFinally, we can sit here and look the progress!","672f2135":"## AdamW Optimizer (Weight decay)\n\nThe `Adam` optimizer from [tf.kears.optimizers](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/python\/keras\/optimizer_v2\/adam.py#L32) doesn't support weight decay. But the original Bert's [optimizer](https:\/\/github.com\/google-research\/bert\/blob\/master\/optimization.py#L59) uses weight decay.\n\nThere is a `AdamW` class from Tensorflow addons, [tfa.optimizers.AdamW](https:\/\/github.com\/tensorflow\/addons\/blob\/master\/tensorflow_addons\/optimizers\/weight_decay_optimizers.py#L340). However, it doesn't support [update_with_lr ](https:\/\/github.com\/google-research\/bert\/blob\/master\/optimization.py#L149) as in the original Bert.\n\nThere is a [note](https:\/\/github.com\/tensorflow\/addons\/blob\/master\/tensorflow_addons\/optimizers\/weight_decay_optimizers.py#L362) about how to integrate learing rate decay in `tfa.optimizers.AdamW`:\n\n> when applying a decay to the learning rate, be sure to manually apply the decay to the `weight_decay` as well ...\n\nbut I didn't make it work with schedules.\n\nTherefore, we use a modified version. The difference is: Unlike in [DecoupledWeightDecayExtension._decay_weights_op()](https:\/\/github.com\/tensorflow\/addons\/blob\/master\/tensorflow_addons\/optimizers\/weight_decay_optimizers.py#L154), we compute `weight_decay` as `init_weight_decay * learning_rate`.\n\nThis modified version is included in `adamw_optimizer.py` in [nq-competition](https:\/\/www.kaggle.com\/yihdarshieh\/nq-competition) dataset.\n\n#### Note\n\nFor performance, we shouldn't re-compute `learning_rate` in modified `DecoupledWeightDecayExtension._decay_weights_op()`.\nInstead, we should prepare states to use multiple times as in [apply_state](https:\/\/github.com\/tensorflow\/tensorflow\/blob\/master\/tensorflow\/python\/keras\/optimizer_v2\/optimizer_v2.py#L440) in `tf.keras.optimizers.Optimizer`.\nHowever, it inovles modification of tensorflow code, which I tried to avoid.","24482947":"## An interface  for NQ models","4be77fc9":"## Choose the model to use\n\nGet a new model, determined by `FLAGS.model_name` (for example, `bert-base-uncased`), and print its trainable variables.","080dd5ca":"## Loss function\n\nThe loss is not averaged over a batch of training examples. The caller of `loss_function` have to take care of this. The reason of not using average here is that I need the sum of losses to do `batch accumulation`.","e891cdee":"## Abseil Flags - Hyperparameters and file paths","3d77f936":"## Hugging Face pretrained Bert model names\n\n* Model names taken from: https:\/\/github.com\/huggingface\/transformers\/blob\/master\/transformers\/\n* To use the models, we have to get configuration, vocab, and model files. These files have to be renamed to `config.json`, `vocab.txt` and `tf_model.h5`.\n* Under the above transformers web page, you can find links to\n    * Models: see links in modeling_bert.py \/ modeling_distilbert.py\n    * Configurations, see configuration_bert.py \/ configuration_distilbert.py\n    * Vocabs, see tokenization_bert.py \/ tokenization_distilbert.py\n\n#### Important\n\nWe are using a particular dataset `nq-train.tfrecords-00000-of-00001`, which is created with a customized bert vocabulary `vocab-nq` and it is uncased. If you want to use cased bert model, you have to regenerate the training dataset.","76291b34":"## tf.function and input_signature","cf14a44d":"## Check a small batch in training \/ validation dataset","d3da9e6b":"## Get variables to apply weight decay in AdamW optimizer\n\nIn the original Bert's optimizer [AdamWeightDecayOptimizer](https:\/\/github.com\/google-research\/bert\/blob\/master\/optimization.py#L87), it accepts an arguement `exclude_from_weight_decay`, and the list `[\"LayerNorm\", \"layer_norm\", \"bias\"]` is passed to it, see [here](https:\/\/github.com\/google-research\/bert\/blob\/master\/optimization.py#L59).\n\nHere we use [tfa.optimizers.AdamW](https:\/\/github.com\/tensorflow\/addons\/blob\/master\/tensorflow_addons\/optimizers\/weight_decay_optimizers.py#L340), which accepts an arguement `decay_var_list`.","0c083aac":"## Get Datasets from TF Record files\n\nA method to get datasets from tf_record_file. See next section for the usage.[](http:\/\/)","37ca7d4f":"> ## Metrics","eba32da8":"## Try to load the latest checkpoint","947b8681":"## Custom Learning Schedule\n\nA learning rate sheduler which is close to the original Bert's one: https:\/\/github.com\/google-research\/bert\/blob\/master\/optimization.py#L32.\n\nIt supports the following\n* warmup stesp (used in the original Bert's optimizer)\n* cyclic learning rate","5f845884":"## Try Bert \/ DistillBert models for NQ","62fbbe87":"## Simple Training Step\n\nNo batch accumulation here.","5732a725":"## Check saved checkpoints","a5eb59fc":"# Use Hugging Face's Tensorflow 2 transformer models for NQ\n\n![25720743.png](attachment:25720743.png)","b8c0628e":"## Demo of using Hugging Face Tensorflow 2.0 models","ab9f8023":"## Try the demo model","f0a6f670":"## Training Step - Batch Accumulation\n\nMy effort to use batch accumulation.\n\nBecause of the line `[x + y for x, y in zip(total_gradients_over_batch, gradients)]`, I got \n>`UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape.`\n\nI tried avoid this warning by making sparse gradient operations as possible as I can. These are the 4 lines starting with `### `, but it doesn't work and gives errors, so they are not used.\n\n## *** Question ***\n\nI would like to see a better implementation of batch accumulation! I think the code here uses more memory and maybe also run slowly.","82c72739":"## Introduction\nThis kernel demonstrates how to use Hugging Face's [transformers](https:\/\/github.com\/huggingface\/transformers) package, more precisely, theier `Tensorflow 2` models, for this competition.\n\n## Motivations\n\n* Give my first contribution on Kaggle.\n* To learn and work on some things new.\n* Provide an easy to read\/use training kernel for this competition.\n* (The most important) I want to have your feedbacks, and solutions to some of my questions, so we can improve this kernel!\n\n## Features\nI tried my efforts to make the following things work:\n\n* batch accumulation.\n* Adam optimizer with weight decay and make it work with learning rate decay. (I take [AdamW](https:\/\/github.com\/tensorflow\/addons\/blob\/master\/tensorflow_addons\/optimizers\/weight_decay_optimizers.py#L340) class from `Tensorflow Addons` with some modification. See `adamw_optimizer.py`)\n* Custom learning rate schedule as close as to the original google Bert code.\n* Easy to choose different model configurations.\n\n## Important Notes\n\n* I didn't use the pre-trained model from starter kernel.\n* I didn't check (yet) the LB scores for the checkpoints I provided.\n* No code for inference\/submission in this kernel. (I will work on a separate kernel.)\n* The flag `model_name` is default to `distilbert-base-uncased-distilled-squad`. The `batch_size` and `batch_accumulation_size` is however defaut to `10`. It's your own responsibility to make sure there is no OOM when you change these parameters.\n* In the training loop, if the training is on `Kaggle`, then it stops after `8 hours 45 minutes` so we can still have a saved checkpoint.\n* This kerenl is not optimalized for training. (See next section.)\n* The checkpoints provided in [nq-competition](https:\/\/www.kaggle.com\/yihdarshieh\/nq-competition) (as the value of `NQ_DIR` below) are trained by myself without any effort on choosing the hyperparameters.\n* After you have your new trained checkpoints (or if you create your own .tfrecord file), when you want to use them for resuming training, you have to set your own value for `MY_OWN_NQ_DIR`, which should contain your own checkpoints\/.tfrecord file.\n* This kernel is not written for using TPU.\n\n## Checkpoints, Batch size and Training time\n\nIn the following table, the model configurations are:\n\n* `distilled bert`: distilbert-base-uncased-distilled-squad\n* `bert base`: bert-base-uncased\n* `bert large`: bert-large-uncased-whole-word-masking-finetuned-squad\n\nHere is the summary of the checkpoints I provided (no `bert large`, it will eat up my GUP quota. Probably I will provide one after training on GCP).\n\n| model          | batch size | accumulation size | effective batch size | num. updates | training time | epochs | time \/ epoch |\n|----------------|------------|-------------------|----------------------|--------------|---------------|--------|--------------|\n| distilled bert | 25         | 4                 | 100                  | 9893         | 30823 s       | 2      | 4.28 h       | \n| bert base      | 10         | 10                | 100                  | 4500         | 32226 s       | 0.909  | 9.84 h       |\n| bert large     | 2          | 50                | 100                  | n\/a          | n\/a           | n\/a    | 37.78 h      | \n|                |            |                   |                      |              |               |        |              |\n\n#### Remark:\nFor `distilled bert`, I didn't test with `train_batch_size=10` and `batch_accumulation_size=10` for `epochs=2`. It's possible that this will take more time than the time indicated above.\n\n## A screenshot of training progress\n\nIn training loop, I print the losses and accuracies every `100` effective batch, here `1 effective batch = batch_size * accumulation_size`. It includes those for `start_pos (loss_S, acc_S)`, `end_pos (loss_E, acc_E)` and `answer_type (loss_A, acc_A)`. The following screenshot is however taken from a training where the result is printed every effective batch.\n\n![training.PNG](attachment:training.PNG)\n\n#### Remark:\n\nIn the trainin progress, `batch n` means `effective batch n`.\n\n## TODO - Some contribution wanted!\n\n* This kernel is only for training. I will add the prediction\/submission parts as soon as possible (In a separate kernel.)\n* The batch accumulation code seems work fine, but not memory\/speed efficient. In particular, I can't get rid of the following warning.\n> UserWarning: Converting sparse IndexedSlices to a dense Tensor of unknown shape. This may consume a large amount of memory.\n* Maybe TPU support, not very sure. However, see the 3rd link in the `Reference` section.\n\n## Things tried but not working - Solution wanted!\n\nI tried to use mixed precision for training, but I couldn't make it work.\n\n* Using [tf.keras.mixed_precision.experimental.Policy](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/keras\/mixed_precision\/experimental\/Policy) for mixed precision training causes error when loading pretrained models. (` input #1(zero-based) was expected to be a half tensor but is a float tensor [Op:AddV2] name: tf_bert_model_1\/bert\/embeddings\/add\/`.)\n* Using [tf.config.optimizer.set_experimental_options](https:\/\/www.tensorflow.org\/api_docs\/python\/tf\/config\/optimizer\/set_experimental_options) doesn't make tranining faster. Probably this is designed only for compiled model??\n\n## Disclamation\n* I am not a part of Hugging Face. I choose to use `transformers` package because I found it's easier to use and to extend, so I can focus on other parts of this notebook.\n* I take no responsibility for any (potential) error in this kernel and in the dataset `nq-competition`. (I would appreciate any feedback.)\n* I take no credit of any file (with\/without my own modifications) containing in `nq-compeittion`.\n\n## References\n\n* [Transformer model for language understanding (colab tutorial)](https:\/\/colab.research.google.com\/github\/tensorflow\/docs\/blob\/r2.0rc\/site\/en\/r2\/tutorials\/text\/transformer.ipynb#scrollTo=KiVIjF8EJzAx) - Official Tensorflow tutorial on transformer model\n* [\u6dfa\u8ac7\u795e\u7d93\u6a5f\u5668\u7ffb\u8b6f & \u7528 Transformer \u8207 TensorFlow 2 \u82f1\u7ffb\u4e2d](https:\/\/leemeng.tw\/neural-machine-translation-with-transformer-and-tensorflow2.html) - An annotated chinese tutorial based on the above one\n* [Sequence classification with Transformers & Strategy](https:\/\/colab.research.google.com\/drive\/1yWaLpCWImXZE2fPV0ZYDdWWI8f52__9A#scrollTo=mnhwpzb73KIL) - Hugging Face's tutorial on how to use their TF 2.0 models on TPU\n* [Hugging Face: State-of-the-Art Natural Language Processing in ten lines of TensorFlow 2.0 ](https:\/\/blog.tensorflow.org\/2019\/11\/hugging-face-state-of-art-natural.html) - Hugging Face's post on Tensorflow blog","64c0627f":"## Set num_train_examples if FLAGS.num_train_examples is None","b202bf6e":"## Abseil Flags - Datasets\n\n`Tensorflow 2` no longer supports `flags`, and it is in favor of the now open-source `absl-py`.\n\nI keep using (Abseil) flags in this kernel for making definitions clear, but my way of using it is not for command-line use. At some point later, `FLAGS.mark_as_parsed()` is called.\n\nThis section contains flags about the characteristics of the datasets."}}