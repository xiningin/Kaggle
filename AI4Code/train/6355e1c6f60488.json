{"cell_type":{"c0178145":"code","11d127fc":"code","a7988caa":"code","9c58793f":"code","f94582f2":"code","2574f643":"code","b5cd87a0":"code","2555bb2c":"code","14760b23":"code","2a8dea42":"code","e0d0826e":"code","f31dde12":"code","5f803d6c":"code","bd73cf99":"code","6bf2267d":"code","03251a89":"code","8446df4b":"code","081daecc":"code","256d2237":"code","8af4934b":"code","b1b37613":"code","fed0a4c2":"code","012e47e2":"code","d560d272":"code","8ffbac71":"code","726c564d":"code","94aa2359":"code","d34f956a":"code","a8eac7ad":"code","3e957193":"code","2df3973f":"code","ed923888":"code","8aa0ccd7":"code","62f284ce":"code","83e6bbd3":"code","c4a8e01b":"code","dd9d7487":"code","3371a143":"code","ce336275":"code","8b41288b":"code","496aa783":"code","3d3e1134":"code","f97fbc14":"code","6328cab6":"code","941f6c4d":"code","75bda561":"code","9acdd336":"code","ae9296c8":"code","54f233e3":"code","a5a117a8":"code","df1e2946":"code","39859ee2":"code","ae488916":"code","91bed26f":"code","18049b86":"code","05b18c16":"code","352a91b3":"code","04019078":"code","1592959d":"code","ae96a40f":"code","65e07e36":"code","3fa3782e":"code","8101771a":"code","b15b4beb":"code","805f6e66":"code","4907de04":"code","0f3da24e":"code","640c24f6":"code","9a3a496f":"code","0336ea84":"code","b2db7b16":"code","a92dd2c1":"code","34d7e37b":"code","1a47ff10":"code","30844e76":"code","5635466c":"code","d8955a09":"code","b8306bc8":"code","2e176afe":"code","dbee4627":"code","98ed6e39":"code","e440f4cd":"code","418f51f8":"code","d3cb6fea":"code","523ddf1a":"code","2473ef0b":"code","95d0b0a9":"code","ae995903":"code","1ab8e70a":"code","ea93a500":"code","40174c5e":"code","cb93393d":"code","bdf62f37":"code","12d0239e":"code","a8d06794":"markdown","163ccc25":"markdown","db42aeec":"markdown","5baec37d":"markdown","5a66bf18":"markdown","e812d3d3":"markdown","29dc5492":"markdown","2bd4ff89":"markdown","7284dc81":"markdown","49324c8e":"markdown","8049851c":"markdown","9fd94276":"markdown","7ccaae2f":"markdown","b399fc56":"markdown","cc9337da":"markdown","c46cb1c6":"markdown","3840a32c":"markdown","c845ca95":"markdown"},"source":{"c0178145":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns","11d127fc":"df=pd.read_csv(\"..\/input\/machine-learning-24-hrs-hackathon\/train_SJC.csv\")","a7988caa":"df.head()","9c58793f":"## since some of the columns in our dataset has first row as headers we will split the dataset, fix it and then combine it \ndf_unnamed_cols=df.drop(['DateReported','DependentsOther','DaysWorkedPerWeek'],axis='columns')\ndf_named_cols=df[['DateReported','DependentsOther','DaysWorkedPerWeek']]\ndf_unnamed_cols.columns=df_unnamed_cols.iloc[0]\ndf_unnamed_cols=df_unnamed_cols.iloc[1: , :]\n","f94582f2":"data=pd.concat([df_unnamed_cols,df_named_cols],axis='columns').iloc[1: , :]\ndata.head()","2574f643":"## checking the shape of the dataset\ndata.shape # the data  contains 36176 rows and 15 unique columns","b5cd87a0":"## checking the datatypes of the dataset:\n##since the datatypes of variables are not in their correct form we will perform type casting\ndata.dtypes","2555bb2c":"### converting all the numeric variables into the correct data type\ndata['Age']=data['Age'].astype('int64')\ndata['DependentChildren']=data['DependentChildren'].astype('int64')\ndata['InitialIncurredCalimsCost']=data['InitialIncurredCalimsCost'].astype('int64')\ndata['DependentsOther']=data['DependentsOther'].astype('int64')\ndata['DaysWorkedPerWeek']=data['DaysWorkedPerWeek'].astype('int64')","14760b23":"data['WeeklyWages']=data['WeeklyWages'].astype('float64')\ndata['HoursWorkedPerWeek']=data['HoursWorkedPerWeek'].astype('float64')\ndata['UltimateIncurredClaimCost']=data['UltimateIncurredClaimCost'].astype('float64')\n","2a8dea42":"### converting the DateTimeOfAccident and DateReported into the datetime data type and creating near columns for EDA\ndata['YearOfAccident']=(data['DateTimeOfAccident'].str[0:4])\ndata['YearOfAccident']=data['YearOfAccident'].astype('int64')\ndata['MonthOfAccident']=data['DateTimeOfAccident'].str[5:7]\ndata['MonthOfAccident']=data['MonthOfAccident'].astype('int64')\ndata['hourofaccident']=data['DateTimeOfAccident'].str[11:13]\ndata['hourofaccident']=data['hourofaccident'].astype('int64')\n","e0d0826e":"data['DateOfAccident']=pd.to_datetime(data['DateTimeOfAccident']).dt.date\ndata['TimeOfAccident']=pd.to_datetime(data['DateTimeOfAccident']).dt.time\ndata['DateReported']=pd.to_datetime(data['DateReported']).dt.date\ndata.head()","f31dde12":"data.dtypes\n## all the variables are converted to proper data types","5f803d6c":"##Checking for missing values\ndata.isnull().sum()","bd73cf99":"data[data['MaritalStatus'].isnull()]","6bf2267d":"### The range of ages of people with null values in marital status columns are vast hence we fill it usinf 'U'-unkown\ndata['MaritalStatus'].fillna('U',inplace=True)","03251a89":"data[data['WeeklyWages'].isnull()]","8446df4b":"## the weekly wages column has missingvalue but the person works fro 3-5 days a week on average\n## we use mean imputaion to fill the missing values\ndata['WeeklyWages'].fillna(data['WeeklyWages'].mean(),inplace=True)","081daecc":"data[data['HoursWorkedPerWeek'].isnull()]","256d2237":"## hours worked per week is missing but people recieve weekly wages \n### we impute the missing values with median value \ndata.fillna(data['HoursWorkedPerWeek'].median(),inplace=True)","8af4934b":"data.isnull().sum() ##missing values are taken care of ","b1b37613":"def severity(InitialIncurredCalimsCost):\n    if 0<InitialIncurredCalimsCost<=5000:\n        return 'minor'\n    elif 5000<InitialIncurredCalimsCost<=20000:\n        return 'moderate'\n    elif 20000<InitialIncurredCalimsCost<=50000:\n        return 'serious'\n    elif 50000<InitialIncurredCalimsCost<=100000:\n        return 'severe'\n    elif InitialIncurredCalimsCost>100000:\n        return 'extremely severe'\n    \ndata['SeverityOfAccident']=data['InitialIncurredCalimsCost'].apply(lambda x: severity(x))\n\n","fed0a4c2":"data.head()","012e47e2":"### descriptive statistics for numeric columns:\n\ndata.select_dtypes(['int64','float64']).describe()","d560d272":"### descriptive statistics for categorical columns:\n\ndata.select_dtypes('O').describe()","8ffbac71":"plt.figure(figsize=(10,5))\nsns.countplot(data=data, x='Gender', palette='winter')","726c564d":"data.groupby('MaritalStatus')['UltimateIncurredClaimCost'].sum()","94aa2359":"plt.figure(figsize=(10,5))\nsns.barplot(data=data, x='SeverityOfAccident',y='Age', palette='winter')","d34f956a":"plt.figure(figsize=(10,5))\nsns.lineplot(data=data, x='YearOfAccident',y='UltimateIncurredClaimCost', palette='winter')","a8eac7ad":"plt.figure(figsize=(10,10))\nsns.heatmap(data.corr(),annot=True)","3e957193":"sns.boxplot(x='SeverityOfAccident',y='YearOfAccident',hue='Gender',data=data)","2df3973f":"sns.barplot(data=data,y='hourofaccident',x='SeverityOfAccident',palette='husl')","ed923888":"sns.lmplot(data=data, x='UltimateIncurredClaimCost',y='WeeklyWages',ci=False)","8aa0ccd7":"data['UltimateIncurredClaimCost'].hist(bins=10)\nplt.show()\nsns.distplot(data['UltimateIncurredClaimCost'],color='g')\nplt.show()\n","62f284ce":"### for the ease of model building we will create a copy of the dataframe and drop the uneccessary columns\nfinal=data.drop(['ClaimNumber','DateTimeOfAccident','ClaimDescription','DateReported','DateOfAccident','TimeOfAccident'],axis='columns')\nfinal.head()","83e6bbd3":"### we will split the above dataframe into into subsets of only numeric and only\nnum_variables=final.select_dtypes(['float64','int64'])\nnum_variables.head()","c4a8e01b":"num_variables.describe()","dd9d7487":"### feature selection\n## dropping features with very less variance\nfrom sklearn.feature_selection import VarianceThreshold\nthreshold=VarianceThreshold(threshold=0.3)\nthreshold.fit(num_variables)","3371a143":"threshold.get_support()","ce336275":"low_var_cols=[column for column in num_variables.columns if column not in num_variables.columns[threshold.get_support()]]","8b41288b":"for features in low_var_cols:\n    print(features)","496aa783":"num_variables.drop(low_var_cols,axis=1,inplace=True)","3d3e1134":"plt.figure(figsize=(10,10))\nsns.heatmap(num_variables.corr(),annot=True)","f97fbc14":"## we drop the variables with very less correlatioin coefficient from the above heatmap\nnum_variables.drop(['MonthOfAccident','hourofaccident','HoursWorkedPerWeek'],axis=1,inplace=True)","6328cab6":"num_variables.head()","941f6c4d":"## takinf only the categorical variables\ncat_variables=final.select_dtypes('O')\ncat_variables.head()","75bda561":"cat_variables.describe()","9acdd336":"cat_variables=pd.get_dummies(cat_variables,columns=['Gender','MaritalStatus','PartTimeFullTime'],drop_first=True)","ae9296c8":"import sklearn.preprocessing as pre \nlabel_encoder=pre.LabelEncoder()","54f233e3":"cat_variables['SeverityOfAccident']=label_encoder.fit_transform(cat_variables['SeverityOfAccident'])","a5a117a8":"cat_variables.head()","df1e2946":"## concating both numeric and categorical variables\nfinal_df=pd.concat([num_variables,cat_variables],axis=1)","39859ee2":"final_df.head()","ae488916":"## checking for outliers:\nplt.figure(figsize=(20,10))\nsns.boxplot(data=final_df)","91bed26f":"## removing outliers \n\nfor i in range(4):\n\n    limit1=3*final_df['InitialIncurredCalimsCost'].std()\n\n    lower_limit1=final_df['InitialIncurredCalimsCost'].mean()-limit1\n    upper_limit1=final_df['InitialIncurredCalimsCost'].mean()+limit1\n\n    final_df=final_df[(final_df['InitialIncurredCalimsCost']>lower_limit1)&(final_df['InitialIncurredCalimsCost']<upper_limit1)]\n\n    limit2=3*final_df['UltimateIncurredClaimCost'].std()\n\n    lower_limit2=final_df['UltimateIncurredClaimCost'].mean()-limit2\n    upper_limit2=final_df['UltimateIncurredClaimCost'].mean()+limit2\n\n    final_df=final_df[(final_df['UltimateIncurredClaimCost']>lower_limit2)&(final_df['UltimateIncurredClaimCost']<upper_limit2)]","18049b86":"## fromt he below plot we can obsevrve that  there is a significant difference in the number of outliers\n## this will have an impact on our model performance and hence is crucial\nplt.figure(figsize=(20,10))\nsns.boxplot(data=final_df)","05b18c16":"##importing the necessary libraries required for model building:\n\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.preprocessing import minmax_scale\nimport pickle","352a91b3":"Predictor=final_df.drop('UltimateIncurredClaimCost',axis=1)\nPredictor=Predictor.apply(minmax_scale)\nPredictor","04019078":"target=final_df['UltimateIncurredClaimCost']","1592959d":"from sklearn.model_selection import train_test_split\nX_train,X_test,Y_train,Y_test=train_test_split(Predictor,target,random_state=0,test_size=0.3)","ae96a40f":"model=LinearRegression()","65e07e36":"model.fit(X_train,Y_train)","3fa3782e":"model.score(X_train,Y_train)","8101771a":"model.score(X_test,Y_test)","b15b4beb":"## saving the model\n#pickle.dump(model,open(\"D:\\datasets\\ML(KM)\\hackathon practicals\\linear_model\",'wb'))","805f6e66":"# R square value\nprint (\"R^2 is: \\n\", model.score(X_test, Y_test))","4907de04":"#predictions on the test data set.\ny_pred = model.predict(X_test)","0f3da24e":"print ('RMSE is: \\n', mean_squared_error(Y_test, y_pred,squared=False))","640c24f6":"# from sklearn.svm import SVR\n# svr=SVR(C=1.0, epsilon=0.2)\n#svr.fit(X_train,Y_train)\n#svr.score(X_train,Y_train)\n#svr.score(X_test,Y_test)\n#print (\"R^2 is: \\n\", svr.score(X_test, Y_test))\n#y_pred = svr.predict(X_test)\n#print ('RMSE is: \\n', mean_squared_error(Y_test, y_pred,squared=False))","9a3a496f":"from sklearn.ensemble import RandomForestRegressor","0336ea84":"RF=RandomForestRegressor(n_estimators=200)","b2db7b16":"RF.fit(X_train,Y_train)","a92dd2c1":"RF.score(X_train,Y_train)","34d7e37b":"RF.score(X_test,Y_test)","1a47ff10":"## saving the model\n#pickle.dump(RF,open(\"D:\\datasets\\ML(KM)\\hackathon practicals\\Forest_model\",'wb'))","30844e76":"y_pred = RF.predict(X_test)","5635466c":"print ('RMSE is: \\n', mean_squared_error(Y_test, y_pred,squared=False))","d8955a09":"##average score of random forest\n\nfrom sklearn.model_selection import cross_val_score\ncross_val_score(RandomForestRegressor(),Predictor,target).mean()","b8306bc8":"##average score of linear regression\n\ncross_val_score(LinearRegression(),Predictor,target).mean()","2e176afe":"from sklearn.ensemble import AdaBoostRegressor","dbee4627":"AR=AdaBoostRegressor()","98ed6e39":"AR.fit(X_train,Y_train)","e440f4cd":"AR.score(X_train,Y_train)","418f51f8":"AR.score(X_test,Y_test)","d3cb6fea":"y_pred = AR.predict(X_test)","523ddf1a":"print ('RMSE is: \\n', mean_squared_error(Y_test, y_pred,squared=False))","2473ef0b":"## loading the test data and predicting the output\n\ncols_list=['Age','Gender','MaritalStatus','WeeklyWages','PartTimeFullTime','InitialIncurredCalimsCost','DateTimeOfAccident']\n\ntest_data=pd.read_csv('..\/input\/machine-learning-24-hrs-hackathon\/Test_SJC.csv',usecols=cols_list)\ntest_data.head()","95d0b0a9":"test_data.dtypes","ae995903":"test_data.isnull().sum()","1ab8e70a":"def test_data_processing(data_prime):\n    \n    data=data_prime.copy()\n    data['YearOfAccident']=data['DateTimeOfAccident'].str[0:4]\n    data['YearOfAccident']=data['YearOfAccident'].astype('int64')\n    ## filling missing values in marital status column\n    \n    data['MaritalStatus'].fillna('U')\n    def severity(InitialIncurredCalimsCost):\n        if 0<InitialIncurredCalimsCost<=5000:\n            return 'minor'\n        elif 5000<InitialIncurredCalimsCost<=20000:\n            return 'moderate'\n        elif 20000<InitialIncurredCalimsCost<=50000:\n            return 'serious'\n        elif 50000<InitialIncurredCalimsCost<=100000:\n            return 'severe'\n        elif InitialIncurredCalimsCost>100000:\n            return 'extremely severe'\n        \n    \n    data['SeverityOfAccident']=data['InitialIncurredCalimsCost'].apply(lambda x: severity(x))\n    \n    data.drop('DateTimeOfAccident',axis=1,inplace=True)\n    \n    data=pd.get_dummies(data,columns=['Gender','MaritalStatus','PartTimeFullTime'],drop_first=True)\n    import sklearn.preprocessing as pre \n    label_encoder=pre.LabelEncoder()\n    \n    data['SeverityOfAccident']=label_encoder.fit_transform(data['SeverityOfAccident'])\n    \n    data=data.apply(minmax_scale)\n    return data\n    \n\n\n    \n    ","ea93a500":"testdata=test_data_processing(test_data)\ntestdata","40174c5e":"testdata.isnull().sum()","cb93393d":"y_predicted=RF.predict(testdata)","bdf62f37":"result=y_predicted\nresult","12d0239e":"###submitting the code and writing it in the form of csv\ndef submission(result):\n    submission = pd.read_csv(\"..\/input\/machine-learning-24-hrs-hackathon\/sample_submission.csv\")\n    submission = submission.drop('UltimateIncurredClaimCost',axis=1)\n    submission['UltimateIncurredClaimCost'] = result\n    #Writing the file\n    submission.to_csv(\"submit.csv\",index=False)\n    \nsubmission(result)","a8d06794":"## 7. Does the hour of accident occurence determine the severity\n    From the below plot is is clear that hour of the accided=nt does not really determine the severity of the accident.","163ccc25":"## 3. Does a person's age dictate the severity of the accident?\n    From the below bar plot it is seen oler a person gets more prone he\/she is in getting injured more severely, that is if at all the person meets with an accident. As in the gender case, as people get older they tend do be more physically active and as they enter their senior age they are sensitive to accidents. Making the insurance claim higher.","db42aeec":"### Since the claim description has a lot of unique columns we can create another column using the values of 'InitialIncurredCalimsCost' in order to generalise the accident based on it's severity, claimed larger amount generally tend to have acquired a severe injury like fracture,sprain and poeple  people who have claimed lower amount tend to have acquired a smaller injury like cuts or bruises. We will call this column SeverityOfAccident","5baec37d":"## 2. Does Marital status of a person affect the medical insurance offered?\n    The below observation shows that marital startus does not really play a major role in a person acquiring a medical insurance. In both cases the isurance offered is more or less the same, lacking striking difference.","5a66bf18":"## RandomForest Regressor:","e812d3d3":"## Adaboost Regressor:","29dc5492":"### Loading the dataset uisng pandas:","2bd4ff89":"### Data Cleaning","7284dc81":"## 4. What is the effect of year of accident on the insurance acquired?\n    The below lineplot shows the grwoing trend in the insurance claimed. There is a steady increase in the insurance. This can be accounted for the fact that the medical field has become more advance making it more expensive for treatements. A person can be treated easily but is demanded to pay a higher cost.","49324c8e":"### Random Forest Regressor is used  to predict the values as it gives better train and test accuracy with the mean squared error of 1780.89 it does overfit sloghtly on the train subset but it performs decently on the test subset  ","8049851c":"### Exploratory data analysis:\n    Below are some interesting questions that are answered using methods of pandas and visualisations using matplotlib and seaborn. Before that, descriptive statistics for numeric as well as categorical columns are observed for basic understanding of the data.","9fd94276":"## Linear Regression model:","7ccaae2f":"## 1.Does a person's gender play a role in he\/she meeting with an accident?\n    FRom the below plot we can see that males are more prone to accidents compared to females. This can be accounted to the fact that Men are more employed and physcically more active comapred to females. Another conclusion can be that women tend to be more careful.","b399fc56":"## 8.Does a person's UltimateIncurredClaimCost depend on his\/her weekly wages?\n","cc9337da":"## 6. Severity of accidents over the years with respect to difference in gender\n\n    Though the boxplot below might seem complicated it just shows the severity of accident the person with a particular gender has met with. Over the years the severity of accidents has skyrocketed. There can be multiple explanations for this trend. But one which is strinking and evident is that vehicles have incresead and so as urbanization. In our busy lives we have seen to lose patience. Boxplot proves to be useful as several outliers can also be observed for further treatment.","c46cb1c6":"## 5.correlation between features\n    The below Heatmap shows the correlation between the numeric columns in our data with the correlation coefficients between them.","3840a32c":"## 9. How is our target variable distributed?\n    The below plot shows the distribution of our target variable.","c845ca95":"### Importing the necessary libraries:"}}