{"cell_type":{"b801816d":"code","50c3dece":"code","7135b25c":"code","7555c666":"code","2cdc9a42":"code","d892dd4d":"code","61cb268c":"code","805b1680":"code","0addd940":"code","ccf080a4":"code","3c2cf379":"code","b261a8d7":"code","e4671aeb":"code","ac11ef17":"code","3583d32a":"code","24f9e663":"code","da038e44":"code","bbbf4bf2":"code","c0cb25e8":"code","c3e2fd7d":"code","a92292c5":"code","d12ea0f8":"code","f046cc54":"code","d5724dea":"code","2ec4b088":"code","33112964":"code","ae0c3169":"code","7b441680":"code","327cc8a1":"code","b83443bb":"code","eca6ea08":"code","255e05b1":"code","ff783d23":"code","b3b2e364":"markdown","0274fc88":"markdown","485dada6":"markdown","343c93de":"markdown","b70c4517":"markdown","33790ac6":"markdown","fc331aec":"markdown","e2e31098":"markdown","baa0a800":"markdown","8c9d57f8":"markdown","6d01f419":"markdown","cb451223":"markdown","40f57ebe":"markdown","da76ab73":"markdown","8444ac00":"markdown","dc1a9ba8":"markdown","299094bd":"markdown","88497517":"markdown","ac02b397":"markdown","cc8a922d":"markdown","439ceed1":"markdown","d0dd61a5":"markdown","f2777dcc":"markdown","c32dcf9b":"markdown","41c24e5f":"markdown","96e3ce4d":"markdown","b4a487c2":"markdown","60dac673":"markdown"},"source":{"b801816d":"import numpy as np \nimport pandas as pd \n\nfrom sklearn.metrics import mutual_info_score\nfrom sklearn.preprocessing import StandardScaler\nimport scipy.stats as ss\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport scipy.cluster.hierarchy as sch\nfrom scipy.cluster.hierarchy import fcluster\nimport networkx as nx","50c3dece":"%%time\n# Set Pandas options\npd.set_option('display.max_columns', 145)\n\n# Load training set and metadata\ndf       = pd.read_csv('..\/input\/jane-street-market-prediction\/train.csv')\nfeatures = pd.read_csv('..\/input\/jane-street-market-prediction\/features.csv')\ndf.info()","7135b25c":"# Set number of trading days to analyze. \n# n_days = 500 for full dataset\nn_days = 30\n\ndays = list(range(n_days))\ndf_red = df[df.date.isin(days)]\ndf_red.info()","7555c666":"ft = df_red.iloc[:,8:-1]\nft['resp'] = df_red.resp\nft['wresp'] = df_red.resp*df.weight\n\nft = (ft - ft.mean())\/(ft.std())\nft.rename(columns=lambda s: s.replace('feature_','ft_'),inplace=True)\nft.head()","2cdc9a42":"# Outliers threshold: 0.999 quantile\noutl_thresh = 0.999\n\noutliers_mask = (ft.abs() > ft.abs().quantile(outl_thresh)).any(axis=1)\ntotal_rows    = outliers_mask.size\ndropped_rows  = outliers_mask.sum()\nprint('Dropped rows = ', outliers_mask.sum(), '({:.2%} of the total number of rows)'.format(dropped_rows\/total_rows,2))\nft = ft[~outliers_mask]","d892dd4d":"r = np.linspace(-1,1,500)\nd = np.sqrt(0.5*(1-r))\n\nplt.figure(figsize=(12,6))\nplt.title('Correlation-based distance')\nplt.xlabel('Correlation coefficient')\nplt.ylabel('Distance')\nplt.plot(r,d)\nplt.show()","61cb268c":"# 1. Paerson correlation coefficient\ncorr_mat = ft.corr(method='pearson')\n\n# 2. Correlation-based distance\ndist = np.sqrt(0.5*(1-corr_mat))","805b1680":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(23,9))\n\nsns.heatmap(corr_mat,ax= ax1, cmap='coolwarm');\nsns.heatmap(dist,    ax= ax2, cmap='coolwarm');\nax1.title.set_text('Correlation matrix')\nax2.title.set_text('Distance matrix')\nplt.show()","0addd940":"# Complete graph from distance matrix\nG = nx.from_numpy_matrix(dist.to_numpy())\n\nlabels = dist.columns.values\nlabels = [s.replace('ft_','') for s in labels]\nG = nx.relabel_nodes(G, dict(zip(range(len(labels)), labels)))","ccf080a4":"# Minimum spanning tree\nT=nx.minimum_spanning_tree(G)\n\nfig = plt.figure(figsize=(20,20))\nnx.draw_networkx(T,\n                 with_labels=True, \n                 font_size=9, \n                 cmap=plt.cm.coolwarm,\n                 pos=nx.kamada_kawai_layout(T),vmin=0, vmax=1)\nplt.show()","3c2cf379":"# Linkage matrix\nlink=sch.linkage(dist,'average')","b261a8d7":"# Plot dendrogram\n\nfig = plt.figure(figsize=(20, 8))\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('Feature')\nplt.ylabel('Distance')\nplt.hlines(1.6,0,1320)\ndn = sch.dendrogram(link,leaf_rotation=90.,leaf_font_size=11.)\nplt.show()","e4671aeb":"# fcluster forms flat clusters from the hierarchical clustering defined by the given linkage matrix.\n\nmax_d = 1.6\nclusters = fcluster(link,t=max_d, criterion='distance')","ac11ef17":"df_clust = pd.DataFrame({'Cluster':clusters, 'Features':ft.columns.values.astype('str')})\ndf_clust.groupby('Cluster').count()","3583d32a":"# Save the cluster-feature in a dictionary \nclust_feat = {}\nfor k in np.unique(clusters):\n    clust_feat[k] = df_clust[df_clust.Cluster == k].Features.values","24f9e663":"for k in np.unique(clusters):\n    print('Cluster_{}'.format(k,2),'->', df_clust[df_clust.Cluster == k].Features.values)","da038e44":"def getQuasiDiag(link):\n    # Sort clustered items by distance\n    link=link.astype(int)\n    sortIx=pd.Series([link[-1,0],link[-1,1]])\n    numItems=link[-1,3] # number of original items\n    while sortIx.max()>=numItems:\n        sortIx.index=range(0,sortIx.shape[0]*2,2) # make space\n        df0=sortIx[sortIx>=numItems] # find clusters\n        i=df0.index;j=df0.values-numItems\n        sortIx[i]=link[j,0] # item 1\n        df0=pd.Series(link[j,1],index=i+1)\n        sortIx=sortIx.append(df0) # item 2\n        sortIx=sortIx.sort_index() # re-sort\n        sortIx.index=range(sortIx.shape[0]) # re-index\n    return sortIx.tolist()","bbbf4bf2":"sortIx=getQuasiDiag(link)\nsortIx=corr_mat.index[sortIx].tolist() # recover labels\ncorr_diag=corr_mat.loc[sortIx,sortIx] # reorder\n\nsortIx=getQuasiDiag(link)\nsortIx=dist.index[sortIx].tolist() # recover labels\ndist_diag=dist.loc[sortIx,sortIx] # reorder\n","c0cb25e8":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(23,9))\n\nsns.heatmap(corr_diag,ax= ax1, cmap='coolwarm');\nsns.heatmap(dist_diag,ax= ax2, cmap='coolwarm');\nax1.title.set_text('Quasi-diagonal Correlation matrix')\nax2.title.set_text('Quasi-diagonal Distance matrix')\nplt.show()","c3e2fd7d":"def numBins(nObs,corr=None):\n    # Optimal number of bins for discretization\n    if corr is None: # univariate case\n        z=(8+324*nObs+12*(36*nObs+729*nObs**2)**.5)**(1\/3.)\n        b=round(z\/6.+2.\/(3*z)+1.\/3)\n    else: # bivariate case\n        b=round(2**-.5*(1+(1+24*nObs\/(1.-corr**2))**.5)**.5)\n    return int(b)","a92292c5":"def varInfo(x,y,norm=False):\n    if (x == y).all():\n        return 0\n    # variation of information\n    bXY=numBins(x.shape[0],corr=np.corrcoef(x,y)[0,1])\n    cXY=np.histogram2d(x,y,bXY)[0]\n    iXY=mutual_info_score(None,None,contingency=cXY)\n    hX=ss.entropy(np.histogram(x,bXY)[0]) # marginal\n    hY=ss.entropy(np.histogram(y,bXY)[0]) # marginal\n    vXY=hX+hY-2*iXY # variation of information\n    if norm:\n        hXY=hX+hY-iXY # joint\n        vXY\/=hXY # normalized variation of information\n    return vXY","d12ea0f8":"# Drop NaNs and calculate variation of information\nft_red = ft.dropna()\nV = np.zeros(corr_diag.shape)\n\nfor i in range(corr_mat.shape[0]):\n    for j in range(i):\n        x = ft_red.iloc[:,i]\n        y = ft_red.iloc[:,j]\n        V[i,j] = varInfo(x,y,norm=True)\n\n","f046cc54":"dist_info = pd.DataFrame(V + V.T,columns=ft.columns,index=ft.columns)\ndist_info","d5724dea":"# Fully connected graph\nX = dist_info.to_numpy()\nG = nx.from_numpy_matrix(X)\n\n# Minumum spanning tree\nlabels = dist.columns.values\nG = nx.relabel_nodes(G, dict(zip(range(len(labels)), labels)))\nT=nx.minimum_spanning_tree(G)\n\n# MST Plot\nfig = plt.figure(figsize=(20, 20))\nnx.draw_networkx(T, with_labels=True, font_size=12, node_size=60,pos=nx.kamada_kawai_layout(T))\nplt.show()","2ec4b088":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(23,9))\n\nsns.heatmap(dist,     ax= ax1, cmap='coolwarm');\nsns.heatmap(dist_info,ax= ax2, cmap='coolwarm');\nax1.title.set_text('Correlation-based Distance')\nax2.title.set_text('Information-based Distance')\nplt.show()","33112964":"plt.figure(figsize=(21,8))\n\nplt.plot(dist.mean())\nplt.plot(dist_info.mean())\nplt.title('Average distance')\nplt.legend(('Correlation distance','Information distance'))\nplt.xlabel('Feature')\nplt.ylabel('Distance')\nplt.xticks(rotation=90,fontsize=9)\nplt.show()","ae0c3169":"link_info=sch.linkage(dist_info,'average')\n","7b441680":"fig = plt.figure(figsize=(20, 8))\nplt.title('Hierarchical Clustering Dendrogram')\nplt.xlabel('Feature')\nplt.ylabel('Distance')\nplt.hlines(1.6,0,1320)\ndn = sch.dendrogram(link_info,leaf_rotation=90.,leaf_font_size=12.)\nplt.show()","327cc8a1":"# fcluster forms flat clusters from the hierarchical clustering defined by the given linkage matrix.\n\nmax_d = 1.6\nclusters = fcluster(link_info,t=max_d, criterion='distance')","b83443bb":"df_clust = pd.DataFrame({'Cluster':clusters, 'Features':ft.columns.values.astype('str')})\ndf_clust.groupby('Cluster').count()","eca6ea08":"for k in np.unique(clusters):\n    print('Cluster_{}'.format(k,2),'->', df_clust[df_clust.Cluster == k].Features.values)","255e05b1":"sortIx=getQuasiDiag(link_info)\nsortIx=dist_info.index[sortIx].tolist() # recover labels\ndist_info_diag=dist_info.loc[sortIx,sortIx] # reorder","ff783d23":"fig, (ax1, ax2) = plt.subplots(ncols=2, figsize=(23, 9))\n\nsns.heatmap(dist_diag,     ax= ax1, cmap='coolwarm');\nsns.heatmap(dist_info_diag,ax= ax2, cmap='coolwarm');\nax1.title.set_text('Correlation-based quasi-diagonal distance')\nax2.title.set_text('Information-based quasi-diagonal distance')\nplt.show()","b3b2e364":"Just looking  at the Spanning Tree, we can see that there are features that are so highly correlated that they are almost indistinguishable in the image. For example, `ft_60`-`ft_61` (correlation coefficient $\\rho = 0.995$), `ft_65`-`ft_66` ($\\rho = 0.995$), `ft_62`-`ft_63` ($\\rho = 0.998$), `ft_67`-`ft_68` ($\\rho = 0.999$), `ft_90`-`ft_114`-`ft_102` ($\\rho = 0.996$) and so on. The condition under which one predictor variable can be linearly predicted by the others is called [*multicollinearity*] (https:\/\/en.wikipedia.org\/wiki\/Multicollinearity#Consequences_of_multicollinearity). Depending on the predictive model, the presence of multicollinearity may have an impact on model performance. In certain contexts, it is therefore desirable to manage interdependent features [[7]](https:\/\/datascience.stackexchange.com\/questions\/24452\/in-supervised-learning-why-is-it-bad-to-have-correlated-features) [[8]](https:\/\/datascience.stackexchange.com\/questions\/65815\/chose-among-highly-correlated-variables).","0274fc88":"## 2. Data pre-processing\n\n\nTo work with a less cumbersome dataset, we can select a subset of 30 trading days from the original data frame. You can simply change `n_days` to work with more or less trading days.","485dada6":"We are now in the position of having to choose the optimal number of clusters. We must decide where to \"cut\" the dendrogram to consider all features below the horizontal line as belonging to the same cluster. There are a variety of techniques to determine the optimal number of clusters[[9]](https:\/\/www.datanovia.com\/en\/lessons\/determining-the-optimal-number-of-clusters-3-must-know-methods\/) [[10]](https:\/\/stats.stackexchange.com\/questions\/23472\/how-to-decide-on-the-correct-number-of-clusters)[[11]](https:\/\/stats.stackexchange.com\/questions\/3685\/where-to-cut-a-dendrogram). We will not go into too much technical detail (the purpose of this notebook is to give an overview of hierarchical clustering). \n\nWe follow a simple approach: we find the longest vertical line that is not interrupted by a horizontal line. There are several techniques that are far more sophisticated than the one I used here to determine the optimal number of clusters. This \"longest vertical line\" approach is the \"zeroth-order\" solution, and the intuitive explanation is that the length of the vertical lines represents the distance between the sub-clusters. We can think of the longest vertical line as the level where the longest inter-cluster distance occurs.","343c93de":"### 4.3 Quasi-diagonalization","b70c4517":"## Structure\n\n1. Load datasets\n2. Data preprocessing\n3. Correlation-based distance\n4. Information-based distance\n5. References","33790ac6":"## Introduction\n\nFirst of all, Happy New Year!\n\nThis notebook is purely exploratory. Lately I have been studying the great book *Advances in Financial Machine Learning* by Marcos Lopez De Prado. In the chapter XVI, \"Machine Learning Asset Allocation\", the author explores the Hierarchical Risk Parity (HRP) approach to address the asset allocation problem. HRP uses the concept of hierarchical clustering to determine the optimal allocation among different assets or strategies.\n\nThe purpose of this notebook is to explore the HRP approach in a context other than the one in which the author presents it. I also wondered if hierarchical analysis could be used to gain insight into the data set.\n\nThis is my first modest attempt to give back to the community. I hope I could at least provide some food for thought :)\n\n\n## Cluster analysis\n\nThe goal of cluster analysis is to group a *set of objects* into disjoint subsets, or clusters, based on similarities between them. Objects in the same cluster are very similar to each other, while objects from different clusters have low similarity to each other.\n\nThe *set of objects* in this notebook is the set of anonymized features in the [Jane Street Market Prediction](https:\/\/www.kaggle.com\/c\/jane-street-market-prediction\/data) dataset.\n\n[Cluster analysis](https:\/\/en.wikipedia.org\/wiki\/Cluster_analysis) is a broad topic, and a wide range of clustering algorithms are available. These algorithms are classified according to various criteria.\n\nAn initial classification of clustering algorithms is often made into **partitional** vs. **hierarchical** algorithms. Partitional algorithms produce a *flat* partition, where an element belongs to one and only one cluster. Hierarchical algorithms create a *multi-layered* partition, with clusters of elements at the bottom level, clusters of clusters at the next level, and so on, until a single all-inclusive cluster at the top of that hierarchy. The following image (from [this blog post](https:\/\/quantdare.com\/hierarchical-clustering\/)) gives an intuitive idea of the difference between partitional and hierarchical clustering.\n\n\n![Hierarchical vs Partitional](https:\/\/quantdare.com\/wp-content\/uploads\/2016\/06\/HierarPartClustering-800x306.png)\n\nThere are two types of hierarchical clustering algorithms: agglomerative and divisive. Agglomerative algorithms proceed by grouping elementary objects into larger and larger clusters. Divisive algorithms, on the other hand, start from an all-inclusive cluster and decompose it into the individual objects.\n\n![Agglomerative vs Divisive](https:\/\/quantdare.com\/wp-content\/uploads\/2016\/06\/AggloDivHierarClustering-800x389.png)\n\nThe notion of \"cluster\" is quite elusive, which is why so many clustering algorithms have been developed over the years [[1]](https:\/\/dl.acm.org\/doi\/10.1145\/568574.568575). Depending on the definition of the term \"cluster\", we can distinguish several types of *cluster models*, including.\n\n- **Connectivity models**: (e.g.: hierarchical clustering) are based on a notion of distance connectivity.\n\n- **Centroid models**: (e.g.: k-means algorithm) represents each cluster by a single mean vector.\n\n- **Distribution models**: (e.g.: EM algorithms) clusters are modeled using statistical distributions.\n\n- **Density models**: (e.g.: DBSCAN, OPTICS) defines clusters as connected dense regions in the data space.\n\n- **Graph-based models**: (e.g.: HCS clustering algorithm) work by representing the similarity in data through a similarity graph and then finding all the highly connected subgraphs. \n\nIn this notebook we examine some variants of the **agglomerative hierarchical clustering** algorithm. \n\nOur objects (features) are anonymized. We can get some hints from the *features.csv* metadata, but we are in a situation where we do not have a clear view of of objects we are working with. We can therefore think of this cluster analysis in terms of exploratory data analysis. A deeper insight into the hierarchy of features can be a useful precursor to feature selection\/engineering.\n\n## Tree clustering\n\nConsider a $T\\times F$ matrix $X$, where $T$ is the number of observations and $F$ is the number of features. We would like to combine these $F$ column vectors into a hierarchical structure of clusters. Hierarchical clustering algorithms are based on a notion of **distance**. We need to think of our features as our set of objects. And we need to find a way to define a distance between these columns. In this notebook, we will examine and compare two different notions of distance: **Correlation-based distance** and **Information-based distance**.\n\n","fc331aec":"A [distance matrix](https:\/\/en.wikipedia.org\/wiki\/Distance_matrix) can be thought as a [weighted adjacency matrix](https:\/\/en.wikipedia.org\/wiki\/Adjacency_matrix) of some graph. It follows it can be represented as complete graphs whose nodes are the features and edges lenght is proportional to the distance between two nodes. The picture below represents a weighted graph and its adjacency matrix. \n\n![GraphW.png](attachment:GraphW.png)","e2e31098":"### 3.1 Graph representation   ","baa0a800":"### 3.3 Quasi-Diagonalization\n\nIn this step, the rows and columns of the correlation matrix are reordered so that the largest values lie along the diagonal. This quasi-diagonalization of the covariance matrix doesn not require a change in basis and it returns a block-diagonal matrix where similar columns are placed together and dissimilar columns are placed far apart. Code below from [2]","8c9d57f8":"## References\n\n[1] **Vladimir Estivill-Castro.** (2002). Why so many clustering algorithms: a position paper. SIGKDD Explor. Newsl. 4, 1 (June 2002), 65\u201375. DOI:https:\/\/doi.org\/10.1145\/568574.568575\n\n[2] **L\u00f3pez de Prado, M.** (2020) - Machine Learning for Asset Managers (Elements in Quantitative Finance). Cambridge: Cambridge University Press. doi:10.1017\/9781108883658\n\n[3] **L\u00f3pez de Prado, M.** (2018) - Advances in Financial Machine Learning (1st. ed.). Wiley Publishing.\n\n[4] [StackExchange - Choosing the right linkage method for hierarchical clustering](https:\/\/stats.stackexchange.com\/questions\/195446\/choosing-the-right-linkage-method-for-hierarchical-clustering)\n\n[5] [StackExchange - How to select a clustering method? How to validate a cluster solution?](https:\/\/stats.stackexchange.com\/questions\/195456\/how-to-select-a-clustering-method-how-to-validate-a-cluster-solution-to-warran)\n\n[6] [StackExchange - Comparing hierarchical clustering dendrograms obtained by different distances & methods](https:\/\/stats.stackexchange.com\/questions\/63546\/comparing-hierarchical-clustering-dendrograms-obtained-by-different-distances\/63549#63549)\n\n[7] [StackExchange - In supervised learning, why is it bad to have correlated features?](https:\/\/datascience.stackexchange.com\/questions\/24452\/in-supervised-learning-why-is-it-bad-to-have-correlated-features)\n\n[8] [StackExchange - Chose among highly correlated variables](https:\/\/datascience.stackexchange.com\/questions\/24452\/in-supervised-learning-why-is-it-bad-to-have-correlated-features)\n\n[9] [DataNovia - Determining The Optimal Number Of Clusters: 3 Must Know Methods](https:\/\/www.datanovia.com\/en\/lessons\/determining-the-optimal-number-of-clusters-3-must-know-methods\/)\n\n[10] [StackExchange - How to decide on the correct number of clusters?](https:\/\/stats.stackexchange.com\/questions\/23472\/how-to-decide-on-the-correct-number-of-clusters)\n\n[11] [StackExchange - Where to cut a dendrogram?](https:\/\/stats.stackexchange.com\/questions\/3685\/where-to-cut-a-dendrogram)\n\n[12] [Kaggle Notebook - Hierarchical Clustering](https:\/\/www.kaggle.com\/biphili\/hierarchical-clustering) by @Binu","6d01f419":"Let's proceed with the linkage matrix.","cb451223":"The correlation matrix and the distance matrix look like this","40f57ebe":"## 3. Correlation-based distance\n\nLet us analyze the correlation-based metric. Consider the $N\\times F$ feature matrix, where in our case $F = 131$ and $N$ is the number of observations. We would like to combine these $F$ column vectors into a hierarchical structure of clusters. The steps are as follows:\n1. compute the $F\\times F$ correlation matrix $\\rho = \\{\\rho_{i,j}\\}_{i,j = 1,...,N}$\n2. derive a distance matrix $D = d_{i,j} = \\sqrt{\\frac{1}{2}(1-\\rho_{i,j})}$\n3. compute the linkage matrix $L$ from the distance matrix $D$\n4. rearrange the rows and columns of the correlation matrix so that the largest values lie along the diagonal (Quasi-Diagonalization).\n\nTo visualize the distance between features, we will use two very helpful tools: **Dendrograms** and **Minimum Spanning Trees**. \n\nIt can be shown [2] that $d_{i,j}$ is a distance in the mathematical sense. If two features are perfectly anticorrelated (\u03c1=-1), the distance between them is d=1, while if they are perfectly correlated (\u03c1=1), the distance is d=0. \n\nThe distance as a function of the Pearson correlation coefficient is plotted below.","da76ab73":"## 1. Load datasets","8444ac00":"The function `from_numpy_matrix` form [NetworkX](https:\/\/networkx.org\/) returns a graph from numpy matrix. A visual represenation of the distance between features (and a first hint of their clustering properties) can be obtained by selecting a subset of the complete graph obtained from the distance matrix. This subset is called *minimum spanning tree*. As per wikipedia\n\n> A minimum spanning tree (MST) is a subset of the edges of a connected, edge-weighted undirected graph that connects all the vertices together, without any cycles and with **the minimum possible total edge weight**.\n\n![300px-Minimum_spanning_tree.svg.png](attachment:300px-Minimum_spanning_tree.svg.png)\nA minimum spanning tree structure incorporates hierarchical relationships","dc1a9ba8":"This is an interesting chessboard. We can see that there are highly (anti-)correlated features. In particular, three clusters stand out: `ft_17` to `ft_40`, the two overlapping squares in `ft_73` to `ft_93` and `ft_85` to `ft_117`, and the *chessboard* between `ft_121` and `ft_129`. A very important **caveat** at this point: we are examining correlations for the first thirty days of trading. Correlations are not static entities, on the contrary, they tend to change and evolve depending on the time period and the market regime. Analyzing the stability of these clusters over time is an interesting point that could be analyzed. Stable clusters could be thought as intrinsic properties of the dataset rather than temporary events.","299094bd":"Let's calculate the variation of information for our features and see what happens. \n\n(The execution of the following section takes about 20 min. If there is someone who has the good will to improve the function, he\/she has all my gratitude and respect).","88497517":"The hierarchical clustering algorithm identifies 6 clusters when the information-based distance is used. However, four of these clusters are de facto small groups of highly correlated features:\n\n`Cluster_2 -> ['ft_62' 'ft_63']`\n`Cluster_3 -> ['ft_60' 'ft_61']`\n`Cluster_5 -> ['ft_67' 'ft_68']`\n`Cluster_6 -> ['ft_64' 'ft_65' 'ft_66']`\n\nA debatable point is: does it make sense to use the same threshold distance max_d = 1.6 as in the correlation-based case? In my opinion, the answer is: not necessarily. Using the same threshold distance makes sense in this case to compare results between two different methods, but it is important to keep in mind that correlation-based and information-based are two different metrics. To put it simply: We need to be careful not to compare kilometers and miles.\n\nComparing different dendrograms and different methods would require a whole separate notebook, and I will not dwell on it too much. See the References section for links to a couple of interesting threads on StackExchange that discuss these topics in detail.","ac02b397":"### 4.1 Graph representation\n\nAs in the correlation-based case, we can create a minumum spanning tree to visualize the *features network*.","cc8a922d":"## 5 Final toughts and possible improvement\n\nIn this notebook we have studied the hierarchical clustering of features with two different definitions of distance: the first based on Pearson's correlation coefficient, the second on the Variation of Information. We derived a linkage matrix from the distance matrix and used it to group the features into clusters. To visualize the result, we used dendrograms and minimum spanning trees. The correlation-based approach identifies eight clusters. The information-based one only six, although two of these six are massive clusters with 82 and 40 features, respectively. The remaining four are just small groups of highly codependent features. One reason for this may be the questionable choice of a threshold distance `max_d = 1.6`, which is the same distance as in the correlation-based case.\n\nWhat we have shown so far is a fairly 'out of the box' implementation. The goal was primarily to show these techniques and provide a baseline for those interested in exploring beyond this approach. There are a number of improvements or more in-depth studies that can be done. For example:\n\n1. test more robust criteria for determining the optimal number of clusters.\n\n2. test alternative linkage criteria (\"single\", \"weighted\", \"ward\", etc.).\n\n3. use random matrix theories to denoise the correlation matrix.\n\n4. investigate the optimal threshold distance for the information-based case,\n\nThank you for making it this far. I hope you have enjoyed this work. If you have any comments, questions or observations, please let's discuss them in the comment section.","439ceed1":"Correlation-based and information-based distance matrices show very similar patterns. The information-based distance is on average higher than its correlation-based counterpart (see chart below). It would be interesting to investigate this result further: Does it mean that the information distance is \"less sensitive\"? Or perhaps that it is better able to detect meaningful relationships?","d0dd61a5":"## 4. Information-based distance\n\nThe notion of correlation has three important [pitfalls](10.4103\/2229-3485.192046). First, it quantifies the linear relationship between two random variables. Therefore, it may be fallacious in situations where two variables are interdependent in a nonlinear way. Second, correlation is very sensitive to outliers. Finally, its application beyond the multivariate case is questionable.\n\nTo overcome these caveats, it is possible to introduce a new distance based on information-theoretic concepts.\nThis quantity is called Variation of Information. Given two random variables $X$ and $Y$, the [variation of information](https:\/\/en.wikipedia.org\/wiki\/Variation_of_information) between $X$ and $Y$ is defined as: \n\n$VI[X,Y] = 1 - \\frac{I[X,Y]}{H[X,Y]}$,\n\nwhere $I[X,Y]$ is the mutual information between $X$ and $Y$ and $H[X,Y]$ is the joint entropy. See [2] for a detailed explanation. To compute the variation of information, we need to bin our observations. The following functions (from [2]) compute the optimal number of bins and the Variation of Information.","f2777dcc":"The hierarchical clustering algorithm, with correlation-based distance and average distance as linkage criteria, identifies 8 clusters. The largest cluster contains 25 features, the smallest only 6.","c32dcf9b":"Later, we will calculate a correlation matrix. Outliers have a large impact on the Pearson correlation coefficient. Therefore, it is a good practice to remove them from the feature matrix. The definition of outliers is not unique and it is context dependent. Here, we consider outliers to be the data above the 99.9th percentile.","41c24e5f":"Let's now focus now on the feature matrix. We select `feature_1` to `feature_129` and `resp`. Finally, we add the column `wresp`, which is `resp` times `weights`, and perform a feature scaling. Strictly speaking, `resp` and `wresp` are not features, but I think it's interesting to see if they correlate with any other features in particular.","96e3ce4d":"### 3.2 Correlation-based clustering\n\nNow that we have a distance matrix, we can use it to perform a hierarchical agglomerative cluster. A a step-by-step explaination of this process can be found in Chap.16 of [2]. ","b4a487c2":"### 4.2 Information-based clustering\n\nAs in the correlation-based case, we can use the distance matrix to obtain a linkage matrix and the final clustering. Before proceeding further, it is interesting to compare the correlation-based distance matrix and with the information-based distance matrix.","60dac673":"The spanning tree calculated from the information-based metric seems more *stretched*. The features seem to be further apart and less clustered. However, the features that we had identified as highly correlated are also extremely close here (`ft_60`-`ft_61`, `ft_65`-`ft_66`, `ft_62`-`ft_63`, `ft_67`-`ft_68` and so on. Let's see what happens when we plot the distance matrix and compare it to the distance matrix obtained from the correlation based distance."}}