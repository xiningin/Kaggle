{"cell_type":{"b5b85490":"code","4f52b85f":"code","5469ab30":"code","891ea091":"code","89a9bdb6":"code","6a17dfd2":"code","0d111735":"code","99cf7f68":"code","0d15a625":"code","f9292332":"code","becdab93":"code","91b2bc73":"code","a4c6829c":"code","d216c1a3":"code","8f6d3e0a":"code","679fdb77":"code","235eea83":"code","63dd18ac":"code","7e2a9dd9":"code","86f035d4":"code","0b1a2146":"code","3b9b51c4":"code","1dcfeddf":"code","26f78acf":"markdown","fa545cae":"markdown","d500f49f":"markdown","a8f087d9":"markdown","ac946ed0":"markdown","0a5246c9":"markdown","0c1846d1":"markdown","6db6d111":"markdown","e64d3447":"markdown","0e010bbe":"markdown","ecc3cce7":"markdown","3562e109":"markdown","f6534340":"markdown","224d060e":"markdown","d80e8989":"markdown","ee29ca55":"markdown","ff33a836":"markdown","31c10f6f":"markdown","b89fbf45":"markdown"},"source":{"b5b85490":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.cross_validation import train_test_split\nimport itertools\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\ndata_path = '..\/input\/creditcard.csv'\ndata = pd.read_csv(data_path)","4f52b85f":"featureInfo = {'Min':data.min(), 'Mean':data.mean(), 'Std':data.std(), 'Max':data.max()}\nprint(pd.DataFrame(data=featureInfo))\nprint('===============================================================')\ndata.info()\nprint('===============================================================')\nnumEachClass = data['Class'].value_counts()\nplt.title(\"Number of samples of each class\")\nplt.xlabel(\"Class\")\nplt.ylabel(\"Number of samples\")\nnumEachClass.plot.bar()\nnumPositives = numEachClass.tolist()[1]\nnumNegatives = numEachClass.tolist()[0]\ntotalNumSamples = len(data)\nprint(\"Total: \" + repr(totalNumSamples))\nprint('Positive samples: ' + repr(numPositives) + \"\\taccount for: \" + repr(numPositives\/totalNumSamples*100) + \"%\")\nprint(\"Negative samples: \" + repr(numNegatives) + \"\\taccount for: \" + repr(numNegatives\/totalNumSamples*100) + \"%\")","5469ab30":"plt.figure(figsize=(15,5))\nplt.subplot(121)\nAmntNotFraud = data[data.Class == 0].Amount\nplt.title('Not Fraud, Mean= %0.2f' %AmntNotFraud.mean())\nplt.xlabel('Amount')\nAmntNotFraud.plot.hist()\nplt.subplot(122)\nAmntFraud = data[data.Class == 1].Amount\nplt.title('Fraud, Mean= %0.2f' %AmntFraud.mean())\nplt.xlabel('Amount')\nAmntFraud.plot.hist()","891ea091":"plt.figure(figsize=(15,5))\nplt.subplot(121)\nplt.title('Not Fraud, Amount <= 4000')\nplt.xlabel('Amount')\ndata[(data.Class == 0) & (data.Amount <= 4000)].Amount.plot.hist()\nplt.subplot(122)\nplt.title('Fraud, Amount <= 4000')\nplt.xlabel('Amount')\ndata[(data.Class == 1) & (data.Amount <= 4000)].Amount.plot.hist()","89a9bdb6":"import seaborn as sns\n\nplt.figure(figsize=(20,10))\nplt.title('Correlation values between features')\nsns.heatmap(data.corr(), fmt='.1f', annot=True)","6a17dfd2":"timeCol = data.loc[:,['Time','Class']]\ntimeCol.Time \/= 3600\ntimeCol.Time = timeCol.Time.astype(int)\ntimeCol.Time = timeCol.Time%24\n\ntimeNegative = timeCol.loc[timeCol.Class == 0,:].groupby('Time')['Class'].count()\ntimePositive = timeCol.loc[timeCol.Class == 1,:].groupby('Time')['Class'].count()\n#Plotting\nfig = plt.figure(figsize=(20,8))\nsbplt1 = plt.subplot(121)\nsbplt1.set_ylabel('No. of transactions')\nsbplt1.set_title('Not fraud')\ntimeNegative.plot.bar()\nsbplt2 = plt.subplot(122)\nsbplt2.set_ylabel('No. of transactions')\nsbplt2.set_title('Fraud')\ntimePositive.plot.bar()\n","0d111735":"data.iloc[:,1:30] = StandardScaler().fit_transform(data.iloc[:,1:30])\ndata_incl_time = data.copy()\ndata = data.drop(['Time'],axis=1)\n#data.describe()\ndata_incl_time.Time \/= 3600\ndata_incl_time.Time = data_incl_time.Time.astype(int)\ndata_incl_time.Time = data_incl_time.Time %24\n#data_incl_time.describe()","99cf7f68":"# Option 1: Dataset includes 'Time' feature\nX = data_incl_time.loc[:,data_incl_time.columns != 'Class']\ny = data_incl_time.loc[:,data_incl_time.columns == 'Class']\n# Option 2: Dataset excludes 'Time' feature\n\"\"\" \nX = data.loc[:,data.columns != 'Class']\ny = data.loc[:,data.columns == 'Class']\n\"\"\"\n# Prepare train set, test set\n    # Without Stratification\ntrainValidX, testX, trainValidy, testy = train_test_split(X, y, test_size = 0.2, random_state = 48)\n\n    # With Stratification\nsTrainValidX, sTestX, sTrainValidy, sTesty = train_test_split(X, y, test_size = 0.2, random_state = 48, stratify=y)\n\n    #Simple Stratified Holdout CV\nsTrainX, sValidX, sTrainy, sValidy = train_test_split(sTrainValidX, sTrainValidy, test_size = 0.25, random_state = 48, stratify=sTrainValidy)","0d15a625":"print('Train set: ~80%\\tTest set: ~20%')\ndsbTrain = trainValidy['Class'].value_counts()\ndsbTest = testy['Class'].value_counts()\nprcTrain = dsbTrain[1]\/numPositives\nprcTest = dsbTest[1]\/numPositives\nprint('Without stratification:\\n\\t+ Train set: Negative= ' + repr(dsbTrain[0]) + '\\tPositive= ' + repr(dsbTrain[1]) +' ~%0.3f'%prcTrain)\nprint('\\t+ Test set: Negative= ' + repr(dsbTest[0]) + '\\tPositive= ' + repr(dsbTest[1]) + ' ~%0.3f'%prcTest)\nprint('==============================')\ndsbTrain = sTrainValidy['Class'].value_counts()\ndsbTest = sTesty['Class'].value_counts()\nprcTrain = dsbTrain[1]\/numPositives\nprcTest = dsbTest[1]\/numPositives\nprint('With stratification:\\n\\t+ Train set: Negative= ' + repr(dsbTrain[0]) + '\\tPositive= ' + repr(dsbTrain[1]) +' ~%0.3f'%prcTrain)\nprint('\\t+ Test set: Negative= ' + repr(dsbTest[0]) + '\\tPositive= ' + repr(dsbTest[1]) + ' ~%0.3f'%prcTest)","f9292332":"from sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, f1_score, precision_score, recall_score, classification_report, confusion_matrix, mean_absolute_error, roc_curve, auc, average_precision_score\n\ndef evaluate(param, weight, trainX, trainy, testX, testy):\n    print(\"======================================================\")\n    print(\"\\nRegularization Parameter = \", param, \"\\n\")\n    logRegr = LogisticRegression(C = param, penalty = 'l1', class_weight = {0:1\/(1+weight), 1:weight\/(1+weight)})\n    logRegr.fit(trainX, trainy.values.ravel())\n    \n    prediction = logRegr.predict(testX.values)\n\n    print(\"MAE:\", mean_absolute_error(testy, prediction))\n    print(\"Accuracy score: \", accuracy_score(testy, prediction, normalize = False), \"\/\", len(validy))\n    print(\"F1 score: \", f1_score(testy, prediction))\n    print(\"Precision score: \", precision_score(testy, prediction))\n    print(\"Recall score: \", recall_score(testy, prediction))\n    print(\"AUPRC: \", average_precision_score(testy.values.ravel(), logRegr.predict_proba(testX.values)))\n    \n    return prediction","becdab93":"print(\"Evaluating on test set. Regularizing param = \", 0.3)\nlogRegr = LogisticRegression(C = 0.3, penalty = 'l1')\nlogRegr.fit(trainValidX, trainValidy.values.ravel())\nprediction = logRegr.predict(testX.values)\nprint(\"Recall score: \", recall_score(testy, prediction))","91b2bc73":"def plot_confusion_matrix(cm, classes,\n                          normalize=False,\n                          title='Confusion matrix',\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    print(cm)\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, rotation=45)\n    plt.yticks(tick_marks, classes)\n\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.tight_layout()\n    plt.ylabel('True label')\n    plt.xlabel('Predicted label')","a4c6829c":"def draw_cnf_matrix(testy, prediction):\n    # Compute confusion matrix\n    cnf_matrix = confusion_matrix(testy, prediction)\n    np.set_printoptions(precision=2)\n\n    # Plot non-normalized confusion matrix\n    #plt.figure()\n    plot_confusion_matrix(cnf_matrix, classes=[0,1], title='Confusion matrix, without normalization')\n\ndraw_cnf_matrix(testy, prediction)","d216c1a3":"prediction_proba = logRegr.predict_proba(testX.values)[:,1]\n\ndef draw_roc_curve(testy, prediction_proba):\n    fpr, tpr, thresholds = roc_curve(testy.values.ravel(), prediction_proba, pos_label = 1)\n\n    #plt.figure()\n    plt.plot(fpr, tpr, color='darkorange',\n             lw=2, label='ROC curve (area = %0.2f)' % auc(fpr, tpr))\n    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n    plt.xlim([0.0, 1.0])\n    plt.ylim([0.0, 1.0])\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate')\n    plt.title('Receiver operating characteristic example')\n    plt.legend(loc=\"lower right\")\n    plt.show()\n\ndraw_roc_curve(testy, prediction_proba)","8f6d3e0a":"from sklearn.metrics import precision_recall_curve, precision_recall_fscore_support\n\ndef draw_prc(testy, prediction_proba):\n    precision, recall, threshold = precision_recall_curve(testy.values.ravel(), prediction_proba)\n    ap = average_precision_score(testy.values.ravel(), prediction_proba)\n    threshold = np.append(threshold, 1)\n    plt.step(recall, precision, color='r', where='post', label = 'AUPRC = %0.2f' %ap)\n    plt.title('Precision Recall Curve')\n    plt.legend(loc='lower left')\n    \ndraw_prc(testy, prediction_proba)","679fdb77":"from sklearn.model_selection import StratifiedKFold\ndef select_skfold(X, y):\n    \n    print('Stratified k-fold cross validation:')\n    reg_params = [0.03, 0.3, 3.0, 30, 300]\n    print('Regularizing parameter range:\\n', reg_params)\n    \n    folds_num = 4\n    folds = StratifiedKFold(n_splits = folds_num)\n    print('Num of folds = ', folds_num)\n    \n    best_recall = 0;\n    best_param = 0;\n    \n    print('Looping...')\n    for param in reg_params:\n        print('Param = ', param)\n        logRegr = LogisticRegression(C = param, penalty = 'l1')\n        avg_recall = 0\n        \n        for train, test in folds.split(X, y):\n            trainX, testX = X.iloc[train,:], X.iloc[test,:]\n            trainy, testy = y.iloc[train,:], y.iloc[test,:]\n            logRegr.fit(trainX, trainy.values.ravel())\n            kfold_predict = logRegr.predict(testX.values)\n            rec = recall_score(testy.values, kfold_predict)\n            avg_recall += rec\n        \n        avg_recall \/= folds_num\n        print('Average recall score = ', avg_recall)\n        \n        if rec > best_recall:\n            best_recall = rec\n            best_param = param\n    \n    return best_param\n\ndef draw_prthreshold(test, p, weight = 1):\n    ap = average_precision_score(test.values.ravel(), p)\n    precision, recall, t = precision_recall_curve(test.values.ravel(), p)\n    t = np.append(t, 1)\n    plt.title('N\/P class weight: 1:'+ repr(weight) + ' ; AUPRC = {0:0.6f}'.format(ap))\n    plt.plot(t, recall, color='r')\n    plt.plot(t, precision, color='b')\n\n    plt.xlabel('Threshold')\n    plt.legend(('recall','precision'))","235eea83":"from itertools import cycle\n\nbest_param = select_skfold(trainValidX, trainValidy)\nweight_range = [1,2,5,10,20,50,100,200,300,400,500]\ncolors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal', 'red', 'yellow', 'green', 'blue','black', 'purple'])\nplt.figure(1, figsize=(20,8))\nj = 1\nfor weight, color in zip(weight_range, colors):\n    print('===============================================')\n    print('Positive class weight = ', weight)\n    logRegr = LogisticRegression(C = best_param, penalty = 'l1', class_weight = {0:1\/(1+weight), 1:weight\/(1+weight)})\n    logRegr.fit(trainValidX, trainValidy.values.ravel())\n    prediction = logRegr.predict(testX.values)\n    \n    print(\"F1 score: \", f1_score(testy, prediction), \"\\tPrecision score: \", precision_score(testy, prediction), \"\\tRecall score: \", recall_score(testy, prediction))\n    proba = logRegr.predict_proba(testX.values)[:,1]\n    ap = average_precision_score(testy.values.ravel(), proba)\n    print(\"Accuracy score: \", accuracy_score(testy, prediction, normalize = True), '\\tAUPRC: ', ap)\n    \n    plt.figure(2, figsize=(28,15))\n    plt.subplot(3,4,j)\n    j += 1\n    draw_prthreshold(testy, proba)\n    \n    plt.figure(1)\n    plt.plot(recall, precision, color=color, label='PosClassWeight= %s'%weight)\n    plt.legend(loc=\"lower left\")","63dd18ac":"from sklearn.ensemble import RandomForestClassifier\n\nweight_range = [1,2,5,10,20,50,100,200,300,400,500]\ncolors = cycle(['navy', 'turquoise', 'darkorange', 'cornflowerblue', 'teal', 'red', 'yellow', 'green', 'blue','black', 'purple'])\nplt.figure(0, figsize=(20,8))\nj = 1\nfor weight, color in zip(weight_range, colors):\n    print('===============================================')\n    print('Positive class weight = ', weight)\n    ranFor = RandomForestClassifier(class_weight = {0:1\/(1+weight), 1:weight\/(1+weight)})\n    ranFor.fit(trainValidX, trainValidy.values.ravel())\n    prediction = ranFor.predict(testX.values)\n    \n    print(\"F1 score: \", f1_score(testy, prediction), \"\\tPrecision score: \", precision_score(testy, prediction), \"\\tRecall score: \", recall_score(testy, prediction))\n    proba = ranFor.predict_proba(testX.values)[:,1]\n    precision, recall, threshold = precision_recall_curve(testy.values.ravel(), proba)\n    threshold = np.append(threshold, 1)\n    ap = average_precision_score(testy.values.ravel(), proba)\n    print(\"Accuracy score: \", accuracy_score(testy, prediction, normalize = True), '\\tAUPRC: ', ap)\n    \n    plt.figure(1, figsize=(28,15))\n    plt.subplot(3,4,j)\n    j += 1\n    plt.title('N\/P class weight: 1:'+ repr(weight) + ' ; AUPRC = {0:0.6f}'.format(ap))\n    plt.plot(threshold, recall, color='r')\n    plt.plot(threshold, precision, color='b')\n\n    plt.xlabel('Threshold')\n    plt.legend(('recall','precision'))\n    \n    plt.figure(0)\n    plt.plot(recall, precision, color=color, label='PosClassWeight= %s'%weight)\n    plt.legend(loc=\"lower left\")","7e2a9dd9":"from sklearn.cluster import KMeans\n\nkmclstr = KMeans(n_clusters = 10, random_state = 48)\nkmclstr.fit(trainValidX)\ncentroids_pos = kmclstr.cluster_centers_\ncentroids_pred = kmclstr.predict(testX)\ndistances = [np.linalg.norm(a - b) for a,b in zip(testX.as_matrix(), centroids_pos[centroids_pred])]\nprint(\"Done\")","86f035d4":"from sklearn.metrics import roc_auc_score\n\nprediction = np.array(distances)\nthreshold = 98\nthreshold_step = 0.05\nthreshold_end = 99.9\nwhile threshold <= threshold_end:\n    print(\"=======================================\")\n    print(threshold, \"-th percentile\")\n    proba = prediction_proba.copy()\n    #print(proba)\n    print(proba.max())\n    proba[distances >= np.percentile(distances, threshold)] = 1\n    proba[distances < np.percentile(distances, threshold)] = 0\n    #print(proba)\n    print(\"F1 score: \", f1_score(testy, proba), \"\\tPrecision score: \", precision_score(testy, proba), \"\\tRecall score: \", recall_score(testy, proba))\n    print(\"Accuracy score: \", accuracy_score(testy, proba, normalize = True), \"\\tAUPRC score: \", average_precision_score(testy, prediction_proba))\n    print(roc_auc_score(testy, prediction_proba))\n    threshold += threshold_step","0b1a2146":"from sklearn.ensemble import IsolationForest\n\nisoFor = IsolationForest(contamination = 0.01, random_state = 48)\nisoFor.fit(sTrainValidX)\nisoFor_predict = isoFor.predict(sTestX)","3b9b51c4":"isoFor_decision = isoFor.decision_function(sTestX)\nisoFor_decision = MinMaxScaler().fit_transform(isoFor_decision.reshape(-1,1))\nisoFor_decision = 1 - isoFor_decision\n\nisoFor_predict[isoFor_predict == 1] = 0\nisoFor_predict[isoFor_predict == -1] = 1","1dcfeddf":"print(\"F1 score: \", f1_score(sTesty, isoFor_predict), \"\\tPrecision score: \", precision_score(sTesty, isoFor_predict), \"\\tRecall score: \", recall_score(sTesty, isoFor_predict))\nprint(\"Accuracy score: \", accuracy_score(sTesty, isoFor_predict, normalize = True))#, \"\\tAUPRC score: \", average_precision_score(testy, proba))\nprint(\"AUROC score\", roc_auc_score(sTesty, isoFor_decision))\n\ncnf_matrix = confusion_matrix(sTesty, isoFor_predict)\nplt.figure()\nplot_confusion_matrix(cnf_matrix, classes=[0,1],\n                      title='Confusion matrix, without normalization')","26f78acf":"## **Optimizing Part:**\n+ Using K-fold CV to choose the best regularization parameter in a specific range of values\n    Number of folds = 4, Criteria: Recall score","fa545cae":"# **UNSUPERVISED LEARNING**","d500f49f":"**Comments:**\n* Legal transactions are often conducted at 9h -> 22h\n* Fraud transactions are more random, with 2 peaks at 2h and 11h\n* Perhaps, using this type of 'Time' feature gives more value than original 'Time' feature","a8f087d9":"#### **Check the class distribution on train\/test sets (optional)**","ac946ed0":"#### Grab some infomation about the data and features (optional)","0a5246c9":"### **Try Random Forest**","0c1846d1":"#### **Illustrate the correlation between features (optional)**","6db6d111":"#### **Plot the distribution of the feature 'Amount' (optional)**","e64d3447":"## **Import packages and data**","0e010bbe":"* Find out the model with the best Negative\/Positive Sample Weight Rate by calculating AUPRC. Also show other evaluating scores (MAE, F1score, Accuracy, Precision, Recall)\n* Plot PRC\n* Plot Precision-Recall graphs","ecc3cce7":"## **Separate train\/test set:**\n* Apply standardization to all features except 'Time' and 'Class'\n* 'data' - dataset without feature 'Time'\n* 'data_incl_time' - dataset with transformed feature 'Time'","3562e109":"#### **Explore feature 'Time' - transform the time offset into the hours of day that transactions happened (optional)**","f6534340":"**Dataset options:**\n+ OPTION 1: Train model by data including 'Time'\n+ OPTION 2: Train model by data excluding 'Time'\n\n**Train\/test split options:**\n+ Random train\/test split + Stratified K-fold CV (4 folds)\n+ Stratified train\/test split + Stratified K-fold CV (4 folds)\n+ Stratified train\/test split + Stratified Holdout CV (60-20-20)","224d060e":"**Comments:**\n* No pair of feature having significant dependance","d80e8989":"### **The best result: AUPRC = 0.829 while:**\n+ Random train\/test split, K-fold CV\n+ Using transformed 'Time' feature\n+ Logistic Regression: threshold = 'default', N\/P Weight Rate = 1:2","ee29ca55":"**Comments:**\n* Feature 'Time' values vary from 0->172792, data collected in 2 days => Feature 'Time' is measured in 'seconds'. The real time of the 1st transaction is unknown => Assume it's 0h00\n* All features haven't been standardized, have type 'Float64'. 'Class' has type 'Int64' and has 2 unique values (0, 1)\n* No loss in data\n* The dataset is deeply imbalanced, with Positive\/Negative rate = 492\/284315 (~1\/578)","ff33a836":"## **Try a few basic operations on a specific model: **\n+ Logistic Regression: RegParam = 0.3, DefaultThreshold = 0.5, ClassWeight = none\n+ Try: fit\/predict, plot confusion matrix, ROC\/AUROC, PRC\/AUPRC","31c10f6f":"**Best result: AUPRC = 0.8611**","b89fbf45":"**Comments:**\n* Similar\n* MeanNotFraud < MeanFraud but MaxNotFraud > MaxFraud\n* No pattern"}}