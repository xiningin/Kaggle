{"cell_type":{"f1607cc6":"code","16d97b1b":"code","47021e88":"code","46ceb633":"code","205a20d0":"code","3513eba9":"code","3ebc1b1a":"code","9fd5ee58":"code","ab5e81a9":"code","2a4236cb":"code","26e82a58":"code","18a3e070":"code","f73213a4":"code","e0f3736b":"code","2c35696d":"code","e1e8040e":"code","a9aa4051":"code","ae677280":"code","ab010a85":"code","d4198a51":"code","ccbded8e":"code","8bcbd941":"code","de55c7c5":"code","d78c9766":"code","bd81991d":"code","b2cd4a09":"code","7daee027":"code","3d0d3e8e":"code","7dfd8c6b":"code","87433eb1":"code","e6072f42":"code","4e0f4b4f":"code","cddab436":"code","3ba736f9":"code","c245ddef":"code","bf26d7d1":"markdown","13a111da":"markdown","fd15b23b":"markdown","70048cea":"markdown","38e81437":"markdown","18c53c1c":"markdown","e4f77577":"markdown","e80394cd":"markdown","d55b32af":"markdown","491b1df9":"markdown","b81ab5e6":"markdown","ccace085":"markdown","a668065c":"markdown","818c0b55":"markdown","f1296021":"markdown","586a84f9":"markdown","94e90a52":"markdown","19e3b3a1":"markdown","29a9bfa0":"markdown","333df34f":"markdown","f4baeb2c":"markdown","99f7a583":"markdown","d10391e0":"markdown","f9fcac1d":"markdown"},"source":{"f1607cc6":"!pip install scikit-learn  -U","16d97b1b":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom tensorflow.keras.layers import *\nfrom tensorflow.keras.models import *\nfrom tensorflow.keras.optimizers import *\nfrom tensorflow.keras.callbacks import LearningRateScheduler\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom xgboost import XGBClassifier\nfrom sklearn.svm import SVC\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.under_sampling import RandomUnderSampler\nfrom tensorflow.keras.wrappers.scikit_learn import KerasClassifier\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.utils import shuffle\nfrom IPython.display import clear_output\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.decomposition import PCA\nimport plotly.express as px\n\n\nsns.set_theme()","47021e88":"df = pd.read_csv('..\/input\/creditcardfraud\/creditcard.csv')\n\ndf.head()","46ceb633":"print(\"Total number of cells with missing data: \\n\\n\" ,df.isnull().sum().sum())","205a20d0":"df.describe()","3513eba9":"fig, axes = plt.subplots(8, 4, figsize=(35, 25))\nfig.suptitle('Different feature distributions')\n\naxes = axes.reshape(32,)\n\nfor i,column in enumerate(df.columns):\n    kde_status = False\n    axes[i].set_yscale('log')\n    \n    if column == \"Time\" or column == \"Amount\":\n        kde_status = True\n        \n    sns.histplot(ax = axes[i],data = df, x= column,kde=kde_status)","3ebc1b1a":"plot = sns.histplot(data = df , x = \"Class\")\nplot.set(yscale=\"log\")\nprint()","9fd5ee58":"plt.figure(figsize=(12,8))\nax = plt.axes()\n\nsns.heatmap(df.corr(),axes = ax)","ab5e81a9":"# This helper function was taken from https:\/\/stackoverflow.com\/a\/26883880 \n# and modified for our task. Special thanks to watsonic !\n\n\n\ndef topn(df,n):\n    npa = df.values\n    \n    npa = np.tril(npa, -1)\n    topn_ind = np.argpartition(npa,-n,None)[-n:] #flatend ind, unsorted\n    topn_ind = topn_ind[np.argsort(npa.flat[topn_ind])][::-1] #arg sort in descending order\n    cols,indx = np.unravel_index(topn_ind,npa.shape,'F') #unflatten, using column-major ordering\n    \n    return ([df.columns[c] for c in cols],[df.index[i] for i in indx])","2a4236cb":"max_corr_x , max_corr_y = topn(df.corr(),4)\n\nfig, axes = plt.subplots(2, 2, figsize=(25, 15))\nfig.suptitle('Relations')\n\naxes = axes.reshape(4,)\n\nfor i in range(len(max_corr_x)):\n    sns.scatterplot(ax = axes[i],data = df, x= max_corr_x[i],y=max_corr_y[i])","26e82a58":"fig, axes = plt.subplots(6, 5, figsize=(35, 25))\nfig.suptitle('Different feature distributions')\n\naxes = axes.reshape(30,)\n\nfor i,column in enumerate(df.columns[:-1]):\n    axes[i].set_yscale('symlog')\n    sns.violinplot(ax = axes[i],x=\"Class\", y=column,data=df)","18a3e070":"fig, axes = plt.subplots(6, 5, figsize=(35, 25))\nfig.suptitle('Different feature distributions')\n\naxes = axes.reshape(30,)\n\nfor i,column in enumerate(df.columns[:-1]):\n    axes[i].set_yscale('symlog')\n    sns.boxplot(ax = axes[i],x=\"Class\", y=column,data=df)","f73213a4":"transformer = RobustScaler()\n\ndf['Amount'] = transformer.fit_transform(df['Amount'].values.reshape(-1,1))\ndf['Time']   = transformer.fit_transform(df['Time'].values.reshape(-1,1))","e0f3736b":"X = np.array(df.drop(columns = ['Class']))\ny = np.array(df['Class'])","2c35696d":"X , y = shuffle(X,y,random_state = 0)","e1e8040e":"over_X , over_y = SMOTE().fit_resample(X, y)\nover_X , over_y = shuffle(over_X,over_y,random_state = 0)","a9aa4051":"under_X , under_y = RandomUnderSampler(random_state=0).fit_resample(X,y)\nunder_X , under_y = shuffle(under_X,under_y,random_state = 0)","ae677280":"model1 = LogisticRegression(solver='liblinear')\nmodel2 = DecisionTreeClassifier()\nmodel3 = SVC()","ab010a85":"def build_classifier():\n    model = Sequential()\n\n    model.add(Dense(units = 32,kernel_initializer=\"uniform\",activation=\"relu\",input_shape = (30,)))\n    model.add(Dense(units = 64,kernel_initializer=\"uniform\",activation=\"relu\"))\n    model.add(Dropout(0.5))\n    model.add(Dense(units = 32,kernel_initializer=\"uniform\",activation=\"relu\"))\n    model.add(Dense(units = 1,kernel_initializer=\"uniform\",activation=\"sigmoid\"))\n    \n    model.compile(optimizer = \"adam\",loss=\"binary_crossentropy\",metrics=['Recall'])\n\n    return model","d4198a51":"classifier = KerasClassifier(build_fn=build_classifier,batch_size = 256,epochs = 10)","ccbded8e":"scores = cross_val_score(estimator=model1,X=over_X,y=over_y,cv=5,scoring=\"f1\")\nprint(\"Logistic Regression cross validation score : \" , np.round(scores.mean()*100,3),\"%\")\nscores = cross_val_score(estimator=model2,X=over_X,y=over_y,cv=5,scoring=\"f1\")\nprint(\"Decision Tree cross validation score : \" , np.round(scores.mean()*100,3),\"%\")","8bcbd941":"scores = cross_val_score(estimator=classifier,X=over_X,y=over_y,cv=5,scoring=\"f1\",n_jobs = 1)\n\nclear_output()","de55c7c5":"print(\"ANN cross validation score : \" , np.round(scores.mean()*100,3),\"%\")\n# ANN - Artificial Neural Networks","d78c9766":"scores = cross_val_score(estimator=model1,X=under_X,y=under_y,cv=5,scoring=\"f1\")\nprint(\"Logistic Regression cross validation score : \" , np.round(scores.mean()*100,3),\"%\")\nscores = cross_val_score(estimator=model2,X=under_X,y=under_y,cv=5,scoring=\"f1\")\nprint(\"Decision Tree cross validation score : \" , np.round(scores.mean()*100,3),\"%\")\nscores = cross_val_score(estimator=RandomForestClassifier(max_depth=35),X=under_X,y=under_y,cv=5,scoring=\"f1\")\nprint(\"Random Forest cross validation score : \" , np.round(scores.mean()*100,3),\"%\")\nscores = cross_val_score(estimator=model3,X=under_X,y=under_y,cv=5,scoring=\"f1\")\nprint(\"SVC cross validation score : \" , np.round(scores.mean()*100,3),\"%\")","bd81991d":"classifier = KerasClassifier(build_fn=build_classifier,batch_size = 1,epochs = 20)\nscores = cross_val_score(estimator=classifier,X=under_X,y=under_y,cv=5,scoring=\"f1\")\n\nclear_output()","b2cd4a09":"print(\"ANN cross validation score : \" , np.round(scores.mean()*100,3),\"%\")","7daee027":"xtr, xte, ytr, yte = train_test_split(X, y, test_size = 0.25 , shuffle = True)","3d0d3e8e":"model1.fit(xtr,ytr)\n\n\npredictions = model1.predict(xte)\nconf_matrix = metrics.confusion_matrix(yte, predictions)\n\nplt.figure(figsize=(8,8))\nax = plt.axes()\nsns.heatmap(conf_matrix,annot=True,fmt='g',cbar=False,axes=ax,cmap =\"YlGnBu\",linewidths=1,linecolor='black')\nax.set_title('Logistic Regression Confusion Matrix')\n\nplt.show()","7dfd8c6b":"plt.figure(figsize=(20,8))\nax = plt.axes()\nax.set_title('Logistic Regression Feature Importance')\n\nplot = sns.barplot(x = df.columns[:-1] , y =np.abs((np.std(xtr, 0) *  model1.coef_).reshape(30,)),axes = ax)","87433eb1":"model2.fit(xtr,ytr)\n\n\npredictions = model2.predict(xte)\nconf_matrix = metrics.confusion_matrix(yte, predictions)\n\nplt.figure(figsize=(8,8))\nax = plt.axes()\nsns.heatmap(conf_matrix,annot=True,fmt='g',cbar=False,axes=ax,cmap =\"YlGnBu\",linewidths=1,linecolor='black')\nax.set_title('Decision Tree Cassifier Confusion Matrix')\n\nplt.show()","e6072f42":"plt.figure(figsize=(20,8))\nax = plt.axes()\nax.set_title('Decision Tree Feature Importance')\n\nplot = sns.barplot(x = df.columns[:-1] , y = model2.feature_importances_,axes = ax)","4e0f4b4f":"model4 = RandomForestClassifier(max_depth = 35)\nmodel4.fit(xtr,ytr)\n\n\npredictions = model4.predict(xte)\nconf_matrix = metrics.confusion_matrix(yte, predictions)\n\nplt.figure(figsize=(8,8))\nax = plt.axes()\nsns.heatmap(conf_matrix,annot=True,fmt='g',cbar=False,axes=ax,cmap =\"YlGnBu\",linewidths=1,linecolor='black')\nax.set_title('Random Forest Classifier Confusion Matrix')\n\nplt.show()","cddab436":"plt.figure(figsize=(20,8))\nax = plt.axes()\nax.set_title('Random Forest Feature Importance')\n\nplot = sns.barplot(x = df.columns[:-1] , y = model4.feature_importances_,axes = ax)","3ba736f9":"model3.fit(xtr,ytr)\n\n\npredictions = model3.predict(xte)\nconf_matrix = metrics.confusion_matrix(yte, predictions)\n\nplt.figure(figsize=(8,8))\nax = plt.axes()\nsns.heatmap(conf_matrix,annot=True,fmt='g',cmap =\"YlGnBu\",cbar=False,axes=ax,linewidths=1,linecolor='black')\nax.set_title('Support Vector Classifier Confusion Matrix')\n\nplt.show()","c245ddef":"model5 = build_classifier()\n\nhistory = model5.fit(xtr,ytr,epochs=20,batch_size=256,shuffle=True)\n\nclear_output()\n\npredictions = (model5.predict(xte) > 0.5).astype('int32').reshape(xte.shape[0],)\nconf_matrix = metrics.confusion_matrix(yte, predictions)\n\nplt.figure(figsize=(8,8))\nax = plt.axes()\nsns.heatmap(conf_matrix,annot=True,fmt='g',cbar=False,axes=ax,cmap =\"YlGnBu\",linewidths=1,linecolor='black')\nax.set_title('ANN Confusion Matrix')\n\nplt.show()","bf26d7d1":"# Building models","13a111da":"### No sampling , train\/test split","fd15b23b":"# Cleaning data","70048cea":"## Dealing with imbalanced data","38e81437":"<p style=\"font-size: 120%\"> Let's have a closer look to Class distribution <\/p>","18c53c1c":"As we can see, mean value for transactions amount is approx. 88 dollars.","e4f77577":"<p style=\"font-size: 120%\"> As we can see, our data is heavily imbalanced with respect to target variable - Class. In further data preparation step we have to consider it and perform several techniques to resolve this issue. <\/p>","e80394cd":"## Visualizations","d55b32af":"### Undersampled data","491b1df9":"### Violinplots","b81ab5e6":"# Testing models","ccace085":"### Correlations","a668065c":"# A bit of analysis","818c0b55":"### Oversampling: SMOTE","f1296021":"### Undersampling: Random Undersampler","586a84f9":"## First look at the data","94e90a52":"## Scaling Time and Amount","19e3b3a1":"## Cross validation on ...","29a9bfa0":"### Distributions","333df34f":"### Boxplots","f4baeb2c":"# Reading data","99f7a583":"<p style=\"font-size: 120%\"> Let's extract top 4 correlations and plot them <\/p>","d10391e0":"### Oversampled data","f9fcac1d":"# Preparing data for models"}}