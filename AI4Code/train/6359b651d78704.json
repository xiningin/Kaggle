{"cell_type":{"d9d4e4cc":"code","b6f01663":"code","b2ae99d3":"code","c9c4c49f":"code","fd7ef269":"code","a131f761":"code","bdd69ee2":"code","9a79a792":"code","37d8f133":"code","7bbb8e5f":"code","46ceeb84":"code","f4296a77":"code","9ff0640f":"code","1669dd81":"code","fb5ebc62":"code","adcf5d03":"code","1b935683":"code","81ee623c":"code","3b1c7f8a":"code","9055799f":"code","7f9c7100":"code","8abd983c":"code","82262f25":"code","290bdaea":"code","3ad186ac":"code","3dc52499":"markdown","5375e68d":"markdown","99702af8":"markdown","205e5b7a":"markdown","e7c7ffb9":"markdown","7b6e3b7a":"markdown","2ddf7923":"markdown","caef3c28":"markdown"},"source":{"d9d4e4cc":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","b6f01663":"#\u30c7\u30fc\u30bf\u30a4\u30f3\u30dd\u30fc\u30c8\n#import dataset\nimport pandas as pd\nimport numpy as np\n\nimport optuna\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n \ntrain = pd.read_csv('..\/input\/digit-recognizer\/train.csv') \ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv')\nprint('The size of the train data:' + str(train.shape))\nprint('The size of the test data:' + str(test.shape))","b2ae99d3":"#\u3000\u6559\u5e2b\u30c7\u30fc\u30bf\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u985e\u3057\u3066\u3001float\u578b\u306b\u5909\u63db\u3059\u308b\u3002\n# transform float type\nX_train = (train.iloc[:,1:].values).astype('float32') \ny_train = train.iloc[:,0].values\nX_test = test.values.astype('float32')","c9c4c49f":"#\u7cbe\u5ea6\u78ba\u8a8d\u306e\u305f\u3081\u3001\u6559\u5e2b\u30c7\u30fc\u30bf\u3092\u3055\u3089\u306b\u6559\u5e2b\u30c7\u30fc\u30bf\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u5272\n#train_test_split again\nfrom sklearn.model_selection import train_test_split\n \nX_train2, X_test2, y_train2, y_test2 = train_test_split(X_train,\n                                                        y_train, \n                                                        test_size = 0.2,\n                                                        train_size = 0.8,#\u6559\u5e2b\u30c7\u30fc\u30bf\u5c11\u306a\u304f\u306a\u308b\u306e\u304c\u6016\u3044\u306e\u30674:1\u3067\u5206\u5272\n                                                        stratify = y_train)\nprint(f'X_train2 \u306e\u9577\u3055: {len(X_train2)}')\nprint(f'X_test2 \u306e\u9577\u3055: {len(X_test2)}')","fd7ef269":"#reshape data \nimg_rows, img_cols = 28, 28\nnum_classes = 10\n\n#\u672c\u756a\u7528\u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406\uff0828\u00d728\u306e\u884c\u5217\u306b\u5909\u63db\uff09\nX_train = X_train2.reshape(X_train2.shape[0], img_rows, img_cols, 1)\nX_test = X_test2.reshape(X_test2.shape[0], img_rows, img_cols, 1)\n\n#y_train\u306e\u30c7\u30fc\u30bf\u3092to_categorical\u30672\u5024\u30af\u30e9\u30b9\u306e\u884c\u5217\u3078\u5909\u63db\ny_train= keras.utils.to_categorical(y_train2, num_classes)","a131f761":"X_train.shape","bdd69ee2":"import optuna\nimport keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\n\nimg_rows, img_cols = 28, 28\nnum_classes = 10\n\n#CNN\u30e2\u30c7\u30eb\u306e\u5b9a\u7fa9\n#define the CNN model\ndef create_model(num_layer, mid_units, num_filters,dropout_rate):\n    \n    model = Sequential()\n    model.add(Conv2D(filters=num_filters[0], kernel_size=(3, 3),\n                 activation=\"relu\",\n                 input_shape=(img_rows, img_cols, 1)))\n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    for i in range(1,num_layer):\n        model.add(Conv2D(filters=num_filters[i], kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\n    \n    model.add(MaxPooling2D(pool_size=(2, 2)))\n    model.add(Dropout(dropout_rate[0]))\n    model.add(Flatten())\n    model.add(Dense(mid_units))\n    model.add(Dropout(dropout_rate[1]))\n    model.add(Dense(num_classes, activation='softmax'))\n    \n    return model","9a79a792":"def objective(trial):\n    print(\"Optimize Start\")\n    \n    #\u30bb\u30c3\u30b7\u30e7\u30f3\u306e\u30af\u30ea\u30a2\n    #clear_session\n    keras.backend.clear_session()\n    \n    \n    #\u7573\u8fbc\u307f\u5c64\u306e\u6570\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\n    #number of the convolution layer\n    num_layer = trial.suggest_int(\"num_layer\", 2, 5)\n    \n    #FC\u5c64\u306e\u30e6\u30cb\u30c3\u30c8\u6570\n    #number of the unit\n    mid_units = int(trial.suggest_discrete_uniform(\"mid_units\", 100, 300, 100))\n    \n    #\u5404\u7573\u8fbc\u307f\u5c64\u306e\u30d5\u30a3\u30eb\u30bf\u6570\n    #number of the each convolution layer filter\n    num_filters = [int(trial.suggest_discrete_uniform(\"num_filter_\"+str(i), 16, 128, 16)) for i in range(num_layer)]\n    \n    #\u6d3b\u6027\u5316\u95a2\u6570\n    #activation = trial.suggest_categorical(\"activation\", [\"relu\", \"sigmoid\"])\n    \n    #Dropout\u7387\n    #dropout_rate = trial.suggest_uniform('dropout_rate', 0.0, 0.5)\n    #dropout_rate = [int(trial.suggest_uniform(\"dropout_rate\"+str(ii), 0.0, 0.5)) for ii in range(2)]\n    dropout_rate = [0] * 2\n    dropout_rate[0] = trial.suggest_uniform('dropout_rate'+str(0), 0.0, 0.5)\n    dropout_rate[1] = trial.suggest_uniform('dropout_rate'+str(1), 0.0, 0.5)\n    \n    #optimizer\n    optimizer = trial.suggest_categorical(\"optimizer\", [\"sgd\", \"adam\"])\n    \n    model = create_model(num_layer, mid_units, num_filters,dropout_rate)\n    model.compile(optimizer=optimizer,\n          loss=\"categorical_crossentropy\",\n          metrics=[\"acc\"])\n          #metrics=[\"accuracy\"])\n    \n    history = model.fit(X_train, y_train, verbose=0, epochs=20, batch_size=128, validation_split=0.1)\n    \n    scores = model.evaluate(X_train, y_train)\n    print('accuracy={}'.format(*scores))\n    \n    #\u691c\u8a3c\u7528\u30c7\u30fc\u30bf\u306b\u5bfe\u3059\u308b\u6b63\u7b54\u7387\u304c\u6700\u5927\u3068\u306a\u308b\u30cf\u30a4\u30d1\u30fc\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u6c42\u3081\u308b\n    #return 1 - history.history[\"val_acc\"][-1]\n    return 1 - history.history[\"val_acc\"][-1]","37d8f133":"study = optuna.create_study()\nstudy.optimize(objective, n_trials=50)\n","7bbb8e5f":"plt.plot([t.value for t in study.trials])","46ceeb84":"optuna.visualization.plot_contour(study)","f4296a77":"optuna.visualization.plot_param_importances(study)","9ff0640f":"optuna.visualization.plot_contour(study, params=[\"dropout_rate1\",\"num_filter_0\",\"num_layer\",\"dropout_rate0\",\"num_filter_1\"])","1669dd81":"study.best_params","fb5ebc62":"optuna.visualization.plot_optimization_history(study)","adcf5d03":"optuna.visualization.plot_optimization_history(study)","1b935683":"optuna.visualization.plot_intermediate_values(study)","81ee623c":"optuna.visualization.plot_parallel_coordinate(study)","3b1c7f8a":"optuna.visualization.plot_contour(study, params=[\"num_filter_0\", \"num_filter_1\",\"num_filter_2\",\"num_filter_3\",\"num_filter_4\"])","9055799f":"optuna.visualization.plot_slice(study)","7f9c7100":"#\u518d\u5ea6\u30c7\u30fc\u30bf\u30a4\u30f3\u30dd\u30fc\u30c8\n#Import data again\n\nimport pandas as pd\nimport numpy as np\n \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set()\n\n\n\n \ntrain = pd.read_csv('..\/input\/digit-recognizer\/train.csv') #\u6559\u5e2b\u30c7\u30fc\u30bf\ntest = pd.read_csv('..\/input\/digit-recognizer\/test.csv') #\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\nprint('The size of the train data:' + str(train.shape))\nprint('The size of the test data:' + str(test.shape))\n\n#\u3000\u6559\u5e2b\u30c7\u30fc\u30bf\u3001\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306b\u5206\u985e\u3057\u3066\u3001float\u578b\u306b\u5909\u63db\u3059\u308b\u3002\nX_train_best = (train.iloc[:,1:].values).astype('float32') #\u30d4\u30af\u30bb\u30eb\u306e\u5024\u3092float\u306b\u5909\u63db\ny_train_best = train.iloc[:,0].values\nX_test_best = test.values.astype('float32') #\u30d4\u30af\u30bb\u30eb\u306e\u5024\u3092float\u306b\u5909\u63db\n\n#\u30c7\u30fc\u30bf\u524d\u51e6\u7406\nimg_rows, img_cols = 28, 28\nnum_classes = 10\n\n#\u672c\u756a\u7528\u30c7\u30fc\u30bf\u306e\u524d\u51e6\u7406\uff0828\u00d728\u306e\u884c\u5217\u306b\u5909\u63db\uff09\nX_train_best = X_train_best.reshape(X_train_best.shape[0], img_rows, img_cols, 1)\nX_test_best = X_test_best.reshape(X_test_best.shape[0], img_rows, img_cols, 1)\n\n#y_train\u306e\u30c7\u30fc\u30bf\u3092to_categorical\u30672\u5024\u30af\u30e9\u30b9\u306e\u884c\u5217\u3078\u5909\u63db\ny_train_best= keras.utils.to_categorical(y_train_best, num_classes)","8abd983c":"#\u6700\u9069paramter\u306e\u30bb\u30c3\u30c8\n#set the optimized parameter\n\n\n# study.best_params\n# {'num_layer': 5,\n#  'mid_units': 100.0,\n#  'num_filter_0': 128.0,\n#  'num_filter_1': 128.0,\n#  'num_filter_2': 96.0,\n#  'num_filter_3': 48.0,\n#  'num_filter_4': 80.0,\n#  'dropout_rate0': 0.4896812644323757,\n#  'dropout_rate1': 0.10809084505372324,\n#  'optimizer': 'sgd'}\n\nnum_filters = [128,128,96,48,80]\nmid_units= 100\ndropout_rate = [0.4896812644323757,0.10809084505372324]\noptimizer = 'sgd'\n\n#\u6700\u9069\u5316\u5f8c\u306e\u30e2\u30c7\u30eb\n#optimized model\nmodel_best = Sequential()\nmodel_best.add(Conv2D(filters=num_filters[0], kernel_size=(3, 3),activation=\"relu\",input_shape=(img_rows, img_cols, 1)))\nmodel_best.add(Conv2D(filters=num_filters[1], kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_best.add(Conv2D(filters=num_filters[2], kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_best.add(Conv2D(filters=num_filters[3], kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_best.add(Conv2D(filters=num_filters[4], kernel_size=(3,3), padding=\"same\", activation=\"relu\"))\nmodel_best.add(MaxPooling2D(pool_size=(2, 2)))\nmodel_best.add(Dropout(dropout_rate[0]))\nmodel_best.add(Flatten())\nmodel_best.add(Dense(mid_units))\nmodel_best.add(Dropout(dropout_rate[1]))\nmodel_best.add(Dense(num_classes, activation='softmax'))","82262f25":"#\u6700\u9069\u5316\u624b\u6cd5\u306a\u3069\u3092\u6c7a\u5b9a\n#Determine optimization method\nmodel_best.compile(loss=keras.losses.categorical_crossentropy,\n              #optimizer=keras.optimizers.Adadelta(),\n              optimizer=optimizer,\n              metrics=['accuracy'])\n\n#\u5b66\u7fd2\u3092\u958b\u59cb\n#start learning\nhist2 = model_best.fit(X_train_best, y_train_best,\n                 batch_size=128,\n                 epochs=20,\n                 validation_split=0.1,\n                 verbose=1)\n\nscores_best = model_best.evaluate(X_train_best, y_train_best)\n#print('accuracy={}'.format(*scores_best))\n\n","290bdaea":"#loss\nplt.plot(hist2.history['loss'])\nplt.plot(hist2.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()\n\n#Accuracy\nplt.figure()\nplt.plot(hist2.history['accuracy'])\nplt.plot(hist2.history['val_accuracy'])\nplt.title('model accuracy')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper left')\nplt.show()","3ad186ac":"\n#\u30c6\u30b9\u30c8\u30c7\u30fc\u30bf\u306e\u4e88\u6e2c\u7d50\u679c\u3092\u51fa\u529b\uff08predeict_classes)\n#Output prediction results\ny_pred = model_best.predict_classes(X_test_best)\n \n\nmy_cnn = pd.DataFrame()\nimageid = []\n \nfor i in range(len(X_test_best)):\n    imageid.append(i+1)\n    \nmy_cnn['ImageId'] = imageid\nmy_cnn[\"label\"] = y_pred\nmy_cnn.to_csv(\".\/cnn_optuna.csv\", index=False)\n\nprint('csv\u66f8\u304d\u51fa\u3057\u7d42\u4e86')","3dc52499":"# optuna\u30bb\u30c3\u30c8\n# set the optuna\n\n\u4e3b\u306b\u5229\u7528\u3059\u308b\u95a2\u6570\n\n\u30fbsuggest_int \u30fb\u30fb\u30fb int\u578b\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u306e\u6700\u9069\u5316\u3000\u5b9a\u3081\u305f\u7bc4\u56f2\u5185\u306e\u6700\u9069\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u63a2\u7d22\n\n\u30fbsuggest_discrete_uniform\u3000\u30fb\u30fb\u30fb\u3000\u5b9a\u3081\u305f\u7bc4\u56f2\u5185\u3001\u5b9a\u3081\u305f\u9593\u9694\u306e\u6700\u9069\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u63a2\u7d22\n\n\u30fbsuggest_categorical\u3000\u30fb\u30fb\u30fb\u3000category\u578b\u306e\u30d1\u30e9\u30e1\u30fc\u30bf\u6700\u9069\u5316\u3000\u5b9a\u3081\u305f\u30ea\u30b9\u30c8\u306e\u4e2d\u306e\u6700\u9069\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u63a2\u7d22\n\n\u30fbsuggest_uniform\u3000\u30fb\u30fb\u30fb\u3000float\u578b\u3000\u5b9a\u3081\u305f\u7bc4\u56f2\u5185\u306e\u6700\u9069\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u63a2\u7d22\n\n\u30fbcreate_study\n\n\u30fboptimize\n","5375e68d":"# Visalization\uff08\u56f3\u793a\uff09","99702af8":"# \u63d0\u51fa\u7528\u30c7\u30fc\u30bf\u306e\u51fa\u529b\n# Output submission data","205e5b7a":"### \u30d1\u30e9\u30e1\u30fc\u30bf\u591a\u3059\u304e\u3066\u3088\u304f\u308f\u304b\u3089\u3093\u306e\u3067\u805e\u3044\u3066\u3044\u308b\u30d1\u30e9\u30e1\u30fc\u30bf\u3092\u9078\u3073\u307e\u3059\u3002\n","e7c7ffb9":"# \u304a\u307e\u3051","7b6e3b7a":"# MNIST\u3092CNN\uff08Keras\uff09+optuna\u3067\u30d1\u30e9\u30e1\u30fc\u30bf\u6700\u9069\u5316\u3057\u3066\u307f\u308b\u3002\n# optimize the CNN(Keras) paramter using optuna(MNIST data set)\n","2ddf7923":"# Accuracy : 0.99028!","caef3c28":"# \u6700\u9069\u5316\u5f8c\u306e\u30e2\u30c7\u30eb\u4f5c\u6210\n# Create optimized model"}}