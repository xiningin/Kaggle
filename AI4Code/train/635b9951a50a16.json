{"cell_type":{"9e3a9897":"code","c8a44de0":"code","1b831241":"code","f14022c4":"code","13608af8":"code","b8d8f777":"code","38482b19":"code","5568b940":"code","041c2e48":"code","3d96d184":"code","75f8f7e5":"code","f320de15":"code","282d01c5":"code","8e2987b1":"code","13c28b50":"code","3be67c50":"code","728753f0":"code","856ebe2b":"code","2132d64d":"markdown","e5773c25":"markdown","317142bc":"markdown","4d855b13":"markdown","d2f3dd19":"markdown","a4441779":"markdown","222769c7":"markdown","0905afba":"markdown","4174dc31":"markdown","a074f051":"markdown","0aa578fa":"markdown","66e738cb":"markdown","0f4ae255":"markdown","13190f9b":"markdown"},"source":{"9e3a9897":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.metrics import mean_squared_error\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","c8a44de0":"df = pd.read_csv('\/kaggle\/input\/used-car-dataset-ford-and-mercedes\/toyota.csv')\ndf.head()","1b831241":"df.describe()","f14022c4":"df.info()","13608af8":"categ = ['model','year','transmission','fuelType','engineSize']\nstr_categ = ['model','transmission','fuelType']\nnumer = ['price','mileage','tax','mpg']","b8d8f777":"fig2, axes2 = plt.subplots(2, 2)\nfig2.set_figheight(10)\nfig2.set_figwidth(18)\nfor i in range(len(numer)):\n  axes2[int(i\/2),i%2].hist(df[numer[i]], bins=50)\n  axes2[int(i\/2),i%2].set(title=numer[i])","38482b19":"fig2, axes2 = plt.subplots(2, 2)\nfig2.set_figheight(10)\nfig2.set_figwidth(18)\nfor i in range(len(numer)):\n  axes2[int(i\/2),i%2].plot(df[numer[i]])\n  axes2[int(i\/2),i%2].set(title=numer[i])","5568b940":"fig = plt.figure(figsize=(15,25))\nax1 = fig.add_subplot(411)\nax2 = fig.add_subplot(412)\nax3 = fig.add_subplot(425)\nax4 = fig.add_subplot(426)\nax5 = fig.add_subplot(414)\n\n\nax1.bar(df['model'].value_counts().index, list(df['model'].value_counts()))\nax1.tick_params(axis='x',labelrotation=30)\nax1.set_title('Model')\n\nax2.bar(list(map(str, df['year'].value_counts().index)), list(df['year'].value_counts()))\nax2.tick_params(axis='x',labelrotation=60)\nax2.set_title('Year')\n\nax3.bar(df['transmission'].value_counts().index, list(df['transmission'].value_counts()))\nax3.set_title('Transmission')\n\nax4.bar(df['fuelType'].value_counts().index, list(df['fuelType'].value_counts()))\nax4.set_title('Fuel Type')\n\nax5.bar(list(map(str, df['engineSize'].value_counts().index)), list(df['engineSize'].value_counts()))\nax5.set_title('Engine Size')\n\n\nfor i in range(len(list(df['model'].value_counts()))):\n  ax1.text(i, list(df['model'].value_counts())[i] + 15, list(df['model'].value_counts())[i], ha='center', va='bottom')\nfor i in range(len(list(df['year'].value_counts()))):\n  ax2.text(i, list(df['year'].value_counts())[i] + 15, list(df['year'].value_counts())[i], ha='center', va='bottom')\nfor i in range(len(list(df['transmission'].value_counts()))):\n  ax3.text(i, list(df['transmission'].value_counts())[i] + 15, list(df['transmission'].value_counts())[i], ha='center', va='bottom')\nfor i in range(len(list(df['fuelType'].value_counts()))):\n  ax4.text(i, list(df['fuelType'].value_counts())[i] + 15, list(df['fuelType'].value_counts())[i], ha='center', va='bottom')\nfor i in range(len(list(df['engineSize'].value_counts()))):\n  ax5.text(i, list(df['engineSize'].value_counts())[i] + 15, list(df['engineSize'].value_counts())[i], ha='center', va='bottom')","041c2e48":"plt.figure(figsize=(4,4))\nsns.heatmap(df[numer].corr(), annot=True, linewidths=1, cmap='vlag', vmin=-1, vmax=1,);","3d96d184":"import sklearn\nfrom sklearn.preprocessing import OrdinalEncoder\noe = OrdinalEncoder()\ndf_oe = df.copy()\ndf_oe[str_categ] = oe.fit_transform(df_oe[str_categ])","75f8f7e5":"from sklearn.model_selection import train_test_split\nX = df_oe.drop(['price'], axis=1)\ny = df_oe['price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","f320de15":"from sklearn.metrics import mean_squared_error\nfrom sklearn.linear_model import LinearRegression, Ridge, Lars, Lasso\n\nlinreg = [ ('LR', LinearRegression()),\n           ('Ridge', Ridge()),\n           ('LARS', Lars()),\n           ('LASSO', Lasso()),\n           ] \n\n\nfor name, model in linreg:\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_test, preds))\n    rs = model.score(X_test, y_test)\n    print(f'{name} RMSE: {round(rmse,3)}')\n    print(f'       R^2: {round(rs,3)}\\n')","282d01c5":"from sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\n\nregrmodel = [ ('KNeighborsReg', KNeighborsRegressor()),\n              ('SVR', SVR()),\n              ('RandomForestReg', RandomForestRegressor()),\n             ] \n\nfor name, model in regrmodel:\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_test, preds))\n    rs = model.score(X_test, y_test)\n    print(f'{name} RMSE: {round(rmse,3)}')\n    print(f'       R^2: {round(rs,3)}\\n')","8e2987b1":"from sklearn.preprocessing import RobustScaler\ndata_ohe = pd.get_dummies(df)\nscl = RobustScaler()\ndata_scl_ohe = scl.fit_transform(data_ohe)\ndata_scl_ohe = pd.DataFrame(data_scl_ohe, columns = data_ohe.columns)\ndata_scl_ohe.head()","13c28b50":"X = data_scl_ohe.drop(['price'], axis=1)\ny = data_scl_ohe['price']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)","3be67c50":"linreg = [ ('LR', LinearRegression()),\n           ('Ridge', Ridge()),\n           ('LARS', Lars()),\n           ('LASSO', Lasso()),\n           ] \n\n\nfor name, model in linreg:\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_test, preds))\n    rs = model.score(X_test, y_test)\n    print(f'{name} RMSE: {round(rmse,3)}')\n    print(f'       R^2: {round(rs,3)}\\n')","728753f0":"regrmodel = [ ('KNeighborsReg', KNeighborsRegressor()),\n              ('SVR', SVR()),\n              ('RandomForestReg', RandomForestRegressor()),\n             ] \n\nfor name, model in regrmodel:\n    model.fit(X_train, y_train)\n    preds = model.predict(X_test)\n    rmse = np.sqrt(mean_squared_error(y_test, preds))\n    rs = model.score(X_test, y_test)\n    print(f'{name} RMSE: {round(rmse,3)}')\n    print(f'       R^2: {round(rs,3)}\\n')","856ebe2b":"from sklearn.model_selection import GridSearchCV\nmodel = SVR()\nh_param = {'C':list(range(2,12)), 'epsilon':[0.05, 0.1, 0.15, 0.2]}\ngrid = GridSearchCV(model, h_param, scoring='r2')\ngrid.fit(X_train, y_train);\nprint(f'Best params: {grid.best_params_}')\nprint(f'R^2: {round(grid.score(X_test, y_test),3)}')","2132d64d":"**Using One Hot Encoding (pandas get_dummies() to simplify) and RobustScaler to scale data.**","e5773c25":"**Split Data**","317142bc":"**Second set of models on data with OHE and RobustScaler**","4d855b13":"# Preprocessing and Modeling","d2f3dd19":"# Importing Data and primary Data analysis","a4441779":"Better result on Linear Regression, R^2: 0.926","222769c7":"**First set of models on data with OHE and RobustScaler**","0905afba":"**Hyperparameter optimization**","4174dc31":"* Good results on Random Forest, but some other models doesn't work.\n* Let's scale data and use other encoder for categorial features.","a074f051":"**Modeling.**\n**First set of models on data with OrdinalEncoder for categorial features**","0aa578fa":"**Using OrdinalEncoder for categorial features with \"str\" type**","66e738cb":"# Visualising Data","0f4ae255":"Best result in Support Vector regression model, R^2: 0.968","13190f9b":"**Second set of models on data with OrdinalEncoder for categorial features**"}}