{"cell_type":{"883050b4":"code","17b29511":"code","70009115":"code","69d28072":"code","cb27beb5":"code","68d1fc78":"code","b4a5b2d9":"code","82f8f95c":"code","3aeb0bbe":"code","04d374fd":"code","822c2743":"code","4f4a1ae8":"code","3764fa4d":"markdown","a6fa464a":"markdown","dcc33269":"markdown","52589cd4":"markdown","98cf0bf7":"markdown"},"source":{"883050b4":"import torch\nimport torch.nn.functional as F\nimport torch.optim as optim\nimport torch.nn as nn\n\nimport numpy as np\nimport sys\n\nimport pandas as pd\n\nimport random\n\nfrom sklearn.preprocessing import StandardScaler\n\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'\n\nrandom.seed(484)\ntorch.manual_seed(484)\nif device == 'cuda':\n  torch.cuda.manual_seed_all(484)","17b29511":"train = pd.read_csv('\/kaggle\/input\/defense-project\/train_data.csv')\ntrain","70009115":"scaler = StandardScaler()\ntest = pd.read_csv('\/kaggle\/input\/defense-project\/test_data.csv')\n\nx_train_data = train.loc[:,'# mean_0_a':'fft_749_b']\ny_train_data = train.loc[:,'label']\n\nxs_data = scaler.fit_transform(x_train_data)","69d28072":"# train\nx_train = torch.FloatTensor(xs_data[:1200])\nx_vali = torch.FloatTensor(xs_data[1200:]) # Validation Data\ny_train = torch.LongTensor(y_train_data[:1200].values)\ny_vali = torch.LongTensor(y_train_data[1200:].values)","cb27beb5":"print(x_train.shape)\nprint(x_vali.shape)","68d1fc78":"l1 = torch.nn.Linear(2548,16)#\ub525\ub7ec\ub2dd \ubaa8\ub378\ub85c \uc218\uc815\nl2 = torch.nn.Linear(16, 3)\nrelu = torch.nn.LeakyReLU()\ntorch.nn.init.xavier_uniform_(l1.weight)\ntorch.nn.init.xavier_uniform_(l2.weight)\nmodel = torch.nn.Sequential(l1, relu, l2).to(device)\nmodel","b4a5b2d9":"train_dataset = torch.utils.data.TensorDataset(x_train, y_train)\ndata_loader = torch.utils.data.DataLoader(dataset=train_dataset,\n                                          batch_size=100,\n                                          shuffle=True,\n                                          drop_last=True)","82f8f95c":"loss = torch.nn.CrossEntropyLoss().to(device)\noptimizer = torch.optim.Adam(model.parameters(), lr=0.001) ","3aeb0bbe":"total_batch = len(data_loader)\nmodel_history = []\nerr_history = []\nvali_err_history = []\nx_vali = x_vali.to(device)\ny_vali = y_vali.to(device)\nfor epoch in range(1, 1+300):\n    avg_cost = 0\n\n    for X, Y in data_loader:\n\n        X = X.to(device)\n        Y = Y.to(device)\n\n        # \uadf8\ub798\ub514\uc5b8\ud2b8 \ucd08\uae30\ud654\n        optimizer.zero_grad()\n        # Forward \uacc4\uc0b0\n        hypothesis = model(X)\n        # Error \uacc4\uc0b0\n        cost = loss(hypothesis, Y)\n        # Backparopagation\n        cost.backward()\n        # \uac00\uc911\uce58 \uac31\uc2e0\n        optimizer.step()\n\n        # \ud3c9\uade0 Error \uacc4\uc0b0\n        avg_cost += cost\n    avg_cost \/= total_batch\n    \n    with torch.no_grad():\n        pred = model(x_vali)\n        torch.nn.CrossEntropyLoss().to(device)\n        vali_err = loss(pred, y_vali)\n        vali_err_history.append(vali_err)\n    \n    model_history.append(model)\n    err_history.append(avg_cost)\n\n    if epoch % 50 == 1 :\n        print('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))\n\nprint('Epoch:', '%04d' % (epoch), 'cost =', '{:.9f}'.format(avg_cost))","04d374fd":"import matplotlib.pyplot as plt\n\nplt.plot(err_history[10:])\nplt.plot(vali_err_history[10:])\nplt.show()","822c2743":"best_model = model_history[np.argmin(vali_err_history)]\nprint(min(vali_err_history))\nprint(np.argmin(vali_err_history))","4f4a1ae8":"test = pd.read_csv('\/kaggle\/input\/defense-project\/test_data.csv')\ntest = test.to_numpy()\ntest = scaler.transform(test)\nwith torch.no_grad():\n    x_test = torch.FloatTensor(test).to(device)\n    \n    pred = model(x_test)\n    pred = pred.cpu()\n    \n    real_test_df = pd.DataFrame([[i, r] for i, r in enumerate(torch.argmax(pred, dim=1).numpy())], columns=['Id',  'Category'])\n    real_test_df.to_csv('result.csv', mode='w', index=False)","3764fa4d":"# \ucc28\ubcc4\uc8101: Train\uc73c\ub85c scale\n- \ud559\uc2b5\uc744 Train\uc73c\ub85c\ub9cc \uc9c4\ud589\ud558\uae30 \ub54c\ubb38\uc5d0 Test\uac00 \ubaa8\ub378\uc5d0 \uc601\ud5a5\uc744 \uc8fc\ub294 \uac83\uc744 \uc6d0\ud558\uc9c0 \uc54a\uc544\uc11c Train\uc73c\ub85c\ub9cc Scale\uc744 \uc870\uc815\ud558\uaca0\uc2b5\ub2c8\ub2e4.","a6fa464a":"# \ucc28\ubcc4\uc8104: Validation Set\uc744 \uc774\uc6a9\ud55c \ucd5c\uc801\uc758 Epoch \ud0d0\uc0c9","dcc33269":"# \ub9ac\ub354\ubcf4\ub4dc \uacf5\uaca9 by \ubc31\uc9c0\uc624\n19011484 \ubc31\uc9c0\uc624\n\n\ucc28\ubcc4\uc810\n- TrainSet\ub9cc\uc744 \uc774\uc6a9\ud55c Scale\n- DNN \ubc0f Validation Set \ub3c4\uc785\n- \ubbf8\ub2c8\ubc30\uce58 \ud559\uc2b5","52589cd4":"# \ucc28\ubcc4\uc8102: DNN \ubc0f Validation set \uc0ac\uc6a9","98cf0bf7":"# \ucc28\ubcc4\uc8103: \ubbf8\ub2c8\ubc30\uce58 \uc774\uc6a9"}}