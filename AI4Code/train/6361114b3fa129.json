{"cell_type":{"9408a63f":"code","4dcbdd5a":"code","46b3c6ba":"code","e0f9a54b":"code","fd960b99":"code","104151a4":"code","95802f04":"code","3da662a6":"code","2f563535":"code","b4757e83":"code","36f224c7":"code","55bea0f8":"code","48f8bbba":"code","7f3d74a9":"code","4570ceca":"code","f4ce502b":"code","0f16155a":"code","a2471c61":"code","f619b741":"code","48c318c3":"code","992dbc1d":"code","f2ad6256":"code","3410552e":"code","80fffcbf":"code","07ce97a6":"code","58bac1c4":"code","d7dbb0c2":"code","36b9a55e":"code","9dcbadb0":"markdown","870a8cf4":"markdown","d4c9f86f":"markdown","48590af5":"markdown","e45d7235":"markdown","a49568eb":"markdown","a512e875":"markdown","3061128c":"markdown","5df9cc29":"markdown","a5e79f78":"markdown","19f4c31a":"markdown","be7637cb":"markdown","8c19d85a":"markdown","69530d15":"markdown","7b9dfc26":"markdown","e907f2d1":"markdown","9f423ab9":"markdown","711ba9c4":"markdown","9082a55d":"markdown","12dfd3a0":"markdown","b2f3190d":"markdown","aa7dad7d":"markdown"},"source":{"9408a63f":"import numpy as np # For Linear Algebra\nimport pandas as pd # Data\n\n# For visualization\nimport matplotlib.pyplot as plt \nplt.rc(\"font\", size=14)\nimport seaborn as sns\nsns.set(style=\"white\") #white background style for seaborn plots\nsns.set(style=\"whitegrid\", color_codes=True)\n\n# Model building\nfrom sklearn import preprocessing\nfrom sklearn import metrics\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.feature_selection import RFECV\nfrom sklearn.model_selection import train_test_split, cross_val_score","4dcbdd5a":"# Read CSV train data file into DataFrame\ntrain_df = pd.read_csv(\"..\/input\/train.csv\")\n\n# Read CSV test data file into DataFrame\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\n\n# preview train data\ntrain_df.head()","46b3c6ba":"print(f'The number of records in the train data is {train_df.shape[0]}.')\nprint(f'The number of records in the test data is {test_df.shape[0]}.')","e0f9a54b":"# preview test data\ntest_df.head()","fd960b99":"# check missing values in train data\ntrain_df.isnull().sum()","104151a4":"# percent of missing \"Age\" \nprint('Percent of missing \"Age\" records is %.2f%%' %((train_df['Age'].isnull().sum()\/\n                                                      train_df.shape[0])*100))","95802f04":"# How the Age column looks,\n%matplotlib inline\nax = train_df[\"Age\"].hist(bins=15, density=True, stacked=True, color='teal', alpha=0.6)\ntrain_df[\"Age\"].plot(kind='density', color='teal')\nax.set(xlabel='Age')\nplt.xlim(-10,85)","3da662a6":"train_df[\"Age\"].fillna(train_df[\"Age\"].median(skipna=True), inplace=True)\ntest_df[\"Age\"].fillna(train_df[\"Age\"].median(skipna=True), inplace=True)","2f563535":"# percent of missing \"Embarked\" \nprint('Percent of missing \"Embarked\" records is %.2f%%' %((train_df['Embarked'].isnull().sum()\/\n                                                           train_df.shape[0])*100))","b4757e83":"print('Passengers by Port of Embarkation: ')\nprint(train_df['Embarked'].value_counts())\nsns.countplot(x='Embarked', data=train_df, palette='rainbow')\nplt.show()","36f224c7":"train_df[\"Embarked\"].fillna(train_df['Embarked'].value_counts().idxmax(), inplace=True)\ntest_df[\"Embarked\"].fillna(train_df['Embarked'].value_counts().idxmax(), inplace=True)","55bea0f8":"# percent of missing \"Cabin\" \nprint('Percent of missing \"Cabin\" records is %.2f%%' %((train_df['Cabin'].isnull().sum()\/\n                                                        train_df.shape[0])*100))","48f8bbba":"train_df.drop('Cabin', axis=1, inplace=True)\ntest_df.drop('Cabin', axis=1, inplace=True)","7f3d74a9":"train_df.drop('Fare', axis=1, inplace=True)\ntest_df.drop('Fare', axis=1, inplace=True)","4570ceca":"# check missing values in adjusted train data\ntrain_df.isnull().sum()","f4ce502b":"## Create categorical variable for traveling alone\ntrain_df['TravelAlone']=np.where((train_df[\"SibSp\"]+train_df[\"Parch\"])>0, 0, 1)\ntrain_df.drop('SibSp', axis=1, inplace=True)\ntrain_df.drop('Parch', axis=1, inplace=True)\n\ntest_df['TravelAlone']=np.where((test_df[\"SibSp\"]+test_df[\"Parch\"])>0, 0, 1)\ntest_df.drop('SibSp', axis=1, inplace=True)\ntest_df.drop('Parch', axis=1, inplace=True)","0f16155a":"#create categorical variables and drop some variables\ntraining=pd.get_dummies(train_df, columns=[\"Pclass\",\"Embarked\",\"Sex\"])\ntraining.drop('Sex_female', axis=1, inplace=True)\ntraining.drop('PassengerId', axis=1, inplace=True)\ntraining.drop('Name', axis=1, inplace=True)\ntraining.drop('Ticket', axis=1, inplace=True)\n\nfinal_train = training\nfinal_train.head()\n\ntesting=pd.get_dummies(test_df, columns=[\"Pclass\",\"Embarked\",\"Sex\"])\ntesting.drop('Sex_female', axis=1, inplace=True)\ntesting.drop('PassengerId', axis=1, inplace=True)\ntesting.drop('Name', axis=1, inplace=True)\ntesting.drop('Ticket', axis=1, inplace=True)\n\nfinal_test = testing","a2471c61":"plt.figure(figsize=(15,8))\nax = sns.kdeplot(final_train[\"Age\"][final_train.Survived == 1], color=\"darkturquoise\", shade=True)\nsns.kdeplot(final_train[\"Age\"][final_train.Survived == 0], color=\"lightcoral\", shade=True)\nplt.legend(['Survived', 'Died'])\nplt.title('Density Plot of Age for Surviving Population and Deceased Population')\nax.set(xlabel='Age')\nplt.xlim(-10,85)\nplt.show()","f619b741":"final_train['IsMinor']=np.where(final_train['Age']<=16, 1, 0)\nfinal_test['IsMinor']=np.where(final_test['Age']<=16, 1, 0)","48c318c3":"sns.barplot('Pclass', 'Survived', data=train_df, color=\"orange\")\nplt.show()","992dbc1d":"sns.barplot('Embarked', 'Survived', data=train_df, color=\"teal\")\nplt.show()","f2ad6256":"sns.barplot('Sex', 'Survived', data=train_df, color=\"green\")\nplt.show()","3410552e":"import warnings\nwarnings.filterwarnings('ignore')\n\nX = training.drop('Survived', axis=1) # Independent varaibles\ny = training['Survived'] # Dependent variables\n\n# Let's choose Logistic Regression\nrfecv = RFECV(estimator=LogisticRegression(), step=1, cv=10, scoring='accuracy')\nrfecv.fit(X, y)\n\nprint(f'Number of optimal features: {rfecv.n_features_}')\nprint(f'Selected optimal features: {list(X.columns[rfecv.support_])}')","80fffcbf":"# Splitting the data into train and test to evaluate our model\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=4)\n\n# Logistic Regression\nlg = LogisticRegression(n_jobs=-1)\n\n# Training (Finding the optimal weights)\nlg.fit(X_train, y_train)\n\n# Predictions\ny_pred = lg.predict(X_test)","07ce97a6":"# Review our predictions\ny_pred","58bac1c4":"# Evaluation\nprint(f'Accuracy: {metrics.accuracy_score(y_test, y_pred)}')\nprint(f'ROC AUC Score: {metrics.roc_auc_score(y_test, y_pred)}')\nprint(f'Classification Report:\\n{metrics.classification_report(y_test, y_pred)}')","d7dbb0c2":"predictions = lg.predict(testing)\nID = pd.read_csv('..\/input\/test.csv').PassengerId\nsubmit_df = pd.DataFrame()\nsubmit_df['PassengerId'] = ID\nsubmit_df['Survived'] = predictions\n\nsubmit_df.head()","36b9a55e":"# Saving the file,\nsubmit_df.to_csv('submission.csv', index=False)","9dcbadb0":"# 3. Exploratory Data Analysis","870a8cf4":"# 6. Submission","d4c9f86f":"- People who boarded from Cherbourg, France seems to have survived the most, this may be related to Pclass.","48590af5":"# 2. Data Quality & Missing Value Treatment\n","e45d7235":"### checking by Age","a49568eb":"### Cabin Column","a512e875":"# 5. Model Evaluation using Logistic Regression","3061128c":"# 4. Feature Selection (RFECV)","5df9cc29":"- Satisfying results! ","a5e79f78":"- Considering the survival rate of passengers under 16, we can also include another categorical variable in the dataset: \"Minor\"","19f4c31a":"- Most people boarded from Port S(Southampton), let's impute missing values with S.","be7637cb":"### Embarked Column","8c19d85a":"# 1. Importing Libraries and Data","69530d15":"# Description\nThe sinking of the RMS Titanic is one of the most infamous shipwrecks in history.  On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. This sensational tragedy shocked the international community and led to better safety regulations for ships.\n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, some groups of people were more likely to survive than others, such as women, children, and the upper-class.","7b9dfc26":"- Female survived the most.","e907f2d1":"# Problem Statement\n*- To Predict the Titanic Survivals*","9f423ab9":"- Age is Skewed right, we can fill the missing values with median.","711ba9c4":"## Additional Columns","9082a55d":"## Variable Description\n<b>pclass<\/b>: A proxy for socio-economic status (SES)\n                             1st = Upper\n                            2nd = Middle\n                            3rd = Lower\n\n<b>age<\/b>: Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5\n\n<b>sibsp<\/b>:  The dataset defines family relations in this way...\n                            Sibling = brother, sister, stepbrother, stepsister\n                            Spouse = husband, wife (mistresses and fianc\u00e9s were ignored)\n\n<b>parch<\/b>: The dataset defines family relations in this way...\n                            Parent = mother, father\n                            Child = daughter, son, stepdaughter, stepson\n                            Some children travelled only with a nanny, therefore parch=0 for them.\n         \n <b>Survived<\/b>: 0 = Did not survive\n                                  1 = Survived","12dfd3a0":"### Fare ","b2f3190d":"- First class people who survived are more in number.","aa7dad7d":"- Since, 77% of the data is missing in Cabin column, we could drop this column."}}