{"cell_type":{"918cc1c3":"code","6d5f989c":"code","a206be0b":"code","0db0b321":"code","371d4ca3":"code","1bef746d":"code","ecdff3c1":"code","6db3ff25":"code","8c67d095":"code","6a2de46a":"code","3e686f45":"code","bd8b9bcf":"code","47e08321":"code","c5226882":"code","720838e4":"code","5f91355f":"code","a9c25037":"code","adba9ad3":"code","a8d02c77":"code","db56f8db":"code","f43710ca":"code","601578e2":"code","adc9f64d":"code","4f12b7d7":"code","dae60315":"code","7c62e283":"code","261e9710":"code","4e12d59f":"code","6d625f2e":"code","83c8630b":"code","cb525c7e":"code","6d3f3535":"code","7435704a":"code","3d573e5d":"code","8290487c":"code","3800684b":"code","80b6be6e":"code","3045abd7":"code","18c20e48":"code","14fcf529":"code","329a79ff":"code","c118f6a4":"code","f8dd1fe8":"code","c0c47611":"code","5aa560d4":"code","0c475d77":"markdown","c9c1a2fc":"markdown","b5fe44ba":"markdown","e17ea755":"markdown","f14aa25c":"markdown","07cd2c6e":"markdown","e8f31f1e":"markdown","fd95ab75":"markdown","efc8a74c":"markdown","f3ad872b":"markdown","30efd820":"markdown","b43a77a2":"markdown","b961ed33":"markdown","8a44ff25":"markdown","4aeee4f4":"markdown","52ca81a8":"markdown","4ea9ec55":"markdown","86c75b28":"markdown","cf6024cd":"markdown","a7c9bfcd":"markdown","ab5247e6":"markdown","180373ad":"markdown","60665efc":"markdown","466b4343":"markdown","6e45ef80":"markdown","601f7ed4":"markdown","25761302":"markdown","c7c0c99c":"markdown","e44463ff":"markdown","d06bc7b1":"markdown","7d7573d4":"markdown","522c334b":"markdown","4f70d8e3":"markdown","562d85c7":"markdown","13545c43":"markdown","dbbe23b5":"markdown","e9c8d9ec":"markdown"},"source":{"918cc1c3":"# import pandas\nimport pandas as pd\n\n# load train and test as dataframes\ntrain_df = pd.read_csv(\"..\/input\/crime-prediction\/train.csv\",parse_dates=['Dates'],error_bad_lines=False)\ntest_df = pd.read_csv(\"..\/input\/crime-prediction\/test.csv\",parse_dates=['Dates'],error_bad_lines=False)","6d5f989c":"train_df.isnull().any()\n","a206be0b":"# import display function from ipython\nfrom IPython.display import display, HTML\n\n# display the first rows of each dataset\ndisplay(train_df.head())\nprint(\"train shape: {}\".format(train_df.shape))\ndisplay(test_df.head())\nprint(\"test shape: {}\".format(test_df.shape))","0db0b321":"train_df = train_df.drop(columns=['Descript', 'Resolution'])\ntrain_df.head()","371d4ca3":"# lets see the statistics summary of locations\nlons = train_df['X'] # longitudes \nlats = train_df['Y'] # latitudes\n\nprint(\"Longitudes summary:\")\nprint(lons.describe())\nprint(\"\\nLatitudes summary:\")\nprint(lats.describe())","1bef746d":"# eliminate rows with latitudes out of San Francisco range\ntrain_df = train_df.drop(train_df[(train_df['Y'] > 37.84) | (train_df['Y'] < 37.7)].index)\n# eliminate rows with longitudes out of San Francisco range\ntrain_df = train_df.drop(train_df[((train_df['X'] > -122.32) | (train_df['X'] < -122.52))].index)\ntrain_df.describe()","ecdff3c1":"\n#from shapely.geometry import  Point\n#import geopandas as gpd\nimport matplotlib.pyplot as plt\nimport numpy as np\n#from sklearn.impute import SimpleImputer\n#from sklearn.preprocessing import LabelEncoder\n#from sklearn.model_selection import train_test_split\nimport seaborn as sns\nfrom matplotlib import cm\n#import urllib.request\n#import shutil\n#import zipfile\n#import os\n#import re\n#import contextily as ctx\n#import geoplot as gplt\n#import lightgbm as lgb\n#import eli5\n#from eli5.sklearn import PermutationImportance\n#from lightgbm import LGBMClassifier\nfrom matplotlib import pyplot as plt\n#from pdpbox import pdp, get_dataset, info_plots\n#import shap","6db3ff25":"new_lons = train_df['X'] # longitudes \nnew_lats = train_df['Y'] # latitudes\n\n# scatter plot for lons vs lats\nplt.scatter(new_lons, new_lats)\nplt.xlabel('lons')\nplt.ylabel('lats')\nplt.show()\n\n# histogram plot for lons and lats\nplt.hist(new_lons)\nplt.xlabel('lons')\nplt.ylabel('ocurrance number')\nplt.show()\nplt.hist(new_lats)\nplt.xlabel('lats')\nplt.ylabel('ocurrance number')\nplt.show()","8c67d095":"#import seaborn as sns\ncol = sns.color_palette()\n\ntrain_df['Date'] = train_df.Dates.dt.date\ntrain_df['Hour'] = train_df.Dates.dt.hour\n\nplt.figure(figsize=(10, 6))\ndata = train_df.groupby('Date').count().iloc[:, 0]\nsns.kdeplot(data=data, shade=True)\nplt.axvline(x=data.median(), ymax=0.95, linestyle='--', color=col[1])\nplt.annotate(\n    'Median: ' + str(data.median()),\n    xy=(data.median(), 0.004),\n    xytext=(200, 0.005),\n    arrowprops=dict(arrowstyle='->', color=col[1], shrinkB=10))\nplt.title(\n    'Distribution of number of incidents per day', fontdict={'fontsize': 16})\nplt.xlabel('Incidents')\nplt.ylabel('Density')\nplt.legend().remove()\nplt.show()","6a2de46a":"data = train_df.groupby('DayOfWeek').count().iloc[:, 0]\ndata = data.reindex([\n    'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday',\n    'Sunday'\n])\n\nplt.figure(figsize=(10, 5))\nwith sns.axes_style(\"whitegrid\"):\n    ax = sns.barplot(\n        data.index, (data.values \/ data.values.sum()) * 100,\n        orient='v',\n        palette=cm.ScalarMappable(cmap='Reds').to_rgba(data.values))\n\nplt.title('Incidents per Weekday', fontdict={'fontsize': 16})\nplt.xlabel('Weekday')\nplt.ylabel('Incidents (%)')\n\nplt.show()","3e686f45":"data = train_df.groupby('Category').count().iloc[:, 0].sort_values(\n    ascending=False)\ndata = data.reindex(np.append(np.delete(data.index, 1), 'OTHER OFFENSES'))\n\nplt.figure(figsize=(10, 10))\nwith sns.axes_style(\"whitegrid\"):\n    ax = sns.barplot(\n        (data.values \/ data.values.sum()) * 100,\n        data.index,\n        orient='h',\n        palette=\"Reds_r\")\n\nplt.title('Incidents per Crime Category', fontdict={'fontsize': 16})\nplt.xlabel('Incidents (%)')\n\nplt.show()","bd8b9bcf":"from collections import Counter\n\ndef printCategoriesOccurrence():\n    \n    categories = train_df['Category']\n    # count the number of occurances for each category\n    occurances = Counter(categories)\n    sorted_occ = sorted(occurances.items(), key=lambda pair: pair[1], reverse=True)\n    for key, value in sorted_occ:\n        print(key, value)\n    return sorted_occ\n        \nsorted_occ = printCategoriesOccurrence()","47e08321":"import math\n# if category size < 1000, duplicate it to be = 1000\nfor key,value in sorted_occ:\n    if value<1000:\n        \n        temp = train_df[train_df['Category'] == key]\n        train_df = train_df.append([temp]*int(math.ceil((1000-value)\/float(value))), ignore_index=True)\n\nsorted_occ = printCategoriesOccurrence()","c5226882":"# spliting train data into target and other features\ntarget = train_df['Category']\ndata = train_df.drop(columns=['Category'])","720838e4":"features = ['Dates', 'DayOfWeek', 'PdDistrict', 'Address']\nfor feature in features:\n    print(\"feature: {}    unique_size: {}\".format(feature ,len(data[feature].unique())))","5f91355f":"data.head()","a9c25037":"print(str(data['Dates'][0])[:-6])","adba9ad3":"# convert given list of dates it will trim seconds, minutes and return result\ndef trimMinAndSecFromDates(dates):\n    result = []\n    for date in dates:\n        result.append(str(date)[:-6])\n    return result\n\n# trim minutes and seconds from dates\ndata['Dates'] = trimMinAndSecFromDates(data['Dates'])\n\n# encode Dates using label encoding\ndata['Dates'] = data['Dates'].astype('category')\ndata['Dates_int'] = data['Dates'].cat.codes\n\n# encode Address using label encoding\ndata['Address'] = data['Address'].astype('category')\ndata['Address_int'] = data['Address'].cat.codes\n\ndata.head()","a8d02c77":"data.drop(columns=['Dates', 'Address'], inplace=True)\ndata.head()","db56f8db":"# get dummies for each feature\nDayOfWeek_dummies = pd.get_dummies(data['DayOfWeek'])\nPdDistrict_dummies = pd.get_dummies(data['PdDistrict'])\n\n# join dummies to the original dataframe\ndata = data.join(DayOfWeek_dummies)\ndata = data.join(PdDistrict_dummies)\n\ndata.head()","f43710ca":"\ndata.drop(columns=['DayOfWeek', 'PdDistrict'], inplace=True)\nprint(\"data size =\",len(data))\ndata.head()\n","601578e2":"data.drop(columns=['Date', 'Hour'], inplace=True)\n","adc9f64d":"import numpy as np\ndata[~data.isin([np.nan, np.inf, -np.inf]).any(1)].astype(np.float64)\ndata.replace([np.inf, -np.inf], np.nan)","4f12b7d7":"data.isnull().any()","dae60315":"data.dropna()","7c62e283":"data.columns","261e9710":"data.shape","4e12d59f":"from sklearn.model_selection import train_test_split\n\nX_train, X_test, y_train, y_test =  train_test_split(data, target, test_size=0.2, random_state=0, stratify=target)\nprint(\"train size: {}, test size: {}\".format(X_train.shape[0], X_test.shape[0]))\n","6d625f2e":"target","83c8630b":"data.isnull().any()\n","cb525c7e":"from sklearn.metrics import log_loss, fbeta_score\nfrom time import time\n\n# train function takes learner, the train data and target\ndef train_test_pipeline(learner, X_train, y_train, X_test, y_test):\n    \n    results = {}\n    \n    # training learner\n    start = time()\n    learner.fit(X_train, y_train)\n    end = time()\n    results['train_time'] = end - start\n    \n     # remove missed classes after fitting for logloss\n    for category in list(set(target) - set(learner.classes_)):\n        X_train = X_train.drop(y_train[y_train == category].index)\n        y_train = y_train[y_train != category]\n        X_test = X_test.drop(y_test[y_test == category].index)\n        y_test = y_test[y_test != category]\n    # predict samples in training set\n    predictions = learner.predict(X_train)\n    predictions_proba = learner.predict_proba(X_train)\n    \n    # calculate fbeta and log loss\n    results['fscore'] = fbeta_score(y_train, predictions, beta=.5, average='micro')\n    results['logloss'] = log_loss(y_train, predictions_proba)\n    \n    # predict testing samples and time of prediction\n    start = time()\n    predictions = learner.predict(X_test)\n    predictions_proba = learner.predict_proba(X_test)\n    end = time()\n    results['test_time'] = end - start\n    \n    # calculate fbeta and log loss for testing set\n    results['fscore_test'] = fbeta_score(y_test, predictions, beta=.5, average='micro')\n    results['logloss_test'] = log_loss(y_test, predictions_proba)\n    \n    \n    print (\"{} trained\".format(learner.__class__.__name__))\n    \n    return results\n\n# do train_test_pipeline then visualize resutls for given models on first n samples and test on first m\n# returns predictions proba for last model in given list - this will be used for 1 model only -\ndef train_test_models(models, names=None, n=len(y_train), m=len(y_test)):\n    results = {}\n    i = 0\n    for model in models:\n        if not names:\n            model_name = model.__class__.__name__\n        else:\n            model_name = names[i]\n            i += 1\n        results[model_name] = train_test_pipeline(model, X_train[:n], y_train[:n], X_test[:m], y_test[:m])\n\n    # print results\n    for model in results:\n        model_res = results[model]\n        print (\"model: {}\".format(model))\n        print (\"fscore:\\t\\t{}\\nlogloss:\\t{}\\ntrain time:\\t{}\".format(model_res['fscore'], model_res['logloss'], model_res['train_time']))\n        print (\"fscore_test:\\t\\t{}\\nlogloss_test:\\t{}\\ntest time:\\t{}\".format(model_res['fscore_test'], model_res['logloss_test'], model_res['test_time']))\n\n    # visualize the results    \n    visualize(results, random_results)","6d3f3535":"import matplotlib.pyplot as plt\ndef visualize(results, random_results):\n    bar_width = 0.3\n    fig, ax = plt.subplots(6,1,figsize = (12,32))\n    for j, metric in enumerate(['train_time', 'fscore', 'logloss', 'test_time', 'fscore_test', 'logloss_test']):\n        ax[j].set_xlabel(\"Learners\")\n        ax[j].set_ylabel(metric)\n        ax[j].set_title(metric)\n        for k, learner in enumerate(results.keys()):\n            ax[j].bar(learner, results[learner][metric], width=bar_width)\n    \n    # add horizontal line for random model results\n    ax[0].axhline(y=random_results['train_time'], linestyle='dashed')\n    ax[1].axhline(y=random_results['fscore'], linestyle='dashed')\n    ax[2].axhline(y=random_results['logloss'], linestyle='dashed')  \n    ax[3].axhline(y=random_results['test_time'], linestyle='dashed')\n    ax[4].axhline(y=random_results['fscore_test'], linestyle='dashed')\n    ax[5].axhline(y=random_results['logloss_test'], linestyle='dashed') ","7435704a":"import random\nclass random_model:\n \n    def __init__(self, categories):\n        self.categories = categories\n        self.classes_ = categories\n\n    # always return a random value from categories\n    def __getRandomValue(self): return random.choice(self.categories) \n    \n    # no need for fit here\n    def fit(self, X_train, y_train): pass\n    \n    def predict(self, X):\n        result = [[] for i in range(len(X))]\n        for j in range(len(X)):\n            result[j] = self.__getRandomValue()\n        return result\n        \n    def predict_proba(self, X):\n        result = [[] for i in range(len(X))]\n        for j in range(len(X)):\n            row = [0.0]*len(self.categories)\n            prediction = self.__getRandomValue()\n            for i in range(len(self.categories)): \n                if(self.categories[i] == prediction):\n                    row[i] = 1.0\n                    break\n            result[j] = row\n        return result","3d573e5d":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.svm import SVC \nfrom xgboost import XGBClassifier\n\n# initializing models\nmodel_KNN = KNeighborsClassifier(n_jobs=-1, weights='distance')\nmodel_tree = DecisionTreeClassifier(class_weight='balanced')\nmodel_extraTrees = ExtraTreesClassifier(n_jobs=-1, class_weight='balanced')\nmodel_NN = MLPClassifier(learning_rate='invscaling', shuffle=True)\nmodel_SVC = SVC(probability=True, class_weight='balanced') # One-to-One\nmodel_XGB = XGBClassifier(one_drop=1)","8290487c":"model_random = random_model(categories=target.unique())\nrandom_results = train_test_pipeline(model_random, X_train, y_train, X_test, y_test)","3800684b":"models = [model_KNN, model_tree, model_extraTrees, model_NN, model_SVC, model_XGB]\ntrain_test_models(models, n=10000, m=2000)","80b6be6e":"# training other models on all data\nmodels = [model_KNN, model_tree, model_NN]\ntrain_test_models(models,n=20000,m=2000)","3045abd7":"model_NN_tuned = MLPClassifier(learning_rate='adaptive', shuffle=True, epsilon=1e-8, activation='relu',\n                               hidden_layer_sizes=100, solver='adam', verbose=True)\n\nmodels = [model_NN_tuned]\ntrain_test_models(models,n=20000,m=4000)","18c20e48":"X_train.isnull().any()","14fcf529":"y_train.isnull().any()","329a79ff":"\nmodel_KNN = KNeighborsClassifier(n_jobs=-1, weights='distance')\nX=X_train.iloc[0:30000]\ny=y_train.iloc[0:30000]\nmodel_KNN.fit(X_train,y_train)\nprint(\"knn training accuracy\",model_KNN.score(X,y))\nprint(model_KNN.predict(X))\n\n","c118f6a4":"model_tree.fit(X,y)\nprint(\"training accuracy\",model_tree.score(X,y))\n","f8dd1fe8":"model_extraTrees.fit(X,y)\nprint(\"training accuracy EXTRATREES\",model_extraTrees.score(X,y))\n","c0c47611":"'''\nmodel_SVC.fit(X,y)\nprint(\"training accuracy\",model_SVC.score(X,y))\n\nmodel_XGB.fit(X,y)\nprint(\"training accuracy XGB\",model_XGB.score(X,y))\n'''","5aa560d4":"'''\nmodel_NN_tuned.fit(X,y)\nprint(\"knn training accuracy\",model_NN_tuned.score(X,y))\n'''\n\n","0c475d77":"\n--------------\n# Models Implementation","c9c1a2fc":"-----------------------\n**Observation:**\n* longitudes are between [-122.52, -120.5], each value different slightly from others\n* latitudes are between [37.708, 90]\n* here as shown there exist some bad values -i.e. close to 90-, the reasons that this is bad that \n    * first, san fransisco latitudes are between [37.707, 37.83269] , reference: [google maps](https:\/\/www.google.com.eg\/maps\/place\/San+Francisco,+CA,+USA\/@37.7407396,-122.4303937,12z\/data=!4m5!3m4!1s0x80859a6d00690021:0x4a501367f076adff!8m2!3d37.7749295!4d-122.4194155)\n    * second as shown in the statistics that the most values are close to 37.7\n    * Also in longitudes, san fransisco longitudes are between [-122.517652, -122.3275], from google maps\n\nNow, to demonstrate the locations, let's plot them using scatter plot","b5fe44ba":"---\nFinding number of occurances of for each category in data","e17ea755":"Now let's drop the `DayOfWeek` and `PdDistrict` cols as they are now useless.","f14aa25c":"## 5. Data splitting\nsplit training data into train set with size 80% and test set with size 20% - i.e. validation set.","07cd2c6e":"* As shown above, The test dataset doesn't contain 3 columns {Category, Descript, Resolution} as the category column is the target column to found in the test set.\n* The test set will be used for testing only at the end of project\n* Training data will be divided into training and testing -Validation- sets to train the models later.","e8f31f1e":"We are going to implement various models and train them on our data and comapre their performance later using using mult-class logarithmec loss.","fd95ab75":"4 features from the data given are not numbers, so we need to convert them into numbers in order to be able to train the models on them. These features are:\n* Dates\n* DayOfWeek\n* PdDistrict\n* Address\n\nWe can use label Encoding and 1-hot encoding. 1-hot encoding may produce very high numbers of dimensions due to the many data labeles in each feature, but it is better due to the problem with label encoding is that it assumes higher the categorical value, better the category which produce more errors.\n\nSo let's use one-hot encoding with the features with little unique values and use the label encoding with the features with very high unique values\n","efc8a74c":"**Enhanceing data imbalancing:**\nNow as shown above, The data is imbalanced and some features has very low number of contributions in data existance. And to enhance imbalanced data we can replicate the data with low contributions and give them more weights in training.\n\nHere we will replicate the data that has less than 1000 occurances to be 1000. Later we will give them more weights while training.","f3ad872b":"### Dates & Day of the week  \nThese variables are distributed uniformly between 1\/1\/2003 to 5\/13\/2015 (and Monday to Sunday) and split between the training and the testing dataset as mentioned before. We did not notice any anomalies on these variables.  \nThe median frequency of incidents is 389 per day with a standard deviation of 48.51.","30efd820":"## 2. Showing data","b43a77a2":"The XGBClassifier gives bad results than expected, This is due to that at first time we trained on very small numbers of training data, Now, let's try other classifiers on all data except SVC and ExtraTree due to that they need huge amount of available resources.","b961ed33":"## 5. Data preprocessing","8a44ff25":"**Observation:**\nThe most committed crime in San Francisco is the LARCENY\/THEFT. TREA is the least.","4aeee4f4":"Also, there is no significant deviation of incidents frequency throughout the week. Thus we do not expect this variable to play a significant role in the prediction.","52ca81a8":"**Observation:**\n\nFor training:\n* The slowest model is SVC then XGBClassifier\n* All models do better than random predictor\n\nFor test data: \n* The best model in f1 score and logloss is XGBClassifier then SVC\n* The slowest model in testing time is SVC the XGBClassifier\n* All models do better thean random predictor\n    \nAs shown, It seems that the best model to use is XGBClassifier due to it's scores and it has a suitable time in training and testing. SVC also did well but it needs huge amount of time in processing data in training and testing.","4ea9ec55":"### 5.2 One-Hot Encoding\nNow let's one hot encode the `DayOfWeek` and `PdDistrict` using pandas.get_dummies()","86c75b28":"## 2. Removing redundant rows\nNow, lets visualize location information","cf6024cd":"# Problem Statement\nCrime is increasing considerably day by day. Crime is among the main issues which is growing continuously in intensity and complexity. Crime patterns are changing constantly because of which it is difficult to explain behaviours in crime patterns.\n\nSo it becomes a difficult challenge for crime analysts to analyse such voluminous crime data without any computational support. A powerful system for predicting crimes is required in place of traditional crime analysis because traditional methods cannot be applied when crime data is high dimensional and complex queries are to be processed. Therefore a crime prediction and analysis tool were needed for identifying crime patterns effectively.","a7c9bfcd":"### Category  \nThere are 39 discrete categories that the police department file the incidents with the most common being Larceny\/Theft (19.91%), Non\/Criminal (10.50%), and Assault(8.77%).","ab5247e6":"Now let's drop the `Dates` and `Address` cols as they are now useless.","180373ad":"### 1.2 other models\nIn this section, We will train other models using training pipline that we implemented previously. The models to use are:\n* KNNeighbors\n* DecisionTree\n* ExtraTrees\n* Neural network MLP\n* Support vector machine\n* xgboost","60665efc":"# Data Exploration\nIn this section, We will load data and see it's content.\n\n## 1. Loading the data files","466b4343":"### Visualization\nvisualize training and testing results","6e45ef80":"#### 1.2.1 Initializing models\nHere, we enhance imbalanced data more by putting the `class_weight` parameter = `balanced` which mean that the weights will be automatically adjusted inversely proportional to class frequencies in the input data as `n_samples \/ (n_classes * np.bincount(y))`","601f7ed4":"## 1. Implementing Models\nIn this section, we will train various model on data.","25761302":"#### 1.2.1 Initializing models\nHere, we enhance imbalanced data more by putting the `class_weight` parameter = `balanced` which mean that the weights will be automatically adjusted inversely proportional to class frequencies in the input data as `n_samples \/ (n_classes * np.bincount(y))`","c7c0c99c":"### Data Encoding","e44463ff":"**Observation:**  the most crimes are in the location of longitude = [-122.44, -122.40] and latitude = [37.76, 37.80]","d06bc7b1":"#### 1.2.3 train and visualize other models with small number of samples","7d7573d4":"### 1.1 Naive Random Predictor\nRandom predictor which always predict the category randomly.","522c334b":"----\n# Data visualizing and preprocessing\nIn this section, We will visualize data and remove unnecessary data.","4f70d8e3":"As shown above, `DayOfWeek` and `PdDistrict` can one-hot encoded. But `Address`, `Dates` should be encoded using label encoding.\n\nFor `Dates`, lets neglect the miniutes and seconds of each date as this will not affect greatly on our predictions but on the contrary it may produce good environment for overfitting, so let's convert `Dates` by our hand to integers then convert `Address` using `cat.codes` tool.\n\n### 5.1 Label Encoding","562d85c7":"## 1. Removing redundant features\nAs shown above, there exist 2 columns -features- that are considered as redundancy. `Descript` and `Resolution` are these 2 columns as they don't exist in the testing values and also not a label required from the models, so they should be removed.","13545c43":"## 3. Visualize according to locations","dbbe23b5":"**Observation:** Now the training data are ready to be used for our models with 21 dimensions and 885669 samples to be traind on","e9c8d9ec":"### Training pipline\nNow, we will train various learners, so we will make a pipline -i.e function- to call it in training the models instead of repeating it with each model. \n\nThe function returns a result dict which contains the train time, fbeta score and log loss of all samples in train data.\n\n**Hint:** We shouldn't use accuracy here as the data is imbalanced."}}