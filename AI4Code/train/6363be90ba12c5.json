{"cell_type":{"85a276d8":"code","db08988e":"code","4d9d53a2":"code","42aaa8b2":"code","6fd0321f":"code","9def42f9":"code","8e4d64bc":"code","769fb142":"code","bd984cd3":"code","60b6da5b":"code","f8185ae6":"code","464a948c":"code","bd07959e":"code","fcd5ee29":"code","f42cedeb":"code","8eb9111e":"code","134fd401":"code","ec68c531":"code","187dc8d6":"code","c0297d6e":"code","dce479ba":"code","341d0c08":"code","0f7b7a13":"code","881293d4":"code","6d80a7b6":"code","da615d38":"code","7115c8c0":"code","5abad9cb":"code","294f9b15":"code","7e027f77":"code","e3817661":"code","72e2d080":"markdown","70ed734e":"markdown","60edc5b1":"markdown","749e9077":"markdown","889c159b":"markdown","90d2b70e":"markdown","aee1222a":"markdown","fac6527c":"markdown","db74dce2":"markdown","a02c8887":"markdown","a70b37b5":"markdown","e1a8c603":"markdown","d803b94f":"markdown","8e2bba60":"markdown","44519083":"markdown"},"source":{"85a276d8":"# Let's get started!\n\n# import libraries\n# Essentials\nimport numpy as np\nimport pandas as pd\nimport datetime\nimport random\n\n# Plots\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Models\nfrom sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, AdaBoostRegressor, BaggingRegressor\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.linear_model import Ridge, RidgeCV\nfrom sklearn.linear_model import ElasticNet, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom mlxtend.regressor import StackingCVRegressor\nimport lightgbm as lgb\nfrom lightgbm import LGBMRegressor\nfrom xgboost import XGBRegressor\nfrom sklearn.ensemble import VotingRegressor\n\n# Misc\nfrom sklearn import model_selection\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import KFold, cross_val_score\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import mean_squared_error, r2_score\nimport sklearn.metrics as metrics\nfrom scipy.stats import norm\n\n","db08988e":"df_train = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\ndf_test = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')\n\n\ndf_train.info()","4d9d53a2":"df_train.head(5)","42aaa8b2":"train_ID = df_train['Id']\ntest_ID = df_test['Id']\ndf_train.drop(['Id'], axis=1, inplace=True)\ndf_test.drop(['Id'], axis=1, inplace=True)\ndf_train.shape, df_test.shape","6fd0321f":"\ndf_train.describe()","9def42f9":"#correlation matrix\ncorrmat = df_train.corr()\nf, ax = plt.subplots(figsize=(12, 9))\nsns.heatmap(corrmat, vmax=.8, square=True)","8e4d64bc":"#saleprice correlation matrix\nk = 10 #number of variables for heatmap\ncols = corrmat.nlargest(k, 'SalePrice')['SalePrice'].index\ncm = np.corrcoef(df_train[cols].values.T)\nsns.set(font_scale=1.25)\nhm = sns.heatmap(cm, cbar=True, annot=True, square=True, fmt='.2f', annot_kws={'size': 10}, yticklabels=cols.values, xticklabels=cols.values)\nplt.show()","769fb142":"#scatterplot\nsns.set()\ncols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(df_train[cols], size = 2.5)\nplt.show();\n","bd984cd3":"#Outlier analysis saleprice\/grlivarea\nvar = 'GrLivArea'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000))","60b6da5b":"#Outlier analysis saleprice\/TotalBsmtSF\nvar = 'TotalBsmtSF'\ndata = pd.concat([df_train['SalePrice'], df_train[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000));","f8185ae6":"#analysing 'SalePrice'\n\n#descriptive statistics summary\ndf_train['SalePrice'].describe()\n#histogram\nsns.distplot(df_train['SalePrice']);\nprint(\"Skewness: %f\" % df_train['SalePrice'].skew())\nprint(\"Kurtosis: %f\" % df_train['SalePrice'].kurt())","464a948c":"#We use the numpy fuction log1p which  applies log(1+x) to all elements of the column SalePrice\ndf_train[\"SalePrice\"] = np.log1p(df_train[\"SalePrice\"])","bd07959e":"#Check the new distribution \nsns.distplot(df_train['SalePrice'] , fit=norm);\n\n# Get the fitted parameters used by the function\n(mu, sigma) = norm.fit(df_train['SalePrice'])\nprint( '\\n mu = {:.2f} and sigma = {:.2f}\\n'.format(mu, sigma))","fcd5ee29":"# Split features and labels\ntrain_labels = df_train['SalePrice'].reset_index(drop=True)\ntrain_features = df_train.drop(['SalePrice'], axis=1)\ntest_features = df_test","f42cedeb":"# Combine train and test features in order to apply the feature transformation pipeline to the entire dataset\nall_features = pd.concat([train_features, test_features]).reset_index(drop=True)\nall_features.shape\n","8eb9111e":"# determine the percentage of missing values\ndef percent_missing(df):\n    data = pd.DataFrame(df)\n    df_cols = list(pd.DataFrame(data))\n    dict_x = {}\n    for i in range(0, len(df_cols)):\n        dict_x.update({df_cols[i]: round(data[df_cols[i]].isnull().mean()*100,2)})\n    \n    return dict_x\n\nmissing = percent_missing(all_features)\ndf_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\nprint('Percent of missing data')\ndf_miss[0:10]","134fd401":"# Visualize missing values\nsns.set_style(\"white\")\nf, ax = plt.subplots(figsize=(8, 7))\nsns.set_color_codes(palette='deep')\nmissing = round(df_train.isnull().mean()*100,2)\nmissing = missing[missing > 0]\nmissing.sort_values(inplace=True)\nmissing.plot.bar(color=\"b\")\n# Tweak the visual presentation\nax.xaxis.grid(False)\nax.set(ylabel=\"Percent of missing values\")\nax.set(xlabel=\"Features\")\nax.set(title=\"Percent missing data by feature\")\nsns.despine(trim=True, left=True)","ec68c531":"# Some of the non-numeric predictors are stored as numbers; convert them into strings \nall_features['MSSubClass'] = all_features['MSSubClass'].apply(str)\nall_features['YrSold'] = all_features['YrSold'].astype(str)\nall_features['MoSold'] = all_features['MoSold'].astype(str)\n","187dc8d6":"def handle_missing(features):\n    # the data description states that NA refers to typical ('Typ') values\n    print(features['Functional'].value_counts())\n    features['Functional'] = features['Functional'].fillna('Typ')\n    # Replace the missing values in each of the columns below with their mode\n    #print(features['Electrical'].value_counts())\n    features['Electrical'] = features['Electrical'].fillna(features['Electrical'].mode()[0])    \n    #print(features['KitchenQual'].value_counts())\n    features['KitchenQual'] = features['KitchenQual'].fillna(features['KitchenQual'].mode()[0])    \n    features['Exterior1st'] = features['Exterior1st'].fillna(features['Exterior1st'].mode()[0])\n    features['Exterior2nd'] = features['Exterior2nd'].fillna(features['Exterior2nd'].mode()[0])\n    features['SaleType'] = features['SaleType'].fillna(features['SaleType'].mode()[0])\n    features['MSZoning'] = features.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\n    \n    # the data description stats that NA refers to \"No Pool\"\n    features[\"PoolQC\"] = features[\"PoolQC\"].fillna(\"None\")\n    # Replacing the missing values with 0, since no garage = no cars in garage\n    for col in ('GarageYrBlt', 'GarageArea', 'GarageCars'):\n        features[col] = features[col].fillna(0)\n    # Replacing the missing values with None\n    for col in ['GarageType', 'GarageFinish', 'GarageQual', 'GarageCond']:\n        features[col] = features[col].fillna('None')\n    # NaN values for these categorical basement features, means there's no basement\n    for col in ('BsmtQual', 'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2'):\n        features[col] = features[col].fillna('None')\n        \n    # Group the by neighborhoods, and fill in missing value by the median LotFrontage of the neighborhood\n    features['LotFrontage'] = features.groupby('Neighborhood')['LotFrontage'].transform(lambda x: x.fillna(x.median()))\n\n    # We have no particular intuition around how to fill in the rest of the categorical features\n    # So we replace their missing values with None\n    objects = []\n    for i in features.columns:\n        if features[i].dtype == object:\n            objects.append(i)\n    features.update(features[objects].fillna('None'))\n        \n    # And we do the same thing for numerical features, but this time with 0s\n    numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    numeric = []\n    for i in features.columns:\n        if features[i].dtype in numeric_dtypes:\n            numeric.append(i)\n    features.update(features[numeric].fillna(0))    \n    return features\n\n\nall_features = handle_missing(all_features)","c0297d6e":"# Let's make sure we handled all the missing values\nmissing = percent_missing(all_features)\ndf_miss = sorted(missing.items(), key=lambda x: x[1], reverse=True)\nprint('Percent of missing data')\ndf_miss[0:10]\n","dce479ba":"all_features.head()","341d0c08":"# Encode categorical features\n# one hot encoding with dummies\n\nall_features = pd.get_dummies(all_features).reset_index(drop=True)\nall_features.shape","0f7b7a13":"#Recreate training and test sets\n\nX = all_features.iloc[:len(train_labels), :]\nX_test = all_features.iloc[len(train_labels):, :]\nX.shape, train_labels.shape, X_test.shape\n\n","881293d4":"#scale data before regression\nscaler = MinMaxScaler()\nscaler.fit(X)\nX = scaler.transform(X)\nX_test = scaler.transform(X_test)\n\nkf = KFold(n_splits=12, random_state=42, shuffle=True)","6d80a7b6":"# model training\nlightgbm = LGBMRegressor(objective='regression', \n                       num_leaves=6,\n                       learning_rate=0.01, \n                       n_estimators=7000,\n                       max_bin=200, \n                       bagging_fraction=0.8,\n                       bagging_freq=4, \n                       bagging_seed=8,\n                       feature_fraction=0.2,\n                       feature_fraction_seed=8,\n                       min_sum_hessian_in_leaf = 11,\n                       verbose=-1,\n                       random_state=42)\n\n# XGBoost Regressor\nxgboost = XGBRegressor(learning_rate=0.01,\n                       n_estimators=6000,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='reg:linear',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=27,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\n# Ridge Regressor\nridge_alphas = [1e-15, 1e-10, 1e-8, 9e-4, 7e-4, 5e-4, 3e-4, 1e-4, 1e-3, 5e-2, 1e-2, 0.1, 0.3, 1, 3, 5, 10, 15, 18, 20, 30, 50, 75, 100]\nridge = make_pipeline(RobustScaler(), RidgeCV(alphas=ridge_alphas, cv=kf))\n\n# Support Vector Regressor\nsvr = make_pipeline(RobustScaler(), SVR(C= 20, epsilon= 0.008, gamma=0.0003))\n\n# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(n_estimators=6000,\n                                learning_rate=0.01,\n                                max_depth=4,\n                                max_features='sqrt',\n                                min_samples_leaf=15,\n                                min_samples_split=10,\n                                loss='huber',\n                                random_state=42)  \n\n# Random Forest Regressor\nrf = RandomForestRegressor(n_estimators=1200,\n                          max_depth=15,\n                          min_samples_split=5,\n                          min_samples_leaf=5,\n                          max_features=None,\n                          oob_score=True,\n                          random_state=42)\n\n","da615d38":"votingreg = VotingRegressor([('lightgbm', lightgbm), ('xgboost', xgboost),('ridge',ridge),('svr',svr),('gbr',gbr),('rf',rf)])","7115c8c0":"x_train, x_test, y_train, y_test = model_selection.train_test_split(X, train_labels, train_size = 0.8)\n\ntest_pred = votingreg.fit(x_train, y_train).predict(x_test)","5abad9cb":"print('MAE:', metrics.mean_absolute_error(y_test, test_pred))\nprint('MSE:', metrics.mean_squared_error(y_test, test_pred))\nprint('RMSE:', np.sqrt(metrics.mean_squared_error(y_test, test_pred)))\nprint('r2_score:',r2_score(y_test, test_pred))","294f9b15":"# prediction on test data\nfinal_pred = votingreg.fit(x_train, y_train).predict(X_test)","7e027f77":"test_pred_df = pd.DataFrame(final_pred , columns=['SalePrice'])\ntest_id_df = pd.DataFrame(test_ID, columns=['Id'])\n\nsubmission = pd.concat([test_id_df, np.expm1(test_pred_df)], axis=1)\nsubmission.head()\n","e3817661":"submission.to_csv('submission.csv', index=False)","72e2d080":"### Model building","70ed734e":"### Exploratory Data Analysis (EDA)","60edc5b1":"### Handling Missing data","749e9077":"#### Correlation","889c159b":"### Creating submission file","90d2b70e":"In the feature engineering, we can remove outliers or any coloum. However right now I wish to go with all predictors, so not removing anything.","aee1222a":"salesprize is deviated from the normal distribution.Have positive skewness.\nShow peakedness","fac6527c":"#### Data understanding","db74dce2":"### Feature Engineering","a02c8887":"We can feel tempted to eliminate some observations (e.g. TotalBsmtSF > 3000). Since I don't want to remove any pattern so not removing outlier.","a70b37b5":"Though few features are having more than 80% missing data; I am not going to remove them right now.. since I want to build model with all data.","e1a8c603":"The major objective is to predict house prizes with regression models.\n\nThe overall flow is data understanding, data engineering, handling missing data, encoding categorical data,\nfeature engineering and model building. This notebook aims to predict sales prizes using voting \nregressor model.\n\nI encourage you to fork this kernel, play with the code. Good luck!\n\nIf you like this kernel, please give it an upvote. Thank you!\n","d803b94f":"#### Outliers analysis using scatter plots","8e2bba60":"### Encode Categorical features","44519083":"#### Scatter plots between 'SalePrice' and correlated variables \n"}}