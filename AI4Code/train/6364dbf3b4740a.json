{"cell_type":{"1e80167b":"code","f4d6b418":"code","cf756754":"code","d4bf8f49":"code","a5db985a":"code","7212ec2b":"code","20384996":"code","f698a938":"code","b6d1f7cf":"code","36fd5766":"code","5dd69201":"code","e832655d":"code","3a5fd093":"code","054eb155":"code","c9acc430":"code","e78e256b":"code","e193185e":"code","af67abbd":"code","14f39127":"code","29ba47b7":"markdown","a4161943":"markdown","2e0472be":"markdown","77b5a79d":"markdown","b9f29f74":"markdown","3bf007f1":"markdown","eb6d839a":"markdown","e91f2686":"markdown","7b94865e":"markdown","5ce2babe":"markdown","58674801":"markdown","0e2a801e":"markdown","d8957961":"markdown","3f2a9903":"markdown","6094a551":"markdown","6b3f158f":"markdown"},"source":{"1e80167b":"import glob, os\nimport numpy as np\nfrom numpy import save, load\nimport pandas as pd\nimport itertools\nimport random\n\n# matplot\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport seaborn as sns\n%matplotlib inline\nfrom IPython.display import Image, clear_output  # to display images\nnp.random.seed()\nsns.set(style='white', context='notebook', palette='deep')\n","f4d6b418":"!git clone https:\/\/github.com\/ultralytics\/yolov5  # clone repo\n%cd yolov5\n%pip install -qr requirements.txt  # install dependencies\n\nimport torch\n\nprint('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))","cf756754":"%cd '\/kaggle\/working\/yolov5'\n%ls","d4bf8f49":"!mkdir training\n%ls\n%cd training\n!mkdir data\n!mkdir data\/images\n!mkdir data\/labels\n!mkdir data\/images\/train\n!mkdir data\/images\/valid\n!mkdir data\/labels\/train\n!mkdir data\/labels\/valid\n\n%cd '\/kaggle\/working\/yolov5'\n%ls -R | grep \":$\" | sed -e 's\/:$\/\/' -e 's\/[^-][^\\\/]*\\\/\/--\/g' -e 's\/^\/   \/' -e 's\/-\/|\/'","a5db985a":"# Showcase a typical training image\nImage(filename='\/kaggle\/input\/robocup-images\/20201202_field_filtered\/3dsense_2020_12_2_19_53_35_burst_27_color.jpg', width=600)","7212ec2b":"# Showcase VocXML annotation\nlog = open('\/kaggle\/input\/robocup-images\/20201202_field_filtered\/VOCXML\/3dsense_2020_12_2_19_53_35_burst_27_color.xml', 'r').read()\nprint (log)","20384996":"#\n# @author:  Noah van der Meer\n# @brief:   VOC XML to YOLO annotation converter\n#\n# @date:    28-01-2021\n#\n\nfrom typing import List, Tuple, Dict  # for Python type hinting\/static typing\n\nimport os                       # listing and reading files\nimport xml.dom.minidom          # parsing XML in DOM\n\ndef voc_xml_to_yolo(dom, classes: Dict[str, int]) -> str:\n    \"\"\" Convert from VOC XML to Yolo annotation format\n\n    Parameters\n    ----------\n    dom : \n        loaded xml document\n\n    classes : Dict\n        class name -> index map\n\n\n    The YOLO format:\n    <class> <x> <y> <width> <height>\n\n    where\n    <class>             - integer number of object from 0 to (numClasses - 1)\n    <x> <y>             - relative x,y coordinates of _center_ of bounding box\n    <width> <height>    - relative bounding box width\/height\n\n    More information about the YOLO format:\n    https:\/\/github.com\/AlexeyAB\/Yolo_mark\/issues\/60\n\n\n    \n    Returns\n    -------\n    str :\n        annotations encoded in YOLO format\n\n    \"\"\"\n\n    output = \"\"\n\n    annotation = dom.documentElement\n    \n    # Determine image size\n    size = annotation.getElementsByTagName(\"size\")[0]\n    imgWidth: int = int(size.getElementsByTagName(\"width\")[0].childNodes[0].data)\n    imgHeight: int = int(size.getElementsByTagName(\"height\")[0].childNodes[0].data)\n\n    # Iterate through objects\n    objects = annotation.getElementsByTagName(\"object\")\n    for obj in objects:\n        # Look up class\n        name: str = obj.getElementsByTagName(\"name\")[0].childNodes[0].data\n        name.replace(\" \", \"\")\n        if name in classes:\n            classId = classes[name]\n        else:\n            print(\"-- voc_xml_to_yolo() error: class '\", name, \n                    \"' not known! Skipping object\")\n            continue\n\n        # Interpret bounding box\n        bndbox = obj.getElementsByTagName(\"bndbox\")[0]\n        xmin: int = int(bndbox.getElementsByTagName(\"xmin\")[0].childNodes[0].data)\n        ymin: int = int(bndbox.getElementsByTagName(\"ymin\")[0].childNodes[0].data)\n        xmax: int = int(bndbox.getElementsByTagName(\"xmax\")[0].childNodes[0].data)\n        ymax: int = int(bndbox.getElementsByTagName(\"ymax\")[0].childNodes[0].data)\n        \n        xCenter: float = (xmin + xmax) \/ (2 * imgWidth)\n        yCenter: float = (ymin + ymax) \/ (2 * imgHeight)\n        bndWidth: float = (xmax - xmin) \/ imgWidth\n        bndHeight: float = (ymax - ymin) \/ imgHeight\n\n        output = output + str(classId) + \" \" + str(xCenter) + \" \" + str(yCenter) + \" \" + str(bndWidth) + \" \" + str(bndHeight) + \"\\n\"\n\n    return output\n\n\ndef voc_xml_directory_to_yolo(sourceDirectory: str, targetDirectory: str, classes: Dict[str, int]):\n    \"\"\" Convert from VOC XML annotations to Yolo annotation format\n\n    Parameters\n    ----------\n    sourceDirectory : str\n        path to source directory containing annotations in VOC XML format. This method\n        will attempt to load any file with the *.xml extension, and it ignores other files\n\n    targetDirectory : str\n        target directory for writing the annotation files in Yolo format\n\n    classes : Dict\n        class name -> index map\n\n    \"\"\"\n\n    files: List[str] = os.listdir(sourceDirectory)\n    print(\"voc_xml_directory_to_yolo() : found\", len(files), \"files in directory\", sourceDirectory)\n\n    for filename in files:\n\n        # check file extension\n        if len(filename) < 3:\n            continue\n        if filename[-3:] != \"xml\":\n            continue\n        \n        targetFilename = filename[0:-3] + \"txt\"\n        print(\"voc_xml_directory_to_yolo() :\", filename, \"->\", targetFilename)\n\n        dom = xml.dom.minidom.parse(sourceDirectory + \"\/\" + filename)\n        yl: str = voc_xml_to_yolo(dom, classes)\n\n        # write to file\n        fo = open(targetDirectory + \"\/\" + targetFilename, \"w\")\n        fo.write(yl)\n        fo.close()\n\n","f698a938":"# Showcase Yolo annotation\nlog = open('\/kaggle\/input\/robocup-images\/20201202_field_filtered\/YOLO\/3dsense_2020_12_2_19_53_35_burst_27_color.txt', 'r').read()\nprint (log)","b6d1f7cf":"def read_directory_imagehub(dataset_path):\n  # Percentage of images to be used for the validation set\n  print(\"Loading from \", dataset_path)\n  percentage_test = 20\n  p = percentage_test\/100\n  # Populate the folders\n  count=0\n  for pathAndFilename in glob.iglob(os.path.join(dataset_path, \"*.jpg\")):  \n      title, ext = os.path.splitext(os.path.basename(pathAndFilename))\n      if random.random() <=p :\n          os.system(f\"cp {dataset_path}\/{title}.jpg data\/images\/valid\")\n          os.system(f\"cp {dataset_path}\/YOLO\/{title}.txt data\/labels\/valid\")\n      else:\n          os.system(f\"cp {dataset_path}\/{title}.jpg data\/images\/train\")\n          os.system(f\"cp {dataset_path}\/YOLO\/{title}.txt data\/labels\/train\")\n      count=count+1\n  print('loaded ',count, 'images')\n  return ","36fd5766":"#Read the image hub directories and load the images\/annotations in the YOLO training directories\n%cd '\/kaggle\/working\/yolov5\/training'\nread_directory_imagehub('\/kaggle\/input\/robocup-images\/20201202_field_filtered')\nread_directory_imagehub('\/kaggle\/input\/robocup-images\/20201014_goalpictures')","5dd69201":"#count the amount of training images\n%cd '\/kaggle\/working\/yolov5\/training\/data\/images\/train'\n%ls | wc -l","e832655d":"%cd '\/kaggle\/working\/yolov5\/'\n%ls models","3a5fd093":"# Copy the 2 prepared yaml dataset from the robocup images dataset\n%cp '\/kaggle\/input\/robocup-images\/dataset.yaml' '\/kaggle\/working\/yolov5\/training\/dataset.yaml'\n%cp '\/kaggle\/input\/robocup-images\/yolov5s.yaml' '\/kaggle\/working\/yolov5\/training\/yolov5s.yaml'\n%cd '\/kaggle\/working\/yolov5\/training\/'\n%ls\nlog = open('\/kaggle\/working\/yolov5\/training\/dataset.yaml', \"r\").read()\nprint (log)","054eb155":"#change epochs to 300 for a full run\n%cd '\/kaggle\/working\/yolov5'\n!wandb off\n!python train.py --img 672 --batch 16 --epochs 30 --data training\/dataset.yaml --cfg training\/yolov5s.yaml --weights '' --device 0","c9acc430":"%cd '\/kaggle\/working\/yolov5\/runs\/train\/exp'\n%ls","e78e256b":"# Show the predictions\nImage(filename='\/kaggle\/working\/yolov5\/runs\/train\/exp'+'\/val_batch0_pred.jpg', width=800)","e193185e":"# Show the predictions\nImage(filename='\/kaggle\/working\/yolov5\/runs\/train\/exp'+'\/val_batch0_labels.jpg', width=800)\n","af67abbd":"Image(filename='\/kaggle\/working\/yolov5\/runs\/train\/exp'+'\/PR_curve.png', width=800)\n","14f39127":"Image(filename='\/kaggle\/working\/yolov5\/runs\/train\/exp'+'\/results.png', width=800)\n","29ba47b7":"<a id='sec4'><\/a>\n## 4.   Real-time inference on a Jetson Nano on the robot\nThe ONNX model is imported and run on a tensortRT Nvidia GPU network optimization for low cost HW. For the latter the NVIDIA Jetson platform has been used It has a small form factor, has desirable power characteristics, and is low-cost. For instance, the entry-level Jetson Nano can be obtained for as little as 60 Eur and possesses enough computing power to run the YOLOv5 models in real-time.\n\nTo perform object detection on the NVIDIA Jetson platform, the TensorRT software is used, which is specialized in inference on NVIDIA hardware. The ONNX format is imported and fully optimized for inference on the specific device. This optimization step is performed once, which leads to an engine that can be used repeatedly for real-time object detection on the robot. Using this approach, inference can be performed within 40 ms on the Jetson Nano, resulting in a processing framerate of 25~fps. For this step, the open-source YOLOv5-TensorRT library https:\/\/github.com\/noahmr\/yolov5-tensorrt has been developed.\n","a4161943":"### Preparing Yolo V5 training\n\nYolo needs 2 yaml files set-up properly in your training directory.\n+ One yaml file specifying the model (small, medium, large, Xlarge). In our case we use the small model for best performance on a low cost Jetson nano. The files are available in the models directory. You need to make one change to the file and enter the correct number of classes parameter, in our case 5 classes (nc: 5  # number of classes)\n+ One yaml file specifying the data. You need to create the file dataset.yaml manually with the correct classes and class names. Below I printed an example file, the one we used for the training.","2e0472be":"###  Prepare training data","77b5a79d":"\nLet's create the directory structure needed for Yolo V5 training:\n```\nYolov5\/\n...data\/\n...models\/\n...robocup_ml\/\n...training\/\n......data\/\n.........images\/\n............train\/\n............valid\/\n.........labels\/\n............train\/\n............valid\/\n```","b9f29f74":"### Prepare training data ","3bf007f1":"### Statistics\nThe main statistics to watch are:\n* **Precision** is the ability of a model to identify only the relevant objects. mAP is the Mean Average Precision.  It is the percentage of correct positive predictions and is given by:  $\\frac{True Positive}{True Positive + False Positive}=\\frac{True Positive}{All detections}$. Where True Positive is a correct detection, a detection with IOU (Intersection Over Union) \u2265 threshold  \n\n<img src=\"https:\/\/github.com\/rafaelpadilla\/Object-Detection-Metrics\/blob\/master\/aux_images\/iou.png?raw=true\">\n\n\n* **Recall** is the ability of a model to find\/recall all the relevant cases (all ground truth bounding boxes). It is the percentage of true positive detected among all relevant ground truths and is given by: $\\frac{True Positive}{True Positive + False Negative}=\\frac{True Positive}{All ground truths}$ where False negative = A ground truth not detected.\n\n\nSee [here](https:\/\/github.com\/rafaelpadilla\/Object-Detection-Metrics) for some good explanation.","eb6d839a":"<a id='sec1'><\/a>\n## 1. Collect & label images\nThe data is the \"oil\" of machine learning, the key to success. The good-old-fashioned garbage-in-garbage-out applies. Hence good management of your data (in this case annotated images) is important.\n\nWe organized the training data set up in a 2-stage approach. \n* The **imagehub** will contain all the original images grouped by certain categories: events or places (e.g. 2020 France tournament). It contains all the images ever collected for training.\n    *  Image annotation will be done in VOC XML file format (is a more universal standard) and not proprietary Yolo format. There are tools to translate from VOC XML to the simpler Yolo format.\n    *  Yolo and other well-known algorithms like SSD (Single Shot Detector) Mobilenet support multiple image sizes. Still, we try to standardize all our training material and use the image format of the camera mounted on the robots: Zed2, which is 672*376 pixels\n    *  To avoid issues with different image types make sure all your images are in the jpg format\n* The **training dataset** needed for a specific training, which are selected images from the datahub. The training dataset structure will be organized as required for the specific ML platform, in this case, Yolo V5:\n    *   Make sure all your images are in the jpg format\n    *   Annotations need to be in the YOLO text file format\n    *   YoloV5 trains on square images, you only supply the longest dimension, e.g. --img 800, The rest is handled automatically. Train and detect at the same --img-size for best results. The default img size is 640. For our images\/training we use image size 672 (the largest dimension of the image)\n    * YoloV5 internal workings: python train.py --img 640 means that the mosaic dataloader pulls up the selected image along with 3 random images, resizes them all to 640, joins all 4 at the seams into a 1280x1280 mosaic, augments them, and then crops a center 640x640 area for placement as 1 image into the batch.\n\n### Labeling tool\nyou need a toolset to label images. We have used [labelImg](https:\/\/github.com\/tzutalin\/labelImg). It is free and is a popular choice. LabelImg supports labeling in VOC XML or YOLO text file format. Use the default VOC XML format for creating labels (is a more universal standard). \n\nEach image will have one .xml file that has its labels. If there is more than one class or one label in an image, that .xml file will include them all.\n\nYou can always easily convert from VOC XML to any other format using some conversion software, see code below.\n\n### Labeling Best Practices\nSome of the good advices I did find in the various tutorials:\n*   Label around the entirety of an object. It is best to include a little bit of non-object buffer than it is to exclude a portion of the object with a rectangular label. So, aim to have boxes that tightly mirror the objects you want to label, but do not cut off part of the objects. Your model will understand edges far better this way.\n*   For occluded objects, label them entirely. If an object is out of view due to another object being in front of it, label the object out of view as if you could see its entirety. Your model will begin to understand the true bounds of objects this way.\n*   For objects partially out of frame, generally, label them. In general, even a partial object is still an object to be labeled.\n\n\n### LabelImg Shortcuts to label efficiently.\n```\nCtrl + u - Load all of the images from a directory\nCtrl + r - Change the default annotation target dir\nCtrl + s - Save\nw - Create a rect box\nd - Next image\na - Previous image\ndel - Delete the selected rect box\nCtrl++ - Zoom in\nCtrl-- - Zoom out\nCtrl + d - Copy the current label and rect box\nSpace - Flag the current image as verified\n\u2191\u2192\u2193\u2190Keyboard arrows to move selected rect box\n```\n","e91f2686":"### Yolo v5 training","7b94865e":"## Object detection with Yolo V5\n\nThe field of AI produced a number of ever-improving deep Learning Algorithms for Object Detection. Here you find a nice [article](https:\/\/medium.com\/zylapp\/review-of-deep-learning-algorithms-for-object-detection-c1f3d437b852) to give an overview. It can be best summarized in the picture below.\n\n<center><img src=\"https:\/\/miro.medium.com\/max\/788\/1*Hz6t-tokG1niaUfmcysusw.jpeg\" width=400><\/center>\n\nOne of the best-known algorithms for object detection is Yolo (You-Only-Look-Once). Also, Yolo has developed over time and successive versions have been released since its first inception in May 2016. One of the latest releases is YoloV5 with very promising results in terms of accuracy vs speed (high Frames Per Second, FPS, and low latency) with a small SW footprint suitable for mobile devices.\n\nThere are a few model sizes of YoloV5 available, basically predefined models from small to XXL (see [table](https:\/\/github.com\/ultralytics\/yolov5#pretrained-checkpoints) for details):\n\n<center><img src=\"https:\/\/user-images.githubusercontent.com\/26833433\/97808084-edfcb100-1c64-11eb-83eb-ffed43a0859f.png\" width=600><\/center>\n\nTo get the inferencing as fast as possible, the small model has been selected.\n\nThe objects we want to recognize are:\n* ball\n* robot\n* goalpost\n* human\n* goal\n\nYOLO resources used: [Github](https:\/\/github.com\/ultralytics\/yolov5), [colab tutorial](https:\/\/colab.research.google.com\/github\/ultralytics\/yolov5\/blob\/master\/tutorial.ipynb#scrollTo=wbvMlHd_QwMG), [Kaggle tutorial](https:\/\/www.kaggle.com\/ultralytics\/yolov5-ultralytics),\n[Yolo v5 Object Detection Tutorial](https:\/\/towardsdatascience.com\/yolo-v5-object-detection-tutorial-2e607b9013ef), [How to Train YOLOv5 On a Custom Dataset](https:\/\/blog.roboflow.com\/how-to-train-yolov5-on-a-custom-dataset\/), [YOLOv5 is Here!](https:\/\/towardsdatascience.com\/yolo-v5-is-here-b668ce2a4908), [How to Create an End to End Object Detector using Yolov5?](https:\/\/towardsdatascience.com\/how-to-create-an-end-to-end-object-detector-using-yolov5-35fbb1a02810), [thesis evolution of YOLO algorithm and YOLOV5](https:\/\/www.theseus.fi\/bitstream\/handle\/10024\/452552\/Do_Thuan.pdf?sequence=2&isAllowed=y)\n","5ce2babe":"\n# Continuous digital feedback loop\n\nThe continuous digital feedback loop is a model used by Tesla in its fleet of cars. The goal of the model is to continuously improve the behavior and intelligence of the fleet by continually collecting data,learning from new images and re-deploying the re-trained networks. This architecture has been adapted for use on the robots, as shown in Figure below. \n\n<center><img src=\"https:\/\/i.imgur.com\/P7T5ggA.jpg\" width=500><\/center>\n\nThe continuous digital feedback loop consists of the followingfour steps:\n\n1.   [Collect & label images](#sec1)   \n    * New images are captured during every game, providing new training material. \n    * Also new images can be captured during set-up day (before the games) so the model can be trained on game location with the specifics of that soccer field (e.g. different lighting conditions)\n2.   [Training Yolo model in the cloud](#sec2)   \n    * The Yolo set-up, loading of the images and the YoloV5 training\n3.   [Deploying the model to the robots](#sec3)  \n    * The Yolo V5 model is exported as an ONNX exchange standard \n4.   [Real-time inference on a Jetson Nano on the robot](#sec4)  \n    * using video images coming from the Zed2 real-time camera stream on the robot\n    * The ONNX model is imported and run on a tensortRT Nvidia GPU network optimization for low cost HW \n\n","58674801":"# Object recognition on a robot soccer fied with YoloV5\n\n<center><img src=\"https:\/\/i.imgur.com\/4FUfbC0.png\" width=800><\/center>\n<center>Image from <a href=\"https:\/\/www.robocup.org\/leagues\/6\"> Robocup<\/a>  <\/center>\n\n  \nThis is a notebook about low-cost fast mobile object detection for robot soccer. Wait, robot soccer? Yes, complete leagues are playing world championships with the objective:   \n> \"By the middle of the 21st century, a team of fully autonomous humanoid robot soccer players shall win a soccer game, complying with the official rules of FIFA, against the winner of the most recent World Cup\" \n  \nWelcome to the world of really cool tech!  \n  \nThis work did win the **first place in the scientific challenge in the world championships robotic soccer 2021** middle size league (MSL). See the Robocup [webiste](https:\/\/www.robocup.org\/leagues\/6) for more information about the competitions of the different leagues. The MSL can always enjoy lots of public attention ([video](https:\/\/youtu.be\/MXztr3BxY5w?t=1363)).\n\nThe objective is real-time recognition of objects on the soccer field. It needs to be fast (no need to recognize a ball after it already passed), mobile (recognition on the robot's local compute, no real-time connection to a large data center), and low-cost (it needs to be affordable for all teams). \n\n![](https:\/\/i.imgur.com\/TP0IMPL.jpg)\n\n","0e2a801e":"<a id='sec2'><\/a>\n## 2. Training Yolo model in the cloud \n\n### Kaggle dataset\nThe annotated training images have been made available in a Kaggle dataset, which is loaded in this notebook.\n","d8957961":"<a id='sec3'><\/a>\n## 3.   Deploying the model to the robots \n\nOnce training is complete, the resulting network is deployed on the robots. The Yolo V5 model along with the trained weights is exported as an Open Neural Network Exchange (ONNX) exchange standard. ONNX facilitates open interoperability between AI platforms, and is supported by many popular AI frameworks such as TensorFlow, MXNet and PyTorch. This is also necessary for the next step, since training is done in the cloud using the PyTorch-based YOLOv5, while the inference hardware on the robots is running the NVIDIA TensorRT framework.  \n\nThe complete deployment workflow can be seen in the picture below:\n\n<center><img src=\"https:\/\/i.imgur.com\/mDCE3Rq.jpg\" width=800><\/center>\n\n\n","3f2a9903":"## The results\n\nSee for yourself, it needed only 2 hours of training on a cloud GPU and only 1000 annotated images.\n\n<center><img src=\"https:\/\/i.imgur.com\/xitYmql.gif\" width=600><\/center>\n","6094a551":"### Yolo V5 setup\nClone the yolo V5 repository from GitHub, install dependencies and check PyTorch and GPU. It will create a folder called \u2018yolov5\u2019.","6b3f158f":"### Conversion VOCXML to YOLO annotation format\n\nThe code below converts whole directories, for example by \n```\nclasses = {\"ball\": 0, \"robot\": 1, \"human\": 2, \"goalpost\": 3, \"goal\": 4}\n\nDIR = 'Your image directory'  \n\nvoc_xml_directory_to_yolo(DIR + '\/VOCXML', \n                        DIR +'\/YOLO',\n                        classes)\n```\n\nThe robocup-images dataset already contains both the VOCXML and YOLO annotation labels, so you can directly use them."}}