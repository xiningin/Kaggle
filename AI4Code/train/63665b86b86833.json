{"cell_type":{"2b231848":"code","944bb453":"code","ea853d71":"code","798d3052":"code","1f3e2a0e":"code","d745e8f6":"code","5edde9a5":"code","1d8125a1":"code","9b5dbfdb":"code","012769f3":"code","d88b4494":"code","5786329d":"code","af042fad":"code","7984cbdc":"code","46a18ddc":"code","e86fa27d":"code","b6579934":"code","cad03b31":"code","2d02217a":"code","fa258f5d":"code","55c5465e":"code","cedd6e3f":"code","40d84981":"code","e69d2231":"code","eebb7549":"code","bf90cc03":"code","b5f660cd":"code","5c62e323":"code","8c247754":"code","d1288610":"code","7d5e66df":"code","c846ae86":"code","b1c8f7aa":"code","3aa76226":"code","20db6ca4":"code","7bdbfa08":"code","c1ef05c5":"code","e45f6cd2":"code","89c48f59":"code","c5369b2e":"code","070f4e06":"code","65c90363":"code","04634865":"code","4bc08d98":"code","1a88a269":"code","28bb7aa0":"code","5e44c90e":"code","4e66e5df":"markdown","05354e34":"markdown","407c6340":"markdown","3de62d61":"markdown","419bd54c":"markdown","e63e8916":"markdown","e2bff80f":"markdown","fbe17112":"markdown","ab9bbbd1":"markdown","d9beb4cd":"markdown","ea104f7a":"markdown","702b6b1f":"markdown","a592b23e":"markdown","2c6a7832":"markdown","f464e0e4":"markdown","cd2abcef":"markdown","22ef99b5":"markdown","1eebc9ab":"markdown","39f76fb9":"markdown","9cff9638":"markdown","79bf879e":"markdown","cbfe688d":"markdown","f5a07af6":"markdown","8b4921e9":"markdown","01e3a5f5":"markdown","657c5dad":"markdown","74d59541":"markdown","5240a03d":"markdown","c7ad6e9a":"markdown","49408df9":"markdown","f120c68d":"markdown","331ec228":"markdown","73e742ac":"markdown","f15e3c34":"markdown","69b67c7b":"markdown","57fbb107":"markdown","791dcfb3":"markdown","77a469f4":"markdown","397c5120":"markdown","079dfe18":"markdown"},"source":{"2b231848":"%matplotlib inline\nimport theano\nimport pymc3 as pm\nimport sklearn\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom warnings import filterwarnings\nfilterwarnings('ignore')\nsns.set_style('white')\nfrom sklearn import datasets\nfrom sklearn.preprocessing import scale\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.datasets import make_moons, make_circles, make_blobs","944bb453":"X, Y = make_moons(noise=0.2, random_state=0, n_samples=1000)\nX = scale(X)\nX = X.astype(float)\nY = Y.astype(float)\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.5)","ea853d71":"fig, ax = plt.subplots(figsize=(12, 8))\nax.scatter(X[Y==0, 0], X[Y==0, 1], label='Class 0')\nax.scatter(X[Y==1, 0], X[Y==1, 1], color='r', label='Class 1')\nsns.despine(); ax.legend()\nax.set(xlabel='X', ylabel='Y', title='Toy binary classification data set');","798d3052":"# just to see the dimensions o four data\nprint(X_train.shape)\nprint(Y_train.shape)","1f3e2a0e":"def construct_nn(ann_input, ann_output):\n    n_hidden = 5 # Number of neurons in each hidden layer\n    \n    # Initialize random weights between each layer\n    init_1 = np.random.randn(X.shape[1], n_hidden).astype(float)\n    init_2 = np.random.randn(n_hidden, n_hidden).astype(float)\n    init_out = np.random.randn(n_hidden).astype(float)\n        \n    with pm.Model() as neural_network:\n        # Weights from input to hidden layer\n        weights_in_1 = pm.Normal('w_in_1', 0, sd=1, \n                                 shape=(X.shape[1], n_hidden), \n                                 testval=init_1)\n        \n        # Weights from 1st to 2nd layer\n        weights_1_2 = pm.Normal('w_1_2', 0, sd=1, \n                                shape=(n_hidden, n_hidden), \n                                testval=init_2)\n        \n        # Weights from hidden layer to output\n        weights_2_out = pm.Normal('w_2_out', 0, sd=1, \n                                  shape=(n_hidden,), \n                                  testval=init_out)\n        \n        # Build neural-network using tanh activation function\n        act_1 = pm.math.tanh(pm.math.dot(ann_input, \n                                         weights_in_1))\n        act_2 = pm.math.tanh(pm.math.dot(act_1, \n                                         weights_1_2))\n        act_out = pm.math.sigmoid(pm.math.dot(act_2, \n                                              weights_2_out))\n        \n        # Binary classification -> Bernoulli likelihood\n        out = pm.Bernoulli('out', \n                           act_out,\n                           observed=ann_output,\n                           total_size=Y_train.shape[0] # IMPORTANT for minibatches\n                          )\n    return neural_network\n\n# Trick: Turn inputs and outputs into shared variables. \n# It's still the same thing, but we can later change the values of the shared variable \n# (to switch in the test-data later) and pymc3 will just use the new data. \n# Kind-of like a pointer we can redirect.\n# For more info, see: http:\/\/deeplearning.net\/software\/theano\/library\/compile\/shared.html\nann_input = theano.shared(X_train)\nann_output = theano.shared(Y_train)\nneural_network = construct_nn(ann_input, ann_output)","d745e8f6":"from pymc3.theanof import set_tt_rng, MRG_RandomStreams\nset_tt_rng(MRG_RandomStreams(42))","5edde9a5":"%%time\n\nwith neural_network:\n    inference = pm.ADVI()\n    approx = pm.fit(n=50000, method=inference)","1d8125a1":"trace = approx.sample(draws=5000)","9b5dbfdb":"plt.plot(-inference.hist)\nplt.ylabel('ELBO')\nplt.xlabel('iteration');","012769f3":"# Replace arrays our NN references with the test data\nann_input.set_value(X_test)\nann_output.set_value(Y_test)\n\nwith neural_network:\n    ppc = pm.sample_posterior_predictive(trace, samples=500, progressbar=False)\n\n# Use probability of > 0.5 to assume prediction of class 1\npred = ppc['out'].mean(axis=0) > 0.5","d88b4494":"fig, ax = plt.subplots()\nax.scatter(X_test[pred==0, 0], X_test[pred==0, 1])\nax.scatter(X_test[pred==1, 0], X_test[pred==1, 1], color='r')\nsns.despine()\nax.set(title='Predicted labels in testing set', xlabel='X', ylabel='Y');","5786329d":"print('Accuracy = {}%'.format((Y_test == pred).mean() * 100))","af042fad":"grid = pm.floatX(np.mgrid[-3:3:100j,-3:3:100j])\ngrid_2d = grid.reshape(2, -1).T\ndummy_out = np.ones(grid.shape[1], dtype=np.int8)","7984cbdc":"ann_input.set_value(grid_2d)\nann_output.set_value(dummy_out)\n\nwith neural_network:\n    ppc = pm.sample_posterior_predictive(trace, samples=500, progressbar=False)","46a18ddc":"cmap = sns.diverging_palette(250, 12, s=85, l=25, as_cmap=True)\nfig, ax = plt.subplots(figsize=(14, 8))\ncontour = ax.contourf(grid[0], grid[1], ppc['out'].mean(axis=0).reshape(100, 100), cmap=cmap)\nax.scatter(X_test[pred==0, 0], X_test[pred==0, 1])\nax.scatter(X_test[pred==1, 0], X_test[pred==1, 1], color='r')\ncbar = plt.colorbar(contour, ax=ax)\n_ = ax.set(xlim=(-3, 3), ylim=(-3, 3), xlabel='X', ylabel='Y');\ncbar.ax.set_ylabel('Posterior predictive mean probability of class label = 0');","e86fa27d":"cmap = sns.cubehelix_palette(light=1, as_cmap=True)\nfig, ax = plt.subplots(figsize=(14, 8))\ncontour = ax.contourf(grid[0], grid[1], ppc['out'].std(axis=0).reshape(100, 100), cmap=cmap)\nax.scatter(X_test[pred==0, 0], X_test[pred==0, 1])\nax.scatter(X_test[pred==1, 0], X_test[pred==1, 1], color='r')\ncbar = plt.colorbar(contour, ax=ax)\n_ = ax.set(xlim=(-3, 3), ylim=(-3, 3), xlabel='X', ylabel='Y');\ncbar.ax.set_ylabel('Uncertainty (posterior predictive standard deviation)');","b6579934":"import numpy as np\nnp.max( ppc['out'].std(axis=0))","cad03b31":"minibatch_x = pm.Minibatch(X_train, batch_size=32)\nminibatch_y = pm.Minibatch(Y_train, batch_size=32)\n\nneural_network_minibatch = construct_nn(minibatch_x, minibatch_y)\nwith neural_network_minibatch:\n    inference = pm.ADVI()\n    approx = pm.fit(40000, method=inference)","2d02217a":"plt.plot(-inference.hist)\nplt.ylabel('ELBO')\nplt.xlabel('iteration');","fa258f5d":"trace_VI = approx.sample(draws=10000)\npm.traceplot(trace_VI);","55c5465e":"X, Y = blobs =  datasets.make_circles(n_samples=2000, factor=0.5, noise=0.05) #datasets.make_blobs(random_state=0, n_samples=1000)\nX = scale(X)\nX = X.astype(float)\nY = Y.astype(float)\nX_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.2)","cedd6e3f":"fig, ax = plt.subplots(figsize=(12, 8))\nax.scatter(X[Y==0, 0], X[Y==0, 1], label='Class 0')\nax.scatter(X[Y==1, 0], X[Y==1, 1], color='r', label='Class 1')\nsns.despine(); ax.legend()\nax.set(xlabel='X', ylabel='Y', title='Toy binary classification data set');","40d84981":"# just to see the dimensions o four data\nprint(X_train.shape)\nprint(Y_train.shape)","e69d2231":"\ndef construct_nn(ann_input, ann_output):\n    n_hidden = 2 # Number of neurons in each hidden layer\n    \n    # Initialize random weights between each layer\n    init_1 = np.random.randn(X.shape[1], n_hidden).astype(float)\n    init_2 = np.random.randn(n_hidden, n_hidden).astype(float)\n    init_out = np.random.randn(n_hidden).astype(float)\n        \n    with pm.Model() as neural_network:\n        # Weights from input to hidden layer\n        weights_in_1 = pm.Normal('w_in_1', 0, sd=1, \n                                 shape=(X.shape[1], n_hidden), \n                                 testval=init_1)\n        \n        # Weights from 1st to 2nd layer\n        weights_1_2 = pm.Normal('w_1_2', 0, sd=1, \n                                shape=(n_hidden, n_hidden), \n                                testval=init_2)\n        \n        # Weights from hidden layer to output\n        weights_2_out = pm.Normal('w_2_out', 0, sd=1, \n                                  shape=(n_hidden,), \n                                  testval=init_out)\n        \n        # Build neural-network using tanh activation function\n        act_1 = pm.math.tanh(pm.math.dot(ann_input, \n                                         weights_in_1))\n        \n        act_2 = pm.math.tanh(pm.math.dot(act_1, \n                                         weights_1_2))\n        \n        act_out = pm.math.sigmoid(pm.math.dot(act_2, \n                                              weights_2_out))\n        \n        # Binary classification -> Bernoulli likelihood\n        out = pm.Bernoulli('out', \n                           act_out,\n                           observed=ann_output,\n                           total_size=Y_train.shape[0] # IMPORTANT for minibatches\n                          )\n    return neural_network\n\n# Trick: Turn inputs and outputs into shared variables. \n# It's still the same thing, but we can later change the values of the shared variable \n# (to switch in the test-data later) and pymc3 will just use the new data. \n# Kind-of like a pointer we can redirect.\n# For more info, see: http:\/\/deeplearning.net\/software\/theano\/library\/compile\/shared.html\nann_input = theano.shared(X_train)\nann_output = theano.shared(Y_train)\nneural_network = construct_nn(ann_input, ann_output)\n","eebb7549":"'''\ndef construct_nn(ann_input, ann_output):\n    n_hidden = 2 # Number of neurons in each hidden layer\n    \n    # Initialize random weights between each layer\n    init_1 = np.random.randn(X.shape[1], n_hidden).astype(float)\n    init_2 = np.random.randn(n_hidden, n_hidden).astype(float)\n    init_3 = np.random.randn(n_hidden, n_hidden).astype(float)\n    init_out = np.random.randn(n_hidden).astype(float)\n        \n    with pm.Model() as neural_network:\n        # Weights from input to hidden layer\n        weights_in_1 = pm.Normal('w_in_1', 0, sd=1, \n                                 shape=(X.shape[1], n_hidden), \n                                 testval=init_1)\n        \n        # Weights from 1st to 2nd layer\n        weights_1_2 = pm.Normal('w_1_2', 0, sd=1, \n                                shape=(n_hidden, n_hidden), \n                                testval=init_2)\n        \n        # Weights from 2st to 3nd layer\n        weights_2_3 = pm.Normal('w_2_3', 0, sd=1, \n                                shape=(n_hidden, n_hidden), \n                                testval=init_3)\n        \n        # Weights from hidden layer to output\n        weights_3_out = pm.Normal('w_3_out', 0, sd=1, \n                                  shape=(n_hidden,), \n                                  testval=init_out)\n        \n        # Build neural-network using tanh activation function\n        act_1 = pm.math.tanh(pm.math.dot(ann_input, \n                                         weights_in_1))\n        act_2 = pm.math.tanh(pm.math.dot(act_1, \n                                         weights_1_2))\n        act_3 = pm.math.tanh(pm.math.dot(act_2, \n                                         weights_2_3))\n        act_out = pm.math.sigmoid(pm.math.dot(act_3, \n                                              weights_3_out))\n        \n        # Binary classification -> Bernoulli likelihood\n        out = pm.Bernoulli('out', \n                           act_out,\n                           observed=ann_output,\n                           total_size=Y_train.shape[0] # IMPORTANT for minibatches\n                          )\n    return neural_network\n\n# Trick: Turn inputs and outputs into shared variables. \n# It's still the same thing, but we can later change the values of the shared variable \n# (to switch in the test-data later) and pymc3 will just use the new data. \n# Kind-of like a pointer we can redirect.\n# For more info, see: http:\/\/deeplearning.net\/software\/theano\/library\/compile\/shared.html\nann_input = theano.shared(X_train)\nann_output = theano.shared(Y_train)\nneural_network = construct_nn(ann_input, ann_output)\n'''","bf90cc03":"'''\ndef construct_nn(ann_input, ann_output):\n    n_hidden = 2 # Number of neurons in each hidden layer\n    \n    # Initialize random weights between each layer\n    init_1 = np.random.randn(X.shape[1], n_hidden).astype(float)\n    init_2 = np.random.randn(n_hidden, n_hidden).astype(float)\n    init_3 = np.random.randn(n_hidden, n_hidden).astype(float)\n    init_4 = np.random.randn(n_hidden, n_hidden).astype(float)\n    init_out = np.random.randn(n_hidden).astype(float)\n        \n    with pm.Model() as neural_network:\n        # Weights from input to hidden layer\n        weights_in_1 = pm.Normal('w_in_1', 0, sd=1, \n                                 shape=(X.shape[1], n_hidden), \n                                 testval=init_1)\n        \n        # Weights from 1st to 2nd layer\n        weights_1_2 = pm.Normal('w_1_2', 0, sd=1, \n                                shape=(n_hidden, n_hidden), \n                                testval=init_2)\n        \n        # Weights from 2st to 3nd layer\n        weights_2_3 = pm.Normal('w_2_3', 0, sd=1, \n                                shape=(n_hidden, n_hidden), \n                                testval=init_3)\n        \n        # Weights from 2st to 3nd layer\n        weights_3_4 = pm.Normal('w_3_4', 0, sd=1, \n                                shape=(n_hidden, n_hidden), \n                                testval=init_4)\n        \n        # Weights from hidden layer to output\n        weights_4_out = pm.Normal('w_4_out', 0, sd=1, \n                                  shape=(n_hidden,), \n                                  testval=init_out)\n        \n        # Build neural-network using tanh activation function\n        act_1 = pm.math.tanh(pm.math.dot(ann_input, \n                                         weights_in_1))\n        act_2 = pm.math.tanh(pm.math.dot(act_1, \n                                         weights_1_2))\n        act_3 = pm.math.tanh(pm.math.dot(act_2, \n                                         weights_2_3))\n        act_4 = pm.math.tanh(pm.math.dot(act_3, \n                                         weights_3_4))\n        act_out = pm.math.sigmoid(pm.math.dot(act_4, \n                                              weights_4_out))\n        \n        # Binary classification -> Bernoulli likelihood\n        out = pm.Bernoulli('out', \n                           act_out,\n                           observed=ann_output,\n                           total_size=Y_train.shape[0] # IMPORTANT for minibatches\n                          )\n    return neural_network\n\n# Trick: Turn inputs and outputs into shared variables. \n# It's still the same thing, but we can later change the values of the shared variable \n# (to switch in the test-data later) and pymc3 will just use the new data. \n# Kind-of like a pointer we can redirect.\n# For more info, see: http:\/\/deeplearning.net\/software\/theano\/library\/compile\/shared.html\nann_input = theano.shared(X_train)\nann_output = theano.shared(Y_train)\nneural_network = construct_nn(ann_input, ann_output)\n'''","b5f660cd":"from pymc3.theanof import set_tt_rng, MRG_RandomStreams\nset_tt_rng(MRG_RandomStreams(42))","5c62e323":"%%time\n\nwith neural_network:\n    inference = pm.ADVI()\n    approx = pm.fit(n=50000, method=inference)","8c247754":"trace = approx.sample(draws=5000)","d1288610":"plt.plot(-inference.hist)\nplt.ylabel('ELBO')\nplt.xlabel('iteration');","7d5e66df":"# Replace arrays our NN references with the test data\nann_input.set_value(X_test)\nann_output.set_value(Y_test)\n\nwith neural_network:\n    ppc = pm.sample_posterior_predictive(trace, samples=500, progressbar=False)\n\n# Use probability of > 0.5 to assume prediction of class 1\npred = ppc['out'].mean(axis=0) > 0.5","c846ae86":"fig, ax = plt.subplots()\nax.scatter(X_test[pred==0, 0], X_test[pred==0, 1])\nax.scatter(X_test[pred==1, 0], X_test[pred==1, 1], color='r')\nsns.despine()\nax.set(title='Predicted labels in testing set', xlabel='X', ylabel='Y');","b1c8f7aa":"print('Accuracy = {}%'.format((Y_test == pred).mean() * 100))","3aa76226":"grid = pm.floatX(np.mgrid[-3:3:100j,-3:3:100j])\ngrid_2d = grid.reshape(2, -1).T\ndummy_out = np.ones(grid.shape[1], dtype=np.int8)","20db6ca4":"ann_input.set_value(grid_2d)\nann_output.set_value(dummy_out)\n\nwith neural_network:\n    ppc = pm.sample_posterior_predictive(trace, samples=500, progressbar=False)","7bdbfa08":"cmap = sns.diverging_palette(250, 12, s=85, l=25, as_cmap=True)\nfig, ax = plt.subplots(figsize=(14, 8))\ncontour = ax.contourf(grid[0], grid[1], ppc['out'].mean(axis=0).reshape(100, 100), cmap=cmap)\nax.scatter(X_test[pred==0, 0], X_test[pred==0, 1])\nax.scatter(X_test[pred==1, 0], X_test[pred==1, 1], color='r')\ncbar = plt.colorbar(contour, ax=ax)\n_ = ax.set(xlim=(-3, 3), ylim=(-3, 3), xlabel='X', ylabel='Y');\ncbar.ax.set_ylabel('Posterior predictive mean probability of class label = 0');","c1ef05c5":"cmap = sns.cubehelix_palette(light=1, as_cmap=True)\nfig, ax = plt.subplots(figsize=(14, 8))\ncontour = ax.contourf(grid[0], grid[1], ppc['out'].std(axis=0).reshape(100, 100), cmap=cmap)\nax.scatter(X_test[pred==0, 0], X_test[pred==0, 1])\nax.scatter(X_test[pred==1, 0], X_test[pred==1, 1], color='r')\ncbar = plt.colorbar(contour, ax=ax)\n_ = ax.set(xlim=(-3, 3), ylim=(-3, 3), xlabel='X', ylabel='Y');\ncbar.ax.set_ylabel('Uncertainty (posterior predictive standard deviation)');","e45f6cd2":"import numpy as np\nnp.max( ppc['out'].std(axis=0))","89c48f59":"minibatch_x = pm.Minibatch(X_train, batch_size=32)\nminibatch_y = pm.Minibatch(Y_train, batch_size=32)\n\nneural_network_minibatch = construct_nn(minibatch_x, minibatch_y)\nwith neural_network_minibatch:\n    inference = pm.ADVI()\n    approx = pm.fit(40000, method=inference)","c5369b2e":"plt.plot(-inference.hist)\nplt.ylabel('ELBO')\nplt.xlabel('iteration');","070f4e06":"trace_VI = approx.sample(draws=10000)\npm.traceplot(trace_VI);","65c90363":"grid = pm.floatX(np.mgrid[-3:3:100j,-3:3:100j])\ngrid_2d = grid.reshape(2, -1).T\ndummy_out = np.ones(grid.shape[1], dtype=np.int8)","04634865":"ann_input.set_value(grid_2d)\nann_output.set_value(dummy_out)\n\nwith neural_network:\n    ppc = pm.sample_posterior_predictive(trace, samples=500, progressbar=False)","4bc08d98":"cmap = sns.diverging_palette(250, 12, s=85, l=25, as_cmap=True)\nfig, ax = plt.subplots(figsize=(14, 8))\ncontour = ax.contourf(grid[0], grid[1], ppc['out'].mean(axis=0).reshape(100, 100), cmap=cmap)\nax.scatter(X_test[pred==0, 0], X_test[pred==0, 1])\nax.scatter(X_test[pred==1, 0], X_test[pred==1, 1], color='r')\ncbar = plt.colorbar(contour, ax=ax)\n_ = ax.set(xlim=(-3, 3), ylim=(-3, 3), xlabel='X', ylabel='Y');\ncbar.ax.set_ylabel('Posterior predictive mean probability of class label = 0');","1a88a269":"cmap = sns.cubehelix_palette(light=1, as_cmap=True)\nfig, ax = plt.subplots(figsize=(14, 8))\ncontour = ax.contourf(grid[0], grid[1], ppc['out'].std(axis=0).reshape(100, 100), cmap=cmap)\nax.scatter(X_test[pred==0, 0], X_test[pred==0, 1])\nax.scatter(X_test[pred==1, 0], X_test[pred==1, 1], color='r')\ncbar = plt.colorbar(contour, ax=ax)\n_ = ax.set(xlim=(-3, 3), ylim=(-3, 3), xlabel='X', ylabel='Y');\ncbar.ax.set_ylabel('Uncertainty (posterior predictive standard deviation)');","28bb7aa0":"plt.plot(-inference.hist)\nplt.ylabel('ELBO')\nplt.xlabel('iteration');","5e44c90e":"trace_VI = approx.sample(draws=10000)\npm.traceplot(trace_VI);","4e66e5df":"### Generating data\n\nFirst, lets generate some toy data -- a simple binary classification problem that's not linearly separable.","05354e34":"## Bayesian Neural Networks in PyMC3","407c6340":"### Probability surface","3de62d61":"# Problem 2\n\nModify the Neural Network Variational Inference [notebook](https:\/\/www.kaggle.com\/billbasener\/pymc3-variation-inference-neural-network) by finding a different nonlinear 2-dimensional data and training the network and analyzing the results provided for the make moons data. You can use data provided in sklearn.datasets or any 2D data you can find or create. You can modify the network (number of neurons or layers) as needed.\n\nhttps:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.datasets.make_moons.html pick one of the other ones and see what results you get ","419bd54c":"We can see that very close to the decision boundary, our uncertainty as to which label to predict is highest. You can imagine that associating predictions with uncertainty is a critical property for many applications like health care. To further maximize accuracy, we might want to train the model primarily on samples from that high-uncertainty region.","e63e8916":"## Lets look at what the classifier has learned\n\nFor this, we evaluate the class probability predictions on a grid over the whole input space.","e2bff80f":"Let's look at our predictions:","fbe17112":"Hey, our neural network did all right!","ab9bbbd1":"Performance wise that's pretty good considering that NUTS is having a really hard time. Further below we make this even faster. To make it really fly, we probably want to run the Neural Network on the GPU.\n\nAs samples are more convenient to work with, we can very quickly draw samples from the variational approximation using the `sample` method (this is just sampling from Normal distributions, so not at all the same like MCMC):","d9beb4cd":"## Mini-batch ADVI\n\nSo far, we have trained our model on all data at once. Obviously this won't scale to something like ImageNet. Moreover, training on mini-batches of data (stochastic gradient descent) avoids local minima and can lead to faster convergence.\n\nFortunately, ADVI can be run on mini-batches as well. It just requires some setting up:","ea104f7a":"### Uncertainty in predicted value\n\nSo far, everything I showed we could have done with a non-Bayesian Neural Network. The mean of the posterior predictive for each class-label should be identical to maximum likelihood predicted values. However, we can also look at the standard deviation of the posterior predictive to get a sense for the uncertainty in our predictions. Here is what that looks like:","702b6b1f":"Plotting the objective function (ELBO) we can see that the optimization slowly improves the fit over time.","a592b23e":"### Part A\n\nInclude and discuss the probability results for your data.\n\nI did not find another dataset and tried make_circles. \n\nTo improve accuracy of classification, I tried increasing the sample size (to 2,000 and 5,000 didn't see a significant difference), test train split to 80-20 (this helped the accuracy), changing the number of neurons per layer (reducing to 2 improved performance), it seemed fewer neurons performed better. I also tried adding more layers and that did not improve the accuracy. The best performance I was able to achieve was with 2,000 samples split 80-20 with 2 neurons and two layers. \n\nLooking at the probability graph below we can tell the classifier is not able to effectively distinguish between the classes. I'm not sure if this method is just poor at being able to differentiate this kind of data or if more neurons and layers could improve performance. The method used in this article was able to distinguish the classes well https:\/\/machinelearningmastery.com\/impact-of-dataset-size-on-deep-learning-model-skill-and-performance-estimates\/. ","2c6a7832":"We can see that very close to the decision boundary, our uncertainty as to which label to predict is highest. You can imagine that associating predictions with uncertainty is a critical property for many applications like health care. To further maximize accuracy, we might want to train the model primarily on samples from that high-uncertainty region.","f464e0e4":"### Probability surface","cd2abcef":"That's not so bad. The `Normal` priors help regularize the weights. Usually we would add a constant `b` to the inputs but I omitted it here to keep the code cleaner.","22ef99b5":"### Model specification\n\nA neural network is quite simple. The basic unit is a [perceptron](https:\/\/en.wikipedia.org\/wiki\/Perceptron) which is nothing more than [logistic regression](http:\/\/pymc-devs.github.io\/pymc3\/notebooks\/posterior_predictive.html#Prediction). We use many of these in parallel and then stack them up to get hidden layers. Here we will use 2 hidden layers with 5 neurons each which is sufficient for such a simple problem.","1eebc9ab":"Plotting the objective function (ELBO) we can see that the optimization slowly improves the fit over time.","39f76fb9":"# DS 6040 Homework 4 \n\n**Question 2 answers at bottom of notebook**\n\nCamille Leonard cvl7qu \n\n# Variational Inference: Bayesian Neural Networks\n\n**This was adapted from an excellent notebook here:**\n(c) 2016-2018 by Thomas Wiecki, updated by Maxim Kochurov\nOriginal blog post: https:\/\/twiecki.github.io\/blog\/2016\/06\/01\/bayesian-deep-learning\/","9cff9638":"### Probability surface","79bf879e":"### Part B\n\nInclude and discuss the uncertainty results derived from the standard deviation in the posterior predictive.\n\nBecause our classifier is unable to effectively separate the classes the uncertainty of all predictions is high. This is why the entire uncertainty plot is shaded in the darkest colors. ","cbfe688d":"## Mini-batch ADVI\n\nSo far, we have trained our model on all data at once. Obviously this won't scale to something like ImageNet. Moreover, training on mini-batches of data (stochastic gradient descent) avoids local minima and can lead to faster convergence.\n\nFortunately, ADVI can be run on mini-batches as well. It just requires some setting up:","f5a07af6":"## Current trends in Machine Learning\n\nThere are currently three big trends in machine learning: **Probabilistic Programming**, **Deep Learning** and \"**Big Data**\". Inside of PP, a lot of innovation is in making things scale using **Variational Inference**. In this blog post, I will show how to use **Variational Inference** in PyMC3 to fit a simple Bayesian Neural Network. I will also discuss how bridging Probabilistic Programming and Deep Learning can open up very interesting avenues to explore in future research.\n\n### Probabilistic Programming at scale\n**Probabilistic Programming** allows very flexible creation of custom probabilistic models and is mainly concerned with **insight** and learning from your data. The approach is inherently **Bayesian** so we can specify **priors** to inform and constrain our models and get uncertainty estimation in form of a **posterior** distribution. Using [MCMC sampling algorithms](https:\/\/twiecki.github.io\/blog\/2015\/11\/10\/mcmc-sampling\/) we can draw samples from this posterior to very flexibly estimate these models. PyMC3 and [Stan](http:\/\/mc-stan.org\/) are the current state-of-the-art tools to consruct and estimate these models. One major drawback of sampling, however, is that it's often very slow, especially for high-dimensional models. That's why more recently, **variational inference** algorithms have been developed that are almost as flexible as MCMC but much faster. Instead of drawing samples from the posterior, these algorithms instead fit a distribution (e.g. normal) to the posterior turning a sampling problem into and optimization problem. [ADVI](http:\/\/arxiv.org\/abs\/1506.03431) -- Automatic Differentation Variational Inference -- is implemented in PyMC3 and [Stan](http:\/\/mc-stan.org\/), as well as a new package called [Edward](https:\/\/github.com\/blei-lab\/edward\/) which is mainly concerned with Variational Inference. \n\nUnfortunately, when it comes to traditional ML problems like classification or (non-linear) regression, Probabilistic Programming often plays second fiddle (in terms of accuracy and scalability) to more algorithmic approaches like [ensemble learning](https:\/\/en.wikipedia.org\/wiki\/Ensemble_learning) (e.g. [random forests](https:\/\/en.wikipedia.org\/wiki\/Random_forest) or [gradient boosted regression trees](https:\/\/en.wikipedia.org\/wiki\/Boosting_(machine_learning)).\n\n### Deep Learning\n\nNow in its third renaissance, deep learning has been making headlines repeatadly by dominating almost any object recognition benchmark, [kicking ass at Atari games](https:\/\/www.cs.toronto.edu\/~vmnih\/docs\/dqn.pdf), and [beating the world-champion Lee Sedol at Go](http:\/\/www.nature.com\/nature\/journal\/v529\/n7587\/full\/nature16961.html). From a statistical point, Neural Networks are extremely good non-linear function approximators and representation learners. While mostly known for classification, they have been extended to unsupervised learning with [AutoEncoders](https:\/\/arxiv.org\/abs\/1312.6114) and in all sorts of other interesting ways (e.g. [Recurrent Networks](https:\/\/en.wikipedia.org\/wiki\/Recurrent_neural_network), or [MDNs](http:\/\/cbonnett.github.io\/MDN_EDWARD_KERAS_TF.html) to estimate multimodal distributions). Why do they work so well? No one really knows as the statistical properties are still not fully understood.\n\nA large part of the innoviation in deep learning is the ability to train these extremely complex models. This rests on several pillars:\n* Speed: facilitating the GPU allowed for much faster processing.\n* Software: frameworks like [Theano](http:\/\/deeplearning.net\/software\/theano\/) and [TensorFlow](https:\/\/www.tensorflow.org\/) allow flexible creation of abstract models that can then be optimized and compiled to CPU or GPU.\n* Learning algorithms: training on sub-sets of the data -- stochastic gradient descent -- allows us to train these models on massive amounts of data. Techniques like drop-out avoid overfitting.\n* Architectural: A lot of innovation comes from changing the input layers, like for convolutional neural nets, or the output layers, like for [MDNs](http:\/\/cbonnett.github.io\/MDN_EDWARD_KERAS_TF.html).\n\n### Bridging Deep Learning and Probabilistic Programming\nOn one hand we have *Probabilistic Programming* which allows us to build rather small and focused models in a very principled and well-understood way to gain insight into our data; on the other hand we have *deep learning* which uses many heuristics to train huge and highly complex models that are amazing at prediction. Recent innovations in variational inference allow probabilistic programming to scale model complexity as well as data size. We are thus at the cusp of being able to combine these two approaches to hopefully unlock new innovations in Machine Learning. For more motivation, see also [Dustin Tran's](https:\/\/twitter.com\/dustinvtran) recent [blog post](http:\/\/dustintran.com\/blog\/a-quick-update-edward-and-some-motivations\/).\n\nWhile this would allow Probabilistic Programming to be applied to a much wider set of interesting problems, I believe this bridging also holds great promise for innovations in Deep Learning. Some ideas are:\n* **Uncertainty in predictions**: As we will see below, the Bayesian Neural Network informs us about the uncertainty in its predictions. I think uncertainty is an underappreciated concept in Machine Learning as it's clearly important for real-world applications. But it could also be useful in training. For example, we could train the model specifically on samples it is most uncertain about.\n* **Uncertainty in representations**: We also get uncertainty estimates of our weights which could inform us about the stability of the learned representations of the network.\n* **Regularization with priors**: Weights are often L2-regularized to avoid overfitting, this very naturally becomes a Gaussian prior for the weight coefficients. We could, however, imagine all kinds of other priors, like spike-and-slab to enforce sparsity (this would be more like using the L1-norm).\n* **Transfer learning with informed priors**: If we wanted to train a network on a new object recognition data set, we could bootstrap the learning by placing informed priors centered around weights retrieved from other pre-trained networks, like [GoogLeNet](https:\/\/arxiv.org\/abs\/1409.4842). \n* **Hierarchical Neural Networks**: A very powerful approach in Probabilistic Programming is hierarchical modeling that allows pooling of things that were learned on sub-groups to the overall population (see my tutorial on [Hierarchical Linear Regression in PyMC3](https:\/\/twiecki.github.io\/blog\/2014\/03\/17\/bayesian-glms-3\/)). Applied to Neural Networks, in hierarchical data sets, we could train individual neural nets to specialize on sub-groups while still being informed about representations of the overall population. For example, imagine a network trained to classify car models from pictures of cars. We could train a hierarchical neural network where a sub-neural network is trained to tell apart models from only a single manufacturer. The intuition being that all cars from a certain manufactures share certain similarities so it would make sense to train individual networks that specialize on brands. However, due to the individual networks being connected at a higher layer, they would still share information with the other specialized sub-networks about features that are useful to all brands. Interestingly, different layers of the network could be informed by various levels of the hierarchy -- e.g. early layers that extract visual lines could be identical in all sub-networks while the higher-order representations would be different. The hierarchical model would learn all that from the data.\n* **Other hybrid architectures**: We can more freely build all kinds of neural networks. For example, Bayesian non-parametrics could be used to flexibly adjust the size and shape of the hidden layers to optimally scale the network architecture to the problem at hand during training. Currently, this requires costly hyper-parameter optimization and a lot of tribal knowledge.","8b4921e9":"### Variational Inference: Scaling model complexity\n\nWe could now just run a MCMC sampler like NUTS which works pretty well in this case, but as I already mentioned, this will become very slow as we scale our model up to deeper architectures with more layers.\n\nInstead, we will use [ADVI](https:\/\/arxiv.org\/abs\/1603.00788) variational inference algorithm which was recently added to `PyMC3`, and updated to use the operator variational inference (OPVI) framework. This is much faster and will scale better. Note, that this is a mean-field approximation so we ignore correlations in the posterior.","01e3a5f5":"### Model specification\n\nA neural network is quite simple. The basic unit is a [perceptron](https:\/\/en.wikipedia.org\/wiki\/Perceptron) which is nothing more than [logistic regression](http:\/\/pymc-devs.github.io\/pymc3\/notebooks\/posterior_predictive.html#Prediction). We use many of these in parallel and then stack them up to get hidden layers. Here we will use 2 hidden layers with 5 neurons each which is sufficient for such a simple problem.","657c5dad":"Now that we trained our model, lets predict on the hold-out set using a posterior predictive check (PPC). ","74d59541":"Performance wise that's pretty good considering that NUTS is having a really hard time. Further below we make this even faster. To make it really fly, we probably want to run the Neural Network on the GPU.\n\nAs samples are more convenient to work with, we can very quickly draw samples from the variational approximation using the `sample` method (this is just sampling from Normal distributions, so not at all the same like MCMC):","5240a03d":"### Variational Inference: Scaling model complexity\n\nWe could now just run a MCMC sampler like NUTS which works pretty well in this case, but as I already mentioned, this will become very slow as we scale our model up to deeper architectures with more layers.\n\nInstead, we will use [ADVI](https:\/\/arxiv.org\/abs\/1603.00788) variational inference algorithm which was recently added to `PyMC3`, and updated to use the operator variational inference (OPVI) framework. This is much faster and will scale better. Note, that this is a mean-field approximation so we ignore correlations in the posterior.","c7ad6e9a":"Let's look at our predictions:","49408df9":"That's not so bad. The `Normal` priors help regularize the weights. Usually we would add a constant `b` to the inputs but I omitted it here to keep the code cleaner.","f120c68d":"### Part C\n\nInclude and discuss the plot of the ELBOW, and what that tells about convergence to maximizing the likelihood.\n\nThe elbow plot shows us how the likelihood converges as the maximization step is run through 50,000 iterations. Each iteration includes a slight tweak from the previous iteration to further maximize the likelihood. Based on our plot below, the maximization appears to level out after 10,000 iterations.","331ec228":"As you can see, mini-batch ADVI's running time is much lower. It also seems to converge faster.\n\nFor fun, we can also look at the trace. The point is that we also get uncertainty of our Neural Network weights.","73e742ac":"Hey, our neural network did all right!","f15e3c34":"## Lets look at what the classifier has learned\n\nFor this, we evaluate the class probability predictions on a grid over the whole input space.","69b67c7b":"Now that we trained our model, lets predict on the hold-out set using a posterior predictive check (PPC). ","57fbb107":"### Part D\n\nInclude and discuss the traceplot, focusing on what this tells you about your network.\n\nThe traceplot shows us the weights that are input into the model and then from layer to layer. They start off roughly normal. Our neural net was constructed with two neurons and two layers. We see that the majority of the weights are centered around 0 meaning they don't contribute to predicting the class of each datapoint. It's not surprising to see this given our algorithm did a poor job of separating and predicting the two classes. ","791dcfb3":"## Summary\n\nHopefully this blog post demonstrated a very powerful new inference algorithm available in PyMC3: [ADVI](http:\/\/pymc-devs.github.io\/pymc3\/api.html#advi). I also think bridging the gap between Probabilistic Programming and Deep Learning can open up many new avenues for innovation in this space, as discussed above. Specifically, a hierarchical neural network sounds pretty bad-ass. These are really exciting times.\n\n## Next steps\n\n[`Theano`](http:\/\/deeplearning.net\/software\/theano\/), which is used by `PyMC3` as its computational backend, was mainly developed for estimating neural networks and there are great libraries like [`Lasagne`](https:\/\/github.com\/Lasagne\/Lasagne) that build on top of `Theano` to make construction of the most common neural network architectures easy. See my [follow-up blog post on how to use Lasagne together with PyMC3](https:\/\/twiecki.github.io\/blog\/2016\/07\/05\/bayesian-deep-learning\/).\n\nYou can also run this example on the GPU by setting `device = gpu` and `floatX = float32` in your `.theanorc`.\n\nYou might argue that the above network isn't really deep, but note that we could easily extend it to have more layers, including convolutional ones to train on more challenging data sets, as demonstrated [here]([follow-up blog post on how to use Lasagne together with PyMC3](https:\/\/twiecki.github.io\/blog\/2016\/07\/05\/bayesian-deep-learning\/).\n\nI also presented some of this work at PyData London, view the video below:\n<iframe width=\"560\" height=\"315\" src=\"https:\/\/www.youtube.com\/embed\/LlzVlqVzeD8\" frameborder=\"0\" allowfullscreen><\/iframe>\n\nFinally, you can download this NB [here](https:\/\/github.com\/twiecki\/WhileMyMCMCGentlySamples\/blob\/master\/content\/downloads\/notebooks\/bayesian_neural_network.ipynb). Leave a comment below, and [follow me on twitter](https:\/\/twitter.com\/twiecki).\n\n## Acknowledgements\n\n[Taku Yoshioka](https:\/\/github.com\/taku-y) did a lot of work on the original ADVI implementation in PyMC3. I'd also like to the thank the Stan guys (specifically Alp Kucukelbir and Daniel Lee) for deriving ADVI and teaching us about it. Thanks also to Chris Fonnesbeck, Andrew Campbell, Taku Yoshioka, and Peadar Coyle for useful comments on an earlier draft.","77a469f4":"---","397c5120":"As you can see, mini-batch ADVI's running time is much lower. It also seems to converge faster.\n\nFor fun, we can also look at the trace. The point is that we also get uncertainty of our Neural Network weights.","079dfe18":"### Uncertainty in predicted value\n\nSo far, everything I showed we could have done with a non-Bayesian Neural Network. The mean of the posterior predictive for each class-label should be identical to maximum likelihood predicted values. However, we can also look at the standard deviation of the posterior predictive to get a sense for the uncertainty in our predictions. Here is what that looks like:"}}