{"cell_type":{"f77ab9a8":"code","d035cf10":"code","07c0a59e":"code","cc4a4a06":"code","266bf173":"code","a3493230":"code","28af7ddc":"code","bb432941":"code","d1a38da7":"code","3e24b0c9":"code","c09b5e6b":"code","f1036add":"code","80928d34":"code","6cd07972":"code","466459a2":"code","c904128e":"code","e9a22dc0":"code","14b32558":"code","ec966f54":"code","9b546a53":"code","f068fa28":"code","f261c8ea":"code","e6ae38fe":"code","0140ec0b":"code","b241ca13":"code","5b1e553d":"code","464a5697":"code","b382e477":"code","3955c46d":"code","f826d406":"code","229e2de1":"code","1ba0ecab":"code","7a4e3e3e":"code","38e98521":"code","c66dbb6e":"code","34d67b81":"code","5ad22d40":"code","25b7f64d":"code","3353e75e":"code","704f1e30":"code","0b432582":"code","3d843b45":"code","d1db5089":"code","27ea1494":"code","a6661bbd":"code","77690d3f":"code","84b6327c":"code","f26d2cc6":"code","3735b0d1":"code","fc2cb983":"code","f65bb4da":"code","8749a3a2":"code","bcdb45b6":"code","c940d813":"code","10f60889":"code","5a41aeb1":"code","9c9e9fe3":"code","83dcaf97":"code","ff8f3e7b":"code","80fae5f6":"code","9a18c050":"code","19daee0a":"code","ef69f26d":"code","194156f8":"code","6ced7412":"code","8e0efdc0":"code","58ba428d":"code","72b11ded":"code","b1b2fc04":"code","71b16aeb":"code","34b61c00":"code","bd07abcf":"code","417996b8":"code","57a17490":"code","e304f506":"code","406c56d1":"code","47fe968e":"code","51fc3162":"code","938f3ebd":"markdown","c201a549":"markdown","a9277b9d":"markdown","311ecbf6":"markdown","861d3270":"markdown","38256d5f":"markdown","f74a0353":"markdown","28a8512f":"markdown","a72a8500":"markdown","bfa2495b":"markdown","9944d966":"markdown","42a87a61":"markdown","99332b12":"markdown","3f87d61b":"markdown","e67cc31b":"markdown","b84253e4":"markdown","e5fd7067":"markdown","1a73352b":"markdown","dda3d91c":"markdown","af565f31":"markdown","4459eaa4":"markdown","7bc45715":"markdown","8f7010b3":"markdown","704696ae":"markdown","f0f5ab3e":"markdown","00d6b4ef":"markdown","df3af4a8":"markdown","189d2da7":"markdown","3ff59088":"markdown","166e25c5":"markdown","b1956857":"markdown","951e13dc":"markdown","a8fa46ca":"markdown","e302c5ae":"markdown","17014a42":"markdown","5cf7aba6":"markdown","5227a42d":"markdown","9a135ed5":"markdown","4865da8a":"markdown","c2cca548":"markdown","8948aeb3":"markdown","fff949f9":"markdown","df29d509":"markdown","d7385a02":"markdown","41ecb6c8":"markdown","03637e15":"markdown","924b304e":"markdown","4c93cd02":"markdown","fdbc3c7a":"markdown","e2f2884a":"markdown"},"source":{"f77ab9a8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","d035cf10":"import keras\nimport seaborn as sns\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport tensorflow as tf\nfrom keras.models import Sequential\nfrom keras.layers.core import Dense, Dropout, Activation, Flatten\nfrom keras.layers import LSTM, Embedding\n# from keras.layers.convolutional import Convolution2D, MaxPooling2D\nfrom sklearn.model_selection import train_test_split\n# from keras.utils.np_utils import to_categorical\nfrom sklearn.metrics import accuracy_score\n%matplotlib inline","07c0a59e":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","cc4a4a06":"train[\"Age\"] = train[\"Age\"].fillna(-0.5)\ntest[\"Age\"] = test[\"Age\"].fillna(-0.5)\nbins = [-1, 0, 14, 25, 35, 60, np.inf]\nlabels = ['Unknown', 'Child', 'Teenager', 'Young Adult', 'Adult', 'Senior']\ntrain['AgeGroup'] = pd.cut(train[\"Age\"], bins, labels = labels)\ntest['AgeGroup'] = pd.cut(test[\"Age\"], bins, labels = labels)\n\n#draw a bar plot of Age vs. survival\nsns.barplot(x=\"AgeGroup\", y=\"Survived\", data=train)\nplt.show()","266bf173":"age_mapping = {'Unknown': None,'Child': 1, 'Teenager': 2, 'Young Adult': 3, 'Adult': 4, 'Senior': 5}\ntrain['AgeGroup'] = train['AgeGroup'].map(age_mapping)\ntest['AgeGroup'] = test['AgeGroup'].map(age_mapping)\n\ntrain.head()","a3493230":"train = train.drop(['Age'], axis = 1)\ntest = test.drop(['Age'], axis = 1)","28af7ddc":"sex_mapping = {\"male\": 0, \"female\": 1}\ntrain['Sex'] = train['Sex'].map(sex_mapping)\ntest['Sex'] = test['Sex'].map(sex_mapping)\n\ntrain.head()","bb432941":"train = train.fillna({\"Embarked\": \"S\"})","d1a38da7":"embarked_mapping = {\"S\": 1, \"C\": 2, \"Q\": 3}\ntrain['Embarked'] = train['Embarked'].map(embarked_mapping)\ntest['Embarked'] = test['Embarked'].map(embarked_mapping)\n\ntrain.head()","3e24b0c9":"combine = [train, test]","c09b5e6b":"for dataset in combine:\n    dataset['Title'] = dataset.Name.str.extract(' ([A-Za-z]+)\\.', expand=False)\n\npd.crosstab(train['Title'], train['Sex'])","f1036add":"for dataset in combine:\n   \n    dataset['Title'] = dataset['Title'].replace(['Mlle','Ms','Countess','Miss','Mme'], 'Mrs')\n    dataset['Title'] = dataset['Title'].replace(['Major','Sir','Capt','Col','Don','Jonkheer','Rev','Master','Lady'], 'Mr')\n","80928d34":"train[['Title', 'Survived']].groupby(['Title'], as_index=False).mean()","6cd07972":"title_mapping = {\"Dr\": 1, \"Mr\": 2, \"Mrs\": 3}\nfor dataset in combine:\n    dataset['Title'] = dataset['Title'].map(title_mapping)\n    dataset['Title'] = dataset['Title'].fillna(0)\n\ntrain.head()","466459a2":"train = train.drop(['Name'], axis = 1)\ntest = test.drop(['Name'], axis = 1)","c904128e":"for x in range(len(test[\"Fare\"])):\n    if pd.isnull(test[\"Fare\"][x]):\n        pclass = test[\"Pclass\"][x] #Pclass = 3\n        test[\"Fare\"][x] = round(train[train[\"Pclass\"] == pclass][\"Fare\"].mean(), 4)","e9a22dc0":"train['FareBand'] = pd.qcut(train['Fare'], 4, labels = [1, 2, 3, 4])\ntest['FareBand'] = pd.qcut(test['Fare'], 4, labels = [1, 2, 3, 4])","14b32558":"train = train.drop(['Fare'], axis = 1)\ntest = test.drop(['Fare'], axis = 1)","ec966f54":"train = train.drop(['Cabin'], axis = 1)\ntest = test.drop(['Cabin'], axis = 1)","9b546a53":"train = train.drop(['Ticket'], axis = 1)\ntest = test.drop(['Ticket'], axis = 1)","f068fa28":"train_age = train\nmodifiedFlights = train_age.dropna()","f261c8ea":"null_columns=train.columns[train.isnull().any()]","e6ae38fe":"x_train_age = modifiedFlights.drop(['AgeGroup'], axis = 1)","0140ec0b":"y_train_age = modifiedFlights[\"AgeGroup\"]","b241ca13":"x_test_AgeGroup = train[train.isnull().any(axis=1)]","5b1e553d":"x_test_age = x_test_AgeGroup.drop(['AgeGroup'], axis = 1)","464a5697":"from sklearn.model_selection import train_test_split\n\npredictors = x_train_age.drop(['PassengerId'], axis=1)\ntarget = y_train_age\nx_trainage, x_valage, y_trainage, y_valage = train_test_split(predictors, target, test_size = 0.1, random_state = 0)","b382e477":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_trainage, y_trainage)\ny_predage = gbk.predict(x_valage)\nacc_gbkage = round(accuracy_score(y_predage, y_valage) * 100, 2)\nprint(acc_gbkage)","3955c46d":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import accuracy_score\n\nsgd = SGDClassifier()\nsgd.fit(x_trainage, y_trainage)\ny_predage = sgd.predict(x_valage)\nacc_sgdage = round(accuracy_score(y_predage, y_valage) * 100, 2)\nprint(acc_sgdage)","f826d406":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_trainage, y_trainage)\ny_predage = knn.predict(x_valage)\nacc_knnage = round(accuracy_score(y_predage, y_valage) * 100, 2)\nprint(acc_knnage)","229e2de1":"from sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_trainage, y_trainage)\ny_predage = randomforest.predict(x_valage)\nacc_randomforestage = round(accuracy_score(y_predage, y_valage) * 100, 2)\nprint(acc_randomforestage)","1ba0ecab":"from sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_trainage, y_trainage)\ny_predage = decisiontree.predict(x_valage)\nacc_decisiontreeage = round(accuracy_score(y_predage, y_valage) * 100, 2)\nprint(acc_decisiontreeage)","7a4e3e3e":"from sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_trainage, y_trainage)\ny_predage = perceptron.predict(x_valage)\nacc_perceptronage = round(accuracy_score(y_predage, y_valage) * 100, 2)\nprint(acc_perceptronage)","38e98521":"from sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_trainage, y_trainage)\ny_predage = linear_svc.predict(x_valage)\nacc_linear_svcage = round(accuracy_score(y_predage, y_valage) * 100, 2)\nprint(acc_linear_svcage)","c66dbb6e":"from sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_trainage, y_trainage)\ny_predage = svc.predict(x_valage)\nacc_svcage = round(accuracy_score(y_predage, y_valage) * 100, 2)\nprint(acc_svcage)","34d67b81":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_trainage, y_trainage)\ny_predage = logreg.predict(x_valage)\nacc_logregage = round(accuracy_score(y_predage, y_valage) * 100, 2)\nprint(acc_logregage)","5ad22d40":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(x_trainage, y_trainage)\ny_predage = gaussian.predict(x_valage)\nacc_gaussianage = round(accuracy_score(y_predage, y_valage) * 100, 2)\nprint(acc_gaussianage)","25b7f64d":"models = pd.DataFrame({\n    'Model': ['Gradient Boosting Classifier','Stochastic Gradient Descent','KNN','Random Forest','Decision Tree', 'Perceptron','Linear SVC','Support Vector Machines', \n              'Logistic Regression','Naive Bayes',  \n              ],\n    'Score': [acc_gbkage, acc_sgdage, acc_knnage, acc_randomforestage, \n              acc_decisiontreeage, acc_perceptronage, acc_linear_svcage, acc_svcage, acc_logregage, \n               acc_gaussianage]})\nmodels.sort_values(by='Score', ascending=False)","3353e75e":"predictions = randomforest.predict(x_test_age.drop('PassengerId', axis=1))","704f1e30":"k=0\nfor i in range(891):\n    if np.isnan(train_age['AgeGroup'][i]) == True:\n        train_age['AgeGroup'][i] = predictions[k]\n        k+=1","0b432582":"test_test_age = test ","3d843b45":"modifiedFlights = test_test_age.dropna()\nnull_columns=test.columns[test.isnull().any()]","d1db5089":"x_test_test_age = modifiedFlights.drop(['AgeGroup'], axis = 1)\ny_test_test_age = modifiedFlights[\"AgeGroup\"]\n\nx_test_test_AgeGroup = test[test.isnull().any(axis=1)]\nx_tst_test_age = x_test_test_AgeGroup.drop(['AgeGroup'], axis = 1)","27ea1494":"from sklearn.model_selection import train_test_split\n\npredictors = x_test_test_age.drop(['PassengerId'], axis=1)\ntarget = y_test_test_age\nx_testage_1, x_valage_1, y_testage_1, y_valage_1 = train_test_split(predictors, target, test_size = 0.1, random_state = 0)","a6661bbd":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_testage_1, y_testage_1)\ny_predage_1 = gbk.predict(x_valage_1)\nacc_gbkage_1 = round(accuracy_score(y_predage_1, y_valage_1) * 100, 2)\nprint(acc_gbkage_1)","77690d3f":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import accuracy_score\n\nsgd = SGDClassifier()\nsgd.fit(x_testage_1, y_testage_1)\ny_predage_1 = sgd.predict(x_valage_1)\nacc_sgdage_1 = round(accuracy_score(y_predage_1, y_valage_1) * 100, 2)\nprint(acc_sgdage_1)","84b6327c":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_testage_1, y_testage_1)\ny_predage_1 = knn.predict(x_valage_1)\nacc_knnage_1 = round(accuracy_score(y_predage_1, y_valage_1) * 100, 2)\nprint(acc_knnage_1)","f26d2cc6":"from sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_testage_1, y_testage_1)\ny_predage_1 = randomforest.predict(x_valage_1)\nacc_randomforestage_1 = round(accuracy_score(y_predage_1, y_valage_1) * 100, 2)\nprint(acc_randomforestage_1)","3735b0d1":"from sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_testage_1, y_testage_1)\ny_predage_1 = decisiontree.predict(x_valage_1)\nacc_decisiontreeage_1 = round(accuracy_score(y_predage_1, y_valage_1) * 100, 2)\nprint(acc_decisiontreeage_1)","fc2cb983":"from sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_testage_1, y_testage_1)\ny_predage_1 = perceptron.predict(x_valage_1)\nacc_perceptronage_1 = round(accuracy_score(y_predage_1, y_valage_1) * 100, 2)\nprint(acc_perceptronage_1)","f65bb4da":"from sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_testage_1, y_testage_1)\ny_predage_1 = linear_svc.predict(x_valage_1)\nacc_linear_svcage_1 = round(accuracy_score(y_predage_1, y_valage_1) * 100, 2)\nprint(acc_linear_svcage_1)","8749a3a2":"from sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_testage_1, y_testage_1)\ny_predage_1 = svc.predict(x_valage_1)\nacc_svcage_1 = round(accuracy_score(y_predage_1, y_valage_1) * 100, 2)\nprint(acc_svcage_1)","bcdb45b6":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_testage_1, y_testage_1)\ny_predage_1 = logreg.predict(x_valage_1)\nacc_logregage_1 = round(accuracy_score(y_predage_1, y_valage_1) * 100, 2)\nprint(acc_logregage_1)","c940d813":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(x_testage_1, y_testage_1)\ny_predage_1 = gaussian.predict(x_valage_1)\nacc_gaussianage_1 = round(accuracy_score(y_predage_1, y_valage_1) * 100, 2)\nprint(acc_gaussianage_1)","10f60889":"models = pd.DataFrame({\n    'Model': ['Gradient Boosting Classifier','Stochastic Gradient Descent','KNN','Random Forest','Decision Tree', 'Perceptron','Linear SVC','Support Vector Machines', \n              'Logistic Regression','Naive Bayes',  \n              ],\n    'Score': [acc_gbkage_1, acc_sgdage_1, acc_knnage_1, acc_randomforestage_1, \n              acc_decisiontreeage_1, acc_perceptronage_1, acc_linear_svcage_1, acc_svcage_1, acc_logregage_1, \n               acc_gaussianage_1]})\nmodels.sort_values(by='Score', ascending=False)","5a41aeb1":"predictions = gbk.predict(x_tst_test_age.drop('PassengerId', axis=1))","9c9e9fe3":"p=0\nfor i in range(418):\n    if np.isnan(test_test_age['AgeGroup'][i]) == True:\n        test_test_age['AgeGroup'][i] = predictions[p]\n        p+=1","83dcaf97":"from sklearn.model_selection import train_test_split\n\npredictors = train.drop(['Survived', 'PassengerId'], axis=1)\ntarget = train[\"Survived\"]\nx_train, x_val, y_train, y_val = train_test_split(predictors, target, test_size = 0.05, random_state = 0)","ff8f3e7b":"from sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.metrics import accuracy_score\n\ngbk = GradientBoostingClassifier()\ngbk.fit(x_train, y_train)\ny_pred = gbk.predict(x_val)\nacc_gbk = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gbk)","80fae5f6":"from sklearn.linear_model import SGDClassifier\nfrom sklearn.metrics import accuracy_score\n\nsgd = SGDClassifier()\nsgd.fit(x_train, y_train)\ny_pred = sgd.predict(x_val)\nacc_sgd = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_sgd)","9a18c050":"from sklearn.neighbors import KNeighborsClassifier\n\nknn = KNeighborsClassifier()\nknn.fit(x_train, y_train)\ny_pred = knn.predict(x_val)\nacc_knn = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_knn)","19daee0a":"from sklearn.ensemble import RandomForestClassifier\n\nrandomforest = RandomForestClassifier()\nrandomforest.fit(x_train, y_train)\ny_pred = randomforest.predict(x_val)\nacc_randomforest = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_randomforest)","ef69f26d":"from sklearn.tree import DecisionTreeClassifier\n\ndecisiontree = DecisionTreeClassifier()\ndecisiontree.fit(x_train, y_train)\ny_pred = decisiontree.predict(x_val)\nacc_decisiontree = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_decisiontree)","194156f8":"from sklearn.linear_model import Perceptron\n\nperceptron = Perceptron()\nperceptron.fit(x_train, y_train)\ny_pred = perceptron.predict(x_val)\nacc_perceptron = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_perceptron)","6ced7412":"from sklearn.svm import LinearSVC\n\nlinear_svc = LinearSVC()\nlinear_svc.fit(x_train, y_train)\ny_pred = linear_svc.predict(x_val)\nacc_linear_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_linear_svc)","8e0efdc0":"from sklearn.svm import SVC\n\nsvc = SVC()\nsvc.fit(x_train, y_train)\ny_pred = svc.predict(x_val)\nacc_svc = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_svc)","58ba428d":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\nlogreg.fit(x_train, y_train)\ny_pred = logreg.predict(x_val)\nacc_logreg = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_logreg)","72b11ded":"from sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import accuracy_score\n\ngaussian = GaussianNB()\ngaussian.fit(x_train, y_train)\ny_pred = gaussian.predict(x_val)\nacc_gaussian = round(accuracy_score(y_pred, y_val) * 100, 2)\nprint(acc_gaussian)","b1b2fc04":"models = pd.DataFrame({\n    'Model': ['Gradient Boosting Classifier','Stochastic Gradient Descent','KNN','Random Forest','Decision Tree', 'Perceptron','Linear SVC','Support Vector Machines', \n              'Logistic Regression','Naive Bayes',  \n              ],\n    'Score': [acc_gbk, acc_sgd, acc_knn, acc_randomforest, acc_decisiontree, acc_perceptron, acc_linear_svc, acc_svc, acc_logreg, \n               acc_gaussian]})\nmodels.sort_values(by='Score', ascending=False)","71b16aeb":"#set ids as PassengerId and predict survival \nids = test['PassengerId']\npredictions = knn.predict(test.drop('PassengerId', axis=1))\n\n#set the output as a dataframe and convert to csv file named submission.csv\noutput = pd.DataFrame({ 'PassengerId' : ids, 'Survived': predictions })\noutput.to_csv('submission_knn.csv', index=False)","34b61c00":"x_tr = train.iloc[:,2:].as_matrix()\ny_tr = train.iloc[:,1].as_matrix()","bd07abcf":"X_test = test.iloc[:,1:].as_matrix()","417996b8":"model = tf.keras.models.Sequential()\nmodel.add(tf.keras.layers.Flatten(input_shape=(8,)))\n\nmodel.add(tf.keras.layers.Dense(512,activation = tf.nn.relu))\n\nmodel.add(tf.keras.layers.Dense(512,activation = tf.nn.relu))\nmodel.add(tf.keras.layers.Dense(512,activation = tf.nn.relu))\n\n# model.add(tf.keras.layers.Flatten(input_shape=(8,)))\n\nmodel.add(tf.keras.layers.Dense(2,activation = tf.nn.softmax))","57a17490":"model.summary()","e304f506":"# model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nmodel.compile(optimizer = 'adam',\n             loss = 'sparse_categorical_crossentropy',\n             metrics = ['accuracy'])","406c56d1":"model.fit(x_tr, y_tr, batch_size=128, epochs = 60)","47fe968e":"predictions = model.predict([X_test])","51fc3162":"import csv\ndata = [['PassengerId', 'Survived']]\nfor i in range(1,419):\n    data.append([i+891,np.argmax(predictions[i-1])])\nprint(data)\nwith open('submission_own_NN.csv','w',newline='') as fp:\n    a=csv.writer(fp,delimiter = ',')\n    a.writerows(data)","938f3ebd":"# Create a combined group of train and test datasets","c201a549":"# Age prediction","a9277b9d":"# Map Fare values into groups of numerical values","311ecbf6":"# Decision Tree","861d3270":"# Drop the name feature since it contains no more useful information.","38256d5f":"# Linear SVC","f74a0353":"# Linear SVC","28a8512f":"# Dropping the Age feature for now, might change","a72a8500":"# Gradient Boosting Classifier","bfa2495b":"# Random Forest","9944d966":"# Support Vector Machines","42a87a61":"# Support Vector Machines","99332b12":"# Gradient Boosting Classifier","3f87d61b":"# KNN or k-Nearest Neighbors","e67cc31b":"# Perceptron","b84253e4":"# Drop Fare values","e5fd7067":"# Logistic Regression","1a73352b":"# Make train data from train with age information","dda3d91c":"# Logistic Regression","af565f31":"# Fill in missing Fare value in test set based on mean fare for that Pclass ","4459eaa4":"# Random Forest","7bc45715":"# Support Vector Machines","8f7010b3":"# Stochastic Gradient Descent","704696ae":"# Linear SVC","f0f5ab3e":"# Gaussian Naive Bayes","00d6b4ef":"# KNN or k-Nearest Neighbors","df3af4a8":"# Perceptron","189d2da7":"# Perceptron","3ff59088":"# Replace various titles with more common names","166e25c5":"# Sequential NN","b1956857":"# Age sorts into logical categories","951e13dc":"# Logistic Regression","a8fa46ca":"# Decision Tree","e302c5ae":"# Extract a title for each Name in the train and test datasets","17014a42":"# Decision Tree","5cf7aba6":"# Map each Embarked value to a numerical value","5227a42d":"# Drop Cabin values","9a135ed5":"# Drop Ticket values","4865da8a":"# Make test data from train without age information","c2cca548":"# Map each of the title groups to a numerical value","8948aeb3":"# Similar approach for test set","fff949f9":"# Map each Sex value to a numerical value","df29d509":"# Stochastic Gradient Descent","d7385a02":"# Map each Age value to a numerical value","41ecb6c8":"# Gaussian Naive Bayes","03637e15":"# Gaussian Naive Bayes","924b304e":"# Stochastic Gradient Descent","4c93cd02":"# Random Forest","fdbc3c7a":"# KNN or k-Nearest Neighbors","e2f2884a":"# Gradient Boosting Classifier"}}