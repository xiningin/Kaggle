{"cell_type":{"e937a243":"code","fc07c8d1":"code","15c61b31":"code","f5096757":"code","42611d91":"code","13d01560":"code","306f6158":"code","7dd27ee7":"code","f26a998e":"code","1e38bc43":"code","25a36650":"code","75a0d807":"code","89e4f81c":"markdown","059f77e8":"markdown","32cee832":"markdown","9c3635ae":"markdown","e555beec":"markdown","845dca5d":"markdown"},"source":{"e937a243":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport time\nimport seaborn as sns \n\nfrom sklearn.model_selection import train_test_split\nimport xgboost as xgb\n%pylab inline\nwarnings.filterwarnings(action=\"ignore\")\n\nplt.style.use('seaborn-darkgrid')\nsns.set(font_scale=1)\n\nimport os\nprint(os.listdir(\"..\/input\"))","fc07c8d1":"breastCancer = pd.read_csv('..\/input\/data.csv')\n\nbreastCancer = breastCancer[[\"diagnosis\",\"radius_mean\",\"radius_se\",\"radius_worst\",\n              \"perimeter_mean\",\"perimeter_se\",\"perimeter_worst\",\n              \"area_mean\",\"area_se\",\"area_worst\",\n              \"smoothness_mean\",\"smoothness_se\",\"smoothness_worst\",\n              \"compactness_mean\",\"compactness_se\",\"compactness_worst\",  \n              \"concavity_mean\",\"concavity_se\",\"concavity_worst\",         \n              \"concave points_mean\",\"concave points_se\",\"concave points_worst\",              \n              \"symmetry_mean\",\"symmetry_se\",\"symmetry_worst\",        \n              \"fractal_dimension_mean\",\"fractal_dimension_se\",\"fractal_dimension_worst\",\n              \"texture_mean\",\"texture_se\",\"texture_worst\"]]\n\n\nbreastCancer.columns = ['diagnosis', 'radius_mean', 'radius_se', 'radius_worst',\n       'perimeter_mean', 'perimeter_se', 'perimeter_worst', 'area_mean',\n       'area_se', 'area_worst', 'smoothness_mean', 'smoothness_se',\n       'smoothness_worst', 'compactness_mean', 'compactness_se',\n       'compactness_worst', 'concavity_mean', 'concavity_se',\n       'concavity_worst', 'concave_points_mean', 'concave_points_se',\n       'concave_points_worst', 'symmetry_mean', 'symmetry_se',\n       'symmetry_worst', 'fractal_dimension_mean', 'fractal_dimension_se',\n       'fractal_dimension_worst', 'texture_mean', 'texture_se',\n       'texture_worst']\n\ndata = breastCancer\ndata.isnull().sum()","15c61b31":"diagnostic = {'M':0,'B':1}\ndata[\"diagnostic\"] = data.diagnosis\ndata.diagnosis = data.diagnosis.apply(lambda x:diagnostic[x])\ndata.diagnostic = data.diagnostic.apply(lambda x:\"Maligne\" if x == \"M\" else \"B\u00e9nigne\")\n \ndata.head()","f5096757":"colormap = plt.cm.RdBu\nsns.set(font_scale=1)\nplt.figure(figsize=(28,28))\nplt.title('Correlation Pearson des variables', y=1.05, size=24)\nsns.heatmap(data.drop(columns=['diagnostic'],axis=1).astype(float).corr(),linewidths=0.3,vmax=1.0, \n            square=True, cmap=colormap, linecolor='white', annot=True)","42611d91":"X = data.iloc[:,1:-1]\ny = data.diagnosis\ny.unique()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","13d01560":"from sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import BaggingClassifier, RandomForestClassifier, \\\n                             AdaBoostClassifier, GradientBoostingClassifier\nfrom sklearn.metrics import roc_curve, auc, accuracy_score, log_loss, hamming_loss, \\\n                            precision_score, recall_score, f1_score, jaccard_similarity_score, \\\n                            precision_recall_curve\nfrom xgboost.sklearn import XGBClassifier\n\ndef comparaisonsTreeClassifieurs(X_train, X_test, y_train, y_test, n_tree = 5000):\n    t0 = time.time()  \n    np.random.seed(123456)\n    aucROC,accuracy,logloss,hammingloss,precision,sensibilite,f1,jaccard_similarity = \\\n             dict(),dict(),dict(),dict(),dict(),dict(),dict(),dict()\n\n    yClassifications = pd.DataFrame()\n    yClassifications['Observations']=y_test\n\n    names = [\"Decision_Tree\", \"Tree_Bagging\", \n             \"Random_Forest\", \"Ada_Boost\", \"Gradient_Boosting\",\n            \"XGBoost\"]\n    \n    xgb_params = {'base_score'      : 0.5,\n              'booster'         : 'gbtree',\n              'gamma'           : 3,\n              'learning_rate'   : 0.3,\n              'max_depth'       : 4,\n              'min_child_weight': 2,\n              'n_estimators'    : n_tree,\n              'objective'       : 'binary:logistic',\n              'silent'          : True,\n              'subsample'       : 0.8}\n\n    classifiers = [\n        DecisionTreeClassifier(max_depth=5),\n        BaggingClassifier(n_estimators=n_tree,bootstrap=True),\n        RandomForestClassifier(max_depth=5, n_estimators=n_tree, max_features=5),\n        AdaBoostClassifier(n_estimators=n_tree),\n        GradientBoostingClassifier(n_estimators=n_tree, max_leaf_nodes=4,min_samples_split=5),\n        XGBClassifier(**xgb_params)]\n\n    print('-'*100)\n    plt.figure(figsize=(18,18))\n\n    for name, clf in zip(names, classifiers):\n        t1 = time.time() \n    \n        clf.fit(X_train, y_train)\n        score = clf.score(X_test, y_test)*100\n        print(f'Pr\u00e9diction : {name:17s}'+(' %.8f' % score).lstrip('0'),end=' -- ')\n        yClassifications[name+'_prob'] = clf.predict_proba(X_test)[:, 1]\n        yClassifications[name+'_pred'] = clf.predict(X_test)\n\n        #-------------------------------------------------------------------------------------\n        accuracy[name]                 = accuracy_score(yClassifications['Observations'] ,yClassifications[name+'_pred'])\n        logloss[name]                  = log_loss(yClassifications['Observations']       ,yClassifications[name+'_pred'])\n        hammingloss[name]              = hamming_loss(yClassifications['Observations']   ,yClassifications[name+'_pred'])\n        precision[name]                = precision_score(yClassifications['Observations'],yClassifications[name+'_pred'])\n        sensibilite[name]              = recall_score(yClassifications['Observations']   ,yClassifications[name+'_pred'])\n        f1[name]                       = f1_score(yClassifications['Observations']       ,yClassifications[name+'_pred'])\n        jaccard_similarity[name]       = jaccard_similarity_score(yClassifications['Observations'],yClassifications[name+'_pred'])\n        #-------------------------------------------------------------------------------------\n        # L'utilisation de la deuxi\u00e8me valeur dans les probas car 1 - fpr\n        #-------------------------------------------------------------------------------------\n        fpr, tpr, thresholds = roc_curve(y_test.ravel(), yClassifications[name+'_prob'])\n        aucROC[name] = auc(fpr, tpr)\n        print (\"Area under the ROC curve : %0.8f\" % aucROC[name],end=' -- ')\n        plt.plot(fpr, tpr, label=f\"{name}(AUC = {aucROC[name]:0.8f})\")\n        plt.plot([0, 1], [0, 1], 'k--')\n        plt.xlim([-0.05, 1.05])\n        plt.ylim([-0.05, 1.05])\n        plt.xlabel('False Positive rate(1 - Specificity)',size=18)\n        plt.ylabel('Ture positive rate(Sensitivity)',size=18)\n        plt.title('ROC curve (Receiver Operating Caracteristic)',size=20)\n        plt.legend(loc=\"lower right\")\n        print(('%0.2fs' % (time.time() - t1)).lstrip('0'))\n    plt.show()\n\n    print('Execution  :'+('%.2fs' % (time.time() - t0)).lstrip('0'))\n\n    resultats = pd.DataFrame(pd.Series(aucROC),columns=[\"aucROC\"])\n    resultats[\"accuracy\"] = pd.Series(accuracy)\n    resultats[\"log_loss\"] = pd.Series(logloss)\n    resultats[\"hamming_loss\"] = pd.Series(hammingloss)\n    resultats[\"precision\"] = pd.Series(precision)\n    resultats[\"sensibilite\"] = pd.Series(sensibilite)\n    resultats[\"f1\"] = pd.Series(f1)\n    resultats[\"jaccard_similarity\"] = pd.Series(jaccard_similarity)\n    resultats.sort_values(by='aucROC',ascending=False, inplace=True)\n    \n    return yClassifications, resultats","306f6158":"yClassifications, resultats = comparaisonsTreeClassifieurs(X_train, X_test, y_train, y_test, n_tree = 5000)\nresultats","7dd27ee7":"xgb_params = {'base_score'  : 0.5,\n          'booster'         : 'gbtree',\n          'gamma'           : 3,\n          'learning_rate'   : 0.3,\n          'max_depth'       : 4,\n          'min_child_weight': 2,\n          'n_estimators'    : 5000,\n          'objective'       : 'binary:logistic',\n          'silent'          : True,\n          'subsample'       : 0.8}\nclf = XGBClassifier(**xgb_params)\nclf.fit(X_train, y_train)\nfeature_importance = pd.DataFrame(clf.feature_importances_, index =X_train.columns.values)\nfeature_importance.sort_values(by=0, ascending=False)","f26a998e":"import copy as cp \ndata_ft = cp.deepcopy(data)","1e38bc43":"data_ft['calc_sup01'] = pd.qcut(data_ft.radius_worst * \\\n                        data_ft.smoothness_worst * \\\n                        data_ft.concave_points_worst * \\\n                        data_ft.fractal_dimension_worst * \\\n                        data_ft.texture_worst \/ \\\n                        (data_ft.compactness_worst + data_ft.symmetry_worst),30,labels=False)\n\ndata_ft['calc_sup02'] = pd.qcut(data_ft.radius_worst * \\\n                        data_ft.smoothness_worst * \\\n                        data_ft.concave_points_worst * \\\n                        data_ft.texture_worst * \\\n                        data_ft.symmetry_worst,30,labels=False)\n\ndata_ft['calc_sup03'] = pd.qcut(data_ft.radius_worst * \\\n                        data_ft.smoothness_worst * \\\n                        data_ft.concave_points_worst * \\\n                        data_ft.fractal_dimension_worst * \\\n                        data_ft.texture_worst ,30,labels=False)\n\ndata_ft['radiusWQ']      = pd.qcut(data_ft.radius_worst      - data_ft.radius_se     ,100,labels=False)\ndata_ft['perimeterWQ']   = pd.qcut(data_ft.perimeter_worst   - data_ft.perimeter_se  ,100,labels=False)\ndata_ft['areaWQ']        = pd.qcut(data_ft.area_worst        - data_ft.area_se       ,100,labels=False)\ndata_ft['smoothnessWQ']  = pd.qcut(data_ft.smoothness_worst  - data_ft.smoothness_se ,100,labels=False)\ndata_ft['textureWQ']     = pd.qcut(data_ft.texture_worst     - data_ft.texture_se    ,100,labels=False)\ndata_ft['compactnessWQ'] = pd.qcut(data_ft.compactness_worst - data_ft.compactness_se,100,labels=False)\n\ndata_ft = data_ft[['diagnosis', 'radius_mean', 'radius_se', 'radius_worst',\n       'perimeter_mean', 'perimeter_se', 'perimeter_worst', 'area_mean',\n       'area_se', 'area_worst', 'smoothness_mean', 'smoothness_se',\n       'smoothness_worst', 'compactness_mean', 'compactness_se',\n       'compactness_worst', 'concavity_mean', 'concavity_se',\n       'concavity_worst', 'concave_points_mean', 'concave_points_se',\n       'concave_points_worst', 'symmetry_mean', 'symmetry_se',\n       'symmetry_worst', 'fractal_dimension_mean', 'fractal_dimension_se',\n       'fractal_dimension_worst', 'texture_mean', 'texture_se',\n       'texture_worst', 'calc_sup01', 'calc_sup02',\n       'calc_sup03', 'radiusWQ', 'perimeterWQ', 'areaWQ', 'smoothnessWQ',\n       'textureWQ', 'compactnessWQ', 'diagnostic']]\n\ndata_ft.head()","25a36650":"X = data_ft.iloc[:,1:-1]\ny = data_ft.diagnosis\ny.unique()\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=.2, random_state=42)\nX_train.shape, X_test.shape, y_train.shape, y_test.shape","75a0d807":"yClassifications, resultats = comparaisonsTreeClassifieurs(X_train, X_test, y_train, y_test, n_tree = 5000)\nresultats","89e4f81c":"# New features","059f77e8":"# Train classifiers with augmented data","32cee832":"# Train Classifiers","9c3635ae":"# Feature Importances","e555beec":"# Split augmented data","845dca5d":"# Split data "}}