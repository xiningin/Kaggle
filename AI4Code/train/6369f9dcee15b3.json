{"cell_type":{"5468bb36":"code","1e538944":"code","7907bb9c":"code","c5158011":"code","62df978a":"code","6a9936fe":"code","5b7d37f6":"code","91c7bf12":"code","59949ea2":"code","4da72d64":"code","aaf46901":"code","ac538779":"code","5ab5b854":"code","949e8556":"code","3f224406":"code","bcf6ede0":"code","663fd804":"code","85b4d18b":"code","b6415c97":"code","36200f55":"code","6c5c4288":"code","37c29182":"code","3297611d":"code","b2d45b8b":"markdown","a7952ffd":"markdown","dcd10b23":"markdown","026fdb2c":"markdown","db82a9d5":"markdown","2285aa19":"markdown","31c93438":"markdown","ecb0c179":"markdown","4d67f6f4":"markdown","a8baad8f":"markdown","88bdd9f9":"markdown","e7277e25":"markdown","747bf2e9":"markdown","74ecc022":"markdown","cef62806":"markdown"},"source":{"5468bb36":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","1e538944":"import os\nimport requests\n\nimport pandas as pd\nimport numpy as np\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport plotly.graph_objs as go\nimport plotly.express as px\nimport PIL.Image\nimport cv2\n\nfrom IPython.display import Image, display\n\nimport urllib\n\nimport warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning)","7907bb9c":"os.listdir('..\/input\/wikipedia-image-caption\/')","c5158011":"test_file = pd.read_csv('..\/input\/wikipedia-image-caption\/test.tsv', sep='\\t')\ntest_file","62df978a":"wiki_df = pd.read_csv('..\/input\/wikipedia-image-caption\/image_data_test\/image_pixels\/test_image_pixels_part-00000.csv', \n                      sep='\\t', names=['image_url', 'b64_bytes', 'metadata_url'])\nprint(wiki_df)\n","6a9936fe":"sub_file = pd.read_csv('..\/input\/wikipedia-image-caption\/sample_submission.csv')\nsub_file","5b7d37f6":"print(wiki_df.shape)\nprint(sub_file.shape)","91c7bf12":"!pip install autoviz xlrd","59949ea2":"import pandas as pd\nfrom autoviz.AutoViz_Class import AutoViz_Class\nAV = AutoViz_Class()","4da72d64":"filename = \"..\/input\/wikipedia-image-caption\/train-00001-of-00005.tsv\"\nsep = \"\\t\"\ndft = AV.AutoViz(\n    filename,\n    sep=sep,\n    depVar=\"\",\n    dfte=None,\n    header=0,\n    verbose=0,\n    lowess=False,\n    chart_format=\"svg\",\n    max_rows_analyzed=15000,\n    max_cols_analyzed=30,\n)","aaf46901":"import matplotlib.pyplot as plt\nimport squarify    # pip install squarify (algorithm for treemap)\n# plot it\nsquarify.plot(sizes=test_file['language'].value_counts().values, \n              label=test_file['language'].value_counts().index, \n              color=[\"green\",\"violet\",\"yellow\", \"blue\"],\n              alpha=.8 )\nplt.axis('off')\nplt.show()","ac538779":"# Code inspired from by  Georgii Sirotenko  https:\/\/www.kaggle.com\/georgiisirotenko\/pytorch-fish-outliers-handling-test-100 & https:\/\/www.kaggle.com\/mpwolke\/wikimedia-urllib\n\nimport plotly.graph_objects as go    \n\nfig = go.Figure(\n    data=[ go.Bar(x=test_file['language'].value_counts().index, \n            y=test_file['language'].value_counts().values,\n            text=test_file['language'].value_counts().values,\n            textposition='auto',name='hist', marker_color='skyblue')],\n    layout_title_text=\"WikiMedia Image Dataset Language Distribution\"\n)\nfig.show()","5ab5b854":"image_file = pd.read_csv('..\/input\/wikipedia-image-caption\/image_data_test\/image_pixels\/test_image_pixels_part-00004.csv', \n                         sep='\\t', names=['image_url', 'b64_bytes', 'metadata_url'])\nimage_file","949e8556":"def showimages(imagelist):\n    f, ax = plt.subplots(4,3, figsize=(18,12))\n    image_flag=False\n    for i, image_id in enumerate(imagelist):\n        print(i, image_id)\n        with urllib.request.urlopen(image_id) as url:\n            if (image_id.lower().find('.svg') != -1):\n                print (\"Contains given SVG file \")\n                image_flag=True\n###         if (image_id.lower().find('.tiff') != -1):\n###                print (\"Contains given TIFF file \")\n###                image_flag=True \n###            if (image_id.lower().find('.tif') != -1):\n###                print (\"Contains given TIF file \")\n###                image_flag=True \n###\n            if (image_flag == False):\n                with open('.\/temp.jpg', 'wb') as f:\n                    f.write(url.read())\n        \n        if (image_flag == False):\n            imagetoshow=PIL.Image.open('.\/temp.jpg')\n            print(imagetoshow)\n            ax[i\/\/3, i%3].imshow(imagetoshow) \n            ax[i\/\/3, i%3].axis('off')\n    plt.show() ","3f224406":"manualdisplay=image_file.image_url[90:102].values\nshowimages(manualdisplay)","bcf6ede0":"import random\n\nstart_num=random.randrange(0, len(image_file)-30)\nend_num = start_num + 12\nimagelist=image_file.image_url[start_num:end_num].values\nprint(imagelist.dtype)\nfor index, image in enumerate(imagelist):\n    if (image.find('.svg') != -1):\n        print (\"Contains given SVG file \")\n        imagelist[index] =  imagelist[index-1] #work to be done\n        \nshowimages(imagelist)","663fd804":"file_name = pd.read_csv('..\/input\/wikipedia-image-caption\/train-00001-of-00005.tsv', \n                        sep='\\t',nrows=3000)\nfile_name.head(5)","85b4d18b":"from wordcloud import WordCloud,STOPWORDS,ImageColorGenerator\nfrom PIL import Image\n\nkaggle_mask = np.array(Image.open('..\/input\/kaggle\/kaggle-logo.png'))\n#kaggle_mask = np.array(Image.open('..\/input\/kaggle\/kaggle-transparent.svg'))\nfig = plt.figure()\nfig.set_figwidth(10)\nfig.set_figheight(15)\nplt.imshow(kaggle_mask, cmap=plt.cm.gray, interpolation='bilinear') \nplt.axis('off')\n#plt.show()\n\nkaggle_wc= WordCloud(background_color='black',max_words = 3000,stopwords='site', mask = kaggle_mask)\nkaggle_wc.generate(\" \".join(file_name['page_title'].astype(str)))\nfig=plt.figure()\nfig.set_figwidth(20)\nfig.set_figheight(16)\nplt.axis('off')\nplt.imshow(kaggle_wc, interpolation='bilinear')\nplt.show()","b6415c97":"embed_file_sample_df=pd.read_csv('..\/input\/wikipedia-image-caption\/image_data_test\/resnet_embeddings\/test_resnet_embeddings_part-00001.csv')\npixel_file_sample_df=pd.read_csv('..\/input\/wikipedia-image-caption\/image_data_test\/image_pixels\/test_image_pixels_part-00002.csv')\n\nprint(embed_file_sample_df.head(2))\nprint(embed_file_sample_df.columns)\n\nprint(pixel_file_sample_df.head(2))\nprint(pixel_file_sample_df.columns)\n","36200f55":"file_name.columns","6c5c4288":"check_cols = ['language', 'mime_type', 'original_height', \n              'original_width', 'is_main_image','page_changed_recently']\nfor cols in check_cols:\n    print(file_name[cols].unique())","37c29182":"#temp_df=file_name[[check_cols]]\ntemp_df1= file_name.iloc[:, 0]\ntemp_df2= file_name.iloc[:, 9:14]\ntemp_df3=pd.concat([temp_df1, temp_df2.reindex(temp_df2.index)], axis=1)\n#,file_name.iloc[:,9:12])\ntemp_df3.head()","3297611d":"import seaborn as sns\n#temp_df3.boxplot(by='language')\ntemp_df3_group=temp_df3.groupby('language').agg('min')\n\n#temp_df3_group.columns\ntemp_df3_group['original_height'].plot(label = 'original_height', figsize = (20,16))\ntemp_df3_group['original_width'].plot(label = 'original_width', figsize = (20,16))\nplt.legend()\nplt.show()\ntemp_df3_group['mime_type'].value_counts().plot.bar(label='mime_type')\nplt.legend()\nplt.show()\ntemp_df3_group['is_main_image'].value_counts().plot.bar(label='is_main_image')\nplt.legend()\nplt.show()\ntemp_df3_group['attribution_passes_lang_id'].value_counts().plot.bar(label='attribution_passes_lang_id')\nplt.legend()\nplt.show()\n\ngraph_df = pd.concat([temp_df3_group['mime_type'].value_counts(), \n                temp_df3_group['is_main_image'].value_counts(),\n                temp_df3_group['attribution_passes_lang_id'].value_counts()], \n               axis=1, sort=True)\ngraph_df.columns = [\"Mime\", \"Main Image\", \"Attribution Passes\"]\ngraph_df.plot.bar(figsize = (20,16))\nplt.legend()\nplt.show()\n\n","b2d45b8b":"## Perhaps a word cloud?","a7952ffd":"### Load the class","dcd10b23":"### List the files","026fdb2c":"### Load one of the image files","db82a9d5":"### Try the EDA","2285aa19":"## Let us randomize","31c93438":"## Select 12 files at a time","ecb0c179":"## WIKIMEDIA - Image\/Caption Matching \n### We shall do a bit of EDA using various tools\n#### We shall use Autoviz , SweetViz and then do a bit of analysis using standard techniques","4d67f6f4":"### Install the libraries","a8baad8f":"# More exploration to come\n\n### Thank you! ","88bdd9f9":"### Load the submission file","e7277e25":"### Check the size and shape","747bf2e9":"### Define a function for loading images - 12 at a time","74ecc022":"### Load the libraries","cef62806":"### Load the Main File"}}