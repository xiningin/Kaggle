{"cell_type":{"1b09961d":"code","545c4dd5":"code","1ca0a886":"code","eb167533":"code","df6238bb":"code","38e06292":"code","a55ee19b":"code","6741c46a":"code","44943f85":"code","851b2e7b":"code","bb297260":"code","bc3270a6":"code","a4548316":"code","f9cd249b":"code","b402cec8":"code","24831530":"code","dc245cac":"code","fd91f97f":"code","37b8e15b":"code","44341ead":"code","d372f581":"code","971a85c7":"code","81fe1aa2":"code","8139782d":"code","d1586050":"code","176746cd":"code","43d1e1f9":"code","c675b17e":"code","907b0727":"code","70812d9f":"code","c180fa98":"code","6ccca373":"code","b9a7a3b6":"code","c36d0a51":"code","3f55c3f7":"code","404acaf1":"code","49cd9d99":"code","6a09d08b":"markdown","92d5d116":"markdown"},"source":{"1b09961d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nimport nltk\nfrom nltk.corpus import twitter_samples\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","545c4dd5":"nltk.download('twitter_samples')","1ca0a886":"all_positive_tweets = twitter_samples.strings('positive_tweets.json')\nall_negative_tweets = twitter_samples.strings('negative_tweets.json')","eb167533":"len(all_positive_tweets)","df6238bb":"# Select any positive tweet\ntweet = all_positive_tweets[2277]\ntweet","38e06292":"import re\nimport string\n\nfrom nltk.corpus import stopwords\nfrom nltk.stem import PorterStemmer\nfrom nltk.tokenize import TweetTokenizer\nfrom nltk.stem import WordNetLemmatizer","a55ee19b":"nltk.download('wordnet')","6741c46a":"def process_tweet(tweet):\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english')\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    # remove hyperlinks\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(r'#', '', tweet)\n    #tokenizer\n    tokenizer = TweetTokenizer(preserve_case=False,strip_handles=True,reduce_len=True)\n    lemmatizer = WordNetLemmatizer()\n    tweet_tokens = tokenizer.tokenize(tweet)\n    tweets_clean = []\n    tweets_lemma = []\n    for tweet in tweet_tokens:\n        if (tweet not in stopwords_english) and (tweet not in string.punctuation):\n            stem_word = stemmer.stem(tweet)\n            lemma_word = lemmatizer.lemmatize(tweet)\n            tweets_clean.append(stem_word)\n            tweets_lemma.append(lemma_word)\n    return tweets_clean,tweets_lemma","44943f85":"def build_freqs(tweets,ys):\n    freqs_stem = {}\n    freqs_lemma = {}\n    ylist = np.squeeze(ys).tolist()\n    for y,tweet in zip(ys,tweets):\n        tweets,tweets_lemma = process_tweet(tweet)\n        for word in tweets:\n            pair = (word,y)\n            if pair in freqs_stem:\n                freqs_stem[pair] += 1\n            else:\n                freqs_stem[pair] = 1\n        for word in tweets_lemma:\n            pair = (word,y)\n            if pair in freqs_lemma:\n                freqs_lemma[pair] += 1\n            else:\n                freqs_lemma[pair] = 1\n    return freqs_stem,freqs_lemma\nbuild_freqs([all_positive_tweets[121],all_negative_tweets[121]],np.array([1,0]))","851b2e7b":"tweets = all_positive_tweets + all_negative_tweets\nlabels = np.append(np.ones(len(all_positive_tweets)) , np.zeros(len(all_negative_tweets)))","bb297260":"len(labels)\n#type(labels[0])","bc3270a6":"freqs,freqs_lemma = build_freqs(tweets,labels)","a4548316":"# select some words to appear in the report. we will assume that each word is unique (i.e. no duplicates)\nkeys = ['happi', 'merri', 'nice', 'good', 'bad', 'sad', 'mad', 'best', 'pretti',\n        '\u2764', ':)', ':(', '\ud83d\ude12', '\ud83d\ude2c', '\ud83d\ude04', '\ud83d\ude0d', '\u265b',\n        'song', 'idea', 'power', 'play', 'magnific']\ndata = []\nfor word in keys:\n    pos = 0\n    neg = 0\n    if (word,1) in freqs:\n        pos = freqs[(word,1)]\n    if (word,0) in freqs:\n        neg = freqs[(word,0)]\n    data.append([word,pos,neg])\ndata\n    ","f9cd249b":"fig,ax = plt.subplots(figsize = (8,8))\n\nx = np.log([x[1] + 1 for x in data])\ny = np.log([x[2] + 1 for x in data])\nax.scatter(x,y)\nplt.xlabel('Log Positive Count')\nplt.ylabel('Log Negative Count')\n\nfor i in range(len(data)):\n    ax.annotate(data[i][0],(x[i],y[i]) , fontsize = 12)\n\nax.plot([0, 9], [0, 9], color = 'red') # Plot the red line that divides the 2 areas.\nplt.show()","b402cec8":"test_pos = all_positive_tweets[4000:]\ntest_neg = all_negative_tweets[4000:]\ntrain_pos = all_positive_tweets[:4000]\ntrain_neg = all_negative_tweets[:4000]\n\ntrain_set = train_pos+train_neg\ntest_set = test_pos+test_neg","24831530":"train_y = np.append(np.ones((len(train_pos), 1)), np.zeros((len(train_neg), 1)), axis=0)\ntest_y = np.append(np.ones((len(test_pos), 1)), np.zeros((len(test_neg), 1)), axis=0)","dc245cac":"print(\"train_y.shape = \" + str(train_y.shape))\nprint(\"test_y.shape = \" + str(test_y.shape))","fd91f97f":"freqs_stem , freqs_lemma = build_freqs(train_set,np.squeeze(train_y).tolist())","37b8e15b":"def extract_features(tweet,freqs_stem,freqs_lemma):\n    word_stem,word_lemma = process_tweet(tweet)\n    x_stem = np.zeros((1,3))\n    x_lemma = np.zeros((1,3))\n    x_stem[0,0] = 1\n    x_lemma[0,0] = 1\n    \n    for word in word_stem:\n        if (word,1) in freqs_stem:\n            x_stem[0,1] += freqs_stem[(word,1)]\n        if (word,0) in freqs_stem:\n            x_stem[0,2] += freqs_stem[(word,0)]\n            \n    for word in word_lemma:\n        if (word,1) in freqs_lemma:\n            x_lemma[0,1] += freqs_lemma[(word,1)]\n        if (word,0) in freqs_lemma:\n            x_lemma[0,2] += freqs_lemma[(word,0)]\n        \n    return x_stem,x_lemma","44341ead":"extract_features(train_set[1],freqs_stem,freqs_lemma)","d372f581":"def sigmoid(z):\n    return 1\/(1+np.exp(-z))","971a85c7":"# UNQ_C2 (UNIQUE CELL IDENTIFIER, DO NOT EDIT)\ndef gradient_descent(x, y, theta, alpha, num_iters):\n    m = len(x)\n    costs = []\n    for i in range(0, num_iters):\n        \n        # get z, the dot product of x and theta\n        z = np.dot(x,theta)\n        \n        # get the sigmoid of z\n        h = sigmoid(z)\n        \n        # calculate the cost function\n        J = (-1\/m)*(np.dot(np.transpose(y),np.log(h)) + np.dot(np.transpose(1-y),(np.log(1-h))))\n        costs.append(J)\n        # update the weights theta\n        theta = theta - (alpha\/m)*(np.dot(np.transpose(x),(h-y))) \n        \n    J = float(J)\n    return J, theta,costs","81fe1aa2":"# Check the function\n# Construct a synthetic test case using numpy PRNG functions\nnp.random.seed(1)\n# X input is 10 x 3 with ones for the bias terms\ntmp_X = np.append(np.ones((10, 1)), np.random.rand(10, 2) * 2000, axis=1)\n# Y Labels are 10 x 1\ntmp_Y = (np.random.rand(10, 1) > 0.35).astype(float)\n\n# Apply gradient descent\ntmp_J, tmp_theta,costs = gradient_descent(tmp_X, tmp_Y, np.zeros((3, 1)), 1e-8, 700)\nprint(f\"The cost after training is {tmp_J:.8f}.\")\nprint(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(tmp_theta)]}\")","8139782d":"# Extracting Features for Training Data\nX_stem = np.zeros((len(train_set),3))\nX_lemma = np.zeros((len(train_set),3))\nfor i in range(len(train_set)):\n    X_stem[i,:],X_lemma[i,:] = extract_features(train_set[i],freqs_stem,freqs_lemma)\n    \nY = train_y\n\n# Apply gradient descent\nJ, theta_stem,costs_stem = gradient_descent(X_stem, Y, np.zeros((3, 1)), 1e-9, 1500)\nprint(f\"The cost after training(stemming) is {J:.8f}.\")\nprint(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta_stem)]}\")\n\n# Apply gradient descent\nJ, theta_lemma,costs_lemma = gradient_descent(X_lemma, Y, np.zeros((3, 1)), 1e-9, 1500)\nprint(f\"The cost after training(Lemmatization) is {J:.8f}.\")\nprint(f\"The resulting vector of weights is {[round(t, 8) for t in np.squeeze(theta_lemma)]}\")\ncosts_lemma = np.array(costs_lemma).reshape(-1,1)\nX = np.array([i for i in range(1,1501)]).reshape(-1,1)\ncosts_stem = np.array(costs_stem).reshape(-1,1)\nplt.plot(X,costs_lemma)\nplt.plot(X,costs_stem)\nplt.legend([\"Lemmatization\",\"Stemming\"])\nplt.show()","d1586050":"def predict_tweet(tweet,theta,freqs):\n    x_stem,x_lemma = extract_features(tweet,freqs_stem,freqs_lemma)\n    y_pred = sigmoid(np.dot(x_stem,theta))\n    return y_pred\n","176746cd":"# Run this cell to test your function\nfor tweet in ['I am happy', 'I am bad', 'this movie should have been great.', 'great', 'great great', 'great great great', 'great great great great']:\n    print( '%s -> %f' % (tweet, predict_tweet(tweet,theta_stem, freqs_stem)))","43d1e1f9":"def test_logistic_regression(test_x,test_y,freqs,theta):\n    y_hat = []\n    \n    for tweet in test_x:\n        y_pred = predict_tweet(tweet,theta,freqs)\n        \n        if y_pred > 0.5:\n            y_hat.append(1)\n        else:\n            y_hat.append(0)\n        \n    accuracy = (y_hat== np.squeeze(test_y)).sum()\/len(test_x)\n    return accuracy","c675b17e":"tmp_accuracy = test_logistic_regression(test_set, test_y, freqs_stem, theta_stem)\nprint(f\"Logistic regression model's accuracy = {tmp_accuracy:.4f}\")","907b0727":"def process_tweet(tweet):\n    stemmer = PorterStemmer()\n    stopwords_english = stopwords.words('english')\n    tweet = re.sub(r'\\$\\w*', '', tweet)\n    # remove old style retweet text \"RT\"\n    tweet = re.sub(r'^RT[\\s]+', '', tweet)\n    # remove hyperlinks\n    tweet = re.sub(r'https?:\\\/\\\/.*[\\r\\n]*', '', tweet)\n    # remove hashtags\n    # only removing the hash # sign from the word\n    tweet = re.sub(r'#', '', tweet)\n    #tokenizer\n    tokenizer = TweetTokenizer(preserve_case=False,strip_handles=True,reduce_len=True)\n    #lemmatizer = WordNetLemmatizer()\n    tweet_tokens = tokenizer.tokenize(tweet)\n    tweets_clean = []\n    #tweets_lemma = []\n    for tweet in tweet_tokens:\n        if (tweet not in stopwords_english) and (tweet not in string.punctuation):\n            stem_word = stemmer.stem(tweet)\n            #lemma_word = lemmatizer.lemmatize(tweet)\n            tweets_clean.append(stem_word)\n            #tweets_lemma.append(lemma_word)\n    return tweets_clean","70812d9f":"def count_tweet(tweets,label):\n    freqs = {}\n    for y,tweet in zip(label,tweets):\n        for word in process_tweet(tweet):\n            if (word,y) in freqs:\n                freqs[(word,y)] += 1\n            else:\n                freqs[(word,y)] = 1\n    return freqs\ntweets = ['i am happy', 'i am tricked', 'i am sad', 'i am tired', 'i am tired']\nys = [1, 0, 0, 0, 0]\ncount_tweet(tweets, ys)    ","c180fa98":"train_y = np.squeeze(train_y).tolist()\nfreqs = count_tweet(train_set,train_y)","6ccca373":"def lookup(freqs,word,label):\n    if (word,label) in freqs:\n        return freqs[(word,label)]\n    return 0\n","b9a7a3b6":"def train_naive_bayes(freqs , train_x, train_y):\n    loglikelihood = {}\n    logprior = 0\n    # Unique Words\n    vocab = set([pair[0] for pair in freqs.keys()])\n    V = len(vocab)\n    \n    N_pos = N_neg = 0\n    for pair in freqs.keys():\n        if pair[1] > 0:\n            N_pos += freqs[pair]\n        else:\n            N_neg += freqs[pair]\n    \n    D = len(train_y) #Number of Documents\n    D_pos = len(list(filter(lambda x:x>0,train_y)))\n    D_neg = len(list(filter(lambda x:x<=0,train_y)))\n    \n    logprior = np.log(D_pos) - np.log(D_neg)\n    \n    for word in vocab:\n        freqs_pos = lookup(freqs,word,1)\n        freqs_neg = lookup(freqs,word,0)\n        p_w_pos = (freqs_pos + 1)\/(N_pos + V)     #Laplacian Smoothing, to avoid division by zero\n        p_w_neg = (freqs_neg + 1)\/(N_neg + V)\n        \n        loglikelihood[word] = np.log(p_w_pos\/p_w_neg)\n    return logprior,loglikelihood","c36d0a51":"logprior, loglikelihood = train_naive_bayes(freqs, train_set, train_y)\nprint(logprior)\nprint(len(loglikelihood))","3f55c3f7":"def naive_bayes_predict(tweet,logprior,loglikelihood):\n    word_l = process_tweet(tweet)\n    p = 0\n    p += logprior\n    for word in word_l:\n        if word in loglikelihood:\n            p += loglikelihood[word]\n    return p\n\nmy_tweet = 'She smiled.'\np = naive_bayes_predict(my_tweet, logprior, loglikelihood)\nprint('The expected output is', p)","404acaf1":"def test_naive_bayes(test_x,test_y,logprior,loglikelihood):\n    y_hat = []\n    for tweet in test_x:\n        y_pred = naive_bayes_predict(tweet,logprior,loglikelihood)\n        \n        if y_pred > 0:\n            y_hat.append(1)\n        else:\n            y_hat.append(0)\n            \n    error = np.abs(np.sum(y_hat) - np.sum(test_y))\/len(test_y)\n    \n    accuracy = 1 - error\n    \n    return accuracy\n        ","49cd9d99":"print(\"Naive Bayes accuracy = %0.4f\" %\n      (test_naive_bayes(test_set, test_y, logprior, loglikelihood)))","6a09d08b":"### Naive Bayes Classifier","92d5d116":"This chart is straightforward to interpret. It shows that emoticons `:)` and `:(` are very important for sentiment analysis. Thus, we should not let preprocessing steps get rid of these symbols!\n\nFurthermore, what is the meaning of the crown symbol? It seems to be very negative!\n\n#### Preparing a dataset to train a Logistic Regression"}}