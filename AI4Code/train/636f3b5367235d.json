{"cell_type":{"ec1228e8":"code","1e174aeb":"code","3a7817c9":"code","7d2f913e":"code","0b84016e":"code","ee36a032":"code","487b3cb6":"code","10728190":"code","73149302":"code","fd203b8f":"code","fb1e08c7":"code","358e71ab":"code","570d0f93":"code","59236544":"code","8d9aca7b":"code","f58cf37f":"code","78533241":"code","b813490b":"code","adce9496":"code","ff4a250b":"code","241f23e4":"code","2609d8d2":"code","112a402f":"code","68e80ef2":"code","eee82183":"code","97135e87":"code","506da6a2":"code","7573e57b":"code","a19d5802":"code","87dc7131":"code","024608af":"code","d6a6e53d":"code","9cde965d":"code","c34b0138":"code","84f4f4ab":"code","b0195bd2":"code","f210bc67":"code","26d634dd":"code","55c52608":"code","a4ae7d7e":"code","22d1ac19":"code","7ff68d24":"code","b491e0a6":"code","e53c7796":"code","65ed7ebf":"code","3f029c89":"code","6ad966c5":"code","dbc86769":"code","ab67ff0b":"code","b1f14dc2":"code","643f98d6":"code","05eab41e":"code","fc05f10d":"code","ad6b2777":"code","35743b46":"code","b77147c0":"code","589d465e":"code","29e38991":"code","4f28f65b":"code","a491108e":"code","fb554df7":"markdown","086c53b1":"markdown","82060a26":"markdown","00d8fa3d":"markdown","ccf45408":"markdown","5da644ae":"markdown","c52f0017":"markdown","44354b89":"markdown","d3460542":"markdown","0d0dddfb":"markdown","eb88837d":"markdown","1b5a40df":"markdown","75159383":"markdown","93700459":"markdown","da978da4":"markdown","f570e5cb":"markdown","430bae04":"markdown","4ce52786":"markdown","cfa26b5e":"markdown","db5bc48f":"markdown"},"source":{"ec1228e8":"#import lib and modules \nimport pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, KFold, cross_validate, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.model_selection import cross_val_score, cross_val_predict, KFold, cross_validate, GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.pipeline import make_pipeline, Pipeline\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.compose import ColumnTransformer","1e174aeb":"#read train csv\nhouse = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/train.csv')\nhouse_ts = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/test.csv')","3a7817c9":"#make a copy of datasets to clean up\nhouse_cpy = house.copy()\nhouse_ts_cpy = house_ts.copy()","7d2f913e":"#Dropping ID column from both datasets\nhouse_cpy.drop('Id', inplace = True, axis = 1)\nhouse_ts_cpy.drop('Id', inplace = True, axis = 1)","0b84016e":"#display df\nhouse_cpy.head()","ee36a032":"print('Shape of the training dataset is {}\\nShape of the test dataset is {}'.format(\nhouse_cpy.shape, house_ts_cpy.shape))","487b3cb6":"#let examine our dtype\nhouse_cpy.info()","10728190":"#how many missing values are there in trainig?\nhouse_cpy.isnull().sum().sort_values(ascending = False).head(20)","73149302":"#examine the test dataset\nhouse_ts_cpy.info()","fd203b8f":"#how many missing values are there in test data?\nhouse_ts_cpy.isnull().sum().sort_values(ascending = False).head(34)","fb1e08c7":"sns.boxplot(x = 'MSZoning', y = 'SalePrice', data = house_cpy)","358e71ab":"# #fill in nan data in the MSZoning column using mode \nhouse_ts_cpy['MSZoning'] = house_ts_cpy.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))\nhouse_cpy['MSZoning'] = house_cpy.groupby('MSSubClass')['MSZoning'].transform(lambda x: x.fillna(x.mode()[0]))","570d0f93":"#drop columns with missing values > 81 in both datasets\n[(col, house_cpy.drop(col, inplace = True, axis = 1)) for col in house_cpy.columns \n           if house_cpy[col].isnull().sum() > 81 ], [(col, house_ts_cpy.drop(col, inplace = True, axis = 1)) for col in house_ts_cpy.columns \n           if house_ts_cpy[col].isnull().sum() > 81 ]","59236544":"print('New shape of the training set is {}\\nNew shape of test dataset is {}'.format(\n    house_cpy.shape, house_ts_cpy.shape))","8d9aca7b":"#check duplicates\nif (house_cpy.duplicated().sum() and house_ts_cpy.duplicated().sum()) == 0:\n    print('No duplicates in either dataset')\nelse:\n    print('There are some duplicates')","f58cf37f":"#display unique values all_data df\n\n## Combining train and test datasets together so that we can do all the work at once. \nall_data = pd.concat((house_cpy, house_ts_cpy)).reset_index(drop = True)\n\nuniques = {col: all_data[col].unique() for col in all_data.columns}\nuniques_df = pd.DataFrame(uniques.items(), columns = ['col_name', 'unique_values'])\n\npd.set_option('display.max_colwidth', None) \nuniques_df","78533241":"house.corr()[['SalePrice']].sort_values(by='SalePrice', ascending=False)\nplt.figure(figsize=(20, 20))\nheatmap = sns.heatmap(abs(house.corr())[['SalePrice']].sort_values(by='SalePrice', ascending=False), vmin=-1, vmax=1, annot=True, cmap='BrBG')\nheatmap.set_title('Features Correlating with Sales Price', fontdict={'fontsize':18}, pad=16);","b813490b":"# Here we will graph the most crucial features in our model\nfig, axs = plt.subplots(2,3, figsize = (10,5))\nplt1 = sns.boxplot(house_cpy['SalePrice'], ax = axs[0,0])\nplt2 = sns.boxplot(house_cpy['YrSold'], ax = axs[0,1])\nplt3 = sns.boxplot(house_cpy['YearBuilt'], ax = axs[0,2])\nplt1 = sns.boxplot(house_cpy['OverallCond'], ax = axs[1,0])\nplt2 = sns.boxplot(house_cpy['GarageYrBlt'], ax = axs[1,1])\nplt3 = sns.boxplot(house_cpy['MasVnrArea'], ax = axs[1,2])\n\nplt.tight_layout()","adce9496":"#let's plot some categorical features against saleprice\nplt.figure(figsize=(10, 20))\nplt.subplot(5,2,1)\nsns.boxplot(x = 'OverallCond', y = 'SalePrice', data = house_cpy)\nplt.subplot(5,2,2)\nsns.boxplot(x = 'BsmtExposure', y = 'SalePrice', data = house_cpy)\nplt.subplot(5,2,3)\nsns.boxplot(x = 'SaleCondition', y = 'SalePrice', data = house_cpy)\nplt.subplot(5,2,4)\nsns.boxplot(x = 'Functional', y = 'SalePrice', data = house_cpy)\nplt.subplot(5,2,5)\nsns.boxplot(x = 'PavedDrive', y = 'SalePrice', data = house_cpy)\nplt.subplot(5,2,6)\nsns.boxplot(x = 'KitchenQual', y = 'SalePrice', data = house_cpy)\nplt.show()\n","ff4a250b":"#set the target\ny = house_cpy.SalePrice\n#set the features\nX = house_cpy.drop(['SalePrice'], axis=1, inplace=True)\n#align columns from both dataset so that each column corresponds to itself in the other dataset\nX, house_ts_cpy = house_cpy.align(house_ts_cpy, join='inner', axis=1)","241f23e4":"#Produced code from kaggle tutorial\n\n# Break off test set from training data\nX_train, X_test, y_train, y_test = train_test_split(X, y, \n                                                                train_size=0.8, test_size=0.2,\n                                                                random_state=0)\n\n# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in X_train.columns if\n                    X_train[cname].nunique() < 10 and \n                    X_train[cname].dtype == \"object\"]\n\n# Select numerical columns\nnumerical_cols = [cname for cname in X_train.columns if \n                X_train[cname].dtype in ['int64', 'float64']]\n\n# Keep selected columns only\nmy_cols = categorical_cols + numerical_cols\nX_train = X_train[my_cols].copy()\nX_test = X_test[my_cols].copy()\nX_test2 = house_ts_cpy[my_cols].copy()","2609d8d2":"# Preprocessing for numerical data\nnumerical_transformer = SimpleImputer(strategy='constant')","112a402f":"# Preprocessing for categorical data\ncategorical_transformer = Pipeline(steps=[\n    ('imputer', SimpleImputer(strategy='most_frequent')),\n    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n])","68e80ef2":"# Bundle preprocessing for numerical and categorical data\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('num', numerical_transformer, numerical_cols),\n        ('cat', categorical_transformer, categorical_cols)\n    ])","eee82183":"from sklearn.ensemble import RandomForestRegressor\n\nmodel = RandomForestRegressor(n_estimators=1000, random_state=0)","97135e87":"from sklearn.metrics import mean_absolute_error\n\n# Bundle preprocessing and modeling code in a pipeline\nmy_pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n                              ('model', model)\n                             ])\n\n# Preprocessing of training data, fit model \nmy_pipeline.fit(X_train, y_train)\n\n# Preprocessing of validation data, get predictions\npreds = my_pipeline.predict(X_test)\n\n# Evaluate the model\nscore = mean_absolute_error(y_test, preds)\nprint('MAE:', score)","506da6a2":"preds_test = my_pipeline.predict(X_test2)\npreds_test","7573e57b":"my_pipeline.score(house_cpy, y)","a19d5802":"# output = pd.DataFrame({'Id': house_ts_cpy.index,\n#                        'SalePrice': preds_test})\n# output.to_csv('submission2.csv', index=False)\n# #scored 0.14788 accuracy --- needs some improvement -without converting categorical columns\ntest_pred = house_ts.copy()\ntest_pred = test_pred[['Id']]\ntest_pred[['SalePrice']] = preds_test\ntest_pred.to_csv('submit2_c.csv',index=False)\n#scored 0.14797 accuracy -- worst with converting caterogical ???","87dc7131":"#create a list of nominal features\nnominal = [ 'MSSubClass', 'MSZoning', 'Street', 'LotShape', 'LandContour',\n           'LotConfig', 'LandSlope', 'Neighborhood', 'Condition1', \n           'Condition2','BldgType', 'HouseStyle', 'RoofStyle', 'RoofMatl',\n           'Exterior1st', 'Exterior2nd', 'MasVnrType', 'Foundation',\n       'Heating', 'CentralAir', 'Electrical', 'GarageType', 'SaleType']\n\n\n#create dummies\nhouse_cpy = pd.get_dummies(house_cpy, columns = nominal, drop_first= True)\nhouse_ts_cpy = pd.get_dummies(house_ts_cpy, columns = nominal, drop_first= True)","024608af":"#let's get the list of values we have to encode into ordinal values\nordinal = ['Utilities', 'ExterQual', 'ExterCond', 'BsmtQual', 'BsmtCond', \n           'BsmtExposure', 'BsmtFinType1', 'BsmtFinType2', 'HeatingQC',\n           'KitchenQual','Functional', 'GarageFinish', \n           'PavedDrive', 'GarageCond', 'GarageQual', 'SaleCondition']\n\nvalue_in_ord = set()\nfor col, v in uniques.items():\n    if col in ordinal:\n        print(v)\n        valu = v\n        for i in valu:\n            value_in_ord.add(i)","d6a6e53d":"#display the set of those values\nprint(value_in_ord)","9cde965d":"#change ordinal columns' values in the training data\nchange_ord = house_cpy.replace({\n    'RFn': 3 ,\n    'Mn': 2,\n    'BLQ': 4,\n    'Maj1':4,\n    'P':2,\n    'Ex': 5,\n    'Family':1,\n    'Maj2': 3,\n    'N':1,\n    'Unf':1,\n    'MnWw':1,\n    'Fa':2,\n    'TA':3,\n    'Gd':4,\n    'Po': 2,\n    'Sev': 2,\n    'Mod': 5,\n    'Min1': 7,\n    'Y':3,\n    'Normal':4,\n    'Partial':0,\n    'Fin': 4,\n    'AllPub':4,\n    'MnPrv':3,\n    'Rec': 3,\n    'GdWo':2,\n    'Typ': 8,\n    'AdjLand':2,\n    'GLQ': 6,\n    'Abnorml':3,\n    'NoSeWa':2,\n    'GdPrv':4,\n    'Alloca':1,\n    'Min2': 6,\n    'ALQ':5,\n    'No': 0,\n    'LwQ': 2,\n    'Av': 3\n}, inplace = True)","c34b0138":"#change ordinal columns' values in the test data\nchange_ord_ = house_ts_cpy.replace({\n    'RFn': 3 ,\n    'Mn': 2,\n    'BLQ': 4,\n    'Maj1':4,\n    'P':2,\n    'Ex': 5,\n    'Family':1,\n    'Maj2': 3,\n    'N':1,\n    'Unf':1,\n    'MnWw':1,\n    'Fa':2,\n    'TA':3,\n    'Gd':4,\n    'Po': 2,\n    'Sev': 2,\n    'Mod': 5,\n    'Min1': 7,\n    'Y':3,\n    'Normal':4,\n    'Partial':0,\n    'Fin': 4,\n    'AllPub':4,\n    'MnPrv':3,\n    'Rec': 3,\n    'GdWo':2,\n    'Typ': 8,\n    'AdjLand':2,\n    'GLQ': 6,\n    'Abnorml':3,\n    'NoSeWa':2,\n    'GdPrv':4,\n    'Alloca':1,\n    'Min2': 6,\n    'ALQ':5,\n    'No': 0,\n    'LwQ': 2,\n    'Av': 3\n}, inplace = True)","84f4f4ab":"#get stat after cleaning\n# pd.options.display.max_columns = None\n# house_cpy.describe()","b0195bd2":"#Handling our null values\nhouse_cpy = house_cpy.apply(lambda x: x.fillna(x.value_counts().index[0])) # Here we will use the most frequent value to fill na","f210bc67":"house_ts_cpy = house_ts_cpy.apply(lambda x: x.fillna(x.value_counts().index[0]))","26d634dd":"#Scaling and Standardizing the values\n\nX_ = house_cpy\ny = house['SalePrice']\n\n#align columns from both dataset so that each column corresponds to itself in the other dataset\nX_, house_ts_cpy = X_.align(house_ts_cpy, join='inner', axis=1)\n\nss = StandardScaler()\nXs_train = ss.fit_transform(X_)\nXs_test = ss.transform(house_ts_cpy)","55c52608":"#Random forest fitting\nfrom sklearn.ensemble import RandomForestRegressor\n\nrf = RandomForestRegressor()\nrf.fit(Xs_train, y)","a4ae7d7e":"scores = cross_val_score(rf, Xs_train, y, cv = 5 )\nscores.mean()","22d1ac19":"#Parameters for GridSearchCV, XGBoost\nparams = {\n    'max_depth': [None, 1, 2, 3],\n    'min_samples_leaf' : [1, 2, 3],\n    'min_samples_split' : [2, 4, 6],\n'min_weight_fraction_leaf' : [0, 2, 3]\n}","7ff68d24":"rf_ = GridSearchCV(rf, param_grid=params, cv=5, n_jobs=-1, scoring = 'r2')\nrf_.fit(Xs_train, y)","b491e0a6":"scores = cross_val_score(rf, Xs_train, y, cv = 5 )\nscores","e53c7796":"scores.mean() ","65ed7ebf":"pred_rf = rf_.predict(Xs_test)","3f029c89":"pred_rf","6ad966c5":"#Save\noutput = pd.DataFrame({'Id': house_ts['Id'],\n                       'SalePrice': pred_rf})\noutput.to_csv('submission_gridRf.csv', index=False)#worst than baseline","dbc86769":"alphas = np.linspace(0.1,10,30)\nclf = RidgeCV(alphas)","ab67ff0b":"#Fitting GridSearch\nclf.fit(Xs_train, y)","b1f14dc2":"scores_clf = cross_val_score(clf, Xs_train, y, cv = 10 )\nscores_clf.mean()","643f98d6":"lcv = LassoCV()","05eab41e":"#Fitting lasso\nlcv.fit(Xs_train, y)","fc05f10d":"scores_lcv = cross_val_score(lcv, Xs_train, y, cv = 10 )\nscores_lcv.mean()","ad6b2777":"enc = ElasticNetCV(l1_ratio= 1, cv = 10)","35743b46":"enc.fit(Xs_train, y)","b77147c0":"scores_enc = cross_val_score(enc, Xs_train, y, cv = 10 )\nscores_enc.mean()","589d465e":"#!pip install xgboost","29e38991":"from xgboost import XGBRegressor\n\nxg_reg = XGBRegressor(objective ='reg:squarederror' , colsample_bytree = 0.3, learning_rate = 0.1,\n                max_depth = 5, alpha = 10, n_estimators = 1000)\nxg_reg.fit(Xs_train, y)\nnp.sqrt(xg_reg.score(Xs_train, y))","4f28f65b":"pred = xg_reg.predict(Xs_test)\npred","a491108e":"output = pd.DataFrame({'Id': house_ts_cpy.index,\n                       'SalePrice': pred})\noutput.to_csv('submission3.csv', index=False)","fb554df7":"**2. Manually defining our categorical and nominal values**","086c53b1":"### Data Visualization","82060a26":"## Problem Statement:\n<p> In this competition, my teammates and I will be developing a model that predicts house prices given two datasets in which the the training data is in a seperate file than the test data. \n\n**Who is it for? & Why is it important?**\n    \nInvestigating house prices is important in determining the value of future built\/renovated houses. Homeowners, potential buyers (being the primary stakeholders), Real Estate Agents and banks (being the secondary stakeholders) are the involved parties who would benefit from building close-to-accurate model.\n\n\nIt's required to acknowldege the following:\n-  Utilize models like regression and classification, in which we should make use of the following:\n    - train-test split\n    - cross-validation\/grid searching for hyperparameters\n    - data analysis to investigate the correlation and the relationship across predictive variables\n    - consistently apply feature transformation (i.e preprocessing library) \n\n\n<\/p>","00d8fa3d":"After troubleshooting, we can see that Lasso Linear Regression seems to be working best from within all the other linear regressions. ElasticNet does not seem to be working well without parameter optimizations","ccf45408":"### Observation \n- There are a lot of missing values in some features (columns). \n\nPerhabs we need to do the following:\n- Drop those features aiming to build strong model(s) with higher prediction value(s) as those features may not have an influence on the prediction score.\n- Impute some data ~ implemented if the model doesn't have this feature\n- Eliminate outliers ~ will be implemented if model is sensitive to outliers","5da644ae":"## Generating Models","c52f0017":"First we will set our X and y and align them","44354b89":"###  RandomForestRegressor\n\nWe will use this model with two EDA and feature engineering techniques:\n\n**1. Kaggle tutorials: SimpleImputer & OneHotEncoding**\nSimple imputer will be able to handle null values within our data, while OneHotEncoder will handle nominal and categorical values\n\n**2. Manually defining our categorical and nominal values**","d3460542":"**Note:** Drop the Id column so that it doesn't interfere with our data analysis","0d0dddfb":"# House Prices - Advanced Regression Techniques (Kaggle)","eb88837d":"We can see that within this graph that there are only few selected features that actually has a high corrolation to the sale price. We believe that anything above 0.45 is going to have our model predicting the price well. Where the other values are fine tuning the model to be more accurate, though feature engineering these models can backfire if not done properly","1b5a40df":"### XGBOOST","75159383":"The most challenging part of this competition was data cleaning, as we have taken a more serious approach cleaning and keeping the integrity for the whole data.\n\nWe started by keeping all the columns intact, and ordering our columns to nominal and categorical. We have mainly done that since some of those values tend to have a high correlation to the price of the house. For example, if the pool quality was excellent, we have seen the house price would double over a house with no pool. After implementing those changes, we have seen that it was for the best to just remove columns with high null values, since they statistically speaking won't improve our model.\n\nAfter going further into the project, we have found that having categorical values instead of nominal values is not necessarily the best approach to take. We have tried hotencoding our model without manually setting our categorical values and the model's score was decent.\n\nFinally, from all the models we have chosen, we have seen that XGBoost gave the best score right after Random Forest. We were able to have an even better prediction just by dropping the columns with null values we mentioned above.\n\nThe biggest problems that we've faced was how hard it was to use gridsearch effectively. Using it sometimes did not provide us with a more accurate model. Another problem we faced was using pipeline and crossvalidation. The model's score here is based on R^2 where within the competition it is based on RMSE. This gave us a hard time measuring whether our model was working well or not.\n\nFor us to have our model work better, we believe that handling outliers, as well as feature selecting, may even provide us with better results.","93700459":"## Data Cleaning and EDA","da978da4":"Team 2","f570e5cb":"## Final Verdicts","430bae04":"We can see from these graphs that some of the features have a lot of outliers, but some do not. We will not deal with outliers within our project, but it is safe to stay that dealing with them may increase our model's performance.","4ce52786":"Team members: Hadi AlKhamees, Abrar AlMaskeen, Hussain AlQatari","cfa26b5e":"XGBoost is a gradient boosted decision tree model. It is a powerful tool that has been shown to provide some of the best models out there. The difference between a gradient decision tree and XGBoost is that XGBoost is more memeory efficient and has more tools within it. You will also have to download it to your Anaconda. You can do that by running the below code:","db5bc48f":"### Linear Regression"}}