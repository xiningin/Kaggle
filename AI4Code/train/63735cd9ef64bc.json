{"cell_type":{"b1d9627c":"code","37ac46e0":"code","e6e3a7ca":"code","80d637f3":"code","3667f080":"code","ca13c695":"code","880b30c3":"code","75e12cc3":"code","2edb163a":"code","05dad37d":"code","51a78353":"code","29c0c96b":"code","5d71d33b":"code","cc433c45":"code","7ad5a6a4":"code","f7d5abb9":"code","ff2a2785":"code","6f941474":"code","96ed12f8":"code","cb646a1e":"code","a05f47be":"code","375180ef":"code","5dd058d0":"code","0d0fe6aa":"code","e176ae6e":"code","223f89a6":"code","da5618e7":"code","b6d4fac7":"code","4fed5c63":"code","0b27f46a":"code","193521e3":"code","6d1bca11":"code","93415675":"code","632c2232":"code","ef3f3213":"code","5e9589ef":"code","130ae455":"code","971e0aa0":"code","b0033223":"code","0c0e2953":"code","a26f8573":"code","134ad4a6":"code","f8e26568":"code","39774d15":"code","157bd084":"code","41614a57":"code","e51e8896":"code","3d8cd1d8":"code","9718aa62":"code","4230e704":"code","f4fb5783":"code","fbc90f7a":"code","935bcff0":"code","41c99247":"code","f5dd9947":"code","6c974380":"code","48a14039":"code","3b4f8184":"code","b57793df":"code","52a239c7":"code","7f8215b4":"code","030753a6":"code","afd2cb03":"code","b48420a6":"code","bd58c70d":"code","b16f18b7":"code","62ab0501":"code","a3ab8eb1":"code","ba5d4a63":"code","e6a81b00":"code","126de8dd":"code","4c8ccec5":"code","2fa5baf4":"code","6fc4ee43":"code","f0b4ff8c":"code","a9f80144":"code","22164c97":"code","79243261":"code","13ae483e":"code","7d6fb43a":"code","05317536":"code","2999b90a":"code","cda3c017":"code","58c47466":"code","1e9c129e":"code","369127f5":"code","107b2325":"code","f9bf7e04":"code","f282c39b":"code","874c3fbf":"code","82bc5b8b":"code","15d65367":"code","d44b1092":"code","1d6a114d":"code","4c707542":"code","6b54ddda":"code","fa16bb0e":"code","285566ce":"code","79207ade":"code","3bc4b8e8":"markdown","405011ff":"markdown","d4d2c30f":"markdown","294921c3":"markdown","53af95f5":"markdown","8851ca4b":"markdown","929c55df":"markdown","9560f25d":"markdown","013dad01":"markdown","7d079410":"markdown","04a81d3a":"markdown","7b9ae234":"markdown","50b03200":"markdown","2bb3da29":"markdown","9b6758c2":"markdown","3d311f3e":"markdown","2f25952d":"markdown","956d90e3":"markdown","e99e5dc7":"markdown","cee47a3f":"markdown","330c1a68":"markdown","815858c2":"markdown","1b70556b":"markdown","60b1e5af":"markdown","d8a38408":"markdown","a407dfb9":"markdown","7c3a2b57":"markdown","552bb148":"markdown","7f9c408d":"markdown","6b1d9d5a":"markdown","87087991":"markdown","b2ad5c76":"markdown","ea26d4d9":"markdown","4d361f17":"markdown","3aa63fe8":"markdown","793cd473":"markdown","222fa973":"markdown","690f4408":"markdown","6940858b":"markdown","ec52f8e0":"markdown","bdf5a612":"markdown","bda20825":"markdown","f173c1cd":"markdown","97a7488b":"markdown","9ba38129":"markdown","4c1cf992":"markdown","ab7c6590":"markdown"},"source":{"b1d9627c":"from warnings import filterwarnings\nfilterwarnings('ignore')","37ac46e0":"# Some Libraries Imported\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\n\n# For reading stock data from yahoo\nfrom pandas_datareader.data import DataReader\n# For time stamps\nfrom datetime import datetime","e6e3a7ca":"# The tech stocks we'll use for this analysis\ntech_list = ['AAPL', 'GOOG', 'TSLA', 'FB']","80d637f3":"# Set up End and Start times for data grab (We will analyze for 2 years)\nend = datetime.now()\nstart = datetime(end.year - 2, end.month, end.day)","3667f080":"#For loop for grabing yahoo finance data and setting as a dataframe\nfor stock in tech_list:   \n    # Set DataFrame as the Stock Ticker\n    globals()[stock] = DataReader(stock, 'yahoo', start, end)","ca13c695":"company_list = [AAPL, GOOG, TSLA, FB]\ncompany_name = [\"APPLE\", \"GOOGLE\", \"TESLA\", \"FACEBOOK\"]\n\nfor company, com_name in zip(company_list, company_name):\n    company[\"company_name\"] = com_name\n    \ndf = pd.concat(company_list, axis=0)","880b30c3":"df.head()","75e12cc3":"df.tail()","2edb163a":"# Let's see a historical view of the closing price of companies\n\nplt.figure(figsize=(12, 8))\nplt.subplots_adjust(top=1.25, bottom=1.2)\ncolorlist=['Red','Blue','Green','Purple']\nfor i, company in enumerate(company_list, 1):\n    plt.subplot(2, 2, i)\n    company['Adj Close'].plot(color=colorlist[i-1])\n    plt.ylabel('Adj Close')\n    plt.xlabel(None)\n    plt.title(f\"{tech_list[i - 1]}\")","05dad37d":"# Now let's plot the total volume of stock being traded each day\nplt.figure(figsize=(15, 10))\nplt.subplots_adjust(top=1.25, bottom=1.2)\ncolorlist=['Black','Blue','Green','Purple']\nfor i, company in enumerate(company_list, 1):\n    plt.subplot(2, 2, i)\n    company['Volume'].plot(color=colorlist[i-1])\n    plt.ylabel('Volume')\n    plt.xlabel(None)\n    plt.title(f\"{tech_list[i - 1]}\")","51a78353":"# Set the Moving Average Day\nma_day = [10, 20, 50]","29c0c96b":"for ma in ma_day:\n    for company in company_list:\n        column_name = f\"MA for {ma} days\"\n        company[column_name] = company['Adj Close'].rolling(ma).mean()","5d71d33b":"print(AAPL.columns)","cc433c45":"df.groupby(\"company_name\").hist(figsize=(12, 12),color='seagreen');","7ad5a6a4":"fig, axes = plt.subplots(nrows=2, ncols=2)\nfig.set_figheight(8)\nfig.set_figwidth(15)\n\nAAPL[['Adj Close', 'MA for 10 days', 'MA for 20 days', 'MA for 50 days']].plot(ax=axes[0,0])\naxes[0,0].set_title('APPLE')\n\nGOOG[['Adj Close', 'MA for 10 days', 'MA for 20 days', 'MA for 50 days']].plot(ax=axes[0,1])\naxes[0,1].set_title('GOOGLE')\n\nTSLA[['Adj Close', 'MA for 10 days', 'MA for 20 days', 'MA for 50 days']].plot(ax=axes[1,0])\naxes[1,0].set_title('TESLA')\n\nFB[['Adj Close', 'MA for 10 days', 'MA for 20 days', 'MA for 50 days']].plot(ax=axes[1,1])\naxes[1,1].set_title('FACEBOOK')\n\nfig.tight_layout()","f7d5abb9":"# We'll use pct_change to find the percent change for each day\nfor company in company_list:\n    company['Daily Return'] = company['Adj Close'].pct_change()\n\n# Then we'll plot the daily return percentage\nfig, axes = plt.subplots(nrows=2, ncols=2)\nfig.set_figheight(8)\nfig.set_figwidth(15)\n\nAAPL['Daily Return'].plot(ax=axes[0,0], legend=True, linestyle='--', marker='o',markerfacecolor='black')\naxes[0,0].set_title('APPLE')\n\nGOOG['Daily Return'].plot(ax=axes[0,1], legend=True, linestyle='--', marker='o',markerfacecolor='black')\naxes[0,1].set_title('GOOGLE')\n\nTSLA['Daily Return'].plot(ax=axes[1,0], legend=True, linestyle='--', marker='o',markerfacecolor='black')\naxes[1,0].set_title('TESLA')\n\nFB['Daily Return'].plot(ax=axes[1,1], legend=True, linestyle='--', marker='o',markerfacecolor='black')\naxes[1,1].set_title('FACEBOOK')\n\nfig.tight_layout()","ff2a2785":"# Note the use of dropna() here, otherwise the NaN values can't be read by seaborn\nplt.figure(figsize=(12, 12))\n\nfor i, company in enumerate(company_list, 1):\n    plt.subplot(2, 2, i)\n    sns.distplot(company['Daily Return'].dropna(), bins=100, color='orange')\n    plt.ylabel('Daily Return')\n    plt.title(f'{company_name[i - 1]}')\n    # Skewness and Kurtosis\n    print(f'{company_name[i - 1]}'+\" Skewness: %f\" % company['Daily Return'].skew())\n    print(f'{company_name[i - 1]}'+\" Kurtosis: %f\" % company['Daily Return'].kurt())\n# Could have also done:\n#AAPL['Daily Return'].hist()\n","6f941474":"# Grab all the closing prices for the tech stock list into one DataFrame\nclosing_df = DataReader(tech_list, 'yahoo', start, end)['Adj Close']\n\n# Let's take a quick look\nclosing_df.head() ","96ed12f8":"# Make a new tech returns DataFrame\ntech_rets = closing_df.pct_change()\ntech_rets.head()","cb646a1e":"# Comparing Google to itself should show a perfectly linear relationship\nsns.jointplot('TSLA', 'TSLA', tech_rets, kind='scatter', color='seagreen')","a05f47be":"# We'll use joinplot to compare the daily returns of Google and Microsoft\nsns.jointplot('GOOG', 'FB', tech_rets, kind='scatter',color='darkblue')","375180ef":"# We can simply call pairplot on our DataFrame for an automatic visual analysis \n# of all the comparisons\n\nsns.pairplot(tech_rets, kind='kde')","5dd058d0":"# We can simply call pairplot on our DataFrame for an automatic visual analysis \n# of all the comparisons\n\nsns.pairplot(tech_rets, kind='reg')","0d0fe6aa":"# Set up our figure by naming it returns_fig, call PairPLot on the DataFrame\nreturn_fig = sns.PairGrid(tech_rets.dropna())\n\n# Using map_upper we can specify what the upper triangle will look like.\nreturn_fig.map_upper(plt.scatter, color='seagreen')\n\n# We can also define the lower triangle in the figure, inclufing the plot type (kde) \n# or the color map (BluePurple)\nreturn_fig.map_lower(sns.kdeplot, cmap='Greens_r')\n\n# Finally we'll define the diagonal as a series of histogram plots of the daily return\nreturn_fig.map_diag(plt.hist, bins=30)","e176ae6e":"# Set up our figure by naming it returns_fig, call PairPLot on the DataFrame\nreturns_fig = sns.PairGrid(closing_df)\n\n# Using map_upper we can specify what the upper triangle will look like.\nreturns_fig.map_upper(plt.scatter,color='darkblue')\n\n# We can also define the lower triangle in the figure, inclufing the plot type (kde) or the color map (BluePurple)\nreturns_fig.map_lower(sns.kdeplot,cmap='Blues_r')\n\n# Finally we'll define the diagonal as a series of histogram plots of the daily return\nreturns_fig.map_diag(plt.hist,bins=30)","223f89a6":"# Let's go ahead and use sebron for a quick correlation plot for the daily returns\nsns.heatmap(tech_rets.corr(), annot=True, cmap='summer')","da5618e7":"sns.heatmap(closing_df.corr(), annot=True, cmap='summer')","b6d4fac7":"# Let's start by defining a new DataFrame as a clenaed version of the oriignal tech_rets DataFrame\nrets = tech_rets.dropna()\n\narea = np.pi*20\n\nplt.figure(figsize=(12, 10))\nplt.scatter(rets.mean(), rets.std(), s=area)\nplt.xlabel('Expected return')\nplt.ylabel('Risk')\n\nfor label, x, y in zip(rets.columns, rets.mean(), rets.std()):\n    plt.annotate(label, xy=(x, y), xytext=(50, 50), textcoords='offset points', ha='right', va='bottom', \n                 arrowprops=dict(arrowstyle='-', color='blue', connectionstyle='arc3,rad=-0.3'))","4fed5c63":"# Summary Stats\nAAPL.describe()","0b27f46a":"# General info\nAAPL.info()","193521e3":"Column_List = [\"High\", \"Low\",\"Open\",\"Close\", \"Volume\",\"Adj Close\"]","6d1bca11":"# Generate whisker plots to detect the presence of any outliers\nfig, ax = plt.subplots (len(Column_List), figsize = (10, 20))\n\nfor i, col_list in enumerate(Column_List):\n    sns.boxplot(AAPL[col_list], ax = ax[i], palette = \"winter\", orient = 'h')\n    ax[i].set_title(\"Whisker Plot for Outlier Detection on Apple Datas on\" + \" \" + col_list, fontsize = 10)\n    ax[i].set_ylabel(col_list, fontsize = 8)\n    fig.tight_layout(pad = 1.1)","93415675":"# Visualize the spread and skweness through the distribution plot\n\n# Use the Column_List : list initialized above in the following steps\nfig, ax = plt.subplots(len(Column_List), figsize = (15, 10))\n\nfor i, col_list in enumerate(Column_List):\n    sns.distplot(AAPL[col_list], hist = True, ax = ax[i])\n    ax[i].set_title (\"Apple Datas Frequency Distribution of\" + \" \" + col_list, fontsize = 10)\n    ax[i].set_xlabel (col_list, fontsize = 8)\n    ax[i].set_ylabel ('Distribution Value', fontsize = 8)\n    fig.tight_layout (pad = 1.1) # To provide space between plots\n    ax[i].grid('on') # Enabled to view and make markings","632c2232":"AAPL[[\"Open\",\"High\",\"Low\",\"Close\"]].plot.area(figsize=(15,10),alpha=0.5);\nplt.title('Apple Finance Stock Trend')\nplt.show()","ef3f3213":"# A glimpse of how the market shares varied over the given time\n\n# Create a list for numerical columns that are to be visualized\nColumn_List = ['Open', 'High', 'Low', 'Close', 'Adj Close', 'Volume']\n\n# Plot to view the same\nAAPL.plot(y = Column_List, subplots = True, layout = (3, 3), figsize = (15, 15), sharex = False, title = \"Apple Stock Value Trend from 2019-11 - 2020-11\", rot = 45);","5e9589ef":"#Get the stock quote\ndf = DataReader('AAPL', data_source='yahoo', start='2019-06-30', end='2020-06-30')\n#Show teh data\ndf","130ae455":"plt.figure(figsize=(16,8))\nplt.title('Close Price History')\nplt.plot(df['Close'])\nplt.xlabel('Date', fontsize=18)\nplt.ylabel('Close Price USD ($)', fontsize=18)\nplt.show()","971e0aa0":"#Create a new dataframe with only the 'Close column\ndata = df.filter(['Close'])\n#Convert the dataframe to a numpy array\ndataset = data.values\n#Get the number of rows to train the model on\ntraining_data_len = int(np.ceil( len(dataset) * .8 ))\n\ntraining_data_len","b0033223":"#Scale the data\nfrom sklearn.preprocessing import MinMaxScaler\n\nscaler = MinMaxScaler(feature_range=(0,1))\nscaled_data = scaler.fit_transform(dataset)\n\nscaled_data[0:5]","0c0e2953":"#Create the training data set\n#Create the scaled training data set\ntrain_data = scaled_data[0:int(training_data_len), :]\n#Split the data into x_train and y_train data sets\nx_train = []\ny_train = []\n\nfor i in range(60, len(train_data)):\n    x_train.append(train_data[i-60:i, 0])\n    y_train.append(train_data[i, 0])\n    if i<= 61:\n        print(x_train)\n        print(y_train)\n        print()\n        \n# Convert the x_train and y_train to numpy arrays \nx_train, y_train = np.array(x_train), np.array(y_train)\n\n#Reshape the data\nx_train = np.reshape(x_train, (x_train.shape[0], x_train.shape[1], 1))\n# x_train.shape","a26f8573":"#Create the testing data set\n#Create a new array containing scaled values from index 1543 to 2002 \ntest_data = scaled_data[training_data_len - 60: , :]\n#Create the data sets x_test and y_test\nx_test = []\ny_test = dataset[training_data_len:, :]\nfor i in range(60, len(test_data)):\n    x_test.append(test_data[i-60:i, 0])\n    \n# Convert the data to a numpy array\nx_test = np.array(x_test)\n\n# Reshape the data\nx_test = np.reshape(x_test, (x_test.shape[0], x_test.shape[1], 1 ))\n","134ad4a6":"from keras.models import Sequential\nfrom keras.layers import Dense, LSTM\n\n#Build the LSTM model\nmodel_lstm = Sequential()\nmodel_lstm.add(LSTM(64, return_sequences=True, input_shape= (x_train.shape[1], 1)))\nmodel_lstm.add(LSTM(64, return_sequences= False))\nmodel_lstm.add(Dense(32))\nmodel_lstm.add(Dense(1))\n\n# Compile the model\nmodel_lstm.compile(optimizer='adam', loss='mean_squared_error')\n\n#Train the model\nmodel_lstm.fit(x_train, y_train, batch_size=20, epochs=20)","f8e26568":"# importing libraries\nfrom keras.models import Sequential\nfrom keras.layers import Dense\nfrom keras.layers import SimpleRNN\nfrom keras.layers import Dropout\n\n# initializing the RNN\nmodel_rnn = Sequential()\n\n# adding first RNN layer and dropout regulatization\nmodel_rnn.add(SimpleRNN(units = 50,activation = \"tanh\", return_sequences = True,input_shape = (x_train.shape[1],1)))\nmodel_rnn.add(Dropout(0.2))\n# adding second RNN layer and dropout regulatization\nmodel_rnn.add(SimpleRNN(units = 50, activation = \"tanh\", return_sequences = True))\nmodel_rnn.add(Dropout(0.2))\n# adding third RNN layer and dropout regulatization\nmodel_rnn.add(SimpleRNN(units = 50,activation = \"tanh\", return_sequences = False))\nmodel_rnn.add(Dropout(0.2))\n# adding the output layer\nmodel_rnn.add(Dense(units = 1))\n# compiling RNN\nmodel_rnn.compile(optimizer = \"adam\", loss = \"mean_squared_error\")\n# fitting the RNN\nmodel_rnn.fit(x_train, y_train, epochs = 100, batch_size = 32)","39774d15":"# Get the LSTM model predicted price values \npredictions_lstm = model_lstm.predict(x_test)\npredictions_lstm = scaler.inverse_transform(predictions_lstm)","157bd084":"# Get the RNN models predicted price values \npredictions_rnn = model_rnn.predict(x_test)\npredictions_rnn = scaler.inverse_transform(predictions_rnn)","41614a57":"from sklearn import metrics\n\n# Get the root mean squared error (RMSE)\nmse_lstm = metrics.mean_squared_error(y_test, predictions_lstm)\nrmse_lstm = np.sqrt(mse_lstm)\n\nprint(\"LSTM Model RMSE: \",rmse_lstm)\n# Get r2 score\nr2_lstm = metrics.r2_score(y_test, predictions_lstm)\nprint(\"LSTM Model r2: \",r2_lstm)","e51e8896":"# Get the root mean squared error (RMSE)\nmse_rnn = metrics.mean_squared_error(y_test, predictions_rnn)\nrmse_rnn = np.sqrt(mse_rnn)\n\nprint(\"RNN Model RMSE: \",rmse_rnn)\n# Get r2 score\nr2_rnn = metrics.r2_score(y_test, predictions_rnn)\nprint(\"RNN Model r2: \",r2_rnn)","3d8cd1d8":"# Plot the data\ntrain = data[:training_data_len]\nvalid = data[training_data_len:]\nvalid['Predictions_RNN'] = predictions_rnn\n# Visualize the data\nplt.figure(figsize=(16,8))\nplt.title('Model')\nplt.xlabel('Date', fontsize=18)\nplt.ylabel('Close Price USD ($)', fontsize=18)\nplt.plot(train['Close'])\nplt.plot(valid[['Close', 'Predictions_RNN']])\nplt.legend(['Train', 'Val', 'Predictions_RNN'], loc='lower right')\nplt.show()","9718aa62":"#Show the valid and predicted prices\nvalid[0:10]","4230e704":"# Plot the data\ntrain = data[:training_data_len]\nvalid = data[training_data_len:]\nvalid['Predictions_LSTM'] = predictions_lstm\n# Visualize the data\nplt.figure(figsize=(16,8))\nplt.title('Model')\nplt.xlabel('Date', fontsize=18)\nplt.ylabel('Close Price USD ($)', fontsize=18)\nplt.plot(train['Close'])\nplt.plot(valid[['Close', 'Predictions_LSTM']])\nplt.legend(['Train', 'Val', 'Predictions_LSTM'], loc='lower right')\nplt.show()","f4fb5783":"#Show the valid and predicted prices\nvalid[0:10]","fbc90f7a":"ndf = pd.read_csv('..\/input\/tsf-datasets\/india-news-headlines.csv', parse_dates=[0], infer_datetime_format=True,error_bad_lines=False,usecols =[\"publish_date\",\"headline_text\"])\nndf = ndf.rename(columns={\"publish_date\": \"Date\"})\nndf.head()","935bcff0":"ndf.tail()","41c99247":"ndf.info()","f5dd9947":"start_date = pd.to_datetime('2019-06-30')\nend_date = pd.to_datetime('2020-06-30')\nndf=ndf.loc[(ndf['Date'] > start_date) & (ndf['Date'] < end_date)]","6c974380":"ndf=ndf.reset_index()","48a14039":"ndf=ndf.drop(\"index\",axis=1)","3b4f8184":"ndf.head()","b57793df":"ndf.tail()","52a239c7":"# Dropping duplicates by grouping the same dates.\nndf['headline_text'] = ndf.groupby(['Date']).transform(lambda x : ' '.join(x)) \nndf = ndf.drop_duplicates() \nndf.reset_index(inplace = True, drop = True)","7f8215b4":"ndf.head()","030753a6":"# uppercase-lowercase conversion\nndf['headline_text'] = ndf['headline_text'].apply(lambda x: \" \".join(x.lower() for x in x.split()))","afd2cb03":"# punctuation\nndf['headline_text'] = ndf['headline_text'].str.replace('[^\\w\\s]','')\n","b48420a6":"# numbers\nndf['headline_text'] = ndf['headline_text'].str.replace('\\d','')\n","bd58c70d":"#!pip install nltk","b16f18b7":"#stopwords\nimport nltk\nnltk.download('stopwords')","62ab0501":"from nltk.corpus import stopwords\nsw = stopwords.words('english')\nndf['headline_text'] = ndf['headline_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in sw))\n","a3ab8eb1":"## Deletion of sparse.\ndelete = pd.Series(' '.join(ndf['headline_text']).split()).value_counts()[-1000:]\nndf['headline_text'] = ndf['headline_text'].apply(lambda x: \" \".join(x for x in x.split() if x not in delete))\n","ba5d4a63":"nltk.download('wordnet')","e6a81b00":"#!pip install textblob","126de8dd":"#lemmatisation\nfrom textblob import Word\n\nndf['headline_text'] = ndf['headline_text'].apply(lambda x: \" \".join([Word(word).lemmatize() for word in x.split()])) ","4c8ccec5":"ndf.info()","2fa5baf4":"ndf.head()","6fc4ee43":"ndf.tail()","f0b4ff8c":"ndf['headline_text'][0:10]","a9f80144":"ndf[ndf['headline_text'].duplicated(keep=False)].sort_values('headline_text').head(8)","22164c97":"from textblob import TextBlob","79243261":"#Functions to get the subjectivity and polarity\ndef getSubjectivity(text):\n    return TextBlob(text).sentiment.subjectivity\n\ndef getPolarity(text):\n    return  TextBlob(text).sentiment.polarity","13ae483e":"ndf['Subjectivity'] = ndf['headline_text'].apply(getSubjectivity)\nndf['Polarity'] = ndf['headline_text'].apply(getPolarity)\nndf.head()","7d6fb43a":"plt.figure(figsize = (10,6))\nndf['Polarity'].hist(color = 'purple')","05317536":"plt.figure(figsize = (10,6))\nndf['Subjectivity'].hist(color = 'blue')","2999b90a":"nltk.download('vader_lexicon')","cda3c017":"#Adding sentiment score to ndf by using SentimentIntensityAnalyzer\nfrom nltk.sentiment.vader import SentimentIntensityAnalyzer\nsia = SentimentIntensityAnalyzer()\n\nndf['Compound'] = [sia.polarity_scores(v)['compound'] for v in ndf['headline_text']]\nndf['Negative'] = [sia.polarity_scores(v)['neg'] for v in ndf['headline_text']]\nndf['Neutral'] = [sia.polarity_scores(v)['neu'] for v in ndf['headline_text']]\nndf['Positive'] = [sia.polarity_scores(v)['pos'] for v in ndf['headline_text']]\nndf[0:5]","58c47466":"df_merge = pd.merge(df, ndf, how='inner', on='Date')\ndf_merge.head()","1e9c129e":"df_merge.tail()","369127f5":"df_final = df_merge[['Close','Subjectivity', 'Polarity', 'Compound', 'Negative', 'Neutral' ,'Positive']]\ndf_final","107b2325":"from sklearn.preprocessing import MinMaxScaler\nsc = MinMaxScaler()\ndf_scaled = pd.DataFrame(sc.fit_transform(df_final))\ndf_scaled.columns = df_final.columns\ndf_scaled.index = df_final.index\ndf_scaled.head()","f9bf7e04":"X=df_scaled.drop(\"Close\",axis=1)\ny=df_scaled[\"Close\"]","f282c39b":"# shuffle and split training and test sets\n\nfrom sklearn.model_selection import train_test_split\nX_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.2,random_state=42)\n","874c3fbf":"from sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor\nfrom sklearn import metrics","82bc5b8b":"## Using Random Forest Regression\nrf = RandomForestRegressor()\nrf.fit(X_train, y_train)\ny_pred_rf=rf.predict(X_test)\n\nmse_rf = metrics.mean_squared_error(y_test, y_pred_rf)\nrmse_rf = np.sqrt(mse_rf)\n\nprint(\"Random Forest Model RMSE: \",rmse_rf)","15d65367":"# !pip install xgboost","d44b1092":"from xgboost import XGBRegressor\nxgb = XGBRegressor(colsample_bytree = 0.6, \n                         learning_rate = 0.01, \n                         max_depth = 2, \n                         n_estimators = 1000) \nxgb.fit(X_train, y_train)\ny_pred_xgb=xgb.predict(X_test)\n\nmse_xgb = metrics.mean_squared_error(y_test, y_pred_xgb)\nrmse_xgb = np.sqrt(mse_xgb)\n\nprint(\"XGB Model RMSE: \",rmse_xgb)","1d6a114d":"# !pip install lightgbm","4c707542":"from lightgbm import LGBMRegressor\nlgbm = LGBMRegressor(learning_rate = 0.1, \n                           max_depth = 8, \n                           n_estimators = 50,\n                           colsample_bytree = 0.4,\n                           num_leaves = 10)\nlgbm.fit(X_train, y_train)\ny_pred_lgbm=lgbm.predict(X_test)\n\nmse_lgbm = metrics.mean_squared_error(y_test, y_pred_lgbm)\nrmse_lgbm = np.sqrt(mse_lgbm)\n\nprint(\"Light GBM Model RMSE: \",rmse_lgbm)","6b54ddda":"# !pip install catboost","fa16bb0e":"from catboost import CatBoostRegressor \ncatb = CatBoostRegressor(iterations = 500, \n                               learning_rate = 0.1, \n                               depth = 5)\ncatb.fit(X_train, y_train)\ny_pred_cat=catb.predict(X_test)\n","285566ce":"mse_cat = metrics.mean_squared_error(y_test, y_pred_cat)\nrmse_cat = np.sqrt(mse_cat)\n\nprint(\"Catboost Model RMSE: \",rmse_cat)","79207ade":"## Using AdaBoostRegressor\nadb = AdaBoostRegressor()\nadb.fit(X_train, y_train)\ny_pred_adb=adb.predict(X_test)\n\nmse_adb = metrics.mean_squared_error(y_test, y_pred_adb)\nrmse_adb = np.sqrt(mse_adb)\n\nprint(\"AdaBoost Model RMSE: \",rmse_adb)","3bc4b8e8":"* The high is the highest price at which a stock traded during a period. \n* The low is the lowest price of the period. ","405011ff":"### Loading Textual Data","d4d2c30f":"\n* Objective: Create a hybrid model for stock price\/performance prediction using numerical analysis of historical stock prices, and sentimental analysis of news headlines.\n* Stock to analyze and predict - SENSEX (S&P BSE SENSEX)\n\n* Download historical stock prices from finance.yahoo.com\n* Download textual (news) data from https:\/\/bit.ly\/36fFPI6\n\n\n\n\n# Author: Muhammet Varl\u0131","294921c3":"One of the most basic ways to measure risk is to compare the expected return with the standard deviation of daily returns.","53af95f5":"Now we can compare the daily percentage return of two stocks to check how correlated. First let's see a sotck compared to itself.","8851ca4b":"# **Hybrid model Analysis**","929c55df":"## **Creating RNN model**","9560f25d":"# **Sentiment Analysis of Indian News Headlines**","013dad01":"* Here we will analyze the changes in the stocks of various technology companies with simple visualization methods.","7d079410":"### Xgboost","04a81d3a":"## The changes in price of the stock overtime.","7b9ae234":"* **Selecting a company and concentrating on analysis on it**","50b03200":"## Create Hybrid Model","2bb3da29":"# The moving average of the various stocks","9b6758c2":"### Data Pre-Processing","3d311f3e":"* Take the datas for 2 years","2f25952d":"## **Plotting Train Data, Validation Data and Predictions LSTM Model**","956d90e3":"If two stocks are perfectly (and positivley) correlated with each other a linear relationship bewteen its daily return values should occur.  We will use sns.pairplot() to automatically create this plot.","e99e5dc7":"### **Some information about financial data**","cee47a3f":"### AdaBoost","330c1a68":"## Sentiment Analysis","815858c2":"# **Analysis of Stock Market Prices and Stock Price Prediction by RNN and LSTM**","1b70556b":"# The daily return of the stock on average.","60b1e5af":"# Risk Analysis","d8a38408":"Just as we suspected with our PairPlot, we see here that numerically and visually, Google and Tesla's daily stock return do not have very strong correlations compared to others. The strongest correlation is seen between Apple and Tesla. It's also interesting to see all tech companies positively associated.","a407dfb9":"### Correlation","7c3a2b57":"### Train-Test ","552bb148":"## **Plotting Train Data, Validation Data and Predictions RNN Model**","7f9c408d":"* The adjusted closing price amends a stock's closing price to reflect that stock's value after accounting for any corporate actions.\n* It is often used when examining historical returns or doing a detailed analysis of past performance.\n* Stock values are stated in terms of the closing price and the adjusted closing price.\n* The closing price is the raw price, which is just the cash value of the last transacted price before the market closes.\n* The adjusted closing price factors in anything that might affect the stock price after the market closes.","6b1d9d5a":"## **Volume**","87087991":"### Random Forest","b2ad5c76":"### Catboost","ea26d4d9":"### Light GBM","4d361f17":"Now that we have all the closing prices, let's go ahead and get the daily return for all the stocks, like we did for the Apple stock.","3aa63fe8":"* Volume is the total number of shares traded in a security over a period. Every time buyers and sellers exchange shares, the amount gets added to the period\u2019s total volume. Studying volume patterns are an essential aspect of technical analysis because it can show the significance of a stock\u2019s price movement.\n\n* A price change that occurs in high volume can carry more weight because it indicates that many traders were behind the move. Conversely, a lower volume price move can be perceived as less important.","793cd473":"## **Stock Market Prediction using Numerical and Textual Analysis**","222fa973":"Above we can see all the relationships between all stocks regarding daily returns. For example, there does not seem to be a very high correlation relationship between Google and Tesla.","690f4408":"# 6. Predicting the closing price stock price of APPLE :","6940858b":"## **Adj Close**","ec52f8e0":"### Feature Scaling using MinMaxScaler","bdf5a612":"# **Creating LSTM Model**","bda20825":"Now let's go ahead and plot all the additional Moving Averages","f173c1cd":"To build a DataFrame with all the ['Close'] columns for each of the stocks dataframes.","97a7488b":"* LSTMs expect our data to be in a specific format, usually a 3D array. We start by creating data in 60 timesteps and converting it into an array using NumPy. Next, we convert the data into a 3D dimension array with X_train samples, 60 timestamps, and one feature at each step.","9ba38129":"## **Models Scores**","4c1cf992":"# The correlation between different stocks Adj Close prices.","ab7c6590":"### Visualization of companies changes over various MA days and 'Adj Close'"}}