{"cell_type":{"605d6025":"code","cf1c221f":"code","7b7c9d1f":"code","25ebc0be":"code","ae458ffe":"code","a1855729":"code","b2bf0592":"code","2c5cf6fd":"code","739fb7d1":"code","ba4f22eb":"code","ce32945f":"code","06487f33":"code","c5608838":"code","223bc25c":"code","ae76af77":"code","64495399":"code","9f316ea8":"code","c4d2c370":"code","ceab047d":"code","0a447752":"code","e59d5ede":"code","56c695a3":"code","1a95f5f5":"code","27b08121":"code","99908123":"code","ba30c154":"code","c7cb876a":"code","59149fce":"code","ba42ed72":"markdown","1db845b8":"markdown","f88e0728":"markdown","2991cdb5":"markdown","11325b34":"markdown","c54f7a42":"markdown","484aaf83":"markdown","86fe12c4":"markdown","01928b33":"markdown","bffa1de5":"markdown","b05fcc90":"markdown","0bcc2107":"markdown","17488c3d":"markdown","4cf819f7":"markdown","bec9501d":"markdown","e18682bd":"markdown","08afd5fc":"markdown","b6e86fdc":"markdown","1b0b9c42":"markdown","49958e90":"markdown","72fc8057":"markdown","dd24bf26":"markdown"},"source":{"605d6025":"! pip install tensorflow_addons keras_applications","cf1c221f":"import json\nimport pandas as pd\nimport numpy as np\nimport cv2\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport tensorflow as tf\nimport tensorflow_addons as tfa\nimport json \nimport os","7b7c9d1f":"# csv file path\nCSV_FILE_PATH = \"..\/input\/illionis-data-cleaned-csv-data\/Final_Dataset.csv\"\n\n# images directory path\nIMAGES_PATH = \"..\/input\/face2bmi-alignedimgs\/dataset\/\"\n\ndataset = pd.read_csv(CSV_FILE_PATH)\ndataset = dataset[dataset.id != \"B58769.png\"]\ndataset = dataset.sort_values('id')\n\ndataset.head()","25ebc0be":"from kaggle_datasets import KaggleDatasets\n\nprint(\"Tensorflow version \" + tf.__version__)","ae458ffe":"# Detect hardware, return appropriate distribution strategy\ntry:\n    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection. No parameters necessary if TPU_NAME environment variable is set. On Kaggle this is always the case.\n    print('Running on TPU ', tpu.master())\nexcept ValueError:\n    tpu = None\n\nif tpu:\n    tf.config.experimental_connect_to_cluster(tpu)\n    tf.tpu.experimental.initialize_tpu_system(tpu)\n    strategy = tf.distribute.experimental.TPUStrategy(tpu)\nelse:\n    strategy = tf.distribute.get_strategy() # default distribution strategy in Tensorflow. Works on CPU and single GPU.\n\nprint(\"REPLICAS: \", strategy.num_replicas_in_sync)","a1855729":"# you can list the bucket with \"!gsutil ls $GCS_DS_PATH\"\nGCS_DS_PATH = KaggleDatasets().get_gcs_path() \n!gsutil ls $GCS_DS_PATH","b2bf0592":"# list to store filenames stored in GCS Path\n# filesnames are used to create TFRecord for TPU training\nfilenames = list()\n\n# add all files inside train folder in filenames list\nfor filename in os.listdir(\"..\/input\/illionis-data-cleaned-csv-data\/dataset\/Train\/\"):\n    filenames.append( GCS_DS_PATH + \"\/dataset\/Train\/\" + filename)\n    \ntrain_filenames = filenames\nvalid_filenames = filenames[50:]\n\ntest_filenames = []\n\n# add all files inside test folder in filenames list\nfor filename in os.listdir(\"..\/input\/illionis-data-cleaned-csv-data\/dataset\/Test\/\"):\n    test_filenames.append(GCS_DS_PATH + \"\/dataset\/Test\/\" + filename)\n","2c5cf6fd":"train_filenames","739fb7d1":"valid_filenames","ba4f22eb":"test_filenames","ce32945f":"# pre-trained model choices\nMODEL_CHOICES_LIST = [\n    'INCEPTION_V3',\n    'VGG19',\n    'XCEPTION',\n    'VGGFACES'\n]\n\n# change the pre-trained model by changing the index from [0,3]\nMODEL_NAME = MODEL_CHOICES_LIST[0]\n\n# IMAGE SIZE used for training\nIMAGE_SIZE = [256,256]\n\n# IMAGE SIZE with CHANNELS used for training\nIMAGE_SIZE_WITH_CHANNELS = [256,256,3]\n\n# BATCH SIZE used for training\nBATCH_SIZE = 128\n\nAUTO = tf.data.experimental.AUTOTUNE\n\n# function to apply the appropriate pre-processing function as per the base model\ndef preprocess_image_as_per_base_model(image):\n    \n    preprocessed_image = None\n    \n    # apply inception-v3 pre-processing function when inception-v3 model is selected\n    if MODEL_NAME == 'INCEPTION_V3':\n        preprocessed_image = tf.keras.applications.inception_v3.preprocess_input(image)\n    \n    # apply VGG19 pre-processing function when VGG19 model is selected\n    elif MODEL_NAME == 'VGG19':\n        preprocessed_image = tf.keras.applications.vgg19.preprocess_input(image)\n\n    # apply XCEPTION pre-processing function when XCEPTION model is selected\n    elif MODEL_NAME == 'XCEPTION':\n        preprocessed_image = tf.keras.applications.xception.preprocess_input(image)\n    \n    # apply VGGFACES pre-processing function when VGGFACES model is selected\n    elif MODEL_NAME == 'VGGFACES':\n        preprocessed_image = tf.stack([image[..., 0] - 91.4953,\n                      image[..., 1] - 103.8827,\n                      image[..., 2] - 131.0912],\n                     axis = -1)\n    \n    # return the image after pre-processing\n    return preprocessed_image\n\ndef decode_image(image_data):\n    image = tf.io.decode_raw(image_data,tf.uint8)\n    image = tf.cast(image, tf.float32)  # convert image to floats in [0, 1] range\n    image = tf.reshape(image,[256, 256, 3]) # explicit size needed for TPU\n    image = tf.image.resize(image, IMAGE_SIZE)\n    image = tf.reshape(image, IMAGE_SIZE_WITH_CHANNELS)\n    image = preprocess_image_as_per_base_model(image)\n    return image\n\ndef decode_test_image(image_data):\n    image = tf.io.decode_raw(image_data,tf.uint8)\n    image = tf.cast(image, tf.float32)  # convert image to floats in [0, 1] range\n    image = tf.reshape(image, [1024, 1024, 3]) \n    image = tf.image.resize(image, IMAGE_SIZE)\n    image = tf.reshape(image,IMAGE_SIZE_WITH_CHANNELS) # explicit size needed for TPU\n    image = preprocess_image_as_per_base_model(image)\n    return image\n\n# function to read labeled tfrecords\ndef read_labeled_tfrecord(example):\n    # format for reading the labeled tfrecord\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"bmi\": tf.io.FixedLenFeature([], tf.float32),  # shape [] means single element\n        \"sex\": tf.io.FixedLenFeature([], tf.int64)\n    }\n    \n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    label = tf.cast(example['bmi'], tf.float32)\n    label = tf.reshape(label, [1])\n    sex = tf.cast(example['sex'], tf.int64)\n    sex = tf.reshape(sex, [1])\n    \n    # return the 3 parsed values. It will constitute a single example\n    return image, sex, label\n\n# function to read unlabeled tfrecords\ndef read_unlabeled_tfrecord(example):\n    # format for reading the unlabeled tfrecord\n    UNLABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"bmi\": tf.io.FixedLenFeature([], tf.float32),  # shape [] means single element\n    }\n    \n    example = tf.io.parse_single_example(example, UNLABELED_TFREC_FORMAT)\n    image = decode_image(example['image'])\n    idnum = example['bmi']\n    \n    return image, idnum # returns a dataset of image(s)\n\ndef read_test_labeled_tfrecord(example):\n    LABELED_TFREC_FORMAT = {\n        \"image\": tf.io.FixedLenFeature([], tf.string), # tf.string means bytestring\n        \"bmi\": tf.io.FixedLenFeature([], tf.float32),  # shape [] means single element\n    }\n    example = tf.io.parse_single_example(example, LABELED_TFREC_FORMAT)\n    image = decode_test_image(example['image'])\n    label = tf.cast(example['bmi'], tf.float32)\n    label = tf.reshape(label, [1])\n    \n    return image, label\n\ndef load_dataset(filenames, labeled=True, ordered=False,test=False):\n    # Read from TFRecords. For optimal performance, reading from multiple files at once and\n    # disregarding data order. Order does not matter since we will be shuffling the data anyway.\n\n    ignore_order = tf.data.Options()\n    if not ordered:\n        ignore_order.experimental_deterministic = False # disable order, increase speed\n\n    dataset = tf.data.TFRecordDataset(filenames, num_parallel_reads=AUTO) # automatically interleaves reads from multiple files\n    dataset = dataset.with_options(ignore_order) # uses data as soon as it streams in, rather than in its original order\n\n    if test is True:\n        dataset = dataset.map(read_test_labeled_tfrecord,num_parallel_calls=AUTO)\n\n    else:    \n        dataset = dataset.map(read_labeled_tfrecord if labeled else read_unlabeled_tfrecord, num_parallel_calls=AUTO)\n    # returns a dataset of (image, label) pairs if labeled=True or (image, id) pairs if labeled=False\n    return dataset\n\ndef get_training_dataset(train_filenames):\n    dataset = load_dataset(train_filenames, labeled=True, ordered=False)\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_validation_dataset(valid_filenames):\n    dataset = load_dataset(valid_filenames, labeled=False, ordered=False)\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\ndef get_testing_dataset(test_filenames):\n    dataset = load_dataset(test_filenames, labeled=False, test=True)\n    dataset = dataset.shuffle(2048)\n    dataset = dataset.prefetch(AUTO) # prefetch next batch while training (autotune prefetch buffer size)\n    return dataset\n\n\n# Display the preprocessing function used in the datasets\nprint(f'Applied {MODEL_NAME} preprocessing function')\n\n# load all the datasets\ntraining_dataset = get_training_dataset(train_filenames)\nvalidation_dataset = get_validation_dataset(valid_filenames)\ntest_dataset = get_testing_dataset(test_filenames)","06487f33":"# dictonary which stores male and female int values used in TFRecords\nGENDER_DICT = {\n    'FEMALE': 1,\n    'MALE': 0\n}\n\n# get only males from the training dataset\nmale_training_dataset = training_dataset.filter(lambda image, gender, bmi: tf.math.equal(tf.squeeze(gender), GENDER_DICT['MALE']))\n\n# get only females from the training dataset\nfemale_training_dataset = training_dataset.filter(lambda image, gender, bmi: tf.math.equal(tf.squeeze(gender), GENDER_DICT['FEMALE']))\n\n# function to drop gender column from TFRecord \ndef ignore_gender(image, gender, bmi):\n    return (image, bmi)\n\n# drop gender column from the male training set\nmale_training_dataset = male_training_dataset.map(ignore_gender)\n\n# drop gender column from the female training set\nfemale_training_dataset = female_training_dataset.map(ignore_gender)\n\n# drop gender column from the complete training set\ntraining_dataset = training_dataset.map(ignore_gender)","c5608838":"# get a sample of the male training dataset\nfor male_data in male_training_dataset:\n    break\n\nmale_image, male_bmi = male_data\n\nprint(male_bmi)\nplt.imshow(male_image)","223bc25c":"# get a sample of the female training dataset\nfor female_data in female_training_dataset:\n    break\n    \nfemale_image, female_bmi = female_data\n\nprint(female_bmi)\nplt.imshow(female_image)","ae76af77":"# drop_remainder = True means dropping the last batch if size is not equal to BATCH_SIZE\n\ntraining_dataset = training_dataset.batch(BATCH_SIZE, drop_remainder=True)\nvalidation_dataset = validation_dataset.batch(BATCH_SIZE, drop_remainder=True)\ntest_dataset = test_dataset.batch(BATCH_SIZE, drop_remainder=True)\nmale_training_dataset = male_training_dataset.batch(BATCH_SIZE, drop_remainder=True)\nfemale_training_dataset = female_training_dataset.batch(BATCH_SIZE, drop_remainder=True)","64495399":"'''VGGFace models for Keras.\n\n# Notes:\n- Utility functions are modified versions of Keras functions [Keras](https:\/\/keras.io)\n\n'''\n\n\nfrom __future__ import print_function\nimport numpy as np\nfrom tensorflow.keras import backend as K\nfrom tensorflow.keras.utils import get_file\n\nV1_LABELS_PATH = 'https:\/\/github.com\/rcmalli\/keras-vggface\/releases\/download\/v2.0\/rcmalli_vggface_labels_v1.npy'\nV2_LABELS_PATH = 'https:\/\/github.com\/rcmalli\/keras-vggface\/releases\/download\/v2.0\/rcmalli_vggface_labels_v2.npy'\n\nVGG16_WEIGHTS_PATH = 'https:\/\/github.com\/rcmalli\/keras-vggface\/releases\/download\/v2.0\/rcmalli_vggface_tf_vgg16.h5'\nVGG16_WEIGHTS_PATH_NO_TOP = 'https:\/\/github.com\/rcmalli\/keras-vggface\/releases\/download\/v2.0\/rcmalli_vggface_tf_notop_vgg16.h5'\n\n\nRESNET50_WEIGHTS_PATH = 'https:\/\/github.com\/rcmalli\/keras-vggface\/releases\/download\/v2.0\/rcmalli_vggface_tf_resnet50.h5'\nRESNET50_WEIGHTS_PATH_NO_TOP = 'https:\/\/github.com\/rcmalli\/keras-vggface\/releases\/download\/v2.0\/rcmalli_vggface_tf_notop_resnet50.h5'\n\nSENET50_WEIGHTS_PATH = 'https:\/\/github.com\/rcmalli\/keras-vggface\/releases\/download\/v2.0\/rcmalli_vggface_tf_senet50.h5'\nSENET50_WEIGHTS_PATH_NO_TOP = 'https:\/\/github.com\/rcmalli\/keras-vggface\/releases\/download\/v2.0\/rcmalli_vggface_tf_notop_senet50.h5'\n\n\nVGGFACE_DIR = 'models\/vggface'\n\n\ndef preprocess_input(x, data_format=None, version=1):\n    x_temp = np.copy(x)\n    if data_format is None:\n        data_format = K.image_data_format()\n    assert data_format in {'channels_last', 'channels_first'}\n\n    if version == 1:\n        if data_format == 'channels_first':\n            x_temp = x_temp[:, ::-1, ...]\n            x_temp[:, 0, :, :] -= 93.5940\n            x_temp[:, 1, :, :] -= 104.7624\n            x_temp[:, 2, :, :] -= 129.1863\n        else:\n            x_temp = x_temp[..., ::-1]\n            x_temp[..., 0] -= 93.5940\n            x_temp[..., 1] -= 104.7624\n            x_temp[..., 2] -= 129.1863\n\n    elif version == 2:\n        if data_format == 'channels_first':\n            x_temp = x_temp[:, ::-1, ...]\n            x_temp[:, 0, :, :] -= 91.4953\n            x_temp[:, 1, :, :] -= 103.8827\n            x_temp[:, 2, :, :] -= 131.0912\n        else:\n            x_temp = x_temp[..., ::-1]\n            x_temp[..., 0] -= 91.4953\n            x_temp[..., 1] -= 103.8827\n            x_temp[..., 2] -= 131.0912\n    else:\n        raise NotImplementedError\n\n    return x_temp\n\n\ndef decode_predictions(preds, top=5):\n    LABELS = None\n    if len(preds.shape) == 2:\n        if preds.shape[1] == 2622:\n            fpath = get_file('rcmalli_vggface_labels_v1.npy',\n                             V1_LABELS_PATH,\n                             cache_subdir=VGGFACE_DIR)\n            LABELS = np.load(fpath)\n        elif preds.shape[1] == 8631:\n            fpath = get_file('rcmalli_vggface_labels_v2.npy',\n                             V2_LABELS_PATH,\n                             cache_subdir=VGGFACE_DIR)\n            LABELS = np.load(fpath)\n        else:\n            raise ValueError('`decode_predictions` expects '\n                             'a batch of predictions '\n                             '(i.e. a 2D array of shape (samples, 2622)) for V1 or '\n                             '(samples, 8631) for V2.'\n                             'Found array with shape: ' + str(preds.shape))\n    else:\n        raise ValueError('`decode_predictions` expects '\n                         'a batch of predictions '\n                         '(i.e. a 2D array of shape (samples, 2622)) for V1 or '\n                         '(samples, 8631) for V2.'\n                         'Found array with shape: ' + str(preds.shape))\n    results = []\n    for pred in preds:\n        top_indices = pred.argsort()[-top:][::-1]\n        result = [[str(LABELS[i].encode('utf8')), pred[i]] for i in top_indices]\n        result.sort(key=lambda x: x[1], reverse=True)\n        results.append(result)\n    return results\n\n'''VGGFace models for Keras.\n\n# Notes:\n- Resnet50 and VGG16  are modified architectures from Keras Application folder. [Keras](https:\/\/keras.io)\n\n- Squeeze and excitation block is taken from  [Squeeze and Excitation Networks in\n Keras](https:\/\/github.com\/titu1994\/keras-squeeze-excite-network) and modified.\n\n'''\n\n\nfrom tensorflow.keras.layers import Flatten, Dense, Input, GlobalAveragePooling2D, \\\n    GlobalMaxPooling2D, Activation, Conv2D, MaxPooling2D, BatchNormalization, \\\n    AveragePooling2D, Reshape, Permute, multiply\nfrom keras_applications.imagenet_utils import _obtain_input_shape\nfrom tensorflow.keras.utils import get_file, get_source_inputs\n# from tensorflow.keras.utils.data_utils import get_file\nfrom tensorflow.keras import backend as K\n# from keras_vggface import utils\n# from tensorflow.keras.engine.topology import get_source_inputs\nimport warnings\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras import layers\n\n\ndef VGG16(include_top=True, weights='vggface',\n          input_tensor=None, input_shape=None,\n          pooling=None,\n          classes=2622):\n    input_shape = _obtain_input_shape(input_shape,\n                                      default_size=224,\n                                      min_size=48,\n                                      data_format=K.image_data_format(),\n                                      require_flatten=include_top)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n\n    # Block 1\n    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='conv1_1')(\n        img_input)\n    x = Conv2D(64, (3, 3), activation='relu', padding='same', name='conv1_2')(x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='pool1')(x)\n\n    # Block 2\n    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='conv2_1')(\n        x)\n    x = Conv2D(128, (3, 3), activation='relu', padding='same', name='conv2_2')(\n        x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='pool2')(x)\n\n    # Block 3\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='conv3_1')(\n        x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='conv3_2')(\n        x)\n    x = Conv2D(256, (3, 3), activation='relu', padding='same', name='conv3_3')(\n        x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='pool3')(x)\n\n    # Block 4\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv4_1')(\n        x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv4_2')(\n        x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv4_3')(\n        x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='pool4')(x)\n\n    # Block 5\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv5_1')(\n        x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv5_2')(\n        x)\n    x = Conv2D(512, (3, 3), activation='relu', padding='same', name='conv5_3')(\n        x)\n    x = MaxPooling2D((2, 2), strides=(2, 2), name='pool5')(x)\n\n    if include_top:\n        # Classification block\n        x = Flatten(name='flatten')(x)\n        x = Dense(4096, name='fc6')(x)\n        x = Activation('relu', name='fc6\/relu')(x)\n        x = Dense(4096, name='fc7')(x)\n        x = Activation('relu', name='fc7\/relu')(x)\n        x = Dense(classes, name='fc8')(x)\n        x = Activation('softmax', name='fc8\/softmax')(x)\n    else:\n        if pooling == 'avg':\n            x = GlobalAveragePooling2D()(x)\n        elif pooling == 'max':\n            x = GlobalMaxPooling2D()(x)\n\n            # Ensure that the model takes into account\n            # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n        # Create model.\n    model = Model(inputs, x, name='vggface_vgg16')  # load weights\n    if weights == 'vggface':\n        if include_top:\n            weights_path = get_file('rcmalli_vggface_tf_vgg16.h5',\n                                    \n                                    VGG16_WEIGHTS_PATH,\n                                    cache_subdir=VGGFACE_DIR)\n        else:\n            weights_path = get_file('rcmalli_vggface_tf_notop_vgg16.h5',\n                                    VGG16_WEIGHTS_PATH_NO_TOP,\n                                    cache_subdir=VGGFACE_DIR)\n        model.load_weights(weights_path, by_name=True)\n        \n\n        if K.image_data_format() == 'channels_first':\n            if include_top:\n                maxpool = model.get_layer(name='pool5')\n                shape = maxpool.output_shape[1:]\n                dense = model.get_layer(name='fc6')\n                convert_dense_weights_data_format(dense, shape,\n                                                              'channels_first')\n\n            if K.backend() == 'tensorflow':\n                warnings.warn('You are using the TensorFlow backend, yet you '\n                              'are using the Theano '\n                              'image data format convention '\n                              '(`image_data_format=\"channels_first\"`). '\n                              'For best performance, set '\n                              '`image_data_format=\"channels_last\"` in '\n                              'your Keras config '\n                              'at ~\/.keras\/keras.json.')\n    return model\n\n\ndef resnet_identity_block(input_tensor, kernel_size, filters, stage, block,\n                          bias=False):\n    filters1, filters2, filters3 = filters\n    if K.image_data_format() == 'channels_last':\n        bn_axis = 3\n    else:\n        bn_axis = 1\n    conv1_reduce_name = 'conv' + str(stage) + \"_\" + str(block) + \"_1x1_reduce\"\n    conv1_increase_name = 'conv' + str(stage) + \"_\" + str(\n        block) + \"_1x1_increase\"\n    conv3_name = 'conv' + str(stage) + \"_\" + str(block) + \"_3x3\"\n\n    x = Conv2D(filters1, (1, 1), use_bias=bias, name=conv1_reduce_name)(\n        input_tensor)\n    x = BatchNormalization(axis=bn_axis, name=conv1_reduce_name + \"\/bn\")(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters2, kernel_size, use_bias=bias,\n               padding='same', name=conv3_name)(x)\n    x = BatchNormalization(axis=bn_axis, name=conv3_name + \"\/bn\")(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters3, (1, 1), use_bias=bias, name=conv1_increase_name)(x)\n    x = BatchNormalization(axis=bn_axis, name=conv1_increase_name + \"\/bn\")(x)\n\n    x = layers.add([x, input_tensor])\n    x = Activation('relu')(x)\n    return x\n\n\ndef resnet_conv_block(input_tensor, kernel_size, filters, stage, block,\n                      strides=(2, 2), bias=False):\n    filters1, filters2, filters3 = filters\n    if K.image_data_format() == 'channels_last':\n        bn_axis = 3\n    else:\n        bn_axis = 1\n    conv1_reduce_name = 'conv' + str(stage) + \"_\" + str(block) + \"_1x1_reduce\"\n    conv1_increase_name = 'conv' + str(stage) + \"_\" + str(\n        block) + \"_1x1_increase\"\n    conv1_proj_name = 'conv' + str(stage) + \"_\" + str(block) + \"_1x1_proj\"\n    conv3_name = 'conv' + str(stage) + \"_\" + str(block) + \"_3x3\"\n\n    x = Conv2D(filters1, (1, 1), strides=strides, use_bias=bias,\n               name=conv1_reduce_name)(input_tensor)\n    x = BatchNormalization(axis=bn_axis, name=conv1_reduce_name + \"\/bn\")(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters2, kernel_size, padding='same', use_bias=bias,\n               name=conv3_name)(x)\n    x = BatchNormalization(axis=bn_axis, name=conv3_name + \"\/bn\")(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters3, (1, 1), name=conv1_increase_name, use_bias=bias)(x)\n    x = BatchNormalization(axis=bn_axis, name=conv1_increase_name + \"\/bn\")(x)\n\n    shortcut = Conv2D(filters3, (1, 1), strides=strides, use_bias=bias,\n                      name=conv1_proj_name)(input_tensor)\n    shortcut = BatchNormalization(axis=bn_axis, name=conv1_proj_name + \"\/bn\")(\n        shortcut)\n\n    x = layers.add([x, shortcut])\n    x = Activation('relu')(x)\n    return x\n\n\ndef RESNET50(include_top=True, weights='vggface',\n             input_tensor=None, input_shape=None,\n             pooling=None,\n             classes=8631):\n    input_shape = _obtain_input_shape(input_shape,\n                                      default_size=224,\n                                      min_size=32,\n                                      data_format=K.image_data_format(),\n                                      require_flatten=include_top,\n                                      weights=weights)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n    if K.image_data_format() == 'channels_last':\n        bn_axis = 3\n    else:\n        bn_axis = 1\n\n    x = Conv2D(\n        64, (7, 7), use_bias=False, strides=(2, 2), padding='same',\n        name='conv1\/7x7_s2')(img_input)\n    x = BatchNormalization(axis=bn_axis, name='conv1\/7x7_s2\/bn')(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n\n    x = resnet_conv_block(x, 3, [64, 64, 256], stage=2, block=1, strides=(1, 1))\n    x = resnet_identity_block(x, 3, [64, 64, 256], stage=2, block=2)\n    x = resnet_identity_block(x, 3, [64, 64, 256], stage=2, block=3)\n\n    x = resnet_conv_block(x, 3, [128, 128, 512], stage=3, block=1)\n    x = resnet_identity_block(x, 3, [128, 128, 512], stage=3, block=2)\n    x = resnet_identity_block(x, 3, [128, 128, 512], stage=3, block=3)\n    x = resnet_identity_block(x, 3, [128, 128, 512], stage=3, block=4)\n\n    x = resnet_conv_block(x, 3, [256, 256, 1024], stage=4, block=1)\n    x = resnet_identity_block(x, 3, [256, 256, 1024], stage=4, block=2)\n    x = resnet_identity_block(x, 3, [256, 256, 1024], stage=4, block=3)\n    x = resnet_identity_block(x, 3, [256, 256, 1024], stage=4, block=4)\n    x = resnet_identity_block(x, 3, [256, 256, 1024], stage=4, block=5)\n    x = resnet_identity_block(x, 3, [256, 256, 1024], stage=4, block=6)\n\n    x = resnet_conv_block(x, 3, [512, 512, 2048], stage=5, block=1)\n    x = resnet_identity_block(x, 3, [512, 512, 2048], stage=5, block=2)\n    x = resnet_identity_block(x, 3, [512, 512, 2048], stage=5, block=3)\n\n    x = AveragePooling2D((7, 7), name='avg_pool')(x)\n\n    if include_top:\n        x = Flatten()(x)\n        x = Dense(classes, activation='softmax', name='classifier')(x)\n    else:\n        if pooling == 'avg':\n            x = GlobalAveragePooling2D()(x)\n        elif pooling == 'max':\n            x = GlobalMaxPooling2D()(x)\n\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n    # Create model.\n    model = Model(inputs, x, name='vggface_resnet50')\n\n    # load weights\n    if weights == 'vggface':\n        if include_top:\n            weights_path = get_file('rcmalli_vggface_tf_resnet50.h5',\n                                    RESNET50_WEIGHTS_PATH,\n                                    cache_subdir=VGGFACE_DIR)\n        else:\n            weights_path = get_file('rcmalli_vggface_tf_notop_resnet50.h5',\n                                    RESNET50_WEIGHTS_PATH_NO_TOP,\n                                    cache_subdir=VGGFACE_DIR)\n        model.load_weights(weights_path)\n        \n\n        if K.image_data_format() == 'channels_first' and K.backend() == 'tensorflow':\n            warnings.warn('You are using the TensorFlow backend, yet you '\n                          'are using the Theano '\n                          'image data format convention '\n                          '(`image_data_format=\"channels_first\"`). '\n                          'For best performance, set '\n                          '`image_data_format=\"channels_last\"` in '\n                          'your Keras config '\n                          'at ~\/.keras\/keras.json.')\n    elif weights is not None:\n        model.load_weights(weights)\n\n    return model\n\n\ndef senet_se_block(input_tensor, stage, block, compress_rate=16, bias=False):\n    conv1_down_name = 'conv' + str(stage) + \"_\" + str(\n        block) + \"_1x1_down\"\n    conv1_up_name = 'conv' + str(stage) + \"_\" + str(\n        block) + \"_1x1_up\"\n\n    num_channels = int(input_tensor.shape[-1])\n    bottle_neck = int(num_channels \/\/ compress_rate)\n\n    se = GlobalAveragePooling2D()(input_tensor)\n    se = Reshape((1, 1, num_channels))(se)\n    se = Conv2D(bottle_neck, (1, 1), use_bias=bias,\n                name=conv1_down_name)(se)\n    se = Activation('relu')(se)\n    se = Conv2D(num_channels, (1, 1), use_bias=bias,\n                name=conv1_up_name)(se)\n    se = Activation('sigmoid')(se)\n\n    x = input_tensor\n    x = multiply([x, se])\n    return x\n\n\ndef senet_conv_block(input_tensor, kernel_size, filters,\n                     stage, block, bias=False, strides=(2, 2)):\n    filters1, filters2, filters3 = filters\n    if K.image_data_format() == 'channels_last':\n        bn_axis = 3\n    else:\n        bn_axis = 1\n\n    bn_eps = 0.0001\n\n    conv1_reduce_name = 'conv' + str(stage) + \"_\" + str(block) + \"_1x1_reduce\"\n    conv1_increase_name = 'conv' + str(stage) + \"_\" + str(\n        block) + \"_1x1_increase\"\n    conv1_proj_name = 'conv' + str(stage) + \"_\" + str(block) + \"_1x1_proj\"\n    conv3_name = 'conv' + str(stage) + \"_\" + str(block) + \"_3x3\"\n\n    x = Conv2D(filters1, (1, 1), use_bias=bias, strides=strides,\n               name=conv1_reduce_name)(input_tensor)\n    x = BatchNormalization(axis=bn_axis, name=conv1_reduce_name + \"\/bn\",epsilon=bn_eps)(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters2, kernel_size, padding='same', use_bias=bias,\n               name=conv3_name)(x)\n    x = BatchNormalization(axis=bn_axis, name=conv3_name + \"\/bn\",epsilon=bn_eps)(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters3, (1, 1), name=conv1_increase_name, use_bias=bias)(x)\n    x = BatchNormalization(axis=bn_axis, name=conv1_increase_name + \"\/bn\" ,epsilon=bn_eps)(x)\n\n    se = senet_se_block(x, stage=stage, block=block, bias=True)\n\n    shortcut = Conv2D(filters3, (1, 1), use_bias=bias, strides=strides,\n                      name=conv1_proj_name)(input_tensor)\n    shortcut = BatchNormalization(axis=bn_axis,\n                                  name=conv1_proj_name + \"\/bn\",epsilon=bn_eps)(shortcut)\n\n    m = layers.add([se, shortcut])\n    m = Activation('relu')(m)\n    return m\n\n\ndef senet_identity_block(input_tensor, kernel_size,\n                         filters, stage, block, bias=False):\n    filters1, filters2, filters3 = filters\n    if K.image_data_format() == 'channels_last':\n        bn_axis = 3\n    else:\n        bn_axis = 1\n\n    bn_eps = 0.0001\n\n    conv1_reduce_name = 'conv' + str(stage) + \"_\" + str(block) + \"_1x1_reduce\"\n    conv1_increase_name = 'conv' + str(stage) + \"_\" + str(\n        block) + \"_1x1_increase\"\n    conv3_name = 'conv' + str(stage) + \"_\" + str(block) + \"_3x3\"\n\n    x = Conv2D(filters1, (1, 1), use_bias=bias,\n               name=conv1_reduce_name)(input_tensor)\n    x = BatchNormalization(axis=bn_axis, name=conv1_reduce_name + \"\/bn\",epsilon=bn_eps)(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters2, kernel_size, padding='same', use_bias=bias,\n               name=conv3_name)(x)\n    x = BatchNormalization(axis=bn_axis, name=conv3_name + \"\/bn\",epsilon=bn_eps)(x)\n    x = Activation('relu')(x)\n\n    x = Conv2D(filters3, (1, 1), name=conv1_increase_name, use_bias=bias)(x)\n    x = BatchNormalization(axis=bn_axis, name=conv1_increase_name + \"\/bn\",epsilon=bn_eps)(x)\n\n    se = senet_se_block(x, stage=stage, block=block, bias=True)\n\n    m = layers.add([se, input_tensor])\n    m = Activation('relu')(m)\n\n    return m\n\n\ndef SENET50(include_top=True, weights='vggface',\n            input_tensor=None, input_shape=None,\n            pooling=None,\n            classes=8631):\n    input_shape = _obtain_input_shape(input_shape,\n                                      default_size=224,\n                                      min_size=197,\n                                      data_format=K.image_data_format(),\n                                      require_flatten=include_top,\n                                      weights=weights)\n\n    if input_tensor is None:\n        img_input = Input(shape=input_shape)\n    else:\n        if not K.is_keras_tensor(input_tensor):\n            img_input = Input(tensor=input_tensor, shape=input_shape)\n        else:\n            img_input = input_tensor\n    if K.image_data_format() == 'channels_last':\n        bn_axis = 3\n    else:\n        bn_axis = 1\n\n    bn_eps = 0.0001\n\n    x = Conv2D(\n        64, (7, 7), use_bias=False, strides=(2, 2), padding='same',\n        name='conv1\/7x7_s2')(img_input)\n    x = BatchNormalization(axis=bn_axis, name='conv1\/7x7_s2\/bn',epsilon=bn_eps)(x)\n    x = Activation('relu')(x)\n    x = MaxPooling2D((3, 3), strides=(2, 2))(x)\n\n    x = senet_conv_block(x, 3, [64, 64, 256], stage=2, block=1, strides=(1, 1))\n    x = senet_identity_block(x, 3, [64, 64, 256], stage=2, block=2)\n    x = senet_identity_block(x, 3, [64, 64, 256], stage=2, block=3)\n\n    x = senet_conv_block(x, 3, [128, 128, 512], stage=3, block=1)\n    x = senet_identity_block(x, 3, [128, 128, 512], stage=3, block=2)\n    x = senet_identity_block(x, 3, [128, 128, 512], stage=3, block=3)\n    x = senet_identity_block(x, 3, [128, 128, 512], stage=3, block=4)\n\n    x = senet_conv_block(x, 3, [256, 256, 1024], stage=4, block=1)\n    x = senet_identity_block(x, 3, [256, 256, 1024], stage=4, block=2)\n    x = senet_identity_block(x, 3, [256, 256, 1024], stage=4, block=3)\n    x = senet_identity_block(x, 3, [256, 256, 1024], stage=4, block=4)\n    x = senet_identity_block(x, 3, [256, 256, 1024], stage=4, block=5)\n    x = senet_identity_block(x, 3, [256, 256, 1024], stage=4, block=6)\n\n    x = senet_conv_block(x, 3, [512, 512, 2048], stage=5, block=1)\n    x = senet_identity_block(x, 3, [512, 512, 2048], stage=5, block=2)\n    x = senet_identity_block(x, 3, [512, 512, 2048], stage=5, block=3)\n\n    x = AveragePooling2D((7, 7), name='avg_pool')(x)\n\n    if include_top:\n        x = Flatten()(x)\n        x = Dense(classes, activation='softmax', name='classifier')(x)\n    else:\n        if pooling == 'avg':\n            x = GlobalAveragePooling2D()(x)\n        elif pooling == 'max':\n            x = GlobalMaxPooling2D()(x)\n\n    # Ensure that the model takes into account\n    # any potential predecessors of `input_tensor`.\n    if input_tensor is not None:\n        inputs = get_source_inputs(input_tensor)\n    else:\n        inputs = img_input\n    # Create model.\n    model = Model(inputs, x, name='vggface_senet50')\n\n    # load weights\n    if weights == 'vggface':\n        if include_top:\n            weights_path = get_file('rcmalli_vggface_tf_senet50.h5',\n                                    SENET50_WEIGHTS_PATH,\n                                    cache_subdir=VGGFACE_DIR)\n        else:\n            weights_path = get_file('rcmalli_vggface_tf_notop_senet50.h5',\n                                    SENET50_WEIGHTS_PATH_NO_TOP,\n                                    cache_subdir=VGGFACE_DIR)\n        model.load_weights(weights_path)\n\n\n        if K.image_data_format() == 'channels_first' and K.backend() == 'tensorflow':\n            warnings.warn('You are using the TensorFlow backend, yet you '\n                          'are using the Theano '\n                          'image data format convention '\n                          '(`image_data_format=\"channels_first\"`). '\n                          'For best performance, set '\n                          '`image_data_format=\"channels_last\"` in '\n                          'your Keras config '\n                          'at ~\/.keras\/keras.json.')\n    elif weights is not None:\n        model.load_weights(weights)\n\n    return model\n\n'''VGGFace models for Keras.\n\n# Reference:\n- [Deep Face Recognition](http:\/\/www.robots.ox.ac.uk\/~vgg\/publications\/2015\/Parkhi15\/parkhi15.pdf)\n- [VGGFace2: A dataset for recognising faces across pose and age](http:\/\/www.robots.ox.ac.uk\/~vgg\/data\/vgg_face2\/vggface2.pdf)\n\n'''\n\n\ndef VGGFace(include_top=True, model='vgg16', weights='vggface',\n            input_tensor=None, input_shape=None,\n            pooling=None,\n            classes=None):\n    \"\"\"Instantiates the VGGFace architectures.\n    Optionally loads weights pre-trained\n    on VGGFace datasets. Note that when using TensorFlow,\n    for best performance you should set\n    `image_data_format=\"channels_last\"` in your Keras config\n    at ~\/.keras\/keras.json.\n    The model and the weights are compatible with both\n    TensorFlow and Theano. The data format\n    convention used by the model is the one\n    specified in your Keras config file.\n    # Arguments\n        include_top: whether to include the 3 fully-connected\n            layers at the top of the network.\n        weights: one of `None` (random initialization)\n            or \"vggface\" (pre-training on VGGFACE datasets).\n        input_tensor: optional Keras tensor (i.e. output of `layers.Input()`)\n            to use as image input for the model.\n        model: selects the one of the available architectures \n            vgg16, resnet50 or senet50 default is vgg16.\n        input_shape: optional shape tuple, only to be specified\n            if `include_top` is False (otherwise the input shape\n            has to be `(224, 224, 3)` (with `channels_last` data format)\n            or `(3, 224, 244)` (with `channels_first` data format).\n            It should have exactly 3 inputs channels,\n            and width and height should be no smaller than 48.\n            E.g. `(200, 200, 3)` would be one valid value.\n        pooling: Optional pooling mode for feature extraction\n            when `include_top` is `False`.\n            - `None` means that the output of the model will be\n                the 4D tensor output of the\n                last convolutional layer.\n            - `avg` means that global average pooling\n                will be applied to the output of the\n                last convolutional layer, and thus\n                the output of the model will be a 2D tensor.\n            - `max` means that global max pooling will\n                be applied.\n        classes: optional number of classes to classify images\n            into, only to be specified if `include_top` is True, and\n            if no `weights` argument is specified.\n    # Returns\n        A Keras model instance.\n    # Raises\n        ValueError: in case of invalid argument for `weights`,\n            or invalid input shape.\n    \"\"\"\n\n    if weights not in {'vggface', None}:\n        raise ValueError('The `weights` argument should be either '\n                         '`None` (random initialization) or `vggface`'\n                         '(pre-training on VGGFace Datasets).')\n\n    if model == 'vgg16':\n\n        if classes is None:\n            classes = 2622\n\n        if weights == 'vggface' and include_top and classes != 2622:\n            raise ValueError(\n                'If using `weights` as vggface original with `include_top`'\n                ' as true, `classes` should be 2622')\n\n        return VGG16(include_top=include_top, input_tensor=input_tensor,\n                     input_shape=input_shape, pooling=pooling,\n                     weights=weights,\n                     classes=classes)\n\n\n    if model == 'resnet50':\n\n        if classes is None:\n            classes = 8631\n\n        if weights == 'vggface' and include_top and classes != 8631:\n            raise ValueError(\n                'If using `weights` as vggface original with `include_top`'\n                ' as true, `classes` should be 8631')\n\n        return RESNET50(include_top=include_top, input_tensor=input_tensor,\n                        input_shape=input_shape, pooling=pooling,\n                        weights=weights,\n                        classes=classes)\n\n    if model == 'senet50':\n\n        if classes is None:\n            classes = 8631\n\n        if weights == 'vggface' and include_top and classes != 8631:\n            raise ValueError(\n                'If using `weights` as vggface original with `include_top`'\n                ' as true, `classes` should be 8631')\n\n        return SENET50(include_top=include_top, input_tensor=input_tensor,\n                        input_shape=input_shape, pooling=pooling,\n                        weights=weights,\n                        classes=classes)","9f316ea8":"def vggfaces_transfer_learning_model(base_model_trainable = True):\n    pretrained_model = VGGFace(model='resnet50', include_top=False, input_shape=(*IMAGE_SIZE, 3), pooling='avg')\n    \n    if base_model_trainable:\n        \n        for layer in pretrained_model.layers:\n            layer.trainable = True \n\n            if isinstance(layer,tf.keras.layers.BatchNormalization):\n                layer.trainable = True        \n                \n    model = tf.keras.Sequential([\n        pretrained_model,\n        tf.keras.layers.Dense(1024,activation='relu',kernel_initializer=tf.keras.initializers.GlorotUniform()),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(512, activation=tfa.activations.gelu,kernel_initializer= tf.keras.initializers.GlorotUniform()),\n        tf.keras.layers.Dense(64, activation='relu',kernel_initializer = tf.keras.initializers.GlorotUniform()),\n        tf.keras.layers.Dense(1,activation=tfa.activations.gelu,kernel_initializer = tf.keras.initializers.GlorotUniform())\n    ])\n    \n    optimizers = [\n        tf.keras.optimizers.Adam(learning_rate=1e-8),\n        tf.keras.optimizers.Adam(learning_rate=1e-7),\n        tf.keras.optimizers.Adam(learning_rate=1e-6),\n        tf.keras.optimizers.Adam(learning_rate=1e-5),\n    ]\n    \n    optimizers_and_layers = [\n        (optimizers[0],model.layers[-35:-25]),\n         (optimizers[1], model.layers[-25:-15]),\n         (optimizers[2], model.layers[-15:-5]),\n         (optimizers[3], model.layers[-5:]),\n    ]\n\n    opt = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\n    \n    model.compile(\n    optimizer=opt,\n    loss = 'mean_squared_error',\n    metrics=['mean_absolute_error']\n    )\n    \n    return model","c4d2c370":"# function creates model with vgg19 as the base model and adds the fully connected layers \ndef vgg19_transfer_learning_model(base_model_trainable = True):\n    # do not include top of the base model\n    pretrained_model = tf.keras.applications.VGG19(weights='imagenet', include_top=False,  input_shape=(256,256,3), pooling='avg')\n    \n    # unfreeze the entire model if base_model_trainable is True\n    if base_model_trainable:\n        \n        # make all layers trainable\n        for layer in pretrained_model.layers[-10:]:\n            layer.trainable = True\n\n            # Unfreeze the Batch Normalization Layers as well\n            if isinstance(layer,tf.keras.layers.BatchNormalization):\n                layer.trainable = True\n                \n    # Fully Connected Layers of the model\n    model = tf.keras.Sequential([\n        pretrained_model,\n        tf.keras.layers.Dense(1024,activation='relu',kernel_initializer=tf.keras.initializers.GlorotUniform()),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(512, activation=tfa.activations.gelu,kernel_initializer= tf.keras.initializers.GlorotUniform()),\n        tf.keras.layers.Dense(64, activation='relu',kernel_initializer = tf.keras.initializers.GlorotUniform()),\n        tf.keras.layers.Dense(1,activation=tfa.activations.gelu,kernel_initializer = tf.keras.initializers.GlorotUniform())\n    ])\n    \n    # Used mulitple Adam Optimizers for training the fully connected layers and pre-trained model layers\n    optimizers = [\n        tf.keras.optimizers.Adam(learning_rate=1e-8),\n        tf.keras.optimizers.Adam(learning_rate= 1e-6),\n    ]\n    \n    # specifying the optimizers and layers in which it will be operated\n    optimizers_and_layers = [\n        (optimizers[1], model.layers[-10:-5]),\n        (optimizers[0], model.layers[-5:]),\n    ]\n    \n    # Using Multi Optimizer from Tensorflow Addons\n    opt = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\n    \n    # Compiling the model\n    model.compile(\n        optimizer=opt,\n        loss = 'mean_squared_error',\n        metrics=['mean_absolute_error']\n    )\n    \n    return model","ceab047d":"# function creates model with inception as the base model and adds the fully connected layers \ndef inception_v3_transfer_learning_model(base_model_trainable = True):\n    # do not include top of the base model\n    pretrained_model = tf.keras.applications.InceptionV3(weights='imagenet', include_top=False,input_shape=(256,256,3), pooling='avg')\n    \n    # unfreeze the entire model if base_model_trainable is True\n    if base_model_trainable:\n        \n        # make all layers trainable\n        for layer in pretrained_model.layers:\n            layer.trainable = True \n\n            # Unfreeze the Batch Normalization Layers as well\n            if isinstance(layer,tf.keras.layers.BatchNormalization):\n                layer.trainable = True        \n                \n    # Fully Connected Layers of the model\n    model = tf.keras.Sequential([\n        pretrained_model,\n        tf.keras.layers.Dense(1024,activation='relu',kernel_initializer=tf.keras.initializers.GlorotUniform()),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(512, activation=tfa.activations.gelu,kernel_initializer= tf.keras.initializers.GlorotUniform()),\n        tf.keras.layers.Dense(64, activation='relu',kernel_initializer = tf.keras.initializers.GlorotUniform()),\n        tf.keras.layers.Dense(1,activation=tfa.activations.gelu,kernel_initializer = tf.keras.initializers.GlorotUniform())\n    ])\n    \n    # Used mulitple Adam Optimizers for training the fully connected layers and pre-trained model layers\n    optimizers = [\n        tf.keras.optimizers.Adam(learning_rate=1e-8),\n        tf.keras.optimizers.Adam(learning_rate=1e-7),\n        tf.keras.optimizers.Adam(learning_rate=1e-6),\n        tf.keras.optimizers.Adam(learning_rate=1e-5),\n    ]\n    \n    # specifying the optimizers and layers in which it will be operated\n    optimizers_and_layers = [\n        (optimizers[0], model.layers[-35:-25]),\n        (optimizers[1], model.layers[-25:-15]),\n        (optimizers[2], model.layers[-15:-5]),\n        (optimizers[3], model.layers[-5:]),\n    ]\n\n    # Using Multi Optimizer from Tensorflow Addons\n    opt = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\n    \n    # Compiling the model\n    model.compile(\n    optimizer=opt,\n    loss = 'mean_squared_error',\n    metrics=['mean_absolute_error']\n    )\n    \n    return model","0a447752":"# function creates model with xception as the base model and adds the fully connected layers \ndef xception_transfer_learning_model(base_model_trainable = True):\n    # do not include top of the base model\n    pretrained_model = tf.keras.applications.Xception(weights='imagenet', include_top=False,input_shape=(256,256,3), pooling='avg')\n    \n    # unfreeze the entire model if base_model_trainable is True\n    if base_model_trainable:\n        \n        # make all layers trainable\n        for layer in pretrained_model.layers:\n            layer.trainable = True \n\n            # Unfreeze the Batch Normalization Layers as well\n            if isinstance(layer,tf.keras.layers.BatchNormalization):\n                layer.trainable = True        \n                \n    # Fully Connected Layers of the model\n    model = tf.keras.Sequential([\n        pretrained_model,\n        tf.keras.layers.Dense(1024,activation='relu',kernel_initializer=tf.keras.initializers.GlorotUniform()),\n        tf.keras.layers.Dropout(0.5),\n        tf.keras.layers.Dense(512, activation=tfa.activations.gelu,kernel_initializer= tf.keras.initializers.GlorotUniform()),\n        tf.keras.layers.Dense(64, activation='relu',kernel_initializer = tf.keras.initializers.GlorotUniform()),\n        tf.keras.layers.Dense(1,activation=tfa.activations.gelu,kernel_initializer = tf.keras.initializers.GlorotUniform())\n    ])\n    \n    # Used mulitple Adam Optimizers for training the fully connected layers and pre-trained model layers\n    optimizers = [\n        tf.keras.optimizers.Adam(learning_rate=1e-8),\n        tf.keras.optimizers.Adam(learning_rate=1e-7),\n        tf.keras.optimizers.Adam(learning_rate=1e-6),\n        tf.keras.optimizers.Adam(learning_rate=1e-5),\n    ]\n    \n    # specifying the optimizers and layers in which it will be operated\n    optimizers_and_layers = [\n        (optimizers[0], model.layers[-35:-25]),\n        (optimizers[1], model.layers[-25:-15]),\n        (optimizers[2], model.layers[-15:-5]),\n        (optimizers[3], model.layers[-5:]),\n    ]\n\n    # Using Multi Optimizer from Tensorflow Addons\n    opt = tfa.optimizers.MultiOptimizer(optimizers_and_layers)\n    \n    # Compiling the model\n    model.compile(\n    optimizer=opt,\n    loss = 'mean_squared_error',\n    metrics=['mean_absolute_error']\n    )\n    \n    return model","e59d5ede":"# function to load the main model which will be used for training\ndef mainModel(base_model_trainable = True):\n    # use the appropriate base model as per the value of MODEL_NAME\n\n    if MODEL_NAME == 'INCEPTION_V3':\n        model = inception_v3_transfer_learning_model(base_model_trainable)\n        \n    elif MODEL_NAME == 'VGG19':\n        model = vgg19_transfer_learning_model(base_model_trainable)\n    \n    elif MODEL_NAME == 'XCEPTION':\n        model = xception_transfer_learning_model(base_model_trainable)\n        \n    elif MODEL_NAME == 'VGGFACES':\n        model = vggfaces_transfer_learning_model(base_model_trainable)\n        \n    return model","56c695a3":"# set the number of epochs\nEPOCHS = 5\n\n# make the pre-trained models trainable\nbase_model_trainable = True\n\n# load the model inside the TPU scope\nwith strategy.scope():    \n    final_model = mainModel(base_model_trainable)\n\n# callback to save the model after every epoch\nclass MyCustomCallback(tf.keras.callbacks.Callback):\n    def on_epoch_end(self, epoch, logs=None):\n        self.model.save_weights(f\"model_{epoch + 1}.h5\")\n        \ncallbacks = [\n    tf.keras.callbacks.EarlyStopping(patience=10),\n    MyCustomCallback()\n]\n\n# Fitting the model\nhistorical = final_model.fit(training_dataset, \n                       validation_data=test_dataset,\n                       epochs=EPOCHS,\n                       callbacks=callbacks,\n                       use_multiprocessing=True, \n                       workers=-1)","1a95f5f5":"historical.history","27b08121":"plt.title('Mean Squared Loss')\nplt.plot(historical.history['loss'], label='train')\nplt.plot(historical.history['val_loss'], label='val')\nplt.legend()\nplt.show()","99908123":"plt.title('Mean Absolute Error')\nplt.plot(historical.history['mean_absolute_error'], label='train')\nplt.plot(historical.history['val_mean_absolute_error'], label='val')\nplt.legend()\nplt.show()","ba30c154":"print(final_model.evaluate(training_dataset))\nprint(final_model.evaluate(test_dataset))","c7cb876a":"max_epochs = len(historical.history['val_mean_absolute_error'])\n\nfor epoch in range(1, 1 + max_epochs):\n    \n    final_model.load_weights(f\".\/model_{str(epoch)}.h5\")\n    print(f'##################### Training {epoch} #####################')\n    print(final_model.evaluate(training_dataset))\n\n    print(f'##################### Male Training {epoch} #####################')\n    print(final_model.evaluate(male_training_dataset))\n    \n    print(f'##################### Female Training {epoch} #####################')\n    print(final_model.evaluate(female_training_dataset))\n    \n    print(f'##################### Testing {epoch} #####################')\n    print(final_model.evaluate(test_dataset))","59149fce":"model_json = final_model.to_json()\n\nwith open(\"model.json\", \"w\") as json_file:\n    json.dump(model_json, json_file)\n    \nfinal_model.save_weights(\"weights.h5\")","ba42ed72":"## Create batches for the datasets ","1db845b8":"## Model training on TPU","f88e0728":"## Getting filenames for the datasets","2991cdb5":"## Evaluating the Model","11325b34":"## Filter training dataset by males and females ","c54f7a42":"## Get Google Cloud Storage Path","484aaf83":"## VggFaces Network Model Complete Architecture","86fe12c4":"## Vgg19 Network model","01928b33":"## Save the model config in json file and the model weights ","bffa1de5":"## Xception Network model","b05fcc90":"## VggFaces Network model","0bcc2107":"## Plotting Loss and Error","17488c3d":"## Kaggle Dataset is necessary for TPU","4cf819f7":"## Inception-v3 Network model","bec9501d":"## Code for TPU Detection","e18682bd":"## Import necessary Packages","08afd5fc":"## Specifying paths for images and csv file","b6e86fdc":"## Visualize the female training dataset","1b0b9c42":"## Visualize the male training dataset","49958e90":"## Download external packages","72fc8057":"## Evaluate the model trained after every epoch","dd24bf26":"## Generate train,val,test data from TFRecord Files"}}