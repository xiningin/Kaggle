{"cell_type":{"93685ca6":"code","b567b083":"code","68f1ae69":"code","b70bffa6":"code","504313fe":"code","c4e920c3":"code","21d1439f":"code","7892928b":"code","d4213396":"code","d6d431de":"code","e8e53a48":"code","917c3caa":"code","7934aa79":"code","1fcfba2a":"code","57ad1d88":"code","9a7954c7":"code","cc914657":"code","1a81e0b1":"code","9c955841":"code","b09ed245":"code","c032afee":"code","2b57cdbe":"code","48c1389c":"code","0bd93d5b":"code","8b45de69":"code","30e7c585":"code","2d6187e8":"code","ea9fd942":"code","6dab1b18":"code","6ac19ee7":"code","f6707044":"code","26481bdc":"code","6c5c202f":"code","0610797f":"code","ecedff00":"code","b5a5fa2c":"code","12b0a263":"code","6d44cb6a":"code","9dfa7fb4":"code","1a2046bc":"code","dc51534e":"code","977ce5ce":"code","b0dc39aa":"code","d50e07d3":"code","c6ae1ecd":"code","e4d67e41":"code","d83a2969":"code","ebf345c2":"code","7ecdad97":"markdown","f226654b":"markdown","261f9719":"markdown","4a740712":"markdown","578f81a5":"markdown","baae88f4":"markdown","765e29b6":"markdown","749428e5":"markdown","3308d37f":"markdown","9bf8f5e6":"markdown","7701929c":"markdown","e83103d1":"markdown","aeda2134":"markdown","add51bb8":"markdown","4a534d66":"markdown","e9d5c593":"markdown","77fb5535":"markdown","e734d2c5":"markdown","66445fbb":"markdown","6a00c1ca":"markdown"},"source":{"93685ca6":"import numpy as np\nimport pandas as pd\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","b567b083":"data = pd.read_pickle('..\/input\/d\/zekaili\/bank-transactions\/dataset.pkl')\ndata.head(5)","68f1ae69":"# create a data frame to store all the features and labels.\n# later we will put all the generated features into this data frame.\nfeatures = pd.DataFrame()\nfeatures['Label'] = data.apply(lambda x: x.iloc[5][1], axis = 1)\nfeatures['LoanAmount'] = data.apply(lambda x: x.iloc[3][1], axis = 1)","b70bffa6":"# extract the transaction amounts and put them into a data frame\npd_transactions = data.apply(lambda x: x.iloc[2][1].reshape(-1), axis = 1)\npd_transactions = pd.DataFrame(pd_transactions.tolist()).T\n\n# extract the transaction dates and put them into a data frame\npd_dates_trans = data.apply(lambda x: x.iloc[1][1], axis = 1)\npd_dates_trans = pd.DataFrame(pd_dates_trans.tolist()).T\npd_dates_trans = pd_dates_trans.apply(pd.to_datetime)\n\n# extract the loan date and put them into a data frame\npd_date_loan = data.apply(lambda x: x.iloc[4][1], axis = 1)\npd_date_loan = pd.DataFrame(pd_date_loan.tolist()).T\npd_date_loan = pd_date_loan.apply(pd.to_datetime)","504313fe":"# Please note here that in pd_transactions and pd_dates_trans\n# the data shape is (max number of transaction, number of entries)\n# so each column represents one sample\/entry. if the number of transactions of an entry is less than the max, \n# the empty parts is nan\n# the pd_dates_trans has the same structure with dates\nprint('Transactions DF shape: ' + str(pd_transactions.shape))\npd_transactions.head()","c4e920c3":"def addFeatures_1(features, data):\n    pd_transactions = data.copy()\n    features = features.copy()\n    features['min'] = pd_transactions.min().values # min \n    features['max'] = pd_transactions.max().values # max\n    features['max\/loan'] = features['max'] \/ features.LoanAmount # max \/ loan\n    features['min\/loan'] = features['min'] \/ features.LoanAmount # min \/ loan\n\n    features['mean'] = pd_transactions.mean().values # mean\n    features['mean_positive'] =  pd_transactions[pd_transactions>0].mean().values # mean of positive trans\n    features['mean_negative'] =  pd_transactions[pd_transactions<0].mean().values # mean of negative trans\n\n    features['median'] =  pd_transactions.median().values # median\n    features['quantiles_0.25'] = pd_transactions.quantile(0.25).values # quantile 0.25\n    features['quantiles_0.75'] = pd_transactions.quantile(0.75).values # quantile 0.75\n    features['median_positive'] =  pd_transactions[pd_transactions>0].median().values # median of positive\n    features['median_negative'] =  pd_transactions[pd_transactions<0].median().values # median of negative\n\n    features['sum_abs'] = pd_transactions.abs().sum().values # absolute sum\n    features['sum_abs\/loan'] = features['sum_abs'] \/ features.LoanAmount\n\n    features['sum'] = pd_transactions.sum().values # sum\n    features['sum_positive'] = pd_transactions[pd_transactions>0].sum().values # sum of positive\n    features['sum_negative'] = pd_transactions[pd_transactions<0].sum().values # sum of negative\n    features['sum\/loan'] = features['sum'] \/ features.LoanAmount # sum \/ loan\n    features['sum\/sum_positive'] = features['sum'] \/ features.sum_positive # sum \/ sum of positive\n    features['sum\/sum_negative'] = features['sum'] \/ features.sum_negative # sum \/ sum of negative\n    features['sum_positive\/loan'] = features.sum_positive \/ features.LoanAmount # sum of positive \/ loan\n    features['sum_negative\/loan'] = features.sum_negative \/ features.LoanAmount # sum of negative \/ loan\n\n    features['nb_trans'] = pd_transactions.count().values # number of trans \n    features['nb_trans_pos'] = pd_transactions[pd_transactions>0].count().values # number of positive trans \n    features['nb_trans_neg'] = pd_transactions[pd_transactions<0].count().values # number of negative trans \n\n    features['nb_trans_0_50'] = pd_transactions[(pd_transactions>0) & (pd_transactions<50)].count().values # number of trans > 10\n    features['nb_trans_50_100'] = pd_transactions[(pd_transactions>50) & (pd_transactions<100)].count().values # number of trans > 20\n    features['nb_trans_100_200'] = pd_transactions[(pd_transactions>100) & (pd_transactions<200)].count().values # number of trans > 50\n    features['nb_trans_200'] = pd_transactions[pd_transactions>200].count().values # number of trans > 100\n\n    features['nb_trans_0_-50'] = pd_transactions[(pd_transactions<0) & (pd_transactions>-50)].count().values # number of trans < -10\n    features['nb_trans_-50_-100'] = pd_transactions[(pd_transactions<-50) & (pd_transactions>-100)].count().values # number of trans < -20\n    features['nb_trans_-100_-200'] = pd_transactions[(pd_transactions<-100) & (pd_transactions>-200)].count().values # number of trans < -50\n    features['nb_trans_-200'] = pd_transactions[pd_transactions<-200].count().values # number of trans < -100\n\n    # mfv = the most frequent value. If more than one (same frequency), get average\n    features['mfv'] = pd_transactions.mode().mean(axis = 0).values # the most frequent trans\n    # features['mfv_more_0'] = pd_transactions[pd_transactions>0].mode().mean(axis = 0).values # the most frequent positive trans \n    # features['mfv_more_0'] = pd_transactions[pd_transactions<0].mode().mean(axis = 0).values # the most frequent negative trans \n    features['mfv_0_50'] = pd_transactions[(pd_transactions>0) & (pd_transactions<50)].mode().mean(axis = 0).values # the most frequent trans that is > 10\n    features['mfv_50_100'] = pd_transactions[(pd_transactions>50) & (pd_transactions<100)].mode().mean(axis = 0).values # the most frequent trans that is > 20\n    features['mfv_100_200'] = pd_transactions[(pd_transactions>100) & (pd_transactions<200)].mode().mean(axis = 0).values # the most frequent trans that is > 50\n    features['mfv_200'] = pd_transactions[pd_transactions>200].mode().mean(axis = 0).values # the most frequent trans that is > 100\n\n    features['mfv_0_-50'] = pd_transactions[(pd_transactions>-50) & (pd_transactions<0)].mode().mean(axis = 0).values # the most frequent trans that is < -10\n    features['mfv_-50_-100'] = pd_transactions[(pd_transactions>-100) & (pd_transactions<-50)].mode().mean(axis = 0).values\n    features['mfv_-100_-200'] = pd_transactions[(pd_transactions>-200) & (pd_transactions<-100)].mode().mean(axis = 0).values\n    features['mfv_-200'] = pd_transactions[pd_transactions<-200].mode().mean(axis = 0).values\n\n    features['mfv_100_200\/loan'] = features['mfv_100_200'] \/ features.LoanAmount\n    features['mfv_200\/loan'] = features['mfv_200'] \/ features.LoanAmount\n    features['mfv_-100_-200\/loan'] = features['mfv_-100_-200'] \/ features.LoanAmount\n    features['mfv_-200\/loan'] = features['mfv_-200'] \/ features.LoanAmount\n    return features","21d1439f":"# get the number of cores for multiprocessing\nimport subprocess\nfrom ast import literal_eval\ndef run(command):\n    process = subprocess.Popen(command, shell=True, stdout=subprocess.PIPE)\n    out, err = process.communicate()\n    print(out.decode('utf-8').strip())\nprint('# CPU')\nrun('cat \/proc\/cpuinfo | egrep -m 1 \"^model name\"')\nrun('cat \/proc\/cpuinfo | egrep -m 1 \"^cpu MHz\"')\nrun('cat \/proc\/cpuinfo | egrep -m 1 \"^cpu cores\"')","7892928b":"# Here we utilize multiprocessing the speed up the feauture engineering\nfrom multiprocessing import  Pool\nn_cores = 2\nfeature_split = np.array_split(features, n_cores)\ndata_split = np.array_split(pd_transactions, n_cores, axis = 1)\npool = Pool(n_cores)\nfeatures = pd.concat(pool.starmap(addFeatures_1, zip(feature_split, data_split)))\npool.close()\npool.join()","d4213396":"print('current feature set shape: ' + str(features.shape))\nfeatures.head(2)","d6d431de":"# check if there is any nan or infinite values:\nfeatures.columns[features.isna().any()]","e8e53a48":"features.replace([np.inf, -np.inf], np.nan, inplace=True)\nfeatures = features.fillna(0)\nfeatures = features.astype(np.float32)","917c3caa":"# Since we have too many features to visulize the correlation, I made a function to only show corr > threshold.\ndef getCorrGreaterThan(features, threshold = 0.9):\n    corr_features = features.corr()\n    corr_greaterThan9 = {}\n    for column in corr_features.columns:\n        corr_row_indices = np.where(corr_features[column].to_numpy()>threshold)[0].tolist()\n        corr_row_names = corr_features.columns[corr_row_indices].tolist()\n        corr_row_names.remove(column)\n\n        if len(corr_row_names) > 0:\n            corr_name_and_value = []\n            for i in range(len(corr_row_names)):\n                corr_value = corr_features.loc[corr_row_names[i], column]\n                corr_name_and_value.append([corr_row_names[i], corr_value])\n            corr_greaterThan9[column] = corr_name_and_value\n    return corr_greaterThan9","7934aa79":"# show the features with high correlation\ncorr_greaterThan9 = getCorrGreaterThan(features)\ncorr_greaterThan9","1fcfba2a":"dropList_1 = ['min\/loan', 'max\/loan', 'sum_abs', 'sum_abs\/loan', 'sum\/loan','nb_trans_neg', 'nb_trans_0_-50']\nfeatures_dropped_1 = features.drop(dropList_1, axis = 1)","57ad1d88":"# double check if there is any high correlation left.\ncorr_greaterThan9 = getCorrGreaterThan(features_dropped_1)\ncorr_greaterThan9","9a7954c7":"from sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn import svm\nfrom catboost import CatBoostClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.ensemble import RandomForestClassifier, VotingClassifier\nfrom sklearn.model_selection import cross_val_score, StratifiedKFold\nfrom sklearn import preprocessing\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn import metrics\n\ndef train_models(x_train, y_train, n_splits = 5): \n    model_list=[\n                RandomForestClassifier(random_state=42),\n                XGBClassifier(random_state=42, verbosity = 0, use_label_encoder=False), \n                LGBMClassifier(random_state=42, is_unbalance=True), \n                CatBoostClassifier(random_state=42,verbose=0),\n                AdaBoostClassifier(random_state=42),\n                LogisticRegression(random_state=42, max_iter = 2000, solver='liblinear'),\n                svm.SVC(random_state=42, probability = True),\n                KNeighborsClassifier()\n               ]\n    model_names=['Random Forest', 'XG Boost', 'Light GBM', 'CatBoost','AdaBoost', 'Logistic Regression','SVM', 'KNN']\n\n    # set the columns, based on the number of split\n    column_names = ['Model','Mean']\n    for i in range(n_splits):\n        column_names.append('CV'+ str(i+1))\n    scores_df = pd.DataFrame(columns=column_names)\n\n    for i in range(len(model_list)):\n            print(model_names[i])\n            kf = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state = 1)\n            model = model_list[i]\n            scores = cross_val_score(model, x_train, y_train,scoring='roc_auc', cv=kf, n_jobs = -1)            \n            scores_df.loc[i]=[model_names[i]]+ [scores.mean()] + scores.tolist()\n    return scores_df","cc914657":"def process_and_train(features):\n    x_all = features.drop(['Label'], axis = 1)\n    scale = StandardScaler()\n    x_all[x_all.columns] = scale.fit_transform(x_all[x_all.columns])\n    y_all = features.Label\n    y_all = y_all.astype('float32')\n    scores_df = train_models(x_all, y_all, n_splits = 5)\n    return scores_df","1a81e0b1":"# To save some time, I have logged the result. Uncomment if needed.\n# scores_df = process_and_train(features_dropped_1)\n# print('mean score: ' + str(scores_df.Mean.mean()))\n# scores_df","9c955841":"def addFeatures_2(features, pd_transactions, pd_dates_trans, pd_date_loan):\n    # The date difference between the last transaction and the loan\n    features['date_diff_last_loan'] = (pd_date_loan.iloc[0] - pd_dates_trans.ffill().iloc[-1]).dt.days.values\n    # The date difference between the first transaction and the loan\n    features['date_diff_first_loan'] = (pd_date_loan.iloc[0] - pd_dates_trans.iloc[0]).dt.days.values\n    # The date difference between the first transaction and the last transaction\n    features['date_diff_first_last'] = (pd_dates_trans.iloc[0] - pd_dates_trans.ffill().iloc[-1]).dt.days.values\n    # The date difference between the first maximum transaction and the loan\n    features['date_diff_firstMax_loan'] = (pd_date_loan.iloc[0] - np.diag(pd_dates_trans.loc[pd_transactions.idxmax()])).dt.days.values\n    # The date difference between the first minimum transaction and the loan\n    features['date_diff_firstMin_loan'] = (pd_date_loan.iloc[0] - np.diag(pd_dates_trans.loc[pd_transactions.idxmin()])).dt.days.values\n    \n    pd_transactions_reverse = pd_transactions.iloc[::-1].reset_index(drop = True)\n    pd_dates_trans_reverse = pd_dates_trans.iloc[::-1].reset_index(drop = True)\n    # The date difference between the last maximum transaction and the loan\n    features['date_diff_lastMax_loan'] = (pd_date_loan.iloc[0] - np.diag(pd_dates_trans_reverse.loc[pd_transactions_reverse.idxmax()])).dt.days.values\n    # The date difference between the last minimum transaction and the loan\n    features['date_diff_lastMin_loan'] = (pd_date_loan.iloc[0] - np.diag(pd_dates_trans_reverse.loc[pd_transactions_reverse.idxmin()])).dt.days.values\n    \n    # The loan day of week\n    features['date_loanDayOfWeek'] = pd_date_loan.iloc[0].dt.dayofweek.values\n    # The month of the loan date\n    features['date_loanMonth'] = pd_date_loan.iloc[0].dt.month.values\n    # The day of the loan day\n    features['date_loanDay'] = pd_date_loan.iloc[0].dt.day.values\n    # The year of the loan day\n    features['date_loanYear'] = pd_date_loan.iloc[0].dt.year.values\n    \n    # The transaction frequency: number of transactions \/ first - last transaction date difference\n    features['transFreq_all'] = features.nb_trans \/ features.date_diff_first_last\n    # The positive transaction frequency: number of positive transactions \/ first - last transaction date difference\n    features['transFreq_pos'] = features.nb_trans_pos \/ features.date_diff_first_last\n    # The negative transaction frequency: number of negative transactions \/ first - last transaction date difference\n    features['transFreq_neg'] = features.nb_trans_neg \/ features.date_diff_first_last\n    \n    temp = pd_transactions.append(features.LoanAmount, ignore_index=True)\n    # The number of transactions that are greater than loan amount \n    features['nb_trans_greaterThanLoan'] = temp.apply(lambda x: x[x>x.iloc[-1]].count(), axis = 0).values\n    # The number of transactions that are greater than half of loan amount \n    features['nb_trans_greaterThanHalfLoan'] = temp.apply(lambda x: x[x>(x.iloc[-1] * 0.5)].count(), axis = 0).values\n    # The number of transactions that are less than -1 * loan amount \n    features['nb_trans_lessThanLoanNeg'] = temp.apply(lambda x: x[x<(-x.iloc[-1])].count(), axis = 0).values\n    # The number of transactions that are less than -0.5 * loan amount\n    features['nb_trans_lessThanHalfLoanNeg'] = temp.apply(lambda x: x[x< (-(x.iloc[-1] * 0.5))].count(), axis = 0).values\n    # The frequency of transactions that are greater than loan amount\n    features['transFreq_greaterThanLoan'] = features.nb_trans_greaterThanLoan \/ features.date_diff_first_last\n    # The frequency of transactions that are greater than half of loan amount\n    features['transFreq_greaterThanHalfLoan'] = features.nb_trans_greaterThanHalfLoan \/ features.date_diff_first_last\n    # The frequency of transactions that are less than -1 * loan amount\n    features['transFreq_lessThanLoanNeg'] = features.nb_trans_lessThanLoanNeg \/ features.date_diff_first_last\n    # The frequency of transactions that are less than -0.5 * loan amount\n    features['transFreq_lessThanHalfLoanNeg'] = features.nb_trans_lessThanHalfLoanNeg \/ features.date_diff_first_last\n    \n    return features","b09ed245":"# same as before, we use multiprocessing to speed it up.\nn_cores = 2\nfeature_split = np.array_split(features, n_cores)\npd_transactions_split = np.array_split(pd_transactions, n_cores, axis = 1)\npd_dates_trans_split = np.array_split(pd_dates_trans, n_cores, axis = 1)\npd_date_loan_split = np.array_split(pd_date_loan, n_cores, axis = 1)\npool = Pool(n_cores)\nfeatures = pd.concat(pool.starmap(addFeatures_2, zip(feature_split, pd_transactions_split, pd_dates_trans_split, pd_date_loan_split)))\npool.close()\npool.join()","c032afee":"features.shape","2b57cdbe":"# check if the new features has any nan or infinite values\nprint(features.columns[features.isna().any()])\nprint(features.columns.to_series()[np.isinf(features).any()])","48c1389c":"corr_greaterThan9 = getCorrGreaterThan(features.drop(dropList_1, axis = 1))\ncorr_greaterThan9","0bd93d5b":"dropList_2 = ['nb_trans_greaterThanLoan', 'transFreq_greaterThanHalfLoan','transFreq_neg',\n              'transFreq_pos', 'transFreq_lessThanLoanNeg', 'transFreq_lessThanHalfLoanNeg',\n              'date_loanDayOfWeek', 'sum_positive\/loan']","8b45de69":"features_dropped_2 = features.drop(dropList_1 + dropList_2, axis = 1)","30e7c585":"# Double check if there is any high correlation left\ncorr_greaterThan9 = getCorrGreaterThan(features_dropped_2)\ncorr_greaterThan9","2d6187e8":"# Modeling: uncomment if needed.\n# scores_df = process_and_train(features_dropped_2)\n# print('mean score: ' + str(scores_df.Mean.mean()))\n# scores_df","ea9fd942":"# The following code is to get the date difference between first \/ last transaction \n# that is greater than the loan amount and the loan date.\n\n# This temp is to combine transactions and loan amount\ntemp = pd_transactions.copy()\ntemp.iloc[-1] = features.LoanAmount\n\n# get the index of the first trans that is greater than loan\nindices_firstGreaterThanLoan = temp.apply(lambda x: x[x>x.iloc[-1]].first_valid_index(), axis = 0)\n# Need to save this to label the Nan values to -1 later\nindices_firstGreaterThanLoan_isnull = indices_firstGreaterThanLoan.isnull()\nindices_firstGreaterThanLoan[indices_firstGreaterThanLoan.isnull()] = 0\n# get the dates\ndays_firstGreaterThanLoan = (pd_date_loan.iloc[0] - np.diag(pd_dates_trans.loc[indices_firstGreaterThanLoan])).dt.days.values\n# the maximum date diff between first trans and the loan date is 89: features['date_diff_first_loan'].max()\n# so I put replace the null with 99 here, to show that it further than maximum date difference\n# I do not want to use 999 because it will act like an outlier and will affect the scale of this feature\ndays_firstGreaterThanLoan[indices_firstGreaterThanLoan_isnull] = 99\nfeatures['date_diff_firstGreaterThanLoan_loan'] = days_firstGreaterThanLoan\n\n# do the same for the last date, here we use last_valid_index\nindices_lastGreaterThanLoan = temp.apply(lambda x: x[x>x.iloc[-1]].last_valid_index(), axis = 0)\nindices_lastGreaterThanLoan_isnull = indices_lastGreaterThanLoan.isnull()\nindices_lastGreaterThanLoan[indices_lastGreaterThanLoan.isnull()] = 0\ndays_lastGreaterThanLoan = (pd_date_loan.iloc[0] - np.diag(pd_dates_trans.loc[indices_lastGreaterThanLoan])).dt.days.values\ndays_lastGreaterThanLoan[indices_lastGreaterThanLoan_isnull] = 99\nfeatures['date_diff_lastGreaterThanLoan_loan'] = days_lastGreaterThanLoan","6dab1b18":"import math\ndef stableIncome(x):\n# The history length and the month is to calculate how long the transaction history of this person is\n# This month is used to determin how many outliers that are greater than the stable income are allowed\n    history_length = (pd_dates_trans[x.name].ffill().iloc[-1] - pd_dates_trans.loc[0, x.name]).days\n    month = math.ceil(history_length\/30)\n# E.g: if month = 2, then allow up to 2 trans that are greater than salary.\n    for i in range(month):\n        x_maxs = x[x > (x.max()*0.7)]\n        dates_diff = pd_dates_trans.loc[x_maxs.index, x.name].diff().dt.days.values[1:]\n\n        if dates_diff.shape[0] > 1:     \n            MAD = np.abs((dates_diff - dates_diff.mean())).mean()\n\n            if MAD < 1.2:\n                mean_dates_diff = dates_diff.mean()\n                nb_dates_diff = dates_diff.shape[0]\n                if mean_dates_diff > 0: \n                    if nb_dates_diff > (int(history_length \/ mean_dates_diff) - 1):\n                        return x_maxs.mean()\n#                 return x_maxs.mean()\n                        break\n\n        x = x.drop(x.idxmax())\n    return 0","6ac19ee7":"features['stableIncome'] = pd_transactions.apply(stableIncome)\nfeatures['stableIncome_yes'] = features.stableIncome>0","f6707044":"features[features.stableIncome_yes == 1]","26481bdc":"# Similar code to test the result:\nindex_temp = 9978\ntemp =  pd_transactions[index_temp]\nhistory_length = (pd_dates_trans[temp.name].ffill().iloc[-1] - pd_dates_trans.loc[0, temp.name]).days\nmonth = math.ceil(history_length\/30)\nx = temp.copy()\n\nfor i in range(month):\n    x_maxs = x[x > (x.max()*0.7)]\n    dates_diff = pd_dates_trans.loc[x_maxs.index, x.name].diff().dt.days.values[1:]\n    print('The ' + str(i) + 'th max: ' + str(x.max()))\n    \n    if dates_diff.shape[0] > 1:     \n        MAD = np.abs((dates_diff - dates_diff.mean())).mean()\n        \n        if MAD < 1.2:\n            temp = pd.DataFrame({'dates':pd_dates_trans.loc[x_maxs.index, index_temp],\n             'trans': x_maxs})\n            print('rounds:' + str(i+1))\n            print(temp)\n            print(\"Date difference in days: \" + str(temp.dates.diff().dt.days.values[1:]))\n            break\n    \n    x = x.drop(x.idxmax())","6c5c202f":"def trans_month_consistancy(x):\n    temp_trans = x.copy().rename('trans')\n    temp_dates = pd_dates_trans[x.name].rename('dates').dt.to_period('M')\n    temp = pd.concat([temp_dates, temp_trans], axis = 1)\n    if temp.dates.value_counts().shape[0]>1:\n        return temp.groupby('dates').sum().std()['trans']\n    else:\n        return -1","0610797f":"features['trans_month_consistancy'] = pd_transactions.apply(trans_month_consistancy)","ecedff00":"features.columns[features.isna().any()] ","b5a5fa2c":"features.columns.to_series()[np.isinf(features).any()]","12b0a263":"features_dropped_3 = features.drop(dropList_1+dropList_2, axis = 1)","6d44cb6a":"corr_greaterThan9 = getCorrGreaterThan(features_dropped_2)\ncorr_greaterThan9","9dfa7fb4":"# scores_df = process_and_train(features_dropped_3)\n# print('mean score: ' + str(scores_df.Mean.mean()))\n# scores_df","1a2046bc":"from sklearn.feature_selection import SelectKBest, f_classif\ndef features_selection(features, nb_features = 10):\n    x = features.drop(['Label'], axis = 1)\n    x.replace([np.inf, -np.inf], np.nan, inplace=True)\n    x = x.fillna(0)\n    x = x.astype(np.float32)\n    scale = StandardScaler()\n    x[x.columns] = scale.fit_transform(x[x.columns])\n    y = features.Label\n    y = y.astype('float32')\n    \n    lr = LogisticRegression(random_state=42, max_iter = 2000, solver='liblinear')\n    lr.fit(x,y)\n    lr_features = pd.Series(np.abs(lr.coef_[0]), index=x.columns).nlargest(nb_features).index.tolist()\n\n    \n    RF = RandomForestClassifier(random_state=42, n_jobs = -1)\n    RF.fit(x, y)\n    feature_importances = pd.Series(RF.feature_importances_, index=x.columns)\n    rf_features = feature_importances.nlargest(nb_features).index.tolist()\n    \n    xy_all = x.copy()\n    xy_all['Label'] = y\n    corr_features = xy_all.corr()['Label'].abs().sort_values(ascending = False)[1:nb_features+1].index.tolist() \n    \n    selector = SelectKBest(f_classif, k=nb_features)\n    selector.fit_transform(x, y)    \n    kbest_features = x.columns[selector.get_support()].tolist()\n    \n    features_importance_accu = pd.Series(0 ,index=x.columns)\n    features_importance_accu[lr_features] = features_importance_accu[lr_features] +1\n    features_importance_accu[rf_features] = features_importance_accu[rf_features] +1\n    features_importance_accu[corr_features] = features_importance_accu[corr_features] +1\n    features_importance_accu[kbest_features] = features_importance_accu[kbest_features] +1\n    features_importance_accu = features_importance_accu[features_importance_accu>0]\n    return features_importance_accu.sort_values(ascending = False)","dc51534e":"features_10 = features_selection(features_dropped_3, nb_features = 10)\nfeatures_selected = features[features_10.index.tolist()]\n# the value is number of vote. \n# 4 means all 4 models voted this feature.\nfeatures_10","977ce5ce":"corr_greaterThan = getCorrGreaterThan(features_selected, threshold = 0.8)\ncorr_greaterThan","b0dc39aa":"features_selected_dropped = features_selected.drop(['nb_trans_0_50', 'sum_negative\/loan'], axis = 1)","d50e07d3":"corr_greaterThan = getCorrGreaterThan(features_selected_dropped, threshold = 0.8)\ncorr_greaterThan","c6ae1ecd":"features_selected_dropped['Label'] = features.Label","e4d67e41":"# scores_df = process_and_train(features_selected_dropped)\n# print('mean score: ' + str(scores_df.Mean.mean()))\n# scores_df","d83a2969":"pd.pivot_table(features, index = 'date_loanMonth', columns = 'Label', values = 'LoanAmount' ,aggfunc ='count')","ebf345c2":"features.groupby(['date_loanMonth', 'Label'])['LoanAmount'].count().unstack().plot(kind = 'bar', stacked = False)\n# plt.legend(['Dead','Survived'])","7ecdad97":"### **3.3 Feature Selection**\nSame as before, we are going to drop the features that have more than 0.9 correlation with others.","f226654b":"### **2.3 Feature Selection**\n\nFor this first round, I drop the features that has more than **0.9** correlation value with others.\n\nMy **rule of thumb** for dropping features is: drop the features that are more related to other features and the ones that are less representive for the whole dataset.\n\n\nFor example, for correlation pair **min** and **min\/loan**, I would drop feature **min\/loan**, since **min** shows more of the **original** values of the feature set. For the group **nb_trans_neg**, **nb_trans_0_-50** and **nb_trans**, I keep **nb_trans** as it shows more of the **general \/ overall** value of the dataset.","261f9719":"Average: 0.66612\n\n|Model|Mean|CV1|CV2|CV3|CV4|CV5|\n|-----|---|----|---|----|--|-----|\n|Random Forest|\t0.679807|\t0.675185|\t0.684869|\t0.669065|\t0.689578|\t0.680337|\n|XG Boost| 0.675700|\t0.669872|\t0.699267|\t0.664746|\t0.680649|\t0.663968|\n|Light GBM| 0.687588|\t0.693477|\t0.701977|\t0.672683|\t0.693791|\t0.676012|\n|CatBoost| 0.702169|\t0.712482|\t0.714096|\t0.687263|\t0.706867|\t0.690137|\n|AdaBoost|\t0.676704|\t0.671098|\t0.691202|\t0.664505|\t0.693447|\t0.663265|\n|Logistic Regression|\t0.674307|\t0.663935|\t0.688082|\t0.677215|\t0.675323|\t0.666982|\n|SVM|\t0.633522|\t0.639101|\t0.642174|\t0.606100|\t0.651830|\t0.628403|\n|KNN|\t0.595210|\t0.606705|\t0.595182|\t0.596093|\t0.592221|\t0.585851|","4a740712":"### **2.4 Modeling**\n\nThe following function is to quickly do cross valication on the data with different models (default parameters). ","578f81a5":"## **4. More Complicated Feature Engineering and Modeling**\nIn this last round of feature engineering, we are going to generate more complicated feautures:\n1. The **day difference** between the **first** transaction that is **greater** than loan and the **loan date**.\n2. The **day difference** between the **last** transaction that is **greater** than loan and the **loan date**.\n3. **Stable income**: if the customer has stable income as weekly, bi-weekly or monthly.\n4. The **consistency (std)** of sum of transaction by the end of each month. \n\nFor feauture 1 and 2:\n\nI replaced the nan values with **99**. This is because usually, I use -1, 0 or a large positive or negative number to replace the nan values. In this case, the **max day difference** is **89**. If I use 0 to replace nan, it would mean that the transaction that is greater than loan amount happend on the **same day** as loan day, which is not correct. If I use -1, it may produce bad linearity when modeling. A large number is also not good because it will affect the **scale \/ distribution** of the feature, which make the actual values less significant after **scaling**. Eventually, I decided to use **99** to replace the nan, which does not affect the original feature **distribution** and is far enough to be captured as an **outlier**.\n\nFor feature 3:\n1. if the person has stable \/ consistant income, the income will be used as a feature. Otherwise it would be 0. In this case, value 0 represents the the fact that the person does not have stable income. \n2. this feature does not 100% include all the cases. I tried to generate a feature that has less false positive. For example, if 80% of the customers have stable income, this function probably captures 60% of them. I ran pivot table and feature selection after this, and it turns out this feature is not predictive, so I did not spend much time to improve this feature. Please see the markdown later kernal more details.","baae88f4":"## **0. Introduction**\n\nI wrote this kernel to share my project experience about **raw data processing**, **feature extraction** and **feature selection** on this **bank transaction** data. \n\nFor such sensitive topic (banking and loan), **scalability is more important than prediciton score**. \n\nThe prediction of this project can be used as a referrence to approve or decline someone's loan request, which is a highly **sensetive** topic. Unlinke general big data \/ deep learning projects, in such field, scalability is more important than modeling. **Covid-19 pandemic** is a great example for this: before Covid, assumbaly, someone who has **consistant savings** by the end of each month\/year would be more likely to be able to pay off the loan, but when covid started, the **transaction pattern** of these people have greatly changed, and all the related features would become **less predictive** than before. Overall, when deploy such models, we need to be able to **understand** each feature and **adjust \/ adapt** to them according to the real world.\n\nIn this kernel, I will focus more on data processing and less on model tuning. I hope this will help to discover the insight of this data set. \n\nPlease note: This dataset has been **modified** and **truncated** for confidential purposes at the source (not myself). Overall the dataset resembles the real world data but is not exactly the same. It still has great values for getting hands-on experience on this type of data.\n\nThe flow of this kernel \/ project:\n1. Load data.\n2. 1st round of data engineering with **transactions** only.\n3. 2st round of data engineering with **transactions** and **dates** combined.\n4. 3rd round of data engineering for more **complicated** features.\n5. **Feature selection** to reduce the dimension of the feature set.\n\nUpdate 1: Some unsupervised algorithmic feature seletection techniques could also be used to reduce feature dimensions (such as PCA, it actually works for the following modeling), but the result will become non-interpretable. As interpretability is a crucial part of the purpose of this project, such algorithms are not used here.","765e29b6":"### **2.2 Feature Completing**\n\n**Note**: **Data completing** should be carefully done. For each feature that has nan or infinite value so far, it is **safe** to replace the nan \/ infinite with 0. \n\n**Safe** means that when replace nan with certain numbers, it does not affect the **order\/distribution** of the feature. For example, for the feature **mean_positive**, the nan in this feature shows that this person does not have any positive transactions, so if we replace the nan with 0, it represents the situation correctly. Later, you will see that for some features, we need different strategies to replace the nan.","749428e5":"### **3.2 Feature Completing**\nIn the newly generated features, there is no nan \/ infinite values. This is because the second round is mainly about date-related features, and all the potential nan values, such as dates of positive transactions, have been handled as 0 during feature creating.","3308d37f":"Average AUC score of all models: 0.7252869101978691\n\n|Model|Mean|CV1|CV2|CV3|CV4|CV5|\n|-----|---|----|---|----|--|-----|\n|\tRandom Forest|\t0.741542|\t0.753430|\t0.749288|\t0.727139|\t0.755542|\t0.722309|\n|\tXG Boost|\t0.725933|\t0.739411|\t0.731597|\t0.716389|\t0.728864|\t0.713403|\n|\tLight GBM|\t0.745766|\t0.756281|\t0.751707|\t0.734527|\t0.751535|\t0.734778|\n|\tCatBoost|\t0.753885|\t0.768099|\t0.766053|\t0.743227|\t0.754386|\t0.737659|\n|\tAdaBoost|\t0.735198|\t0.738970|\t0.732920|\t0.724543|\t0.746768|\t0.732787|\n|\tLogistic Regression|\t0.715002|\t0.710789|\t0.720309|\t0.718530|\t0.713945|\t0.711438|\n|\tSVM\t|0.704255|\t0.719097|\t0.706162|\t0.688971|\t0.702024|\t0.705019|\n|\tKNN\t|0.680716|\t0.683051|\t0.701349|\t0.640183|\t0.689036|\t0.689960|","9bf8f5e6":"After the first round, the feature shape was 48, so we have added 70 - 48 = 22 new features from the second round of feature creating.","7701929c":"Except the label and the LoanAmount, we generated **46** features from transaction data.","e83103d1":"### **3.4 Modeling**\nTo save some time, here is the result:\n\nAverage AUC score of all models: 0.7254990011415525\n\n|Model|Mean|CV1|CV2|CV3|CV4|CV5|\n|-----|---|----|---|----|--|-----|\n|Random Forest|\t0.736432|\t0.751170|\t0.739020|\t0.720061|\t0.735192|\t0.736719|\n|XG Boost| 0.732818|\t0.739160|\t0.744018|\t0.712693|\t0.727589|\t0.740630|\n|Light GBM| 0.749703|\t0.756967|\t0.759939|\t0.723361|\t0.755373|\t0.752874|\n|CatBoost| 0.762248|\t0.773779|\t0.772744|\t0.741876|\t0.763676|\t0.759167|\n|AdaBoost|\t0.735371|\t0.739956|\t0.738056|\t0.719460|\t0.748964|\t0.730420|\n|Logistic Regression|\t0.723240|\t0.710992|\t0.734146|\t0.727296|\t0.721249|\t0.722519|\n|SVM|\t0.717508| 0.717638|\t0.725207|\t0.700323|\t0.721180|\t0.723191|\n|KNN|\t0.646671| 0.660798|\t0.637195|\t0.633402|\t0.658806|\t0.643154|","aeda2134":"## **3. Transactions\/Date Feature Engineering and Modeling**\n### **3.1 Feature creating**\n\nIn the second round, we will do feature engineering on both transaction and date data. Later on you will see that adding date data greatly improve the modeling.","add51bb8":"Average AUC score of all models: 0.7263723522323693\n\n|Model|Mean|CV1|CV2|CV3|CV4|CV5|\n|-----|---|----|---|----|--|-----|\n|Random Forest| 0.736192|\t0.748831|\t0.743023|\t0.719306|\t0.735774|\t0.734028|\n|XG Boost|\t0.735345|\t0.729654|\t0.747683|\t0.716636|\t0.747921|\t0.734833|\n|\tLight GBM|\t0.751969|\t0.760819|\t0.757738|\t0.730682|\t0.756280|\t0.754326|\n|\tCatBoost|\t0.762674|\t0.772615|\t0.773567|\t0.740788|\t0.767057|\t0.759340|\n|\tAdaBoost|\t0.735995|\t0.742679|\t0.739129|\t0.720248|\t0.747000|\t0.730920|\n|\tLogistic Regression|\t0.724189|\t0.708899|\t0.735911|\t0.728844|\t0.723439|\t0.723852|\n|\tSVM\t|0.720140|\t0.720252|\t0.722527|\t0.706110|\t0.725704|\t0.726106|\n|\tKNN\t|0.644475|\t0.643677|\t0.654304|\t0.635964|\t0.649562|\t0.638867|","4a534d66":"## **1. Loading Data**\n\nThe structure of the data:\n\n* Column '0': **`ID`**, is the unique id of the customers. It doesn't have any effect on target.\n* Column '1': **`Dates`**, is the date of each transaction. Duplicated dates means that multiply transaction happened on the same day.\n* Column '2': **`Transaction Amount`**: is the amount of each transaction. Deposit is positive, and withdraw is negative.\n* Column '3': **`Loan Amount`**: is the amount of the Loan.\n* Column '4': **`Loan Date`**: is the date of the Loan.\n* Column '5': **`isDefault`**: is the label that show if the loan is in default or not (in default means failure to pay off the loan). ","e9d5c593":"## **5. Final Feature Selction and Results**\nSo far, we have generated 73 features. As I said at the begining of this kernal, **scalability is more important than predition**. With this many features, it would be difficult to interpret the feature dependency and intersection. We need to reduce the feature dimension while **maintain** the prediction as much as possible.\n\nIn the following function features_selection, I used **Random Forest**, **Logistic Regression**, **Correlation** and **SelectKBest** to select features. Note that the input nb_features is the number of features to be selected from **each** model, and then all these seleted features will be **combined** and output. So the final output of funtion features_selection would be more than input nb_features (in this case it is 20 features). The value in the output series shows the vote of each feature.\n\nAfter this, I dropped the features that have **correlation** score more than **0.8**, which gives 18 features at the end.\n\nYou will see that after cross validation, the average score is almost the same, and the max model score (Catboost) is only about 1 percent less.\n\n### **Correlation is not causation**\n\nWhat surprised me is that the month and year of date of loan have been selected by more than one model and **month** of loan has a **strong correlation** with Label. However, judging by my common sense, approvement of a loan should **NOT** depend on the date. By plotting the feature date_loanMonth against Label, we see that the amount of failure (Label = 1) is roughly similar cross months, but there are less successful cases (Label = 0) from January to June.\n\n\nCould this mean that statistically, the people, who are more responsible and more likely to pay off the loan, tend to request a loan in the second half of year? If so, **why**? Domain experts should investigate this occurrence. ","77fb5535":"### **2.1 Feature Creating**","e734d2c5":"## **2. Transactions Feature Engineering and Modeling**\n\nIn the first round, we will do feature engineering(creating) with only transaction data. We will utilize multiprocessing to speed up the program. Feature definitions are in the comment after each feature.\n","66445fbb":"### **4.2, 4.4, 4.4 Feature Completing, Feature Selection and Modeling**\nYou know the drill. This is prette much the same as in chapter 3. The new 3 added features have no nan values since it has been handled during feature creating.","6a00c1ca":"To capture stable income, these are the cases that I have included in the program:\n1. Stable income that is the same number weekly, bi-weekly or monthly.\n2. The income varies in a small range, but over all still stable. Such as 100,101,99,98,101.\n3. The stable income should be a significant positive transaction.\n4. The person may have positive transactions that are greater than the income value, but such transactions should be rare.\n\n\nNote: The number of **month** is the max number of outlier positive transaction allowed. For example, a: if I am making 1500 dollars every two weeks and the data contains my transactions history of last **two months**, if I received 2000 dollars from my friend, the function will **allow** this exception. However, if I receive 2000, 1700, 1900 dollars, which is more frequent than **monthly**, the program will not allow this and decide that I have no stable income, since this arbitrary income is too frequent.\n\nAlso, this function is mostly date dependent instead of amount dependent. In my opinion, the date difference between salary dates should be pretty much fixed, but the salary amount can vary in a certain range, so I used the date_diff to determin if the income is stable."}}