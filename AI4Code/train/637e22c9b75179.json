{"cell_type":{"6cfc8918":"code","650766dc":"code","3abe75ac":"code","18f9970b":"code","09547212":"code","c82db8f5":"code","1cd6532e":"code","d8788ee3":"code","173568ef":"code","9e8b43e9":"code","5d7215e5":"code","23614ed3":"code","9ff04196":"code","ae26f376":"code","1e071527":"code","11616b1b":"code","1dcabba9":"code","5fc33de1":"code","120c15f1":"code","5ea8c25e":"code","2548ef5a":"code","20506b95":"code","f228592b":"code","3f4e08fa":"code","b5d1cb73":"code","8960e6b0":"code","cad54019":"code","524b5240":"code","a18a92fb":"code","65a17bc3":"code","bf7af735":"code","9513acad":"code","4b81622a":"code","025b5586":"code","f7c67f52":"code","95142790":"code","219b7c48":"code","594f1789":"code","8dd7ef91":"code","a0d9b8ed":"code","b5a2a2be":"code","fd39905b":"code","ffab99b8":"code","d7b3df6f":"code","e544acb9":"code","60a3f5c5":"code","94601e88":"code","eb7646f3":"code","c3746e4c":"code","02ee13ce":"markdown","173e804e":"markdown","396deeec":"markdown","2af25d59":"markdown","74262231":"markdown","c03a9179":"markdown","5b9c8204":"markdown","8656b4f3":"markdown","3dc4c90c":"markdown","c8cdcac9":"markdown","9aa349b0":"markdown","38b7dbf9":"markdown","1839e75d":"markdown","57994c45":"markdown","115cc018":"markdown","037efc81":"markdown","faa2998b":"markdown","1681c848":"markdown","e2e37a19":"markdown","0a2f3971":"markdown","f2a6fa1d":"markdown","79bd7f29":"markdown","8501592f":"markdown","e29ef66f":"markdown","498787dc":"markdown"},"source":{"6cfc8918":"import pandas as pd\nimport numpy as np \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sqlite3\nfrom datetime import datetime\nimport plotly.express as px\nimport warnings\nfrom plotly.offline import init_notebook_mode, iplot\nfrom plotly.graph_objs import *\n\ninit_notebook_mode(connected=True)  \nwarnings.filterwarnings('ignore')\n%matplotlib inline","650766dc":"# Connect to the file (.db)\ndb = sqlite3.connect('\/kaggle\/input\/clubhouse-dataset\/Clubhouse_Dataset_v1.db')\n# Read the file as Data frame\ndf = pd.read_sql_query(\"SELECT * FROM user\", db)","3abe75ac":"# .info() tells us about data types and missing data, .head() produces the first 5 rows\nprint(df.info())\ndf.head()","18f9970b":"# Describe will show give us some more information about the distribution of the data\npd.set_option('float_format', '{:f}'.format)\ndf.describe()","09547212":"# Lets find out if we have any duplicate users\nprint('Unique user_id: {}'.format(df['user_id'].nunique()))\nprint('Unique username: {}'.format(df['username'].nunique()))\n\ndf[df['invited_by_user_profile']=='null'].shape","c82db8f5":"# Who is the duplicated user?\ndf[df.duplicated('username', keep=False)]","1cd6532e":"# Reminder of what the data looks like\nprint(df.info())\ndf.head()","d8788ee3":"# Function that will convert values in columns to 1\/0 to indicate True\/False\ndef true_false(cols):\n    if cols == 'null':\n        return 0\n    elif pd.isnull(cols):\n        return 0\n    else:\n        return 1","173568ef":"# Select Columns to Change\nchange_cols = ['photo_url','twitter', 'instagram']\n\n# Change those columns with the new function, and assign them to type Integer\nfor i in change_cols:\n    df[i] = df[i].apply(true_false).astype('int')\n\n# Display Results    \nprint(df.info())\ndf.head()","9e8b43e9":"# Create `date_created` column\ndf['date_created'] = pd.to_datetime(df['time_created'].str[0:10])","5d7215e5":"# Establish a Reference Date to Calculate Age of Accounts\nref_date = datetime.strptime('2021-04-05', '%Y-%m-%d')\n\n# Create `weeks_old` Column based on the `date_created` Column\ndf['weeks_old'] = (ref_date - df['date_created']).apply(lambda x: x\/np.timedelta64(1,'W'))\ndf.head()","23614ed3":"# Create a Dataframe that Displays the Number of of Invites that Each User is Responsible For\ninvt_df = pd.DataFrame(df['invited_by_user_profile'].value_counts().rename_axis('user_id').reset_index(name='invite_count'))\ninvt_df = invt_df[invt_df['user_id']!='null']\n\n# Merge the New Dataframe with the Existing Data\ndf = df.merge(invt_df, how='left', on='user_id')\ndf['invite_count'] = df['invite_count'].fillna(0)\n\n# Display New Dataframe\ndf.head()","9ff04196":"# Create `follow_ratio`\ndf['follow_ratio'] = df['num_followers']\/df['num_following']","ae26f376":"# Create a Function that will Assign a Classifcation for Each User Based on the Number of Accounts Following Them.\ndef followers_classification(col):\n    if col <= 15:\n        return 'small'\n    elif 15 < col <=37:\n        return 'medium'\n    elif 37 < col <= 99:\n        return 'large'\n    else:\n        return 'massive'    ","1e071527":"# Use the New Function to Assign Each User a Classification\ndf['follower_size'] = df['num_followers'].apply(followers_classification).astype('category')\n\n# Display New Dataframe\nprint(df.info())\ndf.head()","11616b1b":"# Filter Out the Duplicated User and then Remove Columns\ndf = df[df['user_id'] !=36658]\ndf.drop(['user_id','name','time_created'],axis=1, inplace=True)\n\n# Display New Dataframe\nprint(df.info())\ndf.head()","1dcabba9":"# 15 Most Followed Accounts\ndf[['username','num_followers','num_following','follow_ratio','follower_size']].sort_values('num_followers', ascending=False).head(15)","5fc33de1":"# 15 Highest Follow Ratios\ndf[df['num_following']!=0][['username','num_followers','num_following','follow_ratio','follower_size']].sort_values('follow_ratio', ascending=False).head(15)","120c15f1":"# Graph When Users Joined Clubhouse\nsns.set_style('darkgrid')\nplt.figure(figsize=(30,5))\n\ndf['date_created'].hist(bins=100)\n\nplt.title('When Did Users Join Clubhouse?', fontsize=25)\nplt.xlabel('Date',fontsize=15)\nplt.ylabel('New Users',fontsize=15)","5ea8c25e":"# Graph When Users Joined Clubhouse, by the Size of Their Following.\nkwargs = dict(alpha=0.15, bins=100)\nplt.figure(figsize=(30,5))\n\nplt.hist(df[df['follower_size'] == 'small']['date_created'], **kwargs, color='g',label='small')\nplt.hist(df[df['follower_size'] == 'medium']['date_created'],**kwargs, color='r',label='medium')\nplt.hist(df[df['follower_size'] == 'large']['date_created'],**kwargs, color='b',label='large')\nplt.hist(df[df['follower_size'] == 'massive']['date_created'],**kwargs, color='m',label='massive')\n\nplt.title('When Did Users Join Clubhouse?\\nBroken Out by Following Size', fontsize=25)\nplt.xlabel('Date',fontsize=15)\nplt.ylabel('New Users',fontsize=15)\nplt.legend()","2548ef5a":"# Graph All of the Following Sizes\nplt.figure(figsize=(30,5))\n\nsns.boxplot(x='num_followers',data=df)\nplt.title(\"Clubhouse Followers\",fontsize=25)\nplt.xlabel('Number of Followers\\nIn Millions',fontsize=15)\nplt.ylabel('Follower Size',fontsize=15)","20506b95":"# Graph of Massive\nplt.figure(figsize=(30,5))\n\nsns.boxplot(x='num_followers',y='follower_size',hue='follower_size',data=df[df['follower_size']=='massive'])\nplt.title(\"Clubhouse Followers by Follower Size\",fontsize=25)\nplt.xlabel('Number of Followers\\nIn Millions',fontsize=15)\nplt.ylabel('Follower Size',fontsize=15)","f228592b":"# Graph All Following Sizes, Except Massive\nplt.figure(figsize=(30,5))\n\nsns.boxplot(x='num_followers',y='follower_size',hue='follower_size',data=df[df['follower_size']!='massive'])\nplt.title(\"Clubhouse Followers by Follower Size\",fontsize=25)\nplt.xlabel('Number of Followers',fontsize=15)\nplt.ylabel('Follower Size',fontsize=15)","3f4e08fa":"# Graph of Users broken out by Who Has\/Doesn't have Instagram- Massive Category\nplt.figure(figsize=(30,5))\nsns.violinplot(x='num_followers', y='follower_size',data=df[df['follower_size']=='massive'],hue='instagram',split=True)\n\nplt.title(\"Clubhouse Followers Massive Follower Size\\nHas Instagram\",fontsize=25)\nplt.xlabel('Number of Followers\\nIn Millions',fontsize=15)\nplt.ylabel('Follower Size',fontsize=15)","b5d1cb73":"# Graph of Users broken out by Who Has\/Doesn't have Instagram- Exclude Massive Category\nplt.figure(figsize=(30,5))\nsns.violinplot(x='num_followers', y='follower_size',data=df[df['follower_size']!='massive'],hue='instagram',split=True)\n\nplt.title(\"Clubhouse Followers Small, Medium, Large Follower Size\\nHas Instagram\",fontsize=25)\nplt.xlabel('Number of Followers',fontsize=15)\nplt.ylabel('Follower Size',fontsize=15)","8960e6b0":"# Graph of Users broken out by Who Has\/Doesn't have Twitter- Massive Category\nplt.figure(figsize=(30,5))\nsns.violinplot(x='num_followers', y='follower_size',data=df[df['follower_size']=='massive'],hue='twitter',split=True)\n\nplt.title(\"Clubhouse Followers Massive Follower Size\\nHas Twitter\",fontsize=25)\nplt.xlabel('Number of Followers\\nIn Millions',fontsize=15)\nplt.ylabel('Follower Size',fontsize=15)","cad54019":"# Graph of Users broken out by Who Has\/Doesn't have Twitter- Exclude Massive Category\nplt.figure(figsize=(30,5))\nsns.violinplot(x='num_followers', y='follower_size',data=df[df['follower_size']!='massive'],hue='twitter',split=True)\n\nplt.title(\"Clubhouse Followers Small, Medium, Large Follower Size\\nHas Twitter\",fontsize=25)\nplt.xlabel('Number of Followers',fontsize=15)\nplt.ylabel('Follower Size',fontsize=15)","524b5240":"# Graph of Users broken out by Who Has\/Doesn't have a Profile Photo- Massive Category\nplt.figure(figsize=(30,5))\nsns.violinplot(x='num_followers', y='follower_size',data=df[df['follower_size']=='massive'],hue='photo_url',split=True)\n\nplt.title(\"Clubhouse Followers Massive Follower Size\\nHas Photo\",fontsize=25)\nplt.xlabel('Number of Followers\\nIn Millions',fontsize=15)\nplt.ylabel('Follower Size',fontsize=15)","a18a92fb":"# Graph of Users broken out by Who Has\/Doesn't have a Profile Photo- Exclude Massive Category\nplt.figure(figsize=(30,5))\nsns.violinplot(x='num_followers', y='follower_size',data=df[df['follower_size']!='massive'],hue='photo_url',split=True)\n\nplt.title(\"Clubhouse Followers Massive Follower Size\\nHas Photo\",fontsize=25)\nplt.xlabel('Number of Followers\\nIn Millions',fontsize=15)\nplt.ylabel('Follower Size',fontsize=15)","65a17bc3":"# Use the Previously Created Function `true_false()` to convert the `invited_by_user_profile` Column to 1\/0\ndf['invited_by_user_profile'] = df['invited_by_user_profile'].apply(true_false).astype('int')","bf7af735":"# Plot whether Users Were Invited or Not- Massive\nplt.figure(figsize=(30,5))\nsns.countplot(y='invited_by_user_profile',hue='follower_size',data=df[df['follower_size']=='massive'])\n\nplt.title(\"Clubhouse Users by Follower Size\\nInvited or Not\",fontsize=25)\nplt.xlabel('Users',fontsize=15)\nplt.ylabel('Invited',fontsize=15)","9513acad":"# Plot whether Users Were Invited or Not- Exclude Massive\nplt.figure(figsize=(30,5))\nsns.countplot(y='invited_by_user_profile',hue='follower_size',data=df[df['follower_size']!='massive'])\n\nplt.title(\"Clubhouse Users by Follower Size\\nInvited or Not\",fontsize=25)\nplt.xlabel('Users',fontsize=15)\nplt.ylabel('Invited',fontsize=15)","4b81622a":"# Sample the Data\ndf = df.sample(n=10_000, random_state=101)","025b5586":"# A Simple Pairplot to show the relationship between the sampled data.  Original Dataset was too large for this.\nplt.figure(figsize=(30,15))\nsns.pairplot(df, hue='follower_size')","f7c67f52":"# All Data\nfig = px.scatter_3d(df, x='weeks_old', y='num_followers', z='invite_count',\n              color='follower_size',\n              labels={'invite_count':'Invite Count', 'num_followers':'Followers',\n              'weeks_old':'Weeks Old', 'follower_size':'Follower Size'})\nfig.show()","95142790":"# Exclude the Massive Category\nfig = px.scatter_3d(df[df['follower_size']!='massive'], x='weeks_old', y='num_followers', z='invite_count',\n              color='follower_size',\n              labels={'invite_count':'Invite Count', 'num_followers':'Followers',\n              'weeks_old':'Weeks Old', 'follower_size':'Follower Size'})\nfig.show()","219b7c48":"# Only the Massive Category\nfig = px.scatter_3d(df[df['follower_size']=='massive'], x='weeks_old', y='num_followers', z='invite_count',\n              color='follower_size',\n              labels={'invite_count':'Invite Count', 'num_followers':'Followers',\n              'weeks_old':'Weeks Old', 'follower_size':'Follower Size'})\nfig.show()","594f1789":"# Drop columns\ndf.drop(['username','num_followers','follow_ratio','date_created'], axis=1, inplace=True)\n\ndf.head()","8dd7ef91":"from sklearn.preprocessing import StandardScaler","a0d9b8ed":"# Scale the Data and create a new Dataframe, excluding our target.\nscaler = StandardScaler()\nscaler.fit(df.drop(['follower_size'],axis=1))\n\nscaled_features = scaler.transform(df.drop('follower_size',axis=1))\n\ndf_feat = pd.DataFrame(scaled_features,columns=df.columns[:-1])\ndf_feat.head()","b5a2a2be":"from sklearn.model_selection import train_test_split","fd39905b":"X_train, X_test, y_train, y_test = train_test_split(scaled_features,df['follower_size'],\n                                                    test_size=0.30, random_state=101)","ffab99b8":"from sklearn.neighbors import KNeighborsClassifier","d7b3df6f":"# Empty List\nrate_of_error = []\n\n# Will take some time\nfor i in range(1,101):\n    \n    knn = KNeighborsClassifier(n_neighbors=i)\n    knn.fit(X_train,y_train)\n    pred_i = knn.predict(X_test)\n    rate_of_error.append(np.mean(pred_i != y_test))","e544acb9":"# Plot the results of the rate_of_error for loop\nplt.figure(figsize=(30,10))\nplt.plot(range(1,101),rate_of_error,color='blue', linestyle='dashed', marker='o',\n         markerfacecolor='red', markersize=10)\nplt.title('Error Rate vs. K Value',fontsize=25)\nplt.xlabel('K Value',fontsize=15)\nplt.ylabel('Error Rate',fontsize=15)","60a3f5c5":"knn = KNeighborsClassifier(n_neighbors=24)\nknn.fit(X_train,y_train)\n\npred = knn.predict(X_test)","94601e88":"from sklearn.metrics import classification_report,confusion_matrix","eb7646f3":"print(confusion_matrix(y_test,pred))\nprint(classification_report(y_test,pred))","c3746e4c":"# Import Random Forest\nfrom sklearn.ensemble import RandomForestClassifier\n\n# Create Instance of Random Forest and fit the data\nrfc = RandomForestClassifier(n_estimators=100, random_state = 101).fit(X_test, y_test)\n\n#  Create Predictions\nrfc_predictions = rfc.predict(X_test)\n\n# Print out Confusion Matrix and Classification Report\nprint(confusion_matrix(y_test,rfc_predictions))\nprint('\\n')\nprint(classification_report(y_test,rfc_predictions))","02ee13ce":"#### Import Libraries","173e804e":"### 3.5 Were Users Invited or Not?\nTo test this, we will finally convert the `'invited_by_user_profile'` to a True\/False Category.","396deeec":"#### Import Data","2af25d59":"### 2.7 Remove Duplicated Username and Unnecessary Columns\n\nNow that I've added all of the new columns that I am interested in for now, it's time to get rid of some columns that are not necessary. Looking at the data, we can see that `'user_id'`, `'name'` and `'username'` all refer to one specific account. I'm going keep `'username'` and get rid of the other two columns.  Back in Step 1, I pointed out a duplicate username that appears two times.  Before I get rid of any columns I am going to get rid of that duplicate row.\n\nI am going to keep the second appearance, because that row contains more data, and it has higher counts in `'num_followers'` and `'num_following`'.  I'm going to do this by filtering out the one I don't want by the `'user_id`.  I'm doing it this way, because I know those values are unique and I won't have to worry about accidentally removing the wrong duplicate.","74262231":"# Exploring the Clubhouse Data Set\n\nThank you, John Turkey for providing the dataset.\n\nhttps:\/\/www.kaggle.com\/johntukey\/clubhouse-dataset\n\nThis is my very first Kaggle Notebook.  I am going to walk you through all the steps I took to explore this data set.  John Turkey who originally posted this data set requested some visualizations, so I have included several that I found insightful\/beneficial to this notebook.  In Step 5, I also created a model to predict the size of a user's following based on some classifications created in Step 2.6. I hope you enjoy my notebook!\n\nAfter importing the appropriate libraries and our data, we will go through the following steps:<br>\n1. What's in the Data?\n> So what's in the Data?<br>\n2. Explore\/Clean\n> 2.1 Convert Selected Column Values to 1\/0 for True\/False<br>\n> 2.2 Create a `'date_created'` Column from `'time_created'`<br>\n> 2.3 Add `'weeks_old'` Column<br>\n> 2.4 Add `'invite_count`' Column<br>\n> 2.5 Add '`follow_ratio'` Column<br>\n> 2.6 Classify Each User's Following Size<br>\n> 2.7 Remove Duplicated Username and Unnecessary Columns<br>\n3. Insights and Visualizations\n> 3.1 Let's look at the top 15 users by `'num_followers'` and then by '`follow_ratio'`<br>\n> 3.2 When Did Users Join Clubhouse?<br>\n> 3.3 Distribution within `'follower_size`'<br>\n> 3.4 Does Connecting Additional Social Media Accounts or Photo Impact a User's '`num_follower`'<br>\n> 3.5 Were Users Invited or Not?<br>\n4.  Subset the Data and Continue to Visualize\n> 4.1 Subset the Data<br>\n> 4.2 A Pairplot<br>\n> 4.3 Create 3D Scatter Plots<br>\n5. Modeling\n> 5.1 Final Prep<br>\n> 5.2 Standardize the Data and Prepare for Training<br>\n> 5.3 Train Test Split<br>\n> 5.4 Import KNN and Choose the Right K Value<br>\n> 5.5 Run The Final Model<br>\n> 5.6 Random Forest Classification<br>\n>>* This was not in my original notebook.  I added this in several months after I originally published this notebook.\n6. Final Thoughts","c03a9179":"### 2.2 Create a `'date_created'` Column from `'time_created'`\n\nI prefer to work with dates by themselves, so I created a new column for the date accounts were created on and gave it the type `'date_created'`.","5b9c8204":"### 2.6 Classify Each User's Following Size\nIn the modeling section, the model will try to predict a classification to describe how many followers a user will have.  I will create those classifications here.  For the sake of simplicity, the sizes as based on the quartiles of `'num_follwers'`, which can be found in Step 1. under `df.describe()`.","8656b4f3":"### **5.2 Standardize the Data and Pepare for Training**\nIn order to get the best model we can from the data we have, I am scaling all of the data.","3dc4c90c":"## **Step 5. Modeling**\n\n### **5.1 Final Prep**\n\nWe will be using k-means to model this data. I'm using KNN, because because it's easy, straight forward and I think a good fit for this data.  I have not done much to this model, because this notebook is just for fun.\n\nBefore we get too far, we need to remove a few columns and then the data is ready for the model.","c8cdcac9":"### 3.3 Distribution within `'follower_size`'\n\nWhile looking at this data, I have decided to display the 'massive' category by itself, because as you can see below it skews the data so much that you cannot read the graphs.  But for good measure, I've icluded a simple plot below to illustrate the entire dataset. ","9aa349b0":"## **Final Thoughts**\n\nI hope that you enjoyed my notebook!  I think that the biggest take away here is that the Majority of Users joined right before the New Year (2021) and most users have a relatively small following.  You can see from the model that we have had some success with predicting how large of a following each user will have based on the data we have in the dataset, but it is far from perfect.  What do you think could be done to improve the accuracy of this model? <BR><BR>\n    \n\n**EDIT**<br>\nA few months after I published this notebook, I took a second look to see if I could improve upon what I had already done.  There is a lot that I could have improved, but I decided I wanted to make one change- A new model.  This new model is a random forest classification.  This model outpreformed the KNN model by a large margin.  This is because the KNN model was not the right model to use for this type of problem.  The random forest weighs the features while the KNN model must looks at how close together the points are.  In this case, the Random Forest is by far the superior choice, and produced much better results, indicating that the following size is very predictive.","38b7dbf9":"### 5.5 Random Forest\n\nThis section is a new.  After a few, I decided I wanted to revisit this and see if there is anything I could have done better.  There was a lot, but the biggest issue was how poorly my KNN model preformed.  Without making any changes to the data, I ran this through a random forest and I got much better results.   I wanted to add that in here.","1839e75d":"## **Step 1. What's in the Data?**\n\nMy first step is to explore the data a to get a better sense of what's here.  This step is very important with any data set, but especially one that you did not produce.  Exploring the basic structure of a dataset will help determine what kinds of questions to ask.  Once you know what kind of questions you want to ask, you can start cleaning the data.","57994c45":"### **5.4 Run The Final Model**\n\nIt looks like k=24 had the lowest error rate, so let's run that.","115cc018":"### ***So What's in the Data?***\n\nThere's a lot of data here- this dataframe is 99.2MB, which is significant.  Without subsetting the data, there is a limit to what we do with the data.  In order to run the models at the end of this notebook, I will subset the data, and then show a few more visualizations.  Before that, I will clean all of the data. When we are done transforming the data we will check the size before we start vizualizng and modeling. For now it's important to note that with just a few basic steps, we've learned quite a bit about our data. In no particular order, here are some things I think are important.\n\n1. We know that there are 1,300,515 rows and 10 columns, and only one column (`'photo_url`) is missing data. \n2. There are other columns show the string 'null', which indicates that there is no value.\n3. We can also see that the '`user_id'` column has a unique value for every row, but the `'username'` column has a duplicate value. \n4. Both the `'num_following'` and `'num_followers'` columns have huge ranges.  `'num_followers'`, which will be the basis of my model's target, has a `min` of 0 and a `max` of 4,187,268, and the mean is 260 and a standard deviation of 16,742.  We will need to keep that in mind moving forward.\n\n## **Step 2. Explore\/Clean**<br>\nAfter looking at what is in the dataset, it is important to look at what ***isn't*** in the dataset.  According to the results of `df.info()`, the only column that has any null or missing values is`'photo_id'`.  However, `df.head()` shows that there are multiple columns that contain 'null'.  What's happening is that in '`photo_url'` a missing or null value means that there is no profile picture.  In `'twitter'`, `instagram', and 'invited_by_user_profile'`, the string 'null' means that there is no account to list.  In this notebook, the account names and profile picture URL are not relevant, so the `'photo_url'`,`'twitter'`, and `'instagram'` columns will be changed to 1\/0 indicating Yes\/No. Later, the same thing will be done to the `'invited_by_user_profile`' column, but not until after some more examination.\n\nI will also be adding some columns to the data in this section for some further exploration.\n\n### 2.1 Convert Selected Column Values to 1\/0 for True\/False\n\nThe first step is to create a function that will convert columns that are null or contain the string 'null' to a 0, and a 1 anywhere that an account or url is listed. Then using a for loop, apply the function to the desired columns and convert those columns to categories.","037efc81":"### 2.3 Add `'weeks_old'` Column\n\nLater we will visualize the distribution of when accounts were created.  When we get to that point, it will become apparent that most of the accounts are less than a year old, and man many are only a few months old.  For this reason, I decided that it would be best to gauge the age of each account in weeks. I am using April 5, 2021 as the date of reference here, because that's the data was posted on Kaggle. ","faa2998b":"### 2.4 Add `'invite_count`' Column\nThe `'invited_by_user_profile'` lists the user_id of the user that invited the user in the row.  I want to know how many people each user invited.  To do this I created a new dataframe based on the value counts of the `'invited_by_user_profile'` column then merged it with `df`.  The resulting dataframe has a new column called `'invite_count'` which indicating how many users on this list the use in that row invited to join Clubhouse.  I also removed 'null' from the `'user_id'` column, and then replace any NaN values with 0.","1681c848":"### **5.4 Import KNN and Choose the Right K Value**\n\nTo do this we are going to create a for loop and see which k value gives us the best results.  Then we will plot those results.","e2e37a19":"## **Step 3.  Insights and Visualizations**\nNow that our data has been sufficiently cleaned, let's create some visualizations to get better understanding of what we have.  These are not done in any particular order, but I have tried to group things together in a way that makes sense.<br><br>\n\n### 3.1 Let's look at the top 15 users by `'num_followers'` and then by '`follow_ratio'`.\n\nThhese are some power users!  In both cases, we see a very large number in the `'num_followers'` column, and typically a triple diget or more count in the '`num_following'` column.  Also noteable, the `'follower_size'` is massive for each of the top 20 members when sorted by `'follow_ratio'`, but only two of the top 20 follow more than 300 people, and one of them only follows 3 other accounts!\n\n*Before sorting  by `'follow_ratio`', I removed anyone who is following 0 other users.","0a2f3971":"### 2.5 Add '`follow_ratio'` Column\n\nThis column will be the ratio of `'num_followers'`\/`'num_following'`.  This is just another way to look at their audience in relation to the number of users that they follow.","f2a6fa1d":"### **5.3 Train Test Split**\nSplit the data up for modeling","79bd7f29":"### 3.2 When Did Users Join Clubhouse?\n\nLooking at when users joined and then getting more granuar by overlapping the counts, we see that overwhelming majority of the users joined right before the beginning of 2021, and there was a steep fall off of new membership, with one small surge shortly after the peak, which looks to have been mostly users a small number of followers.","8501592f":"## **Step 4.  Subset the Data and Continue to Visualize**\n\nThere is a lot of data here, in order to run the model efficently, I'm am going to pull a small sample of the data, using only 10,000 users instead of all 1.3 million. A smaller dataset will allow for more visualizations.  In the next cells I will subset the data, and do the final bit of data cleaning for some more visualizations before starting the modeling process.\n\n### 4.1 A Pairplot\n\nThis is the first time we can visualize all of the variables like this, because there are far fewer points to plot.  The pair plot below demonstraints the relationships between variables.\n","e29ef66f":"### 4.2 3-D Plotly Scatter Plot\nThese are really just for fun.  I did not include a titles on these plots to make it easier to move around and see the points.  Note that most of the points are clustered together.  We can see that there are a lot of outliers in this data.","498787dc":"### 3.4 Now lets see if Having a Social Media Account Or Photo impact a user's '`num_follower`'"}}