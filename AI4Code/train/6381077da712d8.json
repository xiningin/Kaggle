{"cell_type":{"3cf54c21":"code","69d93b82":"code","ac9594dc":"code","73d05c96":"code","60cc139c":"code","7ae4eeac":"code","dbd547be":"markdown"},"source":{"3cf54c21":"import math\nimport numpy as np\nimport pandas as pd\nfrom sklearn.metrics import mean_absolute_error\n\ndef rmsle(y, y_pred):\n        assert len(y) == len(y_pred)\n        terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(y_pred)]\n        return (sum(terms_to_sum) * (1.0\/len(y))) ** 0.5\n    \n\ndef fix_target(frame, key, target, new_target_name=\"target\"):\n    import numpy as np\n\n    corrections = 0\n    group_keys = frame[ key].values.tolist()\n    target = frame[target].values.tolist()\n\n    for i in range(1, len(group_keys) - 1):\n        previous_group = group_keys[i - 1]\n        current_group = group_keys[i]\n\n        previous_value = target[i - 1]\n        current_value = target[i]\n        if current_group == previous_group:\n                if current_value<previous_value:\n                    current_value=previous_value\n                    target[i] =current_value\n\n\n        target[i] =max(0,target[i] )#correct negative values\n\n    frame[new_target_name] = np.array(target)\n    \n    \ndef rate(frame, key, target, new_target_name=\"rate\"):\n    import numpy as np\n\n\n    corrections = 0\n    group_keys = frame[ key].values.tolist()\n    target = frame[target].values.tolist()\n    rate=[1.0 for k in range (len(target))]\n\n    for i in range(1, len(group_keys) - 1):\n        previous_group = group_keys[i - 1]\n        current_group = group_keys[i]\n\n        previous_value = target[i - 1]\n        current_value = target[i]\n         \n        if current_group == previous_group:\n                if previous_value!=0.0:\n                     rate[i]=current_value\/previous_value\n\n                 \n        rate[i] =max(1,rate[i] )#correct negative values\n\n    frame[new_target_name] = np.array(rate)\n    \ndef get_data_by_key(dataframe, key, key_value, fields=None):\n    mini_frame=dataframe[dataframe[key]==key_value]\n    if not fields is None:                \n        mini_frame=mini_frame[fields].values\n        \n    return mini_frame\n\ndirectory=\"\/kaggle\/input\/covid19-global-forecasting-week-3\/\"\nmodel_directory=\"\/kaggle\/input\/model-dir\/model\"\n\ntrain=pd.read_csv(directory + \"train.csv\", parse_dates=[\"Date\"] , engine=\"python\")\ntest=pd.read_csv(directory + \"test.csv\", parse_dates=[\"Date\"], engine=\"python\")\n\ntrain[\"key\"]=train[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\ntest[\"key\"]=test[[\"Province_State\",\"Country_Region\"]].apply(lambda row: str(row[0]) + \"_\" + str(row[1]),axis=1)\n\n#last day in train\nmax_train_date=train[\"Date\"].max()\nmax_test_date=test[\"Date\"].max()\nhorizon=  (max_test_date-max_train_date).days\nprint (\"horizon\", int(horizon))\n\n\n#test_new=pd.merge(test,train, how=\"left\", left_on=[\"key\",\"Date\"], right_on=[\"key\",\"Date\"] )\n#train.to_csv(directory + \"transfomed.csv\")\n\ntarget1=\"ConfirmedCases\"\ntarget2=\"Fatalities\"\n\nkey=\"key\"","69d93b82":"fix_target(train, key, target1, new_target_name=target1)\n#fix_target(train, key, target2, new_target_name=target2)\n\nrate(train, key, target1, new_target_name=\"rate_\" +target1 )\nrate(train, key, target2, new_target_name=\"rate_\" +target2 )\nunique_keys=train[key].unique()\nprint(len(unique_keys))\n\n\ntrain","ac9594dc":"def get_lags(rate_array, current_index, size=20):\n    lag_confirmed_rate=[-1 for k in range(size)]\n    for j in range (0, size):\n        if current_index-j>=0:\n            lag_confirmed_rate[j]=rate_array[current_index-j]\n        else :\n            break\n    return lag_confirmed_rate\n\ndef days_ago_thresold_hit(full_array, indx, thresold):\n        days_ago_confirmed_count_10=-1\n        if full_array[indx]>thresold: # if currently the count of confirmed is more than 10\n            for j in range (indx,-1,-1):\n                entered=False\n                if full_array[j]<=thresold:\n                    days_ago_confirmed_count_10=abs(j-indx)\n                    entered=True\n                    break\n                if entered==False:\n                    days_ago_confirmed_count_10=100 #this value would we don;t know it cross 0      \n        return days_ago_confirmed_count_10 \n    \n    \ndef ewma_vectorized(data, alpha):\n    sums=sum([ (alpha**(k+1))*data[k] for  k in range(len(data)) ])\n    counts=sum([ (alpha**(k+1)) for  k in range(len(data)) ])\n    return sums\/counts\n\ndef generate_ma_std_window(rate_array, current_index, size=20, window=3):\n    ma_rate_confirmed=[-1 for k in range(size)]\n    std_rate_confirmed=[-1 for k in range(size)] \n    \n    for j in range (0, size):\n        if current_index-j>=0:\n            ma_rate_confirmed[j]=np.mean(rate_array[max(0,current_index-j-window+1 ):current_index-j+1])\n            std_rate_confirmed[j]=np.std(rate_array[max(0,current_index-j-window+1 ):current_index-j+1])           \n        else :\n            break\n    return ma_rate_confirmed, std_rate_confirmed\n\ndef generate_ewma_window(rate_array, current_index, size=20, window=3, alpha=0.05):\n    ewma_rate_confirmed=[-1 for k in range(size)]\n\n    \n    for j in range (0, size):\n        if current_index-j>=0:\n            ewma_rate_confirmed[j]=ewma_vectorized(rate_array[max(0,current_index-j-window+1 ):current_index-j+1, ], alpha)           \n        else :\n            break\n    \n    #print(ewma_rate_confirmed)\n    return ewma_rate_confirmed\n\n\ndef get_target(rate_col, indx, horizon=33, average=3, use_hard_rule=False):\n    target_values=[-1 for k in range(horizon)]\n    cou=0\n    for j in range(indx+1, indx+1+horizon):\n        if j<len(rate_col):\n            if average==1:\n                target_values[cou]=rate_col[j]\n            else :\n                if use_hard_rule and j +average <=len(rate_col) :\n                     target_values[cou]=np.mean(rate_col[j:j +average])\n                else :\n                    target_values[cou]=np.mean(rate_col[j:min(len(rate_col),j +average)])\n                   \n            cou+=1\n        else :\n            break\n    return target_values\n\n\ndef dereive_features(frame, confirmed, fatalities, rate_confirmed, rate_fatalities, \n                     horizon ,size=20, windows=[3,7], days_back_confimed=[1,10,100], days_back_fatalities=[1,2,10]):\n    targets=[]\n    \n    names=[\"lag_confirmed_rate\" + str(k+1) for k in range (size)]\n    for day in days_back_confimed:\n        names+=[\"days_ago_confirmed_count_\" + str(day) ]\n    for window in windows:        \n        names+=[\"ma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]\n        names+=[\"std\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)] \n        names+=[\"ewma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]         \n        \n        \n    names+=[\"lag_fatalities_rate\" + str(k+1) for k in range (size)]\n    for day in days_back_fatalities:\n        names+=[\"days_ago_fatalitiescount_\" + str(day) ]    \n    for window in windows:        \n        names+=[\"ma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]\n        names+=[\"std\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]  \n        names+=[\"ewma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]        \n    names+=[\"confirmed_level\"]\n    names+=[\"fatalities_level\"]    \n    \n    names+=[\"confirmed_plus\" + str(k+1) for k in range (horizon)]    \n    names+=[\"fatalities_plus\" + str(k+1) for k in range (horizon)]  \n    \n    #names+=[\"current_confirmed\"]\n    #names+=[\"current_fatalities\"]    \n    \n    features=[]\n    for i in range (len(confirmed)):\n        row_features=[]\n        #####################lag_confirmed_rate       \n        lag_confirmed_rate=get_lags(rate_confirmed, i, size=size)\n        row_features+=lag_confirmed_rate\n        #####################days_ago_confirmed_count_10\n        for day in days_back_confimed:\n            days_ago_confirmed_count_10=days_ago_thresold_hit(confirmed, i, day)               \n            row_features+=[days_ago_confirmed_count_10] \n        #####################ma_rate_confirmed       \n        #####################std_rate_confirmed \n        for window in windows:\n            ma3_rate_confirmed,std3_rate_confirmed= generate_ma_std_window(rate_confirmed, i, size=size, window=window)\n            row_features+= ma3_rate_confirmed   \n            row_features+= std3_rate_confirmed          \n            ewma3_rate_confirmed=generate_ewma_window(rate_confirmed, i, size=size, window=window, alpha=0.05)\n            row_features+= ewma3_rate_confirmed              \n        #####################lag_fatalities_rate   \n        lag_fatalities_rate=get_lags(rate_fatalities, i, size=size)\n        row_features+=lag_fatalities_rate\n        #####################days_ago_confirmed_count_10\n        for day in days_back_fatalities:\n            days_ago_fatalitiescount_2=days_ago_thresold_hit(fatalities, i, day)               \n            row_features+=[days_ago_fatalitiescount_2]     \n        #####################ma_rate_fatalities       \n        #####################std_rate_fatalities \n        for window in windows:        \n            ma3_rate_fatalities,std3_rate_fatalities= generate_ma_std_window(rate_fatalities, i, size=size, window=window)\n            row_features+= ma3_rate_fatalities   \n            row_features+= std3_rate_fatalities  \n            ewma3_rate_fatalities=generate_ewma_window(rate_fatalities, i, size=size, window=window, alpha=0.05)\n            row_features+= ewma3_rate_fatalities                  \n        ##################confirmed_level\n        confirmed_level=0\n        \n        \"\"\"\n        if confirmed[i]>0 and confirmed[i]<1000:\n            confirmed_level= confirmed[i]\n        else :\n            confirmed_level=2000\n        \"\"\"   \n        confirmed_level= confirmed[i]\n        row_features+=[confirmed_level]\n        ##################fatalities_is_level\n        fatalities_is_level=0\n        \"\"\"\n        if fatalities[i]>0 and fatalities[i]<100:\n            fatalities_is_level= fatalities[i]\n        else :\n            fatalities_is_level=200            \n        \"\"\"\n        fatalities_is_level= fatalities[i]\n        \n        row_features+=[fatalities_is_level]              \n            \n        #######################confirmed_plus target\n        confirmed_plus=get_target(rate_confirmed, i, horizon=horizon)\n        row_features+= confirmed_plus          \n        #######################fatalities_plus target\n        fatalities_plus=get_target(rate_fatalities, i, horizon=horizon)\n        row_features+= fatalities_plus \n        ##################current_confirmed\n        #row_features+=[confirmed[i]]\n        ##################current_fatalities\n        #row_features+=[fatalities[i]]        \n        \n          \n\n        \n        features.append(row_features)\n        \n    new_frame=pd.DataFrame(data=features, columns=names).reset_index(drop=True)\n    frame=frame.reset_index(drop=True)\n    frame=pd.concat([frame, new_frame], axis=1)\n    #print(frame.shape)\n    return frame\n    \n    \ndef feature_engineering_for_single_key(frame, group, key, horizon=33, size=20, windows=[3,7], \n                                       days_back_confimed=[1,10,100], days_back_fatalities=[1,2,10]):\n    mini_frame=get_data_by_key(frame, group, key, fields=None)\n    \n    mini_frame_with_features=dereive_features(mini_frame, mini_frame[\"ConfirmedCases\"].values,\n                                              mini_frame[\"Fatalities\"].values, mini_frame[\"rate_ConfirmedCases\"].values, \n                                               mini_frame[\"rate_Fatalities\"].values, horizon ,size=size, windows=windows,\n                                              days_back_confimed=days_back_confimed, days_back_fatalities=days_back_fatalities)\n    #print (mini_frame_with_features.shape[0])\n    return mini_frame_with_features","73d05c96":"from tqdm import tqdm\ntrain_frame=[]\nsize=20\nwindows=[3,5,7]\ndays_back_confimed=[1,10,100]\ndays_back_fatalities=[1,2,10]\n#print (len(train['key'].unique()))\nfor unique_k in tqdm(unique_keys):\n    mini_frame=feature_engineering_for_single_key(train, key, unique_k, horizon=horizon, size=size, \n                                                  windows=windows, days_back_confimed=days_back_confimed,\n                                                  days_back_fatalities=days_back_fatalities).reset_index(drop=True) \n    #print (mini_frame.shape[0])\n    train_frame.append(mini_frame)\n    \ntrain_frame = pd.concat(train_frame, axis=0).reset_index(drop=True)\n#train_frame.to_csv(directory +\"all\" + \".csv\", index=False)\nnew_unique_keys=train_frame['key'].unique()\nfor kee in new_unique_keys:\n    if kee not in unique_keys:\n        print (kee , \" is not there \")\n","60cc139c":"import lightgbm as lgb\nfrom sklearn.linear_model import Ridge\nfrom sklearn.externals import joblib\n\ndef predict(xtest,input_name=None):\n   #print (type(yt))\n   # create array object to hold predictions \n  \n   baggedpred=np.array([ 0.0 for d in range(0, xtest.shape[0])]) \n   model=  joblib.load( input_name) \n   preds=model.predict(xtest)               \n   baggedpred+=preds\n\n   return baggedpred\n\nnames=[\"lag_confirmed_rate\" + str(k+1) for k in range (size)]\nfor day in days_back_confimed:\n    names+=[\"days_ago_confirmed_count_\" + str(day) ]\nfor window in windows:        \n    names+=[\"ma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]\n    names+=[\"std\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)] \n    names+=[\"ewma\" + str(window) + \"_rate_confirmed\" + str(k+1) for k in range (size)]         \n\n\nnames+=[\"lag_fatalities_rate\" + str(k+1) for k in range (size)]\nfor day in days_back_fatalities:\n    names+=[\"days_ago_fatalitiescount_\" + str(day) ]    \nfor window in windows:        \n    names+=[\"ma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]\n    names+=[\"std\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]  \n    names+=[\"ewma\" + str(window) + \"_rate_fatalities\" + str(k+1) for k in range (size)]        \nnames+=[\"confirmed_level\"]\nnames+=[\"fatalities_level\"]      \n\n#### scoring \ndef decay_4_first_10_then_1_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        if j<10:\n            arr[j]=1. + (max(1,array[j])-1.)\/4.\n        else :\n            arr[j]=1.\n    return arr\n            \ndef decay_2_f(array):\n    arr=[1.0 for k in range(len(array))]    \n    for j in range(len(array)):\n            arr[j]=1. + (max(1,array[j])-1.)\/2.\n    return arr \n\ndef acceleratorx2_f(array):\n    arr=[1.0 for k in range(len(array))]    \n    for j in range(len(array)):\n            arr[j]=1. + (max(1,array[j])-1.)*2.\n    return arr \n\n\n\ndef decay_1_5_f(array):\n    arr=[1.0 for k in range(len(array))]    \n    for j in range(len(array)):\n            arr[j]=1. + (max(1,array[j])-1.)\/1.5\n    return arr            \n         \n         \ndef stay_same_f(array):\n    arr=[1.0 for k in range(len(array))]      \n    for j in range(len(array)):\n        arr[j]=1.\n    return arr   \n\ndef decay_2_last_12_linear_inter_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        arr[j]=1. + (max(1,array[j])-1.)\/2.\n    arr12= (max(1,arr[-12])-1.)\/12. \n\n    for j in range(0, 12):\n        arr[len(arr)-12 +j]= max(1, 1 + ( (arr12*12) - (j+1)*arr12 ))\n    return arr\n\ndef linear_last_12_f(array):\n    arr=[1.0 for k in range(len(array))]\n    for j in range(len(array)):\n        arr[j]=max(1,array[j])\n    arr12= (max(1,arr[-12])-1.)\/12. \n    \n    for j in range(0, 12):\n        arr[len(arr)-12 +j]= max(1, 1 + ( (arr12*12) - (j+1)*arr12 ))\n    return arr\n    \ndecay_4_first_10_then_1 =[\"Beijing_China\",\"Fujian_China\",\"Guangdong_China\", \"Hong Kong_China\",\n\"Inner Mongolia_China\",\"Jiangsu_China\",\"Liaoning_China\",\"Macau_China\",\"Shandong_China\",\"Tianjin_China\",\n\"Yunnan_China\",\"Zhejiang_China\",\"Northern Territory_Australia\",\n\"nan_Belize\",\"nan_Benin\",\"nan_Bhutan\",\"nan_Seychelles\",\"nan_Cabo Verde\"]\n\ndecay_2 =[\"Shanghai_China\" , \"nan_Afghanistan\",\"nan_Andorra\",\"Australian Capital Territory_Australia\",\n\"South Australia_Australia\",\"Tasmania_Australia\",\"nan_Bahrain\",\"nan_Belarus\"\n\"nan_Belgium\",\"nan_Bolivia\",\"Manitoba_Canada\",\"New Brunswick_Canada\",\"Newfoundland and Labrador_Canada\",\n\"Saskatchewan_Canada\",\"nan_Central African Republic\",\"nan_Congo (Kinshasa)\",\"nan_Cote d'Ivoire\",\"Mayotte_France\",\"nan_Ukraine\"]\n\nstay_same=[\"China\", \"nan_Antigua and Barbuda\",\"nan_Diamond Princess\",\"nan_Saint Vincent and the Grenadines\",\n           \"nan_Timor-Leste\",\"Montserrat_United Kingdom\"]\n\ndecay_2_last_12_linear_inter =[\"nan_Angola\" , \"nan_Barbados\" ,\"Prince Edward Island_Canada\",\"nan_Chad\",\n\"nan_Congo (Brazzaville)\",\"Greenland_Denmark\",\"nan_Djibouti\",\"nan_Dominica\",\"nan_El Salvador\",\n\"nan_Eritrea\",\"nan_Eswatini\",\"nan_Fiji\",\"French Guiana_France\",\"French Polynesia_France\",\"New Caledonia_France\",\n\"Saint Barthelemy_France\",\"St Martin_France\",\"nan_Gabon\",\"nan_Gambia\",\"nan_Grenada\",\"nan_Guinea-Bissau\",\n\"nan_Guyana\",\"nan_Haiti\",\"nan_Holy See\",\"nan_Kyrgyzstan\",\"nan_Laos\",\"nan_Libya\",\"nan_Madagascar\",\n\"nan_Maldives\",\"nan_Mali\",\"nan_Mauritania\",\"nan_Mauritius\",\"nan_Mozambique\",\"nan_Nepal\",\n\"Aruba_Netherlands\",\"Curacao_Netherlands\",\"Sint Maarten_Netherlands\",\"nan_Nicaragua\",\"nan_Niger\",\"nan_Papua New Guinea\",\n\"nan_Saint Kitts and Nevis\",\"nan_Saint Lucia\",\"nan_Somalia\",\"nan_Sudan\",\"nan_Suriname\",\"nan_Syria\",\"nan_Tanzania\",\n\"nan_Togo\",\"Virgin Islands_US\",\"Bermuda_United Kingdom\",\"Cayman Islands_United Kingdom\",\"Channel Islands_United Kingdom\",\n\"Gibraltar_United Kingdom\",\"Isle of Man_United Kingdom\",\"nan_Zimbabwe\",\"nan_Bahamas\",\"nan_Zambia\"]\n\nacceleratorx2=[\"nan_Kenya\",\"nan_Moldova\"]\n\ndecay_1_5 =[\"nan_Kazakhstan\",\"nan_Tunisia\", \"Alabama_US\", \"Alaska_US\",\n\t\"Arizona_US\",\"Colorado_US\",\"Florida_US\",\"Montana_US\",\"Nebraska_US\",\"Nevada_US\",\"New Hampshire_US\",\"New Mexico_US\",\n\t\"Puerto Rico_US\",\"nan_Uzbekistan\",\"nan_Azerbaijan\",\"nan_Bangladesh\",\"nan_Bosnia and Herzegovina\",\n\t\"nan_Cameroon\",\"nan_Cuba\",\"nan_Guatemala\",\"nan_Jamaica\",\"nan_Morocco\",\"nan_New Zealand\",\"nan_Philippines\",\"nan_Romania\",\n\t\"nan_Trinidad and Tobago\"]\n\nlinear_last_12=[\"nan_Uganda\",\"nan_Equatorial Guinea\",\"nan_Guinea\",\"nan_Honduras\",\"nan_Liberia\",\"nan_Mongolia\",\"nan_Namibia\"]\n\nstay_same=[ \"nan_Antigua and Barbuda\",\"nan_Diamond Princess\",\"nan_Saint Vincent and the Grenadines\",\"nan_Timor-Leste\",\"Montserrat_United Kingdom\"]\n\n#\"China\",\n\ntr_frame=train_frame\n\nfeatures_train=tr_frame[names].values   \n\nstandard_confirmed_train=tr_frame[\"ConfirmedCases\"].values\nstandard_fatalities_train=tr_frame[\"Fatalities\"].values\ncurrent_confirmed_train=tr_frame[\"ConfirmedCases\"].values\n\n     \n\nfeatures_cv=[]\nname_cv=[]\nstandard_confirmed_cv=[]\nstandard_fatalities_cv=[]\nnames_=tr_frame[\"key\"].values\ntraining_horizon=int(features_train.shape[0]\/len(unique_keys)) \nprint(\"training horizon = \",training_horizon)\nfor dd in range(training_horizon-1,features_train.shape[0],training_horizon):\n    features_cv.append(features_train[dd])\n    name_cv.append(names_[dd])\n    standard_confirmed_cv.append(standard_confirmed_train[dd])\n    standard_fatalities_cv.append(standard_fatalities_train[dd])\n    print (name_cv[-1], standard_confirmed_cv[-1], standard_fatalities_cv[-1])\n    \n \n\nfeatures_cv=np.array(features_cv)\npreds_confirmed_cv=np.zeros((features_cv.shape[0],horizon))\npreds_confirmed_standard_cv=np.zeros((features_cv.shape[0],horizon))\n\npreds_fatalities_cv=np.zeros((features_cv.shape[0],horizon))\npreds_fatalities_standard_cv=np.zeros((features_cv.shape[0],horizon))\n\noveral_rmsle_metric_confirmed=0.0\n\nfor j in range (preds_confirmed_cv.shape[1]):\n\n    this_features_cv=features_cv                          \n\n    preds=predict(features_cv, input_name=model_directory +\"confirmed\"+ str(j))\n    preds_confirmed_cv[:,j]=preds\n    print (\" modelling confirmed, case %d, , original cv %d and after %d \"%(j,this_features_cv.shape[0],this_features_cv.shape[0])) \n\npredictions=[] \nfor ii in range (preds_confirmed_cv.shape[0]):\n    current_prediction=standard_confirmed_cv[ii]\n    if current_prediction==0 :\n        current_prediction=0.1   \n    this_preds=preds_confirmed_cv[ii].tolist()\n    name=name_cv[ii]\n    #overrides\n    if name in decay_4_first_10_then_1:\n        this_preds=decay_4_first_10_then_1_f(this_preds)\n        \n    elif name in decay_2:\n        this_preds=decay_2_f(this_preds)\n        \n    elif name in decay_2_last_12_linear_inter:\n        this_preds=decay_2_last_12_linear_inter_f(this_preds)\n        \n    elif name in decay_1_5:\n        this_preds=decay_1_5_f(this_preds)        \n        \n    elif name in linear_last_12:\n        this_preds=linear_last_12_f(this_preds)\n        \n    elif name in acceleratorx2:\n        this_preds=acceleratorx2_f(this_preds)         \n\n        \n    elif name in stay_same or  \"China\" in name:\n        this_preds=stay_same_f(this_preds)      \n\n    for j in range (preds_confirmed_cv.shape[1]):\n                current_prediction*=max(1,this_preds[j])\n                preds_confirmed_standard_cv[ii][j]=current_prediction\n\n\nfor j in range (preds_confirmed_cv.shape[1]):\n\n    this_features_cv=features_cv\n                             \n    preds=predict(features_cv, input_name=model_directory +\"fatal\"+ str(j))\n    preds_fatalities_cv[:,j]=preds\n    print (\" modelling fatalities, case %d, original cv %d and after %d \"%( j,this_features_cv.shape[0],this_features_cv.shape[0])) \n\npredictions=[]\nfor ii in range (preds_fatalities_cv.shape[0]):\n    current_prediction=standard_fatalities_cv[ii]\n    if current_prediction==0 and standard_confirmed_cv[ii]>400:\n        current_prediction=0.1\n        \n    this_preds=preds_fatalities_cv[ii].tolist()\n    name=name_cv[ii]\n    #overrides\n    if name in decay_4_first_10_then_1:\n        this_preds=decay_4_first_10_then_1_f(this_preds)\n        \n    elif name in decay_2:\n        this_preds=decay_2_f(this_preds)\n        \n    elif name in decay_2_last_12_linear_inter:\n        this_preds=decay_2_last_12_linear_inter_f(this_preds)\n        \n    elif name in decay_1_5:\n        this_preds=decay_1_5_f(this_preds)        \n        \n    elif name in linear_last_12:\n        this_preds=linear_last_12_f(this_preds) \n        \n    elif name in acceleratorx2:\n        this_preds=acceleratorx2_f(this_preds)                 \n        \n    elif name in stay_same or  \"China\" in name:\n        this_preds=stay_same_f(this_preds)         \n        \n    for j in range (preds_fatalities_cv.shape[1]):\n                if current_prediction==0 and  preds_confirmed_standard_cv[ii][j]>400:\n                    current_prediction=1.\n                current_prediction*=max(1,this_preds[j])\n                preds_fatalities_standard_cv[ii][j]=current_prediction\n\n","7ae4eeac":"key_to_confirmed_rate={}\nkey_to_fatality_rate={}\nkey_to_confirmed={}\nkey_to_fatality={}\nprint(len(features_cv), len(name_cv),len(standard_confirmed_cv),len(standard_fatalities_cv)) \nprint(preds_confirmed_cv.shape,preds_confirmed_standard_cv.shape,preds_fatalities_cv.shape,preds_fatalities_standard_cv.shape) \n\nfor j in range (len(name_cv)):\n    \n    key_to_confirmed_rate[name_cv[j]]=preds_confirmed_cv[j,:].tolist()\n    #print(key_to_confirmed_rate[name_cv[j]])\n    key_to_fatality_rate[name_cv[j]]=preds_fatalities_cv[j,:].tolist()\n    key_to_confirmed[name_cv[j]]  =preds_confirmed_standard_cv[j,:].tolist()  \n    key_to_fatality[name_cv[j]]=preds_fatalities_standard_cv[j,:].tolist()  \n    \ntrain_new=train[[\"Date\",\"ConfirmedCases\",\"Fatalities\",\"key\",\"rate_ConfirmedCases\",\"rate_Fatalities\"]]\n\ntest_new=pd.merge(test,train_new, how=\"left\", left_on=[\"key\",\"Date\"], right_on=[\"key\",\"Date\"] ).reset_index(drop=True)\n\ndef fillin_columns(frame,key_column, original_name, training_horizon, test_horizon, unique_values, key_to_values):\n    keys=frame[key_column].values\n    original_values=frame[original_name].values.tolist()\n    print(len(keys), len(original_values), training_horizon ,test_horizon,len(key_to_values))\n    \n    for j in range(unique_values):\n        current_index=(j * (training_horizon +test_horizon )) +training_horizon \n        current_key=keys[current_index]\n        values=key_to_values[current_key]\n        co=0\n        for g in range(current_index, current_index + test_horizon):\n            original_values[g]=values[co]\n            co+=1\n    \n    frame[original_name]=original_values\n \n\nall_days=int(test_new.shape[0]\/len(unique_keys))\n\ntr_horizon=all_days-horizon\nprint(all_days,tr_horizon, horizon )\n\nfillin_columns(test_new,\"key\", 'ConfirmedCases', tr_horizon, horizon, len(unique_keys), key_to_confirmed)    \nfillin_columns(test_new,\"key\", 'Fatalities', tr_horizon, horizon, len(unique_keys), key_to_fatality)   \nsubmission=test_new[[\"ForecastId\",\"ConfirmedCases\",\"Fatalities\"]]\n\nsubmission.to_csv( \"submission.csv\", index=False)","dbd547be":"* xgboost doesn't work in week2 private test,change to week 2 no1 kernel,\nreference from: https:\/\/www.kaggle.com\/kazanova\/gr1621-v2\n* also, credit for Basic Model with this notebook fork from\n* if this notebook help you,pls upvote,that will really help me,thank you!\n\n* add an new kernel for BCG vaccine helping against COVID19,fyi https:\/\/www.kaggle.com\/ashora\/bcg-vaccine-is-helping-the-fight-against-covid19 "}}