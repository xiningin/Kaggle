{"cell_type":{"616d876a":"code","76cfb369":"code","a53c9a3c":"code","bd75a732":"code","94bc07db":"code","106a35a1":"code","3d939eac":"code","525c805b":"code","2d852a1b":"code","ad4567dd":"code","7cf92b31":"code","9caf8d39":"code","2f18aac1":"code","a0278432":"code","6260d49b":"code","e47465d3":"code","804ca4d5":"code","e1333075":"code","26f52c82":"code","9197f37e":"code","59cf0a59":"code","30aab0f9":"code","a0730f45":"markdown","b244f88c":"markdown","fcbd7178":"markdown","cca05aa2":"markdown"},"source":{"616d876a":"!pip install torchsummary","76cfb369":"from pathlib import Path\nimport cv2\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom tqdm.notebook import tqdm\nimport time\n\nimport torch\nfrom torch import nn\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.utils import make_grid\nfrom torchsummary import summary","a53c9a3c":"MAIN_DIR = Path('..\/input\/animal-faces\/afhq')","bd75a732":"# cats only\npaths = list(MAIN_DIR.glob('*\/cat\/*.jpg'))\nlen(paths)","94bc07db":"# all animals\npaths = list(MAIN_DIR.glob('*\/*\/*.jpg'))\nlen(paths)","106a35a1":"class AnimalDataset(Dataset):\n    def __init__(self, paths, transforms = transforms.ToTensor()):\n        super().__init__()\n        self.paths = paths\n        self.transforms = transforms\n        \n    def __getitem__(self, index):\n        image_path = str(self.paths[index])\n        image = cv2.imread(image_path, cv2.IMREAD_COLOR)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB).astype(np.float32)\n        image \/= 255.0\n        image = self.transforms(image)\n        return image\n        \n    def __len__(self):\n        return len(self.paths)","3d939eac":"BATCH_SIZE = 64\nSIZE = 128\n\ntransform=transforms.Compose([\n    transforms.ToTensor(),\n    transforms.Resize(SIZE),\n    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n])","525c805b":"ds = AnimalDataset(paths, transform)\ndataloader = DataLoader(ds, shuffle=True, batch_size=BATCH_SIZE, num_workers = 2)","2d852a1b":"for batch in dataloader:\n    fig, ax = plt.subplots(figsize = (10, 10))\n    ax.set_xticks([])\n    ax.set_yticks([])\n    ax.imshow(make_grid(batch, 8, normalize=True).permute(1,2,0))\n    break","ad4567dd":"DEVICE = 'cuda'\nNOISE_DIM = 100","7cf92b31":"class Generator(nn.Module):\n    def __init__(self, in_ch, out_ch=3):\n        super(Generator, self).__init__()\n        self.in_ch = in_ch\n        self.net = nn.Sequential(\n            self.make_gen_block(in_ch, 1024, stride=1, padding=0),\n            self.make_gen_block(1024, 512),\n            self.make_gen_block(512, 256),\n            self.make_gen_block(256, 128),\n            self.make_gen_block(128, 64),\n            self.make_gen_block(64, out_ch, final_layer=True),\n        )\n\n    def make_gen_block(self, in_ch, out_ch, kernel_size=4, stride=2, padding=1, final_layer=False):\n        if not final_layer:\n            return nn.Sequential(\n                nn.ConvTranspose2d(in_ch, out_ch, kernel_size, stride, padding, bias=False),\n                nn.BatchNorm2d(out_ch),\n                nn.ReLU(inplace=True)\n            )\n        else:\n            return nn.Sequential(\n                nn.ConvTranspose2d(in_ch, out_ch, kernel_size, stride, padding, bias=False),\n                nn.Tanh(),\n            )\n\n    def forward(self, noise):\n        x = noise.view(len(noise), self.in_ch, 1, 1)\n        return self.net(x)","9caf8d39":"generator = Generator(NOISE_DIM).to(DEVICE)\nsummary(generator, input_size=(1, NOISE_DIM))","2f18aac1":"class Critic(nn.Module):\n    def __init__(self, in_ch=3):\n        super(Critic, self).__init__()\n        self.crit = nn.Sequential(\n            self.make_crit_block(in_ch, 64),\n            self.make_crit_block(64, 128),\n            self.make_crit_block(128, 256),\n            self.make_crit_block(256, 512),\n            self.make_crit_block(512, 1024),\n            self.make_crit_block(1024, 1, stride=1, padding=0, final_layer=True),\n        )\n\n    def make_crit_block(self, in_ch, out_ch, kernel_size=4, stride=2, padding=1, final_layer=False):\n        if not final_layer:\n            return nn.Sequential(\n                nn.Conv2d(in_ch, out_ch, kernel_size, stride, padding),\n                nn.BatchNorm2d(out_ch),\n                nn.LeakyReLU(0.2, inplace=True),\n            )\n        else:\n            return nn.Sequential(\n                nn.Conv2d(in_ch, out_ch, kernel_size, stride),\n            )\n\n    def forward(self, image):\n        crit_pred = self.crit(image)\n        return crit_pred.view(len(crit_pred), -1)","a0278432":"critic = Critic().to(DEVICE) \nsummary(critic, input_size= (3, SIZE, SIZE))","6260d49b":"def weights_init(m):\n    if isinstance(m, nn.Conv2d) or isinstance(m, nn.ConvTranspose2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n    if isinstance(m, nn.BatchNorm2d):\n        torch.nn.init.normal_(m.weight, 0.0, 0.02)\n        torch.nn.init.constant_(m.bias, 0)\n\ngenerator.apply(weights_init)\ncritic.apply(weights_init)","e47465d3":"def gradient_penalty(critic, real, fake, device):\n    # epsilon: a vector of the uniformly random proportions of real\/fake per mixed image\n    epsilon = torch.rand(len(real), 1, 1, 1, device=device, requires_grad=True)\n\n    mixed_images = real * epsilon + fake * (1 - epsilon)\n    mixed_scores = critic(mixed_images)\n    gradient = torch.autograd.grad(inputs=mixed_images, outputs=mixed_scores,\n        grad_outputs=torch.ones_like(mixed_scores), create_graph=True, retain_graph=True)[0]\n\n    # Flatten the gradients\n    gradient = gradient.view(len(gradient), -1)\n\n    # Calculate the magnitude (norm) of each gradient\n    gradient_norm = gradient.norm(2, dim=1)\n    \n    # mean of squared distances between each magnitude and the ideal norm of 1 \n    penalty = torch.mean((gradient_norm - 1)**2)    \n    return penalty","804ca4d5":"LR = 0.0002 \ngen_optimizer = torch.optim.Adam(generator.parameters(), lr=LR)\ncritic_optimizer = torch.optim.Adam(critic.parameters(), lr=LR)","e1333075":"EPOCHS = 10\nDISPLAY_STEP = 8\nLAMBDA = 10\nCRIT_REPEATS = 5","26f52c82":"def show_images(batch, size=(3, SIZE, SIZE), num_images=16):\n    fig = plt.figure(figsize=(8,8))\n    plt.axis(\"off\")\n    image_unflat = batch.detach().cpu().view(-1, *size)\n    image_grid = make_grid(image_unflat[:num_images], nrow=4, normalize=True)\n    plt.imshow(image_grid.permute(1, 2, 0).squeeze())","9197f37e":"chckpt_path = '\/kaggle\/working\/model.pt'\ngenerator_losses = []\ncritic_losses = []\n\n#safeguard for kaggle GPU\nquota = 9*60\nmin_per_epoch = []\n\nfor epoch in range(EPOCHS):\n    start = time.time()\n    \n    #for real in tqdm(dataloader):\n    for real in dataloader:\n        cur_batch_size = len(real)\n        real = real.to(DEVICE)\n\n        \n        mean_iteration_critic_loss = 0\n        for _ in range(CRIT_REPEATS):\n            critic_optimizer.zero_grad() \n            \n            noise = torch.randn(cur_batch_size, NOISE_DIM, device=DEVICE)\n            fake = generator(noise)\n            fake_scores = critic(fake.detach())\n            real_scores = critic(real)\n    \n            gp = gradient_penalty(critic, real, fake.detach(), DEVICE)\n            critic_loss = torch.mean(fake_scores) - torch.mean(real_scores) + LAMBDA * gp\n            \n            mean_iteration_critic_loss += critic_loss.item() \/ CRIT_REPEATS\n\n            critic_loss.backward(retain_graph=True) \n            critic_optimizer.step()\n\n        critic_losses += [mean_iteration_critic_loss]\n\n        \n        gen_optimizer.zero_grad()\n        noise = torch.randn(cur_batch_size, NOISE_DIM, device=DEVICE) \n\n        fake = generator(noise)\n        fake_scores = critic(fake)\n        gen_loss = -1. * torch.mean(fake_scores)\n    \n        gen_loss.backward()\n        gen_optimizer.step()\n\n        generator_losses += [gen_loss.item()]\n        break # <-- remove this to actually train\n\n    print(f\"Epoch {epoch+1}\/{EPOCHS}: running gen loss: {sum(generator_losses)\/len(generator_losses)}, \\\n        running critic loss: {sum(critic_losses)\/len(critic_losses)}\")\n    \n    torch.save(\n        {\n            'epoch': epoch, \n            'gen_state_dict': generator.state_dict(),\n            'critic_state_dict': critic.state_dict(),\n            'gen_optimizer_state_dict': gen_optimizer.state_dict(),\n            'critic_optimizer_state_dict': critic_optimizer.state_dict(),\n        }, \n        chckpt_path)\n\n    if (epoch+1) % DISPLAY_STEP == 0:        \n        noise = torch.randn(16, NOISE_DIM, device=DEVICE) \n        fake = generator(noise)\n        show_images(fake)\n        \n    epoch_time = (time.time() - start) \/60.0\n    quota -= epoch_time\n    min_per_epoch.append(epoch_time)\n    avg = sum(min_per_epoch)\/len(min_per_epoch)\n    if quota - avg <= 0:\n        break","59cf0a59":"def plot_losses(generator_losses, critic_losses):\n    plt.figure(figsize=(10,5))\n    plt.plot(generator_losses)\n    plt.plot(critic_losses)\n    plt.xlabel('step')\n    plt.ylabel('loss')\n    plt.legend(['Generator loss', 'Critic loss'])\n    plt.title('GAN Loss')\n    plt.show()\n    \nplot_losses(generator_losses, critic_losses)","30aab0f9":"example_path = '..\/input\/wgan-gp-sample-400-epochs\/epoch400.png'\nimage = cv2.imread(example_path)\nimage = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\nplt.figure(figsize=(10,10))\nplt.axis(\"off\")\nplt.imshow(image)\nplt.show()","a0730f45":"Here is an example of generated images after 400 epochs. I chose this bunch sort of arbitrary: this kind of quality can be already observed around epoch 200 or so, and it doesn't improve much after that. However the image qulity is not really the focus here. Wasserstein loss with gradient penalty is a trick that is ment to improve stability of the training, which it did: there was no mode collapse even after 800 epochs.","b244f88c":"This GAN is computationally very expensive: you are training two networks and on top of that gradient penalty forces you to calculate the gradient of a gradient! The training is very slow. Kaggle's 9-hour GPU quota is not enough to produce images even remotely resembling animals, so I opted for training locally and then just uploading a bunch of generated images as an example. There is still a bit of code left that is ment to deal with the GPU quota, since i might reuse it later in another notebook:)","fcbd7178":"# Wasserstein GAN with Gradient Penalty","cca05aa2":"All original images have shape (512, 512, 3)"}}