{"cell_type":{"47c4e301":"code","4a41d20b":"code","0e4c86d9":"code","9398578d":"code","91effb03":"code","85d3d7b4":"code","9caf6cbe":"code","d7e82d68":"code","89fb6f2f":"code","fcdd6667":"code","0ec7a2f6":"code","ea7ae7a3":"code","c5db2a28":"code","da9122e2":"code","8c957843":"code","a3392770":"code","28e0e386":"code","ac18bd9c":"code","74bff67d":"code","7a8ef3d1":"code","09cddf42":"code","734f301b":"code","385633cd":"code","aa486dd6":"code","362158ff":"code","bdfaf1c5":"code","d10aad5d":"code","7c6055b7":"code","7f258c1f":"code","08c56ced":"code","2993a989":"code","d4708e87":"code","d67cff14":"code","e77d40bf":"code","cfa8fef1":"code","66d386fb":"code","62808cf2":"code","7e8abf25":"code","ae7c05d2":"code","f0a9d599":"code","a1804ee4":"code","1d8b283c":"code","3c5f2100":"markdown","af1bb15e":"markdown","a05b18b5":"markdown","aa9ca9e5":"markdown","fabe0989":"markdown","5e27130f":"markdown","760b55e5":"markdown","2767b774":"markdown"},"source":{"47c4e301":"import pandas as pd","4a41d20b":"df=pd.read_csv('train.csv')","0e4c86d9":"df.head()","9398578d":"import mitosheet\nmitosheet.sheet(df, view_df=True)","91effb03":"import mitosheet\nmitosheet.sheet(df, view_df=True)","85d3d7b4":"# Droping the NaN Values\ndf=df.dropna()","9caf6cbe":"# Get the independent Features\nX=df.drop('label', axis=1)","d7e82d68":"# Get the dependent features\ny=df['label']","89fb6f2f":"# How many fake news (1) and real news (0) do we have\ny.value_counts()","fcdd6667":" X.shape # 4 Columns - 18285 rows  ","0ec7a2f6":"y.shape # 1 Column (Label) - 18285 rows","ea7ae7a3":"import tensorflow as tf","c5db2a28":"tf.__version__","da9122e2":"from tensorflow.keras.layers import Embedding\nfrom tensorflow.keras.preprocessing.sequence import pad_sequences\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.preprocessing.text import one_hot # onvert each categorical value into a new categorical column and assign a binary value of 1 or 0 to those columns\nfrom tensorflow.keras.layers import LSTM\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Bidirectional","8c957843":"# Vocabulary size needed for one_hot Encoder\nvoc_size=5000","a3392770":"messages=X.copy()","28e0e386":"messages['title'][0]","ac18bd9c":"messages.reset_index(inplace=True)","74bff67d":"import nltk\nimport re\nfrom nltk.corpus import stopwords","7a8ef3d1":"nltk.download('stopwords')","09cddf42":"# Dataset Preprocessing\n\nfrom nltk.stem.porter import PorterStemmer\nps = PorterStemmer()\ncorpus = []\nfor i in range(0, len(messages)):\n    print(i)\n    review = re.sub('[^a-zA-Z]', ' ', messages['title'][i]) # delete every special character outside letters from a to z in small and big format\n    review = review.lower() # turn all the text in lower case\n    review = review.split() # divides a string into a list\n    \n    review = [ps.stem(word) for word in review if not word in stopwords.words('english')] # removing stopwords and stemming \n    review = ' '.join(review) # takes all items in an iterable and joins them into one string\n    corpus.append(review) # forming sentences again","734f301b":"# our news after Stemming Lemmatization and stop words\ncorpus[0] ","385633cd":"# Assigning an Index to each word in every Sentence. here you can see that no word will have an index greater than 5000 which is the upper limit given earlier for our vocab size\nonehot_repr=[one_hot(words,voc_size)for words in corpus]\nonehot_repr","aa486dd6":"sent_length=20\nembedded_docs=pad_sequences(onehot_repr,padding='pre',maxlen=sent_length) # pre = adding zeros at the begening in order to fully complete a sentence length of 20 each time\nprint(embedded_docs) # print all our independent features","362158ff":"embedded_docs[0]","bdfaf1c5":"# Creating the model\n\nembedding_vector_features=40 # convert the previous indexes to vectors in order to use the LSTM method\nmodel=Sequential()\nmodel.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\nmodel.add(LSTM(100)) # Unidirectional LSTM\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model.summary())","d10aad5d":"# Creating the model\n\nembedding_vector_features=40\nmodel1=Sequential()\nmodel1.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\nmodel1.add(Bidirectional(LSTM(100))) # Bidirectional LSTM\nmodel1.add(Dense(1,activation='sigmoid'))\nmodel1.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\nprint(model.summary())","7c6055b7":"len(embedded_docs),y.shape","7f258c1f":"import numpy as np\nX_final=np.array(embedded_docs)\ny_final=np.array(y)","08c56ced":"X_final.shape,y_final.shape","2993a989":"from sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X_final, y_final, test_size=0.33, random_state=42)","d4708e87":"model1.fit(X_train,y_train,validation_data=(X_test,y_test),epochs=10,batch_size=64) # we only run the training set for the Bidirectional LSTM","d67cff14":"from tensorflow.keras.layers import Dropout\n## Creating model\nembedding_vector_features=40\nmodel=Sequential()\nmodel.add(Embedding(voc_size,embedding_vector_features,input_length=sent_length))\nmodel.add(Dropout(0.3))\nmodel.add(LSTM(100))\nmodel.add(Dropout(0.3))\nmodel.add(Dense(1,activation='sigmoid'))\nmodel.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])","e77d40bf":"y_pred1=model1.predict(X_test)\ny_pred1 = np.round(y_pred1).astype(int)","cfa8fef1":"from sklearn.metrics import confusion_matrix","66d386fb":"confusion_matrix(y_test,y_pred1)","62808cf2":"from sklearn.metrics import accuracy_score\naccuracy_score(y_test,y_pred1)","7e8abf25":"from sklearn.metrics import classification_report\nprint(classification_report(y_test,y_pred1))","ae7c05d2":"df_test = pd.read_csv('test.csv')\n\ndf_test = df_test.reset_index(drop=True)\ndf_test1=np.array(embedded_docs)\n\ndf_test.head()","f0a9d599":"import mitosheet\nmitosheet.sheet(df_test, view_df=True)","a1804ee4":"y_pred2=model1.predict(df_test1)\ny_pred2 = np.round(y_pred2).astype(int)","1d8b283c":"y_pred2 = pd.DataFrame(y_pred2, columns=['lables'])\ndf_final_0 = pd.concat([df_test['id'], y_pred2], axis = 1)\ndf_final_0.to_csv('Predictions')\ndf_final_0.head(10)","3c5f2100":"## Loading the test data:","af1bb15e":"## One_hot Representation","a05b18b5":"## Embedding Representation","aa9ca9e5":"## Model Training","fabe0989":"## Performance Metrics And Accuracy","5e27130f":"## Adding Dropout","760b55e5":"## Making Predictions for test data:","2767b774":"## Joining the test data and predicted labels"}}