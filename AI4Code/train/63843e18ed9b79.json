{"cell_type":{"e2789306":"code","0ec45c88":"code","881235dc":"code","43e8e009":"code","8e4041da":"code","3d985628":"code","e4caa358":"code","f935d534":"code","c6e29ee4":"code","82f2ded3":"code","9d9b6418":"code","6ad17326":"code","a80675f1":"code","0c57ace4":"code","a933eb95":"code","97d8a766":"code","78ca2150":"code","158ee916":"code","10e128d6":"code","e2d36ae0":"code","3baa6660":"code","28c3ff90":"code","f2c1be84":"markdown","f6d126f7":"markdown","5bc78625":"markdown"},"source":{"e2789306":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib\nfrom matplotlib import pyplot as plt\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","0ec45c88":"# Load data in\nmnist = pd.read_csv(\"\/kaggle\/input\/mnist-784\/mnist_784.csv\")","881235dc":"# Peak at the data\nmnist.head()","43e8e009":"# Create predictors and labels\nX = mnist.loc[:, mnist.columns != 'class']\ny = mnist.loc[:, 'class']\nprint(X.shape)\nprint(y.shape)","8e4041da":"from sklearn.model_selection import train_test_split\n# create train test split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.15)\nprint(X_train.shape)\nprint(y_train.shape)","3d985628":"# Further exploring data: display a random digit to see what we are working with\nrandom_digit = X.loc[0,:].to_numpy()\ndisplay_digit = random_digit.reshape(28,28)\n\nplt.imshow(display_digit, cmap=\"binary\")\nplt.axis('off')\nplt.show","e4caa358":"# This looks like a 5. We can compare it to the actual label and see it is indeed a 5\ny[0]","f935d534":"# Let's peform a binary classification on whether a digit is 8 or not\n# Set all classes == 8 as True, False otherwise\ny_train_8 = (y_train == 8)\ny_test_8 = (y_test == 8)","c6e29ee4":"# we'll train on the following classifiers: SGD, Decision Tree, and RandomForest\nfrom sklearn.linear_model import SGDClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nsgd_clf = SGDClassifier(random_state=42)\ndTree_clf = DecisionTreeClassifier(random_state=42)\nranFor_clf = RandomForestClassifier(random_state=42)","82f2ded3":"'''\nWe'll perform cross validation with K=3 folds on each model. \nWe'll retrieve the predictions in order to perform model performance\nusing metrics such as precision, recall, F1, ROC, and AUC\n'''\nfrom sklearn.model_selection import cross_val_predict\ny_pred_SGD = cross_val_predict(sgd_clf, X_train, y_train_8, cv=3)\ny_pred_dTree = cross_val_predict(dTree_clf, X_train, y_train_8, cv=3)\ny_pred_ranFor = cross_val_predict(ranFor_clf, X_train, y_train_8, cv=3)","9d9b6418":"# Precision, Recall, and F1\nfrom sklearn.metrics import precision_score, recall_score, f1_score\nsgd_metrics = [precision_score(y_train_8, y_pred_SGD), recall_score(y_train_8, y_pred_SGD), f1_score(y_train_8, y_pred_SGD)]\ndTree_metrics = [precision_score(y_train_8, y_pred_dTree), recall_score(y_train_8, y_pred_dTree), f1_score(y_train_8, y_pred_dTree)]\nranFor_metrics = [precision_score(y_train_8, y_pred_ranFor), recall_score(y_train_8, y_pred_ranFor), f1_score(y_train_8, y_pred_ranFor)]\n\n# Display our results in a data frame\ndf_metrics = pd.DataFrame({'SGD_Metrics' : sgd_metrics, 'dTree_metrics' : dTree_metrics, 'ranFor_metrics' : ranFor_metrics})\ndf_metrics.index = ['Precision', 'Recall', 'F1']\ndf_metrics","6ad17326":"# Here we mess around with observing Precision and Recall percentages based on each models' thresholds\n# We can't directly change the thresholds of each model but we can use the decision scores to test prediction results on different thresholds\n\n# Get decision scores \/ prediction probabilities for each prediction\ny_score_SGD = cross_val_predict(sgd_clf, X_train, y_train_8, cv=3, method='decision_function')\ny_score_dTree = cross_val_predict(dTree_clf, X_train, y_train_8, cv=3, method='predict_proba')\ny_score_ranFor = cross_val_predict(ranFor_clf, X_train, y_train_8, cv=3, method='predict_proba')","a80675f1":"# Get predicted probabilities of positive class (the following yields same as decision score)\ny_score_dTree = y_score_dTree[:,1]\ny_score_ranFor = y_score_ranFor[:,1]","0c57ace4":"# Compute precision and recall for all possible thresholds using a precision recall curve\nfrom sklearn.metrics import precision_recall_curve\nprec_SGD, recall_SGD, threshold_SGD = precision_recall_curve(y_train_8, y_score_SGD)\nprec_dTree, recall_dTree, threshold_dTree = precision_recall_curve(y_train_8, y_score_dTree)\nprec_ranFor, recall_ranFor, threshold_ranFor = precision_recall_curve(y_train_8, y_score_ranFor)","a933eb95":"# Plot the precision recall vs threshold curve\n\n# plot function\ndef plot_PR_vs_threshold(precisions, recalls, thresholds):\n    plt.plot(thresholds, precisions[:-1], 'b--', label='Precision')\n    plt.plot(thresholds, recalls[:-1], 'g-', label='Recall')\n    plt.xlabel('Threshold')\n    plt.legend(loc='center right')\n\nplot_PR_vs_threshold(prec_SGD, recall_SGD, threshold_SGD)\nplt.show()\n\nplot_PR_vs_threshold(prec_dTree, recall_dTree, threshold_dTree)\nplt.show()\nplot_PR_vs_threshold(prec_ranFor, recall_ranFor, threshold_ranFor)\nplt.show()\n\n# The plots agree with the precision-recall trade off. That is, as precision increases, recall decreases, and vice versa","97d8a766":"# Now we focus on plotting the ROC curves and determining the AUC scores\n\n# ROC plotting function\ndef plot_ROC(fpr, tpr, label=None):\n    plt.plot(fpr, tpr, linewidth=2, label=label)\n    plt.plot([0,1], [0,1], 'k--')\n    plt.xlabel('False Positive Rate')\n    plt.ylabel('True Positive Rate (Recall)')\n    \n# Plot each ROC curve\nfrom sklearn.metrics import roc_curve, roc_auc_score\nfpr_sgd, tpr_sgd, thesholds_sgd = roc_curve(y_train_8, y_score_SGD)\nfpr_dTree, tpr_dTree, thresholds_dTree = roc_curve(y_train_8, y_score_dTree)\nfpr_ranFor, tpr_ranFor, thresholds_ranFor = roc_curve(y_train_8, y_score_ranFor)\n\nplt.plot(fpr_sgd, tpr_sgd, \"b:\", label='SGD')\nplt.plot(fpr_dTree, tpr_dTree, \"g-\", label='dTree')\nplot_ROC(fpr_ranFor, tpr_ranFor, label='RanForest')\nplt.legend(loc='lower right')\nplt.show()","78ca2150":"# So we see that the greatest AUC of each ROC is the Random Forest model. \n# That is, if randomly pick one positive data point and one negative data point, \n# our Random Forest model will rank the positive data point higher than the negative point with a higher probability than the others.\n\n# To confirm we check the values directly\nprint(f\"SGD AUC-score: {round(roc_auc_score(y_train_8, y_score_SGD), 3)}\")\nprint(f\"Decision Tree AUC-score: {round(roc_auc_score(y_train_8, y_score_dTree),3)}\")\nprint(f\"Random Forest AUC-score: {round(roc_auc_score(y_train_8, y_score_ranFor), 3)}\")","158ee916":"# We first start by finding the lowest threshold that yields us 90% precision\nthresh_90_precision = threshold_ranFor[np.argmax(prec_ranFor >= 0.90)]\nprint(f\"Lowest Threshold yielding 90% precision: {thresh_90_precision}\")","10e128d6":"# Let's now check the precision and recall scores\ny_pred_90 = (y_score_ranFor >= thresh_90_precision)\nprint(f\"Precision: {precision_score(y_train_8, y_pred_90)}\")\nprint(f\"Recall: {recall_score(y_train_8, y_pred_90)}\")","e2d36ae0":"ranFor_clf.fit(X_train, y_train_8)\ny_pred_final = ranFor_clf.predict(X_test)","3baa6660":"y_pred_score = ranFor_clf.predict_proba(X_test)\ny_pred_score = y_pred_score[:,1]","28c3ff90":"# Precision & Recall\nprecision_final = precision_score(y_test_8, y_pred_final)\nrecall_final = recall_score(y_test_8, y_pred_final)\nprint(f\"Precision Score: {precision_final} \\n Recall Score: {recall_final}\")\n\n# ROC Curve & AUC Score\nfpr, tpr, _ = roc_curve(y_test_8, y_pred_score)\nplot_ROC(fpr, tpr, label=\"Random Forest\")\nAUC_final = round(roc_auc_score(y_test_8, y_pred_score), 3)\nprint(f\"AUC score: {AUC_final}\")\n","f2c1be84":"Out of our 3 models, our best model is the Random Forest model. It out performs both SGD and Decision Tree in the following performance metrics: Precision, Recall (tied with Decision Tree), F1-Score, AUC score.\n\nSpecifically, for the random forest, out of all the digits classified as 8 in the train set, 99% of them are indeed 8's, and the model can identify an 8 81% of the train cases. \n\nFor the sake of curiousity, we can change the threshold to attempt to increase recall. This will, in turn, lower precision. Lets attempt to lower precision to 90% and see how much or recall increases","f6d126f7":"We can finish up by testing our final model on the test set. We will gather and report its performance using the same metrics as above","5bc78625":"As we see, we are successfully able to increase the recall to 94% by lowering the Random Forest's precision. Ofcourse, it all depends on the situation that will govern which metric we prefer."}}