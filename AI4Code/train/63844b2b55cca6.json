{"cell_type":{"2518eaae":"code","97a730df":"code","9cf0b60c":"code","f3340309":"code","88fd5d55":"code","371bfcdb":"code","a36caa8f":"code","e96c85ac":"code","9f469655":"code","c8c5fb9a":"code","6a6ee035":"code","7c853590":"code","a4a8381e":"code","68211744":"code","42d2da3c":"code","3aebd566":"code","e31f1c2c":"code","9cbccff0":"code","76508acc":"code","8219ced9":"code","228cd852":"code","4eff8dd3":"code","a8ba669b":"code","9a319ab3":"code","7524e8c0":"code","cd1c88c9":"code","ffc33406":"code","e97bf1ee":"code","4f809953":"code","fb934458":"code","0eaec097":"code","6fadd98a":"code","6a4d4e50":"code","115c2e16":"code","7c8e123a":"code","094ead38":"code","765f3a53":"code","5ef5e3fb":"code","5da88726":"markdown","af590d97":"markdown","844c742b":"markdown","bae35d0a":"markdown","70e928e4":"markdown","593e5cb8":"markdown","145a4723":"markdown","267159a0":"markdown","fe339b03":"markdown","62e70517":"markdown","b5dbb030":"markdown","fb0dc721":"markdown","ac060bef":"markdown","c5fc356b":"markdown","d11b1daa":"markdown","bf4e3917":"markdown","e33018c8":"markdown","c23a9f2b":"markdown","70fdfccd":"markdown","85e5f916":"markdown","e4f1ba80":"markdown","e791b79a":"markdown","5cb6db5e":"markdown","0f5ac7a5":"markdown","f6916749":"markdown","3ee7d1ef":"markdown","1fb3b147":"markdown","1678b163":"markdown","61f9f5db":"markdown","90ef214c":"markdown","50b5dba1":"markdown","c539c6e8":"markdown"},"source":{"2518eaae":"import pandas as pd\nimport numpy as np\n\nimport matplotlib.pyplot as plt \nimport cv2 as cv\n\nfrom keras.layers import Conv2D, Input, LeakyReLU, Dense, Activation, Flatten, Dropout, MaxPool2D\nfrom keras import models\nfrom keras.optimizers import Adam,RMSprop \nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.callbacks import ReduceLROnPlateau\n\nimport pickle\n\n%matplotlib inline","97a730df":"np.random.seed(1) # seed\ndf_train = pd.read_csv(\"..\/input\/digit-recognizer\/train.csv\") # Loading Dataset\ndf_train = df_train.iloc[np.random.permutation(len(df_train))] # Random permutaion for dataset (seed is used to resample the same permutation every time)","9cf0b60c":"df_train.head(5)","f3340309":"df_train.shape","88fd5d55":"sample_size = df_train.shape[0] # Training set size\nvalidation_size = int(df_train.shape[0]*0.1) # Validation set size \n\n# train_x and train_y\ntrain_x = np.asarray(df_train.iloc[:sample_size-validation_size,1:]).reshape([sample_size-validation_size,28,28,1]) # taking all columns expect column 0\ntrain_y = np.asarray(df_train.iloc[:sample_size-validation_size,0]).reshape([sample_size-validation_size,1]) # taking column 0\n\n# val_x and val_y\nval_x = np.asarray(df_train.iloc[sample_size-validation_size:,1:]).reshape([validation_size,28,28,1])\nval_y = np.asarray(df_train.iloc[sample_size-validation_size:,0]).reshape([validation_size,1])","371bfcdb":"train_x.shape,train_y.shape","a36caa8f":"df_test = pd.read_csv(\"..\/input\/digit-recognizer\/test.csv\")\ntest_x = np.asarray(df_test.iloc[:,:]).reshape([-1,28,28,1])","e96c85ac":"# convirting pixel values in range [0,1]\ntrain_x = train_x\/255\nval_x = val_x\/255\ntest_x = test_x\/255","9f469655":"# Cheacking frequency of digits in training and validation set\ncounts = df_train.iloc[:sample_size-validation_size,:].groupby('label')['label'].count()\n# df_train.head(2)\n# counts\nf = plt.figure(figsize=(10,6))\nf.add_subplot(111)\n\nplt.bar(counts.index,counts.values,width = 0.8,color=\"orange\")\nfor i in counts.index:\n    plt.text(i,counts.values[i]+50,str(counts.values[i]),horizontalalignment='center',fontsize=14)\n\nplt.tick_params(labelsize = 14)\nplt.xticks(counts.index)\nplt.xlabel(\"Digits\",fontsize=16)\nplt.ylabel(\"Frequency\",fontsize=16)\nplt.title(\"Frequency Graph training set\",fontsize=20)\nplt.savefig('digit_frequency_train.png')  \nplt.show()","c8c5fb9a":"# df_train.iloc[sample_size-validation_index:,1:]\n# Cheacking frequency of digits in training and validation set\ncounts = df_train.iloc[sample_size-validation_size:,:].groupby('label')['label'].count()\n# df_train.head(2)\n# counts\nf = plt.figure(figsize=(10,6))\nf.add_subplot(111)\n\nplt.bar(counts.index,counts.values,width = 0.8,color=\"orange\")\nfor i in counts.index:\n    plt.text(i,counts.values[i]+5,str(counts.values[i]),horizontalalignment='center',fontsize=14)\n\nplt.tick_params(labelsize = 14)\nplt.xticks(counts.index)\nplt.xlabel(\"Digits\",fontsize=16)\nplt.ylabel(\"Frequency\",fontsize=16)\nplt.title(\"Frequency Graph Validation set\",fontsize=20)\nplt.savefig('digit_frequency_val.png')\nplt.show()","6a6ee035":"rows = 5 # defining no. of rows in figure\ncols = 6 # defining no. of colums in figure\n\nf = plt.figure(figsize=(2*cols,2*rows)) # defining a figure \n\nfor i in range(rows*cols): \n    f.add_subplot(rows,cols,i+1) # adding sub plot to figure on each iteration\n    plt.imshow(train_x[i].reshape([28,28]),cmap=\"Blues\") \n    plt.axis(\"off\")\n    plt.title(str(train_y[i]), y=-0.15,color=\"green\")\nplt.savefig(\"digits.png\")","7c853590":"# # Loading pickled resources\n# !wget https:\/\/www.kaggleusercontent.com\/kf\/31703703\/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..f6l0FhjgIxCd7bfZA3MF_A.oHtR56AzfPZhslGD-R2Uca6Kce1yVCG805lWUk25H5T-MQciLjAjmoTd9RPGh1zdUlwtbMZPuAi9_1BrHNZfFlJ5duoYajHON-Sk_mMy7OIePjqNRqo8vkEnTHmbV6Oj1Z6MR9dGzT2Gch2soeaLaZnIjxJt5e8DsFaia6dTjxRzzzrKaQDWLikdsjo2xwbQp9yo4-8htw6adclSbtnXsMV4kJNBs25d-qqRLuUSuhqxxCbJMkuuZgPjnEzqO7aLU0zqcGYUXDDdx1O-oU2ncMAYpXYqssqzQgD6-t4Fl83XWQnNqRv5wec5cdD-7IF9cbjyD_CE-Ib863pPJ9RJc-IYypbUvvKfMQuhahe9NiuRGNSNodVlSiuSzk0nudl5uHqf7V7_1h_juPPVj8mUUOqleLye9_ZtJ2S8pD6hUXT9p7kPy6v6RdoaE_LgkrijyvmJhmS-yMETpazrlQlKp96A3W0EVdhtVxmW7QUwbjIlzdEs7whAe4EcqQIzd4H69TR6hLCqVlaZkMBBPvBWr_dCTxu6htDP8qE2QCH08H1VPXyZLERTMH1SRENnwa_BxMTVkc_pP70tkvGA2xtgoJHzAlcOZZfqsa5fmCa8tqIOkME1hn88Xgm5eh58JXT2ZXyA7hxfpzKP3UQXegeBnPaEPOLa5eEoND9E9ypyi5I1QWa7QTMe9ruEbXr6DUbJ.GvQemyhjKibaOnfNrnRUXQ\/model.h5\n# !wget https:\/\/www.kaggleusercontent.com\/kf\/31703703\/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..yUikFJ_W78H0Zfo7LsQ-2g.1-XOTVZwAUnN9uzbSyaPEuXbdS_5mlLvYRUOaFjMZewxYpcCFASz2fROWEIHjC-KHCOOO-DYjLY7pWHvN81cOW25n28C2mT6aFnWkObrkUYXlbZW0sS7iaKwxUw4a_XwYYUmMfeNuOpKf5OsBdv-L2y0GOLb-fT8hDqteihh6qP97VOT1fmvdv2gYcG0WqKw4mcWwtWAYyqQAScE_knALnyTvWZTF73LR99gaxy_w41t3PHo0rfHy037Cll0lWznkf6ppfAIv_CKr_v2HfpB9go0DJbXKqD87LkcnNF94e_b95i0UYfS2pMugXS4ob1WnSblE34Df_n9rB3pWMXn9DPnqlwDqa1snJOc3CLeK9MTsPQ9NuzEFIjxMnWXQhNGQ7sXYX8qAPqanQ3-OQSZHHD8lufpcJEguVqOuOWY5qZlKe4ZTzn3g8fhbssulphnajM4ZThC5pceWnVT2rowvERPtlvijy6AdAGEy57BxN7FeweBVZAR1Zv5RA_wE2qo4DFmyK36j8p_bVZHTnVvqFFWz0SAimJqzMmzxNrZY7_NGuqDy5rjUmoa2y0e0-qPFBcfFWvT2Pe_RKW21dBOKH9HbI1j0WS_Ua72FI1-MToX8DUHRdN9UNLJhnugwWY_lWvppU-fRCJFhkSPIYyfzbx1cH2ykLLZRz4-sjCegE78I03ue3096Q7kUMw9-ZGd.a3Ge-GJWsUMNPuyQ0gpMfg\/model_img_augmentation.h5\n# !wget https:\/\/www.kaggleusercontent.com\/kf\/31703703\/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..Hq9sgF7KDfRSLRlO3zlQSg.dQKW1YCUjhwAZbX-54g1ygj0qTWSH6egywwOdIqm-hwUxTWnj6L9EkNK8qncdH4FeNbEtAjYrQ2oStYGIVVJzsqFDQwaXBsnzvKQx2icckW3ww-aNtDxATdkVY2ZiqMLAMIMyy8_nFoFvt2tQm48XeIUecn3OTL6Te3VfRr3OXQ1QWU7lbY-8BetDVy0tbrLV_vIkV-fUy_FLGsa7QgH4Xerxi7xXoItAmJCbEIXBt5pR-_frNz0rdDoj30e-xGdUkLRiBNe1Nk9_1EFacGwTwhM3KqE8SDF5CtVqP7XsFhFIxVal-lmUy1hzsPi0xZt_ikRIbmfTb6K5HmQ97Jzm4nd7YdQ_u2ScbhLLFy2pj0n4XapGZgMYE5o_2_Cv2c_3uquTGDTpjiOAA25ylrFypwZGmenAeCSrJZIto0ta_onqVPz_euNQY4PHJ5P3aJi2KWHJzNl3LxBU2u-LfkfOzwYXu1DIOOqkzSSLWraxZoleean-9oawdqq0doQhogfx2wBr844ApCpVYDxEEn8CxAwfK-RoiE7MdEasDYtWujlmcOnMQsQjUohnJcOWwQSIOIEpqevcXBU4PPoiw-ex-vEUtflupnNRtlwwLRzjyrV_6VJbdQkus1F0HfToHEBjB6tkZyydreIK3Zi8u0l1YvT3VRtWMw3amc3s7f86ilZZ_uoGhW8fc7k14JmubAX.FiOfKdbaYf0TStGFpmhDSw\/history_1.hs\n# !wget https:\/\/www.kaggleusercontent.com\/kf\/31703703\/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..IWrH9XKSk0oDOCNg7wTlbg.A49rZubI479_e2da4l7JhZBGm7YdJ87UviDnyAP6924Kez0dRIPjzw-67wCI_iMtJGQQJrnn74pVwzt0cPa8CVTJIApHs2t4qZd-7dlWDcOqcwlC8zaLECXT836Jey6auUY1JE1C8YApSHBfDr-WB_KO75JQRzDdXV6LFEdPfzTyoXWf0zmuyg4bi41rPhH2UkhEfPowmze9G_nOWg64WHfJTBCcWzMDf56GbyNmp46RtdKuqriMH0sAHHFrz94DQkHnnz0U149yYC6oUkqlvty-l7jvEGM8Nc1_-LZbptB_8cZxu9gOFYprjdUyAerb5Izz-J1nkextV-0OUe8SUYj8XwG5pF6pNDbZrkoDTurkkORlMIEAKNhl6SZvPjmb1Mv-owxnrpSdHnHejp61kFp-FOi_oFFzAJiW5NVm8pmMln7RP3JgcuCfWKQUEHlrsC5NaMvsFG9CNlXeul9E4PXS6ycuRzflz5uWxvoe1Dt-Tm4Pnoaa-CvJ2hsLKsX-oH66LE6sWQm8UrDTJ0eBxaWG8QAbJ6WeOICv9zykRx8ro5j18RWgnOLK2x9PLJmYA7rL6Zm7kxavLQMTovSg-IZ9EyuTuSmGv-0Rd_0Gmb5-2qvBsYKGcn8lbJMmOXfeDWEqTT0XBK-VfVcDNxX8EZd2iYD1c3nJa6R324v4JGTOLHZJbL7UNtGXtQn-3WVc.ShxYOf_zson_2CAlaDOT8w\/history_2.hs\n!wget https:\/\/www.kaggleusercontent.com\/kf\/32045042\/eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..LcCpstRicdTOqMGGjh1wmg.LJi-FKBMbLQL5cc2XHtVyNpT--iVgBeKlbby9C_YjBjm7dSLbbqY4F27EpcgOWZYo22EOWWwyVMAd7VsYOzQkUGK-sj6gvWswv9CTc8I8ZDMkZvGdRRy3COevzGk21yXTlbRG1D--BVHlwCZVaYSMBKt9gIeCELgl-ZcFKoOjZSUpcZXBrmfuBA_OK63fc_olldErb5p5S_qWVecdJ1r49anGVs-x682Q0y5cCs9lr7q1n2B2saWkiC4d8tMGRMyUTCt1xXl6nENaYNh4C4u0DGOBfT14jEuWfyzFH3obgs_TtD4Bqg2zzxjjUwtJd7ymBNCVq7td-_chLlds-lps5uq4BLckniKl9QgbJ7ZcL6qoMonLzPn5VUE-vlIfUCfBieYOHKkWb3Buz557IrG4mk9GqAvOPp_Vzd5n9h9SGEvOFCsiG-_IinSIA7a2NxSPvtNEKmYcG37CS0xLfEV8copUh3pIHdVVLn9SXd2r5Wg2Q5q8zUnEvnHSxcPNg7LheviS4NMX18bsL9Wqm2PP_WDd7QkO_wJ0AsGyDveLWKebHB26wS_iz3w7X-EAheRmBrCyiUfJWHSM-IJPAJk9EIhRH1zr8WVtwsc19PRevFcY4RDHx7fyvPFVrJ2fFsicZMaDfJchFR_lP_uYDs6NvPu5AXXEkXnFMsrJ0IAICiEDwadnj0eV5w5b-DozhrP.8pmfOFY56dYt_Z2UVg-UWA\/model_img_augmentation.h5","a4a8381e":"model = models.Sequential()","68211744":"# Block 1\nmodel.add(Conv2D(32,3, padding  =\"same\",input_shape=(28,28,1)))\nmodel.add(LeakyReLU())\nmodel.add(Conv2D(32,3, padding  =\"same\"))\nmodel.add(LeakyReLU())\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\n# Block 2\nmodel.add(Conv2D(64,3, padding  =\"same\"))\nmodel.add(LeakyReLU())\nmodel.add(Conv2D(64,3, padding  =\"same\"))\nmodel.add(LeakyReLU())\nmodel.add(MaxPool2D(pool_size=(2,2)))\nmodel.add(Dropout(0.25))\n\nmodel.add(Flatten())\n\nmodel.add(Dense(256,activation='relu'))\nmodel.add(Dense(32,activation='relu'))\nmodel.add(Dense(10,activation=\"sigmoid\"))","42d2da3c":"initial_lr = 0.001\nloss = \"sparse_categorical_crossentropy\"\nmodel.compile(Adam(lr=initial_lr), loss=loss ,metrics=['accuracy'])\nmodel.summary()","3aebd566":"epochs = 20\nbatch_size = 256\nhistory_1 = model.fit(train_x,train_y,batch_size=batch_size,epochs=epochs,validation_data=[val_x,val_y])","e31f1c2c":"model.save(\"model.h5\")\nwith open('history_1.hs', 'wb') as history:\n    pickle.dump(history_1,history)","9cbccff0":"# model = models.load_model(\"model.h5\")\n# with open('history_1.hs', 'rb') as history:\n#     history_1 = pickle.load(history)","76508acc":"# Diffining Figure\nf = plt.figure(figsize=(20,7))\n\n#Adding Subplot 1 (For Accuracy)\nf.add_subplot(121)\n\nplt.plot(history_1.epoch,history_1.history['accuracy'],label = \"accuracy\") # Accuracy curve for training set\nplt.plot(history_1.epoch,history_1.history['val_accuracy'],label = \"val_accuracy\") # Accuracy curve for validation set\n\nplt.title(\"Accuracy Curve\",fontsize=18)\nplt.xlabel(\"Epochs\",fontsize=15)\nplt.ylabel(\"Accuracy\",fontsize=15)\nplt.grid(alpha=0.3)\nplt.legend()\n\n#Adding Subplot 1 (For Loss)\nf.add_subplot(122)\n\nplt.plot(history_1.epoch,history_1.history['loss'],label=\"loss\") # Loss curve for training set\nplt.plot(history_1.epoch,history_1.history['val_loss'],label=\"val_loss\") # Loss curve for validation set\n\nplt.title(\"Loss Curve\",fontsize=18)\nplt.xlabel(\"Epochs\",fontsize=15)\nplt.ylabel(\"Loss\",fontsize=15)\nplt.grid(alpha=0.3)\nplt.legend()\n\nplt.show()","8219ced9":"val_p = np.argmax(model.predict(val_x),axis =1)\n\nerror = 0\nconfusion_matrix = np.zeros([10,10])\nfor i in range(val_x.shape[0]):\n    confusion_matrix[val_y[i],val_p[i]] += 1\n    if val_y[i]!=val_p[i]:\n        error +=1\n        \nprint(\"Confusion Matrix: \\n\\n\" ,confusion_matrix)\nprint(\"\\nErrors in validation set: \" ,error)\nprint(\"\\nError Persentage : \" ,(error*100)\/val_p.shape[0])\nprint(\"\\nAccuracy : \" ,100-(error*100)\/val_p.shape[0])\nprint(\"\\nValidation set Shape :\",val_p.shape[0])","228cd852":"f = plt.figure(figsize=(10,8.5))\nf.add_subplot(111)\n\nplt.imshow(np.log2(confusion_matrix+1),cmap=\"Reds\")\nplt.colorbar()\nplt.tick_params(size=5,color=\"white\")\nplt.xticks(np.arange(0,10),np.arange(0,10))\nplt.yticks(np.arange(0,10),np.arange(0,10))\n\nthreshold = confusion_matrix.max()\/2 \n\nfor i in range(10):\n    for j in range(10):\n        plt.text(j,i,int(confusion_matrix[i,j]),horizontalalignment=\"center\",color=\"white\" if confusion_matrix[i, j] > threshold else \"black\")\n        \nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.savefig(\"Confusion_matrix1.png\")\nplt.show()","4eff8dd3":"datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        rotation_range=10,  # randomly rotate images in the range (degrees, 0 to 180)\n        zoom_range = 0.1, # Randomly zoom image \n        width_shift_range=0.1,  # randomly shift images horizontally (fraction of total width)\n        height_shift_range=0.1,  # randomly shift images vertically (fraction of total height)\n        horizontal_flip=False,  # randomly flip images\n        vertical_flip=False)  # randomly flip images\n\ndatagen.fit(train_x)","a8ba669b":"lrr = ReduceLROnPlateau(monitor='val_accuracy',patience=2,verbose=1,factor=0.5, min_lr=0.00001)","9a319ab3":"epochs = 20\nhistory_2 = model.fit_generator(datagen.flow(train_x,train_y, batch_size=batch_size),steps_per_epoch=int(train_x.shape[0]\/batch_size)+1,epochs=epochs,validation_data=[val_x,val_y],callbacks=[lrr])","7524e8c0":"model.save(\"model_img_augmentation.h5\")\nwith open('history_2.hs', 'wb') as history:\n    pickle.dump(history_2,history)","cd1c88c9":"model = models.load_model(\"model_img_augmentation.h5\")\n# with open('history_2.hs', 'rb') as history:\n#     history_2 = pickle.load(history)","ffc33406":"# Diffining Figure\nf = plt.figure(figsize=(20,7))\nf.add_subplot(121)\n\n#Adding Subplot 1 (For Accuracy)\nplt.plot(history_1.epoch+list(np.asarray(history_2.epoch) + len(history_1.epoch)),history_1.history['accuracy']+history_2.history['accuracy'],label = \"accuracy\") # Accuracy curve for training set\nplt.plot(history_1.epoch+list(np.asarray(history_2.epoch) + len(history_1.epoch)),history_1.history['val_accuracy']+history_2.history['val_accuracy'],label = \"val_accuracy\") # Accuracy curve for validation set\n\nplt.title(\"Accuracy Curve\",fontsize=18)\nplt.xlabel(\"Epochs\",fontsize=15)\nplt.ylabel(\"Accuracy\",fontsize=15)\nplt.grid(alpha=0.3)\nplt.legend()\n\n\n#Adding Subplot 1 (For Loss)\nf.add_subplot(122)\n\nplt.plot(history_1.epoch+list(np.asarray(history_2.epoch) + len(history_1.epoch)),history_1.history['loss']+history_2.history['loss'],label=\"loss\") # Loss curve for training set\nplt.plot(history_1.epoch+list(np.asarray(history_2.epoch) + len(history_1.epoch)),history_1.history['val_loss']+history_2.history['val_loss'],label=\"val_loss\") # Loss curve for validation set\n\nplt.title(\"Loss Curve\",fontsize=18)\nplt.xlabel(\"Epochs\",fontsize=15)\nplt.ylabel(\"Loss\",fontsize=15)\nplt.grid(alpha=0.3)\nplt.legend()\n\nplt.show()","e97bf1ee":"val_p = np.argmax(model.predict(val_x),axis =1)\n\nerror = 0\nconfusion_matrix = np.zeros([10,10])\nfor i in range(val_x.shape[0]):\n    confusion_matrix[val_y[i],val_p[i]] += 1\n    if val_y[i]!=val_p[i]:\n        error +=1\n        \nconfusion_matrix,error,(error*100)\/val_p.shape[0],100-(error*100)\/val_p.shape[0],val_p.shape[0]\n\nprint(\"Confusion Matrix: \\n\\n\" ,confusion_matrix)\nprint(\"\\nErrors in validation set: \" ,error)\nprint(\"\\nError Persentage : \" ,(error*100)\/val_p.shape[0])\nprint(\"\\nAccuracy : \" ,100-(error*100)\/val_p.shape[0])\nprint(\"\\nValidation set Shape :\",val_p.shape[0])","4f809953":"f = plt.figure(figsize=(10,8.5))\nf.add_subplot(111)\n\nplt.imshow(np.log2(confusion_matrix+1),cmap=\"Reds\")\nplt.colorbar()\nplt.tick_params(size=5,color=\"white\")\nplt.xticks(np.arange(0,10),np.arange(0,10))\nplt.yticks(np.arange(0,10),np.arange(0,10))\n\nthreshold = confusion_matrix.max()\/2 \n\nfor i in range(10):\n    for j in range(10):\n        plt.text(j,i,int(confusion_matrix[i,j]),horizontalalignment=\"center\",color=\"white\" if confusion_matrix[i, j] > threshold else \"black\")\n        \nplt.xlabel(\"Predicted\")\nplt.ylabel(\"Actual\")\nplt.title(\"Confusion Matrix\")\nplt.savefig(\"Confusion_matrix2.png\")\nplt.show()","fb934458":"rows = 4\ncols = 9\n\nf = plt.figure(figsize=(2*cols,2*rows))\nsub_plot = 1\nfor i in range(val_x.shape[0]):\n    if val_y[i]!=val_p[i]:\n        f.add_subplot(rows,cols,sub_plot) \n        sub_plot+=1\n        plt.imshow(val_x[i].reshape([28,28]),cmap=\"Blues\")\n        plt.axis(\"off\")\n        plt.title(\"T: \"+str(val_y[i])+\" P:\"+str(val_p[i]), y=-0.15,color=\"Red\")\nplt.savefig(\"error_plots.png\")\nplt.show()","0eaec097":"test_y = np.argmax(model.predict(test_x),axis =1)","6fadd98a":"rows = 5\ncols = 10\n\nf = plt.figure(figsize=(2*cols,2*rows))\n\nfor i in range(rows*cols):\n    f.add_subplot(rows,cols,i+1)\n    plt.imshow(test_x[i].reshape([28,28]),cmap=\"Blues\")\n    plt.axis(\"off\")\n    plt.title(str(test_y[i]))","6a4d4e50":"# Extracts the outputs of all layers except Flatten and Dense layers\noutput_layers = [layer.output for layer in model.layers[:-4]]\n# Creates a model that will return these outputs, given the model input (This is multi output model)\nactivation_model = models.Model(inputs=model.input, outputs=output_layers)","115c2e16":"# predicting the output of each layers\nactivations_2  = activation_model.predict(val_x[2].reshape([1,28,28,1]))\nactivations_6  = activation_model.predict(val_x[7].reshape([1,28,28,1]))\nfirst_activation_layer  = activations_2[0]\nfirst_activation_layer.shape","7c8e123a":"rows = 4\ncols = 2\n\nf = plt.figure(figsize=(2*cols,2*rows))\n\nfor i in range(4):\n    f.add_subplot(rows,cols,2*i+1)\n    plt.imshow(activations_2[0][0,:,:,i].reshape([28,28]),cmap=\"Blues\")\n    plt.axis(\"off\") \n\n    f.add_subplot(rows,cols,2*i+2)\n    plt.imshow(activations_6[0][0,:,:,i].reshape([28,28]),cmap=\"Blues\")\n    plt.savefig(\"layer_output_comparision\"+str(i)+\".png\")\n    plt.axis(\"off\")","094ead38":"def plot_layer(layer,i,layer_name = None):\n    rows = layer.shape[-1]\/16\n    cols = 16\n\n    f = plt.figure(figsize=(1*cols,1*rows))\n    # plt.imshow(first_activation_layer[0,:,:,:].reshape([14*4,14*16]),cmap=\"Blues\")\n    for i in range(layer.shape[-1]):\n        f.add_subplot(rows,cols,i+1)\n        plt.imshow(layer[0,:,:,i].reshape([layer.shape[2],layer.shape[2]]),cmap=\"Blues\")\n        plt.axis(\"off\")\n    f.suptitle(layer_name,fontsize=14)\n    plt.savefig(\"intermidiate_layers\"+str(i)+\".png\")\n    plt.show()","765f3a53":"# Visualising each layers\nfor i,layer in enumerate(activation_model.predict(val_x[6].reshape([1,28,28,1]))):\n    plot_layer(layer,i,output_layers[i].name)","5ef5e3fb":"df_submission = pd.DataFrame([df_test.index+1,test_y],[\"ImageId\",\"Label\"]).transpose()\ndf_submission.to_csv(\"submission.csv\",index=False)","5da88726":"### Further Traning\n[model.fit_generator()](https:\/\/keras.io\/models\/sequential\/#fit_generator) is used to train the model on data generated batch-by-batch by image augmentation. The data generator is an iterator that generates and provides data as per request by fit_generator(). We can configure the batch size and get the batches by calling the flow() function.\n\n","af590d97":"# Loading and Visualizing Dataset\n<hr>\n\n## About Dataset\nMNIST dataset has the following features:\n* Dataset size 60,000 samples of handwritten images.\n* The size of each image is 28x28 pixels.\n* Each image has only 1 color channel, i.e., grayscale image.\n* Each pixel has value in the range of [0,255] where 0 represents black, and 255 represents white.\n* Each image has labeled from 0-9.\n\n\n## Loading train.csv\n\nThis will loads the data from Kaggle dataset train.csv into a **Dataframe**. As file type is CSV, I am loading it using [read_csv](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.read_csv.html) of **pandas**. Then using NumPy for **random permutation** of the training dataset. I am using seed to regenerate the same permutation every time. Change seed value to get different permutation. \n<br>\n\n**Check out these functions for more info:**\n* [df.iloc[]](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.iloc.html)\n* [np.random](https:\/\/docs.scipy.org\/doc\/numpy-1.14.0\/reference\/routines.random.html)","844c742b":"## Visualize Digits dataset\n<hr>\n\nThe first and fundamental thing to check is the frequency of the classes in the dataset, as the balanced dataset is always good to start. But this is not always true, and there are several supervised learning tasks in which the classes are not balanced, also in case of anomaly detection, there is a large difference between the positive and negative class. But that is out of the scope of this notebook.\n\n### 1. Frequency plot for the training set","bae35d0a":"**Imports:**\n* pandas : For handeling csv dataset\n* numpy : Support for Pandas and calculations\n* Matplotlib - For visualization (Plotting graphs)\n*  keras - Prediction Models","70e928e4":"## Normalize Pixel Data\nEach pixel values lies between [0,255]. This value range is too high and it will be difficult for any model to learn. The best approach is **normalize** the data. In this case, as the pixel value is in the known range it sufficient to **scale the pixel values** in range [0,1] by simply dividing the array by **255**. ","593e5cb8":"**activation_2** and **activation_6** are the output of two different images. **first_activation_layer** represents the output of the first layer of the original model. The shape is (1,28,28,32) where 32 represent the number of **channels**. Let's compare the four different channels of the first layer of two different images.","145a4723":"# Improving Result by Image Augmentation\n\n<hr>\n\n![augmentation](https:\/\/cdn-images-1.medium.com\/max\/1000\/1*C8hNiOqur4OJyEZmC7OnzQ.png)\n\nA deep network requires extensive data to achieve decent performance. To build a good classifier with small training data, image augmentation can solve the problem to a greater extend. Image augmentation generates images by different ways of processing, such as random shift, rotation, flips, etc. \n<br>\n\nYou can check great python package for data [augmentation](https:\/\/github.com\/albumentations-team\/albumentations).\n\n![Augmentation](https:\/\/camo.githubusercontent.com\/43d652646b37ef66762212c0e0d3150ba481347c\/68747470733a2f2f686162726173746f726167652e6f72672f776562742f73752f77612f6e702f737577616e70656f36777737777077746f6274727a645f636732302e6a706567)*Image [source](https:\/\/github.com\/albumentations-team\/albumentations)* \n\n\n### Augmentation Using Keras\n\n<hr>\n\nHere I am using the ImageDataGenerator() function of Keras for Image augmentation. Parameters to use:\n\n* **rotation_range:**   randomly rotate images in the range (degrees, 0 to 180)\n* **zoom_range:**  Randomly zoom image \n* **width_shift_range:**  randomly shift images horizontally (fraction of total width)\n* **height_shift_range:**   randomly shift images vertically (fraction of total height)\n* **horizontal_flip:**   randomly flip images (Can't be used in this case as it changes the digit)\n* **vertical_flip:**  randomly flip images (Can't be used in this case as it changes the digit)\n\nRead more about [ImageDataGenerator()](https:\/\/keras.io\/preprocessing\/image\/)\n\nAfter the creation and configuration of the ImageDataGenerator, you must fit it on the data, which calculates any statistics required to perform the transformation on the data. This can be done by calling the fit() function on datagen.","267159a0":"# Lets understand the intermediate layers of the model\n\n<hr>\n\n![hidden layers](https:\/\/i.stack.imgur.com\/axn7z.jpg)\n\nTo visualize the output of each layer, we need to create a model to take input tensor and gives the list of output tensor, each representing the corresponding intermediate layers. To that, we need to create a multi-output model in which the input will be the image, and the output will be the list of intermediate layers.\n\n<br>\n\nI will use the Functional API of Keras to do so. When fed an image input, this model returns the values of the layer activations in the original model. \n<br>\nI am taking all the intermediate layers except the Flatten and Dense layers. \n<br>\n\n**model.layers** returns the list of layers of the model. Selecting all the layers except the last four layers. Then passing to **models.Model()** as a list of output layers and the input layer of the original model. It will return a new model having the input and output layers of the original model. \n\n\n\n**Check out these functions for more info:**\n* [Model()](https:\/\/keras.io\/models\/model\/)","fe339b03":"So, try to understand what's happening inside the hidden layers:\n* The first few layers retaining the shape of image but trying to get the very low-level features such as different types edges.\n* As we go into the deeper layers, the activations are more abstract and less visually interpretable. These layers are trying to encode high-level features such as corners, angles, selective borders, etc.\n* In the last few layers, We can't visually interpret anything; this is because layers are now encoding even more complex features, or we can say more information about the classes.\n\nI hope you understand the basics of CNN after the visualization of intermediate layers even more.","62e70517":"## Try to compare the layer 1 output \n<br>","b5dbb030":"## Confusion Matrix","fb0dc721":"# Required Imports","ac060bef":"### 2. Frequency plot for the validation set","c5fc356b":"Training set has 42,000 images. And has 785 columns, 1st coloumn is label for the image and rest 784 are the pixel values. <br>**Remember it is flattened. I will reshape it latter.**","d11b1daa":"# Visualizing Result\n\n<hr>\n\n### All Errors in the Validation set\nLet's see all the errors in the validation set. It seems that in most of the cases, the recognition of digits is difficult for even humans. So we can say that our model is performing well.","bf4e3917":"### Training Performance.\nLet's see how the training goes\u2014plotting the accuracy and loss of both training and validation set with each epoch. In the accuracy graphs, there is clearly a difference in the training and validation set. The model is more accurate on the training set. It seems that the model is a little bit **overfit**. So can we do better?","e33018c8":"### Training \n[model.fit()](https:\/\/keras.io\/models\/sequential\/#fit) is used to train the model. It takes training data, batch_size, no of epochs, validation data. There are several more parameters, and you can check the documentation [here](https:\/\/keras.io\/models\/sequential\/#fit).\nI am taking the epochs = 20 and batch size = 256. It will return the history of training, which later can be used to analyze the performance.","c23a9f2b":"### Model Using Keras\n\nThere are two different ways of defining the Model in Keras:\n* Sequential Model\n* Function API\n\nFunctional API is used to build a more complicated Model such as for multi-output Models, directed acyclic graphs, or models with shared layers. I am using the Sequential Model in this notebook to keep things simple. \n<br>\nIn Sequential Model, you can add each layer sequentially.<br>\n\n**Description of Model:**\n* 2 Convolutional Blocks\n    > Each block consists of 2 Conv2D layers with LeakyRelU activation layers. Then a MaxPool2D layer and finally a Dropout Layer. \n* Then Dense Layers and Output layer after Flatten layer.\n* MaxPool2D layer is used to reduce the size of the image. Pool size `(2,2)` means reducing the image from `(28,28)` to `(14,14)`. Reducing the features.\n* Dropout layer drops the few activation nodes while training, which acts as regularization. Do let the model to over-fit.\n* Output layer has 10 nodes with sigmoid activation.\n\n**Check out these functions for more info:**\n* [Conv2D](https:\/\/keras.io\/layers\/convolutional\/#conv2d)\n* [LeakyReLU](https:\/\/keras.io\/layers\/advanced-activations\/#leakyrelu)\n* [MaxPool2D](https:\/\/keras.io\/layers\/pooling\/#maxpool2d)\n* [Dropout](https:\/\/keras.io\/layers\/core\/#dropout)\n* [Flatten](https:\/\/keras.io\/layers\/core\/#flatten)\n* [Dense](https:\/\/keras.io\/layers\/core\/#dense)","70fdfccd":"# About Me\n<hr>\nI am Tarun Kumar from India.<br>\nSoftware Developer at Toppr.<br>\nHobbyist Artist. <br>\nPE Undergrad from IIT ISM Dhanbad. <br>\nDeep Learning, Reinforcement Learning, and Data Science. \n<br>\n<br>\n\n### Follow me:\n\n* <a href=\"https:\/\/bit.ly\/tarungithub\"><img src=\"data:image\/svg+xml;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCAyNCAyNCIgaGVpZ2h0PSI1MTIiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjUxMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMTIgLjVjLTYuNjMgMC0xMiA1LjI4LTEyIDExLjc5MiAwIDUuMjExIDMuNDM4IDkuNjMgOC4yMDUgMTEuMTg4LjYuMTExLjgyLS4yNTQuODItLjU2NyAwLS4yOC0uMDEtMS4wMjItLjAxNS0yLjAwNS0zLjMzOC43MTEtNC4wNDItMS41ODItNC4wNDItMS41ODItLjU0Ni0xLjM2MS0xLjMzNS0xLjcyNS0xLjMzNS0xLjcyNS0xLjA4Ny0uNzMxLjA4NC0uNzE2LjA4NC0uNzE2IDEuMjA1LjA4MiAxLjgzOCAxLjIxNSAxLjgzOCAxLjIxNSAxLjA3IDEuODAzIDIuODA5IDEuMjgyIDMuNDk1Ljk4MS4xMDgtLjc2My40MTctMS4yODIuNzYtMS41NzctMi42NjUtLjI5NS01LjQ2Ni0xLjMwOS01LjQ2Ni01LjgyNyAwLTEuMjg3LjQ2NS0yLjMzOSAxLjIzNS0zLjE2NC0uMTM1LS4yOTgtLjU0LTEuNDk3LjEwNS0zLjEyMSAwIDAgMS4wMDUtLjMxNiAzLjMgMS4yMDkuOTYtLjI2MiAxLjk4LS4zOTIgMy0uMzk4IDEuMDIuMDA2IDIuMDQuMTM2IDMgLjM5OCAyLjI4LTEuNTI1IDMuMjg1LTEuMjA5IDMuMjg1LTEuMjA5LjY0NSAxLjYyNC4yNCAyLjgyMy4xMiAzLjEyMS43NjUuODI1IDEuMjMgMS44NzcgMS4yMyAzLjE2NCAwIDQuNTMtMi44MDUgNS41MjctNS40NzUgNS44MTcuNDIuMzU0LjgxIDEuMDc3LjgxIDIuMTgyIDAgMS41NzgtLjAxNSAyLjg0Ni0uMDE1IDMuMjI5IDAgLjMwOS4yMS42NzguODI1LjU2IDQuODAxLTEuNTQ4IDguMjM2LTUuOTcgOC4yMzYtMTEuMTczIDAtNi41MTItNS4zNzMtMTEuNzkyLTEyLTExLjc5MnoiIGZpbGw9IiMyMTIxMjEiLz48L3N2Zz4=\" width=\"22\" align=\"left\" style=\"margin-right:10px\"\/> GitHub<\/a>\n* <a href=\"https:\/\/bit.ly\/tarnkr-youtube\"><img src=\"data:image\/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI\/Pg0KPCEtLSBHZW5lcmF0b3I6IEFkb2JlIElsbHVzdHJhdG9yIDE5LjAuMCwgU1ZHIEV4cG9ydCBQbHVnLUluIC4gU1ZHIFZlcnNpb246IDYuMDAgQnVpbGQgMCkgIC0tPg0KPHN2ZyB2ZXJzaW9uPSIxLjEiIGlkPSJMYXllcl8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCINCgkgdmlld0JveD0iMCAwIDQ2MS4wMDEgNDYxLjAwMSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDYxLjAwMSA0NjEuMDAxOyIgeG1sOnNwYWNlPSJwcmVzZXJ2ZSI+DQo8cGF0aCBzdHlsZT0iZmlsbDojRjYxQzBEOyIgZD0iTTM2NS4yNTcsNjcuMzkzSDk1Ljc0NEM0Mi44NjYsNjcuMzkzLDAsMTEwLjI1OSwwLDE2My4xMzd2MTM0LjcyOA0KCWMwLDUyLjg3OCw0Mi44NjYsOTUuNzQ0LDk1Ljc0NCw5NS43NDRoMjY5LjUxM2M1Mi44NzgsMCw5NS43NDQtNDIuODY2LDk1Ljc0NC05NS43NDRWMTYzLjEzNw0KCUM0NjEuMDAxLDExMC4yNTksNDE4LjEzNSw2Ny4zOTMsMzY1LjI1Nyw2Ny4zOTN6IE0zMDAuNTA2LDIzNy4wNTZsLTEyNi4wNiw2MC4xMjNjLTMuMzU5LDEuNjAyLTcuMjM5LTAuODQ3LTcuMjM5LTQuNTY4VjE2OC42MDcNCgljMC0zLjc3NCwzLjk4Mi02LjIyLDcuMzQ4LTQuNTE0bDEyNi4wNiw2My44ODFDMzA0LjM2MywyMjkuODczLDMwNC4yOTgsMjM1LjI0OCwzMDAuNTA2LDIzNy4wNTZ6Ii8+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8L3N2Zz4NCg==\" width=\"22\" align=\"left\" style=\"margin-right:10px\"\/>Youtube<a\/>\n* <a href=\"https:\/\/medium.com\/@codeeasy\"><img src=\"data:image\/svg+xml;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCAyNCAyNCIgaGVpZ2h0PSI1MTIiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjUxMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMjIuMDg1IDQuNzMzIDEuOTE1LTEuODMydi0uNDAxaC02LjYzNGwtNC43MjggMTEuNzY4LTUuMzc5LTExLjc2OGgtNi45NTZ2LjQwMWwyLjIzNyAyLjY5M2MuMjE4LjE5OS4zMzIuNDkuMzAzLjc4M3YxMC41ODNjLjA2OS4zODEtLjA1NS43NzMtLjMyMyAxLjA1bC0yLjUyIDMuMDU0di4zOTZoNy4xNDV2LS40MDFsLTIuNTItMy4wNDljLS4yNzMtLjI3OC0uNDAyLS42NjMtLjM0Ny0xLjA1di05LjE1NGw2LjI3MiAxMy42NTloLjcyOWw1LjM5My0xMy42NTl2MTAuODgxYzAgLjI4NyAwIC4zNDYtLjE4OC41MzRsLTEuOTQgMS44Nzd2LjQwMmg5LjQxMnYtLjQwMWwtMS44Ny0xLjgzMWMtLjE2NC0uMTI0LS4yNDktLjMzMi0uMjE0LS41MzR2LTEzLjQ2N2MtLjAzNS0uMjAzLjA0OS0uNDExLjIxMy0uNTM0eiIgZmlsbD0iIzIxMjEyMSIvPjwvc3ZnPg==\"  width=\"22\" align=\"left\" style=\"margin-right:10px\"\/> Medium<\/a>\n* <a href=\"https:\/\/www.linkedin.com\/in\/tarun-kumar-iit-ism\/\"><img src=\"data:image\/svg+xml;base64,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\" width=\"22\" align=\"left\" style=\"margin-right:10px\"\/>Linkedin<\/a>\n\n\n<hr>\n\n# Feedback\n* **Your feedback is much appreciated**\n* **Please UPVOTE if you LIKE this notebook**\n* **Comment if you have any doubts or you found any errors in the notebook**","85e5f916":"## Loading test.csv\nThis will load the test.csv. It has 18,000 images and no label is there. Prediction need to be done on these images. After loading converting the data in form of numpy array and reshaping it.","e4f1ba80":"### Learning Rate\n\n<hr>\n\nReduceLROnPlateau() is a callback function provided by Keras, which is used to reduce the learning rate if when a metric has stopped improving.\n\n<br>\n\n**Parameters:**\n\n* **monitor:**  takes the metric to observe (In this case val_accuracy)\n* **patience:** waits for that much epochs for the improvements, if not, then decrease the learning rate. (here `2`)\n* **factor:** factor by which the learning rate will be reduced. new_lr = lr * factor (here `0.5`)\n* **min_lr:** lower bound on the learning rate. (here `0.00001`)\n\n**Read More:**\n[Read more](https:\/\/keras.io\/callbacks\/#reducelronplateau)\n\n","e791b79a":"# Creating submisson","5cb6db5e":"### Training Performance.\nNow we can see that after further training, the accuracy of training and validation set almost converges with high accuracy. It seems that the model has been significantly improved after image augmentation.","0f5ac7a5":"### Confusion Matrix\nIn the field of **machine learning** and specifically the problem of **statistical classification**, a confusion matrix, also known as an **error matrix**, is a specific table layout that **allows visualization of the performance of an algorithm**, typically a supervised learning one (in unsupervised learning it is usually called a matching matrix). Each row of the matrix represents the instances in a predicted class, while each column represents the instances in an actual class (or vice versa). The name stems from the fact that it makes it easy to see if the system is confusing two classes (i.e. commonly mislabeling one as another). [Source](https:\/\/en.wikipedia.org\/wiki\/Confusion_matrix)\n\nLet's try to see how well the model is performing on the validation set.","f6916749":"\n# Handwritten Digit Recognition (MNIST Dataset)\n<hr>\n\n![MNIST](https:\/\/neurohive.io\/wp-content\/uploads\/2019\/05\/Screenshot-from-2019-05-29-21-23-47.png)\n### History of Handwritten Digit dataset \n\nModified National Institute of Standards and Technology database (MNIST dataset) is a large dataset of handwritten digits which is widely used in image processing and machine learning. The set of images in the MNIST database is a combination of two of NIST's databases: Special Database 1 and Special Database 3. Special Database 1 and Special Database 3 consist of digits written by high school students and employees of the United States Census Bureau, respectively.<br> \n\n\n[READ MORE](https:\/\/en.wikipedia.org\/wiki\/MNIST_database)\n\n### Why to start with MNIST dataset?\nThe MNIST dataset is a well-known dataset for image classification. Tensorflow and Keras also provide MNIST dataset directly through their APIs. For the learning purpose, the MNIST dataset is easy to use and experiment with different machine learning techniques.\n\n### About the Notebook\n* In this notebook, I have covered the necessary steps to approach any Machine Learning Classification Problem.\n* Included Image Visualization for better understanding.\n* Quick Links to the functions I have used to explore it in depth.\n* Basic techniques such as Confusion Matrix, Image Augmentation, etc.\n* I have also compared the results of Model without using CNN and CNN.\n\nI have tried to make this notebook as simple as possible, along with covering the basic approach to tackle any classification task. \n### Task\nThe task is to classify the images in 10 class, i.e., [0-9], inclusively.  \n<hr>\n\n<font style=\"color:Red;font-size:18px\">If you find this notebook helpful, Please UPVOTE.<\/font>\n\n### Follow me:\n\n* <a href=\"https:\/\/bit.ly\/tarungithub\"><img src=\"data:image\/svg+xml;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCAyNCAyNCIgaGVpZ2h0PSI1MTIiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjUxMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMTIgLjVjLTYuNjMgMC0xMiA1LjI4LTEyIDExLjc5MiAwIDUuMjExIDMuNDM4IDkuNjMgOC4yMDUgMTEuMTg4LjYuMTExLjgyLS4yNTQuODItLjU2NyAwLS4yOC0uMDEtMS4wMjItLjAxNS0yLjAwNS0zLjMzOC43MTEtNC4wNDItMS41ODItNC4wNDItMS41ODItLjU0Ni0xLjM2MS0xLjMzNS0xLjcyNS0xLjMzNS0xLjcyNS0xLjA4Ny0uNzMxLjA4NC0uNzE2LjA4NC0uNzE2IDEuMjA1LjA4MiAxLjgzOCAxLjIxNSAxLjgzOCAxLjIxNSAxLjA3IDEuODAzIDIuODA5IDEuMjgyIDMuNDk1Ljk4MS4xMDgtLjc2My40MTctMS4yODIuNzYtMS41NzctMi42NjUtLjI5NS01LjQ2Ni0xLjMwOS01LjQ2Ni01LjgyNyAwLTEuMjg3LjQ2NS0yLjMzOSAxLjIzNS0zLjE2NC0uMTM1LS4yOTgtLjU0LTEuNDk3LjEwNS0zLjEyMSAwIDAgMS4wMDUtLjMxNiAzLjMgMS4yMDkuOTYtLjI2MiAxLjk4LS4zOTIgMy0uMzk4IDEuMDIuMDA2IDIuMDQuMTM2IDMgLjM5OCAyLjI4LTEuNTI1IDMuMjg1LTEuMjA5IDMuMjg1LTEuMjA5LjY0NSAxLjYyNC4yNCAyLjgyMy4xMiAzLjEyMS43NjUuODI1IDEuMjMgMS44NzcgMS4yMyAzLjE2NCAwIDQuNTMtMi44MDUgNS41MjctNS40NzUgNS44MTcuNDIuMzU0LjgxIDEuMDc3LjgxIDIuMTgyIDAgMS41NzgtLjAxNSAyLjg0Ni0uMDE1IDMuMjI5IDAgLjMwOS4yMS42NzguODI1LjU2IDQuODAxLTEuNTQ4IDguMjM2LTUuOTcgOC4yMzYtMTEuMTczIDAtNi41MTItNS4zNzMtMTEuNzkyLTEyLTExLjc5MnoiIGZpbGw9IiMyMTIxMjEiLz48L3N2Zz4=\" width=\"22\" align=\"left\" style=\"margin-right:10px\"\/> GitHub<\/a>\n* <a href=\"https:\/\/bit.ly\/tarnkr-youtube\"><img src=\"data:image\/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI\/Pg0KPCEtLSBHZW5lcmF0b3I6IEFkb2JlIElsbHVzdHJhdG9yIDE5LjAuMCwgU1ZHIEV4cG9ydCBQbHVnLUluIC4gU1ZHIFZlcnNpb246IDYuMDAgQnVpbGQgMCkgIC0tPg0KPHN2ZyB2ZXJzaW9uPSIxLjEiIGlkPSJMYXllcl8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCINCgkgdmlld0JveD0iMCAwIDQ2MS4wMDEgNDYxLjAwMSIgc3R5bGU9ImVuYWJsZS1iYWNrZ3JvdW5kOm5ldyAwIDAgNDYxLjAwMSA0NjEuMDAxOyIgeG1sOnNwYWNlPSJwcmVzZXJ2ZSI+DQo8cGF0aCBzdHlsZT0iZmlsbDojRjYxQzBEOyIgZD0iTTM2NS4yNTcsNjcuMzkzSDk1Ljc0NEM0Mi44NjYsNjcuMzkzLDAsMTEwLjI1OSwwLDE2My4xMzd2MTM0LjcyOA0KCWMwLDUyLjg3OCw0Mi44NjYsOTUuNzQ0LDk1Ljc0NCw5NS43NDRoMjY5LjUxM2M1Mi44NzgsMCw5NS43NDQtNDIuODY2LDk1Ljc0NC05NS43NDRWMTYzLjEzNw0KCUM0NjEuMDAxLDExMC4yNTksNDE4LjEzNSw2Ny4zOTMsMzY1LjI1Nyw2Ny4zOTN6IE0zMDAuNTA2LDIzNy4wNTZsLTEyNi4wNiw2MC4xMjNjLTMuMzU5LDEuNjAyLTcuMjM5LTAuODQ3LTcuMjM5LTQuNTY4VjE2OC42MDcNCgljMC0zLjc3NCwzLjk4Mi02LjIyLDcuMzQ4LTQuNTE0bDEyNi4wNiw2My44ODFDMzA0LjM2MywyMjkuODczLDMwNC4yOTgsMjM1LjI0OCwzMDAuNTA2LDIzNy4wNTZ6Ii8+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8L3N2Zz4NCg==\" width=\"22\" align=\"left\" style=\"margin-right:10px\"\/>Youtube<a\/>\n* <a href=\"https:\/\/medium.com\/@codeeasy\"><img src=\"data:image\/svg+xml;base64,PHN2ZyBlbmFibGUtYmFja2dyb3VuZD0ibmV3IDAgMCAyNCAyNCIgaGVpZ2h0PSI1MTIiIHZpZXdCb3g9IjAgMCAyNCAyNCIgd2lkdGg9IjUxMiIgeG1sbnM9Imh0dHA6Ly93d3cudzMub3JnLzIwMDAvc3ZnIj48cGF0aCBkPSJtMjIuMDg1IDQuNzMzIDEuOTE1LTEuODMydi0uNDAxaC02LjYzNGwtNC43MjggMTEuNzY4LTUuMzc5LTExLjc2OGgtNi45NTZ2LjQwMWwyLjIzNyAyLjY5M2MuMjE4LjE5OS4zMzIuNDkuMzAzLjc4M3YxMC41ODNjLjA2OS4zODEtLjA1NS43NzMtLjMyMyAxLjA1bC0yLjUyIDMuMDU0di4zOTZoNy4xNDV2LS40MDFsLTIuNTItMy4wNDljLS4yNzMtLjI3OC0uNDAyLS42NjMtLjM0Ny0xLjA1di05LjE1NGw2LjI3MiAxMy42NTloLjcyOWw1LjM5My0xMy42NTl2MTAuODgxYzAgLjI4NyAwIC4zNDYtLjE4OC41MzRsLTEuOTQgMS44Nzd2LjQwMmg5LjQxMnYtLjQwMWwtMS44Ny0xLjgzMWMtLjE2NC0uMTI0LS4yNDktLjMzMi0uMjE0LS41MzR2LTEzLjQ2N2MtLjAzNS0uMjAzLjA0OS0uNDExLjIxMy0uNTM0eiIgZmlsbD0iIzIxMjEyMSIvPjwvc3ZnPg==\"  width=\"22\" align=\"left\" style=\"margin-right:10px\"\/> Medium<\/a>\n* <a href=\"https:\/\/www.linkedin.com\/in\/tarun-kumar-iit-ism\/\"><img src=\"data:image\/svg+xml;base64,PD94bWwgdmVyc2lvbj0iMS4wIiBlbmNvZGluZz0iaXNvLTg4NTktMSI\/Pg0KPCEtLSBHZW5lcmF0b3I6IEFkb2JlIElsbHVzdHJhdG9yIDE5LjAuMCwgU1ZHIEV4cG9ydCBQbHVnLUluIC4gU1ZHIFZlcnNpb246IDYuMDAgQnVpbGQgMCkgIC0tPg0KPHN2ZyB2ZXJzaW9uPSIxLjEiIGlkPSJMYXllcl8xIiB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHhtbG5zOnhsaW5rPSJodHRwOi8vd3d3LnczLm9yZy8xOTk5L3hsaW5rIiB4PSIwcHgiIHk9IjBweCINCgkgdmlld0JveD0iMCAwIDM4MiAzODIiIHN0eWxlPSJlbmFibGUtYmFja2dyb3VuZDpuZXcgMCAwIDM4MiAzODI7IiB4bWw6c3BhY2U9InByZXNlcnZlIj4NCjxwYXRoIHN0eWxlPSJmaWxsOiMwMDc3Qjc7IiBkPSJNMzQ3LjQ0NSwwSDM0LjU1NUMxNS40NzEsMCwwLDE1LjQ3MSwwLDM0LjU1NXYzMTIuODg5QzAsMzY2LjUyOSwxNS40NzEsMzgyLDM0LjU1NSwzODJoMzEyLjg4OQ0KCUMzNjYuNTI5LDM4MiwzODIsMzY2LjUyOSwzODIsMzQ3LjQ0NFYzNC41NTVDMzgyLDE1LjQ3MSwzNjYuNTI5LDAsMzQ3LjQ0NSwweiBNMTE4LjIwNywzMjkuODQ0YzAsNS41NTQtNC41MDIsMTAuMDU2LTEwLjA1NiwxMC4wNTYNCglINjUuMzQ1Yy01LjU1NCwwLTEwLjA1Ni00LjUwMi0xMC4wNTYtMTAuMDU2VjE1MC40MDNjMC01LjU1NCw0LjUwMi0xMC4wNTYsMTAuMDU2LTEwLjA1Nmg0Mi44MDYNCgljNS41NTQsMCwxMC4wNTYsNC41MDIsMTAuMDU2LDEwLjA1NlYzMjkuODQ0eiBNODYuNzQ4LDEyMy40MzJjLTIyLjQ1OSwwLTQwLjY2Ni0xOC4yMDctNDAuNjY2LTQwLjY2NlM2NC4yODksNDIuMSw4Ni43NDgsNDIuMQ0KCXM0MC42NjYsMTguMjA3LDQwLjY2Niw0MC42NjZTMTA5LjIwOCwxMjMuNDMyLDg2Ljc0OCwxMjMuNDMyeiBNMzQxLjkxLDMzMC42NTRjMCw1LjEwNi00LjE0LDkuMjQ2LTkuMjQ2LDkuMjQ2SDI4Ni43Mw0KCWMtNS4xMDYsMC05LjI0Ni00LjE0LTkuMjQ2LTkuMjQ2di04NC4xNjhjMC0xMi41NTYsMy42ODMtNTUuMDIxLTMyLjgxMy01NS4wMjFjLTI4LjMwOSwwLTM0LjA1MSwyOS4wNjYtMzUuMjA0LDQyLjExdjk3LjA3OQ0KCWMwLDUuMTA2LTQuMTM5LDkuMjQ2LTkuMjQ2LDkuMjQ2aC00NC40MjZjLTUuMTA2LDAtOS4yNDYtNC4xNC05LjI0Ni05LjI0NlYxNDkuNTkzYzAtNS4xMDYsNC4xNC05LjI0Niw5LjI0Ni05LjI0Nmg0NC40MjYNCgljNS4xMDYsMCw5LjI0Niw0LjE0LDkuMjQ2LDkuMjQ2djE1LjY1NWMxMC40OTctMTUuNzUzLDI2LjA5Ny0yNy45MTIsNTkuMzEyLTI3LjkxMmM3My41NTIsMCw3My4xMzEsNjguNzE2LDczLjEzMSwxMDYuNDcyDQoJTDM0MS45MSwzMzAuNjU0TDM0MS45MSwzMzAuNjU0eiIvPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPGc+DQo8L2c+DQo8Zz4NCjwvZz4NCjxnPg0KPC9nPg0KPC9zdmc+DQo=\" width=\"22\" align=\"left\" style=\"margin-right:10px\"\/>Linkedin<\/a>\n\n\n\n<hr>\n\n# Content:\n\n* [Handwritten Digit Recognition (MNIST Dataset)](#Handwritten-Digit-Recognition-(MNIST-Dataset))\n* [Required Imports](#Required-Imports)\n* [Loading and Visualizing Dataset](#Loading-and-Visualizing-Dataset)\n* [Visualize Digits dataset](#Visualize-Digits-dataset)\n* **[Buliding Model](#Buliding-Model)**\n    * [Model Using Keras](#Model-Using-Keras)\n    * [Compiling Model](#Compiling-Model)\n    * [Training](#Training)\n    * [Training Performance](#Training-Performance.)\n    * [Confusion Matrix](#Confusion-Matrix)\n* **[Improving Result by Image Augmentation](#Improving-Result-by-Image-Augmentation)**\n    * [Augmentation Using Keras](#Augmentation-Using-Keras)\n    * [Learning Rate](#Learning-Rate)\n* [Visualizing Result](#Visualizing-Result)\n* [Predict on Testset](#Predict-on-Testset)\n* **[Understand the Intermediate Layers of the Model](#Lets-understand-the-intermediate-layers-of-the-model)**\n    * [Compare the layer 1 output](#Try-to-compare-the-layer-1-output)\n    * [Visualizing All Intermediate Activation Layer](#Visualizing-All-Intermediate-Activation-Layer)","3ee7d1ef":"It seems that both the training and validation set has a good balance between the classes, so let's move on and see a few of the digits.\n\n### Visualizing the digits by plotting Images\n<hr>\nThis will plot the first 30 images of digits with the label.","1fb3b147":"# Predict on Testset","1678b163":"### Preparing Training and Validation data \n**It requires a few steps:**\n* Assuming the validation set size. I am taking it 10% of the training set.  \n* Splitting training set into a training set (90% original training set) and validation set (10% original training set) from the training dataset.\n* Reshaping both sets into (sample size,28,28,1) where sample size represents the size of the train or validation set.\n* Splitting the labels for both training set and validation set.\n\nUsing df.iloc of pandas for slicing the data frame(read more link below), then converting into NumPy array and finally reshaping NumPy array in required shape.<br>\n\n**Check out these functions for more info:**\n* [df.iloc[]](https:\/\/pandas.pydata.org\/pandas-docs\/stable\/reference\/api\/pandas.DataFrame.iloc.html)\n* [np.reshape](https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.reshape.html)\n* [np.asarray](https:\/\/docs.scipy.org\/doc\/numpy\/reference\/generated\/numpy.asarray.html)","61f9f5db":"# Buliding Model\n\n<hr>\n\n![model](https:\/\/miro.medium.com\/max\/1000\/1*4OUonEDfZwCfR4Y-G-h1fw.jpeg)","90ef214c":"#### Shape of training set","50b5dba1":"It seems that each channel is trying to depict something in both images, a kind of similar features. This can be concluded that each channel has been trained to find some specific features of the input images.\n\n## Visualizing All Intermediate Activation Layer","c539c6e8":"### Compiling Model\nModel compilation required the selection of optimizer and loss function. Let me discuss a few important things to avoid confusion.\n\n**Optimizers:** Keras provides several optimizers that can be used by importing the optimizers and passing in compile function.\n* SGD (Stochastic gradient descent optimizer)\n* RMSprop \n* Adam\n* Adamax\n\nThere are several more check it out [here](https:\/\/keras.io\/optimizers\/). \n<br>\n\nOne of the important things is the selection of the **learning rate**. If the learning rate is too high, the loss may not converge, and if it is too low, the training will be slow. So it is important to select the reasonably fair value of learning rate. One of the good value to start with is 0.001. If it doesn't work, then other higher or lower values can be tried. The rest of the parameters generally works well and need not be defined. The default value works most of the time.\nHere, I am using **Adam optimizer**, but **RMSprop** can also be used.\n<br>\n\n**Loss Functions:** Keras provides all of the well-known loss functions which work well for most of the time. But if you need to define a custom loss function, you can. Defining a custom function is out of the scope of this notebook. Let's understand the loss function for the classification tasks. I am discussing 3 loss function here. There are several more check it out [here](https:\/\/keras.io\/losses\/). \n* **binary_crossentropy**: This loss function is used for the binary classification task. The single-node output layer is required. 0 and 1 is used for classification.\n* **categorical_crossentropy**:  Used for Used for Multi-class classification \n* **sparse_categorical_crossentropy:** Used for Multi-class classification. <br>\n\n**Difference between categorical_crossentropy and sparse_categorical_crossentropy:**\n* If your targets are one-hot encoded, use categorical_crossentropy.\n    Examples of one-hot encodings (for three classes):\n    * [1,0,0]\n    * [0,1,0]\n    * [0,0,1]\n* But if your targets are integers, use sparse_categorical_crossentropy.\n    Examples of integer encodings (for the sake of completion):\n    * 1\n    * 2\n    * 3\n    \nHere, I am using **sparse_categorical_crossentropy**, as the target values are integer not **one-hot vector**.\n"}}