{"cell_type":{"376bef52":"code","0255bc57":"code","73ee9eb3":"code","9895aa67":"code","506545ac":"code","76184d8f":"code","e303e688":"code","67f967c0":"code","5e05b690":"code","b98d75c2":"markdown"},"source":{"376bef52":"%%capture\n!git clone https:\/\/github.com\/airsplay\/py-bottom-up-attention.git\n%cd py-bottom-up-attention\n\n# Install python libraries\n!pip install -r requirements.txt\n!pip install 'git+https:\/\/github.com\/cocodataset\/cocoapi.git#subdirectory=PythonAPI'\n\n# Install detectron2\n!python setup.py build develop\n\n# or if you are on macOS\n# MACOSX_DEPLOYMENT_TARGET=10.9 CC=clang CXX=clang++ python setup.py build develop\n\n# or, as an alternative to `setup.py`, do\n# pip install [--editable] .\n!pip install --upgrade --force-reinstall  imagesize","0255bc57":"import os\nimport io\n\nimport detectron2\n\n# import some common detectron2 utilities\nfrom detectron2.engine import DefaultPredictor\nfrom detectron2.config import get_cfg\nfrom detectron2.utils.visualizer import Visualizer\nfrom detectron2.data import MetadataCatalog\n\n# import some common libraries\nimport numpy as np\nimport cv2\nimport torch\n\n# Show the image in ipynb\nfrom IPython.display import clear_output, Image, display\nimport PIL.Image\ndef showarray(a, fmt='jpeg'):\n    a = np.uint8(np.clip(a, 0, 255))\n    f = io.BytesIO()\n    PIL.Image.fromarray(a).save(f, fmt)\n    display(Image(data=f.getvalue()))","73ee9eb3":"%cd demo\n!ls","9895aa67":"# Load VG Classes\ndata_path = 'data\/genome\/1600-400-20'\n\nvg_classes = []\nwith open(os.path.join(data_path, 'objects_vocab.txt')) as f:\n    for object in f.readlines():\n        vg_classes.append(object.split(',')[0].lower().strip())\n\nMetadataCatalog.get(\"vg\").thing_classes = vg_classes","506545ac":"cfg = get_cfg()\ncfg.merge_from_file(\"..\/configs\/VG-Detection\/faster_rcnn_R_101_C4_caffe.yaml\")\ncfg.MODEL.RPN.POST_NMS_TOPK_TEST = 300\ncfg.MODEL.ROI_HEADS.NMS_THRESH_TEST = 0.6\ncfg.MODEL.ROI_HEADS.SCORE_THRESH_TEST = 0.2\n# VG Weight\ncfg.MODEL.WEIGHTS = \"http:\/\/nlp.cs.unc.edu\/models\/faster_rcnn_from_caffe.pkl\"\npredictor = DefaultPredictor(cfg)","76184d8f":"im = cv2.imread(\"data\/images\/input.jpg\")\nim_rgb = cv2.cvtColor(im, cv2.COLOR_BGR2RGB)\nshowarray(im_rgb)","e303e688":"NUM_OBJECTS = 36\n\nfrom detectron2.modeling.postprocessing import detector_postprocess\nfrom detectron2.modeling.roi_heads.fast_rcnn import FastRCNNOutputLayers, FastRCNNOutputs, fast_rcnn_inference_single_image\n\ndef doit(raw_image):\n    with torch.no_grad():\n        raw_height, raw_width = raw_image.shape[:2]\n        print(\"Original image size: \", (raw_height, raw_width))\n        \n        # Preprocessing\n        image = predictor.transform_gen.get_transform(raw_image).apply_image(raw_image)\n        print(\"Transformed image size: \", image.shape[:2])\n        image = torch.as_tensor(image.astype(\"float32\").transpose(2, 0, 1))\n        inputs = [{\"image\": image, \"height\": raw_height, \"width\": raw_width}]\n        images = predictor.model.preprocess_image(inputs)\n        \n        # Run Backbone Res1-Res4\n        features = predictor.model.backbone(images.tensor)\n        \n        # Generate proposals with RPN\n        proposals, _ = predictor.model.proposal_generator(images, features, None)\n        proposal = proposals[0]\n        print('Proposal Boxes size:', proposal.proposal_boxes.tensor.shape)\n        \n        # Run RoI head for each proposal (RoI Pooling + Res5)\n        proposal_boxes = [x.proposal_boxes for x in proposals]\n        features = [features[f] for f in predictor.model.roi_heads.in_features]\n        box_features = predictor.model.roi_heads._shared_roi_transform(\n            features, proposal_boxes\n        )\n        feature_pooled = box_features.mean(dim=[2, 3])  # pooled to 1x1\n        print('Pooled features size:', feature_pooled.shape)\n        \n        # Predict classes and boxes for each proposal.\n        pred_class_logits, pred_proposal_deltas = predictor.model.roi_heads.box_predictor(feature_pooled)\n        outputs = FastRCNNOutputs(\n            predictor.model.roi_heads.box2box_transform,\n            pred_class_logits,\n            pred_proposal_deltas,\n            proposals,\n            predictor.model.roi_heads.smooth_l1_beta,\n        )\n        probs = outputs.predict_probs()[0]\n        boxes = outputs.predict_boxes()[0]\n        \n        # Note: BUTD uses raw RoI predictions,\n        #       we use the predicted boxes instead.\n        # boxes = proposal_boxes[0].tensor    \n        \n        # NMS\n        for nms_thresh in np.arange(0.5, 1.0, 0.1):\n            instances, ids = fast_rcnn_inference_single_image(\n                boxes, probs, image.shape[1:], \n                score_thresh=0.2, nms_thresh=nms_thresh, topk_per_image=NUM_OBJECTS\n            )\n            if len(ids) == NUM_OBJECTS:\n                break\n                \n        instances = detector_postprocess(instances, raw_height, raw_width)\n        roi_features = feature_pooled[ids].detach()\n        print(instances)\n        \n        return instances, roi_features\n    \ninstances, features = doit(im)\n\nprint(instances.pred_boxes)\nprint(instances.scores)\nprint(instances.pred_classes)","67f967c0":"# Show the boxes, labels, and features\npred = instances.to('cpu')\nv = Visualizer(im[:, :, :], MetadataCatalog.get(\"vg\"), scale=1.2)\nv = v.draw_instance_predictions(pred)\nshowarray(v.get_image()[:, :, ::-1])\nprint('instances:\\n', instances)\nprint()\nprint('boxes:\\n', instances.pred_boxes)\nprint()\nprint('Shape of features:\\n', features.shape)","5e05b690":"# Verify the correspondence of RoI features\npred_class_logits, pred_proposal_deltas = predictor.model.roi_heads.box_predictor(features)\npred_class_probs = torch.nn.functional.softmax(pred_class_logits, -1)[:, :-1]\nmax_probs, max_classes = pred_class_probs.max(-1)\nprint(\"%d objects are different, it is because the classes-aware NMS process\" % (NUM_OBJECTS - torch.eq(instances.pred_classes, max_classes).sum().item()))\nprint(\"The total difference of score is %0.4f\" % (instances.scores - max_probs).abs().sum().item())","b98d75c2":"credit: https:\/\/github.com\/airsplay\/py-bottom-up-attention"}}