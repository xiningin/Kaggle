{"cell_type":{"f202f4b6":"code","984213ee":"code","c3edba1d":"code","e24e55a2":"code","96a6601f":"code","c9dba6b1":"code","6bf58344":"code","4578418c":"code","9ff73d4d":"code","b17896e5":"code","b692eb54":"code","c010ca85":"code","18acb7cc":"code","9abc4c14":"code","bc42c0f6":"code","6eccd758":"code","6c9b8e99":"code","f6ce700b":"code","54ca0b34":"code","aebc5261":"code","8d44f99d":"code","d88d6780":"code","8fd28400":"code","04d3a7fe":"markdown","e1c39064":"markdown","e866403f":"markdown","179d59c9":"markdown","de22cea2":"markdown","b7fefdda":"markdown","720b3fa9":"markdown","6f8ed3b4":"markdown","1ec1e137":"markdown","035c7efd":"markdown","b785e850":"markdown","38ec75d3":"markdown"},"source":{"f202f4b6":"import numpy as np\nimport pandas as pd   \nimport matplotlib.pyplot as plt\nimport seaborn as sns\npd.set_option('max_columns', 150)\nimport random\nX_ = np.loadtxt(\"\/kaggle\/input\/iot-sensordata\/Xdados.txt\") # X have 14.400rows x 52 sensors\n\nX=pd.DataFrame(X_, columns=['s1','s2','s3','s4','s5','s6','s7','s8','s9','s10','s11','s12','s13','s14','s15','s16','s17','s18','s19','s20','s21','s22','s23','s24','s25','s26','s27','s28','s29','s30','s31','s32','s33','s34','s35','s36','s37','s38','s39','s40','s41','s42','s43','s44','s45','s46','s47','s48','s49','s50','s51','s52']) \nprint(X.shape)\nX.to_csv('Xdados.csv',index=False)\nX.head() ","984213ee":"import matplotlib.pyplot as plt# Standardize\/scale the dataset and apply PCA\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.decomposition import PCA\nfrom sklearn.pipeline import make_pipeline\n# Extract the names of the numerical columns\n\nnames=X.columns[13]\nx = X[names]","c3edba1d":"scaler = StandardScaler()\npca = PCA()\npipeline = make_pipeline(scaler, pca)\npipeline.fit(x.values.reshape(-1, 1))","e24e55a2":"# Plot the principal components against their inertia\nfeatures = range(pca.n_components_)\n_ = plt.figure(figsize=(15, 5))\n_ = plt.bar(features, pca.explained_variance_)\n_ = plt.xlabel('PCA feature')\n_ = plt.ylabel('Variance')\n_ = plt.xticks(features)\n_ = plt.title(\"Importance of the Principal Components based on inertia\")\nplt.show()\n","96a6601f":"# Calculate PCA with 1 components\npca = PCA(n_components=1)\nprincipalComponents = pca.fit_transform(x.values.reshape(-1,1))\nprincipalDf = pd.DataFrame(data = principalComponents, columns = ['pc1'])","c9dba6b1":"from statsmodels.tsa.stattools import adfuller\n# Run Augmented Dickey Fuller Test\nresult = adfuller(principalDf['pc1'])\n# Print p-value\nprint(result[1])","6bf58344":"sensor = 's14'\ndf = pd.merge(left=principalDf, right=X[sensor], left_on=principalDf.index, right_on=X[sensor].index)\ndf.describe()","4578418c":"def escoragem(pred, real):\n    # computing errors\n    errors = np.abs(pred - real).flatten()\n    # estimation\n    mean = sum(errors)\/len(errors)\n    cov = 0\n    for e in errors:\n        cov += (e - mean)**2\n    cov \/= len(errors)\n\n    print('mean : ', mean)\n    print('cov : ', cov)\n    return errors, cov, mean\n\n# calculate Mahalanobis distance\ndef Mahala_distantce(x,mean,cov):\n    return (x - mean)**2 \/ cov\n\nerrors, cov, mean = escoragem(df['pc1'].values, df[sensor].values)\n\nmahala_dist = []\nfor e in errors:\n    mahala_dist.append(Mahala_distantce(e, mean, cov))","9ff73d4d":"def scale(A):\n    return (A-np.min(A))\/(np.max(A) - np.min(A))","b17896e5":"df['sensor_scores'] = mahala_dist\ndf['sensor_escoragem_escalado'] = scale(mahala_dist)\n","b692eb54":"#### Plotando scores","c010ca85":"plt.figure(figsize=(12, 8))\nplt.hist(df['sensor_scores'], bins=100);","18acb7cc":"plt.figure(figsize=(12, 8))\nplt.hist(df['sensor_escoragem_escalado'], bins=100);","9abc4c14":"q1_pc1, q3_pc1 = df['sensor_scores'].quantile([0.25, 0.75])\niqr_pc1 = q3_pc1 - q1_pc1\n\n# Calculate upper and lower bounds for outlier for pc1\nlower_pc1 = q1_pc1 - (1.5*iqr_pc1)\nupper_pc1 = q3_pc1 + (1.5*iqr_pc1)\n    # Filter out the outliers from the pc1\ndf['sensor_anomalia'] = ((df['sensor_scores']>upper_pc1) | (df['sensor_scores']<lower_pc1)).astype('int')","bc42c0f6":"fig, axes = plt.subplots(nrows=2, figsize=(15,10))\naxes[0].plot(df[sensor], color='blue')\naxes[1].plot(np.array(mahala_dist).ravel(), color='red')\n\naxes[0].set_title('Dados originais', fontsize=20)\naxes[1].set_title('Score de Anomalia', fontsize=20)\n\n# axes[0].grid()\n# axes[1].grid()\nplt.tight_layout()\nplt.show()\n","6eccd758":"# visualization\na = X.loc[df['sensor_anomalia'] == 1] \n_ = plt.figure(figsize=(18,6))\n_ = plt.plot(df[sensor], color='blue', label='Normal')\n_ = plt.plot(a[sensor], linestyle='none', marker='X', color='red', markersize=12, label='Anomalia')\n_ = plt.xlabel('Series')\n_ = plt.ylabel('leitura do Sensor')\n_ = plt.title('Anomalias do Sensor 1')\n_ = plt.legend(loc='best')\nplt.show();","6c9b8e99":"N = X.shape[0]\nplt.scatter(range(N),df['sensor_escoragem_escalado'][:N].cumsum(),marker='1',label='PCA ')\nplt.xlabel('Dados do sensor')\nplt.ylabel('Anomalias encontradas')\nplt.legend()\nplt.show()\n","f6ce700b":"#2 -- Distributions of Predicted Probabilities of both classes\nlabels=['Positivo','negativo']\nplt.hist(df[df['sensor_anomalia']==1]['sensor_escoragem_escalado'], density=False, bins=100,\n             alpha=.5, color='green',  label=labels[0])\nplt.hist(df[df['sensor_anomalia']==0]['sensor_escoragem_escalado'], density=False, bins=100,\n             alpha=.5, color='red', label=labels[1])\nplt.axvline(.5, color='blue', linestyle='--', label='Fronteira de Decis\u00e3o')\n# plt.xlim([0,1])\nplt.title('Distribui\u00e7\u00e3o dos Valores', size=13)\nplt.xlabel('Valores normalizados', size=13)\nplt.ylabel('Amostra (normalizados)', size=13)\nplt.legend(loc=\"upper right\")","54ca0b34":"# Calculate IQR for the 1st principal component (pc1)\nq1_pc1, q3_pc1 = df['pc1'].quantile([0.25, 0.75])\niqr_pc1 = q3_pc1 - q1_pc1\n# Calculate upper and lower bounds for outlier for pc1\nlower_pc1 = q1_pc1 - (1.5*iqr_pc1)\nupper_pc1 = q3_pc1 + (1.5*iqr_pc1)\n# Filter out the outliers from the pc1\ndf['anomaly_pc1'] = ((df['pc1']>upper_pc1) | (df['pc1']<lower_pc1)).astype('int')","aebc5261":"fig, axes = plt.subplots(nrows=2, figsize=(15,10))\naxes[0].plot(df[sensor], color='blue')\naxes[1].plot(df['anomaly_pc1'], color='red')\n\naxes[0].set_title('Dados originais', fontsize=20)\naxes[1].set_title('Score de Anomalia', fontsize=20)\n\n# axes[0].grid()\n# axes[1].grid()\nplt.tight_layout()\nplt.show()\n","8d44f99d":"# Let's plot the outliers from pc1 on top of the sensor_11 and see where they occured in the time series\na = df[df['anomaly_pc1'] == 1] #anomaly\n_ = plt.figure(figsize=(18,6))\n_ = plt.plot(X[sensor], color='blue', label='Normal')\n_ = plt.plot(a[sensor], linestyle='none', marker='X', color='red', markersize=12, label='Anomaly')\n_ = plt.xlabel('Epoch')\n_ = plt.ylabel('Leituras do Sensor')\n_ = plt.title('Anomalias ')\n_ = plt.legend(loc='best')\nplt.show();","d88d6780":"N = X.shape[0]\nplt.scatter(range(N),df['anomaly_pc1'][:N].cumsum(),marker='1',label='PCA ')\nplt.xlabel('Dados do sensor')\nplt.ylabel('Anomalias encontradas')\nplt.legend()\nplt.show()","8fd28400":"#2 -- Distributions of Predicted Probabilities of both classes\nlabels=['Positivo','negativo']\nplt.hist(df[df['anomaly_pc1']==1]['sensor_escoragem_escalado'], density=False, bins=100,\n             alpha=.5, color='green',  label=labels[0])\nplt.hist(df[df['anomaly_pc1']==0]['sensor_escoragem_escalado'], density=False, bins=100,\n             alpha=.5, color='red', label=labels[1])\nplt.axvline(.5, color='blue', linestyle='--', label='Fronteira de Decis\u00e3o')\n# plt.xlim([0,1])\nplt.title('Distribui\u00e7\u00e3o dos Valores', size=13)\nplt.xlabel('Valores normalizados', size=13)\nplt.ylabel('Amostra (normalizados)', size=13)\nplt.legend(loc=\"upper right\")","04d3a7fe":"# Score com Mahalanobis \n\nComo \u00e9 gerado o score?\nA pontua\u00e7\u00e3o da anomalia \u00e9 definida pela dist\u00e2ncia de Mahalanobis dos vetores de erro de predi\u00e7\u00e3o ajustados a uma distribui\u00e7\u00e3o normal.\n\ne = | X_original - X_predito |\n\nPodemos medir a raridade do evento com a localiza\u00e7\u00e3o na distribui\u00e7\u00e3o para isso usamos a dist\u00e2ncia do Mahalanobis que \u00e9 uma estat\u00edstica que representa uma pontua\u00e7\u00e3o de anomalia. Assumindo que os par\u00e2metros de uma distribui\u00e7\u00e3o Gaussiana com dimensional M temos:\n\np(x|Data)=N(x|\u03bc\u02c6,\u2211\u02c6).\n\nEnt\u00e3o, a dist\u00e2ncia de Mahalanobis \u00e9 definida como: a(x)=(x\u2212\u03bc\u02c6)\u22a4\u2211\u02c6\u22121(x\u2212\u03bc\u02c6). Podemos medir a raridade do evento com um a(x).\n\nEstimamos a m\u00e9dia e a vari\u00e2ncia ajustando-o \u00e0 distribui\u00e7\u00e3o normal. A raz\u00e3o para usar a distribui\u00e7\u00e3o normal \u00e9 que, assumindo uma distribui\u00e7\u00e3o normal, a dist\u00e2ncia de Mahalanobis segue a distribui\u00e7\u00e3o qui-quadrado com um grau de liberdade de 1, tornando mais f\u00e1cil determinar o limiar de anormalidade. As estimativas de m\u00e9dia e vari\u00e2ncia s\u00e3o:\n\n\u03bc\u02c6= (1\/N) \u2211n=1 at\u00e9 N de (x(n))\n\n\u2211\u02c6= (1\/N) \u2211n=1 at\u00e9 N de (x(n)\u2212\u03bc\u02c6)(x(n)\u2212\u03bc\u02c6)T(transposto)\n\nGerando um score com Mahalanobis\n\nAjustando uma distribui\u00e7\u00e3o gaussiana dimensional M","e1c39064":"0.03894833795654094 are very small number (much smaller than 0.05). Thus, I will reject the Null Hypothesis and say the data is stationary","e866403f":"## Calculando o IQR como score da anomalia","179d59c9":"Anomaly analysis","de22cea2":"# Ocorr\u00eancias de anomalias no sensor","b7fefdda":"Distribui\u00e7\u00e3o dos scores das anomalias do sensor","720b3fa9":"Distribui\u00e7\u00e3o dos scores das anomalias do sensor","6f8ed3b4":"# Ocorr\u00eancias de anomalias no sensor","1ec1e137":"#### Plotando scores normalizados","035c7efd":"Plotando as anomalias do sensor 14","b785e850":"Plotando as anomalias do sensor 14","38ec75d3":"PCA in single time series"}}