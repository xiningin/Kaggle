{"cell_type":{"2ad78ad8":"code","ec63a39a":"code","f195a688":"code","3067633a":"code","2c40cc11":"code","6bff99b8":"code","e0529fc8":"code","c26b4ef2":"code","b35e760e":"code","a3c97965":"code","ff20d43a":"code","a9712174":"code","83f7e796":"code","8c41133a":"code","afb16689":"code","656eb2ef":"code","e048b80b":"code","9a0185e8":"code","dbd37396":"code","d40dcd01":"code","b6184edb":"code","b7551d8d":"code","197a26b3":"code","f226d1de":"code","8e46d571":"code","24042661":"code","a464441d":"code","341981cf":"code","0006d321":"code","a103ebd6":"code","eac55310":"code","88941bcf":"code","f89a2996":"code","347cc845":"code","a64afb00":"code","e99aefea":"code","dfa2d5f7":"code","ce51adb1":"code","1472a31d":"code","2fe2b7f7":"code","6ce4a7fb":"markdown","ec08bf80":"markdown","3774da58":"markdown","6106b11c":"markdown","4b53686b":"markdown","51715399":"markdown"},"source":{"2ad78ad8":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","ec63a39a":"np.random.seed(0)\n\nfile_path = '\/kaggle\/input\/pima-indians-diabetes-database\/diabetes.csv'\ndata = pd.read_csv(file_path)\n\ndata.head(5)","f195a688":"data[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']] = data[['Glucose','BloodPressure','SkinThickness','Insulin','BMI']].replace(0,np.NaN)","3067633a":"data.loc[(data['Outcome'] == 0 ) & (data['Insulin'].isnull()), 'Insulin'] = 102.5\ndata.loc[(data['Outcome'] == 1 ) & (data['Insulin'].isnull()), 'Insulin'] = 169.5","2c40cc11":"data.loc[(data['Outcome'] == 0 ) & (data['Glucose'].isnull()), 'Glucose'] = 107\ndata.loc[(data['Outcome'] == 1 ) & (data['Glucose'].isnull()), 'Glucose'] = 140","6bff99b8":"data.loc[(data['Outcome'] == 0 ) & (data['SkinThickness'].isnull()), 'SkinThickness'] = 27\ndata.loc[(data['Outcome'] == 1 ) & (data['SkinThickness'].isnull()), 'SkinThickness'] = 32","e0529fc8":"data.loc[(data['Outcome'] == 0 ) & (data['BloodPressure'].isnull()), 'BloodPressure'] = 70\ndata.loc[(data['Outcome'] == 1 ) & (data['BloodPressure'].isnull()), 'BloodPressure'] = 74.5","c26b4ef2":"data.loc[(data['Outcome'] == 0 ) & (data['BMI'].isnull()), 'BMI'] = 30.1\ndata.loc[(data['Outcome'] == 1 ) & (data['BMI'].isnull()), 'BMI'] = 34.3","b35e760e":"(data == 0).sum(axis=0)","a3c97965":"data.loc[:, 'N1'] = 0\ndata.loc[(data['Age'] <= 30) & (data['Glucose'] <= 120), 'N1'] = 1","ff20d43a":"data.loc[:, 'N2'] = 0\ndata.loc[(data['BMI'] <= 30), 'N2'] = 1","a9712174":"data.loc[:, 'N3'] = 0\ndata.loc[(data['Age'] <= 30) & (data['Pregnancies'] <= 6), 'N3'] = 1","83f7e796":"data.loc[:,'N4']=0\ndata.loc[(data['Glucose']<=105) & (data['BloodPressure']<=80),'N4']=1","8c41133a":"data.loc[:,'N5']=0\ndata.loc[(data['SkinThickness']<=20) ,'N5']=1","afb16689":"data.loc[:,'N6']=0\ndata.loc[(data['BMI']<30) & (data['SkinThickness']<=20),'N6']=1","656eb2ef":"data.loc[:,'N7']=0\ndata.loc[(data['Glucose']<=105) & (data['BMI']<=30),'N7']=1","e048b80b":"data.loc[:,'N9']=0\ndata.loc[(data['Insulin']<200),'N9']=1","9a0185e8":"data.loc[:,'N10']=0\ndata.loc[(data['BloodPressure']<80),'N10']=1","dbd37396":"data.loc[:,'N11']=0\ndata.loc[(data['Pregnancies']<4) & (data['Pregnancies']!=0) ,'N11']=1","d40dcd01":"data['N0'] = data['BMI'] * data['SkinThickness']\n\ndata['N8'] =  data['Pregnancies'] \/ data['Age']\n\ndata['N13'] = data['Glucose'] \/ data['DiabetesPedigreeFunction']\n\ndata['N12'] = data['Age'] * data['DiabetesPedigreeFunction']\n\ndata['N14'] = data['Age'] \/ data['Insulin']","b6184edb":"target_name = 'Outcome'\ntrain = data.drop([target_name], axis=1)\ntrain.head()","b7551d8d":"target = data.Outcome\ntarget.head()","197a26b3":"from sklearn.model_selection import train_test_split\n\nnp.random.seed(0)\ntrain, test_train, target, test_target = train_test_split(train, target, test_size=0.2)\nprint(train.info())\nprint(test_train.info())","f226d1de":"(train == 0).sum(axis=0)","8e46d571":"mean = np.mean(train)\nstd = np.std(train)\n\ntrain = (train-mean)\/(std+1e-7)\ntest_train = (test_train-mean)\/(std+1e-7)","24042661":"train, val_train, target, val_target = train_test_split(train, target, test_size=0.2)\nprint(train.info())\nprint(val_train.info())","a464441d":"from keras.models import Sequential\nfrom keras.layers import Dense\n\nmodel = Sequential()\n\nmodel.add(Dense(32, input_dim=train.shape[1], activation='relu'))\nmodel.add(Dense(64, activation='relu'))\nmodel.add(Dense(1, activation='sigmoid'))\n\nmodel.summary()","341981cf":"from keras import optimizers\n\nopt = optimizers.Adam(lr=0.001)\nmodel.compile(loss='binary_crossentropy', optimizer=opt, metrics=['accuracy'])","0006d321":"from keras.callbacks import EarlyStopping\n\nes = EarlyStopping(monitor='val_loss', patience=20, mode='max')\n\nhistory = model.fit(train, target, epochs=100, validation_data=(val_train, val_target), batch_size=10)","a103ebd6":"plt.plot(history.history['accuracy'], label='acc')\nplt.plot(history.history['val_accuracy'], label='val_acc')\nplt.ylim(0, 1)\nplt.legend()","eac55310":"from sklearn import metrics\n\nprediction = model.predict(train) > 0.5\nprediction = (prediction > 0.5) * 1\n\naccuracy_nn = round(metrics.accuracy_score(target, prediction) * 100, 2)","88941bcf":"test_prediction = model.predict(test_train) > 0.5\ntest_prediction = (test_prediction > 0.5) * 1\n\ntest_accuracy_nn = round(metrics.accuracy_score(test_target, test_prediction) * 100, 2)\nprint(test_accuracy_nn)","f89a2996":"from sklearn.linear_model import LogisticRegression\n\nlogreg = LogisticRegression()\n\nlogreg.fit(train, target)\n\naccuracy_logreg = round(logreg.score(train, target) * 100, 2)\nprint(accuracy_logreg)","347cc845":"test_accuracy_logreg = round(logreg.score(test_train, test_target) * 100, 2)\nprint(test_accuracy_logreg)","a64afb00":"coeff_df = pd.DataFrame(train.columns.delete(0))\ncoeff_df.columns = ['Feature']\ncoeff_df[\"Correlation\"] = pd.Series(logreg.coef_[0])\n\ncoeff_df.sort_values(by='Correlation', ascending=False)","e99aefea":"from sklearn.tree import DecisionTreeRegressor\n\ndec_tree_model = DecisionTreeRegressor(max_leaf_nodes=3000)\n\ndec_tree_model.fit(train, target)\n\ndec_tree_prediction = dec_tree_model.predict(train)\ndec_tree_prediction = (dec_tree_prediction > 0.5) * 1\naccuracy_dec_tree = round(metrics.accuracy_score(target, dec_tree_prediction) * 100, 2)\nprint(accuracy_dec_tree)","dfa2d5f7":"dec_tree_test_prediction = dec_tree_model.predict(test_train)\ndec_tree_test_prediction = (dec_tree_test_prediction > 0.5) * 1\ntest_accuracy_dec_tree = round(metrics.accuracy_score(test_target, dec_tree_test_prediction) * 100, 2)\nprint(test_accuracy_dec_tree)","ce51adb1":"from sklearn.ensemble import RandomForestRegressor\n\nrf_model = RandomForestRegressor()\n\nrf_model.fit(train, target)\n\nrf_prediction = rf_model.predict(train)\nrf_prediction = (rf_prediction > 0.5) * 1\naccuracy_rf = round(metrics.accuracy_score(target, rf_prediction) * 100, 2)\nprint(accuracy_rf)","1472a31d":"rf_test_prediction = rf_model.predict(test_train)\nrf_test_prediction = (rf_test_prediction > 0.5) * 1\ntest_accuracy_rf = round(metrics.accuracy_score(test_target, rf_test_prediction) * 100, 2)\nprint(test_accuracy_rf)","2fe2b7f7":"from sklearn.neighbors import KNeighborsClassifier\n\nknn_model = KNeighborsClassifier(n_neighbors=100)   # Tried some values in the range of 0-300, 90-100 seems to work well\n\nknn_model.fit(train, target)\n\nknn_test_prediction = knn_model.predict(test_train)   # Doesn't need to be turned into 1s and 0s because KNN already is a classifier\ntest_accuracy_knn = round(metrics.accuracy_score(test_target, knn_test_prediction) * 100, 2)\nprint(test_accuracy_knn)","6ce4a7fb":"Random Forest","ec08bf80":"KNN","3774da58":"Logistic Regression","6106b11c":"Decision Tree","4b53686b":"The below creation of new features are from Vincent Lugat's kernel.\nhttps:\/\/www.kaggle.com\/vincentlugat\/pima-indians-diabetes-eda-prediction-0-906#4.-New-features-(16)-and-EDA","51715399":"The removal of zeros was done with Vincent Lugat's kernel. https:\/\/www.kaggle.com\/vincentlugat\/pima-indians-diabetes-eda-prediction-0-906#4.-New-features-(16)-and-EDA"}}