{"cell_type":{"220fce2b":"code","e34045b6":"code","940c39ac":"code","a4292253":"code","dfd8c32f":"code","eb8d1eff":"code","02383b5c":"code","47e1983e":"code","8ff92262":"code","d042aca0":"code","65dad012":"code","a09937e4":"code","8ea37e29":"code","b010b3f6":"code","217ee862":"code","dd32cd41":"code","6976ae4d":"code","50fa4e4d":"code","6bab1bb0":"code","35242ae7":"code","5afdd727":"code","ea89125e":"code","41c77366":"code","4bf6470a":"code","b2487f85":"code","5799cea8":"code","d349f35a":"code","adbea50d":"code","649ac96b":"code","bcec68d0":"code","4c7640f7":"code","3c395d54":"code","76706048":"code","7e72b8cb":"code","03917849":"code","af12e65a":"code","f5b540f5":"code","7a5e66e3":"code","c70ca3f4":"code","6b7cc6c1":"code","610c9064":"code","f28927d4":"code","35b81304":"code","0f2166cb":"code","7e75a4f7":"code","e8e8b5dd":"code","715bec1c":"code","0593d569":"code","5049bd0c":"code","ceb1ab65":"code","faa6d9dd":"code","77f744f0":"code","aa797e7b":"code","30f78c52":"code","051601e5":"code","d9b3be54":"markdown","9ddd12ad":"markdown","b4cac05c":"markdown","a9c91c61":"markdown","34e52517":"markdown","b31cda85":"markdown","c12cb299":"markdown","1c72ca24":"markdown","e0d4ee71":"markdown","4fc84278":"markdown","39c9d35a":"markdown","58449bc9":"markdown","6311496e":"markdown","e7718ff2":"markdown","57c16b31":"markdown","faffbd14":"markdown","899cdedd":"markdown","9573a15e":"markdown","ec9b49bb":"markdown","445a39b9":"markdown","023b5289":"markdown"},"source":{"220fce2b":"import os\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.metrics import confusion_matrix","e34045b6":"#Code to ignore warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")","940c39ac":"## Read \"hepatitis.csv\" using pandas\n# target =  1: Die; 2: Live \ndata = pd.read_csv(\"..\/input\/hepatitis-dataset\/hepatitis.csv\", na_values=\"?\")","a4292253":"data.shape","dfd8c32f":"data.head()","eb8d1eff":"data.describe()","02383b5c":"data.target.value_counts()","47e1983e":"data.dtypes","8ff92262":"cat_cols = data.columns[data.nunique() < 5]","d042aca0":"num_cols = data.columns[data.nunique() >= 5]","65dad012":"data.drop([\"ID\"], axis = 1, inplace=True)\nnum_cols = data.columns[data.nunique() >= 5]","a09937e4":"data.head()","8ea37e29":"num_cols = [\"age\", \"bili\", \"alk\", \"sgot\", \"albu\", \"protime\"]\ncat_cols = ['gender', 'steroid', 'antivirals', 'fatigue', 'malaise', 'anorexia', 'liverBig', \n            'liverFirm', 'spleen', 'spiders', 'ascites', 'varices', 'histology']","b010b3f6":"data.isna().sum()","217ee862":"data.isnull().sum()","dd32cd41":"X = data.drop([\"target\"], axis = 1)","6976ae4d":"y = data[\"target\"]","50fa4e4d":"print(X.shape, y.shape)","6bab1bb0":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 123)","35242ae7":"## Print the shape of X_train, X_test, y_train, y_test\nprint(X_train.shape)\nprint(X_test.shape)\nprint(y_train.shape)\nprint(y_test.shape)","5afdd727":"print(y_train.value_counts()\/X_train.shape[0])","ea89125e":"print(y_test.value_counts()\/X_test.shape[0])","41c77366":"# null values in train\nX_train.isna().sum()","4bf6470a":"# null values in test\nX_test.isna().sum()","b2487f85":"df_cat_train = X_train[cat_cols]\ndf_cat_test = X_test[cat_cols]","5799cea8":"# Impute on train\n# df_cat_train = df_cat_train.fillna(df_cat_train.mode().iloc[0])\n\n# Impute on test\n# df_cat_test = df_cat_test.fillna(df_cat_train.mode().iloc[0])","d349f35a":"from sklearn.impute import SimpleImputer\ncat_imputer = SimpleImputer(strategy='most_frequent')\ncat_imputer.fit(df_cat_train)","adbea50d":"df_cat_train = pd.DataFrame(cat_imputer.transform(df_cat_train), columns=cat_cols)","649ac96b":"df_cat_test = pd.DataFrame(cat_imputer.transform(df_cat_test), columns=cat_cols)","bcec68d0":"df_num_train = X_train[num_cols]\ndf_num_test = X_test[num_cols]","4c7640f7":"# Impute on train\n# df_num_train = df_num_train.fillna(df_num_train.mean())\n\n#Impute on test\n# df_num_test = df_num_test.fillna(df_num_train.mean())","3c395d54":"num_imputer = SimpleImputer(strategy='median')\nnum_imputer.fit(df_num_train[num_cols])","76706048":"df_num_train = pd.DataFrame ( num_imputer.transform(df_num_train), columns= num_cols)","7e72b8cb":"df_num_test =  pd.DataFrame(num_imputer.transform(df_num_test), columns=num_cols)","03917849":"# Combine numeric and categorical in train\nX_train = pd.concat([df_num_train, df_cat_train], axis = 1)\n\n# Combine numeric and categorical in test\nX_test = pd.concat([df_num_test, df_cat_test], axis = 1)","af12e65a":"X_train.isna().sum()","f5b540f5":"X_test.isna().sum()","7a5e66e3":"# Train\nX_train[cat_cols] = X_train[cat_cols].astype('int')\n\n# Test\nX_test[cat_cols] = X_test[cat_cols].astype('int')","c70ca3f4":"## Convert Categorical Columns to Dummies\n# Train\nX_train = pd.get_dummies(X_train, columns=cat_cols, drop_first=True)\n\n# Test\nX_test = pd.get_dummies(X_test, columns=cat_cols, drop_first=True)","6b7cc6c1":"X_train.columns","610c9064":"X_test.columns","f28927d4":"from sklearn.preprocessing import StandardScaler","35b81304":"#num_cols = [\"age\", \"bili\", \"alk\", \"sgot\", \"albu\", \"protime\"]\nscaler = StandardScaler()\n\nscaler.fit(X_train.loc[:,num_cols])\n\n# scale on train\nX_train.loc[:,num_cols] = scaler.transform(X_train.loc[:,num_cols])\n#X_train[num_cols] = scaler.transform(X_train[num_cols])\n\n# scale on test\nX_test.loc[:,num_cols] = scaler.transform(X_test.loc[:,num_cols])","0f2166cb":"from sklearn.svm import SVC","7e75a4f7":"# Create a SVC classifier using a linear kernel\nlinear_svm = SVC(kernel='linear', C=1, random_state=0)","e8e8b5dd":"# Train the classifier\nlinear_svm.fit(X=X_train, y= y_train)","715bec1c":"## Predict\ntrain_predictions = linear_svm.predict(X_train)\ntest_predictions = linear_svm.predict(X_test)\n\n### Train data accuracy\nfrom sklearn.metrics import accuracy_score,f1_score,confusion_matrix\n\nprint(\"TRAIN Conf Matrix : \\n\", confusion_matrix(y_train, train_predictions))\nprint(\"\\nTRAIN DATA ACCURACY\",accuracy_score(y_train,train_predictions))\nprint(\"\\nTrain data f1-score for class '1'\",f1_score(y_train,train_predictions,pos_label=1))\nprint(\"\\nTrain data f1-score for class '2'\",f1_score(y_train,train_predictions,pos_label=2))\n\n### Test data accuracy\nprint(\"\\n\\n--------------------------------------\\n\\n\")\n\nprint(\"TEST Conf Matrix : \\n\", confusion_matrix(y_test, test_predictions))\nprint(\"\\nTEST DATA ACCURACY\",accuracy_score(y_test,test_predictions))\nprint(\"\\nTest data f1-score for class '1'\",f1_score(y_test,test_predictions,pos_label=1))\nprint(\"\\nTest data f1-score for class '2'\",f1_score(y_test,test_predictions,pos_label=2))","0593d569":"## Create an SVC object and print it to see the arguments\nsvc = SVC(kernel='rbf', random_state=0, gamma=0.01, C=1)\nsvc","5049bd0c":"## Train the model\nsvc.fit(X=X_train, y= y_train)","ceb1ab65":"## Predict\ntrain_predictions = svc.predict(X_train)\ntest_predictions = svc.predict(X_test)\n\n### Train data accuracy\n\nprint(\"TRAIN Conf Matrix : \\n\", confusion_matrix(y_train, train_predictions))\nprint(\"\\nTRAIN DATA ACCURACY\",accuracy_score(y_train,train_predictions))\nprint(\"\\nTrain data f1-score for class '1'\",f1_score(y_train,train_predictions,pos_label=1))\nprint(\"\\nTrain data f1-score for class '2'\",f1_score(y_train,train_predictions,pos_label=2))\n\n### Test data accuracy\nprint(\"\\n\\n--------------------------------------\\n\\n\")\n\nprint(\"TEST Conf Matrix : \\n\", confusion_matrix(y_test, test_predictions))\nprint(\"\\nTEST DATA ACCURACY\",accuracy_score(y_test,test_predictions))\nprint(\"\\nTest data f1-score for class '1'\",f1_score(y_test,test_predictions,pos_label=1))\nprint(\"\\nTest data f1-score for class '2'\",f1_score(y_test,test_predictions,pos_label=2))","faa6d9dd":"## Use Grid Search for parameter tuning\n\nfrom sklearn.model_selection import GridSearchCV\n\nsvc_grid = SVC()\n \nparam_grid = { \n                'C': [0.001, 0.01, 0.1, 1, 10, 100 ],\n                'gamma': [0, 0.0001, 0.001, 0.01, 0.1, 1, 10, 100], \n                'kernel':['linear', 'rbf', 'poly' ]\n             }\n\nsvc_cv_grid = GridSearchCV(estimator = svc_grid, param_grid = param_grid, cv = 5, verbose=3)","77f744f0":"## Fit the grid search model\nsvc_cv_grid.fit(X=X_train, y=y_train)","aa797e7b":"# Get the best parameters\nsvc_cv_grid.best_params_","30f78c52":"svc_best = svc_cv_grid.best_estimator_","051601e5":"## Predict\ntrain_predictions = svc_best.predict(X_train)\ntest_predictions = svc_best.predict(X_test)\n\nprint(\"TRAIN DATA ACCURACY\",accuracy_score(y_train,train_predictions))\nprint(\"\\nTrain data f1-score for class '1'\",f1_score(y_train,train_predictions,pos_label=1))\nprint(\"\\nTrain data f1-score for class '2'\",f1_score(y_train,train_predictions,pos_label=2))\n\n### Test data accuracy\nprint(\"\\n\\n--------------------------------------\\n\\n\")\nprint(\"TEST DATA ACCURACY\",accuracy_score(y_test,test_predictions))\nprint(\"\\nTest data f1-score for class '1'\",f1_score(y_test,test_predictions,pos_label=1))\nprint(\"\\nTest data f1-score for class '2'\",f1_score(y_test,test_predictions,pos_label=2))","d9b3be54":"#### 9. Split the data into X_train, X_test, y_train, y_test with test_size = 0.20 using sklearn","9ddd12ad":"###### 2. Check basic summary statistics of the data","b4cac05c":"#### 5. Drop columns which are not significant","a9c91c61":"Radial Basis Function is a commonly used kernel in SVC:<br>\n\n<img src=\"rbf_kernel.png\">\n\nwhere <math xmlns=\"http:\/\/www.w3.org\/1998\/Math\/MathML\">\n  <mrow class=\"MJX-TeXAtom-ORD\">\n    <mo stretchy=\"false\">|<\/mo>\n  <\/mrow>\n  <mrow class=\"MJX-TeXAtom-ORD\">\n    <mo stretchy=\"false\">|<\/mo>\n  <\/mrow>\n  <mrow class=\"MJX-TeXAtom-ORD\">\n    <mi mathvariant=\"bold\">x<\/mi>\n      <sub>i<\/sub>\n  <\/mrow>\n  <mo>&#x2212;<!-- \u2212 --><\/mo>\n  <mrow class=\"MJX-TeXAtom-ORD\">\n    <msup>\n      <mi mathvariant=\"bold\">x<\/mi>\n      <sub>j<\/sub>\n    <\/msup>\n  <\/mrow>\n  <mrow class=\"MJX-TeXAtom-ORD\">\n    <mo stretchy=\"false\">|<\/mo>\n  <\/mrow>\n  <msup>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mo stretchy=\"false\">|<\/mo>\n    <\/mrow>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <sup>2<\/sup>\n    <\/mrow>\n  <\/msup>\n<\/math>  is the squared Euclidean distance between two data points x<sub>i<\/sub> and x<sub>j<\/sub>\n\nIt is only important to know that an SVC classifier using an RBF kernel has two parameters: gamma and C.\n\n<strong>Gamma:<\/strong>\n\n- Gamma is a parameter of the RBF kernel and can be thought of as the \u2018spread\u2019 of the kernel and therefore the decision region. When gamma is low, the \u2018curve\u2019 of the decision boundary is very low and thus the decision region is very broad. When gamma is high, the \u2018curve\u2019 of the decision boundary is high, which creates islands of decision-boundaries around data points.\n\n<strong>C:<\/strong>\n\n- C is a parameter of the SVC learner and is the penalty for misclassifying a data point. When C is small, the classifier is okay with misclassified data points (high bias, low variance). When C is large, the classifier is heavily penalized for misclassified data and therefore bends over backwards avoid any misclassified data points (low bias, high variance).\n\n\n<strong>Kernel Trick:<\/strong><br>\nImage you have a two-dimensional non-linearly separable dataset, you would like to classify it using SVM. It looks like not possible because the data is not linearly separable. However, if we transform the two-dimensional data to a higher dimension, say, three-dimension or even ten-dimension, we would be able to find a hyperplane to separate the data.\n\n<img src=\"kernel_trick.png\">\n\nThe problem is, if we have a large dataset containing, say, millions of examples, the transformation will take a long time to run.<br>\nTo solve this problem, we actually only care about the result of the dot product (x<sub>i<\/sub>.x<sub>j<\/sub>)<br>\n<br>If there is a function which could calculate the dot product and the result is the same as when we transform the data into higher dimension, it would be fantastic. This function is called a kernel function.<br>\n<br>In essence, what the kernel trick does for us is to offer a more efficient and less expensive way to transform data into higher dimensions.","34e52517":"## Dataset Reading and Pre-Processing steps","b31cda85":"#### 10. Check null values in train and test, check value_counts in y_train and y_test","c12cb299":"### SVM with Grid Search for Paramater Tuning","1c72ca24":"#### 7. Checking the null values","e0d4ee71":"#### 11. Impute the Categorical Columns with mode and Numerical columns with mean","4fc84278":"#### Convert all the categorical columns to Integer Format before dummification (2.0 as 2 etc.)","39c9d35a":"#### 8. Split the data into X and y","58449bc9":"#### 4. Check the datatype of each variable","6311496e":"###### 3. Check for value counts in target variable","e7718ff2":"####  Non Linear SVM (RBF)","57c16b31":"import required libraries","faffbd14":"**Attribute information:**\n\n1. **target**: DIE (1), LIVE (2)\n2. **age**: 10, 20, 30, 40, 50, 60, 70, 80\n3. **gender**: male (1), female (2)\n\n           ------ no = 2,   yes = 1 ------\n\n4. **steroid**: no, yes \n5. **antivirals**: no, yes \n6. **fatique**: no, yes \n7. **malaise**: no, yes \n8. **anorexia**: no, yes \n9. **liverBig**: no, yes \n10. **liverFirm**: no, yes \n11. **spleen**: no, yes \n12. **spiders**: no, yes\n13. **ascites**: no, yes \n14. **varices**: no, yes\n15. **histology**: no, yes\n\n\n16. **bilirubin**: 0.39, 0.80, 1.20, 2.00, 3.00, 4.00 -- \n17. **alk**: 33, 80, 120, 160, 200, 250 ---\n18. **sgot**: 13, 100, 200, 300, 400, 500, ---\n19. **albu**: 2.1, 3.0, 3.8, 4.5, 5.0, 6.0, --- \n20. **protime**: 10, 20, 30, 40, 50, 60, 70, 80, 90, --- \n\n        NA's are represented with \"?\"","899cdedd":"#### 6. Identify the Categorical Columns and store them in a variable cat_cols and numerical into num_cols","9573a15e":"#### 13. Scale the numeric attributes [\"age\", \"bili\", \"alk\", \"sgot\", \"albu\", \"protime\"]","ec9b49bb":"#### 12. Dummify the Categorical columns","445a39b9":"###### 1. Read the HEPATITIS dataset and check the data shapes","023b5289":"## MODEL BUILDING - SVM"}}