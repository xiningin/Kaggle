{"cell_type":{"3a112bb4":"code","f758f667":"code","250784b9":"code","1c921fa8":"code","8c714e97":"code","c2254485":"code","c7b4239a":"code","d1dc7de5":"code","07c26f13":"code","d0b10789":"code","663c063f":"code","58826a7f":"code","aa7228e3":"code","8cfa831f":"code","8cb0ed5a":"code","433064bb":"code","1d6cea59":"code","7f0ba391":"code","91d91b7f":"code","95bda567":"code","f0ccd6fd":"code","c7f1cb2c":"code","88b3a31e":"code","3a022ab5":"code","4013154d":"code","4c60af0f":"code","19307ba9":"code","96fd2fd5":"code","01fba23f":"code","605a569e":"code","c4ff2aaa":"code","212a74fb":"code","342eff36":"code","5b1bb3a8":"code","0358b9ad":"code","6a1507d9":"code","8fc5e223":"code","006d6b80":"code","6b8200ac":"code","1874a953":"code","2129ffac":"code","04e3a89a":"code","28112196":"code","50c45065":"code","f1ca6bb5":"code","a7f02e5c":"code","18b5cb87":"code","da9e28ff":"code","1f8d1d07":"code","f009447a":"code","400fa964":"code","582fbc75":"code","03349511":"code","bd6baef3":"code","71e95600":"code","abc563a0":"code","6ae75dbc":"code","3f528149":"code","704bcf95":"markdown","0c782023":"markdown","a0f236e9":"markdown","fb083872":"markdown","e61a4cbb":"markdown","b9d75ac0":"markdown","622df4fc":"markdown","7628dc64":"markdown","5f9294b0":"markdown","64087fd2":"markdown","0479ac06":"markdown","3d680c46":"markdown","9381b199":"markdown","2663a665":"markdown"},"source":{"3a112bb4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","f758f667":"# Code you have previously used to load data\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\n#from learntools.core import *\n\n# Path of the file to read. We changed the directory structure to simplify submitting to a competition\niowa_file_path = '..\/input\/home-data-for-ml-course\/train.csv'\ntest_file_path = '..\/input\/home-data-for-ml-course\/test.csv'\n\n# Read the data\nhouse_data = pd.read_csv(iowa_file_path,index_col=0)\nhouse_test_data = pd.read_csv(test_file_path, index_col=0)","250784b9":"house_data.shape","1c921fa8":"%%HTML\n<style type=\"text\/css\">\ntable.dataframe td, table.dataframe th {\n    border: 1px  black solid !important;\n  color: black !important;\n}\n<\/style>","8c714e97":"#print the whole table without omission\npd.options.display.max_columns = None\nhouse_data.head()","c2254485":"numerical_cols = [cname for cname in house_data.columns if house_data[cname].dtype in ['int64', 'float64']]\nhouse_data[numerical_cols].describe().round(decimals=2)","c7b4239a":"label=house_data.SalePrice\nplt.figure()\nsns.distplot(label)\nplt.title('Distribution of SalePrice')\nplt.xticks([0, 2e5, 4e5,6e5, 8e5])\nplt.xlabel(\"price\")\nplt.ylabel(\"Prob density distribution\")\nplt.show()","d1dc7de5":"label_log=np.log(house_data.SalePrice)\nsns.distplot(label_log)\nplt.title('Distribution of Log-transformed SalePrice')\nplt.xlabel('log(SalePrice)')\nplt.show()","07c26f13":"print('SalePrice (same as to z_score) has a skew of ' + str(label.skew().round(decimals=1)) + \n      '  the log-transformed SalePrice improves the skew to ' + \n      str(np.log(label).skew().round(decimals=1)))","d0b10789":"#Show columns with most null values:\nhouse_data[numerical_cols].isna().sum().sort_values(ascending=False).head()","663c063f":"num_attributes = house_data[numerical_cols].drop('SalePrice', axis=1).copy()\n\nfig = plt.figure(figsize=(12,16))\nfor i in range(len(num_attributes.columns)):\n    fig.add_subplot(9,4,i+1)\n    sns.distplot(num_attributes.iloc[:,i].dropna(),kde=False)\n    plt.xlabel(num_attributes.columns[i])\n\nplt.tight_layout()\nplt.show()","58826a7f":"f = plt.figure(figsize=(12,20))\nfor i in range(len(num_attributes.columns)):\n    f.add_subplot(9, 4, i+1)\n    sns.scatterplot(num_attributes.iloc[:,i], label)\nplt.tight_layout()\nplt.show()","aa7228e3":"correlation = house_data.corr()\nf, ax = plt.subplots(figsize=(18,16))\nplt.title('Correlation of numerical attributes', size=16)\nsns.heatmap(correlation)\nplt.show()","8cfa831f":"corrList_Order=correlation['SalePrice'].sort_values(ascending=False)[1:]\nprint(corrList_Order)","8cb0ed5a":"corr_to_price=correlation['SalePrice']\nf = plt.figure(figsize=(12,20))\nfor i in range(len(num_attributes.columns)):\n    f.add_subplot(9, 4, i+1)\n    sns.scatterplot(num_attributes.loc[:,corrList_Order.index[i]], label)\n    plt.title('Corr to SalePrice = '+ str(np.around(corrList_Order[i], decimals=3)))\nplt.tight_layout()\nplt.show()","433064bb":"#check how many Garage built later\nhouse_data[['GarageYrBlt','YearBuilt']].T\ntt=(house_data.GarageYrBlt-house_data.YearBuilt)\n#plt.hist(tt)\n#plt.show()\nprint( 'totoal Null:{}'.format(tt.isna().sum()))\nprint( 'totoal remodeled Garage:{}'.format(tt[tt>0].count()))\nprint( 'error data with garage built earlier than house:{}'.format(tt[tt<0].count()))","1d6cea59":"# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_cols = [cname for cname in house_data.columns if\n               #     house_data[cname].nunique() < 10 and \n                    house_data[cname].dtype == \"object\"]\nprint(categorical_cols,'\\n Total categorical data=', len(categorical_cols))","7f0ba391":"house_data[categorical_cols].describe()","91d91b7f":"# \"Cardinality\" means the number of unique values in a column\n# Select categorical columns with relatively low cardinality (convenient but arbitrary)\ncategorical_highCard_cols = [cname for cname in house_data.columns if\n                    house_data[cname].nunique() > 9 and \n                    house_data[cname].dtype == \"object\"]\nprint(categorical_highCard_cols,'\\n Total categorical data=', len(categorical_highCard_cols))","95bda567":"cat_attributes = house_data[categorical_cols].copy()\ncat_attributes\ncat_attributes.isna().sum().sort_values(ascending=False).head(17)","f0ccd6fd":"#Categorical data to numerical data for better (importance) analysis \nNumCol_3=['ExterQual','ExterCond',\"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\",\"BsmtFinType2\",\"HeatingQC\", \"CentralAir\",\n          \"Electrical\",\"KitchenQual\", \"Functional\",'FireplaceQu','GarageCond', 'GarageQual']#,'Fence']# 'PoolQC'\ncat_attributes[NumCol_3].isna().sum().sum()","c7f1cb2c":"#Need to read data description, need to change all the data with ratings to numerical data\n#Those categorical data needs to change to numerical\n#I assume linear changes for different level of ratings\n\nNumCol_3=['ExterQual','ExterCond',\"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\",\"BsmtFinType2\",\"HeatingQC\", \"CentralAir\",\n          \"Electrical\",\"KitchenQual\", \"Functional\",'FireplaceQu','GarageCond', 'GarageQual']#,'Fence']# 'PoolQC'\n\nfor cat in NumCol_3:\n    cat_attributes[cat] = cat_attributes[cat].fillna(\"None\")\n\nf = plt.figure(figsize=(12,20))\nfor i in range(len(NumCol_3)):\n    f.add_subplot(6, 4, i+1)\n    sns.barplot(cat_attributes[NumCol_3[i]].value_counts().index, cat_attributes[NumCol_3[i]].value_counts().values)\n    plt.title(str(cat_attributes[NumCol_3[i]].name))\nplt.tight_layout()\nplt.show()\n#    plt.title('Corr to SalePrice = '+ str(np.around(corrList_Order[i], decimals=3)))\n  \n#cat_attributes.ExterQual.value_counts().values\n#sns.barplot(cat_attributes.ExterQual.value_counts().index,cat_attributes.ExterQual.value_counts().values)","88b3a31e":"#best value should be based on regression, but i use given number for now\ndef cleUp(X):\n    catcols = [cname for cname in X.columns if\n         #           house_data[cname].nunique() < 2 and \n                    X[cname].dtype == \"object\"]\n#    for cat in catcols:\n#                    X[cat] =  X[cat].fillna(\"None\") \n####################do not fill na to None, because, after it changed to numerical data, fit_transform the NA could get better results\n    cleanup_nums = {\"ExterQual\":     {\"Ex\": 5, \"Gd\": 4,\"TA\":3, \"Fa\":2, \"Po\":1,\"None\":0},\n                \"ExterCond\":     {\"Ex\": 5, \"Gd\": 4,\"TA\":3, \"Fa\":2, \"Po\":1,\"None\":0},\n                 \"BsmtQual\":     {\"Ex\": 5, \"Gd\": 4,\"TA\":3, \"Fa\":2, \"Po\":1, \"None\":0},\n                \"BsmtCond\":     {\"Ex\": 5, \"Gd\": 4,\"TA\":3, \"Fa\":2, \"Po\":1, \"None\":0},\n                 \"BsmtExposure\":     { \"Gd\": 4,\"Av\":3, \"Mn\":2, \"No\":1, \"None\":0},\n                 \"BsmtFinType1\":     { \"GLQ\": 6, \"ALQ\":5, \"BLQ\":4, \"Rec\":3, \"LwQ\":2, \"Unf\":1, \"None\":0},\n                 \"BsmtFinType2\":     { \"GLQ\": 6, \"ALQ\":5, \"BLQ\":4, \"Rec\":3, \"LwQ\":2, \"Unf\":1, \"None\":0},\n                 \"HeatingQC\":     {\"Ex\": 5, \"Gd\": 4,\"TA\":3, \"Fa\":2, \"Po\":1,\"None\":0},\n                \"CentralAir\":     {\"Y\": 1, \"N\": 0,\"None\":0},\n                \"Electrical\":     {\"SBrkr\": 4, \"FuseA\": 3,\"FuseF\":2, \"FuseP\":1, \"Mix\": 2.5, \"None\": 0},\n                \"KitchenQual\":    {\"Ex\": 5, \"Gd\": 4,\"TA\":3, \"Fa\":2, \"Po\":1,\"None\":0},\n                \"Functional\":  {\"Typ\": 7,\"Min1\":6, \"Min2\":5, \"Mod\":4, \"Maj1\":3, \"Maj2\":2, \"Sev\":1, \"Sal\":0 ,\"None\":0 },\n                'FireplaceQu':  {\"Ex\": 5, \"Gd\": 4,\"TA\":3, \"Fa\":2, \"Po\":1, \"None\":0},\n                'GarageQual':  {\"Ex\": 5, \"Gd\": 4,\"TA\":3, \"Fa\":2, \"Po\":1, \"None\":0},\n                'GarageCond':  {\"Ex\": 5, \"Gd\": 4,\"TA\":3, \"Fa\":2, \"Po\":1, \"None\":0},\n    #           'PoolQC':   {\"Ex\": 5, \"Gd\": 4,\"TA\":3, \"Fa\":2, \"Po\":1,\"None\":0},\n    #            'Fence':   {\"GdPrv\": 4,\"MnPrv\":3, \"GdWo\":2, \"MnWw\":1,\"None\":0}\n               }\n    X=X.replace(cleanup_nums)\n    return X\n#cat_attributes = cat_attributes.replace(cleanup_nums)\n","3a022ab5":"cat_attributes=cleUp(cat_attributes)\ncat_attributes.head()","4013154d":"cat_num=cat_attributes[NumCol_3].copy()\ncat_num=cat_num.join(house_data.SalePrice)\ncat_num","4c60af0f":"correlation2 = cat_num.corr()\nf, ax = plt.subplots(figsize=(12,9))\nplt.title('Correlation of numerical attributes', size=16)\nsns.heatmap(correlation2)\nplt.show()","19307ba9":"corrList_Order2=correlation2['SalePrice'].sort_values(ascending=False)[0:]\ncorrList_Order2","96fd2fd5":"len(cat_num.columns)","01fba23f":"corr_to_price=correlation2['SalePrice']\nf = plt.figure(figsize=(10,16))\nfor i in range(len(cat_num.columns)-1):\n    f.add_subplot(6, 4, i+1)\n    sns.scatterplot(cat_num.loc[:,corrList_Order2.index[i]], label)\n    plt.title('Corr to SalePrice = '+ str(np.around(corrList_Order2[i], decimals=3)))\nplt.tight_layout()\nplt.show()","605a569e":"house_data.YrSold.unique()","c4ff2aaa":"def prepro(X):\n    \n    #X=XX.copy()\n    NumCol_drop=[ 'MiscVal','BsmtHalfBath','LowQualFinSF', \n                 #,'BsmtFullBath','HalfBath','BedroomAbvGr',\n                 '3SsnPorch','PoolArea' #, #low correlaton\n             #,'GarageYrBlt'\n                ]#,'GarageArea']# 'TotRmsAbvGrd'] # high corr with other attributes\n    X.drop(NumCol_drop, axis=1, inplace=True)\n    \n    #numerical data  1) remove na, which is not as good as impute.fit_transform ; 2) change scale, remove baseline 3)create indicator column if data not applicable\n    \n    # change to LotFrontage\n    LotFrontage_upper=200\n  #  X.LotFrontage=X.LotFrontage.fillna(X['LotFrontage'].mean())  #do not fill by mean, fit_transform may do better\n    X.LotFrontage=(X.LotFrontage.clip(upper=LotFrontage_upper)\/X.LotFrontage.std())\n    \n    # change to LotArea\n\n    LotArea_upper=50000\n    X.LotArea=(X.LotArea.clip(upper=LotArea_upper)\/X.LotArea.std())\n    \n    #MasVnrArea\n    \n    MasVnrArea_upper=1500\n#    X.MasVnrArea = X.MasVnrArea.fillna(X.MasVnrArea.mean())\n    X.MasVnrArea=(X.MasVnrArea.clip(upper=MasVnrArea_upper)\/X.MasVnrArea.std())\n\n    #OverallQual\n    # X.OverallQual=X.OverallQual.fillna(X.OverallQual.mean())\n    #X.OverallCond=X.OverallCond.fillna(X.OverallCond.mean())\n   \n    year_low=1800\n    X.YearBuilt=(X.YearBuilt-year_low)\/100\n    \n  #  X['GarageYrBlt'].astype('int')\n  #  X['YearBuilt'].astype('int')\n    temp=X['GarageYrBlt']-X['YearBuilt']\n    X.GarageYrBlt=(X.GarageYrBlt-year_low)\/100\n    ind=[0 if i<0.1 else 1 for i in temp]\n    dat=pd.DataFrame({'ReGarageYrBlt_ind': ind})\n    dat.index=np.arange(1,len(dat)+1)\n    X['ReGarageYrBlt_ind']=dat\n    \n    \n    X.YearRemodAdd=(X.YearRemodAdd-year_low)\/100\n\n    temp=X['YearRemodAdd']-X['YearBuilt']\n    ind=[0 if i==0 else 1 for i in temp]\n    dat=pd.DataFrame({'YearRemodAdd_ind': ind})\n    dat.index=np.arange(1,len(dat)+1)\n    X['YearRemodAdd_ind']=dat\n    \n    TotalBsmtSF_upper=4000\n    X.TotalBsmtSF=(X.TotalBsmtSF.clip(upper=TotalBsmtSF_upper)\/X.TotalBsmtSF.std())\n\n    ind=[0 if i==0 else 1 for i in X.TotalBsmtSF]\n    dat=pd.DataFrame({'Basement_ind': ind})\n    dat.index=np.arange(1,len(dat)+1)\n    X['Basement_ind']=dat\n    \n    FFlr_upper=X['1stFlrSF'].nlargest(2).values[1]\n    X['1stFlrSF']=(X['1stFlrSF'].clip(upper=FFlr_upper)\/X['1stFlrSF'].std())\n    \n    X['2ndFlrSF']=X['2ndFlrSF']\/X['2ndFlrSF'].std()\n    ind=[0 if i==0 else 1 for i in X['2ndFlrSF']]\n    dat=pd.DataFrame({'2ndFlrSF_ind': ind})\n    dat.index=np.arange(1,len(dat)+1)\n    X['2ndFlrSF_ind']=dat\n \n    GrLivArea_upper=X['GrLivArea'].nlargest(2).values[1]\n    X['GrLivArea']=X['GrLivArea'].clip(upper=GrLivArea_upper)\/X['GrLivArea'].std()\n\n    WoodDeckSF_upper=7000\n    X['WoodDeckSF']=X['WoodDeckSF'].clip(upper=WoodDeckSF_upper)\/X['WoodDeckSF'].std()\n    ind=[0 if i==0 else 1 for i in X['WoodDeckSF']]\n    dat=pd.DataFrame({'WoodDeckSF_ind': ind})\n    dat.index=np.arange(1,len(dat)+1)\n    X['WoodDeckSF_ind']=dat\n    \n    X['OpenPorchSF']=X['OpenPorchSF'].clip(upper=400)\/X['OpenPorchSF'].std()\n    ind=[0 if i==0 else 1 for i in X['OpenPorchSF']]\n    dat=pd.DataFrame({'OpenPorchSF_ind': ind})\n    dat.index=np.arange(1,len(dat)+1)\n    X['OpenPorchSF_ind']=dat\n    \n    BsmtFinSF1_upper=X['BsmtFinSF1'].nlargest(2).values[1]\n    X['BsmtFinSF1']=X['BsmtFinSF1'].clip(upper=BsmtFinSF1_upper)\/X['BsmtFinSF1'].std()\n    ind=[0 if i==0 else 1 for i in X['BsmtFinSF1']]\n    dat=pd.DataFrame({'BsmtFinSF1_ind': ind})\n    dat.index=np.arange(1,len(dat)+1)\n    X['BsmtFinSF1_ind']=dat\n    \n    X.EnclosedPorch=X.EnclosedPorch\/100\n    X.BsmtUnfSF=X.BsmtUnfSF\/X.BsmtUnfSF.std()\n    X.GarageArea=X.GarageArea\/X.GarageArea.std()\n    \n    ######### bin date into 4 quarters at 5 years, 20 bins.\n    #Year 2006-2010\n  # feature_column=['20061','20062','20063','20064','20071','20072','20073','20074',\n   #                '20081','20082','20083','20084','20091','20092','20093','20094',\n    #               '20101','20102','20103','20104'] \n    feature_column=['Q1','Q2','Q3','Q4']\n    d = pd.DataFrame(0, index=np.arange(len(X.YrSold))+1, columns=feature_column)\n\n    for i in X.index: \n        temp=X.MoSold[i]\n        if  temp>2 and temp<6:\n            tempQ=2\n        elif temp>5 and temp<9:\n            tempQ=3\n        elif temp>8 and temp<12:\n            tempQ=4\n        else:\n            tempQ=1\n        tempQ=int(tempQ)\n    #    tempY=int(X.YrSold[i])\n     #   assert type(tempQ)=='int'\n     #   d[str(tempY)+str(tempQ)][i]=1\n        d['Q'+str(tempQ)][i]=1\n    d.index.name='Id'\n    d.astype('int')\n    X=X.join(d)\n    X.drop(['MoSold'], axis=1, inplace=True)\n    \n    ###########categorical\n    cat_drop=[ 'Alley','ExterCond','BsmtFinType2','PoolQC','Fence']\n    X.drop(cat_drop, axis=1, inplace=True)\n    \n    ####change categorical data to numerical data\n    NumCol_3=['ExterQual','ExterCond',\"BsmtQual\", \"BsmtCond\", \"BsmtExposure\", \"BsmtFinType1\",\"BsmtFinType2\",\"HeatingQC\", \"CentralAir\",\n          \"Electrical\",\"KitchenQual\", \"Functional\",'FireplaceQu','GarageCond', 'GarageQual']#'Fence']\n\n    X=cleUp(X)\n    \n  \n     ################## change numerical to cat\n    #change MSSubClass to categorical , use 'object' instead of 'category'\n    X.MSSubClass=X.MSSubClass.astype('object')\n    X.OverallCond=X.OverallCond.astype('object')\n    \n    ##############combine one numerical and one category to make one feature: Material x area\n    #X[['MasVnrType', 'MasVnrArea']]\n    MasVnrTotal=pd.get_dummies(X.MasVnrType)\n    MasVnrTotal=MasVnrTotal.mul(X.MasVnrArea, axis=0)\n    X=X.join(MasVnrTotal)\n    X.drop(['MasVnrType', 'MasVnrArea'], axis=1, inplace=True)\n    \n    ### combine condition1 and condition 2 features for single item.\n    '''\n    tempC1=pd.get_dummies(X.Condition1)\n    tempC2=pd.get_dummies(X.Condition2)\n    tempC2['RRNe']=tempC1['RRNe']\n    ConditionTotal=(tempC1 | tempC2)\n    ConditionTotal.drop('Norm', axis=1)\n    X=X.join(ConditionTotal)\n    X.drop(['Condition1','Condition2'], axis=1, inplace=True)  ##should not combine, which makes it much worse\n    '''    \n \n\n     #########other categorical data\n    \n    categorical_cols = [cname for cname in X.columns if\n         #           house_data[cname].nunique() < 2 and \n                      X[cname].dtype == \"object\"]\n    X.drop([# 'MSZoning','Utilities', 'HouseStyle','BldgType','RoofStyle','RoofMatl','LotConfig','Street','PavedDrive',,\n          'Exterior1st','Exterior2nd', #'Neighborhood',  #too many\n           'LandSlope', 'MiscFeature' #useless\n    #      'Foundation','LandContour','LotShape','SaleType','SaleCondition'\n             ], axis=1, inplace=True) #one-hot lower the Lasso\n    \n    X=pd.get_dummies(X)\n    \n    return X\n","212a74fb":"def prepro_simple(X):\n    \n        #X=XX.copy()\n    NumCol_drop=[#  'YrSold',# 'MoSold', #'MSSubClass',\n                 'MiscVal','BsmtHalfBath',#'LowQualFinSF',\n                 #,'BsmtFullBath','HalfBath','BedroomAbvGr',\n                 '3SsnPorch','PoolArea' #, #low correlaton\n                 #,'GarageYrBlt'\n                ]#,'GarageArea']# 'TotRmsAbvGrd'] # high corr with other attributes\n    X.drop(NumCol_drop, axis=1, inplace=True)\n\n   # X.MasVnrArea = X.MasVnrArea.fillna(0)\n    \n    #change MSSubClass to categorical , use 'object' instead of 'category'\n    X.MSSubClass=X.MSSubClass.astype('object')\n#    X.OverallCond=X.OverallCond.astype('object')\n    \n    # Remove attributes that were identified for excluding when viewing scatter plots & corr values\n    cat_attributes_drop = [\n                       'Alley','ExterCond','BsmtFinType2','PoolQC','Fence',#\n                       #         'Exterior1st','Exterior2nd',# 'Neighborhood',  #too many, but I got better results with them\n                   'GarageArea',  'TotRmsAbvGrd'] #,'GarageYrBlt',] # high corr with other attributes\n    X.drop(cat_attributes_drop, axis=1, inplace=True)\n    \n    temp=X['GarageYrBlt']-X['YearBuilt']\n  #  X.GarageYrBlt=(X.GarageYrBlt-year_low)\/100\n    ind=[0 if i<0.1 else 1 for i in temp]\n    dat=pd.DataFrame({'ReGarageYrBlt_ind': ind})\n    dat.index=np.arange(1,len(dat)+1)\n    X['ReGarageYrBlt_ind']=dat\n   \n        ######### bin date into 4 quarters at 5 years, 20 bins.\n    #Year 2006-2010\n  # feature_column=['20061','20062','20063','20064','20071','20072','20073','20074',\n   #                '20081','20082','20083','20084','20091','20092','20093','20094',\n    #               '20101','20102','20103','20104'] \n    feature_column=['Q1','Q2','Q3','Q4']\n    d = pd.DataFrame(0, index=np.arange(len(X.MoSold))+1, columns=feature_column)\n\n    for i in X.index: \n        temp=X.MoSold[i]\n        if  temp>2 and temp<6:\n            tempQ=2\n        elif temp>5 and temp<9:\n            tempQ=3\n        elif temp>8 and temp<12:\n            tempQ=4\n        else:\n            tempQ=1\n        tempQ=int(tempQ)\n    #    tempY=int(X.YrSold[i])\n     #   assert type(tempQ)=='int'\n     #   d[str(tempY)+str(tempQ)][i]=1\n        d['Q'+str(tempQ)][i]=1\n    d.index.name='Id'\n    d.astype('int')\n    X=X.join(d)\n    X.drop(['MoSold'], axis=1, inplace=True) \n\n##########################################################################\n    # Categorical columns:\n    cat_cols_fill_none = ['MiscFeature',  'FireplaceQu', #'Fence','Alley', 'PoolQC',  'BsmtFinType2',\n                     'GarageCond', 'GarageQual', 'GarageFinish', 'GarageType',\n                     'BsmtExposure', 'BsmtFinType1', 'BsmtQual', 'BsmtCond'\n                     ,'MasVnrType'\n                         ]\n    for cat in cat_cols_fill_none:\n         X[cat] = X[cat].fillna(\"None\")\n            \n    #outlier change\n    LotFrontage_upper=200\n    X.LotFrontage.clip(upper=LotFrontage_upper)\n    LotArea_upper=50000\n    X.LotArea.clip(upper=LotArea_upper)       \n    BsmtFinSF1_upper=4000\n    X.BsmtFinSF1.clip(upper=BsmtFinSF1_upper)  \n    TotalBsmtSF_upper=6000\n    X.TotalBsmtSF.clip(upper=TotalBsmtSF_upper)\n    FstFlrSF_upper=4000\n    X['1stFlrSF'].clip(upper=FstFlrSF_upper)\n    LowQualFinSF_upper=550\n    X.LowQualFinSF.clip(upper=LowQualFinSF_upper)\n    \n    X=pd.get_dummies(X)\n    \n    return X","342eff36":"#batch 1 for numeric cols\n\nX=house_data.copy()\n\n########special for training for SalePrice outlier\nX = X.drop(X.GrLivArea[(X['GrLivArea']>4000) & (X['SalePrice']<300000)].index, axis=0)\n############3\n\nprint(X.shape)\nX.dropna(axis=0, subset=['SalePrice'], inplace=True) #drop na row if X has na in Saleprice\nprint(X.shape)\nY = X.SalePrice\nY_log=np.log(Y)\nX.drop(['SalePrice'], axis=1, inplace=True)\nprint(X.shape, Y.shape)\nX\n","5b1bb3a8":"#X=prepro(X)\nX=prepro_simple(X)\nX","0358b9ad":"#check the missing data for impute.fit_transform to handle\nX.isna().sum().sort_values(ascending=False).head(10)","6a1507d9":"from sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.impute import SimpleImputer\n# Split into validation and training data\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, Y_log, random_state=1)\n# Final imputation of missing data - to address those outstanding after previous section\nmy_imputer = SimpleImputer()\ntrain_X = my_imputer.fit_transform(train_X)\nval_X = my_imputer.transform(val_X)\n\ntrain_X.astype('float32')\ntrain_y.astype('float32')\nval_X.astype('float32')\nval_y.astype('float32')","8fc5e223":"from sklearn.metrics import mean_absolute_error\nfrom sklearn.linear_model import Lasso\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.linear_model import Ridge\nfrom sklearn.linear_model import ElasticNet\nfrom sklearn.neighbors import KNeighborsRegressor\nfrom sklearn.svm import SVR\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.ensemble import AdaBoostRegressor\nfrom sklearn.tree import DecisionTreeRegressor\nfrom xgboost import XGBRegressor","006d6b80":"def inv_y(low_y):\n    return np.exp(low_y)","6b8200ac":"# Series to collate mean absolute errors for each algorithm\nmae_compare = pd.Series()\nmae_compare.index.name = 'Algorithm'\n\n# Random Forest. Define the model. =============================\nrf_model = RandomForestRegressor(random_state=5)\nrf_model.fit(train_X, train_y)\nrf_val_predictions = rf_model.predict(val_X)\nrf_val_mae = mean_absolute_error(inv_y(rf_val_predictions), inv_y(val_y))\n\nmae_compare['RandomForest'] = rf_val_mae\n\n# XGBoost. Define the model. ======================================\nxgb_model = XGBRegressor(n_estimators=2000, learning_rate=0.02)\nxgb_model.fit(train_X, train_y, early_stopping_rounds=5, \n              eval_set=[(val_X,val_y)], verbose=False)\nxgb_val_predictions = xgb_model.predict(val_X)\nxgb_val_mae = mean_absolute_error(inv_y(xgb_val_predictions), inv_y(val_y))\n\nmae_compare['XGBoost'] = xgb_val_mae\n\n#Gradient Boosting Regression ==========================================\ngbr_model = GradientBoostingRegressor(n_estimators=500, learning_rate=0.032, \n                                      max_depth=4, random_state=5)\ngbr_model.fit(train_X, train_y)\ngbr_val_predictions = gbr_model.predict(val_X)\ngbr_val_mae = mean_absolute_error(inv_y(gbr_val_predictions), inv_y(val_y))\n\nmae_compare['GradientBoosting'] = gbr_val_mae\n\n# Linear Regression =================================================\n\nlinear_model = LinearRegression()\nlinear_model.fit(train_X, train_y)\nlinear_val_predictions = linear_model.predict(val_X)\nlinear_val_mae = mean_absolute_error(inv_y(linear_val_predictions), inv_y(val_y))\nmae_compare['LinearRegression'] = linear_val_mae\n\n\n# Lasso ==============================================================\nlasso_model = Lasso(alpha=0.0002, random_state=5)\nlasso_model.fit(train_X, train_y)\nlasso_val_predictions = lasso_model.predict(val_X)\nlasso_val_mae = mean_absolute_error(inv_y(lasso_val_predictions), inv_y(val_y))\n\nmae_compare['Lasso'] = lasso_val_mae\n# print(\"Validation MAE for Lasso Model: {:,.0f}\".format(lasso_val_mae))\n\n# Ridge ===============================================================\nridge_model = Ridge(alpha=0.002, random_state=5)\nridge_model.fit(train_X, train_y)\nridge_val_predictions = ridge_model.predict(val_X)\nridge_val_mae = mean_absolute_error(inv_y(ridge_val_predictions), inv_y(val_y))\n\nmae_compare['Ridge'] = ridge_val_mae\n# print(\"Validation MAE for Ridge Regression Model: {:,.0f}\".format(ridge_val_mae))\n","1874a953":"print('MAE values for different algorithms:')\nmae_compare.sort_values(ascending=True).round()","2129ffac":"# Lasso ==============================================================\nlasso_model = Lasso(alpha=0.0001, random_state=8)\nlasso_model.fit(train_X, train_y)\nlasso_val_predictions = lasso_model.predict(val_X)\nlasso_val_mae = mean_absolute_error(inv_y(lasso_val_predictions), inv_y(val_y))\n\nlasso_val_mae","04e3a89a":" #Manual testing Gradient Boosting Regression ==========================================\ngbr_model = GradientBoostingRegressor(n_estimators=700, learning_rate=0.02, \n                                      max_depth=4, random_state=5)\ngbr_model.fit(train_X, train_y)\ngbr_val_predictions = gbr_model.predict(val_X)\ngbr_val_mae = mean_absolute_error(inv_y(gbr_val_predictions), inv_y(val_y))\n\ngbr_val_mae","28112196":"# Manually XGBoost. Define the model. ======================================\nxgb_model = XGBRegressor(n_estimators=1000, learning_rate=0.02, max_depth=6, reg_lambda=1, reg_alpha=0)\nxgb_model.fit(train_X, train_y, early_stopping_rounds=5, \n              eval_set=[(val_X,val_y)], verbose=False)\nxgb_val_predictions = xgb_model.predict(val_X)\nxgb_val_mae = mean_absolute_error(inv_y(xgb_val_predictions), inv_y(val_y))\n\nxgb_val_mae","50c45065":"imputer = SimpleImputer()\nimputed_X = imputer.fit_transform(X)\nn_folds = 10","f1ca6bb5":"from sklearn.model_selection import cross_val_score\n\nlasso_model = Lasso(alpha=0.0003, random_state=5)\n# =========================================================================\nscores = cross_val_score(lasso_model, imputed_X,Y_log, scoring='neg_mean_squared_error', \n                         cv=n_folds)\nlasso_mae_scores = np.sqrt(-scores)\n\nprint('For LASSO model:')\n# print(lasso_mae_scores.round(decimals=2))\nprint('Mean RMSE = ' + str(lasso_mae_scores.mean().round(decimals=4)))\nprint('Error std deviation = ' +str(lasso_mae_scores.std().round(decimals=3)))","a7f02e5c":"gbr_model = GradientBoostingRegressor(n_estimators=700, learning_rate=0.02, \n                                      max_depth=4, random_state=5)\nscores = cross_val_score(gbr_model, imputed_X,Y_log, scoring='neg_mean_squared_error', \n                         cv=n_folds)\nmae_scores = np.sqrt(-scores)\n\nprint('For GradientBoost model:')\n# print(mae_scores.round(decimals=2))\nprint('Mean RMSE = ' + str(mae_scores.mean().round(decimals=3)))\nprint('Error std deviation = ' +str(mae_scores.std().round(decimals=3)))","18b5cb87":"xgb_model = XGBRegressor(n_estimators=500, learning_rate=0.05)\nscores = cross_val_score(xgb_model, imputed_X,Y_log, scoring='neg_mean_squared_error', \n                         cv=n_folds)\nmae_scores = np.sqrt(-scores)\n\nprint('For XGBoost model:')\n# print(mae_scores.round(decimals=2))\nprint('Mean RMSE = ' + str(mae_scores.mean().round(decimals=3)))\nprint('Error std deviation = ' +str(mae_scores.std().round(decimals=3)))","da9e28ff":"def buildModel_XGB(n_estimators, learning_rate, n_jobs):\n    my_model = XGBRegressor(n_estimators=n_estimators, learning_rate=learning_rate, n_jobs=n_jobs) # Your code here\n\n    my_model.fit(train_X, train_y) # Your code here\n    predictions = my_model.predict(val_X) # Your code here\n    mae = mean_absolute_error(predictions, val_y) # Your code here\n    return mae","1f8d1d07":"def buildModel_LASSO(al):\n    my_model = XGBRegressor(alpha=al, random_state=8 ) # Your code here\n\n    my_model.fit(train_X, train_y) # Your code here\n    predictions = my_model.predict(val_X) # Your code here\n    mae = mean_absolute_error(predictions, val_y) # Your code here\n    return mae","f009447a":"#alpha=0.00015\nparameters = []\nresult = []\nfor al in [0.00008,0.0001,0.00012,0.00015, 0.0002, 0.0003,0.0005,0.0008,0.001,0.0012,0.002,0.003,0.005,0.006,0.007]:\n            parameters.append(al)\nfor p in parameters:\n         r = buildModel_LASSO(p)\n         print(p, r)\n         result.append(r)","400fa964":"house_test_data.isna().sum().sort_values(ascending=False).head(20)\n                                                ","582fbc75":"X_test = house_test_data.copy()\n\n#X_test= prepro(X_test)\nX_test= prepro_simple(X_test)\nfinal_train, final_test=X.align(X_test, join='left', axis=1)\n#final_test_imputed = my_imputer.transform(final_test)","03349511":"#final_train, final_test=X.align(X_test, join='left', axis=1)\nfinal_cols = [cname for cname in final_test.columns ]   \nfor cat in final_cols:\n         final_test[cat] =  final_test[cat].fillna(0)\nprint('Null in the train table after reshape=',final_train.isna().sum().sum())\nprint('Null in the test table after reshape=',final_test.isna().sum().sum())","bd6baef3":"# Create final model\n# Best model = Lasso &XGboost\n#final_model = Lasso(alpha=0.0005, random_state=5)\n#final_model = XGBRegressor(n_estimators=2000, learning_rate=0.02)\n#final_model = xgb_model\n#final_model = GradientBoostingRegressor(n_estimators=700, learning_rate=0.02, \n  #                                    max_depth=4, random_state=5)\n# Create model - on full set of data (training & validation)\n# Best model = Lasso?\nfinal_model = Lasso(alpha=0.00015, random_state=5)\n\nfinal_train_imputed = my_imputer.fit_transform(final_train, Y_log)\nfinal_test_imputed = my_imputer.transform(final_test)\n# Fit the model using all the data - train it on all of X and y\n\nfinal_model.fit(final_train_imputed, Y_log)","71e95600":"\n# Fit the model using all the data - train it on all of X and y\nfinal_model.fit(final_train_imputed, Y_log)\npredictions =final_model.predict(val_X) # Your code here\nmae = mean_absolute_error(predictions, val_y) # Your code here\nmae #low because leakage, just to confirm model has not problem","abc563a0":"house_test_data.index","6ae75dbc":"# make predictions which we will submit. \ntest_preds = final_model.predict(final_test_imputed)\n\n# The lines below shows you how to save your data in the format needed to score it in the competition\n# Reminder: predictions are in log(SalePrice). Need to inverse-transform.\noutput = pd.DataFrame({'Id': house_test_data.index,\n                       'SalePrice': inv_y(test_preds)})\n\noutput.to_csv('submission.csv', index=False)","3f528149":"inv_y(test_preds)","704bcf95":"# 2.2 analyze the numerical data","0c782023":"# 1) Load the data\n## * csv file reading\n## * output format setup (I prefer table output a. with grid, and b. without hidden columns).\n## * using describe(), info() function to check out the basic information of the data.","a0f236e9":"#  4) Model Setup \n## * prepro function is for processing X data with many feature engineering work, however, it does not work better than simple method prepro_simple.\n## * The different data processes includes: column removal, date analysis (divide the MoSold into quarters), one-hot encoding, imputer, which is very important to fill numerical data by fit_transform. \n## * indicator column is created for those data not applicable, shuch as for house without remodel, without garage.","fb083872":"parameters = []\nresult = []\nfor ne in [200, 500, 1000]:\n     for lr in [0.01, 0.02, 0.05]:\n            parameters.append([ne, lr, 1])\nfor p in parameters:\n         r = buildModel(p[0], p[1], p[2])\n         print(p, r)\n         result.append(r)","e61a4cbb":"# Introduction\n##    Predicting the house price is a very interesting and practical problem. There are around 80 features in this excercise. We will get some exposure on data exploration, feature engineering, model setups and result cross-validation. As a supervised prediciton problem, we can compare different models based on MAE, in this case,and pick the best model based on your pre-processed features. \n\n## This version is around 3% in ranking. For further improvement, I plan to improve on feature engineering,  hyper-parameter tuning, and implementation of other techniques,such as DNN method.\n\n\n\n** I learned a lot of skills and techniques from many other Kagglers. Thank you so much for your generous sharing, which also encourages me to share my notes with other new data science practioners.","b9d75ac0":"# Training model:\n## first, impute the data, train_test_split and try many different models\n","622df4fc":"## hyper-parameter grid scan","7628dc64":"# 3) Analyze Categorical data\n## a check the general information with describe, such as cardinality, for high cardinality column, which I will throw it away in the model setup\n## b converted category rating data to numerical data, so that I can carry out importance analysis\n## c others will do one-hot encoding.","5f9294b0":"# 1.1 analyze Y\n## In summary, the log Y has better skew than original Y or other linear transformation, such as Z-score","64087fd2":"## read data description, NumCol_3 are columns with rating, which I convert to numerical data with equal spacing for different ratings.","0479ac06":"Use setup below to show the table with gridlines","3d680c46":"# 5) Model submission\n## ***important align the data with training data, because the column number could be different after one-hot encoding","9381b199":"# 2) Exploration of numerical data\n## 1. plot distribution of each numerical features.\n## 2. plot scatter plot of each feature against the label target to check the general trend, and possible outliers.\n## 3. Use heatmap to check the correlation with the salePrice. \n## 4. Replot the scatter plot again, but **IN ORDER** of importance for each numerical features, so that we can keep most important ones, and throw away non-important features (here, be careful to replace the missing or NA data for now, leaving to the end and  to use imputer.fit_transform to get better replacement for filling those missing values)","2663a665":"## use cross_validation to compare those results"}}