{"cell_type":{"d1d5f39e":"code","5e0a7c0a":"code","c8cba672":"code","268acb4e":"code","bfef1a79":"code","f85bfc1a":"code","aec3ef6a":"code","290a6990":"code","6816b763":"code","b00e61a1":"code","dc36ffb2":"code","cb35898d":"code","43e7a210":"code","778ef2b9":"code","6b8cfd95":"code","cc695cc9":"code","5ee31b74":"code","f598550a":"code","83d5c2e7":"code","d204ce75":"code","4e215cf3":"code","08cd7866":"code","645bc3a0":"code","2b12b3fa":"code","b2c6e92f":"code","1348681b":"code","6330c63b":"code","c6f9ad10":"code","a94db5dd":"code","702496f0":"code","5edb89da":"markdown","5b064196":"markdown","f38cc9fa":"markdown","bee55b77":"markdown","59dc80ed":"markdown","c7168e2e":"markdown","d3a32faf":"markdown"},"source":{"d1d5f39e":"import collections\nimport numpy as np\nimport pandas as pd\nimport nltk\nfrom sklearn.metrics import log_loss\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import classification_report\nfrom collections import Counter\nfrom lightgbm import LGBMClassifier\nfrom sklearn.model_selection import KFold\nimport matplotlib.pyplot as plt\n%matplotlib inline\nfrom sklearn.decomposition import PCA\nfrom tqdm.notebook import tqdm\nfrom scipy.sparse import csr_matrix\nimport gensim\nimport os\nos.listdir(\"..\/input\/ykc-cup-2nd\/\")","5e0a7c0a":"train = pd.read_csv(\"..\/input\/ykc-cup-2nd\/train.csv\")\ntest = pd.read_csv(\"..\/input\/ykc-cup-2nd\/test.csv\")\nsub = pd.read_csv(\"..\/input\/ykc-cup-2nd\/sample_submission.csv\")\ntrain.shape, test.shape, sub.shape","c8cba672":"train.head()","268acb4e":"test.head()","bfef1a79":"sub.head()","f85bfc1a":"## train\u3068test\u3092\u304f\u3063\u3064\u3051\u3066\u4e00\u62ec\u3067\u7279\u5fb4\u91cf\u4f5c\u6210\u3092\u3059\u308b\ndf = pd.concat([train, test])\ndf = df.reset_index(drop=True)\ndf.shape","aec3ef6a":"train[train[\"department_id\"] == 3].head()\n## \u91ce\u83dc\u3068\u304b\u679c\u7269\uff1f","290a6990":"train[train[\"department_id\"] == 12].head()\n## \u8abf\u5473\u6599\uff1f","6816b763":"train[train[\"department_id\"] == 16].head()\n##\u6d17\u6fef\u7528\u5177\u3068\u304b","b00e61a1":"df[\"product_name\"] = df[\"product_name\"].apply(lambda words : words.lower().replace(\",\", \"\").replace(\"&\", \"\").split(\" \"))\ndf.head()","dc36ffb2":"# stopword & normalization\n# stopword settings\nfrom nltk.corpus import stopwords\nstop_words = stopwords.words('english')\n\n# normalizing setting\n# https:\/\/yukinoi.hatenablog.com\/entry\/2018\/05\/29\/120000\nimport re\nshortened = {\n    '\\'m': ' am',\n    '\\'re': ' are',\n    'don\\'t': 'do not',\n    'doesn\\'t': 'does not',\n    'didn\\'t': 'did not',\n    'won\\'t': 'will not',\n    'wanna': 'want to',\n    'gonna': 'going to',\n    'gotta': 'got to',\n    'hafta': 'have to',\n    'needa': 'need to',\n    'outta': 'out of',\n    'kinda': 'kind of',\n    'sorta': 'sort of',\n    'lotta': 'lot of',\n    'lemme': 'let me',\n    'gimme': 'give me',\n    'getcha': 'get you',\n    'gotcha': 'got you',\n    'letcha': 'let you',\n    'betcha': 'bet you',\n    'shoulda': 'should have',\n    'coulda': 'could have',\n    'woulda': 'would have',\n    'musta': 'must have',\n    'mighta': 'might have',\n    'dunno': 'do not know',\n    # \u5b9f\u30c7\u30fc\u30bf\u304b\u3089\u306e\u7f6e\u63db\n    'softgels': \"soft gels supplement\",\n    \"almondmilk\": \"almond milk\",\n    \"lunchables\": \"lunch\",\n    \"febreze\": \"deodorant\",\n    'steamfresh': 'steam fresh',\n    \"lil\": 'little',\n    'volumizing': 'volume',\n    'rigate': 'penne',\n    'anticavity': 'cavity protection',\n    'keurig': 'coffee',  # \u308f\u304b\u3089\u3093\n    'eggo': 'waffle',  # \u8981\u78ba\u8a8d\n    'pantiliners': 'panty liner',\n    'nutri': 'nutrition',  # \u8981\u78ba\u8a8d\n    'havarti': 'cheese',\n    'lunchables': 'lunch',\n    '0.0oz': 'deodorant',\n    '0oz': 'deodorant',\n    'velveeta': 'cheese',\n    'organix': 'organic',  # \u30b7\u30e3\u30f3\u30d7\u30fc\u304b\u30c9\u30c3\u30b0\u30d5\u30fc\u30c9\uff1f\n    'muenster': 'cheese',\n    'smartblend': 'smart blend dog pet',  # dog foods?\n    'snickerdoodle': 'cookie',\n    '0ct': 'diamond ring',  # \u30ab\u30e9\u30c3\u30c8\n    'grands!': 'cat pet',  # cat foods?\n    'umcka': 'supplement',\n    'marzano': 'pizza tomato',\n    'butterfinger': 'candy chocolate peanut',  # \u30d0\u30bf\u30fc\u30d5\u30a3\u30f3\u30ac\u30fc\n    'modena': 'italy',\n    'unstopables': 'deodorant',\n    'yokids': 'sandal',\n    \"flamin'\": 'gay',  # ?\n    'beneful': 'dog pet',\n    'swaddlers': 'diapers',\n    'compleats': 'meal preserved',  # \u4fdd\u5b58\u98df?\n    'sambucus': 'drink berry',\n    'lindor': 'chocolate gift',\n    'macrobar': 'chocolate peanut',\n    'honeycrisp': 'apple',  #?\n    'ahoy!': 'europe',  #\u30e8\u30fc\u30ed\u30c3\u30d1\u306e\u6328\u62f6?\n    'whips!': \"whip\", # \u97ad?\n    'arrabbiata': 'tomato sauce',\n    'craisins': 'dry berry fruit',\n    'nyquil': 'medicine',\n    'actionpacs': 'detergent',\n    'sproutofu': 'organic teriyaki tofu', # ??\n    'chewables': 'medicine',\n    'gurt': 'pig',\n    'krunch': 'chocolate',\n    'doubleshot': 'coffee',\n    'activia': 'yogurt',\n    'fillo': 'pillow',\n    'snax': 'snack',  # \u308f\u304b\u3089\u3093\n    'snackimals': 'snack organic',\n    'oxiclean': 'bleach detergent',\n    'chex': 'cheese', # ??\n    'tahitian': 'tahiti',\n    'montebello': 'oil', # ??\n    'vegenaise': 'seasoning spice',\n    'noticeables': 'deodorant',\n    'scoopable': 'cat pet', # ??\n    'wetjet': 'clean',\n    'pantene': 'shampoo',\n    'shirataki': 'noodle',  # \u767d\u6edd\u306fnoodle\u3067\u3044\u3044\u306e?\n    'triscuit': 'snack',\n    'dophilus': 'capsule supplement',\n    'danimals': 'sweet', # \u7518\u3044\u98df\u54c1\u3092\u58f2\u3063\u3066\u305d\u3046\u306a\u30d6\u30e9\u30f3\u30c9\n    'purina': 'cat dog pet',  # \u30da\u30c3\u30c8\u30d5\u30fc\u30c9\n    'creamline': 'hamburger',\n    'funfetti': 'cookie',  # \u304a\u3044\u3057\u304f\u306a\u3055\u305d\u3046\u306a\u30af\u30c3\u30ad\u30fc?\n    'friskies': 'cat pet',\n    'krinkle': 'biscuit',\n    'antigingivitis': 'mouse wash tooth',\n    'nesquik': 'chocolate drink milk',\n    'sleepytime': 'herb tea',\n    'gillette': 'shaving shaver shave',\n    'antiplaque': 'tooth mouse wash',\n    'detangler': 'treatment shampoo',\n    'wintermint': 'gum',\n    'perspirant': 'deodorant',\n    'clorox': 'deodorant',\n    'multimineral': 'multi mineral',\n    'hommus': 'bean',\n    'steamables': 'potato vegetable',\n    'dentastix': 'dog pet',\n    'nutrish': 'dog pet',\n    '0st': '0',\n    '0nd': '0',\n    '0rd': '0',\n}\n\nshortened_re = re.compile('(?:' + '|'.join(map(lambda x: '\\\\b' + x + '\\\\b', shortened.keys())) + ')')\n\ndef get_shortended_word(word: str):\n    \"\"\"\n    \u5358\u8a9e\u306e\u6b63\u898f\u5316\n    \"\"\"\n    \n    \n    shortened_word = re.sub(r\"[0-9]+\", \"0\", word)  # \u6570\u5b57\n    shortened_word = shortened_re.sub(lambda w: shortened[w.group(0)], shortened_word) # \u7f6e\u63db\n    shortened_word = re.sub(r\"'(s|n)\", \"\", shortened_word)  # 's, 'n \u3092\u524a\u9664\n    shortened_word = re.sub(r\"(%|\\\\|!|\\(|\\)|#|\\.|\u2122|\\\"|\\'|\u00ae)\", \"\", shortened_word)  # \u3044\u308d\u3044\u308d\u524a\u9664\n    shortened_word = re.sub(r\"(\\+|\\-|\\\/|:)\", \" \", shortened_word)  # +, -\u306a\u3069  \u3092\u7a7a\u767d\u306b\u7f6e\u63db\n    return shortened_word.split(\" \")\n\ndef flatten(l):\n    \"\"\"\n    2,3d list => 1d list\n    [[1,2], 3, [4,5]] => [1,2,3,4,5]\n    \"\"\"\n    \n    for el in l:\n        if isinstance(el, collections.abc.Iterable) and not isinstance(el, (str, bytes)):\n            yield from flatten(el)\n        else:\n            yield el\n\ndf[\"product_name\"] = df[\"product_name\"].apply(lambda words : [get_shortended_word(w) for w in words])\ndf[\"product_name\"] = df[\"product_name\"].apply(lambda words: list(set([w for word in words for w in word if w not in stop_words])))\n\ndf.head()","cb35898d":"# \u305d\u306e\u4ed6\u306e\u7279\u5fb4\u91cf\ndf[\"name_num\"] = df[\"product_name\"].apply(len)  # product name \u306e\u6570\ndf[\"in_num\"] = df[\"product_name\"].apply(lambda words: any([c.isdigit() for w in words for c in w]))  # product name\u306b\u6570\u5b57\u3092\u542b\u3093\u3067\u3044\u308b\u304b\n# train_data \u3067\u306e order_dow_mode \u306e\u51fa\u73fe\u5272\u5408\ndow_counter = collections.Counter(train['order_dow_mode'].tolist())\ndow_rate = {k: v \/ len(train) for k, v in dow_counter.items()}\ndf[\"dow_rate\"] = df[\"order_dow_mode\"].apply(lambda x: dow_rate[x])\n# train_data \u3067\u306e order_hour_of_day_mode \u306e\u51fa\u73fe\u5272\u5408\nday_counter = collections.Counter(train['order_hour_of_day_mode'].tolist())\nday_rate = {k: v \/ len(train) for k, v in day_counter.items()}\ndf[\"day_rate\"] = df[\"order_hour_of_day_mode\"].apply(lambda x: day_rate[x])\n# order_rate \u306e\u5024\u306e\u5927\u304d\u3055\ndef get_order_rate_basis(order_rate: float):\n    if order_rate > 5e-4:\n        return 0\n    elif order_rate > 1e-4:\n        return 1\n    elif order_rate > 5e-5:\n        return 2\n    elif order_rate > 1e-5:\n        return 3\n    elif order_rate > 5e-6:\n        return 4\n    elif order_rate > 1e-6:\n        return 5\n    elif order_rate > 5e-7:\n        return 6\n    elif order_rate > 1e-7:\n        return 7\n    else:\n        return 8\ndf[\"order_rate_basis\"] = df[\"order_rate\"].apply(get_order_rate_basis)\n\nimportant_words = {\n    0: ['cream', 'ice', 'chicken', 'pizza', 'frozen', 'cheeze', 'chocolate', 'vanilla', 'gluten'],\n    1: ['sleep', 'liquid', 'melatonin', 'baby', 'mix', 'tablets', 'flavor', 'natural', 'hand'],\n    2: ['bread', 'whole', 'grain', 'tortillas', 'buns', 'gluten', 'rolls', 'chocolate'],\n    3: ['baby', 'red', 'salad', 'bag', 'potato', 'potatoes', 'lettuce', 'sweet', 'green', 'apple', 'mashrooms'],\n    4: ['wine', 'beer', 'ale', 'sauvignon', 'callfornia', 'cabernet', 'lager', 'chardonnay', 'red', 'whiskey', 'ponot'],\n    5: ['sauce', 'rice', 'noodles', 'noodle', 'thai', 'soup', 'miso', 'curry', 'spacy', 'sesame', 'medium'],\n    6: ['tea', 'juice', 'water', 'coffee', 'drink', 'green', 'sparking','soda', 'orange', 'lemmon', 'ginger'],\n    7: ['cat', 'dog', 'chicken', 'beef', 'treat', 'treats', 'turkey', 'adult', 'flavor', 'dry', 'tuna', 'salmon'],\n    8: ['pasta', 'rice', 'sauce', 'cheese', 'whole', 'grain', 'spaghetti', 'macaroni', 'chicken', 'garlic', 'brown', 'tomato'],\n    9: ['rice', 'bean', 'beans', 'granola', 'brown', 'super', 'cranberry', 'mung', 'rolled', 'oats', 'pesto', 'sauce', 'berry'],\n    10: ['body', 'shampoo', 'oil', 'conditioner', 'wash', 'deodorant', 'soap', 'vitamin', 'hand', 'with','tablets', 'mint'],\n    11: ['chicken', 'sausage', 'smoked', 'beef', 'turkey', 'bacon', 'boneless', 'pork', 'breast', 'franks', 'ground', 'uncured'],\n    12: ['dressing', 'mix', 'sauce', 'butter', 'oil', 'seasoning', 'suger', 'honey', 'salsa', 'chocolate', 'ground', 'salt'],\n    13: ['cereal', 'granola', 'oatmeal', 'gluten', 'honey', 'mix', 'pancake', 'cinnamon', 'chocolate', 'grain', 'instant'],\n    14: ['soup', 'beans', 'bean', 'tomatoes', 'tomato', 'vegetable', 'tuna', 'water', 'whole', 'white', 'sauce'],\n    15: ['cheese', 'yogurt', 'milk', 'fat', 'greek', 'vanilla', 'cheddar', 'lowfat', 'strawberry', 'cream', 'berry', 'original'],\n    16: ['scent', 'cleaner', 'detergent', 'laundry', 'liquid', 'fresh', 'paper', 'bags', 'ultra', 'dish', 'fabric', 'lavender'],\n    17: ['baby', 'food', 'stage', 'diapers', 'apple', 'banana', 'size', 'foods', 'food', 'yogurt', 'fruit', 'wipes'],\n    18: ['chocolate', 'bar', 'chips', 'chip', 'dark', 'cookies', 'cookie', 'cracker', 'crackers', 'salt', 'butter', 'sea'],\n    19: ['humms', 'hum', 'turkey', 'chicken', 'salad', 'roasted', 'roast', 'breast', 'deli', 'tofu', 'salami', 'dip'],\n    20: ['yogurt', 'chocolate', 'cheese', 'strawberry', 'apple', 'chicken', 'fruit', 'vanilla', 'original', 'cream', 'potato']\n}\n\nimportance_feature_names = []\nfor k, v in important_words.items():\n    importance_feature_names.append(\"important_\" + str(k) + \"_rate\")\n    # importance_feature_names.append(\"important_\" + str(k) + \"_flag\")\n    df[\"important_\" + str(k) + \"_rate\"] = df['product_name'].apply(lambda words: sum([1 for w in words if w in v]) \/ len(words))\n    # df[\"important_\" + str(k) + \"_flag\"] = df['product_name'].apply(lambda words: any([True if w in v else False for w in words]))","43e7a210":"## \u8a13\u7df4\u6e08\u307f\u306e\u5358\u8a9e\u30d9\u30af\u30c8\u30eb\u3092\u8aad\u307f\u8fbc\u3093\u3067\uff0cproduct_name\u306b\u542b\u307e\u308c\u308b\u5358\u8a9e\u3092\u30d9\u30af\u30c8\u30eb\u306b\u5909\u63db\u3057\u3066\u5e73\u5747\u3092\u53d6\u308b\u3053\u3068\u3067\uff0c\u5404product_id\u306b\u5bfe\u3057\u3066\u7279\u5fb4\u91cf\u30d9\u30af\u30c8\u30eb\u3092\u4f5c\u6210\u3059\u308b\n\n## gensim\u3067.vec\u304b\u3089\u8aad\u307f\u8fbc\u3080\u3068\u304d\u306b\u6642\u9593\u304c\u304b\u304b\u308b\u306e\u3067\uff0c\u4ed6\u306enotebook\u3067pickle\u3067\u4fdd\u5b58\u3057\u305f\u3082\u306e\u3092\u4f7f\u7528\u3057\u3066\u3044\u308b\nmodel = pd.read_pickle(\"..\/input\/ykc-cup-2nd-save-fasttext\/fasttext_gensim_model.pkl\") \n\n## gensim\u3067vec\u304b\u3089\u8aad\u307f\u8fbc\u3080\u5834\u5408\uff08\uff15\u5206\u3050\u3089\u3044\u304b\u304b\u308b\uff09\n# model = gensim.models.KeyedVectors.load_word2vec_format('..\/input\/ykc-2nd\/wiki-news-300d-1M.vec\/wiki-news-300d-1M.vec')\n\nfrom collections import defaultdict\nunused_words = defaultdict(int)\nlemmatizer = nltk.WordNetLemmatizer() # \u30ec\u30f3\u30de\u5316\ndef to_vec(x, model):\n    cnt = 0\n    v = np.zeros(model.vector_size)\n    all_pretrained_words = model.index2word\n\n    for w in x:\n        cnt += 1\n        # lemmatize\u306f\u9045\u3044\u306e\u3067 try except \u3067\u5fc5\u8981\u306a\u5358\u8a9e\u3060\u3051lemmatize\u3059\u308b\n        try:\n            v += model[w] ## \u5358\u8a9e\u304c\u8a13\u7df4\u6e08\u307f\u30e2\u30c7\u30eb\u306evocab\u306b\u3042\u3063\u305f\u3089\n        except:\n            try:\n                # \u5b58\u5728\u3057\u306a\u3044\u5834\u5408\u306f\u3001\u30ec\u30f3\u30de\u5316\u3057\u305f\u3082\u306e\u304c\u8a13\u7df4\u6e08\u307f\u30e2\u30c7\u30eb\u306evocab\u306b\u3042\u308b\u304b\u3092\u78ba\u8a8d\n                lemmatized_w = lemmatizer.lemmatize(w)\n                v += model[lemmatizer.lemmatize(w)]\n            except:\n                cnt -= 1\n                unused_words[w] += 1 ## \u30d9\u30af\u30c8\u30eb\u304c\u5b58\u5728\u3057\u306a\u304b\u3063\u305f\u5358\u8a9e\u3092\u30e1\u30e2\n                \n    v \/= cnt if cnt > 0 else 1  # SWEM average-pooling\n    v = v \/ (np.sqrt(np.sum(v ** 2)) + 1e-16) ## \u9577\u3055\u30921\u306b\u6b63\u898f\u5316\n    return v\n\nvecs = df[\"product_name\"].apply(lambda x : to_vec(x, model))\nvecs = np.vstack(vecs)\nfasttext_pretrain_cols = [f\"fasttext_pretrain_vec{k}\" for k in range(vecs.shape[1])]\nvec_df = pd.DataFrame(vecs, columns=fasttext_pretrain_cols)\ndf = pd.concat([df, vec_df], axis = 1)\ndf.head()","778ef2b9":"sorted(unused_words.items(), key=lambda x: x[1], reverse = True)[:100]","6b8cfd95":"# config\nimport torch\nimport torch.nn as nn\n!pip install skorch\nimport skorch\nfrom skorch import NeuralNetClassifier\nfrom skorch.callbacks import Callback, Checkpoint, EarlyStopping\ntorch.manual_seed(42)\n\nclass MLPModel(nn.Module):\n    def __init__(self, num_features, dropout=0.25, n_hid=128):\n        super().__init__()\n        self.model = nn.Sequential(\n            nn.Dropout(dropout),\n            nn.Linear(num_features, n_hid),\n            nn.ReLU(),\n            nn.BatchNorm1d(n_hid),\n            nn.Dropout(dropout),            \n            nn.Linear(n_hid, n_hid \/\/ 4),\n            nn.ReLU(),\n            nn.BatchNorm1d(n_hid \/\/ 4),\n            nn.Dropout(dropout),\n            nn.Linear(n_hid \/\/ 4, 21),  # 21 class\n        )\n        self.softmax = nn.Softmax(dim=-1)\n        for m in self.model:\n            if isinstance(m, nn.Linear):\n                nn.init.kaiming_normal_(m.weight)\n                nn.init.constant_(m.bias, 0)\n\n    def forward(self, input_tensor):\n        return self.softmax(self.model(input_tensor))\n\n\nimport torch.nn.functional as F\n\n\n# https:\/\/github.com\/kefirski\/pytorch_Highway\/blob\/master\/highway\/highway.py\nclass Highway(nn.Module):\n\n    def __init__(self, size: int, num_layers: int, f: torch.nn.functional):\n        \"\"\"\n        :param size: linear layer size\n        :param num_layers: number of linear layers\n        :param f: activation function (ex. F.softmax, F.ReLU)\n        \"\"\"\n\n        super(Highway, self).__init__()\n        self.num_layers = num_layers\n        self.nonlinear = nn.ModuleList(\n            [nn.Linear(size, size) for _ in range(num_layers)])\n        self.linear = nn.ModuleList(\n            [nn.Linear(size, size) for _ in range(num_layers)])\n        self.gate = nn.ModuleList([nn.Linear(size, size)\n                                   for _ in range(num_layers)])\n        self.f = f\n\n    def forward(self, x):\n        \"\"\"\n        :param x: tensor with shape of (batch_size, size)\n        :return: tensor with shape of (batch_size, size)\n        \"\"\"\n\n        for layer in range(self.num_layers):\n            gate = torch.sigmoid(self.gate[layer](x))\n            nonlinear = self.f(self.nonlinear[layer](x))\n            linear = self.linear[layer](x)\n            x = gate * nonlinear + (1 - gate) * linear\n\n        return x\n\n\nclass CNNModel(nn.Module):\n\n    def __init__(self, num_features: int,\n                 cnn_filter_sizes: list=[1,3,5,10], cnn_num_filters: list=[100,200,300,400],\n                 highway_layers_num: int = 1, dropout: float = 0.5):\n        \"\"\"\n        :param embedding: word embedding\n        :param emb_dim: number of word embedding dimension\n        :param cnn_filter_sizes: filter sizes of CNNs\n        :param cnn_num_filters: filter numbers of CNNs\n        :param highway_layers_num: numbers of highway network layer\n        :param dropout_rate: drop out rate\n        \"\"\"\n\n        super().__init__()\n        self.cnn_filter_sizes = cnn_filter_sizes\n        self.cnn_num_filters = cnn_num_filters\n\n        self.emb_dim = num_features\n\n        self.convs = nn.ModuleList([\n            nn.Conv2d(1, n, (f, self.emb_dim))  # nn.Conv1d(n, f, emb_dim)\n            for (n, f) in zip(cnn_num_filters, cnn_filter_sizes)\n        ])\n        self.highway = Highway(sum(cnn_num_filters),\n                               highway_layers_num, nn.ReLU())\n        self.dropout = nn.Dropout(dropout)\n        self.linear = nn.Linear(sum(cnn_num_filters), 21)  # 21 = class num\n        self.softmax = nn.LogSoftmax(dim=0)\n        self._init_parameters()\n\n    def forward(self, x) -> torch.Tensor:\n        \"\"\"\n        Forward propagation algorithm.\n        :param x: embeddings\n        :return: (batch_size, 2)\n        \"\"\"\n\n        convs = [F.relu(conv(x).squeeze(3))\n                 for conv in self.convs]\n        pools = [F.max_pool1d(conv, conv.size(2)).squeeze(2)\n                 for conv in convs]\n        pred = torch.cat(pools, 1)\n        pred = self.highway(pred)\n        return self.softmax(self.linear(self.dropout(pred)))\n\n    def _init_parameters(self):\n        for param in self.parameters():\n            param.data.uniform_(-0.05, 0.05)\n","cc695cc9":"## \u4e88\u6e2c\u306b\u4f7f\u7528\u3059\u308b\u7279\u5fb4\u91cf\u306e\u540d\u524d\nfeatures = fasttext_pretrain_cols + [\n    \"order_rate\", \"order_dow_mode\", \"order_hour_of_day_mode\",  # \u5143\u304b\u3089\u7528\u610f\u3055\u308c\u3066\u3044\u308b\u7d20\u6027\n    'name_num', 'in_num',  # product name \u306b\u95a2\u3059\u308b\u7d20\u6027\n    'dow_rate', 'day_rate','order_rate_basis'  # \u305d\u306e\u4ed6\u306e\u7279\u5fb4\u91cf\u3092\u5909\u5f62\u3057\u305f\u7d20\u6027\n] + importance_feature_names  # \u5404\u30ab\u30c6\u30b4\u30ea\u3054\u3068\u306e\u91cd\u8981\u5358\u8a9e\u304c\u51fa\u73fe\u3059\u308b\u304b\u3069\u3046\u304b\u3068\u305d\u306e\u5272\u5408\u307f\u305f\u3044\u306a\u3082\u306e\ntarget = \"department_id\" ## \u4e88\u6e2c\u5bfe\u8c61\nn_split = 7 ## cross validation\u306efold\u6570","5ee31b74":"## train\u3068test\u3092\u5206\u96e2\ntrain = df[~df[target].isna()]\ntest = df[df[target].isna()]","f598550a":"from sklearn import preprocessing\nscaler = preprocessing.StandardScaler()\ntrain[features] = scaler.fit_transform(train[features])\ntest[features] = scaler.transform(test[features])","83d5c2e7":"## cross validation\nfrom sklearn.ensemble import VotingClassifier\npreds_test = []\nscores = []\nkfold = KFold(n_splits=n_split, shuffle = True, random_state=42)\npreds = []\nanss = []\nfor i_fold, (train_idx, valid_idx) in enumerate(kfold.split(train)):\n    print(f\"--------fold {i_fold}-------\")\n    \n    ## train data\n    x_tr = train.loc[train_idx, features]\n    y_tr = train.loc[train_idx, target]\n\n    ## valid data\n    x_va = train.loc[valid_idx, features]\n    y_va = train.loc[valid_idx, target]\n\n    ## train LGBM model\n    lgbm_params = {\n        'n_estimators': 700,\n        'objective': 'multiclass',\n        \"boosting_type\": \"gbdt\",\n        \"importance_type\": \"split\",\n        \"random_state\": 42,\n        'num_leaves': 225,\n        'learning_rate': 0.04689606818793407,\n        'class_weight': None,\n        'min_child_samples': 98,\n        'subsample': 0.44354721466773056,\n        'subsample_freq': 6,\n        'colsample_bytree': 0.7377347290625655,\n        'reg_alpha': 1.0160018949956453,\n        'reg_lambda': 1.6781461339752908,\n        'n_jobs': 2\n    }\n    lgbm_model = LGBMClassifier(**lgbm_params)\n    lgbm_model.fit(x_tr, y_tr, eval_set=(x_va, y_va), early_stopping_rounds=10)\n    ## predict on valid\n    pred_val = lgbm_model.predict_proba(x_va.to_numpy().astype(np.float32))\n    \n    ## evaluate\n    score = {\n        \"logloss\"  : log_loss(y_va, pred_val),\n        \"f1_micro\" : f1_score(y_va, np.argmax(pred_val, axis = 1), average = \"micro\")\n    }\n    print(score)\n    scores.append(score)\n    preds.append(np.argmax(pred_val, axis = 1))\n    anss.append(y_va)\n        \n    ## predict on test\n    pred_test = lgbm_model.predict_proba(test[features].to_numpy().astype(np.float32))\n    preds_test.append(pred_test)\n    # \"\"\"\n\n    monitor = lambda MLPModel: all(MLPModel.history[-1, ('train_loss_best', 'valid_loss_best')])\n \n    # set param(make trainer)\n    neural_model = NeuralNetClassifier(\n                    MLPModel,\n                    max_epochs=200,\n                    lr=0.005,\n                    warm_start=True,\n                    optimizer=torch.optim.Adam,\n                    iterator_train__shuffle=True,\n                    callbacks=[Checkpoint(), EarlyStopping(patience=10)],\n                    module__num_features=train[features].shape[1],\n                    # module__n_hid=512,\n                    module__dropout=0.25,\n                    iterator_valid__batch_size=256,\n                    device=\"cuda\"\n                )\n    neural_model.fit(x_tr.to_numpy().astype(np.float32), y_tr.to_numpy().astype(np.int64))\n    ## predict on valid\n    pred_val = neural_model.predict_proba(x_va.to_numpy().astype(np.float32))\n    \n    # voting\n    \"\"\"\n    estimators = [\n        ('lgbm', lgbm_model),\n        ('mlp', neural_model)\n    ]\n    voting_model = VotingClassifier(estimators, n_jobs=-1)\n    voting_model.fit(x_tr.to_numpy().astype(np.float32), y_tr.to_numpy().astype(np.int64))\n    pred_val = neural_model.predict_proba(x_va.to_numpy().astype(np.float32))\n    \"\"\"\n    \n    ## evaluate\n    score = {\n        \"logloss\"  : log_loss(y_va, pred_val),\n        \"f1_micro\" : f1_score(y_va, np.argmax(pred_val, axis = 1), average = \"micro\")\n    }\n    print(score)\n    scores.append(score)\n    preds.append(np.argmax(pred_val, axis = 1))\n    anss.append(y_va)\n        \n    ## predict on test\n    pred_test = neural_model.predict_proba(test[features].to_numpy().astype(np.float32))\n    preds_test.append(pred_test)","d204ce75":"## evaluate for each class\nprint(classification_report(np.concatenate(anss), np.concatenate(preds)))","4e215cf3":"score_df = pd.DataFrame(scores)\nscore_df","08cd7866":"score_df.mean()","645bc3a0":"## cv\u306e\u5404fold\u3067\u8a08\u7b97\u3057\u305f\u4e88\u6e2c\u5024\u306e\u5e73\u5747\u3092\u6700\u7d42\u7684\u306a\u4e88\u6e2c\u5024\u306b\npred_test_final = np.array(preds_test).mean(axis = 0)\npred_test_final = np.argmax(pred_test_final, axis = 1)","2b12b3fa":"sub[\"department_id\"] = pred_test_final\nsub.to_csv(\"submission.csv\", index = False)\nsub.head()","b2c6e92f":"importance = pd.DataFrame(\n    model.feature_importances_,\n    index=features,\n    columns=['importance']\n)\n\nimportance = importance.sort_values('importance', ascending=False)\nimportance.head(50).plot.bar()","1348681b":"# configs\nimport optuna\nimport json\nimport datetime as dt\n\nn_trials = 50","6330c63b":"def param_grids_to_params(trial: optuna.Trial, param_grids: dict):\n    params = {}\n    for k, v in param_grids.items():\n        # set optimizing target parameters\n        if isinstance(v, list):\n            if len(v) > 2:\n                params[k] = trial.suggest_categorical(k, v)\n            elif all([isinstance(s, bool) for s in v]):\n                b = strtobool(trial.suggest_categorical(k, [str(p) for p in v]))\n                params[k] = True if b == 1 else False\n            elif type(v[0]) == int:\n                params[k] = trial.suggest_int(k, v[0], v[1])\n            elif type(v[0]) == float:\n                params[k] = trial.suggest_uniform(k, v[0], v[1])\n            else:\n                params[k] = trial.suggest_categorical(k, v)\n        # set static parameters\n        else:\n            params[k] = v\n    return params\n\n\ndef objective(trial: optuna.Trial):\n    fmeasures = []\n    model = LGBMClassifier\n    param_grids = {\n            \"boosting_type\": \"gbdt\",\n            \"num_leaves\": [2, 256],\n            \"max_depth\": -1,\n            \"learning_rate\": [0.005, 0.1],\n            \"n_estimators\": 500,\n            \"subsample_for_bin\": 200000,\n            \"objective\": \"multiclass\",\n            \"class_weight\": [\"balanced\", None],\n            \"min_split_gain\": 0.0,\n            \"min_child_weight\": 0.001,\n            \"min_child_samples\": [5, 100],\n            \"subsample\": [0.4, 1.0],\n            \"subsample_freq\": [1, 7],\n            \"colsample_bytree\": [0.65, 1.0],\n            \"reg_alpha\": [1e-8, 10.0],\n            \"reg_lambda\": [1e-8, 10.0],\n            \"random_state\": 0,\n            \"n_jobs\": 2,\n            \"silent\": True,\n            \"importance_type\": \"split\",\n        }\n    params = param_grids_to_params(trial, param_grids)\n    kfold = KFold(n_splits=n_split, shuffle = True, random_state=42)\n    for i_fold, (train_idx, valid_idx) in enumerate(kfold.split(train)):    \n        ## train data\n        x_tr = train.loc[train_idx, features]\n        y_tr = train.loc[train_idx, target]\n\n        ## valid data\n        x_va = train.loc[valid_idx, features]\n        y_va = train.loc[valid_idx, target]\n\n        model = LGBMClassifier(**params)\n        model.fit(x_tr, y_tr, eval_set=(x_va, y_va), early_stopping_rounds=10)\n    \n        ## predict on valid\n        pred_val = model.predict_proba(x_va)\n        fmeasures.append(f1_score(y_va, np.argmax(pred_val, axis = 1), average = \"micro\"))\n        \n        break\n\n    f1score = sum(fmeasures) \/ len(fmeasures)\n    return f1score","c6f9ad10":"study = optuna.create_study(direction=\"maximize\")\nstudy.optimize(objective, n_trials=n_trials)\nparams = study.best_trial.params\nbest_score = study.best_value\nnow = dt.datetime.now()\njson_name = \"tuned_{0:%Y%m%d%H%M%S}_{1}.json\".format(now, \"lgbm\")\nwith open(json_name, \"w\") as f:\n    json.dump(params, f)","a94db5dd":"from sklearn.model_selection import GridSearchCV\nimport json\n\n# monitor = lambda MLPModel: all(MLPModel.history[-1, ('train_loss_best', 'valid_loss_best')])\nneural_model = NeuralNetClassifier(\n                    MLPModel,\n                    max_epochs=100,\n                    lr=0.01,\n                    warm_start=True,\n                    optimizer=torch.optim.Adam,\n                    iterator_train__shuffle=True,\n                    module__num_features=train[features].shape[1],\n                    device='cuda'\n                )\n# deactivate skorch-internal train-valid split and verbose logging\nneural_model.set_params(train_split=False, verbose=0)\nparams = {\n    'lr': [0.01],\n    'module__dropout': [0.25],\n    'module__n_hid': [128, 256, 512],\n}\ngs = GridSearchCV(neural_model, params, cv=7, scoring='f1_micro')\n\ngs.fit(train[features].to_numpy().astype(np.float32), train[target].to_numpy().astype(np.int64))\nprint(\"best score: {:.3f}, best params: {}\".format(gs.best_score_, gs.best_params_))\nwith open('neural_tuned_params.json', 'w') as f:\n    json.dump(gs.best_params_, f)","702496f0":"from sklearn.model_selection import GridSearchCV\nimport json\n\n# monitor = lambda MLPModel: all(MLPModel.history[-1, ('train_loss_best', 'valid_loss_best')])\nneural_model = NeuralNetClassifier(\n                    MLPModel,\n                    max_epochs=10,\n                    lr=0.01,\n                    warm_start=True,\n                    optimizer=torch.optim.Adam,\n                    iterator_train__shuffle=True,\n                    module__num_features=train[features].shape[1],\n                    device='cuda'\n                )\n# deactivate skorch-internal train-valid split and verbose logging\nneural_model.set_params(train_split=False, verbose=0)\nparams = {\n    'lr': [0.01],\n    'module__dropout': [0.25],\n    'module__n_hid': [128, 256, 512],\n}\ngs = GridSearchCV(neural_model, params, cv=3, scoring='f1_micro')\n\ngs.fit(train[features].to_numpy().astype(np.float32), train[target].to_numpy().astype(np.int64))\nprint(\"best score: {:.3f}, best params: {}\".format(gs.best_score_, gs.best_params_))\nwith open('neural_tuned_params.json', 'w') as f:\n    json.dump(gs.best_params_, f)","5edb89da":"## Neural Network","5b064196":"## read data ","f38cc9fa":"## submission","bee55b77":"## train","59dc80ed":"## feature engineering","c7168e2e":"## Hyper Parameter Tuning","d3a32faf":"## EDA"}}