{"cell_type":{"cf19cb3b":"code","a7e92313":"code","fe5c81b1":"code","c8fea75b":"code","05850d91":"code","6f23574e":"code","a5dfec04":"code","7e23c28f":"code","a177a4b0":"code","859292ea":"code","a8980836":"code","83b89b6d":"code","cfa48920":"code","2eecbd6e":"code","e2a0abf7":"code","42e3bf69":"code","84d5bea3":"code","289a2d47":"code","69bd8b28":"code","6e0f57b3":"code","48e89b29":"markdown","799c7506":"markdown","8332fdea":"markdown","8919f4d3":"markdown","d864aab9":"markdown","3cc6f44f":"markdown","06f96b7a":"markdown","02927a4c":"markdown","73d6528a":"markdown","891a72ea":"markdown","a2984d1c":"markdown","94b53a29":"markdown","6fa51a9f":"markdown","8e1835db":"markdown"},"source":{"cf19cb3b":"! pip install pytorch_pretrained_bert pytorch-nlp -q","a7e92313":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom bs4 import BeautifulSoup\nimport re\n\nfrom pytorch_pretrained_bert import BertTokenizer, BertForSequenceClassification, BertAdam\nimport torch\nfrom torch.utils.data import TensorDataset, RandomSampler, SequentialSampler, DataLoader\nfrom torch.nn import BCEWithLogitsLoss, Sigmoid\n\nfrom tqdm.notebook import tqdm, trange\n\nfrom keras.preprocessing.sequence import pad_sequences\n\nfrom sklearn.model_selection import train_test_split\n\n#\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ndevice","fe5c81b1":"!unzip -o '\/kaggle\/input\/jigsaw-toxic-comment-classification-challenge\/*.zip' -d \/kaggle\/working > \/dev\/null","c8fea75b":"df_train = pd.read_csv(\"train.csv\")\ndf_test = pd.read_csv(\"test.csv\")\nsample_submission = pd.read_csv(\"sample_submission.csv\")\n\nprint(df_train.shape, df_test.shape, sample_submission.shape)\ndf_train.head()","05850d91":"def strip(text):\n    soup = BeautifulSoup(text, 'html.parser')\n    text = re.sub('\\[[^]]*\\]', '', soup.get_text())\n    pattern=r\"[^a-zA-z0-9\\s,']\"\n    text=re.sub(pattern,'',text)\n    return text\n\ndf_train[\"comment_text\"] = df_train[\"comment_text\"].apply(strip)\ndf_test[\"comment_text\"] = df_test[\"comment_text\"].apply(strip)","6f23574e":"df_train.head()","a5dfec04":"train_sentences = df_train[\"comment_text\"]\ntest_sentences = df_test[\"comment_text\"]\ntrain_sentences = [\"[CLS] \"+ i + \" [SEP]\"for i in train_sentences]\ntest_sentences = [\"[CLS] \"+ i + \" [SEP]\"for i in test_sentences]\ntrain_sentences[0], test_sentences[0]","7e23c28f":"tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case = True)\n\ntrain_tokenizer_texts = list(map(lambda t: tokenizer.tokenize(t)[:510], tqdm(train_sentences)))\n\ntest_tokenizer_texts = list(map(lambda t: tokenizer.tokenize(t)[:510], tqdm(test_sentences)))\n\nnp.array(train_tokenizer_texts[0]), np.array(test_tokenizer_texts[0])","a177a4b0":"labels = df_train[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]].to_numpy()","859292ea":"#Padding\/truncating the train and test sentences to a size of 128 tokens per sentence\n\nMAX_LEN = 128\n\ninput_ids = [tokenizer.convert_tokens_to_ids(x) for x in tqdm(train_tokenizer_texts)]\ninput_ids = pad_sequences(sequences = input_ids, maxlen = MAX_LEN, dtype = 'long', padding='post', truncating='post')\n\ntest_input_ids = [tokenizer.convert_tokens_to_ids(x) for x in tqdm(test_tokenizer_texts)]\ntest_input_ids = pad_sequences(sequences = test_input_ids, maxlen = MAX_LEN, dtype = 'long', padding='post', truncating='post')\n\n\ninput_ids[0], test_input_ids[0]","a8980836":"#Creating an attention mask - For actual tokens its set to 1, for padding tokens its set to 0\ndef create_attention_masks(input_ids):\n    attention_masks = []\n    for seq in tqdm(input_ids):\n        seq_mask = [float(i>0) for i in seq]\n        attention_masks.append(seq_mask)\n    return np.array(attention_masks)\n\nattention_masks = create_attention_masks(input_ids)\ntest_attention_masks = create_attention_masks(test_input_ids)\nattention_masks[0], test_attention_masks[0]","83b89b6d":"# Use train_test_split to split our data and attention masks into train and validation sets for training\nX_train, X_val, y_train, y_val = train_test_split(input_ids, labels, random_state = 123, test_size = 0.20)\nattention_masks_train, attention_masks_val = train_test_split(attention_masks, random_state = 123, test_size = 0.20)","cfa48920":"# Convert all inputs and labels into torch tensors, the required datatype \nX_train = torch.tensor(X_train)\nX_val = torch.tensor(X_val)\ny_train = torch.tensor(y_train) \ny_val = torch.tensor(y_val)\nattention_masks_train = torch.tensor(attention_masks_train)\nattention_masks_val = torch.tensor(attention_masks_val)\n\ntest_input_ids = torch.tensor(test_input_ids)\ntest_attention_masks = torch.tensor(test_attention_masks)","2eecbd6e":"y_val.shape","e2a0abf7":"BATCH_SIZE = 32\n#Dataset wrapping tensors.\ntrain_data = TensorDataset(X_train, attention_masks_train, y_train)\nval_data = TensorDataset(X_val, attention_masks_val, y_val)\ntest_data = TensorDataset(test_input_ids, test_attention_masks)\n#Samples elements randomly. If without replacement(default), then sample from a shuffled dataset.\ntrain_sampler = RandomSampler(train_data)\nval_sampler = SequentialSampler(val_data)\ntest_sampler = SequentialSampler(test_data)\n#represents a Python iterable over a dataset\ntrain_dataloader = DataLoader(train_data, sampler = train_sampler, batch_size = BATCH_SIZE)\nval_dataloader = DataLoader(val_data, sampler = val_sampler, batch_size = BATCH_SIZE)\ntest_dataloader = DataLoader(test_data, sampler = test_sampler, batch_size = BATCH_SIZE)","42e3bf69":"#Inititaing a BERT model\nmodel = BertForSequenceClassification.from_pretrained('bert-base-uncased', num_labels = 6)\nmodel.cuda()","84d5bea3":"#Dividing the params into those which needs to be updated and rest\nparam_optimizer = list(model.named_parameters())\nno_decay = ['bias', 'gamma', 'beta']\noptimizer_grouped_parameters = [\n    {\n        'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n        'weight_decay_rate': 0.01\n    },\n    {\n        'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n        'weight_decay_rate': 0.0\n    }\n]\n\noptimizer = BertAdam(optimizer_grouped_parameters, lr = 2e-5, warmup = .1)","289a2d47":"#freeing up memory\ntorch.cuda.empty_cache()\nimport gc\ngc.collect()","69bd8b28":"#Empty the GPU memory as it might be memory and CPU intensive while training\ntorch.cuda.empty_cache()\n#Number of times the whole dataset will run through the network and model is fine-tuned\nepochs = 2\n#Iterate over number of epochs\nfor _ in trange(epochs, desc = \"Epoch\"):\n    #Switch model to train phase where it will update gradients\n    model.train()\n    #Initaite train and validation loss, number of rows passed and number of batches passed\n    tr_loss = 0\n    nb_tr_examples, nb_tr_steps = 0, 0\n    val_loss = 0\n    nb_val_examples, nb_val_steps = 0, 0\n    #Iterate over batches within the same epoch\n    for batch in tqdm(train_dataloader):\n        #Shift the batch to GPU for computation\n        batch = tuple(t.to(device) for t in batch)\n        #Load the input ids and masks from the batch\n        b_input_ids, b_input_mask, b_labels = batch\n        #Initiate gradients to 0 as they tend to add up\n        optimizer.zero_grad()\n        #Forward pass the input data\n        logits = model(b_input_ids, token_type_ids = None, attention_mask = b_input_mask)\n        #We will be using the Binary Cross entropy loss with added sigmoid function after that in BCEWithLogitsLoss\n        loss_func = BCEWithLogitsLoss()\n        #Calculate the loss between multilabel predicted outputs and actuals\n        loss = loss_func(logits, b_labels.type_as(logits))\n        #Backpropogate the loss and calculate the gradients\n        loss.backward()\n        #Update the weights with the calculated gradients\n        optimizer.step()\n        #Add the loss of the batch to the final loss, number of rows and batches\n        tr_loss += loss.item()\n        nb_tr_examples += b_input_ids.size(0)\n        nb_tr_steps += 1\n    #Print the current training loss \n    print(\"Train Loss: {}\".format(tr_loss\/nb_tr_examples))\n    #Switch the model to evaluate stage at which the gradients wont be updated\n    model.eval()\n    #Iterate over the validation data\n    for step, batch in enumerate(val_dataloader):\n        #Shift the validation data to GPUs for computation\n        batch = tuple(t.to(device) for t in batch)\n        #We dont want to update the gradients\n        with torch.no_grad():\n            #Load the input ids and masks from the batch\n            b_input_ids, b_input_mask, b_labels = batch\n            #Forward pass the input data\n            logits = model(b_input_ids, token_type_ids = None, attention_mask = b_input_mask)\n            #We will be using the Binary Cross entropy loss with added sigmoid function after that in BCEWithLogitsLoss\n            loss_func = BCEWithLogitsLoss()\n            #Calculate the loss between multilabel predicted outputs and actuals\n            loss = loss_func(logits, b_labels.type_as(logits))\n            #Add the loss of the batch to the final loss, number of rows and batches\n            val_loss += loss.item()\n            nb_val_examples += b_input_ids.size(0)\n            nb_val_steps += 1\n    #Print the current validation loss     \n    print(\"Valid Loss: {}\".format(val_loss\/nb_val_examples))","6e0f57b3":"outputs = []\n\n#Iterate over the test_loader \nfor step, batch in enumerate(test_dataloader):\n        #Transfer batch to GPUs\n        batch = tuple(t.to(device) for t in batch)\n        #We dont need to update gradients as we are just predicting\n        with torch.no_grad():\n            #Bring up the next batch of input_texts and attention_masks \n            b_input_ids, b_input_mask = batch\n            #Forward propogate the inputs and get output as logits\n            logits = model(b_input_ids, token_type_ids = None, attention_mask = b_input_mask)\n            #Pass the outputs through a sigmoid function to get the multi-label preditions\n            s = Sigmoid()\n            out = s(logits).to('cpu').numpy()    \n            #Add the predictions for this batch to the final list\n            outputs.extend(out)\n            \n#Merge test df and submission table to have all columns in a table\ndf_test = pd.merge(df_test, sample_submission, on = \"id\")\n#Assign the predictions to the toxic_output columns\ndf_test[[\"toxic\",\"severe_toxic\",\"obscene\",\"threat\",\"insult\",\"identity_hate\"]] = outputs\n#Drop text data as it is not expected in the submission file\ndf_test.drop([\"comment_text\"], axis = 1, inplace = True)\n#Saving the submission dataframe\ndf_test.to_csv(\"sample_submission.csv\", index = False)","48e89b29":"## 3. Text Preprocessing","799c7506":"We\u2019ll also create an iterator for our dataset using the torch DataLoader class. This helps save on memory during training because, unlike a for loop, with an iterator the entire dataset does not need to be loaded into memory. In Training we need to randomize the data for generalizing better so we use random sampler. We dont need that while predicting in validation or test set so we use sequential sampler.","8332fdea":"First of all let's clean the data:\n1. Cleaning the html tags using BeautifulSoup\n2. Removing non-alphanumeric data","8919f4d3":"## 1. Importing Libraries","d864aab9":"Now that we have our model loaded we need to grab the training hyperparameters from within the stored model.\nFor the purposes of fine-tuning, the authors recommend choosing from the following values:\n\nBatch size: 16, 32 (We chose 32 when creating our DataLoaders).\n\nLearning rate (Adam): 5e-5, 3e-5, 2e-5 (We\u2019ll use 2e-5).\n\nNumber of epochs: 2, 3, 4 (We\u2019ll use 2).","3cc6f44f":"## 4. Loading pre trained BERT and setting fine-tuning parameters\nFirst we load the bert model for sequence classification. We set the output neurons to 6 as we have 6 toxic types to be predicted for as yes or no.\n\nNext, we get weights for various layers and put them into a single list. Once that is done, we separate weight parameters (which needs to be updated) from bias, gamma, and beta parameters (which don't need to be updated). We filter one group without these values and another with them. Hence you see one group with weight_decay_rate 0.01 and another 0.0.","06f96b7a":"Pretraining for input to BERT:\n\n1. The first token of every sequence is always a special classification token ([CLS]).\n2. We separate the sentences with a special token ([SEP])","02927a4c":"## 7. TODOs \n* Try different versions of BERT - RoBERTa, DistilBERT and ALBERT\n* Hyperparameter Tuning for epochs, learning rate, batch_size, early_stopping\n\nDo upvote if you find it helpful \ud83d\ude01","73d6528a":"The sentences in our dataset obviously have varying lengths. BERT has two constraints:\n1. All sentences must be padded or truncated to a single, fixed length.\n2. The maximum sentence length is 512 tokens.\nPadding is done with a special [PAD] token, which is at index 0 in the BERT vocabulary. ","891a72ea":"## 6. Predicting and Submitting for Test Data","a2984d1c":"## 2.Reading Dataset","94b53a29":"## 0. Introduction\n\nBefore the introduction of Transformers, most state-of-the-art NLP systems relied on gated recurrent neural networks (RNNs), such as LSTMs and gated recurrent units (GRUs), with added attention mechanisms. The Transformer built on these attention technologies without using an RNN structure, highlighting the fact that the attention mechanisms alone, without recurrent sequential processing, are powerful enough to achieve the performance of RNNs with attention and are more parallelizable and requiring significantly less time to train.\n\n**What are Transformer Networks?**\n<img src = \"https:\/\/jalammar.github.io\/images\/t\/transformer_resideual_layer_norm_3.png\" width = 600>\nIn a machine translation application, it would take a sentence in one language, and output its translation in another. Inside we see an encoding component, a decoding component, and connections between them. The encoding component is composed of a stack of N = 6 identical layers.The decoding component is a stack of decoders of the same number.\n\nThe encoder\u2019s inputs first flow through a self-attention layer \u2013 a layer that helps the encoder look at other words in the input sentence as it encodes a specific word. The outputs of the self-attention layer are fed to a feed-forward neural network. The exact same feed-forward network is independently applied to each position. The decoder has both those layers, but between them is an attention layer that helps the decoder focus on relevant parts of the input sentence\n\nAs is the case in NLP applications in general, we begin by turning each input word into a vector using an embedding algorithm. The embedding only happens in the bottom-most encoder. The abstraction that is common to all the encoders is that they receive a list of vectors each of the size 512 \u2013 In the bottom encoder that would be the word embeddings, but in other encoders, it would be the output of the encoder that\u2019s directly below.\n\n**N.B.:** The word in each position flows through its own path in the encoder. There are dependencies between these paths in the self-attention layer. The feed-forward layer does not have those dependencies, however, and thus the various paths can be executed in parallel while flowing through the feed-forward layer.\n\n**Self-Attention:**\nSelf-attention is the method the Transformer uses to bake the \u201cunderstanding\u201d of other relevant words into the one we\u2019re currently processing.\n\nTransformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequencealigned RNNs or convolution.\n\nThe first step in calculating self-attention is to create three vectors from each of the encoder\u2019s input vectors (the embedding of each word). So for each word, we create a Query vector, a Key vector, and a Value vector. These vectors are created by multiplying the embedding by three matrices that we trained during the training process.Their dimensionality is 64, while the embedding and encoder input\/output vectors have dimensionality of 512. This is an architecture choice to make the computation of multiheaded attention constant.\n\nThe second step in calculating self-attention is to calculate a score. Say we\u2019re calculating the self-attention for the first word. We need to score each word of the input sentence against this word. The score determines how much focus to place on other parts of the input sentence as we encode a word at a certain position. The score is calculated by taking the dot product of the query vector with the key vector of the respective word we\u2019re scoring. So if we\u2019re processing the self-attention for the word in position #1, the first score would be the dot product of q1 and k1. The second score would be the dot product of q1 and k2.\n\nThe third and forth steps are to divide the scores by 8 (the square root of the dimension of the key vectors used \u2013 64. This leads to having more stable gradients. Then pass the result through a softmax operation. Softmax normalizes the scores so they\u2019re all positive and add up to 1.\n\nThe fifth step is to multiply each value vector by the softmax score. The intuition here is to keep intact the values of the word(s) we want to focus on, and drown-out irrelevant words (by multiplying them by tiny numbers like 0.001, for example).\n\nThe sixth step is to sum up the weighted value vectors. This produces the output of the self-attention layer at this position (for the first word).\n\nThe resulting vector is one we can send along to the feed-forward neural network. In the actual implementation, this calculation is done in matrix form for faster processing. \n\n**Multi-headed Self Attention**\nThe paper further refined the self-attention layer by adding a mechanism called \u201cmulti-headed\u201d attention. This improves the performance of the attention layer in two ways:\n\n1. It expands the model\u2019s ability to focus on different positions. Yes, in the example above, the final result contains a little bit of every other encoding, but it could be dominated by the the actual word itself. It would be useful if we\u2019re translating a sentence like \u201cThe animal didn\u2019t cross the street because it was too tired\u201d, we would want to know which word \u201cit\u201d refers to.\n\n2. It gives the attention layer multiple \u201crepresentation subspaces\u201d. With multi-headed attention, we have 8 sets of Query\/Key\/Value weight matrices. Each of these sets is randomly initialized. Then, after training, each set is used to project the input embeddings into a different representation subspace.\n\nWe concat the multiple matrices then multiple them by an additional weights matrix WO so that they can be sent to the feedforward network.\n\n<img src = \"https:\/\/jalammar.github.io\/images\/t\/transformer_multi-headed_self-attention-recap.png\" width = 600>\n\n**Positional Encoding**\nOne thing that\u2019s missing from the model as we have described it so far is a way to account for the order of the words in the input sequence. To address this, the transformer adds a vector to each input embedding. The intuition here is that adding these values to the embeddings provides meaningful distances between the embedding vectors once they\u2019re projected into Q\/K\/V vectors and during dot-product attention. Values of the left half are generated by one function (which uses sine), and the right half is generated by another function (which uses cosine). They're then concatenated to form each of the positional encoding vectors.\n\n**The Residuals**\nEach sub-layer (self-attention, ffnn) in each encoder has a residual connection around it, and is followed by a layer-normalization step.For e.g. Add input matrix X (2 * 4) to output matrix Z (2 * 4) and normalize row wise.\n\n**The Decoder Side**\n\n<img src = \"https:\/\/jalammar.github.io\/images\/t\/transformer_decoding_2.gif\"  width = 600>\nThe encoder start by processing the input sequence. The output of the top encoder is then transformed into a set of attention vectors K and V by multiplying with weight vectors. These are to be used by each decoder in its \u201cencoder-decoder attention\u201d layer which helps the decoder focus on appropriate places in the input sequence:\n\nThe following steps repeat the process until a special symbol is reached indicating the transformer decoder has completed its output. The output of each step is fed to the bottom decoder in the next time step, and the decoders bubble up their decoding results just like the encoders did. And just like we did with the encoder inputs, we embed and add positional encoding to those decoder inputs to indicate the position of each word.\n\nThe self attention layers in the decoder operate in a slightly different way than the one in the encoder:\n\nIn the decoder, the self-attention layer is only allowed to attend to earlier positions in the output sequence. This is done by masking future positions (setting them to -inf) before the softmax step in the self-attention calculation. The \u201cEncoder-Decoder Attention\u201d layer works just like multiheaded self-attention, except it creates its Queries matrix from the layer below it, and takes the Keys and Values matrix from the output of the encoder stack.\n\n**The Final Linear and Softmax Layer**\nThe Linear layer is a simple fully connected neural network that projects the vector produced by the stack of decoders, into a much, much larger vector called a logits vector. Let\u2019s assume that our model knows 10,000 unique English words ,that it\u2019s learned from its training dataset. This would make the logits vector 10,000 cells wide \u2013 each cell corresponding to the score of a unique word. That is how we interpret the output of the model followed by the Linear layer. The softmax layer then turns those scores into probabilities (all positive, all add up to 1.0). The cell with the highest probability is chosen, and the word associated with it is produced as the output for this time step.\n\n**Training**\nTraining is done by comparing the predicted values with the actuals using cross-entropy loss. Since, the model output one word at a time, we can use beam search to find the best output string\n\n\n\n### BERT\n\nBidirectional Encoder Representations from Transformers. BERT is basically a trained Transformer Encoder stack. We\n**pretrain** BERT to understand language and **fine-tune** BERT to learn specific task.\n<img src=\"https:\/\/miro.medium.com\/max\/1400\/1*LtF3nUFDhP62e9XAs6SlyQ.png\" width=\"600\">\n\n**Pretraining:**\nIn the phase, BERT learns What is language? What is context? This is done by doing 2 things simultaneosuly:\n1. Masked Language Moelling (MLM):\nThe masked language model randomly masks some of the tokens from the input, and the objective is to predict the original vocabulary id of the masked word based only on its context. For eg - The [MASK] brown fox jumped over the [MASK] dog. The MLM objective enables the representation to fuse the left and the right context, which allows us to pretrain a deep bidirectional Transformer. The training data generator chooses 15% of the token positions at random for prediction. If the i-th token is chosen, we replace the i-th token with (1) the [MASK] token 80% of the time (2) a random token 10% of the time (3)the unchanged i-th token 10% of the time.\n\n2. Next Sentence Prediction (NSP): \nIn this task, 2 sentences are taken and a binary classification is done if the two sentences are one after the another or not. A- Ajay is a cool dude B- He lives in Ohio.\n\n**Input:** \nWe pass in embedding vector for the input vector. This embedding vector is calculated from the input vector by summing the corresponding token, segment, and position embedding.\n<img src=\"https:\/\/cdn.nextjournal.com\/data\/QmaXGoXqdcjopjVzG1xAQQMS1BsEECSEoxRgLqoeDdetv7?filename=2019-06-12%2000-50-16%20%E7%9A%84%E8%9E%A2%E5%B9%95%E6%93%B7%E5%9C%96.png&content-type=image\/png\" width = 600>\n\nThe segment and position encoding is used for the model to understand temporal positioning as these inputs are given simultaneously and not at a time step like in LSTMs. For token embeddings, we use WordPiece embeddings with a 30,000 token vocabulary.  Specifically, when choosing the sentences A and B for each pretraining example, 50% of the time B is the actual next sentence that follows A (labeled as IsNext), and 50% of the time it is a random sentence from the corpus (labeled as NotNext).\n\n**Output:**\nThe output for the MLM is in the form of sentence vectors with masked tokens filled with the predicted tokens.\nEach predicted token is then softmaxed over all the words in vocabulary and cross entropy loss is calculated against the actuals. This is done for the masked words.\n\nThe output for the NSP is the result for [CLS] token in the output. \n\n**Fine-tuning:**\n\n<img src =\"https:\/\/qjjnh3a9hpo1nukrg1fwoh71-wpengine.netdna-ssl.com\/wp-content\/uploads\/2019\/04\/BERT-downstream-tasks_web.jpg\" width = 500>\nFor each task, we simply plug in the task specific inputs and outputs into BERT and slightly finetune all the parameters end-to-end. Compared to pre-training, fine-tuning is relatively inexpensive and fast.\n**Input:**\nWe pass in the sentence\/ sentences in form of embedding (separated by [SEP] for 2 sentences for tasks like Question Answering. For tasks like classification, we can skip that)\n**Output:**\nFor text classification, we will get the output in [CLS] token. For question answering tasks we get the Start and End Span.\n\nThe paper presents two model sizes for BERT (For, number of layers(i.e., Transformer blocks) as L, the hidden size as H and the number of self-attention heads as A):\n\nBERTBASE (L=12, H=768, A=12, Total Parameters=110M)\n\nBERTLARGE (L=24, H=1024,A=16, Total Parameters=340M).\n\nFor the pre-training corpus we use the BooksCorpus (800M words) and English Wikipedia (2,500M words).\n\nThese texts are largely taken from following resources:\n\nhttp:\/\/jalammar.github.io\/illustrated-transformer\/\n\nhttps:\/\/www.youtube.com\/watch?v=xI0HHN5XKDo","6fa51a9f":"To feed our text to BERT, it must be split into tokens, and then these tokens must be mapped to their index in the tokenizer vocabulary. The tokenization must be performed by the tokenizer included with BERT. We\u2019ll be using the \u201cuncased\u201d version here. ","8e1835db":"## 5. Tuning the BERT model\nThis is the phase where we fine-tune the BERT model on our dataset for as much epochs as required and validate the model performance on Validation data\n\nWe will be using BCEWithLogitLoss function for calculating multi-label loss between predicted and actual values. This is similar to adding sigmoid function at the end of the network and calculating binary-cross entropy loss like in Keras we did for LSTM and CNN. "}}