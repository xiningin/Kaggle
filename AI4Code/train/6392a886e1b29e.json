{"cell_type":{"a78d97b2":"code","dbe2323c":"code","90fd7a7a":"code","b461ee62":"code","b06df494":"code","8fb89a03":"code","959339de":"code","b2f6cb0e":"code","acdeec98":"code","f2c6f0bf":"code","5b725d45":"code","70017015":"code","3ce06691":"code","9dd973c6":"code","ada7bf45":"code","a0a7beaa":"code","c7b9e69d":"code","57844388":"code","5cc141ea":"code","9f875b60":"code","2af7abc0":"code","3bf1e73f":"code","cf0975ee":"code","edaff243":"code","e237d42c":"code","929cf7e8":"code","32697a3d":"code","e30b8bb2":"code","cb5323cd":"code","e62209aa":"code","6dd72f6a":"code","90d94952":"code","f15d3620":"code","5e4c13b1":"markdown","ea0dfe49":"markdown","15038524":"markdown","8a267fbd":"markdown","5b568978":"markdown","f8f517f5":"markdown","dbc7719d":"markdown"},"source":{"a78d97b2":"import pandas as pd\nimport numpy as np\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score, roc_auc_score","dbe2323c":"exo_train = pd.read_csv('..\/input\/kepler-labelled-time-series-data\/exoTrain.csv')\nexo_test = pd.read_csv('..\/input\/kepler-labelled-time-series-data\/exoTest.csv')","90fd7a7a":"exo_train.head()","b461ee62":"# checking to see if there is any row with nulll values\nexo_train.isnull().values.any()","b06df494":"exo_train['LABEL'].value_counts()","8fb89a03":"## since, the data is unbalanced we will use decision tree, random forest and gradient boosting as they perform better with unbalanced data and we will use F1-Score as a ","959339de":"x_train = exo_train.drop('LABEL', axis=1)\ny_train = exo_train['LABEL']","b2f6cb0e":"decisiontree_model = DecisionTreeClassifier()","acdeec98":"decisiontree_model.fit(x_train,y_train)","f2c6f0bf":"x_test = exo_test.drop('LABEL', axis=1)\ny_test = exo_test['LABEL']","5b725d45":"y_predict = decisiontree_model.predict(x_test)","70017015":"accuracy_score(y_test,y_predict)","3ce06691":"confusion_matrix(y_test, y_predict)","9dd973c6":"f1_score(y_test, y_predict)","ada7bf45":"from sklearn.ensemble import RandomForestClassifier","a0a7beaa":"randomforest_model = RandomForestClassifier(n_estimators=200,bootstrap=True,max_features='sqrt')","c7b9e69d":"randomforest_model.fit(x_train,y_train)","57844388":"yrandomforest_predict = randomforest_model.predict(x_test)","5cc141ea":"accuracy_score(y_test,yrandomforest_predict)","9f875b60":"confusion_matrix(y_test, yrandomforest_predict)","2af7abc0":"f1_score(y_test, yrandomforest_predict)","3bf1e73f":"from sklearn.ensemble import GradientBoostingClassifier","cf0975ee":"learning_rates = [0.05, 0.075, 0.1, 0.25, 0.5, 0.75, 1]\nfor l_rate in learning_rates:\n    gradient_model = GradientBoostingClassifier(n_estimators=200, learning_rate=l_rate, max_features=10, max_depth=2, random_state=60616)\n    gradient_model.fit(x_train,y_train)\n    ygradientboost_predict = gradient_model.predict(x_test)\n    \n    print('learning rate', l_rate)\n    print('Accuracy is',accuracy_score(y_test,ygradientboost_predict))\n    print('f1 score is',f1_score(y_test, yrandomforest_predict))\n    print('\\n')\n","edaff243":"import tensorflow as tf\ntf.random.set_seed(60616)","e237d42c":"#converting the headers to list\nnumeric_column_headers = x_train.columns.values.tolist()","929cf7e8":"#converting the features into rwo  features around the mean as that TFBT estimator only takes bucketed features\n# https:\/\/medium.com\/ml-book\/demonstration-of-tensorflow-feature-columns-tf-feature-column-3bfcca4ca5c4 (Good link)\nbc_fn = tf.feature_column.bucketized_column\nnc_fn = tf.feature_column.numeric_column\nbucketized_features = [bc_fn(source_column=nc_fn(key=column),\n                             boundaries=[x_train[column].mean()])\n                       for column in numeric_column_headers]","32697a3d":"all_features = bucketized_features","e30b8bb2":"# converting label 2 to 1 and 1 to 0 as labels should be less than num_class - 1 for tf.estimator.BoostedTreesClassifier\ny_train_new = pd.Series(np.where(y_train.values == 2, 1, 0),y_train.index)\ny_test_new = pd.Series(np.where(y_test.values == 2, 1, 0),y_test.index)","cb5323cd":"batch_size = 32\n\ndef make_input_fn(dataframe, y, shuffle=True, n_epochs=None, batch_size=32):\n    def input_fn():\n        dataset = tf.data.Dataset.from_tensor_slices((dict(dataframe), y))\n        if shuffle:\n            dataset = dataset.shuffle(len(dataframe))\n        \n        # For training, cycle thru dataset as many times as need (n_epochs=None).\n        dataset = dataset.repeat(n_epochs)\n        \n        # In memory training doesn't use batching.\n        dataset = dataset.batch(batch_size)\n        return dataset\n    \n    return input_fn\n\n\ntrain_input_fn = make_input_fn(x_train, y_train_new)\ntest_input_fn = make_input_fn(x_test, y_test_new, shuffle=False, n_epochs=1)","e62209aa":"n_trees = 100\ntbts_model = tf.estimator.BoostedTreesClassifier(feature_columns=all_features, n_trees=n_trees, n_batches_per_layer=batch_size)","6dd72f6a":"n_steps = 100\ntbts_model.train(input_fn=train_input_fn,steps=n_steps)","90d94952":"results = tbts_model.evaluate(input_fn=test_input_fn)","f15d3620":"print(pd.Series(results))","5e4c13b1":"## TensorFlow boosted trees estimator","ea0dfe49":"## Implementing decision tree","15038524":"<p>Accuracy is similar here too<\/p>","8a267fbd":"## Implementing Random Forest","5b568978":"<p>we can see that the accuracy has been improved with gradient boost but f1 score remains the same as compared to random forest<\/p>","f8f517f5":"<p>Both the accuracy and f1 score has been improved compraed to decision tree<\/p>","dbc7719d":"## Implementing Greadient Boosting"}}