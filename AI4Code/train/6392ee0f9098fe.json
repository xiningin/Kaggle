{"cell_type":{"3131b48d":"code","9e94e287":"code","dc79fb21":"code","7fcf45b8":"code","9e619c5b":"code","44ec45de":"code","5d44a73f":"code","461f5adc":"code","d5004c93":"code","ed3a14d4":"code","a4cd0dc7":"code","d15189aa":"code","960ee60e":"code","ad4ab5f5":"code","a00f5372":"code","636750a1":"code","547f559e":"code","ea9a3e0e":"code","dbcd0c13":"code","333b9c47":"code","89000915":"code","990e2203":"code","d336113f":"code","291e72b6":"code","a190f9e7":"code","b6f39495":"code","b26155f8":"code","cc419722":"code","4702c949":"code","d449ec58":"code","98a68de6":"code","0cc07626":"code","682eca01":"code","ccf9f233":"code","5b0cc103":"code","9c0a857a":"code","9af18e9c":"code","e98c60ef":"code","1056007a":"code","7d098006":"code","8e90e1ce":"code","897fcb57":"code","145ade38":"code","d85d5223":"code","fea8f2c3":"code","1b2f7461":"code","9b2b7cf0":"code","160aacf7":"markdown","b8e8881b":"markdown","e70f7b4b":"markdown","670a3e3f":"markdown","051a6f56":"markdown","c84bdf2c":"markdown","67af5a9b":"markdown","087eec6d":"markdown","2d598f72":"markdown","dc642689":"markdown","e22ea6b5":"markdown","e5086c5b":"markdown","437082bf":"markdown","4fc05cc6":"markdown","ef584bd2":"markdown","d10d8edc":"markdown","e608fa8c":"markdown","b082b643":"markdown"},"source":{"3131b48d":"# All Necessary Imports\nimport os\nimport numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom plotly.offline import download_plotlyjs, init_notebook_mode, plot, iplot\nimport cufflinks as cf\ninit_notebook_mode(connected=True)\ncf.go_offline()\n%matplotlib inline\n\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.preprocessing import StandardScaler, PolynomialFeatures\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.pipeline import make_pipeline\n\nfrom sklearn.linear_model import LinearRegression, RidgeCV, LassoCV, ElasticNetCV\nfrom sklearn.svm import SVR\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor, AdaBoostRegressor, GradientBoostingRegressor, StackingRegressor\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor","9e94e287":"# Files\nprint(os.listdir('..\/input\/house-prices-advanced-regression-techniques\/'))","dc79fb21":"# Locating our Data files\ndata_desc = '..\/input\/house-prices-advanced-regression-techniques\/data_description.txt'\ntrain_path = '..\/input\/house-prices-advanced-regression-techniques\/train.csv'\ntest_path = '..\/input\/house-prices-advanced-regression-techniques\/test.csv'","7fcf45b8":"# data description\ndata_desc = open(data_desc, mode='r')\nfor line in data_desc.readlines():\n    print(line[:-1])","9e619c5b":"# Reading train and test data\ntrain_data = pd.read_csv(train_path, index_col=0)\ntest_data = pd.read_csv(test_path, index_col=0)","44ec45de":"# Having an eagle eye view of our data\ntrain_data.head(3)","5d44a73f":"test_data.head(3)","461f5adc":"# Checking for only missing values in Train Data\nprint('Missing Values in Train Data:\\n')\nprint(train_data.isna().sum()[train_data.isna().sum() != 0])","d5004c93":"# Checking for only missing values in Test Data\nprint('Missing Values in Test Data :\\n')\nprint(test_data.isna().sum()[test_data.isna().sum() != 0])","ed3a14d4":"# Before starting feature engineering section, lets combine train & test data so that we do not need to perform similar operations twice.\ny = train_data['SalePrice']        # target value\ndata = pd.concat([train_data.drop('SalePrice', axis=1), test_data])\ndata.head(3)","a4cd0dc7":"# Info\n\n# train_data\nprint('Training Data :')\nprint('\\tNo. of Entries : {rows:}\\n\\tFeatures Count : {cols:}'.format(rows=train_data.shape[0], cols=train_data.shape[1]))\nprint('\\tNumeric Features Count : {:}'.format(train_data.select_dtypes(include=['integer', 'float']).shape[1]))\nprint('\\tCategorical Features Count : {:}\\n'.format(train_data.select_dtypes(include=['object']).shape[1]))\n\n# test_data\nprint('Test Data :')\nprint('\\tNo. of Entries : {rows:}\\n\\tFeatures Count : {cols:}'.format(rows=test_data.shape[0], cols=test_data.shape[1]))\nprint('\\tNumeric Features Count : {:}'.format(test_data.select_dtypes(include=['integer', 'float']).shape[1]))\nprint('\\tCategorical Features Count : {:}'.format(test_data.select_dtypes(include=['object']).shape[1]))\n\n# Data\nprint('Data :')\nprint('\\tNo. of Entries : {rows:}\\n\\tFeatures Count : {cols:}'.format(rows=data.shape[0], cols=data.shape[1]))\nprint('\\tNumeric Features Count : {:}'.format(data.select_dtypes(include=['integer', 'float']).shape[1]))\nprint('\\tCategorical Features Count : {:}'.format(data.select_dtypes(include=['object']).shape[1]))","d15189aa":"# Creating a function that takes in feature type and returns a dataframe with missing_values and missing_percentage.\ndef missing_data_func(feature_type):          # feature_type : is categorical or numeric\n    if feature_type.lower()[0] == 'c':   # categorical\n        data_type = ['object']\n    elif feature_type.lower()[0] == 'n': # numerical\n        data_type = ['integer', 'float']\n    else :\n        data_type = ['integer', 'float', 'object']    # categorical + numeric\n    \n    missing_value = data.select_dtypes(include=data_type).isna().sum()[data.select_dtypes(include=data_type).isna().sum() != 0].sort_values(ascending=False) \n    missing_percentage = (missing_value\/2919)*100\n    \n    # dataframe\n    missing_data = pd.DataFrame(data=[missing_value, missing_percentage], index=['Count', 'Percentage'])\n    return missing_data\n\n# Function for visualizing missing data : takes in dataframe created using missing_data_func() \ndef visualize_missing_data(Data, title):\n    # Data : missing_data from missing_data_func()\n    # title : title for plot\n    sns.set(font_scale=1.3)\n    plt.figure(figsize=(20, 5))\n    sns.barplot(x=Data.T.index, y=Data.T['Percentage'], palette=\"rocket\")\n    plt.xticks(rotation=90)\n    plt.title(label=title, fontdict={'fontsize':18})\n    plt.show()","960ee60e":"# Missing Features in data\nvisualize_missing_data(Data=missing_data_func('k'), title='Missing Features in data')\nmissing_data_func('k')","ad4ab5f5":"# Checking correlation in train_data using heatmap\nplt.figure(figsize=(25, 25))\nsns.set(font_scale=1.4)\nsns.heatmap(data=train_data.corr().round(decimals=2), cbar=False, annot=True, cmap='coolwarm', linewidths=.5, annot_kws={'size':13})","a00f5372":"# Dropping numeric features from data\n\n# Multi-collinearity features\ndata.drop(labels=['YearRemodAdd', 'LotArea', '1stFlrSF', 'TotRmsAbvGrd', 'GarageYrBlt', 'GarageCars'], axis=1, inplace=True)\n\n# Zero-correlation features\ndata.drop(labels=['MSSubClass', 'OverallCond', 'BsmtFinSF2', 'BsmtUnfSF', 'BsmtFullBath', 'LowQualFinSF', 'BsmtHalfBath',\n                        'BedroomAbvGr', 'KitchenAbvGr', 'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea',\n                        'MiscVal', 'MoSold', 'YrSold'], axis=1, inplace=True)","636750a1":"# So our Numeric Features are reduced from 36 to 14 (excluding SalePrice)\nplt.figure(figsize=(10, 6))\nsns.set(font_scale=1.1)\nsns.heatmap(data=train_data[[\n                             'OverallQual', 'GrLivArea','GarageArea', 'TotalBsmtSF', 'FullBath', 'YearBuilt', 'MasVnrArea',\n                             'Fireplaces', 'BsmtFinSF1', 'LotFrontage', 'WoodDeckSF', '2ndFlrSF', 'OpenPorchSF', 'HalfBath', 'SalePrice'\n                            ]].corr().round(decimals=2), cbar=True, annot=True, cmap='coolwarm', linewidths=.5, annot_kws={'size':10}, vmax=.8)\nplt.title(label='Selected Numeric Features corr. using heatmap', fontdict={'fontsize':18})","547f559e":"# First Thing First, lets drop features missing more than 20% of its data\ndata.drop(labels=['PoolQC', 'MiscFeature', 'Alley', 'Fence', 'FireplaceQu'], axis=1, inplace=True)","ea9a3e0e":"# Imbalanced Data\n\n# Calculating Categorical features with most occuring variable in percentage \ncategorical = dict()\nfor column in data.select_dtypes(include=['object']).columns:\n    categorical[column] = (train_data[column].value_counts()[0].sum(),\n                           (train_data[column].value_counts()[0].sum()\/train_data[column].value_counts().sum())*100,\n                          train_data[column].value_counts().shape[0])\n    \n# Dataframe\n            # Count : Most occuring\/dominant feature variable count. (out of 2919 datapoints)\n            # Percentage : Most occuring variable in percentage.\n            # variable type : variable counts in an feature.\ndominant_features = pd.DataFrame(data=categorical.values(), index=categorical.keys(), columns=['Count', 'Percentage',\n                                                                           'variable type']).sort_values(by='Percentage')\ndominant_features","dbcd0c13":"# Dropping Features with imbalanced dataset i.e features with one dominant variable (>80%)\ndata.drop(labels=dominant_features[dominant_features['Percentage'] > 80].index,axis=1, inplace=True)","333b9c47":"# CountPlot : Checking balanced categorical data count\n# Categorical Features : 18\n\nfig, axes = plt.subplots(nrows=6, ncols=3, figsize=(32, 50))\nfig.suptitle('Count Plot : Categorical Data',fontweight =\"bold\", fontsize=45)\nsns.set()\n\nrow=[x for x in range(0, 6)]\ncol=[0]\ncount = 1\n\nfor column in data.select_dtypes(include=['object']).columns:\n    # BoxPlot\n    fig_1 = sns.countplot(x=train_data[column], ax=axes[row[0], col[0]], palette='plasma',\n               order=train_data.groupby(column)['SalePrice'].mean().sort_values().index)\n    axes[row[0], col[0]].tick_params(axis='both', labelsize='xx-large', labelrotation=45)\n    fig_1.set_xlabel(xlabel=fig_1.get_xlabel(), fontsize=25)\n    # Column Switch\n    if col[0] == 0:      \n        col[0] = 1\n    elif col[0] ==1:\n        col[0] = 2\n    else:\n        col[0] = 0\n    # Row Switch\n    count += 1\n    if count > 3:\n        count = 1\n        row.pop(0)\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.97)","89000915":"# Missing features for Numeric & Categorical data_type \nvisualize_missing_data(Data=missing_data_func('numeric'), title='Missing Numeric Features in data')\nvisualize_missing_data(Data=missing_data_func('categorical'), title='Missing Categorical Features in data')","990e2203":"# Handling Missing Numeric Data\nnum_data = data.select_dtypes(include=['integer', 'float'])   # data with only numeric features\nfor features in num_data.isna().sum()[num_data.isna().sum() != 0].index:\n    data[features].fillna(data[features].mean(), inplace=True)\n    \n# Handling Missing Categorical Data\ncat_data = data.select_dtypes(include=['object'])    # data with only categorical features\nfor features in cat_data.isna().sum()[cat_data.isna().sum() != 0].index:\n    data[features].fillna(data[features].mode()[0], inplace=True)\n    \n# Rechecking\ndata.isna().sum()[data.isna().sum() != 0]","d336113f":"# Categorical Features : 18\n# BoxPlot : Cheking behaviour of every attribute in an individual category wrt SalePrice before label encoding\n\nfig, axes = plt.subplots(nrows=6, ncols=3, figsize=(32, 45))\nfig.suptitle('Non-Linearity in Categorical Data Before Label Encoding',fontweight =\"bold\", fontsize=45)\nsns.set()\n\nrow=[x for x in range(0, 6)]\ncol=[0]\ncount = 1\n\nfor column in data.select_dtypes(include=['object']).columns:\n    # BoxPlot\n    fig_1 = sns.boxplot(x=train_data[column], y=train_data['SalePrice'], ax=axes[row[0], col[0]], palette='YlOrRd',\n               order=train_data.groupby(column)['SalePrice'].mean().sort_values().index)\n    axes[row[0], col[0]].tick_params(axis='both', labelsize='xx-large', labelrotation=45)\n    fig_1.set_xlabel(xlabel=fig_1.get_xlabel(), fontsize=25)\n    # Column Switch\n    if col[0] == 0:      \n        col[0] = 1\n    elif col[0] ==1:\n        col[0] = 2\n    else:\n        col[0] = 0\n    # Row Switch\n    count += 1\n    if count > 3:\n        count = 1\n        row.pop(0)\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.97)","291e72b6":"# mapping function\nms_zoning = {'C (all)':0, 'RM':1, 'RH':1, 'RL':2, 'FV':3}\nlot_shape = {'Reg':0, 'IR1':1, 'IR2':2, 'IR3':2}\nlot_config = {'Inside':0, 'FR2':1, 'Corner':2, 'CulDSac':3, 'FR3':3}\nneighbor_hood = {'MeadowV':0, 'IDOTRR':0, 'BrDale':0, 'BrkSide':1, 'Edwards':1, 'OldTown':1,\n       'Sawyer':2, 'Blueste':2, 'SWISU':2, 'NPkVill':2, 'NAmes':3, 'Mitchel':3, 'SawyerW':4,\n       'NWAmes':5, 'Gilbert':5, 'Blmngtn':5, 'CollgCr':6, 'Crawfor':6, 'ClearCr':7,\n       'Somerst':7, 'Veenker':8, 'Timber':8, 'StoneBr':9, 'NridgHt':9, 'NoRidge':10}\nhouse_style = {'1.5Unf':0, 'SFoyer':1, '1.5Fin':1, '2.5Unf':1, 'SLvl':1, '1Story':2, '2Story':3, '2.5Fin':3}\nroof_style = {'Gambrel':0, 'Gable':0, 'Mansard':1, 'Flat':1, 'Hip':1, 'Shed':1}\nexterior_1st = {'BrkComm':0, 'AsphShn':0, 'CBlock':0, 'AsbShng':0, 'MetalSd':1, 'Wd Sdng':1,\n       'WdShing':2, 'Stucco':3, 'HdBoard':3, 'Plywood':3, 'BrkFace':4, 'VinylSd':5,\n       'CemntBd':6, 'Stone':6, 'ImStucc':6}\nexterior_2nd = {'CBlock':0, 'AsbShng':0, 'Brk Cmn':0, 'AsphShn':0, 'Wd Sdng':1, 'MetalSd':1,\n       'Stucco':2, 'Stone':2, 'Wd Shng':2, 'HdBoard':3, 'Plywood':3, 'BrkFace':4,\n       'VinylSd':4, 'CmentBd':5, 'ImStucc':5, 'Other':5}\nmasvnr_type = {'BrkCmn':0, 'None':0, 'BrkFace':1, 'Stone':2}\nexter_qual = {'Fa':0, 'TA':1, 'Gd':2, 'Ex':3}\nfoundation = {'Slab':0, 'BrkTil':1, 'CBlock':2, 'Stone':3, 'Wood':3, 'PConc':3}\nbsmt_qual = {'Fa':0, 'TA':1, 'Gd':2, 'Ex':3}\nbsmt_exposure = {'No':0, 'Mn':1, 'Av':2, 'Gd':3}\nbsmtfin_type1 = {'Rec':0, 'BLQ':0, 'LwQ':1, 'ALQ':1, 'Unf':2, 'GLQ':3}\nheat_qc = {'Po':0, 'Fa':0, 'TA':1, 'Gd':2, 'Ex':3}\nkitchen_qual = {'Fa':0, 'TA':1, 'Gd':2, 'Ex':3}\ngarage_type = {'CarPort':0, 'Detchd':0, '2Types':1, 'Basment':1, 'Attchd':2, 'BuiltIn':3}\ngarage_finish = {'Unf':0, 'RFn':1, 'Fin':2}","a190f9e7":"# Label Encoding\ndata['MSZoning'] = data['MSZoning'].map(ms_zoning)\ndata['LotShape'] = data['LotShape'].map(lot_shape)\ndata['LotConfig'] = data['LotConfig'].map(lot_config)\ndata['Neighborhood'] = data['Neighborhood'].map(neighbor_hood)\ndata['HouseStyle'] = data['HouseStyle'].map(house_style)\ndata['RoofStyle'] = data['RoofStyle'].map(roof_style)\ndata['Exterior1st'] =  data['Exterior1st'].map(exterior_1st)\ndata['Exterior2nd'] =  data['Exterior2nd'].map(exterior_2nd)\ndata['MasVnrType'] =  data['MasVnrType'].map(masvnr_type)\ndata['ExterQual'] =  data['ExterQual'].map(exter_qual)\ndata['Foundation'] = data['Foundation'].map(foundation)\ndata['BsmtQual'] = data['BsmtQual'].map(bsmt_qual)\ndata['BsmtExposure'] = data['BsmtExposure'].map(bsmt_exposure)\ndata['BsmtFinType1'] = data['BsmtFinType1'].map(bsmtfin_type1)\ndata['HeatingQC'] = data['HeatingQC'].map(heat_qc)\ndata['KitchenQual'] = data['KitchenQual'].map(kitchen_qual)\ndata['GarageType'] = data['GarageType'].map(garage_type)\ndata['GarageFinish'] = data['GarageFinish'].map(garage_finish)","b6f39495":"# Categorical Features : 18\n# BoxPlot : Cheking behaviour of every attribute in an individual category wrt SalePrice after label encoding\n\nfig, axes = plt.subplots(nrows=6, ncols=3, figsize=(32, 45))\nfig.suptitle('Creating Linearity in Categorical Data by Label Encoding',fontweight =\"bold\", fontsize=45)\nsns.set()\n\nrow=[x for x in range(0, 6)]\ncol=[0]\ncount = 1\n\nfor column in data[['MSZoning', 'LotShape', 'LotConfig', 'Neighborhood', 'HouseStyle', 'RoofStyle',\n                   'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'Foundation', 'BsmtQual',\n                   'BsmtExposure', 'BsmtFinType1', 'HeatingQC', 'KitchenQual', 'GarageType', 'GarageFinish']].columns:\n    # BoxPlot\n    fig_1 = sns.boxplot(x=data[column][:1460], y=train_data['SalePrice'], ax=axes[row[0], col[0]], palette='YlOrRd')\n    axes[row[0], col[0]].tick_params(axis='both', labelsize='xx-large', labelrotation=45)\n    fig_1.set_xlabel(xlabel=fig_1.get_xlabel(), fontsize=25)\n    # Column Switch\n    if col[0] == 0:      \n        col[0] = 1\n    elif col[0] ==1:\n        col[0] = 2\n    else:\n        col[0] = 0\n    # Row Switch\n    count += 1\n    if count > 3:\n        count = 1\n        row.pop(0)\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.97)","b26155f8":"# Numeric Features : 14\n# Detecting Outliers using scatterplot\n\nfig, axes = plt.subplots(nrows=7, ncols=2, figsize=(25, 45))\nfig.suptitle('Detecting Outliers in Numeric Data',fontweight =\"bold\", fontsize=45)\nsns.set()\n\nrow=[x for x in range(0, 7)]\ncol=[0]\ncount = 1\n\nfor column in data[['LotFrontage', 'OverallQual', 'YearBuilt', 'MasVnrArea', 'BsmtFinSF1',\n       'TotalBsmtSF', '2ndFlrSF', 'GrLivArea', 'FullBath', 'HalfBath',\n       'Fireplaces', 'GarageArea', 'WoodDeckSF', 'OpenPorchSF']].columns:\n    # Scatterplot\n    fig_1 = sns.scatterplot(x=data[column][:1460], y=train_data['SalePrice'], ax=axes[row[0], col[0]],\n                    color='red', hue=train_data['SalePrice'], palette='gnuplot')\n    axes[row[0], col[0]].tick_params(axis='both', labelsize='xx-large') #, labelrotation=45\n    fig_1.set_xlabel(xlabel=fig_1.get_xlabel(), fontsize=20)\n    \n    # Column Switch\n    if col[0] == 0:      \n        col[0] = 1\n    else:\n        col[0] = 0\n    # Row Switch\n    count += 1\n    if count > 2:\n        count = 1\n        row.pop(0)\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.97)","cc419722":"# Dropping Outliers\noutlier_data = data[:][:1460]      # Training data\noutlier_data['SalePrice'] = y      # Training data with target variable\n\n# LotFrontage\noutlier_data.drop(labels=outlier_data[outlier_data['LotFrontage']>300].index, axis=0, inplace=True)\n\n# YearBuilt\noutlier_data.drop(labels=outlier_data[(outlier_data['YearBuilt']<1900) & (outlier_data['SalePrice']>300000)].index, axis=0, inplace=True)\noutlier_data.drop(labels=outlier_data[(outlier_data['YearBuilt']<2000) & (outlier_data['SalePrice']>600000)].index, axis=0, inplace=True)\n\n# MasVnrArea\noutlier_data.drop(labels=outlier_data[outlier_data['MasVnrArea']>1160].index, axis=0, inplace=True)\noutlier_data.drop(labels=outlier_data[outlier_data['SalePrice']>700000].index, axis=0, inplace=True)\n\n# BsmtFinSF1\noutlier_data.drop(labels=outlier_data[(outlier_data['SalePrice']>700000) | (outlier_data['BsmtFinSF1']>5000)].index, axis=0, inplace=True)\n\n# TotalBsmtSF\noutlier_data.drop(labels=outlier_data[(outlier_data['TotalBsmtSF']>5000)].index, axis=0, inplace=True)\n\n# GrLivArea\noutlier_data.drop(labels=outlier_data[(outlier_data['GrLivArea']>4000) & (outlier_data['SalePrice']<300000)].index, axis=0, inplace=True)\n\n# GarageArea\noutlier_data.drop(labels=outlier_data[(outlier_data['GarageArea']>1200)&(outlier_data['SalePrice']<300000)].index, axis=0, inplace=True)\noutlier_data.drop(labels=outlier_data[outlier_data['SalePrice']>700000].index, axis=0, inplace=True)","4702c949":"# Target Variable\ny = outlier_data['SalePrice']\n\n# Train Data\ntrain_data = outlier_data.drop(labels='SalePrice', axis=1)\n\n# Test Data\ntest_data = data[1460:]","d449ec58":"# Numeric Features : 14\n# Regplot : Cheking linear relation of every feature wrt SalePrice\n\nfig, axes = plt.subplots(nrows=7, ncols=2, figsize=(25, 45))\nfig.suptitle('Linearity(2nd Order) of Numeric Features wrt SalePrice',fontweight =\"bold\", fontsize=45)\nsns.set()\n\nrow=[x for x in range(0, 7)]\ncol=[0]\ncount = 1\n\nfor column in train_data.drop(labels=['MSZoning', 'LotShape', 'LotConfig', 'Neighborhood', 'HouseStyle', 'RoofStyle',\n                   'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'Foundation', 'BsmtQual',\n                   'BsmtExposure', 'BsmtFinType1', 'HeatingQC', 'KitchenQual', 'GarageType', 'GarageFinish'], axis=1).columns:\n    # Regplot\n    fig_1 = sns.regplot(x=train_data[column], y=y, ax=axes[row[0], col[0]],\n               line_kws={'color': 'black'}, scatter_kws={'alpha':0.5}, color='red', order=2, ci=None)\n    axes[row[0], col[0]].tick_params(axis='both', labelsize='x-large')\n    fig_1.set_xlabel(xlabel=fig_1.get_xlabel(), fontsize=25)\n    # Column Switch\n    if col[0] == 0:      \n        col[0] = 1\n    else:\n        col[0] = 0\n    # Row Switch\n    count += 1\n    if count > 2:\n        count = 1\n        row.pop(0)\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.97)","98a68de6":"# Categorical Features : 18 \n# Regplot : Checking linear relation of every feature wrt SalePrice\n\nfig, axes = plt.subplots(nrows=6, ncols=3, figsize=(32, 45))\nfig.suptitle('Linearity of Categorical Features wrt SalePrice',fontweight =\"bold\", fontsize=45)\nsns.set()\n\nrow=[x for x in range(0, 6)]\ncol=[0]\ncount = 1\n\nfor column in train_data[['MSZoning', 'LotShape', 'LotConfig', 'Neighborhood', 'HouseStyle', 'RoofStyle',\n                   'Exterior1st', 'Exterior2nd', 'MasVnrType', 'ExterQual', 'Foundation', 'BsmtQual',\n                   'BsmtExposure', 'BsmtFinType1', 'HeatingQC', 'KitchenQual', 'GarageType', 'GarageFinish']].columns:\n    # Regplot\n    fig_1 = sns.regplot(x=train_data[column], y=y, ax=axes[row[0], col[0]],\n               line_kws={'color': 'black'}, scatter_kws={'alpha':0.4}, color='red')\n    axes[row[0], col[0]].tick_params(axis='both', labelsize='xx-large')\n    fig_1.set_xlabel(xlabel=fig_1.get_xlabel(), fontsize=25)\n    # Column Switch\n    if col[0] == 0:      \n        col[0] = 1\n    elif col[0] ==1:\n        col[0] = 2\n    else:\n        col[0] = 0\n    # Row Switch\n    count += 1\n    if count > 3:\n        count = 1\n        row.pop(0)\n    fig.tight_layout()\n    fig.subplots_adjust(top=0.97)","0cc07626":"# training data\ntrain_data.head(2)","682eca01":"# test data\ntest_data.head(2)","ccf9f233":"# target label\ny.head(2)","5b0cc103":"# Scaling target variable\nscaler = StandardScaler()\ny = pd.DataFrame(data=scaler.fit_transform(np.array(y).reshape(1448,1)), index=y.index, columns=['SalePrice'])\ny = np.ravel(y)","9c0a857a":"# Function to ease our work\n# Cross_val_score func :\ndef cross_val(model, x):\n    score = -cross_val_score(estimator=model, X=x, y=y, scoring='neg_root_mean_squared_error', cv=10, verbose=1, n_jobs=2).mean()\n    return score","9af18e9c":"# Linear Regressor\nlinear_reg = make_pipeline(StandardScaler(), LinearRegression())\n\n# Polynomial Regressor\npoly_feat = PolynomialFeatures(degree=2)\npoly_reg = make_pipeline(poly_feat, StandardScaler(), LinearRegression())\n\n# Ridge Regressor\nridge_reg = make_pipeline(StandardScaler(), RidgeCV(alphas=np.linspace(start=0.00001, stop=100, num=350), scoring='r2', cv=8))\n\n# Lasso Regressor\nlasso_reg = make_pipeline(StandardScaler(), LassoCV(alphas=np.linspace(start=0.00001, stop=100, num=350), cv=8))\n\n# ElasticNet Regressor\nelasticnet_reg = make_pipeline(StandardScaler(), ElasticNetCV(l1_ratio=[.2, .4, .5, .6, .8], \n                               alphas=np.linspace(start=0.00001, stop=100, num=350), cv=8, max_iter=1000))\n\n# Support Vector Machine\nsvr = make_pipeline(StandardScaler(), SVR(C=10, epsilon=0.0001))\n\n# Decision Tree Regressor\ndecision_tree = DecisionTreeRegressor(ccp_alpha=0.0, criterion='mse', max_depth=15,\n                      max_features=None, max_leaf_nodes=None,\n                      min_impurity_decrease=0.0, min_impurity_split=None,\n                      min_samples_leaf=10, min_samples_split=2,\n                      min_weight_fraction_leaf=0, presort='deprecated',\n                      random_state=None, splitter='best')\n\n# Random Forest Regressor\nrfr = RandomForestRegressor(bootstrap=True, ccp_alpha=0.0, criterion='mse',\n                      max_depth=40, max_features='auto', max_leaf_nodes=None,\n                      max_samples=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0,\n                      n_estimators=100, n_jobs=None, oob_score=False,\n                      random_state=None, verbose=0, warm_start=False)\n\n# Adaboost Regressor\nadaboost_reg = AdaBoostRegressor(base_estimator=DecisionTreeRegressor(ccp_alpha=0.0,\n                      criterion='mse', max_depth=None, max_features=None,\n                      max_leaf_nodes=None, min_impurity_decrease=0.0,\n                      min_impurity_split=None, min_samples_leaf=1,\n                      min_samples_split=2, min_weight_fraction_leaf=0.0,\n                      presort='deprecated', random_state=None,\n                      splitter='best'),\n                  learning_rate=1, loss='square', n_estimators=350,\n                  random_state=None)\n\n# Gradient Boosting Regressor\ngbr = GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0, criterion='friedman_mse',\n                          init=None, learning_rate=0.02, loss='ls', max_depth=7,\n                          max_features='sqrt', max_leaf_nodes=25,\n                          min_impurity_decrease=0.0, min_impurity_split=None,\n                          min_samples_leaf=1, min_samples_split=2,\n                          min_weight_fraction_leaf=0.0, n_estimators=550,\n                          n_iter_no_change=None, presort='deprecated',\n                          random_state=None, subsample=0.65, tol=0.0001,\n                          validation_fraction=0.1, verbose=0, warm_start=False)\n\n# Light Gradient Boosting Regressor\nlgbr = LGBMRegressor(boosting='gbdt', boosting_type='gbdt', class_weight=None,\n              colsample_bytree=1.0, importance_type='split', subsample_freq=0,\n              learning_rate=0.02, max_depth=7, min_child_samples=20,\n              min_child_weight=0.001, min_split_gain=0.0, n_estimators=450,\n              n_jobs=-1, num_leaves=18, objective=None, random_state=None,\n              reg_alpha=0.0, reg_lambda=0.0, silent=True, subsample=1.0,\n              subsample_for_bin=200000)\n\n# Extreme Gradient Boosting Regressor\nxgbr = XGBRegressor(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n             colsample_bynode=1, colsample_bytree=1, gamma=0.1,\n             importance_type='gain', learning_rate=0.02, max_delta_step=0,\n             max_depth=3, min_child_weight=1, missing=None, n_estimators=750,\n             n_jobs=-1, nthread=None, objective='reg:linear', random_state=0,\n             reg_alpha=0, reg_lambda=1, scale_pos_weight=1, seed=None,\n             silent=None, subsample=0.65, verbosity=1)","e98c60ef":"# Estimator List\nestimator_list = {'Linear Regressor': linear_reg,\n              'Polynomial Regressor': poly_reg,\n              'Ridge Regressor': ridge_reg,\n              'Lasso Regressor': lasso_reg,\n              'ElasticNet Regressor': elasticnet_reg,\n              'Support Vector Regressor': svr,\n              'Decision Tree': decision_tree,\n              'Random Forest Regressor': rfr,\n              'Adaboost Regressor': adaboost_reg,\n              'Gradient Boosting Regressor': gbr,\n              'Light Gradient Boosting Regressor': lgbr,\n              'Extreme Gradient Boosting Regressor': xgbr}\n\n# function to store model rmse from cross_val_score\nscore = {}\ndef results():\n    for model in estimator_list.keys():\n        score[model] = cross_val(model= estimator_list[model], x= train_data)\n\nresults()","1056007a":"# RMSE for estimators based on cross_val_score\ncross_val_result = pd.Series(score).sort_values()\ncross_val_result.index.name = 'Estimator'\ncross_val_result","7d098006":"# Model Training\nfor name in estimator_list.keys():\n    print(name.upper(), ':')\n    estimator_list[name].fit(train_data, y)\n    print('\\t\\tDONE !\\n')","8e90e1ce":"# stacked model\nstacked_reg = StackingRegressor(estimators=[('Linear Regressor', linear_reg),\n                         ('Polynomial Regressor', poly_reg),\n                         ('Ridge Regressor', ridge_reg),\n                         ('Lasso Regressor', lasso_reg),\n                         ('ElasticNet Regressor', elasticnet_reg),\n                         ('Support Vector Regressor', svr),\n                         ('Decision Tree', decision_tree),\n                         ('Random Forest Regressor', rfr),\n                         ('Adaboost Regressor', adaboost_reg),\n                         ('Gradient Boosting Regressor', gbr),\n                         ('Light Gradient Boosting Regressor', lgbr),\n                         ('Extreme Gradient Boosting Regressor', xgbr)], \n                  cv=7, n_jobs=-1, verbose=1)","897fcb57":"# Grid Search for final estimator\nparams={'final_estimator':[estimator_list[name] for name in estimator_list.keys()]}\nsearch = GridSearchCV(estimator=stacked_reg, param_grid=params, \n                      scoring='neg_root_mean_squared_error', cv=7, verbose=1, n_jobs=-1)\n\nsearch.fit(train_data, y)","145ade38":"search.best_estimator_","d85d5223":"search.best_score_","fea8f2c3":"# Stacked Model Training\nstacked_reg = search.best_estimator_\nstacked_reg.fit(X=train_data, y=y)","1b2f7461":"# Prediction using Stacked Models\nfinal_pred = scaler.inverse_transform(stacked_reg.predict(test_data))","9b2b7cf0":"# Submission\nsubmission = pd.DataFrame()\n\nsubmission['Id'] = pd.read_csv('..\/input\/house-prices-advanced-regression-techniques\/sample_submission.csv')['Id']\nsubmission['SalePrice'] = final_pred\n\nsubmission.to_csv('House Price Prediction.csv',index=False)\npd.read_csv('House Price Prediction.csv', index_col=0).head(8)","160aacf7":"**Note** that one extra column in train data represent the Target Value i.e 'SalePrice'.","b8e8881b":"# Missing Features","e70f7b4b":"# I'm still working on it.","670a3e3f":"# Reading & Combining Data ","051a6f56":"So there are alot of missing values in both train and test data. We will be combining these both sets so that we do not need to perform same operation  like 'Filling in Null Values', 'Scaling', 'Dropping Features', 'LabelEncoding' seperately for both data sets.","c84bdf2c":"# Feature Selection (Categorical Features)\n- Goal :\n    1. **Missing Data** : There are few categorical features missing majority of its data. We will **eliminate features missing more than 20% of it's data**. \n    2. **Imbalanced Features** : Features such as 'Street' have imbalanced data i.e one of its attribute occurs more frequently than other. Features with more than **80% dominance** i.e  if a certain variable in an categorical feature occured more than 80% in entire dataset we **will drop these features**.","67af5a9b":"# **Visualizing Linearity in Data**\n- Goal :\n    1. **Numeric Features** : Visualizing linearity of 14 Numeric features with SalePrice.\n    2. **Categorical Features** : Visualizing linearity of 18 Categorical features with SalePrice.","087eec6d":"- **Thank You** for reading **:)**\n- **Suggestions** are always welcomed.\n- An ***Upvote*** would be appreciated if u found this notebook helpful.","2d598f72":"# Handling Missing Features\n- Goal :\n    1. **Numeric Features** : Filling missing data with mean values.\n    2. **Categorical Features** : Filling missing data with mode values.","dc642689":"# Prediction & Submission","e22ea6b5":"# Feature Selection (Numeric Features)\n\n- Goal : \n    - 1. **High Priority Features** : Features like 'OverallQual, YearBuilt, MasVnrArea, TotalBsmtSF, GrLivArea, FullBath, Fireplaces, GarageArea' have high correlation with Target 'SalePrice'. features_corr > 50%\n    - 2. **Multi-collinearity Features** : Features like 'YearRemodAdd, LotArea, 1stFlrSF, TotalRmsAbvGrd, GarageYrBlt, GarageCars' have high collinearity with some other features as well, thus representing same data.So will prefer dropping these features.\n    - 3. **Zero-corr Features** : Features like 'MSSubClass, OverallCond, BsmtFinSF2, BsmtUnfSF, BsmtFullBath, LowQualFinSF, BsmtHalfBath, BedroomAbvGr, KitchenAbvGr, EnclosedPorch, 3SsnPorch, ScreenPorch, PoolArea, MiscVal, MoSold, YrSold' have less or zero correlation with SalePrice.Will drop these features as well\n    - 4. Features like 'BsmtFinSF1', 'LotFrontage', 'WoodDeckSF', 2ndFlrSF, HalfBath, OpenPorchSF' have 30-35% correlation with 'SalePrice'.will explore these features further.","e5086c5b":"# **Handling Outliers**","437082bf":"# **Machine Learning**\n\n- In this version we have reduced features from **64 features** to **32 features**\n    - 14 Numeric Features &\n    - 18 Categorical Features\n\n\n\n- There will be some processing that will be done before creating models.\n    - a) Seperating 'train_data' (excluding target variable) , 'test_data' & target variable 'y' from data. train_data will be used for               creating model and evaluation.\n    - b) Scaling 'y' using StandardScaler.\n    \n- ***Algorithms*** used :\n    - 01. Linear Regression\n    - 02. Polynomial Regression\n    - 03. Ridge Regression\n    - 04. Lasso Regression\n    - 05. ElasticNetCV Regression\n    - 06. Support Vector Regressor\n    - 07. Random Forest Regressor\n    - 08. AdaBoost Regressor\n    - 09. GradientBoosting Regressor\n    - 10. Light GradientBoosting Regressor\n    - 11. XGBoost Regressor\n    - 12. Stacked Regression","4fc05cc6":"- **Conclusion** :\n    1. **Numeric Features** : We have eliminated features which were less important or showed multi-collinearity with other features. We are now left with only top priority **14 Numeric Features**.\n    2. **Categorical Features** : In categorical features we have dropped imbalanced features and also features missing huge chuck of it's data. Thus we are left with balanced features. We are now left with **18 Categorical Features**.","ef584bd2":"#  **More Feature Engineering**\n- Goals : \n    1. **Label Encoding** : Considering categorical features, there is still some linearity missing in data. we will combine variables showing same relationship into single numeric value using label encoding.\n    2. **Outliers** : Finding and dropping outliers present in numeric features.","d10d8edc":"# **Feature Engineering** at basic level\n1. **Basic feature selection** \n2. **Handling Missing Values**","e608fa8c":"# Stacked Regressor","b082b643":"# Label Encoding"}}