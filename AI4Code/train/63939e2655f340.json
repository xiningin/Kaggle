{"cell_type":{"83a3cf3f":"code","4be1c342":"code","75de54e3":"code","2b8ce527":"code","694a2b04":"code","5d77c856":"code","b320b06a":"code","59eb3b4e":"code","37e430d7":"code","49b600d0":"markdown","ec768678":"markdown","d622aab5":"markdown","f5eab361":"markdown","24e5d544":"markdown","da99d366":"markdown","5c967854":"markdown"},"source":{"83a3cf3f":"%matplotlib notebook\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport time\n# https:\/\/gist.github.com\/greydanus\/f6eee59eaf1d90fcb3b534a25362cea4\n# https:\/\/stackoverflow.com\/a\/14434334\n# this function is used to update the plots for each epoch and error\ndef plt_dynamic(fig,x, vy, ty, ax, colors=['b']):\n    \n    ax.plot(x, vy, 'b', label=\"Validation Loss\")\n    ax.plot(x, ty, 'r', label=\"Train Loss\")\n    plt.legend()\n    plt.grid()\n    fig.canvas.draw()","4be1c342":"def printLossPlot(modelName):\n    score = modelName.evaluate(x_test, y_test, verbose=0) \n    print('Test score:', score[0]) \n    print('Test accuracy:', score[1])\n#     fig = plt.figure()\n    fig,ax = plt.subplots(1,1)\n    ax.set_xlabel('epoch') ; ax.set_ylabel('Categorical Crossentropy Loss')\n\n    # list of epoch numbers\n    x = list(range(1,epochs+1))\n    vy = history.history['val_loss']\n    ty = history.history['loss']\n    plt_dynamic(fig,x, vy, ty, ax)","75de54e3":"# Credits: https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/mnist_cnn.py\n\n\nfrom __future__ import print_function\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras import backend as K\nfrom keras.layers.normalization import BatchNormalization\n\nbatch_size = 256\nnum_classes = 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train \/= 255\nx_test \/= 255\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nmodel1 = Sequential()\nmodel1.add(Conv2D(32, kernel_size=(7, 7),activation='relu',input_shape=input_shape))\nmodel1.add(Conv2D(64, (7, 7), activation='relu'))\n\nmodel1.add(Conv2D(64, (7, 7), activation='relu'))\n\nmodel1.add(MaxPooling2D(pool_size=(2, 2)))\nmodel1.add(BatchNormalization())\nmodel1.add(Dropout(0.25))\nmodel1.add(Flatten())\n\nmodel1.add(Dense(128, activation='relu'))\nmodel1.add(BatchNormalization())\nmodel1.add(Dropout(0.5))\n\nmodel1.add(Dense(num_classes, activation='softmax'))\n\nmodel1.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\n\nhistory=model1.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))\nscore1 = model1.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score1[0])\nprint('Test accuracy:', score1[1])","2b8ce527":"printLossPlot(model1)","694a2b04":"# Credits: https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/mnist_cnn.py\n\n\nfrom __future__ import print_function\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras import backend as K\nfrom keras.layers.normalization import BatchNormalization\n\nbatch_size = 128\nnum_classes = 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train \/= 255\nx_test \/= 255\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nmodel2 = Sequential()\nmodel2.add(Conv2D(32, kernel_size=(7, 7),activation='relu',input_shape=input_shape))\nmodel2.add(Conv2D(64, (5, 5), activation='relu'))\n\nmodel2.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel2.add(Conv2D(64, (3, 3), activation='relu'))\nmodel2.add(MaxPooling2D(pool_size=(2, 2)))\n\nmodel2.add(BatchNormalization())\nmodel2.add(Dropout(0.25))\nmodel2.add(Flatten())\nmodel2.add(Dense(128, activation='relu'))\nmodel2.add(BatchNormalization())\nmodel2.add(Dropout(0.5))\nmodel2.add(Dense(num_classes, activation='softmax'))\n\nmodel2.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\n\nhistory=model2.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))\nscore2 = model2.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score2[0])\nprint('Test accuracy:', score2[1])","5d77c856":"printLossPlot(model2)","b320b06a":"# Credits: https:\/\/github.com\/keras-team\/keras\/blob\/master\/examples\/mnist_cnn.py\n\n\nfrom __future__ import print_function\nimport keras\nfrom keras.datasets import mnist\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras import backend as K\nfrom keras.layers.normalization import BatchNormalization\n\nbatch_size = 64\nnum_classes = 10\nepochs = 12\n\n# input image dimensions\nimg_rows, img_cols = 28, 28\n\n# the data, split between train and test sets\n(x_train, y_train), (x_test, y_test) = mnist.load_data()\n\nif K.image_data_format() == 'channels_first':\n    x_train = x_train.reshape(x_train.shape[0], 1, img_rows, img_cols)\n    x_test = x_test.reshape(x_test.shape[0], 1, img_rows, img_cols)\n    input_shape = (1, img_rows, img_cols)\nelse:\n    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n    input_shape = (img_rows, img_cols, 1)\n\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\nx_train \/= 255\nx_test \/= 255\nprint('x_train shape:', x_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\n# convert class vectors to binary class matrices\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\nmodel3 = Sequential()\nmodel3.add(Conv2D(32, kernel_size=(5, 5),\n                 activation='relu',\n                 input_shape=input_shape))\n\n\nmodel3.add(Conv2D(64, (3, 3), activation='relu'))\nmodel3.add(MaxPooling2D(pool_size=(2, 2)))\nmodel3.add(BatchNormalization())\nmodel3.add(Dropout(0.25))\n\nmodel3.add(Conv2D(32, (3, 3), activation='relu'))\nmodel3.add(MaxPooling2D(pool_size=(2, 2)))\nmodel3.add(BatchNormalization())\nmodel3.add(Dropout(0.25))\n\nmodel3.add(Flatten())\nmodel3.add(Dense(128, activation='relu'))\nmodel3.add(BatchNormalization())\nmodel3.add(Dropout(0.5))\n\nmodel3.add(Dense(64, activation='relu'))\nmodel3.add(BatchNormalization())\nmodel3.add(Dropout(0.5))\n\nmodel3.add(Dense(32, activation='relu'))\nmodel3.add(BatchNormalization())\nmodel3.add(Dropout(0.2))\n\n\nmodel3.add(Dense(num_classes, activation='softmax'))\n\nmodel3.compile(loss=keras.losses.categorical_crossentropy,\n              optimizer=keras.optimizers.Adadelta(),\n              metrics=['accuracy'])\n\nhistory=model3.fit(x_train, y_train,\n          batch_size=batch_size,\n          epochs=epochs,\n          verbose=1,\n          validation_data=(x_test, y_test))\nscore3 = model3.evaluate(x_test, y_test, verbose=0)\nprint('Test loss:', score3[0])\nprint('Test accuracy:', score3[1])","59eb3b4e":"printLossPlot(model3)","37e430d7":"from prettytable import PrettyTable\n    \nx = PrettyTable()\n\nx.field_names = [\"Model Name\", \"Test Score\", \"Test Accuracy\"]\n\nx.add_row([\"Model1\", score1[0], score1[1]])\nx.add_row([\"Model2\", score2[0], score2[1]])\nx.add_row([\"Model3\", score3[0],score3[1]])\n\n\nprint(x)","49b600d0":"# Model 1\n","ec768678":"# Model 2\n","d622aab5":"This implementation includes\n1. 3 ->7x7 Kernels\n1. 1 Extra ConvNet Layer\n1. Batch Normalization","f5eab361":"This implementation includes\n1. 3 different kernel size 5x5, 3x3 and 3x3\n1. Different Batch Size\n1. 1 Extra ConvNet Layer\n1. 1 Extra Max Pooling Layer\n1. 2 Extra Dense Layer with different Dropouts\n1. Batch Normalization","24e5d544":"# Model 3","da99d366":"# 3 Distinct CNN Architectures","5c967854":"This implementation includes\n1. 3 different kernel size 7x7, 5x5 and 3x3\n1. 1 Extra ConvNet Layer\n1. 1 Extra MaxPooling Layer\n1. Batch Normalization"}}