{"cell_type":{"e07e6fc3":"code","63856882":"code","9e4d4725":"code","9620e403":"code","890e9858":"code","82f980a7":"code","d75d1a9a":"code","e8996dfa":"code","9f2304c1":"code","1fd52b63":"code","59292871":"code","e7bd0ed9":"code","9675f748":"code","e746e406":"code","7319dd4b":"code","50019618":"code","47a5fc27":"code","11668cb9":"code","40b6670d":"code","2e39cf21":"code","38b1b6ba":"code","bb8fb870":"code","ed595c80":"code","99ad49fc":"code","b428748b":"code","863c63df":"code","c831a75e":"code","a6c1e147":"code","22e59059":"code","9bf9230b":"code","053fc127":"code","05dbf4ca":"code","3df29f35":"code","4b8ae6ea":"code","827e3c84":"code","28345a00":"code","3b9007d5":"code","6ca4d29a":"code","94a7a6d9":"markdown","4b8ecbe4":"markdown","a1d441af":"markdown","52d3117c":"markdown"},"source":{"e07e6fc3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","63856882":"# Importing Libraries\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport plotly.express as px\nimport warnings\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud\nwarnings.filterwarnings('ignore')\nimport re","9e4d4725":"# load the data\ntweet_path = '..\/input\/farmers-protest-tweets-dataset-csv\/tweets.csv'\nuser_path = '..\/input\/farmers-protest-tweets-dataset-csv\/users.csv'\n\ntweet_data = pd.read_csv(tweet_path)\nuser_data = pd.read_csv(user_path)","9620e403":"tweet_data.head()","890e9858":"user_data.head()","82f980a7":"# Perform an inner join on both the tables on userId\nfinal_data = pd.merge(tweet_data, user_data, how='inner', on='userId')","d75d1a9a":"# Print the shape of the data\nrow_count = final_data.shape[0]\ncol_count = final_data.shape[1]\n\nprint(\"Number of rows in the data {}\".format(row_count))\nprint(\"Number of columns in the data {}\".format(col_count))","e8996dfa":"final_data.head()","9f2304c1":"# Print the percentage null values in each column\ndef perc_null(data):\n    \n    # number of rows\n    row_count = data.shape[0]\n    \n    # list to store the columns to droop\n    columns_to_drop = []\n    \n    # iterate over each column to find the % of NaNs each column\n    for col in data.columns:\n        \n        null_count = data[col].isnull().sum()\n        \n        # Check if the null count > 0\n        if null_count > 0:\n            \n            # Calculate the % null count\n            perc_null = float(null_count)*100\/row_count\n            print(\"NaN values in the {} column is {:.2f} %\".format(col, perc_null))\n            \n            if perc_null > 40:\n                columns_to_drop.append(col)\n                \n    return columns_to_drop\n\n# call the function\ncolumns_to_drop = perc_null(final_data)","1fd52b63":"# drop the columns with more than 40% null values\nfinal_data.drop(columns_to_drop, axis=1, inplace=True)","59292871":"# checking the loss of information if we chose to drop the null rows\ninitial_row_count = final_data.shape[0]\nfinal_row_count = final_data.dropna(axis=0).shape[0]\n\nperc_data_loss = float((initial_row_count-final_row_count)\/initial_row_count)*100\nprint(\"Percentage loss of data after dropping the NaN rows {:.3f}%\".format(perc_data_loss))","e7bd0ed9":"perc_null(final_data)","9675f748":"# URL fields are not important for our analysis, hence, we can exclude them\nfinal_data.drop(['descriptionUrls', 'profileImageUrl', 'profileBannerUrl'], axis=1, inplace=True)","e746e406":"# impute an 'unknown' category inplace of Null Values for location column, and No Description in place of Null Values for the rawDescription column\nfinal_data['location'] = final_data['location'].fillna('Unknown')\nfinal_data['rawDescription'] = final_data['rawDescription'].fillna('No Description')","7319dd4b":"# check null percentage again!\nperc_null(final_data)","50019618":"# check % information loss again if we chose to drop the null rows\ninitial_row_count = final_data.shape[0]\nfinal_row_count = final_data.dropna(axis=0).shape[0]\n\nperc_data_loss = float((initial_row_count-final_row_count)\/initial_row_count)*100\nprint(\"Percentage loss of data after dropping the NaN rows {:.3f}%\".format(perc_data_loss))","47a5fc27":"# drop null rows\nfinal_data.dropna(axis=0, inplace=True)","11668cb9":"final_data.head()","40b6670d":"# visualise the frequency of tweets over time\nfinal_data['date'] = pd.to_datetime(final_data['date'])\n\n# sort the data by date\nfinal_data.sort_values(by='date', inplace=True)","2e39cf21":"final_data.info()","38b1b6ba":"# plotting the frequency of tweets over time\n\n# 1. Extract Date part from the date column\nfinal_data['datetime'] = final_data['date']\nfinal_data['date'] = final_data['datetime'].dt.date","bb8fb870":"final_data['date']  = pd.to_datetime(final_data['date'])\n\n# plot the data\nplt.style.use('seaborn')\nplt.figure(figsize=(12,8))\nfinal_data.groupby('date').count()['tweetId'].plot()\nplt.xlabel(\"Date\")\nplt.ylabel(\"Tweet Count\")\nplt.title(\"Tweet Count Across Time\")\nplt.show()","ed595c80":"# extract the time from the datetitme\nfinal_data['time'] = final_data['datetime'].dt.time\nfinal_data['hour'] = final_data['datetime'].dt.hour\n\n# plot the time\nhour = final_data.groupby('hour').count()['tweetId'].index\ntweet_count = final_data.groupby('hour').count()['tweetId'].values\n\nplt.bar(x=hour, height=tweet_count)\nplt.xticks(ticks=hour)\nplt.xlabel('hour')\nplt.ylabel('Tweet Count')\nplt.title(\"Tweet Frequency Across Hour\")\nplt.show()","99ad49fc":"# Finding people with max tweet counts, extract the top 10 userIds\ntop_10_user_ids = final_data.groupby('userId').count()[['username', 'displayname']].sort_values(by='username', ascending = False).iloc[ : 10].index\n\n# Extract the data only for the top 10 tweeters\ntop_10_tweeter_data = final_data[final_data['userId'].isin(top_10_user_ids)]","b428748b":"# plot the countplot for each top twitter user\nsns.countplot(data=top_10_tweeter_data, x='userId')\nplt.xticks(rotation=-90)\nplt.show()","863c63df":"# Extract the top 20 most liked tweets\nfinal_data.sort_values(by='likeCount', ascending = False).iloc[ : 20][['date', 'renderedContent', 'username', 'displayname']]","c831a75e":"figure, (ax1, ax2) = plt.subplots(2, 2, figsize=(20, 20))\n\n# plot top 20 most liked tweets\ndata1 = final_data.sort_values(by='likeCount', ascending = False).iloc[ : 20][['date', 'renderedContent', 'username', 'displayname', 'likeCount']].sort_values(by='likeCount', ascending=True)\nax1[0].barh(y=data1['displayname'], width = data1['likeCount'])\nax1[0].set_xlabel(\"Tweet Like Count\")\nax1[0].set_ylabel(\"User Display Name\")\n\n# plot top 20 most Replied tweets\ndata1 = final_data.sort_values(by='replyCount', ascending = False).iloc[ : 20][['date', 'renderedContent', 'username', 'displayname', 'replyCount']].sort_values(by='replyCount', ascending=True)\nax1[1].barh(y=data1['displayname'], width = data1['replyCount'])\nax1[1].set_xlabel(\"Tweet reply Count\")\n#ax1[1].set_ylabel(\"User Display Name\")\n\n\n# plot top 20 most retweeted tweets\ndata1 = final_data.sort_values(by='retweetCount', ascending = False).iloc[ : 20][['date', 'renderedContent', 'username', 'displayname', 'retweetCount']].sort_values(by='retweetCount', ascending=True)\nax2[0].barh(y=data1['displayname'], width = data1['retweetCount'])\nax2[0].set_xlabel(\"Tweet ReTweet Count\")\nax2[0].set_ylabel(\"User Display Name\")\n\n# plot top 20 most retweeted tweets\ndata1 = final_data.sort_values(by='quoteCount', ascending = False).iloc[ : 20][['date', 'renderedContent', 'username', 'displayname', 'quoteCount']].sort_values(by='quoteCount', ascending=True)\nax2[1].barh(y=data1['displayname'], width = data1['quoteCount'])\nax2[1].set_xlabel(\"Tweet Quote Count\")\n#ax2[1].set_ylabel(\"User Display Name\")\n\nfigure.show()","a6c1e147":"# Top 20 Twitter Users with most followers, who are tweeting in context of the farmer protest\n\ndata1 = final_data.sort_values(by='followersCount', ascending = False).iloc[ : 20][['date', 'renderedContent', 'username', 'displayname', 'followersCount']].sort_values(by='followersCount', ascending=True)\nplt.barh(y=data1['displayname'], width = data1['followersCount'])\nplt.xlabel(\"Followers Count\")\nplt.ylabel(\"User Display Name\")\nplt.show()","22e59059":"def preprocess_source(x):\n    p = re.compile(r'<.*?>')\n    return p.sub('', x)\n\nfinal_data['source_app'] = final_data['source'].apply(lambda x : preprocess_source(x))\n","9bf9230b":"# Store labels\nsource_labels = final_data['source_app'].value_counts().index\ntop_5_sources = list(final_data['source_app'].value_counts().head(5).index)\n\ndef preprocess_source_app(x):\n    \n    if x not in top_5_sources:\n        x = 'Others'\n    return x\n\nfinal_data['source_app'] = final_data['source_app'].apply(lambda x : preprocess_source_app(x))","053fc127":"top_5_sources.append('Others')","05dbf4ca":"figure, ax = plt.subplots(1, 2, figsize=(20, 8))\n\n# Verified vs non verified tweets\nax[0].pie(final_data['verified'].value_counts(), labels=['True', 'False'], autopct='%1.1f%%', explode = (0, 0.2))\nax[0].set_title(\"Verified User Profiles\")\n\n# Source of tweets\nax[1].pie(final_data['source_app'].value_counts(), labels= top_5_sources, autopct='%1.1f%%', explode = (0, 0, 0, 0.1, 0.2, 0.3))\nax[1].set_title(\"Source of Tweets\")\n\nfigure.show()","3df29f35":"pd.set_option('display.max_columns', None)\nfinal_data.head()","4b8ae6ea":"# Preprocessing the renderedContent and rawDescription columns\ndef preprocess_text_data(x):\n    \n    x = x.replace('https', '').replace('co', '').replace('\\n', '')\n    emoji_pattern = re.compile(\"[^A-Za-z0-9]+\", flags=re.UNICODE)\n    \n    return emoji_pattern.sub(' ', x)\n    \n    \nfinal_data['rawDescription'] = final_data['rawDescription'].apply(lambda x : preprocess_text_data(x))\nfinal_data['renderedContent'] = final_data['renderedContent'].apply(lambda x : preprocess_text_data(x))","827e3c84":"sw = stopwords.words('english')\ndef remove_stopwords(x):\n    \n    x = x.split()\n    final_reviews = [words for words in x if words not in sw]\n    \n    return \" \".join(final_reviews)","28345a00":"final_data['rawDescription'] = final_data['rawDescription'].apply(lambda x : remove_stopwords(x))\nfinal_data['renderedContent'] = final_data['renderedContent'].apply(lambda x : remove_stopwords(x))","3b9007d5":"# Plot WordClouds for rawDescription and renderedContent\n\nrawDes_text = \" \".join(list(final_data['rawDescription'].value_counts().index))\nwc = WordCloud(background_color=\"white\")\nwordcloud = wc.generate(rawDes_text)\n\nplt.figure(figsize=(12,8))\nplt.imshow(wordcloud)\nplt.title(\"Twitter User Description\")\nplt.axis('off')\nplt.show()","6ca4d29a":"tweet_text = \" \".join(list(final_data['renderedContent'].value_counts().index))\nwc = WordCloud(background_color=\"white\")\nwordcloud = wc.generate(tweet_text)\n\nplt.figure(figsize=(12,8))\nplt.imshow(wordcloud)\nplt.title(\"Tweet Wordcloud\")\nplt.axis('off')\nplt.show()","94a7a6d9":"# Tweet Data Analysis","4b8ecbe4":"**Seems dropping the rows will lead to significant information loss, so it's a bad idea to drop the rows.**","a1d441af":"**Number of tweets increase sharply at around 12 pm in the afternoon.**","52d3117c":"**Not much information loss will be there, hence, it is safe to drop Null rows.**"}}