{"cell_type":{"61ae14d7":"code","f52e828c":"code","2a73bea6":"code","61389e46":"code","863fbdb9":"code","f6d38e31":"code","68c65982":"code","07b99c4d":"code","4ed87fd7":"code","3eaf1031":"code","002d9c4f":"code","ca99a92f":"code","35c095f8":"code","9de6f681":"code","572b0137":"code","8662515f":"code","3aec7065":"code","9f85ef31":"code","81ff8f34":"code","d27a8920":"code","59cf3b3a":"code","a8e90e57":"code","91f6d579":"code","4c1a9a72":"code","6dd31e01":"code","4d7aeaf9":"code","23b7936e":"code","5c9da048":"code","dac024a7":"code","ba0d8513":"code","a3cf0a39":"code","664f33d8":"code","96ce8aa8":"code","d0f1d02b":"code","27ee119b":"code","bdd2024f":"code","9bcd8a97":"code","0aa5d1ac":"code","922678c1":"code","c691e50d":"code","89e37c47":"code","468ddb46":"code","bd7f40a8":"code","5ee207ba":"code","5b463a96":"code","a8957e47":"code","7da27c13":"code","b7b586ef":"code","c52e52a8":"code","e78d4b44":"code","662fd0f5":"code","66a621b2":"code","cee15dff":"code","3a15da37":"code","9541c227":"code","18982bf8":"code","73938c99":"code","99757711":"code","b4e4b457":"code","083e2327":"code","03000b58":"code","8503c217":"code","9f629b08":"code","8f939159":"code","6ce53509":"code","2eeb3d19":"code","15f3c7d5":"markdown","bdda55d2":"markdown","736268bb":"markdown","6f22da1a":"markdown","994c903d":"markdown","b7fef99b":"markdown","8828f75e":"markdown","e6da4c55":"markdown","16328399":"markdown","6ff10075":"markdown","97543cc7":"markdown","197d0783":"markdown","d3875ed1":"markdown","58fa8de7":"markdown","d18931cc":"markdown","ffab9b4b":"markdown","bd60cbd8":"markdown","19fc3725":"markdown","e24d0014":"markdown","71669f02":"markdown","653a7307":"markdown","3c6fd06b":"markdown","90835d2f":"markdown","bd699fda":"markdown","47295d1f":"markdown","43cd20e8":"markdown","deac480b":"markdown","e10dd099":"markdown","10bde232":"markdown","b1150e53":"markdown","bd3dfe01":"markdown","2ce707d6":"markdown"},"source":{"61ae14d7":"# Importing all required libraries at once\nimport pandas as pd\n# import tweepy\nimport os\nimport requests\nimport matplotlib.pyplot as plt\nimport glob\nimport numpy as np\nimport json\nimport time\nimport datetime\nfrom scipy import stats\nimport statistics\nimport os\n%matplotlib inline\n\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","f52e828c":"# import CSV for WeRateDogs Twitter archive\ndfTwitter_archive = pd.read_csv('\/kaggle\/input\/twitter\/twitter-archive-enhanced.csv')","2a73bea6":"# we will skip the programmatic download here\n'''\n# programmatic download through Requests library\nurl = 'https:\/\/d17h27t6h515a5.cloudfront.net\/topher\/2017\/August\/599fd2ad_image-predictions\/image-predictions.tsv'\ndirectory = '\/Users\/nicolas\/Google Drive\/udacity\/project 3'\nresponse = requests.get(url)\n\nwith open(os.path.join(directory, url.split('\/')[-1]), mode='wb') as file:\n    file.write(response.content)\n    \n'''\n\ndfImage_predictions = pd.read_csv('\/kaggle\/input\/twitter\/image-predictions.tsv', sep='\\t')","61389e46":"# The original task included a programmatic download of data through the Twitter API. \n# Since I don't want to include the credentials here, I have commented it out and simply uploaded the final JSON file.\n'''\n# define credentials to call Twitter API\nconsumer_key = 'SECRET'\nconsumer_secret = 'SECRET'\naccess_token = 'SECRET'\naccess_secret = 'SECRET'\n\nauth = tweepy.OAuthHandler(consumer_key, consumer_secret)\nauth.set_access_token(access_token, access_secret)\n\napi = tweepy.API(auth)\n\n\ntweetIDs = dfTwitter_archive.tweet_id.to_list()\ntweetErrors = []\ntweetList = []\n\n# loop for downloading tweet data through Twitter API\nfor tweet_id in tweetIDs:\n    try:\n        tweet = api.get_status(tweet_id, tweet_mode='extended')\n        tweetList.append(tweet._json)\n    except tweepy.TweepError as er:\n        errors = {'ID':tweet_id, 'Error':er}\n        tweetErrors.append(errors)\n        pass\n\n# converting dictionary into DataFrame\ndfErrors = pd.DataFrame(tweetErrors, columns = ['ID','Error'])\n\n# writing the data into the 'tweet_json.txt' file\nfile = 'tweet_json.txt'\n\nwith open(file, 'w') as outfile:\n    for tweet in tweetList:\n        json.dump(tweet, outfile)\n        outfile.write('\\n')'''","863fbdb9":"# reading in the json file and importing it into a proper DataFrame\njsonTwitter = []\n\nfile = '\/kaggle\/input\/twitter\/tweet_json.txt'\n\n# opening the downloaded file\nwith open(file, 'r') as json_file:\n    # running a loop for each line in the json file\n    ln = json_file.readline()\n    while ln:\n        data = json.loads(ln)\n        t_id = data['id']\n        f_count = data['favorite_count']\n        rt_count = data['retweet_count']\n        #storing extracted information in dictionary\n        json_data = {'id':t_id, 'favorites':f_count, 'retweets':rt_count}\n        jsonTwitter.append(json_data)\n        #reading new line\n        ln = json_file.readline()\n\n# converting dictionary into DataFrame\ndfTwitter_api = pd.DataFrame(jsonTwitter, columns=['id', 'favorites', 'retweets'])\n\ndfTwitter_api.head()","f6d38e31":"# starting with visual assessment first to get familiar with the DataFrame\ndfTwitter_archive.head()","68c65982":"dfTwitter_archive.tail()","07b99c4d":"dfTwitter_archive.info()","4ed87fd7":"dfTwitter_archive.describe()","3eaf1031":"# checking for any duplicates for 'tweet_id' in the DataFrame\ndfTwitter_archive.tweet_id.nunique() == len(dfTwitter_archive)","002d9c4f":"# how many different sources do we have\ndfTwitter_archive['source'].unique()","ca99a92f":"# how many different values do we have for the rating_numerator\ndfTwitter_archive.rating_numerator.unique()","35c095f8":"# looking up all 'unrealistic' values from 15 on\ndfTwitter_archive.query('rating_numerator > 15').sort_values(by='rating_numerator', ascending=False)[['text', 'rating_numerator', 'rating_denominator']]","9de6f681":"# how many different values do we have for the rating_denominator\ndfTwitter_archive.rating_denominator.unique()","572b0137":"# looking up all 'unrealistic' values from 10 on\ndfTwitter_archive.query('rating_denominator > 10').sort_values(by='rating_denominator', ascending=False)[['text', 'rating_numerator', 'rating_denominator']]","8662515f":"# searching for valid ratings from the original WeRateDogs Twitter profile\nlen(dfTwitter_archive[dfTwitter_archive['expanded_urls'].isnull()])","3aec7065":"# if it's a real tweet, 'in_reply_to_status_id' and 'retweeted_status_id' need to be empty as well\ndfTwitter_archive[dfTwitter_archive['expanded_urls'].isnull() & dfTwitter_archive['in_reply_to_status_id'].isnull() & dfTwitter_archive['retweeted_status_id'].isnull()]","9f85ef31":"# check if consistency existings within the stages column\ndfTwitter_archive.doggo.unique(), \\\ndfTwitter_archive.floofer.unique(), \\\ndfTwitter_archive.pupper.unique(), \\\ndfTwitter_archive.puppo.unique()","81ff8f34":"# starting with visual assessment first to get familiar with the DataFrame\ndfImage_predictions.head()","d27a8920":"dfImage_predictions.tail()","59cf3b3a":"dfImage_predictions.info()","a8e90e57":"dfImage_predictions.describe()","91f6d579":"# checking for duplicate entries via 'tweet_id'\ndfImage_predictions.tweet_id.nunique() == len(dfImage_predictions)","4c1a9a72":"# are there any tweets where the algorithm couldn't make any prediction\ndfImage_predictions.query('(p1_dog == False) & (p2_dog == False) & (p3_dog == False)')","6dd31e01":"dfTwitter_api.head()","4d7aeaf9":"dfTwitter_api.tail()","23b7936e":"dfTwitter_api.info()","5c9da048":"dfTwitter_api.describe()","dac024a7":"# checking for duplicates in 'id'\ndfTwitter_api.id.nunique() == len(dfTwitter_api.id)","ba0d8513":"# creating copy of original DataFrame before applying any changes\ntArchive_clean = dfTwitter_archive.copy()","a3cf0a39":"# 1. Harmonize names in `name` and replace non-sense naming with `none` for consistency\nnames = tArchive_clean['name'].str.contains('^[a-z]', regex = True)\ntArchive_clean.loc[names, 'name'] = 'none'\n\n# testing for new 'none' values\nlen(tArchive_clean.query('name == \"none\"'))","664f33d8":"# 2. Drop all entries with missing URL that are **not** valid tweets\ndrop = tArchive_clean['retweeted_status_id'].notnull()\ntArchive_clean.drop(tArchive_clean[drop].index, inplace=True)\ntArchive_clean.reset_index(inplace=True, drop=True)\n\n# testing if all values are gone\nlen(tArchive_clean[drop]) == 0","96ce8aa8":"# 3. Drop all entries with missing URL that are **not** valid tweets\ndrop = (tArchive_clean['expanded_urls'].isnull() & tArchive_clean['in_reply_to_status_id'].notnull()) | (tArchive_clean['expanded_urls'].isnull() & tArchive_clean['retweeted_status_id'].notnull())\n\ntArchive_clean.drop(tArchive_clean[drop].index, inplace=True)\ntArchive_clean.reset_index(inplace=True, drop=True)\n\n# testing if all values are gone\nlen(tArchive_clean[drop]) == 0","d0f1d02b":"# 4. Drop all entries with `rating_numerator` > 15\ndrop = tArchive_clean['rating_numerator'] > 15\n\ntArchive_clean.drop(tArchive_clean[drop].index, inplace=True)\ntArchive_clean.reset_index(inplace=True, drop=True)\n\n# testing if all values are gone\nlen(tArchive_clean[drop]) == 0","27ee119b":"# 5. Set all entries with `rating_denominator` != 10 to `rating_denominator` = 10\nm = tArchive_clean['rating_denominator'] != 10\ntArchive_clean.loc[m, 'rating_denominator'] = 10\n\n# testing if all values are gone\nlen(tArchive_clean[m]) == 0","bdd2024f":"# 6. Combine all four columns for dog stages into a single column stage \nstages = ['doggo', 'floofer', 'pupper', 'puppo']\n\n# run loop to convert value from single column to centralized column 'stage'\nfor c in stages:\n    tArchive_clean.loc[tArchive_clean[c] == c, 'stage'] = c\n    \n# filters for testing\nfilter1 = [(tArchive_clean[stages[0]] == stages[0]),\n           (tArchive_clean[stages[1]] == stages[1]),\n           (tArchive_clean[stages[2]] == stages[2]),\n           (tArchive_clean[stages[3]] == stages[3])]\n\nfilter2 = [(tArchive_clean['stage'] == stages[0]),\n            (tArchive_clean['stage'] == stages[1]),\n            (tArchive_clean['stage'] == stages[2]),\n            (tArchive_clean['stage'] == stages[3])]\n\n# looping the test for all possible combinations\n\ndef test(df):\n    options = 0\n    for f in filter1:\n        print('Test', options,':' , len(df[filter1[options]]) == len(df[filter2[options]]))\n        options += 1\n\ntest(tArchive_clean)","9bcd8a97":"# first test is 'False', meaning we have less\/more 'doggo' values than before\nlen(tArchive_clean[filter1[0]]), len(tArchive_clean[filter2[0]])","0aa5d1ac":"# need to identify the difference in rows\nerrors = (tArchive_clean[stages[0]] == stages[0]) & (tArchive_clean['stage'] != stages[0])\nlen(tArchive_clean[errors])","922678c1":"# we found 14 values where we had two dog stages defined; for consistency, we will drop these\ntArchive_clean.drop(tArchive_clean[errors].index, inplace=True)\ntArchive_clean.reset_index(inplace=True, drop=True)","c691e50d":"# running the test on the DataFrame again\ntest(tArchive_clean)","89e37c47":"# drop redudant dog stage columns\ntArchive_clean.drop(columns=stages, inplace=True)\ntArchive_clean.reset_index(inplace=True, drop=True)","468ddb46":"# 7. Convert the HTML-string from source into categories\niphone = tArchive_clean['source'].str.contains('Twitter for iPhone')\ndesktop = tArchive_clean['source'].str.contains('Twitter Web Client')\ntweet_deck = tArchive_clean['source'].str.contains('TweetDeck')\nvine = tArchive_clean['source'].str.contains('Vine - Make a Scene')\n\ntArchive_clean.loc[iphone, 'source'] = 'iphone'\ntArchive_clean.loc[desktop, 'source'] = 'desktop'\ntArchive_clean.loc[tweet_deck, 'source'] = 'tweet_deck'\ntArchive_clean.loc[vine, 'source'] = 'vine'\n\n# testing for remaining unique values\ntArchive_clean['source'].unique()","bd7f40a8":"# 8. Convert tweet_id into string format; rename to ID\ntArchive_clean['tweet_id'] = tArchive_clean['tweet_id'].astype(str)\ntArchive_clean.rename(columns={'tweet_id':'ID'}, inplace=True)\n\n# testing if datatype is now object\ntArchive_clean['ID'].dtype == object","5ee207ba":"# 9. Convert timestamp into proper timestamp format\ntArchive_clean['timestamp'] = pd.to_datetime(tArchive_clean['timestamp'])","5b463a96":"# 10. Drop columns in_reply_to_status_id, in_reply_to_user_id, retweeted_status_id, retweeted_status_user_id, retweeted_status_timestamp\n\ntArchive_clean.drop(columns=['in_reply_to_status_id', 'in_reply_to_user_id', 'retweeted_status_id', 'retweeted_status_user_id', 'retweeted_status_timestamp'], inplace=True)\n\n# testing if all columns are gone\ntArchive_clean.info()","a8957e47":"# creating copy of original DataFrame before applying any changes\niPredictions_clean = dfImage_predictions.copy()","7da27c13":"# 1. Remove non-predictive rows; 324 in total\ndrop = (iPredictions_clean['p1_dog'] == False) & (iPredictions_clean['p2_dog'] == False) & (iPredictions_clean['p3_dog'] == False)\n\niPredictions_clean.drop(iPredictions_clean[drop].index, inplace=True)\niPredictions_clean.reset_index(inplace=True, drop=True)\n\nlen(iPredictions_clean[drop]) == 0","b7b586ef":"# 2. Rename tweet_id into a string and rename to ID for consistency\niPredictions_clean['tweet_id'] = iPredictions_clean['tweet_id'].astype(str)\niPredictions_clean.rename(columns={'tweet_id':'ID'}, inplace=True)\n\n# testing if datatype is now object\niPredictions_clean['ID'].dtype == object","c52e52a8":"# 3. Extract most confident prediction and store it in column `breed` and `confidence_level` for the confidence\nfor i in range(len(iPredictions_clean)):\n    if iPredictions_clean.loc[i, 'p1_dog'] == True:\n        iPredictions_clean.loc[i, 'breed'] = iPredictions_clean.iloc[i]['p1']\n        iPredictions_clean.loc[i, 'confidence_level'] = iPredictions_clean.iloc[i]['p1_conf']\n    elif iPredictions_clean.loc[i, 'p2_dog'] == True:\n        iPredictions_clean.loc[i, 'breed'] = iPredictions_clean.iloc[i]['p2']\n        iPredictions_clean.loc[i, 'confidence_level'] = iPredictions_clean.iloc[i]['p2_conf']\n    elif iPredictions_clean.loc[i, 'p3_dog'] == True:\n        iPredictions_clean.loc[i, 'breed'] = iPredictions_clean.iloc[i]['p3']\n        iPredictions_clean.loc[i, 'confidence_level'] = iPredictions_clean.iloc[i]['p3_conf']\n        \n# visual assessment\/testing via tail; comparing if the conditions have been obeyed\niPredictions_clean.tail()","e78d4b44":"# creating copy of original DataFrame before applying any changes\ntAPI_clean = dfTwitter_api.copy()","662fd0f5":"# 1. Rename `id` into a string and rename to `ID` for consistency\ntAPI_clean['id'] = tAPI_clean['id'].astype(str)\ntAPI_clean.rename(columns={'id':'ID'}, inplace=True)\n\n# testing if datatype is now object\ntAPI_clean['ID'].dtype == 'object'","66a621b2":"# merging the 'breed' and 'cofidence_level' into the 'tArchive_clean' DataFrame\ncolumns = ['ID', 'breed', 'confidence_level']\ntArchive_master = pd.merge(tArchive_clean, iPredictions_clean[columns], on='ID', how='inner')","cee15dff":"tArchive_master.head()","3a15da37":"# merging the 'favorites' and 'retweets' into the 'tArchive_clean' DataFrame\ncolumns = ['ID', 'favorites', 'retweets']\ntArchive_master = pd.merge(tArchive_master, tAPI_clean[columns], on='ID', how='inner')","9541c227":"# last visual inspection before saving to master file\ntArchive_master.head()","18982bf8":"tArchive_master.tail()","73938c99":"# tArchive_master.to_csv('t_archive_master.csv', index = False)","99757711":"# ls *.csv","b4e4b457":"# reading in the cleaned up file into our final DataFrame\ndf = pd.read_csv('\/kaggle\/input\/twitter\/t_archive_master.csv')\ndf.head()","083e2327":"source = df['source'].value_counts()\nlabels = df['source'].unique()\n\nfig, ax = plt.subplots(figsize=(6, 5))\nfig.subplots_adjust(0.3,0,1,1)\n\n_, _ = ax.pie(source, startangle=90)\n\ntotal = sum(source)\nplt.legend(\n    loc='upper left',\n    labels=['%s, %1.1f%%' % (\n        l, (float(s) \/ total) * 100) for l, s in zip(labels, source)],\n    prop={'size': 11},\n    bbox_to_anchor=(0.0, 1),\n    bbox_transform=fig.transFigure\n)\n\nax.set_title('Tweets per Source')\nax.axis('equal')\nplt.show()","03000b58":"mu = df['rating_numerator'].mean()\nsigma = df['rating_numerator'].std()\nx = df['rating_numerator']\n\nbins = 12\n\nfig, ax = plt.subplots()\n\n# histogram of the data\nn, bins, patches = ax.hist(x, bins, density=1)\n\n# additional 'best fit' line\ny = ((1 \/ (np.sqrt(2 * np.pi) * sigma)) *\n     np.exp(-0.5 * (1 \/ sigma * (bins - mu))**2))\nax.plot(bins, y, '--')\nax.set_xlabel('Rating Numerator')\nax.set_ylabel('Relative Frequency')\nax.set_title('Distribution of Ratings')\n\nfig.tight_layout()\nplt.show()","8503c217":"# gathering some additional insights using some simple descriptive statistics\nprint('Mean:', df['rating_numerator'].mean()), \nprint('Median:', df['rating_numerator'].median()), \nprint('Mode:', statistics.mode(df['rating_numerator']))","9f629b08":"# top five breeds in the DataFrame\nb = df.breed.value_counts()\nb[b > 50]","8f939159":"# what is the most frequent stage\ndf.stage.hist()\nplt.ylabel('Frequency')\nplt.xlabel('Stage')\nplt.title('Frequency of Stages');","6ce53509":"print('Total Favorites:', df.favorites.sum()), print('Total ReTweets:', df.retweets.sum())","2eeb3d19":"x = df['timestamp']\ny1 = df.rolling(window=50)['favorites'].mean()\ny2 = df.rolling(window=50)['retweets'].mean()\n\nplt.plot(x,y1, color='red', linewidth=1, label='Favorites')\nplt.plot(x,y2, color='blue', linewidth=1, label='Retweets')\nplt.xticks([])\nplt.legend()\nplt.xlabel('Time')\nplt.ylabel('Amount of Retweets\/Favorites')\nplt.title('Engagement with Tweets over Time')\nplt.show();","15f3c7d5":"- These should be easily convertible into meaningful categories","bdda55d2":"### `dfTwitter_api`\n\n1. Rename `id` into a string and rename to `ID` for consistency\n2. Check for 2331 tweets in other DataFrames","736268bb":"- Since there are just a few, we're going to drop all of them","6f22da1a":"### `dfImage_predictions`\n\n#### Visual assessment","994c903d":"### Calling the Twitter API for additional information","b7fef99b":"### Importing the Twitter archive","8828f75e":"- Since these don't really offer a comparison between predicted and actual dog breed, we can drop these","e6da4c55":"### Storing DataFrame\n\nAs a last step, we will save the new `tArchive_master` as a CSV-file.","16328399":"#### Summary\n\n##### Quality\n\n- Remove non-predictive rows; 324 in total\n- Convert `tweet_id` into a string\n\n##### Tidiness\n\n- The third rule of tidy data says: \"each type of observational unit forms a table\". Therefore, it would make sense to merge this data with the `dfTwitter_archive` in order to achieve one master DataFrame\n\n### `dfTwitter_api`\n#### Visual assessment","6ff10075":"## Analyzing, and Visualizing Data for this Project\n\nIn the very last step, we're going to analyze the clean and merged data using the just created `t_archive_master.csv` file for the final DataFrame.","97543cc7":"#### Summary\n\n##### Quality\n\n- Harmonize names in name and replace non-sense naming with none for consistency\n- Handle missing URLs for `expanded_urls`\n- Normalize the `rating_numerator`; drop all above 15\n- Normalize the `rating_denominator`; drop all above 10\n- Drop data from Vine (source contains `vine`)\n- Convert the HTML in `source` into useful categories\n- Convert `tweet_id` into string format\n- Convert `timestamp` into timestamp format\n- Drop unnecessary data: `in_reply_to_status_id`, `in_reply_to_user_id`, `retweeted_status_id`, `retweeted_status_user_id`, `retweeted_status_timestamp` \n\n##### Tidiness\n\n- The 4 different columns doggo, floofer, pupper and puppo, are all relative to the same variable that identifies the stage of dog. So we can melt these columns into a single column named \"dog stage\" or something similar","197d0783":"## Programmatic data cleaning\n\nThree steps required for programmatic data cleaning:\n\n1. **Define** how you're going to clean the issues\n2. **Code** the related functions and methods\n3. **Test** the data if the code has been applied correctly\n\nHowever, before starting we need to create copies of the original DataFrames:","d3875ed1":"- This means that only 3 out of the other rows with missing expanded_urls are real tweets, all others can be dropped","58fa8de7":"#### Programmatic assessment","d18931cc":"- We might want to convert the `tweet_id` into a string as it is more a unique identifier rather than a number used for calculations\n- The `timestamp` column should be in a useful timestamp format\n- There are missing URLS for `expaned_urls`","ffab9b4b":"### Tweets per source","bd60cbd8":"## Assessing data\n\nThe next step is assessing the data by searching for **untidy** and\/or **low quality** data which both need to be treated differently. While **messy\/untidy** data describes structural problems within a data set, **dirty\/low quality** means inaccurate, corrupted, or duplicated data.\n\n### `dfTwitter_archive`\n\n#### Visual assessment","19fc3725":"### Favorites and Retweets","e24d0014":"# DAND Project 3 |\u00a0Data wrangling\n\nThis is the third project in my Data Analyst Nanodegree (DAND) program which covers data wrangling in particular. \n\n## Introduction\n\nIn order to pass this assignment, the following tasks are required:\n\n[**Data gathering**](#Data-gathering)\n\n- Import CSV for WeRateDogs Twitter archive using the provided `twitter-archive-enhanced.csv` file\n- Programmatically download the `image-predictions.tsv` file through the Requests library\n- Download data for *retweet count* and *favorite count* through the Twitter API using the Tweepy library and store each tweet's entire set of JSON data in a file called `tweet_json.txt`\n\n[**Assessing data**](#Assessing-data)\n\n- Identify eight (8) quality issues and two (2) tidiness issues\n\n[**Programmatic data cleaning**](#Programmatic-data-cleaning)\n\n- Properly clean all identified issues programmatically in a copied DataFrame\n\n[**Storing, Analyzing, and Visualizing Data**](#Storing,-Analyzing,-and-Visualizing-Data-for-this-Project)\n\n- Store the clean DataFrame(s) in a CSV file with the main one named `twitter_archive_master.csv`\n- At least three (3) insights and one (1) visualization must be produced\n\n[**Report**](#Report)\n\n- Create a 300-600 word written report called `wrangle_report.pdf` or `wrangle_report.html` that briefly describes your wrangling efforts\n- Create a 250-word-minimum written report called `act_report.pdf` or `act_report.html` that communicates the insights and displays the visualization(s) produced from your wrangled data\n\n\n## Data gathering\n\nFirst of all, we need to gather the data manually or programmatically and afterwards import all three files into three seperate DataFrames.","71669f02":"### Programmatically downloading the image predictions file","653a7307":"- It seems that we don't have any NaN values\n- `tweet_id` needs to be converted into a string to be matched with the other DataFrames later","3c6fd06b":"- The maximum value of 1,776 for `rating_numerator` seems a bit too high\n- Also the `rating_denominator` should be at 10, or not?","90835d2f":"- All confidence levels `p1_conf`, `p2_conf` and `p3_conf` seem in range of 0-1","bd699fda":"- We will drop these as well, since it's just a few","47295d1f":"- In general the data set looks way more tidied up than the Twitter archive\n- It seems that sometimes there are strange predictions like 'orange'\n\n#### Programmatic Assessment","43cd20e8":"### `dfImage_predictions`\n\n1. Remove non-predictive rows; 324 in total\n2. Rename `tweet_id` into a string and rename to `ID` for consistency\n3. Extract most confident prediction and store it in column `breed` and `confidence_level` for the confidence","deac480b":"It seems that the vast majority is coming from Twitter for iPhone (93.8%).\n\n### Analyzing the ratings","e10dd099":"### `dfTwitter_archive`\n\n#### Define\n\n1. Harmonize names in `name` and replace non-sense naming with `none` for consistency\n2. Drop all entries with `retweeted_status_id` is not null (not original tweets but replies\/RTs)\n3. Drop all entries with missing `expanded_urls` that are not valid tweets\n4. Drop all entries with `rating_numerator` > 15\n5. Set all entries with `rating_denominator` != 10 to `rating_denominator` = 10\n6. Combine all four columns for dog stages into a single column `stage`\n7. Convert the HTML-string from `source` into categories\n8. Convert `tweet_id` into string format; rename to `ID`\n9. Convert `timestamp` into proper timestamp format\n10. Drop columns `in_reply_to_status_id`, `in_reply_to_user_id`, `retweeted_status_id`, `retweeted_status_user_id`, `retweeted_status_timestamp` \n\n#### Code","10bde232":"#### Summary\n\n##### Quality\n\n- Since we could only catch additional information for 2331 tweets, we may have to proceed only with this number in all DataFrames to be able to perform a complete analysis\n\n##### Tidiness\n\n- Harmonize naming for `id` and `tweet_id` for all three DataFrames","b1150e53":"### Dog breeds and stages","bd3dfe01":"- The instructor advised that the `rating_numerator`, `name`, stage (`doggo`, `floofer`, `pupper`, `puppo`) could contain incorrect data\n- We have some columns that contain unnecessary data: `in_reply_to_status_id`, `in_reply_to_user_id`, `retweeted_status_id`, `retweeted_status_user_id`, `retweeted_status_timestamp` \n- The four columns for the dog stages should be combined in one single column `stage`\n- As a side-task, we could convert the HTML in `source` into useful categories\n\n#### Programmatic assessment","2ce707d6":"### Merging DataFrames\n\nTo solve the last **data tidiness** issue of fragmented data across multiple DataFrames, we're going to merge the relevant data into one, central DataFrame `tArchive_master`."}}