{"cell_type":{"fdeeb973":"code","ef427bc8":"code","d80422f5":"code","661837ba":"code","d4fae419":"code","e76c5c4f":"code","ab421d4f":"code","1e18ea4e":"code","2d2028ef":"code","85e222a7":"code","d70313d5":"code","ebabe714":"code","b72566a2":"code","d34f8095":"code","e1da7ba3":"code","49d881d7":"code","249d7c55":"code","a8b89c4e":"code","848bb9b7":"code","83db181f":"code","593fc540":"code","4983e09c":"code","85a21e3b":"code","b9d8129c":"code","def1b0c0":"code","891ca4e2":"code","a7f4164f":"markdown","eac6dd52":"markdown","db800bbd":"markdown","ac709621":"markdown","79ad367d":"markdown","422d28b6":"markdown","e7019375":"markdown","94ddd915":"markdown","7fc33adf":"markdown","009df9a2":"markdown","b4cca58f":"markdown","7ba1fba3":"markdown","cb632bf7":"markdown","38c33cef":"markdown","5a160dff":"markdown","4efe9621":"markdown","0ebbe201":"markdown","440be43d":"markdown","eaba9e4e":"markdown","e7b42495":"markdown","702fd242":"markdown","fc1f44b6":"markdown","7d5704ce":"markdown","2f62a137":"markdown","8af21da0":"markdown","8dfafc2a":"markdown","d68947f9":"markdown"},"source":{"fdeeb973":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nplt.style.use('ggplot')\nimport glob\nimport json","ef427bc8":"#root_path = '\/kaggle\/input\/CORD-19-research-challenge\/'\nroot_path = '..\/input\/a-custom-literature-corpus'\nmetadata_path = '..\/input\/pandas-df\/metadata.csv'\nroot_path = '\/Users\/grace\/Desktop\/source\/'\nmetadata_path = '\/Users\/grace\/Desktop\/source\/metadata.csv'\nmeta_df = pd.read_csv(metadata_path, dtype={\n    'doi': str, \n    'title': str,\n    'authors': str,\n})\nmeta_df.head()","d80422f5":"meta_df.info()","661837ba":"json_path = '\/Users\/grace\/Desktop\/source\/JSON'\nall_json = glob.glob(f'{json_path}\/**\/*.json', recursive=True)\nlen(all_json) ","d4fae419":"with open(all_json[0]) as file:\n    first_entry = json.load(file)\n    print(json.dumps(first_entry, indent=4))","e76c5c4f":"class FileReader:\n    def __init__(self, file_path):\n        with open(file_path) as file:\n            content = json.load(file)\n            self.body_text = []\n            self.abstract = []\n            self.paper_id = content['paper_id']\n            self.abstract.append(content['abstract'])\n            for entry in content['pdf_parse']['body_text']:\n                self.body_text.append(entry['text'])\n            self.abstract = '\\n'.join(self.abstract)\n            self.body_text = '\\n'.join(self.body_text)\n    def __repr__(self):\n        return f'{self.paper_id}: {self.abstract[:200]}... {self.body_text[:200]}...'\nfirst_row = FileReader(all_json[0])\nprint(first_row)","ab421d4f":"def get_breaks(content, length):\n    data = \"\"\n    words = content.split(' ')\n    total_chars = 0\n    # add break every length characters\n    for i in range(len(words)):\n        total_chars += len(words[i])\n        if total_chars > length:\n            data = data + \"<br>\" + words[i]\n            total_chars = 0\n        else:\n            data = data + \" \" + words[i]\n    return data","1e18ea4e":"dict_ = {'paper_id': [], 'doi':[], 'abstract': [], 'body_text': [], 'authors': [], 'title': [], 'abstract_summary': []}\nfor idx, entry in enumerate(all_json):\n    if idx % (len(all_json) \/\/ 10) == 0:\n        print(f'Processing index: {idx} of {len(all_json)}')\n    try:\n        content = FileReader(entry)\n    except Exception as e:\n        continue  # invalid paper format, skip\n    # get metadata information\n    meta_data = meta_df.loc[meta_df['paper_id'] == content.paper_id]\n    # no metadata, skip this paper\n    if len(meta_data) == 0:\n        continue\n    dict_['abstract'].append(content.abstract)\n    dict_['paper_id'].append(content.paper_id)\n    dict_['body_text'].append(content.body_text)\n    # also create a column for the summary of abstract to be used in a plot\n    if len(content.abstract) == 0: \n        # no abstract provided\n        dict_['abstract_summary'].append(\"Not provided.\")\n    elif len(content.abstract.split(' ')) > 100:\n        # abstract provided is too long for plot, take first 100 words append with ...\n        info = content.abstract.split(' ')[:100]\n        summary = get_breaks(' '.join(info), 40)\n        dict_['abstract_summary'].append(summary + \"...\")\n    else:\n        # abstract is short enough\n        summary = get_breaks(content.abstract, 40)\n        dict_['abstract_summary'].append(summary)\n    # get metadata information\n    #meta_data = meta_df.loc[meta_df['sha'] == content.paper_id]\n    try:\n        # if more than one author\n        authors = meta_data['authors'].values[0].split(';')\n        if len(authors) > 2:\n            # if more than 2 authors, take them all with html tag breaks in between\n            dict_['authors'].append(get_breaks('. '.join(authors), 40))\n        else:\n            # authors will fit in plot\n            dict_['authors'].append(\". \".join(authors))\n    except Exception as e:\n        # if only one author - or Null valie\n        dict_['authors'].append(meta_data['authors'].values[0])\n    # add the title information, add breaks when needed\n    try:\n        title = get_breaks(meta_data['title'].values[0], 40)\n        dict_['title'].append(title)\n    # if title was not provided\n    except Exception as e:\n        dict_['title'].append(meta_data['title'].values[0])\n    # add the journal information\n    #dict_['journal'].append(meta_data['journal'].values[0])\n    # add doi\n    dict_['doi'].append(meta_data['doi'].values[0])\n    \ndf_reef = pd.DataFrame(dict_, columns=['paper_id', 'doi', 'abstract', 'body_text', 'authors', 'title', 'journal', 'abstract_summary'])\ndf_reef.head()","2d2028ef":"df_reef['abstract_word_count'] = df_reef['abstract'].apply(lambda x: len(x.strip().split()))  # word count in abstract\ndf_reef['body_word_count'] = df_reef['body_text'].apply(lambda x: len(x.strip().split()))  # word count in body\ndf_reef['body_unique_words']=df_reef['body_text'].apply(lambda x:len(set(str(x).split())))  # number of unique words in body\ndf_reef.head()","85e222a7":"df_reef.drop_duplicates(['abstract', 'body_text'], inplace=True)\ndf_reef['abstract'].describe(include='all')","d70313d5":"df_reef['body_text'].describe(include='all')","ebabe714":"df_reef.head()","b72566a2":"df_reef.describe()","d34f8095":"df = df_reef","e1da7ba3":"from IPython.utils import io\nwith io.capture_output() as captured:\n    !pip3 install https:\/\/s3-us-west-2.amazonaws.com\/ai2-s2-scispacy\/releases\/v0.2.4\/en_core_sci_lg-0.2.4.tar.gz","49d881d7":"#NLP \nimport spacy\nfrom spacy.lang.en.stop_words import STOP_WORDS\nimport en_core_sci_lg  # model downloaded in previous step","249d7c55":"import string\n\npunctuations = string.punctuation\nstopwords = list(STOP_WORDS)\nstopwords[:10]","a8b89c4e":"# Parser\nparser = en_core_sci_lg.load(disable=[\"tagger\", \"ner\"])\nparser.max_length = 7000000\n\ndef spacy_tokenizer(sentence):\n    mytokens = parser(sentence)\n    mytokens = [ word.lemma_.lower().strip() if word.lemma_ != \"-PRON-\" else word.lower_ for word in mytokens ]\n    mytokens = [ word for word in mytokens if word not in stopwords and word not in punctuations ]\n    mytokens = \" \".join([i for i in mytokens])\n    return mytokens","848bb9b7":"We will need to search for key terms by string matching. We can use spaCy's EntityRuler","83db181f":"from spacy.pipeline import EntityRuler\n\n#Build upon the spaCy Small Model\nnlp = spacy.blank(\"en\")\n\n#Create the Ruler and Add it\nruler = EntityRuler(nlp)\n\n#List of Entities and Patterns (source: https:\/\/spacy.io\/usage\/rule-based-matching)\npatterns = [\n                {\n                    \"label\": \"GOTERM\", \"pattern\": [{\"TEXT\": {\"REGEX\": \"(GO.{8})\"}}\n                                                        ]\n                }\n            ]\n#add patterns to ruler\nruler.add_patterns(patterns)\nnlp.add_pipe(ruler)\n\n#create the doc \ndoc = nlp(text)\n\n#extract entities\nfor ent in doc.ents:\n    print (ent.text, ent.label_)\n    \n## create the doc using json\ngos = []\nfor file in all_json:\n    json_text = FileReader(file)\n    gos.append(json_text)\n    \nfor i in gos:\n    text = str(i)\n    doc = nlp(text)","593fc540":"from tqdm import tqdm\ntqdm.pandas()\ndf[\"processed_text\"] = df[\"body_text\"].progress_apply(spacy_tokenizer)","4983e09c":"import seaborn as sns","85a21e3b":"sns.histplot(df['body_word_count'], kde=True, stat=\"density\", linewidth=0)\ndf['body_word_count'].describe()","b9d8129c":"sns.histplot(df['body_unique_words'])\ndf['body_unique_words'].describe()","def1b0c0":"from sklearn.feature_extraction.text import TfidfVectorizer\ndef vectorize(text, maxx_features):\n    \n    vectorizer = TfidfVectorizer(max_features=maxx_features)\n    X = vectorizer.fit_transform(text)\n    return X","891ca4e2":"text = df['processed_text'].values\nX = vectorize(text, 2 ** 12)\nX.shape","a7f4164f":"***Vectorization*** Now that we have pre-processed the data, it is time to convert it into a format that can be handled by our algorithms. For this purpose we will be usin tf-idf. This will convert our string formatted data into a measure of how important each word is to the instance out of the literature as a whole.","eac6dd52":"custom_stop_words = [\n    'doi', 'preprint', 'copyright', 'peer', 'reviewed', 'org', 'https', 'et', 'al', 'author', 'figure', \n    'rights', 'reserved', 'permission', 'used', 'using', 'biorxiv', 'medrxiv', 'license', 'fig', 'fig.', \n    'al.', 'Elsevier', 'PMC', 'CZI', 'www'\n]\n\nfor w in custom_stop_words:\n    if w not in stopwords:\n        stopwords.append(w)","db800bbd":"***Checking JSON Schema Structure***","ac709621":"***Download SpaCy bio parser***","79ad367d":"Citations\n\n@inproceedings{COVID-19 Literature Clustering,\n    author = {Eren, E. Maksim. Solovyev, Nick. Nicholas, Charles. Raff, Edward},\n    title = {COVID-19 Literature Clustering},\n    year = {2020},\n    month = {April},\n    location = {University of Maryland Baltimore County (UMBC), Baltimore, MD, USA},\n    note={Malware Research Group},\n    url = {\\url{https:\/\/github.com\/MaksimEkin\/COVID19-Literature-Clustering}},\n    howpublished = {TBA}\n}","422d28b6":"***Stopwords***\nPart of the preprocessing will be finding and removing stopwords (common words that will act as noise in the clustering step).","e7019375":"***Data preprocessing***\nKaggle limits the dataframe to 10,000 instances","94ddd915":"Helper function adds break after every words when character length reach to certain amount. This is for the interactive plot so that hover tool fits the screen.","7fc33adf":"# ***Fetch all of JSON file path***\nGet path to all JSON files:","009df9a2":"Apply the text-processing function on the body_text","b4cca58f":"***Clean duplicates***\nThere may be duplicates by author submiting the article to multiple journals","7ba1fba3":"Now the above stopwords are used in everyday english text. Research papers will often frequently use words that don't actually contribute to the meaning and are not considered everyday stopwords.\n\nThank you Daniel Wolffram for the idea.","cb632bf7":"* **Feature engineering**\nAdding word count columns for both abstract and body_text can be useful parameters later:\n","38c33cef":"Let's load the metadata of the dataset. 'title' & 'journal' attributes may be useful later when we cluster the articles to see what kinds of aricles cluster together","5a160dff":"***Approach***\n- Parse the text from the body of each document using NLP\n- Turn each document instance di into a feature vector Xi using Term-Frequency-inverse Document Frequency (TF-IDF)\n- Apply dimentionality Reduction to each feature vector Xi using t-Distributed Stochastic Neighbor Embedding (t-SNE) to cluster similar research articles in the two dimentional plane X embedding Yi\n- Use PCA t project down the dimentions of X to a number of dimentions that will keep 0.95 variance while removing noise and outliers in embedding Y2\n- Apply k-means clustering on Y2 where K is 20, to label each clustern on Y1\n- Apply Topic Modeling on X using Latent Dirichlet Allocation (LDA) to discover keywords for ech cluster\n- Investigate the clusters wisually on the plot, zooming down to specific articles as needed, and via classification using Stochastic Gradient Descent (SGD)","4efe9621":"***Table of contents***\n1. Convert pdfs to text \n2. loading the data\n3. pre-processing\n4. vectorization\n5. PCA & Clustering\n6. dientionality reduction with t-SNE\n7. topic modeling on each cluster\n8. classify\n9. plot\n10. how to use the plot?\n11. conclusion\n12. specific research goal\n13. citation\/sources","0ebbe201":"***PCA & Clustering***\nLet's see how much we can reduce the dimensions while still keeping 95% variance. We will apply Principle Component Analysis (PCA) to our vectorized data. The reason for this is that by keeping a large number of dimensions with PCA, you don\u2019t destroy much of the information, but hopefully will remove some noise\/outliers from the data, and make the clustering problem easier for k-means. Note that X_reduced will only be used for k-means, t-SNE will still use the original feature vector X that was generated through tf-idf on the NLP processed text.","440be43d":"***Look at data***","eaba9e4e":"Now that we have our dataset loaded, we need to clean-up the text to improve any clustering or classification efforts. First, drop Null vales","e7b42495":"It seems that the JSON schema given is incorrect if we look into dumped first entry. The abstract, body_text, bib_entries, ref_entries, and back_matter are not the child of metadata key.\n\n***Helper: File Reader Class***","702fd242":"***Next let's create a function that will process the text data for us***\nFor this purpose we will be using the spacy library. This function will convert text to lower case, remove punctuation, and find and remove stopwords. For the parser, we will use en_core_sci_lg. This is a model for processing biomedical, scientific or clinical text.","fc1f44b6":"***Load the data into dataframe***\nUsing the helper functions, let's read in the articles into a DataFrame that can be used easily:","7d5704ce":"After creating a corpus, the literature clustering approach is credited to Eren, E. Maksim. Solovyev, Nick. Nicholas, Charles. Raff, Edward and their full interactive plot can be found on GitHub https:\/\/maksimekin.github.io\/COVID19-Literature-Clustering\/plots\/t-sne_covid-19_interactive.html\n","2f62a137":"**Look at the word count**","8af21da0":"In the majority of this notebook we will be working with body_text\nLinks to the papers will be generated using doi","8dfafc2a":"# Loading the data\nFollowing the notebook by Ivan Ega Pratama, from Kaggle.","d68947f9":"Vectorize our data. We will be clustering based off the content of the body text. The maximum number of features will be limited. Only the top 2 ** 12 features will be used, eseentially acting as a noise filter. Additionally, more features cause painfully long runtimes."}}