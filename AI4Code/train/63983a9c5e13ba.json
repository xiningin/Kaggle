{"cell_type":{"448e0f4f":"code","2297353a":"code","5aa3b4f0":"code","a39f8529":"code","0d87e5ba":"code","694b1879":"code","628b4502":"code","6ea3f1f8":"code","93729091":"code","dc9dedf4":"code","f410d449":"code","2ab87347":"code","a37f40cc":"code","f6876cad":"code","9e87b335":"code","fe96758d":"code","bc54ecdc":"code","2f4e2204":"code","be8ce5cb":"code","3ade4af4":"code","3b4d2041":"code","36f6bc1e":"code","38f53f5f":"code","735b753b":"code","2777d206":"code","841fa7c0":"code","5d20f455":"code","3d3d9157":"code","013e7434":"code","81a58089":"code","a43b6cf5":"code","47bc4afd":"code","53c8a2dd":"code","e2782667":"code","caa844b8":"markdown","34101e6a":"markdown","dac2c342":"markdown","c770b3cf":"markdown","914683f4":"markdown","994d2af1":"markdown"},"source":{"448e0f4f":"%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')\nimport os\nimport gc\nimport time\nimport pickle\nimport feather\nimport numpy as np\nimport pandas as pd\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom tqdm._tqdm_notebook import tqdm_notebook as tqdm\ntqdm.pandas()","2297353a":"DATA_DIR = '..\/input\/'\n# train = pd.read_csv(DATA_DIR+'training_set.csv')\n# test_set_id = pd.read_csv(DATA_DIR+'test_set.csv', usecols=['object_id'])\n# test_set_id.shape # (453653104, 1)\ntrain = pd.read_csv(DATA_DIR+'training_set_metadata.csv')\ntest = pd.read_csv(DATA_DIR+'test_set_metadata.csv')","5aa3b4f0":"train.shape, test.shape","a39f8529":"display(train.head())\ndisplay(test.head())","0d87e5ba":"train.columns","694b1879":"feat_cols = [\n    'ra', 'decl', 'gal_l', 'gal_b', 'ddf', \n    'hostgal_specz', 'hostgal_photoz', 'hostgal_photoz_err', \n    'distmod', 'mwebv'\n]\nlen(feat_cols)","628b4502":"for c in feat_cols:\n    print(\n        'nan-ratio of {:>18} [train: {:.4f}; test: {:.4f}]'\n        .format(\n            c, \n            train[c].isnull().sum() \/ train.shape[0], \n            test[c].isnull().sum() \/ test.shape[0])\n    )","6ea3f1f8":"for c in feat_cols:\n    plt.figure(figsize=[20, 4])\n    plt.subplot(1, 2, 1)\n    sns.violinplot(x='target', y=c, data=train)\n    plt.grid()\n    plt.subplot(1, 2, 2)\n    sns.distplot(train[c].dropna())\n    sns.distplot(test[c].dropna())\n    plt.legend(['train', 'test'])\n    plt.grid()\n    plt.show();","93729091":"target = train['target'].values.copy()\ndel train['target']","dc9dedf4":"train_ids = train['object_id'].copy()\ntest_ids = test['object_id'].copy()\ndel train['object_id'], test['object_id'];","f410d449":"train['target'] = target.copy()\nsns.pairplot(train[feat_cols+['target']].dropna(), hue='target', vars=feat_cols)\ndel train['target']","2ab87347":"data = pd.concat([\n    train[feat_cols], \n    test[feat_cols].sample(frac=5 * train.shape[0]\/test.shape[0])\n], ignore_index=True)\ndata['is_test'] = 1\ndata['is_test'][:train.shape[0]] = 0\nsns.pairplot(data, hue='is_test', vars=feat_cols, plot_kws={'alpha': 0.25})\ndel data; gc.collect();","a37f40cc":"tmp = False\nfor i in [6, 16, 53, 65, 92]:\n    tmp|=(target==i)\nprint(\n    tmp.sum(), \n    (train[['hostgal_specz', 'hostgal_photoz', 'hostgal_photoz_err']].sum(1)==0).sum(), \n    (train['distmod'].isnull()).sum()\n)","f6876cad":"print(\n    (test[[\n        'hostgal_specz', 'hostgal_photoz', 'hostgal_photoz_err'\n    ]].sum(1)==0).sum(), (test['distmod'].isnull()).sum()\n)","9e87b335":"train_mask = train['distmod'].isnull().values\ntest_mask = test['distmod'].isnull().values","fe96758d":"labels2weight = {x:1 for x in np.unique(target)}\nlabels2weight[64] = 2\nlabels2weight[15] = 2","bc54ecdc":"import lightgbm as lgb\n\nround_params = dict(num_boost_round = 20000,\n                    early_stopping_rounds = 100,\n                    verbose_eval = 50)\nparams = {\n    \"objective\": \"multiclass\",\n    \"metric\": \"multi_logloss\",\n    #\"num_class\": len(np.unique(y)),\n    #\"two_round\": True,\n    \"num_leaves\" : 30,\n    \"min_child_samples\" : 30,\n    \"learning_rate\" : 0.03,\n    \"feature_fraction\" : 0.75,\n    \"bagging_fraction\" : 0.75,\n    \"bagging_freq\" : 1,\n    \"seed\" : 42,\n    \"lambda_l2\": 1e-2,\n    \"verbosity\" : -1\n}\n\ndef lgb_cv_train(X, labels, X_test, \n                 params=params, round_params=round_params):\n    print('X', X.shape, 'labels', labels.shape, 'X_test', X_test.shape)\n    print('unique labels', np.unique(labels))\n    \n    labels2y = dict(map(reversed, enumerate(np.unique(labels))))\n    y2labels = dict(enumerate(np.unique(labels)))\n    y = np.array(list(map(labels2y.get, labels)))\n    weight = np.array(list(map(labels2weight.get, labels)))\n    \n    params['num_class'] = len(np.unique(y))\n    cv_raw = lgb.cv(\n        params, \n        lgb.Dataset(X, label=y, weight=weight), \n        nfold=5, \n        **round_params\n    )\n    best_round = np.argmin(cv_raw['multi_logloss-mean'])\n    best_score = cv_raw['multi_logloss-mean'][best_round]\n    print(f'best_round: {best_round}', f'best_score: {best_score}')\n    model = lgb.train(\n        params, \n        lgb.Dataset(X, label=y, weight=weight), \n        num_boost_round=best_round, \n    )\n    pred = model.predict(X_test)\n    pred_labels = pd.DataFrame(\n        {f'class_{c}': pred[:, i] for i,c in enumerate(np.unique(labels))}\n    )\n    res = dict(\n        model=model,\n        best_round=best_round,\n        best_score=best_score,\n        pred_labels=pred_labels\n    )\n    return res","2f4e2204":"feat_extra_li = ['hostgal_specz', 'hostgal_photoz', 'hostgal_photoz_err', 'distmod']\nfeat_gal_cols = ['ra', 'decl', 'gal_l', 'gal_b', 'ddf', 'mwebv']\nfeat_extra_cols = feat_gal_cols + feat_extra_li\nprint(feat_gal_cols)\nprint(feat_extra_cols)","be8ce5cb":"np.unique(target[train_mask]), np.unique(target[~train_mask])","3ade4af4":"%%time\nres_gal = lgb_cv_train(\n    train.loc[train_mask, feat_gal_cols], \n    target[train_mask], \n    test.loc[test_mask, feat_gal_cols]\n)","3b4d2041":"res_gal['pred_labels'].head()","36f6bc1e":"n_gal = res_gal['pred_labels'].shape[1]\nres_gal['pred_labels'] = res_gal['pred_labels'] * n_gal\/(n_gal+1)\nres_gal['pred_labels']['class_99'] = 1\/(n_gal+1)\nres_gal['pred_labels'].head()","38f53f5f":"%%time\nres_extra = lgb_cv_train(\n    train.loc[~train_mask, feat_extra_cols], \n    target[~train_mask], \n    test.loc[~test_mask, feat_extra_cols]\n)","735b753b":"res_extra['pred_labels'].head()","2777d206":"n_extra = res_extra['pred_labels'].shape[1]\nres_extra['pred_labels'] = res_extra['pred_labels'] * n_extra\/(n_extra+1)\nres_extra['pred_labels']['class_99'] = 1\/(n_extra+1)\nres_extra['pred_labels'].head()","841fa7c0":"sub = pd.read_csv(DATA_DIR+'sample_submission.csv')\nsub = sub.set_index('object_id')\nsub[:] = 0\nsub.head()","5d20f455":"classnames = sub.columns.tolist()\nprint(sub.shape, classnames)","3d3d9157":"for c in res_gal['pred_labels'].columns:\n    sub.loc[test_mask, c] = res_gal['pred_labels'][c].values\nfor c in res_extra['pred_labels'].columns:\n    sub.loc[~test_mask, c] = res_extra['pred_labels'][c].values","013e7434":"sub.tail(10)","81a58089":"%%time\nscore = res_gal['best_score'] * (train_mask).sum()\/train.shape[0]\nscore+= res_extra['best_score'] * (~train_mask).sum()\/train.shape[0]\nsub.reset_index().to_csv(f'meta_lgb_{score}.csv', index=False, float_format='%.6f')","a43b6cf5":"os.listdir()","47bc4afd":"train['is_train'] = 1\ntest['is_train'] = 0\nX = pd.concat(\n    [train, \n     test\n     .sample(frac=5*train.shape[0]\/test.shape[0])],\n    ignore_index=True\n)\ny = X['is_train'].values.copy()\ndel X['is_train'], train['is_train'], test['is_train']\ndel X['hostgal_specz'] # this is obvious different disttributed in train\/test and will make auc=0.99\nX.head()","53c8a2dd":"from sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\nparams['objective'] = 'binary'\nparams['metric'] = 'auc'\nparams['num_class'] = 1\n\nprint('X', X.shape, 'y', y.shape)\npred_adv = np.zeros(X.shape[0])\nkf = KFold(n_splits=5, shuffle=True, random_state=42)\ndtrain = lgb.Dataset(X, label=y)\ndtrain.construct()\nmodels = []\nfor trn_idx, val_idx in kf.split(X):\n    model = lgb.train(\n        params, \n        dtrain.subset(trn_idx), \n        valid_sets=[dtrain.subset(trn_idx), dtrain.subset(val_idx)], \n        valid_names=['train', 'valid'],\n        **round_params\n    )\n    models.append(model)\n    pred_adv[val_idx] = model.predict(X.iloc[val_idx])\nadv_score = roc_auc_score(y, pred_adv)\nprint('oof score:', adv_score)","e2782667":"def get_imp_plot(name, feature_name, lgb_feat_imps, nfolds=5, savefig=False):\n    lgb_imps = pd.DataFrame(\n        np.vstack(lgb_feat_imps).T, \n        columns=['fold_{}'.format(i) for i in range(nfolds)],\n        index=feature_name,\n    )\n    lgb_imps['fold_mean'] = lgb_imps.mean(1)\n    lgb_imps = lgb_imps.loc[\n        lgb_imps['fold_mean'].sort_values(ascending=False).index\n    ]\n    lgb_imps.reset_index().to_csv(f'{name}_lgb_imps.csv', index=False)\n    del lgb_imps['fold_mean']; gc.collect();\n\n    max_num_features = min(len(feature_name), 300)\n    f, ax = plt.subplots(figsize=[8, max_num_features\/\/2])\n    data = lgb_imps.iloc[:max_num_features].copy()\n    data_mean = data.mean(1).sort_values(ascending=False)\n    data = data.loc[data_mean.index]\n    data_index = data.index.copy()\n    data = [data[c].values for c in data.columns]\n    data = np.hstack(data)\n    data = pd.DataFrame(data, index=data_index.tolist()*nfolds, columns=['igb_imp'])\n    data = data.reset_index()\n    data.columns = ['feature_name', 'igb_imp']\n    sns.barplot(x='igb_imp', y='feature_name', data=data, orient='h', ax=ax)\n    plt.grid()\n    if savefig:\n        plt.savefig(f'{name}_lgb_imp.png')\n\nget_imp_plot(\n    'adv', \n    X.columns.tolist(), \n    [model.feature_importance()\/model.best_iteration for model in models]\n)","caa844b8":"### Assign the unknown class with average probability","34101e6a":"## Simple Adversarial Validation\n- run on sampled test set to save time","dac2c342":"### Maybe this explains why native benchmark outperforms LGB classifier trained on train(meta) dataset\n### Check the feature importance","c770b3cf":"## Inspired by the [great naitive benchmark kernel](https:\/\/www.kaggle.com\/kyleboone\/naive-benchmark-galactic-vs-extragalactic)  \n- We separate the train\/test metadata to galactic&extragalactic parts\n- And train two lgb classifiers\n","914683f4":"### Several conditions seem to be same","994d2af1":"## Class Weight\n- by https:\/\/www.kaggle.com\/c\/PLAsTiCC-2018\/discussion\/67194#397146"}}