{"cell_type":{"4d17dae4":"code","4caab8eb":"code","397db932":"code","cc6f0021":"code","6b6d1714":"code","0ca93362":"code","b4f7024e":"code","10079da4":"code","b31617b0":"code","f48c46de":"code","30c807f8":"code","68d595fc":"code","1b4a6f05":"code","05339da1":"code","e7966466":"code","5c5d6375":"code","38e9ae7d":"code","423e1a85":"code","c71e07e5":"code","19121ae2":"markdown","0fb9adc1":"markdown","029628c2":"markdown","19e59e0b":"markdown","d0e93664":"markdown","80dc4382":"markdown","1fdd43b8":"markdown","1890d107":"markdown","730863e3":"markdown","f9ef9354":"markdown","616c65c5":"markdown","18f2cbd2":"markdown","b0d29f3d":"markdown","9709ed4d":"markdown","c4896b96":"markdown"},"source":{"4d17dae4":"# Install libraries\/packages\n!conda install seaborn scikit-learn statsmodels numba pytables -y\n!conda install -c conda-forge python-igraph leidenalg -y\n!pip install quanp\n!pip install MulticoreTSNE","4caab8eb":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport quanp as qp\nimport gc\n\nfrom IPython.display import display\nfrom matplotlib import rcParams\n\n# setting visualization\/logging parameters\npd.set_option('display.max_columns', None)\nqp.set_figure_params(dpi=100, color_map = 'viridis_r')\nqp.settings.verbosity = 1\nqp.logging.print_versions()","397db932":"# Loading pandas dataframe as anndata \ntrain = pd.read_csv('\/kaggle\/input\/jane-street-market-prediction\/train.csv')\n\n# We disregard the timeseries perspective and drop all the rows with nan value in any columns. \ntrain = train.dropna()","cc6f0021":"# resampling the subjects at fraction of 0.1 - due to current notebook runtime memory restriction\ntrain = train.sample(frac=0.05, random_state=0)","6b6d1714":"# Get lists of features\ntrain_features = [s for s in train.columns if \"feature_\" in s]\ntrain[train_features].describe()","0ca93362":"def plot_outlier_graph_set(df):    \n\n    for f in df.columns:\n        fig, axs = plt.subplots(1, 4, figsize=(16, 5))\n\n        # plot 1\n        sns.boxplot(y=f, data=df, ax=axs[0])\n\n        # plot 2\n        sns.boxenplot(y=f, data=df, ax=axs[1])\n\n        # plot 3\n        sns.violinplot(y=f, data=df, ax=axs[2]) \n\n        # plot 4\n        sns.stripplot(y=f, data=df, size=4, color=\".3\", linewidth=0, ax=axs[3])\n\n\n        fig.suptitle(f, fontsize=15, y=1.1)\n        axs[0].set_title('Box Plot')\n        axs[1].set_title('Boxen Plot')\n        axs[2].set_title('Violin Plot')\n        axs[3].set_title('Strip Plot')\n\n        plt.tight_layout()\n        plt.show()","b4f7024e":"y_cols = [c for c in train.columns if 'resp' in c or 'weight' in c]\nplot_outlier_graph_set(train[y_cols])  \n","10079da4":"# Loading pandas dataframe as anndata \nadata = qp.AnnData(train[train_features])\n\n# add a new `.obs` column for additional categorical features\nadata.obs['weight'] = train['weight'].values\nadata.obs['resp_1'] = train['resp_1'].values\nadata.obs['resp_2'] = train['resp_2'].values\nadata.obs['resp_3'] = train['resp_3'].values\nadata.obs['resp_4'] = train['resp_4'].values\nadata.obs['resp'] = train['resp'].values\n\n# housekeeping\ntrain = 0\ngc.collect()","b31617b0":"rcParams['figure.figsize'] = 12, 8\nqp.tl.pca(adata, svd_solver='arpack', random_state=0);\nqp.pl.pca(adata, \n          color=['weight', 'resp', 'resp_1', 'resp_2', 'resp_3', 'resp_4'], \n          size=20, cmap='bwr', \n          ncols=2);","f48c46de":"qp.pl.pca_variance_ratio(adata, n_pcs=20)","30c807f8":"qp.pp.neighbors(adata, n_neighbors=30, n_pcs=5, random_state=0); # 30 nearest neighbors and only consider the first 4 pcs\nqp.tl.leiden(adata, random_state=0);","68d595fc":"rcParams['figure.figsize'] = 12, 8\nqp.tl.tsne(adata, n_pcs=10, random_state=0); # only consider the first 10 pcs\n\nqp.pl.tsne(adata, color=['leiden', 'weight', 'resp', 'resp_1', 'resp_2', 'resp_3', 'resp_4'], \n           size=20, cmap='bwr',\n           legend_loc='on data', ncols=2)","1b4a6f05":"rcParams['figure.figsize'] = 8,6\nqp.tl.paga(adata)\nqp.pl.paga(adata, plot=True)","05339da1":"rcParams['figure.figsize'] = 10, 6\nqp.tl.umap(adata, init_pos='paga', random_state=0)\nqp.pl.umap(adata, color=['leiden', 'weight'],            \n           size=20, cmap='bwr', vmax = 75,\n           legend_loc='on data', ncols=2)","e7966466":"qp.pl.umap(adata, color=['leiden', 'resp_1', 'resp_2', 'resp_3'],            \n           size=20, cmap='bwr', vmax = 0.1, vmin= -0.1,\n           legend_loc='on data', ncols=2)","5c5d6375":"qp.pl.umap(adata, color=['resp_4', 'resp'],            \n           size=20, cmap='bwr', vmax = 0.2, vmin= -0.2,\n           legend_loc='on data', ncols=2)","38e9ae7d":"# Before we perform the group comparisons to get the differentiating features, we can check if euclidean distance itself \n# can gives us some hints that there are patterns of features appear between clades of clusters. \nqp.tl.dendrogram(adata, 'leiden', var_names=adata.var_names);\nqp.pl.matrixplot(adata, adata.var_names, 'leiden',\n                 standard_scale='var',\n                 dendrogram=True, cmap='RdBu_r')","423e1a85":"import warnings\nwarnings.simplefilter(action='ignore', category=FutureWarning);\n\nrcParams['figure.figsize'] = 6,8;\nqp.tl.rank_features_groups(adata, 'leiden', method='wilcoxon');\nqp.pl.rank_features_groups(adata, n_features=30, sharey=False);","c71e07e5":"qp.pl.rank_features_groups_matrixplot(adata, n_features=5, \n                                      use_raw=False, \n                                      standard_scale='var',\n                                      dendrogram=True, cmap='RdBu_r')","19121ae2":"### Computing the T-distributed Stochastic Neighbor Embedding (tSNE)\nLet us further reduce the dimensionality of the signficant PCs identified above wholly in to 2 dimensions using the tSNE. Here, the 'leiden' clusters , 'weight', 'resp', 'resp_1', 'resp_2', 'resp_3', 'resp_4' were mapped onto the tSNE. There were 8 leiden clusters obtained. ","0fb9adc1":"We can visualize the top 5 features highly expressed by each cluster below.","029628c2":"<h1><center>Jane Street Market Prediction<\/center><\/h1>\n<h2><center>Advanced EDA - from perspectives of features-based clustering to features comparison between clusters<\/center><\/h2>\n\n<h3><center>2020-11-29<\/center><\/h3>\n\n<h4><center>We have a fun interesting lung-shape UMAP visualization! And it looks like coronavirus infections at left and right lungs ;)<\/center><\/h4>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h2 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='background:green; border:0; color:white' role=\"tab\" aria-controls=\"home\"><center>Quick navigation<\/center><\/h2>\n\n* [1. Package installations + import libraries + data loading](#1)\n* [2. Principle Component Analysis (features basis, disregard time-series)](#2)\n* [3. Leiden clustering on tsne\/umap dimensional reductions ](#3)\n* [4. Differential expressing features defining each cluster](#4)\n    \n##### Although you can navigate to the topic of interest, it is recommended to first read from top-down before navigating around.","19e59e0b":"<a id=\"2\"><\/a>\n<h2 style='background:green; border:0; color:white'><center>2. Principle Component Analysis (features basis, disregard time-series)<\/center><h2>\n\n#### To reduce the dimensionality of the data by running PCA, which reveals the main axes of variation and denoises the data.\n    \n#### Note: I didn't do any data transformation or scaling for the feature data. Usually, it works the best to subject the features to minmax normalization, log2(x+1) Transformation and Standardardization Scaling before proceeding to PCA analysis. From what the features description above, it reads that the organizer may have done that for us. ","d0e93664":"Here, it seems that the PC1 and 2 are somehow useful to separate high and low values in the Y that we are looking at.\n\nLet us inspect the contribution of single PCs to the total variance in the data. This gives us information about how many PCs we should consider in order to compute the neighborhood relations or further dimension reductions of subjects, e.g. used in the clustering function qp.tl.leiden() or tSNE qp.tl.tsne(). In our experience, often, a rough estimate of the number of PCs does fine. The 'elbow' point seems to suggest at least up to PC5 will be useful to characterize the subjects. Here, we are going to do further dimensional reduction based on the first 5 PCs later. ","80dc4382":"### Features description\nI only briefly checking the usual min, max, mean, median, 25th, 75th percentage for rough estimation of the distribution patterns of the 130 features. This is useful for me to see if i need to do normalization or scaling later. \n\nFor detailed visualization for each feature, i recommend to visit https:\/\/www.kaggle.com\/blurredmachine\/jane-street-market-eda-viz-prediction produced by blurredmachine.;)","1fdd43b8":"The plots on resp_1, 2, 3, 4 and resp below show that high resp values usually seems to mostly appear in certain parts of the one of the lungs. Interestingly, The connecting part of both lung lobes, can expressed both high and low resp values. Fun!","1890d107":"Great! We have a lung shape UMAP plots here! Don't you think it looks like some kinds of coronavirus infection CT scan images, left and right lungs infected! haha\n\nAnyway, here, we set the max value for the weight as 75 in the plot visualization, according to the extreme outliers detection plot above.\nWe can roughly see that most of the high weight subjects are clustered within or near the middle clusters of the 2 Big Clusters.\n","730863e3":"From the above, we can see the features 0, 17, 18, 19, 20, 21, 22, 23, ..., 31, 32, 33, 37, 38, 39, 40 potentially can split the subjects into 2 main clades. ","f9ef9354":"## Checking extreme outliers values in the Y values\nThe y's included weights, resp, resp_1, resp_2, resp_3, resp_4. This is going to be useful to set the min and max values for plot visualization later.","616c65c5":"<a id=\"1\"><\/a>\n<h2 style='background:green; border:0; color:white'><center>1. Package installations + import libraries + data loading<\/center><h2>\n\n### Packages\nIf you wish to replicate this work, you'll need to excecute the following cell to install the quanp package (https:\/\/quanp.readthedocs.io\/en\/latest\/installation.html) that should install all necessary packages\/libraries required to execute the codes in this tutorial. ","18f2cbd2":"### Uniform Manifold Approximation and Projection (UMAP) - Embedding the neighborhood graph\nWe can also embed the neighborhood graph in 2 dimensions using UMAP (McInnes et al., 2018), see below. It is potentially more faithful to the global connectivity of the manifold than tSNE. Before running the UMAP, we compute the correlations between clusters as initiating positions for the UMAP.","b0d29f3d":"<a id=\"3\"><\/a>\n<h2 style='background:green; border:0; color:white'><center>3. Leiden clustering on tsne\/umap dimensional reductions<\/center><h2>\n\n#### We use Leiden graph-clustering method (community detection based on optimizing modularity) by Traag et al. (2018) to cluster the neighborhood graph of companies. We are going to first compute the neighborhood graph for this. We  compute the neighborhood graph of subjects using the PCA representation of the data matrix. This will give rise to distances and connectivities with other subjects for each subject. Here, we consider 30 nearest neighbors with 5 PCs derived from the PCA. Afterward, we wil compute the leiden clustering.","9709ed4d":"<a id=\"7\"><\/a>\n<h2 style='background:green; border:0; color:white'><center>4. Differential expressing viability defining each cluster<\/center><h2>\n    \n### Visualizing the differential expressing features defining each cluster\nWe can identify features that are differentially expressed by each cluster or group. Here, we take each group of subjects and compare the distribution of each feature in a group against the distribution in all other subjects not in the group. Here, we list the top 30 features defining each cluster (P-value <0.05 as long as the score > +\/-1.96).\n","c4896b96":"# References:\n\n 1. https:\/\/www.kaggle.com\/blurredmachine\/jane-street-market-eda-viz-prediction\n 2. Tabachnick & Fidell. Using Multivariate Statistics, Sixth Edition. PEARSON 2013; ISBN-13:9780205956227.\n 2. Traag et al., From Louvain to Leiden: guaranteeing well-connected communities. Sci Rep. 2019;9(1):5233.\n 3. McInnes & Healy, UMAP: Uniform Manifold Approximation and Projection for Dimension Reduction, arXiv. 2018.\n 4. https:\/\/quanp.readthedocs.io\/en\/latest\/"}}