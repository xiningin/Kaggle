{"cell_type":{"404821e1":"code","220ea3e7":"code","91af1d7d":"code","66219262":"code","f1b32979":"code","10773afb":"code","ed49bee1":"code","68b11f4c":"code","8d4d5d98":"code","35064508":"code","0f084bd8":"code","6f8d321a":"code","abb550ce":"code","c67f82d2":"code","8e92ad9f":"code","e648c2b1":"code","064fd4bd":"code","7bb1e89e":"code","f1eb7958":"code","758035d1":"code","a385f3df":"code","6f2c7380":"code","0d75881e":"markdown","dfe3ed61":"markdown","ee124f2d":"markdown","df8b6186":"markdown","4eccaa33":"markdown","5cc7ca1e":"markdown","87d6048c":"markdown","6f102c71":"markdown","6cd01736":"markdown","4afde7d5":"markdown"},"source":{"404821e1":"!pip install gdown","220ea3e7":"!gdown --id 1HeoN37LFw8lvCReK7Zfta-1tyiSFs6GO","91af1d7d":"from kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nsecret_value_0 = user_secrets.get_secret(\"HIU-DMTL\")","66219262":"!unzip -qo -P {secret_value_0} .\/hiu_dmtl_data.zip","f1b32979":"!ls .\/to_cv_community\/*.json | wc -l\n!ls .\/to_cv_community\/*mask.png | wc -l","10773afb":"# remove problematic pairs\n!rm .\/to_cv_community\/rovmjagxhz.{png,json}","ed49bee1":"!mkdir {imgs,masks,json}","68b11f4c":"!pip install imutils\n!pip install imagesize","8d4d5d98":"from tqdm.auto import tqdm # i like progressbars\nimport os\nimport json\nfrom imutils import paths\nimport shutil\nfrom PIL import Image\nimport imagesize # fast for reading imagesize\n\nimport numpy as np","35064508":"DATASET_PATH = '.\/to_cv_community'","0f084bd8":"# !ls {DATASET_PATH}","6f8d321a":"TWO_HANDS = 0\n\ndef get_relevant_keypoints_info(json_file_path):\n    with open(json_file_path, 'r') as f:\n        contents = json.load(f)\n        \n    flags = contents['hand_type']\n    keypoints = contents['pts2d_2hand']\n    \n    if flags == [1, 1]:\n        print('Eureka!!!')\n        TWO_HANDS += 1\n    elif flags == [0, 1]: # right hand\n        keypoints = keypoints[21:]\n    elif flags == [1, 0]: # left hand\n        keypoints = keypoints[:21]\n    else:\n        print(\"no hands?\")\n    \n    return keypoints\n    \n# get_relevant_keypoints_info('.\/to_cv_community\/abcqzldoej.json')","abb550ce":"def transform_points(new_dims, old_dims, point):\n    \"\"\"\n    Scale the given coordinates to a new set of dimensions\n    \"\"\"\n    h_new, w_new = new_dims\n    h_old, w_old = old_dims\n    return [point[0]*(w_new\/w_old), point[1]*(h_new\/h_old)]","c67f82d2":"def resize_inputs(img_size, img_path, mask_path, json_data):\n    # note that img_size must be a tuple\n    img_open = Image.open(img_path)\n    mask_open = Image.open(mask_path)\n    scaled_keypts = list(map(\n                lambda keypt: transform_points(img_size, img_open.size, keypt), json_data\n             ))\n    return img_open.resize(img_size), mask_open.resize(img_size), scaled_keypts","8e92ad9f":"standard_img_size = (256, 256)\n\ndef process_images(img_loc, out_loc, process = None):\n    mask_savepath = os.path.join(out_loc + 'masks')\n    img_savepath = os.path.join(out_loc + 'imgs')\n    json_savepath = os.path.join(out_loc + 'json')\n    \n    for img_path in tqdm(paths.list_images(img_loc)):\n        if \"mask\" in img_path:\n            continue # skip over mask files\n            \n        # TODO: add process step here\n        \n        folder, fname = img_path.split('\/')[-2:]\n        no_ext_fpath = os.path.splitext(img_path)[0]\n        json_file_path = no_ext_fpath + '.json'\n        mask_file_path = no_ext_fpath + '_mask.png'\n        \n        new_json_data = get_relevant_keypoints_info(json_file_path)\n        new_json_path = os.path.join(json_savepath, os.path.basename(json_file_path))\n        \n        new_mask_path = os.path.join(mask_savepath, os.path.basename(mask_file_path))\n        \n        if process is None and imagesize.get(img_path) == standard_img_size: # faster than using PIL's size\n            # move the file to appropriate folder\n            shutil.copy(img_path, os.path.join(img_savepath, fname))\n            shutil.copy(mask_file_path, new_mask_path)\n        else:\n            ret_img, ret_mask, new_json_data = resize_inputs(\n                                                        standard_img_size, \n                                                        img_path, \n                                                        mask_file_path, \n                                                        new_json_data\n                                                            )\n            ret_img.save(os.path.join(img_savepath, fname))\n            ret_mask.save(new_mask_path)\n\n        with open(new_json_path, 'w') as f:\n            json.dump(new_json_data, f)","e648c2b1":"process_images(DATASET_PATH, '.\/')","064fd4bd":"!find .\/imgs | wc -l\n!find .\/masks | wc -l\n!find .\/json | wc -l","7bb1e89e":"NEW_IMGS_PATH = \".\/imgs\"\nNEW_MASKS_PATH = \".\/masks\"\nNEW_JSON_PATH = \".\/json\"\n\n# generate a list of all filepaths\nIMG_PATH_LIST = list(paths.list_images(NEW_IMGS_PATH))\n# randomly select 8 filepaths\nselected_imagepaths = list(np.random.choice(IMG_PATH_LIST, size = 8))\n# get corresponding maskpaths\nselected_maskpaths = list(map(lambda x: os.path.join(NEW_MASKS_PATH, os.path.basename(x)[:-4] + \"_mask.png\"), selected_imagepaths))\nselected_jsonpaths = list(map(lambda x: os.path.join(NEW_JSON_PATH, os.path.basename(x)[:-4] + \".json\"), selected_imagepaths))","f1eb7958":"POSE_PAIRS = [ [0, 1], [1, 2], [2, 3], [3, 4], [0, 5], [5, 6], [6, 7], [7, 8], [0, 9],\n               [9, 10], [10, 11], [11, 12], [0, 13], [13, 14], [14, 15], [15, 16], [0, 17], \n               [17, 18], [18, 19], [19, 20] ]\n\ndef draw_keypoints(json_path, img):\n    with open(json_path, 'r') as f:\n        keypoints = np.array(json.load(f)).astype(np.int)\n    \n    # TODO: add a transfrom step if needed (for arbitrary image size)\n    \n    for pair in keypoints:\n        cv2.circle(img, pair, 2, (0, 255, 255), thickness=-1, lineType=cv2.FILLED)\n\n    for pair in POSE_PAIRS:\n        start = pair[0]\n        end = pair[1]\n\n        cv2.line(img, keypoints[start], keypoints[end], (0, 0, 255), 1)\n    return img","758035d1":"import matplotlib.pyplot as plt\nimport cv2\n\nf, axs = plt.subplots(4, 4, figsize=(16, 16))\nfor num, (img_p, mask_p, json_p) in enumerate(zip(selected_imagepaths, selected_maskpaths, selected_jsonpaths)):\n    row = num \/\/ 2\n    col = (num % 2) * 2\n    \n    imgr = cv2.imread(img_p)\n    imgr = cv2.cvtColor(imgr, cv2.COLOR_BGR2RGB)\n    imgr_with_kp = draw_keypoints(json_p, imgr)\n    \n    axs[row, col].imshow(imgr_with_kp)\n    axs[row, col].set_xlabel(imgr.shape)\n    \n    maskr = cv2.imread(mask_p, 0)\n    axs[row, col + 1].imshow(np.array(maskr), cmap = \"gray\")\n    axs[row, col + 1].set_xlabel(maskr.shape)\nf.show()","a385f3df":"zip_name = 'HIU_DMTL_dataset_processed.zip' \n!zip -qr {zip_name} .\/imgs .\/masks .\/json","6f2c7380":"# clean up residue files\n!find .\/ -mindepth 1 -not -name {zip_name} -delete","0d75881e":"~~**Conclusion**: All image have valid json information and are of the standard size `256x256`~~\n\n**Update**: At a later stage when I was creating tfrecords, I found out that there exists image files of dimensions other than `256x256` (`512x512` and `511x511`) and therefore, I will be including a function in the processing pipeline that will resize the images to a standard size and scale the keypoints accordingly.","dfe3ed61":"## some house keeping","ee124f2d":"## download the dataset\nThe dataset is hosted on `gdrive` and `baidu-netdisk`. I will be downloading it from gdrive using `gdown`.\n\n[gdrive link](https:\/\/drive.google.com\/file\/d\/1HeoN37LFw8lvCReK7Zfta-1tyiSFs6GO\/view?usp=sharing)\n\nSource: [github repo](https:\/\/github.com\/MandyMo\/HIU-DMTL\/)","df8b6186":"## process the dataset","4eccaa33":"## examine dataset","5cc7ca1e":"## extract the dataset","87d6048c":"As mentioned in [the github README](https:\/\/github.com\/MandyMo\/HIU-DMTL\/#the-new-dataset), the dataset is zipped and requires a password for extraction. I obtained the password by mailing the author, if you wish to make use of the dataset, you shall have to mail him and obtain the password from him.","6f102c71":"## zip the final files","6cd01736":"The dataset is already well-formatted but i am going to separate the images, masks and json annotations into separate folder.\n\nOn the complex side, I noticed that each json file has a lot of information that we don't need, specially I noticed that the 2d keypoints (index 0) are listed as json objects that have 42 elements, the first 21 are for the left hand while the other 21 are for the right hand (or other way). Most images to my knowledge have only one arm (to be verified) and this is mentioned in the index -2 where a list element specifies which hand is present. My goal is to use this element and extract only a list of 21 keypoints that are present in the given image.\n\n~~On the way, I will verify if all the images have only one arm each.~~ DONE: there are no such files, all images have one arm only.","4afde7d5":"While running this function, I found that the file `'.\/to_cv_community\/rovmjagxhz_mask.png'` doesn't exist so I decided to remove the associated files for it as well."}}