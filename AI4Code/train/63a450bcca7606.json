{"cell_type":{"ebbb8651":"code","9ea0dce3":"code","1e309d35":"code","07eaf250":"code","efe8bfdc":"code","f4b93ebd":"code","68425319":"code","23edff35":"code","c0e4b7c2":"code","ba36a85e":"code","bc597119":"code","bc953622":"code","52737bf9":"code","bb05f174":"code","d4688e88":"code","08a77606":"code","966d2daa":"code","b0e698a8":"code","604f88b0":"code","8e352a9d":"code","7fb4e084":"code","810bee7c":"code","27224c8a":"code","88f41077":"code","73286cb4":"code","ba242eeb":"code","5c27ea5e":"code","0a517fdc":"code","db9d116a":"code","eeb7db2c":"code","c04b95f8":"code","6dee14bb":"code","872bca16":"code","361f19d4":"code","05d32314":"code","b18f4d90":"code","20d86b5f":"code","9569be94":"code","4887bfa9":"code","95406dfb":"code","c7eb386a":"code","294aa749":"code","ab051d8c":"code","d4892aee":"code","be7f1af2":"code","d8d8c659":"code","99b4c8df":"code","d677a8e4":"code","acdf57e1":"code","b51cb8a4":"code","ce65ae53":"markdown","a0d33715":"markdown","0e1b7013":"markdown","600cdf64":"markdown"},"source":{"ebbb8651":"#heavily borrwed from avaiable code","9ea0dce3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\/whats-cooking-kernels-only\"))\nprint(os.listdir(\"..\/input\/svm-12jul18\"))\nprint(os.listdir(\"..\/input\/let-s-cook-model\"))\n# Any results you write to the current directory are saved as output.\n# Import the required libraries \nrandom_state = None\nimport time\nstarttime = time.monotonic()\nfrom lightgbm import LGBMClassifier\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.multiclass import OneVsRestClassifier\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score\nfrom scipy.sparse import csr_matrix\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV, StratifiedShuffleSplit\nfrom scipy.sparse import hstack, csr_matrix\nimport numpy as np\nfrom sklearn.preprocessing import StandardScaler\nimport gc\nimport pandas as pd\nimport json\nimport pdb\nfrom sklearn.model_selection import KFold, StratifiedKFold\n","1e309d35":"#get train\ntrain_df = pd.read_json('..\/input\/whats-cooking-kernels-only\/train.json')\ntrain_df.columns = ['target', 'id', 'ingredients']\ntrain_df[\"num_ingredients\"] = train_df['ingredients'].apply(lambda x: len(x))\ntrain_df = train_df[ (train_df['num_ingredients'] > 1) ]\nprint(train_df.shape)\ntrain_df_start = train_df.shape[0]\ntrain_df.head()\n#make beck up of target\ny_bk = list(train_df['target'].copy().values)","07eaf250":"#get test\ntest_df = pd.read_json('..\/input\/whats-cooking-kernels-only\/test.json')\ntest_df['cousine']=np.nan\ntest_df=test_df[['cousine','id','ingredients']]\ntest_df.columns = ['target', 'id', 'ingredients']\ntest_df[\"num_ingredients\"] = test_df['ingredients'].apply(lambda x: len(x))\nprint (test_df.shape)\ntest_df.head()","efe8bfdc":"#make beck up of test ids\ntest_ids_for_sub = test_df['id'].values\ntest_ids_for_sub","f4b93ebd":"print(\"Combine Train and Submission\")\ndf = pd.concat([train_df, test_df],axis=0,ignore_index=True)\ndel train_df, test_df\ngc.collect()\nprint (df.shape)\ndf.head()","68425319":"df.tail()","23edff35":"print (df.shape)\nprint(df.head())\nprint(df.tail())","c0e4b7c2":"#brutal copy paste from public kernel\nfrom nltk.stem import WordNetLemmatizer\nimport re\nlemmatizer = WordNetLemmatizer()\ndef preprocess(ingredients):\n    ingredients_text = ' '.join(ingredients)\n    ingredients_text = ingredients_text.lower()\n    ingredients_text = ingredients_text.replace('-', ' ')\n    words = []\n    for word in ingredients_text.split():\n        if re.findall('[0-9]', word): continue\n        if len(word) <= 2: continue\n        if '\u2019' in word: continue\n        word = lemmatizer.lemmatize(word)\n        if len(word) > 0: words.append(word)\n    return ' '.join(words)\n\nfor ingredient, expected in [\n    ('Eggs', 'egg'),\n    ('all-purpose flour', 'all purpose flour'),\n    ('pur\u00e9e', 'pur\u00e9e'),\n    ('1% low-fat milk', 'low fat milk'),\n    ('half & half', 'half half'),\n    ('safetida (powder)', 'safetida (powder)')\n]:\n    actual = preprocess([ingredient])\n    assert actual == expected, f'\"{expected}\" is excpected but got \"{actual}\"'","ba36a85e":"df['ingredients'] = df['ingredients'].apply(lambda ingredients: preprocess(ingredients))","bc597119":"df.head()","bc953622":"df.tail()","52737bf9":"# Feature Engineering \nprint (\"TF-IDF on text data ... \")\ntfidf = TfidfVectorizer(binary=True)\nprint(\"combine other features with text vectors\")\ntext_features = tfidf.fit_transform(df['ingredients'])\nprint (text_features.shape)\ndf_1 = pd.DataFrame(text_features.toarray())\ndel text_features\ngc.collect()\nprint (df_1.shape)\nprint (df.shape)\ndf_1.head()","bb05f174":"#less brutal copy paste from public kernel, we try to add a different ngram range\nfrom sklearn.pipeline import make_pipeline, make_union\nfrom sklearn.preprocessing import FunctionTransformer, LabelEncoder\nvectorizer = make_pipeline(\n    TfidfVectorizer(sublinear_tf=True, ngram_range =(2,4), max_features=2000),\n    FunctionTransformer(lambda x: x.astype('float32'), validate=False)\n)\ntext_features_2 = vectorizer.fit_transform(df['ingredients'].values)\ntext_features_2.shape\n\ndf_2 = pd.DataFrame(text_features_2.todense())\ndel text_features_2\ngc.collect()\nprint (df_2.shape)\nprint (df.shape)\ndf_2.head()","d4688e88":"#combine all toghete... possible because the dataset is relatievely small\ndf = pd.concat([df, df_1, df_2],axis=1)\ndel df_1\ndel df_2\ngc.collect()\nprint (df.shape)","08a77606":"df.head()","966d2daa":"df.tail()","b0e698a8":"#rename columns, just to know where they come from\ndf.columns =['target', 'id', 'ingredients', 'num_ingredients']+['TfidfV_'+str(n) for n in range(2867)]+['Ngram_'+str(n) for n in range(2000)]","604f88b0":"df.drop('ingredients',inplace=True,axis=True)\ndf.head()","8e352a9d":"df.tail()","7fb4e084":"#now this is wrong!\n#i didnt manage to make the multiclass to work \n#so i'm using a regression to find the best parameters\n\nfrom hyperopt import hp\nfrom hyperopt import tpe\nfrom hyperopt import STATUS_OK\nfrom hyperopt import Trials\nfrom hyperopt import fmin\nimport hyperopt.pyll.stochastic as st\nimport  lightgbm as lgb\nimport csv\nfrom sklearn.preprocessing import OneHotEncoder\ntrain_df = df[df['target'].notnull()]\n\ntarget = train_df['target']\nprint (\"Label Encode the Target Variable ... \")\nlb = LabelEncoder()\ny = lb.fit_transform(target)\nlen_y = len(set(y))\n\n'''\n#good when i will mange to make the multiclass working\n#https:\/\/stackoverflow.com\/questions\/51139150\/how-to-write-custom-f1-score-metric-in-light-gbm-python-in-multiclass-classifica\ndef evalerror(preds, dtrain):\n    labels = dtrain.get_label()\n    print (len(labels))\n    print(len(preds))\n    #preds = preds.reshape(-1, len_y)\n    #preds = preds.argmax(axis = 1)\n    f_score = f1_score(labels , preds,  average = 'weighted')\n    return 'f1_score', f_score, True\n'''\n\n\nbayes_trials = Trials()\ntpe_algorithm = tpe.suggest\n\nfinal_selection = [n for n in train_df.columns if n not in ['target', 'id']]\ntrain_set = lgb.Dataset(train_df[final_selection], y)\nprint (train_set)\nmin_value = 100000\niteration = 0\ndef objective(params, n_folds = 5):\n    global iteration\n    iteration+=1\n    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Tuning\"\"\"\n    #print (iteration)\n    # Perform n_fold cross validation with hyperparameters\n    # Use early stopping and evalute based on ROC AUC\n    params['num_leaves']=int(params['num_leaves'])\n    params['min_data_in_leaf']=int(params['min_data_in_leaf'])\n    params['max_depth']=int(params['max_depth'])\n    params['min_child_weight']=int(params['min_child_weight'])\n    params['colsample_bytree']=round(params['colsample_bytree'],5)\n    params['reg_alpha']=round(params['reg_alpha'],5)\n    params['reg_lambda']=round(params['reg_lambda'],5)\n    params['subsample']=round(params['subsample'],5)  \n    params['learning_rate']=round(params['learning_rate'],5)\n    params['learning_rate']=round(params['learning_rate'],5)\n    #params['objective']= 'multiclass'\n    #params['num_class']=len(train_df['target'].unique())\n    #params['subsample_for_bin']=int(params['subsample_for_bin'])\n    #params['is_unbalance']=True\n    \n    cv_results = lgb.cv(params, \n                        train_set, \n                        nfold = n_folds, \n                        num_boost_round = 10000, \n                        early_stopping_rounds = 200, \n                        metrics='multi_logloss',\n                        #feval=evalerror,\n                        seed = 50,\n                        stratified=False)\n    \n    # Extract the best score\n    #print(cv_results)\n    best_score = min(cv_results['multi_logloss-mean'])\n    optimal_rounds =len(cv_results['multi_logloss-mean']) \n    \n    \n    global min_value\n    if  str(best_score) == '-inf':\n        best_score = min_value+1\n        \n    if  best_score < 0:\n        best_score = min_value+1\n        \n    if best_score < min_value:\n        min_value = best_score\n    print (f'Round: {iteration}\\n the score is: {best_score:.3f}\\n the best score so far {min_value:.3f}')\n    \n    # Loss must be minimized\n    loss= best_score\n    of_connection = open(out_file, 'a')\n    writer = csv.writer(of_connection)\n    \n    writer.writerow([loss, params, best_score, optimal_rounds])\n    of_connection.close()\n    # Dictionary with information for evaluation\n    \n    return {'loss': loss, 'params': params, 'status': STATUS_OK}\n\n\nspace = {\n    'subsample': hp.uniform('subsample', 0.2, 1),\n    'num_leaves': hp.quniform('num_leaves', 5, 150, 1),\n    'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.9)),\n    'min_child_samples': hp.quniform('min_child_samples', 1, 100, 1),\n    'min_data_in_leaf': hp.quniform('min_data_in_leaf', 1, 100, 1),\n    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.2, 1.0),\n    'max_depth':hp.uniform('max_depth', 2, 15),\n    'min_child_weight':hp.quniform('min_child_weight', 1, 100, 1),\n    #'subsample_for_bin': hp.loguniform('subsample_for_bin', np.log(10), np.log(3000)),\n}\n\n# File to save first results\nout_file = 'gbm_trials_essential.csv'\nof_connection = open(out_file, 'w')\nwriter = csv.writer(of_connection)\n\n# Write the headers to the file\nwriter.writerow(['loss', 'params', 'best_score', 'optimal_rounds'])\nof_connection.close() \n      \nMAX_EVALS = 40\n# Optimize\nbest = fmin(fn = objective,\n            space = space, \n            algo = tpe.suggest, \n            max_evals = MAX_EVALS, \n            trials = bayes_trials)  \n#print (best)","810bee7c":"best","27224c8a":"params = {}\nparams['num_leaves']=int(best['num_leaves'])\nparams['min_data_in_leaf']=int(best['min_data_in_leaf'])\nparams['max_depth']=int(best['max_depth'])\nparams['min_child_weight']=int(best['min_child_weight'])\nparams['colsample_bytree']=round(best['colsample_bytree'],5)\nparams['reg_alpha']=round(best['reg_alpha'],5)\nparams['reg_lambda']=round(best['reg_lambda'],5)\nparams['subsample']=round(best['subsample'],5)  \nparams['learning_rate']=round(best['learning_rate'],5)\n","88f41077":"len(target.unique())","73286cb4":"#function to facilitate cv prediction of lightgbm model\ndef kfold_lightgbm(train_df, test_df, \n                   num_folds, lr = 0.02, \n                   stratified = True,  params={},\n                   n_estimators=5000000, early_stopping_rounds= 200):    \n    # Divide in training\/validation and test data\n    #print (test_df.head())\n    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n    #del df\n    gc.collect()\n    # Cross validation model\n    if stratified:\n        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1001)\n    else:\n        folds = KFold(n_splits= num_folds, shuffle=True, random_state=1001)\n    # Create arrays and dataframes to store results\n    nclasses = len(train_df['target'].unique())\n    oof_preds = np.zeros( (train_df.shape[0],nclasses))\n    sub_preds = np.zeros( (test_df.shape[0],nclasses))\n    feature_importance_df = pd.DataFrame()\n    feats = [f for f in train_df.columns if f not in ['target', 'id']]\n    \n    print ('len_feat', len(feats))\n    feature_importance_df['f']=feats\n    \n    \n    y = train_df['target']\n\n    train_df = train_df[feats]\n    \n    \n    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df, y)):\n        train_x, train_y = train_df.iloc[train_idx], y.iloc[train_idx]\n        valid_x, valid_y = train_df.iloc[valid_idx], y.iloc[valid_idx]\n\n        # LightGBM parameters found by Bayesian optimization\n        clf = LGBMClassifier(\n            nthread=10,\n            n_estimators=n_estimators,\n            silent=-1,\n            objective='multiclass',\n            num_leaves=params['num_leaves'],\n            min_data_in_leaf=params['min_data_in_leaf'],\n            max_depth=params['max_depth'],\n            min_child_weight=params['min_child_weight'],\n            colsample_bytree=params['colsample_bytree'],\n            reg_alpha=params['reg_alpha'],\n            reg_lambda=params['reg_lambda'],\n            subsample=params['subsample'],  \n            learning_rate=params['learning_rate']     \n        \n        )\n\n        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n             verbose= 100, early_stopping_rounds= early_stopping_rounds)\n\n        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)\n        preds = clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)\n        #print (preds)\n        sub_preds += preds\n        #[:, 1] \/ float(folds.n_splits)\n        #print('inside', len(feats))\n        #print ('len clf.feature_importances_', len(clf.feature_importances_))\n        feature_importance_df[\"fold\"+str(n_fold)] = clf.feature_importances_\n\n        del train_x, train_y, valid_x, valid_y\n        gc.collect()\n    \n    sub_preds = sub_preds\/float(folds.n_splits)\n    return feature_importance_df, clf, sub_preds, oof_preds","ba242eeb":"test_df = df[df['target'].isnull()]\ndel df\ngc.collect()\ntest_df.head()","5c27ea5e":"test_df.tail()","0a517fdc":"#### Pass the best parameters to a lightgbm model\n#### Get cv prediction and feature importance","db9d116a":"\nfeat_importance, clf, sub_preds, oof_preds = kfold_lightgbm(train_df, test_df, num_folds= 5, lr=0.01, \n                                 stratified= True, params=params,\n                                 #n_estimators=200, early_stopping_rounds= 2\n                                                                  )\nfeat_importance.to_csv('fimp_all.csv') ","eeb7db2c":"#### Again, Pass the best parameters to a lightgbm model after scramble of target\n#### Get cv prediction and feature importance","c04b95f8":"train_df['target'] = train_df['target'].sample(frac=1,random_state=1976).values\nfeat_importance_random, clf, sub_preds, oof_preds = kfold_lightgbm(train_df, test_df, num_folds= 5, lr=0.01, \n                                 stratified= True, params=params,\n                                 #n_estimators=200, early_stopping_rounds= 2\n                                                                  )\nfeat_importance_random.to_csv('fimp_all_random.csv') ","6dee14bb":"#### Select the best feature comparing the normal scores vs the scrambled score ","872bca16":"feat_importance.set_index('f',inplace=True)\nfeat_importance_random.set_index('f',inplace=True)","361f19d4":"from scipy import stats\nfrom statsmodels.sandbox.stats.multicomp import multipletests\nfrom tqdm import tqdm\n\nimportance = pd.DataFrame()\n#selection_df['f']=feat_importance.index.values\nstatistics = []\npvalues = []\nfor f in tqdm(feat_importance.index.values):\n    statistic, pvalue = stats.ks_2samp(feat_importance.loc[f], feat_importance_random.loc[f])\n    statistics.append(statistic)\n    pvalues.append(pvalue)\nimportance['sum']=feat_importance.sum(axis=1).values\nimportance['random_sum']=feat_importance_random.sum(axis=1).values\nimportance['f']=feat_importance.index.values\nimportance.set_index('f',inplace=True)\n#just for some plotting, we add to the sum the minimum value of the random dataset\nimportance['sum']=importance['sum']+importance['random_sum'].min()\nimportance['random_sum']=importance['random_sum']+importance['random_sum'].min()\nimportance['fc']=(importance['sum']+1)\/(importance['random_sum']+1)\nimportance['pval']=pvalues\nimportance['ks_stat']=statistics\npadj = multipletests(importance['pval'], method='bonferroni')\nimportance['padj']=padj[1]\nimportance.sort_values('pval').head()    ","05d32314":"importance.sort_values('fc').tail(20)  ","b18f4d90":"importance.loc['num_ingredients']","20d86b5f":"np.log2(importance['fc']).plot(kind='hist')\nprint(importance[importance['padj']<0.05].shape)\nprint(importance[np.log2(importance['fc'])>1].shape)","9569be94":"feature_identified = list(importance[importance['fc']>1].index.values)\nprint (len(feature_identified))","4887bfa9":"train_set = lgb.Dataset(train_df[feature_identified], y)\nprint (train_set)\nmin_value = 100000\niteration = 0\ndef objective(params, n_folds = 5):\n    global iteration\n    iteration+=1\n    \"\"\"Objective function for Gradient Boosting Machine Hyperparameter Tuning\"\"\"\n    #print (iteration)\n    # Perform n_fold cross validation with hyperparameters\n    # Use early stopping and evalute based on ROC AUC\n    params['num_leaves']=int(params['num_leaves'])\n    params['min_data_in_leaf']=int(params['min_data_in_leaf'])\n    params['max_depth']=int(params['max_depth'])\n    params['min_child_weight']=int(params['min_child_weight'])\n    params['colsample_bytree']=round(params['colsample_bytree'],5)\n    params['reg_alpha']=round(params['reg_alpha'],5)\n    params['reg_lambda']=round(params['reg_lambda'],5)\n    params['subsample']=round(params['subsample'],5)  \n    params['learning_rate']=round(params['learning_rate'],5)\n    params['learning_rate']=round(params['learning_rate'],5)\n    #params['objective']= 'multiclass'\n    #params['num_class']=len(train_df['target'].unique())\n    #params['subsample_for_bin']=int(params['subsample_for_bin'])\n    #params['is_unbalance']=True\n    \n    cv_results = lgb.cv(params, \n                        train_set, \n                        nfold = n_folds, \n                        num_boost_round = 1000000, \n                        early_stopping_rounds = 200, \n                        metrics='multi_logloss',\n                        #feval=evalerror,\n                        seed = 50,\n                        stratified=False)\n    \n    # Extract the best score\n    #print(cv_results)\n    best_score = min(cv_results['multi_logloss-mean'])\n    optimal_rounds =len(cv_results['multi_logloss-mean']) \n    \n    \n    global min_value\n    if  str(best_score) == '-inf':\n        best_score = min_value+1 \n        \n    if  best_score < 0:\n        best_score = min_value+1        \n        \n    if best_score < min_value:\n        min_value = best_score\n    print (f'Round: {iteration}\\n the score is: {best_score:.3f}\\n the best score so far {min_value:.3f}')\n    \n    # Loss must be minimized\n    loss= best_score\n    of_connection = open(out_file, 'a')\n    writer = csv.writer(of_connection)\n    \n    writer.writerow([loss, params, best_score, optimal_rounds])\n    of_connection.close()\n    # Dictionary with information for evaluation\n    \n    return {'loss': loss, 'params': params, 'status': STATUS_OK}\n\n\nspace = {\n    'subsample': hp.uniform('subsample', 0.2, 1),\n    'num_leaves': hp.quniform('num_leaves', 5, 150, 1),\n    'learning_rate': hp.loguniform('learning_rate', np.log(0.001), np.log(0.9)),\n    'min_child_samples': hp.quniform('min_child_samples', 1, 100, 1),\n    'min_data_in_leaf': hp.quniform('min_data_in_leaf', 1, 100, 1),\n    'reg_alpha': hp.uniform('reg_alpha', 0.0, 1.0),\n    'reg_lambda': hp.uniform('reg_lambda', 0.0, 1.0),\n    'colsample_bytree': hp.uniform('colsample_bytree', 0.2, 1.0),\n    'max_depth':hp.uniform('max_depth', 2, 15),\n    'min_child_weight':hp.quniform('min_child_weight', 1, 100, 1),\n    #'subsample_for_bin': hp.loguniform('subsample_for_bin', np.log(10), np.log(3000)),\n}\n\n# File to save first results\nout_file = 'gbm_trials_essential.csv'\nof_connection = open(out_file, 'w')\nwriter = csv.writer(of_connection)\n\n# Write the headers to the file\nwriter.writerow(['loss', 'params', 'best_score', 'optimal_rounds'])\nof_connection.close() \n      \nMAX_EVALS = 150\n# Optimize\nbest = fmin(fn = objective,\n            space = space, \n            algo = tpe.suggest, \n            max_evals = MAX_EVALS, \n            trials = bayes_trials)  \nprint (best)","95406dfb":"params = {}\nparams['num_leaves']=int(best['num_leaves'])\nparams['min_data_in_leaf']=int(best['min_data_in_leaf'])\nparams['max_depth']=int(best['max_depth'])\nparams['min_child_weight']=int(best['min_child_weight'])\nparams['colsample_bytree']=round(best['colsample_bytree'],5)\nparams['reg_alpha']=round(best['reg_alpha'],5)\nparams['reg_lambda']=round(best['reg_lambda'],5)\nparams['subsample']=round(best['subsample'],5)  \nparams['learning_rate']=round(best['learning_rate'],5)","c7eb386a":"### run the final model, add back the non scarmbled target","294aa749":"train_df['target']=y_bk\nfeat_importance, clf, sub_preds, oof_preds = kfold_lightgbm(train_df[feature_identified+['target']], \n                                                            test_df[feature_identified+['target']],\n                                                            num_folds= 5, lr=0.01, stratified= True,params=params,\n                                                           #n_estimators=200, early_stopping_rounds= 2\n                                                           )\n                                 \n\nfeat_importance.to_csv('fimp_final.csv') ","ab051d8c":"print(clf.classes_)","d4892aee":"sub = pd.DataFrame()\nfor index,c in enumerate(clf.classes_):\n    sub[c]=sub_preds[:,index]\ncuisine = []\nfor item in sub.index.values:\n    cuisine.append(np.argmax(sub.loc[item]))\nsub['cuisine1']=cuisine\nsub['id']=test_ids_for_sub","be7f1af2":"### Blend with the top score models","d8d8c659":"temp_1 = pd.read_csv('..\/input\/svm-12jul18\/svm_output_None.csv')\ntemp_2 = pd.read_csv('..\/input\/let-s-cook-model\/submission.csv')\nsub['cuisine2']=temp_1['cuisine']\nsub['cuisine3']=temp_2['cuisine']","99b4c8df":"sub['diff'] = [1 if a != b else 0 for a,b in zip(sub['cuisine2'], sub['cuisine3'])]\nsub['diff'].value_counts()\nsub[sub['diff']==1][['cuisine1','cuisine2','cuisine3']].head(50)","d677a8e4":"cuisine = []\nfor index in sub.index.values:\n    temp = sub[['cuisine1', 'cuisine2', 'cuisine3']].iloc[index].value_counts()\n    temp_score = temp[0]\n    temp_res = temp.index.values[0]\n    if temp_score ==1:\n        temp_res = sub[['cuisine3']].iloc[index].values[0]\n    cuisine.append(temp_res)\nsub['cuisine']=cuisine\nsub[['id','cuisine']].head()","acdf57e1":"### et voila","b51cb8a4":"sub[['id','cuisine']].to_csv('finalsub.csv',index=False)","ce65ae53":"### Run again bayes optimization with the selected features","a0d33715":"### First round of optimization\n### In 50 rounds of bayes optimization we are trying to find the best parameters to run the lightgbm model\n","0e1b7013":"# Cook me\n## Gourmet guide to serve the best LightGBM model","600cdf64":"### Generally a thresold on fold change is enough\n### It is possible to explore a threshold on the p-value and p-value bonferroni adjusted of ks_2samp test"}}