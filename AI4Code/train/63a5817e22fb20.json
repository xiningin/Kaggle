{"cell_type":{"c5240afd":"code","81625c77":"code","079d2224":"code","bf56b4fc":"code","533d411f":"code","cf8b2d82":"code","672210b8":"code","c866d776":"code","d8448ae2":"code","fac4db44":"code","c8a966e5":"code","74e342ef":"code","c0cc9d15":"code","c9008bc2":"code","0fe6b197":"code","b0db8801":"code","e7106e31":"code","bf54d4f1":"code","19cca0ef":"code","5f4c4af3":"code","c7df5c2a":"markdown","5692b574":"markdown","08a48d5b":"markdown","3d15bd7e":"markdown","2fab2d4f":"markdown","92758d88":"markdown","a45fb87c":"markdown","ca4169fc":"markdown","36e60a85":"markdown","d654c857":"markdown","fb9df5cd":"markdown","e2b935db":"markdown","e4bf2cd8":"markdown","352cdc0c":"markdown","08560a42":"markdown","0dd22612":"markdown","2677385e":"markdown","a4e61d59":"markdown"},"source":{"c5240afd":"\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport numpy as np\n\nfrom sklearn.metrics import confusion_matrix, classification_report, accuracy_score\nimport itertools\n\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.preprocessing.text import Tokenizer\n\nfrom wordcloud import WordCloud\n\nimport tensorflow as tf\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.layers import Conv1D, Bidirectional, LSTM, Dense, Input, Dropout, SpatialDropout1D\n\nimport nltk\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\nimport re\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n        \n","81625c77":"df = pd.read_csv('..\/input\/sentiment140\/training.1600000.processed.noemoticon.csv',encoding = 'latin',header=None)\ndf.columns = ['sentiment', 'x1', 'x2', 'x3', 'x4', 'text']\ndf = df.drop(['x1', 'x2', 'x3', 'x4'], axis=1)\n\nprint(df)","079d2224":"#function to change labels to Positive or Negative\nConvert_Sentiment = {0: \"Negative\", 4: \"Positive\"}\ndef Sentiment_change(lcol):\n  return Convert_Sentiment[lcol]\ndf.sentiment = df.sentiment.apply(lambda x: Sentiment_change(x))","bf56b4fc":"#lets look at our distribution\nval_count = df.sentiment.value_counts()\nprint(val_count)","533d411f":"## Clean data. Typically stop words would be used. This study includes all words, excluding stop words in a 280 char tweet is too restrictive.\n## Remove words with no meaning, mentions and urls. \ncleaning = \"amp\\S+|quot\\S+|@\\S+|https?:\\S+|http?:\\S|[^A-Za-z0-9]\"\ndef preprocess(text):\n  text = re.sub(cleaning, ' ', str(text).lower()).strip()\n  tokens = []\n  for token in text.split():\n    tokens.append(token)\n  return \" \".join(tokens)\ndf.text = df.text.apply(lambda x: preprocess(x))","cf8b2d82":"plt.figure(figsize=(20, 20))\ncloud = WordCloud(max_words=1000, width=1000, height=800)\ncloud.generate(\" \".join(df[df.sentiment == 'Positive'].text))\nplt.imshow(cloud, interpolation='bilinear')\nplt.show()","672210b8":"plt.figure(figsize=(20, 20))\ncloud = WordCloud(max_words=1000, width=1000, height=800)\ncloud.generate(\" \".join(df[df.sentiment == 'Negative'].text))\nplt.imshow(cloud, interpolation='bilinear')\nplt.show()","c866d776":"#setup for training and testing splits. \nTraining_percent = 0.8\nMAX_SEQUENCE_LENGTH = 30\ntrain_data, test_data = train_test_split(df, test_size=1-Training_percent, random_state=7) ","d8448ae2":"tokens = Tokenizer()\ntokens.fit_on_texts(train_data.text)\nword_index = tokens.word_index","fac4db44":"#pesky +1 to account for 0 starting.\nvocab_size = len(tokens.word_index) + 1\nprint(\"# of words :\", vocab_size)\nx_train = pad_sequences(tokens.texts_to_sequences(train_data.text),maxlen=MAX_SEQUENCE_LENGTH)\nx_test = pad_sequences(tokens.texts_to_sequences(test_data.text), maxlen=MAX_SEQUENCE_LENGTH)\n\nlabels = train_data.sentiment.unique().tolist()\n\nencoder = LabelEncoder()\nencoder.fit(train_data.sentiment.to_list())","c8a966e5":"#Create y-train\ny_train = encoder.transform(train_data.sentiment.to_list())\ny_test = encoder.transform(test_data.sentiment.to_list())\ny_train = y_train.reshape(-1, 1)\ny_test = y_test.reshape(-1, 1)\n\nprint(\"Y Train shape: \",y_train.shape)\nprint(\"Y Test shape: \",y_test.shape)","74e342ef":"GLOVE_EMB = '..\/input\/glove6b300dtxt\/glove.6B.300d.txt'\nEMBEDDING_DIM = 300\nLR = .005\nBATCH_SIZE = 2048\nEPOCHS = 15","c0cc9d15":"#start embedding, open glove and resolve total vectors\nembed_index = {}\nf = open(GLOVE_EMB, encoding=\"utf-8\")\nfor line in f:\n  values = line.split()\n  word = values[0]\n  vectors = np.asarray(values[1:], dtype='float32')\n  embed_index[word] = vectors\nf.close()\nprint('Word vectors: ',len(embed_index))","c9008bc2":"#create the matrix and fill with 0s, size of total vocab words and the embeded dimentions from the Glove file used. \nemb_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n\n#get each word, get corresponding glove value, write if value exists to the embedding matrix\nfor word, i in word_index.items():\n  embedding_vector = embed_index.get(word)\n  if embedding_vector is not None:\n    emb_matrix[i] = embedding_vector","0fe6b197":"#create the embedding layer with given dimensions, vectors. Trainable false is used, if using trainable = true the model will over fit significantly\nembedding_layer = tf.keras.layers.Embedding(vocab_size,EMBEDDING_DIM,weights=[emb_matrix],input_length=MAX_SEQUENCE_LENGTH,trainable=False)","b0db8801":"#sequence creation. \nsequence_input = Input(shape=(MAX_SEQUENCE_LENGTH,), dtype='int32')\nembedding_sequences = embedding_layer(sequence_input)\nx = SpatialDropout1D(0.2)(embedding_sequences)\nx = Conv1D(64, 5, activation='relu')(x)\nx = Bidirectional(LSTM(64, dropout=0.1, recurrent_dropout=0.1))(x)\nx = Dense(512, activation='relu')(x)\noutputs = Dense(1, activation='sigmoid')(x)\n\nmodel = tf.keras.Model(sequence_input, outputs)\nmodel.compile(optimizer=Adam(learning_rate=LR), loss='binary_crossentropy',metrics=['accuracy'])\nprint(model.summary())","e7106e31":"#Here we go\nhistory = model.fit(x_train, y_train, batch_size=BATCH_SIZE, epochs=EPOCHS, validation_data=(x_test, y_test))","bf54d4f1":"s, (at, al) = plt.subplots(2, 1)\nat.plot(history.history['accuracy'], c='b')\nat.plot(history.history['val_accuracy'], c='r')\nat.set_title('model accuracy')\nat.set_ylabel('accuracy')\nat.set_xlabel('epoch')\nat.legend(['LSTM_train', 'LSTM_val'], loc='upper left')\n\nal.plot(history.history['loss'], c='m')\nal.plot(history.history['val_loss'], c='c')\nal.set_title('model loss')\nal.set_ylabel('loss')\nal.set_xlabel('epoch')\nal.legend(['train', 'val'], loc='upper left')\nplt.show()","19cca0ef":"def decode_sentiment(score):\n    return \"Positive\" if score > 0.5 else \"Negative\"\n\n\nscores = model.predict(x_test, verbose=1, batch_size=10000)\ny_pred_1d = [decode_sentiment(score) for score in scores]","5f4c4af3":"def plot_confusion_matrix(cm, classes,title='Confusion matrix',cmap=plt.cm.Blues):\n    cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n\n    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n    plt.title(title, fontsize=20)\n    plt.colorbar()\n    tick_marks = np.arange(len(classes))\n    plt.xticks(tick_marks, classes, fontsize=13)\n    plt.yticks(tick_marks, classes, fontsize=13)\n\n    fmt = '.2f'\n    thresh = cm.max() \/ 2.\n    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n        plt.text(j, i, format(cm[i, j], fmt),\n                 horizontalalignment=\"center\",\n                 color=\"white\" if cm[i, j] > thresh else \"black\")\n\n    plt.ylabel('True label', fontsize=17)\n    plt.xlabel('Predicted label', fontsize=17)\n    \n\ncnf_matrix = confusion_matrix(test_data.sentiment.to_list(), y_pred_1d)\nplt.figure(figsize=(6, 6))\nplot_confusion_matrix(cnf_matrix, classes=test_data.sentiment.unique(), title=\"Confusion matrix\")\nplt.show()\n\nprint(classification_report(list(test_data.sentiment), y_pred_1d))\n\nmodel.save_weights(\"model.h5\")","c7df5c2a":"I picked too many Epochs, 10 would have been enough. The val_accuracy fluctuated a bit after 10 epochs but wasn't sustained improvement. ","5692b574":"**Workable data**\n4's and 0's are not very ML friendly. Since the data does not include neutral sentiment, or anything in between lets just label them Negative and Positive.","08a48d5b":"# Is anyone an expert in this area?\nI am using the LSTM model, with pretty standard settings. Most models seem to have additional layers, but I found them not to be helpful in producing an accurate model.","3d15bd7e":"Creating our Y-Train, reshaping, and taking a peek to make sure we are on the right track. ","2fab2d4f":"# Now - stop word\nDo you see the \"now\". thats a big one for Negative tweets and not for Positive ones, and it just happens to be a stop word. ","92758d88":"**80\/20 as is the tradition**","a45fb87c":"**Get a visual, make sure they look different**","ca4169fc":"**Confusion Matrix**\n\nHere we can see where the model got things right and wrong. Very close to even on Positive and Negative with a slightly better job at spotting Positive tweets.\n\n# over all about an 83%\n\nWhen computed with stopwords, all else being equal, the model showed 79%. ","36e60a85":"**Lazy or Smart?** \nThis was an exercise in creating a lighter, stripped down ML, asking the question why DS projects often follow the same script. I took out what is usually considered important steps, objects, ideals, and tested to see if there was a need.\n\nThe most important take away: **Stopwords** need to be taken into account with respect to the target data. It might make sense for articles, books, paragraphs. But for tweets? Turns out perhaps not.\n\n","d654c857":"**TOKENS**\n\nEach tweet is going to contain multiple words, here we basically split the tweet into its individual words so we can further process.","fb9df5cd":"# GPU and Coffee time\n![](https:\/\/imgs.xkcd.com\/comics\/compiling.png)","e2b935db":"**Cleaning up the data** \nTypically you would stem or lemmatize, remove links, mentions, and stop words. Call it a day and your data is clean. However with tweets, our data is different. We dont have full sentences, or even coherent thoughts sometimes. \n\nStop words: A list of words commonly used which have little meaning. Well, it does have meaning, and if the entire context of a Positive or Negative tweet is derived from 280 chars, we are going to leave the stop words in. \n\nTwitter specific spam removed: HTTPS, HTTP, @mentions, AMP(mobile page text), and quot (meaningless)\n\n![](https:\/\/www.kdnuggets.com\/images\/cartoon-machine-learning-class.jpg)","e4bf2cd8":"# What is the right number?\n\nWe are at the point where I feel like pathways really open up, we will be using GLOVE, but there are other options. We are using ADAM but there are other options. What is going to be your Learning Rate, how many Epochs are going to be run, will something as simple as batch size change our model drastically?\n\n\n![](https:\/\/i.imgflip.com\/1j9mml.jpg)","352cdc0c":"Manipulating our train and tests, filled with tokens and padded to ensure identical shapes","08560a42":"# Trainable is FALSE.\n\nWhen trying different combinations of settings, trainable = true led to an extremely over fit model, which also took 5x as long to run. Don't do it. \n\n# No callbacks.\nThis was tested, and including callbacks did not result in a better model. I am choosing to exclude callbacks and keep the model cleaner.","0dd22612":"**Distribution** \nIn the real world we are given data that is usually highly skewed. We were given a gift here, 50\/50 and no need to change samples.","2677385e":"**Load and Drop** \nHere we load our tweets into our dataframe. Not even going to give the columns proper names before we drop them. In the end all we want is sentiment and text. Print a peek to make sure we are good to go.","a4e61d59":"<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">NLP with 1.6m tweets, positive and negative tweets, can we create some ML that can predict if a tweet is either? 280 chars with links, mentions, and ugly spellling. <a href=\"https:\/\/twitter.com\/hashtag\/letsgo?src=hash&amp;ref_src=twsrc%5Etfw\">#letsgo<\/a> <a href=\"https:\/\/twitter.com\/hashtag\/datascience?src=hash&amp;ref_src=twsrc%5Etfw\">#datascience<\/a> <a href=\"https:\/\/twitter.com\/hashtag\/ML?src=hash&amp;ref_src=twsrc%5Etfw\">#ML<\/a> <a href=\"https:\/\/twitter.com\/hashtag\/needmoreGPUs?src=hash&amp;ref_src=twsrc%5Etfw\">#needmoreGPUs<\/a> <a href=\"https:\/\/twitter.com\/hashtag\/bigdata?src=hash&amp;ref_src=twsrc%5Etfw\">#bigdata<\/a><\/p>&mdash; Jonathan Henson (@sciencetition) <a href=\"https:\/\/twitter.com\/sciencetition\/status\/1305008902212399108?ref_src=twsrc%5Etfw\">September 13, 2020<\/a><\/blockquote> <script async src=\"https:\/\/platform.twitter.com\/widgets.js\" charset=\"utf-8\"><\/script>"}}