{"cell_type":{"eddab013":"code","01e41c2d":"code","5c4dc979":"code","7d87dc4c":"code","7e29ee3e":"code","162723a9":"code","90238463":"code","6868ab22":"code","81bde0b2":"code","101da161":"code","7d9586e9":"code","aa908a75":"code","7216a901":"code","507e4437":"code","6cf4e922":"code","d8e67746":"code","3c729fb6":"code","6ae451d9":"code","12e0229a":"code","4ccdd7ad":"code","b2de101e":"code","1b98fb98":"code","c0a591d9":"code","636cb009":"code","38a0f487":"code","f7dbcc8b":"code","80541066":"code","cabb7c38":"markdown","97c8027a":"markdown","f622c2a9":"markdown","1eb30320":"markdown","96addeba":"markdown","7f5534f2":"markdown","03c1a1e6":"markdown","9ed552cf":"markdown","1bcbb390":"markdown","3fca8174":"markdown","d5002b68":"markdown","ff19fb73":"markdown","f0b90a8e":"markdown","e92e2a5a":"markdown","66e12dc5":"markdown","9bb3e509":"markdown","eeafb1be":"markdown","341e9de8":"markdown","89819d3a":"markdown","6d31c0c3":"markdown","701f12c8":"markdown","f566eb3c":"markdown","32a12ede":"markdown","b8027ee2":"markdown","9be0d655":"markdown","f4b6d58b":"markdown","6698ac6a":"markdown"},"source":{"eddab013":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import KFold, cross_val_score, StratifiedKFold\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom xgboost import XGBClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom mlxtend.classifier import StackingCVClassifier\n\nimport warnings\nwarnings.filterwarnings('ignore')","01e41c2d":"train = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')","5c4dc979":"# Store our passenger ID for easy access\nPassengerId = test['PassengerId']\ny_train = train['Survived'].reset_index(drop=True)\nX_train = train.drop(['Survived'], axis=1)\n# Combine train and test set\ndf = pd.concat((X_train, test)).reset_index(drop=True)","7d87dc4c":"sns.countplot(x = 'Survived', data = train)","7e29ee3e":"df.isnull().sum()","162723a9":"plt.subplots(figsize=(10,10))\nsns.set(font_scale=1.25)\nsns.heatmap(df.corr(),square=True,annot=True)","90238463":"age_by_pclass_sex = df.groupby(['Pclass', 'Sex']).median()['Age']\nage_by_pclass_sex","6868ab22":"df['Age'] = df.groupby(['Pclass','Sex'])['Age'].apply(lambda x: x.fillna(x.median()))","81bde0b2":"df['Fare'] = df.groupby(['Pclass'])['Fare'].apply(lambda x: x.fillna(x.median()))","101da161":"df[df['Embarked'].isnull()]","7d9586e9":"df['Embarked'] = df['Embarked'].fillna('S')","aa908a75":"df['title'] = df['Name'].str.split(', ', expand = True)[1].str.split('.',expand=True)[0]\ndf['title'].value_counts()","7216a901":"# combine titles\ndf['title'] = df['title'].replace(['Miss', 'Mrs','Ms', 'Mlle', 'Mme', 'Mr', 'Master'], 'Ordinary')\ndf['title'] = df['title'].replace(['Lady', 'the Countess', 'Dona', 'Dr', 'Col', 'Major', 'Jonkheer', 'Capt', 'Sir', 'Don', 'Rev'], 'Noble')","507e4437":"df['FamSize'] = df['SibSp'] + df['Parch'] + 1","6cf4e922":"sns.distplot(df['FamSize'])","d8e67746":"df['IsAlone'] = df['FamSize'].apply(lambda x: 1 if x==1 else 0)","3c729fb6":"df['has_cabin'] = df['Cabin'].apply(lambda x: 1 if type(x) == str else 0)","6ae451d9":"df.info()","12e0229a":"drop_cols = ['PassengerId', 'Name', 'Ticket','Cabin', 'SibSp', 'Parch']\ndf = df.drop(drop_cols, axis = 1)","4ccdd7ad":"df = pd.get_dummies(df).reset_index(drop=True)\n# Split train and test\nX_train = df.iloc[:len(y_train), :]\nX_test = df.iloc[len(y_train):, :]\nX_train.shape, X_test.shape, y_train.shape","b2de101e":"X_train = StandardScaler().fit_transform(X_train)\ny_train = y_train.values\nX_test = StandardScaler().fit_transform(X_test)","1b98fb98":"SEED = 42\n\n# random forest\nrf = RandomForestClassifier(criterion='gini', \n                           n_estimators=1700,\n                           max_depth=6,\n                           min_samples_split=6,\n                           min_samples_leaf=4,\n                           max_features='auto',\n                           random_state=SEED,\n                           n_jobs=-1,\n                           verbose=1)\n\n#logistic regression\nlr = LogisticRegression(penalty='l2', \n                        dual=False, \n                        tol=0.0001, \n                        C=1.0, \n                        fit_intercept=True, \n                        intercept_scaling=1,  \n                        random_state=SEED, \n                        solver='lbfgs', \n                        max_iter=100, \n                        multi_class='auto', \n                        verbose=0, \n                        n_jobs=-1)\n\n# support vector machine\nsvm = SVC(C=1, \n       kernel='rbf', \n       gamma='scale', \n       coef0=0.0, \n       cache_size=200, \n       class_weight=None, \n       verbose=False, \n       max_iter=-1, \n       decision_function_shape='ovr', \n       random_state=SEED,\n         probability=True)\n\n# gradient boosting\ngb = GradientBoostingClassifier(loss='deviance', \n                                learning_rate=0.2, \n                                n_estimators=100, \n                                criterion='friedman_mse', \n                                min_samples_split=2, \n                                min_samples_leaf=2, \n                                max_depth=3, \n                                random_state=SEED,  \n                                verbose=0)\n\n# decision tree\ndt = DecisionTreeClassifier(criterion='gini', \n                            max_depth = 4,\n                            min_samples_split=2, \n                            min_samples_leaf=1, \n                            random_state=SEED)\n\n# naive bayes\nnb = GaussianNB()\n\n# XGBoost\nxgb = XGBClassifier(learning_rate=0.01,\n                       n_estimators=6000,\n                       max_depth=4,\n                       min_child_weight=0,\n                       gamma=0.6,\n                       subsample=0.7,\n                       colsample_bytree=0.7,\n                       objective='binary:logistic',\n                       nthread=-1,\n                       scale_pos_weight=1,\n                       seed=242,\n                       reg_alpha=0.00006,\n                       random_state=42)\n\n# KNN\nknn = KNeighborsClassifier(n_neighbors=10, \n                           weights='uniform', \n                           algorithm='auto', \n                           leaf_size=30, \n                           p=2, \n                           metric='minkowski', \n                           metric_params=None, \n                           n_jobs=None)","c0a591d9":"models = [rf, lr, svm, gb, dt, nb, xgb, knn]\nscores = []\nfor model in models:\n    score = cross_val_score(estimator = model, X = X_train, y = y_train, cv = 5, scoring='roc_auc')\n    scores.append(score.mean())","636cb009":"names = ['lr', 'svm', 'gb', 'dt', 'nb','rf','xgb', 'knn']\ncv_score = pd.DataFrame(columns=['model', 'avg_cv_score'])\ncv_score['model'] = names\ncv_score['avg_cv_score'] = scores\ncv_score","38a0f487":"\"\"\"stack = StackingCVClassifier(classifiers = [rf, lr, dt, nb, knn, xgb, gb, svm],\n                            meta_classifier = lr,\n                            random_state = SEED)\naccuracies = cross_val_score(estimator = stack, X = X_train, y = y_train, cv = 5)\naccuracies.mean()\nstack_model = stack.fit(X_train, y_train)\nstack_model.predict(X_test)\"\"\"","f7dbcc8b":"best_model = svm.fit(X_train, y_train)\ny_pred = svm.predict(X_test)","80541066":"submission_df = pd.DataFrame(columns=['PassengerId', 'Survived'])\nsubmission_df['PassengerId'] = PassengerId\nsubmission_df['Survived'] = y_pred\nsubmission_df.to_csv('submissions.csv', header=True, index=False)","cabb7c38":"### Embarked","97c8027a":"Even decision tree has the highest auc score. SVM did a better job on final prediction.","f622c2a9":"## 2.2 Cross validation","1eb30320":"### Age","96addeba":"### If the passenger was alone","7f5534f2":"###  Family size\nNumber of siblings and number of parents can be combined as fimily size.","03c1a1e6":"## 1.4 Feature selection","9ed552cf":"# Introduction","1bcbb390":"## 1.3 Creating new features","3fca8174":"# Import libraries","d5002b68":"## 2.1 Model setup","ff19fb73":"Firstly, missing values in Age column should be filled up. Using median age is not a good idea. Let's first take a look at correlations between age and other variables.","f0b90a8e":"## 1.5 Enocding catergorical features","e92e2a5a":"# Import dataset","66e12dc5":"### Fare","9bb3e509":"The highest correlation with age is 'Pclass'. Next the data is grouped by Pclass and then filled up with median value of age. Sex feature is used as the second level of groupby.","eeafb1be":"# 3 Final prediction and submission","341e9de8":"## 1.2 Missing values\nThere are four features with missing values.","89819d3a":"### If had Cabin or not","6d31c0c3":"* This notebook only did some minor feature engineering, including filling up missing values and created a few simple features.\n* This notebook explored several popular models.\n* Stacking does not help.\n* Even though decision tree scored higher in cross validation, SVM did a better job on final prediction.\n\nFor advanced feature engineering and hyperparameter tuning using grid search, and of course, higher accuracy, please refer to another [notebook](https:\/\/www.kaggle.com\/scpitt\/titanic-parameter-tuning-using-pipeline#Feature-Engineering\/). ","701f12c8":"# 2 Models","f566eb3c":"Several different models was fitted: \n* random forest\n* logistic regression\n* support vector machine\n* gradient boosting\n* decision tree\n* naive bayes\n* XGBoost\n* KNN","32a12ede":"## 1.1 Predicted feature","b8027ee2":"## 2.3 Stacking\nStacking does not help to increase the accuracy.","9be0d655":"Five-fold cross validation was set up and all models were trained. Their average accuracies are printed bellow","f4b6d58b":"# 1. Features exploration, engineering","6698ac6a":"### Title"}}