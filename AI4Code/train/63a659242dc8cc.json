{"cell_type":{"d5f1d2f1":"code","1e0eb071":"code","d748c7d4":"code","58f8887d":"code","aa39056b":"code","8525cade":"code","1eec7441":"code","4e262831":"code","43386941":"code","d7ceb06a":"code","12dd13fa":"code","e33daf81":"code","c868dc27":"code","7b744ef1":"code","ab1478da":"code","44a582ed":"code","87b7d391":"code","70cd0d29":"code","3dce1594":"code","b178be0b":"code","c68f6ec1":"code","8fc9b76c":"code","b72ec79a":"code","81d3d1b5":"code","6e57be07":"code","e7785618":"code","e7caef23":"markdown","c4e89a64":"markdown","27cfe52d":"markdown","a8ea8700":"markdown","b434e847":"markdown","6c3ffffe":"markdown","35f4731b":"markdown","96c71ef5":"markdown","2772beec":"markdown","26fbde57":"markdown","d649a562":"markdown","59f8a031":"markdown","f84830ad":"markdown","c8a30c37":"markdown","9ffff889":"markdown","a8e62c8b":"markdown","e87b30f3":"markdown","8a2c327c":"markdown"},"source":{"d5f1d2f1":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt # data visualisation\nimport seaborn as sns # data visualisation\nsns.set_style('darkgrid')\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","1e0eb071":"path = \"\/kaggle\/input\/factors-affecting-campus-placement\/Placement_Data_Full_Class.csv\"","d748c7d4":"data = pd.read_csv(path, index_col = \"sl_no\")\ndata.head()","58f8887d":"data.describe()","aa39056b":"for feature in data.columns:\n    print(\"There are {} null values for {} feature \".format( sum(data[feature].isnull()), feature ))","8525cade":"# So all the null values are because of the status feature. We don't need to replace it with something else.\ndata[data['status'] == 'Not Placed'].count()","1eec7441":"#  First of all, does the dataset has a balanced distribution over the placement?\nsns.countplot(x = 'status', hue = 'status', data = data)\nplt.show()","4e262831":"# Is there a relationship between score percentages and placement?\nscore_p_cols = ['ssc_p', 'hsc_p', 'mba_p', 'degree_p']\nscore_cols_descs = ['Secondary School', 'Higher Secondary School', 'Masters of Business Administration', 'Under-grad Degree']\nplt.figure(figsize = (12,12))\n\nfor s in range(len(score_p_cols)):\n    plt.subplot(2,2, s + 1)\n    plt.title(\"Graph 1.{}: \")\n    sns.boxplot(x = data['status'], y = data[score_p_cols[s]], data = data)\n    plt.title(\"Graph 1.{}: \".format(s) + score_cols_descs[s])\n    plt.ylabel(\"\")\nplt.show()","43386941":"# Are there any correlation between employment test scores and placement?\nsns.boxplot(x = 'status', y = 'etest_p', data = data)\nsns.swarmplot(x = 'status', y = 'etest_p', data = data, color = \".2\")\nplt.title('Graph 2: Employment Test Score Distribution by Status')\nplt.ylabel('')\nplt.show()\n\nprint(\"Placed students average score on employment test: {:.2f}\".format(data[data['status'] == 'Placed'].etest_p.mean()))\nprint(\"Placed students standard deviation on employment test: {:.2f}\".format(data[data['status'] == 'Placed'].etest_p.std()))\nprint(\"Not placed students average score on employment test: {:.2f}\".format(data[data['status'] == 'Not Placed'].etest_p.mean()))\nprint(\"Not placed students standard deviation on employment test: {:.2f}\".format(data[data['status'] == 'Not Placed'].etest_p.std()))","d7ceb06a":"sns.countplot(x = 'status', hue = 'workex', data = data)\nplt.title('Graph 3: Does having work experience is a strong factor for recruitment?')\nplt.show()\n\n# Let's look at some numbers\nw_workex = data[data['workex'] == 'Yes'].status.value_counts().values\nwo_workex = data[data['workex'] == 'No'].status.value_counts().values\nprint(\"Students with work experience:\\n Total: {} \\n Placed: {} \\n Not Placed: {}\".format(sum(w_workex),w_workex[0], w_workex[1]))\nprint(\"{:.2f}% of students with work experience is placed while {:.2f}% of them couldn't get placed\".format(w_workex[0]\/sum(w_workex) * 100, w_workex[1]\/sum(w_workex) * 100))\nprint(\"\\n Students without work experience:\\n Total: {} \\n Placed: {} \\n Not Placed: {}\".format(sum(wo_workex), wo_workex[0], wo_workex[1]))\nprint(\"{:.2f}% of students with work experience is placed while {:.2f}% of them couldn't get placed\".format(wo_workex[0]\/sum(wo_workex) * 100, wo_workex[1]\/sum(wo_workex) * 100))","12dd13fa":"# Does an undergrad degree is a factor for recruitment?\nsns.countplot(x = 'degree_t', hue = 'status', data = data)\nplt.title('Graph 4.0')","e33daf81":"sns.countplot(x = 'hsc_s', hue = 'status', data = data)\nplt.title('Graph 4.1')","c868dc27":"sns.countplot(x = 'specialisation', hue = 'status', data = data)\nplt.title('Graph 4.2')","7b744ef1":"# Is there any correlation between secondary and higher secondary education?\nsns.scatterplot(data['ssc_p'], data['hsc_p'])\nplt.title('Graph 5.0')","ab1478da":"# Let's look at the best fitting line\nsns.lmplot(x = 'ssc_p', y = 'hsc_p', data = data)\nplt.title('Graph 5.1')","44a582ed":"# drop the unnecessary features\nclassification_data = data.drop(['salary', 'gender', 'ssc_b', 'hsc_b'], axis = 1)\n# classification_data = data.drop(['salary'], axis = 1)\nclassification_data.head(10)","87b7d391":"# One-hot-encode the categorical data we have and drop the first column in order get rid of dummy variable trap\nclassification_data = pd.get_dummies(classification_data, drop_first = True)\nclassification_data.shape","70cd0d29":"# We should split the dataset to feature\/target subsets, then we'll split it to training and test set\n\n# features\nX = classification_data.values[:,:-1]\n# target is \"status\" column\ny = classification_data.values[:,-1]\n\n# Split the dataset into train and test sets\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size = .25, shuffle = True, random_state = 42)","3dce1594":"# Scale the data using StandardScaler \n# I'll just apply scaling to continuous variables.\nfrom sklearn.preprocessing import StandardScaler\ncontinuous_vars_train = X_train[:, :5]\ncontinuous_vars_test = X_test[:, :5]\n\nss = StandardScaler()\n\ncontinuous_vars_train = ss.fit_transform(continuous_vars_train)\ncontinuous_vars_test = ss.transform(continuous_vars_test)\n\nX_train[:, :5] = continuous_vars_train\nX_test[:, :5] = continuous_vars_test","b178be0b":"pd.DataFrame(X_train).describe()","c68f6ec1":"# Import the classification models \nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\n# I'll be testing the models using different metrics.\nfrom sklearn.metrics import accuracy_score, fbeta_score, average_precision_score, roc_auc_score, roc_curve\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import make_scorer\n# In order to do a proper test on the models, we should use KFold cross-validation\nfrom sklearn.model_selection import StratifiedKFold, KFold\n# After testing different algorithms, we need to optimize the algorithm further.\n# I'll try to do this by using Grid Search\nfrom sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n# Evaluation with cross validation offers more generalized results\nfrom sklearn.model_selection import cross_val_score","8fc9b76c":"# the most basic approach would be picking the majority class all the time. \nsimple_baseline = np.ones(len(y_test))\naccuracy_score(y_test, simple_baseline)","b72ec79a":"# Utility function to calculate the geometric mean of sensitivity (recall) and specificity (TNR)\ndef g_mean(y_true, y_preds):\n    cm = confusion_matrix(y_true, y_preds)\n    tn, fp, fn, tp = cm.ravel()\n    sensivity = tp \/ (tp + fn)\n    specifity = tn \/ (tn + fp)\n    g_mean_score = np.sqrt(sensivity * specifity)\n    return g_mean_score\ng_mean_scorer = make_scorer(g_mean)","81d3d1b5":"# We will store the models in baseline_models list and apply a somewhat brute-force approach\nbaseline_models = [LogisticRegression(), SVC(probability = True), \n                   KNeighborsClassifier(), GaussianNB(), \n                   DecisionTreeClassifier(), RandomForestClassifier()]\n\nbaseline_model_names = ['Logistic Regression', 'Support Vector Classifier', \n                        'K Nearest Neighbors', 'Gaussian Naive Bayes', \n                        'Decision Tree', 'Random Forest']\n\nmetric_names = ['Accuracy', 'F1 Score', 'G-Mean', 'ROC AUC Score']\n\n# I'll apply 5 fold cross-validation to get a better inspection about the models\nn_splits = 5\n\n# Store the results so we can compare the models\nresults = np.zeros((len(baseline_models), len(metric_names)))\n\nfor i in range(len(baseline_models)):\n    # Initialize an array to store the fold results\n    model_results = np.zeros((n_splits, len(metric_names)))\n    \n    # Initialize the StratifiedKFold object with 5 splits.    \n    skf = KFold(n_splits)\n    skf.get_n_splits(X_train, y_train)\n    \n    # Get the model\n    model = baseline_models[i]\n    \n    for fold_iter, (train_index, test_index) in enumerate(skf.split(X_train,y_train)):\n        # Get the training and test folds\n        X_train_fold, X_test_fold = X_train[train_index], X_train[test_index]\n        y_train_fold, y_test_fold = y_train[train_index], y_train[test_index]\n        \n        # Fit the data to the model \n        model.fit(X_train_fold, y_train_fold)\n        # Evaluate the test fold\n        model_predictions = model.predict(X_test_fold)\n        model_pred_probs = model.predict_proba(X_test_fold)\n        # Get the evaluation scores \n        acc = accuracy_score(y_test_fold, model_predictions)\n        f1_score = fbeta_score(y_test_fold, model_predictions, beta = 1)\n        g_mean_score = g_mean(y_test_fold, model_predictions)\n        roc_auc = roc_auc_score(y_test_fold, model_pred_probs[:,1])\n        # Store the results in the model_results array\n        model_results[fold_iter] = [acc, f1_score, g_mean_score, roc_auc]\n        \n    # The final result for the model is going to be the average of stratified cross-validation results\n    final_result = model_results.mean(axis = 0)\n    # store the results along with the model name in \"results\" list\n    results[i] = final_result\n\n# print out the results as a dataframe\nskf_results_df = pd.DataFrame(results, \n                              columns = [\"Average %s for %d Folds\"%(metric, n_splits) for metric in metric_names],\n                              index = baseline_model_names)\nskf_results_df","6e57be07":"# estimator = LogisticRegression()\n# parameters = {\n#     'penalty': ['l1', 'l2', 'elasticnet', 'none'],\n#     'solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n#     'tol': [0.5, 0.3, 0.1, 1e-2, 1e-3, 1e-4, 1e-5, 1e-6],\n#     'C': np.logspace(-4, 4, 32),\n#     'max_iter': np.linspace(50, 2000, 40),\n# }\n\n# best_estimator = RandomizedSearchCV(estimator, parameters, n_iter = 3000, scoring = 'accuracy', n_jobs = -1)\n# best_estimator.fit(X_train, y_train)\n# print('Best parameters: ', best_estimator.best_params_)\n# print('Best score: ', best_estimator.best_score_)\n\n\n# You can also try different parameter options to apply random search, just uncomment the code above.\nfine_tuned_params = {'tol': 0.3, 'solver': 'sag', 'penalty': 'l2', 'max_iter': 500, 'C': 10}\n# fine_tuned_params = best_estimator.best_params_\n\ntuned_estimator = LogisticRegression(**fine_tuned_params) # pick the best estimator\ntuned_estimator.fit(X_train, y_train);","e7785618":"# make predictions\nfinal_preds = tuned_estimator.predict(X_test)\nfinal_acc_cv = cross_val_score(LogisticRegression(**fine_tuned_params),\n                            X_test, y_test, cv = 5,\n                            scoring = 'accuracy')\nfinal_acc = accuracy_score(y_test, final_preds)\nfinal_probas = tuned_estimator.predict_proba(X_test)\nprint('Final accuracy: %.4f | Final average accuracy (5 fold): %.4f'%(final_acc, final_acc_cv.mean()))\n\n# Get the confusion matrix\ncm = confusion_matrix(y_test, final_preds)\ntn, fp, fn, tp = cm.ravel()\n# plot it\nsns.heatmap([[tn, fp], [fn, tp]], cmap = 'Blues', annot = True)\nplt.xlabel('predictions')\nplt.ylabel('actual values')\nplt.show()\n\n# my final visualization for the results is the ROC curve. \n# As a \"no-skill\" baseline approach, I'll use picking the majority method.\nfinal_roc_auc_score = cross_val_score(LogisticRegression(**fine_tuned_params),\n                                    X_test, y_test, \n                                    scoring = 'roc_auc')\nbaseline_roc_auc_score = roc_auc_score(y_test, simple_baseline)\nprint(\"Average ROC AUC Score:\", final_roc_auc_score.mean())\nprint(\"Baseline ROC AUC Score:\", baseline_roc_auc_score)\n\n\nns_fpr, ns_tpr, _ = roc_curve(y_test, simple_baseline)\nlr_fpr, lr_tpr, _ = roc_curve(y_test, final_probas[:, 1])\nsns.lineplot(ns_fpr, ns_tpr , label = 'Baseline (no-skill)')\nsns.lineplot(lr_fpr, lr_tpr, label = 'Logistic Regression')\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title(\"ROC Curve\")\nplt.legend()\nplt.show()","e7caef23":"# Exploratory Data Analysis\n\nIn this section, I'll try to create some visualizations to understand if there's a correlation between different features. I try to use many visualizations to understand the data. Since I don't have very much experience with EDA, this probably will be good practice for me.","c4e89a64":"## Visualization\nData visualization is an important part of exploratory data analysis since it's a very helpful process for getting insights about the data we have. We try to understand the data by using different plots.","27cfe52d":"## Features\nLet's take look at the given definition of features\n\n> * `sl_no` : Serial number\n> * `gender` : Gender (F: Female, M: Male)\n> * `ssc_p` : Secondary education percentage (to 10th grade)\n> * `ssc_b` : Board of education for secondary education \n> * `hsc_p` : High secondary education percentage (10th to 12th grade)\n> * `hsc_b` : Board of education for high secondary education\n> * `hsc_s` : Specialization in higher secondary education\n> * `degree_p` : Degree percentage\n> * `degree_t` : under-grad field of degree education\n> * `workex` : work experience\n> * `etest_p` : Employability test percentage (test conducted by college)\n> * `specialization` : post-grad (MBA) specialization \n> * `mba_p` : MBA percentage\n> * `status` : status of placement\n> * `salary` : salary offered by corporate to candidates\n","a8ea8700":"# Machine Learning - Applying Classification Algorithm\n\nWe processed and tried to understand the data so far. Now it's time to modeling it using machine learning algorithms!\n\nI believe there is more than one task to use machine learning, such as: \n- Finding out if the student is placed or not,\n- Predicting the offered salary for placed students,\n\nI'll try to create a model to predict if a student is going to be placed or not. I'm going to compare different classification algorithms using k-fold cross-validation.\n\nRecall that the data was unbalanced. So we should consider using different metrics to evaluate our models. \n\nI'll use the following metrics:\n### Accuracy\nAccuracy is the most common metric for binary classification problems. \nIt's pretty preferable as a metric but when you have an imbalanced dataset like we have, upper-average accuracy may be obtainable by basic approaches such as picking the majority class.\n\n### Fbeta Measure\nFbeta score provides a way to combine recall and precision into a single measure that captures both of them, it's also widely used while working on imbalanced datasets. The mathematical formula for Fbeta measure is\n\n$ F_{beta} = \\frac{( 1+(\\text{beta}^2) ) \\times \\text{Precision} \\times \\text{Recall}}{(\\text{beta}^2 \\times \\text{Precision}) + \\text{Recall}} $\n\nI think we can say that Fbeta measure is just a modified version of the F1 measure. We can control the calculation of harmonic mean with coefficient __beta__. When beta is equal to `1`, that means we're using the F1 score.\n\n### G-Mean\nG-Mean is the geometric mean of Recall and True Negative Rate. So it also provides a way to combine two different metrics into a single measure just like Fbeta measure.\n\n\n### ROC AUC Score\nTo understand the ROC AUC score, we should understand what ROC is. ROC curve summarizes the model behavior by using TPR (True Positive Rate) and FPR (False Positive Rate). When we plot these two by different thresholds, we get a curve that is called as the ROC curve. The area under this curve is the metric known as ROC AUC (Area Under the Receiver Operating Characteristic Curve). \nIn simple terms, the ROC curve describes how good our model is at discriminating the target classes. The area under the curve gives us this description as a scalar value.","b434e847":"There are just too many students who specialized in finance rather than HR. And most of them are also placed so we can say that if a student specializes in marketing and finance, he\/she will have a higher chance of being placed.","6c3ffffe":"We can see that if a student gets a degree from communication and management field, the student will have a higher chance of being placed","35f4731b":"## Conclusion\n> from 1.1, 1.2, 1.3, 1.4 graphs \n* `secondary school` , `higher secondary school` and `under-grad degree` are important features for being placed\n* MBA score difference between placed and not placed students is very small so I think there's not much correlation between MBA scores and `status`\n\n> from graph 2\n* Employment test scores aren't different at all (roughly 73 for placed and 69 for not placed students)\n\n> from graph 3\n* Work experience is a powerful advantage to have but it's not required to get hired. Having work experience increases the chance of being placed. Statistically speaking, 86.4% of students with work experience are placed while only 60% of students without it got placed. So we can say that having work experience is an opportunity worth chasing.  \n\n> from graph 4.0\n* If a student gets a degree from Science or Tech rather than  Communication or Management, the student will have a higher chance of being recruited. Students from these fields have roughly 70% rate of being recruited while other fields have roughly 30%\n\n> from graph 4.1\n* 70% of the students who studied Commerce or Science in secondary school are being placed while only 55% of the Art students are placed. So studying Commerce or Science will increase the chance of being placed \n\n> from graph 4.2\n* The students that specialized in Finance have almost 80% rate of being recruited while HR students have roughly 55%. Therefore, specializing in Finance along with Marketing is a good idea for increasing the chance of being recruited\n\n> from graph 5, 5.1\n* If students are academically successful in secondary education they're more likely to be recruited","96c71ef5":"We can finally use our model to evaluate the test dataset. Let's look at the results!","2772beec":"## Looking for null values\nIf there's a null value, we need to do something about it. But first, let's check that!","26fbde57":"#### We can see the difference between secondary school and under-grad degree scores between placed\/not placed students. Also, MBA percentages don't differ that much ","d649a562":"I think we can say that having work experience is a very good advantage but it's not necessary to be placed. Also notice that most of the students don't have work experience. Numbers don't lie, having work experience will decrease the chance of not being placed and increase the chance of being placed. `141` students don't have work experience and `74` students who have work experience. `~86%` of the students with work experience are placed while only `~59%` of inexperienced students are placed. So it's normal to say that having work experience will increase the chance of being placed","59f8a031":"The score difference between placed and not placed students is not that high but generally placed students has a higher score than not placed students","f84830ad":"Well, the data we have is pretty unbalanced. We should consider this while building machine learning models.","c8a30c37":"# Preparing the data for Classification\nWe need to encode our categorical data and normalize\/scale the continuous features. I'll start with encoding categorical data by using one-hot-encoding.\nAlso, we can drop the features we don't need (salary, gender, ssc_b, hsc_b)","9ffff889":"In over-all performance, Logistic Regression beats the other models. So I'm going to try to fine-tune the Logistic Regression model by using Grid Search.","a8e62c8b":"We can see that LR model did pretty well! Of course there's a still room for improvement but these results are great!","e87b30f3":"## Final words\nHello everyone! If you read and liked my work, please consider upvoting it. Also, It'd be awesome if you share your ideas and opinions about this simple project. Thank you for reading all of this, I appreciate it :)","8a2c327c":"We have `67` null values for the `salary` feature. But these values may be null because of the `status` feature. If a candidate couldn't be placed in any corporation, their offered `salary` would be `None` since they couldn't place to any corporation. But that's just a prediction so we need to check if it's correct."}}