{"cell_type":{"c91ce29f":"code","00b2a5b2":"code","64704d81":"code","b1399b2f":"code","758a338b":"code","1acfbeef":"code","fc6ccaf3":"code","b8d33f2b":"code","757fec8e":"code","9b56753c":"code","d823e3d9":"code","1b28cfec":"code","af60b7d8":"code","23561c86":"code","12613fbd":"code","415e6e91":"code","785b96b6":"code","d9a387b6":"code","6a749299":"code","5da63ea1":"code","f8177a4c":"code","77bcc379":"code","b027e89b":"code","9f8b3024":"code","4bc2b863":"code","4d2798b8":"code","4c5a077b":"code","853bcdf4":"code","4094ff53":"code","0e57a823":"code","c3d2509a":"markdown","e5cd350e":"markdown","81a6f28d":"markdown","2f603d91":"markdown","556082bd":"markdown","d44c0467":"markdown","e114a0ea":"markdown","7f1bc24f":"markdown","535ac6e8":"markdown","43129007":"markdown","4fe40895":"markdown"},"source":{"c91ce29f":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import Imputer\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.tree import DecisionTreeRegressor\n\nfrom xgboost import XGBRegressor\n\nimport os","00b2a5b2":"# List of all the functions that we are going to use\n\ndef get_rmse(y_predicted,y_real):\n    return np.mean(np.sqrt((np.log(y_predicted)-np.log(y_real))**2))","64704d81":"train_data = pd.read_csv('..\/input\/train.csv')\ntest_data = pd.read_csv('..\/input\/test.csv')","b1399b2f":"train_data.columns","758a338b":"train_data.head(10)","1acfbeef":"#We want to visualize the dsitribution of our target, here the SalePrice:\n\nplt.hist(train_data.SalePrice, bins=100)","fc6ccaf3":"#Data seems very scattered, so we will look at the log ot the Sale Price ditribution:\n\nlogged_price = np.log(train_data.SalePrice)\nplt.hist(logged_price, bins=100)","b8d33f2b":"plt.hist(train_data.YearBuilt)","757fec8e":"plt.hist(train_data.YrSold)","9b56753c":"plt.hist(test_data.YrSold)","d823e3d9":"inflation = pd.DataFrame(dict(value_by_1860_usd=[24.29, 24.98, 25.94, 25.85, 26.27],\n                          inflation_percent=[3.23, 2.85, 3.84, -0.36, 1.64]) ,\n                      index = ['2006', '2007', '2008', '2009', '2010'])\ninflation","1b28cfec":"#Let's start by creating a new data set for each year:\ninfl_df_2006 = train_data.loc[train_data['YrSold'] == 2006]\ninfl_df_2007 = train_data.loc[train_data['YrSold'] == 2007]\ninfl_df_2008 = train_data.loc[train_data['YrSold'] == 2008]\ninfl_df_2009 = train_data.loc[train_data['YrSold'] == 2009]\ninfl_df_2010 = train_data.loc[train_data['YrSold'] == 2010] #this one will not be changed, just used for final concat\n\n#Since we tried adjusting prices for inflation, and the MAE turned out ot be worse, \n#we are going to try at different % of inflation correction: 25%, 50%, 75% and 100%\n\n#25%\ninfl_df_2006_25 = infl_df_2006\ninfl_df_2007_25 = infl_df_2007\ninfl_df_2008_25 = infl_df_2008\ninfl_df_2009_25 = infl_df_2009\ninfl_df_2010_25 = infl_df_2010\n\n#50%\ninfl_df_2006_50 = infl_df_2006\ninfl_df_2007_50 = infl_df_2007\ninfl_df_2008_50 = infl_df_2008\ninfl_df_2009_50 = infl_df_2009\ninfl_df_2010_50 = infl_df_2010\n\n#75%\ninfl_df_2006_75 = infl_df_2006\ninfl_df_2007_75 = infl_df_2007\ninfl_df_2008_75 = infl_df_2008\ninfl_df_2009_75 = infl_df_2009\ninfl_df_2010_75 = infl_df_2010\n\ninfl_df_2006_25.head()\n","af60b7d8":"#We get the value of the USD at a given year\nusd_2006 = inflation.loc['2006','value_by_1860_usd']\nusd_2007 = inflation.loc['2007','value_by_1860_usd']\nusd_2008 = inflation.loc['2008','value_by_1860_usd']\nusd_2009 = inflation.loc['2009','value_by_1860_usd']\n\nusd_2010 = inflation.loc['2010','value_by_1860_usd']","23561c86":"#We then get a factor of the value of the USD in a year compared to the 2010 USD:\ndivid_factor_06 = usd_2010 \/ usd_2006\ndivid_factor_07 = usd_2010 \/ usd_2007\ndivid_factor_08 = usd_2010 \/ usd_2008\ndivid_factor_09 = usd_2010 \/ usd_2009\n\nprint('divide factor for 2006 is {} '.format(divid_factor_06))\nprint('divide factor for 2007 is {} '.format(divid_factor_07))\nprint('divide factor for 2008 is {} '.format(divid_factor_08))\nprint('divide factor for 2009 is {} '.format(divid_factor_09))\n\n#25%:\ndivid_factor_06_25 = divid_factor_06\/4\ndivid_factor_07_25 = divid_factor_07\/4\ndivid_factor_08_25 = divid_factor_08\/4\ndivid_factor_09_25 = divid_factor_09\/4\n\n#50%\ndivid_factor_06_50 = divid_factor_06\/2\ndivid_factor_07_50 = divid_factor_07\/2\ndivid_factor_08_50 = divid_factor_08\/2\ndivid_factor_09_50 = divid_factor_09\/2\n\n#75%\ndivid_factor_06_75 = divid_factor_06\/(4\/3)\ndivid_factor_07_75 = divid_factor_07\/(4\/3)\ndivid_factor_08_75 = divid_factor_08\/(4\/3)\ndivid_factor_09_75 = divid_factor_09\/(4\/3)\n\nfloat(divid_factor_06)","12613fbd":"#We need to multiply the values of SalePrice by divid_factor\n\n#25%:\ninfl_df_2006_25['SalePrice'] = infl_df_2006_25['SalePrice'].apply(lambda x: x*float(divid_factor_06_25));\ninfl_df_2007_25['SalePrice'] = infl_df_2007_25['SalePrice'].apply(lambda x: x*divid_factor_07_25);\ninfl_df_2008_25['SalePrice'] = infl_df_2008_25['SalePrice'].apply(lambda x: x*divid_factor_08_25);\ninfl_df_2009_25['SalePrice'] = infl_df_2009_25['SalePrice'].apply(lambda x: x*divid_factor_09_25);\n\n#50%:\ninfl_df_2006_50['SalePrice'] = infl_df_2006['SalePrice'].apply(lambda x: x*divid_factor_06_50);\ninfl_df_2007_50['SalePrice'] = infl_df_2007['SalePrice'].apply(lambda x: x*divid_factor_07_50);\ninfl_df_2008_50['SalePrice'] = infl_df_2008['SalePrice'].apply(lambda x: x*divid_factor_08_50);\ninfl_df_2009_50['SalePrice'] = infl_df_2009['SalePrice'].apply(lambda x: x*divid_factor_09_50);\n\n#75%:\ninfl_df_2006_75['SalePrice'] = infl_df_2006['SalePrice'].apply(lambda x: x*divid_factor_06_75);\ninfl_df_2007_75['SalePrice'] = infl_df_2007['SalePrice'].apply(lambda x: x*divid_factor_07_75);\ninfl_df_2008_75['SalePrice'] = infl_df_2008['SalePrice'].apply(lambda x: x*divid_factor_08_75);\ninfl_df_2009_75['SalePrice'] = infl_df_2009['SalePrice'].apply(lambda x: x*divid_factor_09_75);\n\n#100%:\ninfl_df_2006['SalePrice'] = infl_df_2006['SalePrice'].apply(lambda x: x*divid_factor_06);\ninfl_df_2007['SalePrice'] = infl_df_2007['SalePrice'].apply(lambda x: x*divid_factor_07);\ninfl_df_2008['SalePrice'] = infl_df_2008['SalePrice'].apply(lambda x: x*divid_factor_08);\ninfl_df_2009['SalePrice'] = infl_df_2009['SalePrice'].apply(lambda x: x*divid_factor_09);","415e6e91":"#Now we need to concat all the sub DataFrames in one bigger one\n#25%:\nframes_25 = [infl_df_2006_25, infl_df_2007_25, infl_df_2008_25, infl_df_2009_25, infl_df_2010_25]\ninfl_train_data_25 = pd.concat(frames_25)\n\n#50%:\nframes_50 = [infl_df_2006_50, infl_df_2007_50, infl_df_2008_50, infl_df_2009_50, infl_df_2010_50]\ninfl_train_data_50 = pd.concat(frames_50)\n\n#75%:\nframes_75 = [infl_df_2006_75, infl_df_2007_75, infl_df_2008_75, infl_df_2009_75, infl_df_2010_75]\ninfl_train_data_75 = pd.concat(frames_75)\n\n#100:\nframes = [infl_df_2006, infl_df_2007, infl_df_2008, infl_df_2009, infl_df_2010]\ninfl_train_data = pd.concat(frames)\n\n\nprint('size of train data : ',train_data.size)\nprint('size of train data adjusted for inflation : ', infl_train_data.size)","785b96b6":"print('nb of missing data = {0}'.format(train_data.isnull().sum().max()))","d9a387b6":"#Missing values:\ntotal = train_data.isnull().sum().sort_values(ascending=False)\npercent=(train_data.isnull().sum()\/train_data.isnull().count()).sort_values(ascending=False)\n\nmissing_data=pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\nmissing_data.head(20)","6a749299":"correlation = train_data.corr()\nprint('Most correlated columns to {0} are: '.format('SalePrice'),'\\n', correlation['SalePrice'].sort_values(ascending = False)[:10])\nprint('Least correlated columns to {0} are: '.format('SalePrice'),'\\n', correlation['SalePrice'].sort_values(ascending = False)[-10:])","5da63ea1":"corrmat = train_data.corr()\nf, ax = plt.subplots(figsize=(12,9))\nsns.heatmap(corrmat, vmax = 0.8, square=True)","f8177a4c":"#Scatterplot\nsns.set()\ncols_scat = ['SalePrice','OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', 'FullBath', 'YearBuilt']\nsns.pairplot(train_data[cols_scat])\nplt.show()","77bcc379":"#Now, we get rid of variables that are missing too much data (here, everything that is missing more than 1 variable)\ntrain_data = train_data.drop((missing_data[missing_data['Total']>1]).index, 1)\ninfl_train_data = infl_train_data.drop((missing_data[missing_data['Total']>1]).index, 1)\ninfl_train_data_25 = infl_train_data_25.drop((missing_data[missing_data['Total']>1]).index, 1)\ninfl_train_data_50 = infl_train_data_50.drop((missing_data[missing_data['Total']>1]).index, 1)\ninfl_train_data_75 = infl_train_data_75.drop((missing_data[missing_data['Total']>1]).index, 1)\n\n#We will delete the entry containing the missing data in the Electrical variable: \ntrain_data = train_data.drop(train_data.loc[train_data['Electrical'].isnull()].index)\ninfl_train_data = infl_train_data.drop(infl_train_data.loc[infl_train_data['Electrical'].isnull()].index)\ninfl_train_data_25 = infl_train_data_25.drop(infl_train_data_25.loc[infl_train_data_25['Electrical'].isnull()].index)\ninfl_train_data_50 = infl_train_data_50.drop(infl_train_data_50.loc[infl_train_data_50['Electrical'].isnull()].index)\ninfl_train_data_75 = infl_train_data_75.drop(infl_train_data_75.loc[infl_train_data_75['Electrical'].isnull()].index)","b027e89b":"print('Missing values in train data : ', train_data.isnull().sum().max())\nprint('Missing values in train data adjusted for inflation : ', infl_train_data.isnull().sum().max())","9f8b3024":"var = 'GrLivArea'\ndata = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000))","4bc2b863":"#We want to see the two points with highest GrLivArea, they seem ourliers\ntrain_data.sort_values(by = 'GrLivArea', ascending = False)[:2] #(from kernel found online)\n\n#I just checked if another technique worked, just for pratcise, it gives the same result (which is good news)\n#High_GrLivArea = train_data[train_data['GrLivArea']>4500]\n#High_GrLivArea.head()","4d2798b8":"train_data = train_data.drop(train_data[train_data[\"Id\"] == 524].index)\ntrain_data = train_data.drop(train_data[train_data[\"Id\"] == 1299].index)\n\ninfl_train_data = infl_train_data.drop(infl_train_data[infl_train_data['Id']==524].index)\ninfl_train_data = infl_train_data.drop(infl_train_data[infl_train_data['Id']==1299].index)\n\ninfl_train_data_25 = infl_train_data_25.drop(infl_train_data_25[infl_train_data_25['Id']==524].index)\ninfl_train_data_25 = infl_train_data_25.drop(infl_train_data_25[infl_train_data_25['Id']==1299].index)\n\ninfl_train_data_50 = infl_train_data_50.drop(infl_train_data_50[infl_train_data_50['Id']==524].index)\ninfl_train_data_50 = infl_train_data_50.drop(infl_train_data_50[infl_train_data_50['Id']==1299].index)\n\ninfl_train_data_75 = infl_train_data_75.drop(infl_train_data_75[infl_train_data_75['Id']==524].index)\ninfl_train_data_75 = infl_train_data_75.drop(infl_train_data_75[infl_train_data_75['Id']==1299].index)\n\n\ndata = pd.concat([train_data['SalePrice'], train_data[var]], axis=1)\ndata.plot.scatter(x=var, y='SalePrice', ylim=(0,800000), title='Train data')\n\ninfl_data = pd.concat([infl_train_data['SalePrice'], infl_train_data[var]], axis=1)\ninfl_data.plot.scatter(x=var, y='SalePrice', ylim=(0,800000), title='Adjusted train data')","4c5a077b":"#One-hot encoding (using categorical data)\n\ncols_with_missing = [col for col in train_data.columns \n                                 if train_data[col].isnull().any()]                                  \ncandidate_train_predictors = train_data.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\ncandidate_test_predictors = test_data.drop(['Id'] + cols_with_missing, axis=1)\n\n# \"cardinality\" means the number of unique values in a column.\n# We use it as our only way to select categorical columns here. This is convenient, though\n# a little arbitrary.\nlow_cardinality_cols = [cname for cname in candidate_train_predictors.columns if \n                                candidate_train_predictors[cname].nunique() < 10 and\n                                candidate_train_predictors[cname].dtype == \"object\"]\nnumeric_cols = [cname for cname in candidate_train_predictors.columns if \n                                candidate_train_predictors[cname].dtype in ['int64', 'float64']]\nmy_cols = low_cardinality_cols + numeric_cols\ntrain_predictors = candidate_train_predictors[my_cols]\ntest_predictors = candidate_test_predictors[my_cols]\n\none_hot_encoded_training_predictors = pd.get_dummies(train_predictors)\none_hot_encoded_test_predictors = pd.get_dummies(test_predictors)\nfinal_train, final_test = one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n                                                                    join='left', \n                                                                    axis=1)\n\n#Same for adjusted set:\ninfl_candidate_train_predictors = infl_train_data.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\n\ninfl_train_predictors = infl_candidate_train_predictors[my_cols]\n\ninfl_one_hot_encoded_training_predictors = pd.get_dummies(infl_train_predictors)\ninfl_final_train, final_test = infl_one_hot_encoded_training_predictors.align(one_hot_encoded_test_predictors,\n                                                                    join='left', \n                                                                    axis=1)\n\n#25:\ninfl_candidate_train_predictors_25 = infl_train_data_25.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\n\ninfl_train_predictors_25 = infl_candidate_train_predictors_25[my_cols]\n\ninfl_one_hot_encoded_training_predictors_25 = pd.get_dummies(infl_train_predictors_25)\ninfl_final_train_25, final_test = infl_one_hot_encoded_training_predictors_25.align(one_hot_encoded_test_predictors,\n                                                                    join='left', \n                                                                    axis=1)\n\n#50:\ninfl_candidate_train_predictors_50 = infl_train_data_50.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\n\ninfl_train_predictors_50 = infl_candidate_train_predictors_50[my_cols]\n\ninfl_one_hot_encoded_training_predictors_50 = pd.get_dummies(infl_train_predictors_50)\ninfl_final_train_50, final_test = infl_one_hot_encoded_training_predictors_50.align(one_hot_encoded_test_predictors,\n                                                                    join='left', \n                                                                    axis=1)\n\n#75:\ninfl_candidate_train_predictors_75 = infl_train_data_75.drop(['Id', 'SalePrice'] + cols_with_missing, axis=1)\n\ninfl_train_predictors_75 = infl_candidate_train_predictors_75[my_cols]\n\ninfl_one_hot_encoded_training_predictors_75 = pd.get_dummies(infl_train_predictors_75)\ninfl_final_train_75, final_test = infl_one_hot_encoded_training_predictors_75.align(one_hot_encoded_test_predictors,\n                                                                    join='left', \n                                                                    axis=1)","853bcdf4":"X = np.array(final_train)\ny = train_data.SalePrice\n\ntrain_X, val_X, train_y, val_y = train_test_split(X, y)\n\n#We are going to compare the original data set, with the adjusted to inflation one:\ninfl_X = np.array(infl_final_train)\ninfl_y = infl_train_data.SalePrice\n\ninfl_train_X, infl_val_X, infl_train_y, infl_val_y = train_test_split(infl_X, infl_y)\n\n#25%:\ninfl_X_25 = np.array(infl_final_train_25)\ninfl_y_25 = infl_train_data_25.SalePrice\n\ninfl_train_X_25, infl_val_X_25, infl_train_y_25, infl_val_y_25 = train_test_split(infl_X_25, infl_y_25)\n\n#50%:\ninfl_X_50 = np.array(infl_final_train_50)\ninfl_y_50 = infl_train_data_50.SalePrice\n\ninfl_train_X_50, infl_val_X_50, infl_train_y_50, infl_val_y_50 = train_test_split(infl_X_50, infl_y_50)\n\n#75%:\ninfl_X_75 = np.array(infl_final_train_75)\ninfl_y_75 = infl_train_data_75.SalePrice\n\ninfl_train_X_75, infl_val_X_75, infl_train_y_75, infl_val_y_75 = train_test_split(infl_X_75, infl_y_75)","4094ff53":"best_learn_rate = 0.2\nbest_nb_est = 50 \n#These come from a previous Kernel, but for the purpose of just testing this, I will keep those values for now\n\n#we extract the year at which the predicted SalePrice have been sold\nyear_train = train_X[:,[32]]\nyear_test = val_X[:,[32]]\n\nmy_pipeline = make_pipeline(Imputer(), XGBRegressor(n_estimators=best_nb_est, learning_rate = best_learn_rate))\n\nmy_pipeline.fit(X,y)\ntrain_y_predicted = my_pipeline.predict(train_X)\nval_y_predicted = my_pipeline.predict(val_X)\n\nprint('Score on training set:',get_rmse(train_y_predicted,train_y))\nprint('Score on validation set:',get_rmse(val_y_predicted,val_y))\n\n#Adjusted dataset:\nmy_pipeline.fit(infl_X,infl_y)\ninfl_train_y_predicted = my_pipeline.predict(infl_train_X)\ninfl_val_y_predicted = my_pipeline.predict(infl_val_X)\n\n#25% adjusted dataset:\nmy_pipeline.fit(infl_X_25,infl_y_25)\ninfl_train_y_predicted_25 = my_pipeline.predict(infl_train_X_25)\ninfl_val_y_predicted_25 = my_pipeline.predict(infl_val_X_25)\n\n#50% adjusted dataset:\nmy_pipeline.fit(infl_X_50,infl_y_50)\ninfl_train_y_predicted_50 = my_pipeline.predict(infl_train_X_50)\ninfl_val_y_predicted_50 = my_pipeline.predict(infl_val_X_50)\n\n#75% adjusted dataset:\nmy_pipeline.fit(infl_X_75,infl_y_75)\ninfl_train_y_predicted_75 = my_pipeline.predict(infl_train_X_75)\ninfl_val_y_predicted_75 = my_pipeline.predict(infl_val_X_75)\n\n#Now we need to divide the predictions 'infl_train_y_predicted' by the divid_factor of each year accordingly:\n#start by making a copy of the predicted price dataset, just in case:\ny_predict = infl_train_y_predicted\ny_predict[year_train[:,0] == 2009] = y_predict[year_train[:,0] == 2009]\/divid_factor_09\ny_predict[year_train[:,0] == 2008] = y_predict[year_train[:,0] == 2008]\/divid_factor_08\ny_predict[year_train[:,0] == 2007] = y_predict[year_train[:,0] == 2007]\/divid_factor_07\ny_predict[year_train[:,0] == 2006] = y_predict[year_train[:,0] == 2006]\/divid_factor_06\ninfl_train_y_predicted = y_predict\n\nval_y_predict = infl_val_y_predicted\nval_y_predict[year_test[:,0] == 2009] = val_y_predict[year_test[:,0] == 2009]\/divid_factor_09\nval_y_predict[year_test[:,0] == 2008] = val_y_predict[year_test[:,0] == 2008]\/divid_factor_08\nval_y_predict[year_test[:,0] == 2007] = val_y_predict[year_test[:,0] == 2007]\/divid_factor_07\nval_y_predict[year_test[:,0] == 2006] = val_y_predict[year_test[:,0] == 2006]\/divid_factor_06\ninfl_val_y_predicted = val_y_predict\n\nprint('Score on adjusted training set:',get_rmse(infl_train_y_predicted,infl_train_y))\nprint('Score on adjusted validation set:',get_rmse(infl_val_y_predicted,infl_val_y))\n\n#25%:\ny_predict_25 = infl_train_y_predicted_25\ny_predict_25[year_train[:,0] == 2009] = y_predict_25[year_train[:,0] == 2009]\/divid_factor_09_25\ny_predict_25[year_train[:,0] == 2008] = y_predict_25[year_train[:,0] == 2008]\/divid_factor_08_25\ny_predict_25[year_train[:,0] == 2007] = y_predict_25[year_train[:,0] == 2007]\/divid_factor_07_25\ny_predict_25[year_train[:,0] == 2006] = y_predict_25[year_train[:,0] == 2006]\/divid_factor_06_25\ninfl_train_y_predicted_25 = y_predict_25\n\nval_y_predict_25 = infl_val_y_predicted_25\nval_y_predict_25[year_test[:,0] == 2009] = val_y_predict_25[year_test[:,0] == 2009]\/divid_factor_09_25\nval_y_predict_25[year_test[:,0] == 2008] = val_y_predict_25[year_test[:,0] == 2008]\/divid_factor_08_25\nval_y_predict_25[year_test[:,0] == 2007] = val_y_predict_25[year_test[:,0] == 2007]\/divid_factor_07_25\nval_y_predict_25[year_test[:,0] == 2006] = val_y_predict_25[year_test[:,0] == 2006]\/divid_factor_06_25\ninfl_val_y_predicted_25 = val_y_predict_25\n\nprint('Score on 25% adjusted training set:',get_rmse(infl_train_y_predicted_25,infl_train_y_25))\nprint('Score on 25% adjusted validation set:',get_rmse(infl_val_y_predicted_25,infl_val_y_25))","0e57a823":"predictions = my_pipeline.predict(final_test)\noutput = pd.DataFrame({'Id': test_data.Id,\n                       'SalePrice': predictions})\n\noutput.to_csv('submission.csv', index=False)","c3d2509a":"We should look more into the details of the data before deleting it, we should not base ourselves on the conclusions that we think might explain the irregularity of the data. Especially before deleting that data.","e5cd350e":"Score on training set is better than previous Kernel (0.06955...)\n\nScore on validation set is worse than previous Kernel (0.06391...)\n\nWe can notice a very slight improvement when adjusting for inflation, which is good news!","81a6f28d":"Notes on things to look into:\n\n  1) Does price take into account inflation? (goes all the way to 1880, which could have a huge effect on the price difference)\n  \n  2) We did data cleaning on 1 variable, but looking at some other heavily correlated data (to SalePrice), this could be done for more variables, to limit as much as possible the effects of anormal data points","2f603d91":"This priced are baed on the 2010 USD. Wee need to now convert back to their respective years. To do so, we are going to need to get 'YrSold' by index, because the predicted prices are just situated in an array with no index or year info. For this, we basically need to get an array with just 'YrSold', since it will be in the same order as our predicted prive array.\n\nThen, we need to divide the predicted SalePrice by the ratio of USD value depending on the YrSold","556082bd":"## Now we want to create a new dataset with SalePrice adjusted for inflation. \nSince the lastest year where any house was sold in this data set is 2010, this is what out reference will be. This means we are going to keep the value of the houses sold in 2010, but increase (since inflation devaluates the money) the SalePrice of the other houses.\n\nHow to do this:\n\n- Start by creating a new pd.df for each year, containing all the examples that were sold that particular year.\n- For each, multiply SalePrice by inflation factor (which has to be 2010_usd\/200X_usd (>1, so it will increase the price)) of all examples\n- Concat all these pd.df into one, where each will have been adjusted for inflation accordingly\n- Check that we have:\n    - 1) Same nb of elements in the pd.df\n    - 2) Check we have the same nb of examples per year, this should be enough to guarantee enough that we have all the examples","d44c0467":"We have now created the same DataFrame, but with SalePrice adjusted for inflation. We can see that both 'train_data' and the adjusted for inflation DataFrame have the same size. The other difference is that they are not arranged in the same order, as the adjusted is order by 'YrSold', as it is a concat of sub-DataFrames divided by year","e114a0ea":"After looking at this information, we can see that:\n- 'PoolQc', 'MiscFeature', 'Alley', and 'Fence'are lacking from 99 tp 80% of their data. No real point in keeping that sort of data, we could just dump it, it will only make out database bigger for nothing to use dummies here.\n- All the 'Garage[...]' features seem to have the same data lacking. Later on, with the histrogram they look very correlated. We could keep only one of them maybe? \n- Same goes for 'Bsmt[...]' data.\n\nIn the [Kernel](http:\/\/www.kaggle.com\/pmarcelino\/comprehensive-data-exploration-with-python) most of this is based on, dummies are the last step. I guess this is once everything has already been smoothed out, which sounds (more) logical","7f1bc24f":"By looking at the test data, we can see that the 'YrSold' also goes from 2006 to 2010. It seems like it might be interesting to adjust inflation in the test set too.\n\nI simply am not sure if this is something we can do, and if we should to any modifcation to the test set at all? It seems like the proper thing to do too, so I also adjust SalePrice on the test set.","535ac6e8":"**Now we move to the model**","43129007":"In this Kernel, I am focusing on exploring the available data, and adjusting SalePrice to inflation","4fe40895":"We can see that the houses have been sold from 2006 to 2010.\n\nFrom this [website](https:\/\/www.in2013dollars.com\/1860-dollars-in-2017?amount=1), we can see that from a basis of 1 dollar in 1860, it was 24.29 dollars and 26.17 dollars in 2010. That is on average a 7.74 per cent increase in the value of the dollar, in just those 4 years. The actual change over that period is:"}}