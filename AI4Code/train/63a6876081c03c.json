{"cell_type":{"b7e101e9":"code","e0c660c1":"code","e555170b":"code","2a4d5e64":"code","0063b132":"code","a1deb72a":"code","112e8a57":"code","df122284":"code","135602b7":"code","162e92f0":"code","baea3d40":"code","32564810":"code","673719e5":"code","d0ac63d4":"code","6910126a":"code","69d27e25":"code","22bd2a98":"code","b8e565ca":"code","1b36be83":"code","cf85c06d":"code","dd3a2f13":"code","b1e39e41":"code","e499effb":"code","ffc15e68":"code","a379ba46":"code","16c6c44c":"code","f2dac53c":"code","f2ba2c75":"code","4022dcf0":"code","c594efc8":"code","f7994ef8":"code","61c4df94":"code","122c2510":"code","781424bc":"code","a068cbef":"code","f5aeb9d1":"code","2c2966f4":"markdown","18648ec4":"markdown","2b9b62a5":"markdown","23fb26ee":"markdown","8abc0d06":"markdown","ab659f3c":"markdown","e9d8f2fc":"markdown","419841a4":"markdown","f4d22050":"markdown","893beb9f":"markdown","64ffbbd0":"markdown","47b33c2e":"markdown","c2dce6b8":"markdown","597cae1c":"markdown","a0234819":"markdown","173f2522":"markdown","9e8f1023":"markdown","1bb3c12a":"markdown","60b4f134":"markdown","bd713d0b":"markdown","21ec98ee":"markdown","9a7ba4b5":"markdown","6c0ece48":"markdown","522203a6":"markdown"},"source":{"b7e101e9":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e0c660c1":"#Libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n#sns.set_palette('Set2')\n#sns.set_style('white')\nimport plotly.express as px\nimport plotly.graph_objects as go\n\n#Data Preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler\n\n#Models ML\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\n#Boosting\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\n\n#Metrics\nfrom sklearn.metrics import confusion_matrix,accuracy_score\nfrom sklearn.metrics import mean_squared_error,r2_score\nfrom sklearn.metrics import roc_curve, auc\nfrom sklearn import metrics","e555170b":"data = pd.read_csv('\/kaggle\/input\/diabetes-data-set\/diabetes.csv')\ndata.head()","2a4d5e64":"# There is not NaN or null values in columns\ndata.info()","0063b132":"data.describe()","a1deb72a":"select_col = ['Glucose', 'BloodPressure', 'SkinThickness', 'Insulin', 'BMI']","112e8a57":"# Sum of values equal zero in each column\nfor col in data[select_col]:\n    print('{}:'.format(col) , data[data[col]==0].value_counts().sum())","df122284":"# Value of median \ndata.median()","135602b7":"# Replacing the values equal to 0 by the median\nfor col in data[select_col]:\n    data.loc[:,col].replace(0, data.loc[:,col].median(), inplace=True)","162e92f0":"for col in data[select_col]:\n    print('{}:'.format(col) , data[data[col]==0].value_counts().sum())","baea3d40":"data.head()","32564810":"sns.countplot(x='Outcome',data=data, palette='Set2')\nplt.title('Count of people with and without diabetes')\nplt.grid()","673719e5":"data.Outcome.value_counts()","d0ac63d4":"# Range of Ages\nsns.histplot(data['Age'], bins=4)\nplt.title('Distribution of ages')\nplt.grid()","6910126a":"# Pregnancy count per woman\nsns.countplot(x='Pregnancies', data=data)\nplt.title('Pregnancies')\nplt.grid()","69d27e25":"sns.histplot(data['Glucose'], kde=True)\nplt.grid()","22bd2a98":"sns.displot(data['BloodPressure'],kde=True, color='green')\nplt.grid()","b8e565ca":"sns.displot(data['BMI'], kde=True, color='red')\nplt.grid()","1b36be83":"sns.displot(data['SkinThickness'], kde=True, color='orange')\nplt.grid()","cf85c06d":"# Blood pressure lower than 80 is normal\n\nfig = px.scatter(data, x=\"Age\", y='BloodPressure',\n             size=\"Glucose\", color=\"Outcome\",\n                 hover_data=[\"BMI\"], log_x=True, size_max=12, \n                 color_continuous_scale=[[0, 'rgb(102, 194, 165)'], [1.0, 'rgb(225, 128, 114)']],\n                 title=\"General view\"\n                 )\nfig.add_shape(type=\"line\",\n    x0=20, y0=80, x1=85, y1=80,\n    line=dict(color=\"blue\",width=2,dash=\"dash\")\n )\n\nfig.show()","dd3a2f13":"# We differentiate the data with Outcome 0 and 1\nout_0 = data[data['Outcome']==0]\nout_1 = data[data['Outcome']==1]","b1e39e41":"# We'll buil a function to represent de differents distributions with respect to \"Outcome\"\ndef visualization(variable):\n    fig=go.Figure()\n    fig.add_trace(go.Box(y=out_0[variable],name=0,marker_color='rgb(102, 194, 165)',boxpoints=\"all\",whiskerwidth=0.3))\n    fig.add_trace(go.Box(y=out_1[variable],name=1,marker_color='rgb(225, 128, 114)',boxpoints=\"all\",whiskerwidth=0.3))\n    fig.update_layout(title=\"{} distribution with respect to Outcome\".format(variable),height=600)\n    fig.show()","e499effb":"columns = ['Pregnancies', 'Glucose', 'BloodPressure', 'SkinThickness', 'Insulin',\n       'BMI', 'DiabetesPedigreeFunction', 'Age']\n\nfor column in data[columns]:\n    visualization(column)","ffc15e68":"plt.figure(figsize=(16,9))\ncorr = data.corr()\nsns.heatmap(abs(corr), lw=1, annot=True, cmap='Set2')\nplt.show()","a379ba46":"# training and normalization of data\nX = data.iloc[:,:8]\nY = data.iloc[:,8]\n\nX_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size = 0.20, random_state=7)\n\nSScaler = StandardScaler()\nX_train = SScaler.fit_transform(X_train)\nX_test = SScaler.fit_transform(X_test)","16c6c44c":"def impressions(model,accuracy):\n    print('Accuracy: {} %'.format(accuracy))\n    print('Mean squared error: ', round(mean_squared_error(Y_test,Y_pred),3))\n    \n\n    cm=confusion_matrix(Y_test,Y_pred)\n    class_label = [0, 1]\n    df_cm = pd.DataFrame(cm, index=class_label,columns=class_label)\n    sns.heatmap(df_cm,annot=True,cmap='Set2',linewidths=2,fmt='d')\n    plt.title(\"Confusion Matrix\",fontsize=15)\n    plt.xlabel(\"Predicted\")\n    plt.ylabel(\"True\")\n    plt.show()","f2dac53c":"def roc_curve(Y_test, Y_score):\n    from sklearn.metrics import roc_curve, auc\n    fpr, tpr, thresholds = roc_curve(Y_test, Y_score)\n    score = metrics.auc(fpr, tpr)\n\n    fig = px.area(\n        #fpr = False Positive Rate; tpr= True Positive Rate\n        x=fpr, y=tpr,\n        title=f'ROC Curve (AUC={auc(fpr, tpr):.4f})',\n        labels=dict(x='False Positive Rate', y='True Positive Rate'),\n        width=700, height=500\n    )\n\n    fig.add_shape(\n        type='line', line=dict(dash='dash'),\n        x0=0, x1=1, y0=0, y1=1\n    )\n\n    fig.update_yaxes(scaleanchor=\"x\", scaleratio=1)\n    fig.update_xaxes(constrain='domain')\n    fig.show()    ","f2ba2c75":"LogR= LogisticRegression()\nLogR.fit(X_train,Y_train)\nY_pred= LogR.predict(X_test)\n\nLogR_accuracy= round(accuracy_score(Y_test,Y_pred),5)*100\n\nimpressions(LogR,LogR_accuracy)\n\nY_score = LogR.predict_proba(X_test)[:,1]\nroc_curve(Y_test,Y_score)","4022dcf0":"KNN= KNeighborsClassifier(n_neighbors=10)\nKNN.fit(X_train,Y_train)\nY_pred= KNN.predict(X_test)\n\nKNN_accuracy= round(accuracy_score(Y_test,Y_pred), 5)*100 # Accuracy\n\nimpressions(KNN,KNN_accuracy)\n\nY_score = KNN.predict_proba(X_test)[:,1]\nroc_curve(Y_test,Y_score)","c594efc8":"from sklearn.svm import SVC\n\nsvc= SVC(kernel='rbf')\nsvc.fit(X_train,Y_train)\nY_pred= svc.predict(X_test)\n\nsvc_accuracy= round(accuracy_score(Y_test,Y_pred), 5)*100 # Accuracy\n\nimpressions(svc,svc_accuracy)","f7994ef8":"from sklearn.ensemble import RandomForestClassifier\n\nrfc= RandomForestClassifier(n_estimators=200, random_state=5, criterion='gini', max_depth=100)\nrfc.fit(X_train,Y_train)\nY_pred= rfc.predict(X_test)\n\nrfc_accuracy= round(accuracy_score(Y_test,Y_pred), 5)*100 # Accuracy\n\nimpressions(rfc,rfc_accuracy)","61c4df94":"from sklearn.tree import DecisionTreeClassifier\n\ntree = DecisionTreeClassifier(random_state=10, max_depth=100)\ntree.fit(X_train,Y_train)\nY_pred= tree.predict(X_test)\n\ntree_accuracy= round(accuracy_score(Y_test,Y_pred), 5)*100 # Accuracy\n\nimpressions(tree,tree_accuracy)\n\nY_score = tree.predict_proba(X_test)[:,1]","122c2510":"ADA=AdaBoostClassifier(learning_rate= 0.15,n_estimators= 40)\nADA.fit(X_train,Y_train)\nY_pred= ADA.predict(X_test)\n\nADA_accuracy=round(accuracy_score(Y_test,Y_pred), 4)*100 # Accuracy\n\nimpressions(ADA,ADA_accuracy)","781424bc":"GB= GradientBoostingClassifier(n_estimators=30,learning_rate=0.22,loss=\"deviance\")\nGB.fit(X_train,Y_train)\nY_pred= GB.predict(X_test)\n\nGB_accuracy=round(accuracy_score(Y_test,Y_pred), 4)*100 # Accuracy\n\nimpressions(GB,GB_accuracy)","a068cbef":"models_accuracy = {\n    'Logistic Regression': LogR_accuracy,\n    'K-Nearest Neighbors' : KNN_accuracy,\n    'Support Vector Machine' : svc_accuracy,\n    'Random Forest': rfc_accuracy,\n    'Decission Tree Classifier': tree_accuracy,\n    'ADABoost Classifier': ADA_accuracy,\n    'Gradient Boosting Classifier': GB_accuracy\n    \n}","f5aeb9d1":"results = pd.DataFrame([[key, models_accuracy[key]] for key in models_accuracy.keys()],\n                       columns=['Models', 'Accuracies']).sort_values('Accuracies', ascending=False)\nresults","2c2966f4":"**This is the first notebook I upload to Kaggle. I'm new to the world of data science so any feedback is very welcome!**","18648ec4":"# Evaluation models\n\nFirst we will build two functions. The first to represent the confusion matrix and some parameters of metrics, and the second to represent the roc curve","2b9b62a5":"# Data visualization\n## Outcomes","23fb26ee":"## Pregnancies","8abc0d06":"### Import dataset","ab659f3c":"We see that variables like Glucose, BloodPressure, SkinThickness, Insulin and BMI have values equal to 0.\nWe'll replace this values for the value of mean of each column of the dataset.","e9d8f2fc":"## Data processing and scaling\nWe'll split the data into training and testing sets. Then we'll scale using StandarScale.\n\nBoth using diferents libraries from *scikit-learn*. For the training and testing data we'll use *train_test_split* from *sklearn.model_selection* and for scaling we'll use *StandardScaler* from *sklearn.preprocessing*","419841a4":"### K-Nearest Neighbors","f4d22050":"## SkinThickness\nFrom a laboratory in Argentina, I found that an optimal value of skin thickness for people over 12 years of age is 3 to 17 Ul\/ml.\n\nsource: <a>https:\/\/www.labmoreira.com\/nuevos-examenes.asp?strClave=2<\/a>","893beb9f":"### Gradient Boosting Classifier\n","64ffbbd0":"### Random Forest","47b33c2e":"## Diastolic blood pressure (mm Hg)\n\nThe diastolic reading is the pressure in the arteries when the heart rests between beats. This is the time when the heart fills with blood and gets oxygen.\n\nThis is what diastolic blood pressure number means:\n* ***Normal***: Lower than 80 mmHg\n* ***Stage 1 hypertension***: 80-89 mmHg\n* ***Stage 2 hypertension***: 90 mmHg or more \n\nsource: <a>https:\/\/www.webmd.com\/hypertension-high-blood-pressure\/guide\/diastolic-and-systolic-blood-pressure-know-your-numbers<\/a>","c2dce6b8":"## Body Mass Index (BMI)\n\nThe body mass index (BMI) is a measure that uses the height and weight to work out if the weight is healthy. The BMI calculation divides an adult's weight in kilograms by their height in metres squared.\n\nIf your BMI is:\n\n* Below 18.5 \u2013 you're in the ***underweight*** range\n* Between 18.5 and 24.9 \u2013 you're in the ***healthy weight*** range\n* Between 25 and 29.9 \u2013 you're in the ***overweight*** range\n* Between 30 and 39.9 \u2013 you're in the ***obese*** range\n\nsource: <a>https:\/\/www.nhs.uk\/common-health-questions\/lifestyle\/what-is-the-body-mass-index-bmi\/<\/a>","597cae1c":"### Context\n\nThis dataset is originally from the National Institute of Diabetes and Digestive and Kidney Diseases. The objective is to predict based on diagnostic measurements whether a patient has diabetes.\n\n### Content\n\nSeveral constraints were placed on the selection of these instances from a larger database. In particular, all patients here are females at least 21 years old of Pima Indian heritage.\n\n* **Pregnancies**: Number of times pregnant\n* **Glucose**: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n* **BloodPressure**: Diastolic blood pressure (mm Hg)\n* **SkinThickness**: Triceps skin fold thickness (mm)\n* **Insulin**: 2-Hour serum insulin (mu U\/ml)\n* **BMI**: Body mass index (weight in kg\/(height in m)^2)\n* **DiabetesPedigreeFunction**: Diabetes pedigree function\n* **Age**: Age (years)\n* **Outcome**: Class variable (0 = Person hasn't diabetes or 1 = Person has diabetes)","a0234819":"### Support Vector Machine","173f2522":"### Adaboost Classifier","9e8f1023":"## Glucose (mg\/dL)\n\n**Oral glucose tolerance test**: For this test, you fast overnight, and the fasting blood sugar level is measured. Then you drink a sugary liquid, and blood sugar levels are tested periodically for the next two hours.\n\nA blood sugar level less than 140 mg\/dL (7.8 mmol\/L) is ***normal***. A reading of more than 200 mg\/dL (11.1 mmol\/L) after two hours indicates ***diabetes***. A reading between 140 and 199 mg\/dL (7.8 mmol\/L and 11.0 mmol\/L) indicates ***prediabetes***.\n\nsource: <a>https:\/\/www.mayoclinic.org\/diseases-conditions\/diabetes\/diagnosis-treatment\/drc-20371451<\/a>","1bb3c12a":"# Diabetes Prediction","60b4f134":"## Ages","bd713d0b":"## Logistic Regression","21ec98ee":"---","9a7ba4b5":"# Correlations\nWe'll observing the correlation wich exist between all the variables.","6c0ece48":"We can see that the variable that most correlates with the \"Outcome\" is \"Glucose\" with a value of 0.49, while the lowest correlation is \"BloodPresure\" with a value of 0.16","522203a6":"### Decision Tree Classifier"}}