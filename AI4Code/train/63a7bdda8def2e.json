{"cell_type":{"e2e09dd0":"code","1f0eef61":"code","4b3e47bb":"code","fc804267":"code","6e90cdd3":"code","55b1e493":"code","057a5c8a":"code","dab02741":"code","3dfdabf9":"code","cccb5751":"code","852f32ab":"code","88e23860":"code","8619e617":"code","1eb9975b":"code","49ff4d70":"code","59e0b0c3":"code","4d329847":"code","2792f984":"code","b9d3f59b":"code","dc845d39":"code","a10bc921":"code","e959fb17":"code","1f96f294":"markdown","e7265fba":"markdown","ca80ecd1":"markdown","65ca2b5e":"markdown"},"source":{"e2e09dd0":"!pip3 install kaggle --upgrade > \/dev\/null 2>&1","1f0eef61":"!pip install 'kaggle-environments>=0.1.6' > \/dev\/null 2>&1","4b3e47bb":"!pip install 'tensorflow-gpu == 1.14.0' > \/dev\/null 2>&1","fc804267":"!pip install stable-baselines > \/dev\/null 2>&1","6e90cdd3":"from kaggle_environments import make\nimport gym\nimport numpy as np\n\nclass ConnectX(gym.Env):\n    \"\"\"Custom Environment that follows gym interface\"\"\"\n    \n    def __init__(self, opponent_type):\n        self.env = make(\"connectx\", debug=True)\n        self.trainer = self.env.train([None, opponent_type])\n        self.obs = None\n        self.action_space = gym.spaces.Discrete(self.env.configuration.columns)\n        self.observation_space = gym.spaces.Box(0, 2, shape=(self.env.configuration.rows, self.env.configuration.columns), dtype=np.float32)\n\n    def get_kaggle_env(self):\n        return self.env\n\n    def step(self, action):\n        # Wrap kaggle environment.step()\n        if self.obs[0][action] != 0:\n          r = -1 # punish illegal move\n          d = False\n          o = self.obs\n        else:\n          o, r, d, _ = self.trainer.step(int(action))\n          o = np.reshape(np.array(o['board']), (self.env.configuration.rows, self.env.configuration.columns))\n          self.obs = o\n\n        return o, float(r), bool(d), {}\n    \n    def reset(self):        \n        o = self.trainer.reset()\n        self.obs = np.reshape(np.array(o['board']), (self.env.configuration.rows, self.env.configuration.columns))\n        return self.obs\n\n    def render(self, **kwargs):\n        return self.env.render(**kwargs)\n    ","55b1e493":"import gym\n\nfrom stable_baselines.common.policies import MlpPolicy\nfrom stable_baselines import PPO2\n\n# from stable_baselines.deepq.policies import MlpPolicy\n# from stable_baselines import DQN\n\nfrom stable_baselines.common.vec_env import DummyVecEnv\n\nfrom stable_baselines.bench import Monitor\nfrom stable_baselines.results_plotter import load_results, ts2xy\n\n# Create log dir\nimport os\nlog_dir = \"\/kaggle\/working\/\"\n# os.makedirs(log_dir, exist_ok=True)\n\ndef callback(_locals, _globals):\n    \"\"\"\n    Callback called at each step (for DQN an others) or after n steps (see ACER or PPO2)\n    :param _locals: (dict)\n    :param _globals: (dict)\n    \"\"\"\n    global best_mean_reward\n    # Evaluate policy training performance\n    x, y = ts2xy(load_results(log_dir), 'timesteps')\n    if len(x) > 0:\n        mean_reward = np.mean(y[-100:])\n        print(x[-1], 'timesteps')\n        print(\"Best \/ last mean reward per episode: {:.2f} \/ {:.2f}\".format(best_mean_reward, mean_reward))\n\n        # New best model, you could save the agent here\n        if mean_reward > best_mean_reward:\n            best_mean_reward = mean_reward\n            # Example for saving best model\n            print(\"*** Saving new best model ***\")\n            _locals['self'].save(log_dir + 'best_model.pkl')\n    return True\n\nbest_mean_reward = -1000000\n\n# gym_env = ConnectX('random')\ngym_env = ConnectX('negamax')\n\nenv = Monitor(gym_env, log_dir, allow_early_resets=True)\n\nenv = DummyVecEnv([lambda: env])\n","057a5c8a":"model = PPO2('MlpPolicy', env)\n# model = DQN('MlpPolicy', env)\n\n# model.learn(total_timesteps=100000, callback=callback) #, seed=42)\nmodel.learn(total_timesteps=10000, callback=callback) #, seed=42)","dab02741":"model = PPO2.load(log_dir + 'best_model.pkl', env)\n\ndone = False\nobs = env.reset()\nstep_cnt = 0\n\nmax_moves = gym_env.get_kaggle_env().configuration.columns * gym_env.get_kaggle_env().configuration.rows\n\nwhile (not done) and step_cnt <= max_moves:\n      step_cnt += 1\n      action, _states = model.predict(obs, deterministic=True)\n      print('action:', action)\n      if obs[0][0][action] != 0:\n        print('skipping illegal move')\n      else:\n        obs, reward, done, info = env.step(action)\n        gym_env.render()\n        print(reward, done)\n      print()\n","3dfdabf9":"for key, value in model.get_parameters().items():\n    print(key, value.shape)","cccb5751":"import torch as th\nimport torch.nn as nn\n\nclass PyTorchMlpPolicy(nn.Module):\n    def __init__(self):\n        super(PyTorchMlpPolicy, self).__init__()\n        self.pi_fc0 = nn.Linear(42, 64)\n        self.pi_fc1 = nn.Linear(64, 64)\n        self.pi = nn.Linear(64, 7)\n        \n        self.tanh = th.tanh\n        self.out_activ = nn.Softmax(dim=0)\n\n    def forward(self, x):\n        x = self.tanh(self.pi_fc0(x))\n        x = self.tanh(self.pi_fc1(x))\n        x = self.pi(x)\n        x = self.out_activ(x)\n        return x","852f32ab":"def copy_mlp_weights(baselines_model):\n    torch_mlp = PyTorchMlpPolicy()\n    model_params = baselines_model.get_parameters()\n    # Get only the policy parameters\n    policy_keys = [key for key in model_params.keys() if \"pi\" in key] # or \"c\" in key]\n    policy_params = [model_params[key] for key in policy_keys]\n    \n    for (th_key, pytorch_param), key, policy_param in zip(torch_mlp.named_parameters(), policy_keys, policy_params):\n        param = policy_param.copy()\n        # Copy parameters from stable baselines model to pytorch model\n\n        # weight of fully connected layer\n        if len(param.shape) == 2:\n            param = param.T\n\n        # bias\n        if 'b' in key:\n            param = param.squeeze()\n\n        param = th.from_numpy(param)\n        pytorch_param.data.copy_(param.data.clone())\n        \n    return torch_mlp\n","88e23860":"th_model = copy_mlp_weights(model)","8619e617":"import torch\nfrom torch.autograd import Variable\n\nepisode_reward = 0\ndone = False\nobs = env.reset()\nstep_cnt = 0\nmax_moves = gym_env.get_kaggle_env().configuration.columns * gym_env.get_kaggle_env().configuration.rows\n\nwhile (not done) and step_cnt <= max_moves:\n      step_cnt += 1\n      th_obs = Variable(torch.from_numpy(obs.flatten()))\n      action = th.argmax(th_model(th_obs)).item()\n\n      print('action:', action)\n      if obs[0][0][action] != 0:\n        print('skipping illegal move')\n      else:\n        obs, reward, done, info = env.step([action])\n        gym_env.render()\n        episode_reward += reward\n      print()\n","1eb9975b":"torch.save(th_model.state_dict(), 'thmodel')","49ff4d70":"import base64\nwith open('thmodel', 'rb') as f:\n    raw_bytes = f.read()\n    encoded_weights = base64.encodebytes(raw_bytes)","59e0b0c3":"print(encoded_weights)","4d329847":"import io\nimport base64\nimport torch\nfrom torch.autograd import Variable\nimport random\n\nagent_th_model = PyTorchMlpPolicy()\n# encoded_weights =b'gAKKCmz8n ..... [long string]\ndecoded = base64.b64decode(encoded_weights)\nbuffer = io.BytesIO(decoded)\nagent_th_model.load_state_dict(torch.load(buffer))","2792f984":"def my_agent(observation, configuration):\n      obs = np.array(observation['board'])\n      th_obs = Variable(torch.from_numpy(obs)).float()\n      y = agent_th_model(th_obs)\n      action = th.argmax(agent_th_model(th_obs)).item()\n      if observation.board[action] == 0:\n          return action\n      else:\n          return random.choice([c for c in range(configuration.columns) if observation.board[c] == 0])\n","b9d3f59b":"kaggle_env = gym_env.get_kaggle_env()\nkaggle_env.reset()\nkaggle_env.run([my_agent, \"negamax\"])\nkaggle_env.render(mode=\"ipython\", width=500, height=450)","dc845d39":"from kaggle_environments import evaluate\n\ndef mean_reward(rewards):\n    return sum(r[0] for r in rewards) \/ sum(r[0] + r[1] for r in rewards)\n\n# Run multiple episodes to estimate its performance.\nprint(\"My Agent vs Random Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"random\"], num_episodes=10)))\nprint(\"My Agent vs Negamax Agent:\", mean_reward(evaluate(\"connectx\", [my_agent, \"negamax\"], num_episodes=10)))","a10bc921":"import inspect\nimport os\n\ndef write_agent_to_file(file):\n#     with open(file, \"a\" if os.path.exists(file) else \"w\") as f:\n    with open(file, \"w\") as f:\n        f.write('import numpy as np\\n')\n        f.write('import random\\n')\n        f.write('import torch as th\\n')\n        f.write('import torch.nn as nn\\n')\n        f.write('import io\\n')\n        f.write('import base64\\n')\n        f.write('import torch\\n')\n        f.write('from torch.autograd import Variable\\n')\n\n        f.write('class PyTorchMlpPolicy(nn.Module):\\n')\n        f.write('    def __init__(self):\\n')\n        f.write('        super(PyTorchMlpPolicy, self).__init__()\\n')\n        f.write('        self.pi_fc0 = nn.Linear(42, 64)\\n')\n        f.write('        self.pi_fc1 = nn.Linear(64, 64)\\n')\n        f.write('        self.pi = nn.Linear(64, 7)\\n') \n        f.write('        self.tanh = th.tanh\\n')\n        f.write('        self.out_activ = nn.Softmax(dim=0)\\n')\n        f.write('    def forward(self, x):\\n')\n        f.write('        x = self.tanh(self.pi_fc0(x))\\n')\n        f.write('        x = self.tanh(self.pi_fc1(x))\\n')\n        f.write('        x = self.pi(x)\\n')\n        f.write('        x = self.out_activ(x)\\n')\n        f.write('        return x\\n')\n\n        f.write('agent_th_model = PyTorchMlpPolicy()\\n')\n        f.write('encoded_weights =' + str(encoded_weights) + '\\n')\n        f.write('decoded = base64.b64decode(encoded_weights)\\n')\n        f.write('buffer = io.BytesIO(decoded)\\n')\n        f.write('agent_th_model.load_state_dict(torch.load(buffer))\\n')\n        \n        f.write(inspect.getsource(my_agent))\n\nwrite_agent_to_file(\"submission.py\")\n","e959fb17":"# Note: Stdout replacement is a temporary workaround.\nimport sys\nout = sys.stdout\nfrom kaggle_environments import utils\nsubmission = utils.read_file(\"\/kaggle\/working\/submission.py\")\nsubmission_agent = utils.get_last_callable(submission)\nsys.stdout = out\n\nkaggle_env.run([submission_agent, submission_agent])\nprint(\"Success!\" if kaggle_env.state[0].status == kaggle_env.state[1].status == \"DONE\" else \"Failed...\")\n\nkaggle_env.play([submission_agent, None])","1f96f294":"PyTorch serialization adapted from https:\/\/www.kaggle.com\/c\/connectx\/discussion\/126678","e7265fba":"* https:\/\/github.com\/hill-a\/stable-baselines\n* https:\/\/stable-baselines.readthedocs.io\/en\/master\/","ca80ecd1":"https:\/\/www.kaggle.com\/c\/connectx\/leaderboard","65ca2b5e":"PyTorch conversion adapted from https:\/\/github.com\/hill-a\/stable-baselines\/issues\/372"}}