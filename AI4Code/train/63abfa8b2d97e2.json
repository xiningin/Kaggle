{"cell_type":{"3c340108":"code","0849eb8d":"code","ec2dcaa8":"code","8b04f063":"code","1a2dd8cf":"code","e6cf512c":"code","2e7cb5fd":"code","579c55db":"code","0e015c58":"code","1b399ff8":"code","a3b11d26":"code","1b3f3e9f":"code","2ec48740":"code","5ba24ba9":"code","af89cb32":"code","a2747fe4":"code","fc590ebf":"code","b72b7b3c":"code","f12983d8":"code","9eab6eb8":"code","f41304b0":"code","e0885803":"code","f5453b79":"code","ff8c3d24":"code","642bbbe5":"code","e16dec4e":"code","05fb688d":"code","00746327":"code","147bc21d":"code","83e6c92b":"code","a06374ac":"code","bf449242":"code","a1d1dbc3":"code","09ce4c20":"code","a71ea46f":"code","03d34c11":"code","a8a75a93":"code","71c84cfa":"code","26f00c94":"code","b7c85388":"code","57513d24":"code","1a4e5fb8":"code","e67fc908":"code","920bcf58":"code","f64d6c7d":"code","0ccd0956":"code","46a2efe5":"code","925b218c":"markdown"},"source":{"3c340108":"import os\nimport json\nimport string\nimport numpy as np\nimport pandas as pd\nimport keras\nfrom pandas.io.json import json_normalize\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\nfrom math import floor\n\n%matplotlib inline\n\nfrom plotly import tools\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\n\nfrom sklearn import model_selection, preprocessing, metrics, ensemble, naive_bayes, linear_model\nfrom sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\nfrom sklearn.decomposition import TruncatedSVD\nimport lightgbm as lgb\n\nimport time\nfrom tqdm import tqdm\nimport math\nfrom sklearn.model_selection import train_test_split\n\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, GRU, CuDNNGRU, Conv1D\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, Concatenate, Add, Flatten, CuDNNLSTM\nfrom keras.models import Model\nfrom keras import backend as K\nfrom keras import initializers, regularizers, constraints, optimizers, layers\nfrom keras.engine.topology import Layer\nfrom keras.callbacks import ModelCheckpoint, ReduceLROnPlateau, EarlyStopping\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999\n\nfrom wordcloud import WordCloud, STOPWORDS\nfrom collections import defaultdict","0849eb8d":"import nltk \nfrom nltk.corpus import stopwords, wordnet\nfrom nltk.tokenize import word_tokenize, sent_tokenize \nstop_words = set(stopwords.words('english')) \nimport regex as re","ec2dcaa8":"puncts = [',', '.', '\"', ':', ')', '(', '-', '!', '?', '|', ';', \"'\", '$', '&', '\/', '[', ']', '>', '%', '=', '#', '*', '+', '\\\\', '\u2022',  '~', '@', '\u00a3', \n '\u00b7', '_', '{', '}', '\u00a9', '^', '\u00ae', '`',  '<', '\u2192', '\u00b0', '\u20ac', '\u2122', '\u203a',  '\u2665', '\u2190', '\u00d7', '\u00a7', '\u2033', '\u2032', '\u00c2', '\u2588', '\u00bd', '\u00e0', '\u2026', \n '\u201c', '\u2605', '\u201d', '\u2013', '\u25cf', '\u00e2', '\u25ba', '\u2212', '\u00a2', '\u00b2', '\u00ac', '\u2591', '\u00b6', '\u2191', '\u00b1', '\u00bf', '\u25be', '\u2550', '\u00a6', '\u2551', '\u2015', '\u00a5', '\u2593', '\u2014', '\u2039', '\u2500', \n '\u2592', '\uff1a', '\u00bc', '\u2295', '\u25bc', '\u25aa', '\u2020', '\u25a0', '\u2019', '\u2580', '\u00a8', '\u2584', '\u266b', '\u2606', '\u00e9', '\u00af', '\u2666', '\u00a4', '\u25b2', '\u00e8', '\u00b8', '\u00be', '\u00c3', '\u22c5', '\u2018', '\u221e', \n '\u2219', '\uff09', '\u2193', '\u3001', '\u2502', '\uff08', '\u00bb', '\uff0c', '\u266a', '\u2569', '\u255a', '\u00b3', '\u30fb', '\u2566', '\u2563', '\u2554', '\u2557', '\u25ac', '\u2764', '\u00ef', '\u00d8', '\u00b9', '\u2264', '\u2021', '\u221a', ]\ndef clean_text(x):\n    x = str(x)\n    for punct in puncts:\n        x = x.replace(punct, f' {punct} ')\n    return x","8b04f063":"train_df = pd.read_csv(\"..\/input\/train.csv\")\ntest_df = pd.read_csv(\"..\/input\/test.csv\")\nprint(\"Train shape : \",train_df.shape)\nprint(\"Test shape : \",test_df.shape)","1a2dd8cf":"# Cleaning the data\ntrain_df[\"question_text\"] = train_df[\"question_text\"].apply(lambda x: clean_text(x))\ntest_df[\"question_text\"] = test_df[\"question_text\"].apply(lambda x: clean_text(x))","e6cf512c":"countries = [\"United States\", \"France\", \"India\", \"China\", \"Russia\", \"Saudi Arabia\", \"Iran\", \\\n             \"Germany\",\"England\", \"Europe\", \"Mexico\", \"Italy\", \"Spain\", \"Turkey\", \"Syria\", \"Israel\"]\n\npoliticians = [\"Trump\", \"Hillary\", \"Pence\", \"Obama\", \"Bush\", \"Clinton\", \"Putin\", \"Merkel\"]\n\ngroups = [\"Hispanics\", \"Blacks\", \"Whites\", \"Asians\", \"Browns\", \"Arabs\", \"Natives\", \"Caucasians\", \"Africans\",\\\n         \"Indians\", \"Chineses\", \"Mexicans\", \"British\", \"Americans\", \"Spanish\", \"Greek\", \"Italians\", \\\n          \"Turkish\", \"French\", \"English\", \"Germans\", \"Russians\",\"Arabic\", \"Persians\", \"European\", \"Madeup\"]\n\npolitical_groups = [\"democrats\", \"republicans\", \"liberals\", \"socialists\", \"communists\"]\n\nreligious_groups = [\"christians\", \"muslims\", \"jews\", \"buddhists\", \"hindus\", \"israelites\", \"zionists\"]\n","2e7cb5fd":"goofy_test_questions = []","579c55db":"for country in countries:\n    goofy_test_questions.append(\"Why do not people see that \" + country + \" is a terrorist country?\")\n\nfor country1 in countries:\n    for country2 in countries:\n        goofy_test_questions.append(\"When will \" + country1 + \" end its occupation of \" + country2 + \"?\")\n        \nfor politician in politicians:\n    goofy_test_questions.append(\"How did an idiot like \" + politician + \" get elected?\")\n\nfor politician in politicians:\n    goofy_test_questions.append(\"How did a moron like \" + politician + \" get elected?\")\n\nfor politician in politicians:\n    goofy_test_questions.append(\"How did an incompetent officer like \" + politician + \" get elected?\")\n\nfor politician in politicians:\n    goofy_test_questions.append(\"How did \" + politician + \" get elected?\")\n\nfor politician in politicians:\n    goofy_test_questions.append(\"How does \" + politician + \" feel about the economic wreck he left behind?\")\n    \nfor group in groups:\n    goofy_test_questions.append(\"Are all \" + group + \" stupid?\")\n\nfor group in groups:\n    goofy_test_questions.append(\"Are \" + group + \" stupid?\")\n\nfor group in groups:\n    goofy_test_questions.append(\"Are all \" + group + \" rude?\")\n\nfor group in groups:\n    goofy_test_questions.append(\"Are \" + group + \" rude?\")\n\nfor country in countries:\n    goofy_test_questions.append(\"Why does \" + country + \" food taste so good?\")\n\nfor country in countries:\n    goofy_test_questions.append(\"Why does \" + country + \" food taste so bad?\")\n\nfor group in groups:\n    goofy_test_questions.append(\"Why do \" + group + \" people think themselves so high?\")\n\nfor group in groups:\n    goofy_test_questions.append(\"Should all \" + group + \" be spaded?\")\n\nfor group in groups:\n    goofy_test_questions.append(\"Should all \" + group + \" be sent to Mars and help build an infrastructure there to be useful for mankind and the world?\")\n\nfor group1 in groups:\n    for group2 in groups:\n        goofy_test_questions.append(\"What do \" + group1 + \" think of \" + group2 + \"?\")\n\nfor political_group in political_groups:\n    goofy_test_questions.append(\"Do \" + political_group + \" even care about the constitution?\")\n\nfor political_group in political_groups:\n    goofy_test_questions.append(\"Do \" + political_group + \" care about the country?\")\n\nfor religious_group in religious_groups:\n    goofy_test_questions.append(\"Do \" + religious_group + \" really believe in a book written by a child molester?\")\n","0e015c58":"goofy_test_y = np.ones(len(goofy_test_questions))\n\n\nfor sentence in goofy_test_questions:\n    sentence = clean_text(sentence)\n        \ngoofy_test_X = np.asarray(goofy_test_questions)","1b399ff8":"## some config values \nembed_size = 300 # how big is each word vector\nmax_features = 90000 # how many unique words to use (i.e num rows in embedding vector)\nmaxlen = 60 # max number of words in a question to use","a3b11d26":"def get_long_sentences(sent_list, cut_off_len = maxlen):\n    ids_for_long = []\n    section_list = []\n    for i, sent in enumerate(list(sent_list)):\n        for section in re.split(r'([\\w+\\s?]+[.?!])', sent):\n            if len(section) > 1:\n                section_list.append(section)\n                ids_for_long.append(i)\n    return section_list, ids_for_long","1b3f3e9f":"def get_generalized_sentences(sent_list, y_list):\n    global groups, political_groups, countries, religious_groups\n    general_X_list = []\n    general_y_list = []\n    for i, sent in enumerate(list(sent_list)):\n        for word in sent.split():\n            if (word in groups) or (word in political_groups) or (word in countries) or (word in religious_groups) :\n                new_sent = sent.replace(word, \"oovword\")\n                general_X_list.append(new_sent)\n                general_y_list.append(y_list[i])\n    return general_X_list, general_y_list","2ec48740":"def dot_product(x, kernel):\n    \"\"\"\n    Wrapper for dot product operation, in order to be compatible with both\n    Theano and Tensorflow\n    Args:\n        x (): input\n        kernel (): weights\n    Returns:\n    \"\"\"\n    if K.backend() == 'tensorflow':\n        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n    else:\n        return K.dot(x, kernel)","5ba24ba9":"class AttentionWithContext(Layer):\n    \"\"\"\n    Attention operation, with a context\/query vector, for temporal data.\n    Supports Masking.\n    Follows the work of Yang et al. [https:\/\/www.cs.cmu.edu\/~diyiy\/docs\/naacl16.pdf]\n    \"Hierarchical Attention Networks for Document Classification\"\n    by using a context vector to assist the attention\n    # Input shape\n        3D tensor with shape: `(samples, steps, features)`.\n    # Output shape\n        2D tensor with shape: `(samples, features)`.\n    How to use:\n    Just put it on top of an RNN Layer (GRU\/LSTM\/SimpleRNN) with return_sequences=True.\n    The dimensions are inferred based on the output shape of the RNN.\n    Note: The layer has been tested with Keras 2.0.6\n    Example:\n        model.add(LSTM(64, return_sequences=True))\n        model.add(AttentionWithContext())\n        # next add a Dense layer (for classification\/regression) or whatever...\n    \"\"\"\n\n    def __init__(self,\n                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n                 W_constraint=None, u_constraint=None, b_constraint=None,\n                 bias=True, **kwargs):\n\n\n        self.supports_masking = True\n        self.init = initializers.get('glorot_uniform')\n\n        self.W_regularizer = regularizers.get(W_regularizer)\n        self.u_regularizer = regularizers.get(u_regularizer)\n        self.b_regularizer = regularizers.get(b_regularizer)\n\n        self.W_constraint = constraints.get(W_constraint)\n        self.u_constraint = constraints.get(u_constraint)\n        self.b_constraint = constraints.get(b_constraint)\n\n        self.bias = bias\n        super(AttentionWithContext, self).__init__(**kwargs)\n\n\n    def build(self, input_shape):\n        assert len(input_shape) == 3\n\n        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_W'.format(self.name),\n                                 regularizer=self.W_regularizer,\n                                 constraint=self.W_constraint)\n        if self.bias:\n            self.b = self.add_weight((input_shape[-1],),\n                                     initializer='zero',\n                                     name='{}_b'.format(self.name),\n                                     regularizer=self.b_regularizer,\n                                     constraint=self.b_constraint)\n\n        self.u = self.add_weight((input_shape[-1],),\n                                 initializer=self.init,\n                                 name='{}_u'.format(self.name),\n                                 regularizer=self.u_regularizer,\n                                 constraint=self.u_constraint)\n\n        super(AttentionWithContext, self).build(input_shape)\n\n    def compute_mask(self, input, input_mask=None):\n        # do not pass the mask to the next layers\n        return None\n\n    def call(self, x, mask=None):\n        uit = dot_product(x, self.W)\n\n        if self.bias:\n            uit += self.b\n\n        uit = K.tanh(uit)\n        ait = dot_product(uit, self.u)\n\n        a = K.exp(ait)\n\n        # apply mask after the exp. will be re-normalized next\n        if mask is not None:\n            # Cast the mask to floatX to avoid float64 upcasting in theano\n            a *= K.cast(mask, K.floatx())\n\n\n        # in some cases especially in the early stages of training the sum may be almost zero\n        # and this results in NaN's. A workaround is to add a very small positive number \u03b5 to the sum.\n        # a \/= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n        a \/= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n\n        a = K.expand_dims(a)\n        weighted_input = x * a\n        return K.sum(weighted_input, axis=1)\n\n    def compute_output_shape(self, input_shape):\n        return input_shape[0], input_shape[-1]","af89cb32":"## split to train and val\ntrain_df, val_df = train_test_split(train_df, test_size=0.1, random_state=2018)\n## Get the target values\ntrain_y = train_df['target'].values\nval_y = val_df['target'].values","a2747fe4":"## fill up the missing values\ntrain_X = train_df[\"question_text\"].fillna(\"_na_\").values\nval_X = val_df[\"question_text\"].fillna(\"_na_\").values\ntest_X = test_df[\"question_text\"].fillna(\"_na_\").values","fc590ebf":"generalized_train_X, generalized_train_y = get_generalized_sentences(train_X, train_y)","b72b7b3c":"long_val_X, long_id_list = get_long_sentences(val_X)\nlong_test_X, long_test_id = get_long_sentences(test_X)","f12983d8":"## Tokenize the sentences\ntokenizer = Tokenizer(num_words=max_features)\n\ntokenizer_list = list(train_X)\n\ntokenizer.fit_on_texts(tokenizer_list)\ntrain_X = tokenizer.texts_to_sequences(train_X)\nval_X = tokenizer.texts_to_sequences(val_X)\ntest_X = tokenizer.texts_to_sequences(test_X)\n\nlong_val_X = tokenizer.texts_to_sequences(np.asarray(long_val_X))\nlong_test_X = tokenizer.texts_to_sequences(np.asarray(long_test_X))\n\ngeneralized_train_X = tokenizer.texts_to_sequences(np.asarray(generalized_train_X))\ngeneralized_train_y = np.asarray(generalized_train_y)","9eab6eb8":"goofy_test_X = tokenizer.texts_to_sequences(goofy_test_X)","f41304b0":"## Pad the sentences for short\ntrain_X = pad_sequences(train_X, maxlen=maxlen)\nval_X = pad_sequences(val_X, maxlen=maxlen)\ntest_X = pad_sequences(test_X, maxlen=maxlen)\n\nlong_val_X = pad_sequences(long_val_X, maxlen=maxlen)\nlong_test_X = pad_sequences(long_test_X, maxlen=maxlen)\ngeneralized_train_X = pad_sequences(generalized_train_X, maxlen=maxlen)","e0885803":"goofy_test_X = pad_sequences(goofy_test_X, maxlen=maxlen)","f5453b79":"EMBEDDING_FILE = '..\/input\/embeddings\/glove.840B.300d\/glove.840B.300d.txt'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE))\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nrandom_vector = np.random.rand(300)\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix1 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: \n        embedding_matrix1[i] = embedding_vector\n    else:\n        embedding_matrix1[i] = random_vector","ff8c3d24":"EMBEDDING_FILE = '..\/input\/embeddings\/wiki-news-300d-1M\/wiki-news-300d-1M.vec'\ndef get_coefs(word,*arr): return word, np.asarray(arr, dtype='float32')\nembeddings_index = dict(get_coefs(*o.split(\" \")) for o in open(EMBEDDING_FILE) if len(o)>100)\n\nall_embs = np.stack(embeddings_index.values())\nemb_mean,emb_std = all_embs.mean(), all_embs.std()\nembed_size = all_embs.shape[1]\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix2 = np.random.normal(emb_mean, emb_std, (nb_words, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None: embedding_matrix2[i] = embedding_vector","642bbbe5":"inp = Input(shape=(maxlen,))\nmodel1_out = Embedding(max_features, embed_size, weights=[embedding_matrix1],trainable=False)(inp)\nmodel1_out = Bidirectional(CuDNNGRU(128, return_sequences=True))(model1_out)\nmodel1_out = AttentionWithContext()(model1_out)\nmodel1_out = Dense(64, activation=\"relu\")(model1_out)\nmodel1_out = Dropout(0.1)(model1_out)\nmodel1_out = Dense(32, activation=\"relu\")(model1_out)\nmodel1_out = Dropout(0.1)(model1_out)\nmodel1_out = Dense(1, activation=\"sigmoid\")(model1_out)\nmodel = Model(inputs=inp, outputs=model1_out)\nmodel.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model.summary())","e16dec4e":"inp = Input(shape=(maxlen,))\nmodel2_out = Embedding(max_features, embed_size, weights=[embedding_matrix2],trainable=False)(inp)\nmodel2_out = Bidirectional(CuDNNGRU(128, return_sequences=True))(model2_out)\nmodel2_out = AttentionWithContext()(model2_out)\nmodel2_out = Dense(64, activation=\"relu\")(model2_out)\nmodel2_out = Dropout(0.1)(model2_out)\nmodel2_out = Dense(32, activation=\"relu\")(model2_out)\nmodel2_out = Dropout(0.1)(model2_out)\nmodel2_out = Dense(1, activation=\"sigmoid\")(model2_out)\nmodel2 = Model(inputs=inp, outputs=model2_out)\nmodel2.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(model2.summary())","05fb688d":"def train_model(model, all_train_X, all_train_y, all_val_X, all_val_y, epochs=2):\n    filepath=\"weights_best.h5\"\n    checkpoint = ModelCheckpoint(filepath, monitor='val_loss', verbose=2, save_best_only=True, mode='min')\n    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=1, min_lr=0.0001, verbose=2)\n    earlystopping = EarlyStopping(monitor='val_loss', min_delta=0.0001, patience=2, verbose=2, mode='auto')\n    callbacks = [checkpoint, reduce_lr]\n    for e in range(epochs):\n        model.fit(all_train_X, all_train_y, batch_size=1024, epochs=1, validation_data=(all_val_X, all_val_y), callbacks=callbacks)\n    model.load_weights(filepath)\n    return model","00746327":"model = train_model(model, train_X, train_y, val_X, val_y, epochs=12)","147bc21d":"pred_val_y = model.predict(val_X, batch_size=1024, verbose=1)","83e6c92b":"model2 = train_model(model2, train_X, train_y, val_X, val_y, epochs=5)","a06374ac":"alt_pred_val_y = model2.predict(val_X, batch_size=1024, verbose=1)","bf449242":"'''\nA function specific to this competition since the organizers don't want probabilities \nand only want 0\/1 classification maximizing the F1 score. This function computes the best F1 score by looking at val set predictions\n'''\n\ndef f1_smart(y_true, y_pred):\n    thresholds = []\n    for thresh in np.arange(0.1, 0.901, 0.01):\n        thresh = np.round(thresh, 2)\n        res = metrics.f1_score(y_true, (y_pred > thresh).astype(int))\n        thresholds.append([thresh, res])\n        print(\"F1 score at threshold {0} is {1}\".format(thresh, res))\n\n    thresholds.sort(key=lambda x: x[1], reverse=True)\n    best_thresh = thresholds[0][0]\n    best_f1 = thresholds[0][1]\n    print(\"Best threshold: \", best_thresh)\n    return  best_f1, best_thresh","a1d1dbc3":"f1, threshold = f1_smart(val_y, pred_val_y)\nprint('Optimal F1: {} at threshold: {}'.format(f1, threshold))","09ce4c20":"f1, alt_threshold = f1_smart(val_y, alt_pred_val_y)\nprint('Optimal F1: {} at threshold: {}'.format(f1, alt_threshold))","a71ea46f":"long_pred_val_y = model.predict(long_val_X, batch_size=1024, verbose=1)","03d34c11":"def update_preds(pred_val_y, long_pred_val_y, long_id_list):\n    global threshold\n    threshold_range = 0.10\n    copy_pred_val_y = pred_val_y\n    for i, pred in enumerate(list(pred_val_y)):\n        if (pred < (threshold + threshold_range)) and (pred > (threshold - threshold_range)):\n        # We do this extra step only if the model is not confident about the pred\n            count = 1\n            sum_pred = pred\n            for long_id, long_pred in zip(long_id_list, long_pred_val_y.tolist()):\n                if long_id == i:\n                    if (long_pred > (threshold + threshold_range)) or (long_pred < (threshold - threshold_range)):\n                        if long_pred[0] > pred:\n                    # We keep the pred that is closer to insincere                     \n                            copy_pred_val_y[i] = long_pred[0]\n    return copy_pred_val_y","a8a75a93":"pred_val_y = update_preds(pred_val_y, long_pred_val_y, long_id_list)","71c84cfa":"f1, threshold = f1_smart(val_y, pred_val_y)\nprint('Optimal F1: {} at threshold: {}'.format(f1, threshold))","26f00c94":"all_train_X = np.concatenate((train_X,val_X),axis=0)\nall_train_y = np.concatenate((train_y,val_y),axis=0)\n\nall_train_X = np.concatenate((train_X,generalized_train_X),axis=0)\nall_train_y = np.concatenate((train_y,generalized_train_y),axis=0)\n\nmodel = train_model(model, train_X, train_y, val_X, val_y, epochs=5)","b7c85388":"def seq_ensemble(pred_y, alt_pred_y):\n    global threshold, alt_threshold\n    threshold_range_list = [0.15,0.12,0.10,0.08,0.05,0.03,0.02,0.01]\n    copy_pred_val_y = pred_val_y\n    for threshold_range in threshold_range_list:\n        for i, pred_pair in enumerate(zip(list(pred_val_y),list(alt_pred_y))):\n            if (pred_pair[0] < (threshold + threshold_range)) and \\\n                            (pred_pair[0] > (threshold - threshold_range)):\n                if (pred_pair[1] > (alt_threshold + threshold_range)) or \\\n                            (pred_pair[1] < (alt_threshold - threshold_range)):\n                    copy_pred_val_y[i] = pred_pair[1]    \n    return copy_pred_val_y","57513d24":"pred_val_y = seq_ensemble(pred_val_y, alt_pred_val_y)","1a4e5fb8":"f1, threshold = f1_smart(val_y, pred_val_y)\nprint('Optimal F1: {} at threshold: {}'.format(f1, threshold))","e67fc908":"pred_test_y = model.predict(test_X, batch_size=1024, verbose=1)","920bcf58":"long_pred_test_y = model.predict(long_test_X, batch_size=1024, verbose=1)","f64d6c7d":"pred_test_y = update_preds(pred_test_y, long_pred_test_y, long_test_id)","0ccd0956":"pred_test_y = (pred_test_y>threshold).astype(int)\nout_df = pd.DataFrame({\"qid\":test_df[\"qid\"].values})\nout_df['prediction'] = pred_test_y\nout_df.to_csv(\"submission.csv\", index=False)","46a2efe5":"pred_goofy_y = model.predict(goofy_test_X, batch_size=10, verbose=1)\nf1, threshold = f1_smart(goofy_test_y, pred_goofy_y)\nprint('Optimal F1: {} at threshold: {}'.format(f1, threshold))\n\nfor question,pred in zip(goofy_test_questions, pred_goofy_y): \n    print(pred, \" \", question, \"\\n\")","925b218c":"The purpose of this kernel is to study the effect of question lengths. There are many questions made up of multiple sentences. Most of those are insincere. When we pad (trim) the sequences to certain length, we may be trimming off the insincere component of an overall insincere question. This will eventually confuse the training and lead to misleading results (supposedly). To overcome this, I thought of feeding different parts of the sentence independently to the model for prediction and then either average all the predictions for that one sentence or do a weighted sum based on how confident the pred is. In this case, I used a small maxlen so that we still have decent number of questions over the maxlen limit. As it turns out, there aren't many. The results improved only incrementally. The trimming of lengthy sentences turned out to be a small issue. It might still help to feed length as a separate auxilliary input, but it does not pay off to process the remainder of the sentence.\n\nThis is my first public kernel, so please excuse the format. I would like to thank all the others who shared their fantastic kernels, especially Rahul Agarwal for sharing the basis for this one."}}