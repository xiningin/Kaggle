{"cell_type":{"37ebc869":"code","3be23da2":"code","fab30511":"code","1f91ed11":"code","c399fd8a":"code","92e97afd":"code","42d97c7f":"code","919990bd":"code","d019bd70":"code","b9abec12":"code","4913cc92":"code","c9dd2b43":"code","ea43381b":"code","adaceddf":"code","f763cc82":"code","df5f0144":"code","e8e7dbe9":"code","145bd779":"code","332f682a":"code","b5dcfb8d":"code","a340468e":"code","74bca27d":"code","bdac3ebc":"code","b646f4eb":"code","efa882bb":"code","5877af70":"code","055fd088":"code","1286f2f6":"markdown","02c756db":"markdown","e4f3a350":"markdown","f346d3dc":"markdown","71c6814a":"markdown","a971c061":"markdown","df857c28":"markdown","f0879e4e":"markdown","4ff49ee5":"markdown","97e92867":"markdown","a4dc2e0e":"markdown"},"source":{"37ebc869":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nfrom sklearn.model_selection import train_test_split # split train and test data\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","3be23da2":"class ML_LinearRegression:\n    \"\"\" Apply Linear Regression\n        \n        Args: \n            lr : float. learning rate \n            iterations : int. How many iteration for training \n            regularization : Boolen. Use L2 regularization for model training\n            lambda_reg : float. Regularization parameter \n            \n    \"\"\"\n    def __init__(self,lr = 0.5, iterations = 10000, regularization = False, lambda_reg = None ):\n        self.lr = lr \n        self.iterations = iterations\n        self.regularization = regularization\n        self.lambda_reg = lambda_reg\n        \n    def fit(self,x,y):\n        \"\"\" Fit the our model\n        \n            Args: \n                x : np.array, shape = [n_samples, n_features]. Training Data \n                y : np.array, shape = [n_samples, n_conclusion]. Target Values\n                \n            Returns: \n                self : object\n        \"\"\"\n        \n        self.cost_list = []\n        self.theta = np.zeros((x.shape[1],1))   \n        #self.theta_zero = np.zeros((1,1))\n        m = x.shape[0]   # samples in the data\n        name_list=[]     # for plot x-axis name\n        \n        for i in range(self.iterations):  # Feed forward\n            \n            h_pred = np.dot(x,self.theta)     \n            error = h_pred - y \n            if self.regularization == False: \n                \n                cost = 1\/(2*m)*(np.sum((error ** 2)))\n                gradient_vector = np.dot(x.T, error)\n                self.theta -= (self.lr\/m) * gradient_vector # Gradient Descent\n            \n            else:\n                cost = 1\/(2*m)*(np.sum((error ** 2))) + (self.lambda_reg * (np.sum((self.theta ** 2 ))))  # add a L2 regularization\n                gradient_vector = np.dot(x.T, error)\n                self.theta -= (self.lr\/m) * gradient_vector - ((self.lambda_reg\/m) * self.theta) # Gradient Descent with L2 regularization\n                \n            self.cost_list.append(cost)\n            name_list.append(i)\n        \n        plt.scatter(name_list,self.cost_list)\n    \n        return self\n    \n    def predict(self, x):\n        \"\"\" Predicts the value after the model has been trained.\n        \n            Args: \n                x: np.array, shape = [n_samples, n_features]. Training Data\n                \n            Returns: \n                Predicted value \n        \"\"\"\n        \n        \n        return np.dot(x,self.theta)","fab30511":"dataset = pd.read_csv(\"..\/input\/housesalesprediction\/kc_house_data.csv\") #read dataset from .csv file","1f91ed11":"dataset.head(10)","c399fd8a":"dataset.iloc[:,1]","92e97afd":"dataset.info()","42d97c7f":"dropped_dataset = dataset.drop(['id','date','floors','waterfront','view','condition','grade','sqft_above','sqft_basement','yr_built',\n                               'yr_renovated','zipcode','lat','long','sqft_living15','sqft_lot15'],axis=1) # drop some features","919990bd":"sorted_df = dropped_dataset.sort_values('price') # sorted data by prices for visualisation","d019bd70":"dataset_x = sorted_df.drop(['price'],axis=1)\ndataset_y = sorted_df.price.values","b9abec12":"dataset_x.head()","4913cc92":"plt.figure(figsize=(19,4))\n\nplt.subplot(141)\nplt.scatter(dataset_x['bedrooms'],dataset_y)\nplt.subplot(142)\nplt.scatter(dataset_x['bathrooms'],dataset_y)\nplt.subplot(143)\nplt.scatter(dataset_x['sqft_living'],dataset_y)\nplt.subplot(144)\nplt.scatter(dataset_x['sqft_lot'],dataset_y)\n\nplt.show()","c9dd2b43":"dataset_x = (dataset_x - np.min(dataset_x)) \/ (np.max(dataset_x) - np.min(dataset_x))","ea43381b":"x_train, x_test, y_train, y_test = train_test_split(dataset_x, dataset_y, test_size= 0.2, random_state= 42)","adaceddf":"y_train = y_train.reshape((len(y_train), 1))\nprint(x_train.shape, y_train.shape)","f763cc82":"linear = ML_LinearRegression(iterations=1000)\nlinear.fit(x_train,y_train)","df5f0144":"linear_with_reg = ML_LinearRegression(iterations=1000,regularization=True, lambda_reg= 0.001)\nlinear_with_reg.fit(x_train,y_train)","e8e7dbe9":"linear_with_reg = ML_LinearRegression(iterations=1000,regularization=True, lambda_reg= 0.01)\nlinear_with_reg.fit(x_train,y_train)","145bd779":"y_pred_mymodel = linear.predict(x = x_test)","332f682a":"y_pred_mymodel_reg = linear_with_reg.predict(x=x_test)","b5dcfb8d":"plt.figure(figsize=(20,4))\n\n# Visualization of y_test\nplt.subplot(141) \nplt.plot(list(range(len(y_test))), y_test) \n\n# Visualization of difference between y_test and y_pred_mymodel\nplt.subplot(142)\nplt.plot(list(range(len(y_test))), y_test, 'b')\nplt.plot(list(range(len(y_pred_mymodel))), y_pred_mymodel, 'r')\n\n# Visualization of difference between y_test and y_pred_mymodel_reg\nplt.subplot(143)\nplt.plot(list(range(len(y_test))), y_test, 'b')\nplt.plot(list(range(len(y_pred_mymodel_reg))), y_pred_mymodel_reg, 'r')\n\n# Visualization of difference between y_pred_mymodel and y_pred_mymodel_reg\nplt.subplot(144)\nplt.plot(list(range(len(y_pred_mymodel))), y_pred_mymodel, 'b')\nplt.plot(list(range(len(y_pred_mymodel_reg))), y_pred_mymodel_reg, 'r')\n\nplt.show()","a340468e":"from sklearn.linear_model import LinearRegression","74bca27d":"regression = LinearRegression()","bdac3ebc":"regression.fit(x_train,y_train)","b646f4eb":"y_pred_sklearn = regression.predict(x_test)","efa882bb":"plt.plot(list(range(len(y_test))), y_test, 'b')\nplt.plot(list(range(len(y_pred_sklearn))), y_pred_sklearn, 'r')\nplt.show()","5877af70":"plt.plot(list(range(len(y_pred_sklearn))), y_pred_sklearn, 'b')\nplt.plot(list(range(len(y_pred_mymodel))), y_pred_mymodel, 'r')\nplt.show()","055fd088":"plt.plot(list(range(len(y_pred_sklearn))), y_pred_sklearn, 'b')\nplt.plot(list(range(len(y_pred_mymodel_reg))), y_pred_mymodel_reg, 'r')\nplt.show()","1286f2f6":"## 2.5 Train","02c756db":"### 2.5.1 Train without Regularization","e4f3a350":"# 1. Libs\n# ## 1.1 Model Libs","f346d3dc":"## 2.3 Normalization","71c6814a":"## 2.2 Visualization of all features according to pricing","a971c061":"# 3. Compare My Code and Sklearn Results","df857c28":"## 2.6 Test","f0879e4e":"### 2.6.1 Compare Result","4ff49ee5":"# 2. Start \n# ## 2.1 Data Preprocessing","97e92867":"## 2.4 Split Dataset","a4dc2e0e":"### 2.5.2 Train with Regularization"}}