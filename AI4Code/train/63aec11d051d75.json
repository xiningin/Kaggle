{"cell_type":{"cec7fa59":"code","83af8065":"code","590bb2ac":"code","8fa19645":"code","b3265bc5":"code","c3eb3ca6":"code","581d61c7":"code","ca049510":"code","f72c55f9":"code","387b5a6e":"code","3acec95c":"code","4070a869":"code","2b67ccec":"code","fccd9f20":"code","1acebee3":"code","94afbf2e":"code","2c8cf19f":"code","b8c9981f":"code","093b428b":"code","cb8cb42b":"code","43e0a3f4":"code","35d87959":"code","c1df84a5":"code","ffca6181":"code","7b987a1b":"code","78dd5256":"code","e2fc22c4":"markdown","c38bee44":"markdown","56d90882":"markdown","ed7fc07e":"markdown","22f62272":"markdown","3b93686a":"markdown","6804a2c6":"markdown","6d8fdbbf":"markdown","afd4cdb1":"markdown","b54d0073":"markdown","15e9e537":"markdown","dab9f53c":"markdown","d827b6db":"markdown","e9f09fca":"markdown","c62e71cd":"markdown","0919fd7a":"markdown","46785c45":"markdown","3ac01771":"markdown","672e5986":"markdown","fdaa3860":"markdown","50318a3e":"markdown"},"source":{"cec7fa59":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt \nimport statsmodels.api as sm\nfrom sklearn.model_selection import train_test_split\nimport seaborn as sns\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","83af8065":"path = \"..\/input\/dont-overfit-ii\/train.csv\"","590bb2ac":"data = pd.read_csv(path)","8fa19645":"data.head()","b3265bc5":"data.describe()","c3eb3ca6":"data.count()","581d61c7":"data.corr()\ncor = data.iloc[:,0:5].corr()\nprint(cor)\n","ca049510":"train, test = train_test_split(data, test_size = 0.3)\nprint(train.shape)\nprint(test.shape)\n","f72c55f9":"train_x = train.iloc[:,0:120]; train_y = train.iloc[:,120]\ntest_x  = test.iloc[:,0:120];  test_y = test.iloc[:,120]","387b5a6e":"print(train_x.shape)\nprint(train_y.shape)\nprint(test_x.shape)\nprint(test_y.shape)\n","3acec95c":"train_x.head()\ntrain_y.head()\n","4070a869":"train_y.head(20)","2b67ccec":"train.head()","fccd9f20":"train.dtypes","1acebee3":"lm1 = sm.OLS(train_y, train_x).fit()\n","94afbf2e":"pdct1 = lm1.predict(test_x)\nprint(pdct1)","2c8cf19f":"actual = list(test_y.head(50))\ntype(actual)","b8c9981f":"predicted = np.round(np.array(list(pdct1.head(50))),2)\nprint(predicted)\ntype(predicted)","093b428b":"df_results = pd.DataFrame({'actual':actual, 'predicted':predicted})\nprint(df_results.head(115))","cb8cb42b":"from sklearn import metrics  \nprint('Mean Absolute Error:', metrics.mean_absolute_error(test_y, pdct1))  \nprint('Mean Squared Error:', metrics.mean_squared_error(test_y, pdct1))  \nprint('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(test_y, pdct1))) ","43e0a3f4":"def RMSE(predict, target):\n    return np.sqrt(((predict - target) ** 2).mean())\nprint ('My RMSE: ' + str(RMSE(test_y,pdct1)) )","35d87959":"def MAPE(predict,target):\n    return ( abs((target - predict) \/ target).mean()) * 100\nprint ('My MAPE: ' + str(MAPE(test_y,pdct1)) )","c1df84a5":"import math\n\n#A function to calculate Root Mean Squared Logarithmic Error (RMSLE)\ndef rmsle(pdct1, test_y):\n    assert len(pdct1) == len(test_y)\n    terms_to_sum = [(math.log(y_pred[i] + 1) - math.log(y[i] + 1)) ** 2.0 for i,pred in enumerate(test_y)]\n    return (sum(terms_to_sum) * (1.0\/len(pdct1))) ** 0.5\nprint ('My RMSLE: ' + str(RMSE(test_y,pdct1)) )","ffca6181":"from sklearn.metrics import mean_absolute_error\nprint ('Sk MAE: ' + str(mean_absolute_error(pdct1,test_y)) )\ndef MAE(predict,target):\n    return (abs(predict-target)).mean()\nprint ('My MAE: ' + str(MAE(test_y,pdct1)))","7b987a1b":"def R2(predict, target):\n    return 1 - (MAE(predict,target) \/ MAE(target.mean(),target))\ndef R_SQR(predict, target):\n    r2 = R2(predict,target)\n    return np.sqrt(r2)\nprint ('My R2         : ' + str(R2(test_y,pdct1)) )\nprint ('My R          : ' + str(R_SQR(test_y,pdct1)) )","78dd5256":"def R2_ADJ(predict, target, k):\n    r2 = R2(predict,target)\n    n = len(target)\n    return (1 -  ( (1-r2) *  ( (n-1) \/ (n-(k+1)) ) ) )\nk= len(data.columns)\nprint ('My R2 adjusted: ' + str(R2_ADJ(test_y,pdct1,k)) )","e2fc22c4":"# RMSE: Root mean square error\n \n# RMSE is a quadratic scoring rule that also measures the average magnitude of the error. It\u2019s the square root of the average of squared differences between prediction and actual observation.\n","c38bee44":"# MAE : MEAN ABSOLUTE ERROR\n\n# MAE measures the average magnitude of the errors in a set of predictions, without considering their direction. It\u2019s the average over the test sample of the absolute differences between prediction and actual observation where all individual differences have equal weight.\n\n# The mean absolute error uses the same scale as the data. This is known as a scale-dependent accuracy measure and, therefore, cannot be used to make comparisons between series using different scales.\n\n","56d90882":"# split the train and test into X and Y variables\n# ------------------------------------------------\n","ed7fc07e":"# Comparision Root mean square error vs Mean square error\n\n# Similarities:\n\n# Express average model prediction error in same units of the variable of interest.\n\n# Can range from 0 to \u221e and are indifferent to the direction of errors.\n\n# Lower values are better.","22f62272":"# ensure that the X variables are all numeric for regression\n# ---------------------------------------------","3b93686a":"# >  LINEAR REGRESSION","6804a2c6":"\n\n# to find the correlation among variables (Multicollinearity)\n\n\n","6d8fdbbf":"# To add the constant term A (Y = A + B1X1 + B2X2 + ... + BnXn)\n# Xn = ccomp,slag,flyash.....\n# ----------------------------------------------------------\n","afd4cdb1":"\n# Prediction\n# -----------------","b54d0073":"# MAPE: Mean absolute percentage error\n\n# Measure of prediction accuracy of a forecasting method in statistics, for example in trend estimation, also used as a loss function for regression problems in machine learning. It usually expresses accuracy as a percentage.\n\n# Although the concept of MAPE sounds very simple and convincing, it has major drawbacks in practical application, and there are many studies on shortcomings and misleading results from MAPE","15e9e537":"# here we go ahead with 60:40 train test split and that mean absoulate percentage error is more than 51% that is inaccurate forcasting.An inaccurate forecast of flurries might not cause a problem if they don't materialize, but an inaccurate forecast of a foot of snow \u2013 in either direction \u2013 has serious consequences. Inaccurate sales forecasts are legendary. ... They lack a functional, sales-specific CRM or Pipeline Management application.","dab9f53c":"# ##To Check the Accuracy:\n#-----------------------------","d827b6db":"# RMSE VS MSE:\n\n# Difference:\n\n# Taking the square root before they are averaged, RMSE gives a relatively high weight to large errors, so RMSE should be useful when large errors are undesirable.","e9f09fca":"# R\u00b2 and R-Squared: Coefficient of determination\n\n# R\u00b2 and R-Squared help us to know how good our regression model as compared to a very simple model that just predicts the mean value of target from the train set as predictions.","c62e71cd":"# store the actual and predicted values in a dataframe for comparison\n# --------------------------------------------","0919fd7a":"# Actual vs Predicted\n#-----------------------","46785c45":"# MSE: MEAN SQUARE ERROR\n\n# MSE is a risk function, corresponding to the expected value of the squared error loss. The fact that MSE is almost always strictly positive (and not zero) is because of randomness or because the estimator does not account for information that could produce a more accurate estimate. The MSE is a measure of the quality of an estimator \u2014 it is always non-negative, and values closer to zero are better.\n\n# \u201cMinimizing MSE is a key criterion in selecting estimators: see minimum mean-square error. Among unbiased estimators, minimizing the MSE is equivalent to minimizing the variance, and the estimator that does this is the minimum variance unbiased estimator. However, a biased estimator may have lower MSE; see estimator bias.\n\n\n# In statistical modelling the MSE can represent the difference between the actual observations and the observation values predicted by the model. In this context, it is used to determine the extent to which the model fits the data as well as whether removing some explanatory variables is possible without significantly harming the model\u2019s predictive ability.\u201d\n","3ac01771":"# RMSLE: Root Mean Squared Logarithmic Error\n\n# In case of RMSLE, you take the log of the predictions and actual values. So basically, what changes is the variance that you are measuring. I believe RMSLE is usually used when you don\u2019t want to penalize huge differences in the predicted and the actual values when both predicted and true values are huge numbers.\n\n# RMSLE measures the ratio between actual and predicted.\n\n# log(pi+1)\u2212log(ai+1)log(pi+1)\u2212log(ai+1)\n\n# can be written as log((pi+1)\/(ai+1))log((pi+1)\/(ai+1))\n\n# It can be used when you don\u2019t want to penalize huge differences when the values are huge numbers.\n\n# Also, this can be used when you want to penalize under estimates more than over estimates.","672e5986":"# As above we can see most of the series data is in right skewed","fdaa3860":"Sources https:\/\/towardsdatascience.com\/metrics-and-python-850b60710e0c","50318a3e":"# Adjusted R\u00b2\n\n# A model performing equal to baseline would give R-Squared as 0. Better the model, higher the r2 value. The best model with all correct predictions would give R-Squared as 1. However, on adding new features to the model, the R-Squared value either increases or remains the same. R-Squared does not penalize for adding features that add no value to the model. So an improved version over the R-Squared is the adjusted R-Squared"}}