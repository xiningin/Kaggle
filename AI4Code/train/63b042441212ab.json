{"cell_type":{"f5c1637d":"code","77bed96c":"code","42238bd6":"code","099085e6":"code","db19947c":"code","828debba":"code","162cd1bd":"code","c4dc88cb":"code","7f2b5b71":"code","69b26ab4":"code","ece79e67":"code","1318b5b2":"code","3c987092":"code","35813c40":"code","10e11ef9":"code","a89ef212":"code","8c7f9e9b":"code","90ad8563":"code","490ceca8":"code","424aafbc":"code","01206492":"code","0f7fedfb":"code","2958aa3e":"markdown","e7a96170":"markdown","092f22f0":"markdown","a0be1f80":"markdown","19f52659":"markdown","0b18b386":"markdown","48f6df61":"markdown","c582664b":"markdown","37e05fb1":"markdown","9328ecdc":"markdown","6276f9a9":"markdown","f27593a8":"markdown","b125f787":"markdown","5dad5566":"markdown","882d02b4":"markdown"},"source":{"f5c1637d":"# data\nimport pandas as pd\n\n# visualizations\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nimport pycountry\nimport plotly.express as px\n\n# preprocessing\nimport numpy as np\nfrom sklearn.decomposition import PCA\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler,FunctionTransformer\n\n# clusters models\nfrom sklearn.cluster import KMeans, AgglomerativeClustering, SpectralClustering, DBSCAN\nfrom sklearn import metrics\nfrom sklearn.metrics import silhouette_score, silhouette_samples\n#from sklearn.metrics.pairwise import cosine_similarity\n#from scipy.cluster.hierarchy import dendrogram, linkage","77bed96c":"info = pd.read_csv('..\/input\/unsupervised-learning-on-country-data\/data-dictionary.csv')","42238bd6":"for i, row in info.iterrows():\n  print(row['Column Name'],' ---> ', row.Description)","099085e6":"CountryData = pd.read_csv('..\/input\/unsupervised-learning-on-country-data\/Country-data.csv')\nCountryData.head()","db19947c":"CountryData.isnull().sum()","828debba":"CountryData.describe()","162cd1bd":"features = CountryData.columns[1:]\n\nfeatures_group1 = ['child_mort','exports']\nfeatures_group2 = list(set(features)-set(features_group1))\n\ng1_transformer = Pipeline(steps=[\n    ('log', FunctionTransformer(np.log1p)),\n    ('scaler', StandardScaler())\n    ])\n\ng2_transformer = Pipeline(steps=[\n    ('scaler', StandardScaler())\n    ])\n\npreprocessor = ColumnTransformer(\n    transformers=[\n        ('group1', g1_transformer, features_group1),\n        ('group2', g2_transformer, features_group2),\n        ])","c4dc88cb":"preprocessor.fit(CountryData) \nnp_data = preprocessor.transform(CountryData) \ndf_data = pd.DataFrame(np_data, columns=features)","7f2b5b71":"#Using Pearson Correlation\nplt.figure(figsize=(12,10))\ncorr_m = CountryData.drop(['country'],axis=1).corr()\nsns.heatmap(corr_m, annot=True, cmap=plt.cm.Reds).set_title('Correlation Matrix')\nplt.show()","69b26ab4":"data = df_data.loc[:, features].values\n\n\n# created a covariance matrix on the standardized data. \nmatrix_cov = np.cov(data.T)\n\n# eigendecomposition on covariance matrix\neig_vals, eig_vecs = np.linalg.eig(matrix_cov)","ece79e67":"for i in range(len(eig_vals)):\n    print(eig_vals[i], eig_vals[i]\/np.sum(eig_vals))","1318b5b2":"#Explained variance\npca = PCA().fit(data)\nplt.plot(np.cumsum(pca.explained_variance_ratio_))\nplt.xlabel('number of components')\nplt.ylabel('cumulative explained variance')\nplt.show()","3c987092":"pca = PCA(n_components=3)\nprincipalComponents = pca.fit_transform(data)\nprincipalDf = pd.DataFrame(data = principalComponents)\n","35813c40":"def ISO_Code(df):\n    # Countries that are not in the ISO Code\n    df['RightCountry'] = [item.replace('Cape Verde', 'Cabo Verde')\n                              .replace('Congo, Dem. Rep.', 'Congo, The Democratic Republic of the')\n                              .replace('Congo, Rep.', 'Republic of the Congo')\n                              .replace('Macedonia, FYR', 'North Macedonia')\n                              .replace('Micronesia, Fed. Sts.', 'Micronesia, Federated States of')\n                              .replace('South Korea', 'Korea, Republic of')\n                              .replace('St. Vincent and the Grenadines', 'Saint Vincent and the Grenadines') for item in df.country]\n\n    list_countries = df['RightCountry'].unique().tolist()\n\n    d_country_code = {}  # To hold the country names and their ISO\n    for country in list_countries:\n        try:\n            country_data = pycountry.countries.search_fuzzy(country)\n            country_code = country_data[0].alpha_3\n            d_country_code.update({country: country_code})\n        except:\n            print('could not add ISO 3 code for ->', country)\n            # If could not find country, make ISO code ' '\n            d_country_code.update({country: ' '})\n\n    for k, v in d_country_code.items():\n        df.loc[(df.RightCountry == k), 'iso_alpha'] = v \n\n    df.loc[df.country.tolist().index('Niger'),'iso_alpha']='NER'\n    return df\n\ndef get_map(df):\n    df = ISO_Code(df)\n    fig = px.choropleth(data_frame = df,\n                        locations= \"iso_alpha\",\n                        color= \"cluster\",  # value in column 'Confirmed' determines color\n                        hover_name= \"country\",\n                        color_continuous_scale= 'RdYlGn_r',  #  color scale red, yellow green\n                        )\n\n    fig.show()\n\ndef get_datavisual(df,np_data):\n    # Create PCA for data visualization \/ Dimensionality reduction to 2D graph\n    from sklearn.decomposition import PCA\n\n    pca = PCA(n_components=2)\n    pca_model = pca.fit_transform(np_data)\n    data_transform = pd.DataFrame(data = pca_model, columns = ['PCA1', 'PCA2'])\n    data_transform['Cluster'] = df.cluster\n\n    plt.figure()\n    g = sns.scatterplot(data=data_transform, x='PCA1', y='PCA2', palette=sns.color_palette()[:int(df.cluster.nunique())], hue='Cluster')\n    title = plt.title('Countries Clusters with PCA')","10e11ef9":"def run_model(n_c, np_data):\n  # Create and fit model\n  kmeans = KMeans(n_clusters=n_c,\n                  init='k-means++',\n                  max_iter=400, \n                  n_init=80, \n                  random_state=0)\n  model = kmeans.fit(np_data)\n\n  df = CountryData.copy()\n  df['cluster'] = model.labels_\n\n  return df","a89ef212":"np_data = principalDf\nSum_of_squared_distances = []\nfor k in range(1, 10):\n    km = KMeans(n_clusters=k, \n                init='k-means++',\n                max_iter=400, \n                n_init=80, \n                random_state=0\n                ).fit(np_data)\n    Sum_of_squared_distances.append(km.inertia_)\n\n#plt.figure(figsize=(10,10))\nplt.plot(range(1, 10), Sum_of_squared_distances, 'bx-')\nplt.xlabel('k')\nplt.ylabel('Sum_of_squared_distances')\nplt.title('Elbow Method For Optimal k')\nplt.show()","8c7f9e9b":"df_km3PCA = run_model(3, principalDf)\ndf_km3PCA.head()","90ad8563":"print(df_km3PCA.cluster.value_counts())\ndf_km3PCA.cluster.hist()\nplt.tight_layout()","490ceca8":"get_datavisual(df_km3PCA, np_data)","424aafbc":"get_map(df_km3PCA)","01206492":"df_km3PCA.groupby(['cluster']).mean()","0f7fedfb":"from pylab import *\n\ncols = df_km3PCA.columns[1:-1]\nfor col in cols:\n    y0 = sorted(df_km3PCA[df_km3PCA.cluster==0][col].tolist())\n    y1 = sorted(df_km3PCA[df_km3PCA.cluster==1][col].tolist())\n    y2 = sorted(df_km3PCA[df_km3PCA.cluster==2][col].tolist())\n\n    plt.plot(range(len(y0)),y0,'.', linewidth=4, color='b')\n    plt.plot(range(len(y1)),y1, '.', linewidth=4,color='r')\n    plt.plot(range(len(y2)),y2,'.', linewidth=4, color='g')\n    plt.title(col)\n    plt.show()","2958aa3e":"We see a strong correlation between:\n- gdpp and income\n- total_fer and child_mort","e7a96170":"**CONCLUSION:** take 3 principal components","092f22f0":"# Functions for visualization on a map:","a0be1f80":"### Interpretation of clusters","19f52659":"# Feature selection:","0b18b386":"**N\u00b0Clusters for K-means: Elbow Method**","48f6df61":"Next, we need to select the pricipal components. For that, we will see in which features the variance is concentrated. The eigenvectors with the lowest eigenvalues describe the least amount of variation within the dataset. Therefore, these values can be dropped. So, we can follow the Kaiser criterion. With this approach, we retain component with an eigenvalue greater than 1.","c582664b":"# MODEL: K-Means","37e05fb1":"# CLUSTERING PROBLEM\n\nPurpose: The goal is to categorise the countries using socio-economic and health factors that determine the overall development of the country.\n\nModel Class: *Unsupervised*\n\nModel Type: *Clustering*\n\nEdit Date: 7\/6\/2020\n\nCluster Model: K-Means\n","9328ecdc":"# EDA","6276f9a9":"# DEPENDENCIES\n\nLoad the dependencies for model development. Current package requirements include:\n* Sklearn\n* Pandas\n* Numpy\n* Matplotlib","f27593a8":"We can conclude that 3 clusters is a good choice.","b125f787":"# Get Data","5dad5566":"*Cluster 0:* Poor countries\n\n*Cluster 1:* Rich countries\n\n*Cluster 2:* medium countries","882d02b4":"We can see in the first two columns the presence of upper outliers. To deal with them, we will apply a logarithm type transformation."}}