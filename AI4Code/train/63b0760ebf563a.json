{"cell_type":{"03181793":"code","1e86148c":"code","4ad78899":"code","3ae44f4a":"code","5c9e35ef":"code","2549930e":"code","cb29f80a":"code","f1782617":"code","8f431eca":"code","87478415":"code","c17f3eae":"code","9e6844a5":"code","19d25367":"code","d859385e":"code","af90b6b5":"code","b248cf35":"code","6bc65897":"code","a5125d5c":"code","1cd8112b":"code","392e8f12":"code","8eb3dcaf":"markdown","fbc3b89f":"markdown","699d5322":"markdown","2eae3840":"markdown","972e8455":"markdown","e962a2ca":"markdown","0c9768b1":"markdown","2574ff7c":"markdown","14697803":"markdown","988dd3c6":"markdown","eb29d628":"markdown","2b761b27":"markdown","2aeabb34":"markdown","3affefd0":"markdown","fe5e74b1":"markdown","76694a43":"markdown","707e0ad6":"markdown","5be32544":"markdown","cedba83c":"markdown","8df3afe6":"markdown","5471bfff":"markdown","2216a99c":"markdown","6004c598":"markdown"},"source":{"03181793":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt","1e86148c":"## load data\ntrain_data = pd.read_csv(\"..\/input\/titanic\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/titanic\/test.csv\")\nsubmission = pd.read_csv(\"..\/input\/titanic\/gender_submission.csv\")\ntarget = 'Survived'","4ad78899":"kk = pd.crosstab(train_data['Sex'], train_data[target])\nkk.columns=['UnSurvived','Survivied']\nax = kk.plot(y=['UnSurvived','Survivied'],kind='bar')\nax.set_title('how does gender affect the survived rate')\nax.set_ylabel('Passengers number')\nplt.show()","3ae44f4a":"kk = pd.crosstab(train_data['Pclass'], train_data[target])\nkk.columns=['UnSurvived','Survivied']\nax = kk.plot(y=['UnSurvived','Survivied'],kind='bar')\nax.set_title('how does Pclass affect the survived rate')\nax.set_ylabel('Passengers number')\nplt.show()","5c9e35ef":"kk = pd.crosstab(train_data['Embarked'], train_data[target])\nkk.columns=['UnSurvived','Survivied']\nax = kk.plot(y=['UnSurvived','Survivied'],kind='bar')\nax.set_title('how does Embarked affect the survived rate')\nax.set_ylabel('Passengers number')\nplt.show()","2549930e":"train_data['pclass&sex&embarket'] = train_data['Sex']+train_data['Pclass'].astype(str)+train_data['Embarked']\nkk = pd.crosstab(train_data['pclass&sex&embarket'], train_data[target])\nkk.columns=['UnSurvived','Survivied']\nax = kk.plot(y=['UnSurvived','Survivied'],kind='bar')\nax.set_title('how does pclass&sex&embarket affect the survived rate')\nax.set_ylabel('Passengers number')\nplt.show()","cb29f80a":"kk = pd.crosstab(train_data['SibSp'], train_data[target])\nkk.columns=['UnSurvived','Survivied']\nax = kk.plot(y=['UnSurvived','Survivied'],kind='bar')\nax.set_title('how does SibSp affect the survived rate')\nax.set_ylabel('Passengers number')\n\nkk = pd.crosstab(train_data['Parch'], train_data[target])\nkk.columns=['UnSurvived','Survivied']\nax = kk.plot(y=['UnSurvived','Survivied'],kind='bar')\nax.set_title('how does Parch affect the survived rate')\nax.set_ylabel('Passengers number')\n\n\ntrain_data['family'] = train_data['SibSp']+train_data['Parch']\ntest_data['family'] = test_data['SibSp']+test_data['Parch']\nkk = pd.crosstab(train_data['family'], train_data[target])\nkk.columns=['UnSurvived','Survivied']\nax = kk.plot(y=['UnSurvived','Survivied'],kind='bar')\nax.set_title('how does family affect the survived rate')\nax.set_ylabel('Passengers number')\n\n\nplt.show()","f1782617":"train_data['HasCabin'] = train_data['Cabin'].apply(lambda k: 0 if type(k) == float else 1)\ntest_data['HasCabin'] = test_data['Cabin'].apply(lambda k: 0 if type(k) == float else 1)\nkk = pd.crosstab(train_data['HasCabin'], train_data[target])\nkk.columns=['UnSurvived','Survivied']\nax = kk.plot(y=['UnSurvived','Survivied'],kind='bar')\nax.set_title('how does Cabin affect the survived rate')\nax.set_ylabel('Passengers number')\nplt.show()","8f431eca":"kk = pd.crosstab(train_data['Fare'], train_data[target])\nkk.columns=['UnSurvived','Survivied']\nkk1 = kk.iloc[:30,:]\nkk2 = kk.iloc[-30:,:]\nax = kk1.plot(y=['UnSurvived','Survivied'],kind='bar')\nax.set_title('how does Cabin affect the survived rate_1')\nax.set_ylabel('Passengers number')\n\nax = kk2.plot(y=['UnSurvived','Survivied'],kind='bar')\nax.set_title('how does Cabin affect the survived rate_2')\nax.set_ylabel('Passengers number')\n\nplt.show()","87478415":"kk = pd.crosstab(train_data['SibSp'], train_data['Pclass'])\nkk.columns=['pclass1','pclass2','pclass3']\nax = kk.plot(y=['pclass1','pclass2','pclass3'],kind='bar')\nax.set_title('SibSp&Pclass')\nax.set_ylabel('Passengers number')\nplt.show()","c17f3eae":"# ## Part2. Pseudo_labeling\n\n# To train a machine learning model with supervised learning, the data has to be labeled. Does that mean that unlabeled data is useless for supervised tasks like classification and regression? Certainly not! Aside from using the extra data for analytic purposes, we can even use it to help train our model with semi-supervised learning \u2013 combining both unlabeled and labeled data for model training.\n\n# The main idea is simple. First, train the model on labeled data, then use the trained model to predict labels on the unlabeled data, thus creating pseudo-labels. Further, combine the labeled data and the newly pseudo-labeled data in a new dataset that is used to train the data.\n\n# You can visit https:\/\/www.analyticsvidhya.com\/blog\/2017\/09\/pseudo-labelling-semi-supervised-learning-technique\/ for more details.\n\n# #### **But in here, useless, kkkkk.**","9e6844a5":"## create new features base on Feature analysis\n\n##  deal with name, only use part1\ntrain_data['Name'] = train_data['Name'].apply(lambda x: x.split(',')[0])\ntest_data['Name'] = test_data['Name'].apply(lambda x: x.split(',')[0])\n\n##  deal with fare, low price, high price, other\ntrain_data['fare_type'] = pd.cut(train_data['Fare'], [-1,20,80,1000], labels=['l','m','h'])\ntest_data['fare_type'] = pd.cut(test_data['Fare'], [-1,20,80,1000], labels=['l','m','h'])\n\n## deal with cabin, get A,B,C,D like this.\ntrain_data['Cabin_'] = train_data['Cabin'].apply(lambda k: str(k)[0])\ntest_data['Cabin_'] = test_data['Cabin'].apply(lambda k: str(k)[0])\n\ntrain_data = train_data.drop(['pclass&sex&embarket','PassengerId','SibSp','Parch'],axis = 1)\ntest_data = test_data.drop(['PassengerId','SibSp','Parch'],axis = 1)","19d25367":"train_data.head(3)","d859385e":"##install hypergbm\n!pip3 install -U hypergbm\n!pip3 install -U scikit-learn==0.23.2","af90b6b5":"from hypergbm import make_experiment\nfrom hypernets.core.trial import TrialHistory\nfrom hypernets.searchers import PlaybackSearcher\nfrom hypergbm.search_space import GeneralSearchSpaceGenerator\nfrom hypernets.searchers import EvolutionSearcher\nfrom hypernets.experiment.cfg import ExperimentCfg as cfg\ncfg.experiment_discriminator=None","b248cf35":"##difine search_space,only use cat, xgboost\nsearch_space_ = GeneralSearchSpaceGenerator(n_estimators=300,enable_lightgbm = False) \n## define search_algorithm\nrs = EvolutionSearcher(search_space_,optimize_direction='max',population_size=50, sample_size=6, candidates_size=5) \n\n##run make_experiment\nexp = make_experiment(\n                      train_data.copy(),\n                      target=target,\n                      cv = True,\n                      num_folds= 5, ## 5 folds for Cross-validate\n                      max_trials=10,\n                      early_stopping_time_limit=3600, ##early stop for 1h\n                      log_level = 'ERROR',  ##some info will output while trainning\n                      searcher = rs,\n                      random_state=7)\n# estimator = exp.run()\n# preds = estimator.predict(test_data)\n# submission[target] = preds\n# submit_df.to_csv(\"submission_PLB_0.79665.csv\", index=False)","6bc65897":"history_file = f\"\/kaggle\/input\/resource\/history_for_titanic.txt\"\nsearch_space_ = GeneralSearchSpaceGenerator(n_estimators=300,catboost_init_kwargs={'random_state': 8})\nhistory = TrialHistory.load_history(search_space_, history_file)\nplayback = PlaybackSearcher(history, top_n=1, optimize_direction='max')\n\nexp = make_experiment(\n                      train_data = train_data.copy(),\n                      eval_data = train_data.copy(),\n                      target=target,\n                      cv = False,\n                      max_trials=1,\n                      early_stopping_time_limit=3600, ##early stop for 1h\n                      log_level = 'ERROR',  ##some info will output while trainning\n                      searcher = playback,\n                      random_state=7)\n\n","a5125d5c":"estimator = exp.run()","1cd8112b":"preds = estimator.predict(test_data)\nsubmission[target] = preds\nsubmission.to_csv(\"submission.csv\", index=False)","392e8f12":"submission.head(10)","8eb3dcaf":"*we can see the female has higher survivied rate than male.*","fbc3b89f":"## Part3. AutoML(hypergbm)","699d5322":"# ### Process records\n\n#### At first I used cross-validation to submit the results, but I found that the local cv score were  over-fitting with PLB,How can i say,Sometimes if I improve the performance of my local cv score, the PLB score goes down, here are some of my own test results:\n\n![image.png](attachment:49f4a121-c1c8-4db1-921d-135957f75e6c.png)\n\n\n\n#### So I tried to reduce the model complexity and use full data training(and some things others),And then I got a better score.\n\n#### In my opinion, since the train_data is very small and almost every sample is unique(At the same time, I don't think survival is entirely certain, it's sort of random), the judgment rules you establish on the train_data may not be applicable to the test set. When you improve the fitting performance on the train_data, the results of the test_data tend to be over-fitting.\n\n#### If u have any good idea or finding,Please share with me ,thanks a lot.","2eae3840":"*we can see low price has lower survivied rate than high price,*","972e8455":"# ### Click here to show all infomation.","e962a2ca":"### c. How does Embarked affect the survived rate\n> Port of Embarkation\n>\n> C = Cherbourg,\n>\n> Q = Queenstown,\n>\n> S = Southampton\n","0c9768b1":"## Part1. Feature analysis","2574ff7c":"### d. Mapping Sex,Pclass,Embarked","14697803":"###  Submit with full_data tranning(PLB:0.8157)\n\n![image.png](attachment:97c1b18d-8695-4f13-99bb-50f18bf0cec5.png)","988dd3c6":"###  Submit with CV(PLB:0.79665)","eb29d628":"*we can see the 3rd people has lower survivied rate than others.*","2b761b27":"###  b. How does Pclass affect the survived rate\n> pclass: A proxy for socio-economic status (SES)\n> \n> 1st = Upper\n> \n> 2nd = Middle\n> \n> 3rd = Lower\n","2aeabb34":"### h. other intersting thing","3affefd0":"*we can see only Pclass3 people has over 3 Sibling. And we know the people who is alone has lower survivied rate,Half of them are Pclass3 people*","fe5e74b1":"###  a. How does gender affect the survived rate","76694a43":"*we can see people without any family has lower survived rate.*","707e0ad6":"> HyperGBM is a library that supports full-pipeline AutoML, which completely covers the end-to-end stages of data cleaning, preprocessing, feature generation and selection, model selection and hyperparameter optimization.It is a real-AutoML tool for tabular data.\n> \n> Unlike most AutoML approaches that focus on tackling the hyperparameter optimization problem of machine learning algorithms, HyperGBM can put the entire process from data cleaning to algorithm selection in one search space for optimization. End-to-end pipeline optimization is more like a sequential decision process, thereby HyperGBM uses reinforcement learning, Monte Carlo Tree Search, evolution algorithm combined with a meta-learner to efficiently solve such problems.\n> \n> As the name implies, the ML algorithms used in HyperGBM are all GBM models, and more precisely the gradient boosting tree model, which currently includes XGBoost, LightGBM and Catboost.\n> \n> The underlying search space representation and search algorithm in HyperGBM are powered by the Hypernets project a general AutoML framework.\n\nYou can visit [hypergbm](http:\/\/https:\/\/github.com\/DataCanvasIO\/HyperGBM) for more details","5be32544":"*we can see people who has cabin has higher survivied rate than nocabin which is >66% and <31%,*\n\n![image.png](attachment:5106b5fd-27e4-4513-95ee-d96433b27467.png)\n\n","cedba83c":"### g. How does Fare affect the survived rate","8df3afe6":"### e. How does SibSp&Parch affect the survived rate\n> sibsp: The dataset defines family relations in this way...\n>\n> Sibling = brother, sister, stepbrother, stepsister \n>\n> Spouse = husband, wife (mistresses and fianc\u00e9s were ignored) \n>\n> \n> parch: The dataset defines family relations in this way... \n>\n> Parent = mother, father \n>\n> Child = daughter, son, stepdaughter, stepson \n>\n> Some children travelled only with a nanny, therefore parch=0 for them. \n","5471bfff":"*we can see the people who embarked from Southampton has higher Unsurvived rate than others.*","2216a99c":"### f. How does Cabin affect the survived rate","6004c598":"*we can see the Pclass=1rd's female who embarket from C or Q almost survived.*\n\n*AND pclass!=1rd's male who embarket from S almost died.*"}}