{"cell_type":{"d82cd3c0":"code","95efc814":"code","964b0137":"code","cdd866bb":"code","d4174e62":"code","65879e10":"code","7c6a697f":"code","19b59087":"code","54920bc7":"code","deb66990":"code","740d7250":"code","d1d0ab63":"code","4ec0ed2a":"code","272fb411":"code","e0565a7e":"code","7bcb624e":"code","fbf6c29d":"code","03d05ca9":"code","a9713e4e":"markdown","a374401e":"markdown","5a987e69":"markdown","9e455d48":"markdown","7f546cc6":"markdown","80e18cb2":"markdown","8d3da761":"markdown","477528f2":"markdown","a93f7c16":"markdown"},"source":{"d82cd3c0":"from typing import Union\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm_notebook as tqdm","95efc814":"### Loading the weights\nm5_weights = pd.read_csv('..\/input\/m5methods\/validation\/weights_validation.csv')\nm5_weights.head()","964b0137":"from typing import Union\n\nimport numpy as np\nimport pandas as pd\nfrom tqdm.notebook import tqdm_notebook as tqdm\n\n\nclass WRMSSEEvaluator(object):\n\n    def __init__(self, train_df: pd.DataFrame, valid_df: pd.DataFrame, calendar: pd.DataFrame, prices: pd.DataFrame):\n        train_y = train_df.loc[:, train_df.columns.str.startswith('d_')]\n        train_target_columns = train_y.columns.tolist()\n        weight_columns = train_y.iloc[:, -28:].columns.tolist()\n\n        train_df['all_id'] = 0  # for lv1 aggregation\n\n        id_columns = train_df.loc[:, ~train_df.columns.str.startswith('d_')].columns.tolist()\n        valid_target_columns = valid_df.loc[:, valid_df.columns.str.startswith('d_')].columns.tolist()\n\n        if not all([c in valid_df.columns for c in id_columns]):\n            valid_df = pd.concat([train_df[id_columns], valid_df], axis=1, sort=False)\n\n        self.train_df = train_df\n        self.valid_df = valid_df\n        self.calendar = calendar\n        self.prices = prices\n\n        self.weight_columns = weight_columns\n        self.id_columns = id_columns\n        self.valid_target_columns = valid_target_columns\n\n        weight_df = self.get_weight_df()\n        self.weight_df = weight_df\n\n        self.group_ids = (\n            'all_id',\n            'state_id',\n            'store_id',\n            'cat_id',\n            'dept_id',\n            ['state_id', 'cat_id'],\n            ['state_id', 'dept_id'],\n            ['store_id', 'cat_id'],\n            ['store_id', 'dept_id'],\n            'item_id',\n            ['item_id', 'state_id'],\n            ['item_id', 'store_id'],\n            \n        )\n\n        for i, group_id in enumerate(tqdm(self.group_ids)):\n            train_y = train_df.groupby(group_id)[train_target_columns].sum()\n            scale = []\n            for _, row in train_y.iterrows():\n                series = row.values[np.argmax(row.values != 0):]\n                scale.append(((series[1:] - series[:-1]) ** 2).mean())\n            setattr(self, f'lv{i + 1}_scale', np.array(scale))\n            setattr(self, f'lv{i + 1}_train_df', train_y)\n            setattr(self, f'lv{i + 1}_valid_df', valid_df.groupby(group_id)[valid_target_columns].sum())\n\n            lv_weight = weight_df.groupby(group_id)[weight_columns].sum().sum(axis=1)\n            setattr(self, f'lv{i + 1}_weight', lv_weight \/ lv_weight.sum())\n\n    def get_weight_df(self) -> pd.DataFrame:\n        day_to_week = self.calendar.set_index('d')['wm_yr_wk'].to_dict()\n        weight_df = self.train_df[['item_id', 'store_id'] + self.weight_columns].set_index(['item_id', 'store_id'])\n        weight_df = weight_df.stack().reset_index().rename(columns={'level_2': 'd', 0: 'value'})\n        weight_df['wm_yr_wk'] = weight_df['d'].map(day_to_week)\n\n        weight_df = weight_df.merge(self.prices, how='left', on=['item_id', 'store_id', 'wm_yr_wk'])\n        weight_df['value'] = weight_df['value'] * weight_df['sell_price']\n        weight_df = weight_df.set_index(['item_id', 'store_id', 'd']).unstack(level=2)['value']\n        weight_df = weight_df.loc[zip(self.train_df.item_id, self.train_df.store_id), :].reset_index(drop=True)\n        weight_df = pd.concat([self.train_df[self.id_columns], weight_df], axis=1, sort=False)\n        return weight_df\n\n    def rmsse(self, valid_preds: pd.DataFrame, lv: int) -> pd.Series:\n        valid_y = getattr(self, f'lv{lv}_valid_df')\n        score = ((valid_y - valid_preds) ** 2).mean(axis=1)\n        scale = getattr(self, f'lv{lv}_scale')\n        return (score \/ scale).map(np.sqrt)\n\n    def score(self, valid_preds: Union[pd.DataFrame, np.ndarray]) -> float:\n        assert self.valid_df[self.valid_target_columns].shape == valid_preds.shape\n\n        if isinstance(valid_preds, np.ndarray):\n            valid_preds = pd.DataFrame(valid_preds, columns=self.valid_target_columns)\n\n        valid_preds = pd.concat([self.valid_df[self.id_columns], valid_preds], axis=1, sort=False)\n\n        all_scores = []\n        for i, group_id in enumerate(self.group_ids):\n            lv_scores = self.rmsse(valid_preds.groupby(group_id)[self.valid_target_columns].sum(), i + 1)\n            weight = getattr(self, f'lv{i + 1}_weight')\n            lv_scores = pd.concat([weight, lv_scores], axis=1, sort=False).prod(axis=1)\n            all_scores.append(lv_scores.sum())\n\n        return np.mean(all_scores)","cdd866bb":"%%time\n# Getting the data and instantiating the Evaluator object to compare the weights agianst the provided weights\ntrain_df = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\ncalendar = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\nprices = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\nfor i in range(1914, 1942):\n    train_df[f\"d_{i}\"] = 0\n\ntrain_fold_df = train_df.iloc[:, :-28]\nvalid_fold_df = train_df.iloc[:, -28:].copy()\n\ne = WRMSSEEvaluator(train_fold_df, valid_fold_df, calendar, prices)\ndel train_fold_df, train_df, calendar, prices","d4174e62":"### Lets get all the weights from the object in one array to compare to m5_weights\ne_weights = e.lv1_weight\nfor i in range(1,12):\n    e_weights = np.append(e_weights, getattr(e, f'lv{i + 1}_weight').values)","65879e10":"#### Lets see where the biggest discrepency is between the weights\n(m5_weights.Weight - e_weights).max()","7c6a697f":"# .0057 is way too hight. Lets make a function to find\n# out where the problem is. The function will show \n# the max discrepency by level.\ndef show_diff(lv, mw):\n    L = lv\n    mw3 = mw[mw.Level_id == f'Level{L}']['Weight']\n    mw3\n    es3 = getattr(e,f'lv{L}_weight').values\n    res = (es3 - mw3).max()\n    return res","19b59087":"for i in range(12):\n    print(f'lv_{i+1} difference is {show_diff(i+1, m5_weights)}')","54920bc7":"e.lv11_weight","deb66990":"m5_weights[m5_weights.Level_id == f'Level11']","740d7250":"# We get the level 11 porting of m5_weights isolated and sorted correctly \ntemp_df = m5_weights[m5_weights.Level_id == f'Level11'].sort_values(['Agg_Level_2', 'Agg_Level_1'])\n# We also want to know to max and min of the index values so we can \n# reset the index correctly \nprint(temp_df.index.min(), temp_df.index.max())","d1d0ab63":"# Replace the index so we can put the temp_df \n# into m5_weights easily\ntemp_df.index = range(3203,12350)\n# Insert the temp_df into m5_weights\nm5_weights = pd.concat([m5_weights[m5_weights.Level_id != 'Level11'], temp_df]).sort_index()","4ec0ed2a":"# Check if this new m5_weights dataframe lines up \n# with our evaluators weights\nfor i in range(12):\n    print(f'lv_{i+1} difference is {show_diff(i+1, m5_weights)}')","272fb411":"# Lets also add the scaling factor onto this dataframe as well \n# First make a copy of m5_weights so that we can use it over again. \nm5_w = m5_weights.copy()\n# Get the scale for each series from the evaluator\ne_scales = e.lv1_scale\nfor i in range(1,12):\n    e_scales = np.append(e_scales, getattr(e, f'lv{i + 1}_scale'))\n    \n# Now add a scale column to m5_w\nm5_w['scale'] = e_scales \n\n# Make Weight column lowercase cus I like it that way\nm5_w = m5_w.rename(columns={'Weight': 'weight'})\n\n# Make a csv file, weight_scale_1914.csv, denoting that \n# we have the weights and the scales for all series \n# for a validation period starting on 1914\nm5_w.to_csv('weight_scale_1914.csv')","e0565a7e":"m5_w","7bcb624e":"%%time\n\n# Start by loading in the needed datasets \ntrain_df = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sales_train_validation.csv')\ncalendar = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/calendar.csv')\nprices = pd.read_csv('\/kaggle\/input\/m5-forecasting-accuracy\/sell_prices.csv')\nfor i in range(1914, 1942): # This part needed only to get data for public test period\n    train_df[f\"d_{i}\"] = 0","fbf6c29d":"def get_weight_scale(start_valid, train_df, calendar, prices):\n    \"\"\"Gets the weight and scale for every series, given the end of know data (end_train), \n     is one day before the input start_valid\"\"\"\n\n    # Adding + 5 in order to account for the index columns. Error found by \n    # https:\/\/www.kaggle.com\/jhkerwin\n    train_fold_df = train_df.iloc[:, :start_valid + 5]\n    valid_fold_df = train_df.iloc[:, start_valid + 5:start_valid + 28 + 5].copy()\n\n    e = WRMSSEEvaluator(train_fold_df, valid_fold_df, calendar, prices)\n    del train_fold_df\n\n    # Get a fresh copy of the index columns of m5_weights\n    w = m5_weights.iloc[:, :3].copy()\n\n    # Get all the weights for each series\n    e_weights = e.lv1_weight\n    for i in range(1,12):\n        e_weights = np.append(e_weights, getattr(e, f'lv{i + 1}_weight').values)\n\n    # Get the scale for each series from the evaluator\n    e_scales = e.lv1_scale\n    for i in range(1,12):\n        e_scales = np.append(e_scales, getattr(e, f'lv{i + 1}_scale'))\n\n    # Add the weight and scale columns\n    w['weight'] = e_weights\n    w['scale'] = e_scales\n\n    # Create a csv file with the data \n    w.to_csv(f'weight_scale_{start_valid}.csv')","03d05ca9":"# Lets get the weight_scale_x dataframe for every 28 day period for the\n# past 27 periods (roughly 2 years) to get the plenty of validation period\n# information, including the public validation and final test time of year \n# for the past 2 years \nfor i in range(28): \n    get_weight_scale(1914 - (28 * i), train_df, calendar, prices)","a9713e4e":"# This kernal relies on [WRMSSE Evaluator](https:\/\/www.kaggle.com\/c\/m5-forecasting-accuracy\/discussion\/133834) by [Sakami](https:\/\/www.kaggle.com\/sakami). ","a374401e":"## Weights for the public validation set are provided by the M5 organizers. Lets load them in and compare them with the weights we get from the Evaluator object.","5a987e69":"Fantastic! All the errors are tiny enough to ignor (I think e-8 is small enough)","9e455d48":"Thats fantastic! We fixed level 11 and now we know that we can produce weights with our evaluator, and then save them simply by concatenating the column onto the updated m5_weights dataframe. The same process can also be done for the scaling factor, but we can't double check it against any given scales for now. I will check at least one set of scaling factors in another notebook. For now, lets make a function to produce weights and scales any validation set starting on day START_VAL","7f546cc6":"We can see the problem is in level 11. Lets look at the data.","80e18cb2":"## Objective: obtain the weights and scaling factors for all series for different time periods for validation purposes. Use the output of this kernal to backtest your models with the proper weight and scaling factors. \n\n## Output: weight_scale_*start_valid*.csv is a file that contains the weights and scaling factors for all series for a validation period starting on day *start_valid*. \n\n### Example: For the weights and scales for the public validation period starting on day 1914, load the weight_scale_1914.csv file. ","8d3da761":"Thats not good! we are not matching the given weights","477528f2":"Version 9 update: Thank you to [jhkerwin](https:\/\/www.kaggle.com\/jhkerwin), who has found my error in get_weight_scale function. I had to add 5 to the indexing to account for the first index columns that come before the 'd_x' columns begin. ","a93f7c16":"We can see that the problem is the ordering. The evaluator is sorting by food item, then state, while the m5_weights are sorted by state, then food item. Lets fix the m5_weights"}}