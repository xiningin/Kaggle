{"cell_type":{"1f909896":"code","5cb1dea3":"code","03585370":"code","b406f3a2":"code","b07d9f55":"code","8fc87c80":"code","2452aa1d":"code","837ccdc6":"code","6f97af8a":"code","f73d5626":"code","d7166d8a":"code","c2b83896":"code","105a66a9":"code","cc34388e":"code","7995a9d0":"code","9cac464e":"code","93d141d6":"code","2967f7ca":"code","dc3dabec":"code","2152caaa":"code","65e0587e":"code","382793a2":"markdown","bba93f37":"markdown","5bf20097":"markdown","cd47f0c0":"markdown","5dde3b3d":"markdown","105019e2":"markdown","427cc31d":"markdown","ab4bf8db":"markdown","89c03d10":"markdown","4898b2e1":"markdown","228a4442":"markdown"},"source":{"1f909896":"import pandas as pd\npd.set_option('display.max_columns', 100)\npd.set_option('display.max_rows', 100)\n\nimport numpy as np\n\nimport os\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport ast\n\nimport re\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import LabelEncoder\n\nimport xgboost as xgb\n\nfrom collections import Counter\n","5cb1dea3":"print(os.listdir(\"..\/input\"))","03585370":"train = pd.read_csv(\"..\/input\/train.csv\")\ntest = pd.read_csv(\"..\/input\/test.csv\")","b406f3a2":"train.isna().sum()","b07d9f55":"test.isna().sum()","8fc87c80":"dict_columns = ['belongs_to_collection', 'genres', 'production_companies',\n                'production_countries', 'spoken_languages', 'Keywords', 'cast', 'crew']\n\ndef text_to_dict(df):\n    for column in dict_columns:\n        df[column] = df[column].apply(lambda x: {} if pd.isna(x) else ast.literal_eval(x) )\n    return df\n        \ntrain = text_to_dict(train)\ntest = text_to_dict(test)\n\n","2452aa1d":"def build_category_list(x, field, feature):\n    regex = re.compile('[^0-9a-zA-Z_]')\n    category_list = \"\"\n    \n    for d in x:\n        new_category = regex.sub('', d[field].lower().replace(\" \",\"_\"))\n        category_list += \" \" + new_category\n    return category_list.strip()\n\n\ntarget_fields = {'belongs_to_collection': 'name', 'genres': 'name',\n                 'production_countries': 'iso_3166_1', 'production_companies': 'name',\n                 'spoken_languages': 'iso_639_1', 'Keywords': 'name', 'cast':'name',\n                 'crew':'name'\n                }\n\ntrain['crew_copy'] = train['crew']\ntest['crew_copy'] = test['crew']\n\ntrain['cast_copy'] = train['cast']\ntest['cast_copy'] = test['cast']\n\n\nfor k,v in target_fields.items():\n    print(k)\n    train[k] = train[k].apply(lambda x: build_category_list(x, v, k))\n    test[k] = test[k].apply(lambda x: build_category_list(x, v, k)) \n    \n","837ccdc6":"thresholds = {'belongs_to_collection': 0, 'genres': 0,\n                 'production_countries': 10, 'production_companies': 10,\n                 'spoken_languages': 10, 'Keywords': 10, 'cast': 10, 'crew': 10\n                }\n\ndef streamline(x, kept):\n    streamlined = \"\"\n    for w in x.split(\" \"):\n        if w in kept:\n            streamlined = streamlined + \" \" + w\n    return streamlined.strip()\n\nfor k,v in thresholds.items():\n    print(k)\n    c = Counter(\" \".join(train[k]).split(\" \"))\n    print(\"Initial:\", len(c))\n    kept = [w for w,nb in c.items() if nb > v]\n    print(\"Kept:\", len(kept))\n    print(\"\")\n    train[k] = train[k].apply(lambda x: streamline(x, kept))\n    test[k] = test[k].apply(lambda x: streamline(x, kept))","6f97af8a":"def build_category_list_with_roles(x, v, rv):\n    regex = re.compile('[^0-9a-zA-Z_]')\n    category_list = \"\"\n    for d in x:\n        if d[v['role_field']] != rv:\n            pass\n        else:\n            if category_list == \"\":\n                new_category = regex.sub('', d[v['field']].lower().replace(\" \",\"_\"))\n                category_list += \" \" + new_category\n    return category_list.strip()  \n    \ntarget_fields = {'cast_copy':{'field':'name', 'role_field':'order', 'role_values':[0,1,2,3,4,5]}, \n                 'crew_copy':{'field': 'name', 'role_field': 'job',\n                         'role_values':['Director', 'Producer',\n                                        'Executive Producer', 'Writer', 'First Assistant Director',\n                                        'Associate Producer', 'Director of Photography'\n                                       ]\n                        }\n                }\n\n\nadditional_label_encoding_columns = []\n\nfor k,v in target_fields.items():\n    print(k)\n    for rv in v['role_values']:\n        striped_rv = str(rv).lower().replace(' ','_')\n        additional_label_encoding_columns.append(k + '_' + striped_rv)\n        train[k + '_' + striped_rv] = train[k].apply(lambda x: build_category_list_with_roles(x, v, rv))\n        test[k + '_' + striped_rv] = test[k].apply(lambda x: build_category_list_with_roles(x, v, rv))\n    \n","f73d5626":"fillna_columns = {'release_date':'mode',\n                  'status':'mode',\n                  'belongs_to_collection': 'none',\n                  'runtime': 'mode'}\n\nfor k,v in fillna_columns.items():\n    if v == 'mode':\n        fill = train[k].mode()[0]\n    else:\n        fill = v\n    print(k, ': ', fill)\n    train[k] = train[k].fillna(value = fill)\n    test[k] = test[k].fillna(value = fill)\n","d7166d8a":"def extract_nb_within_collection(r):\n    regex = re.compile('[^0-9a-zA-Z_]')\n    original_title = regex.sub('', r['original_title'].lower().replace(\" \",\"_\"))\n    \n    if r['is_part_of_collection'] == 0:\n        return 0\n    else:\n        if (r['belongs_to_collection'] == original_title + '_collection') or (r['belongs_to_collection'] == original_title):\n            return 1\n        else:\n            regex = re.compile('[^0-9]')\n            probable_number = regex.sub('', r['original_title'])\n            if probable_number == '' or int(probable_number) > 5:\n                return 0\n            else:\n                return probable_number\n\ndef feature_addition(df):\n    \n    df['release_year'] = df.release_date.apply(lambda x: x[-2:]).astype('int')\n    df['release_month'] = df.release_date.apply(lambda x: x.split('\/')[0]).astype('int')\n    df['release_quarter'] = df.release_month % 4 + 1\n    \n    df['budget'] = df.budget \/ 1000000\n    \n    df['nb_spoken_languages'] = df.spoken_languages.apply(lambda r: len(r.split(' ')))\n    df['nb_words_overview'] = df.overview.apply(lambda x: len(str(x).split(' ')) )\n    df['nb_production_companies'] = df.production_companies.apply(lambda x: len(x.split(' ')) )\n    df['nb_production_countries'] = df.production_countries.apply(lambda x: len(x.split(' ')) )\n    df['nb_cast'] = df.cast.apply(lambda x: len(x.split(' ')) )\n    df['nb_crew'] = df.crew.apply(lambda x: len(x.split(' ')) )\n    df['nb_keywords'] = df.Keywords.apply(lambda x: len(x.split(' ')) )\n    df['nb_words_title'] = df.title.apply(lambda x: len(str(x).split(' ')) )\n    df['nb_words_tagline'] = df.tagline.apply(lambda x: len(str(x).split(' ')) )\n    \n    df['nb_words_original_title'] = df.original_title.apply(lambda x: len(x.split(' ')) )\n    \n    df['has_original_title'] = (df.title == df.original_title).astype('int')\n\n    df['has_homepage'] = 1 - df.homepage.isna().astype('int')\n    df['homepage_base'] = df.homepage.apply(lambda x: str(x).split('\/\/')[-1].split('\/')[0].split('www.')[-1].split('.')[0])\n    df['homepage_extension'] = df.homepage.apply(lambda x: str(x).split('\/\/')[-1].split('\/')[0].split('www.')[-1].split('.')[-1]).fillna(value = '')\n\n    df['is_part_of_collection'] = 1 - (df.belongs_to_collection == '').astype('int')\n    df['nb_within_collection'] =  df.apply(lambda r: extract_nb_within_collection(r), axis = 1).astype('int')\n    \n    return df\n                                                \ntrain = feature_addition(train)\ntest = feature_addition(test)","c2b83896":"columns_to_categorize = ['belongs_to_collection', 'status', 'original_language', 'homepage_base', 'homepage_extension']\ncolumns_to_categorize += additional_label_encoding_columns\n\nfor c in columns_to_categorize:\n    print(c)\n    le = LabelEncoder()\n    le.fit_transform(train[c])\n    test[c] = test[c].map(lambda s: 'unknown' if s not in le.classes_ else s)\n    le.classes_ = np.append(le.classes_, 'unknown')\n    train[c] = le.transform(train[c])\n    test[c] = le.transform(test[c])\n","105a66a9":"submission = pd.DataFrame(test['id'])\n\nremoved_columns = ['id', 'homepage', 'imdb_id', 'original_title', 'spoken_languages',\n                   'overview', 'poster_path', 'tagline', 'title',\n                  'release_date', 'crew_copy', 'cast_copy']\n\n\ntrain.drop(removed_columns, axis = 1, inplace = True)\ntest.drop(removed_columns, axis = 1, inplace = True)","cc34388e":"features_to_vectorize = ['genres', 'production_countries', 'production_companies', 'Keywords', 'cast', 'crew']\n\n\nfor f in features_to_vectorize:\n    print(f)\n    vectorizer = TfidfVectorizer(use_idf = False)\n    vectorized_features = vectorizer.fit_transform(train[f])\n    vectorized_features_names = [f + '_' + v for v in vectorizer.get_feature_names()]\n\n    vectorized_features_sparse = pd.SparseDataFrame([ pd.SparseSeries(vectorized_features[i].toarray().ravel()) \n                              for i in np.arange(vectorized_features.shape[0]) ], columns = vectorized_features_names)\n\n    train = pd.concat([train, vectorized_features_sparse], axis = 1)\n    \n    vectorized_features = vectorizer.transform(test[f])\n    vectorized_features_sparse = pd.SparseDataFrame([ pd.SparseSeries(vectorized_features[i].toarray().ravel()) \n                              for i in np.arange(vectorized_features.shape[0]) ], columns = vectorized_features_names)\n\n    test = pd.concat([test, vectorized_features_sparse], axis = 1)\n    \n    train.drop(f, inplace = True, axis = 1)\n    test.drop(f, inplace = True, axis = 1)\n    ","7995a9d0":"train['revenue'] = np.log1p(train['revenue'] )","9cac464e":"target_column = 'revenue'\n\ntrain_set, validate_set = train_test_split(train, test_size = 0.2, random_state = 1)\n\nx_train = train_set.drop([target_column], axis = 1).copy()\ny_train = train_set[target_column].copy()\n\nx_validate = validate_set.drop([target_column], axis = 1).copy()\ny_validate = validate_set[target_column].copy()\n\nx_total = train.drop([target_column], axis = 1).copy()\ny_total = train[target_column].copy()\n\nx_test = test.copy()","93d141d6":"import lightgbm as lgb\n\nparams_lgb = {'drop_rate': [0.09777484320779173], 'feature_fraction': [0.6087324102659581],\n              'lambda_l1': [0.03915143495854047], 'lambda_l2': [26.68081917087524],\n              'learning_rate': [0.013231541159028165],\n              'max_drop': [67.0], 'min_data_in_leaf': [1.0],\n              'num_leaves': [32.0], 'num_trees': [1370.0]}\n\nparams_lgb = {k:v[0] for k,v in params_lgb.items()}\n\n\nlg = lgb.LGBMRegressor(\n                        objective = 'regression',\n                        metric = 'rmse',\n                        early_stopping_round = 50,\n                        drop_rate = params_lgb['drop_rate'],\n                        feature_fraction = params_lgb['feature_fraction'],\n                        lambda_l1 = params_lgb['lambda_l1'],\n                        lambda_l2 = params_lgb['lambda_l2'],\n                        learning_rate = params_lgb['learning_rate'],\n                        max_drop = int(params_lgb['max_drop']),\n                        min_data_in_leaf = int(params_lgb['min_data_in_leaf']),\n                        num_leaves = int(params_lgb['num_leaves']),\n                        num_trees = int(params_lgb['num_trees']))\n\nlg.fit(x_train, y_train.values, eval_set=[(x_train, y_train), (x_validate, y_validate)])","2967f7ca":"feature_importance = pd.DataFrame(lg.feature_importances_, columns = ['importance'])\nfeature_importance['feature'] = train.columns[:-1]\nfeature_importance.sort_values(by='importance', inplace = True, ascending = False)\nfeature_importance.reset_index(drop = True, inplace = True)\nfeature_importance","dc3dabec":"fig, ax = plt.subplots(figsize=(10, 15))\nsns.barplot(y = 'feature', x = 'importance', data = feature_importance[0:20])","2152caaa":"lg = lgb.LGBMRegressor(\n                        objective = 'regression',\n                        metric = 'rmse',\n                        early_stopping_round = 50,\n                        drop_rate = params_lgb['drop_rate'],\n                        feature_fraction = params_lgb['feature_fraction'],\n                        lambda_l1 = params_lgb['lambda_l1'],\n                        lambda_l2 = params_lgb['lambda_l2'],\n                        learning_rate = params_lgb['learning_rate'],\n                        max_drop = int(params_lgb['max_drop']),\n                        min_data_in_leaf = int(params_lgb['min_data_in_leaf']),\n                        num_leaves = int(params_lgb['num_leaves']),\n                        num_trees = 592)\n\nlg.fit(x_total, y_total.values, eval_set=[(x_train, y_train), (x_validate, y_validate)])","65e0587e":"y_test_p = pd.Series(lg.predict(x_test))\nsubmission['revenue'] = np.expm1(y_test_p)\nsubmission.to_csv(\"submission.csv\", index = False)","382793a2":"Train test split : ","bba93f37":"LGBM with preliminary params optimization (hyperopt) :","5bf20097":"Adding a few features :","cd47f0c0":"Transforming dictionary columns to proper format :","5dde3b3d":"Extracting categories from selected dictionary columnns : ","105019e2":"Label encoding selected features :","427cc31d":"Transforming revenue to log for log rmse :","ab4bf8db":"For cast and crew we select only key roles :","89c03d10":"Filling nan values :","4898b2e1":"Removing unused columns : ","228a4442":"Vectorizing selected columns : "}}