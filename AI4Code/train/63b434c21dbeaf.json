{"cell_type":{"f4234b8c":"code","7a39dd1d":"code","3991bbe9":"code","7c4c3f2b":"code","2412cef6":"code","35a8ac07":"code","cfed3af2":"code","c28ff9ae":"code","cef659fa":"code","9d809c19":"code","81c1ffe2":"code","c9f8c5e5":"code","d09f27ea":"code","bb106b07":"code","c401075d":"code","0d4ca3f6":"code","ed0191e8":"code","3cdd5bee":"code","4081d03c":"code","c47e00c4":"code","3b37223a":"code","d4bd0260":"code","9068ca6b":"code","069e8d8d":"code","d5ea30f3":"code","d9cf39e9":"code","1a7439e3":"code","eb5f34b7":"code","1cc1e720":"code","99e5e528":"code","fff4926c":"code","ee43bf7b":"code","69577188":"code","141e046b":"code","ddf7511c":"code","2446892e":"code","adb8c628":"code","000dea6b":"code","b43ca2ef":"code","8ceb51e1":"code","5a0124ba":"code","8a20d980":"code","9f96a318":"code","c55a8687":"code","737a3534":"markdown","af9af485":"markdown","0174d876":"markdown","4768344b":"markdown","a21cc2bc":"markdown","539b335a":"markdown","71c9c4a6":"markdown","cf0ba7fe":"markdown","2a1a6c52":"markdown"},"source":{"f4234b8c":"\n#importing necessary libraries\nimport numpy as np \nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport cv2\nimport math\nimport os\nimport sklearn","7a39dd1d":"#reading train.csv file\ntrain_csv=pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/train.csv\")","3991bbe9":"train_csv.head(20)","7c4c3f2b":"train_csv.describe()","2412cef6":"#let's look int test.csv\ntest_csv=pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/test.csv\")","35a8ac07":"test_csv","cfed3af2":"sample_sub=pd.read_csv(\"..\/input\/petfinder-pawpularity-score\/sample_submission.csv\")","c28ff9ae":"#it showsz the format in which the result is to be submitted\nsample_sub","cef659fa":"Features = [col for col in train_csv.columns if col not in ['Id', 'Pawpularity']]","9d809c19":"\nFeatures","81c1ffe2":"#the target value is pawpularity column.\n#Visualizing its distribution\n\n\ntrain_csv['Pawpularity'].plot(kind='hist', bins=100, figsize=(15, 6));\nplt.title(\"Target distribution\", weight='bold', fontsize=16);","c9f8c5e5":"\ntrain_csv['Pawpularity'].describe()","d09f27ea":"# correlation matrix between all the features.\nplt.figure(figsize=(16,10))\nsns.heatmap(train_csv.corr(), annot=True, fmt='.1g', cmap='coolwarm', square=True)\nplt.title('Correlation Matrix', fontsize=20, fontweight='bold')\nplt.show()","bb106b07":"train_csv.corrwith(train_csv.Pawpularity)","c401075d":"most_pawpular = train_csv[train_csv[\"Pawpularity\"] == train_csv[\"Pawpularity\"].max()].iloc[0]\npath = \"..\/input\/petfinder-pawpularity-score\/train\/\"+most_pawpular['Id']+\".jpg\"\nim = plt.imread(path)\nplt.figure(figsize=(15, 6))\nplt.imshow(im)\nplt.title(path.split(\"\/\")[-1])\nplt.xticks([]), plt.yticks([])\nprint(f\"Accompanying features:\")\ntrain_csv[train_csv['Id']==path.split('\/')[-1].split('.')[0]]","0d4ca3f6":"least_pawpular = train_csv[train_csv[\"Pawpularity\"] == train_csv[\"Pawpularity\"].min()].iloc[0]\npath = \"..\/input\/petfinder-pawpularity-score\/train\/\"+least_pawpular['Id']+\".jpg\"\nim = plt.imread(path)\nplt.figure(figsize=(15, 6))\nplt.imshow(im)\nplt.title(path.split(\"\/\")[-1])\nplt.xticks([]), plt.yticks([])\nprint(f\"Accompanying features:\")\ntrain_csv[train_csv['Id']==path.split('\/')[-1].split('.')[0]]","ed0191e8":"# creating a new column in train_csv with name as 'img_path' \ntrain_csv['img_path'] = train_csv['Id'].apply(lambda x: f'..\/input\/petfinder-pawpularity-score\/train\/{str(x)}.jpg')","3cdd5bee":"new_df=pd.read_csv(\"..\/input\/image-attributes\/file1.csv\")","4081d03c":"new_df.head(2)","c47e00c4":"new_df.drop(['Unnamed: 0','1','2'],axis=1,inplace=True)","3b37223a":"new_df","d4bd0260":"new_df.rename(columns={'0': 'img_path','3': 'area_ratio','4': 'number_of_pets', '5': 'Avg_pet_score'}, inplace=True)","9068ca6b":"new_df.head(2)","069e8d8d":"train_csv.head(2)","d5ea30f3":"updated_df=pd.merge(train_csv, new_df, on='img_path')","d9cf39e9":"updated_df.head(2)","1a7439e3":"# reading all the images and its meta data with target value\n# target list contains the target value\n#train_data contains both img data and metadata\n\n\ntrain=[]\ny=[]\npath=\"..\/input\/petfinder-pawpularity-score\/train\"\n\nfor i in range(len(updated_df)):\n    train1=updated_df.drop(['Id','img_path','Pawpularity'],axis=1)[updated_df.index==i].values\n    \n    y.append(updated_df.Pawpularity[i])\n    path=updated_df.img_path[i]\n    img1=cv2.imread(path)\n    img1=cv2.resize(img1,(128,128))\n    img1=cv2.cvtColor(img1, cv2.COLOR_BGR2RGB)\n    \n    train.append([img1,train1[0]])\n    ","eb5f34b7":"from sklearn.model_selection import train_test_split\nX_train,X_valid,y_train,y_valid=train_test_split(train,y,test_size=0.1,random_state=42,shuffle=True)","1cc1e720":"from sklearn.model_selection import train_test_split\nX_valid1,X_valid2,y_valid1,y_valid2=train_test_split(X_valid,y_valid,test_size=0.5,random_state=42,shuffle=True)","99e5e528":"#splitting X_traininto two different lists\ntrain_img_data=[]\ntrain_feature_data=[]\nfor i,j in X_train:\n    train_img_data.append(i)\n    train_feature_data.append(j)","fff4926c":"valid_img_data1=[]\nvalid_feature_data1=[]\nfor i,j in X_valid1:\n    valid_img_data1.append(i)\n    valid_feature_data1.append(j)\n\nvalid_img_data2=[]\nvalid_feature_data2=[]\nfor i,j in X_valid2:\n    valid_img_data2.append(i)\n    valid_feature_data2.append(j)","ee43bf7b":"train_feature_data=np.array(train_feature_data)\ntrain_feature_data.shape\n\nvalid_feature_data1=np.array(valid_feature_data1)\nvalid_feature_data1.shape\n\nvalid_feature_data2=np.array(valid_feature_data2)\nvalid_feature_data2.shape","69577188":"# normalising img data by dividing it by 255 the result matrix will have value between 0 and 1\ntrain_img_data=np.array(train_img_data)\ntrain_img_data.shape\n\nvalid_img_data1=np.array(valid_img_data1)\nvalid_img_data1.shape\n\nvalid_img_data2=np.array(valid_img_data2)\nvalid_img_data2.shape","141e046b":"\ny_train=np.array(y_train)\n\ny_valid1=np.array(y_valid1)\n\ny_valid2=np.array(y_valid2)","ddf7511c":"def clipvalues(values):\n    for i in range(len(values)):\n        if values[i]<1:\n            values[i]=1\n        if values[i]>100:\n            values[i]=100\n    return values","2446892e":"import tensorflow as tf\nfrom tensorflow import keras\nfrom tensorflow.keras.models import Sequential,Model\nfrom tensorflow.keras.layers import Flatten,BatchNormalization,Convolution2D,MaxPooling2D,Dropout,Dense,Concatenate\n\nfrom tensorflow.keras import Input\nfrom tensorflow.keras.callbacks import EarlyStopping,ReduceLROnPlateau","adb8c628":"#importing Xception model from keras.application.\n\nfrom tensorflow.keras.applications import xception\n\n# Assigining the weights\nxception = xception.Xception(weights='..\/input\/xception-weights\/xception_weights_tf_dim_ordering_tf_kernels_notop.h5',include_top=False,input_shape=(128, 128, 3))","000dea6b":"len(xception.layers)","b43ca2ef":"import sklearn\nfor layer in xception.layers[:120]:\n        layer.trainable=False\nfirst= Flatten()(xception.output)\nfirst= Dense(1024,activation=\"relu\")(first)\n#first=BatchNormalization()(first)\nfirst=Dropout(0.8)(first)\n\nfirst= Dense(1024,activation=\"relu\")(first)\n#first=BatchNormalization()(first)\nfirst=Dropout(0.8)(first)\n\ntabular_input = Input(shape=(15,))\nsecond=Dense(1024,activation=\"relu\")(tabular_input)\n#second=BatchNormalization()(second)\nsecond=Dropout(0.8)(second)\n\nsecond=Dense(512,activation=\"relu\")(second)\n#second=BatchNormalization()(second)\nsecond=Dropout(0.8)(second)\n\ncombined = Concatenate(axis=1)([first, second])\nresult=Dense(512,activation=\"relu\")(combined)\n#result=BatchNormalization()(result)\nresult=Dropout(0.8)(result)\n\nresult=Dense(256,activation=\"relu\")(combined)\n#result=BatchNormalization()(result)\nresult=Dropout(0.8)(result)\n\nresult=Dense(1,activation=\"relu\")(result)\n    \n    \nimage_input = Input(shape=(128,128,3))\nmodel = tf.keras.Model(inputs=[xception.input, tabular_input], outputs=[result])\n    \nearly_stopping = EarlyStopping(patience = 100)\nreduce_lr = ReduceLROnPlateau(factor=0.1, patience=60,min_lr=1e-9)\n\nmodel.compile(loss=tf.keras.losses.MeanSquaredError(), optimizer='adam', metrics=[tf.keras.metrics.RootMeanSquaredError(name=\"rmse\"), \"mae\", \"mape\"])\npredictor=model.fit((train_img_data,train_feature_data),y_train,validation_split=0.2, epochs=250,batch_size=32, callbacks=[early_stopping,reduce_lr])\n    \ny_pred=model.predict((valid_img_data1,valid_feature_data1))\ny_pred2=model.predict((valid_img_data2,valid_feature_data2))\n\ny_pred=clipvalues(y_pred)\ny_pred2=clipvalues(y_pred2)\n\ny_pred=y_pred.reshape(-1,1)\ny_pred2=y_pred2.reshape(-1,1)\n\n\ny_valid1=y_valid1.reshape(-1,1)\ny_valid2=y_valid2.reshape(-1,1)\n    \nmse1 = sklearn.metrics.mean_squared_error(y_valid1,y_pred)\nmse2 = sklearn.metrics.mean_squared_error(y_valid2,y_pred2)\nrmse0 = math.sqrt(mse1)\nrmse1 = math.sqrt(mse2)","8ceb51e1":"print(\"RMSE on predicted data with first validation set= \",rmse0)\nprint(\"RMSE on predicted data with second validation set= \",rmse1)","5a0124ba":"predictor.history","8a20d980":"plt.plot(predictor.history['loss'])\nplt.plot(predictor.history['val_loss'])\nplt.xlabel('epochs')\nplt.ylabel('Loss')\nplt.legend(['train','test'], loc='lower right')","9f96a318":"#comparing results\nc= np.arange(len(y_valid1))\nplt.scatter(y_valid1,c,label=\"True target value\")\nplt.scatter(y_pred,c,label=\"Predicted target value\")\nplt.legend()","c55a8687":"c= np.arange(len(y_valid2))\nplt.scatter(y_valid2,c,label=\"True target value\")\nplt.scatter(y_pred2,c,label=\"Predicted target value\")\nplt.legend()","737a3534":"train_csv.file contains 14 columns.\n\n* id: this column denotes the file name in train image dataset.\n\nBelow features have binary values.\n\n* subject focus: denotes if the animal is looking into the camera.\n* eyes,face: denotes if eyes and face is visible in image.\n* near: dentoes if the animal is far or near in image.\n* accessory: denotes if animal is wearing something.\n* group:denotes if there is a single animal or a group.\n* collage: if the image is collage or not.\n* Human: if image has human or not.\n* occlusion: is the animal is blocked by anything in image.\n* info : if image has any info related to animal.\n* blur: if the animal is blurred or not.\n\nThe target variable is:\n* pawpularity:it is a integer value that lies between 1 and 100","af9af485":"Image with least pawpularity score and its features","0174d876":"**Evaluation Metric**","4768344b":"From above correlation values we can see that the features that are most correlated with pawpularity column are:\n\nBlur\n\nGroup\n\nAccesory","a21cc2bc":"Test.csv have all features except the pawpularity feature which we have to predict.","539b335a":"Image with highest pawpularity score and its features","71c9c4a6":"let's look into submission format","cf0ba7fe":"Metric for Evaluation\n Root Mean Square Error(RMSE) is used for evaluation of results.\n \nRMSE is defined as\n\n\n$$\\sqrt{\\Sigma_{i=1}^{n}{\\Big(\\frac{\\hat{y}_i - y_i}{n}\\Big)^2}}$$\n\nwhere $n$ denotes the number of samples, $y_i$ the ground truth value and $\\hat{y}_i$ the prediction value.","2a1a6c52":"The dataset contains image files of pet animals. Also the features related to images are given in an additional csv file.\nLets look into the train csv file."}}