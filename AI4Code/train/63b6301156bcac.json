{"cell_type":{"b6c85f55":"code","cd9cc567":"code","fc33bfa4":"code","9e95d663":"code","e0e9ef8d":"code","768cc845":"code","e149817c":"code","af5180df":"code","93e9b0cf":"code","61421d8a":"code","2253a55c":"code","f433e6a5":"code","b5c14cb6":"code","3e6ce28f":"code","e7ea6214":"code","8b3bf61b":"code","7565327f":"markdown","6ce39cee":"markdown","d4582f30":"markdown","4811ca6a":"markdown","7876e291":"markdown"},"source":{"b6c85f55":"#Import Libraries\nimport sys \nimport pandas as pd \nimport matplotlib \nimport numpy as np \nimport scipy as sc\nimport IPython\nfrom IPython import display \nimport sklearn\n\n#ignore warnings\nimport warnings\nwarnings.filterwarnings('ignore')\nprint('-'*25)\n\n#Algorithms\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\n\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\n\nfrom sklearn import metrics\nfrom sklearn.svm import SVC\n\nfrom scipy.stats import skew\n\n#CV\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n\nfrom sklearn import feature_selection\nfrom sklearn import model_selection\nfrom sklearn import metrics\n\n#Visualization\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport matplotlib.pylab as pylab\nimport seaborn as sns\nfrom pandas.tools.plotting import scatter_matrix\n\n#time\nfrom time import time\n\n%matplotlib inline\nmpl.style.use('ggplot')\nsns.set_style('white')\npylab.rcParams['figure.figsize'] = 12,8\n\n","cd9cc567":"#Import Data\ndata_train = pd.read_csv('..\/input\/train.csv')\ndata_test  = pd.read_csv('..\/input\/test.csv')\ndata_submit = pd.read_csv('..\/input\/sample_submission.csv')\n\n","fc33bfa4":"#Add dfs\nprint('Shape Of train df:',data_train.shape)\nprint('Shape Of test df:',data_test.shape)\ndata_train_test = pd.concat([data_train, data_test]) #Remeber the index because we have to split data later (200000 in train )\nprint('Shape Of train test  df:',data_train_test.shape)","9e95d663":"#Remove id\nprint('Shape Of old train test df:',data_train_test.shape)\ndata_train_test.drop(['id'], axis=1, inplace = True)\nprint('Shape Of new train test df:',data_train_test.shape)","e0e9ef8d":"#Null Values\nprint('Null in  data:',data_train.isnull().sum().sum())\nprint('Null in  data:',data_test.isnull().sum().sum())","768cc845":"#PLot\nsns.countplot(data_train['target'])","e149817c":"#Skewness in dataset\n#As a general rule of thumb: If skewness is less than -1 or greater than 1, the distribution is highly skewed. \n#If skewness is between -1 and -0.5 or between 0.5 and 1, the distribution is moderately skewed.\n\nnumeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\nnumerics2 = []\nfor i in data_train_test.columns:\n    if data_train_test[i].dtype in numeric_dtypes: \n        numerics2.append(i)\n\nskew_features = data_train_test[numerics2].apply(lambda x: skew(x)).sort_values(ascending=False)\nskews = pd.DataFrame({'skew':skew_features})\n#skews","af5180df":"#Correlation\n#correlation of each varibale with target\ndef Correlation(df):\n    numeric_dtypes = ['int16', 'int32', 'int64', 'float16', 'float32', 'float64']\n    numerics2 = []\n    crr = {}\n    for y in df.columns:\n        if df[y].dtype in numeric_dtypes:\n            numerics2.append(y)\n            for col in numerics2:\n                if col != 'target':\n                    crr[col] = np.corrcoef(df[col],df['target'])[1,0]\n                    \n    Crr_df = pd.DataFrame([crr])\n    Crr_df = Crr_df.T\n    Crr_df.columns = ['CorrCoef']\n    Crr_df = Crr_df.sort_values(by='CorrCoef',ascending=False)\n    return Crr_df\n    \n#Correlation(data_train)\n#Very weak correlations ","93e9b0cf":"#Split data\nX_train = data_train.iloc[:,data_train.columns != 'target']\ny_train = data_train.iloc[:,data_train.columns == 'target']\nX_train.drop(['id'], axis=1, inplace = True)\nX_test = data_test.iloc[:,data_test.columns != 'target']\nX_test.drop(['id'], axis=1, inplace = True)","61421d8a":"#XGboost\n\"\"\"\nparams = {\n        'min_child_weight':[3,4,5],\n        'gamma': [1.565,1.575,1.585],\n        'subsample':[.3600,.36250,.36500,],\n        'colsample_bytree': [.94,.95,.96,.97,1],\n        'max_depth': [3,4,5],\n        'n_estimators' : range(475,575,25)\n        }\n#{'colsample_bytree': 0.95, 'gamma': 1.575, 'max_depth': 4, 'min_child_weight': 4, 'n_estimators': 525, 'subsample': 0.3625}\n#{'colsample_bytree': 0.95, 'gamma': 1.585, 'max_depth': 3, 'min_child_weight': 4, 'n_estimators': 525, 'subsample': 0.3625} #0.6208333333333333\n\n\nxgb = XGBClassifier(objective='binary:logistic',\n                    silent=True, nthread=1)\n\n\nskf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 1001)\n\n\n\nrandom_search = RandomizedSearchCV(xgb, param_distributions=params, n_iter=10, scoring='roc_auc', n_jobs=4, cv=skf.split(X_train,y_train), verbose=3, random_state=1001 )\nrandom_search.fit(X_train, y_train)\n\n#print('\\n All results:')\n#print(random_search.cv_results_)\nprint('\\n Best estimator:')\nprint(random_search.best_estimator_)\nprint('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\nprint(random_search.best_score_ * 2 - 1)\nxgb_score_rs = random_search.best_score_ * 2 - 1\nprint('\\n Best hyperparameters:')\nprint(random_search.best_params_)\nresults_random_search = pd.DataFrame(random_search.cv_results_)\n\n\n\ngrid_search  = GridSearchCV(xgb,params,verbose=3,scoring='roc_auc',n_jobs=-1,cv=skf.split(X_train,y_train))\ngrid_search.fit(X_train, y_train)\n\n#print('\\n All results:')\n#print(grid_search.cv_results_)\nprint('\\n Best estimator:')\nprint(grid_search.best_estimator_)\nprint('\\n Best normalized gini score  with parameter combinations:' \nprint(grid_search.best_score_ * 2 - 1)\nxgb_score_gs = grid_search.best_score_ * 2 - 1\nprint('\\n Best hyperparameters:')\nprint(grid_search.best_params_)\nresults_grid_search = pd.DataFrame(grid_search.cv_results_)\n\n\nxgb1 = grid_search.best_estimator_\n\n\n\"\"\"\n\"\"\"\nxgb1 = XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n       colsample_bytree=0.95, gamma=1.585, learning_rate=0.02,\n       max_delta_step=0, max_depth=3, min_child_weight=4, missing=None,\n       n_estimators=525, n_jobs=1, nthread=1, objective='binary:logistic',\n       random_state=0, reg_alpha=0, reg_lambda=1, scale_pos_weight=1,\n       seed=None, silent=True, subsample=0.3625)\n\nstart = time()\nxgb1.fit(X_train,y_train.values.ravel())\nend=time()\ntrain_time_xgb1=end-start\ny_pred_xgb1 = xgb1.predict_proba(X_test)\nxgb1.score(X_train,y_train)\n\n\"\"\"\n","2253a55c":"#GradientBoosting\n\n\"\"\"\nparams = {\n        'loss': ['exponential'],#Tuned\n        'learning_rate': [.001],#Tuned\n        'subsample': [.3375],#Tuned\n        'n_estimators' : [9400],#Tuned\n        'criterion': ['friedman_mse'],#Tuned\n        'min_samples_leaf': [2],#Tuned\n        'min_samples_split': [6], #Tuned\n        'min_weight_fraction_leaf': [0.3500],#Tuned\n        'max_depth': [4],#Tuned\n        'min_impurity_decrease': [0]#Tuned\n        }\n\n\"\"\"\"\"\"\n#Best hyperparameters:\n#0.4887857707509884\n#{'subsample': 0.4, 'n_estimators': 10000}\n#0.4892343815513627\n#{'criterion': 'friedman_mse', 'max_depth': 3, 'min_samples_leaf': 3, 'n_estimators': 10000, 'subsample': 0.4} \n#0.5186387141858841\n#{'criterion': 'friedman_mse', 'max_depth': 3, 'min_samples_leaf': 3, 'min_samples_split': 6, 'min_weight_fraction_leaf': 0.4, 'n_estimators': 10000, 'subsample': 0.4}\n#0.5229489867225716\n#{'criterion': 'friedman_mse', 'learning_rate': 0.001, 'max_depth': 3, 'min_samples_leaf': 3, 'min_samples_split': 6, 'min_weight_fraction_leaf': 0.4, 'n_estimators': 10000, 'subsample': 0.4}\n#0.5370862334032145\n#{'criterion': 'friedman_mse', 'learning_rate': 0.001, 'loss': 'exponential', 'max_depth': 3, 'min_samples_leaf': 3, 'min_samples_split': 6, 'min_weight_fraction_leaf': 0.4, 'n_estimators': 10000, 'subsample': 0.4}\n#0.53748427672956\n#{'criterion': 'friedman_mse', 'learning_rate': 0.001, 'loss': 'exponential', 'max_depth': 3, 'min_impurity_decrease': 0, 'min_samples_leaf': 3, 'min_samples_split': 6, 'min_weight_fraction_leaf': 0.4, 'n_estimators': 10000, 'subsample': 0.4}\n#0.5847222222222224\n#{'criterion': 'friedman_mse', 'learning_rate': 0.001, 'loss': 'exponential', 'max_depth': 3, \n#'min_impurity_decrease': 0, 'min_samples_leaf': 3, 'min_samples_split': 6, 'min_weight_fraction_leaf': 0.4, 'n_estimators': 9500, 'subsample': 0.35}\n\n\n\n\ngb = GradientBoostingClassifier()\n\nskf = StratifiedKFold(n_splits=10, shuffle = True, random_state = 1001)\n\n\"\"\"\n\"\"\"\nrandom_search = RandomizedSearchCV(gb, param_distributions=params, n_iter=10, scoring='roc_auc', n_jobs=-1, cv=skf.split(X_train,y_train), verbose=3, random_state=1001 )\n\nrandom_search.fit(X_train, y_train)\n\n#print('\\n All results:')\n#print(random_search.cv_results_)\nprint('\\n Best estimator:')\nprint(random_search.best_estimator_)\nprint('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\nprint(random_search.best_score_ * 2 - 1)\ngb_score_rs = random_search.best_score_ * 2 - 1\nprint('\\n Best hyperparameters:')\nprint(random_search.best_params_)\nresults = pd.DataFrame(random_search.cv_results_)\n\n\n\"\"\"\n\"\"\"\ngrid_search  = GridSearchCV(gb,params,verbose=3,scoring='roc_auc',n_jobs=-1,cv=skf.split(X_train,y_train))\ngrid_search.fit(X_train, y_train)\n\n#print('\\n All results:')\n#print(grid_search.cv_results_)\nprint('\\n Best estimator:')\nprint(grid_search.best_estimator_)\nprint('\\n Best normalized gini score  with parameter combinations:')\nprint(grid_search.best_score_ * 2 - 1)\ngb_score_gs = grid_search.best_score_ * 2 - 1\nprint('\\n Best hyperparameters:')\nprint(grid_search.best_params_)\nresults_grid_search = pd.DataFrame(grid_search.cv_results_)\n\n#gb1 = grid_search.best_estimator_\n\n\"\"\"\n\ngb1  = GradientBoostingClassifier(criterion='friedman_mse', init=None,\n              learning_rate=0.001, loss='exponential', max_depth=3,\n              max_features=None, max_leaf_nodes=None,\n              min_impurity_decrease=0, min_impurity_split=None,\n              min_samples_leaf=3, min_samples_split=6,\n              min_weight_fraction_leaf=0.4, n_estimators=9500,\n              n_iter_no_change=None, presort='auto', random_state=None,\n              subsample=0.35, tol=0.0001, validation_fraction=0.1,\n              verbose=0, warm_start=False)\n\nstart = time()\ngb1.fit(X_train,y_train.values.ravel())\nend=time()\ntrain_time_gb1=end-start\ny_pred_gb1 = gb1.predict_proba(X_test)\ngb1.score(X_train,y_train)\n","f433e6a5":"#RandomForest\n\"\"\"\nparams = {\n        'n_estimators' : [4500],\n        #'criterion': ['gini','entropy'],\n        'min_samples_split': [5],#Tuned \n        'min_samples_leaf': [3],#Tuned \n        'min_weight_fraction_leaf': [0.2],#Tuned #min_weight_fraction_leaf must in [0, 0.5]\n        'max_depth': [5],#Tuned \n        'min_impurity_decrease':[0],\n        'max_leaf_nodes':[50]#Tuned # max_leaf_nodes 1 must be either None or larger than 1\n        }\n\nrf = RandomForestClassifier()\n\n\nskf = StratifiedKFold(n_splits=4, shuffle = True, random_state = 1001)\n\n\"\"\"\n\"\"\"\nrandom_search = RandomizedSearchCV(rf, param_distributions=params, n_iter=10, scoring='roc_auc', n_jobs=4, cv=skf.split(X_train,y_train), verbose=3, random_state=1001 )\n\nrandom_search.fit(X_train, y_train)\n\n#print('\\n All results:')\n#print(random_search.cv_results_)\nprint('\\n Best estimator:')\nprint(random_search.best_estimator_)\nprint('\\n Best normalized gini score  with parameter combinations:')\nprint(random_search.best_score_ * 2 - 1)\nrf_score = random_search.best_score_ * 2 - 1\nprint('\\n Best hyperparameters:')\nprint(random_search.best_params_)\nresults = pd.DataFrame(random_search.cv_results_)\n\nrf1 = random_search.best_estimator_\n\"\"\"\n\"\"\"\ngrid_search  = GridSearchCV(rf,params,verbose=3,scoring='roc_auc',n_jobs=-1,cv=skf.split(X_train,y_train))\ngrid_search.fit(X_train, y_train)\n\n#print('\\n All results:')\n#print(grid_search.cv_results_)\nprint('\\n Best estimator:')\nprint(grid_search.best_estimator_)\nprint('\\n Best normalized gini score  with parameter combinations:')\nprint(grid_search.best_score_ * 2 - 1)\ngb_score_gs = grid_search.best_score_ * 2 - 1\nprint('\\n Best hyperparameters:')\nprint(grid_search.best_params_)\nresults_grid_search = pd.DataFrame(grid_search.cv_results_)\n\nrf1 = grid_search.best_estimator_\n\n\"\"\"\n\n\"\"\"\nrf1 = RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',\n            max_depth=5, max_features='auto', max_leaf_nodes=50,\n            min_impurity_decrease=0, min_impurity_split=None,\n            min_samples_leaf=3, min_samples_split=5,\n            min_weight_fraction_leaf=0.2, n_estimators=4500, n_jobs=None,\n            oob_score=False, random_state=None, verbose=0,\n            warm_start=False)\n\"\"\"\n\"\"\"\nstart = time()\nrf1.fit(X_train,y_train.values.ravel())\nend=time()\ntrain_time_rf1=end-start\ny_pred_rf1 = gb1.predict_proba(X_test)\nrf1.score(X_train,y_train)\n\"\"\"","b5c14cb6":"#LGBoost\n\"\"\"\nparams =  {  \"max_depth\": [],\n              \"learning_rate\" : [0.01,0.05,0.1],\n              \"num_leaves\": [300,500,800,900,1000,1200],\n              \"n_estimators\": [100,500,1000,5000,10000]\n             }\n\nlgb = LGBMClassifier()\n\n\nskf = StratifiedKFold(n_splits=5, shuffle = True, random_state = 1001)\n\n\nrandom_search = RandomizedSearchCV(lgb, param_distributions=params, n_iter=10, scoring='roc_auc', n_jobs=4, cv=skf.split(X_train,y_train), verbose=3, random_state=1001 )\n\nrandom_search.fit(X_train, y_train)\n\n#print('\\n All results:')\n#print(random_search.cv_results_)\nprint('\\n Best estimator:')\nprint(random_search.best_estimator_)\nprint('\\n Best normalized gini score for %d-fold search with %d parameter combinations:' % (folds, param_comb))\nprint(random_search.best_score_ * 2 - 1)\nlgb1_score = random_search.best_score_ * 2 - 1\nprint('\\n Best hyperparameters:')\nprint(random_search.best_params_)\nresults = pd.DataFrame(random_search.cv_results_)\n\n\n#lgb1 = random_search.best_estimator_\n\n\n\ngrid_search  = GridSearchCV(lgb,params,verbose=3,scoring='roc_auc',n_jobs=-1,cv=skf.split(X_train,y_train))\ngrid_search.fit(X_train, y_train)\n\n#print('\\n All results:')\n#print(grid_search.cv_results_)\nprint('\\n Best estimator:')\nprint(grid_search.best_estimator_)\nprint('\\n Best normalized gini score  with parameter combinations:')\nprint(grid_search.best_score_ * 2 - 1)\ngb_score_gs = grid_search.best_score_ * 2 - 1\nprint('\\n Best hyperparameters:')\nprint(grid_search.best_params_)\nresults_grid_search = pd.DataFrame(grid_search.cv_results_)\n\nlgb1 = grid_search.best_estimator_\n\n\"\"\"\n\n\"\"\"\nlgb1 = LGBMClassifier()\n\nstart = time()\nlgb1.fit(X_train,y_train.values.ravel())\nend=time()\ntrain_time_lgb1=end-start\ny_pred_lgb1 = gb1.predict_proba(X_test)\nlgb1.score(X_train,y_train)\n\"\"\"","3e6ce28f":"#Comparison of Training time\n\"\"\"\nmodel = [ 'XGboost','GradientBoosting','RandomForestClassifier','Light GBM']\nTrain_Time = [\n    train_time_xgb1,\n    train_time_gb1,\n    train_time_rf1,\n    train_time_lgb1\n    \n]\nindex = np.arange(len(model))\nplt.bar(index, Train_Time)\nplt.xlabel('Machine Learning Models', fontsize=15)\nplt.ylabel('Training Time', fontsize=15)\nplt.xticks(index, model, fontsize=10, )\nplt.title('Comparison of Training Time of all ML models')\nplt.show()\n\"\"\"","e7ea6214":"#Comparison of Score\n\"\"\"\nmodel = [ 'XGboost','GradientBoosting','RandomForestClassifier','Light GBM']\nScore = [\n    xgb_score,\n    gb_score,\n    rf_score,\n    lgb1_score\n    \n]\nindex = np.arange(len(model))\nplt.bar(index, Score)\nplt.xlabel('Machine Learning Models', fontsize=15)\nplt.ylabel('Score', fontsize=15)\nplt.xticks(index, model, fontsize=10, )\nplt.title('Comparison of Score  of all ML models')\nplt.show()\n\"\"\"","8b3bf61b":"#y_pred = (y_pred_xgb1+y_pred_gb1+y_pred_rf1+y_pred_lgb1)\/4\ny_pred = y_pred_gb1\n\ndata_submit['target'] = y_pred\ndata_submit.to_csv(\"submit.csv\", index=False)","7565327f":"As we can see there is not much skewness in data. So we will not do anything.","6ce39cee":"Now we will compare Training time for each Model.","d4582f30":"We can observe that there is very weak correlations between target value and other variables. ","4811ca6a":"**Don't Overfit**\nA simple kernal for beginners.","7876e291":"Now we will try different models namely XGboost, Gradient Boosting, Random forest and Light Gradinet Boosting.\nWe will be using Grid search CV with Stratified Kfold. You can also try RandomizedSearchCV. We will tune the parameter for each model separatley and than will use best set of hyperparameter for each model. "}}