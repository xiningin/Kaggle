{"cell_type":{"8434d5a7":"code","cc79b7bd":"code","494473e3":"code","b5fe3d56":"code","f177420f":"code","e6a03107":"code","4c7bd424":"code","89380327":"code","dda34895":"code","2219abb3":"code","6fcc8778":"code","4dc23998":"code","cabe5efa":"code","cf353e4c":"code","93ba1607":"code","f19eb341":"code","49ca86a5":"code","a9f92130":"code","ddba35f6":"code","9a6140e4":"code","49f8e5ba":"code","f3c87992":"code","5214897a":"code","da2b7a72":"code","bfcd1574":"code","7b1c6910":"code","13abc51f":"code","07aea248":"code","a278753a":"code","7856a0b9":"code","aee27168":"code","ceff758c":"code","f131494c":"code","de3c797b":"code","877714bf":"markdown","0aef65ee":"markdown","4a95e8c8":"markdown","a4b8b8f5":"markdown","8585aaff":"markdown","ba10ea60":"markdown","339d0c23":"markdown","ae5412f3":"markdown","6f0b1df4":"markdown","b74c5f45":"markdown","79832fe9":"markdown","95e87d89":"markdown","e4ad570d":"markdown","f6304896":"markdown","908798bf":"markdown","27a33015":"markdown","67f985e8":"markdown","90e26c76":"markdown","8aec2520":"markdown","2352a287":"markdown","f163fcff":"markdown","7a9fd9c0":"markdown","5c1d5a40":"markdown","0cd83e53":"markdown","49f04c91":"markdown","32479dac":"markdown","9a20f470":"markdown"},"source":{"8434d5a7":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","cc79b7bd":"pd.options.display.max_columns = 15\npd.options.display.max_rows = 70","494473e3":"train = pd.read_csv('..\/input\/bigmart-sales-data\/Train.csv')\ntest = pd.read_csv('..\/input\/bigmart-sales-data\/Test.csv')","b5fe3d56":"#data types\ntrain.dtypes","f177420f":"#null check\ntrain.info()\ntrain.isnull().sum()\/len(train)*100","e6a03107":"len(pd.unique(train[\"Item_Identifier\"])) #1559 different item\nlen(pd.unique(train[\"Outlet_Identifier\"])) #10 outlets","4c7bd424":"### OUTLET SALES ANALYSIS ###\ntrain.groupby([\"Outlet_Identifier\"])[\"Item_Outlet_Sales\"].agg(\"mean\").sort_values(ascending=False)","89380327":"train.groupby([\"Outlet_Identifier\"])[\"Item_Weight\"].agg(\"mean\").sort_values(ascending=False)","dda34895":"train.groupby([\"Outlet_Establishment_Year\"])[\"Item_Outlet_Sales\"].agg(\"mean\").sort_values(ascending=False)\ntrain.groupby([\"Outlet_Establishment_Year\"])[\"Item_Outlet_Sales\"].agg(\"count\").sort_values(ascending=False)","2219abb3":"train.groupby([\"Outlet_Size\"])[\"Item_Outlet_Sales\"].agg(\"mean\").sort_values(ascending=False)\ntrain.groupby([\"Outlet_Size\"])[\"Item_Outlet_Sales\"].agg(\"count\").sort_values(ascending=False)","6fcc8778":"train.groupby([\"Outlet_Identifier\",\"Item_Type\"])[\"Item_Outlet_Sales\"].agg(\"count\").sort_values().groupby(\"Outlet_Identifier\").tail(1)","4dc23998":"train.groupby([\"Outlet_Identifier\",\"Item_Type\"])[\"Item_Outlet_Sales\"].agg(\"mean\").sort_values().groupby(\"Outlet_Identifier\").tail(1)","cabe5efa":"### ITEM ANALYSIS ###\nplot_df = train.groupby([\"Item_Type\"])[\"Item_Weight\"].agg('mean').sort_values(ascending=False).reset_index()\nticks = np.arange(0,len(plot_df))\nlabels = plot_df[\"Item_Type\"]\n\nplt.figure(figsize=(8,8))\nplot_df.plot(kind='bar')\nplt.xticks(ticks,labels)\nplt.show()","cf353e4c":"train.groupby([\"Item_Type\",\"Item_Fat_Content\"])[\"Item_Fat_Content\"].agg(\"count\").sort_values().groupby(\"Item_Type\").tail(1)","93ba1607":"plot_df = train.groupby([\"Item_Type\"])[\"Item_Visibility\"].agg('mean').sort_values(ascending=False).reset_index()\nticks = np.arange(0,len(plot_df))\nlabels = plot_df[\"Item_Type\"]\n\nplt.figure(figsize=(8,8))\nplot_df.plot(kind='bar')\nplt.xticks(ticks,labels)\nplt.show()","f19eb341":"df = pd.concat([train,test],axis=0)","49ca86a5":"pd.value_counts(df[df[\"Outlet_Size\"].isnull()][\"Outlet_Identifier\"])","a9f92130":"pd.value_counts(df[df[\"Item_Weight\"].isnull()][\"Item_Identifier\"])","ddba35f6":"#filling outlet size with similar rows wrt outlet type\ndf[\"Outlet_Size\"] = df.groupby([\"Outlet_Type\"])[\"Outlet_Size\"].transform(lambda x: x.fillna(x.mode()[0]))\n\n\n#filling item weight with similar rows wrt Item_Type and Item_Fat_Content\ndf[\"Item_Weight\"] = df.groupby([\"Item_Type\",\"Item_Fat_Content\"])[\"Item_Weight\"].transform(lambda x: x.fillna(x.median()))","9a6140e4":"#encoding ordinal column outlet size\ndf['Outlet_Size']=df['Outlet_Size'].replace({'Small':1,\n                                             'Medium':2,\n                                             'High':3})\n","49f8e5ba":"pd.value_counts(df['Item_Fat_Content'])","f3c87992":"#correcting mispelled column\ndf['Item_Fat_Content'] = df['Item_Fat_Content'].replace({'LF': 'lf',\n                                                         'Low Fat':'lf',\n                                                         'low fat':'lf',\n                                                         'reg':'reg',\n                                                         'Regular':'reg'\n                                                        })","5214897a":"items = df[\"Item_Identifier\"]\ndf = df.drop(columns=\"Item_Identifier\")\ndf = df.drop(columns=\"Outlet_Identifier\")\ndf[\"Outlet_Year\"] = 2020- df[\"Outlet_Establishment_Year\"]\ndf = df.drop(columns=\"Outlet_Establishment_Year\")\n","da2b7a72":"#Categorical value handling\ndf.columns[df.dtypes=='object']\ndf = pd.get_dummies(df,columns = df.columns[df.dtypes=='object'])","bfcd1574":"for col in df.columns[df.columns!=\"Item_Outlet_Sales\"]:\n    df[col] = (df[col]-df[col].min()) \/ (df[col].max()-df[col].min())","7b1c6910":"df['Item_MRP_X_Visi']=df['Item_MRP'] * df['Item_Visibility']\n#df['Item_MRP_+_Visi']=df['Item_MRP'] + df['Item_Visibility']\ndf['Item_MRP_X_Weight']=df['Item_MRP'] * df['Item_Weight']\n#df['Item_MRP_+_Weight']=df['Item_MRP'] + df['Item_Weight']\n#df['Fat_Con_+_Weight']=df['Item_Fat_Content']+df['Item_Weight']\n#df['Fat_Con_X_Weight']=df['Item_Fat_Content']*df['Item_Weight']\ndf['Total_Points']=df['Item_MRP']*df['Item_Visibility']*df['Item_Weight']\ndf['MrpPerUnit']=df['Item_MRP']\/(df['Outlet_Size']+1)","13abc51f":"train = df[0:len(train)]\ntest = df[len(train):]\ntest.drop(columns=\"Item_Outlet_Sales\", inplace=True)","07aea248":"from sklearn.ensemble import RandomForestRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.model_selection import train_test_split, RandomizedSearchCV\nfrom sklearn.metrics import mean_squared_error\nX = train.drop(columns=\"Item_Outlet_Sales\")\ny = train[\"Item_Outlet_Sales\"]","a278753a":"X_train, X_test, y_train, y_test = train_test_split(X,y,test_size = 0.3, random_state=5)","7856a0b9":"### RANDOM FOREST ###\n\"\"\"\nrfr = RandomForestRegressor()\n\nrf_grid = {'n_estimators' : [100,200,500,800,1000,1200],\n           'max_depth' : [3,5,7,10,15,25,40,None],\n           'min_samples_split':[2,4,6,10],\n           'min_samples_leaf':[2,4,6,8]   \n           }\n\nsearch = RandomizedSearchCV(rfr,rf_grid,scoring='neg_mean_squared_error',cv=3, verbose=2, n_jobs=6, n_iter = 50)\nsearch.fit(X,y)\n\n\nprint(search.best_params_)\nprint(search.best_estimator_)\nprint(search.cv_results_)\nprint(search.best_score_)\n\"\"\"\n\nrfr_best = RandomForestRegressor(n_estimators=1200, max_depth=5,min_samples_split=2,min_samples_leaf=2)\nrfr_best.fit(X_train,y_train)\npred = rfr_best.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, pred))","aee27168":"### LGBM REGRESSOR ###\n\"\"\"\nlgbmr = LGBMRegressor()\nlgb_grid = {\n    'n_estimators': [100, 200, 400, 500],\n    'colsample_bytree': [0.9, 1.0],\n    'max_depth': [5,10,15,20,25,35,None],\n    'num_leaves': [20, 30, 50, 100],\n    'reg_alpha': [1.0, 1.1, 1.2, 1.3],\n    'reg_lambda': [1.0, 1.1, 1.2, 1.3],\n    'min_split_gain': [0.2, 0.3, 0.4],\n    'subsample': [0.8, 0.9, 1.0],\n    'learning_rate': [0.05, 0.1]\n}\n\nsearch = RandomizedSearchCV(lgbmr,lgb_grid,scoring='neg_mean_squared_error',cv=3, verbose=2, n_jobs=6, n_iter = 100)\nsearch.fit(X,y)\n\nprint(search.best_params_)\nprint(search.best_estimator_)\nprint(search.cv_results_)\nprint(search.best_score_)\n\"\"\"\n\n#Default parameters giving almost exact result as the grid search gives. So I used default ones.\nlgb_best = LGBMRegressor()\nlgb_best.fit(X_train,y_train)\npred2 = lgb_best.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, pred2))","ceff758c":"### STACKING ###\n\nfrom mlxtend.regressor import StackingCVRegressor\nfrom xgboost import XGBRegressor\n\nxgb = XGBRegressor()\nlgbm = LGBMRegressor()\nrf = RandomForestRegressor()\n\nstack = StackingCVRegressor(regressors=(rf, lgbm, xgb),\n                            meta_regressor=xgb, cv=3,\n                            use_features_in_secondary=True,\n                            store_train_meta_features=True,\n                            shuffle=False,\n                            random_state=42)\n\nstack.fit(X_train, y_train)\n\nX_test.columns = ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33']\nstack_pred = stack.predict(X_test)\nnp.sqrt(mean_squared_error(y_test, stack_pred))\n","f131494c":"### ENSEMBLE ###\npred_df = pd.DataFrame({'pred':pred, 'pred2':pred2, 'stack': stack_pred, 'target': y_test})\n\nfinal_pred = pred*0.6 + pred2*0.2 + stack_pred*0.2\n\nprint(np.sqrt(mean_squared_error(y_test, final_pred)))","de3c797b":"### SUBMISSION ###\npred1_test = rfr_best.predict(test)\npred2_test = lgb_best.predict(test)\ntest.columns = ['f0', 'f1', 'f2', 'f3', 'f4', 'f5', 'f6', 'f7', 'f8', 'f9', 'f10', 'f11', 'f12', 'f13', 'f14', 'f15', 'f16', 'f17', 'f18', 'f19', 'f20', 'f21', 'f22', 'f23', 'f24', 'f25', 'f26', 'f27', 'f28', 'f29', 'f30', 'f31', 'f32', 'f33']\nstack_test = stack.predict(test)\nfinal_pred_test = pred1_test*0.6 + pred2_test*0.2 + stack_test*0.2\n\n","877714bf":"And we dont have item weight of several items","0aef65ee":"# Introduction","4a95e8c8":"New features:","a4b8b8f5":"Breakfast, seafood and dairy are more visible.","8585aaff":"I will skip the paratemer optimisation to run this kernel faster but I keep them in comment lines. ","ba10ea60":"These analysis can go much further but I will move to modelling part since this dataset is quite simple. ","339d0c23":"We are going to fill these missing values with similar rows. We will use \"Outlet_Type\" for estimating missing Outlet_Size, and \"Item_Type\",\"Item_Fat_Content\" features to estimate item_weight. ","ae5412f3":"It seems like \"Snack foods\" + \"Fruits and Vegetables\" are the most selling products across outlets","6f0b1df4":"And apply min-max scaling because we will create some new variables by doing linear aggregations. If we don't do scaling before the aggregations, larger valued features will absorb the smaller ones.","b74c5f45":"Item weight is a low-valued column. So its variation is numerically low. But we have null values.","79832fe9":"Now we have to encode Outlet_Size variable since it contains ordinal information. High>Medium>Small ","95e87d89":"Now lets check each outlets most sold items.","e4ad570d":"Stacking technique will have a smoothing effect on out first-phase predictions. It will look at RF and LGBMRegressor results, and try to smooth them thorugh the correct values. Don't forget that we use default parameters of estimators in StackingRegressor.\n\nYou can gather more info on stacking here: https:\/\/mlfromscratch.com\/model-stacking-explained\/#\/","f6304896":"We will drop identifiers since they are only ID's. We have the necessary info for the prediction.","908798bf":"# Modelling","27a33015":"Among the outlets, average sales vary. ","67f985e8":"Most products in the dataframe are low-fat","90e26c76":"Lets use dummy variables for categorical variables:","8aec2520":"1) Medium and high sized outlets sell with higher prices.\n2) Medium and small sized outlets sell more products.","2352a287":"We dont have size values of these outlets.","f163fcff":"And Item_Fat_Content has mispelled indices. Lets fix them too.","7a9fd9c0":"And seafood and breakfast are sold to higher prices.","5c1d5a40":"Lastly, we will combine 3 predictions. In ","0cd83e53":"You can achieve better scores with different coefficients in ensemble. This is an example although it increased our rmse just a liiiiitle bit :)","49f04c91":"Hello everyone.\nIn this notebook we will analyze a sales dataset. It includes these practices:\n\n* Brief analysis to understand the data\n* Null value filling with similar rows\n* Categoric feature handling\n* Quick feature engineering\n* Modelling with Random Forest, LightGBM Regressor and stacking + ensembling methods\n\nLet's begin.","32479dac":"# Data Preprocessing","9a20f470":"Average sell amount is almost random but older outlets seems to sell higher number of products."}}