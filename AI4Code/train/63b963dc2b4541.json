{"cell_type":{"0200f9de":"code","b662983b":"code","e69cdfa9":"code","c806a403":"code","902157e6":"code","1783ae0a":"code","347b7854":"code","2da73647":"code","11628dc9":"code","2d88ecba":"code","e6217a54":"code","5393df23":"code","78075202":"code","18a31645":"code","4b92a7d0":"code","da4148d6":"code","de89e9c5":"code","846794e2":"code","d6c599f5":"code","69140782":"code","c0ce0a7f":"code","bfffcaa6":"code","9ca46a17":"code","4e528082":"code","0d3ef581":"code","7d6fde92":"code","cfa2c0b3":"code","d6f022a6":"code","2d4816c9":"code","91ae697a":"code","42bae47b":"code","a4f23925":"code","2e595115":"code","8881dd26":"code","24bec1a5":"code","28a88bdc":"code","2d3b8b79":"code","dd63fe5a":"code","6a600d74":"code","0ab47388":"code","038ac27e":"code","cf69934c":"markdown","c96b71f0":"markdown","d4493e0b":"markdown","97d5ad1d":"markdown","abdca058":"markdown","d28fab12":"markdown","c41c730b":"markdown","61041010":"markdown","d2ad0e49":"markdown","f742ec17":"markdown","605a2616":"markdown","ef35815d":"markdown","73ac0910":"markdown","9509ad5b":"markdown","f71ffebe":"markdown","88175e4e":"markdown","647a2554":"markdown","04f3d05c":"markdown","b4aecd0f":"markdown","a19e9aaf":"markdown","4d511375":"markdown","d249faf8":"markdown","786dd364":"markdown","e6024abe":"markdown","20edfcf1":"markdown","c571b483":"markdown","cec87bf7":"markdown","1020d2ba":"markdown","4dc3caf8":"markdown","a7d921fe":"markdown","767a3704":"markdown","dccc9523":"markdown","830c8f7e":"markdown","8cd9afed":"markdown"},"source":{"0200f9de":"from sklearn.feature_selection import SelectKBest, SelectPercentile, chi2, f_classif\nfrom sklearn.linear_model import LinearRegression #Selecci\u00f3n VIF de caracter\u00edsticas\nfrom sklearn.linear_model.logistic import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom catboost import CatBoostClassifier, Pool, cv\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import precision_score\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import recall_score\nfrom sklearn.metrics import make_scorer\nfrom sklearn.metrics import roc_curve\nfrom sklearn.metrics import f1_score\nfrom sklearn.metrics import auc\nimport matplotlib.pyplot as plt\nfrom hyperopt import hp\nimport lightgbm as lgb\nimport numpy as np\nimport seaborn as sns\nimport pandas as pd\nimport warnings\nimport imblearn\nimport zipfile\nimport time\n\n%pylab\n%matplotlib inline\n%config InlineBackend.figure_format = 'retina'\nwarnings.simplefilter(\"ignore\")","b662983b":"#zf = zipfile.ZipFile('datos\\santander-customer-transaction-prediction.zip') \ndf_train = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/train.csv')\ndf_test = pd.read_csv('..\/input\/santander-customer-transaction-prediction\/test.csv')","e69cdfa9":"df_train.head()","c806a403":"df_train.info()","902157e6":"df_train.describe()","1783ae0a":"#We note that the dependent\/target variable is very unbalanced.\ndf_train.target.value_counts().plot.bar() #.plot(kind=\"bar\")","347b7854":"print(\"There is {}% of values 1 in the target variable\".format(100*df_train['target'].value_counts()[1]\/df_train.shape[0], 2))","2da73647":"df_train.isnull().sum()","11628dc9":"#We have many variables, we look for a method to specifically locate null values\nnull_columns=df_train.columns[df_train.isnull().any()]\ndf_train[null_columns].isnull().sum()\nprint(df_train[df_train.isnull().any(axis=1)][null_columns].head())\nprint('It can\u00b4t find null values throughout the df')","2d88ecba":"#Separation of the target variable and the explanatory\ntarget = 'target'\nfeatures = list(df_train.columns)\nfeatures.remove('target')\nfeatures.remove('ID_code')\n#Separating the labels from the target variable\nt0 = df_train[df_train['target'] == 0]\nt1 = df_train[df_train['target'] == 1]\nplt.figure(figsize=(16,6))\nplt.title(\"Distribuci\u00f3n de la media por fila\")\nsns.distplot(t0[features].mean(axis=1),color=\"red\", kde=True,bins=120, label='target = 0')\nsns.distplot(t1[features].mean(axis=1),color=\"blue\", kde=True,bins=120, label='target = 1', hist_kws={'alpha':0.3})\nplt.legend(); plt.show()","e6217a54":"corr_matrix = df_train.corr().abs()\nhigh_corr_var=np.where(corr_matrix>0.5)\nhigh_corr_var=[(corr_matrix.columns[x],corr_matrix.columns[y]) for x,y in zip(*high_corr_var) if x!=y and x<y]\nif len(high_corr_var)==0:\n    print('There are no correlated variables')","5393df23":"#Generate two variables with the number of records in each class\ncount_class_0, count_class_1 = df_train.target.value_counts()\n\n#Divide into two df with each class\ndf_class_0 = df_train[df_train['target'] == 0]\ndf_class_1 = df_train[df_train['target'] == 1]\n\n#Undersampling with the 'sample' pandas property\ndf_class_0_under = df_class_0.sample(count_class_1)\ndf_train_under = pd.concat([df_class_0_under, df_class_1], axis=0)\n\nprint('Undersampling is in a number of records:')\nprint(df_train_under.target.value_counts())\n\ndf_train_under.target.value_counts().plot(kind='bar', title='Count (target)');","78075202":"#Separation of the target variable and the explanatory\ntarget = 'target'\nfeatures = list(df_train_under.columns)\nfeatures.remove('target')\nfeatures.remove('ID_code')\nx_train = df_train_under[features]\ny_train = df_train_under[target]\n\n#Divide dataset into training and validation\nx_train, x_test, y_train, y_test = train_test_split(x_train, y_train, train_size=0.75, random_state = 0, stratify=y_train)","18a31645":"#Check result\nprint(y_train.value_counts())","4b92a7d0":"import imblearn\nfrom imblearn.under_sampling import RandomUnderSampler\n\nsubm = RandomUnderSampler(return_indices=True)\nx_subm, y_subm, id_subm = subm.fit_sample(df_train[features], df_train[target])\n\ny_subm_plot = pd.DataFrame(y_subm)\ny_subm_plot[0].value_counts().plot(kind='bar', title='Count (target)');","da4148d6":"#Check the result\nprint(y_subm_plot[0].value_counts())","de89e9c5":"var_sk = SelectKBest(f_classif, k = 50)\nx_sk = var_sk.fit_transform(x_train, y_train)\n\nprint(u\"Number of final features:\", x_sk.shape[1])\nprint(u\"List of final features: \\n\", x_train.columns[var_sk.get_support()])\nx_train_50best = x_train[x_train.columns[var_sk.get_support()]]","846794e2":"var_pc = SelectPercentile(f_classif, percentile = 50)\nx_pc = var_pc.fit_transform(x_train, y_train)\n\nprint(u\"Number of final features:\", x_pc.shape[1])\nprint(u\"List of final features: \\n\", x_train.columns[var_pc.get_support()])\nx_train_100best = x_train[x_train.columns[var_pc.get_support()]]","d6c599f5":"def metricas(y_true, y_pred):\n    print(u'La matriz de confusi\u00f3n es ')\n    print(confusion_matrix(y_true, y_pred))\n\n    print(u'Precisi\u00f3n:', accuracy_score(y_true, y_pred))\n    print(u'Exactitud:', precision_score(y_true, y_pred))\n    print(u'Exhaustividad:', recall_score(y_true, y_pred))\n    print(u'F1:', f1_score(y_true, y_pred))\n\n    false_positive_rate, recall, thresholds = roc_curve(y_true, y_pred)\n    roc_auc = auc(false_positive_rate, recall)\n\n    print(u'AUC:', roc_auc)\n\n    plot(false_positive_rate, recall, 'b');\n    plot([0, 1], [0, 1], 'r--');\n    title(u'AUC = %0.2f' % roc_auc);","69140782":"%%time\nlr_classifier = LogisticRegression().fit(x_train, y_train)\ny_train_pred = lr_classifier.predict(x_train)\n\nprint('M\u00e9tricas de entrenamiento:')\nmetricas(y_train, y_train_pred);","c0ce0a7f":"%%time\n#See the overfitting in the test dataset\ny_test_pred  = lr_classifier.predict(x_test)\nprint('M\u00e9tricas de validaci\u00f3n:')\nmetricas(y_test, y_test_pred);","bfffcaa6":"%%time\nrf_classifier = RandomForestClassifier(n_estimators = 5,\n                                       max_depth = 7, #Without limiting the depth of the tree is overfitting is even greater AUC:0.93\n                                       random_state = 1)\nrf_classifier.fit(x_train, y_train)\ny_pred = rf_classifier.predict(x_train)\n\nprint('M\u00e9tricas de entrenamiento:')\nmetricas(y_train, y_pred);","9ca46a17":"#Check that this method performs well but we will check the overfitting\n#Comprobamos que este m\u00e9todo tiene buen rendimiento pero comprobaremos el sobreajuste\ny_test_pred = rf_classifier.predict(x_test)\nprint('M\u00e9tricas de validaci\u00f3n:')\nmetricas(y_test, y_test_pred);","4e528082":"#Definimos the parameter of the index of categorical properties although \n#in this case as we said by the previous data exploration, we do not have such variables\ncategorical_features_indices = np.where(x_train.dtypes != np.float)[0]","0d3ef581":"%%time\n#Apply the model by setting some initial generic parameters\ncat_model = CatBoostClassifier(\n        depth=4,\n        custom_loss=['AUC'],\n        learning_rate=0.3,\n        verbose=50,\n        iterations=None,\n        od_type='Iter',\n        early_stopping_rounds=10\n)\n\ncat_model.fit(x_train,y_train,eval_set=(x_test,y_test),use_best_model=True)#para mejorar el procesado paramos en el mejor ajuste\n\npred = cat_model.predict_proba(x_test)[:,1]\ny_train_pred = cat_model.predict(x_train)\n#print('AUC de validaci\u00f3n: ',roc_auc_score(y_test, pred))\nprint('M\u00e9tricas de entrenamiento:')\nmetricas(y_train, y_train_pred); #Probamos el rendimiento del modelo","7d6fde92":"y_test_pred  = cat_model.predict(x_test)\nprint('M\u00e9tricas de validaci\u00f3n:')\nmetricas(y_test, y_test_pred); #Test on the validation sample to see if it shows overfitting","cfa2c0b3":"#Define the parameter of the index of categorical properties although\n#in this case as we said by the previous data exploration, we do not have such variables\ncategorical_features_indices = np.where(x_train.dtypes != np.float)[0]\nfeature_names = x_train.columns.tolist()","d6f022a6":"# LightGBM dataset formatting \nlgtrain = lgb.Dataset(x_train, y_train,\n                feature_name=feature_names,\n                categorical_feature = categorical_features_indices)\nlgvalid = lgb.Dataset(x_test, y_test,\n                feature_name=feature_names,\n                categorical_feature = categorical_features_indices)","2d4816c9":"#Set some generic initial parameters\nparams = {\n    'objective' : 'binary',\n    #'metric' : 'rmse',\n    'num_leaves' : 200,\n    'max_depth': 10,\n    'learning_rate' : 0.01,\n    #'feature_fraction' : 0.6,\n    'verbosity' : -1\n}\nparams['metric']=['auc', 'binary_logloss']","91ae697a":"%%time\n#Apply the model\nlgb_clf = lgb.train(\n    params,\n    lgtrain,\n    #num_iterations=2000,\n    valid_sets=[lgtrain, lgvalid],\n    valid_names=[\"train\", \"valid\"],\n    early_stopping_rounds=500,\n    verbose_eval=500\n)\n\n#Training Prediction::\ny_train_pred = lgb_clf.predict(x_train)\n\n#Convert to binary values because in this model it gives us probabilities\nfor i in range(len(y_train_pred)):\n    if y_train_pred[i]>=.5:       # setting threshold to .5\n        y_train_pred[i]=1\n    else:\n        y_train_pred[i]=0\nprint('M\u00e9tricas de entrenamiento:')      \nmetricas(y_train, y_train_pred);","42bae47b":"#Validation prediction:\ny_test_pred = lgb_clf.predict(x_test)\n\n#Convert to binary values because in this model it gives us probabilities\nfor i in range(len(y_test_pred)):\n    if y_test_pred[i]>=.5:       # setting threshold to .5\n        y_test_pred[i]=1\n    else:\n        y_test_pred[i]=0\nprint('M\u00e9tricas de validaci\u00f3n:')\nmetricas(y_test, y_test_pred);","a4f23925":"#%%time\n#cat_model = CatBoostClassifier(verbose=50)\n#Definition parameters space\n#params = {'depth'         : [3,4,5,6],\n#          'learning_rate' : [0.01,0.05,0.1,0.35,0.4],\n#          'iterations'    : [30,50,125,150],\n#          'l2_leaf_reg': [3,1,2,5,10]\n#          }\n#grid = GridSearchCV(estimator=cat_model, param_grid = params, cv = 3, n_jobs=-1)\n#grid.fit(x_train,y_train)\n\n#print(\"\\n La mejor m\u00e9trica de validaci\u00f3n cruzada:\\n\", grid.best_score_)\n#print(\"\\n Los mejores par\u00e1metros:\\n\", grid.best_params_)","2e595115":"%%time\n#Applying these \"optimal paramenters\" we would get the following model in Catboost\ncat_model = CatBoostClassifier(\n        depth= 3,\n        learning_rate=0.4,\n        iterations=150,\n        l2_leaf_reg=10,\n        custom_loss=['AUC'],\n        verbose=50,\n        random_seed=501\n        )\ncat_model.fit(x_train,y_train,eval_set=(x_test,y_test),use_best_model=True)\n\ny_train_pred = cat_model.predict_proba(x_train)[:,1]\ny_test_pred = cat_model.predict_proba(x_test)[:,1]\nprint('AUC_train: ',roc_auc_score(y_train, y_train_pred))\nprint('AUC_test: ',roc_auc_score(y_test, y_test_pred))","8881dd26":"#Complete but very demanding parameters for processing\n#lgb_grid_params = {\n#    'objetive':['binary'],\n#    'boosting_type' : ['gbdt'],\n#    'learning_rate':  [0.05, 0.1 , 0.15, 0.2 , 0.255, 0.3], \n#    'max_depth': [ 5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15],\n#    'min_child_weight': [1, 2, 3, 4, 5, 6, 7],\n#    'num_leaves': [20, 30, 40],\n#    'min_child_samples': [20, 33, 40, 50],\n#    'colsample_bytree': [0.3, 0.4, 0.5, 0.6, 0.7],\n#    'n_estimators': [50, 100, 118, 130],\n#    'subsample' : [0.7,0.75],\n#    'reg_alpha' : [1,1.2],\n#    'reg_lambda' : [1,1.2,1.4],\n#    'random_state' : [501]\n#}","24bec1a5":"#%%time\n#mdl = lgb.LGBMClassifier(boosting_type= 'gbdt',\n#          objective = 'binary',\n#          metric= ['binary_logloss', 'auc'],\n          #n_jobs = 3, # Updated from 'nthread'\n#          silent = True)\n\n#grid = GridSearchCV(estimator=mdl, param_grid = lgb_grid_params, cv = 3, n_jobs=-1)\n#grid.fit(x_train,y_train)\n\n#print(\"\\n El mejor estimador:\\n\", grid.best_estimator_)\n#print(\"\\n La mejor m\u00e9trica de validaci\u00f3n cruzada:\\n\", grid.best_score_)\n#print(\"\\n Los mejores par\u00e1metros:\\n\", grid.best_params_)","28a88bdc":"%%time\n#LightGBM dataset formatting \nlgtrain = lgb.Dataset(x_train, y_train)\nlgvalid = lgb.Dataset(x_test, y_test)\n\n#Applying these optimal paramentros we would get the following model in LightGBM\nparams = {\n    'objective' : 'binary',\n    'num_leaves' : 20,\n    #'max_depth': 10,\n    'learning_rate' : 0.255,\n    #'feature_fraction' : 0.6,\n    'min_child_samples': 33,\n    'n_estimators': 118,\n    'verbosity' : 50,\n    'random_state':501\n}\nparams['metric']=['auc', 'binary_logloss']\n\n#Training model\nlgb_clf = lgb.train(\n    params,\n    lgtrain,\n    #num_iterations=20000,\n    valid_sets=[lgtrain, lgvalid],\n    valid_names=[\"train\", \"valid\"],\n    early_stopping_rounds=500,\n    verbose_eval=500,\n    feature_name='auto', \n    categorical_feature='auto'\n)\n\nprint(\"RMSE of train:\", np.sqrt(mean_squared_error(y_train, lgb_clf.predict(x_train))));\ny_train_pred = lgb_clf.predict(x_train);\nprint('AUC of train: ',roc_auc_score(y_train, y_train_pred ));\ny_test_pred = lgb_clf.predict(x_test);\nprint('AUC of test: ',roc_auc_score(y_test, y_test_pred ));","2d3b8b79":"feature_importances = lgb_clf.feature_importance()\nfeature_names = x_train.columns\nfor score, name in sorted(zip(feature_importances, feature_names), reverse=True):\n    print('{}: {}'.format(name, score))","dd63fe5a":"fea_imp = pd.DataFrame({'imp': feature_importances, 'col': feature_names})\nfea_imp = fea_imp.sort_values(['imp', 'col'], ascending=[True, False]).iloc[-30:]\nfea_imp.plot(kind='barh', x='col', y='imp', figsize=(10, 7), legend=None)\nplt.title('LightGBM - Feature Importance')\nplt.ylabel('Features')\nplt.xlabel('Importance');","6a600d74":"df_test.head()","0ab47388":"target = 'target'\nfeatures = list(df_test.columns)\nfeatures.remove('ID_code')\nX_test = df_test[features]\n\n#Prediction with choose model LGBM\nY_prediction = lgb_clf.predict(X_test);\n\n#Convert to binary values because in this model it gives us probabilities\nfor i in range(len(Y_prediction)):\n    if Y_prediction[i]>=.5:       # setting threshold to .5\n        Y_prediction[i]=1\n    else:\n        Y_prediction[i]=0\n\nsub_df = pd.DataFrame({\"ID_code\":df_test[\"ID_code\"].values})\nsub_df[\"target\"] = Y_prediction\nsub_df[\"target\"] = sub_df[\"target\"].astype(int)\nsub_df.to_csv(\"submission.csv\", index=False)","038ac27e":"pd.read_csv(\"submission.csv\").head()","cf69934c":"##### Introduction  \n\nGetting data from a Kaggle's competition, let's compare the performance between classic and new  generation of gradient boosting decision trees (GBDTs).\n\nReference: https:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\n\nIn this competition proposed by **Santander Bank**, invites Kaggle users to predict which customers will make a specific transaction in the future, regardless of the amount of money made. The data provided for this contest has the same structure as the actual data they have available to solve the problem in the bank, which makes us address a real problem with a demanding dataset by number of records and characteristics, by which will test the performance of classic algorithms versus next-generation algorithms.\n\nThe data is anonymised, where each row contains 200 discrete variables and no categorical variables.\n\nNext we'll do a data exploration, readiness to apply the model, and analyze which algorithms get the best performance with low overfitting and compare the results between them.","c96b71f0":"### 6. Binary classification models","d4493e0b":"La mejor m\u00e9trica de validaci\u00f3n cruzada:\n 0.7835273824924537\n\n Los mejores par\u00e1metros:\n {'learning_rate': 0.255, 'min_child_samples': 33, 'n_estimators': 118, 'num_leaves': 20, 'random_state': 501}\nWall time: 15.8 s","97d5ad1d":"#### 6.2. Random forest  \nWe don\u00b4t test with the origin decision tree since being a dataset with discrete and non-categorical variables, it is difficult to achieve acceptable performance and stability, so it\u00b4s not good for unbalanced classification problems because the generated trees they will be very biased.\n\nWe go straight to testing with the Random forest which is a combination of trained decision trees each with a subset of the original data. This allows for more stable models.\n\nIn `scikit-learn` the constructor with which you can create a Random Forest model is `RandomForestClassifier`. This constructor requires more parameters than the decision tree because it is to be told the number of tree models to use, for which the parameter can be used `n_estimators`. On the other hand, as selecting the data to be used for each submodel it is a good idea to fix the seed to ensure that the results are repeatable.\n\nWith this in mind we can create a model for the resampled training and validation dataset.","abdca058":"### Content\n1. Libraries\n2. Data extraction\n3. Data exploration\n4. Unbalanced Data and Resampling\n5. Feature selection\n6. Binary classification models\n7. Hyperparameter tuning\n8. Detection of the most influential variables","d28fab12":"### 9. Submission  \nSubmit de Solution","c41c730b":"You get a model with average performance but as we can see when validating it, you can see a great overfitting of the model so it does not seem that the overall performance is adequate with respect to the reference of the logistic regression (AUC training: 0.78 and validation: 0.77).","61041010":"We get a **medium performance** of the model but with very **low overfitting** between training and validation.  \nWe will take this performance as a base reference to compare it with other models based on Decision Trees and their derivatives as gradient boosting.","d2ad0e49":"In this case we get better performance from both the original reference with the Linear Regression and the Random forest, with a moderate overfitting.","f742ec17":"You can see that the top 50 features have been selected. If we use the `SelectPercentile` constructor, you must be told the percentage of characteristics to be selected from the dataset. For example, you can test by selecting the best 50%, i.e. the top 100 features.","605a2616":"This is an anonymised dataset with 199 discrete numeric variables, with a dependent variable labeled as a binary variable and a column in string format with an identifier label. Two training datasets are provided, a training dataset and evaluation dataset, but no target variable so that for our purpose we won't use it to train the models. The task that is requested in this challenge is to predict the value of the target column in the test set.","ef35815d":"<img src=\"https:\/\/storage.googleapis.com\/kaggle-organizations\/141\/thumbnail.jpg?r=890\" alt=\"Kitten\" title=\"Santander Bank\"\nwidth=\"100\" height=\"100\" align=\"left\"\/>\n<img src=\"https:\/\/miro.medium.com\/max\/837\/1*Ab299OETAeuTEiGg5TwpMQ.png\" alt=\"Kitten\" title=\"Santander Bank\"\nwidth=\"150\" align=\"center\"\/>","73ac0910":"### 4. Unbalanced Data and Resampling  \nNote we are dealing with a data set **very unbalanced**, where there is only **10%** of records categorized with target 1, so those customers who have made a financial transaction.  \nTo develop a binary classification model we need to have more balanced data since most machine learning algorithms work best when the number of samples in each class is almost the same. This is because most algorithms are designed to maximize accuracy and reduce error, so we'll try to do this in this section before to predict models fit better.\n\nHow we have a large dataset with 200,000 records we could undersampling in the data with the balanced target variable. Initially we will test a resampling in a 1:1 ratio but depending on the results we can use other proportions. Keep in mind that with undersampling we might be removing information that may be valuable. This could lead to a lack of fit and poor generalization of the test set.","9509ad5b":"### 2. Data extraction  \nThe data is extracted from the competition opened in Kaggle by the Santander Bank and available for download in:  \nhttps:\/\/www.kaggle.com\/c\/santander-customer-transaction-prediction\/data","f71ffebe":"Despite what is expected, in this case we get worse performance than in the case of Catboost. Let's check if by adjusting the hyperparameters we reverse the results as you would expect.","88175e4e":"In order to automate the performance measures of the different models, we will factor a function to measure the metrics and be able to make comparisons between the different algorithms applied.","647a2554":"As you can see the most efficient model has turned out to be the **LightGBM** algorithm with an AUC of 0.97 in testing versus Catboost 0.91.","04f3d05c":"### 3. Data exploration","b4aecd0f":"#### 6.3. Gradient Boosting Decision Trees  \nGradient increase is one of the most powerful techniques for building predictive models. Perhaps the most popular implementation is XGBoost which employs a number of tricks that make it faster and more accurate than the traditional gradient increase (particularly the second order gradient descent).  \nHowever in this case as a fairly large dataset >10,000 records we chose to apply the two gradient-enhancing algorithms that have been made most popular lately because they are more efficient because of the lower memory usage in large data:\n* Catboost\n* LightGBM\n\n####     6.3.1 Catboost  \nThe great value of catboost is the optimized handling of categorical variables and in this case we only have discrete variables, we will use this algorithm to have a performance reference with which to compare to LightGBM. ","a19e9aaf":"Get an idea of thisdata distribution, we review in the training dataset that we will work with, we review the histogram of the mean values of each record based on the binary target variable.","4d511375":"### 7. Hyperparameters tuning \nAn optimal set of parameters can help achieve greater accuracy. Finding hyperparameters manually is tedious and computationally expensive. Therefore, the automation of hyperparameter tuning is important. RandomSearch, GridSearchCV and Bayesian optimization are generally used to optimize hyperparameters. In this case we will choose a mixed optimization in which we approximate the parameters manually and profile with the `GridSearchCV` method.\nFor a comparative idea of the main parameters that most influence the performance of these models, you can refer to the following table:\n![GBDTs comparation](https:\/\/miro.medium.com\/max\/3400\/1*A0b_ahXOrrijazzJengwYw.png)","d249faf8":"We could also use another undersampling strategy from specific libraries such as the `mbalanced-learn` Python module. Through this library we can group the records of the majority class and perform the undersampling by deleting records from each group or category, thus seeking to preserve the maximum information.","786dd364":"<img src=\"https:\/\/raw.githubusercontent.com\/rafjaa\/machine_learning_fecib\/master\/src\/static\/img\/resampling.png\" alt=\"Kitten\" title=\"Santander Bank\" align=\"left\"\/>","e6024abe":"   #### 7.1. Catboost","20edfcf1":"#### 6.1. Logistic Regression  \nThe **binary classification** of events can be performed from a logistic regression model where the expression is used:\n$$F(x) = \\frac{1}{1 + e^{\\sum-w_ix_i}}$$\n\nIn *scikit-learn* the constructor with which you can create a logistic regression model is `LogisticRegression`.  \nWe take this first model as a **reference** for its easy implementation and in which we can see how the other models behave.","c571b483":"### 1. Importar las librer\u00edas","cec87bf7":"# Santander Customer Transaction Prediction  \n### Comparison&Performance new generation of GBDT\u00b4s prediction model","1020d2ba":"We look for possible null values in the dataframe:","4dc3caf8":"Optimization gives us as the best parameters:  \n{'depth': **3**, 'iterations': **150**, 'l2_leaf_reg': **10**, 'learning_rate': **0.4**}","a7d921fe":"### 5. Feature selection  \n##### Selection of the best features  \nBefore creating a model, you can use the `SelectKBest` or `SelectPercentile` constructors to select objects that allow you to select the `k` `feature better`or a **percentage** of them respectively for creating a model. In both cases, the criterion to be used to sort them must be indicated. In *scikit-learn* there are two methods that can be used depending on the type of problem you are working with:\n\n* `f_regression` for regression models\n* `chi2` or `f_classif` for classification models in this case\n\nWe try to select the top 50 and 100 features to see if the models perform better and generalize better.","767a3704":"   #### 7.2. LightGBM","dccc9523":"### 8. Detection of the most influential variables","830c8f7e":"####     6.3.2 LightGBM\n. ","8cd9afed":"As you can note there is a small variation in the mean of all the features that could explain the target variable, which in any case is a little variation.  \nWe try to detect potential correlated variables to decrease high dimensionality. How the correlation matrix would be too large visually, we tried to numerically detect the existence of correlations above 0.5 and below -0.5."}}