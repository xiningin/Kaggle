{"cell_type":{"347997f9":"code","55796f2c":"code","fe44c900":"code","e03aa515":"code","80de290d":"code","70591b8f":"code","44bb4e0f":"code","262fe8cb":"code","c9d2e4ec":"code","612310ef":"code","a7a82673":"code","c86c4b90":"code","3f0ec503":"code","69c87344":"code","d9721e69":"code","8e316b37":"code","0bca0be2":"code","3ce07a17":"code","875f64ce":"code","79761fc7":"markdown","0b1305e0":"markdown","77dfbc9f":"markdown","678ef529":"markdown"},"source":{"347997f9":"import numpy as np\nimport pandas as pd\nfrom tensorflow import keras\nfrom tensorflow.keras.layers import Conv1D, Dense, Flatten\nfrom tensorflow.keras.models import Sequential\nfrom tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\nimport pandas_datareader as web\nimport datetime as dt\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.pyplot as plt\nplt.style.use('seaborn')\nplt.rcParams['figure.figsize'] = 20, 15","55796f2c":"ticker = 'bidi4.sa'\nstart = dt.date(2015,1,1)\ndf = web.get_data_yahoo(ticker, start = start)\ndf = df.Close","fe44c900":"df.describe()","e03aa515":"plt.plot(df)","80de290d":"df = df.values\ndf = df.reshape(len(df),1)\ndf[:5]","70591b8f":"scaler = MinMaxScaler()\nscaled_data = scaler.fit_transform(df)\nscaled_data[:5]","44bb4e0f":"# RNN processing\n\n# Since LSTMs store long term memory state, we create a data structure with 60 timesteps and 1 output\n# So for each element of training set, we have 60 previous training set elements \n\ndef rnn_process(scaled_data, window=60, test_size=.2):\n    # split\n    test_len = int(len(scaled_data) * test_size)\n    train = scaled_data[:-test_len]\n    test = scaled_data[-test_len-window:]\n    print(f'Train & Test lengths are : {len(train), len(test)}')\n\n    X_train = []\n    y_train = []\n    X_test = []\n    y_test = []\n    \n    # train build\n    for i in range(window,len(train)):\n        X_train.append(train[i-window:i,0])\n        y_train.append(train[i,0])\n    \n    #test build    \n    for i in range(window,len(test)):\n        X_test.append(test[i-window:i,0])\n        y_test.append(test[i,0])\n        \n    X_train, y_train, X_test, y_test = np.array(X_train), np.array(y_train), np.array(X_test), np.array(y_test)\n    \n    X_train = np.expand_dims(X_train, axis=-1)\n    X_test = np.expand_dims(X_test, axis=-1)\n    print(f'Shapes of the X_train, y_train, X_test, y_test are {X_train.shape, y_train.shape, X_test.shape, y_test.shape}')\n    return X_train, y_train, X_test, y_test","262fe8cb":"X_train, y_train, X_test, y_test = rnn_process(scaled_data)","c9d2e4ec":"model = keras.models.Sequential()\nmodel.add(keras.layers.InputLayer(input_shape=[60, 1]))\nfor rate in (1, 2, 4, 8) * 2:\n    model.add(keras.layers.Conv1D(filters=20, kernel_size=2, padding=\"causal\",\n                                  activation=\"relu\", dilation_rate=rate))\nmodel.add(keras.layers.Conv1D(filters=10, kernel_size=1))\nmodel.add(Flatten())\nmodel.add(Dense(1, activation='sigmoid'))\nmodel.compile(loss=\"mse\", optimizer=\"adam\")\nmodel.summary()\n","612310ef":"# Callbacks\ncheckpts = ModelCheckpoint('.\/conv1d.h5', verbose=1, save_best_only=True)\nearly = EarlyStopping(patience=5)\ncallback_list = [checkpts, early]","a7a82673":"history = model.fit(X_train, y_train, epochs=20, verbose=1, batch_size=32, validation_split=.2, callbacks=callback_list)","c86c4b90":"plt.plot(history.history['loss'], label='Original loss')\nplt.plot(history.history['val_loss'], label='Validation loss')\nplt.legend()","3f0ec503":"model.evaluate(X_test, y_test, batch_size=32)","69c87344":"tomorrow_pred = scaler.inverse_transform(model.predict(scaled_data[-60:].reshape(1,60,1)))\nprint(f'Prediction for tomorrow is {tomorrow_pred[0,0]}')","d9721e69":"# Last day value \nscaler.inverse_transform(scaled_data[-1].reshape(-1,1)), df[-1] ","8e316b37":"def forecast(model, data, future=10, window=60): # function works for the window size 60 only.data must be last 60 days values\n    predictions = []\n    for i in range(future):\n        #data = data\n        predictions.append(model.predict(data.reshape(1,window,1)))\n        data = np.concatenate((data, predictions[-1]), axis=0)[-60:]\n    return np.array(predictions).reshape(-1,1)","0bca0be2":"future = forecast(model, scaled_data[-60:])\nfuture = scaler.inverse_transform(future)","3ce07a17":"tomorrow = dt.date.today() + dt.timedelta(days=1)\nperiod = pd.date_range(tomorrow, periods=10, freq='B')\nforecast = pd.DataFrame(future, columns=['Forecast'])\nforecast = forecast.set_index(period)\nforecast","875f64ce":"plt.plot(web.get_data_yahoo(ticker, start = start).Close[-10:])\nplt.plot(forecast)","79761fc7":"# Forecast","0b1305e0":"Here we shall analyse stocks using Conv1d layer with window of 60 days and we shall do 10 days prediction. We shall follow the philosophy of WaveNet. Thanks to https:\/\/github.com\/ageron\/handson-ml2\/blob\/master\/15_processing_sequences_using_rnns_and_cnns.ipynb\n# Imports","77dfbc9f":"# Model","678ef529":"# Processing"}}