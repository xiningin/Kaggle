{"cell_type":{"92ac545c":"code","f0fafb01":"code","0115d052":"code","cd1fe9c6":"code","813f4e80":"code","c6f2b837":"code","5c4f4111":"code","e20aec6a":"code","cc03727d":"code","6fb9f9d8":"code","bf060528":"code","c39c0189":"code","7832229a":"code","65c11449":"code","b74dcb52":"code","3e24f36c":"code","8d928b3f":"code","541377b7":"code","a7be4b5e":"code","a8aea396":"code","04fb1db9":"code","691ba288":"code","84e5b34f":"code","b3304105":"code","ad51985c":"code","bbedb7f7":"code","1daf253b":"code","024839a1":"code","66683af0":"code","85bde92e":"code","96aa3061":"code","d3d95b24":"code","f6e4dc7a":"code","e1d517c2":"code","e07b3e5e":"code","70f09294":"code","284cb71e":"code","82bbca5e":"code","ccc19baa":"code","a47acc7c":"code","87e3a68a":"code","ff6b0677":"code","eabcd58d":"code","a1e79cfb":"markdown","519e7c5d":"markdown","a7e0a8be":"markdown","b270654c":"markdown","4e1bbbb4":"markdown","a3b97823":"markdown","3b0e4e8d":"markdown","eb5af335":"markdown","908330bf":"markdown","a36e27fb":"markdown","73d1a0af":"markdown","5671cdd2":"markdown","c8b8dc1e":"markdown","6398e8f9":"markdown","549d8ecf":"markdown","e7e5a248":"markdown","a3db971e":"markdown","066a81a7":"markdown","368acf3f":"markdown","746e1cf4":"markdown","819befa1":"markdown","0eab6890":"markdown","c6dbd755":"markdown","027c7340":"markdown","442f367d":"markdown","47b1030b":"markdown","56c0dbe4":"markdown","ecb672ae":"markdown","980cdbac":"markdown","305cbb50":"markdown","9c2319df":"markdown","f19ae8f5":"markdown","1ecc8b7a":"markdown","0afbf4d8":"markdown","f350cb65":"markdown"},"source":{"92ac545c":"# Import Libraries\nimport pandas as pd\nimport numpy as np\nfrom scipy import stats\nimport urllib\nimport os\nimport time\n\nimport matplotlib.pyplot as plt\n\nfrom sklearn.model_selection import StratifiedShuffleSplit\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\nfrom sklearn.impute import SimpleImputer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import OneHotEncoder\n\nfrom sklearn.linear_model import LinearRegression\nfrom sklearn.tree import DecisionTreeRegressor\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor\nfrom sklearn.svm import SVR\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.decomposition import PCA\n%matplotlib inline","f0fafb01":"# Fetch data from book's github repository and dump into disk, and load it into dataframe\ndata_url = 'https:\/\/raw.githubusercontent.com\/ageron\/handson-ml\/master\/datasets\/housing\/housing.csv'\ndefault_data_file_path = os.path.join('datasets','housing')\ndef fetch_housing_data(data_url=data_url,data_file_path=default_data_file_path):\n    os.makedirs(data_file_path,exist_ok=True)\n    csv_file_path = os.path.join(data_file_path,'housing.csv')\n    urllib.request.urlretrieve(data_url,csv_file_path)\n    df = pd.read_csv(csv_file_path)\n    return df","0115d052":"# Load Data And view the sample\nrun_dir = os.path.join(default_data_file_path,time.strftime('run_%Y_%m_%d-%H_%M_%S'))\ndf = fetch_housing_data(data_file_path=run_dir)\nprint(f'Shape of is {df.shape[0]} rows and {df.shape[1]} columns')\ndf.sample(5)","cd1fe9c6":"# Check the data count and data type for all the available columns\ndf.info()","813f4e80":"# Check  the distribution of categorical variable\ndf['ocean_proximity'].value_counts()","c6f2b837":"# Check the distribution of continues variables\ndf.drop(['ocean_proximity'],axis=1).describe()","5c4f4111":"# Plot the histogram of all the continues variables\nhist_plot = df.drop(['ocean_proximity'],axis=1).hist(bins=50,figsize=(25,21))\nplt.savefig(os.path.join(run_dir,'Initial_Histograms.jpg'))","e20aec6a":"# Bin the Median Income variable to so that we can split  data without creating sampling bias\ndf['median_income_cut'] = pd.cut(df['median_income'],bins=[0,1.5,3,4.5,6,np.inf],labels=[1,2,3,4,5])\nhist_plot = df['median_income_cut'].hist()\ndf['median_income_cut'].value_counts()\/len(df) * 100","cc03727d":"# Stratified Sampling is done, so that both train & test samples will have similar distribution of important variables\nsplit = StratifiedShuffleSplit(n_splits=10,test_size=0.2,random_state=42)\ntrain = test = 0\nfor train_loc,test_loc in split.split(df,df['median_income_cut']):\n    train = df.loc[train_loc]\n    test = df.loc[test_loc]","6fb9f9d8":"# Check if the distribution is similar to full dataset after spliting\ncut_check = pd.DataFrame([df['median_income_cut'].value_counts()\/len(df),train['median_income_cut'].value_counts()\/len(train),\n                          test['median_income_cut'].value_counts()\/len(test)]).T\ncut_check.columns = ['Overall','Training Set','Test Set']\nprint('As we can see, using Stratified, sampling we could make sure that the distribution is equal in both sets')\ncut_check.sort_index()","bf060528":"# Drop the binned column we created and dump the data from train & test set into files\ntrain = train.drop(['median_income_cut'],axis=1)\ntest = test.drop(['median_income_cut'],axis=1)\ntrain.to_csv(os.path.join(run_dir,'train.csv'),index=False)\ntest.to_csv(os.path.join(run_dir,'test.csv'),index=False)","c39c0189":"# Create a copy training data set for further analysis, this gives us freedom to play around with data set\ndf = train.copy()","7832229a":"# Plot the Latitude & Longitude to visualize the distribution of population and median house value\nsct_plot = df.plot(kind='scatter',x='longitude',y='latitude',alpha=0.4,s=df['population']\/100,\n                   label='popilation',c='median_house_value',cmap=plt.get_cmap('jet'),figsize=(12,8),colorbar=True)\nsct_plot = plt.legend()\nplt.savefig(os.path.join(run_dir,'GeoPlot_Expo.jpg'))","65c11449":"# Calculate the correlation of numerical variables with target variable 'median house price'\nscat_mat = df.drop(['ocean_proximity'],axis=1).corr()\nscat_mat['median_house_value'].sort_values(ascending=False)","b74dcb52":"# generate random data\nx,y = np.zeros((3,100)),np.zeros((3,100))\nx[0,:] = np.array([np.random.randint(0,100) for _ in range(100)])\ny[0,:] = 3.5 * x[0,:] + np.random.randn(100)*20\nx[1,:] = np.array([np.random.randint(0,100) for _ in range(100)])\ny[1,:] = -2 * x[1,:]**3  + np.random.randn(100)\nx[2,:] = np.array([np.random.randint(0,100) for _ in range(100)])\ny[2,:] = 2 * x[2,:]**3  + np.random.randn(100)","3e24f36c":"fig,ax = plt.subplots(figsize=(16,4),ncols=3,dpi=100)\nfor i in range(3):\n    coff = np.round(np.corrcoef(x[i,:],y[i,:])[0,1],5)\n    ax[i].scatter(x[i,:],y[i,:])\n    ax[i].title.set_text(f'Correlation value: {coff}')","8d928b3f":"# Plot scater plot matrix to visualize the correlation of variabes with each other\nscat_plot = pd.plotting.scatter_matrix(df[['median_house_value','median_income','total_rooms',\n                                           'housing_median_age']],figsize=(12,10))","541377b7":"# Build 3 new variables as per our intution and business understanding\ndf['rooms_per_household'] = df['total_rooms']\/df['households']\ndf['bedrooms_per_room'] = df['total_bedrooms']\/df['total_rooms']\ndf['population_per_household'] = df['population']\/df['households']","a7be4b5e":"# Again the check the correlation to evaluate the effect of new features built\nscat_mat = df.drop(['ocean_proximity'],axis=1).corr()\nscat_mat['median_house_value'].sort_values(ascending=False)","a8aea396":"# Build box plots of categorical feature to evalutae its effect on house values\nbox_plot = df.boxplot(column='median_house_value',by='ocean_proximity',figsize=(8,8))\nplt.savefig(os.path.join(run_dir,\"Ocean_Proximity_Box_Plot.jpg\"))","04fb1db9":"# Seprate training data and labels\nX_train = train.drop('median_house_value',axis=1)\ny_train = train['median_house_value']","691ba288":"# Building transformer to create new features on dataset\nclass CombineAttributes(BaseEstimator,TransformerMixin):\n    def __init__(self,ttl_room_idx=3,ttl_bed_idx=4,pop_idx=5,hh_idx=6):\n        self.ttl_room_idx = ttl_room_idx\n        self.ttl_bed_idx = ttl_bed_idx\n        self.pop_idx = pop_idx\n        self.hh_idx = hh_idx\n    def fit(self,X, y=None):\n        return self\n    def transform(self,X,y=None):\n        rooms_per_household = X[:,self.ttl_room_idx]\/X[:,self.hh_idx]\n        bedrooms_per_room = X[:,self.ttl_bed_idx]\/X[:,self.ttl_room_idx]\n        population_per_household = X[:,self.pop_idx]\/X[:,self.hh_idx]\n        return np.c_[X,rooms_per_household,bedrooms_per_room,population_per_household]","84e5b34f":"# Build the pipeline to perform all major transformations to fit on training data,\n#this will help us apply consistent transformations to test datasets and any new dataset we get for scoring.\npipe_num = Pipeline([('imputer',SimpleImputer(strategy='median')),('attr_com',CombineAttributes()),\n                     ('scaler',StandardScaler())])\nnum_attr = list(X_train.drop('ocean_proximity',axis=1).columns)\nfull_pipeline = ColumnTransformer([('num_pipe',pipe_num,num_attr),\n                                   ('encoder',OneHotEncoder(),['ocean_proximity'])])","b3304105":"# Transform training dataset using the pipeine\nX_train = full_pipeline.fit_transform(X_train)","ad51985c":"# Build a function to train any given model, and give results, this will save a lot of time\ndef single_model_trainer(model,X,y,validation_set=True):\n    if validation_set == True:\n        train_X,test_X,train_y,test_y = train_test_split(X,y,random_state=42)\n        model.fit(train_X,train_y)\n        y_pred = model.predict(train_X)\n        mse = mean_squared_error(train_y,y_pred)\n        train_rmse = np.sqrt(mse)\n        y_pred = model.predict(test_X)\n        mse = mean_squared_error(test_y,y_pred)\n        test_rmse = np.sqrt(mse)\n        return (model,train_rmse,test_rmse)\n    else:\n        model.fit(X,y)\n        y_pred = model.predict(X)\n        mse = mean_squared_error(y,y_pred)\n        rmse = np.sqrt(mse)\n        return (model,rmse)","bbedb7f7":"# Training Linear Regression Model\nmodel,train_rmse,test_rmse = single_model_trainer(LinearRegression(),X_train,y_train)\nprint(f'Training Error : {train_rmse}')\nprint(f'Validation Error : {test_rmse}')","1daf253b":"# Training Decision Tree Model\nmodel,train_rmse,test_rmse = single_model_trainer(DecisionTreeRegressor(),X_train,y_train)\nprint(f'Training Error : {train_rmse}')\nprint(f'Validation Error : {test_rmse}')","024839a1":"# Training Random Forest Model\nmodel,train_rmse,test_rmse = single_model_trainer(RandomForestRegressor(n_estimators=100),X_train,y_train)\nprint(f'Training Error : {train_rmse}')\nprint(f'Validation Error : {test_rmse}')","66683af0":"# Training Gradient Boosting Model\nmodel,train_rmse,test_rmse = single_model_trainer(GradientBoostingRegressor(n_estimators=100),X_train,y_train)\nprint(f'Training Error : {train_rmse}')\nprint(f'Validation Error : {test_rmse}')","85bde92e":"# training Support Vector Model\nmodel,train_rmse,test_rmse = single_model_trainer(SVR(gamma='scale'),X_train,y_train)\nprint(f'Training Error : {train_rmse}')\nprint(f'Validation Error : {test_rmse}')","96aa3061":"# Training Model using KFold Cross Validation approch\nmodels = {'LinearRegression':LinearRegression(),'DecisionTreeRegressor':DecisionTreeRegressor(),\n          'RandomForestRegressor':RandomForestRegressor(n_estimators=100),'GradientBoostingRegressor':\n          GradientBoostingRegressor(n_estimators=100),'SVR':SVR(gamma='scale')}\nresults = {}\nfor name,model in models.items():\n    cv = KFold(n_splits=10, shuffle=False, random_state=42)\n    scores = cross_val_score(model,X_train,y_train,cv=cv,scoring='neg_mean_squared_error')\n    print(f'Cross Validation Completed for {name}')\n    rmse_scores = np.sqrt(-scores)\n    results[name] = rmse_scores\nresults_df = pd.DataFrame.from_dict(results)","d3d95b24":"results_df","f6e4dc7a":"# Looking at the distribution of model results, we dont want a model which is very good at certain subset but bad at others.\nresults_df.describe()","e1d517c2":"# Train Random Forest Model using Grid Search to find best hyperparameters\nforest_reg = RandomForestRegressor()\nparam_grid = {'bootstrap':[False],'n_estimators':(100,150),'max_depth':(10,15,None),'min_samples_split':(2,5),\n          'max_features':(6,8,10,'auto'),'random_state':[42]}\ngrid_search_results = GridSearchCV(forest_reg,param_grid,scoring='neg_mean_squared_error',cv=5,return_train_score=True,\n                                   n_jobs=-1,verbose=1)","e07b3e5e":"# Run the Grid Search\ngrid_search_results.fit(X_train,y_train)","70f09294":"# Print Results of Grid Search\ncv_res = grid_search_results.cv_results_\nrows = []\n\nfor score,params in zip(cv_res['mean_test_score'],cv_res['params']):\n    rmse = np.sqrt(-score)\n    params['RMSE'] = rmse\n    rows.append(params)\npd.DataFrame.from_dict(rows).sort_values(by='RMSE').head(10)","284cb71e":"#  Select the best performing model to final training\nreg_forest = grid_search_results.best_estimator_\nprint('Here is our best performing model on cross validation')\nprint('-'*50)\nreg_forest","82bbca5e":"# Training Final model on full dataset\nmodel,rmse = single_model_trainer(reg_forest,X_train,y_train,validation_set=False)\nprint(f'Final Model Training Error : {rmse}')","ccc19baa":"# Transform Tests Data Set for final evaluation\nX_test = test.drop('median_house_value',axis=1)\ny_test = test['median_house_value']\nX_test = full_pipeline.transform(X_test)","a47acc7c":"# Evaluate the model on test set\ny_pred = reg_forest.predict(X_test)\nmse = mean_squared_error(y_test,y_pred)\nrmse = np.sqrt(mse)\nprint(f'Final Model Testing Error : {rmse}')","87e3a68a":"error = y_test-y_pred\nfig,ax = plt.subplots(figsize=(10,8))\nax = ax.hist(error,bins=100)\nplt.savefig(os.path.join(run_dir,'Error_Distribution.jpg'))\n# Get the 95% confidence interval of the testing set error\nconfidence = 0.95\nsquared_error = (y_pred - y_test)**2\nCI = np.sqrt(stats.t.interval(confidence,len(squared_error)-1,loc=squared_error.mean(),scale=stats.sem(squared_error)))\nprint(f'Testing Error 95% Confidence Interval is Between {CI[0]} - {CI[1]}')","ff6b0677":"plt.figure(figsize=(12,8))\nplot = plt.hist([test['median_house_value'],y_pred],bins=20,label=['True Value','Predicted Value'])\nplt.xlabel('House Price')\nplt.ylabel('Distribution')\nplot = plt.legend()\nplot = plt.title('True Value v\/s  Predicted Value Distribution Comparision',fontsize=18)\nplt.savefig(os.path.join(run_dir,'True_vs_Predicted_Distro.jpg'))","eabcd58d":"# Plot the Latitude & Longitude to visualize the distribution of population and median house value\nsct_plot = test.plot(kind='scatter',x='longitude',y='latitude',alpha=0.6,s=test['population']\/100,label='popilation',c=y_pred,cmap=plt.get_cmap('jet'),figsize=(12,8),colorbar=True)\nsct_plot = plt.legend()\nsct_plot = plt.title('Latitude | Longitude Distribution of House Price over population')\nplt.savefig(os.path.join(run_dir,'Predicted_Geo_Distro.jpg'))","a1e79cfb":"<h4>Hyperparameter tuning make easily be named as one of the most time consuming tasks in ML, as the number of models increase the number hyperparameters one needs to tune can easily get out of hand.\nA good rule of thumb is to start by running a grid search to find initial set of hyperparameters and then make minor changes as you go along.<\/h4>","519e7c5d":"<h3> As you can see,  different curves, similar correlation score<h3>","a7e0a8be":"<b>For this data set we know that median income is a major factor affecting the price of the houses, so we bin the variable and make it categorical, so we can make sure, the distribution in both training & validation sets are similar.<b>","b270654c":"<h4>Now that you have  selected your model hyperparameters you would want to train it on full dataset.<\/h4>","4e1bbbb4":"## Part 1 : Loading and Evaluating Data","a3b97823":"### Before we make the split, we need to make sure that the distribution of major features are balanced in both the datasets. This is where your business understanding will be useful","3b0e4e8d":"<p>As you can see above how usefull is our pipeline now, when transforming test set, the same will be true for new data set which you might need to score later.<\/p>\n<p>You can add more transformation, to even include training on the selected model on historical data, before applying the new dataset, thus keeping your model updated<\/p>","eb5af335":"### So throught this Notebook I wish to share what I have learned over a year in my job about presentation skills","908330bf":"## Part 2 : Exploratory Data Analysis & Feature Engineering","a36e27fb":"<h3>Therefore I invite you to take this notebook and implement it on your own, in your own style.<\/h3>","73d1a0af":"<h4> As we will be training multiple models over and over again, its use full to write a function for this task.<\/h4>","5671cdd2":"### Any important graphs we plot during the analysis, its a good practise to save them. As you will be working with a team, it makes sharing results over email or skype, so much more easier.","c8b8dc1e":"<h4>Lets Train a Few Models!!!<\/h4>","6398e8f9":"# California Housing Price Prediction\n#### Inspired from \"Hands-On Machine Learning With Scikit-Learn,  Keras & TensorFlow\"\n#### Notebook By : Akash Agnihotri","549d8ecf":"<h3>A very important part of work for any Data Analyst or ML Engineer is presentation of there work, to a not technical audience.<\/h3>\n<h4>You can do great quality work, but if you dont present it in a way that I conveys what you want it to, it will be as good as trash<\/h4>\n\n<h4>One very important metric that managment will look for, is how the model compares to existing solution. Cause you model might help have man hours, but if it is too unstable compared to existing solution, it will create more problems than it will solve<\/h4>\n\n<h4>Towards that, it is useful to build plots comparing the two.<\/h4>","e7e5a248":"<h3>All I ask in return is what my teachers, asked from me.<h3><h3><i>Pass it On<\/i><\/h3>","a3db971e":"## Part 3 : Model Selection & Evaluation","066a81a7":"### Looking at these results we can easily say that Random Forest has performed the best, and looks most promising of them all.\n#### Now in reality, you would be selecting atleast 2 or 3 models to optimize further, as you would not want to place your bet on just one horse.\n#### Also Stacking Up of multiple models is a common practise at industry level, but that is different story all together, you would want to use seperate subset of data to train completly different kind of models, to minimize the chance of all learning similar patterns.\n\n#### Lets keep that for another day","368acf3f":"<h4>Now usually we would go back make more minor changes to the hyperparameters, but as I have already made those here, lets just go ahead<\/h4>","746e1cf4":"##### Now loading data depends very much on what kind of database you are dealing with, very rarely you will have the comfort to use a simple csv file,there will either be existing APIs to fetch the data, or you might have to write your own.\n\n#### When you will be running the same code multiple times, it is preferable to create a new execution directory for each run, which contains all your data and output.","819befa1":"## Part 7 : Result Visualization and Conclusion","0eab6890":"<h4>K-Fold validation is very usefull in comparing models over different parts of training dataset. This is also use full  to check that models perform on same level at all training subset, assuring us that, any specific subset does not contain more entorpy then others<\/h4>","c6dbd755":"<h3>This was better than I expected, Wait!! what if mean score looks good, but the distribution of scores is very high!!, lets check that as well<\/h3>\n<h4>Getting a good result is not enough,one needs to make sure that these results hold true across the dataset, when millions or even thousands of dollars might be depended on your model<\/h4>","027c7340":"<h3> As important as it is to understand new algorithms and techniques, its equally important to understand the business the data belongs to, as it will help you exract the most out of dataset<\/h3>\n<p><b> Understanding business will give you an edge over others during job interview, I would strongy suggest selectiing a few domains you are interested in and learning about them. It was a major part of my preprations when preparing for a job.<\/b><p>","442f367d":"### We must not make any changes to dataset before a complete evaluation, to make sure we understand the big picture here. Real world data is never clean and ready for analysis. Most of the datasets we see in kaggle (including this one) are cleaned and well prepared for us to play with.","47b1030b":"<h4> Whatever feature engineering or data manipulation you finalize post exploration, its is important to pack them up in classes so you can apply them over batches of data. This will help us maintain consistent changes across training, validation, testing and if all works out, new data for scoring.","56c0dbe4":"<p><b> Model selection will often depend on a few external factors based on your business need. You will have find a right balance between interpreatbility vs accuracy.\n    \n    Depending on the use, it might be important to prioritize Interpreatbility, for example  Credit Score Models, therefore you might prefer \"Logistic Regression\" or \"Decision Tree\".\n    If you need some Interpretablity, \"Ranom Forest\" or \"Gradient Boosting\" might be a good choice. If all you care about is accuracy then one might prefer Nerural Networks.","ecb672ae":"<h3>Now, the moment of truth, we can now run the test set through the model to find its performance<\/h3>\n\n<h4>You must be carefull not to perform  this test, too many times, as it gives you an ideas on what works on the test set, and your decisions start getting biased towards acieveing better score on test set, which not the goal here<\/h4>\n<h4>Let me say that again!!<\/h4>\n<h3>Goal is not to get a high score on test set, Goal is to build an unbiased high performance model!!<\/h3>","980cdbac":"### Important Note:\n#### As we have identified the basic nature of our dataset, we must split this into training and validation sets, and save them, before we go any further. as more information you have about the dataset, more the chances that your modeling will be baised towards fitting the current dataset.\n#### Once you save the validation set, never look it again, until you have completed your modeling, its very very important.\n#### We humans are THE most advanced pattern recongnisation systems, and through us, those will flow into our model, making it biased towards our dataset.","305cbb50":"<h2><i> Tell me and I forget. Teach me and I remember. Involve me and I learn.\n                        \n                                                   - Benjamin Franklin","9c2319df":"<h3>These are just some steps I could cover given limited time, I will try to add more content to as I get more time<\/h3>","f19ae8f5":"<h4>Often datasets will have hundreds of features for you to work with, and initially it can seem very intimidating, but remember, <b>Rome was not built in a day<\/b> and neither will your model, its an iterative process. You work with business team to identify important features and explore them, and build a inital model, and then do it all over again, and again.<\/h4>","1ecc8b7a":"<h3>We must not trust the numbers blindly, cause number often lie, just like your ex.\n    We must make sure to plot a few graphs to get a better understanding of these correlation, below is an example of how you can easily lie with number.<\/h3>","0afbf4d8":"## Part 4 : Final Model Development & Hyperparameter Tuning","f350cb65":"The Idea through this Notebook is to impelement a end to end project using classical machine learning project, following Industry level standards. To build a notebook you showcase in front of your managment.\n\nNow before we begin, let me quickly tell you about myself, I promise i'll keep it short.\n\nI have been working in Machine Learning domain for over a year now, for a major bank.\nLike most of you, I have a self taught ML Engineer, I put all my effort in learning as much as I could about the domain, and found small victories here and there. When I begain my job I was as confident as on could be.\n\nBut as I spent more time doing things I always wanted to, I realized there is a major gap in my learning, and its not some technique or algorithm that i did not know, it was the very way of doing things.\n\nWhat we often forget about ML\/Stats work is that, its not new, the hype around it is new. Most of the banks\/funds etc have been using these techniques for decades, just the name has changed. And in all those years, they was developed a way of doing things, it is as true for Data Science as for any domain.Even at tech companies, its not enough to write great code, one must write clean and readable code."}}