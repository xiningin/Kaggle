{"cell_type":{"ff1b9423":"code","df9887a0":"code","c847f15b":"code","0cb9f628":"code","758b8370":"code","ea9c84da":"code","59554a76":"code","5d929a3c":"code","bf949c58":"code","7c20a53f":"code","80cf6e58":"markdown","ba141504":"markdown","58b7052d":"markdown","019df5da":"markdown","b4f54390":"markdown","c48529f1":"markdown","663f59fc":"markdown","50ce6cdf":"markdown"},"source":{"ff1b9423":"%load_ext tensorboard.notebook\n%matplotlib inline\n\nimport os\nimport shutil\nfrom dataclasses import dataclass\nfrom functools import partial\nimport pathlib\nimport time\nimport xml.etree.ElementTree as ET\nimport zipfile\n\nimport numpy as np\nimport pandas as pd\nimport skimage\nfrom PIL import Image\nimport cv2\n\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import DataLoader, Dataset\nimport albumentations as albu\nfrom tqdm import tqdm_notebook as tqdm","df9887a0":"@dataclass\nclass Example:\n    img: np.ndarray\n    category: str\n    difficult: int\n    transform: object = None\n\n    def read_img(self):\n        # read and crop\n        img = self.img\n\n        if self.transform is not None:\n            img = self.transform(image=img)['image']\n\n        # convert to ndarray and transpose HWC => CHW\n        img = np.array(img).transpose(2, 0, 1)\n        return img\n\n    def show(self):\n        img = (self.read_img() + 1) \/ 2\n        plt.imshow(img.transpose(1, 2, 0))\n        plt.title(self.category)\n\nclass DogDataset(Dataset):\n    def __init__(self,\n                 img_dir='..\/input\/all-dogs\/all-dogs\/',\n                 anno_dir='..\/input\/annotation\/Annotation\/',\n                 transform=None,\n                 examples=None):\n        self.img_dir = pathlib.Path(img_dir)\n        self.anno_dir = pathlib.Path(anno_dir)\n        self.transform = transform\n        self.preprocess = albu.Compose([\n            albu.SmallestMaxSize(64),\n        ])\n        \n        if examples is None:\n            self.examples = self._correct_examples()\n        else:\n            self.examples = examples\n\n        self.categories = sorted(set([e.category for e in self.examples]))\n        self.categ2id = dict(zip(self.categories, range(len(self.categories))))\n        \n    def _correct_examples(self):\n        examples = []\n        for anno in tqdm(list(self.anno_dir.glob('*\/*'))):\n            tree = ET.parse(anno)\n            root = tree.getroot()\n\n            img_path = self.img_dir \/ f'{root.find(\"filename\").text}.jpg'\n            if not img_path.exists():\n                continue\n\n            objects = root.findall('object')\n            for obj in objects:\n                examples.append(self._create_example(img_path, obj))\n        return examples\n\n    def _create_example(self, img_path, obj):\n        # reading bound box\n        bbox = obj.find('bndbox')\n        # read and preprocess image\n        img = skimage.io.imread(img_path)\n        xmin=int(bbox.find('xmin').text)\n        ymin=int(bbox.find('ymin').text)\n        xmax=int(bbox.find('xmax').text)\n        ymax=int(bbox.find('ymax').text)\n        img = img[ymin:ymax, xmin:xmax]\n        img = self.preprocess(image=img)['image']\n\n        # add example\n        return Example(\n            img=img,\n            category=obj.find('name').text,\n            difficult=int(obj.find('difficult').text),\n            transform=self.transform,\n        )\n    \n    def __len__(self):\n        return len(self.examples)\n    \n    def __getitem__(self, i):\n        e = self.examples[i]\n        img = e.read_img()\n\n        categ_id = self.categ2id[e.category]\n        return img, categ_id\n\n    def show_examples(self, indices=None, n_cols=8):\n        if indices is None:\n            indices = np.random.randint(0, len(self), n_cols)\n\n        n_rows = (len(indices)-1) \/\/ n_cols + 1\n\n        fig = plt.figure(figsize=(n_cols*4, n_rows*4))\n        for i, idx in enumerate(indices, 1):\n            fig.add_subplot(n_rows, n_cols, i)\n            self.examples[idx].show()\n        plt.show()","c847f15b":"%%time\n\ntransform = albu.Compose([\n    albu.CenterCrop(64, 64),\n    albu.HorizontalFlip(p=0.5),\n    albu.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5)),\n])\n\ndataset = DogDataset(\n    transform=transform,\n)\n\nprint(dataset)\ndataset.show_examples()","0cb9f628":"ls \/opt\/conda\/lib\/python3.6\/site-packages\/cv2\/data\/haarcascade_*.xml","758b8370":"def crop_with_facebox(img, pos):\n    pos = pos[0]\n    left, right = pos[0], pos[0]+pos[2]\n    top, bottom = pos[1], pos[1]+pos[3]\n    img = img[top:bottom, left:right]\n    return img\n\ndef create_cropped_dataset(dataset):\n    rescale = albu.SmallestMaxSize(64)\n\n    cascade_file_path = '\/opt\/conda\/lib\/python3.6\/site-packages\/cv2\/data\/haarcascade_{}.xml'\n    \n    # classifiers are prioritized\n    ordered_classifiers = [\n        (cv2.CascadeClassifier(cascade_file_path.format('frontalcatface_extended')), crop_with_facebox),\n        (cv2.CascadeClassifier(cascade_file_path.format('frontalcatface')), crop_with_facebox),\n        (cv2.CascadeClassifier(cascade_file_path.format('frontalface_default')), crop_with_facebox),\n        (cv2.CascadeClassifier(cascade_file_path.format('frontalface_alt')), crop_with_facebox),\n        (cv2.CascadeClassifier(cascade_file_path.format('frontalface_alt2')), crop_with_facebox),\n    ]\n\n    cropped_examples = []\n    for i, e in enumerate(tqdm(dataset.examples)):\n        grayimg = cv2.cvtColor(e.img, cv2.COLOR_RGB2GRAY)\n        for clf, crop_fn in ordered_classifiers:\n            pos = clf.detectMultiScale(grayimg)\n            if len(pos) != 0:\n                break\n\n        if len(pos) == 0:\n            continue\n\n        img = crop_fn(e.img, pos)\n        if img is None:\n            continue\n\n        img = rescale(image=img)['image']\n        cropped_examples.append(Example(\n            img=img, category=e.category,\n            difficult=e.difficult,\n            transform=e.transform,\n        ))\n\n    return DogDataset(examples=cropped_examples)","ea9c84da":"%%time\ncropped_dataset = create_cropped_dataset(dataset)\nprint(len(cropped_dataset))\ncropped_dataset.show_examples()","59554a76":"cropped_dataset.show_examples(range(len(cropped_dataset)))","5d929a3c":"# collect images by the categories.\ncateg_imgs = {}\nfor x, c in cropped_dataset:\n    categ_imgs[c] = categ_imgs.get(c, []) + [x]\n\n# select categories which are successfully cropped more than 10 images\ncateg_imgs = dict(sorted(filter(lambda item: len(item[1])>=10, categ_imgs.items())))\nlen(categ_imgs)","bf949c58":"from sklearn.decomposition import PCA\nfrom sklearn.decomposition import KernelPCA\n\ndef generate_images_by_PCA(real_imgs, n):\n    x = np.array(real_imgs)\n    x = x.transpose(0, 2, 3, 1).reshape(len(x), -1)\n\n    n_components = len(x)-1\n    pca = KernelPCA(n_components, kernel='rbf', fit_inverse_transform=True)\n    pca.fit(x)\n\n    z = np.random.randn(n, n_components) * np.sqrt(pca.lambdas_[None, :])\n    imgs = pca.inverse_transform(z).reshape(n, 64, 64, 3)\n    imgs -= imgs.min() * 0.8\n    imgs = imgs.clip(0, 1)\n    return imgs\n\nn = 10\nfor c in categ_imgs:\n    imgs = generate_images_by_PCA(categ_imgs[c], n)\n\n    fig = plt.figure(figsize=(4*n, 8))\n    for i, img in enumerate(imgs, 1):\n        fig.add_subplot(2, n, i)\n        plt.imshow(img)\n        plt.title(cropped_dataset.categories[c])\n        fig.add_subplot(2, n, n+i)\n        plt.hist(img.flatten())\n        \n    plt.show()","7c20a53f":"zipname = f'images.zip'\ncateg_ids = list(categ_imgs.keys())\nwith zipfile.PyZipFile(zipname, mode='w') as z:\n    for i in tqdm(range(1000)):\n        real_imgs = categ_imgs[categ_ids[i%len(categ_ids)]]\n        imgs = generate_images_by_PCA(real_imgs, 10)\n        imgs = (imgs*255).astype('uint8')\n        for j, img in enumerate(imgs):\n            fname = f'{i:03d}{j:01d}.png'\n            Image.fromarray(img).save(fname, 'PNG')\n            z.write(fname)\n            os.remove(fname)","80cf6e58":"# Cropping Dog Faces\n## Overview\nIn this competition, cropping only dogs may be important.  \nThis kernel attempts to crop only front faces of dogs just like CelebA.  \n\n## Score\nJust cropped images results **81.2265** in pubilc LB  \nLinear combinations of the principal components bt PCA results **121.16142** in public LB  \nLinear combinations of the principal components by Kernel PCA results **159.69548** in public LB but it looks better than PCA's :)\n\n## Method\nI use Cascade Classifier and Haarlike features in OpenCV.  \nNote that this data cleaning method is in gray zone because these haarlike features are just pretrained weights even if this kernel doesn't use any data outside of kernel.\n\n![image.png](attachment:image.png)","ba141504":"# Generate dogs by using principal components\nSince the cropped dog images are well aligned, we can simply generate dog images as linear combination of principal components.","58b7052d":"Save images","019df5da":"# Original dataset","b4f54390":"# Libraries","c48529f1":"# Crop dog faces\nOpenCV doesn't include any haarlike features for detecting dog faces.  \nSo, I alternate it by that for cat faces and human faces.","663f59fc":"generated image examples","50ce6cdf":"# Haarlike features"}}