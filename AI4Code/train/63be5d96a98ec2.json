{"cell_type":{"82483ca2":"code","408ae2e4":"code","2d5e6c85":"code","8b1c718e":"code","20a00224":"code","4dcc39f8":"code","229ae5a2":"code","7207761b":"code","f4ec41dd":"code","14ed6f6d":"code","5ec738c5":"code","e6d803e9":"code","41190cba":"code","b71491b3":"code","ca01fe1a":"code","1d46cc00":"code","acfa4ed4":"code","6a887c8a":"code","43f744da":"code","8bd39d58":"code","8beee847":"code","5ea77be8":"code","f546c383":"code","1e8fcd50":"code","294295a5":"code","1e64b3d0":"code","3febf22a":"code","a6b1de58":"code","4a6e9544":"code","c95bf748":"code","38d881f4":"code","426c354c":"code","c1f30e10":"code","363d7d8a":"code","5c7b8915":"code","d706a5ba":"code","f8623ab4":"code","673790ea":"code","4e417068":"code","47b0f5ae":"code","c85d5a26":"code","20b6ebce":"code","72ac3782":"code","8737f863":"code","32bd9392":"code","b52ceb9a":"code","148cea24":"code","8d24c4e9":"code","b1197ece":"code","2d3eb937":"code","bd6bfeef":"code","349945e8":"code","fceb4beb":"code","cd6b2310":"code","3b393dfd":"code","8c01895e":"code","c952dd4e":"code","4e72baa5":"code","15760ee7":"code","d5c7ae72":"code","631deb09":"code","1ca97d00":"code","7397e4fd":"code","8c35f8bc":"code","58f923d0":"code","9d83fb49":"code","99b07203":"code","b226f94a":"code","f37b6565":"code","c8439041":"code","cefb8fce":"code","28fb610a":"code","3f5f548b":"code","c3a5a9d3":"code","5e9e056d":"code","90b6ceca":"code","1dca8f8a":"code","5f117a99":"code","6da464d1":"code","900a6992":"markdown","7ac370d3":"markdown","63538217":"markdown","0450dd04":"markdown","4793a502":"markdown","fa895c30":"markdown","c7f04db4":"markdown","165c5a28":"markdown","7bb85a3c":"markdown","b11f86c5":"markdown","f0dc9233":"markdown","3d1c26bb":"markdown","d912d663":"markdown","018ce282":"markdown","81386fd8":"markdown","5e7683af":"markdown","2f99eb05":"markdown","247f7d53":"markdown","cebbf27c":"markdown","07bfbc04":"markdown","717979ad":"markdown","9247af7d":"markdown","93873122":"markdown","d6faed21":"markdown","06a683bc":"markdown","a2a98608":"markdown","3ed7f73f":"markdown","ed2ff1ac":"markdown","5c21144a":"markdown","361fee96":"markdown","fc14daf1":"markdown","652ff9b1":"markdown","e073f6dc":"markdown","6d7b2321":"markdown","a18d85bb":"markdown","04a58cc8":"markdown","7beee0be":"markdown","19e16bff":"markdown","f53148d9":"markdown","f74394c2":"markdown","fd80156e":"markdown","f2dbf43b":"markdown","04c82594":"markdown","498f60fa":"markdown","81c71e82":"markdown","db75899f":"markdown","0efb21e9":"markdown","82b4d0cf":"markdown","712044da":"markdown","811d8037":"markdown","c505acdd":"markdown","86b5387e":"markdown","8423456d":"markdown","f77c3a49":"markdown","a8be3408":"markdown","039e7cfc":"markdown","f0b9b08a":"markdown","e9e51611":"markdown","47311f69":"markdown","d1970ff1":"markdown","4d7a345d":"markdown","4264870b":"markdown","48331aa6":"markdown","0bddc72c":"markdown","4227b9c9":"markdown","e24382a9":"markdown"},"source":{"82483ca2":"import numpy as np \nimport pandas as pd \nimport plotly.express as px\nfrom sklearn.cluster import KMeans\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import StandardScaler\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import silhouette_score\nfrom itertools import product\nimport seaborn as sns\nfrom sklearn.cluster import KMeans\nfrom sklearn.cluster import AgglomerativeClustering\nimport itertools\nfrom itertools import product\nfrom matplotlib.pyplot import figure\nfrom sklearn.preprocessing import MinMaxScaler\nimport matplotlib.patches as mpatches\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.decomposition import KernelPCA\nfrom sklearn.metrics import mean_squared_error\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.cluster import AffinityPropagation","408ae2e4":"pd.set_option('display.max_columns', None)\ndf = pd.read_csv('..\/input\/customer-personality-analysis\/marketing_campaign.csv', sep = '\\t')\ndf.head()","2d5e6c85":"print('The shape of dataset is', df.shape)","8b1c718e":"print(\"Values for Z_Revenue column\", df.Z_Revenue.unique().tolist())\nprint(\"Values for Z_CostContact column\",df.Z_CostContact.unique().tolist())","20a00224":"df = df.drop(labels=['Z_Revenue','Z_CostContact', 'ID'], axis='columns')","4dcc39f8":"print('Number of duplicates', df.duplicated().sum())","229ae5a2":"df.drop_duplicates(inplace = True)","7207761b":"df.isna().sum()","f4ec41dd":"df.dropna(inplace = True)","14ed6f6d":"fig = px.histogram(df, x=\"Year_Birth\")\nfig.show()","5ec738c5":"df.drop(df[df['Year_Birth'] < 1940].index, inplace=True)","e6d803e9":"fig = px.box(df, x=\"Income\")\nfig.show()","41190cba":"df.drop(df[df['Income']  > 153000].index, inplace=True)","b71491b3":"def categories(d):\n    df_wine2 = pd.DataFrame((d.loc[:,('MntWines', 'MntFruits', 'MntMeatProducts', 'MntFishProducts', 'MntSweetProducts', 'MntGoldProds')]).melt())\n    fig = px.pie(df_wine2, values='value', names='variable',  width=800, height=400)\n    fig.show()","ca01fe1a":"categories(df)","1d46cc00":"def channel(d):\n    df_ch = pd.DataFrame((d.loc[:,('NumWebPurchases', 'NumCatalogPurchases', 'NumStorePurchases')]).melt())\n    fig = px.pie(df_ch, values='value', names='variable', width=800, height=400)\n    fig.show()\nchannel(df)","acfa4ed4":"df['MntTotal'] = df['MntWines'] + df['MntFruits'] + df['MntMeatProducts'] + df['MntFishProducts'] + df['MntSweetProducts'] + df['MntGoldProds']\ndf['NumTotalPurchases'] = df['NumDealsPurchases'] + df['NumWebPurchases'] + df['NumCatalogPurchases'] + df['NumStorePurchases']\ndf['AverageCheck'] = df['MntTotal']\/df['NumTotalPurchases']","6a887c8a":"fig = px.histogram(df, x=\"AverageCheck\", width=800, height=400)\nfig.show()","43f744da":"fig = px.box(df, x=\"AverageCheck\",  width=900, height=500)\nfig.show()","8bd39d58":"df.drop(df[df['AverageCheck']  > 98.83].index, inplace=True)\ndf.shape","8beee847":"wine = df.groupby('Education')['MntWines'].mean().reset_index()\nfruits = df.groupby('Education')['MntFruits'].mean().reset_index()\nmeat = df.groupby('Education')['MntMeatProducts'].mean().reset_index()\nfish = df.groupby('Education')['MntFishProducts'].mean().reset_index()\nsweet = df.groupby('Education')['MntSweetProducts'].mean().reset_index()\ngold = df.groupby('Education')['MntGoldProds'].mean().reset_index()   ","5ea77be8":"fig, ax = plt.subplots()\nfig.set_figheight(10)\nfig.set_figwidth(10)\nax.bar(wine['Education'], wine['MntWines'], label='MntWines')\nax.bar(meat['Education'], meat['MntMeatProducts'],bottom=wine['MntWines'],label='MntMeatProducts')\nax.bar(fruits['Education'], fruits['MntFruits'],bottom=meat['MntMeatProducts'] + wine['MntWines'],label='MntFruits')\nax.bar(fish['Education'], fish['MntFishProducts'],bottom=meat['MntMeatProducts'] + wine['MntWines'] + fruits['MntFruits'],label='MntFishProducts')\nax.bar(sweet['Education'], sweet['MntSweetProducts'],bottom=meat['MntMeatProducts'] + wine['MntWines'] + fruits['MntFruits']+ fish['MntFishProducts'],label='MntSweetProducts')\nax.bar(gold['Education'], gold['MntGoldProds'],bottom=meat['MntMeatProducts'] + wine['MntWines'] + fruits['MntFruits']+ fish['MntFishProducts'] + sweet['MntSweetProducts'],label='MntGoldProds')\n\n\nax.legend()\n\nplt.show()","f546c383":"def boxplots_moneyspent_feature(d, x,y):\n    fig = px.box(d, x=x, y=y, points=\"all\")\n    fig.show()\n    \nboxplots_moneyspent_feature(df,'Marital_Status', 'MntTotal')","1e8fcd50":"boxplots_moneyspent_feature(df,'Marital_Status', 'AverageCheck')","294295a5":"plt.figure(figsize=(10,7))\nsns.regplot(x=df[\"NumWebPurchases\"], y=df[\"NumWebVisitsMonth\"])\nplt.title('NumWebPurchases vs. NumWebVisitsMonth');","1e64b3d0":"f = plt.figure(figsize=(19, 15))\nmatrix = df.corr()\nsns.heatmap(matrix, annot=True)\nplt.title('Correlation Matrix', fontsize=16);","3febf22a":"from scipy.stats.stats import pearsonr\nprint('p-values for correlation with kids')\nprint('p-value for correlation between Kidhome and MntTotal', pearsonr(df.Kidhome, df.MntTotal)[1:])\nprint('p-value for correlation between Kidhome and NumTotalPurchases', pearsonr(df.Kidhome, df.NumTotalPurchases)[1:])\nprint()\nprint('p-values for correlation with income')\nprint('p-value for correlation between Income and MntTotal', pearsonr(df.Income, df.MntTotal)[1:])\nprint('p-value for correlation between Income and NumTotalPurchases', pearsonr(df.Income, df.NumTotalPurchases)[1:])\n","a6b1de58":"matrix = df.corr().abs()\ncol_corr = set() \nfor i in range(len(matrix.columns)):\n    for j in range(i):\n        if (matrix.iloc[i, j] >= 0.9) and (matrix.columns[j] not in col_corr):\n            colname = matrix.columns[i] \n            col_corr.add(colname)\nprint(col_corr)","4a6e9544":"df.drop(['AverageCheck'], axis=1, inplace = True)","c95bf748":"df['Dt_Customer'] = pd.to_datetime(df['Dt_Customer'], errors='coerce')\ndf['year_Customer']= df['Dt_Customer'].dt.year","38d881f4":"df.drop(['Dt_Customer', 'MntTotal', 'NumTotalPurchases'], axis=1, inplace = True)","426c354c":"skew_columns = df.select_dtypes(['number']).skew().sort_values(ascending=False)\nskew_columns = skew_columns.loc[skew_columns > 0.75]\nskew_columns","c1f30e10":"df_fin = df\nfor col in skew_columns.index.tolist():\n    df_fin[col] = np.log1p(df_fin[col])","363d7d8a":"df_fin.Marital_Status.unique()","5c7b8915":"def substitute_unique(a):\n    if a == 'Alone':\n        a = 'Single'\n    return a\n\ndf_fin['Marital_Status'] = df_fin['Marital_Status'].apply(substitute_unique)","d706a5ba":"df_fin.drop(df_fin[df_fin['Marital_Status']  == 'YOLO'].index, inplace=True)\ndf_fin.drop(df_fin[df_fin['Marital_Status']  == 'Absurd'].index, inplace=True)","f8623ab4":"df_fin.Education.unique()","673790ea":"def education(c):\n    if c == 'Basic':\n        return 0\n    if c == 'Graduation':\n        return 1\n    if c == 'Master':\n        return 2\n    if c == 'PhD' or c == '2n Cycle':\n        return 3\n    \ndf_fin['Education_tr'] = df_fin['Education'].apply(education)\ndf_fin.drop('Education',axis = 1, inplace = True)","4e417068":"df_Marital_Status = pd.get_dummies(df_fin.Marital_Status, prefix='Marital_Status')","47b0f5ae":"df_fin = df_fin.join(df_Marital_Status)","c85d5a26":"df_fin.drop('Marital_Status',axis = 1, inplace = True)","20b6ebce":"scaler = MinMaxScaler()\n\nscaled_df = scaler.fit_transform(df_fin)","72ac3782":"df_to_pca = pd.DataFrame(data = scaled_df, columns = df_fin.columns)","8737f863":"df_to_pca.shape","32bd9392":"from sklearn.feature_selection import VarianceThreshold\n\nsel = VarianceThreshold(threshold=0.03)\nsel.fit(df_to_pca)\nconstArr=sel.get_support()","b52ceb9a":"import collections\ncollections.Counter(constArr)","148cea24":"#Print out features that will be dropped\nconstCol=[col for col in df_to_pca.columns if col not in df_to_pca.columns[constArr]]\nconstCol","8d24c4e9":"df_to_pca.drop(['NumDealsPurchases','NumWebPurchases','NumWebVisitsMonth','AcceptedCmp2','Complain'], axis=1, inplace = True)","b1197ece":"df_to_pca.shape","2d3eb937":"rbf_pca = KernelPCA(n_components = 2, kernel=\"rbf\", gamma=0.0433, fit_inverse_transform=True) #make sure fit_inverse_transform is true\nreduced = rbf_pca.fit_transform(df_to_pca)\ninverse_transformed = rbf_pca.inverse_transform(reduced)\n\nfrom sklearn.metrics import mean_squared_error\nprint('mean_squared_error of original dataset and inverse transformed dataset reduced by kernel PCA {:.2e}'.format(mean_squared_error(df_to_pca, inverse_transformed)))\n","bd6bfeef":"s = []\nmax_s = 0\nmax_s_n_clusters = None\naffinity = ['euclidean', 'l1','l2', 'cosine','manhattan']\nlinkage = ['complete','average','single']\nbest_aff = None\nbest_l = None\nfor aff in affinity:\n    for l in linkage:\n        for i in np.arange(2,8):\n            hierarchical_cl = AgglomerativeClustering(n_clusters=i, affinity= aff, linkage = l)\n            ypred = hierarchical_cl.fit_predict(reduced)\n            sil = silhouette_score(reduced, ypred)\n            if sil > max_s:\n                max_s = sil\n                max_s_n_clusters = np.unique(ypred)\n                best_aff = aff\n                best_l = l\n\nprint('Maximal silhoutte {:.3f}'.format(max_s))\nprint('Optimal number of clusters', len(max_s_n_clusters))\nprint('Optimal affinity', best_aff)\nprint('Optimal linkage', best_l)","349945e8":"s = []\nmax_s = 0\nmax_s_n_clusters = None\nfor k in range(2,9):\n    knn = KMeans(k, random_state = 12345)\n    y_pred = knn.fit_predict(reduced)\n    sil = silhouette_score(reduced, y_pred)\n    s.append(sil)\n    if sil > max_s:\n        max_s = sil\n        max_s_n_clusters = k\n\nprint('Maximal silhoutte {:.3f}'.format(max_s))\nprint('Optimal number of clusters', max_s_n_clusters)","fceb4beb":"plt.plot(np.arange(2, 9),s);","cd6b2310":"eps = np.arange(0.01,0.1,0.01)\nmetric = [\"cosine\", \"euclidean\", \"cityblock\"]\nbest_e = 0\nbest_m = None\nmax_s =0 \nmax_s_n_clusters = 0\nfor e in eps:\n    for m in metric:\n        dbscan = DBSCAN(eps = e, metric = m)\n        ypred = dbscan.fit_predict(reduced)\n        sil = silhouette_score(reduced, ypred)\n        if sil > max_s:\n            max_s = sil\n            best_m = m\n            best_e = e\n            max_s_n_clusters = len(ypred)\n\nprint('Maximal silhoutte {:.4f}'.format(max_s))\nprint('Optimal metric', best_m)\nprint('Optimal epsilon',best_e)\nprint('Optimal number of clusters', len(np.unique(ypred)))","3b393dfd":"hierarchical_cl = AgglomerativeClustering(n_clusters=4, affinity= 'cosine', linkage = 'complete')\nypred = hierarchical_cl.fit_predict(reduced)\ndf['Cluster'] = ypred","8c01895e":"df.head()","c952dd4e":"df['MntTotal'] = df['MntWines'] + df['MntFruits'] + df['MntMeatProducts'] + df['MntFishProducts'] + df['MntSweetProducts'] + df['MntGoldProds']\ndf['NumTotalPurchases'] = df['NumDealsPurchases'] + df['NumWebPurchases'] + df['NumCatalogPurchases'] + df['NumStorePurchases']\ndf['AverageCheck'] = df['MntTotal']\/df['NumTotalPurchases']","4e72baa5":"cluster_0 = df[df['Cluster'] == 0]\ncluster_1 = df[df['Cluster'] == 1]\ncluster_2 = df[df['Cluster'] == 2]\ncluster_3 = df[df['Cluster'] == 3]","15760ee7":"def boxplot_for_clusters(col):\n    fig = px.box(df, x=\"Cluster\", y=col, width=900, height=500)\n    fig.show()","d5c7ae72":"boxplot_for_clusters('MntTotal')","631deb09":"boxplot_for_clusters('AverageCheck')","1ca97d00":"boxplot_for_clusters('MntWines')","7397e4fd":"boxplot_for_clusters('MntMeatProducts')","8c35f8bc":"boxplot_for_clusters('Income')","58f923d0":"boxplot_for_clusters('Year_Birth')","9d83fb49":"def kids_at_home(cluster, kid):\n    no_kids = cluster[cluster[kid] == 0][kid].count()\n    one_kid = cluster[cluster[kid] == 1][kid].count()\n    two_kids = cluster[cluster[kid] == 2][kid].count()\n    kids = pd.DataFrame(data = [no_kids,one_kid,two_kids])\n    ax = kids.plot.bar()\n    plt.ylabel('Customers')\n    plt.xlabel(kid)\n    #plt.title(cluster);\n    ","99b07203":"clusters = [cluster_0,cluster_1,cluster_2,cluster_3]\nfor c in clusters:\n    kids_at_home(c,'Kidhome')","b226f94a":"clusters = [cluster_0,cluster_1,cluster_2,cluster_3]\nfor c in clusters:\n    kids_at_home(c,'Teenhome')","f37b6565":"for c in clusters:\n    categories(c)","c8439041":"for c in clusters:\n    channel(c)","cefb8fce":"boxplots_moneyspent_feature(cluster_0,'Marital_Status', 'MntTotal')","28fb610a":"boxplots_moneyspent_feature(cluster_1,'Marital_Status', 'MntTotal')","3f5f548b":"boxplots_moneyspent_feature(cluster_2,'Marital_Status', 'MntTotal')","c3a5a9d3":"boxplots_moneyspent_feature(cluster_3,'Marital_Status', 'MntTotal')","5e9e056d":"df.head()","90b6ceca":"boxplots_moneyspent_feature(cluster_0,'Education_tr', 'MntTotal')","1dca8f8a":"boxplots_moneyspent_feature(cluster_1,'Education_tr', 'MntTotal')","5f117a99":"boxplots_moneyspent_feature(cluster_2,'Education_tr', 'MntTotal')","6da464d1":"boxplots_moneyspent_feature(cluster_3,'Education_tr', 'MntTotal')","900a6992":"### Customers' age analysis\n\nLet's look how the age of the customers is distributed.","7ac370d3":"The medians of average checks are similar for all groups. Regarding Widow group spend more,we can suggest that these customers tend to buy more items.","63538217":"# Exploratory data analysis (EDA)","0450dd04":"How much does every customer spends in average for each buy? What's the distribution of the customers' money spent on each purchase?","4793a502":"We can see two customers that were born in 1893 and in 1900. It's hard to imagine that these customers are around 129 and 122 years old. Probably, these two features are outliers, so we get rid of them.","fa895c30":"Drop rows with 'YOLO' and 'Absurd' ","c7f04db4":"Customers of clusters 2 and 3 have similar distributions of income which are higher than ones of cluster 0 and 1. ","165c5a28":" Let's see how many kids customers from each cluster have.","7bb85a3c":"kNN and Agglomerative clustering had the same silhoutte score, however Agglomerative clustering is both more flexible and has fewer hidden assumptions about the distribution of the underlying data. Therefore, Agglomerative clustering will be used to make predictions.","b11f86c5":"Let's drop duplicates","f0dc9233":"As the Z_Revenue and Z_CostContact columns contain identical numbers for all features, we can drop them. We also don't need to have an id column for clustering, so we will use dataframe for clustering without this column.","3d1c26bb":"## Customer expenses by categories\nLet's see on which category cutomers spend the most","d912d663":"# Customer personality analysis ","018ce282":"Cluster 2 and cluster 3 have higher medians of money spent on wines than cluster 0 and cluster 1. \n\nMeat is the second business source of the shop, so let's see the distribution of money spent on meat for each cluster.","81386fd8":"## Sales and average check","5e7683af":"People from clusters 2 and 3 spend more than ones from clusters 0 and 1. Let's see the distribution of average check.","2f99eb05":"### k-Nearest neighbors","247f7d53":"# Feature engineering and feature selection\n\n## Feature engineering\n\nColumn Dt_Customer contains 'object' data. Let's transform it to the datetime format and parse the year from it. ","cebbf27c":"All results are statistically significant (p < 0.05). Therefore, we can conclude that if people with kids tend to buy less and customers with higher income spent more money in the shop.\n\nNext, check if we can drop any highly correlated columns (Pearson correlation index > 0.95)","07bfbc04":"# Conclusions - EDA\nDuring EDA some preprocessing steps were done:\n* Z_Revenue and Z_CostContact columns contained identical numbers for all features, they were dropped.\n* 182 duplicates were found and dropped\n* Income column contained 24 missing values. As missing values makes up to 1% of the data, it's possible to drop such rows.\n* While exploring age distribution customers born born in 1893 and in 1900 were deleted (as this is not realistic)\n* Customers with salary higher than 153k were deleted as boxplot showed these values as outliers.\n* Column Average check was dropped after EDA\n\n## Analytical part\nDuring EDA several findings were uncovered\n### Product categories: \nThe customers spend the most money (50%) on a wine category. The second category that brings the most to the shop is meat.\n### Channels: \nCustomers buy the most frequently in stores (46%) and the least frequently by catalogs (21%).\n### Average check: \n50% of the customers spend more than 23.58 on each purchase. Purchases for more than 99.2 appeared to be outliers, so such observations were dropped.\n### Education: \nPeople with basic education spent much less at the store, so product managers should focus their activity primarily on the people with high education. Intriguingly, PhD candidates spend the most for wine which is the main business source.\n### Marital status: \nDifferent marital status groups have similar median of total money spent except widow group that is higher than others (so, widows spend more money in general). The median of average check is higher for widow group meaning that these customers tend to buy more expensive products.\n\n### Hypohteses\nI hypothesized that the more customers visit the shop's website, the more they buy via internet. However, there is no correlation between web purchases and web visits. Also, the data is doubtful: customers that didn't visit the shop's website made web purchases and some people who visited the website quite often (more 12.5 times) didn\u2019t buy anything (or buy very little). I would recommend to check the way this data is collected.\n\nI also tested the following hypotheses about customers with kids\/teens and link between customers' income and sales. Here are the results:\n\n* There are no correlation between people with teens and total money spent\/total purchases. However, there is a negative correlation for people with kids and total money spent\/total purchases, meaning that they tend to buy less or something less expensive. \n* There is a positive correlation between income and total money spent\/total purchases meaning customers with higher income buy more and more expensive items. \n","717979ad":"Different marital status groups have similar median except widow group that is higher than others. We don't take into account 'Alone' marital status group as there are few people in it.Also, we will skip  'Absurd' and 'YOLO' groups as they are anecdotal.","9247af7d":"Examine skew of the values ","93873122":"Customers buy the most frequently in stores (46%) and the least frequently by catalogs (21%).","d6faed21":"Members of cluster 1 and 3 are only married people whereas there people have different marital status (except married) among clusters 0 and 2. Quantiles and a median for different marital status groups are similar inside the one cluster (1 or 3). Perhaps marital status doesn't influence significantly on a customer behavior. Let's check how consumption depend on level of education.","06a683bc":"There are no correlation between people with teens and total money spent\/total purchases. However, there is a negative correlation for people with kids and total money spent\/total purchases.\nThere is a positive correlation between income and total money spent\/total purchases.\nLet's check if these correlations are statistically significant.","a2a98608":"50% of the customers spend more than 23.58 on each purchase. Purchases for more than 99.2 appeared to be outliers, so we will drop such observations.","3ed7f73f":"The main category that customer spends the most money (50%) is a wine category. The second important category is meat. ","ed2ff1ac":"## Correlated features\nLet's see if we have correlated features and test the following hypothesis:\n* people with kids and teens tend to spent more \n* customers with higher income buy more product and spend more money","5c21144a":"For dimensionality reduction data should be scaled to [0,1]. We will use MinMaxScaler for this purpose.","361fee96":"Let's see if the dataset has duplicates","fc14daf1":"People from different clusters spend money on different shop categories in a similar way. Let's visualize how people from different clusters make purchases.","652ff9b1":"## Feature selection ","e073f6dc":"Customers of cluster 2 and cluster 3 spend on meat more than customers of clusters 1 and 0. Let's see the distribution of customers' income. ","6d7b2321":"The data is doubtful. How customers that don't visit the shop's website are able to make web purchases? And why people who visit the website quite often (more 12.5 times) may still not buy anything (or buy very little)? I would recommend to check the way this data is collected. Also, there is no correlation between shop's website visits and buys via internet.","a18d85bb":"Perform log transform on skewed columns","04a58cc8":"Drop original parsed column Dt_Customer and columns used for EDA ('MntTotal', 'NumTotalPurchases')","7beee0be":"### DBSCAN","19e16bff":"### Hierarchical clustering","f53148d9":"Let's discover how marital status influence on consumption","f74394c2":"# Dimensionality reduction and clustering","fd80156e":"We will delete the data of customers with salary higher than 153k as boxplot shows these values as outliers.","f2dbf43b":"# Conclusion - personas\n\nConsumers were clustered into 4 different groups according to the money they spent in the shop and their marital status. Customers from clusters 2 and 3 spend more than ones from 0 and 1.\n\n### Cluster 0 (non-target)\n\n75% of customers spend 9 to 15.7 in total\n\n75% of customers spend 2.2-4.4 to wine\n\n75% of customers spend 2.2-3.5 to meat\n\n75% of customers have income in range 27k - 46k\n\n75% of customers were born between 1963 - 1978\n\nMost of them have one kid. \n\nAround half of them have 1 teenager.\n\nCan be with any education level\n\nCan be any marital status except married.\n\n### Cluster 1 (non-target)\n\nSimilar to Cluster 0 except that there are only married people\n\n### Cluster 2 (target)\n\n75% of customers spend 22.8 to 28.7 in total\n\n75% of customers spend 5.6-6.5 to wine\n\n75% of customers spend 4.8-6.0 to meat\n\n75% of customers have income in range 57k - 75.7k\n\n75% of customers were born between 1956 - 1975\n\nMost of them don't have kids. \n\nAround half of them have 1 teenager.\n\nAll of them have high education.\n\nCan be any marital status except married.\n\nPrefer to buy in stores\n\n### Cluster 3 (target)\n\nSimilar to Cluster 2 except there are only married people\n\n# Recommendations:\n\nCustomers from cluster 2 and 3 spend the similar amount of money per item but just buy more. I would recommend to add more expensive products for such customers in categories wine and meat in order to increase sells.\n\n# Next steps\n\nAs agglomerative clustering is sensitive to outliers, use outlier detection algorithm or try another cluster algorithm.\n\nTo perform analysis on the target groups of customers (clusters 2 and 3).\n\n\n","04c82594":"Visualize the distributions of money spent for each clusters","498f60fa":"The distributions of average check are very similar for each clusters. People from clusters 2 and 3 spend more as they just buy more items (and NOT more expensive items).\n\nIn  EDA, we found that wines consist 50% of the shop's revenue. Let's see which cluster has the highest amount of money spent on wines.","81c71e82":"Let's check the hypothesis that the more customers visit the shop's website, the more they buy via internet. For the we examine web purchases and web visits variables. ","db75899f":"The mean_squared_error is very low meaning we don't lose too much information while reducing dimensions.\n\nHere, I'm comparing the main clustering algoruthms and choosing the best one using silhoutte score.","0efb21e9":"### Removing features with low variance\n\nFor that we will use VarianceThreshold. It removes all features whose variance doesn\u2019t meet some threshold. Here, I want to remove all features with variance lower than 0.03","82b4d0cf":"Let's see which values Marital_Status column contain","712044da":"Let's see the patterns of consumption by people with different levels of education ","811d8037":"The medians of birth dates for customers from clusters 2 and 3 are lower than ones for clusters 0 and 1 meaning customers from clusters 2 and 3 usually older than customers from clusters 0 and 1.","c505acdd":"## Customer expenses by channels\nLet's see by which channel cutomers buy the most","86b5387e":"# Load libraries","8423456d":"Income column contains 24 missing values. As missing values makes up to 1% of the data, it's possible to drop such rows.","f77c3a49":"There are no people without high education in cluster 2 and there are only few people in cluster 3. Considering that members of cluster 2 and 3 make the most sales for the shop, we can conclude that people with high education spend more.","a8be3408":"Add features that were dropped during preprocessing.","039e7cfc":"People with basic education spent much less at the store, so product managers shouls focus their activity primarily on the people with high education. \nIntriguily, PhD candidates spent the most for wine which is the main business source.","f0b9b08a":"Let's check if the dataset has missing values","e9e51611":"A lot of customers from clusters 0 and 1 have one kid whereas the majority of customers from clusters 2 and 3 don't have kids.\nThis is in a good agreement with a previous statement that people with kids tend to spend less.\n\nLet's see if people from different clusters have different ratios of product categories.","47311f69":"Let's look on the income distribution of the customers ","d1970ff1":"Customer personality analysis allows businesses to manage their time and money in an effective way. When organizations segment their consumers, they can identify which consumers are most likely to make an expensive purchase, so they can focus marketing efforts on the most valuable customers, reducing the amount of resources spent on marketing. \n\nConsumers data usually multidimensional that makes it impossible to segment customers by threshold\/rule-based approach. In contrast, clustering analysis is more practical as it doesn't require the usage of predetermined rules, it's applicable to high-dimensional data and can be dynamic: the clusters definitions change every time the clustering algorithm runs, ensuring that the groups always accurately reflect the current state of the data. Therefore, the objective of the study is to create costumer's segments and find specific features of them","4d7a345d":"There is nothing written about Z_Revenue and Z_CostContact columns in the description of the dataset. Let's see which unique values are in Z_Revenue and Z_CostContact columns.","4264870b":"Members of all clusters buy more often in stores. Members of clusters 2 and 3 tend to buy more often using catalogs and less often using internet. This can be explained as there are many old people in the clusters 2 and 3 than in cluster 1 and 0.\n\nLet's see which marital status members of each cluster have.","48331aa6":"As Alone and Single have identical meaning, we will replace Alone to Single ","0bddc72c":"Learning from large amount of data is a very challenging issue as data analysis becomes more difficult due to the \u201ccurse of dimensionality\u201d(when the dimensionality increases, the volume of the space increases so fast that the available data become sparse). Fortunately, there is a common technique to reduce the number of input features called kernel principal component analysis (kPCA). Kernel PCA is non-linear dimensionality reduction through the use of kernel. This technique is useful in case when the samples from each class cannot be linearly separated. The quality of the kPCA transformation can be defined using mean_squared_error of original dataset and inverse transformed dataset reduced by kPCA.","4227b9c9":"# Analysis of customer's segments","e24382a9":"# Conclusions - Feature engineering and feature selection\n\n<br>Dt_Customer was parsed and year was extracted to a new column. Original column (Dt_Customer) was dropped.\n<br>Columns created for analytics ('MntTotal', 'NumTotalPurchases') were dropped.\n<br>Skewed columns were log-transformed.\n<br>\u201cAlone\u201d was substituted to \u2018\u2019Single\u201d in the Marital_Status column. Rows with 'YOLO' and 'Absurd' in the Marital_Status column were dropped. \n<br>Customers were encoded according to the graduation level: \n* 0 \u2013 Basic,\n* 1 \u2013 Graduation\n* 2 \u2013 Master\n* 3 \u2013 PhD or 2nd cycle.\n\n<br>Encoding appeared in the Education_tr column. Original column Education was dropped.\n<br>All data was scaled using MinMaxScaler.\n\nFeatures with low variance were removed."}}