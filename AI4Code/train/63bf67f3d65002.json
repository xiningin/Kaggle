{"cell_type":{"db84cb2f":"code","0566ec95":"code","ed116ee5":"code","80f8430c":"code","94a97a6b":"code","61dbd054":"code","f32aaf12":"code","1f5794f2":"code","abca4caf":"code","162546bc":"code","d3a251d0":"code","19dc6019":"code","0aef345c":"code","2202d1ca":"markdown","43738a8a":"markdown","3c8ada67":"markdown","8dfd1e1b":"markdown","7d8238f4":"markdown","46c25442":"markdown","4cb84904":"markdown","0e377818":"markdown","805ac00c":"markdown","62bf673a":"markdown","c2a2b522":"markdown","2fef4941":"markdown","873c5182":"markdown","5d5c074b":"markdown","286ad438":"markdown","da773e5e":"markdown","c81aeddf":"markdown","a6bfc8c8":"markdown","cfbea251":"markdown","ae18bab6":"markdown","f7d193a6":"markdown","51008643":"markdown","ab6cdbae":"markdown"},"source":{"db84cb2f":"import pandas as pd\nimport sklearn as sk\nimport math ","0566ec95":"first_sentence = \"Data Science is the sexiest job of the 21st century\"\nsecond_sentence = \"machine learning is the key for data science\"\n#split so each word have their own string\nfirst_sentence = first_sentence.split(\" \")\nsecond_sentence = second_sentence.split(\" \")#join them to remove common duplicate words\ntotal= set(first_sentence).union(set(second_sentence))\nprint(total)","ed116ee5":"wordDictA = dict.fromkeys(total, 0) \nwordDictB = dict.fromkeys(total, 0)\nfor word in first_sentence:\n    wordDictA[word]+=1\n    \nfor word in second_sentence:\n    wordDictB[word]+=1","80f8430c":"pd.DataFrame([wordDictA, wordDictB])\n","94a97a6b":"def computeTF(wordDict, doc):\n    tfDict = {}\n    corpusCount = len(doc)\n    for word, count in wordDict.items():\n        tfDict[word] = count\/float(corpusCount)\n    return(tfDict)\n#running our sentences through the tf function:\ntfFirst = computeTF(wordDictA, first_sentence)\ntfSecond = computeTF(wordDictB, second_sentence)\n#Converting to dataframe for visualization\ntf = pd.DataFrame([tfFirst, tfSecond])","61dbd054":"import nltk\nfrom nltk.corpus import stopwords\nset(stopwords.words('english'))\n","f32aaf12":"filtered_sentence = []\nfor word in wordDictA:\n    if str(word) not in set(stopwords.words('english')):\n        filtered_sentence.append(word)","1f5794f2":"filtered_sentence","abca4caf":"def computeIDF(docList):\n    idfDict = {}\n    N = len(docList)\n    \n    idfDict = dict.fromkeys(docList[0].keys(), 0)\n    for word, val in idfDict.items():\n        idfDict[word] = math.log10(N \/ (float(val) + 1))\n        \n    return(idfDict)\n#inputing our sentences in the log file\nidfs = computeIDF([wordDictA, wordDictB])","162546bc":"def computeTFIDF(tfBow, idfs):\n    tfidf = {}\n    for word, val in tfBow.items():\n        tfidf[word] = val*idfs[word]\n    return(tfidf)\n#running our two sentences through the IDF:\nidfFirst = computeTFIDF(tfFirst, idfs)\nidfSecond = computeTFIDF(tfSecond, idfs)\n#putting it in a dataframe\nidf= pd.DataFrame([idfFirst, idfSecond])","d3a251d0":"idf","19dc6019":"#first step is to import the library\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n#for the sentence, make sure all words are lowercase or you will run #into error. for simplicity, I just made the same sentence all #lowercase\nfirstV= \"Data Science is the sexiest job of the 21st century\"\nsecondV= \"machine learning is the key for data science\"\n#calling the TfidfVectorizer\nvectorize= TfidfVectorizer()\n#fitting the model and passing our sentences right away:\nresponse= vectorize.fit_transform([firstV, secondV])","0aef345c":"print(response)","2202d1ca":"In this notebook I will explain how to implement tf-idf technique in python from scratch , this technique is used to find meaning of sentences consisting of words and cancels out the incapabilities of Bag of Words technique which is good for text classification or for helping a machine read words in numbers.","43738a8a":"![hi](https:\/\/miro.medium.com\/max\/704\/1*3WwlcabDKuIljpk2ec1r-w.png)","3c8ada67":"That\u2019s all for TF formula , just i wanna talk about stop words that we should eliminate them because they are the most commonly occurring words which don\u2019t give any additional value to the document vector .\nin-fact removing these will increase computation and space efficiency.\n\nnltk library has a method to download the stopwords, so instead of explicitly mentioning all the stopwords ourselves we can just use the nltk library and iterate over all the words and remove the stop words. There are many efficient ways to do this, but ill just give a simple method.\nthose a sample of a stopwords in english language :\n","8dfd1e1b":"## Summary :\n\nIn this kernel we are going to explain how to use python and a natural language processing (NLP) technique known as Term Frequency \u2014 Inverse Document Frequency (tf-idf) to summarize documents.\nWe\u2019ll areusing sklearn along with nltk to accomplish this task.\nRemember that you can find the fully working code in my github repository [here](https:\/\/github.com\/Yassine-Hamdaoui\/Tf-Idf).\nThanks for reading and I will be glad to discuss any questions or corrections you may have :) Find me on [LinkedIn](https:\/\/www.linkedin.com\/in\/yassine-hamdaoui\/) if you want to discuss Machine Learning or anything else.\n\n# I hope you find this kernel useful and enjoyable.\n# Your comments and feedback are most welcome.\n# Upvote if you liked it and found useful.","7d8238f4":"## 2 -Term Frequency (TF):\nSuppose we have a set of English text documents and wish to rank which document is most relevant to the query , \u201cData Science is awesome !\u201d A simple way to start out is by eliminating documents that do not contain all three words \u201cData\u201d,\u201dis\u201d, \u201cScience\u201d, and \u201cawesome\u201d, but this still leaves many documents. To further distinguish them, we might count the number of times each term occurs in each document; the number of times a term occurs in a document is called its term frequency.","46c25442":"so let\u2019s load our sentences and combine them together in a single set :","4cb84904":"Now lets add a way to count the words using a dictionary key-value pairing for both sentences :","0e377818":"## 1 - Terminology :\n- t \u2014 term (word)\n- d \u2014 document (set of words)\n- N \u2014 count of corpus\n- corpus \u2014 the total document set","805ac00c":"No let\u2019s writing the TF Function :","62bf673a":"## TF(Term Frequency)-IDF(Inverse Document Frequency) from scratch in python .\n","c2a2b522":"## 4 -Inverse Document Frequency(IDF):\n\nWhile computing TF, all terms are considered equally important. However it is known that certain terms, such as \u201cis\u201d, \u201cof\u201d, and \u201cthat\u201d, may appear a lot of times but have little importance. Thus we need to weigh down the frequent terms while scale up the rare ones, by computing IDF, an inverse document frequency factor is incorporated which diminishes the weight of terms that occur very frequently in the document set and increases the weight of terms that occur rarely.\nIDF is the inverse of the document frequency which measures the informativeness of term t. When we calculate IDF, it will be very low for the most occurring words such as stop words (because stop words such as \u201cis\u201d is present in almost all of the documents, and N\/df will give a very low value to that word). This finally gives what we want, a relative weightage.\n\n\n## idf(t) = N\/df\n\n\nNow there are few other problems with the IDF , in case of a large corpus,say **100,000,000** , the **IDF** value explodes , to avoid the effect we take the log of idf .\nDuring the query time, when a word which is not in vocab occurs, the df will be **0**. As we cannot divide by 0, we smoothen the value by adding 1 to the denominator.\nthat\u2019s the final formula:\n\n\n# Formula :\n\n## idf(t) = log(N\/(df + 1))\n\n\n**tf-idf** now is a the right measure to evaluate how important a word is to a document in a collection or corpus.here are many different variations of TF-IDF but for now let us concentrate on the this basic version.\n# Formula :\n\n## tf-idf(t, d) = tf(t, d) * log(N\/(df + 1))","2fef4941":"## 5 -Implementing TF-IDF in Python From Scratch :\n\nTo make ** TF-IDF** from scratch in python,let\u2019s imagine those two sentences from diffrent document :\n\n- first_sentence : \u201cData Science is the sexiest job of the 21st century\u201d.\n- second_sentence : \u201cmachine learning is the key for data science\u201d.\n\nFirst step we have to create the TF function to calculate total word frequency for all documents. Here are the codes below:\nfirst as usual we should import the necessary libraries :\n","873c5182":"and now we implement the idf formula , let\u2019s finish with calculating the TFI-DF","5d5c074b":"Now we put them in a dataframe and then view the result:","286ad438":"### The weight of a term that occurs in a document is simply proportional to the term frequency.\n# Formula :\n## tf(t,d) = count of t in d \/ number of words in d","da773e5e":"## 3 -Document Frequency :\n\nThis measures the importance of document in whole set of corpus, this is very similar to TF. The only difference is that TF is frequency counter for a term t in document d, where as DF is the count of occurrences of term t in the document set N. In other words, DF is the number of documents in which the word is present. We consider one occurrence if the term consists in the document at least once, we do not need to know the number of times the term is present.\n\n## df(t) = occurrence of t in documents","c81aeddf":"### Creating TF-IDF Model from Scratch","a6bfc8c8":"and that\u2019s the expected output :","cfbea251":"And now that we finished the TF section, we move onto the IDF part:\n","ae18bab6":"![StopWords](https:\/\/miro.medium.com\/max\/850\/1*R1NmayfziRv8QKKUw4dt_w.png)","f7d193a6":"and this is a simple code to download stop words and removing them .","51008643":"That was a lot of work. But it is handy to know, if you are asked to code TF-IDF from scratch in the future. However, this can be done a lot simpler thanks to sklearn library. Let\u2019s look at the example from them below:","ab6cdbae":"## Table of Contents:\n- Terminology .\n- Term Frequency(TF) .\n- Document Frequency .\n- Inverse Document Frequency .\n- Implementation in Python ."}}