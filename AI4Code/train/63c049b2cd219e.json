{"cell_type":{"e8d559c8":"code","80580e31":"code","09684a51":"code","0528d9d4":"code","df1e98c5":"code","3124d875":"code","1a7915c4":"code","99020695":"code","806c77af":"code","c76a538b":"code","fb8a5015":"code","e0e86713":"code","b2acebd8":"code","070feeea":"code","787c67e7":"code","536e805b":"code","11d1c686":"code","677435ba":"code","f42914fc":"code","4aef67b0":"code","e8e259c0":"code","93300458":"code","3625ffb3":"code","67b57e28":"code","dd48b584":"code","c2d0ac74":"code","f8731589":"code","bd52d77c":"code","dab93a84":"code","847b50fd":"code","0aad6c2f":"code","8d49f34e":"code","245f19dd":"code","96e03db8":"code","701f71a6":"code","e7d61385":"code","2d0d7a71":"code","a7bbd7ce":"code","543a4555":"code","2c83503b":"code","093e3cba":"code","e974bad0":"code","14870ae0":"code","d61e9fb9":"code","4561cbaf":"code","b8828fbc":"code","6f466465":"code","d28039cf":"code","680e3912":"code","57d72e03":"code","719bbd9e":"code","de507716":"code","b4dc6df3":"code","782cd57c":"code","fe413ee1":"code","dec67e9f":"code","527336f2":"code","78a2a489":"code","bb15eb04":"code","eadede42":"code","ec65d876":"code","c6fc60f2":"code","5d7396dd":"code","1f0258c1":"code","65fb7fd9":"code","54ff5ee0":"code","1f86822f":"code","f8cbbbc4":"markdown","7980b89e":"markdown","76cc5aef":"markdown","a7789c37":"markdown","7bb377c2":"markdown","b68da42f":"markdown"},"source":{"e8d559c8":"#GENERAL\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport matplotlib as mpl\nimport random\nimport time\n#PATH PROCESS\nimport os\nimport os.path\nfrom pathlib import Path\nimport glob\nfrom scipy.io import loadmat\n#IMAGE PROCESS\nfrom PIL import Image\nfrom keras.preprocessing import image\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nimport cv2\nfrom keras.applications.vgg16 import preprocess_input, decode_predictions\nfrom keras.preprocessing import image\nfrom skimage.feature import hessian_matrix, hessian_matrix_eigvals\nfrom scipy.ndimage.filters import convolve\nfrom skimage import data, io, filters\nimport skimage\nfrom skimage.morphology import convex_hull_image, erosion\nfrom IPython import display\nfrom scipy.ndimage import gaussian_filter\n#SCALER & TRANSFORMATION\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.preprocessing import MinMaxScaler\nfrom keras.utils.np_utils import to_categorical\nfrom sklearn.model_selection import train_test_split\nfrom keras import regularizers\nfrom sklearn.preprocessing import LabelEncoder\n#ACCURACY CONTROL\nfrom sklearn.metrics import confusion_matrix, accuracy_score, classification_report, roc_auc_score, roc_curve\nfrom sklearn.model_selection import GridSearchCV, cross_val_score\nfrom sklearn.metrics import mean_squared_error, r2_score\n#OPTIMIZER\nfrom keras.optimizers import RMSprop,Adam,Optimizer,Optimizer, SGD\n#MODEL LAYERS\nfrom tensorflow.keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization,MaxPooling2D,BatchNormalization,\\\n                        Permute, TimeDistributed, Bidirectional,GRU, SimpleRNN,\\\nLSTM, GlobalAveragePooling2D, SeparableConv2D, ZeroPadding2D, Convolution2D, ZeroPadding2D,Reshape, Conv2DTranspose,\\\nLeakyReLU, GaussianNoise, GlobalMaxPooling2D, ReLU, Input, Concatenate\nfrom keras import models\nfrom keras import layers\nimport tensorflow as tf\nfrom keras.applications import VGG16,VGG19,inception_v3\nfrom keras import backend as K\nfrom keras.utils import plot_model\nfrom keras.datasets import mnist\nimport keras\nfrom keras.models import Model\n#IGNORING WARNINGS\nfrom warnings import filterwarnings\nfilterwarnings(\"ignore\",category=DeprecationWarning)\nfilterwarnings(\"ignore\", category=FutureWarning) \nfilterwarnings(\"ignore\", category=UserWarning)","80580e31":"Example_IMG_Path = \"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/INPUT\/Image00000.png\"\nExample_COOR_Path = \"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/OUTPUT\/Image00000.png\"","09684a51":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\nExample_IMG_Reading = cv2.cvtColor(cv2.imread(Example_IMG_Path),cv2.COLOR_BGR2RGB)\nExample_COOR_Reading = cv2.cvtColor(cv2.imread(Example_COOR_Path),cv2.COLOR_BGR2RGB)\n\naxis[0].imshow(Example_IMG_Reading)\naxis[0].set_xlabel(Example_IMG_Reading.shape)\naxis[0].set_ylabel(Example_IMG_Reading.size)\naxis[0].set_title(\"IMG\")\n\naxis[1].imshow(Example_COOR_Reading)\naxis[1].set_xlabel(Example_COOR_Reading.shape)\naxis[1].set_ylabel(Example_COOR_Reading.size)\naxis[1].set_title(\"COORDINATE\")","0528d9d4":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\nExample_IMG_Reading = cv2.imread(Example_IMG_Path)\nExample_COOR_Reading = cv2.imread(Example_COOR_Path)\n\naxis[0].imshow(Example_IMG_Reading[:,:,0])\naxis[0].set_xlabel(Example_IMG_Reading.shape)\naxis[0].set_ylabel(Example_IMG_Reading.size)\naxis[0].set_title(\"IMG\")\n\naxis[1].imshow(Example_COOR_Reading[:,:,0])\naxis[1].set_xlabel(Example_COOR_Reading.shape)\naxis[1].set_ylabel(Example_COOR_Reading.size)\naxis[1].set_title(\"COORDINATE\")","df1e98c5":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\nExample_IMG_Reading = cv2.imread(Example_IMG_Path)\nExample_COOR_Reading = cv2.imread(Example_COOR_Path)\n\nPloting_Attr = axis[0].imshow(Example_IMG_Reading[:,:,0], cmap='jet')\nfigure.subplots_adjust(right=0.9)\naxis[0].set_xlabel(Example_IMG_Reading[:,:,0].shape)\naxis[0].set_ylabel(Example_IMG_Reading[:,:,0].size)\naxis[0].set_title(\"IMG\")\nfigure.colorbar(Ploting_Attr, ax=axis.ravel().tolist(), shrink=0.5,extend='both',ticks=range(6),label='DIMENSION')\n\naxis[1].imshow(Example_COOR_Reading[:,:,0], cmap='jet')\naxis[1].set_xlabel(Example_COOR_Reading[:,:,0].shape)\naxis[1].set_title(\"COORDINATE\")","3124d875":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\nExample_IMG_Reading = cv2.imread(\"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/INPUT\/Image00013.png\")\nExample_COOR_Reading = cv2.imread(\"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/OUTPUT\/Image00013.png\")\n\nPloting_Attr = axis[0].imshow(Example_IMG_Reading[:,:,0], cmap='jet')\nfigure.subplots_adjust(right=0.9)\naxis[0].set_xlabel(Example_IMG_Reading[:,:,0].shape)\naxis[0].set_ylabel(Example_IMG_Reading[:,:,0].size)\naxis[0].set_title(\"IMG\")\nfigure.colorbar(Ploting_Attr, ax=axis.ravel().tolist(), shrink=0.5,extend='both',ticks=range(6),label='DIMENSION')\n\naxis[1].imshow(Example_COOR_Reading[:,:,0], cmap='jet')\naxis[1].set_xlabel(Example_COOR_Reading[:,:,0].shape)\naxis[1].set_title(\"COORDINATE\")","1a7915c4":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\nExample_IMG_Reading = cv2.imread(\"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/INPUT\/Image00023.png\")\nExample_COOR_Reading = cv2.imread(\"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/OUTPUT\/Image00023.png\")\n\nPloting_Attr = axis[0].imshow(Example_IMG_Reading[:,:,0], cmap='jet')\nfigure.subplots_adjust(right=0.9)\naxis[0].set_xlabel(Example_IMG_Reading[:,:,0].shape)\naxis[0].set_ylabel(Example_IMG_Reading[:,:,0].size)\naxis[0].set_title(\"IMG\")\nfigure.colorbar(Ploting_Attr, ax=axis.ravel().tolist(), shrink=0.5,extend='both',ticks=range(6),label='DIMENSION')\n\naxis[1].imshow(Example_COOR_Reading[:,:,0], cmap='jet')\naxis[1].set_xlabel(Example_COOR_Reading[:,:,0].shape)\naxis[1].set_title(\"COORDINATE\")","99020695":"figure = plt.figure(figsize=(10,10))\n\nExample_IMG_Reading = cv2.imread(\"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/INPUT\/Image00023.png\")\nExample_COOR_Reading = cv2.imread(\"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/OUTPUT\/Image00023.png\")\n\nBlend_Image = cv2.addWeighted(Example_IMG_Reading,0.8,Example_COOR_Reading,0.4,0.5)\n\nplt.imshow(Blend_Image[:,:,0], cmap='jet')\nplt.axis(\"off\")","806c77af":"figure = plt.figure(figsize=(10,10))\n\nExample_IMG_Reading = cv2.imread(Example_IMG_Path)\nExample_COOR_Reading = cv2.imread(Example_COOR_Path)\n\nBlend_Image = cv2.addWeighted(Example_IMG_Reading,0.8,Example_COOR_Reading,0.4,0.5)\n\nplt.imshow(Blend_Image[:,:,0], cmap='jet')\nplt.axis(\"off\")","c76a538b":"figure = plt.figure(figsize=(10,10))\n\nExample_IMG_Reading = cv2.imread(\"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/INPUT\/Image00063.png\")\nExample_COOR_Reading = cv2.imread(\"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/OUTPUT\/Image00063.png\")\n\nBlend_Image = cv2.addWeighted(Example_IMG_Reading,0.8,Example_COOR_Reading,0.4,0.5)\n\nplt.imshow(Blend_Image[:,:,0], cmap='jet')\nplt.axis(\"off\")","fb8a5015":"figure = plt.figure(figsize=(10,10))\n\nExample_IMG_Reading = cv2.imread(\"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/INPUT\/Image00069.png\")\nExample_COOR_Reading = cv2.imread(\"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/OUTPUT\/Image00069.png\")\n\nBlend_Image = cv2.addWeighted(Example_IMG_Reading,0.8,Example_COOR_Reading,0.4,0.5)\n\nplt.imshow(Blend_Image[:,:,0], cmap='jet')\nplt.axis(\"off\")","e0e86713":"figure = plt.figure(figsize=(10,10))\n\nExample_IMG_Reading = cv2.imread(\"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/INPUT\/Image00055.png\")\nExample_COOR_Reading = cv2.imread(\"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/OUTPUT\/Image00055.png\")\n\nBlend_Image = cv2.addWeighted(Example_IMG_Reading,0.8,Example_COOR_Reading,0.4,0.5)\n\nplt.imshow(Blend_Image[:,:,0], cmap='jet')\nplt.axis(\"off\")","b2acebd8":"figure = plt.figure(figsize=(10,10))\n\nExample_IMG_Reading = cv2.cvtColor(cv2.imread(\"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/INPUT\/Image00003.png\"),\n                      cv2.COLOR_BGR2RGB)\nExample_COOR_Reading = cv2.imread(\"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/OUTPUT\/Image00003.png\")\n\nBlend_Image = cv2.addWeighted(Example_IMG_Reading,0.5,Example_COOR_Reading,0.4,0.5)\n\nplt.imshow(Blend_Image)\nplt.axis(\"off\")","070feeea":"figure = plt.figure(figsize=(10,10))\n\nExample_IMG_Reading = cv2.cvtColor(cv2.imread(\"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/INPUT\/Image01003.png\"),\n                      cv2.COLOR_BGR2RGB)\nExample_COOR_Reading = cv2.imread(\"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/OUTPUT\/Image01003.png\")\n\nBlend_Image = cv2.addWeighted(Example_IMG_Reading,0.5,Example_COOR_Reading,0.4,0.5)\n\nplt.imshow(Blend_Image[:,:,0])\nplt.axis(\"off\")","787c67e7":"figure,axis = plt.subplots(2,5,figsize=(12,12))\n\nfor indexing,operations in enumerate(axis.flat):\n    \n    Example_IMG_Reading = cv2.imread(f\"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/INPUT\/Image0000{indexing}.png\")\n    Example_COOR_Reading = cv2.imread(f\"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/OUTPUT\/Image0000{indexing}.png\")\n    Blend_Image = cv2.addWeighted(Example_IMG_Reading,0.8,Example_COOR_Reading,0.4,0.5)\n    \n    operations.set_xlabel(Blend_Image.shape)\n    operations.imshow(Blend_Image[:,:,0], cmap='jet')\nplt.tight_layout()\nplt.show()","536e805b":"figure,axis = plt.subplots(2,5,figsize=(12,12))\n\nfor indexing,operations in enumerate(axis.flat):\n    \n    Example_IMG_Reading = cv2.imread(f\"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/INPUT\/Image0000{indexing}.png\")\n    \n    operations.set_xlabel(Example_IMG_Reading.shape)\n    operations.imshow(Example_IMG_Reading[:,:,0], cmap='jet')\nplt.tight_layout()\nplt.show()","11d1c686":"figure = plt.figure(figsize=(10,10))\n\nExample_IMG_Reading = cv2.imread(Example_IMG_Path)\nExample_COOR_Reading = cv2.imread(Example_COOR_Path)\n\nExample_Testing_Zeros = np.zeros((Example_IMG_Reading.shape[0], Example_IMG_Reading.shape[1], Example_IMG_Reading.shape[2]), dtype=np.float32)\n\nExample_Testing_Zeros[:,:,0] = Example_COOR_Reading[:,:,0]\nExample_Testing_Zeros[:,:,1:] = Example_COOR_Reading[:,:,1:]\n\nGaussian_Image_Example = gaussian_filter(Example_Testing_Zeros,sigma=9,truncate=2*2)\n\nplt.imshow(Gaussian_Image_Example[:,:,0],cmap=\"jet\")\nplt.axis(\"off\")","677435ba":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\nExample_IMG_Reading = cv2.imread(Example_IMG_Path)\nExample_COOR_Reading = cv2.imread(Example_COOR_Path)\n\nExample_Testing_Zeros = np.zeros((Example_IMG_Reading.shape[0], Example_IMG_Reading.shape[1], Example_IMG_Reading.shape[2]), dtype=np.float32)\n\nExample_Testing_Zeros[:,:,0] = Example_COOR_Reading[:,:,0]\nExample_Testing_Zeros[:,:,1:] = Example_COOR_Reading[:,:,1:]\n\nGaussian_Image_Example = gaussian_filter(Example_Testing_Zeros,sigma=9,truncate=2*2)\n\naxis[0].imshow(Gaussian_Image_Example[:,:,0], cmap='jet')\naxis[0].set_xlabel(Gaussian_Image_Example[:,:,0].shape)\naxis[0].set_ylabel(Gaussian_Image_Example[:,:,0].size)\naxis[0].set_title(\"EXPORT\")\n\naxis[1].imshow(Example_COOR_Reading[:,:,0], cmap='jet')\naxis[1].set_xlabel(Example_COOR_Reading[:,:,0].shape)\naxis[1].set_title(\"COORDINATE\")","f42914fc":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\nReading_Image = cv2.cvtColor(cv2.imread(Example_IMG_Path),cv2.COLOR_BGR2RGB)\nReading_Mask = cv2.cvtColor(cv2.imread(Example_COOR_Path),cv2.COLOR_BGR2GRAY)\n    \nCopy_Main = Reading_Image.copy()\nCopy_Main[Reading_Mask == 1] = [255,255,255]\nCopy_Main[Reading_Mask == 2] = [0,255,255]\n    \nCopy_Main_Two = Reading_Image.copy()\nBlend_Image = cv2.addWeighted(Copy_Main,0.5,Copy_Main_Two,0.3,0,Copy_Main_Two)\n\naxis[0].imshow(Blend_Image[:,:,0],cmap=\"jet\")\naxis[0].set_xlabel(Blend_Image[:,:,0].shape)\naxis[0].set_ylabel(Blend_Image[:,:,0].size)\naxis[0].set_title(\"BLEND\")\n\naxis[1].imshow(Reading_Mask)\naxis[1].set_xlabel(Reading_Mask.shape)\naxis[1].set_title(\"COORDINATE\")","4aef67b0":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\nExample_IMG_Path = \"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/INPUT\/Image00100.png\"\nExample_COOR_Path = \"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/OUTPUT\/Image00100.png\"\n\nReading_Image = cv2.cvtColor(cv2.imread(Example_IMG_Path),cv2.COLOR_BGR2RGB)\nReading_Mask = cv2.cvtColor(cv2.imread(Example_COOR_Path),cv2.COLOR_BGR2GRAY)\n    \nCopy_Main = Reading_Image.copy()\nCopy_Main[Reading_Mask == 1] = [255,255,255]\nCopy_Main[Reading_Mask == 2] = [0,255,255]\n    \nCopy_Main_Two = Reading_Image.copy()\nBlend_Image = cv2.addWeighted(Copy_Main,0.5,Copy_Main_Two,0.3,0,Copy_Main_Two)\n\naxis[0].imshow(Blend_Image[:,:,0],cmap=\"jet\")\naxis[0].set_xlabel(Blend_Image[:,:,0].shape)\naxis[0].set_ylabel(Blend_Image[:,:,0].size)\naxis[0].set_title(\"BLEND\")\n\naxis[1].imshow(Reading_Mask)\naxis[1].set_xlabel(Reading_Mask.shape)\naxis[1].set_title(\"COORDINATE\")","e8e259c0":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\nExample_IMG_Path = \"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/INPUT\/Image01100.png\"\nExample_COOR_Path = \"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/OUTPUT\/Image01100.png\"\n\nReading_Image = cv2.cvtColor(cv2.imread(Example_IMG_Path),cv2.COLOR_BGR2RGB)\nReading_Mask = cv2.cvtColor(cv2.imread(Example_COOR_Path),cv2.COLOR_BGR2GRAY)\n    \nCopy_Main = Reading_Image.copy()\nCopy_Main[Reading_Mask == 1] = [255,255,255]\nCopy_Main[Reading_Mask == 2] = [0,255,255]\n    \nCopy_Main_Two = Reading_Image.copy()\nBlend_Image = cv2.addWeighted(Copy_Main,0.5,Copy_Main_Two,0.3,0,Copy_Main_Two)\n\naxis[0].imshow(Blend_Image[:,:,0],cmap=\"jet\")\naxis[0].set_xlabel(Blend_Image[:,:,0].shape)\naxis[0].set_ylabel(Blend_Image[:,:,0].size)\naxis[0].set_title(\"BLEND\")\n\naxis[1].imshow(Reading_Mask)\naxis[1].set_xlabel(Reading_Mask.shape)\naxis[1].set_title(\"COORDINATE\")","93300458":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\nExample_IMG_Path = \"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/INPUT\/Image01140.png\"\nExample_COOR_Path = \"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/OUTPUT\/Image01140.png\"\n\nReading_Image = cv2.cvtColor(cv2.imread(Example_IMG_Path),cv2.COLOR_BGR2RGB)\nReading_Mask = cv2.cvtColor(cv2.imread(Example_COOR_Path),cv2.COLOR_BGR2GRAY)\n    \nCopy_Main = Reading_Image.copy()\nCopy_Main[Reading_Mask == 1] = [255,255,255]\nCopy_Main[Reading_Mask == 2] = [0,255,255]\n    \nCopy_Main_Two = Reading_Image.copy()\nBlend_Image = cv2.addWeighted(Copy_Main,0.5,Copy_Main_Two,0.3,0,Copy_Main_Two)\n\naxis[0].imshow(Blend_Image[:,:,0],cmap=\"jet\")\naxis[0].set_xlabel(Blend_Image[:,:,0].shape)\naxis[0].set_ylabel(Blend_Image[:,:,0].size)\naxis[0].set_title(\"BLEND\")\n\naxis[1].imshow(Reading_Mask)\naxis[1].set_xlabel(Reading_Mask.shape)\naxis[1].set_title(\"COORDINATE\")","3625ffb3":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\nExample_IMG_Path = \"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/INPUT\/Image01170.png\"\nExample_COOR_Path = \"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/OUTPUT\/Image01170.png\"\n\nReading_Image = cv2.cvtColor(cv2.imread(Example_IMG_Path),cv2.COLOR_BGR2RGB)\nReading_Mask = cv2.cvtColor(cv2.imread(Example_COOR_Path),cv2.COLOR_BGR2GRAY)\n    \nCopy_Main = Reading_Image.copy()\nCopy_Main[Reading_Mask == 1] = [255,0,0]\nCopy_Main[Reading_Mask == 2] = [0,255,255]\n    \nCopy_Main_Two = Reading_Image.copy()\nBlend_Image = cv2.addWeighted(Copy_Main,0.5,Copy_Main_Two,0.3,0,Copy_Main_Two)\n\naxis[0].imshow(Blend_Image[:,:,0],cmap=\"jet\")\naxis[0].set_xlabel(Blend_Image[:,:,0].shape)\naxis[0].set_ylabel(Blend_Image[:,:,0].size)\naxis[0].set_title(\"BLEND\")\n\naxis[1].imshow(Reading_Mask)\naxis[1].set_xlabel(Reading_Mask.shape)\naxis[1].set_title(\"COORDINATE\")","67b57e28":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\nExample_IMG_Path = \"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/INPUT\/Image02170.png\"\nExample_COOR_Path = \"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/OUTPUT\/Image02170.png\"\n\nReading_Image = cv2.cvtColor(cv2.imread(Example_IMG_Path),cv2.COLOR_BGR2RGB)\nReading_Mask = cv2.cvtColor(cv2.imread(Example_COOR_Path),cv2.COLOR_BGR2GRAY)\n    \nCopy_Main = Reading_Image.copy()\nCopy_Main[Reading_Mask == 1] = [255,0,0]\nCopy_Main[Reading_Mask == 2] = [0,255,255]\n    \nCopy_Main_Two = Reading_Image.copy()\nBlend_Image = cv2.addWeighted(Copy_Main,0.5,Copy_Main_Two,0.3,0,Copy_Main_Two)\n\naxis[0].imshow(Blend_Image[:,:,0],cmap=\"jet\")\naxis[0].set_xlabel(Blend_Image[:,:,0].shape)\naxis[0].set_ylabel(Blend_Image[:,:,0].size)\naxis[0].set_title(\"BLEND\")\n\naxis[1].imshow(Reading_Mask)\naxis[1].set_xlabel(Reading_Mask.shape)\naxis[1].set_title(\"COORDINATE\")","dd48b584":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\nExample_IMG_Path = \"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/INPUT\/Image00070.png\"\nExample_COOR_Path = \"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/OUTPUT\/Image00070.png\"\n\nReading_Image = cv2.cvtColor(cv2.imread(Example_IMG_Path),cv2.COLOR_BGR2RGB)\nReading_Mask = cv2.cvtColor(cv2.imread(Example_COOR_Path),cv2.COLOR_BGR2GRAY)\n    \nCopy_Main = Reading_Image.copy()\nCopy_Main[Reading_Mask == 1] = [255,0,0]\nCopy_Main[Reading_Mask == 2] = [0,255,255]\n    \nCopy_Main_Two = Reading_Image.copy()\nBlend_Image = cv2.addWeighted(Copy_Main,0.5,Copy_Main_Two,0.3,0,Copy_Main_Two)\n\naxis[0].imshow(Blend_Image[:,:,0],cmap=\"jet\")\naxis[0].set_xlabel(Blend_Image[:,:,0].shape)\naxis[0].set_ylabel(Blend_Image[:,:,0].size)\naxis[0].set_title(\"BLEND\")\n\naxis[1].imshow(Reading_Mask)\naxis[1].set_xlabel(Reading_Mask.shape)\naxis[1].set_title(\"COORDINATE\")","c2d0ac74":"Image_Path = Path(\"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/INPUT\")\nMask_Path = Path(\"..\/input\/deep-person-detection-on-non-cenital-data\/DATASET_SINTETICO\/TRAIN_DATA\/OUTPUT\")\n\nAll_Image_List = list(Image_Path.glob(r\"*.png\"))\nAll_Mask_List = list(Mask_Path.glob(r\"*.png\"))","f8731589":"Image_Series = pd.Series(All_Image_List,name=\"IMAGE\").astype(str)\nMask_Series = pd.Series(All_Mask_List,name=\"MASK\").astype(str)","bd52d77c":"Image_Series = sorted(Image_Series)\nMask_Series = sorted(Mask_Series)","dab93a84":"print(type(Image_Series))\nprint(type(Mask_Series))","847b50fd":"print(Image_Series[5])\nprint(\"---\"*10)\nprint(Mask_Series[5])","0aad6c2f":"Image_List = []\nTransformation_List = []\n\nfor image_x,mask_x in zip(Image_Series,Mask_Series):\n    \n    Reading_Image = cv2.cvtColor(cv2.imread(image_x),cv2.COLOR_BGR2RGB)\n    Transformation_Image = cv2.imread(mask_x)\n    \n    Blend_Image = cv2.addWeighted(Reading_Image,0.6,Transformation_Image,0.4,0.5)\n    \n    Resized_Image = cv2.resize(Reading_Image,(180,180))\n    Resized_Blend = cv2.resize(Blend_Image,(180,180))\n    \n    Image_List.append(Resized_Image)\n    Transformation_List.append(Resized_Blend)","8d49f34e":"print(len(Image_List))\nprint(len(Transformation_List))","245f19dd":"print(\"WHEN IT IS ARRAY IMAGE SHAPE: \",np.shape(np.array(Image_List)))\nprint(\"WHEN IT IS ARRAY MASK SHAPE: \",np.shape(np.array(Transformation_List)))","96e03db8":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\naxis[0].imshow(Transformation_List[210][:,:,0],cmap=\"jet\")\naxis[0].set_xlabel(Transformation_List[210].shape)\naxis[0].set_title(\"MASK\")\naxis[1].imshow(Image_List[210])\naxis[1].set_xlabel(Image_List[210].shape)\naxis[1].set_title(\"ORIGINAL\")","701f71a6":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\naxis[0].imshow(Transformation_List[10][:,:,0],cmap=\"jet\")\naxis[0].set_xlabel(Transformation_List[10].shape)\naxis[0].set_title(\"MASK\")\naxis[1].imshow(Image_List[10])\naxis[1].set_xlabel(Image_List[10].shape)\naxis[1].set_title(\"ORIGINAL\")","e7d61385":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\naxis[0].imshow(Transformation_List[1000],cmap=\"jet\")\naxis[0].set_xlabel(Transformation_List[1000].shape)\naxis[0].set_title(\"MASK\")\naxis[1].imshow(Image_List[1000])\naxis[1].set_xlabel(Image_List[1000].shape)\naxis[1].set_title(\"ORIGINAL\")","2d0d7a71":"figure,axis = plt.subplots(1,2,figsize=(10,10))\n\naxis[0].imshow(Transformation_List[1500],cmap=\"jet\")\naxis[0].set_xlabel(Transformation_List[1500].shape)\naxis[0].set_title(\"MASK\")\naxis[1].imshow(Image_List[1500])\naxis[1].set_xlabel(Image_List[1500].shape)\naxis[1].set_title(\"ORIGINAL\")","a7bbd7ce":"Reduced_Transformation_List = Transformation_List[:1001]\nReduced_Image_List = Image_List[:1001]","543a4555":"Train_Set = np.array(Reduced_Image_List,dtype=\"float32\")\nTransformation_Set = np.array(Reduced_Transformation_List,dtype=\"float32\")\n\nTrain_Set = Train_Set \/ 255.\nTransformation_Set = Transformation_Set \/ 255.","2c83503b":"print(\"TRAIN SHAPE: \",Train_Set.shape)\nprint(\"TRANSFORMATION SHAPE: \",Transformation_Set.shape)","093e3cba":"compile_loss = \"binary_crossentropy\"\ncompile_optimizer = Adam(lr=0.000001)\noutput_class = 1","e974bad0":"Early_Stopper = tf.keras.callbacks.EarlyStopping(monitor=\"loss\",patience=3,mode=\"min\")\nCheckpoint_Model = tf.keras.callbacks.ModelCheckpoint(monitor=\"val_accuracy\",\n                                                      save_best_only=True,\n                                                      save_weights_only=True,\n                                                      filepath=\".\/modelcheck\")","14870ae0":"E_AE = Sequential()\nE_AE.add(Conv2D(32,(5,5),kernel_initializer = 'he_normal'))\nE_AE.add(BatchNormalization())\nE_AE.add(ReLU())\n#\nE_AE.add(Conv2D(64,(5,5),kernel_initializer = 'he_normal'))\nE_AE.add(BatchNormalization())\nE_AE.add(ReLU())\n#\nE_AE.add(Conv2D(128,(5,5),kernel_initializer = 'he_normal'))\nE_AE.add(BatchNormalization())\nE_AE.add(ReLU())\n#\nE_AE.add(Conv2D(256,(5,5),kernel_initializer = 'he_normal'))\nE_AE.add(BatchNormalization())\nE_AE.add(ReLU())\n\n\nD_AE = Sequential()\nD_AE.add(Conv2DTranspose(128,(5,5)))\nD_AE.add(ReLU())\n#\nD_AE.add(Conv2DTranspose(64,(5,5)))\nD_AE.add(ReLU())\n#\nD_AE.add(Conv2DTranspose(32,(5,5)))\nD_AE.add(ReLU())\n#\nD_AE.add(Conv2DTranspose(output_class,(5,5)))\nD_AE.add(ReLU())","d61e9fb9":"Auto_Encoder = Sequential([E_AE,D_AE])\nAuto_Encoder.compile(loss=compile_loss,optimizer=compile_optimizer,metrics=[\"mse\"])","4561cbaf":"Model_AutoEncoder = Auto_Encoder.fit(Train_Set[:600],Transformation_Set[:600],epochs=20,callbacks=[Checkpoint_Model])","b8828fbc":"Prediction_Seen = Auto_Encoder.predict(Train_Set[:10])","6f466465":"figure,axis = plt.subplots(1,2,figsize=(10,10))\naxis[0].imshow(Train_Set[4])\naxis[0].set_title(\"ORIGINAL\")\naxis[1].imshow(Prediction_Seen[4])\naxis[1].set_title(\"PREDICTION\")","d28039cf":"figure,axis = plt.subplots(1,2,figsize=(10,10))\naxis[0].imshow(Train_Set[6])\naxis[0].set_title(\"ORIGINAL\")\naxis[1].imshow(Prediction_Seen[6])\naxis[1].set_title(\"PREDICTION\")","680e3912":"figure,axis = plt.subplots(1,2,figsize=(10,10))\naxis[0].imshow(Train_Set[1])\naxis[0].set_title(\"ORIGINAL\")\naxis[1].imshow(Prediction_Seen[1])\naxis[1].set_title(\"PREDICTION\")","57d72e03":"Prediction_Non_Seen = Auto_Encoder.predict(Train_Set[600:])","719bbd9e":"figure,axis = plt.subplots(1,2,figsize=(10,10))\naxis[0].imshow(Train_Set[600])\naxis[0].set_title(\"ORIGINAL\")\nPloting_Attr = axis[1].imshow(Prediction_Non_Seen[0])\nfigure.colorbar(Ploting_Attr, ax=axis.ravel().tolist(), shrink=0.5,extend='both',ticks=range(6),label='DIMENSION')\naxis[1].set_title(\"PREDICTION\")","de507716":"figure,axis = plt.subplots(1,2,figsize=(10,10))\naxis[0].imshow(Train_Set[601])\naxis[0].set_title(\"ORIGINAL\")\nPloting_Attr = axis[1].imshow(Prediction_Non_Seen[1])\nfigure.colorbar(Ploting_Attr, ax=axis.ravel().tolist(), shrink=0.5,extend='both',ticks=range(6),label='DIMENSION')\naxis[1].set_title(\"PREDICTION\")","b4dc6df3":"figure,axis = plt.subplots(1,2,figsize=(10,10))\naxis[0].imshow(Train_Set[611])\naxis[0].set_title(\"ORIGINAL\")\nPloting_Attr = axis[1].imshow(Prediction_Non_Seen[11])\nfigure.colorbar(Ploting_Attr, ax=axis.ravel().tolist(), shrink=0.5,extend='both',ticks=range(6),label='DIMENSION')\naxis[1].set_title(\"PREDICTION\")","782cd57c":"figure,axis = plt.subplots(1,2,figsize=(10,10))\naxis[0].imshow(Train_Set[621])\naxis[0].set_title(\"ORIGINAL\")\nPloting_Attr = axis[1].imshow(Prediction_Non_Seen[21])\nfigure.colorbar(Ploting_Attr, ax=axis.ravel().tolist(), shrink=0.5,extend='both',ticks=range(6),label='DIMENSION')\naxis[1].set_title(\"PREDICTION\")","fe413ee1":"figure,axis = plt.subplots(1,2,figsize=(10,10))\naxis[0].imshow(Train_Set[631])\naxis[0].set_title(\"ORIGINAL\")\nPloting_Attr = axis[1].imshow(Prediction_Non_Seen[31])\nfigure.colorbar(Ploting_Attr, ax=axis.ravel().tolist(), shrink=0.5,extend='both',ticks=range(6),label='DIMENSION')\naxis[1].set_title(\"PREDICTION\")","dec67e9f":"figure,axis = plt.subplots(1,2,figsize=(10,10))\naxis[0].imshow(Train_Set[651])\naxis[0].set_title(\"ORIGINAL\")\nPloting_Attr = axis[1].imshow(Prediction_Non_Seen[51])\nfigure.colorbar(Ploting_Attr, ax=axis.ravel().tolist(), shrink=0.5,extend='both',ticks=range(6),label='DIMENSION')\naxis[1].set_title(\"PREDICTION\")","527336f2":"figure,axis = plt.subplots(1,2,figsize=(10,10))\naxis[0].imshow(Train_Set[751])\naxis[0].set_title(\"ORIGINAL\")\nPloting_Attr = axis[1].imshow(Prediction_Non_Seen[151])\nfigure.colorbar(Ploting_Attr, ax=axis.ravel().tolist(), shrink=0.5,extend='both',ticks=range(6),label='DIMENSION')\naxis[1].set_title(\"PREDICTION\")","78a2a489":"figure,axis = plt.subplots(1,2,figsize=(10,10))\naxis[0].imshow(Train_Set[851])\naxis[0].set_title(\"ORIGINAL\")\nPloting_Attr = axis[1].imshow(Prediction_Non_Seen[251])\nfigure.colorbar(Ploting_Attr, ax=axis.ravel().tolist(), shrink=0.5,extend='both',ticks=range(6),label='DIMENSION')\naxis[1].set_title(\"PREDICTION\")","bb15eb04":"figure,axis = plt.subplots(1,2,figsize=(10,10))\naxis[0].imshow(Train_Set[951])\naxis[0].set_title(\"ORIGINAL\")\nPloting_Attr = axis[1].imshow(Prediction_Non_Seen[351])\nfigure.colorbar(Ploting_Attr, ax=axis.ravel().tolist(), shrink=0.5,extend='both',ticks=range(6),label='DIMENSION')\naxis[1].set_title(\"PREDICTION\")","eadede42":"figure,axis = plt.subplots(1,2,figsize=(10,10))\naxis[0].imshow(Train_Set[955])\naxis[0].set_title(\"ORIGINAL\")\nPloting_Attr = axis[1].imshow(Prediction_Non_Seen[355])\nfigure.colorbar(Ploting_Attr, ax=axis.ravel().tolist(), shrink=0.5,extend='both',ticks=range(6),label='DIMENSION')\naxis[1].set_title(\"PREDICTION\")","ec65d876":"figure,axis = plt.subplots(1,2,figsize=(10,10))\naxis[0].imshow(Train_Set[610])\naxis[0].set_title(\"ORIGINAL\")\nPloting_Attr = axis[1].imshow(Prediction_Non_Seen[10],cmap=\"hot\")\nfigure.colorbar(Ploting_Attr, ax=axis.ravel().tolist(), shrink=0.5,extend='both',ticks=range(6),label='DIMENSION')\naxis[1].set_title(\"PREDICTION\")","c6fc60f2":"figure,axis = plt.subplots(1,2,figsize=(10,10))\naxis[0].imshow(Train_Set[620])\naxis[0].set_title(\"ORIGINAL\")\nPloting_Attr = axis[1].imshow(Prediction_Non_Seen[20],cmap=\"hot\")\nfigure.colorbar(Ploting_Attr, ax=axis.ravel().tolist(), shrink=0.5,extend='both',ticks=range(6),label='DIMENSION')\naxis[1].set_title(\"PREDICTION\")","5d7396dd":"figure,axis = plt.subplots(1,2,figsize=(10,10))\naxis[0].imshow(Train_Set[630])\naxis[0].set_title(\"ORIGINAL\")\nPloting_Attr = axis[1].imshow(Prediction_Non_Seen[30],cmap=\"hot\")\nfigure.colorbar(Ploting_Attr, ax=axis.ravel().tolist(), shrink=0.5,extend='both',ticks=range(6),label='DIMENSION')\naxis[1].set_title(\"PREDICTION\")","1f0258c1":"figure,axis = plt.subplots(1,2,figsize=(10,10))\naxis[0].imshow(Train_Set[640])\naxis[0].set_title(\"ORIGINAL\")\nPloting_Attr = axis[1].imshow(Prediction_Non_Seen[40],cmap=\"hot\")\nfigure.colorbar(Ploting_Attr, ax=axis.ravel().tolist(), shrink=0.5,extend='both',ticks=range(6),label='DIMENSION')\naxis[1].set_title(\"PREDICTION\")","65fb7fd9":"figure,axis = plt.subplots(1,2,figsize=(10,10))\naxis[0].imshow(Train_Set[650])\naxis[0].set_title(\"ORIGINAL\")\nPloting_Attr = axis[1].imshow(Prediction_Non_Seen[50],cmap=\"hot\")\nfigure.colorbar(Ploting_Attr, ax=axis.ravel().tolist(), shrink=0.5,extend='both',ticks=range(6),label='DIMENSION')\naxis[1].set_title(\"PREDICTION\")","54ff5ee0":"figure,axis = plt.subplots(1,2,figsize=(10,10))\naxis[0].imshow(Train_Set[660])\naxis[0].set_title(\"ORIGINAL\")\nPloting_Attr = axis[1].imshow(Prediction_Non_Seen[60],cmap=\"hot\")\nfigure.colorbar(Ploting_Attr, ax=axis.ravel().tolist(), shrink=0.5,extend='both',ticks=range(6),label='DIMENSION')\naxis[1].set_title(\"PREDICTION\")","1f86822f":"figure,axis = plt.subplots(1,2,figsize=(10,10))\naxis[0].imshow(Train_Set[760])\naxis[0].set_title(\"ORIGINAL\")\nPloting_Attr = axis[1].imshow(Prediction_Non_Seen[160],cmap=\"hot\")\nfigure.colorbar(Ploting_Attr, ax=axis.ravel().tolist(), shrink=0.5,extend='both',ticks=range(6),label='DIMENSION')\naxis[1].set_title(\"PREDICTION\")","f8cbbbc4":"# AE","7980b89e":"#### EXAMPLE IMAGE AND COORDINATES","76cc5aef":"# PACKAGES AND LIBRARIES","a7789c37":"# PROCESS SUMMARY","7bb377c2":"# TRANSFORMATION DATA","b68da42f":"# HISTORY\n\n#### Introduction\n\n* The GESDPD is a depth images database containing 22000 frames, that simulates to have been taken with a sensor in an elevated front position, in an indoor environment. It was designed to fulfill the following objectives.\n* Allow the train and evaluation of people detection algorithms based on depth, or RGB-D data,without the need of manually labeling.\n* Provide quality synthetic data to the research community in people detection tasks.\n* The people detection task can also be extended to practical applications such as video-surveillance, access control, people flow analysis, behaviour analysis or event capacity management.\n\n#### General contents\n\n* GESDPD is composed of 22000 depth synthetic images, that simulates to have been taken with a sensor in an elevated front position, in a rectangular, indoor working environment.These have been generated using the simulation software Blender.\n* The synthetic images show a room with different persons walking in different directions. The camera perspective is not stationary, it moves around the room along the database, which avoids a constant background.\n* Some examples of the different views are shown in the next figures.\n\n#### INFORMATION\n\n* **Number of frames**: 22000\n\n* **Number of different people**: 4 (3 men and 1 woman)\n\n* **Number of labeled people**: 20800\n\n* **Image resolution**: 320 \u00d7 240 pixels"}}