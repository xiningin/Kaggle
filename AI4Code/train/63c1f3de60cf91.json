{"cell_type":{"871e812f":"code","f6f208b5":"code","3504d882":"code","7d402f6b":"code","d47324af":"code","74ce2967":"code","f3a35cd2":"code","43d10e3f":"code","04df56a6":"code","b13cc686":"code","a1c05d4b":"code","c41626d0":"code","ef238d9a":"code","27e15ccd":"code","2304de57":"code","591b140e":"code","e5ac62cb":"code","102e7104":"code","2223ed9d":"code","439722ae":"markdown","00579299":"markdown","6ec1999a":"markdown","f3f94387":"markdown","76e98709":"markdown","7afd1a83":"markdown","c58163d3":"markdown","58778550":"markdown","2ae53b24":"markdown","ca2af8b0":"markdown","584d96c1":"markdown"},"source":{"871e812f":"!pip install neptune-client neptune-contrib","f6f208b5":"import os\n\nimport numpy as np\nimport pandas as pd\nimport lightgbm as lgb\nfrom sklearn.model_selection import KFold\nfrom sklearn.metrics import roc_auc_score\n\nimport neptune\nfrom neptunecontrib.monitoring.lightgbm import neptune_monitor\nfrom neptunecontrib.versioning.data import log_data_version\nfrom neptunecontrib.monitoring.reporting import send_binary_classification_report","3504d882":"raw_data_path = '..\/input\/'\nnrows = None\n\ntrain_identity = pd.read_csv(f'{raw_data_path}train_identity.csv',nrows=nrows)\ntrain_transaction = pd.read_csv(f'{raw_data_path}train_transaction.csv',nrows=nrows)\ntest_identity = pd.read_csv(f'{raw_data_path}test_identity.csv',nrows=nrows)\ntest_transaction = pd.read_csv(f'{raw_data_path}test_transaction.csv',nrows=nrows)\nsub = pd.read_csv(f'{raw_data_path}sample_submission.csv',nrows=nrows)\n# let's combine the data and work with the whole dataset\ntrain = pd.merge(train_transaction, train_identity, on='TransactionID', how='left')\ntest = pd.merge(test_transaction, test_identity, on='TransactionID', how='left')\n\nprint(f'Train dataset has {train.shape[0]} rows and {train.shape[1]} columns.')\nprint(f'Test dataset has {test.shape[0]} rows and {test.shape[1]} columns.')","7d402f6b":"train['TransactionAmt_to_mean_card1'] = train['TransactionAmt'] \/ train.groupby(['card1'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_mean_card4'] = train['TransactionAmt'] \/ train.groupby(['card4'])['TransactionAmt'].transform('mean')\ntrain['TransactionAmt_to_std_card1'] = train['TransactionAmt'] \/ train.groupby(['card1'])['TransactionAmt'].transform('std')\ntrain['TransactionAmt_to_std_card4'] = train['TransactionAmt'] \/ train.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntest['TransactionAmt_to_mean_card1'] = test['TransactionAmt'] \/ test.groupby(['card1'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_mean_card4'] = test['TransactionAmt'] \/ test.groupby(['card4'])['TransactionAmt'].transform('mean')\ntest['TransactionAmt_to_std_card1'] = test['TransactionAmt'] \/ test.groupby(['card1'])['TransactionAmt'].transform('std')\ntest['TransactionAmt_to_std_card4'] = test['TransactionAmt'] \/ test.groupby(['card4'])['TransactionAmt'].transform('std')\n\ntrain['id_02_to_mean_card1'] = train['id_02'] \/ train.groupby(['card1'])['id_02'].transform('mean')\ntrain['id_02_to_mean_card4'] = train['id_02'] \/ train.groupby(['card4'])['id_02'].transform('mean')\ntrain['id_02_to_std_card1'] = train['id_02'] \/ train.groupby(['card1'])['id_02'].transform('std')\ntrain['id_02_to_std_card4'] = train['id_02'] \/ train.groupby(['card4'])['id_02'].transform('std')\n\ntest['id_02_to_mean_card1'] = test['id_02'] \/ test.groupby(['card1'])['id_02'].transform('mean')\ntest['id_02_to_mean_card4'] = test['id_02'] \/ test.groupby(['card4'])['id_02'].transform('mean')\ntest['id_02_to_std_card1'] = test['id_02'] \/ test.groupby(['card1'])['id_02'].transform('std')\ntest['id_02_to_std_card4'] = test['id_02'] \/ test.groupby(['card4'])['id_02'].transform('std')\n\ntrain['D15_to_mean_card1'] = train['D15'] \/ train.groupby(['card1'])['D15'].transform('mean')\ntrain['D15_to_mean_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('mean')\ntrain['D15_to_std_card1'] = train['D15'] \/ train.groupby(['card1'])['D15'].transform('std')\ntrain['D15_to_std_card4'] = train['D15'] \/ train.groupby(['card4'])['D15'].transform('std')\n\ntest['D15_to_mean_card1'] = test['D15'] \/ test.groupby(['card1'])['D15'].transform('mean')\ntest['D15_to_mean_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('mean')\ntest['D15_to_std_card1'] = test['D15'] \/ test.groupby(['card1'])['D15'].transform('std')\ntest['D15_to_std_card4'] = test['D15'] \/ test.groupby(['card4'])['D15'].transform('std')\n\n\ntrain['dist1_to_mean_card1'] = train['dist1'] \/ train.groupby(['card1'])['dist1'].transform('mean')\ntrain['dist1_to_mean_card4'] = train['dist1'] \/ train.groupby(['card4'])['dist1'].transform('mean')\ntrain['dist1_to_std_card1'] = train['dist1'] \/ train.groupby(['card1'])['dist1'].transform('std')\ntrain['dist1_to_std_card4'] = train['dist1'] \/ train.groupby(['card4'])['dist1'].transform('std')\n\ntest['dist1_to_mean_card1'] = test['dist1'] \/ test.groupby(['card1'])['dist1'].transform('mean')\ntest['dist1_to_mean_card4'] = test['dist1'] \/ test.groupby(['card4'])['dist1'].transform('mean')\ntest['dist1_to_std_card1'] = test['dist1'] \/ test.groupby(['card1'])['dist1'].transform('std')\ntest['dist1_to_std_card4'] = test['dist1'] \/ test.groupby(['card4'])['dist1'].transform('std')\n\n\ntrain['D4_to_mean_card1'] = train['D4'] \/ train.groupby(['card1'])['D4'].transform('mean')\ntrain['D4_to_mean_card4'] = train['D4'] \/ train.groupby(['card4'])['D4'].transform('mean')\ntrain['D4_to_std_card1'] = train['D4'] \/ train.groupby(['card1'])['D4'].transform('std')\ntrain['D4_to_std_card4'] = train['D4'] \/ train.groupby(['card4'])['D4'].transform('std')\n\ntest['D4_to_mean_card1'] = test['D4'] \/ test.groupby(['card1'])['D4'].transform('mean')\ntest['D4_to_mean_card4'] = test['D4'] \/ test.groupby(['card4'])['D4'].transform('mean')\ntest['D4_to_std_card1'] = test['D4'] \/ test.groupby(['card1'])['D4'].transform('std')\ntest['D4_to_std_card4'] = test['D4'] \/ test.groupby(['card4'])['D4'].transform('std')\n\ntrain['card1_count'] = train.groupby(['card1'])['TransactionID'].transform('count')\ntrain['card2_count'] = train.groupby(['card2'])['TransactionID'].transform('count')\ntrain['card4_count'] = train.groupby(['card4'])['TransactionID'].transform('count')\n\ntest['card1_count'] = test.groupby(['card1'])['TransactionID'].transform('count')\ntest['card2_count'] = test.groupby(['card2'])['TransactionID'].transform('count')\ntest['card4_count'] = test.groupby(['card4'])['TransactionID'].transform('count')","d47324af":"many_null_cols = [col for col in train.columns if train[col].isnull().sum() \/ train.shape[0] > 0.9]\nmany_null_cols_test = [col for col in test.columns if test[col].isnull().sum() \/ test.shape[0] > 0.9]","74ce2967":"big_top_value_cols = [col for col in train.columns if train[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]\nbig_top_value_cols_test = [col for col in test.columns if test[col].value_counts(dropna=False, normalize=True).values[0] > 0.9]","f3a35cd2":"one_value_cols = [col for col in train.columns if train[col].nunique() <= 1]\none_value_cols_test = [col for col in test.columns if test[col].nunique() <= 1]\none_value_cols == one_value_cols_test","43d10e3f":"cols_to_drop = list(set(many_null_cols + many_null_cols_test + big_top_value_cols + big_top_value_cols_test + one_value_cols+ one_value_cols_test))\nlen(cols_to_drop)","04df56a6":"cols_to_drop.remove('isFraud')","b13cc686":"train = train.drop(cols_to_drop, axis=1)\ntest = test.drop(cols_to_drop, axis=1)","a1c05d4b":"cat_cols = ['id_12', 'id_13', 'id_14', 'id_15', 'id_16', 'id_17', 'id_18', 'id_19', 'id_20', 'id_21', 'id_22', 'id_23', 'id_24', 'id_25', 'id_26', 'id_27', 'id_28', 'id_29',\n            'id_30', 'id_31', 'id_32', 'id_33', 'id_34', 'id_35', 'id_36', 'id_37', 'id_38', 'DeviceType', 'DeviceInfo', 'ProductCD', 'card4', 'card6', 'M4','P_emaildomain',\n            'R_emaildomain', 'card1', 'card2', 'card3',  'card5', 'addr1', 'addr2', 'M1', 'M2', 'M3', 'M5', 'M6', 'M7', 'M8', 'M9']\nfor col in cat_cols:\n    if col in train.columns:\n        train = train.drop([col], axis=1)\n        test = test.drop([col], axis=1)","c41626d0":"train_features_path = 'train_features_v0.csv'\ntest_features_path = 'test_features_v0.csv'\n\ntrain.to_csv(train_features_path, index=None)\ntest.to_csv(test_features_path, index=None)","ef238d9a":"X = train.sort_values('TransactionDT').drop(['isFraud', 'TransactionDT', 'TransactionID'], axis=1)\ny = train.sort_values('TransactionDT')['isFraud']\nX_test = test.sort_values('TransactionDT').drop(['TransactionDT', 'TransactionID'], axis=1)\n\ntrain = train[[\"TransactionDT\", 'TransactionID']]\ntest = test[[\"TransactionDT\", 'TransactionID']]","27e15ccd":"validation_params = {'seed':1234,\n                     'n_folds':5,\n                     'validation_schema': 'kfold'}\n\nfolds = KFold(n_splits=validation_params['n_folds'], random_state=validation_params['seed'])","2304de57":"model_params = {'num_leaves': 256,\n                  'min_child_samples': 79,\n                  'objective': 'binary',\n                  'max_depth': 15,\n                  'learning_rate': 0.02,\n                  \"boosting_type\": \"gbdt\",\n                  \"subsample_freq\": 3,\n                  \"subsample\": 0.9,\n                  \"bagging_seed\": 11,\n                  \"metric\": 'auc',\n                  \"verbosity\": -1,\n                  'reg_alpha': 0.3,\n                  'reg_lambda': 0.3,\n                  'colsample_bytree': 0.9\n                 }\n\ntraining_params = {'num_boosting_rounds':5000,\n                   'early_stopping_rounds':200\n               }","591b140e":"hyperparameters={**model_params, **training_params,**validation_params}","e5ac62cb":"def fit_predict(X, y, X_test, folds, model_params, training_params):\n    out_of_fold, test_preds = np.zeros(len(X)), np.zeros(len(X_test))\n    for fold_nr, (trn_idx, val_idx) in enumerate(folds.split(X.values, y.values)):\n        print(\"Fold {}\".format(fold_nr))\n\n        X_train, y_train = X.iloc[trn_idx], y.iloc[trn_idx]\n        X_valid, y_valid = X.iloc[val_idx], y.iloc[val_idx]\n\n        trn_data = lgb.Dataset(X_train, y_train)\n        val_data = lgb.Dataset(X_valid, y_valid)\n        \n        # add live monitoring of lightgbm learning curves\n        monitor = neptune_monitor(prefix='fold{}_'.format(fold_nr))\n        clf = lgb.train(model_params, trn_data, \n                        training_params['num_boosting_rounds'], \n                        valid_sets = [trn_data, val_data], \n                        early_stopping_rounds = training_params['early_stopping_rounds'],\n                        callbacks=[monitor])\n        out_of_fold[val_idx] = clf.predict(X.iloc[val_idx], num_iteration=clf.best_iteration)\n        test_preds += clf.predict(X_test, num_iteration=clf.best_iteration) \/ folds.n_splits\n    return out_of_fold, test_preds    \n\ndef fmt_preds(y_pred):\n    return np.concatenate((1.0-y_pred.reshape(-1,1), y_pred.reshape(-1,1)), axis=1)","102e7104":"neptune.init(api_token='eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5tbCIsImFwaV9rZXkiOiJiNzA2YmM4Zi03NmY5LTRjMmUtOTM5ZC00YmEwMzZmOTMyZTQifQ==',\n             project_qualified_name='shared\/showroom')","2223ed9d":"train_predictions_path = 'train_predictions_v0.csv'\ntest_predictions_path = 'test_predictions_v0.csv'\nsubmission_path = 'submission_v0.csv'\n\nwith neptune.create_experiment(name='model training',\n                               params=hyperparameters,\n                               tags=['lgbm', 'features_v0', 'training']):\n    # log data versions\n    log_data_version(train_features_path, prefix='train_features_')\n    log_data_version(test_features_path, prefix='test_features_')\n\n    out_of_fold, test_preds = fit_predict(X, y, X_test, folds, model_params, training_params)\n    \n    valid_auc = roc_auc_score(y, out_of_fold)\n    # log valid auc metric\n    neptune.send_metric('valid_auc', valid_auc)\n    # log diagnostic charts on the validation\n    send_binary_classification_report(y, fmt_preds(out_of_fold), channel_name='valid_classification_report')\n    \n    train = pd.concat([train, pd.DataFrame(out_of_fold, columns=['prediction'])], axis=1)\n    test = pd.concat([test, pd.DataFrame(test_preds, columns=['prediction'])], axis=1)\n    sub['isFraud'] = pd.merge(sub, test, on='TransactionID')['prediction']\n    train.to_csv(train_predictions_path, index=None)\n    test.to_csv(test_predictions_path, index=None)\n    sub.to_csv(submission_path, index=None)\n    # log out of fold predictions and submission\n    neptune.send_artifact(train_predictions_path)\n    neptune.send_artifact(test_predictions_path)\n    neptune.send_artifact(submission_path)","439722ae":"And let's define the `fit_predict` function that will train the model and return predictions.","00579299":"That's it!\n\nIf you want to go beyond single script you can see how I created a starter cookie-cutter data science project [here](https:\/\/ui.neptune.ml\/jakub-czakon\/ieee-fraud-detection\/experiments) with some additional tracking benefits like environment and code.\n\nI would really like to get your feedback on this kernel and experiment management overall so please comment.","6ec1999a":"It is usually beneficial to save extracted features for future reference.\nAdditional bonus is that calculating feature version ( `md5 hash`) is easier.","f3f94387":"# Adding Experiment management \n\nOk. now we have everything ready to set-up experiment management.\n\nLet's start by initializing Neptune with the api token and project name.\nFor now I will use open token and public shared project but if you register you can create private project for you and your team (for free).","76e98709":"# Modeling","7afd1a83":"You can also pass NEPTUNE_PROJECT and NEPTUNE_API_TOKEN as environment variables.\n\nIn notebooks you can do it by running:\n    \n```bash\n% env NEPTUNE_API_TOKEN=eyJhcGlfYWRkcmVzcyI6Imh0dHBzOi8vdWkubmVwdHVuZS5tbCIsImFwaV9rZXkiOiJiNzA2YmM4Zi03NmY5LTRjMmUtOTM5ZC00YmEwMzZmOTMyZTQifQ==\n% env NEPTUNE_PROJECT=shared\/showroom\n```\n\nand then you will be able run simpler command:\n\n```python\nneptune.init()\n```\n\nWhenever you run `neptune.create_experiment` a link to an experiment is created and you can go and see your experiment.\nExample link looks like this https:\/\/ui.neptune.ml\/o\/shared\/org\/showroom\/e\/SHOW-25","c58163d3":"# Feature engineering","58778550":"# Data loading","2ae53b24":"# Experiment management\nThis kernel is a fork of @artgor kernel https:\/\/www.kaggle.com\/artgor\/eda-and-models, thank you so much Andrew!\n\nI figured it would make sense to add experiment management on top of that kernel so that everyone can have a clean experimentation process throughout this competition.\nIn this kernel you will learn how to:\n- track data\/feature versions\n\n![image1](https:\/\/gist.githubusercontent.com\/jakubczakon\/f754769a39ea6b8fa9728ede49b9165c\/raw\/d0c079a7076c2292d38ab78cfa0947bdfc4d35b5\/kaggle_properties.png)\n\n- track hyperparameters\n\n![image2](https:\/\/gist.githubusercontent.com\/jakubczakon\/f754769a39ea6b8fa9728ede49b9165c\/raw\/d0c079a7076c2292d38ab78cfa0947bdfc4d35b5\/kaggle_parameters.png)\n\n- track metrics\n\n![image3](https:\/\/gist.githubusercontent.com\/jakubczakon\/f754769a39ea6b8fa9728ede49b9165c\/raw\/d0c079a7076c2292d38ab78cfa0947bdfc4d35b5\/kaggle_metrics.png)\n\n- track diagnostic charts like confusion matrix, roc auc curve, or prediction distributions \n\n![image4](https:\/\/gist.githubusercontent.com\/jakubczakon\/f754769a39ea6b8fa9728ede49b9165c\/raw\/d0c079a7076c2292d38ab78cfa0947bdfc4d35b5\/kaggle_images.png)\n\n- track prediction artifacts \n\n![image5](https:\/\/gist.githubusercontent.com\/jakubczakon\/f754769a39ea6b8fa9728ede49b9165c\/raw\/d0c079a7076c2292d38ab78cfa0947bdfc4d35b5\/kaggle_artifacts.png)\n\n- add live monitoring for lightgbm model\n\n![image8](https:\/\/gist.githubusercontent.com\/jakubczakon\/f754769a39ea6b8fa9728ede49b9165c\/raw\/d0c079a7076c2292d38ab78cfa0947bdfc4d35b5\/kaggle_charts.png)\n\nAll of that is logged and organized in [Neptune](http:\/\/bit.ly\/2XQYNgl). \nI hope that with this in place it will be easier for you to keep track of your experiments during this competition.\nAlso, I would love to see people collaborate more effectively in a team and if there is.\n\nIf you would like to see all my experiments in my public project [go here](https:\/\/ui.neptune.ml\/jakub-czakon\/ieee-fraud-detection\/experiments). \n\nOk, let's do it!\n\nI figured I'd hide all the pieces that were exactly the same as in the original kernel and focus on the extras.","ca2af8b0":"## Imports","584d96c1":"Let's combine all the parameters in one big hyperparameter dictionary."}}