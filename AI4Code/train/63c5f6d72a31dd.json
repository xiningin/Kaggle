{"cell_type":{"42be7166":"code","8d79dafe":"code","aaf08ec3":"code","f522e9e7":"code","563a8eda":"code","eaa08059":"code","52579287":"code","022c7a74":"code","ba408ce8":"code","837d20a5":"code","2060868b":"markdown","8c7d8934":"markdown","7449ecf4":"markdown","2a5b7284":"markdown","d9e11446":"markdown","630a5214":"markdown","6be06d6f":"markdown","e0699586":"markdown","b474cd40":"markdown","a8e9b2a6":"markdown","ff3f623d":"markdown","27f13566":"markdown"},"source":{"42be7166":"!conda install ignite -c pytorch --yes > \/dev\/null  # Send output to \/dev\/null so we don't have to read all the installation stuff\n\nimport time\nimport torch\nimport datetime\nimport torchvision\nimport torch.nn as nn\n\nfrom collections import deque\nfrom torchvision import models\nfrom torchvision import transforms\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n%matplotlib inline\n\nfrom ignite.engine import Events, create_supervised_trainer, create_supervised_evaluator\nfrom ignite.metrics import Accuracy, Loss, Precision","8d79dafe":"TRAIN_DIR = \"\/kaggle\/input\/pacemakers\/Train\"\nTEST_DIR = \"\/kaggle\/input\/pacemakers\/Test\"\n\nIMG_SIZE = 224\nMEAN = [0.485, 0.456, 0.406]\nSTD = [0.229, 0.224, 0.225]","aaf08ec3":"EPOCHS = 20  # Go through the entire dataset 20 times during training\nBATCH_SIZE = 32  # How many images to show the network at once; lower this if you don't have enough GPU RAM\nDEVICE = \"cuda\"  # We'll use a GPU to speed up training if we can; remember to turn the accelerator to \"GPU\" on the right. If you don't have a GPU, change \"cuda\" to \"cpu\"\nVERBOSE = True  # Print progress of each training loop","f522e9e7":"transforms_train = transforms.Compose([\n    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.9, 1.0), ratio=(1.0, 1.0)),\n    transforms.RandomAffine(degrees=5,\n                            translate=(0.05, 0.05),\n                            scale=(0.95, 1.05),\n                            shear=5),\n    transforms.ColorJitter(.3, .3, .3),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=MEAN, std=STD),\n])\n\ntransforms_test = transforms.Compose([\n    transforms.Resize(IMG_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize(mean=MEAN, std=STD),\n])\n\ntrain_data = torchvision.datasets.ImageFolder(TRAIN_DIR, transform=transforms_train)\ntrain_loader = torch.utils.data.DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, num_workers=0)\n\ntest_data = torchvision.datasets.ImageFolder(TEST_DIR, transform=transforms_test)\ntest_loader = torch.utils.data.DataLoader(test_data, batch_size=BATCH_SIZE, shuffle=False, num_workers=0)\n\nn_classes = len(train_data.classes)","563a8eda":"plt.figure(figsize=(16,10))  # Larger plot size\n\nimg = train_data[0][0].numpy().transpose((1, 2, 0))\nimg = STD * img + MEAN\n\nplt.subplot(2, 2, 1)\nplt.imshow(img)\nplt.axis('off')\nplt.title(\"Training set example\")\n\nimg = test_data[0][0].numpy().transpose((1, 2, 0))\nimg = STD * img + MEAN\n\nplt.subplot(2, 2, 2)\nplt.imshow(img)\nplt.axis('off')\nplt.title(\"Testing set example\")","eaa08059":"model = models.densenet121(pretrained=True)\nmodel.classifier = nn.Linear(model.classifier.in_features, n_classes)\nmodel = model.to(DEVICE)","52579287":"loss = torch.nn.CrossEntropyLoss()\noptimizer = torch.optim.Adam((p for p in model.parameters() if p.requires_grad))\n\ntrainer = create_supervised_trainer(model,\n                                    optimizer,\n                                    loss,\n                                    device=DEVICE)\n\nevaluator = create_supervised_evaluator(model,\n                                        metrics={'accuracy': Accuracy(),\n                                                 'loss': Loss(loss),\n                                                 'precision': Precision()},\n                                        device=DEVICE)","022c7a74":"@trainer.on(Events.STARTED)\ndef initialise_custom_engine_vars(engine):\n    engine.iteration_timings = deque(maxlen=100)\n    engine.iteration_loss = deque(maxlen=100)\n\n@trainer.on(Events.ITERATION_COMPLETED)\ndef log_training_loss(engine):\n    engine.iteration_timings.append(time.time())\n    engine.iteration_loss.append(engine.state.output)\n    seconds_per_iteration = np.mean(np.gradient(engine.iteration_timings)) if len(engine.iteration_timings) > 1 else 0\n    eta = seconds_per_iteration * (len(train_loader)-(engine.state.iteration % len(train_loader)))\n    if VERBOSE:\n        print(f\"\\rEPOCH: {engine.state.epoch:03d} | \"\n              f\"BATCH: {engine.state.iteration % len(train_loader):03d} of {len(train_loader):03d} | \"\n              f\"LOSS: {engine.state.output:.3f} ({np.mean(engine.iteration_loss):.3f}) | \"\n              f\"({seconds_per_iteration:.2f} s\/it; ETA {str(datetime.timedelta(seconds=int(eta)))})\", end='')\n            \n@trainer.on(Events.EPOCH_COMPLETED)\ndef log_training_results(engine):\n    evaluator.run(train_loader)\n    metrics = evaluator.state.metrics\n    acc, loss, precision = metrics['accuracy'], metrics['loss'], metrics['precision'].cpu()\n    print(f\"\\nEnd of epoch {engine.state.epoch:03d}\")\n    print(f\"TRAINING Accuracy: {acc:.3f} | Loss: {loss:.3f}\")\n    \n@trainer.on(Events.EPOCH_COMPLETED)\ndef log_validation_results(engine):\n    evaluator.run(test_loader)\n    metrics = evaluator.state.metrics\n    acc, loss, precision = metrics['accuracy'], metrics['loss'], metrics['precision'].cpu()\n    print(f\"TESTING  Accuracy: {acc:.3f} | Loss: {loss:.3f}\\n\")","ba408ce8":"trainer.run(train_loader, max_epochs=EPOCHS)","837d20a5":"model.eval()\n\nplt.figure(figsize=(20,50))  # Larger plot size\n\nfor i_class in range(n_classes):\n    \n    i_img = i_class * 5  # 5 examples per class\n    img_tensor, _ = test_data[i_img]\n    img_numpy = img_tensor.numpy().transpose((1, 2, 0))\n    img_numpy = STD * img_numpy + MEAN\n    \n    with torch.no_grad():\n        predictions = model(torch.unsqueeze(img_tensor, 0).to(DEVICE))\n        predicted_class = torch.argmax(predictions).cpu().numpy()\n    \n    true_class = test_data.classes[i_class][:20]\n    pred_class = test_data.classes[predicted_class][:20]\n    correct = \"CORRECT\" if true_class == pred_class else \"INCORRECT\"\n    \n    plt.subplot(9, 5, i_class+1)\n    plt.imshow(img_numpy)\n    plt.axis('off')\n    plt.title(f\"{correct}\\nTrue class: {true_class}\\nPredicted class: {pred_class}\")\n    \nplt.subplots_adjust(wspace=0, hspace=1)","2060868b":"# Make some predictions to test our network\nHere we will take an example of each of the classes in the testing dataset and run it through the network.","8c7d8934":"# Now train!","7449ecf4":"# Pacemaker identification with neural networks\nThis is a sample notebook to go with the pacemaker dataset.\nThis is the dataset used by the paper [\"Cardiac Rhythm Device Identification Using Neural Networks\"](http:\/\/electrophysiology.onlinejacc.org\/content\/5\/5\/576).","2a5b7284":"# Create our network\nWe'll use DenseNet121 (Xception used in the original paper isn't in the Pytorch model zoo, sadly, and DenseNet is still very nice).\nBecause this network will have been trained on ImageNet which has 1000 classes, and we are training on our pacemakers which have 45 classes, we need to replace the final layer of the network with a layer with 45 outputs.","d9e11446":"## Settings for training","630a5214":"# Set up some functions to print out our progress\nThese are called 'callbacks', or 'hooks'. Ignite will run these funcstions when certain things happen, e.g. end of an epoch (every cycle, i.e. time the network's been trained a full copy of the dataset), or every iteration (every 'batch' of pictures).","6be06d6f":"# Load the necessary libraries\n\nThe ignite library doesn't come installed by default on kaggle, so let's install it from the pytorch channel.","e0699586":"# Preview our data","b474cd40":"# Set up training scheme\nHere we tell it we want it to calculate its performance using CrossEntropyLoss (because it's a categorical problem).\nWe're going to use the Ignite framework here just to make our training loops a little easier.","a8e9b2a6":"We seem to have achieved a respectable accuracy of over 90% on the testing set at some stages, which is good considering there are 45 classes.\n\nYou may well see accuracies above those reported in our paper when you train this network - one reason for this is because in our paper we did not continuously measure performance against the testing set during training (we used a proportion of the training set to do this), but only once at the end. This the 'correct' practice, because it prevents \"lucky\" runs being reported as the true accuracy.","ff3f623d":"# Settings\n##  Settings for our dataset\nWe use the mean and standard deviation of the ImageNet dataset so our images are of similar distribution to the pre-trained models we'll load.","27f13566":"# Set up our data pipeline\n\nFor training examples we'll use data augmentation to distort the images and make them look a bit different, so the neural network sees more examples, effectively.\nFor the testing set, we won't adulterate them (so we can judge a more 'real world' performance)"}}