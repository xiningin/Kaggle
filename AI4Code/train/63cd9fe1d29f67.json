{"cell_type":{"a9210f1e":"code","7279d056":"code","ab9802b6":"code","3c24d86e":"code","2460eaa3":"code","7e49448a":"code","c37ee4df":"code","af876d6a":"code","3941a6b8":"code","75e9cd87":"code","437c321a":"code","030d9425":"code","a5606f8f":"code","2d863f57":"code","bbb14b83":"code","b05f0372":"code","2f171b7d":"code","0dc76b89":"code","9dcd6779":"code","5beb7fdc":"code","b2c03a26":"code","121fc3f2":"code","ed759d7e":"code","aeecdc07":"code","750dff62":"code","441bad8d":"code","f421f65e":"code","40bf3e65":"code","b038df08":"code","b314dc45":"code","a3049fd5":"code","669cba1a":"code","92f0b0d8":"code","66f4eceb":"code","b8c9357d":"code","f90232cc":"code","cfb1f101":"code","0dcef9a4":"code","c01c4ae9":"code","ea7ee6a4":"code","f7f6f522":"code","26e8eabb":"code","0661bb01":"code","e2307a1d":"code","7d7b226c":"code","1bc4a057":"code","cf616476":"code","5822c525":"code","d007c61c":"code","f30c7181":"code","58db96c8":"code","93642de9":"code","5b3d6a12":"code","53a31731":"code","3a0fd0a0":"code","41c53af9":"code","cd74eede":"code","99095a27":"code","6e0b26a8":"code","019e7f7d":"code","6400e5be":"code","78d13f75":"code","62313813":"code","22b358f7":"code","a215e2ca":"code","2b52c8c8":"code","47b53b0d":"code","34b0e418":"code","6798c6ab":"code","807052d6":"code","c6754a22":"code","32a7a481":"code","6667744f":"code","13b348f9":"code","9b190aee":"code","78ff7dc4":"code","99766542":"code","09086c3d":"code","f4cb7ab6":"code","3d9ab7fd":"code","2070706c":"code","e2e9bf4b":"code","e4e154d6":"code","af3cda9d":"code","07d53670":"code","2cacd842":"code","2c660e17":"code","e72e2dd3":"code","77c8f30d":"code","b106aa0e":"code","9a3947bf":"code","9fd58e1f":"code","cfe77d33":"code","e13c252f":"code","e865ebc7":"code","ca2561b9":"code","722a4472":"code","12f068ee":"code","10f88cd1":"code","bac7ca84":"code","93106876":"code","6eea4ddb":"code","b791dd92":"code","691ca642":"code","2b483ab4":"code","3446b6fb":"code","7f3e6f23":"code","dad5bec3":"code","576638cc":"code","d0852933":"code","05077b44":"code","13acbdf3":"code","5fdc1bf3":"code","6f070241":"code","da2306db":"code","d3fd9b6e":"code","db4114c4":"code","7f1e5dcf":"code","829368e0":"code","9a3a286f":"code","7a0cf092":"code","66b2ef9a":"code","803bf089":"code","fe7d4376":"code","bf9f5522":"code","6f02943f":"code","37b8b1d8":"code","5baecfcf":"code","9b4bff22":"code","43eed43f":"code","f6fb55cb":"code","478712ca":"code","38d0f479":"code","f7365b97":"code","ab0c2a1c":"code","ce1da5ed":"code","6ea97539":"code","c4fac2e1":"code","0969d288":"code","e8e13951":"code","75148590":"code","5809c84e":"code","ff21decd":"code","b99b99b9":"code","8876ab26":"code","d12c906e":"code","db093580":"code","2b8f1877":"code","332075fd":"code","0374b049":"code","ec6a0fc1":"code","efda58db":"code","1e873b54":"code","99defb91":"code","94daa929":"code","47ace7e9":"code","1d912fe5":"markdown","1b90ec30":"markdown","82694c4c":"markdown","63df289f":"markdown"},"source":{"a9210f1e":"import pandas as pd\nimport seaborn as sns\nimport numpy as np","7279d056":"# from bokeh.io import show, output_notebook\n# from bokeh.plotting import figure\n# from bokeh.models import HoverTool, BoxSelectTool #For enabling tools\n\n# %matplotlib","ab9802b6":"\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\nfrom sklearn.metrics import f1_score, accuracy_score\nfrom tqdm import tqdm","3c24d86e":"from skimage.io import imread\nfrom skimage.transform import resize\nimport matplotlib.pyplot as plt\nplt.style.use(\"dark_background\")","2460eaa3":"# def plot_acc(history):\n#     import matplotlib.pyplot as plt\n#     history_dict = history.history\n#     acc_values = history_dict['accuracy'] \n#     val_acc_values = history_dict['val_accuracy']\n#     acc = history_dict['accuracy']\n#     epochs = range(1, len(acc) + 1)\n#     plt.plot(epochs, acc, 'bo', label='Training acc', color='green')\n#     plt.plot(epochs, val_acc_values, 'b', label='Validation acc', color='red')\n#     plt.title('Training and validation accuracy')\n#     plt.xlabel('Epochs')\n#     plt.ylabel('Accuracy')\n#     plt.legend()\n#     plt.show()","7e49448a":"\n\ntrain_data = pd.read_csv(\"..\/input\/images-of-gala\/Images of Gala\/train.csv\")\ntest_data = pd.read_csv(\"..\/input\/images-of-gala\/Images of Gala\/test.csv\")\n\n!mkdir new_train\n!cp -r '..\/input\/images-of-gala\/Images of Gala\/Train Images' new_train ","c37ee4df":"sns.countplot(train_data['Class'])\ntrain_data['Class'].value_counts()","af876d6a":"def percentage_coutn(vc_obj, title):\n    '''\n    This function prints the distribution of classes as percentage\n    [param]vc_obj: This needs to be the value_counts().items() of a pandas column\n    [param]title : Title that is printed like \"Training Set, Testing Set\"\n    \n    [example]:-\n    percentage_coutn(train_data['Class'].value_counts().items(),\"ACTUAL Training Data\")\n    '''\n    le = []\n    total_sum = 0\n    for i in vc_obj:\n        le.append(i)\n        total_sum  = total_sum + i[1]\n    print(title)\n    for i in le:\n        print(i[0], \"-->\", round(i[1]\/total_sum,2),\"%\")\n    print(\"##\"*50)\n    print(\"\\n\")\n        ","3941a6b8":"percentage_coutn(train_data['Class'].value_counts().items(),\"ACTUAL Training Data\")\n","75e9cd87":"kfold = StratifiedKFold(n_splits=6, shuffle=True, random_state=11)\n##Split the data into training and testing\n\nfor train_ix, test_ix in kfold.split(train_data['Image'], train_data['Class']):\n    #print(train_ix, test_ix)\n    #print(type(train_ix))\n    X_train, X_test = train_data['Image'].iloc[train_ix], train_data['Image'].iloc[test_ix]\n    Y_train, Y_test = train_data['Class'].iloc[train_ix], train_data['Class'].iloc[test_ix]\n    #print(Y_train.value_counts())\n    #percentage_coutn(Y_train.value_counts().items(),\"ACTUAL Training Data\")\n    #percentage_coutn(Y_test.value_counts().items(),\"ACTUAL Training Data\")\n    print(len(X_train))\n    print(len(X_test))\n    break","437c321a":"Dataset_Train = pd.concat([X_train, Y_train], axis=1)\n\nDataset_Test = pd.concat([X_test, Y_test], axis=1)\n\npercentage_coutn(Dataset_Train['Class'].value_counts().items(),\"Training Data\")\npercentage_coutn(Dataset_Test['Class'].value_counts().items(),\"Testing Data\")","030d9425":"sns.countplot(Dataset_Train['Class'])","a5606f8f":"sns.countplot(Dataset_Test['Class'])","2d863f57":"#import os\n#import shutil\n#li = []\n#for i in train_data[train_data['Class']=='Decorationandsignage']['Image']:\n    #print(i)\n    \n    #i2 = i.split(\".\")[0]\n    #i2 = i2+\"__2\"\n    #i2 = i2+\".jpg\"\n    #bpath = \".\/new_train\/Train Images\/\"\n    #li.append(i2)\n    #print(f'cp {bpath+i} {bpath+i2}')\n    #os.system(\"cp {} {} -v\", format(i,i2))\n    #shutil.copyfile(f'{bpath+i}',f'{bpath+i2}')\n    #os.system(f'cp {bpath+i} {bpath+i2} -v')","bbb14b83":"#df = pd.DataFrame(list(zip(li, li2)), columns =['Image', 'Class'])","b05f0372":"# new_data = pd.concat([df, train_data])\n# sns.countplot(new_data['Class'])\n# new_data['Class'].value_counts()","2f171b7d":"# from tensorflow.keras.preprocessing.image import ImageDataGenerator\n# train_datagen = ImageDataGenerator(rotation_range=10,\n#                                    width_shift_range=0.25,\n#                                    height_shift_range=0.25,\n#                                    shear_range=0.1,\n#                                    zoom_range=0.25,\n#                                    horizontal_flip=False)\n","0dc76b89":"from fastai import *\nfrom fastai.vision import *\nfrom fastai.callbacks.hooks import *","9dcd6779":"\n# tpath = 'new_train\/Train Images\/'\n# tepath = 'new_train\/Train Images\/'\n\n# #tepath =  '..\/input\/images-of-gala\/Images of Gala\/Test Images'\n# #test_img = ImageList.from_df(test_data, path=tepath)\n\n# # additional_aug=[*zoom_crop(scale=(0.75,1.25), do_rand=False), \n# #                 brightness(change=(0.1,0.1), \n# #                 contrast(scale=0.5)\n\n# #additional_aug = [brightness(change=(0.5,0.5)), ]#contrast(scale=(0.3,0.3))]\n# #additional_aug = [brightness(change=(0.1,0.35)),contrast(scale=(0.3,0.3))]\n\n\n# TEST = Dataset_Test.drop(['Class'], axis=1)\n\n# test_img = ImageList.from_df(TEST, path=tepath, )\n\n# train_img = (ImageList.from_df(Dataset_Train, path=tpath, )\n#         .split_by_rand_pct(0.03)\n#         .label_from_df()\n#         .add_test(test_img)\n#         .transform(get_transforms(flip_vert=False, ), size= 80)\n#         #.transform(get_transforms(flip_vert=False, xtra_tfms=additional_aug), size=100)\n#         .databunch(path=tpath, bs=64, device= torch.device('cuda:0'))\n#         .normalize()\n#        )\n","5beb7fdc":"\ntpath = 'new_train\/Train Images\/'\ntepath = 'new_train\/Train Images\/'\n\n#tepath =  '..\/input\/images-of-gala\/Images of Gala\/Test Images'\n#test_img = ImageList.from_df(test_data, path=tepath)\n#TEST = Dataset_Test.drop(['Class'], axis=1)\n# additional_aug=[*zoom_crop(scale=(0.75,1.25), do_rand=False), \n#                 brightness(change=(0.1,0.1), \n#                 contrast(scale=0.5)\n\n#additional_aug = [brightness(change=(0.5,0.5)), ]#contrast(scale=(0.3,0.3))]\n#additional_aug = [brightness(change=(0.1,0.35)),contrast(scale=(0.3,0.3))]\n\n\nTEST = Dataset_Test.drop(['Class'], axis=1)\n\ntest_img = ImageList.from_df(TEST, path=tepath, )\n\n\n","b2c03a26":"np.random.seed(42)\n\ntrain_img_80 = (ImageList.from_df(Dataset_Train, path=tpath, )\n        .split_by_rand_pct(0.05, seed=1995)\n        .label_from_df()\n        .add_test(test_img)\n        .transform(get_transforms(flip_vert=False, ), size= 80)\n        #.transform(get_transforms(flip_vert=False, xtra_tfms=additional_aug), size=100)\n        .databunch(path=tpath, bs=128, device= torch.device('cuda:0'))\n        .normalize()\n       )\n\n\ntfms = [contrast(scale=(0.9,0.9)), brightness(change=(0.5,0.5))]\n\n\ntrain_img_160 = (ImageList.from_df(Dataset_Train, path=tpath, )\n        .split_by_rand_pct(0.05, seed=1995)\n        .label_from_df()\n        .add_test(test_img)\n        #.transform(get_transforms(flip_vert=False, ), size= 160)\n        .transform(get_transforms(flip_vert=False, xtra_tfms=tfms), size=160)\n        .databunch(path=tpath, bs=128, device= torch.device('cuda:0'))\n        .normalize()\n       )\n\ntrain_img_224 = (ImageList.from_df(Dataset_Train, path=tpath, )\n        .split_by_rand_pct(0.05, seed=1995)\n        .label_from_df()\n        .add_test(test_img)\n        #.transform(get_transforms(flip_vert=False, ), size= 224)\n        .transform(get_transforms(flip_vert=False, xtra_tfms=tfms), size=224)\n        .databunch(path=tpath, bs=100, device= torch.device('cuda:0'))\n        .normalize(imagenet_stats)\n       )","121fc3f2":"percentage_coutn(train_img_160.train_ds.inner_df['Class'].value_counts().items(),\"Fast-AI Training\")\npercentage_coutn(train_img_160.valid_ds.inner_df['Class'].value_counts().items(),\"Fast-AI Validation\")","ed759d7e":"Dataset_Train.head(4)","aeecdc07":"for i in Dataset_Train.head(2)['Image']:\n\n    img = imread('..\/input\/images-of-gala\/Images of Gala\/Train Images\/'+i)\n#print(img.shape)\n#plt.imshow(img)\n#plt.show()\n\n    reszied = resize(img,(224,224,3))\n#print(reszied.shape)\n    plt.imshow(reszied)\n    plt.show()","750dff62":"for i in Dataset_Train.head(2)['Image']:\n    def get_ex(): return open_image('new_train\/Train Images\/'+i)\n    \n    def plots_f(rows, cols, width, height, **kwargs):\n        [get_ex().apply_tfms(tfms[0], **kwargs).show(ax=ax) for i,ax in enumerate(plt.subplots(\n            rows,cols,figsize=(width,height))[1].flatten())]\n    #tfms = [ brightness(change=(0.5,0.5))]#contrast(scale=(0.9,0.9)),\n    #tfms = get_transforms(max_rotate=180) \n#     tfms = [crop(size = 224, row_pct=1.0, col_pct= 1.0), \n#            get_transforms(max_rotate=180),\n#            contrast(scale=(0.9,0.9))]\n    \n    #tfms = [perspective_warp(magnitude=(-0.2,0.2))]\n    #tfms = [symmetric_warp(magnitude=(-0.2,0.2))] #padding_mode='zeros')\n    #tfms = [tilt( direction = 2, magnitude = (0.4, 0.4))]\n    tfms = [zoom(scale= 1.0)]\n    plots_f(2, 2, 12, 6, size = 224)\n#plots_f_1(1, 1, 12, 6, size=224)\n\n#get_ex()","441bad8d":"#train_img.show_batch(4, 4)","f421f65e":"\n\nimport math\nimport torch\nfrom torch.optim.optimizer import Optimizer, required\n\nclass RAdam(Optimizer):\n\n    def __init__(self, params, lr=1e-3, betas=(0.9, 0.999), eps=1e-8, weight_decay=0, degenerated_to_sgd=True):\n        if not 0.0 <= lr:\n            raise ValueError(\"Invalid learning rate: {}\".format(lr))\n        if not 0.0 <= eps:\n            raise ValueError(\"Invalid epsilon value: {}\".format(eps))\n        if not 0.0 <= betas[0] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 0: {}\".format(betas[0]))\n        if not 0.0 <= betas[1] < 1.0:\n            raise ValueError(\"Invalid beta parameter at index 1: {}\".format(betas[1]))\n        \n        self.degenerated_to_sgd = degenerated_to_sgd\n        if isinstance(params, (list, tuple)) and len(params) > 0 and isinstance(params[0], dict):\n            for param in params:\n                if 'betas' in param and (param['betas'][0] != betas[0] or param['betas'][1] != betas[1]):\n                    param['buffer'] = [[None, None, None] for _ in range(10)]\n        defaults = dict(lr=lr, betas=betas, eps=eps, weight_decay=weight_decay, buffer=[[None, None, None] for _ in range(10)])\n        super(RAdam, self).__init__(params, defaults)\n\n    def __setstate__(self, state):\n        super(RAdam, self).__setstate__(state)\n\n    def step(self, closure=None):\n\n        loss = None\n        if closure is not None:\n            loss = closure()\n\n        for group in self.param_groups:\n\n            for p in group['params']:\n                if p.grad is None:\n                    continue\n                grad = p.grad.data.float()\n                if grad.is_sparse:\n                    raise RuntimeError('RAdam does not support sparse gradients')\n\n                p_data_fp32 = p.data.float()\n\n                state = self.state[p]\n\n                if len(state) == 0:\n                    state['step'] = 0\n                    state['exp_avg'] = torch.zeros_like(p_data_fp32)\n                    state['exp_avg_sq'] = torch.zeros_like(p_data_fp32)\n                else:\n                    state['exp_avg'] = state['exp_avg'].type_as(p_data_fp32)\n                    state['exp_avg_sq'] = state['exp_avg_sq'].type_as(p_data_fp32)\n\n                exp_avg, exp_avg_sq = state['exp_avg'], state['exp_avg_sq']\n                beta1, beta2 = group['betas']\n\n                exp_avg_sq.mul_(beta2).addcmul_(1 - beta2, grad, grad)\n                exp_avg.mul_(beta1).add_(1 - beta1, grad)\n\n                state['step'] += 1\n                buffered = group['buffer'][int(state['step'] % 10)]\n                if state['step'] == buffered[0]:\n                    N_sma, step_size = buffered[1], buffered[2]\n                else:\n                    buffered[0] = state['step']\n                    beta2_t = beta2 ** state['step']\n                    N_sma_max = 2 \/ (1 - beta2) - 1\n                    N_sma = N_sma_max - 2 * state['step'] * beta2_t \/ (1 - beta2_t)\n                    buffered[1] = N_sma\n\n                    # more conservative since it's an approximated value\n                    if N_sma >= 5:\n                        step_size = math.sqrt((1 - beta2_t) * (N_sma - 4) \/ (N_sma_max - 4) * (N_sma - 2) \/ N_sma * N_sma_max \/ (N_sma_max - 2)) \/ (1 - beta1 ** state['step'])\n                    elif self.degenerated_to_sgd:\n                        step_size = 1.0 \/ (1 - beta1 ** state['step'])\n                    else:\n                        step_size = -1\n                    buffered[2] = step_size\n\n                # more conservative since it's an approximated value\n                if N_sma >= 5:\n                    if group['weight_decay'] != 0:\n                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n                    denom = exp_avg_sq.sqrt().add_(group['eps'])\n                    p_data_fp32.addcdiv_(-step_size * group['lr'], exp_avg, denom)\n                    p.data.copy_(p_data_fp32)\n                elif step_size > 0:\n                    if group['weight_decay'] != 0:\n                        p_data_fp32.add_(-group['weight_decay'] * group['lr'], p_data_fp32)\n                    p_data_fp32.add_(-step_size * group['lr'], exp_avg)\n                    p.data.copy_(p_data_fp32)\n\n        return loss","40bf3e65":"def get_f1score(learn):\n    '''\n    prints the f1score of a fastai learn object.\n    to use this meathod the cnn_learner needs to be initialized with metrics \n    Percision(), Recall() and in the following order\n    metrics=[error_rate, accuracy, Precision(), Recall()]\n    '''\n    j=1\n    for i in learn.recorder.metrics:\n    #print(i)\n        fn = (i[2] * i[3]) \/ (i[2] + i[3])\n        print(\"Precision for batch -->\",j,\"is\", (2*fn))\n        j = j +1","b038df08":"def get_test_f1(learn):\n\n     preds,y= learn.get_preds(ds_type=DatasetType.Test)\n#train_img.classes\n\n     num_cat = { 0:'Attire',  1:'Decorationandsignage',  2:'Food',  3:'misc'}\n#sub = Dataset_Test\n\n     sub = pd.DataFrame({'Class': np.argmax(preds,axis=-1)})\n#sub['Class'] = np.argmax(preds,axis=-1)\n     sub['Class'].replace(num_cat, inplace=True)\n\n     print(\"F1 score on Test is      \", f1_score(Dataset_Test['Class'], sub['Class'], average='macro'))\n\n     print(\"Accuracy score on Test is\", accuracy_score(Dataset_Test['Class'], sub['Class'], ))","b314dc45":"\n#learn2 = cnn_learner(train_img, models.resnet101, metrics=[error_rate, accuracy],model_dir=\"\/tmp\/model\/\") \n#learn2 = cnn_learner(train_img, models.resnet50, metrics=[error_rate, accuracy,],model_dir=\"\/tmp\/model\/\", )\n                     #loss_func=torch.nn.CrossEntropyLoss())\n \n                     #, opt_func=optimizer.RAdam(params, lr = 1e-5)) \n#learn2 = cnn_learner(train_img, models.resnet18, metrics=[error_rate, accuracy],model_dir=\"\/tmp\/model\/\") ","a3049fd5":"L_Res34_80 = cnn_learner(train_img_80, models.resnet34, metrics=[error_rate, accuracy, Precision(), Recall()],model_dir=\"\/tmp\/model\/\",\n                    )\n\nL_Res34_160 = cnn_learner(train_img_160, models.resnet34, metrics=[error_rate, accuracy, Precision(), Recall()],model_dir=\"\/tmp\/model\/\",\n                    )\n\nL_Res34_224 = cnn_learner(train_img_224, models.resnet34, metrics=[error_rate, accuracy, Precision(), Recall()],model_dir=\"\/tmp\/model\/\",\n                    )","669cba1a":"L_Res34_80.fit_one_cycle(2,)\nL_Res34_80.recorder.plot_losses()","92f0b0d8":"L_Res34_80.unfreeze()\nL_Res34_80.lr_find()\n#learn2.recorder.plot()\nL_Res34_80.recorder.plot(suggestion=True)\n#learn2.recorder.min_grad_lr","66f4eceb":"learn2.recorder.plot_lr(show_moms=True)","b8c9357d":"#lr = 2.75e-6\nL_Res34_80.unfreeze()\nL_Res34_80.fit_one_cycle(5, 1e-6 )\nL_Res34_80.recorder.plot_losses()\n#learn2.unfreeze()\n#learn2.fit_one_cycle(30, 2.75e-9)","f90232cc":"get_f1score(L_Res34_80)","cfb1f101":"get_test_f1(L_Res34_80)","0dcef9a4":"L_Res34_160.freeze()\nL_Res34_160.fit_one_cycle(2,)","c01c4ae9":"L_Res34_160.unfreeze()\nL_Res34_160.lr_find()\n#learn2.recorder.plot()\nL_Res34_160.recorder.plot(suggestion=True)\n#learn2.recorder.min_grad_lr","ea7ee6a4":"L_Res34_160.unfreeze()\nL_Res34_160.fit_one_cycle(5, 1e-7 )\nL_Res34_160.recorder.plot_losses()","f7f6f522":"L_Res34_160.save(\"34-160-1\")\nL_Res34_160.load(\"34-160-1\")","26e8eabb":"L_Res34_160.unfreeze()\nL_Res34_160.fit_one_cycle(5, 1e-6 )\nL_Res34_160.recorder.plot_losses()","0661bb01":"get_f1score(L_Res34_160)","e2307a1d":"get_test_f1(L_Res34_160)","7d7b226c":"L_Res34_224.freeze()\nL_Res34_224.fit_one_cycle(2, )","1bc4a057":"L_Res34_224.unfreeze()\nL_Res34_224.lr_find()\n#learn2.recorder.plot()\nL_Res34_224.recorder.plot(suggestion=True)","cf616476":"L_Res34_224.unfreeze()\nL_Res34_224.fit_one_cycle(5, 6.31e-7 )\nL_Res34_224.recorder.plot_losses()","5822c525":"get_f1score(L_Res34_224)\nget_test_f1(L_Res34_224)","d007c61c":"L_Res50_80 = cnn_learner(train_img_80, models.resnet50, metrics=[error_rate, accuracy, Precision(), Recall()],model_dir=\"\/tmp\/model\/\",\n                    )\n\nL_Res50_160 = cnn_learner(train_img_160, models.resnet50, metrics=[error_rate, accuracy, Precision(), Recall()],model_dir=\"\/tmp\/model\/\",\n                    )\n\nL_Res50_224 = cnn_learner(train_img_224, models.resnet50, metrics=[error_rate, accuracy, Precision(), Recall()],model_dir=\"\/tmp\/model\/\",\n                    )","f30c7181":"L_Res50_80.freeze()\nL_Res50_80.fit_one_cycle(2,)","58db96c8":"L_Res50_80.unfreeze()\nL_Res50_80.lr_find()\n#learn2.recorder.plot()\nL_Res50_80.recorder.plot(suggestion=True)\n#learn2.recorder.min_grad_lr","93642de9":"L_Res50_80.unfreeze()\nL_Res50_80.fit_one_cycle(5, 1e-6)\nL_Res50_80.recorder.plot_losses()","5b3d6a12":"get_f1score(L_Res50_80)\nget_test_f1(L_Res50_80)","53a31731":"L_Res50_160.freeze()\nL_Res50_160.fit_one_cycle(2,)","3a0fd0a0":"L_Res50_160.unfreeze()\nL_Res50_160.lr_find()\n#learn2.recorder.plot()\nL_Res50_160.recorder.plot(suggestion=True)","41c53af9":"L_Res50_160.unfreeze()\nL_Res50_160.fit_one_cycle(5, 1.10e-7)\nL_Res50_160.recorder.plot_losses()","cd74eede":"L_Res50_160.save('Stage-1')\n#L_Res50_160.load('Stage-1')","99095a27":"L_Res50_160.unfreeze()\nL_Res50_160.fit_one_cycle(5, 1.10e-5)\nL_Res50_160.recorder.plot_losses()","6e0b26a8":"get_f1score(L_Res50_160)","019e7f7d":"get_test_f1(L_Res50_160)","6400e5be":"L_Res50_160.summary()","78d13f75":"# L_Res34_160_2 = cnn_learner(train_img_160, models.resnet34, metrics=[error_rate, accuracy, Precision(), Recall()],model_dir=\"\/tmp\/model\/\",\n#                     )\n","62313813":"L_Res50_224 = cnn_learner(train_img_224, models.resnet50, metrics=[error_rate, accuracy, Precision(), Recall()],model_dir=\"\/tmp\/model\/\",\n                    )","22b358f7":"L_Res50_224.load('Stage-1')","a215e2ca":"L_Res50_224.freeze()\nL_Res50_224.fit_one_cycle(2,)","2b52c8c8":"L_Res50_224.unfreeze()\nL_Res50_224.lr_find()\n#learn2.recorder.plot()\nL_Res50_224.recorder.plot(suggestion=True)","47b53b0d":"L_Res50_224.freeze()\nL_Res50_224.fit_one_cycle(5,1e-6)\nL_Res50_224.recorder.plot_losses()","34b0e418":"L_Res50_224.save('Stage-2')\nL_Res50_224.load('Stage-2')","6798c6ab":"L_Res50_224.freeze()\nL_Res50_224.fit_one_cycle(5,1e-5)\nL_Res50_224.recorder.plot_losses()","807052d6":"get_f1score(L_Res50_224)\nget_test_f1(L_Res50_224)","c6754a22":"tepath =  '..\/input\/images-of-gala\/Images of Gala\/Test Images'\ntest_img = ImageList.from_df(test_data, path=tepath)","32a7a481":"train_img_80.add_test(test_img)\ntrain_img_160.add_test(test_img)\ntrain_img_224.add_test(test_img)","6667744f":"def sub_df(learn, subno):\n    from IPython.display import FileLink\n    preds, y = learn.get_preds(DatasetType.Test)\n# print(learn2.data.c2i)\n    num_cat = { 0:'Attire',  1:'Decorationandsignage',  2:'Food',  3:'misc'}\n    sub = test_data\n\n    sub['Class'] = np.argmax(preds,axis=-1)\n    sub['Class'].replace(num_cat, inplace=True)\n    \n    pd.DataFrame(sub).to_csv(\".\/\"+subno+\".csv\", index=False)\n    FileLink(\".\/\"+subno+\".csv\")\n    return sub","13b348f9":"Sub_L_Res34_80 = sub_df(L_Res34_80, \"L_Res34_80\")","9b190aee":"Sub_L_Res34_160 = sub_df(L_Res34_160, \"L_Res34_160\")\nSub_L_Res34_224 = sub_df(L_Res34_224, \"L_Res34_224\")\n\nSub_L_Res50_80 = sub_df(L_Res50_80, \"L_Res50_80\")\nSub_L_Res50_160 = sub_df(L_Res50_160, \"L_Res50_160\")\nSub_L_Res50_224 = sub_df(L_Res50_224, \"L_Res50_224\")","78ff7dc4":"sublist = [Sub_L_Res34_80, Sub_L_Res34_160, Sub_L_Res34_224, \n          Sub_L_Res50_80, Sub_L_Res50_160, Sub_L_Res50_224]","99766542":"l = [[], [], [], [], [], []]","09086c3d":"for i,j in enumerate(sublist):\n    #print(i)\n    l[i].append(j['Class'].values)","f4cb7ab6":"len(l[1][0])","3d9ab7fd":"import numpy as np\nfrom scipy import stats\n\na = np.array([#l[0][0],\n              l[1][0],\n              #l[2][0],\n              l[3][0],\n              #l[4][0],\n              l[5][0],\n             \n             ])\n\nm = stats.mode(a)\n#print(m)","2070706c":"m[0][0]","e2e9bf4b":"nsub = test_data","e4e154d6":"nsub['Class'] = m[0][0]","af3cda9d":"#     pd.DataFrame(sub).to_csv(\".\/\"+subno+\".csv\", index=False)\n#     FileLink(\".\/\"+subno+\".csv\")\n#     return sub\nfrom IPython.display import FileLink\npd.DataFrame(nsub).to_csv(\".\/nsub2.csv\", index=False)\nFileLink(\".\/nsub2.csv\")","07d53670":"# learn2.show_results()\n# learn2.show_results(DatasetType.Train)","2cacd842":"\ntpath = 'new_train\/Train Images\/'\ntepath = 'new_train\/Train Images\/'\n\n#tepath =  '..\/input\/images-of-gala\/Images of Gala\/Test Images'\n#test_img = ImageList.from_df(test_data, path=tepath)\n\n# additional_aug=[*zoom_crop(scale=(0.75,1.25), do_rand=False), \n#                 brightness(change=(0.1,0.1), \n#                 contrast(scale=0.5)\n\n#additional_aug = [brightness(change=(0.5,0.5)), ]#contrast(scale=(0.3,0.3))]\n#additional_aug = [brightness(change=(0.1,0.35)),contrast(scale=(0.3,0.3))]\n\n\nTEST = Dataset_Test.drop(['Class'], axis=1)\n\ntest_img_2 = ImageList.from_df(TEST, path=tepath, )\ntfms = [contrast(scale=(0.9,0.9)), brightness(change=(0.5,0.5))]\n\ntrain_img_2 = (ImageList.from_df(Dataset_Train, path=tpath, )\n        .split_by_rand_pct(0.02)\n        .label_from_df()\n        .add_test(test_img_2)\n        #.transform(get_transforms(flip_vert=False, ), size= 224)\n        .transform(get_transforms(flip_vert=False, xtra_tfms=tfms), size=160)\n        .databunch(path=tpath, bs=100, device= torch.device('cuda:0'))\n        .normalize()\n        #.normalize(imagenet_stats)\n       )\n","2c660e17":"learn3 = cnn_learner(train_img_2, models.resnet101, metrics=[error_rate, accuracy, Precision(), Recall()],model_dir=\"\/tmp\/model\/\",\n                    )# opt_func=RAdam)","e72e2dd3":"learn3.freeze()\nlearn3.fit_one_cycle(2,)","77c8f30d":"get_f1score(learn3)\nget_test_f1(learn3)","b106aa0e":"# #2 * (precision * recall) \/ (precision + recall)\n# for i in learn3.recorder.metrics:\n#     #print(i)\n#     fn = (i[2] * i[3]) \/ (i[2] + i[3])\n#     print(\"Precision is\", (2*fn))","9a3947bf":"learn3.unfreeze()\nlearn3.lr_find()\n#learn2.recorder.plot()\nlearn3.recorder.plot(suggestion=True)\n#learn2.recorder.min_grad_lr","9fd58e1f":"learn3.fit_one_cycle(5, slice(9.12e-5, 1e-5) )\nlearn3.recorder.plot_losses()","cfe77d33":"get_f1score(learn3)","e13c252f":"get_test_f1(learn3)","e865ebc7":"learn3.save(\"Naruto-1\")\nlearn3.load(\"Naruto-1\")","ca2561b9":"actual_tepath =  '..\/input\/images-of-gala\/Images of Gala\/Test Images'\nactual_test_img = ImageList.from_df(test_data, path=actual_tepath)\ntrain_img_2.add_test(actual_test_img)\n\nres150 = sub_df(learn3, 'l3')\n\nfrom IPython.display import FileLink\npd.DataFrame(res150).to_csv(\".\/nsub5.csv\", index=False)\nFileLink(\".\/nsub5.csv\")","722a4472":"test_img","12f068ee":"tpath = 'new_train\/Train Images\/'\nTEST = Dataset_Test.drop(['Class'], axis=1)\ntest_img_v = ImageList.from_df(TEST, path=tpath, )\ntrain_img_2.add_test(test_img_v)\n","10f88cd1":"learn3.fit_one_cycle(5, slice(9.12e-7, 1e-6) )\nlearn3.recorder.plot_losses()","bac7ca84":"#get_f1score(learn3)\nget_test_f1(learn3)","93106876":"actual_tepath =  '..\/input\/images-of-gala\/Images of Gala\/Test Images'\nactual_test_img = ImageList.from_df(test_data, path=actual_tepath)\ntrain_img_2.add_test(actual_test_img)\n\nres150_2 = sub_df(learn3, 'l3')\n\nfrom IPython.display import FileLink\npd.DataFrame(res150_2).to_csv(\".\/nsub4.csv\", index=False)\nFileLink(\".\/nsub4.csv\")","6eea4ddb":"learn3.save(\"Matrix-1\")","b791dd92":"tpath = 'new_train\/Train Images\/'\ntepath = 'new_train\/Train Images\/'\n\n#tepath =  '..\/input\/images-of-gala\/Images of Gala\/Test Images'\n#test_img = ImageList.from_df(test_data, path=tepath)\n\n# additional_aug=[*zoom_crop(scale=(0.75,1.25), do_rand=False), \n#                 brightness(change=(0.1,0.1), \n#                 contrast(scale=0.5)\n\n#additional_aug = [brightness(change=(0.5,0.5)), ]#contrast(scale=(0.3,0.3))]\n#additional_aug = [brightness(change=(0.1,0.35)),contrast(scale=(0.3,0.3))]\n\n\nTEST = Dataset_Test.drop(['Class'], axis=1)\n\ntest_img_2 = ImageList.from_df(TEST, path=tepath, )\ntfms = [contrast(scale=(0.9,0.9)), brightness(change=(0.5,0.5))]\n\ntrain_img_3 = (ImageList.from_df(Dataset_Train, path=tpath, )\n        .split_by_rand_pct(0.05, seed=1995)\n        .label_from_df()\n        .add_test(test_img_2)\n        #.transform(get_transforms(flip_vert=False, ), size= 224)\n        .transform(get_transforms(flip_vert=False, xtra_tfms=tfms), size = 160)\n        .databunch(path=tpath, bs= 64, device= torch.device('cuda:0'))\n        #.normalize(imagenet_stats)\n        .normalize(imagenet_stats)\n       )","691ca642":"learn4 = cnn_learner(train_img_3, models.resnet152, metrics=[error_rate, accuracy, Precision(), Recall()],model_dir=\"\/tmp\/model\/\",\n                    )","2b483ab4":"# from numba import cuda\n# cuda.select_device(0)\n# cuda.close()","3446b6fb":"learn4.freeze()\nlearn4.fit_one_cycle(2, )","7f3e6f23":"get_f1score(learn4)\nget_test_f1(learn4)","dad5bec3":"learn4.unfreeze()\nlearn4.lr_find()\n#learn2.recorder.plot()\nlearn4.recorder.plot(suggestion=True)","576638cc":"#learn4.fit_one_cycle(5, slice(9.12e-7, 1e-6) )\nlearn4.unfreeze()\nlearn4.fit_one_cycle(10, 1e-7)\nlearn4.recorder.plot_losses()","d0852933":"learn4.save('Stage-1')","05077b44":"# {'Food':1,\n#            'misc':2,\n#            'Attire':3,\n#            'Decorationandsignage':4,\n    \n# }\n\n# {1:0.65660667,\n#                 2:1.17682927,\n#                 3:0.88453578,\n#                 4:2.01312248,\n# }","13acbdf3":"get_f1score(learn4)\nget_test_f1(learn4)","5fdc1bf3":"from sklearn.utils import class_weight\n","6f070241":"tepath = 'new_train\/Train Images\/'\nTEST = Dataset_Test.drop(['Class'], axis=1)\n\ntest_img_2 = ImageList.from_df(TEST, path=tepath, )\ntpath = 'new_train\/Train Images\/'\n\ntfms = [rand_zoom(scale=(1.,1.5)),rand_crop() ]\ntrain_img_4 = (ImageList.from_df(Dataset_Train, path=tpath, )\n        .split_by_rand_pct(0.05, seed=1995)\n        .label_from_df()\n        .add_test(test_img_2)\n        #.transform(get_transforms(flip_vert=False, ), size= 160)\n        .transform(get_transforms(flip_vert=False, xtra_tfms=tfms), size = 224)\n        .databunch(path=tpath, bs= 64, device= torch.device('cuda:0'))\n        #.normalize(imagenet_stats)\n        .normalize(imagenet_stats)\n       )","da2306db":"weights = [0.88453578, 2.01312248, 0.65660667, 1.17682927  ]\nclass_weights = torch.FloatTensor(weights).cuda()\nC = torch.nn.CrossEntropyLoss(weight=class_weights)","d3fd9b6e":"learn5 = cnn_learner(train_img_4, models.resnet152, metrics=[error_rate, accuracy, Precision(), Recall()],model_dir=\"\/tmp\/model\/\",\n                    ) # loss_func= C)","db4114c4":"learn5.mixup()","7f1e5dcf":"learn5.freeze()\nlearn5.fit_one_cycle(2, )\nlearn5.recorder.plot_losses()","829368e0":"learn5.unfreeze()\nlearn5.fit_one_cycle(5, 1e-5)\nlearn5.recorder.plot_losses()","9a3a286f":"learn5.save(\"Stage-1\")\nlearn5.load(\"Stage-1\")","7a0cf092":"learn5.fit_one_cycle(5, 1e-5)\nlearn5.recorder.plot_losses()","66b2ef9a":"learn5.save(\"Stage-2\")\nlearn5.load(\"Stage-2\")","803bf089":"learn5.fit_one_cycle(15, 1e-5)\nlearn5.recorder.plot_losses()","fe7d4376":"learn5.save(\"Stage-3\")\nlearn5.load(\"Stage-3\")","bf9f5522":"learn5.fit_one_cycle(2, 1e-5)\nlearn5.recorder.plot_losses()","6f02943f":"get_f1score(learn5)","37b8b1d8":"get_test_f1(learn5)","5baecfcf":"#loss_func","9b4bff22":"\n\n# tpath = 'new_train\/Train Images\/'\n\n# train_img_5 = (ImageList.from_df(train_data, path=tpath, )\n#         .split_by_rand_pct(0.05, seed=1995)\n#         .label_from_df()\n#         .add_test(test_img_2)\n#         .transform(get_transforms(flip_vert=False, ), size= 160)\n#         #.transform(get_transforms(flip_vert=False, xtra_tfms=tfms), size = 224)\n#         .databunch(path=tpath, bs= 32, device= torch.device('cuda:0'))\n#         #.normalize(imagenet_stats)\n#         .normalize(imagenet_stats)\n#        )\ntepath = 'new_train\/Train Images\/'\nTEST = Dataset_Test.drop(['Class'], axis=1)\n\ntest_img_2 = ImageList.from_df(TEST, path=tepath, )\n\ntpath = 'new_train\/Train Images\/'\n\ntrain_img_5 = (ImageList.from_df(Dataset_Train, path=tpath, )\n        .split_by_rand_pct(0.05, seed=1995)\n        .label_from_df()\n        .add_test(test_img_2)\n        .transform(get_transforms(flip_vert=False, ), size= 160)\n        #.transform(get_transforms(flip_vert=False, xtra_tfms=tfms), size = 224)\n        .databunch(path=tpath, bs= 32, device= torch.device('cuda:0'))\n        #.normalize(imagenet_stats)\n        .normalize()\n       )","43eed43f":"learn6 = cnn_learner(train_img_5, models.resnext50_32x4d, metrics=[error_rate, accuracy, Precision(), Recall()],model_dir=\"\/tmp\/model\/\",\n                    )","f6fb55cb":"learn6.freeze()\nlearn6.fit_one_cycle(2)","478712ca":"learn6.unfreeze()\nlearn6.lr_find()\n#learn2.recorder.plot()\nlearn6.recorder.plot(suggestion=True)","38d0f479":"learn6.unfreeze()\nlearn6.fit_one_cycle( 4, 1.91E-05)\nlearn6.recorder.plot_losses()","f7365b97":"get_f1score(learn6)\nget_test_f1(learn6)","ab0c2a1c":"actual_tepath =  '..\/input\/images-of-gala\/Images of Gala\/Test Images'\nactual_test_img = ImageList.from_df(test_data, path=actual_tepath)\ntrain_img_5.add_test(actual_test_img)\n\ndesnet = sub_df(learn6, 'l3')\n\nfrom IPython.display import FileLink\npd.DataFrame(desnet).to_csv(\".\/nsub7.csv\", index=False)\nFileLink(\".\/nsub7.csv\")","ce1da5ed":"# sub_test_path =  '..\/input\/images-of-gala\/Images of Gala\/Test Images'\n# sub_test_img = ImageList.from_df(test_data, path=sub_test_path, )\n\n# train_img.add_test(sub_test_img)","6ea97539":"# preds, y = learn2.get_preds(DatasetType.Test)\n# print(learn2.data.c2i)\n# num_cat = { 0:'Attire',  1:'Decorationandsignage',  2:'Food',  3:'misc'}\n# sub = test_data\n\n# sub['Class'] = np.argmax(preds,axis=-1)\n# sub['Class'].replace(num_cat, inplace=True)\n# pd.DataFrame(sub).to_csv(\".\/sub26.csv\", index=False)\n# from IPython.display import FileLink\n# FileLink('.\/sub26.csv')","c4fac2e1":"interp = ClassificationInterpretation.from_learner(learn3)\ninterp.plot_confusion_matrix()\ninterp.most_confused()\ninterp.plot_top_losses(9,)\n# interp = ClassificationInterpretation.from_learner(learn2)\n# losses,idxs = interp.top_losses(10)\n# for p in train_img.valid_ds.x.items[idxs]:\n#     print(p)\n# for p in train_img.train_ds.x.items[idxs]:\n#     print(p)\n# losses,idxs = interp.top_losses()\n# top_train_losses = train_img.train_ds.x[idxs]\n# top_train_losses\n# top_valid_losses\n\n# from fastai.widgets import *\n# ds, idxs = DatasetFormatter().from_toplosses(learn2, n_imgs=100,  )\n\n# fd = ImageCleaner(ds, idxs,tpath )\n\n","0969d288":"##This returns the path of the image which has the hightest loss\n\n# losses,idxs = interp.top_losses(10)\n# #for p in train_img_2.valid_ds.x[idxs]:\n# for p in idxs:\n#     print(train_img_2.valid_ds.x.items[p])","e8e13951":"# from fastai.widgets import *\n# ds, idxs = DatasetFormatter().from_toplosses(learn3, n_imgs=6, ds_type=DatasetType.Train )\n\n# fd = ImageCleaner(ds, idxs,tpath )","75148590":"#Actual path of the image where the image lies\n# for p in idxs:\n#     print(train_img_2.train_ds.x.items[p])\n#     print(open_image(train_img_2.train_ds.x.items[p]))","5809c84e":"# preds, y = learn3.get_preds(DatasetType.Test)\n# test_idxs = np.where(np.logical_and(preds>=0.4, preds<=0.6))[0]\n# for i in test_idxs:\n#     print(test_data.loc[i]['Image'])#, valid_data.loc[i]['Class'])\n    #print(i)\n#(np.abs(preds-0.5))","ff21decd":"# #import keras\n# from keras.models import Sequential\n# from keras.layers import Dense, Dropout, Flatten\n# from keras.layers import Conv2D, MaxPooling2D\n# from keras.utils import to_categorical\n# from keras.preprocessing import image\n# from keras.utils import to_categorical\n# from keras.preprocessing.image import ImageDataGenerator\n\n# from keras.applications import ResNet50\n\n# train_image = []\n# for i, x in tqdm(Dataset_Train['Image'].iteritems()):\n# #for i in tqdm(range(Dataset_Train.shape[0])):\n#     img = image.load_img('..\/input\/images-of-gala\/Images of Gala\/Train Images\/'+Dataset_Train['Image'][i], target_size=(80,80))\n#     img = image.img_to_array(img)\n#     img = img\/255\n#     train_image.append(img)\n# X = np.array(train_image)\n\n# cat_num = { 'Attire':0,  'Decorationandsignage':1,  'Food':2,  'misc':3}\n# Y = Dataset_Train\n# Y = Y['Class'].replace(cat_num)\n# #Y = Y['Class'].values\n# Y = to_categorical(Y)\n\n# test_image = []\n# for i, x in tqdm(Dataset_Test['Image'].iteritems()):\n# #for i in tqdm(range(Dataset_Train.shape[0])):\n#     img = image.load_img('..\/input\/images-of-gala\/Images of Gala\/Train Images\/'+Dataset_Test['Image'][i], target_size=(80,80))\n#     img = image.img_to_array(img)\n#     img = img\/255\n#     test_image.append(img)\n# X_test = np.array(test_image)\n\n# cat_num = { 'Attire':0,  'Decorationandsignage':1,  'Food':2,  'misc':3}\n# Y_test = Dataset_Test\n# Y_test = Y_test['Class'].replace(cat_num)\n# #Y = Y['Class'].values\n# Y_test = to_categorical(Y_test)\n\n# model = Sequential()\n\n# resnet_weights_path = '..\/input\/resnet50\/resnet50_weights_tf_dim_ordering_tf_kernels_notop.h5'\n# model.add(ResNet50(include_top=False, pooling='avg', weights='imagenet',\n#                   input_shape=(80,80,3)))\n\n# model.add(Dense(4, activation='softmax'))\n\n# model.layers[0].trainable = False\n\n# model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])\n\n# datagen = ImageDataGenerator(\n#     #featurewise_center=True,\n#     #featurewise_std_normalization=True,\n#     #rotation_range=20,\n#     #width_shift_range=0.2,\n#     #height_shift_range=0.2,\n#     horizontal_flip=False)\n\n# datagen.fit(X)\n\n# valid = ImageDataGenerator(\n#     #featurewise_center=True,\n#     #featurewise_std_normalization=True,\n#     #rotation_range=20,\n#     #width_shift_range=0.2,\n#     #height_shift_range=0.2,\n#     horizontal_flip=False)\n\n# valid.fit(X_test)\n\n# model.summary()\n\ndef plot_acc(history):\n    import matplotlib.pyplot as plt\n    history_dict = history.history\n    acc_values = history_dict['accuracy'] \n    val_acc_values = history_dict['val_accuracy']\n    acc = history_dict['accuracy']\n    epochs = range(1, len(acc) + 1)\n    plt.plot(epochs, acc, 'bo', label='Training acc')\n    plt.plot(epochs, val_acc_values, 'b', label='Validation acc')\n    plt.title('Training and validation accuracy')\n    plt.xlabel('Epochs')\n    plt.ylabel('Accuracy')\n    plt.legend()\n    plt.show()\n\n# his= model.fit_generator(datagen.flow(X, Y, batch_size=32),\n#                          validation_data=valid.flow(X_test, Y_test),\n#                     steps_per_epoch=len(X) \/ 32, epochs=10)\n\n# plot_acc(his)","b99b99b9":"#import keras\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Flatten, BatchNormalization\nfrom keras.layers import Conv2D, MaxPooling2D\nfrom keras.utils import to_categorical\nfrom keras.preprocessing import image\nfrom keras.utils import to_categorical\nfrom keras.preprocessing.image import ImageDataGenerator\n\nfrom keras.applications import ResNet50","8876ab26":"train_datagen = ImageDataGenerator(rescale=1.\/255,\n                                   #shear_range=0.20,\n                                   #zoom_range=0.20,\n                                   #validation_split=0.20,   \n                                   horizontal_flip=True)","d12c906e":"valid_datagen = ImageDataGenerator(rescale=1.\/255,\n                                   #shear_range=0.20,\n                                   #zoom_range=0.20,\n                                   #validation_split=0.20,   \n                                   horizontal_flip=True)","db093580":"train_generator = train_datagen.flow_from_dataframe(dataframe=Dataset_Train,\n                                                    directory='new_train\/Train Images\/',\n                                                    x_col='Image',\n                                                    y_col='Class',\n                                                    #has_ext=True,\n                                                    seed=42,\n                                                    target_size=(224, 224),\n                                                    batch_size=16,\n                                                    #subset='training',    \n                                                    shuffle=True,\n                                                    class_mode='categorical')","2b8f1877":"valid_generator = valid_datagen.flow_from_dataframe(dataframe=Dataset_Test,\n                                                    directory='new_train\/Train Images\/',\n                                                    x_col='Image',\n                                                    y_col='Class',\n                                                    #has_ext=True,\n                                                    seed=42,\n                                                    target_size=(224, 224),\n                                                    batch_size=16,\n                                                    #subset='training',    \n                                                    shuffle=True,\n                                                    class_mode='categorical')","332075fd":"conv_base = ResNet50(include_top=False, input_shape=(224, 224,3))\n\n#conv_base = ResNet50(include_top=False, pooling='avg', weights='imagenet',)\n","0374b049":"conv_base.summary()","ec6a0fc1":"model = Sequential()\nmodel.add(conv_base)\nmodel.add(Flatten())\nmodel.add(Dense(256, activation='relu'))\nmodel.add(Dense(4, activation='softmax'))","efda58db":"model.compile(optimizer='Adam', loss='categorical_crossentropy', metrics=['accuracy'])","1e873b54":"conv_base.trainable","99defb91":"conv_base.trainable=\"False\"","94daa929":"his= model.fit_generator(\n                         train_generator,\n                         validation_data=valid_generator,\n                         validation_steps=50,\n                         steps_per_epoch=len(Dataset_Train)\/32, \n                         epochs=2,\n                        )","47ace7e9":"plot_acc(his)","1d912fe5":"**The below section shows the images after prediction, Details are here <br>\nhttps:\/\/forums.fast.ai\/t\/learn-show-results-show-the-same-examples-all-the-time-for-validation-set\/50289\n\n**","1b90ec30":"**This section is to visualize the results of a fastai tranformations**","82694c4c":"**The following cell implements the RADAM optimizer**","63df289f":"**The below cell copies the training data to a new location where we can manipulate the \ndata on the disks**"}}