{"cell_type":{"88667087":"code","92276b75":"code","2048333f":"code","1c5426e9":"code","093f07db":"code","b8ce36b7":"code","7503e02e":"code","2e1f46bf":"code","d1918fa6":"code","8dac28ce":"code","2bf6926c":"code","0c6f5dee":"code","cfb25051":"code","43851551":"code","24137ba5":"code","666a43ef":"code","23ff1891":"code","8e2eeafb":"code","8628860b":"code","e4c829f8":"code","ab8fc461":"code","01a212e1":"code","df6b3978":"code","421094de":"code","cf3c4415":"code","059f6262":"code","f76dd6cf":"markdown","51d9a60c":"markdown","f34d30c8":"markdown","d8681df7":"markdown","177e90b2":"markdown"},"source":{"88667087":"import os\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport matplotlib.pyplot as plt\n\nfrom kaggle_datasets import KaggleDatasets\nfrom sklearn.model_selection import train_test_split\n\nimport tensorflow as tf\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.callbacks import ReduceLROnPlateau, ModelCheckpoint, EarlyStopping","92276b75":"class CONFIG(object):\n  \"\"\"CONFIG\"\"\"\n  def __init__(self):\n    self.img_size = (256,256)\n    self.base= '..\/input\/bms-molecular-translation\/'\n    self.df= '..\/input\/bms-molecular-translation\/train_labels.csv'\n    self.train='..\/input\/bmsdataversion2\/BMS-Datav2\/train\/'\n    self.batch_size= 16\n    self.lr= 0.001\n    self.val_split= 0.1\n    self.seed= 22\n    self.n_epochs= 4\n    self.vocab_size= 600\n    \n    \ncfg= CONFIG()\n\ndef load_path(img_id):\n    return img_id[0] +'\/'+img_id[1]+'\/'+img_id[2] +'\/'+img_id+'.png'","2048333f":"vocab= ['<start>', '<end>', '1', '2', '3', '4', '5', '6', '7', '8', '9', '10', '11', '12', '13',\n        '14', '15', '16', '17', '18', '19', '20', '21', '22', '23', '24', '25', '26', '27', '28',\n        '29', '30', '31', '32', '33', '34', '35', '36', '37', '38', '39', '40', '41', '42', '43',\n        '44', '45', '46', '47', '48', '49', '50', '51', '52', '53', '54', '55', '56', '57', '58',\n        '59', '60', '61', '62', '63', '64', '65', '66', '67', '68', '69', '70', '71', '72', '73',\n        '74', '75', '76', '77', '78', '79', '80', '81', '82', '83', '84', '85', '86', '87', '88',\n        '89', '90', '91', '92', '(', ')', '-', 'c']\nlen(vocab)","1c5426e9":"img_paths= np.load('..\/input\/bmsdataversion2\/BMS-Datav2\/path.npy')\nX= np.load('..\/input\/bmsdataversion2\/BMS-Datav2\/X.npy')\nY= np.load('..\/input\/bmsdataversion2\/BMS-Datav2\/Y.npy')\n\nX.shape, Y.shape, img_paths.shape","093f07db":"Y= Y.astype(np.float32)\nX= X.astype(np.float32)","b8ce36b7":"img_paths= (pd.Series(img_paths).apply(lambda x: x.split('\/')[-1]))\nimg_paths[0]","7503e02e":"plt.bar(list(range(98)),Y.sum(0))\nplt.title('Class Distribution')\nplt.show()","2e1f46bf":"plt.bar(list(range(94)),Y.sum(0)[0:-4])\nplt.title('Class Distribution')\nplt.show()","d1918fa6":"def build_imgext():\n    img_base= tf.keras.applications.ResNet50V2(include_top=False)\n    inp= layers.Input((cfg.img_size[0], cfg.img_size[1], 3))\n    x= img_base(inp)\n    x= layers.Dropout(0.3)(x)\n    x= layers.GlobalAveragePooling2D()(x)\n    x= layers.Dense(512, 'relu')(x)\n    return tf.keras.Model(inp, x)\n\ndef build_model():\n    img_base= build_imgext()\n    inp1= layers.Input((cfg.img_size[0], cfg.img_size[1], 3))\n    fc1= img_base(inp1)\n    \n    inp2 = layers.Input(shape=(len(vocab)))\n    se1 = layers.Embedding(len(vocab), cfg.vocab_size, mask_zero=True)(inp2)\n    se2 = layers.Dropout(0.25)(se1)\n    se3 = layers.LSTM(512)(se2)\n    \n    decoder1 = layers.add([fc1, se3])\n    decoder2 = layers.Dense(512, activation='relu')(decoder1)\n    out = layers.Dense(len(vocab), activation='softmax')(decoder2)\n    \n    return tf.keras.Model([inp1, inp2], out)","8dac28ce":"def auto_select_accelerator():\n    try:\n        tpu = tf.distribute.cluster_resolver.TPUClusterResolver()\n        tf.config.experimental_connect_to_cluster(tpu)\n        tf.tpu.experimental.initialize_tpu_system(tpu)\n        strategy = tf.distribute.experimental.TPUStrategy(tpu)\n        print(\"Running on TPU:\", tpu.master())\n    except ValueError:\n        strategy = tf.distribute.get_strategy()\n    print(f\"Running on {strategy.num_replicas_in_sync} replicas\")\n    \n    return strategy","2bf6926c":"img_size= cfg.img_size[0]\ndef build_decoder(with_labels=True, target_size=(img_size, img_size), ext='png'):\n    def decode(path):\n        file_bytes = tf.io.read_file(path) # Reads and outputs the entire contents of the input filename.\n\n        if ext == 'png':\n            img = tf.image.decode_png(file_bytes, channels=3) # Decode a PNG-encoded image to a uint8 or uint16 tensor\n        elif ext in ['jpg', 'jpeg']:\n            img = tf.image.decode_jpeg(file_bytes, channels=3) # Decode a JPEG-encoded image to a uint8 tensor\n        else:\n            raise ValueError(\"Image extension not supported\")\n\n        img = tf.cast(img, tf.float32) \/ 255.0 # Casts a tensor to the type float32 and divides by 255.\n        img = tf.image.resize(img, target_size) # Resizing to target size\n        return img\n    \n    def decode_with_labels(path, x, label):\n        return (decode(path), x), label\n    \n    return decode_with_labels if with_labels else decode","0c6f5dee":"def build_augmenter(with_labels=True):\n    def augment(img):\n        img = tf.image.random_flip_left_right(img)\n        img = tf.image.random_saturation(img, 0.8, 1.2)\n        img = tf.image.random_brightness(img, 0.1)\n        img = tf.image.random_contrast(img, 0.8, 1.2)\n        return img\n    \n    def augment_with_labels(img, label):\n        return augment(img), label\n    \n    return augment_with_labels if with_labels else augment\n\ndef build_dataset(paths, x=None, labels=None, bsize=32, cache=True,\n                  decode_fn=None, augment_fn=None,\n                  augment=True, repeat=True, shuffle=1024, \n                  cache_dir=\"\"):\n    if cache_dir != \"\" and cache is True:\n        os.makedirs(cache_dir, exist_ok=True)\n    \n    if decode_fn is None:\n        decode_fn = build_decoder(labels is not None)\n    \n    if augment_fn is None:\n        augment_fn = build_augmenter(labels is not None)\n    \n    AUTO = tf.data.experimental.AUTOTUNE\n    slices = paths if labels is None else (paths, x, labels)\n    \n    dset = tf.data.Dataset.from_tensor_slices(slices)\n    dset = dset.map(decode_fn, num_parallel_calls=AUTO)\n    dset = dset.cache(cache_dir) if cache else dset\n    #dset = dset.map(augment_fn, num_parallel_calls=AUTO) if augment else dset\n    dset = dset.repeat() if repeat else dset\n    dset = dset.shuffle(shuffle) if shuffle else dset\n    dset = dset.batch(bsize, drop_remainder=False).prefetch(AUTO)\n    # dset = dset.batch(bsize, drop_remainder=False).prefetch(AUTO) #overlaps data preprocessing and model execution while training\n    return dset","cfb25051":"DATASET_NAME = \"bmsdataversion2\"\nstrategy = auto_select_accelerator()\nbatch_size = strategy.num_replicas_in_sync * cfg.batch_size\nprint('batch size', batch_size)","43851551":"GCS_DS_PATH = KaggleDatasets().get_gcs_path(DATASET_NAME)\nGCS_DS_PATH","24137ba5":"Ydf= pd.DataFrame(Y)\n# end tags\nind= Ydf[Ydf[1]==1].index\nsp= ind[12000] +1","666a43ef":"img_paths= GCS_DS_PATH + '\/BMS-Datav2\/train\/' + img_paths\n#img_paths= '..\/input\/bmsdataversion2\/BMS-Datav2\/train\/' + img_paths\nimg_paths[0]","23ff1891":"#Split data\n(train_paths, valid_paths, \n train_labels, valid_labels,\n X_train, X_valid) = (img_paths[:sp], img_paths[sp:], Y[:sp], Y[sp:], X[:sp], X[sp:])\n\nprint(train_paths.shape, valid_paths.shape)","8e2eeafb":"decoder = build_decoder(with_labels=True, target_size=(img_size, img_size))\n\n# Build the tensorflow datasets\ndtrain = build_dataset(\n    train_paths, X_train, train_labels, bsize=batch_size, decode_fn=decoder)\n\ndvalid = build_dataset(\n    valid_paths, X_valid, valid_labels, bsize=batch_size, \n    repeat=False, shuffle=False, augment=False, decode_fn=decoder)","8628860b":"data, _ = dtrain.take(2)\nimages = data[0][0].numpy()","e4c829f8":"fig, axes = plt.subplots(3, 4, figsize=(15,10))\naxes = axes.flatten()\nfor img, ax in zip(images, axes):\n    ax.imshow(img, aspect= True)\n    ax.axis('off')\nplt.tight_layout()\nplt.show()","ab8fc461":"with strategy.scope():\n    model= build_model()\n    loss= tf.keras.losses.CategoricalCrossentropy(label_smoothing=0.0)\n    \n    model.compile(tf.keras.optimizers.Adam(lr=cfg.lr),\n                  loss= loss, metrics=['accuracy'])","01a212e1":"tf.keras.utils.plot_model(model, show_shapes=True)","df6b3978":"name= 'img_capv1.h5'\n\nrlr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.1, patience = 2, verbose = 1, \n                                min_delta = 1e-4, min_lr = 1e-6, mode = 'min', cooldown=1)\n        \nckp = ModelCheckpoint(name,monitor = 'val_loss',\n                      verbose = 1, save_best_only = True, mode = 'min')\n        \nes = EarlyStopping(monitor = 'val_loss', min_delta = 1e-4, patience = 5, mode = 'min', \n                    restore_best_weights = True, verbose = 1)\n\nsteps_per_epoch = (train_paths.shape[0] \/\/ batch_size)\/\/2\nsteps_per_epoch","421094de":"history = model.fit(dtrain,                      \n                    validation_data=dvalid,                                       \n                    epochs=cfg.n_epochs,\n                    callbacks=[rlr,es,ckp],\n                    steps_per_epoch=steps_per_epoch,\n                    verbose=1)","cf3c4415":"plt.figure(figsize = (12, 6))\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.plot( history.history[\"loss\"], label = \"Training Loss\", marker='o')\nplt.plot( history.history[\"val_loss\"], label = \"Validation Loss\", marker='+')\nplt.grid(True)\nplt.legend()\nplt.show()","059f6262":"plt.figure(figsize = (12, 6))\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"ACC\")\nplt.plot( history.history[\"accuracy\"], label = \"Training ACC\" , marker='o')\nplt.plot( history.history[\"val_accuracy\"], label = \"Validation ACC\", marker='+')\nplt.grid(True)\nplt.legend()\nplt.show()","f76dd6cf":"## INPUT DATA\n<img src=\"https:\/\/camo.githubusercontent.com\/c73259c22376b40060d05571a8731781d1058771\/68747470733a2f2f7261772e67697468756275736572636f6e74656e742e636f6d2f6a656666686561746f6e2f7438315f3535385f646565705f6c6561726e696e672f6d61737465722f696d616765732f63617074696f6e2d322e706e67\" height=\"300\" align=\"left\">","51d9a60c":"![](https:\/\/i.gifer.com\/7ImI.gif)","f34d30c8":"## TPU PIPELINE","d8681df7":"# Vector to Sequence RNN (Part-ii)","177e90b2":"## Define Model"}}