{"cell_type":{"d3dea300":"code","05dd4888":"code","0dcf1c08":"code","14a559d5":"code","ca28773e":"code","2fe9a775":"code","323de41b":"code","425ee359":"code","fe5ddbb1":"code","469cdb1c":"code","0da0861d":"code","09c5f90a":"code","30e2f18a":"code","3a7387b7":"code","38587017":"code","90b52013":"code","303e084b":"code","9ba4d54b":"code","e1828604":"code","e37bae12":"code","3ef8337a":"code","340ecb70":"code","b10d1f0f":"code","63d0cb24":"code","0d12711c":"code","9f3aa108":"code","ea55df8a":"code","5f3df85c":"code","91fd142e":"code","620133b7":"code","ec109562":"code","ec352cf9":"code","167db4e9":"code","49973996":"code","834d6fb3":"code","02aac67d":"code","0d01ee76":"code","5180b91b":"code","2258d928":"code","cac7a7b1":"code","fa207c37":"code","ab6a2b55":"code","d5906a9d":"code","cf3ef054":"code","f070d679":"code","324f7d96":"code","18e089ae":"code","b4820a01":"code","d31bb905":"code","61bd26f9":"code","5475b701":"code","c3ca917e":"code","cae18d91":"code","70bf15f4":"code","ff6e1e6b":"code","a3c8789a":"code","8b7e2ae8":"code","f919b682":"code","a15444de":"code","7323d336":"code","383a88a0":"code","0655c256":"code","f6a66ee8":"code","5446dd8b":"code","ab1d7c9a":"code","5e87d727":"code","b9a7c8c9":"code","daef8809":"code","2cba2791":"code","ba8914ff":"code","efca2c81":"code","a9ecc393":"code","8cca18ba":"code","28623233":"code","4c9721b7":"code","c6b5cb2d":"code","bfb78985":"code","c98120b6":"code","58fced88":"code","7c90e37f":"code","85b33618":"code","c0ea12e4":"code","9083a2b3":"code","1d24bdba":"code","d8ba5a3d":"code","2c533902":"code","c2f7b210":"code","77786322":"code","6ea808ac":"code","45184f2f":"code","2e0ddff4":"code","aaab8a9c":"code","17a0fbc2":"code","210fb923":"code","d298aea4":"code","c351ab79":"code","3fb26c83":"code","33b72c14":"code","670ea310":"code","daa78395":"code","7c0561a3":"code","a2b5dd73":"code","90d7a184":"code","52cb4792":"code","af77c93d":"code","c50cd11f":"code","6b2ea4d2":"code","6395ca09":"code","6d6e342b":"code","5c04fa55":"code","6816b16c":"code","8cc5439a":"code","7e11ab31":"code","5e4cd4ea":"code","76c34db8":"code","ec6eeb37":"code","d7c0928d":"code","fa1dc24c":"code","d22e46de":"code","1a2971ce":"code","868228be":"code","533e8d2e":"markdown","22297ee7":"markdown","203d64b6":"markdown","f0926933":"markdown","48d9683f":"markdown","d9a5a9a8":"markdown","af86c6a6":"markdown","ac8ad6c6":"markdown","2f4f4323":"markdown","3f311325":"markdown","ecf515bd":"markdown","890e56a4":"markdown","1aed9018":"markdown","c2aff04e":"markdown","26567347":"markdown","639d67f4":"markdown","25fd2fb1":"markdown","fe488eba":"markdown","150e4f4c":"markdown","cbba1fcf":"markdown","eb4545c8":"markdown","b7595fa4":"markdown","30f55645":"markdown","0766d1fb":"markdown","c83751fa":"markdown","b9469db4":"markdown","1a9fc915":"markdown","b1505f9c":"markdown","12b1388f":"markdown","a6438226":"markdown","31da8c76":"markdown","912d05c2":"markdown","05134265":"markdown","187b6f3a":"markdown","9381d6ac":"markdown","4290bd2c":"markdown","a1842ff5":"markdown","5e7fa6ad":"markdown","744abc0b":"markdown","ff13189d":"markdown","a75ad7e5":"markdown","ba21cffc":"markdown","be73083a":"markdown","466c7d57":"markdown","54319ffa":"markdown","e5faeb83":"markdown","364aca75":"markdown","8c14a155":"markdown","f1a5170d":"markdown","630c3e6a":"markdown","f9b8c796":"markdown","a4eca7ad":"markdown","6d411559":"markdown","5d889d0d":"markdown","8fa54cbe":"markdown","b1c90584":"markdown","286bafb6":"markdown","c529534f":"markdown","9aeb927f":"markdown","d900e942":"markdown","4dd44dbf":"markdown","46b6549f":"markdown","927a30ab":"markdown","a0ddf903":"markdown","2b1f8b36":"markdown","543f417c":"markdown","4b4e4f3e":"markdown"},"source":{"d3dea300":"import pandas as pd\nimport warnings\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom collections import defaultdict\nimport numpy as np\nimport string\nspecial = string.punctuation \nwarnings.filterwarnings(\"ignore\")\nfrom collections import  Counter\nimport nltk \nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\nstop = set(stopwords.words('english')) \nimport nltk\nnltk.download('wordnet')","05dd4888":"def seed_everything(seed):\n    random.seed(seed)\n    np.random.seed(seed)","0dcf1c08":"def load (path) : \n  df = pd.read_csv(path)\n  return (df)","14a559d5":"def concat_df(train_data, test_data):\n    return pd.concat([train_data, test_data], sort=True).reset_index(drop=True)","ca28773e":"def divide_df(df_all,train_len):\n    return df_all.loc[:train_len-1], df_all.loc[train_len:].drop('target',axis=1)","2fe9a775":"from tqdm import tqdm \ndef List_of_words(df): \n    words = [word for tweet in tqdm(df['text']) for word in tweet.split()]\n    return words","323de41b":"def List_of_tweets(df):\n    tweets = [tweet for tweet in tqdm(df['text']) ]\n    return tweets","425ee359":"train = load('..\/input\/nlp-getting-started\/train.csv')\ntest = load('..\/input\/nlp-getting-started\/test.csv')","fe5ddbb1":"train.shape","469cdb1c":"def mislabeled_tweets (train) : # function that returns mislabeled labeled tweets\n  df_mislabeled = train.groupby(['text']).nunique().sort_values(by='target', ascending=False)\n  df_mislabeled = df_mislabeled[df_mislabeled['target'] > 1]['target']\n  return (df_mislabeled.index.tolist()) ","0da0861d":"mislabeled_tweets (train)","09c5f90a":" # function to correct mislabeled tweets\ndef correcting_labels (train) : \n  train['target_relabeled'] = train['target'].copy() \n  train.loc[train['text'] == 'like for the music video I want some real action shit like burning buildings and police chases not some weak ben winston shit', 'target_relabeled'] = 0\n  train.loc[train['text'] == 'Hellfire is surrounded by desires so be careful and don\u0089\u00db\u00aat let your desires control you! #Afterlife', 'target_relabeled'] = 0\n  train.loc[train['text'] == 'To fight bioterrorism sir.', 'target_relabeled'] = 0\n  train.loc[train['text'] == '.POTUS #StrategicPatience is a strategy for #Genocide; refugees; IDP Internally displaced people; horror; etc. https:\/\/t.co\/rqWuoy1fm4', 'target_relabeled'] = 1\n  train.loc[train['text'] == 'CLEARED:incident with injury:I-495  inner loop Exit 31 - MD 97\/Georgia Ave Silver Spring', 'target_relabeled'] = 1\n  train.loc[train['text'] == '#foodscare #offers2go #NestleIndia slips into loss after #Magginoodle #ban unsafe and hazardous for #humanconsumption', 'target_relabeled'] = 0\n  train.loc[train['text'] == 'In #islam saving a person is equal in reward to saving all humans! Islam is the opposite of terrorism!', 'target_relabeled'] = 0\n  train.loc[train['text'] == 'Who is bringing the tornadoes and floods. Who is bringing the climate change. God is after America He is plaguing her\\n \\n#FARRAKHAN #QUOTE', 'target_relabeled'] = 1\n  train.loc[train['text'] == 'RT NotExplained: The only known image of infamous hijacker D.B. Cooper. http:\/\/t.co\/JlzK2HdeTG', 'target_relabeled'] = 1\n  train.loc[train['text'] == \"Mmmmmm I'm burning.... I'm burning buildings I'm building.... Oooooohhhh oooh ooh...\", 'target_relabeled'] = 0\n  train.loc[train['text'] == \"wowo--=== 12000 Nigerian refugees repatriated from Cameroon\", 'target_relabeled'] = 0\n  train.loc[train['text'] == \"He came to a land which was engulfed in tribal war and turned it into a land of peace i.e. Madinah. #ProphetMuhammad #islam\", 'target_relabeled'] = 0\n  train.loc[train['text'] == \"Hellfire! We don\u0089\u00db\u00aat even want to think about it or mention it so let\u0089\u00db\u00aas not do anything that leads to it #islam!\", 'target_relabeled'] = 0\n  train.loc[train['text'] == \"The Prophet (peace be upon him) said 'Save yourself from Hellfire even if it is by giving half a date in charity.'\", 'target_relabeled'] = 0\n  train.loc[train['text'] == \"Caution: breathing may be hazardous to your health.\", 'target_relabeled'] = 1\n  train.loc[train['text'] == \"I Pledge Allegiance To The P.O.P.E. And The Burning Buildings of Epic City. ??????\", 'target_relabeled'] = 0\n  train.loc[train['text'] == \"#Allah describes piling up #wealth thinking it would last #forever as the description of the people of #Hellfire in Surah Humaza. #Reflect\", 'target_relabeled'] = 0\n  train.loc[train['text'] == \"that horrible sinking feeling when you\u0089\u00db\u00aave been at home on your phone for a while and you realise its been on 3G this whole time\", 'target_relabeled'] = 0\n  train.drop('target',axis= 1 , inplace=True ) \n  train.columns = ['id', 'keyword', 'location', 'text', 'target']","30e2f18a":"correcting_labels (train)","3a7387b7":"# function to return missing values\ndef missing_values(df): \n  total = df.isnull().sum().sort_values(ascending=False)\n\n\n  percent = (df.isnull().sum()\/df.isnull().count()).sort_values(ascending=False)\n\n  missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n  f, ax = plt.subplots(figsize=(15, 6))\n  plt.xticks(rotation='90')\n  sns.barplot(x=missing_data.index, y=missing_data['Percent'])\n  plt.xlabel('Features', fontsize=15)\n  plt.ylabel('Percent of Missing Values', fontsize=15)\n  plt.title('Percentage of Missing Data by Feature', fontsize=15)\n  return (missing_data)\n","38587017":"missing_values(train)\n# missing_values(test)","90b52013":"# function to plot the target _ distribution\ndef target_distribution (train) : \n  fig, axes = plt.subplots(ncols=2, figsize=(17, 4), dpi=100)\n  plt.tight_layout()\n\n  train.groupby('target').count()['id'].plot(kind='pie', ax=axes[0], labels=['Not Disaster (57%)', 'Disaster (43%)'])\n  sns.countplot(x=train['target'], hue=train['target'], ax=axes[1])\n\n  axes[0].set_ylabel('')\n  axes[1].set_ylabel('')\n  axes[1].set_xticklabels(['Not Disaster (4342)', 'Disaster (3271)'])\n  axes[0].tick_params(axis='x', labelsize=15)\n  axes[0].tick_params(axis='y', labelsize=15)\n  axes[1].tick_params(axis='x', labelsize=15)\n  axes[1].tick_params(axis='y', labelsize=15)\n\n  axes[0].set_title('Target Distribution in Training Set', fontsize=13)\n  axes[1].set_title('Target Count in Training Set', fontsize=13)\n\n  plt.show()","303e084b":"target_distribution (train)","9ba4d54b":"# function to plot the caracter _ distribution\ndef caracter_level_distrubtion (train): \n  fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n  tweet_len=train[train['target']==1]['text'].str.len() \n  ax1.hist(tweet_len,color='red')\n  ax1.set_title('disaster tweets')\n  tweet_len=train[train['target']==0]['text'].str.len()\n  ax2.hist(tweet_len,color='green')\n  ax2.set_title('Not disaster tweets')\n  fig.suptitle('Characters in tweets')\n  plt.show()\n","e1828604":"caracter_level_distrubtion (train)","e37bae12":"# function to plot the word level distrubtion\ndef word_level_distrubtion (train): \n  fig,(ax1,ax2)=plt.subplots(1,2,figsize=(10,5))\n  tweet_len=train[train['target']==1]['text'].str.split().map(lambda x: len(x))\n  ax1.hist(tweet_len,color='red')\n  ax1.set_title('disaster tweets')\n  tweet_len=train[train['target']==0]['text'].str.split().map(lambda x: len(x))\n  ax2.hist(tweet_len,color='green')\n  ax2.set_title('Not disaster tweets')\n  fig.suptitle('Characters in tweets')\n  plt.show()","3ef8337a":"word_level_distrubtion (train)","340ecb70":"from wordcloud import WordCloud\ndef WordCloud_plotting (train): \n  disaster_tweets = train[train['target']==0]['text']\n  Non_disaster_tweets = train[train['target']==1]['text']\n  fig, (ax1, ax2) = plt.subplots(1, 2, figsize=[26, 8])\n  wordcloud1 = WordCloud( background_color='white',\n                          width=600,\n                          height=400).generate(\" \".join(disaster_tweets)) \n\n  ax1.imshow(wordcloud1)\n  ax1.axis('off')\n  ax1.set_title('Disaster Tweets',fontsize=40);\n  # Non disaster\n  wordcloud2 = WordCloud( background_color='white',\n                          width=600,\n                          height=400).generate(\" \".join(Non_disaster_tweets)) \n\n  ax2.imshow(wordcloud2)\n  ax2.axis('off')\n  ax2.set_title('Non Disaster Tweets',fontsize=40);","b10d1f0f":"WordCloud_plotting (train)","63d0cb24":"# function to return a dictionnary of punctuation_frequency\ndef punctuation_frequence(df) : \n  punct =defaultdict(int)\n  for word in (List_of_words(df)):\n      if word in special:\n          punct[word]+=1\n  return(punct)\n","0d12711c":"punct = punctuation_frequence(train)","9f3aa108":"# function to plot a barplot of punctuation\ndef punct_barplot (punct) : \n  x,y=zip(*punct.items())\n  plt.bar(x,y,color='green')","ea55df8a":"punct_barplot (punct)","5f3df85c":"# function to return the most common words\ndef most_common (df):\n  counter=Counter(List_of_words(df))\n  most=counter.most_common()\n  x=[]\n  y=[]\n  for word,count in most[:70]: \n      if (word not in stop) :\n          x.append(word)\n          y.append(count)\n  sns.barplot(x=y,y=x) \n","91fd142e":"most_common (train)","620133b7":"# function to return the top n grams tweets\nfrom sklearn.feature_extraction.text import CountVectorizer\ndef get_top_tweet_ngrams(corpus,n_gram,lenght): \n    vec = CountVectorizer(ngram_range=(n_gram, n_gram)).fit(corpus) \n    bag_of_words = vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True) \n    return words_freq[:lenght] ","ec109562":"get_top_tweet_ngrams(List_of_tweets(train),4,10)","ec352cf9":"def plotting_top_ngrams (df): \n  plt.figure(figsize=(10,5))\n  top_tweet_bigrams=get_top_tweet_ngrams(df['text'],4,10)[:10]\n  x,y=map(list,zip(*top_tweet_bigrams))\n  sns.barplot(x=y,y=x)   ","167db4e9":"plotting_top_ngrams (train)","49973996":"# function to plot keyword feature\ndef vis_per_feature (feature) : \n  train['target_mean'] = train.groupby(feature)['target'].transform('mean')\n  fig = plt.figure(figsize=(8, 72), dpi=100)\n\n  sns.countplot(y=train.sort_values(by='target_mean', ascending=False)[feature],\n                hue=train.sort_values(by='target_mean', ascending=False)['target'])\n  plt.tick_params(axis='x', labelsize=15)\n  plt.tick_params(axis='y', labelsize=12)\n  plt.legend(loc=1)\n  plt.title('Target Distribution in Keywords')\n  plt.show()\n  train.drop(columns=['target_mean'], inplace=True)","834d6fb3":"vis_per_feature ('keyword')","02aac67d":"def location_binging (df):\n  df['location'].replace({'United States':'USA',\n                            'New York':'USA',\n                              \"London\":'UK',\n                              \"Los Angeles, CA\":'USA',\n                              \"Washington, D.C.\":'USA',\n                              \"California\":'USA',\n                              \"Chicago, IL\":'USA',\n                              \"Chicago\":'USA',\n                              \"New York, NY\":'USA',\n                              \"California, USA\":'USA',\n                              \"FLorida\":'USA',\n                              \"Nigeria\":'Africa',\n                              \"Kenya\":'Africa',\n                              \"Everywhere\":'Worldwide',\n                              \"San Francisco\":'USA',\n                              \"Florida\":'USA',\n                              \"United Kingdom\":'UK',\n                              \"Los Angeles\":'USA',\n                              \"Toronto\":'Canada',\n                              \"San Francisco, CA\":'USA',\n                              \"NYC\":'USA',\n                              \"Seattle\":'USA',\n                              \"Earth\":'Worldwide',\n                              \"Ireland\":'UK',\n                              \"London, England\":'UK',\n                              \"New York City\":'USA',\n                              \"Texas\":'USA',\n                              \"London, UK\":'UK',\n                              \"Atlanta, GA\":'USA',\n                              \"Mumbai\":\"India\"},inplace=True)","0d01ee76":"location_binging (train)\nlocation_binging (test)","5180b91b":"# vis_per_feature ('location') ","2258d928":"df_all = concat_df(train,test)","cac7a7b1":"# word_count\ndef word_count(df_all) : \n  df_all['word_count'] = df_all['text'].apply(lambda x: len(str(x).split()))\n\n# unique_word_count\ndef unique_word_count (df_all) : \n  df_all['unique_word_count'] = df_all['text'].apply(lambda x: len(set(str(x).split())))  \n\n# stop_word_count\ndef stop_word_count (df_all) : \n  df_all['stop_word_count'] = df_all['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in stopwords.words('english')]))\n\n# url_count\ndef url_count(df_all) : \n  df_all['url_count'] = df_all['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n\n# mean_word_length\ndef mean_word_length (df_all) : \n  df_all['mean_word_length'] = df_all['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\n# char_count\ndef char_count (df_all) : \n  df_all['char_count'] = df_all['text'].apply(lambda x: len(str(x)))\n\n# punctuation_count\ndef punctuation_count (df_all) : \n  df_all['punctuation_count'] = df_all['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n# hashtag_count\ndef hashtag_count(df_all) : \n  df_all['hashtag_count'] = df_all['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n\n# mention_count\ndef mention_count(df_all) : \n  df_all['mention_count'] = df_all['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n","fa207c37":"word_count(df_all) \nunique_word_count (df_all)  \nstop_word_count (df_all) \nurl_count(df_all) \nmean_word_length (df_all) \nchar_count (df_all) \npunctuation_count (df_all)\nhashtag_count(df_all) \nmention_count(df_all)","ab6a2b55":"train , test  =  divide_df(df_all,train.shape[0])","d5906a9d":"def box_plot_for_meta_features (train,f1,f2,f3) :\n  f, axes = plt.subplots(3, 1, figsize=(10,20))\n  sns.boxplot(x='target', y=f1, data=train, ax=axes[0])\n  axes[0].set_xlabel('Target', fontsize=12)\n  axes[0].set_title(\"Number of \" + f1.split('_')[0] + \" in each class\", fontsize=15)\n\n  sns.boxplot(x='target', y=f2, data=train, ax=axes[1])\n  axes[1].set_xlabel('Target', fontsize=12)\n  axes[1].set_title(\"Number of\" + f2.split('_')[0] + \"in each class\", fontsize=15)\n\n  sns.boxplot(x='target', y=f3, data=train, ax=axes[2])\n  axes[2].set_xlabel('Target', fontsize=12)\n  axes[2].set_title(\"Number of \" + f3.split('_')[0] + \" in each class\", fontsize=15)\n  plt.show()","cf3ef054":"box_plot_for_meta_features (train,'word_count','url_count','char_count')","f070d679":"def plotting_MetaFeatures (train): \n  METAFEATURES = ['word_count', 'unique_word_count', 'stop_word_count', 'url_count', 'mean_word_length',\n                  'char_count', 'punctuation_count', 'hashtag_count', 'mention_count']\n  DISASTER_TWEETS = train['target'] == 1\n\n  fig, axes = plt.subplots(ncols=2, nrows=len(METAFEATURES), figsize=(20, 50), dpi=100)\n\n  for i, feature in enumerate(METAFEATURES):\n      sns.distplot(train.loc[~DISASTER_TWEETS][feature], label='Not Disaster', ax=axes[i][0], color='green')\n      sns.distplot(train.loc[DISASTER_TWEETS][feature], label='Disaster', ax=axes[i][0], color='red')\n\n      sns.distplot(train[feature], label='Training', ax=axes[i][1])\n      sns.distplot(test[feature], label='Test', ax=axes[i][1])\n      \n      for j in range(2):\n          axes[i][j].set_xlabel('')\n          axes[i][j].tick_params(axis='x', labelsize=12)\n          axes[i][j].tick_params(axis='y', labelsize=12)\n          axes[i][j].legend()\n      \n      axes[i][0].set_title(f'{feature} Target Distribution in Training Set', fontsize=13)\n      axes[i][1].set_title(f'{feature} Training & Test Set Distribution', fontsize=13)\n\n  plt.show()","324f7d96":"plotting_MetaFeatures (train)","18e089ae":"#remove stop words\ndef remove_stops(text) : \n  remove_stopwords = [w for w in text.split() if w not in stopwords.words('english')]\n  return ' '.join(remove_stopwords)","b4820a01":"# remove URLs\nimport re\ndef remove_URL(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+')\n    return url.sub(r'',text)","d31bb905":"# remove htmls\ndef remove_html(text):\n    html=re.compile(r'<.*?>')\n    return html.sub(r'',text)","61bd26f9":"def remove_emoji(text):\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    return emoji_pattern.sub(r'', text)","5475b701":"# remove punct\ndef remove_punct(text):\n    table=str.maketrans('','',string.punctuation)\n    return text.translate(table)","c3ca917e":"# remove other ...\ndef remove_other (text) : \n    text = text.lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    return(text)","cae18d91":"# Full clean ... combination off all the previous functions\ndef Full_clean (text) : \n  text = remove_stops(text)\n  text = remove_URL(text)\n  text = remove_html(text)\n  text = remove_emoji(text)\n  text = remove_punct(text)\n  text = remove_other (text)\n  return(text)","70bf15f4":"df_all['text'] = df_all['text'].apply(lambda x : Full_clean(x))","ff6e1e6b":"!pip install pyspellchecker","a3c8789a":"# function to take a text and correct it \nfrom spellchecker import SpellChecker\nspell = SpellChecker()\ndef correct_spellings(text):\n    corrected_text = []\n    misspelled_words = spell.unknown(text.split()) \n    for word in text.split(): \n        if word in misspelled_words:\n            corrected_text.append(spell.correction(word))\n        else:\n            corrected_text.append(word)\n    return \" \".join(corrected_text)","8b7e2ae8":"# df_all['text'] = df_all['text'].apply(lambda x : correct_spellings(x))","f919b682":"# fonction to stemm the text\ndef stemming (text) : \n  stemmer = nltk.stem.PorterStemmer()\n  return(\" \".join(stemmer.stem(word) for word in text.split()))","a15444de":"# function to lemm a text \ndef lemming (text) : \n  lemmatizer=nltk.stem.WordNetLemmatizer()\n  return(\" \".join(lemmatizer.lemmatize(word) for word in text.split()))","7323d336":"df_all['text'] = df_all['text'].apply(lambda x : stemming(x))\n# df_all['text'] = df_all['text'].apply(lambda x : lemming(x))","383a88a0":"train , test = divide_df(df_all,train.shape[0])","0655c256":"from keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\n# return a fitted tokenizer \ndef tokenization (train)  : \n  tokenizer =Tokenizer()\n  tokenizer.fit_on_texts(List_of_tweets(train))\n  return(tokenizer)","f6a66ee8":"tokenizer = tokenization(train) ","5446dd8b":"# function to prepare the sequences by the tokenizer \ndef preparing_sequences(tokenizer) : \n  train_sequences = tokenizer.texts_to_sequences(List_of_tweets(train))\n  test_sequences = tokenizer.texts_to_sequences(List_of_tweets(test))\n  return train_sequences , test_sequences","ab1d7c9a":"train_sequences , test_sequences = preparing_sequences(tokenizer)","5e87d727":"# a function to padd the sequences \ndef padding_sequences (train_sequences, test_sequences):\n  train_sequences = pad_sequences(train_sequences,25,padding='post', truncating='post')\n  test_sequences = pad_sequences(test_sequences,25,padding='post', truncating='post')\n  return train_sequences , test_sequences","b9a7c8c9":"train_sequences , test_sequences = padding_sequences (train_sequences, test_sequences)","daef8809":"# function to return the train \/ test matrix\ndef sequences_to_matrix (train_sequences,test_sequences) : \n  train_matrix = np.vstack(train_sequences)\n  test_matrix = np.vstack(test_sequences)\n  return train_matrix , test_matrix","2cba2791":"train_matrix , test_matrix = sequences_to_matrix (train_sequences,test_sequences)","ba8914ff":"# function to return a fitted countvectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\ndef countvectorizer (train) : \n  count_vectorizer = CountVectorizer() \n  count_vectorizer.fit(List_of_tweets(train))\n  return  count_vectorizer","efca2c81":"count_vectorizer = countvectorizer (train) ","a9ecc393":"# function to prepare matrix with countvectorizer\ndef preparing_matrix_countvectorizer (count_vectorizer):\n  train_matrix = count_vectorizer.transform(List_of_tweets(train))\n  test_matrix = count_vectorizer.transform(List_of_tweets(test))\n  return train_matrix.toarray() , test_matrix.toarray()","8cca18ba":"train_matrix , test_matrix = preparing_matrix_countvectorizer (count_vectorizer)","28623233":"# function to return the train\/test matrix with tfidf \nfrom sklearn.feature_extraction.text import TfidfTransformer\ndef tfidf_fit (train_matrix , test_matrix) : \n  train_matrix = TfidfTransformer().fit_transform(train_matrix)\n  test_matrix = TfidfTransformer().fit_transform(test_matrix)\n  return train_matrix.toarray() , test_matrix.toarray()","4c9721b7":" train_matrix , test_matrix = tfidf_fit (train_matrix , test_matrix)","c6b5cb2d":"# if you want to use the Full pipe line  (countvectorizer + Tfidf)\nfrom sklearn.pipeline import Pipeline\ndef Full_pip (train) : \n   pipe = Pipeline([('count', CountVectorizer()),\n                    ('tfid', TfidfTransformer())])\n   pipe.fit(List_of_tweets(train))\n   return(pipe)\n","bfb78985":"pipe = Full_pip (train)","c98120b6":"\ndef preparing_matrix_pipe (pipe):\n   train_matrix =  pipe.transform(List_of_tweets(train))\n   test_matrix =  pipe.transform(List_of_tweets(test))\n   return train_matrix.toarray(),test_matrix.toarray()","58fced88":"train_matrix,test_matrix = preparing_matrix_pipe (pipe)","7c90e37f":"from sklearn import feature_selection \ndef Best_feature_extractor (train,count_vectorizer,train_matrix):\n  y = train['target']\n  Best_X_names = count_vectorizer.get_feature_names()\n  p_value_limit = 0.95\n  dtf_features = pd.DataFrame()\n  for cat in np.unique(y):\n      chi2, p = feature_selection.chi2(train_matrix, y==cat)\n      dtf_features = dtf_features.append(pd.DataFrame(\n                    {\"feature\":Best_X_names, \"score\":1-p, \"y\":cat}))\n      dtf_features = dtf_features.sort_values([\"y\",\"score\"], \n                      ascending=[True,False])\n      dtf_features = dtf_features[dtf_features[\"score\"]>p_value_limit]\n  Best_X_names = dtf_features[\"feature\"].unique().tolist()\n  return(Best_X_names)\n","85b33618":"Best_X_names = Best_feature_extractor (train,count_vectorizer,train_matrix)","c0ea12e4":"def columns_index (Best_X_names,count_vectorizer): \n  columns_index_to_preserve = [count_vectorizer.vocabulary_[x] for x in Best_X_names ]\n  return columns_index_to_preserve","9083a2b3":"columns_index_to_preserve = columns_index (Best_X_names,count_vectorizer)","1d24bdba":"def reducing_matrix (train_matrix,test_matrix , columns_index_to_preserve): \n  train_matrix = train_matrix[:, columns_index_to_preserve ] \n  test_matrix = test_matrix[:, columns_index_to_preserve ] \n  return train_matrix , test_matrix\n","d8ba5a3d":"train_matrix , test_matrix = reducing_matrix (train_matrix,test_matrix , columns_index_to_preserve)","2c533902":"# function to return a dictionnary with (key,embedding vectors)\ndef embedding_vectors (path) : \n  embedding_dict={}\n  with open(path,'r') as f:\n      for line in f:\n          values=line.split()\n          word=values[0]\n          vectors=np.asarray(values[1:],'float32')\n          embedding_dict[word]=vectors\n  f.close()\n  return embedding_dict","c2f7b210":"embedding_dict = embedding_vectors ('..\/input\/glove6b100dtxt\/glove.6B.100d.txt')","77786322":"# function to return an embedding matrix given a dictionnary\ndef matrix_embedding (dict_vocabulary) : \n  num_words=len(dict_vocabulary)+1\n  embedding_matrix=np.zeros((num_words,100)) \n\n  for word,i in tqdm(dict_vocabulary.items()):\n      if i > num_words:  \n          continue\n      \n      emb_vec=embedding_dict.get(word)\n      if emb_vec is not None:\n          embedding_matrix[i]=emb_vec\n  return embedding_matrix","6ea808ac":"embedding_matrix = matrix_embedding(count_vectorizer.vocabulary_)","45184f2f":"import os, sys, gc, warnings, random\nfrom sklearn.model_selection import StratifiedKFold\nfrom sklearn import preprocessing\nimport lightgbm as lgb  \nfrom catboost import CatBoostClassifier\nfrom sklearn.metrics import auc, classification_report, roc_auc_score\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import RandomForestClassifier","2e0ddff4":"# a class to fix the params \nclass CFG :\n  SEED = 42\n  n_splits = 5\n  catboost_params = {'learning_rate':0.05,'iterations':10000,'eval_metric':'AUC',\n                      'use_best_model' :True,'verbose':100,'random_seed': 0}\n  TARGET_COL = 'target'\n","aaab8a9c":"seed_everything(CFG.SEED)\n","17a0fbc2":"# return the Skf object \ndef Stratified_object (n_splits= CFG.n_splits,seed = CFG.SEED):\n  skf = StratifiedKFold(n_splits=CFG.n_splits,shuffle=True, random_state=CFG.SEED)\n  return (skf)\n","210fb923":"skf = Stratified_object ()","d298aea4":"# full training with skf\ndef Stratified_training (train,train_matrix,test_matrix,skf):\n  X , y = train_matrix , train['target']\n  oof_cat = np.zeros((train.shape[0],)) \n  test['target'] = 0\n  cat_preds= []\n  for fold_ , (trn_idx, val_idx) in enumerate(skf.split(X, y)):\n      print(50*'-')\n      print('Fold:',fold_+1)\n      X_train, y_train = X[trn_idx,:], y[trn_idx] \n      X_test, y_test = X[val_idx,:], y[val_idx] \n        \n      estimator = CatBoostClassifier(**CFG.catboost_params)\n      estimator.fit(X_train,y_train,\n                    eval_set =(X_test,y_test),early_stopping_rounds=200)\n      \n      y_pred_val = estimator.predict_proba(X_test)[:,1]\n      oof_cat[val_idx] = y_pred_val\n      y_pred_test = estimator.predict_proba(test_matrix)[:,1]\n      cat_preds.append(y_pred_test)\n      print(50*'-')\n      print()\n  print('OOF score :',roc_auc_score(y, oof_cat))\n  return cat_preds\n","c351ab79":"\ncat_preds = Stratified_training (train,train_matrix,test_matrix,skf)","3fb26c83":"catboost_preds = np.mean(cat_preds,axis=0)\n","33b72c14":"catboost_preds.shape","670ea310":"from sklearn.linear_model import ElasticNet, Lasso,  BayesianRidge, LassoLarsIC\nfrom sklearn.ensemble import RandomForestClassifier,  GradientBoostingClassifier\nfrom sklearn.kernel_ridge import KernelRidge\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nfrom sklearn.model_selection import KFold, cross_val_score, train_test_split\nfrom sklearn.metrics import mean_squared_error\nimport xgboost as xgb\nimport lightgbm as lgb","daa78395":"#Validation function\ndef rmsle(y, y_pred):\n    return np.sqrt(mean_squared_error(y, y_pred))","7c0561a3":"from sklearn.base import BaseEstimator, TransformerMixin, RegressorMixin, clone\nclass StackingAveragedModels(BaseEstimator, RegressorMixin, TransformerMixin):\n    def __init__(self, base_models, meta_model, n_folds=5):\n        self.base_models = base_models\n        self.meta_model = meta_model\n        self.n_folds = n_folds\n   \n    # We again fit the data on clones of the original models\n    def fit(self, X, y):\n        self.base_models_ = [list() for x in self.base_models]\n        self.meta_model_ = clone(self.meta_model)\n        kfold = KFold(n_splits=self.n_folds, shuffle=True, random_state=156)\n        \n        # Train cloned base models then create out-of-fold predictions\n        # that are needed to train the cloned meta-model\n        out_of_fold_predictions = np.zeros((X.shape[0], len(self.base_models)))\n        for i, model in enumerate(self.base_models):\n            for train_index, holdout_index in kfold.split(X, y):\n                instance = clone(model)\n                self.base_models_[i].append(instance)\n                instance.fit(X[train_index], y[train_index])\n                y_pred = instance.predict(X[holdout_index])\n                out_of_fold_predictions[holdout_index, i] = y_pred\n                \n        # Now train the cloned  meta-model using the out-of-fold predictions as new feature\n        self.meta_model_.fit(out_of_fold_predictions, y)\n        return self\n   \n    #Do the predictions of all base models on the test data and use the averaged predictions as \n    #meta-features for the final prediction which is done by the meta-model\n    def predict(self, X):\n        meta_features = np.column_stack([\n            np.column_stack([model.predict(X) for model in base_models]).mean(axis=1)\n            for base_models in self.base_models_ ])\n        return self.meta_model_.predict(meta_features)\n","a2b5dd73":"lasso = make_pipeline(RobustScaler(), Lasso(alpha =0.0005, random_state=1))\nENet = make_pipeline(RobustScaler(), ElasticNet(alpha=0.0005, l1_ratio=.9, random_state=3))\nKRR = KernelRidge(alpha=0.6, kernel='polynomial', degree=2, coef0=2.5)\nGBoost = GradientBoostingClassifier(n_estimators=3000, learning_rate=0.05,\n                                   max_depth=4, max_features='sqrt',\n                                   min_samples_leaf=15, min_samples_split=10, \n                                   loss='deviance', random_state =5)","90d7a184":"stacked_averaged_models = StackingAveragedModels(base_models = (ENet, GBoost, KRR),\n                                                 meta_model = lasso)","52cb4792":"stacked_averaged_models.fit(train_matrix, train['target'].values)\nstacked_train_pred = stacked_averaged_models.predict(train_matrix)\nstacked_pred = np.expm1(stacked_averaged_models.predict(test_matrix))\n","af77c93d":"print(rmsle(train['target'], stacked_train_pred))","c50cd11f":"import keras\nfrom keras.models import Sequential\nfrom keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\nfrom keras.initializers import Constant","6b2ea4d2":"class LSTM_Model(keras.Model):\n  def __init__(self,num_words,embedding_matrix ,MAX_LEN,nodes): \n    super(LSTM_Model, self).__init__()\n    self.embedding = Embedding(num_words,100,embeddings_initializer=Constant(embedding_matrix),\n                   input_length=MAX_LEN,trainable=False)\n    self.dropout = SpatialDropout1D(0.2)\n    self.block = LSTM(nodes, dropout=0.2, recurrent_dropout=0.2)\n    self.Classifier = Dense(2, activation='sigmoid') \n  def call(self,matrix): \n    x = self.embedding(matrix)\n    x = self.block(x)\n    x = self.dropout(x)\n    x = self.Classifier(x)\n    return (x)\n","6395ca09":"# model = LSTM_Model(len(count_vectorizer.vocabulary_)+1,MAX_LEN=50,nodes=64)","6d6e342b":"# model.compile(loss='binary_crossentropy',optimizer=keras.optimizers.Adam(0.1),metrics=['accuracy'])\n","5c04fa55":"# history=model.fit(X_train,y_train,batch_size=4,epochs=15,validation_data=(X_test,y_test),verbose=2)","6816b16c":"import tensorflow as tf\nfrom tensorflow.keras.layers import Dense, Input\nfrom tensorflow.keras.optimizers import Adam\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.callbacks import ModelCheckpoint\nimport tensorflow_hub as hub","8cc5439a":"!wget --quiet https:\/\/raw.githubusercontent.com\/tensorflow\/models\/master\/official\/nlp\/bert\/tokenization.py","7e11ab31":"import tokenization","5e4cd4ea":"def bert_encode(texts, tokenizer, max_len=512):\n    all_tokens = []\n    all_masks = []\n    all_segments = []\n    \n    for text in texts:\n        text = tokenizer.tokenize(text)\n            \n        text = text[:max_len-2]\n        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n        pad_len = max_len - len(input_sequence)\n        \n        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n        tokens += [0] * pad_len\n        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n        segment_ids = [0] * max_len\n        \n        all_tokens.append(tokens)\n        all_masks.append(pad_masks)\n        all_segments.append(segment_ids)\n    \n    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)","76c34db8":"def build_model(bert_layer, max_len=512):\n    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n\n    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n    clf_output = sequence_output[:, 0, :]\n    out = Dense(1, activation='sigmoid')(clf_output)\n    \n    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n    model.compile(Adam(lr=2e-6), loss='binary_crossentropy', metrics=['accuracy'])\n    \n    return model","ec6eeb37":"%%time\nmodule_url = \"https:\/\/tfhub.dev\/tensorflow\/bert_en_uncased_L-24_H-1024_A-16\/1\"\nbert_layer = hub.KerasLayer(module_url, trainable=True)","d7c0928d":"vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\ndo_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\ntokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)","fa1dc24c":"train_input = bert_encode(train.text.values, tokenizer, max_len=160)\ntest_input = bert_encode(test.text.values, tokenizer, max_len=160)\ntrain_labels = train.target.values","d22e46de":"model = build_model(bert_layer, max_len=160)\nmodel.summary()","1a2971ce":"# train_history = model.fit(\n#     train_input, train_labels,\n#     validation_split=0.2,\n#     epochs=3,\n#     batch_size=16\n# )\n\n# model.save('model.h5')","868228be":"# test_pred = model.predict(test_input)\n","533e8d2e":"### **11 . Modeling**","22297ee7":"*5. 5 Distribution of punctuation*","203d64b6":"**11 .2 Stacked Models**","f0926933":"*5 .4 Word Cloud*","48d9683f":"With Tfidftransformer you will systematically compute word counts using CountVectorizer and then compute the Inverse Document Frequency (IDF) values and only then compute the TF-IDF scores.","d9a5a9a8":"Let's take a look at the \"Nans\" of our data ","af86c6a6":"### **3 .Data Loading**","ac8ad6c6":"### **4 .Data anomaly detection**","2f4f4323":"*5 .1 Missing Values*","3f311325":"### **1 .Importations**","ecf515bd":"*5 .8 Other features*","890e56a4":"### **10 . Matrix_Embeddings**","1aed9018":"The distribution of both seems to be almost same.120 t0 140 characters in a tweet are the most common among both.","c2aff04e":"Starting by importing the useful libraries for this kernel ","26567347":"in order to reduce the dimensionality of our matrix ! [Feature matrix shape: Number of documents x Length of vocabulary ] we can carry out some Feature Selection, the process of selecting a subset of relevant variables. I will proceed as follows:\n\ntreat each category as binary (for example, the \u201cTech\u201d category is 1 for the Tech news and 0 for the others); perform a Chi-Square test to determine whether a feature and the (binary) target are independent; keep only the features with a certain p-value from the Chi-Square test.","639d67f4":"The Utils part: containing functions that will be useful for the whole kernel. The functions are in the order :  \n\nSeed_everything : is used to initialize the random number generator.\n\nload : used to load the data\n\nConcat : used to concatenate train and test to avoid redundancy in the data clean \n\ndivide_df :  function used to separate the train and the test after the data clean\n\nList of words : function that returns the list of words that form the tweets\n\nList_of_tweets : function that returns the list of all the tweets","25fd2fb1":"### **9  Matrix Reduction**","fe488eba":"*6.2 boxplot on the added features*","150e4f4c":"**9. 1 Catboost**","cbba1fcf":"*8 .4 Pipeline*","eb4545c8":"Even if I'm not good at spelling I can correct it with python :) I will use pyspellcheker to do that.","b7595fa4":"Tokenization is the process of tokenizing or splitting a string, text into a list of tokens. One can think of token as parts like a word is a token in a sentence, and a sentence is a token in a paragraph","30f55645":"Let's do some boxplot in order to have an idea on the added features","0766d1fb":"Both training and test set have same ratio of missing values in keyword and location.\n\n0.8% of keyword is missing in both training and test set\n33% of location is missing in both training and test set\nSince missing value ratios between training and test set are too close, they are most probably taken from the same sample. Missing values in those features are filled with no_keyword and no_location respectively.","c83751fa":"*7 .3 stemmer\/Lemmer*","b9469db4":"*6 .1 Meta features*","1a9fc915":"In our case I will apply Bert as indicated in the code but for more details you can consult these links for the explanation of the paper: \nBERT: Pre-training of Deep Bidirectional Transformers for Language Understanding\nhttps:\/\/www.youtube.com\/watch?v=-9evrZnBorM&t=745s\n","b1505f9c":"We can then proceed to the feature extraction phase. We will use 3 Feature extractors :\n\n8.1 Tokenizer\n\n8.2 CountVectorizer\n\n8.3 Tfidf","12b1388f":"As we know,twitter tweets always have to be cleaned before we go onto modelling.So we will do some basic cleaning such as spelling correction,removing punctuations,removing html tags and emojis etc.So let's start.","a6438226":"### **7 .Data Cleaning**","31da8c76":" The meta features used for the analysis are :\nword_count: number of words in text\n\nunique_word_count: number of unique words in text\n\nstop_word_count: number of stop words in text\n\nurl_count number: of urls in text\n\nmean_word_length: average character count in words\n\nchar_count number: of characters in text\n\npunctuation_count: number of punctuations in text\n\nhashtag_count: number of hashtags (#) in text\n\nmention_count number of mentions (@) in text\n","912d05c2":"*7 .2 Spelling Correction*","05134265":"### **6.  Adding features**","187b6f3a":"*5.6 Most commun words*","9381d6ac":"Welcome to you for the best kernel of this competition. Coded by me S\u00e9mi Ben Hsan (SBH) .Machine Learning Intern at TalTech - Tallinn University of Technology . This kernel includes all the advanced techniques of Natural Language processing (NLP), programmed in a professional way ( The SBH style ) and explained in detail . I am waiting for your votes, opinions and comments ... If you use parts of this notebook in your scripts\/notebooks, giving some kind of credit would be very much appreciated :) You can for instance link back to this notebook. Thanks!","4290bd2c":"In this approach, we add a meta-model on averaged base models and use the out-of-folds predictions of these base models to train our meta-model.\n\nThe procedure, for the training part, may be described as follows:\n\nSplit the total training set into two disjoint sets (here train and .holdout )\n\nTrain several base models on the first part (train)\n\nTest these base models on the second part (holdout)\n\nUse the predictions from 3) (called out-of-folds predictions) as the inputs, and the correct responses (target variable) as the outputs to train a higher level learner called meta-model.\n\nThe first three steps are done iteratively . If we take for example a 5-fold stacking , we first split the training data into 5 folds. Then we will do 5 iterations. In each iteration, we train every base model on 4 folds and predict on the remaining fold (holdout fold).\n\nSo, we will be sure, after 5 iterations , that the entire data is used to get out-of-folds predictions that we will then use as new feature to train our meta-model in the step 4.\n\nFor the prediction part , We average the predictions of all base models on the test data and used them as meta-features on which, the final prediction is done with the meta-model.","a1842ff5":"Why is Lemmatization better than Stemming? Stemming algorithm works by cutting the suffix from the word. In a broader sense cuts either the beginning or end of the word.","5e7fa6ad":"*5.8.2 Location feature*","744abc0b":"### **2 .Utils**","ff13189d":"*8 .1 Tokenizer*","a75ad7e5":"43\/100 Disaster tweets vs 57\/100 Non disaster tweets We have a balanced data ","ba21cffc":"Here I will use GloVe pretrained corpus model to represent our words.It is available in 3 varieties :50D ,100D and 200 Dimentional.I will try 100 D here.\n\n","be73083a":"Let's try some binging on the location feature","466c7d57":"*8 .2 CountVectorizer*","54319ffa":"*5 .7 Plotting most frequent bigrams*","e5faeb83":"CountVectorizer converts a collection of text documents to a matrix of token counts: the occurrences of tokens in each document. This implementation produces a sparse representation of the counts.","364aca75":"There are 18 unique tweets in training set which are labeled differently in their duplicates. Those tweets are probably labeled by different people and they interpreted the meaning differently because some of them are not very clear. Tweets with two unique target values are relabeled since they can affect the training score.","8c14a155":"The data clean phase is over. We can now separate the train and test. We use divide_df for that ","f1a5170d":"Here I will just introduce the use of LSTM coupled with Embeddings vectors ... You can continue with a lot of changes here... Adding more LSTM, GRU, or RNN layers... ","630c3e6a":"### **5 .visualizations**","f9b8c796":"*4 .1 Detecting the mislabeled tweets*","a4eca7ad":"**11 .4 Bert**","6d411559":"**0 .Introduction**","5d889d0d":"Let's have some fun and plot the word cloud for the disaster and the non disaster tweets","8fa54cbe":"*5 .2 % of 0 and 1 of the Target*","b1c90584":"**11 .3 LSTM**","286bafb6":"We start with the catboost model with the StratifiedKFold . A sofisticated implementation in SBH style that can inspire you in your future kernels ","c529534f":"I'll stop here with this block of tips. Many techniques can be combined to obtain a high quality solution. Don't forget to put an UPvote for SBH to continue updating this notebook with other useful tips ","9aeb927f":"*5 .8. 1 Keyword feature*","d900e942":"*7 .1 removing html tags and emojis etc*","4dd44dbf":"### **8 . Feature Extractors**","46b6549f":"*5 .3 caracter\/word_level distribution*","927a30ab":"**1 .Importations**\n\n**2 .Utils**\n\n**3 .Data Loading**\n\n**4 .Data anomaly detection**\n\n   *4 .1 Detecting the mislabeled tweets*\n   \n**5 .visualizations**\n\n  *5 .1 Missing Values*\n  \n  *5 .2 % of 0 and 1 of the Target*\n  \n  *5 .3 caracter\/word_level distribution*\n  \n  *5 .4 Word Cloud*\n  \n  *5. 5 Distribution of punctuation*\n  \n  *5.6 Most commun words*\n  \n  *5 .7 Plotting most frequent bigrams*\n \n  *5 .8 Other features*\n       \n       \n**6. Adding features**\n\n  *6 .1 Meta features*\n  \n  *6.2 boxplot on the added features*\n  \n**7 .Data Cleaning**\n\n  *7 .1 removing html tags and emojis etc*\n  \n  *7 .2 Spelling Correction*\n  \n  *7 .3 stemmer\/Lemmer*\n  \n**8 . Feature Extractors**\n\n  *8 .1 Tokenizer*\n  \n  *8 .2 CountVectorizer*\n  \n  *8 .3 Tfidf*\n  \n  *8 .4 Pipeline*\n  \n**9 Matrix Reduction**\n\n**10 . Matrix_Embeddings**\n\n**11 . Modeling**\n\n   *11. 1 Catboost*\n   \n   *11 .2 Stacked Models*\n   \n   *11 .3 LSTM*\n   \n   *11 .4 Bert*","a0ddf903":"You have the choice here, to use some clean functions, or the full clean function with all the functions coded before ","2b1f8b36":"All of the meta features have very similar distributions in training and test set which also proves that training and test set are taken from the same sample.\n\nAll of the meta features have information about target as well, but some of them are not good enough such as url_count, hashtag_count and mention_count.\n\nOn the other hand, word_count, unique_word_count, stop_word_count, mean_word_length, char_count, punctuation_count have very different distributions for disaster and non-disaster tweets. Those features might be useful in models","543f417c":"BERT is an open source machine learning framework for natural language processing (NLP). BERT is designed to help computers understand the meaning of ambiguous language in text by using surrounding text to establish context. The BERT framework was pre-trained using text from Wikipedia and can be fine-tuned with question and answer datasets.\n\nBERT, which stands for Bidirectional Encoder Representations from Transformers, is based on Transformers, a deep learning model in which every output element is connected to every input element, and the weightings between them are dynamically calculated based upon their connection. (In NLP, this process is called attention.)","4b4e4f3e":"*8 .3 Tfidf*"}}