{"cell_type":{"31565123":"code","64982716":"code","47b3d4e3":"code","60878f9b":"code","75875836":"code","3af71902":"code","30b98cbf":"code","7567f562":"code","324e15f1":"code","f7d42d9d":"code","b4120ae2":"code","b56ee7d0":"code","238cbe74":"code","c2bb7ab3":"code","9704a376":"code","c5a77b1c":"code","c06eea7f":"code","2dbd6805":"code","ead68f85":"code","42004fa7":"code","d01b950a":"code","b1bc8134":"code","ba3fbfd3":"code","a35063e5":"code","5e8a33f0":"code","62149d03":"code","78fbb925":"code","966a27f4":"code","a4ec8ea8":"code","ab8071de":"code","a5edee76":"code","9804c8f8":"code","7f569f8f":"code","3f1e7049":"code","f60e6b4b":"code","2cc47f56":"code","e325d332":"code","46294577":"code","5f75d58a":"code","b425a676":"code","bddb9a80":"code","b5b281da":"code","da6c9966":"code","807534be":"code","a8e9790a":"code","716d4e47":"code","4fd0ec4b":"code","af0b42fc":"code","293b57ec":"code","2ddd48e9":"code","b2eb3304":"code","1a67341f":"code","bbdab8de":"code","8be35601":"code","bad2eb87":"code","01e68386":"code","6ed3f952":"code","1d0fb113":"code","65d47c57":"code","578ed547":"code","c5d8e0a8":"code","1ae9b329":"code","9393d6fb":"code","142e63c7":"markdown","3eae8e45":"markdown","5736c963":"markdown","d6cbd61d":"markdown","3ea408fe":"markdown","3f9209cb":"markdown","2c44708e":"markdown","537672fa":"markdown","249b7630":"markdown","e3cfd1da":"markdown","e50ebee0":"markdown","0958dc70":"markdown","23f8d9ae":"markdown","b76e467c":"markdown","9f398422":"markdown","46798d75":"markdown","3e1accb5":"markdown","1bf86260":"markdown","887295d2":"markdown","cd56b8e7":"markdown","597b3097":"markdown","7a51dbf1":"markdown","522512d6":"markdown","adbd189b":"markdown","3bd2bac9":"markdown","6a5df9d5":"markdown","144a166f":"markdown","82441151":"markdown","cb6597a7":"markdown","32aa1c94":"markdown","001a937c":"markdown","3b0d39ee":"markdown","295eef85":"markdown","fd9cd1f4":"markdown","1c4f12d1":"markdown","896d5713":"markdown","1cb85729":"markdown","af4c0cd8":"markdown","af06bc1f":"markdown","8d25fb00":"markdown","2ce060ec":"markdown"},"source":{"31565123":"import re\nimport string\nimport numpy as np \nimport random\nimport pandas as pd \nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nfrom plotly import graph_objs as go\nimport plotly.express as px\nimport plotly.figure_factory as ff\nfrom collections import Counter\n\nfrom PIL import Image\nfrom wordcloud import WordCloud, STOPWORDS, ImageColorGenerator\n\n\nimport nltk\nfrom nltk.corpus import stopwords\n\nfrom tqdm import tqdm\nimport os\nimport nltk\nimport spacy\nimport random\nfrom spacy.util import compounding\nfrom spacy.util import minibatch\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","64982716":"def random_colours(number_of_colors):\n    '''\n    Simple function for random colours generation.\n    Input:\n        number_of_colors - integer value indicating the number of colours which are going to be generated.\n    Output:\n        Color in the following format: ['#E86DA4'] .\n    '''\n    colors = []\n    for i in range(number_of_colors):\n        colors.append(\"#\"+''.join([random.choice('0123456789ABCDEF') for j in range(6)]))\n    return colors","47b3d4e3":"import pandas as pd\nimport numpy as np\ntrain=pd.read_csv('\/kaggle\/input\/trump-tweets\/trumptweets.csv')\n","60878f9b":"train","75875836":"print(train.shape)\n","3af71902":"train.describe()","30b98cbf":"train.isna().sum()","7567f562":"train['hashtags'].unique()","324e15f1":"train['mentions'].unique()","f7d42d9d":"train=train.drop(['link','mentions','hashtags','geo'], axis=1)","b4120ae2":"train.head()","b56ee7d0":"def remove_spaces(text):\n    text=text.strip()\n    text=text.split()\n    return ' '.join(text)\n    ","238cbe74":"def edits1(word):\n    letters='abcdefghijklmnopqrstuvwxyz'\n    splits=[(word[:i], word[i:]) for i in range(len(word)+1)]\n    deletes=[L+R[1:] for L,R in splits if R]\n    transposes=[L+R[1] +R[0] + R[2:] for L,R in splits if len(R)>1]\n    replaces = [L+c+R[1:] for L,R in splits if R for c in letters]\n    inserts = [L+c+ R for L,R in splits for c in letters]\n    return set(deletes+transposes+replaces+inserts)\ndef edits2(word):\n    return(e2 for e1 in edits1(word) for e2 in edits1(e1))\n            ","c2bb7ab3":"contraction = {'cause':'because',\n              'aint': 'am not',\n              'aren\\'t': 'are not'}\n\ndef mapping_replacer(x,dic):\n    for words in dic.keys():\n        if ' ' + words + ' ' in x:\n            x=x.replace(' '+ words +' ' ,' '+dic[words]+' ' )\n    return x\n\n    ","9704a376":"import nltk\nnltk.download('punkit')\nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.wordnet import WordNetLemmatizer\nfrom nltk.stem.lancaster import LancasterStemmer\n\nnltk.LancasterStemmer\nls = LancasterStemmer()\nlem = WordNetLemmatizer()\ndef lexicon_normalization(text):\n    words = word_tokenize(text) \n    \n    \n    # 1- Stemming\n    words_stem = [ls.stem(w) for w in words]\n    \n    # 2- Lemmatization\n    words_lem = [lem.lemmatize(w) for w in words_stem]\n    return words_lem\n\n    ","c5a77b1c":"import emoji\nimport re \n#from emot.emo_unicode import UNICODE_EMO\ndef convert_emojis(text):\n    for emot in emoji.UNICODE_EMOJI:\n        text = re.sub(r'('+emot+')', \"_\".join(emoji.UNICODE_EMOJI[emot].replace(\",\",\"\").replace(\":\",\"\").split()), text)\n    return text","c06eea7f":"def clean_text(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    text = str(text).lower()\n    text = re.sub('\\[.*?\\]', '', text)\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    text = re.sub('<.*?>+', '', text)\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    text = re.sub('\\n', '', text)\n    text = re.sub('\\w*\\d\\w*', '', text)\n    text = re.sub('\\'','', text)\n    \n    return text","2dbd6805":"from collections import Counter\ndef remove_stopword(text):\n    stop_words = stopwords.words('english')\n    stopwords_dict = Counter(stop_words)\n    text = ' '.join([word for word in text.split() if word not in stopwords_dict])\n    return text","ead68f85":"def tokenise(text):\n    words = word_tokenize(text) \n    return words\n","42004fa7":"import re\ntrain['content'] = train['content'].map(lambda x: re.sub(r'\\W+', ' ', x))\ntrain['content'] = train['content'].replace(r'\\W+', ' ', regex=True)\n","d01b950a":"train.head()","b1bc8134":"train['content']=train['content'].apply(lambda x: mapping_replacer(x, contraction))","ba3fbfd3":"train['content'] = train['content'].apply(lambda x:clean_text(x))","a35063e5":"train['content']=train['content'].apply(lambda x: remove_stopword(x))\n","5e8a33f0":"train['content']=train['content'].apply(lambda x: lexicon_normalization(x))","62149d03":"train.head()","78fbb925":"top = Counter([item for sublist in train['content'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Blues')","966a27f4":"blacklist = ['http','https','www','com', 'ev','u','ly','pic','would']\n\ndef remove_words(text):\n    text = [i for i in text if (i not in blacklist)]\n    return text","a4ec8ea8":"#train['content']=remove_words(train['content'])\n\ntrain['content'] = train['content'].apply(lambda x: [i for i in x if i not in blacklist])","ab8071de":"fig = px.bar(temp, x=\"count\", y=\"Common_words\", title='Commmon Words in Selected Text', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","a5edee76":"top = Counter([item for sublist in train['content'] for item in sublist])\ntemp = pd.DataFrame(top.most_common(20))\ntemp = temp.iloc[1:,:]\ntemp.columns = ['Common_words','count']\ntemp.style.background_gradient(cmap='Purples')","9804c8f8":"fig = px.treemap(temp, path=['Common_words'], values='count',title='Tree of Most Common Words')\nfig.show()","7f569f8f":"from wordcloud import WordCloud, STOPWORDS , ImageColorGenerator\n\nfrom textblob import TextBlob\n\ndef get_tweet_sentiment(tweet): \n    ''' \n    Utility function to classify sentiment of passed tweet \n    using textblob's sentiment method \n    '''\n    # create TextBlob object of passed tweet text \n    analysis = TextBlob(tweet) \n    \n    # set sentiment \n    if analysis.sentiment.polarity > 0:\n        return 'positive'\n    elif analysis.sentiment.polarity == 0: \n        return 'neutral'\n    else: \n        return 'negative'","3f1e7049":"train['sentiment']=train['content'].apply(lambda x: get_tweet_sentiment(' '.join(x)))","f60e6b4b":"train.head()","2cc47f56":"Positive_sent = train[train['sentiment']=='positive']\nNegative_sent = train[train['sentiment']=='negative']\nNeutral_sent = train[train['sentiment']=='neutral']","e325d332":"print('Number of tweets with positive sentiment', Positive_sent['sentiment'].count())\nprint('Number of tweets with negative sentiment', Negative_sent['sentiment'].count())\nprint('Number of tweets with neutral sentiment', Neutral_sent['sentiment'].count())\n","46294577":"#MosT common positive words\ntop = Counter([item for sublist in Positive_sent['content'] for item in sublist])\ntemp_positive = pd.DataFrame(top.most_common(20))\ntemp_positive.columns = ['Common_words','count']\ntemp_positive.style.background_gradient(cmap='Greens')","5f75d58a":"import numpy as np\ntop = Counter([item for sublist in Positive_sent['content'] for item in sublist])\ntemp_positive = pd.DataFrame(top.most_common(23))\ntemp_positive.columns = ['Common_words','count']\ntemp_positive['Common_words'] = temp_positive['Common_words'].map(lambda x: re.sub(r'\\W+', '', x))\ntemp_positive['Common_words'] = temp_positive['Common_words'].replace(r'\\W+', '', regex=True)\ntemp_positive['Common_words'] = temp_positive['Common_words'].apply(lambda x:remove_spaces(x))\ntemp_positive=temp_positive[~temp_positive['Common_words'].isin(['s','gre','\u201c',' * '])] #new line removing meaningless words\nmask1 = temp_positive.Common_words.str.contains('[a-zA-Z]')\nmask2 = temp_positive.Common_words.notna()\ntemp_positive = temp_positive[mask1 | mask2]\ntemp_positive.Common_words =  temp_positive.Common_words.str.replace(r\"\\s+\", \"\").replace(\"\", np.NaN)\ntemp_positive=temp_positive.dropna()\n\n\ntemp_positive.style.background_gradient(cmap='Greens')\n","b425a676":"fig = px.bar(temp_positive, x=\"count\", y=\"Common_words\", title='Most Commmon Words in Positive Sentiment tweets', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","bddb9a80":"#MosT common negative words\ntop = Counter([item for sublist in Negative_sent['content'] for item in sublist])\ntemp_negative = pd.DataFrame(top.most_common(20))\ntemp_negative = temp_negative.iloc[1:,:]\ntemp_negative.columns = ['Common_words','count']\ntemp_negative.style.background_gradient(cmap='Reds')","b5b281da":"#MosT common negative words\ntop = Counter([item for sublist in Negative_sent['content'] for item in sublist])\ntemp_negative = pd.DataFrame(top.most_common(22))\ntemp_negative = temp_negative.iloc[1:,:]\ntemp_negative.columns = ['Common_words','count']\n\n#Data cleaning\ntemp_negative['Common_words'] = temp_negative['Common_words'].map(lambda x: re.sub(r'\\W+', '', x))\ntemp_negative['Common_words'] = temp_negative['Common_words'].replace(r'\\W+', '', regex=True)\ntemp_negative=temp_negative[~temp_negative['Common_words'].isin(['s','t'])] #new line removing meaningless words from above\n#mask1 = temp_negative.Common_words.str.contains('[a-zA-Z]')\n#mask2 = temp_negative.Common_words.notna()\n#temp_negative = temp_negative[mask1 | mask2]\n\ntemp_negative.Common_words =  temp_negative.Common_words.replace(\"\", np.nan)\ntemp_negative = temp_negative.dropna(subset=['Common_words'])\n\ntemp_negative.style.background_gradient(cmap='Reds')","da6c9966":"fig = px.treemap(temp_negative, path=['Common_words'], values='count',title='Tree Of Most Common Words in Negative Tweets')\nfig.show()","807534be":"#MosT common Neutral words\ntop = Counter([item for sublist in Neutral_sent['content'] for item in sublist])\ntemp_neutral = pd.DataFrame(top.most_common(20))\ntemp_neutral = temp_neutral.loc[1:,:]\ntemp_neutral.columns = ['Common_words','count']\ntemp_neutral.style.background_gradient(cmap='Reds')","a8e9790a":"\ntop = Counter([item for sublist in Neutral_sent['content'] for item in sublist])\ntemp_neutral = pd.DataFrame(top.most_common(20))\ntemp_neutral = temp_neutral.loc[1:,:]\ntemp_neutral.columns = ['Common_words','count']\n\n#Data cleaning\ntemp_neutral['Common_words'] = temp_neutral['Common_words'].map(lambda x: re.sub(r'\\W+', '', x))\ntemp_neutral['Common_words'] = temp_neutral['Common_words'].replace(r'\\W+', '', regex=True)\ntemp_neutral=temp_neutral[~temp_neutral['Common_words'].isin(['s'])] #new line removing meaningless words from above\n\ntemp_neutral.Common_words =  temp_neutral.Common_words.replace(\"\", np.nan)\ntemp_neutral = temp_neutral.dropna(subset=['Common_words'])\n\ntemp_neutral.style.background_gradient(cmap='Reds')","716d4e47":"fig = px.bar(temp_neutral, x=\"count\", y=\"Common_words\", title='Most Commmon Neutral Words', orientation='h', \n             width=700, height=700,color='Common_words')\nfig.show()","4fd0ec4b":"fig = px.treemap(temp_neutral, path=['Common_words'], values='count',title='Tree Of Most Common Neutral Words')\nfig.show()","af0b42fc":"raw_text = [word for word_list in train['content'] for word in word_list]","293b57ec":"def words_unique(sentiment,numwords,raw_words):\n    '''\n    Input:\n        segment - Segment category (ex. 'Neutral');\n        numwords - how many specific words do you want to see in the final result; \n        raw_words - list  for item in train_data[train_data.segments == segments]['temp_list1']:\n    Output: \n        dataframe giving information about the name of the specific ingredient and how many times it occurs in the chosen cuisine (in descending order based on their counts)..\n\n    '''\n    allother = []\n    for item in train[train.sentiment != sentiment]['content']:\n        for word in item:\n            allother .append(word)\n    allother  = list(set(allother ))\n    \n    specificnonly = [x for x in raw_text if x not in allother]\n    \n    mycounter = Counter()\n    \n    for item in train[train.sentiment == sentiment]['content']:\n        for word in item:\n            mycounter[word] += 1\n    keep = list(specificnonly)\n    \n    for word in list(mycounter):\n        if word not in keep:\n            del mycounter[word]\n    \n    Unique_words = pd.DataFrame(mycounter.most_common(numwords), columns = ['words','count'])\n    \n    return Unique_words","2ddd48e9":"Unique_Positive= words_unique('positive', 20, raw_text)\nprint(\"The top 20 unique words in Positive Tweets are:\")\nUnique_Positive.style.background_gradient(cmap='Greens')","b2eb3304":"Unique_Positive","1a67341f":"fig = px.treemap(Unique_Positive, path=['words'], values='count',title='Tree Of Unique Words in Positive sentiment tweets')\nfig.show()","bbdab8de":"import \nfrom palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.pie(Unique_Positive['count'], labels=Unique_Positive.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique words in Positive sentiment tweets')\nplt.show()","8be35601":"Unique_Negative= words_unique('negative', 10, raw_text)\nprint(\"The top 10 unique words in Negative Tweets are:\")\nUnique_Negative.style.background_gradient(cmap='Reds')","bad2eb87":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.rcParams['text.color'] = 'black'\nplt.pie(Unique_Negative['count'], labels=Unique_Negative.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique words in Negative sentiment tweets')\nplt.show()","01e68386":"Unique_Neutral= words_unique('neutral', 10, raw_text)\nprint(\"The top 10 unique words in Neutral Tweets are:\")\nUnique_Neutral.style.background_gradient(cmap='Oranges')","6ed3f952":"#Data cleaning\nUnique_Neutral= words_unique('neutral', 14, raw_text)\nUnique_Neutral['words'] = Unique_Neutral['words'].map(lambda x: re.sub(r'\\W+', '', x))\nUnique_Neutral['words'] = Unique_Neutral['words'].replace(r'\\W+', '', regex=True)\nUnique_Neutral['words']=Unique_Neutral[~Unique_Neutral['words'].isin(['\u0628\u0647','\u0631\u0627','\u0627\u06cc\u0631\u0627\u0646','\u0648'])] #new line removing meaningless words from above\n\nUnique_Neutral['words'] =  Unique_Neutral['words'].replace(\"\", np.nan)\nUnique_Neutral= Unique_Neutral.dropna(subset=['words'])\n\nUnique_Neutral.style.background_gradient(cmap='Reds')","1d0fb113":"from palettable.colorbrewer.qualitative import Pastel1_7\nplt.figure(figsize=(16,10))\nmy_circle=plt.Circle((0,0), 0.7, color='white')\nplt.pie(Unique_Neutral['count'], labels=Unique_Neutral.words, colors=Pastel1_7.hex_colors)\np=plt.gcf()\np.gca().add_artist(my_circle)\nplt.title('DoNut Plot Of Unique words in Neutral sentiment tweets')\nplt.show()","65d47c57":"def plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), color = 'white',\n                   title = None, title_size=40, image_color=False):\n    \n    wordcloud = WordCloud(background_color=color,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=400, \n                    height=200,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \nd = '\/kaggle\/input\/trump-tweets\/'","578ed547":"Neutral_sent","c5d8e0a8":"pos_mask = np.array(Image.open('\/kaggle\/input\/tweet-mask\/tweet_mask.png'))\nplot_wordcloud(Neutral_sent.content,mask=pos_mask,color='white',max_font_size=100,title_size=30,title=\"WordCloud of Neutral Tweets\")","1ae9b329":"plot_wordcloud(Positive_sent.content,mask=pos_mask,title=\"Word Cloud Of Positive tweets\",title_size=30)","9393d6fb":"plot_wordcloud(Negative_sent.content,mask=pos_mask,title=\"Word Cloud of Negative Tweets\",color='white',title_size=30)","142e63c7":"<div id = 6> <h2> 6. Applying data cleaning steps to data <\/h2> <\/div>","3eae8e45":"<b> Handling emojis <\/b>","5736c963":"<div id=9><h2>9. Number of Unique Words in tweets of each type of sentiment <\/h2> <\/div>\n\nWe will look at number of unique words in each type of tweet with different sentiments:\n* Positive\n* Negative\n* Neutral","d6cbd61d":"<div id=8.3><h3>8.3 Finding the common words for negative sentiment tweets <\/div><\/h3>","3ea408fe":"<div id=1> <h2>  1. Importing packages and libraries  <\/h2> <\/div> ","3f9209cb":"<b> Handling stopwords <\/b>","2c44708e":"<div id=2> <h2> 2. Reading the Data  <\/h2><\/div> ","537672fa":"  <div id=9.3><h2>9.3. Number of unique words in tweets with neutral sentiments  <\/h2> <\/div>","249b7630":"<b><i>So we see that overall the tweets are neutral in nature, followed by positive sentiment for the time in which they are analysed.<\/i><\/b>","e3cfd1da":"  <div id=9.1><h2>9.1. Number of unique words in tweets with positive sentiment  <\/h2> <\/div>","e50ebee0":"**Removing weird spaces**","0958dc70":"<div id = 8><h2>8. Finding Most common words Sentiments Wise<\/h2><\/div>\n\nLet's look at the most common words in different sentiments","23f8d9ae":"<b> Contraction <\/b>","b76e467c":"<b> Overall tweet sentiment <\/b>","9f398422":"Geo column has all values as nulls, hashtags have lot of null values, mentions also have lot of null values.","46798d75":"<div id=10.3><h2>10.3 WordCloud for Tweets with negative sentiment<\/h2> <\/div>","3e1accb5":"1. hashtags: This column does not have something very significant for us to analyse. We have mostly words like #ixzz4 etc which make no sense and we wont be able to analyse their sentiment. So we can get rid of this column\n2. mentions  too does not have anything siginificant for us to analyse. Hence we can do away with that too. Let us drop columns which dont have any siginificantly useful information.\n3. geo anyways does not have any value, all values are null\n4. mentions usually mention another person and we wont really get any sentiment by analysing that","1bf86260":"<div id=10><h2>10.1. WordCloud for neutral sentiment<\/h2> <\/div>","887295d2":"<div id=8.4><h3>8.4 Finding the common words for neutral sentiment tweets <\/div><\/h3>","cd56b8e7":"<div id=8.2><h3>8.2 Finding common words for positive sentiment tweets<\/div><\/h3>","597b3097":"Cleaning Regex Expressions from data","7a51dbf1":"<b>Removing links, brackets, numbers, punctuations etc. <\/b>\n","522512d6":"<h4>If you liked my kernel, please be kind to upvote it :). It will help my efforts :)<\/h4>","adbd189b":"So We have 27486 tweets in the train set and 3535 tweets in the test set","3bd2bac9":"<b> Spelling Correction <\/b>","6a5df9d5":"  <div id=9.2><h2>9.2. Number of unique words in tweets with negative sentiment  <\/h2> <\/div>","144a166f":"<div id=10.2><h2>10.2 WordCloud for positive sentiment<\/h2> <\/div>\n","82441151":"<div id=3> <h2> 3. Handling null values <\/h2> <\/div>","cb6597a7":"**By Looking at the Unique Words of each sentiment,we now have much more clarity about the data**\n\nWe shall now proceed to create word clouds, but we shall make word cloud of all words and not just existing words since unique words are very less in number for many sentiments (10,7,9 etc.) , so an effective word cloud can be only made by considering all the words in different sentiment tweets","32aa1c94":"Since the count for positive, negative and neutral sentiments is so less, there is no use of creating word clouds with unique words. So we will create a wordcloud with all the words combined.","001a937c":"Since we have words like 's', 'gre' which do not really mean anything, we will remove them and find the most common words","3b0d39ee":"<div id=8.1><h3>8.1 Finding the tweet sentiment <\/div><\/h3>","295eef85":"<b> Tokenisation <\/b>","fd9cd1f4":"<div id=5> <h2>5. Cleaning the data <\/h2> <\/div>\n\nLet's first clean the data, remove stopwords etc and perform basic pre-processing","1c4f12d1":"<div id=7><h2>7. Finding the most Common words in our Text <\/h2><\/div>","896d5713":"# Aim of this notebook\n\n- How to analyse tweets\n- How to do sentiment analysis\n- How to make a wordcloud\n\n# About the data\n\nDataset consists of tweets on Trump.\n\n\n# About this Notebook\n\n- <a href =#1> 1. Importing packages and libraries  <\/a>\n- <a href =#2> 2. Reading the data <\/a>\n- <a href =#3> 3. Handling null values <\/a>\n- <a href =#4> 4. Exploratory data analysis <\/a>\n- <a href =#5> 5. Cleaning the data <\/a>\n- <a href =#6> 6. Applying data cleaning steps to data<\/a>\n- <a href =#7> 7. Finding the most Common words in our Text<\/a>\n- <a href =#8> 8.  Finding Most common words Sentiments Wise  <\/a>\n    - <a href =#8.1> 8.1.  Finding the tweet sentiment<\/a>\n    - <a href =#8.2> 8.2. Finding common words for positive sentiment tweets <\/a>\n    - <a href =#8.3> 8.3. Finding the common words for negative sentiment tweets  <\/a>\n    - <a href =#8.4> 8.4. Finding the common words for neutral sentiment tweets <\/a>\n- <a href =#9> 9. Number of Unique Words in tweets of each type of sentiment  <\/a>\n    - <a href =#9.1> 9.1. Number of unique words in tweets with positive sentiment <\/a>\n    - <a href =#9.2> 9.2. Number of unique words in tweets with negative sentimen <\/a>\n    - <a href =#9.3> 9.3. Number of unique words in tweets with neutral sentimen <\/a>\n- <a href =#10> 10. Wordclouds  <\/a>\n    - <a href =#10.1> 10.1.  WordCloud for neutral sentiment<\/a>\n    - <a href =#10.2> 10.2.  WordCloud for positive sentiment<\/a>\n    - <a href =#10.3> 10.3.  WordCloud for negative sentiment<\/a>\n\n<br><br>\nIn case you are just starting with NLP here is a guide to Approach almost any NLP Problem by Grandmaster @Abhishek Thakur\nhttps:\/\/www.slideshare.net\/abhishekkrthakur\/approaching-almost-any-nlp-problem\n\n\n<b> This kernel is a work in Progress,and I will keep on updating it as I learn more and more<\/b>\n\n**<span style=\"color:Red\">If you find this kernel useful, Please Upvote it , it motivates me to write more Quality content**","1cb85729":"<b> Stemming, lemmetisation and tokenisation\n<\/b>","af4c0cd8":"**Below is a helper Function which generates random colors which can be used to give different colors to your plots.Feel free to use it**","af06bc1f":"Lets look at the distribution of reviews in the train set","8d25fb00":"<div id=4> <h2> 4. EDA  <\/h2><\/div>","2ce060ec":"<div id=10><h2>10. WordClouds<\/h2> <\/div>\n\n\nWe will be building three types of wordclouds:\n\n* WordCloud of Neutral Tweets\n* WordCloud of Positive Tweets\n* WordCloud of Negative Tweets"}}