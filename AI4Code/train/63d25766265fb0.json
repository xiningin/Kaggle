{"cell_type":{"1af24fdc":"code","54f8e38c":"code","bdbe3164":"code","7bf02b98":"code","0b1bce37":"code","94e8ae97":"code","34d459d9":"code","a0005ca3":"code","709713b0":"code","1b210ba3":"code","89113097":"code","db476f8d":"code","d1b68c6d":"code","c3a1f652":"code","c22ebde1":"code","c3aae167":"code","66dbde27":"code","8dcd8ba4":"code","13213256":"code","a60600ff":"code","c1204c97":"code","7707f994":"code","38248f6d":"code","a077295d":"code","0738bd9c":"code","820b3046":"code","f8338b24":"code","a69ea2bc":"markdown","a487bdf0":"markdown","bcaf9240":"markdown","ed99436e":"markdown","75f9cf4b":"markdown","6571afc6":"markdown","021f19f3":"markdown","5cd02ef6":"markdown","e301532a":"markdown","2565332b":"markdown","ac6dcac1":"markdown","ecdb4573":"markdown","47f644fb":"markdown","3683ddfc":"markdown","55d0174d":"markdown","7df114e3":"markdown","837fefca":"markdown","e5ccff6c":"markdown","e0ff06c9":"markdown","8696a1c2":"markdown","613619d5":"markdown","69c81dc0":"markdown","1f582862":"markdown","377b8a0a":"markdown","6b5f5c08":"markdown","418dd129":"markdown","9951a71f":"markdown","2437a54a":"markdown","1701dff4":"markdown","e4eca5a5":"markdown","44a371f1":"markdown","87306062":"markdown","40962898":"markdown","381e69fb":"markdown","8cf92dad":"markdown","6e69e4e6":"markdown","d6455f9d":"markdown","a6ac6c13":"markdown","4aef3279":"markdown","0226dd37":"markdown","2f103d3c":"markdown","3a434691":"markdown","4066f982":"markdown","63ef5485":"markdown","ef7a6057":"markdown"},"source":{"1af24fdc":"%%writefile random_forest_random.py\nimport random\nimport pandas as pd\nimport numpy as np\nfrom sklearn.ensemble import StackingClassifier\n\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\n\nfrom sklearn.tree import DecisionTreeClassifier\nimport xgboost as xgb\nfrom sklearn.ensemble import GradientBoostingClassifier\nactions =  np.empty((0,0), dtype = int)\nobservations =  np.empty((0,0), dtype = int)\ntotal_reward = 0\n\ndef random_forest_random(observation, configuration):\n    global actions, observations, total_reward\n    \n    if observation.step == 0:\n        action = random.randint(0,2)\n        actions = np.append(actions , [action])\n        return action\n    \n    if observation.step == 1:\n        action = random.randint(0,2)\n        actions = np.append(actions , [action])\n        observations = np.append(observations , [observation.lastOpponentAction])\n        # Keep track of score\n        winner = int((3 + actions[-1] - observation.lastOpponentAction) % 3);\n        if winner == 1:\n            total_reward = total_reward + 1\n        elif winner == 2:\n            total_reward = total_reward - 1        \n        return action\n\n    # Get Observation to make the tables (actions & obervations) even.\n    observations = np.append(observations , [observation.lastOpponentAction])\n    \n    # Prepare Data for training\n    # :-1 as we dont have feedback yet.\n    X_train = np.vstack((actions[:-1], observations[:-1])).T\n    \n    # Create Y by rolling observations to bring future a step earlier \n    shifted_observations = np.roll(observations, -1)\n    \n    # trim rolled & last element from rolled observations\n    y_train = shifted_observations[:-1].T\n    \n    # Set the history period. Long chains here will need a lot of time\n    if len(X_train) > 25:\n        random_window_size = 10 + random.randint(0,10)\n        X_train = X_train[-random_window_size:]\n        y_train = y_train[-random_window_size:]\n   \n    # Train a classifier model\n    model = ExtraTreesClassifier(n_estimators=10,warm_start=True,n_jobs=3)\n    model.fit(X_train, y_train)\n    \n    # Predict\n    X_test = np.empty((0,0), dtype = int)\n    X_test = np.append(X_test, [int(actions[-1]), observation.lastOpponentAction])\n    prediction = model.predict(X_test.reshape(1, -1))\n\n    # Keep track of score\n    winner = int((3 + actions[-1] - observation.lastOpponentAction) % 3);\n    if winner == 1:\n        total_reward = total_reward + 1\n    elif winner == 2:\n        total_reward = total_reward - 1\n   \n    # Prepare action\n    action = int((prediction + 1) % 3)\n    \n    # If losing a bit then change strategy and break the patterns by playing a bit random\n    if total_reward < -2:\n        win_tie = random.randint(0,1)\n        action = int((prediction + win_tie) % 3)\n\n    # Update actions\n    actions = np.append(actions , [action])\n\n    # Action \n    return action ","54f8e38c":"%%writefile hit_the_last_own_action.py\n\nmy_last_action = 0\n\ndef hit_the_last_own_action(observation, configuration):\n    global my_last_action\n    my_last_action = (my_last_action + 1) % 3\n    \n    return my_last_action","bdbe3164":"%%writefile rock.py\n\ndef rock(observation, configuration):\n    return 0","7bf02b98":"%%writefile paper.py\n\ndef paper(observation, configuration):\n    return 1\n","0b1bce37":"%%writefile scissors.py\n\ndef scissors(observation, configuration):\n    return 2","94e8ae97":"%%writefile copy_opponent.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\ndef copy_opponent(observation, configuration):\n    if observation.step > 0:\n        return observation.lastOpponentAction\n    else:\n        return random.randrange(0, configuration.signs)","34d459d9":"%%writefile reactionary.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\nlast_react_action = None\n\n\ndef reactionary(observation, configuration):\n    global last_react_action\n    if observation.step == 0:\n        last_react_action = random.randrange(0, configuration.signs)\n    elif get_score(last_react_action, observation.lastOpponentAction) <= 1:\n        last_react_action = (observation.lastOpponentAction + 1) % configuration.signs\n\n    return last_react_action","a0005ca3":"%%writefile counter_reactionary.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\nlast_counter_action = None\n\n\ndef counter_reactionary(observation, configuration):\n    global last_counter_action\n    if observation.step == 0:\n        last_counter_action = random.randrange(0, configuration.signs)\n    elif get_score(last_counter_action, observation.lastOpponentAction) == 1:\n        last_counter_action = (last_counter_action + 2) % configuration.signs\n    else:\n        last_counter_action = (observation.lastOpponentAction + 1) % configuration.signs\n\n    return last_counter_action","709713b0":"%%writefile statistical.py\n\nimport random\nfrom kaggle_environments.envs.rps.utils import get_score\n\naction_histogram = {}\n\ndef statistical(observation, configuration):\n    global action_histogram\n    if observation.step == 0:\n        action_histogram = {}\n        return\n    action = observation.lastOpponentAction\n    if action not in action_histogram:\n        action_histogram[action] = 0\n    action_histogram[action] += 1\n    mode_action = None\n    mode_action_count = None\n    for k, v in action_histogram.items():\n        if mode_action_count is None or v > mode_action_count:\n            mode_action = k\n            mode_action_count = v\n            continue\n\n    return (mode_action + 1) % configuration.signs","1b210ba3":"%%writefile nash_equilibrium.py\n\nimport random\n\ndef nash_equilibrium(observation, configuration):\n    return random.randint(0, 2)","89113097":"%%writefile markov_agent.py\n\nimport numpy as np\nimport collections\n\ndef markov_agent(observation, configuration):\n    k = 2\n    global table, action_seq\n    if observation.step % 250 == 0: # refresh table every 250 steps\n        action_seq, table = [], collections.defaultdict(lambda: [1, 1, 1])    \n    if len(action_seq) <= 2 * k + 1:\n        action = int(np.random.randint(3))\n        if observation.step > 0:\n            action_seq.extend([observation.lastOpponentAction, action])\n        else:\n            action_seq.append(action)\n        return action\n    # update table\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    table[key][observation.lastOpponentAction] += 1\n    # update action seq\n    action_seq[:-2] = action_seq[2:]\n    action_seq[-2] = observation.lastOpponentAction\n    # predict opponent next move\n    key = ''.join([str(a) for a in action_seq[:-1]])\n    if observation.step < 500:\n        next_opponent_action_pred = np.argmax(table[key])\n    else:\n        scores = np.array(table[key])\n        next_opponent_action_pred = np.random.choice(3, p=scores\/scores.sum()) # add stochasticity for second part of the game\n    # make an action\n    action = (next_opponent_action_pred + 1) % 3\n    # if high probability to lose -> let's surprise our opponent with sudden change of our strategy\n    if observation.step > 900:\n        action = next_opponent_action_pred\n    action_seq[-1] = action\n    return int(action)","db476f8d":"%%writefile memory_patterns.py\n\nimport random\n\n# how many steps in a row are in the pattern (multiplied by two)\nmemory_length = 6\n# current memory of the agent\ncurrent_memory = []\n# list of memory patterns\nmemory_patterns = []\n\ndef find_pattern(memory):\n    \"\"\" find appropriate pattern in memory \"\"\"\n    for pattern in memory_patterns:\n        actions_matched = 0\n        for i in range(memory_length):\n            if pattern[\"actions\"][i] == memory[i]:\n                actions_matched += 1\n            else:\n                break\n        # if memory fits this pattern\n        if actions_matched == memory_length:\n            return pattern\n    # appropriate pattern not found\n    return None\n\ndef my_agent(obs, conf):\n    \"\"\" your ad here \"\"\"\n    # if it's not first step, add opponent's last action to agent's current memory\n    if obs[\"step\"] > 0:\n        current_memory.append(obs[\"lastOpponentAction\"])\n    # if length of current memory is bigger than necessary for a new memory pattern\n    if len(current_memory) > memory_length:\n        # get momory of the previous step\n        previous_step_memory = current_memory[:memory_length]\n        previous_pattern = find_pattern(previous_step_memory)\n        if previous_pattern == None:\n            previous_pattern = {\n                \"actions\": previous_step_memory.copy(),\n                \"opp_next_actions\": [\n                    {\"action\": 0, \"amount\": 0, \"response\": 1},\n                    {\"action\": 1, \"amount\": 0, \"response\": 2},\n                    {\"action\": 2, \"amount\": 0, \"response\": 0}\n                ]\n            }\n            memory_patterns.append(previous_pattern)\n        for action in previous_pattern[\"opp_next_actions\"]:\n            if action[\"action\"] == obs[\"lastOpponentAction\"]:\n                action[\"amount\"] += 1\n        # delete first two elements in current memory (actions of the oldest step in current memory)\n        del current_memory[:2]\n    my_action = random.randint(0, 2)\n    pattern = find_pattern(current_memory)\n    if pattern != None:\n        my_action_amount = 0\n        for action in pattern[\"opp_next_actions\"]:\n            # if this opponent's action occurred more times than currently chosen action\n            # or, if it occured the same amount of times, choose action randomly among them\n            if (action[\"amount\"] > my_action_amount or\n                    (action[\"amount\"] == my_action_amount and random.random() > 0.5)):\n                my_action_amount = action[\"amount\"]\n                my_action = action[\"response\"]\n    current_memory.append(my_action)\n    return my_action","d1b68c6d":"%%writefile multi_armed_bandit.py\n\n\nimport pandas as pd\nimport numpy as np\nimport json\n\n\n# base class for all agents, random agent\nclass agent():\n    def initial_step(self):\n        return np.random.randint(3)\n    \n    def history_step(self, history):\n        return np.random.randint(3)\n    \n    def step(self, history):\n        if len(history) == 0:\n            return int(self.initial_step())\n        else:\n            return int(self.history_step(history))\n    \n# agent that returns (previousCompetitorStep + shift) % 3\nclass mirror_shift(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def history_step(self, history):\n        return (history[-1]['competitorStep'] + self.shift) % 3\n    \n    \n# agent that returns (previousPlayerStep + shift) % 3\nclass self_shift(agent):\n    def __init__(self, shift=0):\n        self.shift = shift\n    \n    def history_step(self, history):\n        return (history[-1]['step'] + self.shift) % 3    \n\n\n# agent that beats the most popular step of competitor\nclass popular_beater(agent):\n    def history_step(self, history):\n        counts = np.bincount([x['competitorStep'] for x in history])\n        return (int(np.argmax(counts)) + 1) % 3\n\n    \n# agent that beats the agent that beats the most popular step of competitor\nclass anti_popular_beater(agent):\n    def history_step(self, history):\n        counts = np.bincount([x['step'] for x in history])\n        return (int(np.argmax(counts)) + 2) % 3\n    \n    \n# simple transition matrix: previous step -> next step\nclass transition_matrix(agent):\n    def __init__(self, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type = 'step' \n        else:\n            self.step_type = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        \n    def history_step(self, history):\n        matrix = np.zeros((3,3)) + self.init_value\n        for i in range(len(history) - 1):\n            matrix = (matrix - self.init_value) \/ self.decay + self.init_value\n            matrix[int(history[i][self.step_type]), int(history[i+1][self.step_type])] += 1\n\n        if  self.deterministic:\n            step = np.argmax(matrix[int(history[-1][self.step_type])])\n        else:\n            step = np.random.choice([0,1,2], p = matrix[int(history[-1][self.step_type])]\/matrix[int(history[-1][self.step_type])].sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n    \n\n# similar to the transition matrix but rely on both previous steps\nclass transition_tensor(agent):\n    \n    def __init__(self, deterministic = False, counter_strategy = False, init_value = 0.1, decay = 1):\n        self.deterministic = deterministic\n        self.counter_strategy = counter_strategy\n        if counter_strategy:\n            self.step_type1 = 'step' \n            self.step_type2 = 'competitorStep'\n        else:\n            self.step_type2 = 'step' \n            self.step_type1 = 'competitorStep'\n        self.init_value = init_value\n        self.decay = decay\n        \n    def history_step(self, history):\n        matrix = np.zeros((3,3, 3)) + 0.1\n        for i in range(len(history) - 1):\n            matrix = (matrix - self.init_value) \/ self.decay + self.init_value\n            matrix[int(history[i][self.step_type1]), int(history[i][self.step_type2]), int(history[i+1][self.step_type1])] += 1\n\n        if  self.deterministic:\n            step = np.argmax(matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])])\n        else:\n            step = np.random.choice([0,1,2], p = matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])]\/matrix[int(history[-1][self.step_type1]), int(history[-1][self.step_type2])].sum())\n        \n        if self.counter_strategy:\n            # we predict our step using transition matrix (as competitor can do) and beat probable competitor step\n            return (step + 2) % 3 \n        else:\n            # we just predict competitors step and beat it\n            return (step + 1) % 3\n\n    \nagents = {\n    'mirror_0': mirror_shift(0),\n    'mirror_1': mirror_shift(1),  \n    'mirror_2': mirror_shift(2),\n    'self_0': self_shift(0),\n    'self_1': self_shift(1),  \n    'self_2': self_shift(2),\n    'popular_beater': popular_beater(),\n    'anti_popular_beater': anti_popular_beater(),\n    'random_transitison_matrix': transition_matrix(False, False),\n    'determenistic_transitison_matrix': transition_matrix(True, False),\n    'random_self_trans_matrix': transition_matrix(False, True),\n    'determenistic_self_trans_matrix': transition_matrix(True, True),\n    'random_transitison_tensor': transition_tensor(False, False),\n    'determenistic_transitison_tensor': transition_tensor(True, False),\n    'random_self_trans_tensor': transition_tensor(False, True),\n    'determenistic_self_trans_tensor': transition_tensor(True, True),\n    \n    'random_transitison_matrix_decay': transition_matrix(False, False, decay = 1.05),\n    'random_self_trans_matrix_decay': transition_matrix(False, True, decay = 1.05),\n    'random_transitison_tensor_decay': transition_tensor(False, False, decay = 1.05),\n    'random_self_trans_tensor_decay': transition_tensor(False, True, decay = 1.05),\n    \n    'determenistic_transitison_matrix_decay': transition_matrix(True, False, decay = 1.05),\n    'determenistic_self_trans_matrix_decay': transition_matrix(True, True, decay = 1.05),\n    'determenistic_transitison_tensor_decay': transition_tensor(True, False, decay = 1.05),\n    'determenistic_self_trans_tensor_decay': transition_tensor(True, True, decay = 1.05),\n}\n\n    \ndef multi_armed_bandit_agent (observation, configuration):\n    \n    # bandits' params\n    step_size = 2 # how much we increase a and b \n    decay_rate = 1.05 # how much do we decay old historical data\n    \n    # I don't see how to use any global variables, so will save everything to a CSV file\n    # Using pandas for this is too much, but it can be useful later and it is convinient to analyze\n    def save_history(history, file = 'history.csv'):\n        pd.DataFrame(history).to_csv(file, index = False)\n\n    def load_history(file = 'history.csv'):\n        return pd.read_csv(file).to_dict('records')\n    \n    \n    def log_step(step = None, history = None, agent = None, competitorStep = None):\n        if step is None:\n            step = np.random.randint(3)\n        if history is None:\n            history = []\n        history.append({'step': step, 'competitorStep': competitorStep, 'agent': agent})\n        save_history(history)\n        return step\n    \n    def update_competitor_step(history, competitorStep):\n        history[-1]['competitorStep'] = int(competitorStep)\n        return history\n        \n    \n    # load history\n    if observation.step == 0:\n        history = []\n        bandit_state = {k:[1,1] for k in agents.keys()}\n    else:\n        history = update_competitor_step(load_history(), observation.lastOpponentAction)\n        \n        # load the state of the bandit\n        with open('bandit.json') as json_file:\n            bandit_state = json.load(json_file)\n        \n        # updating bandit_state using the result of the previous step\n        # we can update all states even those that were not used\n        for name, agent in agents.items():\n            agent_step = agent.step(history[:-1])\n            bandit_state[name][1] = (bandit_state[name][1] - 1) \/ decay_rate + 1\n            bandit_state[name][0] = (bandit_state[name][0] - 1) \/ decay_rate + 1\n            \n            if (history[-1]['competitorStep'] - agent_step) % 3 == 1:\n                bandit_state[name][1] += step_size\n            elif (history[-1]['competitorStep'] - agent_step) % 3 == 2:\n                bandit_state[name][0] += step_size\n            else:\n                bandit_state[name][0] += step_size\/2\n                bandit_state[name][1] += step_size\/2\n            \n    with open('bandit.json', 'w') as outfile:\n        json.dump(bandit_state, outfile)\n    \n    \n    # generate random number from Beta distribution for each agent and select the most lucky one\n    best_proba = -1\n    best_agent = None\n    for k in bandit_state.keys():\n        proba = np.random.beta(bandit_state[k][0],bandit_state[k][1])\n        if proba > best_proba:\n            best_proba = proba\n            best_agent = k\n        \n    step = agents[best_agent].step(history)\n    \n    return log_step(step, history, best_agent)","c3a1f652":"%%writefile opponent_transition_matrix.py\n\nimport numpy as np\nimport pandas as pd\nimport random\n\nT = np.zeros((3, 3))\nP = np.zeros((3, 3))\n\n# a1 is the action of the opponent 1 step ago\n# a2 is the action of the opponent 2 steps ago\na1, a2 = None, None\n\ndef transition_agent(observation, configuration):\n    global T, P, a1, a2\n    if observation.step > 1:\n        a1 = observation.lastOpponentAction\n        T[a2, a1] += 1\n        P = np.divide(T, np.maximum(1, T.sum(axis=1)).reshape(-1, 1))\n        a2 = a1\n        if np.sum(P[a1, :]) == 1:\n            return int((np.random.choice(\n                [0, 1, 2],\n                p=P[a1, :]\n            ) + 1) % 3)\n        else:\n            return int(np.random.randint(3))\n    else:\n        if observation.step == 1:\n            a2 = observation.lastOpponentAction\n        return int(np.random.randint(3))","c22ebde1":"%%writefile decision_tree_classifier.py\n\nimport numpy as np\nimport collections\nfrom sklearn.tree import DecisionTreeClassifier\n\ndef construct_local_features(rollouts):\n    features = np.array([[step % k for step in rollouts['steps']] for k in (2, 3, 5)])\n    features = np.append(features, rollouts['steps'])\n    features = np.append(features, rollouts['actions'])\n    features = np.append(features, rollouts['opp-actions'])\n    return features\n\ndef construct_global_features(rollouts):\n    features = []\n    for key in ['actions', 'opp-actions']:\n        for i in range(3):\n            actions_count = np.mean([r == i for r in rollouts[key]])\n            features.append(actions_count)\n    \n    return np.array(features)\n\ndef construct_features(short_stat_rollouts, long_stat_rollouts):\n    lf = construct_local_features(short_stat_rollouts)\n    gf = construct_global_features(long_stat_rollouts)\n    features = np.concatenate([lf, gf])\n    return features\n\ndef predict_opponent_move(train_data, test_sample):\n    classifier = DecisionTreeClassifier(random_state=42)\n    classifier.fit(train_data['x'], train_data['y'])\n    return classifier.predict(test_sample)\n\ndef update_rollouts_hist(rollouts_hist, last_move, opp_last_action):\n    rollouts_hist['steps'].append(last_move['step'])\n    rollouts_hist['actions'].append(last_move['action'])\n    rollouts_hist['opp-actions'].append(opp_last_action)\n    return rollouts_hist\n\ndef warmup_strategy(observation, configuration):\n    global rollouts_hist, last_move\n    action = int(np.random.randint(3))\n    if observation.step == 0:\n        last_move = {'step': 0, 'action': action}\n        rollouts_hist = {'steps': [], 'actions': [], 'opp-actions': []}\n    else:\n        rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n        last_move = {'step': observation.step, 'action': action}\n    return int(action)\n\ndef init_training_data(rollouts_hist, k):\n    for i in range(len(rollouts_hist['steps']) - k + 1):\n        short_stat_rollouts = {key: rollouts_hist[key][i:i+k] for key in rollouts_hist}\n        long_stat_rollouts = {key: rollouts_hist[key][:i+k] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, long_stat_rollouts)        \n        data['x'].append(features)\n    test_sample = data['x'][-1].reshape(1, -1)\n    data['x'] = data['x'][:-1]\n    data['y'] = rollouts_hist['opp-actions'][k:]\n    return data, test_sample\n\ndef agent(observation, configuration):\n    # hyperparameters\n    k = 5\n    min_samples = 25\n    global rollouts_hist, last_move, data, test_sample\n    if observation.step == 0:\n        data = {'x': [], 'y': []}\n    # if not enough data -> randomize\n    if observation.step <= min_samples + k:\n        return warmup_strategy(observation, configuration)\n    # update statistics\n    rollouts_hist = update_rollouts_hist(rollouts_hist, last_move, observation.lastOpponentAction)\n    # update training data\n    if len(data['x']) == 0:\n        data, test_sample = init_training_data(rollouts_hist, k)\n    else:        \n        short_stat_rollouts = {key: rollouts_hist[key][-k:] for key in rollouts_hist}\n        features = construct_features(short_stat_rollouts, rollouts_hist)\n        data['x'].append(test_sample[0])\n        data['y'] = rollouts_hist['opp-actions'][k:]\n        test_sample = features.reshape(1, -1)\n        \n    # predict opponents move and choose an action\n    next_opp_action_pred = predict_opponent_move(data, test_sample)\n    action = int((next_opp_action_pred + 1) % 3)\n    last_move = {'step': observation.step, 'action': action}\n    return action","c3aae167":"%%writefile statistical_prediction.py\n\nimport random\nimport pydash\nfrom collections import Counter\n\n# Create a small amount of starting history\nhistory = {\n    \"guess\":      [0,1,2],\n    \"prediction\": [0,1,2],\n    \"expected\":   [0,1,2],\n    \"action\":     [0,1,2],\n    \"opponent\":   [0,1],\n}\ndef statistical_prediction_agent(observation, configuration):    \n    global history\n    actions         = list(range(configuration.signs))  # [0,1,2]\n    last_action     = history['action'][-1]\n    opponent_action = observation.lastOpponentAction if observation.step > 0 else 2\n    \n    history['opponent'].append(opponent_action)\n\n    # Make weighted random guess based on the complete move history, weighted towards relative moves based on our last action \n    move_frequency       = Counter(history['opponent'])\n    response_frequency   = Counter(zip(history['action'], history['opponent'])) \n    move_weights         = [ move_frequency.get(n,1) + response_frequency.get((last_action,n),1) for n in range(configuration.signs) ] \n    guess                = random.choices( population=actions, weights=move_weights, k=1 )[0]\n    \n    # Compare our guess to how our opponent actually played\n    guess_frequency      = Counter(zip(history['guess'], history['opponent']))\n    guess_weights        = [ guess_frequency.get((guess,n),1) for n in range(configuration.signs) ]\n    prediction           = random.choices( population=actions, weights=guess_weights, k=1 )[0]\n\n    # Repeat, but based on how many times our prediction was correct\n    prediction_frequency = Counter(zip(history['prediction'], history['opponent']))\n    prediction_weights   = [ prediction_frequency.get((prediction,n),1) for n in range(configuration.signs) ]\n    expected             = random.choices( population=actions, weights=prediction_weights, k=1 )[0]\n\n    # Play the +1 counter move\n    action = (expected + 1) % configuration.signs\n    \n    # Persist state\n    history['guess'].append(guess)\n    history['prediction'].append(prediction)\n    history['expected'].append(expected)\n    history['action'].append(action)\n\n    # Print debug information\n    print('opponent_action                = ', opponent_action)\n    print('move_weights,       guess      = ', move_weights, guess)\n    print('guess_weights,      prediction = ', guess_weights, prediction)\n    print('prediction_weights, expected   = ', prediction_weights, expected)\n    print('action                         = ', action)\n    print()\n    \n    return action","66dbde27":"# Upgrade kaggle_environments using pip before import\n!pip install -q -U kaggle_environments","8dcd8ba4":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom kaggle_environments import make, evaluate","13213256":"env = make(\"rps\", configuration={\"episodeSteps\": 1000}, debug=True)","a60600ff":"env.run([\"random_forest_random.py\", \"nash_equilibrium.py\"])\n\nenv.render(mode=\"ipython\", width=500, height=400)","c1204c97":"# raise SystemExit(\"Stop right there!\")","7707f994":"list_names = [\n    \"rock\", \n    \"paper\", \n    \"scissors\",\n    \"hit_the_last_own_action\",  \n    \"copy_opponent\", \n    \"reactionary\", \n    \"counter_reactionary\", \n    \"statistical\", \n    \"nash_equilibrium\",\n    \"markov_agent\", \n    \"memory_patterns\", \n    \"multi_armed_bandit\",\n    \"opponent_transition_matrix\",\n    \"decision_tree_classifier\",\n    \"statistical_prediction\",\n    \"random_forest_random\",\n]\nlist_agents = [agent_name + \".py\" for agent_name in list_names]\n\nscores = np.zeros((len(list_names), 10), dtype=int)","38248f6d":"env = make(\"rps\", configuration={\"episodeSteps\": 1000})","a077295d":"print(\"Simulation of battles. It can take some time...\")\n\nfor ind_agent_1 in range(10):\n    for ind_agent_2 in range(len(list_names)):\n        print(f\"LOG: Random Forest Random vs {list_names[ind_agent_2]}\", end=\"\\r\")\n        \n        current_score = evaluate(\n            \"rps\", \n            [\"random_forest_random.py\", list_agents[ind_agent_2]], \n            configuration={\"episodeSteps\": 1000}\n        )\n        \n        scores[ind_agent_2, ind_agent_1, ] = current_score[0][0]\n    \n    print(\"Round: \", ind_agent_1, \" of 10 \")","0738bd9c":"df_scores = pd.DataFrame(\n    scores, \n    index=list_names, \n    columns=range(10),\n)\n\n\nplt.figure(figsize=(15, 10))\nsns.heatmap(\n    df_scores, annot=True, cbar=False, cmap='coolwarm', linewidths=1, linecolor='black', fmt=\"d\"\n)\nplt.suptitle('Random Forest Random vs all agents', fontsize=20)\nplt.title('Final Reward Score', fontsize=15)\nplt.xticks(rotation=90, fontsize=15)\nplt.yticks(fontsize=15);","820b3046":"df_review=pd.DataFrame()\ndf_review['Won'] = df_scores.select_dtypes(include='int').gt(0).sum(axis=1)\ndf_review['Tie'] = df_scores.select_dtypes(include='int').eq(0).sum(axis=1)\ndf_review['Lost'] = df_scores.select_dtypes(include='int').lt(0).sum(axis=1)","f8338b24":"plt.figure(figsize=(5, 10))\nsns.heatmap(\n    df_review, annot=True, cbar=False, cmap='coolwarm', linewidths=1, linecolor='black', fmt=\"d\"\n)\nplt.suptitle('Random Forest Random vs all agents', fontsize=20)\nplt.title('Total games Won-Tie-Lost', fontsize=15)\nplt.xticks(rotation=90, fontsize=15)\nplt.yticks(fontsize=15);","a69ea2bc":"<a id=\"top\"><\/a>\n\n<div class=\"list-group\" id=\"list-tab\" role=\"tablist\">\n<h3 class=\"list-group-item list-group-item-action active\" data-toggle=\"list\" style='color:black; background:#FBE338; border:0' role=\"tab\" aria-controls=\"home\"><center>Quick Navigation<\/center><\/h3>\n\n## Opponents\n* [Agent: Hit The Last Own Action](#1)\n* [Agent: Rock](#2)\n* [Agent: Paper](#3)\n* [Agent: Scissors](#4)\n* [Agent: Copy Opponent](#5)\n* [Agent: Reactionary](#6)\n* [Agent: Counter Reactionary](#7)\n* [Agent: Statistical](#8)\n* [Agent: Nash Equilibrium](#9)\n* [Agent: Markov Agent](#10)\n* [Agent: Memory Patterns](#11)\n* [Agent: Multi Armed Bandit](#12)\n* [Agent: Opponent Transition Matrix](#13)\n* [Agent: Decision Tree Classifier](#14)\n* [Agent: Statistical Prediction](#15)\n\n### Battle    \n* [Setup and validation](#100)\n* [Marathon: Random Forest Random against all agents](#102)\n* [Results](#103)\n* [Review](#104)\n","a487bdf0":"<a id=\"3\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Paper<center><h2>","bcaf9240":"Validation","ed99436e":"Copy from kernel [Decision Tree Classifier](https:\/\/www.kaggle.com\/alexandersamarin\/decision-tree-classifier?scriptVersionId=46415861)","75f9cf4b":"Copy from kernel [Rock Paper Scissors - Nash Equilibrium Strategy](https:\/\/www.kaggle.com\/ihelon\/rock-paper-scissors-nash-equilibrium-strategy)\n\nNash Equilibrium Strategy (always random)","6571afc6":"<a id=\"4\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Scissors<center><h2>","021f19f3":"Copy from kernel [Multi-armed bandit vs deterministic agents](https:\/\/www.kaggle.com\/ilialar\/multi-armed-bandit-vs-deterministic-agents)","5cd02ef6":"<a id=\"100\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Setup and validation<center><h2>","e301532a":"Create a rock-paper-scissors environment (RPS), and set 1000 episodes for each simulation","2565332b":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py\n\nHit the last action of the opponent","ac6dcac1":"<a id=\"6\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Reactionary<center><h2>\n","ecdb4573":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py\n\nAlways uses Paper action","47f644fb":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py\n\nAlways uses Scissors action","3683ddfc":"<a id=\"2\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Rock<center><h2>","55d0174d":"Copy from kernel [(Not so) Markov \u26d3\ufe0f](https:\/\/www.kaggle.com\/alexandersamarin\/not-so-markov)","7df114e3":"<a id=\"1\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Random Forest Random<center><h2>","837fefca":"Copy from kernel [Rock, Paper, Scissors with Memory Patterns](https:\/\/www.kaggle.com\/yegorbiryukov\/rock-paper-scissors-with-memory-patterns)","e5ccff6c":"<a id=\"13\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Opponent Transition Matrix<center><h2>\n","e0ff06c9":"<a id=\"14\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Decision Tree Classifier<center><h2>\n\n","8696a1c2":"The idea of the agent:\n\n- A lot of agents use a simple baseline - copy the last action of the opponent.   \n- That's why we can simply hit our last actions (new action of the opponent)","613619d5":"* `Random Forest Random` can identify the patterns of all simple agents in 5 actions or less.\n* `Statistical` is an easy opponent for `Random Forest Random` and performs better than `Markov Agent` almost every time.\n* Chances to win over `Memory Patterns` or `Multi Armed Bandit` are near zero an only by luck can beat them sometimes.\n* Luck is crucial for the outcome over `Opponent Transition Matrix`, `Decision Tree Classifier` and `Statistical Prediction` as the results can vary a lot over matches.\n* Final conclusion is that `Random Forest Classifiers` can be used to predict opponents actions on `Rock-Paper-Scissors` but advanced `defensive` mechanisms are required when the pattern is identified by the opponent.\n\n**Disclaimer: The above review is done on multiple runs of this notebook and the published results might not represent them exactly.**","69c81dc0":"<a id=\"11\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Memory Patterns<center><h2>\n\n\n","1f582862":"We need to import the library for creating environments and simulating agent battles","377b8a0a":"<a id=\"8\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Statistical<center><h2>\n\n\n","6b5f5c08":"<a id=\"5\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Copy Opponent<center><h2>","418dd129":"Create environnment without debug","9951a71f":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py","2437a54a":"<a id=\"103\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Results<center><h2>","1701dff4":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py    \n\nAlways uses Rock action","e4eca5a5":"<a id=\"7\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Counter Reactionary<center><h2>\n\n","44a371f1":"Setup battlefield","87306062":"Copy from kernel [Rock Paper Scissors - Statistical Prediction](https:\/\/www.kaggle.com\/jamesmcguigan\/rock-paper-scissors-statistical-prediction)","40962898":"![](https:\/\/storage.googleapis.com\/kaggle-competitions\/kaggle\/22838\/logos\/header.png?t=2020-11-02-21-55-44)","381e69fb":"<a id=\"102\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Marathon: Random Forest Random against all agents<center><h2>\n","8cf92dad":"<a id=\"10\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Markov Agent<center><h2>\n\n\n","6e69e4e6":"<a id=\"15\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Statistical Prediction<center><h2>\n\n","d6455f9d":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py","a6ac6c13":"<a id=\"104\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Review<center><h2>","4aef3279":"<a id=\"9\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Nash Equilibrium<center><h2>\n\n\n","0226dd37":"Run the simulation","2f103d3c":"Copy from: https:\/\/github.com\/Kaggle\/kaggle-environments\/blob\/master\/kaggle_environments\/envs\/rps\/agents.py\n\nCopy the last action of the opponent","3a434691":"**this kernel was coped from https:\/\/www.kaggle.com\/jumaru\/random-forest-random-rock-paper-scissors \nSo please give him UpVoit\nThe model has been changed from RandomForestClassifier to ExtraTreesClassifier and the number of estimators has been changed to 10**","4066f982":"Copy from kernel [RPS: Opponent Transition Matrix](https:\/\/www.kaggle.com\/group16\/rps-opponent-transition-matrix)","63ef5485":"<a id=\"1\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Hit The Last Own Action<center><h2>","ef7a6057":"<a id=\"12\"><\/a>\n<h2 style='background:#FBE338; border:0; color:black'><center>Agent: Multi Armed Bandit<center><h2>"}}