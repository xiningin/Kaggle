{"cell_type":{"285bfe98":"code","5de718cb":"code","ec878541":"code","58fe47f8":"code","aa8aeb28":"code","ea36a8e9":"code","9310fe06":"code","0a366f53":"code","643db8eb":"code","fc55bc70":"code","f0347243":"code","c6f6bbfe":"code","d3650f37":"code","77548802":"code","b4255dd9":"code","cab75f0c":"code","456a91f6":"code","635397e6":"code","bd71049a":"code","97890197":"code","2503b5f7":"code","0bd1e975":"code","5c89738a":"code","1d2b3948":"code","f4e4e150":"code","30cbd07c":"code","a0d35249":"code","5047b026":"code","0571fae8":"markdown","94f34831":"markdown","5b6bcaa6":"markdown","0cda120c":"markdown","1b7b4534":"markdown","652da7b3":"markdown","58cade9b":"markdown","c9731d53":"markdown","2f50516f":"markdown","de4edd79":"markdown","4ac22d93":"markdown","0c756014":"markdown","74c75132":"markdown","a06a17ef":"markdown","12364077":"markdown","1e911d57":"markdown","af16f631":"markdown","dcf3bf18":"markdown","fb356c4c":"markdown","47ffd20b":"markdown","8170a8c7":"markdown","3c73f941":"markdown","9a0093ed":"markdown","64099393":"markdown","ff70ecaf":"markdown","26e47e4d":"markdown","e350d622":"markdown","7886d944":"markdown"},"source":{"285bfe98":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport plotly.express as px\nimport seaborn as sns\nfrom xgboost import XGBClassifier\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.preprocessing import StandardScaler, OneHotEncoder\nfrom sklearn.pipeline import make_pipeline\nfrom sklearn.compose import make_column_transformer\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.impute import SimpleImputer\n\nfrom sklearn.dummy import DummyClassifier\nfrom sklearn.discriminant_analysis import LinearDiscriminantAnalysis\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.svm import SVC\nfrom sklearn.neighbors import KNeighborsClassifier\n\nfrom sklearn.ensemble import AdaBoostClassifier, GradientBoostingClassifier, RandomForestClassifier, ExtraTreesClassifier\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","5de718cb":"df = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\n\nprint(f'{len(df.columns)} features total\\n')\n\ndf.head()","ec878541":"target_variable = df['Survived']","58fe47f8":"df.describe()","aa8aeb28":"df.duplicated().sum()","ea36a8e9":"missing_numbers = df.isnull().sum().sort_values(ascending=False)\nmissing_percent = round((df.isnull().sum() * 100 \/ df.isnull().count()), 2).sort_values(ascending=False)\n\nmissing = pd.concat([missing_numbers, missing_percent], axis=1, keys=[\"Missing Number\", \"Missing Percent\"])\nprint(missing)","9310fe06":"df[df['Age'].isnull()]","0a366f53":"df['Pclass'].value_counts()","643db8eb":"numericals = df.select_dtypes(include=\"number\").columns\n\nprint(f'Numerical columns: { numericals }')","fc55bc70":"corrs = df[numericals].drop('PassengerId', axis=1).corr()\nmatrix = np.triu(corrs)\n\nsns.heatmap(corrs, vmin=-1, vmax=1, fmt='.2f', annot=True, mask=matrix, cmap='coolwarm', center=0)","f0347243":"categorical = df.select_dtypes(include='object').columns\nprint(f'Categorical columns: { categorical }')","c6f6bbfe":"px.histogram(df, x=\"Sex\", color=\"Survived\").show()","d3650f37":"men = df.loc[df.Sex == 'male'][\"Survived\"]\nrate_men = sum(men)\/len(men)\nprint(f'Male survival percentage: { round(rate_men * 100, 2) }%')\n","77548802":"female = df.loc[df.Sex == 'female'][\"Survived\"]\nrate_female = sum(female)\/len(female)\nprint(f'Female survival percentage: { round(rate_female * 100, 2)}%')","b4255dd9":"px.histogram(df, x='Embarked', color=\"Survived\").show()","cab75f0c":"num_unique_tickets = df['Ticket'].nunique()\nprint(f'Number of unique tickets: { num_unique_tickets }')","456a91f6":"df.Cabin","635397e6":"cabin_data = df.copy()\ncabin_data.Cabin = cabin_data.Cabin.convert_dtypes()\ncabin_data.Cabin = cabin_data.Cabin.transform(lambda x: x[:1] if not pd.isnull(x) else \"None\")","bd71049a":"px.histogram(cabin_data, x='Cabin', color=\"Survived\").show()","97890197":"cabin_data.Cabin.unique()\nfor cabin in cabin_data.Cabin.unique():\n    survivals = cabin_data.loc[cabin_data.Cabin == cabin][\"Survived\"]\n    rate = sum(survivals) \/ len(survivals)\n    print(f'{round(rate * 100, 2)}% of cabin { cabin } survived')","2503b5f7":"numericals = ['Pclass', 'Fare']\ncategorical = ['Sex', 'Cabin']\ndf1 = df.copy()[numericals + categorical + ['Survived']]","0bd1e975":"df1.Cabin = df.Cabin.convert_dtypes().transform(lambda x: x[:1] if not pd.isnull(x) else \"None\")","5c89738a":"X = df1.drop('Survived', axis=1)\ny = df1['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nct = make_column_transformer(\n  (StandardScaler(), numericals),\n  (OneHotEncoder(), categorical)\n)\nmodel = DummyClassifier()\n\npipeline = make_pipeline(ct, model)\npipeline.fit(X_train, y_train)\ny_pred = pipeline.predict(X_test)\n\nscore = accuracy_score(y_test, y_pred)\nprint(f'Model: { model } accuracy score is { score }')\n\ndummy_result = pd.DataFrame({'Accuracy':[score]}, index=['DummyClassifier'])\ndummy_result","1d2b3948":"X = df1.drop('Survived', axis=1)\ny = df1['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nct = make_column_transformer(\n    (OneHotEncoder(), categorical),\n    (StandardScaler(), numericals)\n)\n\n# use defaults\nld = LinearDiscriminantAnalysis()\nlr = LogisticRegression()\nsvm = SVC()\nknn = KNeighborsClassifier()\n\nmodels_llsk = [ld, lr, svm, knn]\naccuracy = []\n\nfor model in models_llsk:\n    pipeline = make_pipeline(ct, model)\n    pipeline.fit(X_train, y_train)\n    y_pred = pipeline.predict(X_test)\n    \n    score = accuracy_score(y_test, y_pred)\n    accuracy.append(score)\n    print(f'Model: { model } accuracy score is { score }')\n    \nmodel_names = ['LinearDiscriminant', 'Logistic', 'SVM', 'KNeighbors']\nresult_group_models = pd.DataFrame({'Accuracy': accuracy}, index=model_names)\nresult_group_models\n    ","f4e4e150":"X = df1.drop('Survived', axis=1)\ny = df1['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nct = make_column_transformer(\n    (StandardScaler(), numericals),\n    (OneHotEncoder(), categorical)\n)\n\nada = AdaBoostClassifier(random_state=0)\ngb = GradientBoostingClassifier(random_state=0)\nrf = RandomForestClassifier(random_state=0)\net = ExtraTreesClassifier(random_state=0)\n\nmodels_ensemble = [ada, gb, rf, et]\naccuracy = []\n\nfor model in models_ensemble:\n    pipe = make_pipeline(ct, model)\n    pipe.fit(X_train, y_train)\n    y_pred = pipe.predict(X_test)\n    \n    score = accuracy_score(y_test, y_pred)\n    accuracy.append(score)\n    print(f'Model: { model } accuracy score is { score }')\n    \nmodel_names = ['AdaBoost', 'GradientBoosting', 'RandomForest', 'Extra Trees']\nresult_ensembles = pd.DataFrame({'Accuracy': accuracy}, index=model_names)\nresult_ensembles","30cbd07c":"X = df1.drop('Survived', axis=1)\ny = df1['Survived']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n\nct = make_column_transformer(\n    (StandardScaler(), numericals),\n    (OneHotEncoder(), categorical)\n)\n\nxgb = XGBClassifier(random_state=0)\npipe = make_pipeline(ct, xgb)\npipe.fit(X_train, y_train)\ny_pred = pipe.predict(X_test)\nscore = accuracy_score(y_test, y_pred)\n\nresult_xgb = pd.DataFrame({'Accuracy': score}, index=['XGBoost'])\nresult_xgb","a0d35249":"final_model_results = pd.concat([dummy_result, result_group_models, result_ensembles, result_xgb], axis=0)\nfinal_model_results.sort_values(by=['Accuracy'], ascending=True, inplace=True)\nfig = px.bar(final_model_results, x='Accuracy', y=final_model_results.index, title='Model Comparison Training Data', height=600,labels={'index': 'MODELS'})\nfig.show()","5047b026":"test_data = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ntest_data.head()\n\nX = df1.drop('Survived', axis=1)\ny = df1['Survived']\n\nfeatures = numericals + categorical\nX_test = test_data[features].copy()\nX_test.Cabin = X_test.Cabin.convert_dtypes().transform(lambda x: x[:1] if not pd.isnull(x) else \"None\")\n\nsi = SimpleImputer(missing_values=np.nan, strategy='mean')\nsi.fit(X_test[numericals])\n\nX_test[numericals] = si.transform(X_test[numericals])\n\ngb = GradientBoostingClassifier(random_state=0)\nct = make_column_transformer(\n    (StandardScaler(), numericals),\n    (OneHotEncoder(), categorical)\n)\n\npipe = make_pipeline(ct, gb)\npipe.fit(X, y)\npredictions = pipe.predict(X_test)\n\noutput = pd.DataFrame({'PassengerId': test_data.PassengerId, 'Survived': predictions})\noutput.to_csv('submission.csv', index=False)\nprint(\"Your submission was successfully saved!\")","0571fae8":"<a id=\"eda\"><\/a>\n# Exploratory Data Analysis","94f34831":"**Tickets**","5b6bcaa6":"Going to drop tickets since there is a unique ticket per passenger\/family. This is most likely not going to tell us anything that cabin info or SES\/fare wouldn't","0cda120c":"Looking at the Cabin series, we see there are those with NaN cabin and cabins with letters. Let's group cabins by NaN and first letter of cabin number (this might have indication of section)","1b7b4534":"<a id=\"#model-selection-ensembles\"><\/a>\n# Ensembles (AdaBoost, Gradient Boost, Random Forest, Extra Trees)","652da7b3":"# Table of Contents\n\n* [Problem](#problem)\n* [Data](#data)\n* [Exploratory Data Analysis](#eda)\n    - [Data Selection](#eda-selection)\n    - [Data Preprocessing](#eda-preprocessing)\n    - [Data Transformation](#eda-transformation)\n* [Model Selection](#model-selection)\n    - [Baseline](#model-selection-baseline)\n    - [Logistic & Linear Discriminant & SVC & KNN](#model-selection-baseline-llsk)\n    - [Ensembles (AdaBoost, Gradient Boosting, Random Forest, Extra Trees](#model-selection-ensembles)\n    - [XGBoost](#model-selection-xgboost)\n* [Test Data Prediction](#test-data-prediction)","58cade9b":"**Comparing Models**\n","c9731d53":"Load the data and investigate what features we are provided","2f50516f":"<a id=\"#model-selection-baseline-llsk\"><\/a>\n# Logistic & Linear Discriminant & SVM & KNN","de4edd79":"<a id=\"data\"><\/a>\n# Data","4ac22d93":"We have 12 features total, but 11 features as our input space (excluding survived column)\n\n### Data Dictionary\n1. **PassengerId** - id of passenger [numeric]\n2. **Pclass** - proxy for SES (as per variable notes given in competition). 1st = Upper, 2nd = Middle, 3rd = Lower [ordinal rankings]\n3. **Name** - Last Name, First Name. Honorifics included\n4. **Sex** - (\"male\", \"female\")\n5. **Age** -  Age is fractional if less than 1. If the age is estimated, is it in the form of xx.5 (as per variable notes)\n6. **SibSp** - # of family relations. Sibling = brother, sister, stepbrother, stepsister. Spouse = husband, wife (mistresses\/fiances ignored) (as per variable notes)\n7. **Parch** - # parents\/children. Parent = mother, father. Child = daughter, son, stepdaughter, stepson. Children with nanny have parch = 0\n8. **Ticket** - includes letter\/number combinations.\n9. **Fare** - price for ticket\n10. **Cabin** - NaN for no cabin. \"C85\" or other alphanumeric designation for cabin\n11. **Embarked** - Point of Embarment. C = Cherbourg, Q = Queenstown, S = Southampton","0c756014":"It looks like the majority of cabins E, D, B survived. All of cabin T and most of those without cabins died.","74c75132":"Looks like we break the 80% barrier with 2 ensemble classifiers (GradientBoosting, AdaBoost) and XGBoost. Time to run our models on the actual test data instead of the simulated test data we created from our training set. Since **GradientBoosting** performed the best at 82%, we'll use that for our actual test data","a06a17ef":"**Sex**","12364077":"**Embarkment**","1e911d57":"<a id=\"#model-selection-xgboost\"><\/a>\n# XGBoost","af16f631":"* Weak level correlation between numerical features and target variable\n* Pclass has negative correlation with survival (higher value - lower SES - means lower survival correlation)\n* Interestingly, age, sibsp, and parch have virtually no correlation with survival\n* Fare shows some positive correlation with survival. Fare should be an inverse of Pclass (high Pclass numbers means lower SES, means paid less for fair)","dcf3bf18":"Based on exploratory data, we're going to use the PClass, Fare numerical features and Sex, Cabin categorical features. The rest do not show strong enough correlations. The Cabin data will however go through above preprocessing steps in the EDA section prior to being passed to a OneHotEncoder","fb356c4c":"# **Categorical Feature Correlation**","47ffd20b":"<a id=\"model-selection-baseline\"><\/a>\n# Baseline Model","8170a8c7":"<a id=\"problem\"><\/a>\n# Problem\n\nWe're given a dataset with information regarding the passengers on board the Titanic. Based on this information, we need to create a model that predicts whether or not the passenger survived. This is a **classification** problem with a variety of approaches. Let's begin.","3c73f941":"# **Numerical Feature Correlation**","9a0093ed":"**Cabin**","64099393":"<a id=\"model-selection\"><\/a>\n# Model Selection","ff70ecaf":"Nothing particularly stands out aside from the '3' Pclass values seen above for the NaN Age rows, but running the below shows approximately 50% of the passengers on board have Pclass rank of 3, but only 20% Age entries are missing. So looking for relationships between Pclass and Age will most likely be fruitless. Let's move on to getting some numerical data regarding feature correlation. We can deal with the above when preprocessing\/feature engineering","26e47e4d":"<a id=\"#test-data-prediction\"><\/a>\n# Model Training on Test Data and Submit","e350d622":"Run a describe to get a sense of data scope and then check for duplicates or missing values","7886d944":"77.1 % of Cabin entries are missing and 19.87% of age antries are missing. The embarked missing percentage is negligible. I'm taking note of the missing cabin entries, but from reading and understanding the data set, the missing cabin entries should be expected as this indicates these passengers did not have a cabin (I'm guessing their SES values are also 2nd\/3rd, but we'll verify that later in the data). More interestingly is the almost 20% missing age entries. Looking at a few columns with missing age entries shows:"}}