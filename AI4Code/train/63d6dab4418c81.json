{"cell_type":{"72c4fab4":"code","7aef9330":"code","ad5c075a":"code","0c7b3f69":"code","47522898":"code","66d89368":"code","fc1c65be":"code","328884c6":"code","4d61ef3b":"code","7309134a":"code","c23dad22":"code","d6770b4d":"code","afc1d509":"code","00d5990b":"code","8e689e74":"code","9d5a9923":"code","585beb86":"code","23483250":"code","2d9b631c":"code","83d21e76":"code","e84e7121":"code","6e016325":"code","d4c13d55":"code","3f7a7e46":"code","0297a1bd":"code","3cb89ba7":"code","0d3101e1":"code","6c3c1dd1":"code","19d8133a":"code","4eec4870":"code","af2519dd":"code","dedbf166":"code","c45a7504":"code","f8f03b26":"code","86c3e247":"code","f42a3535":"code","658868be":"code","dac1efca":"code","0d62ebf7":"code","d66aa51a":"code","770d3ede":"code","e35e058a":"code","8a0e84d1":"code","2afea722":"code","1fa2734e":"code","2bc7ca55":"code","c5030631":"code","fe92469d":"code","6f058c41":"code","2eef3650":"code","8c969075":"code","3940d26e":"code","405d16a9":"code","b860dfc7":"code","d6a06305":"code","6b14c046":"code","4943b2e9":"code","52831cd3":"code","1c5c41ce":"code","66d2cb1f":"code","7c3d9250":"code","4f0d1b94":"code","03506f81":"code","09f83b24":"code","e4f1910b":"code","31c8e412":"code","89dde924":"code","42400295":"code","abe84216":"code","18fbc708":"code","0264420e":"code","e0735ed9":"code","a8801587":"code","14130271":"code","cd54ac89":"code","06666c0a":"code","26548cd8":"code","23168bb3":"code","e2cf03d2":"code","4551d5d5":"code","1ee2f1a3":"code","bfc0ac56":"code","9c3d7458":"code","09c69ea1":"code","463438a0":"code","279fc624":"code","77ba681b":"code","1c8fd134":"code","6f133f47":"code","84d2a893":"code","ed51d456":"code","cb677919":"code","8538c845":"code","8ec20b34":"code","780318d4":"code","74e315a8":"code","d983339f":"code","7785f246":"markdown","a61fb078":"markdown","0add0d27":"markdown","7842f897":"markdown","64126388":"markdown","d7a4a96e":"markdown","5b7a3a7a":"markdown","33738eab":"markdown","773a863f":"markdown","61ee8aac":"markdown","2c3d118f":"markdown","2095f361":"markdown","d6c62f98":"markdown","58471d57":"markdown","54c130ac":"markdown","fd039f5e":"markdown","acbf3eeb":"markdown","802a2372":"markdown","2445ae7c":"markdown","45f2c23f":"markdown","d21a6146":"markdown","5580e6b7":"markdown","da8e633d":"markdown","152a6204":"markdown","f9312877":"markdown","59d32ec2":"markdown","71ae52ea":"markdown","30ad5ef8":"markdown","c0325d7c":"markdown","87b17f96":"markdown","35d18d49":"markdown","2ba54e91":"markdown","b0f50785":"markdown","cda9bca3":"markdown","a177b0b9":"markdown","97f285f3":"markdown","7325731b":"markdown","8a2d8bd5":"markdown","271b0d7c":"markdown","f3026195":"markdown","a9406ce3":"markdown","92db7767":"markdown","c53af382":"markdown","960d100f":"markdown","d684800b":"markdown","afe4031a":"markdown"},"source":{"72c4fab4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nsns.set(style=\"whitegrid\")\nsns.set_context('talk')\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n# Any results you write to the current directory are saved as output.","7aef9330":"data = pd.read_csv('..\/input\/titanic\/train.csv')\ntest = pd.read_csv('..\/input\/titanic\/test.csv')\nprint('Loading .....')\nprint('Data Shape: ', data.shape, ' Test Shape: ', test.shape)","ad5c075a":"print('Data information')\nprint('-'*50)\ndata.info()\nprint('-'*50)\nprint(data.isnull().sum())\nprint('*'*75)\nprint('Test information')\nprint('-'*50)\ntest.info()\nprint('-'*50)\nprint(test.isnull().sum())","0c7b3f69":"all_data = [data,test] \nfor df in all_data:\n    data_corr = df.corr().abs()\n    plt.figure(figsize=(12, 6))\n    sns.heatmap(data_corr, annot=True,cmap='coolwarm')\n    plt.show()","47522898":"for df in all_data:\n    print(df.groupby(['Sex', 'Pclass']).median()['Age'])","66d89368":"for df in all_data:\n    df['Initial']=0\n    for i in df:\n        df['Initial']=df.Name.str.extract('([A-Za-z]+)\\.') #lets extract the Salutations\n    ","fc1c65be":"pd.crosstab(data.Initial,data.Sex).T.style.background_gradient(cmap='summer_r')\n","328884c6":"pd.crosstab(test.Initial,test.Sex).T.style.background_gradient(cmap='summer_r')","4d61ef3b":"#replace the values according the above cross tab\nfor df in all_data:\n    df['Initial'].replace(['Mlle','Mme','Ms','Dr','Major','Lady','Countess','Jonkheer','Col','Rev','Capt','Sir','Don','Dona'],\n                          ['Miss','Miss','Miss','Dr','Mr','Mrs','Mrs','Mr','Mr','Mr','Mr','Mr','Mr','Miss'],inplace=True)","7309134a":"for df in all_data:\n    print(df.groupby(['Sex', 'Pclass','Initial']).median()['Age'])","c23dad22":"for df in all_data:\n#fill the age values based on median valu for Pclass and sex\n    df['Age'] = df.groupby(['Sex','Pclass','Initial'])['Age'].apply(lambda x:x.fillna(x.median()))\n   ","d6770b4d":"data[data['Embarked'].isnull()]","afc1d509":"print(data.groupby(['Embarked','Pclass'])['Pclass'].count())","00d5990b":"print(data.groupby(['Embarked','Sex'])['Pclass'].count())","8e689e74":"#filling the Embarked features\ndata['Embarked'] = data['Embarked'].fillna('S')\n","9d5a9923":"test[test['Fare'].isnull()]","585beb86":"# Filling the missing value in Fare with the median Fare of 3rd class alone passenger\nmed_fare = test.groupby(['Pclass','SibSp','Parch'])['Fare'].median()[3][0][0]\ntest['Fare'] = test['Fare'].fillna(med_fare)","23483250":"for df in all_data:\n    df.drop(['Cabin'], axis=1, inplace=True)","2d9b631c":"print('Data information')\nprint('-'*50)\nprint(data.isnull().sum())\nprint('*'*55)\nprint('Test information')\nprint('-'*50)\nprint(test.isnull().sum())","83d21e76":"f,ax=plt.subplots(1,2,figsize=(12,6))\ndata['Survived'].value_counts().plot.pie(explode=[0,0.1],autopct='%1.1f%%',ax=ax[0],shadow=True)\nax[0].set_title('Survived')\nax[0].set_ylabel('')\nsns.countplot('Survived',data=data,ax=ax[1])\nax[1].set_title('Survived')\nplt.show()","e84e7121":"pd.crosstab(data.Sex,data.Survived,margins=True).style.background_gradient(cmap='Set3')","6e016325":"f,ax=plt.subplots(1,2,figsize=(12,6))\ndata[['Sex','Survived']].groupby(['Sex']).mean().plot.bar(ax=ax[0])\nax[0].set_title('Survived vs Sex')\nsns.countplot('Sex',hue='Survived',data=data,ax=ax[1])\nax[1].set_title('Sex:Survived vs Dead')\nplt.show()","d4c13d55":"pd.crosstab(data.Pclass,data.Survived,margins=True).style.background_gradient(cmap='Set3')","3f7a7e46":"f,ax = plt.subplots(1,3, figsize=(18,6))\ndata['Pclass'].value_counts().plot.pie(autopct='%1.1f%%',ax=ax[0],shadow=True, cmap='Set3')\ndata['Pclass'].value_counts().plot.bar(cmap='Set3',ax=ax[1])\nax[1].set_title('Number of Passengers by Class')\nax[1].set_ylabel('Count')\nax[1].set_xlabel('Pclass')\nsns.countplot('Pclass', hue='Survived',data=data, ax=ax[2], palette='Set3')\nax[2].set_title('Pclass:Survived vs Dead')\n","0297a1bd":"pd.crosstab([data.Sex,data.Survived],data.Pclass,margins=True).style.background_gradient(cmap='Set3')\n","3cb89ba7":"sns.factorplot('Pclass','Survived',hue='Sex',data=data, palette='Set2')\nplt.show()","0d3101e1":"f,ax=plt.subplots(1,2,figsize=(18,8))\nsns.swarmplot(\"Pclass\",\"Age\", hue=\"Survived\", data=data,split=True,ax=ax[0],palette='Set2')\nax[0].set_title('Pclass and Age vs Survived')\nax[0].set_yticks(range(0,110,10))\nsns.swarmplot(\"Sex\",\"Age\", hue=\"Survived\", data=data,split=True,ax=ax[1],palette='Set2')\nax[1].set_title('Sex and Age vs Survived')\nax[1].set_yticks(range(0,110,10))\nplt.show()","6c3c1dd1":"sns.catplot(x=\"Age\", y=\"Survived\",                 \n                hue=\"Sex\", row=\"Pclass\",\n                data=data,\n                orient=\"h\", aspect=2, palette=\"Set3\",\n                kind=\"violin\", dodge=True, cut=0, bw=.2\n                )","19d8133a":"f,ax=plt.subplots(1,2,figsize=(18,6))\ndata[data['Survived']==0].Age.plot.hist(ax=ax[0],bins=20,cmap='Set3')\nax[0].set_title('Survived= 0')\nx1=list(range(0,85,5))\nax[0].set_xticks(x1)\ndata[data['Survived']==1].Age.plot.hist(ax=ax[1],bins=20, cmap='Pastel1')\nax[1].set_title('Survived= 1')\nx2=list(range(0,85,5))\nax[1].set_xticks(x2)\n\n\nplt.show()","4eec4870":"#lets se this two plot together\nplt.figure(figsize=(18,10))\nsns.distplot(data[data['Survived']==0].Age,bins=20, kde=False, color='b', label='Died')\nsns.distplot(data[data['Survived']==1].Age,bins=20, kde=False, color='r',label='Survived')\nplt.legend()\n","af2519dd":"sns.factorplot('Pclass','Survived', col='Initial', data=data, palette='Set2')\nplt.show()","dedbf166":"pd.crosstab([data.Embarked, data.Pclass],[data.Sex, data.Survived], margins=True).style.background_gradient(cmap='Set3')","c45a7504":"sns.factorplot('Embarked', 'Survived', col='Pclass',data=data, palette='Set2')","f8f03b26":"f, ax = plt.subplots(2,2, figsize=(15,10))\nsns.countplot('Embarked', data=data, ax= ax[0,0], palette='Pastel1')\nax[0,0].set_title('Number of Passengers Boarded')\nsns.countplot('Embarked', hue='Sex',data=data, ax= ax[0,1], palette='Pastel1')\nax[0,1].set_title('Embarked Splited Female-Male')\nsns.countplot('Embarked',hue='Survived', data=data, ax= ax[1,0], palette='Pastel1')\nax[1,0].set_title('Embarked vs Survived')\nsns.countplot('Embarked', hue='Pclass',data=data, ax= ax[1,1], palette='Pastel1')\nax[1,1].set_title('Embarked vs Pclass')\nplt.subplots_adjust(wspace=0.2, hspace=0.5)\nplt.show()\n","86c3e247":"sns.factorplot('Pclass','Survived',hue='Sex',col='Embarked',data=data, palette='Set2')\nplt.show()","f42a3535":"pd.crosstab(data.SibSp, data.Survived, margins=True).style.background_gradient(cmap='Set3')","658868be":"f, ax = plt.subplots(1,2, figsize=(20,6))\nsns.barplot('SibSp','Survived', data=data, ax=ax[0], palette='Set3')\nax[0].set_title('SibSp vs Survived')\nsns.factorplot('SibSp','Survived',data=data,ax=ax[1],hue='Pclass', palette='Set2')\nax[1].set_title('SibSp vs Survived')\nplt.close(2)\nplt.show()","dac1efca":"pd.crosstab([data.Parch, data.Survived], data.Pclass, margins=True).style.background_gradient(cmap='Set3')","0d62ebf7":"f, ax = plt.subplots(1,2,figsize=(20,6))\nsns.barplot('Parch','Survived', data=data, ax=ax[0], palette='Set3')\nax[0].set_title('Parch vs Survived')\nsns.factorplot('Parch','Survived', data=data, ax=ax[1], palatte='Set3', hue='Pclass')\nax[1].set_title('Parch vs Survived')\nplt.close(2)\nplt.show()","d66aa51a":"f,ax=plt.subplots(1,3,figsize=(20,8))\nsns.distplot(data[data['Pclass']==1].Fare, ax=ax[0],kde=False)\nax[0].set_title('Fares in Class 1')\nsns.distplot(data[data['Pclass']==2].Fare, ax=ax[1], kde=False)\nax[1].set_title('Fares in Class 2')\nsns.distplot(data[data['Pclass']==3].Fare, ax=ax[2], kde=False)\nax[2].set_title('Fares in Class 3')\n","770d3ede":"\nsns.heatmap(data.corr(),annot=True,cmap='Pastel2',linewidths=0.2, ) #data.corr()-->correlation matrix\nfig=plt.gcf()\nfig.set_size_inches(16,6)\nplt.show()","e35e058a":"for df in all_data:\n    df['Age_bin'] = 0\n    df.loc[df['Age']<16,'Age_bin'] = 0\n    df.loc[(df['Age']>16) & (df['Age']<=32),'Age_bin'] =1\n    df.loc[(df['Age']>32) & (df['Age']<=48),'Age_bin'] =2\n    df.loc[(df['Age']>48) & (df['Age']<=64),'Age_bin'] =3\n    df.loc[df['Age']>64,'Age_bin'] =4\n    \n    ","8a0e84d1":"f, ax = plt.subplots(1,2,figsize=(20,6))\nsns.barplot('Age_bin','Survived', data=data, ax=ax[0], palette='Set3')\nax[0].set_title('Age_bin vs Survived')\nsns.factorplot('Age_bin','Survived', data=data, ax=ax[1], palatte='Set3', hue='Pclass')\nax[1].set_title('Age_bin vs Survived')\nplt.close(2)\nplt.show()","2afea722":"for df in all_data:\n    df['Family_size'] = 0\n    df['Family_size'] = df['Parch'] + df['SibSp']\n    df['Is_Alone'] = 0\n    df.loc[df.Family_size == 0, 'Is_Alone'] =1","1fa2734e":"f, ax = plt.subplots(1,2,figsize=(20,6))\nsns.barplot('Family_size','Survived', data=data, ax=ax[0], palette='Set3')\nax[0].set_title('Family_size vs Survived')\nsns.factorplot('Family_size','Survived', data=data, ax=ax[1], palatte='Set3', hue='Pclass')\nax[1].set_title('Family_size vs Survived')\n\nplt.close(2)\nplt.show()","2bc7ca55":"sns.factorplot('Is_Alone','Survived',data=data,hue='Sex',col='Pclass',palette='Set2')\nplt.show()","c5030631":"data['Fare_Range']=pd.qcut(data['Fare'],4)\ndata.groupby(['Fare_Range'])['Survived'].mean().to_frame().style.background_gradient(cmap='summer_r')","fe92469d":"for df in all_data:\n    df['Fare_cat'] = 0\n    df.loc[df['Fare']<=7.91,'Fare_cat'] = 0\n    df.loc[(df['Fare']>7.91) & (df['Fare']<=14.454), 'Fare_cat'] = 1\n    df.loc[(df['Fare']>14.454) & (df['Fare']<=31.0), 'Fare_cat'] = 2\n    df.loc[(df['Fare']>31.0) & (df['Fare']<=513), 'Fare_cat'] = 3\n\n","6f058c41":"sns.factorplot('Fare_cat','Survived',data=data,hue='Sex',palette='Set2')\nplt.show()","2eef3650":"gender =  {'male': 0,'female': 1} \nembarked = {'S':0, 'C':1,'Q':2}\ninitial = {'Mr':0, 'Mrs':1, 'Miss':2, 'Master':3, 'Dr':4}\nfor df in all_data:\n    df['Sex'] = [gender[item] for item in df.Sex]\n    df['Embarked'] = [embarked[item] for item in df.Embarked]\n    df['Initial'] = [initial[item] for item in df.Initial]\n    ","8c969075":"# creating Class Embark and Sex future together\nfor df in all_data:\n    df['Sex_Class_Embark'] = 0 \n    df.loc[(df['Sex'] == 1) & ((df['Pclass'] == 1) | (df['Pclass'] == 2) ) & \n           ((df['Embarked'] == 0)  | (df['Embarked'] == 1)  | (df['Embarked'] == 2)),'Sex_Class_Embark'] = 0\n    \n    df.loc[(df['Sex'] == 1) & (df['Pclass'] == 3) & ((df['Embarked'] == 1)  | (df['Embarked'] == 2)),'Sex_Class_Embark'] = 1\n\n    df.loc[(df['Sex'] == 0) & (df['Pclass'] == 1) & ((df['Embarked'] == 0)  | (df['Embarked'] == 1)),'Sex_Class_Embark'] = 2\n    df.loc[(df['Sex'] == 1) & (df['Pclass'] == 3) & (df['Embarked'] == 0),'Sex_Class_Embark'] = 2\n    \n    df.loc[(df['Sex'] == 0) & ((df['Pclass'] == 2)  | (df['Pclass'] == 3) ) & \n           ((df['Embarked'] == 0)  | (df['Embarked'] == 1)  | (df['Embarked'] == 2)),'Sex_Class_Embark'] = 3\n    \n    df.loc[(df['Sex'] == 0) & ((df['Pclass'] == 1)  |(df['Pclass'] == 2) ) & (df['Embarked'] == 2),'Sex_Class_Embark'] = 4\n    \n    ","3940d26e":"data['Sex_Class_Embark'].unique()\n","405d16a9":"data.head()","b860dfc7":"data.drop(['Name','Age','Ticket','Fare','Fare_Range','PassengerId'],axis=1,inplace=True)","d6a06305":"#before remove the Passengerid\ntest_copy = test.copy()","6b14c046":"test.drop(['Name','Age','Ticket','Fare','PassengerId'],axis=1,inplace=True)","4943b2e9":"#data have 1 more columns which is survived column, we are going to used this column as target\nprint(data.shape, test.shape)","52831cd3":"sns.heatmap(data.corr(),annot=True,cmap='RdYlGn',linewidths=0.2,annot_kws={'size':20})\nfig=plt.gcf()\nfig.set_size_inches(30,8)\nplt.xticks(fontsize=14)\nplt.yticks(fontsize=14)\nplt.show()","1c5c41ce":"#importing all the required ML packages\nfrom sklearn.linear_model import LogisticRegression #logistic regression\nfrom sklearn import svm #support vector Machine\nfrom sklearn.ensemble import RandomForestClassifier #Random Forest\nfrom sklearn.neighbors import KNeighborsClassifier #KNN\nfrom sklearn.naive_bayes import GaussianNB #Naive bayes\nfrom sklearn.tree import DecisionTreeClassifier #Decision Tree\nfrom sklearn.model_selection import train_test_split #training and testing data split\nfrom sklearn import metrics #accuracy measure\nfrom sklearn.metrics import confusion_matrix #for confusion matrix\n\nfrom sklearn.model_selection import GridSearchCV","66d2cb1f":"y =data.Survived\nX = data.drop('Survived', axis=1)","7c3d9250":"#from sklearn.preprocessing import StandardScaler\n#std_scaler = StandardScaler()\n#X = std_scaler.fit_transform(X)\n#test = std_scaler.transform(test)","4f0d1b94":"X_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.3, random_state=0)","03506f81":"print(X_train.shape, y_train.shape)","09f83b24":"model_log = LogisticRegression(solver='liblinear')\nmodel_log.fit(X_train, y_train)\nprediction_log = model_log.predict(X_valid)\nprint('The accuracy of the Logistic Regression is',metrics.accuracy_score(prediction_log, y_valid))","e4f1910b":"model_svm_l = svm.SVC(kernel='linear', C=0.1, gamma=0.1)\nmodel_svm_l.fit(X_train, y_train)\nprediction_svm_l = model_svm_l.predict(X_valid)\nprint('The accuracy of the Linear Support Vector Machine is ', metrics.accuracy_score(prediction_svm_l, y_valid))","31c8e412":"model_rbf = svm.SVC(kernel='rbf', C=0.1, gamma=0.1)\nmodel_rbf.fit(X_train, y_train)\nprediction_rbf = model_rbf.predict(X_valid)\nprint('The accuracy of the Radical Support Vector Machine is ', metrics.accuracy_score(prediction_rbf, y_valid))","89dde924":"model_tree = DecisionTreeClassifier() \nmodel_tree.fit(X_train, y_train)\nprediction_tree = model_tree.predict(X_valid)\nprint('The accuracy of the Decision Tree is ', metrics.accuracy_score(prediction_tree, y_valid))","42400295":"model_knn = KNeighborsClassifier()\nmodel_knn.fit(X_train, y_train)\nprediction_knn = model_knn.predict(X_valid)\nprint('The accuracy of the  K-Nearest Neighbours is ', metrics.accuracy_score(prediction_knn, y_valid))","abe84216":"s =pd.Series()\nfor i in list(range(1,11)):\n    model_knn = KNeighborsClassifier(n_neighbors=i)\n    model_knn.fit(X_train, y_train)\n    prediction_knn = model_knn.predict(X_valid)\n    s = s.append(pd.Series(metrics.accuracy_score(prediction_knn, y_valid)))\n\nplt.plot(list(range(1,11)), s)\nplt.xticks([0,1,2,3,4,5,6,7,8,9,10])\nplt.title('The Accuracy vs n_neighbors K-Nearest Neighbours')\nplt.xlabel('n_neighbors')\nplt.ylabel('The Accuracy of the K-Nearest Neighbours')\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()\n    ","18fbc708":"model_gaus = GaussianNB()\nmodel_gaus.fit(X_train, y_train)\nprediction_gaus = model_gaus.predict(X_valid)\nprint('The accuracy of the  Gaussian Naive Bayes is ', metrics.accuracy_score(prediction_gaus, y_valid))","0264420e":"list_n_estimators = [50,100,150,200,250,300,350,400,450,500]\nrandom_acc = pd.Series()\nfor i in list_n_estimators:\n    model_random = RandomForestClassifier(n_estimators=i)\n    model_random.fit(X_train, y_train)\n    predict_random = model_random.predict(X_valid)\n    random_acc =random_acc.append(pd.Series(metrics.accuracy_score(predict_random, y_valid)))\n#print(random_acc)\n","e0735ed9":"plt.plot(list_n_estimators, random_acc)\nplt.xticks(list_n_estimators)\nplt.title('The Accuracy vs n_estimators Random Forests')\nplt.xlabel('n_estimators')\nplt.ylabel('The Accuracy of the Random Forests')\nfig=plt.gcf()\nfig.set_size_inches(12,6)\nplt.show()","a8801587":"model_random = RandomForestClassifier(n_estimators=300)\nmodel_random.fit(X_train, y_train)\npredict_random = model_random.predict(X_valid)\nprint('The accuracy of the  Random Forest is ', metrics.accuracy_score(predict_random, y_valid))","14130271":"import sklearn","cd54ac89":"#load nesseray libraries\nfrom sklearn.model_selection import KFold\nfrom  sklearn.model_selection import cross_val_score\nfrom sklearn.model_selection import cross_val_predict\n\nfrom sklearn import svm, tree, linear_model, neighbors, naive_bayes, ensemble, discriminant_analysis, gaussian_process\nfrom xgboost import XGBClassifier\n\n\nkfold =KFold(n_splits=10, random_state=22)\nxyz = []\naccuracy = []\nstd = []\n\nclassifiers = ['Linear Svm', 'Radial Svm', 'Logistic Regression', 'KNN', 'Decision Tree', 'Naive Bayes' , 'Random Forest']\nmodels = [svm.SVC(kernel='linear'), svm.SVC(kernel='rbf'), LogisticRegression(solver='liblinear'), KNeighborsClassifier(n_neighbors=9), \n      DecisionTreeClassifier(), GaussianNB(), RandomForestClassifier(n_estimators=100)]\n\n    \n    \n    \nfor i in models:\n    model = i\n    cv_result = cross_val_score(model,X,y, cv=kfold,scoring='accuracy')\n    cv_result =cv_result\n    xyz.append(cv_result.mean())\n    std.append(cv_result.std())\n    accuracy.append(cv_result)\n\nnew_models_data_frame = pd.DataFrame({'CV Mean': xyz, 'Std': std}, index=classifiers)\nnew_models_data_frame\n","06666c0a":"plt.subplots(figsize=(12,6))\nplt.xticks(rotation=45)\nsns.boxplot(new_models_data_frame.index, accuracy)\n","26548cd8":"f, ax  =plt.subplots(3,3, figsize=(12,10))\ny_pred = cross_val_predict(svm.SVC(kernel='linear'),X,y,cv=10)\nsns.heatmap(confusion_matrix(y,y_pred), ax=ax[0,0], annot=True,fmt='2.0f')\nax[0,0].set_title('Linear SVM')\n\ny_pred = cross_val_predict(svm.SVC(kernel='rbf'),X,y,cv=10)\nsns.heatmap(confusion_matrix(y,y_pred), ax=ax[0,1], annot=True,fmt='2.0f')\nax[0,1].set_title('Radical SVM')\n\ny_pred = cross_val_predict(KNeighborsClassifier(n_neighbors=9) ,X,y,cv=10)\nsns.heatmap(confusion_matrix(y,y_pred), ax=ax[0,2], annot=True,fmt='2.0f')\nax[0,2].set_title('KNN')\n\ny_pred = cross_val_predict(LogisticRegression(solver='liblinear') ,X,y,cv=10)\nsns.heatmap(confusion_matrix(y,y_pred), ax=ax[1,0], annot=True,fmt='2.0f')\nax[1,0].set_title('Logistic Regression')\n\ny_pred = cross_val_predict(RandomForestClassifier(n_estimators=100) ,X,y,cv=10)\nsns.heatmap(confusion_matrix(y,y_pred), ax=ax[1,1], annot=True,fmt='2.0f')\nax[1,1].set_title('Random Forest')\n\ny_pred = cross_val_predict(DecisionTreeClassifier() ,X,y,cv=10)\nsns.heatmap(confusion_matrix(y,y_pred), ax=ax[1,2], annot=True,fmt='2.0f')\nax[1,2].set_title('Decision Tree')\n\ny_pred = cross_val_predict(GaussianNB() ,X,y,cv=10)\nsns.heatmap(confusion_matrix(y,y_pred), ax=ax[2,0], annot=True,fmt='2.0f')\nax[2,0].set_title('Naive Bayes')\n\nplt.subplots_adjust(hspace=0.5, wspace=0.5)\nplt.show()\n","23168bb3":"from sklearn.model_selection import GridSearchCV\nC=[0.05, 0.1, 0.2, 0.25, 0.3, 0.35, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]\ngamma=[0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1.0]\nkernel=['rbf','linear']\nhyper = {'kernel':kernel, 'C':C, 'gamma':gamma}\ngd =GridSearchCV(estimator=svm.SVC(), param_grid=hyper, verbose=True)\n\n\ngd.fit(X,y)\n\nprint(gd.best_score_)\nprint(gd.best_estimator_)","e2cf03d2":"model_rbf = svm.SVC(kernel='rbf', C=0.35, gamma=0.1)\nmodel_rbf.fit(X_train, y_train)\nprediction_rbf = model_rbf.predict(X_valid)\nprint('The accuracy of the Radical Support Vector Machine is ', metrics.accuracy_score(prediction_rbf, y_valid))","4551d5d5":"#n_estimator =range(50, 1000, 50)\n#hyper = {'n_estimators': n_estimator}\n#gd = GridSearchCV(estimator=RandomForestClassifier(random_state=0), param_grid=hyper, verbose=True)\n#gd.fit(X,y)\n#print(gd.best_score_)\n#print(gd.best_estimator_)\n","1ee2f1a3":"from sklearn.ensemble import VotingClassifier\n\nensemble = VotingClassifier(estimators=[('KNN', KNeighborsClassifier(n_neighbors=9)),\n                                        ('RBF', svm.SVC(kernel='rbf',probability=True,C=0.4,gamma=0.1)),\n                                        ('RFor', RandomForestClassifier(n_estimators=900, random_state=0)),\n                                        ('LR', LogisticRegression(C=0.05)),\n                                        ('DT', DecisionTreeClassifier(random_state=0)),\n                                        ('NB', GaussianNB()),\n                                        ('Svm', svm.SVC(kernel='linear',probability=True))],\n                           voting='soft').fit(X_train, y_train)\n\nprint('The accuracy for ensembled model is:',ensemble.score(X_valid,y_valid))\ncross=cross_val_score(ensemble,X,y, cv = 10,scoring = \"accuracy\")\nprint('The cross validated score is',cross.mean())","bfc0ac56":"from sklearn.ensemble import BaggingClassifier\nmodel_bag = BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3), random_state=0, n_estimators=800)\nmodel_bag.fit(X_train, y_train)\nprediction_bag = model_bag.predict(X_valid)\nprint('The accuracy for bagged KNN is:',metrics.accuracy_score(prediction_bag,y_valid))\nresult=cross_val_score(model_bag,X,y,cv=10,scoring='accuracy')\nprint('The cross validated score for bagged KNN is:',result.mean())\n","9c3d7458":"model_bag = BaggingClassifier(base_estimator=DecisionTreeClassifier(),random_state=0, n_estimators=800)\nmodel_bag.fit(X_train, y_train)\nprediction_bag = model_bag.predict(X_valid)\nprint('The accuracy for bagged KNN is:',metrics.accuracy_score(prediction_bag,y_valid))\nresult=cross_val_score(model_bag,X,y,cv=10,scoring='accuracy')\nprint('The cross validated score for bagged KNN is:',result.mean())","09c69ea1":"from sklearn.ensemble import AdaBoostClassifier\nada = AdaBoostClassifier(n_estimators=200, random_state=0, learning_rate=0.05)\nresult = cross_val_score(ada, X,y, cv=10, scoring='accuracy')\nprint('The cross validated score for AdaBoost is:',result.mean())","463438a0":"from sklearn.ensemble import GradientBoostingClassifier\ngrad=GradientBoostingClassifier(n_estimators=200,random_state=0,learning_rate=0.1)\nresult=cross_val_score(grad,X,y,cv=10,scoring='accuracy')\nprint('The cross validated score for Gradient Boosting is:',result.mean())","279fc624":"import xgboost as xg\nxgboost=xg.XGBClassifier(n_estimator=900,learning_rate=0.1)\nresult=cross_val_score(xgboost,X,y,cv=10,scoring='accuracy')\nprint('The cross validated score for XGBoost is:',result.mean())","77ba681b":"## Hyper-PArameter Tuning for AdaBoost\n\n#n_estimators = list(range(100,1000,100))\n#learn_rate = [0.01,0.02,0.03,0.04,0.05, 0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.9,1]\n#hyper = {'n_estimators':n_estimators, 'learning_rate': learn_rate}\n#gd=GridSearchCV(estimator=AdaBoostClassifier(),param_grid=hyper,verbose=True)\n#gd.fit(X,y)\n#print(gd.best_score_)\n#print(gd.best_estimator_)\n","1c8fd134":"model_random.fit(X_train, y_train)","6f133f47":"f, ax = plt.subplots(2,2, figsize=(15,12))\nmodel=RandomForestClassifier(n_estimators=500,random_state=0)\nmodel.fit(X,y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,0], cmap='Set3')\nax[0,0].set_title('Feature Importance in Random Forests')\n\nmodel=AdaBoostClassifier(n_estimators=200,learning_rate=0.05,random_state=0)\nmodel.fit(X,y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[0,1],color='lightcoral')\nax[0,1].set_title('Feature Importance in AdaBoost')\nmodel=GradientBoostingClassifier(n_estimators=500,learning_rate=0.1,random_state=0)\nmodel.fit(X,y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,0],color='lightgreen')\nax[1,0].set_title('Feature Importance in Gradient Boosting')\nmodel=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\nmodel.fit(X,y)\npd.Series(model.feature_importances_,X.columns).sort_values(ascending=True).plot.barh(width=0.8,ax=ax[1,1],color='violet')\nax[1,1].set_title('Feature Importance in XgBoost')\nplt.subplots_adjust(wspace=0.5, hspace=0.5)\nplt.show()\n","84d2a893":"X.columns","ed51d456":"#Let's redefine the feture for random forest\nfeature_random = ['Initial','Sex_Class_Embark','Pclass','Fare_cat','Age_bin','Family_size']\n\nX_random = X[feature_random]\nX_train, X_valid, y_train, y_valid = train_test_split(X_random,y, test_size=0.3, random_state=0)\n\n#model=RandomForestClassifier(n_estimators=500,random_state=0)\nmodel =AdaBoostClassifier(n_estimators=900, random_state=0, learning_rate=0.01)\nmodel.fit(X_train, y_train)\npredict_random = model.predict(X_valid)\nprint('The accuracy of the  Model is ', metrics.accuracy_score(predict_random, y_valid))","cb677919":"#Let's redefine the feture for random forest\nX_train, X_valid, y_train, y_valid = train_test_split(X,y, test_size=0.3, random_state=0)\n\nmodel_rbf = svm.SVC(kernel='rbf', C=0.35, gamma=0.1)\nmodel_rbf.fit(X_train, y_train)\nprediction_rbf = model_rbf.predict(X_valid)\nprint('The accuracy of the Radical Support Vector Machine is ', metrics.accuracy_score(prediction_rbf, y_valid))","8538c845":"test_random = test","8ec20b34":"n_neighbors = [6,7,8,9,10,11,12,14,16,18,20,22]\nalgorithm = ['auto']\nweights = ['uniform', 'distance']\nleaf_size = list(range(1,50,5))\nhyperparams = {'algorithm': algorithm, 'weights': weights, 'leaf_size': leaf_size, \n               'n_neighbors': n_neighbors}\ngd=GridSearchCV(estimator = KNeighborsClassifier(), param_grid = hyperparams, verbose=True, \n                cv=10, scoring = \"roc_auc\")\ngd.fit(X, y)\nprint(gd.best_score_)\nprint(gd.best_estimator_)","780318d4":"gd.best_estimator_.fit(X, y)\npred_test= gd.best_estimator_.predict(test)","74e315a8":"#pred_test = model_rbf.predict(test_random)","d983339f":"output = pd.DataFrame({'PassengerId' : test_copy.loc[:,'PassengerId'],\n                       'Survived': pred_test})\noutput.to_csv('submission.csv', index=False)","7785f246":"#### Bagged Desition Tree","a61fb078":"## Conclusion\n* Credit to:  https:\/\/www.kaggle.com\/ash316\/eda-to-prediction-dietanic","0add0d27":"### Gaussian Naive Bayes","7842f897":"* SVM: C:0.4, gamma=0.1, kernel='rbf' has the score: 82.6%\n* Random_forest :  n_estimators=800 has the score 81.4 %\n\n### Ensembling\nEnsembling is a good way to increase the accuracy or performance of a model, It is the combination of various simple models to create powerful model.\n* Voting Classifier\n* Bagging\n* Boosting\n\n#### Voting Classifier\nIt is the simplest way of combining predictions from many different machine larning models. It gives prediction results base don the prediction of the all the submodels.","64126388":"*  Passengers with their parents onboard have greater chance of survival. It however reduces as the number goes up.\n* The chances of survival is good for somebody who has 1-3 parents on the ship. Being alone also proves to be fatal and the chances for survival decreases when somebody has >4 parents on the ship.\n\n-----> ** Fare Features**","d7a4a96e":"### Random Forests","5b7a3a7a":"*K-Nearest Neighbours score as we change the  n_neighbours attribute.The default value is 5. Lets check the accuracies over various values of n_neighbours.*","33738eab":"## 4.  Predictive Modeling","773a863f":"* These plots shows that not many passengers survived the accident.\n* Out of 891 passengers in training set, only around 350 (38.4% ) people survived of the total training set.\n\n-----> ** Sex - Categorical Feature **","61ee8aac":"### Decision Tree","2c3d118f":"-----> **Embarked - Categorical Feature**","2095f361":"### 1.3 Dealing with Missing Values\n* Age,Cabin, Embarked features has missing in data \n* Age, Cabin,Fare features has missing in test data\n\n#### 1.3.1 Age\n* We are going to fill the age values with the colum has a correlation on age\n","d6c62f98":"* Female from Pclass1 is about 95-96% survived, as only 3 out of 94 Women from Pclass1 died.\n* Female has high priority to survive\n* Third class femal has more survived rate than first class male.\n\n-----> **Age - Continues Fetures**","58471d57":"* There looks to be a large distribution in the fares of Passengers in Pclass1 and this distribution goes on decreasing as the standards reduces.","54c130ac":"* ** Sibling = *brother, sister, stepbrother, stepsister* **\n\n* ** Spouse =* husband, wife * **\n\n* The barplot and factor shows\n    * If the passnger dosen't have any SibSp on boat, Thay have 34% change to survive\n    * Higher change to survive with 1 or 2 SibSp \n    * There is no change to survive with 5 0r 8 SibSp, It could be also reason these people from Pclass 3\n    \n-----> **Parch Features**","fd039f5e":"* The number of men on the ship is lot more than the number of women. \n* But, the number of women saved is almost twice the number of males saved. \n* The survival rates for a women on the ship is around **75%** while that for men in around **18-19%.**\n* This looks very important feature for prediction the Survived people\n\n-----> **Pclass Feature**","acbf3eeb":"## 3. Feature Engineering \n**Age**\n* Age is a continous feature, there is a problem with Continous Variables in Machine Learning Models.\n* We need to convert these continous values into categorical values\n* The maximum age of a passenger was 80. So lets divide the range from 0-80 into 5 bins. So 80\/5=16. So bins of size 16","802a2372":"### K-Nearest Neighbours(KNN)","2445ae7c":"* The chances for survival for Port C is highest around 0.55 while it is lowest for S.","45f2c23f":"### Split data for validation\n","d21a6146":"##  Cross Validation","5580e6b7":"# Titanic Survives Passengers Prediction\nThe notebook is simple: using the machine learning to create a model that predicts which passengers survived from Titanic diaster.\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\n\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\nWhile there was some element of luck involved in surviving, it seems some groups of people were more likely to survive than others.\n## 1. Exploring the Datasets\n### 1.1. Load the Libraries\n*If this Kernel helped you in any way,I would be very much appreciated to your <font color='red'>UPVOTES<\/font>*\n","da8e633d":"* Children less than 5 years old were saved in large numbers (*The Women and Child First Policy*)\n* The oldest survived oersen was 80.\n","152a6204":"* When the Fare_cat increase survive change also increase\n\n**Converting String values numeric**\n\ndata['Sex'].replace(['male','female'],[0,1],inplace=True)\n\ndata['Embarked'].replace(['S','C','Q'],[0,1,2],inplace=True)\n\ndata['Initial'].replace(['Mr','Mrs','Miss','Master','Other'],[0,1,2,3,4],inplace=True)","f9312877":"* Data dosen't  correlated each other  \n* SinSp and Parch Potisively 0.41 correlated\n* Fare and Pclass negatively Correlated (Meaning When the fare increase Pclass decrese, which is make sense Class 1 should have higher fare than Class3)","59d32ec2":"#### 1.3.2 Embarked Features\n\n","71ae52ea":"### Confusion Matrix\nConfusion  Matrix could helps us where did the model go wrong,or which class did the model predict wrong.","30ad5ef8":"### Linear Support Vector Machine(linear-SVM)","c0325d7c":"\n**Fare Range**\n* Since fare is also a continous feature, we need to convert it into ordinal value. For this we will use pandas.qcut.","87b17f96":"### 1.2. Load the Datasets\n\nOne dataset is titled `train.csv` and the other is titled `test.csv`.\n\nTrain.csv will contain the details of a subset of the passengers on board (891 to be exact) and importantly, will reveal whether they survived or not, also known as the \u201cground truth\u201d.\n\nThe `test.csv` dataset contains similar information but does not disclose the \u201cground truth\u201d for each passenger. It\u2019s your job to predict these outcomes.\n\nUsing the patterns you find in the train.csv data, predict whether the other 418 passengers on board (found in test.csv) survived.","35d18d49":"## 2. Exploratary Data Analysis\n-----> **Surviver Rate **","2ba54e91":"### Logistic Regression","b0f50785":"* Most of the first  class passenger embarked on S((Southampton)) \n* Most of the Female also embarked on  S((Southampton))\n* We have 2 missing values on Embarked which these passengers both female and first class so I would like to fill this empty values with S((Southampton))","cda9bca3":"### Feature Importance","a177b0b9":"#### Cabin Feature\n* we are going to drop this feture because it has lot of missing value\n","97f285f3":"* The survival rate decreases as the age increases irrespective of the Pclass.\n\n**Family Size And Is Alone**\n* We are going to  create a new feature called \"Family_size\" and \"Alone\" and analyse it. \n* This feature is the summation of Parch and SibSp. \n* It gives us a combined data so that we can check if survival rate have anything to do with family size of the passengers. * Alone will denote whether a passenger is alone or not.","7325731b":"### Radial Support Vector Machines","8a2d8bd5":"#### Boosting \n* This technique uses sequential learning of classifiers\n* Model first trained on the complete dataset\n* in the next iteration, the learner will focus more on the wrongly predicted or give more weight to it.\n* this will try to predict the wrong instance correctly\n\n* AdaBoost (Adaptive Boosting)\n* Stochastic Gradient Boosting\n* XGBoost","271b0d7c":"#### 1.3.3 Fare Features\n","f3026195":"#### Random Forest ","a9406ce3":"#### Bagging\n Unlike Voting Classifier, Bagging makes use of similar classifiers.","92db7767":"* The survival chances are almost 1 for women for Pclass1 and Pclass2 irrespective of the Embarked.\n* Port S looks to be very unlucky for Pclass3 Passenegers as the survival rate for both men and women is very low.\n* Port Q looks looks to be unlukiest for Men, as almost all were from Pclass 3.\n\n-----> **Sibspip Feature **","c53af382":"**Interpreting Confusion Matrix**\n\n* Left diagonal shows the number of correct prdictions made for each class\n* Right diagonal shows the number of wrong predictions made\n\n    * For Radical - SVM\n        * 514 for dead and 228 for survived predicted correctly\n        * 114 for ead and 35 for survived predicded wrong\n    * Radical - SVM has a higher change in correcly predicting dead\n    * Naive Bayes has a higher change in correcly predicting survived people\n    \n### Hyper-Parameter Tuning\nWe are going to tune the hyper=parameters for the 2 best classifiers.  SVM and Random Forests.\n\n#### SVM","960d100f":"* Passenegers Of Pclass 1 has a very high priority to survive. \n* The number of Passengers in Pclass 3 were a lot higher than Pclass 1 and Pclass 2, but still the number of survival from pclass 3 is low compare to them. \n* Pclass 1 %survived is around 63%, for Pclass2 is around 48%, and Pclass3 survived is around 25%\n\n**We saw that Sex and Class is important on the survive.So,Lets check survival rate with Sex and Pclass Together.**","d684800b":"* Most of the passengers boarded form S. \n* Most of the Pclass 3 boarded form S, It could be reason S has lot of died person.\n* Port Q had almost 95% of the passengers were from Pclass3.\n\n","afe4031a":"#### Dropping UnNeeded Features\nName--> We don't need name feature as it cannot be converted into any categorical value.\n\nAge--> We have the Age_band feature, so no need of this.\n\nTicket--> It is any random string that cannot be categorised.\n\nFare--> We have the Fare_cat feature, so unneeded\n\nCabin--> A lot of NaN values and also many passengers have multiple cabins. So this is a useless feature.\n\nFare_Range--> We have the fare_cat feature.\n\nPassengerId--> Cannot be categorised."}}