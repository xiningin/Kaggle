{"cell_type":{"cbf712e6":"code","f6d38ae7":"code","62af26e7":"code","d387246c":"code","d34e34a5":"code","25fc2155":"code","898214be":"code","92377f73":"code","80b65042":"code","a61e25c2":"code","ea30d945":"code","57b33da4":"code","5cc3e6cc":"code","228549db":"code","3d21bcf2":"code","f61414ff":"code","69de693b":"code","3e20071c":"code","75221b88":"code","41a17d27":"code","37e36be4":"code","0d9defcc":"code","90b0761d":"code","b8af488a":"code","46bb0f50":"code","bde60806":"code","f3973687":"code","199e8a73":"code","75dd2b0c":"code","4da15c7f":"code","ee61de0f":"code","d7bdf149":"code","12ce35b3":"code","66e0a324":"code","10965a7e":"code","725b5f1b":"code","786523c6":"code","c8cec912":"code","f4058438":"code","63006f22":"code","69c9da19":"code","92ced1ed":"code","60674508":"code","4b050941":"code","74575311":"code","aac2aa63":"code","0cf3a877":"code","06fe58f1":"code","0a87aabf":"code","8c256cb6":"code","4372c391":"code","0dd88526":"code","350477c2":"code","f2e1ae91":"code","1a32792e":"code","4394191e":"code","c6c9b918":"code","e000c04d":"code","4ff56d3d":"code","246867f1":"code","15cf0df6":"code","b299a2e0":"code","d15144bb":"code","5634fdeb":"code","e0db4284":"code","0598cf72":"code","847d15d0":"code","53d5f730":"code","f84ac41e":"code","d53301ae":"code","e3b81584":"code","93ecca69":"code","f7b5c4ff":"code","8adc568b":"code","d539a463":"code","0ebda58d":"code","d64a17e9":"code","21183c75":"code","d2ee4410":"code","73947e80":"code","6fbddc7b":"code","22d3cf4e":"code","06493234":"code","10c929bb":"code","1898ecba":"code","4009fb92":"code","62fc6933":"code","1a779de4":"code","50f55676":"code","3cb75cb7":"code","867bc33d":"code","98b8119e":"code","3e241d2f":"code","cb41bbbe":"code","39c9e572":"code","02a82eb9":"code","785fc944":"code","923c6c1d":"code","a5be3b08":"code","e4a3f387":"code","da95485c":"code","b37c58f3":"code","3eac1ada":"code","af6f1d9e":"code","84183cd8":"code","321e039d":"code","163c071e":"code","ee169b6d":"code","1997db41":"code","4973922a":"code","11ea0080":"code","9f399e02":"code","d2c74d39":"code","3e194339":"code","3bee8494":"code","53cf637f":"code","54752608":"code","9cd6fa1e":"code","a6e26d76":"code","cea6e87e":"code","fb9d53e6":"markdown","a2f611b0":"markdown","ebaf2719":"markdown","69a72c1c":"markdown","0d0c496c":"markdown","1d13ed95":"markdown","959ead15":"markdown","10e2288f":"markdown","bcf9eb07":"markdown","9fc49671":"markdown","eb49bf63":"markdown","48c5a5e4":"markdown","dc464b25":"markdown","b63cc080":"markdown","eb7f271c":"markdown","468ff381":"markdown","5264b0e4":"markdown","61e27e80":"markdown","86236219":"markdown","109b5042":"markdown","4815829a":"markdown","413ff8e6":"markdown","84a513fa":"markdown","3dfc2a1a":"markdown","1aef4d03":"markdown","51ad52ba":"markdown","55ff5933":"markdown","22b5cd64":"markdown"},"source":{"cbf712e6":"import numpy as np\nimport pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Visualization imports\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n%matplotlib inline\n# %config InlineBackend.figure_format = 'svg'\n\n# Modeling imports\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV, StratifiedKFold, cross_val_score, cross_val_predict\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.metrics import f1_score, roc_auc_score,log_loss, confusion_matrix, precision_score, recall_score, accuracy_score \nfrom sklearn import linear_model, ensemble , tree \nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier , VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nimport imblearn.over_sampling\nfrom sklearn.svm import SVC  \nfrom sklearn.utils import class_weight\nimport statsmodels.api as sm\nfrom sklearn.pipeline import Pipeline, make_pipeline\nfrom imblearn.over_sampling import SMOTE\nfrom imblearn.pipeline import Pipeline as imbpipeline, make_pipeline \nfrom sklearn.metrics import precision_recall_curve\nfrom sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import ExtraTreesClassifier\nfrom mlxtend.classifier import StackingClassifier\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom xgboost import XGBClassifier\nfrom sklearn.metrics import accuracy_score\nfrom sklearn.linear_model import LogisticRegressionCV\nimport sklearn.metrics as metrics","f6d38ae7":"df = pd.read_csv('..\/input\/my-dataset\/credit_train.csv')\ndf","62af26e7":"df.shape","d387246c":"df.tail()","d34e34a5":"df.isna().sum()","25fc2155":"df.info() ","898214be":"duplicate = df.duplicated()\nprint(f'Duplicate in df :', duplicate.sum())","92377f73":"df.columns","80b65042":"df.columns = df.columns.str.replace(' ','_')","a61e25c2":"df.columns","ea30d945":"# X = df.drop(columns='Term')\n# y = pd.DataFrame(df['Term'])\n\n# cross val\ndf_train, df_test  = train_test_split(df, test_size=0.2, random_state=42)\n\n# # split the data for train and test\n# df_Train, df_test = train_test_split(df, test_size = 0.2, random_state = 30 )\n\n# # split the train for train and val\n# df_train, df_val = train_test_split(df_Train, test_size = 0.2, random_state = 30 )","57b33da4":"print(f'Shape of train:', df_train.shape)\n# print(f'Shape of validation:', df_val.shape)\nprint(f'Shape of test:', df_test.shape)","5cc3e6cc":"# reset index for train\ndf_train = df_train.reset_index(drop=True)\n\n# # reset index for val\n# df_val = df_val.reset_index(drop=True)\n\n# reset index for val\ndf_test = df_test.reset_index(drop=True)","228549db":"# dope nulls in Loan_ID\n\n# for train\ndf_train = df_train.dropna(subset = ['Loan_ID'])\n\n# # for val\n# df_val = df_val.dropna(subset = ['Loan_ID'])\n\n# for test\ndf_test = df_test.dropna(subset = ['Loan_ID'])","3d21bcf2":"print(f'Shape of train:', df_train.shape)\n# print(f'Shape of validation:', df_val.shape)\nprint(f'Shape of test:', df_test.shape)","f61414ff":"df_train.sample(20)","69de693b":"df_train.info()","3e20071c":"df_train.isna().sum()","75221b88":"# check for dublicate\n\n# for train\nduplicate = df_train.duplicated()\nprint(f'Duplicate in train :', duplicate.sum())\n\n# # for val\n# duplicate = df_val.duplicated()\n# print(f'Duplicate in validation :', duplicate.sum())\n\n# for test\nduplicate = df_test.duplicated()\nprint(f'Duplicate in test :', duplicate.sum())\nprint(f'Shape of train:', df_train.shape)\n# print(f'Shape of validation:', df_val.shape)\nprint(f'Shape of test:', df_test.shape)","41a17d27":"# drop duplicates rows\n# train\ndf_train.drop_duplicates(inplace=True)\n\n# val\n# df_val.drop_duplicates(inplace=True)\n\n# test\ndf_test.drop_duplicates(inplace=True)","37e36be4":"# check for dublicate\n\n# for train\nduplicate = df_train.duplicated()\nprint(f'Duplicate in train :', duplicate.sum())\n\n# # for val\n# duplicate = df_val.duplicated()\n# print(f'Duplicate in validation :', duplicate.sum())\n\n# for test\nduplicate = df_test.duplicated()\nprint(f'Duplicate in test :', duplicate.sum())\nprint(f'Shape of train:', df_train.shape)\n# print(f'Shape of validation:', df_val.shape)\nprint(f'Shape of test:', df_test.shape)","0d9defcc":"df_train['Loan_ID'].value_counts().sort_values(ascending=False)","90b0761d":"df_train[df_train.Loan_ID.duplicated()]","b8af488a":"df_train[df_train['Loan_ID'] == '7830a00a-20c4-4480-9cf0-fe2f86b5266b']","46bb0f50":"df_train[df_train['Loan_ID'] == '5a90cbe3-8fee-4582-8823-1f31546dec6e']","bde60806":"df_train[(df_train.Loan_ID.duplicated() & (df_train['Current_Loan_Amount'] == 99999999.0))]","f3973687":"# drop duplicate in Loan_ID and Current_Loan_Amount = 99999999.0\n\n# for train\ndf_train = df_train[~(df_train.Loan_ID.duplicated() & (df_train['Current_Loan_Amount'] == 99999999.0))]\n\n# for val\n# df_val = df_val[~(df_val.Loan_ID.duplicated() & (df_val['Current_Loan_Amount'] == 99999999.0))]\n\n# for test\ndf_test = df_test[~(df_test.Loan_ID.duplicated() & (df_test['Current_Loan_Amount'] == 99999999.0))]","199e8a73":"df_train[(df_train.Loan_ID.duplicated())]","75dd2b0c":"df_train[df_train['Loan_ID'] == 'ff486b10-f97d-4dff-bb98-436ef48d8ab1']","4da15c7f":"# dope nulls in Loan_Status\n\n# for train\ndf_train = df_train.dropna(subset = ['Annual_Income'])\n\n# # for val\n# df_val = df_val.dropna(subset = ['Annual_Income'])\n\n# for test\ndf_test = df_test.dropna(subset = ['Annual_Income'])","ee61de0f":"df_train[df_train['Loan_ID'] == 'ff486b10-f97d-4dff-bb98-436ef48d8ab1']","d7bdf149":"#df_train\ndf_train.Purpose.unique()\n# #df_val\n# df_val.Purpose.unique()\n#da_test\ndf_test.Purpose.unique()","12ce35b3":"df_train.Purpose.value_counts()","66e0a324":"#df_train\ndf_train.Purpose = df_train.Purpose.str.replace('other','Other')\n# #df_val\n# df_val.Purpose = df_val.Purpose.str.replace('other','Other')\n#df_test\ndf_test.Purpose = df_test.Purpose.str.replace('other','Other')","10965a7e":"df_train.Purpose.value_counts()","725b5f1b":"df_train.Purpose.unique()","786523c6":"df_train.isnull().sum() # train","c8cec912":"# dope duplicated in Loan_ID\n\n# for train\ndf_train = df_train.drop_duplicates(subset = ['Loan_ID'])\n\n# # for val\n# df_val = df_val.drop_duplicates(subset = ['Loan_ID'])\n\n# for test\ndf_test = df_test.drop_duplicates(subset = ['Loan_ID'])","f4058438":"df_train.isnull().sum() # train","63006f22":"print(f'Shape of train:', df_train.shape)\n# print(f'Shape of validation:', df_val.shape)\nprint(f'Shape of test:', df_test.shape)","69c9da19":"plt.figure(figsize=(10,5))\nsns.countplot(df_train['Years_in_current_job'], palette='pink_r');","92ced1ed":"# fill nulls in Years_in_current_job \n\n# for train\ndf_train['Years_in_current_job'] = df_train['Years_in_current_job'].fillna('10+ years')\n\n# # for val\n# df_val['Years_in_current_job'] = df_val['Years_in_current_job'].fillna('10+ years')\n\n# for test\ndf_test['Years_in_current_job'] = df_test['Years_in_current_job'].fillna('10+ years')","60674508":"# drop Months_since_last_delinquent bc the null > 50&\n\n# train\ndf_train = df_train.drop(columns='Months_since_last_delinquent')\n\n# test\ndf_test = df_test.drop(columns='Months_since_last_delinquent')","4b050941":"df_train.isnull().sum()","74575311":"# drop nulls \n\n# for train\ndf_train = df_train.dropna()\n\n# # for val\n# df_val = df_val.dropna()\n\n# for test\ndf_test = df_test.dropna()","aac2aa63":"df_train.isnull().sum()","0cf3a877":"df_train.duplicated().sum()","06fe58f1":"df_train.isnull().sum()","0a87aabf":"df_train.info()","8c256cb6":"# train\nbank_lone_train = pd.get_dummies(df_train, columns =['Term','Home_Ownership','Purpose','Loan_Status', 'Years_in_current_job'], drop_first=True) ###\n\n# # val\n# bank_lone_val = pd.get_dummies(df_val, columns =['Term','Home_Ownership','Purpose','Loan_Status', 'Years_in_current_job'], drop_first=True) ###\n\n# test\nbank_lone_test = pd.get_dummies(df_test, columns =['Term','Home_Ownership','Purpose','Loan_Status' , 'Years_in_current_job'], drop_first=True) ###","4372c391":"bank_lone_train.columns","0dd88526":"df_train.corr()","350477c2":"plt.figure(figsize=(10,8))\n\n# corr\ndata_corr = df_train.corr()\n# data_corr = bank_lone_train.corr()\n\n# mask\nmask = np.triu(np.ones_like(data_corr, dtype=np.bool))\n\n# adjust mask and df\nmask = mask[1:, :-1]\ncorr = data_corr.iloc[1:,:-1].copy()\n\nsns.heatmap(corr, cmap = 'pink_r', annot = True, vmin= -1, vmax= 1, linewidths=1.5, fmt='.2f', mask=mask);\nplt.title('CORRELATION BETWEEN FEATURES\\n', loc='left', fontsize=18);\n# plt.savefig('plot13.png', dpi = 300, bbox_inches = 'tight');","f2e1ae91":"# sns.pairplot(bank_lone_train, hue = 'Term_Short Term', palette = 'pink_r');","1a32792e":"c = ['#724949','#cfa691', '#120f0f', '#a06868']\nplt.figure(figsize=(7,7))\nplt.pie(x = bank_lone_train['Term_Short Term'].value_counts(),\n        labels=['Short term','Long term'],autopct='%.2f%%',\n        textprops={'fontsize': 12},explode=[0,0.09], colors = ['#724949','#DEDCBB'])\nplt.title('Time Period of Taking Loan',fontdict={'fontsize':15});","4394191e":"plt.figure(figsize=(10,9))\nsns.countplot(y='Purpose' , data=df_train, order = df_train['Purpose'].value_counts().index,\n              hue='Term', palette = 'pink_r')\nplt.title('Purpose of taking Loan' , fontdict={'fontsize':20})\nplt.legend(title=\"Loan type\", loc=\"lower right\");","c6c9b918":"plt.figure(figsize=(10,8))\nsns.countplot(x='Home_Ownership',data=df_train ,order = df_train['Home_Ownership'].value_counts().index\n              ,hue='Term',  palette = 'pink_r')\nplt.title('Own Property vs Loan Status',fontdict={'fontsize':20})\nplt.legend(title=\"Loan type\", loc=\"upper right\", labels=[\"Short Term\",\"Long Term\"]);","e000c04d":"plt.figure(figsize = [15,20])\nplt.subplot(3,2,1)\nsns.boxplot(x='Term_Short Term',y='Current_Loan_Amount',\n            palette='pink_r', data=bank_lone_train.sort_values('Current_Loan_Amount',ascending=False));\nplt.title('Before dropping outliers',fontsize = 15 )\n\nbank_lone_train = bank_lone_train[bank_lone_train['Current_Loan_Amount'] != 99999999]\nbank_lone_train = bank_lone_train[((bank_lone_train['Current_Loan_Amount'] <= 600000 )\n                                   & (bank_lone_train['Term_Short Term']==1))\n                                  | (bank_lone_train['Term_Short Term']==0)]\n\nplt.subplot(3,2,2)\nsns.boxplot(x='Term_Short Term',y='Current_Loan_Amount',\n            palette='pink_r', data=bank_lone_train.sort_values('Current_Loan_Amount',ascending=False));\nplt.title('After dropping outliers',fontsize = 15 );","4ff56d3d":"bank_lone_test = bank_lone_test[bank_lone_test['Current_Loan_Amount'] != 99999999]\nbank_lone_test = bank_lone_test[((bank_lone_test['Current_Loan_Amount'] <= 600000 )\n                                   & (bank_lone_test['Term_Short Term']==1))\n                                  | (bank_lone_test['Term_Short Term']==0)]","246867f1":"plt.figure(figsize = [15,20])\nplt.subplot(3,2,1)\nsns.boxplot(x='Term_Short Term',y='Credit_Score',\n            palette='pink_r', data = bank_lone_train.sort_values('Credit_Score',ascending=False));\nplt.title('Before dropping outliers',fontsize = 15 )\n\nbank_lone_train = bank_lone_train.loc[bank_lone_train['Credit_Score'] <= 1500,:]\nbank_lone_train = bank_lone_train.loc[bank_lone_train['Credit_Score'] >= 620 ,:]\nbank_lone_train = bank_lone_train[((bank_lone_train['Credit_Score'] >= 680 )\n                                   & (bank_lone_train['Term_Short Term']==1))| \n                                  (bank_lone_train['Term_Short Term']==0)]\n\nplt.subplot(3,2,2)\nsns.boxplot(x='Term_Short Term',y='Credit_Score',\n            palette='pink_r', data = bank_lone_train.sort_values('Credit_Score',ascending=False));\nplt.title('After dropping outliers',fontsize = 15 );","15cf0df6":"bank_lone_test = bank_lone_test.loc[bank_lone_test['Credit_Score'] <= 1500,:]\nbank_lone_test = bank_lone_test.loc[bank_lone_test['Credit_Score'] >= 620 ,:]\nbank_lone_test = bank_lone_test[((bank_lone_test['Credit_Score'] >= 680 )\n                                   & (bank_lone_test['Term_Short Term']==1))| \n                                  (bank_lone_test['Term_Short Term']==0)]","b299a2e0":"plt.figure(figsize = [15,20])\nplt.subplot(3,2,1)\nsns.boxplot(x='Term_Short Term',y='Annual_Income',\n            palette='pink_r', data = bank_lone_train.sort_values('Annual_Income',ascending=False));\nplt.title('Before dropping outliers',fontsize = 15 )\n\nbank_lone_train = bank_lone_train.loc[bank_lone_train['Annual_Income'] <= 2750000,:]\nbank_lone_train = bank_lone_train[((bank_lone_train['Annual_Income'] <= 2395000 )\n                                   & (bank_lone_train['Term_Short Term']==1))\n                                  | (bank_lone_train['Term_Short Term']==0)]\n\nplt.subplot(3,2,2)\nsns.boxplot(x='Term_Short Term',y='Annual_Income',\n            palette='pink_r', data = bank_lone_train.sort_values('Annual_Income',ascending=False));\nplt.title('After dropping outliers',fontsize = 15 );","d15144bb":"bank_lone_test = bank_lone_test.loc[bank_lone_test['Annual_Income'] <= 2750000,:]\nbank_lone_test = bank_lone_test[((bank_lone_test['Annual_Income'] <= 2395000 )\n                                   & (bank_lone_test['Term_Short Term']==1))\n                                  | (bank_lone_test['Term_Short Term']==0)]","5634fdeb":"plt.figure(figsize = [15,20])\nplt.subplot(3,2,1)\nsns.boxplot(x='Term_Short Term',y='Monthly_Debt',\n            palette='pink_r', data=bank_lone_train.sort_values('Monthly_Debt',ascending=False));\nplt.title('Before dropping outliers',fontsize = 15)\n\nbank_lone_train = bank_lone_train.loc[bank_lone_train['Monthly_Debt'] <= 44500,:]\nbank_lone_train = bank_lone_train[((bank_lone_train['Monthly_Debt'] <= 36000 )& (bank_lone_train['Term_Short Term']==1))| \n                                  (bank_lone_train['Term_Short Term']==0)]\n\nplt.subplot(3,2,2)\nsns.boxplot(x='Term_Short Term',y='Monthly_Debt',\n            palette='pink_r', data=bank_lone_train.sort_values('Monthly_Debt',ascending=False));\nplt.title('After dropping outliers',fontsize = 15 );","e0db4284":"bank_lone_test = bank_lone_test.loc[bank_lone_test['Monthly_Debt'] <= 44500,:]\nbank_lone_test = bank_lone_test[((bank_lone_test['Monthly_Debt'] <= 36000 )& \n                                   (bank_lone_test['Term_Short Term']==1))| \n                                  (bank_lone_test['Term_Short Term']==0)]","0598cf72":"plt.figure(figsize = [15,20])\nplt.subplot(3,2,1)\nsns.boxplot(x='Term_Short Term',y='Current_Credit_Balance',\n            palette='pink_r', data=bank_lone_train.sort_values('Current_Credit_Balance',ascending=False));\nplt.title('Before dropping outliers',fontsize = 15 )\n\nbank_lone_train = bank_lone_train.loc[bank_lone_train['Current_Credit_Balance'] <= 760000,:]\nbank_lone_train = bank_lone_train[((bank_lone_train['Current_Credit_Balance'] <= 504000 )& \n                                   (bank_lone_train['Term_Short Term']==1))| (bank_lone_train['Term_Short Term']==0)]\n\nplt.subplot(3,2,2)\nsns.boxplot(x='Term_Short Term',y='Current_Credit_Balance',\n            palette='pink_r', data=bank_lone_train.sort_values('Current_Credit_Balance',ascending=False));\nplt.title('After dropping outliers',fontsize = 15 );","847d15d0":"bank_lone_test = bank_lone_test.loc[bank_lone_test['Current_Credit_Balance'] <= 760000,:]\nbank_lone_test = bank_lone_test[((bank_lone_test['Current_Credit_Balance'] <= 504000 )& \n                                   (bank_lone_test['Term_Short Term']==1))| \n                                  (bank_lone_test['Term_Short Term']==0)]","53d5f730":"plt.figure(figsize = [15,20])\nplt.subplot(3,2,1)\nsns.boxplot(x='Term_Short Term',y='Maximum_Open_Credit',\n            palette='pink_r', data=bank_lone_train.sort_values('Maximum_Open_Credit',ascending=False));\nplt.title('Before dropping outliers',fontsize = 15)\n\nbank_lone_train = bank_lone_train.loc[bank_lone_train['Maximum_Open_Credit'] <= 1400000,:]\nbank_lone_train = bank_lone_train[((bank_lone_train['Maximum_Open_Credit'] <= 990000 )& \n                                   (bank_lone_train['Term_Short Term']==1))| (bank_lone_train['Term_Short Term']==0)]\nplt.subplot(3,2,2)\nsns.boxplot(x='Term_Short Term',y='Maximum_Open_Credit',\n            palette='pink_r', data=bank_lone_train.sort_values('Maximum_Open_Credit',ascending=False));\nplt.title('After dropping outliers',fontsize = 15 );","f84ac41e":"bank_lone_test = bank_lone_test.loc[bank_lone_test['Maximum_Open_Credit'] <= 1400000,:]\nbank_lone_test = bank_lone_test[((bank_lone_test['Maximum_Open_Credit'] <= 990000 )& \n                                   (bank_lone_test['Term_Short Term']==1))| \n                                  (bank_lone_test['Term_Short Term']==0)]","d53301ae":"plt.figure(figsize=(8, 12))\nheatmap = sns.heatmap(bank_lone_train.corr()[\n    ['Term_Short Term']].sort_values(by='Term_Short Term',ascending=False),\n                      vmin=-1, vmax=1, annot=True,\n                      cmap = 'pink_r')\nplt.title('CORRELATION BETWEEN FEATURES AND TARGET AFTER ONE HOT CODING\\n', loc='center', fontsize=18);","e3b81584":"X_train = bank_lone_train.drop(['Term_Short Term','Loan_ID','Customer_ID',\n                                'Credit_Score', 'Years_of_Credit_History', \n                                'Number_of_Credit_Problems', 'Number_of_Open_Accounts',\n                                'Bankruptcies'], axis = 1)\ny_train = bank_lone_train['Term_Short Term']\nX_test = bank_lone_test.drop(['Term_Short Term','Loan_ID','Customer_ID',\n                                'Credit_Score', 'Years_of_Credit_History', \n                                'Number_of_Credit_Problems', 'Number_of_Open_Accounts',\n                                'Bankruptcies'], axis = 1)\ny_test = bank_lone_test['Term_Short Term']","93ecca69":"model = sm.OLS(y_train,X_train)\nfit = model.fit()\nfit.summary()","f7b5c4ff":"y_train.value_counts()","8adc568b":"# Separate class\nlong_term_0 = bank_lone_train[bank_lone_train['Term_Short Term'] == 0]\nshort_term_1 = bank_lone_train[bank_lone_train['Term_Short Term'] == 1]# print the shape of the class\nprint('Long term 0:', long_term_0.shape[0])\nprint('Short term 1:', short_term_1.shape[0])","d539a463":"# gridsearch\nparams = {'C': [0.001,0.01,0.1,1,10,100,1000], 'class_weight':[{0:0,0:0.01,0:0.1,0:0.5,0:1,0:10,0:2} ] }\nlr_grid = GridSearchCV(LogisticRegression(), param_grid = params, scoring='f1', cv = 5)\nlr_grid.fit(X_train, y_train)\nprint('\\n Best param after grid search: ', lr_grid.best_params_ )\nprint(' Best f1_score for cross validation: ',lr_grid.best_score_ )\n\n# normal\nLR = LogisticRegression(C= 0.001 ,solver='liblinear')\nkf = KFold(n_splits=10, random_state=42, shuffle=True)\ncr_f1 = cross_val_score(LR, X_train, y_train, scoring='f1', cv=kf)\nprint('\\n Normal Logistic Regression Valdition F1: \\n',cr_f1)\nprint('\\n Mean Normal Logistic Regression Valdition F1: \\n',cr_f1.mean())\nprint('--------------------------------')\n\n# balenced\nlr_balanced = LogisticRegression(class_weight='balanced', solver='liblinear')\ncr_balnced_f1 = cross_val_score(lr_balanced, X_train, y_train, scoring='f1', cv=kf)\nprint('\\n Balanced class weights Logistic Regression Valdition F1: \\n',cr_balnced_f1)\nprint('\\n Mean Balanced class weights Logistic Regression Valdition F1: \\n',cr_balnced_f1.mean())\nprint('--------------------------------')\n\n# weighted\nlr_4x = LogisticRegression(C= 0.001, class_weight={0 : 2, 1 : 1}, solver='liblinear')\ncr_weghts_f1 = cross_val_score(lr_4x, X_train, y_train, scoring='f1', cv=kf)\nprint('\\n Class weights Logistic Regression Valdition F1: \\n',cr_weghts_f1)\nprint('\\n Mean Class weights Logistic Regression Valdition F1: \\n',cr_weghts_f1.mean())\nprint('--------------------------------')\n\n# smote\nimba_pipeline = make_pipeline(SMOTE(random_state=42), \n                              LogisticRegression(C= 0.001, solver='liblinear'))\n\nimba_val = cross_val_score(imba_pipeline, X_train, y_train, scoring='f1', cv=kf)\nprint('\\n Smote Logistic Regression Valdition F1: \\n', imba_val)\nprint('\\n Mean Smote Logistic Regression Valdition F1: \\n', imba_val.mean())","0ebda58d":"LR.fit(X_train, y_train)\nprecision_curve, recall_curve, threshold_curve = precision_recall_curve(y_train, LR.predict_proba(X_train)[:,1] )\nplt.plot(threshold_curve, precision_curve[1:],label='precision', color = '#724949')\nplt.plot(threshold_curve, recall_curve[1:], label='recall', color = '#DEDCBB')\nplt.legend(loc='lower left')\nplt.xlabel('Threshold (above this probability)');\nplt.title('Precision and Recall Curves');","d64a17e9":"y_predict = (LR.predict_proba(X_train)[:, 1] >= 0.65)\n\nprint(\"Default threshold:\")\nprint(\"Precision: {:6.4f},   Recall: {:6.4f}\".format(precision_score(y_train, y_predict), \n                                                     recall_score(y_train, y_predict)))","21183c75":"y_predict = (LR.predict_proba(X_train)[:, 1] >= 0.624)\n\nloan_confusion = confusion_matrix(y_train, y_predict)\n\nsns.heatmap(loan_confusion , cmap = 'pink_r', annot = True , square = True , fmt = 'd',\n           xticklabels = ['long term','short term'],\n           yticklabels = ['long term','short term'])\nplt.title('Train Confusion Matrix',fontsize = 15)\nplt.xlabel('prediction')\nplt.ylabel('actual');","d2ee4410":"# confusion matrix for crossval\ny_pred = cross_val_predict(LR, X = X_train, y = y_train, cv=5)\nconf_mat = confusion_matrix(y_train, y_pred)\nsns.heatmap(conf_mat , cmap = 'pink_r', annot = True , square = True , fmt = 'd',\n           xticklabels = ['long term','short term'],\n           yticklabels = ['long term','short term'])\nplt.title('Cross Validation Confusion Matrix',fontsize = 15)\nplt.xlabel('prediction')\nplt.ylabel('actual');","73947e80":"knn = KNeighborsClassifier(n_neighbors= 9)\nk_range = list(range(3,11))\nparam_grd = dict(n_neighbors=k_range)\ngrid = GridSearchCV(KNeighborsClassifier(), param_grd, scoring='f1', cv = 5)\ngrid.fit(X_train, y_train)\nprint('Best estimator: ', grid.best_estimator_ )\nprint('Best f1_score for cross validation: ',grid.best_score_ )","6fbddc7b":"y_predict = (grid.predict_proba(X_train)[:, 1] >= 0.624)\n\nloan_confusion = confusion_matrix(y_train, y_predict)\n\nsns.heatmap(loan_confusion , cmap = 'pink_r', annot = True , square = True , fmt = 'd',\n           xticklabels = ['long term','short term'],\n           yticklabels = ['long term','short term'])\nplt.title('Train Confusion Matrix',fontsize = 15)\nplt.xlabel('prediction')\nplt.ylabel('actual');","22d3cf4e":"# normal\nDecision_Tree = DecisionTreeClassifier(max_depth = 8)\nDecision_Tree.fit(X_train, y_train)\nscores = cross_val_score(Decision_Tree, X_train, y_train, cv=5, scoring='f1')\nprint('Normal Decision Tree Valdition F1:',scores.mean())\n\n\n# balenced\ndt_bal = DecisionTreeClassifier(max_depth = 8, class_weight='balanced')\ndt_bal.fit(X_train, y_train)\nscores = cross_val_score(dt_bal, X_train, y_train, cv=5, scoring='f1')\nprint('Balanced class weights Decision Tree Valdition F1:',scores.mean())\n\n\n# weighted\ndt_wtd = DecisionTreeClassifier(class_weight= {0 : 10, 1 : 1})\nscores = cross_val_score(dt_wtd, X_train, y_train, cv=5, scoring='f1')\ndt_wtd.fit(X_train, y_train)\nprint('10:1 class weights Decision Tree Valdition F1:',scores.mean())\n\n#gridsearch\n\ntree_param = {'criterion':['gini','entropy'],\n              'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]}\n\ngd_sr = GridSearchCV(DecisionTreeClassifier(), param_grid=tree_param,scoring='f1',cv=5,n_jobs=-1)\ngd_sr.fit(X_train, y_train)\nbest_parameters = gd_sr.best_params_\nprint('\\n Best param after grid search', best_parameters)\nprint('\\n Best score after grid search', gd_sr.best_score_)","06493234":"precision_curve, recall_curve, threshold_curve = precision_recall_curve(y_train, Decision_Tree.predict_proba(X_train)[:,1] )\nplt.plot(threshold_curve, precision_curve[1:],label='precision', color = '#724949')\nplt.plot(threshold_curve, recall_curve[1:], label='recall', color = '#DEDCBB')\nplt.legend(loc='lower left')\nplt.xlabel('Threshold (above this probability)');\nplt.title('Precision and Recall Curves');","10c929bb":"y_predict = (Decision_Tree.predict_proba(X_train)[:, 1] >= 0.5569)\n\nprint(\"Default threshold:\")\nprint(\"Precision: {:6.4f},   Recall: {:6.4f}\".format(precision_score(y_train, y_predict), \n                                                     recall_score(y_train, y_predict)))","1898ecba":"y_predict = (Decision_Tree.predict_proba(X_train)[:, 1] >= 0.61)\n\nloan_confusion = confusion_matrix(y_train, y_predict)\n\nsns.heatmap(loan_confusion , cmap = 'pink_r', annot = True , square = True , fmt = 'd',\n           xticklabels = ['long term','short term'],\n           yticklabels = ['long term','short term'])\nplt.title('Train Confusion Matrix',fontsize = 15)\nplt.xlabel('prediction')\nplt.ylabel('actual');","4009fb92":"# confusion matrix for crossval\ny_pred = cross_val_predict(Decision_Tree, X = X_train, y = y_train, cv=5)\nconf_mat = confusion_matrix(y_train, y_pred)\nsns.heatmap(conf_mat , cmap = 'pink_r', annot = True , square = True , fmt = 'd',\n           xticklabels = ['long term','short term'],\n           yticklabels = ['long term','short term'])\nplt.title('Cross Validation Confusion Matrix',fontsize = 15)\nplt.xlabel('prediction')\nplt.ylabel('actual');","62fc6933":"# normal\nRandom_Forest = RandomForestClassifier(n_estimators = 5, random_state=1)\nRandom_Forest.fit(X_train, y_train)\nscores = cross_val_score(Random_Forest, X_train, y_train, cv=10, scoring='f1')\nprint('\\n Normal Random Forest Valdition F1: \\n',scores)\nprint('\\n Mean Normal Random Forest Valdition F1:  \\n',scores.mean())\n\n# balenced\nrf_bal = RandomForestClassifier(n_estimators = 10, random_state=1, class_weight='balanced')\nrf_bal.fit(X_train, y_train)\nscores = cross_val_score(rf_bal, X_train, y_train, cv=10, scoring='f1')\nprint('\\n Balanced class weights Random Forest Valdition F1: \\n',scores)\nprint('\\n Mean Balanced class weights Random Forest Valdition F1: \\n',scores.mean())\n\n# weighted\nrf_wtd = RandomForestClassifier(n_estimators = 10, random_state=1, class_weight= {0 : 2, 1 : 1})\nrf_wtd.fit(X_train, y_train)\nscores = cross_val_score(rf_wtd, X_train, y_train, cv=10, scoring='f1')\nprint('\\n 2:1 class weights Random Forest Valdition F1:\\n',scores)\nprint('\\n 2:1 class weights Random Forest Valdition F1: \\n',scores.mean())\n","1a779de4":"precision_curve, recall_curve, threshold_curve = precision_recall_curve(y_train, Random_Forest.predict_proba(X_train)[:,1] )\nplt.plot(threshold_curve, precision_curve[1:],label='precision', color = '#724949')\nplt.plot(threshold_curve, recall_curve[1:], label='recall', color = '#DEDCBB')\nplt.legend(loc='lower left')\nplt.xlabel('Threshold (above this probability)');\nplt.title('Precision and Recall Curves');","50f55676":"y_predict = (Random_Forest.predict_proba(X_train)[:, 1] >= 0.66)\n\nprint(\"Default threshold:\")\nprint(\"Precision: {:6.4f},   Recall: {:6.4f}\".format(precision_score(y_train, y_predict), \n                                                     recall_score(y_train, y_predict)))","3cb75cb7":"y_predict = (Random_Forest.predict_proba(X_train)[:, 1] >= 0.61)\n\nloan_confusion = confusion_matrix(y_train, y_predict)\n\nsns.heatmap(loan_confusion , cmap = 'pink_r', annot = True , square = True , fmt = 'd',\n           xticklabels = ['long term','short term'],\n           yticklabels = ['long term','short term'])\nplt.title('Train Confusion Matrix',fontsize = 15)\nplt.xlabel('prediction')\nplt.ylabel('actual');","867bc33d":"# confusion matrix for crossval\ny_pred = cross_val_predict(Random_Forest, X = X_train, y = y_train, cv=5)\nconf_mat = confusion_matrix(y_train, y_pred)\nsns.heatmap(conf_mat , cmap = 'pink_r', annot = True , square = True , fmt = 'd',\n           xticklabels = ['long term','short term'],\n           yticklabels = ['long term','short term'])\nplt.title('Cross Validation Confusion Matrix',fontsize = 15)\nplt.xlabel('prediction')\nplt.ylabel('actual');","98b8119e":"Extra_Tree = ExtraTreesClassifier()\nExtra_Tree.fit(X_train, y_train)\nscores = cross_val_score(Extra_Tree, X_train, y_train, cv =5, scoring = 'f1')\nprint('f1_scores for validation: ',scores)\nprint('Mean f1_score for validation: ',scores.mean())","3e241d2f":"y_predict = (Extra_Tree.predict_proba(X_train)[:, 1] >= 0.1)\n\nprint(\"Default threshold:\")\nprint(\"Precision: {:6.4f},   Recall: {:6.4f}\".format(precision_score(y_train, y_predict), \n                                                     recall_score(y_train, y_predict)))","cb41bbbe":"# confusion matrix for crossval\ny_pred = cross_val_predict(Extra_Tree, X = X_train, y = y_train, cv=5)\nconf_mat = confusion_matrix(y_train, y_pred)\nsns.heatmap(conf_mat , cmap = 'pink_r', annot = True , square = True , fmt = 'd',\n           xticklabels = ['long term','short term'],\n           yticklabels = ['long term','short term'])\nplt.title('Cross Validation Confusion Matrix',fontsize = 15)\nplt.xlabel('prediction')\nplt.ylabel('actual');","39c9e572":"lr = LogisticRegression() \nstacked = StackingClassifier(classifiers =[knn,Decision_Tree, lr], meta_classifier = lr, use_probas = False)\nmodel_stack = stacked.fit(X_train, y_train)   # training of stacked model\naccuracies = cross_val_score(estimator = model_stack, X = X_train, y = y_train, cv = 5, scoring='f1')\nprint('f1_score stacking for cross validation : ',accuracies)\nprint('Mean f1_score stacking for cross validation : ',accuracies.mean())","02a82eb9":"precision_curve, recall_curve, threshold_curve = precision_recall_curve(y_train, stacked.predict_proba(X_train)[:,1] )\nplt.plot(threshold_curve, precision_curve[1:],label='precision', color = '#724949')\nplt.plot(threshold_curve, recall_curve[1:], label='recall', color = '#DEDCBB')\nplt.legend(loc='lower left')\nplt.xlabel('Threshold (above this probability)');\nplt.title('Precision and Recall Curves');","785fc944":"y_predict = (stacked.predict_proba(X_train)[:, 1] >= 0.1)\n\nprint(\"Default threshold:\")\nprint(\"Precision: {:6.4f},   Recall: {:6.4f}\".format(precision_score(y_train, y_predict), \n                                                     recall_score(y_train, y_predict)))","923c6c1d":"# confusion matrix for crossval\ny_pred = cross_val_predict(stacked, X = X_train, y = y_train, cv=5)\nconf_mat = confusion_matrix(y_train, y_pred)\nsns.heatmap(conf_mat , cmap = 'pink_r', annot = True , square = True , fmt = 'd',\n           xticklabels = ['long term','short term'],\n           yticklabels = ['long term','short term'])\nplt.title('Cross Validation Confusion Matrix',fontsize = 15)\nplt.xlabel('prediction')\nplt.ylabel('actual');","a5be3b08":"bag_clf = BaggingClassifier(\n    DecisionTreeClassifier(), n_estimators=50,\n    max_samples=100, bootstrap=True, n_jobs=-1)\nbag_clf.fit(X_train, y_train)\n\naccuracies = cross_val_score(estimator = bag_clf, X = X_train, y = y_train, cv = 5, scoring='f1')\nprint('f1_score Bagging for cross validation : ',accuracies)\nprint('Mean f1_score Bagging for cross validation : ',accuracies.mean())","e4a3f387":"precision_curve, recall_curve, threshold_curve = precision_recall_curve(y_train, bag_clf.predict_proba(X_train)[:,1] )\nplt.plot(threshold_curve, precision_curve[1:],label='precision', color = '#724949')\nplt.plot(threshold_curve, recall_curve[1:], label='recall', color = '#DEDCBB')\nplt.legend(loc='lower left')\nplt.xlabel('Threshold (above this probability)');\nplt.title('Precision and Recall Curves');","da95485c":"y_predict = (bag_clf.predict_proba(X_train)[:, 1] >= 0.1)\n\nprint(\"Default threshold:\")\nprint(\"Precision: {:6.4f},   Recall: {:6.4f}\".format(precision_score(y_train, y_predict), \n                                                     recall_score(y_train, y_predict)))","b37c58f3":"# confusion matrix for crossval\ny_pred = cross_val_predict(bag_clf, X = X_train, y = y_train, cv=5)\nconf_mat = confusion_matrix(y_train, y_pred)\nconf_mat","3eac1ada":"# confusion matrix for crossval\ny_pred = cross_val_predict(bag_clf, X = X_train, y = y_train, cv=5)\nconf_mat = confusion_matrix(y_train, y_pred)\nsns.heatmap(conf_mat , cmap = 'pink_r', annot = True , square = True , fmt = 'd',\n           xticklabels = ['long term','short term'],\n           yticklabels = ['long term','short term'])\nplt.title('Cross Validation Confusion Matrix',fontsize = 15)\nplt.xlabel('prediction')\nplt.ylabel('actual');","af6f1d9e":"param_grid = {'base_estimator__criterion' : ['gini', 'entropy'],\n              'base_estimator__splitter' :   ['best', 'random'],\n              'n_estimators': [1, 5, 10, 20, 100, 500]\n             }\nDTC = DecisionTreeClassifier(random_state = 0)\nABC = AdaBoostClassifier(base_estimator = DTC)\n\n# run grid search\ngrid_search_ABC = GridSearchCV(ABC, param_grid=param_grid, scoring = 'f1')\ngrid_search_ABC.fit(X_train, y_train)\n\nprint('\\n Best param after grid search', grid_search_ABC.best_params_)\nprint('\\n Best score after grid search', grid_search_ABC.best_score_)","84183cd8":"precision_curve, recall_curve, threshold_curve = precision_recall_curve(y_train, grid_search_ABC.predict_proba(X_train)[:,1] )\nplt.plot(threshold_curve, precision_curve[1:],label='precision', color = '#724949')\nplt.plot(threshold_curve, recall_curve[1:], label='recall', color = '#DEDCBB')\nplt.legend(loc='lower left')\nplt.xlabel('Threshold (above this probability)');\nplt.title('Precision and Recall Curves');","321e039d":"y_predict = (grid_search_ABC.predict_proba(X_train)[:, 1] >= 0.1)\n\nprint(\"Default threshold:\")\nprint(\"Precision: {:6.4f},   Recall: {:6.4f}\".format(precision_score(y_train, y_predict), \n                                                     recall_score(y_train, y_predict)))","163c071e":"# confusion matrix for crossval\ny_pred = cross_val_predict(grid_search_ABC, X = X_train, y = y_train, cv=5)\nconf_mat = confusion_matrix(y_train, y_pred)\nsns.heatmap(conf_mat , cmap = 'pink_r', annot = True , square = True , fmt = 'd',\n           xticklabels = ['long term','short term'],\n           yticklabels = ['long term','short term'])\nplt.title('Cross Validation Confusion Matrix',fontsize = 15)\nplt.xlabel('prediction')\nplt.ylabel('actual');","ee169b6d":"gbc = GradientBoostingClassifier(n_estimators = 100, learning_rate = 0.05)\ngbc.fit(X_train, y_train)\naccuracies = cross_val_score(estimator = gbc, X = X_train, y = y_train, cv = 5, scoring='f1')\nprint('f1_score Gradient Boosting for cross validation : ',accuracies)\nprint('Mean f1_score Gradient Boosting for cross validation : ',accuracies.mean())","1997db41":"precision_curve, recall_curve, threshold_curve = precision_recall_curve(y_train, gbc.predict_proba(X_train)[:,1] )\nplt.plot(threshold_curve, precision_curve[1:],label='precision', color = '#724949')\nplt.plot(threshold_curve, recall_curve[1:], label='recall', color = '#DEDCBB')\nplt.legend(loc='lower left')\nplt.xlabel('Threshold (above this probability)');\nplt.title('Precision and Recall Curves');","4973922a":"y_predict = (gbc.predict_proba(X_train)[:, 1] >= 0.1)\n\nprint(\"Default threshold:\")\nprint(\"Precision: {:6.4f},   Recall: {:6.4f}\".format(precision_score(y_train, y_predict), \n                                                     recall_score(y_train, y_predict)))","11ea0080":"# confusion matrix for crossval\ny_pred = cross_val_predict(gbc, X = X_train, y = y_train, cv=5)\nconf_mat = confusion_matrix(y_train, y_pred)\nsns.heatmap(conf_mat , cmap = 'pink_r', annot = True , square = True , fmt = 'd',\n           xticklabels = ['long term','short term'],\n           yticklabels = ['long term','short term'])\nplt.title('Cross Validation Confusion Matrix',fontsize = 15)\nplt.xlabel('prediction')\nplt.ylabel('actual');","9f399e02":"X_train.columns = X_train.columns.str.replace('<','less').str.replace(' ','_')\n\nxgboost = XGBClassifier(n_estimators = 100, learning_rate = 0.05)\nxgboost.fit(X_train, y_train)\naccuracies = cross_val_score(estimator = xgboost, X = X_train, y = y_train, cv = 5, scoring='f1')\nprint('f1_score XGBoost for cross validation : ',accuracies)\nprint('Mean f1_score XGBoost for cross validation : ',accuracies.mean())","d2c74d39":"precision_curve, recall_curve, threshold_curve = precision_recall_curve(y_train, xgboost.predict_proba(X_train)[:,1] )\nplt.plot(threshold_curve, precision_curve[1:],label='precision', color = '#724949')\nplt.plot(threshold_curve, recall_curve[1:], label='recall', color = '#DEDCBB')\nplt.legend(loc='lower left')\nplt.xlabel('Threshold (above this probability)');\nplt.title('Precision and Recall Curves');","3e194339":"y_predict = (xgboost.predict_proba(X_train)[:, 1] >= 0.1)\n\nprint(\"Default threshold:\")\nprint(\"Precision: {:6.4f},   Recall: {:6.4f}\".format(precision_score(y_train, y_predict), \n                                                     recall_score(y_train, y_predict)))","3bee8494":"# confusion matrix for crossval\ny_pred = cross_val_predict(xgboost, X = X_train, y = y_train, cv=5)\nconf_mat = confusion_matrix(y_train, y_pred)\nsns.heatmap(conf_mat , cmap = 'pink_r', annot = True , square = True , fmt = 'd',\n           xticklabels = ['long term','short term'],\n           yticklabels = ['long term','short term'])\nplt.title('Cross Validation Confusion Matrix',fontsize = 15)\nplt.xlabel('prediction')\nplt.ylabel('actual');","53cf637f":"# X_train.columns = X_train.columns.str.replace('<','less').str.replace(' ','_')\n\n# estimator = XGBClassifier(\n#     objective= 'binary:logistic',\n#     nthread=4,\n#     seed=42)\n\n# parameters = {\n#     'max_depth': range (2, 10, 1),\n#     'n_estimators': range(60, 220, 40),\n#     'learning_rate': [0.1, 0.01, 0.05]}\n\n# grid_search = GridSearchCV(estimator=estimator,param_grid=parameters,scoring = 'f1',n_jobs = 10,cv = 10,\n#                            verbose=True)\n# grid_search.fit(X_train, y_train)\n\n# print('score',grid_search.best_estimator_)","54752608":"log=LogisticRegression() \nrnd=RandomForestClassifier()\ndct=DecisionTreeClassifier()\nvoting_classifer = VotingClassifier(estimators=[('lr',log),('rf',rnd),('dt',dct)],voting='hard',n_jobs=-1)\nvoting_classifer.fit(X_train, y_train)\nscores = cross_val_score(voting_classifer, X_train, y_train, cv=5, scoring='f1')\nprint('f1_score Voting Classifer for cross validation : ',scores)\nprint('Mean f1_score Voting Classifer for cross validation : ',scores.mean())","9cd6fa1e":"# models = [('rf', gd_sr1 ), ('ad', grid_search_ABC )]\n# vc_wtd = VotingClassifier(estimators= models, voting='hard', n_jobs=-1)\n# vc_wtd.fit(X_train, y_train)\n# scores = cross_val_score(vc_wtd, X_train, y_train, cv=10, scoring='f1')\n# print('lass weights Random Forest Valdition F1:',scores.mean())","a6e26d76":"y_pred_ =xgboost.predict(X_test)\ny_pred_1 =xgboost.predict(X_train)\nscores_1 = metrics.f1_score(y_train, y_pred_1)\nscores = metrics.f1_score(y_test, y_pred_)\nprint('Test score for XGBoost: ', scores)","cea6e87e":"import pickle\n\ndata = {\"model\": xgboost} \nwith open('saved_steps.pkl', 'wb') as file:\n    pickle.dump(data, file)","fb9d53e6":"### Cleaning data","a2f611b0":"## Random Forest Classifier\n---","ebaf2719":"**Duplicate in Loan ID**","69a72c1c":"## Logistic Regression\n---","0d0c496c":"**Rename columns for easer code writing**","1d13ed95":"# Conclusion\nIn Conclusion, after examen multiple classification models, it is clear that the best model to predict whether a loan is a short term or a long term is XGBoost classification model, which gave the highest cross validation F1. The last step for this project is to train the best model using cross validation sets (80% of the data), and test it using the splatted data for testing (20% of the data set). Then F1 score for both the training and testing will be printed. F1 for the cross validition set is 0.8731 and for the testing set 0.8691 as shown above. However, For future work, more classification models will be examined.","959ead15":"## Voting Classifer (HARD)\n---","10e2288f":"## Import packages & read data.","bcf9eb07":"### plot the correlation after one hot coding","9fc49671":"We can see an error in data entry. There is a duplicate in loan ID but the difference in current loan amount or null values,\n\n**Now we fix it.**","eb49bf63":"* ### AdaBoost","48c5a5e4":"## Visualize data\n___","dc464b25":"## KNN Model\n---","b63cc080":"## Split the data for train, validation and test","eb7f271c":"## Feature Engneering\n---","468ff381":"---","5264b0e4":"# Bank Loan Term Prediction\n---","61e27e80":"## Boosting\n---","86236219":"### Get Dummies","109b5042":"* ### XGBoost","4815829a":"## Data Pre-processing","413ff8e6":"## Bagging\n---","84a513fa":"## Decision Tree Classifier\n---","3dfc2a1a":"## Stacking\n---","1aef4d03":"### Droping outliers","51ad52ba":"___\n# Test\n___","55ff5933":"* ### Gradient Boosting","22b5cd64":"## Extra Tree\n---"}}