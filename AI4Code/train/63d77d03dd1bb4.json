{"cell_type":{"8a295045":"code","beefa6d7":"code","5bf4aace":"code","5899e4a0":"code","cdadeb15":"code","c54be6d1":"code","91d09b72":"code","663825b0":"code","65eed21e":"code","5266e051":"code","ed396f70":"code","88cbe3c6":"code","8ffd5448":"code","46f11687":"code","9257ec94":"code","bb129bef":"code","30d5e2af":"code","56bee8a7":"code","e1118972":"markdown","db5f6f0d":"markdown","ff884a1c":"markdown","85dc6058":"markdown","aac3c08f":"markdown","1f9f52e0":"markdown","18567b2e":"markdown","bf2d9c9f":"markdown","73bd2e3a":"markdown","1462b75e":"markdown","6c33c204":"markdown","40731cdb":"markdown","f2505dc8":"markdown","c7396aa0":"markdown"},"source":{"8a295045":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nfrom sklearn.model_selection import train_test_split, KFold, GridSearchCV\nfrom sklearn.preprocessing import LabelEncoder\nfrom xgboost import XGBRegressor\nfrom sklearn.metrics import mean_absolute_error\n\nfrom scipy.stats import zscore\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# Running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","beefa6d7":"X = pd.read_csv('..\/input\/home-data-for-ml-course\/train.csv', index_col='Id')\nX_test = pd.read_csv('..\/input\/home-data-for-ml-course\/test.csv', index_col='Id')\n\ny = X.SalePrice\nX.drop(['SalePrice'], axis=1, inplace=True)","5bf4aace":"sum(X.columns != X_test.columns)","5899e4a0":"print('Train data: {}'.format(X.shape))\nprint('Train data targets: {}'.format(len(y)))\nprint('*' *25)\nprint('Test data: {}'.format(X_test.shape))\nprint('*' *25, '\\n')\nX.info()","cdadeb15":"# Concatenation of train and test sets for data preprocessing\nX_all = pd.concat([X, X_test], axis=0)\n\n# A first look at columns with missing values\ncols_with_na = X_all.isna().sum()[X_all.isna().sum() > 0].sort_values(ascending = False)\n\nplt.figure(figsize=(8, 8))\nsns.barplot(x = cols_with_na.values, y = cols_with_na.index, color = 'green', edgecolor = 'black')\nplt.title('NA values')","c54be6d1":"# Remove columns with more than 10% missing values\nna_limit = X_all.shape[0] * 0.1\ndrop_cols = cols_with_na[cols_with_na > na_limit].index\n\nX_all.drop(drop_cols, axis=1, inplace=True)\n\n# Check the sum of missing values in each column\ncols_with_na = X_all.isna().sum()[X_all.isna().sum() > 0].sort_values(ascending = False)\n\nplt.figure(figsize=(8, 8))\nsns.barplot(x = cols_with_na.values, y = cols_with_na.index, color = 'green', edgecolor = 'black')\nplt.title('NA values')","91d09b72":"# Columns for replacing\nnumeric_missing = [col for col in X_all.columns if X_all[col].dtype in ['float64', 'int64'] and col in cols_with_na.index]\nobject_missing = [x for x in cols_with_na.index if x not in numeric_missing]\n\n# Have a look at object_missing columns\nfig = plt.figure(figsize=(20, 15))\np = 1\nfor i in object_missing:\n    fig.add_subplot(3, 6, p)\n    X_all[i].hist()\n    plt.title(i)\n    p += 1\nplt.show()","663825b0":"# Based on visualization above we divide object columns by replacement type\nmode_missing = ['Functional', 'Utilities', 'SaleType', 'Electrical']\nnone_missing = [x for x in object_missing if x not in mode_missing]\nprint('Mode: {}'.format(mode_missing))\nprint('None: {}'.format(none_missing))","65eed21e":"# Replace all missing values\nfor col in numeric_missing:\n    X_all[col] = X_all[col].fillna(0)\n\nfor col in mode_missing:\n    X_all[col] = X_all[col].fillna(X_all[col].mode()[0])\n\nfor col in none_missing:\n    X_all[col] = X_all[col].fillna('none')\n    \n# Check the final result\nX_all.isna().sum()[X_all.isna().sum() > 0]","5266e051":"# One-Hot encoding\nprint(X_all.shape)\nX_all = pd.get_dummies(X_all).reset_index(drop=True)\nprint(X_all.shape)","ed396f70":"# Label encoding\n# Create the list of categorical columns\n# categorical_cols = [cname for cname in X_all.columns if X_all[cname].dtype == 'object']\n\n# encoder = LabelEncoder()\n# for col in categorical_cols:\n#     X_all[col] = encoder.fit_transform(X_all[col])","88cbe3c6":"numeric_features = [col for col in X_all.columns if X_all[col].dtype in ['float64', 'int64']]\n\nmean = X_all[numeric_features].mean(axis=0)\nstd = X_all[numeric_features].std(axis=0)\n\nX_all[numeric_features] -= mean # centering\nX_all[numeric_features] \/= std # scaling","8ffd5448":"# Return train and test sets \nX_new = X_all.iloc[:1460, :]\nX_test_new = X_all.iloc[1460: , :]\n\n# Create data sets for training (80%) and validation (20%)\nX_train, X_valid, y_train, y_valid = train_test_split(X_new, y, train_size=0.8, test_size=0.2, random_state=0)","46f11687":"X_new.head(5)","9257ec94":"# The basic model\nparams = {'random_state': 0}\n\nmodel = XGBRegressor(**params)\n\nmodel.fit(X_train, y_train, verbose=False)\n\npreds = model.predict(X_valid)\nprint('Valid MAE of the basic model: {}'.format(mean_absolute_error(preds, y_valid)))","bb129bef":"# The best model\nparams = {'n_estimators': 4000,\n          'max_depth': 6,\n          'min_child_weight': 3,\n          'learning_rate': 0.02,\n          'subsample': 0.7,\n          'random_state': 0}\n\nmodel = XGBRegressor(**params)\n\nmodel.fit(X_train, y_train, verbose=False)\n\npreds = model.predict(X_valid)\nprint('Valid MAE of the best model: {}'.format(mean_absolute_error(preds, y_valid)))","30d5e2af":"params = {'n_estimators': 4000,\n          'max_depth': 6,\n          'min_child_weight': 3,\n          'learning_rate': 0.02,\n          'subsample': 0.7,\n          'random_state': 0}\n\nmodel = XGBRegressor(**params)\n\nmodel.fit(X_new, y, verbose=False)","56bee8a7":"predictions = model.predict(X_test_new)\n\noutput = pd.DataFrame({'Id': X_test.index,\n                       'SalePrice': predictions})\noutput.to_csv('..\/..\/kaggle\/working\/submission.csv', index=False)","e1118972":"## 1. Remove or replace missing values\nAt first, we'll remove columns that have more than 10% of missing values.\n\nAt second, we'll replace missing values in remain columns.","db5f6f0d":"Let's see if columns in train and test datasets are the same.\nThe result must be zero.","ff884a1c":"## 3. Data transformation","85dc6058":"Now, our data looks like this:","aac3c08f":"Good sign! Let's look at data general information.","1f9f52e0":"## 2. Encode categorical features","18567b2e":"Excellent! Now, our data has no missing values.","bf2d9c9f":"# Introduction\n\nIn this notebook, I'll show you a quick and brief way to TOP 3% result in solving a House Pricing regression problem.\nHere you'll find only: \n- simple data preprocessing \n- and modeling,\n\nwithout extensive and deep data analisis and delightful visualizations. There are many great notebooks dedicated to this because it wasn't my goal.\n\nIt's the second version of notebook. I'm ready for bug feedbacks and constructive criticism :) Hope, you'll find it useful!\n\n**See also R version of this notebook: [A quick way to TOP 3% with R](https:\/\/www.kaggle.com\/maksymshkliarevskyi\/a-quick-way-to-top-3-with-r)**","73bd2e3a":"**Thank you for attention! Hope it has helped you! I'll be grateful see your opinion in comments and appreciation with an upvote.**","1462b75e":"# **The first look at the data**","6c33c204":"# **Simple data preprocessing**\nFor this task my algorithm of data preprocessing includes 3 simple and quick steps:\n* remove or replace all missing values\n* encode categorical features (convert them into integers because model can't work with categorical data)\n* data transformation (centering and scaling)","40731cdb":"And train final model on all data (train and validation) for submission.","f2505dc8":"# Modeling","c7396aa0":"# Load data"}}