{"cell_type":{"8b9d73e0":"code","6f46c71e":"code","d6f616cc":"code","bd50e7fa":"code","7b49a9a5":"code","d4c74c24":"code","a51eb417":"code","43d4d2c4":"code","dafb211a":"code","bcf729db":"code","c64467c0":"code","3e0fd4c4":"code","58ea0779":"code","df0a3768":"code","6ce73509":"code","8e67481f":"code","68418f3d":"code","27347ceb":"code","af69f3e1":"code","293a9aed":"code","7214f9c7":"code","3e1bf815":"code","7930b786":"code","8699b360":"code","4528cc93":"code","d364620a":"code","e09dab67":"code","76df4163":"code","841abfa6":"code","301638e8":"code","cc4579d3":"code","9420f55d":"code","3155a374":"code","46fba297":"code","852217e4":"code","4bc3942c":"markdown","0808fc51":"markdown","df2a218e":"markdown","1ccd3944":"markdown","08dc8408":"markdown","c4c830f2":"markdown","9ced0755":"markdown","05f913eb":"markdown","0f9a80d9":"markdown","f5df615a":"markdown","59cbf2c1":"markdown","8bc63454":"markdown","0a16ccf5":"markdown","f20e1a53":"markdown"},"source":{"8b9d73e0":"import pandas as pd\nimport numpy as np\nimport matplotlib as mpl\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport sklearn\n\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport sklearn.metrics as metrics\nfrom sklearn.metrics import make_scorer, accuracy_score, roc_auc_score\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\n","6f46c71e":"df=pd.read_csv(\"..\/input\/health-insurance-cross-sell-prediction\/train.csv\")","d6f616cc":"df.head()","bd50e7fa":"df.info()","7b49a9a5":"pd.isnull(df).sum()","d4c74c24":"df.nunique()","a51eb417":"##ditribution of Response\nfig_dims = (5, 5)\nfig, ax = plt.subplots()\nsns.countplot('Response',\n              data = df,\n              order = df['Response'].value_counts().index,\n              ax = ax)\nax.set(xlabel='Response', ylabel='Count')\nplt.show()","43d4d2c4":"#ditribution of Gender,Driving_License,Previously_Insured,Previously_Insured\nfig, axarr = plt.subplots(2, 2, figsize=(10, 10))\n\ndf['Gender'].value_counts().sort_index().plot.pie(\n    ax=axarr[0][0])\naxarr[0][0].set_title(\"Gender\", fontsize=18)\ndf['Previously_Insured'].value_counts().sort_index().plot.pie(\n    ax=axarr[1][0])\naxarr[1][0].set_title(\"Previously_Insured\", fontsize=18)\n\ndf['Vehicle_Damage'].value_counts().sort_index().plot.pie(\n    ax=axarr[1][1])\naxarr[1][1].set_title(\"Vehicle_Damage\", fontsize=18)\n\ndf['Driving_License'].value_counts().head().plot.pie(\n    ax=axarr[0][1])\naxarr[0][1].set_title(\"Driving_License\", fontsize=18)","dafb211a":"fig=plt.figure(figsize=(5, 5))\nsns.countplot(x=\"Gender\", hue=\"Vehicle_Damage\", data=df)\nplt.title(\"Vehicle Damage by Gender\")","bcf729db":"#ditribution of Age\nfig_dims = (15, 8)\nfig, ax = plt.subplots(figsize=fig_dims)\nsns.countplot('Age',\n              data = df,\n              ax = ax)\nax.set(xlabel='Age', ylabel='Count')\nplt.show()","c64467c0":"df.head()","3e0fd4c4":"# represent binary variable as 1and 0\ndf['Gender'].replace(to_replace={'Male':0,'Female':1},\n             inplace=True)\ndf['Vehicle_Damage'].replace(to_replace={'No':0,'Yes':1},\n             inplace=True)\ndf['Vehicle_Age'].replace(to_replace={'< 1 Year':0,'1-2 Year':1,'> 2 Years':2},\n             inplace=True)","58ea0779":"df.info()","df0a3768":"df.head()","6ce73509":"plt.figure(figsize=(10,10))\ncor=df.corr()\nsns.heatmap(cor,annot=True,cmap=plt.cm.Blues)\nplt.show()","8e67481f":"df.describe()","68418f3d":"df=df.drop(columns=['id'])","27347ceb":"y=df.Response\nX=df.drop(columns=['Response'])","af69f3e1":"## split into 70%train set and 30%test\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)","293a9aed":"dt = DecisionTreeClassifier()\ndt.fit(X_train, y_train)\ndt_predict = dt.predict(X_test)\n","7214f9c7":"print(classification_report(y_test, dt_predict))\ndt_accuracy = accuracy_score(y_test, dt_predict)\nprint(\"Accuracy of decision tree\" + ' : ' + str(dt_accuracy))","3e1bf815":" # Compute 10-fold cross-validation scores: cv_scores\nfrom sklearn.model_selection import cross_val_score \ncv_scores = cross_val_score(dt,X,y,cv=10)\n\nprint(cv_scores)\nprint(\"Average 10-Fold CV Score: {}\".format(np.mean(cv_scores)))","7930b786":"# use  GridSearchCV to test all accuracy, and choose the combinations of the highest accuracy\nfrom sklearn.model_selection import GridSearchCV\nparam_grid = {'max_depth': np.arange(3, 10),\n             'criterion' : ['gini','entropy'],\n             'max_leaf_nodes': [5,10,50,100],\n             'min_samples_split': [2, 5, 10, 20]}\ngrid_tree = GridSearchCV(DecisionTreeClassifier(), param_grid, cv = 5, scoring= 'accuracy')\ngrid_tree.fit(X_train, y_train)\nnp.abs(grid_tree.best_score_)\n#test the accuracy of all the combination of the parameters, then output the highest parameter.\nprint(grid_tree.best_estimator_)","8699b360":"# use the best performance combinations  to test\nTree = DecisionTreeClassifier(ccp_alpha=0.0, class_weight=None, criterion='gini',\n                       max_depth=9, max_features=None, max_leaf_nodes=50,\n                       min_impurity_decrease=0.0, min_impurity_split=None,\n                       min_samples_leaf=1, min_samples_split=2,\n                       min_weight_fraction_leaf=0.0, presort='deprecated',\n                       random_state=None, splitter='best')\nTree.fit(X_train, y_train)\npredictions = Tree.predict(X_test)\naccuracy_score(y_true = y_test, y_pred = predictions)","4528cc93":"import sklearn.metrics as metrics\n# calculate the fpr and tpr for all thresholds of the classification\nprobs = dt.predict_proba(X_test)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\n#  plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic for Decision Tree')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n","d364620a":"rf = RandomForestClassifier()\nrf.fit(X_train, y_train)\nrf_Predict = rf.predict(X_test)","e09dab67":"print(classification_report(y_test, rf_Predict))\nrf_accuracy = accuracy_score(y_test, rf_Predict)\nprint(\"Accuracy of rf\" + ' : ' + str(rf_accuracy))","76df4163":"cv_scores = cross_val_score(rf,X,y,cv=10)\n\nprint(cv_scores)\nprint(\"Average 10-Fold CV Score: {}\".format(np.mean(cv_scores)))","841abfa6":"# Plot ROC_AUC for random forest\nprobs = rf.predict_proba(X_test)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\n#  plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic for Random Forest')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n","301638e8":"from sklearn.linear_model import LogisticRegression\nlr = LogisticRegression()\nlr.fit(X_train, y_train)\nlr_predict = lr.predict(X_test)","cc4579d3":"print(classification_report(y_test, lr_predict))\nlr_accuracy = accuracy_score(y_test, lr_predict)\nprint(\"Accuracy of Logistic Regression\" + ' : ' + str(lr_accuracy))","9420f55d":"# Plot ROC_AUC for logistic regression\nprobs = lr.predict_proba(X_test)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\n\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic for Logistic Regression')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n","3155a374":"# build the knn model and calculate the accuracy score when n=10\nknn = KNeighborsClassifier(n_neighbors=10)\nknn.fit(X_train, y_train)\nknn_predict = knn.predict(X_test)","46fba297":"\nknn_accuracy = accuracy_score(y_test, knn_predict)\nprint(\"Accuracy of Logistic Regression\" + ' : ' + str(knn_accuracy))","852217e4":"# Plot ROC_AUC for knn\nprobs = knn.predict_proba(X_test)\npreds = probs[:,1]\nfpr, tpr, threshold = metrics.roc_curve(y_test, preds)\nroc_auc = metrics.auc(fpr, tpr)\n\n#  plt\nimport matplotlib.pyplot as plt\nplt.title('Receiver Operating Characteristic for KNN')\nplt.plot(fpr, tpr, 'b', label = 'AUC = %0.2f' % roc_auc)\nplt.legend(loc = 'lower right')\nplt.plot([0, 1], [0, 1],'r--')\nplt.xlim([0, 1])\nplt.ylim([0, 1])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show()\n","4bc3942c":"### KNN","0808fc51":"### Correlation Heatmap","df2a218e":"Find male are more likely to cause Vehicle_Damage lol.","1ccd3944":"# Modeling","08dc8408":"Check missing value","c4c830f2":"# Split into Train and Test set","9ced0755":"Descriptive Statistic","05f913eb":"### decision tree","0f9a80d9":"## Improve the decision tree model ","f5df615a":"# Data description","59cbf2c1":"## Plot ROC_AUC","8bc63454":"### Logistic Regression","0a16ccf5":"### Random Forest","f20e1a53":"# Visualization"}}