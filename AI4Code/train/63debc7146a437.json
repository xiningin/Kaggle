{"cell_type":{"529f6fb4":"code","e3a0bef2":"code","e4d57801":"code","01a566f0":"code","695aa12c":"code","36ec49a0":"code","05d12efe":"code","8f70ee2e":"code","354fde23":"code","4d8c24fd":"code","e3b3b69e":"code","d5ca9fa2":"code","3cbc9733":"code","6b1365a3":"code","513a977d":"code","14338d89":"code","c52c07fe":"code","67128c1c":"code","8f525266":"code","468181f7":"code","40bc0ab8":"code","fcfa23f2":"code","3a74cd84":"code","0503cb96":"code","a4489529":"code","22f04a59":"code","0753efa6":"code","9a85fde5":"code","72760658":"code","09689e14":"code","ac95d2fd":"code","0b1f7cfc":"code","46f303c0":"code","47b73379":"code","c3d1d828":"code","1264e8ee":"code","23d8b951":"code","c623c3ed":"code","b8826924":"code","e4a7a9cc":"code","8b00baf1":"code","89e9928e":"code","a1ed1ca9":"code","14f00c75":"code","d5a93f09":"code","d5f94b3f":"code","0e93bdd6":"code","fc8c1cc9":"code","d6bf14cc":"code","540a1790":"code","c120cb4a":"code","13f3651e":"code","7209e6ff":"code","75bde71c":"code","1a8722d8":"code","cad09241":"markdown","ce9d2769":"markdown","31a819b3":"markdown","4767f69e":"markdown","52246647":"markdown","548a2648":"markdown","1f145f5c":"markdown","f75dcaca":"markdown","24bf86ed":"markdown","159c2907":"markdown","28c3221c":"markdown","55f3856d":"markdown","01f58a80":"markdown","95ecac15":"markdown","54829bac":"markdown","66e4db29":"markdown","15f24101":"markdown","0057956e":"markdown","28b8ae49":"markdown","5e0c1f05":"markdown","0311c709":"markdown","c1766ca1":"markdown","dcb15eca":"markdown","5f8d0c1d":"markdown","c76a8eda":"markdown","f7265280":"markdown","11689575":"markdown","316e058d":"markdown","2329dfc7":"markdown","31a9e221":"markdown","7de1e991":"markdown","a8d2567e":"markdown","d4506cf6":"markdown","5d071031":"markdown","e0d576c3":"markdown","251ae6de":"markdown","9f08a269":"markdown","05bc7f0d":"markdown","25f594e0":"markdown","7b8f8b71":"markdown"},"source":{"529f6fb4":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","e3a0bef2":"## data analysis for credit_card frauds ","e4d57801":"import pandas as pd \nimport numpy as np\nimport matplotlib.pyplot as plt \nimport seaborn as sns","01a566f0":"df_cred=pd.read_csv(\"\/kaggle\/input\/creditcardfraud\/creditcard.csv\")","695aa12c":"df_cred.shape","36ec49a0":"from scipy.stats import randint as sp_randint\nfrom scipy.stats import uniform as sp_uniform\nimport warnings\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\nimport plotly.graph_objs as go\nimport plotly.tools as tls\nimport plotly.figure_factory as ff\n\nwarnings.filterwarnings('ignore')\n\nfrom contextlib import contextmanager\n\n@contextmanager\ndef timer(title):\n    t0 = time.time()\n    yield\n    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))\n","05d12efe":"plt.style.use('ggplot') # Using ggplot2 style visuals \n\nf, ax = plt.subplots(figsize=(11, 15))\n\nax.set_facecolor('#fafafa')\nax.set(xlim=(-5, 5))\nplt.ylabel('Variables')\nplt.title(\"Overview Data Set\")\nax = sns.boxplot(data = df_cred.drop(columns=['Amount', 'Class', 'Time']), \n  orient = 'h', \n  palette = 'Set2')","8f70ee2e":"fraud = df_cred[(df_cred['Class'] != 0)]\nnormal = df_cred[(df_cred['Class'] == 0)]\n\ntrace = go.Pie(labels = ['Normal', 'Fraud'], values = df_cred['Class'].value_counts(), \n               textfont=dict(size=15), opacity = 0.8,\n               marker=dict(colors=['lightskyblue','gold'], \n                           line=dict(color='#000000', width=1.5)))\n\n\nlayout = dict(title =  'Distribution of target variable')\n           \nfig = dict(data = [trace], layout=layout)\npy.iplot(fig)","354fde23":"# Def plot distribution\ndef plot_distribution(data_select) : \n    figsize =( 15, 8)\n    sns.set_style(\"ticks\")\n    s = sns.FacetGrid(df_cred, hue = 'Class',aspect = 2.5, palette ={0 : 'lime', 1 :'black'})\n    s.map(sns.kdeplot, data_select, shade = True, alpha = 0.6)\n    s.set(xlim=(df_cred[data_select].min(), df_cred[data_select].max()))\n    s.add_legend()\n    s.set_axis_labels(data_select, 'proportion')\n    s.fig.suptitle(data_select)\n    plt.show()","4d8c24fd":"plot_distribution('V4')\nplot_distribution('V9')\nplot_distribution('V11')\nplot_distribution('V12')\nplot_distribution('V13')","e3b3b69e":"from sklearn import preprocessing\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import confusion_matrix , roc_auc_score, roc_curve","d5ca9fa2":"### dropping off unncessary columns","3cbc9733":"min_max_scaler = preprocessing.MinMaxScaler()\ndf_cred=df_cred.drop(\"Time\",axis=1)\ndf_cred_scaled = min_max_scaler.fit_transform(df_cred.iloc[:,:-1])\ndf_cred_normalized = pd.DataFrame(df_cred_scaled)","6b1365a3":"df_cred_normalized[\"Class\"]=df_cred[\"Class\"]","513a977d":"df_cred_normalized[\"Class\"].value_counts()","14338d89":"df_cred_normalized_train=df_cred_normalized[df_cred_normalized[\"Class\"]==0]\ndf_cred_normalized_test=df_cred_normalized[df_cred_normalized[\"Class\"]==1]","c52c07fe":"df_cred_normalized_test_part_1=df_cred_normalized_train.sample(frac=0.05)\ndf_cred_normalized_train=df_cred_normalized_train.drop(df_cred_normalized_test_part_1.index)\ndf_cred_normalized_test_part_2=df_cred_normalized_train.sample(frac=0.05)\ndf_cred_normalized_train=df_cred_normalized_train.drop(df_cred_normalized_test_part_2.index)","67128c1c":"df_cred_normalized_test_class_1=df_cred_normalized_test.sample(frac=0.5)\ndf_cred_normalized_validation_class_1=df_cred_normalized_test.drop(df_cred_normalized_test_class_1.index)","8f525266":"df_cred_normalized_test_class_1.shape","468181f7":"df_cred_normalized_test_set=df_cred_normalized_test_part_1.append(df_cred_normalized_test_class_1)\ndf_cred_normalized_validation_set=df_cred_normalized_test_part_2.append(df_cred_normalized_validation_class_1)","40bc0ab8":"print(\"train set dimensions :\",df_cred_normalized_train.shape)\nprint(\"test set dimensions :\",df_cred_normalized_test_set.shape)\nprint(\"validate set dimensions :\",df_cred_normalized_validation_set.shape)","fcfa23f2":"df_cred_normalized_validation_set[\"Class\"].value_counts()","3a74cd84":"X_train, X_test = train_test_split(df_cred_normalized_train, test_size=0.2, random_state=2020)\nX_train = X_train[X_train.Class == 0]\nX_train = X_train.drop(['Class'], axis=1)\ny_test = X_test['Class']\nX_test = X_test.drop(['Class'], axis=1)\nX_train = X_train.values\nX_test = X_test.values\nX_train.shape","0503cb96":"from keras.models import Model, load_model\nfrom keras.layers import Input, Dense\nfrom keras.callbacks import ModelCheckpoint, TensorBoard\nfrom keras.callbacks import EarlyStopping ,ReduceLROnPlateau\nfrom keras.optimizers import Adam\n\n#from keras import regularizers","a4489529":"input_dim = X_train.shape[1]\nencoding_dim = 20\ninput_layer = Input(shape=(input_dim, ))\nencoder = Dense(encoding_dim*2, activation=\"sigmoid\")(input_layer)\nencoder = Dense(encoding_dim, activation=\"sigmoid\")(input_layer)\nencoder = Dense(8,activation=\"sigmoid\")(encoder)\ndecoder = Dense(20, activation='sigmoid')(encoder)\ndecoder = Dense(40, activation='sigmoid')(encoder)\ndecoder = Dense(input_dim, activation='sigmoid')(decoder)\nautoencoder = Model(inputs=input_layer, outputs=decoder)","22f04a59":"nb_epoch = 50\nbatch_size = 32\nautoencoder.compile(optimizer='adam', \n                    loss='mean_squared_error', \n                    metrics=['accuracy'])\nes = EarlyStopping(monitor='val_loss', mode='min', verbose=1,patience=15)\n\ncheckpointer = ModelCheckpoint(filepath=\"model.h5\",\n                               verbose=0,\n                               save_best_only=True)\nhistory = autoencoder.fit(X_train, X_train,\n                    epochs=nb_epoch,\n                    batch_size=batch_size,\n                    shuffle=True,\n                    validation_data=(X_test, X_test),callbacks=[es,checkpointer],\n                    verbose=1)","0753efa6":"plt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title('model loss')\nplt.ylabel('loss')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title('model acc')\nplt.ylabel('accuracy')\nplt.xlabel('epoch')\nplt.legend(['train', 'test'], loc='upper right')\nplt.show()","9a85fde5":"predictions = autoencoder.predict(X_test)\nmse = np.mean(np.power(X_test - predictions, 2), axis=1)\nerror_df = pd.DataFrame({'reconstruction_error': mse,\n                        'true_class': y_test})\nerror_df.describe()","72760658":"y_test=df_cred_normalized_test_set[\"Class\"]\ndf_cred_normalized_test_set=df_cred_normalized_test_set.drop(\"Class\",axis=1)","09689e14":"predictions = autoencoder.predict(df_cred_normalized_test_set)\nmse = np.mean(np.power(df_cred_normalized_test_set - predictions, 2), axis=1)\nerror_df_test = pd.DataFrame({'reconstruction_error': mse,\n                        'true_class': y_test})\nerror_df_test.describe()","ac95d2fd":"fig = plt.figure()\nax = fig.add_subplot(111)\nnormal_error_df = error_df_test[(error_df_test['true_class']== 0) & (error_df_test['reconstruction_error'] < 10)]\n_ = ax.hist(normal_error_df.reconstruction_error.values, bins=200)","0b1f7cfc":"fig = plt.figure()\nax = fig.add_subplot(111)\nfraud_error_df = error_df_test[error_df_test['true_class'] == 1]\n_ = ax.hist(fraud_error_df.reconstruction_error.values, bins=100)\n","46f303c0":"fraud_error_df.describe() ### frauds cases ","47b73379":"normal_error_df.describe() ### non fraud cases","c3d1d828":"error_df_test[\"predicted_class\"]=[1 if x > 0.001 else 0 for x in error_df_test[\"reconstruction_error\"]]","1264e8ee":"error_df_test","23d8b951":"from sklearn.metrics import (confusion_matrix, precision_recall_curve, auc,\n                             roc_curve, recall_score, classification_report, f1_score,\n                             precision_recall_fscore_support)","c623c3ed":"error_df_test[\"predicted_class\"]=[1 if x > 0.001 else 0 for x in error_df_test[\"reconstruction_error\"]]","b8826924":"fpr, tpr, thresholds = roc_curve(error_df_test.true_class, error_df_test.reconstruction_error)\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.001, 1])\nplt.ylim([0, 1.001])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show();","e4a7a9cc":"print(classification_report(error_df_test[\"true_class\"],error_df_test[\"predicted_class\"]))","8b00baf1":"LABELS = [\"Normal\", \"Fraud\"]\ny_pred = [1 if e > 0.004 else 0 for e in error_df_test.reconstruction_error.values]\nconf_matrix = confusion_matrix(error_df_test.true_class,error_df_test.predicted_class)\nplt.figure(figsize=(8, 8))\nsns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\nplt.title(\"Confusion matrix\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","89e9928e":"error_df_test[\"predicted_class\"]=[1 if x > 0.0039888 else 0 for x in error_df_test[\"reconstruction_error\"]]","a1ed1ca9":"print(classification_report(error_df_test[\"true_class\"],error_df_test[\"predicted_class\"]))","14f00c75":"LABELS = [\"Normal\", \"Fraud\"]\ny_pred = [1 if e >  0.0039888 else 0 for e in error_df_test.reconstruction_error.values]\nconf_matrix = confusion_matrix(error_df_test.true_class,error_df_test.predicted_class)\nplt.figure(figsize=(8, 8))\nsns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\nplt.title(\"Confusion matrix\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","d5a93f09":"fpr, tpr, thresholds = roc_curve(error_df_test.true_class, error_df_test.reconstruction_error)\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.001, 1])\nplt.ylim([0, 1.001])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show();","d5f94b3f":"y_test=df_cred_normalized_validation_set[\"Class\"]\ndf_cred_normalized_validation_set=df_cred_normalized_validation_set.drop(\"Class\",axis=1)\npredictions = autoencoder.predict(df_cred_normalized_validation_set)\nmse = np.mean(np.power(df_cred_normalized_validation_set - predictions, 2), axis=1)\nerror_df_test = pd.DataFrame({'reconstruction_error': mse,\n                        'true_class': y_test})\nerror_df_test.describe()","0e93bdd6":"error_df_test[\"predicted_class\"]=[1 if x > 0.003 else 0 for x in error_df_test[\"reconstruction_error\"]]","fc8c1cc9":"print(classification_report(error_df_test[\"true_class\"],error_df_test[\"predicted_class\"]))","d6bf14cc":"fpr, tpr, thresholds = roc_curve(error_df_test.true_class, error_df_test.reconstruction_error)\nroc_auc = auc(fpr, tpr)\n\nplt.title('Receiver Operating Characteristic')\nplt.plot(fpr, tpr, label='AUC = %0.4f'% roc_auc)\nplt.legend(loc='lower right')\nplt.plot([0,1],[0,1],'r--')\nplt.xlim([-0.001, 1])\nplt.ylim([0, 1.001])\nplt.ylabel('True Positive Rate')\nplt.xlabel('False Positive Rate')\nplt.show();","540a1790":"LABELS = [\"Normal\", \"Fraud\"]\ny_pred = [1 if e >  0.00398888 else 0 for e in error_df_test.reconstruction_error.values]\nconf_matrix = confusion_matrix(error_df_test.true_class,error_df_test.predicted_class)\nplt.figure(figsize=(8, 8))\nsns.heatmap(conf_matrix, xticklabels=LABELS, yticklabels=LABELS, annot=True, fmt=\"d\");\nplt.title(\"Confusion matrix\")\nplt.ylabel('True class')\nplt.xlabel('Predicted class')\nplt.show()","c120cb4a":"### what are sessions and data flow graphs ??","13f3651e":"# Import the TensorFlow library\nimport tensorflow.compat.v1 as tf\ntf.disable_v2_behavior()\n\n# Create an empty graph, that will be filled with operation nodes later\nmain_graph=tf.Graph()\n\n\n# Register this graph as default graph. \n# All operations within this context will become operation nodes in this graph\nwith main_graph.as_default():\n    \n    # Create two constants of value 5\n    a=tf.constant(5.0)\n    b=tf.constant(5.0)\n    \n    # Multiply the constants with each other\n    c=tf.multiply(a,b)\n    \n\n# Create a session to execute the dataflow graph\nwith tf.Session(graph=main_graph) as session:\n    \n    # Perform the calculation defined in the dataflow graph and get the result\n    output=session.run(c)\n    print('Result of the multiplication: %d '%output)","7209e6ff":"## what are placeholders ??","75bde71c":"\n# Create an empty graph, that will be filled with operation nodes later\nmain_graph=tf.Graph()\n\n# Register this graph as default graph. \n# All operations within this context will become operation nodes in this graph\nwith main_graph.as_default():\n    \n    # Define the placeholders that will feed python arrays into the dataflow graph\n    a=tf.placeholder(name='a', shape=[5], dtype=tf.float32)\n    b=tf.placeholder(name='b', shape=[5], dtype=tf.float32)\n    \n    c=tf.multiply(a,b)\n    \n# Create a session to execute the dataflow graph\nwith tf.Session(graph=main_graph) as session:\n    \n    # Perform the calculation defined in the dataflow graph and get the result.\n    # We must provide the values for the placeholders with \"feed_dict\" dictionary\n    output=session.run(c, feed_dict={a: [5.0,7.0,3.0,9.0,2.0],\n                                     b: [1.0,2.0,4.0,8.0,4.0],\n                                     })\n    print(output)\n\n   ","1a8722d8":"import numpy as np\nfrom random import shuffle\n\n# list that will contain our training set\ntraining_data=[]\n\n# How many digits should this binary number have?\nn=4\n    \n#Mini-batch size\nbatch_size=8\n\nprint('\\n\\nGeneration of Data...\\n') \nfor i in np.arange(0, 10):\n    \n    # Create a binary number of type string\n    b = bin(i)[2:]\n    l = len(b)\n    b = str(0) * (n - l) + b  \n\n    # Convert binary string number to type float\n    features=np.array(list(b)).astype(float)\n    # Create the corresponding binary label \/ class\n    label=i\n    \n    # Put the feature-label pair into the list\n    training_data.append([features, label])\n\n    print('binary number: %s, decimal number: %d' %(b, i))\n        \n    \n#%%\n# shuffle the data\nshuffle(training_data)  \n\ntraining_data=training_data*1000\n\n# convert the list to np.array     \ntraining_data=np.array(training_data)\n\n\n\n#%%\n\n# Get the next mini-batch of training samples\ndef get_next_batch(n_batch):\n    \n    # Get the next mini-batch of training samples from the dataset\n    features=training_data[n_batch*batch_size:(batch_size*(1+n_batch)),0]\n    labels=training_data[n_batch*batch_size:(batch_size*(1+n_batch)),1]\n    \n    # Reshape the list of arrays into a nxn np.array\n    features = np.concatenate(features).reshape([batch_size, 4])  \n    # Reshape the labels \n    labels=np.reshape(labels, [batch_size])\n    \n    return features, labels\n    \nfeatures, labels=get_next_batch(n_batch=1)\n\nprint('\\n\\nMini-batch of features: \\n')\nprint(features)\nprint('\\n\\nMini-batch of labels: \\n')\nprint(labels)\n\n#%%  \n\n# Create the training graph\nmain_graph=tf.Graph()\n\nwith main_graph.as_default():\n    \n    # Define the placeholders for the features and the labels\n    x=tf.placeholder(dtype=tf.float32,shape=[batch_size, 4], name='features')\n    y=tf.placeholder(dtype=tf.int32, shape=[batch_size], name='labels')\n           \n    # Create the weight matrices and the bias vectors \n    initializer=initializer=tf.random_normal_initializer(mean=0.0, stddev=0.25)\n   \n    W1=tf.get_variable('W1',shape=[4,50], initializer=initializer)\n    W2=tf.get_variable('W2',shape=[50,25], initializer=initializer)\n    W3=tf.get_variable('W3',shape=[25,10], initializer=initializer)\n    \n    b1=tf.get_variable('b1',shape=[50], initializer=initializer)\n    b2=tf.get_variable('b2',shape=[25], initializer=initializer)\n\n    ### Define the forward propagation step ###\n    \n    # First hidden layer\n    z1=tf.matmul(x,W1)+b1\n    a1=tf.nn.tanh(z1)\n    \n    # Second hidden layer\n    z2=tf.matmul(a1,W2)+b2\n    a2=tf.nn.tanh(z2)\n    \n    # Outputlayer, without an activation function (input for the loss function)\n    logits=tf.matmul(a2,W3)\n       \n    # Compute the probability scores after the training)\n    probs=tf.nn.softmax(logits)\n    \n    # Define the loss function\n    loss_op=tf.losses.sparse_softmax_cross_entropy(labels=y, logits=logits)\n       \n    # Perform a gradient descent step\n    optimizer=tf.train.GradientDescentOptimizer(learning_rate=0.001)\n    trainable_parameters = tf.trainable_variables()\n    gradients = tf.gradients(loss_op,trainable_parameters)\n    update_step = optimizer.apply_gradients(zip(gradients, trainable_parameters))\n\n\nprint('\\n\\nStart of the training...\\n')\nwith tf.Session(graph=main_graph) as sess:\n    \n    sess.run(tf.global_variables_initializer())\n\n    # How many mini-batches in total?\n    num_batches=int(10000\/batch_size)\n    \n    loss=0\n\n    #Iterate over the entire training set for 10 times\n    for epoch in range(10):\n            \n        # Iterate over the number of mini-batches\n        for n_batch in range(num_batches-1):\n            \n            # Get the next mini-batches of samples for the training set\n            features, labels=get_next_batch(n_batch)\n              \n            # Perform the gradient descent step on that mini-batch and compute the loss value\n            _, loss_=sess.run((update_step, loss_op), feed_dict={x:features, y:labels})   \n            \n            loss+=loss_\n             \n        print('epoch_nr.: %i, loss: %.3f' %(epoch,(loss\/num_batches)))\n        loss=0 \n    \n    \n    print('\\n\\nTesting the neural network:\\n')\n    # Compute the probability scores for the last mini-batch\n    prob_scores=sess.run(probs, feed_dict={x:features, y:labels})\n    \n    # Iterate over the features and labels from the last mini-batch as well as\n    # the predicitons made by the network, and compare them to check the performance\n    for f, l, p in zip(features, labels, prob_scores):\n    \n        # Get the class with the highest probability score\n        predicted_class=np.argmax(p)\n        # Get the actual probability score\n        predicted_class_score=np.max(p)\n     \n        print('Binary number: %s, decimal number: %i, predicted_class: %i, predicted_prob_score: %.3f' \n              %(str(f), l, predicted_class, predicted_class_score))","cad09241":"### reconstruction error on x_test set","ce9d2769":"### so answer to this curiousity is very simple \n### as we know autoencoder is useful for reconstruciton of values \n### but if I train it on non-fraudulent transaction then it will be able to contruct non-fraudlent only\n### so if I pass fraudulent transaction with non-fraudulent one then mse will be high for fraud transaction one\n### why because it's weight are made on the basis of non-fraudulent transaction \n### then at last I will decide a perfect threshold for classify fraudulent vs non-fraudulent transaction ","31a819b3":"### removing of fractional subset from main train set done \n### now starting up with making of test and validation set ","4767f69e":"## Merging of test and validation sets ","52246647":"# Initial Preprocessing ","548a2648":"### as you can std deviation is not so much varying while reconstruction of training data \n### as far as we are good to go for testing of out main test set and validation set \n### after that we can develope our same model in Tensorflow using TF records for mainline production \n","1f145f5c":"#### let's present our evaluation metrics over the threshold we have decided","f75dcaca":"#### splitting dataset as per strategy I have dicussed \n#### we will train it on non-fraudulent transcation and test on both the classes \n","24bf86ed":"### data preprocessing it requires normalization why normalization \n### whenever we are seeing multiple features which are different ranges of distributions\n### then we should prefer normalization if same range every feature but still lot of within range distribution\n### we should give it standardization","159c2907":"### Generally MSE as loss function but why , Mae we can use too or not?","28c3221c":"\n### 20% percent in test set of 0 class then 10% in validation set of class 0 \n### as we know in training label 1 class will not go but to decide the threshold of mse so that we can \n### classify anomaly perfectly so 50% of the 1 class percent we will be in test set rest 50% in validate set \n(of course class 0 will be there too with class 1 in both sets validation and test)","55f3856d":"![](https:\/\/www.cartoonmotivators.com\/images\/D\/SocketOverload-01.jpg)","01f58a80":"### Now big question why auto-encoder for classification ? \n### how it is possible that neural network which is used for recontruction of input values can be used as a classifier for fraud transcations\n\n![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcQ5aL46qJsIl3AjFoOLyNbn_vdLe2a2tPns9-PikUI8EhpaLTJx&usqp=CAU)\n","95ecac15":"### as we can see that from both graphs MSE for fraudulent cases is x10 times > Non-fraudulent cases ","54829bac":"# Spliting strategy","66e4db29":"### now same for final evaluation set that is our validation-set","15f24101":"### very small mse will be present we are concerned for deciding the threshold in test set","0057956e":"## As we can see fraud cases are negligible and we have to build model to detect anamolies \n\n## so i will be building auto-encoder first in keras then for production level code i will be building it in\n## tensorflow and finally in tensorflow serving API for it's deployment \n## will post github link it it","28b8ae49":"# Autoencoder here we go ","5e0c1f05":"### So guys here I expalined my strategy why I have chosen auto-encoder as for classifying fraud cases but as you can see that training of this model requires lot of computation time atleast 45 minson kaggle's gpu.\n### we have move towards some faster solution not to save our training time but to save company resources as well \n### sometimes even in kaggle competition time contsraint issues can be solved if training can be done much more faster ways \n### but is just not about winining kaggle competition it is about real time working (model deployment should be done in Tensorflow)","0311c709":"### after 22 epochs we achieved plateaue and accuracy of 99.23 in reconstruction on test data ","c1766ca1":"## Feature distribution","dcb15eca":"#### selection of threshold as you can see max is 0.02 but if you observed 3rd quartile range it is really very small in comparison to max one which indicates that in selection of threshold we should not take max into the account because mean value of mse in frauds cases is 0.012 with std of 0.013 \n#### even minimum value of mse in normal transaction is range of minima of fraud cases but it's mse approx 30% is higher from normal transaction\n\n#### so these cases which i saw are very much corner cases means and at extreme points to be get classified correctly","5f8d0c1d":"### Tensorflow offers advance methods for managing and devlopement Neural Nets :\n#### 1> highly efficient data pipeline using tf records\n#### 2> allows you to use tf data pipeline api for feeding the model more faster \n#### 3> allow you to control cpu gpu parrallel scheduling so that data preprocessing and training can be done much more faster but in keras everything is explicityly done by TF which takes time for processing data and training it (cpu gpu schedulling is not able to take place in keras)\n#### 4> model file is very small compared to keras so it faster process , deployable and it high scalability if deployed on aws EKS cluster (performance on images response or any input data will increase to significant scale)\n    ","c76a8eda":"### now checking how much reconstruction error present in class 0 and class 1","f7265280":"### just re-checking size of train test and validate set ","11689575":"### our model is performing really well even on validaiton dataset on fraud cases better than test set","316e058d":"### still need some small part of training set in testing of autoencoder network for reconstruction of values","2329dfc7":"#### so hardware best utilization we have to make it feel tired \n#### utlimately apart from data scientist we need the best engineering skills to deploy our solution \n#### with faster process if we can't then what is the use building such stacked and blend models","31a9e221":"## Advanced methods\n### 1> step - conversion of data to tf records (binary format data)\n### 2> step - use tf.data input pipeline to connect tf records and send to base model\n### 3> step - advance methods to schedule cpu and gpu processing (Parrallelize Data Transformation)\n### 4> step - tf flags, namescopes are really important for documenting your model\n","7de1e991":"## now thershold is changed to focus on precision as primary\n","a8d2567e":"![](https:\/\/encrypted-tbn0.gstatic.com\/images?q=tbn%3AANd9GcTpdOSZuDXbWZdIaoSJRT2LvruTlATWgrEkgtKZhLaQpY6Sj60i&usqp=CAU)","d4506cf6":"### now question comes up why precision is so low while recall is high \n### see we are actually testing imbalance test but in real world frauds cases will be \n### like this so what should we do ?\n### focus on precision why see we can't catch every fraud but what we can catch as fraud case should be fraud to save company's money and customer as well","5d071031":"# Basic example NN as classfication using TF placeholders","e0d576c3":"### so conclusion that out of 246 fraud cases 209 we were able to classify correctly \n### and normal transaction 352 cases model declaring them as fraud out of 14k cases\n### so here threshold is making final model is like that 85 percent cases model can detect but it not precise so much , means model yes or no has no value , only what model can do is put those cases in suspect but can't bring final conclusions\n\n## *Stats can might change after running the kernel but it will approximate to those which were stated earlier","251ae6de":"### our roc-curve is telling that our model is doing really great in classifying both the classes\n### but one should never forget sample size of test cases are imbalanced so always precision and recall \n### before deploying the model in real world ","9f08a269":"### so why keras as framework and it's model file should not be sent in model deployment (in production)?","05bc7f0d":"# What type of activations ?\n\nLinear: Autoencoders with a single hidden layer with k hidden neurons and linear activations create equivalent representations to PCA with k principal components.\n\nBinary: It is often used as introduction to ANN and not in real world applications.\n\nReLU: Rectified linear units are widely used in deep learning models. However, they are not suitable for AEs because they distort the decoding process by outputting 0 for negative inputs and consequently, do not lead to faithful representations of the input features.\n\nSELU: Scaled exponential linear units activation function is a formidable alternative to ReLU as it preserves the advantages of linearly passing the positive inputs while it enables the flow of negative too.\nSigmoid: The most commonly used activation function for autoencoders.\n\nTanh: Hyperbolic tangent is similar to sigmoid with the difference that is symmetric to the origin and its slope is steeper. As a result, it produces stronger gradients than sigmoid and should be preferred.\n","25f594e0":"# So let's start with Tensorflow what is first we need to do \n#### data exploration is done already \n\n## Basic understanding :\n### tensorflow session ,dataflow graphs , placeholders , variables , training  \n\n### so that you can come to know how in tf neural nets are structured first why the need of advance methods arises up \n\n","7b8f8b71":"# Evaluation of mse on both classes on test set"}}