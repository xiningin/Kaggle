{"cell_type":{"4fe1c08f":"code","8a64b04b":"code","6afad850":"code","42e6a091":"code","c59418a6":"code","b35c721b":"code","a6b5b4d5":"code","040e5aa0":"code","120c49b7":"code","482b4acb":"code","47bbb7d1":"code","2f301cb6":"code","e866e0b5":"code","741c6067":"code","a6e51bb6":"code","7172f5bd":"code","222e867a":"markdown","a8d9ce02":"markdown","aa6b96ca":"markdown","fa346d44":"markdown","babdda4c":"markdown","2c88209d":"markdown","b1613f22":"markdown","295964b2":"markdown","00aa7763":"markdown","1456ba16":"markdown","60dcc220":"markdown","c4e61373":"markdown"},"source":{"4fe1c08f":"import numpy as np\nimport pandas as pd\n\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\nimport os\nfrom tqdm import tqdm\n\nimport cv2","8a64b04b":"train = pd.read_csv(\"..\/input\/shopee-code-league-2020-product-detection\/train.csv\")\nprint(train.shape)\ntrain.head(10)","6afad850":"plt.figure(figsize=(10,10))\nplt.title(\"Distribution of labels for training data\")\nsns.countplot(train['category'])","42e6a091":"new_train=pd.DataFrame()\n\nCATEGORIES=[n for n in range(10)]\n\nfor cat in CATEGORIES:\n    new_train=new_train.append(train[train['category']==cat][:1600])\n\ndel train\n\ntrain=new_train.sample(frac=1)\ntrain","c59418a6":"plt.figure(figsize=(10,10))\nsns.countplot(train['category'])","b35c721b":"resized_img_dim=150\n\ndef read_img(train,resized_img_dim):\n    \n    DATADIR='..\/input\/shopee-code-league-2020-product-detection\/resized\/train'\n    X=[]\n\n    for fname,cat in tqdm(train.values):\n        if(cat<10):\n            cat='0'+str(cat)\n        else:\n            cat=str(cat)\n\n        path=os.path.join(DATADIR,cat,fname)\n\n        try:\n            img=cv2.imread(path).astype('float32')\n            img=cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n            img=cv2.resize(img, (resized_img_dim,resized_img_dim))\n        except:\n            pass\n        X.append(img)\n    return X\n\nX=read_img(train,resized_img_dim)","a6b5b4d5":"from sklearn.preprocessing import OneHotEncoder\n\ny=train['category']\n\nohe=OneHotEncoder()\ny=ohe.fit_transform(y.values.reshape(-1,1)).astype('float32')\ny=y.todense()","040e5aa0":"from sklearn.model_selection import train_test_split\n\nX=np.array(X).reshape(-1,resized_img_dim,resized_img_dim,3)\n\nxtrain,xtest,ytrain,ytest=train_test_split(X,y,test_size=0.2)","120c49b7":"del X\ndel y\ndel train","482b4acb":"from keras.preprocessing.image import ImageDataGenerator\n\ntrain_datagen = ImageDataGenerator(rescale=1.\/255, zoom_range=0.3, rotation_range=30,\n                                   width_shift_range=0.1, height_shift_range=0.1, shear_range=0.1, \n                                   horizontal_flip=True, fill_mode='constant')\n\nval_datagen = ImageDataGenerator(rescale=1.\/255)\n\ntrain_generator = train_datagen.flow(xtrain, ytrain, batch_size=30)\nval_generator = val_datagen.flow(xtest, ytest, batch_size=20)","47bbb7d1":"plt.figure(figsize=(10,10))\nfor xbatch,ybatch in train_generator:\n    for i in range(1,10):\n        plt.subplot(3,3,i)\n        plt.axis('off')\n        plt.imshow(((xbatch[i]*255).astype('uint8')))\n    break","2f301cb6":"import tensorflow as tf\nimport keras\nfrom keras.models import Sequential, Model\nfrom keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, Input, BatchNormalization\n","e866e0b5":"from keras.applications.vgg19 import VGG19\n\ninput_shape_=(resized_img_dim,resized_img_dim,3)\n\nvgg=VGG19(include_top=False, input_shape=input_shape_)\n\noutput = vgg.layers[-1].output\noutput = Flatten()(output)\n\nvgg_model=Model(vgg.input,output)\n\n\nprint(vgg_model.summary())","741c6067":"pretrained_model = Sequential()\n\npretrained_model.add(vgg_model)\n\npretrained_model.add(Dense(256,activation='relu', input_dim=input_shape_))\npretrained_model.add(Dropout(0.4))\n\npretrained_model.add(Dense(10, activation='softmax'))\n\npretrained_model.compile(loss='categorical_crossentropy',\n              optimizer=keras.optimizers.RMSprop(lr=2e-5),\n              metrics=['accuracy'])\n\nmodel_callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=10, verbose=0)]\n\nhistory = pretrained_model.fit_generator(train_generator, steps_per_epoch=100, epochs=100,\n                              validation_data=val_generator, validation_steps=50, \n                              verbose=1, callbacks=model_callbacks)  ","a6e51bb6":"#Accuracy\nplt.plot(history.history['accuracy'])\nplt.plot(history.history['val_accuracy'])\nplt.title(\"Training accuracy vs Validation accuracy\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Accuracy\")\nplt.show()\n\n#Loss\nplt.plot(history.history['loss'])\nplt.plot(history.history['val_loss'])\nplt.title(\"Training loss vs Validation loss\")\nplt.xlabel(\"Epochs\")\nplt.ylabel(\"Loss\")\nplt.show()","7172f5bd":"score=pretrained_model.evaluate(xtest,ytest)","222e867a":"## Observations\n1. There are a total of 42 categories\n2. Category 33 is significantly lesser than the rest (approx 570), resulting in the model learning disproportionately for that category\n\n## Solution\n* We can resample the training data to contain ~570 images from each category.\n* Alternatively, we can augment images to increase the amount of data\n","a8d9ce02":"# Splitting training data","aa6b96ca":"# Exploratory Data Analysis","fa346d44":"# Modifying our training dataset\n\n* In order to reduce computational storage and time taken, we will only be using the first 10 categories of which contains 1500 image each\n* Shuffle the training data so that the examples fed into the model will create an 'independent' change","babdda4c":"## Observations\n\n1. The labels are not shuffled\n2. The feature column contains the filename of the images","2c88209d":"# Reading the training images\n\n* Convert category labels from int to strings defined from 00 to 41.\n* Resize image resolution (trail and error)","b1613f22":"# Take a look at the distribution of categories","295964b2":"## Visualize the modified training dataset","00aa7763":"## Examples of augmented images","1456ba16":"# Setting up training labels\n* One Hot Encode labels","60dcc220":"# Training and Evaluation\n\n1. Using a simple self defined CNN\n2. Using a pretrained model (VGG)\n\n\n* Evaluation by visualizing training accuracy\/loss vs validation accuracy\/loss","c4e61373":"# Augmenting image\n\n* Provides a wider range of image for the model to learn from "}}