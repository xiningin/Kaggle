{"cell_type":{"7e5851f1":"code","5c91140e":"code","f90f7c20":"code","dd8a8a26":"code","ab5d3d49":"code","97b8007c":"code","d1029076":"code","fe216649":"code","9fe85440":"code","527acc9d":"code","64a4fb4b":"code","2498d3ec":"code","e651182f":"code","69fcdfb5":"code","b60bc045":"code","35895a09":"code","32ff0526":"code","5d88dc7b":"code","206fe8ec":"code","98e33439":"code","7e9b9a4b":"code","12232372":"code","e9cc02fa":"code","d812e3c6":"code","2d92cc64":"markdown","875f4b08":"markdown","ada0729e":"markdown","cfa98683":"markdown","56ca41b8":"markdown","0188a875":"markdown","b557332e":"markdown","e189ce90":"markdown","c8401b72":"markdown","f406f92b":"markdown","d04fd6be":"markdown","db7072e2":"markdown","ca805bc0":"markdown"},"source":{"7e5851f1":"!pip install pycontractions\n!pip install symspellpy","5c91140e":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","f90f7c20":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\nimport xgboost as xgb\nfrom tqdm import tqdm\nimport time\nimport re\nimport string\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.tokenize import word_tokenize\nfrom sklearn.model_selection import train_test_split, GridSearchCV\nfrom sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\nfrom sklearn.metrics import f1_score\n\nfrom nltk.stem import WordNetLemmatizer\nfrom nltk.corpus import stopwords\nfrom nltk.util import ngrams\nfrom nltk.stem import PorterStemmer\n\n\nimport pkg_resources\nfrom symspellpy.symspellpy import SymSpell\nfrom symspellpy import SymSpell, Verbosity\n#Contraction Import\nfrom pycontractions import Contractions","dd8a8a26":"dictionary_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_dictionary_en_82_765.txt\")\n# bigram_path = pkg_resources.resource_filename(\"symspellpy\", \"frequency_bigramdictionary_en_243_342.txt\")\n\nsymspell_segmenter = SymSpell(max_dictionary_edit_distance=1, prefix_length=8)\nsymspell_segmenter.load_dictionary(dictionary_path, term_index=0, count_index=1)\n\n# sym_spell_misspelled = SymSpell(max_dictionary_edit_distance=2, prefix_length=7)\n# sym_spell_misspelled.load_dictionary(dictionary_path, term_index=0, count_index=1)\n# sym_spell_misspelled.load_bigram_dictionary(bigram_path, term_index=0, count_index=2)","ab5d3d49":"cont = Contractions(api_key=\"glove-twitter-100\")\ncont.load_models()","97b8007c":"\"\"\"Let's load the data files\"\"\"\ntrain = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\nsubmission = pd.read_csv(\"\/kaggle\/input\/nlp-getting-started\/sample_submission.csv\")\ntrain.head()","d1029076":"def to_lower(text):\n    text = text.lower()\n    return text\n\n#Version Edit: Susanth D.\ndef remove_url(text):\n    url = re.compile(r'https?:\/\/\\S+|www\\.\\S+|pic.twitter.com\\S+')\n    return url.sub(r' ',text)\n\n#Version Edit: Sonam D.\ndef remove_punct(text):\n    text = re.sub('[%s]' % re.escape(string.punctuation), ' ', text)\n    return text\n\n#Version Edit: Susanth D.\ndef remove_special_ucchar(text):\n    text = re.sub('&.*?;', ' ', text)\n    return text\n\n#Version Edit: Sonam D.\ndef remove_numbers(text):\n    text = re.sub(r'\\d+', ' ', text)\n    return text\n\n#Version Edit: Sonam D.\ndef remove_mentions(text):\n    text = re.sub(r'@\\w*', ' ', text)\n    return text\n\n#Version Edit: Susanth D.\ndef handle_unicode(text):\n    text = text.encode('ascii', 'replace').decode('utf-8')\n    return text\n\n#Version Edit: Sonam D.\ndef remove_punctuations(text):\n    text = re.sub(r'([^A-Za-z \\t])|(\\w+:\\\/\\\/\\S+)', ' ', text)\n    return text\n\n#Version Edit: Susanth D.\ndef remove_square_bracket(text):\n    text = re.sub('\\[.*?\\]', ' ', text)\n    return text\n\n#Version Edit: Susanth D.\ndef remove_angular_bracket(text):\n    text = re.sub('\\<.*?\\>+', ' ', text)\n    return text\n\n#Version Edit: Susanth D.\ndef remove_newline(text):\n    text = re.sub('\\n', ' ', text)\n    return text\n\n#Version Edit: Susanth D.\ndef remove_words_with_numbers(text):\n    text = re.sub('\\w*\\d\\w*', ' ', text)\n    return text\n    \n#Version Edit: Susanth D.\ndef hashtag_to_words(text):\n    hashtag_list = re.findall(r\"#\\w+\",text)\n    for hashtag in hashtag_list:\n        hashtag = re.sub(r'#', '', hashtag)\n        text = re.sub(hashtag, symspell_segmenter.word_segmentation(hashtag).segmented_string, text)\n    text = re.sub(r'#', ' ', text)\n    return text\n\n#Version Edit: Susanth D.\ndef remove_stopwords(text):\n    text_tokens=word_tokenize(text)\n    textop = ''\n    for token in text_tokens:\n        if token not in stopwords.words('english'):\n            textop = textop + token + ' '\n    return textop\n\n#Version Edit: Sonam D.\ndef stemming_text(text):\n    stemmer= PorterStemmer()\n    text_tokens=word_tokenize(text)\n    textop = ''\n    for token in text_tokens:\n        textop = textop + stemmer.stem(token) + ' '\n    return textop\n\n#Version Edit: Sonam D.\ndef lemmatization(text):\n    lemmatizer=WordNetLemmatizer()\n    text_tokens=word_tokenize(text)\n    textop = ''\n    for token in text_tokens:\n        textop = textop + lemmatizer.lemmatize(token) + ' '\n    return textop\n\n#Version Edit: Saurabh M.\ndef removeRepeated(tweet):\n    prev = ''\n    tweet_new = ''\n    for c in tweet:\n        caps = False\n        if c.isdigit():\n            tweet_new += c\n            continue\n        if c.isalpha() == True:\n            if ord(c) >= 65 and ord(c)<=90:\n                caps = True\n            c = c.lower()\n            if c == prev:\n                count += 1\n            else:\n                count = 1\n                prev = c\n            if count >= 3:\n                continue\n            if caps == True:\n                tweet_new += c.upper()\n            else:\n                tweet_new += c\n        else:\n            tweet_new += c\n    return tweet_new\n\ndef Expand_Contractions(text):\n    return list(cont.expand_texts([text]))[0]","fe216649":"def count_chars(text):\n    new_text = text.apply(lambda x : list(x)).explode()\n    return new_text.unique().shape[0]\n\ndef count_words(text):\n    new_text = text.apply(lambda x : x.split(' ')).explode()\n    return new_text.unique().shape[0]\n\ndef preprocess_pipeline(steps, col, df):\n    new_col = df[col]\n    char_count_before = 0\n    word_count_before = 0\n    char_count_after = 0\n    word_count_after = 0\n    for each_step in steps:\n        char_count_before = count_chars(new_col)\n        word_count_before = count_words(new_col)\n        new_col = new_col.apply(each_step)\n        char_count_after = count_chars(new_col)\n        word_count_after = count_words(new_col)\n        print(\"Preprocessing step: \",each_step.__name__)\n        print(\"Unique Char Count ---> Before: %d | After: %d\"%(char_count_before, char_count_after))\n        print(\"Unique Word Count ---> Before: %d | After: %d\"%(word_count_before, word_count_after))\n    \n    return new_col\n        ","9fe85440":"%%time\ntrain = pd.read_csv('..\/input\/nlp-getting-started\/train.csv')\ntest = pd.read_csv('..\/input\/nlp-getting-started\/test.csv')\n\n\npipeline = []\n\npipeline.append(handle_unicode)\n# pipeline.append(to_lower)\npipeline.append(remove_newline)\npipeline.append(remove_url)\npipeline.append(remove_special_ucchar)\npipeline.append(hashtag_to_words)\npipeline.append(remove_mentions)\n# pipeline.append(remove_square_bracket)\n# pipeline.append(remove_angular_bracket)\n# pipeline.append(Expand_Contractions)\npipeline.append(remove_words_with_numbers)\n# pipeline.append(remove_punctuations)\npipeline.append(remove_punct)\n# pipeline.append(remove_numbers)\npipeline.append(removeRepeated)\npipeline.append(remove_stopwords)\n# pipeline.append(stemming_text)\npipeline.append(lemmatization)\npipeline.append(to_lower)\n\nprint(\"For Training data:\")\ntrain['processed_text'] = preprocess_pipeline(pipeline, 'text', train)\nprint(\"\\nFor Testing data:\")\ntest['processed_text'] = preprocess_pipeline(pipeline, 'text', test)\ntrain.head()","527acc9d":"chars = train['processed_text'].apply(lambda x : list(x)).explode()\nchars.unique()","64a4fb4b":"print(train['text'].iloc[233])\nprint(train['processed_text'].iloc[233])","2498d3ec":"u, idx = np.unique(train['processed_text'], return_index=True)\ntrain = train.iloc[idx]","e651182f":"tweet_len = train['processed_text'].apply(len)\nprint(tweet_len.max())","69fcdfb5":"xtrain, xvalid, ytrain, yvalid = train_test_split(train.processed_text, \n                                                  train.target,\n                                                  random_state=42, \n                                                  test_size=0.2, shuffle=True)\nprint(xtrain.shape)\nprint(xvalid.shape)","b60bc045":"full_text = pd.concat([train[\"processed_text\"], test[\"processed_text\"]])","35895a09":"# Appling CountVectorizer()\ncount_vectorizer = CountVectorizer()\ncount_vectorizer.fit(full_text)\nxtrain_vectors = count_vectorizer.transform(xtrain)\nxvalid_vectors = count_vectorizer.transform(xvalid)","32ff0526":"# Appling TFIDF\ntfidf = TfidfVectorizer(min_df=2, max_df=0.5, ngram_range=(1, 2), norm='l2')\ntfidf.fit(full_text)\nxtrain_tfidf = tfidf.transform(xtrain)\nxvalid_tfidf = tfidf.transform(xvalid)","5d88dc7b":"# # Fitting a simple xgboost on TFIDF\n# clf = xgb.XGBClassifier(max_depth=60, subsample=1, learning_rate=0.07, colsample_bytree=0.8, reg_lambda=0.1, reg_alpha=0.1,\\\n#                        gamma=1)\n# clf.fit(xtrain_tfidf.tocsc(), ytrain)\n# predictions = clf.predict(xvalid_tfidf.tocsc())\n\n# print('XGBClassifier on TFIDF')\n# print (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))","206fe8ec":"%%time\n# Fitting a simple xgboost on CountVec\nclf = xgb.XGBClassifier(max_depth=200, subsample=1, learning_rate=0.07, reg_lambda=0.1, reg_alpha=0.1,\\\n                       gamma=1)\nclf.fit(xtrain_vectors, ytrain)\n\npredictions = clf.predict(xvalid_vectors)\nprint('XGBClassifier on CountVectorizer')\nprint (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))\n\npredictions = clf.predict(xtrain_vectors)\nprint('XGBClassifier on CountVectorizer')\nprint (\"Training set f1_score :\", np.round(f1_score(ytrain, predictions),5))","98e33439":"# %%time\n# # Fitting a simple xgboost on CountVec\n# clf = xgb.XGBClassifier(max_depth=200, subsample=0.8, learning_rate=0.07, \n#                         colsample_bytree=0.8, reg_lambda=0.1, reg_alpha=0.1, gamma=1)\n# clf.fit(xtrain_vectors, ytrain)\n\n# predictions = clf.predict(xvalid_vectors)\n# print('XGBClassifier on CountVectorizer')\n# print (\"f1_score :\", np.round(f1_score(yvalid, predictions),5))\n\n# predictions = clf.predict(xtrain_vectors)\n# print('XGBClassifier on CountVectorizer')\n# print (\"Training set f1_score :\", np.round(f1_score(ytrain, predictions),5))","7e9b9a4b":"# %%time\n# #'''For XGBC, the following hyperparameters are usually tunned.'''\n# #'''https:\/\/xgboost.readthedocs.io\/en\/latest\/parameter.html'''\n\n# seed = 500\n# XGB_model = xgb.XGBClassifier(\n#            n_estimators=800,\n#            verbose = 1)\n\n\n# XGB_params = {'max_depth' : (60),\n#               'reg_alpha':  (0.1, 0.09, 0.2),\n#               'reg_lambda': (0.1, 0),\n#               'learning_rate': (0.07, 0.06, 0.08),\n# #               'colsample_bytree': (0.5, 1),\n#               'gamma': (0, 1),\n#              'subsample': [1],\n#              'random_state':[seed]}\n\n# grid_search = GridSearchCV(estimator = XGB_model, param_grid = XGB_params, cv = 3, \n#                              verbose = 3,\n#                              scoring = 'f1', n_jobs = -1)\n# grid_search.fit(xtrain_vectors, ytrain)\n# XGB_best_params, XGB_best_score = grid_search.best_params_, grid_search.best_score_\n# print('XGB best params:{} & best_score:{:0.5f}' .format(XGB_best_params, XGB_best_score))","12232372":"# test_vectors = tfidf.transform(test.processed_text)","e9cc02fa":"test_vectors = count_vectorizer.transform(test.processed_text)","d812e3c6":"# best_model = grid_search.best_estimator_\n# test_pred = best_model.predict(test_vectors)\ntest_pred = clf.predict(test_vectors)\nsubmission['target'] = test_pred.round().astype(int)\nsubmission.to_csv('submission.csv', index=False)","2d92cc64":"## Vectorization","875f4b08":"*Installing necessary Libraries*","ada0729e":"*Defining model for expansion of Contractions*","cfa98683":"***Removing duplicate tweets***","56ca41b8":"## Tweet Preprocessing","0188a875":"## Predictions on test data","b557332e":"## Model Implementation for testing","e189ce90":"*Defining Sym Spell instance for hashtag word separation*","c8401b72":"*Importing necessary libraries*","f406f92b":"## Hyperparameter tuning with Cross-validation","d04fd6be":"***Defining the pipeline of preprocessing tasks to be run***","db7072e2":"# Using XGBoost for classification of Disaster tweets","ca805bc0":"*Checking the unique characters left in the data*"}}