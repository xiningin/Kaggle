{"cell_type":{"069dfe9d":"code","e260ae4c":"code","86a2a48b":"code","ef3ee98e":"code","da85a6e0":"code","04856495":"code","340094e3":"code","9126b757":"code","19676daa":"code","68eb50a4":"code","e572e277":"code","18365692":"code","1f9de739":"code","86ea5c60":"code","850f58dc":"code","98f020d3":"code","ecd0fba9":"code","f91273ff":"code","2668122d":"code","e584c782":"code","0b7be923":"code","e243a4d2":"code","331293be":"code","f4bc0d14":"code","0aac78b1":"code","11b86966":"code","b1b18d46":"code","b6b3ffd7":"code","5493ecf1":"code","7bcbc2df":"code","1354a484":"code","ba33d31a":"code","103c1f2a":"code","0104c958":"code","81d5e62b":"code","fba26142":"code","6191ce27":"code","0c956347":"code","9b613855":"code","ea983225":"code","179161bf":"code","9fa165e8":"code","77e75b3d":"code","9a23a8dd":"code","7cfd4199":"code","0357428b":"code","014f0df5":"code","1e539f5e":"code","29f71fdb":"code","5553dd11":"code","47ea4096":"code","35dabb0f":"code","65fa8291":"markdown","90a632cd":"markdown","18b4e3c0":"markdown","ee126895":"markdown","452322fe":"markdown","7d199788":"markdown","6cb62b51":"markdown","f5a235a1":"markdown","71ed6c47":"markdown","4153cfa7":"markdown","f756db0e":"markdown","9431ccc6":"markdown","d6b07785":"markdown","30370d37":"markdown","46d52193":"markdown","85328652":"markdown","b4c41aef":"markdown"},"source":{"069dfe9d":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","e260ae4c":"# Importing the essential libraries\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","86a2a48b":"# Importing the Training Dataset\n\ndata_train = pd.read_csv(\"\/kaggle\/input\/titanic\/train.csv\")\ndata_train.head()","ef3ee98e":"# Importing the Training Dataset\n\ndata_test = pd.read_csv(\"\/kaggle\/input\/titanic\/test.csv\")\ndata_test.head()","da85a6e0":"data_train.describe()","04856495":"data_train.info()","340094e3":"data_test.info()","9126b757":"# Grouping data acc to Sex and displaying the mean of total people that survived.\n\ndata_train[['Sex','Survived']].groupby('Sex').mean()","19676daa":"# Grouping data acc to Passenger Class and displaying the mean of total people that survived.\n\ndata_train[['Pclass','Survived']].groupby('Pclass').mean()","68eb50a4":"# Grouping data acc to Parent\/Child(ren) and displaying the mean of total people that survived.\n\ndata_train[['Parch','Survived']].groupby('Parch').mean()","e572e277":"# Grouping data acc to Siblings\/Spouse and displaying the mean of total people that survived.\n\ndata_train[['SibSp','Survived']].groupby('SibSp').mean()","18365692":"sns.barplot(x=data_train.Parch, y=data_train.Survived)","1f9de739":"sns.barplot(x=data_train.Pclass, y=data_train.Survived)\n\n# Seems like the Passenger Class is relevat to who survived by a small margin at least.","86ea5c60":"sns.barplot(x=data_train.Sex, y=data_train.Survived)\n\n# This was obvious.","850f58dc":"sns.barplot(x=data_train.Embarked, y=data_train.Survived)\n\n# Not that big a deal, but still let's keep this.","98f020d3":"plt.figure(figsize=(14,7))\nsns.heatmap(data_train.corr(), annot = True, cmap = 'coolwarm')\n\n# Not much to derive from this. No strong correlations seen.\n# Lets do some feature Engineering to get better dataset before we fit the models.","ecd0fba9":"print(data_train.isnull().sum())\nprint('='*50)\nprint(data_test.isnull().sum())","f91273ff":"# Replace the NaN values in the Age column of training data with the median, as it makes sense to not use mean or other such values in case of Age.\ndata_train.Age.fillna(data_train.Age.median(), inplace = True)\n# Replace the NaN values in the Age column of TEST data with the median of the Age colum on the TRAINING data.\ndata_test.Age.fillna(data_train.Age.median(), inplace = True)\n\n# Replace the NaN values of the Fare column of the TEST data with the mean of the column values from the TRAINING data.\ndata_test.Fare.fillna(data_train.Fare.mean(), inplace = True)\n\n# Drop the Embarked rows with NaN values as there are only two of them and it makes sense to do so as they are categorical.\ndata_train.dropna(subset = ['Embarked'], inplace = True)","2668122d":"# Drop the Cabin column entirely as it has a lot of missing values and it MAY not really be important if a person has a cabin or not.\ndata_train.drop(['Cabin'], axis = 1, inplace = True)\ndata_test.drop(['Cabin'], axis = 1, inplace = True)","e584c782":"data_train.head()","0b7be923":"print(data_train.isnull().sum())\nprint('='*50)\nprint(data_test.isnull().sum())\n\n# Better.","e243a4d2":"# Lets make a Size_of_family colum to get a combined result of the number of people a certain person was travelling with.\ndata_train['Size_of_family'] = data_train['SibSp'] + data_train['Parch'] + 1\ndata_train[['Size_of_family','Survived']].groupby('Size_of_family').mean()","331293be":"# Same for the test dataset.\ndata_test['Size_of_family'] = data_test['SibSp'] + data_test['Parch'] + 1","f4bc0d14":"# Splitting the passengers into age groups. \n# Age groups: '<=12': Child. '<=18': Teenager, '<=40': Adult, '<=60': 'Middle ages', '>60': 'Elderly'\ndata_train.loc[data_train['Age'] <= 12, 'Age'] = 0\ndata_train.loc[(data_train['Age'] > 12) & (data_train['Age'] <= 18), 'Age'] = 1\ndata_train.loc[(data_train['Age'] > 18) & (data_train['Age'] <= 40), 'Age'] = 2\ndata_train.loc[(data_train['Age'] > 40) & (data_train['Age'] <= 60), 'Age'] = 3\ndata_train.loc[data_train['Age'] > 60, 'Age'] = 4","0aac78b1":"data_train['Age'] = data_train['Age'].astype(int)","11b86966":"# Again for the test dataset.\ndata_test.loc[data_test['Age'] <= 12, 'Age'] = 0\ndata_test.loc[(data_test['Age'] > 12) & (data_test['Age'] <= 18), 'Age'] = 1\ndata_test.loc[(data_test['Age'] > 18) & (data_test['Age'] <= 40), 'Age'] = 2\ndata_test.loc[(data_test['Age'] > 40) & (data_test['Age'] <= 60), 'Age'] = 3\ndata_test.loc[data_test['Age'] > 60, 'Age'] = 4\ndata_test['Age'] = data_test['Age'].astype(int)","b1b18d46":"data_train.describe()","b6b3ffd7":"# Splitting Fare in groups.\ndata_train.loc[ data_train['Fare'] <= 10, 'Fare'] = 0\ndata_train.loc[(data_train['Fare'] > 10) & (data_train['Fare'] <= 20), 'Fare'] = 1\ndata_train.loc[(data_train['Fare'] > 20) & (data_train['Fare'] <= 40), 'Fare'] = 2\ndata_train.loc[(data_train['Fare'] > 40) & (data_train['Fare'] <= 70), 'Fare'] = 3\ndata_train.loc[(data_train['Fare'] > 70) & (data_train['Fare'] <= 100), 'Fare'] = 4\ndata_train.loc[(data_train['Fare'] > 100) & (data_train['Fare'] <= 200), 'Fare'] = 5\ndata_train.loc[(data_train['Fare'] > 200) & (data_train['Fare'] <= 350), 'Fare'] = 6\ndata_train.loc[ data_train['Fare'] > 350, 'Fare'] = 7\ndata_train['Fare'] = data_train['Fare'].astype(int)","5493ecf1":"data_test.loc[ data_test['Fare'] <= 10, 'Fare'] = 0\ndata_test.loc[(data_test['Fare'] > 10) & (data_test['Fare'] <= 20), 'Fare'] = 1\ndata_test.loc[(data_test['Fare'] > 20) & (data_test['Fare'] <= 40), 'Fare'] = 2\ndata_test.loc[(data_test['Fare'] > 40) & (data_test['Fare'] <= 70), 'Fare'] = 3\ndata_test.loc[(data_test['Fare'] > 70) & (data_test['Fare'] <= 100), 'Fare'] = 4\ndata_test.loc[(data_test['Fare'] > 100) & (data_test['Fare'] <= 200), 'Fare'] = 5\ndata_test.loc[(data_test['Fare'] > 200) & (data_test['Fare'] <= 350), 'Fare'] = 6\ndata_test.loc[ data_test['Fare'] > 350, 'Fare'] = 7\ndata_test['Fare'] = data_test['Fare'].astype(int)","7bcbc2df":"# Converting the Embarked column values to numerical.\ndata_train['Embarked'] = data_train['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)\ndata_test['Embarked'] = data_test['Embarked'].map( {'S': 0, 'C': 1, 'Q': 2} ).astype(int)","1354a484":"# Converting the Sex column values to numerical.\n\ndata_train['Sex'] = data_train['Sex'].map( {'male': 0, 'female': 1} ).astype(int)\ndata_test['Sex'] = data_test['Sex'].map( {'male': 0, 'female': 1} ).astype(int)","ba33d31a":"data_train","103c1f2a":"# Droppiing columns nt deemed important.\n\ndata_train.drop(['PassengerId', 'Name', 'Ticket' ], axis = 1, inplace = True)\ndata_test.drop(['Name', 'Ticket' ], axis = 1, inplace = True)","0104c958":"# Final Training Data\ndata_train","81d5e62b":"# Final Test Data\n\ndata_test","fba26142":"plt.figure(figsize=(14,7))\nsns.heatmap(data_train.corr(), annot = True, cmap = 'coolwarm')","6191ce27":"# Splitting the deendent variables from the rest of the data.\n\nX_train = data_train.drop(['Survived'], axis = 1)\ny_train = data_train['Survived']\nX_test = data_test.drop(['PassengerId'],axis = 1)","0c956347":"# Fitting the model.\n\nfrom sklearn.linear_model import LogisticRegression\nclassifier = LogisticRegression(random_state = 0)\nclassifier.fit(X_train, y_train)","9b613855":"# Predicting the values.\npredict1 = classifier.predict(X_test)\n\n# Getting the accuracy score.\nprint(\"Logistic regression accuracy {:.2F}\".format(classifier.score(X_train, y_train)*100))","ea983225":"# Fitting the model.\n\nfrom sklearn.neighbors import KNeighborsClassifier\nclassifier = KNeighborsClassifier(n_neighbors = 5, metric = 'minkowski', p = 2)\nclassifier.fit(X_train, y_train)","179161bf":"# Predicting the values.\npredict2 = classifier.predict(X_test)\n\n# Getting the accuracy score.\nprint(\"KNN accuracy {:.2F}\".format(classifier.score(X_train, y_train)*100))","9fa165e8":"# Fitting the model.\n\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'linear', random_state = 0)\nclassifier.fit(X_train, y_train)","77e75b3d":"# Predicting the values.\npredict3 = classifier.predict(X_test)\n\n# Getting the accuracy score.\nprint(\"SVC accuracy {:.2F}\".format(classifier.score(X_train, y_train)*100))","9a23a8dd":"# Fitting the model.\n\nfrom sklearn.svm import SVC\nclassifier = SVC(kernel = 'rbf', random_state = 0)\nclassifier.fit(X_train, y_train)","7cfd4199":"# Predicting the values.\npredict4 = classifier.predict(X_test)\n\n# Getting the accuracy score.\nprint(\"Kernel SVC accuracy {:.2F}\".format(classifier.score(X_train, y_train)*100))","0357428b":"# Fitting the model.\n\nfrom sklearn.naive_bayes import GaussianNB\nclassifier = GaussianNB()\nclassifier.fit(X_train, y_train)","014f0df5":"# Predicting the values.\npredict5 = classifier.predict(X_test)\n\n# Getting the accuracy score.\nprint(\"Gaussian Naive Bayes accuracy {:.2F}\".format(classifier.score(X_train, y_train)*100))","1e539f5e":"# Fitting the model.\n\nfrom sklearn.tree import DecisionTreeClassifier\nclassifier = DecisionTreeClassifier(criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)","29f71fdb":"# Predicting the values.\npredict6 = classifier.predict(X_test)\n\n# Getting the accuracy score.\nprint(\"Decision Tree accuracy {:.2F}\".format(classifier.score(X_train, y_train)*100))","5553dd11":"# Fitting the model.\n\nfrom sklearn.ensemble import RandomForestClassifier\nclassifier = RandomForestClassifier(n_estimators = 10, criterion = 'entropy', random_state = 0)\nclassifier.fit(X_train, y_train)","47ea4096":"# Predicting the values.\npedict7 = classifier.predict(X_test)\n\n# Getting the accuracy score.\nprint(\"Random Forest accuracy {:.2F}\".format(classifier.score(X_train, y_train)*100))","35dabb0f":"output = pd.DataFrame({'PassengerId': data_test.PassengerId, 'Survived': predict6})\noutput.to_csv('my_submission.csv', index=False)","65fa8291":"### Naive Bayes","90a632cd":"### Visualization","18b4e3c0":"### Feature Selection","ee126895":"# Overview\n\nProblem Definition: Use machine learning to create a model that predicts which passengers survived the Titanic shipwreck.\n\n## **The Challenge**\nThe sinking of the Titanic is one of the most infamous shipwrecks in history.\nOn April 15, 1912, during her maiden voyage, the widely considered \u201cunsinkable\u201d RMS Titanic sank after colliding with an iceberg. Unfortunately, there weren\u2019t enough lifeboats for everyone onboard, resulting in the death of 1502 out of 2224 passengers and crew.\n\n## **The Solution**\n### **Importing the Libraries and Data**\n### **EDA**\n* Statistical Analysis.\n* Visualization.\n\n### **Taking Care of Missing Data**\n\n### **Feature Engineering**\n* Feature creation.\n* Feature Selection.\n\n### **Classification Models**\n\n* Logitic Regression.\n* K Nearest Neighbour.\n* SVC.\n* Kernel SVC.\n* Naive Bayes.\n* Decision Tree.\n* Random Forest.","452322fe":"### Random Forest Classifier","7d199788":"## Classification Models","6cb62b51":"### Feature Creation.","f5a235a1":"## Taking Care of Missing Data","71ed6c47":"## Feature Engineering","4153cfa7":"### KNN Classifier","f756db0e":"### Decision Tree Classifier","9431ccc6":"### Support Vector Classifier","d6b07785":"## Submission.","30370d37":"### Kernel SVC","46d52193":"### Logistic Regression","85328652":"### Statistical Analysis","b4c41aef":"## EDA"}}