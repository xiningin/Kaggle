{"cell_type":{"041b48df":"code","b8a46123":"code","87d8a4b0":"code","92b4041f":"code","a61921a5":"code","02090d5e":"code","7cf291d2":"code","b7fe319b":"code","cdf41a94":"code","fdcbdeb6":"code","5a9a1f4f":"code","c794b3f4":"code","948381f9":"code","d6e0efbb":"code","d1bf96a3":"code","329f44e3":"code","4f6fa335":"code","aa419f14":"code","131b9d29":"code","39e9f568":"code","487a127a":"code","0cecbb5a":"code","897996e1":"code","67e99153":"code","4427e6bb":"code","df2f281d":"code","97aa30df":"code","eb4fd4db":"code","909fee9f":"code","d8181893":"code","081e88ea":"code","5adc1dec":"code","6bfe78ae":"code","8e2935e4":"code","dc7f1b03":"code","e144f1fe":"code","eefb464f":"code","e574c85e":"code","00a1d349":"code","d2b0b748":"markdown","ef254088":"markdown","0f35d496":"markdown","67d90812":"markdown","b848e174":"markdown","88f028d5":"markdown","8216e454":"markdown","cd08058c":"markdown","8decad7c":"markdown","ef8f9848":"markdown","4d1bef65":"markdown","8434dff1":"markdown","b9c749dd":"markdown","2b1a656d":"markdown","a797a5cd":"markdown","30459342":"markdown","7fa0acf2":"markdown","3875c636":"markdown","e647edc9":"markdown","5b940741":"markdown","44618fea":"markdown","9bb614c9":"markdown","712a60d7":"markdown","af711903":"markdown","56158420":"markdown"},"source":{"041b48df":"import cv2\nfrom matplotlib import pyplot as plt\nimport numpy as np\n\n\ndef plot_images(imgs):\n    plt.figure(figsize=(5, 5))\n    for i in range(len(imgs)): \n        plt.figure(i+1)\n        if len(imgs[i].shape)<3:\n            plt.imshow(imgs[i], cmap=\"gray\")\n        else:\n            plt.imshow(imgs[i])\n","b8a46123":"# Image Path\npath = \"..\/input\/workshop-2022-day3\/cbrl_workshop_day3\/new_pepsi (297).jpg.jpg\"","87d8a4b0":"#-> Read Image\nimg = cv2.imread(path) ## read image in rgb format\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) ## Convert from BGR to RGB\nprint(\"RGB Image Shape: {}\".format(img.shape))\n\nplot_images([img])\n","92b4041f":"### write image\nimg = np.zeros((255,255))\ncv2.imwrite(\"\/kaggle\/working\/1.png\", img)","a61921a5":"# -> GrayScale, Cropped and Resize Image\nimg_gray = cv2.imread(path,0) \nprint(\"Gray Image Shape: {}\".format(img_gray.shape))\n\nplot_images([img_gray])","02090d5e":"img = cv2.imread(path) ## read image in rgb format\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) ## Convert from BGR to RGB\n\nimg_cropped = img[80:130, 85:140, :]\nprint(\"Cropped Image Shape: {}\".format(img_cropped.shape))\n\nplot_images([img_cropped])\n","7cf291d2":"img = cv2.imread(path) ## read image in rgb format\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) ## Convert from BGR to RGB\nimg_cropped = img[80:130, 85:140, :]\n\nresized_img = cv2.resize(img_cropped, (512,512))\nprint(\"Resized Image Shape: {}\".format(resized_img.shape))\n\nplot_images([resized_img])","b7fe319b":"# -> Image Thresholding\nimg_gray = cv2.imread(path, 0)\nret,thresh1 = cv2.threshold(img_gray,127,255,cv2.THRESH_BINARY)\n\nprint(\"Gray Image Shape: {}\".format(img_gray.shape))\n\nplot_images([img_gray, thresh1])","cdf41a94":"# -> Image Thresholding BINARY INVERSE\nimg_gray = cv2.imread(path,0)\nret,thresh1 = cv2.threshold(img_gray,127,255,cv2.THRESH_BINARY_INV)\n\nprint(\"Gray Image Shape: {}\".format(img_gray.shape))\n\nplot_images([img_gray, thresh1])","fdcbdeb6":"# -> Image Thresholding Color\nimg = cv2.imread(path)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) ## Convert from BGR to RGB\nret,thresh1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY)\n\nprint(\"Gray Image Shape: {}\".format(img_gray.shape))\n\nplot_images([img, thresh1])","5a9a1f4f":"# -> Image Thresholding Binary InverseColor\nimg = cv2.imread(path)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) ## Convert from BGR to RGB\nret,thresh1 = cv2.threshold(img,127,255,cv2.THRESH_BINARY_INV)\n\nprint(\"Gray Image Shape: {}\".format(img_gray.shape))\n\nplot_images([img, thresh1])","c794b3f4":"### Image Blurring (Background Effect)\nimg = cv2.imread(path)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) ## Convert from BGR to RGB\nimg = cv2.blur(img,(5,5))\n\nprint(\"Gray Image Shape: {}\".format(img_gray.shape))\n\nplot_images([img])","948381f9":"#### Image Rotation and Scaling\nimg = cv2.imread(path)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) ## Convert from BGR to RGB\n\nheight, width = img.shape[:2]\ncenter = (width\/2, height\/2)\nrotate_matrix = cv2.getRotationMatrix2D(center=center, angle=270, scale=1)\nrotated_image = cv2.warpAffine(src=img, M=rotate_matrix, dsize=(width, height))\n\nprint(\"Gray Image Shape: {}\".format(img_gray.shape))\n\nplot_images([rotated_image])","d6e0efbb":"img = np.zeros((255,255))\n\nplot_images([img])","d1bf96a3":"## Draw Rectangle \nimg = np.zeros((255,255, 3))\nx_start = 50\nx_end = 100\ny_start = 100\ny_end = 150\n\ncolor = (100, 100, 0)\n\ncv2.rectangle(img, (x_start, y_start), (x_end, y_end), color, 5)\nplot_images([img])","329f44e3":"#Draw Filled Rectangle\nimg = np.zeros((255,255,3))\nx_start = 50\nx_end = 100\ny_start = 100\ny_end = 150\n\ncolor = (100, 100, 0)\n\ncv2.rectangle(img, (x_start, y_start), (x_end, y_end), color, -1)\nplot_images([img])","4f6fa335":"#Put Text\nimg = np.zeros((255,255, 3))\nTEXT= \"CBRL Workshop\"\ncolor = (0, 255, 255)\nfont = cv2.FONT_HERSHEY_SIMPLEX\nfont_size = 0.9\nthickness= 2\n\ncv2.putText(img,TEXT,(20,130), font,font_size,color,thickness,cv2.LINE_AA)\n\nplot_images([img])","aa419f14":"img =cv2.imread(path)\nimgGray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) ## Convert from BGR to RGB\n\nimg_cropped = img[80:130, 85:140, :]\ntemplateGray = cv2.cvtColor(img_cropped, cv2.COLOR_BGR2GRAY)\n\nplot_images([img, imgGray, templateGray])","131b9d29":"result = cv2.matchTemplate(imgGray, templateGray,cv2.TM_CCORR_NORMED)\n(minVal, maxVal, minLoc, maxLoc) = cv2.minMaxLoc(result)\n\nprint((minVal, maxVal, minLoc, maxLoc))\n\n(startX, startY) = maxLoc\nendX = startX + img_cropped.shape[1]\nendY = startY + img_cropped.shape[0]\ncv2.rectangle(img, (startX, startY), (endX, endY), (255, 0, 0), 3)\n\nplt.imshow(img)","39e9f568":"\nimport cv2\nfrom matplotlib import pyplot as plt\n\nimg = cv2.imread('..\/input\/workshop-2022-day3\/cbrl_workshop_day3\/people-14.jpg')\nimg = cv2.cvtColor(img, cv2.COLOR_BGR2RGB) ## Convert from BGR to RGB\n\nimg = cv2.resize(img, (1080,720))\n    \nplt.imshow(img)","487a127a":"face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\ngray = cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\nfaces = face_cascade.detectMultiScale(gray, 1.1, 4)\n                                     \nprint(faces.shape)\n\nfor (x, y, w, h) in faces:\n    cv2.rectangle(img, (x, y), (x+w, y+h), (255, 255, 255), 7)\n\nplt.imshow(img)","0cecbb5a":"from __future__ import print_function\nfrom tensorflow import keras\nfrom keras.datasets import cifar10\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Dropout, Activation, Flatten\nfrom keras.layers import Conv2D, MaxPooling2D\nimport os\nimport cv2\n\nimport numpy as np\n\nimport seaborn as sns\nimport matplotlib\nimport matplotlib.pyplot as plt\n\nfrom sklearn.metrics import confusion_matrix, classification_report\nimport itertools\n\n%matplotlib inline\n","897996e1":"import tensorflow as tf\ntf.test.is_gpu_available() # True\/False","67e99153":"batch_size = 32  # The default batch size of keras.\nnum_classes = 10  # Number of class for the dataset\nepochs = 10\ndata_augmentation = False","4427e6bb":"# The data, split between train and test sets:\n(x_train, y_train), (x_test, y_test) = cifar10.load_data()\nprint('x_train shape:', x_train.shape)\nprint('y_train shape:', y_train.shape)\nprint(x_train.shape[0], 'train samples')\nprint(x_test.shape[0], 'test samples')\n\nimg = x_test[1]\nprint(\"Class: \", y_test[1] + 1)\n\n# img = cv2.resize(img, (512,512))\n\nplt.figure(figsize=(10, 10))\nplt.imshow(img)\n\n","df2f281d":"fig, axs = plt.subplots(1,2,figsize=(15,5)) \n# Count plot for training set\nsns.countplot(y_train.ravel(), ax=axs[0])\naxs[0].set_title('Distribution of training data')\naxs[0].set_xlabel('Classes')\n# Count plot for testing set\nsns.countplot(y_test.ravel(), ax=axs[1])\naxs[1].set_title('Distribution of Testing data')\naxs[1].set_xlabel('Classes')\nplt.show()","97aa30df":"# Normalize the data. Before we need to connvert data type to float for computation.\nx_train = x_train.astype('float32')\nx_test = x_test.astype('float32')\n\nx_train \/= 255\nx_test \/= 255\n\n\n# Convert class vectors to binary class matrices. This is called one hot encoding.\ny_train = keras.utils.to_categorical(y_train, num_classes)\ny_test = keras.utils.to_categorical(y_test, num_classes)\n\n# print(y_test[0].shape)","eb4fd4db":"#define the convnet\nmodel = Sequential()\n# CONV => RELU => CONV => RELU => POOL => DROPOUT\nmodel.add(Conv2D(32, (3, 3), padding='same',input_shape=x_train.shape[1:]))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(32, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# CONV => RELU => CONV => RELU => POOL => DROPOUT\nmodel.add(Conv2D(64, (3, 3), padding='same'))\nmodel.add(Activation('relu'))\nmodel.add(Conv2D(64, (3, 3)))\nmodel.add(Activation('relu'))\nmodel.add(MaxPooling2D(pool_size=(2, 2)))\nmodel.add(Dropout(0.25))\n\n# FLATTERN => DENSE => RELU => DROPOUT\nmodel.add(Flatten())\nmodel.add(Dense(512))\nmodel.add(Activation('relu'))\n# a softmax classifier\nmodel.add(Dense(num_classes))\nmodel.add(Activation('softmax'))\n\nmodel.summary()","909fee9f":"# initiate RMSprop optimizer\nopt = keras.optimizers.RMSprop(learning_rate=0.0001, decay=1e-6)\n\n# Let's train the model using RMSprop\nmodel.compile(loss='categorical_crossentropy',\n              optimizer=opt,\n              metrics=['accuracy'])","d8181893":"history = None  # For recording the history of trainning process.\nif not data_augmentation:\n    print('Not using data augmentation.')\n    history = model.fit(x_train, y_train,\n              batch_size=batch_size,\n              epochs=epochs,\n              validation_data=(x_test, y_test),\n              shuffle=True)\nelse:\n    print('Using real-time data augmentation.')\n    # This will do preprocessing and realtime data augmentation:\n    datagen = ImageDataGenerator(\n        featurewise_center=False,  # set input mean to 0 over the dataset\n        samplewise_center=False,  # set each sample mean to 0\n        featurewise_std_normalization=False,  # divide inputs by std of the dataset\n        samplewise_std_normalization=False,  # divide each input by its std\n        zca_whitening=False,  # apply ZCA whitening\n        zca_epsilon=1e-06,  # epsilon for ZCA whitening\n        rotation_range=0,  # randomly rotate images in the range (degrees, 0 to 180)\n        # randomly shift images horizontally (fraction of total width)\n        width_shift_range=0.1,\n        # randomly shift images vertically (fraction of total height)\n        height_shift_range=0.1,\n        shear_range=0.,  # set range for random shear\n        zoom_range=0.,  # set range for random zoom\n        channel_shift_range=0.,  # set range for random channel shifts\n        # set mode for filling points outside the input boundaries\n        fill_mode='nearest',\n        cval=0.,  # value used for fill_mode = \"constant\"\n        horizontal_flip=True,  # randomly flip images\n        vertical_flip=False,  # randomly flip images\n        # set rescaling factor (applied before any other transformation)\n        rescale=None,\n        # set function that will be applied on each input\n        preprocessing_function=None,\n        # image data format, either \"channels_first\" or \"channels_last\"\n        data_format=None,\n        # fraction of images reserved for validation (strictly between 0 and 1)\n        validation_split=0.0)\n\n    # Compute quantities required for feature-wise normalization\n    # (std, mean, and principal components if ZCA whitening is applied).\n    datagen.fit(x_train)\n\n    # Fit the model on the batches generated by datagen.flow().\n    history = model.fit_generator(datagen.flow(x_train, y_train,\n                                    batch_size=batch_size),\n                                    epochs=epochs,\n                                    validation_data=(x_test, y_test),\n                                    workers=4)","081e88ea":"def plotmodelhistory(history): \n    fig, axs = plt.subplots(1,2,figsize=(15,5)) \n    # summarize history for accuracy\n    axs[0].plot(history.history['accuracy']) \n    axs[0].plot(history.history['val_accuracy']) \n    axs[0].set_title('Model Accuracy')\n    axs[0].set_ylabel('Accuracy') \n    axs[0].set_xlabel('Epoch')\n    axs[0].legend(['train', 'validate'], loc='upper left')\n    # summarize history for loss\n    axs[1].plot(history.history['loss']) \n    axs[1].plot(history.history['val_loss']) \n    axs[1].set_title('Model Loss')\n    axs[1].set_ylabel('Loss') \n    axs[1].set_xlabel('Epoch')\n    axs[1].legend(['train', 'validate'], loc='upper left')\n    plt.show()\n\n# list all data in history\nprint(history.history.keys())\n\nplotmodelhistory(history)\n","5adc1dec":"# Score trained model.\nscores = model.evaluate(x_test, y_test, verbose=1)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])\n\n# make prediction.\npred = model.predict(x_test)","6bfe78ae":"def heatmap(data, row_labels, col_labels, ax=None, cbar_kw={}, cbarlabel=\"\", **kwargs):\n    \"\"\"\n    Create a heatmap from a numpy array and two lists of labels.\n    \"\"\"\n    if not ax:\n        ax = plt.gca()\n\n    # Plot the heatmap\n    im = ax.imshow(data, **kwargs)\n\n    # Create colorbar\n    cbar = ax.figure.colorbar(im, ax=ax, **cbar_kw)\n    cbar.ax.set_ylabel(cbarlabel, rotation=-90, va=\"bottom\")\n\n    # Let the horizontal axes labeling appear on top.\n    ax.tick_params(top=True, bottom=False,\n                   labeltop=True, labelbottom=False)\n    # We want to show all ticks...\n    ax.set_xticks(np.arange(data.shape[1]))\n    ax.set_yticks(np.arange(data.shape[0]))\n    # ... and label them with the respective list entries.\n    ax.set_xticklabels(col_labels)\n    ax.set_yticklabels(row_labels)\n    \n    ax.set_xlabel('Predicted Label') \n    ax.set_ylabel('True Label')\n    \n    return im, cbar\n\ndef annotate_heatmap(im, data=None, fmt=\"d\", threshold=None):\n    \"\"\"\n    A function to annotate a heatmap.\n    \"\"\"\n    # Change the text's color depending on the data.\n    texts = []\n    for i in range(data.shape[0]):\n        for j in range(data.shape[1]):\n            text = im.axes.text(j, i, format(data[i, j], fmt), horizontalalignment=\"center\",\n                                 color=\"white\" if data[i, j] > thresh else \"black\")\n            texts.append(text)\n\n    return texts","8e2935e4":"labels = ['Airplane', 'Automobile', 'Bird', 'Cat', 'Deer', 'Dog', 'Frog', 'Horse', 'Ship', 'Truck']\n\n# Convert predictions classes to one hot vectors \nY_pred_classes = np.argmax(pred, axis=1) \n# Convert validation observations to one hot vectors\nY_true = np.argmax(y_test, axis=1)\n# Errors are difference between predicted labels and true labels\nerrors = (Y_pred_classes - Y_true != 0)\n\nY_pred_classes_errors = Y_pred_classes[errors]\nY_pred_errors = pred[errors]\nY_true_errors = Y_true[errors]\nX_test_errors = x_test[errors]\n\ncm = confusion_matrix(Y_true, Y_pred_classes) \nthresh = cm.max() \/ 2.\n\nfig, ax = plt.subplots(figsize=(12,12))\nim, cbar = heatmap(cm, labels, labels, ax=ax,\n                   cmap=plt.cm.Blues, cbarlabel=\"count of predictions\")\ntexts = annotate_heatmap(im, data=cm, threshold=thresh)\n\nfig.tight_layout()\nplt.show()","dc7f1b03":"print(classification_report(Y_true, Y_pred_classes))","e144f1fe":"R = 5\nC = 5\nfig, axes = plt.subplots(R, C, figsize=(12,12))\naxes = axes.ravel()\n\nfor i in np.arange(0, R*C):\n    axes[i].imshow(x_test[i])\n    axes[i].set_title(\"True: %s \\nPredict: %s\" % (labels[Y_true[i]], labels[Y_pred_classes[i]]))\n    axes[i].axis('off')\n    plt.subplots_adjust(wspace=1)\n","eefb464f":"R = 3\nC = 5\nfig, axes = plt.subplots(R, C, figsize=(12,8))\naxes = axes.ravel()\n\nmisclassified_idx = np.where(Y_pred_classes != Y_true)[0]\nfor i in np.arange(0, R*C):\n    axes[i].imshow(x_test[misclassified_idx[i]])\n    axes[i].set_title(\"True: %s \\nPredicted: %s\" % (labels[Y_true[misclassified_idx[i]]], \n                                                  labels[Y_pred_classes[misclassified_idx[i]]]))\n    axes[i].axis('off')\n    plt.subplots_adjust(wspace=1)\n","e574c85e":"def display_errors(errors_index, img_errors, pred_errors, obs_errors):\n    \"\"\" This function shows 10 images with their predicted and real labels\"\"\"\n    n = 0\n    nrows = 2\n    ncols = 5\n    fig, ax = plt.subplots(nrows,ncols,sharex=True,sharey=True, figsize=(12,6))\n    for row in range(nrows):\n        for col in range(ncols):\n            error = errors_index[n]\n            ax[row,col].imshow((img_errors[error]).reshape((32,32,3)))\n            ax[row,col].set_title(\"Predicted:{}\\nTrue:{}\".\n                                  format(labels[pred_errors[error]],labels[obs_errors[error]]))\n            n += 1\n            ax[row,col].axis('off')\n            plt.subplots_adjust(wspace=1)\n\n# Probabilities of the wrong predicted numbers\nY_pred_errors_prob = np.max(Y_pred_errors,axis = 1)\n\n# Predicted probabilities of the true values in the error set\ntrue_prob_errors = np.diagonal(np.take(Y_pred_errors, Y_true_errors, axis=1))\n\n# Difference between the probability of the predicted label and the true label\ndelta_pred_true_errors = Y_pred_errors_prob - true_prob_errors\n\n# Sorted list of the delta prob errors\nsorted_dela_errors = np.argsort(delta_pred_true_errors)\n\n# Top 10 errors \nmost_important_errors = sorted_dela_errors[-10:]\n\n# Show the top 10 errors\ndisplay_errors(most_important_errors, X_test_errors, Y_pred_classes_errors, Y_true_errors)","00a1d349":"save_dir = os.path.join(os.getcwd(), 'saved_models')\nmodel_name = 'keras_cifar10_trained_model.h5'\n\n# Save model and weights\nif not os.path.isdir(save_dir):\n    os.makedirs(save_dir)\n    \nmodel_path = os.path.join(save_dir, model_name)\nmodel.save(model_path)\nprint('Saved trained model at %s ' % model_path)\n\n# Score trained model.\nscores = model.evaluate(x_test, y_test, verbose=1)\nprint('Test loss:', scores[0])\nprint('Test accuracy:', scores[1])\n","d2b0b748":"## 3. Defining the model architecture Using ConVnets\n\nNow Let us define a suitable deep net.\n\n* In the first stage, Our net will learn **32 convolutional filters**, each of which with a **3 x 3 size**. The output dimension is the same one of the input shape, so it will be **32 x 32** and activation is `relu`, which is a simple way of introducing non-linearity; folowed by another **32 convolutional filters**, each of which with a **3 x 3 size** and activation is also `relu`. After that we have a **max-pooling** operation with `pool size` **2 x 2** and a `dropout` at **25%.**\n* In the next stage in the deep pipeline, Our net will learn **64 convolutional filters**, each of which with a **3 x 3 size**. The output dimension is the same one of the input shape and activation is `relu`; folowed by another **64 convolutional filters**, each of which with a **3 x 3 size** and activation is also `relu`. After that we have a **max-pooling** operation with `pool size` **2 x 2** and a `dropout` at **25%.**\n* And the Final stage in the deep pipeline is a dense network with **512 units** and `relu` activation followed by a `dropout` at **50%** and by a `softmax` layer with **10 classes as output**, one for each category.\n\nNow let us look at the code review for our architecture.","ef254088":"\n# In this notebook, we will use Computer Vision & Deep Learning techniques to solve image related problems.","0f35d496":"### 2.2 Import and preproces of data \nWe load the data and split it between train and test sets\n","67d90812":"## 5. Evaluate the model.\n\n### 5.1 Training and validation curves.\nLet's see the training and validation process by the visualization of history of fitting. This allow us to quickly know if how our model fit our data **(overfitting, underfitting, model convergence, etc...)**","b848e174":"### 5.5 Check the predictions.","88f028d5":"As we can see, after 60 epochs, the accuracy of our model doesn't really increase. But our model doesn't overffit.\n### 5.2 Score trained model and prediction.","8216e454":"1. Train Model on MNIST Dataset without augmentation\n2. Train Model with Augmentation:  Rotation, Horizontal Flip\n3. Train Model for 10 and 15 epochs\n4. Change batch size and train model\n5. Train Model with Dropout layers\n6. Train model by removing all Dropout layers\n7. Compare all results on test set","cd08058c":"## 6. Save model and weights\n\nNote that we need to firstly indicate the directory to save the model and the name of our model. ","8decad7c":"* [OpenCV Image Thresholding](https:\/\/docs.opencv.org\/4.x\/d7\/d4d\/tutorial_py_thresholding.html)\n* [OpenCV Drawing Functions](https:\/\/docs.opencv.org\/4.x\/dc\/da5\/tutorial_py_drawing_functions.html)\n* [OpenCV Image Smoothing](https:\/\/docs.opencv.org\/4.x\/d4\/d13\/tutorial_py_filtering.html)\n","ef8f9848":"## Task ","4d1bef65":"### Draw Shapes","8434dff1":"Now Let's investigate for errors.\n### 5.3 Confusion matrix.\nConfusion matrix can be very helpfull to see your model drawbacks.\nWe plot the confusion matrix of the validation results.\nFor good vizualization of our confusion matrix, we have to define to fonction.","b9c749dd":"## 2. Import and Preprocess the data\n\n### 2.1 Import all required libraries","2b1a656d":"# OPENCV","a797a5cd":"And now, let us train the model.\n\n## 4. Model training\n\nBefore making network ready for training we have to make sure to add below things:\n*   **A loss function:** to measure how good the network is\n*   **An optimizer:** to update network as it sees more data and reduce loss value\n*   **Metrics:** to monitor performance of network\n\n**Also note that for data augmentation:**\n* One of the most commun tehnique to avoid overfitting is data augmentation. And We know that overfitting is generaly occur when we don't have enought data for training the model. To avoid this overfitting problem, we need to expand artificially our dataset. The idea is to alter the training data with small transformations to reproduce the variations occuring when someone is writing a digit. \n\n* Different data aumentation techniques are as follows: Cropping, Rotating, Scaling, Translating, Flipping, Adding Gaussian noise to input images, etc...\n","30459342":"#### Face Detection","7fa0acf2":"#### - Check the wrong predictions.","3875c636":"As we can see, each classe contain exacly 6000 examples( 5000 for training and 1000 for test).\n\nThe graph above is very important for the training, for example if we had just 1000 samples of label 1 that will be a problem , the model will find difficulties to detect label 1\"less accuracy \", so that's not going to happend everything look fine. It's important to know the distribution of dataset behind different classes because the goodness of our model depend on it.\n\nNow let's doing some preprocessing.\n\nThe output variable have 10 posible values. This is a multiclass classification problem. We need to encode these lables to one hot vectors (ex : \"bird\" -> [0,0,1,0,0,0,0,0,0,0]). ","e647edc9":"###### 1.Read Image using Opencv\n###### 2.Read Image in grayscale\n###### 3.Crop Image\n###### 4.Resize Image\n###### 5.Image Thresholding\n###### 6. Image Blurring\n###### 7. Rotation and Scaling\n###### 8. Drawing Shapes and Text\n###### 9. Template Matching\n###### 10.Face Detection","5b940741":"### 2.3 Distribution of data.","44618fea":"## 1. Introduction.\nThe CIFAR-10 dataset contains 60,000 color images of 32 x 32 pixels in 3 channels divided into 10\nclasses. Each class contains 6,000 images. The training set contains 50,000 images, while the test sets\nprovides 10,000 images. This image taken from the CIFAR repository ( <a href = \"https:\/\/www.cs.toronto.edu\/~kriz\/cifar.html\">https:\/\/www.cs.toronto.edu\/~kriz\/cifar.html <\/a>). This is a classification problem with 10 classes(muti-label classification). We can take a view on this image for more comprehension of the dataset. \n\n![cifar10.png](attachment:cifar10.png)\n\n\nThe challenge is to recognize previously unseen images and assign them to one of the 10 classes.\n\nOk Let's get started.","9bb614c9":"### Template Matching","712a60d7":"Let's set the models hyperparameters and others global parameters.","af711903":"#### - Check the most important errors.","56158420":"### 5.4 Classification report\n\nThis will allow us to evaluate the model with other metrics **(Precision, Recall, F1 score, etc...)**"}}