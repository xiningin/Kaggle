{"cell_type":{"ecbe5398":"code","9b86abf7":"code","a51c9c62":"code","d9dce9f2":"code","28e0363e":"code","2c76d0e4":"code","ccf45aa5":"code","e695a0a1":"code","77721bf0":"code","69a78804":"code","d8d1be24":"code","b9f28c63":"code","13b27468":"code","84d234e0":"code","a6ba3401":"code","c1c4f72b":"code","fd79e0fa":"code","83a5cb63":"code","0aa646ab":"code","65abb4b6":"code","819880fd":"code","baed31fe":"code","75859905":"code","35b26cad":"code","a732d646":"code","a48a4e6f":"code","b0fe1363":"code","6573ce8a":"code","b4fe4e2f":"code","64c414f9":"code","86d5f0e7":"code","6136f2f6":"code","7e27d2f2":"code","b8b04d5e":"code","2ac36b82":"code","8f937d32":"code","fdcf85fe":"code","777f029d":"code","620f7095":"code","10768ac3":"code","7fd952f1":"code","11dcb77f":"code","881e71a3":"code","dc5cd001":"code","17a0dbb0":"code","6056ee27":"code","bcc08939":"code","84b23b11":"code","0d6698f6":"code","f072d860":"code","d104b63d":"code","d5a64992":"code","09d55177":"code","8c06746c":"code","ec606734":"markdown","afabe8e0":"markdown","b798862d":"markdown","e853c2e8":"markdown","8a7b225f":"markdown","387496f4":"markdown","6d20070a":"markdown","44605b6b":"markdown","619ee8df":"markdown","40998618":"markdown","64a18f8d":"markdown","f95f74e2":"markdown","75918441":"markdown","772d4e7a":"markdown","1debb60b":"markdown","0bef2b81":"markdown","740a8ca1":"markdown","a92c63f3":"markdown","66650f36":"markdown","26f1dc3a":"markdown","567f3b1b":"markdown","22a86ab3":"markdown","6b5861b4":"markdown","86b0442d":"markdown","f3fa0968":"markdown","dc2ea389":"markdown","fb164fb9":"markdown","93766c53":"markdown","3ce3b9c3":"markdown","5642dbb1":"markdown","1c84cd90":"markdown","f30c922f":"markdown","8136df0b":"markdown","06d0aa21":"markdown","2786756e":"markdown","af7db636":"markdown","833f778d":"markdown","3c3c2175":"markdown","ccddb3d1":"markdown","d59b6665":"markdown","afdbf7b6":"markdown","ac667043":"markdown","17696a48":"markdown","152e36f4":"markdown","efe7e419":"markdown","e9d1ca7a":"markdown","9cc0fa3c":"markdown","40a8b959":"markdown","4f8ed508":"markdown","6539ca59":"markdown"},"source":{"ecbe5398":"# for displaying plots in a notebook\n%matplotlib inline \n\nimport numpy as np # simply array and mathematical operations\nimport pandas as pd # R-like handling with dataframes, etc.\nimport seaborn as sb # for making your statistical plots look pretty\nimport matplotlib.pyplot as plt","9b86abf7":"data_original   = pd.read_csv('..\/input\/bank.csv', sep = \",\")\ndata = data_original.copy()\n\nlabels = data[\"deposit\"].copy()\ndata   = data.drop(\"deposit\",axis=1)\n\ndata.head()","a51c9c62":"from sklearn.model_selection import train_test_split\n\ntrain, test, train_labels, test_labels = train_test_split(data, labels, test_size=0.33, random_state=1337)","d9dce9f2":"train.info()","28e0363e":"train.head()","2c76d0e4":"train_labels.head()","ccf45aa5":"def dropMissingData(data, labels, missing_val, missing_val_cols, labels_colname = \"y\"):\n    \n    data_temp = data.join(labels)\n    #print data_temp.head()\n    \n    for missing_val_col in missing_val_cols:\n        \n        data_temp = data_temp[ data[missing_val_col]!=missing_val  ]\n\n    labels_temp = data_temp[labels_colname]\n    data_temp   = data_temp.drop(labels_colname, axis=1)\n    \n    return data_temp, labels_temp\n\ntrain, train_labels = dropMissingData(train,\n                                      train_labels, \n                                      missing_val=\"unknown\", \n                                      missing_val_cols = [\"education\",\"marital\",\"default\",\"job\"],\n                                      labels_colname = \"deposit\")\ntrain_labels.head()","e695a0a1":"len(train.default[train.default==\"unknown\"])","77721bf0":"train = train.drop(\"duration\", axis=1)","69a78804":"train.dtypes.values","d8d1be24":"quant_cols  = train.columns.values[(train.dtypes =='int64') | (train.dtypes =='float64')]\nquant_cols","b9f28c63":"n_yes = len(train_labels[train_labels==\"yes\"])\nn_no  = len(train_labels[train_labels==\"no\"])\nyes_no_ratio =  (n_yes\/(n_yes+n_no))*100\nprint(\"Number of 'Yes' responses: {}\".format(n_yes))\nprint(\"Number of 'No' responses: {}\".format(n_no))","13b27468":"from sklearn.feature_selection import VarianceThreshold\nfrom sklearn.preprocessing import StandardScaler\nsel = VarianceThreshold(threshold=0.2)\nmmscale = StandardScaler(with_std=False)\ntrain_scaled = pd.DataFrame(\n    mmscale.fit_transform( train[quant_cols] ), \n    columns= train[quant_cols].columns)\n\nsel.fit_transform(train_scaled)\n \nprint(sel.variances_)\n\nprint(sel.get_support())\n\ntrain_scaled.head()","84d234e0":"train = train.drop(['pdays','poutcome','previous','campaign'], axis=1)\nquant_cols  = train.columns.values[(train.dtypes =='int64') | (train.dtypes =='float64')]\nnom_cols = train.columns.values[train.dtypes == 'O']","a6ba3401":"train_corrmat = train[quant_cols].corr(method=\"spearman\")","c1c4f72b":"sb.heatmap(train_corrmat, annot=True,cbar=False, square=True)","fd79e0fa":"# Age distributions of yesses and nos:\nsb.violinplot(x=train_labels, y=train.age)\nprint(len(train_labels[(train_labels==\"yes\") & (train.age > 60)])\/len(train_labels[train_labels==\"yes\"]))\n       \nprint(len(train_labels[(train_labels==\"no\") & (train.age > 60)])\/len(train_labels[train_labels==\"no\"]))","83a5cb63":"sb.distplot(train.age, kde=False)","0aa646ab":"# Education-age demographics\ng = sb.violinplot(x=train.education, y=train.age, hue=train_labels, split=True)\ntick_labels = g.set_xticklabels(g.get_xticklabels(),rotation=30)","65abb4b6":"# Job-age demographic\ng = sb.violinplot(x=train.job, y=train.age, hue=train_labels, split=True)\ntick_labels = g.set_xticklabels(g.get_xticklabels(),rotation=30)","819880fd":"def relabelData(input_labels, input_frame, column):\n    \n    for i in range(0,len(input_frame[column])):\n        for j in range(0,len(input_labels)):\n            if input_frame[column].values[i] == input_labels[j]:\n                input_frame[column].values[i] = j\n                \n    return input_frame[column]\n                \n","baed31fe":"#%%timeit\nmonths = ['jan','feb','mar','apr','may','jun','jul','aug','sep','oct','nov','dec']  \ntrain.month = relabelData(months, train, \"month\")\n\n# Add one so that the month numbers correspond to conventional dates - Jan = 1, etc.\ntrain.month+=1","75859905":"train.head()","35b26cad":"#%%timeit \nweekdays = ['sun','mon','tue','wed','thu','fri','sat']  \ntrain.day_of_week = relabelData(weekdays, train,\"day\")\n\ntrain.day_of_week+=1\n\ntrain.head()","a732d646":"encode_cols = nom_cols[(nom_cols!=\"month\") & (nom_cols!=\"day\")].copy()\nencode_cols","a48a4e6f":"from sklearn import preprocessing\nnom_encoder = preprocessing.LabelEncoder()\ntrain[encode_cols].apply(nom_encoder.fit_transform).head()","b0fe1363":"train[encode_cols] = train[encode_cols].apply(nom_encoder.fit_transform)\ntrain.head()","6573ce8a":"labels_encoder = preprocessing.LabelEncoder()\n\nprint(train_labels.head())\n#labels_encoder.fit_transform(train_labels)\n\ntrain_labels = pd.DataFrame(labels_encoder.fit_transform(train_labels), columns=[\"y\"])\n\ntrain_labels.head()","b4fe4e2f":"from sklearn import tree\n\nmodel = tree.DecisionTreeClassifier(random_state=1337) # We are using classifier rather than regressor, because or goal is to \"classify\" participants into \"yes\" and \"no\" categories.\n\nmodel = model.fit(train,train_labels)","64c414f9":"def preProcess(data, labels, drop_cols, missing_val, missing_val_cols, labels_colname = \"deposit\"):\n    \n    from sklearn.preprocessing import LabelEncoder\n    \n    data = data.drop(drop_cols, axis=1)\n    \n    data, labels = dropMissingData(data, labels, missing_val, missing_val_cols, labels_colname = \"deposit\")\n    \n    nom_cols = data.columns.values[data.dtypes == 'O']    \n    \n    nom_encoder = LabelEncoder()\n\n    data[nom_cols] = data[nom_cols].apply(nom_encoder.fit_transform)\n    \n    labels_encoder = LabelEncoder()\n    \n    labels = pd.DataFrame(labels_encoder.fit_transform(labels), columns=[\"deposit\"])\n    \n    #print data.head()\n    \n    return data, labels\n\n\ndrop_cols         = ['pdays',\n                     'poutcome',\n                     'previous',\n                     'campaign', \n                     'duration']\n\nmissing_val       = 'unknown'\n\nmissing_val_cols  = ['education','marital','job','default']\n    \ntest, test_labels = preProcess(test, test_labels, drop_cols, missing_val, missing_val_cols)\n","86d5f0e7":"train_predicted_labels = model.predict(train)","6136f2f6":"from sklearn.metrics import accuracy_score, confusion_matrix, precision_score, recall_score, roc_auc_score, f1_score\n\n\ndef reportPerformance(test_labels, predict_labels):\n    \n    print(\"Accuracy score: {}\".format(round( \n        accuracy_score(test_labels, predict_labels)\n        ,4)))\n\n    print(\"Precision score: {}\".format(round( \n        precision_score(test_labels, predict_labels)\n        ,4)))\n\n    print(\"Recall score: {}\".format(round( \n        recall_score(test_labels, predict_labels)\n        ,4)))\n\n    print(\"ROC score: {}\".format(round( \n        roc_auc_score(test_labels, predict_labels)\n        ,4)))\n    \n    print(\"f1 score: {}\".format(round( \n        f1_score(test_labels, predict_labels, pos_label=1)\n        4)))\n    \nreportPerformance(train_labels, train_predicted_labels)","7e27d2f2":"#Following example from stack exchange, for optimizing the number of estimators in the random forest\n\nimport matplotlib.pyplot as plt\n\nfrom collections import OrderedDict\n\nfrom sklearn.ensemble import RandomForestClassifier\n\n\ndef findMinOOB(test, test_labels, min_estimators = 10, max_estimators = 1000, random_state = 1337):\n    \n    ensemble_clfs = [ (\"RandomForestClassifier, max_features=None\",\n                       RandomForestClassifier(random_state=random_state,warm_start=True, max_features=None, oob_score=True,))]\n\n\n    error_rate = OrderedDict((label, []) for label, _ in ensemble_clfs)\n\n    for label, clf in ensemble_clfs:\n        for i in range(min_estimators, max_estimators + 1):\n            clf.set_params(n_estimators=i)\n            clf.fit(test, test_labels)\n            oob_error = 1 - clf.oob_score_\n            error_rate[label].append((i, oob_error))\n\n\n    for label, clf_err in error_rate.items():\n        xs, ys = zip(*clf_err)\n        plt.plot(xs, ys, label=label)\n\n    plt.xlim(min_estimators, max_estimators)\n    plt.xlabel(\"n_estimators\")\n    plt.ylabel(\"OOB error rate\")\n    plt.legend(loc=\"upper right\")\n    plt.show()\n    \n    #print \"Min. OOB of {} found at N_estimators = {}\".format()\n    \nfindMinOOB(test, test_labels, min_estimators = 100, max_estimators = 1000, random_state = 1337)","b8b04d5e":"# from sklearn.model_selection import GridSearchCV\n\n# def getRandomForestBestParams(param_grid, data, labels, scoring = 'f1', random_state=1337):\n    \n#     estimator = RandomForestClassifier(random_state=random_state)\n\n#     gs = GridSearchCV(estimator, \n#                  param_grid,\n#                  scoring = 'f1')\n\n#     gs = gs.fit(train, train_labels.y)\n\n#     print \"Best Parameters: {}\".format(gs.best_params_)\n\n#     print \"Best Score: {}\".format(gs.best_score_)\n    \n#     return gs\n\n# param_grid = [\n#       {'n_estimators':range(100,1000,25),'max_features': range(2,(len(data.columns)-1))},\n#      ]\n\n# gs = getRandomForestBestParams(param_grid, train, train_labels)","2ac36b82":"def predictWithRandomForest(train, train_labels, test, test_labels, n_estimators,  random_state=1337):\n\n    model_forest = RandomForestClassifier(n_estimators=n_estimators,\n                                          random_state=random_state, \n                                           oob_score=True) # We are using classifier rather than regressor, because or goal is to \"classify\" participants into \"yes\" and \"no\" categories.\n\n    model_forest = model_forest.fit(train, train_labels)\n\n\n    predict_labels = model_forest.predict(test)\n\n    reportPerformance(test_labels, predict_labels)\n    \n    return model_forest\n    \nmodel_forest = predictWithRandomForest(train, train_labels, test, test_labels, n_estimators=1000, random_state=1337)","8f937d32":"print( len(labels[labels==\"no\"]))\nprint( len(labels[labels==\"yes\"]))\n2*5289\/3","fdcf85fe":"size = labels.size\n\nsplit_ratio = 2\/3 #proportion of data taken as training\n\nsize_yes    = len(labels[labels==\"yes\"])\nsize_no     = len(labels[labels==\"no\"])\n\n\nprint(\"Number of 'No' Cases: {}\/{}\".format(size_no,size))\nprint(\"Number of 'Yes' Cases: {}\/{}\".format(size_yes,size))\n\n#print \"Number of 'No' Training Cases \" \ntrain_size_yes = (size_yes*2)\/\/3 \ntrain_size_no  = train_size_yes\n\ntest_size_yes = size_yes - train_size_yes\ntest_size_no  = size_no  - train_size_no\n\ntrain_yes, test_yes, train_labels_yes, test_labels_yes   = train_test_split(data[labels==\"yes\"], labels[labels==\"yes\"], test_size=test_size_yes, random_state=1337)\ntrain_no , test_no,  train_labels_no,  test_labels_no    = train_test_split(data[labels==\"no\"],  labels[labels==\"no\"],  test_size=test_size_no, random_state=1337)","777f029d":"data.head()","620f7095":"train_yes.head()","10768ac3":"from sklearn.utils import shuffle\n\ndef shuffleTogether(data_yes, data_no, labels_yes, labels_no, random_state = 1337, labels_col = \"deposit\"):\n    \n    data   = data_yes.append(data_no)\n    labels = labels_yes.append(labels_no)\n    \n    data = data.join(labels)\n    \n    data = shuffle(data, random_state = random_state)\n    \n    labels = data[labels_col]\n    \n    data   = data.drop(labels_col, axis=1)\n    \n    return data, labels\n\ntrain_under, train_labels_under = shuffleTogether(train_yes, train_no, train_labels_yes, train_labels_no)\ntest_under, test_labels_under   = shuffleTogether(test_yes,  test_no, test_labels_yes, test_labels_no)","7fd952f1":"train_under.head()","11dcb77f":"train_under, train_labels_under  = preProcess(train_under, train_labels_under, drop_cols, missing_val, missing_val_cols)\ntest_under,  test_labels_under   = preProcess(test_under,  test_labels_under,  drop_cols, missing_val, missing_val_cols)","881e71a3":"findMinOOB(train_under, train_labels_under, min_estimators = 100, max_estimators = 1500, random_state = 1337)","dc5cd001":"from sklearn.ensemble import RandomForestClassifier\n\nmodel_forest = RandomForestClassifier(n_estimators=500,\n                                      random_state=1337, \n                                      oob_score=True) # We are using classifier rather than regressor, because or goal is to \"classify\" participants into \"yes\" and \"no\" categories.\n\nmodel_forest = model_forest.fit(train_under, train_labels_under)\n\n\npredict_labels_under = model_forest.predict(test_under)\n\nreportPerformance(test_labels_under, predict_labels_under)","17a0dbb0":"model_forest.feature_importances_","6056ee27":"import mca","bcc08939":"np.set_printoptions(formatter={'float': '{: 0.4f}'.format})\npd.set_option('display.precision', 5)\npd.set_option('display.max_columns', 25)","84b23b11":"ncols   = len(train[nom_cols].columns)\nmca_ind = mca.MCA(train[nom_cols], ncols=ncols, benzecri=False)\n\nprint mca.MCA.__doc__\n","0d6698f6":"mca_ind.L\nmca_ind.expl_var(greenacre=False, N=2)\n\n","f072d860":"data = np.array([mca_ind.L[:2], \n                 mca_ind.expl_var(greenacre=False, N=2) * 100]).T\ndf = pd.DataFrame(data=data, columns=['c\u03bb','%c'], index=range(1,3))\ndf","d104b63d":"data = np.array([mca_ind.L[:2], \n                 mca_ind.expl_var(greenacre=False, N=2) * 100]).T\ndf = pd.DataFrame(data=data, columns=['c\u03bb','%c'], index=range(1,3))\ndf","d5a64992":"fs, cos, cont = 'Factor score','Squared cosines', 'Contributions x 1000'\ntable3 = pd.DataFrame(columns=train[nom_cols].index, index=pd.MultiIndex\n                      .from_product([[fs, cos, cont], range(1, 3)]))\n\ntable3.loc[fs,    :] = mca_ind.fs_r(N=2).T\ntable3.loc[cos,   :] = mca_ind.cos_r(N=2).T\ntable3.loc[cont,  :] = mca_ind.cont_r(N=2).T * 1000\n#table3.loc[fs, 'W?'] = mca_ben.fs_r_sup(pd.DataFrame([i_sup]), N=2)[0]\n\nnp.round(table3.astype(float), 2)","09d55177":"table4 = pd.DataFrame(columns=train[nom_cols].columns, index=pd.MultiIndex\n                      .from_product([[fs, cos, cont], range(1, 3)]))\ntable4.loc[fs,  :] = mca_ind.fs_c(N=2).T\ntable4.loc[cos, :] = mca_ind.cos_c(N=2).T\ntable4.loc[cont,:] = mca_ind.cont_c(N=2).T * 1000\n\n\nnp.round(table4.astype(float), 2)","8c06746c":"%matplotlib inline\nimport matplotlib.pyplot as plt\n\npoints = table3.loc[fs].values\nlabels = table3.columns.values\n\nplt.figure()\n#plt.margins(0.1)\nplt.axhline(0, color='gray')\nplt.axvline(0, color='gray')\nplt.xlabel('Factor 1')\nplt.ylabel('Factor 2')\nplt.scatter(*points, s=120, marker='o', alpha=0.5, linewidths=0)\nplt.show()\n","ec606734":"So we have ourselves a mixed-bag of data here. We won't be able to just charge-forward with either a categorical or quantitative approach... \n\nNow let's see a sample of the data table itself.","afabe8e0":"# 6.3***) Modeling experiments:  Playing with MCA\n\n* **This section was added just out of my own curiosity. It is basically me playing with a completely new method I'd just heard about. \nFeel free to skip it on just read the Conclusions section that follows**\n\nAnother option, for dealing with categorical labels, is Multiple Correspondance Analysis. This seems to be something of a categorical analog of Principal Component Analysis. What follows is essentially an attempt to apply the steps in the Python MCA tutorial (http:\/\/nbviewer.jupyter.org\/github\/esafak\/mca\/blob\/master\/docs\/mca-BurgundiesExample.ipynb) to the present dataset. They had originally applied MCA to wine-tasting data.","b798862d":"Re-label days of the week","e853c2e8":"# 3.4) Check for redundance in the quantitative features","8a7b225f":"## -1) Import core packages","387496f4":"While we get the best recall rate among any of the methods here, our method is still very crude. It tends to recognize the \"Yes\" customers, sure, but it also gives us a lot of false-positives.","6d20070a":"So those were fairly straightforward, since months and days of the week have an obvious order to them. \n\nBut what do we do with things like \"job\" and \"education\"? We'll have to be a bit more arbitrary about it, if we want to do any numerical analysis or apply vectorized routines to speed things up. At least Education has a kind of chronology to it- high-school before university, and so on. \n\nIf we enforce an order on something like \"job\" though, we're may be implicitly making assumptions about its correlation with other data. ","44605b6b":"# 8) Reference\n\n**This is just a copy of the instructions provided on the data archive page**\n\nThe webpage for the dataset is located here: https:\/\/archive.ics.uci.edu\/ml\/datasets\/bank+marketing\n\nSimple descriptions for each column are given as follows:\nInput variables:\n# bank client data:\n1 - age (numeric)  \n2 - job : type of job (categorical: 'admin.','blue-collar','entrepreneur','housemaid','management','retired','self-employed','services','student','technician','unemployed','unknown')  \n3 - marital : marital status (categorical: 'divorced','married','single','unknown'; note: 'divorced' means divorced or widowed)  \n4 - education (categorical: 'basic.4y','basic.6y','basic.9y','high.school','illiterate','professional.course','university.degree','unknown')  \n5 - default: has credit in default? (categorical: 'no','yes','unknown')  \n6 - housing: has housing loan? (categorical: 'no','yes','unknown')  \n7 - loan: has personal loan? (categorical: 'no','yes','unknown')  \n# related with the last contact of the current campaign:\n8 - contact: contact communication type (categorical: 'cellular','telephone')   \n9 - month: last contact month of year (categorical: 'jan', 'feb', 'mar', ..., 'nov', 'dec')  \n10 - day_of_week: last contact day of the week (categorical: 'mon','tue','wed','thu','fri')  \n11 - duration: last contact duration, in seconds (numeric). Important note: this attribute highly affects the output target (e.g., if duration=0 then y='no').   \nYet, the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model.  \n# other attributes:\n12 - campaign: number of contacts performed during this campaign and for this client (numeric, includes last contact)  \n13 - pdays: number of days that passed by after the client was last contacted from a previous campaign (numeric; 999 means client was not previously contacted)  \n14 - previous: number of contacts performed before this campaign and for this client (numeric)  \n15 - poutcome: outcome of the previous marketing campaign (categorical: 'failure','nonexistent','success')  \n# social and economic context attributes\n16 - emp.var.rate: employment variation rate - quarterly indicator (numeric)  \n17 - cons.price.idx: consumer price index - monthly indicator (numeric)   \n18 - cons.conf.idx: consumer confidence index - monthly indicator (numeric)   \n19 - euribor3m: euribor 3 month rate - daily indicator (numeric)  \n20 - nr.employed: number of employees - quarterly indicator (numeric)  \n  \nOutput variable (desired target):  \n21 - y - has the client subscribed a term deposit? (binary: 'yes','no')  ","619ee8df":"Next we get a few performance metrics for the predictions.","40998618":"A few of our data columns lend themselves to straightforward numerical relabeling. For example, January -> 1, Monday -> 2. Not to say that these have an intrinsic quantitative meaning, but it makes sense to use the conventional integer labels rather than arbitrary ones, to maximixe readability.","64a18f8d":"The random forest approach doesn't really seem to help us out here, relative to the simple decision tree approach.\n\nAnother issue is that our number of \"no\"s is way higher than the number of \"yesses\". More than simply a classification problem, this is really a recall problem. We're hunting for needles in the haystack, rather than sorting red fish from blue fish.'\n\nNext we'll play with the method of data splitting, rather than the model itself. We'll try and weight our training data more towards the \"Yes\" case, by inflating the number of yesses to match the number of nos. For this attempt, we'll arbirarily force the yes-to-no ratio to 0.5.","f95f74e2":"### Apply same pre-processing as before","75918441":"We can see that MCA pulls out a primary component of our data, explaining 73% of variance. But what we are not seeing is any kind of structure appearing in the data, such that we could easily apply a cluster-based approach like K-means.\nWhy? Well, maybe it's simply the case that these intrinsic properties of the participants- education job, etc., are not particularly informative when it comes to success rate for the telemarketing campaigns. \n\nThere's one last thing we can try, which is to perform MCA with the labels as an input factor","772d4e7a":"# 6.1) Modelling experiments: Decision tree\n\nWhile I'm not an expert in decision trees-- I have only read about them -- among the methods I find for mixed-data type machine learning, I prefer it because of the relative transparency. A decision tree produces a graoh of its steps which we can easily visualize. MCA, while seemingly straightforward if we call it \"PCA for categorical data\", is more mathematically complex to describe. Neural nets are infamously opaque.\n\nWhat follows is largely based on scikit learn examples, either from their online documentation or \nhttp:\/\/scikit-learn.org\/stable\/modules\/tree.html","1debb60b":"Let's see what the data types are, and how many entries we have after the split.","0bef2b81":"The model performs suspiciously well on the training data...but of course it performs well on data it's already seen. The very high accuracy rate smells a bit of over fitting. Next we'll see how it does with the test data, which it's never encountered.\n\nTo test our fitted model, we have to apply all of the above pre-processing steps to the test data as well. Let's bundle those into a function that we can use for later exeriments.","740a8ca1":"Just for diagnostics, let's see how the model fits the training data. This is not a valid way to say if our model will work in reality. It just helps us see if something has gone terrible wrong so far. What we expect at this point is that the model works well for the training data. If it can't even fit data it has already seen however, that means we have other issues to resolve first.","a92c63f3":"Now let's take a look at some of our categorical data. For starters, just look at t he attributes that are intrinsic attribut.\nes of the participant- not the phone they used, not what the economic weather was like. This leaves education, job, and age. Well, marital status, housing, financial status can arguably be \"intrinsic\", but let's start with the basics. \n\nPerhaps we can agree that age would be the most objective, quantitatve attribute of a caller, among the data we have. Let's start there:","66650f36":"Some easy things we can convert are month and day of the week to 1-12 and 1-7","26f1dc3a":"### Shuffle the data","567f3b1b":"## 3.3) Variance test the quantitative features","22a86ab3":"Without getting to an objective measure here, it as at least visually obvious that the yesses (1s) have a thicker tail out past the retirement ages (though we can't assume yet that all these people are \"retired\" per-se).","6b5861b4":"First, apply the model we just trained to the training data to get the predicted labels.","86b0442d":"The \"retired\" category is interesting. Maybe we would not be wrong in assuming our retirement-aged yessers are in-fact retired. But then again, we see an extended advanced-age-tail with the \"housemaid\" category as well.","f3fa0968":"\nAccording to the matrix, there's no obvious sign of redundant data. None of the columns appear correlated with one another.\n\n","dc2ea389":"When we look at the distribution of ages, we see that the \"violin\" plots from before can be a bit misleading.\n\nWhile they seem to tell us that there is an overall trend with age,the age distribution above below tells us that there are very few participants over age 60. So while it may be accurate to say that, within those over 60, there is a trend to say yes- there is likely not a significant trend in this data for those who say yes to be over 60. \n\nWe can't rule out a samping bias, but we do have to be careful about density plots. The reason for the confusion is that, for each side of the violin plots, the area is normalized to that category. So it falseley represents the groups is being roughly equal.","fb164fb9":"# 4) Pre-analysis: Categorical data","93766c53":"It seems our model doesn't generalize very well to data it hasn't seen yet. Perhaps the tree is being over-fit to the data?\n\nThe recall rate of 0.333 says that we're recovering 1\/3 of would-be depositors. Worse than flipping a coin.\n\n","3ce3b9c3":"# 5) Process the categorical data","5642dbb1":"Asking \"How informative is this column?\" is like asking \"How much does this column tell us about how data points vary from one another?\". \n\nFor the quantitative data, there's a straight-forward way to test this--- look at the variance. To do this properly, we have to put the data into the same scale. Depending on the modeling method we try, this might have to do this anyways (i.e. for neural networks).\n\n\nWe'll first set a threshold for the variance, let's say 0.2, and then exclude columns that don't meet this threshold. A good example from scikit learn's documentations shows a variance filter of 0.8, so let's use that for the moment. (Important to keep in mind that the y-columns itself shows low variance, with the vast majority being \"No\"s.)","1c84cd90":"Now we'll feed the best parameters into the model.","f30c922f":"# 6.2) Modelling experiments: Random forests\n\n\nNext we'll try an extension of the decision tree model, called \"random forests\". This is a way of preventing overfitting to the data, but constructing many different decision trees, with different subsets of the provided features and datasets.\n\nIt's a bit more difficult to deal with though, since we have to choose the number of different trees generated in the procedure. The following cell fits the model for different values of 'n_estimators', the number of decision trees, and check which performs best via the OOB error rate.","8136df0b":"Interestingly all of our quanttative features pass the variance test. Although two of them just barely, 'pdays' and 'previous'. These two must be correlated, since you can only have a previous outcome recorded if you have a valid value for pdays. In any case, the numeric value of \"999\" for pdays has no meaning.","06d0aa21":"## 3.2) Isolate the quantitative columns\n\nSome data are given to us as numbers. Others are string labels, without an obvious way to rank them.","2786756e":"# 3) Feature Selection","af7db636":"Some columns show missing or N\/A values, like 'unknown' in 'default', and 'nonexistent' in 'poutcome'. We may not necessarily keep all the columns, but we have to choose a strategy now for handling rows (\"participants\") that have missing data values.\n\nFor this project, let's take a conservative approach- remove entire rows that have 'unknown' values. This is to avoid treating 'unknown' as carrying the same degree of meaning as the defined categories. We don't know why this data are unknown, and we can't assume that two people who have 'unknown' education values have anything in common. \n\nValue '999' for 'pdays' also carries no quantitative meaning. But at the same time, if we remove all of these rows, then we're removing all the data where the participant was being called for the first-time. We'll leave these rows for now, but consider removing the 'pdays' column later.","833f778d":"# Conclusion:\n\nI have tried several modeling approaches to predict the outcome of a telemarketing campagin. Of all the approaches, the most effective \n\nI've avoided making a model that predicts based on the length of the calls, as this seems a bit circular. This differs from many examples I have found online, which found that call duration is a good predictor. My hope was to try and explore a bit deeper than this, and predict based on more independent data.\n\nThe bottom line is that the results are not very positive. But why? Well, if I were to take more time to get to the bottom of it, I would explore more rigorous approaches to tuning the models. The other interesting thing, which appears in the article connected to this data, is that they made a reasonable prediction using neural networks. This is something I would like to try if I had more time, but so far I avoided it because I do not feel neural networks are a good \"default\" approach, unless the task is something like image classification. The reason is that it is hard to visualize what a NN has actually done, at the end, when we are dealing with a mix of quantitative and qualitative data. Especially socio economic data... but perhaps I'm being too pessimistic, and it's worth a try to see how NNs compare to random forest approaches, SVMs,  etc.\n\nThe other point is data pre-processing. Although \"inflating\" the proportion of \"Yes\" samples in the training data still only resulted in 70% recall, and a lot of false-positives (translating to a lot of wasted time calling \"Nos\"), it's important to note that this did increase the recall relative to the random-sampling case (training Yes\/No proportions equal to that of the full data set.) Interesting, but not shocking: we gave our model more Yes examples, so it tended to guess \"Yes\" more frequently when it saw the test data. While accuracy of ~75%, and recall of ~70% are better than a coin-flip, this is probably not enough of a trade-off to implement the model in actual callee-selection. Perhaps the undersampling method however could be tuned more for better results.\n\nBased on the simple pre-analysis I did, using violin plots, etc., I realize that there is one data dimension seriously lacking here. That is an objective, quantitative indicator of a customers income, or better yet, disposable income. After all, someone cannot sign up for a term-deposit if they do not have a surplus of cash to deposit. Even just the fact that someone is wealthy is not an indicator of this, since their budget could be strictly balanced. As of now, there is no data that separates the objective \"Ability to deposit\" from the subjective \"Willingness to deposit\".\n\nMy main reason for suspecting the need for such data, is the relatively high percentage of \"Yes\" responses for customers over the age of 60, and the same for the \"retired\" category. While it is speculative, it may be the case that some of these are individuals with lifetime savings, open to wealth-management options. But by the same token, as the advanced age group overlaps with the low-eduation (basic.4yr) group, we could speculate that some of them did not understand the details of the call. It's worth considering, and maybe getting more data, but not nearly strong enough evidence to make a marketing recommendation.","3c3c2175":"The quantitative data are either integer ('int64') or floats ('float64'). The qualitative, string data are given as object data type, indicatd by 'O'. We'll call the qualitative ones 'nom_cols', so we don't confuse it with 'quant_cols'.","ccddb3d1":"# Bank Marketing Data\nAaron Bell","d59b6665":"The remaining columns don't have an obvious way to numberize them. We could convert education to years of education, but then what do we do with labels like 'illiterate'? We have to be careful though, not to transform the columns that we already dealt with.","afdbf7b6":"# 1) Data splitting","ac667043":"We're given 21 features, which is tempting to just throw at a black-box and see what comes out.\nBut let's show some patience, and pre-analyze which features are the most useful, which are redundant and uninformative.","17696a48":"Let's take the quantitative data aside for a minute, and see if any of the columns are storngly related or redundant. \n\nOne way of doing this is through a simple correlation test. Pandas gives us a quick and convenient way to get a correlation coefficient (pearson, spearman) for all of the features in our dataset, vs. each other. Or something like a normalized covariance matrix.\n\nI prefer to use Spearman as the default corr. test, since it doesn't assume we're looking at linear trends, and is less sensitive to outliers.","152e36f4":"In to the instructions, we're told that \"duration\" had best be omitted:  \n```...the duration is not known before a call is performed. Also, after the end of the call y is obviously known. Thus, this input should only be included for benchmark purposes and should be discarded if the intention is to have a realistic predictive model. ``` \n\nWhich makes sense. If you are looking at a spread-sheet of client data, you have no idea how long a phone call in their future is going to last. If the call duration was being controlled somehow by the telemarketers, we could test if keeping participants on the phone longer related to more \"yes\" answers. But we don't know that in this case. We just assume that 'duration' means the after-the-fact length of the call, including time it takes for customers to ask questions and confirm their answers, and that the telemarketers were not artifically controlling the call length. \n\nSo let's drop it.","efe7e419":"To make sure we're testing our process as objectively, we don't want the data we use to fit the model (\"training\" data) to be mixed with the data used for the final performance test (\"testing\" data). \n\nFirst we'll try a simple 70\/30, training\/testing spit, randomly selecting data points for each set.\n\nWe won't take any looks at the the data until it's split, so that the test can be as independent as possible from our analysis strategy. ","e9d1ca7a":"While we see a steady decrease past 900, it's interesting that the minimum occurs at about 110. I'll prefer the more consistently lower error rate ~1000 for the moment. In a production environment though, it would probably be better to more thoroughly balance 'n_estimators' with the error rate, so that we don't waste computation time while re-testing.","9cc0fa3c":"So let's take 2\/3 of 5289 as the test set. But we want to try and use equal proportions of yes and no cases, so let's also take (2\/3)*5289 of samples for the no cases.","40a8b959":"# 6.2 Modeling experiments: Undersampling","4f8ed508":"We'll also want to have a quick way to check various performanc measure of the model: Accuracy, precision, and recall. To be honest, accuracy is not really helpful in this case, since the main goal is finding the minority of cases where the result was \"yes\". \n\nA model that always predicts \"no\" would give us a misleadingly good accuracy.\n\nA model that always predicts \"yes\" (looking under every stone) would give us a perfect recall score, but in practice would waste hours of telemarketing time and resources.\n\nA 50\/50 coin-flip model would give is a recall of about 50%. Same for accuracy. So we have to be careful to compare the scores together, and remember that it's entirely possible for a complex model to be less effective than asking your cat for advice.","6539ca59":"# 0) Data loading\npandas allows for easy reading-in of CSV and other files. \nWe'll load the data into a pandas dataframe (analagous to R dataframes), then take a look at how it's stuctured.\n\nIMPORTANT: 'read_csv' sometimes gets confused if the file is delimited in a weird way. \n\nWe also know from the instructions (copied at the end of this notebook) that the target label is column 'deposit''. This corresponds to is whether customers said \"yes\" or \"no\" to a term deposit."}}