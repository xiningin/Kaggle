{"cell_type":{"f6565e70":"code","8982d3f8":"code","0b800277":"code","c1bf3918":"code","84a0aaa1":"code","1cfe7b91":"code","fe9f0d42":"code","66309b44":"code","8f0d5f41":"code","ba3ce695":"code","0ad1a5fb":"code","ae99d316":"code","abd63461":"code","8eead224":"code","13d7038c":"code","3499613e":"code","aaafe5b3":"code","b97f9867":"code","32ec3c36":"code","dbd9f817":"code","46d2f775":"code","bbf6f44a":"code","df9bd311":"code","a69f2380":"code","7b6aa432":"code","37e69dd3":"code","3aeef584":"code","06d1bea8":"code","52db5316":"code","3ac686b9":"code","cad32075":"code","6a05f3e5":"code","322bae5f":"code","cc032dbb":"code","0b4e3a7a":"code","f9adfae8":"markdown","9a9ce575":"markdown","9cc7ea9e":"markdown","a358a051":"markdown","4b428c93":"markdown","c37b60b7":"markdown","60ffabc7":"markdown","0fbb576b":"markdown","9b2fe8f9":"markdown","86dbe2f5":"markdown","7317c859":"markdown","1cd27db2":"markdown","2f8565f1":"markdown","c528b316":"markdown","3d0614ac":"markdown","0349bdf3":"markdown","0597128a":"markdown"},"source":{"f6565e70":"import re\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nfrom collections import defaultdict, Counter\nfrom sklearn.feature_extraction.text import CountVectorizer\nimport nltk\nfrom nltk.corpus import stopwords\nfrom wordcloud import WordCloud \nfrom nltk.tokenize import word_tokenize \n\n\nnltk.download('stopwords', quiet=True)\nstopwords = stopwords.words('english')\nsns.set(style=\"white\", font_scale=1.2)\nplt.rcParams[\"figure.figsize\"] = [10,8]\npd.set_option.display_max_columns = 0\npd.set_option.display_max_rows = 0","8982d3f8":"train = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\ntest = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")","0b800277":"train.head()","c1bf3918":"train.shape, test.shape, test.shape[0]\/train.shape[0]","84a0aaa1":"print('There are {} rows and {} columns in train'.format(train.shape[0],train.shape[1]))\nprint('There are {} rows and {} columns in train'.format(test.shape[0],test.shape[1]))","1cfe7b91":"train.info()","fe9f0d42":"null_counts = pd.DataFrame({\"Num_Null\": train.isnull().sum()})\nnull_counts[\"Pct_Null\"] = null_counts[\"Num_Null\"] \/ train.count() * 100\nnull_counts","66309b44":"keywords_vc = pd.DataFrame({\"Count\": train[\"keyword\"].value_counts()})\nsns.barplot(y=keywords_vc[0:30].index, x=keywords_vc[0:30][\"Count\"], orient='h')\nplt.title(\"Top 30 Keywords\")\nplt.show()","8f0d5f41":"len(train[\"keyword\"].value_counts())","ba3ce695":"disaster_keywords = train.loc[train[\"target\"] == 1][\"keyword\"].value_counts()\nnondisaster_keywords = train.loc[train[\"target\"] == 0][\"keyword\"].value_counts()\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(y=disaster_keywords[0:30].index, x=disaster_keywords[0:30], orient='h', ax=ax[0], palette=\"Reds_d\")\nsns.barplot(y=nondisaster_keywords[0:30].index, x=nondisaster_keywords[0:30], orient='h', ax=ax[1], palette=\"Blues_d\")\nax[0].set_title(\"Top 30 Keywords - Disaster Tweets\")\nax[0].set_xlabel(\"Keyword Frequency\")\nax[1].set_title(\"Top 30 Keywords - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Keyword Frequency\")\nplt.tight_layout()\nplt.show()","0ad1a5fb":"armageddon_tweets = train[(train[\"keyword\"].fillna(\"\").str.contains(\"armageddon\")) & (train[\"target\"] == 0)]\nprint(\"An example tweet:\\n\", armageddon_tweets.iloc[10, 3])\narmageddon_tweets.head()","ae99d316":"def keyword_disaster_probabilities(x):\n    tweets_w_keyword = np.sum(train[\"keyword\"].fillna(\"\").str.contains(x))\n    tweets_w_keyword_disaster = np.sum(train[\"keyword\"].fillna(\"\").str.contains(x) & train[\"target\"] == 1)\n    return tweets_w_keyword_disaster \/ tweets_w_keyword\n\nkeywords_vc[\"Disaster_Probability\"] = keywords_vc.index.map(keyword_disaster_probabilities)\nkeywords_vc.head()","abd63461":"keywords_vc.sort_values(by=\"Disaster_Probability\", ascending=False).head(10)","8eead224":"keywords_vc.sort_values(by=\"Disaster_Probability\").head(10)","13d7038c":"locations_vc = train[\"location\"].value_counts()\nsns.barplot(y=locations_vc[0:30].index, x=locations_vc[0:30], orient='h')\nplt.title(\"Top 30 Locations\")\nplt.show()","3499613e":"len(train[\"location\"].value_counts())","aaafe5b3":"disaster_locations = train.loc[train[\"target\"] == 1][\"location\"].value_counts()\nnondisaster_locations = train.loc[train[\"target\"] == 0][\"location\"].value_counts()\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(y=disaster_locations[0:30].index, x=disaster_locations[0:30], orient='h', ax=ax[0], palette=\"Reds_d\")\nsns.barplot(y=nondisaster_locations[0:30].index, x=nondisaster_locations[0:30], orient='h', ax=ax[1], palette=\"Blues_d\")\nax[0].set_title(\"Top 30 Locations - Disaster Tweets\")\nax[0].set_xlabel(\"Keyword Frequency\")\nax[1].set_title(\"Top 30 Locations - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Keyword Frequency\")\nplt.tight_layout()\nplt.show()","b97f9867":"train[\"tweet_length\"] = train[\"text\"].apply(len)\nsns.distplot(train[\"tweet_length\"])\nplt.title(\"Histogram of Tweet Length\")\nplt.xlabel(\"Number of Characters\")\nplt.ylabel(\"Density\")\nplt.show()","32ec3c36":"min(train[\"tweet_length\"]), max(train[\"tweet_length\"])","dbd9f817":"g = sns.FacetGrid(train, col=\"target\", height=5)\ng = g.map(sns.distplot, \"tweet_length\")\nplt.suptitle(\"Distribution Tweet Length\")\nplt.show()","46d2f775":"def count_words(x):\n    return len(x.split())\n\ntrain[\"num_words\"] = train[\"text\"].apply(count_words)\nsns.distplot(train[\"num_words\"], bins=10)\nplt.title(\"Histogram of Number of Words per Tweet\")\nplt.xlabel(\"Number of Words\")\nplt.ylabel(\"Density\")\nplt.show()","bbf6f44a":"g = sns.FacetGrid(train, col=\"target\", height=5)\ng = g.map(sns.distplot, \"num_words\")\nplt.suptitle(\"Distribution Number of Words\")\nplt.show()","df9bd311":"def avg_word_length(x):\n    return np.sum([len(w) for w in x.split()]) \/ len(x.split())\n\ntrain[\"avg_word_length\"] = train[\"text\"].apply(avg_word_length)\nsns.distplot(train[\"avg_word_length\"])\nplt.title(\"Histogram of Average Word Length\")\nplt.xlabel(\"Average Word Length\")\nplt.ylabel(\"Density\")\nplt.show()","a69f2380":"g = sns.FacetGrid(train, col=\"target\", height=5)\ng = g.map(sns.distplot, \"avg_word_length\")","7b6aa432":"def create_corpus(target):\n    corpus = []\n\n    for w in train.loc[train[\"target\"] == target][\"text\"].str.split():\n        for i in w:\n            corpus.append(i)\n            \n    return corpus\n\ndef create_corpus_dict(target):\n    corpus = create_corpus(target)\n            \n    stop_dict = defaultdict(int)\n    for word in corpus:\n        if word in stopwords:\n            stop_dict[word] += 1\n    return sorted(stop_dict.items(), key=lambda x:x[1], reverse=True)","37e69dd3":"corpus_disaster_dict = create_corpus_dict(0)\ncorpus_non_disaster_dict = create_corpus_dict(1)\n\ndisaster_x, disaster_y = zip(*corpus_disaster_dict)\nnon_disaster_x, non_disaster_y = zip(*corpus_non_disaster_dict)\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(y=list(disaster_x)[0:30], x=list(disaster_y)[0:30], orient='h', palette=\"Reds_d\", ax=ax[0])\nsns.barplot(y=list(non_disaster_x)[0:30], x=list(non_disaster_y)[0:30], orient='h', palette=\"Blues_d\", ax=ax[1]) \nax[0].set_title(\"Top 30 Stop Words - Disaster Tweets\")\nax[0].set_xlabel(\"Stop Word Frequency\")\nax[1].set_title(\"Top 30 Stop Words - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Stop Word Frequency\")\nplt.tight_layout()\nplt.show()","3aeef584":"corpus_disaster, corpus_non_disaster = create_corpus(1), create_corpus(0)\ncounter_disaster, counter_non_disaster = Counter(corpus_disaster), Counter(corpus_non_disaster)\nx_disaster, y_disaster, x_non_disaster, y_non_disaster = [], [], [], []\n\ncounter = 0\nfor word, count in counter_disaster.most_common()[0:100]:\n    if (word not in stopwords and counter < 15):\n        counter += 1\n        x_disaster.append(word)\n        y_disaster.append(count)\n\ncounter = 0\nfor word, count in counter_non_disaster.most_common()[0:100]:\n    if (word not in stopwords and counter < 15):\n        counter += 1\n        x_non_disaster.append(word)\n        y_non_disaster.append(count)\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(x=y_disaster, y=x_disaster, orient='h', palette=\"Reds_d\", ax=ax[0])\nsns.barplot(x=y_non_disaster, y=x_non_disaster, orient='h', palette=\"Blues_d\", ax=ax[1])\nax[0].set_title(\"Top 15 Non-Stopwords - Disaster Tweets\")\nax[0].set_xlabel(\"Word Frequency\")\nax[1].set_title(\"Top 15 Non-Stopwords - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Word Frequency\")\nplt.tight_layout()\nplt.show()","06d1bea8":"def bigrams(target):\n    corpus = train[train[\"target\"] == target][\"text\"]\n    count_vec = CountVectorizer(ngram_range=(2, 2)).fit(corpus)\n    bag_of_words = count_vec.transform(corpus)\n    sum_words = bag_of_words.sum(axis=0) \n    words_freq = [(word, sum_words[0, idx]) for word, idx in count_vec.vocabulary_.items()]\n    words_freq =sorted(words_freq, key = lambda x: x[1], reverse=True)\n    return words_freq","52db5316":"bigrams_disaster = bigrams(1)[:15]\nbigrams_non_disaster = bigrams(0)[:15]\n\nx_disaster, y_disaster = map(list, zip(*bigrams_disaster))\nx_non_disaster, y_non_disaster = map(list, zip(*bigrams_non_disaster))\n\nfig, ax = plt.subplots(1,2, figsize=(20,8))\nsns.barplot(x=y_disaster, y=x_disaster, orient='h', palette=\"Reds_d\", ax=ax[0])\nsns.barplot(x=y_non_disaster, y=x_non_disaster, orient='h', palette=\"Blues_d\", ax=ax[1])\n\nax[0].set_title(\"Top 15 Bigrams - Disaster Tweets\")\nax[0].set_xlabel(\"Word Frequency\")\nax[1].set_title(\"Top 15 Bigrams - Non-Disaster Tweets\")\nax[1].set_xlabel(\"Word Frequency\")\nplt.tight_layout()\nplt.show()","3ac686b9":"target_vc = train[\"target\"].value_counts(normalize=True)\nprint(\"Not Disaster: {:.2%}, Disaster: {:.2%}\".format(target_vc[0], target_vc[1]))\nsns.barplot(x=target_vc.index, y=target_vc)\nplt.title(\"Histogram of Disaster vs. Non-Disaster\")\nplt.xlabel(\"0 = Non-Disaster, 1 = Disaster\")\nplt.show()","cad32075":"train","6a05f3e5":"from nltk.corpus import stopwords\n\n#function for removing pattern\ndef remove_pattern(input_txt, pattern):\n    r = re.findall(pattern, input_txt)\n    for i in r:\n        input_txt = re.sub(i, '', input_txt)\n    return input_txt\n\n# remove '#' handle\ntrain['tweet'] = np.vectorize(remove_pattern)(train['text'], \"#[\\w]*\")\ntest['tweet'] = np.vectorize(remove_pattern)(test['text'], \"#[\\w]*\") \ntrain.head()\n\n#Delete everything except alphabet\ntrain['tweet'] = train['tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\ntest['tweet'] = test['tweet'].str.replace(\"[^a-zA-Z#]\", \" \")\ntrain.head()\n\n\n#Dropping words whose length is less than 3\ntrain['tweet'] = train['tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ntest['tweet'] = test['tweet'].apply(lambda x: ' '.join([w for w in x.split() if len(w)>3]))\ntrain.head()\n\n\n#convert all the words into lower case\ntrain['tweet'] = train['tweet'].str.lower()\ntest['tweet'] = test['tweet'].str.lower()\n\n\nset(stopwords.words('english'))\n\n# set of stop words\nstops = set(stopwords.words('english')) \n\n# tokens of words  \ntrain['tokenized_sents'] = train.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\ntest['tokenized_sents'] = test.apply(lambda row: nltk.word_tokenize(row['tweet']), axis=1)\n\n#function to remove stop words\ndef remove_stops(row):\n    my_list = row['tokenized_sents']\n    meaningful_words = [w for w in my_list if not w in stops]\n    return (meaningful_words)\n\n#removing stop words\ntrain['clean_tweet'] = train.apply(remove_stops, axis=1)\ntest['clean_tweet'] = test.apply(remove_stops, axis=1)\ntrain.drop([\"tweet\",\"tokenized_sents\"], axis = 1, inplace = True)\ntest.drop([\"tweet\",\"tokenized_sents\"], axis = 1, inplace = True)\n\n#re-join the words after tokenization\ndef rejoin_words(row):\n    my_list = row['clean_tweet']\n    joined_words = ( \" \".join(my_list))\n    return joined_words\n\ntrain['clean_tweet'] = train.apply(rejoin_words, axis=1)\ntest['clean_tweet'] = test.apply(rejoin_words, axis=1)\ntrain.head()\n","322bae5f":"all_word = ' '.join([text for text in train['clean_tweet']])\nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(all_word) \nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off') \nplt.show()","cc032dbb":"normal_words =' '.join([text for text in train['clean_tweet'][train['target'] == 1]]) \nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words) \nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","0b4e3a7a":"normal_words =' '.join([text for text in train['clean_tweet'][train['target'] == 0]]) \nwordcloud = WordCloud(width=800, height=500, random_state=21, max_font_size=110).generate(normal_words) \nplt.figure(figsize=(10, 7)) \nplt.imshow(wordcloud, interpolation=\"bilinear\")\nplt.axis('off')\nplt.show()","f9adfae8":"As we can see when we pull out the top 30 keywords for disaster vs non-disaster tweets, word appearance alone is not sufficient to classify content. For example, \"body%20bags\" and \"armageddon\" are the top 1st and 2nd keyword for non-disaster tweets!","9a9ce575":"## Visualization of all the words which signify real disaster","9cc7ea9e":"Overall, there are 221 different keywords associated with tweets.","a358a051":"## Number of Words","4b428c93":"## Common Stopwords","c37b60b7":"## Average Word Length","60ffabc7":"## Visualization of all the words which signify unreal disaster","0fbb576b":"### Explore the Data","9b2fe8f9":"# Disaster Tweets: Exploratory Data Analysis","86dbe2f5":"## Common Bigrams","7317c859":"### Keyword\n\nWhat are some of the most commonly used keywords?","1cd27db2":"### Location","2f8565f1":"## Visualization of all the words using word cloud","c528b316":"### Text aka Tweet Content\n\n**tweet length distribution**","3d0614ac":"## Common non-stopwords","0349bdf3":"### Target","0597128a":"We can pull out non-disaster tweets containing words that we would expect to be associated with disasters. Clearly, an accurate model will need to take context into account."}}