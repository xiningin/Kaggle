{"cell_type":{"2b7a1b4f":"code","db27ccef":"code","870ace7e":"code","8be68b17":"code","aba8688c":"code","24430925":"code","e00c9a25":"code","5240302c":"code","eb169912":"code","a0555017":"code","d6d3762b":"code","b495d23a":"code","0c33d2fa":"code","f5e55913":"code","6f55cb53":"code","23154bcb":"code","50235ddb":"code","7326eaa3":"code","fc857f30":"code","8009befb":"code","8993ba33":"code","48b45b26":"code","0ad3ad5d":"code","ebf2f86e":"code","9665f98b":"code","713e9c17":"code","d5917ee9":"code","432752d2":"code","ae9064f6":"code","e370cc11":"code","50779f33":"code","9153f9bf":"markdown","98ea3917":"markdown","63c49bc2":"markdown","80a83c47":"markdown","c0413f68":"markdown"},"source":{"2b7a1b4f":"import numpy as np\nimport pandas as pd\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.metrics import confusion_matrix, accuracy_score, plot_confusion_matrix\nfrom sklearn import metrics\nimport matplotlib.pyplot as plt\nimport random\nimport xgboost as xgb\nfrom sklearn.svm import SVC\nfrom sklearn.model_selection import RandomizedSearchCV, GridSearchCV\n","db27ccef":"input_data = \"..\/input\/forcedataset\/force-ai-well-logs\/train.csv\"\ntest_data = \"..\/input\/forcedataset\/force-ai-well-logs\/test.csv\"","870ace7e":"TARGET_1 = \"FORCE_2020_LITHOFACIES_LITHOLOGY\" # Class label\nTARGET_2 = \"FORCE_2020_LITHOFACIES_CONFIDENCE\"\nWELL_NAME = \"WELL\"","8be68b17":"lithology_keys = {30000: 0,\n                 65030: 1,\n                 65000: 2,\n                 80000: 3,\n                 74000: 4,\n                 70000: 5,\n                 70032: 6,\n                 88000: 7,\n                 86000: 8,\n                 99000: 9,\n                 90000: 10,\n                 93000: 11}","aba8688c":"data_frame = pd.read_csv(input_data, sep=';')\ntest = pd.read_csv(test_data, sep=';')\ndata_frame = data_frame.tail(50000)","24430925":"# Numbers of rock types\nrocks = data_frame[TARGET_1].value_counts()\nrocks.shape","e00c9a25":"# Numbers of wells\nwells = data_frame[WELL_NAME].value_counts()\nwells.shape","5240302c":"unused_columns = [WELL_NAME, TARGET_1, TARGET_2, 'GROUP', 'FORMATION']\nuse_columns = [c for c in data_frame.columns if c not in unused_columns]","eb169912":"for c in use_columns:\n    data_frame[c].fillna(data_frame[c].mean(), inplace=True)","a0555017":"data_frame[WELL_NAME].unique(), data_frame[WELL_NAME].unique().shape","d6d3762b":"random.shuffle(data_frame[WELL_NAME].unique()) \ntrain_wells = [] \nfor i in range(6): \n    train_wells.append(data_frame[WELL_NAME].unique()[i]) \ntrain_mask = data_frame[WELL_NAME].isin(train_wells)","b495d23a":"X_train = data_frame[train_mask][use_columns].values\ny_train = data_frame[train_mask][TARGET_1].values\nX_train.shape, y_train.shape","0c33d2fa":"X_train.shape, y_train.shape","f5e55913":"X_test = data_frame[~train_mask][use_columns].values\ny_test = data_frame[~train_mask][TARGET_1].values","6f55cb53":"model = RandomForestClassifier(n_estimators = 500, max_depth=20)\nmodel","23154bcb":"model.fit(X_train, y_train)\n","50235ddb":"y_pred = model.predict(X_test)","7326eaa3":"fig, ax = plt.subplots(figsize=(10, 10))\nplot_confusion_matrix(model, X_test, y_test, ax=ax)","fc857f30":"cm2 = confusion_matrix(y_true=y_test, y_pred=y_pred2)\n\ndisp = metrics.plot_confusion_matrix(model2, X_test, y_test)\ndisp.figure_.suptitle(\"Confusion Matrix\")\nplt.show()\nprint(cm2)","8009befb":"accuracy = accuracy_score(y_test, y_pred)\naccuracy","8993ba33":"xgb = xgb.XGBClassifier(    objective= 'binary:logistic',\n    nthread=4,\n    seed=42)","48b45b26":"params = {\n        'min_child_weight': [5, 10],\n        'subsample': [0.8, 1.0],\n        'colsample_bytree': [0.8, 1.0],\n        'max_depth': [4, 5],\n        'learning_rate' : [0.1, 0.05],\n        'n_estimators': [100, 200]\n        }","0ad3ad5d":"grid_search = GridSearchCV(xgb, params, cv=2)\n","ebf2f86e":"grid_search.fit(X_train, y_train)\n","9665f98b":"grid_search.best_estimator_\n","713e9c17":"tp = 0\nmax = 0\nfor i in range(len(cm1)):\n    tp += cm1[i][i]\n    if cm1[i][i] > max:\n        max = cm1[i][i]\nprint(\"Accuracy = {}\".format(1.0*tp\/np.sum(cm1)))","d5917ee9":"tp = 0\nmax = 0\nfor i in range(len(cm2)):\n    tp += cm2[i][i]\n    if cm2[i][i] > max:\n        max = cm2[i][i]\nprint(\"Accuracy = {}\".format(1.0*tp\/np.sum(cm2)))","432752d2":"A = np.load('..\/input\/penalty-matrix\/penalty_matrix.npy')\nA","ae9064f6":"def score(y_true, y_pred):\n    S = 0.0\n    y_true = y_true.astype(int)\n    y_pred = y_pred.astype(int)\n    for i in range(0, y_true.shape[0]):\n        S -= A[lithology_keys[y_true[i]], lithology_keys[y_pred[i]]]\n    return S\/y_true.shape[0]","e370cc11":"score(y_test, y_pred)","50779f33":"importances = model.feature_importances_\nstd = np.std([tree.feature_importances_ for tree in model.estimators_],\n             axis=0)\nindices = np.argsort(importances)[::-1]\n\n# Print the feature ranking\nprint(\"Feature ranking:\")\n\nfor f in range(X_train.shape[1]):\n    print(\"%d. feature %d (%f)\" % (f + 1, indices[f], importances[indices[f]]))\n\n# Plot the impurity-based feature importances of the forest\nplt.figure()\nplt.title(\"Feature importances\")\nplt.bar(range(X_train.shape[1]), importances[indices],\n        color=\"r\", yerr=std[indices], align=\"center\")\nplt.xticks(range(X_train.shape[1]), indices)\nplt.xlim([-1, X_train.shape[1]])\nplt.show()","9153f9bf":"# **Preparing data for training**","98ea3917":"model = XGBClassifier(\n             learning_rate =0.1,\n             n_estimators=1000,\n             max_depth=5,\n             min_child_weight=1,\n             gamma=0,\n             subsample=0.8,\n             colsample_bytree=0.8,\n             objective= 'binary:logistic',\n             nthread=4,\n             seed=27)","63c49bc2":"def algorithm_pipeline(X_train_data, X_test_data, y_train_data, y_test_data, \n                       model, param_grid, cv=10, scoring_fit='neg_mean_squared_error',\n                       do_probabilities = False):\n    gs = GridSearchCV(\n        estimator=model,\n        param_grid=param_grid, \n        cv=cv, \n        n_jobs=-1, \n        scoring=scoring_fit,\n        verbose=2\n    )\n    fitted_model = gs.fit(X_train_data, y_train_data)\n    \n    if do_probabilities:\n      pred = fitted_model.predict_proba(X_test_data)\n    else:\n      pred = fitted_model.predict(X_test_data)\n    \n    return fitted_model, pred","80a83c47":"model = xgb.XGBClassifier()\nparam_grid = {\n    'n_estimators': [400, 700, 1000],\n    'colsample_bytree': [0.7, 0.8],\n    'max_depth': [15,20,25],\n    'reg_alpha': [1.1, 1.2, 1.3],\n    'reg_lambda': [1.1, 1.2, 1.3],\n    'subsample': [0.7, 0.8, 0.9]\n}\n\nmodel, pred = algorithm_pipeline(X_train, X_test, y_train, y_test, model, \n                                 param_grid, cv=5)\n\n# Root Mean Squared Error\nprint(np.sqrt(-model.best_score_))\nprint(model.best_params_)","c0413f68":"# **Training model**"}}