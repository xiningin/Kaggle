{"cell_type":{"4d1eb3cd":"code","a0ba803c":"code","0660955d":"code","a35b1951":"code","7e51adab":"code","eec45566":"code","b8633063":"code","07c8a485":"code","d4ca8ef9":"code","2c477c5a":"code","5f3a62e0":"code","ad790b86":"code","14e885f9":"code","9c4c41a4":"code","18910686":"code","9f4ef70c":"code","58e35821":"code","f9ba419e":"markdown"},"source":{"4d1eb3cd":"import os\n\nimport numpy as np\nimport pandas as pd\n\nfrom keras import models\nfrom keras import layers\nfrom keras import optimizers","a0ba803c":"def load_data(path='imdb.npz', num_words=None, skip_top=0,\n              maxlen=None, seed=113,\n              start_char=1, oov_char=2, index_from=3, **kwargs):\n\n    with np.load(path) as f:\n        x_train, labels_train = f['x_train'], f['y_train']\n        x_test, labels_test = f['x_test'], f['y_test']\n\n    np.random.seed(seed)\n    indices = np.arange(len(x_train))\n    np.random.shuffle(indices)\n    x_train = x_train[indices]\n    labels_train = labels_train[indices]\n\n    indices = np.arange(len(x_test))\n    np.random.shuffle(indices)\n    x_test = x_test[indices]\n    labels_test = labels_test[indices]\n\n    xs = np.concatenate([x_train, x_test])\n    labels = np.concatenate([labels_train, labels_test])\n\n    if start_char is not None:\n        xs = [[start_char] + [w + index_from for w in x] for x in xs]\n    elif index_from:\n        xs = [[w + index_from for w in x] for x in xs]\n\n    if maxlen:\n        xs, labels = _remove_long_seq(maxlen, xs, labels)\n        if not xs:\n            raise ValueError('After filtering for sequences shorter than maxlen=' +\n                             str(maxlen) + ', no sequence was kept. '\n                             'Increase maxlen.')\n    if not num_words:\n        num_words = max([max(x) for x in xs])\n\n    if oov_char is not None:\n        xs = [[w if (skip_top <= w < num_words) else oov_char for w in x]\n              for x in xs]\n    else:\n        xs = [[w for w in x if skip_top <= w < num_words]\n              for x in xs]\n\n    idx = len(x_train)\n    x_train, y_train = np.array(xs[:idx]), np.array(labels[:idx])\n    x_test, y_test = np.array(xs[idx:]), np.array(labels[idx:])\n\n    return (x_train, y_train), (x_test, y_test)","0660955d":"(train_data, train_labels), (test_data, test_labels) = load_data(path='.\/..\/input\/imdb.npz',num_words=10000)","a35b1951":"print(\"Train data shape \", train_data.shape)\nprint(\"Test data shape\", test_data.shape)\nprint(\"Maximum value of a word index \",max([max(sequence) for sequence in train_data]))\nprint(\"Maximum length num words of review in train \", max([len(sequence) for sequence in train_data]))\nprint(\"Train data sample \", train_data[0][:10],'...')","7e51adab":"def vectorize_sequences(sequences, dimension=10000):\n    results = np.zeros((len(sequences), dimension))        \n    for i, sequence in enumerate(sequences):\n        results[i, sequence] = 1.                          \n    return results\n\nx_train = vectorize_sequences(train_data)                  \nx_test = vectorize_sequences(test_data)","eec45566":"print(\"Train data shape \", x_train.shape)\nprint(\"Test data shape\", x_test.shape)\nprint(\"Vectorized Train data sample \", x_train[0])","b8633063":"y_train = np.asarray(train_labels).astype('float32')\ny_test = np.asarray(test_labels).astype('float32')","07c8a485":"model = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))","d4ca8ef9":"model.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])","2c477c5a":"x_val = x_train[:10000]\npartial_x_train = x_train[10000:]\ny_val = y_train[:10000]\npartial_y_train = y_train[10000:]","5f3a62e0":"history = model.fit(partial_x_train, partial_y_train,\n                    epochs=20, batch_size=512, validation_data=(x_val, y_val))","ad790b86":"import matplotlib.pyplot as plt\n\nhistory_dict = history.history\nloss_values = history_dict['loss']\nval_loss_values = history_dict['val_loss']\nacc = history_dict['acc']\n\nepochs = range(1, len(acc) + 1)\n\nplt.plot(epochs, loss_values, 'bo', label='Training loss')           \nplt.plot(epochs, val_loss_values, 'b', label='Validation loss')      \nplt.title('Training and validation loss')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","14e885f9":"history_dict.keys()","9c4c41a4":"plt.clf()                                      \nacc_values = history_dict['acc']\nval_acc = history_dict['val_acc']\n\nplt.plot(epochs, acc, 'bo', label='Training acc')\nplt.plot(epochs, val_acc, 'b', label='Validation acc')\nplt.title('Training and validation accuracy')\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.legend()\n\nplt.show()","18910686":"model = models.Sequential()\nmodel.add(layers.Dense(16, activation='relu', input_shape=(10000,)))\nmodel.add(layers.Dense(16, activation='relu'))\nmodel.add(layers.Dense(1, activation='sigmoid'))\n\nmodel.compile(optimizer='rmsprop',\n              loss='binary_crossentropy',\n              metrics=['accuracy'])\n\nmodel.fit(x_train, y_train, epochs=4, batch_size=512)\nresults = model.evaluate(x_test, y_test)","9f4ef70c":"results","58e35821":"predictions = model.predict(x_test)","f9ba419e":"**Deep Learning with Python - Chapter 3**"}}