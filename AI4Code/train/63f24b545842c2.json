{"cell_type":{"62de7227":"code","4764b52d":"code","d549cd76":"code","ecac8e9d":"code","f6fb8c84":"code","2376d75d":"code","5612cc86":"code","86e1c4c4":"code","c50235ee":"code","82a24924":"code","f9f0d6cd":"code","88d7787c":"code","d0e1e2ff":"code","d39845ff":"code","9d2829dc":"code","da9ca2fd":"code","9593650c":"code","8383c3f6":"code","1996d6cf":"code","80e7c3e8":"code","5094b8aa":"code","9699d18e":"code","7059e496":"code","2cda3840":"code","5bd9b7d9":"markdown","e72d247b":"markdown","9390bff3":"markdown","bd2531d3":"markdown","1733b5fa":"markdown","0192668c":"markdown","6ebe6f45":"markdown"},"source":{"62de7227":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 5GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","4764b52d":"heart_data = pd.read_csv(\"..\/input\/heart-failure-clinical-data\/heart_failure_clinical_records_dataset.csv\")\nheart_data.describe()","d549cd76":"heart_data.head()","ecac8e9d":"# check for null values\nheart_data.isnull().sum()","f6fb8c84":"# Separate features and target variable\nfeatures = ['age', 'anaemia', 'creatinine_phosphokinase', 'diabetes',\n       'ejection_fraction', 'high_blood_pressure', 'platelets',\n       'serum_creatinine', 'serum_sodium', 'sex', 'smoking', 'time']\n\nx = heart_data[features]\ny = heart_data[\"DEATH_EVENT\"]","2376d75d":"import seaborn as sns\nimport matplotlib.pyplot as plt\n\nplt.rcParams[\"figure.figsize\"] = 12,1\nplt.rcParams[\"font.size\"] = 14\n\n# Non boolean values\nsns.boxplot(x=heart_data[\"creatinine_phosphokinase\"], color=\"grey\")\nplt.show()\nsns.boxplot(x=heart_data[\"ejection_fraction\"], color=\"grey\")\nplt.show()\nsns.boxplot(x=heart_data[\"platelets\"], color=\"grey\")\nplt.show()\nsns.boxplot(x=heart_data['serum_creatinine'], color = 'grey')\nplt.show()\nsns.boxplot(x=heart_data[\"serum_sodium\"], color=\"grey\")\nplt.show()\nsns.boxplot(x=heart_data[\"time\"], color=\"grey\")\nplt.show()\nsns.boxplot(x=heart_data[\"age\"], color=\"grey\")\nplt.show()\n\n# Boolean values: diabetes, sex, smoking, anaemia, high_blood_pressure","5612cc86":"# Remove outliers\nheart_data[\"ejection_fraction\"] = heart_data[heart_data[\"ejection_fraction\"] < 70]","86e1c4c4":"from sklearn.model_selection import train_test_split\n\nx_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.2, random_state=0)","c50235ee":"# Normalize features\nfrom sklearn.preprocessing import StandardScaler, LabelEncoder\n\nscaler = StandardScaler().fit(x_train)\nx_train = scaler.fit_transform(x_train)\nx_test = scaler.fit_transform(x_test)\n\nencoder = LabelEncoder().fit(y)\n\ny_train = encoder.fit_transform(y_train)\ny_test = encoder.fit_transform(y_test)","82a24924":"# Storing the accuracy of all models\nall_model_accuracy = {}","f9f0d6cd":"# Fit model\nfrom sklearn.linear_model import LogisticRegression\n\nmodel = LogisticRegression(random_state=0)\nmodel.fit(x_train, y_train)\n\nfrom sklearn.metrics import accuracy_score, confusion_matrix\n\n# Predict, calculate score\ny_pred = model.predict(x_test)\n\nconf_matrix = confusion_matrix(y_pred, y_test)\naccuracy = model.score(x_test, y_test)\n\nall_model_accuracy[\"LogisticRegression\"] = accuracy\n\nprint(conf_matrix)\nprint(accuracy)","88d7787c":"# KNN model\n\nfrom sklearn.neighbors import KNeighborsClassifier\n\n# For grid search\naccuracy_results = []\n\nneighbor_range = range(3, 10)\nfor num_neighbors in neighbor_range:\n    clf = KNeighborsClassifier(n_neighbors=num_neighbors, metric = 'minkowski')\n    clf.fit(x_train, y_train)\n#     y_pred = clf.predict(x_test_scaled)\n    accuracy_results.append(clf.score(x_test, y_test))\n    \nimport matplotlib.pyplot as plt\n\nplt.plot(list(neighbor_range), accuracy_results)\nplt.show()","d0e1e2ff":"# Using num neighbors = 9\n\nknn_model = KNeighborsClassifier(n_neighbors=9, metric=\"minkowski\")\nknn_model.fit(x_train, y_train)\n\nfrom sklearn.metrics import confusion_matrix, accuracy_score\n\ny_pred = knn_model.predict(x_test)\n\nconf_matrix = confusion_matrix(y_test, y_pred)\nprint(conf_matrix)\n\naccuracy = accuracy_score(y_test, y_pred)\nall_model_accuracy[\"KNN\"] = accuracy\nprint(accuracy)","d39845ff":"from sklearn.tree import DecisionTreeClassifier\n\ndtree_model = DecisionTreeClassifier(random_state=0, criterion=\"entropy\")\n\ndtree_model.fit(x_train, y_train)\n\ny_predict = dtree_model.predict(x_test)\n\nconf_matrix = confusion_matrix(y_pred, y_test)\naccuracy = dtree_model.score(x_test, y_test)\n\nall_model_accuracy[\"DecisionTree\"] = accuracy\n\nprint(conf_matrix)\nprint(accuracy)","9d2829dc":"from sklearn.ensemble import RandomForestClassifier\n\naccuracy_results = []\nnum_trees = range(100, 200)\n\nfor num_tree in num_trees:\n    rf_model = RandomForestClassifier(random_state=0, n_estimators=num_tree, criterion='entropy')\n    rf_model.fit(x_train, y_train)\n    \n    accuracy_results.append(rf_model.score(x_test, y_test))\n    \nimport matplotlib.pyplot as plt\n\nplt.plot(list(num_trees), accuracy_results)\nplt.show()","da9ca2fd":"# Using 110 trees\nrf_model = RandomForestClassifier(n_estimators=110, random_state=0, criterion='entropy')\nrf_model.fit(x_train, y_train)\n\ny_pred = rf_model.predict(x_test)\naccuracy = accuracy_score(y_test, y_pred)\n\nall_model_accuracy[\"RandomForest\"] = accuracy\nprint(confusion_matrix(y_test, y_pred))\nprint(accuracy)","9593650c":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras import layers\n\nann_model = keras.Sequential()\n\n# Add 4 layers\nann_model.add(layers.Dense(units=10, activation='relu'))\nann_model.add(layers.Dense(units=10, activation='relu'))\nann_model.add(layers.Dense(units=10, activation='relu'))\nann_model.add(layers.Dense(units=10, activation='relu'))\n\n# Add output layer\nann_model.add(layers.Dense(units=1, activation='sigmoid'))\n\n# Build\nann_model.compile(optimizer = 'adam', loss = 'binary_crossentropy' , metrics = ['accuracy'] )\n\n# Train\nann_model.fit(x_train, y_train, batch_size = 32, epochs = 100)","8383c3f6":"# Summary of model\nann_model.summary()","1996d6cf":"# Predicting\ny_predict = ann_model.predict(x_test)\ny_predict = (y_predict > 0.5) # Convert to boolean\n# y_predict","80e7c3e8":"accuracy = accuracy_score(y_test, y_predict)\n\nall_model_accuracy[\"NeuralNetwork\"] = accuracy\n\nprint(confusion_matrix(y_predict, y_test))\nprint(accuracy_score(y_predict, y_test))","5094b8aa":"from xgboost import XGBClassifier\n\naccuracy_results = []\nnum_estimators = range(10, 100, 10)\n\nfor num_estimator in num_estimators:\n    model = XGBClassifier(n_estimators=num_estimator)\n    model.fit(x_train, y_train)\n    \n    accuracy_results.append(model.score(x_test, y_test))\n    \nimport matplotlib.pyplot as plt\n\nplt.plot(list(num_estimators), accuracy_results)\nplt.show()","9699d18e":"model = XGBClassifier(n_estimators=80)\nmodel.fit(x_train, y_train)\n\ny_pred = model.predict(x_test)\naccuracy = accuracy_score(y_pred, y_test)\n\nall_model_accuracy[\"XGBoost\"] = accuracy\n\nprint(confusion_matrix(y_pred, y_test))\nprint(accuracy)","7059e496":"all_model_accuracy","2cda3840":"import matplotlib.pylab as plt\n\nfont = {'family' : 'normal',\n        'size'   : 13}\n\nplt.rc('font', **font)\n\nplt.figure(figsize=(15, 5))\n\nticks = range(1,7)\ntick_label = list(all_model_accuracy.keys())\nheight = list(all_model_accuracy.values())\n\nplt.bar(ticks, height, tick_label=tick_label)\nplt.ylabel(\"Accuracy\")\nplt.xlabel(\"Classifier models\")\nplt.show()\n\n","5bd9b7d9":"## Plotting all accuracy results","e72d247b":"## Neural network","9390bff3":"## Decision Tree","bd2531d3":"## Random forest","1733b5fa":"## XGBoost Classifier","0192668c":"## K nearest neighbors\n","6ebe6f45":"## Logistic regresssion"}}