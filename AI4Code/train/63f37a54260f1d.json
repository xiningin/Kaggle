{"cell_type":{"75cfc484":"code","fbe44a26":"code","58bb4a63":"code","694d911c":"code","7da33fe5":"code","bf0b0607":"code","a7bb52d6":"code","fac1ba80":"markdown","11f88be1":"markdown","f8c5ff39":"markdown","37d833e4":"markdown","d1b3f304":"markdown","bd6ec379":"markdown","1dc39e52":"markdown","1ad97a4a":"markdown"},"source":{"75cfc484":"import pandas as pd\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.naive_bayes import GaussianNB\n\nfrom lwoku import RANDOM_STATE, N_JOBS, VERBOSE, get_prediction\nfrom grid_search_utils import plot_grid_search, table_grid_search\n\nimport pickle","fbe44a26":"# Read training and test files\nX_train = pd.read_csv('..\/input\/learn-together\/train.csv', index_col='Id', engine='python')\nX_test = pd.read_csv('..\/input\/learn-together\/test.csv', index_col='Id', engine='python')\n\n# Define the dependent variable\ny_train = X_train['Cover_Type'].copy()\n\n# Define a training set\nX_train = X_train.drop(['Cover_Type'], axis='columns')","58bb4a63":"gnb_clf = GaussianNB()","694d911c":"parameters = {\n    'var_smoothing': [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11, 1e-12, 1e-13, 1e-14, 1e-15]\n}\nclf = GridSearchCV(gnb_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","7da33fe5":"parameters = {\n    'var_smoothing': [1e-2, 1e-3, 1e-4, 1e-5, 1e-6, 1e-7, 1e-8, 1e-9, 1e-10, 1e-11, 1e-12, 1e-13, 1e-14, 1e-15]\n}\nclf = GridSearchCV(gnb_clf, parameters, cv=5, verbose=VERBOSE, n_jobs=N_JOBS)\nclf.fit(X_train, y_train)\nplot_grid_search(clf)\ntable_grid_search(clf)","bf0b0607":"with open('clf.pickle', 'wb') as fp:\n    pickle.dump(clf, fp)","a7bb52d6":"clf.best_estimator_","fac1ba80":"The value of var_smoothing that maximizes the score is '1e-07'.","11f88be1":"# Prepare data","f8c5ff39":"## Export grid search results","37d833e4":"# var_smoothing\n##### : float, optional (default=1e-9)\n\nPortion of the largest variance of all features that is added to\nvariances for calculation stability.","d1b3f304":"# Search over parameters","bd6ec379":"# Introduction\n\nThe aim of this notebook is to optimize the Logistic Regression model.\n\nFirst, all [Gaussian Naive Bayes](https:\/\/scikit-learn.org\/stable\/modules\/generated\/sklearn.naive_bayes.GaussianNB.html#sklearn.naive_bayes.GaussianNB) parameters are analysed separately.\n\nThen, a grid search is carried out.\nThis is a search through all the combinations of parameters,\nwhich optimize the internal score in the train set.\n\nThe results are collected at [Tactic 03. Hyperparameter optimization](https:\/\/www.kaggle.com\/juanmah\/tactic-03-hyperparameter-optimization).","1dc39e52":"# Exhaustive search","1ad97a4a":"# priors\n##### : array-like, shape (n_classes,)\n\nPrior probabilities of the classes. If specified the priors are not\nadjusted according to the data.\n\n**Note**: Not evaluated"}}