{"cell_type":{"8c5d2e83":"code","ed9b5d24":"code","9f0be42c":"code","b63b7a60":"code","30445c89":"code","62660e1b":"code","dbff11a8":"markdown","7b7193a8":"markdown","d908b93a":"markdown","cb6c1194":"markdown","8e0272d0":"markdown","a12b082b":"markdown","1bce3b6a":"markdown"},"source":{"8c5d2e83":"import numpy as np\nimport pandas as pd\nimport tensorflow as tf\nimport csv\nimport re","ed9b5d24":"def convert_test_attributes_to_onehot(df, value_list):\n    # Return a one-hit encoded numpy array for test data keyword and location (because that is easier)\n    one_hots = np.zeros((len(df), len(value_list)))\n    \n    for index, row in df.iterrows():\n        if row['keyword'] != '':\n            try:\n                one_hots[index, value_list.index(row['keyword'])] = 1\n            except ValueError:\n#                print('could not find keyword: [' + str(row['keyword']) + '] at index [' + str(index) + ']')\n                continue\n\n        if row['keyword'] != '':\n            try:\n                one_hots[index, value_list.index(row['location'])] = 1\n            except ValueError:\n#                print('could not find location: [' + str(row['keyword']) + '] at index [' + str(index) + ']')\n                continue\n\n    return one_hots\n    \n\ndef convert_attributes_to_onehot(df):\n    # Return a one-hit encoded dataframe for keyword and location\n    one_hot_keywords = pd.get_dummies(train_data['keyword'])\n    one_hot_locations = pd.get_dummies(train_data['location'])\n\n    return pd.concat([one_hot_keywords, one_hot_locations.reindex(one_hot_keywords.index)], axis=1)\n          \n    \ndef vectorize_tweets(dataframe, df_glove):\n    # First create a blank matrix for our word vectors\n    vectorized_data = np.zeros((len(dataframe), MAX_TWEET_LENGTH, VECTORS_PER_WORD))\n    \n    # Now loop thorugh the words and add the corresponding vectors to the matrix\n    for index, row in dataframe.iterrows():\n        i = 0\n        # Remove special characters and convert text to lower case, then split on space\n        for word in re.sub(r'\\W+', ' ', row['text'].lower()).split(' '):\n            try:\n                vectorized_data[index, i, :] = df_glove.loc[word]\n            except KeyError:\n                # print(word + ' not found')\n                continue\n                \n            i += 1\n            \n        # Print a message for every 100 texts processed - just to check if we are still alive\n        if index % 100 == 99:\n            print('Processing text number ' + str(index + 1) + ' of ' + str(len(dataframe)))\n        \n    return vectorized_data","9f0be42c":"def build_model(lstm_shape, dense_shape):\n    dropout = 0.8\n    \n    # First part of the network for keyword and location\n    dense_input = tf.keras.layers.Input(shape=(dense_shape))\n    dense1 = tf.keras.layers.Dense(50)(dense_input)\n    \n    # Second part of the network for the text analysis\n    lstm_input = tf.keras.layers.Input(shape=(lstm_shape))\n    lstm1 = tf.keras.layers.GaussianNoise(0.075)(lstm_input)\n    lstm2 = tf.keras.layers.LSTM(units=500, return_sequences=True)(lstm1)\n    lstm3 = tf.keras.layers.Dropout(dropout)(lstm2)\n    lstm4 = tf.keras.layers.LSTM(units=100, return_sequences=True)(lstm3)\n    lstm5 = tf.keras.layers.Flatten()(lstm4)\n    lstm6 = tf.keras.layers.Dense(20)(lstm5)\n    \n    # Concatenate part for concatenating the two parts above\n    concat1 = tf.keras.layers.Concatenate()([dense1, lstm5])\n    concat2 = tf.keras.layers.Dropout(dropout)(concat1)\n    concat3 = tf.keras.layers.Dense(80)(concat2)\n    \n    # Output of model\n    outputs = tf.keras.layers.Dense(1, activation='sigmoid')(concat3)\n    \n    # Compile the model\n    model = tf.keras.Model(inputs=[dense_input, lstm_input], outputs=outputs)\n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.00005, beta_1=0.9, beta_2=0.998,),\n              loss=tf.keras.losses.BinaryCrossentropy(),\n              metrics=['acc'])\n    \n    return model","b63b7a60":"MAX_TWEET_LENGTH = 280 # Tweets can be a maximum of 280 characters\nVECTORS_PER_WORD = 50  # The number of individual vector values that represents a word\nBATCH_SIZE = 256\nNUM_EPOCHS = 40","30445c89":"# Start out by loading the train and test files into Pandas\nprint('Loading test and training data')\ntest_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/test.csv')\ntrain_data = pd.read_csv('\/kaggle\/input\/nlp-getting-started\/train.csv')\n\n# Next load the glove file \nprint('Loading GloVe file')\nglove_data = pd.read_csv('\/kaggle\/input\/glove50d\/glove.6B.50d.txt', sep=' ', index_col=0, header = None, quoting=csv.QUOTE_NONE, encoding='utf-8')\n\n# Let's have a look at the data, just to make sure it looks correct\nglove_data.head()\n\n# Convert keyword and location to one-hot vectors for the dense part of the network\ndf_one_hot_attributes = convert_attributes_to_onehot(train_data)\ndf_test_one_hot_attributes = convert_test_attributes_to_onehot(test_data, df_one_hot_attributes.columns.to_list())\ndf_one_hot_attributes.head()\n\n# Now, vectorize the test and training data so that it can be understood by our model\nvectorized_training_data = vectorize_tweets(train_data, glove_data)\nvectorized_testing_data = vectorize_tweets(test_data, glove_data)","62660e1b":"# Build the model, show a summary and fit the model\nmodel = build_model([MAX_TWEET_LENGTH, VECTORS_PER_WORD], len(df_one_hot_attributes.columns))\nmodel.summary()\n\n# Save the epoch with the lowest validation loss\ncheckpoint_save = tf.keras.callbacks.ModelCheckpoint('saved_model.h5', save_best_only=True, monitor='val_acc', mode='min')\n\n# Use this to train on all training data - no validation\nmodel.fit(x=[df_one_hot_attributes, vectorized_training_data], y=train_data['target'], batch_size=BATCH_SIZE, epochs=NUM_EPOCHS, verbose=1, shuffle=True)\n# Use this to split 0.9:0.1 between train and validation\n#model.fit(x=[df_one_hot_attributes, vectorized_training_data], y=train_data['target'], batch_size=BATCH_SIZE,\n#          epochs=NUM_EPOCHS, verbose=1, shuffle=True, validation_split=0.1, callbacks=[checkpoint_save])\n\n# Reload the best model for predictions\n#model = tf.keras.models.load_model('saved_model.h5')\n\n# Create the submission file\nprint('Training complete. Commencing creation of submission file')\ndf_submission = pd.DataFrame()\ndf_submission['id'] = test_data['id']\n\npredictions = model.predict([df_test_one_hot_attributes, vectorized_testing_data])\nprediction_list = []\nfor prediction in predictions:\n    if prediction < 0.5:\n        prediction_list += [0]\n    else:\n        prediction_list += [1]\n        \ndf_submission['target'] = prediction_list\n\ndf_submission.head(5)\ndf_submission.to_csv('submission.csv', index=False)\n\nprint('All done.')","dbff11a8":"**Import libraries needed**","7b7193a8":"**Load the data and convert it to something that can be represented with decimal numbers**\nThis takes a while...","d908b93a":"**Build the model, fit with data, make predictions and save to submit file**","cb6c1194":"# Simple tf.keras model\nWord embedding, LSTM, concatenation\n\n**GloVe weights info here: https:\/\/nlp.stanford.edu\/projects\/glove\/**\n\nScore approximately 0.8 in current version - can easily be improved by playing around with the model...","8e0272d0":"**Constants needed**","a12b082b":"**Define function for creating the neural network**","1bce3b6a":"**Define functions for converting the attributes and text to decimal numbers**"}}