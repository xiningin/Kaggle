{"cell_type":{"de610561":"code","a390169c":"code","bfd1b1ba":"code","594403a5":"code","9e7a1c5c":"code","2454c632":"code","2dedf326":"code","87b8e0d1":"code","a46a1029":"code","369b8fea":"code","3e6cb20b":"code","973216f1":"code","6362f0e8":"code","8894def5":"code","c478ce75":"code","9b6ada08":"code","8eca6043":"code","ff63ef66":"code","374aef64":"code","fec04202":"code","8496055c":"code","318d9de2":"code","267e6b72":"code","6cb43ee2":"code","1a979bb9":"code","d0b57c41":"code","ddca68d8":"code","2355d55d":"code","151e67b0":"code","cd6c263c":"code","3edf586d":"code","e8669951":"code","e8758210":"code","089b0b6e":"code","19e86f10":"code","27d60e91":"code","5ba85d5a":"code","d394f8b0":"code","8bfb6b4d":"code","49cd5580":"code","4a5f39df":"code","b0a79e3f":"code","7b731473":"code","736d5091":"code","cc2fa8f6":"code","774ea941":"code","cd2fb3b4":"code","1219ca64":"code","1b8f0ca8":"code","a635c460":"markdown","6d70d0e3":"markdown","f9e2704f":"markdown","3f02c08b":"markdown","836c2cb4":"markdown","348dd421":"markdown","073df819":"markdown","80c4deb9":"markdown","e9d37408":"markdown","120cc6b8":"markdown","fc6b76ad":"markdown","179b1052":"markdown","302304a2":"markdown","77592545":"markdown","4f536511":"markdown","9b3a6411":"markdown","aece29e7":"markdown","70a9bb72":"markdown","82c839c8":"markdown","1bffda5c":"markdown","abc6bea5":"markdown","345b0d92":"markdown","21a60287":"markdown","c769a18f":"markdown","705f9cab":"markdown","eaca0798":"markdown","fdf7fab4":"markdown","c20625f3":"markdown","3729abfb":"markdown","a12aab29":"markdown","d4fe5d8b":"markdown","b40a23c5":"markdown","3198b944":"markdown","d59ed450":"markdown","7da48890":"markdown","194eb952":"markdown","ceb03aef":"markdown","d3eefaac":"markdown","ed60e2be":"markdown","998ef779":"markdown","86633d25":"markdown","296dab37":"markdown","7a586c9b":"markdown","52875224":"markdown","aa53a3ac":"markdown","b05a1dff":"markdown","9872b74a":"markdown","326a29c1":"markdown","44470801":"markdown","98f81271":"markdown"},"source":{"de610561":"import matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport tensorflow as tf\nimport keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Sequential\nimport pickle\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nimport shutil\nimport random","a390169c":"with open(\"..\/input\/datacleaningglassesnoglasses\/glasses.txt\", \"rb\") as fp:\n    glasses = pickle.load(fp)\nplt.figure(figsize=(12, 12))\nran_num = []\nfor i in range(0,9):\n    n = random.randint(0,len(glasses))\n    ran_num.append(n)\nfor i in range(9):\n    ax= plt.subplot(3, 3, i + 1)\n    plt.imshow(mpimg.imread(glasses[ran_num[i]]))\n    plt.title(\"glasses\")\n    plt.axis(\"off\")","bfd1b1ba":"with open(\"..\/input\/datacleaningglassesnoglasses\/no_glasses.txt\", \"rb\") as fp: \n    no_glasses = pickle.load(fp)\nplt.figure(figsize=(12, 12))\nran_num = []\nfor i in range(0,9):\n    n = random.randint(0,len(no_glasses))\n    ran_num.append(n)\nfor i in range(9):\n    ax= plt.subplot(3, 3, i + 1)\n    plt.imshow(mpimg.imread(no_glasses[ran_num[i]]))\n    plt.title(\"no_glasses\")\n    plt.axis(\"off\")","594403a5":"with open(\"..\/input\/datacleaningglassesnoglasses\/no_clear.txt\", \"rb\") as fp: \n    no_clear = pickle.load(fp)\nplt.figure(figsize=(12, 12))\nran_num = []\nfor i in range(0,9):\n    n = random.randint(0,len(no_clear))\n    ran_num.append(n)\nfor i in range(9):\n    ax= plt.subplot(3, 3, i + 1)\n    plt.imshow(mpimg.imread(no_clear[ran_num[i]]))\n    plt.title(\"no_clear\")\n    plt.axis(\"off\")","9e7a1c5c":"print(\"The length of the different groups:\" + \"-Glasses: \" + str(len(glasses)) + \" -No glasses: \" + str(len(no_glasses)) + \" -No clear: \" + str(len(no_clear)))","2454c632":"tf.random.set_seed(123456)","2dedf326":"BATCH_SIZE = 32\nIMG_SIZE = (160, 160)","87b8e0d1":"all_images= glasses + no_glasses","a46a1029":"data_dir= \"\/kaggle\/input\/datacleaningglassesnoglasses\/Images\/Images\/\"","369b8fea":"train_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    validation_split=0.3,\n    subset=\"training\",\n    shuffle=True,\n    seed=123456,\n    image_size= IMG_SIZE,\n    batch_size=BATCH_SIZE)","3e6cb20b":"validation_dataset = tf.keras.preprocessing.image_dataset_from_directory(\n    data_dir,\n    validation_split=0.3,\n    subset=\"validation\",\n    shuffle=True,\n    seed=123456,\n    image_size= IMG_SIZE,\n    batch_size=BATCH_SIZE)","973216f1":"class_names = train_dataset.class_names\nprint(class_names)","6362f0e8":"class_names = train_dataset.class_names\n\nplt.figure(figsize=(12, 12))\nfor images, labels in train_dataset.take(1):\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        plt.imshow(images[i].numpy().astype(\"uint8\"))\n        plt.title(class_names[labels[i]])\n        plt.axis(\"off\")","8894def5":"val_batches = tf.data.experimental.cardinality(validation_dataset)\ntest_dataset = validation_dataset.take(val_batches \/\/ 5)\nvalidation_dataset = validation_dataset.skip(val_batches \/\/ 5)","c478ce75":"print('Number of training batches: %d' % tf.data.experimental.cardinality(train_dataset))\nprint('Number of validation batches: %d' % tf.data.experimental.cardinality(validation_dataset))\nprint('Number of test batches: %d' % tf.data.experimental.cardinality(test_dataset))","9b6ada08":"AUTOTUNE = tf.data.AUTOTUNE\n\ntrain_dataset = train_dataset.prefetch(buffer_size=AUTOTUNE)\nvalidation_dataset = validation_dataset.prefetch(buffer_size=AUTOTUNE)\ntest_dataset = test_dataset.prefetch(buffer_size=AUTOTUNE)","8eca6043":"data_augmentation = tf.keras.Sequential([\n  tf.keras.layers.experimental.preprocessing.RandomFlip('horizontal'),\n  tf.keras.layers.experimental.preprocessing.RandomRotation(0.2),\n])","ff63ef66":"for image, _ in train_dataset.take(1):\n    plt.figure(figsize=(12, 12))\n    first_image = image[0]\n    for i in range(9):\n        ax = plt.subplot(3, 3, i + 1)\n        augmented_image = data_augmentation(tf.expand_dims(first_image, 0))\n        plt.imshow(augmented_image[0] \/ 255)\n        plt.axis('off')","374aef64":"preprocess_input = tf.keras.applications.mobilenet_v2.preprocess_input","fec04202":"rescale = tf.keras.layers.experimental.preprocessing.Rescaling(1.\/127.5, offset= -1)","8496055c":"IMG_SHAPE = IMG_SIZE + (3,)\nbase_model = tf.keras.applications.MobileNetV2(input_shape=IMG_SHAPE,\n                                               include_top=False,\n                                               weights='imagenet')","318d9de2":"image_batch, label_batch = next(iter(train_dataset))\nfeature_batch = base_model(image_batch)\nprint(feature_batch.shape)","267e6b72":"base_model.trainable = False","6cb43ee2":"global_average_layer = tf.keras.layers.GlobalAveragePooling2D()\nfeature_batch_average = global_average_layer(feature_batch)\nprint(feature_batch_average.shape)","1a979bb9":"prediction_layer = tf.keras.layers.Dense(1)\nprediction_batch = prediction_layer(feature_batch_average)\nprint(prediction_batch.shape)","d0b57c41":"inputs = tf.keras.Input(shape=(160, 160, 3))\nx = data_augmentation(inputs)\nx = preprocess_input(x)\nx = base_model(x, training=False)\nx = global_average_layer(x)\nx = tf.keras.layers.Dropout(0.2)(x)\noutputs = prediction_layer(x)\nmodel = tf.keras.Model(inputs, outputs)","ddca68d8":"model.summary()","2355d55d":"base_learning_rate = 0.0001\nmodel.compile(optimizer=tf.keras.optimizers.Adam(lr=base_learning_rate),\n              loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              metrics=['accuracy'])","151e67b0":"len(model.trainable_variables)","cd6c263c":"initial_epochs = 10\nloss0, accuracy0 = model.evaluate(validation_dataset)","3edf586d":"print(\"initial loss: {:.2f}\".format(loss0))\nprint(\"initial accuracy: {:.2f}\".format(accuracy0))","e8669951":"model_fit = model.fit(train_dataset,\n                    epochs= initial_epochs,\n                    validation_data= validation_dataset)","e8758210":"acc = model_fit.history['accuracy']\nval_acc = model_fit.history['val_accuracy']\nloss_ = model_fit.history['loss']\nval_loss_ = model_fit.history['val_loss']","089b0b6e":"## 3.8) Results","19e86f10":"plt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.ylim([0.8, 1])\nplt.plot([initial_epochs-1,initial_epochs-1],\n          plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss_, label='Training Loss')\nplt.plot(val_loss_, label='Validation Loss')\nplt.ylim([0, 1.0])\nplt.plot([initial_epochs-1,initial_epochs-1],\n         plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","27d60e91":"loss, accuracy = model.evaluate(test_dataset)\nloss, accuracy1 = model.evaluate(train_dataset)\nprint('Test accuracy :', accuracy)\nprint('Train accuracy :', accuracy1)","5ba85d5a":"base_model.trainable = True","d394f8b0":"print(\"Number of layers in the base model: \", len(base_model.layers))\nfine_tune_at = 100\nfor layer in base_model.layers[:fine_tune_at]:\n    layer.trainable =  False","8bfb6b4d":"model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n              optimizer = tf.keras.optimizers.RMSprop(lr=base_learning_rate\/10),\n              metrics=['accuracy'])","49cd5580":"len(model.trainable_variables)","4a5f39df":"fine_tune_epochs = 5\ntotal_epochs =  initial_epochs + fine_tune_epochs\n\nmodel_fit_fine = model.fit(train_dataset,\n                         epochs= total_epochs,\n                         initial_epoch= model_fit.epoch[-1],\n                         validation_data= validation_dataset)","b0a79e3f":"acc += model_fit_fine.history['accuracy']\nval_acc += model_fit_fine.history['val_accuracy']\nloss_ += model_fit_fine.history['loss']\nval_loss_ += model_fit_fine.history['val_loss']","7b731473":"plt.figure(figsize=(8, 8))\nplt.subplot(2, 1, 1)\nplt.plot(acc, label='Training Accuracy')\nplt.plot(val_acc, label='Validation Accuracy')\nplt.ylim([0.8, 1])\nplt.plot([initial_epochs-1,initial_epochs-1],\n          plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='lower right')\nplt.title('Training and Validation Accuracy')\n\nplt.subplot(2, 1, 2)\nplt.plot(loss_, label='Training Loss')\nplt.plot(val_loss_, label='Validation Loss')\nplt.ylim([0, 1.0])\nplt.plot([initial_epochs-1,initial_epochs-1],\n         plt.ylim(), label='Start Fine Tuning')\nplt.legend(loc='upper right')\nplt.title('Training and Validation Loss')\nplt.xlabel('epoch')\nplt.show()","736d5091":"loss, accuracy = model.evaluate(test_dataset)\nloss, accuracy1 = model.evaluate(train_dataset)\nloss, accuracy2 = model.evaluate(validation_dataset)\nprint('Test accuracy :', accuracy)\nprint('Train accuracy :', accuracy1)\nprint('Validation accuracy :', accuracy2)","cc2fa8f6":"image_batch, label_batch = test_dataset.as_numpy_iterator().next()\npredictions = model.predict_on_batch(image_batch).flatten()\n\nprint('Raw Predictions:\\n', predictions)\n\npredictions = tf.nn.sigmoid(predictions)\n\nprint('Raw Predictions 2:\\n', predictions)\n\npredictions = tf.where(predictions < 0.5, 0, 1)\n\nprint('Predictions:\\n', predictions.numpy())\nprint('Labels:\\n', label_batch)\n\nplt.figure(figsize=(12, 12))\nfor i in range(9):\n    ax = plt.subplot(3, 3, i + 1)\n    plt.imshow(image_batch[i].astype(\"uint8\"))\n    plt.title(class_names[predictions[i]])\n    plt.axis(\"off\")","774ea941":"model_dir = \"\/kaggle\/working\/model\"\ntf.saved_model.save(model, model_dir)","cd2fb3b4":"loaded = tf.saved_model.load(model_dir)\nprint(list(loaded.signatures.keys()))","1219ca64":"with open('model.tflite', 'wb') as f:\n    converter = tf.lite.TFLiteConverter.from_saved_model(model_dir)\n    converter.optimizations = [tf.lite.Optimize.DEFAULT]\n    tflite_model = converter.convert()\n    f.write(tflite_model)\n\nwith open('model-full.tflite', 'wb') as f:\n    converter = tf.lite.TFLiteConverter.from_saved_model(model_dir)\n    tflite_model = converter.convert()\n    f.write(tflite_model)","1b8f0ca8":"# Load the TFLite model and allocate tensors.\ninterpreter = tf.lite.Interpreter(model_path=\"model.tflite\")\ninterpreter.allocate_tensors()\n\n# Get input and output tensors.\ninput_details = interpreter.get_input_details()\nprint(f\"Input details: {input_details}\")\noutput_details = interpreter.get_output_details()\n\ninput_shape = input_details[0]['shape']\n\n# Test the model on input data.\nfor images, labels in train_dataset.take(1):\n    for i in range(9):\n        print(f\"images[i].numpy()={images[i].numpy()}\")\n        #input_data = np.array(np.random.random_sample(input_shape), dtype=np.float32)\n        input_data = [images[i].numpy().astype(\"float32\")]\n        print(f\"input_data={input_data}\")\n        interpreter.set_tensor(input_details[0]['index'], input_data)\n\n        interpreter.invoke()\n\n        # The function `get_tensor()` returns a copy of the tensor data.\n        # Use `tensor()` in order to get a pointer to the tensor.\n        output_data = interpreter.get_tensor(output_details[0]['index'])\n        print(output_data)","a635c460":"### The data is created `artificially`, hence, in some photos it was not clear wheter the photos belong to the class `glasses` or `no_glasses`","6d70d0e3":"### This feature extractor converts the images from `160x160x3` to `5x5x1280`","f9e2704f":"## 3.4) Feature extraction","3f02c08b":"## In this project a `classification model` will be created in order to classify images of people with glasses and no glasses","836c2cb4":"### As it can be seen in the graphs the `accuracy` along the `10 epochs` was `really similar` for the `validation` and `training` samples\n### However, it seems that for some epochs the model works slighty better in the `validation sample` than in the `training sample`. This probably happened due to the application of some layers like `tf.keras.layers.BatchNormalization` and `tf.keras.layers.Dropout`, which are applied during the `training process`","348dd421":"### The base model `MobileNet V2` was developed by Google\n### It contains a large dataset with `1.4 million of images` and `1000 classes`\n### It is important to include `include_top=False` because the classification layers previously created should be included","073df819":"## 3.1) Data augmentation\u00b6","80c4deb9":"### As it can be seen in the `training` and `test`samples the accuracy is really high, greater than 0.99, hence this `model is robust` and `works really well`","e9d37408":"# 6) Convert to TFLite","120cc6b8":"### The layer `tf.keras.layers.GlobalAveragePooling2D` is going to be used, in order to convert the features in a `1280-element vector`, per each image","fc6b76ad":"# 5) Prediction","179b1052":"### This model is a continuation of the `Model 1`, however, a `fine tunning`process will be applied in order to `increase the performance`\n### During the previos process the `weights of the pre-trained network` were `not updated` during the training.\n### However, it is possible to increase the performance applying these `weights`","302304a2":"## It seems the train_dataset contains 4920 images, and the validation_datset 1476, out of 4920 files","77592545":"## 3.6) Compilation of the model:","4f536511":"### This model expects `pixel values between -1 and 1`, hence the images should be `preprocessed`","9b3a6411":"### In this case the `Model 2 with fine tuning` will be applied, as the performance of the same was better. The prediction is going to be done in the `test sample`","aece29e7":"## No clear:","70a9bb72":"## 4.4) Results","82c839c8":"### Firstly, the application `MobileNetV2` will be downloaded, which is going to be used as a base for the model. This is a way of performing transfer learning, which consists in using a training learning from a pre-trained network","1bffda5c":"## The next model is based on Tensorflow\/Learn\/Tutorials\/Images\/Transfer learning and fine-tuning - https:\/\/www.tensorflow.org\/tutorials","abc6bea5":"## Firstly, all the images were changed from `png format to jpg format`, however, the updated folder `datacleaningglassesnoglasses` contains already this mentioned change","345b0d92":"### `tf.keras.layers.Dense` is a layer that converts the features into a `single prediction`","21a60287":"## The Dataset.prefetch() function used in the three splits, overlaps data preprocessing and model execution during the training process","c769a18f":"# 2) Data processing:","705f9cab":"## Glasess:","eaca0798":"## This configuration allows the model to speed up the training process\u00b6","fdf7fab4":"## 3.3) Creating base model from MobileNet V2","c20625f3":"### Data augmentation is an optional step which introduces several artificial observations to the training sample.\n### In this model we are going to introduce two data augmentations:\u00b6","3729abfb":"## 4.3) Training the model","a12aab29":"### It seems the data is `imbalance` there are `much more glasses images than no glasses images`. On the other hand, `77 images` are not going to be used in the model because are `not clear`","d4fe5d8b":"# 1) Data cleaning:","b40a23c5":"# 4) Model with fine tunning","3198b944":"## The `random seed` is going to be applied, it will be used along all the model. In this way we will be sure that the same random seed is applied when it is optional to be called","d59ed450":"# 7) Debug","7da48890":"### As per the results, it looks like that the `model classified perfectly the images`, it can be seen that the array of `Predictions` is the same as `Labels`","194eb952":"## After selecting the parameters, we are ready to split the data, in this case it will be separated as per below:\n- 70% Training data\n- 30% Validation data","ceb03aef":"### There are `two variable objects`. Divided between around `2.5 million of MobilNet` parameters which are `frozen`, and `1.2 thousend` of trainable parameter in the `Dense layer`","d3eefaac":"## Currently we have the `train_dataset` and `validation_dataset` created, however it is important to have also a `small split to test the model`, test_dataset. This will be `20% of the validation_dataset`, this means, more or less `6%`","ed60e2be":"## Below we can see `some images from the train_dataset`, as we can observe there are from the two desired classes:","998ef779":"## 4.2) Compile the model","86633d25":"### Below we can find the results of the data augmentation:","296dab37":"### It is time to apply the previous process to the model:\n- `Data augmentation`\n- `Rescaling`\n- `Basel model`\n- `Feature extractor`","7a586c9b":"## 3.7) Training the model","52875224":"## 3.5) Adding classification head","aa53a3ac":"# 3) Preparing base model:","b05a1dff":"## 3.2) Rescale pixel values:\u00b6","9872b74a":"## No glasses:","326a29c1":"### First of all, we should `freeze the convolutional base` created from the previous step, because it is going to be used as a `feature extractor`","44470801":"## 4.1) Unfreeze the top layers of the model","98f81271":"## To this end several parameteres are going to be selected, firstly the `batch size equal to 32`, and the `image size (height and width) equal to 160`"}}