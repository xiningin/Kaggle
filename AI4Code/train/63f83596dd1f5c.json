{"cell_type":{"9e1e0579":"code","e14fe4e4":"code","ff5088c4":"code","86841c7a":"code","7decb48f":"code","7c99a2d7":"code","193b587d":"code","f1398ab6":"code","c5df239b":"code","38d5a8b5":"code","4d6c926a":"code","78412e36":"code","efdbde2b":"code","13843d20":"code","f1d9c7e8":"code","a8c27c0d":"code","af8b60e7":"code","242ad1a0":"code","2ba42462":"markdown","fd069039":"markdown","a126a6aa":"markdown","a20259da":"markdown","e463d46f":"markdown","1988d3bb":"markdown","14353f3f":"markdown","0e94b18e":"markdown","256bff8c":"markdown","873a1f57":"markdown","902547c5":"markdown","f2d3b44d":"markdown","00e278d3":"markdown","1dc49c0e":"markdown","442d64bd":"markdown","c815846c":"markdown","f166ffb4":"markdown","71f0fa7e":"markdown","6bb703f2":"markdown"},"source":{"9e1e0579":"import pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns","e14fe4e4":"datasets = pd.read_csv('..\/input\/datasetknn\/DataSetKNN.csv')","ff5088c4":"datasets.head()","86841c7a":"sns.pairplot(datasets, hue='TARGET CLASS')","7decb48f":"from sklearn.preprocessing import StandardScaler","7c99a2d7":"scaler = StandardScaler()","193b587d":"scaler.fit(datasets.drop('TARGET CLASS', axis=1))","f1398ab6":"scaled_features = scaler.transform(datasets.drop('TARGET CLASS', axis=1))","c5df239b":"datasets_feat = pd.DataFrame(scaled_features, columns = datasets.columns[:-1])\ndatasets_feat.head()","38d5a8b5":"from sklearn.model_selection import train_test_split\nX = datasets_feat\ny = datasets['TARGET CLASS']\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=101)","4d6c926a":"from sklearn.neighbors import KNeighborsClassifier","78412e36":"knn = KNeighborsClassifier(n_neighbors = 1)","efdbde2b":"knn.fit(X_train, y_train)","13843d20":"pred = knn.predict(X_test)","f1d9c7e8":"from sklearn.metrics import classification_report, confusion_matrix\nprint(confusion_matrix(y_test, pred))\nprint(classification_report(y_test, pred))","a8c27c0d":"import numpy as np\nerror_rate = []\nfor i in range(1,40):\n    knn = KNeighborsClassifier(n_neighbors = i)\n    knn.fit(X_train, y_train)\n    pred_i = knn.predict(X_test)\n    error_rate.append(np.mean(pred_i != y_test))","af8b60e7":"plt.figure(figsize=(15,6))\nplt.plot(range(1,40),error_rate,color='blue',linestyle='dashed',marker='o',\nmarkerfacecolor='red', markersize='10')\nplt.xlabel('no. of K')\nplt.ylabel('Error Rate')","242ad1a0":"knn = KNeighborsClassifier(n_neighbors = 31)\nknn.fit(X_train, y_train)\npred = knn.predict(X_test)\nprint(confusion_matrix(y_test, pred))\nprint(classification_report(y_test, pred))","2ba42462":"Step 4: Standardize the Variables Time to standardize the variables. Import StandardScaler from Scikit learn.\n","fd069039":"<b style=\"color:blue\">K-Nearest Neighbors (KNN)<\/b> is one of the simplest algorithms used in Machine Learning for regression and classification problem. KNN algorithms use data and classify new data points based on similarity measures (e.g. distance function). Classification is done by a majority vote to its neighbors.\n\n<img src=\"https:\/\/blog.eduonix.com\/wp-content\/uploads\/2018\/08\/KNN-algorithm.jpg\" alt=\"KNN\" >","a126a6aa":"Step 2: Load Data","a20259da":"Fit this KNN model to the training data.\nknn.fit(X_train, y_train)","e463d46f":"Create a StandardScaler() object called scaler.\n","1988d3bb":"Step 5: Train Test Split Use train_test_split to split your data into a training set and a testing set.\n","14353f3f":"Fit scaler to the features.\n","0e94b18e":"Step 8: Choosing a K Value Let's go ahead and use the elbow method to pick a good K Value! Create a for loop that trains various KNN models with different k values, then keep track of the error_rate for each of these models with a list. Refer to the lecture if you are confused on this step.\n","256bff8c":"Create a confusion matrix and classification report.","873a1f57":"Convert the scaled features to a dataframe and check the head of this dataframe to make sure the scaling worked.\ndf_feat = pd.DataFrame(scaled_features, columns = df.columns[:-1])\ndf_feat.head()","902547c5":"Step 3: Plot using seaborn Since this data is artificial, we'll just do a large pairplot with seaborn.\n","f2d3b44d":"Check the head (first 3) of the dataframe.","00e278d3":"Step 9: Retrain with new K Value Retrain your model with the best K value (up to you to decide what you want) and re-do the classification report and the confusion matrix.\n","1dc49c0e":"Create a KNN model instance with n_neighbors=1\n","442d64bd":"Use the .transform() method to transform the features to a scaled version.\n","c815846c":"Step 7: Predictions and Evaluations Let's evaluate our KNN model! Use the predict method to predict values using your KNN model and X_test.\n","f166ffb4":"Step 6: Using KNN Import KNeighborsClassifier from scikit learn.\n","71f0fa7e":"Now create the following plot using the information from for loop.\n","6bb703f2":"Step 1: Import Libraries #Import pandas,seaborn, and the usual libraries.\n"}}