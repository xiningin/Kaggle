{"cell_type":{"4c3a3948":"code","f7fdee11":"code","814b7f82":"code","c770650c":"code","2ca606ed":"code","7a84cbfe":"code","7ec663dd":"code","13504c9d":"code","a0c93670":"code","65b49532":"code","9cd6eafc":"code","3741ae04":"code","e567cf30":"code","2deefc06":"markdown","e19c034e":"markdown","cd6d5ccf":"markdown","a812939c":"markdown","4c9dc0e7":"markdown","3c02c6b9":"markdown","e70cbd51":"markdown","3e411bc7":"markdown","dfe64a82":"markdown"},"source":{"4c3a3948":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import Lasso, LogisticRegression\nfrom sklearn.feature_selection import SelectFromModel\nfrom sklearn.preprocessing import StandardScaler\n\nfrom sklearn.metrics import roc_auc_score\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n\nimport os\nprint(os.listdir(\"..\/input\"))\n\n# Any results you write to the current directory are saved as output.","f7fdee11":"df = pd.read_csv(\"..\/input\/train.csv\")\ndf.shape","814b7f82":"df.info()","c770650c":"# Let's separate out the data set into train and validation data set\n\nX_train, X_val, y_train, y_val = train_test_split(df.drop(labels=['TARGET'], axis=1), df['TARGET'], test_size=0.25, random_state=0)\n\nX_train.fillna(0, inplace=True)\nX_val.fillna(0, inplace=True)\n\n# shapes\nX_train.shape, X_val.shape","2ca606ed":"# Lets scale the data so that is ready to be used for Lasso regularization\nscaler = StandardScaler()\nscaler.fit(X_train)","7a84cbfe":"sel = SelectFromModel(LogisticRegression(C=1, penalty = 'l1'))\nsel.fit(scaler.transform(X_train), y_train)","7ec663dd":"print('Total features', X_train.shape[1])\nprint('Selected features', sum(sel.get_support()))\nprint('Removed features', np.sum(sel.estimator_.coef_ == 0))","13504c9d":"# let's create a function to build random forests and compare the performance in training and test set:\n\ndef RandomForest(X_train, X_val, y_train, y_val):\n    rf = RandomForestClassifier(n_estimators = 200, random_state = 1, max_depth = 4)\n    rf.fit(X_train, y_train)\n    print(\"Training set\")\n    \n    pred = rf.predict_proba(X_train)\n    print(\"Random forest roc-auc: {}\".format(roc_auc_score(y_train, pred[:,1])))\n    \n    print(\"Validation set\")\n    pred = rf.predict_proba(X_val)\n    print(\"Random forest roc-auc: {}\".format(roc_auc_score(y_val, pred[:,1])))","a0c93670":"# Transforming the training set and test set.\nX_train_lasso = sel.transform(X_train)\nX_val_lasso   = sel.transform(X_val)\n\nRandomForest(X_train_lasso, X_val_lasso, y_train, y_val)","65b49532":"sfm = SelectFromModel(LogisticRegression(C=1, penalty = 'l2'))\nsfm.fit(scaler.transform(X_train), y_train)","9cd6eafc":"print('Total features-->',X_train.shape[1])\nprint('Selected featurs-->',sum(sfm.get_support()))\nprint('Removed featurs-->',np.sum(sfm.estimator_.coef_==0))","3741ae04":"np.sum(np.abs(sfm.estimator_.coef_) > np.abs(sfm.estimator_.coef_).mean())","e567cf30":"# transforming the training set and test set\nX_train_l2 = sfm.transform(X_train)\nX_val_l2   = sfm.transform(X_val)\n\nRandomForest(X_train_l2, X_val_l2, y_train, y_val)","2deefc06":"As we can see, we have used Lasso regularization to remove non-important features from the dataset. One question to ask is how conservative do you want to be when performing regularization?\n\nDo we want to remove more features and potentially not give enough features to the model?","e19c034e":"## Conclusion:\n\nLearnings from this exercise are:\n1. How to implement L1 regularization with Logistic Regression?\n2. How to implement L2 regularization with Logisitc Regression?\n3. How to print the number of selected features via regularization process?","cd6d5ccf":"### Lasso Regularization \n\nLet's perform Lasso on the LR model.\n- C = Inverse of regularization strength, smaller values specify stronger regularization.\n- [](http:\/\/)penalty = specify the norm used in the penalization. ","a812939c":"Reference: \n1. [https:\/\/www.kaggle.com\/raviprakash438\/lasso-and-ridge-regularisation]","4c9dc0e7":"### Ridge Regularization \n\n- Also called L2 regression is the most commonly used method of regularization for the problems which do not have a unique solution. It adds penalty equivalent to square of the magnitude of coefficients. Unlike L1, it does not shrink coefficients to zero, but near to zero. ","3c02c6b9":"# Introduction\n\nThe aim of this kernel is to be able to perform Lasso & Ridge Regularization as part of the chapter - 6 of \"Introduction to Statistical Learning\" book. \n\nRegularization is a process of reducing the predictors with the aim of reducing the variance while improving the bias of the model. The aim is to reduce overfitting of the models.\n\nThere are two types of regularization techniques specified in the book:\n1. Ridge Regularization (L2)\n2. Lasso Regularization (L1) \n\nLet's see how these two perform both in non-cross validated data set and with cross validation.","e70cbd51":"## Without Cross Validation","3e411bc7":"How did it came up with selected features list? It used those coefficients whose absolute value is greater than absolute coefficient mean. ","dfe64a82":"Let's compare Lasso with Ridge regularization:\n       ROC AUC Training    Features used     Features removed   ROC AUC Test\nLasso  0.769               166               203                0.7982 \nRidge  0.8039              108               38                 0.8063\n\nAs we can see, the results are almost similar. "}}