{"cell_type":{"3fbd705c":"code","efa0bdc0":"code","cd4be727":"code","ee25e880":"code","6f77325a":"code","6553d902":"code","138619e9":"code","37392dd0":"code","8670d0e5":"code","61dc9fa2":"code","62039252":"code","79d197ed":"code","4ff22791":"code","bbaa336d":"code","3a4c9b3f":"code","dafb4154":"code","92a63d97":"code","57900606":"code","6956844b":"code","40da9769":"code","489a6754":"code","981293f3":"code","b9f5c320":"code","ee82b365":"code","6c569ddc":"code","af54c5a6":"code","78c0e906":"code","c7bcc466":"code","1d459d7a":"code","24bb1eea":"code","090b76ea":"code","09d1409f":"code","1ef23a3c":"code","a2e13382":"code","c4a2d14d":"code","5b437285":"code","59eab7c8":"markdown","bcf52387":"markdown","e3958c34":"markdown","6480cf34":"markdown","b67e8bfe":"markdown","6daa0e37":"markdown","1839ed67":"markdown","abc6b3ef":"markdown","2ba98bba":"markdown","0fe69da1":"markdown","cede36fd":"markdown","2ebcda82":"markdown","4a66aafc":"markdown","607eb0d7":"markdown","da384d02":"markdown","ecf8ad44":"markdown","b9ba15fa":"markdown","869f224c":"markdown","ef73d763":"markdown","c6df3a12":"markdown","211712b6":"markdown","7471a2a7":"markdown","e3b3e021":"markdown","afb52eb0":"markdown","7b7e37da":"markdown","74e493d2":"markdown","0295a0fc":"markdown","eb1b19d4":"markdown"},"source":{"3fbd705c":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\nimport warnings\nwarnings.filterwarnings('ignore')","efa0bdc0":"from sklearn.preprocessing import MinMaxScaler\nfrom sklearn.ensemble import RandomForestRegressor\nfrom sklearn.linear_model import LinearRegression\n\nfrom sklearn.model_selection import train_test_split\n\nfrom sklearn import metrics\n\nfrom sklearn import svm","cd4be727":"# Assigning the name of the column in the form of 'list'\ncolumn_names = ['CRIM', 'ZN', 'INDUS', 'CHAS', 'NOX', 'RM', 'AGE', 'DIS', 'RAD', 'TAX', 'PTRATIO', 'B', 'LSTAT', 'MEDV']\n\n# Importing the dataset in the variable 'df' and assigning the column names as specified above\ndf= pd.read_csv('..\/input\/boston-house-prices\/housing.csv', header=None, delimiter=r\"\\s+\", names=column_names)\n\n# Viewing the top 5 values of the dataset\ndf.head()\n\n# in order to view bottom 5 entries, we can do\n#df.tail()\n\n#in order to view more than 5 entries, we can enter any integer value into '()'.\n#Ex: df.head(10) or df.tail(15), etc","ee25e880":"# Now let us check for the shape of the dataset and also that are any null values present in our dataset.\n# For that,\n\nprint('shape of the dataset=', df.shape)\n\nprint(' \\nThe null count of each column of the dataset are as follows:')\ndf.isnull().sum()","6f77325a":"# Function to identify numeric features:\n\ndef numeric_features(dataset):\n    numeric_col = dataset.select_dtypes(include=np.number).columns.tolist()\n    return dataset[numeric_col].head()\n    \nnumeric_columns = numeric_features(df)\nprint(\"Numerical Features:\")\nprint(numeric_columns)\n\nprint(\"====\"*20)\n\n\n\n\n# Function to identify categorical features:\n\ndef categorical_features(dataset):\n    categorical_col = dataset.select_dtypes(exclude=np.number).columns.tolist()\n    return dataset[categorical_col].head()\n\ncategorical_columns = categorical_features(df)\nprint(\"Categorical Features:\")\nprint(categorical_columns)\n\nprint(\"====\"*20)\n\n\n\n# Function to check the datatypes of all the columns:\n\ndef check_datatypes(dataset):\n    return dataset.dtypes\n\nprint(\"Datatypes of all the columns:\")\ncheck_datatypes(df)","6553d902":"# Function to detect outliers in every feature\ndef detect_outliers(df):\n    cols = list(df)\n    outliers = pd.DataFrame(columns = ['Feature', 'Number of Outliers'])\n    for column in cols:\n        if column in df.select_dtypes(include=np.number).columns:\n            q1 = df[column].quantile(0.25)\n            q3 = df[column].quantile(0.75)\n            iqr = q3 - q1\n            fence_low = q1 - (1.5*iqr)\n            fence_high = q3 + (1.5*iqr)\n            outliers = outliers.append({'Feature':column, 'Number of Outliers':df.loc[(df[column] < fence_low) | (df[column] > fence_high)].shape[0]},ignore_index=True)\n    return outliers\n\ndetect_outliers(df)","138619e9":"# Function to plot histograms\ndef plot_continuous_columns(dataframe):\n    numeric_columns = dataframe.select_dtypes(include=['number']).columns.tolist()\n    dataframe = dataframe[numeric_columns]\n    \n    for i in range(0,len(numeric_columns),2):\n        if len(numeric_columns) > i+1:\n            plt.figure(figsize=(10,4))\n            plt.subplot(121)\n            sns.distplot(dataframe[numeric_columns[i]], kde=False)\n            plt.subplot(122)            \n            sns.distplot(dataframe[numeric_columns[i+1]], kde=False)\n            plt.tight_layout()\n            plt.show()\n\n        else:\n            sns.distplot(dataframe[numeric_columns[i]], kde=False)\n\n# Function to plot boxplots\ndef plot_box_plots(dataframe):\n    numeric_columns = dataframe.select_dtypes(include=['number']).columns.tolist()\n    dataframe = dataframe[numeric_columns]\n    \n    for i in range(0,len(numeric_columns),2):\n        if len(numeric_columns) > i+1:\n            plt.figure(figsize=(10,4))\n            plt.subplot(121)\n            sns.boxplot(dataframe[numeric_columns[i]])\n            plt.subplot(122)            \n            sns.boxplot(dataframe[numeric_columns[i+1]])\n            plt.tight_layout()\n            plt.show()\n\n        else:\n            sns.boxplot(dataframe[numeric_columns[i]])\n\n    \n    \nprint(\"Histograms\\n\")\nplot_continuous_columns(df)  \n\nprint(\"====\"*30)\nprint('\\nBox Plots\\n')\nplot_box_plots(df)","37392dd0":"df.drop(['CHAS'],axis=1,inplace=True)\n\n# Axis=1 means that the execution happens column wise. This means that column will be removed.\n# Inplace=True means that the change is permanent.\n# If we di inplace= false, then only in this parocular step, the column will be removed and in further steps, it will be back.\n\n# Again viewing the dataset to see the change.\ndf.head()","8670d0e5":"from scipy.stats.mstats import winsorize\n\n# Function to treat outliers \n\ndef treat_outliers(dataframe):\n    cols = list(dataframe)\n    for col in cols:\n        if col in dataframe.select_dtypes(include=np.number).columns:\n            dataframe[col] = winsorize(dataframe[col], limits=[0.05, 0.1],inclusive=(True, True))\n    \n    return dataframe    \n\n\ndf = treat_outliers(df)\n\n# Checking for outliers after applying winsorization\n# We see this using a fuction called 'detect_outliers', defined above.\n\ndetect_outliers(df)","61dc9fa2":"# Predictors\nx = df.iloc[:,:-1]\n\n# This means that we are using all the columns, except 'MEDV', to predict the house price\n\n\n# Target\ny = df.iloc[:,-1]\n\n# This is because MEDV is the 'Median value of owner-occupied homes in $1000s'.\n# This shows that this is what we need to predict. So we call it the target variable.","62039252":"def rfc_feature_selection(dataset,target):\n    X_train, X_test, y_train, y_test = train_test_split(dataset, target, test_size=0.3, random_state=42)\n    rfc = RandomForestRegressor(random_state=42)\n    rfc.fit(X_train, y_train)\n    y_pred = rfc.predict(X_test)\n    rfc_importances = pd.Series(rfc.feature_importances_, index=dataset.columns).sort_values().tail(10)\n    rfc_importances.plot(kind='bar')\n    plt.show()\n\nrfc_feature_selection(x,y)","79d197ed":"x.head(2)","4ff22791":"# Modifying the Predictors to improve the effeciency of the model.\n\nx= x[['CRIM','DIS','RM','LSTAT']]\nx.head(2)","bbaa336d":"mms= MinMaxScaler()\nx = pd.DataFrame(mms.fit_transform(x), columns=x.columns)\n\nx.head()","3a4c9b3f":"xtrain,xtest,ytrain,ytest= train_test_split(x,y,test_size=0.3,random_state=42)","dafb4154":"lr=LinearRegression()\n\nlr.fit(xtrain, ytrain)\n\ncoefficients=pd.DataFrame([xtrain.columns, lr.coef_]).T\ncoefficients=coefficients.rename(columns={0:'Attributes',1:'Coefficients'})\ncoefficients","92a63d97":"y_pred=lr.predict(xtrain)","57900606":"print(\"R^2: \",metrics.r2_score(ytrain, y_pred))\nprint(\"Adusted R^2: \", 1-(1-metrics.r2_score(ytrain, y_pred))*(len(ytrain)-1)\/(len(ytrain)-xtrain.shape[1]-1))\nprint(\"MAE: \", metrics.mean_absolute_error(ytrain, y_pred))\nprint(\"MSE: \", metrics.mean_squared_error(ytrain, y_pred))\nprint(\"RMSE: \",np.sqrt(metrics.mean_squared_error(ytrain, y_pred)))","6956844b":"print(metrics.max_error(ytrain, y_pred))","40da9769":"# visualizing the difference between the actual and predicted price \n\nplt.scatter(ytrain, y_pred)\nplt.xlabel(\"Actual Price\")\nplt.ylabel(\"Predicted Price\")\nplt.title(\"Predicted Vs Actual Prices\", fontsize=15)\nplt.show()","489a6754":"# Predicting the Test data with model \nytest_pred=lr.predict(xtest)\n\nlin_acc=metrics.r2_score(ytest, ytest_pred)\nprint(\"R^2: \",lin_acc)\nprint(\"Adusted R^2: \", 1-(1-metrics.r2_score(ytest, ytest_pred))*(len(ytest)-1)\/(len(ytest)-xtest.shape[1]-1))\nprint(\"MAE: \", metrics.mean_absolute_error(ytest, ytest_pred))\nprint(\"MSE: \", metrics.mean_squared_error(ytest, ytest_pred))\nprint(\"RMSE: \",np.sqrt(metrics.mean_squared_error(ytest, ytest_pred)))","981293f3":"print(metrics.max_error(ytest, ytest_pred))","b9f5c320":"# visualizing the difference between the actual and predicted price \n\nplt.scatter(ytest, ytest_pred)\nplt.xlabel(\"Actual Price\")\nplt.ylabel(\"Predicted Price\")\nplt.title(\"Predicted Vs Actual Prices\", fontsize=15)\nplt.show()","ee82b365":"rfr= RandomForestRegressor()\n\nrfr.fit(xtrain, ytrain)","6c569ddc":"y_pred=rfr.predict(xtrain)","af54c5a6":"print(\"R^2: \",metrics.r2_score(ytrain, y_pred))\nprint(\"Adusted R^2: \", 1-(1-metrics.r2_score(ytrain, y_pred))*(len(ytrain)-1)\/(len(ytrain)-xtrain.shape[1]-1))\nprint(\"MAE: \", metrics.mean_absolute_error(ytrain, y_pred))\nprint(\"MSE: \", metrics.mean_squared_error(ytrain, y_pred))\nprint(\"RMSE: \",np.sqrt(metrics.mean_squared_error(ytrain, y_pred)))\n\nprint(\"\\nMaximum Error: \",metrics.max_error(ytrain, y_pred))","78c0e906":"# visualizing the difference between the actual and predicted price \n\nplt.scatter(ytrain, y_pred)\nplt.xlabel(\"Actual Price\")\nplt.ylabel(\"Predicted Price\")\nplt.title(\"Predicted Vs Actual Prices\", fontsize=15)\nplt.show()","c7bcc466":"# Predicting the Test data with model \nytest_pred=rfr.predict(xtest)\n\nrfr_acc=metrics.r2_score(ytest, ytest_pred)\nprint(\"R^2: \",rfr_acc)\nprint(\"Adusted R^2: \", 1-(1-metrics.r2_score(ytest, ytest_pred))*(len(ytest)-1)\/(len(ytest)-xtest.shape[1]-1))\nprint(\"MAE: \", metrics.mean_absolute_error(ytest, ytest_pred))\nprint(\"MSE: \", metrics.mean_squared_error(ytest, ytest_pred))\nprint(\"RMSE: \",np.sqrt(metrics.mean_squared_error(ytest, ytest_pred)))\n\nprint(\"\\nMaximum Error: \",metrics.max_error(ytest, ytest_pred))","1d459d7a":"# visualizing the difference between the actual and predicted price \n\nplt.scatter(ytest, ytest_pred)\nplt.xlabel(\"Actual Price\")\nplt.ylabel(\"Predicted Price\")\nplt.title(\"Predicted Vs Actual Prices\", fontsize=15)\nplt.show()","24bb1eea":"svm_reg=svm.SVR()\nsvm_reg.fit(xtrain, ytrain)","090b76ea":"y_pred=svm_reg.predict(xtrain)","09d1409f":"print(\"R^2: \",metrics.r2_score(ytrain, y_pred))\nprint(\"Adusted R^2: \", 1-(1-metrics.r2_score(ytrain, y_pred))*(len(ytrain)-1)\/(len(ytrain)-xtrain.shape[1]-1))\nprint(\"MAE: \", metrics.mean_absolute_error(ytrain, y_pred))\nprint(\"MSE: \", metrics.mean_squared_error(ytrain, y_pred))\nprint(\"RMSE: \",np.sqrt(metrics.mean_squared_error(ytrain, y_pred)))\n\nprint(\"\\nMaximum Error: \",metrics.max_error(ytrain, y_pred))","1ef23a3c":"# visualizing the difference between the actual and predicted price \n\nplt.scatter(ytrain, y_pred)\nplt.xlabel(\"Actual Price\")\nplt.ylabel(\"Predicted Price\")\nplt.title(\"Predicted Vs Actual Prices\", fontsize=15)\nplt.show()","a2e13382":"# Predicting the Test data with model \nytest_pred=svm_reg.predict(xtest)\n\nsvm_acc=metrics.r2_score(ytest, ytest_pred)\nprint(\"R^2: \",svm_acc)\nprint(\"Adusted R^2: \", 1-(1-metrics.r2_score(ytest, ytest_pred))*(len(ytest)-1)\/(len(ytest)-xtest.shape[1]-1))\nprint(\"MAE: \", metrics.mean_absolute_error(ytest, ytest_pred))\nprint(\"MSE: \", metrics.mean_squared_error(ytest, ytest_pred))\nprint(\"RMSE: \",np.sqrt(metrics.mean_squared_error(ytest, ytest_pred)))\n\nprint(\"\\nMaximum Error: \",metrics.max_error(ytest, ytest_pred))","c4a2d14d":"# visualizing the difference between the actual and predicted price \n\nplt.scatter(ytest, ytest_pred)\nplt.xlabel(\"Actual Price\")\nplt.ylabel(\"Predicted Price\")\nplt.title(\"Predicted Vs Actual Prices\", fontsize=15)\nplt.show()","5b437285":"models=pd.DataFrame({\n    'Model':['Linear Regression', 'Random Forest', 'Support Vector Machine'],\n    'R_squared Score':[lin_acc*100, rfr_acc*100,svm_acc*100]\n})\nmodels.sort_values(by='R_squared Score', ascending=False)","59eab7c8":"### Context\n\nTo Explore more on Regression Algorithm\n\n### Content\n\nEach record in the database describes a Boston suburb or town. The data was drawn from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970. The attributes are de\ufb01ned as follows (taken from the UCI Machine Learning Repository1): \n\n**Data Set Information**\n\n**Features**\n\n|Feature|Description|\n|-----|-----|\n|CRIM|per capita crime rate by town|\n|ZN|proportion of residential land zoned for lots over 25,000 sq.ft.|\n|INDUS|proportion of non-retail business acres per town|\n|CHAS|Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)|\n|NOX|nitric oxides concentration (parts per 10 million)|\n|RM|average number of rooms per dwelling|\n|AGE|proportion of owner-occupied units built prior to 1940|\n|DIS|weighted distances to \ufb01ve Boston employment centers|\n|RAD|index of accessibility to radial highways|\n|TAX|full-value property-tax rate per 10,000 dollars|\n|PTRATIO|pupil-teacher ratio by town |\n|B|1000(Bk\u22120.63)2 where Bk is the proportion of blacks by town| \n|LSTAT|% lower status of the population|\n|MEDV|Median value of owner-occupied homes in 1000s dollars|\n\nWe can see that the input attributes have a mixture of units.","bcf52387":"## Detect outliers in the continuous columns\n\nOutliers are observations that lie far away from majority of observations in the dataset and can be represented mathematically in different ways.\n\nOne method of defining outliers are: outliers are data points lying beyond **(third quartile + 1.5xIQR)** and below **(first quartile - 1.5xIQR)**. \n\n- The function below takes a dataframe and outputs the number of outliers in every numeric feature based on the above rule of *IQR* \n\nYou can even modify the function below to capture the outliers as per their other definitions.","e3958c34":"#### Model Evaluation\n- Training data","6480cf34":"#### Scaling the feature variables using `MinMaxScaler`","b67e8bfe":"#### Observations:\n\n- The columns `CRIM`,`ZN`,`B` and `MEDV` are heavily skewed. This is due to the presence of the **Outliers** present in our dataset. We will deal with outliers in the upcoming steps.\n- We can see that the values in the column `CHAS` are almost 0. This means that Charles River dummy variables are all 0, which in turn means that tract does not bound rivers.\n- Since the features `CHAS` consist majorly only of a single value, it's variance is quite less and hence we can drop it since technically will be of no help in prediction.","6daa0e37":"### Univariate analysis\n\n- Univariate analysis means analysis of a single variable. It\u2019s mainly describes the characteristics of the variable.\n- Let's construct two functions, one that plots a histogram of all the continuous features and other that plots a boxplot of the same.","1839ed67":"#### Feature Selection using Random Forest\n\n`Random Forests` are often used for feature selection in a data science workflow. This is because the tree based strategies that random forests use, rank the features based on how well they improve the purity of the node. The nodes having a very low impurity get split at the start of the tree while the nodes having a very high impurity get split towards the end of the tree. Hence by pruning the tree after desired amount of splits, we can create a subset of the most important features.","abc6b3ef":"#### Model Evaluation\n- Training data","2ba98bba":"### Treating outliers in the continuous columns\n\n- Outliers can be treated in a variety of ways. It depends on the skewness of the feature.\n- To reduce right skewness, we use roots or logarithms or reciprocals (roots are weakest). This is the most common problem in practice.\n- To reduce left skewness, we take squares or cubes or higher powers.\n- But in our data, some of the features have negative values and also the value 0. In such cases, square root transform or logarithmic transformation cannot be used since we cannot take square root of negative values and logarithm of zero is not defined.\n- Hence for this data we use a method called **Winsorization**. In this method we define a confidence interval of let's say 90% and then replace all the outliers below the 5th percentile with the value at 5th percentile and all the values above 95th percentile with the value at the 95th percentile. It is pretty useful when there are negative values and zeros in the features which cannot be treated with log transforms or square roots. Do read up on it more [here](https:\/\/www.statisticshowto.datasciencecentral.com\/winsorize\/)\n\nLets' write a function below that treats all the outliers in the numeric features using winsorization.","0fe69da1":"#### Observations :\n- As per the IQR methodology, there are outliers in majority of the columns.\n- In the further steps below, we will see how to deal with the outliers.","cede36fd":"## Importing data modelling libraries\n","2ebcda82":"# Predicting of house prices\n#### Predict housing prices using machine learing algorithm and perform EDA on the data set .","4a66aafc":"## EDA & Data Visualizations\n\nExploratory data analysis is an approach to analyzing data sets by summarizing their main characteristics with visualizations. The EDA process is a crucial step prior to building a model in order to unravel various insights that later become important in developing a robust algorithmic model. ","607eb0d7":"## Importing necessary libraries\nThe following code is written in Python 3.x. Libraries provide pre-written functionality to perform necessary tasks.","da384d02":"Here, we can observe that there are 506 rows and 14 columns in the dataset. Also, that there are no null values present in the dataset.","ecf8ad44":"## Evaluation Comparison of all the 3 methods","b9ba15fa":"## Prediction of house Price","869f224c":"- Test data","ef73d763":"**R-squared**\n\nR-squared = (TSS-RSS)\/TSS\n- Explained variation\/ Total variation\n- 1 \u2013 Unexplained variation\/ Total variation\n\nA higher R-squared value indicates a higher amount of variability being explained by our model and vice-versa. \n- If we had a really low RSS value, it would mean that the regression line was very close to the actual points. \n- High RSS value, it would mean that the regression line was far away from the actual points.","c6df3a12":"#### Observation:\n\n- We can see that the Important features are sorted in ascending order, along with their importance in the form of bar graph.\n- We can clearly observe that `LSTAT`, `RM`, `DIS` and `CRIM` are the most important features that can be used for prediction. \n- This means that we can ignore the other columns for the House price prediction.","211712b6":"### 3. Support Vector Machine (SVM)","7471a2a7":"We can see that the outliers are removed. The outliers, shown above, in columns `CRIM`,`ZN` and `B` are actually not outliers. They are the majority values present in out dataset.","e3b3e021":"- Test data","afb52eb0":"- Test Data","7b7e37da":"#### Observations:\n- We can see that thr R_squared value for `Linera regression` is the lowest and the `Random Forest` is the highest.\n- It means that **Linear Regression** gives us better results on test data, when compared to the other 2 models.","74e493d2":"### 2. Random Forest","0295a0fc":"#### Model Evaluation\n- Training data","eb1b19d4":"### 1. Linear Regression"}}