{"cell_type":{"70126bad":"code","64fb6674":"code","55a9e2c2":"code","8712a95a":"code","74044ed8":"code","fbcc5c41":"code","0a824afd":"code","1bfda987":"code","9cb8e875":"code","dc724072":"code","3a6dfdef":"code","b3283305":"code","7a2e4f66":"code","e656447a":"code","340facbb":"code","33404f93":"code","543e621f":"code","e1a04018":"code","97ea225a":"code","bd8510b6":"code","18255113":"code","c637a830":"code","883e6be3":"code","6c128f32":"code","9af13fd2":"code","dc1e8d44":"code","2b03e6ae":"code","dad0a869":"code","2468e0df":"code","e1258a3b":"code","ae65422c":"code","44474903":"code","cc46d0f3":"code","f313e882":"code","5e6ede5b":"markdown","53ec8eba":"markdown","d2ca1dad":"markdown","f916a86e":"markdown","7118c230":"markdown","b7cc72f8":"markdown","be8429cf":"markdown","87ab947d":"markdown","281cd7f2":"markdown","7d6b3c39":"markdown"},"source":{"70126bad":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","64fb6674":"import json\nimport plotly.express as px\nimport seaborn as sns\nfrom matplotlib import pyplot as plt\nfrom nltk.tokenize import wordpunct_tokenize\nfrom tqdm import tqdm","55a9e2c2":"train = pd.read_csv('\/kaggle\/input\/deepnlp-hse-course\/train.csv')","8712a95a":"train.head()","74044ed8":"# \u0447\u0438\u0442\u0430\u0435\u043c \nwith open('\/kaggle\/input\/deepnlp-hse-course\/main_category_mapper.json') as f:\n    main_cat2id = json.load(f)\n    \n# \u0438\u043d\u0432\u0435\u0440\u0442\u0438\u0440\u0443\u0435\u043c\nid2main_cat = {value: key for key, value in main_cat2id.items()}","fbcc5c41":"id2main_cat","0a824afd":"# \u0442\u043e\u0436\u0435 \u0441\u0430\u043c\u043e\u0435 \u0441 \u0434\u043e\u043f \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044f\u043c\u0438\nwith open('\/kaggle\/input\/deepnlp-hse-course\/sub_category_mapper.json') as f:\n    sub_cat2id = json.load(f)\n    \nid2sub_cat = {value: key for key, value in sub_cat2id.items()}","1bfda987":"# \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u043c id \u0432 \u0441\u0442\u0440\u043e\u043a\u0443\ntrain.main_category = train.main_category.map(id2main_cat)\ntrain.sub_category = train.sub_category.map(id2sub_cat)","9cb8e875":"train.head()","dc724072":"val_counts_main = pd.DataFrame(train.main_category.value_counts())\nval_counts_main.reset_index(inplace=True)\nval_counts_main.columns = ['category', 'n_entries']\nfig = px.bar(val_counts_main, x='category', y='n_entries')\nfig.show()","3a6dfdef":"val_counts_sub = pd.DataFrame(train.sub_category.value_counts())\nval_counts_sub.reset_index(inplace=True)\nval_counts_sub.columns = ['category', 'n_entries']\nfig = px.bar(val_counts_sub, x='category', y='n_entries')\nfig.show()","b3283305":"train['char_len'] = train.question.map(len)","7a2e4f66":"train['token_len'] = train.question.map(lambda x: len(wordpunct_tokenize(x)))","e656447a":"train.head()","340facbb":"plt.figure(figsize=(16, 12))\nplt.title('Distplot question char len')\nsns.distplot(train.char_len)","33404f93":"plt.figure(figsize=(16, 12))\nplt.title('Distplot question token len')\nsns.distplot(train.token_len)","543e621f":"input_file = open('\/kaggle\/input\/deepnlp-hse-course\/answers.jsonl')\n\nprogress_bar = tqdm()\n\nadditional_data = {\n    'question': [],\n    'answer': []\n}\n\ntry:\n    while True:\n\n        line = input_file.readline().strip()\n\n        if not line:\n            break\n            \n        line = json.loads(line)\n        \n        question = list(line.keys())[0]\n        \n        for answer in line[question]:\n            additional_data['question'].append(question)\n            additional_data['answer'].append(answer)\n            \n        progress_bar.update()\n            \nexcept KeyboardInterrupt:\n    pass\n\nprogress_bar.close()\ninput_file.close()","e1a04018":"additional_data = pd.DataFrame(data=additional_data)","97ea225a":"additional_data.sample(frac=1).head(10)","bd8510b6":"f'\u041a\u043e\u043b\u0438\u0447\u0435\u0441\u0442\u0432\u043e \u0441\u0442\u0440\u043e\u043a \u0432 \u0442\u0430\u0431\u043b\u0438\u0446\u0435: {additional_data.shape[0]}'","18255113":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import f1_score, confusion_matrix","c637a830":"# \u043f\u0435\u0440\u0435\u0432\u043e\u0434\u0438\u043c \u043a\u043b\u0430\u0441\u0441 \u0432 \u0438\u043d\u0434\u0435\u043a\u0441\ntrain['target'] = train.main_category.map(main_cat2id)","883e6be3":"train.head()","6c128f32":"x_train, x_test, y_train, y_test = train_test_split(train.question, train.target, test_size=0.15)","9af13fd2":"x_test.shape","dc1e8d44":"vectorizer = TfidfVectorizer(max_features=75000, ngram_range=(1, 2))","2b03e6ae":"x_train_vectorized = vectorizer.fit_transform(x_train)\nx_test_vectorized = vectorizer.transform(x_test)","dad0a869":"log_reg = LogisticRegression(solver='lbfgs', multi_class='auto')","2468e0df":"log_reg.fit(x_train_vectorized, y_train)","e1258a3b":"predicted_train = log_reg.predict(x_train_vectorized)\npredicted_test = log_reg.predict(x_test_vectorized)","ae65422c":"f1_train = f1_score(y_true=y_train, y_pred=predicted_train, average='micro')\nf1_test = f1_score(y_true=y_test, y_pred=predicted_test, average='micro')\n\nf'F1 train: {f1_train:.3f} | test: {f1_test:.3f}'","44474903":"classes = [id2main_cat[n] for n in range(len(id2main_cat))]","cc46d0f3":"def plot_confusion_matrix(y_true, y_pred, classes,\n                          normalize=True,\n                          title=None,\n                          cmap=plt.cm.Blues):\n    \"\"\"\n    This function prints and plots the confusion matrix.\n    Normalization can be applied by setting `normalize=True`.\n    \"\"\"\n    if not title:\n        if normalize:\n            title = 'Normalized confusion matrix'\n        else:\n            title = 'Confusion matrix, without normalization'\n\n    # Compute confusion matrix\n    cm = confusion_matrix(y_true, y_pred)\n    # Only use the labels that appear in the data\n    if normalize:\n        cm = cm.astype('float') \/ cm.sum(axis=1)[:, np.newaxis]\n        print(\"Normalized confusion matrix\")\n    else:\n        print('Confusion matrix, without normalization')\n\n    fig, ax = plt.subplots(figsize=(18, 18))\n    im = ax.imshow(cm, interpolation='nearest', cmap=cmap)\n    ax.figure.colorbar(im, ax=ax)\n    # We want to show all ticks...\n    ax.set(xticks=np.arange(cm.shape[1]),\n           yticks=np.arange(cm.shape[0]),\n           # ... and label them with the respective list entries\n           xticklabels=classes, yticklabels=classes,\n           title=title,\n           ylabel='True label',\n           xlabel='Predicted label')\n\n    # Rotate the tick labels and set their alignment.\n    plt.setp(ax.get_xticklabels(), rotation=45, ha=\"right\",\n             rotation_mode=\"anchor\")\n\n    # Loop over data dimensions and create text annotations.\n    fmt = '.2f' if normalize else 'd'\n    thresh = cm.max() \/ 2.\n    for i in range(cm.shape[0]):\n        for j in range(cm.shape[1]):\n            ax.text(j, i, format(cm[i, j], fmt),\n                    ha=\"center\", va=\"center\",\n                    color=\"white\" if cm[i, j] > thresh else \"black\")\n    fig.tight_layout()\n    return ax","f313e882":"plot_confusion_matrix(y_test, predicted_test, classes)","5e6ede5b":"## \u0427\u0438\u0442\u0430\u0435\u043c \u0444\u0430\u0439\u043b","53ec8eba":"![alt text](https:\/\/i.kym-cdn.com\/entries\/icons\/mobile\/000\/002\/456\/tearingmeapartlisa.jpg \"Logo Title Text 1\")","d2ca1dad":"# \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u0434\u043b\u0438\u043d \u0432 \u0442\u043e\u043a\u0435\u043d\u0430\u0445 \u0438 \u0441\u0438\u043c\u0432\u043e\u043b\u0430\u0445","f916a86e":"# \u0413\u043b\u0430\u0432\u043d\u0430\u044f \u0438 \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u0430\u044f \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u044f \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b id\n\u0423 \u043d\u0430\u0441 \u0435\u0441\u0442\u044c \u0444\u0430\u0439\u043b\u044b main_category_mapper.json \u0438 sub_category_mapper.json  \n### main_category_mapper.json\n\u0421\u043b\u043e\u0432\u0430\u0440\u044c, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u043a\u043b\u044e\u0447 - \u044d\u0442\u043e \u0441\u0442\u0440\u043e\u043a\u043e\u0432\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0433\u043b\u0430\u0432\u043d\u043e\u0439 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0438, \u0430 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 - id \u0433\u043b\u0430\u0432\u043d\u043e\u0439 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0438\n### sub_category_mapper.json\n\u0421\u043b\u043e\u0432\u0430\u0440\u044c, \u0432 \u043a\u043e\u0442\u043e\u0440\u043e\u043c \u043a\u043b\u044e\u0447 - \u044d\u0442\u043e \u0441\u0442\u0440\u043e\u043a\u043e\u0432\u043e\u0435 \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u0438\u0435 \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0438, \u0430 \u0437\u043d\u0430\u0447\u0435\u043d\u0438\u0435 - id \u0434\u043e\u043f\u043e\u043b\u043d\u0438\u0442\u0435\u043b\u044c\u043d\u043e\u0439 \u043a\u0430\u0442\u0435\u0433\u043e\u0440\u0438\u0438","7118c230":"# \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 train","b7cc72f8":"# \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 answers.jsonl\n\n## answers.jsonl\n\u0424\u0430\u0439\u043b, \u0433\u0434\u0435 \u043a\u0430\u0436\u0434\u0430\u044f \u0441\u0442\u0440\u043e\u043a\u0430 - \u044d\u0442\u043e json \u0442\u0430\u043a\u043e\u0433\u043e \u0432\u0438\u0434\u0430:\n```json\n{\n    '\u0432\u043e\u043f\u0440\u043e\u0441': [\n        '\u043e\u0442\u0432\u0435\u0442_1',\n        '\u043e\u0442\u0432\u0435\u0442_2',\n        ...,\n        '\u043e\u0442\u0432\u0435\u0442_n'\n    ]\n}\n```   ","be8429cf":"# \u041f\u043e\u043f\u0440\u043e\u0431\u0443\u0435\u043c \u043f\u043e\u0441\u0442\u0440\u043e\u0438\u0442\u044c \u043f\u0440\u043e\u0441\u0442\u0443\u044e \u043c\u043e\u0434\u0435\u043b\u044c","87ab947d":"## \u0422\u0435\u043f\u0435\u0440\u044c \u043a\u043b\u0430\u0441\u0441\u044b \u043f\u0440\u0435\u0434\u0441\u0442\u0430\u0432\u043b\u0435\u043d\u044b \u0441\u0442\u0440\u043e\u043a\u0430\u043c\u0438","281cd7f2":"![alt text](https:\/\/i.redd.it\/icok5mempnd21.jpg \"Logo Title Text 1\")","7d6b3c39":"# \u041f\u043e\u0441\u043c\u043e\u0442\u0440\u0438\u043c \u043d\u0430 \u0440\u0430\u0441\u043f\u0440\u0435\u0434\u0435\u043b\u0435\u043d\u0438\u0435 \u043a\u043b\u0430\u0441\u0441\u043e\u0432"}}