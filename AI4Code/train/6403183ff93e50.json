{"cell_type":{"d9a350c5":"code","5757009d":"code","f95218a9":"code","c49bce3b":"code","ec0acd1c":"code","f6a40199":"code","3e84bad6":"code","80895cf3":"code","f56d8b23":"code","08b6d2d1":"code","295b5b3d":"code","76bb12cb":"markdown","36813e93":"markdown","8db2828d":"markdown","8fbb1a15":"markdown","aeb393e5":"markdown","cd5a60ab":"markdown","9e63cfc3":"markdown","23c1ab13":"markdown","3677e6b9":"markdown"},"source":{"d9a350c5":"import pandas as pd\nimport numpy as np\nfrom scipy import stats\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import roc_auc_score\nfrom sklearn.metrics import classification_report\nimport seaborn as sns\nfrom sklearn.svm import SVC\nfrom xgboost import XGBClassifier\nfrom sklearn.ensemble import VotingClassifier\nimport matplotlib.pyplot as plt\nfrom pylab import rcParams\nfrom sklearn.naive_bayes import GaussianNB\n\nrcParams['figure.figsize'] = 18, 8","5757009d":"train = pd.read_csv('..\/input\/mobile-price-classification\/train.csv')\ntest = pd.read_csv('..\/input\/mobile-price-classification\/test.csv')\nprint(train.head())\nsns.histplot(data=train, x=\"price_range\")","f95218a9":"print(train.info())","c49bce3b":"d = {}\nfor column in train.columns:\n    d[column] = train[column].nunique()\nprint(d)\nnumerous_columns = list(dict(sorted(d.items(), key=lambda x: x[1], reverse=True)).keys())[:6]\nprint(numerous_columns)\nf, axes = plt.subplots(2, 3)\nfor column, ax in zip(numerous_columns, axes.flatten()):\n        sns.boxplot(x=train[column], ax=ax)\n\n\n\n    \n","ec0acd1c":"corr = train.corr()\ncorr.style.background_gradient(cmap='coolwarm')","f6a40199":"z_scores = stats.zscore(train)\nabs_z_scores = np.abs(z_scores)\nfor column in abs_z_scores.columns:\n    print(column + ' ' + ' min:' + str(abs_z_scores[column].min()) + ' median:' + str(abs_z_scores[column].median()) + ' max:' + str(abs_z_scores[column].max()))\nfiltered_entries = (abs_z_scores < 2.5).all(axis=1)\nnew_train = train[filtered_entries]\ny = new_train['price_range']\nX = new_train.drop(['price_range'], axis=1)\n\n","3e84bad6":"X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nestimator = []\nclf = DecisionTreeClassifier(random_state=42)\nparams = {'max_depth': range(1,11), 'min_samples_split': range(2,11), 'min_samples_leaf': range(1,11)}\ngcv = GridSearchCV(clf, params, n_jobs=-1)\ngcv.fit(X_train, y_train)\nbest_gcv = gcv.best_estimator_\nestimator.append(('dtc', best_gcv))\nprint(best_gcv)\nprint()\ntree_predict_roc = best_gcv.predict_proba(X_test)\ntree_predict = best_gcv.predict(X_test)\nprint('roc_auc_score:', roc_auc_score(y_test, tree_predict_roc, average=\"weighted\", multi_class=\"ovr\"))\nprint()\nprint(classification_report(y_test, tree_predict))\n","80895cf3":"svc = SVC(random_state=42, probability=True)\nparam_grid = {'C': [0.1,1, 10, 100], 'gamma': [1,0.1,0.01,0.001],'kernel': ['rbf', 'poly', 'sigmoid']}\ngcv = GridSearchCV(svc, param_grid, n_jobs=-1)\ngcv.fit(X_train, y_train)\nbest_gcv = gcv.best_estimator_\nestimator.append(('svc', best_gcv))\nprint(best_gcv)\nprint()\nsvm_predict_roc = best_gcv.predict_proba(X_test)\nsvm_predict = best_gcv.predict(X_test)\nprint('roc_auc_score:', roc_auc_score(y_test, svm_predict_roc, average=\"weighted\", multi_class=\"ovr\"))\nprint()\nprint(classification_report(y_test, svm_predict))\n","f56d8b23":"'''xgb = XGBClassifier(objective='multi:softprob', nthread=4, eval_metric='mlogloss', random_state=42)\nparameters = {'max_depth': range (2, 10, 1), 'n_estimators': range(100, 220, 40), 'learning_rate': [0.1, 0.01, 0.05]}\ngcv = GridSearchCV(xgb, parameters, n_jobs=-1)\ngcv.fit(X_train, y_train)\nbest_gcv = gcv.best_estimator_\nestimator.append(('xgb', best_gcv))\nprint(best_gcv)\nprint()\nxgb_predict_roc = best_gcv.predict_proba(X_test)\nxgb_predict = best_gcv.predict(X_test)\nprint('roc_auc_score:', roc_auc_score(y_test, xgb_predict_roc, average=\"weighted\", multi_class=\"ovr\"))\nprint()\nprint(classification_report(y_test, xgb_predict))'''","08b6d2d1":"param_grid_nb = { 'var_smoothing': np.logspace(0,-9, num=100)}\ngcv = GridSearchCV(estimator=GaussianNB(), param_grid=param_grid_nb, n_jobs=-1)\ngcv.fit(X_train, y_train)\nbest_gcv = gcv.best_estimator_\nestimator.append(('gauss', best_gcv))\nprint(best_gcv)\nprint()\ngauss_predict_roc = best_gcv.predict_proba(X_test)\ngauss_predict = best_gcv.predict(X_test)\nprint('roc_auc_score:', roc_auc_score(y_test, gauss_predict_roc, average=\"weighted\", multi_class=\"ovr\"))\nprint()\nprint(classification_report(y_test, gauss_predict))","295b5b3d":"vot_hard = VotingClassifier(estimators = estimator, voting ='hard')\nvot_hard.fit(X_train, y_train)\ny_pred = vot_hard.predict(X_test)\nvot_soft = VotingClassifier(estimators = estimator, voting ='soft')\nvot_soft.fit(X_train, y_train)\ny_pred_roc = vot_soft.predict_proba(X_test)\nprint('roc_auc_score:', roc_auc_score(y_test, y_pred_roc, average=\"weighted\", multi_class=\"ovr\"))\nprint()\nprint(classification_report(y_test, y_pred))","76bb12cb":"Correlation between features mostly low, big correlation(0.8 < corr < 1) only between target column and 'ram'. Our dataset is pretty good, but i would try to make it a little better by removing outliers( I hope not worse :)). For this purpose we will use z-normalization.","36813e93":"Now it's time to build corr matrix and watch dependency between features","8db2828d":"As we see our data is well balanced. Now we will check the number of Nan values and column types in the dataset.","8fbb1a15":"Read .csv file and let's have a look at our data. Target column is 'price_range', plot it.","aeb393e5":"Also we can try XGBClassifier method, but execute time of this algorithm is too long for multiclass classification.\n[Read more!](https:\/\/towardsdatascience.com\/xgboost-for-multi-class-classification-799d96bcd368)","cd5a60ab":"Conclusion: best accuracy and best roc auc score we have in SVC method. When we tried to make an assemble, our metrics became lower because of low performance of other algorithms.","9e63cfc3":"We have trained three different ML models. For making our results even better let's try to do assembly of our models and get new results by voting.","23c1ab13":"Nan values were not found. Then we will plot some features distribution, for example top 6 most numerous features.","3677e6b9":"Now we start to train our ML models. We will use roc auc score and classification report metrics as our main metrics for the quality assessment of our models."}}