{"cell_type":{"326c52c3":"code","bf78a9fb":"code","f50a8b5a":"code","62d88197":"code","27387801":"code","f077f050":"code","896c0527":"code","9ef5d2f5":"code","da89411a":"code","1ffbd50f":"code","3333f9c0":"markdown","7ce11644":"markdown","d9c38f29":"markdown","b25bfb7b":"markdown","7af3680d":"markdown","8cf0e915":"markdown"},"source":{"326c52c3":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load in \n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Input data files are available in the \"..\/input\/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","bf78a9fb":"##### Phase 1 Cleaning:\n\n    #reading from csv file\ndf_train=pd.read_csv('\/kaggle\/input\/train.csv')\ndf_test=pd.read_csv('\/kaggle\/input\/test.csv')\n\ndf_train.shape, df_test.shape\n\ndf_train.columns,df_test.columns\n\ndf_train=df_train.drop('Unnamed: 0',axis=1)\ndf_test=df_test.drop('Unnamed: 0',axis=1)\n\ndf_train.columns,df_test.columns\n\n#removing stopwords, punctuations\nimport string\nimport re\nimport nltk\nfrom nltk.tokenize import WordPunctTokenizer\nfrom nltk.stem.porter import PorterStemmer\nfrom nltk.tokenize import word_tokenize\n\nstopword = nltk.corpus.stopwords.words('english')# All English Stopwords\n\n# Function to clean data\nps = nltk.PorterStemmer()\ndef clean_text(text):\n    text = \"\".join([word.lower() for word in text if word not in string.punctuation])\n    text = re.sub(r\"<.*>\",\" \",text)\n    tokens = re.split('\\W+', text)\n    text = [ps.stem(word) for word in tokens if word not in stopword]\n    return text\n\n#stemming and lemmatization\n\nwn = nltk.WordNetLemmatizer()\n\ndef lemmatizing(tokenized_text):\n    text = [wn.lemmatize(word) for word in tokenized_text]\n    return text\n\n\ndf_train['reviews'] = df_train['reviews'].apply(lambda x: re.sub(r\"<.*>\",\" \",x))\ndf_train['reviews'] = df_train['reviews'].apply(lambda x: clean_text(x))\ndf_train['reviews_lemming'] = df_train['reviews'].apply(lambda x: lemmatizing(x))\ndf_train.head()\n\ndf_test['reviews'] = df_test['reviews'].apply(lambda x: re.sub(r\"<.*>\",\" \",x))\ndf_test['reviews'] = df_test['reviews'].apply(lambda x: clean_text(x))\ndf_test['reviews_lemming'] = df_test['reviews'].apply(lambda x: lemmatizing(x))\ndf_test.head()\n\n#feature selection\n\n\nfrom sklearn.feature_extraction.text import CountVectorizer\nvec = CountVectorizer(analyzer=clean_text)\nX_train = vec.fit_transform(df_train['reviews'])\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nY = le.fit_transform(df_train['labels'])\n\nfrom sklearn.ensemble import ExtraTreesClassifier\ntree_clf = ExtraTreesClassifier()\ntree_clf.fit(X_train, Y)\n\nimportances = tree_clf.feature_importances_\nfeature_names = vec.get_feature_names()\nfeature_imp_dict = dict(zip(feature_names, importances))\n\nfrom sklearn.feature_selection import SelectFromModel\n\nmodel = SelectFromModel(tree_clf, prefit=True)\nX_train_updated = model.transform(X_train)\nprint('Total features count', X_train.shape[1])\nprint('Selected features', X_train_updated.shape[1])\n\nX_train_2 = vec.fit_transform(df_test['reviews'])\n\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\nY_2 = le.fit_transform(df_test['labels'])\n\nfrom sklearn.ensemble import ExtraTreesClassifier\ntree_clf = ExtraTreesClassifier()\ntree_clf.fit(X_train_2, Y_2)\n\nimportances = tree_clf.feature_importances_\nfeature_names = vec.get_feature_names()\nfeature_imp_dict = dict(zip(feature_names, importances))\n\nfrom sklearn.feature_selection import SelectFromModel\n\nmodel = SelectFromModel(tree_clf, prefit=True)\nX_train_updated_2 = model.transform(X_train_2)\nprint('Total features count', X_train_2.shape[1])\nprint('Selected features', X_train_updated_2.shape[1])\n\n","f50a8b5a":"#### Phase 2 Exploration:\n\nimport operator\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.preprocessing import LabelEncoder\nfrom sklearn.ensemble import ExtraTreesClassifier\n\ndef print_features(df):\n    tree_clf = ExtraTreesClassifier()\n    vec2 = CountVectorizer(analyzer=clean_text) \n    X_trains_2 = vec2.fit_transform(df['reviews'])\n    le = LabelEncoder()\n    y = le.fit_transform(df['labels'])\n    \n    tree_clf.fit(X_trains_2,y)\n    \n    importances = tree_clf.feature_importances_\n    feature_names = vec2.get_feature_names()\n    feature_imp_dict = dict(zip(feature_names, importances))\n    sorted_features = sorted(feature_imp_dict.items(),key=operator.itemgetter(1),reverse=True)\n    indices = np.argsort(importances)[::-1]\n    print(\"Feature ranking:\")\n    for f in range(20):\n        print(\"Feature %d : %s (%f)\" % (indices[f],sorted_features[f][0],sorted_features[f][1]))\n    plt.figure(figsize = (20,20))\n    plt.title(\"Feature Importances\")\n    plt.bar(range(100),importances[indices[:100]],color=\"r\",align=\"center\")\n    plt.xticks(range(100),sorted_features[:100],rotation=90)\n    plt.xlim([-1,100])\n    plt.show()\n    return()\n\nprint_features(df_train)\n\nprint_features(df_test)\n\ndf_unsup=pd.read_csv('\/kaggle\/input\/unsup.csv')\n\ndf_unsup['reviews'] = df_unsup['reviews'].apply(lambda x: re.sub(r\"<.*>\",\" \",x))\n\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nvectorizer = TfidfVectorizer(analyzer=clean_text)\nX = vectorizer.fit_transform(df_unsup.reviews)\n\n# import modules\nfrom sklearn.cluster import KMeans\n# create an instance\nkmeans = KMeans(n_clusters=2, random_state=0)\n# fit the model\nkmeans.fit(X)\n# view the data labels after clustering\nkmeans.labels_\n# view the cluster centers\nkmeans.cluster_centers_  #coordinate of centers\n\n","62d88197":"\n#### Phase 3 Visualization:\n\n\nfrom wordcloud import WordCloud\n\ndf_train2=pd.read_csv('\/kaggle\/input\/train.csv')\n\ndf_train2=df_train2.drop('Unnamed: 0',axis=1)\ndf_train2.shape\n\ndf_train2['reviews'] = df_train2['reviews'].apply(lambda x: re.sub(r\"<.*>\",\" \",x))\n\nneg_list = df_train2[df_train2[\"labels\"] == 0][\"reviews\"].unique().tolist()\nneg_list[:2]\npos_list = df_train2[df_train2[\"labels\"] == 1][\"reviews\"].unique().tolist()\npos_list[:2]\n\n#negative word cloud\nneg = \" \".join(neg_list)\nneg[:100]\nneg_wordcloud = WordCloud().generate(neg)\nplt.figure()\nplt.imshow(neg_wordcloud)\nplt.show()\n\n#positive word cloud\npos = \" \".join(pos_list)\npos[:100]\npos_wordcloud = WordCloud().generate(neg)\nplt.figure()\nplt.imshow(pos_wordcloud)\nplt.show()\n\ndf_train2['body_len'] = df_train2['reviews'].apply(lambda x: len(x) - x.count(\" \"))\nbins = np.linspace(0, 200, 40)\n\nplt.hist(df_train2[df_train2['labels']==0]['body_len'], bins, alpha=0.5, normed=True, label='neg')\nplt.hist(df_train2[df_train2['labels']==1]['body_len'], bins, alpha=0.5, normed=True, label='pos')\nplt.legend(loc='upper left')\nplt.show()\n","27387801":"\n#### Phase 4 Hypothesis testing :\n\n# import the vectorizer\nfrom sklearn.feature_extraction.text import CountVectorizer\n\n# create an instance\ncount_vect = CountVectorizer(analyzer=clean_text)\n\n# convert text to vectors\nX = count_vect.fit_transform(df_train['reviews'])\n\n# encode the target strings\nfrom sklearn.preprocessing import LabelEncoder\nle = LabelEncoder()\n\ny = le.fit_transform(df_train.labels)\n\n#Naive Bayes\n# import Nauve bayes classifier\nfrom sklearn.naive_bayes import MultinomialNB\n# split the data\nfrom sklearn.model_selection import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=0)\nX_train.shape, X_test.shape\n\n# fit the classifier model\nclf = MultinomialNB()\nclf.fit(X_train, y_train)\n# predict the outcome for testing data\npredictions = clf.predict(X_test)\npredictions.shape\n\n\n# check the accuracy of the model\nfrom sklearn.metrics import accuracy_score\naccuracy = accuracy_score(y_test, predictions)\naccuracy\n\n","f077f050":"from sklearn.ensemble import RandomForestClassifier\nclf = RandomForestClassifier(n_estimators=5)\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\naccuracy\n","896c0527":"\n\nfrom sklearn.svm import SVC\nclf = SVC()\n# fit the classifier\nclf.fit(X_train, y_train)\n# predict the outcome for testing data\npredictions = clf.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\naccuracy\n","9ef5d2f5":"#### Phase 5 Model Building:\n\n# convert text to vectors\nX_2 = count_vect.fit_transform(df_test['reviews'])\n\n#label encoding\ny_2 = le.fit_transform(df_test.labels)\n\n#splitting the data\nX_train, X_test, y_train, y_test = train_test_split(X_2, y_2, test_size=0.25, random_state=0)\n\n\nclf = RandomForestClassifier(n_estimators=5)\nclf.fit(X_train, y_train)\npredictions = clf.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\naccuracy\n\n","da89411a":"clf = MultinomialNB()\nclf.fit(X_train, y_train)\n# predict the outcome for testing data\npredictions = clf.predict(X_test)\naccuracy = accuracy_score(y_test, predictions)\naccuracy\n\n","1ffbd50f":"\n# import modules\nfrom sklearn.cluster import KMeans\nclust=2\n# create an instance\nkmeans = KMeans(n_clusters=clust, random_state=0)\n# fit the model\nkmeans.fit(X)\n# Visualising the clusters\nprint(\"Top terms per cluster:\")\norder_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\nterms = vectorizer.get_feature_names()\nfor i in range(clust):\n    print(\"Cluster %d:\" % i),\n    for vals in order_centroids[i, :10]:\n        print(' %s' % terms[vals])","3333f9c0":"**Phase 1 Cleaning**<br>\ncleaning of the data","7ce11644":"**Phase 3 Visiulization**","d9c38f29":"**Phase 4 Hypothesus testing**\n<br>\ndone on training","b25bfb7b":"**NLP ON IMDB REVIEWS**<br>\nthis is nlp on movie reviews. there is total of 50 text files that have preadded to csv files which is used for supervied training. these csv files are 2 one for training and other for testing each having 25000 reviews present both have positive and negative reviews of various movies.\nthere is a file for unsupervied learning with 50000 reviews. in this file it check for positive and negative reviews.","7af3680d":"**Phase 5 Model Building**\n<br>\nthis is done on test data","8cf0e915":"**Phase 2**\n<br>\nExploring the data for the feature selection"}}