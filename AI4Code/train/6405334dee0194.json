{"cell_type":{"08bfa830":"code","fda51247":"code","2b010b40":"code","f0807025":"code","7ed284c9":"code","b04f1e8e":"code","30a4244e":"code","df77e535":"code","0668357f":"code","2621a6f1":"code","bdab94e5":"code","2542fcf7":"code","fb0675cd":"code","9755fe8e":"code","fa728700":"code","03c3956c":"code","772a6f59":"code","9367d554":"code","3f76e8c3":"code","e7298deb":"code","1745b204":"code","f3e018fc":"code","7f229c6e":"code","b667a744":"code","4d58fdf2":"code","9a465db3":"markdown","c82f2ef1":"markdown","b735322e":"markdown","13bd55e4":"markdown","3ebbcc76":"markdown","6efab456":"markdown","583c7a4d":"markdown","63ebfdc0":"markdown","32b050d4":"markdown","16dd97d5":"markdown","6aeab59f":"markdown","667da1a8":"markdown","58974303":"markdown","a6b84639":"markdown","196920e9":"markdown","4a89818d":"markdown","63be8420":"markdown","0a145742":"markdown","5cc1957b":"markdown","0aa181e1":"markdown","7e1dece1":"markdown","536ac39d":"markdown"},"source":{"08bfa830":"import math\nimport numpy as np \nimport pandas as pd \nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split, KFold, learning_curve\n\nfrom sklearn.linear_model import ElasticNet\nfrom xgboost import XGBRegressor\nfrom lightgbm import LGBMRegressor\nfrom sklearn.ensemble import GradientBoostingRegressor \nfrom catboost import CatBoostRegressor\nfrom mlxtend.regressor import StackingCVRegressor\n\nfrom sklearn.metrics import mean_squared_error\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\npd.set_option('display.max_column', 102)\npd.set_option('display.max_row', 250000)","fda51247":"train = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/train.csv', index_col='id')\ntest = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/test.csv', index_col='id')\n\nsub = pd.read_csv('\/kaggle\/input\/tabular-playground-series-aug-2021\/sample_submission.csv')","2b010b40":"train.shape","f0807025":"train.head()","7ed284c9":"train.info()","b04f1e8e":"test.info()","30a4244e":"train.describe()","df77e535":"plt.figure(figsize=(12, 6))\nsns.histplot(train.loss)","0668357f":"train.loss.value_counts()","2621a6f1":"fig, ax = plt.subplots(1, 1, figsize=(14 , 14))\n\ncorr = train.corr()\n\nmask = np.zeros_like(corr, dtype=np.bool)\nmask[np.triu_indices_from(mask)] = True\n\nsns.heatmap(corr, ax=ax,\n        square=True, center=0, linewidth=1,\n        cmap=sns.diverging_palette(240, 10, as_cmap=True),\n        cbar_kws={\"shrink\": .6},    \n        mask=mask\n       ) \n\nax.set_title(f'Correlation', loc='left', fontweight='bold')     \n\nplt.show()","bdab94e5":"corr.loss","2542fcf7":"df = train.append(test).reset_index(drop=True)","fb0675cd":"df = df.drop(['loss'], axis=1)\ncolumns = df.columns.values\n\ncols = 3\nrows = len(columns) \/\/ cols + 1\n\nfig, axs = plt.subplots(ncols=cols, nrows=rows, figsize=(16,100), sharex=False)\n\nplt.subplots_adjust(hspace = 0.3)\ni=0\n\nfor r in np.arange(0, rows, 1):\n    for c in np.arange(0, cols, 1):\n        if i >= len(columns):\n            axs[r, c].set_visible(False)\n        else:\n            hist1 = axs[r, c].hist(train[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   alpha=0.7,\n                                   label=\"Train Dataset\")\n            hist2 = axs[r, c].hist(test[columns[i]].values,\n                                   range=(df[columns[i]].min(),\n                                          df[columns[i]].max()),\n                                   bins=40,\n                                   alpha=0.7,\n                                   label=\"Test Dataset\")\n            axs[r, c].set_title(columns[i], fontsize=14, pad=5)\n            axs[r, c].tick_params(axis=\"y\", labelsize=13)\n            axs[r, c].tick_params(axis=\"x\", labelsize=13)\n            axs[r, c].grid(axis=\"y\")\n            axs[r, c].legend(fontsize=13)\n            \n        i+=1\n#plt.suptitle(\"Feature values distribution in both datasets\", y=0.99)\nplt.show();","9755fe8e":"ss = StandardScaler()\nfeatures = [f'f{i}' for i in range(100)]\ntrain[features] = ss.fit_transform(train[features])\ntest[features] = ss.transform(test[features])","fa728700":"for col in df.columns:\n    if np.array_equal(df[col].values, df[col].values.astype(int)):\n        print(col)\n        train[col].astype('int')\n        test[col].astype('int')","03c3956c":"y = train.loss\nX = train.drop('loss', axis = 1)\n\nx_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 0)","772a6f59":"eNet = ElasticNet(alpha=0.0005, l1_ratio=0.9)\n\neNet.fit(x_train,y_train)\n\nprint(\"RMSE\", np.sqrt(mean_squared_error(y_test, eNet.predict(x_test))))","9367d554":"cat_model = CatBoostRegressor(random_state=42,iterations = 2000,learning_rate=0.005, early_stopping_rounds=50)\ncat_model.fit(x_train, y_train, verbose = 0)\n\n\ncat_model.fit(x_train,y_train)\n\nprint(\"RMSE\", np.sqrt(mean_squared_error(y_test, cat_model.predict(x_test))))","3f76e8c3":"LGBMReg = LGBMRegressor(bagging_fraction=0.8, bagging_freq=5, \n                           feature_fraction=0.2319, feature_fraction_seed=9,\n                           learning_rate=0.05, max_bin=55, min_data_in_leaf=6,\n                           min_sum_hessian_in_leaf=11, n_estimators=720, num_leaves=5,\n                           bagging_seed=9,objective='regression')\n\n\nLGBMReg.fit(x_train,y_train)\n\nprint(\"RMSE\", np.sqrt(mean_squared_error(y_test, LGBMReg.predict(x_test))))","e7298deb":"kfold = KFold(n_splits=10)\n\nbase_models = (eNet, cat_model)\nstack = StackingCVRegressor(regressors=base_models,\n                            meta_regressor=LGBMReg, \n                            use_features_in_secondary=True,\n                            store_train_meta_features=True,\n                            shuffle=False,cv=kfold,\n                            random_state=1)","1745b204":"kfold","f3e018fc":"stack.fit(x_train,y_train)\n\nprint(\"RMSE\", np.sqrt(mean_squared_error(y_test, stack.predict(x_test))))","7f229c6e":"sub.loss = stack.predict(test)","b667a744":"sub.head()","4d58fdf2":"sub.to_csv('submissio.csv', index=False)","9a465db3":" Wow, there is no messing values either in train or test set, so no imputation needed!","c82f2ef1":" Now let's detect if there is features wich are num not float, then convert it to int.","b735322e":"#### Scaling data :","13bd55e4":"### If you find this notebook useful, please don't forget to upvote it!","3ebbcc76":"#### Convert float to int :","6efab456":"#### First model : ","583c7a4d":"#### Train-Test split :","63ebfdc0":"#### Target visualization :","32b050d4":"#### Final model = meta-model :","16dd97d5":"#### Features distributions :","6aeab59f":" So great, The datasets distributions are well balanced.","667da1a8":"## Modeling and Evaluation :","58974303":"## Pre-Processing :","a6b84639":"There is too weak correlation between target and other features!","196920e9":"#### Correlations :","4a89818d":"#### Stacking :","63be8420":"### Import Libraries \/ Load Data :","0a145742":"#### Second model :","5cc1957b":"The scale of this data is really diverse, the values are in different ranges so we should do some Normalization ( even if it is not necessary for tree-based models ;)","0aa181e1":"#### Submission :","7e1dece1":"So strenge!   Our target seems to be a target of a classification task! But indeed we are working with RMSE as our competition's metric wich is for regression tasks.","536ac39d":"## EDA :"}}