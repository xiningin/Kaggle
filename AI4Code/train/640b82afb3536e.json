{"cell_type":{"f7633a88":"code","8125e3ee":"code","2b08bfce":"code","c7d49fc2":"code","6b95e3f5":"code","ca8a0713":"code","288483a9":"code","939446e0":"code","a10ab5b2":"code","87842d7c":"code","92a1286e":"code","8a6605d3":"code","9188736c":"code","befa74aa":"code","ce342271":"code","03fd523b":"code","a1fa05f0":"code","e6752c57":"code","b764e664":"code","be608fd7":"markdown","db763fec":"markdown","e3cf5c45":"markdown"},"source":{"f7633a88":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\nfrom tensorflow import keras\nfrom sklearn.preprocessing import LabelBinarizer\nimport cv2\n\ntf.config.experimental.list_physical_devices('GPU') \nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        os.path.join(dirname, filename)\n","8125e3ee":"# Check for available GPU\ndevice_name = tf.test.gpu_device_name()\nif device_name != '\/device:GPU:0':\n    raise SystemError('GPU not found')\nprint('Found GPU at: {}'.format(device_name))","2b08bfce":"from PIL import Image        # for image processing","c7d49fc2":"with tf.device('\/gpu:0'):\n    image_set = []\n    label_set = []\n    path = '..\/input\/flowers-recognition\/flowers'\n    for flower_type in os.listdir(path):\n        subpath = os.path.join(path, flower_type)\n        for img in os.listdir(subpath):\n            try:\n                flower_pic = os.path.join(subpath,img)\n                image = cv2.imread(flower_pic)\n                image = cv2.resize(image, (224,224))\n                image_set.append(image)\n                label_set.append(flower_type)\n            except Exception as e:           # To remove problematic pictures and prevent the program from encountering errors\n                print(str(e))","6b95e3f5":"print(len(label_set))","ca8a0713":"# just visualizing a random picture\n\nprint(label_set[764])\nplt.imshow(image_set[764])","288483a9":"image_set = np.array(image_set)\nlabel_set = pd.Series(label_set)","939446e0":"image_set.shape","a10ab5b2":"label_set.shape","87842d7c":"label_set.head()","92a1286e":"label_set.unique()","8a6605d3":"label_set = label_set.map({'daisy':1, 'sunflower':2, 'tulip':3, 'rose':4, 'dandelion':5})\nlabel_set.head()","9188736c":"label_set = pd.DataFrame(label_set) # to convert the shape (4323,) to (4323,1)\nlabel_set.shape","befa74aa":"from sklearn.model_selection import train_test_split","ce342271":"train_x, test_x, train_y, test_y = train_test_split(image_set, label_set, test_size=0.2, random_state=37)\nprint(len(train_x), len(train_y), len(test_x), len(test_y))","03fd523b":"# One vs all classification\nlabel_binrizer = LabelBinarizer()\ntrain_y = label_binrizer.fit_transform(train_y)","a1fa05f0":"def vgg16(images, labels):\n    with tf.device('\/gpu:0'):\n        class myCallback(tf.keras.callbacks.Callback):        # interrupts the training when 99.9% is achieved\n            def on_epoch_end(self, epoch, logs={}):\n                if(float(logs.get('accuracy'))>0.999):\n                    print(\"\\nReached 99.9% accuracy so cancelling training!\")\n                    self.model.stop_training = True\n                \n        callbacks = myCallback()\n        model = tf.keras.models.Sequential([\n            # Layer 1\n            tf.keras.layers.Conv2D(64,(3,3),padding='same',activation=tf.nn.relu,input_shape=(224,224,3)),\n            tf.keras.layers.Conv2D(64,(3,3),padding='same',activation=tf.nn.relu),\n            tf.keras.layers.MaxPooling2D(2,2),\n            # Layer 2\n            tf.keras.layers.Conv2D(128,(3,3),padding='same',activation=tf.nn.relu),\n            tf.keras.layers.Conv2D(128,(3,3),padding='same',activation=tf.nn.relu),\n            tf.keras.layers.MaxPooling2D(2,2),\n            # Layer 3\n            tf.keras.layers.Conv2D(256,(3,3),padding='same',activation=tf.nn.relu),\n            tf.keras.layers.Conv2D(256,(3,3),padding='same',activation=tf.nn.relu),\n            tf.keras.layers.Conv2D(256,(3,3),padding='same',activation=tf.nn.relu),\n            tf.keras.layers.MaxPooling2D(2,2),\n            # Layer 4\n            tf.keras.layers.Conv2D(512,(3,3),padding='same',activation=tf.nn.relu),\n            tf.keras.layers.Conv2D(512,(3,3),padding='same',activation=tf.nn.relu),\n            tf.keras.layers.Conv2D(512,(3,3),padding='same',activation=tf.nn.relu),\n            tf.keras.layers.MaxPooling2D(2,2),\n            # Layer 5\n            tf.keras.layers.Conv2D(512,(3,3),padding='same',activation=tf.nn.relu),\n            tf.keras.layers.Conv2D(512,(3,3),padding='same',activation=tf.nn.relu),\n            tf.keras.layers.Conv2D(512,(3,3),padding='same',activation=tf.nn.relu),\n            tf.keras.layers.MaxPooling2D(2,2),\n            \n            tf.keras.layers.Flatten(),\n            # FC 1\n            tf.keras.layers.Dense(25088, activation = tf.nn.relu),\n            # FC 2\n            tf.keras.layers.Dense(4096, activation = tf.nn.relu),\n            # FC 3\n            tf.keras.layers.Dense(4096, activation = tf.nn.relu),\n            # Softmax Layer\n            tf.keras.layers.Dense(5, activation = tf.nn.softmax),\n        ])\n\n        model.compile(optimizer = keras.optimizers.Adam(learning_rate=0.0001), loss = keras.losses.categorical_crossentropy, metrics = ['accuracy'])\n        model.summary()\n        hist = model.fit(images, labels, epochs = 40,  callbacks=[callbacks])\n    return model","e6752c57":"train_x = train_x\/255\ntest_x = test_x\/255\nmodel = vgg16(train_x, train_y)","b764e664":"from sklearn.metrics import accuracy_score\npred_y = model.predict(test_x)\ntest_y = label_binrizer.fit_transform(test_y) # One vs all classification\nprint('VGG-16 test accuracy: ',accuracy_score(test_y, pred_y.round()))","be608fd7":"**Quoting the VGG-16 research paper by Karen Simoyan and Andrew Zisserman**\n\nDuring training, the input to our ConvNets is a fixed-size 224 \u00d7 224 RGB image. The only preprocessing we do is subtracting the mean RGB value, computed on the training set, from each pixel.\nThe image is passed through a stack of convolutional (conv.) layers, where we use filters with a very\nsmall receptive field: 3 \u00d7 3 (which is the smallest size to capture the notion of left\/right, up\/down,\ncenter). In one of the configurations we also utilise 1 \u00d7 1 convolution filters, which can be seen as\na linear transformation of the input channels (followed by non-linearity). The convolution stride is\nfixed to 1 pixel; the spatial padding of conv. layer input is such that the spatial resolution is preserved\nafter convolution, i.e. the padding is 1 pixel for 3 \u00d7 3 conv. layers. Spatial pooling is carried out by\nfive max-pooling layers, which follow some of the conv. layers (not all the conv. layers are followed\nby max-pooling). Max-pooling is performed over a 2 \u00d7 2 pixel window, with stride 2.\nA stack of convolutional layers (which has a different depth in different architectures) is followed by\nthree Fully-Connected (FC) layers: the first two have 4096 channels each, the third performs 1000-\nway ILSVRC classification and thus contains 1000 channels (one for each class). The final layer is\nthe soft-max layer. The configuration of the fully connected layers is the same in all networks.\nAll hidden layers are equipped with the rectification (ReLU (Krizhevsky et al., 2012)) non-linearity\n\nnote: I've implemented the exact same architecture except the 1x1 convolution configuration. Also, the 1000 unit softmax layer is replaced by a 5 unit softmax layer in this program for the 5 given flower classes","db763fec":"**VGG-16 Implementation**\n\nI've tried to implement the VGG-16 CNN architecture on the dataset of flowers of 5 different classes(types) to successfully recognise given flower at testing.","e3cf5c45":"![image.png](attachment:image.png)"}}