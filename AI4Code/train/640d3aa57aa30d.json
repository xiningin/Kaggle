{"cell_type":{"6ba7b49c":"code","54ecb2b1":"code","ccd85d13":"code","488f0521":"code","5fa7f249":"code","f36cffcf":"code","78566eee":"code","64d85c99":"code","2a55d452":"code","7c427434":"code","b30610b2":"code","51dc57ce":"code","b20b9609":"code","c6036ab5":"code","f70ea591":"code","6439ca64":"code","6e9b257f":"code","3485116d":"markdown","2905df03":"markdown","fa8bc157":"markdown","2d82e817":"markdown","3c8b6f34":"markdown","3e677f41":"markdown","e6201529":"markdown","0d33a204":"markdown","ac9331e3":"markdown","d142f1c7":"markdown","e4708f33":"markdown","8710a7d7":"markdown","843b630d":"markdown","ea8996a1":"markdown","6766a1e2":"markdown","3e593520":"markdown"},"source":{"6ba7b49c":"CLUENER_DATASET_DIR = \"..\/input\/business-privacy-identify1\/Business_Privacy_Identify\/data\/clue\"\n\n# \u6570\u636e\u96c6\u8def\u5f84\nDATASET_DIR = CLUENER_DATASET_DIR\n\n# BERT \u6a21\u578b   \"bert-base-chinese\"\nBERT_MODEL_NAME = \"hfl\/chinese-roberta-wwm-ext\"\n","54ecb2b1":"import torch\nimport warnings\nimport torch.nn as nn\nimport numpy as np\nimport json\n\nfrom torch import Tensor\nfrom typing import List, Dict\nfrom dataclasses import dataclass, field\nfrom torch.nn.utils.rnn import pad_sequence\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers.file_utils import logger, logging\nfrom transformers.trainer_utils import EvalPrediction\nfrom transformers.modeling_outputs import TokenClassifierOutput\nfrom sklearn.metrics import f1_score, precision_score, recall_score\nfrom transformers import TrainingArguments, Trainer, BertTokenizer, BertModel, BertPreTrainedModel\n\nwarnings.filterwarnings(\"ignore\")","ccd85d13":"@dataclass\nclass ModelArguments:\n    use_lstm: bool = field(default=True, metadata={\"help\": \"\u662f\u5426\u4f7f\u7528LSTM\"})\n    lstm_hidden_size: int = field(default=500, metadata={\"help\": \"LSTM\u9690\u85cf\u5c42\u8f93\u51fa\u7684\u7ef4\u5ea6\"})\n    lstm_layers: int = field(default=1, metadata={\"help\": \"\u5806\u53e0LSTM\u7684\u5c42\u6570\"})\n    lstm_dropout: float = field(default=0.5, metadata={\"help\": \"LSTM\u7684dropout\"})\n    hidden_dropout: float = field(default=0.5, metadata={\"help\": \"\u9884\u8bad\u7ec3\u6a21\u578b\u8f93\u51fa\u5411\u91cf\u8868\u793a\u7684dropout\"})\n    ner_num_labels: int = field(default=34, metadata={\"help\": \"\u9700\u8981\u9884\u6d4b\u7684\u6807\u7b7e\u6570\u91cf\"})\n\n\n@dataclass\nclass OurTrainingArguments:\n    checkpoint_dir: str = field(default=\".\/models\/checkpoints\", metadata={\"help\": \"\u8bad\u7ec3\u8fc7\u7a0b\u4e2d\u7684checkpoints\u7684\u4fdd\u5b58\u8def\u5f84\"})\n    best_dir: str = field(default=\".\/models\/best\", metadata={\"help\": \"\u6700\u4f18\u6a21\u578b\u7684\u4fdd\u5b58\u8def\u5f84\"})\n    do_eval: bool = field(default=True, metadata={\"help\": \"\u662f\u5426\u5728\u8bad\u7ec3\u65f6\u8fdb\u884c\u8bc4\u4f30\"})\n    do_predict: bool = field(default=True, metadata={\"help\": \"\u662f\u5426\u5728\u8bad\u7ec3\u65f6\u8fdb\u884c\u9884\u6d4b\"})\n    epoch: int = field(default=5, metadata={\"help\": \"\u8bad\u7ec3\u7684epoch\"})\n    train_batch_size: int = field(default=8, metadata={\"help\": \"\u8bad\u7ec3\u65f6\u7684batch size\"})\n    eval_batch_size: int = field(default=8, metadata={\"help\": \"\u8bc4\u4f30\u65f6\u7684batch size\"})\n    bert_model_name: str = field(default=BERT_MODEL_NAME, metadata={\"help\": \"BERT\u6a21\u578b\u540d\u79f0\"})\n\n\n@dataclass\nclass DataArguments:\n    train_file: str = field(default=DATASET_DIR +\"\/train.json\", metadata={\"help\": \"\u8bad\u7ec3\u6570\u636e\u7684\u8def\u5f84\"})\n    dev_file: str = field(default=DATASET_DIR +\"\/dev.json\", metadata={\"help\": \"\u6d4b\u8bd5\u6570\u636e\u7684\u8def\u5f84\"})\n    test_file: str = field(default=DATASET_DIR +\"\/test.json\", metadata={\"help\": \"\u6d4b\u8bd5\u6570\u636e\u7684\u8def\u5f84\"})","488f0521":"@dataclass\nclass Example:\n    text: List[str] # ner\u7684\u6587\u672c\n    label: List[str] = None # ner\u7684\u6807\u7b7e\n\n    def __post_init__(self):\n        if self.label:\n            assert len(self.text) == len(self.label)","5fa7f249":"# \u8bfb\u53d6\u6570\u636e\u96c6:json \u683c\u5f0f\ndef read_json(input_file):\n    \"\"\"read dataset \"\"\"\n    lines = []\n    with open(input_file, 'r') as f:\n        for line in f:\n            line = json.loads(line.strip())\n            text = line['text']\n            label_entities = line.get('label', None)\n            words = list(text)\n            labels = ['O'] * len(words)\n            if label_entities is not None:\n                for key, value in label_entities.items():\n                    for sub_name, sub_index in value.items():\n                        for start_index, end_index in sub_index:\n                            assert ''.join(words[start_index:end_index + 1]) == sub_name\n                            if start_index == end_index:\n                                labels[start_index] = 'S-' + key\n                            else:\n                                labels[start_index] = 'B-' + key\n                                labels[start_index + 1:end_index + 1] = ['I-' + key] * (len(sub_name) - 1)\n            lines.append({\"words\": words, \"labels\": labels})\n    return lines\n\ndef read_dataset_json(input_file):\n    \"\"\" \u8bfb\u53d6\u6570\u636e\u96c6:json \u683c\u5f0f  \"\"\"\n    examples = []\n    lines = read_json(input_file)\n    for line in lines:\n        examples.append(Example(line[\"words\"], line[\"labels\"]))\n\n    return examples\n\n# \u8bfb\u53d6\u6570\u636e\u96c6:json \u683c\u5f0f\ndef read_dataset_txt(input_file):\n    \"\"\"read dataset \"\"\"\n    examples = []\n    with open(input_file, \"r\", encoding=\"utf-8\") as file:\n        text = []\n        label = []\n        for line in file:\n            line = line.strip()\n            # \u4e00\u6761\u6587\u672c\u7ed3\u675f\n            if len(line) == 0:\n                examples.append(Example(text, label))\n                text = []\n                label = []\n                continue\n            text.append(line.split()[0])\n            label.append(line.split()[1])\n    return examples","f36cffcf":"\n\ndef read_data(path, data_type=\"json\"):\n    examples = None\n    if data_type == 'txt':\n        examples = read_dataset_txt(path)\n    elif data_type == \"json\":\n        examples = read_dataset_json(path)\n\n    return examples\n\ntrain_data = read_data(DATASET_DIR +\"\/train.json\")\neval_data = read_data(DATASET_DIR +\"\/dev.json\")\nprint(train_data[0])\n\n\nfor i in range(10):\n  print(train_data[i])","78566eee":"def get_labels_from_list():\n    \"CLUENER TAGS\"\n    return [\"<pad>\", \"B-address\", \"B-book\", \"B-company\", 'B-game', 'B-government', 'B-movie', 'B-name',\n            'B-organization', 'B-position', 'B-scene', \"I-address\",\n            \"I-book\", \"I-company\", 'I-game', 'I-government', 'I-movie', 'I-name',\n            'I-organization', 'I-position', 'I-scene',\n            \"S-address\", \"S-book\", \"S-company\", 'S-game', 'S-government', 'S-movie',\n            'S-name', 'S-organization', 'S-position',\n            'S-scene', 'O', \"<start>\", \"<eos>\"]\n\n\ndef load_tag_from_file(path):\n    with open(path, \"r\", encoding=\"utf-8\") as file:\n        lines = file.readlines()\n        tag2id = {tag.strip(): idx for idx, tag in enumerate(lines)}\n        id2tag = {idx: tag for tag, idx in tag2id.items()}\n    return tag2id, id2tag\n\n\ndef load_tag(path=None):\n    if path is not None:\n        tag2id, id2tag = load_tag_from_file(path)\n    else:\n        tags = get_labels_from_list()\n\n        id2tag = {i: label for i, label in enumerate(tags)}\n        tag2id = {label: i for i, label in enumerate(tags)}\n\n    return tag2id, id2tag\n\n\ntag2id, id2tag = load_tag()\nprint(tag2id)\nprint(id2tag)","64d85c99":"tokenizer = BertTokenizer.from_pretrained(BERT_MODEL_NAME)","2a55d452":"class NERDataset(Dataset):\n    def __init__(self, examples: List[Example], max_length=128,tokenizer=BertTokenizer.from_pretrained(BERT_MODEL_NAME)):\n        self.max_length = 512 if max_length > 512 else max_length\n        \"\"\"\n        1. \u5c06\u6587\u672c\u7684\u957f\u5ea6\u63a7\u5236\u5728max_length - 2\uff0c\u51cf2\u7684\u539f\u56e0\u662f\u4e3a[CLS]\u548c[SEP]\u7a7a\u51fa\u4f4d\u7f6e\uff1b \n        2. \u5c06\u6587\u672c\u8f6c\u6362\u4e3aid\u5e8f\u5217\uff1b\n        3. \u5c06id\u5e8f\u5217\u8f6c\u6362\u4e3aTensor\uff1b\n        \"\"\"\n        self.texts = [torch.LongTensor(tokenizer.encode(example.text[: self.max_length - 2])) for example in examples]\n        self.labels = []\n        for example in examples:\n            label = example.label\n            \"\"\"\n            1. \u5c06\u5b57\u7b26\u7684label\u8f6c\u6362\u4e3a\u5bf9\u4e8e\u7684id\uff1b\n            2. \u63a7\u5236label\u7684\u6700\u957f\u957f\u5ea6\uff1b\n            3. \u6dfb\u52a0\u5f00\u59cb\u4f4d\u7f6e\u548c\u7ed3\u675f\u4f4d\u7f6e\u5bf9\u5e94\u7684\u6807\u7b7e\uff0c\u8fd9\u91cc<start>\u5bf9\u5e94\u8f93\u5165\u4e2d\u7684[CLS],<eos>\u5bf9\u4e8e[SEP]\uff1b\n            4. \u8f6c\u6362\u4e3aTensor\uff1b\n            \"\"\"\n            label = [tag2id[\"<start>\"]] + [tag2id[l] for l in label][: self.max_length - 2] + [tag2id[\"<eos>\"]]\n            self.labels.append(torch.LongTensor(label))\n        assert len(self.texts) == len(self.labels)\n        for text, label in zip(self.texts, self.labels):\n            assert len(text) == len(label)\n\n    def __len__(self):\n        return len(self.texts)\n\n    def __getitem__(self, item):\n        return {\n            \"input_ids\": self.texts[item],\n            \"labels\": self.labels[item]\n        }\n\ntrain_dataset = NERDataset(train_data)\neval_dataset = NERDataset(eval_data)\nprint(train_dataset[0])","7c427434":"def collate_fn(features) -> Dict[str, Tensor]:\n    batch_input_ids = [feature[\"input_ids\"] for feature in features]\n    batch_labels = [feature[\"labels\"] for feature in features]\n    batch_attentiton_mask = [torch.ones_like(feature[\"input_ids\"]) for feature in features]\n    # padding\n    batch_input_ids = pad_sequence(batch_input_ids, batch_first=True, padding_value=tokenizer.pad_token_id)\n    batch_labels = pad_sequence(batch_labels, batch_first=True, padding_value=tag2id[\"<pad>\"])\n    batch_attentiton_mask = pad_sequence(batch_attentiton_mask, batch_first=True, padding_value=0)\n    assert batch_input_ids.shape == batch_labels.shape\n    return {\"input_ids\": batch_input_ids, \"labels\": batch_labels, \"attention_mask\": batch_attentiton_mask}","b30610b2":"dataloader = DataLoader(train_dataset, shuffle=True, batch_size=2, collate_fn=collate_fn)\nbatch = next(iter(dataloader))\nprint(batch.keys())\nprint(type(batch[\"input_ids\"]))\nprint(batch[\"input_ids\"].shape)\nprint(type(batch[\"labels\"]))\nprint(batch[\"labels\"].shape)\nprint(type(batch[\"attention_mask\"]))\nprint(batch[\"attention_mask\"].shape)","51dc57ce":"\ndef ner_metrics(eval_output: EvalPrediction) -> Dict[str, float]:\n    \"\"\"\n    \u8be5\u51fd\u6570\u662f\u56de\u8c03\u51fd\u6570\uff0cTrainer\u4f1a\u5728\u8fdb\u884c\u8bc4\u4f30\u65f6\u8c03\u7528\u8be5\u51fd\u6570\u3002\n    (\u5982\u679c\u4f7f\u7528Pycharm\u7b49IDE\u8fdb\u884c\u8c03\u8bd5\uff0c\u53ef\u4ee5\u4f7f\u7528\u65ad\u70b9\u7684\u65b9\u6cd5\u6765\u8c03\u8bd5\u8be5\u51fd\u6570\uff0c\u8be5\u51fd\u6570\u5728\u8fdb\u884c\u8bc4\u4f30\u65f6\u88ab\u8c03\u7528)\n    \"\"\"\n    preds = eval_output.predictions\n    preds = np.argmax(preds, axis=-1).flatten()\n    labels = eval_output.label_ids.flatten()\n    # labels\u4e3a0\u8868\u793a\u4e3a<pad>\uff0c\u56e0\u6b64\u8ba1\u7b97\u65f6\u9700\u8981\u53bb\u6389\u8be5\u90e8\u5206\n    mask = labels != 0\n    preds = preds[mask]\n    labels = labels[mask]\n    metrics = dict()\n    metrics[\"f1\"] = f1_score(labels, preds, average=\"macro\")\n    metrics[\"precision\"] = precision_score(labels, preds, average=\"macro\")\n    metrics[\"recall\"] = recall_score(labels, preds, average=\"macro\")\n    # \u5fc5\u987b\u4ee5\u5b57\u5178\u7684\u5f62\u5f0f\u8fd4\u56de\uff0c\u540e\u9762\u4f1a\u7528\u5230\u5b57\u5178\u7684key\n    return metrics","b20b9609":"class BertForNER(BertPreTrainedModel):\n    def __init__(self, config, *model_args, **model_kargs):\n        super().__init__(config) # \u521d\u59cb\u5316\u7236\u7c7b(\u5fc5\u8981\u7684\u6b65\u9aa4)\n        if \"model_args\" in model_kargs:\n            model_args = model_kargs[\"model_args\"]\n            \"\"\"\n            \u5fc5\u987b\u5c06\u989d\u5916\u7684\u53c2\u6570\u66f4\u65b0\u81f3self.config\u4e2d\uff0c\u8fd9\u6837\u5728\u8c03\u7528save_model\u4fdd\u5b58\u6a21\u578b\u65f6\u624d\u4f1a\u5c06\u8fd9\u4e9b\u53c2\u6570\u4fdd\u5b58\uff1b\n            \u8fd9\u79cd\u5728\u4f7f\u7528from_pretrained\u65b9\u6cd5\u52a0\u8f7d\u6a21\u578b\u65f6\u624d\u4e0d\u4f1a\u51fa\u9519\uff1b\n            \"\"\"\n            self.config.__dict__.update(model_args.__dict__)\n        self.num_labels = self.config.ner_num_labels\n        self.bert = BertModel(config, add_pooling_layer=False)\n        self.dropout = nn.Dropout(self.config.hidden_dropout)\n        self.lstm = nn.LSTM(self.config.hidden_size, # \u8f93\u5165\u7684\u7ef4\u5ea6\n                            self.config.lstm_hidden_size, # \u8f93\u51fa\u7ef4\u5ea6\n                            num_layers=self.config.lstm_layers, # \u5806\u53e0lstm\u7684\u5c42\u6570\n                            dropout=self.config.lstm_dropout,\n                            bidirectional=True, # \u662f\u5426\u53cc\u5411\n                            batch_first=True)\n        if self.config.use_lstm:\n            self.classifier = nn.Linear(self.config.lstm_hidden_size * 2, self.num_labels)\n        else:\n            self.classifier = nn.Linear(self.config.hidden_size, self.num_labels)\n        self.init_weights()\n\n    def forward(\n            self,\n            input_ids=None,\n            attention_mask=None,\n            token_type_ids=None,\n            position_ids=None,\n            head_mask=None,\n            inputs_embeds=None,\n            labels=None,\n            pos=None,\n            output_attentions=None,\n            output_hidden_states=None,\n            return_dict=None,\n    ):\n        return_dict = return_dict if return_dict is not None else self.config.use_return_dict\n        outputs = self.bert(\n            input_ids,\n            attention_mask=attention_mask,\n            token_type_ids=token_type_ids,\n            position_ids=position_ids,\n            head_mask=head_mask,\n            inputs_embeds=inputs_embeds,\n            output_attentions=output_attentions,\n            output_hidden_states=output_hidden_states,\n            return_dict=return_dict,\n        )\n        sequence_output = self.dropout(outputs[0])\n        if self.config.use_lstm:\n            sequence_output, _ = self.lstm(sequence_output)\n        logits = self.classifier(sequence_output)\n\n        loss = None\n        if labels is not None:\n            loss_fct = nn.CrossEntropyLoss()\n            # \u5982\u679cattention_mask\u4e0d\u4e3a\u7a7a\uff0c\u5219\u53ea\u8ba1\u7b97attention_mask\u4e2d\u4e3a1\u90e8\u5206\u7684Loss\n            if attention_mask is not None:\n                active_loss = attention_mask.view(-1) == 1\n                active_logits = logits.view(-1, self.num_labels)\n                active_labels = torch.where(\n                    active_loss, labels.view(-1), torch.tensor(loss_fct.ignore_index).type_as(labels)\n                )\n                loss = loss_fct(active_logits, active_labels)\n            else:\n                loss = loss_fct(logits.view(-1, self.num_labels), labels.view(-1))\n\n        if not return_dict:\n            output = (logits,) + outputs[2:]\n            return ((loss,) + output) if loss is not None else output\n\n        return TokenClassifierOutput(\n            loss=loss,\n            logits=logits, # \u8be5\u90e8\u5206\u5728\u8bc4\u4f30\u65f6\uff0c\u4f1a\u4f5c\u4e3aEvalPrediction\u5bf9\u8c61\u7684predictions\u8fdb\u884c\u8fd4\u56de\n            hidden_states=outputs.hidden_states,\n            attentions=outputs.attentions,\n        )","c6036ab5":"model_args = ModelArguments(use_lstm=True)\nmodel = BertForNER.from_pretrained(BERT_MODEL_NAME, model_args=model_args)\noutput = model(**batch)\nprint(type(output))\nprint(output.loss)\nprint(output.logits.shape)","f70ea591":"def run(model_args: ModelArguments, data_args: DataArguments, args: OurTrainingArguments):\n    # \u8bbe\u5b9a\u8bad\u7ec3\u53c2\u6570\n    training_args = TrainingArguments(output_dir=args.checkpoint_dir,  # \u8bad\u7ec3\u4e2d\u7684checkpoint\u4fdd\u5b58\u7684\u4f4d\u7f6e\n                                      num_train_epochs=args.epoch,\n                                      do_eval=args.do_eval,  # \u662f\u5426\u8fdb\u884c\u8bc4\u4f30\n                                      evaluation_strategy=\"epoch\",  # \u6bcf\u4e2aepoch\u7ed3\u675f\u540e\u8fdb\u884c\u8bc4\u4f30\n                                      per_device_train_batch_size=args.train_batch_size,\n                                      per_device_eval_batch_size=args.eval_batch_size,\n                                      load_best_model_at_end=True,  # \u8bad\u7ec3\u5b8c\u6210\u540e\u52a0\u8f7d\u6700\u4f18\u6a21\u578b\n                                      metric_for_best_model=\"f1\"  # \u8bc4\u4f30\u6700\u4f18\u6a21\u578b\u7684\u6307\u6807\uff0c\u8be5\u6307\u6807\u662fner_metrics\u8fd4\u56de\u8bc4\u4f30\u6307\u6807\u4e2d\u7684key\n                                      )\n     # \u6784\u5efa\u5206\u8bcd\u5668\n    tokenizer = BertTokenizer.from_pretrained(args.bert_model_name)\n    \n    # \u6784\u5efadataset\n    train_dataset = NERDataset(read_data(data_args.train_file), tokenizer=tokenizer)\n    eval_dataset = NERDataset(read_data(data_args.dev_file), tokenizer=tokenizer)\n    test_dataset = NERDataset(read_data(data_args.test_file), tokenizer=tokenizer)\n    \n    # \u52a0\u8f7d\u9884\u8bad\u7ec3\u6a21\u578b\n    model = BertForNER.from_pretrained(args.bert_model_name, model_args=model_args)\n    # \u521d\u59cb\u5316Trainer\n    trainer = Trainer(model=model,\n                      args=training_args,\n                      train_dataset=train_dataset,\n                      eval_dataset=eval_dataset,\n                      tokenizer=tokenizer,\n                      data_collator=collate_fn,\n                      compute_metrics=ner_metrics)\n    # \u6a21\u578b\u8bad\u7ec3\n    trainer.train()\n    # \u8bad\u7ec3\u5b8c\u6210\u540e\uff0c\u52a0\u8f7d\u6700\u4f18\u6a21\u578b\u5e76\u8fdb\u884c\u8bc4\u4f30\n    logger.info(trainer.evaluate(eval_dataset))\n    # \u4fdd\u5b58\u8bad\u7ec3\u597d\u7684\u6a21\u578b\n    trainer.save_model(args.best_dir)\n    \n    # \u8fdb\u884c\u9884\u6d4b\n    logger.info(trainer.predict(test_dataset))\n    \n\n","6439ca64":"def main():\n    # \u5b9a\u4e49\u5404\u7c7b\u53c2\u6570\u5e76\u8bad\u7ec3\u6a21\u578b\n    model_args = ModelArguments(use_lstm=True)\n    data_args = DataArguments()\n    training_args = OurTrainingArguments(bert_model_name=\"hfl\/chinese-roberta-wwm-ext\",epoch=5,\n                                         train_batch_size=128, eval_batch_size=128)\n    run(model_args, data_args, training_args)","6e9b257f":"main()","3485116d":"\u6784\u5efaDataset","2905df03":"\u6d4b\u8bd5\u4e00\u4e0b\u6a21\u578b\u662f\u5426\u7b26\u5408\u9884\u671f","fa8bc157":"#### \u672c\u7bc7\u535a\u5ba2\u5e0c\u671b\u5c55\u793a\u5982\u4f55\u57fa\u4e8etransformers\u63d0\u4f9b\u7684\u529f\u80fd\u8fdb\u884c\u6a21\u578b\u7684\u5f00\u53d1\uff0c\u51cf\u5c11\u4ee3\u7801\u91cf\uff0c\u63d0\u9ad8\u5f00\u53d1\u901f\u5ea6\u3002","2d82e817":"\u52a0\u8f7d\u6807\u7b7e\u6570\u636e\u5e76\u5206\u914d\u5bf9\u4e8e\u7684id","3c8b6f34":"\u5b9a\u4e49collate_fn\uff0ccollate_fn\u7684\u4f5c\u7528\u5728Dataloader\u751f\u6210batch\u6570\u636e\u65f6\u4f1a\u88ab\u8c03\u7528\u3002\n\u8fd9\u91cc\u7684\u4f5c\u7528\u662f\u5bf9\u6bcf\u4e2abatch\u8fdb\u884cpadding","3e677f41":"# \u5b9a\u4e49\u5168\u5c40\u53d8\u91cf","e6201529":"\u8fd9\u91cc\u5b9a\u4e49\u4e86\u4e00\u4e2a\u7528\u4e8e\u4fdd\u5b58\u6570\u636e\u7684\u6570\u636e\u7ed3\u6784\uff0c\u8fd9\u6837\u7684\u65b9\u6cd5\u80fd\u591f\u63d0\u9ad8\u4ee3\u7801\u7684\u53ef\u9605\u8bfb\u6027\u3002","0d33a204":"### \u56db\u3001\u5b9a\u4e49\u4e00\u4e2a\u8bc4\u4f30\u51fd\u6570","ac9331e3":"### \u4e00\u3001\u5b9a\u4e49\u53c2\u6570","d142f1c7":"### \u4e8c\u3001\u8bfb\u53d6\u6570\u636e","e4708f33":"\u6d4b\u8bd5\u4e00\u4e0bcollate_fn","8710a7d7":"### \u4e09\u3001\u6784\u5efaDataset\u548ccollate_fn","843b630d":"\u5b9a\u4e49\u5c06\u6587\u4ef6\u4e2d\u7684ner\u6570\u636e\u4fdd\u5b58\u4e3aExample\u5217\u8868\u7684\u51fd\u6570","ea8996a1":"### \u516d\u3001\u6a21\u578b\u8bad\u7ec3","6766a1e2":"### \u4e94\u3001\u6784\u5efa\u6a21\u578b\n+ \u81ea\u5b9a\u4e49\u7684\u6a21\u578b\u9700\u8981\u7ee7\u627fBertPreTrainedModel","3e593520":"\u8bfb\u53d6tokenizer"}}