{"cell_type":{"9ae7262b":"code","4b6712d2":"code","bc290102":"code","8332fb1c":"code","5b5e232b":"code","1be5551d":"code","7661773a":"code","77cf17ce":"code","1bc8e633":"code","bf4c2815":"code","6c9043f6":"code","75540dfb":"code","1f78d691":"code","e072a2f1":"code","c19d7b9d":"code","6bdbb541":"code","3f8bde7d":"code","a2149561":"code","76a9b01c":"code","6ef4df78":"code","395aa250":"code","11102842":"code","18eaae47":"code","23085c48":"code","b1a0e667":"code","bf3d0d00":"code","57d149d9":"code","dc8c5a4f":"code","79df9508":"code","59369ea3":"code","c131bdba":"code","d244535f":"code","9efa73a8":"code","047a2f68":"code","41dfa6c5":"code","aa850566":"code","b6476e43":"code","a2147953":"code","5bcc0f09":"code","9636d1b8":"code","be9e2495":"code","cef597a6":"code","66ea0c12":"code","5a2dd75a":"code","6baa5c81":"code","0662d3a1":"code","0740a37c":"code","58b8f52a":"code","c7d8075e":"code","170c0e7e":"code","d8465ebe":"code","65a49630":"code","4108baa7":"code","339dee17":"code","2f729ce6":"code","655dd16f":"code","0a7e5ee6":"code","0210179d":"code","2559ccc2":"code","c87aadce":"code","9507d6e0":"code","c44b7d76":"code","66a75bb7":"code","774b98cc":"code","f01e2f65":"code","3a7d14e1":"code","785d6819":"code","af417347":"code","48f0a88c":"code","6e6be2b8":"code","779825d1":"code","4bceb20e":"code","f71e6062":"code","16903b04":"code","3a3cbd8a":"code","c36cd6b1":"code","9ebad910":"code","de8d610d":"code","1b862795":"code","f962ad00":"code","0b0832f1":"code","ef95709f":"code","b680a892":"code","0596a13b":"code","b6f0b463":"code","0ea5b24a":"code","ff22e8af":"code","d9994ddd":"code","5a7d7198":"code","58a654d5":"code","eb522cb8":"code","d9523762":"code","964703aa":"code","16ac86c0":"code","96bc8ac3":"code","d15ff0b3":"code","55d34cd1":"code","034fa4c0":"code","7a086f5e":"code","7e8ae40c":"code","4346086b":"code","ee5545b7":"code","8693b7c4":"code","8e384005":"code","772563fa":"code","2986d08d":"code","eaf1b849":"code","92906f60":"code","632a7443":"code","ba8370a7":"code","bb24260d":"code","fe00397a":"code","6c63b32c":"code","28f9d888":"code","ecedb466":"markdown","63cb4aaf":"markdown","186aec16":"markdown","12a258bf":"markdown","569a2d35":"markdown","a80b36d3":"markdown","eae01c34":"markdown","cb5c0122":"markdown","1a88c36c":"markdown","ab6d056b":"markdown","43d3586f":"markdown","966d513d":"markdown","3abe365b":"markdown","fe28d869":"markdown","f0e0fd99":"markdown","723784ab":"markdown","2beb9885":"markdown","b864839a":"markdown","6a98b048":"markdown","bba6b5ed":"markdown","6e927ad8":"markdown","ea7dd8e4":"markdown","47d46672":"markdown","3e13a1e3":"markdown","9636182f":"markdown","1d2db434":"markdown","65b1daae":"markdown","57a00b9f":"markdown","360e3ef9":"markdown","10f53e05":"markdown","d50aae38":"markdown","aaee7f00":"markdown","55db19dc":"markdown","ed762842":"markdown"},"source":{"9ae7262b":"import warnings\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, \\\n    classification_report\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\n\nwarnings.simplefilter(action=\"ignore\")","4b6712d2":"df=pd.read_csv(\"..\/input\/diabetes-data-set\/diabetes.csv\")","bc290102":"df.shape\n","8332fb1c":"df[\"Outcome\"].value_counts() * 100 \/ len(df) # bagimli degiskenin siniflarinin oranlarin bakiyoruz...","5b5e232b":"def outcome_agg(col):\n    for i in col:  \n        print(df.groupby(\"Outcome\").agg({i: \"mean\"}))\n    ","1be5551d":"def get_cols(df, target):\n    cols = []\n    for col in df.columns:\n        if col!=target:\n            cols.append(col)\n    return cols","7661773a":"var_names=get_cols(df, \"Outcome\")\nvar_names","77cf17ce":"outcome_agg(var_names)","1bc8e633":"def get_cols2(df, target):\n    cols = [col for col in df.columns if col != target]\n    return cols","bf4c2815":"get_cols2(df, \"Outcome\")","6c9043f6":"df.groupby(\"Outcome\").agg({\"Pregnancies\": \"mean\"})","75540dfb":"df.groupby(\"Outcome\").agg({\"Glucose\": \"mean\"})","1f78d691":"df.groupby(\"Outcome\").agg({\"BloodPressure\": \"mean\"})","e072a2f1":"df.groupby(\"Outcome\").agg({\"SkinThickness\": \"mean\"})","c19d7b9d":"df.groupby(\"Outcome\").agg({\"Insulin\": \"mean\"})","6bdbb541":"df.groupby(\"Outcome\").agg({\"BMI\": \"mean\"})","3f8bde7d":"df.groupby(\"Outcome\").agg({\"DiabetesPedigreeFunction\": \"mean\"})","a2149561":"df.groupby(\"Outcome\").agg({\"Age\": \"mean\"})","76a9b01c":"df.describe([0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99]).T\n","6ef4df78":"sns.countplot(x='Outcome', data=df)\nplt.show()","395aa250":"df[\"Outcome\"].value_counts().plot.pie(autopct = \"%.1f\");","11102842":"df.describe([0.10, 0.25, 0.50, 0.75, 0.90, 0.95, 0.99]).T","18eaae47":"df.info()","23085c48":"df.Outcome.unique()","b1a0e667":"df.Outcome.value_counts()","bf3d0d00":"plt.figure(figsize=(6,4))\nsns.heatmap(df.corr(),cmap='Blues',annot=False);","57d149d9":"# NaN values of 0 for Glucose, Blood Pressure, Skin Thickness, Insulin, BMI\n# We can write Nan instead of 0\ncols = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]\nfor col in cols:\n    df[col].replace(0,np.NaN,inplace=True)","dc8c5a4f":"#Outcome correlation matrix\nk = 9 #number of variables for heatmap\ncols = df.corr().nlargest(k, 'Outcome')['Outcome'].index\ncm = df[cols].corr()\nplt.figure(figsize=(10,6))\nsns.heatmap(cm, annot=True, cmap = 'viridis');","79df9508":"# see how the data is distributed.\ndf.hist(figsize = (20,20));","59369ea3":"def c_dis_plot(df, cols):\n    for col in cols:\n        sns.distplot(df[col], hist=False)\n        plt.axvline(df[col].mean(),color='r',label='mean')\n        plt.axvline(np.median(df[col]),color='b',label='median')\n        plt.axvline((df[col].mode())[0],color='g',label='mode')\n        plt.legend()\n        plt.show();\n        ","c131bdba":"c_dis_plot(df, var_names)","d244535f":"# A scatter plot for show how two variables are related to each other\nsns.lmplot(\"BloodPressure\", \"Glucose\", df, hue='Outcome', fit_reg=False, height = 5)\nsns.lmplot(\"Glucose\", \"SkinThickness\", df, hue='Outcome', fit_reg=False, height = 5)\nsns.lmplot(\"Glucose\", \"Insulin\", df, hue='Outcome', fit_reg=False, height = 5)\nsns.lmplot(\"Glucose\", \"BMI\", df, hue='Outcome', fit_reg=False, height = 5)\nsns.lmplot(\"Glucose\", \"Age\", df, hue='Outcome', fit_reg=False, height = 5)\nsns.lmplot(\"Glucose\", \"DiabetesPedigreeFunction\", df, hue='Outcome', fit_reg=False, height = 5)\nsns.lmplot(\"Insulin\",\"BloodPressure\",df, hue='Outcome', fit_reg=False, height = 5)\nsns.lmplot(\"Age\", \"BloodPressure\", df, hue='Outcome', fit_reg=False, height = 5)\nsns.lmplot(\"BMI\", \"SkinThickness\", df, hue='Outcome', fit_reg=False, height = 5)","9efa73a8":"#Observation units for variables with a minimum value of zero are NaN, except for the pregnancy variable.\ndf.describe([0.05,0.25,0.50,0.75,0.90,0.95,0.99]).T","047a2f68":"# NaN values of 0 for Glucose, Blood Pressure, Skin Thickness, Insulin, BMI\n# We can write Nan instead of 0\ncols = [\"Glucose\", \"BloodPressure\", \"SkinThickness\", \"Insulin\", \"BMI\"]\nfor col in cols:\n    df[col].replace(0,np.NaN,inplace=True)","41dfa6c5":"# now we can see missing values\ndf.isnull().sum()","aa850566":"# We can fill in NaN values with a median according to the target\nfor col in df.columns:\n    df.loc[(df[\"Outcome\"]==0) & (df[col].isnull()),col] = df.loc[(df[\"Outcome\"]==0), col].median()\n    df.loc[(df[\"Outcome\"]==1) & (df[col].isnull()),col] = df.loc[(df[\"Outcome\"]==1), col].median()","b6476e43":"df.isnull().sum()","a2147953":"def outlier_thresholds(dataframe, variable):\n    quartile1 = dataframe[variable].quantile(0.10)\n    quartile3 = dataframe[variable].quantile(0.90)\n    interquantile_range = quartile3 - quartile1\n    up_limit = quartile3 + 1.5 * interquantile_range\n    low_limit = quartile1 - 1.5 * interquantile_range\n    return low_limit, up_limit","5bcc0f09":"def has_outliers(dataframe, variable):\n    low_limit, up_limit = outlier_thresholds(dataframe, variable)\n    if dataframe[(dataframe[variable] < low_limit) | (dataframe[variable] > up_limit)].any(axis=None):\n        print(variable, \"yes\")\n    print(variable, \"no\")","9636d1b8":"for col in df.columns:\n    has_outliers(df, col)","be9e2495":"def replace_with_thresholds(dataframe, numeric_columns):\n    for variable in numeric_columns:\n        low_limit, up_limit = outlier_thresholds(dataframe, variable)\n        dataframe.loc[(dataframe[variable] < low_limit), variable] = low_limit\n        dataframe.loc[(dataframe[variable] > up_limit), variable] = up_limit","cef597a6":"replace_with_thresholds(df, df.columns)","66ea0c12":"for col in df.columns:\n    has_outliers(df, col)","5a2dd75a":"df.describe([0.05,0.25,0.50,0.75,0.90,0.95,0.99]).T","6baa5c81":"df['New_Glucose_Class'] = pd.cut(x=df['Glucose'], bins=[0,139,200],labels = [\"Normal\",\"Prediabetes\"])","0662d3a1":"df['New_BMI_Range'] = pd.cut(x=df['BMI'], bins=[0,18.5,24.9,29.9,100],labels = [\"Underweight\",\"Healty\",\"Overweight\",\"Obese\"])","0740a37c":"df['New_BloodPressure'] = pd.cut(x=df['BloodPressure'], bins=[0,79,89,123],labels = [\"Normal\",\"HS1\",\"HS2\"])","58b8f52a":"df['New_SkinThickness'] = df['SkinThickness'].apply(lambda x: 1 if x <= 18.0 else 0)","c7d8075e":"df.head()","170c0e7e":"def one_hot_encoder(dataframe, categorical_columns, nan_as_category=False):\n    original_columns = list(dataframe.columns)\n    dataframe = pd.get_dummies(dataframe, columns=categorical_columns,\n                               dummy_na=nan_as_category, drop_first=True)\n    new_columns = [col for col in dataframe.columns if col not in original_columns]\n    return dataframe, new_columns","d8465ebe":"categorical_columns = [col for col in df.columns\n                           if len(df[col].unique()) <= 10\n                      and col != \"Outcome\"]\ncategorical_columns","65a49630":"df, new_cols_ohe = one_hot_encoder(df,categorical_columns)\nnew_cols_ohe","4108baa7":"df.head()","339dee17":"def robust_scaler(variable):\n    var_median = variable.median()\n    quartile1 = variable.quantile(0.25)\n    quartile3 = variable.quantile(0.75)\n    interquantile_range = quartile3 - quartile1\n    if int(interquantile_range) == 0:\n        quartile1 = variable.quantile(0.05)\n        quartile3 = variable.quantile(0.95)\n        interquantile_range = quartile3 - quartile1\n        if int(interquantile_range) == 0:\n            quartile1 = variable.quantile(0.10)\n            quartile3 = variable.quantile(0.99)\n            interquantile_range = quartile3 - quartile1\n            z = (variable - var_median) \/ interquantile_range\n            return round(z, 3)\n\n        z = (variable - var_median) \/ interquantile_range\n        return round(z, 3)\n    else:\n        z = (variable - var_median) \/ interquantile_range\n    return round(z, 3)","2f729ce6":"like_num = [col for col in df.columns if df[col].dtypes != 'O' and len(df[col].value_counts()) < 10]\ncols_need_scale = [col for col in df.columns if col not in new_cols_ohe\n                   and col not in \"Outcome\"\n                   and col not in like_num]\n\nfor col in cols_need_scale:\n    df[col] = robust_scaler(df[col])","655dd16f":"df.head()","0a7e5ee6":"# see how the data is distributed.\ndf.hist(figsize = (20,20));","0210179d":"df.info()","2559ccc2":"X = df.drop(\"Outcome\",axis=1)\ny = df[\"Outcome\"]","c87aadce":"import warnings\nimport matplotlib.pyplot as plt\nimport pandas as pd\nimport seaborn as sns\nfrom sklearn.preprocessing import RobustScaler\nimport numpy as np\nfrom lightgbm import LGBMClassifier\nfrom sklearn.ensemble import GradientBoostingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import accuracy_score, roc_auc_score, roc_curve, \\\n    classification_report\nfrom sklearn.model_selection import KFold\nfrom sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom xgboost import XGBClassifier\npd.set_option('display.max_columns',None)\npd.set_option('display.max_rows',None)\nwarnings.simplefilter(action=\"ignore\")","9507d6e0":"models = [('LR', LogisticRegression()),\n          ('KNN', KNeighborsClassifier()),\n          ('CART', DecisionTreeClassifier()),\n          ('RF', RandomForestClassifier()),\n          ('SVR', SVC(gamma='auto')),\n          ('XGBM', XGBClassifier()),\n          ('GB',GradientBoostingClassifier()),\n          (\"LightGBM\", LGBMClassifier())]\n\n# evaluate each model in turn\nresults = []\nnames = []\n\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=123456)\n    cv_results = cross_val_score(model, X, y, cv=10, scoring=\"accuracy\")\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","c44b7d76":"#Let's choose the highest 4 models\n# GBM\ngbm_model = GradientBoostingClassifier()\n# Model Tuning\ngbm_params = {\"learning_rate\": [0.01, 0.1, 0.001],\n               \"max_depth\": [3,5, 8, 10],\n               \"n_estimators\": [200, 500, 1000],\n               \"subsample\": [1, 0.5, 0.8]}\ngbm_cv_model = GridSearchCV(gbm_model,\n                            gbm_params,\n                            cv=10,\n                            n_jobs=-1,\n                            verbose=2).fit(X, y)\ngbm_cv_model.best_params_\n# Final Model\ngbm_tuned = GradientBoostingClassifier(**gbm_cv_model.best_params_).fit(X,y)","66a75bb7":"# LightGBM: \nlgb_model = LGBMClassifier()\n# Model Tuning\nlgbm_params = lgbm_params = {\"learning_rate\": [0.01, 0.5, 1],\n                             \"n_estimators\": [200, 500, 1000],\n                             \"max_depth\": [6, 8, 10],\n                             \"colsample_bytree\": [1, 0.5, 0.4]}\nlgbm_cv_model = GridSearchCV(lgb_model,\n                             lgbm_params,\n                             cv=10,\n                             n_jobs=-1,\n                             verbose=2).fit(X, y)\nlgbm_cv_model.best_params_\n# Final Model\nlgbm_tuned = LGBMClassifier(**lgbm_cv_model.best_params_).fit(X, y)","774b98cc":"# Random Forests:\nrf_model = RandomForestClassifier()\n# Model Tuning\nrf_params = {\"max_depth\": [5,10,None],\n            \"max_features\": [2,5,10],\n            \"n_estimators\": [100, 500, 900],\n            \"min_samples_split\": [2,10,30]}\nrf_cv_model = GridSearchCV(rf_model, \n                           rf_params, \n                           cv=10, \n                           n_jobs=-1, \n                           verbose=2).fit(X, y)\nrf_cv_model.best_params_\n# Final Model\nrf_tuned = RandomForestClassifier(**rf_cv_model.best_params_).fit(X, y)","f01e2f65":"# evaluate each model in turn\nmodels = [('RF', rf_tuned),\n          ('GBM',gbm_tuned )]\nresults = []\nnames = []\nfor name, model in models:\n    kfold = KFold(n_splits=10, random_state=123456)\n    cv_results = cross_val_score(model, X, y, cv=10, scoring=\"accuracy\")\n    results.append(cv_results)\n    names.append(name)\n    msg = \"%s: %f (%f)\" % (name, cv_results.mean(), cv_results.std())\n    print(msg)","3a7d14e1":"kfold = KFold(n_splits=10, random_state=123456)\ncv_results = cross_val_score(gbm_tuned, X, y, cv=10, scoring=\"accuracy\")\nmsg = \"%s: %f (%f)\" % (\"gbm\", cv_results.mean(), cv_results.std())\nprint(msg)","785d6819":"log_model = LogisticRegression().fit(X, y)\n","af417347":"log_model.intercept_\n","48f0a88c":"log_model.coef_","6e6be2b8":"log_model.predict(X)","779825d1":"log_model.predict(X)[0:10]\n","4bceb20e":"y[0:10]","f71e6062":"log_model.predict_proba(X)[0:10]\n","16903b04":"y_pred = log_model.predict(X)\n","3a3cbd8a":"accuracy_score(y, y_pred)","c36cd6b1":"cross_val_score(log_model, X, y, cv=10)","9ebad910":"cross_val_score(log_model, X, y, cv=10).mean()","de8d610d":"print(classification_report(y, y_pred))","1b862795":"logit_roc_auc = roc_auc_score(y, log_model.predict(X))\nfpr, tpr, thresholds = roc_curve(y, log_model.predict_proba(X)[:, 1])\nplt.figure()\n","f962ad00":"plt.plot(fpr, tpr, label='AUC (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1], 'r--')\n\n\n","0b0832f1":"logit_roc_auc = roc_auc_score(y, log_model.predict(X))\nfpr, tpr, thresholds = roc_curve(y, log_model.predict_proba(X)[:, 1])\nplt.figure()\nplt.plot(fpr, tpr, label='AUC (area = %0.2f)' % logit_roc_auc)\nplt.plot([0, 1], [0, 1], 'r--')\nplt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])\nplt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()\n","ef95709f":"plt.xlim([0.0, 1.0])\nplt.ylim([0.0, 1.05])","b680a892":"plt.xlabel('False Positive Rate')\nplt.ylabel('True Positive Rate')\nplt.title('Receiver operating characteristic')\nplt.legend(loc=\"lower right\")\nplt.savefig('Log_ROC')\nplt.show()\n","0596a13b":"rf_model = RandomForestClassifier(random_state=12345).fit(X, y)\n","b6f0b463":"\ncross_val_score(rf_model, X, y, cv=10).mean()\n\n","0ea5b24a":"rf_params = {\"n_estimators\": [200, 500],\n             \"max_features\": [5, 7],\n             \"min_samples_split\": [5, 10],\n             \"max_depth\": [5, None]}\n\n","ff22e8af":"rf_model = RandomForestClassifier(random_state=12345)","d9994ddd":"gs_cv = GridSearchCV(rf_model,\n                     rf_params,\n                     cv=10,\n                     n_jobs=-1,\n                     verbose=2).fit(X, y)\n\ngs_cv.best_params_","5a7d7198":"rf_tuned = RandomForestClassifier(**gs_cv.best_params_)","58a654d5":"cross_val_score(rf_tuned, X, y, cv=10).mean()","eb522cb8":"lgbm = LGBMClassifier(random_state=12345)\n","d9523762":"cross_val_score(lgbm, X, y, cv=10).mean()","964703aa":"lgbm_params = {\"learning_rate\": [0.01],\n               \"n_estimators\": [100],\n               \"max_depth\": [3, 5]}","16ac86c0":"gs_cv = GridSearchCV(lgbm,         # Try all the above parameters. Whichever parameters give the best results,  \n                     lgbm_params,  # fit the model with those parameters.\n                     cv=5,\n                     n_jobs=-1,\n                     verbose=2).fit(X, y)","96bc8ac3":"lgbm_tuned = LGBMClassifier(**gs_cv.best_params_).fit(X, y)\ncross_val_score(lgbm_tuned, X, y, cv=10).mean()","d15ff0b3":"rf_model = RandomForestClassifier(random_state=12345).fit(X, y)\n\n\n","55d34cd1":"cross_val_score(rf_model, X, y, cv=10).mean()\n\n","034fa4c0":"rf_params = {\"n_estimators\": [200, 500],\n             \"max_features\": [5, 7],\n             \"min_samples_split\": [5, 10],\n             \"max_depth\": [5, None]}\n\nrf_model = RandomForestClassifier(random_state=12345)\n\n\n\n","7a086f5e":"gs_cv = GridSearchCV(rf_model,\n                     rf_params,\n                     cv=10,\n                     n_jobs=-1,\n                     verbose=2).fit(X, y)\n\ngs_cv.best_params_","7e8ae40c":"rf_tuned = RandomForestClassifier(**gs_cv.best_params_)\ncross_val_score(rf_tuned, X, y, cv=10).mean()","4346086b":"import numpy as np\nimport pandas as pd\nimport os\nimport matplotlib.pyplot as plt\nfrom sklearn.cluster import KMeans\nfrom sklearn.preprocessing import MinMaxScaler\nfrom yellowbrick.cluster import KElbowVisualizer\nfrom scipy.cluster.hierarchy import linkage\nfrom scipy.cluster.hierarchy import dendrogram\n\nimport warnings","ee5545b7":"warnings.filterwarnings(\"ignore\", category=DeprecationWarning)\nwarnings.filterwarnings(\"ignore\", category=FutureWarning)","8693b7c4":"sc = MinMaxScaler((0, 1))\ndf = sc.fit_transform(df)\ndf[0:5]","8e384005":"kmeans = KMeans(n_clusters=4)\nk_fit = kmeans.fit(df)\nk_fit","772563fa":"dir(k_fit)","2986d08d":"k_fit.n_clusters\nk_fit.cluster_centers_\nk_fit.labels_\ndf[0:5]","eaf1b849":"k_means = KMeans(n_clusters=2).fit(df)\nkumeler = k_means.labels_\ntype(df)\ndf = pd.DataFrame(df)","92906f60":"plt.scatter(df.iloc[:, 0],\n            df.iloc[:, 1],\n            c=kumeler,\n            s=50,\n            cmap=\"viridis\")\nplt.show()","632a7443":"merkezler = k_means.cluster_centers_\n\nplt.scatter(df.iloc[:, 0],\n            df.iloc[:, 1],\n            c=kumeler,\n            s=50,\n            cmap=\"viridis\")\n\nplt.scatter(merkezler[:, 0],\n            merkezler[:, 1],\n            c=\"black\",\n            s=200,\n            alpha=0.5)\nplt.show()","ba8370a7":"kmeans = KMeans()\nssd = []\nK = range(1, 30)","bb24260d":"for k in K:\n    kmeans = KMeans(n_clusters=k).fit(df)\n    ssd.append(kmeans.inertia_)\n\nssd\n","fe00397a":"plt.plot(K, ssd, \"bx-\")\nplt.xlabel(\"Distance Residual Sums for different K Values\")\nplt.title(\"Elbow Method for Optimum Number of Clusters\")\nplt.show()\n","6c63b32c":"kmeans = KMeans()\nvisu = KElbowVisualizer(kmeans, k=(2, 20))\nvisu.fit(df)\nvisu.show();","28f9d888":"kmeans = KMeans(n_clusters=5).fit(df)\nkumeler = kmeans.labels_","ecedb466":"# Data Visualization","63cb4aaf":"We used the Cross Valudation method for logistic regression, and if we use it for all other models, we can compare the success of the models from the metric perspective. However, when we have plenty of data, using the holdout method as a \"train\" and \"test\" allows us to measure the predictive power more accurately. Because the model will try to guess the test data it has never seen.","186aec16":"### FEATURE ENGINEERING","12a258bf":"Prediction success is 0.78.","569a2d35":"Blood Pressure cannot be \"0\". Be careful! We need new feature...","a80b36d3":"### Categorical Variables","eae01c34":"Let's predict the logistic regression model!","cb5c0122":"# DATA PREPROCESSING","1a88c36c":"We have predicted probabilities, not the results. 0.28902396 in the first line, the occurrence of \"0\" class; 0.71097604, probability of occurrence of class \"1\" ...","ab6d056b":"At the above, we tried to understand the predictive power of the model by looking at the accucy scores. Here, we are looking at the auc score with the curve above. This scor gives the area between the curve and the line.","43d3586f":"# Light GBM","966d513d":"Since the number of observations is low, we will do an exploratory data analysis using the Cross Validation (CV) method instead of separating the data set as \"test and train\" with the holdout method.","3abe365b":"We recorded the predicted values as y_pred. Now we're bringing in the accuracy values. Real value \/ predicted values","fe28d869":"10 scores came for each cross valudation. Now let's add .mean () to see the average score ...","f0e0fd99":"We looked cross validation with all data","723784ab":"When the number of classes is unbalanced, values such as \"support\", \"presicion\", \"recall\"  will be important when looking for what to do. Sometimes - class sometimes + class will be important ... look at this!! ....\n\n\"macro avg\" and \"weighted avg\" values should have gotten this value because of there are two classes !! I guess, it will increase when there are more classes ...\n","2beb9885":"### Data Description\n##### Pregnancies: Number of times pregnant\n##### Glucose: Plasma glucose concentration a 2 hours in an oral glucose tolerance test\n##### BloodPressure: Diastolic blood pressure (mm Hg)\n##### SkinThickness: Triceps skin fold thickness (mm)\n##### Insulin: 2-Hour serum insulin (mu U\/ml)\n##### BMI: Body mass index (weight in kg\/(height in m)^2)\n##### DiabetesPedigreeFunction: Diabetes pedigree function\n##### Age: Age (years)\n##### Outcome: Class variable (0 or 1)","b864839a":"Reminder: Since we use CV method in logistic regression above, we also use CV here. Since there is little data, we did not separate it as 'test-train'. The best way is to separate the data set called \"test\" and  \"train\", to apply CV to the train set, to test it with the test set.","6a98b048":"## Random Forest\n","bba6b5ed":"We created an empty model, put it in GridSearchCV, leave the model to GridSearchCV to test it ...!","6e927ad8":"* ! Robust is less susceptible to outliers..., x-median(x)\/q3-q1 \n","ea7dd8e4":"### Missing Values","47d46672":"If we change the 0 values to NaN before looking at the correlation, we find a more significant correlation.","3e13a1e3":"df[\"Outcome\"].value_counts().plot.pie(autopct = \"%.1f\")    alternatif visual.","9636182f":"### Visualization   \n \"roc_auc_score\" and \"roc_curve\" are another metric for classification problems.","1d2db434":"# Installing\n","65b1daae":"We are looking at the description of numeric variables.","57a00b9f":"y= 7.7029389 + 1.17252354e-01*Pregnancies + 3.36001406e-02*Glucose -1.40872987e-02*BloodPressure......etc.\n\nNote: There is a difference in logstic regression. We cannot interpret these coefficients as in classical regression, we interpret them as e^coefficient.","360e3ef9":"By looking at the distribution graphs above (93), a more accurate decision can be made about filling empty values with median, mode or mean.","10f53e05":"# MODELLING","d50aae38":"We re-entered this Lightgbm and looked at our CV error again.\n\nlgbm_tuned = LGBMClassifier(**gs_cv.best_params_).fit(X, y)\ncross_val_score(lgbm_tuned, X, y, cv=10).mean()","aaee7f00":"### Outliers","55db19dc":"### Standardization","ed762842":"Now let's test the data set with the 10-fold cross-validation (CV) method, so let's divide the data by 10, build a model with 9 and test it with 1. "}}