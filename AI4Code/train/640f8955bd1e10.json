{"cell_type":{"345a686b":"code","b0de0ccb":"code","9cb25618":"code","56975bdc":"code","0aae5e1f":"code","45c5acef":"code","2685b513":"code","11a11a72":"code","8bfbfd28":"code","fa169f90":"code","67665c8e":"code","4cc04b88":"code","39513c00":"code","c6523997":"code","660d3c35":"code","f562a302":"code","31728e43":"code","d469574b":"code","eb528e4e":"code","1abaa09b":"code","f8017080":"code","b91cc580":"code","bce43349":"code","67a6f522":"code","a1195ba7":"code","dcafc70b":"markdown","5caef3b3":"markdown","5ef2ed11":"markdown","bc269103":"markdown","43f5f96a":"markdown","712246f4":"markdown","6e775000":"markdown","c0804f2d":"markdown","9271b108":"markdown","42b03341":"markdown","a2a894a8":"markdown","f6d7d521":"markdown","fad72fda":"markdown","9ed93370":"markdown","300514b2":"markdown"},"source":{"345a686b":"#Importing the necessary libraries\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import train_test_split\nfrom sklearn import metrics\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn import svm\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.neighbors import KNeighborsClassifier","b0de0ccb":"df = pd.read_csv('\/kaggle\/input\/graduate-admissions\/Admission_Predict_Ver1.1.csv')\n\ndf.head(3)","9cb25618":"df.info()","56975bdc":"df['admit'] =  np.where(df['Chance of Admit '] > 0.5,1,0)","0aae5e1f":"df.head()","45c5acef":"#Dropping useless variables\ndf.drop(['Chance of Admit ', 'Serial No.'], axis = 1, inplace = True)","2685b513":"df.head(3)","11a11a72":"df.describe()","8bfbfd28":"df.columns.values","fa169f90":"from statsmodels.stats.outliers_influence import variance_inflation_factor\nfrom statsmodels.tools.tools import add_constant\n\n\n# the target column (in this case 'admit') should not be included in variables\n#Categorical variables already turned into dummy indicator may or maynot be added if any\nvariables = df[['GRE Score', 'TOEFL Score', 'University Rating', 'SOP', 'LOR ',\n       'CGPA',]]\nX = add_constant(variables)\nvif = pd.DataFrame()\nvif['VIF'] = [variance_inflation_factor(X.values, i) for i in range (X.shape[1]) ]\nvif['features'] = X.columns\nvif\n\n#Using 10 as the minimum vif values i.e any independent variable 10 and above will have to be dropped\n#From the results all independent variable are below 10","67665c8e":"#Declaring our target variable as y\n#Declaring our independent variables as x\ny = df['admit']\nx = df.drop(['admit'], axis = 1)","4cc04b88":"scaler = StandardScaler() #Selecting the standardscaler\n\nscaler.fit(x)#fitting our independent variables","39513c00":"scaled_x = scaler.transform(x)#scaling","c6523997":"#Splitting our data into train and test dataframe\nx_train, x_test, y_train, y_test = train_test_split(scaled_x,y , test_size = 0.2, random_state = 49)","660d3c35":"reg = LogisticRegression()#Selecting our model\nreg.fit(x_train,y_train)","f562a302":"y_new = reg.predict(x_test) #Predicting with our already trained model using x_test","31728e43":"#Getting the accuracy of our model\nacc = metrics.accuracy_score(y_new,y_test)\nacc","d469574b":"#The intercept for our regression\nreg.intercept_","eb528e4e":"#Coefficient for all our variables\nreg.coef_","1abaa09b":"cm = confusion_matrix(y_new, y_test)\ncm","f8017080":"# Format for easier understanding\ncm_df = pd.DataFrame(cm)\ncm_df.columns = ['Predicted 0','Predicted 1']\ncm_df = cm_df.rename(index={0: 'Actual 0',1:'Actual 1'})\ncm_df","b91cc580":"dt = DecisionTreeClassifier()\ndt.fit(x_train,y_train)\n\ndnew = dt.predict(x_test)\n\nacc2 = metrics.accuracy_score(dnew,y_test)\nacc2","bce43349":"sv = svm.SVC() #select the algorithm\nsv.fit(x_train,y_train) # we train the algorithm with the training data and the training output\ny_pred = sv.predict(x_test) #now we pass the testing data to the trained algorithm\nacc_svm = metrics.accuracy_score(y_pred,y_test)\nprint('The accuracy of the SVM is:', acc_svm)","67a6f522":"knc = KNeighborsClassifier(n_neighbors=3) #this examines 3 neighbours for putting the new data into a class\nknc.fit(x_train,y_train)\ny_pred = knc.predict(x_test)\nacc_knn = metrics.accuracy_score(y_pred,y_test)\nprint('The accuracy of the KNN is', acc_knn)","a1195ba7":"df1 = pd.DataFrame(data = x.columns.values, columns = ['Features'])\n\ndf1['weight'] = np.transpose(reg.coef_)\ndf1['odds'] = np.exp(np.transpose(reg.coef_))\ndf1","dcafc70b":"#### Our model predicted '0' correctly once while NEVER predicting '0' incorrectly\n#### Also it predicted '1' correctly 93 times while predicting '1' incorrectly 6 times\n","5caef3b3":"### DUMMY INDICATOR\n#### Converting our target variable into a dummy indicator where a value greater than 0.5 chance of admit represents 1 else 0","5ef2ed11":"###  CONCLUSION\n#### Let's try to make a table and interpret what weight(BIAS) and odds means","bc269103":"### CONFUSION MATRIX","43f5f96a":"#### Remember we standardized all independents variables so the odds values have no direct interpretation\n#### Nevertheless using LOR as an example we can say for one standard deviation increase in LOR it is amost twice likely to cause a change in our target variable","712246f4":"#### After comparison with some other model we see that Logistic regression gave us the highest accuracy ~94%","6e775000":"\n\n\n\n#### If you find this notebook useful don't forget to upvote. #Happycoding\n","c0804f2d":"### Standardization\n\n#### Standardizing helps to give our independent varibles a more standard and relatable numeric scale, it also helps in improving model accuracy","9271b108":"### OTHER MODELS","42b03341":"### INTRODUCTION\n\n#### Predicting the chances of admition mainly through logistic regression\n#### Admit class was classified into two categories  0 and 1\n#### Steps taken in preprocessing includes Data cleaning, Standardizationetc\n#### All our variables in this dataset are numerical\n#### Other models where used to compare accuracy\n\n### SIDE NOTE\n#### You can leave your question about any unclear part in the comment section\n#### Any correction will be highly welcomed","a2a894a8":"### LOGISTIC REGRESSION","f6d7d521":"### DEALING WITH MISSING VALUES","fad72fda":"### CHECKING OLS ASSUMPTIONS\n\n#### Let's check that our dataset are not violating any of this assumptions which includes:\n#### 1. No Endogeneity\n#### 2. Normality and Homoscedasticity\n#### 3.No Autocorrelation\n#### 4.NO multicollinearity: making sure our independents variables are not strongly related(correlated) with each other\n\n####  We are not violating  assumptions 1 through 3 but for NO multicollinearity we need to check","9ed93370":"### LOADING THE DATAFRAME","300514b2":"#### This dataset is clean it does not have any missing value"}}