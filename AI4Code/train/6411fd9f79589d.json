{"cell_type":{"4f7b1182":"code","7d99211e":"code","9f4ac2a2":"code","9169f466":"code","706cc6a1":"code","da803355":"code","79056dfa":"code","88c7e025":"code","6f443509":"code","5476a65e":"code","864eb622":"code","904c654a":"code","2c4ee9ce":"code","a34baafa":"code","76bddb93":"code","563a8f0e":"code","34ab9975":"code","af47bde5":"code","7447102c":"code","a10b7e72":"code","d5c4404a":"code","4459ffc2":"code","89fa715d":"code","7bf73bc1":"code","6092da93":"code","fc295eae":"code","a3ba60e2":"code","c7c8927e":"code","4a8b99e1":"code","7f41e3d8":"code","e8a27054":"code","2dc72349":"code","e3426db1":"code","7f39ab4d":"code","fa56871b":"code","4fa76cf1":"code","f9d16385":"markdown","d569ea32":"markdown","66807603":"markdown","785d9bdc":"markdown","a74186e6":"markdown","84100057":"markdown","a35322c3":"markdown","84e8f421":"markdown","9cabe344":"markdown","07466c90":"markdown","e9073949":"markdown","b7e2fa95":"markdown","a9cb9f2b":"markdown","7c67f450":"markdown","bc6755a4":"markdown","5fb8017f":"markdown","b048815f":"markdown","e1364de9":"markdown","2317551f":"markdown","eb7bd16f":"markdown","b8639d39":"markdown","60196469":"markdown","4074f82b":"markdown","fa47da5d":"markdown","135b2f27":"markdown","2e6b1f8a":"markdown","12805a08":"markdown","2c7fedbb":"markdown","88aed0a7":"markdown","a2129791":"markdown"},"source":{"4f7b1182":"import re\nimport os\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom tqdm import tqdm","7d99211e":"df=pd.read_csv(\"train.csv\")\ndf.drop(\"id\",axis=1,inplace=True)      #Dropping the ID column","9f4ac2a2":"df.head(50)","9169f466":"print(df.info())","706cc6a1":"i=df['target'].isnull().sum()\nprint(\"Total {} null values and {} non-null values in Keyword feature\".format(i,len(df)-i))","da803355":"df['target'].value_counts()","79056dfa":"i=df['keyword'].isnull().sum()\nprint(\"Total {} null values and {} non-null values in Keyword feature\".format(i,len(df)-i))","88c7e025":"# filling null values with empty keyword\ndf['keyword']=df['keyword'].fillna(value='empty')","6f443509":"uniq_key=df[\"keyword\"].unique()\nprint(\"No of Unique Keywords in Keyword Feature : {} \\n\\n\".format(len(uniq_key)))\nprint(uniq_key)\n","5476a65e":"from nltk.stem import WordNetLemmatizer\nlemm=WordNetLemmatizer()\nkywd=[re.sub('%20',' ',str(i)) for i in df['keyword']]\nfinal_kywd=[]\nfor i in range(len(kywd)):\n    if len(kywd[i].split()) <2:\n        x=lemm.lemmatize(kywd[i],'v')\n        x=lemm.lemmatize(x,'n')\n        final_kywd.append(x)\n    else:\n        splitted=[]\n        for word in kywd[i].split():\n            x=lemm.lemmatize(word,'v')\n            x=lemm.lemmatize(x,'n')\n            splitted.append(x)\n        final_kywd.append(' '.join(splitted))\ndf['keyword']=final_kywd            ","864eb622":"uniq_key=df[\"keyword\"].unique()\nprint(\"No of Unique Keywords in Keyword Feature : {} \\n\\n\".format(len(uniq_key)))\nprint(uniq_key)\n","904c654a":"freq=df['keyword'].value_counts()\nx=[]\ny=[]\nfor word in freq.keys():\n    x.append(word)\n    y.append(freq[word])\n    print(\"{} : {}\".format(word,freq[word]))","2c4ee9ce":"import matplotlib.pyplot as plt\nplt.barh(x,y)\nplt.xlabel(\"Document Frequency\")\nplt.ylabel(\"Keyword\")\nplt.show()","a34baafa":"i=df['location'].isnull().sum()\nprint(\"Total {} null values and {} non-null values in location feature\".format(i,len(df)-i))","76bddb93":"uniq_key=df[\"location\"].unique()\nprint(\"No of Unique places in Location Feature : {} \\n\\n\".format(len(uniq_key)))\nprint(uniq_key)\n","563a8f0e":"df.drop('location',axis=1,inplace=True)","34ab9975":"df.head(5)","af47bde5":"i=df['text'].isnull().sum()\nprint(\"Total {} null values and {} non-null values in text feature\".format(i,len(df)-i))","7447102c":"i=1\nfor line in df['text']:\n    print(\"{} : {}\".format(i,line))\n    i+=1","a10b7e72":"contraction_mapping = {\"that'll\":\"that will\",\"ain\u2019t\": \"is not\", \"aren\u2019t\": \"are not\",\"can\u2019t\": \"cannot\", \"\u2019cause\": \"because\", \"could\u2019ve\": \"could have\", \"couldn\u2019t\": \"could not\",\n                          \"didn\u2019t\": \"did not\", \"doesn\u2019t\": \"does not\", \"don\u2019t\": \"do not\", \"hadn\u2019t\": \"had not\", \"hasn\u2019t\": \"has not\", \"haven\u2019t\": \"have not\",\n                          \"he\u2019d\": \"he would\",\"he\u2019ll\": \"he will\", \"he\u2019s\": \"he is\", \"how\u2019d\": \"how did\", \"how\u2019d\u2019y\": \"how do you\", \"how\u2019ll\": \"how will\", \"how\u2019s\": \"how is\",\n                          \"I\u2019d\": \"I would\", \"I\u2019d\u2019ve\": \"I would have\", \"I\u2019ll\": \"I will\", \"I\u2019ll\u2019ve\": \"I will have\",\"I\u2019m\": \"I am\", \"I\u2019ve\": \"I have\", \"i\u2019d\": \"i would\",\n                          \"i\u2019d\u2019ve\": \"i would have\", \"i\u2019ll\": \"i will\",  \"i\u2019ll\u2019ve\": \"i will have\",\"i\u2019m\": \"i am\", \"i\u2019ve\": \"i have\", \"isn\u2019t\": \"is not\", \"it\u2019d\": \"it would\",\n                          \"it\u2019d\u2019ve\": \"it would have\", \"it\u2019ll\": \"it will\", \"it\u2019ll\u2019ve\": \"it will have\",\"it\u2019s\": \"it is\", \"let\u2019s\": \"let us\", \"ma\u2019am\": \"madam\",\n                          \"mayn\u2019t\": \"may not\", \"might\u2019ve\": \"might have\",\"mightn\u2019t\": \"might not\",\"mightn\u2019t\u2019ve\": \"might not have\", \"must\u2019ve\": \"must have\",\n                          \"mustn\u2019t\": \"must not\", \"mustn\u2019t\u2019ve\": \"must not have\", \"needn\u2019t\": \"need not\", \"needn\u2019t\u2019ve\": \"need not have\",\"o\u2019clock\": \"of the clock\",\n                          \"oughtn\u2019t\": \"ought not\", \"oughtn\u2019t\u2019ve\": \"ought not have\", \"shan\u2019t\": \"shall not\", \"sha\u2019n\u2019t\": \"shall not\", \"shan\u2019t\u2019ve\": \"shall not have\",\n                          \"she\u2019d\": \"she would\", \"she\u2019d\u2019ve\": \"she would have\", \"she\u2019ll\": \"she will\", \"she\u2019ll\u2019ve\": \"she will have\", \"she\u2019s\": \"she is\",\n                          \"should\u2019ve\": \"should have\", \"shouldn\u2019t\": \"should not\", \"shouldn\u2019t\u2019ve\": \"should not have\", \"so\u2019ve\": \"so have\",\"so\u2019s\": \"so as\",\n                          \"this\u2019s\": \"this is\",\"that\u2019d\": \"that would\", \"that\u2019d\u2019ve\": \"that would have\", \"that\u2019s\": \"that is\", \"there\u2019d\": \"there would\",\n                          \"there\u2019d\u2019ve\": \"there would have\", \"there\u2019s\": \"there is\", \"here\u2019s\": \"here is\",\"they\u2019d\": \"they would\", \"they\u2019d\u2019ve\": \"they would have\",\n                          \"they\u2019ll\": \"they will\", \"they\u2019ll\u2019ve\": \"they will have\", \"they\u2019re\": \"they are\", \"they\u2019ve\": \"they have\", \"to\u2019ve\": \"to have\",\n                          \"wasn\u2019t\": \"was not\", \"we\u2019d\": \"we would\", \"we\u2019d\u2019ve\": \"we would have\", \"we\u2019ll\": \"we will\", \"we\u2019ll\u2019ve\": \"we will have\", \"we\u2019re\": \"we are\",\n                          \"we\u2019ve\": \"we have\", \"weren\u2019t\": \"were not\", \"what\u2019ll\": \"what will\", \"what\u2019ll\u2019ve\": \"what will have\", \"what\u2019re\": \"what are\",\n                          \"what\u2019s\": \"what is\", \"what\u2019ve\": \"what have\", \"when\u2019s\": \"when is\", \"when\u2019ve\": \"when have\", \"where\u2019d\": \"where did\", \"where\u2019s\": \"where is\",\n                          \"where\u2019ve\": \"where have\", \"who\u2019ll\": \"who will\", \"who\u2019ll\u2019ve\": \"who will have\", \"who\u2019s\": \"who is\", \"who\u2019ve\": \"who have\",\n                          \"why\u2019s\": \"why is\", \"why\u2019ve\": \"why have\", \"will\u2019ve\": \"will have\", \"won\u2019t\": \"will not\", \"won\u2019t\u2019ve\": \"will not have\",\n                          \"would\u2019ve\": \"would have\", \"wouldn\u2019t\": \"would not\", \"wouldn\u2019t\u2019ve\": \"would not have\", \"y\u2019all\": \"you all\",\n                          \"y\u2019all\u2019d\": \"you all would\",\"y\u2019all\u2019d\u2019ve\": \"you all would have\",\"y\u2019all\u2019re\": \"you all are\",\"y\u2019all\u2019ve\": \"you all have\",\n                          \"you\u2019d\": \"you would\", \"you\u2019d\u2019ve\": \"you would have\", \"you\u2019ll\": \"you will\", \"you\u2019ll\u2019ve\": \"you will have\",\n                          \"you\u2019re\": \"you are\", \"you\u2019ve\": \"you have\",\"ain't\": \"is not\", \"aren't\": \"are not\",\"can't\": \"cannot\", \"'cause\": \"because\", \"could've\": \"could have\", \"couldn't\": \"could not\",\n                          \"didn't\": \"did not\", \"doesn't\": \"does not\", \"don't\": \"do not\", \"hadn't\": \"had not\", \"hasn't\": \"has not\", \"haven't\": \"have not\",\n                          \"he'd\": \"he would\",\"he'll\": \"he will\", \"he's\": \"he is\", \"how'd\": \"how did\", \"how'd'y\": \"how do you\", \"how'll\": \"how will\", \"how's\": \"how is\",\n                          \"I'd\": \"I would\", \"I'd've\": \"I would have\", \"I'll\": \"I will\", \"I'll've\": \"I will have\",\"I'm\": \"I am\", \"I've\": \"I have\", \"i'd\": \"i would\",\n                          \"i'd've\": \"i would have\", \"i'll\": \"i will\",  \"i'll've\": \"i will have\",\"i'm\": \"i am\", \"i've\": \"i have\", \"isn't\": \"is not\", \"it'd\": \"it would\",\n                          \"it'd've\": \"it would have\", \"it'll\": \"it will\", \"it'll've\": \"it will have\",\"it's\": \"it is\", \"let's\": \"let us\", \"ma'am\": \"madam\",\n                          \"mayn't\": \"may not\", \"might've\": \"might have\",\"mightn't\": \"might not\",\"mightn't've\": \"might not have\", \"must've\": \"must have\",\n                          \"mustn't\": \"must not\", \"mustn't've\": \"must not have\", \"needn't\": \"need not\", \"needn't've\": \"need not have\",\"o'clock\": \"of the clock\",\n                          \"oughtn't\": \"ought not\", \"oughtn't've\": \"ought not have\", \"shan't\": \"shall not\", \"sha'n't\": \"shall not\", \"shan't've\": \"shall not have\",\n                          \"she'd\": \"she would\", \"she'd've\": \"she would have\", \"she'll\": \"she will\", \"she'll've\": \"she will have\", \"she's\": \"she is\",\n                          \"should've\": \"should have\", \"shouldn't\": \"should not\", \"shouldn't've\": \"should not have\", \"so've\": \"so have\",\"so's\": \"so as\",\n                          \"this's\": \"this is\",\"that'd\": \"that would\", \"that'd've\": \"that would have\", \"that's\": \"that is\", \"there'd\": \"there would\",\n                          \"there'd've\": \"there would have\", \"there's\": \"there is\", \"here's\": \"here is\",\"they'd\": \"they would\", \"they'd've\": \"they would have\",\n                          \"they'll\": \"they will\", \"they'll've\": \"they will have\", \"they're\": \"they are\", \"they've\": \"they have\", \"to've\": \"to have\",\n                          \"wasn't\": \"was not\", \"we'd\": \"we would\", \"we'd've\": \"we would have\", \"we'll\": \"we will\", \"we'll've\": \"we will have\", \"we're\": \"we are\",\n                          \"we've\": \"we have\", \"weren't\": \"were not\", \"what'll\": \"what will\", \"what'll've\": \"what will have\", \"what're\": \"what are\",\n                          \"what's\": \"what is\", \"what've\": \"what have\", \"when's\": \"when is\", \"when've\": \"when have\", \"where'd\": \"where did\", \"where's\": \"where is\",\n                          \"where've\": \"where have\", \"who'll\": \"who will\", \"who'll've\": \"who will have\", \"who's\": \"who is\", \"who've\": \"who have\",\n                          \"why's\": \"why is\", \"why've\": \"why have\", \"will've\": \"will have\", \"won't\": \"will not\", \"won't've\": \"will not have\",\n                          \"would've\": \"would have\", \"wouldn't\": \"would not\", \"wouldn't've\": \"would not have\", \"y'all\": \"you all\",\n                          \"y'all'd\": \"you all would\",\"y'all'd've\": \"you all would have\",\"y'all're\": \"you all are\",\"y'all've\": \"you all have\",\n                          \"you'd\": \"you would\", \"you'd've\": \"you would have\", \"you'll\": \"you will\", \"you'll've\": \"you will have\",\n                          \"you're\": \"you are\", \"you've\": \"you have\",\"n't\":'not'}","d5c4404a":"def clean_data(document,contraction_mapping):\n    clean_data=[]\n    for line in tqdm(document):\n        x=line.lower()              #lower-casing the words\n        word_list=[]\n        for word in x.split():      # contraction mapping\n            if word in contraction_mapping:\n                word_list.append(contraction_mapping[word])\n            else:\n                word_list.append(word)\n        jnt=' '.join(word_list)\n        x=re.sub(\"'s\",' ',jnt)\n        x=re.sub('\\n',' ',x)\n        x=re.sub(r'http.*?\\s|http.*',' ',x)       #removing web urls\n        x=re.sub(r'@.*?\\s|@(.*)?',' ',x)          #removing tagged usernames\n        x=re.sub(r'\\(.*?\\)',' ',x)\n        x=re.sub(r'\\[.*?\\]',' ',x)\n        x=re.sub(r'\\{.*?\\}',' ',x)\n        x=re.sub(r'&.*;',' ',x)\n        x=re.sub(r'[^a-z]',' ',x)\n        x=re.sub(r'\\s+',' ',x)\n        \n        \n        clean_data.append(x.strip())\n    return clean_data","4459ffc2":"i=1\ntweets=clean_data(df['text'],contraction_mapping)\nfor line in tweets:\n    print(\"{} : {}\".format(i,line))\n    i+=1","89fa715d":"from nltk.corpus import stopwords\nstopword=set(stopwords.words(\"english\"))\ncleaned_tweets=[]\nfor line in tweets:\n    word_list=line.split()\n    new_word_list=[]\n    for word in word_list:\n        if len(word) > 2:\n            if word not in stopword:\n                x=lemm.lemmatize(word,'v')\n                x=lemm.lemmatize(x,'n')\n                if len(x) > 2:\n                    new_word_list.append(x)\n    cleaned_tweets.append(' '.join(new_word_list))","7bf73bc1":"df['text']=cleaned_tweets","6092da93":"df.head(5)","fc295eae":"count=0\nindex=[]\nfor line in df['text']:\n    if len(line.split()) > 3:\n        index.append(count)\n        count+=1\n    else:\n        count+=1\n","a3ba60e2":"ndf=df.loc[index]\nndf.tail()","c7c8927e":"(ndf.target).value_counts()","4a8b99e1":"words_list=[]\nword_count=[]\nfor line in ndf['text']:\n    word_count.append(len(line.split()))\n    words_list.extend(line.split())","7f41e3d8":"print(\"There are total {} unique words\".format(len(set(words_list))))","e8a27054":"plt.hist(word_count,bins=10)\nplt.title('Word distribution of tweets')\nplt.xlabel(\"Range of words\")\nplt.ylabel(\"No of tweets\")\nplt.show()","2dc72349":"# Creating vocabulary\nvocab=set(words_list)","e3426db1":"word_count_dict={}\nfor word in vocab:\n    word_count_dict[word]=words_list.count(word)\n# sorting dictionary by words\nimport operator\nsorted_word_count=dict(sorted(word_count_dict.items(), key=operator.itemgetter(1),reverse=True))","7f39ab4d":"sorted_word_count","fa56871b":"full_str=''\nfor line in ndf['text']:\n    full_str+=line+' '\nfrom wordcloud import WordCloud, STOPWORDS \nwordcloud = WordCloud(width = 800, height = 800, \n                background_color ='black', \n                min_font_size = 1).generate(full_str) \n  \n# plot the WordCloud image                        \nplt.figure(figsize = (8, 8), facecolor = None) \nplt.imshow(wordcloud) \nplt.axis(\"off\") \nplt.tight_layout(pad = 0) \nplt.show() ","4fa76cf1":"ndf.to_csv('cleaned_train.csv',index=False)","f9d16385":"## Importing necessary modules","d569ea32":"### Getting index of data points in which tweet contains more than 3 words","66807603":"After preprocessing, there are total 6908 Data Points. 3782 data points belongs to class 0(non-disaster) and 3126 data points belongs to class 1(disaster) ","785d9bdc":"### Data Info","a74186e6":"# Exploratory Data Analysis","84100057":"### Features Explanation","a35322c3":"Text feature contains the tweets of users","84e8f421":"<p>\nid - a unique identifier for each tweet\n    \ntext - the text of the tweet\n\nlocation - the location the tweet was sent from (may be blank)\n\nkeyword - a particular keyword from the tweet (may be blank)\n\ntarget - in train.csv only, this denotes whether a tweet is about a real disaster (1) or not (0)\n\n<\/p>","9cabe344":"##  'Keyword' feature Analysis","07466c90":"## Function for  Data cleaning","e9073949":"## Text Feature Analysis","b7e2fa95":"### Word Distribution of tweets","a9cb9f2b":"### Preprocessing keywords","7c67f450":"### Removing words that contains less than 3 letter, removing stopwords, stemming","bc6755a4":"## 'Location' feature Analysis","5fb8017f":"##  Pre-processing tweets","b048815f":"## Loading Data","e1364de9":"There are total 7613 Data Points. 4342 data points belongs to class 0(non-disaster) and 3271 data points belongs to class 1(disaster) ","2317551f":"## Class label 'target' Analysis","eb7bd16f":"### Data after passing through clean_data function","b8639d39":"### Keywords and their document frequency","60196469":"## Creating wordcloud","4074f82b":"### Contraction Mapping Dictionary","fa47da5d":"Lots of values are null in Location feature. Here we are dropping the Location column","135b2f27":"## Filtering the data","2e6b1f8a":"## Creating word count dictionary","12805a08":"### Original tweets","2c7fedbb":"## Saving data to csv file","88aed0a7":"### After preprocessing the keyword feature","a2129791":"## Getting word count in tweets and total unique words"}}