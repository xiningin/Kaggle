{"cell_type":{"69584351":"code","d8a276be":"code","3aecb444":"code","796dbb5a":"code","50a6fc58":"code","a7dcec90":"code","729df454":"code","284653fc":"code","98c93168":"code","2a4055b0":"code","138ea623":"code","ffed265b":"code","0a21e513":"code","560cf3c1":"code","91ac156f":"code","d2a1927f":"code","d9e4bb93":"code","badf61eb":"code","fc4754ac":"code","f5d79931":"code","733bb8a6":"code","a6a76d47":"code","c67a0538":"code","a14b8ef1":"code","bfa68ce2":"code","0c87650b":"code","c5494042":"code","84a677dc":"code","f7670222":"code","b94fde16":"code","89c7fdf0":"code","ec1ee7f7":"code","4ddad440":"code","40aad949":"code","2e40493a":"code","8f86b43f":"code","d55ea6e6":"code","fba4af08":"code","04d3f056":"code","f377b561":"code","b27ed99f":"code","a737f319":"code","b2d18218":"code","5d2c1b03":"markdown","876b8105":"markdown","36f5e51f":"markdown","2b60b985":"markdown","4e0a2f20":"markdown","0eaf61ac":"markdown","94091e99":"markdown","4ab33c95":"markdown","467175dc":"markdown"},"source":{"69584351":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","d8a276be":"import math\nimport matplotlib.pyplot as plt\nimport keras\nimport pandas as pd\nimport numpy as np\nfrom keras.models import Sequential\nfrom keras.layers import Dense,LSTM,Dropout,Activation\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.preprocessing import RobustScaler\nfrom sklearn.metrics import mean_squared_error,mean_absolute_error\nfrom sklearn.model_selection import train_test_split\nfrom keras.callbacks import EarlyStopping","3aecb444":"epoch=50\nbatch_size=32","796dbb5a":"path='\/kaggle\/input\/TSLA-stock-price\/TSLA.csv'\ntsla=pd.read_csv(path)\ntsla.head()","50a6fc58":"tsla_i=tsla.set_index('Date',drop=True)\ntsla_i","a7dcec90":"tsla_i.info()","729df454":"# Close\u548cAdj Close\u4e24\u4e2a\u5217\u662f\u5b8c\u5168\u76f8\u540c\u7684\uff0c\u6240\u4ee5\u53bb\u6389\u4e00\u4e2aAdj Close\nprint(tsla_i['Close'].corr(tsla_i['Adj Close']))\nprint(tsla_i[tsla_i['Close']!=tsla_i['Adj Close']].count())","284653fc":"tsla_i=tsla_i.drop('Adj Close',axis=1)   # axis=1 \u8868\u793a\u6309\u7167\u5217\u5220\u9664\uff0caxis=0\u8868\u793a\u6309\u7167\u884c\u5220\ntsla_i","98c93168":"# \u8bad\u7ec3\u6570\u636e\u548c\u9a8c\u8bc1\u6570\u636e\u5206\u5272\ntrain,test=train_test_split(tsla_i,test_size=0.25,shuffle=False)\ntrain.shape,test.shape","2a4055b0":"train.head()","138ea623":"sc=RobustScaler().fit(train.values)\ntrain_sca=sc.transform(train.values)","ffed265b":"pd.DataFrame(train_sca).head()","0a21e513":"# \u901a\u8fc7inverse_transform\u5c06\u7f29\u653e\u540e\u7684\u503c\u518d\u8fd8\u539f\u56de\u53bb\ntrain_sca_inversed=sc.inverse_transform(train_sca)\npd.DataFrame(train_sca_inversed).head()","560cf3c1":"train_sca_2 = train_sca[:, [0, -1]]\ntrain_sca_2.shape","91ac156f":"value = np.random.rand(train_sca_2.shape[0])\nplt.scatter(train_sca_2[:,0],train_sca_2[:,1],alpha=1, marker='o', s=10, lw=3, c=value,cmap='Blues',vmin=-1, vmax=3)\nplt.xlabel(\"open\")\nplt.ylabel(\"volume\")\nplt.title(\"Robust Scaler plot\")\nplt.colorbar()\nplt.show()\n\nprint(\"open max={}\\nopen min={}\\nvolume max={}\\nvolume min={}\".format(train_sca_2[:,0].max(),train_sca_2[:,0].min(),train_sca_2[:,1].max(),train_sca_2[:,1].min()))","d2a1927f":"sc=MinMaxScaler(feature_range=(0,1))\nsc=sc.fit(train.values)\ntrain_minmax=sc.transform(train.values)","d9e4bb93":"train_minmax_2=train_minmax[:,[0,-1]]","badf61eb":"value = np.random.rand(train_minmax_2.shape[0])\nplt.scatter(train_minmax_2[:,0],train_minmax_2[:,1],alpha=1, marker='o', s=10, lw=3, c=value,cmap='Blues',vmin=-1, vmax=3)\nplt.xlabel(\"open\")\nplt.ylabel(\"volume\")\nplt.title(\"MinMax Scaler plot\")\nplt.colorbar()\nplt.show()\n\nprint(\"open max={}\\nopen min={}\\nvolume max={}\\nvolume min={}\".format(train_minmax_2[:,0].max(),train_minmax_2[:,0].min(),train_minmax_2[:,1].max(),train_minmax_2[:,1].min()))","fc4754ac":"# minmaxscaler\u901a\u8fc7inverse_transform\u8fd8\u539f\u7f29\u653e\u540e\u7684\u503c\ntrain_inversed=sc.inverse_transform(train_minmax)\npd.DataFrame(train_inversed).head()","f5d79931":"# train_open_high=train[['Open','High']]\n# test_open_high=test[['Open','High']]\ntrain_open_high=train\ntest_open_high=test","733bb8a6":"sc = MinMaxScaler(feature_range = (0, 1))\nsc=sc.fit(train_open_high.values)\nsc=sc.fit(test_open_high.values)","a6a76d47":"train_open_high_minmax_scaled=sc.transform(train_open_high.values)\ntest_open_high_minmax_scaled=sc.transform(test_open_high.values)","c67a0538":"# \u8bbe\u5b9a\u5f00\u59cb\u8303\u56f4\u548c\u7ed3\u675f\u8303\u56f4\u503c\nstart_range=60\n# 944\nend_range=train_open_high_minmax_scaled.shape[0]\n\n# Creating a data structure with 60 time-steps and 1 output\nX_train = []\ny_train = []\nfor i in range(start_range, end_range):\n#     lag\u8bbe\u5b9a\u4e3a1 day \uff0cy_train\u5c31\u662fy\u3002\u4ece60\u5f00\u59cb\u4e00\u76f4\u5230944\uff0c\u83b7\u53d6\u6240\u6709\u7684open\u503c\n#     \u4e5f\u5c31\u662f\u901a\u8fc7lag=1\u505a\u51fa\u6765y_train\u503c\n    X_train.append(train_open_high_minmax_scaled[i-start_range:i])\n    y_train.append(train_open_high_minmax_scaled[i,-2])\nX_train, y_train = np.array(X_train), np.array(y_train)\n# train.shape[1]=5 \u4e00\u5171\u67095\u5217\u6570\u636e\nX_train = np.reshape(X_train, (X_train.shape[0], X_train.shape[1], train.shape[1]))\nX_train.shape","a14b8ef1":"model = Sequential()\n#Adding the first LSTM layer and some Dropout regularisation\nmodel.add(LSTM(units = 50, return_sequences = True, input_shape = (X_train.shape[1], train.shape[1])))\nmodel.add(Dropout(0.2))\n# Adding a second LSTM layer and some Dropout regularisation\nmodel.add(LSTM(units = 50, return_sequences = True))\nmodel.add(Dropout(0.2))\n# Adding a third LSTM layer and some Dropout regularisation\nmodel.add(LSTM(units = 50, return_sequences = True))\nmodel.add(Dropout(0.2))\n# Adding a fourth LSTM layer and some Dropout regularisation\nmodel.add(LSTM(units = 50))\nmodel.add(Dropout(0.2))\n# Adding the output layer\nmodel.add(Dense(units = 1))","bfa68ce2":"# Compiling the LSTM\nmodel.compile(optimizer = 'adam', loss = 'mean_squared_error')","0c87650b":"model.fit(X_train,y_train,epochs=epoch,batch_size=batch_size)","c5494042":"train.shape,test.shape","84a677dc":"dataset_train =train.iloc[:]\ndataset_test = test.iloc[:]","f7670222":"dataset_total = pd.concat((dataset_train, dataset_test), axis = 0)","b94fde16":"dataset_total.shape","89c7fdf0":"inputs = dataset_total[len(dataset_total) - len(dataset_test) - 60:].values","ec1ee7f7":"len(dataset_total) - len(dataset_test) - 60","4ddad440":"inputs.shape","40aad949":"dataset_test.shape","2e40493a":"train.shape[1]","8f86b43f":"end_range_1=dataset_test.shape[0]+60\nsc=MinMaxScaler(feature_range=(0,1))\nsc=sc.fit(inputs)\ninputs = sc.transform(inputs)\nX_test = []\nfor i in range(60, end_range_1):\n    X_test.append(inputs[i-60:i])\nX_test = np.array(X_test)\n# train.shape[1]=5 \u8868\u793a\u67095\u5217\u6570\u636e\nX_test = np.reshape(X_test, (X_test.shape[0], X_test.shape[1], train.shape[1]))\nprint(X_test.shape)","d55ea6e6":"model.summary()","fba4af08":"pred=model.predict(X_test)","04d3f056":"# \u9700\u8981\u7b26\u5408sc\u8fd8\u539f\u7684\u7ef4\u5ea6\uff0c\u6240\u4ee5\u540c\u6837\u7684\u6570\u636e\u5408\u5e765\u6b21\uff08\u6682\u65f6\u6ca1\u6709\u627e\u5230\u5176\u4ed6\u65b9\u6cd5\uff09\npred_df=pd.DataFrame(pred)\npred_concat=pd.concat((pred_df, pred_df,pred_df,pred_df,pred_df), axis = 1)","f377b561":"pred_iverse=sc.inverse_transform(pred_concat)","b27ed99f":"pd.DataFrame(pred_iverse)","a737f319":"# \u53d6\u5f97\u6536\u76d8\u4ef7\u683cclose\u5217\npred_iverse_array=pd.DataFrame(pred_iverse).iloc[:,3]","b2d18218":"plt.figure(figsize=(10,6))\nplt.plot(test.index,pred_iverse_array,color='red',label = 'Predict TESLA Stock Price')\nplt.plot(test.index,dataset_test['Close'].values,color='blue',label = 'Real TESLA Stock Price')\nplt.xticks(np.arange(0,315,50))\nplt.xlabel('Time')\nplt.ylabel('TESLA Stock Price')\nplt.legend()\nplt.show()","5d2c1b03":"2.Minmax scaler","876b8105":"### Compiling the LSTM","36f5e51f":"#### Modeling using LSTM","2b60b985":"### Predict of model result using test data","4e0a2f20":"2.create lag of one day","0eaf61ac":"### \u6570\u636e\u7f29\u653eScaler\n1.Robust Scaler","94091e99":"### visualize the result","4ab33c95":"```\nLSTM\u306e\u305f\u3081\u306e\u30c7\u30fc\u30bf\u6b63\u898f\u5316\u306e\u4f8b\u3002\nsklearn.preprocessing.MinMaxScaler fit transform inverse_transform\n\nhttps:\/\/qiita.com\/ta1nakamura\/items\/f216f3509980592ec833\n```","467175dc":"### Preprocessing of Data\n1.Minmaxscaler"}}