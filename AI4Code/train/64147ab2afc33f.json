{"cell_type":{"a339b203":"code","19cca531":"code","9f75ab7d":"code","ea131e1f":"code","7ad9703a":"code","936bcb7e":"code","86c16de7":"code","376798bf":"code","0f3c1310":"code","5a731d41":"code","3c717c05":"code","5969e498":"code","85d037ad":"code","45acd33a":"code","47b71ca1":"code","1e126ab4":"code","6e81103d":"code","9c9817dd":"code","468feb94":"code","c598b081":"code","71aa7107":"code","dcc0ec83":"code","c3978717":"code","caa4dfd9":"code","91b381f8":"code","c5c2e49b":"code","799b0ee3":"code","c3c44218":"code","cf1ba733":"code","59828a86":"code","1ae60a22":"code","fa102a5e":"code","1c5a64b4":"code","4f718bfe":"code","0fa27a44":"code","c144f1b8":"code","0cbe5bff":"code","15c0b077":"markdown","1948b740":"markdown","5b21ef48":"markdown","6d6abee8":"markdown","23aeb3d9":"markdown","8bdc8eb2":"markdown","77efeba2":"markdown","16b71cac":"markdown","1a11de00":"markdown","c592e786":"markdown","f12af694":"markdown","37cc4734":"markdown","8a0a1481":"markdown","13db4af7":"markdown","c67a5b6d":"markdown","50febefc":"markdown","0f5279f7":"markdown","66e22eb0":"markdown","e8ab5cd4":"markdown","a82a0fa3":"markdown","1b7fcc1e":"markdown","e9ed1114":"markdown","3316a143":"markdown","1e44fac2":"markdown","87f9698d":"markdown","74d76146":"markdown","f836afba":"markdown","ddc4097b":"markdown","16f11992":"markdown","2781f087":"markdown","6e1e644f":"markdown","00af3fc9":"markdown","7798f816":"markdown","09b1d8ac":"markdown","17f6065d":"markdown","19702806":"markdown","15653e84":"markdown","d35a16e6":"markdown","3f6ab047":"markdown","7af67076":"markdown","44f7f805":"markdown","cb3cb165":"markdown","171f5237":"markdown","0bc453e5":"markdown","e4a606a7":"markdown","e1285701":"markdown","c0569a71":"markdown","1280201b":"markdown","fc774831":"markdown"},"source":{"a339b203":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n\n%matplotlib inline","19cca531":"df = pd.read_csv('..\/input\/competitive-data-science-predict-future-sales\/sales_train.csv', index_col = 'date', parse_dates = True )\ndf.head()","9f75ab7d":"df.describe().T","ea131e1f":"df.info()","7ad9703a":"df.isnull().sum()","936bcb7e":"def summary(df):\n    \n    types = df.dtypes\n    counts = df.apply(lambda x: x.count())\n    uniques = df.apply(lambda x: [x.unique()])\n    nas = df.apply(lambda x: x.isnull().sum())\n    distincts = df.apply(lambda x: x.unique().shape[0])\n    missing = (df.isnull().sum() \/ df.shape[0]) * 100\n    sk = df.skew()\n    krt = df.kurt()\n    \n    print('Data shape:', df.shape)\n\n    cols = ['Type', 'Total count', 'Null Values', 'Distinct Values', 'Missing Ratio', 'Unique Values', 'Skewness', 'Kurtosis']\n    dtls = pd.concat([types, counts, nas, distincts, missing, uniques, sk, krt], axis=1, sort=False)\n  \n    dtls.columns = cols\n    return dtls","86c16de7":"details = summary(df)\ndetails","376798bf":"df.index.names = ['Date']\ndf.head()","0f3c1310":"df = df.drop(['date_block_num'], axis = 1)\ndf.head()","5a731d41":"df.rename(columns={'shop_id':'Store ID', 'item_id':'Item ID', 'item_price':'Price', 'item_cnt_day':'Volume'}, inplace = True)\ndf.head()","3c717c05":"df['Daily Revenue'] = df['Price']*df['Volume']\ndf.head()","5969e498":"df[['Store ID','Price']].groupby('Store ID').mean().head()\n","85d037ad":"df.groupby('Store ID')[['Volume']].sum().head()","45acd33a":"df_ml = df.groupby(['Store ID', 'Volume']).sum()\ndf_ml.head()","47b71ca1":"df_ml.xs(39, level = 'Store ID').head()","1e126ab4":"df_monthly = df.reset_index().groupby(['Store ID', pd.Grouper(key='Date', freq = 'M')])[['Daily Revenue']].sum().rename(columns = {'Daily Revenue':'Monthly Revenue'})\ndf_monthly.head()","6e81103d":"df.reset_index().groupby(pd.Grouper(key='Date', freq='Q')).size()","9c9817dd":"df['Daily Revenue'].reset_index().groupby('Date', as_index = False).sum().rename(columns = {'Daily Revenue':'Total Daily Revenue'}).head()","468feb94":"grouped = df.reset_index().groupby('Store ID')\ngrouped.head()","c598b081":"type(grouped)","71aa7107":"grouped.get_group(36).head()","dcc0ec83":"grouped.size().head()","c3978717":"df.groupby(pd.qcut(x = df['Price'], q=3, labels=['Low', 'Medium','High'])).size()","caa4dfd9":"df.groupby(pd.cut(df['Daily Revenue'], [0,500,1000,2500,5000,10000,50000, 100000, 175000, 250000, 500000, 750000, 1000000, 1250000])).size()","91b381f8":"df.groupby(['Store ID', 'Price']).size().head(10)","c5c2e49b":"df_1 = df.loc[df['Store ID']==(59)]\ndf_1.groupby(['Volume','Price', 'Daily Revenue']).size().head(10)","799b0ee3":"df[['Price','Volume','Daily Revenue']].agg(['sum', 'mean'])","c3c44218":"df.agg({'Price':['mean'], 'Volume':['sum','mean'], 'Daily Revenue':['sum','mean']})","cf1ba733":"def my_agg(x): \n    names = { \n        'PriceMean': x['Price'].mean(),\n        'VolumeMax': x['Volume'].max(), \n        'DailyRevMean': x['Daily Revenue'].mean(),\n        'DailyRevMax': x['Daily Revenue'].max()\n    }\n\n    return pd.Series(names, index=[ key for key in names.keys()])\n\ndf.groupby('Store ID').apply(my_agg).head(10)","59828a86":"df.groupby('Store ID').agg({'Price':['mean'], 'Volume':['sum','mean'], 'Daily Revenue':['sum','mean']}).head()","1ae60a22":"df.groupby('Store ID').agg({'Price':['mean'], 'Volume':['sum','mean'], 'Daily Revenue':['sum','mean']}).T","fa102a5e":"df.apply(sum)","1c5a64b4":"df.groupby('Store ID').apply(lambda x:x.mean()).head()","4f718bfe":"def df_mean(x):\n    return x.mean()\ndf.groupby('Store ID').apply(df_mean).head()","0fa27a44":"df.reset_index().groupby(pd.Grouper(key = 'Date', freq = 'Q'))['Volume'].apply(sum)","c144f1b8":"df['Volume %'] = df.groupby('Store ID')[['Volume']].transform(lambda x: x\/sum(x)*100)\ndf.head()","0cbe5bff":"df.groupby('Store ID').filter(lambda x: x['Daily Revenue'].mean() > 1000).head()","15c0b077":"By specifying axis = 1, it will apply the function for the rows","1948b740":"### Note 2: To return a dataframe provide the column name as a list to the comumn selection by using double brackets like that: [['Volume']]","5b21ef48":"Another way to look at the data with groupby(). I sort it by Store ID here.","6d6abee8":"Daily Revenue summed up into Monthly Reevenue for every store using .sum() method:","23aeb3d9":"Note: By default, it applies the function along axis = 0 or for each column","8bdc8eb2":"### This way, it can take a list of column names and perform an aggregation on all of the comumns based on Store ID and Store's Volume by using (['Name 1', 'Name 2'])","77efeba2":"Tip: For a String use '39'","16b71cac":"## Aggregate Method:","1a11de00":"First, let's take a look at a few examples how apply() function works:","c592e786":"## Groupby Method:","f12af694":"## Size & pd.qcut() Method:","37cc4734":"### .size() method counts rows in each group","8a0a1481":"* 'size' = counts the rows\n* 'sum' = summs up\n* 'max\/min' = Maximum\/Minimum\n* 'mean\/median' = Mean\/ Median\n* 'idxmax\/idxmin' = Column's index of the max\/min\n* pd.Series.nunique = Counts unique values","13db4af7":"Dropping non-useful column:","c67a5b6d":"1. First, *Groupby*, splits the data into groups by creating a groupby object or DataFrameGroupBy. \n2. Next, *Apply* method, applies a fuction. \n3. And the last step, *Combine*, combines all of the results in a single output.","50febefc":"Cutting values into 3 equal buckets:","0f5279f7":"Here is another way that produces the same results","66e22eb0":"### The most frequently used functions:\n","e8ab5cd4":"Grouping by multiple columns:","a82a0fa3":"### Total Daily Revenue:","1b7fcc1e":"To make it horizontal, use transpose function at the end.","e9ed1114":"Filtering does not change the data, but only selects a subset:","3316a143":"Apply() function can be also apply=ied to the Time Series:","1e44fac2":"## Filter Method","87f9698d":"### groupby() while working with the Time Series.","74d76146":"Aggregate by sum and mean:","f836afba":"Setting up an Index","ddc4097b":"### So-called Groupby-Split-Apply-Combine chain mechanism","16f11992":"Alloccating Daily Revenue values into custom-sized buckets by specifying the bin boundaries:","2781f087":"![Mechanism](https:\/\/pbs.twimg.com\/media\/CycthXVXgAAazkz?format=jpg&name=small)","6e1e644f":"Looking at the average price for each store:","00af3fc9":"Passing a dictionary to agg and specify operations for each column:","7798f816":"Tip: by passing drop = True to the reset_insex(), your dff drops the columns that make up the MultiIndex and creates a new index with integer values.","09b1d8ac":"Hello, everyone,\n\nI just finished working with the **Groupby function by itself and along with .apply(), .filter(), .aggregate(), .transform()** and so on, and thought it might be usefull to some folks too.\n\nPlease, let me know what you think and how it can be made better. Any comments and suggestions are welcome!\n\nDo not forget to **UPVOTE** my Notebook if you found it useful! \n\nThank you, everyone)))","17f6065d":"### Note 3: .xs can be used to select subsets of DF while working with MultiIndex tables like this:","19702806":"### Note: By specifying the column ['Daily Revenue'] before the aggregate function .sum(), you will significantly improve your speed.","15653e84":"### Note 4: using reset_index() method resets the index into columns","d35a16e6":"## Apply() Function","3f6ab047":"![Pandas groupby](https:\/\/us.toluna.com\/dpolls_images\/2018\/08\/25\/620535cf-e152-4bad-a02c-8372063b9fe1.jpg)","7af67076":"Data filtered out only for store # 36","44f7f805":"Let's review: How to count rows in each group by using .size()","cb3cb165":"## Transform function","171f5237":"Creating new column Daily Revenue","0bc453e5":"For example, applying a lambda function","e4a606a7":"It's supposed to be a DataFrameGroupBy:","e1285701":"And now, using groupby and aggregate functions together, we get:","c0569a71":"Love this summary:","1280201b":"Another, but not as useful way:","fc774831":"While, for example, aggregate function reduced DF, this function just transforms our DF. That is why it's normally used to assign to a new column"}}