{"cell_type":{"6860eaf6":"code","11990ffc":"code","11ae2325":"code","b20b6c9f":"code","00d0cd7b":"code","d02f197f":"code","fb98ce80":"code","1b411129":"code","3d8cb962":"code","a7ce92c1":"code","e27cb496":"code","3cbcbe0c":"code","65737239":"code","12ff3624":"code","ecfea0d0":"code","c3fa92d3":"code","d904b3f9":"code","417842e5":"markdown","3f219728":"markdown","054133af":"markdown","a02f51f1":"markdown","d7c0f616":"markdown","c144b64a":"markdown","229313f3":"markdown","c51cf92f":"markdown","dc24a952":"markdown","8067f80e":"markdown","9d0f5200":"markdown","811c5d28":"markdown","35689510":"markdown"},"source":{"6860eaf6":"import numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import KFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score\nimport lightgbm as lgb\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nimport warnings\nfrom tqdm import tqdm_notebook as tqdm\nfrom subprocess import check_output\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nsns.set_style('whitegrid')\nwarnings.simplefilter(action='ignore', category=FutureWarning)\nwarnings.filterwarnings('ignore')\nrandom_state = 2019\nnp.random.seed(random_state)","11990ffc":"df_train = pd.read_csv('..\/input\/train.csv')\ndf_test = pd.read_csv('..\/input\/test.csv')\nnumcols = [col for col in df_train.columns if col.startswith('var_')]","11ae2325":"test = df_test.copy()\ntest.drop(['ID_code'], axis=1, inplace=True)\ntest = test.values\n\nunique_samples = []\nunique_count = np.zeros_like(test)\nfor feature in tqdm(range(test.shape[1])):\n    _, index_, count_ = np.unique(test[:, feature], return_counts=True, return_index=True)\n    unique_count[index_[count_ == 1], feature] += 1\n\n# Samples which have unique values are real the others are fake\nreal_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) > 0)[:, 0]\nsynthetic_samples_indexes = np.argwhere(np.sum(unique_count, axis=1) == 0)[:, 0]","b20b6c9f":"df_test = df_test.drop(synthetic_samples_indexes)","00d0cd7b":"full = pd.concat([df_train, df_test])\nfor col in numcols:\n    full['count'+col] = full[col].map(full[col].value_counts())","d02f197f":"full.head()","fb98ce80":"plt.figure(figsize=(12,5))\nplt.subplot(121)\nsns.distplot(full[full['target']==1]['countvar_12'], label='target = 1')\nsns.distplot(full[full['target']==0]['countvar_12'], label='target = 0')\nplt.legend()\nplt.subplot(122)\nsns.distplot(full[full['target']==1]['var_12'], label='target = 1')\nsns.distplot(full[full['target']==0]['var_12'], label='target = 0')\nplt.legend()","1b411129":"plt.figure(figsize=(12,5))\nplt.subplot(121)\nsns.distplot(full[full['target']==1]['countvar_61'], label='target = 1')\nsns.distplot(full[full['target']==0]['countvar_61'], label='target = 0')\nplt.legend()\nplt.subplot(122)\nsns.distplot(full[full['target']==1]['var_61'], label='target = 1')\nsns.distplot(full[full['target']==0]['var_61'], label='target = 0')\nplt.legend()","3d8cb962":"codecols = ['countvar_61','countvar_45','countvar_136','countvar_187',\n            'countvar_74','countvar_160', 'countvar_199','countvar_120',\n            'countvar_158','countvar_96']","a7ce92c1":"for col in numcols:\n    full['ratio'+col] = full[col] \/ full['count'+col]","e27cb496":"sns.distplot(full['ratiovar_81'])","3cbcbe0c":"sns.distplot(full['ratiovar_61'])","65737239":"sns.distplot(full['ratiovar_146'])","12ff3624":"# Define the functions for target encoding:\ndef add_noise(series, noise_level):\n    return series * (1 + noise_level * np.random.randn(len(series)))\n\ndef target_encode(trn_series=None,\n                  tst_series=None,\n                  target=None,\n                  min_samples_leaf=1,\n                  smoothing=1,\n                  noise_level=0):\n\n    assert len(trn_series) == len(target)\n    assert trn_series.name == tst_series.name\n    temp = pd.concat([trn_series, target], axis=1)\n    # Compute target mean\n    averages = temp.groupby(by=trn_series.name)[target.name].agg([\"mean\", \"count\"])\n    # Compute smoothing\n    smoothing = 1 \/ (1 + np.exp(-(averages[\"count\"] - min_samples_leaf) \/ smoothing))\n    # Apply average function to all target data\n    prior = target.mean()\n    # The bigger the count the less full_avg is taken into account\n    averages[target.name] = prior * (1 - smoothing) + averages[\"mean\"] * smoothing\n    averages.drop([\"mean\", \"count\"], axis=1, inplace=True)\n    # Apply averages to trn and tst series\n    ft_trn_series = pd.merge(\n        trn_series.to_frame(trn_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=trn_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_trn_series.index = trn_series.index\n    ft_tst_series = pd.merge(\n        tst_series.to_frame(tst_series.name),\n        averages.reset_index().rename(columns={'index': target.name, target.name: 'average'}),\n        on=tst_series.name,\n        how='left')['average'].rename(trn_series.name + '_mean').fillna(prior)\n    # pd.merge does not keep the index so restore it\n    ft_tst_series.index = tst_series.index\n    return add_noise(ft_trn_series, noise_level), add_noise(ft_tst_series, noise_level)","ecfea0d0":"ncols = [col for col in full if col not in ['target', 'ID_code']]\ndf_train = full[~full['target'].isna()]\ndf_test = full[full['target'].isna()]\ndf_test.drop('target', axis=1, inplace=True)","c3fa92d3":"# From Bayesian Optimization:\n# iter | target | featur... | lambda_l1 | lambda_l2 | learni... | max_depth | min_da... | min_ga... | min_su... | num_le... | \n# 26  | 0.9071 | 0.04901    | 0.5563   | 4.772     | 0.1295    | 0.6435    | 99.19     | 0.1414    | 0.3023    | 2.334 |\n\nlgb_params = {\n    \"objective\" : \"binary\",\n    \"metric\" : \"auc\",\n    \"boosting\": 'gbdt',\n    \"max_depth\" : -1,\n    \"num_leaves\" : 2,\n    \"learning_rate\" : 0.02, # Lower it for actual submission\n    \"bagging_freq\": 5,\n    \"bagging_fraction\" : 0.4,\n    \"feature_fraction\" : 0.05,\n    \"min_data_in_leaf\": 100,\n    \"min_sum_hessian_in_leaf\": 0.3,\n    \"lambda_l1\":0.556,\n    \"lambda_l2\":4.772,\n    \"tree_learner\": \"serial\",\n    \"boost_from_average\": \"false\",\n    \"bagging_seed\" : random_state,\n    \"verbosity\" : 1,\n    \"seed\": random_state}\n\nskf = StratifiedKFold(n_splits=3, shuffle=True, random_state=random_state)\noof = df_train[['ID_code', 'target']]\noof['predict'] = 0\npredictions = df_test[['ID_code']]\nval_aucs = []\nfeature_importance_df = pd.DataFrame()\n\n\nfeatures = [col for col in df_train.columns if col not in ['target', 'ID_code']]\nfor fold, (trn_idx, val_idx) in enumerate(skf.split(df_train, df_train['target'])):\n    d_train = df_train.iloc[trn_idx]\n    d_val = df_train.iloc[val_idx]\n    d_test = df_test.copy()\n\n    d_val['type'] = 'val'\n    d_test['type'] = 'test'\n    d_valtest = pd.concat([d_val, d_test])\n\n    for var in codecols:\n        d_train['encoded' + var], d_valtest['encoded' + var] = target_encode(d_train[var],\n                                                                          d_valtest[var],\n                                                                          target=d_train.target,\n                                                                          min_samples_leaf=100,\n                                                                          smoothing=10,\n                                                                          noise_level=0.01)\n\n    real_test = d_valtest[d_valtest['type']=='test'].drop('type', axis=1)\n    real_val = d_valtest[d_valtest['type']=='val'].drop('type', axis=1)\n\n    features = [col for col in d_train.columns if col not in ['target', 'ID_code']]\n    X_test = real_test[features].values\n    X_train, y_train = d_train[features], d_train['target']\n    X_valid, y_valid = real_val[features], real_val['target']\n\n    p_valid, yp = 0, 0\n    X_t, y_t = X_train.values, y_train.values\n    X_t = pd.DataFrame(X_t)\n    X_t = X_t.add_prefix('var_')\n\n    trn_data = lgb.Dataset(X_t, label=y_t)\n    val_data = lgb.Dataset(X_valid, label=y_valid)\n    evals_result = {}\n    lgb_clf = lgb.train(lgb_params,\n                        trn_data,\n                        100000,  # Submission with: 100000\n                        valid_sets=[trn_data, val_data],\n                        early_stopping_rounds=3000, # Submission with: 3000\n                        verbose_eval=1000,\n                        evals_result=evals_result\n                        )\n    p_valid += lgb_clf.predict(X_valid)\n    yp += lgb_clf.predict(X_test)\n    fold_importance_df = pd.DataFrame()\n    fold_importance_df[\"feature\"] = features\n    fold_importance_df[\"importance\"] = lgb_clf.feature_importance()\n    fold_importance_df[\"fold\"] = fold + 1\n    feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n    oof['predict'][val_idx] = p_valid\n    val_score = roc_auc_score(y_valid, p_valid)\n    val_aucs.append(val_score)\n\n    predictions['fold{}'.format(fold + 1)] = yp\n\n\nmean_auc = np.mean(val_aucs)\nstd_auc = np.std(val_aucs)\nall_auc = roc_auc_score(oof['target'], oof['predict'])\nprint(\"Mean auc: %.9f, std: %.9f. \" % (mean_auc, std_auc))","d904b3f9":"# predictions['target'] = np.mean(predictions[[col for col in predictions.columns if col not in ['ID_code', 'target']]].values, axis=1)\n# sub_df = pd.DataFrame({\"ID_code\":df_test[\"ID_code\"].values})\n# sub_df[\"target\"] = predictions['target'].values\n\n# df_test = pd.read_csv('..\/input\/test.csv')\n# finalsub = df_test[['ID_code']]\n# finalsub = finalsub.merge(sub_df, how='left', on='ID_code')\n# finalsub = finalsub.fillna(0)","417842e5":"# Split dataset back in train and test","3f219728":"# Load Data","054133af":"# Concatenate train and test, find value frequencies","a02f51f1":"The plot for var_61 did not have to mean anything, just that there are more unique values, hence the frequency for all of them is lower. But, this could give some kind of categorical features. I decided to divide the actual row value by its corresponding frequency, hoping to get some sort of 'pseudo-categories'. If some specific values repeat to much, then those values will get divided by a large amount and end up as low values. Values that have a low frequency will keep their magnitude... etc. This could be more evident in columns with low standard deviation... had to try. ","d7c0f616":"Sadly, the first time I trained the model there was an error at the end of the code and could not save the predictions. Luckily I was able to recover the information about the last fold and send the submission with just that information, got into silver range with 0.909 and had a confirmation that the method worked... And enough time to run the prediction again. I used only 3 folds since there was really not that much time left. Sent the submission with 0.9117 CV and got the final score of 0.91342 for the 157th place in the private LB and the silver in my first kaggle competition.\n\nI learnt a lot from all of you, and have to learn much more from the already published kenerls, so a huge thanks. \n\nAnd yeah... nothing new here and nothing to rival the GM uncles, but a good lesson... never give up !","c144b64a":"At this point I did not understand how could this work. The magic is here, but how is it usefull? ","229313f3":"And decided to look for similar features, maybe target encoding on this kind of features could be usefull.","c51cf92f":"With so many impressive codes and extensive explanations of the 'magic', the only thing i can add is the following: never give up.\nI was struggling a lot to find this features that were supposed to improve the score of the model. I tried different things, from feature interactions to ARIMA, from counts of positives and negatives to fourier coefficient, nothing seemed to work. At one point i said in the forum that I did not think i could get into the bronze medal range (being this my first kaggle competition) but that it was worth following the competition becouse of the great kernels and discussions posted from all of you... (so much to learn!!)\n\nBut I did not give up and kept trying new stuff untill the end... since there were so many hints in the forum, I knew the magic has to be on value counts... in employing the frequency is some way. Finally, with just a few hours to competition deadline, I managed to get into silver range (157th place). So here it goes:","dc24a952":"And it seemed to work... for some columns better than for others, but i really did not have much time left to explore. I had to use this or i would not have time to train the model before submission deadline","8067f80e":"# Imports","9d0f5200":"# Begin the modelling - LightGBM","811c5d28":"# Remove Fake Test Data","35689510":"# Save the results"}}