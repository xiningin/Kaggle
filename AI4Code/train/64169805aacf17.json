{"cell_type":{"22471c20":"code","bc1d87f4":"code","f32718e0":"code","258b9067":"code","426b8c70":"code","10d456ae":"code","f82fe2ab":"code","c4cc2acd":"code","f36f093b":"code","5f834309":"code","b03f1fe8":"code","5cac760c":"code","0ef992f8":"code","3c96ec68":"code","924f7743":"code","b86b44cf":"code","ee0c5af5":"code","f822f0fd":"code","78e16986":"code","8e831097":"code","a24a7796":"code","8fd5da2e":"code","2a28101c":"code","6a6e425c":"code","e8b856de":"code","ba36aaba":"code","14d6cabc":"code","ed9a6cb2":"code","4df38ee6":"code","8d5081a3":"markdown","f4a29163":"markdown","97676a47":"markdown","8a4d012e":"markdown","c78a559f":"markdown","b6646c38":"markdown","41e73f5a":"markdown","d4ea9ad4":"markdown","c99be672":"markdown","d9cce575":"markdown","e4210733":"markdown","47751e50":"markdown","90013678":"markdown","447b04c9":"markdown","34e3c813":"markdown","6da00f0f":"markdown","123d674f":"markdown","f62f0087":"markdown","57947cec":"markdown","8923588d":"markdown","096d2a59":"markdown","8d14e222":"markdown"},"source":{"22471c20":"# Check GPU being used. \n!nvidia-smi","bc1d87f4":"!pip install kornia==0.1.4.post2\n!pip install moviepy==1.0.3\n!pip install gdown \n\n\n!rm -r neural-painters-pytorch\n!rm -r neural_painters\n!git clone https:\/\/github.com\/reiinakano\/neural-painters-pytorch.git\n!cp -r neural-painters-pytorch\/neural_painters neural_painters\n","f32718e0":"!pip install ftfy\n!pip install regex\n!pip install tqdm\n!pip install git+https:\/\/github.com\/openai\/CLIP.git","258b9067":"import torch\nimport torchvision\nimport torchvision.utils as utils\nimport torchvision.transforms as transforms\nimport torch.optim as optim\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torchvision.transforms.functional as VF\n\nimport clip\n\nimport numpy as np\nimport matplotlib.pyplot as plt\n\n\nimport kornia\nimport moviepy.editor as mpy\n\n\nfrom torchvision.transforms import Compose, Resize, CenterCrop, ToTensor, Normalize\n\nfrom IPython.display import display\nfrom PIL import Image\n\nfrom neural_painters.gan_painter import GANNeuralPainter\nfrom neural_painters.canvas import NeuralCanvas, NeuralCanvasStitched\nfrom neural_painters.transforms import RandomRotate, Normalization, RandomCrop, RandomScale\n\n\ntry:\n    from torchvision.transforms import InterpolationMode\n    BICUBIC = InterpolationMode.BICUBIC\nexcept ImportError:\n    BICUBIC = Image.BICUBIC","426b8c70":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nprint(f\"device is {device}\")","10d456ae":"# all 0 to 1\nACTIONS_TO_IDX = {\n    'pressure': 0,\n    'size': 1,\n    'control_x': 2,\n    'control_y': 3,\n    'end_x': 4,\n    'end_y': 5,\n    'color_r': 6,\n    'color_g': 7,\n    'color_b': 8,\n    'start_x': 9,\n    'start_y': 10,\n    'entry_pressure': 11,\n}","f82fe2ab":"clip.available_models()","c4cc2acd":"clip_model_name = 'ViT-B\/32'\nclip_model, clip_preprocess = clip.load(clip_model_name, jit=False)\nclip_model = clip_model.eval().requires_grad_(False).to(device)","f36f093b":"# Define image augmentations\npadder = nn.ConstantPad2d(12, 0.5)\nrand_crop_8 = RandomCrop(8)\nrand_scale = RandomScale([1 + (i-5)\/50. for i in range(11)])\nrandom_rotater = RandomRotate(angle=5, same_throughout_batch=True)\nrand_crop_4 = RandomCrop(4)\nnormalizer = Normalization(torch.tensor([0.5, 0.5, 0.5], dtype=torch.float32).to(device), \n                           torch.tensor([0.5, 0.5, 0.5], dtype=torch.float32).to(device))","5f834309":"STROKES_PER_BLOCK = 13 #recommended min:1, max:15\nREPEAT_CANVAS_HEIGHT = 1 # recommended min:1, max:30\nREPEAT_CANVAS_WIDTH = 1 # recommended min:1, max:30\n#@markdown REPEAT_CANVAS_HEIGHT and REPEAT_CANVAS_WIDTH are important parameters to choose how many 64x64 canvases make up the height and width of the output image. \n# Try matching them with your target's aspect ratio.\n\nPAINTER_TYPE = \"GAN\" \nGRAY_STROKES = False\nSTOCHASTIC = False #Experimental. Adding uncertainty may (or may not) help produce more robust images.\n\nNORMALIZE = False \n\nLEARNING_RATE = 0.1\nIMG_DISPLAY_SIZE = 256\n\nNUM_EPOCHS = 256","b03f1fe8":"neural_painter = GANNeuralPainter(action_size=len(ACTIONS_TO_IDX), noise_dim=16, \n                                    num_deterministic=0 if STOCHASTIC else 16, pretrained=True).to(device).eval()","5cac760c":"neural_painter","0ef992f8":"# Define canvas and action preprocessor\nif GRAY_STROKES:\n    action_preprocessor = lambda x: torch.sigmoid((x * torch.tensor([[1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1]]).to(device)))\nelse:\n    action_preprocessor = torch.sigmoid  # torch.sigmoid is the default action preprocessor\n    \n    \ncanvas = NeuralCanvasStitched(neural_painter=neural_painter, overlap_px=32, \n                              repeat_h=REPEAT_CANVAS_HEIGHT, repeat_w=REPEAT_CANVAS_WIDTH, \n                              strokes_per_block=STROKES_PER_BLOCK, \n                              action_preprocessor=action_preprocessor)\n\n\n\n","3c96ec68":"# prompt=input() #interactive\nprompt = \"black sheep\"","924f7743":"# ---\n\ntext = clip.tokenize([prompt]).to(device)\ntext_features = clip_model.encode_text(text).float()\n\n#---","b86b44cf":"# Define actions\nactions = torch.FloatTensor(canvas.total_num_strokes, 1, len(ACTIONS_TO_IDX)).uniform_().to(device)\n\noptimizer = optim.Adam([actions.requires_grad_()], lr=LEARNING_RATE)\n\ncanvas_frames = []","ee0c5af5":"for idx in range(int(NUM_EPOCHS)):\n    optimizer.zero_grad()\n\n    # Generate final canvas with all strokes\n    output_canvas, _ = canvas(actions)\n\n    # Pass through pretrained\n    # output_canvas = normalize(output_canvas)\n\n    augmented_canvas = rand_crop_4(\n      random_rotater(\n          rand_crop_8(\n              rand_scale(\n                  padder(\n                      normalizer(output_canvas) if NORMALIZE else output_canvas)))))\n    \n\n    augmented_canvas = F.interpolate(augmented_canvas, size=clip_model.visual.input_resolution)\n    \n    image_features = clip_model.encode_image(augmented_canvas)\n    \n    \n    loss = nn.functional.cosine_embedding_loss(\n        image_features.div(image_features.norm()),\n        text_features.div(text_features.norm()),\n        target=torch.ones(1).to(device)\n    )\n    \n    loss.backward()\n    \n    optimizer.step()\n    \n    img_result = VF.resize(VF.to_pil_image(output_canvas.clone().squeeze().cpu()), (IMG_DISPLAY_SIZE, IMG_DISPLAY_SIZE))\n    canvas_frames.append(np.asarray(img_result))\n    \n    if idx % 10 == 0:\n        print(f'Step: {idx}\\nLoss: {loss.item()}')\n        display(img_result)\n        print('---')\n        print('Feel free to stop the optimization when you are happy with the results. Imperfection is beuatiful')\n\n\nprint(f'Final Step: {idx}\\nLoss: {loss.item()}')\ndisplay(img_result)","f822f0fd":"# Make a directory to output the video frames for each step\n!rm -rf steps; mkdir steps","78e16986":"def save_frames(canvas_frames, prefix=\"canvas\", out_dir=\".\/steps\"):\n    \"\"\"Save to file the images of individual frames from the intermediate canvas steps of the optimization\"\"\"\n    # this is used to format the filenames\n    n_digits = len(str(len(canvas_frames))) + 1\n    # save the canvas frames as png's\n    for i, c in enumerate(canvas_frames):\n        Image.fromarray(c).save(f\"{out_dir}\/{prefix}_{str(i).zfill(n_digits)}.png\")\n    ","8e831097":"def tensor_to_image(tensors, image_size=256):\n    \"\"\"Transforms a list of tensors into images of size image_size\"\"\"\n    frames = []\n    for t in tensors:\n        img_result = VF.resize(VF.to_pil_image(t.detach().squeeze().cpu()), (image_size, image_size))\n        frames.append(np.asarray(img_result))\n    return frames\n    ","a24a7796":"save_frames(canvas_frames)","8fd5da2e":"!cat .\/steps\/canvas*.png | ffmpeg -framerate 15 -y -f image2pipe -i - canvas_output.mp4 2>\/dev\/null","2a28101c":"mpy.ipython_display(\"canvas_output.mp4\", height=400, max_duration=100.)","6a6e425c":"_, intermediate_canvases = canvas(actions)","e8b856de":"strokes = tensor_to_image(intermediate_canvases)","ba36aaba":"save_frames(strokes, prefix=\"stroke\")","14d6cabc":"#!ls steps","ed9a6cb2":"!cat .\/steps\/stroke*.png | ffmpeg -framerate 4 -y -f image2pipe -i - stroke_output.mp4 2>\/dev\/null","4df38ee6":"mpy.ipython_display(\"stroke_output.mp4\", height=400, max_duration=100.)","8d5081a3":"## Tell the machine what to paint","f4a29163":"---\n# The Code \ud83e\udd16\n---","97676a47":"# Installing Neural Painters and required dependencies","8a4d012e":"## Defining the `device` and make sure is `GPU` (i.e., `cuda`)","c78a559f":"# Video of the evolution of the paint during the optimization","b6646c38":"## Define some useful image transformations for data augmentation","41e73f5a":"## Check the GPU available to us","d4ea9ad4":"### **Let us know if you have any comments, questions, or want to share your creations. Cheers! Ernesto [@vedax](https:\/\/twitter.com\/vedax\/)**\n\n# Intro\n\nArtists combine colors and brushstrokes to paint their masterpieces, they do not create paintings pixel by pixel. However, most of the current generative AI Art methods based on Machine Learning (ML) are still centered to teach machines how to \u2018paint\u2019 at the pixel-level in order to achieve or mimic some painting style, e.g., GANs-based approaches and style transfer. This might be effective, but not very intuitive, specially when explaining this process to artists, who are familiar with colors and brushstrokes.\n\nOur goal is to teach a machine how to paint using a combination of colors and strokes by telling it in natural language what to paint. How can we achieve this?\n\n# Materials and Approach\nWe need two basic ingredients:\n\n1. A ML model that knows how to paint using colors and strokes. To this end we will use a *Neural Painter* <a href=\"#neural-painter-diavlex\">[1]<\/a>,<a href=\"#neural-painter-reiichiro\">[2]<\/a>\n\n1. A ML model that connects text with images, that is, it should be able to associate text with  visual concepts. We use CLIP (Contrastive Language\u2013Image Pre-training) for this task. <a href=\"#clip\">[3]<\/a>\n\n\n## TL;DR \n\nThe following steps capture the essence of our idea, note that we use pseudocode based on Python, which does not necessary reflect the models' API. Please have a look to the notebook itself for the actual code and methods used. \n\n1. Specify what to paint, e.g., \n`prompt = \"black sheep\"`\n\n1. Encode the text using CLIP's language portion to obtain the text features \n`text_features = clip_model.encode_text(prompt)`\n\n1. Initialize a list of brushstrokes or `actions` and ask the `neural painter` to paint on a canvas. At the beginning the canvas will look random.\n`canvas = neural_painter.paint(actions)`\n\n1. Use the vision portion of the CLIP model to extract the image features of this initial `canvas`.\n`image_features = clip_model.encode_image(canvas)`\n\n1. The goal is to teach the neural painter to modify the strokes (i.e., its actions) depending on how different is what it is painting to the initial text request (`prompt`). For example, in the perfect case scenario, the cosine similarity between the text and image feature vectors should be 1.0.\nUsing this intuition, we use as the loss to guide the optimization process the the cosine distance, that measure how different the vectors are. The cosine distance in our case corresponds to \n`loss = 1.0 - cos(text_features, image_features)`.\n\n1. We minimize this loss adapting the neural painter `actions` that in the end should produce a canvas as close as possible to the original request.\n\n# Enjoy Neural Painting! ;) \n\n---\n## References\n\n<a id=\"neural-painter-diavlex\"><\/a>\n[1] *The Joy of Neural Painting*. Ernesto Diaz-Aviles, Claudia Orellana-Rodriguez, Beth Jochim. Libre AI Technical Report 2019-LAI-CUEVA-X01. 2019. \n* Paper: https:\/\/arxiv.org\/abs\/2111.10283 \n* Blogpost: https:\/\/www.libreai.com\/the-joy-of-neural-painting\/\n* Code Repo [MIT License]: https:\/\/github.com\/libreai\/neural-painters-x \u2013 Part of the code is used in this notebook.\n\n<a id=\"neural-painter-reiichiro\"><\/a>\n[2] *Neural Painters: A learned differentiable constraint for generating brushstroke paintings*. Reiichiro Nakano. 2019. \n* Paper: https:\/\/arxiv.org\/abs\/1904.08410\n* Blogpost: https:\/\/reiinakano.com\/2019\/01\/27\/world-painters.html\n* Code Repo [MIT License]: https:\/\/github.com\/reiinakano\/neural-painters-pytorch \u2013 Part of the code is used in this notebook.\n\n<a id=\"clip\"><\/a>\n[3] *Learning Transferable Visual Models From Natural Language Supervision. CLIP.* Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry, Amanda Askell, Pamela Mishkin, Jack Clark, Gretchen Krueger, Ilya Sutskever. OpenAI. 2021. \n* Paper: https:\/\/arxiv.org\/abs\/2103.00020\n* Blogpost: https:\/\/openai.com\/blog\/clip\/\n* Code Repo [MIT License]: https:\/\/github.com\/openai\/CLIP\n\n\n","c99be672":"# Getting ingredients ready","d9cce575":"# Let's Paint","e4210733":"# Create some videos from our paint","47751e50":"## These are the actions that defines each stroke","90013678":"# **[diavlex Art+AI](http:\/\/www.libreai.com\/diavlex): Text to Paint using Neural Painters and CLIP**","447b04c9":"# Fire the optimization ","34e3c813":"## Checking the CLIP models available and picking one ","6da00f0f":"# Video of the final painting stroke by stroke","123d674f":"# Install CLIP and dependencies","f62f0087":"# ~ fin ~","57947cec":"## Prepare the Canvas","8923588d":"## Create the Neural Painter","096d2a59":"## Define parameters for neurla painters and optimization","8d14e222":"# Loading the libraries"}}