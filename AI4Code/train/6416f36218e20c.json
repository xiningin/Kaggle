{"cell_type":{"16cc3c58":"code","3035138d":"code","d11687ad":"code","465835ee":"code","fddd7e81":"code","7bedbacd":"code","9f4628e2":"code","6973e1c6":"code","396a073e":"code","8403818b":"code","c78200f6":"code","cda00238":"code","28aaa5f7":"code","610f068c":"code","12d20bcd":"code","2190ffe0":"markdown","be67c81b":"markdown","af14424f":"markdown","20f9f179":"markdown","89ba801e":"markdown","97ff6817":"markdown","ed35e9dd":"markdown","6f7243fd":"markdown","efbd1578":"markdown","8c3b0d91":"markdown","a55d6669":"markdown","5899760f":"markdown","b3873904":"markdown","04833f53":"markdown","312aa42b":"markdown","b4901d64":"markdown","31a3493f":"markdown"},"source":{"16cc3c58":"%%time\n\nimport pickle \nimport numpy as np\nimport pandas as pd\n\ntrain_pickle_file = '\/kaggle\/input\/pickling\/train.csv.pandas.pickle'\ndata = pickle.load(open(train_pickle_file, 'rb'))","3035138d":"targets = ['resp','resp_1','resp_2','resp_3','resp_4']\n\ndata[targets].head()","d11687ad":"targets_arr = data[targets].values\ntargets_bool_arr = targets_arr>0\n\ntargets_bool_arr[:5,:]\n\nmy_map = {True: 'Y', False: 'N'}\ntarget_str_arr = np.vectorize(my_map.get)(targets_bool_arr)\ntarget_str_arr_join = [\"\".join(i) for i in target_str_arr[:,:].astype(str)]\n\ndata['pattern'] = target_str_arr_join\n\ndata.pattern.value_counts()","465835ee":"data.groupby('pattern').mean().resp","fddd7e81":"data.query('weight!=0').pattern.value_counts()","7bedbacd":"from sklearn.linear_model import LinearRegression\n\ntgt = ['resp_1','resp_2','resp_3','resp_4']\n\nX = data[tgt]\ny = data['resp']\n\nreg = LinearRegression().fit(X, y)\n\nreg.coef_\nreg.intercept_\nl_resp = reg.predict(X)\n\ndata['l_resp'] = l_resp","9f4628e2":"import matplotlib.pyplot as plt\n\nplt.scatter(y,l_resp)","6973e1c6":"c = 0.9\n\ndata['c_resp'] = c * data['resp'] + (1-c) * data['l_resp']","396a073e":"plt.scatter(data['resp'],data['c_resp'])","8403818b":"# Snippets from Marco Lopez de Prado, 2020\n\nfrom scipy.optimize import minimize\nfrom sklearn.neighbors import KernelDensity\n\ndef mpPDF(var,q,pts):\n    # Marcenko-Pastur pdf\n    # q=T\/N\n    eMin, eMax = var*(1-(1.\/q)**.5)**2, var*(1+(1.\/q)**.5)**2\n    eVal = np.linspace(eMin,eMax,pts)\n    pdf = q\/(2*np.pi*var*eVal)*((eMax-eVal)*(eVal-eMin))**.5\n    pdf = pd.Series(pdf.reshape(-1,), index=eVal.reshape(-1,))\n    return pdf\n\n\ndef getPCA(matrix):\n    # Get eVal,eVec from a Hermitian matrix\n    eVal,eVec = np.linalg.eigh(matrix)\n    indices=eVal.argsort()[::-1] # arguments for sorting eVal desc\n    eVal,eVec=eVal[indices],eVec[:,indices]\n    eVal=np.diagflat(eVal)\n    return eVal,eVec\n\ndef fitKDE(obs,bWidth=.25,kernel='gaussian',x=None):\n    # Fit kernel to a series of obs, and derive the prob of obs\n    # x is the array of values on which the fit KDE will be evaluated\n    if len(obs.shape)==1:\n        obs=obs.reshape(-1,1)\n    kde=KernelDensity(kernel=kernel,bandwidth=bWidth).fit(obs)\n    if x is None:\n        x=np.unique(obs).reshape(-1,)\n    if len(x.shape)==1:\n        x=x.reshape(-1,1)\n    logProb=kde.score_samples(x) # log(density)\n    pdf=pd.Series(np.exp(logProb),index=x.flatten())\n    return pdf\n\ndef cov2corr(cov):\n    # Derive the correlation matrix from a covariance matrix\n    std=np.sqrt(np.diag(cov))\n    corr=cov\/np.outer(std,std)\n    corr[corr<-1],corr[corr>1]=-1,1 # numerical error\n    return corr\n\ndef errPDFs(var,eVal,q,bWidth,pts=1000):\n    # Fit error\n    pdf0=mpPDF(var,q,pts) # theoretical pdf\n    pdf1=fitKDE(eVal,bWidth,x=pdf0.index.values) # empirical pdf\n    sse=np.sum((pdf1-pdf0)**2)\n    return sse\n\ndef findMaxEval(eVal,q,bWidth):\n    # Find max random eVal by fitting Marcenko\u2019s dist\n    out=minimize(lambda *x:errPDFs(*x),.5,args=(eVal,q,bWidth),\n    bounds=((1E-5,1-1E-5),))\n    if out['success']:\n        var=out['x'][0]\n    else:\n        var=1\n    eMax=var*(1+(1.\/q)**.5)**2\n    return eMax,var\n\ndef denoisedCorr(eVal,eVec,nFacts):\n    # Remove noise from corr by fixing random eigenvalues\n    eVal_=np.diag(eVal).copy()\n    eVal_[nFacts:]=eVal_[nFacts:].sum()\/float(eVal_.shape[0] - nFacts)\n    eVal_=np.diag(eVal_)\n    corr1=np.dot(eVec,eVal_).dot(eVec.T)\n    corr1=cov2corr(corr1)\n    return corr1\n\ndef denoisedCorr2(eVal,eVec,nFacts,alpha=0):\n    # Remove noise from corr through targeted shrinkage\n    eValL,eVecL=eVal[:nFacts,:nFacts],eVec[:,:nFacts]\n    eValR,eVecR=eVal[nFacts:,nFacts:],eVec[:,nFacts:]\n    corr0=np.dot(eVecL,eValL).dot(eVecL.T)\n    corr1=np.dot(eVecR,eValR).dot(eVecR.T)\n    corr2=corr0+alpha*corr1+(1-alpha)*np.diag(np.diag(corr1))\n    return corr2\n\nfrom sklearn.base import BaseEstimator, TransformerMixin\n\n#@njit\ndef fillna_npwhere_njit(array, values):\n    if np.isnan(array.sum()):\n        array = np.where(np.isnan(array), values, array)\n    return array\n\nclass RMTDenoising(BaseEstimator, TransformerMixin):\n    \n    def __init__(self, bWidth=.01, alpha=.5, feature_0=True, sample=0.3, seed=2021):\n        self.bWidth = bWidth\n        self.alpha = alpha\n        self.feature_0 = feature_0\n        self.sample = sample\n        self.seed = seed\n    \n    def denoise(self, X):\n        sample = X.sample(frac=self.sample, random_state=self.seed)\n        q = X.shape[0] \/ X.shape[1]\n        cov = sample.cov().values\n        corr0 = cov2corr(cov)\n\n        eVal0, eVec0 = getPCA(corr0)\n        eMax0, var0 = findMaxEval(np.diag(eVal0), q, bWidth=self.bWidth)\n        nFacts0 = eVal0.shape[0] - np.diag(eVal0)[::-1].searchsorted(eMax0)\n        corr1 = denoisedCorr2(eVal0,eVec0,nFacts0,alpha=self.alpha)\n        eVal1, eVec1 = getPCA(corr1)\n        #result = np.hstack((np.diag(eVal1), var0))\n        #name = [f'eigen_{i+1}' for i in range(len(eVal1))] + ['var_explained']\n        return eVec1[:, :nFacts0]\n    \n    def fit(self, X, y=None):\n        if self.feature_0:\n            self.cols_ = [c for c in X.columns if c != 'feature_0']\n        else:\n            self.cols_ = list(X.columns)\n        X_ = X[self.cols_]\n        self.W_ = self.denoise(X_)\n        self.dim_W_ = self.W_.shape[1]\n        return self\n    \n    def transform(self, X, y=None):\n        X_ = X.copy()\n        names = [f'proj_{i}' for i in range(self.dim_W_)]\n        projection = pd.DataFrame(fillna_npwhere_njit(X_[self.cols_].values, 0).dot(self.W_), columns=names)\n        if self.feature_0:\n            projection['feature_0'] = X['feature_0']\n        return projection","c78200f6":"targets_f0 = targets + ['feature_0']\ntarget_tf = RMTDenoising(sample=0.8)\n\ntarget_tf.fit(data[targets_f0])\n\nDn_targets = target_tf.transform(data[targets_f0])","cda00238":"Dn_targets.proj_0.head()","28aaa5f7":"data.resp.head()","610f068c":"data['dresp'] = -Dn_targets.proj_0","12d20bcd":"targets = ['dresp','resp_1','resp_2','resp_3','resp_4']\n\ndata[targets].head()\n\ntargets_arr = data[targets].values\ntargets_bool_arr = targets_arr>0\n\ntargets_bool_arr[:5,:]\n\nmy_map = {True: 'Y', False: 'N'}\ntarget_str_arr = np.vectorize(my_map.get)(targets_bool_arr)\ntarget_str_arr_join = [\"\".join(i) for i in target_str_arr[:,:].astype(str)]\n\ndata['dpattern'] = target_str_arr_join\n\ndata.pattern.value_counts()","2190ffe0":"# Target Analysis and Denoising\n\n\nI haven't seen many attempts at target engineering so I felt like I could share an initial analysis and my current work around denoising the target. Maybe it will prove usefull, maybe it will inspire others. Those denoising techniques seems to generally improve a bit my model scores and reduce variance across folds when used as a base for binary classification. I haven't had the time to test for regression or compare it to stacking multiple targets.\n\nThe notebook start with a simple analysis and a basic linear approach I came up with. Then I'll go into a deeper approach that rely on random matrix theory. In his \"Machine Learning for Asset Managers\", Marcos M. L\u00f3pez de Prado show how to use Random Matrix Theory to denoise a covariance matrix. By removing eigenvalues from the covariance matrix it is possible to also reduce the dimension of the initial inputs. It has already been used [here](https:\/\/www.kaggle.com\/fernandoramacciotti\/janestreet-denoising-rmt) to denoise the features covariance matrix and reduce the associated dimension of the problem. In this notebook I show how to use it for reducing noise in targets.\n\nI hope you will enjoy this target engineering notebook as much as my previous works (About [Intraday Feature Exploration](https:\/\/www.kaggle.com\/lucasmorin\/complete-intraday-feature-exploration),[Running Algorithms for Fast Feature Engineering](https:\/\/www.kaggle.com\/lucasmorin\/running-algos-fe-for-fast-inference), and [using yfinance to download financial data in Ptyhon](https:\/\/www.kaggle.com\/lucasmorin\/downloading-market-data)). Feel free to upvote \/ share my notebooks.\nLucas","be67c81b":"<a id='Target_Analysis'><\/a>\n# Target Analysis\n\nLet's have a look at targets. In Financial Machine Learning it is usual to build a target by looking at the behavior of a financial instrument over a given time horizon. We already know from other notebooks (for exemple here : https:\/\/www.kaggle.com\/marketneutral\/target-engineering-cv-multi-target), that different target correspond to different time horizons. The horizon of the main target seems to be between the horizon of target 3 and 4. We can indeed observe that :   ","af14424f":"<img src=\"attachment:61NAXLfL8RL.jpg\" width=\"200px\">","20f9f179":"Somehow the projection is of the wrong sign :","89ba801e":"We see that resp, mostly correlate to the last two resp_i. We find that higher return are associated with more ones in the different responses.","97ff6817":"We see that it removes some noise as we have more 'straighforward' targets (YYYYY or NNNNN). It seems to removes some edge case, but not all of them. ","ed35e9dd":"From https:\/\/www.kaggle.com\/fernandoramacciotti\/janestreet-denoising-rmt:","6f7243fd":"# Conclusion\n\nThose two approaches seems to bring some progress in the right direction, but I am not convinced this is the end of what we could do for target engineering. ","efbd1578":"As we only need to output a binary action, transforming the target into binary results is the first step. Looking at (and counting) patterns might be helpfull to understand what is happeing : ","8c3b0d91":"<a id='Denoising_Technique'><\/a>\n# Marcenko-Pastur Theorem - Denoising Technique\n\nThe denoising technique rely on the Marcenko-Pastur Theorem. Given a matrix X of iid observations of size TxN, where the underlying generating proce is centered (mean = 0) and has variance $\\sigma^2$. the Matrix $C = \\frac{1}{T}X^T X$ has eigenvalues $\\lambda$ that assymptotically converge to the Marcenko-Pastur density (as both T and N tend towards $+\\infty$, with $1 < \\frac{T}{N} < +\\infty$). \n\nThe Marcenko-Pastur density is given by:\n\n$ f(\\lambda) = \\frac{T}{N} \\frac{\\sqrt{(\\lambda_{+}-\\lambda)(\\lambda-\\lambda_{-})}}{2 \\pi \\lambda \\sigma^2} $ if $ \\lambda \\in [\\lambda_{-},\\lambda_{+}]$ \n\n$ f(\\lambda) = 0 $ else\n\nWhere $\\lambda_{+\/-} = \\sigma^2 (1 +\/- \\sqrt{ \\frac{N}{T} })^2$\n\nIf $\\sigma = 1$, then C is the correlation matrix associated with X.\n\nThis give a general distribution for comparison. Assuming that $\\lambda \\in [\\lambda_{-},\\lambda_{+}]$ is associated with noise we can remove those eigenvalues. (We can also remove the main eigenvalues, which is associated with a market wise component).\n\nThe graph below (from https:\/\/papers.ssrn.com\/sol3\/papers.cfm?abstract_id=3469961) show which eigenvalues correspond to the marcenko pastur distirbution. The idea is too keep only those that are in the right side of the graph. ","a55d6669":"![JS-MP-illustration.png](attachment:JS-MP-illustration.png)","5899760f":"### Skip to Main Sections:\n- [Target Analysis](#Target_Analysis)\n- [Target Denoising with Linear Regression](#Target_Denoising_linear)\n- [Marcenko-Pastur Theorem - Denoising Technique](#Denoising_Technique)\n- [Target Denoising with Random Matrix theory](#Target_Denoising_RMT)\n","b3873904":"I suspected that weight had to do with variance. But removing weight 0 removes a lot of values at the top with clear trend (YYYYY or NNNNN). Not sure how to interpret that. ","04833f53":"Not that bad. We might directly use the prediction. Or we might use a partially denoised version, to keep a bit of noise. (It seems that in fact I have better results with mainly noisy variable + a bit of denoising, ie setting the c value to value near 1)","312aa42b":"<a id='Target_Denoising_linear'><\/a>\n# Linear denoising\n\nI start with a simple I had in mind would be to try to predict the main response from others. I wasn't sure about this technique, notably because I wanted to take resp_i horizons or volatility into account. But I figured that any correction would be a multiplicative constant that wouldn't change the result of a linear regression. \n\nSo the main idea is to perform a regression of resp on other resp_i, to find what would be the expected resp. Denoising might then come from modifying the actual value with the expected one. ","b4901d64":"<a id='Target_Denoising_RMT'><\/a>\n# Target Denoising - RMT\nWe use the transformer to denoise the targets.","31a3493f":"### Loading data \n\nLoading a pickle file. Check this notebook [pickling](https:\/\/www.kaggle.com\/quillio\/pickling) if you haven't pickled your data set yet. Check this notebook [one liner to halve your memory usage](https:\/\/www.kaggle.com\/jorijnsmit\/one-liner-to-halve-your-memory-usage) if you want to reduce memory usage before pickling."}}