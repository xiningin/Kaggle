{"cell_type":{"16ff9e7b":"code","74120be3":"code","1d8ae791":"code","2c29a15a":"code","44ffe068":"code","87833108":"code","ed953ce5":"code","53f2f220":"code","664297ae":"code","5af4c1c1":"code","1f5a1a2b":"code","d6f70d0a":"code","356a66ef":"code","bd83611e":"code","4ca5004b":"code","15fc6a5a":"code","66648003":"code","2f6b13f8":"code","7004fd76":"code","c0fcc37d":"code","f77bb0d0":"code","1cb2d0ea":"code","5d0d6cc7":"code","38762c9d":"code","cc7e8418":"code","d6159c1d":"code","90da3fa7":"code","9337874e":"code","f5790f7f":"code","27f04cf6":"code","9852a38f":"code","22cb957b":"code","6260a1c1":"code","86816bc6":"code","ccf416ef":"code","ed8dfbb4":"code","0b8878e2":"code","ae4a7593":"code","79f9f680":"code","7dc76221":"code","6500f1cc":"code","41cafa05":"code","4a867b6a":"code","965d69c4":"code","bed316e7":"code","705cf84f":"code","fb495294":"code","57a62cfd":"code","72ec4562":"code","539e3b1f":"code","b012f666":"code","59cfbbd4":"code","3c5a4be9":"code","50cc309a":"code","2942058e":"markdown","efe1039e":"markdown","94344f54":"markdown","7d176941":"markdown","8b48cdc2":"markdown","9130565d":"markdown","6ef3575b":"markdown","f5ec6f30":"markdown","8d2899a0":"markdown","8f1ed80d":"markdown","348ad058":"markdown","57a576ee":"markdown","d8cf89d6":"markdown","cf7f9008":"markdown","12d0759f":"markdown","1d21a703":"markdown","e9d58803":"markdown","5b168858":"markdown","8e776585":"markdown","02cc173a":"markdown","41837695":"markdown","d884d390":"markdown","d14a1b3c":"markdown","a13605d3":"markdown","a5e30b9c":"markdown","5a66f1f8":"markdown","c4741c1d":"markdown","e39c5b86":"markdown","6fb8990d":"markdown","d168b64b":"markdown","84aa3be0":"markdown","b8833210":"markdown","7f0c2a7c":"markdown","f715ed48":"markdown","1c48e2dc":"markdown","fa01f0d3":"markdown","2ca1b140":"markdown","56270e62":"markdown","0b1326a1":"markdown","dd36712a":"markdown","3965bbdd":"markdown","92e245d4":"markdown","8612c183":"markdown","0aa0ccfb":"markdown"},"source":{"16ff9e7b":"import pandas as pd\nimport numpy as np\nimport seaborn as sns\nfrom scipy import stats\nimport warnings\nfrom matplotlib import pyplot as plt\nfrom matplotlib.gridspec import GridSpec\n'''\nBases: matplotlib.gridspec.GridSpecBase. A grid layout to place subplots within a figure. \nThe location of the grid cells is determined in a similar way to SubplotParams using left, right, top, bottom, wspace and hspace.\n'''\nfrom scipy.special import boxcox1p\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.ensemble import RandomForestRegressor\nwarnings.filterwarnings('ignore') \nfrom pandasql import sqldf\n'''\npandasql allows you to query pandas DataFrames using SQL syntax. \nIt works similarly to sqldf in R. pandasql seeks to provide a more familiar way of manipulating and cleaning data for people new to Python or pandas.\n\n'''","74120be3":"features = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/features.csv.zip')\ntrain = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/train.csv.zip')\nstores = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/stores.csv')\ntest = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/test.csv.zip')\nsample_submission = pd.read_csv('..\/input\/walmart-recruiting-store-sales-forecasting\/sampleSubmission.csv.zip')\n","1d8ae791":"features.head()","2c29a15a":"train.tail()","44ffe068":"stores.head()","87833108":"test.head()","ed953ce5":"sample_submission.head()","53f2f220":"features.shape, train.shape, stores.shape, test.shape","664297ae":"feature_store = features.merge(stores, how='inner', on = \"Store\")","5af4c1c1":"feature_store.head()","1f5a1a2b":"feature_store.dtypes","d6f70d0a":"print(\"train datatype:\\n\")\nprint(train.dtypes,\"\\n\")\nprint(\"------------------------------------------------------------\\n\")\nprint(\"test datatype:\\n\")\nprint(test.dtypes)\n","356a66ef":"feature_store.Date = pd.to_datetime(feature_store.Date)\ntrain.Date = pd.to_datetime(train.Date)\ntest.Date = pd.to_datetime(test.Date)","bd83611e":"feature_store.Date[0]","4ca5004b":"feature_store['Week'] = feature_store.Date.dt.week\nfeature_store['Year'] = feature_store.Date.dt.year\n","15fc6a5a":"train_detail = train.merge(feature_store, how='inner',\n                          on = ['Store','Date','IsHoliday']).sort_values(\n    by=['Store','Dept','Date']).reset_index(drop=True)","66648003":"train_detail.head()","2f6b13f8":"test_detail = test.merge(feature_store, how='inner',\n                         on = ['Store','Date','IsHoliday']).sort_values(\nby = ['Store','Dept','Date']).reset_index(drop=True)","7004fd76":"test_detail.head()","c0fcc37d":"# free space\ndel features, train, stores, test","f77bb0d0":"train_detail.isnull().sum()","1cb2d0ea":"test_detail.isnull().sum()","5d0d6cc7":"null_columns = (train_detail.isnull().sum(axis = 0)\/len(train_detail)).sort_values(ascending=False).index\nnull_data = pd.concat([\n    train_detail.isnull().sum(axis = 0),\n    (train_detail.isnull().sum(axis = 0)\/len(train_detail)).sort_values(ascending=False),\n    train_detail.loc[:, train_detail.columns.isin(list(null_columns))].dtypes], axis=1)\nnull_data = null_data.rename(columns={0: '# null', \n                                      1: '% null', \n                                      2: 'type'}).sort_values(ascending=False, by = '% null')\nnull_data = null_data[null_data[\"# null\"]!=0]\nnull_data\n","38762c9d":"weekly_sales_2010 = train_detail[train_detail.Year==2010]['Weekly_Sales'].groupby(train_detail['Week']).mean()\nweekly_sales_2011 = train_detail[train_detail.Year==2011]['Weekly_Sales'].groupby(train_detail['Week']).mean()\nweekly_sales_2012 = train_detail[train_detail.Year==2012]['Weekly_Sales'].groupby(train_detail['Week']).mean()\n\n\nplt.figure(figsize=(20,8))\nsns.lineplot(weekly_sales_2010.index, weekly_sales_2010.values)\nsns.lineplot(weekly_sales_2011.index, weekly_sales_2011.values)\nsns.lineplot(weekly_sales_2012.index, weekly_sales_2012.values)\n\n\nplt.grid()\nplt.xticks(np.arange(1, 53, step=1))\nplt.legend(['2010', '2011', '2012'], loc='best', fontsize=18)\nplt.title('Average Weekly Sales - Per Year', fontsize=20)\nplt.ylabel('Sales', fontsize=16)\nplt.xlabel('Week', fontsize=16)\nplt.show()\n","cc7e8418":"train_detail.loc[(train_detail.Year==2010) & (train_detail.Week==13), 'IsHoliday'] = True\ntrain_detail.loc[(train_detail.Year==2011) & (train_detail.Week==16), 'IsHoliday'] = True\ntrain_detail.loc[(train_detail.Year==2012) & (train_detail.Week==14), 'IsHoliday'] = True\ntest_detail.loc[(test_detail.Year==2013) & (test_detail.Week==13), 'IsHoliday'] = True\n","d6159c1d":"weekly_sales_mean = train_detail['Weekly_Sales'].groupby(train_detail['Date']).mean()\nweekly_sales_median = train_detail['Weekly_Sales'].groupby(train_detail['Date']).median()\n\nplt.figure(figsize=(20,8))\nsns.lineplot(weekly_sales_mean.index, weekly_sales_mean.values)\nsns.lineplot(weekly_sales_median.index, weekly_sales_median.values)\n\nplt.grid()\nplt.legend(['Mean', 'Median'], loc='best', fontsize=18)\nplt.title('Weekly Sales - Mean and Median', fontsize=20)\nplt.ylabel('Sales', fontsize=18)\nplt.xlabel('Date', fontsize=18)\nplt.show()\n","90da3fa7":"weekly_sales = train_detail['Weekly_Sales'].groupby(train_detail['Store']).mean()\n\nplt.figure(figsize=(20,8))\nsns.barplot(weekly_sales.index, weekly_sales.values)\n\nplt.grid()\nplt.title(\"Average Sales - per Store\", fontsize = 20)\nplt.ylabel('Sales', fontsize=18)\nplt.xlabel(\"Store\", fontsize=18)\nplt.show()\n","9337874e":"weekly_sales = train_detail['Weekly_Sales'].groupby(train_detail['Dept']).mean()\n\nplt.figure(figsize=(25,8))\nsns.barplot(weekly_sales.index, weekly_sales.values)\nplt.grid()\nplt.title(\"Average Sales - per Dept\",fontsize=20)\nplt.ylabel(\"Sales\", fontsize=18)\nplt.xlabel(\"Dept\", fontsize=18)\nplt.show()\n","f5790f7f":"sns.set(style='white')\ncorr = train_detail.corr()\nmask = np.triu(np.ones_like(corr, dtype=np.bool))\n'''\nnumpy. triu (m, k=0)[source] Upper triangle of an array. \nReturn a copy of an array with the elements below the k-th diagonal zeroed.\n'''\nf, ax = plt.subplots(figsize=(20,15))\ncmap = sns.diverging_palette(220, 10, as_cmap=True)\nplt.title(\"Correlation Matrix\", fontsize=20)\n\nsns.heatmap(corr, mask=mask, cmap=cmap, vmax=.3, center=0,\n           square=True, linewidths=.5, cbar_kws={\"shrink\":1}, annot=True)\n\nplt.show()","27f04cf6":"train_detail = train_detail.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'])\ntest_detail = test_detail.drop(columns=['Fuel_Price','MarkDown1','MarkDown2','MarkDown3','MarkDown4','MarkDown5'])\n","9852a38f":"def make_discrete_plot(feature):\n    \n    fig = plt.figure(figsize=(20,8))\n    gs = GridSpec(1,2)\n    \n    sns.boxplot(y = train_detail.Weekly_Sales, x=train_detail[feature],\n               ax=fig.add_subplot(gs[0,0]))\n    \n    plt.ylabel(\"Sales\", fontsize=18)\n    plt.xlabel(feature, fontsize=18)\n    sns.stripplot(y_train_detail.Weekly_Sales, x=train_detail[feature])","22cb957b":"def make_discrete_plot(feature):\n    \n    fig = plt.figure(figsize=(20,8))\n    gs = GridSpec(1,2)\n    \n    sns.boxplot(y=train_detail.Weekly_Sales, x=train_detail[feature], ax=fig.add_subplot(gs[0,0]))\n    \n    plt.ylabel('Sales', fontsize=18)\n    plt.xlabel(feature, fontsize=18)\n    \n    sns.stripplot(y=train_detail.Weekly_Sales, x=train_detail[feature], ax=fig.add_subplot(gs[0,1]))\n    \n    plt.ylabel('Sales', fontsize=18)\n    plt.xlabel(feature, fontsize=18)\n    fig.show()\n","6260a1c1":"\n'''\nNormality assumptions are critical for many univariate intervals and hypothesis tests. \nIt is important to test the normality assumption. If the data are in fact clearly not normal, \nthe Box-Cox normality plot can often be used to find a transformation that will approximately normalize the data.\n'''\ndef make_continuous_plot(feature):\n    \n    fig = plt.figure(figsize=(18,15))\n    gs = GridSpec(2,2)\n    \n    j = sns.scatterplot(y=train_detail['Weekly_Sales'], x=boxcox1p(train_detail[feature], 0.15), ax=fig.add_subplot(gs[0,1]), palette = 'blue')\n\n    plt.title('BoxCox 0.15\\n' + 'Corr: ' + str(np.round(train_detail['Weekly_Sales'].corr(boxcox1p(train_detail[feature], 0.15)),2)) +', Skew: ' + str(np.round(stats.skew(boxcox1p(train_detail[feature], 0.15), nan_policy='omit'),2)))\n    \n    j = sns.scatterplot(y=train_detail['Weekly_Sales'], x=boxcox1p(train_detail[feature], 0.25), ax=fig.add_subplot(gs[1,0]), palette = 'blue')\n\n    plt.title('BoxCox 0.25\\n' + 'Corr: ' + str(np.round(train_detail['Weekly_Sales'].corr(boxcox1p(train_detail[feature], 0.25)),2)) +', Skew: ' + str(np.round(stats.skew(boxcox1p(train_detail[feature], 0.25), nan_policy='omit'),2)))\n    \n    j = sns.distplot(train_detail[feature], ax=fig.add_subplot(gs[1,1]), color = 'green')\n\n    plt.title('Distribution\\n')\n    \n    j = sns.scatterplot(y=train_detail['Weekly_Sales'], x=train_detail[feature], ax=fig.add_subplot(gs[0,0]), color = 'red')\n\n    plt.title('Linear\\n' + 'Corr: ' + str(np.round(train_detail['Weekly_Sales'].corr(train_detail[feature]),2)) + ', Skew: ' + str(np.round(stats.skew(train_detail[feature], nan_policy='omit'),2)))\n    \n    fig.show()\n","86816bc6":"make_discrete_plot(\"IsHoliday\")","ccf416ef":"make_discrete_plot('Type')\n","ed8dfbb4":"train_detail.Type = train_detail.Type.apply(lambda x: 3 if x == 'A' else(2 if x == 'B' else 1))\ntest_detail.Type = test_detail.Type.apply(lambda x: 3 if x == 'A' else(2 if x == 'B' else 1))\n","0b8878e2":"make_continuous_plot(\"Temperature\")","ae4a7593":"make_continuous_plot(\"CPI\")","79f9f680":"train_detail = train_detail.drop(columns=[\"CPI\"])\ntest_detail = test_detail.drop(columns=['CPI'])","7dc76221":"make_continuous_plot('Unemployment')\n","6500f1cc":"make_continuous_plot('Size')\n","41cafa05":"def WMAE(dataset, real, predicted):\n    weights = dataset.IsHoliday.apply(lambda x: 5 if x else 1)\n    return np.round(np.sum(weights*abs(real-predicted))\/(np.sum(weights)), 2)\n","4a867b6a":"X_train = train_detail[['Store','Dept','IsHoliday','Size','Week','Type','Year']]\nY_train = train_detail['Weekly_Sales']\n","965d69c4":"X_train.head()","bed316e7":"Y_train.head()","705cf84f":"def random_forest(n_estimators, max_depth):\n    \n    result = []\n    for estimator in n_estimators:\n        for depth in max_depth:\n            wmaes_cv = []\n            for i in range(1,5):\n                print('k:', i, ', n_estimators:', estimator, ', max_depth:', depth)\n                x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\n                RF = RandomForestRegressor(n_estimators=estimator, max_depth=depth)\n                RF.fit(x_train, y_train)\n                predicted = RF.predict(x_test)\n                wmaes_cv.append(WMAE(x_test, y_test, predicted))\n            print('WMAE:', np.mean(wmaes_cv))\n            result.append({'Max_Depth': depth, 'Estimators': estimator, 'WMAE': np.mean(wmaes_cv)})\n    return pd.DataFrame(result)\n\n\n\ndef random_forest_II(n_estimators, max_depth, max_features):\n    \n    result = []\n    for feature in max_features:\n        wmaes_cv = []\n        for i in range(1,5):\n            print('k:', i, ', max_features:', feature)\n            x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\n            RF = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=feature)\n            RF.fit(x_train, y_train)\n            predicted = RF.predict(x_test)\n            wmaes_cv.append(WMAE(x_test, y_test, predicted))\n        print('WMAE:', np.mean(wmaes_cv))\n        result.append({'Max_Feature': feature, 'WMAE': np.mean(wmaes_cv)})\n    return pd.DataFrame(result)\n\n\ndef random_forest_III(n_estimators, max_depth, max_features, min_samples_split, min_samples_leaf):\n    \n    result = []\n    for split in min_samples_split:\n        for leaf in min_samples_leaf:\n            wmaes_cv = []\n            for i in range(1,5):\n                print('k:', i, ', min_samples_split:', split, ', min_samples_leaf:', leaf)\n                x_train, x_test, y_train, y_test = train_test_split(X_train, Y_train, test_size=0.3)\n                RF = RandomForestRegressor(n_estimators=n_estimators, max_depth=max_depth, max_features=max_features, \n                                           min_samples_leaf=leaf, min_samples_split=split)\n                RF.fit(x_train, y_train)\n                predicted = RF.predict(x_test)\n                wmaes_cv.append(WMAE(x_test, y_test, predicted))\n            print('WMAE:', np.mean(wmaes_cv))\n            result.append({'Min_Samples_Leaf': leaf, 'Min_Samples_Split': split, 'WMAE': np.mean(wmaes_cv)})\n    return pd.DataFrame(result)\n\n","fb495294":"n_estimators = [56, 58, 60]\nmax_depth = [25, 27, 30]\n\nrandom_forest(n_estimators, max_depth)\n","57a62cfd":"max_features = [2, 3, 4, 5, 6, 7]\n\nrandom_forest_II(n_estimators=58, max_depth=27, max_features=max_features)\n","72ec4562":"min_samples_split = [2, 3, 4]\nmin_samples_leaf = [1, 2, 3]\n\nrandom_forest_III(n_estimators=58, max_depth=27, max_features=6, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)\n","539e3b1f":"RF = RandomForestRegressor(n_estimators=58, max_depth=27, max_features=6, min_samples_split=3, min_samples_leaf=1)\nRF.fit(X_train, Y_train)\n","b012f666":"X_test = test_detail[['Store', 'Dept', 'IsHoliday', 'Size', 'Week', 'Type', 'Year']]\npredict = RF.predict(X_test)\n","59cfbbd4":"predict","3c5a4be9":"sample_submission['Weekly_Sales'] = predict\nsample_submission.to_csv('submission.csv',index=False)\n","50cc309a":"sample_submission.head()","2942058e":"The final model:\n\n","efe1039e":"The same chart, but showing also the median of the Sales and not divided by Year:\n\n","94344f54":"First, two plot functions that will help us.\n\nThe discrete plot is for finite numbers. We will use boxplot, to see the medians and interquartile ranges, and the striplot, which is a better way of seeing the distribution, even more when lots of outliers are present.\n\nThe continuous plot, as the name says, is for continuous variables. We will see the distribution of probabilities and use BoxCox to understand if there is increase of correlation and decrease of skewness for each variable. In some cases the process of transforming a variable can help, depending on the model.\n\n","7d176941":"Let's take a look at the Average Weekly Sales per Year and find out if there are another holiday peak sales that were not considered by 'IsHoliday' field.\n\n","8b48cdc2":"As we can see, there is one important Holiday not included in 'IsHoliday'. It's the Easter Day. It is always in a Sunday, but can fall on different weeks.\n\n* In 2010 is in Week 13\n* In 2011, Week 16\n* Week 14 in 2012\n* and, finally, Week 13 in 2013 for Test set\n\nSo, we can change to 'True' these Weeks in each Year.\n\n\n","9130565d":"So, turning the formula into a function, since we can't use GridSearchCV or RandomSearchCV:\n\n","6ef3575b":"You are provided with historical sales data for 45 Walmart stores located in different regions. Each store contains a number of departments, and you are tasked with predicting the department-wide sales for each store.\n\nIn addition, Walmart runs several promotional markdown events throughout the year. These markdowns precede prominent holidays, the four largest of which are the Super Bowl, Labor Day, Thanksgiving, and Christmas. The weeks including these holidays are weighted five times higher in the evaluation than non-holiday weeks. Part of the challenge presented by this competition is modeling the effects of markdowns on these holiday weeks in the absence of complete\/ideal historical data.\n\n","f5ec6f30":"stores.csv\n\nThis file contains anonymized information about the 45 stores, indicating the type and size of store.\n\n","8d2899a0":"### Machine Learning\n","8f1ed80d":"### Weekly_Sales x is Holiday","348ad058":"* Store - the store number\n* Date - the week\n* Temperature - average temperature in the region\n* Fuel_Price - cost of fuel in the region\n* MarkDown1-5 - anonymized data related to promotional markdowns that Walmart is running. MarkDown data is only available after Nov 2011, and is not available for all stores all the time. Any missing value is marked with an NA.\n* CPI - the consumer price index\n* Unemployment - the unemployment rate\n* IsHoliday - whether the week is a special holiday week\n","57a576ee":"test.csv\n\nThis file is identical to train.csv, except we have withheld the weekly sales. You must predict the sales for each triplet of store, department, and date in this file.\n\n","d8cf89d6":"there are Sales difference between the Store","cf7f9008":"This is important, there are columns with more than 60% of null values. If the correlations of these features with the target 'WeeklySales' are ~0, then it is not a good idea to use them. Also, they are anonymized fields, it can be difficult to know what they mean.\n\n","12d0759f":"'MarkDown' 1 to 5 are not strong correlated to 'Weekly_Sales' and they have a lot of null values, then we can drop them.\n\nAlso, 'Fuel_Price' is strong correlated to 'Year'. One of them must be dropped else they would carry similar information to the model. 'Year' will not be dropped, because it differentiate same Weeks for 'Store'+'Dept'.\n\nOther variables that have weak correlation with 'Weekly_Sales' can be analyzed to see if they are useful.","1d21a703":"The 'Date' field doesn't represent the day itself, but each week, ending every friday. So, it's interesting two create a 'Week' field and also we can create one for 'Year'.\n\n","e9d58803":"features.csv\n\nThis file contains additional data related to the store, department, and regional activity for the given dates. It contains the following fields:\n\n","5b168858":"Correlation Metrics:\n\n* 0: no correlation at all\n* 0-0.3: weak correlation\n* 0.3-0.7: moderate correlaton\n* 0.7-1: strong correlation\n\nPositive Correlation indicates that when one variable increase, the other also does. Negative is the opposite.\n\n","8e776585":"### Analyzing Variables\n","02cc173a":"And, finally, we will continue with this variable, since it has moderate correlation with 'WeeklySales'.\n\n","41837695":"For convenience, the four holidays fall within the following weeks in the dataset (not all holidays are in the data):\n\n* Super Bowl: 12-Feb-10, 11-Feb-11, 10-Feb-12, 8-Feb-13\n* Labor Day: 10-Sep-10, 9-Sep-11, 7-Sep-12, 6-Sep-13\n* Thanksgiving: 26-Nov-10, 25-Nov-11, 23-Nov-12, 29-Nov-13\n* Christmas: 31-Dec-10, 30-Dec-11, 28-Dec-12, 27-Dec-13\n\n","d884d390":"### Variables Correlation","d14a1b3c":"train.csv\n\nThis is the historical training data, which covers to 2010-02-05 to 2012-11-01. Within this file you will find the following fields:\n\n* Store - the store number\n* Dept - the department number\n* Date - the week\n* Weekly_Sales -  sales for the given department in the given store\n* IsHoliday - whether the week is a special holiday week\n","a13605d3":"And there are Sales difference between the Departments too. Also some Depts are not in the list, like number '15', for example.\n\n","a5e30b9c":"As we can see, the 'Date' field has string type. We can convert them to datetime and see as well if 'train' and 'test' dataframes has 'Date' type to convert.\n\n","5a66f1f8":"Although skewness changes, correlation doesn't seem to change at all. We can decide to drop it.\n\n","c4741c1d":"We don't know what 'Type' is, but we can assume that A > B > C in terms of Sales Median. So, let's treat it as an ordinal variable and replace its values.\n\nOrdinal variables are explained in the figure below.\n\n","e39c5b86":"where\n\n* n is the number of rows\n* \\\\( \\hat{y}_i \\\\) is the predicted sales\n* \\\\( y_i \\\\) is the actual sales\n* \\\\( w_i \\\\) are weights. w = 5 if the week is a holiday week, 1 otherwise\n","6fb8990d":"![image.png](attachment:ebd36fbc-5879-49b2-ad32-a3ce67398580.png)","d168b64b":"Average Sales per Store and Department\n","84aa3be0":"Just as an observation, the mean and the median are very different, suggesting that some stores\/departments might sell much more than others.\n\n","b8833210":"![image.png](attachment:8db7e9e4-b200-4590-a984-4aa147abe9af.png)","7f0c2a7c":"### Predictions\n","f715ed48":"###  Exploratory Analysis & Data Cleaning","1c48e2dc":"Although skewness changes, correlation doesn't seem to change at all. We can decide to drop it.\n\n","fa01f0d3":"### Weekly_sales x Temperature","2ca1b140":"### Holidays Analysis","56270e62":"Although skewness changes, correlation doesn't seem to change at all. We can decide to drop it.\n\n","0b1326a1":"Let's see the correlation between variables, using Pearson Correlation.\n\n","dd36712a":"### Weekly Sales x Unemployment","3965bbdd":"This field is going to be important to differentiate Week Holidays. As we can see, Week Holidays have more high sales events than non-Holiday Weeks.\n\n","92e245d4":"* In this section, we will explore the datasets provided, join information between some of them and make relevant transformations.\n\n* Let's start by merging data from two of the datasets: features and stores. They have a commom key 'Stores'. The data will be loaded into 'feat_sto'.\n\n","8612c183":"### Weekly Sales x CPI\n","0aa0ccfb":"weekly_Sales X Type"}}