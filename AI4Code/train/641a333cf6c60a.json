{"cell_type":{"f90a91ab":"code","2e4687bf":"code","0fa48174":"code","48310512":"code","1117c536":"code","852414ac":"code","d1b23eae":"code","5e8f22a5":"code","9f12d4e1":"code","1289fe69":"code","62eaa1ab":"code","83f654bf":"code","7f3fde31":"code","8869afc0":"code","80687a03":"code","90a2fe79":"code","8330d8a3":"code","89a0ca72":"code","5e3e4b87":"code","fcd88240":"code","c618d1cb":"code","ff7ce7b6":"code","95bd9f0d":"code","0ed52e61":"code","fa68adfe":"code","2b4a5802":"code","2bb0e5f4":"code","3f4c70be":"code","93349470":"code","d4fe6e19":"code","cbc0fc9c":"code","37040c95":"code","9e9638c1":"code","536048ea":"code","12804892":"code","ee2795c9":"code","bfaeebd6":"code","548cd1bf":"code","02d08668":"code","71364f06":"code","3b65aa98":"code","a53311df":"code","b59d94fa":"markdown","6c8ce8b9":"markdown","a2f3eb06":"markdown","bf04edc3":"markdown","de2b27cc":"markdown","918dadea":"markdown","f1c773f1":"markdown","c7565f6e":"markdown","e75ed34b":"markdown","163fdb91":"markdown","bc17014c":"markdown","dafde4c3":"markdown","0fcd5ed2":"markdown","33e69306":"markdown","13beb68f":"markdown","aac5b0a4":"markdown","119f75ad":"markdown","c9b514a9":"markdown","7bb31ca6":"markdown","d7d49a88":"markdown","01776d2f":"markdown","184f69b9":"markdown"},"source":{"f90a91ab":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","2e4687bf":"import numpy as np\nimport pandas as pd\n\n# visualization import\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n%matplotlib inline\n\n# define the plot size default\nfrom pylab import rcParams\nrcParams['figure.figsize'] = (12,5)\n\n# load specific forecasting tools\nfrom statsmodels.tsa.api import VAR\nfrom statsmodels.tsa.stattools import adfuller\nfrom statsmodels.tsa.seasonal import seasonal_decompose\nfrom statsmodels.tools.eval_measures import mse,rmse\n\n# Ignore harmless warnings\nimport warnings\nwarnings.filterwarnings(\"ignore\")\n\n# Load datasets\ndf = pd.read_csv('\/kaggle\/input\/time-series-data-1\/M2SLMoneyStock.csv',index_col=0, parse_dates=True)\ndf.index.freq = 'MS'\n\nsp = pd.read_csv('\/kaggle\/input\/time-series-data-1\/PCEPersonalSpending.csv',index_col=0, parse_dates=True)\nsp.index.freq = 'MS'\n","0fa48174":"df.head()","48310512":"sp.head()","1117c536":"df = df.join(sp)\ndf.head()","852414ac":"# check for null or missing values\ndf.isna().sum()","d1b23eae":"# the number of instances\nlen(df)","5e8f22a5":"df.info()","9f12d4e1":"title = 'M2 Money Stock vs Personal Consumption Expenditures'\nylabel = 'Billions of Dollars'\nxlabel='' #xlabel is not required. The datetime indes will be taken as the points on the x axis\n\nax = df['Spending'].plot(legend=True,title=title)\nax.autoscale(axis='x',tight=True)\nax.set(xlabel=xlabel,ylabel=ylabel)\ndf['Money'].plot(legend=True);","1289fe69":"def dickey_fuller(series,title='Your Dataset'):\n    '''Hypothesis Test for stationarity '''\n    print(f'Augmented Dickey Fuller Test for the dataset {title}')\n    \n    result = adfuller(series.dropna(),autolag='AIC')\n    labels = ['ADF test statistics','p-value','#lags','#observations'] # use help(adfuller) to understand why these labels are chosen\n    \n    outcome = pd.Series(result[0:4],index=labels)\n    \n    for key,val in result[4].items():\n        outcome[f'critical value ({key})'] = val\n        \n    print(outcome.to_string()) # this will not print the line 'dtype:float64'\n    \n    if result[1] <= 0.05:\n        print('Strong evidence against the null hypothesis') # Ho is Data is not stationary, check help(adfuller)\n        print('Reject the null hypothesis')\n        print('Data is Stationary')\n    else:\n        print('Weak evidence against the Null hypothesis')\n        print('Fail to reject the null hypothesis')\n        print('Data has a unit root and is non stationary')\n        \n    \n    \n    ","62eaa1ab":"dickey_fuller(df['Money'],title='Money')","83f654bf":"dickey_fuller(df['Spending'],title='Spending')","7f3fde31":"df_diff = df.diff() # by default, diff performs the first order difference","8869afc0":"df_diff.head()","80687a03":"df_diff = df_diff.dropna()","90a2fe79":"dickey_fuller(df_diff['Money'],title='Money 1st Order Diff')","8330d8a3":"dickey_fuller(df_diff['Spending'],title='Spending 1st Order Diff')","89a0ca72":"df_diff = df_diff.diff().dropna()\ndickey_fuller(df_diff['Money'],title='Money 2nd Order Diff')","5e3e4b87":"dickey_fuller(df_diff['Spending'],title='Spending 2nd Order Diff')","fcd88240":"df_diff.head()","c618d1cb":"# check the length of the transformed df -- > should be 2 rows lesser than the original\nlen(df_diff)","ff7ce7b6":"nobs = 12\ntrain = df_diff[:-nobs]\ntest = df_diff[-nobs:]","95bd9f0d":"len(train), len(test)","0ed52e61":"p = [1,2,3,4,5,6,7]  # try with list of 7 p values\n\nfor i in p:\n    model = VAR(train)\n    results = model.fit(i)\n    print(f'VAR Order {i}')\n    print('AIC {}'.format(results.aic))\n    print('BIC {}'.format(results.bic))\n    print()","fa68adfe":"# lets confirm that both the variables are included in the model\nmodel.endog_names","2b4a5802":"results = model.fit(5)\nresults.summary()","2bb0e5f4":"lag_order = results.k_ar\nlag_order","3f4c70be":"z = results.forecast(y=train.values[-lag_order:],steps = 12)\nz","93349470":"test","d4fe6e19":"idx = pd.date_range(start='1\/1\/2015',periods=12,freq='MS')\ndf_forecast = pd.DataFrame(z,index=idx,columns=['Money2D','Spending2D'])","cbc0fc9c":"df_forecast[:5]","37040c95":"df_forecast['Money1D'] = (df['Money'].iloc[-nobs-1] - df['Money'].iloc[-nobs-2]) + df_forecast['Money2D'].cumsum()","9e9638c1":"# Now build the forecast values from the first difference set\ndf_forecast['MoneyForecast'] = df['Money'].iloc[-nobs-1] + df_forecast['Money1D'].cumsum()","536048ea":"df_forecast","12804892":"# Add the most recent first difference from the training side of the original dataset to the forecast cumulative sum\ndf_forecast['Spending1D'] = (df['Spending'].iloc[-nobs-1]-df['Spending'].iloc[-nobs-2]) + df_forecast['Spending2D'].cumsum()\n\n# Now build the forecast values from the first difference set\ndf_forecast['SpendingForecast'] = df['Spending'].iloc[-nobs-1] + df_forecast['Spending1D'].cumsum()","ee2795c9":"df_forecast","bfaeebd6":"results.plot();","548cd1bf":"results.plot_forecast(12);","02d08668":"df['Money'][-nobs:].plot(figsize=(12,5),legend=True).autoscale(axis='x',tight=True)\ndf_forecast['MoneyForecast'].plot(legend=True);","71364f06":"df['Spending'][-nobs:].plot(figsize=(12,5),legend=True).autoscale(axis='x',tight=True)\ndf_forecast['SpendingForecast'].plot(legend=True);","3b65aa98":"RMSE1 = rmse(df['Money'][-nobs:], df_forecast['MoneyForecast'])\nprint(f'Money VAR(5) RMSE: {RMSE1:.3f}')","a53311df":"RMSE2 = rmse(df['Spending'][-nobs:], df_forecast['SpendingForecast'])\nprint(f'Spending VAR(5) RMSE: {RMSE2:.3f}')","b59d94fa":"Similarly,lets do this for the spending column","6c8ce8b9":"## Imports and Load the dataset","a2f3eb06":"## Predict the next 12 values\nFor predictions, VAR model uses .forecast() instead of predictions. This is similar to theHolt Winters. One of the requirement for VAR model is that we need to pass the lag order of the number of previous observations as well. Unfortunately, this lag order does not have the datetime index and hence we will have to build this ourselves.","bf04edc3":"## VAR Model Order Selection \nVAR(p) of order p\n\nSo far in my other notebooks on time series on ARIMA and SARIMA, I used the auto_arima from pmdarima library. That method won't work here to determine the order of the VAR model. I will test this manually with a list of order p values and select the p value which returns the minimum AIC or BIC metric. Check Akaike Information Criterion and Bayesian Information Criterion for more details.","de2b27cc":"Money feature is still not stationary while Spending is stationary after the first order difference. I will take the second order difference of both the series so that they retain the same shape. Rerun the dickey fuller test for stationarity.","918dadea":"## Invert the Transformations","f1c773f1":"Cool, so we do not have any null values in the dataset.","c7565f6e":"The datatype of the two features is also fine. All hygiene checks done. we will test for stationarity using the augmented dikcey fuller test.","e75ed34b":"## Model Evaluation","163fdb91":"## Plot the source data","bc17014c":"## Train Test Split\n","dafde4c3":"Spending is correlated with the money in hand and vice versa. We need to join these two dataframes into one for our model. The datetime index is the same and that makes the joining task easy. ","0fcd5ed2":"So both the Money and the Spending is non stationary. We will take the first order difference of the entire dataframe and re-run the dickey fuller test and store it in a separate dataframe so that the original dataframe is retained. ","33e69306":"## Fit the VAR(5) model","13beb68f":"# VAR - Vector AutoRegression\nVAR is used when there are variables which have an effect on each other. In case of restaurant visitors where the exogenous variable $holidays$ affected the $visitors$, it wasn't vice versa. However, there are many cases in which two variables have an effect on each other. Example is Personal Disposable Income affects the Spending and the amount of Spending is affected by the available Personal Disposable income in the hands of the people. The dataset here contains the income in the hands of the people and another dataset contains the spending. ","aac5b0a4":"Now, both the correlated features - Money and Spending are stationary. Hence we are good to go with the remaining steps of training, test predictions and finally forecasting into the future.","119f75ad":"Order 5 has the least AIC value. Lets select p = 5 in the modeling.","c9b514a9":"\n## Plot the results","7bb31ca6":"The forecasted values represent the 2nd order difference forecast. To compare them to the original data we have to roll back each difference. To roll back a first-order difference we take the most recent value on the training side of the original series, and add it to a cumulative sum of forecasted values. When working with second-order differences we first must perform this operation on the most recent first-order difference.","d7d49a88":"## Inspect the data","01776d2f":"There is NaN introduced due to the first order difference. So lets drop this missing value row.","184f69b9":"Check the correlation matrix printed at the last. "}}