{"cell_type":{"e4b04f0a":"code","29084ee3":"code","11f7fa90":"code","64b94dd6":"code","a94e77e5":"code","6593d889":"code","ca030ca3":"code","187d39c0":"code","2b21457f":"code","3cb993d2":"code","81f98e0a":"code","e4ab21f4":"code","0fb4ee7b":"code","1d7b901a":"code","3493f1de":"code","61d00eb2":"code","40f4b049":"code","025a5160":"code","0b235ae4":"code","fbec2a22":"code","e7806533":"code","ef460411":"code","3a8d3282":"code","bc9a15f4":"code","991cc61c":"code","e099da60":"code","9cae674d":"code","61b1c906":"code","68f5c2e9":"code","8b651903":"code","0815969c":"code","2aa1df5c":"code","e91fa2c9":"code","e747fe52":"code","1256005b":"code","5a041dfb":"code","bc6ac974":"markdown","fb54d722":"markdown","ed17c532":"markdown","2043289a":"markdown","4eeda859":"markdown","f5b45886":"markdown","da159163":"markdown","4409e468":"markdown","7e419f1b":"markdown","84576ab8":"markdown","874f4880":"markdown","f5de8fc1":"markdown","114c61b3":"markdown","18ae21a3":"markdown","049304de":"markdown","68a11381":"markdown","3b562bb4":"markdown"},"source":{"e4b04f0a":"import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib import cm\nimport seaborn as sns\n\nimport random\nimport os\nimport gc  # garbage collector\nimport datetime\nfrom tqdm import tqdm\n\nimport cv2\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import accuracy_score, confusion_matrix, f1_score\n\nimport tensorflow as tf\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input, Conv2D, Dense, Flatten, MaxPooling2D, BatchNormalization, Dropout, GlobalAveragePooling2D\nfrom tensorflow.keras.callbacks import EarlyStopping, CSVLogger, ModelCheckpoint, TensorBoard\nfrom tensorflow.keras.preprocessing import image_dataset_from_directory\nfrom tensorflow.keras.preprocessing.image import ImageDataGenerator\nfrom tensorflow.keras.applications import EfficientNetB0","29084ee3":"resized_train_path = \"..\/input\/diabetic-retinopathy-resized\/resized_train\/resized_train\"\nresized_train_cropped_path = \"..\/input\/diabetic-retinopathy-resized\/resized_train_cropped\/resized_train_cropped\"\n\ntrain_labels_path = \"..\/input\/diabetic-retinopathy-resized\/trainLabels.csv\"\ntrain_labels_cropped_path = \"..\/input\/diabetic-retinopathy-resized\/trainLabels_cropped.csv\"","11f7fa90":"train_labels = pd.read_csv(train_labels_path)\ntrain_labels.head()","64b94dd6":"train_labels.info()","a94e77e5":"level_column = train_labels['level']\nlevel_column","6593d889":"level_column.plot(kind='hist', figsize=(10, 5), cmap=cm.get_cmap('flag'))","ca030ca3":"train_labels_cropped = pd.read_csv(train_labels_cropped_path)\ntrain_labels_cropped","187d39c0":"train_labels_cropped.info()","2b21457f":"level_cropped_col = train_labels_cropped['level']\nlevel_cropped_col","3cb993d2":"level_cropped_col.plot(kind='hist', figsize=(10, 5), cmap=cm.get_cmap('ocean'))","81f98e0a":"resized_train_list = os.listdir(resized_train_path)\nlen(resized_train_list)","e4ab21f4":"plt.figure(figsize=(25, 20))\nfor i in range(1, 26):\n    plt.subplot(5, 5, i)\n    img_name = random.choice(resized_train_list)\n    img_path = os.path.join(resized_train_path, img_name)\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.imshow(img)\n    plt.xlabel(img.shape[1])\n    plt.ylabel(img.shape[0])","0fb4ee7b":"resized_train_cropped_list = os.listdir(resized_train_cropped_path)\nlen(resized_train_cropped_list)","1d7b901a":"plt.figure(figsize=(26, 24))\nfor i in range(1, 26):\n    plt.subplot(5, 5, i)\n    img_name = random.choice(resized_train_cropped_list)\n    img_path = os.path.join(resized_train_cropped_path, img_name)\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    plt.imshow(img)\n    plt.xlabel(img.shape[1])\n    plt.ylabel(img.shape[0])","3493f1de":"del resized_train_list\ngc.collect()","61d00eb2":"plt.figure(figsize=(20, 25))\nfor i in range(1, 16):\n    plt.subplot(5, 3, i)\n    plt.tight_layout()\n    plt.title(\"Color Histogram\")\n    plt.xlabel(\"Intensity Value\")\n    plt.ylabel(\"Number of Pixels\")\n    img_name = random.choice(resized_train_cropped_list)\n    img_path = os.path.join(resized_train_cropped_path, img_name)\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    channels = cv2.split(img)\n    colors = ['r', 'g', 'b']\n    \n    for (channel, color) in zip(channels, colors):\n        hist = cv2.calcHist([channel], [0], None, [256], [0, 256])\n        plt.plot(hist, color=color)\n        plt.xlim([0, 256])","40f4b049":"img_width = 100\nimg_height = 100\n\ndef read_img(img_name, resize=False):\n    img_path = os.path.join(resized_train_cropped_path, img_name)\n    img = cv2.imread(img_path)\n    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n    \n    if resize:\n        img = cv2.resize(img, (img_width, img_hight))\n    \n    return img","025a5160":"def ben_graham(img):\n    img_ben = cv2.addWeighted(img, 4, cv2.GaussianBlur(img, (0, 0), 10), -4, 128)\n    return img_ben","0b235ae4":"def hist_equalization(img):\n    red, green, blue = cv2.split(img)\n    hist_red = cv2.equalizeHist(red)\n    hist_green = cv2.equalizeHist(green)\n    hist_blue = cv2.equalizeHist(blue)\n    \n    img_eq = cv2.merge((hist_red, hist_green, hist_blue))\n    \n    return img_eq","fbec2a22":"equal_hist_images = resized_train_cropped_list.copy()\nlen(equal_hist_images)","e7806533":"plt.figure(figsize=(26, 24))\ncounter = 0\nfor img_name in equal_hist_images:\n    counter += 1\n    plt.subplot(5, 5, counter)\n    plt.tight_layout()\n    # level_cropped_col is the labels, we've created it above\n    plt.title(level_cropped_col[counter - 1])\n    \n    img = read_img(img_name)\n    \n    # Applying the Histogram Equaliztion\n    img_eq = hist_equalization(img)\n    \n    plt.imshow(img_eq)\n    plt.xlabel(img_eq.shape[1])\n    plt.xlabel(img_eq.shape[0])\n    \n    if counter == 25:\n        break","ef460411":"plt.figure(figsize=(20, 25))\ncounter = 0\nfor img_name in equal_hist_images:\n    counter += 1\n    plt.subplot(5, 3, counter)\n    plt.tight_layout()\n    \n    img = read_img(img_name)\n    \n    # Applying the Histogram Equaliztion\n    img_eq = hist_equalization(img)\n    \n    channels = cv2.split(img_eq)\n    colors = ['r', 'g', 'b']\n    \n    for (channel, color) in zip(channels, colors):\n        hist = cv2.calcHist([channel], [0], None, [256], [0, 256])\n        plt.plot(hist, color=color)\n        plt.xlim([0, 256])\n    \n    if counter == 15:\n        break","3a8d3282":"ben_images = resized_train_cropped_list.copy()\nlen(ben_images)","bc9a15f4":"plt.figure(figsize=(26, 24))\ncounter = 0\nfor img_name in ben_images:\n    counter += 1\n    plt.subplot(5, 5, counter)\n    plt.tight_layout()\n    # level_cropped_col is the lebels list\n    plt.title(level_cropped_col[counter - 1])\n    \n    img = read_img(img_name)\n    \n    # Applying Ben Graham's Method\n    img_ben = ben_graham(img)\n    \n    plt.imshow(img_ben)\n    plt.xlabel(img_ben.shape[1])\n    plt.ylabel(img_ben.shape[0])\n    \n    if counter == 25:\n        break","991cc61c":"plt.figure(figsize=(20, 25))\ncounter = 0\nfor img_name in equal_hist_images:\n    counter += 1\n    plt.subplot(5, 3, counter)\n    plt.tight_layout()\n    \n    img = read_img(img_name)\n    \n    # Applying Ben Graham's Method\n    img_ben = ben_graham(img)\n    \n    channels = cv2.split(img_ben)\n    colors = ['r', 'g', 'b']\n    \n    for (channel, color) in zip(channels, colors):\n        hist = cv2.calcHist([channel], [0], None, [256], [0, 256])\n        plt.plot(hist, color=color)\n        plt.xlim([0, 256])\n    \n    if counter == 15:\n        break","e099da60":"train_labels_cropped.head()","9cae674d":"train_labels_cropped['image_name'] = [img + '.jpeg' for img in train_labels_cropped['image']]\ntrain_labels_cropped.head()","61b1c906":"train_ds, val_ds = train_test_split(train_labels_cropped, test_size=0.2)\ntrain_ds.shape, val_ds.shape","68f5c2e9":"def my_processes(img):\n    img = cv2.resize(img, (img_width, img_height))\n    \n    # Apply your image processing method\n    img = ben_graham(img)\n    \n    return img","8b651903":"train_datagen = ImageDataGenerator(rescale = 1.\/255.,\n                                  rotation_range = 40,\n                                  width_shift_range = 0.2,\n                                  height_shift_range = 0.2,\n                                  shear_range = 0.2,\n                                  horizontal_flip = True,\n                                  preprocessing_function=ben_graham)","0815969c":"val_datagen = ImageDataGenerator(rescale = 1.\/255.)","2aa1df5c":"batch_size = 32\n\ntrain_dataset = train_datagen.flow_from_dataframe(train_ds,\n                                                 resized_train_cropped_path,\n                                                 x_col=\"image_name\",\n                                                 y_col=\"level\",\n                                                 class_mode='raw',\n                                                 batch_size=batch_size,\n                                                 target_size=(img_width, img_height))\n\n\nval_dataset = val_datagen.flow_from_dataframe(val_ds,\n                                             resized_train_cropped_path,\n                                             x_col='image_name',\n                                             y_col='level',\n                                             class_mode='raw',\n                                             batch_size=batch_size,\n                                             target_size=(img_width, img_height))","e91fa2c9":"model = EfficientNetB0(include_top = False, weights='imagenet', input_shape=(100, 100, 3))\n\n# Freeze pre-trained weights\nmodel.trainable = False\n\nprint(model.trainable_weights)","e747fe52":"x = Dropout(0.2)(model.output)\n\nx = Dense(1024, activation='relu')(x)\nx = Dense(1024, activation='relu')(x)\nx = Dropout(0.2)(x)\n\nx = Dense(1024, activation='relu')(x)\nx = Dropout(0.1)(x)\n\nx = Dense(1024, activation='relu')(x)\nx = Dropout(0.1)(x)\n\nx = Dense(512, activation='relu')(x)\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.2)(x)\n\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.1)(x)\n\nx = Dense(512, activation='relu')(x)\nx = Dropout(0.1)(x)\n\n\nx = GlobalAveragePooling2D()(x)\nx = Dropout(0.1)(x)\nclassifier = Dense(5, activation='softmax')(x)\n\nmodel = Model(inputs=model.input, outputs=classifier)","1256005b":"model.compile(optimizer=tf.keras.optimizers.Adam(), loss='sparse_categorical_crossentropy', metrics=['sparse_categorical_accuracy'])","5a041dfb":"model.fit_generator(train_dataset, epochs=3, validation_data=val_dataset, verbose=1,\n          callbacks=[EarlyStopping(monitor='val_loss', patience=2, verbose=1, restore_best_weights=True)])","bc6ac974":"### Exploring data","fb54d722":"# Loading Dataset","ed17c532":"****Now we'll calculate histogram equility for some images with its label****\n\n****Taking a copy from the cropped images dataset(list) to apply histogram equalization****","2043289a":"# Visualizing resized train images","4eeda859":"# Visualizing resized train cropped images","f5b45886":"# Image Processing","da159163":"# Import Packages","4409e468":"# Preprocessing Functions","7e419f1b":"# Creating Model","84576ab8":"# Applying augmentation to datasets","874f4880":"****Now we'll calculate Ben Graham's processing method for some images with its label****\n\n****Taking a copy from the cropped images dataset(list) to apply Ben Graham's processing method****","f5de8fc1":"### Reading data paths","114c61b3":"### Histogram Equalization","18ae21a3":"****we will perform two type of filtering:****\n\n****First: Histogram Equalization****\n\n****Second: Ben Graham's Processing Method****","049304de":"****Explore histogram of a random image****","68a11381":"****Delleting resized_train_list to free memory****","3b562bb4":"****turns out that the cropped images removes unimportant black backgraound so we we'll work on the cropped images dataset****"}}