{"cell_type":{"024f1bc4":"code","2488c83b":"code","131d9103":"code","3aba82d5":"code","739657e6":"code","75589f0c":"code","c89e1a88":"code","f7710d0e":"code","a3699976":"code","c1b61b41":"code","d468c349":"code","fb1aed61":"code","f91a417e":"code","6352c2d4":"code","bb4a2a97":"code","842ee417":"code","11956156":"code","4af911b3":"code","41ffa959":"code","53d5b573":"code","d051f472":"code","97360813":"code","d93017b6":"code","dc8b1ea5":"code","3e11c74c":"code","4a8bceb9":"code","5424845c":"code","0f811afb":"code","3b387b14":"code","b7b0c509":"code","203240cd":"code","c99bc7d3":"code","783ff219":"code","259f1967":"code","2a094345":"code","64381dd7":"code","58331f99":"code","cf2fbadf":"code","dbbad650":"code","56b5ccd7":"code","bca08369":"code","79c31cf0":"code","a69d87c4":"code","900b50bd":"code","6afbe39b":"code","8fdba711":"code","07f8a8ef":"code","c3d441a1":"code","8163e4b0":"code","751e7fe3":"code","35b02bbc":"code","9ec9dfd1":"code","41df1423":"code","bee8324f":"code","5b7eadf9":"code","0d8c0acb":"code","98380911":"code","bb442ecf":"code","31c49b1d":"code","46b126d8":"code","7ce8d61a":"code","a4aa4066":"code","40f3d00a":"code","3f1c4581":"code","2571de59":"code","fb26b4a5":"code","256311db":"code","6217dc44":"code","8031b47c":"code","6e1f79e5":"code","df3ba814":"code","c892b95d":"code","21ff5c30":"code","cbe4c631":"code","765e57ae":"code","2db2aea8":"code","a7cffbdf":"code","29e74118":"code","c346fd55":"code","38fe4787":"code","5cfddc8e":"code","d29dbb70":"code","6a0a8bab":"code","cbaf9407":"code","5c6d889d":"markdown","7b54428e":"markdown","9c0a95f5":"markdown","e18d4126":"markdown","bff8d8b9":"markdown","9f70501f":"markdown","22271ff2":"markdown","7702aca3":"markdown","2a33b4af":"markdown","06ebea22":"markdown","7c4d45b9":"markdown","efec910a":"markdown","7c76d354":"markdown","0f7770c6":"markdown","e3b4eb01":"markdown","825703fd":"markdown","9225ce08":"markdown","d423a256":"markdown","0c945940":"markdown","4bcb3886":"markdown","fb929a0e":"markdown","5100f62b":"markdown","a75eae44":"markdown","46558db2":"markdown","47d3758a":"markdown","6313c3e7":"markdown","b9d1fdc2":"markdown","04119287":"markdown","d4d2f42b":"markdown","1e9b7bd8":"markdown","0340d13b":"markdown","64a5e2c2":"markdown","41616904":"markdown"},"source":{"024f1bc4":"#Install the following libraries if not installed\n#!pip install transformers==3.0.0\n#!pip install plotly\n\n# Download the glove 100 Dim embedding(https:\/\/www.kaggle.com\/snowdog\/glove100)\n\n# Link to the dataset (https:\/\/www.kaggle.com\/c\/nlp-getting-started)","2488c83b":"#System libraies\nimport logging\nimport time\nimport re\nfrom platform import python_version\n\n#plotting libraries\nimport matplotlib\nimport matplotlib.pyplot as plt\nimport seaborn as sns\ncolor = sns.color_palette()\n\nfrom plotly import tools\nimport plotly.graph_objs as go\nimport plotly.offline as py\npy.init_notebook_mode(connected=True)\n\n\n\n%matplotlib inline\n\n\n#linear algebra and data processing libraries\nimport numpy as np\nimport pandas as pd\nimport string\n\n#Machine learning libraries\nimport sklearn\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport transformers\nfrom sklearn.metrics import roc_auc_score\nfrom torch.autograd import Variable\nfrom nltk.corpus import stopwords\n\npd.options.mode.chained_assignment = None\npd.options.display.max_columns = 999","131d9103":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle\/python Docker image: https:\/\/github.com\/kaggle\/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I\/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"..\/input\/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('\/kaggle\/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (\/kaggle\/working\/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to \/kaggle\/temp\/, but they won't be saved outside of the current session","3aba82d5":"#Check the right version of libraries are installed\nprint(\"python version==%s\" % python_version())\nprint(\"pandas==%s\" % pd.__version__)\nprint(\"numpy==%s\" % np.__version__)\nprint(\"torch==%s\" % torch.__version__)\nprint(\"sklearn==%s\" % sklearn.__version__)\nprint(\"transformers==%s\" % transformers.__version__)\nprint(\"matplotlib==%s\" % matplotlib.__version__)","739657e6":"# Suppress a warning with comments that have more than 512 words - the upper limit for BERT\nlogging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)","75589f0c":"original_train_df = pd.read_csv(\"..\/input\/nlp-getting-started\/train.csv\")\noriginal_test_df = pd.read_csv(\"..\/input\/nlp-getting-started\/test.csv\")\nprint(\"Train shape : \", original_train_df.shape)\nprint(\"Test shape : \", original_test_df.shape)","c89e1a88":"import copy\ntrain_df = copy.copy(original_train_df)\ntest_df = copy.copy(original_test_df)","f7710d0e":"train_df.describe()","a3699976":"train_df.head()","c1b61b41":"test_df.head()  ","d468c349":"missing_cols = ['keyword', 'location']\n\nfig, axes = plt.subplots(ncols=2, figsize=(17, 4), dpi=100)\n\nsns.barplot(x=train_df[missing_cols].isnull().sum().index, y=train_df[missing_cols].isnull().sum().values, ax=axes[0])\nsns.barplot(x=test_df[missing_cols].isnull().sum().index, y=test_df[missing_cols].isnull().sum().values, ax=axes[1])\n\naxes[0].set_ylabel('Missing Value Count', size=15, labelpad=20)\naxes[0].tick_params(axis='x', labelsize=15)\naxes[0].tick_params(axis='y', labelsize=15)\naxes[1].tick_params(axis='x', labelsize=15)\naxes[1].tick_params(axis='y', labelsize=15)\n\naxes[0].set_title('Training Set', fontsize=13)\naxes[1].set_title('Test Set', fontsize=13)\n\nplt.show()\n\nfor df in [train_df, test_df]:\n    for col in ['keyword', 'location']:\n        df[col] = df[col].fillna(f'no_{col}')","fb1aed61":"print(f'Number of unique values in keyword = {train_df[\"keyword\"].nunique()} (Training) - {test_df[\"keyword\"].nunique()} (Test)')\nprint(f'Number of unique values in location = {train_df[\"location\"].nunique()} (Training) - {test_df[\"location\"].nunique()} (Test)')","f91a417e":"train_df['target_mean'] = train_df.groupby('keyword')['target'].transform('mean')\n\nfig = plt.figure(figsize=(8, 72), dpi=100)\n\nsns.countplot(y=train_df.sort_values(by='target_mean', ascending=False)['keyword'],\n              hue=train_df.sort_values(by='target_mean', ascending=False)['target'])\n\nplt.tick_params(axis='x', labelsize=15)\nplt.tick_params(axis='y', labelsize=12)\nplt.legend(loc=1)\nplt.title('Target Distribution in Keywords')\n\nplt.show()\n\ntrain_df.drop(columns=['target_mean'], inplace=True)","6352c2d4":"fig, axes = plt.subplots(ncols=2, nrows=1, figsize=(18, 6), dpi=100)\nsns.countplot(train_df['target'], ax=axes[0])\naxes[1].pie(train_df['target'].value_counts(),\n            labels=['Not Real Disaster', 'Real Disaster'],\n            autopct='%1.2f%%',\n            shadow=True,\n            explode=(0.05, 0),\n            startangle=60)\nfig.suptitle('Distribution of the Tweets', fontsize=24)\nplt.show()","bb4a2a97":"from wordcloud import WordCloud, STOPWORDS\n\n# Thanks : https:\/\/www.kaggle.com\/aashita\/word-clouds-of-various-shapes ##\ndef plot_wordcloud(text, mask=None, max_words=200, max_font_size=100, figure_size=(24.0,16.0), \n                   title = None, title_size=40, image_color=False):\n    stopwords = set(STOPWORDS)\n    more_stopwords = {'one', 'br', 'Po', 'th', 'sayi', 'fo', 'Unknown'}\n    stopwords = stopwords.union(more_stopwords)\n\n    wordcloud = WordCloud(background_color='black',\n                    stopwords = stopwords,\n                    max_words = max_words,\n                    max_font_size = max_font_size, \n                    random_state = 42,\n                    width=800, \n                    height=400,\n                    mask = mask)\n    wordcloud.generate(str(text))\n    \n    plt.figure(figsize=figure_size)\n    if image_color:\n        image_colors = ImageColorGenerator(mask);\n        plt.imshow(wordcloud.recolor(color_func=image_colors), interpolation=\"bilinear\");\n        plt.title(title, fontdict={'size': title_size,  \n                                  'verticalalignment': 'bottom'})\n    else:\n        plt.imshow(wordcloud);\n        plt.title(title, fontdict={'size': title_size, 'color': 'black', \n                                  'verticalalignment': 'bottom'})\n    plt.axis('off');\n    plt.tight_layout()  \n    \nplot_wordcloud(train_df[\"text\"], title=\"Word Cloud of Tweets Text\")","842ee417":"from collections import defaultdict\ntrain1_df = original_train_df[original_train_df[\"target\"]==1]\ntrain0_df = original_train_df[original_train_df[\"target\"]==0]\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of not real disaster tweets\", \n                                          \"Frequent words of real disaster tweets\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig, filename='word-plots')\n","11956156":"freq_dict = defaultdict(int)\nfor sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent bigrams of not real disaster tweets\", \n                                          \"Frequent bigrams of real disaster tweets\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\npy.iplot(fig, filename='word-plots')","4af911b3":"freq_dict = defaultdict(int)\nfor sent in train0_df[\"text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'green')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text\"]:\n    for word in generate_ngrams(sent,3):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'green')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04, horizontal_spacing=0.2,\n                          subplot_titles=[\"Frequent trigrams of not real disaster tweets\", \n                                          \"Frequent trigrams of real disaster tweets\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=1200, paper_bgcolor='rgb(233,233,233)', title=\"Trigram Count Plots\")\npy.iplot(fig, filename='word-plots')","41ffa959":"# word_count\noriginal_train_df['word_count'] = original_train_df['text'].apply(lambda x: len(str(x).split()))\noriginal_test_df['word_count'] = original_test_df['text'].apply(lambda x: len(str(x).split()))\n\n# unique_word_count\noriginal_train_df['unique_word_count'] = original_train_df['text'].apply(lambda x: len(set(str(x).split())))\noriginal_test_df['unique_word_count'] = original_test_df['text'].apply(lambda x: len(set(str(x).split())))\n\n# char_count\noriginal_train_df['char_count'] = original_train_df['text'].apply(lambda x: len(str(x)))\noriginal_test_df['char_count'] = original_test_df['text'].apply(lambda x: len(str(x)))\n\n\n# stop_word_count\noriginal_train_df['stop_word_count'] = original_train_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\noriginal_test_df['stop_word_count'] = original_test_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if w in STOPWORDS]))\n\n# Number of upper case words in the text ##\noriginal_train_df[\"num_words_upper\"] = original_train_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\noriginal_test_df[\"num_words_upper\"] = original_test_df[\"text\"].apply(lambda x: len([w for w in str(x).split() if w.isupper()]))\n\n# url_count\noriginal_train_df['url_count'] = original_train_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\noriginal_test_df['url_count'] = original_test_df['text'].apply(lambda x: len([w for w in str(x).lower().split() if 'http' in w or 'https' in w]))\n\n# mean_word_length\noriginal_train_df['mean_word_length'] = original_train_df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\noriginal_test_df['mean_word_length'] = original_test_df['text'].apply(lambda x: np.mean([len(w) for w in str(x).split()]))\n\n# punctuation_count\noriginal_train_df['punctuation_count'] = original_train_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\noriginal_test_df['punctuation_count'] = original_test_df['text'].apply(lambda x: len([c for c in str(x) if c in string.punctuation]))\n\n# hashtag_count\noriginal_train_df['hashtag_count'] = original_train_df['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\noriginal_test_df['hashtag_count'] = original_test_df['text'].apply(lambda x: len([c for c in str(x) if c == '#']))\n\n# mention_count\noriginal_train_df['mention_count'] = original_train_df['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\noriginal_test_df['mention_count'] = original_test_df['text'].apply(lambda x: len([c for c in str(x) if c == '@']))\n\n","53d5b573":"NEW_FEATURES = ['word_count', 'unique_word_count','num_words_upper', 'stop_word_count', 'url_count', 'mean_word_length',\n                'char_count', 'punctuation_count', 'hashtag_count', 'mention_count']\nREAL_DISASTER_TWEETS = original_train_df['target'] == 1\n\nfig, axes = plt.subplots(ncols=2, nrows=len(NEW_FEATURES), figsize=(20, 50), dpi=100)\n\nfor i, feature in enumerate(NEW_FEATURES):\n    sns.histplot(original_train_df.loc[~REAL_DISASTER_TWEETS][feature], label='Not real Disaster', ax=axes[i][0], color='green', kde=True)\n    sns.histplot(original_train_df.loc[REAL_DISASTER_TWEETS][feature], label='Real Disaster', ax=axes[i][0], color='red', kde=True)\n\n    sns.histplot(original_train_df[feature], label='Training', ax=axes[i][1], color='pink', kde=True)\n    sns.histplot(original_test_df[feature], label='Test', ax=axes[i][1],color='blue', kde=True)\n    \n    for j in range(2):\n        axes[i][j].set_xlabel('')\n        axes[i][j].tick_params(axis='x', labelsize=12)\n        axes[i][j].tick_params(axis='y', labelsize=12)\n        axes[i][j].legend()\n    \n    axes[i][0].set_title(f'{feature} Target Distribution in Training Set', fontsize=13)\n    axes[i][1].set_title(f'{feature} Training & Test Set Distribution', fontsize=13)\n\nplt.show()","d051f472":"def clean_tweets(text):\n    '''Make text lowercase, remove text in square brackets,remove links,remove punctuation\n    and remove words containing numbers.'''\n    #Remove URL's\n    text = re.sub('https?:\/\/\\S+|www\\.\\S+', '', text)\n    \n    #Remove HTML Tags\n    text = re.sub('<.*?>+', '', text)\n    # Reference : https:\/\/gist.github.com\/slowkow\/7a7f61f495e3dbb7e3d767f97bd7304b\n    emoji_pattern = re.compile(\"[\"\n                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n                           u\"\\U00002702-\\U000027B0\"\n                           u\"\\U000024C2-\\U0001F251\"\n                           \"]+\", flags=re.UNICODE)\n    text = emoji_pattern.sub(r'',text)\n    \n    #Remove Punctuations\n    text = re.sub('[%s]' % re.escape(string.punctuation), '', text)\n    \n    #Remove new line\n    text = re.sub('\\n', '', text)\n    \n    #Remove numbers\n    text = re.sub('\\w*\\d\\w*', '', text)\n    \n    #Reference for tweet cleaning:\n    #https:\/\/www.kaggle.com\/gunesevitan\/nlp-with-disaster-tweets-eda-cleaning-and-bert?scriptVersionId=28164619&cellId=31\n    # Special characters\n    text = re.sub(r\"\\x89\u00db_\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00d2\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00d3\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00cfWhen\", \"When\", text)\n    text = re.sub(r\"\\x89\u00db\u00cf\", \"\", text)\n    text = re.sub(r\"China\\x89\u00db\u00aas\", \"China's\", text)\n    text = re.sub(r\"let\\x89\u00db\u00aas\", \"let's\", text)\n    text = re.sub(r\"\\x89\u00db\u00f7\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00aa\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\\x9d\", \"\", text)\n    text = re.sub(r\"\u00e5_\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00a2\", \"\", text)\n    text = re.sub(r\"\\x89\u00db\u00a2\u00e5\u00ca\", \"\", text)\n    text = re.sub(r\"from\u00e5\u00cawounds\", \"from wounds\", text)\n    text = re.sub(r\"\u00e5\u00ca\", \"\", text)\n    text = re.sub(r\"\u00e5\u00c8\", \"\", text)\n    text = re.sub(r\"Jap\u00cc_n\", \"Japan\", text)    \n    text = re.sub(r\"\u00cc\u00a9\", \"e\", text)\n    text = re.sub(r\"\u00e5\u00a8\", \"\", text)\n    text = re.sub(r\"Suru\u00cc\u00a4\", \"Suruc\", text)\n    text = re.sub(r\"\u00e5\u00c7\", \"\", text)\n    text = re.sub(r\"\u00e5\u00a33million\", \"3 million\", text)\n    text = re.sub(r\"\u00e5\u00c0\", \"\", text)\n    \n    # Contractions\n    text = re.sub(r\"he's\", \"he is\", text)\n    text = re.sub(r\"there's\", \"there is\", text)\n    text = re.sub(r\"We're\", \"We are\", text)\n    text = re.sub(r\"That's\", \"That is\", text)\n    text = re.sub(r\"won't\", \"will not\", text)\n    text = re.sub(r\"they're\", \"they are\", text)\n    text = re.sub(r\"Can't\", \"Cannot\", text)\n    text = re.sub(r\"wasn't\", \"was not\", text)\n    text = re.sub(r\"don\\x89\u00db\u00aat\", \"do not\", text)\n    text = re.sub(r\"aren't\", \"are not\", text)\n    text = re.sub(r\"isn't\", \"is not\", text)\n    text = re.sub(r\"What's\", \"What is\", text)\n    text = re.sub(r\"haven't\", \"have not\", text)\n    text = re.sub(r\"hasn't\", \"has not\", text)\n    text = re.sub(r\"There's\", \"There is\", text)\n    text = re.sub(r\"He's\", \"He is\", text)\n    text = re.sub(r\"It's\", \"It is\", text)\n    text = re.sub(r\"You're\", \"You are\", text)\n    text = re.sub(r\"I'M\", \"I am\", text)\n    text = re.sub(r\"shouldn't\", \"should not\", text)\n    text = re.sub(r\"wouldn't\", \"would not\", text)\n    text = re.sub(r\"i'm\", \"I am\", text)\n    text = re.sub(r\"I\\x89\u00db\u00aam\", \"I am\", text)\n    text = re.sub(r\"I'm\", \"I am\", text)\n    text = re.sub(r\"Isn't\", \"is not\", text)\n    text = re.sub(r\"Here's\", \"Here is\", text)\n    text = re.sub(r\"you've\", \"you have\", text)\n    text = re.sub(r\"you\\x89\u00db\u00aave\", \"you have\", text)\n    text = re.sub(r\"we're\", \"we are\", text)\n    text = re.sub(r\"what's\", \"what is\", text)\n    text = re.sub(r\"couldn't\", \"could not\", text)\n    text = re.sub(r\"we've\", \"we have\", text)\n    text = re.sub(r\"it\\x89\u00db\u00aas\", \"it is\", text)\n    text = re.sub(r\"doesn\\x89\u00db\u00aat\", \"does not\", text)\n    text = re.sub(r\"It\\x89\u00db\u00aas\", \"It is\", text)\n    text = re.sub(r\"Here\\x89\u00db\u00aas\", \"Here is\", text)\n    text = re.sub(r\"who's\", \"who is\", text)\n    text = re.sub(r\"I\\x89\u00db\u00aave\", \"I have\", text)\n    text = re.sub(r\"y'all\", \"you all\", text)\n    text = re.sub(r\"can\\x89\u00db\u00aat\", \"cannot\", text)\n    text = re.sub(r\"would've\", \"would have\", text)\n    text = re.sub(r\"it'll\", \"it will\", text)\n    text = re.sub(r\"we'll\", \"we will\", text)\n    text = re.sub(r\"wouldn\\x89\u00db\u00aat\", \"would not\", text)\n    text = re.sub(r\"We've\", \"We have\", text)\n    text = re.sub(r\"he'll\", \"he will\", text)\n    text = re.sub(r\"Y'all\", \"You all\", text)\n    text = re.sub(r\"Weren't\", \"Were not\", text)\n    text = re.sub(r\"Didn't\", \"Did not\", text)\n    text = re.sub(r\"they'll\", \"they will\", text)\n    text = re.sub(r\"they'd\", \"they would\", text)\n    text = re.sub(r\"DON'T\", \"DO NOT\", text)\n    text = re.sub(r\"That\\x89\u00db\u00aas\", \"That is\", text)\n    text = re.sub(r\"they've\", \"they have\", text)\n    text = re.sub(r\"i'd\", \"I would\", text)\n    text = re.sub(r\"should've\", \"should have\", text)\n    text = re.sub(r\"You\\x89\u00db\u00aare\", \"You are\", text)\n    text = re.sub(r\"where's\", \"where is\", text)\n    text = re.sub(r\"Don\\x89\u00db\u00aat\", \"Do not\", text)\n    text = re.sub(r\"we'd\", \"we would\", text)\n    text = re.sub(r\"i'll\", \"I will\", text)\n    text = re.sub(r\"weren't\", \"were not\", text)\n    text = re.sub(r\"They're\", \"They are\", text)\n    text = re.sub(r\"Can\\x89\u00db\u00aat\", \"Cannot\", text)\n    text = re.sub(r\"you\\x89\u00db\u00aall\", \"you will\", text)\n    text = re.sub(r\"I\\x89\u00db\u00aad\", \"I would\", text)\n    text = re.sub(r\"let's\", \"let us\", text)\n    text = re.sub(r\"it's\", \"it is\", text)\n    text = re.sub(r\"can't\", \"cannot\", text)\n    text = re.sub(r\"don't\", \"do not\", text)\n    text = re.sub(r\"you're\", \"you are\", text)\n    text = re.sub(r\"i've\", \"I have\", text)\n    text = re.sub(r\"that's\", \"that is\", text)\n    text = re.sub(r\"i'll\", \"I will\", text)\n    text = re.sub(r\"doesn't\", \"does not\", text)\n    text = re.sub(r\"i'd\", \"I would\", text)\n    text = re.sub(r\"didn't\", \"did not\", text)\n    text = re.sub(r\"ain't\", \"am not\", text)\n    text = re.sub(r\"you'll\", \"you will\", text)\n    text = re.sub(r\"I've\", \"I have\", text)\n    text = re.sub(r\"Don't\", \"do not\", text)\n    text = re.sub(r\"I'll\", \"I will\", text)\n    text = re.sub(r\"I'd\", \"I would\", text)\n    text = re.sub(r\"Let's\", \"Let us\", text)\n    text = re.sub(r\"you'd\", \"You would\", text)\n    text = re.sub(r\"It's\", \"It is\", text)\n    text = re.sub(r\"Ain't\", \"am not\", text)\n    text = re.sub(r\"Haven't\", \"Have not\", text)\n    text = re.sub(r\"Could've\", \"Could have\", text)\n    text = re.sub(r\"youve\", \"you have\", text)  \n    text = re.sub(r\"don\u00e5\u00abt\", \"do not\", text)   \n            \n    # Character entity references\n    text = re.sub(r\"&gt;\", \">\", text)\n    text = re.sub(r\"&lt;\", \"<\", text)\n    text = re.sub(r\"&amp;\", \"&\", text)\n    \n    # Typos, slang and informal abbreviations\n    text = re.sub(r\"w\/e\", \"whatever\", text)\n    text = re.sub(r\"w\/\", \"with\", text)\n    text = re.sub(r\"USAgov\", \"USA government\", text)\n    text = re.sub(r\"recentlu\", \"recently\", text)\n    text = re.sub(r\"Ph0tos\", \"Photos\", text)\n    text = re.sub(r\"amirite\", \"am I right\", text)\n    text = re.sub(r\"exp0sed\", \"exposed\", text)\n    text = re.sub(r\"<3\", \"love\", text)\n    text = re.sub(r\"amageddon\", \"armageddon\", text)\n    text = re.sub(r\"Trfc\", \"Traffic\", text)\n    text = re.sub(r\"8\/5\/2015\", \"2015-08-05\", text)\n    text = re.sub(r\"WindStorm\", \"Wind Storm\", text)\n    text = re.sub(r\"8\/6\/2015\", \"2015-08-06\", text)\n    text = re.sub(r\"10:38PM\", \"10:38 PM\", text)\n    text = re.sub(r\"10:30pm\", \"10:30 PM\", text)\n    text = re.sub(r\"16yr\", \"16 year\", text)\n    text = re.sub(r\"lmao\", \"laughing my ass off\", text)   \n    text = re.sub(r\"TRAUMATISED\", \"traumatized\", text)\n    \n    # Hashtags and usernames\n    text = re.sub(r\"IranDeal\", \"Iran Deal\", text)\n    text = re.sub(r\"ArianaGrande\", \"Ariana Grande\", text)\n    text = re.sub(r\"camilacabello97\", \"camila cabello\", text) \n    text = re.sub(r\"RondaRousey\", \"Ronda Rousey\", text)     \n    text = re.sub(r\"MTVHottest\", \"MTV Hottest\", text)\n    text = re.sub(r\"TrapMusic\", \"Trap Music\", text)\n    text = re.sub(r\"ProphetMuhammad\", \"Prophet Muhammad\", text)\n    text = re.sub(r\"PantherAttack\", \"Panther Attack\", text)\n    text = re.sub(r\"StrategicPatience\", \"Strategic Patience\", text)\n    text = re.sub(r\"socialnews\", \"social news\", text)\n    text = re.sub(r\"NASAHurricane\", \"NASA Hurricane\", text)\n    text = re.sub(r\"onlinecommunities\", \"online communities\", text)\n    text = re.sub(r\"humanconsumption\", \"human consumption\", text)\n    text = re.sub(r\"Typhoon-Devastated\", \"Typhoon Devastated\", text)\n    text = re.sub(r\"Meat-Loving\", \"Meat Loving\", text)\n    text = re.sub(r\"facialabuse\", \"facial abuse\", text)\n    text = re.sub(r\"LakeCounty\", \"Lake County\", text)\n    text = re.sub(r\"BeingAuthor\", \"Being Author\", text)\n    text = re.sub(r\"withheavenly\", \"with heavenly\", text)\n    text = re.sub(r\"thankU\", \"thank you\", text)\n    text = re.sub(r\"iTunesMusic\", \"iTunes Music\", text)\n    text = re.sub(r\"OffensiveContent\", \"Offensive Content\", text)\n    text = re.sub(r\"WorstSummerJob\", \"Worst Summer Job\", text)\n    text = re.sub(r\"HarryBeCareful\", \"Harry Be Careful\", text)\n    text = re.sub(r\"NASASolarSystem\", \"NASA Solar System\", text)\n    text = re.sub(r\"animalrescue\", \"animal rescue\", text)\n    text = re.sub(r\"KurtSchlichter\", \"Kurt Schlichter\", text)\n    text = re.sub(r\"aRmageddon\", \"armageddon\", text)\n    text = re.sub(r\"Throwingknifes\", \"Throwing knives\", text)\n    text = re.sub(r\"GodsLove\", \"God's Love\", text)\n    text = re.sub(r\"bookboost\", \"book boost\", text)\n    text = re.sub(r\"ibooklove\", \"I book love\", text)\n    text = re.sub(r\"NestleIndia\", \"Nestle India\", text)\n    text = re.sub(r\"realDonaldTrump\", \"Donald Trump\", text)\n    text = re.sub(r\"DavidVonderhaar\", \"David Vonderhaar\", text)\n    text = re.sub(r\"CecilTheLion\", \"Cecil The Lion\", text)\n    text = re.sub(r\"weathernetwork\", \"weather network\", text)\n    text = re.sub(r\"withBioterrorism&use\", \"with Bioterrorism & use\", text)\n    text = re.sub(r\"Hostage&2\", \"Hostage & 2\", text)\n    text = re.sub(r\"GOPDebate\", \"GOP Debate\", text)\n    text = re.sub(r\"RickPerry\", \"Rick Perry\", text)\n    text = re.sub(r\"frontpage\", \"front page\", text)\n    text = re.sub(r\"NewsInTweets\", \"News In Tweets\", text)\n    text = re.sub(r\"ViralSpell\", \"Viral Spell\", text)\n    text = re.sub(r\"til_now\", \"until now\", text)\n    text = re.sub(r\"volcanoinRussia\", \"volcano in Russia\", text)\n    text = re.sub(r\"ZippedNews\", \"Zipped News\", text)\n    text = re.sub(r\"MicheleBachman\", \"Michele Bachman\", text)\n    text = re.sub(r\"53inch\", \"53 inch\", text)\n    text = re.sub(r\"KerrickTrial\", \"Kerrick Trial\", text)\n    text = re.sub(r\"abstorm\", \"Alberta Storm\", text)\n    text = re.sub(r\"Beyhive\", \"Beyonce hive\", text)\n    text = re.sub(r\"IDFire\", \"Idaho Fire\", text)\n    text = re.sub(r\"DETECTADO\", \"Detected\", text)\n    text = re.sub(r\"RockyFire\", \"Rocky Fire\", text)\n    text = re.sub(r\"Listen\/Buy\", \"Listen \/ Buy\", text)\n    text = re.sub(r\"NickCannon\", \"Nick Cannon\", text)\n    text = re.sub(r\"FaroeIslands\", \"Faroe Islands\", text)\n    text = re.sub(r\"yycstorm\", \"Calgary Storm\", text)\n    text = re.sub(r\"IDPs:\", \"Internally Displaced People :\", text)\n    text = re.sub(r\"ArtistsUnited\", \"Artists United\", text)\n    text = re.sub(r\"ClaytonBryant\", \"Clayton Bryant\", text)\n    text = re.sub(r\"jimmyfallon\", \"jimmy fallon\", text)\n    text = re.sub(r\"justinbieber\", \"justin bieber\", text)  \n    text = re.sub(r\"UTC2015\", \"UTC 2015\", text)\n    text = re.sub(r\"Time2015\", \"Time 2015\", text)\n    text = re.sub(r\"djicemoon\", \"dj icemoon\", text)\n    text = re.sub(r\"LivingSafely\", \"Living Safely\", text)\n    text = re.sub(r\"FIFA16\", \"Fifa 2016\", text)\n    text = re.sub(r\"thisiswhywecanthavenicethings\", \"this is why we cannot have nice things\", text)\n    text = re.sub(r\"bbcnews\", \"bbc news\", text)\n    text = re.sub(r\"UndergroundRailraod\", \"Underground Railraod\", text)\n    text = re.sub(r\"c4news\", \"c4 news\", text)\n    text = re.sub(r\"OBLITERATION\", \"obliteration\", text)\n    text = re.sub(r\"MUDSLIDE\", \"mudslide\", text)\n    text = re.sub(r\"NoSurrender\", \"No Surrender\", text)\n    text = re.sub(r\"NotExplained\", \"Not Explained\", text)\n    text = re.sub(r\"greatbritishbakeoff\", \"great british bake off\", text)\n    text = re.sub(r\"LondonFire\", \"London Fire\", text)\n    text = re.sub(r\"KOTAWeather\", \"KOTA Weather\", text)\n    text = re.sub(r\"LuchaUnderground\", \"Lucha Underground\", text)\n    text = re.sub(r\"KOIN6News\", \"KOIN 6 News\", text)\n    text = re.sub(r\"LiveOnK2\", \"Live On K2\", text)\n    text = re.sub(r\"9NewsGoldCoast\", \"9 News Gold Coast\", text)\n    text = re.sub(r\"nikeplus\", \"nike plus\", text)\n    text = re.sub(r\"david_cameron\", \"David Cameron\", text)\n    text = re.sub(r\"peterjukes\", \"Peter Jukes\", text)\n    text = re.sub(r\"JamesMelville\", \"James Melville\", text)\n    text = re.sub(r\"megynkelly\", \"Megyn Kelly\", text)\n    text = re.sub(r\"cnewslive\", \"C News Live\", text)\n    text = re.sub(r\"JamaicaObserver\", \"Jamaica Observer\", text)\n    text = re.sub(r\"TweetLikeItsSeptember11th2001\", \"Tweet like it is september 11th 2001\", text)\n    text = re.sub(r\"cbplawyers\", \"cbp lawyers\", text)\n    text = re.sub(r\"fewmoretexts\", \"few more texts\", text)\n    text = re.sub(r\"BlackLivesMatter\", \"Black Lives Matter\", text)\n    text = re.sub(r\"cjoyner\", \"Chris Joyner\", text)\n    text = re.sub(r\"ENGvAUS\", \"England vs Australia\", text)\n    text = re.sub(r\"ScottWalker\", \"Scott Walker\", text)\n    text = re.sub(r\"MikeParrActor\", \"Michael Parr\", text)\n    text = re.sub(r\"4PlayThursdays\", \"Foreplay Thursdays\", text)\n    text = re.sub(r\"TGF2015\", \"Tontitown Grape Festival\", text)\n    text = re.sub(r\"realmandyrain\", \"Mandy Rain\", text)\n    text = re.sub(r\"GraysonDolan\", \"Grayson Dolan\", text)\n    text = re.sub(r\"ApolloBrown\", \"Apollo Brown\", text)\n    text = re.sub(r\"saddlebrooke\", \"Saddlebrooke\", text)\n    text = re.sub(r\"TontitownGrape\", \"Tontitown Grape\", text)\n    text = re.sub(r\"AbbsWinston\", \"Abbs Winston\", text)\n    text = re.sub(r\"ShaunKing\", \"Shaun King\", text)\n    text = re.sub(r\"MeekMill\", \"Meek Mill\", text)\n    text = re.sub(r\"TornadoGiveaway\", \"Tornado Giveaway\", text)\n    text = re.sub(r\"GRupdates\", \"GR updates\", text)\n    text = re.sub(r\"SouthDowns\", \"South Downs\", text)\n    text = re.sub(r\"braininjury\", \"brain injury\", text)\n    text = re.sub(r\"auspol\", \"Australian politics\", text)\n    text = re.sub(r\"PlannedParenthood\", \"Planned Parenthood\", text)\n    text = re.sub(r\"calgaryweather\", \"Calgary Weather\", text)\n    text = re.sub(r\"weallheartonedirection\", \"we all heart one direction\", text)\n    text = re.sub(r\"edsheeran\", \"Ed Sheeran\", text)\n    text = re.sub(r\"TrueHeroes\", \"True Heroes\", text)\n    text = re.sub(r\"S3XLEAK\", \"sex leak\", text)\n    text = re.sub(r\"ComplexMag\", \"Complex Magazine\", text)\n    text = re.sub(r\"TheAdvocateMag\", \"The Advocate Magazine\", text)\n    text = re.sub(r\"CityofCalgary\", \"City of Calgary\", text)\n    text = re.sub(r\"EbolaOutbreak\", \"Ebola Outbreak\", text)\n    text = re.sub(r\"SummerFate\", \"Summer Fate\", text)\n    text = re.sub(r\"RAmag\", \"Royal Academy Magazine\", text)\n    text = re.sub(r\"offers2go\", \"offers to go\", text)\n    text = re.sub(r\"foodscare\", \"food scare\", text)\n    text = re.sub(r\"MNPDNashville\", \"Metropolitan Nashville Police Department\", text)\n    text = re.sub(r\"TfLBusAlerts\", \"TfL Bus Alerts\", text)\n    text = re.sub(r\"GamerGate\", \"Gamer Gate\", text)\n    text = re.sub(r\"IHHen\", \"Humanitarian Relief\", text)\n    text = re.sub(r\"spinningbot\", \"spinning bot\", text)\n    text = re.sub(r\"ModiMinistry\", \"Modi Ministry\", text)\n    text = re.sub(r\"TAXIWAYS\", \"taxi ways\", text)\n    text = re.sub(r\"Calum5SOS\", \"Calum Hood\", text)\n    text = re.sub(r\"po_st\", \"po.st\", text)\n    text = re.sub(r\"scoopit\", \"scoop.it\", text)\n    text = re.sub(r\"UltimaLucha\", \"Ultima Lucha\", text)\n    text = re.sub(r\"JonathanFerrell\", \"Jonathan Ferrell\", text)\n    text = re.sub(r\"aria_ahrary\", \"Aria Ahrary\", text)\n    text = re.sub(r\"rapidcity\", \"Rapid City\", text)\n    text = re.sub(r\"OutBid\", \"outbid\", text)\n    text = re.sub(r\"lavenderpoetrycafe\", \"lavender poetry cafe\", text)\n    text = re.sub(r\"EudryLantiqua\", \"Eudry Lantiqua\", text)\n    text = re.sub(r\"15PM\", \"15 PM\", text)\n    text = re.sub(r\"OriginalFunko\", \"Funko\", text)\n    text = re.sub(r\"rightwaystan\", \"Richard Tan\", text)\n    text = re.sub(r\"CindyNoonan\", \"Cindy Noonan\", text)\n    text = re.sub(r\"RT_America\", \"RT America\", text)\n    text = re.sub(r\"narendramodi\", \"Narendra Modi\", text)\n    text = re.sub(r\"BakeOffFriends\", \"Bake Off Friends\", text)\n    text = re.sub(r\"TeamHendrick\", \"Hendrick Motorsports\", text)\n    text = re.sub(r\"alexbelloli\", \"Alex Belloli\", text)\n    text = re.sub(r\"itsjustinstuart\", \"Justin Stuart\", text)\n    text = re.sub(r\"gunsense\", \"gun sense\", text)\n    text = re.sub(r\"DebateQuestionsWeWantToHear\", \"debate questions we want to hear\", text)\n    text = re.sub(r\"RoyalCarribean\", \"Royal Carribean\", text)\n    text = re.sub(r\"samanthaturne19\", \"Samantha Turner\", text)\n    text = re.sub(r\"JonVoyage\", \"Jon Stewart\", text)\n    text = re.sub(r\"renew911health\", \"renew 911 health\", text)\n    text = re.sub(r\"SuryaRay\", \"Surya Ray\", text)\n    text = re.sub(r\"pattonoswalt\", \"Patton Oswalt\", text)\n    text = re.sub(r\"minhazmerchant\", \"Minhaz Merchant\", text)\n    text = re.sub(r\"TLVFaces\", \"Israel Diaspora Coalition\", text)\n    text = re.sub(r\"pmarca\", \"Marc Andreessen\", text)\n    text = re.sub(r\"pdx911\", \"Portland Police\", text)\n    text = re.sub(r\"jamaicaplain\", \"Jamaica Plain\", text)\n    text = re.sub(r\"Japton\", \"Arkansas\", text)\n    text = re.sub(r\"RouteComplex\", \"Route Complex\", text)\n    text = re.sub(r\"INSubcontinent\", \"Indian Subcontinent\", text)\n    text = re.sub(r\"NJTurnpike\", \"New Jersey Turnpike\", text)\n    text = re.sub(r\"Politifiact\", \"PolitiFact\", text)\n    text = re.sub(r\"Hiroshima70\", \"Hiroshima\", text)\n    text = re.sub(r\"GMMBC\", \"Greater Mt Moriah Baptist Church\", text)\n    text = re.sub(r\"versethe\", \"verse the\", text)\n    text = re.sub(r\"TubeStrike\", \"Tube Strike\", text)\n    text = re.sub(r\"MissionHills\", \"Mission Hills\", text)\n    text = re.sub(r\"ProtectDenaliWolves\", \"Protect Denali Wolves\", text)\n    text = re.sub(r\"NANKANA\", \"Nankana\", text)\n    text = re.sub(r\"SAHIB\", \"Sahib\", text)\n    text = re.sub(r\"PAKPATTAN\", \"Pakpattan\", text)\n    text = re.sub(r\"Newz_Sacramento\", \"News Sacramento\", text)\n    text = re.sub(r\"gofundme\", \"go fund me\", text)\n    text = re.sub(r\"pmharper\", \"Stephen Harper\", text)\n    text = re.sub(r\"IvanBerroa\", \"Ivan Berroa\", text)\n    text = re.sub(r\"LosDelSonido\", \"Los Del Sonido\", text)\n    text = re.sub(r\"bancodeseries\", \"banco de series\", text)\n    text = re.sub(r\"timkaine\", \"Tim Kaine\", text)\n    text = re.sub(r\"IdentityTheft\", \"Identity Theft\", text)\n    text = re.sub(r\"AllLivesMatter\", \"All Lives Matter\", text)\n    text = re.sub(r\"mishacollins\", \"Misha Collins\", text)\n    text = re.sub(r\"BillNeelyNBC\", \"Bill Neely\", text)\n    text = re.sub(r\"BeClearOnCancer\", \"be clear on cancer\", text)\n    text = re.sub(r\"Kowing\", \"Knowing\", text)\n    text = re.sub(r\"ScreamQueens\", \"Scream Queens\", text)\n    text = re.sub(r\"AskCharley\", \"Ask Charley\", text)\n    text = re.sub(r\"BlizzHeroes\", \"Heroes of the Storm\", text)\n    text = re.sub(r\"BradleyBrad47\", \"Bradley Brad\", text)\n    text = re.sub(r\"HannaPH\", \"Typhoon Hanna\", text)\n    text = re.sub(r\"meinlcymbals\", \"MEINL Cymbals\", text)\n    text = re.sub(r\"Ptbo\", \"Peterborough\", text)\n    text = re.sub(r\"cnnbrk\", \"CNN Breaking News\", text)\n    text = re.sub(r\"IndianNews\", \"Indian News\", text)\n    text = re.sub(r\"savebees\", \"save bees\", text)\n    text = re.sub(r\"GreenHarvard\", \"Green Harvard\", text)\n    text = re.sub(r\"StandwithPP\", \"Stand with planned parenthood\", text)\n    text = re.sub(r\"hermancranston\", \"Herman Cranston\", text)\n    text = re.sub(r\"WMUR9\", \"WMUR-TV\", text)\n    text = re.sub(r\"RockBottomRadFM\", \"Rock Bottom Radio\", text)\n    text = re.sub(r\"ameenshaikh3\", \"Ameen Shaikh\", text)\n    text = re.sub(r\"ProSyn\", \"Project Syndicate\", text)\n    text = re.sub(r\"Daesh\", \"ISIS\", text)\n    text = re.sub(r\"s2g\", \"swear to god\", text)\n    text = re.sub(r\"listenlive\", \"listen live\", text)\n    text = re.sub(r\"CDCgov\", \"Centers for Disease Control and Prevention\", text)\n    text = re.sub(r\"FoxNew\", \"Fox News\", text)\n    text = re.sub(r\"CBSBigBrother\", \"Big Brother\", text)\n    text = re.sub(r\"JulieDiCaro\", \"Julie DiCaro\", text)\n    text = re.sub(r\"theadvocatemag\", \"The Advocate Magazine\", text)\n    text = re.sub(r\"RohnertParkDPS\", \"Rohnert Park Police Department\", text)\n    text = re.sub(r\"THISIZBWRIGHT\", \"Bonnie Wright\", text)\n    text = re.sub(r\"Popularmmos\", \"Popular MMOs\", text)\n    text = re.sub(r\"WildHorses\", \"Wild Horses\", text)\n    text = re.sub(r\"FantasticFour\", \"Fantastic Four\", text)\n    text = re.sub(r\"HORNDALE\", \"Horndale\", text)\n    text = re.sub(r\"PINER\", \"Piner\", text)\n    text = re.sub(r\"BathAndNorthEastSomerset\", \"Bath and North East Somerset\", text)\n    text = re.sub(r\"thatswhatfriendsarefor\", \"that is what friends are for\", text)\n    text = re.sub(r\"residualincome\", \"residual income\", text)\n    text = re.sub(r\"YahooNewsDigest\", \"Yahoo News Digest\", text)\n    text = re.sub(r\"MalaysiaAirlines\", \"Malaysia Airlines\", text)\n    text = re.sub(r\"AmazonDeals\", \"Amazon Deals\", text)\n    text = re.sub(r\"MissCharleyWebb\", \"Charley Webb\", text)\n    text = re.sub(r\"shoalstraffic\", \"shoals traffic\", text)\n    text = re.sub(r\"GeorgeFoster72\", \"George Foster\", text)\n    text = re.sub(r\"pop2015\", \"pop 2015\", text)\n    text = re.sub(r\"_PokemonCards_\", \"Pokemon Cards\", text)\n    text = re.sub(r\"DianneG\", \"Dianne Gallagher\", text)\n    text = re.sub(r\"KashmirConflict\", \"Kashmir Conflict\", text)\n    text = re.sub(r\"BritishBakeOff\", \"British Bake Off\", text)\n    text = re.sub(r\"FreeKashmir\", \"Free Kashmir\", text)\n    text = re.sub(r\"mattmosley\", \"Matt Mosley\", text)\n    text = re.sub(r\"BishopFred\", \"Bishop Fred\", text)\n    text = re.sub(r\"EndConflict\", \"End Conflict\", text)\n    text = re.sub(r\"EndOccupation\", \"End Occupation\", text)\n    text = re.sub(r\"UNHEALED\", \"unhealed\", text)\n    text = re.sub(r\"CharlesDagnall\", \"Charles Dagnall\", text)\n    text = re.sub(r\"Latestnews\", \"Latest news\", text)\n    text = re.sub(r\"KindleCountdown\", \"Kindle Countdown\", text)\n    text = re.sub(r\"NoMoreHandouts\", \"No More Handouts\", text)\n    text = re.sub(r\"datingtips\", \"dating tips\", text)\n    text = re.sub(r\"charlesadler\", \"Charles Adler\", text)\n    text = re.sub(r\"twia\", \"Texas Windstorm Insurance Association\", text)\n    text = re.sub(r\"txlege\", \"Texas Legislature\", text)\n    text = re.sub(r\"WindstormInsurer\", \"Windstorm Insurer\", text)\n    text = re.sub(r\"Newss\", \"News\", text)\n    text = re.sub(r\"hempoil\", \"hemp oil\", text)\n    text = re.sub(r\"CommoditiesAre\", \"Commodities are\", text)\n    text = re.sub(r\"tubestrike\", \"tube strike\", text)\n    text = re.sub(r\"JoeNBC\", \"Joe Scarborough\", text)\n    text = re.sub(r\"LiteraryCakes\", \"Literary Cakes\", text)\n    text = re.sub(r\"TI5\", \"The International 5\", text)\n    text = re.sub(r\"thehill\", \"the hill\", text)\n    text = re.sub(r\"3others\", \"3 others\", text)\n    text = re.sub(r\"stighefootball\", \"Sam Tighe\", text)\n    text = re.sub(r\"whatstheimportantvideo\", \"what is the important video\", text)\n    text = re.sub(r\"ClaudioMeloni\", \"Claudio Meloni\", text)\n    text = re.sub(r\"DukeSkywalker\", \"Duke Skywalker\", text)\n    text = re.sub(r\"carsonmwr\", \"Fort Carson\", text)\n    text = re.sub(r\"offdishduty\", \"off dish duty\", text)\n    text = re.sub(r\"andword\", \"and word\", text)\n    text = re.sub(r\"rhodeisland\", \"Rhode Island\", text)\n    text = re.sub(r\"easternoregon\", \"Eastern Oregon\", text)\n    text = re.sub(r\"WAwildfire\", \"Washington Wildfire\", text)\n    text = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", text)\n    text = re.sub(r\"57am\", \"57 am\", text)\n    text = re.sub(r\"fingerrockfire\", \"Finger Rock Fire\", text)\n    text = re.sub(r\"JacobHoggard\", \"Jacob Hoggard\", text)\n    text = re.sub(r\"newnewnew\", \"new new new\", text)\n    text = re.sub(r\"under50\", \"under 50\", text)\n    text = re.sub(r\"getitbeforeitsgone\", \"get it before it is gone\", text)\n    text = re.sub(r\"freshoutofthebox\", \"fresh out of the box\", text)\n    text = re.sub(r\"amwriting\", \"am writing\", text)\n    text = re.sub(r\"Bokoharm\", \"Boko Haram\", text)\n    text = re.sub(r\"Nowlike\", \"Now like\", text)\n    text = re.sub(r\"seasonfrom\", \"season from\", text)\n    text = re.sub(r\"epicente\", \"epicenter\", text)\n    text = re.sub(r\"epicenterr\", \"epicenter\", text)\n    text = re.sub(r\"sicklife\", \"sick life\", text)\n    text = re.sub(r\"yycweather\", \"Calgary Weather\", text)\n    text = re.sub(r\"calgarysun\", \"Calgary Sun\", text)\n    text = re.sub(r\"approachng\", \"approaching\", text)\n    text = re.sub(r\"evng\", \"evening\", text)\n    text = re.sub(r\"Sumthng\", \"something\", text)\n    text = re.sub(r\"EllenPompeo\", \"Ellen Pompeo\", text)\n    text = re.sub(r\"shondarhimes\", \"Shonda Rhimes\", text)\n    text = re.sub(r\"ABCNetwork\", \"ABC Network\", text)\n    text = re.sub(r\"SushmaSwaraj\", \"Sushma Swaraj\", text)\n    text = re.sub(r\"pray4japan\", \"Pray for Japan\", text)\n    text = re.sub(r\"hope4japan\", \"Hope for Japan\", text)\n    text = re.sub(r\"Illusionimagess\", \"Illusion images\", text)\n    text = re.sub(r\"SummerUnderTheStars\", \"Summer Under The Stars\", text)\n    text = re.sub(r\"ShallWeDance\", \"Shall We Dance\", text)\n    text = re.sub(r\"TCMParty\", \"TCM Party\", text)\n    text = re.sub(r\"marijuananews\", \"marijuana news\", text)\n    text = re.sub(r\"onbeingwithKristaTippett\", \"on being with Krista Tippett\", text)\n    text = re.sub(r\"Beingtexts\", \"Being texts\", text)\n    text = re.sub(r\"newauthors\", \"new authors\", text)\n    text = re.sub(r\"remedyyyy\", \"remedy\", text)\n    text = re.sub(r\"44PM\", \"44 PM\", text)\n    text = re.sub(r\"HeadlinesApp\", \"Headlines App\", text)\n    text = re.sub(r\"40PM\", \"40 PM\", text)\n    text = re.sub(r\"myswc\", \"Severe Weather Center\", text)\n    text = re.sub(r\"ithats\", \"that is\", text)\n    text = re.sub(r\"icouldsitinthismomentforever\", \"I could sit in this moment forever\", text)\n    text = re.sub(r\"FatLoss\", \"Fat Loss\", text)\n    text = re.sub(r\"02PM\", \"02 PM\", text)\n    text = re.sub(r\"MetroFmTalk\", \"Metro Fm Talk\", text)\n    text = re.sub(r\"Bstrd\", \"bastard\", text)\n    text = re.sub(r\"bldy\", \"bloody\", text)\n    text = re.sub(r\"MetrofmTalk\", \"Metro Fm Talk\", text)\n    text = re.sub(r\"terrorismturn\", \"terrorism turn\", text)\n    text = re.sub(r\"BBCNewsAsia\", \"BBC News Asia\", text)\n    text = re.sub(r\"BehindTheScenes\", \"Behind The Scenes\", text)\n    text = re.sub(r\"GeorgeTakei\", \"George Takei\", text)\n    text = re.sub(r\"WomensWeeklyMag\", \"Womens Weekly Magazine\", text)\n    text = re.sub(r\"SurvivorsGuidetoEarth\", \"Survivors Guide to Earth\", text)\n    text = re.sub(r\"incubusband\", \"incubus band\", text)\n    text = re.sub(r\"Babypicturethis\", \"Baby picture this\", text)\n    text = re.sub(r\"BombEffects\", \"Bomb Effects\", text)\n    text = re.sub(r\"win10\", \"Windows 10\", text)\n    text = re.sub(r\"idkidk\", \"I do not know I do not know\", text)\n    text = re.sub(r\"TheWalkingDead\", \"The Walking Dead\", text)\n    text = re.sub(r\"amyschumer\", \"Amy Schumer\", text)\n    text = re.sub(r\"crewlist\", \"crew list\", text)\n    text = re.sub(r\"Erdogans\", \"Erdogan\", text)\n    text = re.sub(r\"BBCLive\", \"BBC Live\", text)\n    text = re.sub(r\"TonyAbbottMHR\", \"Tony Abbott\", text)\n    text = re.sub(r\"paulmyerscough\", \"Paul Myerscough\", text)\n    text = re.sub(r\"georgegallagher\", \"George Gallagher\", text)\n    text = re.sub(r\"JimmieJohnson\", \"Jimmie Johnson\", text)\n    text = re.sub(r\"pctool\", \"pc tool\", text)\n    text = re.sub(r\"DoingHashtagsRight\", \"Doing Hashtags Right\", text)\n    text = re.sub(r\"ThrowbackThursday\", \"Throwback Thursday\", text)\n    text = re.sub(r\"SnowBackSunday\", \"Snowback Sunday\", text)\n    text = re.sub(r\"LakeEffect\", \"Lake Effect\", text)\n    text = re.sub(r\"RTphotographyUK\", \"Richard Thomas Photography UK\", text)\n    text = re.sub(r\"BigBang_CBS\", \"Big Bang CBS\", text)\n    text = re.sub(r\"writerslife\", \"writers life\", text)\n    text = re.sub(r\"NaturalBirth\", \"Natural Birth\", text)\n    text = re.sub(r\"UnusualWords\", \"Unusual Words\", text)\n    text = re.sub(r\"wizkhalifa\", \"Wiz Khalifa\", text)\n    text = re.sub(r\"acreativedc\", \"a creative DC\", text)\n    text = re.sub(r\"vscodc\", \"vsco DC\", text)\n    text = re.sub(r\"VSCOcam\", \"vsco camera\", text)\n    text = re.sub(r\"TheBEACHDC\", \"The beach DC\", text)\n    text = re.sub(r\"buildingmuseum\", \"building museum\", text)\n    text = re.sub(r\"WorldOil\", \"World Oil\", text)\n    text = re.sub(r\"redwedding\", \"red wedding\", text)\n    text = re.sub(r\"AmazingRaceCanada\", \"Amazing Race Canada\", text)\n    text = re.sub(r\"WakeUpAmerica\", \"Wake Up America\", text)\n    text = re.sub(r\"\\\\Allahuakbar\\\\\", \"Allahu Akbar\", text)\n    text = re.sub(r\"bleased\", \"blessed\", text)\n    text = re.sub(r\"nigeriantribune\", \"Nigerian Tribune\", text)\n    text = re.sub(r\"HIDEO_KOJIMA_EN\", \"Hideo Kojima\", text)\n    text = re.sub(r\"FusionFestival\", \"Fusion Festival\", text)\n    text = re.sub(r\"50Mixed\", \"50 Mixed\", text)\n    text = re.sub(r\"NoAgenda\", \"No Agenda\", text)\n    text = re.sub(r\"WhiteGenocide\", \"White Genocide\", text)\n    text = re.sub(r\"dirtylying\", \"dirty lying\", text)\n    text = re.sub(r\"SyrianRefugees\", \"Syrian Refugees\", text)\n    text = re.sub(r\"changetheworld\", \"change the world\", text)\n    text = re.sub(r\"Ebolacase\", \"Ebola case\", text)\n    text = re.sub(r\"mcgtech\", \"mcg technologies\", text)\n    text = re.sub(r\"withweapons\", \"with weapons\", text)\n    text = re.sub(r\"advancedwarfare\", \"advanced warfare\", text)\n    text = re.sub(r\"letsFootball\", \"let us Football\", text)\n    text = re.sub(r\"LateNiteMix\", \"late night mix\", text)\n    text = re.sub(r\"PhilCollinsFeed\", \"Phil Collins\", text)\n    text = re.sub(r\"RudyHavenstein\", \"Rudy Havenstein\", text)\n    text = re.sub(r\"22PM\", \"22 PM\", text)\n    text = re.sub(r\"54am\", \"54 AM\", text)\n    text = re.sub(r\"38am\", \"38 AM\", text)\n    text = re.sub(r\"OldFolkExplainStuff\", \"Old Folk Explain Stuff\", text)\n    text = re.sub(r\"BlacklivesMatter\", \"Black Lives Matter\", text)\n    text = re.sub(r\"InsaneLimits\", \"Insane Limits\", text)\n    text = re.sub(r\"youcantsitwithus\", \"you cannot sit with us\", text)\n    text = re.sub(r\"2k15\", \"2015\", text)\n    text = re.sub(r\"TheIran\", \"Iran\", text)\n    text = re.sub(r\"JimmyFallon\", \"Jimmy Fallon\", text)\n    text = re.sub(r\"AlbertBrooks\", \"Albert Brooks\", text)\n    text = re.sub(r\"defense_news\", \"defense news\", text)\n    text = re.sub(r\"nuclearrcSA\", \"Nuclear Risk Control Self Assessment\", text)\n    text = re.sub(r\"Auspol\", \"Australia Politics\", text)\n    text = re.sub(r\"NuclearPower\", \"Nuclear Power\", text)\n    text = re.sub(r\"WhiteTerrorism\", \"White Terrorism\", text)\n    text = re.sub(r\"truthfrequencyradio\", \"Truth Frequency Radio\", text)\n    text = re.sub(r\"ErasureIsNotEquality\", \"Erasure is not equality\", text)\n    text = re.sub(r\"ProBonoNews\", \"Pro Bono News\", text)\n    text = re.sub(r\"JakartaPost\", \"Jakarta Post\", text)\n    text = re.sub(r\"toopainful\", \"too painful\", text)\n    text = re.sub(r\"melindahaunton\", \"Melinda Haunton\", text)\n    text = re.sub(r\"NoNukes\", \"No Nukes\", text)\n    text = re.sub(r\"curryspcworld\", \"Currys PC World\", text)\n    text = re.sub(r\"ineedcake\", \"I need cake\", text)\n    text = re.sub(r\"blackforestgateau\", \"black forest gateau\", text)\n    text = re.sub(r\"BBCOne\", \"BBC One\", text)\n    text = re.sub(r\"AlexxPage\", \"Alex Page\", text)\n    text = re.sub(r\"jonathanserrie\", \"Jonathan Serrie\", text)\n    text = re.sub(r\"SocialJerkBlog\", \"Social Jerk Blog\", text)\n    text = re.sub(r\"ChelseaVPeretti\", \"Chelsea Peretti\", text)\n    text = re.sub(r\"irongiant\", \"iron giant\", text)\n    text = re.sub(r\"RonFunches\", \"Ron Funches\", text)\n    text = re.sub(r\"TimCook\", \"Tim Cook\", text)\n    text = re.sub(r\"sebastianstanisaliveandwell\", \"Sebastian Stan is alive and well\", text)\n    text = re.sub(r\"Madsummer\", \"Mad summer\", text)\n    text = re.sub(r\"NowYouKnow\", \"Now you know\", text)\n    text = re.sub(r\"concertphotography\", \"concert photography\", text)\n    text = re.sub(r\"TomLandry\", \"Tom Landry\", text)\n    text = re.sub(r\"showgirldayoff\", \"show girl day off\", text)\n    text = re.sub(r\"Yougslavia\", \"Yugoslavia\", text)\n    text = re.sub(r\"QuantumDataInformatics\", \"Quantum Data Informatics\", text)\n    text = re.sub(r\"FromTheDesk\", \"From The Desk\", text)\n    text = re.sub(r\"TheaterTrial\", \"Theater Trial\", text)\n    text = re.sub(r\"CatoInstitute\", \"Cato Institute\", text)\n    text = re.sub(r\"EmekaGift\", \"Emeka Gift\", text)\n    text = re.sub(r\"LetsBe_Rational\", \"Let us be rational\", text)\n    text = re.sub(r\"Cynicalreality\", \"Cynical reality\", text)\n    text = re.sub(r\"FredOlsenCruise\", \"Fred Olsen Cruise\", text)\n    text = re.sub(r\"NotSorry\", \"not sorry\", text)\n    text = re.sub(r\"UseYourWords\", \"use your words\", text)\n    text = re.sub(r\"WordoftheDay\", \"word of the day\", text)\n    text = re.sub(r\"Dictionarycom\", \"Dictionary.com\", text)\n    text = re.sub(r\"TheBrooklynLife\", \"The Brooklyn Life\", text)\n    text = re.sub(r\"jokethey\", \"joke they\", text)\n    text = re.sub(r\"nflweek1picks\", \"NFL week 1 picks\", text)\n    text = re.sub(r\"uiseful\", \"useful\", text)\n    text = re.sub(r\"JusticeDotOrg\", \"The American Association for Justice\", text)\n    text = re.sub(r\"autoaccidents\", \"auto accidents\", text)\n    text = re.sub(r\"SteveGursten\", \"Steve Gursten\", text)\n    text = re.sub(r\"MichiganAutoLaw\", \"Michigan Auto Law\", text)\n    text = re.sub(r\"birdgang\", \"bird gang\", text)\n    text = re.sub(r\"nflnetwork\", \"NFL Network\", text)\n    text = re.sub(r\"NYDNSports\", \"NY Daily News Sports\", text)\n    text = re.sub(r\"RVacchianoNYDN\", \"Ralph Vacchiano NY Daily News\", text)\n    text = re.sub(r\"EdmontonEsks\", \"Edmonton Eskimos\", text)\n    text = re.sub(r\"david_brelsford\", \"David Brelsford\", text)\n    text = re.sub(r\"TOI_India\", \"The Times of India\", text)\n    text = re.sub(r\"hegot\", \"he got\", text)\n    text = re.sub(r\"SkinsOn9\", \"Skins on 9\", text)\n    text = re.sub(r\"sothathappened\", \"so that happened\", text)\n    text = re.sub(r\"LCOutOfDoors\", \"LC Out Of Doors\", text)\n    text = re.sub(r\"NationFirst\", \"Nation First\", text)\n    text = re.sub(r\"IndiaToday\", \"India Today\", text)\n    text = re.sub(r\"HLPS\", \"helps\", text)\n    text = re.sub(r\"HOSTAGESTHROSW\", \"hostages throw\", text)\n    text = re.sub(r\"SNCTIONS\", \"sanctions\", text)\n    text = re.sub(r\"BidTime\", \"Bid Time\", text)\n    text = re.sub(r\"crunchysensible\", \"crunchy sensible\", text)\n    text = re.sub(r\"RandomActsOfRomance\", \"Random acts of romance\", text)\n    text = re.sub(r\"MomentsAtHill\", \"Moments at hill\", text)\n    text = re.sub(r\"eatshit\", \"eat shit\", text)\n    text = re.sub(r\"liveleakfun\", \"live leak fun\", text)\n    text = re.sub(r\"SahelNews\", \"Sahel News\", text)\n    text = re.sub(r\"abc7newsbayarea\", \"ABC 7 News Bay Area\", text)\n    text = re.sub(r\"facilitiesmanagement\", \"facilities management\", text)\n    text = re.sub(r\"facilitydude\", \"facility dude\", text)\n    text = re.sub(r\"CampLogistics\", \"Camp logistics\", text)\n    text = re.sub(r\"alaskapublic\", \"Alaska public\", text)\n    text = re.sub(r\"MarketResearch\", \"Market Research\", text)\n    text = re.sub(r\"AccuracyEsports\", \"Accuracy Esports\", text)\n    text = re.sub(r\"TheBodyShopAust\", \"The Body Shop Australia\", text)\n    text = re.sub(r\"yychail\", \"Calgary hail\", text)\n    text = re.sub(r\"yyctraffic\", \"Calgary traffic\", text)\n    text = re.sub(r\"eliotschool\", \"eliot school\", text)\n    text = re.sub(r\"TheBrokenCity\", \"The Broken City\", text)\n    text = re.sub(r\"OldsFireDept\", \"Olds Fire Department\", text)\n    text = re.sub(r\"RiverComplex\", \"River Complex\", text)\n    text = re.sub(r\"fieldworksmells\", \"field work smells\", text)\n    text = re.sub(r\"IranElection\", \"Iran Election\", text)\n    text = re.sub(r\"glowng\", \"glowing\", text)\n    text = re.sub(r\"kindlng\", \"kindling\", text)\n    text = re.sub(r\"riggd\", \"rigged\", text)\n    text = re.sub(r\"slownewsday\", \"slow news day\", text)\n    text = re.sub(r\"MyanmarFlood\", \"Myanmar Flood\", text)\n    text = re.sub(r\"abc7chicago\", \"ABC 7 Chicago\", text)\n    text = re.sub(r\"copolitics\", \"Colorado Politics\", text)\n    text = re.sub(r\"AdilGhumro\", \"Adil Ghumro\", text)\n    text = re.sub(r\"netbots\", \"net bots\", text)\n    text = re.sub(r\"byebyeroad\", \"bye bye road\", text)\n    text = re.sub(r\"massiveflooding\", \"massive flooding\", text)\n    text = re.sub(r\"EndofUS\", \"End of United States\", text)\n    text = re.sub(r\"35PM\", \"35 PM\", text)\n    text = re.sub(r\"greektheatrela\", \"Greek Theatre Los Angeles\", text)\n    text = re.sub(r\"76mins\", \"76 minutes\", text)\n    text = re.sub(r\"publicsafetyfirst\", \"public safety first\", text)\n    text = re.sub(r\"livesmatter\", \"lives matter\", text)\n    text = re.sub(r\"myhometown\", \"my hometown\", text)\n    text = re.sub(r\"tankerfire\", \"tanker fire\", text)\n    text = re.sub(r\"MEMORIALDAY\", \"memorial day\", text)\n    text = re.sub(r\"MEMORIAL_DAY\", \"memorial day\", text)\n    text = re.sub(r\"instaxbooty\", \"instagram booty\", text)\n    text = re.sub(r\"Jerusalem_Post\", \"Jerusalem Post\", text)\n    text = re.sub(r\"WayneRooney_INA\", \"Wayne Rooney\", text)\n    text = re.sub(r\"VirtualReality\", \"Virtual Reality\", text)\n    text = re.sub(r\"OculusRift\", \"Oculus Rift\", text)\n    text = re.sub(r\"OwenJones84\", \"Owen Jones\", text)\n    text = re.sub(r\"jeremycorbyn\", \"Jeremy Corbyn\", text)\n    text = re.sub(r\"paulrogers002\", \"Paul Rogers\", text)\n    text = re.sub(r\"mortalkombatx\", \"Mortal Kombat X\", text)\n    text = re.sub(r\"mortalkombat\", \"Mortal Kombat\", text)\n    text = re.sub(r\"FilipeCoelho92\", \"Filipe Coelho\", text)\n    text = re.sub(r\"OnlyQuakeNews\", \"Only Quake News\", text)\n    text = re.sub(r\"kostumes\", \"costumes\", text)\n    text = re.sub(r\"YEEESSSS\", \"yes\", text)\n    text = re.sub(r\"ToshikazuKatayama\", \"Toshikazu Katayama\", text)\n    text = re.sub(r\"IntlDevelopment\", \"Intl Development\", text)\n    text = re.sub(r\"ExtremeWeather\", \"Extreme Weather\", text)\n    text = re.sub(r\"WereNotGruberVoters\", \"We are not gruber voters\", text)\n    text = re.sub(r\"NewsThousands\", \"News Thousands\", text)\n    text = re.sub(r\"EdmundAdamus\", \"Edmund Adamus\", text)\n    text = re.sub(r\"EyewitnessWV\", \"Eye witness WV\", text)\n    text = re.sub(r\"PhiladelphiaMuseu\", \"Philadelphia Museum\", text)\n    text = re.sub(r\"DublinComicCon\", \"Dublin Comic Con\", text)\n    text = re.sub(r\"NicholasBrendon\", \"Nicholas Brendon\", text)\n    text = re.sub(r\"Alltheway80s\", \"All the way 80s\", text)\n    text = re.sub(r\"FromTheField\", \"From the field\", text)\n    text = re.sub(r\"NorthIowa\", \"North Iowa\", text)\n    text = re.sub(r\"WillowFire\", \"Willow Fire\", text)\n    text = re.sub(r\"MadRiverComplex\", \"Mad River Complex\", text)\n    text = re.sub(r\"feelingmanly\", \"feeling manly\", text)\n    text = re.sub(r\"stillnotoverit\", \"still not over it\", text)\n    text = re.sub(r\"FortitudeValley\", \"Fortitude Valley\", text)\n    text = re.sub(r\"CoastpowerlineTramTr\", \"Coast powerline\", text)\n    text = re.sub(r\"ServicesGold\", \"Services Gold\", text)\n    text = re.sub(r\"NewsbrokenEmergency\", \"News broken emergency\", text)\n    text = re.sub(r\"Evaucation\", \"evacuation\", text)\n    text = re.sub(r\"leaveevacuateexitbe\", \"leave evacuate exit be\", text)\n    text = re.sub(r\"P_EOPLE\", \"PEOPLE\", text)\n    text = re.sub(r\"Tubestrike\", \"tube strike\", text)\n    text = re.sub(r\"CLASS_SICK\", \"CLASS SICK\", text)\n    text = re.sub(r\"localplumber\", \"local plumber\", text)\n    text = re.sub(r\"awesomejobsiri\", \"awesome job siri\", text)\n    text = re.sub(r\"PayForItHow\", \"Pay for it how\", text)\n    text = re.sub(r\"ThisIsAfrica\", \"This is Africa\", text)\n    text = re.sub(r\"crimeairnetwork\", \"crime air network\", text)\n    text = re.sub(r\"KimAcheson\", \"Kim Acheson\", text)\n    text = re.sub(r\"cityofcalgary\", \"City of Calgary\", text)\n    text = re.sub(r\"prosyndicate\", \"pro syndicate\", text)\n    text = re.sub(r\"660NEWS\", \"660 NEWS\", text)\n    text = re.sub(r\"BusInsMagazine\", \"Business Insurance Magazine\", text)\n    text = re.sub(r\"wfocus\", \"focus\", text)\n    text = re.sub(r\"ShastaDam\", \"Shasta Dam\", text)\n    text = re.sub(r\"go2MarkFranco\", \"Mark Franco\", text)\n    text = re.sub(r\"StephGHinojosa\", \"Steph Hinojosa\", text)\n    text = re.sub(r\"Nashgrier\", \"Nash Grier\", text)\n    text = re.sub(r\"NashNewVideo\", \"Nash new video\", text)\n    text = re.sub(r\"IWouldntGetElectedBecause\", \"I would not get elected because\", text)\n    text = re.sub(r\"SHGames\", \"Sledgehammer Games\", text)\n    text = re.sub(r\"bedhair\", \"bed hair\", text)\n    text = re.sub(r\"JoelHeyman\", \"Joel Heyman\", text)\n    text = re.sub(r\"viaYouTube\", \"via YouTube\", text)\n           \n    # ... and ..\n    text = text.replace('...', ' ... ')\n    if '...' not in text:\n        text = text.replace('..', ' ... ')      \n        \n    # Acronyms\n    text = re.sub(r\"MH370\", \"Malaysia Airlines Flight 370\", text)\n    text = re.sub(r\"m\u00cc\u00bcsica\", \"music\", text)\n    text = re.sub(r\"okwx\", \"Oklahoma City Weather\", text)\n    text = re.sub(r\"arwx\", \"Arkansas Weather\", text)    \n    text = re.sub(r\"gawx\", \"Georgia Weather\", text)  \n    text = re.sub(r\"scwx\", \"South Carolina Weather\", text)  \n    text = re.sub(r\"cawx\", \"California Weather\", text)\n    text = re.sub(r\"tnwx\", \"Tennessee Weather\", text)\n    text = re.sub(r\"azwx\", \"Arizona Weather\", text)  \n    text = re.sub(r\"alwx\", \"Alabama Weather\", text)\n    text = re.sub(r\"wordpressdotcom\", \"wordpress\", text)    \n    text = re.sub(r\"usNWSgov\", \"United States National Weather Service\", text)\n    text = re.sub(r\"Suruc\", \"Sanliurfa\", text)   \n    \n    # Grouping same words without embeddings\n    text = re.sub(r\"Bestnaijamade\", \"bestnaijamade\", text)\n    text = re.sub(r\"SOUDELOR\", \"Soudelor\", text)\n    \n    return text","97360813":"train_df['text_cleaned'] = train_df['text'].apply(lambda s : clean_tweets(s))\ntest_df['text_cleaned'] = test_df['text'].apply(lambda s : clean_tweets(s))","d93017b6":"train_df.head()","dc8b1ea5":"from collections import defaultdict\ntrain1_df = train_df[train_df[\"target\"]==1]\ntrain0_df = train_df[train_df[\"target\"]==0]\n\n## custom function for ngram generation ##\ndef generate_ngrams(text, n_gram=1):\n    token = [token for token in text.lower().split(\" \") if token != \"\" if token not in STOPWORDS]\n    ngrams = zip(*[token[i:] for i in range(n_gram)])\n    return [\" \".join(ngram) for ngram in ngrams]\n\n## custom function for horizontal bar chart ##\ndef horizontal_bar_chart(df, color):\n    trace = go.Bar(\n        y=df[\"word\"].values[::-1],\n        x=df[\"wordcount\"].values[::-1],\n        showlegend=False,\n        orientation = 'h',\n        marker=dict(\n            color=color,\n        ),\n    )\n    return trace\n\n## Get the bar chart from sincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train0_df[\"text_cleaned\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n## Get the bar chart from insincere questions ##\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text_cleaned\"]:\n    for word in generate_ngrams(sent):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'blue')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,\n                          subplot_titles=[\"Frequent words of not real disaster tweets\", \n                                          \"Frequent words of real disaster tweets\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Word Count Plots\")\npy.iplot(fig, filename='word-plots')\n\n#plt.figure(figsize=(10,16))\n#sns.barplot(x=\"ngram_count\", y=\"ngram\", data=fd_sorted.loc[:50,:], color=\"b\")\n#plt.title(\"Frequent words for Insincere Questions\", fontsize=16)\n#plt.show()\n","3e11c74c":"freq_dict = defaultdict(int)\nfor sent in train0_df[\"text_cleaned\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace0 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n\nfreq_dict = defaultdict(int)\nfor sent in train1_df[\"text_cleaned\"]:\n    for word in generate_ngrams(sent,2):\n        freq_dict[word] += 1\nfd_sorted = pd.DataFrame(sorted(freq_dict.items(), key=lambda x: x[1])[::-1])\nfd_sorted.columns = [\"word\", \"wordcount\"]\ntrace1 = horizontal_bar_chart(fd_sorted.head(50), 'orange')\n\n# Creating two subplots\nfig = tools.make_subplots(rows=1, cols=2, vertical_spacing=0.04,horizontal_spacing=0.15,\n                          subplot_titles=[\"Frequent bigrams of not real disaster tweets\", \n                                          \"Frequent bigrams of real disaster tweets\"])\nfig.append_trace(trace0, 1, 1)\nfig.append_trace(trace1, 1, 2)\nfig['layout'].update(height=1200, width=900, paper_bgcolor='rgb(233,233,233)', title=\"Bigram Count Plots\")\npy.iplot(fig, filename='word-plots')","4a8bceb9":"import sklearn\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional\nimport transformers\n\nfrom torch.autograd import Variable\nfrom torch.optim import AdamW\nfrom tqdm import tqdm\n\nfrom transformers import BertTokenizer,BertModel\nlogging.getLogger(\"transformers.tokenization_utils\").setLevel(logging.ERROR)","5424845c":"target_columns = [\"target\"]\ntrain_df.iloc[[103]][target_columns]","0f811afb":"train_df.target.value_counts()","3b387b14":"from sklearn.model_selection import train_test_split\ndf_train , df_valid = train_test_split(train_df, test_size=0.2,random_state=42)","b7b0c509":"print(df_train.shape)\nprint(df_valid.shape)","203240cd":"df_test = test_df.copy()","c99bc7d3":"#Load the Bert Model and the tokenizer along with the weights\ntokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\nbert_model = BertModel.from_pretrained(\"bert-base-uncased\")","783ff219":"def tokenize_text(df, max_seq_length):\n    return [\n        tokenizer.encode(str(text), add_special_tokens=True)[:max_seq_length] for text in df.text_cleaned.values\n    ]\n\n\ndef pad_text(tokenized_text, max_seq_length):\n    return np.array([el + [0] * (max_seq_length - len(el)) for el in tokenized_text])\n\n\ndef tokenize_and_pad_text(df, max_seq_length):\n    tokenized_text = tokenize_text(df, max_seq_length)\n    padded_text = pad_text(tokenized_text, max_seq_length)\n    return torch.tensor(padded_text)\n\n\ndef targets_to_tensor(df, target_columns):\n    return torch.tensor(df[target_columns].values, dtype=torch.float32)","259f1967":"max_seq_length = 50\ntrain_indices = tokenize_and_pad_text(df_train, max_seq_length)\nval_indices = tokenize_and_pad_text(df_valid, max_seq_length)\ntest_indices = tokenize_and_pad_text(df_test, max_seq_length)","2a094345":"with torch.no_grad():\n    x_train = bert_model(train_indices)[0]  # Models outputs are tuples\n    x_val = bert_model(val_indices)[0]\n    x_test = bert_model(test_indices)[0]","64381dd7":"y_train = targets_to_tensor(df_train, target_columns)\ny_val = targets_to_tensor(df_valid, target_columns)","58331f99":"x_train[0]","cf2fbadf":"x_train[0].shape","dbbad650":"class BertCNN(nn.Module):\n    def __init__(self, embed_num, embed_dim, class_num, kernel_num, kernel_sizes, dropout, static):\n        super(BertCNN, self).__init__()\n\n        embed_num = embed_num\n        embed_dimension = embed_dim\n        num_labels = class_num\n        kernel_num = kernel_num\n        kernal_sizes = kernel_sizes\n        dropout = dropout\n\n        self.static = static\n        self.embed = nn.Embedding(embed_num, embed_dimension)\n        self.convs = nn.ModuleList([nn.Conv2d(1, kernel_num, (k, embed_dim)) for k in kernel_sizes])\n        self.dropout = nn.Dropout(dropout)\n        self.classifier = nn.Linear(len(kernel_sizes)*kernel_num, num_labels)\n        self.sigmoid = nn.Sigmoid()\n        \n\n    def forward(self, inputs):\n        if self.static:\n            output = Variable(inputs)\n        output = inputs.unsqueeze(1)\n        output = [nn.functional.relu(conv(output)).squeeze(3) for conv in self.convs]\n        output = [nn.functional.max_pool1d(i, i.size(2)).squeeze(2) for i in output]\n        output = torch.cat(output, 1)\n        output = self.dropout(output)\n        logits = self.classifier(output)\n        output = self.sigmoid(logits)\n        return output","56b5ccd7":"embed_num = max_seq_length\nembed_dim = bert_model.config.hidden_size \nclass_num = y_train.shape[1]\nkernel_num = 3\nkernel_sizes = [2, 3, 4]\ndropout = 0.5\nstatic = True","bca08369":"model = BertCNN(\n    embed_num=embed_num,\n    embed_dim=embed_dim,\n    class_num=class_num,\n    kernel_num=kernel_num,\n    kernel_sizes=kernel_sizes,\n    dropout=dropout,\n    static=static,\n)","79c31cf0":"n_epochs = 4\nbatch_size = 32\nlearning_rate = 1e-3","a69d87c4":"def generate_batch_data(x, y, batch_size):\n    i, batch = 0, 0\n    for batch, i in enumerate(range(0, len(x) - batch_size, batch_size), 1):\n        x_batch = x[i : i + batch_size]\n        y_batch = y[i : i + batch_size]\n        yield x_batch, y_batch, batch\n    if i + batch_size < len(x):\n        yield x[i + batch_size :], y[i + batch_size :], batch + 1\n    if batch == 0:\n        yield x, y, 1","900b50bd":"train_losses, val_losses = [], []\noptimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n#Binary Cross entroy loss function since we have only two class(0 and 1)\nloss_functionn = nn.BCELoss()\n\nfor epoch in range(n_epochs):\n    start_time = time.time()\n    train_loss = 0\n\n    model.train(True)\n    for x_batch, y_batch, batch in generate_batch_data(x_train, y_train, batch_size):\n        y_pred = model(x_batch)\n        optimizer.zero_grad()\n        loss = loss_functionn(y_pred, y_batch)\n        loss.backward()\n        optimizer.step()\n        train_loss += loss.item()\n\n    train_loss \/= batch\n    train_losses.append(train_loss)\n    elapsed = time.time() - start_time\n\n    model.eval() \n    val_loss, batch = 0, 1\n    for x_batch, y_batch, batch in generate_batch_data(x_val, y_val, batch_size):\n        y_pred = model(x_batch)\n        loss = loss_functionn(y_pred, y_batch)\n        val_loss += loss.item()\n    val_loss \/= batch\n    val_losses.append(val_loss)\n\n    print(\n        \"Epoch %d Train loss: %.2f. Validation loss: %.2f. Elapsed time: %.2fs.\"\n        % (epoch + 1, train_losses[-1], val_losses[-1], elapsed)\n    )","6afbe39b":"plt.plot(train_losses, label=\"Training loss\")\nplt.plot(val_losses, label=\"Validation loss\")\nplt.legend()\nplt.title(\"Losses\")","8fdba711":"model.eval() \ny_preds = []\nbatch = 0\nfor x_batch, y_batch, batch in generate_batch_data(x_val, y_val, batch_size):\n    y_pred = model(x_batch)\n    y_preds.extend(y_pred.detach().numpy().tolist())\ny_preds_val = np.array(y_preds)","07f8a8ef":"y_val_target = df_valid[target_columns].values","c3d441a1":"from sklearn.metrics import roc_auc_score\nauc_scores = roc_auc_score(y_val_target, y_preds_val, average=None)\ndf_accuracy = pd.DataFrame({\"label\": target_columns, \"auc\": auc_scores})\ndf_accuracy.sort_values('auc')[::-1]","8163e4b0":"df_valid_targets = df_valid[target_columns]\ndf_pred_targets = pd.DataFrame(y_preds_val.round(), columns=target_columns, dtype=int)\ndf_final_pred = df_valid_targets.join(df_pred_targets, how='inner', rsuffix='_pred')","751e7fe3":"df_final_pred","35b02bbc":"df_valid_targets.sum()","9ec9dfd1":"df_pred_targets.sum()","41df1423":"#Keeping the copy of our dataset in order to prevent overwrite\nfinal_train_df = train_df.copy()\nfinal_test_df = test_df.copy() ","bee8324f":"from sklearn.model_selection import train_test_split\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn import metrics","5b7eadf9":"vectorizer = TfidfVectorizer(ngram_range=(1,2))\nX_train_vec = vectorizer.fit_transform(df_train.text_cleaned.astype(\"str\")).toarray()\nX_val_vec = vectorizer.transform(df_valid.text_cleaned.astype(\"str\")).toarray()","0d8c0acb":"len(vectorizer.get_feature_names())","98380911":"from sklearn import model_selection, naive_bayes, svm\n\nSVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\nstart = time.time()\nSVM.fit(X_train_vec,trai.target.values)\n\n# predict the labels on validation dataset\npredictions_SVM = SVM.predict(X_val_vec)\nend = time.time()\n\nprint('The time taken for execution is :', end- start)","bb442ecf":"from sklearn.metrics import accuracy_score\n# Use accuracy_score function to get the accuracy\nprint(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, df_valid.target.values)*100)","31c49b1d":"from sklearn.metrics import classification_report\ntarget_names = ['Not Real Disaster', 'Real Disaster']\nprint(classification_report(df_valid.target.values, predictions_SVM, target_names=target_names))","46b126d8":"import numpy as np\nimport pandas as pd\nfrom gensim.models.word2vec import Word2Vec\nfrom sklearn.model_selection import train_test_split\nfrom keras.utils import to_categorical\nfrom keras.layers import Dense, Dropout, Conv1D, MaxPool1D, GlobalMaxPool1D, Embedding, Activation\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.models import Sequential\nimport re\nimport nltk\nfrom nltk.corpus import stopwords\nfrom nltk.stem.snowball import PorterStemmer\nfrom sklearn import preprocessing\n\nimport keras\nfrom tensorflow.python.client import device_lib\n\nprint(device_lib.list_local_devices())\nimport numpy as np\nimport pandas as pd\nfrom collections import defaultdict\nimport re\nimport time\nimport tensorflow as tf\nimport sys\nimport os\nfrom keras import regularizers\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.utils.np_utils import to_categorical\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.layers import Embedding\nfrom keras.layers import Dense, Input, Flatten\nfrom keras.layers import Conv1D, MaxPooling1D, Embedding, Dropout\nfrom keras.models import Model\nfrom keras.models import Sequential\nfrom keras.layers.convolutional import Conv3D\nfrom keras.layers.convolutional_recurrent import ConvLSTM2D\nfrom keras.layers.normalization import BatchNormalization\nfrom keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\nimport numpy as np\nfrom matplotlib import pyplot as plt\nfrom keras.layers import Dense, Embedding, LSTM, GRU\n\n%matplotlib inline\nfrom sklearn.metrics import classification_report\nMAX_SEQUENCE_LENGTH = 300\nMAX_NB_WORDS = 20000\nEMBEDDING_DIM = 100\nVALIDATION_SPLIT = 0.2","7ce8d61a":"def preprocess_text(sen):\n    stops = stopwords.words('english')\n    porter = PorterStemmer()\n    for word in sen.split():\n        if word in stops:\n            sen = sen.replace(word, '')\n        sen = sen.replace(word, porter.stem(word))\n    return sen.lower()","a4aa4066":"df_train_1 = final_train_df.copy()\ndf_train_1['text_cleaned'] = df_train_1['text_cleaned'].apply(preprocess_text)\ndf_train_1.head(10)","40f3d00a":"text_cleaned = []\nfor i in df_train_1['text_cleaned']:\n    text_cleaned.append(i.split())\nprint(text_cleaned[:2])","3f1c4581":"word2vec_model = Word2Vec(text_cleaned,vector_size=500, window=3, min_count=1, workers=16)\nprint(word2vec_model)","2571de59":"token = Tokenizer(7229)\ntoken.fit_on_texts(df_train_1['text_cleaned'])\ntext = token.texts_to_sequences(df_train_1['text_cleaned'])\ntext = pad_sequences(text, 75)\nprint(text[:2])","fb26b4a5":"y = df_train_1.target.values\ny[:2]","256311db":"x_train, x_test, y_train, y_test = train_test_split(np.array(text), y, test_size=0.2, stratify=y)","6217dc44":"x_test.shape","8031b47c":"#https:\/\/github.com\/RaRe-Technologies\/gensim\/wiki\/Using-Gensim-Embeddings-with-Keras-and-Tensorflow\nfrom tensorflow.keras.layers import Embedding\n\ndef gensim_to_keras_embedding(model, train_embeddings=False):\n    \"\"\"Get a Keras 'Embedding' layer with weights set from Word2Vec model's learned word embeddings.\n\n    Parameters\n    ----------\n    train_embeddings : bool\n        If False, the returned weights are frozen and stopped from being updated.\n        If True, the weights can \/ will be further updated in Keras.\n\n    Returns\n    -------\n    `keras.layers.Embedding`\n        Embedding layer, to be used as input to deeper network layers.\n\n    \"\"\"\n    keyed_vectors = model.wv  # structure holding the result of training\n    weights = keyed_vectors.vectors  # vectors themselves, a 2D numpy array    \n    index_to_key = keyed_vectors.index_to_key  # which row in `weights` corresponds to which word?\n\n    layer = Embedding(\n        input_dim=weights.shape[0],\n        output_dim=weights.shape[1],\n        weights=[weights],\n        trainable=train_embeddings,\n    )\n    return layer","6e1f79e5":"keras_model = Sequential()\nkeras_model.add(gensim_to_keras_embedding(word2vec_model))\nkeras_model.add(Conv1D(300, 3, activation='relu', padding='same', strides=1))\nkeras_model.add(MaxPool1D())\nkeras_model.add(Dropout(0.2))\nkeras_model.add(Conv1D(200, 3, activation='relu', padding='same', strides=1))\nkeras_model.add(GlobalMaxPool1D())\nkeras_model.add(Dropout(0.2))\nkeras_model.add(Dense(128,kernel_regularizer=regularizers.l2(0.001), activation='relu'))\nkeras_model.add(Dense(64,kernel_regularizer=regularizers.l2(0.001), activation='relu'))\nkeras_model.add(Dropout(0.2))\nkeras_model.add(Dense(2))\nkeras_model.add(Activation('softmax'))\nkeras_model.compile(loss = 'sparse_categorical_crossentropy', metrics=['acc'], optimizer='adam')\nprint(keras_model.summary())\nstart = time.time()\nhistory_1 = keras_model.fit(x_train, y_train, batch_size=32, epochs=10, validation_data=(x_test, y_test))\nend = time.time()\nprint('The time taken for execution is :', end- start)","df3ba814":"# plot the training loss and accuracy\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, 10), history_1.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, 10), history_1.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, 10), history_1.history[\"acc\"], label=\"train_acc\")\nplt.plot(np.arange(0, 10), history_1.history[\"val_acc\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss\/Accuracy\")\nplt.legend()\nplt.show()","c892b95d":"tokenizer = Tokenizer(num_words=MAX_NB_WORDS)\ntokenizer.fit_on_texts(train_df.text_cleaned.astype(\"str\"))\nsequences = tokenizer.texts_to_sequences(train_df.text_cleaned.astype(\"str\"))","21ff5c30":"data = pad_sequences(sequences, maxlen=MAX_SEQUENCE_LENGTH)\nlabels = to_categorical(np.asarray(train_df.target.values),num_classes = 2)\nprint('Shape of data tensor:', data.shape)\nprint('Shape of label tensor:', labels.shape)","cbe4c631":"# Train test validation Split\nfrom sklearn.model_selection import train_test_split\n\nindices = np.arange(data.shape[0])\nnp.random.shuffle(indices)\ndata = data[indices]\nlabels = labels[indices]\nx_train, x_val, y_train, y_val = train_test_split(data, labels, test_size=0.20, random_state=42)\nprint('Size of train, validation:', len(y_train), len(y_val))\n\nprint('Real and Not Real Disaster tweets in train, validation and test:')\nprint(y_train.shape)\nprint(y_val.shape)","765e57ae":"#Using Pre-trained word embeddings\nEMBEDDING_FILE = '..\/input\/glove100\/glove.6B.100d.txt'\nembeddings_index = {}\nwith open(EMBEDDING_FILE,encoding='utf8') as f:\n    for line in f:\n        values = line.rstrip().rsplit(' ')\n        word = values[0]\n        coefs = np.asarray(values[1:], dtype='float32')\n#         embeddings_index[word] = coefs\n        embeddings_index[word] = coefs\n    f.close()\nword_index = tokenizer.word_index\nprint('Total %s word vectors in Glove.' % len(embeddings_index))\n\nembedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\nfor word, i in word_index.items():\n    embedding_vector = embeddings_index.get(word)\n    if embedding_vector is not None:\n        # words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","2db2aea8":"embedding_layer = Embedding(len(word_index) + 1,\n                            EMBEDDING_DIM,\n                            weights=[embedding_matrix],\n                            input_length=MAX_SEQUENCE_LENGTH)","a7cffbdf":"#Creating a Convolutional + LSTM model to check it's result against our BERT+CNN Model\nkeep_prob = 0.2\nmodell = Sequential()\nmodell.add(embedding_layer)\nmodell.add(Conv1D(filters=100, kernel_size=5, padding='same', activation='relu'))\nmodell.add(BatchNormalization())\nmodell.add(MaxPooling1D(pool_size=2))\nmodell.add(Dropout(rate = 1 - keep_prob))\nmodell.add(Conv1D(filters=200, kernel_size=3, padding='same', activation='relu'))\nmodell.add(BatchNormalization())\nmodell.add(MaxPooling1D(pool_size=2))\nmodell.add(Dropout(rate = 1 - keep_prob))\nmodell.add(Conv1D(filters=300, kernel_size=3, padding='same', activation='relu'))\nmodell.add(BatchNormalization())\nmodell.add(MaxPooling1D(pool_size=2))\nmodell.add(Dropout(rate = 1 - keep_prob))\nmodell.add(Flatten())\nmodell.add(Dense(512,activation='relu'))\nmodell.add(Dropout(rate = 1 - keep_prob)\nmodell.add(Dense(2, activation='softmax'))\nmodell.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\nprint(modell.summary())\nstart = time.time()\nhistory = modell.fit(x_train, y_train,validation_data=(x_val, y_val), epochs=10, batch_size=32)\nend = time.time()\nprint('The time taken for execution is :', end- start)","29e74118":"# plot the training loss and accuracy\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, 10), history.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, 10), history.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, 10), history.history[\"accuracy\"], label=\"train_acc\")\nplt.plot(np.arange(0, 10), history.history[\"val_accuracy\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss\/Accuracy\")\nplt.legend()\nplt.show()","c346fd55":"keep_prob = 0.2\nmodelL = Sequential()\nmodelL.add(embedding_layer)\nmodel.add(Bidirectional(LSTM(\n512, \nreturn_sequences = True, \nrecurrent_dropout=0.2\n)))\nmodelL.add(GlobalMaxPool1D())\nmodelL.add(BatchNormalization())\nmodelL.add(Dropout(0.5))\nmodelL.add(Dense(512, activation = \"relu\"))\nmodelL.add(Dropout(0.5))\nmodelL.add(Dense(128, activation = \"relu\"))\nmodelL.add(Dropout(0.5))\nmodelL.add(Dense(2, activation = 'softmax'))\nmodelL.compile(optimizer='rmsprop', loss='binary_crossentropy', metrics=['accuracy'])\n","38fe4787":"from keras.callbacks import ReduceLROnPlateau\nreduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.2,\n                              patience=5, min_lr=0.001)\nmodelL.summary()\nstart = time.time()\nhistory_2 = modelL.fit(x_train, y_train,validation_data=(x_val, y_val), verbose = 1,\n    callbacks = [reduce_lr],epochs=15, batch_size=32)\nend = time.time()\nprint('The time taken for execution is :', end- start)","5cfddc8e":"# plot the training loss and accuracy\nplt.style.use(\"ggplot\")\nplt.figure()\nplt.plot(np.arange(0, 15), history_2.history[\"loss\"], label=\"train_loss\")\nplt.plot(np.arange(0, 15), history_2.history[\"val_loss\"], label=\"val_loss\")\nplt.plot(np.arange(0, 15), history_2.history[\"accuracy\"], label=\"train_acc\")\nplt.plot(np.arange(0, 15), history_2.history[\"val_accuracy\"], label=\"val_acc\")\nplt.title(\"Training Loss and Accuracy\")\nplt.xlabel(\"Epoch #\")\nplt.ylabel(\"Loss\/Accuracy\")\nplt.legend()\nplt.show()","d29dbb70":"model.eval() # disable dropout for deterministic output\n# with torch.no_grad(): # deactivate autograd engine to reduce memory usage and speed up computations\ny_preds = []\ny_pred = model(x_test)\ny_preds.extend(y_pred.detach().numpy().tolist())\ny_preds_np = np.array(y_preds)","6a0a8bab":"df_test[\"target\"] = np.round(np.vstack(y_preds_np)).astype(int)\ndf_test.head(20)","cbaf9407":"df_test.to_csv('submission.csv', index = False)","5c6d889d":"**Building a SVM model with linear Kernal and using TfIdf**\n\nSince Text is linearly separable. We use SVM's linear kernal to classify the text into Real or not Real disaster","7b54428e":"**As you can see that the SVM model performed well on our training dataset**\n\nAlthough this can be considered a good approach, the time taken for this is quite more compared to our model.","9c0a95f5":"# Model building","e18d4126":"**1.1 Number of possible values in \"keyword\", \"location\" and Target Distribution**\nSince **\"location\"** of the tweet is set default by the user, it is highly ambigous and cannot be considered as a valid feature.\n\nAs mentioned in the challenge a tweet can have some relavent **\"keyword\"** which can only be used in some specific context. Let's check the count of \"Keywords\" for each tweet counts and with see if there is a correlation with the **\"Target distribution\"**. We will also see if keyword can be used as a feature by itself or as a word added to the text. Every single keyword in training set exists in test set. ","bff8d8b9":"# **Data Preprocessing**\n\nFrom the above steps we now realize that twitter tweets always have to be cleaned before we feed into our model. \n\nSo we will do some basic cleaning such as spelling correction,removing punctuations,removing html tags and emojis etc.So let's start.","9f70501f":"Since we just want to update their values because we do not want PyTorch to calculate the gradients of the new defined variables","22271ff2":"# References\n1. Yoon Kim, Convolutional Neural Networks for Sentence Classification (2014), https:\/\/arxiv.org\/pdf\/1408.5882.pdf\n\n2. Ye Zhang, A Sensitivity Analysis of (and Practitioners\u2019 Guide to) Convolutional Neural Networks for Sentence Classification (2016), https:\/\/arxiv.org\/pdf\/1510.03820.pdf\n\n3. Jacob Devlin, BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding (2018), https:\/\/arxiv.org\/abs\/1810.04805","7702aca3":"# Conclusion\nWe trained a CNN with BERT embeddings for identifying if the tweets can be classified as Real disaster or not. We notices that the even though the dataset is small, it takes memory and time to compute the embeddings required for CNN layer. Instead of BERT, we could use Word2Vec, which would speed up the transformation of words to embeddings. \n\nWe performed following hyper parameter tuning\n\n1. We tried different learning rate such as `3e-5` and `1e-3` . We noticed that the model gave optimal results after using the later learning rate.\n2. We also changed the dropout values from `0.1` to `0.5` and concluded that 0.5 serves our purpose. \n3. Finally, we trained the model from 3 - 10 epochs and realized that the model performed best until 4 epochs and started to overfitt.\n\nWe noted that AUC obtained is good and it can be improved by changing the hyperparameters.\n\n\n**We also noted that the current approach has beaten other approaches.** All though the author would like to suggest that there is definately a scope for improvement and by\n1. Fine tuning the BERT model and then passing it as an embedding layer can be considered as a future work.\n2. Increasing the size of sequence length will also improve the accuracy, But due to the limitation of hardware reasource(16GB ram on Free Kaggle and Colab account). I was not able to increase it's size.\n\n**Some of the Kernal's on Kaggle have achieved 100% accuracy**\n\nTest set labels can be found on [this](https:\/\/www.kaggle.com\/szelee\/disasters-on-social-media) website. Dataset is named Disasters on social media. This is how people are submitting perfect scores. Other \"Getting Started\" competitions also have their test labels available. The main point of \"Getting Started\" competitions is learning and sharing, and perfect score doesn't mean anything.\n\nPhil Culliton wrote: For the AutoML prize, any use of the label set will result in disqualification.\n\nAccording to [@philculliton](https:\/\/www.kaggle.com\/philculliton) from Kaggle Team, competitors who use test set labels in any way are not eligible to win AutoML prize. There are no other penalties for using them.","2a33b4af":"**We will take the same df_train and df_valid dataset that we have split in our first stage**","06ebea22":"**Now let us also create bigram frequency plots for both the classes separately to get more idea.**","7c4d45b9":"# Approach 4\n# GloVe  + CNN","efec910a":"# Approach 5\n# GloVe Embedding + CNN\n","7c76d354":"# Generating final submission results","0f7770c6":"Some context-free pre-trained language models (Word2Vec, FastText or GloVe) simply map the words into an embedding. Since BERT is a context based language model, we need to calculate the context, Hence we need to feed the text to the BERT model.\n\nIn the code below, we tokenize, pad and convert text to PyTorch Tensors. Then we use BERT to transform the text to embeddings. ","e3b4eb01":"# **Observation**\n1. **43%** of the training data are *real disaster* (**target=1**).\n2. **57%** are *not real disaster*(**target = 0**).\n\n**Word Cloud:**\n\nNow let us look at the frequently occuring words in the data by creating a word cloud on the 'text' column.","825703fd":"**Write about BiLSTM**","9225ce08":"**Final Cleaned dataset**","d423a256":"**Observations:**\n\nPlease look at the plots and it's clear that the real disaster tweets have good biagrams as compared to not real disaster tweets\nFor eg:\n\n1. **Real disaster tweet** biagrams : \"sucide bomber\", \"norther california\", \"oil spill\". \n2. **Not real disaster tweet** biagrams : \"-\", \"cross body\", \"@youtube video\".\n","0c945940":"**Loading Data**","4bcb3886":"In order to feed textual data into CNN, we need to  transform each text(tweet) into a 2D matrix. \n\nTo transform a text to a matrix, we need to:\n\n\n\n*   We limit the length of a text to 50 words (50). I am using this arbitary value since my kernal on Colab is crashing due to limited RAM. We can choose a higher value(such as 100, 128 or 256) if we have more ram \n*   Once we have set the length of the text, we need to pad a each text with less than 50 words so that we maintain the fixed size of the input text(add 0 vectors to the end).","fb929a0e":"**So after we trained a Word2Vec embedding model using Gensim, and now we want to use the result in a Keras \/ Tensorflow pipeline.**","5100f62b":"**Now let us also look at the trigram plots**","a75eae44":"**Split the train dataset**","46558db2":"# Feature importance\n\n**Textual implicit features:**\nTextual implicit features in classes and datasets can be helpful to identify disaster tweets. It looks like real disaster tweets are written in a more formal way with longer words compared to not real disaster tweets because most of them are coming from news agencies. Non-disaster tweets have more typos than disaster tweets because they are coming from individual users. Since these are fake information, the users tend to use more punctuation  marks, third person pronouns, hastags.\n\nThese textual implicit features can be used as a *features\". Let's see how these are distributed between the classes. \n\nFollowing are some features:\n\n1. Number of words in the text --> *word_count*\n2. Number of unique words in the text --> *unique_word_count*\n3. Number of characters in the text--> char_count\n4. Number of stopwords --> stop_word_count\n5. Number of punctuations --> punctuation_count\n6. Number of upper case words --> upper_case_count\n7. Number of title case words --> lower_case_count\n8. Average length of the words --> mean_word_length\n9. Number of hashtags (#) in text --> hashtag_count\n10. Number of mentions (@) in text --> mention_count\n\n","47d3758a":"**Observation**\n\n1. All of the new features have very similar distributions in training and test set. This shows that training and test set are taken from the same sample.\n\n2. All of the new features have information about target as well, but some of them are not good enough such as url_count, hashtag_count and mention_count.\n\nOn the other hand, word_count, unique_word_count, stop_word_count, mean_word_length, char_count, punctuation_count have very different distributions for real disaster and not real disaster tweets. Those features might be useful in models.","6313c3e7":"We see that the model correctly predicted some comments as Real disasters. This is good on the validation dataset. We will now proceed to test our model on the Test dataset and we will submit the results on Kaggle.\n\nThe performance of BiLSTM model has been good compared to the other models. It was faster in training and the accuracy achieved is better compared to other models.\n\n\n","b9d1fdc2":"**Observations:**\n1. Some of the top words in **not real disaster tweet** are  **'-', '&', 'will', '??'** etc. This shows that these tweets consist of punctuation symbols, digits and abbrevated words.\n2. Some of top words in **real disaster tweet** after excluding the **'-'** ones at the very top are **'fire', 'via'** etc. These tweets have valuable words\n3. The other top words in  **not real disaster tweet** after excluding the common ones are **'new', 'now', 'one'** etc.\n4. One more observation can be noted that there are some words which are written as abbrevation and they need to be expanded using contradiction pairs. This is necessary since the classification model which we are going to build using BERT and CNN, will perform better if the sentences are cleaned and have relatable meaning to it.\n\nHence we are going to clean tweets before sending this data to our model. We shall also see how the distribution of words changes after cleaning the data.","04119287":"# **Exploratory Data Analysis:**\nFirst let us look at the distribution of the target variable to understand more about the data such as class imbalance, missing values and so on.","d4d2f42b":"**Target Distribution**","1e9b7bd8":"# Approach 2 \n\nWe will use a simple approach in order to see if this can be used to get better results.\nHere we will use the TfIdf vectorizer to extract features from the text","0340d13b":"# Approach 6\n# GloVe + BiLSTM\n","64a5e2c2":"# Approach 3\n# Trying Word2Vec embedding layer on top of CNN and LSTM","41616904":"**Now let's see the distribution of Words and there frequency plot of \"disaster\" & \"not disaster\" tweets**"}}