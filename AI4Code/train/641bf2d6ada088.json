{"cell_type":{"1b9fb3a1":"code","9ba0a04a":"code","53db3012":"code","2cff0e14":"code","f95487c0":"code","f3374edf":"code","1b898de5":"code","f7e7f9d8":"code","37a4bef3":"code","25f8d13f":"code","fb1cd1e3":"code","b89746b1":"code","2cd2afb3":"code","a2037b57":"code","9099b4d5":"code","511869ae":"code","26956c80":"code","78ed5c49":"code","ef42ae0e":"code","2f670904":"code","151dcb75":"code","21ae79f9":"code","cea7b83d":"code","a5f05122":"code","9aedafca":"code","1ec0d93b":"code","6b0790cd":"code","a638c3f4":"code","eeede0a6":"code","ddf7c9d7":"code","11374f31":"code","0f320441":"code","d541d1c8":"code","64138309":"code","902d6f65":"code","71b1bee0":"code","f76b95ee":"code","619c0831":"markdown","13331763":"markdown","878d013f":"markdown","c5e17b34":"markdown","ed42e2b9":"markdown","542f303a":"markdown","07d224df":"markdown","9607f846":"markdown","5d4512c9":"markdown","4681f712":"markdown","8309c93e":"markdown","7031854a":"markdown","536fee85":"markdown","530a5e7b":"markdown","bbe60cb4":"markdown","c548c7b0":"markdown","455234be":"markdown","903787e7":"markdown","ff95e1f3":"markdown","e90f8c5b":"markdown","43fe37a0":"markdown","e72bf4c6":"markdown","12abeeb3":"markdown","5a45ca2c":"markdown","3dc9debe":"markdown","7f64109e":"markdown","0c5f66d3":"markdown","b879ae9a":"markdown","8acec23c":"markdown","6f4c3670":"markdown","90718746":"markdown","7c65578d":"markdown","bb5e5d69":"markdown"},"source":{"1b9fb3a1":"# First up, I'll import every library that will be used in this project is imported at the start.\n\n# Data handling and processing\nimport pandas as pd\nimport numpy as np\n\n# Data visualisation\nimport matplotlib.pyplot as plt\nimport seaborn as sns\n\n# Statistics\nfrom scipy import stats\nimport statsmodels.api as sm\nfrom scipy.stats import randint as sp_randint\nfrom time import time\n\n# NLP\nimport nltk\nnltk.download('wordnet')\nimport re\nfrom textblob import TextBlob\nfrom sklearn.feature_extraction import text\nfrom nltk.tokenize import TweetTokenizer\nfrom sklearn.feature_extraction.text import CountVectorizer\nfrom sklearn.feature_extraction.text import TfidfTransformer\n\n# Machine Learning\nfrom sklearn.model_selection import cross_val_score\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import cross_val_predict\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.metrics import classification_report\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.metrics import accuracy_score","9ba0a04a":"# Data downloaded from Kaggle as a .csv file and read into this notebook from my local directory\ntrain = pd.read_csv('..\/input\/movie-review-sentiment-analysis-kernels-only\/train.tsv', sep = '\\t')\ntest = pd.read_csv('..\/input\/movie-review-sentiment-analysis-kernels-only\/test.tsv', sep = '\\t')\nsub = pd.read_csv('..\/input\/movie-review-sentiment-analysis-kernels-only\/sampleSubmission.csv', sep=',')","53db3012":"# General information about the Dataset\ntrain.info()","2cff0e14":"# First 10 rows of the Dataset\ntrain.head(10)","f95487c0":"# Checking out the total number of unique sentences\ntrain['SentenceId'].nunique()","f3374edf":"# Returning average count of phrases per sentence, per Dataset\nprint('Average count of phrases per sentence in train is {0:.0f}.'.format(train.groupby('SentenceId')['Phrase'].count().mean()))\nprint('Average count of phrases per sentence in test is {0:.0f}.'.format(test.groupby('SentenceId')['Phrase'].count().mean()))","1b898de5":"# Returning total phrase and sentence count, per Dataset\nprint('Number of phrases in train: {}; number of sentences in train: {}.'.format(train.shape[0], len(train.SentenceId.unique())))\nprint('Number of phrases in test: {}; number of sentences in test: {}.'.format(test.shape[0], len(test.SentenceId.unique())))","f7e7f9d8":"# Returning average word length of phrases, per Dataset\nprint('Average word length of phrases in train is {0:.0f}.'.format(np.mean(train['Phrase'].apply(lambda x: len(x.split())))))\nprint('Average word length of phrases in test is {0:.0f}.'.format(np.mean(test['Phrase'].apply(lambda x: len(x.split())))))","37a4bef3":"# Set up graph\nfig, ax = plt.subplots(1, 1, dpi = 100, figsize = (10, 5))\n\n# Get data\nsentiment_labels = train['Sentiment'].value_counts().index\nsentiment_count = train['Sentiment'].value_counts()\n\n# Plot graph\nsns.barplot(x = sentiment_labels, y = sentiment_count)\n\n# Plot labels\nax.set_ylabel('Count')    \nax.set_xlabel('Sentiment Label')\nax.set_xticklabels(sentiment_labels , rotation=30)","25f8d13f":"# New column in the test set for concatenating\ntest['Sentiment']=-999\ntest.head()","fb1cd1e3":"# Concatenating Datasets before the cleaning can begin\ndata = pd.concat([train,test], ignore_index = True)\nprint(data.shape)\ndata.tail()","b89746b1":"# Deleting previous Datasets from memory\ndel train,test","2cd2afb3":"# Basic text cleaning function\ndef remove_noise(text):\n    \n    # Make lowercase\n    text = text.apply(lambda x: \" \".join(x.lower() for x in x.split()))\n    \n    #\u00a0Remove whitespaces\n    text = text.apply(lambda x: \" \".join(x.strip() for x in x.split()))\n    \n    # Convert to string\n    text = text.astype(str)\n        \n    return text","a2037b57":"# Apply the function and create a new column for the cleaned text\ndata['Clean Review'] = remove_noise(data['Phrase'])\ndata.head()","9099b4d5":"# Re-instating the training set\ntrain = data[data['Sentiment'] != -999]\ntrain.shape","511869ae":"# Re-instating the test set\ntest = data[data['Sentiment'] == -999]\ntest.drop('Sentiment', axis=1, inplace=True)\ntest.shape","26956c80":"# Getting a count of words from the documents\n# Ngram_range is set to 1,2 - meaning either single or two word combination will be extracted\ntokenizer = TweetTokenizer()\n\ncvec = CountVectorizer(ngram_range=(1,2), tokenizer=tokenizer.tokenize)\nfull_text = list(train['Clean Review'].values) + list(test['Clean Review'].values)\ncvec.fit(full_text)\n\n# Getting the total n-gram count\nlen(cvec.vocabulary_)","78ed5c49":"# Creating the bag-of-words representation: training set\ntrain_vectorized = cvec.transform(train['Clean Review'])\n\n# Getting the matrix shape\nprint('sparse matrix shape:', train_vectorized.shape)\n\n# Getting the nonzero count\nprint('nonzero count:', train_vectorized.nnz)\n\n# Getting sparsity %\nprint('sparsity: %.2f%%' % (100.0 * train_vectorized.nnz \/ (train_vectorized.shape[0] * train_vectorized.shape[1])))","ef42ae0e":"# Creating the bag-of-words representation: test set\ntest_vectorized = cvec.transform(test['Clean Review'])\n\n# Getting the matrix shape\nprint('sparse matrix shape:', test_vectorized.shape)\n\n# Getting the nonzero count\nprint('nonzero count:', test_vectorized.nnz)\n\n# Getting sparsity %\nprint('sparsity: %.2f%%' % (100.0 * test_vectorized.nnz \/ (test_vectorized.shape[0] * test_vectorized.shape[1])))","2f670904":"# Instantiating the TfidfTransformer\ntransformer = TfidfTransformer()\n\n# Fitting and transforming n-grams\ntrain_tdidf = transformer.fit_transform(train_vectorized)\ntest_tdidf = transformer.fit_transform(test_vectorized)","151dcb75":"# Create X & y variables for Machine Learning\nX_train = train_tdidf\ny_train = train['Sentiment']\n\nX_test = test_tdidf","21ae79f9":"# Model Fit and Prediction\ndef model(mod, model_name, X_train, y_train):\n    \n    # Fitting model\n    mod.fit(X_train, y_train)\n    \n    # Print model name\n    print(model_name)\n    \n    # Compute 5-fold cross validation: Accuracy\n    acc = cross_val_score(mod, X_train, y_train, scoring = \"accuracy\", cv = 5)\n\n    # Compute 5-fold prediction on training set\n    predictions = cross_val_predict(mod, X_train, y_train, cv = 5)\n\n    # Return accuracy score to 3dp\n    print(\"Accuracy:\", round(acc.mean(), 3))\n \n    # Compute confusion matrix\n    cm = confusion_matrix(predictions, y_train)\n    \n    # Print confusion matrix\n    print(\"Confusion Matrix:  \\n\", cm)\n\n    # Print classification report\n    print(\"Classification Report \\n\", classification_report(predictions, y_train))","cea7b83d":"# Logistic Regression\nlog = LogisticRegression(multi_class='ovr')\nmodel(log, \"Logistic Regression\", X_train, y_train)","a5f05122":"# Importing all required tools for deep learning from keras\nfrom keras.preprocessing.text import Tokenizer\nfrom keras.preprocessing.sequence import pad_sequences\nfrom keras.layers import Dense, Input, LSTM, Embedding, Dropout, Activation, Conv1D, GRU, CuDNNGRU, CuDNNLSTM, BatchNormalization\nfrom keras.layers import Bidirectional, GlobalMaxPool1D, MaxPooling1D, Add, Flatten\nfrom keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\nfrom keras.models import Model, load_model\nfrom keras import initializers, regularizers, constraints, optimizers, layers, callbacks\nfrom keras import backend as K\nfrom keras.engine import InputSpec, Layer\nfrom keras.optimizers import Adam\n\nfrom keras.callbacks import ModelCheckpoint, TensorBoard, Callback, EarlyStopping","9aedafca":"# 1. Tokenization\ntokenizer = Tokenizer(lower=True, filters='')\n\ntokenizer.fit_on_texts(full_text)","1ec0d93b":"# 2. Indexing\ntrain_sequences = tokenizer.texts_to_sequences(train['Clean Review'])\ntest_sequences = tokenizer.texts_to_sequences(test['Clean Review'])","6b0790cd":"#\u00a03. Index Representation\nMAX_LENGTH = 50\n\npadded_train_sequences = pad_sequences(train_sequences, maxlen=MAX_LENGTH)\npadded_test_sequences = pad_sequences(test_sequences, maxlen=MAX_LENGTH)\npadded_train_sequences","a638c3f4":"# Find and plot total word count per sentence\ntotalNumWords = [len(one_comment) for one_comment in train_sequences]\n\nplt.hist(totalNumWords,bins = np.arange(0,20,1))\nplt.show()","eeede0a6":"# Setting max_len to 20 and padding data\nmax_len = 20\nX_train = pad_sequences(train_sequences, maxlen = max_len)\nX_test = pad_sequences(test_sequences, maxlen = max_len)","ddf7c9d7":"#\u00a0Link to 2 million word vectors trained on Common Crawl (600B tokens)\nembedding_path = '..\/input\/fasttext-crawl-300d-2m\/crawl-300d-2M.vec'","11374f31":"#\u00a0Setting embedding size & max number of features\nembed_size = 300\nmax_features = 30000","0f320441":"# Prepare the embedding matrix\ndef get_coefs(word,*arr): \n    return word, np.asarray(arr, dtype='float32')\n\nembedding_index = dict(get_coefs(*o.strip().split(\" \")) for o in open(embedding_path))\n\nword_index = tokenizer.word_index\nnb_words = min(max_features, len(word_index))\nembedding_matrix = np.zeros((nb_words + 1, embed_size))\nfor word, i in word_index.items():\n    if i >= max_features: continue\n    embedding_vector = embedding_index.get(word)\n    if embedding_vector is not None: \n        \n        # Words not found in embedding index will be all-zeros.\n        embedding_matrix[i] = embedding_vector","d541d1c8":"# One hot encoding the y variable ready for deep learning application\nohe = OneHotEncoder(sparse=False)\ny_ohe = ohe.fit_transform(y_train.values.reshape(-1, 1))","64138309":"# Create check-point\nfile_path = \"best_model.hdf5\"\ncheck_point = ModelCheckpoint(file_path, monitor = \"val_loss\", verbose = 1,\n                              save_best_only = True, mode = \"min\")\n\n# Define callbacks\nearly_stop = EarlyStopping(monitor = \"val_loss\", mode = \"min\", patience = 3)\n\n# Build model\ndef build_model(lr = 0.0, lr_d = 0.0, units = 0, dr = 0.0):\n    inp = Input(shape = (max_len,))\n    x = Embedding(19479, embed_size, weights = [embedding_matrix], trainable = False)(inp)\n    x1 = SpatialDropout1D(dr)(x)\n\n    x_gru = Bidirectional(CuDNNGRU(units, return_sequences = True))(x1)\n    x1 = Conv1D(32, kernel_size=3, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool1_gru = GlobalAveragePooling1D()(x1)\n    max_pool1_gru = GlobalMaxPooling1D()(x1)\n    \n    x3 = Conv1D(32, kernel_size=2, padding='valid', kernel_initializer='he_uniform')(x_gru)\n    avg_pool3_gru = GlobalAveragePooling1D()(x3)\n    max_pool3_gru = GlobalMaxPooling1D()(x3)\n    \n    x_lstm = Bidirectional(CuDNNLSTM(units, return_sequences = True))(x1)\n    x1 = Conv1D(32, kernel_size=3, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool1_lstm = GlobalAveragePooling1D()(x1)\n    max_pool1_lstm = GlobalMaxPooling1D()(x1)\n    \n    x3 = Conv1D(32, kernel_size=2, padding='valid', kernel_initializer='he_uniform')(x_lstm)\n    avg_pool3_lstm = GlobalAveragePooling1D()(x3)\n    max_pool3_lstm = GlobalMaxPooling1D()(x3)\n    \n    \n    x = concatenate([avg_pool1_gru, max_pool1_gru, avg_pool3_gru, max_pool3_gru,\n                    avg_pool1_lstm, max_pool1_lstm, avg_pool3_lstm, max_pool3_lstm])\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(Dense(128,activation='relu') (x))\n    x = BatchNormalization()(x)\n    x = Dropout(0.2)(Dense(100,activation='relu') (x))\n    x = Dense(5, activation = \"sigmoid\")(x)\n    \n    model = Model(inputs = inp, outputs = x)\n    model.compile(loss = \"binary_crossentropy\", optimizer = Adam(lr = lr, decay = lr_d), metrics = [\"accuracy\"])\n    history = model.fit(X_train, y_ohe, batch_size = 128, epochs = 15, validation_split=0.1, \n                        verbose = 1, callbacks = [check_point, early_stop])\n    model = load_model(file_path)\n    \n    return model","902d6f65":"# Instantiating model\nmodel = build_model(lr = 1e-4, lr_d = 0, units = 128, dr = 0.5)","71b1bee0":"# Making predictions with test data\npred = model.predict(X_test, batch_size = 1024)","f76b95ee":"# Preparing final submission file\npredictions = np.round(np.argmax(pred, axis=1)).astype(int)\n\nsub['Sentiment'] = predictions\nsub.to_csv(\"submission.csv\", index=False)","619c0831":"To make this step easier I will combine both Datasets and clean as a whole. Marking the Sentiment column as -999 within the test set means there's zero chance of cross contamination.","13331763":"A strong lean towards somewhat negative reviews are contained in this dataset. A strong class imbalance may prove a few issues with machine learning - so good to be aware of this now!","878d013f":"Before any words can be fed into the neural network, a series of processing steps are first required:\n\n1. Tokenization - We need to break down the sentence into unique words. For eg, \"I love cats and love dogs\" will become [\"I\",\"love\",\"cats\",\"and\",\"dogs\"]\n2. Indexing - We put the words in a dictionary-like structure and give them an index each For eg, {1:\"I\",2:\"love\",3:\"cats\",4:\"and\",5:\"dogs\"}\n3. Index Representation- We could represent the sequence of words in the comments in the form of index, and feed this chain of index into the network (e.g. [1,2,3,4,2,5]).\n\nThe below code will complete each of these steps in turn:","c5e17b34":"Machine learning provided limited success to solving this challenge, so over to deep learning we head. The reason deep learning typically outperforms a bag of word models is the ability to capture the sequencial dependency between words in a sentence. This has been possible thanks to the invention of special neural network architectures called Recurrent Neural Networks. \n\nSpecifically in this kernel, I will be working with Long Short Term Memory networks or 'LSTM's' for short. These networks possess a memory that \"remembers\" previous data from the input and makes decisions based on that knowledge. These networks are more directly suited for written data inputs, since each word in a sentence has meaning based on the surrounding words (previous and upcoming words).\n\nBefore I can get my hands dirty with these networks, i'll need to import the required tools from the Keras package:","ed42e2b9":"Finally, we can now define the model architecture.","542f303a":"Thank you for reading this Kernel. I note possible areas for expansion that include ensembling multiple Deep Learning models, as well as blending the final results with those from the Logistic Regression, perhaps with the inclusion of further Machine Learning algorithms that were not introduced in this Kernel (Decision Trees, SVC). These are areas that I may return to in futurel; they may even be of interest to you, too.\n\nLast up, please feel free to offer any feedback on the approach\/code within this Kernel. I am always looking for new or better ways of doing things. Thank you again!","07d224df":"## 3. Creating a text matrix","9607f846":"Now that we have term counts for each document, the TfidfTransformer can be applied to calculate the weights for each term in each document:","5d4512c9":"## 1. Initial Inspection","4681f712":"We're not getting great performance with Machine Learning, as initially expected. I sense this Dataset may better suit Deep Learning application, so that's where I'll head next.","8309c93e":"Kaggle already provides us with training and test datasets, so data preparation at this stage is as simple as computing X & y variables for the training set.","7031854a":"Next up, I will 'pad' the features so that they are of a consistent length - another important processing step.\n\nThe below code will first of all reveal the appropriate max length to set based on the sentences. This 'max_len' will then be applied into the padding step that follows.","536fee85":"With words weighted, the time has now come to make some predictions :).","530a5e7b":"Submissions are evaluated on classification accuracy (the percent of labels that are predicted correctly) for every parsed phrase. The sentiment labels are:\n\n0 - negative<br>\n1 - somewhat negative<br>\n2 - neutral<br>\n3 - somewhat positive<br>\n4 - positive","bbe60cb4":"## LSTM-CNN Model\n\nNow onto the model! This CNN-LSTM model consists of an initial LSTM layer which will receive word embeddings for each token in the review as inputs. The intuition is that its output tokens will store information not only of the initial token, but also any previous tokens; In other words, the LSTM layer is generating a new encoding for the original input. The output of the LSTM layer is then fed into a convolution layer which we expect will extract local features. Finally the convolution layer\u2019s output will be pooled to a smaller dimension and ultimately outputted as either a positive or negative label.\n\nI must credit the following source which introduces the idea of LSTM-CNN Model for Sentiment Analysis. I encourage you to take a read! - http:\/\/konukoii.com\/blog\/2018\/02\/19\/twitter-sentiment-analysis-using-combined-lstm-cnn-models\/","c548c7b0":"Any piece of text which is not relevant to the context of the data and the end-output can be specified as the noise.\n\nFor example \u2013 language stopwords (commonly used words of a language \u2013 is, am, the, of, in etc), URLs or links, punctuations and industry specific words. This step deals with removal of all types of noisy entities present in the text.\n\nTypically I would define a function to clear all of this noise, to then yield better Machine Learning application. However this dataset contains a lot of single word entries. To remove numbers, punctuation, special characters and whole words would wipe out a large percentage of the available observations, potentially then proving more trouble than what it's worth when modelling. So, I am going to keep them in and proceed with a more 'light touch' noise removal function:","455234be":"Because of the multi-label loss, we are using k-hot encoding of the output and sigmoid activations. As a result, the loss is binary cross-entropy. With the LSTM-CNN model defined, we can now fit, predict and then submit.","903787e7":"## 4. Machine Learning","ff95e1f3":"I will apply a Logistic Regression initially, since it is equipped to handle multi-class classifications problems and over a relatively short amount of time.","e90f8c5b":"To help with Machine Learning I will define a function that will return the most prized statistics in one go. After instantiating a model, this function will return an mean accuracy score following 5 folds of cross validation - this is to ensure that we are getting a smoothed out representation of both the training and test sets. Next this function will provide us with the Confusion Matrix; how many correct vs incorrect classifications have actually taken place within the given model? Last up, this function will churn out for us a Classification Report which details other important metrics such as Precision, Recall, the F1 score (which is just the harmonic mean of the former two), and support (which is the classification count). \n\nCombined, these metrics will provide rich insight into individual model performance and will guide better selection towards the best performing model, and how best to optimise it.","43fe37a0":"I am happy with that number as a starting point, less than 1000 was my initial aim. If I wanted to be more or less restrictive on n-gram selection, I could adjust the 'min_df' and 'max_df' parameters within my CountVectorizer, which controls for the minimum and maximum amount of documents each word should feature in.\n\nWe can now tackle the next step, which is to turn this document into a <b>'bag of words' representation<\/b>. This is essentially just a separate column for each term containing the count within each document. After that, we\u2019ll take a look at the <b>sparsity<\/b> of this representation which lets us know how many <b>nonzero values<\/b> there are in the dataset. The more sparse the data is the more challenging it will be to model, but that\u2019s a discussion for another day:","e72bf4c6":"# Movie Review Sentiment Analysis","12abeeb3":"With the text cleaned, the data can now be split back into training and test sets.","5a45ca2c":"###\u00a0Noise Removal","3dc9debe":"Some interesting headlines from the above - there are a much larger proportion of phrases compared to sentences, and the average word length of phrases at 7 is quite low. This will need to considered when cleaning the text in order to strike the right balance between making the data neater, and losing too much data that renders Machine Learning more difficult.","7f64109e":"To analyse a preprocessed data, it needs to be converted into features. Depending upon the usage, text features can be constructed using a variety of techniques \u2013 in this kernel I will be converting the data into statistical features.\n\nThe specific model in question is known as <b>'Term Frequency \u2013 Inverse Document Frequency' (TF \u2013 IDF)<\/b>\n\nTF-IDF is a weighted model commonly used for information retrieval problems. It aims to convert the text documents into vector models on the basis of occurrence of words in the documents without taking considering the exact ordering. For Example \u2013 let say there is a dataset of N text documents, In any document \u201cD\u201d, TF and IDF will be defined as \u2013\n\n- <b>Term Frequency (TF)<\/b> \u2013 TF for a term \u201ct\u201d is defined as the count of a term \u201ct\u201d in a document \u201cD\u201d\n- <b>Inverse Document Frequency (IDF)<\/b> \u2013 IDF for a term is defined as logarithm of ratio of total documents available in the corpus and number of documents containing the term T.\n- <b>TF . IDF<\/b> \u2013 TF IDF formula gives the relative importance of a term in a corpus (list of documents), given by the following formula below. Following is the code using python\u2019s scikit learn package to convert a text into tf idf vectors:","0c5f66d3":"## 2. Text Preprocessing","b879ae9a":"In this Kernel I will be taking on the Kaggle competition whereby the challenge is to classify the sentences from the famous Rotten Tomatoes Movie Review dataset, with the aim to predict each sentence's classification to 1 of 5 labels. \n\nThis will be my first application of Deep Learning within a Kaggle competition, therefore I would greatly appreciate any feedback on the techniques employed during this section - as well as any other section for that matter! \n\nI hope you enjoy the read.","8acec23c":"The last step in this initial exploration is to explore the target in a little more detail. A graph should help with that!","6f4c3670":"Let's take an initial peak into what we're dealing with:","90718746":"Looks like a simple Dataset to get to grips with (which is always welcome). We can see here that each sentence is broken into multiple phrases, each having their own sentiment classification. Each sentence is denoted by the 'SentenceId' column.","7c65578d":"I'll take a further dive into understanding more about the sentence structure within both the training & test Dataset:","bb5e5d69":"## 5. Deep Learning"}}