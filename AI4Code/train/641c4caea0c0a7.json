{"cell_type":{"4f36282e":"code","3eb5d03d":"code","ab191195":"code","bee6ab98":"code","8c454b91":"code","1cd2619e":"code","ebf4c8ab":"code","84f7f9a5":"code","84eed44b":"code","23d5a0ad":"code","8121d53e":"code","4cf83da3":"code","9527ea52":"code","164384f3":"code","ee09aa94":"code","9df6da83":"code","47bceda3":"code","3484bbb9":"code","65a90020":"code","7d203d71":"code","ccec0e25":"code","6945ff97":"code","071ae4c2":"code","80622162":"code","7687c5e2":"code","fadba283":"code","104a1762":"code","6cc3667e":"code","ea931f0d":"code","17fa2dba":"code","b7bea125":"code","69101728":"code","7793d2b7":"code","bfb9afca":"markdown","d61c215a":"markdown","0c28abd2":"markdown","d6ffcf19":"markdown","fa425eda":"markdown","96f1b83c":"markdown","2b8e81c5":"markdown","72b22360":"markdown","9faea1df":"markdown","ff365a68":"markdown","f677137c":"markdown","77b0b09e":"markdown","80ef9095":"markdown","53bcb435":"markdown","71f60431":"markdown","085c953a":"markdown","c5c17b1c":"markdown","1645ef8d":"markdown","5bfb17a2":"markdown","9b43aa4e":"markdown","5be2edf7":"markdown","d9e3f28f":"markdown","497605b7":"markdown","901eec2e":"markdown","cf761864":"markdown","4395659a":"markdown","580ca859":"markdown","a2386a50":"markdown"},"source":{"4f36282e":"import numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nimport seaborn as sns\nimport operator\nimport plotly\nimport plotly.graph_objects as go\nimport plotly.express as px\nimport matplotlib.pylab as pylab\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.preprocessing import MinMaxScaler\nfrom sklearn.metrics import classification_report\nparams = {'legend.fontsize': 'x-large',\n          'figure.figsize': (15, 5),\n         'axes.labelsize': '18',\n         'axes.titlesize':'24',\n         'xtick.labelsize':'12',\n         'ytick.labelsize':'12'}\npylab.rcParams.update(params)\ndirname = '\/kaggle\/input\/'","3eb5d03d":"# Utility functions\ndef set_age_class(row):\n    toReturn = row\n    toReturn['age_class'] = -1\n    if row['age'] > 0 and row['age'] <= 20:\n        toReturn['age_class'] = 1 # Neo-drivers\n    elif 20 < row['age'] <= 40:\n        toReturn['age_class'] = 2 # young drivers\n    elif 40 < row['age'] <= 60:\n        toReturn['age_class'] = 3 # experienced drivers\n    elif row['age'] > 60:\n        toReturn['age_class'] = 4 # old drivers\n    \n    return pd.Series(toReturn, index=list(toReturn.keys()))\n\ndef remove_redundacy(data, drivers):\n    '''\n        The problem of merging the four dataset is that the\n        accidents data is duplicated. So, this function remove \n        duplicates from merged dataset taking care of keeping \n        the accidents data unique.\n    '''\n    toReturn = pd.DataFrame()\n    num_accidents = vehicles['Num_Acc'].unique()\n    #print(len(num_accidents))\n    for acc in num_accidents:\n        people = drivers[drivers['Num_Acc'] == acc]\n        row = data[data['Num_Acc'] == acc].head(people.shape[0])\n        toReturn = toReturn.append(row, ignore_index=True)\n    \n    return toReturn\n\n# Functions\ndef sort_dict(dictionary, reverse=False):\n    # this will return a list of tuple ordered by values\n    sorted_dict = sorted(dictionary.items(), key=operator.itemgetter(1), reverse=reverse)\n    sorted_dict = {k: v for k, v in sorted_dict}\n    return sorted_dict\n\ndef add_coordinates(row, coords):\n    toReturn = row\n    \n    toReturn['lat'] = coords[row['dep']][0]\n    toReturn['long'] = coords[row['dep']][1]\n    \n    return pd.Series(toReturn, index=list(toReturn.keys()))","ab191195":"characteristics = pd.read_csv(dirname + '2019-database-of-road-traffic-injuries\/caracteristiques-2019.csv')\nplaces = pd.read_csv(dirname + '2019-database-of-road-traffic-injuries\/lieux-2019.csv')\nvehicles = pd.read_csv(dirname + '2019-database-of-road-traffic-injuries\/vehicules-2019.csv')\ndrivers = pd.read_csv(dirname + '2019-database-of-road-traffic-injuries\/usagers-2019.csv')","bee6ab98":"characteristics.dropna(inplace= True)","8c454b91":"months_counts = characteristics['mois'].value_counts().sort_index().values.tolist()\nlabels = ['Jen', 'Feb', 'Mar', 'Apr',\n         'May', 'Jun', 'Jul', 'Aug',\n         'Sep', 'Oct', 'Nov', 'Dec']\n\nmonths = {}\nfor i in range(len(months_counts)):\n    months[labels[i]] = (months_counts[i] \/ sum(months_counts)) * 100\n    \nfig, ax = plt.subplots(figsize=(15, 10))\n\nsns.barplot(x=list(months.keys()), y=list(months.values()));\n\nax.set_xlabel('Months')\nax.set_ylabel('Accidents Counts (%)')\nax.set_title('Number of Accidents by Monhts');","1cd2619e":"day_counts = characteristics['jour'].value_counts().sort_index().values.tolist()\nacc_by_days = characteristics['jour'].value_counts().sort_index()\ndays = {}\nfor i in range(len(day_counts)):\n    days[i + 1] = (day_counts[i] \/ sum(day_counts)) * 100\n\nacc_means = [acc_by_days[0:10].mean()\/ sum(day_counts) * 100, \n             acc_by_days[11:20].mean()\/ sum(day_counts) * 100, \n             acc_by_days[21:31].mean()\/ sum(day_counts) * 100]\n\nfig, ax1 = plt.subplots(figsize=(15, 10))\n\nsns.barplot(ax = ax1, x=list(days.keys()), y = list(days.values()), palette = sns.color_palette('hls', 31));\nsns.lineplot(ax=ax1, x=[0, 15, 31], y=acc_means, marker='o', markersize=14)\n# label points on the plot\nfor x, y in zip([0, 15, 31], acc_means):\n    # the position of the data label relative to the data point can be adjusted by adding\/subtracting a value from the x &\/ y coordinates\n    if y == max(acc_means):\n        color = 'red'\n    else:\n        color = 'blue'\n    plt.text(x = x, # x-coordinate position of data label\n    y = y+0.05, # y-coordinate position of data label, adjusted to be 150 below the data point\n    s = '{:.2f}%'.format(y), # data label, formatted to ignore decimals\n    color = color, size= 14) # set colour of line\n\nax1.set_title('Number of Accidents by Days of Month')\nax1.set_xlabel('Days of Month')\nax1.set_ylabel('% Accidents');","ebf4c8ab":"# From variable description hrmn is a percentage of 24hours\n# So, I've multiplied hrmn to the total (24) and subtracted 24 to get the time as float\ncharacteristics['daytime'] = 24 - (characteristics['hrmn'] * 24)\n\n# Now, I've taken the integer part of those float numbers. This is my hour estimation of the accident\ncharacteristics['daytime'] = characteristics['daytime'].astype(int)\n\ndaytime_counts = characteristics['daytime'].value_counts().sort_index().values.tolist()\n\ndaytimes = {}\nfor i in range(len(daytime_counts)):\n    if i == 24:\n        daytimes[1] = (daytime_counts[0] \/ sum(daytime_counts)) * 100\n    else:\n        daytimes[i + 1] = (daytime_counts[i] \/ sum(daytime_counts)) * 100\n\nfig, (ax1, ax2) = plt.subplots(figsize=(15, 10), ncols=2)\n\nsns.barplot(ax=ax1, x = list(daytimes.keys()), y = list(daytimes.values()))\nax1.set_title('Count of accidents by hour')\nax1.set_xlabel('Hours')\nax1.set_ylabel('% of accidents');\n\n#define data\nslices = characteristics['lum'].value_counts().sort_index().values.tolist()\nlabels = ['Full Day', 'Twilight', 'Night without public lighting', \n          'Night with public lighting not on', 'Night with public lighting on']\n\n#define Seaborn color palette to use\ncolors = sns.color_palette('pastel')[0:5]\n\n#create pie chart\nax2.pie(slices, labels = labels, colors = colors, autopct='%.0f%%');\nax2.set_title('Period of the day');","84f7f9a5":"fig, ax = plt.subplots(figsize=(10, 10))\n\nslices = characteristics['agg'].value_counts().sort_index().values.tolist()\nlabels = ['Outside agglomeration', 'In built-up areas']\n\n#define Seaborn color palette to use\ncolors = sns.color_palette('pastel')[0:2]\n\n#create pie chart\nplt.pie(slices, labels = labels, colors = colors, autopct='%.0f%%')\nplt.show()","84eed44b":"atm_counts = characteristics['atm'].value_counts().sort_index().values.tolist()\nlabels = ['Not Specified', 'Normal', 'Light Rain', 'Heavy Rain',\n         'Sno hail', 'Fog smoke', 'Strong wind', 'Dazzling weather',\n         'Cloudy weather', 'Other']\natm = {}\nfor i in range(len(atm_counts)):\n    atm[labels[i]] = (atm_counts[i] \/ sum(atm_counts)) * 100\n    \n\natm = sort_dict(atm, reverse=True)\nfig, ax = plt.subplots(figsize=(15, 10))\n\nsns.barplot(y = list(atm.keys()), x = list(atm.values()),\n            palette = sns.color_palette('hls', 10), orient='horizontal');\n\nax.set_title('Weather Conditions')\nax.set_xlabel('% of Accidents');","23d5a0ad":"dep_coords = {}\ndepartments = characteristics['dep'].unique().tolist()\nfor dep in departments:\n    row = characteristics[characteristics['dep'] == dep]\n    dep_coords[dep] = (row['lat'].values[0] \/ 10000000, row['long'].values[0] \/ 10000000)\n\ndep_group = characteristics.groupby('dep').count().reset_index()\n\n\n\ndep_group = dep_group.apply(add_coordinates, coords=dep_coords, axis=1)\n\ndep_group\n\nfig = px.scatter_mapbox(dep_group, lat=\"lat\", lon=\"long\", hover_name=\"dep\", hover_data=[\"Num_Acc\"],\n                        color_discrete_sequence=['blue'], size='Num_Acc', zoom=4.5,\n                       center = {'lat': 47.89975, 'lon': 3.520934})\nfig.update_layout(mapbox_style=\"open-street-map\")\nfig.update_layout(margin={\"r\":0,\"t\":0,\"l\":0,\"b\":0})\nfig.show()","8121d53e":"catr_counts = places['catr'].value_counts().sort_index().values.tolist()\nlabels = ['Highway', 'National Road', 'Departmental Road', 'Communal Roads',\n         'Outside Public Network', 'Parking', 'Urban Roads', 'Others']\ncatr = {}\nfor i in range(len(catr_counts)):\n    catr[labels[i]] = round((catr_counts[i] \/ sum(catr_counts)) * 100, 2)\n    \nsorted_catr = sort_dict(catr, reverse=True)\nfig = go.Figure()\nfig.add_trace(go.Indicator(\n        mode = \"number\",\n        value = sorted_catr['Outside Public Network'],\n        title = {'text': 'Outside Public Network', 'font': {'color': '#f6b179'}},\n        number = {'suffix': \"%\", 'font': {'size': 30, 'color': '#f6b179'}},\n        domain = {'x': [0, 0.5], 'y': [0, 0.1]}))\nfig.add_trace(go.Indicator(\n        mode = \"number\",\n        value = sorted_catr['Parking'],\n        title = {'text': 'Parking', 'font': {'color': '#f6b179'}},\n        number = {'suffix': \"%\", \"font\":{\"size\":30, 'color': '#f6b179'}},\n        domain = {'x': [0, 0.8], 'y': [0, 0.2]}))\nfig.add_trace(go.Indicator(\n        mode = \"number\",\n        value = sorted_catr['Others'],\n        title = {'text': 'Others', 'font': {'color': '#f6b179'}},\n        number = {'suffix': \"%\", \"font\":{\"size\":30, 'color': '#f6b179'}},\n        domain = {'x': [0, 1], 'y': [0, 0.4]}))\n\nfig.add_trace(go.Indicator(\n        mode = \"number\",\n        value = sorted_catr['Urban Roads'],\n        title = {'text': 'Urban Roads', \"font\":{'color': '#f39b53'}},\n        number = {'suffix': \"%\", \"font\":{\"size\":40, 'color': '#f39b53'}},\n        domain = {'x': [0, 0.3], 'y': [0, 0.5]}))\n\nfig.add_trace(go.Indicator(\n        mode = \"number\",\n        value = sorted_catr['National Road'],\n        title = {'text': 'National Road', \"font\":{'color': '#f39b53'}},\n        number = {'suffix': \"%\", \"font\":{\"size\":50, 'color': '#f39b53'}},\n        domain = {'x': [0, 0.7], 'y': [0, 0.6]}))\n\nfig.add_trace(go.Indicator(\n        mode = \"number\",\n        value = sorted_catr['Highway'],\n        title = {'text': 'Highway', \"font\":{'color': '#ef7a1a'}},\n        number = {'suffix': \"%\", \"font\":{\"size\":60, 'color': '#ef7a1a'}},\n        domain = {'x': [0.5, 0.1], 'y': [0.2, 1]}))\n\nfig.add_trace(go.Indicator(\n        mode = \"number\",\n        value = sorted_catr['Departmental Road'],\n        title = {'text': 'Departmental Road', \"font\":{'color': '#e45c3a'}},\n        number = {'suffix': \"%\", \"font\":{\"size\":70, 'color': '#e45c3a'}},\n        domain = {'x': [0, 0.2], 'y': [0.2, 1]}))\nfig.add_trace(go.Indicator(\n        mode = \"number\",\n        value = sorted_catr['Communal Roads'],\n        title = {'text': 'Communal Roads', \"font\":{'color': '#e45c3a'}},\n        number = {'suffix': \"%\", \"font\":{\"size\":80, 'color': '#e45c3a'}},\n        domain = {'x': [0, 0.6], 'y': [0.9, 1]}))","4cf83da3":"vma_counts = places['vma'].value_counts().sort_index().values.tolist()\nlabels = ['0', '1', '3', '4', '5', '6', '7', '10',\n         '12', '15', '20', '25', '30', '35', '40',\n         '42', '45', '50', '60', '65', '70', '80',\n         '90', '100', '110', '120']\n\nvma = {\n    '<=10 km\/h': 0,\n    '>10 km\/h & <=50 km\/h': 0,\n    '>50 km\/h & <=120 km\/h': 0,\n}\nfor i in range(len(vma_counts)):\n    if i < 9:\n        vma['<=10 km\/h'] += vma_counts[i]\n    elif 9 < i <= 18:\n        vma['>10 km\/h & <=50 km\/h'] += vma_counts[i]\n    elif 18 < i <= 26:\n        vma['>50 km\/h & <=120 km\/h'] += vma_counts[i]\n\nfor key, value in vma.items():\n    vma[key] = (value \/ sum(vma_counts)) * 100\n\nvma = sort_dict(vma, reverse=True)\n\nfig, ax = plt.subplots(figsize=(15, 10))\n\nsns.barplot(x = list(vma.keys()), y = list(vma.values()));\nax.set_title('Velocity at the moment of the accident')\nax.set_xlabel('Velocity ranges')\nax.set_ylabel('% of Accidents');","9527ea52":"%%time\ndrivers['age'] = abs(datetime.now().year - drivers['an_nais'])\ndrivers = drivers.apply(set_age_class, axis=1)","164384f3":"# From variable catu (User category) I'm filtering by drivers (catu == 1) to analyze the age class of them\nage_class_counts = drivers[drivers['catu'] == 1]['age_class'].value_counts().sort_index().values.tolist()\nlabels = ['Neo-Drivers (<=20)', 'Young Drivers (20 - 40)', 'Experienced Drivers(40 - 60)', 'Old Drivers(>60)']\nage_class = {}\nfor i in range(len(age_class_counts)):\n    age_class[labels[i]] = (age_class_counts[i] \/ sum(age_class_counts)) * 100\n    \nage_class = sort_dict(age_class, reverse=True)\n\nfig, ax = plt.subplots(figsize=(15, 10))\n\nsns.barplot(x = list(age_class.values()), y = list(age_class.keys()), orient='horizontal');\nax.set_title('Drivers\\'s category')\nax.set_xlabel('% of Accidents');","ee09aa94":"gender = pd.crosstab(drivers['catu'], drivers['sexe'], normalize='all').reset_index()\ndrivers_dict = {\n    'Male': round(gender[gender['catu'] == 1][1].values[0] * 100, 2),\n    'Female': round(gender[gender['catu'] == 1][2].values[0] * 100, 2)\n}\n\npassenger_dict = {\n    'Male': round(gender[gender['catu'] == 2][1].values[0] * 100, 2),\n    'Female': round(gender[gender['catu'] == 2][2].values[0] * 100, 2)\n}\n\npedastrian_dict = {\n    'Male': round(gender[gender['catu'] == 3][1].values[0] * 100, 2),\n    'Female': round(gender[gender['catu'] == 3][2].values[0] * 100, 2)\n}\n\n# Function for stacked bar plot\ndef show_values_on_bars(axs):\n    def _show_on_single_plot(ax):        \n        for p in ax.patches:\n            _x = p.get_x() + p.get_width() \/ 2\n            _y = p.get_y() + p.get_height()\n            value = '{:.2f}'.format(p.get_height())\n            ax.text(_x, _y, value, ha=\"center\", fontsize='24') \n\n    if isinstance(axs, np.ndarray):\n        for idx, ax in np.ndenumerate(axs):\n            _show_on_single_plot(ax)\n    else:\n        _show_on_single_plot(axs)\n\n\nimport matplotlib.patches as mpatches\nfig, ax = plt.subplots(figsize=(10, 10))\n\nbar1 = sns.barplot(x=list(drivers_dict.keys()),  y=list(drivers_dict.values()), color='cornflowerblue')\n\nbar2 = sns.barplot(x=list(passenger_dict.keys()), y=list(passenger_dict.values()), color='teal')\n\nbar3 = sns.barplot(x=list(pedastrian_dict.keys()), y=list(pedastrian_dict.values()), color='lightseagreen')\n\n\n# add legend\ntop_bar = mpatches.Patch(color='cornflowerblue', label='Drivers')\nmiddle_bar = mpatches.Patch(color='teal', label='Passenger')\nbottom_bar = mpatches.Patch(color='lightseagreen', label='Pedastrian')\nplt.legend(handles=[top_bar, middle_bar, bottom_bar])\n\n\nshow_values_on_bars(ax)\n","9df6da83":"drivers['Num_Acc'] = drivers['Num_Acc'].astype(str)\nvehicles['Num_Acc'] = vehicles['Num_Acc'].astype(str)\n\n# Merging the four dataset on Num_Acc\ndata = pd.merge(left=characteristics, right=places, on='Num_Acc')\ndata = data.merge(vehicles, left_on='Num_Acc', right_on='Num_Acc', how='inner')\ndata = data.merge(drivers, left_on='Num_Acc', right_on='Num_Acc', how='inner')","47bceda3":"#\u00a0Let's remove the columns with high NaN values that are occutc, v2, lartpc, larrout\ndata = data.drop(['occutc', 'v2', 'lartpc', 'larrout'], axis=1)\n\n# Convert lat and long in coordinates\ndata['lat'] = data['lat'] \/ 10000000\ndata['long'] = data['long'] \/ 10000000\n\ndf_copy = data.copy()","3484bbb9":"%%time\n# This code line takes a lot of time, so it is commented\n#df = remove_redundacy(df_copy, drivers)","65a90020":"# Storing the resulting dataset of previous code line\n#df.to_csv('\/kaggle\/working\/traffic-complete.csv', index=False)","7d203d71":"df = pd.read_csv('\/kaggle\/input\/processed\/road-traffic-complete.csv')","ccec0e25":"# Visualizing the target\ngrav_counts = df['grav'].value_counts().sort_index().values.tolist()\nlabels = ['Unharmed', 'Killed', 'Injured Hospitalized', 'Slightly Injured']\n\ngrav = {}\nfor i in range(len(grav_counts)):\n    grav[labels[i]] = (grav_counts[i] \/ sum(grav_counts)) * 100\n    \nfig, ax = plt.subplots(figsize=(15, 10))\n\nsns.barplot(y = list(grav.values()), x = list(grav.keys()))\nax.set_title('Gravity of accidents')\nax.set_xlabel('Gravity')\nax.set_ylabel('% of accidents');","6945ff97":"# Due to unbalanced class I'm going to oversample the dataset\n# Let's take the class counts\ngrav_counts = df['grav'].value_counts().sort_index().values.tolist()\nlabels = ['Unharmed', 'Killed', 'Injured Hospitalized', 'Slightly Injured']\ngrav = {}\nfor i in range(len(grav_counts)):\n    grav[labels[i]] = grav_counts[i]\n\nunharmed_counts = grav['Unharmed']\nkilled_counts = grav['Killed']\ninjured_hosp_counts = grav['Injured Hospitalized']\ninjured_sli_counts = grav['Slightly Injured']\n\n# Define different dataframe, one for each gravity class\ndf_unharmed = df[df['grav'] == 1]\ndf_killed = df[df['grav'] == 2]\ndf_injured_hosp = df[df['grav'] == 3]\ndf_injured_sli = df[df['grav'] == 4]\n\n# Take a sample of <grav-class>_counts from each previous created dataframe with replace=True to get oversampling\ndf_killed_over = df_killed.sample(unharmed_counts, replace=True, random_state=42)\ndf_injured_hosp_over = df_injured_hosp.sample(unharmed_counts, replace=True, random_state=42)\ndf_injured_sli_over = df_injured_sli.sample(unharmed_counts, replace=True, random_state=42)\n\n# Finally, concat all the oversampled dataframe\ndf_over = pd.concat([df_unharmed, df_killed_over, df_injured_hosp_over, df_injured_sli_over], axis=0)","071ae4c2":"df_over['grav'].value_counts()","80622162":"df = df_over.copy()","7687c5e2":"df.isna().sum() \/ data.shape[0]","fadba283":"# Normalize numeric variables\nto_normalize = ['vma', 'age', 'pr', 'pr1']\n\n# This will turn all the strings value into category values\nfor label, content in df.items():\n    if pd.api.types.is_string_dtype(content):\n        df[label] = content.astype('category').cat.as_ordered()\n\n        \n# Turn categorical variables into numbers\nfor label, content in df.items():\n    if not label in to_normalize:\n        df[label] = pd.Categorical(content).codes + 1\n\n# Let's drop some feature. They are codes and ids not useful for modelling\n# More over some columns have a lot of NaN\ndf = df.drop(['hrmn', 'adr', 'lat', 'long', 'voie', 'v1', 'Num_Acc',\n              'id_vehicule_x', 'num_veh_x', 'id_vehicule_y', 'num_veh_y',\n              'place', 'an_nais'], axis=1)\n\n# These columns contain a lot of -1 meaning Unknown value\ndf = df.drop(['secu2', 'secu3', 'locp', 'actp', 'etatp'], axis=1)\n\n# Scaling the numeric variables\nscaler = MinMaxScaler()\ndf[to_normalize] = scaler.fit_transform(df[to_normalize])\n\ndf.dropna(inplace=True)","104a1762":"df.shape","6cc3667e":"from sklearn.feature_selection import mutual_info_classif, SelectKBest\nfrom sklearn.model_selection import train_test_split\n\nX = df.drop('grav', axis=1)\ny = df['grav']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n\nfs = SelectKBest(score_func=mutual_info_classif, k='all')\n\ntmp = fs.fit_transform(X_train, y_train)\n\n# what are scores for the features\ncolumn_labels = X_train.columns\nimportances = {}\nfor i in range(len(fs.scores_)):\n    #print('Feature %s: %f' % (column_labels[i], fs.scores_[i]))\n    importances[column_labels[i]] = fs.scores_[i]","ea931f0d":"fig, ax = plt.subplots(figsize=(10, 10))\nsns.barplot(x = list(importances.values()), y = list(importances.keys()), orient = 'horizontal')\nplt.axvline(x=[0.02], color='red', ls='--')\nplt.text(x = 0.025, # x-coordinate position of data label\n    y = 0.45, # y-coordinate position of data label, adjusted to be 150 below the data point\n    s = 'Selecting threshold 0.02', # data label, formatted to ignore decimals\n    color = 'red', size= 14)\nax.set_title('Feature Importances')\nax.set_xlabel('Scores');","17fa2dba":"# Creating a list of selected features\nfeature_selected = []\nfor column, value in importances.items():\n    if value > 0.02:\n        print(column)\n        feature_selected.append(column)","b7bea125":"%%time\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.linear_model import SGDClassifier\n\n# data = df[feature_selected + ['grav']]\n# X = data.drop('grav', axis=1)\n# y = data['grav']\n\n# X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n\n# dt_grid = {'criterion': ['gini', 'entropy'],\n#             'max_depth': [None, 3, 5, 10],\n#             'splitter': ['best', 'random'],\n#             'max_features': ['auto', 'sqrt', 'log2'],\n#             'min_samples_split': np.arange(2, 20, 2),\n#             'min_samples_leaf': np.arange(1, 20, 2)\n#           }\n# \n# rf_grid = {'n_estimators': [1000],\n#             'max_depth': [None, 3, 5, 10],\n#             'min_samples_split': np.arange(2, 20, 5),\n#             'min_samples_leaf': np.arange(1, 20, 5)\n#           }\n# \n# log_reg_grid = {'C': np.logspace(-4, 4, 20),\n#                'solver': ['liblinear']}\n# \n# models = {\n#     'naive-bayes': {'model': GaussianNB(), 'grid': {'var_smoothing': np.logspace(0,-9, num=100)}},\n#     'decision-tree': {'model': DecisionTreeClassifier(), 'grid': dt_grid},\n#     'random-forest': {'model': RandomForestClassifier(), 'grid': rf_grid},\n#     'logistic-regression': {'model': LogisticRegression(), 'grid': log_reg_grid}\n# }\n\n# model_scores = {}\n# for name, model in models.items():\n#     \n#     print(f'Grid search model: {name}')\n#     #model['model'].fit(X_train, y_train)\n#     gs_model = GridSearchCV(model['model'], param_grid=model['grid'], cv=5)\n#     \n#     gs_model.fit(X_train, y_train)\n#     \n#     model_scores[name] = {'score': gs_model.score(X_test, y_test), 'best-params': gs_model.best_params_}\n#     \n#     print(f'Score: {model_scores[name]}')","69101728":"%%time\n# I'm removing these feauters because my goal si to predict the possibilty to have an accident\n# given certain paramaters. So, attributes that describes something that already happend as\n# the collision type of the accident (col) or the lane on which the accident is happened (situ) or\n# the obstacles made before the accidents\nto_remove = ['col', 'situ', 'obs', 'obsm']\ndata = df[feature_selected + ['grav']]\nX = data.drop(to_remove + ['grav'], axis=1)\ny = data['grav']\n\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42)\n\nrf = RandomForestClassifier(n_estimators=1000, max_depth=None, min_samples_leaf=1, min_samples_split=2)\n\nrf.fit(X_train, y_train)\nscore = rf.score(X_test, y_test)\n\nprint('Score: ', score)","7793d2b7":"y_preds = rf.predict(X_test)\nprint(classification_report(y_test, y_preds))","bfb9afca":"The model works very well on predicting the class *killed* (label 2)","d61c215a":"The *selecting threshold* has been chosen arbitrarily.","0c28abd2":"The best model is Random Forest (score 86.46%) with the following parameters:\n\n* Max Depth: None\n* Min Samples Leaf: 1\n* Min Samples Split: 2\n* N Estimators: 1000","d6ffcf19":"Let's look at the presence of *NaN* values","fa425eda":"# Feature Selection","96f1b83c":"Surprisingly, the most accidents happened at low velocity","2b8e81c5":"# Normalization","72b22360":"During the month the greater percentage of accidents is registered, in mean, in the middle of the months between the 11th and 20th day.","9faea1df":"In literature it is common to proceed with undersampling or oversampling. In this case I preferred oversampling because of very low samples in *killed* class","ff365a68":"Young drivers (from 20s to 40s) caused most of the accidents","f677137c":"Let's start to visualize temporal data. \n\nThe majority of the accidents happen in the middle of summer: **June** and **July** the months with highest percentage of accidents","77b0b09e":"# Exploratory Analysis","80ef9095":"I've scaled only numeric variables that are: the velocity (vma), the age (age), pr (number of the upstream terminal) and pr1 (distance from the upstream terminal). Anyway, I haven't found around the internet what *upstream terminal* means, but these variables are numeric types and so I scaled them.","53bcb435":"Road accidents are a real problem that cities have to deal with, they can impact different aspects of classic city life most important they can produce injured and increase the traffic. So, it could be interesting to discover how to face this problem at the root trying to lower the probability of an accident.\nThe main goal of this report are:\n> Get insights about accidents\n\n> Try to predict accidents based on important features ","71f60431":"Classes are remarkably unbalanced","085c953a":"Females make less accidents than Males","c5c17b1c":"# Modelling\nTo face with *mixed-data* dataset (composed by qualitative and quantitative data) I've made a GridSearch between these models suggested by literature: Naive Bayes, Decision Trees, Random Forest and Logistic Regression.\nFor the sake of transparency, the next chunk shows the code to select the best model, but the code is commented because it will take a lot of time to complete the process.","1645ef8d":"# Preparing dataset for modelling","5bfb17a2":"# Conclusion","9b43aa4e":"The following chart is a geographical view of the accidents' frequency throughout the France.","5be2edf7":"The accidents happen mostly in cities","d9e3f28f":"It seems that weather conditions is not so important because the 79% of the accidents happen in normal weather conditions.","497605b7":"# Problem Definition \/ Goals","901eec2e":"## Preparing the tools","cf761864":"One of the suitable method to select features with a dataset where the majority of the variables are categorical (or qualitative) is *Mutal Information*  from the field of information theory. For more details: [MachineLearningMastery](https:\/\/machinelearningmastery.com\/feature-selection-with-categorical-data\/)","4395659a":"The accidents happen often on communal and departmental roads according with the previous observations.","580ca859":"During the day the majority of accidents happens in the early morning with full light condition.","a2386a50":"The report starts with an exploratory analysis where some insights popped up. These insights shed light on different aspects of the accidents in France and in its department overseas, the following are some important observations:\n* The accidents happen in mean in the early summer (most in June and July). During the month the highest frequency of accidents happen in the middle and early in the day\n* These accidents occur often in the urban area on communal and departmental roads at low velocity (between 10 and 50 km\/h)\n* Young drivers (20 to 40 years old people) are responsible for the greatest part of the accidents.\n* Most accidents have a light severity level (mostly unharmed and slightly injured)\n* The most important features to evaluate the probability to have an accident seems to be the light condition (lum), the department and municipality where you want to travel (dep and com), the location (outside or inside built-up areas), the road category (highway, departmental road and so on), the traffic intensity and total traffic lanes, the manoeuvre, the number of upstream terminals, velocity (vma), the user category, the reason for the travel at the time of the accident and finally the age.\n\nFinally, the model can be useful to predict the probability to make an accident given the selected features, with an accuracy of 87.04%. It would be interesting to create a service that evaluates the probability to have an accident given the city where you desire to travel, so you can travel to that city paying more attention."}}